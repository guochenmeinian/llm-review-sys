# The Power of Resets in Online Reinforcement Learning

 Zakaria Mhammedi

Google Research

mhammedi@google.com &Dylan J. Foster

Microsoft Research

dylanfoster@microsoft.com &Alexander Rakhlin

MIT

rakhlin@mit.edu

###### Abstract

Simulators are a pervasive tool in reinforcement learning, but most existing algorithms cannot efficiently exploit simulator access--particularly in high-dimensional domains that require general function approximation. We explore the power of simulators through online reinforcement learning with _local simulator access_ (or, local planning), an RL protocol where the agent is allowed to _reset_ to previously observed states and follow their dynamics during training. We use local simulator access to unlock new statistical guarantees that were previously out of reach:

1. We show that MDPs with low _coverability_[63]--a general structural condition that subsumes Block MDPs and Low-Rank MDPs--can be learned in a sample-efficient fashion with _only \(Q^{*}\)-realizability_ (realizability of the optimal state-value function); existing online RL algorithms require significantly stronger representation conditions.
2. As a consequence, we show that the notorious _Exogenous Block MDP_ problem [22] is tractable under local simulator access. The results above are achieved through a computationally-inefficient algorithm. We complement them with a more computationally efficient algorithm, RVFS (_Recursive Value Function Search_), which achieves provable sample complexity guarantees under strengthened statistical assumption known as _pushforward coverability_. RVFS can be viewed as a principled, provable counterpart to a successful empirical paradigm that combines recursive search (e.g., MCTS) with value function approximation.

## 1 Introduction

Simulators are a widely used tool in reinforcement learning. Many of the most well-known benchmarks for reinforcement learning research make use of simulators (Atari [9], MuJoCo [55], OpenAI Gym [11], DeepMind Control Suite [53]), and high-quality simulators are available for a wide range of real-world control tasks, including robotic control [45, 2], autonomous vehicles [10, 6], and game playing [51, 52]. Simulators also provide a useful abstraction for _planning_ with a known or learned model, an important building block for many RL techniques [48]. Yet, in spite of the ubiquity of simulators, almost all existing research into algorithm design--empirical and theoretical--has focused on the _online reinforcement learning_ (where only trajectory-based feedback is available), and does not take advantage of the extra information available through the simulator. Relatively little is known about the full power of RL with simulator access, either in terms of algorithmic principles or fundamental limits.

We explore the power of simulators through online reinforcement learning with _local simulator access_ (RLLS for short), also known as _local planning_[57, 40, 3, 59, 65, 66]. Here, the agent learns by repeatedly executing policies and observing the resulting trajectories (as in online RL), but is allowed to _reset_ to previously observed states and follow their dynamics during training.

Empirically, algorithms based on local simulators have received limited investigation, but with promising results. Notably, the Go-Explore algorithm [19, 20] uses local simulator access to achieve state-of-the-art performance for Montezuma's Revenge (a difficult Atari game that requires systematic

[MISSING_PAGE_FAIL:2]

\(\Delta(\mathcal{A})\}_{h=1}^{h}\); we use \(\Pi_{5}\) to denote the set of all such functions. When a policy is executed, it generates a trajectory \((\mathbf{x}_{1},\mathbf{a}_{1},\mathbf{r}_{1}),\ldots,(\mathbf{x}_{H},\mathbf{a}_{H},\mathbf{r}_{h})\) via the process \(\mathbf{a}_{h}\sim\pi_{h}(\mathbf{x}_{h}),\mathbf{r}_{h}\sim R_{h}(\mathbf{x}_{h},\mathbf{a}_{h}), \mathbf{x}_{h+1}\sim T_{h}(\cdot\mid\mathbf{x}_{h},\mathbf{a}_{h})\), initialized from \(\mathbf{x}_{1}\sim T_{0}(\cdot\mid\varnothing)\) (we use \(\mathbf{x}_{H+1}\) to denote a terminal state with zero reward). We write \(\mathbb{P}^{\pi}[\cdot]\) and \(\mathbb{E}^{\pi}[\cdot]\) to denote the law and expectation under this process.

For a policy \(\pi\), \(J(\pi)\coloneqq\mathbb{E}^{\pi}\big{[}\sum_{h=1}^{H}\mathbf{r}_{h}\big{]}\) denotes expected reward, and the value functions are given by \(V_{h}^{\pi}(x)=\mathbb{E}^{\pi}\big{[}\sum_{h^{\prime}=h}^{H}\mathbf{r}_{h^{ \prime}}\mid\mathbf{x}_{h}=x\big{]}\), and \(Q_{h}^{\pi}(x,a)\coloneqq\mathbb{E}^{\pi}\big{[}\sum_{h^{\prime}=h}^{H}\mathbf{r}_ {h^{\prime}}\mid\mathbf{x}_{h}=x,\mathbf{a}_{h}=a\big{]}\). We denote by \(\pi^{*}\) the optimal deterministic policy that maximizes \(Q^{\pi^{*}}\), and write \(Q^{*}\coloneqq Q^{\pi^{*}}\) and \(V^{*}\coloneqq V^{\pi^{*}}\).

**Online reinforcement learning with a local simulator.** In the standard online reinforcement learning framework, the learner repeatedly interacts with an (unknown) MDP by executing a policy and observing the resulting trajectory, with the goal of maximizing the total reward. Formally, for each episode \(\tau\in[N_{\mathsf{episodes}}]\), the learner selects a policy \(\pi^{(*)}=\{\pi^{(*)}_{h}\}_{h=1}^{H}\), executes it in the underlying MDP \(\mathcal{M}^{*}\) and observes the trajectory \(\{(\mathbf{x}_{h}^{(\tau)},\mathbf{a}_{h}^{(\tau)},\mathbf{r}_{h}^{(\tau)})\}_{h=1}^{H}\). After all \(N_{\mathsf{episodes}}\) episodes conclude, the learner produces a policy \(\vec{\pi}\in\Pi_{5}\) with the goal of minimizing the risk given by \(\mathbb{E}[J(\pi^{*})-J(\vec{\pi})]\).

In online RL with local simulator access, or RLLS, [57; 40; 65; 59; 66], we augment the online RL protocol as follows: At each episode \(\tau\in[N]\), instead of starting from a random initial state \(\mathbf{x}_{1}\sim T_{0}(\cdot\mid\varnothing)\), the agent can _reset_ the MDP to any layer \(h\in[H]\) and any state \(\mathbf{x}_{h}\) previously encountered, and proceed with a new episode starting from \(\mathbf{x}_{h}\). As in the online RL protocol, the goal is to produce a policy \(\vec{\pi}\in\Pi_{5}\) such that \(\mathbb{E}[J(\pi^{*})-J(\vec{\pi})]\leq\varepsilon\) with as few episodes of interaction as possible; our main results take \(N_{\mathsf{episodes}}=\operatorname{poly}(C,\varepsilon^{-1})\) for a suitable problem parameter \(C\).

**Executable versus non-executable policies.** We focus on learning policies that can be executed without access to a local simulator (in other words, the local simulator used at train time, but not test time). Some recent work using local simulators for RL with linear function approximation [57] considers a more permissive setting where the final policy \(\pi\) produced by the learner can be _non-executable_; our function approximation requirements can be slightly relaxed in this case.

**Definition 2.1** (Non-executable policy).: _We refer to a policy \(\pi\) for which computing \(\pi(x)\in\Delta(\mathcal{A})\) for any \(x\in\mathcal{X}\) requires \(n\) local simulator queries as a non-executable policy with sample complexity \(n\)._

Additional notation.For any \(m,n\in\mathbb{N}\), we denote by \([m\,.\,n]\) the integer interval \(\{m,\ldots,n\}\). We also let \([n]\coloneqq[1\,.\,n]\). We refer to a scalar \(c>0\) as an _absolute constant_ to indicate that it is independent of all problem parameters and use \(\widetilde{Q}(\cdot)\) to denote a bound up to factors poly-logarithmic in parameters appearing in the expression. We define \(\pi_{\mathsf{unif}}\in\Pi_{5}\) as the random policy that selects actions in \(\mathcal{A}\) uniformly. We define the occupancy measure for policy \(\pi\) via \(d_{h}^{\pi}(x,a)\coloneqq\mathbb{P}^{\pi}[\mathbf{x}_{h}=x,\mathbf{a}_{h}=a]\). For functions \(g:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\) and \(f:\mathcal{X}\to\mathbb{R}\), we define Bellman backup operators by \(\mathcal{T}_{h}[g](x,a)=\mathbb{E}[\mathbf{r}_{h}+\max_{a^{\prime}\in\mathcal{A}} g(\mathbf{x}_{h+1},a^{\prime})\mid\mathbf{x}_{h}=x,\mathbf{a}_{h}=a]\) and \(\mathcal{P}_{h}[f](x,a)=\mathbb{E}[\mathbf{r}_{h}+f(\mathbf{x}_{h+1})\mid\mathbf{x}_{h}= x,\mathbf{a}_{h}=a]\). For a stochastic policy \(\pi\in\Pi_{5}\), we will occasionally use the bold notation \(\mathbf{\pi}_{h}(x)\) as shorthand for the random variable \(\mathbf{a}_{h}\sim\pi_{h}(x)\in\Delta(\mathcal{A})\). For a function \(f:\mathcal{A}\to\mathbb{R}\), we write \(a^{\prime}\in\operatorname*{arg\,max}_{a\in\mathcal{A}}f(a)\) to denote the action that maximizes \(f\). If there are ties, we break them by picking the action with the smallest index; we assume without loss of generality that actions in \(\mathcal{A}\) are index from \(1,\ldots,|\mathcal{A}|\).

## 3 New Sample-Efficient Learning Guarantees via Local Simulators

This section presents our most powerful results for RLLS. We present a new algorithm for learning with local simulator access, SimGolf (Section 3.1), and show that it enables sample-efficient RL for MDPs with low _coverability_[63] using only \(Q^{*}\)-realizability (Section 3.2). We then give implications for the Exogenous Block MDP problem (Section 3.3).

**Function approximation setup and coverability.** To achieve sample complexity guarantees for online reinforcement learning that are suitable for large, high-dimensional state spaces, we appeal to _value function approximation_. We assume access to a function class \(\mathcal{Q}\subset(\mathcal{X}\times\mathcal{A}\times[H]\to[0,H])\) that contains the optimal state-action value function \(Q^{*}\); we define \(\mathcal{Q}_{h}=\{Q_{h}\mid Q\in\dot{\mathcal{Q}}\}\).

**Assumption 3.1** (\(Q^{*}\)-realizability).: _For all \(h\in[H]\), we have \(Q^{*}_{h}\in\mathcal{Q}_{h}\)._

\(Q^{*}\)-realizability is widely viewed as a minimal representation condition for online RL [61; 17; 16; 39; 58; 56]. The class \(\mathcal{Q}\) encodes the learner's prior knowledge about the MDP, and can be parameterized by rich function approximators like neural networks. We assume for simplicity of exposition that and \(\Pi\) are finite, and aim for sample complexity guarantees scaling with \(\log[\mathcal{Q}]\) and \(\log[\Pi]\); extending our results to infinite classes via standard uniform convergence arguments is straightforward.

**Coverability.** Beyond representation conditions like realizability, online RL algorithms require _structural conditions_ that limit the extent to which deliberately designed algorithms can be surprised by substantially new state distributions. We focus on a structural condition known as _coverability_[63], which is inspired by connections between online and offline RL.

**Assumption 3.2**.: _The coverability coefficient is \(C_{\text{cov}}\coloneqq\max_{h\in[H]}\inf_{\mu_{h}\in\Delta(\mathcal{X}\times \mathcal{A})}\sup_{\pi\in\Pi_{5}}\left\|\frac{d_{h}^{\ast}}{\mu_{h}}\right\|_{ \infty}\)._

Coverability is an intrinsic strutural property of the underlying MDP. Examples of MDP families with low coverability include (Exogenous) Block MDPs, which have \(C_{\text{cov}}\leq|\mathcal{S}||\mathcal{A}|\), where \(\mathcal{S}\) is the _latent state space_[63], and Low-Rank MDPs, which have \(C_{\text{cov}}\leq d|\mathcal{A}|\), where \(d\) is the feature dimension [29]; importantly, these settings exhibit high-dimensional state spaces and require nonlinear function approximation. As in prior work [63, 4], our algorithms require _no prior knowledge_ of the distribution \(\mu_{h}\) that achieves the minimum in Assumption 3.2.

### Algorithm

Our main algorithm, SimGolf, is displayed in Algorithm 1. The algorithm is a variant of the GOLF method of Jin et al. [32], Xie et al. [63] with novel adaptations to exploit the availability of a local simulator. Like GOLF, SimGolf explores using the principle of _global optimisim_: At each iteration \(t\in[N]\), it maintains a confidence set (or, version space) \(\mathcal{Q}^{(\epsilon)}\subset\mathcal{Q}\) of candidate value functions with low squared Bellman error under the data collected so far, and chooses a new exploration policy \(\pi^{(\epsilon)}\) by picking the most "optimistic" value function in this set. As the algorithm gathers more data, the confidence set shrinks, leaving only near-optimal policies.

The main novelty in SimGolf arises in the data collection strategy and design of confidence sets. Like GOLF, SimGolf algorithm constructs the confidence set \(\mathcal{Q}^{(\epsilon)}\subset\mathcal{Q}\) such that all value functions \(g\in\mathcal{Q}^{(\epsilon)}\) have small squared Bellman error:

\[\sum_{i<t}\mathbb{E}^{\pi^{(\epsilon)}}\Big{[}\big{(}g_{h}(\mathbf{x}_{h},\mathbf{a}_{ h})-\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h},\mathbf{a}_{h})\big{)}^{2}\Big{]}\lesssim\log| \mathcal{Q}|,\quad\forall h\in[H]. \tag{1}\]

Due to the presence of the Bellman backup \(\mathcal{T}_{h}[g_{h+1}]\) in Eq. (1), naively estimating squared Bellman error leads to the notorious _double sampling_ problem. To avoid this, the approach taken with GOLF and related work [67, 32] is to adapt a certain de-biasing technique to remove double sampling bias, but this requires access to a value function class that satisfies _Bellman completeness_, a representation significantly more restrictive than realizability (e.g., Foster et al. [26]).

The idea behind SimGolf is to use local simulator access to directly produce high-quality estimates for the Bellman backup function \(\mathcal{T}_{h}[g_{h+1}]\) in Eq. (1). In particular, for a given state-action pair \((x,a)\in\mathcal{X}\times\mathcal{A}\), we can estimate the Bellman backup \(\mathcal{T}_{h}[g_{h+1}](x,a)\) for all functions \(g\in\mathcal{Q}\) simultaneously by collecting \(K\) next-state transitions \(\widetilde{\mathbf{x}}_{h+1}^{(1)},\ldots,\widetilde{\mathbf{x}}_{h+1}^{(K)}\stackrel{ {\text{iid}}}{{\sim}}T_{h}(\cdot\mid x,a)\) and \(K\) rewards \(\widetilde{\mathbf{r}}_{h}^{(1)},\ldots,\widetilde{\mathbf{r}}_{h}^{(K)}\stackrel{{ \text{iid}}}{{\sim}}R_{h}(x,a)\), then taking the empirical mean: \(\mathcal{T}_{h}[g_{h+1}](x,a)\approx\frac{1}{K}\sum_{k=1}^{K}\big{(}\widetilde {\mathbf{r}}_{h}^{(k)}+\max_{a^{\prime}\in\mathcal{A}}g_{h+1}(\widetilde{\mathbf{x}}_{ h+1}^{(k)},a^{\prime})\big{)}\). Line 8 of SimGolf uses this technique to directly estimate the Bellman residual backup under a trajectory gathered with \(\pi^{(\epsilon)}\), sidestepping the double sampling problem and removing the need for Bellman completeness. We suspect this technique (estimation with respect to squared Bellman error using local simulator access) may find broader use.

### Main Result

We now state the main guarantee for SimGolf and discuss some of its implications.

**Theorem 3.1** (Main guarantee for SimGolf).: _Let \(\varepsilon,\delta\in(0,1)\) be given and suppose Assumption 3.1 (\(Q^{\ast}\)-realizability) and Assumption 3.2 (coverability) hold with \(C_{\text{cov}}>0\). Then the policy \(\widetilde{\pi}\) produced by \(\texttt{SimGolf}(\mathcal{Q},C_{\text{cov}},\varepsilon,\delta)\) (Algorithm 1) has \(J(\pi^{\ast})-\mathbb{E}[J(\widetilde{\pi})]\leq\varepsilon\) with probability at least \(1-\delta\). The total sample complexity in the RLLS framework is bounded by \(\widetilde{\mathcal{O}}\big{(}H^{5}C_{\text{cov}}^{2}\log(|\mathcal{Q}|/ \delta)\cdot\varepsilon^{-4}\big{)}\)._

This result (whose proof is in Appendix E) shows that under only \(Q^{\ast}\)-realizability and coverability, SimGolf learns an \(\varepsilon\)-optimal policy with polynomial sample complexity, significantly relaxing the representation assumptions (Bellman completeness, weight function realizability) required by prior algorithms for coverability [63, 4]. This is the first instance we are aware of where local simulator access unlocks sample complexity guarantees for reinforcement learning with _nonlinear_ function approximation that were previously out of reach; perhaps the most important technical idea here is our approach to combining global optimism with local simulator access, in contrast to greedy layer-by-layer schemes used in prior work on local simulators (with the exception of Weisz et al. [57]). In particular, we suspect that the idea of performing estimation with respect to squared Bellman error directly using local simulator access may find broader use beyond coverability. Improving the polynomial dependence on problem parameters is an interesting question for future work.

**A conjecture.** By analogy to results in offline reinforcement learning, where \(Q^{*}\)-realizability and concentrability (the offline counterpart to coverability) alone are known to be insufficient for sample-efficient learning [12, 26], we conjecture that \(Q^{*}\)-realizability and coverability alone are not sufficient for polynomial sample complexity in vanilla online RL. If true, this would imply a new separation between online RL with and without local simulators.

### Implications for Exogenous Block MDPs

We now apply SimGolf and Theorem 3.1 to the _Exogenous Block MDP_ (ExBMDP) problem [22, 21, 38, 30], a challenging rich-observation reinforcement learning setting in which the observed states \(\mathbf{x}_{h}\) are high-dimensional, while the underlying dynamics of the system are low-dimensional, yet confounded by temporally correlated exogenous noise.

Formally, an Exogenous Block MDP \(\mathcal{M}=(\mathcal{X},\mathcal{S},\Xi,\mathcal{A},H,T,R,g)\) is defined by a _latent state space_ and an _observation space_. We begin with the latent state space. Starting from an initial _endogenous state_\(\mathbf{s}_{1}\in\mathcal{S}\) and _exogenous state_\(\mathbf{\xi}_{1}\in\Xi\), the latent state \(\mathbf{z}_{h}=(\mathbf{s}_{h},\mathbf{\xi}_{h})\) evolves for \(h\in[H]\) via \(\mathbf{s}_{h+1}\sim T_{h}^{\text{endo}}(\cdot\mid\mathbf{s}_{h},\mathbf{a}_{h})\) and \(\mathbf{\xi}_{h+1}\sim T_{h}^{\text{exo}}(\cdot\mid\xi_{h})\), where \(\mathbf{a}_{h}\in\mathcal{A}\) is the agent's action at layer \(h\); we adopt the convention that \(\mathbf{s}_{1}\sim T_{0}^{\text{endo}}(\cdot\mid\varnothing)\) and \(\mathbf{\xi}_{1}\sim T_{0}^{\text{exo}}(\cdot\mid\varnothing)\). Note that only the endogenous state is causally influenced by the action. The latent state is not observed; instead, at each step \(h\), the agent receives an _observation_\(\mathbf{x}_{h}\in\mathcal{X}\) generated via1\(\mathbf{x}_{h}=g_{h}^{\text{obs}}(\mathbf{s}_{h},\mathbf{\xi}_{h})\), where \(g_{h}^{\text{obs}}:\mathcal{S}\times\Xi\rightarrow\mathcal{X}\) is the _emission function_. We assume the endogenous latent space \(\mathcal{S}\) and action space \(\mathcal{A}\) are finite, and define \(S\coloneqq|\mathcal{S}|\) and \(A\coloneqq|\mathcal{A}|\). However, the exogenous state space \(\Xi\) and observation space \(\mathcal{X}\) may be arbitrarily large or infinite, with \(|\Xi|,|\mathcal{X}|\gg|\mathcal{S}|\).2

Footnote 1: A more standard formulation [22, 21, 38, 30] assumes that observations are generated via \(\mathbf{x}_{h}\sim q_{h}(\mathbf{s}_{h},\mathbf{\xi}_{h})\), where \(q_{h}(\cdot,\cdot)\) is a conditional distribution with the decodability property. This is equivalent to \(\mathbf{x}_{h}=g_{h}^{\text{obs}}(\mathbf{s}_{h},\mathbf{\xi}_{h})\), as randomness in the emission process can be included in the exogenous state w.l.o.g.

Footnote 2: To simplify presentation, we assume that \(\Xi\) and \(\mathcal{X}\) are countable; our results trivially extend to the case where the corresponding variables are continuous with an appropriate measure-theoretic treatment.

The final property of the ExBMDP model is _decodability_, which asserts the existence of a _decoder_ such that \(\phi_{\star}\colon\mathcal{X}\rightarrow\mathcal{S}\) such that \(\phi_{\star}(\mathbf{x}_{h})=\mathbf{s}_{h}\) a.s. for all \(h\in[H]\) with \(\mathbf{x}_{h}=g_{h}^{\text{obs}}(\mathbf{s}_{h},\mathbf{\xi}_{h})\),, Informally, decodability ensures the existence of an (unknown to the learner) mapping that allows one to perfectly recover the endogenous latent state from observations. In addition to decodability, we assume the rewards in the ExBMDP are _endogenous_; that is, the reward distribution \(R_{h}(\mathbf{x}_{h},\mathbf{a}_{h})\) only depends on the observations \((\mathbf{x}_{h})\) through the corresponding latent states \((\phi^{*}(\mathbf{x}_{h})=\mathbf{s}_{h})\). To enable sample-efficient learning, we assume access to a _decoder class_\(\Phi\) that contains \(\phi^{*}\), as in prior work.

**Assumption 3.3** (Decoder realizability).: _We have access to a decoder class \(\Phi\) such that \(\phi^{*}\in\Phi\)._

**Applying SimGolf and Theorem 3.1**.: To apply Theorem 3.1 to the ExBMDP problem, we need to verify that \(Q^{*}\)-realizability and coverability hold. Realizability is a straightforward consequence of decodability (Lemma D.1 in Part II of the appendix). For coverability, Xie et al. [63] show that ExBMDPs have \(C_{\text{cov}}\leq SA\) under decodability, in spite of the time-correlated exogenous noise process \((\mathbf{\xi}_{h})\) and potentially infinite observation space \(\mathcal{X}\) (interestingly, coverability is essentially the only useful structural property that ExBMDPs are known to satisfy, which is our primary motivation for studying it). This leads to the following corollary of Theorem 3.1.

**Corollary 3.1** (SimGolf for ExBMDPs).: _Consider the ExBMDP setting. Suppose that Assumption 3.3 holds, and let \(\mathcal{Q}\) be constructed as in Lemma D.1 of Part II. Then for any \(\varepsilon,\delta\in(0,1)\), the policy \(\widehat{\pi}=\texttt{SimGolf}(\mathcal{Q},SA,\varepsilon,\delta)\) has \(J(\pi^{*})-J(\widehat{\pi})\leq\varepsilon\) with probability at least \(1-\delta\). The total sample complexity in the RLLS framework is \(N=\widetilde{O}\big{(}H^{5}S^{3}A^{3}\log[\Phi]\cdot\varepsilon^{-4}\big{)}\)._

This shows for the first time that general ExBMDPs are learnable with local simulator access. Prior to this work, online RL algorithms for ExBMDPs required either (i) deterministic latent dynamics [22], or (ii) factored emission structure [21]. Xie et al. [63] observed that ExBMDPs admit low coverability, but their algorithm requires Bellman completeness, which is not satisfied by ExBMDPs (see Islam et al. [30]). See Appendix A for more discussion.

## 4 Computationally Efficient Learning with Local Simulators

Our result in Section 3 show that local simulator access facilitates sample-efficient learning in MDPs with low coverability, a challenging setting that was previously out of reach. However, our algorithm SimGolf is computationally-inefficient because it relies on global optimism, a drawback found in most prior work on RL with general function approximation [31, 32, 18]. It remains an open question whether any form of global optimism can be implemented efficiently, and some variants have provable barriers to efficient implementation [14].

To address this drawback, in this section we present a new algorithm, RVFS (Recursive Value Function Search; Algorithm 5), which requires stronger versions of the coverability and realizability assumptions in Section 3, but is computationally efficient in the sense that it reduces to convex optimization over the state-value function class \(\mathcal{V}\). RVFS makes use of a sophisticated recursive exploration scheme based on core-sets, sidestepping the need for global optimism.

### Function Approximation and Statistical Assumptions

To begin, we require the following strengthening of the coverability assumption in Assumption 3.2.

**Assumption 4.1** (Pushforward coverability).: _The pushforward coverability coefficient \(C_{\text{push}}>0\) is given by \(C_{\text{push}}=\max_{h\in[H]}\inf_{\mu_{h}\in\Delta(\mathcal{X})}\sup_{(x_ {h-1},\sigma_{h-1},x_{h})\in\mathcal{X}_{h-1}\times\mathcal{A}_{h}\times \mathcal{X}}\frac{T_{h-1}(x_{h}|x_{h-1},\sigma_{h-1})}{\mu_{h}(x_{h})}\)._

Pushforward coverability is inspired by the _pushforward concentrability_ condition used in offline RL by [62, 26]. Concrete examples include, (i) Block MDPs with latent space \(\mathcal{S}\), which admit \(C_{\text{push}}\leq|\mathcal{S}|\), (ii) Low-Rank MDPs in dimension \(d\), which admit \(C_{\text{push}}\leq d\)[62], and (iii) Exogenous Block MDPs for which the exogenous noise process satisfies a _weak correlation condition_ that we introduce in Appendix B. Note that \(C_{\text{cov}}\leq C_{\text{push}}|\mathcal{A}|\), but the converse is not true in general.

Instead of state-action value function approximation as in SimGolf, in this section we make use of a state value function class \(\mathcal{V}\subset(\mathcal{X}\times[H]\rightarrow[0,H])\), but require somewhat stronger representation conditions than in Section 3. We consider two complementary setups:

* **Setup I:** Assumptions 4.2 and 4.3 (\(V^{*}/\pi^{*}\)-realizability) and Assumption 4.4 (\(\Delta\)-gap) hold.
* **Setup II:** Assumption 4.5 (\(V^{\pi}\)-realizability) and Assumption 4.6 (\(\pi\)-realizability) hold.

We describe these assumptions in more detail below.

**Function approximation setup I.** First, instead of \(Q^{*}\)-realizability, we consider the weaker \(V^{*}\)-realizability [31, 57, 3].

**Assumption 4.2** (\(V^{*}\)-realizability).: _For all \(h\in[H]\), we have \(V^{*}_{h}\in\mathcal{V}_{h}\)._Under \(V^{*}\)-realizability, our algorithm learns a near-optimal policy, but the policy is _non-executable_ (cf. Definition 2.1); this property is shared by prior work on local simulator access with value function realizability [57]. To produce executable policies, we additionally require access to a policy class \(\Pi\subset\Pi_{5}\) containing \(\pi^{*}\); we define \(\Pi_{h}=\{\pi_{h}\mid\pi\in\Pi\}\).

**Assumption 4.3** (\(\pi^{*}\)-realizability).: _The policy class \(\Pi\) contains the optimal policy \(\pi^{*}\)._

\(V^{*}\)-realizability (Assumption 4.2) and \(\pi^{*}\)-realizability (Assumption 4.3) are both implied by \(Q^{*}\)-realizability, and hence are weaker. However, we also assume the optimal \(Q\)-function admits constant gap (this makes the representation conditions for **Setup I** incomparable to Assumption 3.1).

**Assumption 4.4** (\(\Delta\)-Gap).: _The optimal action \(\pi_{h}^{*}(x)\) is unique, and there exists \(\Delta>0\) such that for all \(h\in[H]\), \(x\in\mathcal{X}\), and \(a\in\mathcal{A}\setminus\{\pi_{h}^{*}(x)\}\), \(Q_{h}^{*}(x,\pi_{h}^{*}(x))>Q_{h}^{*}(x,a)+\Delta\)._

This condition has been used in a many prior works on computationally efficient RL with function approximation [16, 17, 24, 56].

Function approximation setup II.We also provide guarantees under the assumption that the class \(\mathcal{V}\) satisfies _all-policy realizability_[59, 65, 60] in the sense that \(V^{\pi}\in\mathcal{V}\) for all \(\pi\in\Pi_{5}\).

**Assumption 4.5** (\(V^{\pi}\)-realizability).: _The class \(\mathcal{V}=\mathcal{V}_{1:H}\) has \(V_{h}^{\pi}\in\mathcal{V}_{h}\) for all \(\pi\in\Pi_{5}\) and \(h\in[H]\)._

This assumption will be sufficient to learn a non-executable policy, but to learn executable policies we require an analogous strengthening of Assumption 4.5.

**Assumption 4.6** (\(\pi\)-realizability).: _For all \(\pi\in\Pi_{5}\), we have that \(x\mapsto\arg\max_{a\in\mathcal{A}}\mathcal{P}_{h}[V_{h+1}^{\pi}](\cdot,a)\in\Pi\)._

This assumption has been used by a number of prior works on computationally efficient RL [8, 44]. Assumptions 4.5 and 4.6 are both implied by the slightly simpler-to-state assumption of \(Q^{\pi}\)-_realizability_[59, 65, 60], which asserts access to a class \(\mathcal{Q}\) that contains \(Q^{\pi}\) for all \(\pi\in\Pi_{5}\).

### Algorithm

For ease of exposition, we defer the full version of our algorithm, RVFS (Algorithm 5), to Appendix F and present a simplified version here (Algorithm 2). The algorithms are nearly identical, except that the simplified version assumes that certain quantities of interest (e.g., Bellman backups) can be computed exactly, while the full version (provably) approximates them from samples.

RVFS maintains a value function estimator \(\widehat{V}=\widehat{V}_{1:H}\) that aims to approximate the optimal value function \(V_{1:H}^{\star}\), as well as _core sets_\(\mathcal{C}_{1},\ldots,\mathcal{C}_{H}\) of state-action pairs that are used to perform estimation and guide exploration. At a high level, RVFS alternates between (i) fitting the value function \(\widehat{V}_{h}\) for a given layer \(h\in[H]\) based on Monte-Carlo rollouts, and (ii) using the core-sets to test whether the current value function estimates \(\widehat{V}_{h+1:H}\) remain accurate as the roll-in policy induced by \(\widehat{V}_{h}\) changes.

In more detail, RVFS is based on recursion across the layers \(h\in[H]\). When invoked for layer \(h\) with value function estimates \(\widehat{V}_{h+1:H}\) and core-sets \(\mathcal{C}_{h},\ldots,\mathcal{C}_{H},\)\(\mathsf{RVFS}_{h}\) performs two steps:

1. For each state-action pair \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\),4 the algorithm gathers \(N_{\text{test}}\) trajectories by rolling out from \((x_{h-1},a_{h-1})\) with the greedy policy \(\widehat{\pi}_{\ell}(x)\in\arg\max_{a\in\mathcal{A}}\mathcal{P}_{\ell}[ \widehat{V}_{\ell+1}](x,a)\) that optimizes the estimated value function; in the full version of RVFS (see Algorithm 5), we estimate the bellman backup \(\mathcal{P}_{\ell}[\widehat{V}_{\ell+1}](x,a)\) using the local simulator. For all states \(x_{\ell-1}\in\{\mathbf{x}_{h},\ldots,\mathbf{x}_{H-1}\}\) encountered during this process, the algorithm checks whether \(\left|\mathbb{E}[\widehat{V}_{\ell}(\mathbf{x}_{\ell})-V_{\ell}^{\star}(\mathbf{x}_{ \ell})\mid\mathbf{x}_{\ell-1}=x_{\ell-1},\mathbf{a}_{\ell-1}=a_{\ell-1}]\right|\leq\varepsilon\) for all \(a_{\ell-1}\in\mathcal{A}\) using a test based on (implicitly maintained) confidence sets. If the test fails, this indicates that distribution shift has occurred, and the algorithm adds the pair \((x_{\ell-1},a_{\ell-1})\) to \(\mathcal{C}_{\ell}\) and recurses on layer \(\ell\) via \(\mathsf{RVFS}_{\ell}\). Footnote 4: Informally, \(\mathcal{C}_{h}\) represents a collection of state-action pairs \((x_{h-1},a_{h-1})\) at layer \(h-1\) for which we want \(\mathbb{E}[\left|\widehat{V}_{h}(\mathbf{x}_{h})-V_{h}^{\star}(\mathbf{x}_{h})\right| \mid\mathbf{x}_{h-1}=x_{h-1},\mathbf{a}_{h-1}=a_{h-1}]\leq\varepsilon\) for some small \(\varepsilon>0\).
2. If all tests above pass, this means that \(\widehat{V}_{h+1},\ldots,\widehat{V}_{H}\) are accurate, and no distribution shift has occurred. In this case, the algorithm fits \(\widehat{V}_{h}\) by collecting Monte-Carlo rollouts from all state-action pairs in the core-set \(\mathcal{C}_{h}\) with \(\widehat{\pi}_{\ell}(x)\in\arg\max_{a\in\mathcal{A}}\mathcal{P}_{\ell}[ \widehat{V}_{\ell+1}](x,a)\) (cf. Line 16), and returns.

When the tests in Item 1 succeed for all \(h\in[H]\), the algorithm returns the estimated value functions \(\widehat{V}_{1:H}\); in this case, the greedy policy \(\widehat{\pi}_{\ell}(x)\in\arg\max_{a\in\mathcal{A}}\mathcal{P}_{\ell}[ \widehat{V}_{\ell+1}](x,a)\) is guaranteed to be near optimal. The full version of RVFS in Algorithm 5 uses local simulator access to estimate the Bellman backups \(\mathcal{P}_{h}[\widehat{V}_{h+1}](x,a)\) for different state-action pairs \((x,a)\). These backups are used to (i) compute actions of the greedy policy that maximizes \(\widehat{V}_{1:H}\) via (e.g., Eq. (2)); (ii) generate trajectories by rolling out from state-action pairs in the core-sets (Line 6); and (iii) perform the test in Item 1 (Line 8).

RVFS is inspired by the DMQ algorithm [16, 56] originally introduced in the context of online reinforcement learning with linearly realizable \(Q^{*}\). RVFS incorporates local simulator access (most critically, via core-set construction) to allow for more general _nonlinear_ function approximation without restrictive statistical assumptions. Prior algorithms for RLLS have used core-sets of state-action pairs in a similar fashion [40, 65, 59], but in a way that is tailored to linear function approximation.

In what follows, we discuss various features of the algorithm in greater detail.

Bellman backup policies.Since RVFS works with state value functions instead of state-action value functions, we need a way to extract policies from the former. The most natural way to extract a policy from estimated value functions \(\widehat{V}_{1:H}\in\mathcal{V}\) is as follows: for all \(h\in[H]\), define \(\widehat{\pi}_{h}(x)\in\operatorname*{arg\,max}_{a\in\mathcal{A}}\mathcal{P}_ {h}[\widehat{V}_{h+1}](x,a)\). In reality, we do not have access to \(\mathcal{P}_{h}[\widehat{V}_{h+1}](x,a)\) directly, so the full version of RVFS (Algorithm 5) estimates this quantity on the fly using the local simulator using the following scheme (Algorithm 7 in Appendix F): Given a state \(x\), for each \(a\), we sample \(K\) rewards \(\mathbf{r}_{h}\sim R_{h}(x,a)\) and next-state transitions \(\mathbf{x}_{h+1}\sim T_{h}(\cdot\mid x,a)\), then approximate \(\mathcal{P}_{h}[\widehat{V}_{h+1}](x,a)\) by the empirical mean. We remark that the use of these Bellman backup policies is actually crucial in the analysis for RVFS; even if we were to work with estimated state-action value functions \(\widehat{Q}_{1:H}\) instead, our analysis would require executing the Bellman backup policies \(\widehat{\pi}_{h}(x)\in\operatorname*{arg\,max}_{a\in\mathcal{A}}\mathcal{T}_ {h}[\widehat{Q}_{h+1}](x,a)\) (instead of naively using \(\widehat{\pi}_{h}(x)\in\operatorname*{arg\,max}_{a\in\mathcal{A}}\widetilde{Q }_{h}(x,a)\)).

**Invoking the algorithm.** The base invocation of RVFS takes the form

\[\widetilde{V}_{1:H}\leftarrow\mathsf{RVFS}_{0}(\widetilde{V}_{1:H}=\mathsf{arbitrary },\widetilde{V}_{1:H}=\{\mathcal{V}_{h}\}_{h=1}^{H},\mathcal{C}_{0:H}=\{ \varnothing\}_{h=0}^{H};;\mathcal{V},\varepsilon,\delta).\]

Whenever this call returns, the greedy policy induced by \(\widehat{V}_{1:H}\) is guaranteed to be near-optimal. Naively, the approximate Bellman backup policy induced by \(\widehat{V}_{1:H}\) (described above) is non-executable, and must be computed by invoking the local simulator. To provide an end-to-end guarantee to learn an executable policy, we give an outer-level algorithm, RVFS.bc (Algorithm 6, deferred to Appendix F for space), which invokes RVFS\({}_{0}\), then extracts an executable policy from \(\widetilde{V}_{1:H}\) using behavior cloning. Subsequent recursive calls to RVFS take the form \(\ \left(\widehat{V}_{h:H},\widehat{\mathcal{V}}_{h:H},\mathcal{C}_{h:H} \right)\leftarrow\mathsf{RVFS}_{h}(\widehat{V}_{h+1:H},\widehat{V}_{h+1:H}, \mathcal{C}_{h:H};\mathcal{V},\varepsilon,\delta).\) The arguments here are: Importantly, the confidence sets \(\widehat{\mathcal{V}}_{h+1:H}\) do not need to be explicitly maintained, and can be used implicitly whenever a _regression oracle_ for the value function class is available (discussed below).

**Remark 4.1** (Oracle-efficiency).: RVFS _is computationally efficient in the sense that it reduces to convex optimization over the value function class \(\mathcal{V}\). In particular, the only computationally intensive steps in the algorithm are (i) the regression step in Line 16, and (ii) the test in Line 8 involving the confidence set \(\widetilde{\mathcal{V}}_{\ell}\). For the latter, we do not explicitly need to maintain \(\widehat{\mathcal{V}}_{\ell}\), as the optimization problem over this set in Line 8 (for the full version of RVFS in Algorithm 5) reduces to solving \(\arg\max_{V\in\mathcal{V}}\{\pm\sum_{i=1}^{n}V\left(\widetilde{x}^{(i)}\ \right)\ |\ \sum_{i=1}^{n}(V(x^{(i)})-y^{(i)})^{2}\leq\beta^{2}\}\) for a dataset \(\{(x^{(i)},\widetilde{x}^{(i)},y^{(i)})\}_{i=1}^{n}\). This is convex optimization problem in function space, and in particular can be implemented in a provably efficient fashion whenever \(\mathcal{V}\) is linearly parameterized. We expect that the problem can also be reduced to a square loss regression by adapting the techniques in Krishnamurthy et al. [37], Foster et al. [23], but we do not pursue this here._

### Main Result

We present the main guarantee for RVFS under the function approximation assumptions in Section 4.1.

**Theorem 4.1** (Main guarantee for RVFS).: _Let \(\varepsilon,\delta\in(0,1)\) be given, and suppose that Assumption 4.1 (pushforward coverability) holds with \(C_{\mathrm{push}}>0\). Further, suppose that one the following holds:_

* **Setup I:** _Assumptions_ 4.2 _and_ 4.3 _(_\(V^{*}/\pi^{*}\)_-realizability) and_ Assumption_ 4.4 _(_\(\Delta\)_-gap) hold, and_ \(\varepsilon\leq 6H\cdot\Delta\)_._
* **Setup II:** _Assumption_ 4.5 _(_\(V^{*}\)_-realizability) and Assumption_ 4.6 _(_\(\pi\)_-realizability) hold._

_Then,_ RVFS.bc\((\Pi,\mathcal{V},\varepsilon,\delta)\) _(Algorithm 6) returns a policy \(\widehat{\pi}_{1:H}\) such that \(J(\pi^{*})-J(\widehat{\pi}_{1:H})\leq 2\varepsilon\) with probability at least \(1-\delta\), and has total sample complexity bounded by_

\[\widetilde{\mathcal{O}}\left(C_{\mathrm{push}}^{8}H^{23}A\cdot\varepsilon^{-1 3}\right).\]

_Furthermore, the algorithm makes at most \(\mathrm{poly}\!\left(C_{\mathrm{push}},H,A,\varepsilon^{-1}\right)\) calls to the convex optimization oracle over value function space described in Remark 4.1._

Theorem 4.1 shows for the first time that sample- and computationally-efficient RL with local simulator access is possible under pushforward coverability. In particular, RVFS is the first computationally efficient algorithm for RL with local simulator access that supports nonlinear function approximation. The assumptions in Theorem 4.1, while stronger than those in Section 3, are not known to enable sample-efficient RL without simulator access. Nonetheless, understanding whether RVFS can be strengthened to support general coverability or weaker function approximation is an important open problem. See Appendix H.1 for an overview of the analysis; we remark (Appendix I.1) that the result is actually proven under slightly weaker assumptions than those in **Setup I/Setup II**.

**Connection to empirical algorithms.** RVFS bears some similarity to Monte-Carlo Tree Search (MCTS) [13, 35] and AlphaZero [52], which perform planning with local simulator. Informally, MCTS can be viewed as a form of breadth-first search over the state space (where each node represents a state at a given layer), and AlphaZero is a particular instantiation of a MCTS that leverages \(V-\)value function approximation to allow for generalization across states. Compared to RVFS, MCTS and AlphaZero perform exploration via simple bandit-style heuristics, and are not explicitly designed to handle _distribution shifts_ that arise in settings where actions have long-term downstream effects. What is more, MCTS requires finite states to iterate over all possible child nodes of each state, making it inapplicable in environments with continuous states. RVFS may be viewed as a provable counterpartthat can handle continuous states and uses function approximation to address distribution shift in a principled fashion (in particular, through the use of confidence sets and the test in Line 14).5

Footnote 5: We note in passing that in the context of tree search, the pushforward coverability assumption (Assumption 4.1) may be viewed as the stochastic analogue of branching factor.

**Applying RVFS to Exogenous Block MDPs.** ExBMDPs satisfy coverability (Assumption 3.2), but do not satisfy the pushforward coverability assumption (Assumption 4.1) in general. However, it turns out that ExBMDPs _do_ satisfy pushforward coverability when the exogenous noise process is weakly correlated across time, a new statistical assumption we refer to the _weak correlation condition_. In Appendix B (Theorem B.1), we give a variant of RVFS for ExBMDPs that succeeds under (i) weak correlation, and (ii) decoder realizability, sidestepping the need for the \(\Delta\)-gap or \(V^{\pi}\)-realizability.

## 5 Discussion and Future Work

In this paper, we demonstrated that resets can substantially expand the range of reinforcement learning (RL) settings that are tractable, both statistically and computationally. Our practical algorithm, RVFS, provides a principled counterpart to MCTS by supporting continuous state spaces and offering provable guarantees, setting it apart from traditional MCTS. Statistically, our results extend to MDPs with a finite Sequential Estimation Coefficient (SEC) [63], capturing a broader class of MDPs beyond those with finite coverability--encompassing low Bellman Eluder MDPs [32] and MDPs with finite bilinear rank [18]. Although not formally developed here, it is possible to generalize push-forward coverability and our analysis to encompass a range of linear function approximation settings [57, 40, 3, 59, 65, 66], thereby recovering known positive results under local access in these settings.

While our focus has been on theoretical contributions--analyzing the sample and computational complexity of RL with access to a local simulator--this work also raises promising empirical questions. We are particularly interested in exploring these questions in future work, aiming to bridge these theoretical advances with empirical validation in practical RL settings.

## References

* Agarwal et al. [2014] A. Agarwal, D. Hsu, S. Kale, J. Langford, L. Li, and R. Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In _International Conference on Machine Learning_, pages 1638-1646, 2014.
* Akkaya et al. [2019] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, et al. Solving rubik's cube with a robot hand. _arXiv preprint arXiv:1910.07113_, 2019.
* Amortila et al. [2022] P. Amortila, N. Jiang, D. Madeka, and D. P. Foster. A few expert queries suffices for sample-efficient rl with resets and linear value approximation. _Advances in Neural Information Processing Systems_, 35:29637-29648, 2022.
* Amortila et al. [2024] P. Amortila, D. J. Foster, N. Jiang, A. Sekhari, and T. Xie. Harnessing density ratios for online reinforcement learning. _International Conference on Learning Representations (ICLR)_, 2024.
* Amortila et al. [2024] P. Amortila, D. J. Foster, and A. Krishnamurthy. Scalable online exploration via coverability. _arXiv preprint arXiv:2403.06571_, 2024.
* Aradi [2020] S. Aradi. Survey of deep reinforcement learning for motion planning of autonomous vehicles. _IEEE Transactions on Intelligent Transportation Systems_, 23(2):740-759, 2020.
* Badia et al. [2020] A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi, Z. D. Guo, and C. Blundell. Agent57: Outperforming the atari human benchmark. In _International conference on machine learning_, pages 507-517. PMLR, 2020.
* Bagnell et al. [2003] J. Bagnell, S. M. Kakade, J. Schneider, and A. Ng. Policy search by dynamic programming. _Advances in neural information processing systems_, 16, 2003.
* Bellemare et al. [2012] M. Bellemare, J. Veness, and M. Bowling. Investigating contingency awareness using atari 2600 games. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 26, pages 864-871, 2012.
* Bojarski et al. [2016] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, et al. End to end learning for self-driving cars. _arXiv preprint arXiv:1604.07316_, 2016.
* Brockman et al. [2016] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. _arXiv preprint arXiv:1606.01540_, 2016.
* Chen and Jiang [2019] J. Chen and N. Jiang. Information-theoretic considerations in batch reinforcement learning. In _International Conference on Machine Learning_, pages 1042-1051. PMLR, 2019.
* Coulom [2006] R. Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In _International conference on computers and games_, pages 72-83. Springer, 2006.
* Dann et al. [2018] C. Dann, N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. On oracle-efficient PAC RL with rich observations. In _Advances in neural information processing systems_, pages 1422-1432, 2018.
* Du et al. [2019] S. Du, A. Krishnamurthy, N. Jiang, A. Agarwal, M. Dudik, and J. Langford. Provably efficient RL with rich observations via latent state decoding. In _International Conference on Machine Learning_, pages 1665-1674. PMLR, 2019.
* Du et al. [2019] S. S. Du, Y. Luo, R. Wang, and H. Zhang. Provably efficient Q-learning with function approximation via distribution shift error checking oracle. In _Advances in Neural Information Processing Systems_, pages 8060-8070, 2019.
* Du et al. [2020] S. S. Du, S. M. Kakade, R. Wang, and L. F. Yang. Is a good representation sufficient for sample efficient reinforcement learning? In _International Conference on Learning Representations_, 2020.

* [18] S. S. Du, S. M. Kakade, J. D. Lee, S. Lovett, G. Mahajan, W. Sun, and R. Wang. Bilinear classes: A structural framework for provable generalization in RL. _International Conference on Machine Learning_, 2021.
* [19] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune. Go-explore: a new approach for hard-exploration problems. _arXiv preprint arXiv:1901.10995_, 2019.
* [20] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune. First return, then explore. _Nature_, 590(7847):580-586, 2021.
* [21] Y. Efroni, D. J. Foster, D. Misra, A. Krishnamurthy, and J. Langford. Sample-efficient reinforcement learning in the presence of exogenous information. In _Conference on Learning Theory_, pages 5062-5127. PMLR, 2022.
* [22] Y. Efroni, D. Misra, A. Krishnamurthy, A. Agarwal, and J. Langford. Provably filtering exogenous distractors using multistep inverse dynamics. In _International Conference on Learning Representations_, 2022.
* [23] D. J. Foster, A. Agarwal, M. Dudik, H. Luo, and R. E. Schapire. Practical contextual bandits with regression oracles. _International Conference on Machine Learning_, 2018.
* [24] D. J. Foster, A. Rakhlin, D. Simchi-Levi, and Y. Xu. Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective. _Conference on Learning Theory (COLT)_, 2020.
* [25] D. J. Foster, S. M. Kakade, J. Qian, and A. Rakhlin. The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_, 2021.
* [26] D. J. Foster, A. Krishnamurthy, D. Simchi-Levi, and Y. Xu. Offline reinforcement learning: Fundamental barriers for value function approximation. In _Conference on Learning Theory_, pages 3489-3489. PMLR, 2022.
* [27] Z. Guo, S. Thakoor, M. Pislar, B. Avila Pires, F. Altche, C. Tallec, A. Saade, D. Calandriello, J.-B. Grill, Y. Tang, et al. Byol-explore: Exploration by bootstrapped prediction. _Advances in neural information processing systems_, 35:31855-31870, 2022.
* [28] W. Hoeffding. Probability inequalities for sums of bounded random variables. _Journal of the American Statistical Association_, 58(301):13-30, 1963.
* [29] A. Huang, J. Chen, and N. Jiang. Reinforcement learning in low-rank mdps with density features. _International Conference on Machine Learning (ICML)_, 2023.
* [30] R. Islam, M. Tomar, A. Lamb, Y. Efroni, H. Zang, A. Didolkar, D. Misra, X. Li, H. van Seijen, R. T. d. Combes, et al. Agent-controller representations: Principled offline rl with rich exogenous information. _International Conference on Machine Learning (ICML)_, 2023.
* [31] N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. Contextual decision processes with low Bellman rank are PAC-learnable. In _International Conference on Machine Learning_, pages 1704-1713, 2017.
* [32] C. Jin, Q. Liu, and S. Miryoosefi. Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. _Neural Information Processing Systems_, 2021.
* [33] S. M. Kakade. _On the sample complexity of reinforcement learning_. University of London, University College London (United Kingdom), 2003.
* [34] M. Kearns and S. Singh. Finite-sample convergence rates for q-learning and indirect algorithms. _Advances in neural information processing systems_, 11, 1998.
* [35] L. Kocsis and C. Szepesvari. Bandit based monte-carlo planning. In _European conference on machine learning_, pages 282-293. Springer, 2006.
* [36] A. Krishnamurthy, A. Agarwal, and J. Langford. PAC reinforcement learning with rich observations. In _Advances in Neural Information Processing Systems_, pages 1840-1848, 2016.

* [37] A. Krishnamurthy, A. Agarwal, T.-K. Huang, H. Daume III, and J. Langford. Active learning for cost-sensitive classification. In _International Conference on Machine Learning_, pages 1915-1924, 2017.
* [38] A. Lamb, R. Islam, Y. Efroni, A. R. Didolkar, D. Misra, D. J. Foster, L. P. Malu, R. Chari, A. Krishnamurthy, and J. Langford. Guaranteed discovery of control-endogenous latent states with multi-step inverse models. _Transactions on Machine Learning Research_, 2023.
* [39] T. Lattimore, C. Szepesvari, and G. Weisz. Learning with good feature representations in bandits and in rl with a generative model. In _International Conference on Machine Learning_, pages 5662-5670. PMLR, 2020.
* [40] G. Li, Y. Chen, Y. Chi, Y. Gu, and Y. Wei. Sample-efficient reinforcement learning is feasible for linearly realizable mdps with limited revisiting. _Advances in Neural Information Processing Systems_, 34:16671-16685, 2021.
* [41] Q. Liu, P. Netrapalli, C. Szepesvari, and C. Jin. Optimistic mle: A generic model-based algorithm for partially observable sequential decision making. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, pages 363-376, 2023.
* [42] Z. Mhammedi, D. J. Foster, and A. Rakhlin. Representation learning with multi-step inverse kinematics: An efficient and optimal approach to rich-observation rl. _International Conference on Machine Learning (ICML)_, 2023.
* [43] D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstraction and provably efficient rich-observation reinforcement learning. _arXiv preprint arXiv:1911.05815_, 2019.
* [44] D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstraction and provably efficient rich-observation reinforcement learning. In _International conference on machine learning_, pages 6961-6971. PMLR, 2020.
* [45] M. A. Qassem, I. Abuhadrous, and H. Elaydi. Modeling and simulation of 5 dof educational robot arm. In _2010 2nd International Conference on Advanced Computer Control_, volume 5, pages 569-574. IEEE, 2010.
* [46] S. Ross and D. Bagnell. Efficient reductions for imitation learning. In _Proceedings of the thirteenth international conference on artificial intelligence and statistics_, pages 661-668. JMLR Workshop and Conference Proceedings, 2010.
* [47] T. Salimans and R. Chen. Learning montezuma's revenge from a single demonstration. _arXiv preprint arXiv:1812.03381_, 2018.
* [48] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. _Nature_, 588(7839):604-609, 2020.
* [49] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR, 2015.
* [50] A. Sidford, M. Wang, X. Wu, L. Yang, and Y. Ye. Near-optimal time and sample complexities for solving markov decision processes with a generative model. _Advances in Neural Information Processing Systems_, 31, 2018.
* [51] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _nature_, 529(7587):484-489, 2016.
* [52] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. _Science_, 362(6419):1140-1144, 2018.
* [53] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. _arXiv preprint arXiv:1801.00690_, 2018.

* Tavakoli et al. [2018] A. Tavakoli, V. Levdik, R. Islam, C. M. Smith, and P. Kormushev. Exploring restart distributions. _arXiv preprint arXiv:1811.11298_, 2018.
* Todorov et al. [2012] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ international conference on intelligent robots and systems_, pages 5026-5033. IEEE, 2012.
* Wang et al. [2021] Y. Wang, R. Wang, and S. M. Kakade. An exponential lower bound for linearly-realizable MDPs with constant suboptimality gap. _Neural Information Processing Systems (NeurIPS)_, 2021.
* Weisz et al. [2021] G. Weisz, P. Amortila, B. Janzer, Y. Abbasi-Yadkori, N. Jiang, and C. Szepesvari. On query-efficient planning in mdps under linear realizability of the optimal state-value function. In _Conference on Learning Theory_, pages 4355-4385. PMLR, 2021.
* Weisz et al. [2021] G. Weisz, P. Amortila, and C. Szepesvari. Exponential lower bounds for planning in MDPs with linearly-realizable optimal action-value functions. In _Algorithmic Learning Theory_, pages 1237-1264. PMLR, 2021.
* Weisz et al. [2022] G. Weisz, A. Gyorgy, T. Kozuno, and C. Szepesvari. Confident approximate policy iteration for efficient local planning in \(q^{\pi}\)-realizable mdps. _Advances in Neural Information Processing Systems_, 35:25547-25559, 2022.
* Weisz et al. [2023] G. Weisz, A. Gyorgy, and C. Szepesvari. Online rl in linearly \(q^{\pi}\)-realizable mdps is as easy as in linear mdps if you learn what to ignore. _arXiv preprint arXiv:2310.07811_, 2023.
* Wen and Van Roy [2017] Z. Wen and B. Van Roy. Efficient reinforcement learning in deterministic systems with value function generalization. _Mathematics of Operations Research_, 42(3):762-782, 2017.
* Xie and Jiang [2021] T. Xie and N. Jiang. Batch value-function approximation with only realizability. In _International Conference on Machine Learning_, pages 11404-11413. PMLR, 2021.
* Xie et al. [2023] T. Xie, D. J. Foster, Y. Bai, N. Jiang, and S. M. Kakade. The role of coverage in online reinforcement learning. In _The Eleventh International Conference on Learning Representations_, 2023.
* Yang and Wang [2019] L. Yang and M. Wang. Sample-optimal parametric Q-learning using linearly additive features. In _International Conference on Machine Learning_, pages 6995-7004. PMLR, 2019.
* Yin et al. [2022] D. Yin, B. Hao, Y. Abbasi-Yadkori, N. Lazic, and C. Szepesvari. Efficient local planning with linear function approximation. In _International Conference on Algorithmic Learning Theory_, pages 1165-1192. PMLR, 2022.
* Yin et al. [2023] D. Yin, S. Thiagarajan, N. Lazic, N. Rajaraman, B. Hao, and C. Szepesvari. Sample efficient deep reinforcement learning via local planning. _arXiv preprint arXiv:2301.12579_, 2023.
* Zanette et al. [2020] A. Zanette, A. Lazaric, M. Kochenderfer, and E. Brunskill. Learning near optimal policies with low inherent bellman error. In _International Conference on Machine Learning_, pages 10978-10989. PMLR, 2020.
* Zhang et al. [2022] X. Zhang, Y. Song, M. Uehara, M. Wang, A. Agarwal, and W. Sun. Efficient reinforcement learning in block mdps: A model-free representation learning approach. In _International Conference on Machine Learning_, pages 26517-26547. PMLR, 2022.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims do accurately reflect the paper's contribution. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, see the discussion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide a proof-sketch in the main paper and detailed proofs in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. *3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: This paper has only mathematical congent. There are no experiments in this paper. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: This paper has only mathematical congent. There are no experiments in this paper. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.

* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: This paper has only mathematical congent. There are no experiments in this paper. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics**Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: This paper does conform to the code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This paper provides a purely mathematical contribution. As such, it is subject to the standard ethical concerns present for all mathematical papers, but no further ones. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper has only mathematical congent. There are no experiments in this paper. Guidelines: ** The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper has only mathematical congettent. There are no experiments in this paper. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper has only mathematical congettent. There are no experiments in this paper. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used.

* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper has only mathematical congettent. There are no experiments in this paper. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper has only mathematical congettent. There are no experiments in this paper. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.

###### Contents of Appendix

* A Additional Related Work
* I Additional Results
* B Applying RVFS to Exogenous Block MDPs
* C Helper Lemmas
* C.1 Concentration and Probability
* C.2 Regression
* C.3 Reinforcement Learning
* II Proofs for SimGolf (Section 3)
* D Preliminary Lemmas for Proof of Theorem 3.1
* E Proof of Theorem 3.1

## Appendix III Proofs for RVFS (Section 4)
* F Full Version of RVFS
* F.1 RVFS Pseudocode
* F.2 RVFS\({}^{\texttt{exo}}\) Pseudocode
* G Organization
* H Overview of Analysis and Preliminaries
* H.1 Overview of Analysis
* H.2 Benchmark Policy Class and Randomized Policies
* H.3 Additional Preliminaries
* I Guarantee under \(V^{\pi}\)-Realizability (Proof of Theorem 4.1, Setup II)
	* I.1 Analysis: Proof of Theorem 4.1 (Setup II)
	* I.2 Proof of Lemma I.1 (Number of Test Failures)
	* I.3 Proof of Lemma I.2 (Consequence of Passing the Tests)
	* I.4 Proof of Lemma I.3 (Value Function Regression Guarantee)
	* I.5 Proof of Lemma I.4 (Guarantee for Confidence Sets)
	* I.6 Proof of Theorem I.1 (Main Guarantee of RVFS)
	* I.7 Proof of Theorem I.2 (Guarantee of RVFS.bc)
* J Guarantee under \(V^{*}\)-Realizability (Proof of Theorem 4.1, Setup I)
* J.1 Analysis: Proof of Theorem 4.1 (Setup I)
* J.2 Proof of Lemma J.1 (Relaxed \(V^{\pi}\)-Realizability under Gap)
* K Guarantee for Weakly Correlated ExBMDPs (Proof of Theorem B.1)
* K.1 Analysis: Proof of Theorem B.1
* K.2 Proof of Lemma K.1 (Endogenous Benchmark Policies)
* K.3 Proof of Lemma K.2 (Snapping Probability)
* K.4 Proof of Lemma K.3 (Coverability in Weakly Correlated ExBMDP)
* K.5 Proof of Lemma K.6 (Confidence Sets)
* K.6 Proof of Lemma K.7 (Main Guarantee of RVFS\({}^{\texttt{exo}}\))
* L Additional Technical Lemmas
* M BehaviorCloning Algorithm and Analysis
Additional Related Work

Local simulators: Theoretical research.RL with local simulators has received extensive interest in the context of linear function approximation. Most notably, Weisz et al. [57] show that reinforcement learning with linear \(V^{*}\) is tractable with local simulator access, and Li et al. [40] show that RL with linear \(Q^{*}\) and a state-action gap is tractable; online RL is known to be intractable under the same assumptions [57, 56]. Amortila et al. [3] show that the gap assumption can be removed if a small number of expert queries are available. Also of note are the works of Yin et al. [65], Weisz et al. [59], which give computationally efficient algorithms under linear \(Q^{*}\)-realizability for all \(\pi\); this setting is known to be tractable in the online RL model [60], but computationally efficient algorithms are currently only known for RLLS.

_Global simulators_--in which the agent can query arbitrary state-action pairs and observe next state transitions--have also received theoretical investigation, but like local simulators, results are largely restricted to tabular reinforcement learning and linear models [34, 33, 50, 17, 64, 39].

Local simulators: Empirical research.The Go-Explore algorithm [19, 20] uses local simulator access to achieve state-of-the-art performance for the Atari games Montezuma's Revenge and Pitfall--both notoriously difficult games that require systematic exploration. To the best of our knowledge, the performance of Go-Explore on these tasks has yet to be matched by online reinforcement learning; the performing agents [7, 27] are roughly a factor of four worse in terms of cumulative reward. Interestingly, like RVFS, Go-Explore makes use of core sets of informative state-action pairs to guide exploration. However, Go-Explore uses an ad-hoc, domain specific approach to designing the core set, and does not use function approximation to drive exploration.

Recent work of Yin et al. [66] provides an empirical framework for online RL with local planning that can take advantage of deep neural function approximation, and is inspired by the theoretical works in Weisz et al. [57], Li et al. [40], Yin et al. [65], Weisz et al. [59]. This approach does not have provable guarantees, but achieves super-human performance at Montezuma's Revenge.

Other notable empirical works that incorporate local simulator access, as highlighted by Yin et al. [66], include Schulman et al. [49], Salimans and Chen [47], Tavakoli et al. [54].

Planning.RL with local simulator access is a convenient abstraction for the problem of _planning_: Given a known (e.g., learned) model, compute an optimal policy. Planning with a learned model is an important task in theory [25, 41] and practice (e.g., MuZero [48]). Since the model is known, computing an optimal policy is a purely computational problem, not a statistical problem. Nonetheless, for planning problems in large state spaces, where enumerating over all states is undesirable, algorithms for online RL with local simulator access can be directly applied, treating the model as if it were the environment the agent is interacting with. Here, any computationally efficient RLLS algorithm immediately yields an efficient algorithm for planning.

Empirically, Monte-Carlo Tree Search [13, 35] is a successful paradigm for planning, acting as a key component in AlphaGo [51] and AlphaZero [52].6 Viewed as a planning algorithm, a potential advantage of RVFS is that it is well suited to stochastic environments, and provides a principled way to use estimated (neural) value function estimates to guide exploration.

Footnote 6: Compare to our work, a small difference is that these works are not concerned with producing executable policies, c.f. Definition 2.1.

Coverability.Xie et al. [63] introduced coverability as a structural parameter for online reinforcement learning, inspired by connections between online and offline RL. Existing guarantees for the online RL framework based on coverability require either Bellman completeness [63], model-based realizability [5], or weight function realizability [4, 5]), and it is not currently known whether value function realizability is sufficient in this framework.

Exogenous Block MDPs.Our results in Section 3.3 (Corollary 3.1) show that general Exogenous Block MDPs are learnable with local simulator access. Prior work, on learning EXBMDPs in the online RL model requires additional assumptions:

* _Deterministic ExBMDP_[22]. In this setting, the latent transition distribution \(T^{\mathsf{endo}}\) is assumed to be deterministic. In this case, it suffices to learn _open-loop_ policies (i.e., policies that play a deterministic sequence of actions). This avoids compounding errors due to learning imperfect decoders that depend on the exogenous noise, making this setting much less challenging than the general ExBMDP setting.
* _Factored ExMDP_[21]. This is an ExBMDP setting with a restrictive structure in which the observation is a \(d\)-dimensional vector and the latent state is a \(k\)-dimensional subset of the observed coordinates. This structure prevents the setting from subsuming the basic (non-exogenous) Block MDP framework, and makes it possible to learn decoders that act only on the endogenous state, preventing compounding errors.
* _Bellman completeness_. Xie et al. [63] observed that ExBMDPs admit low coverability, but their algorithm requires Bellman completeness, which is not satisfied by ExBMDPs (see Efroni et al. [22], Islam et al. [30]).

## Part I Additional Results

This section of the appendix contains additional results omitted from the main body due to space constraints.

### Appendix B Applying RVFS to Exogenous Block MDPs

We now apply RVFS to the Exogenous Block MDP (ExBMDP) model introduced in Section 3.3. ExBMDPs satisfy coverability (Assumption 3.2), but do not satisfy the pushforward coverability assumption (Assumption 4.1) required by RVFS in general. However, it turns out that ExBMDPs _do_ satisfy pushforward coverability when the exogenous noise process is weakly correlated across time; we refer to this new statistical assumption as the _weak correlation condition_.

**Assumption B.1** (Weak correlation condition).: _For the underlying ExBMDP \(\mathcal{M}\), there is a constant \(C_{\mathsf{exo}}\geq 1\) such that for all \(h\in[H-1]\) and \((\xi,\xi^{\prime})\in\Xi_{h-1}\times\Xi_{h}\), we have7_

Footnote 7: Throughout this paper, when considering the law for the exogenous variables \(\mathbf{\xi}_{1},\ldots,\mathbf{\xi}_{H}\), we write \(\mathbb{P}[\cdot]\) instead of \(\mathbb{P}^{\pi}[\cdot]\) to emphasize that the law is independent of the agents policy.

\[\mathbb{P}[\mathbf{\xi}_{h}=\xi,\mathbf{\xi}_{h+1}=\xi^{\prime}]\leq C_{\mathsf{exo}} \cdot\mathbb{P}[\mathbf{\xi}_{h}=\xi]\cdot\mathbb{P}[\mathbf{\xi}_{h+1}=\xi^{\prime}].\]

The weak correlation property asserts that the joint law for the exogenous noise variables \(\mathbf{\xi}_{h}\) and \(\mathbf{\xi}_{h+1}\) is at most a multiplicative factor \(C_{\mathsf{exo}}\geq 1\) larger than the corresponding product distribution obtained by sampling \(\mathbf{\xi}_{h}\) and \(\mathbf{\xi}_{h+1}\) independently from their marginals. This setting strictly generalizes the (non-exogenous) Block MDP model [36, 15, 43, 68, 42], by allowing for arbitrary stochastic dynamics for the endogenous state and an arbitrary emission process, but requires that temporal correlations in the exogenous noise decay over time.

We show that under Assumption B.1, pushforward coverability is satisfied with \(C_{\mathsf{push}}\leq C_{\mathsf{exo}}\cdot SA\) (Lemma K.3 in Appendix K.1). In addition, \(V^{*}\)-realizability is implied by decoder realizability (Lemma D.1). Thus, by applying Theorem 4.1 (**Setup I**), we conclude that RVFS efficiently learns a near-optimal policy for any weakly correlated ExBMDP for which the optimal value function has \(\Delta\)-gap.

An improved algorithm for ExBMDPs: RVFS20.At first glance, removing the gap assumption for RVFS in ExBMDPs seems difficult: The \(V^{\pi}\)-realizability assumption required to invoke Theorem 4.1 (**Setup II**) is not satisfied by ExBMDPs, as decoder realizability only implies \(V^{\pi}\) realizability for _endogenous_ policies \(\pi\).8 In spite of this, we now show that with a slight modification, RVFS can efficiently learn any weakly correlated ExBMDP under decoder realizability alone (without gap or \(V^{\pi}\)-realizability).

Footnote 8: We say that a policy \(\pi\) is _endogenous_ if it does not depend on exogenous noise, in the sense that \(\pi(\mathbf{x}_{h})\) is a measurable function of \(\phi^{*}(\mathbf{x}_{h})\).

Our new variant of RVFS, RVFS20, is presented in Algorithm 8 (deferred to Appendix F for space). The algorithm is almost identical to RVFS (Algorithm 5), with the main difference being that we use an additional _randomized rounding_ step to compute the policies \(\widehat{\pi}_{1:H}\) from the learned value functions \(\widehat{V}_{1:H}\). In particular, instead of directly defining the policies \(\widehat{\pi}_{1:H}\) based on the bellman backups \(\mathcal{P}_{h}[\widehat{V}_{h+1}]\) as in Eq. (14), RVFS20 targets a "rounded" version of the backup given by

Footnote 20: We say that a policy \(\pi\) is _endogenous_ if it does not depend on exogenous noise, in the sense that \(\pi(\mathbf{x}_{h})\) is a measurable function of \(\phi^{*}(\mathbf{x}_{h})\).

\[\varepsilon\cdot[\mathcal{P}_{h}[\widehat{V}_{h+1}](x,a)/\varepsilon+\zeta_{h} ], \tag{3}\]

where \(\varepsilon\in(0,1)\) is a rounding parameter and \(\zeta_{1},\ldots,\zeta_{H}\) are i.i.d. random variables sampled uniformly at random from the interval \([0,1/2]\) (at the beginning of the algorithm's execution). Concretely, RVFS20 estimates the bellman backup \(\mathcal{P}_{h}[\widehat{V}_{h+1}](x,a)\) in Eq. (3) using the local simulator (as in Eq. (14) of Algorithm 5), and defines its policies via

Footnote 20: We say that a policy \(\pi\) is _endogenous_ if it does not depend on exogenous noise, in the sense that \(\pi(\mathbf{x}_{h})\) is a measurable function of \(\phi^{*}(\mathbf{x}_{h})\).

\[\widehat{\pi}_{h}(\cdot)\in\operatorname*{arg\,max}_{a\in\mathcal{A}}[\mathcal{ P}_{h}[\widehat{V}_{h+1}](\cdot,a)/\varepsilon+\zeta_{h}]. \tag{4}\]This rounding scheme, which quantizes the Bellman backup into \(\varepsilon^{-1}\) bins with a random offset, is designed to emulate certain properties implied by the \(\Delta\)-gap assumption (Assumption 4.4). Specifically, we show that with constant probability over the draw of \(\zeta_{1:H}\), the policy \(\widehat{\pi}\) in (4) "snaps" on to an _endogenous_ policy \(\pi\). This means that for \(\mathsf{RVFS}^{\mathsf{exo}}\) to succeed (with constant probability), it suffices to pass it a class \(\mathcal{V}\) that realizes the value functions \(\left(V_{h}^{\pi}\right)\) for endogenous policies \(\pi\in\Pi_{5}\). Fortunately, such a function class can be constructed explicitly under decoder realizability (Assumption 3.3).

**Lemma B.1** ([21]).: _For the ExBMDP setting, under Assumption 3.3, the function class \(\mathcal{V}_{h}=\{x\mapsto f(\phi(x)):f\in[0,H]^{S},\phi\in\Phi\}\) is such that \(V_{h}^{\pi}\in\mathcal{V}_{h}\) for all endogenous policies \(\pi\). Furthermore, the policy class \(\Pi_{h}\coloneqq\left\{\pi(\cdot)\in\arg\max_{a\in\mathcal{A}}f(\phi(\cdot), a):f\in[0,H]^{S\times A},\phi\in\Phi\right\}\) contains all endogenous policies._

A small technical challenge with the scheme above is that it is only guaranteed to succeed with constant probability over the draw of the rounding parameters \(\zeta_{1},\ldots,\zeta_{H}\). To address this, we provide an outer-level algorithm, \(\mathsf{RVFS}^{\mathsf{exo}}.\mathsf{bc}\) (Algorithm 9, deferred to Appendix F for space), which performs confidence boosting by invoking \(\mathsf{RVFS}^{\mathsf{exo}}\) multiple times independently, and extracts a high-quality executable policy using behavior cloning.

Main result.We now state the main guarantee for \(\mathsf{RVFS}^{\mathsf{exo}}\) (the proof is in Appendix K).

**Theorem B.1** (Main guarantee of \(\mathsf{RVFS}^{\mathsf{exo}}\) for EXBMDPs).: _Consider the ExBMDP setting. Suppose the decoder class \(\Phi\) satisfies Assumption 3.3, and that Assumption B.1 holds with \(C_{\mathsf{exo}}>0\). Let \(\varepsilon,\delta\in(0,1)\) be given, and let \(\mathcal{V}_{h}\) and \(\Pi_{h}\) be as in Lemma B.1. Then \(\mathsf{RVFS}^{\mathsf{exo}}.\mathsf{bc}(\Pi,\mathcal{V}_{1:H},\varepsilon, \zeta_{1:H},\delta)\) (Algorithm 9) produces a policy \(\widehat{\pi}_{1:H}\) such that \(J(\pi^{*})-J(\widehat{\pi}_{1:H})\leq\varepsilon\), and has total sample complexity_

\[\widetilde{\mathcal{O}}\left(C_{\mathsf{exo}}^{8}S^{8}H^{36}A^{9}\cdot \varepsilon^{-26}\right).\]

This result shows for the first time that sample- and computationally-efficient learning is possible for ExBMDPs beyond deterministic or factored settings [22, 21].

We mention in passing that our use of randomized rounding to emulate certain consequences of the \(\Delta\)-gap assumption leverages the fact that ExBMDPs have a finite number of (endogenous) latent states. It is unclear if this technique can be used when the (latent) state space is large or infinite.

```
1:parameters: Value function class \(\mathcal{V}\), suboptimality \(\varepsilon\in(0,1)\), seeds \(\zeta_{1:H}\in(0,1)\), confidence \(\delta\in(0,1)\).
2:input: Level \(h\in[0\,..\,H]\), value function estimates \(\widehat{V}_{h+1:H}\), confidence sets \(\widehat{\mathcal{V}}_{h+1:H}\), state-action collections \(\mathcal{C}_{h:H}\), and buffers \(\mathcal{B}_{h:H}\), and counters \(t_{h:H}\). /* Initialize parameters. */
3: Set \(M\leftarrow[8\varepsilon^{-2}C_{\text{exe}}SAH]\).
4: Set \(N_{\text{test}}\gets 2^{8}M^{2}H\varepsilon^{-2}\log(8M^{6}H^{8} \varepsilon^{-2}\delta^{-1})\), \(N_{\text{reg}}\gets 2^{8}M^{2}\varepsilon^{-2}\log(8|\mathcal{V}|HM^{2} \delta^{-1})\).
5: Set \(N_{\text{est}}(k)\gets 2N_{\text{reg}}^{2}\log(8AN_{\text{reg}}Hk^{3}/\delta)\) and \(\delta^{\prime}\prec\delta/(4M^{7}N_{\text{test}}^{2}H^{8}|\mathcal{V}|)\).
6: Set \(\varepsilon_{\text{reg}}^{2}\leftarrow\frac{9MH^{2}\log(8M^{2}H|\mathcal{V}|/ \delta)}{N_{\text{est}}}+\frac{34MH^{3}\log(8M^{6}N_{\text{test}}^{2}H^{8}/ \delta)}{N_{\text{test}}}\).
7: Set \(\beta(t)\leftarrow\sqrt{\log_{1/\delta^{\prime}}(8MA|\mathcal{V}|t^{2}/\delta)}\). /* Test the fit for the estimated value functions \(\widehat{V}_{h+1:H}\) at future layers. */
8:for\((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\)do
9:for layer \(\ell=H,\ldots,h+1\)do
10:for\(n=1,\ldots,N_{\text{test}}\)do
11: Draw \(\mathbf{x}_{h}\sim T_{h-1}(\cdot\mid x_{h-1},a_{h-1})\), then draw \(\mathbf{x}_{\ell-1}\) by rolling out with \(\widehat{\pi}_{h+1:H}\), where \[\forall\tau\in[H],\ \ \widehat{\pi}_{\tau}(\cdot)\in\operatorname*{arg\,max}_{a \in\mathcal{A}}\bigl{[}\widehat{\mathbf{\mathcal{P}}}_{\tau,\varepsilon^{2}, \delta^{\prime}}[\widehat{V}_{\tau+1}](\cdot,a)\cdot\varepsilon^{-1}+\zeta_{ \tau}\bigr{]},\quad\text{with}\quad\widehat{V}_{H+1}\equiv 0.\]
12:for\(a_{\ell-1}\in\mathcal{A}\)do
13: Update \(t_{\ell}\gets t_{\ell}+1\). /* Test fit; if test fails, re-fit value functions \(\widehat{V}_{h+1:U}\) up to layer \(\ell\). */
14:if\(\sup_{f\in\widehat{\mathcal{V}}_{\ell}}|(|\widehat{\mathbf{\mathcal{P}}}_{\ell-1, \varepsilon^{2},\delta^{\prime}}[\widehat{V}_{\ell}]-\widehat{\mathbf{\mathcal{P}} }_{\ell-1,\varepsilon^{2},\delta^{\prime}}[f_{\ell}])(\mathbf{x}_{\ell-1},a_{\ell -1})|>\varepsilon^{2}+\varepsilon^{2}\cdot\beta(t_{\ell})\)then
15:\(\mathcal{C}_{\ell}\leftarrow\mathcal{C}_{\ell}\cup\{(\mathbf{x}_{\ell-1},a_{\ell-1 })\}\) and \(\mathcal{B}_{\ell}\leftarrow\mathcal{B}_{\ell}\cup\{(\mathbf{x}_{\ell-1},a_{\ell-1 },\widehat{V}_{\ell},\widehat{V}_{\ell},t_{\ell})\}\).
16:for\(\tau=\ell,\ldots,h+1\)do
17:\((\widehat{\mathcal{V}}_{\tau:H},\widehat{\mathcal{V}}_{\tau:H},\mathcal{C}_{ \tau:H},\mathbf{B}_{\tau:H},t_{\tau:H})\leftarrow\mathsf{RVF}_{\tau}^{\text{exe} }(\widehat{\mathcal{V}}_{\tau+1:H},\widehat{\mathcal{V}}_{\tau+1:H},\mathcal{ C}_{\tau:H},\mathcal{B}_{\tau:H},t_{\tau:H};\mathcal{V},\varepsilon,\zeta_{1:H},\delta)\).
18:go to line 8.
19:if\(h=0\)thenreturn \((\widehat{V}_{1:H},\cdot,\cdot,\cdot,\cdot)\). /* Re-fit \(\widehat{V}_{h}\) and build a new confidence set. */
20:for\((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\)do
21: Set \(\mathcal{D}_{h}(x_{h-1},a_{h-1})\leftarrow\varnothing\).
22:for\(i=1,\ldots,N_{\text{reg}}\)do
23: Sample \(\mathbf{x}_{h}\sim T_{h-1}(\cdot\mid x_{h-1},a_{h-1})\).
24: For each \(a\in\mathcal{A}\), let \(\widehat{V}_{h}(\mathbf{x}_{h})\) be a Monte-Carlo estimate for \(\mathbb{E}^{\widehat{\pi}_{h:H}}\bigl{[}\sum_{\ell=h}^{H}\mathbf{r}_{\ell}\mid\mathbf{x }_{h}\bigr{]}\) computed by collecting \(N_{\text{est}}(|\mathcal{C}_{h}|)\) trajectories starting from \(\mathbf{x}_{h}\) and rolling out with \(\widehat{\pi}_{h:H}\).
25: Update \(\mathcal{D}(x_{h-1},a_{h-1})\leftarrow\mathcal{D}(x_{h-1},a_{h-1})\cup\{(\mathbf{x }_{h},\widehat{V}_{h}(\mathbf{x}_{h}))\}\).
26: Let \(\widehat{V}_{h}:=\operatorname*{arg\,min}_{f\in\mathcal{V}_{h}}\sum_{(x_{h-1},a _{h-1})\in\mathcal{C}_{h}}\sum_{(x_{h},v_{h})\in\mathcal{D}_{h}(x_{h-1},a_{h-1} )}(f(x_{h})-v_{h})^{2}\).
27: Compute value function confidence set \[\widehat{\mathcal{V}}_{h}=\left\{f\in\mathcal{V}_{h}\ \middle|\ \sum_{(x_{h-1},a_{h-1})\in\mathcal{C}_{h}}\frac{1}{N_{\text{reg}}}\sum_{(x_{h}, \cdot)\in\mathcal{D}_{h}(x_{h-1},a_{h-1})}\bigl{(}\widehat{V}_{h}(x_{h})-f(x_{h}) \bigr{)}^{2}\leq\varepsilon_{\text{reg}}^{2}\right\}.\] (5)
28:return\((\widehat{V}_{h:H},\widehat{\mathcal{V}}_{h:H},\mathcal{C}_{h:H},\mathcal{B}_{h:H},t_{h:H})\).
```

**Algorithm 3**\(\mathsf{RVF}_{h}^{\text{exe}}\): Recursive Value Function Search for Exogenous Block MDPs```
1:input: Decoder class \(\Phi\), suboptimality \(\varepsilon\in(0,1)\), confidence \(\delta\in(0,1)\).
2:/* Set parameters for RVFS and define the value function and policy classes. */
3: Set \(\varepsilon_{\text{RwFS}}\leftarrow\varepsilon H^{-1}/48\).
4: Set \(\mathcal{V}=\mathcal{V}_{1:H}\), where \(\mathcal{V}_{h}=\{x\mapsto f(\phi(x)):f\in[0,H]^{S},\phi\in\Phi\}\), \(\forall h\in[H]\).
5: Set \(\Pi=\Pi_{1:H}\), where \(\Pi_{h}=\{\pi(\cdot)\in\arg\max_{a\in A}f(\phi(\cdot),a):f\in[0,H]^{S\times A}, \phi\in\Phi\}\), \(\forall h\in[H]\).
6:/* Set parameters for BehaviorCloning. */
7: Set \(N_{\text{bc}}\leftarrow{8H^{2}\log(4H|\Pi|/\delta)}/{\varepsilon}\), \(N_{\text{boost}}\leftarrow{\log(1/\delta)}/{\log(24SAH\varepsilon)}\), \(N_{\text{eval}}\gets 16^{2}\varepsilon^{-2}\log(2N_{\text{boost}}/\delta)\).
8: Set \(M\leftarrow\lceil\mathbb{S}_{\text{RwFS}}^{-1}SAC_{\text{cov}}H\rceil\), \(N_{\text{test}}\gets 2^{8}M^{2}\varepsilon_{\text{RwFS}}^{-1}\log(80M^{6}H^{8}N_{ \text{boost}}\varepsilon_{\text{RwFS}}^{-2}\delta^{-1})\), and \(\delta^{\prime}=\frac{\delta}{40M^{7}N^{2}H^{8}|\mathcal{V}|N_{\text{boost}}}\).
9: Set \(N_{\text{reg}}\leftarrow{2^{8}M^{2}\varepsilon_{\text{RwFS}}^{-1}\log(80| \Phi|^{2}HM^{2}N_{\text{boost}}\delta^{-1})}\).
10: Set \(\widetilde{V}_{1:H}\leftarrow\) arbitrary, \(\widetilde{\mathcal{V}}_{1:H}\leftarrow\mathcal{V}\), \(\mathcal{C}_{0:H}\leftarrow\varnothing\), \(\mathcal{B}_{0:H}\leftarrow\varnothing\), \(i_{\text{opt}}=1\), and \(J_{\max}=0\). /* Repeatedly invoke RVFS\({}^{\text{ego}}\) and extract policy with BehaviorCloning to boost confidence. */
11:for\(i=1,\ldots,N_{\text{boost}}\)do /* Invoke RVFS\({}^{\text{ego}}\). */
12:\((\widetilde{V}_{1:H}^{i},\cdot,\cdot,\cdot)\leftarrow\) RVFS\({}^{\text{ego}}_{0}\)\((\widetilde{V}_{1:H},\widetilde{\mathcal{V}}_{1:H},\mathcal{C}_{0:H},\mathcal{B}_{0:H}; \mathcal{V},N_{\text{reg}},N_{\text{test}},\varepsilon_{\text{RwFS}},\delta/(10 N_{\text{boost}}))\). /* Imitation learning with BehaviorCloning. */
13: Define \(\widetilde{\pi}_{h}^{\text{RwFS}}(\cdot)\in\arg\max_{a\in A}\mathcal{P}_{h,c \in\text{rus},\delta^{\prime}}[\widetilde{V}_{h}^{(i)}]\), \(a\).
14: Compute \(\widetilde{\pi}_{1:H}^{(i)}\leftarrow\) BehaviorCloning\((\Pi,\varepsilon,\widetilde{\pi}_{1:H}^{\text{RwFS}},\delta/(2N_{\text{boost}}))\). /* Evaluate current policy. */
15:\(v=0\).
16:for\(=1,\ldots,N_{\text{eval}}\)do
17: Sample trajectory \((\mathbf{x}_{1},\mathbf{a}_{1},\mathbf{r}_{1},\ldots,\mathbf{x}_{H},\mathbf{a}_{H},\mathbf{r}_{H})\) by executing \(\widetilde{\pi}_{1:H}^{(i)}\).
18: Set \(v\gets v+\sum_{h=1}^{H}r_{h}\).
19: Set \(\widetilde{\mathcal{J}}(\widetilde{\pi}_{1:H}^{(i)})\gets v/N_{\text{eval}}\).
20:if\(\widetilde{\mathcal{J}}(\widetilde{\pi}_{1:H}^{(i)})>J_{\max}\)then
21: Set \(i_{\text{opt}}=i\).
22: Set \(J_{\max}=\widetilde{\mathcal{J}}(\widetilde{\pi}_{1:H}^{(i_{\text{opt}})})\).
23:return:\(\widetilde{\pi}_{1:H}=\widetilde{\pi}_{1:H}^{(i_{\text{opt}})}\).
```

**Algorithm 4** RVFS\({}^{\text{ego}}\).bc: Learn an executable policy with RVFS\({}^{\text{ego}}\) via imitation learning.

## Appendix C Helper Lemmas

This section of the appendix contains supporting lemmas used within the proofs of our main results.

### Concentration and Probability

**Lemma C.1**.: _Let \(\delta\in(0,1)\) and \(H\geq 1\) be given. If a sequence of events \(\mathcal{E}_{1},\ldots,\mathcal{E}_{H}\) satisfies \(\mathbb{P}[\mathcal{E}_{h}\mid\mathcal{E}_{1},\ldots,\mathcal{E}_{h-1}]\geq 1 -\delta/H\) for all \(h\in[H]\), then_

\[\mathbb{P}[\mathcal{E}_{1:H}]\geq 1-\delta.\]

Proof of Lemma c.1.: By the chain rule, we have

\[\mathbb{P}[\mathcal{E}_{1:H}]=\prod_{h\in[H]}\mathbb{P}[\mathcal{E}_{h}\mid \mathcal{E}_{1},\ldots,\mathcal{E}_{h-1}]\geq\prod_{h\in[H]}(1-\delta/H)=(1- \delta/H)^{H}\geq 1-\delta.\]

We make use of the following version of Freedman's inequality, due to Agarwal et al. [1, Lemma 9]:

**Lemma C.2**.: _Let \(R>0\) be given and let \(\mathbf{w}_{1},\ldots\mathbf{w}_{n}\) be a sequence of real-valued random variables adapted to filtration \(\mathcal{H}_{1},\cdots,\mathcal{H}_{n}\). Assume that for all \(t\in[n]\), \(\mathbf{w}_{i}\leq R\) and \(\mathbb{E}[\mathbf{w}_{i}\mid\mathcal{H}_{i-1}]=0\). Define \(\mathbf{S}_{n}\coloneqq\sum_{t=1}^{n}\mathbf{w}_{i}\) and \(V_{n}\coloneqq\sum_{t=1}^{n}\mathbb{E}[\mathbf{w}_{i}^{2}\mid\mathcal{H}_{i-1}]\). Then, for any \(\delta\in(0,1)\) and \(\lambda\in[0,1/R]\), with probability at least \(1-\delta\),_

\[\mathbf{S}_{n}\leq\lambda V_{n}+\log(1/\delta)/\lambda.\]

We will also use the following lemma, which is a standard consequence of Freedman's inequality.

**Lemma C.3** (e.g., Foster et al. [25]).: _Let \((\mathbf{w}_{t})_{t\leq T}\) be a sequence of random variables adapted to a filtration \((\mathcal{H}_{t})_{t\leq T}\). If \(0\leq\mathbf{w}_{t}\leq R\) almost surely, then with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}\mathbf{w}_{t}\leq\frac{3}{2}\sum_{t=1}^{T}\mathbb{E}_{t-1}[\mathbf{w}_{t }]+4R\log(2\delta^{-1}),\]

_and_

\[\sum_{t=1}^{T}\mathbb{E}_{t-1}[\mathbf{w}_{t}]\leq 2\sum_{t=1}^{T}\mathbf{w}_{t}+8 R\log(2\delta^{-1}).\]

### Regression

Using Lemmas C.2 and C.3, we obtain the following concentration lemma, which will be used to prove guarantees for square loss regression within our algorithms.

**Lemma C.4**.: _Let \(B>0\) and \(n\in\mathbb{N}\) be given, and let \(\mathcal{Y}\) be an abstract set. Further, let \(\mathcal{Q}\subseteq\{g:\mathcal{Y}\to[0,B]\}\) be a finite function class and \(\mathbf{y}_{1},\ldots,\mathbf{y}_{n}\) be a sequence of random variables in \(\mathcal{Y}\) adapted to filtration a \(\mathcal{H}_{1},\cdots,\mathcal{H}_{n}\). Then, for any \(\delta\in(0,1)\), with probability at least \(1-\delta\), we have_

\[\forall g\in\mathcal{Q},\quad\frac{1}{2}\|g\|^{2}-2B^{2}\log(2| \mathcal{Q}|/\delta)\leq\|g\|_{n}^{2}\leq 2\|g\|^{2}+2B^{2}\log(2|\mathcal{Q}|/ \delta),\]

_where \(\|g\|^{2}=\sum_{i\in[n]}\mathbb{E}[g(\mathbf{y}_{i})^{2}\mid\mathcal{H}_{i-1}]\) and \(\|g\|_{n}^{2}=\sum_{i=1}^{n}g(\mathbf{y}_{i})^{2}\)._

Proof of Lemma C.4.: Fix \(g\in\mathcal{Q}\). Applying Lemma C.2 with \(\mathbf{w}_{i}=g(\mathbf{y}_{i})^{2}-\mathbb{E}[g(\mathbf{y}_{i})^{2}\mid\mathcal{H}_{i-1}]\), for all \(i\in[n]\), and \((R,\lambda)=(B^{2},1/B^{2})\), we get that with probability at least \(1-\delta/(2|\mathcal{Q}|)\):

\[\|g\|_{n}^{2}-\|g\|^{2}\leq\lambda B^{2}\|g\|^{2}+\log(2|\mathcal{ Q}|/\delta)/\lambda.\]

By substituting \(\lambda=B^{-2}\) and rearranging, we get

\[\|g\|_{n}^{2}\leq 2\|g\|^{2}+B^{2}\log(2|\mathcal{Q}|/\delta). \tag{6}\]

Similarly, applying Lemma C.2 with \(\mathbf{w}_{i}=\mathbb{E}[g(\mathbf{y}_{i})^{2}\mid\mathcal{H}_{i-1}]-g(\mathbf{y}_{i})^ {2}\), for all \(i\in[n]\), and \((R,\lambda)=(B^{2},1/(2B^{2}))\), we get that with probability at least \(1-\delta/(2|\mathcal{Q}|)\):

\[\|g\|^{2}-\|g\|_{n}^{2}\leq\lambda B^{2}\|g\|^{2}+\log(2|\mathcal{ Q}|/\delta)/\lambda.\]

By substituting \(\lambda=2^{-1}B^{-2}\) and rearranging, we get

\[\|g\|_{n}^{2}\geq\frac{1}{2}\|g\|^{2}-2B^{2}\log(2|\mathcal{Q}|/\delta).\]

Combining this with (6) and the union bound, we get the desired result. 

With this lemma, we now prove the following key result for square loss regression.

**Lemma C.5** (Generic regression guarantee).: _Let \(B>0\) and \(n\in\mathbb{N}\) be given and \(\mathcal{Y}\) be an abstract set. Further, let \(\mathcal{F}\subseteq\{f:\mathcal{Y}\to[0,B]\}\) be a finite function class, and suppose that there is a function \(f_{*}\in\mathcal{F}\) and a sequence of random variables \((\mathbf{y}_{1},\mathbf{x}_{1}),\ldots,(\mathbf{y}_{n},\mathbf{x}_{n})\in\mathcal{Y}\times \mathbb{R}\) such that for all \(i\in[n]\):_

* \(\mathbf{x}_{i}=f_{*}(\mathbf{y}_{i})+\mathbf{\varepsilon}_{i}+\mathbf{b}_{i}\)_;_
* \(|\mathbf{b}_{i}|\leq\xi\)_;_
* \(\mathbf{\varepsilon}_{i}\in[-B,B]\)_; and_
* \(\mathbb{E}[\mathbf{\varepsilon}_{i}\mid\mathfrak{F}_{i}]=0\)_, where_ \(\mathfrak{F}_{i}\coloneqq\sigma(\mathbf{y}_{1:i},\mathbf{\varepsilon}_{1:i-1},\mathbf{x}_ {1:i-1},\mathbf{b}_{1:i-1})\)_._

_Then, for \(\widehat{f}\in\operatorname*{arg\,min}_{f\in\mathcal{F}}\sum_{i=1}^{n}(f(\mathbf{y }_{i})-\mathbf{x}_{i})^{2}\) and any \(\delta\in(0,1)\), with probability at least \(1-\delta/2\),_

\[\|\widehat{f}-f_{*}\|_{n}^{2}\leq 4B^{2}\log(2|\mathcal{F}|/\delta)+4B\sum_{i=1}^{ n}|\mathbf{b}_{i}|,\]

_where \(\|\widehat{f}-f_{*}\|_{n}^{2}\coloneqq\sum_{i=1}^{n}(\widehat{f}(\mathbf{y}_{i})-f^{*}( \mathbf{y}_{i}))^{2}\)._Proof of Lemma c.5.: Fix \(\delta\in(0,1)\) and let \(\widehat{L}_{n}(f)\coloneqq\sum_{i=1}^{n}(f(\mathbf{y}_{i})-\mathbf{x}_{i})^{2}\), for \(f\in\mathcal{F}\), and note that since \(\widehat{f}\in\arg\min_{f\in\mathcal{F}}\widehat{L}_{n}(f)\), we have

\[0\geq\widehat{L}_{n}(\widehat{f})-\widehat{L}_{n}(f_{\star})=\nabla\widehat{L} _{n}(f_{\star})[\widehat{f}-f_{\star}]+\|\widehat{f}-f_{\star}\|_{n}^{2},\]

where \(\nabla\) denotes directional derivative. Rearranging, we get that

\[\|\widehat{f}-f_{\star}\|_{n}^{2} \leq-2\nabla\widehat{L}_{n}(f_{\star})[\widehat{f}-f_{\star}]-\| \widehat{f}-f_{\star}\|_{n}^{2},\] \[=4\sum_{i=1}^{n}(\mathbf{x}_{i}-f_{\star}(\mathbf{y}_{i}))(\widehat{f}( \mathbf{y}_{i})-f_{\star}(\mathbf{y}_{i}))-\|\widehat{f}-f_{\star}\|_{n}^{2},\] \[\leq 4\sum_{i=1}^{n}(\mathbf{\varepsilon}_{i}+\mathbf{b}_{i})(\widehat{f }(\mathbf{y}_{i})-f_{\star}(\mathbf{y}_{i}))-\|\widehat{f}-f_{\star}\|_{n}^{2},\] \[\leq 4\underbrace{\sum_{i=1}^{n}\mathbf{\varepsilon}_{i}\cdot( \widehat{f}(\mathbf{y}_{i})-f_{\star}(\mathbf{y}_{i}))-\|\widehat{f}-f_{\star}\|_{n}^ {2}}_{\text{I}}+4\underbrace{\sum_{i=1}^{n}\mathbf{b}_{i}\cdot(\widehat{f}(\mathbf{y}_ {i})-f_{\star}(\mathbf{y}_{i}))}_{\text{II}}. \tag{7}\]

Bounding Term I.: To bound Term I, we apply Lemma c.2 with \(\mathbf{w}_{i}=\mathbf{\varepsilon}_{i}\cdot(\widehat{f}(\mathbf{y}_{i})-f_{\star}(\mathbf{y }_{i}))\), \(R=B^{2}\), \(\lambda=1/(8B^{2})\), and \(\mathcal{H}_{i}=\mathfrak{F}_{i+1}^{-}\), and use

1. the union bound over \(f\in\mathcal{F}\); and
2. the facts that \(\mathbb{E}[\mathbf{y}_{i}\mid\mathfrak{F}_{i}^{-}]=\mathbf{y}_{i}\) and \(\mathbb{E}[\mathbf{\varepsilon}_{i}\mid\mathfrak{F}_{i}^{-}]=0\),

to get that with probability at least \(1-\delta/2\),

\[4\sum_{i=1}^{n}\mathbf{\varepsilon}_{i}\cdot(\widehat{f}(\mathbf{y}_{i})-f_{\star}( \mathbf{y}_{i}))\leq\|\widehat{f}-f_{\star}\|_{n}^{2}+4B^{2}\log(2|\mathcal{F}|/ \delta).\]

By rearranging, we get that with probability at least \(1-\delta/2\),

\[\text{Term I}\leq 4B^{2}\log(2|\mathcal{F}|/\delta).\]

Bounding Term II.: We now bound the second term in (7). For this, note that since \(\|\widehat{f}-f_{\star}\|_{\infty}\leq B\), we have

\[\text{Term II}\leq 4B\sum_{i=1}^{n}|\mathbf{b}_{i}|.\]

This completes the proof. 

### Reinforcement Learning

**Lemma C.6** (Performance Difference Lemma (e.g., Kakade [33])).: _For any two policies \(\widehat{\pi},\pi\in\Pi_{5}\) and \(t\in[H]\), we have_

\[\mathbb{E}^{\pi}\left[V_{t}^{\pi}(\mathbf{x}_{t})-V_{t}^{\overline{\pi}}(\mathbf{x}_{t })\right]=\mathbb{E}^{\pi}\left[\sum_{h=t}^{H}Q_{h}^{\overline{\pi}}(\mathbf{x}_{ h},\mathbf{\pi}_{h}(\mathbf{x}_{h}))-Q_{h}^{\overline{\pi}}(\mathbf{x}_{h},\widehat{\mathbf{\pi}} _{h}(\mathbf{x}_{h}))\right].\]

_In particular, applying this for \(t=1\) gives_

\[J(\pi)-J(\widehat{\pi})=\mathbb{E}^{\pi}\left[\sum_{h=1}^{H}Q_{t}^{\overline{ \pi}}(\mathbf{x}_{h},\mathbf{\pi}_{h}(\mathbf{x}_{h}))-Q_{h}^{\overline{\pi}}(\mathbf{x}_{h}, \widehat{\mathbf{\pi}}_{h}(\mathbf{x}_{h}))\right].\]

**Lemma C.7** (Potential lemma [63]).: _Fix \(h\in[H]\). Suppose we have a sequence of functions \(g^{{(1)}},\ldots,g^{{(T)}}\in[0,B]\) and policies \(\pi^{{(1)}},\ldots,\pi^{{(T)}}\) such that_

\[\forall t\in[T],\quad\sum_{i<t}\mathbb{E}^{\pi^{{(4)}}} \big{[}(g^{{(t)}}(\mathbf{x}_{h}))^{2}\big{]}\leq\beta^{2}\]

[MISSING_PAGE_EMPTY:31]

Finally, we have

\[\sum_{t=1}^{T}\sum_{x\in\mathcal{X}}\frac{(d_{h}^{(t)}(x))^{2}}{ \widehat{d}_{h}^{(t)}(x)}\mathbb{I}\{t\geq\tau_{h}(x)\} \leq 2\sum_{t=1}^{T}\sum_{x\in\mathcal{X}}\frac{(d_{h}^{(t)}(x))^{2} }{\widehat{d}_{h}^{(t)}(x)+C_{\text{push}}\mu_{h}(x)}\] \[\leq 2C_{\text{push}}\sum_{t=1}^{T}\sum_{x\in\mathcal{X}}\mu_{h}(x )\frac{d_{h}^{(t)}(x)}{\widehat{d}_{h}^{(t)}(x)+C_{\text{push}}\mu_{h}(x)}\] \[=2C_{\text{push}}\sum_{x\in\mathcal{X}}\mu_{h}(x)\sum_{t=1}^{T} \frac{d_{h}^{(t)}(x)}{\widehat{d}_{h}^{(t)}(x)+C_{\text{push}}\mu_{h}(x)}\] \[=4C_{\text{push}}\log(T+1),\]

where the last line uses Lemma 4 of Xie et al. [63].

## Part II Proofs for SimGolf (Section 3)

As described in Section 3.1, the main difference between SimGolf and GOLF lies in the construction of the confidence sets. The most important new step in the proof of Theorem 3.1 is to show that the local simulator-based confidence set construction in Line 9 is valid in the sense that the property Eq. (1) holds with high probability. From here, the sample complexity bound follows by adapting the change-of-measure argument based on coverability from Xie et al. [63].

To this end, this part of the appendix is organized as follows. We first state and prove technical lemmas concerning realizability (Lemma D.1) and the confidence set construction (Lemma D.2 and Lemma D.3) in Appendix D. Then, in Appendix E, we prove Theorem 3.1 as a consequence.

### Appendix D Preliminary Lemmas for Proof of Theorem 3.1

For this section, we define

\[\ell_{h}^{(\iota)}(g)\coloneqq\left(g_{h}(\mathbf{x}_{h}^{(\iota)}, \mathbf{a}_{h}^{(\iota)})-\frac{1}{K}\sum_{k=1}^{K}\Big{(}\mathbf{r}_{h}^{(\iota,k)}+ \max_{a\in\mathcal{A}}g_{h+1}(\mathbf{x}_{h+1}^{(\iota,k)},a)\Big{)}\right)^{2}\]

and

\[\widetilde{\ell}_{h}^{(\iota)}(g)\coloneqq\mathbb{E}^{\pi^{(\iota)}}\Big{[} \big{(}g_{h}(\mathbf{x}_{h},\mathbf{a}_{h})-\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h},\mathbf{a} _{h})\big{)}^{2}\Big{]},\]

where \((\mathbf{x}_{h}^{(\iota)},\mathbf{a}_{h}^{(\iota)},\mathbf{r}_{h}^{(\iota,k)},\mathbf{x}_{h+1} ^{(\iota,k)})\) are as in Algorithm 1.

**Lemma D.1** ([21]).: _For the ExBMDP setting, under Assumption 3.3, the function class \(\mathcal{Q}_{h}\coloneqq\{(x,a)\mapsto g(\phi(x),a):g\in[0,H]^{S},\phi\in\Phi\}\) satisfies Assumption 3.1 and has \(\log[\mathcal{Q}_{h}]=\log[\Pi_{h}]=\widetilde{O}(SA+\log[\Phi])\).9_

Footnote 9: Formally, this requires a standard covering number argument; we omit the details.

**Lemma D.2**.: _With probability at least \(1-\delta\), for all \(h\in[H]\), \(t\in[N]\), and \(g\in\mathcal{Q}\),_

\[\sum_{i\in t}\ell_{h}^{(\iota)}(g)\leq 3\sum_{i\in t}\mathbb{E}^{\pi^{(\iota)}} \Big{[}\big{(}g_{h}(\mathbf{x}_{h},\mathbf{a}_{h})-\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h},\mathbf{a}_{h})\big{)}^{2}\Big{]}+\frac{8N}{K}+16\log(2HN|\mathcal{Q}|\delta^{-1}),\]

_and_

\[\sum_{i\in t}\mathbb{E}^{\pi^{(\iota)}}\Big{[}\big{(}g_{h}(\mathbf{x} _{h},\mathbf{a}_{h})-\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h},\mathbf{a}_{h})\big{)}^{2} \Big{]}\leq 4\sum_{i\in t}\ell_{h}^{(\iota)}(g)+\frac{8N}{K}+64\log(2HN| \mathcal{Q}|\delta^{-1}).\]

**Proof of Lemma D.2.** Let \(t\in[N]\) and \(h\in[H]\) be fixed. Let us denote \(\mathbf{z}_{h}^{(\iota)}=\big{\{}(\mathbf{r}_{h}^{(\iota,k)},\mathbf{x}_{h+1}^{(\iota,k)} )\big{\}}_{k\in[K]}\). Define a filtration

\[\mathcal{H}^{(\iota)}=\sigma(\mathbf{\tau}^{(\iota)},\mathbf{z}_{1}^{(1)}, \ldots,\mathbf{z}_{H}^{(1)},\ldots,\mathbf{\tau}^{(\iota)},\mathbf{z}_{1}^{(\iota)},\ldots,\mathbf{z}_{H}^{(\iota)}),\]

where \(\mathbf{\tau}^{(\iota)}\) is the trajectory generated in the \(i\)th iteration of Algorithm 1 (see Line 7). Fix \(g\in\mathcal{Q}\). Observe that \(\ell_{h}^{(\iota)}(g)\in[0,4]\), so Lemma C.3 ensures that with probability at least \(1-\delta\),

\[\sum_{i\in t}\ell_{h}^{(\iota)}(g)\leq\frac{3}{2}\sum_{i\in t} \mathbb{E}\big{[}\ell_{h}^{(\iota)}(g)\mid\mathcal{H}^{(\iota-1)}\big{]}+16 \log(2\delta^{-1}),\]

and

\[\sum_{i\in t}\mathbb{E}\big{[}\ell_{h}^{(\iota)}(g)\mid\mathcal{H} ^{(\iota-1)}\big{]}\leq 2\sum_{i\in t}\ell_{h}^{(\iota)}(g)+32\log(2 \delta^{-1}). \tag{8}\]By the AM-GM inequality, for all \(i<t\), we can bound

\[\mathbb{E}\Big{[}\ell_{h}^{{(i)}}(g)\mid\mathcal{H}^{{(i-1)}} \Big{]}\] \[=\mathbb{E}\Bigg{[}\Bigg{(}g_{h}(\mathbf{x}_{h}^{{(i)}},\mathbf{a}_{h}^{{(i)}})-\frac{1}{K}\sum_{k=1}^{K}\Big{(}\mathbf{r}_{h}^{{(i,k)}}+ \max_{a\in\mathcal{A}}g_{h+1}(\mathbf{x}_{h+1}^{{(i,k)}},a)\Big{)}\Bigg{)}^{2}\mid \mathcal{H}^{{(i-1)}}\Bigg{]}\] \[\leq 2\,\mathbb{E}\Big{[}\Big{(}g_{h}(\mathbf{x}_{h}^{{(i)}},\mathbf{a}_{h}^{{(i)}})-\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h}^{{(i)}},\mathbf{a}_{h}^{{(i)}})\Big{)}^{2}\mid\mathcal{H}^{{(i-1)}}\Big{]}\] \[\quad+2\,\mathbb{E}\Bigg{[}\Bigg{(}\mathcal{T}_{h}[g_{h+1}](\mathbf{x }_{h}^{{(i)}},\mathbf{a}_{h}^{{(i)}})-\frac{1}{K}\sum_{k=1}^{K}\Big{(}\mathbf{r}_{h}^ {{(i,k)}}+\max_{a\in\mathcal{A}}g_{h+1}(\mathbf{x}_{h+1}^{{(i,k)}},a) \Big{)}\Bigg{)}^{2}\mid\mathcal{H}^{{(i-1)}}\Bigg{]}.\]

and

\[\mathbb{E}\Big{[}\ell_{h}^{{(i)}}(g)\mid\mathcal{H}^{{(i-1)}} \Big{]}\] \[\geq\frac{1}{2}\,\mathbb{E}\Big{[}\big{(}g_{h}(\mathbf{x}_{h}^{{(i)}},\mathbf{a}_{h}^{{(i)}})-\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h}^{{(i)}},\mathbf{a}_{h}^{{(i)}})\big{)}^{2}\mid\mathcal{H}^{{(i-1)}} \Big{]}\] \[\quad-\mathbb{E}\Bigg{[}\Bigg{(}\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{ h}^{{(i)}},\mathbf{a}_{h}^{{(i)}})-\frac{1}{K}\sum_{k=1}^{K}\Big{(}\mathbf{r}_{h}^{ {(i,k)}}+\max_{a\in\mathcal{A}}g_{h+1}(\mathbf{x}_{h+1}^{{(i,k)}},a) \Big{)}\Bigg{)}^{2}\mid\mathcal{H}^{{(i-1)}}\Bigg{]}.\]

We have

\[\mathbb{E}\Big{[}\big{(}g_{h}(\mathbf{x}_{h}^{{(i)}},\mathbf{a}_{h}^{{(i)}} )-\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h}^{{(i)}},\mathbf{a}_{h}^{{(i)}}) \big{)}^{2}\mid\mathcal{H}^{{(i-1)}}\Big{]}=\mathbb{E}^{{(i)}} \Big{[}\big{(}g_{h}(\mathbf{x}_{h},\mathbf{a}_{h})-\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h},\mathbf{a}_{h})\big{)}^{2}\Big{]}\]

and

\[\mathbb{E}\Bigg{[}\Bigg{(}\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h}^{{(i)}},\mathbf{a}_{h}^{{(i)}})-\frac{1}{K}\sum_{k=1}^{K}\Big{(}\mathbf{r}_{h}^{{(i,k)}}+\max_ {a\in\mathcal{A}}g_{h+1}(\mathbf{x}_{h+1}^{{(i,k)}},a)\Big{)}\Bigg{)}^{2}\mid \mathcal{H}^{{(i-1)}}\Bigg{]}\] \[=\mathbb{E}\Bigg{[}\mathbb{E}\Bigg{[}\Bigg{(}\mathcal{T}_{h}[g_{h +1}](\mathbf{x}_{h},\mathbf{a}_{h})-\frac{1}{K}\sum_{k=1}^{K}\Big{(}\mathbf{r}_{h}^{{(i,k)}} +\max_{a\in\mathcal{A}}g_{h+1}(\mathbf{x}_{h+1}^{{(i,k)}},a)\Big{)}\Bigg{)}^{2}\mid \mathbf{x}_{h}=\mathbf{x}_{h}^{{(i)}},\mathbf{a}_{h}=\mathbf{a}_{h}^{{(i)}}\Bigg{]}\mid \mathcal{H}^{{(i-1)}}\Bigg{]}.\]

Since

\[\mathbb{E}\Big{[}\mathbf{r}_{h}^{{(i,k)}}+\max_{a\in\mathcal{A}}g_{h+1}(\mathbf{x}_{h+ 1}^{{(i,k)}},a)\mid\mathbf{x}_{h}=\mathbf{x}_{h}^{{(i)}},\mathbf{a}_{h}=\mathbf{a}_{h}^{{(i)}} \Big{]}=\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h}^{{(i)}},\mathbf{a}_{h}^{{(i)}})\]

and \(\big{\{}(\mathbf{r}_{h}^{{(i,k)}},\mathbf{x}_{h+1}^{{(i,k)}})\big{\}}_{k\in[K]}\) are i.i.d. conditioned on \((\mathbf{x}_{h},\mathbf{a}_{h})=(\mathbf{x}_{h}^{{(i)}},\mathbf{a}_{h}^{{(i)}})\), we have,

\[\mathbb{E}\Bigg{[}\Bigg{(}\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h}, \mathbf{a}_{h})-\frac{1}{K}\sum_{k=1}^{K}\Big{(}\mathbf{r}_{h}^{{(i,k)}}+\max_{a\in \mathcal{A}}g_{h+1}(\mathbf{x}_{h+1}^{{(i,k)}},a)\Big{)}\Bigg{)}^{2}\mid\mathbf{x}_{h}= \mathbf{x}_{h}^{{(i)}},\mathbf{a}_{h}=\mathbf{a}_{h}^{{(i)}}\Bigg{]}\] \[=\frac{1}{K}\,\mathbb{E}\Bigg{[}\Bigg{(}\mathcal{T}_{h}[g_{h+1}]( \mathbf{x}_{h},\mathbf{a}_{h})-\Big{(}\mathbf{r}_{h}^{{(i,k)}}+\max_{a\in\mathcal{A}}g_{h+1} (\mathbf{x}_{h}^{{(i,k)}},a)\Big{)}\Bigg{)}^{2}\mid\mathbf{x}_{h}=\mathbf{x}_{h}^{{(i)}}, \mathbf{a}_{h}=\mathbf{a}_{h}^{{(i)}}\Bigg{]}\] \[\leq\frac{4}{K},\]

so that

\[\mathbb{E}\Bigg{[}\Bigg{(}\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h}^{{(i)}},\mathbf{a}_{h} ^{{(i)}})-\frac{1}{K}\sum_{k=1}^{K}\Big{(}\mathbf{r}_{h}^{{(i,k)}}+\max_{a\in \mathcal{A}}g_{h+1}(\mathbf{x}_{h+1}^{{(i,k)}},a)\Big{)}\Bigg{)}^{2}\mid\mathcal{H}^ {{(i-1)}}\Bigg{]}\leq\frac{4}{K}.\]

Combining these bounds with (8) and rearranging thus gives

\[\sum_{i<t}\ell_{h}^{{(i)}}(g)\leq 3\sum_{i<t}\mathbb{E}^{\pi^{{(i)}}} \Big{[}\big{(}g_{h}(\mathbf{x}_{h},\mathbf{a}_{h})-\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h}, \mathbf{a}_{h})\big{)}^{2}\Big{]}+\frac{8N}{K}+16\log(2\delta^{-1}),\]

and

\[\sum_{i<t}\mathbb{E}^{\pi^{{(i)}}}\Big{[}\big{(}g_{h}(\mathbf{x}_{h},\mathbf{a}_{h})- \mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h},\mathbf{a}_{h})\big{)}^{2}\Big{]}\leq 4\sum_{i<t}\ell_{h}^{{(i)}}(g)+\frac{8N}{K}+64\log(2\delta^{-1}).\]

Taking a union bound yields the result.

**Lemma D.3**.: _Define \(\beta_{\mathrm{stat}}=16\log(2HN|\mathcal{Q}|\delta^{-1})\). Suppose we set \(K\geq\frac{8N}{\beta_{\mathrm{stat}}}\) and \(\beta\geq 2\beta_{\mathrm{stat}}\). Then with probability at least \(1-\delta\), for all \(t\in[N]\) and \(h\in\mathcal{H}\):_

* \(Q^{*}\in\mathcal{Q}^{(\iota)}\)_._
* _All_ \(g\in\mathcal{Q}^{(\iota)}\) _satisfy_ \[\sum_{i<t}\mathbb{E}^{\pi^{(\iota)}}\Big{[}\big{(}g_{h}(\mathbf{x}_{h},\mathbf{a}_{h}) -\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h},\mathbf{a}_{h})\big{)}^{2}\Big{]}\leq 9\beta.\]

Proof of Lemma d.3.: Condition on the event in Lemma D.2. For any fixed \(t\in[N]\) and \(h\in[H]\), we have that

\[\sum_{i<t}\ell_{h}^{(\iota)}(Q^{*}) \leq 3\sum_{i<t}\mathbb{E}^{\pi^{(\iota)}}\Big{[}\big{(}Q^{*}_{h} (\mathbf{x}_{h},\mathbf{a}_{h})-\mathcal{T}_{h}[Q^{*}_{h+1}](\mathbf{x}_{h},\mathbf{a}_{h}) \big{)}^{2}\Big{]}+\frac{8N}{K}+16\log(2HN|\mathcal{Q}|\delta^{-1})\] \[\leq\frac{8N}{K}+16\log(2HN|\mathcal{Q}|\delta^{-1})\leq 2\beta_{ \mathrm{stat}},\]

where the first inequality uses that \(Q^{*}_{h}-\mathcal{T}_{h}[Q^{*}_{h+1}]\) and the second inequality uses our choice for \(K\). It follows that \(Q^{*}\in\mathcal{Q}^{(\iota)}\) as long as \(\beta\geq 2\beta_{\mathrm{stat}}\).

To prove the second claim, we note that for all \(g\in\mathcal{Q}^{(\iota)}\), by construction,

\[\sum_{i<t}\mathbb{E}^{\pi^{(\iota)}}\Big{[}\big{(}g_{h}(\mathbf{x}_{h })-\mathcal{T}_{h}[g_{h+1}](\mathbf{x}_{h},\mathbf{a}_{h})\big{)}^{2}\Big{]} \leq 4\sum_{i<t}\ell_{h}^{(\iota)}(g)+\frac{8N}{K}+64\log(2HN| \mathcal{Q}|\delta^{-1})\] \[\leq 4\sum_{i<t}\ell_{h}^{(\iota)}(g)+5\beta_{\mathrm{stat}}\leq 9\beta.\]

## Appendix E Proof of Theorem 3.1

Proof of Theorem 3.1.: From Lemma D.3, the parameter setting in the theorem statement ensures that with probability at least \(1-\delta\), for all \(t\in[2\,..\,N]\), \(Q^{*}\in\mathcal{Q}^{(\iota)}\), and all \(g\in\mathcal{Q}^{(\iota)}\) satisfy

\[\sum_{i<t}\mathbb{E}^{\pi^{(\iota)}}\Big{[}\big{(}g_{h}(\mathbf{x}_{h})-\mathcal{T }_{h}[g_{h+1}](\mathbf{x}_{h},\mathbf{a}_{h})\big{)}^{2}\Big{]}\leq 9\beta. \tag{9}\]

for all \(h\). Let us condition on this event going forward. First, note that since \(Q^{*}\in\mathcal{Q}^{(\iota)}\) for all \(t\in[2\,..\,N]\), we have that

\[J(\pi^{*})\leq\mathbb{E}\Big{[}\max_{a\in\mathcal{A}}Q^{*}_{1}(\mathbf{x}_{1},a) \Big{]}\leq\sup_{g\in\mathcal{Q}^{(\iota)}}\mathbb{E}\Big{[}\max_{a\in \mathcal{A}}g_{1}(\mathbf{x}_{1},a)\Big{]}. \tag{10}\]

On the other hand, we have \(g^{(\iota)}\in\arg\max_{g\in\mathcal{Q}^{(\iota)}}\sum_{s<t}\max_{a\in \mathcal{A}}g_{1}(\mathbf{x}_{1}^{(\iota)},a)\), and so since \(\mathbf{x}_{1}^{(1)},\mathbf{x}_{1}^{(2)},\ldots\) are i.i.d. and any \(g\in\mathcal{Q}^{(\iota)}\) take values in \([0,H]\), we have that by Hoeffding's inequality, there is an event \(\mathcal{E}\) of probability at least \(1-\delta\) under which

\[\forall t\in[2\,..\,N],\forall g\in\mathcal{Q},\quad\left|\mathbb{E}\Big{[} \max_{a\in\mathcal{A}}g_{1}(\mathbf{x}_{1},a)\Big{]}-\frac{1}{t-1}\sum_{s<t}\max_{a \in\mathcal{A}}g_{1}(\mathbf{x}_{1}^{(\iota)},a)\right|\leq\sqrt{(t-1)^{-1}\log(2 N|\mathcal{Q}|/\delta)}. \tag{11}\]

This implies that under \(\mathcal{E}\), we have

\[\forall t\in[2\,..\,N],\quad\sup_{g\in\mathcal{Q}^{(\iota)}} \mathbb{E}\Big{[}\max_{a\in\mathcal{A}}g_{1}(\mathbf{x}_{1},a)\Big{]} \leq\sup_{g\in\mathcal{Q}^{(\iota)}}\frac{1}{t-1}\sum_{s<t}\max_{a \in\mathcal{A}}g_{1}(\mathbf{x}_{1}^{(\iota)},a)+\sqrt{(t-1)^{-1}\log(2N|\mathcal{ Q}|/\delta)},\] \[=\frac{1}{t-1}\sum_{s<t}\max_{a\in\mathcal{A}}g_{1}^{(\iota)}(\mathbf{ x}_{1}^{(\iota)},a)+\sqrt{(t-1)^{-1}\log(2N|\mathcal{Q}|/\delta)},\] \[\leq\mathbb{E}\left[\max_{a\in\mathcal{A}}g_{1}^{(\iota)}(\mathbf{x} _{1},a)\right]+2\sqrt{(t-1)^{-1}\log(2N|\mathcal{Q}|/\delta)}, \tag{12}\]where in the last inequality we have used (11) with \(f=g^{(\epsilon)}\). Thus, summing (12) for \(t=2,\ldots N\) and using (10) gives that under \(\mathcal{E}\):

\[\sum_{t=2}^{N}J(\pi^{\star})\leq\sum_{t=2}^{N}\mathbb{E}[g_{1}^{(\epsilon)}( \boldsymbol{x}_{1},\boldsymbol{a}_{1})]+4\sqrt{N\log(2N|\mathcal{Q}|/\delta)},\]

and so since \(J(\pi^{\star})\leq H\),

\[\sum_{t=1}^{N}J(\pi^{\star})\leq\sum_{t=1}^{N}\mathbb{E}[g_{1}^{(\epsilon)}( \boldsymbol{x}_{1},\boldsymbol{a}_{1})]+4\sqrt{N\log(2N|\mathcal{Q}|/\delta)}+H. \tag{13}\]

On the other hand, using that \(g_{H+1}^{(\epsilon)}\equiv 0\), we get

\[\sum_{t=1}^{N}(\mathbb{E}\left[g_{1}^{(\epsilon)}(\boldsymbol{x} _{1},\boldsymbol{a}_{1})\right]-J(\pi^{(\epsilon)}))\] \[\leq\sum_{t=1}^{N}\sum_{h=1}^{H}\mathbb{E}^{\pi^{(\epsilon)}} \left[g_{h}^{(\epsilon)}(\boldsymbol{x}_{h},\boldsymbol{a}_{h})-\boldsymbol{ r}_{h}-\max_{a\in\mathcal{A}}g_{h+1}^{(\epsilon)}(\boldsymbol{x}_{h+1},a) \right],\] \[=\sum_{t=1}^{N}\sum_{h=1}^{H}\mathbb{E}^{\pi^{(\epsilon)}}\left[g _{h}^{(\epsilon)}(\boldsymbol{x}_{h},\boldsymbol{a}_{h})-\mathbb{E}\left[ \boldsymbol{r}_{h}+\max_{a\in\mathcal{A}}g_{h+1}^{(\epsilon)}(\boldsymbol{x}_{ h+1},a)\mid\boldsymbol{x}_{h},\boldsymbol{a}_{h}\right]\right],\quad(\text{law of total expectation})\] \[=\sum_{t=1}^{N}\sum_{h=1}^{H}\mathbb{E}^{\pi^{(\epsilon)}}\left[g _{h}^{(\epsilon)}(\boldsymbol{x}_{h},\boldsymbol{a}_{h})-\mathcal{T}_{h}[g_{h +1}^{(\epsilon)}](\boldsymbol{x}_{h},\boldsymbol{a}_{h})\right].\]

and so, by the potential lemma (Lemma C.7) and (9), we have

\[\leq 6H\sqrt{C_{\text{cov}}\beta N\log(2N)}+2H^{2}C_{\text{cov}}.\]

Combining this with (13), we obtain that with probability at least \(1-2\delta\),

\[\sum_{t=1}^{N}(J(\pi^{\star})-J(\pi^{(\epsilon)}))\leq 6H\sqrt{C_{\text{cov}} \beta N\log(2N)}+4\sqrt{N\log(2N|\mathcal{Q}|/\delta)}+3H^{2}C_{\text{cov}}.\]

It follows that if \(N=\widetilde{O}(H^{2}C_{\text{cov}}\beta/\varepsilon^{2})\), then the policy

\[\widehat{\pi}\in\text{unif}\left(\pi^{(1)},\ldots,\pi^{(N)}\right)\]

returned by SimGolf satisfies, with probability at least \(1-\delta\):

\[J(\pi^{\star})-\mathbb{E}[J(\widehat{\pi})]\leq\varepsilon.\]

Sample complexity.We now bound the number of episodes. Note that that within an iteration \(t\) of SimGolf, the local simulator is called \(KH\) times to update the confidence set, where \(K\leq N/\log(2HN|\mathcal{Q}|/\delta))\). Consequently, the total sample complexity is bounded by

\[HNK\leq\widetilde{O}(H^{5}C_{\text{cov}}^{2}\log(|\mathcal{Q}|/\delta)/ \varepsilon^{4}).\]

## Part III Proofs for RVFS (Section 4)

### Appendix F Full Version of RVFS

Algorithm 5 displays the full version of RVFS. Algorithm 6 contains an "outer-level" wrapper for RVFS, RVFS.bc, which invokes RVFS and extracts an executable policy with imitation learning, and Algorithm 7 contains the subroutine used within Algorithm 5 to approximate Bellman backups for value functions using local simulator access. Additionally, we display the variant of RVFS for Exogenous Block MDPs, described in Appendix B, in Algorithms 8 and 9. Before diving into the proof, we first describe how the full version of the algorithm differs from the informal version presented in the main body in greater detail.

**Differences between full version (Algorithm 5) and informal version (Algorithm 2) of RVFS.** The main difference between Algorithm 2 and its full version in Algorithm 5 is that in the former we simply assume access to quantities involving conditional expectations such as:

* The bellman backups \(\mathcal{P}_{h}[\widehat{V}_{h+1}]\), which are required to evaluate the actions of RVFS's policies (see (2)), and to perform the tests in Line 8; and
* The value functions \(\mathbb{E}^{\widehat{\pi}_{h+1:H}}\big{[}\sum_{\ell=h}^{H}\boldsymbol{r}_{ \tau}\mid\boldsymbol{x}_{h}=x,\boldsymbol{a}_{h}=a\big{]}\) in Line 15, which are needed in the regression problem in Line 16.

These quantities are not available to the algorithm directly, but they can be estimated using the local simulator. This is reflected in the full version of RVFS in Algorithm 5.

Extracting policies from value functions.Let us briefly comment in more detail on how Algorithm 5 extracts the policy \(\pi^{(\iota)}\) from the optimistic value function \(f^{(\iota)}\in\mathcal{V}\) at iteration \(t\). From the Bellman equation, the ideal choice would be to set \(\pi^{(\iota)}_{h}(x)=\arg\max_{a\in\mathcal{A}}\mathcal{P}_{h}\big{[}f^{(\iota )}_{h+1}\big{]}(x,a)\), but this requires knowledge of the transition distribution. Instead, given parameters \(\varepsilon,\delta\in(0,1)\), SimGolf invokes Algorithm 7 via \(\pi^{(\iota)}_{h}(x)\in\arg\max_{a\in\mathcal{A}}\widehat{\boldsymbol{P}}_{h, \varepsilon,\delta}\big{[}f^{(\iota)}_{h+1}\big{]}(x,a)\). The operator \(\widehat{\boldsymbol{P}}_{h,\varepsilon,\delta}[f]\) (Algorithm 7), when given input \((x,a)\in\mathcal{X}\times\mathcal{A}\) and \(f_{h+1}:\mathcal{X}\to\mathbb{R}\), uses the local simulator to generate \(N_{\text{sim}}\geq 1\) next states \(\boldsymbol{x}^{(1)}_{h+1},\ldots,\boldsymbol{x}^{(N_{\text{sim}})}_{h+1} \stackrel{{\text{i.i.d.}}}{{\sim}}T_{h}(\cdot\mid x,a)\) to estimate the bellman back-up \(\mathcal{P}_{h}[f_{h+1}]\) via \(\frac{1}{N_{\text{sim}}}\sum_{i=1}^{N_{\text{sim}}}(\boldsymbol{r}^{(\iota)}_ {h}+f_{h+1}(\boldsymbol{x}^{(\iota)}_{h+1}))\), where \(\boldsymbol{r}^{(1)}_{h},\ldots,\boldsymbol{r}^{(N_{\text{sim}})}_{h} \stackrel{{\text{i.i.d.}}}{{\sim}}R_{h}(x,a)\). The number of samples \(N_{\text{sim}}\) in Algorithm 7 is set as a function of \((\varepsilon,\delta)\) such that with probability at least \(1-\delta\), \(|\widehat{\boldsymbol{P}}_{h,\varepsilon,\delta}[f_{h+1}](x,a)-\mathcal{P}_{h }[f_{h+1}](x,a)|\leq\varepsilon\).

Invoking the algorithm.The base invocation of RVFS takes the form

\[\widehat{\mathcal{V}}_{1:H}\leftarrow\mathsf{RVFS}_{0}(\widehat{\mathcal{V}}_ {1:H}=\mathsf{arbitrary},\widehat{\mathcal{V}}_{1:H}=\{\mathcal{V}\}_{h=1}^{H}, \mathcal{C}_{0:H}=\{\varnothing\}_{h=0}^{H},\mathcal{B}_{0:H}=\{\varnothing\} _{h=0}^{H},t_{1:H}=\{1\}_{h=1}^{H};\cdots).\]

Whenever this call returns, the greedy policy induced by \(\widehat{\mathcal{V}}_{1:H}\) is guaranteed to be near-optimal. Naively, the policy induced by \(\widehat{\mathcal{V}}_{1:H}\) is non-executable, and must be computed by invoking the local simulator through Line 14. To provide an end-to-end guarantee to learn an executable policy, the outer-level algorithm, RVFS.bc (Algorithm 6, invokes RVFS\({}_{0}\), then extracts an executable policy from \(\widehat{V}_{1:H}\) using imitation learning.

Subsequent recursive calls take the form

\[(\widehat{V}_{h:H},\widehat{\mathcal{V}}_{h:H},\mathcal{C}_{h:H},\mathcal{B}_ {h:H},t_{h:H})\leftarrow\mathsf{RVFS}_{h}(\widehat{\mathcal{V}}_{h+1:H}, \widehat{\mathcal{V}}_{h+1:H},\mathcal{C}_{h:H},\mathcal{B}_{h:H},t_{h:H}; \mathcal{V},\varepsilon,\delta).\]

For such a call, the arguments above are:

* \(\widehat{V}_{h+1:H}\): Value function estimates for subsequent layers.
* \(\widehat{\mathcal{V}}_{h+1:H}\): Value function confidence sets \(\widehat{\mathcal{V}}_{h+1:H}\in\mathcal{V}_{h+1:H}\), which are used in the test on Line 14 to quantify uncertainty on new state-action pairs and decide whether to expand the core-sets.
* \(\mathcal{C}_{h:H}\): Core-sets for current and subsequent layers.

* \(\mathcal{B}_{h:H}\): Buffers of tuples \((x_{h-1},a_{h-1},\mathcal{\tilde{V}}_{h},\mathcal{\tilde{V}}_{h},t_{h})\), which record relevant features of the algorithm's state whenever the test on Line 14 fails and a recursive call is performed.
* \(t_{h:H}\): Counters that track the number of times Algorithm 7 is called in the test on Line 14, which facilitate tuning of confidence parameters.

Importantly, the confidence sets \(\mathcal{\tilde{V}}_{h+1:H}\) do not need to be explicitly maintained, and can be invoked implicitly whenever a _regression oracle_ for the value function class is available (cf. discussion in Section 4). Likewise, the buffers \(\mathcal{B}_{h:H}\) are only used in our analysis, and do not need to be explicitly maintained.

### RVFS Pseudocode

```
1:parameters: Value function class \(\mathcal{V}\), suboptimality \(\varepsilon\in(0,1)\), confidence \(\delta\in(0,1)\).
2:input:
* Level \(h\in\{0,\ldots,H\}\).
* Value function estimates \(\widehat{\mathcal{V}}_{h+1:H}\), confidence sets \(\widehat{\mathcal{V}}_{h+1:H}\), state-action collections \(\mathcal{C}_{h:H}\), buffers \(\mathcal{B}_{h:H}\), and counters \(t_{h:H}\).

/* Initialize parameters. */
3: Set \(M\leftarrow[8\varepsilon^{-1}C_{\text{push}}H]\).
4: Set \(N_{\text{test}}\gets 2^{8}M^{2}H\varepsilon^{-1}\log(8M^{6}H^{8} \varepsilon^{-2}\delta^{-1})\), \(N_{\text{reg}}\gets 2^{8}M^{2}\varepsilon^{-1}\log(8|\mathcal{V}|^{2}HM^{2} \delta^{-1})\),
5: Set \(N_{\text{est}}(k)\gets 2^{N_{\text{reg}}}\log(8AN_{\text{reg}}Hk^{3}/\delta)\) and \(\delta^{\prime}\leftarrow\delta/(8M^{7}N_{\text{test}}^{2}H^{8}|\mathcal{V}|)\).
6: Set \(\varepsilon_{\text{reg}}^{2}\leftarrow\frac{9MH^{2}\log(8M^{2}H^{2}|\mathcal{ V}|^{2}/\delta)}{N_{\text{reg}}}+\frac{34MH^{3}\log(8M^{6}N_{\text{test}}^{2}H^{8}/ \delta)}{N_{\text{test}}}\).
7: Set \(\beta(t)\leftarrow\sqrt{2\log_{1/\delta}(8AM|\mathcal{V}|t^{2}/\delta)}\). /* Test the fit for the estimated value functions \(\widehat{\mathcal{V}}_{h+1:H}\) at future layers. */
8:for\((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\)do
9:for layer \(\ell=H,\ldots,h+1\)do
10:for\(n=1,\ldots,N_{\text{test}}\)do
11: Draw \(\mathbf{x}_{h}\sim T_{h-1}(\cdot\mid x_{h-1},a_{h-1})\), then draw \(\mathbf{x}_{\ell-1}\) by rolling out with \(\widehat{\pi}_{h:H}\), where [10] \[\forall\tau\in[H],\quad\widehat{\mathbf{\pi}}_{\tau}(\cdot)\in\operatorname*{arg \,max}_{a\in\mathcal{A}}\widehat{\mathbf{\mathcal{P}}}_{\tau,\varepsilon,\delta^{ \prime}}[\widehat{\mathcal{V}}_{\tau+1}](\cdot,a),\quad\text{with}\quad \widehat{\mathcal{V}}_{H+1}\equiv 0.\] (14)
12:for\(a_{\ell-1}\in\mathcal{A}\)do /* Number of times \(\widehat{\mathcal{P}}_{\ell-1,\varepsilon,\delta^{\prime}}\) (Algorithm 7) is called in the test on Line
14. */
13: Update \(t_{\ell}\gets t_{\ell}+1\). /* Test fit; if test fails, re-fit value functions \(\widehat{\mathcal{V}}_{h+1:H}\) up to layer \(\ell\). */
14:if\(\sup_{f\in\mathcal{P}_{\ell}}|(\widehat{\mathbf{\mathcal{P}}}_{\ell-1,\varepsilon, \delta^{\prime}}[\widehat{\mathbf{\mathcal{P}}}_{\ell}]-\widehat{\mathbf{\mathcal{P}} }_{\ell-1,\varepsilon,\delta^{\prime}}[f_{\ell}])(\mathbf{x}_{\ell-1},a_{\ell-1})|> \varepsilon+\varepsilon\cdot\beta(t_{\ell})\)then
15:\(\mathcal{C}_{\ell}\leftarrow\mathcal{C}_{\ell}\cup\{(\mathbf{x}_{\ell-1},a_{\ell-1 })\}\) and \(\mathcal{B}_{\ell}\leftarrow\mathcal{B}_{\ell}\cup\{(\mathbf{x}_{\ell-1},a_{\ell-1 },\widehat{\mathcal{V}}_{\ell},\widehat{\mathcal{V}}_{\ell},t_{\ell})\}\).
16:for\(\tau=\ell,\ldots,h+1\)do
17:\((\widehat{\mathcal{V}}_{\tau:H},\widehat{\mathcal{V}}_{\tau:H},\mathcal{C}_{ \tau:H},\mathcal{B}_{\tau:H},t_{\tau:H})\leftarrow\mathsf{RVFS}_{\tau}( \widehat{\mathcal{V}}_{\tau+1:H},\widehat{\mathcal{V}}_{\tau+1:H},\mathcal{C} _{\tau:H},\mathcal{B}_{\tau:H},t_{\tau:H};\mathcal{V},\varepsilon,\delta)\).
18:go to line 8.
19:if\(h=0\)then return:\((\widehat{\mathcal{V}}_{1:H},\cdot,\cdot,\cdot)\). /* Re-fit \(\widehat{\mathcal{V}}_{h}\) and build a new confidence set. */
20:for\((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\)do
21:Set \(\mathcal{D}_{h}(x_{h-1},a_{h-1})\leftarrow\varnothing\).
22:for\(i=1,\ldots,N_{\text{reg}}\)do
23: Sample \(\mathbf{x}_{h}\sim T_{h-1}(\cdot\mid x_{h-1},a_{h-1})\).
24: Let \(\widehat{\mathcal{V}}_{h}(\mathbf{x}_{h})\) be a Monte-Carlo estimate for \(\mathbb{E}^{\widehat{\pi}_{h:H}}[\sum_{\ell=h}^{H}\mathbf{r}_{\ell}\mid\mathbf{x}_{h}]\) computed by collecting \(N_{\text{est}}(|\mathcal{C}_{h}|)\) trajectories starting from \(\mathbf{x}_{h}\) and rolling out with \(\widehat{\pi}_{h:H}\).
25: Update \(\mathcal{D}_{h}(x_{h-1},a_{h-1})\leftarrow\mathcal{D}_{h}(x_{h-1},a_{h-1}) \cup\{(\mathbf{x}_{h},\widehat{\mathcal{V}}_{h}(\mathbf{x}_{h}))\}\).
26: Let \(\widehat{\mathcal{V}}_{h}:=\operatorname*{arg\,min}_{f\in\mathcal{V}}\sum_{(x _{h-1},a_{h-1})\in\mathcal{C}_{h}}\sum_{(x_{h},v_{h})\in\mathcal{D}_{h}(x_{h-1},a_{h-1})}(f(x_{h})-v_{h})^{2}\).
27: Compute value function confidence set \[\widehat{\mathcal{V}}_{h}:=\left\{f\in\mathcal{V}\ \middle|\ \sum_{(x_{h-1},a_{h-1})\in\mathcal{C}_{h}}\frac{1}{N_{\text{reg}}}\sum_{(x_{h}, \cdot)\in\mathcal{D}_{h}(x_{h-1},a_{h-1})}\left(\widehat{\mathcal{V}}_{h}(x_{h})-f(x_{h}) \right)^{2}\leq\varepsilon_{\text{reg}}^{2}\right\}.\] (15)
28:return\((\widehat{\mathcal{V}}_{h:H},\widehat{\mathcal{V}}_{h:H},\mathcal{C}_{h:H}, \mathcal{B}_{h:H},t_{h:H})\).
```

**Algorithm 5**RVFS\({}_{h}\): Recursive Value Function Search```
1:input: Value function class \(\mathcal{V}\), policy class \(\Pi\), suboptimality \(\varepsilon\in(0,1)\), confidence \(\delta\in(0,1)\). /* Set parameters for RVFS. */
2:Set \(\varepsilon_{\text{RFS}}\leftarrow\varepsilon H^{-1}/48\).
3:Set \(\widetilde{V}_{1:H}\leftarrow\) arbitrary, \(\widetilde{\mathcal{V}}_{1:H}\leftarrow\mathcal{V}\), \(\mathcal{C}_{0:H}\leftarrow\)\(\varnothing\), \(\mathcal{B}_{0:H}\leftarrow\varnothing\), and \(t_{i}\gets 0\), for all \(i\in[0\,..\,H]\). /* Set parameters for BehaviorCloning. */
4:Set \(M\leftarrow[8e_{\text{RFS}}^{-1}C_{\text{push}}H]\) and \(N_{\text{test}}\gets 2^{8}M^{2}H\varepsilon_{\text{RFS}}^{-1}\log(80M^{6}H^{8} \varepsilon_{\text{RFS}}^{-2}\delta^{-1})\).
5:\(N_{\text{reg}}\gets 2^{8}M^{2}\varepsilon_{\text{RFS}}^{-1}\log(80| \mathcal{V}|^{2}HM^{3}\delta^{-1})\) and \(\delta^{\prime}=\frac{\delta}{40M^{7}N^{2}H^{8}|\mathcal{V}|}\). /* Get value functions from RVFS */
6:\((\widetilde{\mathcal{V}}_{1:H},\cdot,\cdot,\cdot,\cdot)\leftarrow\) RVFS\({}_{0}(\widetilde{\mathcal{V}}_{1:H},\widetilde{\mathcal{V}}_{1:H},\mathcal{C}_{0:H}, \mathcal{B}_{0:H},t_{0:H};\mathcal{V},N_{\text{reg}},N_{\text{test}},\varepsilon _{\text{RFS}},\delta/10)\). /* Extract executable policy via BehaviorCloning algorithm for imitation learning. */
7:Define \(\widehat{\pi}_{h}^{\text{RFS}}(\cdot)\in\arg\max_{a\in A}\widehat{\mathbf{\mathcal{P }}}_{h,\text{crs},\beta^{\prime}}[\widehat{V}_{h:H}](,a)\).
8:Compute \(\widehat{\pi}_{1:H}\leftarrow\texttt{BehaviorCloning}(\Pi,\varepsilon,\widehat{ \pi}_{1:H}^{\text{RFS}},\delta/2)\)
9:return:\(\widehat{\pi}_{1:H}\).
```

**Algorithm 7**\(\widetilde{\mathbf{\mathcal{P}}}_{h,\varepsilon,\delta}[f]\): Estimate conditional expectation \(\mathbb{E}[\mathbf{r}_{h}+f(\mathbf{x}_{h+1})\,|\,\,\mathbf{x}_{h}=\cdot,\mathbf{a}_{h}=\cdot]\).

```
1:parameters: Layer \(h\), suboptimality \(\varepsilon\in(0,1)\), confidence \(\delta\in(0,1)\), target function \(f\).
2:input:\((x,a)\in\mathcal{X}\times\mathcal{A}\).
3:Set \(N_{\texttt{sin}}\coloneqq 2\log(1/\delta)/\varepsilon^{2}\).
4:Set \(\mathcal{D}\leftarrow\varnothing\)
5:for\(i=1,\ldots,N_{\texttt{sin}}\)do
6: Sample \(\mathbf{r}_{h}\sim R_{h}(x,a)\) and \(\mathbf{x}_{h+1}\sim T_{h}(\cdot\,|\,\,x,a)\). // Uses local simulator access.
7: Update \(\mathcal{D}\leftarrow\mathcal{D}\cup\{(\mathbf{r}_{h},\mathbf{x}_{h+1})\}\).
8:return:\(N_{\texttt{sin}}^{-1}\cdot\sum_{(r,x)\in\mathcal{D}}(r+f(x))\).
```

**Algorithm 8**\(\widetilde{\mathbf{\mathcal{P}}}_{h,\varepsilon,\delta}[f]\): Estimate conditional expectation \(\mathbb{E}[\mathbf{r}_{h}+f(\mathbf{x}_{h+1})\,|\,\,\mathbf{x}_{h}=\cdot,\mathbf{a}_{h}=\cdot]\).

[MISSING_PAGE_FAIL:41]

```
1:input: Decoder class \(\Phi\), suboptimality \(\varepsilon\in(0,1)\), confidence \(\delta\in(0,1)\).
2:/* Set parameters for RVFS and define the value function and policy classes. */
3: Set \(\varepsilon_{\text{RwFS}}\leftarrow\varepsilon H^{-1}/48\).
4: Set \(\mathcal{V}=\mathcal{V}_{1:H}\), where \(\mathcal{V}_{h}=\{x\mapsto f(\phi(x)):f\in[0,H]^{S},\phi\in\Phi\}\), \(\forall h\in[H]\).
5: Set \(\Pi=\Pi_{1:H}\), where \(\Pi_{h}=\{\pi(\cdot)\in\arg\max_{a\in A}f(\phi(\cdot),a):f\in[0,H]^{S\times A}, \phi\in\Phi\}\), \(\forall h\in[H]\).
6:/* Set parameters for BehaviorCloning. */
7: Set \(N_{\text{bc}}\leftarrow{8H^{2}\log(4H|\Pi|/\delta)}/{\varepsilon}\), \(N_{\text{boost}}\leftarrow{\log(1/\delta)}/{\log(24SAH\varepsilon)}\), \(N_{\text{eval}}\gets 16^{2}\varepsilon^{-2}\log(2N_{\text{boost}}/\delta)\).
8: Set \(M\leftarrow\lceil\mathbb{S}_{\text{RwFS}}^{-1}SAC_{\text{cov}}H\rceil\), \(N_{\text{test}}\gets 2^{8}M^{2}\varepsilon_{\text{RwFS}}^{-1}\log(80M^{6}H^{8}N_{ \text{boost}}\varepsilon_{\text{RwFS}}^{-2}\delta^{-1})\), and \(\delta^{\prime}=\frac{\delta}{40M^{7}N^{2}H^{8}|\mathcal{V}|N_{\text{boost}}}\).
9: Set \(\widehat{V}_{1:H}\leftarrow\) arbitrary, \(\widehat{\mathcal{V}}_{1:H}\leftarrow\mathcal{V}\), \(\mathcal{C}_{0:H}\leftarrow\varnothing\), \(\mathcal{B}_{0:H}\leftarrow\varnothing\), \(i_{\text{opt}}=1\), and \(J_{\max}=0\). /* Repeatedly invoke RVFS\({}^{\text{ego}}\) and extract policy with BehaviorCloning to boost confidence. */
10:for\(i=1,\ldots,N_{\text{boost}}\)do /* Invoke RVFS\({}^{\text{ego}}\). */
11:\(\left(\widehat{V}_{1:H}^{i_{0}},\cdot,\cdot,\cdot\right)\leftarrow\text{RFS} \widehat{\mathcal{C}}_{0}^{\text{ego}}(\widehat{V}_{1:H},\widehat{\mathcal{V} }_{1:H},\mathcal{C}_{0:H},\mathcal{B}_{0:H};\mathcal{V},N_{\text{reg}},N_{ \text{test}},\varepsilon_{\text{RwFS}},\delta/(10N_{\text{boost}}))\). /* Imitation learning with BehaviorCloning. */
12: Define \(\widehat{\pi}_{h}^{\text{RwFS}}(\cdot)\in\arg\max_{a\in A}\mathcal{P}_{h, \varepsilon_{\text{res}},\delta^{\prime}}[\widehat{V}_{h}^{(i)}]\{\cdot,a\}\).
13: Compute \(\widehat{\pi}_{1:H}^{(i)}\leftarrow\) BehaviorCloning\((\Pi,\varepsilon,\widehat{\pi}_{1:H}^{\text{RwFS}},\delta/(2N_{\text{boost}}))\). /* Evaluate current policy. */
14:\(v=0\).
15:for\(=1,\ldots,N_{\text{eval}}\)do
16: Sample trajectory \((\mathbf{x}_{1},\mathbf{a}_{1},\mathbf{r}_{1},\ldots,\mathbf{x}_{H},\mathbf{a}_{H},\mathbf{r}_{H})\) by executing \(\widehat{\pi}_{1:H}^{(i)}\).
17: Set \(v\gets v+\sum_{h=1}^{H}r_{h}\).
18: Set \(\widehat{\mathcal{J}}(\widehat{\pi}_{1:H}^{(i)})\gets v/N_{\text{eval}}\).
19:if\(\widehat{\mathcal{J}}(\widehat{\pi}_{1:H}^{(i)})>J_{\max}\)then
20: Set \(i_{\text{opt}}=i\).
21: Set \(J_{\max}=\widehat{\mathcal{J}}(\widehat{\pi}_{1:H}^{(i_{\text{opt}})})\).
22:return:\(\widehat{\pi}_{1:H}=\widehat{\pi}_{1:H}^{(i_{\text{opt}})}\).
```

**Algorithm 9** RVFS\({}^{\text{ego}}\).bc: Learn an executable policy with RVFS\({}^{\text{ego}}\) via imitation learning.

Organization

This remainder of Part III of the appendix contains the proofs for the main results concerning the RVFS algorithm (Theorem 4.1 and Theorem B.1).

* First, in Appendix H we give a brief overview of the analysis and introduce a restricted set of _benchmark policies_ which will be used throughout the proofs for Theorem 4.1 and Theorem B.1. The benchmark policy class is central to the regret decomposition for RVFS, and facilitates an analysis that does not require optimism.
* In Appendix I, we prove Theorem 4.1 under **Setup II** (\(V^{\pi}\)-realizability). This constitutes the main technical development for Theorem 4.1. The central technical results proven here are Theorem 1.1, Theorem 1.2 and which generalize Theorem 4.1.
* In Appendix J, we prove Theorem 4.1 under **Setup I** (\(V^{*}\)-realizability), as a straightforward consequence of the tools developed in Appendix I (Theorem I.1 and Theorem I.2).
* Finally, in Appendix K, we prove Theorem B.1 (analysis of RVFS\({}^{\mathsf{exo}}\) for the ExBMDP problem). This analysis has a similar structure to the proof of Theorem 4.1 under **Setup II** in Appendix I, and builds on the same analysis techniques, but requires specialized arguments due to extra technical challenges in the ExBMDP setting.
* Appendix M gives a self-contained presentation of the Behavior Cloning algorithm for imitation learning, which is used within RVFS.bc and RVFS\({}^{\mathsf{exo}}\).bc.

## Appendix H Overview of Analysis and Preliminaries

In this section, we present some notation, technical tools, and preliminary results we require for the analysis of RVFS in the settings we described in Section 4. We start by defining a set of restricted _benchmark policies_ used in the regret decomposition for RVFS.

### Overview of Analysis

In this section we give a brief overview of the analysis techniques behind Theorem 4.1 and Theorem B.1. We focus on Theorem 4.1 to begin.

Recall that RVFS is recursive in the sense that whenever the test in Line 1 fails for a layer \(h\in[H]\), an recursive call RVFS\({}_{h}\) is initiated. Throughout the recursion, via the steps in Item 1 and Item 2, RVFS maintains the following invariant: whenever a call to RVFS\({}_{h}\) (an instance of RVFS initiated at layer \(h\)) terminates, the confidence sets \(\widetilde{\mathcal{V}}_{h+1:H}\) that it outputs satisfy, with high probability:

\[\forall\ell\in[h+1\,..\,H],\quad V^{*}_{\ell}\in\widetilde{\mathcal{V}}_{\ell}.\] (Inv1)

In addition, RVFS\({}_{h}\) can only return if the value function tests in Line 1 (which involve the confidence sets \(\widetilde{\mathcal{V}}_{h+1:H}\)) all succeed. From the invariant in (Inv1), it can be shown that the tests only succeed if the estimated value functions \(\widetilde{V}_{h+1:H}\) satisfy:

\[\forall\ell\in[h+1\,..\,H],\quad\mathbb{P}^{\pi}\left[\sup_{a\in\mathcal{A}}|( \mathcal{P}_{\ell}[\widetilde{V}_{\ell+1}]-\mathcal{P}_{\ell}[V^{*}_{\ell+1}]) (\mathbf{x}_{h},a)|\geq 3\varepsilon\right]\leq\varepsilon_{\text{test}},\] (Inv2)

where \(\varepsilon_{\text{test}}>0\) is a parameter set by the algorithm. We use pushforward coverability to show that RVFS can only expand the core-sets \(\mathcal{C}_{1:H}\) a polynomial number of times before the algorithm terminates and (Inv2) is satisfied.

The invariant in (Inv2) is useful because it ensures that the greedy policies

\[\widetilde{\pi}_{h}(x)\approx\operatorname*{arg\,max}_{a\in\mathcal{A}} \mathcal{P}_{h}[\widetilde{V}_{h+1}](x,a)\]

induced by the learned value functions \(\widetilde{V}_{1:H}\) are near-optimal. To make this precise, recall that given parameters \(\varepsilon,\delta\in(0,1)\), the action \(\widetilde{\pi}_{h}(x)\) of RVFS's policy at layer \(h\) and state \(x\in\mathcal{X}\), is given by

\[\widetilde{\pi}_{h}(x)\in\operatorname*{arg\,max}_{a\in\mathcal{A}}\widetilde {\mathcal{P}}_{h,\varepsilon,\delta}[\widetilde{V}_{h+1}](x,a), \tag{17}\]

where \(\widetilde{V}_{h+1}\) is the estimated value function at layer \(h+1\). The operator \(\widehat{\mathcal{P}}_{h,\varepsilon,\delta}[\widetilde{V}_{h+1}]\) (Algorithm 7), when given input \((x,a)\in\mathcal{X}\times\mathcal{A}\), ensures that probability at least \(1-\delta\), \(|\widehat{\mathcal{P}}_{h,\varepsilon,\delta}[\widetilde{V}_{h+1}](x,a)-\sum_ {a\in\mathcal{A}}\widetilde{\mathcal{P}}_{h,\varepsilon,\delta}[\widetilde{V }_{h+1}](x,a)|\) is the expected value function at layer \(h\).
\(\mathcal{P}_{h}[\widehat{V}_{h+1}](x,a)|\leq\varepsilon\). Combining this with the invariant in (Inv2) and the fact that \(V_{h}^{*}\equiv\mathcal{P}_{h}[V_{h+1}^{*}]\), one can see that with high probability (over the randomness in \(\mathbf{x}_{h}\sim\mathbb{P}^{\mathbb{P}}\) and \(\widehat{\mathbf{\mathcal{P}}}\)), we have:

\[\max_{a\in\mathcal{A}}|\widehat{\mathbf{\mathcal{P}}}_{h,\varepsilon,\delta}[ \widehat{V}_{h+1}](\mathbf{x}_{h},a)-V_{h}^{*}(\mathbf{x}_{h},a)|\leq 4\varepsilon. \tag{18}\]

Analysis under Setup I.For Theorem 4.1 (**Setup I**), Eq. (18) together with the definition of \(\widehat{\pi}_{h}\) in Eq. (17) and the gap assumption (Assumption 4.4) implies that if \(\varepsilon\leq\Delta/8\), we have that with high probability (over the randomness in \(\mathbf{x}_{h}\sim\mathbb{P}^{\mathbb{P}}\) and \(\widehat{\mathbf{\mathcal{P}}}\)),

\[\widehat{\pi}_{h}(\mathbf{x}_{h})\in\operatorname*{arg\,max}_{a\in\mathcal{A}} \widehat{\mathbf{\mathcal{P}}}_{h,\varepsilon,\delta}[\widehat{V}_{h+1}](\mathbf{x}_{ h},a)=\operatorname*{arg\,max}_{a\in\mathcal{A}}V_{h}^{*}(x,a)=\pi_{h}^{*}(\mathbf{x} _{h}).\]

This suffices to show that \(\widehat{\pi}\) is near-optimal, since by the performance lemma [33], the suboptimality of \(\widehat{\pi}\) can be bounded as

\[J(\pi^{*})-J(\widehat{\pi})\leq\sum_{h=1}^{H}\mathbb{P}^{\mathbb{P}}[\widehat{ \pi}_{h}(\mathbf{x}_{h})\neq\pi_{h}^{*}(\mathbf{x}_{h})]. \tag{19}\]

This suffices to prove the performance bound in Theorem 4.1 under **Setup I**.

Analysis under Setup II.For Theorem 4.1 (**Setup II**), an immediate challenge in applying a similar analysis to **Setup I** is the lack of suboptimality gap \(\Delta\), which makes it impossible to directly bound the probability that \(\widehat{\pi}\neq\pi^{*}\) in Eq. (19). To address this, we introduce a _restricted benchmark policy class_\(\Pi_{\varepsilon}\subset\Pi_{5}\) in Appendix H.2 below. The class \(\Pi_{\varepsilon}\subset\Pi_{5}\) is constructed such that that there exists a policy \(\pi\in\Pi_{\varepsilon}\) that (i) is \(O(\varepsilon)\)-suboptimal, and (ii) emulates certain properties of a gap. Together, these properties facilitate analysis similar to **Setup I**. Overall, this argument is similar to the "virtual policy iteration" analysis in Yin et al. [65].

Analysis of \(\mathsf{RVF5}^{\mathsf{exo}}\).The analysis of \(\mathsf{RVF5}^{\mathsf{exo}}\) for ExBMDPs (Theorem B.1) uses the same idea as the analysis for **Setup II**, except that we can only realize \(V^{\pi}\) for _endogenous policies_ that act on \(\phi^{*}(\mathbf{x}_{h})\). To address this, we use the randomized rounding scheme in \(\mathsf{RVF5}^{\mathsf{exo}}\), and the crux of the proof is to show that with high probability, the rounded policies in \(\mathsf{RVF5}^{\mathsf{exo}}\) "snap" onto endogenous policies, facilitating an argument similar to **Setup II**.

Generalizing the analysis.We mention in passing that \(\mathsf{RVF5}\) can be slightly modified to recover other existing sample complexity guarantees for RL with linear function approximation and local simulator access that do not require pushforward coverability, including linear-\(Q^{*}\) realizability with gap [40] and \(Q^{\pi}\)-realizability [65]; we leave a more general treatment for future work.

### Benchmark Policy Class and Randomized Policies

As described above, central to our analysis is a set of \(O(\varepsilon)\)-suboptimal policies against which we benchmark the policies returned \(\mathsf{RVF5}\), which emulate certain consequences of the \(\Delta\)-gap assumption (Assumption 4.4). Before introducing this concept formally, we first define the notion of a _randomized policy_.

Induced stochastic policies.Given an arbitrary collection of independent random variables \(\widetilde{\mathbf{Q}}=(\widetilde{Q}_{h}(x,a))_{(h,x,a)\in[H]\times\mathcal{X} \times\mathcal{A}}\), we say that a policy \(\pi\) is _induced_ by \(\widetilde{\mathbf{Q}}\) if \(\pi\) satisfies

\[\forall h\in[H],\forall x\in\mathcal{X},\quad\mathbf{\pi}_{h}(x)\in\operatorname*{ arg\,max}_{a^{\prime}\in\mathcal{A}}\widetilde{\mathbf{Q}}_{h}(x,a^{\prime}), \tag{20}\]

where we use the bold notation \(\mathbf{\pi}_{h}(x)\) as shorthand for the random variable \(\mathbf{a}_{h}\sim\pi_{h}(x)\in\Delta(\mathcal{A})\); in other words, for each \(x\in\mathcal{X}\), \(\pi_{h}(x)\in\Delta(\mathcal{A})\) is the distribution induced by sampling \(\mathbf{Q}_{h}(x,\cdot)\) and playing the action \(\mathbf{\pi}_{h}(x)\in\operatorname*{arg\,max}_{a^{\prime}\in\mathcal{A}}\widetilde {\mathbf{Q}}_{h}(x,a^{\prime})\). If there are ties in (20), we break them by picking the action with the smallest index; we assume without loss of generality that actions in \(\mathcal{A}\) are index from \(1,\ldots,|\mathcal{A}|\).

Benchmark policy class.We now define the benchmark policy class as follows.

**Definition H.1** (Benchmark policy class).: _For \(\varepsilon\in(0,1)\), let \(\Pi_{\varepsilon}\subseteq\Pi_{5}\) be the set of stochastic policies such that \(\pi\in\Pi_{\varepsilon}\) if and only if there exists a collection of independent random variables \(\widetilde{\mathbf{Q}}=(\widetilde{\mathbf{Q}}_{h}(x,a))_{(h,x,a)\in[H]\times\mathcal{X} \times\mathcal{A}}\) in \([0,H]\) such that:_

* \(\pi\) _is induced by_ \(\widetilde{\mathbf{Q}}\) _(i.e. Eq. (_20_) is satisfied); and_* _For all_ \((h,x,a)\in[H]\times\mathcal{X}\times\mathcal{A}\)_, we have_ \(|(\widetilde{\mathbf{Q}}_{h}-Q_{h}^{\pi})(x,a)|\leq\varepsilon\)_, almost surely under the draw of_ \(\widetilde{\mathbf{Q}}\)_._

Intuitively, the set \(\Pi_{\varepsilon}\) contains the set of all (stochastic) policies corresponding to (randomized) state-action value functions that are point-wise \(O(\varepsilon)\) close to \(Q^{*}\). We formalize this claim in the next lemma.

**Lemma H.1** (Suboptimality of benchmark policies).: _Let \(\varepsilon\in(0,1)\) be given. Let \(\tilde{\pi}\in\Pi_{\varepsilon}\) be a stochastic policy induced by a collection of (independent) random state-action value functions \((\widetilde{\mathbf{Q}}_{h}(x,a))_{(h,x,a)\in[H]\times\mathcal{X}\times\mathcal{A}} \subset[0,H]\) such that for all \(h\in[H]\) and all \((x,a)\in\mathcal{X}\times\mathcal{A}\):_

\[|\widetilde{\mathbf{Q}}_{h}(x,a)-Q_{h}^{\tilde{\pi}}(x,a)|\leq\varepsilon\ \ \text{ almost surely, and }\ \ \tilde{\pi}_{h}(x)\in\operatorname*{arg\,max}_{a^{\prime}\in\mathcal{A}}\widetilde{\mathbf{Q}}_{h}(x,a^{\prime}).\]

_Then, for all \(h\in[H]\),_

\[\forall x\in\mathcal{X},\quad V_{h}^{*}(x)\leq V_{h}^{\tilde{\pi}}(x)+3H\varepsilon. \tag{21}\]

Proof of Lemma H.1.: Using backward induction over \(\ell=H,\ldots,1\), we start by showing that all \(\ell\):

\[\forall(x,a)\in\mathcal{X}\times\mathcal{A},\quad Q_{\ell}^{*}(x,a)\leq \widetilde{\mathbf{Q}}_{\ell}(x,a)+2\varepsilon\cdot(H-\ell+1). \tag{22}\]

almost surely. We then instantiate this with \(\ell=1\) and use the fact that \(\|\widetilde{\mathbf{Q}}_{h}-Q_{h}^{\tilde{\pi}}\|_{\infty}\leq\varepsilon\) to get the desired result.

Base case \([\ell=H]\).By definition of the state-action value function, we have, for all \(\pi\in\Pi_{5}\), \(Q_{H}^{*}\equiv Q_{H}^{\pi}\). Thus, since \(\sup_{(x,a)\in\mathcal{X}\times\mathcal{A}}|(\widetilde{\mathbf{Q}}_{H}-Q_{H}^{ \tilde{\pi}})(x,a)|\leq\varepsilon\) almost surely (by definition of \(\widetilde{\mathbf{Q}}_{1:H}\)), we get that

\[\forall(x,a)\in\mathcal{X}\times\mathcal{A},\quad|\widetilde{\mathbf{Q}}_{H}(x,a) -Q_{H}^{*}(x,a)|\leq\varepsilon,\]

almost surely. This implies (21) for \(\ell=H\).

General case \([\ell<h]\).Fix \(h\in[H-1]\) and suppose that (22) holds for all \(\ell\in[h+1,\ldots,H]\) almost surely. We show that it holds for \(\ell=h\) almost surely. Fix \((x,a)\in\mathcal{X}\times\mathcal{A}\). We have

\[Q_{h}^{*}(x,a)-\widetilde{\mathbf{Q}}_{h}(x,a) =\mathcal{T}_{h}[Q_{h+1}^{*}](x,a)-\mathcal{T}_{h}[\widetilde{ \mathbf{Q}}_{h+1}](x,a)+\mathcal{T}_{h}[\widetilde{\mathbf{Q}}_{h+1}](x,a)-\widetilde{ \mathbf{Q}}_{h}(x,a),\] \[\leq 2\varepsilon\cdot(H-h)+\mathcal{T}_{h}[\widetilde{\mathbf{Q}}_{h +1}](x,a)-\widetilde{\mathbf{Q}}_{h}(x,a), \tag{23}\]

almost surely, where the last step follows by the induction hypothesis. We now bound \(\mathcal{T}_{h}[\widetilde{\mathbf{Q}}_{h+1}](x,a)-\widetilde{\mathbf{Q}}_{h}(x,a)\). We have, almost surely, that

\[\mathcal{T}_{h}[\widetilde{\mathbf{Q}}_{h+1}](x,a)-\widetilde{\mathbf{Q} }_{h}(x,a) =\mathcal{T}_{h}[\widetilde{\mathbf{Q}}_{h+1}](x,a)-\mathcal{P}_{h}[V _{h+1}^{\tilde{\pi}}](x,a)+\mathcal{P}_{h}[V_{h+1}^{\tilde{\pi}}](x,a)- \widetilde{\mathbf{Q}}_{h}(x,a),\] \[=\mathcal{T}_{h}[\widetilde{\mathbf{Q}}_{h+1}](x,a)-\mathcal{P}_{h}[V _{h+1}^{\tilde{\pi}}](x,a)+Q_{h}^{\tilde{\pi}}(x,a)-\widetilde{\mathbf{Q}}_{h}(x,a),\] \[=\mathcal{T}_{h}[\widetilde{\mathbf{Q}}_{h+1}](x,a)-\mathcal{P}_{h}[V _{h+1}^{\tilde{\pi}}](x,a)+\varepsilon,\quad\text{(by the assumption on $\widetilde{\mathbf{Q}}_{h}$)}\] \[=\mathbb{E}\left[\max_{a^{\prime}\in\mathcal{A}}\widetilde{\mathbf{ Q}}_{h+1}(\mathbf{x}_{h+1},a^{\prime})-Q_{h+1}^{\tilde{\pi}}(\mathbf{x}_{h+1},\tilde{ \pi}_{h+1}(\mathbf{x}_{h+1}))\mid\mathbf{x}_{h}=x,\mathbf{a}_{h}=a\right]+\varepsilon,\] \[=\mathbb{E}\left[\widetilde{\mathbf{Q}}_{h+1}(\mathbf{x}_{h+1},\tilde{ \pi}_{h+1}(\mathbf{x}_{h+1}))-Q_{h+1}^{\tilde{\pi}}(\mathbf{x}_{h+1},\tilde{\pi}_{h+1} (\mathbf{x}_{h+1}))\mid\mathbf{x}_{h}=x,\mathbf{a}_{h}=a\right]+\varepsilon,\] \[\leq 2\varepsilon, \tag{24}\]

where the penultimate inequality follows by the definition of \(\tilde{\pi}_{h+1}\), and the last inequality follows by the fact that \(\|\widetilde{\mathbf{Q}}_{h+1}-Q_{h+1}^{\tilde{\pi}}\|_{\infty}\leq\varepsilon\) almost surely, by assumption. Plugging (24) into (23) completes the induction, and so we have that

\[\forall(x,a)\in\mathcal{X}\times\mathcal{A},\quad Q_{1}^{*}(x,a)\leq\widetilde {\mathbf{Q}}_{1}(x,a)\leq 2H\varepsilon.\]

In particular, taking the max over \(a\) on both sides and using the definition of \(\tilde{\pi}\), we get that

\[\forall x\in\mathcal{X},\quad V_{1}^{*}(x)\leq\widetilde{\mathbf{Q}}_{1}(x,\tilde{ \pi}_{1}(x))\leq 2H\varepsilon,\]

almost surely. Combining this with the fact that \(\widetilde{\mathbf{Q}}_{1}(x,\tilde{\pi}_{1}(x))\leq Q_{1}^{\tilde{\pi}}(x,\tilde{ \pi}_{1}(x))+\varepsilon\), almost surely (since \(\|\widetilde{\mathbf{Q}}_{1}-Q_{1}^{\tilde{\pi}}\|_{\infty}\leq\varepsilon\) almost surely by assumption) implies that

\[V_{1}^{*}(x)\leq Q_{1}^{\pi}(x,\tilde{\pi}_{1}(x))+2H\varepsilon+\varepsilon.\]

Taking expectation over \(\tilde{\pi}_{1}(x)\) and bounding \(2H\varepsilon+\varepsilon\) by \(3H\varepsilon\) leads to the desired result.

### Additional Preliminaries

The following lemma gives a guarantee for the Bellman backup approximation algorithm \(\widehat{\mathbf{\mathcal{P}}}\) (Algorithm 7) that is tailored to the analysis of RVFS.

**Lemma H.2**.: _Let \(\varepsilon,\delta,\delta^{\prime}\in(0,1)\), \(B>0\), and \(h\in[H]\), be given and let \(\mathcal{V}\) be a finite function class. For any sequence \((x_{i})_{i\geq 1}\subset\mathcal{X}\) of state action pairs, the outputs \((\widehat{\mathbf{\mathcal{P}}}_{h,\varepsilon,\delta^{\prime}}[f](x_{i},a))_{i \geq 1,a\in\mathcal{A}}\) of Algorithm 7 satisfy, with probability at least \(1-\delta\),_

\[\forall i\geq 1,\forall f\in\mathcal{V},\forall a\in\mathcal{A},\quad| \widehat{\mathbf{\mathcal{P}}}_{h,\varepsilon,\delta^{\prime}}[f](x_{i},a)-\mathcal{ P}_{h}[f](x_{i},a)|\leq\varepsilon\cdot\sqrt{2\log_{1/\delta^{\prime}}(2 Ai^{2}|\mathcal{V}|/\delta)}.\]

Proof of Lemma H.2.: By Hoeffding's inequality [28] and the union bound over \(a\in\mathcal{A}\) and \(f\in\mathcal{V}\), we have that for any \(i\geq 1\), with probability at least \(1-\delta/(2i^{2})\),

\[\forall f\in\mathcal{V},\forall a\in\mathcal{A},\quad|\widehat{\mathbf{\mathcal{P }}}_{h,\varepsilon,\delta^{\prime}}[f](x_{i},a)-\mathcal{P}_{h}[f](x_{i},a)| \leq\varepsilon\cdot\sqrt{2\log_{1/\delta^{\prime}}(2Ai^{2}|\mathcal{V}|/ \delta)}.\]

The desired result follows by the union bound over \(i\geq 1\) and the fact that \(\sum_{i\geq 1}1/i^{2}=\pi^{2}/6\leq 2\). 

## Appendix I Guarantee under \(V^{\pi}\)-Realizability (Proof of Theorem 4.1, Setup II)

In this section, we prove Theorem 4.1 under **Setup II**. First, in Appendix I.1 we state a number of supporting technical lemmas, then use them to prove a more general version of Theorem 4.1, Theorem 1.2, which holds under a weaker realizability assumption (informally, \(V^{\pi}\)-realizability only for _near-optimal_ policies \(\pi\)); Theorem 4.1 follows as an immediate consequence. The remainder of the section (Appendix I.2 through to Appendix I.6) contains the proofs for the intermediate results.

### Analysis: Proof of Theorem 4.1 (Setup II)

We analyze RVFS in the setting of Theorem 4.1 (**Setup II**), where we have a function class \(\mathcal{V}\) satisfying \(V^{\pi}\)-realizability (Assumption 4.5). We will actually show that the conclusion of Theorem 4.1 holds under a weaker function approximation setup we refer to as _relaxed \(V^{\pi}\)-realizability_: instead of requiring \(V^{\pi}\)-realizability for all \(\pi\in\Pi_{5}\), we only require it for policies \(\pi\) in the set of near-optimal policies corresponding to the _benchmark policy class_\(\Pi_{\varepsilon_{\text{real}}}\) for some \(\varepsilon_{\text{real}}>0\) (\(\Pi_{\varepsilon}\) is defined in Appendix H).

**Assumption I.1** (Relaxed \(V^{\pi}\)-realizability).: _For \(\varepsilon_{\text{real}}>0\) and all \(\pi\in\Pi_{\varepsilon_{\text{real}}}\) and \(h\in[H]\), we have \(V_{h}^{\pi}(x)\in\mathcal{V}\subseteq\{f:\mathcal{X}\to[0,H]\}\)._

We will analyze RVFS under Assumption I.1 and Assumption 4.1. However, it turns out that all of the main results for RVFS can be derived under this assumption: As we will see in Appendix J in the sequel, when the \(\Delta\)-gap assumption (Assumption 4.4) is satisfied, then \(\Pi_{\varepsilon_{\text{real}}}=\{\pi^{*}\}\) for all \(\varepsilon_{\text{real}}<\Delta\), allowing us to prove Theorem 4.1 under **Setup I** as a special case of relaxed \(V^{\pi}\)-realizability. Our analysis for the ExBMDP setting in Appendix K requires more work, but uses that for ExBMDPs, Assumption I.1 is satisfied for a subset of \(\Pi_{\varepsilon_{\text{real}}}\) corresponding to endogenous policies.

We begin with our analysis under **Setup II** by bounding the number of times the test in Line 14 fails. Since the sizes of the core sets \(\mathcal{C}_{1:H}\) in RVFS are directly related to the number of test failures, the next result, which bounds \(|\mathcal{C}_{h}|\) for \(h\in[H]\), allows us to show that RVFS terminates in polynomial iterations with high probability. The proof is in Appendix I.2.

**Lemma I.1** (Bounding the number of test failures).: _Let \(\delta,\varepsilon\in(0,1)\) be given, and suppose that Assumption 4.1 (pushforward coverability) holds with parameter \(C_{\text{push}}>0\). Further, let \(f\in\mathcal{V}\), be given, where \(\mathcal{V}\) is an arbitrary function class. Then there is an event \(\mathcal{E}\) of probability at least \(1-\delta\) under which any call \(\mathsf{RVFS}_{0}(f,\mathcal{V}_{1:H},\varnothing,\varnothing,0;\mathcal{V}, \varepsilon,\delta)\) (Algorithm 5) terminates, and throughout the execution of \(\mathsf{RVFS}_{0}\), we have_

\[\forall h\in[H],\quad|\mathcal{C}_{h}|\leq\lceil 8\varepsilon^{-1}C_{\text{push}}H\rceil. \tag{25}\]

In particular, Lemma I.1 ensures that with high probability, every call to RVFS\({}_{h}\) (made recursively via the call to RVFS\({}_{0}\)) terminates in polynomial iterations. When RVFS\({}_{h}\) terminates, all the tests in Line 14 must have passed for all \(\ell>h\). Using this and a standard concentration argument, we get the following guarantee for the estimated value functions and confidence sets returned by each call to RVFS\({}_{h}\). The proof is in Appendix I.3.

**Lemma I.2** (Consequence of passing the tests).: _Let \(h\in[0\,..\,H]\) and \(\varepsilon,\delta\in(0,1)\) be given and consider a call to \(\mathsf{RVF}_{50}\) in the setting of Lemma I.1. Further, let \(\mathcal{E}\) be the event of Lemma I.1. There exists an event \(\mathcal{E}^{\prime}_{h}\) of probability at least \(1-\delta/H\) such that under \(\mathcal{E}\cap\mathcal{E}^{\prime}_{h}\), if a call to \(\mathsf{RVF}_{5h}\) within the execution of \(\mathsf{RVF}_{50}\) terminates and returns \((\widehat{V}_{h:H},\widehat{V}_{h:H},\mathcal{C}_{h:H},\mathcal{B}_{h:H},t_{h: H})\), then for any \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\) and \(\ell\in[h+1\,..\,H+1]\):_

\[\mathbb{P}^{\vec{\pi}}\!\left[\sup_{f\in\widehat{\mathcal{V}}_{\ell}}\max_{a\in \mathcal{A}}\left|\mathcal{P}_{\ell-1}[\widehat{V}_{\ell}]-\mathcal{P}_{\ell- 1}[f_{\ell}](\mathbf{x}_{\ell-1},a)\right|>3\varepsilon\mid\mathbf{x}_{h-1}=x_{h-1}, \mathbf{a}_{h-1}=a_{h-1}\right]\leq\frac{4\log(8M^{6}N_{\mathrm{test}}^{2}H^{8}/ \delta)}{N_{\mathrm{test}}}, \tag{26}\]

_where \((\widehat{\pi}_{\tau})_{\tau\geq h}\) is the stochastic policy induced by \(\widehat{V}_{h:H}\) and \(M\) and \(N_{\mathrm{test}}\) are defined as in Algorithm 5. Furthermore, under the event \(\mathcal{E}\), the total number of times the operator \(\widehat{\mathbf{\mathcal{P}}}\) is called in the test of Line 14 of Algorithm 5 is at most \(O(C_{\mathrm{push}}N_{\mathrm{test}}H^{4}\varepsilon^{-1})\)._

We now give a guarantee for the estimated value functions \(\widehat{V}_{1:H}\) computed within \(\mathsf{RVF}_{50}\) in Line 16 (the proof is in Appendix I.4).

**Lemma I.3** (Value function regression guarantee).: _Let \(h\in[0\,..\,H]\) and \(\delta,\varepsilon^{\prime}\in(0,1)\) be given, and consider a call to \(\mathsf{RVF}_{50}\) in the setting of Lemma I.1. Further, let \(\Pi^{\prime}\subseteq\Pi_{5}\) be a finite policy class such that the class \(\mathcal{V}\) realizes the value functions \(V^{\pi}\) for \(\pi\in\Pi^{\prime}\) (i.e. \(\mathcal{V}\) satisfies Assumption I.1 with \(\Pi_{\varepsilon_{\mathrm{real}}}\) replaced by \(\Pi^{\prime}\)). Then, there is an event \(\mathcal{E}^{\prime\prime}_{h}\) of probability at least \(1-\delta/H\) under which for all \(k\geq 1\), if_

1. \(\mathsf{RVF}_{5h}\) _gets called for the_ \(k\)_th time during the execution of_ \(\mathsf{RVF}_{50}\)_; and_
2. _this_ \(k\)_th call terminates and returns_ \((\widehat{V}_{h:H},\widehat{\mathcal{V}}_{h:H},\mathcal{C}_{h:H},\mathcal{B}_ {h:H},t_{h:H})\)_,_

_then if \((\widehat{\pi}_{\tau})_{\tau\geq h}\) is the policy induced by \(\widehat{V}_{h:H}\) and \(N_{\mathrm{reg}}\) is set as in Algorithm 5, we have that for all \(\pi\in\Pi^{\prime}\),_

\[\sum_{(x_{h-1},a_{h-1})\in\mathcal{C}_{h}}\frac{1}{N_{\mathrm{reg }}}\sum_{(x_{h-1})\in\mathcal{D}_{h}(x_{h-1},a_{h-1})}\left(\widehat{V}_{h}(x _{h})-V^{\pi}_{h}(x_{h})\right)^{2}\] \[\leq\frac{9kH^{2}\log(8k^{2}H|\Pi^{\prime}|\mathcal{V}|/\delta)} {N_{\mathrm{reg}}}+8H^{2}\sum_{(x_{h-1},a_{h-1})\in\mathcal{C}_{h}}\sum_{\tau \to h}^{H}\mathbb{E}^{\vec{\pi}}\left[D_{\mathrm{tv}}(\widehat{\pi}_{\tau}( \mathbf{x}_{\tau}),\pi_{\tau}(\mathbf{x}_{\tau}))\mid\mathbf{x}_{h-1}=x_{h-1},\mathbf{a}_{h-1}= a_{h-1}\right],\]

_where the datasets \(\{\mathcal{D}_{h}(x,a):(x,a)\in\mathcal{C}_{h}\}\) are as in the definition of \(\widehat{\mathcal{V}}_{h}\) in (15)._

Next, we use this result to show that the confidence sets \(\widehat{\mathcal{V}}_{1:H}\) returned by \(\mathsf{RVF}_{50}\) are "valid" in the sense that they contain a value function \((V^{\vec{\pi}}_{h})\) corresponding to a near-optimal stochastic policy \(\widetilde{\pi}\) in the benchmark class \(\Pi_{4\varepsilon}\). In the sequel, we use this fact to substitute \(V^{\vec{\pi}}_{\ell}\) for \(f_{\ell}\) in Eq. (26) and bound the suboptimality of the learned policy \(\widetilde{\pi}\).

**Lemma I.4** (Confidence sets).: _Let \(\varepsilon,\delta\in(0,1)\) be given and suppose that Assumption IV.1 (push-forward coverability) holds with parameter \(C_{\mathrm{push}}>0\). Let \(f\in\mathcal{V}\) be arbitrary, and suppose that \(\mathcal{V}\) satisfies Assumption I.1 with \(\varepsilon_{\mathrm{real}}=4\varepsilon\). Then, there is an event \(\mathcal{E}^{\prime\prime\prime}\) of probability at least \(1-3\delta\) under which a call to \(\mathsf{RVF}_{50}(f,\mathcal{V},\varnothing,0;\mathcal{V},\varepsilon,\delta)\) (Algorithm 5) terminates and returns tuple \((\widehat{V}_{1:H},\widehat{\mathcal{V}}_{1:H},\mathcal{C}_{1:H},\mathcal{B}_ {1:H},t_{1:H})\) such that_

\[\forall h\in[H],\quad V^{\vec{\pi}}_{h}\in\widehat{\mathcal{V}}_{h},\]

_where \(\widetilde{\pi}_{1:H}\in\Pi_{5}\) is the stochastic policy defined recursively via_

\[\forall x\in\mathcal{X},\;\;\widetilde{\pi}_{\tau}(x)\in\operatorname*{arg\, max}_{a\in\mathcal{A}}\left\{\begin{array}{ll}\widehat{\mathbf{Q}}_{\tau}(x,a),& \text{if }|\widehat{\mathbf{Q}}_{\tau}(x,\cdot)-\mathcal{P}_{\tau}[V^{\vec{\pi}}_{ \tau+1}](x,\cdot)\|_{\infty}\leq 4\varepsilon,&\text{for }\tau=H,\ldots,1,\\ \mathcal{P}_{\tau}[V^{\vec{\pi}}_{\tau+1}](x,a),&\text{otherwise},\end{array} \right. \tag{27}\]

_where \(\widehat{\mathbf{Q}}_{\tau}(x,a)\coloneqq\widehat{\mathbf{P}}_{\tau,\varepsilon,\delta} [\widehat{V}_{\tau+1}](x,a)\) is a realization of the stochastic output of the \(\widehat{\mathbf{\mathcal{P}}}\) operator in Algorithm 7 given input \((x,a)\), and \(\delta^{\prime}\) is as in Algorithm 5. Furthermore, we have \(\widetilde{\pi}\in\Pi_{4\varepsilon}\)._

The proof of the lemma is in Appendix I.5.

Equipped with the preceding lemmas, we now state the main technical result of this section, Theorem I.1, a generalization of Theorem IV.1 which holds under relaxed \(V^{\pi}\)-realizability (Assumption I.1). The proof is in Appendix I.6.

**Theorem I.1** (Guarantee for RVFS under relaxed \(V^{\pi}\)-realizability).: _Let \(\delta,\varepsilon\in(0,1)\) be given, and suppose that Assumption 4.1 (pushforward coverability) holds with parameter \(C_{\mathrm{push}}>0\). Let \(f\in\mathcal{V}\) be arbitrary, and assume that \(\mathcal{V}\) that satisfies Assumption I.1 with \(\varepsilon_{\mathrm{real}}=4\varepsilon\). Then, with probability at least \(1-5\delta\), \(\mathsf{RVFS}_{0}(f,\mathcal{V},\varnothing,\varnothing,0;\mathcal{V}, \varepsilon,\delta)\) (Algorithm 5) terminates and returns value functions \(\widetilde{V}_{1:H}\) that satisfy_

\[\forall h\in[H],\quad\mathbb{E}^{\pi}[D_{\mathrm{tv}}(\widetilde{\pi}_{h}( \boldsymbol{x}_{h}),\tilde{\pi}_{h}(\boldsymbol{x}_{h}))]\leq\frac{ \varepsilon}{4H^{3}C_{\mathrm{push}}},\]

_where \(\widetilde{\pi}_{h}(x)\in\arg\max_{a\in\mathcal{A}}\widetilde{\boldsymbol{ \mathcal{P}}}_{h,\varepsilon,\beta^{\prime}}[\widetilde{V}_{h+1}](x,a)\) for all \(h\in[H]\), with \(\tilde{\pi}\in\Pi_{4\varepsilon}\) defined as in Lemma I.4 and \(\delta^{\prime}\) defined as in Algorithm 5. Furthermore, the number of episodes is bounded by_

\[\widetilde{O}(C_{\mathrm{push}}^{8}H^{10}A\cdot\varepsilon^{-13}).\]

Next, we state a guarantee for the outer-level algorithm, RVFS.bc, under relaxed \(V^{\pi}\)-realizability. Recall that RVFS.bc invokes RVFS\({}_{0}\), then extracts an executable policy by applying the BehaviorCloning algorithm (see Appendix M), with the "expert" policy set to be the output of RVFS.

**Theorem I.2** (Main guarantee of RVFS.bc).: _Let \(\delta,\varepsilon\in(0,1)\) be given, and define \(\varepsilon_{\mathsf{RVFS}}=\varepsilon H^{-1}/48\). Suppose that_

* _Assumption_ 4.1 _(pushforward coverability) holds with parameter_ \(C_{\mathrm{push}}>0\)_;_
* _the function class_ \(\mathcal{V}\) _satisfies Assumption_ I.1 _with_ \(\varepsilon_{\mathrm{real}}=1\) _(i.e. all_ \(\pi\)_-realizability); and_
* _the policy class_ \(\Pi\) _satisfies Assumption_ 4.3_._

_Then, with probability at least \(1-\delta\), \(\widetilde{\pi}_{1:H}=\mathsf{RVFS}.\operatorname{bc}(\Pi,\mathcal{V}, \varepsilon,\delta)\) (Algorithm 6) satisfies_

\[J(\pi^{*})-J(\widetilde{\pi}_{1:H})\leq\varepsilon. \tag{28}\]

_Furthermore, the total number sample complexity in the RLLS framework is bounded by_

\[\widetilde{O}\left(C_{\mathrm{push}}^{8}H^{23}A\varepsilon^{-13}\right).\]

The proof is in Appendix I.7. Note that Theorem I.2 is a restatement of Theorem 4.1 in **Setup II** (restated for convenience). As a result, Theorem 4.1 is an immediate corollary.

Proof of Theorem 4.1.: The result follows from Theorem I.2, since Assumption 4.5 is stronger than Assumption I.1. 

### Proof of Lemma I.1 (Number of Test Failures)

**Proof of Lemma I.1**.: Fix \(h\in[H]\). We note that the size of \(\mathcal{C}_{h}\) corresponds to the number of times the test in Line 14 fails for \(\ell=h\) throughout the execution of \(\mathsf{RVFS}_{0}(f,\mathcal{V},\varnothing,\varnothing;\mathcal{V}, \varepsilon,\delta)\).

Let \(M=\lceil 8\varepsilon^{-1}\mathcal{C}_{\mathrm{push}}H\rceil\) denote the desired upper bound on \(|\mathcal{C}_{h}|\). Suppose that the test in Line 14 fails at least twice for \(\ell=h\) (if the test fails at most twice, then \(|\mathcal{C}_{h}|\leq 2\) and so (25) holds for \(\ell=h\) trivially), and let

\[(x_{h-1}^{{}^{(1)}},a_{h-1}^{{}^{(1)}},\widetilde{V}_{h}^{{}^{(1)}}, \widetilde{V}_{h}^{{}^{(1)}},t_{h}^{{}^{(1)}}),(x_{h-1}^{{}^{(2)}},a_{h-1}^{{} ^{(2)}},\widetilde{V}_{h}^{{}^{(2)}},\widetilde{V}_{h}^{{}^{(2)}},t_{h}^{{}^ {(2)}}),\ldots\]

denote the elements of the set \(\mathcal{B}_{h}\) in the order at which they are added to the latter in Line 15 of Algorithm 5. Note that \(|\mathcal{B}_{h}|=|\mathcal{C}_{h}|\). Note also that \(t_{h}^{{}^{(i)}}\) represents the number of times the subroutine \(\widetilde{\boldsymbol{\mathcal{P}}}_{h-1,\varepsilon,\delta^{\prime}}\) has been called in the test of Line 14 throughout the execution of \(\mathsf{RVFS}_{0}\) and up to the time the test failed for \((x_{h-1}^{{}^{(i)}},a_{h-1}^{{}^{(i)}})\). We will use this fact in a concentration argument in the sequel.

By definition of \((\widetilde{\mathcal{V}}_{h}^{{}^{(i)}})\) and Lemma C.4 (Freedman's inequality) instantiated with

* \(\mathcal{Q}=\{\widetilde{V}_{h}^{{}^{(i)}}-f_{h}:f\in\widetilde{\mathcal{V}}_{h }^{{}^{(i)}}\}\);
* \(\boldsymbol{y}_{h}=\boldsymbol{x}_{h}\);
* \(B=H\); and* \(n=N_{\mathsf{reg}}\cdot i\);

and the union bound over \(i\in[M\wedge|\mathcal{C}_{h}|]\), we get that there is an event \(\mathcal{E}_{h}\) of probability at least \(1-\delta/(2H)\) under which

\[\forall i\in[M\wedge|\mathcal{C}_{h}|],\forall f\in\widetilde{ \mathcal{V}}_{h}^{(i)}, \sum_{j<i}\mathbb{E}\big{[}\big{(}\widetilde{V}_{h}^{(i)}(\mathbf{x}_ {h})-f_{h}(\mathbf{x}_{h})\big{)}^{2}\mid\mathbf{x}_{h-1}=x_{h-1}^{(j)},\mathbf{a}_{h-1}= a_{h-1}^{(j)}\big{]}\] \[\leq\tilde{\varepsilon}_{\mathsf{reg}}^{2}\coloneqq 2\varepsilon_{ \mathsf{reg}}^{2}+\frac{4H^{2}\log(4MH|\mathcal{V}|/\delta)}{N_{\mathsf{reg}}}. \tag{29}\]

Now, define \(f_{h}^{(i)}\in\operatorname*{arg\,max}_{f\in\widetilde{\mathcal{V}}_{h}^{(i)}} \big{|}\mathbb{E}\big{[}\widehat{\mathcal{V}}_{h}^{(i)}(\mathbf{x}_{h})-f_{h}(\bm {x}_{h})\mid\mathbf{x}_{h-1}=x_{h-1}^{(i)},\mathbf{a}_{h-1}=a_{h-1}^{(i)}\big{]}\big{|}\). From (29), we have that under \(\mathcal{E}_{h}\):

\[\forall i\in[M\wedge|\mathcal{C}_{h}|], \sum_{j<i}\mathbb{E}\big{[}\big{(}\widehat{\mathcal{V}}_{h}^{(i)}(\mathbf{x}_{h} )-f_{h}^{(i)}(\mathbf{x}_{h})\big{)}^{2}\mid\mathbf{x}_{h-1}=x_{h-1}^{(j)},\mathbf{a}_{h-1 }=a_{h-1}^{(j)}\big{]}\leq\tilde{\varepsilon}_{\mathsf{reg}}^{2}. \tag{30}\]

We now use this to bound the number of times the test in Line 14 fails for \(\ell=h\). Suppose for the sake of contradiction that the test fails at least \(N\) times for some \(N\geq M\) (i.e. \(|\mathcal{C}_{h}|=N\geq M\)). Conditioned on \(\mathcal{E}_{h}\), we have by Lemma C.8 and Eq. (30),

\[\min_{i\in[M]}\sup_{f\in\widetilde{\mathcal{V}}_{h}^{(i)}}\big{|} \mathbb{E}\big{[}\widehat{\mathcal{V}}_{h}^{(i)}(\mathbf{x}_{h})-f_{h}(\mathbf{x}_{h}) \mid\mathbf{x}_{h-1}=x_{h-1}^{(i)},\mathbf{a}_{h-1}=a_{h-1}^{(i)}\big{]}\big{|}\] \[=\min_{i\in[M]}\big{|}\mathbb{E}\big{[}\widehat{\mathcal{V}}_{h} ^{(i)}(\mathbf{x}_{h})-f_{h}^{(i)}(\mathbf{x}_{h})\mid\mathbf{x}_{h-1}=x_{h-1}^{(i)},\mathbf{a }_{h-1}=a_{h-1}^{(i)}\big{]}\big{|},\] \[\leq 2\bigg{(}\frac{C_{\mathrm{push}}}{M^{2}}M\tilde{ \varepsilon}_{\mathsf{reg}}^{2}\log(2M)\bigg{)}^{1/2}+\frac{2C_{\mathrm{push}}H }{M}.\]

Now, substituting the expression of \(\tilde{\varepsilon}_{\mathsf{reg}}^{2}\) in (29) and using the definition of \(\varepsilon_{\mathsf{reg}}^{2}\) in Line 6 of Algorithm 5, we get

\[=2\bigg{(}\frac{C_{\mathrm{push}}}{M}\cdot\bigg{(}2\varepsilon_{ \mathsf{reg}}^{2}+\frac{4MH^{2}\log(4MH|\mathcal{V}|/\delta)}{N_{\mathsf{reg}} }\bigg{)}\bigg{)}^{1/2}+\frac{2C_{\mathrm{push}}H}{M},\] \[\leq 2\bigg{(}\frac{C_{\mathrm{push}}}{M}\cdot\bigg{(}\frac{22 MH^{2}\log(8M^{2}H|\mathcal{V}|^{2}/\delta)}{N_{\mathsf{reg}}}+\frac{68MH^{3}\log(8M^ {6}N_{\mathsf{test}}^{2}H^{8}/\delta)}{N_{\mathsf{test}}}\bigg{)}\bigg{)}^{1/2} +\frac{2C_{\mathrm{push}}H}{M},\] \[\leq\varepsilon, \tag{31}\]

where the last inequality uses that \(M=\lceil 8\varepsilon^{-1}C_{\mathrm{push}}H\rceil\) and

\[N_{\mathsf{reg}}=2^{8}M^{2}\varepsilon^{-1}\log(8|\mathcal{V}|^{2}HM^{2} \delta^{-1})\text{ and }N_{\mathsf{test}}=2^{8}M^{2}H\varepsilon^{-1}\log(8M^{6}H^{8} \varepsilon^{-2}\delta^{-1});\]

see Line 5 of Algorithm 5.

On the other hand, by Lemma H.2, there is an event \(\mathcal{E}_{h}^{\prime}\) of probability at least \(1-\delta/(2MH)\) under which for all \(f\in\mathcal{V}\), all \(i\in[M]\), and \(\delta^{\prime}\) as in Algorithm 5:

\[\big{|}\widehat{\mathbf{\mathcal{P}}}_{h-1,\varepsilon,\delta^{\prime }}[\widehat{\mathcal{V}}_{h}^{(i)}](x_{h-1}^{(i)},a_{h-1}^{(i)})-\widehat{\mathbf{ \mathcal{P}}}_{h-1,\varepsilon,\delta^{\prime}}[f_{h}](x_{h-1}^{(i)},a_{h-1}^{ (i)})\big{|}\] \[\leq\big{|}P_{h-1}[\widehat{\mathcal{V}}_{h}^{(i)}-f_{h}](x_{h-1 }^{(i)},a_{h-1}^{(i)})\big{|}+\varepsilon\cdot\sqrt{2\log_{1/\delta^{\prime}} (4MAH|\mathcal{V}|(\ell_{h}^{(i)})^{2}/\delta)},\] \[=\big{|}\mathcal{P}_{h-1}[\widehat{\mathcal{V}}_{h}^{(i)}-f_{h}]( x_{h-1}^{(i)},a_{h-1}^{(i)})\big{|}+\varepsilon\cdot\beta(t_{h}^{(i)}), \tag{32}\]

where \(\beta(t_{h}^{(i)})\) is as in Algorithm 5. Thus, under \(\mathcal{E}_{h}^{\prime}\), the test in Line 14 fails for \(\ell=h\) at least \(M\) times only if

\[\forall i\in[M], \varepsilon<\sup_{f\in\widetilde{\mathcal{V}}_{h}^{(i)}}\big{|}( \widehat{\mathbf{\mathcal{P}}}_{h-1,\varepsilon,\delta^{\prime}}[\widehat{\mathcal{ V}}_{h}^{(i)}]-\widehat{\mathbf{\mathcal{P}}}_{h-1,\varepsilon,\delta^{\prime}}[f_{h}] \big{)}(x_{h-1}^{(i)},a_{h-1}^{(i)})\big{|}\] \[-\varepsilon\cdot\beta(t_{h}^{(i)}),\] \[\leq\sup_{f\in\widetilde{\mathcal{V}}_{h}^{(i)}}\big{|}\mathbb{E} \big{[}\widehat{\mathcal{V}}_{h}^{(i)}(\mathbf{x}_{h})-f_{h}(\mathbf{x}_{h})\mid\mathbf{x}_{h -1}=x_{h-1}^{(i)},\mathbf{a}_{h-1}=a_{h-1}^{(i)}\big{]}\big{|}\quad\text{ (by \eqref{eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:Unless \(N<M\), this is a contradiction to Eq. (31). We conclude that under the event \(\mathcal{E}_{h}\cap\mathcal{E}_{h}^{\prime}\), the test in Line 14 fails at most \(N<M=\lceil 8\varepsilon^{-1}C_{\text{push}}H\rceil\) times for \(\ell=h\), and so under \(\mathcal{E}_{1}\cap\mathcal{E}_{1}^{\prime}\cap\cdots\cap\mathcal{E}_{H}\cap \mathcal{E}_{H}\), we have

\[\forall h\in[H],\quad|\mathcal{C}_{h}|\leq\lceil 8\varepsilon^{-1}C_{\text{push} }H\rceil.\]

By the union bound, we have \(\mathbb{P}[\mathcal{E}_{1}\cap\mathcal{E}_{1}^{\prime}\cap\cdots\cap\mathcal{E}_ {H}\cap\mathcal{E}_{H}^{\prime}]\geq 1-\delta\), which completes the proof. 

### Proof of Lemma I.2 (Consequence of Passing the Tests)

Proof of Lemma I.2.: Let \(h\in[H]\) be given. Fix \(\ell\in[h+1\,..\,H]\) and let \(\mathbf{x}_{\ell-1}^{(1)},\mathbf{x}_{\ell-1}^{(2)},\dots\) denote the sequence of states used in the tests of Line 14 throughout the execution of \(\mathsf{RVFS}_{0}\); we assume that the sequence is _ordered_ in the sense that if \(i<j\), then \(\mathbf{x}_{\ell-1}^{(i)}\) is used in the test of Line 14 before \(\mathbf{x}_{\ell-1}^{(i)}\). Let \(\mathbf{T}_{\ell}\in\mathbb{N}\cup\{+\infty\}\) be the random variable representing the total number of times the operator \(\widehat{\mathbf{P}}_{\ell-1,\varepsilon,\delta^{\prime}}\) is invoked in Line 14 throughout the execution of \(\mathsf{RVFS}_{0}\) (\(\mathbf{T}_{\ell}\) is also the random length of the sequence \(\mathbf{x}_{\ell-1}^{(1)},\mathbf{x}_{\ell-1}^{(2)},\dots\); if \(\mathsf{RVFS}_{0}\) terminates, then \(\mathbf{T}_{\ell}\) is finite. The first step of the proof will be to show that under the event \(\mathcal{E}\) of Lemma I.1, \(\mathbf{T}_{\ell}\) is no larger than \(M^{3}N_{\text{test}}H^{3}\) at any point during the execution of \(\mathsf{RVFS}_{0}\). This will help us establish key concentration results, leading to the desired inequality (26).

Bounding \(\mathbf{T}_{\ell}\) under \(\mathcal{E}\).First, note that under the event \(\mathcal{E}\) of Lemma I.1, we have that for any \(\tau\in[H]\),

\[|\mathcal{C}_{\tau}|\leq M\coloneqq\lceil 8\varepsilon^{-1}C_{\text{push}}H\rceil, \tag{33}\]

and so \(\mathsf{RVFS}_{\tau}\) gets called at most \(M\) times throughout the execution of \(\mathsf{RVFS}_{0}\). For the rest of this paragraph, we condition on \(\mathcal{E}\) and fix \(\tau\in[0\,..\,H]\). Within any given call to \(\mathsf{RVFS}_{\tau}\) (throughout the execution of \(\mathsf{RVFS}_{0}\)), the operator \(\widehat{\mathbf{P}}_{\ell-1,\varepsilon,\delta^{\prime}}\) is invoked at most

\[\underbrace{|\mathcal{C}_{\tau}|N_{\text{test}}H}_{\text{Due to the for-loops in Line 8, Line 9, \& Line 10}}\times\underbrace{HM}_{\text{Number of times $\mathsf{RVFS}_{\tau}$ returns to Line 8 (see below)}}\leq M^{2}N_{\text{test}}H^{2}\]

times. This is because the for-loop in Line 8 of \(\mathsf{RVFS}_{\tau}\) resumes whenever a test in Line 14 fails for one of the layers \(\tau+1,\dots,H\) (see Line 18) once the recursive calls return, and the total number of test failures across all these layers is bounded by \(HM\) (by (33)). Now, since \(\mathsf{RVFS}_{\tau}\) gets called at most \(M\) times throughout the execution of \(\mathsf{RVFS}_{0}\) (as argued in the prequel), the total number of times the operator \(\widehat{\mathbf{P}}_{\ell-1,\varepsilon,\delta^{\prime}}\) is invoked in Line 14 within \(\mathsf{RVFS}_{\tau}\) is at most

\[M^{3}N_{\text{test}}H^{2}.\]

Finally, the total number of times the operator \(\widehat{\mathbf{P}}_{\ell-1,\varepsilon,\delta^{\prime}}\) is called in Line 14 throughout the execution of \(\mathsf{RVFS}_{0}\) is at most \(H\) times larger (accounting for the contributions from \(\mathsf{RVFS}_{\tau}\) for all \(\tau\in[H]\)); that is, it is at most \(M^{3}N_{\text{test}}H^{3}\). We conclude that the random variable \(\mathbf{T}_{\ell}\) satisfies

\[\mathbf{T}_{\ell}\leq M^{3}N_{\text{test}}H^{3} \tag{34}\]

under \(\mathcal{E}\).

Specifying \(\mathcal{E}_{h}^{\prime}\).In this paragraph, we no longer condition on \(\mathcal{E}\). We will specify the event \(\mathcal{E}_{h}^{\prime}\) in the lemma statement. Let \(\delta^{\prime}\) be defined as in Algorithm 5. By Lemma H.2, we have that there is an event \(\mathcal{E}_{h,\ell}^{\prime}\) of probability at least \(1-\delta/(2H^{2})\) under which:

\[\forall i\in[\mathbf{T}_{\ell}],\forall a_{\ell-1}\in\mathcal{A} :\sup_{f\in\widehat{\mathcal{V}}_{\ell}}|(\widehat{\mathbf{P}}_{\ell-1, \varepsilon,\delta^{\prime}}[\widehat{V}_{\ell}]-\widehat{\mathbf{P}}_{\ell-1, \varepsilon,\delta^{\prime}}[f_{\ell}])(\mathbf{x}_{\ell-1}^{(i)},a_{\ell-1})|- \varepsilon-\varepsilon\cdot\beta(\mathbf{T}_{\ell}) \tag{35}\] \[=\sup_{f\in\widehat{\mathcal{V}}_{\ell}}|(\widehat{\mathbf{P}}_{\ell-1,\varepsilon,\delta^{\prime}}[\widehat{V}_{\ell}]-\widehat{\mathbf{P}}_{\ell-1, \varepsilon,\delta^{\prime}}[f_{\ell}])(\mathbf{x}_{\ell-1}^{(i)},a_{\ell-1})|- \varepsilon-\varepsilon\cdot\sqrt{2\log_{1/\delta^{\prime}}(8AH^{2}M|\mathcal{ V}|\mathbf{T}_{\ell}^{2}/\delta)},\] \[\geq\sup_{f\in\widehat{\mathcal{V}}_{\ell}}|(\mathcal{P}_{\ell-1 }[\widehat{V}_{\ell}]-\mathcal{P}_{\ell-1}[f_{\ell}])(\mathbf{x}_{\ell-1}^{(i)},a_{ \ell-1})|-\varepsilon-\varepsilon\cdot\sqrt{2\log_{1/\delta^{\prime}}(8AH^{2}M |\mathcal{V}|\mathbf{T}_{\ell}^{2}/\delta)}\] \[\qquad-\varepsilon\cdot\sqrt{2\log_{1/\delta^{\prime}}(4AH^{2}M| \mathcal{V}|i^{2}/\delta)},\quad\text{(Lemma H.2)}\] \[\geq\sup_{f\in\widehat{\mathcal{V}}_{\ell}}|(\mathcal{P}_{\ell-1 }[\widehat{V}_{\ell}]-\mathcal{P}_{\ell-1}[f_{\ell}])(\mathbf{x}_{\ell-1}^{(i)},a_{ \ell-1})|-\varepsilon-2\varepsilon\cdot\sqrt{2\log_{1/\delta^{\prime}}(8AH^{2} M|\mathcal{V}|\mathbf{T}_{\ell}^{2}/\delta)}.\]

On the other hand, for \(k\in[\mathbf{T}_{\ell}-N_{\text{test}}+1]\), we have by Lemma C.4 (Freedman's inequality) instantiated with * \(n=N_{\text{test}}\) and \(\mathbf{y}_{i}=\mathbb{I}\big{\{}\sup_{f\in\mathcal{V}_{\ell}}\max_{a\in\mathcal{A}} \big{|}(\mathcal{P}_{\ell-1}[\widehat{V}_{\ell}]-\mathcal{P}_{\ell-1})[f_{\ell} ](\mathbf{x}_{\ell-1}^{(k+i)},a)\big{|}>3\varepsilon\big{\}}\), for all \(i\in[N_{\text{test}}]\);
* \(\mathcal{Q}=\{\mathrm{id}\}\);
* \(B=1\); and
* \(\lambda=1\);

that there is an event \(\mathcal{E}_{h,\ell,k}^{\prime\prime}\) of probability at least \(1-\delta/(4k^{2}H^{2})\) under which

\[\sum_{0\leq i<N_{\text{test}}}\mathbb{P}\!\left[\sup_{f\in \widehat{\mathcal{V}}_{\ell}}\max_{a\in\mathcal{A}}\big{|}(\mathcal{P}_{\ell-1 }[\widehat{V}_{\ell}]-\mathcal{P}_{\ell-1}[f_{\ell}])(\mathbf{x}_{\ell-1}^{(k+i)},a)\big{|}>3\varepsilon\right]\] \[\leq 4\log(8H^{2}\mathbf{T}_{\ell}^{2}/\delta)+\sum_{0\leq i<N_{\text{ test}}}\mathbb{I}\left\{\sup_{f\in\widehat{\mathcal{V}}_{\ell},a\in\mathcal{A}} \left|(\mathcal{P}_{\ell-1}[\widehat{V}_{\ell}]-\mathcal{P}_{\ell-1}[f_{\ell} ])(\mathbf{x}_{\ell-1}^{(k+i)},a)\right|>3\varepsilon\right\}.\]

Now, let \(\mathcal{E}_{h,\ell}^{\prime\prime}\coloneqq\bigcap_{k\in\{\mathbb{T}_{k}-N_{ \text{test}}+1\}}\mathcal{E}_{h,\ell,k}^{\prime\prime}\). By the union bound and the fact that \(\sum_{k\geq 1}1/k^{2}=\pi^{2}/6\leq 2\), we have that \(\mathbb{P}[\mathcal{E}_{h,\ell}^{\prime\prime}]\geq 1-\delta/(2H^{2})\). Furthermore, under \(\mathcal{E}_{h,\ell}^{\prime\prime}\), we have

\[\forall k\in[\mathbf{T}_{\ell}-N_{\text{test}}+1],\] \[\sum_{0\leq i<N_{\text{test}}}\mathbb{P}\!\left[\sup_{f\in \widehat{\mathcal{V}}_{\ell}}\max_{a\in\mathcal{A}}\big{|}(\mathcal{P}_{\ell- 1}[\widehat{V}_{\ell}]-\mathcal{P}_{\ell-1}[f_{\ell}])(\mathbf{x}_{\ell-1}^{(k+i)},a)\big{|}>3\varepsilon\right]\] \[\leq 4\log(8H^{2}\mathbf{T}_{\ell}^{2}/\delta)+\sum_{0\leq i<N_{\text {test}}}\mathbb{I}\left\{\sup_{f\in\widehat{\mathcal{V}}_{\ell},a\in\mathcal{ A}}|(\mathcal{P}_{\ell-1}[\widehat{V}_{\ell}]-\mathcal{P}_{\ell-1}[f_{\ell}])(\mathbf{x}_{ \ell-1}^{(k+i)},a)|>3\varepsilon\right\}. \tag{36}\]

We define \(\mathcal{E}_{h}^{\prime}\coloneqq\mathcal{E}_{h,1}^{\prime}\cap\mathcal{E}_{h, 1}^{\prime\prime}\cap\cdots\mathcal{E}_{h,H}^{\prime}\cap\mathcal{E}_{h,H}^{\prime\prime}\). Note that by the union bound, we have \(\mathbb{P}[\mathcal{E}_{h}^{\prime}]\geq 1-\frac{\delta}{H}\) as desired.

Termination of \(\mathsf{R\WFS}_{h}\) under \(\mathcal{E}\cap\mathcal{E}_{h}^{\prime}\).We now show that under \(\mathcal{E}\cap\mathcal{E}_{h}^{\prime}\), if \(\mathsf{R\WFS}_{h}\) terminates, its output satisfies (26). For the rest of the proof, we condition on \(\mathcal{E}\cap\mathcal{E}_{h}^{\prime}\). Suppose that \(\mathsf{R\WFS}_{h}\) terminates and returns \((\widehat{V}_{h:H},\widehat{V}_{h:H},\mathcal{C}_{h:H},\mathcal{B}_{h:H},t_{h: H})\). In this case, the value function \(\widehat{V}_{\ell}\) must have passed the tests in Line 14 for all \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\), \(n\in N_{\text{test}}\), and \(a_{\ell-1}\in\mathcal{A}\). Fix \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\) and let \(k\in[\mathbf{T}_{\ell}-N_{\text{test}}\cdot A+1]\) be such that \((\mathbf{x}_{\ell-1}^{k+j})_{j\in\{0..N_{\text{test}}-1\}}\) represents a subsequence of states that pass the tests in Line 14 at layer \(\ell\) for \((x_{h-1},a_{h-1})\) within the call to \(\mathsf{R\WFS}_{h}\). The fact that the sequence \((\mathbf{x}_{\ell-1}^{(i)})_{i\geq 1}\) is ordered (see definition in the first paragraph of this proof) and that \((\mathbf{x}_{\ell-1}^{k+j})_{j\in\{0..N_{\text{test}}-1\}}\) pass the tests imply that

1. The states \((\mathbf{x}_{\ell-1}^{(k+i)})_{i\in[0..N_{\text{test}}-1]}\) at layer \(\ell-1\) are i.i.d., and are obtained by rolling out with \(\widehat{\pi}_{h:H}\) starting from \((x_{h-1},a_{h-1})\); and
2. The test in Line 14 succeeds for all \((\mathbf{x}_{\ell-1}^{(k+j)})_{j\in\{0..N_{\text{test}}-1\}}\); that is \[\forall j\in[0..N_{\text{test}}-1],\forall a_{\ell-1}\in\mathcal{A}, \sup_{f\in\widehat{\mathcal{V}}_{\ell}}|(\widehat{\mathbf{P}}_{\ell-1, \varepsilon,\delta}[\widehat{V}_{\ell}]-\widehat{\mathbf{P}}_{\ell-1,\varepsilon, \delta}[f_{\ell}])(\mathbf{x}_{\ell-1}^{(k+j)},a_{\ell-1})|\] \[\leq\varepsilon+\varepsilon\cdot\beta(k+j),\] \[\leq\varepsilon+\varepsilon\cdot\sqrt{2\log_{1/\delta^{\prime}}(8 AM|\mathcal{V}|(k+j)^{2}/\delta)},\] \[\leq\varepsilon+\varepsilon\cdot\sqrt{2\log_{1/\delta^{\prime}}(8 AM|\mathcal{V}|\mathbf{T}_{\ell}^{2}/\delta)}.\]

[MISSING_PAGE_EMPTY:52]

where \(N_{\mathsf{reg}}\) is as in Line 5, and the last inequality follows by \(|\mathcal{C}_{h}|\leq k\) since we are considering the \(k\)th call to RVFS\({}_{h}\). Thus, under \(\mathcal{E}^{\prime\prime}_{h,k}\), we have

\[\forall(x_{h-1},a_{h-1})\in\mathcal{C}_{h},\forall(x_{h},v_{h}) \in\mathcal{D}_{h}(x_{h-1},a_{h-1}):\] \[|V_{h}^{\overline{\pi}}(x_{h})-v_{h}|=\big{|}V_{h}^{\overline{\pi }}(x_{h})-\widehat{V}_{h}(x_{h})\big{|}\leq H\sqrt{\frac{2\log(8N_{\mathsf{ reg}}Hk^{3}/\delta)}{N_{\mathsf{est}}(k)}}\leq\frac{H}{N_{\mathsf{reg}}}, \tag{40}\]

where the second-to-last inequality is by (39) and the last inequality follows by the choice of \(N_{\mathsf{est}}\) in Algorithm 5.

Bounding the discrepancy \(V_{h}^{\overline{\pi}}-V_{h}^{\pi}\).On the other hand, by the performance difference lemma, the value function \(V_{h}^{\overline{\pi}}\) satisfies:

\[\forall x\in\mathcal{X},\quad|V_{h}^{\overline{\pi}}(x)-V_{h}^{ \pi}(x)| \leq\sum_{\tau=h}^{H}\mathbb{E}^{\overline{\pi}}[|Q_{\tau}^{\pi} (\boldsymbol{x}_{\tau},\boldsymbol{\pi}_{\tau}(\boldsymbol{x}_{\tau}))-Q_{ \tau}^{\pi}(\boldsymbol{x}_{\tau},\overline{\boldsymbol{\pi}}_{\tau}( \boldsymbol{x}_{\tau}))|\mid\boldsymbol{x}_{h}=x],\] \[\leq H\sum_{\tau=h}^{H}\mathbb{E}^{\overline{\pi}}[D_{\mathrm{tv }}(\overline{\pi}_{\tau}(\boldsymbol{x}_{\tau}),\pi_{\tau}(\boldsymbol{x}_{ \tau}))\mid\boldsymbol{x}_{h}=x]. \tag{41}\]

Now, let \((x_{h-1}^{(1)},a_{h-1}^{(1)}),(x_{h-1}^{(2)},a_{h-1}^{(2)}),\dots\) denote the elements of \(\mathcal{C}_{h}\) in the order in which they are added to the latter in Line 15. By Lemma C.2 (Freedman's inequality) instantiated with

* \(n=N_{\mathsf{reg}}\cdot k\).
* \(\boldsymbol{w}_{i}=|V_{h}^{\pi}(\boldsymbol{x}_{h}^{(i)})-V_{h}^{\overline{ \pi}}(\boldsymbol{x}_{h}^{(i)})|-\mathbb{E}[|V_{h}^{\pi}(\boldsymbol{x}_{h})-V _{h}^{\overline{\pi}}(\boldsymbol{x}_{h})|\mid\boldsymbol{x}_{h-1}=x_{h-1}^{ (j)},\boldsymbol{a}_{h-1}=a_{h-1}^{(j)}]\), for all \(i\in[n]\) and \(j=[i/N_{\mathsf{reg}}]+1\), where \(\boldsymbol{x}_{h}^{(N_{\mathsf{reg}})},\dots,\boldsymbol{x}_{h}^{(N_{ \mathsf{reg}}+N_{\mathsf{reg}}-1)\overset{i.i.d.}{\sim}}\)\(T_{h}(\cdot\mid\boldsymbol{x}_{h-1}=x_{h-1}^{(j)},\boldsymbol{a}_{h-1}=a_{h-1}^{ (j)})\);
* \(\mathcal{H}_{i}=\sigma(\boldsymbol{x}_{h}^{(1)},\dots\boldsymbol{x}_{h}^{(i-1 )})\), for all \(i\in[n]\);
* \(R=H\); and
* \(\lambda=1/H\);

we get that there is an event \(\widetilde{\mathcal{E}}^{\prime\prime}_{h,k,\pi}(\mathcal{S}_{k})\) of probability at least \(1-\delta/(8k^{2}H|\Pi^{\prime}|)\) under which:

\[\sum_{(x_{h-1},a_{h-1})\in\mathcal{D}_{h}\;(x_{h-1})\prec\mathcal{ D}_{h}\;(x_{h-1},a_{h-1})}|V_{h}^{\pi}(x_{h})-V_{h}^{\overline{\pi}}(x_{h})|\] \[=2N_{\mathsf{reg}}\sum_{(x_{h-1},a_{h-1})\in\mathcal{D}_{h}\; \mathcal{E}}\mathbb{E}\big{[}|V_{h}^{\pi}(\boldsymbol{x}_{h})-V_{h}^{\overline {\pi}}(\boldsymbol{x}_{h})|\mid\boldsymbol{x}_{h-1}=x_{h-1},\boldsymbol{a}_{h -1}=a_{h-1}\big{]}+H\log(8k^{2}|\Pi^{\prime}|H/\delta),\] \[\leq 2HN_{\mathsf{reg}}\sum_{(x_{h-1},a_{h-1})\prec\mathcal{D}_{h} \;\sum_{\tau=h}^{H}\mathbb{E}^{\overline{\pi}}\left[D_{\mathrm{tv}}(\widehat{ \pi}_{\tau}(\boldsymbol{x}_{\tau}),\pi_{\tau}(\boldsymbol{x}_{\tau}))\mid \boldsymbol{x}_{h-1}=x_{h-1},\boldsymbol{a}_{h-1}=a_{h-1}\right]}+H\log(8k^{2 }|\Pi^{\prime}|H/\delta), \tag{42}\]

where the last inequality follows by (41) and the law of total expectation.

Regression guarantee.Since \(\pi\in\Pi^{\prime}\subseteq\Pi_{5}\) and Assumption I.1 holds, Lemma C.5 (regression guarantee) instantiated with

* \(f_{*}(x)=V_{h}^{\pi}(x)\);
* \(B=H\);
* \(\boldsymbol{b}_{i}=\boldsymbol{v}_{h}-V_{h}^{\pi}(\boldsymbol{x}_{h})\) (where \(\boldsymbol{v}_{h}\coloneqq\max_{a\in\mathcal{A}}\widetilde{Q}_{h}( \boldsymbol{x}_{h},a)\)); and
* \(\xi=H\);implies that there is an event \(\tilde{\mathcal{E}}^{\prime\prime}_{h,k,\pi}(\mathcal{S}_{k})\) of probability at least \(1-\delta/(4k^{2}H|\Pi^{\prime}|)\) under which we have:

\[\sum_{(x_{h-1},a_{h-1})\in\mathcal{C}_{h}}\frac{1}{N_{\text{reg}}} \sum_{(x_{h},-)\in\mathcal{D}_{h}(x_{h-1},a_{h-1})}\big{(}\widehat{V}_{h}(x_{h} )-V_{h}^{\pi}(x_{h})\big{)}^{2}\] \[\leq\frac{4kH^{2}\log(4k^{2}H|\Pi^{\prime}|\mathcal{V}|/\delta)}{N _{\text{reg}}}+\frac{4H}{N_{\text{reg}}}\sum_{(x_{h-1},a_{h-1})\in\mathcal{C}_ {h}}\sum_{(x_{h},v_{h})\in\mathcal{D}_{h}(x_{h-1},a_{h-1})}|V_{h}^{\pi}(x_{h})- v_{h}|,\] \[\leq\frac{4kH^{2}\log(4k^{2}H|\Pi^{\prime}|\mathcal{V}|/\delta)}{N _{\text{reg}}}+\frac{4H}{N_{\text{reg}}}\sum_{(x_{h-1},a_{h-1})\in\mathcal{C}_ {h}}\sum_{(x_{h},v_{h})\in\mathcal{D}_{h}(x_{h-1},a_{h-1})}|V_{h}^{\pi}(x_{h})- v_{h}|\] \[\quad+\frac{4H}{N_{\text{reg}}}\sum_{(x_{h-1},a_{h-1})\in\mathcal{ C}_{h}}\sum_{(x_{h},v_{h})\in\mathcal{D}_{h}(x_{h-1},a_{h-1})}|V_{h}^{\pi}(x_{h} )-V_{h}^{\pi}(x_{h})|, \tag{43}\]

where the last step follows by the triangle inequality. Thus, by plugging (42) and (40) into (43), we get that under \(\mathcal{E}^{\prime\prime}_{h,k}(\mathcal{S}_{k})\cap\tilde{\mathcal{E}}^{ \prime\prime}_{h,k,\pi}(\mathcal{S}_{k})\cap\tilde{\mathcal{E}}^{\prime\prime} _{h,k,\pi}(\mathcal{S}_{k})\):

\[\sum_{(x_{h-1},a_{h-1})\in\mathcal{C}_{h}}\frac{1}{N_{\text{reg} }}\sum_{(x_{h},-)\in\mathcal{D}_{h}(x_{h-1},a_{h-1})}\big{(}\widehat{V}_{h}(x_ {h})-V_{h}^{\pi}(x_{h})\big{)}^{2}\] \[\leq\frac{9kH^{2}\log(8k^{2}H|\Pi^{\prime}|\mathcal{V}|/\delta)}{ N_{\text{reg}}}+8H^{2}\sum_{(x_{h-1},a_{h-1})\in\mathcal{C}_{h}}\sum_{\tau =h}^{H}\mathbb{E}^{\pi}\left[D_{\text{tv}}(\widehat{\pi}_{\tau}(\mathbf{x}_{\tau} ),\pi_{\tau}(\mathbf{x}_{\tau}))\mid\mathbf{x}_{h-1}=x_{h-1},\mathbf{a}_{h-1}=a_{h-1}\right]. \tag{44}\]

Applying the union bound to conclude.Let \(\mathbf{\mathcal{S}}_{k}\) be the random state of \(\mathsf{RVFS}_{0}\) during the \(k^{\text{th}}\) call to \(\mathsf{RVFS}_{h}\) and immediately before Line 20, i.e. immediately before gathering data for the regression step in \(\mathsf{RVFS}_{h}\). Further, let \(\mathbf{\mathcal{S}}_{k}^{+}\) be the random state of \(\mathsf{RVFS}_{0}\) during the \(k^{\text{th}}\) call to \(\mathsf{RVFS}_{h}\) and immediately before Line 26, i.e. immediately before the regression step in \(\mathsf{RVFS}_{h}\). If \(\mathsf{RVFS}_{0}\) terminates before the \(k^{\text{th}}\) call to \(\mathsf{RVFS}_{h}\), we use the convention that \(\mathbf{\mathcal{S}}_{k}=\mathbf{\mathcal{S}}_{k}^{+}=\mathsf{t}\), where \(\mathsf{t}\) denotes a terminal state, and define \(\mathcal{E}^{\prime\prime}_{h,k}(\mathsf{t})=\tilde{\mathcal{E}}^{\prime\prime }_{h,k,\pi}(\mathsf{t})=\tilde{\mathcal{E}}^{\prime\prime}_{h,k,\pi}(\mathsf{ t})=\{\mathsf{t}\}\). Further, we define

\[\mathcal{E}^{\prime\prime}_{h}:=\left\{\prod_{k\in\mathbb{N},\pi\in\Pi^{\prime }}\mathbb{I}(\mathbf{\mathcal{S}}_{k}^{+}\in\mathcal{E}^{\prime\prime}_{h,k}(\bm {\mathcal{S}}_{k})\cap\tilde{\mathcal{E}}^{\prime\prime}_{h,k,\pi}(\mathbf{ \mathcal{S}}_{k})\cap\tilde{\mathcal{E}}^{\prime\prime}_{h,k,\pi}(\mathbf{ \mathcal{S}}_{k})\}=1\right\}.\]

Note that by the argument in the sequel and the union bound, we have that

\[\forall k\geq 1,\forall\mathcal{S}_{k},\quad\mathbb{P}[\mathbf{\mathcal{S}}_{k}^{ +}\in\mathcal{E}^{\prime\prime}_{h,k}(\mathcal{S}_{k})\cap\tilde{\mathcal{E}} ^{\prime\prime}_{h,k,\pi}(\mathcal{S}_{k})\cap\tilde{\mathcal{E}}^{\prime\prime }_{h,k,\pi}(\mathcal{S}_{k})]\geq 1-\frac{\delta}{2k^{2}H}, \tag{45}\]

where \(\mathcal{S}_{k}\) denotes the state of \(\mathsf{RVFS}_{0}\) during the \(k^{\text{th}}\) call to \(\mathsf{RVFS}_{h}\) and immediately before Line 20. By letting \(\mathbf{\mathcal{S}}_{1}^{\prime},\mathbf{\mathcal{S}}_{2}^{\prime},\ldots\) denote an identical, independent copy of the sequence \(\mathbf{\mathcal{S}}_{1},\mathbf{\mathcal{S}}_{2},\ldots\), we have by the chain rule:

\[\mathbb{P}[\mathcal{E}^{\prime\prime}_{h}] =\mathbb{E}_{\mathbf{\mathcal{S}}_{1}^{\prime},\mathbf{\mathcal{S}}_{2}^{ \prime},\ldots}\left[\prod_{k\geq 1}\mathbb{P}[\mathbf{\mathcal{S}}_{k}^{+}\in\mathcal{E}^{ \prime\prime}_{h,k}(\mathbf{\mathcal{S}}_{k})\cap\tilde{\mathcal{E}}^{\prime\prime }_{h,k,\pi}(\mathbf{\mathcal{S}}_{k})\cap\tilde{\mathcal{E}}^{\prime\prime}_{h,k, \pi}(\mathbf{\mathcal{S}}_{k})\mid\mathbf{\mathcal{S}}_{k}=\mathbf{\mathcal{S}}_{k}^{\prime} ]\right],\] \[\geq\prod_{k\geq 1}\left(1-\frac{\delta}{2k^{2}H}\right),\quad \text{(by \eqref{eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq

**Lemma I.5**.: _Let \(h\in[0\,..\,H]\) be given, and consider the setting of Lemma I.4. Further, consider a call to \(\mathsf{RVF}\mathsf{S}_{0}(f,\mathcal{V},\varnothing,\varnothing;\mathcal{V}, \varepsilon,\delta)\) that terminates, and let \(h\in[H]\) be any layer such that \(\mathsf{RVF}\mathsf{S}_{h}\) is called during the execution of \(\mathsf{RVF}\mathsf{S}_{0}\). Then, after the last call to \(\mathsf{RVF}\mathsf{S}_{h}\) terminates, no instance of \(\mathsf{RVF}\mathsf{S}\) in \((\mathsf{RVF}\mathsf{S}_{\tau})_{\tau>h}\) is called before \(\mathsf{RVF}\mathsf{S}_{0}\) terminates._

Proof of Lemma I.5.: Suppose there is an instance of \(\mathsf{RVF}\mathsf{S}\) in \((\mathsf{RVF}\mathsf{S}_{\tau})_{\tau>h}\) that is called after the last call to \(\mathsf{RVF}\mathsf{S}_{h}\) terminates. Let \(\tau>h\) be the lowest layer where \(\mathsf{RVF}\mathsf{S}_{\tau}\) is called after the last call to \(\mathsf{RVF}\mathsf{S}_{h}\) terminates. Let \(\mathsf{RVF}\mathsf{S}_{\tau}^{\mathrm{last}}\) denote the corresponding instance of \(\mathsf{RVF}\mathsf{S}_{\tau}\). Further, let \(\ell<\tau\) be such that \(\mathsf{RVF}\mathsf{S}_{\ell}\) is the parent instance of \(\mathsf{RVF}\mathsf{S}_{\tau}^{\mathrm{last}}\) (i.e. the instance that called \(\mathsf{RVF}\mathsf{S}_{\tau}^{\mathrm{last}}\)). Note that we cannot have \(\ell=h\) as this would imply that an instance of \(\mathsf{RVF}\mathsf{S}_{h}\) terminates after \(\mathsf{RVF}\mathsf{S}_{\tau}^{\mathrm{last}}\), and we have assumed that \(\mathsf{RVF}\mathsf{S}_{\tau}^{\mathrm{last}}\) terminates after the last call \(\mathsf{RVF}\mathsf{S}_{h}\). It is also not possible to have \(\ell>h\) as this would imply that \(\tau\) is not the lowest layer where \(\mathsf{RVF}\mathsf{S}_{\tau}\) is called after the last call to \(\mathsf{RVF}\mathsf{S}_{h}\) terminates. Thus, we must have that \(\ell<h\). Now, the for-loop in Line 16 ensures that that there is an instance of \(\mathsf{RVF}\mathsf{S}_{h}\) that is called after \(\mathsf{RVF}\mathsf{S}_{\tau}^{\mathrm{last}}\) terminates and before \(\mathsf{RVF}\mathsf{S}_{\ell}\) does. This contradicts the assumption that \(\mathsf{RVF}\mathsf{S}_{\tau}^{\mathrm{last}}\) is called after the last call to \(\mathsf{RVF}\mathsf{S}_{h}\). 

Proof of Lemma I.4.: We start by showing that \(\tilde{\pi}\in\Pi_{4\varepsilon}\) by constructing the corresponding collection of random state-action value functions \(\big{(}\widetilde{\mathbf{Q}}_{h}(x,a)\big{)}_{(h,x,a)\in[H]\times\mathcal{X} \times\mathcal{A}}\subset[0,H]\) in the definition of \(\Pi_{4\varepsilon}\). In particular, for \((h,x,a)\in[H]\times\mathcal{X}\times\mathcal{A}\), we define

\[\widetilde{\mathbf{Q}}_{h}(x,a)=\left\{\begin{array}{ll}\widetilde{\mathbf{Q}}_{h}( x,a),&\text{if }|\widetilde{\mathbf{Q}}_{h}(x,\cdot)-\mathcal{P}_{h}[V_{h+1}^{\tilde{\pi}}](x, \cdot)|_{\infty}\leq 4\varepsilon,\\ \mathcal{P}_{h}[V_{h+1}^{\tilde{\pi}}](x,a),&\text{otherwise},\end{array} \right.\quad\text{for }h=H,\ldots,1.\]

where \(\widetilde{\mathbf{Q}}_{\tau}(x,a)\coloneqq\widetilde{\mathbf{P}}_{\tau,\varepsilon, \delta^{\prime}}[\widetilde{V}_{\tau+1}](x,a)\). Note that \(\widetilde{\mathbf{Q}}_{h}(x,a)\) only depends on the randomness of \(\widetilde{\mathbf{P}}_{h,\varepsilon,\delta^{\prime}}[\widetilde{V}_{h+1}](x,a)\), and so \((\widetilde{\mathbf{Q}}_{h}(x,a))_{(h,x,a)\in[H]\times\mathcal{X}\times\mathcal{ A}}\) are independent random variables. Furthermore, since \(\mathcal{P}_{h}[V_{h+1}^{\tilde{\pi}}]\equiv Q_{h}^{\tilde{\pi}}\), we have that

\[\forall(x,a)\in\mathcal{X}\times\mathcal{A},\quad\|\widetilde{\mathbf{Q}}_{h}(x,a )-Q_{h}^{\tilde{\pi}}(x,a)\|\leq 4\varepsilon.\]

Finally, since \(\tilde{\pi}_{h}(\cdot)\in\arg\max_{a\in\mathcal{A}}\widetilde{\mathbf{Q}}_{h}( \cdot)\), we have that \(\tilde{\pi}\in\Pi_{4\varepsilon}\).

We show \(V_{h}^{\tilde{\pi}}\in\widehat{\mathcal{V}}_{h}\). We prove that \(V_{h}^{\tilde{\pi}}\in\widehat{\mathcal{V}}_{h}\), for all \(h\in[H]\), under the event \(\mathcal{E}^{\prime\prime\prime}\coloneqq\mathcal{E}\cap\mathcal{E}_{1}^{ \prime}\cap\mathcal{E}_{1}^{\prime\prime}\cap\cdots\cap\)\(\mathcal{E}_{H}^{\prime}\cap\mathcal{E}_{H}^{\prime\prime}\), where \(\mathcal{E}\), \((\mathcal{E}_{h}^{\prime})\), and \((\mathcal{E}_{h}^{\prime\prime})\) are the events defined in Lemma I.1, Lemma I.2, and Lemma I.3, respectively. Throughout, we condition on \(\mathcal{E}^{\prime\prime\prime}\). First, note that by Lemma I.1, \(\mathsf{RVF}\mathsf{S}_{0}\) terminates. Let \((\widehat{V}_{1:H},\widehat{\mathcal{V}}_{1:H},\mathcal{C}_{1:H},\mathcal{B}_{1 :H},t_{1:H})\) be the tuple it returns.

We will show via backwards induction over \(\ell=H+1,\ldots,1\), that

\[V_{\ell}^{\tilde{\pi}}\in\widehat{\mathcal{V}}_{\ell}, \tag{47}\]

where \(\tilde{\pi}_{1:H}\) is the stochastic policy defined recursively via

\[\tilde{\pi}_{\tau}(x)\in\arg\max_{a\in\mathcal{A}}\left\{\begin{array}{ll} \widetilde{\mathbf{Q}}_{\tau}(x,a)\coloneqq\widetilde{\mathbf{P}}_{\tau,\varepsilon, \delta^{\prime}}[\widehat{V}_{\tau+1}](x,a),&\text{if }|\widetilde{\mathbf{Q}}_{\tau}(x, \cdot)-\mathcal{P}_{\tau}[V_{\tau+1}^{\tilde{\pi}}](x,\cdot)|_{\infty}\leq 4 \varepsilon,\\ \mathcal{P}_{\tau}[V_{\tau+1}^{\tilde{\pi}}](x,a),&\text{otherwise},\end{array} \right.\quad\text{for }\tau=H,\ldots,1,\]

where \(\widetilde{\mathbf{Q}}_{\tau}(x,a)\coloneqq\widetilde{\mathbf{P}}_{\tau,\varepsilon, \delta^{\prime}}[\widehat{V}_{\tau+1}](x,a)\).

Base case [\(\ell=H+1\)]This holds trivially because \(V_{H+1}^{\pi}\equiv 0\) for any \(\pi\in\Pi_{5}\) by convention.

General case [\(\ell\leq H\)]Fix \(h\in[H]\) and suppose that (47) holds for all \(\ell\in[h+1\,..\,H+1]\). We show as a consequence that (47) holds for \(\ell=h\). First, note that if \(\mathsf{RVF}\mathsf{S}_{h}\) is never called during the execution of \(\mathsf{RVF}\mathsf{S}_{0}\), then \(\widehat{\mathcal{V}}_{h}=\mathcal{V}\), and so (47) trivially holds for \(\ell=h\) under Assumption I.1 with \(\varepsilon_{\mathrm{real}}=4\varepsilon\).

Now, suppose that \(\mathsf{RVF}\mathsf{S}_{h}\) is called at least once, and let \(\big{(}\widehat{V}_{h:H}^{*},\widehat{\mathcal{V}}_{h:H}^{*},\mathcal{C}_{h:H} ^{*},\mathcal{B}_{h:H}^{*},t_{h:H}^{+}\big{)}\) be the output of the last call to \(\mathsf{RVF}\mathsf{S}_{h}\) during the execution of \(\mathsf{RVF}\mathsf{S}_{0}\). We claim that

\[\big{(}\widehat{V}_{h:H}^{*},\widehat{V}_{h:H}^{*},\mathcal{C}_{h:H}^{*}\big{)} =(\widehat{V}_{h:H},\widehat{\mathcal{V}}_{h:H},\mathcal{C}_{h:H}). \tag{48}\]

To see this, first note that the for-loop in Line 16 ensures that no instance of \((\mathsf{RVF}\mathsf{S}_{\tau})_{\tau>h}\) can be called after the last call to \(\mathsf{RVF}\mathsf{S}_{h}\) (by Lemma I.5). Thus, the estimated value functions, confidence sets, and core sets for layers \(h+1,\ldots,H\) remain unchanged after the last call to \(\mathsf{RVF}\mathsf{S}_{h}\); that is, (48) holds.

Thus, by Lemma I.2, and since we are conditioning on \(\mathcal{E}^{\prime}_{h+1:H}\), we have that for all \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\) and \(\ell\in[h+1\,..\,H+1]\):

\[\mathbb{P}^{\vec{\pi}}\left[\sup_{f\in\widehat{\mathcal{V}}_{\ell}}\max_{a\in \mathcal{A}}\big{|}(\mathcal{P}_{\ell-1}[\widehat{\mathcal{V}}_{\ell}]- \mathcal{P}_{\ell-1}[f_{\ell}])(\mathbf{x}_{\ell-1},a)\big{|}>3\varepsilon\mid\bm {x}_{h-1}=x_{h-1},\mathbf{a}_{h-1}=a_{h-1}\right]\leq\frac{4\log(8M^{6}N_{\text{ test}}^{2}H^{8}/\delta)}{N_{\text{test}}}, \tag{49}\]

where \(M=\lceil 8\varepsilon^{-1}C_{\text{push}}H\rceil\). Now, by the induction hypothesis, we have \(V_{\ell}^{\vec{\pi}}\in\widehat{\mathcal{V}}_{\ell}\), and so substituting \(V_{\ell}^{\vec{\pi}}\) for \(f_{\ell}\) in (49), we get that for all \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\) and \(\ell\in[h+1\,..\,H+1]\):

\[\mathbb{P}^{\vec{\pi}}\Big{[}\max_{a\in\mathcal{A}}\big{|}(\mathcal{P}_{\ell-1} [\widehat{\mathcal{V}}_{\ell}]-\mathcal{P}_{\ell-1}[V_{\ell}^{\vec{\pi}}])(\bm {x}_{\ell-1},a)\big{|}>3\varepsilon\mid\mathbf{x}_{h-1}=x_{h-1},\mathbf{a}_{h-1}=a_{h -1}\Big{]}\leq\frac{4\log(8M^{6}N_{\text{test}}^{2}H^{8}/\delta)}{N_{\text{ test}}}.\]

Therefore, by Lemma I.1 (instantiated with \(\mu[\cdot]=\mathbb{P}^{\vec{\pi}}[\cdot\mid\mathbf{x}_{h-1}=x_{h-1},\mathbf{a}_{h-1}=a_{h-1}]\), \(\tau=\ell-1\), and \(V_{\tau+1}=V_{\ell}^{\vec{\pi}}\)), we have that \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\) and \(\ell\in[h+1\,..\,H+1]\):

\[\mathbb{E}^{\vec{\pi}}\big{[}D_{\text{tv}}(\widehat{\pi}_{\ell-1}(\mathbf{x}_{\ell- 1}),\tilde{\pi}_{\ell-1}(\mathbf{x}_{\ell-1}))\mid\mathbf{x}_{h-1}=x_{h-1},\mathbf{a}_{h-1 }=a_{h-1}\big{]}\leq\frac{4\log(8M^{6}N_{\text{test}}^{2}H^{8}/\delta)}{N_{ \text{test}}}+\delta^{\prime}, \tag{50}\]

where \(\delta^{\prime}\) is as in Algorithm 5.

Applying the regression guarantee to conclude the induction.Note that \(\tilde{\pi}\in\Pi^{\prime}\), where \(\Pi^{\prime}\subset\Pi_{5}\) is the set of stochastic policies such that \(\pi\in\Pi^{\prime}\) if and only if there exists \(V_{1:H}\in\mathcal{V}\) such that \(\pi\) is defined recursively as

\[\mathbf{\pi}_{\tau}(x)\in\operatorname*{arg\,max}_{a\in\mathcal{A}}\left\{\begin{array}[ ]{ll}\mathbf{Q}_{\tau}(x,a)\coloneqq\widehat{\mathbf{\mathcal{P}}}_{\tau,\varepsilon, \delta^{\prime}}[V_{\tau+1}](x,a),&\text{if }\|\mathbf{Q}_{\tau}(x,\cdot)-\mathcal{P}_{\tau}[V_{ \tau+1}^{\pi}](x,\cdot)\|_{\infty}\leq 4\varepsilon,\\ \mathcal{P}_{\tau}[V_{\tau+1}^{\pi}](x,a),&\text{otherwise},\end{array}\right.\]

for \(\tau=H,\ldots,1\), where \(\mathbf{Q}_{\tau}(x,a)=\widehat{\mathbf{\mathcal{P}}}_{\tau,\varepsilon,\delta^{ \prime}}[V_{\tau+1}](x,a)\). The policy class \(\Pi^{\prime}\) is finite and \(|\Pi^{\prime}|\leq|\mathcal{V}|\). Furthermore, we have \(\Pi^{\prime}\subseteq\Pi_{4\varepsilon}\) as shown at the beginning of this proof. Therefore, if we let \(\{\mathcal{D}_{h}(x,a):(x,a)\in\mathcal{C}_{h}\}\) be the datasets in the definition of \(\widehat{\mathcal{V}}_{h}\) in (15), we have by Lemma I.3 (under Assumption I.1 with \(\varepsilon_{\text{real}}=4\varepsilon\)) and the conditioning on \(\mathcal{E}^{\prime}_{h+1:H}\) and \(\mathcal{E}\):

\[\sum_{(x_{h-1},a_{h-1})\in\mathcal{C}_{h}}\frac{1}{N_{\text{reg}}} \sum_{(x_{h},\rightharpoonup)\in\mathcal{D}_{h}(x_{h-1},a_{h-1})}\big{(} \widehat{V}_{h}(x_{h})-V_{h}^{\tilde{\pi}}(x_{h})\big{)}^{2}\] \[\leq\frac{9[\mathcal{C}_{h}]H^{2}\log(8[\mathcal{C}_{h}]H|\mathcal{ V}|^{2}/\delta)}{N_{\text{reg}}}+8H^{2}\sum_{(x_{h-1},a_{h-1})\in\mathcal{C}_{h}} \sum_{\tau=h}^{H}\mathbb{E}^{\vec{\pi}}\left[D_{\text{tv}}(\widehat{\pi}_{\tau} (\mathbf{x}_{\tau}),\tilde{\pi}_{\tau}(\mathbf{x}_{\tau}))\mid\mathbf{x}_{h-1}=x_{h-1},\mathbf{a }_{h-1}a_{h-1}\right], \tag{51}\]

where the last inequality follows by the fact that \(|\mathcal{C}_{h}|\leq M\) under \(\mathcal{E}\). Combining (51) with (50), implies that

\[\sum_{(x_{h-1},a_{h-1})\in\mathcal{C}_{h}}\frac{1}{N_{\text{reg}}} \sum_{(x_{h},\rightharpoonup)\in\mathcal{D}_{h}(x_{h-1},a_{h-1})}\big{(} \widehat{V}_{h}(x_{h})-V_{h}^{\tilde{\pi}}(x_{h})\big{)}^{2}\] \[\leq\frac{9MH^{2}\log(8M^{2}H|\mathcal{V}|^{2}/\delta)}{N_{\text{ reg}}}+8MH^{3}\cdot\frac{4\log(8M^{6}N_{\text{test}}^{2}H^{8}/\delta)}{N_{\text{ test}}}+8MH^{3}\delta^{\prime},\] \[=\frac{9MH^{2}\log(8M^{2}H|\mathcal{V}|^{2}/\delta)}{N_{\text{ reg}}}+8MH^{3}\cdot\frac{4\log(8M^{6}N_{\text{test}}^{2}H^{8}/\delta)}{N_{\text{ test}}}+8MH^{3}\frac{\delta}{4M^{7}N_{\text{test}}^{2}H^{8}|\mathcal{V}|}, \tag{52}\]

where the last inequality follows by the fact that \(\delta\in(0,1)\) and the definition of \(\varepsilon_{\text{reg}}^{2}\) in Algorithm 5. By the definition of \(\widehat{\mathcal{V}}_{h}\) in (15), (52) implies that \(V_{h}^{\tilde{\pi}}\in\widehat{\mathcal{V}}_{h}\), which completes the induction.

### Proof of Theorem I.1 (Main Guarantee of RVFS)

**Proof of Theorem I.1.** We condition on the event \(\widetilde{\mathcal{E}}\coloneqq\mathcal{E}\cap\mathcal{E}^{\prime\prime\prime} \cap\mathcal{E}^{\prime}_{1}\cap\cdots\cap\mathcal{E}^{\prime}_{H}\), where \(\mathcal{E}\), \(\mathcal{E}^{\prime\prime\prime}\), and \((\mathcal{E}^{\prime}_{h})\) are the events in Lemma I.1, Lemma I.4, and Lemma I.2, respectively. Note that by the union bound, we have \(\mathbb{P}[\widetilde{\mathcal{E}}]\geq 1\) - \(5\delta\). By Lemma I.2, we have that

\[\forall h\in[H],\quad\mathbb{P}^{\pi}\!\!\left[\sup_{f\in\widetilde{\mathcal{D }}_{h}}\max_{a\in\mathcal{A}}\big{|}(\mathcal{P}_{h-1}[\widetilde{V}_{h}]- \mathcal{P}_{h-1}[f_{h}])(\mathbf{x}_{h-1},a)\big{|}>3\varepsilon\right]\leq\frac{4 \log(8M^{6}N_{\text{test}}^{2}H^{8}/\delta)}{N_{\text{test}}}, \tag{53}\]

where \(M=\lceil 8\varepsilon^{-1}C_{\text{push}}H\rceil\) and \(N_{\text{test}}=2^{8}M^{2}H\varepsilon^{-1}\log(8M^{6}H^{8}\varepsilon^{-2} \delta^{-1})\). On the other hand, by Lemma I.4, we have

\[\forall h\in[H],\quad V_{h}^{\pi}\in\widetilde{\mathcal{V}}_{h}.\]

Thus, substituting \(V_{h}^{\pi}\) for \(f_{h}\) in (53) we get that for all \(h\in[H+1]\).

\[\mathbb{P}^{\pi}\!\!\left[\max_{a\in\mathcal{A}}\big{|}(\mathcal{P}_{h-1}[ \widetilde{V}_{h}]-\mathcal{P}_{h-1}[V_{h}^{\tilde{\pi}}])(\mathbf{x}_{h-1},a) \big{|}>3\varepsilon\right]\leq\frac{4\log(8M^{6}N_{\text{test}}^{2}H^{8}/ \delta)}{N_{\text{test}}}.\]

This together with Lemma I.1, instantiated with \(\mu[\cdot]=\mathbb{P}^{\pi}[\cdot]\); \(\tau=h-1\); \(V_{\tau+1}=V_{h}^{\tilde{\pi}}\); and \(\delta=\delta^{\prime}\) (with \(\delta^{\prime}\) as in Algorithm 5), translates to:

\[\forall h\in[H],\quad\mathbb{E}^{\widetilde{\pi}}[D_{\text{tv}} (\widetilde{\pi}_{h}(\mathbf{x}_{h}),\tilde{\pi}_{h}(\mathbf{x}_{h}))] \leq\frac{4\log(8M^{6}N_{\text{test}}^{2}H^{8}/\delta)}{N_{ \text{test}}}+\delta^{\prime},\] \[=\frac{4\log(8M^{6}N_{\text{test}}^{2}H^{8}/\delta)}{N_{\text{ test}}}+\frac{\delta}{4M^{7}N_{\text{test}}^{2}H^{8}|\mathcal{V}|},\] \[\leq\frac{\varepsilon}{4H^{3}C_{\text{push}}},\]

where the last step follows from the fact that \(N_{\text{test}}=2^{8}M^{2}H\varepsilon^{-1}\log(8M^{6}H^{8}\varepsilon^{-2} \delta^{-1})\) (with \(M\) as in Line 3).

Bounding the sample complexity.We now bound the number of episodes used by Algorithm 5 under the event \(\widetilde{\mathcal{E}}\). First, we fix \(h\in[H]\), and focus on the number of episodes used within a to call RVFS\({}_{h}\), excluding any episodes used by any subsequent calls to RVFS\({}_{\tau}\) for \(\tau>h\). We start by counting the number of episodes used to test the fit of the estimated value functions \(\widetilde{V}_{h+1:H}\). Starting from Line 8, there are for-loops over \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\), \(\ell=H,\ldots,h+1\), and \(n\in[N_{\text{test}}]\) to collect partial episodes using the learned policy \(\widetilde{\pi}\) in Algorithm 5, where \(N_{\text{test}}=2^{8}M^{2}H\varepsilon^{-1}\log(8M^{6}H^{8}\varepsilon^{-2} \delta^{-1})\) and \(M=\lceil 8\varepsilon^{-1}C_{\text{push}}H\rceil\). Note that executing \(\widetilde{\pi}\) requires the local simulator and uses \(N_{\text{sim}}=2\log(4M^{7}N_{\text{test}}^{2}H^{2}|\mathcal{V}|/\delta)/ \varepsilon^{2}\) local simulator queries to output an action at each layer (since Algorithm 5 calls Algorithm 7 with confidence level \(\delta^{\prime}=\delta/(8M^{7}N_{\text{test}}^{2}H^{8}|\mathcal{V}|)\)). Also, note that whenever a test fails in Line 14 and the recursive RVFS calls return, the for-loop in Line 8 resumes. We also know (by Lemma I.1) that the number of times the test fails in Line 14 is at most \(M\). Thus, the number of times the for-loop in Line 8 resumes is bounded by \(HM\); here, \(H\) accounts for possible test failures across all layers \(\tau\in[h+1\mathinner{.\,.}H]\). Thus, the total sample complexity required to generate episodes between lines Line 8 and Line 11 is bounded by

\[\text{\# episodes for roll-outs}\leq\underbrace{MH}_{\text{\# of times Line 8 resumes}}\cdot\underbrace{MH^{2}N_{\text{test}}N_{\text{sim}}}_{\text{Sample complexity in case of no test failures}}. \tag{54}\]

Note that the test in Line 14 also uses episodes because it calls the operator \(\widetilde{\mathbf{\mathcal{P}}}\) for every \(a\in\mathcal{A}\). Thus, the number of episodes used for the test in Line 14 is bounded by

\[\text{\# episodes for the tests}\leq\underbrace{MH}_{\text{\# of times Line 8 resumes}}\cdot\underbrace{MHAN_{\text{test}}N_{\text{sim}}}_{\text{Sample complexity for Line 14}}. \tag{55}\]

We now count the number of episodes used to re-fit the value function in Line 16 and onwards. Note that starting from Line 16, there are for-loops over \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\) and \(i\in[N_{\text{reg}}]\) to generate \(A\cdot N_{\text{est}}(|\mathcal{C}_{h}|)\leq A\cdot N_{\text{est}}(M)\) partial episodes using \(\widetilde{\pi}\), where \(N_{\text{est}}(k)=2N_{\text{reg}}^{2}\log(8AN_{\text{reg}}Hk^{3}/\delta)\)is defined as in Algorithm 5. Since \(\widetilde{\pi}\) uses the local simulator and requires \(N_{\mathsf{est}}\) samples (see Algorithm 7) to output an action at each layer, the number of episodes used to refit the value function is bounded by

\[\text{\# episodes for $V$-refitting}\leq MN_{\mathsf{reg}}AN_{\mathsf{est}}(M)HN_{ \mathsf{sin}}. \tag{56}\]

Therefore, by (54), (55), and (56), the number of episodes used within a single call to RVFS\({}_{h}\) (not accounting for episodes used by recursive calls to RVFS\({}_{\tau}\), for \(\tau>h\)) is bounded by

\[\text{\# episodes used locally within}\,\texttt{RVFS}_{h}\leq M^{2}H(H+A)N_{\mathsf{ test}}N_{\mathsf{sin}}+MN_{\mathsf{reg}}AN_{\mathsf{est}}(M)HN_{\mathsf{sin}}. \tag{57}\]

Finally, by Lemma I.1, RVFS\({}_{h}\) may be called at most \(M\) times throughout the execution of RVFS\({}_{0}\). Using this together with (57) and accounting for the number of episodes from all layers \(h\in[H]\), we get that the total number of episodes is bounded by

\[M^{3}H^{2}(H+A)N_{\mathsf{test}}N_{\mathsf{sin}}+M^{2}H^{2}N_{\mathsf{reg}}AN_ {\mathsf{est}}(M)N_{\mathsf{sin}}.\]

Substituting the expressions of \(M\), \(N_{\mathsf{test}}\), \(N_{\mathsf{est}}\), \(N_{\mathsf{sin}}\), and \(N_{\mathsf{reg}}\) from Algorithm 5 and Algorithm 7, we obtain the desired number of episodes, which concludes the proof. 

### Proof of Theorem I.2 (Guarantee of RVFS.bc)

**Proof of Theorem I.2.** Let \(\widetilde{V}_{1:H}\) be the value function estimates produced by RVFS\({}_{0}\) within Algorithm 6, and let \(\widetilde{\pi}_{h}^{\texttt{RVFS}}(\cdot)\in\arg\max_{a\in\mathcal{A}} \widetilde{\boldsymbol{\mathcal{P}}}_{h,\varepsilon_{\mathsf{RwFS}},\delta^{ \prime}}[\widetilde{V}_{h+1}](\cdot,a)\), for all \(h\in[H]\) with \(\widetilde{V}_{H+1}\equiv 0\) with \(\varepsilon_{\texttt{RwFS}}\) and \(\delta^{\prime}\) as in Algorithm 6. Further, let and let \(\widetilde{\pi}_{1:H}\in\Pi_{5}\) be the stochastic policy defined recursively via

\[\forall x\in\mathcal{X},\ \ \widetilde{\pi}_{\tau}(x)\in\arg\max_{a\in\mathcal{A}} \left\{\begin{array}{ll}\widetilde{\boldsymbol{Q}}_{\tau}(x,a),&\text{if } \|\widetilde{\boldsymbol{Q}}_{\tau}(x,\cdot)-\mathcal{P}_{\tau}[V_{\tau+1}^{ \widetilde{\pi}}](x,\cdot)\|_{\infty}\leq 4\varepsilon_{\texttt{RwFS}},\\ \mathcal{P}_{\tau}[V_{\tau+1}^{\widetilde{\pi}}](x,a),&\text{otherwise},\end{array}\right. \tag{58}\]

for \(\tau=H,\ldots,1\), where \(\widetilde{\boldsymbol{Q}}_{\tau}(x,a)=\widetilde{\boldsymbol{\mathcal{P}}}_ {\tau,\varepsilon_{\mathsf{RwFS}},\delta^{\prime}}[\widetilde{V}_{\tau+1}](x,a)\). By Theorem I.1, there is an event \(\widetilde{\mathcal{E}}\) of probability at least \(1-\delta/2\) under which:

\[\widetilde{\pi}\in\Pi_{4\varepsilon_{\mathsf{RwFS}}}, \tag{59}\]

and

\[\forall h\in[H],\quad\mathbb{E}^{\widetilde{\pi}^{\mathsf{RwFS}}}\big{[}D_{ \mathrm{tv}}(\widetilde{\pi}_{h}^{\texttt{RVFS}}(\boldsymbol{x}_{h}),\widetilde {\pi}_{h}(\boldsymbol{x}_{h}))\big{]}\leq\frac{\varepsilon_{\texttt{RVFS}}}{4H ^{3}C_{\mathsf{push}}}\leq\frac{\varepsilon}{4H^{2}}, \tag{60}\]

where the last inequality follows by the choice of \(\varepsilon_{\texttt{RVFS}}\) in Algorithm 6.

For the rest of the proof, we condition on \(\widetilde{\mathcal{E}}\). By (59), (63), and Proposition M.1 instantiated with \(\varepsilon_{\mathsf{mis}}=0\) (due to all \(\pi\)-realizability), we have that there is an event \(\widetilde{\mathcal{E}}^{\prime}\) of probability at least \(1-\delta/2\) under which the policy \(\widetilde{\pi}_{1:H}\) produced by BehaviorCloning ensures that

\[J(\widetilde{\pi}_{1:H}^{\texttt{RVFS}})-J(\widetilde{\pi}_{1:H})\leq\frac{ \varepsilon}{2}. \tag{61}\]

Now, by Lemma C.6 (the performance difference lemma), we have for \(\widetilde{\pi}\) as in (58):

\[J(\widetilde{\pi})-J(\widetilde{\pi}_{1:H}^{\texttt{RVFS}}) =\sum_{h=1}^{H}\mathbb{E}^{\widetilde{\pi}^{\mathsf{RwFS}}}[Q_{h} ^{\widetilde{\pi}}(\boldsymbol{x}_{h},\widetilde{\pi}_{h}(\boldsymbol{x}_{h}) )-Q_{h}^{\widetilde{\pi}}(\boldsymbol{x}_{h},\widetilde{\boldsymbol{\pi}}_{h}^{ \texttt{RVFS}}(\boldsymbol{x}_{h}))],\] \[\leq H\sum_{h=1}^{H}\mathbb{E}^{\widetilde{\pi}^{\mathsf{RwFS}}} [D_{\mathrm{tv}}(\widetilde{\pi}_{h}^{\texttt{RVFS}}(\boldsymbol{x}_{h}), \widetilde{\pi}_{h}(\boldsymbol{x}_{h}))],\]

and so by (63), we have

\[J(\widetilde{\pi})-J(\widetilde{\pi}_{1:H}^{\texttt{RVFS}})\leq\varepsilon/4. \tag{62}\]

Finally, since \(\widetilde{\pi}\in\Pi_{4\varepsilon_{\mathsf{RwFS}}}\) (see (59)), we have by Lemma H.1,

\[J(\pi^{*})-J(\widetilde{\pi})\leq 12H\varepsilon_{\texttt{RVFS}}\leq\varepsilon/4,\]

where the last inequality follows by the choice \(\varepsilon_{\texttt{RVFS}}\) in Algorithm 6. Combining this with (61) and (62), we conclude that under \(\widetilde{\mathcal{E}}\cap\widetilde{\mathcal{E}}^{\prime}\):

\[J(\pi^{*})-J(\widetilde{\pi}_{1:H})\leq\varepsilon.\]

By the union bound, we have \(\mathbb{P}[\widetilde{\mathcal{E}}\cap\widetilde{\mathcal{E}}^{\prime}]\geq 1-\delta\), and so the desired suboptimality guarantee in (28) holds with probability at least \(1-\delta\).

Bounding the sample complexity.The sample complexity is dominated by the call to RVFS\({}_{0}\) within RVFS.bc (Algorithm 6). Since RVFS.bc calls RVFS\({}_{0}\) with \(\varepsilon=\varepsilon_{\textsc{Rwfs}}=\varepsilon H^{-1}/48\), we conclude from Theorem I.1 that the total sample complexity is bounded by

\[\widetilde{\mathcal{O}}\left(C_{\mathrm{push}}^{8}H^{23}A\cdot\varepsilon^{-13} \right).\]

## Appendix J Guarantee under \(V^{*}\)-Realizability (Proof of Theorem 4.1, Setup I)

In this section, we prove Theorem 4.1 under **Setup I** (\(V^{*}/\pi^{*}\)-realizability (Assumptions 4.2 and 4.3) and \(\Delta\)-gap (Assumption 4.4)). We prove this result as a consequence of the more general results (Theorem I.2) in Appendix I by appealing to the relaxed \(V^{\pi}\)-realizability condition in Assumption I.1.

### Analysis: Proof of Theorem 4.1 (Setup I)

We begin by showing that Assumption 4.2 and Assumption 4.4 together imply that Assumption I.1 holds for any \(\varepsilon_{\mathrm{real}}\leq\Delta/2\); we prove this by showing that the benchmark policy class \(\Pi_{\varepsilon^{\prime}}\) (Appendix H.2) reduces to \(\{\pi^{*}\}\) when \(\varepsilon^{\prime}\leq\Delta/2\).

**Lemma J.1**.: _Assume that \(\mathcal{V}\) satisfies Assumption 4.2 (\(V^{*}\)-realizability), and that Assumption 4.4 (gap) holds with \(\Delta>0\). Then, for all \(\varepsilon^{\prime}\leq\Delta/2\), we have \(\Pi_{\varepsilon^{\prime}}=\{\pi^{*}\}\) and \(\mathcal{V}\) satisfies Assumption I.1 with \(\varepsilon_{\mathrm{real}}=\varepsilon^{\prime}\)._

Informally Lemma J.1, whose proof is in Appendix J.2, states that under Assumption 4.2, Assumption 4.4 with \(\Delta>0\), and Assumption 4.1 (pushforward coverability) with \(C_{\mathrm{push}}>0\), we are essentially in the setting of Theorem I.1 (guarantee of RVFS under relaxed \(V^{\pi}\)-realizability), as long as we choose \(\varepsilon_{\mathrm{real}}\leq\Delta/2\). With this, we now state and prove a central guarantee for RVFS under \(V^{\star}\)-realizability with a gap.

**Lemma J.2** (Intermediate guarantee for RVFS under **Setup I)**.: _Let \(\delta\in(0,1)\) be given, and suppose that:_

* _Assumption_ 4.1 _(pushforward coverability) holds with parameter_ \(C_{\mathrm{push}}>0\)_;_
* _Assumption_ 4.4 _(gap) holds with parameter_ \(\Delta>0\)_;_
* _The function class_ \(\mathcal{V}\) _satisfies Assumption_ 4.2 (\(V^{*}\)-realizability)._

_Then, for any \(f\in\mathcal{V}\) and \(\varepsilon\in(0,\Delta/8)\), with probability at least \(1-\delta\), \(\mathsf{RVFS}_{0}(f,\mathcal{V},\varnothing,\varnothing;\mathcal{V},\varepsilon,\delta)\) (Algorithm 5) terminates and returns value functions \(\widetilde{V}_{1:H}\) that satisfy_

\[\forall h\in[H],\quad\mathbb{P}^{\mathbb{F}}[\widetilde{\pi}_{h}(\mathbf{x}_{h}) \neq\pi_{h}^{*}(\mathbf{x}_{h})]\leq\frac{\varepsilon}{4C_{\mathrm{push}}H^{3}},\]

_where \(\widetilde{\pi}_{h}(x)\in\arg\max_{a\in\mathcal{A}}\widetilde{\mathbf{\mathcal{P}} }_{\tau,\varepsilon,\delta^{\prime}}[\widetilde{V}_{h+1}](x,a)\), for all \(h\in[H]\), with \(\delta^{\prime}\) is defined as in Algorithm 5._

**Proof of Lemma J.2**.: From Lemma J.1, we have that \(\Pi_{4:e}=\{\pi^{*}\}\), and so Theorem I.1 implies that with probability at least \(1-\delta\),

\[\forall h\in[H],\quad\frac{\varepsilon}{4C_{\mathrm{push}}H^{3}}\geq\mathbb{ E}^{\mathbb{F}}[D_{\mathrm{tv}}(\widetilde{\pi}_{h}(\mathbf{x}_{h}),\pi_{h}^{*}(\mathbf{x} _{h})]=\mathbb{P}^{\mathbb{F}}[\widetilde{\pi}_{h}(\mathbf{x}_{h})\neq\pi_{h}^{*}( \mathbf{x}_{h})],\]

where the equality follows by the fact that \(\pi^{*}\) is deterministic. 

From here, Theorem 4.1 follows swiftly as a consequence.

**Proof of Theorem 4.1 (Setup I).** Let \(\widetilde{V}_{1:H}\) be the value function estimates produced by RVFS\({}_{0}\) within Algorithm 6, and let \(\widetilde{\pi}_{h}^{\textsc{Rwfs}}(\cdot)\in\arg\max_{a\in\mathcal{A}} \widetilde{\mathbf{\mathcal{P}}}_{h,\varepsilon_{\textsc{Rwfs}},\delta^{\prime}}[ \widetilde{V}_{h+1}](\cdot,a)\), for all \(h\in[H]\) with \(\widetilde{V}_{H+1}\equiv 0\) with \(\varepsilon_{\textsc{Rwfs}}\) and \(\delta^{\prime}\) as in Algorithm 6. By Lemma J.2, there is an event \(\widetilde{\mathcal{E}}\) of probability at least \(1-\delta/2\) under which:

\[\mathbb{P}^{\mathbb{F}}[\widetilde{\pi}_{h}(\mathbf{x}_{h})\neq\pi_{h}^{*}(\mathbf{x} _{h})]\leq\frac{\varepsilon_{\textsc{Rwfs}}}{4H^{3}C_{\mathrm{push}}}\leq\frac {\varepsilon}{4H^{2}}, \tag{63}\]

where the last inequality follows by the choice of \(\varepsilon_{\textsc{Rwfs}}\) in Algorithm 6.

[MISSING_PAGE_FAIL:60]

almost surely. Using this, we have, almost surely

\[Q_{h}^{\star}(x,\mathbf{\pi}_{h}(x)) \geq\widetilde{\mathbf{Q}}_{h}(x,\mathbf{\pi}_{h}(x))-\varepsilon^{\prime},\] \[\geq\widetilde{\mathbf{Q}}_{h}(x,\pi_{h}^{\star}(x))-\varepsilon^{\prime},\] \[\geq Q_{h}^{\star}(x,\pi_{h}^{\star}(x))-2\varepsilon^{\prime}=Q_{ h}^{\star}(x,\pi_{h}^{\star}(x))-\Delta. \tag{66}\]

On the other hand, if \(\mathbf{\pi}_{h}(x)\neq\pi_{h}^{\star}(x)\), then

\[Q_{h}^{\star}(x,\mathbf{\pi}_{h}(x))<Q_{h}^{\star}(x,\pi_{h}^{\star}(x))-\Delta,\]

which would contradict (66). Thus, \(\mathbf{\pi}_{h}(x)=\pi_{h}^{\star}(x)\), which concludes the induction and shows that \(\pi\equiv\pi^{\star}\). We conclude that \(\Pi_{\varepsilon^{\prime}}=\{\pi^{\star}\}\). 

## Appendix K Guarantee for Weakly Correlated ExBMDPs (Proof of Theorem b.1)

In this section, we prove Theorem B.1, the main guarantee for \(\mathsf{RVFS}^{\mathsf{exo}}\). First, in Appendix K.1 we state a number of supporting technical lemmas and use them to prove Theorem B.1. The remainder of the section (Appendix K.2 through Appendix K.6) contains the proofs for the intermediate results.

### Analysis: Proof of Theorem b.1

Recall that the the \(V^{\pi}\)-realizability assumption required by \(\mathsf{RVFS}\) for Theorem 4.1 is not satisfied in ExBMDPs, as the value functions for policies that act on the exogenous noise variables cannot be realized as a function of the true decoder \(\phi^{\star}\). In \(\mathsf{RVFS}^{\mathsf{exo}}\), we address this issue by applying the randomized rounding technique in Line 11 to the learned value functions. The crux of the analysis will be to show that for an appropriate choice of the rounding parameters \(\zeta_{1:H}\), the policies produced by \(\mathsf{RVFS}^{\mathsf{exo}}\) are endogenous in the sense that we can write \(\pi(x)=\pi(\phi^{\star}(x))\) for all \(x\in\mathcal{X}\). This will allow us to leverage the decoder realizability (Assumption 3.3), which implies that the function class \(\mathcal{V}=\mathcal{V}_{1:H}\) given by

\[\mathcal{V}_{h}=\{x\mapsto f(\phi(x)):f\in[0,H]^{S},\phi\in\Phi\}, \tag{67}\]

satisfies \(V^{\pi}\)-realizability for all endogenous policies \(\pi\).

In what follows, we first motivate the randomized rounding approach in \(\mathsf{RVFS}^{\mathsf{exo}}\) in detail and prove that it succeeds, then use this result to proceed with an analysis similar to that of Theorem 4.1 (**Setup II**), re-using many of the technical tools developed for Theorem 4.1.

#### k.1.1 Randomized Rounding for Endogeneity

Naively, to ensure that the policies we execute are endogenous, it would seem that we require knowledge of the true decoder \(\phi^{\star}\). Alas, knowing \(\phi^{\star}\) trivializes the ExBMDP problem by reducing it to the tabular setting. To avoid requiring knowledge of \(\phi^{\star}\), we apply a _randomized rounding_ to the policies learned by \(\mathsf{RVFS}^{\mathsf{exo}}\) to ensure their endogeneity.

Let \(\varepsilon>0\) be fixed going forward. Recall that compared to \(\mathsf{RVFS}\), \(\mathsf{RVFS}^{\mathsf{exo}}\) (Algorithm 8) takes an additional input \(\zeta_{1:H}\subset(0,1/2)\) and executes the following coarsened policies:

\[\widetilde{\mathbf{\pi}}_{h}(\cdot)\in\operatorname*{arg\,max}_{a\in\mathcal{A}} \llbracket\widetilde{\mathbf{\mathcal{P}}}_{h,\varepsilon,\delta}\llbracket \widetilde{V}_{h+1}\rrbracket(\cdot,a)/\varepsilon+\zeta_{h}\rrbracket.\]

The _rounding parameters_\(\zeta_{1:H}\), which can be thought of as an offset, are chosen randomly; this will be elucidated in the sequel.

Following a similar analysis to Appendix I (**Setup II**), we can associate a near-optimal benchmark policy \(\tilde{\pi}\in\Pi_{2\varepsilon}\) with \(\tilde{\pi}\) in order to emulate certain properties of the \(\Delta\)-gap assumption. In particular, generalizing the construction in Eq. (27), we define a near-optimal benchmark policy \(\tilde{\pi}\) recursively via:

\[\forall x\in\mathcal{X},\ \tilde{\mathbf{\pi}}_{\tau}(x;\zeta_{1:H},\varepsilon, \delta)\in\operatorname*{arg\,max}_{a\in\mathcal{A}}\left\{\begin{array}{ll} \llbracket\widetilde{\mathbf{Q}}_{\tau}(x,a)/\varepsilon+\zeta_{\tau}\rceil,& \text{if }\|\widetilde{\mathbf{Q}}_{\tau}(x,\cdot)-\mathcal{P}_{\tau}[V_{\tau+1}^{ \tilde{\pi}}](x,\cdot)\|_{\infty}\leq 4\varepsilon^{2},\\ \llbracket\mathcal{P}_{\tau}[V_{\tau+1}^{\tilde{\pi}}](x,a)/\varepsilon+ \zeta_{\tau}\rceil,&\text{otherwise},\end{array}\right. \tag{68}\]

for \(\tau=H,\ldots,1\), where \(\widetilde{\mathbf{Q}}_{\tau}(\cdot,a)\coloneqq\widetilde{\mathbf{\mathcal{P}}}_{ \tau,\varepsilon,\delta}\llbracket\widetilde{V}_{\tau+1}\rrbracket(\cdot,a)\).

Naively, to use the benchmark policy \(\tilde{\pi}\) within the analysis based on relaxed \(V^{\pi}\)-realizability (Assumption I.1) in Appendix I, we would require the function class \(\mathcal{V}\) to realize \((V^{\tilde{\pi}}_{h})\). However, as argued earlier, this is not feasible unless \(\tilde{\pi}\) is an endogenous policy. Fortunately, it turns out that if \(\zeta_{1:H}\) (the additional input to \(\mathsf{RVF5}^{\mathsf{exo}}\)) are drawn randomly from uniform distribution over \([0,1/2]\), then with constant probability, \(\tilde{\pi}\) is indeed endogenous. What's more, under such an event, and for all possible choices of \((\widehat{V}_{h})\) in (68) uniformly, \(\tilde{\pi}\) "snaps" onto the stochastic endogenous policy \(\bar{\pi}(\cdot;\zeta_{1:H},\varepsilon)\) defined recursively as follows:

\[\bar{\pi}_{h}\big{(}\cdot;\zeta_{1:H},\varepsilon\big{)}\in \operatorname*{arg\,max}_{a\in\mathcal{A}}\!\!\left[\mathcal{P}_{h}[V^{\bar{ \pi}}_{h+1}](x,a)/\varepsilon+\zeta_{h}\right], \tag{69}\]

for \(h=H,\ldots,1\). Informally, this happens because, as long as \(\zeta_{1:H}\subset\big{(}0,1/2\big{)}\) avoid certain pathological locations in \(\big{(}0,1/2\big{)}\), the coarsened state-action value functions \(\varepsilon\cdot[\mathcal{P}_{\tau}[V^{\bar{\pi}}_{\tau+1}](x,a)/\varepsilon +\zeta_{h}\,]\) defining \(\bar{\pi}\) exhibit a "gap" of order \(\Theta(\varepsilon^{2})\) separating optimal actions from the rest. This "snapping" behavior is analogous to what happens in **Setup I** with \(V^{\star}\)-realizability and \(\Delta\)-gap, where \(\Pi_{\varepsilon}\) reduces to \(\{\pi^{\star}\}\) for all \(\varepsilon<\Delta/2\) (see Lemma J.1). We formalize these claims in the next two lemmas. We start by showing that \(\bar{\pi}\) is endogenous and that \(\bar{\pi}\in\Pi_{2\varepsilon}\). The proof is in Appendix K.2.

**Lemma K.1** (Endogenous Benchmark policies).: _For any \(\delta\in(0,1)\), \(\varepsilon\in(0,1/2)\), and \(\zeta_{1:H}\subset\big{(}0,1/2\big{)}\), the stochastic policy \(\bar{\pi}(\cdot;\zeta_{1:H},\varepsilon)\) defined in Eq.69 is endogenous, and we have \(\bar{\pi}(\cdot;\zeta_{1:H},\varepsilon)\in\Pi_{2\varepsilon}\)._

Next, we show that \(\bar{\pi}\) "snaps" onto \(\bar{\pi}\) for the certain choices of \(\zeta_{1:H}\). The proof is in Appendix K.3.

**Lemma K.2** (Snapping probability).: _Let \(\delta\in(0,1)\), \(\varepsilon\in(0,1/2)\) be given, and \(\mathbb{P}^{\zeta}\) denote the probability law of \(\zeta_{1},\ldots,\zeta_{H}\sim\operatorname*{unif}\!\big{(}[0,1/2]\big{)}\). Then, there is an event \(\mathcal{E}_{\mathsf{rand}}\) of probability at least \(1-24SAH\varepsilon\) under \(\boldsymbol{\zeta}_{1:H}\sim\mathbb{P}^{\zeta}\) such that for all \(\widehat{V}\in\big{(}\mathcal{X}\times[H]\to[0,H]\big{)}\) simultaneously,_

\[\forall h\in[H],\quad\tilde{\pi}_{h}(\cdot;\widehat{V},\boldsymbol{\zeta}_{1 :H},\varepsilon,\delta)=\bar{\pi}_{h}(\cdot;\zeta_{1:H},\varepsilon),\]

_where \(\tilde{\pi}_{h}(\cdot;\widehat{V},\boldsymbol{\zeta}_{1:H},\varepsilon,\delta)\) is defined as in (68) with \(\widehat{V}=\widetilde{V}\), and \(\bar{\pi}\) is defined as in (69)._

The lemma together, with Lemma K.1, implies that with constant probability under \(\mathbb{P}^{\zeta}\), the benchmark policies \((\tilde{\pi}_{h})\) used in the analysis of \(\mathsf{RVF5}^{\mathsf{exo}}\) are endogenous and satisfy \(\tilde{\pi}\in\Pi_{2\varepsilon}\).

#### k.1.2 Pushforward Coverability

In order to proceed with the analysis strategy in Appendix I, we need to verify that pushforward coverability is satisfied for ExBMDPs under the weak correlation assumption. We do so in the next lemma; see Appendix K.4 for a proof.

**Lemma K.3** (Pushforward coverability).: _A weakly correlated ExBMDP with constant \(C_{\mathsf{exo}}\) (see AssumptionB.1) satisfies \(C_{\mathsf{push}}\)-pushforward coverability (Assumption4.1) with \(C_{\mathsf{push}}=C_{\mathsf{exo}}\cdot SA\), where \(S\in\mathbb{N}\) is the number of endogenous states._

Equipped with the preceding lemmas, we proceed with an analysis similar to the approach for Theorem4.1 (**Setup II**) in Appendix I. In what follows, we state a number of technical lemmas that apply the relevant results from Appendix I to the ExBMDP setting we consider here.

#### k.1.3 Bounding the Number of Test Failures

Since the size of the core sets \(\mathcal{C}_{1:H}\) in \(\mathsf{RVF5}^{\mathsf{exo}}\) is directly proportional to the number of test failures, the next result, which bounds \(|\mathcal{C}_{h}|\) for all \(h\in[H]\), allows us to show that \(\mathsf{RVF5}^{\mathsf{exo}}\) (Algorithm 8) terminates in a polynomial number of iterations.

**Lemma K.4** (Bounding the number of test failures).: _Let \(\delta,\varepsilon\in(0,1)\) and \(\zeta_{1:H}\in[0,1/2]\) be given, and suppose that AssumptionB.1 (weak correlation) holds with \(C_{\mathsf{exo}}>0\). Let \(f\in\mathcal{V}\), be given, where \(\mathcal{V}\) is an arbitrary function class. Then, there is an event \(\mathcal{E}\) of probability at least \(1-\delta\) under which the call to \(\mathsf{RVF5}^{\mathsf{exo}}_{0}(f,\mathcal{V}^{H},\varnothing,\varnothing,0; \mathcal{V},\varepsilon,\zeta_{1:H},\delta)\) (Algorithm 8) terminates, and throughout the execution of \(\mathsf{RVF5}^{\mathsf{exo}}_{0}\), we have_

\[|\mathcal{C}_{h}|\leq\lceil 8\varepsilon^{-2}C_{\mathsf{exo}}SAH\rceil.\]

Proof of Lemmak.4.: The results follows from Lemma K.3 and Lemma I.1.

#### k.1.4 Value Function Regression Guarantee

We next give a guarantee for the estimated value functions \(\widehat{V}_{1:H}\) computed within \(\mathsf{RVF5}^{\mathsf{exo}}\) in Line 26 of Algorithm 8.

**Lemma K.5** (Value function regression guarantee).: _Let \(h\in[0\dots H]\), \(\delta,\varepsilon\in(0,1)\), and \(\zeta_{1:H}\in[0,1/2]\) be given, and consider a call to \(\mathsf{RVF5}^{\mathsf{exo}}_{0}\) in the setting of Lemma K.4. Further, let \(\mathcal{V}\) be defined as in Eq. (67), and assume that \(\Phi\) satisfies Assumption 3.3. Then, for any endogenous policy \(\pi\) in \(\Pi_{\mathsf{S}}\), there is an event \(\mathcal{E}^{\prime\prime}_{h}\) of probability at least \(1-\delta/H\) under which for any \(k\geq 1\), if_

* \(\mathsf{RVF5}^{\mathsf{exo}}_{h}\) _gets called for the_ \(k\)_th time during the execution of_ \(\mathsf{RVF5}^{\mathsf{exo}}_{0}\)_; and_
* _this_ \(k\)_th call terminates and returns_ \((\widehat{V}_{h:H},\widehat{\mathcal{V}}_{h:H},\mathcal{C}_{h:H},\mathcal{B}_{ h:H},t_{h:H})\)_,_

_then if \((\widehat{\pi}_{\tau})_{\tau\geq h}\) is the policy induced by \(\widehat{V}_{h:H}\) and \(N_{\mathsf{reg}}\) is set as in Algorithm 8, we have_

\[\sum_{(x_{h-1},a_{h-1})\in\mathcal{C}_{h}}\frac{1}{N_{\mathsf{reg} }}\sum_{(x_{h},\cdot)\in\mathcal{D}_{h}(x_{h-1},a_{h-1})}\big{(}\widehat{V}_{h }(x_{h})-V^{\pi}_{h}(x_{h})\big{)}^{2}\] \[\leq\frac{9kH^{2}\log(8k^{2}H|\mathcal{V}|/\delta)}{N_{\mathsf{reg }}}+8H^{2}\sum_{(x_{h-1},a_{h-1})\in\mathcal{C}_{h}}\sum_{\tau=h}^{H}\mathbb{E}^ {\pi}\left[D_{\mathsf{tv}}(\widehat{\pi}_{\tau}(\boldsymbol{x}_{\tau}),\pi_{ \tau}(\boldsymbol{x}_{\tau}))\mid\boldsymbol{x}_{h-1}=x_{h-1},\boldsymbol{a}_{ h-1}=a_{h-1}\right],\]

_where the datasets \(\{\mathcal{D}_{h}(x,a):(x,a)\in\mathcal{C}_{h}\}\) are as in the definition of \(\widehat{\mathcal{V}}_{h}\) in (16)._

Proof of Lemma k.5.: Since \(\Phi\) satisfies Assumption 3.3, the function class \(\mathcal{V}=\mathcal{V}_{1:H}\) satisfies \(V^{\pi}\)-realizability for all endogenous policies \(\pi\) (see Lemma D.1). Thus, the proof of Lemma K.5 follows from that of Lemma I.3 (see Appendix I.4). 

#### k.1.5 Confidence Sets

We now state a version of the confidence set validity lemma (Lemma I.4) that supports the ExBMDP setting.

**Lemma K.6** (Confidence sets).: _Let \(\varepsilon\in(0,1/2)\) and \(\zeta_{1:H}\subset[0,1/2]\) be given, and suppose that_

* _Assumption_ B.1 _holds with_ \(C_{\mathsf{exo}}>0\)_;_
* _The decoder class_ \(\Phi\) _satisfies Assumption_ 3.3_;_
* \(\zeta_{1:H}\in\mathcal{E}_{\mathsf{rand}}\)_, where_ \(\mathcal{E}_{\mathsf{rand}}\) _is the event in Lemma_ K.2_._

_Let \(f\in\mathcal{V}\) be arbitrary. There is an event \(\mathcal{E}^{\prime\prime\prime}\) of probability at least \(1-3\delta\) under which a call to \(\mathsf{RVF5}^{\mathsf{exo}}_{0}(f,\mathcal{V},\varnothing,\varnothing,0; \mathcal{V},\varepsilon,\zeta_{1:H},\delta)\) terminates and returns tuple \((\widehat{V}_{1:H},\widehat{\mathcal{V}}_{1:H},\mathcal{C}_{1:H},\mathcal{B}_{ 1:H},t_{1:H})\) such that_

\[\forall h\in[H],\quad V^{\pi}_{h}\in\widehat{\mathcal{V}}_{h},\]

_where \(\bar{\pi}_{1:H}\) is the policy defined recursively via_

\[\bar{\pi}_{\tau}(x)\in\operatorname*{arg\,max}_{a\in\mathcal{A}}[\mathcal{P}_ {\tau}[V^{\bar{\pi}}_{\tau+1}](x,a)/\varepsilon+\zeta_{h}],\quad\text{for } \tau=H,\dots,1. \tag{70}\]

While the proof of this lemma is very similar to that of Lemma I.4, we need a dedicated treatment to handle the rounding in \(\mathsf{RVF5}^{\mathsf{exo}}\). The fully proof of Lemma K.8 is in Appendix I.5.

#### k.1.6 Main Guarantee for \(\mathsf{RVF5}^{\mathsf{exo}}\)

We now state the central technical guarantee for \(\mathsf{RVF5}^{\mathsf{exo}}\), Lemma K.7, which shows that the base invocation of the algorithm returns a set of value functions \(\widehat{V}_{1:H}\) that induce a near-optimal policy \(\widehat{\pi}\), as long as the randomized rounding parameters \(\zeta_{1:H}\) satisfy \(\zeta_{1:H}\in\mathcal{E}_{\mathsf{rand}}\), where \(\mathcal{E}_{\mathsf{rand}}\) is the success event in Lemma K.2. The proof of the theorem is in Appendix K.6.

**Lemma K.7** (Main guarantee for \(\mathsf{RVF5}^{\mathsf{exo}}\)).: _Let \(\delta,\varepsilon\in(0,1)\) and \(\zeta_{1:H}\subset[0,\frac{1}{2}]\) be given, and suppose that_

* _Assumption_ B.1 _holds with_ \(C_{\mathsf{exo}}>0\)_;_
* _The decoder class_ \(\Phi\) _satisfies Assumption_ 3.3_;_
* \(\zeta_{1:H}\in\mathcal{E}_{\mathsf{rand}}\)_, where_ \(\mathcal{E}_{\mathsf{rand}}\) _is the event in Lemma_ K.2_.__Then, for any \(f\in\mathcal{V}\), with probability at least \(1-5\delta\), a call to \(\mathsf{RVFS}^{\mathsf{exo}}_{0}(f,\mathcal{V}^{H},\varnothing,\varnothing,0; \mathcal{V},\varepsilon,\zeta_{1:H},\delta)\) (Algorithm 8) terminates and returns value functions \(\widehat{\mathcal{V}}_{1:H}\) such that_

\[\forall h\in[H],\quad\mathbb{P}^{\mathbb{F}}\big{[}\widehat{\pi}_{h}(\mathbf{x}_{h })\neq\bar{\pi}_{h}(\mathbf{x}_{h})\big{]}\leq\frac{\varepsilon^{2}}{4H^{3}SAC_{ \mathsf{exo}}},\]

_where \(\widehat{\pi}_{h}(x)\in\arg\max_{a\in\mathcal{A}}[\widehat{\mathbf{\mathcal{P}}}_{h,\varepsilon,\delta^{\prime}}[\widehat{\mathcal{V}}_{h+1}](x,a)/\varepsilon+ \zeta_{h}]\), for all \(h\in[H]\), \(\bar{\pi}\) is defined as in Eq. (69), and \(\delta^{\prime}\) is defined as in Algorithm 8. Furthermore, the number of episodes used by \(\mathsf{RVFS}^{\mathsf{exo}}_{0}\) is bounded by_

\[\widetilde{\mathcal{O}}\left(C^{8}_{\mathsf{exo}}S^{8}H^{10}A^{9}\cdot \varepsilon^{-26}\right).\]

#### k.1.7 Concluding: Main Guarantee for \(\mathsf{RVFS}^{\mathsf{exo}}\).bc

To conclude, we prove Theorem B.1, which shows that \(\mathsf{RVFS}^{\mathsf{exo}}\).bc succeeds with high probability. Recall that \(\mathsf{RVFS}^{\mathsf{exo}}\).bc (i) wedges \(\mathsf{RVFS}^{\mathsf{exo}}\) multiple times for random samples \(\zeta_{1:H}\) to ensure that the success event for Lemma K.7 occurs for at least one invocation, and (ii) extracts an executable policy using behavior cloning. Regarding the former point, note that the probability of the success event of Lemma K.2 can be boosted by sampling i.i.d. \(\zeta^{(1)}_{1:H},\ldots,\zeta^{(n)}_{1:H}\sim\mathbb{P}^{\zeta}\) inputs to \(\mathsf{RVFS}^{\mathsf{exo}}\) for \(n\geq 1\); as long as \(n\) is polynomially large, with high probability at least one of the inputs \(\zeta^{(1)}_{1:H},\ldots,\zeta^{(n)}_{1:H}\) will satisfy the conclusion of Lemma K.2. Thus, it suffices to pick the policy with the highest value function among the different calls to \(\mathsf{RVFS}^{\mathsf{exo}}\). Using this, we prove Theorem B.1.

Proof of Theorem B.1.: Recall that Algorithm 9 picks the final policy \(\widehat{\pi}^{(i_{\mathsf{opt}})}_{1:H}\) based on empirical value function estimates. In particular, for every \(i\in[N_{\mathsf{boost}}]\) (with \(N_{\mathsf{boost}}\) as in Algorithm 9), the estimate \(\widetilde{\mathcal{J}}(\widehat{\pi}^{(i)}_{1:H})\) for \(J(\widehat{\pi}^{(i)}_{1:H})\) is computed using \(N_{\mathsf{eval}}\) episodes. Thus, by Hoeffding's inequality and the union bound, we have that there is an event \(\widetilde{\mathcal{E}}\) of probability at least \(1-\delta/4\) under which

\[\forall i\in[N_{\mathsf{boost}}],\quad|J(\widehat{\pi}^{(i)}_{1:H})-\widetilde {\mathcal{J}}(\widehat{\pi}^{(i)}_{1:H})|\leq\sqrt{2\log(2N_{\mathsf{boost}}/ \delta)/N_{\mathsf{eval}}}.\]

Therefore, by definition of \(i_{\mathsf{opt}}\) in Algorithm 9, we have that under \(\widetilde{\mathcal{E}}\):

\[\forall i\in[N_{\mathsf{boost}}],\quad J(\widehat{\pi}^{(i)}_{1:H}) \leq J(\widehat{\pi}^{(i_{\mathsf{opt}})}_{1:H})+\sqrt{2\log(2N_{ \mathsf{boost}}/\delta)/N_{\mathsf{eval}}},\] \[\leq J(\widehat{\pi}^{(i_{\mathsf{opt}})}_{1:H})+\varepsilon/8, \tag{71}\]

where the last inequality follows by the choice of \(N_{\mathsf{eval}}\) in Algorithm 9. On the other hand, by Lemma K.2, there is an event \(\widetilde{\mathcal{E}}^{\text{success}}\) of \(\mathbb{P}^{\zeta}\)-probability at least

\[1-(24SAH\varepsilon)^{N_{\mathsf{boost}}}\geq 1-\delta/4\quad\text{(by the choice of $N_{\mathsf{boost}}$ in Algorithm 9)}\]

under which there exists \(j\in[N_{\mathsf{boost}}]\) such that \(\zeta^{(j)}_{1:H}\in\mathcal{E}_{\mathsf{rand}}\), where \(\mathcal{E}_{\mathsf{rand}}\) is defined as in Lemma K.2. In what follows, we condition on the event \(\mathcal{E}^{\mathsf{success}}\) and let \(j\in[N_{\mathsf{boost}}]\) be such that \(\zeta^{(j)}_{1:H}\in\mathcal{E}_{\mathsf{rand}}\). Further, we use \(\widehat{\pi}^{\mathsf{RVFS}}_{1:H}\) to denote the policy returned by the instance of \(\mathsf{RVFS}^{\mathsf{exo}}\) that is used by \(\mathsf{RVFS}^{\mathsf{exo}}\).bc to learn \(\widehat{\pi}^{(j)}_{1:H}\).

By Proposition M.1 (instantiated with \(\varepsilon_{\mathsf{mis}}=0\)), there is an event \(\widetilde{\mathcal{E}}^{\prime}\) of probability at least \(1-\delta/4\) under which the policy \(\widehat{\pi}^{(j)}\) produced by BehaviorCloning satisfies

\[J\big{(}\widehat{\pi}^{\mathsf{RVFS}}_{1:H}\big{)}-J\big{(}\widehat{\pi}^{(j)} _{1:H}\big{)}\leq\frac{\varepsilon}{2}. \tag{72}\]

By Lemma K.7 and the fact that \(\zeta^{(j)}_{1:H}\in\mathcal{E}_{\mathsf{rand}}\), there is an event \(\widetilde{\mathcal{E}}\) of probability at least \(1-\delta/2\) under which:

\[\forall h\in[H],\quad\forall h\in[H],\quad\mathbb{P}^{\mathbb{F}}\big{[}\widehat {\pi}_{h}(\mathbf{x}_{h})\neq\bar{\pi}_{h}(\mathbf{x}_{h})\big{]}\leq\frac{\varepsilon ^{2}_{\mathsf{RVFS}}}{4H^{3}SAC_{\mathsf{exo}}}\leq\frac{\varepsilon}{4H^{2}}, \tag{73}\]

where \(\bar{\pi}(\cdot)\coloneqq\bar{\pi}(\cdot;\zeta^{(j)}_{1:H},\varepsilon_{ \mathsf{RVFS}})\) which is defined in (69).

Moving forward, we condition on \(\widetilde{\mathcal{E}}\cap\widetilde{\mathcal{E}}^{\prime}\cap\widetilde{ \mathcal{E}}^{\prime}\). By Lemma C.6 (the performance difference lemma), we have

\[J(\bar{\pi})-J(\widehat{\pi}^{\mathsf{RVFS}}_{1:H}) =\sum_{h=1}^{H}\mathbb{E}^{\pi^{\mathsf{prs}}}[Q^{\bar{\pi}}_{h}( \mathbf{x}_{h},\bar{\pi}_{h}(\mathbf{x}_{h}))-Q^{\bar{\pi}}_{h}(\mathbf{x}_{h},\widehat{ \pi}^{\mathsf{RVFS}}_{h}(\mathbf{x}_{h}))],\] \[\leq H\sum_{h=1}^{H}\mathbb{P}^{\bar{\pi}}\big{[}\widehat{\pi}_{h }(\mathbf{x}_{h})\neq\bar{\pi}_{h}(\mathbf{x}_{h})\big{]},\] \[\leq\varepsilon/4, \tag{74}\]where the last inequality follows by (73). Now, by Lemma K.1, we have \(\bar{\pi}\in\Pi_{2\varepsilon_{\text{n\'{e}rs}}}\), and so by Lemma H.1,

\[J(\pi^{\star})-J(\bar{\pi})\leq 6H\varepsilon_{\text{R\'{e}rs}}\leq \varepsilon/8, \tag{75}\]

where the last inequality follows by the choice of \(\varepsilon_{\text{R\'{e}rs}}\) in Algorithm 9. Combining (75) with (71), (72), and (74), we get that

\[J(\pi^{\star})-J(\bar{\pi}_{1:H})=J(\pi^{\star})-J(\bar{\pi}_{1:H}^{\prime( \varepsilon_{\text{n\'{e}rs}})})\leq\varepsilon.\]

Finally, by the union bound, we have \(\mathbb{P}[\breve{\mathcal{E}}\cap\widetilde{\mathcal{E}}\cap\widetilde{ \mathcal{E}}^{\prime}]\geq 1-\delta\), and so the desired suboptimality guarantee holds with probability at least \(1-\delta\).

Bounding the sample complexity.The sample complexity is dominated by the calls to \(\mathsf{R\'{e}\mathsf{F}\mathsf{S}^{\mathsf{e}\mathsf{x}\mathsf{o}}_{0}}\) within \(\mathsf{R\'{e}\mathsf{F}\mathsf{S}^{\mathsf{e}\mathsf{x}\mathsf{o}}.\text{bc}}\) (Algorithm 9). Since \(\mathsf{R\'{e}\mathsf{F}\mathsf{S}^{\mathsf{e}\mathsf{x}\mathsf{o}}.\text{bc}}\) calls \(\mathsf{R\'{e}\mathsf{F}\mathsf{S}^{\mathsf{e}\mathsf{x}\mathsf{o}}_{0}}\) with suboptimality parameter \(\varepsilon_{\text{R\'{e}rs}}=\varepsilon H^{-1}/48\), we get by Lemma K.7 that the total sample complexity is bounded by

\[\widetilde{O}\left(C^{8}_{\text{\rm{exp}}}S^{8}H^{36}A^{9}\cdot \varepsilon^{-26}\right).\]

### Proof of Lemma K.1 (Endogenous Benchmark Policies)

Proof of Lemma K.1.: Fix \(\delta\in(0,1)\), \(\varepsilon\in(0,1/2)\), and \(\zeta^{\prime}_{1:H}\subset[0,1/2]\). We show via backward induction over \(\ell=H+1,\ldots,1\) that \(\bar{\pi}_{\tau}(\cdot;\zeta^{\prime}_{1:H},\varepsilon)\) is endogenous for all \(\tau\in[\ell\,..\,H+1]\), with the convention that \(\bar{\pi}_{H+1}=\pi_{\text{un\'{i}}\bar{\tau}}\). The base case holds trivially by convention.

Fix \(h\in[H]\) and suppose that the induction hypothesis holds for all \(\ell\in[h+1\,..\,H+1]\). We show that it holds for \(\ell=h\). First, by the induction hypothesis, \(\bar{\pi}_{\ell}(\cdot;\zeta^{\prime}_{1:H},\varepsilon)\) is endogenous for all \(\ell\in[h+1\,..\,H]\). Thus, there exists a function \(f_{h+1}:\mathcal{S}\to[0,H-h]\) such that

\[V^{\bar{\pi}}_{h+1}(x^{\prime})=f_{h+1}(\phi^{\star}(x^{\prime})),\quad\forall x ^{\prime}\in\mathcal{X}.\]

Therefore, we have for all \((x,a)\in\mathcal{X}\times\mathcal{A}\):

\[\mathcal{P}_{h}[V^{\bar{\pi}}_{h+1}](x,a) =r_{h}(x,a)+\mathbb{E}[f_{h+1}(\phi^{\star}(\mathbf{x}_{h+1}))\mid\bm {x}_{h}=x,\mathbf{a}_{h}=a],\] \[=r_{h}(x,a)+\mathbb{E}[f_{h+1}(\mathbf{s}_{h+1})\mid\mathbf{x}_{h}=x,\mathbf{ a}_{h}=a],\] \[=r_{h}(x,a)+\mathbb{E}[f_{h+1}(\mathbf{s}_{h+1})\mid\mathbf{s}_{h}=\phi^{ \star}(x),\mathbf{a}_{h}=a], \tag{76}\]

where the last equality follows by the ExBMDP transition structure. Eq. (76) together with the fact that the rewards are endogenous (by assumption) implies that there exists \(g_{h}:\mathcal{S}\times\mathcal{A}\to[0,H-h+1]\) such that

\[\forall(x,a)\in\mathcal{X}\times\mathcal{A},\quad\mathcal{P}_{h}[V^{\bar{\pi} }_{h+1}](x,a)=g_{h}(\phi^{\star}(x),a),\]

which in turn implies that \(x\mapsto[\mathcal{P}_{h}[V^{\bar{\pi}}_{h+1}](x,a)/\varepsilon+\zeta^{\prime} _{h}]\) is only a function of \(x\) through \(\phi^{\star}(x)\) for all \(a\in\mathcal{A}\). Thus, \(\bar{\pi}_{h}\) is an endogenous policy and the induction is completed.

For the second claim, observe that for the functions \(\widetilde{Q}_{1},\cdot\cdot\cdot,\widetilde{Q}_{H}\in[0,H]^{\mathcal{X}\times \mathcal{A}}\) defined as

\[\forall h\in[H],\forall(x,a)\in\mathcal{X}\times\mathcal{A},\quad\widetilde{Q }_{h}(x,a)=\varepsilon\cdot[\mathcal{P}_{h}[V^{\bar{\pi}}_{h+1}](x,a)/ \varepsilon+\zeta^{\prime}_{h}],\]

we have

\[\forall h\in[H],\quad\bar{\pi}_{h}(\cdot;\zeta^{\prime}_{1:H},\varepsilon)\in \operatorname*{arg\,max}_{a\in\mathcal{A}}\widetilde{Q}_{h}(\cdot,a)\quad \text{and}\quad\|\widetilde{Q}_{h}-Q^{\bar{\pi}}_{h}\|_{\infty}\leq 2\varepsilon,\]

which implies that \(\bar{\pi}(\cdot;\zeta^{\prime}_{1:H},\varepsilon)\in\Pi_{2\varepsilon}\). 

### Proof of Lemma K.2 (Snapping Probability)

Proof of Lemma K.2.: Fix \(\varepsilon\in(0,1)\) and \(\delta\in(0,1/2)\). For \(\tau\leq\ell\in[H]\), let \(\mathbb{P}^{\zeta}_{\tau:\ell}\) denote the probability law of \(\zeta_{\tau},\ldots,\zeta_{\ell}\). We also use the shorthand \(\mathbb{P}^{\zeta}_{\tau}\) for \(\mathbb{P}^{\zeta}_{\tau:\tau}\), for all \(\tau\in[H]\). We show via backward induction over \(\ell=H+1,\ldots,1\) that there exists an event \(\mathcal{E}_{\ell}\) of \(\mathbb{P}^{\zeta}_{\ell:H}\)-probability at least \(1-24SA(H-\ell+1)\varepsilon\) under which for all \(\widetilde{V}\in(\mathcal{X}\times[H]\to[0,H])\):

\[\forall\tau\in[\ell\,..\,H],\quad\tilde{\mathbf{\pi}}_{\tau}(\cdot;\widetilde{V}, \zeta_{1:H},\varepsilon,\delta)=\bar{\pi}_{\tau}(\cdot;\zeta_{1:H}, \varepsilon),\]with the convention that \(\bar{\pi}_{H+1}\equiv\bar{\pi}_{H+1}\equiv\pi_{\text{unif}}\). We then set \(\mathcal{E}_{\text{rand}}=\mathcal{E}_{1}\).

The base case follows trivially by convention.

We now proceed with the inductive step. Fix \(h\in[H]\) and suppose that the induction hypothesis holds for all \(\ell\in[h+1\,..\,H]\). We show that it holds for \(\ell=h\). Throughout, we condition on \(\mathcal{E}_{h+1}\). By definition of \(\mathcal{E}_{h+1}\), we have for all \(\widetilde{V}\in(\mathcal{X}\times[H]\to[0,H])\):

\[\forall\ell\in[h+1\,..\,H],\quad\bar{\pi}_{\ell}(\cdot;\mathbf{\zeta}_{1:H}, \varepsilon)=\tilde{\pi}_{\ell}(\cdot;\widetilde{V},\mathbf{\zeta}_{1:H}, \varepsilon,\delta).\]

This implies that for all \(\widetilde{V}\in(\mathcal{X}\times[H]\to[0,H])\):

\[\forall\,x\in\mathcal{X},\;\;\tilde{\pi}_{h}(x;\widetilde{V},\mathbf{\zeta}_{1:H}, \varepsilon,\delta)\in\operatorname*{arg\,max}_{a\in\mathcal{A}}\left\{\begin{array}{ ll}[\mathbf{\widetilde{Q}}_{h}(x,a)/\varepsilon+\mathbf{\zeta}_{h}],&\text{if }\|\mathbf{\widetilde{Q}}_{h}(x,\cdot)-\mathcal{P}_{h}[V_{h+1}^{\bar{\pi}}](x, \cdot)\|_{\infty}\leq 4\varepsilon^{2},\\ [\mathcal{P}_{h}[V_{h+1}^{\bar{\pi}}](x,a)/\varepsilon+\mathbf{\zeta}_{h}],&\text{ otherwise,}\end{array}\right.\]

by the definition of \(\tilde{\pi}_{h}\) in (68), where \(\mathbf{\widetilde{Q}}_{h}(\cdot,a)=\mathbf{\widetilde{P}}_{h,\varepsilon,\delta}[ \widetilde{V}_{h+1}](\cdot,a)\). From this, we see that to prove

\[\tilde{\pi}_{h}(\cdot;\widetilde{V},\mathbf{\zeta}_{1:H},\varepsilon,\delta)= \tilde{\pi}_{h}(\cdot;\mathbf{\zeta}_{1:H},\varepsilon)\text{ for all }\widetilde{V},\text{ it suffices to show that for all }x\in\mathcal{X}\text{ and }\widetilde{V},\]

\[\operatorname*{arg\,max}_{a\in\mathcal{A}}[\mathbf{\widetilde{Q}}_{h}(x,a)/ \varepsilon+\mathbf{\zeta}_{h}]=\operatorname*{arg\,max}_{a\in\mathcal{A}}[\mathcal{ P}_{h}[V_{h+1}^{\bar{\pi}}](x,a)/\varepsilon+\mathbf{\zeta}_{h}],\text{ whenever }\;|\mathbf{\widetilde{Q}}_{h}(x,a)-\mathcal{P}_{h}[V_{h+1}^{\bar{\pi}}](x,a)| \leq 4\varepsilon^{2}.\]

Observe that a sufficient condition for this to hold is that

\[\forall\,x\in\mathcal{X},\forall\,a\in\mathcal{A},\forall\,\delta\in[-4 \varepsilon^{2},4\varepsilon^{2}],\quad\big{[}(\mathcal{P}_{h}[V_{h+1}^{\bar{ \pi}}](x,a)+\delta)\cdot\varepsilon^{-1}+\mathbf{\zeta}_{h}\big{]}=\big{[}\mathcal{ P}_{h}[V_{h+1}^{\bar{\pi}}](x,a)\cdot\varepsilon^{-1}+\mathbf{\zeta}_{h}\big{]}, \tag{77}\]

where \(\delta\) represents all the possible values that the difference \(\mathbf{\widetilde{Q}}_{h}(x,a)-\mathcal{P}_{h}[V_{h+1}^{\bar{\pi}}](x,a)\) is allowed to take. By Lemma K.1, we know that \(\bar{\pi}\) is endogenous, and so there exists a function \(g_{h}:\mathcal{S}\times\mathcal{A}\to[0,H-h+1]\) such that

\[\forall\,x\in\mathcal{X},a\in\mathcal{A},\quad\mathcal{P}_{h}[V_{h+1}^{\bar{ \pi}}](x,a)=g_{h}(\phi^{*}(x),a).\]

Toward proving Eq. (77), observe that for any \((s,a)\in\mathcal{S}\in\mathcal{A}\), if \(\mathbf{\zeta}_{h}\) is such that

\[\begin{split} g_{h}(s,a)/\varepsilon+\mathbf{\zeta}_{h}+4\varepsilon \leq&\big{[}g_{h}(s,a)/\varepsilon+\mathbf{\zeta}_{h}\big{]},\\ \text{and}& g_{h}(s,a)/\varepsilon+\mathbf{\zeta}_{h}-4 \varepsilon>&\big{[}g_{h}(s,a)/\varepsilon+\mathbf{\zeta}_{h}\big{]}-1,\end{split} \tag{78}\]

then, for all \(\delta\in[-4\varepsilon^{2},4\varepsilon^{2}]\) and all \(x\in\mathcal{X}\) such that \(\phi^{*}(x)=s\), we have

\[\big{[}(\mathcal{P}_{h}[V_{h+1}^{\bar{\pi}}](x,a)+\delta)/\varepsilon+\mathbf{ \zeta}_{h}\big{]}=\big{[}(g_{h}(s,a)+\delta)/\varepsilon+\mathbf{\zeta}_{h}\big{]} =\big{[}g_{h}(s,a)/\varepsilon+\mathbf{\zeta}_{h}\big{]}=\big{[}\mathcal{P}_{h}[V_ {h+1}^{\bar{\pi}}](x,a)/\varepsilon+\mathbf{\zeta}_{h}\big{]}.\]

Therefore, if we let \(\mathcal{E}_{h}(s,a)\) denote the event in (78), then under \(\bigcap_{(s,a)\in\mathcal{S}\times\mathcal{A}}\mathcal{E}_{h}(s,a)\), the desired condition in (77) holds. At this point, setting \(\mathcal{E}_{h}=(\bigcap_{(s,a)\in\mathcal{S}\times\mathcal{A}}\mathcal{E}_{h}( s,a))\cap\mathcal{E}_{h+1}\) would complete the induction as long as \(\mathbb{P}_{h:H}^{\zeta}[\mathcal{E}_{h}]\geq 1-24SA(H-h+1)\varepsilon\). We now show that this is indeed the case by bounding the probability of the event \(\bigcap_{(s,a)\in\mathcal{S}\times\mathcal{A}}\mathcal{E}_{h}(s,a)\). By the union bound, we have

\[\mathbb{P}_{h:H}^{\zeta}\left[\bigcap_{(s,a)\in\mathcal{S}\times\mathcal{A}} \mathcal{E}_{h}(s,a)\mid\mathcal{E}_{h+1}\right]\geq 1-\sum_{(s,a)\in\mathcal{S} \times\mathcal{A}}\mathbb{P}_{h:H}^{\zeta}\left[\mathcal{E}_{h}(s,a)^{c}\mid \mathcal{E}_{h+1}\right], \tag{79}\]

where \(\mathcal{E}_{h}(s,a)^{c}\) denotes the complement of \(\mathcal{E}_{h}(s,a)\). We now bound the probability

\[\mathbb{P}_{h:H}^{\zeta}\left[\mathcal{E}_{h}(s,a)^{c}\mid\mathcal{E}_{h+1} \right].\]

Fix \((s,a)\in\mathcal{S}\times\mathcal{A}\). We have that \(\mathbf{\zeta}_{h}\in\mathcal{E}(s,a)^{c}\) if and only if

\[\begin{split} g_{h}(s,a)/\varepsilon+\mathbf{\zeta}_{h}+4\varepsilon>& \big{[}g_{h}(s,a)/\varepsilon+\mathbf{\zeta}_{h}\big{]},\\ \text{or}& g_{h}(s,a)/\varepsilon+\mathbf{\zeta}_{h}-4 \varepsilon\leq&\big{[}g_{h}(s,a)/\varepsilon+\mathbf{\zeta}_{h}\big{]}-1.\end{split} \tag{80}\]

Now, since \(\mathbf{\zeta}_{h}\in[0,1/2]\), Lemma L.3 (instantiated with \((x,\zeta,\nu)=(g_{h}(s,a)/\varepsilon,\mathbf{\zeta}_{h},4\varepsilon)\)) implies that (80) holds only if

\[\big{[}g_{h}(s,a)/\varepsilon\big{]}-4\varepsilon\leq g_{h}(s,a)/\varepsilon+ \mathbf{\zeta}_{h}\leq\big{[}g_{h}(s,a)/\varepsilon\big{]}+4\varepsilon\quad\text{ or}\quad 0\leq\mathbf{\zeta}_{h}\leq 4\varepsilon. \tag{81}\]

Further, note that since \(\mathbf{\zeta}_{h}\) is uniformly distributed over \([0,1/2]\), the \(\mathbb{P}_{h}^{\zeta}\)-probability of the event in (81) is at most the sum of the lengths of the intervals

\[\big{[}\big{[}g_{h}(s,a)/\varepsilon\big{]}-g_{h}(s,a)/\varepsilon-4 \varepsilon,\big{[}g_{h}(s,a)/\varepsilon\big{]}-g_{h}(s,a)/\varepsilon+4 \varepsilon\big{]}\quad\text{and}\quad[0,4\varepsilon],\]multiplied by \(2\), which is equal to \(24\varepsilon\). Therefore, we have

\[\mathbb{P}^{\zeta}_{h:H}\left[\mathcal{E}_{h}(s,a)^{c}\mid\mathcal{E }_{h+1}\right]\] \[\leq\mathbb{P}^{\zeta}_{h}\left[\lceil g_{h}(s,a)/\varepsilon\rceil- 4\varepsilon\leq g_{h}(s,a)/\varepsilon+\zeta_{h}\leq\lceil g_{h}(s,a)/ \varepsilon\rceil+4\varepsilon\ \ \text{or}\ \ 0\leq\zeta_{h}\leq 4\varepsilon \right]\leq 24\varepsilon.\]

Combining this with (79), we obtain

\[\mathbb{P}^{\zeta}_{h:H}\left[\bigcap_{(s,a)\in\mathcal{S}\times \mathcal{A}}\mathcal{E}_{h}(s,a)\mid\mathcal{E}_{h+1}\right]\geq 1-\sum_{(s,a) \in\mathcal{S}\times\mathcal{A}}\mathbb{P}^{\zeta}_{h:H}\left[\mathcal{E}_{h}( s,a)^{c}\mid\mathcal{E}_{h+1}\right]\geq 1-24SA\varepsilon.\]

Thus, by setting \(\mathcal{E}_{h}=\left(\bigcap_{(s,a)\in\mathcal{S}\times\mathcal{A}}\mathcal{E }_{h}(s,a)\right)\cap\mathcal{E}_{h+1}\), we get that

\[\mathbb{P}^{\zeta}_{h:H}\{\mathcal{E}_{h}\}\geq\mathbb{P}^{\zeta}_{h+1:H}[ \mathcal{E}_{h+1}]\cdot\mathbb{P}^{\zeta}_{h:H}[\mathcal{E}_{h}\mid\mathcal{E} _{h+1}]\geq(1-24SA(H-h)\varepsilon)(1-24SA\varepsilon),\]

which completes the induction. 

### Proof of Lemma k.3 (Coverability in Weakly Correlated ExBMDP)

**Proof of Lemma k.3.** Fix \(h\in[2\,..\,H]\) and define the measure \(\mu\) as

\[\mu(x)\coloneqq\sum_{\xi^{\prime}\in\Xi}q(x^{\prime}\mid(\phi^{*}_{h}(x^{ \prime}),\xi^{\prime}))\cdot\mathbb{P}[\mathbf{\xi}_{h}=\xi^{\prime}]\cdot\mathbb{P }[\mathbf{\xi}_{h}=\phi^{*}_{h}(x^{\prime})\mid\mathbf{s}_{h-1}=\phi^{*}_{h-1}(x),\mathbf{ a}_{h-1}=a],\]

for all \(h\in[H]\) and \(x\in\mathcal{X}\). We show that \(\mu\) satisfies Assumption 4.1 with \(C_{\text{push}}=C_{\text{exo}}\cdot SA\). First, note that \(\mu\) is indeed a probability measure over \(\mathcal{X}\). Fix \((x,a,x^{\prime})\in\mathcal{X}\times\mathcal{A}\times\mathcal{X}\). We have

\[\mathbb{P}[\mathbf{x}_{h}=x^{\prime}\mid\mathbf{x}_{h-1}=x,\mathbf{a}_{h-1}=a]\] \[=\frac{\mathbb{P}[\mathbf{x}_{h}=x^{\prime},\mathbf{x}_{h-1}=x\mid\mathbf{a}_ {h-1}=a]}{\mathbb{P}[\mathbf{x}_{h-1}=x\mid\mathbf{a}_{h-1}=a]},\] \[=\frac{\mathbb{P}[\mathbf{x}_{h}=x^{\prime},\mathbf{x}_{h-1}=x\mid\mathbf{a}_ {h-1}=a]}{\mathbb{P}[\mathbf{x}_{h-1}=x]},\] \[=\frac{\sum_{\xi,\xi^{\prime}\in\Xi}q(x^{\prime}\mid(\phi^{*}_{h} (x^{\prime}),\xi^{\prime}))\cdot q(x\mid(\phi^{*}_{h-1}(x),\xi))\cdot\mathbb{P }[\mathbf{s}_{h}=\phi^{*}_{h}(x^{\prime}),\mathbf{\xi}_{h}=\xi^{\prime},\mathbf{s}_{h-1}= \phi^{*}_{h-1}(x),\mathbf{\xi}_{h-1}=\xi\mid\mathbf{a}_{h-1}=a]}{\sum_{\xi\in\Xi}q(x \mid(\phi^{*}_{h-1}(x),\xi))\cdot\mathbb{P}[\mathbf{s}_{h-1}=\phi^{*}_{h-1}(x),\bm {\xi}_{h-1}=\xi]},\]

and so by the ExBMDP structure:

\[=\frac{\sum_{\xi,\xi^{\prime}\in\Xi}q(x^{\prime}\mid(\phi^{*}_{h} (x^{\prime}),\xi^{\prime}))\cdot q(x\mid(\phi^{*}_{h-1}(x),\xi))\cdot\mathbb{P }[\mathbf{\xi}_{h}=\xi^{\prime},\mathbf{\xi}_{h-1}=\xi]\cdot\mathbb{P}[\mathbf{s}_{h}=\phi^ {*}_{h}(x^{\prime})\mid\mathbf{s}_{h-1}=\phi^{*}_{h-1}(x),\mathbf{a}_{h-1}=a]}{\sum_{ \xi\in\Xi}q(x\mid(\phi^{*}_{h}(x),\xi))\cdot\mathbb{P}[\mathbf{\xi}_{h-1}=\xi]},\]

and by Assumption B.1

\[\leq C_{\text{exo}}\sum_{\xi,\xi^{\prime}\in\Xi}q(x^{\prime}\mid( \phi^{*}_{h}(x^{\prime}),\xi^{\prime}))\cdot q(x\mid(\phi^{*}_{h-1}(x),\xi)) \cdot\mathbb{P}[\mathbf{\xi}_{h}=\xi^{\prime}]\cdot\mathbb{P}[\mathbf{\xi}_{h-1}= \xi]\cdot\mathbb{P}[\mathbf{s}_{h}=\phi^{*}_{h}(x^{\prime})\mid\mathbf{s}_{h-1}=\phi^ {*}_{h-1}(x),\mathbf{a}_{h-1}=a]}{\sum_{\xi\in\Xi}q(x\mid(\phi^{*}_{h}(x),\xi)) \cdot\mathbb{P}[\mathbf{\xi}_{h-1}=\xi]},\] \[=C_{\text{exo}}SA\cdot\mu(x^{\prime}),\]

This completes the proof. 

### Proof of Lemma k.6 (Confidence Sets)

To prove Lemma k.6, we need the following consequence of tests in Line 1 passing for all \(\ell\in[h+1\,..\,H]\).

**Lemma K.8** (Consequence of passed tests).: _Let \(h\in[0\,..\,H]\), \(\varepsilon>0\), and \(\zeta_{1:H}\in[0,1/2]\) be given and consider a call to \(\mathsf{RVF}S^{\mathsf{exo}}_{0}\) (Algorithm 8) in the setting of Lemma k.4. Further, let \(\mathcal{E}\) be the event of Lemma k.4. There exists an event \(\mathcal{E}^{\prime}_{h}\) of probability at least \(1-\delta/H\) such that under \(\mathcal{E}\cap\mathcal{E}^{\prime}_{h}\), if a call to \(\mathsf{RVF}S^{\mathsf{exo}}_{h}\) during the execution of \(\mathsf{RVF}S^{\mathsf{exo}}_{h}\) terminates and returns \((\widehat{V}_{h:H},\widehat{V}_{h:H},\mathcal{C}_{h:H},\mathcal{B}_{h:H},t_{h:H})\), then for any \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\) and \(\ell\in[h+1\,..\,H+1]\):_

\[\mathbb{P}^{\Xi}\left[\sup_{f\in\widehat{\mathcal{V}}_{f}}\max_{a\in\mathcal{A} }\left|(\mathcal{P}_{\ell-1}[\widehat{V}_{\ell}]-\mathcal{P}_{\ell-1}[f_{\ell} ])(\mathbf{x}_{\ell-1},a)\right|>3\varepsilon^{2}\mid\mathbf{x}_{h-1}=x_{h-1},\mathbf{a}_{h -1}=a_{h-1}\right]\leq\frac{4\log(8M^{6}N^{2}_{\text{test}}H^{8}/\delta)}{N_{ \text{test}}},\]

_where \((\widehat{\pi}_{\tau})_{\tau\geq h}\subset\Pi_{\text{S}}\), \(M\), and \(N_{\text{test}}\) are as in \(\mathsf{RVF}S^{\mathsf{exo}}_{h}\) (Algorithm 8)._Proof of Lemma k.8.: This is just a restatement of Lemma 1.2, and the proof is exactly the same as the latter. 

We will also use Lemma 1.5; even though this result is stated in section for the \(V^{\pi}\)-realizable setting, it is also applicable to the ExBMDP variant of RVFS as it merely says something about the order in which the \((\mathsf{RVFS}^{\mathsf{exo}}_{h})\) instances are called. With this, we now prove Lemma K.6.

Proof of Lemma k.6.: The proof is very similar to that of Lemma 1.2, with differences to account for the "coarsening" of the learned and benchmark policies.

We prove the desired result for \(\mathcal{E}^{\prime\prime\prime}=\mathcal{E}\cap\mathcal{E}^{\prime}_{1}\cap \mathcal{E}^{\prime\prime}_{1}\cap\cdots\cap\mathcal{E}^{\prime}_{H}\cap \mathcal{E}^{\prime\prime}_{H}\), where \(\mathcal{E}\), \((\mathcal{E}^{\prime}_{h})\), and \((\mathcal{E}^{\prime\prime}_{h})\) are the events in Lemma K.4, Lemma K.8, and Lemma K.5, respectively. Throughout, we condition on \(\mathcal{E}^{\prime\prime\prime}\). First, note that by Lemma K.4, \(\mathsf{RVFS}^{\mathsf{exo}}_{0}\) terminates. Let \((\widehat{V}_{1:H},\widehat{\mathcal{V}}_{1:H},\mathcal{C}_{1:H},\mathcal{B}_ {1:H},t_{1:H})\) be its returned tuple.

We show via backward induction over \(\ell=H+1,\ldots,1\), that

\[V^{\hat{\pi}}_{\ell}\in\widehat{\mathcal{V}}_{\ell}, \tag{82}\]

where \(\tilde{\pi}_{1:H}\) is the stochastic policy defined recursively via

\[\forall x\in\mathcal{X},\ \tilde{\pi}_{\tau}(x;\zeta_{1:H},\varepsilon,\delta) \in\operatorname*{arg\,max}_{a\in\mathcal{A}}\left\{\begin{array}{ll}[ \widehat{\mathcal{G}}_{\tau}(x,a)/\varepsilon+\zeta_{\tau}],&\text{if }| \widehat{\mathcal{G}}_{\tau}(x,\cdot)-\mathcal{P}_{\tau}[V^{\hat{\pi}}_{\tau+1}] (x,\cdot)|_{\infty}\leq 4\varepsilon^{2},\\ [\mathcal{P}_{\tau}[V^{\hat{\pi}}_{\tau+1}](x,a)/\varepsilon+\zeta_{\tau}],& \text{otherwise},\end{array}\right.\]

for \(\tau=H,\ldots,1\), where \(\widehat{\mathcal{G}}_{\tau}(\cdot,a)\coloneqq\widehat{\mathcal{P}}_{\tau, \varepsilon,\delta}[\widehat{V}_{\tau+1}](\cdot,a)\). Note that since \(\zeta_{1:H}\in\mathcal{E}_{\text{rand}}\) (for \(\mathcal{E}_{\text{rand}}\) is defined in Lemma K.2), we have \(\tilde{\pi}\equiv\tilde{\pi}\), where \(\tilde{\pi}\) is as in (70). Thus, instantiating the induction hypothesis with \(\ell=h\) and using the definition of the confidence sets \((\widehat{\mathcal{V}}_{\ell})\) in (16) together with \(V^{\hat{\pi}}_{h}\equiv V^{\tilde{\pi}}_{h}\) (since \(\tilde{\pi}\equiv\tilde{\pi}\)) implies the desired result.

Base case [\(\ell=H+1\)]Holds trivially since \(V^{\tilde{\pi}}_{H+1}\equiv 0\) for any \(\pi\in\Pi_{5}\) by convention.

General case [\(\ell\leq H\)]Fix \(h\in[H]\) and suppose that (82) holds for all \(\ell\in[h+1\,..\,H+1]\). We show that this remains true for \(\ell=h\). First, note that if \(\mathsf{RVFS}^{\mathsf{exo}}_{h}\) is never called during the execution of \(\mathsf{RVFS}^{\mathsf{exo}}_{0}\), then \(\widehat{\mathcal{V}}_{h}=\mathcal{V}_{h}\), and so (82) holds for \(\ell=h\), since \(\tilde{\pi}=\tilde{\pi}\) is endogenous under \(\zeta_{1:H}\in\mathcal{E}_{\text{rand}}\), where \(\mathcal{E}_{\text{rand}}\) is the event in Lemma K.2.

Now, suppose that \(\mathsf{RVFS}^{\mathsf{exo}}_{h}\) is called at least once, and let \((\widehat{V}^{+}_{h;H},\widehat{\mathcal{V}}^{+}_{h;H},\mathcal{C}^{+}_{h;H},\mathcal{B}^{+}_{h;H},t^{+}_{h;H})\) be the output of the last call to \(\mathsf{RVFS}^{\mathsf{exo}}_{h}\) throughout the execution of \(\mathsf{RVFS}^{\mathsf{exo}}_{0}\). Next, we show that

\[(\widehat{V}^{+}_{h:H},\widehat{\mathcal{V}}^{+}_{h:H},\mathcal{C}^{+}_{h:H}) =(\widehat{V}_{h:H},\widehat{\mathcal{V}}_{h:H},\mathcal{C}_{h:H}). \tag{83}\]

The for-loop in Line 16 ensures that no instance of \((\mathsf{RVFS}^{\mathsf{exo}}_{\tau})_{\tau\times h}\) can be called after the last call to \(\mathsf{RVFS}^{\mathsf{exo}}_{h}\) (see Lemma I.5). Thus, the estimated value functions, confidence sets, and core sets for layers \(h+1,\ldots,H\) remain unchanged after the last call to \(\mathsf{RVFS}^{\mathsf{exo}}_{h}\); that is, (83) holds. Thus, by Lemma K.8, and since we are conditioning on \(\mathcal{E}^{\prime}_{h+1:H}\), we have that for all \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\) and \(\ell\in[h+1\,..\,H+1]\):

\[\mathbb{P}^{\mathbb{P}^{\mathbb{P}}}\!\!\sup_{f\in\widehat{\mathcal{V}}_{\ell}} \max_{a\in\mathcal{A}}\!\big{|}(\mathcal{P}_{\ell-1}[\widehat{V}_{\ell}]- \mathcal{P}_{\ell-1}[f_{\ell}])(\mathbf{x}_{\ell-1},a)\big{|}\!>3\varepsilon^{2} \mid\mathbf{x}_{h-1}=x_{h-1},\mathbf{a}_{h-1}=a_{h-1}\bigg{]}\!\leq\frac{4\log(8M^{6}N ^{2}_{\text{test}}H^{8}/\delta)}{N_{\text{test}}}. \tag{84}\]

Now, by the induction hypothesis, we have \(V^{\hat{\pi}}_{\ell}\in\widehat{\mathcal{V}}_{\ell}\), and so substituting \(V^{\hat{\pi}}_{\ell}\) for \(f_{\ell}\) in (84), we get that for all \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\) and \(\ell\in[h+1\,..\,H+1]\):

\[\mathbb{P}^{\mathbb{P}^{\mathbb{P}}}\!\!\bigg{[}\max_{a\in\mathcal{A}}\!\big{|}( \mathcal{P}_{\ell-1}[\widehat{V}_{\ell}]-\mathcal{P}_{\ell-1}[V^{\hat{\pi}}_{ \ell}])(\mathbf{x}_{\ell-1},a)\big{|}\!>3\varepsilon^{2}\mid\mathbf{x}_{h-1}=x_{h-1}, \mathbf{a}_{h-1}=a_{h-1}\bigg{]}\leq\frac{4\log(8M^{6}N^{2}_{\text{test}}H^{8}/\delta )}{N_{\text{test}}}.\]

Therefore, by Lemma L.2 (instantiated with \(\mu[\cdot]=\mathbb{P}^{\mathbb{P}}[\cdot\mid\mathbf{x}_{h-1}=x_{h-1},\mathbf{a}_{h-1} =a_{h-1}]\), \(\tau=\ell-1\), \(\varepsilon^{\prime}=\varepsilon^{2}\), and \(V_{\tau+1}=V^{\hat{\pi}}_{\ell}\)), we have that for all \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\) and \(\ell\in[h+1\,..\,H+1]\):

\[\mathbb{E}^{\mathbb{P}}[D_{\text{tv}}(\tilde{\pi}_{\ell-1}(\mathbf{x}_{\ell-1}), \tilde{\pi}_{\ell-1}(\mathbf{x}_{\ell-1}))\mid\mathbf{x}_{h-1}=x_{h-1},\mathbf{a}_{h-1}=a_{h -1}]\leq\frac{4\log(8M^{6}N^{2}_{\text{test}}H^{8}/\delta)}{N_{\text{test}}}+ \delta^{\prime}. \tag{85}\]

[MISSING_PAGE_FAIL:69]

Bounding the sample complexity.We now bound the number of episodes used by Algorithm 8 under \(\widetilde{\mathcal{E}}\). First, we fix \(h\in[H]\), and focus on the number of episodes used within a to call \(\mathsf{RVFS}^{\mathsf{exo}}_{h}\); excluding any episodes used by any recursive calls to \(\mathsf{RVFS}^{\mathsf{exo}}_{\tau}\) for \(\tau>h\). We start by counting the number of episodes used to test the fit of the estimated value functions \(\widehat{V}_{h+1:H}\). Starting from Line 8, there are for-loops over \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\), \(\ell=H,\ldots,h+1\), and \(n\in[N_{\mathsf{test}}]\) to collected partial episodes using the learned policy \(\widetilde{\pi}\) in Algorithm 8, where \(N_{\mathsf{test}}=2^{8}M^{2}H\varepsilon^{-2}\log(8M^{2}H\varepsilon^{-2} \delta^{-1})\) and \(M=[8\varepsilon^{-2}C_{\mathsf{exo}}SAH]\). Note that \(\widetilde{\pi}\) uses the local simulator and requires \(N_{\mathsf{si}}=2\log(4M^{7}N_{\mathsf{test}}^{2}H^{2}|\mathcal{V}|/\delta)/ \varepsilon^{2}\) samples to output an action at each layer (since Algorithm 8 calls Algorithm 7 with confidence level \(\delta^{\prime}=\delta/(8M^{7}N_{\mathsf{test}}^{2}H^{8}|\mathcal{V}|)\)). Also, note that whenever a test fails in Line 14, the for-loop in Line 8 resumes. We also know (by Lemma 1) that the number of times the test fails in Line 14 is at most \(M\). Thus, the number of times the for-loop in Line 8 resumes is bounded by \(HM\); here, \(H\) accounts for test failures for all layers \(\tau\in[h+1\ldots H]\). Thus, the number of episodes required to between lines Line 8 and Line 11 is bounded by

\[\#\text{episodes for roll-outs}\leq\underbrace{MH}_{\#\text{ of times Line 8 resumes}}\cdot\underbrace{MH^{2}N_{\mathsf{test}}N_{\mathsf{Sim}}}_{\text{Number of episodes in case of no test failures}}\cdot \tag{90}\]

Note that the test in Line 14 also uses local simulator access because it calls the operator \(\widetilde{\mathcal{P}}\) for every \(a\in\mathcal{A}\). Thus, the number of episodes used for the test in Line 14 is bounded by

\[\#\text{episodes for the tests}\leq\underbrace{MH}_{\#\text{ of times Line 8 resumes}}\cdot\underbrace{MHAN_{\mathsf{test}}N_{\mathsf{sim}}}_{\text{Number of episodes used in Line 14}}\cdot \tag{91}\]

We now count the number of episodes used to re-fit the value function; Line 16 onwards. Note that starting from Line 16, there are for-loops over \((x_{h-1},a_{h-1})\in\mathcal{C}_{h}\) and \(i\in[N_{\mathsf{reg}}]\) to generate \(A\cdot N_{\mathsf{est}}(\mathcal{C}_{h}|)\leq A\cdot N_{\mathsf{est}}(M)\) partial episodes using \(\widetilde{\pi}\), where \(N_{\mathsf{est}}(k)=2N_{\mathsf{reg}}^{2}\log(8AN_{\mathsf{reg}}Hk^{3}/\delta)\) is as in Algorithm 8. And, since \(\widetilde{\pi}\) uses local simulator access and requires \(N_{\mathsf{est}}\) samples (see Algorithm 7) to output an action at each layer, the number of episodes used to refit the value function is bounded by

\[\#\text{episodes for }V\text{-refitting}\leq MN_{\mathsf{reg}}AN_{\mathsf{est}}(M) HN_{\mathsf{sin}}. \tag{92}\]

Therefore, by (90), (91), and (92), the number of episodes used within a single call to \(\mathsf{RVFS}^{\mathsf{exo}}_{h}\) (not accounting for episodes used by recursive calls to \(\mathsf{RVFS}^{\mathsf{exo}}_{\tau}\), for \(\tau>h\)) is bounded by

\[\#\text{episodes used locally within }\mathsf{RVFS}^{\mathsf{exo}}_{h}\leq M^{2}H(H+A)N_{\mathsf{ test}}N_{\mathsf{sin}}+MN_{\mathsf{reg}}AN_{\mathsf{est}}(M)HN_{\mathsf{sin}}. \tag{93}\]

Finally, by Lemma 1, \(\mathsf{RVFS}^{\mathsf{exo}}_{h}\) may be called at most \(M\) times throughout the execution of \(\mathsf{RVFS}^{\mathsf{exo}}_{0}\). Using this together with (93) and accounting for the number of episodes from all layers \(h\in[H]\), we get that the total number of episodes is bounded by

\[M^{3}H^{2}(H+A)N_{\mathsf{test}}N_{\mathsf{sin}}+M^{2}H^{2}N_{\mathsf{reg}} AN_{\mathsf{est}}(M)N_{\mathsf{sin}}.\]

Substituting the expressions of \(M\), \(N_{\mathsf{test}}\), \(N_{\mathsf{est}}\), \(N_{\mathsf{sin}}\), and \(N_{\mathsf{reg}}\) from Algorithm 8 and Algorithm 7, we obtain the desired number of episodes, which concludes the proof. 

## Appendix L Additional Technical Lemmas

**Lemma L.1**.: _Let \(\tau\in[H]\) and \(\varepsilon,\delta,\nu\in(0,1)\) be given. Consider two value functions \(V_{\tau+1},\widehat{V}_{\tau+1}\in[0,H]\) and a measure \(\mu\in\Delta(\mathcal{X})\) such that_

\[\mathbb{P}_{\pi_{\tau}\sim\mu}\Big{[}\mathbb{I}\left\{\max_{a\in\mathcal{A}} \big{|}(\mathcal{P}_{\tau}[\widehat{V}_{\tau+1}]-\mathcal{P}_{\tau}[V_{\tau+1 }])(\mathbf{x}_{\tau},a)\big{|}>3\varepsilon\right\}\Big{]}\leq\nu. \tag{94}\]

_Further, for \(x\in\mathcal{X}\), let \(\widehat{\mathbf{\pi}}_{\tau}(x)\in\arg\max_{a\in\mathcal{A}}\widetilde{\mathbf{Q}}_{ \tau}(x,a)\coloneqq\widetilde{\mathbf{\mathcal{P}}}_{\tau,\varepsilon,\delta}[ \widehat{V}_{\tau+1}](x,a)\) and inductively define a randomized policy \(\widetilde{\pi}\) via_

\[\tilde{\mathbf{\pi}}_{\tau}(x)\in\arg\max_{a\in\mathcal{A}}\left\{\begin{array}{ ll}\widetilde{\mathbf{Q}}_{\tau}(x,a),&\text{if }\|\widetilde{\mathbf{Q}}_{\tau}(x,\cdot)-\mathcal{P}_{\tau}[V_{\tau+1}](x,\cdot) \|_{\infty}\leq 4\varepsilon,\\ \mathcal{P}_{\tau}[V_{\tau+1}](x,a),&\text{otherwise}.\end{array}\right.\]

_Then, we have_

\[\mathbb{E}_{\mathbf{x}_{\tau}\sim\mu}[D_{\mathrm{tv}}(\widetilde{\pi}_{\tau}(\mathbf{x} _{\tau}),\tilde{\pi}_{\tau}(\mathbf{x}_{\tau}))]\leq\nu+\delta.\]

**Proof of Lemma 1.1.** In this proof, we let \(\mathbb{P}_{\mu}\) denote the probability law of \(\mathbf{x}_{\tau}\) and \(\mathbb{P}_{\mathcal{P}}\) denote the probability law of \(\widetilde{\mathbf{\mathcal{P}}}_{\tau,\varepsilon,\delta}\). Denote by \(\mathcal{E}\) the \(\mathbb{P}_{\mu}\)-measurable event that \(\max_{a\in\mathcal{A}}\left|\left(\mathcal{P}_{\tau}[\widehat{V}_{\tau+1}]- \mathcal{P}_{\tau}[V_{\tau+1}]\right)(\mathbf{x}_{\tau},a)\right|\leq 3\varepsilon\). Fix \(x\in\mathcal{E}\), and let \(\mathcal{E}_{x}\) be the \(\mathbb{P}_{\mathcal{P}}\)-measurable event that \(\max_{a\in\mathcal{A}}|\widetilde{\mathbf{\mathcal{P}}}_{\tau,\varepsilon,\delta}[ \widehat{V}_{\tau+1}](x,a)-\mathcal{P}_{\tau}[V_{\tau+1}](x,a)|\leq 4\varepsilon\). From the definition of \(\tilde{\pi}_{\tau}\), we have that

\[D_{\mathrm{tv}}(\tilde{\pi}_{\tau}(x),\tilde{\pi}_{\tau}(x))\] \[=\frac{1}{2}\sum_{a\in\mathcal{A}}\left\|\mathbb{P}_{\mathcal{P} }\left[\tilde{\mathbf{\pi}}_{\tau}(x)=a\right]-\mathbb{P}_{\mathcal{P}}\left[ \tilde{\mathbf{\pi}}_{\tau}(x)=a\right]\right|,\] \[=\frac{1}{2}\sum_{a\in\mathcal{A}}\left\|\mathbb{P}_{\mathcal{P} }[\mathcal{E}_{x}]\mathbb{P}_{\mathcal{P}}\left[\tilde{\mathbf{\pi}}_{\tau}(x)=a \right.\left|\mathcal{E}_{x}\right]+\mathbb{P}_{\mathcal{P}}[\mathcal{E}_{x}^{ c}]\mathbb{P}_{\mathcal{P}}\left[\tilde{\mathbf{\pi}}_{\tau}(x)=a\right. \left|\mathcal{E}_{x}\right]-\mathbb{P}_{\mathcal{P}}[\mathcal{E}_{x}]\mathbb{ P}_{\mathcal{P}}\left[\tilde{\mathbf{\pi}}_{\tau}(x)=a\right.\left|\mathcal{E}_{x} \right]-\mathbb{P}_{\mathcal{P}}[\mathcal{E}_{x}^{c}]\mathbb{P}_{\mathcal{P}} \left[\tilde{\mathbf{\pi}}_{\tau}(x)=a\right.\left|\mathcal{E}_{x}\right]\right|\] \[\leq\frac{1}{2}\sum_{a\in\mathcal{A}}\mathbb{P}_{\mathcal{P}}[ \mathcal{E}_{x}]\cdot\left\|\mathbb{P}_{\mathcal{P}}\left[\tilde{\mathbf{\pi}}_{ \tau}(x)=a\right.\left|\mathcal{E}_{x}\right]-\mathbb{P}_{\mathcal{P}}\left[ \tilde{\mathbf{\pi}}_{\tau}(x)=a\right.\left|\mathcal{E}_{x}\right]\right|\] \[\quad+\sum_{a\in\mathcal{A}}\mathbb{P}_{\mathcal{P}}[\mathcal{E}_ {x}^{c}]\cdot\left\|\mathbb{P}_{\mathcal{P}}\left[\tilde{\mathbf{\pi}}_{\tau}(x)=a \right.\left|\mathcal{E}_{x}^{c}\right]-\mathbb{P}_{\mathcal{P}}\left[\tilde{ \mathbf{\pi}}_{\tau}(x)=a\right.\left|\mathcal{E}_{x}^{c}\right]\right|,\quad( \text{Jensen's inequality})\]

and since \(\mathbb{P}_{\mathcal{P}}\left[\tilde{\mathbf{\pi}}_{\tau}(x)=a\right.\left| \mathcal{E}_{x}\right]=\mathbb{P}_{\mathcal{P}}\left[\tilde{\mathbf{\pi}}_{\tau}( x)=a\right.\left|\mathcal{E}_{x}\right]\)\(\forall a\in\mathcal{A}\), we have that

\[=\frac{1}{2}\sum_{a\in\mathcal{A}}\mathbb{P}_{\mathcal{P}}[ \mathcal{E}_{x}^{c}]\cdot\left\|\mathbb{P}_{\mathcal{P}}\left[\tilde{\mathbf{\pi}}_{ \tau}(x)=a\right.\left|\mathcal{E}_{x}^{c}\right]-\mathbb{P}_{\mathcal{P}} \left[\tilde{\mathbf{\pi}}_{\tau}(x)=a\right.\left|\mathcal{E}_{x}^{c}\right] \right|,,\] \[\leq\mathbb{P}_{\mathcal{P}}\left[\mathcal{E}_{x}^{c}\right],\] \[=\mathbb{P}_{\mathcal{P}}\left[\max_{a\in\mathcal{A}}|\widetilde {\mathbf{\mathcal{P}}}_{\tau,\varepsilon,\delta}[\widehat{V}_{\tau+1}](x,a)- \mathcal{P}_{\tau}[V_{\tau+1}](x,a)|>4\varepsilon\right],\] \[\leq\mathbb{P}_{\mathcal{P}}\left[\max_{a\in\mathcal{A}}| \widetilde{\mathbf{\mathcal{P}}}_{\tau,\varepsilon,\delta}[\widehat{V}_{\tau+1}](x,a)-\mathcal{P}_{\tau}[\widehat{V}_{\tau+1}](x,a)|>\varepsilon\right],\quad( \text{see below}) \tag{95}\] \[\leq\delta, \tag{96}\]

where (95) follows from \(x\in\mathcal{E}\) and the last inequality follows from Lemma 1.2. Therefore, we have

\[\mathbb{E}_{\mu}[D_{\mathrm{tv}}(\tilde{\pi}_{\tau}(\mathbf{x}_{\tau}),\tilde{\pi}_{\tau}(\mathbf{x}_{\tau}))] \leq\mathbb{P}_{\mu}[\mathcal{E}]\cdot\mathbb{E}_{\mu}[D_{ \mathrm{tv}}(\tilde{\pi}_{\tau}(\mathbf{x}_{\tau}),\tilde{\pi}_{\tau}(\mathbf{x}_{\tau}) )\mid\mathcal{E}]+\mathbb{P}_{\mu}[\mathcal{E}^{c}],\] \[\leq\delta+\nu,\]

where the first inequality follows by the fact that the total variation distance is bounded by \(1\), and the last inequality follows by (94) and (96). 

**Lemma 1.2**.: _Let \(\tau\in[H]\) and \(\varepsilon^{\prime},\delta,\nu\in(0,1)\), and \(\zeta_{1:H}\in[0,1/2]\) be given. Further, consider two value functions \(V_{\tau+1},\widehat{V}_{\tau+1}\in[0,H]\) and measure \(\mu\in\Delta(\mathcal{X})\) such that_

\[\mathbb{P}_{\mathbf{x}_{\tau}\sim\mu}\bigg{[}\mathbb{I}\left\{\max_{ a\in\mathcal{A}}\left|(\mathcal{P}_{\tau}[\widehat{V}_{\tau+1}]- \mathcal{P}_{\tau}[V_{\tau+1}])(\mathbf{x}_{\tau},a)\right|>3\varepsilon^{\prime} \right\}\bigg{]}\leq\nu. \tag{97}\]

_Further, for \(x\in\mathcal{X}_{\tau}\), let \(\bar{\mathbf{\pi}}_{\tau}(x)\in\operatorname*{arg\,max}_{a\in\mathcal{A}}[\widetilde {\mathbf{Q}}_{\tau}(x,a)/\varepsilon^{\prime}+\zeta_{\tau}]\), where \(\widetilde{\mathbf{Q}}_{\tau}(x,a)\coloneqq\widetilde{\mathbf{\mathcal{P}}}_{\tau, \varepsilon^{\prime},\delta}[\widehat{V}_{\tau+1}](x,a)\), and inductively define_

\[\tilde{\mathbf{\pi}}_{\tau}(x)\in\operatorname*{arg\,max}_{a\in\mathcal{A}}\left\{ \begin{array}{ll}\left[\widetilde{\mathbf{Q}}_{\tau}(x,a)/\varepsilon^{\prime}+ \zeta_{\tau}\right],&\text{if }|\widetilde{\mathbf{Q}}_{\tau}(x,\cdot)-\mathcal{P}_{\tau}[V_{\tau+1}](x, \cdot)|_{\infty}\leq 4\varepsilon^{\prime},\\ \left|\mathcal{P}_{\tau}[V_{\tau+1}](x,a)/\varepsilon^{\prime}+\zeta_{\tau} \right],&\text{otherwise}.\end{array}\right.\]

_Then, we have_

\[\mathbb{E}_{\mathbf{x}_{\tau}\sim\mu}[D_{\mathrm{tv}}(\tilde{\pi}_{ \tau}(\mathbf{x}_{\tau}),\tilde{\pi}_{\tau}(\mathbf{x}_{\tau}))]\leq\nu+\delta.\]

**Proof of Lemma 1.2.** In this proof, we let \(\mathbb{P}_{\mu}\) denote the probability law of \(\mathbf{x}_{\tau}\) and \(\mathbb{P}_{\mathcal{P}}\) denote the probability law of \(\widehat{\mathbf{\mathcal{P}}}_{\tau,\varepsilon,\delta}\). Denote by \(\mathcal{E}\) be the \(\mathbb{P}_{\mu}\)-measurable event that \(\max_{a\in\mathcal{A}}\left|(\mathcal{P}_{\tau}[\widehat{V}_{\tau+1}]-\mathcal{P}_ {\tau}[V_{\tau+1}])(\mathbf{x}_{\tau},a)\right|\leq 3\varepsilon^{\prime}\). Fix \(x\in\mathcal{E}\), and let \(\mathcal{E}_{x}\) be the \(\mathbb{P}_{\mathcal{P}}\)-measurable eventthat \(\max_{a\in A}\left|\mathbb{P}_{\tau,\tau^{\prime},\xi}[\widehat{V}_{\tau+1}](x,a)- \mathcal{P}_{\tau}[V_{\tau+1}](x,a)\right|\leq 4\varepsilon^{\prime}\). From the definition of \(\tilde{\pi}_{\tau}\), we have that

\[D_{\mathrm{tv}}(\tilde{\pi}_{\tau}(x),\tilde{\pi}_{\tau}(x))\] \[=\frac{1}{2}\sum_{a\in A}\left|\mathbb{P}_{\mathcal{P}}\left[ \widehat{\pi}_{\tau}(x)=a\right]-\mathbb{P}_{\mathcal{P}}\left[\tilde{\pi}_{ \tau}(x)=a\right]\right|,\] \[=\frac{1}{2}\sum_{a\in A}\left|\mathbb{P}_{\mathcal{P}}[\mathcal{ E}_{x}]\mathbb{P}_{\mathcal{P}}\left[\tilde{\pi}_{\tau}(x)=a\mid\mathcal{E}_{x} \right]+\mathbb{P}_{\mathcal{P}}[\mathcal{E}_{x}^{c}]\mathbb{P}_{\mathcal{P}} \left[\tilde{\pi}_{\tau}(x)=a\mid\mathcal{E}_{x}\right]-\mathbb{P}_{\mathcal{ P}}[\mathcal{E}_{x}]\mathbb{P}_{\mathcal{P}}\left[\tilde{\pi}_{\tau}(x)=a \mid\mathcal{E}_{x}\right]-\mathbb{P}_{\mathcal{P}}[\mathcal{E}_{x}^{c}] \mathbb{P}_{\mathcal{P}}\left[\tilde{\pi}_{\tau}(x)=a\mid\mathcal{E}_{x} \right]\right|\] \[\leq\frac{1}{2}\sum_{a\in A}\mathbb{P}_{\mathcal{P}}[\mathcal{E}_ {x}]\cdot\left|\mathbb{P}_{\mathcal{P}}\left[\tilde{\pi}_{\tau}(x)=a\mid \mathcal{E}_{x}\right]-\mathbb{P}_{\mathcal{P}}\left[\tilde{\pi}_{\tau}(x)=a \mid\mathcal{E}_{x}\right]\right|\] \[\quad+\sum_{a\in A}\mathbb{P}_{\mathcal{P}}[\mathcal{E}_{x}^{c}] \cdot\left|\mathbb{P}_{\mathcal{P}}\left[\tilde{\pi}_{\tau}(x)=a\mid\mathcal{E }_{x}^{c}\right]-\mathbb{P}_{\mathcal{P}}\left[\tilde{\pi}_{\tau}(x)=a\mid \mathcal{E}_{x}^{c}\right]\right|,\quad\text{(Jensen's inequality)}\]

and since \(\mathbb{P}_{\mathcal{P}}\left[\tilde{\pi}_{\tau}(x)=a\mid\mathcal{E}_{x} \right]=\mathbb{P}_{\mathcal{P}}\left[\tilde{\pi}_{\tau}(x)=a\mid\mathcal{E}_{ x}\right]\)\(\forall a\in\mathcal{A}\),

\[=\frac{1}{2}\sum_{a\in A}\mathbb{P}_{\mathcal{P}}[\mathcal{E}_{x} ^{c}]\cdot\left|\mathbb{P}_{\mathcal{P}}\left[\tilde{\pi}_{\tau}(x)=a\mid \mathcal{E}_{x}^{c}\right]-\mathbb{P}_{\mathcal{P}}\left[\tilde{\pi}_{\tau}(x )=a\mid\mathcal{E}_{x}^{c}\right]\right|,\] \[\leq\mathbb{P}_{\mathcal{P}}\left[\mathcal{E}_{x}^{c}\right],\] \[=\mathbb{P}_{\mathcal{P}}\left[\max_{a\in A}\left|\widehat{ \boldsymbol{\mathcal{P}}}_{\tau,\tau^{\prime},\delta}[\widehat{V}_{\tau+1}](x,a)-\mathcal{P}_{\tau}[V_{\tau+1}](x,a)\right|>4\varepsilon^{\prime}\right],\] \[\leq\mathbb{P}_{\mathcal{P}}\left[\max_{a\in A}\left|\widehat{ \boldsymbol{\mathcal{P}}}_{\tau,\tau^{\prime},\delta}[\widehat{V}_{\tau+1}](x,a)-\mathcal{P}_{\tau}[\widehat{V}_{\tau+1}](x,a)\right|>\varepsilon^{\prime} \right],\quad\text{(see below)} \tag{98}\] \[\leq\delta, \tag{99}\]

where (98) follows from \(x\in\mathcal{E}\) and the last inequality follows from Lemma H.2. Therefore, we have

\[\mathbb{E}_{\boldsymbol{x}_{\tau}\sim\mu}\big{[}D_{\mathrm{tv}} \big{(}\tilde{\pi}_{\tau}(\boldsymbol{x}_{\tau}),\tilde{\pi}_{\tau}( \boldsymbol{x}_{\tau})\big{)}\big{]} \leq\mathbb{P}_{\mu}[\mathcal{E}]\cdot\mathbb{E}_{\boldsymbol{x} _{\tau}\sim\mu}\big{[}D_{\mathrm{tv}}\big{(}\tilde{\pi}_{\tau}(\boldsymbol{x} _{\tau}),\tilde{\pi}_{\tau}(\boldsymbol{x}_{\tau})\big{)}\mid\mathcal{E}]+ \mathbb{P}_{\mu}[\mathcal{E}^{c}],\] \[\leq\delta+\nu,\]

where the first inequality follows by the fact that the total variation is bounded by \(1\), and the last inequality follows by (97) and (99). 

**Lemma L.3**.: _Let \(x\in\mathbb{R}\) and \(\nu\in(0,1/2)\) be given. Further, let \(\zeta\in(0,1/2)\). Then,_

\[x+\zeta+\nu>\lceil x+\zeta\rceil\quad\text{or}\quad x+\zeta-\nu\leq\lceil x+ \zeta\rceil-1,\]

_only if_

\[\lceil x\rceil-\nu\leq x+\zeta\leq\lceil x\rceil+\nu\quad\text{or}\quad \zeta\leq\nu.\]

Proof of Lemma l.3.: To prove the claim, it suffices to show the following items:

1. \(x+\zeta+\nu>\lceil x+\zeta\rceil\) only if \(\lceil x\rceil\geq x+\zeta>\lceil x\rceil-\nu\); and
2. \(x+\zeta-\nu\leq\lceil x+\zeta\rceil-1\) only if \(\lceil x\rceil<x+\zeta\leq\lceil x\rceil+\nu\) or \(\zeta\leq\nu\).

We start by showing the first item. We proceed by showing the contrapositive; that is, we will show that if \(x+\zeta\leq\lceil x\rceil-\nu\) or \(x+\zeta>\lceil x\rceil\), then \(x+\zeta+\nu\leq\lceil x+\zeta\rceil\). Suppose that \(x+\zeta\leq\lceil x\rceil-\nu\). This, together with the fact that \(\zeta\geq 0\), implies that

\[\lceil x+\zeta\rceil=\lceil x\rceil\geq x+\zeta+\nu.\]

Now, suppose that \(x+\zeta>\lceil x\rceil\). Then, we have

\[\lceil x+\zeta\rceil\geq\lceil x\rceil+1\geq\lceil x\rceil+\zeta+\nu\geq x+\zeta+\nu,\]

where the penultimate inequality follows by \(\zeta,\nu\in(0,1/2)\).

We now prove the second claim. Again, we proceed by showing the contrapositive; that is, we will show that if \(\{\lceil x\rceil+\nu<x+\zeta\text{ or }\lceil x\rceil\geq x+\zeta\}\) and \(\zeta>\nu\), then \(x+\zeta-\nu>\lceil x+\zeta\rceil-1\).

Suppose that \(\lceil x\rceil+\nu<x+\zeta\) and \(\zeta>\nu\). The first inequality together with \(\nu\geq 0\) implies that \(\lceil x+\zeta\rceil>\lceil x\rceil\). On the other hand, since \(\zeta\leq 1/2\), we have \(\lceil x+\zeta\rceil\leq\lceil x\rceil+1\), and so

\[\lceil x+\zeta\rceil-1=\lceil x\rceil<x+\zeta-\nu,\]where the last inequality follows by the current assumption that \(\lceil x\rceil+\nu<x+\zeta\).

Now, suppose that \(\lceil x\rceil\geq x+\zeta\) and that \(\zeta>\nu\). Then, we have

\[\lceil x+\zeta\rceil\leq\lceil x\rceil\leq x+1<x+\zeta-\nu+1, \tag{100}\]

where the last inequality follows by \(\zeta>\nu\). Rearranging (100) completes the proof. 

## Appendix M BehaviorCloning Algorithm and Analysis

In this section, we give a self-contained presentation and analysis for the standard _behavior cloning_ algorithm for imitation learning (e.g., Ross and Bagnell [46]), displayed in Algorithm 10. Given access to trajectories from an expert policy \(\widehat{\pi}_{1:H}\) (which may be non-executable in the sense of Definition 2.1) the algorithm learns an executable policy \(\pi^{\text{bc}}\) with similar performance. We use this scheme within RVFS.bc and RVFS\({}^{\text{eso}}\).bc.

```
1:input: Policy class \(\Pi\in\Pi_{\text{s}}\), expert policy \(\widehat{\pi}_{1:H}\), suboptimality \(\varepsilon\in(0,1)\), and confidence \(\delta\in(0,1)\).
2: Set \(N_{\text{bc}}=16H^{2}\log(|\Pi|/\delta)/\varepsilon\).
3: Set \(\mathcal{D}\leftarrow\varnothing\).
4:for\(i=1,\ldots,N_{\text{bc}}\)do
5: Generate trajectory \(\boldsymbol{\tau}=((\boldsymbol{x}_{1},\boldsymbol{a}_{1}),\ldots,(\boldsymbol {x}_{H},\boldsymbol{a}_{H}))\sim\mathbb{P}^{\mathbb{F}}\).
6: Update \(\mathcal{D}\leftarrow\mathcal{D}\cup\{\boldsymbol{\tau}\}\).
7: Compute \(\pi^{\text{bc}}\in\arg\min_{\pi\in\Pi}\sum_{((x_{1},a_{1}),\ldots,(x_{H},a_{H} ))\in\mathcal{D}}\sum_{h\in[H]}\mathbb{I}\{\boldsymbol{\pi}_{h}(x_{h})\neq a_{ h}\}\).
8: Return \(\pi^{\text{bc}}\).
```

**Algorithm 10**BehaviorCloning: Imitation Learning Algorithm.

**Proposition M.1**.: _Let \(\varepsilon,\delta\in(0,1)\) be given and let \(\Pi\subseteq\Pi_{\text{s}}\) and \(\widehat{\pi}_{1:H}\) be an expert policy such that_

\[\inf_{\pi\in\Pi}\sum_{h=1}^{H}\mathbb{P}^{\mathbb{F}}[\widehat{ \boldsymbol{\pi}}_{h}(\boldsymbol{x}_{h})\neq\boldsymbol{\pi}_{h}(\boldsymbol {x}_{h})]\leq\varepsilon_{\text{\emph{nis}}}. \tag{101}\]

_Then, the policy \(\pi^{\text{bc}}_{1:H}=\texttt{BehaviorCloning}(\Pi,\varepsilon,\widehat{\pi}_{1 :H},\delta)\) returned by Algorithm 10 satisfies, with probability at least \(1-\delta\),_

\[J(\widehat{\pi})-J(\pi^{\text{bc}})\leq 4H\varepsilon_{\text{\emph{nis}}}+ \varepsilon/2.\]

**Proof of Proposition M.1.** First, by the performance difference lemma, we have

\[\mathbb{E}[V_{1}^{\mathbb{F}}(\boldsymbol{x}_{1})]-\mathbb{E}[V_ {1}^{\pi^{\text{bc}}}(\boldsymbol{x}_{1})] =\sum_{h=1}^{H}\mathbb{E}^{\mathbb{F}}[Q_{h}^{\pi^{\text{bc}}}( \boldsymbol{x}_{h},\widehat{\boldsymbol{\pi}}_{h}(\boldsymbol{x}_{h}))-Q_{h}^ {\pi^{\text{bc}}}(\boldsymbol{x}_{h},\boldsymbol{\pi}^{\text{bc}}_{h}( \boldsymbol{x}_{h}))],\] \[\leq H\sum_{h=1}^{H}\mathbb{P}^{\mathbb{F}}[\widehat{\boldsymbol {\pi}}_{h}(\boldsymbol{x}_{h})+\boldsymbol{\pi}^{\text{bc}}_{h}(\boldsymbol{x} _{h})]. \tag{102}\]

We now bound the probability terms on the right-hand side. Fix \(h\in[H]\) and let \(\mathcal{D}\) be the dataset in Algorithm 10, which consists of \(N_{\text{bc}}\) i.i.d. trajectories \(((\boldsymbol{x}_{1},\boldsymbol{a}_{1}),\ldots,(\boldsymbol{x}_{H}, \boldsymbol{a}_{H}))\) generated by rolling with \(\widehat{\pi}_{1:H}\). By Lemma C.4 (with i.i.d. data, \(B=H\), and \(\mathcal{Q}=\Pi\)), we have that, with probability at least \(1-\delta\),

\[\forall\pi\in\Pi,\quad\sum_{((x_{1},a_{H}),\ldots,(x_{H},a_{H} ))\in\mathcal{D}}\sum_{h\in[H]}\mathbb{I}\{\boldsymbol{\pi}_{h}(x_{h})\neq \widehat{\boldsymbol{\pi}}(x_{h})\} \leq 2\sum_{h\in[H]}\mathbb{P}^{\mathbb{F}}[\boldsymbol{\pi}_{h}( \boldsymbol{x}_{h})\neq\widehat{\boldsymbol{\pi}}_{h}(\boldsymbol{x}_{h})]\] \[\quad+\frac{2H\log(2|\Pi|/\delta)}{N_{\text{bc}}}, \tag{103}\]

and

\[\forall\pi\in\Pi,\quad\sum_{h\in[H]}\mathbb{P}^{\mathbb{F}}[ \boldsymbol{\pi}_{h}(\boldsymbol{x}_{h})\neq\widehat{\boldsymbol{\pi}}_{h}( \boldsymbol{x}_{h})] \leq 2\sum_{((x_{1},a_{H}),\ldots,(x_{H},a_{H}))\in\mathcal{D}}\sum_{h \in[H]}\mathbb{I}\{\boldsymbol{\pi}_{h}(x_{h})\neq\widehat{\boldsymbol{\pi}}( x_{h})\}\] \[\quad+\frac{4H\log(2|\Pi|/\delta)}{N_{\text{bc}}}. \tag{104}\]Taking the infimum over \(\pi\) on both sides of (103) and using the definition of \(\pi_{h}^{\rm bc}\) in Algorithm 10 gives:

\[\sum_{((x_{1},a_{H}),\ldots,(x_{H},a_{H}))\in\mathcal{D}}\sum_{h\in[H ]}\mathbb{I}\{\mathbf{\pi}_{h}^{\rm bc}(x_{h})\neq\mathbf{\widetilde{\pi}}(x_{h})\} \leq 2\inf_{\pi\in\Pi}\sum_{h\in[H]}\mathbb{P}^{\overline{\pi}}[\bm {\pi}_{h}(\mathbf{x}_{h})\neq\mathbf{\widetilde{\pi}}_{h}(\mathbf{x}_{h})]\] \[\quad+\frac{2H\log(2|\Pi|/\delta)}{N_{\rm bc}},\] \[\leq 2\varepsilon_{\texttt{mis}}+\frac{2H\log(2|\Pi|/\delta)}{N_ {\rm bc}},\]

where the last inequality follows from (101). Using this together with (104), instantiated with \(\pi\equiv\pi^{\rm bc}\), we get that with probability at least \(1-\delta\):

\[\sum_{h\in[H]}\mathbb{P}^{\overline{\pi}}[\mathbf{\pi}_{h}^{\rm bc}(\mathbf{x}_{h}) \neq\mathbf{\widetilde{\pi}}_{h}(\mathbf{x}_{h})]\leq 4\varepsilon_{\texttt{mis}}+ \frac{8H\log(2|\Pi|/\delta)}{N_{\rm bc}}.\]

Plugging this into (102), we get that with probability at least \(1-\delta\):

\[\mathbb{E}[V_{1}^{\overline{\pi}}(\mathbf{x}_{1})]-\mathbb{E}[V_{1}^{\pi^{\rm bc}} (\mathbf{x}_{1})]\leq 4H\varepsilon_{\texttt{mis}}+\frac{8H^{2}\log(2|\Pi|/ \delta)}{N_{\rm bc}}\leq 4H\varepsilon_{\texttt{mis}}+\varepsilon/2,\]

where the last inequality follows by the fact that \(N_{\rm bc}=16H^{2}\log(2|\Pi|/\delta)/\varepsilon\). This completes the proof.