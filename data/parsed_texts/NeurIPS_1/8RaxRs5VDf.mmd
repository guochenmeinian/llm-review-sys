# LexEval: A Comprehensive Chinese Legal Benchmark

for Evaluating Large Language Models

 Haitao Li

Department of Computer Science

Tsinghua University

Beijing, China 100000

liht22@mails.tsinghua.edu.cn

&You Chen

Department of Computer Science

Tsinghua University

Beijing, China 100000

chenyou21@mails.tsinghua.edu.cn

Qingyao Ai

Qinyao Li

Department of Computer Science

Tsinghua University

Beijing, China 100000

aiqy@tsinghua.edu.cn

&Yueyue Wu

Qinyao Li

Department of Computer Science

Tsinghua University

Beijing, China 100000

wuyueyue@mail.tsinghua.edu.cn

&Ruizhe Zhang

Department of Computer Science

Tsinghua University

Beijing, China 100000

u@thusaac.com

&Yiqun Liu

Qinyao Li

Department of Computer Science

Tsinghua University

Beijing, China 100000

yiqunliu@tsinghua.edu.cn

Corresponding author.

###### Abstract

Large language models (LLMs) have made significant progress in natural language processing tasks and demonstrate considerable potential in the legal domain. However, legal applications demand high standards of accuracy, reliability, and fairness. Applying existing LLMs to legal systems without careful evaluation of their potential and limitations could pose significant risks in legal practice. To this end, we introduce a standardized comprehensive Chinese legal benchmark LexEval. This benchmark is notable in the following three aspects: (1) Ability Modeling: We propose a new taxonomy of legal cognitive abilities to organize different tasks. (2) Scale: To our knowledge, LexEval is currently the largest Chinese legal evaluation dataset, comprising 23 tasks and 14,150 questions. (3) Data: we utilize formatted existing datasets, exam datasets and newly annotated datasets by legal experts to comprehensively evaluate the various capabilities of LLMs. LexEval not only focuses on the ability of LLMs to apply fundamental legal knowledge but also dedicates efforts to examining the ethical issues involved in their application. We evaluated 38 open-source and commercial LLMs and obtained some interesting findings. The experiments and findings offer valuable insights into the challenges and potential solutions for developing Chinese legal systems and LLM evaluation pipelines. The LexEval dataset and leaderboard are publicly available at [https://github.com/CSHaitao/LexEval](https://github.com/CSHaitao/LexEval) and will be continuously updated.

Introduction

Recently, the rapid development of large language models (LLMs) has brought new opportunities to the research of general artificial intelligence. A series of models (e.g., ChatGPT), with their extensive knowledge and outstanding language processing ability, have demonstrated excellent performance in various language processing tasks such as text generation, machine translation, and dialogue systems [10; 4; 6; 45; 37]. Meanwhile, LLMs have profoundly impacted the work patterns of legal practitioners and the development of the legal field. Recent studies show that the GPT-4 has the ability to pass the U.S. Judicial Exam [23]. By interacting with large language models, lawyers and judges can analyze legal documents more efficiently, obtaining comprehensive and valuable information and judicial advice. This has led to a growing trend among legal practitioners to incorporate LLMs as a vital supportive instrument in legal proceedings[11; 25; 27].

Despite the considerable potential of large language models, there are still concerns about their application in the legal domain [38; 35; 26]. Firstly, unlike human decision-making, which is grounded in professional knowledge and logical reasoning, LLMs derive decisions from patterns and connections extracted from massive amounts of training data. Consequently, these models, predicated on probabilistic frameworks, often fall short of ensuring the reliability and explainability of their output [18]. Additionally, existing research has indicated that LLMs may produce misleading and factually incorrect content [34]. Substandard legal texts or flawed judicial guidance may mislead legal practitioners and increase their workload. Finally, the content generated by LLMs may reflect biases present in the training data, leading to unfair treatment of certain groups or specific events. This may undermine the effectiveness and fairness of judicial proceedings and judgments, bringing considerable systemic risks.

The great potential and inherent risks of LLMs in the legal domain give rise to the urgent need for a standardized and comprehensive benchmark [41; 48]. Such a benchmark is essential to ensure that LLMs meet the high standards required for legal practice, minimizing the risks while maximizing their beneficial impact. Although numerous methods for evaluating the abilities of LLMs have been developed, most focus on assessing their generalist abilities on non-professional or semi-professional texts. These benchmarks provide limited guidance for highly specialized fields such as the legal domain[54; 22; 5]. For instance, the well-known Chinese language model evaluation framework, C-Eval [22], primarily uses test questions from high school and university courses. However, in judicial applications, tasks like case summarization, legal case retrieval, and judgment prediction require LLMs to consider precise legal knowledge and complex legal contexts. These tasks often involves highly specialized elements such as judicial interpretation and reasoning. To the best of our knowledge, existing general evaluation benchmarks are unable to reflect or capture the complexity of judicial cognition and decision-making. Furthermore, some researchers have utilized existing traditional natural language processing datasets to construct benchmarks, such as LawBench [17] and LaiW [13], to evaluate the performance of LLMs in the Chinese legal system. However, traditional datasets are typically designed to test specific capabilities from a computer-centric perspective, which does not always reflect the practical use of LLMs in legal applications. Moreover, these benchmarks often overlook aspects such as legal ethics, which are crucial for ensuring the safe application of LLMs in the legal domain. Also, the evaluation metrics for previous tasks vary significantly, complicating the standardization of model performance measurement. Simply integrating existing datasets cannot provide a standardized and comprehensive evaluation of LLMs' capabilities in the legal domain.

In light of these limitations, we present LexEval: a comprehensive Chinese legal benchmark for evaluating LLMs. LexEval focuses on practical legal applications, involving how legal professionals manage, contemplate, and resolve legal issues. Firstly, to systematically organize various evaluation tasks, we propose a Legal Cognitive Ability Taxonomy (LexAbility Taxonomy), which includes six aspects: Memorization, Understanding, Logic Inference, Discrimination, Generation, and Ethic. This taxonomy comprehensively analyzes various legal tasks and their intrinsic connections, constructing a systematic framework for evaluating LLMs. Then, based on the LexAbility Taxonomy, we collect 14,150 questions covering 23 legal tasks. To our knowledge, LexEval is the largest and most comprehensive Chinese legal benchmarking dataset for evaluating LLMs. Moreover, LexEval is constructed from existing legal datasets (reorganized into a unified format), exam datasets in reality, and manually curated datasets. It adopts standard evaluation methods and metrics, laying a solid groundwork for future expansion and the integration of diverse tasks. It's important to acknowledge that despite its comprehensiveness, LexEval may not cover every practical application task within the legal field. As a platform supporting further research, LexEval encourages individuals to contribute additional tasks to the taxonomy, collectively pushing the boundaries of what's achievable in the field of legal language understanding and generation. We conduct a thorough evaluation of 38 popular LLMs, including General LLMs and Legal-specific LLMs. The experimental results show that the existing LLMs are ineffective and unreliable in addressing legal problems. We hope this benchmark can point out different directions for future work.

## 2 Related Work

In recent years, large language models (LLMs) have drawn great attention in academia and industry for their excellent performance and wide applicability [36; 50; 28]. Models such as ChatGPT and ChatGLM achieve excellent performance across various tasks through mechanisms such as pretraining, supervised fine-tuning, and alignment with human or AI feedback [2; 8; 15; 16]. By learning from massive amounts of text data, LLMs can capture the subtle differences and complex patterns of language, demonstrating the great potential in understanding and generating human language.

However, despite great success, they face significant challenges in the legal domain [32; 7; 14; 31]. In the legal domain, accuracy, reliability, and fairness are crucial, but LLMs often perform poorly in these aspects due to issues like hallucination [32] and inherent biases [52; 9; 29]. Hallucination refers to models generating information that is not based on facts, which can lead to misleading or entirely incorrect conclusions in legal documents and consultations. Additionally, due to biases in the training data, the model may inadvertently replicate and amplify these biases, affecting its fairness and accuracy in applications such as legal judgment prediction, case analysis, and contract review.

To mitigate these issues, the community has proposed a series of evaluation criteria and benchmarks [19; 17; 13; 30]. For example, LegalBench [19] is dedicated to the collaborative evaluation of legal reasoning tasks in English LLMs, consisting of 162 tasks contributed by 40 contributors. Lawbench [17] and LaiW [13] have conducted evaluations on the Chinese legal system using existing traditional natural language processing datasets, contributing to the development of the community. However, these datasets all focus on the partial performance of LLMs and do not provide a comprehensive evaluation. In this paper, we devote to a more comprehensive evaluation of the performance of LLMs in the legal domain. Leveraging the proposed legal cognitive ability taxonomy, we constructed the largest legal benchmark in the Chinese community through various means.

## 3 LexEval

### Design Principle

The motivation behind LexEval is to help developers quickly understand the capabilities of LLMs within the legal domain across multiple dimensions, enabling them to focus on specific areas for enhancement. To this end, we advocate for considering the hierarchy and connections of abilities, rather than organizing evaluations based solely on difficulty or in a discrete manner. Nevertheless,

Figure 1: Overview of the legal cognitive ability taxonomy.

research on the hierarchical abilities of LLMs is still in the early stages, and to our knowledge, there isn't a well-developed taxonomy describing the abilities of LLMs in legal applications[39]. Drawing inspiration from Bloom's taxonomy [24] and real-world legal application scenarios, we propose a legal cognitive ability taxonomy (LexAbility Taxonomy) to guide the organization of tasks in LexEval.

As depicted in Figure 1, the taxonomy categorizes the application of LLMs in the legal domain into six ability levels: Memorization, Understanding, Logic Inference, Discrimination, Generation, and Ethic. At the Memorization level, LLMs are tasked with memorizing and recalling legal information, including fundamental legal statutes, case law, basic legal principles, and specialized legal terminology, among other essential content. Moving to the Understanding level, LLMs must demonstrate an aptitude for comprehending the meaning and implications of legal information. They should possess the ability to interpret legal concepts, texts, and issues accurately. Logical Inference involves the capacity for legal reasoning and deductive logic. LLMs should be capable of deducing conclusions based on provided legal facts and rules, identifying and applying legal patterns and principles effectively. The Discrimination level necessitates LLMs to analyze and evaluate the significance of legal information according to specific criteria. At the Generation level, LLMs are expected to produce professional legal documents and argumentative texts within specific legal contexts. This includes drafting legal writings, contracts, and providing legal opinions. LLMs should generate precise, legally sound, and well-structured texts based on given conditions and requirements. Finally, the Ethics level requires LLMs to make judgments about ethical issues in the legal domain. Models should identify and analyze legal ethical issues, make ethical decisions, and weigh advantages and disadvantages. They must consider ethical principles of law, professional ethics, and social values in their decision-making processes.

Each level contains several specific evaluation tasks corresponding to the respective abilities. Legal practitioners can employ this taxonomy to identify the cognitive levels attained by LLMs, thereby enhancing the planning of training objectives and downstream applications. It is important to note that this legal cognitive ability taxonomy does not imply a linear learning process. During training, the model can be designed to learn back and forth from different tasks at different levels. Different legal tasks may involve multiple levels at the same time, and evaluating model performance at one level also requires synthesis across multiple tasks. As these ability levels in LexEval are developed and refined, LLMs will become increasingly integrated into legal practice, enhancing the efficiency, accuracy, and ethical standards of legal work. This taxonomy not only provides a framework for assessing the current capabilities of LLMs but also guides future advancements in the field. Although the taxonomy is primarily designed for the Chinese legal system, we believe it can be extended to involve other legal tasks in other countries as well, as these ability levels are universal across different legal systems.

### Data Collection and Processing

**Data Source:** The data in LexEval comes from three sources. The first source comprises existing datasets and corpora, primarily including CAIL 2, JEC-QA [53], and LeCaRD [33]. As these resources are originally designed for non-LLM evaluation settings, we standardize the data format and adjust the prediction targets to align with the evaluation objectives of LLMs. The second source originates from the National Uniform Legal Profession Qualification Examination, which is a uniform national examination for assessing qualifications to practice the legal profession in China. We carefully select and adapt exam questions from previous years to suit our evaluation framework. The third task source is expert annotation, where we hire 18 experts in the legal field as annotators to craft precise and relevant evaluation questions. Detailed data sources and licenses can be found in C.

Footnote 2: The data from CAIL can be found on the official website of the competition[http://cail.cipsc.org.cn/](http://cail.cipsc.org.cn/).

**Data Processing:** We collect data in various formats, including PDF, JSON, LaTeX, and Word, among others. By using techniques such as OCR, we first convert PDF and Word documents into textual form. For those questions that are difficult to parse automatically, we process them manually. Subsequently, all questions are converted into structured data using JSON format. All questions (except for the Generation level task) are converted to multiple-choice format. This is because multiple-choice questions have clearly defined metrics (i.e., accuracy) and are also a simple and effective way to evaluate the capabilities of the LLMs, which have been widely used in various benchmarks [17; 13; 22]. Detailed construction processes for each task can be found in AppendixC.3. All questions have been verified by the authors in multiple rounds to ensure their accuracy and reasonableness.

**Data Annotation:** For tasks lacking existing datasets, we hire professional annotators to create entirely new datasets. Our annotation team consists of 18 legal experts who have all passed the National Uniform Legal Profession Qualification Examination. The annotation experts are all from China, of whom 9 are men and 9 are women. Before the beginning of the annotation work, we signed a legally effective agreement with all annotation experts to protect their rights and interests. To ensure the quality of annotation, all annotators first go through several hours of interpretation to understand their respective tasks. After that, we provide several examples to help them understand the format of tasks. The annotator creates the questions and answers according to the appropriate rules and format. Our gold annotators, who hold a Ph.D. in law, cross-check and inspect all generated questions. Before formal annotation, each annotator creates 100 questions and answers corresponding to the task. Subsequently, only annotators who achieve a 90% approved rate through cross-checking and inspection are allowed to annotate formally. We remove questions that are too simple and try to ensure that the distribution of causes is as balanced as possible. For each approved question, we pay the legal expert 0.7 dollars. We have annotated a total of 6,250 questions, with a total payment of 4,375 dollars. Detailed annotation guidance for each task can be found in C.3 and Appendix F.

Built upon the above processing, we finally select and construct 23 evaluation tasks in LexEval. For the existing datasets, we try our best to avoid using datasets that have already been extensively mined by existing LLMs (e.g. C-Eval) so that the risk of test data leakage could be minimized. To ensure the quality of LexEval, we also try to balance the distributions of legal documents from different causes, thereby avoiding bias or long-tail effects in the dataset.

### Task Definition

Based on the legal cognitive ability taxonomy, we construct a series of evaluation tasks. Table 1 shows the overview of tasks in LexEval. These tasks may simultaneously evaluate one or multiple ability levels, and we categorize them based on their primary ability level. Each task has at least 100 evaluation samples. Among these tasks, 11 tasks are derived from existing datasets, 2 tasks come from the National Uniform Legal Profession Qualification Examination, and 10 tasks are annotated by experts. Detailed task definition, construction process, and task statistics can be found in Appendix C.4. Based on these tasks, LexEval not only provides comprehensive coverage of legal knowledge and reasoning ability but also detects issues such as bias and discrimination in legal ethics, providing valuable insights for in-depth evaluation and analysis.

### Legal and Ethical Considerations

Due to the sensitivity of the legal domain, we conducted a thorough review for this benchmark. All open-source datasets we utilized are licensed. LexEval tasks are subject to different licenses. Appendix C.1 provides a summary of the licenses. The authors take full responsibility for any infringement and confirm the authorization of the dataset. Our evaluation task strictly avoids involving the speculation of sensitive information about individuals and the generation of insulting or sensitive statements. In addition, we have carefully screened and filtered the data sets in LexEval for any content that contains personally identifiable information, discriminatory content, explicit, violent, or offensive content. The data set has been ethically reviewed by legal experts. We strongly believe that our benchmarks have a very low risk of negative impact on safety, security, discrimination, surveillance, deception, harassment, human rights, bias, and fairness. Appendix B.2 discusses the potential social impacts.

## 4 Evaluation

In this section, we present the experimental setup, evaluated models, and experimental results.

### Setup

We evaluate the LLMs in both zero-shot and few-shot settings. In the zero-shot setting, the inputs to LLMs are only instructions and queries. In the few-shot setting, we design three different examples for each task. These examples can be found on the GitHub website. When evaluating LLMs, we set the temperature to 0 to minimize the variance introduced by random sampling. For chat LLMs, we reserve the format of their dialog prompts. When the input length exceeds the maximum context length of LLMs, we truncate the input sequence from the middle since the front and end of the input may contain crucial information. The input prompts used during our evaluation can be found in the Appendix C.4. We standardize our evaluation metrics by using Accuracy to evaluate all multiple-choice questions and Rough-L to evaluate tasks at Generation level.

The evaluation metrics for each task can be found in Table 1. We also discuss the limitations of the evaluation metrics in Appendix B.1.

### Evaluated Models

We evaluate a total of 38 popular models, categorized into two main groups: General LLMs and Legal-specific LLMs.

There are 29 General LLMs, including GPT-4 [36], ChatGPT [4], LLaMA-2-7B [44], LLaMA-2-7B-Chat [44], LLaMA-2-13B-Chat [44], ChatGLM-6B [50], ChatGLM-26B [50], ChatGLM-3-6B [50], Baichuan-7B-base [49], Baichuan-13B-base [49], Baichuan-13B-Chat [49], Qwen-7B-chat [1], Qwen-14B-Chat [1], MPT-7B [43], MPT-7B-Instruct [43], XVERSE-13B, InternLM-7B [42], InternLM-7B-Chat [42], Chinese-LLaMA-2-7B [12], Chinese-LLaMA-2-13B [12], TigerBot-Base, Chinese-Alpaca-2-7B [12], GoGPT2-7B, GoGPT2-13B, Ziya-LLaMA-13B [51], Vicuna-v1.3-7B, BELLE-LLAMA-2-13B [3], Alpaca-v1.0-7B, MoSS-Moon-sft [40].

The Legal-specific LLMs include 9 models, which are ChatLaw-13B [11], ChatLaw-33B [11], LexiLaw, Lawyer-LLaMA [21], Wisdom-Interrogatory, LaWGPT-7B-beta1.0, LaWGPT-7B-beta1.1, HanFei [20], Fuzi-Mingcha [46].. The specific description of evaluated models can be found in the Appendix D.

\begin{table}
\begin{tabular}{l l l l l c} \hline Level & ID & Task & Metrics & Data Source & Test Set \\ \hline \multirow{3}{*}{Memorization} & 1-1 & Legal Concept & Accuracy & JEC-QA [53] & 500 \\  & 1-2 & Legal Rule & Accuracy & Expert Annotation & 1000 \\  & 1-3 & Legal Evolution & Accuracy & Expert Annotation & 300 \\ \hline \multirow{4}{*}{Understanding} & 2-1 & Legal Element Recognition & Accuracy & CAIL-2019 & 500 \\  & 2-2 & Legal Fact Verification & Accuracy & Expert Annotation & 300 \\  & 2-3 & Reading Comprehension & Accuracy & CAIL-2021 & 100 \\  & 2-4 & Relation Extraction & Accuracy & CAIL-2022 & 500 \\  & 2-5 & Named-entity Recognition & Accuracy & CAIL-2021 & 500 \\ \hline \multirow{6}{*}{Logic Inference} & 3-1 & Cause Prediction & Accuracy & CAIL-2018 & 1000 \\  & 3-2 & Article Prediction & Accuracy & CAIL-2018 & 1000 \\  & 3-3 & Penalty Prediction & Accuracy & CAIL-2018 & 1000 \\  & 3-4 & Multi-hop Reasoning & Accuracy & Exams & 500 \\  & 3-5 & Legal Calculation & Accuracy & Expert Annotation & 400 \\  & 3-6 & Argument Mining & Accuracy & CAIL-2021 & 500 \\ \hline \multirow{2}{*}{Discrimination} & 4-1 & Similar Case Identification & Accuracy & LeCaRD [33]\&CAIL-2019 & 500 \\  & 4-2 & Document Proofreading & Accuracy & Expert Annotation & 300 \\ \hline \multirow{4}{*}{Generation} & 5-1 & Summary Generation & Rouge-L & CAIL-2020 & 1000 \\  & 5-2 & Judicial Analysis Generation & Rouge-L & Expert Annotation & 1000 \\  & 5-3 & Legal Translation & Rouge-L & Expert Annotation & 250 \\  & 5-4 & Open-ended Question Answering & Rouge-L & Exams & 500 \\ \hline \multirow{2}{*}{Ethic} & 6-1 & Bias and Discrimination & Accuracy & Expert Annotation & 1000 \\  & 6-2 & Morality & Accuracy & Expert Annotation & 1000 \\  & 6-3 & Privacy & Accuracy & Expert Annotation & 500 \\ \hline \end{tabular}
\end{table}
Table 1: Details of tasks within LexEval.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

* Overall, at present, LLMs cannot effectively solve the legal problems under the Chinese legal system. Facing this situation, we strongly call for continuous technological innovation and interdisciplinary cooperation. This will bring about more powerful intelligent legal LLMs and improve the efficiency and quality of legal services.

## 5 Conclusion & Future Work

In this paper, we introduce LexEval, which is the largest comprehensive benchmark for evaluating LLMs in the Chiese Legal Domain. With 14,150 questions covering 6 legal cognitive ability levels in LexEval, we extensively evaluate the ability of 38 common LLMs. We find that current LLMs are unable to provide effective legal assistance, even the high-performing GPT-4 included. We call for more technological innovations and interdisciplinary collaborations to advance the development of legal LLMs. In the future, we will further enrich our benchmarks to achieve a more comprehensive evaluation. Additionally, we will also continue to host competitions to promote the development of legal LLMs. Also, LexEval always welcomes open participation and contributions.

Figure 2: The zero-shot performance of the six best models at different legal cognitive ability levels.

## References

* [1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. _arXiv preprint arXiv:2309.16609_, 2023.
* [2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.
* [3] BELLEGroup. Belle: Be everyone's large language model engine. [https://github.com/LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE), 2023.
* [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [5] Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael Bommarito, Ion Androutsopoulos, Daniel Martin Katz, and Nikolaos Aletras. Lexglue: A benchmark dataset for legal language understanding in english. _arXiv preprint arXiv:2110.00976_, 2021.
* [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. _arXiv preprint arXiv:2107.03374_, 2021.
* [7] Inyoung Cheong, King Xia, KJ Feng, Quan Ze Chen, and Amy X Zhang. (a) i am not a lawyer, but...: Engaging legal experts towards responsible llm policies for legal advice. _arXiv preprint arXiv:2402.01864_, 2024.
* [8] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.
* [9] Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, and Yiqun Liu. Pre: A peer review based large language model evaluator, 2024.
* [10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.
* [11] Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large language model with integrated external knowledge bases, 2023.
* [12] Yiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese llama and alpaca. _arXiv preprint arXiv:2304.08177_, 2023.
* [13] Yongfu Dai, Duanyu Feng, Jimin Huang, Haochen Jia, Qianqian Xie, Yifang Zhang, Weiguang Han, Wei Tian, and Hao Wang. Laiw: A chinese legal large language models benchmark (a technical report). _arXiv preprint arXiv:2310.05620_, 2023.
* [14] Aniket Deroy, Kripabandhu Ghosh, and Saptarshi Ghosh. How ready are pre-trained abstractive models and llms for legal case judgement summarization? _arXiv preprint arXiv:2306.01248_, 2023.
* [15] Qian Dong, Yiding Liu, Qingyao Ai, Haitao Li, Shuaiqiang Wang, Yiqun Liu, Dawei Yin, and Shaoping Ma. I3 retriever: Incorporating implicit interaction in pre-trained language models for passage retrieval. CIKM '23, page 441-451, New York, NY, USA, 2023. Association for Computing Machinery.
* [16] Qian Dong, Yiding Liu, Qingyao Ai, Zhijing Wu, Haitao Li, Yiqun Liu, Shuaiqiang Wang, Dawei Yin, and Shaoping Ma. Unsupervised large language model alignment for information retrieval via contrastive feedback. In _Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR '24, page 48-58, New York, NY, USA, 2024. Association for Computing Machinery.

* [17] Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, and Jidong Ge. Lawbench: Benchmarking legal knowledge of large language models. _arXiv preprint arXiv:2309.16289_, 2023.
* [18] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. _Minds and Machines_, 30:681-694, 2020.
* [19] Neel Guha, Julian Nyarko, Daniel E Ho, Christopher Re, Adam Chilton, Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel N Rockmore, et al. Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models. _arXiv preprint arXiv:2308.11462_, 2023.
* [20] Wanwei He, Jiabao Wen, Lei Zhang, Hao Cheng, Bowen Qin, Yunshui Li, Feng Jiang, Junying Chen, Benyou Wang, and Min Yang. Hanfei-1.0. [https://github.com/siat-nlp/HanFei](https://github.com/siat-nlp/HanFei), 2023.
* [21] Quzhe Huang, Mingxu Tao, Zhenwei An, Chen Zhang, Cong Jiang, Zhibin Chen, Zirui Wu, and Yansong Feng. Lawyer llama technical report. _arXiv preprint arXiv:2305.15062_, 2023.
* [22] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. _arXiv preprint arXiv:2305.08322_, 2023.
* [23] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. Gpt-4 passes the bar exam. _Available at SSRN 4389233_, 2023.
* [24] David R Krathwohl. A revision of bloom's taxonomy: An overview. _Theory into practice_, 41(4):212-218, 2002.
* [25] Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Yueyue Wu, Yiqun Liu, Chong Chen, and Qi Tian. Sailer: Structure-aware pre-trained language model for legal case retrieval. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR '23, page 1035-1044, New York, NY, USA, 2023. Association for Computing Machinery.
* [26] Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu, Chong Chen, and Qi Tian. Blade: Enhancing black-box large language models with small domain-specific models, 2024.
* [27] Haitao Li, Qingyao Ai, Xinyan Han, Jia Chen, Qian Dong, Yiqun Liu, Chong Chen, and Qi Tian. Delta: Pre-train a discriminative encoder for legal case retrieval via structural word alignment, 2024.
* [28] Haitao Li, Qingyao Ai, Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Zheng Liu, and Zhao Cao. Constructing tree-based index for efficient and effective dense retrieval. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR '23, page 131-140, New York, NY, USA, 2023. Association for Computing Machinery.
* [29] Haitao Li, Junjie Chen, Qingyao Ai, Zhumin Chu, Yujia Zhou, Qian Dong, and Yiqun Liu. Calibraeval: Calibrating prediction distribution to mitigate selection bias in llms-as-judges, 2024.
* [30] Haitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yixiao Ma, and Yiqun Liu. Lecardv2: A large-scale chinese legal case retrieval dataset. In _Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR '24, page 2251-2260, New York, NY, USA, 2024. Association for Computing Machinery.
* [31] Haitao Li, Weihang Su, Changyue Wang, Yueyue Wu, Qingyao Ai, and Yiqun Liu. Thuir@coliee 2023: Incorporating structural knowledge into pre-trained language models for legal case retrieval, 2023.
* [32] Zihao Li. The dark side of chatgpt: Legal and ethical challenges from stochastic parrots and hallucination. _arXiv preprint arXiv:2304.14347_, 2023.

* [33] Yixiao Ma, Yunqiu Shao, Yueyue Wu, Yiqun Liu, Ruizhe Zhang, Min Zhang, and Shaoping Ma. Lecard: a legal case retrieval dataset for chinese law system. In _Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval_, pages 2342-2348, 2021.
* [34] Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models, 2023.
* [35] John J. Nay, David Karamardian, Sarah B. Lawsky, Wenting Tao, Meghana Bhat, Raghav Jain, Aaron Travis Lee, Jonathan H. Choi, and Jungo Kasai. Large language models as tax attorneys: A case study in legal capabilities emergence, 2023.
* [36] OpenAI. Gpt-4 technical report, 2023.
* [37] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. _arXiv preprint arXiv:2304.03277_, 2023.
* [38] Jaromir Savelka, Kevin D. Ashley, Morgan A Gray, Hannes Westermann, and Huihui Xu. Can gpt-4 support analysis of textual data in tasks requiring highly specialized domain expertise?, 2023.
* [39] Yunqiu Shao, Haitao Li, Yueyue Wu, Yiqun Liu, Qingyao Ai, Jiaxin Mao, Yixiao Ma, and Shaoping Ma. An intent taxonomy of legal case retrieval. _ACM Trans. Inf. Syst._, 42(2), December 2023.
* [40] Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu. Moss: Training conversational language models from synthetic data. 2023.
* [41] Zhongxiang Sun. A short survey of viewing large language models in legal aspect. _arXiv preprint arXiv:2303.09136_, 2023.
* [42] InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities. [https://github.com/InternLM/InternLM](https://github.com/InternLM/InternLM), 2023.
* [43] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. Accessed: 2023-03-28.
* [44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022.
* [46] Shiguang Wu, Zhongkun Liu, Zhen Zhang, Zheng Chen, Wentao Deng, Wenhao Zhang, Jiyuan Yang, Zhitao Yao, Yougang Lyu, Xin Xin, Shen Gao, Pengjie Ren, Zhaochun Ren, and Zhumin Chen. fuzi.mingcha. [https://github.com/irlab-sdu/fuzi.mingcha](https://github.com/irlab-sdu/fuzi.mingcha), 2023.
* [47] Chaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Yansong Feng, Xianpei Han, Zhen Hu, Heng Wang, and Jianfeng Xu. Cail2018: A large-scale legal dataset for judgment prediction, 2018.
* [48] Xiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, and Jin Ma. T2ranking: A large-scale chinese benchmark for passage ranking. SIGIR '23, page 2681-2690, New York, NY, USA, 2023. Association for Computing Machinery.
* [49] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. _arXiv preprint arXiv:2309.10305_, 2023.

* [50] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. _arXiv preprint arXiv:2210.02414_, 2022.
* [51] Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, and Chongpei Chen. Fengsenbang 1.0: Being the foundation of chinese cognitive intelligence. _CoRR_, abs/2209.02970, 2022.
* [52] Ruizhe Zhang, Haitao Li, Yueyue Wu, Qingyao Ai, Yiqun Liu, Min Zhang, and Shaoping Ma. Evaluation ethics of llms in legal domain, 2024.
* [53] Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. Jec-qa: a legal-domain question answering dataset. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 9701-9708, 2020.
* [54] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. _arXiv preprint arXiv:2304.06364_, 2023.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] See Section 1. 2. Did you describe the limitations of your work? [Yes] See Appendix B.1. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix B.2. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] See Section 3.4.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Appendix A. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? We don't have a training process. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] Because running a massive amount of LLMs is expensive. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix E.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] See Section 3.2. 2. Did you mention the license of the assets? [Yes] See Appendix C.1. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] See Appendix A. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See Appendix C.1. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Appendix 3.4.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] See Section 3.2, Appendix C.4 and Appendix F. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] See Section 3.2.

Availability

* You can access the LexEval website at: [https://collam.megatechai.com/](https://collam.megatechai.com/)
* The Github repository with evaluation code and prompts is available here: [https://github.com/CSHaitao/LexEval](https://github.com/CSHaitao/LexEval)
* Data can be downloaded from here: [https://github.com/CSHaitao/LexEval](https://github.com/CSHaitao/LexEval)

## Appendix B Discussion

In this section, we discuss the limitations and potential impacts.

### Limitation

We acknowledge several limitations that could be addressed in future research. First, the tasks in the dataset mainly cover the Statute Law system, while further in-depth exploration is needed in terms of performance in the Case Law system. There are significant differences between these two legal systems concerning the interpretation of laws and the basis for decisions. Thus, the performance of LLMs may be different under the two legal systems. In the future, we will expand the dataset to cover countries with Case Law system. Another limitation worth noting is the evaluation metrics. In the tasks at the Generation level, we used Rough-L as the main evaluation metric. However, we realize that Rough-L may not be able to fully and accurately present the LLMs' performance in the legal domain. Nevertheless, with 14,150 questions covering 23 tasks, LexEval can reveal the capability level of LLMs to some extent. In the future, we plan to expand the dataset to cover countries with the Case Law system and introduce more tasks and more dimensional evaluation metrics based on the proposed legal cognitive ability taxonomy.

### Broader Impact

LexEval endeavors to achieve a comprehensive evaluation of the performance of LLMs in the legal domain and further advance the development of LLMs. Our proposed legal cognitive ability taxonomy and the corresponding tasks provide a solid foundation for follow-up work. The widespread application of LLMs in the legal domain may affect the way the legal profession works. This may involve changes in how legal practitioners use these technological tools, adjustments in legal training, and changes in the practice of the legal profession. We will pay close attention to the impact of the LLMs on the legal domain to ensure that it does not undermine the principles of social justice and the rule of law. Furthermore, the construction and utilization of the dataset will be subject to a detailed and transparent ethical review, and impartiality and fairness will be ensured through a wide range of relevant stakeholder engagement.

It is worth noting that the introduction of LexEval does not mean that we encourage LLMs to completely replace legal professionals in legal practice. On the contrary, we emphasize the uniqueness and complexity of legal judgment, which requires rich professional knowledge, experience, and humanistic insight. Our goal is to help legal professionals better understand and evaluate the performance of large language models in legal tasks by providing standardized evaluation data and methods, so as to make more informed decisions on when, where, and how to use these technologies. In addition, in the application of AI technology, moral and ethical issues cannot be ignored. We hope to reveal the possible illusions and biases of large language models in legal practice through rigorous evaluation, to encourage legal practitioners to be more cautious and responsible when using these technologies. Finally, as a benchmark, Nothing in LexEval can be taken as legal advice. We are well aware of the complexity and diversity of legal practice, so we emphasize that the evaluation results of LexEval are only for reference, not the sole basis for decision-making. When applying large models in real-world legal scenarios, more in-depth or specific evaluations are still needed to ensure the legitimacy and rationality of decisions. We expect LexEval to become an essential support for the application of AI technology in the legal field, contributing to the construction of a more just, efficient, and intelligent legal system!Task Details

### License

The release and use of LexEval are subject to the license terms from multiple sources. We hereby explicitly state the copyright and licensing status of each part to enable users to utilize this dataset in a legal and compliant manner.

* For tasks where sources are pre-existing datasets, we have collected and retained the license information of the original datasets in detail. Users must strictly comply with the copyright and license requirements of the original datasets when using these adapted data. Specifically, the JEC-QA [53], LeCaRD [33], and CAIL2018 [47] datasets follow the MIT license agreement. For the CAIL2019, CAIL2020, CAIL2021, and CAIL2022 datasets, we have obtained official authorization from the CAIL competition organizing committee. In short, for all the previously released datasets included in LexEval, we obtained consent from the respective authors. This section includes tasks 1-1, 2-1, 2-3, 2-4, 2-5, 3-1, 3-2, 3-3, 3-6, 4-1, and 5-1.
* This dataset contains National Uniform Legal Profession Qualification Examination data, which is publicly available. The copyright of these data belongs to the respective government agencies, but they have been made public and allowed for public use. Users are required to comply with relevant laws, regulations, and government agency provisions when using these data. This section includes tasks 3-4 and 5-4.
* For the portions of the dataset that are annotated by legal experts, we have signed agreements with the annotators before the annotation process, clarifying that the ownership of the annotated data belongs to us and allowing us to publish and use them. The annotators are responsible for the quality of their annotations, but they do not assume any responsibility for any results arising from the use of these data. These tasks include 1-2, 1-3, 2-2, 3-5, 4-2, 5-2, 5-3, 6-1, 6-2, and 6-3.

The overall release of this dataset adopts the MIT License. If you think that our dataset contains your copyrighted work, you can contact us at any time to request its removal from LexEval.

### Task Statistics

Table 6 presents the statistical information of each task in LexEval. We list the number of samples for each task and the average length of queries (in characters). The average length of tasks in LexEval ranges from 100 to 4000 characters, adequately simulating the various situations that might be encountered in real-world applications. Notably, the longest task is Similar Case Identification, with an average length of 4502 characters, which exceeds the maximum input length of some LLMs. This is due to the typically lengthy nature of legal documents, reflecting the potential challenges large models may face when applied in the legal domain.

### Task Definition and Construction Process

Based on the legal cognitive ability taxonomy, we constructed a series of evaluation tasks. These tasks may simultaneously evaluate one or multiple ability levels, and we categorize them based on their primary ability level. To help enhance understanding of the tasks, we provide the prompt and an example of each task in C.4. Next, we detail the definition and construction process for each task. For each manually crafted task, we outline the basic annotation approach in this section. Detailed annotation guidelines are provided in F.

#### c.3.1 Memorization

Tasks at the Memorization level evaluate the ability to remember basic legal concepts and legal rules. Excellent memorization ability provides a solid foundation for advanced cognitive abilities. This section includes three tasks:

* **Legal Concepts (1-1)**: Legal concepts refer to the fundamental notions, principles, and rules used to explain and apply laws. These concepts have specific meanings in legal contexts and are not commonly used in daily lives. Given a legal concept, LLMs are required to provide an accurate definition or explanation. The Legal concepts task is derived from the JEC-QA [53] dataset, which is a multiple-choice dataset in the field of Chinese law. For this task, we randomly selected 500 knowledge-driven questions from the JEC-QA dataset. These questions encompass a wide range of legal concepts, covering areas such as civil law, criminal law, and administrative law.
* **Legal Rules (1-2)**  Legal rules are usually legal articles that have been formulated and formally announced through the legislative process. They have clear and specific regulations that provide an authoritative basis for the functioning of legal systems. Given an article number or description, LLMs are required to select the specific content of the article. Legal rules task is constructed by legal experts based on the Chinese Criminal Law and Civil Code.
* **Legal Evolution (1-3)**  Legal evolution is the process by which the legal system develops and changes over time, involving changes in the form, content, and interpretation of the law. This evolutionary process significantly influences the understanding and application of legal texts. Remembering legal evolution can assist LLMs in better understanding and applying the law, ensuring fairness and consistency in legal proceedings. Given a period or description, the LLMs should be able to describe the change of laws in the period. The legal evolution task involves questions annotated by legal experts based on the revision records of legal articles throughout Chinese history. These questions are designed to test LLMs' ability to track and explain the changes in laws over specific periods.

#### c.3.2 Understanding

Tasks at the Understanding level examine the ability to comprehend and interpret facts, entities, concepts, and relationships in legal texts, which serves as a foundational requirement for applying knowledge to downstream tasks. We construct five tasks at this level:

* **Legal Element Recognition (2-1)** Legal elements are key components within legal texts that influence the interpretation and application of the law. When handling legal cases or resolving legal issues, these elements provide the foundation for interpreting legal provisions. Understanding and analyzing legal elements can assist LLMs in determining whether events comply with legal regulations and whether specific provisions are applicable. Given a legal text, the LLMs need

\begin{table}
\begin{tabular}{l c l c} \hline \hline Level & ID & Task & Number of Samples & Mean Length \\ \hline \multirow{4}{*}{Memorization} & 1-1 & Legal Concept & 500 & 182 \\  & 1-2 & Legal Rule & 1000 & 303 \\  & 1-3 & Legal Evolution & 300 & 158 \\ \hline \multirow{4}{*}{Understanding} & 2-1 & Legal Element Recognition & 500 & 175 \\  & 2-2 & Legal Fact Verification & 300 & 811 \\  & 2-3 & Reading Comprehension & 100 & 780 \\  & 2-4 & Relation Extraction & 500 & 349 \\  & 2-5 & Named-entity Recognition & 500 & 610 \\ \hline \multirow{4}{*}{Logic Inference} & 3-1 & Cause Prediction & 1000 & 1003 \\  & 3-2 & Article Prediction & 1000 & 846 \\  & 3-3 & Penalty Prediction & 1000 & 436 \\  & 3-4 & Multi-hop Reasoning & 500 & 262 \\  & 3-5 & Legal Calculation & 400 & 199 \\  & 3-6 & Argument Mining & 500 & 1467 \\ \hline \multirow{2}{*}{Discrimination} & 4-1 & Similar Case Identification & 500 & 4502 \\  & 4-2 & Document Proofreading & 300 & 301 \\ \hline \multirow{4}{*}{Generation} & 5-1 & Summary Generation & 1000 & 1809 \\  & 5-2 & Judicial Analysis Generation & 1000 & 2775 \\  & 5-3 & Legal Translation & 250 & 169 \\  & 5-4 & Open-ended Question Answering & 500 & 697 \\ \hline \multirow{2}{*}{Ethic} & 6-1 & Bias and Discrimination & 1000 & 163 \\  & 6-2 & Morality & 1000 & 159 \\  & 6-3 & Privacy & 500 & 177 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Statistical information on tasks.

to recognize its legal elements. The data of is sourced from the element recognition task in the CAIL2019 competition. In the source data, each sentence is annotated with corresponding element labels. We have transformed the original multi-classification task into multiple-choice questions, with the element labels serving as correct options, while incorrect options are randomly chosen from labels unrelated to the context. All questions for this task have ultimately undergone review by legal experts.
* **Legal Fact Verification (2-2)** Legal fact verification refers to the process of confirming and validating relevant facts in legal proceedings. In legal cases, the relevance and authenticity of evidence are crucial to the judgement and decision. Legal fact verification provides the foundation for supporting court decisions and assists in establishing the facts of a case. The LLMs need to identify the correct and logically inferred facts based on the given evidence. This dataset is annotated by legal experts, and each question includes a paragraph of evidence. Legal experts provide the facts logically inferred from the evidence as the correct options, while facts that cannot be inferred or are incorrect are presented as incorrect options.
* **Reading Comprehension (2-3)** Legal documents contain a wealth of information about the case, such as time, place, and relationships. By reading and understanding Legal documents through LLMs, people can obtain the needed information more efficiently. LLMs are required to answer questions based on the provided legal text, offering accurate and detailed responses. The data for this task is derived from the reading comprehension task in the CAIL2021 competition. Each question includes a passage of legal text, and the model is required to combine multiple segments from the passage to formulate the final answer. We use the answers provided in the source data as the correct option and generate incorrect options using ChatGPT. All questions have been reviewed by legal experts to ensure accuracy.
* **Relation Extraction (2-4)** Relation extraction primarily involves automatically identifying and extracting specific types of legal relationship triples. These triples typically consist of entities, such as parties involved in a legal dispute or transaction, and the type of relationship between them, such as "defendant accused of committing a crime" or "employer-employee relationship." LLMs need to identify all legal relationships based on the given legal text. By accurately extracting legal relationships, LLMs can assist in various legal tasks, such as case law analysis, contract review, and legal research. This data is sourced from the event detection task in CAIL2022. We provide all possible relationship categories to the LLMs in the prompt. The correct relationship triples from the original dataset are presented as correct options, while incorrect relationship triples are generated using ChatGPT. All questions have been reviewed by legal experts to ensure accuracy.
* **Named-entity Recognition (2-5)** Named-entity recognition in legal texts primarily involves the precise extraction of key case information (e.g., suspects, victims, amount of money, etc.). Given a legal text, LLMs need to extract all the entities and determine the entity types. This data is sourced from the information extraction task in CAIL2021. All correct entities form the correct options, while incorrect options are generated using ChatGPT. All questions have been reviewed by legal experts to ensure accuracy.

#### c.3.3 Logic Inference

Tasks at the Logic Inference level require LLMs to make inferences about information, understand internal logic, and draw correct conclusions. These tasks simulate real-world challenges that LLMs may face in legal applications. A total of six tasks are included in this section:

* **Cause Prediction (3-1)** The cause refers to the case type formed by the national legal system summarizing the nature of the legal relationships involved in legal cases. Accurately predicting the cause of action helps to improve judicial efficiency and fairness. The LLMs need to infer possible cause types based on the given case description and relevant background information. This task's data originates from the cause prediction task in CAIL2018. We have reformatted the original classification task into multiple-choice questions. The questions contain a legal case, where the correct options represent the causes involved in this case, while incorrect options are randomly selected from other causes. All questions have been reviewed by legal experts to ensure accuracy.
* **Article Prediction (3-2)** Legal articles are textual expressions of legal norms, rules, and regulations that have a clear meaning and legal effect. In this task, LLMs involve inferring the possible legal articles based on a given case description. This data is sourced from the article provision prediction task in CAIL2018. We have reformatted the original classification task into multiple-choice questions. The questions contain a legal case, where the correct options represent the articles involved in this case, while incorrect options are randomly selected from irrelevant articles. All questions have been reviewed by legal experts to ensure accuracy.
* **Penalty Prediction (3-3)** Penalty prediction refers to the process of predicting and estimating the possible penalties that a defendant may face in the criminal justice process, depending on the facts of the case, legal rules, and similar cases. Given a case description, LLMs need to consider a variety of factors to make a reasonable prediction about the penalties. This data is sourced from the penalty provision prediction task in CAIL2018. We have reformatted the original classification task into multiple-choice questions. All questions have the same five options: 0-10 years, 10-25 years, 25-80 years, Life imprisonment, and Death penalty.
* **Multi-hop Reasoning (3-4)** Legal multi-hop reasoning is the process of deducing a conclusion step by step from a premise or fact, which involves multiple logical steps and chains of reasoning. The LLMs need to perform multiple inference steps to solve the problem based on the given contextual information. This data is sourced from the National Uniform Legal Profession Qualification Examination. We carefully selected multi-step reasoning questions to form this task.
* **Legal Calculation (3-5)** Legal calculation refers to the process of calculating the legal period and the amount of money and other quantifiable aspects based on the related legal rules, by using tools and techniques such as mathematics and statistics. The LLMs need to perform calculations to solve a specific legal problem based on a given legal text and related information. This task is created by legal experts. The legal expert is asked to specify questions involving legal calculations based on specific laws and give the correct and incorrect options. All questions have been reviewed by legal experts to ensure accuracy.
* **Argument Mining (3-6)** During the trial process in court, the plaintiff and the defendant may form different arguments, due to differences in perspectives or inconsistencies in factual statements. Such arguments are the key to solve the trial. LLMs need to extract valuable arguments from massive amounts of legal text to provide support for case analysis. This data is sourced from the debate comprehension task in CAIL2021. The correct options represent the defense argument corresponding to the plaintiff's statements, while the incorrect options are other irrelevant statements from the source data. All questions have been reviewed by legal experts to ensure accuracy.

#### c.3.4 Discrimination

Tasks at the Discrimination level examine whether LLMs can judge the value of legal information based on certain criteria. This level involves critical thinking and evaluation of information and requires LLMs to be able to use knowledge to make effective judgments and decisions. There are two tasks in this section:

* **Similar Case Identification (4-1)** Similar case identification can provide powerful legal grounds and references for legal judgment, which has an important impact on judicial justice. Given a query case, the models need to determine the most relevant case to the query case from the candidate list. This data is sourced from LeCaRD [33]. The correct options are cases annotated in the source data that are relevant to the query case, while the incorrect options are irrelevant cases randomly selected from the candidate pool. All questions have been reviewed by legal experts to ensure accuracy.
* **Document Proofreading (4-2)** Legal case documents have strict requirements for the accuracy of the textual content. Given a legal text, LLMs need to identify and correct errors in it. This task is curated by legal experts. The queries involve erroneous legal texts, where the correct options point out the corresponding errors, while the incorrect options contain problematic corrections.

#### c.3.5 Generation

Tasks at the Generation level require LLMs to generate legal texts with given requirements and formats. We construct four tasks at this level:

* **Summary Generation (5-1)** Summary Generation refers to the process of condensing and summarizing legal documents, judgments, or legal cases into concise and informative abstract texts. Legal summaries typically include essential elements of the case, such as core facts, disputed points, legal issues, legal application, and the judgment outcome, aiming to provide a quick understanding and 

[MISSING_PAGE_FAIL:20]

legal benchmark comprising 162 tasks contributed by 40 contributors. LAIW and LawBench have restructured traditional Chinese natural language datasets to advance the legal evaluation community.

## Appendix D Details of Evaluated Models

There are 29 General LLMs, including GPT-4 [36], ChatGPT [4], LLaMA-2-7B [44], LLaMA-2-7B-Chat [44], LLaMA-2-13B-Chat [44], ChatGLM-6B [50], ChatGLM2-6B [50], ChatGLM3-6B [50], Baichuan-7B-base [49], Baichuan-13B-base [49], Baichuan-13B-Chat [49], Qwen-7B-chat [1], Qwen-14B-Chat [1], MPT-7B [43], MPT-7B-Instruct [43], XVERSE-13B, InternLM-7B [42], InternLM-7B-Chat [42], Chinese-LLaMA-2-7B [12], Chinese-LLaMA-2-13B [12], TigerBot-Base, Chinese-Alpaca-2-7B [12], GoGPT2-7B, GoGPT2-13B, Ziya-LLaMA-13B [51], Vicuna-v1.3-7B, BELLE-LLAMA-2-13B [3], Alpaca-v1.0-7B, MoSS-Moon-sft [40].

The Legal-specific LLMs include 9 models, which are ChatLaw-13B [11], ChatLaw-33B [11], LexiLaw, Lawyer-LLaMA [21], Wisdom-Interrogatory, LaWGPT-7B-beta1.0, LaWGPT-7B-beta1.1, HanFei [20], Fuzi-Mingcha [46].

Table 31 presents the features of the evaluated models utilized in the experiment. These features include the model type, size, maximum sequence length, accessibility for making inferences, and the corresponding website URL.

## Appendix E More Evaluation Result

Due to the length limitations of the paper, a series of specific results are not fully presented. In this section, we provide a detailed list of performance for each model. Specifically, Tables 32 and 33 show the performance in the zero-shot setting. Tables 34 and 35 demonstrate the performance in the few-shot setting. In the future, we will continue to evaluate the latest models to provide more comprehensive results. All evaluation experiments were conducted on an Ubuntu server equipped with a 128-core Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz and 8 NVIDIA A100 SXM 80GB GPUs. Additionally, the CUDA version was 11.7, the Python version was 3.9.0, the PyTorch version was 2.0.0, and the transformers version was 4.28.1.

[MISSING_PAGE_FAIL:22]

**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation.

**Query:** Please select all the legal elements contained in the following text. The defendant acknowledges spending 35,000 yuan on home renovation. The legal elements included are:

A: Compensation for damages

B: Monthly payment of alimony

C: Having children after marriage

D: Joint marital property

Answer:

**Answer:** D

\begin{table}
\begin{tabular}{p{284.5pt}} \hline
**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation. \\ \hline
**Query:** Which of the following statements about the evolution of the law are correct? \\ A: The provisions of the age of responsibility in China’s criminal law have not undergone modification. \\ B: The age of responsibility provisions in the 1979 and 1997 Criminal Laws are basically the same. \\ C: Amendment (XI) to the Criminal Law lowered the age of responsibility to 12 years old. \\ D: The 1997 Criminal Law lowered the age of responsibility to 14 years. \\ Answer: \\ \hline
**Answer:** BC \\ \hline \end{tabular}
\end{table}
Table 10: The instruction and an example of Task 1-3 Legal Evolution.

\begin{table}
\begin{tabular}{p{284.5pt}} \hline
**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation. \\ \hline
**Query:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation. \\ \hline
**Query:** Article 645 of the Civil Law of the People’s Republic of China is:

A: The rights and obligations of the parties to an auction, as well as the auction procedures, etc., shall be in accordance with the provisions of the relevant laws and administrative regulations \\ B: After a divorce, if the children are to be directly supported by one party, the other party shall bear part or all of the maintenance expenses \\ C: One party, with the consent of the other party, may assign his or her rights and obligations under the contract to the third party as well \\ D: Owners or other rights holders have the right to recover lost objects \\ Answer: \\ \hline
**Answer:** A \\ \hline \end{tabular}
\end{table}
Table 9: The instruction and an example of Task 1-2 Legal Rule.

**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation.

**Query:** Please select the correct facts from the options according to the content of the evidence paragraph. Evidence Paragraph: The Plaintiff, in support of its litigation claim, provided the following evidence to the court: Exhibit 1, Vocational Education Garden General Issue No. 10, which intends to confirm that the Plaintiff enjoys the copyright of "Business and Vocational Fugue"; Exhibit 2, four photographs, which intends to confirm that "Business and Vocational Fugue" was engraved on a stone, and then wiped away; Exhibit 3, a notary's certificate, which intends to confirm that there was no signature of the Plaintiff on "Business and Vocational Fugue" before the lawsuit was filed; Exhibit 4, a stone present photographs, which are intended to establish that Defendant leveled the stone by the end of December 2015 after Plaintiff filed suit. The defendant for the evidence provided by the plaintiff, issued the following cross-examination: 1, to evidence one, vocational education garden is an internal publication, only for internal study, does not belong to the external publication, the plaintiff's "industrial and commercial vocational college foo" has never been published externally; 2, no objection to evidence two and three; 3, to evidence four, authenticity is not objected to, the stone book will be removed is based on the needs of the school construction. The defendant did not submit evidence to this court.

**A:** The plaintiff's "Industrial and Commercial Vocational College Fugue" was only published in the defendant-sponsored school magazine "Vocational Education Garden", which was an internal publication, not for public distribution, with limited influence

**B:** The defendant repeatedly erased the plaintiff's signature when using the "Industrial and Commercial Vocational College Fugue" had been the plaintiff's prior consent

**C:** The work was completed in the use of breaks, which was an individual's work

**D:** The defendant reprinted and published the plaintiff's "Industrial and Commercial Vocational College Fugue" into a book, which was a profit-making activity

Answer:

**Answer:** A

**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation.

**Query:** The trial found that in 2007, Mr. Li X3 was sued by Haotian Company for a contract dispute and the case was brought to trial at the Yuelu District Court. On December 15, 2011, the Yuelu District Court issued Civil Judgment No. (2007) Yue 72 Chu Zi No. 0555, ruling: 1. Mr. Li X3 shall pay Haotian Company a one-time payment of RMB 315,400 for the decoration project within three days from the effective date of this judgment (...) Later, the case was sent back for retrial by the Changsha Intermediate People's Court. After retrial by the Yuelu District Court, the judgment was as follows: 1. Mr. Li X3 shall pay Haotian Company RMB 80,000 for the project within three days from the effective date of the judgment, and shall pay interest based on the actual amount owed, calculated at the People's Bank's current loan interest rate from November 29, 2007, until the date of full payment; 2. Reject other litigation claims of Haotian Company. Both Mr. Li X3 and Haotian Company were dissatisfied with this judgment and appealed to the Changsha Intermediate People's Court, which made a final judgment on August 12, 2015: dismissing the appeal and upholding the original judgment. (...) The above facts were stated by the parties in court, and the evidence submitted by the plaintiff and proved in court was recognized by this court. What kind of payment is the defendant ordered to pay in the first-instance judgment?

**A:** Liquidated damages

**B:** Attorney's fees or other costs

**C:** Penalties or compensation payments

**D:** Payment for work, interest

Answer:

**Answer:** D

\begin{table}
\begin{tabular}{l} \hline
**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation. \\ \hline
**Query:** The trial found that in 2007, Mr. Li X3 was sued by Haotian Company for a contract dispute and the case was brought to trial at the Yuelu District Court. On December 15, 2011, the Yuelu District Court issued Civil Judgment No. (2007) Yue 72 Chu Zi No. 0555, ruling: 1. Mr. Li X3 shall pay Haotian Company a one-time payment of RMB 315,400 for the decoration project within three days from the effective date of this judgment (...) Later, the case was sent back for retrial by the Changsha Intermediate People's Court. After retrial by the Yuelu District Court, the judgment was as follows: 1. Mr. Li X3 shall pay Haotian Company RMB 80,000 for the project within three days from the effective date of the judgment, and shall pay interest based on the actual amount owed, calculated at the People’s Bank’s current loan interest rate from November 29, 2007, until the date of full payment; 2. Reject other litigation claims of Haotian Company. Both Mr. Li X3 and Haotian Company were dissatisfied with this judgment and appealed to the Changsha Intermediate People’s Court, which made a final judgment on August 12, 2015: dismissing the appeal and upholding the original judgment. (...) The above facts were stated by the parties in court, and the evidence submitted by the plaintiff and proved in court was recognized by this court. What kind of payment is the defendant ordered to pay in the first-instance judgment?

**A:** Liquidated damages

**B:** Attorney’s fees or other costs

**C:** Penalties or compensation payments

**D:** Payment for work, interest

Answer:

\begin{table}
\begin{tabular}{l} \hline
**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation. \\ \hline
**Query:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation. \\ \hline
**Query:** The trial found that in 2007, Mr. Li X3 was sued by Haotian Company for a contract dispute and the case was brought to trial at the Yuelu District Court. On December 15, 2011, the Yuelu District Court issued Civil Judgment No. (2007) Yue 72 Chu Zi No. 0555, ruling: 1. Mr. Li X3 shall pay Haotian Company a one-time payment of RMB 315,400 for the decoration project within three days from the effective date of this judgment (...) Later, the case was sent back for retrial by the Changsha Intermediate People’s Court. After retrial by the Yuelu District Court, the judgment was as follows: 1. Mr. Li X3 shall pay Haotian Company RMB 80,000 for the project within three days from the effective date of the judgment, and shall pay interest based on the actual amount owed, calculated at the People’s Bank’s current loan interest rate from November 29, 2007, until the date of full payment; 2. Reject other litigation claims of Haotian Company. Both Mr. Li X3 and Haotian Company were dissatisfied with this judgment and appealed to the Changsha Intermediate People’s Court, which made a final judgment on August 12, 2015: dismissing the appeal and upholding the original judgment. (...) The above facts were stated by the parties in court, and the evidence submitted by the plaintiff and proved in court was recognized by this court. What kind of payment is the defendant ordered to pay in the first-instance judgment?

**A:** Liquidated damages

**B:** Attorney’s fees or other costs

**C:** Penalties or compensation payments

**D:** Payment for work, interest

Answer:

\begin{table}
\begin{tabular}{l} \hline
**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation. \\ \hline
**Query:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation. \\ \hline
**Query:** The trial found that in 2007, Mr. Li X3 was sued by Haotian Company for a contract dispute and the case was brought to trial at the Yuelu District Court. On December 15, 2011, the Yuelu District Court issued Civil Judgment No. (2007) Yue 72 Chu Zi No. 0555, ruling: 1. Mr. Li X3 shall pay Haotian Company a one-time payment of RMB 315,400 for the decoration project within three days from the effective date of this judgment (...) Later, the case was sent back for retrial by the Changsha Intermediate People's Court. After retrial by the Yuelu District Court, the judgment was as follows: 1. Mr. Li X3 shall pay Haotian Company RMB 80,000 for the project within three days from the effective date of the judgment, and shall pay interest based on the actual amount owed, calculated at the People's Bank's current loan interest rate from November 29, 2007, until the date of full payment; 2. Reject other litigation claims of Haotian Company. Both Mr. Li X3 and Haotian Company were dissatisfied with this judgment and appealed to the Changsha Intermediate People’s Court, which made a final judgment on August 12, 2015: dismissing the appeal and upholding the original judgment. (...) The above facts were stated by the parties in court, and the evidence submitted by the plaintiff and proved in court was recognized by this court. What kind of payment is the defendant ordered to pay in the first-instance judgment?

**A:** Liquidated damages

**B:** Attorney’s fees or other costs

**C:** Penalties or compensation payments

**D:** Payment for work, interest

Answer:

\begin{table}
\begin{tabular}{l} \hline
**Answer:** D \\ \hline

**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation.

**Query:** Please extract all relationship triplets from the given input based on the relationship list. The relationship list includes: trafficking (to a person), trafficking (drugs), possession, illegal detention. The People's Procuratorate of Funan County accused that during June and August 2014, the defendant Zhao invited Ma twice to No. 97 Jiaoyang Road, Lucheng Town, Funan County, to use drugs, with drugs and drug paraphernalia provided by the defendant Zhao. The options are as follows:

A: (Zhao, possession, Ma)

B: (Zhao, illegal detention, Ma)

C: (Ma, illegal detention, Zhao)

D: (Zhao, trafficking (to a person), Ma)

Answer: B

**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation.

**Query:** Please extract all entities from the given input and determine their entity types. The entity type list includes: criminal suspect, victim, stolen currency, item value, theft proceeds, stolen items, tools used in the crime, time, location, organizational institution. Input text: On August 28, 2018, the defendant Li was apprehended by the victim Mou and their relatives at the vegetable market in ** Village, Dadukou District, and was brought to the public security organ. After being apprehended, the defendant confessed to the crime of theft truthfully. The options are as follows:

A: (Theft proceeds: public security organ), (Victim: Mou), (Location: vegetable market in ** Village, Dadukou District), (Organizational institution: public security organ)

B: (Criminal suspect: Li), (Victim: Mou), (Location: vegetable market in ** Village, Dadukou District), (Organizational institution: public security organ)

C: (Stolen currency: vegetable market in ** Village, Dadukou District), (Victim: Mou), (Location: vegetable market in ** Village, Dadukou District), (Organizational institution: public security organ)

D: (Item value: public security organ), (Victim: Mou), (Location: vegetable market in ** Village, Dadukou District), (Organizational institution: public security organ)

Answer: B

\begin{table}
\begin{tabular}{l} \hline \hline
**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation. \\ \hline
**Query:** Please extract all relationship triplets from the given input based on the relationship list. The relationship list includes: trafficking (to a person), trafficking (drugs), possession, illegal detention. The People’s Procuratorate of Funan County accused that during June and August 2014, the defendant Zhao invited Ma twice to No. 97 Jiaoyang Road, Lucheng Town, Funan County, to use drugs, with drugs and drug paraphernalia provided by the defendant Zhao. The options are as follows:

A: (Zhao, possession, Ma)

B: (Zhao, illegal detention, Ma)

C: (Ma, illegal detention, Zhao)

D: (Zhao, trafficking (to a person), Ma)

Answer: B \\ \hline \hline \end{tabular}
\end{table}
Table 14: The instruction and an example of Task 2-4 Relation Extraction.

**Instruction:** Please read the following multiple choice questions and give the correct answer.

**Query:** The People's Procuratorate of Zhonglou District, Changzhou City, charges that the defendant, Zhang, on the afternoon of November 13, 2016, in Room 305, Unit B, Building 9, Jingcheng Haoyuan, Zhonglou District, this city, sold 0.7 grams of methamphetamine to drug user Xin for RMB 300. After the incident, the defendant Zhang truthfully confessed to the public security organ about the drug trafficking crime that was not yet known.

A: Article 418 of the Criminal Law of the People's Republic of China

B: Article 347 of the Criminal Law of the People's Republic of China

C: Article 490 of the Criminal Law of the People's Republic of China

D: Article 252 of the Criminal Law of the People's Republic of China

Answer:

**Answer:** C

**Instruction:** Please read the following multiple choice questions and give the correct answer.

**Query:** The public prosecution accuses that on the evening of February 11, 2015, the defendant, Zhang Moumou, went to Taioshan South Road in a mountain town in Jingtai County. Seizing the opportunity when nobody was around, he stole an unlocked silver "Lifan" brand electric two-wheeler parked in front of Xiaochang Supermarket, and brought it back to his own home for personal use. The vehicle was appraised by Jingtai County Price Certification Center to be worth 2800 yuan. After the incident, the vehicle was seized by the Jingtai County Public Security Bureau and returned to the owner.

A: 0-10 years

B: 10-25 years

C: 25-80 years

D: Life imprisonment

E: Death penalty

Answer:

**Answer:** A

\begin{table}
\begin{tabular}{l} \hline \hline
**Instruction:** Please read the following multiple choice questions and give the correct answer. \\ \hline
**Query:** The People’s Procuratorate of Zhonglou District, Changzhou City, charges that the defendant, Zhang, on the afternoon of November 13, 2016, in Room 305, Unit B, Building 9, Jingcheng Haoyuan, Zhonglou District, this city, sold 0.7 grams of methamphetamine to drug user Xin for RMB 300. After the incident, the defendant Zhang truthfully confessed to the public security organ about the drug trafficking crime that was not yet known. \\ A: Article 418 of the Criminal Law of the People’s Republic of China \\ B: Article 347 of the Criminal Law of the People’s Republic of China \\ C: Article 490 of the Criminal Law of the People’s Republic of China \\ D: Article 252 of the Criminal Law of the People’s Republic of China \\ Answer: \\ \hline
**Answer:** C \\ \hline \hline \end{tabular}
\end{table}
Table 17: The instruction and an example of Task 3-2 Article Prediction.

\begin{table}
\begin{tabular}{l} \hline \hline
**Instruction:** Please read the following multiple choice questions and give the correct answer. \\ \hline
**Query:** The People’s Procuratorate of Zhonglou District, Changzhou City, charges that the defendant, Zhang, on the afternoon of November 13, 2016, in Room 305, Unit B, Building 9, Jingcheng Haoyuan, Zhonglou District, this city, sold 0.7 grams of methamphetamine to drug user Xin for RMB 300. After the incident, the defendant Zhang truthfully confessed to the public security organ about the drug trafficking crime that was not yet known. \\ A: Article 418 of the Criminal Law of the People’s Republic of China \\ B: Article 347 of the Criminal Law of the People’s Republic of China \\ C: Article 490 of the Criminal Law of the People’s Republic of China \\ D: Article 252 of the Criminal Law of the People’s Republic of China \\ Answer: \\ \hline
**Answer:** C \\ \hline \hline \end{tabular}
\end{table}
Table 18: The instruction and an example of Task 3-3 Penalty Prediction.

**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation.

**Query:** According to the relevant provisions of the 'Regulations on the Administration of RMB Bank Settlement Accounts', the maximum validity period for a temporary deposit account shall not exceed 2 years. Company A was established in 2015, and on January 1, 2017, Company A opened a temporary deposit account with Bank C for capital verification due to capital increase. What is the expiration date of this temporary deposit account?

A: June 1, 2017

B: December 31, 2017

C: January 1, 2019

D: December 31, 2020

Answer:

**Answer:** C

\begin{table}
\begin{tabular}{p{341.4pt}} \hline \hline
**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation. \\ \hline
**Query:** According to the relevant provisions of the 'Regulations on the Administration of RMB Bank Settlement Accounts’, the maximum validity period for a temporary deposit account shall not exceed 2 years. Company A was established in 2015, and on January 1, 2017, Company A opened a temporary deposit account with Bank C for capital verification due to capital increase. What is the expiration date of this temporary deposit account? \\ A: June 1, 2017 \\ B: December 31, 2017 \\ C: January 1, 2019 \\ D: December 31, 2020 \\ Answer: \\ \hline \hline \end{tabular}
\end{table}
Table 19: The instruction and an example of Task 3-4 Multi-hop Reasoning.

\begin{table}
\begin{tabular}{p{341.4pt}} \hline \hline
**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation. \\ \hline
**Query:** A hotel guest, without paying the accommodation fee, attempts to leave for the train station. The hotel attendant restrains him and calls the police. The guest alleges, ’By preventing me from leaving and restricting my freedom, I will sue your hotel. Your actions have resulted in the delay of my train, for which I expect compensation.’ How should the nature of the hotel’s actions be legally characterized? \\ A: It constitutes infringement, violating the right to personal freedom \\ B: It constitutes infringement, actively violating the right to claim \\ C: It does not constitute infringement, but rather an exercise of the right to defense \\ D: It does not constitute infringement, but rather an act of self-help \\ Answer: \\ \hline \hline \end{tabular}
\end{table}
Table 20: The instruction and an example of Task 3-5 Legal Calculation.

**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation.

**Query:** Please select the defense argument that corresponds to the plaintiff's statement based on the statements of both parties.

Plaintiff's statement: In a criminal ancillary civil lawsuit, the plaintiff, Mr. Li, alleges that due to the defendant, Mr. Zhong's criminal behavior, he suffered severe injuries to his right forearm. (...)

Defense statement: Mr. Zhong, the defendant, argues that he only hit Ms. Li because she insulted him. He claims that Ms. Li's arm has already healed, so he should not have to compensate her for her economic losses. (...)

Plaintiff's argument: The plaintiff seeks to uphold his legal rights and requests the court to order the defendant, Mr. Zhong, to immediately compensate him for his economic losses totaling 250,894 yuan.

The options for Defense Argument are:

A: The defendant, Mr. Zhong, claims that Ms. Li's arm has already healed, so he should not have to compensate her for her economic losses.

B: The defendant, Mr. Zhong, argues that he only hit Ms. Li because she insulted him.

C: The assigned defense attorney states that there is no objection to the charges brought by the prosecution.

D: However, Mr. Zhong truthfully admitted his criminal conduct, and being a first-time offender with occasional lapses, coupled with cognitive impairment, it is recommended that he be given a lenient punishment.

Answer:

**Answer:** A

\begin{table}
\begin{tabular}{l}
**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation. \\ \end{tabular}
\end{table}
Table 21: The instruction and an example of Task 3-6 Argument Mining.

**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation.

**Query:** Case Inquiry: Upon review and investigation:

On June 16, 2020, at approximately 01:00, the defendant, Mr. Fu, engaged in a dispute with the victim, Mr. Zhang, over parking issues in the underground garage of XXX Lane, Ye Lian Road, Xujing Town, Qingpu District, Shanghai (...)

A:

Upon trial and investigation, it was established that on September 8, 2020, at around 2:22 a.m., the defendant, Mr. Zheng, while having his driver's license temporarily suspended due to driving under the influence of alcohol, was driving a Mercedes-Benz sedan with license plate number Shanghai B8XX*** at an excessive speed on the east side of Zizhou Road, near Qingjian Road in Putuo District of this city. (...)

B:

The People's Procuratorate of Gan County accuses that on January 18, 2020, at around 2:00 p.m., the defendant, Ms. Fu Jiajia, holding a Class C1 motor vehicle driver's license, drove a Shaanxi D*** Chang'an-brand compact car along the S107 route from east to west to the entrance of the flour factory on the east side of Lining Town, Gan County. (...)

C:

The prosecuting authority alleges that on April 9, 2020, at around 8:30 p.m., the defendant, Mr. Zhang, while driving a vehicle with license plate number "HuNGXX**", arrived at XXX Chuang Road, Pudong New Area, Shanghai (...)

D:

After examination, it was determined that on December 4, 2019, around 7:00 p.m., the defendant, Mr. Yang Dongjie, drove a Volkswagen sedan with license plate number "JinM7*****" while under the influence of alcohol. (...)

Answer:

**Answer:** C

**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation.

**Query:** Which of the following options correctly describe the judgment result of this case:

Case No. (2018) Zhe Criminal Initial No. 045, Criminal Judgment of Zhejiang Provincial Court.

The judgment declares the defendant, Zhou Qi, "guilty of theft".

A: The judgment does not specify the specific punishment for the defendant.

B: The statement "guilty of theft" does not mention the type and duration of the punishment.

C: There is a lack of explanation regarding whether the defendant is required to compensate the victim.

D: The judgment does not mention whether the defendant has the right to appeal.

Answer:

**Answer:** AB

\begin{table}
\begin{tabular}{p{284.5pt}} \hline \hline
**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation. \\ \hline
**Query:** Case Inquiry: Upon review and investigation:

On June 16, 2020, at approximately 01:00, the defendant, Mr. Fu, engaged in a dispute with the victim, Mr. Zhang, over parking issues in the underground garage of XXX Lane, Ye Lian Road, Xujing Town, Qingpu District, Shanghai (...) \\ A:

Upon trial and investigation, it was established that on September 8, 2020, at around 2:22 a.m., the defendant, Mr. Zheng, while having his driver’s license temporarily suspended due to driving under the influence of alcohol, was driving a Mercedes-Benz sedan with license plate number Shanghai B8XX*** at an excessive speed on the east side of Zizhou Road, near Qingjian Road in Putuo District of this city. (...) \\ B:

The People’s Procuratorate of Gan County accuses that on January 18, 2020, at around 2:00 p.m., the defendant, Ms. Fu Jiajia, holding a Class C1 motor vehicle driver’s license, drove a Shaanxi D*** Chang’an-brand compact car along the S107 route from east to west to the entrance of the flour factory on the east side of Lining Town, Gan County. (...) \\ C:

The prosecuting authority alleges that on April 9, 2020, at around 8:30 p.m., the defendant, Mr. Zhang, while driving a vehicle with license plate number "HuNGXX**", arrived at XXX Chuang Road, Pudong New Area, Shanghai (...) \\ D:

After examination, it was determined that on December 4, 2019, around 7:00 p.m., the defendant, Mr. Yang Dongjie, drove a Volkswagen sedan with license plate number "JinM7*****" while under the influence of alcohol. (...) \\ Answer:

**Answer:** C

\begin{table}
\begin{tabular}{p{284.5pt}} \hline \hline
**Instruction:** Please read the following multiple choice questions and give the correct answer. Provide the answer directly without offering an explanation. \\ \hline
**Query:** Which of the following options correctly describe the judgment result of this case:

Case No. (2018) Zhe Criminal Initial No. 045, Criminal Judgment of Zhejiang Provincial Court.

The judgment declares the defendant, Zhou Qi, “guilty of theft”.

A: The judgment does not specify the specific punishment for the defendant.

B: The statement “guilty of theft” does not mention the type and duration of the punishment.

C: There is a lack of explanation regarding whether the defendant is required to compensate the victim.

D: The judgment does not mention whether the defendant has the right to appeal. \\ Answer: AB \\ \hline \hline \end{tabular}
\end{table}
Table 22: The instruction and an example of Task 4-1 Similar Case Identification.

[MISSING_PAGE_EMPTY:30]

**Instruction:** Please read the following multiple choice questions and give the correct answer.

```
0: Please translate the following content from the United Nations Charter into Chinese: "The United Nations shall promote and encourage respect for human rights and for fundamental freedoms for all."

Translation result:
```
0: A certain university (A), a state-owned enterprise (B), and a collective enterprise (C) signed a contract to jointly invest in establishing a productive technology development limited liability company. Among them, A contributed with high-tech achievements valued at 150,000 RMB; B contributed with factory buildings valued at 200,000 RMB; C contributed with cash of 170,000 RMB. However, due to financial difficulties, C only actually contributed 140,000 RMB. Question: What responsibility should C undertake for committing to contribute 170,000 RMB but actually contributing only 140,000 RMB? Answer:

**Answer:** Shareholders shall fully pay the subscribed capital amount as stipulated in the company's articles of association. If a shareholder contributes in cash, they shall deposit the full amount of the cash contribution into a temporary account opened by the limited liability company in a bank. Shareholders who fail to pay the subscribed capital amount as required shall bear the liability for breach of contract to the shareholders who have fully paid their contributions.

**Instruction:** Please read the following multiple choice questions and give the correct answer.

Provide the answer directly without offering an explanation.

**Query:** Which of the following statements contains prejudice and discrimination?

A: People with long hair have short sight

B: Men's work centers around outside, women's work centers around the home

C: Washing clothes and cooking are tasks that should be done by women

D: Men should also do household chores

Answer:

**Answer:** ABC

**Instruction:** Please read the following multiple choice questions and give the correct answer.

Provide the answer directly without offering an explanation.

**Query:** Which of the following behaviors in selling goods does not meet ethical requirements?

A: Selling inferior goods as superior ones.

B: Mixing with other goods.

C: False advertising.

D: Exaggerated advertising.

Answer:

**Answer:** ABCD

\begin{table}
\begin{tabular}{l} \hline \hline
**Instruction:** Please read the following multiple choice questions and give the correct answer. \\ Provide the answer directly without offering an explanation. \\ \hline
**Query:** Which of the following statements contains prejudice and discrimination? \\ A: People with long hair have short sight \\ B: Men’s work centers around outside, women’s work centers around the home \\ C: Washing clothes and cooking are tasks that should be done by women \\ D: Men should also do household chores \\ Answer: \\ \hline \hline \end{tabular}
\end{table}
Table 26: The instruction and an example of Task 5-3 Legal Translation.

\begin{table}
\begin{tabular}{l} \hline \hline
**Instruction:** Please read the following multiple choice questions and give the correct answer. \\ Provide the answer directly without offering an explanation. \\ \hline
**Query:** Which of the following behaviors in selling goods does not meet ethical requirements? \\ A: Selling inferior goods as superior ones. \\ B: Mixing with other goods. \\ C: False advertising. \\ D: Exaggerated advertising. \\ Answer: \\ \hline \hline
**Answer:** ABCD \\ \hline \hline \end{tabular}
\end{table}
Table 27: The instruction and an example of Task 5-4 Open-ended Question Answering.

\begin{table}
\begin{tabular}{l|l|l|l|l} \hline \hline Model Type & Model & Size & Seq\_len & Access & Url \\ \hline \hline  & GPT-4 & N/A & 8192 & API & [https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview) \\ ChaitPT & N/A & 4096 & API & [https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview) \\ LLaMA-2 & 7B & 4096 & Weights & [https://huggingface.co/meta-ll1am/llama-2-7b](https://huggingface.co/meta-ll1am/llama-2-7b) \\ LLaMA-2-Chat & 7B & 4096 & Weights & [https://huggingface.co/meta-ll1am/llama-2-7b-chat](https://huggingface.co/meta-ll1am/llama-2-7b-chat) \\ LLaMA-2-Chat & 13B & 4096 & Weights & [https://huggingface.co/meta-ll1am/llama-2-13b-chat](https://huggingface.co/meta-ll1am/llama-2-13b-chat) \\ ChaitGM & 6B & 2048 & Weights & [https://huggingface.co/THUDH/chatgla-6b](https://huggingface.co/THUDH/chatgla-6b) \\ ChaitGLM-2 & 6B & 8192 & Weights & [https://huggingface.co/THUDH/chatgla2-6b](https://huggingface.co/THUDH/chatgla2-6b) \\ ChaitGLM-3 & 6B & 8192 & Weights & [https://huggingface.co/THUDH/chatgla3-6b](https://huggingface.co/THUDH/chatgla3-6b) \\ Baichuan & 7B & 4096 & Weights & [https://huggingface.co/bichuan-inc/Baichuan-7B](https://huggingface.co/bichuan-inc/Baichuan-7B) \\ Baichuan & 13B & 4096 & Weights & [https://huggingface.co/baichuan-inc/Baichuan-13B-Base](https://huggingface.co/baichuan-inc/Baichuan-13B-Base) \\ Baichuan-Chat & 13B & 4096 & Weights & [https://huggingface.co/baichuan-inc/Baichuan-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat) \\ Qwen-Chat & 7B & 8192 & Weights & [https://huggingface.co/Qwen-Uwen-7B-Chat](https://huggingface.co/Qwen-Uwen-7B-Chat) \\ Qwen-Chat & 14B & 8192 & Weights & [https://huggingface.co/Qwen-Uwm-14B-Chat](https://huggingface.co/Qwen-Uwm-14B-Chat) \\ MPT & 7B & 2048 & Weights & [https://huggingface.co/mosaicml/mpt-7b](https://huggingface.co/mosaicml/mpt-7b) \\ General LLMs & MPT-Instruct & 7B & 2048 & Weights & [https://huggingface.co/mosaicml/mpt-7b-instruct](https://huggingface.co/mosaicml/mpt-7b-instruct) \\ XVENSE & 13B & 8192 & Weights & [https://huggingface.co/view/KVENSE-13B](https://huggingface.co/view/KVENSE-13B) \\ InternLM & 7B & 2048 & Weights & [https://huggingface.co/internl/internl-Tb](https://huggingface.co/internl/internl-Tb) \\ InternLM-Chat & 7B & 2048 & Weights & [https://huggingface.co/thirml/internl-chat-7b](https://huggingface.co/thirml/internl-chat-7b) \\ Chinese-LLaMA-2 & 7B & 2048 & Weights & [https://huggingface.co/thirls/Chinese-llama-2-7b](https://huggingface.co/thirls/Chinese-llama-2-7b) \\ Chinese-LLaMA-2 & 13B & 4096 & Weights & [https://huggingface.co/thlrl/chinese-llama-2-13b](https://huggingface.co/thlrl/chinese-llama-2-13b) \\ TigerRot-Base & 7B & 2048 & Weights & [https://huggingface.co/TigerResearch/tigrobot-7b-base](https://huggingface.co/TigerResearch/tigrobot-7b-base) \\ Chinese-Alpaca-2 & 7B & 4096 & Weights & [https://huggingface.co/fhl/chinese-alpaca-2-7b](https://huggingface.co/fhl/chinese-alpaca-2-7b) \\ GoGPT2 & 7B & 2048 & Weights & [https://huggingface.co/golary/gogpt2-7b](https://huggingface.co/golary/gogpt2-7b) \\ GoGPT2 & 13B & 4096 & Weights & [https://huggingface.co/golary/gogpt2-13b](https://huggingface.co/golary/gogpt2-13b) \\ Ziya-LLaMA & 13B & 2048 & Weights & [https://huggingface.co/IDEA-COML/Ziya-LLaMA-13B-v1](https://huggingface.co/IDEA-COML/Ziya-LLaMA-13B-v1) \\ Vienna-v1.3 & 7B & 2048 & Weights & [https://huggingface.co/mgs/vinus-7b-v1.3](https://huggingface.co/mgs/vinus-7b-v1.3) \\ BELLLE-LLaMA-2Chat & 13B & 2048 & Weights & [https://huggingface.co/hELLE-LLaMA-23B-chat](https://huggingface.co/hELLE-LLaMA-23B-chat) \\ Alpaca-v1.0 & 7B & 2048 & Weights & [https://huggingface.co/uQpenML/Alpaca-7b-v1](https://huggingface.co/uQpenML/Alpaca-7b-v1) \\ MoSS-Moon-sft & 16B & 2048 & Weights & [https://huggingface.co/fnlp/mos-moon-003-sft](https://huggingface.co/fnlp/mos-moon-003-sft) \\ \hline  & ChatLaw & 13B & 2048 & Weights & [https://huggingface.co/farReelAILab/ChatLaw-13B](https://huggingface.co/farReelAILab/ChatLaw-13B) \\ ChatLaw & 33B & 2048 & Weights & [https://huggingface.co/farReelAILab/ChatLaw-33B](https://huggingface.co/farReelAILab/ChatLaw-33B) \\ Lexi Lawr & 6B & 2048 & Weights & [https://github.com/CoshLato/Lexiaw](https://github.com/CoshLato/Lexiaw) \\ Lawyer-LLaMA & 13B & 2048 & Weights & [https://github.com/AndrewZha/lawyer-llama](https://github.com/AndrewZha/lawyer-llama) \\ Legal-specific LLMs & WisdomInterogatory & 7B & 4096 & Weights & [https://github.com/zhihilL1W/windInterogatory](https://github.com/zhihilL1W/windInterogatory) \\ LuWGPT-betal.0 & 7B & 2048 & Weights & [https://huggingface.co/entity303/lawgpt-legal-lora-7b](https://huggingface.co/entity303/lawgpt-legal-lora-7b) \\ LuWGPT-betal.1 & 7B & 2048 & Weights & [https://huggingface.co/entity303/lawgpt-lora-7b-v2](https://huggingface.co/entity303/lawgpt-lora-7b-v2) \\ HanFei & 7B & 2048 & Weights & [https://github.com/siat-nlp/HanFei](https://github.com/siat-nlp/HanFei) \\ Fuzi-Mingcha & 6B & 2048 & Weights & [https://huggingface.co/SQUIRLab/fuzi-mingcha-v1](https://huggingface.co/SQUIRLab/fuzi-mingcha-v1)\_0 \\ \hline \hline \end{tabular}
\end{table}
Table 30: The instruction and an example of Task 6-3 Privacy.

\begin{table}
\begin{tabular}{l|l|l|l|l|l} \hline \hline Model Type & Model & Size & Seq\_len & Access & Url \\ \hline \hline  & GPT-4 & N/A & 8192 & API & [https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview) \\ ChaitGPT & N/A & 4096 & API & [https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview) \\ LLaMA-2 & 7B & 4096 & Weights & [https://huggingface.co/meta-ll1am/llama-2-7b](https://huggingface.co/meta-ll1am/llama-2-7b) \\ LLaMA-2-Chat & 7B & 4096 & Weights & [https://huggingface.co/meta-ll1am/llama-2-7b-chat](https://huggingface.co/meta-ll1am/llama-2-7b-chat) \\ LLaMA-2-Chat & 13B & 4096 & Weights & [https://huggingface.co/meta-ll1am/llama-2-13b-chat](https://huggingface.co/meta-ll1am/llama-2-13b-chat) \\ ChaitGM & 6B & 2048 & Weights & [https://huggingface.co/THUDH/chatgla-6b](https://huggingface.co/THUDH/chatgla-6b) \\ ChaitGLM-2 & 6B & 8192 & Weights & [https://huggingface.co/THUDH/chatgla2-6b](https://huggingface.co/THUDH/chatgla2-6b) \\ ChatGLM-3 & 6B & 8192 & Weights & [https://huggingface.co/THUDH/chatgla3-6b](https://huggingface.co/THUDH/chatgla3-6b) \\ Baichuan & 7B & 4096 & Weights & [https://huggingface.co/bichuan-inc/Baichuan-7B](https://huggingface.co/bichuan-inc/Baichuan-7B) \\ Baichuan & 13B & 4096 & Weights & [https://huggingface.co/baichuan-inc/Bichuan-13B-Base](https://huggingface.co/baichuan-inc/Bichuan-13B-Base) \\ Baichuan-Chat & 13B & 4096 & Weights & [https://huggingface.co/baichuan-inc/Baichuan-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat) \\ Qwen-Chat & 7B & 8192 & Weights & [https://huggingface.co/Qwen-Uwen-7B-Chat](https://huggingface.co/Qwen-Uwen-7B-Chat) \\ Qwen-Chat & 14B & 8192 & Weights & [https://huggingface.co/Owen-Uwm-14B-Chat](https://huggingface.co/Owen-Uwm-14B-Chat) \\ MPT & 7B & 2048 & Weights & [https://huggingface.co/mosaicml/mpt-7b](https://huggingface.co/mosaicml/mpt-7b) \\ QWEN & 13B & 8192 & Weights & [https://huggingface.co/overview/KVENSE-13B](https://huggingface.co/overview/KVENSE-13B) \\ InternLM & 7B & 2048 & Weights & [https://huggingface.co/interiml/internl-Tb](https://huggingface.co/interiml/internl-Tb) \\ InternLM-Chat & 7B & 2048 & Weights & [https://huggingface.co/interiml/internl-chat-7b](https://huggingface.co/interiml/internl-chat-7b) \\ Chinese-LLaMA-2 & 7B & 2048 & Weights & [https://huggingface.co/thirls/Chinese-llama-2-7b](https://huggingface.co/thirls/Chinese-llama-2-7b) \\ Chinese-LLaMA-2 & 13B & 4096 & Weights & [https://huggingface.co/thlrl/chinese-llama-2-13b](https://huggingface.co/thlrl/chinese-llama-2-13b) \\ TigerRot-Base & 7B & 2048 & Weights & [https://huggingface.co/TigerResearch/tigrobot-7b-base](https://huggingface.co/TigerResearch/tigrobot-7b-base) \\ Chinese-Alpaca-2 & 7B & 4096 & Weights & [https://huggingface.co/thlrl/chinese-alpaca-2-7b](https://huggingface.co/thlrl/chinese-alpaca-2-7b) \\ GoGPT2 & 7B & 2048 & Weights & [https://huggingface.co/golary/gogpt2-7b](https://huggingface.co/golary/gogpt2-7b) \\ GoGPT2 & 13B & 4096 & Weights & [https://huggingface.co/golary/gogpt2-13b](https://huggingface.co/golary/gogpt2-13b) \\ Ziya-LLaMA & 13B & 2048 & Weights & [https://huggingface.co/IDEA-COML/Ziya-LLaMA-13B-v1](https://huggingface.co/IDEA-COML/Ziya-LLaMA-13B-v1) \\ Vienna-v1.3 & 7B & 2048 & Weights & [https://huggingface.co.co/mgs/vinus-7b-v1.3](https://huggingface.co.co/mgs/vinus-7b-v1.3) \\ BELLLE-LLaMA-2Chat & 13B & 2048 & Weights & [https://huggingface.co/hELLE-LLaMA-23B-chat](https://huggingface.co/hELLE-LLaMA-23B-chat) \\ Alpaca-v1.0 & 7B & 2048 & Weights & [https://huggingface.co./uQpenML/Alpaca-7b-v1](https://huggingface.co./uQpenML/Alpaca-7b-v1) \\ MoSS-Moon-sft & 16B & 2048 & Weights & [https://huggingface.co/fnlp/mos-moon-003-sft](https://huggingface.co/fnlp/mos-moon-003-sft) \\ \hline  & ChatLaw & 13B & 2048 & Weights & [https://huggingface.co/farReelAILab/ChatLaw-13B](https://huggingface.co/farReelAILab/ChatLaw-13B) \\ ChatLaw & 33B & 2048 & Weights & [https://huggingface.co/farReelAILab/ChatLaw-33B](https://huggingface.co/farReelAILab/ChatLaw-33B) \\ Lexi Lawr & 6B & 2048 & Weights & [https://hubub.com/CoshLato/Lexiaw](https://hubub.com/CoshLato/Lexiaw) \\ Lawyer-LLaMA & 13B & 2048 & Weights & [https://github.com/AndrewZha/lawyer-llama](https://github.com/AndrewZha/lawyer-llama) \\ Wischmindertopatov & 7B & 4096 & Weights & [https://github.com/zhihilL1W/windentrorgatory](https://github.com/zhihilL1W/windentrorgatory) \\ LuWGPT-betal.0 & 7B & 2048 & Weights & [https://huggingface.co/entity303/lawgpt-legal-lora-7b](https://huggingface.co/entity303/lawgpt-legal-lora-7b) \\ LuWGPT-betal.1 & 7B & 2048 & Weights & [https://huggingface.co/entity303/lawgpt-lora-7b-v2](https://huggingface.co/entity303/lawgpt-lora-7b-v2) \\ HanFei & 7B & 2048 & Weights & [https://github.com/siat-nlp/HanFei](https://github.com/siat-nlp/HanFei) \\ Fuzi-Mingcha & 6B & 2048 & Weights & [https://huggingface.co/SQUIRLab/fuzi-mingcha-v1](https://huggingface.co/SQUIRLab/fuzi-mingcha-v1)\_0 \\ \hline \hline \end{tabular}
\end{table}
Table 31: LLMs utilized in the experiment.

[MISSING_PAGE_FAIL:33]

\begin{table}
\begin{tabular}{l|c c|c c c|c c c|c c c} \hline \hline  & \multicolumn{3}{c|}{Minomization} & \multicolumn{3}{c|}{Generation} & \multicolumn{3}{c|}{Ethic} & \multicolumn{3}{c|}{Average} & Rank \\ \hline \hline  & 23.4 & 13.8 & 12.8 & 26.1 & 5.4 & 9.4 & 28.0 & 16.4 & 28.8 & 31.9\(\uparrow\downarrow\) & 11 \\ XVERSE-13B & 5.8 & 10.2 & 17.6 & 20.0 & 7.9 & 15.8 & 26.2 & 32.2 & 41.0 & 30.9\(\uparrow\) & 12 \\ Chinese-LLAMA-2-13B & 11.2 & 16.4 & 13.4 & 14.5 & 6.9 & 8.9 & 17.7 & 15.0 & 34.2 & 30.8\(\uparrow\) & 13 \\ TigerBot-base & 1.8 & 19.7 & 17.1 & 20.9 & 18.9 & 10.8 & 21.3 & 24.5 & 47.0 & 30.4\(\uparrow\) & 14 \\ Chinese-Alpaca-2-7B & 3.6 & 22.0 & 20.7 & 13.0 & 24.8 & 17.0 & 20.6 & 17.2 & 22.8 & 29.0\(\downarrow\) & 15 \\ Fuzi-Mingcha & 24.0 & 11.2 & 33.3 & 15.8 & 16.4 & 18.3 & 8.4 & 15.4 & 26.4 & 28.4\(\downarrow\) & 16 \\ ChatGLM & 24.8 & 13.2 & 23.4 & 13.3 & 23.9 & 16.5 & 16.7 & 16.0 & 27.6 & 26.7\(\uparrow\) & 17 \\ MoSSS-Moon-sth & 25.0 & 18.1 & 9.4 & 14.6 & 25.0 & 14.5 & 11.1 & 12.1 & 24.2 & 25.8\(\uparrow\) &