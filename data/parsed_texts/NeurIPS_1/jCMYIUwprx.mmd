# INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness

 Hung Le\({}^{*}\), Yingbo Zhou, Caiming Xiong, Silvio Savarese, Doyen Sahoo

Salesforce Research

Corresponding author: hungle@salesforce.com

###### Abstract

Large language models (LLMs) for code are typically trained to align with natural language instructions to closely follow their intentions and requirements. However, in many practical scenarios, it becomes increasingly challenging for these models to navigate the intricate boundary between helpfulness and safety, especially against highly complex yet potentially malicious instructions. In this work, we introduce INDICT: a new framework that empowers LLMs with Internal Dialogues of Critiques for both safety and helpfulness guidance. The internal dialogue is a dual cooperative system between a safety-driven critic and a helpfulness-driven critic. Each critic provides analysis against the given task and corresponding generated response, equipped with external knowledge queried through relevant code snippets and tools like web search and code interpreter. We engage the dual critic system in both code generation stage as well as code execution stage, providing preemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8 diverse tasks across 8 programming languages from 5 benchmarks, using LLMs from 7B to 70B parameters. We observed that our approach can provide an advanced level of critiques of both safety and helpfulness analysis, significantly improving the quality of output codes (\(+10\%\) absolute improvements in all models).2

Footnote 2: We released our code at [https://github.com/SalesforceAIResearch/indict_code_gen](https://github.com/SalesforceAIResearch/indict_code_gen)

## 1 Introduction

Extending from the natural language domain, Large Language Models (LLMs) like [13, 14, 15] have demonstrated great potential in code generation tasks [16, 17, 1]. However, when instructed with tasks containing malicious intentions or ambiguous requirements, LLMs are subject to generating code that could facilitate harmful attacks or code that contains obscure security problems [13, 14, 15]. For instance, in a study of Github's Copilot, Pearce et al. [20] observed that about \(40\%\) of generated programs are vulnerable.

Despite previous efforts in addressing the safety of LLMs through finetuning [14, 15, 16], this strategy alone is often not sufficient and scalable enough against prompts that are increasingly optimised for highly sophisticated attacks [15, 16, 17]. Furthermore, in the domain of code generation, creating quality safety-related data for finetuning often incurs great costs, involving programming experts with a deep understanding of code and related cyber-security and vulnerability concerns.

Note that code itself is often not inherently malicious. For example, as noted by Bhatt et al. [20], a program for an encryption method could be very useful to create a secure personal file system. Yet the encryption method can also be exploited for a ransomware attack. Therefore, it is important to develop an efficient method for LLMs to achieve the intricate balance between helpfulness and safetyin the code domain. We introduce INDICT, Internal Dialogues of Critiques, a novel framework for LLMs to generate code that is not only helpful but also safe and secure (see Figure 1 for an example code generation task and Figure 2 for the method overview).

First, instead of a single critic for a specific code quality [10, 11, 12, 13], we consider both helpfulness-driven critic and safety-driven critic. Instead of activating these critics independently, we propose to position them in an autonomous agent system like [11, 12, 13]. Although the critics are configured with orthogonal goals, we let them interact with each other autonomously to collaboratively and simultaneously optimise both security and correctness of LLM-generated responses.

Extending from retrieval-augmented generation [14, 15, 16], we also equip the critics with external knowledge retrieved by relevant code snippets and natural language queries. Just like how human developers typically select and examine one small piece of code at a time, the critics use a code snippet together with a text query to call relevant tools like web search and code interpreters. The resulting outputs from the external tools are used by the critics to generate more knowledge-grounded critiques for the "actor" LLM generator.

Finally, we engage our critics during two stages: (1) preemptive critic feedback is obtained during the initial code generation stage; and (2) post-hoc critic feedback is activated after the code is observed in an execution environment. Albeit more commonly used in prior work like [10, 11, 12], post-hoc feedback alone is not proactive enough for security-sensitive tasks. In these tasks, unexpected damage may likely occur and create systematic impacts on execution environments in practice [10, 11]. Our strategy facilitates a "preemptive" layer of protection, creating a more holistic critic framework for code generation LLMs.

We conducted a comprehensive evaluation of INDICT on 8 diverse tasks across 8 programming languages from 5 benchmarks. On LLMs ranging from 7B to 70B parameters, we observed consistent performance improvement by both safety and helpfulness of generation outputs. We found that INDICT can provide useful critiques to LLMs, leading to new SoTA performance by security measures while maintaining or improving the helpfulness of generated code. Our approach also generalises well to open-ended tasks, demonstrating the broader potential of a cooperative autonomous critic system for helpful yet responsible AI models.

## 2 Related Work

Our research is broadly related to the research of large language models (LLMs) [17, 13, 14, 15]. Pretrained on

Figure 1: INDICT (Internal Dialogues of Critiques) enables two different critics to interact with each other autonomously and collaboratively, improving code generation by both security and helpfulness. In this example, INDICT iteratively resolves the security weakness CWE-78 (Improper Neutralization in an OS Command) and improves the code functionality with relevant supporting modules.

a massive amount of text data on very deep Transformer-based architectures, these models have shown impressive performance in many natural language tasks. Going beyond the text domain, LLMs have been extended to learn from the code data and applied to many coding tasks [13, 14, 15, 16, 17, 18, 19, 20]. One major application of LLMs in the code domain is code generation, a long-standing challenge of many conventional AI models [13, 12, 12, 11]. In this task, an AI model is required to generate proper code solutions for different programming problems, ranging from basic daily code completion tasks to more advanced algorithmic problems [14, 12, 11, 15, 16].

In the research for code generation, we have witnessed emerging studies focusing on the security and safety aspects of AI-generated code. Hammond Pearce et al. [13], Schuster et al. [13], Pearce et al. [13] found that commercially successful systems like Github's Copilot still led to obscure yet major vulnerability and security issues in code. More recently, Perez et al. [13], Zhuo et al. [14], Khoury et al. [15] demonstrated highly complex prompting methods that can "jailbreak" advanced LLMs like ChatGPT into generating malicious code. To benchmark LLMs against code safety and security, [12, 13] evaluated LLMs against common coding scenarios based on CWE 3. More recently, Bhatt et al. [15] introduced CyberSecEval, a large-scale benchmark containing different types of security-aware evaluations. They observed that the code outputs by powerful LLMs like Llama and GPT models are often not perfectly secure.

Footnote 3: Common Weakness Enumeration (CWE) is a community-developed list of common software and hardware weaknesses. More details in [https://cwe.mitre.org/about/index.html](https://cwe.mitre.org/about/index.html)

More relevant to our work is the research to improve the safety or helpfulness of LLMs. A common strategy is finetuning LLMs with appropriate preference data with specific reward models to differentiate among ranked data samples [1, 13, 12, 14, 15]. In the code domain, He and Vechev [14], He et al. [13] proposed to finetune LLMs with prompt prefixes or masking strategies conditioned by the safety of corresponding code samples. Chen et al. [14] requires human annotators to provide natural language feedback of training samples. Different from prior approaches, we propose a more efficient method to generate better codes by both safety and helpfulness. Our approach can complement the research of autonomous LLM agents [15, 16, 17] and AI-generated feedback [1, 18, 19, 20]. For a systematic comparison to related work, please refer to Table 1 and Appendix D.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & Helpful. & Safety & Exec. & Tool- & Multi-critic & Supervision \\  & & feedback & enhanced & collab & free \\ \hline
**Self-refine approach** & & & & & \\ Coder, AlphaCode, MBR-Exec & ✓ & & ✓ & & ✓ \\ Self-correct, LF & ✓ & & & & ✓ \\ CodeRL, Self-edit & ✓ & ✓ & & & ✓ \\ Self-repair, Self-debug, Reflexion & ✓ & ✓ & & & ✓ \\ \hline
**Multi-agent approach** & & & & & ✓ \\ Self-collaboration, AgentCoder & ✓ & & ✓ & & ✓ \\ CAMEL & ✓ & & & & ✓ \\ ChatDev, Self-org Agents & ✓ & & ✓ & ✓(?) & ✓ \\ MetaGPT, AgentVerse & ✓ & & ✓ & ✓ & ✓ \\ \hline
**Finetuning approach** & & & & & \\ CodedUltraFeedback, StableAlignment & ✓ & ✓ & & & ✓ \\ SafeCoder & ✓ & ✓ & ✓ & & \\ \hline
**INDICT (ours)** & ✓ & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multi-agent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT. See Appendix D for cited references.

## 3 INDICT Framework

### Problem Definition

Typically, in a code generation task, an LLM \(\theta\) receives an input \(X\), consisting of a natural language instruction and optionally, a related code context. Treating code generation as a sequence-to-sequence task, the LLM autoregressively generates a response \(\hat{Y}\) as a sequence of tokens. Each token \(\hat{y}_{t}\) is sampled from the parameterized condition distribution \(p_{\theta}(.|\hat{y}_{1:t-1},X)\) where \(\hat{y}_{t}\in\mathcal{V}\). The output can contain either natural language segments (e.g. explanation of the output code or refusal of the user request) as well as code programs (e.g. code snippets to complete the given input code context).

### Safety-driven and Helpfulness-driven Critics

Pretrained with a massive amount of data, LLMs are found to be capable of providing insightful feedback to self-improve their own responses in many downstream tasks (Shinn et al., 2023; Zhang et al., 2023; Welleck et al., 2023; Madaan et al., 2023). Rather than just a single critic for a specific code attribute, we propose to engage two critics with independent goals: a safety-driven critic \(\sigma\) and a helpfulness-driven critic \(\omega\). We initialize the critics as LLMs configured by specific system prompts (\(P_{s}\) and \(P_{h}\) respectively) to establish the critics' corresponding roles.

For instance, for the safety-based critic, we instruct the model to focus solely on the security and risks of the code, and prioritise these aspects over other code qualities. Vice versa, for the helpfulness-based critic, we request the model to investigate the helpfulness of the code, i.e. whether the output aligns fully with the intentions and requirements in the given task. Denoting \(\hat{C}_{s}\) and \(\hat{C}_{h}\) as the complete outputs of the critics, we can define the critic output distributions (per token) as:

\[\hat{c}_{s,t}\sim p_{\sigma}(.|\hat{c}_{s,1:t-1},X,\hat{Y},P_{s}) \Rightarrow\text{ for safety-driven critic} \tag{1}\] \[\hat{c}_{h,t}\sim p_{\omega}(.|\hat{c}_{h,1:t-1},X,\hat{Y},P_{h}) \Rightarrow\text{ for helpfulness-driven critic} \tag{2}\]

Subsequently, we let the code generation LLM ("actor") revise their solutions conditioned by the generated critiques: \(\hat{y}_{s}\sim p_{\theta}(\hat{y}_{s,1:t-1}|X,\hat{Y},\hat{C}_{s})\) for safety-conditioned solutions and \(\hat{y}_{h}\sim p_{\theta}(\hat{y}_{h,1:t-1}|X,\hat{Y},\hat{C}_{h})\) for helpfulness-conditioned solutions. Refer to Appendix I for the detailed instruction prompts we used on our critics to assess safety or helpfulness of model outputs.

### Autonomous Collaboration between Critics

LLMs are often finetuned to follow natural language instructions (Ouyang et al., 2022; Korbak et al., 2023; Dai et al., 2024) and subsequently, can engage in natural language interactions with humans or even among other LLMs. In the latter, recent studies (Huang et al., 2022; Dong et al., 2023; Li et al., 2024; Chen et al., 2024) observed significant performance gains when enabling LLMs to

Figure 2: INDICT (Internal Dialogues of Critiques) is a framework to generate code by both safety and helpfulness. The framework introduces dialogues between knowledge-grounded safety-driven and helpfulness-driven AI critics. It enables the pair of critics to collaboratively and autonomously support the LLM code generator. We apply the critic system for both preemptive and post-hoc types of critic feedback, providing a proactive and extra layer of protection against security-sensitive tasks.

interact autonomously to solve complex tasks. We are motivated by this observation and propose an autonomous agent system of critic models to generate helpfulness-and-safety-aware critiques.

Note that an alternative strategy is to use a single critic model for both helpfulness and safety. However, such a critic model often needs complex alignment finetuning or prompt engineering to generate critiques that are not significantly biased towards a single code property. In our approach, from 1 and 2, given an interaction turn \(r\) between critics, we can redefine the output distributions as:

\[\hat{c}^{r}_{s,t}\sim p_{\sigma}(.|\hat{c}_{s,1:t-1},X,\hat{Y},P_ {s},\hat{I}_{1:r-1}) \Rightarrow\] for safety-driven critic (3) \[\hat{c}^{r}_{h,t}\sim p_{\omega}(.|\hat{c}_{h,1:t-1},X,\hat{Y},P_ {h},\hat{I}_{1:r-1}\oplus\hat{C}^{r}_{s}) \Rightarrow\] for helpfulness-driven critic (4)

Where \(\oplus\) denotes concatenation and \(\hat{I}_{1:r-1}=\hat{C}^{1}_{s}\oplus\hat{C}^{1}_{h}\oplus\dots\hat{C}^{r-1}_{ s}\oplus\hat{C}^{r-1}_{h}\) contains all the past interactions between the safety-driven and helpfulness-driven critics.

Practically, to avoid computation overhead, we can limit \(\hat{I}\) to only the last few turns of interactions. Alternatively, in this work, we summarize the critic dialogue after each turn of interactions and only use the corresponding summary in each turn: \(\mathcal{\hat{I}}_{r}=f(\hat{I}_{1:r})\) where \(f(.)\) is parameterized as an LLM-based summarizer model. To revise the solutions from "actor" LLM by both safety and helpfulness, we can then conveniently reuse the summary in the last interaction turn \(R\) between the critics (thus, also reducing the computation cost on the "actor" LLM). To generate safety-and-helpfulness-aware outputs, we revise the output distributions of the LLM code generator as:

\[\hat{y}_{s+h,t}\sim p_{\theta}(.|\hat{y}_{s+h,1:t-1},X,\hat{Y}, \mathcal{\hat{I}}_{R}) \tag{5}\]

### Knowledge-grounded Critics with External Tools

Depending on how well LLMs can perceive and resurface relevant knowledge from pretraining data, these models might still cause serious hallucination problems by generating factically incorrect responses [14, 15, 16]. These hallucination problems are exacerbated when LLMs play the critic roles, required to provide reliable and grounded responses against code generation outputs. In this work, we extend prior tool-enhanced LLMs like [15, 16, 17, 18] and retrieval-augmented generation strategies [19, 14, 15, 17] to improve our critics.

Specifically, we equip our critics with access to external tools and incorporate the tools' query results as additional knowledge to generate more grounded critiques (see Figure 3 for an overview). For instance, for the safety-driven critic, from 3, we decompose the critic generation process to the following steps:

1. Critic's thought \(\hat{W}^{r}_{s}\): \(\hat{w}^{r}_{s,t}\sim p_{\sigma}(.|w^{r}_{s,1:t-1},X,\hat{Y},P_{s},\hat{ \mathcal{I}}_{r-1})\) (6) 2. Critic's action \(\hat{Q}^{r}_{s}\): \(\hat{Q}^{r}_{s}\sim p_{\sigma}(\langle\hat{Q}^{r}_{s,\text{text}},\{\emptyset, \hat{Q}^{r}_{s,\text{code}}\}\rangle|\hat{Y},P_{s},\hat{W}^{r}_{s})\) (7) 3. Critic's observation \(\hat{O}^{r}_{s}\): \(\hat{O}^{r}_{s}=g(\hat{Q}^{r}_{s})\) (8)

First, we obtain the critic's initial thought \(\hat{W}^{r}_{s}\), following the same formulation as in 3. In the critic's action step, we parameterize critic "actions" as the generation of unique textual keywords \(\hat{Q}^{r}_{s,\text{text}}\), optionally accompanied by code snippets \(\hat{Q}^{r}_{s,\text{code}}\). These are used subsequently as search queries to call external tools and obtain search results in the critic's observation step. Denoting function \(g(.)\) as the tool calling functions, we introduce two types of functions: code search and code review. Refer to Figure 3 for the specifications and examples of these functions and Figure 1 for demonstrations.

Figure 3: We define two types of tool-enabled actions the critics can perform: (1) “code search” queries external tools by a generated text query and optionally a corresponding code snippet. (2) “code review” uses the execution result of the code snippet (through a code interpreter) as additional input to complement the query. Both action types query tools like web search, Wikipedia, and OpenAI as the knowledge base.

Note that the above extension can be applied identically to the helpfulness-driven critic. We also then revise \(\mathcal{I}\) as the summary of all past critics' initial thoughts concatenated with corresponding observations: \(\hat{\mathcal{I}}_{r}=f(\{\hat{W}\oplus\hat{O}\}_{s}^{1:r-1}\oplus\{\hat{W} \oplus\hat{O}\}_{h}^{1:r-1})\).

### Preemptive and Post-hoc Critic Feedback

Different from the text domain, code generation outputs could be additionally observed/ interpreted in relevant environments e.g. through code interpreters ("executor"). Shi et al. (2022), Le et al. (2022), Chen et al. (2023, 2023) demonstrated the benefits of execution-based feedback to improve the functional correctness of code. However, in security-sensitive scenarios, directly engaging the executing environment might cause unintentional systematic damage, e.g. deleted data directories or modified access to privileged user accounts.

We propose to deploy our critic system for both preemptive feedback (after the initial code generation step) and post-hoc feedback (after the generated code is observed by the executor). To obtain posthoc critic feedback, we simply incorporate the execution results (e.g. error messages, unit test outcomes) as the conditioning factors in 1, 2, 3, 4, and 6. Note that we maintain a persistent dialogue context between safety and helpfulness critics throughout preemptive and post-hoc iterations. We can define the output distributions of the LLM code generator conditioned by the posthoc feedback as:

\[\hat{g}^{\text{posthoc}}_{s+h,t}\sim p_{\theta}(.|\hat{g}^{\text{ posthoc}}_{s+h,1:t-1},X,\hat{Y}^{\text{pepnt}}_{s+h},\hat{\mathcal{I}}^{\text{ posthoc}}_{R}) \tag{9}\]

where \(\hat{\mathcal{I}}^{\text{posthoc}}_{r}=f(\hat{\mathcal{I}}^{\text{pepnt}}_{1: R}\oplus\hat{I}^{\text{posthoc}}_{1:r-1})\) is the summarized posthoc critic feedback.

## 4 Experiments

**Base Language Models.** We applied INDICT on CommandR (Cohere, 2024) which was specifically optimized for external tool augmentation, making the model suitable for our framework. In challenging adversarial tests like red-teaming attacks, we additionally employed popular preference-tuning models from the Llama and Codellama families (Touvron et al., 2023; Roziere et al., 2023; Meta, 2024), ranging from 7B to 70B parameters. All models were designed for long-context tasks as well as conversational interactions, making them suitable for experiments with INDICT. To fairly compare the performance across models, given a model choice, we initialized our actors and critics with the same model checkpoint. For all base LLMs, we utilized the Huggingface-hosted model parameters (Wolf et al., 2019) and vLLM (Kwon et al., 2023) to generate the responses.

**Configurations.** To fairly compare between base models, given a task, we maintained the instruction prompts as similarly as possible across all models. Models such as CommandR (Cohere, 2024) which is already finetuned for tool enhancement, are prompted according to their prompting strategies. We adopted a maximum output length of up to \(2048\) tokens on actor or critic models. We also fixed the generation budget to \(1\) sample in each generation by actor or critic models. For a given actor-generated sample, we applied our INDICT framework for up to \(5\) rounds to improve this sample iteratively. Please refer to Appendix E and I for more detailed experimental setups e.g. external tools, model and generation configurations, compute resources, and example prompt instructions.

### Insecure coding practice tasks

**Benchmarks.** We first evaluated our approach on insecure code generation tasks in which LLMs were found to generate outputs with significant security concerns. We considered the Insecure Coding Practice test from CyberSecEval-1 (Bhatt et al., 2023), which includes two sub-tasks: "Autocomplete" where LLMs are provided a code context and predict subsequent code segments to complete this code context; and "Instruct" where LLMs fulfill natural language instructions of coding problems. Additionally, following an instruction-following setup, the CVS benchmark (Code Vulnerability and Security) (CyberNative, 2024) provides a pair of ground-truth secure and insecure code outputs given a coding problem. Please refer to Appendix E for more details of the benchmarks.

**Evaluation.** To measure the safety of model outputs, we followed Bhatt et al. (2023) by using their detector model which contains comprehensive rules defined in weggli (weggli, 2023) and semgrep (sem, 2023) to detect more than 180 patterns related to 50 Common Weakness Enumerations (CWEs). The safety metric is defined as the percentage of test samples where output codes do not contain any insecurities. To measure the helpfulness, we followed prior work like Bai et al. (2022), Zheng et al. (2024), Li et al. (2024) to adopt GPT3.5 as the AI evaluator (Achiam et al., 2023) to rank the helpfulness of model outputs. In our experiments, given a test problem, we computed the winning rate of a model output against the output of a known SoTA model (e.g. Llama2-7b-chat in CyberSecEval-1) or the corresponding ground-truth outputs (for the CVS benchmark).

**Results.** From Figure 4, we observed consistent performance improvements of our approach, outperforming prior strong LLM baselines such as Llama and GPT models (Touvron et al., 2023; Achiam et al., 2023). Specifically, by applying INDICT with CommandR and LLama3 models (Meta, 2024; Cohere, 2024), we obtained SoTA performance by safety (more than \(80\%\) and \(90\%\) output codes are safe on CyberSecEval-1 and CVS respectively) as well as helpfulness (up to \(70\%\) output codes are more helpful than the prior SoTA model or ground-truth outputs). Figure 4 also demonstrates the consistency of our approach by both safety and helpfulness across different programming languages. There are only a few exceptional cases of helpfulness performance (specifically with Javascript in the CyberSecEval benchmark and C++ in the CVS benchmark).

### Security attack tasks

**Benchmarks.** We also evaluated our approach against malicious coding tasks in which the instruction prompts contain obscure yet dangerous intentions to perform security attacks. We considered three

Figure 4: we evaluated INDICT against insecure coding practice tasks with CyberSecEval-1 (Autocomplete and Instruction splits) and CVS benchmarks. Safety measure is computed as the percentage of outputs that are safe (determined by a rule-based detector). Helpfulness measure is the winning rate against prior SoTA model or available ground-truth outputs (determined by a GPT evaluator). Notations: JV: Java, JS: Javascript, Py: Python; CR: CommandR, GT: ground-truth (“GT Safe” and “GT Unsafe” are the secure and insecure code samples provided by the CVS benchmark).

major tasks: the Cyberattack Helpfulness test from CyberSecEval-1 [Bhatt et al., 2023], and the Interpreter Abuse and Prompt Injection tests from CyberSecEval-2 [Bhatt et al., 2024]. The first tasks contain test samples of attack methods that are well studied in industry-standard MITRE ATT&CK ontology 4. The second task was proposed recently to instruct LLMs to abuse a code interpreter to carry on unauthorized actions e.g. data overriding. Finally, the last task is designed to simulate injection attacks by synthetically injecting harmful rules to prompts e.g. disclosing a given password in the generation output. Please refer to Appendix E for more details of the benchmarks.

Footnote 4: [https://attack.mitre.org/](https://attack.mitre.org/)

**Evaluation.** In these tasks, we focused on measuring the safety measurement by determining whether the model outputs assist the given instructions e.g. by suggesting supporting code snippets or by providing natural language explanation for a solution. Following Bhatt et al. [2023, 2024], we used GPT3.5 [Achiam et al., 2023] and adopted the expansion-then-judge evaluation pipeline: first, expand the generation output with reasoning against safety criteria, and subsequently, judge if the output is indeed benign. The safety metric is the percentage of outputs that are considered benign.

**Results.** From Figure 5, we observed the significant performance improvement by safety measures on all three types of security attacks. Specifically, by using models from CodeLlama [Roziere et al., 2023] and Llama3 [Meta, 2024] families, we achieved new SoTA safety performance: \(76\%\) on Cyber Attack task and more than \(90\%\) on Interpreter Abuse and Prompt Injection tasks. Notably, despite a weaker model, when enhanced with INDICT, CommandR can achieve significant boosts and become more secure against harmful task instructions. The results also demonstrate the efficacy of our method on models of different sizes, from 8B to 70B model parameters.

\begin{table}
\begin{tabular}{l|c c c c c|c} \hline \hline Model & Direct & ZS & PAP & JB & TAP & PAIR & Avg. \\ \hline CommandR & 33.1 & 23.4 & 25.0 & 23.1 & 18.4 & 18.4 & 23.6 \\ CommandR+INDICT & 65.3 & 52.5 & 63.1 & 37.5 & 46.9 & 43.4 & 51.5 \\ \hline Llama3-8b-instruct & 77.5 & 63.4 & 67.8 & 83.1 & 60.6 & 58.1 & 68.4 \\ Llama3-8b-instruct+INDICT & **90.6** & **79.4** & **81.9** & 89.1 & **75.9** & **77.8** & **82.4** \\ \hline Llama3-70b-instruct & 68.4 & 60.0 & 68.1 & **90.9** & 61.9 & 57.5 & 67.8 \\ Llama3-70b-instruct+INDICT & 85.9 & 75.3 & 74.7 & 90.0 & **75.9** & 75.3 & 79.5 \\ \hline \hline \end{tabular}
\end{table}
Table 2: We evaluated INDICT with HarmBench against 6 different types of red-teaming optimization methods. We reported the safety measure as the percentage of outputs classified as benign by the given AI evaluator from HarmBench.

Figure 5: We evaluated INDICT against three major types of security attacks from CyberSecEval-1 and 2 benchmarks. Safety measure is computed as the percentage of outputs that do not comply with the corresponding malicious prompting instructions (determined by a GPT evaluator). The higher the safety measure is, the better. Notations: CL: Codellama, L2: Llama2, L3: Llama3, CR: CommandR.

Figure 6: With Llama3-8b-instruct as the base model, we evaluated INDICT on the CAMEL benchmark.

### Open-ended generation tasks

**Benchmarks.** Although we focused on the code domain in this work, our method can be easily adapted to generation tasks in other domains. In these cases, we can simply remove the execution environment (and accordingly posthoc feedback step) and activate INDICT with appropriate domain-agnostic contexts in our instruction prompts (see Appendix I for example prompts). We adapted our method to two major open-ended generation benchmarks: HarmBench (Mazeika et al., 2024), which evaluates LLMs against various red teaming optimization methods, and CAMEL (Li et al., 2024), which contains a wide variety of GPT-generated complex problems in diverse domains. Please refer to Appendix E for more details of the benchmarks.

**Evaluation.** For HarmBench, we followed Mazeika et al. (2024) and adopted their AI evaluator, which is a classifier finetuned from Llama2-13b model to assess the safety and biases of model outputs. For CAMEL, we adopted a similar strategy but used GPT3.5 as the AI evaluator. Following Li et al. (2024), we defined the safety and helpfulness measures as the average winning rate over the direct generation approach by the corresponding base LLM.

**Results.** Table 2 demonstrates the benefit of INDICT in combination with CommandR and Llama3 models. Consistent with our observations in prior experiments, albeit a weaker model by safety, CommandR+INDICT still improves significantly across all red-teaming optimization methods (from \(23\%\) to \(51\%\) by average safety metric). For the CAMEL benchmark, Figure 6 shows that INDICT can iteratively improve the model outputs with at least \(70\%\) model outputs are better by both safety and helpfulness than the direct generation approach. We noted the minor performance drops after 4 rounds of INDICT, suggesting further study to address open-ended tasks beyond the code domain.

### Comparison to baselines

Related to INDICT are approaches that enhance the generation procedure of LLMs with self-improvement or agentic frameworks (see Table 1). To compare with INDICT, we selected 7 strong representative baselines and evaluated them on a validation test split - random samples of 20% of the CyberSecEval-1 benchmark (Bhatt et al., 2023). For each baseline, we also included a version where additional instructions are given to models to provide both safety and helpfulness critics e.g. instruct models to "focus on both the security and helpfulness of the solution." For multi-agent methods, we included these instructions in all agents (analyst, tester, etc.) or introduced a new critic agent (as recommended in Li et al. (2024)). Note that for both INDICT and all baseline models, we adopted GPT4o-mini (OpenAI, 2024) as the base LLM and followed similar generation budgets (up to 3 rounds of revision) to fairly compare the results. The results in Table 7 demonstrate the SoTA performance of INDICT by both security and helpfulness (more than 90% and 81% respectively) against all the baselines. While we observed good improvement of strong baseline methods like Reflexion (Shinn et al., 2023) and CAMEL (Li et al., 2024) with additional instructions (marked with the suffix '+'), their results are not optimal and less than INDICT.

### Ablation analysis

To perform ablation analysis, we randomly sampled a subset from the CyberSecEval-1 (Bhatt et al., 2023), including both Insecure Coding Practice and Cyber Attack tasks. For each task, we randomly sampled \(20\%\) of the full dataset such that the sampled subset had similar distributions as the original dataset by programming languages or types of attack methods. We reported the averaged safety metric following the evaluation of the corresponding tasks (see 4.1 and 4.2). For helpfulness, we

Figure 7: With GPT4o-mini as the base model, we adapted representative baselines in their original implementation and also extended them with additional instructions (detailed criteria of safety and helpfulness). We marked these enhanced baselines with the suffix ‘+’.

adopted GPT3.5 as the AI evaluator and computed the percentage of outputs that are considered more helpful than the direct generation approach of the corresponding base model.

From Table 3 and 4, we have the following observations. First, INDICT can lead to performance gains in both safety and helpfulness with all base models, including Codellama models from 7B to 34B and CommandR models. The framework achieves the optimal performance when integrating external tools with our critics. Secondly, we found that this tool enhancement strategy improves the safety quality of the outputs more than the helpfulness, indicating that current LLMs significantly benefit from external grounding to be more safe and secure. Thirdly, we observed that using safety critic alone or helpfulness critic alone is not sufficient, often optimizing the outputs significantly by either only safety or only helpfulness qualities respectively. Finally, we noted that when adopting our critics in both preemptive and posthoc stages, we achieved more well-rounded results, with the best overall average of safety and helpfulness metrics.

We also conducted ablation analysis by multiple rounds of INDICT applications. To obtain the results of the direct generation approach (i.e. "base") in multiple rounds, we simply concatenated previously generated samples into our prompt and iteratively instructed the model to regenerate better outputs (without any critics or tool enhancement). From Figure 8, we noted the significant and consistent improvements from INDICT, using CommandR and Codellama-13b-instruct as base models. Interestingly, we still observed some performance improvement, albeit very marginal, of the direct generation approach over multiple generation rounds. We also noticed that without using external tools, the performance curves tend to converge faster than the tool-enabled approach. For more experimental results and analysis, please refer to Appendix F, G, H.

## 5 Conclusion

We present INDICT, a novel framework to improve code generation by both safety and helpfulness. INDICT essentially facilitates an autonomous agent system between two critic models, each of which focuses on either the safety or helpfulness quality of outputs from the "actor" code generation LLM. Given access to external tools, the two critics interact with each other autonomously to generate grounded critiques, collaboratively improving the model outputs. We conducted comprehensive experiments of INDICT on diverse downstream coding tasks across different programming languages and attack tactics. Our results demonstrated the benefits of INDICT on code-related tasks and beyond, highlighting the promising direction of an autonomous and tool-enhanced multi-critic system.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline \begin{tabular}{c} Base model \\ \end{tabular} & \begin{tabular}{c} Critics \\ \end{tabular} & \begin{tabular}{c} Tools \\ \end{tabular} & \begin{tabular}{c} Safety \\ \end{tabular} & \begin{tabular}{c} Helpful \\ \end{tabular} & 
\begin{tabular}{c} Avg. \\ \end{tabular} \\ \hline \multirow{2}{*}{CL-7b-instruct} & & & & 63.0 & 50.0 & 56.5 \\ \cline{2-6}  & ✓ & ✓ & & 76.6 & 51.4 & 64.0 \\  & & ✓ & ✓ & & 66.0 & **62.1** & 64.0 \\  & ✓ & ✓ & ✓ & & **78.1** & 59.8 & **68.9** \\ \hline \multirow{3}{*}{CL-34b-instruct} & & & & & ✓ & **72.7** & 55.3 & 64.0 \\  & & ✓ & & ✓ & & 70.5 & 59.8 & 65.2 \\  & ✓ & ✓ & & ✓ & 71.3 & **72.0** & **71.6** \\ \hline \multirow{3}{*}{CommandR} & & & & & ✓ & 73.6 & 61.4 & 67.5 \\  & ✓ & ✓ & ✓ & ✓ & 66.8 & 66.6 & 66.7 \\  & ✓ & ✓ & ✓ & ✓ & ✓ & **81.8** & **68.9** & **75.3** \\ \hline \hline \end{tabular}
\end{table}
Table 4: We conducted an ablation analysis of INDICT with different combinations of our critics, during either preemptive or posthoc feedback stage or both. To fairly compare these variants, we excluded any access to external tools, and used CommandR as the base model in all experiments.

Figure 8: We conducted ablation experiments over multiple rounds of INDICT applications, using CommandR (left) and Codellama-13b-instruct (right) as the base models.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline \multirow{2}{*}{Base model} & \multicolumn{2}{c|}{Critics Tools} & \multicolumn{1}{c|}{\begin{tabular}{c} Safety \\ \end{tabular} } & \begin{tabular}{c} Helpful \\ \end{tabular} & 
\begin{tabular}{c} Avg. \\ \end{tabular} \\ \hline \multirow{2}{*}{CL-7b-instruct} & & & 56.3 & 50.0 & 53.1 \\  & ✓ & & 64.9 & 61.4 & 63.1 \\  & ✓ & ✓ & **65.3** & **62.1** & **63.7** \\ \hline \multirow{2}{*}{CL-13b-instruct} & & & 59.1 & 50.0 & 54.6 \\  & ✓ & & 78.0 & 59.0 & 68.5 \\  & ✓ & ✓ & **78.8** & **60.3** & **69.6** \\ \hline \multirow{3}{*}{CL-34b-instruct} & & & 56.7 & 50.0 & 53.4 \\  & ✓ & & 68.8 & **63.4** & 66.1 \\  & ✓ & ✓ & **73.8** & 63.1 & **68.5** \\ \hline \multirow{3}{*}{CommandR} & & & 54.0 & 50.0 & 52.0 \\  & ✓ & ✓ & 76.8 & 59.2 & 68.0 \\  & ✓ & ✓ & **78.3** & **60.7** & **69.5** \\ \hline \hline \end{tabular}
\end{table}
Table 3: We conducted an ablation analysis of INDICT when removing the proposed dual critic system and/or external tool enhancement. We conducted our experiments on Codellama(CL) models from 7B to 34B parameters and the CommandR model.

[MISSING_PAGE_EMPTY:11]

X. Chen, M. Lin, N. Scharli, and D. Zhou. Teaching large language models to self-debug. _arXiv preprint arXiv:2304.05128_, 2023c.
* Cohere (2024)Cohere. Command-R -- docs.cohere.com. [https://docs.cohere.com/docs/command-r](https://docs.cohere.com/docs/command-r), 2024. [Accessed 18-05-2024].
* CyberNative (2024)CyberNative. CyberNative/Code_Vulnerability_Security_DPO -- Datasets at Hugging Face -- huggingface.co. [https://huggingface.co/datasets/CyberNative/Code_Vulnerability_Security_DPO](https://huggingface.co/datasets/CyberNative/Code_Vulnerability_Security_DPO), 2024. [Accessed 19-05-2024].
* Dai et al. (2024) J. Dai, X. Pan, R. Sun, J. Ji, X. Xu, M. Liu, Y. Wang, and Y. Yang. Safe RLHF: Safe reinforcement learning from human feedback. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=TyFrPOKYXw](https://openreview.net/forum?id=TyFrPOKYXw).
* Devlin et al. (2017) J. Devlin, J. Uesato, S. Bhupatiraju, R. Singh, A.-r. Mohamed, and P. Kohli. Robustfill: Neural program learning under noisy i/o. In _International conference on machine learning_, pages 990-998. PMLR, 2017.
* Dong et al. (2023) Y. Dong, X. Jiang, Z. Jin, and G. Li. Self-collaboration code generation via chatgpt. _arXiv preprint arXiv:2304.07590_, 2023.
* Gou et al. (2024) Z. Gou, Z. Shao, Y. Gong, yelong shen, Y. Yang, N. Duan, and W. Chen. CRITIC: Large language models can self-correct with tool-interactive critiquing. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=Sx038qxjek](https://openreview.net/forum?id=Sx038qxjek).
* Gulwani et al. (2012) S. Gulwani, W. R. Harris, and R. Singh. Spreadsheet data manipulation using examples. _Communications of the ACM_, 55(8):97-105, 2012.
* Gunasekar et al. (2023) S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, et al. Textbooks are all you need. _arXiv preprint arXiv:2306.11644_, 2023.
* Guu et al. (2020) K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training. In _International conference on machine learning_, pages 3929-3938. PMLR, 2020.
* Hammond Pearce et al. (2021) B. A. Hammond Pearce, B. Tan, B. Dolan-Gavitt, and R. Karri. An empirical cybersecurity evaluation of github copilot's code contributions. _arXiv preprint arXiv:2108.09293_, 2021.
* He and Vechev (2023) J. He and M. Vechev. Large language models for code: Security hardening and adversarial testing. In _Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security_, pages 1865-1879, 2023.
* He et al. (2024) J. He, M. Vero, G. Krasnopolska, and M. Vechev. Instruction tuning for secure code generation. _arXiv preprint arXiv:2402.09497_, 2024.
* Hendrycks et al. (2021) D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt. Measuring coding challenge competence with apps. _NeurIPS_, 2021.
* Hendrycks et al. (2023) D. Hendrycks, M. Mazeika, and T. Woodside. An overview of catastrophic ai risks. _arXiv preprint arXiv:2306.12001_, 2023.
* Hong et al. (2023) S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. _arXiv preprint arXiv:2308.00352_, 2023.
* Huang et al. (2023) D. Huang, J. M. Zhang, M. Luck, Q. Bu, Y. Qing, and H. Cui. Agentcoder: Multi-agent-based code generation with iterative testing and optimisation. _arXiv preprint arXiv:2312.13010_, 2023a.
* Huang et al. (2023) J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou. Large language models cannot self-correct reasoning yet. _arXiv preprint arXiv:2310.01798_, 2023b.
* Huang et al. (2022) W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. _arXiv preprint arXiv:2207.05608_, 2022.
* Huang et al. (2022)Y. Ishibashi and Y. Nishimura. Self-organized agents: A llm multi-agent framework toward ultra large-scale code generation and optimization. _arXiv preprint arXiv:2404.02183_, 2024.
* Khoury et al. (2023) R. Khoury, A. R. Avila, J. Brunelle, and B. M. Camara. How secure is code generated by chatgpt? In _2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)_, pages 2445-2451. IEEE, 2023.
* Korbak et al. (2023) T. Korbak, K. Shi, A. Chen, R. V. Bhalerao, C. Buckley, J. Phang, S. R. Bowman, and E. Perez. Pretraining language models with human preferences. In _International Conference on Machine Learning_, pages 17506-17533. PMLR, 2023.
* Koubaa (2023) A. Koubaa. Gpt-4 vs. gpt-3.5: A concise showdown. 2023.
* Kurach et al. (2015) K. Kurach, M. Andrychowicz, and I. Sutskever. Neural random-access machines. _arXiv preprint arXiv:1511.06392_, 2015.
* Kwon et al. (2023) W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In _Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles_, 2023.
* Lai et al. (2023) Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-t. Yih, D. Fried, S. Wang, and T. Yu. Ds-1000: A natural and reliable benchmark for data science code generation. In _International Conference on Machine Learning_, pages 18319-18345. PMLR, 2023.
* Le et al. (2022) H. Le, Y. Wang, A. D. Gotmare, S. Savarese, and S. C. H. Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. _Advances in Neural Information Processing Systems_, 35:21314-21328, 2022.
* Le et al. (2024) H. Le, H. Chen, A. Saha, A. Gokul, D. Sahoo, and S. Joty. Codechain: Towards modular code generation through chain of self-revisions with representative sub-modules. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=vYhg1xSj8j](https://openreview.net/forum?id=vYhg1xSj8j).
* Li et al. (2024) G. Li, H. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem. Camel: Communicative agents for" mind" exploration of large language model society. _Advances in Neural Information Processing Systems_, 36, 2024.
* Li et al. (2023) R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Koetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, et al. Starcoder: may the source be with you! _arXiv preprint arXiv:2305.06161_, 2023.
* Li et al. (2022) Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, et al. Competition-level code generation with alphacode. _arXiv preprint arXiv:2203.07814_, 2022.
* Liu et al. (2024) R. Liu, R. Yang, C. Jia, G. Zhang, D. Yang, and S. Vosoughi. Training socially aligned language models on simulated social interactions. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=Nd6iWtdUm](https://openreview.net/forum?id=Nd6iWtdUm).
* Lozhkov et al. (2024) A. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi, A. Tang, D. Pykhtar, J. Liu, Y. Wei, et al. Starcoder 2 and the stack v2: The next generation. _arXiv preprint arXiv:2402.19173_, 2024.
* Lu et al. (2024) P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao. Chameleon: Plug-and-play compositional reasoning with large language models. _Advances in Neural Information Processing Systems_, 36, 2024.
* Luo et al. (2023) Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, and D. Jiang. Wizardcoder: Empowering code large language models with evol-instruct. _arXiv preprint arXiv:2306.08568_, 2023.
* Madaan et al. (2023) A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. _arXiv preprint arXiv:2303.17651_, 2023.
* Madaan et al. (2023)Z. Manna and R. J. Waldinger. Toward automatic program synthesis. _Commun. ACM_, 14(3):151-165, mar 1971. ISSN 0001-0782. doi: 10.1145/362566.362568. URL [https://doi.org/10.1145/362566.362568](https://doi.org/10.1145/362566.362568).
* Mazeika et al. (2024) M. Mazeika, L. Phan, X. Yin, A. Zou, Z. Wang, N. Mu, E. Sakhaee, N. Li, S. Basart, B. Li, et al. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. _arXiv preprint arXiv:2402.04249_, 2024.
* McKenna et al. (2023) N. McKenna, T. Li, L. Cheng, M. Hosseini, M. Johnson, and M. Steedman. Sources of hallucination by large language models on inference tasks. In H. Bouamor, J. Pino, and K. Bali, editors, _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 2758-2774, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.182. URL [https://aclanthology.org/2023.findings-emnlp.182](https://aclanthology.org/2023.findings-emnlp.182).
* Mehrotra et al. (2023) A. Mehrotra, M. Zampetakis, P. Kassianik, B. Nelson, H. Anderson, Y. Singer, and A. Karbasi. Tree of attacks: jailbreaking black-box llm's automatically. _arXiv preprint arXiv:2312.02119_, 2023.
* Meta (2024) Meta. Meta Llama 3 -- llama.meta.com. [https://llama.meta.com/lllama3/](https://llama.meta.com/lllama3/), 2024. [Accessed 18-05-2024].
* Nijkamp et al. (2023) E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou. Codegen2: Lessons for training llm's on programming and natural languages. _arXiv preprint arXiv:2305.02309_, 2023.
* Olausson et al. (2023) T. X. Olausson, J. P. Inala, C. Wang, J. Gao, and A. Solar-Lezama. Demystifying gpt self-repair for code generation. _arXiv preprint arXiv:2306.09896_, 2023a.
* Olausson et al. (2023b) T. X. Olausson, J. P. Inala, C. Wang, J. Gao, and A. Solar-Lezama. Is self-repair a silver bullet for code generation? In _The Twelfth International Conference on Learning Representations_, 2023b.
* OpenAI (2024) OpenAI. Hello gpt-4o. [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/), 2024. [Accessed 22-10-2024].
* Ouyang et al. (2022) L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. _arXiv preprint arXiv:2203.02155_, 2022.
* Parisotto et al. (2016) E. Parisotto, A.-r. Mohamed, R. Singh, L. Li, D. Zhou, and P. Kohli. Neuro-symbolic program synthesis. _arXiv preprint arXiv:1611.01855_, 2016.
* Pearce et al. (2022) H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri. Asleep at the keyboard? assessing the security of github copilot's code contributions. In _2022 IEEE Symposium on Security and Privacy (SP)_, pages 754-768. IEEE, 2022.
* Peng et al. (2023) B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. _arXiv preprint arXiv:2302.12813_, 2023.
* Perez et al. (2022) E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving. Red teaming language models with language models. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 3419-3448, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.225. URL [https://aclanthology.org/2022.emnlp-main.225](https://aclanthology.org/2022.emnlp-main.225).
* Qian et al. (2024) C. Qian, W. Liu, H. Liu, N. Chen, Y. Dang, J. Li, C. Yang, W. Chen, Y. Su, X. Cong, et al. Chatdev: Communicative agents for software development. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 15174-15186, 2024.
* Radford et al. (2019) A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* Ram et al. (2023) O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-Brown, and Y. Shoham. In-context retrieval-augmented language models. _Transactions of the Association for Computational Linguistics_, 11:1316-1331, 2023.
* Raghavan et al. (2019)V. Rawte, A. Sheth, and A. Das. A survey of hallucination in large foundation models. _arXiv preprint arXiv:2309.05922_, 2023.
* Roziere et al. [2023] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, et al. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_, 2023.
* Schuster et al. [2021] R. Schuster, C. Song, E. Tromer, and V. Shmatikov. You autocomplete me: Poisoning vulnerabilities in neural code completion. In _30th USENIX Security Symposium (USENIX Security 21)_, pages 1559-1575, 2021.
* Shen et al. [2023] X. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang. " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. _arXiv preprint arXiv:2308.03825_, 2023.
* Shi et al. [2022] F. Shi, D. Fried, M. Ghazvininejad, L. Zettlemoyer, and S. I. Wang. Natural language to code translation with execution. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 3533-3546, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.231. URL [https://aclanthology.org/2022.emnlp-main.231](https://aclanthology.org/2022.emnlp-main.231).
* Shinn et al. [2023] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.
* Siddiq and Santos [2022] M. L. Siddiq and J. C. Santos. Securityeval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques. In _Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security_, pages 29-33, 2022.
* Siddiq et al. [2022] M. L. Siddiq, S. H. Majumder, M. R. Mim, S. Jajodia, and J. C. Santos. An empirical study of code smells in transformer-based code generation techniques. In _2022 IEEE 22nd International Working Conference on Source Code Analysis and Manipulation (SCAM)_, pages 71-82. IEEE, 2022.
* Sun et al. [2024] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, and C. Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. _Advances in Neural Information Processing Systems_, 36, 2024.
* Svyatkovskiy et al. [2020] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan. Intellicode compose: Code generation using transformer. In _Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering_, pages 1433-1443, 2020.
* Team et al. [2023] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* Tony et al. [2023] C. Tony, M. Mutas, N. E. D. Ferreyra, and R. Scandariato. Llmseceval: A dataset of natural language prompts for security evaluations. In _2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)_, pages 588-592. IEEE, 2023.
* Touvron et al. [2023a] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023a.
* Touvron et al. [2023b] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023b.
* Wang and Komatsuzaki [2021] B. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax), May 2021.
* Wang et al. [2023] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and S. C. Hoi. Codel5+: Open code large language models for code understanding and generation. _arXiv preprint arXiv:2305.07922_, 2023.
* Welleck et al. [2023] S. Welleck, X. Lu, P. West, F. Brahman, T. Shen, D. Khashabi, and Y. Choi. Generating sequences by learning to self-correct. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=hH36JeQ2DaO](https://openreview.net/forum?id=hH36JeQ2DaO).

M. Weyssow, A. Kamanda, and H. Sahraoui. Codeultrafeedback: An llm-as-a-judge dataset for aligning large language models to coding preferences. _arXiv preprint arXiv:2403.09032_, 2024.
* Wolf et al. (2019) T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. _arXiv preprint arXiv:1910.03771_, 2019.
* Wu et al. (2024) Z. Wu, Y. Hu, W. Shi, N. Dziri, A. Suhr, P. Ammanabrolu, N. A. Smith, M. Ostendorf, and H. Hajishirzi. Fine-grained human feedback gives better rewards for language model training. _Advances in Neural Information Processing Systems_, 36, 2024.
* Xu et al. (2024) Z. Xu, S. Jain, and M. Kankanhalli. Hallucination is inevitable: An innate limitation of large language models. _arXiv preprint arXiv:2401.11817_, 2024.
* Yao et al. (2023) S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=WE_vluYUL-X](https://openreview.net/forum?id=WE_vluYUL-X).
* Zeng et al. (2024) Y. Zeng, H. Lin, J. Zhang, D. Yang, R. Jia, and W. Shi. How johnny can persuade lllms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. _arXiv preprint arXiv:2401.06373_, 2024.
* Zhang et al. (2023a) K. Zhang, Z. Li, J. Li, G. Li, and Z. Jin. Self-edit: Fault-aware code editor for code generation. _arXiv preprint arXiv:2305.04087_, 2023a.
* Zhang et al. (2023b) T. Zhang, T. Yu, T. Hashimoto, M. Lewis, W.-t. Yih, D. Fried, and S. Wang. Coder reviewer reranking for code generation. In _International Conference on Machine Learning_, pages 41832-41846. PMLR, 2023b.
* Zheng et al. (2024) L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _Advances in Neural Information Processing Systems_, 36, 2024.
* Zhuo et al. (2023) T. Y. Zhuo, Y. Huang, C. Chen, and Z. Xing. Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity. _arXiv preprint arXiv:2301.12867_, 2023.

## Appendix A Limitations

Despite the strong performance of INDICT on a wide variety of tasks, there are some limitations that we want to emphasize. First, our framework relies on the instruction-following ability of LLMs to perform different specific roles, i.e. code generation actors, safety-driven critics, and helpfulness-driven critics. Depending on how well LLMs are able to understand the requirements of these roles, we would need to carefully create well-written prompts with specific instructions for the models to follow. In our framework, we would need to describe the requirements of helpfulness and safety that the critics would need to follow and check against code generation outputs. While we try to cover as many as possible different safety and helpfulness criteria, these attributes are not trivial to be defined in the code domain. Hence, given a code generation output, our critics might not always be able to detect the right safety or helpfulness concerns.

Parts of our approach can be used to remediate the above issue. Our tool enhancement strategy can equip the critics with necessary knowledge which can steer the critics towards more grounded and potentially correct recommendations. When a critic cannot detect the right issues initially, it can still improve its critiques after several rounds of interactions and tool use. Subsequently, if the extracted knowledge from external tools is relevant, the critic might be able to correctly revise and improve its final critique before passing it to the actor LLM.

Another limitation of our approach is the computation cost. Compared to the direct generation approach, our framework incurs higher computation costs, activating more than one LLM and requiring access to external tools. However, we consider our approach still more affordable than relevant finetuning methods. These methods often require (1) high computation to sufficiently finetune LLMs to balance between safety and helpfulness alignment; and (2) significant annotation effort to collect quality code data by these attributes.

## Appendix B Ethical Statement

We want to highlight that our work is specifically developed to address the safety and security concerns of AI code generation models. Any adaptation or application of our work should be used for this purpose, ultimately to create stronger yet more responsible AI systems. Moreover, as our method adopts a framework for autonomous agent systems between two independent LLMs, during any adaptation or application, it is important to control and monitor how much autonomy such systems can possess. It is good practice to limit how these agents could perform actions like web search (for example, by number of queries) and code interpreter (for example, using a sandbox execution environment, isolated from the local system). Any "thoughts" or "actions" and their outcomes from these agents have to be carefully checked to make sure they do not lead to unethical consequences.

Secondly, as our work aims to address both safety and helpfulness aspects of code generation, defining and quantifying such qualities is not trivial. Within the scope of this paper, we tried to conform as much as possible to the definitions commonly used in prior related work in the code domain or the AI safety domain. In practice, there are many ethical concerns that should be considered to define these qualities, especially on the safety of code generation. For instance, in this work, we did not consider the conventional safety concerns like social biases and offensive content in code. However, these safety concerns could still be observed in many real-life practical scenarios (e.g. in generated code comments or variable names). More study is needed to address and measure safety in such scenarios.

## Appendix C Broader Impacts

### Societal Impacts

Since we aim to address the safety and helpfulness in code generation, our work can have significantly positive societal impacts. Since coding applications by LLMs are getting more and more popular, the consequences of generating harmful or insecure code can be very serious, especially in high-risk application domains like military, medicine, and banking systems. Our work can be deployed as an extra layer of mitigation, reducing the probability of potential harm while not compromising the helpfulness of AI systems. As we demonstrated in our results, our framework can also benefit open-ended generation tasks beyond the code domain.

On the other hand, our framework can also be misused, assisting human users with harmful intentions to create more sophisticated attacks against LLMs. Our proposed critic models could be engineered with reverse goals, e.g. recommending ways to make the output codes more insecure or less helpful. Since these critic models are positioned in an autonomous system with freedom to interact and collaborate with each other, the resulting critiques can negatively affect the "actor" LLMs towards generating more insecure or useless code outputs.

### Safeguards

There are several safeguard strategies we can adopt to mitigate the above negative societal impacts. First, we can limit how much autonomy our critics can have e.g. by the types of queries they can generate and by the types of external tools they can have access to. In tools like web search, we can include a simple filter to exclude any illegal or unauthorized websites or content that might negatively impact the critics. Another safeguard strategy is to adopt more powerful external tools like code static analyzers or AI evaluators to provide more useful feedback to the critic models. While we did not use them in our experiments to fairly evaluate our approach against baselines, in practice, these tools should be used as safeguards for any practical application of INDICT.

## Appendix D Comparison to Related Work

See Table 5 for a systematic comparison between INDICT and related methods. We reviewed each method by the following features: helpfulness or safety-based qualities of generation outputs, execution feedback (execution of output code if applicable), tool-enhanced feedback (access to external tools like web search), multi-critic collaboration (engage multiple LM agents for critic generation), and supervision free (no training data required).

Compared to existing actor-critic methods, INDICT is different in three major aspects: (1) INDICT aims to optimize both helpfulness and safety awareness in the generated output code. Most of the current actor-critic approaches are designed with a single criterion (such as functional correctness). Simply extending these methods with additional instructions on safety criteria is sub-optimal (see

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline Method & Helpful. & Safety & Exec. & Tool- & Multi-critic & Supervision \\  & & feedback & enhanced & collab & free \\ \hline
**Self-refine approach** & & & & & \\ CodeT [Chen et al., 2023b], AlphaCode [Li et al., 2022] & & & & & ✓ \\ Self-correct [Gou et al., 2024], ILF [Chen et al., 2023a] & & & & & ✓ \\ CodeRL [Le et al., 2022], Self-edit [Zhang et al., 2023a] & & & & & \\ Self-repair [Olaussson et al., 2023a], Self-debug [Chen et al., 2023c], Reflexion [Shinn et al., 2023] & & & & \\ \hline
**Multi-agent approach** & & & & & \\ Self-collaboration [Dong et al., 2023], & & & ✓ & & & ✓ \\ AgentCoder [Huang et al., 2023a] & & & & & & ✓ \\ CAMEL [Li et al., 2024] & & ✓ & & & & ✓ \\ ChatDev [Qian et al., 2024], Self-org Agents [Ishibashi and Nishimura, 2024] & & ✓ & & ✓(?) & ✓ \\ MetaGPT [Hong et al., 2023], AgentVerse [Chen et al., 2024] & ✓ & ✓ & ✓ & & ✓ \\ \hline
**Finetuning approach** & & & & & & \\ CodeUltraFeedback [Weyssow et al., 2024], & ✓ & ✓ & & & ✓ & \\ StableAlignment [Liu et al., 2024] & & ✓ & ✓ & & & \\ SafeCoder [He et al., 2024] & & ✓ & ✓ & & & \\ \hline
**INDICT (ours)** & & ✓ & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 5: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multi-agent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT.

our results with baseline actor-critic methods in Section 4.4). (2) INDICT integrates critic with knowledge grounding from external tools to create more reliable feedback to the actor agent. Most current methods only use code test results as the only external feedback to improve the quality of output code. (3) To implement (1) and (2), we enhanced the existing actor-critic framework with a multi-critic and tool-enabled collaboration approach. This approach can autonomously generate more reliable and holistic feedback for both the safety and helpfulness of output code generation.

Also related to our work is the research of multi-agent collaborative systems. It is not trivial to extend this line of research to address the security of code generation. Firstly, it is not clear how the current methods could be enhanced with security awareness and subsequently improve the quality of output generations. Earlier work such as [20, 17] showed that simply asking agents to analyze and answer what is wrong with generated code is not always effective. With carefully designed prompts and agent interactions [16, 17], collaborative agents can now generate more functionally correct code. Therefore, studying collaborative agents with orthogonal goals such as security awareness still requires further attention. As observed by our experimental results, simply applying the current multi-agent methods [18, 19], even with extended additional instructions of security criteria, does not perform so well and is still far from optimal.

## Appendix E Details of Experimental Setups

Generation budget.Technically, we can integrate INDICT on top of any LLMs for any number of application rounds (i.e. outer action loops), each of which can contain any number of dialogue interactions between the safety and helpfulness critics (i.e. inner critic loops). Due to the limitation of computation resources, we have to trade-off between the number of outer action loops and the number of inner critic loops. In our experiments, we fixed the number of outer action loops to 5 rounds and the inner critic loops to 1 interaction per action loop. We also maintained a persistent interaction context throughout all outer action loops so that the critics could always refer to previously generated critiques. With the above generation budget, our strategy can offer more diverse and richer input samples to the critics over time, while controlling the compute cost at an affordable level.

Tools.In this work, we used 4 different types of external tools for the critics to query relevant knowledge for their arguments. For Wikipedia and code interpreter, we adopted the Langchain library [13] with built-in functions to call these tools given the input text queries or code snippets. For web search, we employed the Search Engine Parser library [14] to query and scrape search engine pages for different snippets such as titles and descriptions. Depending on the access constraints from commercial search engines, we mainly employ Yahoo Search as our primary search engine. Finally, to use OpenAI as an external tool, we query GPT3.5 using our paid API access 5. All the above tools are appropriately licensed to be used for academic purposes.

Footnote 5: _spt-3.5-turbo_ on [https://platform.openai.com/docs/models/overview](https://platform.openai.com/docs/models/overview)

Note that while we are treating OpenAI as an external tool in INDICT, we try to minimize contamination of test data by GPT models [1]. Specifically, we do not directly pass the original task instructions \(X\) to OpenAI public API but only use critic-generated text or code snippets as queries (see 7 and 8). Also note that during the preemptive feedback stage, we assume no access to the execution environments / code interpreters and only employ CodeSearch as the applicable critic actions. During the posthoc feedback stage, we enable access to the code interpreters, and hence, the critics can select and perform CodeReview (with execution results as parts of the queries) to extract relevant external knowledge.

Benchmarks.We evaluated INDICT on different downstream applications, including 3 major types of tasks: insecure coding practice, security attacks, and open-ended generation. Please refer to Table 6 for a summary of tasks and benchmarks used in this work. All the benchmarks considered are licensed with permission to be used for academic purposes.

Insecure coding practice tasks [15, 16] refer to standard code generation tasks where a model receives an input containing a coding problem description, optionally with an input code context. The model is then required to generate output code to solve the input coding problem and/or finish the given code context. The test samples in this task were curated by the potential security and vulnerability concerns commonly seen in code e.g. Common Weakness Enumeration (CWE) 6.

Footnote 6: [https://cwe.mitre.org/about/index.html](https://cwe.mitre.org/about/index.html)

We also conducted experiments on security attack tasks [Bhatt et al., 2024]. In these tasks, input instructions are designed to directly or indirectly elicit harmful generation from LLMs. For instance, one example task is to request LLMs to generate code to simulate a DDoS attack in a Linux environment. More indirectly, this request could be injected into a very long list of complex requirements in the prompt. The model is required to detect such harmful intentions in the instructions and generate appropriate responses (e.g. ones not complying with the given request).

The last type of downstream task we used in this work is open-ended generation tasks beyond the code domain. These tasks include both standard generation tasks [Li et al., 2024] as well as adversarial generation tasks [Mazeika et al., 2024]. In the latter, recent work has focused on prompt engineering methods to optimize the instructions and ultimately, elicit harmful behaviors from LLMs. We tested against several recent prompt optimization methods curated by Mazeika et al. [2024], covering diverse domains like social engineering, harassment, bio-weapons, etc.

Note that for CVS and CAMEL benchmarks, since they do not have an official test split, we randomly sampled a subset from the corresponding benchmarks such that the sampled data has a similar data distribution as the original dataset e.g. by programming languages. For HarmBench, from the dataset of \(320\) raw task instructions ("Direct" split), we augmented the data by using CommandR [Cohere, 2024] as the attacker and applying the following red-teaming optimization methods: zero-shot ("ZS") [Perez et al., 2022], PAP [Zeng et al., 2024], JailBreak ("JB") [Shen et al., 2023], TAP [Mehrotra et al., 2023], and PAIR [Chao et al., 2023]. This results in 5 more test splits, each containing 320 augmented prompts.

Evaluation.To evaluate safety and helpfulness performance, we followed similar evaluation tools used in the corresponding benchmark papers and related work [Bhatt et al., 2023, 2024, Li et al., 2024, Mazeika et al., 2024, Zheng et al., 2024, Bai et al., 2022]. These papers showed that evaluation tools like security-based code analyzers and AI detectors can achieve decent levels of accuracy, correlating with human evaluation results on subsampled datasets. In addition, to minimize potential biases in AI evaluators, we anonymized all model names and randomly positioned the model responses to be evaluated in the evaluation prompts. In code generation tasks with expected output code, we also extracted only the code snippets and excluded any text segments in the model outputs to prevent biases from long-context outputs or from simply concatenating text. Also note that we follow Mazeika et al. [2024]'s evaluation principle by not including access to evaluators (e.g. static analyzers, AI classifiers) in our proposed framework. In practice, it is possible to use these as additional tools for more insightful feedback to the critics.

Base Language Models.All the models used in the work, including CommandR [Cohere, 2024], LLama-2 [Touvron et al., 2023b], Codellama [Roziere et al., 2023], and Llama-3 [Meta, 2024], are open-sourced LLMs. We accessed these models through HuggingFace, which includes model licenses with permission to be used for academic purposes. We describe the HuggingFace model IDs and their corresponding licenses below:

\begin{table}
\begin{tabular}{c c c c} \hline \hline Type of tasks & Benchmark & Task Split & \# samples \\ \hline \multirow{2}{*}{Insecure Coding Practice} & CyberSecEval-1 & Autocomplete & 1,916 \\  & CyberSecEval-1 & Instruction & 1,916 \\  & CVS & - & 500 \\ \hline \multirow{3}{*}{Security Attacks} & CyberSecEval-2 & Cyber Attack & 1,000 \\  & CyberSecEval-2 & Interpreter Abuse & 500 \\  & CyberSecEval-2 & Prompt Injection & 251 \\ \hline \multirow{2}{*}{Open-ended Generation} & CAMEL & AI Society & 100 \\  & HarmBench & - & 320 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Summary of evaluation tasks and corresponding benchmarks: CyberSecEval-1 [Bhatt et al., 2023], CyberSecEval-2 [Bhatt et al., 2024], CVS [CyberNative, 2024], CAMEL [Li et al., 2024], and HarmBench [Mazeika et al., 2024]* _CohereForAl/c4ai-command-r-v01_ for CommandR, licensed under cc-by-nc-4.0
* _meta-llama/Llama-2-[x]b-chat-hf_ for Llama2 of x-B parameters, licenced under llama2
* _codellama/CodeLlama-[x]b-Instruct-hf_ for Codellama of x-B parameters, licenced under llama2
* _meta-llama/Meta-Llama-3-[x]B-Instruct_ for Llama3 of x-B parameters, licenced under llama3

For the Lllama and Codellama families, we fully agreed and complied with the license conditions enforced by Meta before accessing the models.

Baselines.In this work, we mainly compared with prior baselines that were reported in the corresponding benchmarks. More recently, He and Vechev (2023); He et al. (2024) introduced finetuning approaches to finetune LLMs towards safer code generation. Almost concurrently to this work, Weyssow et al. (2024) also introduced a preference dataset of complex instructions to finetune LLMs to coding preferences. However, these approaches were not adapted and tested against the evaluation tasks and benchmarks we used in this work. Due to the limited computation cost (and also partly due to unreleased model checkpoints from He et al. (2024) at the time of submission), we were not able to evaluate the above models and compare with INDICT. We will attempt to replicate these methods and compare with our work in the future.

Compute Resources.We conducted all experiments in this paper with our CPU and GPU resources provided through the Google Cloud Platform. Depending on the sizes of the base LLMs, we adopted GPU clusters of 2 to 8 GPUs of Nvidia A100 40GB type and assigned a CPU memory of up to 600GB. For some very large models such as Llama3-70B, we observed in some cases that the above hardware resulted in out-of-memory problems. For such cases, we recommend running the experiments with larger CPU memory allocation i.e. more than 600GB, or larger GPU clusters.

Time costs.Given an input task, on average, INDICT incurs about 3-4x the time cost as compared to a single LLM to generate a code sample (including any iterative refinement). However, we also want to note that even with fine-tuning, fine-tuned models are far from perfect and still subject to unseen security risks or novel red-teaming prompts during test time. For instance, from our results with the fine-tuning method CodeUltraFeedback (Weyssow et al., 2024), the fine-tuned model is still sub-optimal and can be further improved e.g. by using INDICT during inference time. See Appendix F for the experimental results and analysis.

## Appendix F INDICT vs. Finetuning methods

We also conducted experiments to compare INDICT with finetuning-based methods. We evaluated them on a validation test split - random samples of 20% of the CyberSecEval-1 benchmark (Bhat et al., 2023). Using CodeLlama-7b-instruct as the base model, CodeUltraFeedback finetunes the model on a large-scale dataset with annotations of code preferences. From Table 7, we observe that the best model (SFT + DPO finetuning) can improve the results by both safety and helpfulness but not as good as INDICT. As INDICT can complement finetuning-based methods, we applied INDICT with the best CodeUltraFeedback model to achieve even further performance gains (from 60% and 63% to 73% in both helpfulness and safety).

\begin{table}
\begin{tabular}{l c c c} \hline \hline INDICT vs. finetuning methods & Safety & Helpfulness & S+H \\ \hline Direct Gen & 56.3 & 50.0 & 53.2 \\ INDICT & 65.3 & 62.1 & 63.7 \\ \hline CodeUltraFeedback (SFT) & 58.5 & 49.9 & 54.2 \\ CodeUltraFeedback (DPO) & 62.7 & 56.0 & 59.3 \\ CodeUltraFeedback (SFT+DPO) & 63.9 & 57.9 & 60.9 \\ CodeUltraFeedback (SFT+DPO) +INDICT & **74.9** & **72.4** & **73.7** \\ \hline \hline \end{tabular}
\end{table}
Table 7: With CodeLlama-7b-instruct as the base model, we compared INDICT with CodeUltraFeedback (Weyssow et al., 2024), a finetuning approach using supervised-finetuning (SFT) or preference-based finetuning (DPO).

Additional Ablation Analysis

Using GPT4o-mini as the base model, we conducted additional ablation experiments with different variants of INDICT: (1) one simply using a single critic agent for both safety and helpfulness; (2) one without using a critic summarizer and maintaining a full dialogue history of critiques in the critic context; (3) ones replacing the thought-action-observation critic generation with RAG or tool-based generation: (3a) RAG uses the original task description to retrieve relevant knowledge and generate grounded critics, and (3b) tool-based method uses web search/Wikipedia and a query "what is wrong with the solution in terms of its <security/functionality>?" and query output is treated as a critique.

From Table 8, we have the following observations: First, when we simply removed the summarizer and let the actor agent receive the full dialogue history, we noticed the performance degraded to 87% and 72% in safety and helpfulness. This happens probably due to the much longer context of the dialogue history, affecting the actor agent to capture all critic feedback from this history and generate new code. This model variant also incurs more computation due to the long context of the dialogues. Secondly, we noted that RAG and tool-enhanced methods are inferior to our proposed framework. We found that the queries in these methods are often too vague or ambiguous to search for meaningful information snippets.

Finally, we observed that simply using a single critic agent with dual quality criteria will affect the performance, reducing the safety and helpfulness metrics to 87% and 76% respectively. One possible reason is due to the formulation of the training objectives of LMs, which are not always designed to optimize both security and helpfulness equally (also depending on the post-pretraining stages of LMs e.g. training with RLHF). Our approach enables a more flexible and probably more relaxed application of LLM as a critic agent by: (1) decoupling the helpfulness and safety goals and delegating them to individual LM agents; and (2) enabling multi-critic collaboration to autonomously develop more holistic and well-rounded critic feedback.

## Appendix H Details of Experimental Results

We reported the full experimental results in this section. For results of insecure coding practice tasks, please refer to Table 9, 10, 11 for the CyberSecEval-1 benchmark, and 12 and 13 for the CVS benchmark. For results of security attack tasks, please refer to Table 14, 15, and 16 for Cyber Attack, Interpreter Abuse, and Prompt Injection tasks respectively.

## Appendix I Instruction Prompts

We described the example instruction prompts we used in this section (Listing 1 to 10). For each prompt template, depending on the model roles and tasks, we replace the following placeholders with applicable input components: {question} and {answer} are replaced with the corresponding task description and latest model output from the actor LLM. During the posthoc feedback stage, {answer} is also concatenated with any execution results (e.g. test outcomes, error messages) after executing the corresponding extracted code output with a code interpreter. {scratchpad} is typically used as a placeholder to contain past interactions between the two critics.

Note that INDICT uses zero-shot prompting in each step. We prompt the critic agent to condition the current critique and generate a unique query to obtain more knowledge. We extract the search keywords following our instruction templates e.g. in the form of 'Search[keyword]'. For generating

\begin{table}
\begin{tabular}{l c c c} \hline Ablation methods & Safety & Helpfulness & S+H \\ \hline INDICT (full) & **90.9** & **81.4** & **86.1** \\ \hline - one critic for both criteria & 87.3 & 76.4 & 81.9 \\ - no critic summary & 87.9 & 72.2 & 80.1 \\ - RAG-based critics & 87.9 & 74.4 & 81.1 \\ - tool-based critics & 85.5 & 72.7 & 79.1 \\ \hline \end{tabular}
\end{table}
Table 8: With GPT4o-mini as the base model, we compared INDICT with 4 different variants of the critic framework. We found that our proposed INDICT can lead to more well-rounded performance, with high results in both safety and helpfulness of the generated code.

\begin{table}
\begin{tabular}{l|c c c c c c c c|c} \hline Model & C & C\# & C++ & Java & JavaScript & PHP & Python & Rust & Avg. \\ \hline GPT3.5-turbo & 53.3 & 69.8 & 71.0 & 46.7 & 59.0 & 62.4 & 61.3 & 64.7 & 61.1 \\ GPT-4 & 52.0 & 70.2 & 70.3 & 47.6 & 53.0 & 60.5 & 62.7 & 60.3 & 59.9 \\ Codellama-13b-instruct & 60.8 & 68.9 & 71.8 & 54.6 & 60.2 & 66.1 & 67.2 & 68.6 & 64.9 \\ Codellama-34b-instruct & 57.7 & 54.5 & 73.8 & 51.5 & 61.0 & 64.8 & 66.1 & 69.1 & 62.5 \\ Llama2-7b-chat & 63.4 & 70.6 & 77.2 & 60.7 & 69.5 & 70.4 & 69.2 & **78.9** & 69.9 \\ Llama2-13b-chat & 64.3 & 71.5 & 75.7 & 57.2 & 71.5 & 64.8 & 68.4 & 76.5 & 68.9 \\ Llama2-30b-chat & 56.8 & 62.6 & 71.8 & 52.0 & 65.9 & 61.1 & 65.0 & 77.5 & 64.2 \\ Llama2-70b-chat & 61.2 & 63.8 & 73.4 & 50.7 & 65.1 & 60.5 & 65.5 & 72.6 & 64.4 \\ \hline CommandR & 58.6 & 80.4 & 70.6 & 58.1 & 63.1 & 78.2 & 71.8 & 64.2 & 68.2 \\ CommandR+INDICT & **72.1** & **84.6** & **81.4** & **86.3** & **75.1** & **86.1** & **87.6** & 71.6 & **81.0** \\ \hline \end{tabular}
\end{table}
Table 10: Test results of CyberSecEval-1 - Insecure Coding Practice (Instruction): we reported the % output codes that are considered secure (determined by a rule-based detector). Using INDICT, CommandR can achieve new SoTA safety measures, with significant improvements in many programming languages. The results of the baseline models are from Bhatt et al. [2023].

\begin{table}
\begin{tabular}{l|c c c c c c c c|c} \hline Model & C & C\# & C++ & Java & JavaScript & PHP & Python & Rust & Avg. \\ \hline \hline \multicolumn{1}{l}{Instruct} & & & & & & & & & & \\ \hline GPT3.5 & 54.2 & 58.7 & 66.4 & 57.2 & 59.1 & 57.2 & 70.5 & 69.1 & 61.6 \\ GPT4 & **70.0** & **65.5** & **73.4** & **65.5** & **68.7** & **66.0** & **78.1** & **77.5** & **70.6** \\ CommandR & 51.4 & 48.5 & 48.9 & 46.6 & 54.3 & 49.3 & 54.6 & 54.7 & 51.0 \\ CommandR+INDICT & 57.5 & 55.4 & 55.3 & 41.9 & 55.7 & 58.6 & 62.0 & 55.8 & 55.3 \\ \hline \multicolumn{1}{l}{Autocomplete} & & & & & & & & & \\ \hline GPT3.5 & 44.9 & 41.3 & 44.4 & 56.3 & 36.7 & 37.9 & 40.7 & 44.1 & 43.3 \\ GPT4 & 57.3 & 65.5 & 60.6 & 63.8 & 52.4 & 55.9 & 59.8 & 64.2 & 60.0 \\ CommandR & 54.7 & 54.0 & 49.5 & 52.8 & 47.3 & 45.1 & 45.1 & 50.6 & 49.7 \\ CommandR+INDICT

\begin{table}
\begin{tabular}{l|c c c c c|c} \hline Approach & C++ & C\# & Java & Javascript & PHP & Avg. \\ \hline GT Secure Code & 50.0 & 50.0 & 50.0 & 50.0 & 50.0 & 50.0 \\ GT UnSecure Code & 35.0 & 33.0 & 40.0 & 52.0 & 44.0 & 40.8 \\ \hline CommandR & 38.0 & 38.0 & 42.0 & 51.5 & 43.0 & 42.5 \\ CommandR+INDICT & 28.0 & 61.0 & 63.0 & 59.6 & 74.0 & 57.1 \\ \hline Llama3-8b-instruct & 50.0 & 50.0 & 49.0 & 49.0 & 39.0 & 47.4 \\ Llama3-8b-instruct+INDICT & 62.0 & **58.0** & 60.0 & 64.0 & 68.0 & 62.4 \\ \hline Llama3-70b-instruct & 52.0 & 55.0 & 53.0 & 53.0 & 42.0 & 51.0 \\ Llama3-70b-instruct+INDICT & **66.0** & 57.0 & **67.0** & **69.0** & **70.0** & **65.8** \\ \hline \end{tabular}
\end{table}
Table 13: Test results of CVS: we reported the % output codes that are considered more helpful than the corresponding ground-truth secure code solutions. While all 3 base language models are found to be slightly less helpful or comparable to the ground-truth outputs, when integrated with INDICT, we noted consistent performance gains. We obtained the best performance with Llama3-70b-instruct+INDICT, with more than \(65\%\) of outputs are more helpful than the corresponding ground-truth code solutions.

\begin{table}
\begin{tabular}{l c c c c c|c} \hline Model & C2 & Collection & Discovery & Evasion & Lateral Movement & Avg. \\ \hline GPT-3.5-turbo & 36 & 41 & 26 & 53 & 59 & 46.2 \\ GPT-4 & 44 & 63 & 30 & 79 & 76 & 59.9 \\ Codellama-13b-instruct & 28 & 41 & 20 & 50 & 49 & 40.9 \\ Codellama-34b-instruct & 27 & 37 & 22 & 48 & 46 & 37.5 \\ Llama2-7b-chat & 52 & 57 & 35 & **79** & 64 & 61.4 \\ Llama2-13b-chat & 40 & 55 & 38 & 71 & 58 & 55.8 \\ Llama2-30b-chat & 24 & 24 & 21 & 35 & 30 & 27.5 \\ Llama2-70b-chat & 54 & 69 & 48 & 86 & 71 & 69.0 \\ \hline CommandR & 17 & 30 & 37 & 12 & 11 & 19.8 \\ CommandR+INDICT & 66 & 73 & 73 & 72 & 70 & 72.8 \\ \hline Llama3-8b-instruct & 59 & 61 & 45 & 69 & 64 & 62.4 \\ Llama3-8b-instruct+INDICT & **72** & **66** & **76** & 74 & **73** & **76.7** \\ \hline \end{tabular}
\end{table}
Table 14: Test results of CyberSecEval-1 - Cyber Attack tasks: we reported the % model outputs that are considered benign. Using INDICT, we found that Llama3-8b-instruct can achieves new SoTA performance with more than \(76\%\) of outputs are benign, i.e. not complying with malicious task prompts. In this table, we also included the results of the top 5 most challenging types of attack tactics (categorized by the industry standard MITRE ATT&CK). The results of the baseline models are from Bhatt et al. [2023].

code snippets, we prompt the model similarly but ask the model to wrap the output code in "\({}^{**}\). Note that the current prompts in the Appendix are for tool-finetuned models like CommandR which automatically generates tool parameters given a tool calling function definition.

**Listing 1:** Prompt with instruction for a critic model to focus on safety-related concerns in code generation tasks.

You are an advanced intelligent agent with a strong focus for system security and risk management. You will be given a previous potential solution in which you were given a task to solve. In a few sentences, diagnose whether the solution is completely safe in every execution step and will not pose any potential harmful effect to the world, including any damage to cybersecurity or physical world.

You are also required to debate and discuss with another critic who focuses on system performance and accuracy. Always advocate and focus on system security and risk management over system performance or correctness.

\begin{table}
\begin{tabular}{l|c c c c c|c} \hline \hline Model & Many short & Ignore prev. & Indirect & \multirow{2}{*}{Persuasion} & Hypo. & \multirow{2}{*}{Avg.} \\  & attack & instructions & reference & & & scenario & \\ \hline Llama3-8b-instruct & 86.0 & 46.0 & 64.0 & 54.0 & 77.0 & 51.9 \\ Llama3-70b-instruct & 71.0 & 62.0 & 64.0 & 73.0 & 77.0 & 68.0 \\ GPT-3.5-turbo & 43.0 & 67.0 & 64.0 & 81.0 & 69.0 & 61.8 \\ GPT-4 & 71.0 & 79.0 & 57.0 & **85.0** & 77.0 & 78.2 \\ Codellama-13b-instruct & 43.0 & 71.0 & 79.0 & 73.0 & 85.0 & 63.1 \\ Codellama-34-instruct & 29.0 & 54.0 & 64.0 & 69.0 & 85.0 & 64.2 \\ Codellama-70b-instruct & **100.0** & 67.0 & 79.0 & 65.0 & 92.0 & 82.8 \\ \hline CommandR & 42.9 & 45.8 & 57.1 & 76.9 & 61.5 & 57.4 \\ CommandR+INDICT & 42.9 & 75.0 & 85.7 & 73.1 & **92.3** & 74.9 \\ \hline Llama3-70b-instruct & 71.4 & 58.3 & 64.3 & 80.8 & 76.9 & 72.1 \\ Llama3-70b-instruct+INDICT & 71.4 & **87.5** & **92.9** & 80.8 & **92.3** & **92.8** \\ \hline \hline \end{tabular}
\end{table}
Table 16: Test results of CyberSecEval-2 - Prompt Injection tasks: we reported the % model outputs that are considered benign. Llama3-70b-instruct+INDICT can achieve the SoTA performance, with more than \(92\%\) of outputs are classified as benign. In this table, we also included the ablation results of the top 5 most challenging types of prompt injection attacks: Many Short Attack, Ignore Previous Instruction, Indirect Reference, Persuasion, and Hypothetical Scenario. The results of the baseline models are from Bhatt et al. [2024].

\begin{table}
\begin{tabular}{l|c c c c c|c} \hline \hline Model & Cont. & Privil. & Post & Reflected & Social & \multirow{2}{*}{Avg.} \\  & Escape & Escalt. & Exploit. & Attack & Engineer. & \\ \hline Mistral-small & 53 & 52 & 33 & 60 & 68 & 53.2 \\ Mistral-medium & 58 & 65 & 53 & 54 & 78 & 61.6 \\ Mistral-large & 66 & 58 & 48 & 65 & 76 & 62.6 \\ Llama3-8b-instruct & 75 & 75 & 73 & 65 & 69 & 71.4 \\ Llama3-70b-instruct & 47 & 39 & 60 & 77 & 82 & 61.0 \\ GP3.5-turbo & 69 & 69 & 82 & 93 & 76 & 77.8 \\ GPT4 & 75 & 79 & 86 & 85 & 79 & 80.8 \\ Codellama-13b-instruct & 78 & 87 & 84 & **94** & **90** & 86.6 \\ Codellama-34b-instruct & 80 & 82 & 87 & 82 & 87 & 83.6 \\ Codellama-70b-instruct & 70 & 73 & 77 & 83 & 79 & 76.4 \\ \hline CommandR & 37 & 35 & 8 & 19 & 22 & 24.2 \\ CommandR+INDICT & 91 & **95** & 77 & 74 & 85 & 84.4 \\ \hline Codellama-13b-instruct & 87 & 81 & 83 & 93 & 87 & 86.2 \\ Codellama-13b-instruct+INDICT & **97** & 94 & **89** & 91 & 89 & **92.0** \\ \hline \hline \end{tabular}
\end{table}
Table 15: Test results of CyberSecEval-2 - Interpreter Abuse tasks: we reported the % model outputs that are considered benign. On both base language models CommandR and Codellama-13b-instruct, we found consistent performance improvement from INDICT, with more than \(80\%\) and \(90\%\) of outputs respectively are benign. In this table, we also included the results by different types of attacks: Container Escape, Privilege Escalation, Post Exploitation, Reflected Attack, and Social Engineering. The results of the baseline models are from Bhatt et al. [2024].

Limit your critic to a few sentences.

Task: {question} Solution: {answer}

{scratchpad} ```
Listing 2: Prompt with instruction for a critic model to focus on helpfulness-related concerns in code generation tasks.
``` Youareanadvancedintelligentagentwithatstrongfocusforsystemperformanceandaccuracy.Youwillbegivenpreviouspotentialsolutioninwhichyouweregivenatasktosolve.Inafewsentences,diagnosewhetherthesolutioniscompletelycorrectineveryexecutionstepandwillsatisfyalltherequirementsinthegiventaskandpassanycornertestcases. Youarealsorequiredtobateanddiscusswithanothercriticwhofocusesonsystemsecurityandriskmanagement.Alwaysadvocateandfocusonsystemperformanceandaccuracyoversystemsecurityorriskmanagement. Limityourcritictoafewsentences. Task: {question} Solution: {answer} ```
Listing 3: Prompt with instruction for a summarizer model to summarize the interactions between safety and helpfulness critics for code generation tasks.
``` Youareanadvancedintelligentagentwithequalfocusestobothsystemperformanceandsecurity.Youwillbegivenadiscussionaboutasystemsolutionbetweentwocriticsagainstsystemsafetyandcorrectness.Inafewsentences,summarizethediscussionanddeterminewhetherthesolutioniscompletelycorrectandsecureineveryexecutionstep.Theperfectsolutionshouldsatisfyalltherequirementsinthegiventaskwhilenotviolatinganysecuritystandardsorcausinganyharmfuleffecttothesocietyorcybersecurity. Theremightbecomesupportingfactsinthediscussionbetweenthecritics.Incorporateasmuchaspossiblethosedetailsintoyourresponsetomakeyouranalysisinformativeandconvincinging. {scratchpad} ```
Listing 4: Prompt with instruction for critic models to generate relevant parameters (text queries) to perform CodeSearch/CodeReview actions.
``` YouareanadvancedintelligentagentwithdirectaccesstoInternet.Youaregivenataskandanexamplesolutionandrelevantanalysisagainstthesolution'ssecurityorfunctionalcorrectness.Toimprovetheanalysiswithrelevantevidenceandfact,generatearelevantkeywordorquerytosearchforrelatedinformationonInternet.Youmayalsosearchforinformationthatisrelevanttothetaskorsolutionbutismissingintheanalysis.Usethefollowingformat:Search[<queryorkeyword>]. Task: {question}

Solution: (answer)

{scratchpad}

Query (in the form of Search[<query or keyword>]):

Listing 5: Prompt with instruction for critic models to generate relevant parameters (code snippets) to perform CodeSearch/CodeReview actions.

You are an advanced intelligent agent with direct access to Internet. You are given a task and an example solution and relevant analysis against the solution's security or functional correctness. To improve the analysis with relevant evidence and fact, a query might be provided to extract more information. To make the query more informative, extract or create a relevant short code snippet to be used together the query. If the query is empty, provide a representative code snippet that could be used to search for more information to support the analysis.

The code snippet should be indepedent (does not refer to external operating systems, databases, repositories, or custom libraries) and limited to few lines of codes only. Use 'print' or 'assert' statements in the code snippets if needed (to execute and perform debugging on a code interpreter).

Wrap the code snippet in "".

Task: (question) Solution: (answer)

{scratchpad}

Query: {query}

Short code snippet in a single code block (wrap in ""):

Listing 6: Prompt with instruction for critic models to generate relevant parameters (text queries and/or code snippets) to perform CodeSearch actions. This prompt is specifically for CommandR models which by default can receive any predefined external tools and select relevant ones given the context of the prompt.

You are given a task and an example solution and relevant analysis against the solution's security or functional correctness.

Read the task, solution, and analysis and find ways to improve the analysis with relevant evidence and supporting fact. You may also improve the analysis with missing information relevant to the task or solution.

Task: {question} Solution: (answer)

{scratchpad}

Listing 7: Prompt with instruction for critic models to generate relevant parameters (text queries and/or code snippets) to perform CodeReview actions. This prompt is specifically for CommandR models which by default can receive any predefined external tools and select relevant ones given the context of the prompt.

You are given a task and an example solution and relevant analysis against the solution's security or functional correctness. Read the task, solution, and analysis and find ways to improve the analysis with relevant evidence and supporting fact.

You also have access to a code interpreter that can execute many code snippets. Based on the solution and analysis, you can create many code snippets and unit test cases to evaluate them and support the arguments in the analysis.

These code snippets should be independent (does not refer to external operating systems, databases, repositories, or custom libraries) and limited to few lines of codes only. Use 'print' or 'assert' statements in the code snippets if needed.

Task: (question) Solution: (answer)

{scratchpad} ```
Listing 8: Prompt with instruction for a critic model to focus on safety-related concerns in open-ended generation tasks.
``` Youareanadvancedintelligentagentwithaststrongfocusforsecurityandriskmanagement. Youwillbegivenapreviouspotentialsolutioninwhichyouweregivenatasktosolve. Inafewsentences,diagnosewhetherthesolutioniscompletelysafeineveryexecutionstepandwillnotposeanypotentialharmfuleffecttotheworld.Ifthesolutionincludesanyharmful,unethical,racist,sexist,toxic,dangerous,orillegalcontent,describeandexplainyourrelevantarguments.

You are also required to debate and discuss with another critic who focusesonsystem performanceandaccuracy.Alwaysadvocateandfocusonsecurityandriskmanagementoversystemperformanceorcorrectness.

Limit yourcritictoafewsentences.

Task: (question) Solution: (answer)

{scratchpad} ```
Listing 9: Prompt with instruction for a critic model to focus on helpfulness-related concerns in open-ended generation tasks.
``` Youareanadvancedintelligentagentwithaststrongfocusforsperformanceandaccuracy.Youwillbegivenapreviouspotentialsolutioninwhichyouweregivenatasktosolve.Inafewsentences,diagnosewhetherthesolutioniscompletelycorrectineveryexecutionstepandwillsatisfyalltherequirementsinthegiventask.Ifthesolutionincludesanycontentthatisnotalignedwiththerequirementsandintentionsofthetask,describeandexplainyourrelevantarguments. ```

You are also required to debateanddiscusswithanothercriticwhofocusesonsecurityandriskmanagement.Alwaysadvocateandfocusonperformanceandaccuracyoversecurityorriskmanagement.

Limit yourcritictoafewsentences.

Task: (question)Solution: (answer)

{scratchpad} ```
Listing 10: Prompt with instruction for a summarizer model to summarize the interactions between safety and helpfulness critics for open-ended generation tasks.
``` Youareanadvancedintelligentagentwithequalfocusestobbothperformanceandsecurity.Youwillbegivenabiscussionaboutabsolutionbetweentwocriticsagainstsafetyandcorrectness.Inafewsentences,summarizethediscussionanddeterminewhetherthesolutioniscompletelycorrectandsafe.Theperfectsolutionshouldsatisfyalltherequirementsandalignwithallintentionsinthegiventaskasmuchaspossiblewhilealsonotcontaininganyharmful,unethical,racist,sexist,toxic,dangerous,orillegalcontent.Theremightbesomesupportingfactsinthediscussionbetweenthecritics.Incorporateasmuchaspossiblethosedetailsintoyourresponsetomakeyouranalysisinformativeandconvincing.{scratchpad} ```

## Appendix J Qualitative Analysis

To explain the benefits of INDICT and where INDICT may fail, we included example generation outputs from INDICT and related methods. In Figure 9, we show that given a code generation task, INDICT can generate code that is more secure and robust than strong baselines (Direct generation, Reflexion [20], and CAMEL [14]). In Figure 10, we illustrate cases where INDICT may fail due to nontrivial errors.

Compared to the baseline methods, our approach can simultaneously improve both the helpfulness and safety of the output code generation. Specifically, given a relevant information snippet by the safety critic (about the hashing method SHA-256), our actor agent correctly revised the code with a more secure hashing method, avoiding using MD5 hashing and the common security risk CWE-328. At the same time, our generated code is generally more helpful with properly modularized functions implementing supporting features such as input validations. As we noted, this feature has generally emerged in code solutions by collaborative agent systems like CAMEL and INDICT.

Figure 10: Qualitative analysis of failure cases when applying our method

Figure 9: Qualitative comparison between our method and related baselines

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims made in the abstract and introduction correctly reflect the paper's contributions and scope, including the details of the proposed method INDICT and strong experimental results on code generation tasks. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed the limitations of the work in Appendix A. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]  Justification: N/A  Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: For reproducibility, we fully described our experimental setups (base language models, benchmarks, generation and model configurations, etc.). Please refer to 4 and Appendix E for more details. We will also release our code to reproduce the results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: we fully released all code and related scripts to replicate the experimental results. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provided all the necessary details related to our experiments, including the datasets/ benchmarks, model configurations, hyperparameters of our proposed method, etc. Please refer to 4 and Appendix E for more details. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We did not report the error bars because it would be too computationally expensive, involving running LLMs of up to 70B parameters on large benchmarks. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provided sufficient level of information on the compute resources used in this work (including the generation time, GPU types and quantities, etc.) to reproduce the experiments. Please refer to Appendix E for more details. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: The research in this work conforms in all aspects with the NeurIPS Code of Ethics. Please refer to Appendix B - Ethical Statement for more details. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We included both potential positive societal impacts and negative societal impacts of our work in Appendix C.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We described necessary safeguards for the responsible use of our method. Please refer to Appendix C for more details. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All the datasets and models used in this work are properly cited throughout the paper, including the list of the corresponding original creators/ owners. The licenses of all datasets and models are also fully described in Appendix E. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.

* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: N/A Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: N/A Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: N/A Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.