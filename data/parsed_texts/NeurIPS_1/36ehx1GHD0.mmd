# CLImage: Human-Annotated Datasets for Complementary-Label Learning

Hsiu-Hsuan Wang, Tan-Ha Mai, Nai-Xuan Ye, Wei-I Lin, Hsuan-Tien Lin

National Taiwan University

{b09902033, d10922024, b09902008, r10922076, htlin}@csie.ntu.edu.tw

###### Abstract

Complementary-label learning (CLL) is a weakly-supervised learning paradigm that aims to train a multi-class classifier using only complementary labels, which indicate classes to which an instance does not belong. Despite numerous algorithmic proposals for CLL, their practical applicability remains unverified for two reasons. Firstly, these algorithms often rely on assumptions about the generation of complementary labels, and it is not clear how far the assumptions are from reality. Secondly, their evaluation has been limited to synthetic datasets. To gain insights into the real-world performance of CLL algorithms, we developed a protocol to collect complementary labels from human annotators. Our efforts resulted in the creation of four datasets: CLCIFAR10, CLCIFAR20, CLMicroImageNet10, and CLMicroImageNet20, derived from well-known classification datasets CIFAR10, CIFAR100, and TinyImageNet200. These datasets represent the very first real-world CLL datasets. Through extensive benchmark experiments, we discovered a notable decrease in performance when transitioning from synthetic datasets to real-world datasets. We investigated the key factors contributing to the decrease with a thorough dataset-level ablation study. Our analyses highlight annotation noise as the most influential factor in the real-world datasets. In addition, we discover that the biased-nature of human-annotated complementary labels and the difficulty to validate with only complementary labels are two outstanding barriers to practical CLL. These findings suggest that the community focus more research efforts on developing CLL algorithms and validation schemes that are robust to noisy and biased complementary-label distributions.

## 1 Introduction

Ordinary multi-class classification methods rely heavily on high-quality labels to train effective classifiers. However, such labels can be expensive and time-consuming to collect in many real-world applications. To address this challenge, researchers have turned their attention towards weakly-supervised learning, which aims to learn from incomplete, inexact, or inaccurate data sources [20; 28]. This learning paradigm includes but is not limited to noisy-label learning [5], partial-label learning [2], positive-unlabeled learning [3], and complementary-label learning [8].

In this work, we focus on complementary-label learning (CLL). This learning problem involves training a multi-class classifier using only complementary labels, which indicate the classes that a data instance does not belong to. Although several algorithms have been proposed to learn from complementary labels, they were only benchmarked on synthetic datasets with some idealisticassumptions on complementary-label generation [1; 8; 9; 16; 21]. Thus, it remains unclear how well these algorithms perform in practical scenarios.

In particular, current CLL algorithms heavily rely on the _uniform assumption_ for generating complementary labels [8], which specifies that complementary labels are generated by uniformly sampling from the set of all possible complementary labels. To alleviate the restrictiveness of the uniform assumption, Yu et al. [27] considered a more general _class-conditional assumption_, where the distribution of the complementary labels only depends on its ordinary labels. These assumptions have been used in many subsequent works to generate the _synthetic complementary datasets_ for examining CLL algorithms [1; 9; 16; 21; 25]. Although these assumptions simplify the design and analysis of CLL algorithms, it remains unknown whether these assumptions hold true in practice and whether violation of these assumptions will significantly affect the performance of CLL algorithms. In addition to the uniform or class-conditional assumptions, most existing studies implicitly assumes that the complementary labels are noise-free. That is, they do not mistakenly represent the ordinary labels. While some studies claim to be more robust to noisy complementary labels [14], they were only tested on synthetic scenarios. It remains unclear how noisy the real-world datasets are, and how such noise affects the performance of current CLL algorithms.

To understand how much the real-world scenario differs from the assumptions, we started by collecting the datasets CLCIFAR10 and CLCIFAR20, which are derived from the famous CIFAR datasets for ordinary multi-class classification [12]. Since their release in 2023, the datasets [22] have been utilized by several emerging CLL studies [15; 23; 24; 26], demonstrating their instantaneous impact. We continue to extend the collection and form two additional human-annotated datasets, CLMicroImageNet10 and CLMicroImageNet20, which are derived from TinyImageNet200 [13; 19]. The extension verifies that our observations on CIFAR-derived datasets hold true for other image datasets. For all four datasets, we analyze the collected complementary labels, including their noise rates and non-uniform nature. Then, we perform benchmark experiments with diverse state-of-the-art CLL algorithms and conduct dataset-level ablation study on the assumptions of complementary-label generation using the collected datasets. Our studies reveal annotation noise as the most influential factor in the real-world datasets, and confirm that the non-uniform nature of human-annotated complementary labels cause certain CLL algorithms more susceptible to overfitting. These findings immediately suggest that the community focus more research efforts on developing CLL algorithms that are robust to noisy and non-uniform complementary-label distributions. In addition, we used the collected datasets to demonstrate that existing complementary-label-only validation schemes are not mature yet, suggesting the community a novel research direction for making CLL practical. Our contributions are summarized as follows:

* We designed a collection protocol of complementary labels (CLs) for images, and verified that the protocol collects reasonable human-annotated CLs across different datasets.
* We released **CLImage**, the collected set of four real-world CL datasets to support the continuous research of the community, publicly released at [https://github.com/ntucllab/CLImage_Dataset](https://github.com/ntucllab/CLImage_Dataset).
* We analyzed the collected datasets with extensive benchmarking experiments, which provides novel and valuable insights for the community.

## 2 Preliminaries on CLL

### Complementary-label learning

In ordinary multi-class classification, a dataset \(D=\left\{(\mathbf{x}_{i},y_{i})\right\}_{i=1}^{n}\) that is _i.i.d._ sampled from an unknown distribution is given to the learning algorithm. For each \(i\), \(\mathbf{x}_{i}\in\mathbb{R}^{M}\) represents the \(M\)-dimension feature of the \(i\)-th instance and \(y_{i}\in[K]=\left\{1,2,\ldots,K\right\}\) represents the class \(\mathbf{x}_{i}\) belongs to. The goal of the learning algorithm is to learn a classifier from \(D\) that can predict the labels of unseen instances correctly. The classifier is typically parameterized by a scoring function \(\mathbf{g}\colon\mathbb{R}^{M}\rightarrow\mathbb{R}^{K}\), and the prediction is made by \(\operatorname*{arg\,max}_{k\in[K]}\mathbf{g}(\mathbf{x})_{k}\) given an instance \(\mathbf{x}\), where denotes the \(k\)-th output of \(\mathbf{g}(\mathbf{x})\). In contrast to ordinary multi-class classification, CLL shares the same goal of learning a classifier but trains with different labels. In CLL, the ordinary label \(y_{i}\) is not accessible to the learning algorithm. Instead, a complementary label \(\bar{y}_{i}\) is provided, which is a class that the instance \(\mathbf{x}_{i}\) does _not_ belong to. The goal of CLL is to learn a classifier that is able to predict the correct labels of unseen instances from a complementary-label dataset \(\bar{D}=\{(\mathbf{x}_{i},\bar{y}_{i})\}_{i=1}^{n}\).

### Common assumptions on CLL

Researchers have made some additional assumptions on the generation process of complementary labels to facilitate the analysis and design of CLL algorithms. One common assumption is the _class-conditional assumption_[27]. It assumes that the distribution of a complementary label only depends on its ordinary label and is independent of the underlying example's feature, i.e., \(P(\bar{y}_{i}\mid\mathbf{x}_{i},y_{i})=P(\bar{y}_{i}\mid y_{i})\) for each \(i\). One special case of the class-conditional assumption is the _uniform assumption_, which further specifies that the complementary labels are generated uniformly. That is, \(P(\bar{y}_{i}=k|y_{i}=j)=\frac{1}{K-1}\) for all \(k\in[K]\backslash\{j\}\)[8; 9; 14].

For convenience, a \(K\times K\) matrix \(T\), called _transition matrix_, is often used to represent how the complementary labels are generated under the class-conditional assumption. \(T_{j,k}\) is defined to be the probability of obtaining a complementary label \(k\) if the underlying ordinary label is \(j\), i.e., \(T_{j,k}=P(\bar{y}=k\mid y=j)\) for each \(j,k\in[K]\). The diagonals of \(T\) hold the conditional probabilities that a complementary label mistakenly represents the ordinary label. That is, they indicate the noise level of the complementary labels. When \(T\) contains all zeros on its diagonals, the CLL scenario is called _noiseless_. For instance, the uniform and noiseless assumption can be represented by \(T_{j,j}=0\) for each \(j\in[K]\) and \(T_{j,k}=\frac{1}{K-1}\) for each \(k\neq j\). Class-conditional CLL scenarios based on any other non-uniform \(T\) are often called _biased_.

### A brief overview of CLL algorithms

The pioneering work by Ishida et al. [8] studied how to learn from complementary labels under the _uniform assumption_ by converting the risk estimator in ordinary multi-class classification to an unbiased risk estimator (**URE**) in CLL [8]. **URE** is then found to be prone to overfitting because of negative empirical risks, and is upgraded with two tricks, non-negative risk estimator (**URE-NN**) and gradient accent (**URE-GA**) [9]. The _surrogate complementary loss_ (**SCL**) algorithm mitigates the overfitting issue of **URE** by a different loss design that decreases the variance of the empirical estimation. However, these algorithms either rely on the uniform assumption in design or are only tested on the synthetic datasets that obeys the uniform assumption.

To make CLL one step closer to practice, researchers have explored algorithms to go beyond the uniform (and thus noiseless) assumption. Yu et al. [27] utilized the forward-correction loss (**FWD**) to accommodate biased complementary label generation by adapting techniques from noisy label learning [18] to change the loss. Additionally, Gao and Zhang [6] proposed the **L-W** algorithm based on discriminatively modeling the distribution of complementary labels through a weighting function, further improving the performance in bias scenario. Furthermore, Ishiguro et al. [10] designed robust loss functions for learning from noisy CLs, including **MAE** and **WMAE**, by applying the gradient ascent technique [9] to handle noisy scenarios.

Besides CLL algorithms, a crucial component for making CLL practical is model validation. In ordinary-label learning, this can be done by naively calculating the classification accuracy on a validation dataset. In CLL, this scheme can be intractable if there are not enough ordinary labels. One generic way of model validation is based on the result of Ishida et al. [9] by calculating the unbiased risk estimator of the zero-one loss, i.e.,

\[\hat{R}_{01}(\mathbf{g})=\frac{1}{N}\sum_{i=1}^{N}\mathrm{e}_{\bar{y}_{i}}^{ \top}(T^{-1})\ell_{01}(\mathbf{g}(x_{i})) \tag{1}\]

where \(\mathrm{e}_{\bar{y}_{i}}\) denotes the one-hot vector of \(\bar{y}_{i}\), \(\ell_{01}(\mathbf{g}(x_{i}))\) denotes the \(K\)-dimensional vector \(\left(\ell_{01}(\mathbf{g}(x_{i}),1),\ldots,\ell_{01}(\mathbf{g}(x_{i})),K) \right)^{T}\), and \(\ell_{01}(\mathbf{g}(x_{i}),k)=0\) if \(\arg\max_{k\in[K]}\mathbf{g}(x_{i})=k\) and \(1\) otherwise, representing the zero-one loss of \(\mathbf{g}(x_{i})\) if the ordinary label is \(k\). This estimator will be used in the experiments in Section 6. Another validation objective, surrogate complementary esimation loss (SCEL), was proposed by Lin and Lin [14]. SCEL measures the log loss of the complementary probability estimates induced by the probability estimates on the ordinary label space. The formula to calculate SCEL is as follows,

\[\hat{R}_{\text{SCEL}}(\mathbf{g})=\frac{1}{N}\sum_{i=1}^{N}-\log\left(e_{\overline {y}_{i}}^{\top}T^{\top}\operatorname{softmax}(\mathbf{g}(x_{i}))\right). \tag{2}\]

## 3 Construction of the CLImage collection

In this section, we introduce the four complementary-labeled datasets that we collected, CLCIFAR10, CLCIFAR20, CLMicroImageNet10 and CLMicroImageNet20. All datasets are labeled by human annotators on Amazon Mechanical Turk (MTurk)1.

Footnote 1: [https://www.mturk.com/](https://www.mturk.com/)

### Datasets and goals

The complementary-labeled datasets are derived from ordinary multi-class classification datasets. CIFAR10, CIFAR100 and TinyImageNet200 [12, 13, 19]. This selection is motivated by the real-world noisy label dataset by Wei et al. [25]. Building upon the CIFAR and TinyImageNet200 datasets allow us to estimate the noise rate and the empirical transition matrix easily, as they already contain nearly noise-free ordinary labels. In addition, many of the state-of-the-art CLL algorithms have been benchmarked on synthetic complementary labels with the CIFAR datasets [4, 11, 17]. Our CLCIFAR counterparts immediately allow a fair comparison to those results with the same network architecture.

In addition to our CLCIFAR extensions, we are the first to introduce (Tiny)ImageNet-derived datasets to the CLL literature. Such datasets serve two purposes. First, it allows us to confirm the validity of our collection protocol and findings beyond CIFAR-derived datasets. Second, ImageNet knowingly contains images of higher complexity than CIFAR and can thus be used to challenge the ability of existing CLL algorithms more realistically.

There is a historical note that is worth sharing with the community: We initially attempted to collect complementary labels based on the \(100\) classes in CIFAR100. But some preliminary testing soon revealed that state-of-the-art CLL algorithms cannot produce meaningful classifiers for \(100\) classes even on synthetic complementary labels that are uniformly and noiselessly generated. We thus set our collection goals to be \(10\)-class classification, which is the focus of most current CLL studies, and \(20\)-class classification, which extends the horizon of CLL and matches the \(20\) super-class structure in CIFAR.

### Complementary label collection protocol

To collect only complementary labels from the CIFAR, TinyImageNet datasets, for each image in the training split, we first randomly sample four distinct labels and ask the human annotators to select any of the _incorrect_ one from them. To leave room for analyzing the annotators' behavior, each image is labeled by three different annotators. The four labels are re-sampled for each annotator on each image. That is, each annotator possibly receives a different set of four labels to choose from. An algorithmic description of the protocol is as follows. For each image \(\mathbf{x}\),

1. Uniformly sample four labels without replacement from the label set \([K]\).
2. Ask the annotator to select any one of the complementary label \(\bar{y}\) from the four sampled labels.
3. Add the pair \((\mathbf{x},\bar{y})\) to the complementary dataset.

Note that if the annotators always select one of the correct complementary labels uniformly, the empirical transition matrix will also be uniform in expectation. We will inspect the empirical transition matrix in Section 4. The labeling tasks are deployed on MTurk by dividing them into smaller we first divide the total images into smaller human intelligence tasks (HITs). For instance, for constructing the CLCIFAR datasets, we first divide the 50,000 images into five batches of 10,000 images. Then, each batch is further divided into 1,000 HITs with each HIT containing 10 images. Each HIT is deployed to three annotators, who receive 0.03 dollar as the reward by annotating 10 images. To make the labeling task easier and increase clarity, the size of the images are enlarged to \(200\times 200\) pixels.

## 4 Result analysis

Next, we closely examine the collected complementary labels. We first analyze the error rates of the collected labels, and then verify whether the transition matrix is uniform or not. Finally, we end with an analysis on the behavior of the human annotators observed in the label collection protocol.

**Observation 1: noise rate compared to ordinary label collection** We first look at the noise rate of the collected complementary labels. A complementary label is considered to be incorrect if it is actually the ordinary label. The mean error rate made by the human annotators is 3.93% for CLCIFAR10, 2.80% for CLCIFAR20, 5.19% for CLMicroImageNet10 and 3.21% for CLMicroImageNet20. In theory, we can estimate a random annotator achieves a noise rate of \(\frac{1}{K}\) for complementary label annotation and a noise rate of \(\frac{K-1}{K}\) for ordinary label annotation. If we compare the human annotators to a random annotator, then for CLCIFAR10, human annotators have \(60.7\)% less noisy labels than the random annotator whereas for CIFAR10-N, human annotators have \(80\)% less noisy labels. This demonstrates that human annotators are more competent compared to a random annotator in the ordinary-label annotation. Similarly, human annotators have \(44\)% less noise than a random annotator for CLCIFAR20 and \(73.05\)% less noise for CIFAR100N-coarse. This observation reveals that while the absolute noise rate is lower in annotating complementary labels, it may be more difficult to be competent against random labels than the ordinary label annotation.

**Observation 2: imbalanced complementary label annotation** Next, we analyze the distribution of the collected complementary labels. The frequency of the complementary labels for the CLCIFAR10 and CLMicroImageNet10 (CLMIN10) datasets are reported in Figure 1. As we can see in the figure, the annotators exhibit specific biases towards certain labels. For instance, in CLCIFAR10, annotators prefer "airplane" and "automobile," while in CLMIN10, they prefer "pizza" and "torch". In CLCIFAR10, the bias is towards labels in different categories, as vehicles ("airplane," "automobile") versus animals ("cat", "bird"). In contrast, in CLMIN10, the bias is towards items that are easily recognizable ("pizza" and "torch") and against those that are less familiar ("cardigan" or "alp").

**Observation 3: biased transition matrix** Finally, we visualize the empirical transition matrix using the collected CLs in Figure 2. Based on the first two observations, we could imagine that the transition matrix is biased. By inspecting Figure 2, we further discover that the bias in the complementary labels are dependent on the true labels. For instance, in CLCIFAR10, despite we see more annotations on airplane and automobile in aggregate, conditioning on the transportation-related labels ("airplane",

Figure 1: The label distribution of CLCIFAR10 and CLMicroImageNet10 datasets.

"automobile", etc), the distribution of the complementary labels becomes more biased towards other animal-related labels ("bird", "cat", etc.) Furthermore, this observation holds true on CLMIN10 as well. Next, we study the impact of the bias and noise on existing CLL algorithms.

We discovered similar patterns in all four human-annotated datasets, validating that our design methodology is practical for collecting real-world CLL image datasets. Due to space limitations, we have included the detailed analysis of CLCIFAR20 and CLMicroImageNet20 in Appendix B.4.

## 5 Experiments

In this section, we benchmarked several state-of-the-art CLL algorithms on CLImage. A significant performance gap between the models trained on the humanly annotated CLCIFAR, CLMicroImageNet dataset and those trained on the synthetically generated complementary labels (CL) was observed in Section 5.1, which motivates us to analyze the possible reasons for the gap with the following experiments. To do so, we discuss the effect of three factors in the label generating process, feature dependency, noise, and biasedness, in Section 5.2, Section 5.3, and Section 5.4, respectively. From our experiment results, we conclude that noise is the dominant factor affecting the performance of the CLL algorithms on CLCIFAR2.

Footnote 2: Due to space and time constraints, we only provide the results and discussion on the CLCIFAR datasets.

### Standard benchmark on CLImage

**Baseline methods** Several state-of-the-art CLL algorithms were selected for this benchmark. Some of them take the transition matrix \(T\) as inputs, which we call \(T\)**-informed** methods, including two version of forward correction [27]: **FWD-U** and **FWD-R**, two version of unbiased risk estimator with gradient ascent [9]: **URE-GA-U** and **URE-GA-R**, and robust loss [10] for learning from noisy CL: **CCE**, **MAE**, **WMAE**, **GCE**, and **SL3**. We also included some algorithms that assume the transition matrix \(T\) to be uniform, called \(T\)**-agnostic** methods, including surrogate complementary loss **SCL-NL** and **SCL-EXP**[1], discriminative modeling **L-W** and its weighted variant (**L-UW**) [6], and pairwise-comparison (**PC**) with the sigmoid loss [8]. The details of the algorithms mentioned above are discussed in Appendix D.

Footnote 3: Due to space limitations, we only provided the results of MAE. The remaining results and discussions related to the robust loss methods can be found in Appendix B.3

**Implementation details** We collected and released three CLs per image to prepare for future studies. However, for this standard benchmark, we chose the first CL from the collected labels for each data instances to form a single CLL dataset, ensuring reproducibility. Then, we trained a ResNet18 [7] model using the baseline methods mentioned above on the single CLL dataset using

Figure 2: The empirical transition matrices of CLCIFAR10 and CLMicroImageNet10.

[MISSING_PAGE_FAIL:7]

complementary datasets, CLCIFAR10-_iid_ and CLCIFAR20-_iid_ by i.i.d. sampling CLs from the empirical transition matrix in CLCIFAR10 and CLCIFAR20, respectively. We proceeded to benchmark the CLL algorithms on CLCIFAR-_iid_ and presented the accuracy difference compared to CLCIFAR in Table 2.

**Results and discussion** From Table 2, we observed that the accuracy barely changes on the resampled CLCIFAR-_iid_, suggesting that even if the complementary labels in CLCIFAR could be feature-dependent, this dependency does not affect the model performance significantly. Hence, there might be other factors contributing to the performance gap.

### Labeling noise

In this experiment, we further investigated the impact of the label noise on the performance gap. Specifically, we measured the accuracy on the noise-removed versions of CLCIFAR datasets, where varying percentages (0%, 25%, 50%, 75%, or 100%) of noisy labels are eliminated.

**Results and discussion** We present the performance of FWD trained on the noise-removed CLCIFAR10 dataset in the left figure in Figure 3. The results for other algorithms and the noise-removed CLCIFAR20 dataset can be found in Appendix E. From the figure, we observe a strong positive correlation between the performance and the proportion of removed noisy labels. When more noisy labels are removed, the performance gap diminishes and the accuracy approaches that of the ideal uniform-CLFAR dataset. Therefore, we conclude that the performance gap between the humanly annotated CLs and the synthetically generated CLs are primarily attributed to the label noise.

### Biasedness of complementary labels

To further study the biasedness of CL as a potential factor contributing to the performance gap, we removed the biasedness from the noise-removed CLCIFAR dataset and examined the resulting accuracy. Specifically, we introduced the same level of uniform noise in uniform-CIFAR dataset and reevaluated the performance of FWD algorithms.

**Results and discussion** The striking similarity between the two curves in the right figure in Figure 3 shows that the accuracy is significantly influenced by label noise, while the biasedness of CL has a negligible impact on the results. Furthermore, we observe that the accuracy difference between the results of the last epoch and the best accuracy of validation set (or early-stopping: **ES**) results becomes smaller when the model is trained on the uniformly generated CLs. That is, the \(T\)-informed methods are more prone to overfitting when there is a bias in the CL generation.

With the experiment results in Section 5.2, 5.3, and 5.4, we can conclude that the performance gap between humanly annotated CL and synthetically generated CL is primarily attributed to label noise. Additionally, the biasedness of CLs may potentially contribute to overfitting, while the feature-dependent CLs do not detrimentally affect performance empirically. It is worth noting that in the last row of Table 1, the MAE methods that can learn from noisy CL fails to generalize well in the practical dataset. These results suggest that more research on learning with noisy complementary labels can potentially make CLL more realistic.

## 6 Validation Objectives

Validation is a crucial component in applying CLL algorithms in practice. With the collection of the real-world datasets, we are now able to estimate the difference between using ordinary labels for validation (the common practice in existing CLL studies, as what we do in Section 5) and using complementary labels for validation.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline  & FWD-U & FWD-R & URE-GA-U & URE-GA-R & SCL-NL & SCL-EXP & L-W & L-UW & PC-sigmoid \\ \hline _CLCIFAR10-iid_ & -1.1\(\pm\)1.27 & -0.36\(\pm\)1.15 & -3.03\(\pm\)1.25 & 0.74\(\pm\)0.35 & -0.67\(\pm\)1.81 & -1.97\(\pm\)1.16 & -2.5\(\pm\)0.56 & -3.53\(\pm\)1.36 & -2.03\(\pm\)2.05 \\ _CLCIFAR20-iid_ & -0.64\(\pm\)0.39 & -3.53\(\pm\)1.13 & -0.37\(\pm\)0.51 & 1.79\(\pm\)2.34 & -0.28\(\pm\)0.61 & -0.39\(\pm\)0.69 & -0.5\(\pm\)1.37 & -0.82\(\pm\)0.04 & -2.24\(\pm\)0.52 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Mean accuracy difference (\(\pm\) standard deviation) of different CLL algorithms. A plus indicates the performance on is calculated as CLCIFAR-_i.i.d._ accuracy minus CLCIFAR accuracy.

[MISSING_PAGE_FAIL:9]

## References

* [1] Y.-T. Chou, G. Niu, H.-T. Lin, and M. Sugiyama. Unbiased risk estimators can mislead: A case study of learning with complementary labels, 2020.
* [2] T. Cour, B. Sapp, and B. Taskar. Learning from partial labels. _The Journal of Machine Learning Research_, 12:1501-1536, 2011.
* [3] F. Denis. Pac learning from positive statistical queries. In _Algorithmic Learning Theory: 9th International Conference, ALT'98 Otzenhausen, Germany, October 8-10, 1998 Proceedings 9_, pages 112-126. Springer, 1998.
* [4] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [5] B. Frenay and M. Verleysen. Classification in the presence of label noise: a survey. _IEEE transactions on neural networks and learning systems_, 25(5):845-869, 2013.
* [6] Y. Gao and M.-L. Zhang. Discriminative complementary-label learning with weighted loss. In _International Conference on Machine Learning_, pages 3587-3597. PMLR, 2021.
* [7] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2016.
* [8] T. Ishida, G. Niu, W. Hu, and M. Sugiyama. Learning from complementary labels. _Advances in neural information processing systems_, 30, 2017.
* [9] T. Ishida, G. Niu, A. K. Menon, and M. Sugiyama. Complementary-label learning for arbitrary losses and models, 2019.
* [10] H. Ishiguro, T. Ishida, and M. Sugiyama. Learning from noisy complementary labels with robust loss functions. _IEICE TRANSACTIONS on Information and Systems_, 105(2):364-376, 2022.
* [11] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby. Big transfer (bit): General visual representation learning. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V 16_, pages 491-507. Springer, 2020.
* [12] A. Krizhevsky. Learning multiple layers of features from tiny images. _University of Toronto_, 05 2012.
* [13] Y. Le and X. S. Yang. Tiny imagenet visual recognition challenge. 2015.
* [14] W.-I. Lin and H.-T. Lin. Reduction from complementary-label learning to probability estimates. In _Proceedings of the Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)_, May 2023.
* [15] W.-Y. Lin. Reduction from complementary-label learning to probability estimates. Master's thesis, 2023.
* [16] S. Liu, Y. Cao, Q. Zhang, L. Feng, and B. An. Consistent complementary-label learning via order-preserving losses. In _International Conference on Artificial Intelligence and Statistics_, pages 8734-8748. PMLR, 2023.
* [17] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.
* [18] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: a loss correction approach. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2017.
* [19] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recognition challenge. _Int. J. Comput. Vision_, 115(3), 2015.

* [20] M. Sugiyama, H. Bao, T. Ishida, N. Lu, T. Sakai, and G. Niu. _Machine learning from weak supervision: An empirical risk minimization approach_. MIT Press, 2022.
* [21] D.-B. Wang, L. Feng, and M.-L. Zhang. Learning from complementary labels via partial-output consistency regularization. In _IJCAI_, pages 3075-3081, 2021.
* [22] H.-H. Wang, W.-I. Lin, and H.-T. Lin. Clcifar: Cifar-derived benchmark datasets with human annotated complementary labels, 2023.
* [23] W. Wang, T. Ishida, Y.-J. Zhang, G. Niu, and M. Sugiyama. Learning with complementary labels revisited: The selected-completely-at-random setting is more practical.
* [24] W. Wang, T. Ishida, Y.-J. Zhang, G. Niu, and M. Sugiyama. Learning with complementary labels revisited: A consistent approach via negative-unlabeled learning. _arXiv preprint arXiv:2311.15502_, 2023.
* [25] J. Wei, Z. Zhu, H. Cheng, T. Liu, G. Niu, and Y. Liu. Learning with noisy labels revisited: A study using real-world human annotations, 2022.
* [26] Y. You, J. Huang, B. Wang, and Q. Tong. Rethinking one-vs-the-rest loss for instance-dependent complementary label learning.
* [27] X. Yu, T. Liu, M. Gong, and D. Tao. Learning with biased complementary labels, 2018.
* [28] Z.-H. Zhou. A brief introduction to weakly supervised learning. _National science review_, 5(1): 44-53, 2018.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? We describe some potential direction to make CLL algorithms more practical in the conclusion section. 3. Did you discuss any potential negative societal impacts of your work? [Yes] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them?
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? Please see the link in Section 1 or Appendix J. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen? Please see the "Implementation Details" paragraph in Section 5.1 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? The standard deviation of four trials is indicated after \(\pm\) sign in Table 1 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? We mentioned them in Section 5.1.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? Please refer to the footnote 2. Did you mention the license of the assets? The authors of the CIFAR, TinyImageNet datasets require the paper using them cite their paper, which we did. 3. Did you include any new assets either in the supplemental material or as a URL? The reference is at 1 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? We provide the screenshots of the mTurk interface at our github repo. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? We describe how we paid the annonators in Section 3.2.