# SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality

 Cheng-Yu Hsieh\({}^{1}\), Jieyu Zhang\({}^{1}\), Zixian Ma\({}^{1}\), Aniruddha Kembhavi\({}^{2}\), Ranjay Krishna\({}^{1,2}\)

\({}^{1}\)University of Washington, \({}^{2}\)Allen Institute for Artificial Intelligence

{cydhsieh,jieyu22,zixianma,ranjay}@cs.washington.edu, anik@allenai.org

The authors contribute equally to this work.

###### Abstract

In the last year alone, a surge of new benchmarks to measure _compositional_ understanding of vision-language models have permeated the machine learning ecosystem. Given an image, these benchmarks probe a model's ability to identify its associated caption amongst a set of compositional distractors. Surprisingly, we find significant biases in _all_ these benchmarks rendering them hackable. This hackability is so dire that blind models with no access to the image outperform state-of-the-art vision-language models. To remedy this rampant vulnerability, we introduce SugarCrepe, a new benchmark for vision-language compositionality evaluation. We employ large language models, instead of rule-based templates used in previous benchmarks, to generate fluent and sensical hard negatives, and utilize an adversarial refinement mechanism to maximally reduce biases. We re-evaluate state-of-the-art models and recently proposed compositionality inducing strategies, and find that their improvements were hugely overestimated, suggesting that more innovation is needed in this important direction. We release SugarCrepe and the code for evaluation at: [https://github.com/RAIVNLab/sugar-crepe](https://github.com/RAIVNLab/sugar-crepe).

## 1 Introduction

Scholars today herald _compositionality_ as a fundamental presupposition characterizing both human perception and linguistic processing [10]. Through compositional reasoning, humans can comprehend new scenes and describe those scenes by composing known atoms [19, 17, 3, 9]. For instance, compositionality allows people to differentiate between a photo of "a girl in white facing a man in black" and "a girl in black facing a man in white". For a while now, vision-language research has sought to develop models that can similarly comprehend scenes and express them through compositional language [22, 20, 29, 15].

Given its importance, a surge of new benchmarks have been proposed to evaluate whether vision-language models exhibit compositionality. Recently, Winoground [45], VL-CheckList [53], ARO [49], CREPE [30], and Cola [38] have entered the machine learning zeitgeist. Evaluation is mostly done through an image-to-text retrieval task formulation [53, 49, 30]: by measuring how often models pick the description, "a girl in white facing a man in black" when presented with an image of it, and avoid choosing the incorrect _hard negative_ description, "a girl in black facing a man in white".

In this work, we uncover a crucial vulnerability in not just one but all these image-to-text compositionality benchmarks: We find that a _blind_ model that never looks at the image, can identify the correct caption and avoid choosing the supposed "hard negatives". This blind model outperforms a wide array of pretrained vision-language models across the suite of benchmarks [36, 18, 14]. We explain this undesired hackability in existing benchmarks by showcasing that there exists a significant distributional gap between the positive and hard negative captions. For instance, in the ARO benchmark [49], human-generated positive captions differ drastically from the hard negative texts generated by randomly shuffling words in the positive captions. As new research has begun to propose methods that claim to improve compositionality on these benchmarks [49; 38], we find it critical to highlight our findings and propose a solution.

We propose a solution to existing hackable benchmarks by introducing SugarCrepe, a new benchmark to faithfully evaluate compositionality. In curating SugarCrepe, we identify two main _biases_2 that result in the distributional gap between positive and hard negatives; and employ mechanisms to fix the shifts. In particular, we find the current procedure in generating hard negatives introduces descriptions that are (1) not plausible and (2) non-fluent. For example, while the caption "olives and grapes on a plate" is a sensical fluent caption, benchmarks often have non-plausible hard negatives like "olives and grapes inside a plate" or simply incomprehensible ones like "right has word another word. There is a words" (see Table 1 for more examples). We mitigate such biases by first leveraging a modern large language model, ChatGPT [32], to generate plausible and natural hard negative texts instead of relying on simple rule-based templates employed by existing benchmarks [30; 49]. Then, we subsample the dataset through an adversarial refinement process to ensure the identified biases are maximally removed by drawing on recent dataset debiasing work [50; 41; 23]. Taken together, this workflow is where SugarCrepe derived its name: **S**ynthetic yet **U**nbiased **G**eneration with **A**dversarially **R**efined **C**ompositional **RE**resentation **E**valuation. We qualitatively and quantitatively verify through both human and automatic evaluations that SugarCrepe effectively fixes these biases.

Footnote 2: We use biases and artifacts interchangeably in the paper.

With SugarCrepe, we _re_-evaluate recent methods proposed to improve compositionality. Specifically, we focus on one prominent approach that aims to improve compositionality through data augmentation. This method trains models by generating compositional hard negatives and injecting them within a training batch [13; 49]. Unfortunately, we observe that the effectiveness of this simple data augmentation approach is hugely _overestimated_ when evaluated on existing benchmarks, leading to limited improvements on SugarCrepe. Finally, we evaluate a wide variety of \(17\) pretrained CLIP models [36; 18; 14], and find that current models still lack compositionality. Our results suggest that to improve compositionality, future work may need more innovative techniques.

## 2 Related Work

We situate our paper amongst existing work on vision-language compositionality, and debiasing datasets for model evaluation.

**Evaluating vision-language compositionality.** Recent works have introduced benchmarks to evaluate the compositionality of vision-language models [36]; they find that current models exhibit little compositional understanding [49; 45; 53; 30; 38] despite their remarkable performance on downstream tasks [36; 25; 43; 1; 47; 48; 52]. Models have a hard time discerning between text containing the same words ordered differently [45]. Models also fail to link objects to their attributes, or understand the relationship between objects [53; 49; 38]. Our work finds that many of the benchmarks used to evaluate compositionality have hackable biases; blind models that do not even look at the image outperform state-of-the-art vision-language models.

**Improving vision-language compositionality.** To enhance vision-language models' compositionality, new proposals suggest training strategies that utilize additional data, models, and/or losses [49; 5; 38; 13; 44]. Amongst them, one prominent approach is to explicitly train the models to distinguish hard negatives from the correct captions [49; 13]. While these approaches appear to improve compositionality on benchmarks, it is unclear if these models achieve such improvements by actually acquiring compositional understanding or by exploiting biases in these datasets. We answer this question in our evaluation.

**Debiasing dataset for faithful model evaluation.** Several prior manuscripts have pointed out that biased datasets could lead to an overestimation of models' true capabilities [16]. They have proposed dataset de-biasing methods to enable more faithful model evaluations [39; 50; 41; 23; 34]. For instance, adversarial filtering [50] iteratively trains an ensemble of classifiers on different training splits and uses them to filter out "easy" negatives for each instance. Building upon adversarial filtering, AFLite [41; 23] filters data instances in a more light-weight manner without retraining a model at each iteration and leads to benchmarks that more accurately represent the underlying tasks. We use adversarial refinement to remove biases that creep into the generation of compositionality benchmarks.

## 3 Limit and biases of current compositionality benchmarks

A majority of existing compositionality benchmarks for vision-language models formulate the evaluation task as image-to-text retrieval [53; 49; 30]. We focus on these benchmarks and discuss others [45; 38] in Appendix B. Given an image, the model is probed to select text that correctly describes the image from a pool of candidates. Unlike standard retrieval tasks where the negative (incorrect) candidates differ a lot from the _positive_ (correct) text, compositionality benchmarks intentionally design _hard negative_ texts that differ minimally from the positive text, in order to test whether the model understands the fine-grained atomic concepts that compose the scene.

**Existing hard negative generation process introduces undesirable biases.** Existing benchmarks generate hard negative texts through rule-based programmatic procedures [53; 49; 30], which produce hard negatives by replacing a word of specific type (an object, attribute, or relation) in the original text, by swapping two words, or by shuffling the word order. We find that such procedures introduce unintentional biases in the generated hard negatives (see Table 1); specifically, we observe two major types of undesirable artifacts: (1) _nonsensical_ artifacts, and (2) _non-fluent_ artifacts. In order to quantitatively measure these biases, we utilize Vera [27], a plausibility estimation model, to characterize the nonsensical bias. Specifically, we define \(\operatorname{Vera}(T)\) to be the plausibility score of a caption \(T\), where a higher score suggests more sensical the caption is. Similarly, to capture the non-fluent bias, we leverage a grammar-check model [31] that assigns high scores, \(\operatorname{Grammar}(T)\), to more grammatically correct texts. In Figure 1, we find that Vera and the grammar model assign higher scores to positive texts, suggesting that many hard negatives are nonsensical and not fluent.

**Dataset biases render current compositionality benchmarks ineffective.** Given the heavily-skewed score gaps, we show that blind models (_i.e._, Vera and the grammar model) that simply select the higher-scoring texts as positives and admittedly do not possess any vision-language compositionality, can achieve state-of-the-art performances on existing benchmarks. We compare the the blind models against \(17\) pretrained CLIP models from three sources: OpenAI's in-house WebImageText dataset [36], LAION [42], and Datacomp [14]. We plot the performances of the blind models and the best-performing CLIP models from each category (Figure 2). Blind models achieves state-of-the-art performances on \(9\) out of \(10\) existing benchmark tasks. We provide full evaluation results in Appendix D.1.

## 4 SugarCrepe

We introduce SugarCrepe, a new benchmark for faithful evaluation of vision-language models' compositionality based on the image-text pairs of COCO [26]. SugarCrepe presents two key contributions over existing benchmarks: (1) it drastically reduces the two identified dataset biases (Sec. 4.1), and (2) it covers a broad range of fine-grained types of hard negatives (Sec. 4.2). We present a summary comparison on compositionality benchmarks in Appendix B.

\begin{table}
\begin{tabular}{l l l} \hline \hline Dataset & Nonsensical Hard Negatives & Non-fluent Hard Negatives \\ \hline CREPE [30] & Olives and grape inside a plate. & A door with panes not in a room; the door has windows. \\  & Ground in a bucket on the flowers. & Right has word another word. There is a words. \\  & A h hair wearing a necklace, with her lady on a table. & A shelf with books in something. There is no background. \\ \hline ARO [49] & The grass is eating the horse. & At brown cat a in looking a gray dog sitting is and white bathtub. \\  & A gray bathtub is looking at a white cat. & Scene with remarkable all blue a green behind chair. \\  & Green ball with a remarkable chair behind a blue scene. & Books the looking at people are. \\ \hline VL-CheckList [53] & Sheep is hardwood. & An man fishing a food from a wrapper using a paw at a open. \\  & Empty arehex. & It having at a city. \\  & The bush speaking in the garden. & An grouping subdiang at a room access. \\ \hline \hline \end{tabular}
\end{table}
Table 1: Existing compositionality benchmarks rely on procedurally-generated hard negatives which often do not make logical sense or are not fluent due to grammatical errors.

### SugarCrepe generation workflow alleviates dataset biases

The generation procedure of SugarCrepe consists of three main stages, centered around creating sensical and fluent hard negatives that close the distributional gaps to the positive texts, and ensuring a balanced distribution on the score gaps to make the final dataset robust to the identified biases.

**Stage 1: Generate sensical and fluent hard negatives with a large language model.** Observing the capability of modern large language models in generating fluent and plausible texts, we leverage ChatGPT [32] to generate hard negative texts where we explicitly instruct it to avoid commonsense (logical) and fluency (grammatical) errors. To guide ChatGPT in re-writing a given positive text into its hard negative counterparts, we provide few-shot demonstrations written by the authors and leverage its in-context learning ability [4] to generalize to unseen texts. Figure 3 shows an example demonstration used and an actual hard negative generated. We detail all the prompt templates in Appendix C.2. Table 3 shows the comparisons between hard negatives generated from ChatGPT in SugarCrepe and that from existing benchmarks.

**Stage 2: Filter false negatives with human validation.** A generated text is considered a valid hard negative only if it incorrectly describes the corresponding image. For example, given an image with a positive caption "a man and a child sitting on a sofa", a compositional change that replaces "child" with "girl" may still result in a correct caption. To ensure the validity of the hard negatives in SugarCrepe, we filter out _false_ negatives by manually examining the generated hard negatives and their corresponding images.

**Stage 3: De-bias dataset with adversarial refinement.** While ChatGPT yields more sensical and fluent text, there is no guarantee that the bias between positive and negative texts is negligible. Following dataset de-biasing work [50; 41; 23], we develop an adversarial refinement mechanism that maximally reduces the undesirably exploitable artifacts in SugarCrepe. Specifically, our goal is to ensure that performance improvements on SugarCrepe cannot be achieved by exploiting the

Figure 1: Top row: We define _Vera score gap_ as the score difference between the positive and hard negative texts: \(\mathrm{Vera}(T^{\mathrm{p}})-\mathrm{Vera}(T^{n})\). The entire Vera score gap distribution lies on the positive spectrum, indicating that the template-generated hard negative texts usually have low plausibility. Bottom row: Similarly, _Grammar score gap_ is defined by: \(\mathrm{Grammar}(T^{\mathrm{p}})-\mathrm{Grammar}(T^{n})\). On grammar score, we also find that the distribution largely rests on the positive side, suggesting that most hard negative texts in existing benchmarks exhibit grammatical errors.

Figure 2: Blind commonsense Vera model and Grammar model outperform state-of-the-art CLIP models on nearly _all_ existing benchmarks by exploiting the nonsensical and non-fluent artifacts. This suggests that existing benchmarks are hackable and ineffective in measuring compositionality.

identified nonsensical and non-fluent biases. To accomplish this, we characterize the biases again with the commonsense and grammar models [27, 31], and subsample the dataset to ensure symmetric score gap distributions on both the positive and negative sides, as shown in Figure 4. We note the symmetry around zero implies that the commonsense and grammar scores can no longer be used to infer the ground truth positive texts. We provide the adversarial refinement algorithm in Algorithm 1.

```
0: Text-only model \(M_{1}\) and \(M_{2}\); Number of grids \(K\); A set of candidates \(\mathcal{D}=\left\{I_{i},T_{i}^{p},T_{i}^{n}\right\}_{i\in[N]}\), where \(I_{i}\), \(T_{i}^{p}\), and \(T_{i}^{n}\) are \(i\)-th image, positive caption, and negative caption.
0: A subset \(\bar{\mathcal{D}}\subset\mathcal{D}\)
1: Calculate the model score gap for each candidate \(g_{i}^{(1)}=M_{1}(T_{i}^{p})-M_{1}(T_{i}^{n})\) and \(g_{i}^{(2)}=M_{2}(T_{i}^{n})-M_{2}(T_{i}^{n})\)
2: Split the 2D space \([-1,1]\times[-1,1]\) to \(K\times K\) equal-size grids.
3: Place each candidate to a grid based on the score gaps \(g_{i}^{(1)}\) and \(g_{i}^{(2)}\).
4: Initialize \(\bar{\mathcal{D}}=\{\}\)
5:for each pair of grid \((G_{j},G_{j}^{*})\) symmetric about the original point \((0,0)\)do
6:if\(|G_{j}|>|G_{j}^{*}|\)then
7: Sample \(|G_{j}^{*}|\) candidates from \(G_{j}\) and put them to \(\bar{\mathcal{D}}\).
8: Put candidates in \(G_{j}^{*}\) to \(\bar{\mathcal{D}}\).
9:else
10: Sample \(|G_{j}|\) candidates from \(G_{j}^{*}\) and put them to \(\bar{\mathcal{D}}\).
11: Put candidates in \(G_{j}\) to \(\bar{\mathcal{D}}\).
```

**Algorithm 1** Adversarial Refinement

### SugarCrepe covers a broad range of hard negative types

To test different aspects of vision-language models' compositional understanding, we follow CREPE [30] to consider various _forms_ of hard negatives, and follow VL-CheckList [53] and ARO [49] to consider different fine-grained _categories_ of the atomic concepts. In total, SugarCrepe covers \(7\) fine-grained types of hard negatives, as shown in Table 2. We introduce the dataset taxonomy below, starting from the _form_ of the hard negatives to its different _finer-grained_ variants.

**The Replace form.** Given a positive text describing a scene, we generate a Replace hard negative by replacing an atomic concept in the original text with a new concept that makes the text mismatch with the original scene. Based on the type of the atomic concept--object, attribute, or relation--we further categorize Replace hard negatives into Replace-Obj, Replace-Att, and Replace-Rel.

**The Swap form.** Different from Replace, Swap does not introduce new concepts in the hard negatives, but a Swap hard negative is generated by swapping two atomic concepts of the same category in the positive text. We further categorize Swap into Swap-Obj and Swap-Att, and omit swapping two relationships since it generally results in nonsensical texts.

**The Add form.** Similar to the Replace form, but instead of replacing an atomic concept with a new one, we generate an Add hard negative by adding a new atomic concept to the positive text that makes it mismatch with the original scene. We only further categorize Add into Add-Obj (adding object concept) and Add-Att (adding attribute concept), as adding new relationship concepts to the positive texts often make them highly implausible.

**Dataset overview.** The final evaluation set of SugarCrepe consists of \(7,512\) examples, where the numbers for each fine-grained type are listed in Table 2. Each example is an image-to-text retrieval task composed of an image, a positive text, and a hard negative. On SugarCrepe, random chance performance has an average accuracy of \(50\%\). We note that ARO and CREPE additionally consider Shuffle (randomly shuffling words in a sentence) and Negate (adding negation keywords "no/not"

Figure 3: Example prompt (black) and actual hard negative (green) generated from ChatGPT.

[MISSING_PAGE_FAIL:6]

score gap distributions on the final SugarCrepe evaluation set are symmetric around zero. This implies that the previously identified artifacts can no longer be exploited to infer the positive texts. As a result, we show that the previous commonsense and grammar attacks that are extremely successful on existing benchmarks do not work on SugarCrepe. As shown in Table 6, these blind models now consistently rank the _last_ on SugarCrepe as compared to other pretrained CLIP models.

### Re-evaluating recent methods for improving compositionality

Given the vulnerability of existing compositionality benchmarks, it is unclear whether recently proposed methods that show state-of-the-art performances on these benchmarks are indeed effective. Thus, we re-evaluate these methods with SugarCrepe.

Hard negative augmented training.Specifically, we focus on evaluating one common _data-augmentation_ approach considered in [49, 13], where the core idea is to explicitly create hard negatives and train the model to distinguish them. We broadly refer to this training scheme as NegCLIP following [49]. We evaluate two NegCLIP training schemes: finetuning and training from scratch. For finetuning, in addition to taking the model released in [49], we finetune another three NegCLIP models (using ViT-B/32 following [49]) with three respective types of hard negatives (_i.e._, Replace, Swap, Negate) generated using CREPE's [30] source code. For training from scratch, we use RN50 as the base model and train variants of NegCLIP by augmenting the training examples with different types of hard negatives. We perform both training and finetuning on COCO [26].

\begin{table}
\begin{tabular}{l l c c c} \hline \hline  & & \multicolumn{3}{c}{Average Score} \\ \cline{3-5} Hard-negative Type & Metric & ARO+CREPE & SugarCrepe & Pairwise Better Ratio \\ \hline \multirow{2}{*}{Replace} & Commonsense & 37.46 & 50.21 & 77.71 \\  & Grammar & 76.79 & 88.96 & 86.85 \\ \hline \multirow{2}{*}{Swap} & Commonsense & 23.09 & 41.57 & 78.76 \\  & Grammar & 45.67 & 80.46 & 87.02 \\ \hline \multirow{2}{*}{Negate / Add} & Commonsense & 25.24 & 50.20 & 87.24 \\  & Grammar & 65.09 & 90.07 & 95.03 \\ \hline \hline \end{tabular}
\end{table}
Table 4: We compare the commonsense and grammar scores on hard negatives in ARO+CREPE and SugarCrepe. We report both their respective average scores and the ratio where SugarCrepe has higher score than ARO+CREPE in pairwise comparison. Overall, SugarCrepe has hard negatives with better commonsense and grammar.

Figure 4: We compare the Vera (top row) and Grammar (bottom row) score gap distributions between ARO+CREPE (leftmost column), SugarCrepe without adversarial refinement (middle), and SugarCrepe (rightmost). Top row: We see that Vera score gap distribution shifts from the positive spectrum to more centered around zero from ARO+CREPE to SugarCrepe without refinement. After adversarial refinement, we ensure the score gap distribution is centered around zero on SugarCrepe. Bottom row: Similarly, from ARO+CREPE to SugarCrepe, we see the Grammar score gap distribution shifts from the positive spectrum to centered around zero.

**Improvements are overestimated due to unintentionally overfitting.** In Table 5, we first see that NegCLIP finetuned models show significant improvements on ARO+CREPE, boosting the performance more than 10% compared to standard CLIP finetuning on 11 out of 16 cases (highlighted in green). The lifts are especially large when the hard negative type used in finetuning matches that used in evaluation, where NegCLIP finetuned models can achieve near human-level performances. For instance, by finetuning with Replace hard negatives, NegCLIP reaches 94% on ARO+CREPE evaluated with Replace hard negatives (human performance is 95%). While the results on ARO+CREPE suggest that NegCLIP is seemingly sufficient in equipping models with strong compositionality, we however see that the improvements brought by NegCLIP are much smaller on SugarCrepe. In fact, none of the improvements on SugarCrepe is larger than 10%, and the best performing NegCLIP finetuned models still have large gaps to human-level performances, _e.g._, best NegCLIP model lags behind human by 23% on SugarCrepe's Swap hard negatives. Similarly, when trained from scratch, we observe the same trend that NegCLIP's improvements are much larger on ARO+CREPE than on SugarCrepe. The improvements on ARO+CREPE are again most pronounced when the training and testing hard negative type matches.

We attribute the stark contrast in NegCLIP's effectiveness on ARO+CREPE and SugarCrepe to model's unintentional overfitting: The NegCLIP models learned to exploit artifacts that can be used to easily distinguish hard negatives from positives on ARO+CREPE, instead of actually improving compositionality. Thus, when evaluated on SugarCrepe where the artifacts are removed, the improvement from NegCLIP drastically reduces. These results imply that NegCLIP's effectiveness is overestimated on existing benchmarks, and we may still need further innovations to fundamentally improve a model's compositionality. 4

Footnote 4: In Appendix D.3, we provide further results on training NegCLIP with hard negatives filtered with our adversarial refinement mechanism.

### Comprehensive evaluations on existing pretrained vision-language models

We present four key findings in our evaluation over \(17\) pretrained CLIP models on SugarCrepe, with results reported in Table 6 and visualized in Figure 5.

**The best pretrained CLIP models demonstrate some compositional understanding but still have overall large rooms for improvements.** Table 6 shows that the largest pretrained CLIP models, _e.g._, OpenAI's RN50x64, LAION's xlm-roberta-large-ViT-H-14, and DataComp's ViT-L-14, achieve near-human performance on Replace-Obj. However, on Replace-Obj, smaller models pretrained on small datasets still suffer from big drops in performance -- 23% and 43% respectively for DataComp's small and medium models -- compared to humans. Additionally, on nearly all other hard negative types, there are clear gaps (larger than 10%) between the best model performances and human performances, showing an overall large room for improvements in current models' compositionality.

\begin{table}
\begin{tabular}{l l l l l l l l l l} \hline \hline  & & & & \multicolumn{3}{c}{**ARO+CREPE**} & \multicolumn{3}{c}{**SugarCrepe**} \\ \cline{3-10}
**Model** & **Training** & & **Hard Negative Used** & **Replace** & **Swap** & **NeGate** & **Simple** & **Replace** & **Swap** & **Add** \\ \hline Human & & & 95.33 & 100 & 99.33 & 96.00 & 98.67 & 99.50 & 99.00 \\ \hline \multirow{4}{*}{ViT-B/32} & Pretrained & N/A & 75.71 & 71.58 & 76.89 & 72.06 & 80.76 & 63.27 & 75.09 \\  & CLIP finetuned & N/A & 77.06 & 68.81 & 61.19 & 63.04 & 84.76 & 70.83 & 85.58 \\ \cline{2-10}  & \multirow{3}{*}{NeGCLIP finetuned} & Replace & 94.51 & 90.04 & 85.06 & 88.15 & 88.27 & 74.89 & 90.16 \\  & & Swap & 82.88 & 94.48 & 77.57 & 87.00 & 85.54 & 76.21 & 86.56 \\  & & Niedate & 77.24 & 68.91 & 99.54 & 64.28 & 84.97 & 70.29 & 85.84 \\  & & Released in [49] & 85.72 & 94.35 & 83.51 & 90.45 & 85.36 & 75.33 & 87.29 \\ \hline \multirow{4}{*}{RN50} & CLIP from scratch & N/A & 69.93 & 59.96 & 55.36 & 68.78 & 69.54 & 60.33 & 67.63 \\ \cline{2-10}  & \multirow{3}{*}{NeGCLIP from scratch} & Replace & 89.04 & 66.51 & 60.90 & 75.23 & 74.32 & 62.65 & 72.92 \\ \cline{1-1}  & & Swap & 72.33 & 92.29 & 64.51 & 84.84 & 73.31 & 68.35 & 71.93 \\ \cline{1-1}  & & Negate & 70.09 & 60.29 & 99.45 & 69.03 & 72.74 & 60.89 & 70.47 \\ \cline{1-1}  & & Rep + Sw + Neg & 86.30 & 88.60 & 99.34 & 82.93 & 75.26 & 67.69 & 73.08 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Re-evaluating hard negative augmented training shows that the method’s improvements on existing benchmarks (ARO+CREPE) are hugely overestimated, particularly when the test hard negative type matches the one used in training, which can be attributed to overfitting the artifacts. Color notations: Gains compared to standard CLIP (finetuned / from scratch) \(>10\%\)

[MISSING_PAGE_FAIL:9]

Discussions

Our investigation reveals significant biases present in existing benchmarks for the compositional comprehension capability of vision-language models. The severity of this vulnerability is exemplified by text-only models without access to the image outperforming vision-language models. To address this, we introduce SugarCrepe, a novel benchmark for evaluating the compositionality of vision-language understanding. Unlike previous benchmarks that relied on rule-based templates, we leverage large language models to generate less biased negatives and employ adversarial filtering mechanisms to minimize biases. Through reassessment of state-of-the-art models and recently proposed compositionality inducing mechanisms, we uncover a significant overestimation of their advancements, underscoring the need for further innovation.

### Limitation and future work

Scope of the compositionality benchmarks and vision-language models.We focus our scope on compositionality benchmarks formulated as image-to-text retrieval task. While this is currently the most prevailing evaluation framework, future research can characterize compositionality evaluation as text-to-image retrieval problem, as in the initial efforts considered by [38; 45]. More importantly, we hope our work can guide future efforts in creating and ensuring faithful compositionality benchmarks in text-to-image form. In addition, we focus our evaluations on contrastively learned vision-language models [36]. Future work should include and characterize the compositionality of modern generative vision-language models [1; 7; 24; 46].

**Potential biases imposed by language models.** In this work, we identify _two_ human interpretable dataset biases, the nonsensical and non-fluent biases, which may not cover all dataset artifacts that could possibly be exploited by a model. By leveraging ChatGPT in generating hard negatives, the generated captions may also exhibit hard to detect biases imposed by the language model, e.g., watermarks [21]. Future work may utilize more sophisticated adversarial filtering techniques that train models to detect and remove spurious dataset artifacts beyond human comprehension [51; 23].

**Shifts in language model behavior.** Our work leverages ChatGPT to generate hard negatives. However, recent work has pointed out that the underlying model behind these APIs may change, resulting in model behavior shifts [6; 28]. We discuss how this potential model behavior shift may affect our proposed dataset construction pipeline. Specifically, while there may be variances on the quality of the generated texts, we note that our employed adversarial refinement mechanism can ensure that the final evaluation set is free of the identified artifacts. In the case when ChatGPT improves and generates higher-quality captions, the refinement mechanism will filter out less examples and we can more efficiently create the final evaluation set. On the other hand, if ChatGPT degrades and shifts towards generating less fluent and plausible captions, the refinement mechanism will filter out more generated examples and we would need to generate more candidates in order to create an evaluation set of the same desired size. As a result, while the efficiency of the proposed dataset construction pipeline depends on quality of the language model used, our pipeline ensures the generated set does not contain the identified biases. In the large language model era, we see these capable models as productive tools one can leverage to efficiently process and create data. We do however deem careful validation mechanisms, such as our manual and automatic filtering technique, necessary to ensure that the ultimate goal is properly achieved.

### Societal impact

As vision-language models such as CLIP [36] are becoming the foundation models for many downstream applications [40; 37], it is imperative to understand the limitations of these models to avoid misuses and undesirable outcomes [8; 2]. Compositionality benchmarks probe a model's understanding of finer-grained concepts, and hence allow us to identify blind spots [49; 53; 30] of seemingly powerful models deemed by standard classification and retrieval benchmarks [11; 26]. Our work further alleviates common artifacts in existing compositionality benchmarks that result in overestimation of a model's capability. We hope our proposed benchmark SugarCrepe leads to more faithful assessment of a vision-language model's compositionality, and can hence guide more accurate usages of the models. Nevertheless, we note that strong performances on SugarCrepe do not imply perfect models. We envision SugarCrepe being one of the many benchmarks used to comprehensively understand the abilities of vision-language models from various aspects.

## References

* [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _Advances in Neural Information Processing Systems_, 35:23716-23736, 2022.
* [2] Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng, Debora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, and Aylin Caliskan. Easily accessible text-to-image generation amplifies demographic stereotypes at large scale. _arXiv preprint arXiv:2211.03759_, 2022.
* [3] Leon Bottou. From machine learning to machine reasoning. _Machine learning_, 94(2):133-149, 2014.
* [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [5] Paola Cascante-Bonilla, Khaled Shehada, James Seale Smith, Sivan Doveh, Donghyun Kim, Rameswar Panda, Gul Varol, Aude Oliva, Vicente Ordonez, Rogerio Feris, and Leonid Karlinsky. Going beyond nouns with vision & language models using synthetic data, 2023.
* [6] Lingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt's behavior changing over time? _arXiv preprint arXiv:2307.09009_, 2023.
* [7] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. _arXiv preprint arXiv:2209.06794_, 2022.
* [8] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of text-to-image generative transformers. _arXiv preprint arXiv:2202.04053_, 2022.
* [9] Noam Chomsky and Morris Halle. Some controversial questions in phonological theory. _Journal of linguistics_, 1(2):97-138, 1965.
* [10] MJ Cresswell. Logics and languages. 1973.
* [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pages 248-255, 2009.
* [12] Anuj Diwan, Layne Berry, Eunsol Choi, David Harwath, and Kyle Mahowald. Why is winoground hard? investigating failures in visuolinguistic compositionality. _arXiv preprint arXiv:2211.00768_, 2022.
* [13] Sivan Doveh, Assaf Arbelle, Sivan Harary, Eli Schwartz, Roei Herzig, Raja Giryes, Rogerio Feris, Rameswar Panda, Shimon Ullman, and Leonid Karlinsky. Teaching structured vision & language concepts to vision & language models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2657-2668, 2023.
* [14] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. _arXiv preprint arXiv:2304.14108_, 2023.
* [15] Madeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. Agqa: A benchmark for compositional spatio-temporal reasoning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021.
* [16] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. Annotation artifacts in natural language inference data. _arXiv preprint arXiv:1803.02324_, 2018.

* [17] Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: How do neural networks generalise? _Journal of Artificial Intelligence Research_, 67:757-795, 2020.
* [18] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021.
* [19] Theo MV Janssen and Barbara H Partee. Compositionality. In _Handbook of logic and language_, pages 417-473. Elsevier, 1997.
* [20] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos Niebles. Action genome: Actions as compositions of spatio-temporal scene graphs. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10236-10247, 2020.
* [21] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. _arXiv preprint arXiv:2301.10226_, 2023.
* [22] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _International journal of computer vision_, 123(1):32-73, 2017.
* [23] Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew Peters, Ashish Sabharwal, and Yejin Choi. Adversarial filters of dataset biases. In _International Conference on Machine Learning_, pages 1078-1088. PMLR, 2020.
* [24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In _ICML_, 2023.
* [25] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 12888-12900. PMLR, 2022.
* [26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.
* [27] Jiacheng Liu, Wenya Wang, Dianzhuo Wang, Noah A Smith, Yejin Choi, and Hannaneh Hajishirzi. Vera: A general-purpose plausibility estimation model for commonsense statements. _arXiv preprint arXiv:2305.03695_, 2023.
* [28] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large language models' alignment. _arXiv preprint arXiv:2308.05374_, 2023.
* [29] Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. Visual relationship detection with language priors. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part I 14_, pages 852-869. Springer, 2016.
* [30] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. Crepe: Can vision-language foundation models reason compositionally? _arXiv preprint arXiv:2212.07796_, 2022.
* [31] John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 119-126, 2020.

* [32] OpenAI. Chatgpt. 2022.
* [33] OpenAI. Gpt-4v(ision) system card. 2023.
* [34] Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, and Anna Rohrbach. Benchmark for compositional text-to-image synthesis. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)_, 2021.
* [35] Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson. Data cards: Purposeful and transparent dataset documentation for responsible ai. In _2022 ACM Conference on Fairness, Accountability, and Transparency_, pages 1776-1826, 2022.
* [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR, 2021.
* [37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [38] Arijit Ray, Filip Radenovic, Abhimanyu Dubey, Bryan A. Plummer, Ranjay Krishna, and Kate Saenko. Cola: How to adapt vision-language models to compose objects localized with attributes?, 2023.
* [39] Yuval Reif and Roy Schwartz. Fighting bias with bias: Promoting model robustness by amplifying dataset biases. _arXiv preprint arXiv:2305.18917_, 2023.
* [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [41] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106, 2021.
* [42] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmareczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* [43] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15638-15650, 2022.
* [44] Harman Singh, Pengchuan Zhang, Qifan Wang, Mengjiao Wang, Wenhan Xiong, Jingfei Du, and Yu Chen. Coarse-to-fine contrastive learning in image-text-graph space for improved vision-language compositionality. 2023.
* [45] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5238-5248, 2022.
* [46] Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer. Image captioners are scalable vision learners too. _arXiv preprint arXiv:2306.07915_, 2023.

* [47] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, and Lu Yuan. Omnivl: One foundation model for image-language and video-language tasks. _arXiv preprint arXiv:2209.07526_, 2022.
* [48] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. _arXiv preprint arXiv:2208.10442_, 2022.
* [49] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In _International Conference on Learning Representations_, 2023.
* [50] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale adversarial dataset for grounded commonsense inference. _arXiv preprint arXiv:1808.05326_, 2018.
* [51] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? _arXiv preprint arXiv:1905.07830_, 2019.
* [52] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18123-18133, 2022.
* [53] Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin. Vl-checklist: Evaluating pre-trained vision-language models with objects, attributes and relations. _arXiv preprint arXiv:2207.00221_, 2022.

Implementation details

### Hardware information

All experiments are run on a machine with an Intel(R) Xeon(R) CPU E5-2678 v3 with a 512G memory and two 48G NVIDIA RTX A6000 GPUs.

### Dataset sources

We obtain all existing datasets from their original sources released by the authors. We refer readers to these sources for the dataset licenses. To the best of our knowledge, the data we use does not contain personally identifiable information or offensive content.

* CREPE [30]: We obtain CREPE dataset from its official repository 5. Footnote 5: [https://github.com/RAIVNLab/CREPE](https://github.com/RAIVNLab/CREPE)
* ARO [49]: We obtain ARO dataset from its official repository 6. Footnote 6: [https://github.com/mertyg/vision-language-models-are-bows](https://github.com/mertyg/vision-language-models-are-bows)
* VL-CheckList [53]: We obtain VL-CheckList dataset from its official repository 7. Footnote 7: [https://github.com/om-ai-lab/VL-CheckList](https://github.com/om-ai-lab/VL-CheckList)
* COCO [26]: We obtain COCO from its official project website 8. Footnote 8: [https://cocodataset.org/](https://cocodataset.org/)

### Software configuration

**Models.** We detail the sources of the pretrained models we use in the paper, and the hyper-parameters used in training our own models.

* Vera model [27]: We obtain pretrained Vera model released by its author 9. Footnote 9: [https://huggingface.co/liujch1998/vera](https://huggingface.co/liujch1998/vera)
* Grammar model [31]: We obtain the Grammar model released by the authors 10. Footnote 10: [https://huggingface.co/textattack/disilbert-base-uncased-CoLA](https://huggingface.co/textattack/disilbert-base-uncased-CoLA)
* All pretrained CLIP models: We obtain all pretrained CLIP models' weights from OpenCLIP 11. Footnote 11: [https://github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)
* NegCLIP models: We obtain weights for pretrained NegCLIP released by the authors 12. For training from scratch and finetuning, we train RN50 and ViT-B/32 based on OpenCLIP codebase and set hyper-parameters as the following: number of warmup steps is 1000, batch size is 256, learning rate is 1e-4, weight decay is 0.1, number of epochs is 30. We augment the original CLIP loss with hard negative captions following NegCLIP [49]. Footnote 12: [https://github.com/mertyg/vision-language-models-are-bows](https://github.com/mertyg/vision-language-models-are-bows)

**Evaluations.** We base our evaluation framework on OpenCLIP [18]. We follow all default hyperparameters used for evaluating models.

## Appendix B Vision-language compositionality benchmarks

We provide an overview of existing vision-language compositionality benchmarks below, with Table 7 summarizing the dataset comparisons.

### Image-to-text formulation

A majority of current benchmarks formulate the evaluation task as image-to-text retrieval problem. These benchmarks generate hard negative texts procedurally through rule-based templates, where each benchmark considers different types of hard negatives.

**VL-Checklist [53].** VL-CheckList aims at evaluating vision-language models' understanding of different objects, attributes, and relationships. It contains Replace hard negatives generated by replacing atomic parts of the positive texts with other foils. VL-CheckList further breaks the hard negatives down into more granular categories based on the type of the replaced atomic part, _i.e_., object, attribute, or relationship.

**ARO [49].** ARO focuses on models' understanding of different relationships, attributes, and order information. It considers Swap and Shuffle hard negatives. Swap hard negatives are generated by swapping two words in the positive texts; on the other hand, Shuffle hard negatives are generated by shuffling words in the positive texts. ARO further divides Swap hard negatives into attribute or relationship type.

**CREPE [30].** CREPE is a large-scale evaluation benchmark that includes three types of hard negatives: Replace, Swap and Negate. Replace and Swap hard negatives are generated as in VL-CheckList and ARO. In addition, Negate hard negatives are generated by adding negation keywords (_i.e_., _not_ or _no_) to the original positive texts. The hard negatives are not further divided into fine-grained types (object, attribute, or relations).

### Text-to-image formulation

Complementary to image-to-text formulation, compositionality can as well be evaluated by probing a model to select an image that best matches a given text description, against other hard negative images as distractors. Unlike hard negative texts, hard negative images are more difficult to obtain and thus current text-to-image compositionality benchmarks are smaller at scale.

Winoground [45].Winoground is a small dataset manually curated by human annotators. Each example in the dataset contains two images and two matching captions, where both captions contain identical words that appear in different orders. Note that Winoground can be used for either image-to-text or text-to-image retrieval. While the original intention for Winoground is to evaluate vision-language compositionality, recent work [12] has pointed out that solving the tasks in Winoground requires not just compositional vision-language understanding, but additionally a suite of other abilities such as commonsense reasoning, or distinguishing visually difficult images.

Cola [38].Cola tests a vision-language model's ability to select an image that correctly matches a given caption, against another distractor image with the same objects and attributes but in the wrong composition. The image pairs are mined from existing datasets. As a result, the final evaluation set is relatively small in size (\(210\) examples in total).

We deem text-to-image evaluation as important as image-to-text evaluation. Future work can explore approaches to generate or mine compositional hard negative images at scale, as preliminarily explored in [38, 49].

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & & & \multicolumn{4}{c}{**Hard Negative Text Type**} \\ \cline{3-7}
**Benchmark** & **Task Formulation** & **Scale** & **Shuffle** & **Replace** & **Swap** & **Negate** & **Add** \\ \hline VL-CheckList [53] & Image-to-Text & \(>1000\) & & ✓ & & \\ \hline ARO [49] & Image-to-Text & \(>1000\) & ✓ & & ✓ & \\ \hline CREPE [30] & Image-to-Text & \(>1000\) & & ✓ & ✓ & ✓ & \\ \hline Winoground [45] & Image-to-Text / Text-to-Image & \(400\) & & & ✓ & \\ \hline Cola [38] & Text-to-Image & \(210\) & & & N/A & \\ \hline \hline SugarCrepe & Image-to-Text & \(>1000\) & & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 7: Summary on vision-language compositionality benchmarks. SugarCrepe considers image-to-text formulation to enable larger scale evaluation set. In addition, SugarCrepe considers a wide range of hard negative types. Shuffle and Negate are omitted as they introduce inevitable biases discussed in Sec. 4.2.

SugarCrepe

### Taxonomy

Figure 6 shows the taxonomy of SugarCrepe. We first categorize the hard negatives based on their forms: Replace, Swap, and Add. We then further divide each type of hard negatives into finer-grained sub-categories based on the type (object, attribute, or relation) of the atomic concept altered. SugarCrepe covers a total of \(7\) fine-grained hard negative types.

### Hard negative generation procedure and templates

To generate hard negatives in SugarCrepe, we come up with three different prompt templates for the three hard negative types considered: Replace, Swap, and Add. Each template consists of task instruction for generating the corresponding type of hard negatives and several (7 or more) few-shot demonstrations. We describe the general generation procedure and example prompt templates below and refer readers to our dataset repository for the full prompts used 13.

Footnote 13: [https://github.com/RAIVNLab/sugar-crepe](https://github.com/RAIVNLab/sugar-crepe)

**Generating Replace hard negatives.** To best leverage ChatGPT's capabilities, we devise a three-step workflow to generate Replace hard negatives: (1) We prompt ChatGPT in locating the desired atomic concepts (_e.g._, objects) in the sentence; (2) We prompt ChatGPT to generate a new concept to replace a randomly selected old concept; (3) We let ChatGPT compose a new sentence by replacing the old concept with the new one. For steps (1) and (3), we prompt ChatGPT with a temperature of \(0.0\) to get stable outputs. For step (2), however, we diversify the outputs by prompting ChatGPT with a higher temperature of \(1.5\). With this design, we are able to generate diverse Replace hard negatives. Figure 7 shows the example templates and outputs for Replace hard negatives.

**Generating Swap hard negatives.** To generate swap hard negatives, which do not require any new concepts, we simply prompt ChatGPT once with a temperature of 0.0. Unlike Replace, Swap hard negatives are only possible when there are at least two atomic concepts of the same category, _i.e._, either object or attribute. Thus, our prompt first queries ChatGPT whether it is possible to swap two atomic concepts in the input sentence to generate a new description. Only if the answer is yes, will ChatGPT then proceed to identify two swappable concepts and compose the corresponding new sentence by swapping the two concepts. Figure 8 shows the example templates and outputs for Swap hard negatives.

**Generating Add hard negatives.** Similar to the Replace, we also employ a three-step prompting procedure to generate Add hard negatives. The only difference in the procedure is that we prompt ChatGPT to add the generated new concept to the original caption, instead of using it to replace an old concept. Figure 9 shows the example templates and outputs for Add hard negatives.

### Adversarial refinement

We detail the adversarial refinement procedure below. Given a text model \(M\), we denote its output score for the positive and negative caption of \(i\)-th image as \(M(p_{i})\) and \(M(n_{i})\). If \(M(p_{i})>M(n_{i})\), then the model could identify the correct caption for the \(i\)-th image without referring to it. For a test set to be unattackable given the text model \(M\), the expectation of \(M\)'s identifying the correct caption should be as close to random guess as possible; in particular, we hope that \(E_{i}[M(p_{i})>M(n_{i})]=0.5\). To achieve this for both the grammar model \(M_{1}\) and plausibility model \(M_{2}\), we first calculate the score

Figure 6: Taxonomy of hard negatives considered in SugarCrepe.

difference \(g_{i}^{(1)}=M_{1}(p_{i})-M_{1}(n_{i})\) and \(g_{i}^{(2)}=M_{2}(p_{i})-M_{2}(n_{i})\), where the range of both \(g^{(1)}\) and \(g^{(2)}\) is \([-1,1]\). Then we split the 2D space of the joint range of \(g^{(1)}\) and \(g^{(2)}\) into \(100\times 100\) equal grids, and for each pair of symmetric grids, _e.g._, \(\{(g^{(1)},g^{(2)})|g^{(1)}\in(0.02,0.04],g^{(2)}\in(-0.04,0.06]\}\) and \(\{(g^{(1)},g^{(2)})|g^{(1)}\in(-0.02,-0.04],g^{(2)}\in(0.04,-0.06]\}\), we preserve the same number of data for both grids, therefore we ensure that for the resultant set, \(E_{i}[M_{1}(p_{i})>M_{1}(n_{i})]=0.5\) and \(E_{i}[M_{2}(p_{i})>M_{2}(n_{i})]=0.5\).

### Dataset construction cost

We provide a high-level overview to the cost used to build SugarCrepe by utilizing OpenAI's ChatGPT API for generating hard negatives. In building SugarCrepe, we use approximately \(40\) API calls to generate hard negatives for each COCO test caption, including all different fine-grained types of hard negatives. This amounts to a total of \(25,000\times 40=1,000,000\) API calls to ChatGPT. With each API call costing around \(\$0.0005\), it took roughly \(\$500\) to build SugarCrepe.

Figure 7: Example prompt templates (black) and outputs (green) from ChatGPT for Replace hard negatives.

[MISSING_PAGE_FAIL:19]

* filename: The id to an image
* caption: Positive text correctly describing the image
* negative_caption: Hard negative text incorrectly describing the image

**Maintenance plan.** We are committed to maintain the dataset to address any technical issues. We actively monitor issues in the repository.

**Licensing.** We license our work using MIT License 16. All the source data we use is publicly released by prior work [26].

Footnote 16: [https://github.com/RAIVNLab/sugar-crepe/blob/main/LICENSE](https://github.com/RAIVNLab/sugar-crepe/blob/main/LICENSE)

**Author statement.** We the authors will bear all responsibility in case of violation of rights.

## Appendix D Detailed evaluation results

### Full evaluation results on existing benchmarks

We provide the full evaluation results over \(17\) pretrained CLIP models as well as \(2\) text-only models, Vera [27] and the Grammar model [31], on existing compositionality benchmarks in Table 8. We see that the text-only models, arguably without any vision-language compositionality, outperform most of the pretrained CLIP models, achieving state-of-the-art performances on many benchmark tasks. This implies that current benchmarks fail to faithfully reflect a model's vision-language compositionality.

### SugarCrepe human evaluation

To compare the quality of the hard negatives generated in SugarCrepe to those in current benchmarks (_i.e._, ARO+CREPE), we randomly sample \(100\) examples for each of the hard negative types: Replace, Swap, and Negate / Add. Each example is organized to consist of (1) the original positive text, (2) its hard negative in ARO+CREPE, and (3) its hard negative in SugarCrepe. For each example, a human user rates whether the hard negative in ARO+CREPE or that in SugarCrepe is better (or tie) in terms of commonsense and grammatical correctness, respectively. Note that we compare Negate in ARO+CREPE to Add in SugarCrepe, as both hard negatives are intended to probe a model's understanding of the _existence or not_ of an atomic concept. Table 9 shows that hard negatives in SugarCrepe are much more sensical and fluent than that in ARO+CREPE across all three different types. For instance, SugarCrepe has \(68\%\) more sensical and \(46\%\) more fluent hard negatives than ARO+CREPE on Swap.

### Additional NegCLIP results

In this section, we conduct preliminary experiments to answer whether models' performances on SugarCrepe would increase hugely if the models are trained with hard negatives generated through

\begin{table}
\begin{tabular}{l l l l l l l l l l l l l} \hline \hline  & & \multicolumn{4}{c}{**CRAPPE**} & \multicolumn{4}{c}{**ARO**} & \multicolumn{4}{c}{**V1-Checkable**} \\ \cline{3-14}
**Source** & **Model** & **Aromatic** & **Swap** & **Negate** & **V1-Checkable** & **V1-Checkable** & **V1-Checkable** & **Coco-Check** & **Flexible-Not-Check** & **Object** & **Attribute** & **Boldering** \\ \hline \multirow{2}{*}{Test-only model} & Vera [27] & 43.20 & 30.80 & 60.65 & 61.71 & 25.59 & 59.81 & 46.52 & 32.48 & 35.99 & 85.72 \\  & **Grammer**[31] & 18.15 & 30.88 & 9.77 & 59.55 & 58.38 & 74.33 & 76.26 & 57.95 & 52.55 & **68.30** \\ \hline \multirow{6}{*}{OpenAI [34]} & RNS00 & 28.47 & 28.52 & 31.25 & 53.87 & 63.37 & 44.89 & 52.46 & 36.85 & 68.30 & 75.55 \\  & RNS01 & 27.58 & 23.74 & 25.09 & 52.43 & 62.95 & 29.86 & 39.34 & 86.44 & 67.45 & 67.31 \\  & RNS00 & 26.34 & 26.52 & 9.41 & 51.59 & 62.27 & 59.99 & 34.56 & 87.23 & 68.54 & 75.81 \\  & V19.23 & 22.31 & 26.55 & 73.88 & 51.12 & 61.33 & 37.14 & 41.78 & 52.50 & 68.40 & 72.04 \\  & RNS00 & 26.88 & 20.09 & 25.57 & 51.00 & 62.56 & 40.54 & 48.74 & 87.71 & 68.64 & 74.97 \\  & V11.4 & 26.36 & 25.66 & 24.74 & 53.84 & 61.50 & 36.11 & 46.08 & 57.86 & 68.27 & 75.50 \\ \hline \multirow{6}{*}{LAMON [34]} & W18-B4 & 23.30 & 25.22 & 16.84 & 58.33 & 65.93 & 25.79 & 30.96 & 38.59 & 68.46 & 71.13 \\  & V17.44 & 23.39 & 24.78 & 21.09 & 51.00 & 61.29 & 25.57 & 30.10 & 86.67 & 69.03 & 71.63 \\  & V17.44 & 23.39 & 22.66 & 21.74 & 31.73 & 64.00 & 24.71 & 30.25 & 85.41 & 68.62 & 68.40 \\  & V17.82 & 21.16 & 23.00 & 21.76 & 47.01 & 50.73 & 23.85 & 30.32 & 86.74 & 68.50 \\  & **all-order-W18-B4** & 24.16 & 23.80 & 20.05 & 46.44 & 57.84 & 28.05 & 30.00 & 86.76 & 70.41 & 63.01 \\  & **all-order-W18-B4** & 24.16 & 23.80 & 20.05 & 46.44 & 57.84 & 28.05 & 30.00 & 87.89 & 70.25 & 63.50 \\ \hline \multirow{6}{*}{DataComp [14]} & ***117.82** & 13.64 & 27.88 & 14.84 & 50.83 & 50.17 & 13.55 & 14.02 & 68.72 & 58.80 & 57.00 \\  & **all-order-W18-B5** & 18.42 & 20.35 & 11.05 & 50.45 & 54.21 & 16.49 & 16.56 & 18.43 & 65.52 & 65.84 \\  & **all-order-W18-B4** & 24.16 & 22.57 & 16.88 & 48.54 & 60.01 & 23.19 & 20.52 & 86.66 & 67.01 & 67.93 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Blind models (_i.e._, Vera and Grammar model) outperform all \(17\) existing pretrained CLIP models on nearly all existing benchmark tasks. This implies that current benchmarks fail to faithfully measure a model’s vision-language compositionality.

the same procedure as how we create SugarCrepe. Since generating hard negatives for training with ChatGPT would incur substantial cost, we create hard negatives using a proxy method. In particular, we start with template-generated hard negatives on the COCO training set and apply our adversarial refinement technique to remove the biases. We use this adversarially refined dataset for NegCLIP training. We show the results in Table 10. While we observe that the method improves over vanilla CLIP training without hard negatives, it performs similarly to NegCLIP and does not saturate the performance on SugarCrepe. This suggests that while the adversarial refinement mechanism prevents SugarCrep from being attacked as an evaluation benchmark, leveraging the approach alone for training does not saturate the performance on SugarCrepe. Future work may characterize how LLMs could be used to generate better hard negatives for training to genuinely improve vision-language models' compositionality.

\begin{table}
\begin{tabular}{l l c c c} \hline \hline  & & \multicolumn{3}{c}{SugarCrepe} \\ \cline{3-5} Model & Hard negative & Replace & Swap & Add \\ \hline CLIP without hard negatives & N/A & 69.54 & 60.33 & 67.63 \\ \hline NegCLIP with template hard negatives & Replace & 74.32 & 62.65 & 72.92 \\ NegCLIP with adversarial refined hard negatives & Replace & 73.37 & 61.40 & 72.84 \\ \hline NegCLIP with template hard negatives & Swap & 73.31 & 68.35 & 71.93 \\ NegCLIP with adversarial refined hard negatives & Swap & 72.07 & 65.13 & 69.68 \\ \hline NegCLIP with template hard negatives & Negate & 72.74 & 60.89 & 70.47 \\ NegCLIP with adversarial refined hard negatives & Negate & 72.70 & 60.75 & 68.70 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Model performances on SugarCrepe when trained with hard negatives generated through similar procedure as how SugarCrepe is created.

\begin{table}
\begin{tabular}{l l c c c} \hline \hline  & & \multicolumn{3}{c}{Human counts of better examples} \\ \cline{3-5} Hard-negative Type & Evaluation & ARO+CREPE & SugarCrepe & Tie \\ \hline \multirow{2}{*}{Replace} & Commonsense & 11 & 29 & 60 \\  & Grammar & 4 & 33 & 63 \\ \hline \multirow{2}{*}{Swap} & Commonsense & 4 & 68 & 28 \\  & Grammar & 4 & 46 & 50 \\ \hline \multirow{2}{*}{Negate / Add} & Commonsense & 1 & 26 & 73 \\  & Grammar & 1 & 35 & 64 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Human evaluation results on the comparisons between hard negatives in ARO+CREPE and SugarCrepe. We report the counts (out of \(100\) sampled examples) that the human user considers better or tie, w.r.t. both commonsense and grammatical correctness.