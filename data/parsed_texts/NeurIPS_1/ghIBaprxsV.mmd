# Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration

Longlin Yu\({}^{1,*}\), Tianyu Xie\({}^{1,*}\), Yu Zhu\({}^{3,4,*}\), Tong Yang\({}^{5}\), Xiangyu Zhang\({}^{5}\), Cheng Zhang\({}^{1,2,}\)

\({}^{1}\) School of Mathematical Sciences, Peking University

\({}^{2}\) Center for Statistical Science, Peking University

\({}^{3}\) Institute of Automation, Chinese Academy of Sciences

\({}^{4}\) Beijing Academy of Artificial Intelligence

\({}^{5}\) Megvii Technology Inc.

{llyu, tianyuxie}@pku.edu.cn, zhuyu2022@ia.ac.cn,

{yangtong, zhangxiangyu}@megvii.com, chengzhang@math.pku.edu.cn

Equal contribution. This work was done during an internship at Megvii Technology Inc.Corresponding Author.

###### Abstract

Semi-implicit variational inference (SIVI) has been introduced to expand the analytical variational families by defining expressive semi-implicit distributions in a hierarchical manner. However, the single-layer architecture commonly used in current SIVI methods can be insufficient when the target posterior has complicated structures. In this paper, we propose hierarchical semi-implicit variational inference, called HSIVI, which generalizes SIVI to allow more expressive multi-layer construction of semi-implicit distributions. By introducing auxiliary distributions that interpolate between a simple base distribution and the target distribution, the conditional layers can be trained by progressively matching these auxiliary distributions one layer after another. Moreover, given pre-trained score networks, HSIVI can be used to accelerate the sampling process of diffusion models with the score matching objective. We show that HSIVI significantly enhances the expressiveness of SIVI on several Bayesian inference problems with complicated target distributions. When used for diffusion model acceleration, we show that HSIVI can produce high quality samples comparable to or better than the existing fast diffusion model based samplers with a small number of function evaluations on various datasets.

## 1 Introduction

Variational inference (VI) is an approximate Bayesian inference method that is gaining in popularity, where one tries to find an approximation to the target posterior distribution using an optimization approach (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2016). To do that, it first posits a family of variational distributions and then seeks the closest member from this family that minimizes some statistical distance to the target posterior, usually the Kullback-Leibler (KL) divergence. As the posterior is not analytically available, an equivalent formulation is often adopted in practice where one maximizes the evidence lower bound (ELBO) instead (Jordan et al., 1999).

One classical VI method is mean-field VI, which assumes a factorizable structure of the variational distributions over the parameters or latent variables (Bishop & Tipping, 2000). This often leads to closed-form coordinate-ascent update rules when certain conditional conjugacy conditions are satisfied. In practice, the conditional conjugacy may not hold and the true posterior could be muchmore complicated than what a factorized variational distribution can accurately approximate. In recent years, several attempts have been made in VI that alleviate these constraints by designing more flexible variational families (Jaakkola and Jordan, 1998; Saul and Jordan, 1996; Giordano et al., 2015; Tran et al., 2015; Rezende and Mohamed, 2015; Dinh et al., 2017; Kingma et al., 2016; Papamakarios et al., 2019), together with generic training algorithms via Monte Carlo gradient estimators (Nott et al., 2012; Paisley et al., 2012; Ranganath et al., 2014; Rezende et al., 2014; Kingma and Welling, 2014). While successful, these approaches all assume tractable densities of variational distributions. To further expand the capacity of variational families, one approach is to incorporate the implicit models that have intractable densities but are easy to sample from (Huszar, 2017; Tran et al., 2017; Mescheder et al., 2017; Shi et al., 2018, 2018; Song et al., 2019). However, as the densities are intractable for implicit models, one often resorts to density ratio estimation for ELBO evaluation during training, which is known to be difficult in high dimensional settings (Sugiyama et al., 2012). To avoid density ratio estimation, semi-implicit variational inference (SIVI) has been proposed where the variational distributions are formed through a semi-implicit hierarchical construction, and various training criteria have been employed (Yin and Zhou, 2018; Moens et al., 2021; Titsias and Ruiz, 2019; Yu and Zhang, 2023).

While striking a good balance between approximation flexibility and training efficiency, current SIVI methods often use a single conditional layer which can be insufficient when the target posterior possesses complicated structures (e.g., multimodality, see an example in Section 5.1). To enhance the expressiveness of single-layer models, an intuitive but effective approach is to extend them to multi-layer hierarchical models (Vahdat and Kautz, 2020; Ranganath et al., 2016; Sobolev and Vetrov, 2019). In this paper, we propose hierarchical semi-implicit variational inference (HISVI), which is a generalization of SIVI that allows multiple conditional layers. Instead of training the hierarchical semi-implicit model end to end, we introduce auxiliary distributions that interpolate between a simple base distribution and the target distribution to guide the intermediate semi-implicit distributions toward the target distribution. The conditional layers are then trained sequentially to match these auxiliary bridging distributions given the fitted semi-implicit distributions from the previous layers (Figure 1), using different criteria from before. This way, HISVI allows progressive learning of the target distribution that significantly reduces the burden of each conditional layer. Moreover, HISVI with the score matching objective can also be used to accelerate the sampling process of diffusion models where the pre-trained score networks corresponding to different noise levels provide a natural sequence of bridging distributions. In experiments, we demonstrate the effectiveness of HISVI on both Bayesian inference tasks with complicated target distributions and diffusion model acceleration.

## 2 Background on semi-implicit variational inference

The semi-implicit variational family (Yin and Zhou, 2018; Titsias and Ruiz, 2019) is defined as

\[q_{\phi}(\mathbf{x})=\int q_{\phi}(\mathbf{x}|\mathbf{z})q(\mathbf{z})\mathrm{d}\mathbf{z}, \tag{1}\]

where \(\phi\) are the variational parameters, \(q_{\phi}(\mathbf{x}|\mathbf{z})\) is called the conditional layer, and \(q(\mathbf{z})\) is called the mixing layer. This variational family is said to be semi-implicit as \(q_{\phi}(\mathbf{x}|\mathbf{z})\) is required to be explicit and \(q(\mathbf{z})\) is often implicit. The semi-implicit variational family is capable of capturing more complicated dependencies between variables (Yin and Zhou, 2018; Titsias and Ruiz, 2019; Yu and Zhang, 2023) than explicit variational families without the hierarchical structure. Given the observed data \(D\), the classical VI methods often use the evidence lower bound (ELBO) for training, which is defined as \(\text{ELBO}:=\mathbb{E}_{q_{\phi}(\mathbf{x})}\left[\log p(D,\mathbf{x})-\log q_{\phi}( \mathbf{x})\right]\). However, as \(q_{\phi}(\mathbf{x})\) is no longer tractable in SIVI, alternative training objectives have been introduced.

ELBO related objectivesYin and Zhou (2018) considered a sequence of lower bounds of the ELBO

\[\mathcal{L}_{\text{SIVI-LB}}(p(\mathbf{x}|D)\|q_{\phi}(\mathbf{x})):=\mathbb{E}_{\mathbf{x }\sim q(\mathbf{x}),\mathbf{x}\sim q_{\phi}(\mathbf{x},\mathbf{z})}\mathbb{E}_{\{\mathbf{z}^{(i)} \}_{i=1}^{K}\times\mathbb{d}^{-}q(\mathbf{z})}\log\frac{p(D,\mathbf{x})}{\frac{1}{K+1} \left(q_{\phi}(\mathbf{x}|\mathbf{z})+\sum_{k=1}^{K}q_{\phi}(\mathbf{x}|\mathbf{z}^{(k)}) \right)}. \tag{2}\]

It is an asymptotically exact surrogate in the sense that \(\lim_{K\rightarrow\infty}\mathcal{L}_{\text{SIVI-LB}}=\text{ELBO}\). Titsias and Ruiz (2019) proposed unbiased implicit variational inference (UIVI) which uses samples from the inverse conditional distribution \(q_{\phi}(\mathbf{z}|\mathbf{x})\) (from an MCMC run, e.g. Hamiltonian Monte Carlo (Neal, 2011)) to provide an unbiased gradient estimator of the exact ELBO. See more details of UIVI in Appendix B.

Score matching objectiveBesides the ELBO, score based distance measures have also been used for variational inference where the score function \(\mathbf{S}(\mathbf{x}):=\nabla_{\mathbf{x}}\log p(\mathbf{x}|D)=\nabla_{\mathbf{x}}\log p(D,\mathbf{x})\) is assumed to be tractable (Liu et al., 2016; Zhang et al., 2018; Hu et al., 2018). Yu & Zhang (2023) considered the following Fisher divergence between the target distribution and the semi-implicit variational distribution

\[\mathcal{D}_{\text{Fisher}}(p(\mathbf{x}|D)\|q_{\phi}(\mathbf{x})):=\mathbb{E}_{\mathbf{x }\sim q_{\phi}(\mathbf{x})}\|\mathbf{S}(\mathbf{x})-\nabla_{\mathbf{x}}\log q_{\phi}(\mathbf{x})\| _{2}^{2}. \tag{3}\]

By reformulating \(\mathcal{D}_{\text{Fisher}}\) as the maximum of the following optimization problem

\[\mathcal{D}_{\text{Fisher}}(p(\mathbf{x}|D)\|q_{\phi}(\mathbf{x}))=\max_{\mathbf{f}(\mathbf{x} )}\left[2\mathbf{f}(\mathbf{x})^{T}(\mathbf{S}(\mathbf{x})-\nabla_{\mathbf{x}}\log q_{\phi}(\mathbf{x }))-\|\mathbf{f}(\mathbf{x})\|_{2}^{2}\right],\]

and using a similar trick as in denoising score matching (Vincent, 2011; Song & Ermon, 2019), one can transform the minimization of \(\mathcal{D}_{\text{Fisher}}\) into the following minimax problem which is tractable

\[\min_{\phi}\max_{\psi}\ \mathcal{L}_{\text{SIVI-SM}}(p(\mathbf{x}|D)\|q_{\phi}( \mathbf{x})):=\mathbb{E}_{\mathbf{x}\sim q(\mathbf{x}),\mathbf{x}\sim q_{\phi}(\mathbf{x}|\mathbf{x})} \left[2\mathbf{f}_{\psi}(\mathbf{x})^{T}[\mathbf{S}(\mathbf{x})-\nabla_{\mathbf{x}}\log q_{\phi}( \mathbf{x}|\mathbf{x})]-\|\mathbf{f}_{\psi}(\mathbf{x})\|_{2}^{2}\right]. \tag{4}\]

In practice, \(\mathbf{f}_{\psi}(\mathbf{x})\) is parametrized using neural networks. The above minimax optimization problem can be efficiently solved by optimizing \(\psi\) and \(\phi\) alternately.

## 3 Hierarchical semi-implicit variational inference

The semi-implicit variational family \(q_{\phi}(\mathbf{x})\) in equation (1) is indeed a single-layer model in the sense that it contains only one conditional layer. Our main idea is to expand this single-layer semi-implicit variational family into its multi-layer variants and introduce a sequence of auxiliary distributions to guide the semi-implicit distributions toward the target distribution. This leads to a new SIVI method which we call hierarchical semi-implicit variational inference (HISVI). We start with the following definition which is motivated by equation (1).

**Definition 1** (Hierarchical Semi-Implicit Distribution).: _Let \(\mathbf{x}_{T}\sim q_{T}(\mathbf{x}_{T})\) for some \(T\in\mathbb{N}^{\star}\), where \(q_{T}(\mathbf{x}_{T})\) is called the variational prior. Let \(q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})\) be the \(t\)-th conditional layer for \(0\leq t\leq T-1\). Denote \(\{\phi_{k}\}_{k=t}^{T-1}\) by \(\phi_{\geq t}\). The \(t\)-th layer hierarchical semi-implicit distribution \(q_{t}(\mathbf{x}_{t};\phi_{\geq t})\) is defined recursively from \(T-1\) to \(0\) by_

\[q_{t}(\mathbf{x}_{t};\phi_{\geq t})=\int q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})q_ {t+1}(\mathbf{x}_{t+1};\phi_{\geq t+1})\mathrm{d}\mathbf{x}_{t+1},\quad 0\leq t\leq T -1, \tag{5}\]

_where \(q_{T}(\mathbf{x}_{T};\phi_{\geq T}):=q_{T}(\mathbf{x}_{T})\). Here, the \(t\)-th conditional layer \(q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})\) is required to be explicit and reparametrizable with a tractable score function \(\nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})\)._

Compared to the single-layer semi-implicit variational family (1), the family of hierarchical semi-implicit distributions provides a principled way to construct more expressive mixing layers using multi-layer architectures. Also, unlike the hierarchical variational models (Ranganath et al., 2016) which require an extra reverse model and explicit variational prior, hierarchical semi-implicit distributions inherit the advantage of SIVI that allows \(q_{t}(\mathbf{x}_{t};\phi_{\geq t})\) to be implicit, and as shown next, they do not require a reverse model and can be progressively trained using the simple algorithms of SIVI for each conditional layer, from \(t=T-1\) to \(t=0\).

Figure 1: An example for 4-layer HISVI. The target distribution \(p_{0}(\mathbf{x})\) is a Gaussian mixture and the auxiliary distributions \(\{p_{i}(\mathbf{x})\}_{i=0}^{3}\) are constructed using the diffusion bridge. The auxiliary distributions are plotted in the squares, where the blue heatmap describes the probability density and the arrows represent the score functions of the auxiliary distributions.

### Progressive approximation with the auxiliary bridge

In this section, we introduce a bridging technique for progressively approximating the target distribution \(p(\mathbf{x})\) using hierarchical semi-implicit distributions. Rather than approximating \(p(\mathbf{x})\) with \(q_{0}(\mathbf{x};\phi_{\geq 0})\) directly, we construct a sequence of intermediate auxiliary distributions \(\{p_{t}(\mathbf{x})\}_{t=0}^{T-1}\) as a bridge between the target distribution \(p_{0}(\mathbf{x}):=p(\mathbf{x})\) and an easy-to-approximate distribution \(p_{T-1}(\mathbf{x})\), to amortize the difficulty of one-pass fitting. A typical example of an auxiliary bridge is the geometric interpolation as described below.

**Example 1** (Geometric Interpolation).: _Let \(\mathbf{S}(\mathbf{x}):=\nabla\log p(\mathbf{x})\) be the score function of target distribution \(p(\mathbf{x})\) and \(\mathbf{S}_{\text{base}}(\mathbf{x}):=\nabla\log p_{\text{base}}(\mathbf{x})\) be the score function of a base distribution \(p_{\text{base}}(\mathbf{x})\). In geometric interpolation (Neal, 2001; Bernton et al., 2019), each auxiliary distribution \(p_{t}(\mathbf{x})\) for \(0\leq t\leq T-1\) has the following probability density function (pdf) and score function_

\[p_{t}(\mathbf{x})\propto p_{\text{base}}(\mathbf{x})^{1-\lambda_{t}}p(\mathbf{x})^{\lambda _{t}},\;\mathbf{S}_{t}(\mathbf{x}):=\nabla_{\mathbf{x}}\log p_{t}(\mathbf{x})=(1-\lambda_{t}) \mathbf{S}_{\text{base}}(\mathbf{x})+\lambda_{t}\mathbf{S}(\mathbf{x}), \tag{6}\]

_where \(\{\lambda_{t}\}_{t=0}^{T-1}\) is a non-negative decreasing sequence satisfying \(\lambda_{0}=1\)._

Intuitively, we expect the distance between two neighboring distributions \(p_{t}(\mathbf{x})\) and \(p_{t+1}(\mathbf{x})\) to be not too large so that it would be easy to construct a conditional distribution \(q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1})\) such that \(p_{t}(\mathbf{x}_{t})\approx\int q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1})p_{t+1}(\mathbf{x}_{t+1 })\mathrm{d}\mathbf{x}_{t+1}\). Note that the auxiliary bridge \(\{p_{t}(\mathbf{x})\}_{t=0}^{T-1}\) does not necessarily need to have analytical pdfs (up to a constant). In fact, it suffices if they have tractable score functions \(\{\mathbf{S}_{t}(\mathbf{x})\}_{t=0}^{T-1}\) which lead to another type of auxiliary bridge (Example 2 in Section 4).

### Sequential training of HSIVI

Given the auxiliary distributions \(\{p_{t}(\mathbf{x}_{t})\}_{t=0}^{T-1}\), a natural approach is to progressively train the hierarchical semi-implicit distribution \(q_{t}(\mathbf{x}_{t};\phi_{\geq t})\) to match \(p_{t}(\mathbf{x}_{t})\) from \(t=T-1\) to \(t=0\). Let the parameters \(\phi_{t}\) in the \(t\)-th conditional layer be independent across different \(t\)s. We first train \(q_{T-1}(\mathbf{x}_{T-1};\phi_{T-1})\) to match \(p_{T-1}(\mathbf{x}_{T-1})\) by optimizing \(\phi_{T-1}\) w.r.t. the single-layer SIVI objective \(\mathcal{L}_{\text{SIVI-}f}\left(p_{T-1}(\mathbf{x}_{T-1})\middle|q_{T-1}(\mathbf{x}_{ T-1};\phi_{T-1})\right)\). For \(t=T-2,\ldots,0\), given the trained semi-implicit distribution \(q_{t+1}(\mathbf{x}_{t+1};\phi_{\geq t+1})\), we can fix it as the mixing layer and train the \(t\)-th conditional layer \(q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})\) by optimizing \(\phi_{t}\) w.r.t. the single-layer SIVI objective \(\mathcal{L}_{\text{SIVI-}f}\left(p_{t}(\mathbf{x}_{t})\middle|q_{t}(\mathbf{x}_{t};\phi _{\geq t})\right)\) as well. Note this is fine as the mixing layer can be implicit in SIVI. Here, \(f\) is some distance criterion, e.g. \(\mathcal{L}_{\text{SIVI-IB}}\) in equation (2) or \(\mathcal{L}_{\text{SIVI-SM}}\) in equation (4). In this article, we mainly focus on \(\mathcal{L}_{\text{SIVI-IB}}\) and \(\mathcal{L}_{\text{SIVI-SM}}\), while other distance criteria can also be applied. We summarize this sequential training procedure in Algorithm 1.

```
Input: Auxiliary bridge \(\{p_{t}(\mathbf{x})\}_{t=0}^{T-1}\); initial value of parameters \(\phi^{(0)}=\{\phi_{t}^{(0)}\}_{t=0}^{T-1}\). Output: The optimal parameters \(\phi^{*}\).  Initialization: \(\phi\leftarrow\phi^{(0)}\). for\(t=T-1\)to 0do while not converge do  Sample a minibatch \(\{\mathbf{x}_{T}^{(k)}\}_{k=1}^{K}\) from the variational prior \(q_{T}(\mathbf{x}_{T})\). if\(t<T-1\)then  Sequentially sample \(\{\mathbf{x}_{t+1}^{(k)}\}_{k=1}^{K}\) through \(q(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{i})\) from \(i=T-1\) to \(i=t+1\).  Detach the computation graphs from \(\{\mathbf{x}_{t+1}^{(k)}\}_{k=1}^{K}\). endif  Update \(\phi_{t}\) by optimizing the \(\mathcal{L}_{\text{SIVI-}f}\left(p_{t}(\mathbf{x}_{t})\middle|q_{t}(\mathbf{x}_{t};\phi _{\geq t})\right)\) based on the minibatch \(\{\mathbf{x}_{t+1}^{(k)}\}_{k=1}^{K}\). endwhile \(\phi_{t}^{*}\leftarrow\phi_{t}\). endfor \(\phi^{*}\leftarrow\{\phi_{t}^{*}\}_{t=0}^{T-1}\).
```

**Algorithm 1** Hierarchical semi-implicit variational inference (sequential training)

Score based trainingIn addition to the common assumption that \(p_{t}(\mathbf{x})\) is known up to a constant, it is worth noting that \(\mathcal{L}_{\text{SIVI-IB}}\) is also applicable when only the score functions \(\{\mathbf{S}_{t}(\mathbf{x})\}_{t=0}^{T-1}\) are available which is important for the diffusion bridge construction of auxiliary distributions in Example 2. Concretely, assume \(q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})\) is induced by a parametrized transform \(\mathbf{x}_{t}=\mathbf{h}_{t}(\mathbf{x}_{t+1},\mathbf{\epsilon};\phi_{t})\)where \(\mathbf{\epsilon}\sim p_{\mathbf{\epsilon}}(\mathbf{\epsilon})\) is a random noise. The only term in \(\mathcal{L}_{\text{SIVI-AB}}\left(p_{t}(\mathbf{x}_{t})\|q_{t}(\mathbf{x}_{t};\phi_{\geq t })\right)\) containing \(p_{t}(\mathbf{x}_{t})\) is \(\mathbb{E}_{q_{\mathbf{\epsilon}}(\mathbf{x}_{t};\phi_{\geq t})}\log p_{t}(\mathbf{x}_{t})\) (see equation (2)) whose gradient takes the form

\[\nabla_{\phi_{t}}\mathbb{E}_{q_{\mathbf{\epsilon}}(\mathbf{x}_{t};\phi_{\geq t})}\log p _{t}(\mathbf{x}_{t})=\mathbb{E}_{q_{t+1}(\mathbf{x}_{t+1};\phi_{\geq t+1})p_{\mathbf{ \epsilon}}(\mathbf{\epsilon})}\mathbf{S}_{t}\left(\mathbf{h}_{t}(\mathbf{x}_{t+1},\mathbf{ \epsilon};\phi_{t})\right)\nabla_{\phi_{t}}\mathbf{h}_{t}(\mathbf{x}_{t+1},\mathbf{ \epsilon};\phi_{t}). \tag{7}\]

In the training of HSIVI-SM, each term \(\mathcal{L}_{\text{SIVI-SM}}\left(p_{t}(\mathbf{x}_{t})\|q_{t}(\mathbf{x}_{t};\phi_{ \geq t})\right)\) involves a nested optimization of \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi_{t})\). When the score functions are computationally expensive, we find that an alternative parametrization \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi_{t}):=\mathbf{S}_{t}(\mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t}; \psi_{t})\) is useful to avoid the time-consuming evaluation of \(\mathbf{S}_{t}(\mathbf{x}_{t})\) when optimizing \(\psi_{t}\) in equation (4). The reason for this lies in Proposition 1. See Appendix C.2 for the proof of Proposition 1.

**Proposition 1**.: _Let \(q_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi_{\geq t})=q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi _{t})q_{t+1}(\mathbf{x}_{t+1};\phi_{\geq t+1})\). The minimax optimization of \(\mathcal{L}_{\text{SIVI-SM}}\left(p_{t}(\mathbf{x}_{t})\|q_{t}(\mathbf{x}_{t};\phi_{\geq t })\right)\) is equivalent to_

\[\min_{\phi_{t}} \mathbb{E}_{q_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi_{\geq t})}\left[ \mathbf{S}_{t}(\mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t};\psi_{t})\right]^{T}\left[\mathbf{S}_ {t}(\mathbf{x}_{t})+\mathbf{g}_{t}(\mathbf{x}_{t};\psi_{t})-2\nabla_{\mathbf{x}_{t}}\log q_{t }(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})\right],\] \[\min_{\psi_{t}} \mathbb{E}_{q_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi_{\geq t})}\|\mathbf{g} _{t}(\mathbf{x}_{t};\psi_{t})-\nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1 };\phi_{t})\|_{2}^{2}.\]

Marginal approximation v.s. joint approximationPrevious works (Bernton et al., 2019; Bao et al., 2022) often construct a joint distribution \(p(\mathbf{x}_{0:T})\) and minimize \(\mathrm{KL}(p(\mathbf{x}_{0:T-1})\|q(\mathbf{x}_{0:T-1}))\) where \(q(\mathbf{x}_{0:T-1})\) is a variational distribution. In HSIVI, we directly approximate \(p_{t}(\mathbf{x}_{t})\) using the semi-implicit variational distributions. When \(p(\mathbf{x}_{0:T-1})\) is complex and \(T\) is small, the variational distribution \(q(\mathbf{x}_{0:T-1})\) may be insufficient to fully capture the joint distribution \(p(\mathbf{x}_{0:T-1})\). For example, the optimal fit of the joint distribution for diffusion models established by Analytic-DPM (Bao et al., 2022) does not guarantee that the marginal distributions would be approximated well (see Table 2 for comparison).

## 4 Application to diffusion model acceleration

### Review of diffusion models

Recently, diffusion models have shown great success on many generative modeling benchmarks, including image generation (Ho et al., 2020; Song et al., 2020, 2020, 2020), graph generation (Niu et al., 2020), and text generation (Austin et al., 2021). Diffusion models work by adding noise to the training data in the forward process and then removing the noise to recover the data in the backward process, which can be integrated into a general stochastic differential equation (SDE) framework. The forward process \(\{\mathbf{u}_{s}\}_{s\in[0,L]}\) is usually described by

\[\mathrm{d}\mathbf{u}_{s}=\mathbf{f}(\mathbf{u}_{s},s)\mathrm{d}s+g(s)\mathrm{d}\mathbf{w}_{s}, \quad\mathbf{u}_{0}\sim p_{0}(\cdot), \tag{8}\]

where \(p_{0}(\cdot)\) is the data distribution, \(\mathbf{w}_{s}\) is a standard Brownian motion, and \(\mathbf{f}(\mathbf{u}_{s},s)\) and \(g(s)\) are the drift and diffusion coefficients respectively. To generate samples from the data distribution, one can run the following backward process

\[\mathrm{d}\mathbf{u}_{s}=[\mathbf{f}(\mathbf{u}_{s},s)-g^{2}(s)\nabla_{\mathbf{u}_{s}}\log p_{ s}(\mathbf{u}_{s})]\mathrm{d}s+g(s)\mathrm{d}\tilde{\mathbf{w}}_{s},\quad\mathbf{u}_{L} \sim p_{L}(\cdot), \tag{9}\]

where \(p_{s}(\cdot)\) is the pdf of \(\mathbf{u}_{s}\) and \(\tilde{\mathbf{w}}_{s}\) is a standard Brownian motion when time flows from \(L\) to \(0\). As the score function \(\nabla_{\mathbf{u}_{s}}\log p_{s}(\mathbf{u}_{s})\) is intractable, we need to estimate it by denoising score matching (Vincent, 2011; Song et al., 2020). See more details of diffusion models and the training objectives in Appendix A.

### Diffusion model acceleration via HSIVI

While diffusion models prove effective for generative modeling, it often takes a large number of discretization steps in the backward process (9) to produce high quality samples, which caps their potential for real time applications. Note that the forward process (8) naturally provides another type of auxiliary bridge, which combined with HSIVI, can be used to accelerate the sampling process of diffusion models.

**Example 2** (Diffusion Bridge).: _Consider the forward process \(\{\mathbf{u}_{s}\}_{s\in[0,L]}\) with \(L>0\) (defined in equation (8)) in diffusion models. We choose \(T\) discrete time steps \(0\approx s_{0}<\cdots<s_{T-1}\leq L\) and _let \(\mathbf{x}_{t}:=\mathbf{u}_{s_{t}}\) with probability density function \(p_{t}(\cdot)\). Assume each auxiliary distributions \(p_{t}(\cdot)\) for \(0\leq t\leq T-1\) admits a score function as_

\[\mathbf{S}_{t}(\mathbf{x}):=\nabla_{\mathbf{x}}\log p_{t}(\mathbf{x})\approx\mathbf{S}^{*}(\mathbf{x},s_ {t}),\ 0\leq t\leq T-1,\]

_where \(\mathbf{S}^{*}(\mathbf{x},s)\) is a pre-trained score model with the denoising score matching loss (equation (13) in Appendix A). Let us denote \(\mathbf{S}^{*}(\mathbf{x},s_{t})\) by \(\mathbf{S}^{*}_{t}(\mathbf{x})\) for short. With sufficient samples from the data distribution \(p_{0}(\mathbf{x})\) and model capacity, the approximation \(\mathbf{S}^{*}_{t}(\mathbf{x})\) can be reasonably accurate for almost all \(\mathbf{x}\) and \(t\)(Song et al., 2020b)._

As the pre-trained score model provides a diffusion bridge from the simple distribution \(p_{T-1}\) (e.g., standard Gaussian) to the data distribution, we can train the hierarchical semi-implicit distributions to approximate the diffusion bridge within the HSIVI framework. Given the expressiveness of hierarchical semi-implicit distributions, we may expect an accurate approximation of the data distribution with a small number \(T\) and hence acceleration can be achieved.

However, the memory usage during the sequential training process for HSIVI might be large because of the necessity for independent parameters. Therefore, we may employ a parameter sharing scheme which is commonly assumed in diffusion models (Song and Ermon, 2019; Ho et al., 2020) such that different conditional layers share the same parameters \(\phi\). Note that sequential training is not suitable in this setting. Therefore, we propose a joint training procedure that minimizes a weighted sum of the SIVI objectives

\[\mathcal{L}_{\text{HSIVI-}f}(\phi)=\sum_{t=0}^{T-1}\beta(t)\mathcal{L}_{\text{ SIVI-}f}\left(p_{t}(\mathbf{x}_{t})\|q_{t}(\mathbf{x}_{t};\phi)\right), \tag{10}\]

where \(\beta(t):\{0,\ldots,T-1\}\rightarrow\mathbb{R}_{+}\) is a positive weighting function and \(f\) is some distance criterion. See Algorithm 2 in Appendix C.3 for more details of joint training.

More specifically, in this work, we mainly focus on building the diffusion bridge with variance preserving SDE (VP-SDE) (Song et al., 2020b) such that \(\mathbf{u}_{s}|\mathbf{u}_{0}\sim\mathcal{N}(\sqrt{\alpha(s)}\mathbf{u}_{0},(1-\alpha(s)) \mathbf{I})\) with a decreasing function \(\alpha(s)\) of \(s\). We use \(\mathcal{L}_{\text{HSIVI-SM}}\) in equation (10) for training and set the weighting function \(\beta(t)=1-\alpha(s_{t})\) as recommended in Song et al. (2020b), which tends to train layers that are far from \(t=0\) first during the training, resembling the sequential training. Another popular formulation of diffusion models is to fit a noise model \(\mathbf{\epsilon}^{*}(\mathbf{x},s)\) that predicts the noise added to a noisy sample \(\mathbf{x}\) at time \(s\)(Ho et al., 2020). HSIVI-SM also generalizes to the case where a pre-trained noise model is available. The pre-trained noise model forms a (generalized) diffusion bridge by letting \(\mathbf{\epsilon}^{*}_{t}(\mathbf{x})=\mathbf{\epsilon}^{*}(\mathbf{x},s_{t})\), and we call the corresponding training method "\(\epsilon\)-training". We provide a reparametrized objective function \(\tilde{\mathcal{L}}_{\text{HSIVI-SM}}\) for \(\epsilon\)-training in Appendix C.4.

Several efforts have been made to accelerate the sampling process of diffusion models, including faster numerical ordinary differential equation (ODE) solvers (Song et al., 2020a; Zhang and Chen, 2022; Lu et al., 2022) and distillation techniques (Luhman and Luhman, 2021; Salimans and Ho, 2022; Zheng et al., 2022). Our approach is different from these previous efforts in that we accelerate the stochastic diffusion model directly (hence would provide more diverse samples (Figure 6)) and do not require sampling datasets from the diffusion models prior to distillation which is computationally expensive. From a Bayesian perspective, HSIVI is related to Song and Ermon (2019), where the authors used the annealed Langevin dynamics guided by a pre-trained score model to sample from the data distribution. By solving this problem using a variational inference approach, HSIVI enjoys faster sampling speed and scales better to high-dimensional data.

## 5 Experiments

In this section, we first compare HSIVI to its single-layer counterpart, SIVI, on two inference tasks. We use the sequential training method where each conditional layer in the hierarchical semi-implicit variational distributions has independent parameters. We then apply HSIVI-SM to diffusion model acceleration on various datasets. As the memory consumption for generative models is large, we use the joint training method where the conditional layers in hierarchical semi-implicit distributions have shared parameters across different \(t\)s. For all experiments, each conditional layer is modeled as a Gaussian distribution with parametrized mean and variance. More details of the model architectures and hyper-parameters are included in Appendix E. The code is available at [https://github.com/longinYu/HSIVI](https://github.com/longinYu/HSIVI).

### Target distribution approximation

Gaussian mixture modelWe first evaluate HSIVI and SIVI on a two-dimensional Gaussian mixture model. The target distribution \(p(\mathbf{x})\) takes the form \(p(\mathbf{x})=\sum_{i=1}^{8}1/8\cdot\mathcal{N}(\mathbf{x};\mathbf{\mu}_{i},\sigma^{2}\mathbf{ I})\) where \(\mathbf{\mu}_{i}=[10\cos(\frac{i\pi}{4}),10\sin(\frac{i\pi}{4})]^{T}\), \(\sigma=1\). For HSIVI, we construct an auxiliary bridge of \(T=5\) with geometric interpolation in Example 1, where \(p_{\text{base}}(\mathbf{x})=\mathcal{N}(\mathbf{x};\mathbf{0},\mathbf{I})\) and \(\lambda_{t}=1-t/5\). The results are presented in Figure 2. Note that the modes in this Gaussian mixture model are far apart from each other, and both SIVI-LB and SIVI-SM are trapped in local modes. In contrast, both HSIVI-LB and HSIVI-SM discover all modes and provide an accurate approximation of the target distribution with HSIVI-SM being better for recovering the right scale of variance.

High-dimensional conditioned diffusionThe second example is a high-dimensional Bayesian inference problem arising from the following Langevin SDE

\[\mathrm{d}x_{s}=10x_{s}(1-x_{s}^{2})\mathrm{d}s+\mathrm{d}w_{s}, \tag{11}\]

where \(x_{0}=0\) and \(w_{s}\) is a one-dimensional standard Brownian motion. This system describes the motion of a particle with negligible mass trapped in an energy potential with thermal fluctuations represented by the Brownian forcing (Cui et al., 2016). Using an Euler-Maruyama scheme with step size \(\Delta s=0.01\) on a time interval \([0,3]\), we discretize the SDE (11) into \(\mathbf{x}=(x_{d_{1}},\ldots,x_{d_{000}})\) where \(d_{i}=0.01i\), which gives the prior distribution \(p_{\text{prior}}(\mathbf{x})\) of the 300-dimensional variable \(\mathbf{x}\). The noisy observations \(\mathbf{y}\) is obtained by \(\mathbf{y}=\mathbf{x}+\mathbf{\xi}\), where \(\mathbf{\xi}\sim\mathcal{N}(\mathbf{0},\sigma^{2}\mathbf{I})\) with \(\sigma=0.1\). Our goal is to infer the posterior distribution of the latent states \(p(\mathbf{x}|\mathbf{y})\propto p_{\text{prior}}(\mathbf{x})p(\mathbf{y}|\mathbf{x})\). The ground truth is formed by running 100,000 independent stochastic gradient Langevin dynamics (SGLD) chains with a step size of 0.0001 and collecting the results after 10,000 iterations.

For HSIVI, we form the auxiliary bridge using geometric interpolation with \(p_{\text{base}}(\mathbf{x})=\mathcal{N}(\mathbf{x};\mathbf{y},\sigma^{2}\mathbf{I})\) and \(\lambda_{t}=1-t/(T-1)\) for \(t=0,\ldots,T-1\). Figure 3 shows the estimated posteriors obtained by different methods. We see that SIVI-SM severely underestimates the variance. With \(T=5\) layers, HSIVI-SM fits the variance better and hence provides more accurate posterior estimates. For both HSIVI-SM and HSIVI-LB, the estimated covariance matrix becomes more accurate as \(T\) increases (Table 4 in Appendix D.2), demonstrating the effectiveness of hierarchical models for fitting complicated distributions.

Figure 3: The posterior estimates obtained by different methods. For each method, we collect 100,000 samples to calculate the sample mean and confidence interval.

Figure 2: Comparison of 10,000 generated samples from SIVI and 5-layer HSIVI on a two-dimensional Gaussian mixture model (blue).

### Diffusion model acceleration

2D toy examplesIn this toy model example, we test four synthetic 2D datasets: Checkerboard, Circles, Moons, and Swissroll (Pedregosa et al., 2011). We first pre-train the score model \(\mathbf{S}^{*}(\mathbf{x},s)\) for \(s\in[0,1]\) with quadratic noise schedule \(1-\alpha(s)=s^{2}\). For constructing the \(T\)-layer diffusion bridge, we select \(\{s_{t}\}_{t=0}^{T-1}\) so that \(1-\alpha(s_{t})=[0.01+(\sqrt{0.8}-0.01)t/T]^{2}\). Figure 4 shows the sample trajectories (\(\mathbf{x}_{9}\), \(\mathbf{x}_{7}\), \(\mathbf{x}_{5}\) and \(\mathbf{x}_{0}\)) progressively generated from 10-layer HSIVI-SM. We see clearly how the semi-implicit distributions are guided towards the target distribution and all modes are discovered. We also report the Jensen-Shannon (JS) divergence between the target distributions and the estimated distributions in Table 1. We see that HSIVI-SM significantly improves upon DDIM and DDPM in both cases with 5 and 10 steps. Also, 10-layer HSIVI-SM is comparable to DDPM with 1000 full steps. See Figure 10 in Appendix D.3 for visualization of samples from different methods.

MnistOn MNIST, we use the noise model \(\mathbf{\epsilon}^{*}(\mathbf{x},s)\) instead of the score model and use \(\epsilon\)-training to train HSIVI-SM. The structure of \(\mathbf{\epsilon}^{*}(\mathbf{x},s)\) follows the UNet in Ho et al. (2020) by reducing the number of input and output channels to one. With the same noise schedule employed in Song et al. (2020), we first pre-train the noise model \(\mathbf{\epsilon}^{*}(\mathbf{x},s)\) with 1000 discretization steps and then form the \(T\)-layer diffusion bridge for HSIVI-SM by selecting \(T\) discrete time steps. Figure 5 shows the samples from DDPM, DDIM, and HSIVI-SM with \(T=5\) steps. We see that the samples produced by HSIVI-SM are much cleaner and more recognizable than those produced by DDPM and DDIM.

CIFAR-10, CelebA & ImageNetOn both CIFAR-10 and CelebA, the structure of our pre-trained noise model \(\mathbf{\epsilon}^{*}(\mathbf{x},s)\) follows the UNet structure(Ronneberger et al., 2015) employed by Ho et al. (2020), instead of the huge VP deep continuous-time model (Song et al., 2020) that has more channels and layers. We also provide additional results on ImageNet (64\(\times\)64) with more powerful pre-trained score nets in (Nichol and Dhariwal, 2021)(bigger models with more parameters). Since this generative modeling has been formulated as a score-based VI problem, we do not have to use any training data for training HSIVI-SM.

Following the noise schedule employed in Song et al. (2020), we first pre-train the noise model \(\mathbf{\epsilon}^{*}(\mathbf{x},s)\) with 1000 discretization steps and then form the \(T\)-layer diffusion bridge for HSIVI-SM

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{Name} & \multicolumn{4}{c}{\(T=5\)} & \multicolumn{4}{c}{\(T=10\)} & \multicolumn{2}{c}{\(T=1000\)} \\ \cline{2-9}  & DDPM & DDIM & HSIVI-SM & DDPM & DDIM & HSIVI-SM & DDPM \\ \hline Checkerboard & 0.891 & 0.591 & **0.068\(\pm\)0.006** & 0.521 & 0.373 & **0.030\(\pm\)0.005** & 0.058 \\ Swissroll & 1.037 & 0.332 & **0.126\(\pm\)0.006** & 0.334 & 0.164 & **0.082\(\pm\)0.003** & 0.042 \\ Circles & 0.907 & 0.397 & **0.083\(\pm\)0.015** & 0.364 & 0.201 & **0.073\(\pm\)0.005** & 0.032 \\ Moons & 0.961 & 0.355 & **0.096\(\pm\)0.013** & 0.352 & 0.137 & **0.059\(\pm\)0.007** & 0.036 \\ \hline \hline \end{tabular}
\end{table}
Table 1: JS divergences between the target distribution and the variational approximation on the four toy datasets. The results of HSIVI-SM are averaged by 5 independent runs with standard deviation in the subscripts. JS divergences are calculated by the ITE package (Szabo, 2014) with 10,000 samples.

Figure 4: Sample trajectories generated from 10-layer HSIVI-SM on four 2D toy examples. The arrows represent the estimated score function in HSIVI-SM. The sample size is 10,000.

by selecting \(T\) discrete time steps as before. For HSIVI-SM with \(\epsilon\)-training, the conditional layer \(q_{t}(\cdot|\mathbf{x}_{t+1};\phi)\) is modeled as a Gaussian distribution with mean \(\mathbf{\mu}_{t}(\mathbf{x}_{t+1};\phi^{\mu})\) and diagonal variance matrix \(\mathbf{\Sigma}_{t}(\phi^{\sigma})\) where \(\{\phi^{\mu},\phi^{\sigma}\}=\phi\) are the variational parameters. In our implementations, both \(\mathbf{\mu}_{t}(\mathbf{x}_{t+1};\phi^{\mu})\) and \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\) use the same architecture as \(\mathbf{\epsilon}^{*}(\mathbf{x},s)\). The number of layers, which is also the number of function evaluations (NFE), is set to be \(T=5,10,15\) in our experiments. We train HSIVI-SM with the same setting for \(T=10,15\). The 5-layer HSIVI-SM is trained by further fine-tuning the well-trained 15-layer HSIVI-SM and we find this strategy leads to better results. During each nested training loop of \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\), we update \(\psi\) 20 times before each update of \(\phi\), since we find \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\) needs more training empirically to provide reliable guidance.

For each method, we draw 50,000 samples and use the Frechet inception distance (FID) score (Karras et al., 2022) to evaluate the sample quality (Table 2). We find that HSIVI-SM performs on par or better than the other baselines on both CIFAR-10 and CelebA, and the advantage is evident when the NFE is small. The sampling trajectories of 10-layer HSIVI-SM on CelebA with the same starting point but different random seeds are shown in Figure 6. We see that HSIVI-SM is capable of producing more diverse samples due to its stochastic nature, which is different from existing ODE based fast diffusion model samplers.

### Additional Study

Ablation of layers numberIn Figure 7, We provide a failure case on fitting the checkerboard target with diffusion bridge, demonstrating that the HSIVI-SM algorithm fails when the layer number \(T\) is small (the distances of auxiliary distributions at successive time steps are large) on a checkerboard

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline Dataset & \multicolumn{4}{c}{CIFAR-10 (32\(\times\)32)} & \multicolumn{4}{c}{CelebA (64\(\times\)64)} & \multicolumn{4}{c}{ImageNet (64\(\times\)64)} \\ \cline{2-10} NFE & 5 & 10 & 15 & 5 & 10 & 15 & 5 & 10 & 15 \\ \hline DDPM (Ho et al., 2020) & 320.16 & 278.65 & 198.00 & 366.10 & 309.95 & 206.92 & 402.68 & 358.80 & 284.00 \\ DDIM (Song et al., 2020a) & 41.53 & 13.73 & 8.78 & 27.38 & 10.89 & 7.78 & 147.03 & 42.31 & 24.85 \\ FastDPM (Kong \& Ping, 2021) & 67.64 & 9.85 & 6.16 & 27.63 & 15.44 & 12.05 & N/A & N/A & N/A \\ Analytic-DDFM (Bao et al., 2022) & 93.16 & 34.54 & 20.03 & 50.92 & 28.93 & 21.84 & N/A & 60.65 & 45.98 \\ Analytic-DDM (Bao et al., 2022) & 51.86 & 14.08 & 8.65 & 29.40 & 15.74 & 12.25 & N/A & 70.62 & 41.56 \\ DPM-Solver-fast (Lu et al., 2022) & 329.13 & 10.89 & 4.67 & 355.96 & 6.76 & 2.98 & 402.43 & 28.96 & 20.03 \\
**HSIVI-SM (ours)** & **6.27** & **4.31** & **4.17** & **6.22** & **3.09** & **2.23** & **40.43** & **17.67** & **15.49** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Sample quality measured by FID (\(\downarrow\)) on CIFAR-10, CelebA and ImageNet with a varying number of function evaluations (NFE). Results of baselines are calculated by running their official codes, where the architectures of score model (or noise model) are the UNet employed in Ho et al. (2020) for CIFAR-10 and CelebA and (Nichol & Dhariwal, 2021) in ImageNet.

Figure 5: Comparison of the quality of uncurated samples generated by DDPM, DDIM, and HSIVI-SM with 5 discrete time steps on MNIST.

Figure 6: Sample trajectories of 10-layer HSIVI-SM with the same starting point \(\mathbf{x}_{10}\) on CelebA.

distribution. In fact, the score function on the checkerboard target is sharp on the boundaries but vanishes elsewhere. Therefore, fitting this target distribution is somewhat challenging.

Ablation of the variational familyTo validate the improvement of HSIVI-SM on diffusion models, we train HSIVI-SM with isotropic conditional layers in consistency with denoising-diffusion sampling, like DDPM and DDIM. We report the results of FID on the CIFAR-10 dataset in Table 3. These results provide further evidence for the statement outlined in Section 3.2. HSIVI-SM matches the marginal distributions \(q_{t}(\mathbf{x}_{t})\) and \(p_{t}(\mathbf{x}_{t})\) directly via score matching and would ensure a better fit for \(p_{0}(\mathbf{x}_{0})\). The enhancement of HSIVI-SM over DDPM stems not only from its more expressive variational distribution but also from the direct alignment of the marginal distributions.

## 6 Conclusions

We introduced HSIVI, a hierarchical semi-implicit variational inference method that enables more expressive multi-layer construction of semi-implicit distributions. Given appropriate auxiliary distributions that interpolate between a simple base distribution and the target distribution, the conditional layers in hierarchical semi-implicit distributions can be progressively trained one layer after another. In experiments, we showed that HSIVI outperforms previous single-layer SIVI methods on several Bayesian inference tasks with complicated posteriors. HSIVI can also be used to accelerate the sampling process of diffusion models, where pre-trained score networks serve as a natural sequence of bridging distributions, which allows for direct acceleration of the stochastic diffusion model and does not require expensive sampling from the diffusion models during training. We showed that HSIVI can produce high quality samples comparable to or better than existing fast diffusion model samplers with few function evaluations on various datasets. Limitations are discussed in Appendix F.

## Acknowledgements

This work was supported by National Natural Science Foundation of China (grant no. 12201014 and grant no. 12292983). The research of Cheng Zhang was supported in part by National Engineering Laboratory for Big Data Analysis and Applications, the Key Laboratory of Mathematics and Its Applications (LMAM) and the Key Laboratory of Mathematical Economics and Quantitative Finance (LMEQF) of Peking University. The authors appreciate the anonymous NeurIPS reviewers for their constructive feedback.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline NFE & DDPM & DDIM & HSIVI-SM (isotropic) & HSIVI-SM (non-isotropic) \\ \hline
5 & 320.16 & 41.53 & 7.33 & **6.27** \\
10 & 278.65 & 13.73 & 4.78 & **4.31** \\
15 & 198.00 & 8.78 & 4.46 & **4.17** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of non-isotropic conditional layers and isotropic conditional layers on CIFAR10, the sample quality is measured by FID (\(\downarrow\)).

Figure 7: Failure cases of HSIVI-SM. The quivers show the estimated score by the \(f\) function. \(T\) is the layers number of HSIVI-SM. The generated samples in orange show that smaller \(T\) may fail on this example.

## References

* Austin et al. (2021) Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_, 34:17981-17993, 2021.
* Bao et al. (2022) Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. _arXiv preprint arXiv:2201.06503_, 2022.
* Bernton et al. (2019) Espen Bernton, Jeremy Heng, Arnaud Doucet, and Pierre E. Jacob. Schrodinger bridge samplers. _arXiv preprint arXiv:1912.13170_, 2019.
* Bishop and Tipping (2000) Christopher M. Bishop and Michael E Tipping. Variational relevance vector machines. In _Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence_, pp. 46-53, 2000.
* 877, 2016.
* Cui et al. (2016) Tiangang Cui, Kody JH Law, and Youssef M Marzouk. Dimension-independent likelihood-informed mcmc. _Journal of Computational Physics_, 304:109-137, 2016.
* Dinh et al. (2017) Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In _International Conference on Learning Representations_, 2017.
* Dockhorn et al. (2022) Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-based generative modeling with critically-damped langevin diffusion. In _International Conference on Learning Representations (ICLR)_, 2022.
* Giordano et al. (2015) R. J. Giordano, T. Broderick, and M. I. Jordan. Linear response methods for accurate covariance estimates from mean field variational bayes. In _Advances in Neural Information Processing Systems_, 2015.
* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* Hu et al. (2018) Tianyang Hu, Zixiang Chen, Hanxi Sun, Jincheng Bai, Mao Ye, and Guang Cheng. Stein neural sampler. _arXiv preprint arXiv:1810.03545_, 2018.
* Huszar (2017) Ferenc Huszar. Variational inference using implicit distributions. _arXiv preprint arXiv: 1702.08235_, 2017.
* Hyvarinen (2005) Aapo Hyvarinen. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4):695-709, 2005. URL [http://jmlr.org/papers/v6/hyvarinen05a.html](http://jmlr.org/papers/v6/hyvarinen05a.html).
* Jaakkola and Jordan (1998) T. S. Jaakkola and M. I. Jordan. Improving the mean field approximation via the use of mixture distributions. In _Learning in Graphical Models_, pp. 173-173, 1998.
* Jordan et al. (1999) Michael I. Jordan, Zoubin Ghahramani, T. Jaakkola, and Lawrence K. Saul. An introduction to variational methods for graphical models. _Machine Learning_, 37:183-233, 1999.
* Karras et al. (2022) Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In _Proc. NeurIPS_, 2022.
* Kingma and Ba (2015) D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.
* Kingma and Welling (2014) D. P. Kingma and M. Welling. Auto-encoding variational bayes. In _ICLR_, 2014.
* Kingma et al. (2021) Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. _Advances in neural information processing systems_, 34:21696-21707, 2021.
* Kingma et al. (2016) Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In _Advances in Neural Information Processing Systems_, pp. 4743-4751, 2016.
* Kingma et al. (2016)Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. _arXiv preprint arXiv:2106.00132_, 2021.
* Liu et al. (2016) Qiang Liu, Jason Lee, and Michael Jordan. A kernelized stein discrepancy for goodness-of-fit tests. In _International conference on machine learning_, pp. 276-284. PMLR, 2016.
* Lu et al. (2022) Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _arXiv preprint arXiv:2206.00927_, 2022.
* Luhman and Luhman (2021) Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. _arXiv preprint arXiv:2101.02388_, 2021.
* Mescheder et al. (2017) L. M. Mescheder, S. Nowozin, and A. Geiger. Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks. In _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, 2017.
* Moens et al. (2021) V. Moens, H. Ren, A. Maraval, R. Tutunov, J. Wang, and H. Ammar. Efficient semi-implicit variational inference. _arXiv preprint arXiv:2101.06070_, 2021.
* Neal (2011) Radford Neal. MCMC using hamiltonian dynamics. In S Brooks, A Gelman, G Jones, and XL Meng (eds.), _Handbook of Markov Chain Monte Carlo_, Chapman & Hall/CRC Handbooks of Modern Statistical Methods. Taylor & Francis, 2011. ISBN 9781420079425. URL [http://books.google.com/books?id=qfRsAIKZ4r1C](http://books.google.com/books?id=qfRsAIKZ4r1C).
* Neal (2001) Radford M Neal. Annealed importance sampling. _Statistics and computing_, 11:125-139, 2001.
* Nichol and Dhariwal (2021) Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models, 2021.
* Niu et al. (2020) Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation invariant graph generation via score-based generative modeling. In _International Conference on Artificial Intelligence and Statistics_, 2020.
* Nott et al. (2012) D. J. Nott, S. L. Tan, M. Villani, and R. Kohn. Regression density estimation with variational methods and stochastic approximation. _Journal of Computational and Graphical Statistics_, 21(3):797-820, 2012.
* Paisley et al. (2012) J. W. Paisley, D. M. Blei, and M. I. Jordan. Variational bayesian inference with stochastic search. In _Proceedings of the 29th International Conference on Machine Learning ICML_, 2012.
* Papamakarios et al. (2019) G. Papamakarios, E. Nalisnick, D. Rezende, S. Mohamed, and B. Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. _ArXiv Preprint arXiv:1912.02762_, 2019.
* Pedregosa et al. (2011) Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. _the Journal of machine Learning research_, 12:2825-2830, 2011.
* Ranganath et al. (2014) R. Ranganath, S. Gerrish, and D. M. Blei. Black box variational inference. In _AISTATS_, pp. 814-822, 2014.
* Ranganath et al. (2016) Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In _International conference on machine learning_, pp. 324-333. PMLR, 2016.
* Rezende and Mohamed (2015) D. Rezende and S. Mohamed. Variational inference with normalizing flows. In _Proceedings of The 32nd International Conference on Machine Learning_, pp. 1530-1538, 2015.
* Rezende et al. (2014) D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In _International Conference on Machine Learning_, 2014.
* Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _International Conference on Medical image computing and computerassisted intervention_. Springer, 2015.
* Salimans and Ho (2022) Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. _ArXiv_, abs/2202.00512, 2022.
* Salimans et al. (2016)L. K. Saul and M. I. Jordan. Exploiting tractable substructures in intractable networks. In _Advances in Neural Information Processing Systems_, 1996.
* Shi et al. (2018a) J. Shi, S. Sun, and J. Zhu. Kernel implicit variational inference. In _International Conference on Learning Representations_, 2018a.
* Shi et al. (2018b) J. Shi, S. Sun, and J. Zhu. A spectral approach to gradient estimation for implicit distributions. In _International Conference on Machine Learning_, 2018b.
* Sobolev and Vetrov (2019) Artem Sobolev and Dmitry P. Vetrov. Importance weighted hierarchical variational inference. In _Neural Information Processing Systems_, 2019.
* Song et al. (2020a) Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020a.
* Song et al. (2019) Y. Song, S. Garg, J. Shi, and S. Ermon. Sliced score matching: A scalable approach to density and score estimation. In _Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence_, 2019.
* Song and Ermon (2019) Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.
* Song et al. (2020b) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020b.
* Sugiyama et al. (2012) Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. _Density ratio estimation in machine learning_. Cambridge University Press, 2012.
* Szabo (2014) Zoltan Szabo. Information theoretical estimators toolbox. _Journal of Machine Learning Research_, 15:283-287, 2014.
* Titsias and Ruiz (2019) Michalis K. Titsias and Francisco J. R. Ruiz. Unbiased implicit variational inference. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pp. 167-176. PMLR, 2019.
* Tran et al. (2015) D. Tran, D. M. Blei, and E. M. Airoldi. Copula variational inference. In _Advances in Neural Information Processing Systems_, 2015.
* Tran et al. (2017) D. Tran, R. Ranganath, and D. M. Blei. Hierarchical implicit models and likelihood-free variational inference. In _Advances in Neural Information Processing Systems_, 2017.
* Vahdat and Kautz (2020) Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In _Neural Information Processing Systems (NeurIPS)_, 2020.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Vincent (2011) Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural Computation_, 23(7):1661-1674, 2011. doi: 10.1162/NECO_a_00142.
* Wainwright and Jordan (2008) M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. _Foundations and Trends in Machine Learning_, 1(1-2):1-305, 2008.
* Yin and Zhou (2018) Mingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In _International Conference on Machine Learning_, pp. 5646-5655, 2018.
* Yu and Zhang (2023) Longlin Yu and Cheng Zhang. Semi-implicit variational inference via score matching. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=sd90a2ytrt](https://openreview.net/forum?id=sd90a2ytrt).
* Zhang et al. (2018) C. Zhang, B. Shahbaba, and H. Zhao. Variational hamiltonian monte carlo via score matching. _Bayesian Analysis_, 13(2):485-506, 2018.
* Zhang et al. (2018)Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In _NeurIPS 2022 Workshop on Score-Based Methods_, 2022.
* Zheng et al. (2022) Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. In _NeurIPS 2022 Workshop on Score-Based Methods_, 2022.

Details of diffusion models

Diffusion models work by adding noise to the training data in the forward process and then removing the noise to recover the data in the backward process, which can be integrated into a general stochastic differential equation (SDE) framework (Song et al., 2020). The forward process \(\{\mathbf{u}_{s}\}_{s\in[0,L]}\) is usually described by the SDE

\[\mathrm{d}\mathbf{u}_{s}=\mathbf{f}(\mathbf{u}_{s},s)\mathrm{d}s+g(s)\mathrm{d}\mathbf{w}_{s}, \quad\mathbf{u}_{0}\sim p_{0}(\cdot),\]

where \(p_{0}(\cdot)\) is the data distribution, \(\mathbf{w}_{s}\) is a standard Brownian motion, \(\mathbf{f}(\mathbf{u}_{s},s)\) and \(g(s)\) are the drift and diffusion coefficient respectively. To generate samples from the data distribution, one can run the following reversed SDE

\[\mathrm{d}\mathbf{u}_{s}=[\mathbf{f}(\mathbf{u}_{s},s)-g^{2}(s)\nabla_{\mathbf{u}_{s}}\log p_{ s}(\mathbf{u}_{s})]\mathrm{d}s+g(s)\mathrm{d}\tilde{\mathbf{w}}_{s},\quad\mathbf{u}_{L} \sim p_{L}(\cdot),\]

where \(p_{s}(\cdot)\) is the probability density function (pdf) of \(\mathbf{u}_{s}\) and \(\tilde{\mathbf{w}}_{s}\) is a standard Brownian motion when time flows from \(L\) to \(0\). There exists deterministic process shares the same marginal probability densities \(\{p_{s}(\cdot)\}_{s\in[0,L]}\) described by the following ordinary differential equation (ODE)

\[\mathrm{d}\mathbf{u}_{s}=[\mathbf{f}(\mathbf{u}_{s},s)-\frac{1}{2}g^{2}(s)\nabla_{\mathbf{u}_{ s}}\log p_{s}(\mathbf{u}_{s})]\mathrm{d}s,\quad\mathbf{u}_{L}\sim p_{L}(\cdot),\]

called probability flow (PF) ODE.

In practice, Song et al. (2020) and Kingma et al. (2021) designed several examples of the forward process such that it diffuses the data distribution \(p_{0}(\cdot)\) to a fixed unstructured distribution \(p_{L}(\cdot)\). Here we mainly consider the Variance Preserving SDE (VP-SDE) used in DDPM (Ho et al., 2020; Song et al., 2020). Let the drift coefficient \(\mathbf{f}(\mathbf{u}_{s},s)=\frac{\mathrm{d}\log\alpha(s)}{2\mathrm{d}\mathbf{s}}\mathbf{u}_{s}\) and the diffusion coefficient \(g^{2}(s)=-\frac{\mathrm{d}\log\alpha(s)}{\mathrm{d}s}\), where \(\alpha(s)\in\mathbb{R}^{+}\) is a decreasing smooth function with \(\alpha(0)=1,\alpha(L)\approx 0\). Then the distribution of \(\mathbf{u}_{s}\) conditioned on \(\mathbf{u}_{0}\) is explicit as

\[\mathbf{u}_{s}|\mathbf{u}_{0}\sim\mathcal{N}\left(\sqrt{\alpha(s)}\mathbf{\bar{x}}_{0},(1- \alpha(s))\mathbf{I}\right),\text{ i.e. }\mathbf{u}_{s}=\sqrt{\alpha(s)}\mathbf{u}_{0}+ \sqrt{1-\alpha(s)}\mathbf{\epsilon}, \tag{12}\]

where \(\mathbf{\epsilon}\) is a standard Gaussian noise. In practice, diffusion models use a neural network \(\mathbf{S}_{\theta}(\mathbf{u}_{s},s)\) to approximate the score function \(\mathbf{S}_{\theta}(\mathbf{u}_{s},s)\) by optimizing the denoising score matching objective (Vincent, 2011)

\[\mathcal{L}_{\text{dsm}}(\theta,\omega(s)):=\frac{1}{2}\int_{0}^{L}\omega(s) \mathbb{E}_{\mathbf{u}_{0}\sim p_{0}(\mathbf{u}_{0}),\mathbf{\epsilon}\sim\mathcal{N}(0, \mathbf{I})}\left\|\mathbf{S}_{\theta}(\mathbf{u}_{s},s)+\mathbf{\epsilon}/\sqrt{1-\alpha (s)}\right\|_{2}^{2}\mathrm{d}s, \tag{13}\]

where \(\omega(s)\) is a positive weighting function. Instead of modeling the score function, Ho et al. (2020) proposed to predict the conditional noise \(\mathbf{\epsilon}\) based on \(\mathbf{u}_{t}\). This leads to the following DDPM loss

\[\mathcal{L}_{\text{ddpm}}(\theta,\bar{\omega}(s)):=\frac{1}{2}\int_{0}^{L} \bar{\omega}(s)\mathbb{E}_{\mathbf{u}_{0}\sim p_{0}(\mathbf{u}_{0}),\mathbf{\epsilon}\sim \mathcal{N}(0,\mathbf{I})}\|\mathbf{\epsilon}_{\theta}(\mathbf{u}_{s},s)-\mathbf{\epsilon} \|_{2}^{2}\mathrm{d}s, \tag{14}\]

where \(\bar{\omega}(s)\) is a positive weighting function. In fact, we have the relationship

\[\mathbf{S}_{\theta}(\mathbf{u}_{s},s)=-\mathbf{\epsilon}_{\theta}(\mathbf{u}_{s},s)/\sqrt{1- \alpha(s)}. \tag{15}\]

We call \(\mathcal{L}_{\text{dsm}}\) "score-prediction" training and \(\mathcal{L}_{\text{ddpm}}\) "\(\epsilon\)-prediction" training.

With the pre-trained score model \(\mathbf{S}_{\theta}(\mathbf{u}_{s},s)\) or noise model \(\mathbf{\epsilon}_{\theta}(\mathbf{u}_{s},s)\), Song et al. (2020) shows that the samples of \(p_{0}(\cdot)\) can be generated by simulating the backward SDE, e.g. the sampling scheme of DDPM (Ho et al., 2020). Moreover, Bao et al. (2022) proposed Analytic-DPM, the optimal discretization form responding to the KL divergence of the joint distribution on the discrete time steps. Also, several high-order ODE solvers (Song et al., 2020; Zhang and Chen, 2022; Lu et al., 2022) were proposed to achieve faster sampling.

## Appendix B More details of UIVI

Unlike optimizing the surrogate ELBO, Titsias and Ruiz (2019) proposed unbiased implicit variational inference (UIVI) which relies on an unbiased gradient estimator for the exact ELBO. To elaborate further, reparametrize the conditional \(q_{\phi}(\mathbf{x}|\mathbf{z})\) such as \(\mathbf{x}=T_{\phi}(\mathbf{z},\mathbf{\epsilon}),\mathbf{\epsilon}\sim q_{\mathbf{\epsilon}}(\mathbf{ \epsilon})\), then

\[\nabla_{\phi}\text{ELBO} =\nabla_{\phi}\mathbb{E}_{\mathbf{\epsilon}\sim q_{\mathbf{\epsilon}}(\bm {\epsilon}),\mathbf{z}\sim q(\mathbf{z})}\left[\log p(D,\mathbf{x})-\log q_{\phi}(\mathbf{x}) \right]_{\mathbf{x}=T_{\phi}(\mathbf{z},\mathbf{\epsilon})}\] \[=\mathbb{E}_{\mathbf{\epsilon}\sim q_{\mathbf{\epsilon}}(\mathbf{\epsilon}), \mathbf{z}\sim q(\mathbf{z})}\left[g_{\phi}^{\text{mod}}(\mathbf{z},\mathbf{\epsilon})+g_{\phi} ^{\text{ent}}(\mathbf{z},\mathbf{\epsilon})\right],\]where

\[g_{\phi}^{\text{mod}}(\mathbf{z},\mathbf{\epsilon}) :=\left.\nabla_{\mathbf{x}}\log p(D,\mathbf{x})\right|_{\mathbf{x}=T_{\phi}(\mathbf{ z},\mathbf{\epsilon})}\nabla_{\phi}T_{\phi}(\mathbf{z},\mathbf{\epsilon}),\] \[g_{\phi}^{\text{ent}}(\mathbf{z},\mathbf{\epsilon}) :=-\left.\mathbb{E}_{q_{\phi}(\mathbf{z}^{\prime}|\mathbf{x})}\nabla_{\mathbf{ x}}\log q_{\phi}(\mathbf{x}|\mathbf{z}^{\prime})\right|_{\mathbf{x}=T_{\phi}(\mathbf{z},\mathbf{ \epsilon})}\nabla_{\phi}T_{\phi}(\mathbf{z},\mathbf{\epsilon}). \tag{16}\]

The second gradient term \(g_{\phi}^{\text{ent}}\) involves an expectation w.r.t. the reverse conditional \(q_{\phi}(\mathbf{z}^{\prime}|\mathbf{x})\) which is estimated by an MCMC sampler in UIVI. However, the inner-loop MCMC runs may require long iterations for convergence.

## Appendix C More details of HSIVI

### Score-based training of HSIVI-LB

In the sequential training of HSIVI-LB, although the objective \(\mathcal{L}_{\text{SIVI-LB}}\left(p_{t}(\mathbf{x}_{t})\|q_{t}(\mathbf{x}_{t};\phi_{ \geq t})\right)\) is calculated based on \(p_{t}(\mathbf{x})\), the gradient of it w.r.t. \(\phi_{t}\) has a closed form containing only the score function \(\mathbf{S}_{t}(\mathbf{x})\) without knowing the corresponding pdfs. This derivation is important in the tasks where score functions of the auxiliary distributions are tractable while pdfs (up to a constant) of them are unavailable (for example, the diffusion bridge in Example 2). Concretely, assume the \(t\)-th conditional layer \(q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})\) is induced by a parametrized transform \(\mathbf{x}_{t}=\mathbf{h}_{t}(\mathbf{x}_{t+1},\mathbf{\epsilon};\phi_{t})\) where \(\mathbf{\epsilon}\sim p_{\mathbf{\epsilon}}(\mathbf{\epsilon})\) is a random noise, since \(q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})\) is reparametrizable according to Definition 1. The only term in \(\mathcal{L}_{\text{SIVI-LB}}\left(p_{t}(\mathbf{x}_{t})\|q_{t}(\mathbf{x}_{t};\phi_{ \geq t})\right)\) containing \(p_{t}(\mathbf{x}_{t})\) is \(\mathbb{E}_{q_{t}(\mathbf{x}_{t};\phi_{\geq t})}\log p_{t}(\mathbf{x}_{t})\) (see equation (2)) whose gradient takes the form

\[\nabla_{\phi_{t}}\mathbb{E}_{q_{t}(\mathbf{x}_{t};\phi_{\geq t})}\log p _{t}(\mathbf{x}_{t}) =\nabla_{\phi_{t}}\mathbb{E}_{q_{t+1}(\mathbf{x}_{t+1};\phi_{\geq t+1} )p_{\mathbf{\epsilon}}(\mathbf{\epsilon})}\log p_{t}(\mathbf{h}_{t}(\mathbf{x}_{t+1},\mathbf{ \epsilon};\phi_{t}))\] \[=\mathbb{E}_{q_{t+1}(\mathbf{x}_{t+1};\phi_{\geq t+1})p_{\mathbf{ \epsilon}}(\mathbf{\epsilon})}\mathbf{S}_{t}\left(\mathbf{h}_{t}(\mathbf{x}_{t+1},\mathbf{\epsilon };\phi_{t})\right)\nabla_{\phi_{t}}\mathbf{h}_{t}(\mathbf{x}_{t+1},\mathbf{\epsilon};\phi _{t})\]

by the chain rule, where \(\nabla_{\phi_{t}}\mathbf{h}_{t}(\mathbf{x}_{t+1},\mathbf{\epsilon};\phi_{t})\) is the jacobian matrix of \(\mathbf{h}_{t}(\mathbf{x}_{t+1},\mathbf{\epsilon};\phi_{t})\).

In our implementation of HSIVI (in both sequential training and joint training), we generally assume the conditional layer \(q_{t}(\cdot|\mathbf{x}_{t+1};\phi_{t})\) is induced by

\[\mathbf{h}_{t}(\mathbf{x}_{t+1},\mathbf{\epsilon};\phi_{t})=\mathbf{\mu}_{t}(\mathbf{x}_{t+1};\phi_ {t})+\mathbf{\Sigma}_{t}^{1/2}(\mathbf{x}_{t+1};\phi_{t})\mathbf{\epsilon} \tag{17}\]

where \(\mathbf{\Sigma}_{t}(\mathbf{x}_{t+1};\phi_{t})\) is a positive definite covariance matrix and \(\mathbf{\epsilon}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\) is a standard multivariate gaussian variable. In equation (17), \(\phi_{t}\) should be replaced by \(\phi\) in the joint training case.

### Proof of Proposition 1

**Proposition 1**.: _Let \(q_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi_{\geq t})=q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi _{t})q_{t+1}(\mathbf{x}_{t+1};\phi_{\geq t+1})\). The minimax optimization of \(\mathcal{L}_{\text{SIVI-SM}}\left(p_{t}(\mathbf{x}_{t})\|q_{t}(\mathbf{x}_{t};\phi_{ \geq t})\right)\) is equivalent to_

\[\min_{\phi_{t}} \mathbb{E}_{q_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi_{\geq t})}\left[\bm {S}_{t}(\mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t};\psi_{t})\right]^{T}\left[\mathbf{S}_{t} (\mathbf{x}_{t})+\mathbf{g}_{t}(\mathbf{x}_{t};\psi_{t})-2\nabla_{\mathbf{x}_{t}}\log q_{t}( \mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})\right],\] \[\min_{\psi_{t}} \mathbb{E}_{q_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi_{\geq t})}\|\mathbf{g} _{t}(\mathbf{x}_{t};\psi_{t})-\nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1} ;\phi_{t})\|_{2}^{2}.\]

Proof of Proposition 1The minimax optimization problem of \(\mathcal{L}_{\text{SIVI-SM}}\left(p_{t}(\mathbf{x}_{t})\|q_{t}(\mathbf{x}_{t};\phi_{ \geq t})\right)\) is

\[\min_{\phi_{t}}\max_{\psi_{t}}\mathbb{E}_{q_{t}(\mathbf{x}_{t},\mathbf{x}_{t};\phi_{ \geq t})}\left[2\mathbf{f}_{t}(\mathbf{x}_{t};\psi_{t})^{T}[\mathbf{S}_{t}(\mathbf{x}_{t})- \nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})]-\|\mathbf{f}_{t}( \mathbf{x};\psi_{t})\|_{2}^{2}\right]\]

according to equation (4). For minimization w.r.t. \(\phi_{t}\), this target is equivalent to

\[\mathbb{E}_{q_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi_{\geq t})}\left[2 \mathbf{f}_{t}(\mathbf{x}_{t};\psi_{t})^{T}[\mathbf{S}_{t}(\mathbf{x}_{t})-\nabla_{\mathbf{x}_{t}} \log q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})]-\|\mathbf{f}_{t}(\mathbf{x};\psi_{t})\|_{2 }^{2}\right]\] \[= \mathbb{E}_{q_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi_{\geq t})}\mathbf{f}_{ t}(\mathbf{x}_{t};\psi_{t})^{T}[2\mathbf{S}_{t}(\mathbf{x}_{t})-\mathbf{f}_{t}(\mathbf{x}_{t};\psi_{t})-2 \nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})]\] \[= \mathbb{E}_{q_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi_{\geq t})}\big{[} \mathbf{S}_{t}(\mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t};\psi_{t})\big{]}^{T}[\mathbf{S}_{t}(\mathbf{x} _{t})+\mathbf{g}_{t}(\mathbf{x}_{t};\psi_{t})-2\nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t}| \mathbf{x}_{t+1};\phi_{t})].\]

For maximization w.r.t. \(\psi_{t}\), this target is equivalent to

\[\mathbb{E}_{q_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi_{\geq t})}\left[2 \mathbf{f}_{t}(\mathbf{x}_{t};\psi_{t})^{T}[\mathbf{S}_{t}(\mathbf{x}_{t})-\nabla_{\mathbf{x}_{t}} \log q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})]-\|\mathbf{f}_{t}(\mathbf{x};\psi_{t})\|_{2 }^{2}\right]\] \[= -\mathbb{E}_{q_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi_{\geq t})}\|\mathbf{f}_ {t}(\mathbf{x};\psi_{t})-\mathbf{S}_{t}(\mathbf{x}_{t})+\nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{ t}|\mathbf{x}_{t+1};\phi_{t})\|_{2}^{2}+C\] \[= -\mathbb{E}_{q_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi_{\geq t})}\|\mathbf{g}_ {t}(\mathbf{x};\psi_{t})-\nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1}; \phi_{t})\|_{2}^{2}+C,\]where \(C\) is a term that does not contain \(\psi_{t}\). Therefore, the minimax optimization problem is equivalent to

\[\min_{\phi_{t}} \mathbb{E}_{q_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi_{t})}\left[\mathbf{S}_{ t}(\mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t};\psi_{t})\right]^{T}\left[\mathbf{S}_{t}(\mathbf{x}_{ t})+\mathbf{g}_{t}(\mathbf{x}_{t};\psi_{t})-2\nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t}| \mathbf{x}_{t+1};\phi_{t})\right],\] \[\min_{\psi_{t}} \mathbb{E}_{q_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi_{t})}\|\mathbf{g}_{t}( \mathbf{x}_{t};\psi_{t})-\nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi _{t})\|_{2}^{2}.\]

### Joint training of HSIVI

As mentioned in Section 4.2, when parameter sharing scheme is used in the conditional layers for application to diffusion model acceleration, sequential training from \(t=T-1\) to \(t=0\) is not feasible. Therefore, we consider the following training objective

\[\mathcal{L}_{\text{HSIVI-}f}(\phi)=\sum_{t=0}^{T-1}\beta(t)\mathcal{L}_{\text{ SIVI-}f}\left(p_{t}(\mathbf{x}_{t})\|q_{t}(\mathbf{x}_{t};\phi)\right).\]

An intuitive method is to randomly sample a batch of time steps \(\{t_{k}\}_{k=1}^{K}\) and for each \(t_{k}\) train \(\mathcal{L}_{\text{SIVI-}f}\left(p_{t_{k}}(\mathbf{x}_{t_{k}})\|q_{t_{k}}(\mathbf{x}_{ t_{k}};\phi)\right)\) directly. However, sequentially sampling \(\mathbf{x}_{t_{k}}\) through \(q(\mathbf{x}_{i}|\mathbf{x}_{i+1};\phi)\) from \(i=T-1\) to \(i=t_{k}\) is still necessary in this case, making it memory-consuming to preserve the computation graphs of the entire sampling process.

In order to reduce the cost of accumulating computation graphs, for each \(t\), we treat \(q_{t+1}(\mathbf{x}_{t+1};\phi)\) as a fixed mixing layer denoted by \(\tilde{q}_{t+1}(\mathbf{x}_{t+1})\) and only fit the conditional layer \(q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi)\). More specifically, for HSIVI-SM, we consider the following optimization problem

\[\min_{\phi}\sum_{t=0}^{T-1}\beta(t)\mathbb{E}_{\tilde{q}_{t}(\mathbf{ x}_{t},\mathbf{x}_{t+1};\phi)}\left[\mathbf{S}_{t}(\mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t}; \psi)\right]^{T}\left[\mathbf{S}_{t}(\mathbf{x}_{t})+\mathbf{g}_{t}(\mathbf{x}_{t};\psi)-2 \nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi)\right], \tag{18}\] \[\min_{\psi}\sum_{t=0}^{T-1}\beta(t)\mathbb{E}_{\tilde{q}_{t}(\mathbf{ x}_{t},\mathbf{x}_{t+1};\phi)}\|\mathbf{g}_{t}(\mathbf{x}_{t};\psi)-\nabla_{\mathbf{x}_{t}} \log q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi)\|_{2}^{2}, \tag{19}\]

where \(\tilde{q}_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi)=q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi )\tilde{q}_{t+1}(\mathbf{x}_{t+1})\). In what follows, we demonstrate that the above problem also ensures an accurate approximation of the target score function.

For equation (19), by the denoising score matching trick (Hyvarinen, 2005), the optimal point of \(\psi\), denoted by \(\psi^{*}(\phi)\), satisfies

\[\mathbf{g}_{t}(\mathbf{x}_{t};\psi^{*}(\phi))=\nabla_{\mathbf{x}_{t}}\log\tilde{q}_{t}(\bm {x}_{t};\phi),\]

where \(\tilde{q}_{t}(\mathbf{x}_{t};\phi)=\int q(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi)\tilde{q}( \mathbf{x}_{t+1})\mathrm{d}\mathbf{x}_{t+1}\). By plugging in the optimal point \(\psi^{*}(\phi)\), each term in equation (18) is equivalent to

\[\mathbb{E}_{\tilde{q}_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi)}\left[\mathbf{S }_{t}(\mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t};\psi^{*}(\phi))\right]^{T}\left[\mathbf{S}_ {t}(\mathbf{x}_{t})+\mathbf{g}_{t}(\mathbf{x}_{t};\psi^{*}(\phi))-2\nabla_{\mathbf{x}_{t}} \log q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi)\right]\] \[= \mathbb{E}_{\tilde{q}_{t}(\mathbf{x}_{t};\phi)}\left[\mathbf{S}_{t}^{2}( \mathbf{x}_{t})-\mathbf{g}_{t}^{2}(\mathbf{x}_{t};\psi^{*}(\phi))\right]-2\iint\tilde{q}( \mathbf{x}_{t+1})\left[\mathbf{S}_{t}(\mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t};\psi^{*}( \phi))\right]^{T}\nabla_{\mathbf{x}_{t}}q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi) \mathrm{d}\mathbf{x}_{t+1}\mathrm{d}\mathbf{x}_{t}\] \[= \mathbb{E}_{\tilde{q}_{t}(\mathbf{x}_{t};\phi)}[\mathbf{S}_{t}^{2}(\mathbf{x }_{t})-\mathbf{g}_{t}^{2}(\mathbf{x}_{t};\psi^{*}(\phi))]-2\int\left[\mathbf{S}_{t}(\mathbf{x }_{t})-\mathbf{g}_{t}(\mathbf{x}_{t};\psi^{*}(\phi))\right]^{T}\nabla_{\mathbf{x}_{t}} \tilde{q}_{t}(\mathbf{x}_{t};\phi)\mathrm{d}\mathbf{x}_{t}\] \[= \mathbb{E}_{\tilde{q}_{t}(\mathbf{x}_{t};\phi)}\left[\mathbf{S}_{t}^{2}( \mathbf{x}_{t})-2\mathbf{S}_{t}(\mathbf{x}_{t})^{T}\nabla_{\mathbf{x}_{t}}\log\tilde{q}_{t}(\bm {x}_{t};\phi)+(\nabla_{\mathbf{x}_{t}}\log\tilde{q}_{t}(\mathbf{x}_{t};\phi))^{2}\right]\] \[= \mathbb{E}_{\tilde{q}_{t}(\mathbf{x}_{t};\phi)}\|\mathbf{S}_{t}(\mathbf{x}_{t })-\nabla_{\mathbf{x}_{t}}\log\tilde{q}_{t}(\mathbf{x}_{t};\phi)\|^{2}.\]

Therefore, the global optimal point \(\phi^{*}\) also ensures that the score of the variational distribution fits the target score function.

Based on the training objectives (18) (19) mentioned above, we propose Algorithm 2 for joint training, which does not need to store the computation graphs of the sample sequences. Moreover, by assuming an increasing weighting function \(\beta(t)\), we assign larger weights \(\beta(t)\) for those \(t\) close to \(T-1\), which tends to train the conditional layers that are close to \(T-1\) first during the training, resembling the sequential training.

### \(\epsilon\)-training of HSIVI-SM

Another popular formulation of diffusion models is modeling the conditional noise \(\mathbf{\epsilon}_{\theta}(\mathbf{u}_{s},s)\) by optimizing the DDPM loss in equation (14) where \(\mathbf{u}_{s}=\sqrt{\alpha(s)}\mathbf{u}_{0}+\sqrt{1-\alpha(s)}\mathbf{\epsilon}\), introduced as "\(\epsilon\)-prediction" in Appendix A. Now, let us assume the diffusion bridge is constructed with VP-SDE and we have a pre-trained model of conditional noise \(\mathbf{\epsilon}^{*}(\mathbf{u},s)\). Similarly, we construct a sequence of noise models \(\{\mathbf{\epsilon}^{*}_{t}(\mathbf{x}_{t})\}_{t=0}^{T-1}\) by letting \(\mathbf{x}_{t}=\mathbf{u}_{s_{t}}\) and \(\mathbf{\epsilon}^{*}_{t}(\mathbf{x})=\mathbf{\epsilon}^{*}_{t}(\mathbf{x},s_{t})\) which forms a (generalized) \(T\)-layer diffusion bridge. We only discuss how \(\epsilon\)-training can be applied to joint training and the derivation for sequential training is similar. In what follows, we consider the transformation of the joint training objective \(\mathcal{L}_{\text{HSIVI-SM}}\) for diffusion model acceleration.

By letting the weighting function \(\beta(t)=1-\alpha(s_{t})\) and considering the reparametrization form (17) where \(\phi_{t}\) is replaced by \(\phi\), the objective of HSIVI-SM takes the form

\[\mathcal{L}_{\text{HSIVI-SM}}(\phi,\psi)=\sum_{t=0}^{T-1}\mathbb{ E}_{\tilde{q}_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi)}\left[2\sqrt{\beta(t)}\mathbf{f}_{t}( \mathbf{x}_{t};\psi)^{T}[\sqrt{\beta(t)}\mathbf{S}^{*}_{t}(\mathbf{x}_{t})+\sqrt{\beta(t)} \mathbf{\Sigma}^{-1/2}_{t}(\mathbf{x}_{t+1};\phi)\mathbf{\epsilon})]\right.\\ \left.-\|\sqrt{\beta(t)}\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\|_{2}^{2}\right]. \tag{20}\]

where \(\mathbf{S}^{*}_{t}(\mathbf{x}_{t})\) is a pre-trained score model. Note that we have \(\sqrt{\beta(t)}\mathbf{S}^{*}_{t}(\mathbf{x}_{t})=-\mathbf{\epsilon}^{*}_{t}(\mathbf{x}_{t})\) by equation (15). Define

\[\tilde{\mathbf{f}}_{t}(\mathbf{x}_{t};\psi) =\sqrt{\beta(t)}\mathbf{f}_{t}(\mathbf{x}_{t};\psi),\] \[\tilde{\mathbf{\Sigma}}_{t}(\mathbf{x}_{t+1};\phi) =\mathbf{\Sigma}_{t}(\mathbf{x}_{t+1};\phi)/\beta(t).\]

The HSIVI-SM objective (20) then takes the form

\[\tilde{\mathcal{L}}_{\text{HSIVI-SM}}(\phi,\psi)=\sum_{t=0}^{T-1 }\mathbb{E}_{\tilde{q}_{t}(\mathbf{x}_{t},\mathbf{x}_{t+1};\phi)}\left[2\tilde{\mathbf{f}} _{t}(\mathbf{x}_{t};\psi)^{T}[-\mathbf{\epsilon}^{*}_{t}(\mathbf{x}_{t})+\tilde{\mathbf{\Sigma }}^{-1/2}_{t}(\mathbf{x}_{t+1};\phi)\mathbf{\epsilon})]-\|\tilde{\mathbf{f}}_{t}(\mathbf{x}_{t };\psi)\|_{2}^{2}\right] \tag{21}\]

and we call it the objective for \(\epsilon\)-training. In our implementation of \(\epsilon\)-training, we directly parametrize \(\tilde{\mathbf{f}}_{t}(\mathbf{x}_{t};\psi)\) and \(\tilde{\mathbf{\Sigma}}_{t}(\mathbf{x}_{t+1};\phi)\) instead of \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\) and \(\mathbf{\Sigma}_{t}(\mathbf{x}_{t+1};\phi)\). The objective (21) is more numerically stable since the magnitude of \(\tilde{\mathbf{\Sigma}}_{t}(\mathbf{x}_{t+1};\phi)\) is generally larger than \(\mathbf{\Sigma}_{t}(\mathbf{x}_{t+1};\phi)\).

### Complexity comparison of HSIVI

For methods that we discussed in SIVI variants, which use a single conditional layer (i.e., T=1) and hence would be much cheaper to sample from than HSIVI-SM that uses multiple layers \(T>1\). For methods that we discussed for diffusion models, the computational complexity would be similar if they had the same \(T\). That is because we used the same neural network architecture for the conditional layers in HSIVI and the score nets in diffusion models. We have a comparison of the sampling time of different methods in Figure 13.

## Appendix D Additional results of experiments

### Gaussian mixture model

For HSIVI on the Gaussian mixture model, the auxiliary distributions can also be constructed with diffusion bridge in Example 2. Concretely, the diffusion bridge is constructed by

\[\mathbf{x}_{t}|\mathbf{x}_{0}\sim\mathcal{N}(\sqrt{\alpha_{t}}\mathbf{x}_{0},(1-\alpha_{t}) \mathbf{I}),\quad\mathbf{x}_{0}\sim p_{0}(\mathbf{x}_{0}).\]

where \(\alpha_{t}=\alpha(s_{t})\) with \(\alpha(s)\) defined in equation (12). In this example, the score function \(\mathbf{S}_{t}(\mathbf{x}_{t})=\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{x}_{t})\) has an analytical form

\[\mathbf{S}_{t}(\mathbf{x}_{t})=\mathbf{S}_{0}\left(\mathbf{x}_{t};\sqrt{\alpha_{t}}\mathbf{\mu},( \alpha_{t}\sigma^{2}+1-\alpha_{t})\mathbf{I}\right),\quad 0\leq t\leq T-1.\]

where \(\mathbf{S}_{0}(\mathbf{x};\mathbf{\mu},\sigma^{2}\mathbf{I})\) is the score function of the Gaussian mixture model \(p(\mathbf{x};\mathbf{\mu},\sigma^{2}\mathbf{I})=\sum_{i=1}^{8}1/8\cdot\mathcal{N}(\bm {x};\mathbf{\mu}_{i},\sigma^{2}\mathbf{I})\). We set the number of layers \(T=5\) and \(\alpha_{t}=1-t/5\) for \(t=0,\dots,4\). Figure 8 shows the sample trajectories generated by HSIVI. We see clearly that semi-implicit distributions are guided toward the target distribution following the diffusion bridge.

### High-dimensional conditioned diffusion

We also test SIVI-LB and HSIVI-LB for fitting the posterior in high-dimensional conditioned diffusion. The auxiliary bridge is formed using the same geometric interpolation as for HSIVI-SM,

Figure 8: **Upper row**: Sample trajectories progressively generated by 5-layer HSIVI-LB guided by diffusion bridge. **Bottom row**: Sample trajectories progressively generated by 5-layer HSIVI-SM guided by diffusion bridge.

Figure 9: The posterior estimates for conditioned diffusion obtained by SIVI-LB and 5-layer HSIVI-LB. For each method, we collect 100,000 samples to calculate the sample mean and confidence interval.

i.e.

\[p_{\text{base}}=\mathcal{N}(\mathbf{x};\mathbf{y},\sigma^{2}\mathbf{I}),\quad\lambda_{t}=1 -\frac{t}{T-1}\;\;\text{for}\;\;0\leq t\leq T-1.\]

From Figure 9, we see that SIVI-LB also underestimates the posterior variance and 5-layer HSIVI-LB fits the variance better. This phenomenon is also observed in the performances of SIVI-SM and HSIVI-SM in Figure 3. The quantitative comparison between different numbers of layers is reported in Table 4, where we see that for both HSIVI-SM and HSIVI-LB, the variational approximation gets more accurate with more layers. We also find that HSIVI-SM fits better than HSIVI-LB consistently.

### Toy examples of diffusion model acceleration

We compare the samples from DDPM, DDIM, and our proposed HSIVI-SM with 5 and 10 steps in Figure 10. We find that DDIM and DDPM fail to converge to the target distribution with a small number of steps, while HSIVI-SM can provide noticeably better samples. Moreover, DDPM tends to underestimate the variance as evidenced by the narrower region occupied by the samples.

### Mnist

Figure 11 shows the samples from DDPM, DDIM, and HSIVI-SM with \(T=10\) steps. We see that the samples produced by HSIVI-SM is much cleaner and more recognizable than those produced by DDPM and DDIM.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \(T=1\) & \(T=2\) & \(T=3\) & \(T=5\) \\ \hline HSIVI-SM & 0.0886 & 0.0813 & 0.0431 & **0.0333** \\ HSIVI-LB & 0.0883 & 0.0825 & 0.0722 & **0.0433** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Frobenius distances between the estimated covariance matrices and that of the ground truth. For each method, we collect 100,000 samples to estimate the covariance matrix.

Figure 10: Comparison of 10,000 samples generated by DDPM, DDIM, and HSIVI-SM.

### CIFAR-10 & CelebA

Figure 12 shows the uncurated samples from our proposed HSIVI-SM method with different numbers of layers on CIFAR-10 (\(28\times 28\)), CelebA (\(64\times 64\)) and ImageNet (\(64\times 64\)). We also compare the sampling time of different methods when \(\text{NFE}=5\) in Figure 13.

One can observe that HSIVI-SM has almost the same running time as the simplest DDIM algorithm. Finally, we report the number of parameters in the score model (or noise model) used by different methods in Table 5, which corresponds to Table 2 and Figure 13. In our implementations of HSIVI-SM, the number of parameters in the noise model equals that in the conditional layer \(q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi)\). We find that our model with the same parameters reaches better results in Table 2.

## Appendix E Experimental details

### Target distribution approximation

In this part, we set the conditional layer to be \(q_{\phi}(\mathbf{x}|\mathbf{z})=\mathcal{N}(\mathbf{x};\mathbf{\mu}(\mathbf{z};\phi^{\mu}),\mathrm{ diag}\{\exp(\phi^{\sigma})\})\) and the mixing layer to be \(\mathcal{N}(\mathbf{0},\mathbf{I})\) for SIVI. Here, \(\{\phi^{\mu},\phi^{\sigma}\}=\phi\) are the variational parameters. For \(T\)-layer hierarchical semi-implicit variational distribution with \(T\geq 2\), the variational prior \(q_{T}(\mathbf{x}_{T})\) is set to be \(\mathcal{N}(\mathbf{0},\mathbf{I})\). Each conditional layer \(q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})\) for \(t=0,\dots,T-1\) is a conditional Gaussian distribution

\[q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi_{t})=\mathcal{N}(\mathbf{x}_{t};\mathbf{\mu}(\mathbf{x}_ {t+1};\phi_{t}^{\mu}),\mathrm{diag}\{\exp(\phi_{t}^{\sigma})\}).\]

Note that the \(\phi^{\sigma}\) and \(\{\phi_{t}^{\sigma}\}_{t=0}^{T-1}\) above are all vectors with the same dimension as \(\mathbf{x}\). We use sequential training for HSIVI in the two experiments in this part. The parameters \(\{\phi_{t}\}_{t=0}^{T-1}\) are independent across different \(t\). If not otherwise specified, we use the Adam optimizer (Kingma & Ba, 2015) with \(\beta=(0.9,0.99)\) for training.

The noise levels in the diffusion bridge are \(1-\alpha(s_{t})=1-t/5\) for \(t\in\{0,1,\cdots,4\}\). We set the learning rate of variational parameters \(\phi_{t}\) (or \(\phi\)) to 0.001 and the learning rate of \(\psi_{t}\) (or \(\psi\)) to 0.002 in both SIVI and HSIVI. For HSIVI-LB and HSIVI-SM, we run 80000 variational parameter updates for every conditional layer; for SIVI-LB and SIVI-SM, we run 5\(\times\)80000 variational parameter updates. For HSIVI-SM and SIVI-SM, in each nested training loop of \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi_{t})\) (or \(\mathbf{f}(\mathbf{x};\psi)\)), we update \(\psi_{t}\) (or \(\psi\)) one time after each update of \(\phi_{t}\) (or \(\phi\)). All the algorithms are trained with a batch size of 64.

#### e.1.2 High-dimensional conditioned diffusion

For the experiment on high-dimensional conditioned diffusion, we examine the performances of SIVI and 5-layer HSIVI. The ground truth is formed by running 100,000 independent stochastic gradient Langevin dynamics (SGLD) chains with a step size of 0.0001 and collecting the results

Figure 12: Uncurated samples generated by HSIVI-SM with different numbers of layers on CIFAR-10, CelebA and ImageNet.

after 10,000 iterations. For \(t=0,\ldots,T-2\), the mean of each conditional layer \(\mathbf{\mu}(\mathbf{x}_{t+1};\phi_{t}^{\mu})\) in HSIVI has a residual form, i.e. \(\mathbf{\mu}(\mathbf{x}_{t+1};\phi_{t}^{\mu})=\mathbf{x}_{t+1}+\mathbf{\hat{\mu}}(\mathbf{x}_{t+1}; \phi_{t}^{\mu})\). For SIVI and \(t=T-1\) in HSIVI, we assume \(\mathbf{\mu}(\mathbf{z};\phi^{\mu})=\mathbf{\bar{\mu}}(\mathbf{z};\phi^{\mu})\) and \(\mathbf{\mu}(\mathbf{x}_{t+1};\phi_{t}^{\mu})=\mathbf{\bar{\mu}}(\mathbf{x}_{t+1};\phi_{t}^{\mu})\). For each \(t\), \(\mathbf{\bar{\mu}}_{t}(\mathbf{x};\phi_{t}^{\mu})\) in HSIVI and \(\mathbf{\bar{\mu}}(\mathbf{z};\phi^{\mu})\) in SIVI are MLPs with layer widths \([300,512,512,300]\) and ReLU activation functions. For each \(t\), \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi_{t})\) in HSIVI-SM and \(\mathbf{f}(\mathbf{x};\psi)\) in SIVI-SM are MLPs with layer widths \([300,512,512,300]\) and ReLU activation functions. For both SIVI and HSIVI, we train each conditional layer for 100,000 iterations with a batch size of 128. For HSVI-SM and SIVI-SM, in each nested training loop of \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi_{t})\) (or \(\mathbf{f}(\mathbf{x};\psi)\)), we update \(\psi_{t}\) (or \(\psi\)) one time after each update of \(\phi_{t}\) (or \(\phi\)). We set the learning rate to be 0.0001 for \(\phi_{t}\) (or \(\phi\)) and 0.0005 for \(\psi_{t}\) (or \(\psi\)).

### Diffusion model acceleration

In this part, we use the diffusion bridge to construct the auxiliary distributions and joint training as mentioned in Section 4.2. With a pre-trained score model or noise model, we consider the generative tasks as score-based variational inference problems. Therefore, we do not use any training data to train HSIVI-SM.

For HSIVI-SM, the variational prior \(q_{T}(\mathbf{x}_{T})\) is set to be \(\mathcal{N}(\mathbf{0},\mathbf{I})\). To avoid the large memory consumption, we use the joint training method where the parameters of the conditional layers \(q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi)\) and \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\) are the same across different \(t\). The \(t\)-th conditional layer is a conditional Gaussian distribution

\[q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi)=\mathcal{N}\left(\mathbf{x}_{t};\mathbf{\mu}_{t}( \mathbf{x}_{t+1};\phi^{\mu}),\mathrm{diag}\left(\sigma_{t}^{2}\exp(\phi^{\sigma}) \right)\right),\]

where \(\{\phi^{\mu},\phi^{\sigma}\}=\phi\) are the variational parameters, \(\phi^{\sigma}\) is a vector with the same dimension as \(\mathbf{x}\), and \(\sigma_{t}\) is a fixed scalar value. We use the generalized inference process in DDIM (Song et al., 2020a) with the noise level \(\eta>0\) to initialize \(\mathbf{\mu}_{t}(\mathbf{x}_{t+1};\phi^{\mu})\) and determine the value of \(\sigma_{t}\) for each \(t\). If not otherwise specified, we use the Adam optimizer (Kingma & Ba, 2015) with \(\beta=(0.9,0.99)\) for training.

For our implementation, we referenced the training code of diffusion model acceleration for our models in the repository from (Dockhorn et al., 2022).

#### e.2.1 Toy examples of diffusion model acceleration

For pre-training the score model \(\mathbf{S}^{*}(\mathbf{x},s)\), we consider quadratic noise levels \(1-\alpha(s)=s^{2}\) for \(s\in[0,1]\). We then train \(\mathbf{S}^{*}(\mathbf{x},s)\) on 1000 fixed noise levels \(\{1-\alpha(i/1000)\}_{i=1}^{1000}\) by optimizing the DDPM loss in equation (13) for 200,000 iterations with a learning rate of 0.0003 and a batch size of 100. For constructing the diffusion bridge, we choose \(T\) discrete time steps \(\{s_{t}\}_{t=0}^{T-1}\) so that \(1-\alpha(s_{t})=[0.01+(\sqrt{0.8}-0.1)t/T]^{2}\) for \(t=0,1,\ldots,T-1\).

Model architectureThe model architecture of \(\mathbf{S}^{*}(\mathbf{x},s)\) is

\[\mathbf{S}^{*}(\mathbf{x},s)=\mathrm{MLP^{\text{dec}}}\left(\mathrm{MLP^{\text{embx}}}( \mathbf{x})+\mathrm{MLP^{\text{embt}}}(1-\alpha(s))\right),\]

where \(\mathrm{MLP^{\text{dec}}}\) is a decoder implemented as MLPs with layer widths \([128,128,128,2]\), \(\mathrm{MLP^{\text{embx}}}\) is a data embedding block implemented as MLPs with layer widths \([2,128]\), and \(\mathrm{MLP^{\text{embt}}}\) is a time

Figure 13: Sampling time (\(\downarrow\)) of different methods when \(\mathrm{NFE}=5\) on CIFAR-10 and CelebA. Results are averaged by 100 independent runs with a batch size of 128 on a single Nvidia 2080Ti GPU.

embedding block implemented as MLPs with layer widths \([256,128,128]\). We use the sinusoidal positional embedding (Vaswani et al., 2017) of \(1-\alpha(s)\) as the input of \(\mathrm{MLP}^{\text{emlht}}\). All these three MLPs use GELU as activation functions. We use the generalized inference process with noise level \(\eta=1.0\) to initialize the conditional layers. The architecture of \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\) is the same as that of \(\mathbf{S}^{*}(\mathbf{x},s)\). We initialize \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\) with \(\mathbf{S}^{*}_{t}(\mathbf{x}_{t}):=\mathbf{S}^{*}(\mathbf{x}_{t},s_{t})\).

Training settingThe learning rate is set to be 0.0002 for \(q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi)\) and 0.0005 for \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\) on Swissroll, Circles, and Moons for both \(T=5,10\). On Checkerboard, the learning rate is set to be 0.00001 (0.00002) for \(q_{t}(\cdot|\mathbf{x}_{t+1};\phi)\) and 0.00005 (0.0001) for \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\) when \(T=5\) (\(T=10\)). We train HSIVI-SM for 25,000 iterations with a batch size of 64 in all cases. In each nested training loop of \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\), we update \(\psi\) 3 times after each update of \(\phi\).

#### e.2.2 Mnist

For the experiment on MNIST, we use the pre-trained noise model \(\epsilon^{*}(\mathbf{x},s)\) and train HSIVI-SM with \(\epsilon\)-training introduced in Section C.4. The following construction of noise schedule comes from Song et al. (2020a). Let \(\beta_{j}=\beta_{\min}+\frac{\beta_{\max}-\beta_{\min}}{999}j\) for \(j=0,\ldots,999\), where \(\beta_{\min}=0.0001,\beta_{\max}=0.02\). We pre-train the noise model on the 1000 fixed noise levels \(1-\alpha(s):=\prod_{j=0}^{s}\beta_{j}\) for \(s=0,\ldots,999\) by equation (14). The noise model is trained for 100,000 iterations with a learning rate of 0.0001 and a batch size of 64. We then choose \(T\) discrete time steps \(s_{t}=\lfloor 800\cdot\frac{t^{2}}{T^{2}}\rfloor\) for \(t=0,\ldots,T-1\) to construct the \(T\)-layer diffusion bridge.

Model architectureThe pre-trained noise model \(\epsilon^{*}(\mathbf{x},s)\) follows the UNet structure employed by Ho et al. (2020) where the number of input channels and output channels is reduced to one. Additionally, we pad the image size to \(32\times 32\) to fit \(\epsilon^{*}(\mathbf{x},s)\). We use the generalized inference process with noise level \(\eta=0.2\) to initialize the conditional layers. The architecture of \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\) is the same as that of \(\mathbf{\epsilon}^{*}(\mathbf{x},s)\). We initialize \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\) with \(-\mathbf{\epsilon}^{*}(\mathbf{x}_{t},s_{t})/\sqrt{1-\alpha(s_{t})}\).

Training settingFor both \(T=5,10\), the learning rate is set to be \(1.6\times 10^{-5}\) for \(\phi\) and \(6.4\times 10^{-5}\) for \(\psi\). We train HSIVI-SM for 10,000 iterations with a batch size of 64 in all cases. In each nested training loop of \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\), we update \(\psi\) 20 times after each update of \(\phi\).

#### e.2.3 CIFAR-10, CelebA & ImageNet

For experiments on CIFAR-10 and CelebA, we use the pre-trained noise model \(\epsilon^{*}(\mathbf{x},s)\) and train HSIVI-SM with \(\epsilon\)-training introduced in Section C.4. We use the same noise schedule as in the experiment on MNIST. Let \(\beta_{j}=\beta_{\min}+\frac{\beta_{\max}-\beta_{\min}}{999}j\) for \(j=0,\ldots,999\), where \(\beta_{\min}=0.0001,\beta_{\max}=0.02\). We take the pretrained noise model for CIFAR10 and ImageNet seperately from [https://github.com/tqch/ddpm-torch/releases/download/chekpoints/cifar10_2040.pt](https://github.com/tqch/ddpm-torch/releases/download/chekpoints/cifar10_2040.pt) and [https://openaipublic.blob.core.windows.net/diffusion/march-2021/imagenet64_uncond_100M_1500K.pt](https://openaipublic.blob.core.windows.net/diffusion/march-2021/imagenet64_uncond_100M_1500K.pt). On CelebA, We pre-train the noise model on the 1000 fixed noise levels \(1-\alpha(s):=\prod_{j=0}^{s}\beta_{j}\) for \(s=0,\ldots,999\) by optimizing equation (14). The noise model is trained for 600 epochs, with a learning rate of 0.00002 and batch size of 128. We then choose \(T\) discrete time steps \(s_{t}=\lfloor 800\cdot\frac{t^{2}}{T^{2}}\rfloor\) for \(t=0,\ldots,T-1\) to construct the \(T\)-layer diffusion bridge.

Model architectureOn CIFAR-10 and CelebA, the structure of \(\mathbf{\epsilon}^{*}(\mathbf{x},s)\) is exactly the UNet3 employed in Ho et al. (2020) without modification; on ImageNet, the structure of \(\mathbf{\epsilon}^{*}(\mathbf{x},s)\) is exactly the UNet in Nichol & Dhariwal (2021). We use the generalized inference process with noise level \(\eta=0.2\) to initialize the conditional layers. The architecture of \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\) is the same as that of \(\mathbf{\epsilon}^{*}(\mathbf{x},s)\). We initialize \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\) with \(-\mathbf{\epsilon}^{*}(\mathbf{x}_{t},s_{t})/\sqrt{1-\alpha(s_{t})}\).

Footnote 3: We use the Pytorch implementation of UNet structure in [https://github.com/tqch/ddpm-torch](https://github.com/tqch/ddpm-torch).

Training settingThe number of layers, which is also the number of function evaluations (NFE), is set to be \(T=5,10,15\) in our test cases. On CIFAR-10, the learning rate is set to be \(1.6\times 10^{-5}\) for \(q_{t}(\cdot|\mathbf{x}_{t+1};\phi)\) and \(8\times 10^{-5}\) for \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\); on CelebA, the learning rate is set to be \(1.2\times 10^{-6}\) for \(q_{t}(\cdot|\mathbf{x}_{t+1};\phi)\) and \(6\times 10^{-6}\) for \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\); on ImageNet, the learning rate is set to be \(1\times 10^{-5}\)for \(q_{t}(\cdot|\mathbf{x}_{t+1};\phi)\) and \(5\times 10^{-5}\) for \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\). We trained HSIVI-SM for 10,000 iterations with a batch size of 128. During each nested training loop of \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\), we update \(\psi\) 20 times after each update of \(\phi\), since we find \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\) needs more training empirically to provide reliable guidance. For \(T=10,15\), we use the above training settings; for \(T=5\), we find that further fine-tuning on the well-trained 15-layer HSIVI-SM for 1,000 iterations yields better results, and we utilize this strategy to optimize the 5-layer HSIVI-SM with a 0.1\(\times\) smaller learning rate. Experiments need about 1.5 days on CIFAR-10, need about 3 days on CelebA and 4 days on ImageNet using 8 Nvidia 2080 Ti GPUs. During the training, we find that HSIVI-SM converges in the first 30% iterations on CIFAR-10 and converges in the first 50% iterations on CelebA.

## Appendix F Limitations

For the application of accelerating the sampling process of diffusion models, our HSIVI-SM training involves three models: the score model (or noise model), the conditional layers \(q_{t}(\mathbf{x}_{t}|\mathbf{x}_{t+1};\phi)\), and \(\mathbf{f}_{t}(\mathbf{x}_{t};\psi)\). As a result, HSIVI-SM requires higher memory consumption due to the involvement of multiple models. Additionally, since our HSIVI algorithm approximates the target distribution using the score function, it necessitates a pre-trained score model (or noise model) with high accuracy and additional training steps. Finally, we recognize that the alternative method HSIVI-LB remains unexplored for accelerating the diffusion model, and we defer this aspect to future research.