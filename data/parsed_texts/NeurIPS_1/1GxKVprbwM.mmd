# On Computing Pairwise Statistics

with Local Differential Privacy

Badih Ghazi

Google Research

Mountain View, CA, US

badihghazi@gmail.com

&Pritish Kamath

Google Research

Mountain View, CA, US

pritish@alum.mit.edu

&Ravi Kumar

Google Research

Mountain View, CA, US

ravi.k53@gmail.com

&Pasin Manurangsi

Google Research

Bangkok, Thailand

pasin@google.com

&Adam Sealfon

Google Research

New York, NY, US

adamsealfon@google.com

###### Abstract

We study the problem of computing pairwise statistics, i.e., ones of the form \(\binom{n}{2}^{-1}\sum_{i\neq j}f(x_{i},x_{j})\), where \(x_{i}\) denotes the input to the \(i\)th user, with differential privacy (DP) in the local model. This formulation captures important metrics such as Kendall's \(\tau\) coefficient, Area Under Curve, Gini's mean difference, Gini's entropy, etc. We give several novel and generic algorithms for the problem, leveraging techniques from DP algorithms for linear queries.

## 1 Introduction

Differential privacy (DP) [14] is a widely studied and used notion for quantifying the privacy protections of algorithms in data analytics and machine learning. It has witnessed many practical deployments [1, 1, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9]. On a high level, DP dictates that the output of a (randomized) algorithm remains statistically indistinguishable if the input of any single user is modified; the degree of statistical indistinguishability is quantified by the privacy parameter \(\varepsilon>0\).

An important setting of DP is the _local_ model [1, 2], where each of \(n\) users holds an input that they wish to keep private. An analyst wishes to compute some known function of the users inputs. The analyst and the users engage in an interactive protocol at the end of which the analyst is supposed to compute an estimate to the value of the function on the user inputs. When the aforementioned statistical indistinguishability property is enforced on the algorithm's transcript, the algorithm is said to be \(\varepsilon\)-local DP (see Section 2 for a formal definition). _Non-interactive_ algorithms refer to the setting where each user sends a single (DP) message to the analyst, who is then supposed to output an estimate of the desired value without any further interaction with the users. As usual in the interactive local DP setting, we assume a broadcast model where every communication is visible to all users and to the analyst (and subject to the DP constraint). The number of rounds of an interactive protocol refers to the number of back-and-forths between the set of users and the analyst.

While the local DP setting offers a compelling trust model compared to the central DP model (where an analyst is assumed to have access to the raw user data and the privacy guarantee is only enforced at its output), the local setting is often limited by lower bounds on the error incurred by private protocols (e.g., [1, 2]). In this work, we study some basic tasks in analytics and learning, and give local DP protocols with significantly smaller error than what was previously known.

**Quadratic Form Computation.** Throughout the paper, we consider the following task:

**Definition 1** (Quadratic Form Computation).: _Given a matrix \(W\in\mathbb{R}^{k\times k}\). Each user \(i\) receives \(x_{i}\in[k]\). The goal is to compute the quadratic form \(h_{\mathbf{x}}^{T}Wh_{\mathbf{x}}\) where \(h_{\mathbf{x}}\in\mathbb{Z}_{\geq 0}^{k}\) denotes the normalized histogram of the input, i.e., \((h_{\mathbf{x}})_{b}:=\frac{1}{n}\cdot|\{i\in[n]\mid x_{i}=b\}|\)._

An algorithm computing quadratic forms immediately implies an algorithm for computing pairwise statistics (also known as \(U\)_-statistics of degree \(2\)_) defined as \(\mathcal{F}(X)=\frac{1}{\binom{n}{2}}\sum_{1\leq i<j\leq n}f(x_{i},x_{j})\), where \(f:\mathcal{X}^{2}\rightarrow\mathbb{R}\) is a symmetric function (called _kernel_), and \(x_{i}\) is the input to the \(i\)th user. The family of pairwise statistics is notable in that it contains several statistical quantities that are widely used in different areas, including the Area Under the Curve (AUC), Kendall's \(\tau\) coefficient, the Gini's mean difference, and the Gini's diversity index (aka Gini-Simpson index or Gini's impurity). Computing quadratic forms has been studied in the context of local DP by Bell et al. [2] who gave algorithms for functions \(f\) that are Lipschitz and for estimating the AUC of a classifier; the latter was improved recently [14]. While these problems are simple in central DP (as we can directly add noise to the function value), we note that a clear challenge for computing pairwise statistics (and hence quadratic forms) in local DP is that each summand depends on data points held by two different users.

**Linear Queries.** Computation of quadratic forms is a natural "degree-2" variant of the so-called _linear queries_ (defined below), a well-studied problem in the DP literature.

**Definition 2** (Linear Queries).: _Given a workload matrix \(W\in\mathbb{R}^{k\times k}\), the goal is to compute \(Wh_{\mathbf{x}}\), where \(h_{\mathbf{x}}\in\mathbb{Z}_{\geq 0}^{k}\) denotes the normalized histogram of the input._

There is a long line of work on privately answering linear queries. In both the central and local models, the optimal errors achievable by DP algorithms are (mostly) well-understood for various types of errors, such as the \(\ell_{2}^{2}\)-error or the \(\ell_{\infty}\)-error [12, 1, 13, 14, 15, 16, 17, 18, 19, 20]. Recall that the _MSE_ of an estimate \(\hat{z}\) of a real value \(z\) is defined as \(\operatorname{MSE}(\hat{z},z):=\mathbb{E}_{\hat{z}}[(\hat{z}-z)^{2}]\). For our purpose, we consider the following natural measure of accuracy:

**Definition 3** (mMSE).: _Given a mechanism \(\mathcal{M}\) that answers linear queries for a workload matrix \(W\in\mathbb{R}^{k\times k}\), its maximum mean square error (mMSE) is defined as \(\operatorname{mMSE}(\mathcal{M};W):=\max_{\mathbf{x}\in[k]^{n}}\max_{j\in[k]} \operatorname{MSE}(\mathcal{M}(\mathbf{x})_{j},(Wh_{\mathbf{x}})_{j})\)._

In a recent seminal work, Edmonds et al. [20] provide a nearly optimal characterization of the error achievable by non-interactive \(\varepsilon\)-local DP algorithms. To describe their result, we need some additional definitions of norms on matrices and related quantities:

* \(1\to 2\) norm: For any matrix \(A\), \(\|A\|_{1\to 2}\) is the maximum (\(\ell_{2}\)-)norm of its columns.
* \(\ell_{\infty}\)-norm: For any matrix \(A\), \(\|A\|_{\infty}\) is the maximum absolute value among its entries.
* Factorization norm: For \(W\in\mathbb{R}^{k\times k}\), let \(\gamma_{2}(W):=\min_{L^{T}R=W}\|L\|_{1\to 2}\|R\|_{1\to 2}\).
* Approximate-Factorization norm1: For \(W\in\mathbb{R}^{k\times k}\) and \(\alpha\geq 0\), let \(\gamma_{2}(W;\alpha):=\min_{\|W-W\|_{\infty}\leq\alpha}\gamma_{2}(\tilde{W})\). Footnote 1: Note that, unlike the three quantities above, this is not actually a norm.

Finally, we define \(\zeta(W,n):=\min_{\alpha\geq 0}\left(\gamma_{2}(W;\alpha)+\alpha\varepsilon \sqrt{n}\right)\). (Note that \(\zeta(W,n)\leq\gamma_{2}(W)\) as we can pick \(\alpha=0\).) The result of Edmonds et al. [20] states that this quantity, up to polylogarithmic factors, governs the best error achievable for linear queries in the non-interactive local DP model2:

Footnote 2: We remark that [20] did not state their bounds in the form we present in Theorem 4 (or even for \(\operatorname{mMSE}\)); we provide more detail on how to interpret their lower bound in this form in Appendix B. The upper bound is via the _matrix mechanism_, introduced in central DP earlier in [12, 15].

**Theorem 4** ([20]).: _For any workload matrix \(W\), there is a non-interactive \(\varepsilon\)-local DP mechanism for linear queries with \(\operatorname{mMSE}\) at most \(O\left(\frac{\zeta(W,n)^{2}}{\varepsilon^{2}n}\right)\). Furthermore, any non-interactive \(\varepsilon\)-local DP mechanism must incur \(\operatorname{mMSE}\) at least \(\tilde{\Omega}\left(\frac{\zeta(W,n)^{2}}{\varepsilon^{2}n}\right)\)._

Given such a tight and generic characterization for linear queries, it is natural to ask:

Can we characterize the error of computing the quadratic form for any matrix \(W\) with local DP?

### Our Contributions

In this work, we answer this question by establishing connections between the problems of computing quadratic forms and linear queries. Leveraging the wealth of knowledge on the latter, we obtain several new (and general) upper and lower bounds for the former.

**Non-Interactive Local DP.** First, in the non-interactive setting, we give the following upper and lower bounds. Together, they show that the error one can expect for computing quadratic forms is essentially the same as that for computing linear queries (as provided in Theorem 4).

**Theorem 5** (Non-interactive Algorithm).: _For any \(W\in\mathbb{R}^{k\times k}\), there is a non-interactive \(\varepsilon\)-local DP mechanism for estimating quadratic forms on \(W\) with \(\operatorname{MSE}\) at most \(O\left(\frac{\zeta(W,n)^{2}(\log k)}{\varepsilon^{2}n}\right)\)._

**Theorem 6** (Non-interactive Lower Bound).: _For any symmetric \(W\in\mathbb{R}^{k\times k}\), any non-interactive \(\varepsilon\)-local DP mechanism for estimating quadratic forms on \(W\) must incur \(\operatorname{MSE}\) at least \(\tilde{\Omega}\left(\frac{\zeta(W,n)^{2}}{\varepsilon^{2}n}\right)\)._

Our results above are generic and can be applied to any pairwise statistics. To demonstrate the power of the algorithm, we now state implications for several classes of statistics that have been studied in the privacy literature. Due to space constraints, we defer their definitions to Section 5.

**Corollary 7**.: _There is a non-interactive \(\varepsilon\)-local DP mechanism for computing pairwise statistics for:_

* _Any_ \(G\)_-Lipschitz function_ \(f:[0,1]\to\mathbb{R}\)_, with_ \(\operatorname{MSE}O\left(\frac{G^{2}\log n}{\varepsilon^{2}n}\right)\)_,_
* _Kendall's_ \(\tau\) _coefficient, with_ \(\operatorname{MSE}O\left(\frac{(\log k)^{5}}{\varepsilon^{2}n}\right)\)_,_
* _AUC under the balancedness assumption_3_, with_ \(\operatorname{MSE}O\left(\frac{(\log k)^{3}}{\varepsilon^{2}n}\right)\)_,_ Footnote 3: The balancedness assumption for AUC states that there are \(\Omega(n)\) examples with each label 0, 1. This is a required assumption to achieve an error in the form presented, as otherwise when e.g., there is a single 0-labeled example, it is impossible to achieve any non-trivial guarantee.
* _Gini's diversity index, with_ \(\operatorname{MSE}O\left(\frac{\log k}{\varepsilon^{2}n}\right)\)_._

For \(O(1)\)-Lipschitz functions, which includes several well-known metrics such as Gini mean difference, we improve upon the algorithm of Bell et al. [1] whose \(\operatorname{MSE}\) is \(O\left(\frac{1}{\varepsilon\sqrt{n}}\right)\). We are not aware of any previous bounds on Kendall's \(\tau\) coefficient before; the metric was mentioned in [1] without any error guarantee given. For AUC, our bound is the same as in [1] but worse than a follow-up work [12] by a \(\log k\) factor; nonetheless, we stress that our result is derived as a corollary of a generic algorithm without relying too deeply on AUC (except for the \(\gamma_{2}\)-norm of its matrix). For Gini's diversity index, our bound is again worse than that from Bravo et al. [1] by \(\log k\) factor, but their algorithm requires the use of public randomness; we do not use any public randomness. (Throughout this work, when we refer to the local model without further specification, we assume no public randomness, aka _private-coin_ model.)

**Remark.** Since our non-interactive algorithm only requires a vector-summation primitive, we can also apply protocols for vector summation in the shuffle model (e.g., [1, 1, 2]) to obtain an \((\varepsilon,\delta)\)-DP protocol in the shuffle model, reducing the \(\operatorname{MSE}\) by a factor of \(n\) for each setting in Theorem 5 and Corollary 7.

**Interactive Local DP.** Finally, we also provide an interactive algorithm whose \(\operatorname{MSE}\) does not depend on \(\gamma_{2}(W)\), as long as \(n\) is sufficiently large:

**Theorem 8** (Interactive Algorithm).: _For any \(W\in\mathbb{R}^{k\times k}\), there is a three-round \(\varepsilon\)-DP algorithm for estimating quadratic forms on \(W\) such that, for \(n\geq(\frac{\gamma_{2}(W)\cdot\log k}{\left\|W\right\|_{\infty}\cdot \varepsilon})^{O(1)}\), the \(\operatorname{MSE}\) is \(O\left(\frac{\left\|W\right\|_{\infty}^{2}}{\varepsilon^{2}n}\right)\)._

We note that the dependence \(O\left(\frac{\left\|W\right\|_{\infty}^{2}}{\varepsilon^{2}n}\right)\) is the best possible: even when the \(k=2\) and \(W\) is binary, the problem is as hard as binary summation, which is known to require \(\operatorname{MSE}\) at least \(\Omega\left(\frac{1}{\varepsilon^{2}n}\right)\) even for an arbitrary number of rounds of interactions [1], and we can rescale this hard instance to get any desired \(\ell_{\infty}\)-norm.

Combining the non-interactive lower bound in Theorem 6 and the interactive upper bound in Theorem 8, our results imply that there exists a matrix \(W\in\mathbb{R}^{k\times k},\varepsilon>0,n\in\mathbb{N}\) such that interactive algorithms are provably more accurate than non-interactive ones for privately computing the quadratic form on \(W\). This places computing pairwise statistics as one of the few problems (and perhaps the most natural) that are known to separate interactive and non-interactive local DP. Due to space constraints, we defer further discussion of this to the Appendix.

### Technical Overview

We now give a brief technical overview of our proofs.

**Black-Box Reduction from Linear Queries (Lower Bound).** As mentioned earlier, our results are shown through connections between computing quadratic forms and linear queries. We will start with black-box reductions between the two. Suppose that we have a non-interactive \(\varepsilon\)-local DP algorithm \(\mathbb{A}\) for computing the quadratic form on \(W\). We will construct a non-interactive \(\varepsilon\)-local DP algorithm \(\mathbb{A}^{\prime}\) for computing linear queries of \(W\). This will allow us to establish our lower bound (Theorem 6) as a consequence of the lower bound for linear queries (Theorem 4).

For simplicity of this overview, instead of considering the full quadratic form, suppose that \(\mathbb{A}\) works on \(2n\) users with inputs \(x_{1},\dots,x_{n},y_{1},\dots,y_{n}\) and computes an estimate of \(h_{\mathbf{y}}^{T}Wh_{\mathbf{x}}\). On input \(x_{1},\dots,x_{n}\), algorithm \(\mathbb{A}\) works as follows:

* Each user \(i\) runs the randomizer of \(\mathbb{A}\) on \(x_{i}\) to get a response \(o_{i}\) and sends it to the analyst.
* For each \(j\in[k]\), the analyst _simulates_ running the randomizer of \(\mathbb{A}\) on \(y_{1}=\dots=y_{n}=j\) to get responses \(o_{1}^{\prime},\dots,o_{n}^{\prime}\). Then, the analyst lets \(\hat{z}_{j}\) be the estimator of \(\mathbb{A}\) based on the responses \(o_{1},\dots,o_{n},o_{1}^{\prime},\dots,o_{n}^{\prime}\).
* The analyst then outputs \((\hat{z}_{1},\dots,\hat{z}_{k})\) as its estimate.

In other words, the randomized responses from \(\mathbb{A}\) are used as an "oracle" for \(\mathbb{A}^{\prime}\) to compute the different linear queries. The key observation here is that, when we set \(y_{1}=\dots=y_{n}=j\), \(\mathbb{A}\) produces an estimate for \(h_{\mathbf{y}}^{T}Wh_{\mathbf{x}}=\mathbf{1}_{j}^{T}Wh_{\mathbf{x}}=(Wh_{ \mathbf{x}})_{j}\) as desired. Note that this reduction only works in the non-interactive setting: if we were in the interactive setting, the responses on \(x_{1},\dots,x_{n}\) (of protocol \(\mathbb{A}\)) would have been dependent on those of \(y_{1},\dots,y_{n}\). Therefore, the first step of the reduction would have been impossible.

While this encapsulates the high-level ideas of our proof, there are some details that needs to be handled. E.g., if \(\mathbb{A}\) outputs the quadratic form \(h_{\mathbf{x},\mathbf{y}}^{T}Wh_{\mathbf{x},\mathbf{y}}\), there are "cross terms" of the form \(h_{\mathbf{x}}^{T}Wh_{\mathbf{x}}\) that need to be removed. We formally describe and analyze the full reduction in Section 4.

**Black-Box Reduction to Linear Queries (Algorithm).** We can also give a reduction in the reverse direction, although this results in an additional round of communication. Specifically, let \(\mathbb{A}^{\prime}\) be a non-interactive \(\varepsilon\)-local DP algorithm for computing linear queries of \(W\). We can construct a two-round \((2\varepsilon)\)-local DP algorithm \(\mathbb{A}\) for computing the quadratic form on \(W\) as follows.

* **First Round:** Run \(\mathbb{A}^{\prime}\) on all users to compute an estimate \((\hat{z}_{1},\dots,\hat{z}_{k})\) for \(Wh_{\mathbf{x}}\).
* **Second Round:** Each user \(j\in[n]\), sends \(o_{j}=\hat{z}_{x_{j}}+\kappa_{j}\) to the analyst where \(\kappa_{j}\) is (appropriately calibrated) Laplace noise. The analyst then outputs \(\frac{1}{n}(o_{1}+\dots+o_{n})\).

Again, we omit some details for simplicity, such as the fact that each \(\hat{z}_{j}\) may not be bounded a priori, which may make the second step violate DP. However, these are relatively straightforward to handle.

If there were no noise, we would have \(o_{j}=\mathbf{1}_{x_{j}}^{T}Wh_{\mathbf{x}}\) and thus \(\frac{1}{n}\left(o_{1}+\dots+o_{n}\right)=h_{\mathbf{x}}^{T}Wh_{\mathbf{x}}\) as desired. Furthermore, it is not hard to see that the error from \(\kappa_{1},\dots,\kappa_{n}\) is dominated by the error from \(\mathbb{A}^{\prime}\) in the first step. In other words, we get an error similar to the one in Theorem 5 here, but the protocol is interactive (two-round). Making the protocol non-interactive requires us to step away from the black-box approach and open up the linear query algorithm (Theorem 4).

**White-Box Algorithms.** For simplicity, in this section we describe protocols with accuracy that depends on \(\gamma_{2}(W)\), which can be larger than \(\zeta(W,n)\). The desired error dependence on \(\zeta(W,n)\) can be obtained from this bound via a reduction, as shown in Lemma 10.

To understand our algorithm, we first describe the _matrix mechanism_ for linear queries (cf. [1]). That algorithm works as follows: factorize \(W=L^{T}R\). Then, user \(i\) sendswhere \(z_{i}^{R}\) is a appropriately selected random noise. In other words, each user privatizes \(W\mathbf{1}_{x_{i}}\) and sends it to the analyst. Finally, the analyst outputs \(L^{T}\left(\frac{1}{n}(o_{1}^{R}+\cdots+o_{n}^{R})\right)\). This is exactly equal to \(Wh_{\mathbf{x}}+L^{T}Z^{R}\) where \(Z^{R}:=\frac{1}{n}(z_{1}^{R}+\cdots+z_{n}^{R})\). It is possible to select the noise in such a way that \(Z^{R}\) is \((\left\|R\right\|_{1\to 2}\!/\!\varepsilon\sqrt{n})\)-sub-Gaussian. This leads to an \(\mathrm{mMSE}\) of \(O\left(\frac{\gamma_{2}(W)^{2}}{\varepsilon^{2}n}\right)\).

This suggests a natural approach for quadratic forms: in addition to sending (a privatized version of) \(R\mathbf{1}_{x_{i}}\), the user sends a privatized version of \(L\mathbf{1}_{x_{i}}\), i.e., \(o_{i}^{L}=L\mathbf{1}_{x_{i}}+z_{i}^{L}\) where \(z_{i}^{L}\) is a random noise, to the analyst. The analyst then outputs \(\left\langle\frac{1}{n}(o_{1}^{L}+\cdots+o_{n}^{L}),\frac{1}{n}(o_{1}^{R}+ \cdots+o_{n}^{R})\right\rangle\). Letting \(Z^{L}:=\frac{1}{n}(z_{1}^{L}+\cdots+z_{n}^{L})\), the output can be written as \(h_{\mathbf{x}}^{T}Wh_{\mathbf{x}}+\left\langle Lh_{\mathbf{x}},Z^{R}\right\rangle +\left\langle Rh_{\mathbf{x}},Z^{L}\right\rangle+\left\langle Z^{L},Z^{R}\right\rangle\). There are three error terms: \(\left\langle Lh_{\mathbf{x}},Z^{R}\right\rangle,\left\langle Rh_{\mathbf{x}},Z ^{L}\right\rangle\), and \(\left\langle Z^{L},Z^{R}\right\rangle\). Similar to linear queries analysis, it is not hard to see that the first two terms contribute \(O\left(\frac{\gamma_{2}(W)^{2}}{\varepsilon^{2}n}\right)\) to the \(\mathrm{MSE}\). Unfortunately, the last term is problematic for us: a simple calculation shows that it contributes \(O\left(\frac{\ell}{\varepsilon^{4}n^{2}}\right)\) to the \(\mathrm{MSE}\), where \(\ell\) denotes the number of rows of \(L,R\). A priori, this term can be quite large as there is no obvious bound on \(\ell\). In fact, if \(W\) is full rank (which is the case for most popular pairwise statistics), then we know that \(\ell\) must be at least \(k\). This leads to an undesired error term \(O\left(\frac{k}{\varepsilon^{4}n^{2}}\right)\), which dominates the first term for small-to-moderate values of \(n\), i.e., when \(n\leq\frac{k}{\varepsilon^{2}\gamma_{2}(W)^{2}}\).

To overcome this, we observe that, if we only look for _approximate factorization_ (in the same sense as \(\gamma_{2}(W;\alpha)\) defined above), then it is always possible to reduce \(\ell\) via dimensionality reduction techniques (e.g., [1]). Namely, we may pick a (e.g., random Gaussian) matrix \(A\) and replace \(L,R\) with \(AL,AR\) respectively. Selecting the number of rows of \(A\) appropriately then yields Theorem 5.

**Additional Interactive Algorithms.** To describe our algorithm, let us start by instantiating the above black-box two-round reduction using the matrix mechanism. In this context, the reduction yields the following algorithm:

* **First Round:** Each user \(i\) sends \(o_{i}^{L}=L\mathbf{1}_{x_{i}}+z_{i}^{L}\) to the analyst, where \(z_{i}^{L}\) is appropriately selected random noise. The analyst then broadcasts \(O^{L}=\frac{1}{n}(o_{1}^{L}+\cdots+o_{n}^{L})\) to each user.
* **Second Round:** Each user \(j\in[n]\) sends \(o_{j}=\left\langle O^{L},R\mathbf{1}_{x_{j}}\right\rangle+\kappa_{j}\) to the analyst, where \(\kappa_{j}\) is appropriately calibrated Laplace noise. The analyst then outputs \(\frac{1}{n}(o_{1}+\cdots+o_{n})\).

It is not hard to see that the \(\kappa_{j}\) noise terms together contribute at most \(O(1/\varepsilon^{2}n)\) to the \(\mathrm{MSE}\). Therefore, the main noise comes from the first step. Similar to the previous discussion, this noise can be written as \(\left\langle Rh_{\mathbf{x}},Z^{L}\right\rangle\) where \(Z^{L}:=\frac{1}{n}(z_{1}^{L}+\cdots+z_{n}^{L})\). The contribution of this noise to the \(\mathrm{MSE}\) is then \(O\left(\frac{\gamma_{2}(W)^{2}}{\varepsilon^{2}n}\right)\). The \(\gamma_{2}^{2}(W)\) term shows up in the error because \(\left\|Rh_{\mathbf{x}}\right\|_{2}\) can be as large as \(\left\|R\right\|_{1\to 2}\). The idea motivating our improvement is simple: Can we replace the \(Rh_{\mathbf{x}}\) term with a term that is much smaller?

This brings us to the following strategy. We will use an additional round at the beginning to compute a rough estimate \(\mu\) of \(Rh_{\mathbf{x}}\). Using the so-called _projection mechanism_[1], it is possible to compute \(\mu\) such that \(\left\|Rh_{\mathbf{x}}-\mu\right\|\leq\frac{(\gamma_{2}(W)\cdot(\log k)/ \varepsilon)^{O(i)}}{n^{2\Omega(\Gamma)}}\). The subspace orthogonal to \(\mu\) can be processed in a similar manner as before, but now the error term will just be \(\left\langle Rh_{\mathbf{x}}-\mu,Z^{L}\right\rangle\). Since \(\left\|Rh_{\mathbf{x}}-\mu\right\|\) is now much smaller (approaches \(0\) as \(n\to\infty\)), this gives us the improved error bound. We note that the direction of \(\mu\) can be handled by having each user \(i\) directly send \(\left\langle o_{i}^{L},\mu\right\rangle\) plus an appropriately calibrated noise. This summarizes the high-level idea of our approach.

Due to space constraints, we focus on the non-interactive algorithms in the main body and defer the proof of the interactive algorithm to the Appendix.

## 2 Preliminaries

**Differential Privacy.** For \(\varepsilon\geq 0\), an algorithm \(\mathcal{M}\) is _\(\epsilon\)-DP_ if for every pair \(X,X^{\prime}\) of inputs that differ on one user's input and for every possible output \(o\), \(\Pr[\mathcal{M}(X)=o]\leq e^{\varepsilon}\cdot\Pr[\mathcal{M}(X^{\prime})=o]\).

An algorithm \(\mathbb{A}\) in the local DP model consists of a randomizer \(\mathcal{R}\) and an analyst, computed as \(\mathbb{A}(X)=\operatorname{analyst}(\mathcal{R}(x_{1}),\ldots,\mathcal{R}(x_{n}))\) for input \(X=\{x_{1},\ldots,x_{n}\}\). \(\mathbb{A}\) is said to be (non-interactive) \(\varepsilon\)-local DP if \(\mathcal{R}\) is \(\varepsilon\)-DP.

A real-valued random variable \(Z\) is \(\sigma\)-sub-Gaussian iff \(\mathbb{E}[\exp(Z^{2}/\sigma^{2})]\leq 2\). A \(\mathbb{R}^{d}\)-valued random variable \(Z\) is \(\sigma\)-sub-Gaussian iff \(\langle\theta,Z\rangle\) is \(\sigma\)-sub-Gaussian for all unit vectors \(\theta\in\mathbb{R}^{d}\).

**Theorem 9** ([19]).: _For any \(C,\varepsilon>0\), there is a (non-interactive) \(\varepsilon\)-local DP algorithm \(\operatorname{VRand}_{\varepsilon,C}\) that takes in \(x\in\mathbb{R}^{d}\) such that \(\|x\|_{2}\leq C\) and outputs \(Y\in\mathbb{R}^{d}\) such that \(\mathbb{E}[Y]=x\) and \(Y-x\) is \(\sigma\)-sub-Gaussian for \(\sigma=O(C/\varepsilon)\)._

Error: Factorization vs Approximate-Factorization.We show that it suffices to give errors in terms of \(\gamma_{2}(W)\) instead of \(\zeta(W;n)\); this will be convenient for our subsequent proofs. Due to space constraints, the proof of the following statement is deferred to Appendix A.

**Lemma 10**.: _Suppose that, for all \(W\in\mathbb{R}^{k\times k}\), there is a non-interactive \(\varepsilon\)-local DP protocol \(\mathbb{A}\) for quadratic form on \(W\) with \(\operatorname{MSE}O\left(c(n,\varepsilon,k)\cdot\frac{\gamma_{2}(W)^{2}}{ \varepsilon^{2}n}\right)\) where \(c(n,\varepsilon,k)\geq\Omega(1)\). Then there is also a non-interactive \(\varepsilon\)-local DP protocol \(\mathbb{A}^{\prime}\) with \(\operatorname{MSE}O\left(c(n,\varepsilon,k)\cdot\frac{\zeta(W;n)^{2}}{ \varepsilon^{2}n}\right)\)._

## 3 Non-Interactive Algorithm

In this section, we prove Theorem 5. To do so, let us start by defining (approximate) _rank-restricted factorization norms5_, which are the same as \(\gamma_{2}(W),\gamma_{2}(W;\alpha)\) except we now restrict the number of rows of \(L,R\) to be at most \(\ell\):

Footnote 5: These are not actually norms, but we use the term for consistency with other similar quantities.

* For \(W\in\mathbb{R}^{k\times k},\ell\in\mathbb{N}\), let \(\gamma_{2}^{\ell}(W):=\min_{L^{T}R=W;L,R\in\mathbb{R}^{\ell\times k}}\|L\|_{1 \to 2}\|R\|_{1\to 2}\).
* For \(W\in\mathbb{R}^{k\times k},\ell\in\mathbb{N}\) and \(\alpha\geq 0\), let \(\gamma_{2}^{\ell}(W;\alpha):=\min_{\|W-W\|_{\infty}\leq\alpha}\gamma_{2}^{\ell }(\tilde{W})\).

We can now use the approximate rank-restricted factorization to perform the algorithm as outlined in Section 1.2 with an error term that depends on \(\ell\) (and \(\alpha\)):

**Lemma 11**.: _For any \(W\in\mathbb{R}^{k\times k},\ell\in\mathbb{N}\), and \(\alpha\geq 0\), there is a non-interactive \(\varepsilon\)-local DP algorithm that estimates the quadratic form on \(W\) to within an MSE of \(O\left(\alpha^{2}+\gamma_{2}^{\ell}(W;\alpha)^{2}\cdot\left(\frac{1}{ \varepsilon^{2}n}+\frac{\ell}{\varepsilon^{2}n^{2}}\right)\right)\)._

Proof.: By definition of \(\gamma_{2}^{\ell}(W;\alpha)\), there exists \(\tilde{W}\in\mathbb{R}^{k\times k},L,R\in\mathbb{R}^{\ell\times k}\) such that \(\|\tilde{W}-W\|_{\infty}\leq\alpha\) and \(\tilde{W}=L^{T}R\) where, w.l.o.g., by rescaling, \(\|L\|_{1\to 2}=\|R\|_{1\to 2}=\sqrt{\gamma_{2}^{\ell}(W)}=:C\).

**Algorithm Description.** Let \(\bar{\varepsilon}=\varepsilon/2\). The algorithm works as follows:

* Each user \(i\in[n]\) sends \(y_{i}^{L}\leftarrow\operatorname{VRand}_{\varepsilon,C}(L\mathbf{1}_{x_{i}})\) and \(y_{i}^{R}\leftarrow\operatorname{VRand}_{\varepsilon,C}(R\mathbf{1}_{x_{i}})\) to the analyst (where \(\operatorname{VRand}.(\cdot)\) is from Theorem 9).
* The analyst outputs \(\left\langle\frac{1}{n}\sum_{i\in[n]}y_{i}^{L},\frac{1}{n}\sum_{i\in[n]}y_{i}^ {R}\right\rangle\).

As each user runs an \((\varepsilon/2)\)-DP randomizer twice, the algorithm is \(\varepsilon\)-DP.

**Utility Analysis.** Let \(z_{i}^{L}=y_{i}^{L}-L\mathbf{1}_{x_{i}}\) and \(z_{i}^{R}=y_{i}^{R}-R\mathbf{1}_{x_{i}}\). From Theorem 9, \(z_{i}^{L},z_{i}^{R}\) are zero-mean and \(\sigma\)-sub-Gaussian for \(\sigma=O(C/\varepsilon)\). Let \(Z^{L}:=\frac{1}{n}\sum_{i\in[n]}z_{i}^{L},Z^{R}:=\frac{1}{n}\sum_{i\in[n]}z_{i }^{R}\); we then have that these are zero-mean and \(\sigma^{\prime}\)-sub-Gaussian for \(\sigma^{\prime}=\sigma/\sqrt{n}\). The MSE of the protocol is given by

\[\mathbb{E}\left[\left(\left\langle\frac{1}{n}\sum_{i\in[n]}y_{i} ^{L},\frac{1}{n}\sum_{i\in[n]}y_{i}^{R}\right\rangle-h_{\mathbf{x}}^{T}Wh_{ \mathbf{x}}\right)^{2}\right]\] \[=\mathbb{E}\left[\left(h_{\mathbf{x}}^{T}(\tilde{W}-W)h_{ \mathbf{x}}+\left\langle Z^{L},Rh\right\rangle+\left\langle Lh,Z^{R}\right \rangle+\left\langle Z^{L},Z^{R}\right\rangle\right)^{2}\right]\] \[\lesssim\mathbb{E}(h_{\mathbf{x}}^{T}(\tilde{W}-W)h_{\mathbf{x} })^{2}+\mathbb{E}\left\langle Z^{L},Rh\right\rangle^{2}+\mathbb{E}\left\langle Lh,Z^{R}\right\rangle^{2}+\mathbb{E}\left\langle Z^{L},Z^{R}\right\rangle^{2}\] \[\lesssim\|\tilde{W}-W\|_{\infty}^{2}+(\sigma^{\prime})^{2}\|Rh\|_{ 2}^{2}+(\sigma^{\prime})^{2}\|Lh\|_{2}^{2}+\ell\cdot(\sigma^{\prime})^{4}\]\[\leq\alpha^{2}+(\sigma^{\prime})^{2}\|R\|_{1\to 2}^{2}+(\sigma^{\prime})^{2}\|L\|_{1\to 2}^{2}+\ell\cdot(\sigma^{\prime})^{4}\] \[\lesssim\alpha^{2}+(\sigma^{\prime})^{2}C^{2}+\ell\cdot(\sigma^{ \prime})^{4}\] \[\lesssim\alpha^{2}+\frac{C^{4}}{\varepsilon^{2}n}+\frac{\ell\cdot C ^{4}}{\varepsilon^{4}n^{2}}.\qed\]

**Rank-Restricted Approximate Factorization via JL.** We next show that w.l.o.g. we can take \(\ell\) to be quite small in Lemma 11 above.

**Lemma 12**.: _Let \(W\in\mathbb{R}^{k\times k}\) and \(\alpha\in(0,\gamma_{2}(W))\), there is \(\ell\lesssim\frac{\gamma_{2}(W)^{2}\log k}{\alpha^{2}}\) with \(\gamma_{2}^{\ell}(W;\alpha)\lesssim\gamma_{2}(W)\)._

As mentioned earlier, this lemma is proved by applying Johnson-Lindenstrauss (JL) dimensionality reduction to each column of \(L,R\). We summarize a simplified version of the JL lemma below.

**Lemma 13** (Johnson-Lindenstrauss Lemma (e.g., [3])).: _Let \(\beta\in(0,1)\) and \(U=\{u_{1},\ldots u_{m}\}\subseteq\mathbb{R}^{d}\). For some \(\ell\leq O(\beta^{-2}\log m)\), there exists a matrix \(A\in\mathbb{R}^{\ell\times d}\) such that for all \(u,v\in U\), we have \((1-\beta)\|u-v\|_{2}^{2}\leq\|Au-Av\|_{2}^{2}\leq(1+\beta)\|u-v\|_{2}^{2}\)._

We are now ready to prove Lemma 12.

Proof of Lemma 12.: By definition of \(\gamma_{2}(W)\), there exists \(L,R\in\mathbb{R}^{d\times k}\) for some \(d\in\mathbb{N}\) such that \(W=L^{T}R\) where\(\|L\|_{1\to 2}=\|R\|_{1\to 2}=\sqrt{\gamma_{2}(W)}=:C\).

Let \(L_{1},\ldots,L_{k}\) (resp. \(R_{1},\ldots,R_{k}\)) be the columns of \(L\) (resp. \(R\)). Consider \(U=\{0,L_{1},\ldots,L_{k},R_{1},\ldots,R_{k},-L_{1},\ldots,-L_{k},-R_{1},\ldots, -R_{k}\}\) and \(\beta=0.5\alpha/C^{2}\); let \(\ell=O\left(\beta^{-2}\log k\right)=O\left(\frac{\gamma_{2}(W)^{2}\log k}{ \alpha^{2}}\right)\) and \(A\in\mathbb{R}^{\ell\times d}\) be as guaranteed by Lemma 13.

Let \(\tilde{L}=AL,\tilde{R}=AR\), and \(\tilde{W}=\tilde{L}^{T}\tilde{R}\). For all \(i\in[k]\), we have \(\|\tilde{L}_{i}\|_{2}^{2}\leq(1+\beta)\|L_{i}\|_{2}^{2}\) and \(\|\tilde{R}_{i}\|_{2}^{2}\leq(1+\beta)\|R_{i}\|_{2}^{2}\). Therefore, \(\|\tilde{L}\|_{1\to 2},\|\tilde{R}\|_{1\to 2}\leq O(C)\), i.e., \(\gamma_{2}^{\ell}(\tilde{W})\leq O(\gamma_{2}(W))\). Moreover, for each \(i,j\in[k]\), we have

\[\left|\tilde{W}_{i,j}-W_{i,j}\right| =\left|\left\langle\tilde{L}_{i},\tilde{R}_{j}\right\rangle- \left\langle L_{i},R_{j}\right\rangle\right|\] \[\leq\frac{1}{4}\left(\left\|\|\tilde{L}_{i}+\tilde{R}_{j}\|_{2}^ {2}-\|L_{i}+R_{j}\|_{2}^{2}\right|+\left\|\|\tilde{L}_{i}-\tilde{R}_{j}\|_{2}^ {2}-\|L_{i}-R_{j}\|_{2}^{2}\right\|\right)\] \[\leq\frac{\beta}{4}\left(\|L_{i}+R_{j}\|_{2}^{2}+\|L_{i}-R_{j}\|_ {2}^{2}\right)\] \[\leq 2\beta C^{2}\leq\alpha.\]

Thus, \(\|\tilde{W}-W\|_{\infty}\leq\alpha\) and therefore \(\gamma_{2}^{\ell}(W;\alpha)\leq O(\gamma_{2}(W))\) as claimed. 

We end this section by proving Theorem 5, which is a simple combination of Lemma 11 and Lemma 12.

Proof of Theorem 5.: Pick6\(\alpha=\frac{\gamma_{2}(W)}{\varepsilon\sqrt{n}}\) and apply Lemma 12: There exists \(\ell=O\left(\frac{\gamma_{2}(W)^{2}\log k}{\alpha^{2}}\right)=O\left((\log k) \varepsilon^{2}n\right)\) such that \(\gamma_{2}^{\ell}(W;\alpha)\leq O(\gamma_{2}(W))\). Plugging this back into Lemma 11 then gives us a non-interactive \(\varepsilon\)-local DP protocol with MSE

Footnote 6: We may assume w.l.o.g. that \(\alpha\leq\gamma_{2}(W)\); otherwise, the guarantee in Theorem 5 is trivial, i.e., always outputting 0 satisfies the bound.

\[\lesssim\alpha^{2}+\gamma_{2}^{\ell}(W;\alpha)^{2}\cdot\left(\frac{1}{ \varepsilon^{2}n}+\frac{\ell}{\varepsilon^{4}n^{2}}\right)\lesssim\gamma_{2}(W )^{2}\left(\frac{1}{\varepsilon^{2}n}+\frac{\log k}{\varepsilon^{2}n}\right) \lesssim\frac{\gamma_{2}(W)^{2}\log k}{\varepsilon^{2}n}.\]

Applying Lemma 10 then concludes the proof. 

## 4 Lower Bounds for Non-Interactive Algorithms

In this section we formalize the reduction from linear queries to computing quadratic forms, as outlined in Section 1.2. The properties of the reduction are stated in the theorem below.

**Theorem 14**.: _Let \(W\in\mathbb{R}^{k\times k}\) be symmetric. Suppose that there is a non-interactive \(\varepsilon\)-local DP mechanism for computing the quadratic form on \(W\) with \(\mathrm{MSE}\) at most \(\alpha(\varepsilon,n)\). Then there is a non-interactive \(\varepsilon\)-local DP protocol for computing the linear queries of \(W\) with \(\mathrm{mMSE}\)\(O(\alpha(\varepsilon/2,n)+\alpha(\varepsilon/2,2n))\)._

Theorem 14 and the lower bound in Theorem 4 immediately imply Theorem 6. In the proof below, we use subscripts \(\varepsilon,n\) to denote the privacy loss parameter and the number of users in the protocol.

Proof of Theorem 14.: Let \(\mathbb{A}_{\varepsilon,n}\) be the \(\varepsilon\)-local DP protocol for computing the quadratic form on \(W\), and let \(\mathcal{R}_{\varepsilon,n}\) denote its randomizer. We construct an algorithm \(\mathbb{A}_{\varepsilon,n}^{\prime}\) for linear queries with workload matrix \(W\). On input \(x_{1},\ldots,x_{n}\), proceed as follows:

* Run the protocol of \(\mathbb{A}_{\varepsilon/2,n}\) on \(x_{1},\ldots,x_{n}\) to compute an estimate \(\hat{z}\) for \(h_{\mathbf{x}}^{T}\mathbb{A}h_{\mathbf{x}}\).
* In the same round as above, run the randomizer \(\mathcal{R}_{\varepsilon/2,2n}\) of \(\mathbb{A}_{\varepsilon/2,2n}\) on \(x_{1},\ldots,x_{n}\) to get the responses \(\mathcal{R}_{\varepsilon/2,2n}(x_{1}),\ldots,\mathcal{R}_{\varepsilon/2,2n}(x _{n})\).
* For each \(j\in[k]\), do the following:
* Run the randomizer \(\mathcal{R}\) of \(\mathbb{A}\) on \(y_{1}=\cdots=y_{n}=j\) to get the responses \(\mathcal{R}_{\varepsilon/2,2n}(y_{1})\),..., \(\mathcal{R}_{\varepsilon/2,2n}(y_{n})\).
* Compute the estimator \(z_{j}^{\prime}\) of \(\mathbb{A}\) on the \(2n\) responses \(\mathcal{R}_{\varepsilon/2,2n}(x_{1}),\ldots,\mathcal{R}_{\varepsilon/2,2n}(x _{n})\), \(\mathcal{R}_{\varepsilon/2,2n}(y_{1}),\ldots,\mathcal{R}_{\varepsilon/2,2n}(y_{ n})\).
* Set \(\hat{z}_{j}=2z_{j}^{\prime}-\frac{1}{2}\hat{z}-\frac{1}{2}\mathbf{1}_{j}^{T}W \mathbf{1}_{j}\).
* Output \((\hat{z}_{1},\ldots,\hat{z}_{j})\) as the estimates for the linear queries.

Since \((\varepsilon/2)\)-local DP randomizers are run on each input \(x_{i}\) twice, the basic composition theorem implies that this is a \(\varepsilon\)-local DP algorithm as desired.

For each \(j\in[k]\), we now compute the \(\mathrm{MSE}\) of \(\hat{z}_{j}\). First, observe that

\[(Wh_{\mathbf{x}})_{j}=\mathbf{1}_{j}^{T}Wh_{\mathbf{x}}=2\left(h_{\mathbf{x} \cup\mathbf{y}}^{T}Wh_{\mathbf{x}\cup\mathbf{y}}\right)-\frac{1}{2}h_{\mathbf{ x}}^{T}Wh_{\mathbf{x}}^{T}-\frac{1}{2}\mathbf{1}_{j}^{T}W\mathbf{1}_{j},\]

where \(\mathbf{y}\) denotes the dataset with \(n\) copies of \(j\).

Therefore, we can bound the \(\mathrm{MSE}\) of \(\hat{z}_{j}\) as follows:

\[\mathrm{MSE}(\hat{z}_{j};(Wh_{\mathbf{x}})_{j}) =\mathbb{E}\left[(\hat{z}_{j}-(Wh_{\mathbf{x}})_{j})^{2}\right]\] \[=\mathbb{E}\left[\left(2\left(z_{j}^{\prime}-h_{\mathbf{x}\cup \mathbf{y}}^{T}Wh_{\mathbf{x}\cup\mathbf{y}}\right)+\frac{1}{2}\left(\hat{z}-h _{\mathbf{x}}^{T}Wh_{\mathbf{x}}^{T}\right)\right)^{2}\right]\] \[\lesssim\mathbb{E}\left[\left(z_{j}^{\prime}-h_{\mathbf{x}\cup \mathbf{y}}^{T}Wh_{\mathbf{x}\cup\mathbf{y}}\right)^{2}\right]+\mathbb{E}\left[ \left(\hat{z}-h_{\mathbf{x}}^{T}Wh_{\mathbf{x}}^{T}\right)^{2}\right]\] \[\leq\alpha(\varepsilon/2,2n)+\alpha(\varepsilon/2,n),\]

where the last inequality follows from the guarantees of \(\mathbb{A}\). Thus, the \(\mathrm{mMSE}\) of \(\mathbb{A}^{\prime}\) is at most \(O(\alpha(\varepsilon/2,2n)+\alpha(\varepsilon/2,n))\), as desired. 

## 5 Upper Bounds for Specific Metrics

In this section, we obtain concrete upper bounds for many well-known U-statistics of degree 2. For a kernel \(f:\mathcal{X}\rightarrow\mathbb{R}\), let \(W^{f}\in\mathbb{R}^{\mathcal{X}\times\mathcal{X}}\) denote the matrix defined by \(W^{f}_{x,x^{\prime}}=f(x,x^{\prime})\). Our proof of Corollary 7 proceeds by providing an upper bound on \(\gamma_{2}(W^{f})\) for each U-statistic with kernel \(f\); the bounds immediately follow from Theorem 5. Similar to before, let \(k\) denote \(|\mathcal{X}|\).

The following (non-trivial) facts about the factorization norm are useful to keep in mind:

**Fact 15**.: _The factorization norm \(\gamma_{2}\) satisfies the following properties:_

1. _[_18_]_ _For any_ \(A,B\)_, we have_ \(\gamma_{2}(A+B)\leq\gamma_{2}(A)+\gamma_{2}(B)\)_._
2. _[_20_]_ _For any_ \(A,B\)_,_ \(\gamma_{2}(A\otimes B)=\gamma_{2}(A)\cdot\gamma_{2}(B)\)

**Gini's diversity index.** For \(\mathcal{X}\subseteq\mathbb{R}\), the kernel \(f(x,x^{\prime})=\mathbf{1}[x\neq x^{\prime}]\) captures _Gini's diversity index_. From Fact 15(1), we have \(\gamma_{2}(W^{f})\leq\gamma_{2}(\mathbf{1}_{k\times k})+\gamma_{2}(\mathbf{I}_ {k\times k})\leq 1+1\) where the inequality \(\gamma_{2}(\mathbf{1}_{k\times k})\leq 1\) is from the factorization \(L=R=\mathbf{1}_{k}\) and \(\gamma_{2}(\mathbf{I}_{k\times k})\leq 1\) is from \(L=R=\mathbf{I}_{k\times k}\).

**Kendall's \(\boldsymbol{\tau}\) coefficient and AUC.** For \(\mathcal{X}=A\times B\subseteq\mathbb{R}^{2}\) with \(x_{i}=(y_{i},z_{i})\), the kernel \(f((y_{i},z_{i}),(y_{j},z_{j}))=\operatorname{sgn}(y_{i}-y_{j})\cdot \operatorname{sgn}(z_{i}-z_{j})\) yields _Kendall's \(\tau\) coefficient_. Let \(U_{m}\in\{-1,1\}^{m}\) denote the matrix that has \(+1\) on all entries above the main diagonal (inclusive) and \(-1\) elsewhere. It is known that \(\gamma_{2}(U_{m})=\Theta(\log m)\). We can bound \(\gamma_{2}(W^{f})\) for Kendall's \(\tau\) coefficient by observing that \(W^{f}=U_{A}\otimes U_{B}\). From Fact 15(2), \(\gamma_{2}(W_{f})\leq\gamma_{2}(U_{A})\cdot\gamma_{2}(U_{B})\lesssim(\log|A|) \,(\log|B|)\leq(\log k)^{2}\).

_AUC_ for binary classification is defined in a similar manner as Kendall's tau coefficient, except that (i) \(B=\{0,1\}\) and (ii) the normalization constant being \(\frac{1}{n^{+}n^{-}}\) instead if \(\frac{1}{\binom{2}{2}}\) where \(n^{+}\) (resp., \(n^{-}\)) denotes the number of 1-labeled (resp., 0-labeled) examples. The AUC result follows from the above since \(|B|=2\) in this case. We remark that, for the AUC case, we also have to split the privacy budget and use half of it to estimate \(n^{+}\) so that we can renormalize correctly. It is not hard to see that this renormalization procedure results in at most an additive factor of \(O\left(\frac{1}{\varepsilon^{n}n}\right)\) in the \(\operatorname{MSE}\), under the _balancedness assumption_ that \(n^{-},n^{+}\geq\Omega(n)\).

**Lipschitz Losses.** Let \(\mathcal{X}=[0,1]\) and let \(f:\mathcal{X}\to\mathbb{R}\) be any function such that \(|f(x)-f(x^{\prime})|\leq G\cdot|x-x^{\prime}|\); we call \(f\)_G-Lipschitz_. This class includes U-statistics such as the _Gini's mean difference_, which is given by the kernel \(f(x_{i},x_{j})=|x_{i}-x_{j}|\), which is 1-Lipschitz.

Similar to [2], we use a discrete case where \(\mathcal{X}=[k]\) as a subroutine. Our algorithm for this is stated below. Note that Corollary 16 immediately implies the bound for the continuous case: given any function \(f:[0,1]\to\mathbb{R}\), we may select \(k\) to be sufficiently large, e.g., \(k=\Theta(\varepsilon n^{2})\), and discretize the function over the points \(1/k,2/k,\ldots,k/k\). Defining \(g:[k]\to\mathbb{R}\) by \(g(i)=f(i/k)\) allows us to use Corollary 16 with Lipschitz constant \(G/k\). This leads to a \(\operatorname{MSE}\) of \(O\left(\frac{G^{2}\log(\varepsilon n^{2})}{\varepsilon^{2}n}\right)\). The \(\operatorname{MSE}\) resulting from the discretization error is then at most \(\frac{G^{2}n^{2}}{k^{2}}<\frac{G^{2}}{\varepsilon^{2}n}\).

**Corollary 16** (Discrete Lipschitz).: _Assuming \(\mathcal{X}=[k]\) and that \(f\) is \(G\)-Lipschitz. There is a non-interactive \(\varepsilon\)-local DP algorithm for computing pairwise statistics for \(f\) with \(\operatorname{MSE}O\left(\frac{G^{2}k^{2}(\log k)}{\varepsilon^{2}n}\right)\)._

Again, the above corollary follows from Theorem 5 and the following factorization.

**Lemma 17**.: _Assuming that \(f:[k]\to\mathbb{R}\) is \(G\)-Lipschitz, then \(\gamma_{2}(W^{f})\leq O(Gk)\)._

Proof.: We assume w.l.o.g. that \(k=2^{q}-1\) for some \(q\in\mathbb{N}\) and \(\|W^{f}\|_{\infty}\leq Lk\); otherwise, we may shift \(W^{f}\) (and the answer) without incurring any additional error. We arrange \([k]\) into a balanced binary search tree \(\mathcal{T}\) of depth \(q-1\) naturally (where the root is \(2^{q-1}\) and the leaves are \(1,3,\ldots,k\)). Let \(P(j)\) denote the path from node \(j\) to the root (inclusive) in \(\mathcal{T}\), and let \(\ell(j)\) denote the depth of \(j\) (where the root has depth 0). Furthermore, let \(\operatorname{parent}(j)\) denote the parent of \(j\) in \(\mathcal{T}\). For notational convenience, let \(\operatorname{parent}(2^{q-1})=\perp\) and let \(f(i,\perp)=0\) for all \(i\in[k]\).

We construct \(L,R\in\mathbb{R}^{k\times k}\) as follows.

* For all \(i,j\in[k]\), let \(R_{i,j}=\left(\frac{5}{6}\right)^{\ell(i)}\mathbf{1}[i\in P(j)]\).
* For all \(i,j\in[k]\), \(L_{j,i}=\left(\frac{6}{5}\right)^{\ell(j)}(f(i,j)-f(i,\operatorname{parent}(j)))\).

For \(i,j\in[k]\), we have \((L^{T}R)_{i,j}=\sum_{t\in P(j)}(f(i,t)-f(i,\operatorname{parent}(t)))=f(i,j)\). Thus, \(L^{T}R=W\).

Furthermore, \(\|R\|_{1\to 2}^{2}=1+\left(\frac{5}{6}\right)^{2}+\cdots+\left(\frac{5}{6} \right)^{2(q-1)}\lesssim 1\). Meanwhile, we can bound \(\|L\|_{1\to 2}^{2}\) by

\[\max_{i\in[k]}\sum_{j\in[k]}\left(\left(\frac{6}{5}\right)^{\ell(j)}(f(i,j)-f(i,\operatorname{parent}(j)))\right)^{2}\lesssim\sum_{d=0}^{q-1}2^{d}\left(\left( \frac{6}{5}\right)^{d}\cdot\frac{Gk}{2^{d}}\right)^{2}\lesssim G^{2}k^{2},\]

where the first inequality follows since \(f\) is \(G\)-Lipschitz. Thus \(\gamma_{2}(W^{f})\leq O(Gk)\).

Conclusion and Open Questions

In this work, we systematically study the problem of privately computing pairwise statistics. We give a non-interactive local DP algorithm and a nearly-matching lower bound for the problem. Furthermore, we show that, for some metrics, improvements can be made if interaction is allowed.

There are several immediate questions from our work. For example, is it possible to remove the \(\log k\) multiplicative factor in our non-interactive algorithm (Theorem 5)? Similarly, can the second additive term in our interactive algorithm be removed? As also suggested by [1], an intriguing research direction is to study more complicated statistics such as the "higher-degree" ones (e.g., those involving triplets instead of pairs). It would be interesting to see if techniques from linear queries and from our work can be applied to these problems.

## References

* [Abo18] John M Abowd. The US Census Bureau adopts differential privacy. In _KDD_, pages 2867-2867, 2018.
* [ACG\({}^{+}\)16] Martin Abadi, Andy Chu, Ian J. Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _SIGSAC_, pages 308-318, 2016.
* [App17] Apple Differential Privacy Team. Learning with privacy at scale. _Apple Machine Learning Journal_, 2017.
* [BBGK20] James Bell, Aurelien Bellet, Adria Gascon, and Tejas Kulkarni. Private protocols for U-statistics in the local model and beyond. In _AISTATS_, volume 108, pages 1573-1583, 2020.
* [BBGN20] Borja Balle, James Bell, Adria Gascon, and Kobbi Nissim. Private summation in the multi-message shuffle model. In _CCS_, pages 657-676, 2020.
* [BBNS19] Jaroslaw Blasiok, Mark Bun, Aleksandar Nikolov, and Thomas Steinke. Towards instance-optimal private query release. In _SODA_, pages 2480-2497, 2019.
* [BDKT12] Aditya Bhaskara, Daniel Dadush, Ravishankar Krishnaswamy, and Kunal Talwar. Unconditional differentially private mechanisms for linear queries. In _STOC_, pages 1269-1284, 2012.
* [BHBFG\({}^{+}\)22] Gecia Bravo-Hermsdorff, Robert Busa-Fekete, Mohammad Ghavamzadeh, Andres Munoz Medina, and Umar Syed. Private and communication-efficient algorithms for entropy estimation. _NeurIPS_, 35:15382-15393, 2022.
* [BNO08] Amos Beimel, Kobbi Nissim, and Eran Omri. Distributed private data analysis: Simultaneously solving how and what. In _CRYPTO_, pages 451-468. Springer, 2008.
* [Bub15] Sebastien Bubeck. Convex optimization: Algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 8(3-4):231-357, 2015.
* [BUV18] Mark Bun, Jonathan R. Ullman, and Salil P. Vadhan. Fingerprinting codes and the price of approximate differential privacy. _SIAM J. Comput._, 47(5):1888-1938, 2018.
* [CM23] Graham Cormode and Igor L. Markov. Federated calibration and evaluation of binary classifiers. _Proc. VLDB Endow._, 16(11):3253-3265, 2023.
* [CSS12] TH Hubert Chan, Elaine Shi, and Dawn Song. Optimal lower bound for differentially private multi-party aggregation. In _ESA_, pages 277-288, 2012.
* [CSU\({}^{+}\)19] Albert Cheu, Adam D. Smith, Jonathan R. Ullman, David Zeber, and Maxim Zhilyaev. Distributed differential privacy via shuffling. In _EUROCRYPT_, pages 375-403, 2019.
* [DG03] Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of johnson and lindenstrauss. _Random Struct. Algorithms_, 22(1):60-65, 2003.
* [DKY17] Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data privately. In _NeurIPS_, pages 3571-3580, 2017.
* [DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivity in private data analysis. In _TCC_, pages 265-284, 2006.
* [EGS03] Alexandre Evfimievski, Johannes Gehrke, and Ramakrishnan Srikant. Limiting privacy breaches in privacy preserving data mining. In _PODS_, pages 211-222, 2003.
* [ENU20] Alexander Edmonds, Aleksandar Nikolov, and Jonathan R. Ullman. The power of factorization mechanisms in local and central differential privacy. In _STOC_, pages 425-438, 2020.
* [EPK14] Ulfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: Randomized aggregatable privacy-preserving ordinal response. In _CCS_, pages 1054-1067, 2014.

* [GMPV20] Badih Ghazi, Pasin Manurangsi, Rasmus Pagh, and Ameya Velingker. Private aggregation from fewer anonymous messages. In _EUROCRYPT_, pages 798-827, 2020.
* but not your data. _Wired, June_, 13, 2016.
* [HT10] Moritz Hardt and Kunal Talwar. On the geometry of differential privacy. In _STOC_, pages 705-714, 2010.
* [KLN\({}^{+}\)11] Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam D. Smith. What can we learn privately? _SIAM J. Comput._, 40(3):793-826, 2011.
* [KT18] Krishnaram Kenthapadi and Thanh T. L. Tran. Pripearl: A framework for privacy-preserving analytics and reporting at linkedin. In _CIKM_, pages 2183-2191, 2018.
* [LMH\({}^{+}\)15] Chao Li, Gerome Miklau, Michael Hay, Andrew McGregor, and Vibhor Rastogi. The matrix mechanism: optimizing linear counting queries under differential privacy. _VLDB J._, 24(6):757-781, 2015.
* [LMSS07] Nati Linial, Shahar Mendelson, Gideon Schechtman, and Adi Shraibman. Complexity measures of sign matrices. _Comb._, 27(4):439-463, 2007.
* [LSS08] Troy Lee, Adi Shraibman, and Robert Spalek. A direct product theorem for discrepancy. In _CCC_, pages 71-80, 2008.
* [Nik23] Aleksandar Nikolov. Private query release via the johnson-lindenstrauss transform. In Nikhil Bansal and Viswanath Nagarajan, editors, _SODA_, pages 4982-5002, 2023.
* [NTZ16] Aleksandar Nikolov, Kunal Talwar, and Li Zhang. The geometry of differential privacy: The small database and approximate cases. _SIAM J. Comput._, 45(2):575-616, 2016.
* [RE19] Carey Radebaugh and Ulfar Erlingsson. Introducing TensorFlow Privacy: Learning with Differential Privacy for Training Data, March 2019. blog.tensorflow.org.
* [RSP\({}^{+}\)21] Ryan Rogers, Subbu Subramaniam, Sean Peng, David Durfee, Seunghyun Lee, Santosh Kumar Kancha, Shraddha Sahay, and Parvez Ahammad. Linkedin's audience engagements API: A privacy preserving data analytics system at scale. _J. Priv. Confiden._, 11(3), 2021.
* [Sha14] Stephen Shankland. How Google tricks itself to protect Chrome user privacy. _CNET, October_, 2014.
* [TJ89] Nicole Tomczak-Jaegermann. Banach-Mazur distances and finite-dimensional operator ideals. _Pure and Applied Mathematics_, 38:395, 1989.
* [TM20] Davide Testuggine and Ilya Mironov. PyTorch Differential Privacy Series Part 1: DP-SGD Algorithm Explained, August 2020. medium.com.

Additional Preliminaries

We use \(d_{\mathrm{tv}},d_{\mathrm{KL}}\) to denote the total variation (TV) distance and the Kullback-Leibler (KL) divergence between two distributions. Pinsker's inequality states that

\[d_{\mathrm{tv}}(P,Q)\leq\sqrt{\frac{1}{2}\cdot d_{\mathrm{KL}}(P\parallel Q)}, \tag{1}\]

for all distributions \(P,Q\).

#### Interactive Local DP

For interactive local DP, we consider protocols that proceed in rounds. In each round, the analyst may send a message (which may depend on what the analyst has received in the previous rounds) to each user, who then replies back with a (randomized) response. The DP guarantee is then enforced on the view of the analyst.

#### Clipping and Resulting Error

_Clipping_ is a standard technique in DP to achieve bounded sensitivity (cf. [1]). In our interactive algorithm, we will use clipping on real values to bound their sensitivity. More specifically, for \(\tau\geq 0\), let \(\mathrm{clip}_{\tau}:\mathbb{R}\to\mathbb{R}\) denote the function:

\[\mathrm{clip}_{\tau}(x)=\begin{cases}\tau&\text{ if }x>\tau,\\ x&\text{ if }\tau\geq x\geq-\tau,\\ -\tau&\text{ if }-\tau>x.\end{cases}\]

There could be additional errors resulting from clipping. For the purpose of bounding such terms, we will use the following simple lemma:

**Lemma 18**.: _Let \(X\) denote any random variable over \(\mathbb{R}\) and any \(\tau\geq 0\). Then, we have_

\[\mathbb{E}[(X-\mathrm{clip}_{\tau}(X))^{2}]\leq\sqrt{\mathbb{E}[X^{4}]\cdot \Pr[|X|>\tau]}.\]

Proof.: We can bound the LHS term by

\[\mathbb{E}[(X-\mathrm{clip}_{\tau}(X))^{2}] \leq\mathbb{E}[X^{2}\cdot\mathbf{1}[|X|>\tau]]\] \[\leq\sqrt{\mathbb{E}[X^{4}]\cdot\mathbb{E}[\mathbf{1}[|X|>\tau]^{ 2}]}\] \[=\sqrt{\mathbb{E}[X^{4}]\cdot\Pr[|X|>\tau]},\]

where in the second step we used the Cauchy-Schwarz inequality. 

#### Factorization vs Approximate-Factorization: Proof of Lemma 10

Proof of Lemma 10.: Let \(W\) be any matrix. By definition of \(\zeta\), there exists \(\tilde{W}\) such that \(\zeta(W;n)=\gamma_{2}(\tilde{W})+\|\tilde{W}-W\|_{\infty}\cdot(\varepsilon \sqrt{n})\). The new protocol \(\mathbb{A}^{\prime}\) simply runs \(\mathbb{A}\) but on \(\tilde{W}\). Let \(\hat{z}\) denote the output of \(\mathbb{A}^{\prime}\). Its MSE can be bounded by

\[\mathbb{E}\left[\left(\hat{z}-h_{\mathbf{x}}^{T}Wh_{\mathbf{x}} \right)^{2}\right] =\mathbb{E}\left[\left(\left(\hat{z}-h_{\mathbf{x}}^{T}\tilde{W}h _{\mathbf{x}}\right)+\left(h_{\mathbf{x}}^{T}\tilde{W}h_{\mathbf{x}}-h_{ \mathbf{x}}^{T}Wh_{\mathbf{x}}\right)\right)^{2}\right]\] \[\lesssim\mathbb{E}\left[\left(\hat{z}-h_{\mathbf{x}}^{T}\tilde{ W}h_{\mathbf{x}}\right)^{2}\right]+\mathbb{E}\left[\left(h_{\mathbf{x}}^{T}\tilde{W}h _{\mathbf{x}}-h_{\mathbf{x}}^{T}Wh_{\mathbf{x}}\right)^{2}\right]\] \[\lesssim\left(c(n,\varepsilon,k)\cdot\frac{\gamma_{2}(\tilde{W} )^{2}}{\varepsilon^{2}n}\right)+\|\tilde{W}-W\|_{\infty}^{2}\] \[\lesssim c(n,\varepsilon,k)\cdot\frac{\zeta(W;n)^{2}}{\varepsilon ^{2}n}+\frac{\zeta(W;n)^{2}}{\varepsilon^{2}n}\] \[\lesssim c(n,\varepsilon,k)\cdot\frac{\zeta(W;n)^{2}}{\varepsilon ^{2}n}.\qed\]

#### Projection Mechanism

As outlined in Section 1.2, we will use the _projection mechanism_ as a subroutine for our algorithm. For \(W\in\mathbb{R}^{\ell\times k}\), we define \(W^{\Delta}\) to denote the set \(\{Wx\mid x\in\mathbb{R}^{k},\|x\|_{1}\leq 1\}\). The guarantees of the algorithm are summarized below.

**Theorem 19** ([3]).: _For any workload \(W\in\mathbb{R}^{\ell\times k}\), there is a non-interactive \(\varepsilon\)-local DP mechanism \(\mathcal{M}\) such that_

\[\mathbb{E}_{\mu\sim\mathcal{M}(\mathbf{x})}[\|\mu-Wh_{\mathbf{x}}\|_{2}^{2}] \lesssim\frac{\|W\|_{1\to 2}^{2}\sqrt{\log k}}{\varepsilon\sqrt{n}}.\]

_Moreover, the output \(\mathcal{M}(W)\) always belongs to \(W^{\Delta}\)._

We provide the proof below for completeness.

Proof of Theorem 19.: Let \(C:=\|W\|_{1\to 2}\). The algorithm works as follows:

* Each user \(i\in[n]\) sends \(y_{i}:=\operatorname{VRand}_{\varepsilon,C}(W\mathbf{1}_{x_{i}})\) to the analyst (where \(\operatorname{VRand}.(\cdot)\) is from Theorem 9).
* The analyzer computes \(Y:=\frac{1}{n}\sum_{i\in[n]}y_{i}\) and then outputs \(\mu:=\operatorname{argmin}_{u\in W^{\Delta}}\|u-Y\|_{2}\).

The privacy guarantee of the algorithm follows from Theorem 9.

Let \(z_{i}=y_{i}-W\mathbf{1}_{x_{i}}\). From Theorem 9, \(z_{i}\) is zero-mean and \(\sigma\)-sub-Gaussian for \(\sigma=O(C/\varepsilon)\). Let \(Z:=\frac{1}{n}\sum_{i\in[n]}z_{i}\); we then have that it is zero-mean and \(\sigma^{\prime}\)-sub-Gaussian for \(\sigma^{\prime}=\sigma/\sqrt{n}\). By the definition of sub-Gaussian random variable and a union bound, for every \(\beta>0\), with probability \(1-\beta\) the following holds:

\[\left|\left\langle W_{j},Z\right\rangle\right| \lesssim\frac{C^{2}}{\varepsilon\sqrt{n}}\cdot\sqrt{\log(k/\beta)} \forall j\in[k], \tag{2}\]

where \(W_{j}\) denote the \(j\)th column of \(W\).

It is well known7 that if we define \(\mu\) as we did (i.e., as projection of \(Y\) on \(W^{\Delta}\)), then we have

Footnote 7: See e.g., [3, Lemma 3.1].

\[\langle\mu-w,\mu-Y\rangle\leq 0,\]

for all \(w\in W^{\Delta}\).

Plugging in \(w=Wh_{\mathbf{x}}\), we have

\[\|\mu-Wh_{\mathbf{x}}\|_{2}^{2} =\langle\mu-Wh_{\mathbf{x}},\mu-Y\rangle+\langle\mu-Wh_{\mathbf{ x}},Z\rangle\] \[\leq\langle\mu-Wh_{\mathbf{x}},Z\rangle\] \[\leq\max_{w^{\prime}\in W^{\Delta}}\left\langle w^{\prime},Z\right\rangle\] \[=\max_{j\in[k]}|\left\langle W_{j},Z\right\rangle|,\]

where the equality follows from the fact \(W^{\Delta}\) is the convex hull of \(W_{1},\ldots,W_{k},-W_{1},\ldots,-W_{k}\).

Putting together (2) and the above then yields

\[\mathbb{E}[\|\mu-Wh_{\mathbf{x}}\|_{2}^{2}] \lesssim\frac{C^{2}\sqrt{\log k}}{\varepsilon\sqrt{n}}.\qed\]

## Appendix B On the Lower Bound for Linear Queries from [1]

In this section, we provide details on how we can interpret the bounds of [1] as stated in the form of Theorem 4. The lower bound in [1] is originally for the \(\ell_{\infty}\)-error; we make the observation below that a simple modification of their proof can be made so that it applies to \(\operatorname{mMSE}\). Note that, in addition to getting a stronger result in terms of error metric (because a lower bound on\(\mathrm{mMSE}\) implies the same lower bound on \(\ell_{\infty}\)-error), we also get a quantitatively stronger bound that does not depend on \(k\) because we avoid having to take a union bound over \(k\) queries (which was required for the proof for \(\ell_{\infty}\)-error in [11]).

In [11, Section 3.4], it was shown that w.l.o.g. it suffices to consider "symmetric" workload matrix \(W\) (i.e., ones that can be written as \([W^{\prime},-W^{\prime}]\) for some \(W^{\prime}\)). We will thus do so throughout the rest of this section. Following their notation, we also assume that \(W\in\mathbb{R}^{k\times\mathcal{X}}\), i.e., \(\mathcal{X}\) is the input space and there are \(k\) linear queries. We also recall the following two lemmas from their paper.

**Lemma 20** ([11, Lemma 11]).: _Let \(\varepsilon\in(0,1]\). For any distribution \(\lambda_{1},\ldots,\lambda_{m},\mu_{1},\ldots,\mu_{m}\) on \(\mathcal{X}\), distribution \(\pi\) over \([m]\) and \(\varepsilon\)-local DP randomizer \(\mathcal{R}\), we have8_

Footnote 8: Here \(\|M\|_{\ell_{\infty}\to L_{2}(\pi)}^{2}:=\max_{\|x\|_{\infty}=1}\|Mx\|_{L_{2} (\pi)}\) where \(\|a\|_{L_{2}(\pi)}:=\sqrt{\sum_{v\in[m]}\pi(v)a_{v}^{2}}\). Note that we will not be dealing with this quantity further than here.

\[\mathbb{E}_{v\sim\pi}[d_{\mathrm{KL}}(\mathcal{R}(\lambda_{v})^{n}\parallel \mathcal{R}(\mu_{v})^{n})]\lesssim n\varepsilon^{2}\cdot\|M\|_{\ell_{\infty} \to L_{2}(\pi)}^{2},\]

_where \(M\in\mathbb{R}^{m\times\mathcal{X}}\) is a matrix such that \(M_{v,x}=\lambda_{v}(x)-\mu_{v}(x)\)._

We note that in [11, Lemma 11], the LHS is not exactly the same as in our version above. However, inspecting the very first inequality from their proof shows that their bound goes through the quantity on the LHS of Lemma 20.

The next lemma, which describes the properties of the hard distributions, is arguably the main technical contribution of the lower bound in [11].

**Lemma 21** ([11, Lemma 21]).: _Let \(W\in\mathbb{R}^{k\times\mathcal{X}}\) be any symmetric workload matrix and \(\alpha>0\). There exist 9\(\xi\geq 0\), and distributions \(\tilde{\lambda}_{1},\ldots,\tilde{\lambda}_{k},\tilde{\mu}_{1},\ldots,\tilde{ \mu}_{k}\) on \(\mathcal{X}\) and \(\tilde{\pi}\) over \([k]\) such that_

Footnote 9: In [11], \(\xi\) is related to the dual solution as \(\xi=W\bullet U-\alpha\) where \(U\) is the dual witness of the approximate factorization norm, i.e., one with \(\gamma_{2}(W)=\frac{W\bullet U-\alpha U\parallel_{1}}{\gamma_{2}(U)}\). (See [11, Section 2.3] for more details.) However, these specifics are not used in the remainder of the proof.

1. \((W\tilde{\lambda}_{v})_{i}=0\) _for all_ \(v\in\mathrm{supp}(\tilde{\pi})\)_,_ \((W\tilde{\mu}_{v})_{v}\gtrsim\frac{\alpha}{\log(\|W\|_{\infty}/\alpha)}\)_._ Footnote 10: Note that the \(\|W\|_{\infty}\) term does not show up in [11] since they assume that \(\|W\|_{\infty}\leq 1\)._

1. \((W\tilde{\lambda}_{v})_{i}=0\) _for all_ \(i,v\in[k]\)_._
2. _for all_ \(v\in\mathrm{supp}(\tilde{\pi})\)_,_ \((W\tilde{\mu}_{v})_{v}\gtrsim\frac{\alpha}{\log(\|W\|_{\infty}/\alpha)}\)_._
3. _Let_ \(\tilde{M}\) _be defined similarly as in Lemma_ 20_. Then_ \(\|\tilde{M}\|_{\ell_{\infty}\to L_{2}(\tilde{\pi})}\lesssim\frac{\alpha}{ \gamma_{2}(W,\alpha)}\)_._

Proof Sketch.: This is constructed by taking \(\tilde{\lambda}_{1},\ldots,\tilde{\lambda}_{k},\tilde{\mu}_{1},\ldots,\tilde{ \mu}_{k}\) and \(\tilde{\pi}\) from Lemma 21. Then, replace each \(\tilde{\mu}_{v}\) by the mixture \((1-\beta)\cdot\tilde{\lambda}_{v}+\beta\cdot\tilde{\mu}_{v}\) where \(\beta=\min\{1,\alpha/\xi\}\), for all \(v\in[k]\). Both the lower bound for \((W\tilde{\mu}_{v})_{v}\) and the upper bound for \(\|\tilde{M}\|_{\ell_{\infty}\to L_{2}(\tilde{\pi})}\) scale linearly with \(\beta\), yielding the desired bounds. 

We can now prove the following, which is a more qualitative version of the lower bound in Theorem 4.

**Theorem 23**.: _Let \(W\in\mathbb{R}^{k\times\mathcal{X}}\) be any symmetric workload matrix. Any non-interactive \(\varepsilon\)-local DP mechanism must incur an \(\mathrm{mMSE}\) at least \(\Omega\left(\frac{\zeta(W,n)^{2}}{\varepsilon^{2}n}\cdot\frac{1}{\log\left( \frac{\sigma^{2}n\parallel W\|_{\infty}^{2}}{\zeta(W,n)^{2}}\right)^{2}}\right)\)._

1. _[label=()]_
2. \(\left\|M\|_{\ell_{\infty}\to L_{2}(\pi)}^{2}:=\max_{\|x\|_{\infty}=1}\|Mx\|_{L_{2 }(\pi)}\) _where_ \(\|a\|_{L_{2}(\pi)}:=\sqrt{\sum_{v\in[m]}\pi(v)a_{v}^{2}}\) _. Note that we will not be dealing with this quantity further than here._

Footnote 2: In [11], \(\xi\) is related to the dual solution as \(\xi=W\bullet U-\alpha\) where \(U\) is the dual witness of the approximate factorization norm, i.e., one with \(\gamma_{2}(W)=\frac{W\bullet U-\alpha U\parallel_{1}}{\gamma_{2}(U)}\). (See [11, Section 2.3] for more details.) However, these specifics are not used in the remainder of the proof.

Note that the \(\|W\|_{\infty}\) term does not show up in [11] since they assume that \(\|W\|_{\infty}\leq 1\).

Proof.: For notational convenience, we write \(\zeta\) as a shorthand for \(\zeta(W,n)\). Let \(\alpha=\frac{C_{1}\zeta}{\varepsilon\sqrt{n}}\) where \(C_{1}\in(0,0.1)\) is a sufficiently small constant. Suppose for the sake of contradiction that there exists a non-interactive \(\varepsilon\)-local DP protocol (whose randomizer is \(\mathcal{R}\)) with \(\mathrm{mMSE}\) at most \((\alpha^{\prime})^{2}\) for \(\alpha^{\prime}=\frac{C_{2}\alpha}{\log(\|W\|_{\infty}/\alpha)}\) where \(C_{2}\in(0,0.1)\) is a sufficiently small constant.

Recall the definition of \(\zeta\); we must have \(\zeta\leq\gamma_{2}\left(W,\frac{\zeta}{2\varepsilon\sqrt{n}}\right)+\frac{ \zeta}{2}\). This gives

\[\gamma_{2}\left(W,\frac{\zeta}{2\varepsilon\sqrt{n}}\right)\geq\frac{\zeta}{2}.\]

Since \(\gamma_{2}(W,\cdot)\) is an increasing function, we thus have

\[\frac{\alpha\varepsilon\sqrt{n}}{\gamma_{2}(W,\alpha)}\leq C_{1}. \tag{3}\]

Let \(\tilde{\lambda}_{1},\ldots,\tilde{\lambda}_{k},\tilde{\mu}_{1},\ldots,\tilde{ \mu}_{k}\) and \(\tilde{\pi}\) be as in Lemma 22. By Lemma 20 and Lemma 22(iii), we have

\[\mathbb{E}_{v\sim\pi}[d_{\mathrm{KL}}(\mathcal{R}(\lambda_{v})^{n}\;\|\; \mathcal{R}(\mu_{v})^{n})]\lesssim\left(\frac{\alpha\varepsilon\sqrt{n}}{\gamma _{2}(W,\alpha)}\right)^{2}. \tag{4}\]

On the other hand, since the protocol has \(\mathrm{mMSE}\) at most \((\alpha^{\prime})^{2}\), we may use the following algorithm to distinguish \(\mathcal{R}(\lambda_{v})^{n}\) from \(\mathcal{R}(\mu_{v})^{n}\): let the analyst computes an estimate for \(Wh_{\mathbf{x}}\) (using the \(n\) samples from \(\mathcal{R}(\cdot)\) provided). If the estimate has absolute value less than \(10\alpha^{\prime}\), then output \(\lambda_{v}\); otherwise, output \(\mu_{v}\). From Lemma 22(i)(ii), it is not hard to see that, for \(C_{2}\) that is sufficiently small, this algorithm is correct with probability at least 2/3. This means that \(d_{\mathrm{tv}}(\mathcal{R}(\lambda_{v})^{n},\mathcal{R}(\mu_{v})^{n})\geq 1/3\). Pinsker's inequality (i.e., eq. (1)) then yields \(d_{\mathrm{KL}}(\mathcal{R}(\lambda_{v})^{n}\;\|\;\mathcal{R}(\mu_{v})^{n}) \gtrsim 1\). Comparing this with the above eq. (4), we get \(\frac{\alpha\varepsilon\sqrt{n}}{\gamma_{2}(W,\alpha)}\gtrsim 1\), which contradicts eq. (3) when \(C_{1}\) is sufficiently small. 

## Appendix C Three-Round Algorithm

In this section, we describe and analyze our interactive algorithm. The error guarantees of the algorithm are stated below. (Note that Theorem 24 is a more precise version of Theorem 8.)

**Theorem 24**.: _For any \(W\in\mathbb{R}^{k\times k}\) and \(n\geq\tilde{\Omega}\left(\frac{\gamma_{2}(W)^{4}\log k}{\|W\|_{\infty} \varepsilon^{2}}\right)\), there is a three-round \(\varepsilon\)-DP algorithm for estimating quadratic forms on \(W\) such that the \(\mathrm{MSE}\) is \(O\left(\frac{\|W\|_{\infty}^{2}}{\varepsilon^{2}n}\right)\)._

Proof.: Throughout the proof, we assume that \(n\geq Q\cdot\frac{\gamma_{2}(W)^{4}\log k}{\|W\|_{\infty}^{4}\varepsilon^{2} }\cdot\log\left(\frac{10\gamma_{2}(W)\log k}{\|W\|_{\infty}\varepsilon}\right)\), where \(Q\) is a sufficiently large constant. Recall from the definition of \(\gamma_{2}(W)\) that there must exist \(L,R\) such that \(W=L^{T}R\) and \(\|L\|_{1\to 2}=\|R\|_{1\to 2}=\sqrt{\gamma_{2}(W)}=:C\).

**Algorithm Description.** Let \(\bar{\varepsilon}=\varepsilon/4\) and \(\tau=4\|W\|_{\infty}\). The algorithm works as follows:

\(\triangleright\)**First Round:**: Run the \(\varepsilon\)-local DP protocol from Theorem 19 to get an estimate \(\mu^{R}\) of \(Rh_{\mathbf{x}}\). \(\triangleright\)**Second Round:**:

\(\triangleright\) The analyzer forwards \(\mu^{R}\) to all users. \(\triangleright\) Each user \(i\in[n]\) sends the following to the analyst:

\(\triangleright\)\(y_{i}^{L}\leftarrow\mathrm{VRand}_{\bar{\varepsilon},C}(L\mathbf{1}_{x_{ i}})\) (where \(\mathrm{VRand}.(\cdot)\) is from Theorem 9).

\(\triangleright\)\(a_{i}\leftarrow\langle L\mathbf{1}_{x_{i}},\mu_{R}\rangle+\kappa_{i}\) where \(\kappa_{i}\sim\mathrm{Lap}(2\|W\|_{\infty}/\bar{\varepsilon})\) back to the analyst. \(\triangleright\)**Third Round:**:

\(\triangleright\) The analyst then computes \(Y^{L}:=\frac{1}{n}\sum_{i\in[n]}y_{i}^{L}\) and forwards it to the users.

\(\triangleright\) Each user \(i\in[n]\) sends \(v_{i}\leftarrow\mathrm{clip}_{\tau}\big{(}\big{\langle}Y^{L},R\mathbf{1}_{x_{ i}}-\mu_{R}\big{\rangle}\big{)}+z_{i}\) where \(z_{i}\sim\mathrm{Lap}(2\tau/\bar{\varepsilon})\).

Finally, the analyst outputs \(\frac{1}{n}\left(\sum_{i\in[n]}a_{i}\right)+\frac{1}{n}\left(\sum_{i\in[n]}v_{ i}\right)\).

Privacy Analysis.Each user \(i\)'s input is used four times:

* To produce \(\mu^{R}\). This step is \(\varepsilon\)-DP by Theorem 19.
* To produce \(y_{i}^{L}\). This step is \(\varepsilon\)-DP by Theorem 9.
* To produce \(a_{i}\). From the guarantee of Theorem 19, \(\mu_{R}\) belong to \(R^{\Delta}\). As a result, \(\left|\left\langle L\mathbf{1}_{x_{i}},\mu_{R}\right\rangle\right|\leq\|W\|_{\infty}\). In other words, the sensitivity of \(\left\langle L\mathbf{1}_{x_{i}},\mu_{R}\right\rangle\) (as a function of \(x_{i}\)) is at most \(2\|W\|_{\infty}\). Since \(\kappa_{i}\) is sampled from \(\operatorname{Lap}(2\|W\|_{\infty}/\varepsilon)\), this step is also \(\varepsilon\)-DP.
* To produce \(v_{i}\). Due to clipping, the sensitivity of \(\operatorname{clip}_{\tau}\bigl{(}\left\langle Y^{L},R\mathbf{1}_{x_{i}}-\mu_ {R}\right\rangle\bigr{)}\) (as a function of \(x_{i}\)) is at most \(2\tau\). Thus, since we are adding a noise \(z_{i}\) drawn from \(\operatorname{Lap}(2\tau/\varepsilon)\), this step is also \(\varepsilon\)-DP.

Hence, by the basic composition theorem, the entire algorithm is \(\varepsilon\)-local DP as desired.

Utility Analysis.First, notice that, for a fixed \(\mu_{R}\in R^{\Delta}\), we have

\[h_{\mathbf{x}}^{T}Wh_{\mathbf{x}}=\left\langle Lh_{\mathbf{x}}, Rh_{\mathbf{x}}\right\rangle =\left\langle Lh_{\mathbf{x}},\mu_{R}\right\rangle+\left\langle Lh _{\mathbf{x}},Rh_{\mathbf{x}}-\mu_{R}\right\rangle\] \[=\frac{1}{n}\sum_{i\in[n]}\left\langle L\mathbf{1}_{x_{i}},\mu_{R }\right\rangle+\frac{1}{n}\sum_{i\in[n]}\left\langle Lh_{\mathbf{x}},R\mathbf{1 }_{\mathbf{x}_{i}}-\mu_{R}\right\rangle.\]

Let \(z_{i}^{L}=y_{i}^{L}-L\mathbf{1}_{x_{i}}\). From Theorem 9, \(z_{i}^{L}\) is zero-mean and \(\sigma\)-sub-Gaussian for \(\sigma=O(C/\varepsilon)\). Let \(Z^{L}:=\frac{1}{n}\sum_{i\in[n]}z_{i}^{L}\); we then have that \(Z^{L}\) is zero-mean and \(\sigma^{\prime}\)-sub-Gaussian for \(\sigma^{\prime}=\sigma/\sqrt{n}\). Note that \(Y^{L}=Lh_{\mathbf{x}}+Z^{L}\). Furthermore, let us write \(\theta_{i}\) as a shorthand for \(\operatorname{clip}_{\tau}\bigl{(}\left\langle Y^{L},R\mathbf{1}_{x_{i}}-\mu _{R}\right\rangle\bigr{)}-\left\langle Y^{L},R\mathbf{1}_{x_{i}}-\mu_{R}\right\rangle\).

The MSE can be bounded by

\[\mathbb{E}\left[\left(\frac{1}{n}\left(\sum_{i\in[n]}a_{i}\right) +\frac{1}{n}\left(\sum_{i\in[n]}v_{i}\right)-h_{\mathbf{x}}^{T}Wh_{\mathbf{x }}\right\}^{2}\right]\] \[=\mathbb{E}\left[\left(\frac{1}{n}\sum_{i\in[n]}\kappa_{i}+\frac {1}{n}\sum_{i\in[n]}z_{i}+\left\langle Z^{L},Rh_{\mathbf{x}}-\mu_{R}\right\rangle +\frac{1}{n}\sum_{i\in[n]}\theta_{i}\right)^{2}\right]\] \[\lesssim\mathbb{E}\left[\left(\frac{1}{n}\sum_{i\in[n]}\kappa_{i }\right)^{2}\right]+\mathbb{E}\left[\left(\frac{1}{n}\sum_{i\in[n]}z_{i} \right)^{2}\right]+\mathbb{E}\left\langle Z^{L},Rh_{\mathbf{x}}-\mu_{R}\right\rangle ^{2}+\mathbb{E}\left[\left(\frac{1}{n}\sum_{i\in[n]}\theta_{i}\right)^{2}\right]\] \[\lesssim\frac{\|W\|_{\infty}^{2}}{\varepsilon^{2}n}+\mathbb{E} \left\langle Z^{L},Rh_{\mathbf{x}}-\mu_{R}\right\rangle^{2}+\frac{1}{n}\sum_{i \in[n]}\mathbb{E}\left[\theta_{i}^{2}\right], \tag{5}\]

where the last step is by applying the Cauchy-Schwarz inequality to the last term.

To handle the middle term in eq. (5), note that \(Z^{L}\) is independent of \(\mu_{R}\) and, as stated earlier, is \(\sigma^{\prime}\)-sub-Gaussian. Thus, we have

\[\mathbb{E}\left\langle Z^{L},Rh_{\mathbf{x}}-\mu_{R}\right\rangle ^{2} =\mathbb{E}_{\mu_{R}}\mathbb{E}_{Z^{L}}\left\langle Z^{L},Rh_{ \mathbf{x}}-\mu_{R}\right\rangle^{2}\] \[\lesssim\mathbb{E}_{\mu_{R}}(\sigma^{\prime})^{2}\|Rh_{\mathbf{ x}}-\mu_{R}\|_{2}^{2}\] \[\left(\text{From Theorem \ref{thm:main}}\right) \lesssim(\sigma^{\prime})^{2}\cdot\frac{C^{2}\sqrt{\log k}}{ \varepsilon\sqrt{n}}\] \[\lesssim\frac{C^{4}\sqrt{\log k}}{\varepsilon^{3}n^{3/2}}\] \[\lesssim\frac{\|W\|_{\infty}^{2}}{\varepsilon^{2}n}, \tag{6}\]

where the last inequality is due to our assumption on \(n\).

As for the last term in eq. (5), notice that

\[\left\langle Y^{L},R\mathbf{1}_{x_{i}}-\mu_{R}\right\rangle=\left\langle Z^{L},R\mathbf{1}_{x_{i}}-\mu_{R}\right\rangle+\left\langle Lh_{\mathbf{x}},R \mathbf{1}_{x_{i}}-\mu_{R}\right\rangle.\]Since \(\mu_{R}\) belongs to \(R^{\Delta}\), we have \(\frac{1}{2}\left(R\mathbf{1}_{x_{i}}-\mu_{R}\right)\in R^{\Delta}\), which implies that \(|\left\langle Lh_{\mathbf{x}},R\mathbf{1}_{x_{i}}-\mu_{R}\right\rangle|\leq 2 \|W\|_{\infty}\) and \(\|R\mathbf{1}_{x_{i}}-\mu_{R}\|_{2}\leq 2\tilde{C}\). As such, we have

\[\mathbb{E}\left\langle Y^{L},R\mathbf{1}_{x_{i}}-\mu_{R}\right\rangle^{4} \lesssim\mathbb{E}\left\langle Z^{L},R\mathbf{1}_{x_{i}}-\mu_{R} \right\rangle^{4}+\|W\|_{\infty}^{4}\] \[\lesssim(\sigma^{\prime})^{4}\cdot C^{4}+\|W\|_{\infty}^{4}\] \[\lesssim\frac{C^{8}}{\varepsilon^{4}n^{2}}+\|W\|_{\infty}^{4}\] \[\lesssim\|W\|_{\infty}^{4},\]

where the last inequality is due to our assumption on \(n\) and from \(\gamma_{2}(W)\geq\|W\|_{\infty}\).

Meanwhile, since \(\tau=4\|W\|_{\infty}\), we have

\[\Pr[|\left\langle Y^{L},R\mathbf{1}_{x_{i}}-\mu_{R}\right\rangle|> \tau] \leq\Pr[|\left\langle Z^{L},R\mathbf{1}_{x_{i}}-\mu_{R}\right\rangle| >\tau/2]\] \[\leq\exp\left(-\Omega(\tau/(\sigma^{\prime}\cdot 2C))^{2}\right)\] \[=\exp\left(-\Omega(\varepsilon\|W\|_{\infty}\sqrt{n}/C^{2})^{2}\right)\] \[\leq\frac{1}{\varepsilon^{4}n^{2}},\]

where the last inequality is again due to our assumption on \(n\) and from \(\gamma_{2}(W)\geq\|W\|_{\infty}\).

Thus, we may apply Lemma 18 to conclude that

\[\mathbb{E}[\theta_{i}^{2}]\lesssim\sqrt{\|W\|_{\infty}^{4}\cdot\frac{1}{ \varepsilon^{4}n^{2}}}=\frac{\|W\|_{\infty}^{2}}{\varepsilon^{2}n}. \tag{7}\]

Combining eqs. (5) to (7), the MSE of the estimate is at most \(O\left(\frac{\|W\|_{\infty}^{2}}{\varepsilon^{2}n}\right)\) as desired. 

### On Separating Non-Interactive and Interactive Local DP

We end by observing that our interactive local DP algorithm (Theorem 24) together with the non-interactive lower bound (Theorem 6 and particularly, the more quantitative version, Theorem 23) gives an asymptotic separation on the \(\operatorname{MSE}\) achievable by \(\varepsilon\)-local DP interactive algorithms and those that are non-interactive, as long as we pick \(W\) together with \(n\in\mathbb{N}\) such that the following holds:

* \(n\gtrsim\frac{\gamma_{2}(W)^{4}\log k}{\|W\|_{\infty}^{4}}\cdot\log\left( \frac{10\gamma_{2}(W)\log k}{\|W\|_{\infty}}\right)\) and
* \(\frac{\zeta(W;n)}{\|W\|_{\infty}}\geq\omega(\log n)\),

where the first condition is from Theorem 24 (and letting \(\varepsilon=1\)) and the second condition ensures that the error from Theorem 24 is asymptotically larger than the lower bound from Theorem 23.

It is simple to verify that it suffices to pick \(W\) such that \(\|W\|_{\infty}=1,\gamma_{2}(W,0.1),\gamma_{2}(W)=\Theta(k^{c})\) for any constant \(c>0\) and pick \(n\) to precisely satisfy the bound in the first condition. Again, it is not hard to construct such a matrix e.g., by taking \(W\) to be a random \((k\times k)\) matrix where each entry is an i.i.d. Rademacher random variable, which yields \(\gamma_{2}(W,0.1),\gamma_{2}(W)=\Theta(\sqrt{k})\) w.h.p. [12].