# Convergence of Adafactor under Non-Convex Smooth Stochastic Optimization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

As model sizes in deep learning continue to expand, memory-efficient optimizers are increasingly critical to manage the substantial memory demands of popular algorithms like Adam and AdamW. Among these, Adafactor has emerged as one of the widely adopted choices for training deep learning tasks, particularly large language models. However, despite its practical success, there is limited theoretical analysis on Adafactor's convergence. This paper presents a comprehensive analysis on Adafactor in a non-convex smooth setting, demonstrating its convergence to find a stationary point at a rate of \(\tilde{\mathcal{O}}(1/\sqrt{T})\). We find that the default hyper-parameter setting results in a sub-optimal rate in our framework, and propose an alternative setting that could theoretically achieve optimal convergence rate. This finding is further supported by some experimental results. We also prove that Adafactor with a suitable time-varying clipping threshold could also converge, achieving performance in experiments comparable to that of the standard constant setting.

## 1 Introduction

The adaptive gradient-based methods, such as the well-known AdaGrad [9; 29], RMSProp [30], Adadelta [35], Adam [15] and AdamW [22], have become the preferred approaches in solving the following unconstrained stochastic optimization problem in deep learning fields:

\[\min_{\mathbf{X}\in\mathbb{R}^{n\times m}}f(\mathbf{X})=\mathbb{E}_{\mathbf{Z}\in\mathcal{ P}}[l(\mathbf{X};\mathbf{Z})], \tag{1}\]

where the object function \(f\) is non-convex and \(\mathcal{P}\) denotes a probability distribution. During the training process, these adaptive methods require to store the historical gradients' information so as to adaptively tune their step-sizes. For example, both Adam and AdamW maintain the exponential average of gradients and squared gradients, and AdaGrad stores the cumulative of squared gradients. Despite their effectiveness, these algorithms pose substantial memory challenges for GPUs to save these additional gradients' information, especially when training large language models (LLMs), such as GPT-3 [5], which contains over 175 billion parameters.

To address memory constraints, several memory-efficient optimization algorithms have been developed, e.g., [26; 1; 23; 17]. One of the most popular optimizers is Adafactor [26] which employs a rank-1 matrix factorization to approximate the second moment matrix in Adam. For an \(n\times m\) weight matrices, this technique reduces memory usage from \(\mathcal{O}(mn)\) to \(\mathcal{O}(m+n)\) by only tracking the moving averages of the row and column sums of the squared gradients matrix. Additionally, Adafactor eliminates the first-order momentum used in Adam and incorporates update clipping to enhance training stability.

The empirical results reveal that Adafactor achieves comparable performance to Adam in training Transformer models [26]. In real applications, several LLMs including PaLM [6] and T5 [24] haveapplied Adafactor as their main optimizers [38]. In spite of Adafactor's widely usage, there is still limited understanding on its convergence in theory, especially the effect of the matrix approximation and update clipping, and the explanation for its hyper-parameter setting in experiments.

In this paper, we take a closer look on Adafactor's convergence under non-convex smooth optimization problems, considering the typical bounded gradient setting as those for AdaGrad [19; 32] and Adam [34]. We aim to provide a convergence rate for Adafactor and explain the influence of the hyper-parameters for the convergence speed. We also prove in theory why the default parameter setting is effective in practical scenarios. The analysis to Adafactor is non-trivial compared to other adaptive methods such as AdaGrad and Adam due to the unique matrix factorization and update clipping mechanisms. Based on a new proxy step-size construction and some new compositions as well as estimations, we analyze the additional error terms in the Descent Lemma introduced by the matrix approximation and update clipping. Our main contributions are summarized as follows.

#### Contributions

* We provide a convergence analysis for the full-batch Adafactor considering bounded gradients and a broader range of parameter setting which covers the default one in [26]. The result shows that Adafactor could converge to find a stationary point with a rate of \(\tilde{\mathcal{O}}(1/\sqrt{T})\) where \(T\) denotes the total iteration number.
* We further investigate the more realistic stochastic Adafactor. It's found that a simple variant of Adafactor, which drops the update clipping, could attain the best convergence rate of \(\tilde{\mathcal{O}}(1/\sqrt{T})\) when the second moment decay rate is \(1-1/k\). We also verify that the default decay rate \(1-1/k^{0.8}\) could lead to a sub-optimal convergence rate in our framework. To illustrate this finding, we provide some empirical results, showing that the potential best hyper-parameter setting in theory could perform better than the default one used in experiments.
* We extend our study to include a time-varying clipping threshold. Our analysis implies that with proper selections of clipping threshold and hyper-parameters, Adafactor could also achieve the best convergence rate of \(\tilde{\mathcal{O}}(1/\sqrt{T})\). We also do some experiments to show that the new clipping threshold scheme achieves comparable performance and training stability to the original constant threshold setting.

The rest of the paper is organized as follows. The next section provides some most relevant works. Section 3 presents some necessary notations definitions and problem setup. Section 4 reviews Adafactor and introduces its essential mechanism. In Section 5 and Section 6, we separately provide convergence bounds for full-batch Adafactor and stochastic Adafactor without update clipping. We further discuss the hyper-parameters' dependency. In Section 7, we investigate Adafactor using a time-increasing update clipping threshold. Section 8 provides experimental results to support our theory. All the detailed proof could be found in the appendix.

## 2 Related work

In this paper, we mainly investigate the theoretical convergence of Adafactor. Although there is limited works on Adafactor in theory, it's necessary to briefly discuss related works on the convergence of other adaptive methods, particularly on non-convex smooth optimization. Here, we briefly list some of the most related works.

Convergence of adaptive methodsSeveral studies address the convergence of AdaGrad in non-convex settings. For example, [19] considered a simple variant with delayed step-size, while [32] and [39] assumed bounded stochastic gradients. Other works [14; 10; 21; 3; 31; 27; 33] derived convergence bounds under more relaxed assumptions. Another line of research has investigated the convergence of Adam. For instance, [34; 7; 39; 11; 8] assumed bounded gradients. [28; 36; 31] considered more relaxed noise assumptions without relying on bounded gradients. Additionally, [18] derived convergence bounds for Adam under generalized smooth conditions.

Overall, the convergence analysis of optimizers typically starts with standard assumptions, such as bounded gradients and smooth objective functions. In subsequent studies, these assumptions are gradually relaxed to investigate the convergence properties of the optimizers under less stringent conditions.

Memory efficient algorithmsAs large models are increasingly used in deep learning, memory constraints have become a central issue during training. Consequently, several memory-efficient optimizers have been developed to address this challenge.

One approach to save memory involves applying matrix factorization to oeptimization algorithms. For instance, [25] used matrix factorization in the second moment estimator of gradients in Adam, similar to the concept behind Adafactor. [23] introduced CAME, a variant of Adafactor, which incorporates a confidence-guided strategy to mitigate instability caused by erroneous updates. [37] proposed Adapprox, leveraging randomized low-rank matrix approximation for Adam's second moment estimator, demonstrating superior performance and reduced memory usage compared to AdamW.

There are some other techniques to save the memory. For example, [12] relied on a "Shampoo" technique to reduce the storage requirement of full-matrix preconditioning methods. Notably, their method could be further extended to the more realistic tensor case. [1] presented a memory-saved version of AdaGrad, called SM3, by maintaining \(k\) sets gradient accumulator. They proved the convergence guarantee of SM3 on online convex optimization and the effectiveness in experiments. Recently, [17] built a 4-bit Adam using quantization techniques to compress the first and second moment estimators in Adam, also reducing memory usage.

In summary, many existing optimizers, particularly adaptive methods like AdaGrad and Adam, face memory overhead. In response, the discussed works have designed memory-efficient optimizers that aim to achieve comparable performance to these existing methods while achieving memory benefits.

## 3 Problem setup

To start with, we introduce some necessary notations.

NotationsFor any two matrices \(\mathbf{X}=(x_{ij})_{ij},\mathbf{Y}=(y_{ij})_{ij}\in\mathbb{R}^{n\times m}\), we define \(\langle\mathbf{X},\mathbf{Y}\rangle=\sum_{i=1}^{n}\sum_{j=1}^{m}x_{ij}y_{ij}\). \(\mathbf{X}\odot\mathbf{Y}\), \(\mathbf{X}/\mathbf{Y}\) and \(\sqrt{\mathbf{X}}\) denote the coordinate-wise product, quotient and squared root respectively. \(\mathbf{0}_{n}\) and \(\mathbf{1}_{n}\) denote the zero and one \(n\)-dimensional vector respectively, and \(\mathbf{1}_{n\times m}\) denotes the one \(n\times m\)-dimensional matrix. The index set \([n]\) denotes \(\{1,2,\cdots,n\}\). \(\|\cdot\|_{F}\) denotes the Frobenius norm. For a positive sequence \(\{\alpha_{i}\}_{i\geq 1}\), we define \(\sum_{i=a}^{b}\alpha_{i}=0\) and \(\prod_{i=a}^{b}\alpha_{i}=1\) if \(a>b\). The operator \(\text{RMS}(\cdot)\) denotes

\[\text{RMS}(\mathbf{X})=\sqrt{\frac{1}{mn}\sum_{i=1}^{n}\sum_{j=1}^{m}x_{ij}^{2}}.\]

We consider unconstrained stochastic optimization (1) over \(\mathbb{R}^{n\times m}\) with the Frobenius norm. The objective function \(f:\mathbb{R}^{n\times m}\rightarrow\mathbb{R}\) is differentiable. Given an \(n\times m\) matrix \(\mathbf{X}\), we assume a gradient oracle that returns a random matrix \(g(\mathbf{X},\mathbf{Z})\in\mathbb{R}^{n\times m}\) dependent by the random sample \(\mathbf{Z}\). The deterministic gradient of \(f\) at \(\mathbf{X}\) is denoted by \(\nabla f(\mathbf{X})\in\mathbb{R}^{n\times m}\).

AssumptionsWe make the following standard assumptions throughout the paper.

* (**A1**) \(L\)-smoothness: For any \(\mathbf{X},\mathbf{Y}\in\mathbb{R}^{n\times m}\), \(\|\nabla f(\mathbf{Y})-\nabla f(\mathbf{X})\|_{F}\leq L\|\mathbf{Y}-\mathbf{X}\|_{F}\);
* (**A2**) Bounded below: There exists \(f^{*}>-\infty\) such that \(f(\mathbf{X})\geq f^{*},\forall\mathbf{X}\in\mathbb{R}^{n\times m}\);
* (**A3**) Unbiased estimator: The gradient oracle provides an unbiased estimator of \(\nabla f(\mathbf{X})\), i.e., \(\mathbb{E}_{\mathbf{Z}}\left[g(\mathbf{X},\mathbf{Z})\right]=\nabla f(\mathbf{X}),\forall\mathbf{ X}\in\mathbb{R}^{n\times m}\);
* (**A4**) Almost surely bounded stochastic gradient: for any \(\mathbf{X}\in\mathbb{R}^{n\times m}\), \(\|g(\mathbf{X},\mathbf{Z})\|_{F}\leq G\), a.s..

Combining (A3) and (A4), it's easy to verify that \(\|\nabla f(\mathbf{X})\|\leq G,\forall\mathbf{X}\in\mathbb{R}^{n\times m}\). Assumptions (A1)-(A3) are standard in the non-convex smooth convergence analysis. Although Assumption (A4) is a bit strong since it requires an almost surely bounded stochastic gradients instead of an expected one, it's still commonly used to derive the high probability convergence bound, see e.g., [32; 14], which is a stronger result than an expected convergence. In coordinate-wise algorithm, another standard assumption is \(l_{\infty}\)-bounded gradient where \(\|g(\mathbf{X},\mathbf{Z})\|_{\infty}\leq G_{\infty}\), see e.g., [8]. These two types of assumption are equivalent up to dimension factors.

Review of Adafactor

In this section, we briefly discuss Adafactor based on the reference [26]. The pseudocode for Adafactor is presented in Algorithm 1.

```
Input: Initialization point \(\mathbf{X}_{1}\in\mathbb{R}^{n\times m}\), \(\mathbf{R}_{0}=\mathbf{0}_{m}\), \(\mathbf{C}_{0}=\mathbf{0}_{n}^{\top}\), relative step-sizes \(\{\rho_{k}\}_{k\geq 1}\), decay rate \(\{\beta_{2,k}\}_{k\geq 1}\in[0,1)\), regularization constants \(\epsilon_{1},\epsilon_{2}>0\), clipping threshold \(d\). for\(k=1,\cdots,T\)do \(\mathbf{G}_{k}=g(\mathbf{X}_{k},\mathbf{Z}_{k})\); \(\mathbf{R}_{k}=\beta_{2,k}\mathbf{R}_{k-1}+(1-\beta_{2,k})(\mathbf{G}_{k}\odot\mathbf{G}_{k}+ \epsilon_{1}\mathbf{1}_{n}\mathbf{1}_{m}^{\top})\mathbf{1}_{m}\); \(\mathbf{C}_{k}=\beta_{2,k}\mathbf{C}_{k-1}+(1-\beta_{2,k})\mathbf{1}_{n}^{\top}(\mathbf{G}_{k} \odot\mathbf{G}_{k}+\epsilon_{1}\mathbf{1}_{n}\mathbf{1}_{m}^{\top})\); \(\mathbf{W}_{k}=(\mathbf{R}_{k}\mathbf{C}_{k})/\mathbf{1}_{n}^{\top}\mathbf{R}_{k}\); \(\mathbf{U}_{k}=\mathbf{G}_{k}/\sqrt{\mathbf{W}_{k}}\); \(\eta_{k}=\max\{\epsilon_{2},\text{RMS}(\mathbf{X}_{k})\}\rho_{k}/\max\{1,\text{RMS} (\mathbf{U}_{k})/d\}\); \(\mathbf{X}_{k+1}=\mathbf{X}_{k}-\eta_{k}\cdot\mathbf{G}_{k}/\sqrt{\mathbf{W}_{k}}\); endfor
```

**Algorithm 1** Adafactor

Matrix factorizationAdafactor could be severed as a saved-memory version of Adam. Throughout the training process, Adam maintain two \(n\times m\) matrices \(\mathbf{M}_{k}\) and \(\mathbf{V}_{k}\) using exponential moving average update,

\[\mathbf{M}_{k}=\beta_{1,k}\mathbf{M}_{k-1}+(1-\beta_{1,k})\mathbf{G}_{k},\quad\mathbf{V}_{k}= \beta_{2,k}\mathbf{V}_{k-1}+(1-\beta_{2,k})\mathbf{G}_{k}\odot\mathbf{G}_{k}, \tag{2}\]

where \(\beta_{1,k},\beta_{2,k}\in(0,1)\), thereby tripling the memory usage. The innovation in Adafactor lies in its method of approximating \(\mathbf{V}_{k}\) by factoring it into two rank-1 matrices, specifically the row sums and column sums of \(\mathbf{V}_{k}\). This approximation is guided by maintaining a minimal general Kullback-Leibler (KL) divergence as follows,

\[\min_{\mathbf{X}\in\mathbb{R}^{n},\mathbf{Y}\in\mathbb{R}^{1\times m}}\sum_{i=1}^{n} \sum_{j=1}^{m}d\left((\mathbf{V}_{k})_{ij},(\mathbf{XY})_{ij}\right),\quad\text{s.t.} \quad(\mathbf{X})_{i}\geq 0,(\mathbf{Y})_{j}\geq 0,\forall i\in[n],j\in[m],\]

where \(d(p,q)=p\log(p/q)-p+q\). The choice of KL-divergence over the more typical Frobenius norm allows for an analytical solution to be derived, specifically given by

\[\mathbf{X}=\mathbf{V}_{k}\mathbf{1}_{m},\quad\mathbf{Y}=\mathbf{1}_{n}^{\top}\mathbf{V}_{k}/\left(\bm {1}_{n}^{\top}\mathbf{V}_{k}\mathbf{1}_{m}\right).\]

Therefore, Adafactor only requires to maintain two vectors \(\mathbf{R}_{k}=\mathbf{V}_{k}\mathbf{1}_{m}\), \(\mathbf{C}_{k}=\mathbf{1}_{n}^{\top}\mathbf{V}_{k}\), sufficiently reducing the memory from \(2mn\) to \(m+n\). Although this factorization sacrifices some information of the squared gradients, Adafactor still delivers performance comparable to Adam in many real application tasks, making it a practical choice where memory is a constraint.

Increasing decay rateIn Adam, corrective terms are introduced into \(\mathbf{M}_{k}\) and \(\mathbf{V}_{k}\), resulting in two increasing-to-one decay rates. Theoretically, it has been demonstrated that a value closed to one for \(\beta_{2,k}\) would ensure the convergence, e.g., [8, 39, 36]. Inspired by this observation, Adafactor used an increasing second moment decay rate \(\beta_{2,k}=1-1/k^{c},c\geq 0\), and the empirical default setting is \(c=0.8\). As pointed out by [26], this setting allows for enjoying the stability of a low \(\beta_{2,k}\) at the early stage of training and the insurance of convergence from a high \(\beta_{2,k}\) as the run progresses. Moreover, it also leverages the bias correction.

Update clippingAdafactor modifies the update process by discarding the first-order moment \(\mathbf{M}_{k}\) and instead applies an update clipping technique inside the step-size \(\eta_{k}\). This involves dividing the root-mean-square of the update \(\mathbf{\tilde{U}}_{k}\), denoted as \(\text{RMS}(\mathbf{U}_{k})\), when it exceeds a threshold \(d\). This mechanism helps to calibrate the second moment estimator \(\mathbf{W}_{k}\) when it's larger-than-desired \(\mathbf{G}_{k}\odot\mathbf{G}_{k}\). Empirical findings in [26] indicated that implementing update clipping leads to significant performance improvements when the warm-up technique is not used.

Relative step-sizesAdafactor incorporates a step-size proportional to scale of \(\mathbf{X}_{k}\), denoted by \(\text{RMS}(\mathbf{X}_{k})\), which is shown in experiments more resilient to the more naive parameter initialization and scaling schemes [26].

Convergence result for full-batch Adafactor

We first provide the convergence bound for full-batch Adafactor. At each iteration, full-batch Adafactor obtains the deterministic gradient \(\nabla f(\mathbf{X}_{k})\) and then updates \(\mathbf{R}_{k},\mathbf{C}_{k}\) using \(\nabla f(\mathbf{X}_{k})\) instead of \(\mathbf{G}_{k}\) in Algorithm 1.

**Theorem 5.1**.: _Let \(\{\mathbf{X}_{k}\}_{k\geq 1}\) be generated by Algorithm 1 with \(g(\mathbf{X}_{k},\mathbf{Z}_{k})=\nabla f(\mathbf{X}_{k}),\forall k\geq 1\). If Assumptions (A1) and (A2) hold, \(\|\nabla f(\mathbf{X}_{k})\|_{F}\leq G,\forall k\geq 1\), \(\beta_{2,1}=\frac{1}{2}\) and_

\[\rho_{k}=\rho_{0}/\sqrt{k},\quad 0<\beta_{2,k}<1,\quad\forall k\geq 1, \tag{3}\]

_for some positive constant \(\rho_{0}\), then for any \(T\geq 1\),_

\[\min_{k\in[T]}\|\nabla f(\mathbf{X}_{k})\|_{F}^{2}\leq\mathcal{O}\left(\frac{\log T }{\sqrt{T}}\right). \tag{4}\]

The result indicates that full-batch Adafactor could find a stationary point at a rate of \(\mathcal{O}(\log T/\sqrt{T})\) under the non-convex smooth case, which is similar to gradient descent but with a sub-optimal rate compared to \(\mathcal{O}(1/T)\)[4]. The hyper-parameter setting in (3) only requires \(\beta_{2,k}\in(0,1)\), denoting a much wider range including the default one which requires \(\beta_{2,k}\) to increase to one. The detailed version for the above result can be found in Theorem A.1 from the appendix.

## 6 Stochastic Adafactor without update clipping

In the stochastic case, we start from the simple scenario of

\[\eta_{k}=\max\{\epsilon_{2},\text{RMS}(\mathbf{X}_{k})\}\rho_{k} \tag{5}\]

without considering the update clipping \(1/\max\{1,\text{RMS}(\mathbf{U}_{k})/d)\}\) in Algorithm 1, where the main reasons are as follows.

* As pointed out in the experiments from [26], Adafactor's performance shows little difference with and without update clipping when implementing learning rate warm-up. Since the warm-up technique is a popular method in deep learning [38], it's reasonable to drop the update clipping.
* In stochastic Adafactor, the correlation between \(\mathbf{G}_{k}\) and \(\eta_{k}\) would be more complex if the update clipping is involved. The proof would be simpler when dropping the update clipping, which could help to better understand the analysis for Adafactor.

We now present the probabilistic convergence bound for Adafactor without update clipping as follows, where we summarize different convergence rate with respect to the factor \(c\) from \(\beta_{2,k}=1-1/k^{c},c\in[1/2,1]\).

**Theorem 6.1**.: _Let \(\{\mathbf{X}_{k}\}_{k\geq 1}\) be generated by Algorithm 1 without update clipping where \(\eta_{k}\) is given by (5) for each \(k\geq 1\). If Assumptions (A1)-(A4) hold, and_

\[\beta_{2,1}=1/2,\quad\rho_{1}=\rho_{0}, \tag{6}\] \[\beta_{2,k}=1-1/k^{c},\quad\rho_{k}=\rho_{0}/\sqrt{k},\quad\forall k \geq 2,\]

_for some constants \(1/2\leq c\leq 1,\rho_{0}>0\), then for any \(T\geq 1,\delta\in(0,1)\), with probability at least \(1-\delta\),_

\[\min_{k\in[T]}\|\nabla f(\mathbf{X}_{k})\|_{F}^{2}\leq\mathcal{O}\left(\frac{1}{T^ {c-1/2}}\log\left(\frac{T}{\delta}\right)\right).\]

The above result indicates that with appropriate hyper-parameters, Adafactor without update clipping could approximately find a stationary point. When the decay rate \(\beta_{2,k}\) is \(1-1/k\), the convergence rate could attain to \(\mathcal{O}(\log T/\sqrt{T})\), matching the rate of stochastic gradient descent [4] and the lower rate in [2] up to only a logarithm factor. The hyper-parameter setting in (6) covers the experimental default setting where \(c=0.8\). The result shows a sub-optimal rate of \(\mathcal{O}(\log T/T^{0.3})\) under the default setting. This finding is further complemented by the coming numerical experiments in Section 8. The detailed version of the above results can be found in Theorem B.1 from the appendix.

### Discussion of the hyper-parameter dependency

In this section, we discuss the dependency of several important hyper-parameters in Theorem 6.1 and the detailed version in Theorem B.1 in the appendix. It's worthy to mention that the dominated order in our convergence bound is determined by the total iteration number \(T\), whereas other hyper-parameters could be regarded as constants. However, we hope to improve the dependency of these hyper-parameters as much as possible to make the convergence bound tight.

Discussion of \(c\) and the optimal rateThe convergence bound in Theorem 6.1 reveals that when \(c=1,\beta_{2,k}=1-1/k\) and \(\rho_{k}=\rho_{0}/\sqrt{k}\), the convergence rate attains the optimal rate matching the lower bound. In addition, when \(c\) is closed to \(1/2\), the convergence rate deteriorates. This phenomenon somehow explains that a small decay rate \(\beta_{2,k}\) (\(c\) is low) may harm the convergence speed, as \(\beta_{2,k}\) should be closed enough to \(1\) to ensure the convergence, which has been pointed out similarly for Adam in [8; 39; 36].

The theoretical best parameter setting remains a small gap to the default one of \(c=0.8\). To verify our theoretical finding, we provide some empirical evidence in Section 8, showing that \(\beta_{2,k}=1-1/k\) performs even better than the default one and the performance would be better when \(c\) increases from \(1/2\) to \(1\).

Dependency to \(mn\)It's clear to see that the convergence bounds in Theorem A.1 and Theorem B.1 are free of the curse of the dimension factor \(mn\) as \(mn\) only appears on the denominator in each coefficient. We think that solving the curse of dimension is vital since the applied range for Adafactor includes many deep learning tasks where \(mn\) are comparable large to \(T\).

Dependency to \(\epsilon_{1},\epsilon_{2}\)The convergence bounds in (37) and (39) from Theorem B.1 has a dependency of \(\mathcal{O}(\epsilon_{1}^{-1}\log(1/\epsilon_{1}))\) on \(\epsilon_{1}\).1 Although the polynomial dependency to \(\epsilon_{1}\) is a bit worse since \(\epsilon_{1}\) ususally takes a small value in experiments, e.g., the default setting is \(10^{-30}\), it's still common in some theoretical convergence results, e.g., [34; 18]. We also perform some experiments to show that a relatively large \(\epsilon_{1}\), roughly \(10^{-3}\), makes no observable effect on the performance. Thereby, \(\epsilon_{1}\) could be regarded as a constant in comparison to \(T\) and the influence brought by \(1/\epsilon_{1}\) could be somehow acceptable.

Footnote 1: The detailed calculation could be found in (45) and (46) in the appendix.

Since the default value of \(\epsilon_{2}\) is \(10^{-3}\) in experiments, it could also be regarded as a constant compared to \(T\). Therefore, the dependency \(\mathcal{O}(1/\epsilon_{2})\) on \(\epsilon_{2}\) shows little effect on convergence bounds given the sufficiently large \(T\).

Dependency to the scale of parameters.The convergence bounds in Theorem B.1 contain a \(\mathcal{O}(\Theta_{\max})\) factor where \(\Theta_{\max}\) denotes the maximum values of \(\|\mathbf{X}_{k}\|_{\infty}\) along the training process. It's reasonable to assume that \(\Theta_{\max}\leq G_{0}\) for a comparable large constant \(G_{0}\) in practice.

## 7 Convergence of Adafactor with update clipping

In this section, we take a closer look on the comprehensive Adafactor with both matrix factorization and update clipping. We slightly change the update clipping threshold \(d\) in Algorithm 1 to a time-varying threshold \(d_{k}\). The step-size \(\eta_{k}\) then becomes

\[\eta_{k}=\frac{\max\{\epsilon_{2},\text{RMS}(\mathbf{X}_{k})\}\rho_{k}}{\max\{1, \text{RMS}(\mathbf{U}_{k})/d_{k}\}}. \tag{7}\]

Then, we present the following convergence bound.

**Theorem 7.1**.: _Let \(\{\mathbf{X}_{k}\}_{k\geq 1}\) be generated by Algorithm 1 with \(\eta_{k}\) given by (7) for each \(k\geq 1\). If Assumptions (A1)-(A4) hold, and_

\[\begin{split} d_{1}&=1,\quad\beta_{2,1}=1/2,\quad \rho_{1}=\rho_{0},\\ d_{k}&=k^{\frac{c}{2(\alpha-1)}},\quad\beta_{2,k}=1 -1/k^{c},\quad\rho_{k}=\rho_{0}/\sqrt{k},\quad\forall k\geq 2,\end{split} \tag{8}\]_for some constants \(\alpha>1,1/2\leq c\leq 1,\rho_{0}>0\), then for any \(T\geq 1,\delta\in(0,1)\), with probability at least \(1-\delta\),_

\[\min_{k\in[T]}\|\nabla f(\mathbf{X}_{k})\|_{F}^{2}\leq\mathcal{O}\left( \frac{1}{T^{c-1/2}}\log\left(\frac{T}{\delta}\right)\right).\]

Discussion of Theorem 7.1The convergence result indicates that with a proper selection of the clipping threshold, along with the commonly used step-size \(\rho_{k}\) and decay rate \(\beta_{2,k}\), Adafactor can find a stationary point when \(T\) is sufficiently large. The dependency of convergence bound on \(c\) remains consistent with Theorem 6.1, achieving the optimal order when \(c=1\). In addition, the convergence bound can still avoid the curse of dimension, which is shown in the detailed version Theorem C.1 from the appendix.

The additional hyper-parameter \(\alpha\) primarily influences the dependency on \(\epsilon_{1}\), specifically as \(\mathcal{O}\left(\epsilon_{1}^{-\alpha}\log(1/\epsilon_{1})\right)\). Thus, our convergence bound may deteriorate as \(\alpha\) increases, possibly due to the limitation of our proof framework. This dependency could be potentially improved to \(\mathcal{O}\left(\epsilon_{1}^{-1}\log(1/\epsilon_{1})\right)\) when \(mn\) is comparable to \(1/\epsilon_{1}\), which is practical when implementing a large-size model.1 In our experiments, we tested different values of \(\alpha\) and found that suitably small values, such as \(\alpha=4,6,7,8\) can lead to performance and training stability comparable to the default setting, even without implementing the warm-up technique. This finding suggests that our new threshold setting plays a similar role in enhancing training stability as the default one, which is also the main motivation of update clipping. Since \(\epsilon_{1}\) can be set to a relatively large value, e.g., \(10^{-3}\), a dependency like \(\mathcal{O}(\epsilon_{1}^{-4}\log(1/\epsilon_{1}))\) is somewhat acceptable for sufficiently large \(T\).

Footnote 1: The online-index \(d_{k}\) provides the following intuition: As shown in [26, Figure 1], during the early stages of training, a high decay rate \(\beta_{2,k}\) can cause larger-than-desired updates and training instability. Therefore, we set a low threshold \(d_{k}\) to ensure that the update clipping mechanism effectively calibrates these larger-than-desired updates. As training progresses, the sequences and updates become more stable, and the second moment estimator \(\mathbf{W}_{k}\) becomes more accurate in estimating the squared gradients, which is also shown in [26, Figure 1]. Consequently, there is less need for update clipping, corresponding to a relatively large \(d_{k}\). We have also verified through experiments that our setting can achieve performance comparable to the default setting of \(d=1\).

## 8 Experiments

In this section, we will report our experimental results based on the insights obtained in our theory. We will mainly provide the following three experiments:

* We test Adafactor without update clipping under different decay rate parameters \(c\), aiming to demonstrate performance improvement as \(c\) increases from \(0.5\) to \(1\) with optimal performance at \(c=1\), as indicated in Theorem 6.1 and Theorem 7.1.
* We evaluate the sensitivity of Adafactor to different values of \(\epsilon_{1}\), particularly showing that a relatively large \(\epsilon_{1}\) does not significantly impact performance.
* We assess the performance of Adafactor with a time-increasing \(d_{k}\) setting, as described in Theorem 7.1, and compare it to the default constant setting.

### Experiment setup

In all experiments, the initialization is \(\mathbf{R}_{0}=\mathbf{0}_{m}\) and \(\mathbf{C}_{0}=\mathbf{0}_{n}^{\top}\). We use a learning rate with the warm-up technique as described in [26], specifically \(\rho_{k}=\min\{10^{-6}\cdot k,1/\sqrt{k}\}\) for all experiments unless otherwise specified. The batch size is set to 256, and the total number of epochs is 400 by default. Our models are ResNet-20 and ResNet-110 [13], and we use the CIFAR-10 and CIFAR-100 datasets [16] without any data augmentation. The experiments are conducted using the PyTorch implementation of Adafactor on a single NVIDIA GeForce RTX 4090 GPU.

### Report on Experiment 1

We test Adafactor without update clipping using decay rate parameter \(c\) ranging from \(0.5\) to \(1.0\) in increments of \(0.05\), while keeping other hyper-parameters at their default values. Each experiment is run 10 times with 100 epochs, and we plot the average test accuracy and standard deviation (shallow blue region) in Figure 1. The results indicate that \(c=1.0\) yields better performance and stability compared to \(c<1.0\) on different models and datasets, corresponding to the highest test accuracy and thinner shallow blue band. These performances show a noticeable improving trend as \(c\) increases from 0.5 to 1.0, aligning roughly with the results in Theorem 6.1.

### Report on Experiment 2

In the second experiment, we test Adafactor without update clipping under different \(\epsilon_{1}\) values. We plot the training loss against the step \(t\) on different models and datasets in Figure 2. The performance for \(\epsilon_{1}=10^{-8}\) and \(\epsilon_{1}=10^{-5}\) is nearly identical to that for \(\epsilon_{1}=10^{-30}\). Moreover, even a larger value of \(10^{-3}\) achieves comparable training performance, though with a slower decrease in loss. Notably, \(\epsilon_{1}=10^{-3}\) requires approximately the same number of steps (\(t\approx 20000\)) as \(\epsilon_{1}=10^{-30}\) to achieve near-zero training loss. We conclude that Adafactor is not sensitive to the choice of \(\epsilon_{1}\), and a relatively large \(\epsilon_{1}\) can still lead to convergence, making the polynomial dependency \(\mathcal{O}(1/\epsilon_{1})\) in our convergence bounds acceptable.

### Report on Experiment 3

In this experiment, we explore the appropriate values of \(\alpha\) in Theorem 7.1 to achieve performance comparable to the default setting of \(d=1\). As indicated by Theorem 7.1, a relatively small \(\alpha\) is desirable for better dependency on \(\epsilon_{1}\). We train models with \(\alpha\) set to 4, 6, 7, 8, and 9, keeping other hyper-parameters at their default values. We also train models with the default \(d=1\) setting as the baseline. We plot the training loss against the steps in Figures 3 without step-size warm-up and 4 with step-size warm-up.

Figure 1: Average test accuracy and standard deviation (shallow blue region) under different decay rate parameters \(c\).

Figure 2: Training loss vs. steps using Adafactor without update clipping under different \(\epsilon_{1}\). The step-size \(\eta_{t}\), decay rate \(\beta_{2,k}\), and learning rate warm-up are set by default.

The results indicate that, for these values of \(\alpha\), Adafactor achieves comparable or even better convergence speed compared to the default threshold (represented by "Baseline"). The comparable results to the "Baseline" in Figure 3 further suggest that the time-increasing \(d_{k}\) in Theorem 7.1 plays a role similar to that of the default setting, enhancing training stability even when the step-size warm-up is turned off.

## 9 Conclusions

In this paper, we investigate the convergence behavior of Adafactor on non-convex smooth landscapes, considering bounded stochastic gradients. We introduce a new proxy step-size to decouple the stochastic gradients from the unique adaptive step-size. Additionally, we use new estimations to control the errors introduced by matrix factorization and update clipping in Adafactor.

Our findings reveal that full-batch Adafactor is capable of finding a stationary point, requiring only a step-size \(\eta_{k}\sim\mathcal{O}(1/\sqrt{k})\) and a second moment decay rate \(\beta_{2,k}\in(0,1)\), denoting a wide range including the default setup. In the case of stochastic Adafactor without update clipping, the convergence rate can achieve the optimal order \(\tilde{\mathcal{O}}(1/\sqrt{T})\) when \(\beta_{2,k}=1-1/k^{c},c=1\). However, performance deteriorates as \(c\) decreases. This finding is supported by experimental results. We also explore Adafactor with a time-increasing clipping threshold and derive similar convergence results. The empirical results demonstrate that the new clipping threshold provides performance comparable to the default constant setting.

LimitationsThere are several limitations in our work that warrant further investigation. First, the polynomial dependency on \(\epsilon_{1}\) in our convergence bounds may be further improved to a better dependency, such as \(\log(1/\epsilon_{1})\). Second, although we provide convergence results for several variants of Adafactor and demonstrate comparable performance to the original one in experiments, the convergence bound for stochastic vanilla Adafactor remains unknown. Finally, our experimental results primarily focus on traditional deep learning tasks due to our GPU limitations. It would be beneficial to test the scalability of our theoretical results, e.g., on large language models.

Figure 4: Training loss vs. steps on different models and datasets. We use step-size with warm-up technique by default and test under different \(\alpha\).

Figure 3: Training loss vs. steps on different models and datasets. We use step-size without warm-up technique and test under different \(\alpha\).

## References

* Anil et al. [2019] Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory efficient adaptive optimization. In _Advances in Neural Information Processing Systems_, 2019.
* Arjevani et al. [2023] Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. _Mathematical Programming_, 199(1-2):165-214, 2023.
* Attia and Koren [2023] Amit Attia and Tomer Koren. SGD with AdaGrad stepsizes: full adaptivity with high probability to unknown parameters, unbounded gradients and affine variance. In _International Conference on Machine Learning_, 2023.
* Bottou et al. [2018] Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. _SIAM Review_, 60(2):223-311, 2018.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in Neural Information Processing Systems_, 33:1877-1901, 2020.
* Chowdhery et al. [2023] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 24(240):1-113, 2023.
* De et al. [2018] Soham De, Anirbit Mukherjee, and Enayat Ullah. Convergence guarantees for RMSProp and Adam in non-convex optimization and an empirical comparison to Nesterov acceleration. _arXiv preprint arXiv:1807.06766_, 2018.
* Defossez et al. [2022] Alexandre Defossez, Leon Bottou, Francis Bach, and Nicolas Usunier. A simple convergence proof of Adam and Adagrad. _Transactions on Machine Learning Research_, 2022.
* Duchi et al. [2011] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. _Journal of Machine Learning Research_, 12(7):2121-2159, 2011.
* Faw et al. [2022] Matthew Faw, Isidoros Tziotis, Constantine Caramanis, Aryan Mokhtari, Sanjay Shakkottai, and Rachel Ward. The power of adaptivity in SGD: self-tuning step sizes with unbounded gradients and affine variance. In _Conference on Learning Theory_, 2022.
* Guo et al. [2021] Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. A novel convergence analysis for algorithms of the Adam family. In _Annual Workshop on Optimization for Machine Learning_, 2021.
* Gupta et al. [2018] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization. In _International Conference on Machine Learning_, pages 1842-1850. PMLR, 2018.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2016.
* Kavis et al. [2022] Ali Kavis, Kfir Yehuda Levy, and Volkan Cevher. High probability bounds for a class of nonconvex algorithms with AdaGrad stepsize. In _International Conference on Learning Representations_, 2022.
* Kingma and Ba [2015] Diederik P Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In _International Conference on Learning Representations_, 2015.
* Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Li et al. [2024] Bingrui Li, Jianfei Chen, and Jun Zhu. Memory efficient optimizers with 4-bit states. _Advances in Neural Information Processing Systems_, 36, 2024.

* [18] Haochuan Li, Ali Jadbabaie, and Alexander Rakhlin. Convergence of Adam under relaxed assumptions. In _Advances in Neural Information Processing Systems_, 2023.
* [19] Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. In _International Conference on Artificial Intelligence and Statistics_, 2019.
* [20] Xiaoyu Li and Francesco Orabona. A high probability analysis of adaptive SGD with momentum. In _Workshop on International Conference on Machine Learning_, 2020.
* [21] Zijian Liu, Ta Duy Nguyen, Thien Hang Nguyen, Alina Ene, and Huy Nguyen. High probability convergence of stochastic gradient methods. In _International Conference on Machine Learning_, 2023.
* [22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2019.
* [23] Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang, Xin Jiang, and Yang You. CAME: Confidence-guided adaptive memory efficient optimization. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics_, 2023.
* [24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020.
* [25] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In _International Conference on Learning Representations_, 2017.
* [26] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In _International Conference on Machine Learning_, 2018.
* [27] Li Shen, Congliang Chen, Fangyu Zou, Zequn Jie, Ju Sun, and Wei Liu. A unified analysis of AdaGrad with weighted aggregation and momentum acceleration. _IEEE Transactions on Neural Networks and Learning Systems_, 2023.
* [28] Naichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun. RMSProp converges with proper hyper-parameter. In _International Conference on Learning Representations_, 2020.
* [29] Matthew Streeter and H Brendan McMahan. Less regret via online conditioning. _arXiv preprint arXiv:1002.4862_, 2010.
* [30] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude. _COURSERA: Neural Networks for Machine Learning_, 2012.
* [31] Bohan Wang, Jingwen Fu, Huishuai Zhang, Nanning Zheng, and Wei Chen. Closing the gap between the upper bound and lower bound of Adam's iteration complexity. In _Advances in Neural Information Processing Systems_, 2023.
* [32] Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: sharp convergence over nonconvex landscapes. _Journal of Machine Learning Research_, 21(1):9047-9076, 2020.
* [33] Junchi Yang, Xiang Li, and Niao He. Nest your adaptive algorithm for parameter-agnostic nonconvex minimax optimization. In _Advances in Neural Information Processing Systems_, 2022.
* [34] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods for nonconvex optimization. In _Advances in Neural Information Processing Systems_, 2018.
* [35] Matthew D Zeiler. Adadelta: an adaptive learning rate method. _arXiv preprint arXiv:1212.5701_, 2012.
* [36] Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. Adam can converge without any modification on update rules. In _Advances in Neural Information Processing Systems_, 2022.

* [37] Pengxiang Zhao, Ping Li, Yingjie Gu, Yi Zheng, Stephan Ludger Kolker, Zhefeng Wang, and Xiaoming Yuan. Adapprox: Adaptive approximation in adam optimization via randomized low-rank matrices. _arXiv preprint arXiv:2403.14958_, 2024.
* [38] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _arXiv preprint arXiv:2303.18223_, 2023.
* [39] Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for convergences of Adam and RMSProp. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2019.

Proof detail for full-batch case

We first provide the full-batch Adafactor as follows. The only difference to Algorithm (1) is the replacement of stochastic gradient by deterministic gradient \(\nabla f(\mathbf{X}_{k})\) at each iteration.

```
Input: Initialization point \(\mathbf{X}_{1}\in\mathbb{R}^{n\times m}\), \(\mathbf{R}_{0}=\mathbf{0}_{n},\mathbf{C}_{0}=\mathbf{0}_{m}^{\top}\), relative step-sizes \(\{\rho_{k}\}_{k\geq 1}\), decay rate \(\{\beta_{2,k}\}_{k\geq 1}\in[0,1)\), regularization constants \(\epsilon_{1},\epsilon_{2}>0\), clipping threshold \(d\). for\(k=1,\cdots,T\)do \(\bar{\mathbf{G}}_{k}=\nabla f(\mathbf{X}_{k})\); \(\bar{\mathbf{R}}_{k}=\beta_{2,k}\bar{\mathbf{R}}_{k-1}+(1-\beta_{2,k})(\bar{\mathbf{G}}_{k }\odot\bar{\mathbf{G}}_{k}+\epsilon_{1}\mathbf{1}_{n}\mathbf{1}_{m}^{\top})\mathbf{1}_{m}\); \(\bar{\mathbf{C}}_{k}=\beta_{2,k}\bar{\mathbf{C}}_{k-1}+(1-\beta_{2,k})\mathbf{1}_{n}^{\top }(\bar{\mathbf{G}}_{k}\odot\bar{\mathbf{G}}_{k}+\epsilon_{1}\mathbf{1}_{n}\mathbf{1}_{m}^{\top })\); \(\bar{\mathbf{W}}_{k}=(\bar{\mathbf{R}}_{k}\bar{\mathbf{C}}_{k})/\mathbf{1}_{n}^{\top}\bar{\mathbf{R }}_{k}\); \(\bar{\mathbf{U}}_{k}=\bar{\mathbf{G}}_{k}/\sqrt{\bar{\mathbf{W}}_{k}}\); \(\bar{\eta}_{k}=\max\{\epsilon_{2},\text{RMS}(\mathbf{X}_{k})\}\rho_{k}/\max\{1, \text{RMS}(\bar{\mathbf{U}}_{k})/d\}\); \(\mathbf{X}_{k+1}=\mathbf{X}_{k}-\hat{\eta}_{k}\cdot\bar{\mathbf{G}}_{k}/\sqrt{\bar{\mathbf{W}} _{k}}\); endfor
```

**Algorithm 2** Full-batch Adafactor

Then, we provide the detailed version of Theorem 5.1 as follows.

**Theorem A.1**.: _Let \(\{\mathbf{X}_{k}\}_{k\geq 1}\) be generated by Algorithm 2. If Assumptions (A1), (A2) hold, \(\|\nabla f(\mathbf{X}_{k})\|_{F}\leq G,\forall k\geq 1\) and_

\[\rho_{k}=\rho_{0}/\sqrt{k},\quad 0<\beta_{2,k}<1,\quad\forall k\geq 1,\]

_for some positive constant \(\rho_{0}\), then for any \(T\geq 1\),_

\[\begin{split}\min_{k\in[T]}\|\nabla f(\mathbf{X}_{k})\|_{F}^{2}& \leq\frac{A_{0}A_{1}(f(\mathbf{X}_{1})-f^{*}+\Delta_{0}^{2}\log T+ \Delta_{0}^{2})}{\sqrt{T}},\\ \min_{k\in[T]}\|\nabla f(\mathbf{X}_{k})\|_{F}^{2}&\leq \frac{A_{0}A_{1}^{\prime}(f(\mathbf{X}_{1})-f^{*}+\tilde{\Delta}_{0}^{2}\log T+ \Delta_{0}^{2})}{\sqrt{T}},\end{split} \tag{9}\]

_where we define_

\[\Theta_{\min}=\min_{k\in[T]}\|\mathbf{X}_{k}\|_{\infty},\quad\Theta_{\max}=\max_{k \in[T]}\|\mathbf{X}_{k}\|_{\infty},\quad\mathcal{G}=G^{2}+mn\epsilon_{1}, \tag{10}\]

_and the other constant parameters are given by_

\[\begin{split}&\Delta_{0}^{2}=\frac{Ld^{2}mn(\epsilon_{2}+\Theta_{ \max})^{2}\rho_{0}^{2}}{2},\quad\tilde{\Delta}_{0}^{2}=\frac{LG^{2}\mathcal{G} (\epsilon_{2}+\Theta_{\max})^{2}\rho_{0}^{2}}{2mn\epsilon_{1}^{2}(1-\beta_{2, 1})^{2}},\\ & A_{0}=\frac{\max\left\{1,\frac{G\sqrt{\mathcal{G}}}{d\epsilon_{ 1}mn(1-\beta_{2,1})}\right\}}{\rho_{0}\max\{\epsilon_{2},\Theta_{\min}\}},A_{ 1}=\sqrt{G^{4}+G^{2}(m+n)\epsilon_{1}+mn\epsilon_{1}^{2}},\\ & A_{1}^{\prime}=\sqrt{2\left(\frac{G^{4}}{mn\epsilon_{1}}+G^{2}+ \epsilon_{1}\right)}.\end{split} \tag{11}\]

### Preliminary

We first denote the auxiliary matrix \(\bar{\mathbf{G}}_{k,\epsilon_{1}}^{2}=\bar{\mathbf{G}}_{k}\odot\bar{\mathbf{G}}_{k}+ \epsilon_{1}\mathbf{1}_{n}\mathbf{1}_{m}^{\top}\). In addition, we define \(\bar{\mathbf{V}}_{k}=\left(\bar{v}_{ij}^{(k)}\right)_{ij}\) as follows,

\[\bar{\mathbf{V}}_{0}=\mathbf{0}_{n\times m},\quad\bar{\mathbf{V}}_{k}=\beta_{2,k}\bar{\mathbf{V }}_{k-1}+(1-\beta_{2,k})\bar{\mathbf{G}}_{k,\epsilon_{1}}^{2},\quad k\geq 1. \tag{12}\]

To simplify the notation, we let \(\bar{\mathbf{G}}_{k}=\left(\bar{g}_{ij}^{(k)}\right)_{ij}\), \(R_{\bar{\mathbf{V}}_{k}}^{(i)}\), \(C_{\bar{\mathbf{V}}_{k}}^{(j)}\) and \(S_{\bar{\mathbf{V}}_{k}}\) be the \(i\)-th row sum, \(j\)-th column sum and the coordinate sum of \(\bar{\mathbf{V}}_{k}\) respectively. The same definition principal is applied to the notation \(R_{\bar{\mathbf{G}}_{k,\epsilon_{1}}}^{(i)}\) and \(C_{\bar{\mathbf{G}}_{k,\epsilon_{1}}}^{(j)}\). We also use \(\bar{w}_{ij}^{(k)},\bar{v}_{ij}^{(k)},\bar{u}_{ij}^{(k)}\) to denote the coordinates of \(\bar{\mathbf{W}}_{k},\bar{\mathbf{V}}_{k},\bar{\mathbf{U}}_{k}\) in Algorithm 2 respectively. We also define values \(\mathcal{G}_{1},\mathcal{G}_{2},\mathcal{G}\) as follows:

\[\mathcal{G}_{1}=G^{2}+m\epsilon_{1},\quad\mathcal{G}_{2}=G^{2}+n\epsilon_{1}, \quad\mathcal{G}=G^{2}+mn\epsilon_{1}. \tag{13}\]

### Technical lemmas

Following the descent lemma for a \(L\)-smooth objective function \(f\), we derive that

\[f(\mathbf{Y})\leq f(\mathbf{X})+\langle\nabla f(\mathbf{X}),\mathbf{Y}-\mathbf{X}\rangle+\frac{L}{2} \|\mathbf{Y}-\mathbf{X}\|_{F}^{2},\quad\forall\mathbf{X},\mathbf{Y}\in\mathbb{R}^{n\times m}. \tag{14}\]

In the following, we will provide some necessary technical lemmas.

**Lemma A.1**.: _Let \(\beta_{2,k}\in(0,1)\) and \(\Gamma_{k}\) be defined by_

\[\Gamma_{0}=0,\quad\Gamma_{k}=\beta_{2,k}\Gamma_{k-1}+(1-\beta_{2,k}),\quad \forall k\geq 1.\]

_Then, \((1-\beta_{2,1})\leq\Gamma_{k}\leq 1,\forall k\geq 1\)._

Proof.: We could prove the result by induction. Since \(\Gamma_{0}=0\), it's easy to derive that \((1-\beta_{2,1})=\Gamma_{1}\leq 1\). Suppose that for any \(j\in[k-1]\), \((1-\beta_{2,1})\leq\Gamma_{j}\leq 1\). Then

\[\Gamma_{k}\geq\beta_{2,k}(1-\beta_{2,1})+(1-\beta_{2,k})\geq 1-\beta_{2,1}, \quad\Gamma_{k}\leq\beta_{2,k}+(1-\beta_{2,k})\leq 1.\]

The induction is then complete. 

**Lemma A.2**.: _Let \(\bar{\mathbf{V}}_{k}\) be defined in (12). For any \(k\geq 0\), it holds that_

\[\bar{\mathbf{R}}_{k}=\bar{\mathbf{V}}_{k}\mathbf{1}_{m},\quad\bar{\mathbf{C}}_{k}=\mathbf{1}_{n}^{ \top}\bar{\mathbf{V}}_{k},\quad S_{\bar{\mathbf{V}}_{k}}=\mathbf{1}_{n}^{\top}\bar{\mathbf{R}}_ {k}=\mathbf{1}_{n}^{\top}\bar{\mathbf{V}}_{k}\mathbf{1}_{m}.\]

_As a consequence,_

\[R_{\bar{\mathbf{V}}_{k}}^{(i)}=\beta_{2,k}R_{\bar{\mathbf{V}}_{k-1}}^{(i)}+(1-\beta_{2, k})R_{\bar{\mathbf{G}}_{k,\epsilon_{1}}}^{(i)},\quad C_{\bar{\mathbf{V}}_{k}}^{(j)}= \beta_{2,k}C_{\bar{\mathbf{V}}_{k-1}}^{(j)}+(1-\beta_{2,k})C_{\bar{\mathbf{G}}_{k, \epsilon_{1}}^{2}}^{(j)}. \tag{15}\]

Proof.: Note that \(\bar{\mathbf{R}}_{0}=\bar{\mathbf{V}}_{0}\mathbf{1}_{m}=\mathbf{0}_{n}\) and \(\bar{\mathbf{C}}_{0}=\mathbf{1}_{n}^{\top}\bar{\mathbf{V}}_{0}=\mathbf{0}_{m}^{\top}\). Suppose that for any \(j\leq k-1\), \(\bar{\mathbf{R}}_{j}=\bar{\mathbf{V}}_{j}\mathbf{1}_{m},\bar{\mathbf{C}}_{j}=\mathbf{1}_{n}^{\top} \bar{\mathbf{V}}_{j}\). Then using the updated rule in Algorithm 2 and (12),

\[\bar{\mathbf{R}}_{k} =\beta_{2,k}\bar{\mathbf{R}}_{k-1}+(1-\beta_{2,k})\bar{\mathbf{G}}_{k, \epsilon_{1}}^{(k)}\mathbf{1}_{m}=\left(\beta_{2,k}\bar{\mathbf{V}}_{k-1}+(1-\beta_{2, k})\bar{\mathbf{G}}_{k,\epsilon_{1}}^{2}\right)\mathbf{1}_{m}=\bar{\mathbf{V}}_{k}\mathbf{1}_{m}, \tag{16}\] \[\bar{\mathbf{C}}_{k} =\beta_{2,k}\bar{\mathbf{C}}_{k-1}+(1-\beta_{2,k})\mathbf{1}_{n}^{\top} \bar{\mathbf{G}}_{k,\epsilon_{1}}^{2}=\mathbf{1}_{n}^{\top}\left(\beta_{2,k}\bar{\mathbf{V }}_{k-1}+(1-\beta_{2,k})\bar{\mathbf{G}}_{k,\epsilon_{1}}^{2}\right)=\mathbf{1}_{n}^{ \top}\bar{\mathbf{V}}_{k}.\]

Since \(S_{\bar{\mathbf{V}}_{k}}\) represents the coordinate sum of \(\bar{\mathbf{V}}_{k}\), we could derive that

\[S_{\bar{\mathbf{V}}_{k}}=\sum_{i=1}^{n}\sum_{j=1}^{m}\bar{v}_{ij}^{(k)}=\mathbf{1}_{n}^ {\top}\bar{\mathbf{R}}_{k}=\mathbf{1}_{n}^{\top}\bar{\mathbf{V}}_{k}\mathbf{1}_{m}. \tag{17}\]

Since \(R_{\bar{\mathbf{V}}_{k}}^{(i)}\) denotes the \(i\)-th row sum of \(\bar{\mathbf{V}}_{k}\), it's the \(i\)-th coordinate of \(\bar{\mathbf{R}}_{k}\). Hence, for each coordinate of \(\bar{\mathbf{R}}_{k}\), using (16),

\[R_{\bar{\mathbf{V}}_{k}}^{(i)}=\beta_{2,k}R_{\bar{\mathbf{V}}_{k-1}}^{(i)}+(1-\beta_{2,k})R_{\bar{\mathbf{G}}_{k,\epsilon_{1}}}^{(i)}.\]

Similarly, we could derive the results related to \(C_{\bar{\mathbf{V}}_{k}}^{(j)}\). 

**Lemma A.3**.: _Following the parameter setting in (3), for any \(i\in[n],j\in[m],k\geq 1\), it holds that_

\[R_{\bar{\mathbf{V}}_{k}}^{(i)}\in[m\epsilon_{1}(1-\beta_{2,1}),\mathcal{G}_{1}], \quad C_{\bar{\mathbf{V}}_{k}}^{(j)}\in[n\epsilon_{1}(1-\beta_{2,1}),\mathcal{G}_ {2}],\quad S_{\bar{\mathbf{V}}_{k}}\in[mn\epsilon_{1}(1-\beta_{2,1}),\mathcal{G}].\]

Proof.: Recalling the definition of \(\bar{\mathbf{V}}_{k}\) in (12) and \(\|\nabla f(\mathbf{X}_{k})\|_{F}\leq G,\forall k\geq 1\), we derive that

\[S_{\bar{\mathbf{V}}_{k}} =\sum_{i=1}^{n}\sum_{j=1}^{m}\bar{v}_{ij}^{(k)}=\sum_{i=1}^{n}\sum _{j=1}^{m}\sum_{p=1}^{k}(1-\beta_{2,p})\left(\left(\bar{g}_{ij}^{(p)}\right) ^{2}+\epsilon_{1}\right)\left(\prod_{l=p+1}^{k}\beta_{2,l}\right)\] \[\leq\sum_{p=1}^{k}(1-\beta_{2,p})\left(\prod_{l=p+1}^{k}\beta_{2,l}\right)\|\bar{\mathbf{G}}_{p}\|_{F}^{2}+\Gamma_{k}mn\epsilon_{1}\leq G^{2} \Gamma_{k}+mn\epsilon_{1}\leq\mathcal{G}, \tag{18}\]where the last inequality comes from Lemma A.1. Following (18) and Lemma A.1, we also derive that

\[S_{\bar{\mathbf{V}}_{k}}\geq mn\epsilon_{1}\Gamma_{k}\geq mn\epsilon_{1}(1-\beta_{2, 1}).\]

We also derive the upper bounds for \(R^{(i)}_{\bar{\mathbf{V}}_{k}}\) and \(C^{(j)}_{\bar{\mathbf{V}}_{k}}\) as follows,

\[R^{(i)}_{\bar{\mathbf{V}}_{k}}=\sum_{j=1}^{m}\bar{v}^{(k)}_{ij}\leq \sum_{p=1}^{k}(1-\beta_{2,p})\left(\prod_{l=p+1}^{k}\beta_{2,l}\right)\left\| \bar{\mathbf{G}}_{p}\right\|_{F}^{2}+\Gamma_{k}m\epsilon_{1}\leq G^{2}\Gamma_{k}+m \epsilon_{1}\leq\mathcal{G}_{1},\] \[C^{(j)}_{\bar{\mathbf{V}}_{k}}=\sum_{i=1}^{n}\bar{v}^{(k)}_{ij}\leq \sum_{p=1}^{k}(1-\beta_{2,p})\left(\prod_{l=p+1}^{k}\beta_{2,l}\right)\left\| \bar{\mathbf{G}}_{p}\right\|_{F}^{2}+\Gamma_{k}n\epsilon_{1}\leq G^{2}\Gamma_{k}+n \epsilon_{1}\leq\mathcal{G}_{2}. \tag{19}\]

Similarly, the lower bound could be derived by

\[R^{(i)}_{\bar{\mathbf{V}}_{k}}\geq m\epsilon_{1}\Gamma_{k}\geq m\epsilon_{1}(1- \beta_{2,1}),\quad C^{(j)}_{\bar{\mathbf{V}}_{k}}\geq n\epsilon_{1}\Gamma_{k}\geq n \epsilon_{1}(1-\beta_{2,1}).\]

### Proof of Theorem a.1

Now we move to prove the main result. Using (14) and the updated rule in Algorithm 2,

\[f(\mathbf{X}_{k+1}) \leq f(\mathbf{X}_{k})+\langle\bar{\mathbf{G}}_{k},\mathbf{X}_{k+1}-\mathbf{X}_{k }\rangle+\frac{L}{2}\|\mathbf{X}_{k+1}-\mathbf{X}_{k}\|_{F}^{2}\] \[=f(\mathbf{X}_{k})-\hat{\eta}_{k}\left\langle\bar{\mathbf{G}}_{k},\frac{ \mathbf{G}_{k}}{\sqrt{\bar{\mathbf{W}}_{k}}}\right\rangle+\frac{L\hat{\eta}_{k}^{2}}{2} \left\|\frac{\mathbf{G}_{k}}{\sqrt{\bar{\mathbf{W}}_{k}}}\right\|_{F}^{2}.\]

We then re-arrange the order, sum up both sides over \(k\in[t]\) and apply \(f(\mathbf{X}_{t+1})\geq f^{*}\) from Assumption (A2) to get,

\[\underbrace{\sum_{k=1}^{t}\hat{\eta}_{k}\left\|\frac{\bar{\mathbf{G}}_{k}}{\sqrt{ \bar{\mathbf{W}}_{k}}}\right\|_{F}^{2}}_{(\mathbf{a})}\leq f(\mathbf{X}_{1})-f^{*}+ \underbrace{\frac{L}{2}\sum_{k=1}^{t}\hat{\eta}_{k}^{2}\left\|\frac{\bar{\mathbf{G }}_{k}}{\sqrt{\bar{\mathbf{W}}_{k}}}\right\|_{F}^{2}}_{(\mathbf{b})}. \tag{20}\]

Since \(\Theta_{\min}\leq\|\mathbf{X}_{k}\|_{\infty}\leq\Theta_{\max}\), we have \(\Theta_{\min}\leq\text{RMS}(\mathbf{X}_{k})\leq\Theta_{\max}\) for any \(k\geq 1\). Hence, using \(\hat{\eta}_{k}\) defined in Algorithm 2,

\[\hat{\eta}_{k}=\frac{\max\{\epsilon_{2},\text{RMS}(\mathbf{X}_{k})\}\rho_{k}}{\max \left\{1,\|\bar{\mathbf{U}}_{k}\|_{F}/(d\sqrt{mn})\right\}}\leq(\epsilon_{2}+\Theta _{\max})\rho_{k}\min\left\{1,\frac{d\sqrt{mn}}{\|\bar{\mathbf{U}}_{k}\|_{F}}\right\}. \tag{21}\]

Using (21), \(\bar{\mathbf{U}}_{k}=\bar{\mathbf{G}}_{k}/\sqrt{\bar{\mathbf{W}}_{k}}\), \(\Delta_{0}\) in (11) and \(\rho_{k}=\rho_{0}/\sqrt{k}\), we thus derive that

\[(\mathbf{b})\leq\frac{Ld^{2}mn(\epsilon_{2}+\Theta_{\max})^{2}}{2}\sum_{k=1}^{ t}\rho_{k}^{2}\cdot\frac{\|\bar{\mathbf{U}}_{k}\|_{F}^{2}}{\|\bar{\mathbf{U}}_{k}\|_{F}^{ 2}}=\Delta_{0}^{2}\sum_{k=1}^{t}\frac{1}{k}. \tag{22}\]

To lower bound **(a)**, we first discuss the maximum operator inside \(\hat{\eta}_{k}\). Let

\[E_{1}=\left\{k\in[t]\mid\|\bar{\mathbf{U}}_{k}\|_{F}\geq d\sqrt{mn}\right\},\quad E _{2}=\left\{k\in[t]\mid\|\bar{\mathbf{U}}_{k}\|_{F}\leq d\sqrt{mn}\right\}.\]

When \(k\in E_{1}\), since \(\|\mathbf{X}_{k}\|_{\infty}\geq\Theta_{\min}\), it derives that

\[\hat{\eta}_{k}\geq\frac{d\sqrt{mn}\max\{\epsilon_{2},\Theta_{\min}\}\rho_{k}}{ \|\bar{\mathbf{U}}_{k}\|_{F}}. \tag{23}\]

Using Lemma A.2, we first derive that \(\bar{w}^{(k)}_{ij}=(R^{(i)}_{\bar{\mathbf{V}}_{k}}C^{(j)}_{\bar{\mathbf{V}}_{k}})/S_{ \bar{\mathbf{V}}_{k}}\). Then, applying Lemma A.3 and \(\|\nabla f(\mathbf{X}_{k})\|_{F}\leq G\), we could upper bound \(\|\bar{\mathbf{U}}_{k}\|_{F}^{2}\) as follows,

\[\|\bar{\mathbf{U}}_{k}\|_{F}^{2}=\sum_{i=1}^{n}\sum_{j=1}^{m}\frac{\left(\bar{g}^{(k )}_{ij}\right)^{2}S_{\bar{\mathbf{V}}_{k}}}{R^{(i)}_{\bar{\mathbf{V}}_{k}}C^{(j)}_{ \bar{\mathbf{V}}_{k}}}\leq\frac{\|\bar{\mathbf{G}}_{k}\|_{F}^{2}\mathcal{G}}{mn \epsilon_{1}^{2}(1-\beta_{2,1})^{2}}\leq\frac{G^{2}\mathcal{G}}{mn\epsilon_{1 }^{2}(1-\beta_{2,1})^{2}}. \tag{24}\]Hence, combining with (23) and (24), we have

\[\sum_{k\in E_{1}}\hat{\eta}_{k}\left\|\frac{\mathbf{\bar{G}}_{k}}{ \sqrt[4]{\mathbf{\bar{W}}_{k}}}\right\|_{F}^{2} \geq d\sqrt{mn}\max\{\epsilon_{2},\Theta_{\min}\}\sum_{k\in E_{1}} \frac{\rho_{k}}{\|\mathbf{\bar{U}}_{k}\|_{F}}\left\|\frac{\mathbf{\bar{G}}_{k}}{\sqrt[4 ]{\mathbf{\bar{W}}_{k}}}\right\|_{F}^{2}\] \[\geq\frac{d\epsilon_{1}mn(1-\beta_{2,1})\max\{\epsilon_{2},\Theta_ {\min}\}}{G\sqrt{\mathcal{G}}}\sum_{k\in E_{1}}\rho_{k}\left\|\frac{\mathbf{\bar{G} }_{k}}{\sqrt[4]{\mathbf{\bar{W}}_{k}}}\right\|_{F}^{2}. \tag{25}\]

When \(k\in E_{2}\), we obtain that \(\hat{\eta}_{k}=\max\{\epsilon_{2},\mathsf{RMS}(\mathbf{X}_{k})\}\rho_{k}\geq\max\{ \epsilon_{2},\Theta_{\min}\}\rho_{k}\) and thus

\[\sum_{k\in E_{2}}\hat{\eta}_{k}\left\|\frac{\mathbf{\bar{G}}_{k}}{ \sqrt[4]{\mathbf{\bar{W}}_{k}}}\right\|_{F}^{2}\geq\max\{\epsilon_{2},\Theta_{\min }\}\sum_{k\in E_{2}}\rho_{k}\left\|\frac{\mathbf{\bar{G}}_{k}}{\sqrt[4]{\mathbf{\bar{W }}_{k}}}\right\|_{F}^{2}. \tag{26}\]

Combining with (25) and (26), we derive that

\[(\mathbf{a})\geq\max\{\epsilon_{2},\Theta_{\min}\}\min\left\{1, \frac{d\epsilon_{1}mn(1-\beta_{2,1})}{G\sqrt{\mathcal{G}}}\right\}\sum_{k=1}^{ t}\rho_{k}\left\|\frac{\mathbf{\bar{G}}_{k}}{\sqrt[4]{\mathbf{\bar{W}}_{k}}}\right\|_{F}^{2}. \tag{27}\]

We also derive from Lemma A.2 and Lemma A.3 that for any \(i\in[n],j\in[m]\),

\[\bar{w}_{ij}^{(k)}=\frac{R_{\hat{\mathbf{V}}_{k}}^{(i)}C_{\hat{\mathbf{V}}_{k}}^{(j)}}{ S_{\hat{\mathbf{V}}_{k}}}\leq\frac{R_{\hat{\mathbf{V}}_{k}}^{(i)}C_{\hat{\mathbf{V}}_{k}}^{(j)} }{\sqrt{R_{\hat{\mathbf{V}}_{k}}^{(i)}C_{\hat{\mathbf{V}}_{k}}^{(j)}}}\leq\sqrt{R_{ \hat{\mathbf{V}}_{k}}^{(i)}C_{\hat{\mathbf{V}}_{k}}^{(j)}}\leq\sqrt{\mathcal{G}_{1} \mathcal{G}_{2}}. \tag{28}\]

Using (28), we have

\[\left\|\frac{\mathbf{\bar{G}}_{k}}{\sqrt[4]{\mathbf{\bar{W}}_{k}}}\right\|_{F}^{2}=\sum _{i=1}^{n}\sum_{j=1}^{m}\frac{\left(\bar{g}_{ij}^{(k)}\right)^{2}}{\sqrt{\bar{ w}_{ij}^{(k)}}}\geq\frac{\|\mathbf{\bar{G}}_{k}\|_{F}^{2}}{\sqrt{\mathcal{G}_{1} \mathcal{G}_{2}}}=\frac{\|\mathbf{\bar{G}}_{k}\|_{F}^{2}}{A_{1}}, \tag{29}\]

where \(A_{1}\) has been defined in (11). Plugging (29) into (27), we derive that

\[(\mathbf{a})\geq\frac{\max\{\epsilon_{2},\Theta_{\min}\}}{A_{1}} \min\left\{1,\frac{d\epsilon_{1}mn(1-\beta_{2,1})}{G\sqrt{\mathcal{G}}}\right\} \sum_{k=1}^{t}\rho_{k}\|\mathbf{\bar{G}}_{k}\|_{F}^{2}. \tag{30}\]

Plugging (22) and (30) into (20), and using \(\rho_{k}=\rho_{0}/\sqrt{k}\), we thus derive that

\[\min_{k\in[t]}\|\mathbf{\bar{G}}_{k}\|_{F}^{2}\sum_{k=1}^{t}\frac{1 }{\sqrt{k}}\leq\sum_{k=1}^{t}\frac{\rho_{k}\|\mathbf{\bar{G}}_{k}\|_{F}^{2}}{\rho_ {0}}\leq A_{0}A_{1}\left(f(\mathbf{X}_{1})-f^{*}+\Delta_{0}^{2}\sum_{k=1}^{t}\frac {1}{k}\right),\]

where \(A_{0}\) is given in (11). Moreover, we have the following results,

\[\sum_{k=1}^{t}\frac{1}{k}\leq 1+\int_{1}^{t}\frac{1}{x}dx=1+\log t, \quad\sum_{k=1}^{t}\frac{1}{\sqrt{k}}\geq\sqrt{t}. \tag{31}\]

We thus derive the first desired result in (9) as follows,

\[\min_{k\in[t]}\|\mathbf{\bar{G}}_{k}\|_{F}^{2}\leq\frac{A_{0}A_{1}}{ \sqrt{t}}\left(f(\mathbf{X}_{1})-f^{*}+\Delta_{0}^{2}+\Delta_{0}^{2}\log t\right). \tag{32}\]

Avoiding the curse of dimensionTo derive a free-dimension numerator bound, we first derive from (21) and (24) with \(\rho_{k}=\rho_{0}/\sqrt{k}\) that

\[(\mathbf{b})\leq\frac{L(\epsilon_{2}+\Theta_{\max})^{2}}{2}\sum_ {k=1}^{t}\rho_{k}^{2}\|\mathbf{\bar{U}}_{k}\|_{F}^{2}\leq\frac{LG^{2}\mathcal{G}( \epsilon_{2}+\Theta_{\max})^{2}}{2mn\epsilon_{1}^{2}(1-\beta_{2,1})^{2}}\sum_ {k=1}^{t}\rho_{k}^{2}=\tilde{\Delta}_{0}^{2}\sum_{k=1}^{t}\frac{1}{k}, \tag{33}\]

where \(\tilde{\Delta}_{0}\) has been defined in (11). In addition, we derive from Lemma A.2, Lemma A.3 and (13) that

\[\bar{w}_{ij}^{(k)}=\frac{R_{\hat{\mathbf{V}}_{k}}^{(i)}C_{\hat{\mathbf{V}}_{k}}^{(j)}}{ S_{\hat{\mathbf{V}}_{k}}}\leq\frac{2\mathcal{G}_{1}\mathcal{G}_{2}}{mn\epsilon_{1}} \leq 2\left(\frac{G^{4}}{mn\epsilon_{1}}+G^{2}+\epsilon_{1}\right)=(A_{1}^{ \prime})^{2}, \tag{34}\]where we use \(m+n\leq mn\) and \(A_{1}^{\prime}\) in (11). Thereby, we have

\[\left\|\frac{\bar{\mathbf{G}}_{k}}{\sqrt[4]{\mathbf{\bar{W}}_{k}}}\right\|_{F}^{2}=\sum_ {i=1}^{n}\sum_{j=1}^{m}\frac{\left(\bar{g}_{ij}^{(k)}\right)^{2}}{\sqrt{\bar{w} _{ij}^{(k)}}}\geq\frac{\|\bar{\mathbf{G}}_{k}\|_{F}^{2}}{A_{1}^{\prime}}.\]

Combining with (27), we thus derive that

\[(\mathbf{a})\geq\frac{\max\{\epsilon_{2},\Theta_{\min}\}}{A_{1}^{\prime}}\min \left\{1,\frac{d\epsilon_{1}mn(1-\beta_{2,1})}{G\sqrt{\mathcal{G}}}\right\} \sum_{k=1}^{t}\rho_{k}\|\bar{\mathbf{G}}_{k}\|_{F}^{2} \tag{35}\]

Plugging (33) and (35) into (20), and using \(\rho_{k}=\rho_{0}/\sqrt{k}\), we derive that

\[\min_{k\in[t]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2}\sum_{k=1}^{t}\frac{1}{\sqrt{k}}\leq \sum_{k=1}^{t}\frac{\rho_{k}\|\bar{\mathbf{G}}_{k}\|_{F}^{2}}{\rho_{0}}\leq A_{0}A _{1}^{\prime}\left(f(\mathbf{X}_{1})-f^{*}+\tilde{\Delta}_{0}^{2}\sum_{k=1}^{t} \frac{1}{k}\right),\]

where \(A_{0}\) has been defined in (11). Using (31), we derive the second desired result in (9).

\[\min_{k\in[t]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2}\leq\frac{A_{0}A_{1}^{ \prime}}{\sqrt{t}}\left(f(\mathbf{X}_{1})-f^{*}+\tilde{\Delta}_{0}^{2}+\tilde{ \Delta}_{0}^{2}\log t\right). \tag{36}\]

## Appendix B Proof detail for stochastic Adafactor without update clipping

We first provide the detailed version of Theorem 6.1.

**Theorem B.1** (_Formal statement of Theorem 6.1_).: _Let \(\{\mathbf{X}_{k}\}_{k\geq 1}\) be generated by Algorithm 1 without update clipping where \(\eta_{k}\) is given by (5) for each \(k\geq 1\). If Assumptions (A1)-(A4) hold, and_

\[\beta_{2,1}=1/2,\quad\rho_{1}=\rho_{0},\] \[\beta_{2,k}=1-1/k^{c},\quad\rho_{k}=\rho_{0}/\sqrt{k},\quad\forall k \geq 2,\]

_for some constants \(1/2\leq c\leq 1,\rho_{0}>0\), then for any \(T\geq 1,\delta\in(0,1)\), we have the following results._

_When \(c=1\), with probability at least \(1-\delta\),_

\[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2} \leq\frac{C_{0}}{\sqrt{T}}\left(C_{1}\log\left(\frac{T}{\delta} \right)+C_{2}\log T+C_{2}+C_{3}\right), \tag{37}\] \[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2} \leq\frac{C_{0}^{\prime}}{\sqrt{T}}\left(C_{1}\log\left(\frac{T}{ \delta}\right)+(C_{2}^{\prime}+C_{3}^{\prime})\log T+C_{2}^{\prime}+C_{3}^{ \prime}\right). \tag{38}\]

_When \(1/2\leq c<1\), with probability at least \(1-\delta\),_

\[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2} \leq\frac{C_{0}}{\sqrt{T}}\left(C_{1}\log\left(\frac{T}{\delta} \right)+\frac{C_{2}}{1-c}\cdot T^{1-c}+C_{2}+C_{3}\right), \tag{39}\] \[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2} \leq\frac{C_{0}}{\sqrt{T}}\left(C_{1}\log\left(\frac{T}{\delta} \right)+\frac{2C_{2}^{\prime}}{1-c}\cdot T^{1-c}+C_{3}^{\prime}\log T+C_{2}^{ \prime}+C_{3}^{\prime}\right). \tag{40}\]

_Here, \(\Theta_{\min},\Theta_{\max}\) and \(\mathcal{G}\) are as in (10), and_

\[C_{1}=f(\mathbf{X}_{1})-f^{*}+\frac{24G^{2}(\epsilon_{2}+\Theta_{\max})\rho_{0}}{ \sqrt{\epsilon_{1}}}. \tag{41}\]

_The \(C_{0},C_{2},C_{3}\) are constants defined as_

\[C_{0}=\frac{2\sqrt{2\mathcal{G}}}{\rho_{0}\max\{\epsilon_{2}, \Theta_{\min}\}},\quad C_{3}=\frac{C_{2}}{4}\log\left(2+\frac{2G^{2}}{ \epsilon_{1}}\right),\] \[C_{2}=\frac{32mn\mathcal{G}^{\frac{2}{2}}(\epsilon_{2}+\Theta_{ \max})\rho_{0}}{\max\{m,n\}\epsilon_{1}}+\frac{4Lmn\mathcal{G}(\epsilon_{2}+ \Theta_{\max})^{2}\rho_{0}^{2}}{\max\{m,n\}\epsilon_{1}}. \tag{42}\]_The \(C_{0}^{\prime},C_{2}^{\prime},C_{3}^{\prime}\) are positive constants (that could be further upper bounded by constants independent from \(m,n\)), defined by_

\[C_{0}^{\prime}=\frac{2\sqrt{2\left(\frac{G^{2}}{mn\epsilon_{1}}+G+\epsilon_{1} \right)}}{\rho_{0}\max\{\epsilon_{2},\Theta_{\min}\}},C_{2}^{\prime}=4G_{3}(G_ {1}+G_{2})(\epsilon_{2}+\Theta_{\max})\rho_{0},C_{3}^{\prime}=\frac{LG_{3}( \epsilon_{2}+\Theta_{\max})^{2}\rho_{0}^{2}}{2}, \tag{43}\]

_and \(G_{1},G_{2},G_{3}\) are given by_

\[G_{1} =\sqrt{6\left(\frac{G^{4}}{mn\epsilon_{1}}+G^{2}+\epsilon_{1} \right)},\quad G_{3}=\frac{4(G^{4}+G^{2}mn\epsilon_{1})}{mn\epsilon_{1}^{2}},\] \[G_{2} =2\left(\frac{G^{3}}{mn\epsilon_{1}}+\frac{2G^{2}}{\sqrt{mn \epsilon_{1}}}+\frac{G}{\sqrt{mn}}+G+\sqrt{\epsilon_{1}}\right). \tag{44}\]

Calculation of hyper-parameter dependencyTo derive a free dimension bound, we shall use the convergence bounds in (38) and (40). From (43), it's easy to show that \(m,n\) could only exist in the denominator of \(C_{0}^{\prime},C_{2}^{\prime},C_{3}^{\prime}\), which could avoid the curse of dimension.

To calculate the dependency of \(\epsilon_{1}\), we first show that its dependency in coefficients \(C_{0},C_{1},C_{2},C_{3}\) as follows, based on the assumption that \(0<\epsilon_{1}<1\),

\[C_{0}\sim\mathcal{O}\left(1\right),\quad C_{1}\sim\mathcal{O}\left(1/\sqrt{ \epsilon_{1}}\right),\quad C_{2}\sim\mathcal{O}\left(1/\epsilon_{1}\right), \quad C_{3}\sim\mathcal{O}\left(C_{2}\log(1/\epsilon_{1})\right). \tag{45}\]

Thereby, with the convergence bounds in (37) and (39), it's easy to show that

\[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2}\leq\mathcal{O}\left(\epsilon_{1}^{- 1}\log(1/\epsilon_{1})\right). \tag{46}\]

**Proposition B.1**.: _Following the same assumptions and settings in Theorem 6.1, then with probability at least \(1-\delta\),_

\[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2}\leq\frac{C_{0}}{\sqrt{T}}\left(C_{ 1}\log\left(\frac{T}{\delta}\right)+C_{2}\sum_{k=1}^{T}\frac{1}{k^{c}}+C_{3} \right),\]

_and with probability at least \(1-\delta\),_

\[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2}\leq\frac{C_{0}^{\prime}}{\sqrt{T}} \left(C_{1}\log\left(\frac{T}{\delta}\right)+C_{2}^{\prime}\sum_{k=1}^{T}\frac {1}{k^{c/2+1/2}}+C_{3}^{\prime}\sum_{k=1}^{T}\frac{1}{k}\right),\]

_where all constants are given as in Theorem B.1._

### Preliminary

We first follow the notations of \(\bar{\mathbf{G}}_{k}=\left(\bar{g}_{ij}^{(k)}\right)_{ij}\) and \(\mathcal{G},\mathcal{G}_{1},\mathcal{G}_{2}\) in (13). Let \(\mathbf{G}_{k}=\left(g_{ij}^{(k)}\right)_{ij}\) and \(\mathbf{\xi}_{k}=\mathbf{G}_{k}-\bar{\mathbf{G}}_{k}\). We also define \(\mathbf{G}_{k,\epsilon_{1}}^{2}=\mathbf{G}_{k}\odot\mathbf{G}_{k}+\epsilon_{1}\mathbf{1}_{n} \mathbf{1}_{m}^{\top}\) and \(\mathbf{V}_{k}=\left(v_{ij}^{(k)}\right)_{ij}\) as follows,

\[\mathbf{V}_{0}=\mathbf{0}_{n\times m},\quad\mathbf{V}_{k}=\beta_{2,k}\mathbf{V}_{k-1}+(1-\beta _{2,k})\mathbf{G}_{k,\epsilon_{1}}^{2},\quad k\geq 1. \tag{47}\]

We also define \(R_{\mathbf{V}_{k}}^{(i)},C_{\mathbf{V}_{k}}^{(j)}\) and \(S_{\mathbf{V}_{k}}\) as the \(i\)-th row sum, \(j\)-th column sum and coordinate sum of \(\mathbf{V}_{k}\) respectively. \(R_{\mathbf{G}_{k,\epsilon_{1}}^{2}}^{(i)}\) and \(C_{\mathbf{G}_{k,\epsilon_{1}}^{2}}^{(j)}\) represent the same definitions with respect to \(\mathbf{G}_{k,\epsilon_{1}}^{2}\). Then, using a similar deduction in Lemma A.2, we also obtain that for all \(k\geq 1\),

\[R_{\mathbf{V}_{k}}^{(i)}=\beta_{2,k}R_{\mathbf{V}_{k-1}}^{(i)}+(1-\beta_{2,k})\mathbf{G}_{k,\epsilon_{1}}^{2}\mathbf{1}_{m},\quad C_{\mathbf{V}_{k}}^{(j)}=\beta_{2,k}C_{\mathbf{V}_{ k-1}}^{(j)}+(1-\beta_{2,k})\mathbf{1}_{n}^{\top}\mathbf{G}_{k,\epsilon_{1}}^{2}. \tag{48}\]

As a consequence of (48), each coordinate of \(\mathbf{W}_{k}\) satisfies that

\[w_{ij}^{(k)}=\frac{R_{\mathbf{V}_{k}}^{(i)}C_{\mathbf{V}_{k}}^{(j)}}{S_{\mathbf{V}_{k}}}= \frac{\left(\beta_{2,k}R_{\mathbf{V}_{k-1}}^{(i)}+(1-\beta_{2,k})R_{\mathbf{G}_{k, \epsilon_{1}}^{(i)}}^{(i)}\right)\left(\beta_{2,k}C_{\mathbf{V}_{k-1}}^{(j)}+(1- \beta_{2,k})C_{\mathbf{G}_{k,\epsilon_{1}}^{2}}^{(j)}\right)}{\beta_{2,k}S_{\mathbf{V}_ {k-1}}+(1-\beta_{2,k})S_{\mathbf{G}_{k,\epsilon_{1}}^{2}}}. \tag{49}\]Next, we introduce a proxy step-size matrix \(\mathbf{A}_{k}=\left(a_{ij}^{(k)}\right)_{ij}\) such that

\[a_{ij}^{(k)}=\frac{\left(\beta_{2,k}R_{\mathbf{V}_{k-1}}^{(i)}+(1-\beta_{2,k})\mathcal{ G}_{1}\right)\left(\beta_{2,k}C_{\mathbf{V}_{k-1}}^{(j)}+(1-\beta_{2,k})\mathcal{G}_{2} \right)}{\beta_{2,k}S_{\mathbf{V}_{k-1}}+(1-\beta_{2,k})\mathcal{G}}. \tag{50}\]

The proxy step-size technique is a standard way in the convergence analysis of adaptive methods, e.g., [32, 8]. We provide a new proxy step-size in (50) to handle the matrix factorization in Adafactor. This construction satisfies two properties. First, it's independent from \(\mathbf{Z}_{k}\) in order to disrupt the correlation of stochastic gradients and adaptive step-sizes. Second, it needs to remain sufficiently close to the original adaptive step-size \(w_{ij}^{(k)}\) to avoid generating divergent terms.

### Technical lemmas

In the following, we first provide some more necessary technical lemmas. We introduce a concentration inequality for the martingale difference sequence, see [20] for a proof.

**Lemma B.1**.: _Suppose that \(\{Z_{s}\}_{s\in[T]}\) is a martingale difference sequence with respect to \(\zeta_{1},\cdots,\zeta_{T}\). Assume that for each \(s\in[T]\), \(\sigma_{s}\) is a random variable dependent on \(\zeta_{1},\cdots,\zeta_{s-1}\) and satisfies that_

\[\mathbb{E}\left[\exp\left(\frac{Z_{s}^{2}}{\sigma_{s}^{2}}\right)\mid\zeta_{1},\cdots,\zeta_{s-1}\right]\leq\mathrm{e}.\]

_Then for any \(\lambda>0\), and for any \(\delta\in(0,1)\), it holds that_

\[\mathbb{P}\left(\sum_{s=1}^{T}Z_{s}>\frac{1}{\lambda}\log\left(\frac{1}{\delta} \right)+\frac{3}{4}\lambda\sum_{s=1}^{T}\sigma_{s}^{2}\right)\leq\delta.\]

**Lemma B.2**.: _Following the parameter setting in (6), for any \(i\in[n],j\in[m],k\geq 1\), it holds that_

\[R_{\mathbf{G}_{k,\epsilon_{1}}^{(i)}}^{(i)},R_{\mathbf{V}_{k}}^{(i)}\in[m\epsilon_{1}/2,\mathcal{G}_{1}],\quad C_{\mathbf{G}_{k,\epsilon_{1}}^{(j)}}^{(j)},C_{\mathbf{V}_{k}} ^{(j)}\in[n\epsilon_{1}/2,\mathcal{G}_{2}],\quad S_{\mathbf{G}_{k,\epsilon_{1}}^{ 2}},S_{\mathbf{V}_{k}}\in[mn\epsilon_{1}/2,\mathcal{G}].\]

Proof.: First, using Assumption (A4), we derive that

\[mne_{1}/2\leq S_{\mathbf{G}_{k,\epsilon_{1}}^{2}}=\sum_{i=1}^{n}\sum _{j=1}^{m}\left(\left(g_{ij}^{(k)}\right)^{2}+\epsilon_{1}\right)=\|\mathbf{G}_{k }\|_{F}^{2}+mne_{1}\leq\mathcal{G},\] \[m\epsilon_{1}/2\leq R_{\mathbf{G}_{k,\epsilon_{1}}^{2}}^{(i)}=\sum_{ j=1}^{m}\left(\left(g_{ij}^{(k)}\right)^{2}+\epsilon_{1}\right)\leq\|\mathbf{G}_{k}\|_{F} ^{2}+m\epsilon_{1}\leq\mathcal{G}_{1},\] \[n\epsilon_{1}/2\leq C_{\mathbf{G}_{k,\epsilon_{1}}^{2}}^{(j)}=\sum_{ i=1}^{n}\left(\left(g_{ij}^{(k)}\right)^{2}+\epsilon_{1}\right)\leq\|\mathbf{G}_{k}\|_{F} ^{2}+n\epsilon_{1}\leq\mathcal{G}_{2}.\]

Using the similar deduction for Lemma A.3, we could show that \(m\epsilon_{1}(1-\beta_{2,1})\leq R_{\mathbf{V}_{k}}^{(i)}\leq\mathcal{G}_{1}\). Since \(\beta_{2,1}=1/2\) from (6), we then obtain the desired result. The bounds for \(C_{\mathbf{V}_{k}}^{(j)},S_{\mathbf{V}_{k}}\) could be also derived by using similar arguments. 

We have the following lemma to upper bound each coordinate of the proxy step-size matrix \(\mathbf{A}_{k}\) defined in (50).

**Lemma B.3**.: _For any \(k\geq 1\), it holds that_

\[\beta_{2,k}(1-\beta_{2,k})\epsilon_{1}\leq a_{ij}^{(k)}\leq 2\min\left\{ \mathcal{G},\frac{G^{2}}{mn\epsilon_{1}}+G+\epsilon_{1}\right\},\quad\forall i \in[n],j\in[m].\]

Proof.: We first have

\[\frac{\beta_{2,k}R_{\mathbf{V}_{k-1}}^{(i)}+(1-\beta_{2,k})\mathcal{G}_{1}}{\beta_{ 2,k}S_{\mathbf{V}_{k-1}}+(1-\beta_{2,k})\mathcal{G}}\leq\frac{\beta_{2,k}R_{\mathbf{V} _{k-1}}^{(i)}}{\beta_{2,k}S_{\mathbf{V}_{k-1}}}+\frac{(1-\beta_{2,k})\mathcal{G}_{1 }}{(1-\beta_{2,k})\mathcal{G}}\leq 2. \tag{51}\]Then, recalling the definition of \(a_{ij}^{(k)}\) in (50) and Lemma B.2, it derives that \(C_{\mathbf{V}_{k-1}}^{(j)}\leq\mathcal{G}_{2}\) and thereby \(\beta_{2,k}C_{\mathbf{V}_{k-1}}^{(j)}+(1-\beta_{2,k})\mathcal{G}_{2}\leq\mathcal{G} _{2}\leq\mathcal{G}\). Then combining with (51), we derive \(a_{ij}^{(k)}\leq 2\mathcal{G}\). We also derive a free dimension bound from Lemma B.2 for \(a_{ij}^{(k)}\) as follows,

\[a_{ij}^{(k)}\leq\frac{2\mathcal{G}_{1}\mathcal{G}_{2}}{mn\epsilon_{1}}=\frac{2( G^{2}+G(m+n)\epsilon_{1}+mn\epsilon_{1}^{2})}{mn\epsilon_{1}}\leq 2\left(\frac{G^{2} }{mn\epsilon_{1}}+G+\epsilon_{1}\right),\]

where we use \(m+n\leq mn\) when \(m,n\geq 2\) and \(\beta_{2,k}S_{\mathbf{V}_{k-1}}+(1-\beta_{2,k})\mathcal{G}\geq mn\epsilon_{1}/2\). To lower bound \(a_{ij}^{(k)}\), we derive from Lemma B.2 that \(\beta_{2,k}S_{\mathbf{V}_{k-1}}+(1-\beta_{2,k})\mathcal{G}\leq\mathcal{G}\). Thereby,

\[a_{ij}^{(k)} \geq\frac{\beta_{2,k}(1-\beta_{2,k})\left(R_{\mathbf{V}_{k-1}}^{(i)} \mathcal{G}_{2}+C_{\mathbf{V}_{k-1}}^{(j)}\mathcal{G}_{1}\right)}{\mathcal{G}}\geq \beta_{2,k}(1-\beta_{2,k})\cdot\frac{(m\mathcal{G}_{2}+n\mathcal{G}_{1}) \epsilon_{1}}{2\mathcal{G}}\] \[=\beta_{2,k}(1-\beta_{2,k})\cdot\frac{[(m+n)G^{2}+2mn\epsilon_{1} ]\epsilon_{1}}{2(G^{2}+mn\epsilon_{1})}\geq\beta_{2,k}(1-\beta_{2,k})\epsilon_ {1}.\]

**Lemma B.4**.: _Let \(\mathbf{W}_{k}\) and \(\mathbf{V}_{k}\) be defined in Algorithm 1 without update clipping where \(\eta_{k}\) is given by (5) and (47) respectively. For any \(k\geq 1\), it holds that_

\[\left\|\frac{\mathbf{G}_{k}}{\sqrt{\mathbf{W}_{k}}}\right\|_{F}^{2}\leq\frac{2 \mathcal{G}}{\max\{m,n\}\epsilon_{1}}\left\|\frac{\mathbf{G}_{k}}{\sqrt{\mathbf{V}_{k }}}\right\|_{F}^{2}.\]

Proof.: Recalling (49), \(v_{ij}^{(k)}\leq R_{\mathbf{V}_{k}}^{(i)}\),\(v_{ij}^{(k)}\leq C_{\mathbf{V}_{k}}^{(j)}\) and Lemma B.2, one could verify that

\[\frac{\left(g_{ij}^{(k)}\right)^{2}}{w_{ij}^{(k)}}=\frac{\left(g_ {ij}^{(k)}\right)^{2}S_{\mathbf{V}_{k}}}{R_{\mathbf{V}_{k}}^{(i)}C_{\mathbf{V}_{k}}^{(j)} }\leq\frac{2\left(g_{ij}^{(k)}\right)^{2}\mathcal{G}}{n\epsilon_{1}v_{ij}^{(k )}},\quad\frac{\left(g_{ij}^{(k)}\right)^{2}}{w_{ij}^{(k)}}=\frac{\left(g_{ij }^{(k)}\right)^{2}S_{\mathbf{V}_{k}}}{R_{\mathbf{V}_{k}}^{(i)}C_{\mathbf{V}_{k}}^{(j)}} \leq\frac{2\left(g_{ij}^{(k)}\right)^{2}\mathcal{G}}{m\epsilon_{1}v_{ij}^{(k )}},\]

which leads to the desired result that

\[\|\mathbf{U}_{k}\|_{F}^{2}=\left\|\frac{\mathbf{G}_{k}}{\sqrt{\mathbf{W}_{k}}} \right\|_{F}^{2}\leq\frac{2\mathcal{G}}{\max\{m,n\}\epsilon_{1}}\left\|\frac{ \mathbf{G}_{k}}{\sqrt{\mathbf{V}_{k}}}\right\|_{F}^{2}.\]

The following lemma is inspired by [8, Lemma 5.2] where they considered a constant \(\beta_{2,k}\). Here, we generalize the result to the case of time-varying \(\beta_{2,k}\) and provide the proof detail.

**Lemma B.5**.: _For any \(t\geq 1\), if \(\beta_{2,k}\) are as in (6), then it holds that_

\[\sum_{k=1}^{t}(1-\beta_{2,k})\left\|\frac{\mathbf{G}_{k}}{\sqrt{\mathbf{V}_{k}}}\right\| _{F}^{2}\leq mn\log\left(\frac{2(G^{2}+\epsilon_{1})}{\epsilon_{1}}\right)+4mn \sum_{k=1}^{t}(1-\beta_{2,k}).\]

Proof.: Recalling the definition of \(\mathbf{V}_{k}\) and since \(\mathbf{V}_{0}=\mathbf{0}_{n\times m}\), we have that for any \(k\geq 1\),

\[v_{ij}^{(k)} =\beta_{2,k}v_{ij}^{(k-1)}+(1-\beta_{2,k})\left[\left(g_{ij}^{(k) }\right)^{2}+\epsilon_{1}\right]\] \[=\sum_{p=1}^{k}(1-\beta_{2,p})\left[\left(g_{ij}^{(p)}\right)^{2 }+\epsilon_{1}\right]\left(\prod_{l=p+1}^{k}\beta_{2,l}\right).\]

Then, we have

\[(1-\beta_{2,k})\cdot\frac{\left(g_{ij}^{(k)}\right)^{2}}{v_{ij}^{(k)}}=\frac{x_ {k}}{y_{k}+\theta_{k}}, \tag{52}\]where we set \(y_{0}=0,\theta_{0}=0\) and

\[x_{k} =(1-\beta_{2,k})\left(g_{ij}^{(k)}\right)^{2},\quad y_{k}=\sum_{p=1 }^{k}(1-\beta_{2,p})\left(g_{ij}^{(p)}\right)^{2}\left(\prod_{l=p+1}^{k}\beta_{ 2,l}\right),\] \[\theta_{k} =\epsilon_{1}\sum_{p=1}^{k}(1-\beta_{2,p})\left(\prod_{l=p+1}^{k} \beta_{2,l}\right),\quad\forall k\geq 1.\]

Then we have \(y_{k}-x_{k}=\beta_{2,k}y_{k-1},\forall k\geq 1\). Moreover, since \(y_{k}\geq x_{k}\), we could use \(\log x\geq 1-1/x,\forall x\geq 1\) to derive that

\[\frac{x_{k}}{y_{k}+\theta_{k}} \leq\log(y_{k}+\theta_{k})-\log(y_{k}+\theta_{k}-x_{k})=\log(y_{ k}+\theta_{k})-\log(\beta_{2,k}y_{k-1}+\theta_{k})\] \[=\log\left(\frac{y_{k}+\theta_{k}}{y_{k-1}+\theta_{k-1}}\right)+ \log\left(\frac{y_{k-1}+\theta_{k-1}}{\beta_{2,k}y_{k-1}+\theta_{k}}\right).\]

Noting that \(\theta_{k}=\beta_{2,k}\theta_{k-1}+(1-\beta_{2,k})\epsilon_{1}\), which leads to \(\beta_{2,k}\theta_{k-1}\leq\theta_{k}\). Hence, we further have

\[\frac{x_{k}}{y_{k}+\theta_{k}}\leq\log\left(\frac{y_{k}+\theta_{k}}{y_{k-1}+ \theta_{k-1}}\right)+\log\left(\frac{y_{k-1}+\theta_{k-1}}{\beta_{2,k}(y_{k-1} +\theta_{k-1})}\right)=\log\left(\frac{y_{k}+\theta_{k}}{y_{k-1}+\theta_{k-1}} \right)-\log\beta_{2,k}. \tag{53}\]

Hence, summing up on both sides of (52) and (53) over \(k\in[t]\), and noting that \(x_{1}=y_{1}\), we obtain that

\[\sum_{k=1}^{t}(1-\beta_{2,k})\cdot\frac{\left(g_{ij}^{(k)}\right) ^{2}}{v_{ij}^{(k)}}=\frac{x_{1}}{y_{1}+\theta_{1}}+\sum_{k=2}^{t}\frac{x_{k}}{ y_{k}+\epsilon_{k}}\] \[\leq 1+\log\left(\frac{y_{t}+\theta_{t}}{y_{1}+\theta_{1}}\right)- \sum_{k=2}^{t}\log\beta_{2,k}. \tag{54}\]

Note that \(y_{1}+\theta_{1}\geq(1-\beta_{2,1})\epsilon_{1}=\epsilon_{1}/2\). Moreover, using Lemma A.1 and Assumption (A4), we have \(\theta_{t}=\Gamma_{t}\epsilon_{1}\leq\epsilon_{1}\) and \(y_{t}\leq\Gamma_{t}G^{2}\leq G^{2}\). We then derive that

\[\frac{y_{t}+\theta_{t}}{y_{1}+\theta_{1}}\leq\frac{2(G^{2}+\epsilon_{1})}{ \epsilon_{1}}. \tag{55}\]

Noting that for \(k\geq 2\), \(c\in[1/2,1]\), \(\beta_{2,k}\geq\beta_{2,2}=1-1/2^{c}\geq 1-1/\sqrt{2}\), we then derive that

\[-\log\beta_{2,k}\leq\frac{1-\beta_{2,k}}{\beta_{2,k}}\leq\frac{\sqrt{2}(1- \beta_{2,k})}{\sqrt{2}-1}\leq 4(1-\beta_{2,k}). \tag{56}\]

Finally, plugging (55), (56) into (54), and then summing (54) up over \(i\in[n],j\in[m]\), we obtain the desired result. 

Next, we have the following probabilistic result relying on the property of the martingale difference sequence which is commonly used in the analysis of adaptive methods.

**Lemma B.6**.: _Following the parameter setting in (6), for any \(T\geq 1\) and \(\lambda>0\), with probability at least \(1-\delta\), \(\forall t\in[T],\)_

\[-\sum_{k=1}^{t}\eta_{k}\left\langle\bar{\mathbf{G}}_{k},\frac{\mathbf{\xi}_{k}}{\sqrt{ \mathbf{A}_{k}}}\right\rangle\leq\frac{1}{4}\sum_{k=1}^{t}\eta_{k}\left\|\frac{ \bar{\mathbf{G}}_{k}}{\sqrt[4]{\mathbf{A}_{k}}}\right\|_{F}^{2}+\frac{24G^{2}(\epsilon_ {2}+\Theta_{\max})\rho_{0}}{\sqrt{\epsilon_{1}}}\log\left(\frac{T}{\delta} \right).\]

Proof.: Let \(\zeta_{k}=-\eta_{k}\left\langle\bar{\mathbf{G}}_{k},\frac{\mathbf{\xi}_{k}}{\sqrt{\mathbf{ A}_{k}}}\right\rangle\) and the filtration \(\mathcal{F}_{k}=\sigma\left(\mathbf{Z}_{1},\cdots,\mathbf{Z}_{k}\right)\) where \(\sigma(\cdot)\) denotes the \(\sigma\)-algebra. Note that \(\eta_{k}\), \(\bar{\mathbf{G}}_{k}\) and \(\mathbf{A}_{k}\) are dependent by \(\{\mathbf{X}_{1},\cdots,\mathbf{X}_{k-1}\}\) and thereby \(\mathcal{F}_{k-1}\). Since \(\mathbf{\xi}_{k}\) is dependent by \(\mathcal{F}_{k}\), we could prove that \(\{\zeta_{k}\}_{k\geq 1}\) is a martingale difference sequence since

\[\mathbb{E}\left[\zeta_{k}\mid\mathcal{F}_{k-1}\right]=-\eta_{k}\left\langle \bar{\mathbf{G}}_{k},\frac{\mathbb{E}\left[\mathbf{\xi}_{k}\mid\mathcal{F}_{k-1}\right] }{\sqrt{\mathbf{A}_{k}}}\right\rangle=0,\]where we apply that \(\mathbb{E}\left[\mathbf{\xi}_{k}\mid\mathcal{F}_{k-1}\right]=\mathbb{E}_{\mathbf{Z}_{k}}[ \mathbf{\xi}_{k}]=0\) from Assumption (A3). Then, using Assumption (A3) and Assumption (A4), we have

\[\|\mathbf{\tilde{G}}_{k}\|_{F}=\|\mathbb{E}_{\mathbf{Z}_{k}}[\mathbf{G}_{k}]\|_{F}\leq \mathbb{E}_{\mathbf{Z}_{k}}\|\mathbf{G}_{k}\|_{F}\leq G,\quad\|\mathbf{\xi}_{k}\|_{F}=\|\bm {G}_{k}-\mathbf{\tilde{G}}_{k}\|_{F}\leq 2G.\]

Let \(\omega_{k}=2G\eta_{k}\left\|\frac{\mathbf{\tilde{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\| _{F}\). We thus derive from the Cauchy-Schwarz inequality that

\[\mathbb{E}\left[\exp\left(\frac{\zeta_{k}^{2}}{\omega_{k}^{2}} \right)\mid\mathcal{F}_{k-1}\right]\leq\mathbb{E}\left[\exp\left(\frac{\left\| \frac{\mathbf{\tilde{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\|_{F}^{2}\|\mathbf{\tilde{G}}_ {k}\|_{F}^{2}}{4G^{2}\left\|\frac{\mathbf{\tilde{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\| _{F}^{2}}\right)\mid\mathcal{F}_{k-1}\right]\leq\exp(1).\]

Then, using Lemma B.1, it leads to that for any \(\lambda>0\), with probability at least \(1-\delta\),

\[-\sum_{k=1}^{t}\eta_{k}\left\langle\mathbf{\tilde{G}}_{k},\frac{\mathbf{ \xi}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\rangle \leq 3\lambda G^{2}\sum_{k=1}^{t}\eta_{k}^{2}\left\|\frac{\mathbf{\tilde{ G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\|_{F}^{2}+\frac{1}{\lambda}\log\left(\frac{1}{ \delta}\right) \tag{57}\] \[=3\lambda G^{2}\sum_{k=1}^{t}\sum_{i=1}^{n}\sum_{j=1}^{m}\frac{ \eta_{k}}{\sqrt{a_{ij}^{(k)}}}\cdot\eta_{k}\frac{\left(\bar{g}_{ij}^{(k)} \right)^{2}}{\sqrt{a_{ij}^{(k)}}}+\frac{1}{\lambda}\log\left(\frac{1}{\delta} \right).\]

Meanwhile, when \(\Theta_{\min}\leq\|\mathbf{X}_{k}\|_{\infty}\leq\Theta_{\max}\), \(\rho_{k}=\rho_{0}/\sqrt{k}\), we have

\[\Theta_{\min}\leq\text{RMS}(\mathbf{X}_{k})\leq\Theta_{\max},\quad\frac{\max\{ \epsilon_{2},\Theta_{\min}\}\rho_{0}}{\sqrt{k}}\leq\eta_{k}\leq\frac{(\epsilon _{2}+\Theta_{\max})\rho_{0}}{\sqrt{k}}. \tag{58}\]

Combining with Lemma B.3, we derive that

\[\frac{\eta_{k}}{\sqrt{a_{ij}^{(k)}}} \leq\frac{\eta_{k}}{\sqrt{\beta_{2,k}(1-\beta_{2,k})\epsilon_{1}} }\leq\frac{(\epsilon_{2}+\Theta_{\max})\rho_{0}}{\sqrt{\beta_{2,k}\epsilon_{1 }}}\cdot\frac{k^{c/2}}{\sqrt{k}} \tag{59}\] \[\leq\frac{(\epsilon_{2}+\Theta_{\max})\rho_{0}}{\sqrt{\min\{\beta _{2,1},\beta_{2,2}\}\epsilon_{1}}}\leq\frac{2(\epsilon_{2}+\Theta_{\max})\rho_ {0}}{\sqrt{\epsilon_{1}}}, \tag{60}\]

where we use \(\beta_{2,1}=1/2,\beta_{2,2}=1-1/2^{c}\geq 1-1/\sqrt{2},c\in[1/2,1]\) from (6) in the last inequality. Hence, plugging (60) into (57) and then re-scaling the \(\delta\), we found that with probability at least \(1-\delta\), for all \(t\in[T]\),

\[-\sum_{k=1}^{t}\eta_{k}\left\langle\mathbf{\tilde{G}}_{k},\frac{\mathbf{\xi}_{k}}{\sqrt {\mathbf{A}_{k}}}\right\rangle\leq\frac{6\lambda G^{2}(\epsilon_{2}+\Theta_{\max} )\rho_{0}}{\sqrt{\epsilon_{1}}}\sum_{k=1}^{t}\eta_{k}\left\|\frac{\mathbf{\tilde{ G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\|_{F}^{2}+\frac{1}{\lambda}\log\left(\frac{T}{ \delta}\right).\]

Setting \(\lambda=\sqrt{\epsilon_{1}}/(24G^{2}(\epsilon_{2}+\Theta_{\max})\rho_{0})\), we derive the desired result. 

The following key lemma provides an upper bound for the error brought by the proxy step-size \(a_{ij}^{(k)}\), illustrating the error is controllable.

**Lemma B.7**.: _For any \(k\geq 1,i\in[n],j\in[m]\), it holds that_

\[\frac{\left|w_{ij}^{(k)}-a_{ij}^{(k)}\right|}{\sqrt{a_{ij}^{(k)}}}\leq\sqrt{1- \beta_{2,k}}\min\{4\sqrt{\mathcal{G}},G_{1}+G_{2}\}, \tag{61}\]

_where \(\mathcal{G}\) is as in (13) and \(G_{1},G_{2}\) are as in (44)._

Proof.: To simplify the notation, we let

\[X =\beta_{2,k}R_{\mathbf{V}_{k-1}}^{(i)}+(1-\beta_{2,k})R_{\mathbf{G}_{k,*_{ 1}}}^{(i)},\quad\Delta X=(1-\beta_{2,k})(\mathcal{G}_{1}-R_{\mathbf{G}_{k,*_{1}}}^{( i)}),\] \[Y =\beta_{2,k}C_{\mathbf{V}_{k-1}}^{(j)}+(1-\beta_{2,k})C_{\mathbf{G}_{k,*_{ 1}}}^{(j)},\quad\Delta Y=(1-\beta_{2,k})(\mathcal{G}_{2}-C_{\mathbf{G}_{k,*_{1}}}^{( j)}),\] \[Z =\beta_{2,k}S_{\mathbf{V}_{k-1}}+(1-\beta_{2,k})S_{\mathbf{G}_{k,*_{1}}^{ 2}},\quad\Delta Z=(1-\beta_{2,k})(\mathcal{G}-S_{\mathbf{G}_{k,*_{1}}^{2}}). \tag{62}\]Then we have

\[\left|w_{ij}^{(k)}-a_{ij}^{(k)}\right|=\left|\frac{XY}{Z}-\frac{(X+\Delta X)(Y+ \Delta Y)}{Z+\Delta Z}\right|=\left|\frac{XY\Delta Z-XZ\Delta Y-YZ\Delta X-Z( \Delta X\Delta Y)}{Z(Z+\Delta Z)}\right|.\]

Applying Lemma B.2, we could verify that \(X,Y,Z\geq 0\) and

\[0\leq\Delta X\leq(1-\beta_{2,k})\mathcal{G}_{1},\quad 0\leq\Delta Y\leq(1-\beta_{ 2,k})\mathcal{G}_{2},\quad 0\leq\Delta Z\leq(1-\beta_{2,k})\mathcal{G}. \tag{63}\]

Hence, we derive that

\[\frac{\left|w_{ij}^{(k)}-a_{ij}^{(k)}\right|}{\sqrt{a_{ij}^{(k)}}} =\frac{|XY\Delta Z-XZ\Delta Y-YZ\Delta X-Z(\Delta X\Delta Y)|}{Z \sqrt{(X+\Delta X)(Y+\Delta Y)(Z+\Delta Z)}}\] \[\leq\underbrace{\frac{|X\Delta Y+Y\Delta X+(\Delta X\Delta Y)|}{ \sqrt{(X+\Delta X)(Y+\Delta Y)(Z+\Delta Z)}}}_{(\mathbf{I})}+\underbrace{ \frac{XY\Delta Z}{Z\sqrt{(X+\Delta X)(Y+\Delta Y)(Z+\Delta Z)}}}_{(\mathbf{II} )}. \tag{64}\]

Since \(XY\geq 0\) from (62), Term **(I)** could be bounded as

\[(\mathbf{I})\leq\frac{|X\Delta Y+Y\Delta X+(\Delta X\Delta Y)|}{\sqrt{(X\Delta Y +Y\Delta X+(\Delta X\Delta Y))(Z+\Delta Z)}}\leq\sqrt{\frac{X\Delta Y+Y\Delta X +(\Delta X\Delta Y)}{Z+\Delta Z}}. \tag{65}\]

Recalling the definition, we have \(R_{\mathbf{V}_{k-1}}^{(i)}\leq S_{\mathbf{V}_{k-1}}\), \(C_{\mathbf{V}_{k-1}}^{(j)}\leq S_{\mathbf{V}_{k-1}}\) for any \(i\in[n],j\in[m]\). Further, applying Lemma B.2 and (63), we derive that

\[\frac{X\Delta Y}{Z+\Delta Z} \leq\left(\frac{R_{\mathbf{V}_{k-1}}^{(i)}}{S_{\mathbf{V}_{k-1}}}+\frac{R_ {\mathbf{G}_{k,\mathbf{v}_{1}}}^{(i)}}{\mathcal{G}}\right)\Delta Y\leq 2(1-\beta_{2,k}) \mathcal{G}_{2}.\] \[\frac{Y\Delta X}{Z+\Delta Z} \leq\left(\frac{C_{\mathbf{V}_{k-1}}^{(j)}}{S_{\mathbf{V}_{k-1}}}+\frac{C_ {\mathbf{G}_{k,\mathbf{v}_{1}}}^{(j)}}{\mathcal{G}}\right)\Delta X\leq 2(1-\beta_{2,k}) \mathcal{G}_{1},\] \[\frac{\Delta X\Delta Y}{Z+\Delta Z} \leq\frac{\Delta X(1-\beta_{2,k})\mathcal{G}}{(1-\beta_{2,k}) \mathcal{G}}\leq(1-\beta_{2,k})\mathcal{G}_{1}.\]

We then derive from (65), \(\mathcal{G}_{1}\leq\mathcal{G}\) and \(\mathcal{G}_{2}\leq\mathcal{G}\) that

\[(\mathbf{I})\leq\sqrt{5(1-\beta_{2,k})\mathcal{G}}. \tag{66}\]

To derive a free dimension bound, we could obtain from Lemma B.2, (63) and \(\mathcal{G}\geq mn\epsilon_{1}/2\) that \(Z+\Delta Z\geq mn\epsilon_{1}/2\). Hence,

\[\frac{X\Delta Y}{Z+\Delta Z}\leq\frac{2(1-\beta_{2,k})\mathcal{G}_{1} \mathcal{G}_{2}}{mn\epsilon_{1}},\quad\frac{Y\Delta X}{Z+\Delta Z}\leq\frac{ 2(1-\beta_{2,k})\mathcal{G}_{1}\mathcal{G}_{2}}{mn\epsilon_{1}},\quad\frac{ \Delta X\Delta Y}{Z+\Delta Z}\leq\frac{2(1-\beta_{2,k})\mathcal{G}_{1} \mathcal{G}_{2}}{mn\epsilon_{1}}.\]

We then derive that

\[(\mathbf{I})\leq\sqrt{\frac{6(1-\beta_{2,k})\mathcal{G}_{1}\mathcal{G}_{2}}{ mn\epsilon_{1}}}=\sqrt{\frac{6(1-\beta_{2,k})(G^{4}+G^{2}\epsilon_{1}(m+n)+mn \epsilon_{1}^{2})}{mn\epsilon_{1}}}\leq G_{1}\sqrt{1-\beta_{2,k}}, \tag{67}\]

where we used \(m+n\leq mn\), and \(G_{1}\) is defined in (44). Then, combining with (66) and (67), we have

\[(\mathbf{I})\leq\sqrt{1-\beta_{2,k}}\min\{\sqrt{5\mathcal{G}},G_{1}\}, \tag{68}\]

where we applied that \(m+n\leq mn\) when \(m,n\geq 2\). Then we move to bound **(II)**. Recalling the definitions in (62), we have \(X\leq Z,Y\leq Z\). Applying (63), we have

\[(\mathbf{II})\leq\frac{XY\Delta Z}{Z\sqrt{XY\Delta Z}}\leq\frac{\sqrt{XY\Delta Z }}{Z}\leq\sqrt{\Delta Z}\leq\sqrt{(1-\beta_{2,k})\mathcal{G}}.\]Similarly, we derive from Lemma B.2 that \(Z\geq mn\epsilon_{1}/2\), \(X\leq\mathcal{G}_{1},Y\leq\mathcal{G}_{2}\). Hence,

\[(\mathbf{II}) \leq\frac{\sqrt{XY\Delta Z}}{Z}\leq\frac{2\sqrt{(1-\beta_{2,k}) \mathcal{G}_{1}\mathcal{G}_{2}\mathcal{G}}}{mn\epsilon_{1}}\] \[\leq 2\sqrt{1-\beta_{2,k}}\left(\frac{G^{3}}{mn\epsilon_{1}}+ \frac{2G^{2}}{\sqrt{mn\epsilon_{1}}}+G+\frac{G}{\sqrt{mn}}+\sqrt{\epsilon_{1}} \right)\leq G_{2}\sqrt{1-\beta_{2,k}},\]

where \(G_{2}\) has been defined in (44). We thus derive that

\[(\mathbf{II})\leq\sqrt{1-\beta_{2,k}}\min\{\sqrt{\mathcal{G}},G_{2}\}. \tag{69}\]

Combining (68) with (69), we then derive the desired result. 

### Proof of Proposition b.1

Using the inequality in (14), we have

\[f(\mathbf{X}_{k+1}) \leq f(\mathbf{X}_{k})+\langle\mathbf{G}_{k},\mathbf{X}_{k+1}-\mathbf{X}_{k} \rangle+\frac{L}{2}\|\mathbf{X}_{k+1}-\mathbf{X}_{k}\|_{F}^{2}\] \[\leq f(\mathbf{X}_{k})-\eta_{k}\left\langle\mathbf{\tilde{G}}_{k},\frac{ \mathbf{G}_{k}}{\sqrt{\mathbf{W}_{k}}}\right\rangle+\frac{L\eta_{k}^{2}}{2}\left\| \frac{\mathbf{G}_{k}}{\sqrt{\mathbf{W}_{k}}}\right\|_{F}^{2}.\]

Introducing the proxy step-size matrix \(\mathbf{A}_{k}\) in (50) and then summing up both sides over \(k\in[t]\), we derive that

\[f(\mathbf{X}_{t+1}) \leq f(\mathbf{X}_{1})\underbrace{-\sum_{k=1}^{t}\eta_{k}\left\langle \mathbf{\tilde{G}}_{k},\frac{\mathbf{G}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\rangle}_{\mathbf{A}}\] \[+\underbrace{\sum_{k=1}^{t}\eta_{k}\left\langle\mathbf{\tilde{G}}_{k},\mathbf{G}_{k}\odot\left(\frac{1}{\sqrt{\mathbf{A}_{k}}}-\frac{1}{\sqrt{\mathbf{W}_{k}}} \right)\right\rangle}_{\mathbf{B}}+\underbrace{\sum_{k=1}^{t}\frac{L\eta_{k}^{2}}{ 2}\left\|\frac{\mathbf{G}_{k}}{\sqrt{\mathbf{W}_{k}}}\right\|_{F}^{2}}_{\mathbf{C}}. \tag{70}\]

Estimation for \(\mathbf{A}\)We first introduce \(\mathbf{\xi}_{k}\) into \(\mathbf{\Lambda}\),

\[\mathbf{A}=-\sum_{k=1}^{t}\eta_{k}\left\|\frac{\mathbf{\tilde{G}}_{k}}{\sqrt{\mathbf{ A}_{k}}}\right\|_{F}^{2}-\sum_{k=1}^{t}\eta_{k}\left\langle\mathbf{\tilde{G}}_{k}, \frac{\mathbf{\xi}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\rangle. \tag{71}\]

Then, using Lemma B.6, with probability at least \(1-\delta\), for all \(t\in[T]\),

\[\mathbf{A}=-\frac{3}{4}\sum_{k=1}^{t}\eta_{k}\left\|\frac{\mathbf{\tilde{G}}_{k}}{ \sqrt{\mathbf{A}_{k}}}\right\|_{F}^{2}+\frac{24G^{2}(\epsilon_{2}+\Theta_{\max}) \rho_{0}}{\sqrt{\epsilon_{1}}}\log\left(\frac{T}{\delta}\right). \tag{72}\]

Estimation for \(\mathbf{B}\)Term \(\mathbf{B}\) is essentially the error brought by the proxy step-size \(\mathbf{A}_{k}\). We will first calculate the gap of \(1/\sqrt{w_{ij}^{(k)}}\) and \(1/\sqrt{a_{ij}^{(k)}}\) as follows,

\[\left|\frac{1}{\sqrt{w_{ij}^{(k)}}}-\frac{1}{\sqrt{a_{ij}^{(k)}}}\right|=\frac {1}{\sqrt{w_{ij}^{(k)}}\sqrt{a_{ij}^{(k)}}}\left|\sqrt{w_{ij}^{(k)}}-\sqrt{a_{ ij}^{(k)}}\right|\leq\frac{1}{\sqrt{w_{ij}^{(k)}}\sqrt{a_{ij}^{(k)}}}\sqrt{ \left|w_{ij}^{(k)}-a_{ij}^{(k)}\right|}. \tag{73}\]

We then apply (73) and Young's inequality,

\[\mathbf{B} \leq\sum_{k=1}^{t}\sum_{i=1}^{n}\sum_{j=1}^{m}\eta_{k}\left|\bar{ g}_{ij}^{(k)}g_{ij}^{(k)}\right|\left|\frac{1}{\sqrt{w_{ij}^{(k)}}}-\frac{1}{ \sqrt{a_{ij}^{(k)}}}\right|\] \[\leq\sum_{k=1}^{t}\sum_{i=1}^{n}\sum_{j=1}^{m}\eta_{k}\frac{\left| \bar{g}_{ij}^{(k)}g_{ij}^{(k)}\right|}{\sqrt{w_{ij}^{(k)}}\sqrt{a_{ij}^{(k)}}} \sqrt{\left|w_{ij}^{(k)}-a_{ij}^{(k)}\right|}\] \[\leq\frac{1}{4}\sum_{k=1}^{t}\sum_{i=1}^{n}\sum_{j=1}^{m}\eta_{k }\cdot\frac{\left(\bar{g}_{ij}^{(k)}\right)^{2}}{\sqrt{a_{ij}^{(k)}}}+4\sum_{k =1}^{t}\sum_{i=1}^{n}\sum_{j=1}^{m}\eta_{k}\cdot\frac{\left|w_{ij}^{(k)}-a_{ ij}^{(k)}\right|}{\sqrt{a_{ij}^{(k)}}}\cdot\left(\frac{g_{ij}^{(k)}}{\sqrt{w_{ij}^{(k)}}} \right)^{2}. \tag{74}\]Thus, plugging (61) in Lemma B.7 into (74), we derive that

\[\mathbf{B} \leq\frac{1}{4}\sum_{k=1}^{t}\eta_{k}\left\|\frac{\bar{\mathbf{G}}_{k}} {\sqrt[4]{\mathbf{A}}_{k}}\right\|_{F}^{2}+4\sqrt{\mathcal{G}}\sum_{k=1}^{t}\eta_{k }\sqrt{1-\beta_{2,k}}\left\|\frac{\mathbf{G}_{k}}{\sqrt{\mathbf{W}}_{k}}\right\|_{F}^{2}\] \[\leq\frac{1}{4}\sum_{k=1}^{t}\eta_{k}\left\|\frac{\bar{\mathbf{G}}_{k} }{\sqrt[4]{\mathbf{A}}_{k}}\right\|_{F}^{2}+4\sqrt{\mathcal{G}}\sum_{k=1}^{t}\frac {(\epsilon_{2}+\Theta_{\max})\rho_{0}}{\sqrt{k}}\sqrt{1-\beta_{2,k}}\left\| \frac{\mathbf{G}_{k}}{\sqrt{\mathbf{W}}_{k}}\right\|_{F}^{2}\] \[\leq\frac{1}{4}\sum_{k=1}^{t}\eta_{k}\left\|\frac{\bar{\mathbf{G}}_{k} }{\sqrt[4]{\mathbf{A}}_{k}}\right\|_{F}^{2}+4\sqrt{\mathcal{G}}\sum_{k=1}^{t}( \epsilon_{2}+\Theta_{\max})\rho_{0}(1-\beta_{2,k})\left\|\frac{\mathbf{G}_{k}}{\sqrt {\mathbf{W}}_{k}}\right\|_{F}^{2}, \tag{75}\]

where we used (58) in the second inequality and \(1/\sqrt{k}\leq 1/k^{c/2},c\in[1/2,1]\). Furthermore, using Lemma B.4 and Lemma B.5, we derive that

\[\mathbf{B}\leq\frac{1}{4}\sum_{k=1}^{t}\eta_{k}\left\|\frac{\bar{ \mathbf{G}}_{k}}{\sqrt[4]{\mathbf{A}}_{k}}\right\|_{F}^{2}+\frac{8mn\mathcal{G}^{\frac {3}{2}}(\epsilon_{2}+\Theta_{\max})\rho_{0}}{\max\{m,n\}\epsilon_{1}}\left[\log \left(2+\frac{2G^{2}}{\epsilon_{1}}\right)+4\sum_{k=1}^{t}(1-\beta_{2,k}) \right]. \tag{76}\]

Estimating CUsing the similar deduction in (75) and (76), we derive that

\[\mathbf{C}\leq\frac{Lmn\mathcal{G}(\epsilon_{2}+\Theta_{\max})^{2}\rho_{0}^{2} }{\max\{m,n\}\epsilon_{1}}\left[\log\left(2+\frac{2G^{2}}{\epsilon_{1}}\right) +4\sum_{k=1}^{t}(1-\beta_{2,k})\right]. \tag{77}\]

Putting togetherWe first re-arrange the order in (70) and use \(f(\mathbf{X}_{t+1})\geq f^{*}\) in Assumption (A2) to derive that

\[0\leq f(\mathbf{X}_{1})-f^{*}+\mathbf{A}+\mathbf{B}+\mathbf{C}. \tag{78}\]

We then plug (72), (76), (77) into (78) and set \(t=T\), which leads to that with probability at least \(1-\delta\),

\[\frac{1}{2}\sum_{k=1}^{T}\eta_{k}\left\|\frac{\bar{\mathbf{G}}_{k}}{ \sqrt[4]{\mathbf{A}}_{k}}\right\|_{F}^{2}\leq C_{1}\log\left(\frac{T}{\delta}\right)+C_{2}\sum _{k=1}^{T}(1-\beta_{2,k})+C_{3}, \tag{79}\]

where \(C_{1},C_{2},C_{3}\) are as in Theorem B.1. Moreover, using Lemma B.3 and (58), we have

\[\frac{1}{2}\sum_{k=1}^{T}\eta_{k}\left\|\frac{\bar{\mathbf{G}}_{k}}{ \sqrt[4]{\mathbf{A}}_{k}}\right\|_{F}^{2}\geq\sum_{k=1}^{T}\frac{\eta_{k}\left\| \bar{\mathbf{G}}_{k}\right\|_{F}^{2}}{2\max_{i,j}\sqrt{a_{ij}^{(k)}}}\geq\frac{ \rho_{0}\max\{\epsilon_{2},\Theta_{\min}\}}{2\sqrt{2\mathcal{G}}}\sum_{k=1}^{T }\frac{\left\|\bar{\mathbf{G}}_{k}\right\|_{F}^{2}}{\sqrt{k}}. \tag{80}\]

Combining with (80) and (79), and using \(\sum_{k=1}^{T}1/\sqrt{k}\geq\sqrt{T}\), we derive that

\[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|^{2}\leq\frac{C_{0}}{\sqrt{T}} \left(C_{1}\log\left(\frac{T}{\delta}\right)+C_{2}\sum_{k=1}^{T}(1-\beta_{2,k })+C_{3}\right), \tag{81}\]

where \(C_{0}\) has already been defined in (42). We then derive the first desired result that

\[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|^{2}\leq\frac{C_{0}}{\sqrt{T}} \left(C_{1}\log\left(\frac{T}{\delta}\right)+C_{2}\sum_{k=1}^{T}\frac{1}{k^{c }}+C_{3}\right).\]

Free dimension boundWe follow the similar deduction in (75) and use Lemma B.7 to derive that

\[\mathbf{B}\leq\frac{1}{4}\sum_{k=1}^{t}\eta_{k}\left\|\frac{\bar{ \mathbf{G}}_{k}}{\sqrt[4]{\mathbf{A}}_{k}}\right\|_{F}^{2}+4(G_{1}+G_{2})(\epsilon_{2 }+\Theta_{\max})\rho_{0}\sum_{k=1}^{t}\frac{1}{k^{c/2+1/2}}\left\|\frac{\mathbf{G }_{k}}{\sqrt{\mathbf{W}}_{k}}\right\|_{F}^{2}. \tag{82}\]

Recalling the definition of \(w_{ij}^{(k)}\) in (49) and Lemma B.2, we derive that

\[w_{ij}^{(k)}=\frac{R_{\mathbf{V}_{k}}^{(i)}C_{\mathbf{V}_{k}}^{(j)}}{S_{ \mathbf{V}_{k}}}\geq\frac{mn\epsilon_{1}^{2}}{4\mathcal{G}},\quad\left\|\frac{ \mathbf{G}_{k}}{\sqrt{\mathbf{W}}_{k}}\right\|_{F}^{2}\leq\frac{\|\mathbf{G}_{k}\|_{F}^{2 }}{\min_{i,j}w_{ij}^{(k)}}\leq\frac{4G^{2}\mathcal{G}}{mn\epsilon_{1}^{2}}\leq G _{3}, \tag{83}\]where \(G_{3}\) is as in (44). We thus derive from (82) and (83) that

\[\mathbf{B}\leq\frac{1}{4}\sum_{k=1}^{t}\eta_{k}\left\|\frac{\bar{\mathbf{G}}_{k}}{ \sqrt[4]{\mathbf{A}_{k}}}\right\|_{F}^{2}+4G_{3}(G_{1}+G_{2})(\epsilon_{2}+\Theta_{ \max})\rho_{0}\sum_{k=1}^{t}\frac{1}{k^{c/2+1/2}}. \tag{84}\]

Using (58) and (83), we derive that

\[\mathbf{C}=\sum_{k=1}^{t}\frac{L\eta_{k}^{2}}{2}\left\|\frac{\mathbf{G}_{k}}{\sqrt{ \mathbf{W}_{k}}}\right\|_{F}^{2}\leq\frac{LG_{3}(\epsilon_{2}+\Theta_{\max})^{2} \rho_{0}^{2}}{2}\sum_{k=1}^{t}\frac{1}{k}. \tag{85}\]

Plugging the unchanged estimation for \(\mathbf{A}\) in (72), (84) and (85) into (70), we have that with probability at least \(1-\delta\), for all \(t\in[T]\),

\[\frac{1}{2}\sum_{k=1}^{t}\eta_{k}\left\|\frac{\bar{\mathbf{G}}_{k}}{\sqrt[4]{\mathbf{ A}_{k}}}\right\|_{F}^{2}\leq C_{1}\log\left(\frac{T}{\delta}\right)+C_{2}^{\prime }\sum_{k=1}^{t}\frac{1}{k^{c/2+1/2}}+C_{3}^{\prime}\sum_{k=1}^{t}\frac{1}{k}, \tag{86}\]

where \(C_{2}^{\prime},C_{3}^{\prime}\) are given as in (43) and \(C_{1}\) is as in (41). Further, using Lemma B.3 and the similar deduction for (80),

\[\frac{1}{2}\sum_{k=1}^{t}\eta_{k}\left\|\frac{\bar{\mathbf{G}}_{k}}{\sqrt[4]{\mathbf{A }_{k}}}\right\|_{F}^{2}\geq\sum_{k=1}^{t}\frac{\eta_{k}\left\|\bar{\mathbf{G}}_{k} \right\|_{F}^{2}}{2\max_{i,j}\sqrt{a_{ij}^{(k)}}}\geq\frac{1}{C_{0}^{\prime}} \sum_{k=1}^{t}\frac{\left\|\bar{\mathbf{G}}_{k}\right\|_{F}^{2}}{\sqrt{k}}, \tag{87}\]

where \(C_{0}^{\prime}\) is as in (43). Combining with (86) and (87), and setting \(t=T\), we derive the second desired result in Proposition B.1 that

\[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|^{2}\leq\frac{C_{0}^{\prime}}{\sqrt{T}} \left(C_{1}\log\left(\frac{T}{\delta}\right)+C_{2}^{\prime}\sum_{k=1}^{T} \frac{1}{k^{c/2+1/2}}+C_{3}^{\prime}\sum_{k=1}^{T}\frac{1}{k}\right).\]

### Proof of Theorem b.1

Now based on the result in Proposition B.1, we could further derive the final convergence rate. Noting that when \(c=1\), we could bound that

\[\sum_{k=1}^{T}\frac{1}{k}\leq 1+\int_{1}^{T}\frac{1}{x}dx\leq 1+\log T. \tag{88}\]

Then, we obtain that

\[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2} \leq\frac{C_{0}}{\sqrt{T}}\left(C_{1}\log\left(\frac{T}{\delta} \right)+C_{2}\log T+C_{2}+C_{3}\right),\] \[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2} \leq\frac{C_{0}^{\prime}}{\sqrt{T}}\left(C_{1}\log\left(\frac{T}{ \delta}\right)+(C_{2}^{\prime}+C_{3}^{\prime})\log T+C_{2}^{\prime}+C_{3}^{ \prime}\right).\]

When \(1/2\leq c<1\), we have

\[\sum_{k=1}^{T}\frac{1}{k^{c}}\leq 1+\int_{1}^{T}\frac{1}{x^{c}}dx \leq 1+\frac{T^{1-c}}{1-c},\] \[\sum_{k=1}^{T}\frac{1}{k^{c/2+1/2}}\leq 1+\int_{1}^{T}\frac{1}{x^{c/2+ 1/2}}dx\leq 1+\frac{2T^{(1-c)/2}}{1-c}. \tag{89}\]

Then, we obtain that

\[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2} \leq\frac{C_{0}}{\sqrt{T}}\left(C_{1}\log\left(\frac{T}{\delta} \right)+\frac{C_{2}}{1-c}\cdot T^{1-c}+C_{2}+C_{3}\right),\] \[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2} \leq\frac{C_{0}}{\sqrt{T}}\left(C_{1}\log\left(\frac{T}{\delta} \right)+\frac{2C_{2}^{\prime}}{1-c}\cdot T^{1-c}+C_{3}^{\prime}\log T+C_{2}^{ \prime}+C_{3}^{\prime}\right).\]Proof detail for stochastic Adafactor with update clipping

We first provide the detailed version of Theorem 7.1 as follows.

**Theorem C.1**.: _Let \(\{\mathbf{X}_{k}\}_{k\geq 1}\) be the sequence generated by Algorithm 1 with (7). If Assumptions (A1) -(A4) hold, and_

\[\rho_{k}=\rho_{0}/\sqrt{k},\quad d_{k}=k^{\frac{c}{2(\alpha-1)}}, \quad\forall k\geq 1,\] \[\beta_{2,1}=1/2,\quad\beta_{2,k}=1-1/k^{c},\forall k\geq 2.\]

_When \(c=1\), with probability at least \(1-\delta\),_

\[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2} \leq\frac{D_{0}}{\sqrt{T}}\left(C_{1}\log\left(\frac{T}{\delta} \right)+(C_{2}+D_{1}(\alpha))\log T+C_{2}+D_{1}(\alpha)+C_{3}\right), \tag{90}\] \[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2} \leq\frac{D_{0}}{\sqrt{T}}\left(C_{1}\log\left(\frac{T}{\delta} \right)+(C_{2}^{\prime}+C_{3}^{\prime}+D_{1}(\alpha))\log T+C_{2}^{\prime}+C_{ 3}^{\prime}+D_{1}(\alpha)\right). \tag{91}\]

_When \(1/2\leq c<1\), with probability at least \(1-\delta\),_

\[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2} \leq\frac{D_{0}}{\sqrt{T}}\left(C_{1}\log\left(\frac{T}{\delta} \right)+\frac{C_{2}+D_{1}(\alpha)}{1-c}\cdot T^{1-c}+C_{2}+D_{1}(\alpha)+C_{3} \right), \tag{92}\] \[\min_{k\in[T]}\|\bar{\mathbf{G}}_{k}\|_{F}^{2} \leq\frac{D_{0}}{\sqrt{T}}\left(C_{1}\log\left(\frac{T}{\delta} \right)+C_{3}^{\prime}\log T+\frac{2(C_{2}^{\prime}+D_{1}(\alpha))}{1-c}\cdot T ^{\frac{1-c}{2}}+C_{2}^{\prime}+C_{3}^{\prime}+D_{1}(\alpha)\right), \tag{93}\]

_where \(C_{1},C_{2},C_{3},C_{2}^{\prime},C_{3}^{\prime}\) are as in Theorem B.1 and_

\[D_{0}=\min\{C_{0},C_{0}^{\prime}\},\quad D_{1}(\alpha)=\frac{G^{1+\alpha}G_{4} ^{1-\alpha}\sqrt{\mathcal{G}}(\epsilon_{2}+\Theta_{\max})\rho_{0}}{\sqrt{mn} \epsilon_{1}},\quad G_{4}=\frac{mn\epsilon_{1}}{2\sqrt{\mathcal{G}}}. \tag{94}\]

Calculation of hyper-parameters' dependencyWe first calculate the dependency on \(m,n,\epsilon_{1},\alpha\) in the additional coefficient \(D_{1}(\alpha)\) as follows,

\[D_{1}(\alpha)\sim\mathcal{O}\left(\left(\frac{\sqrt{1+mn\epsilon_{1}}}{mn \epsilon_{1}}\right)^{\alpha-1}\sqrt{\frac{1}{mn\epsilon_{1}^{2}}+\frac{1}{ \epsilon_{1}}}\right), \tag{95}\]

which is free of the curse of dimension since \(mn\) exists in the denominator. Recalling the definitions of \(C_{0}^{\prime},C_{1},C_{2}^{\prime},C_{3}^{\prime}\) in (41) and (43), it's easy to verify that these coefficients are also free of the curse of dimension factor \(m,n\) since \(m,n\) exist in the denominator. Thereby, we also derive a free dimension bound selecting (91) and (93).

To calculate the dependency on \(\epsilon_{1}\), we could combine with (45) and (95) to derive that

\[C_{0}D_{1}(\alpha)\sim\mathcal{O}\left(\epsilon_{1}^{-\alpha} \right),\quad C_{0}C_{1}\sim\mathcal{O}\left(1/\epsilon_{1}^{-1/2}\right), \quad C_{0}C_{3}\sim\mathcal{O}\left(\epsilon_{1}^{-1}\log(1/\epsilon_{1}) \right).\]

Thereby, selecting the bounds in (90) and (92) and noting that \(\alpha>1\), we derive that the order on \(\epsilon_{1}\) is

\[\mathcal{O}\left(\frac{1}{\epsilon_{1}^{\alpha}}\log\left(\frac{ 1}{\epsilon_{1}}\right)\right). \tag{96}\]

Moreover, it's clear to reveal that there exist \(mn\) in denominator, which could improve the dependency on \(\epsilon_{1}\). If we suppose that \(mn\) is comparable to \(\epsilon_{1}\), then we derive that \(C_{0}D_{1}(\alpha)\sim\mathcal{O}(\epsilon_{1}^{-1/2})\) and the order on \(\epsilon_{1}\) is

\[\mathcal{O}\left(\frac{1}{\epsilon_{1}}\log\left(\frac{1}{\epsilon _{1}}\right)\right). \tag{97}\]

### Proof of Theorem C.1

We define

\[\tilde{\mathbf{G}}_{k}=\frac{\mathbf{G}_{k}}{\max\{1,\|\mathbf{U}_{k}\|_{F}/(d _{k}\sqrt{mn})\}},\quad\hat{\rho}_{k}=\max\{\epsilon_{2},\text{RMS}(\mathbf{X}_{k} )\}\rho_{k}. \tag{98}\]Since \(\text{RMS}(\mathbf{U}_{k})=\|\mathbf{U}_{k}\|_{F}/\sqrt{mn}\), \(\Theta_{\min}\leq\text{RMS}(\mathbf{X}_{k})\leq\Theta_{\max}\), we derive that

\[\mathbf{X}_{k+1}=\mathbf{X}_{k}-\hat{\rho}_{k}\frac{\tilde{\mathbf{G}}_{k}}{ \sqrt{\mathbf{W}_{k}}},\] \[\frac{\max\{\epsilon_{2},\Theta_{\min}\}\rho_{0}}{\sqrt{k}} \leq\hat{\rho}_{k}\leq\frac{(\epsilon_{2}+\Theta_{\max})\rho_{0}}{\sqrt{k}} \leq(\epsilon_{2}+\Theta_{\max})\rho_{0}\sqrt{1-\beta_{2,k}}, \tag{99}\]

where we applied that \(1/\sqrt{k}\leq 1/k^{c/2},c\in[1/2,1]\) and \(\beta_{2,k}=1-1/k^{c}\) in the last inequality. Using the inequalities in (14) and (99), we have

\[f(\mathbf{X}_{k+1}) \leq f(\mathbf{X}_{k})+\langle\tilde{\mathbf{G}}_{k},\mathbf{X}_{k+1}-\mathbf{X}_ {k}\rangle+\frac{L}{2}\|\mathbf{X}_{k+1}-\mathbf{X}_{k}\|_{F}^{2}\] \[\leq f(\mathbf{X}_{k})-\hat{\rho}_{k}\left\langle\tilde{\mathbf{G}}_{k}, \frac{\tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{W}_{k}}}\right\rangle+\frac{L\hat{\rho}_{k }^{2}}{2}\left\|\frac{\tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{W}_{k}}}\right\|_{F}^{2}.\]

Summing up both sides over \(k\in[t]\) and using \(f(\mathbf{X}_{t+1})\geq f^{*}\) from Assumption (A2), we derive that

\[0\leq f(\mathbf{X}_{1})-f^{*}+\underbrace{\sum_{k=1}^{t}-\hat{\rho}_{k}\left\langle \tilde{\mathbf{G}}_{k},\frac{\tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{W}_{k}}}\right\rangle}_{ \text{D}}+\underbrace{\sum_{k=1}^{t}\frac{L\hat{\rho}_{k}^{2}}{2}\left\|\frac{ \tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{W}_{k}}}\right\|_{F}^{2}}_{\text{E}}. \tag{100}\]

Introducing \(\mathbf{A}_{k}\) in (50), we further have the following decomposition,

\[\mathbf{D} =-\sum_{k=1}^{t}\hat{\rho}_{k}\left\langle\tilde{\mathbf{G}}_{k}, \frac{\tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\rangle+\underbrace{\sum_{k= 1}^{t}\hat{\rho}_{k}\left\langle\tilde{\mathbf{G}}_{k},\left(\frac{1}{\sqrt{\mathbf{A }_{k}}}-\frac{1}{\sqrt{\mathbf{W}_{k}}}\right)\odot\tilde{\mathbf{G}}_{k}\right\rangle} _{\text{D}.\mathbf{1}}\] \[=-\sum_{k=1}^{t}\hat{\rho}_{k}\left\|\frac{\tilde{\mathbf{G}}_{k}}{ \sqrt{\mathbf{A}_{k}}}\right\|_{F}^{2}+\mathbf{D}.\mathbf{1}\] \[\underbrace{-\sum_{k=1}^{t}\hat{\rho}_{k}\left\langle\tilde{\mathbf{ G}}_{k},\frac{\tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}-\mathbb{E}_{\mathbb{Z}_{k}} \left[\frac{\tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right]\right\rangle}_{\text{D }.\mathbf{2}}+\underbrace{\sum_{k=1}^{t}\hat{\rho}_{k}\left\langle\tilde{\mathbf{ G}}_{k},\frac{\tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}-\mathbb{E}_{\mathbb{Z}_{k}} \left[\frac{\tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right]\right\rangle}_{\text{D }.\mathbf{3}}. \tag{101}\]

Estimating EHence, using (98), (99), Lemma B.4 and Lemma B.5, we derive that

\[\mathbf{E} \leq\frac{L}{2}\sum_{k=1}^{t}\hat{\rho}_{k}^{2}\left\|\frac{\mathbf{G }_{k}}{\sqrt{\mathbf{W}_{k}}}\right\|_{F}^{2}\leq\frac{L(\epsilon_{2}+\Theta_{ \max})^{2}\rho_{0}^{2}}{2}\sum_{k=1}^{t}(1-\beta_{2,k})\left\|\frac{\mathbf{G}_{k} }{\sqrt{\mathbf{W}_{k}}}\right\|_{F}^{2}\] \[\leq\frac{Lmn\mathcal{G}(\epsilon_{2}+\Theta_{\max})^{2}\rho_{0} ^{2}}{\max\{m,n\}\epsilon_{1}}\left[\log\left(2+\frac{2G^{2}}{\epsilon_{1}} \right)+4\sum_{k=1}^{t}(1-\beta_{2,k})\right]. \tag{102}\]

To avoid the curse of dimension, we drive from (98) and (83) that

\[\left\|\frac{\tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{W}_{k}}}\right\|_{F}^{2}=\frac{1}{ \left(\max\{1,\|\mathbf{U}_{k}\|_{F}/(d_{k}\sqrt{mn})\}\right)^{2}}\left\|\frac{ \mathbf{G}_{k}}{\sqrt{\mathbf{W}_{k}}}\right\|_{F}^{2}\leq\left\|\frac{\mathbf{G}_{k}}{ \sqrt{\mathbf{W}_{k}}}\right\|_{F}^{2}\leq G_{3}. \tag{103}\]

Then, using (99) and (103), we derive that

\[\mathbf{E}\leq\frac{LG_{3}(\epsilon_{2}+\Theta_{\max})^{2}\rho_{0}^{2}}{2}\sum_ {k=1}^{t}\frac{1}{k}. \tag{104}\]Estimating D.1We could follow the similar deduction in (73) and (74) to derive that

\[\mathbf{D.1} \leq\sum_{k=1}^{t}\sum_{i=1}^{n}\sum_{j=1}^{m}\hat{\rho}_{k}|\bar{g }_{ij}^{(k)}\tilde{g}_{ij}^{(k)}\left|\left|\frac{1}{\sqrt{w_{ij}^{(k)}}}-\frac {1}{\sqrt{a_{ij}^{(k)}}}\right|\right.\] \[\leq\sum_{k=1}^{t}\sum_{i=1}^{n}\sum_{j=1}^{m}\hat{\rho}_{k}\frac {|\bar{g}_{ij}^{(k)}\tilde{g}_{ij}^{(k)}|}{\sqrt{w_{ij}^{(k)}}\sqrt{a_{ij}^{(k )}}}\sqrt{\left|w_{ij}^{(k)}-a_{ij}^{(k)}\right|}\] \[\leq\frac{1}{4}\sum_{k=1}^{t}\sum_{i=1}^{n}\sum_{j=1}^{m}\hat{ \rho}_{k}\cdot\frac{\left(\tilde{g}_{ij}^{(k)}\right)^{2}}{\sqrt{a_{ij}^{(k)} }}+4\sum_{k=1}^{t}\sum_{i=1}^{n}\sum_{j=1}^{m}\hat{\rho}_{k}\cdot\frac{\left|w _{ij}^{(k)}-a_{ij}^{(k)}\right|}{\sqrt{a_{ij}^{(k)}}}\cdot\left(\frac{\tilde{ g}_{ij}^{(k)}}{\sqrt{w_{ij}^{(k)}}}\right)^{2}. \tag{105}\]

Using Lemma B.7 and (105), we further derive that

\[\mathbf{D.1} \leq\frac{1}{4}\sum_{k=1}^{t}\hat{\rho}_{k}\left\|\frac{\tilde{ \mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\|_{F}^{2}+4\sqrt{\mathcal{G}}\sum_{k=1}^ {t}\hat{\rho}_{k}\sqrt{1-\beta_{2,k}}\left\|\frac{\tilde{\mathbf{G}}_{k}}{\sqrt{ \mathbf{W}_{k}}}\right\|_{F}^{2}\] \[\leq\frac{1}{4}\sum_{k=1}^{t}\hat{\rho}_{k}\left\|\frac{\tilde{ \mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\|_{F}^{2}+4\sqrt{\mathcal{G}}\sum_{k=1}^ {t}\hat{\rho}_{k}\sqrt{1-\beta_{2,k}}\left\|\frac{\mathbf{G}_{k}}{\sqrt{\mathbf{W}_{k} }}\right\|_{F}^{2}.\]

Using (99), Lemma B.4 and Lemma B.5, we further have

\[\mathbf{D.1} \leq\frac{1}{4}\sum_{k=1}^{t}\hat{\rho}_{k}\left\|\frac{\tilde{ \mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\|_{F}^{2}+4\sqrt{\mathcal{G}}(\epsilon_{ 2}+\Theta_{\max})\rho_{0}\sum_{k=1}^{t}(1-\beta_{2,k})\left\|\frac{\mathbf{G}_{k}} {\sqrt{\mathbf{W}_{k}}}\right\|_{F}^{2}\] \[\leq\frac{1}{4}\sum_{k=1}^{t}\hat{\rho}_{k}\left\|\frac{\tilde{ \mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\|_{F}^{2}+\frac{8mn\mathcal{G}^{\frac{3}{ 2}}(\epsilon_{2}+\Theta_{\max})\rho_{0}}{\max\{m,n\}\epsilon_{1}}\left[\log \left(2+\frac{2G^{2}}{\epsilon_{1}}\right)+4\sum_{k=1}^{t}(1-\beta_{2,k}) \right]. \tag{106}\]

To avoid the curse of dimension, we apply Lemma B.7, (99) and (83) to derive that

\[\mathbf{D.1} \leq\frac{1}{4}\sum_{k=1}^{t}\hat{\rho}_{k}\left\|\frac{\tilde{ \mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\|_{F}^{2}+4(G_{1}+G_{2})\sum_{k=1}^{t} \hat{\rho}_{k}\sqrt{1-\beta_{2,k}}\left\|\frac{\mathbf{G}_{k}}{\sqrt{\mathbf{W}_{k}}} \right\|_{F}^{2}\] \[\leq\frac{1}{4}\sum_{k=1}^{t}\hat{\rho}_{k}\left\|\frac{\tilde{ \mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\|_{F}^{2}+4(G_{1}+G_{2})(\epsilon_{2}+ \Theta_{\max})\rho_{0}\sum_{k=1}^{t}\frac{1}{k^{c/2+1/2}}\left\|\frac{\mathbf{G}_{ k}}{\sqrt{\mathbf{W}_{k}}}\right\|_{F}^{2}\] \[\leq\frac{1}{4}\sum_{k=1}^{t}\hat{\rho}_{k}\left\|\frac{\tilde{ \mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\|_{F}^{2}+4G_{3}(G_{1}+G_{2})(\epsilon_ {2}+\Theta_{\max})\rho_{0}\sum_{k=1}^{t}\frac{1}{k^{c/2+1/2}}. \tag{107}\]

Estimating D.2Since \(\mathbf{A}_{k}\) is independent from \(\mathbf{Z}_{k}\), it further leads to

\[\mathbf{D.2}=-\sum_{k=1}^{t}\hat{\rho}_{k}\left\langle\frac{\tilde{\mathbf{G}}_{k}}{ \sqrt{\mathbf{A}_{k}}},\tilde{\mathbf{G}}_{k}-\mathbb{E}_{\mathbf{Z}_{k}}\left[\tilde{\bm {G}}_{k}\right]\right\rangle.\]

Then, the deduction for estimating \(\mathbf{D.2}\) follows the similar idea as in Lemma B.6, relying on a martingale difference sequence.

Let us set \(\varphi_{k}=-\hat{\rho}_{k}\left\langle\frac{\tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{ k}}},\tilde{\mathbf{G}}_{k}-\mathbb{E}_{\mathbf{Z}_{k}}\left[\tilde{\mathbf{G}}_{k} \right]\right\rangle\) and the filtration \(\mathcal{F}_{k}=\sigma\left(\mathbf{Z}_{1},\cdots,\mathbf{Z}_{k}\right)\). Noting that \(\hat{\rho}_{k}\), \(\tilde{\mathbf{G}}_{k}\) and \(\mathbf{A}_{k}\) are dependent by \(\mathcal{F}_{k-1}\). Since \(\mathbf{\xi}_{k}\) is dependent by \(\mathcal{F}_{k}\), we could prove that \(\{\varphi_{k}\}_{k\geq 1}\) is a martingale difference sequence by showing that

\[\mathbb{E}\left[\varphi_{k}\mid\mathcal{F}_{k-1}\right]=-\hat{\rho}_{k}\left\langle \frac{\tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}},\mathbb{E}_{\mathbf{Z}_{k}}\left[ \tilde{\mathbf{G}}_{k}-\mathbb{E}_{\mathbf{Z}_{k}}[\tilde{\mathbf{G}}_{k}]\right]\right\rangle =0.\]

In addition, using Assumptions (A3), (A4) and Jensen's inequality, we have

\[\|\tilde{\mathbf{G}}_{k}\|_{F}=\frac{\|\mathbf{G}_{k}\|_{F}}{\max\{1,\|\mathbf{U}_{k}\|/(d_{k }\sqrt{mn})\}}\leq\|\mathbf{G}_{k}\|_{F}\leq G,\quad\|\mathbb{E}_{\mathbf{Z}_{k}}[ \tilde{\mathbf{G}}_{k}]\|_{F}\leq\mathbb{E}_{\mathbf{Z}_{k}}\|\tilde{\mathbf{G}}_{k}\|_{F} \leq G.\]Therefore, we derive that

\[\|\tilde{\mathbf{G}}_{k}-\mathbb{E}_{\mathbf{Z}_{k}}[\tilde{\mathbf{G}}_{k}]\|_{F}\leq\|\tilde{ \mathbf{G}}_{k}\|_{F}+\|\mathbb{E}_{\mathbf{Z}_{k}}[\tilde{\mathbf{G}}_{k}]\|_{F}\leq 2G. \tag{108}\]

Let \(\omega_{k}^{\prime}=2G\hat{\rho}_{k}\left\|\frac{\tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{ A}_{k}}}\right\|_{F}\). We thus derive from the Cauchy-Schwarz inequality and (108) that

\[\mathbb{E}\left[\exp\left(\frac{\varphi_{k}^{2}}{(\omega_{k}^{\prime})^{2}} \right)\mid\mathcal{F}_{k-1}\right]\leq\mathbb{E}\left[\exp\left(\frac{\left\| \frac{\tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\|_{F}^{2}\|\tilde{\mathbf{G}}_{ k}-\mathbb{E}_{\mathbf{Z}_{k}}[\tilde{\mathbf{G}}_{k}]\|_{F}^{2}}{4G^{2}\left\|\frac{ \tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\|_{F}^{2}}\right)\mid\mathcal{F}_ {k-1}\right]\leq\exp(1).\]

Then, using Lemma B.1, it leads to that for any \(\lambda>0\), with probability at least \(1-\delta\),

\[\mathbf{D}.\mathbf{2} =\sum_{k=1}^{t}\varphi_{k}\leq 3\lambda G^{2}\sum_{k=1}^{t}\hat{ \rho}_{k}^{2}\left\|\frac{\tilde{\mathbf{G}}_{k}}{\sqrt{\mathbf{A}_{k}}}\right\|_{F}^{ 2}+\frac{1}{\lambda}\log\left(\frac{1}{\delta}\right)\] \[=3\lambda G^{2}\sum_{k=1}^{t}\sum_{i=1}^{n}\sum_{j=1}^{m}\frac{ \hat{\rho}_{k}}{\sqrt{a_{ij}^{(k)}}}\cdot\hat{\rho}_{k}\frac{\left(\bar{g}_{ij} ^{(k)}\right)^{2}}{\sqrt{a_{ij}^{(k)}}}+\frac{1}{\lambda}\log\left(\frac{1}{ \delta}\right).\]

Since \(\{\beta_{2,k}\}_{k\geq 2}\) is non-decreasing, we could apply Lemma B.3 to derive that

\[\frac{1}{\sqrt{a_{ij}^{(k)}}}\leq\sqrt{\frac{1}{\beta_{2,k}(1-\beta_{2,k}) \epsilon_{1}}}\leq\sqrt{\frac{1}{\min\{\beta_{2,1},\beta_{2,2}\}(1-\beta_{2,k} )\epsilon_{1}}}\leq\frac{2}{\sqrt{(1-\beta_{2,k})\epsilon_{1}}}.\]

Then, we apply (99), and re-scale \(\delta\) to obtain that for any \(\lambda>0\), with probability at least \(1-\delta\), for all \(t\in[T]\),

\[\mathbf{D}.\mathbf{2}\leq\frac{6\lambda G^{2}\rho_{0}(\epsilon_{2}+\Theta_{ \max})}{\sqrt{\epsilon_{1}}}\sum_{k=1}^{t}\hat{\rho}_{k}\left\|\frac{\tilde{ \mathbf{G}}_{k}}{\sqrt[4]{\mathbf{A}_{k}}}\right\|_{F}^{2}+\frac{1}{\lambda}\log\left( \frac{T}{\delta}\right).\]

Setting \(\lambda=\sqrt{\epsilon_{1}}/(24G^{2}\rho_{0}(\epsilon_{2}+\Theta_{\max}))\), we derive that

\[\mathbf{D}.\mathbf{2}\leq\frac{1}{4}\sum_{k=1}^{t}\hat{\rho}_{k}\left\|\frac{ \tilde{\mathbf{G}}_{k}}{\sqrt[4]{\mathbf{A}_{k}}}\right\|_{F}^{2}+\frac{24G^{2}\rho_{0 }(\epsilon_{2}+\Theta_{\max})}{\sqrt{\epsilon_{1}}}\log\left(\frac{T}{\delta }\right). \tag{109}\]

Estimating D.3First, since \(\mathbf{A}_{k}\) is independent from \(\mathbf{Z}_{k}\) and \(\mathbb{E}_{\mathbf{Z}_{k}}[\mathbf{G}_{k}]=\tilde{\mathbf{G}}_{k}\), we have

\[\mathbf{D}.\mathbf{3} =\sum_{k=1}^{t}\hat{\rho}_{k}\left\langle\mathbf{G}_{k},\frac{\mathbb{ E}_{\mathbf{Z}_{k}}[\mathbf{G}_{k}]}{\sqrt{\mathbf{A}_{k}}}-\frac{\mathbb{E}_{\mathbf{Z}_{k}}[ \tilde{\mathbf{G}}_{k}]}{\sqrt{\mathbf{A}_{k}}}\right\rangle\] \[\leq\sum_{k=1}^{t}\hat{\rho}_{k}\left\|\frac{\tilde{\mathbf{G}}_{k}} {\sqrt{\mathbf{A}_{k}}}\right\|_{F}\cdot\left\|\mathbb{E}_{\mathbf{Z}_{k}}\underbrace{ \left[\mathbf{G}_{k}-\frac{\mathbf{G}_{k}}{\max\{1,\|\mathbf{U}_{k}\|_{F}/(d_{k}\sqrt{mn}) \}}\right]}_{\Omega_{k}}\right\|_{F}. \tag{110}\]

We define the random variable \(S_{k}^{(1)}\), \(S_{k}^{(2)}\) and \(\tilde{S}_{k}^{(1)}\) using the indicator function \(\chi\) and \(G_{4}\) in (94) as follows,

\[S_{k}^{(1)}=\chi_{\{\|\mathbf{U}_{k}\|_{F}>d_{k}\sqrt{mn}\}},\quad S_{k}^{(2)}=\chi _{\{\|\mathbf{U}_{k}\|_{F}\leq d_{k}\sqrt{mn}\}},\quad\tilde{S}_{k}^{(1)}=\chi_{\{\| \mathbf{G}_{k}\|_{F}\geq d_{k}G_{4}\}}.\]

From (83), we derive that

\[\|\mathbf{U}_{k}\|_{F}\leq\|\mathbf{G}_{k}\|_{F}\cdot\frac{2\sqrt{\mathcal{G}}}{\sqrt{ mn}\epsilon_{1}}.\]

Hence, \(S_{k}^{(1)}\leq\tilde{S}_{k}^{(1)},\forall k\geq 1\). Note that when \(S_{k}^{(2)}=1\), it's equivalent to \(\Omega_{k}=0\). Then, we derive that

\[\left\|\mathbb{E}_{\mathbf{Z}_{k}}[\Omega_{k}]\right\|_{F}=\left\| \mathbb{E}_{\mathbf{Z}_{k}}[\Omega_{k}S_{k}^{(1)}]+\mathbb{E}_{\mathbf{Z}_{k}}[\Omega_{k }S_{k}^{(2)}]\right\|_{F}=\left\|\mathbb{E}_{\mathbf{Z}_{k}}[\Omega_{k}S_{k}^{(1)}] \right\|_{F}\] \[\leq\mathbb{E}_{\mathbf{Z}_{k}}\left\|\Omega_{k}S_{k}^{(1)}\right\|_{F }\leq\mathbb{E}_{\mathbf{Z}_{k}}\left\|\Omega_{k}\tilde{S}_{k}^{(1)}\right\|_{F} \leq\mathbb{E}_{\mathbf{Z}_{k}}\left\|\mathbf{G}_{k}\tilde{S}_{k}^{(1)}\right\|_{F}\leq G ^{\alpha}\left(d_{k}G_{4}\right)^{1-\alpha}, \tag{111}\]

[MISSING_PAGE_FAIL:31]

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

### The checklist answers are an integral part of your paper submission.

They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

### IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: [NA] Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: [NA]Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: [NA] Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: [NA] Guidelines:

* The answer NA means that the paper does not include experiments.

* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: Our code is based on Pytorch package which is standard. In addition, we have clarified the detailed experimental setup in our paper and the experiments are easy to reproduce. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).

* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: [NA] Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: [NA] Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: [NA] Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ** The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: [NA] Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper poses no such risks.

* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **License for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: [NA] Guidelines: [1015] * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA]Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**

Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: [NA]

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.