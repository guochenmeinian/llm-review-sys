# Boosting Alignment for Post-Unlearning Text-to-Image Generative Models

Myeongseob Ko

Virginia Tech

myeongseob@vt.edu

&Henry Li

Yale University

henry.li@yale.edu

&Zhun Wang

University of California, Berkeley

zhun.wang@berkeley.edu

&Jonathan Patsenker

Yale University

jonathan.patsenker@yale.edu

&Jiachen T. Wang

Princeton University

tianhaowang@princeton.edu

&Qinbin Li

University of California, Berkeley

liqinbin1998@gmail.com

&Ming Jin

Virginia Tech

jimming@vt.edu

&Dawn Song

University of California, Berkeley

dawnsong@berkeley.edu

&Ruoxi Jia

Virginia Tech

ruoxijia@vt.edu

Equal contributions

###### Abstract

Large-scale generative models have shown impressive image-generation capabilities, propelled by massive data. However, this often inadvertently leads to the generation of harmful or inappropriate content and raises copyright concerns. Driven by these concerns, machine unlearning has become crucial to effectively purge undesirable knowledge from models. While existing literature has studied various unlearning techniques, these often suffer from either poor unlearning quality or degradation in text-image alignment after unlearning, due to the competitive nature of these objectives. To address these challenges, we propose a framework that seeks an optimal model update at each unlearning iteration, ensuring monotonic improvement on both objectives. We further derive the characterization of such an update. In addition, we design procedures to strategically diversify the unlearning and remaining datasets to boost performance improvement. Our evaluation demonstrates that our method effectively removes target classes from recent diffusion-based generative models and concepts from stable diffusion models while maintaining close alignment with the models' original trained states, thus outperforming state-of-the-art baselines. Our code will be made available at [https://github.com/reds-lab/Restricted_gradient_diversity_unlearning.git](https://github.com/reds-lab/Restricted_gradient_diversity_unlearning.git).

## 1 Introduction

Large-scale text-to-image generative models have recently gained considerable attention for their impressive image-generation capabilities. Despite being at the height of their popularity, these models, trained on vast amounts of public data, inevitably face concerns related to harmful content generation [11] and copyright infringement [22]. Althoughexact machine unlearning--retraining the model by excluding target data--is a direct solution, its computational challenge has driven continued research on approximate machine unlearning.

To address this challenge, recent studies (Fan et al., 2023; Gandikota et al., 2023; Heng and Soh, 2024), have introduced approximate unlearning techniques aimed at boosting efficiency while preserving effectiveness. These approaches have successfully demonstrated the ability to remove target concepts while maintaining the model's general image generation capabilities, with generation quality assessed using the Frechet Inception Distance. However, these studies generally overlook the impact of unlearning on image-text alignment, which pertains to the semantic accuracy between generated images and their text descriptions (Lee et al., 2024). While pretrained generative models generally demonstrate high alignment scores, our study reveals a critical gap: state-of-the-art unlearning techniques fall short in achieving comparable text-image alignment scores after unlearning, as illustrated in Figure 1. This could lead to potentially problematic behaviors in real-world deployments, necessitating further investigation.

We attribute the failure of existing techniques to maintain text-image alignment to two primary factors. Firstly, the unlearning objective often conflicts with the goal of maintaining low loss on the retained data, illustrating the competitive nature of these two objectives. Traditionally, approaches to optimizing these objectives have simply aggregated the gradients from both; however, this method of updating the model typically advances one objective at the expense of the other. Hence, while these approaches may successfully remove target concepts, they often compromise text-image alignment for retained concepts in the process. Secondly, current methods employ a simplistic approach to constructing a dataset for loss minimization on retained concepts. For example, in Fan et al. (2023), this dataset is composed of images generated from a single prompt associated with the concept to be removed. This lack of diversity in the dataset might lead to overfitting, which in turn hampers the text-image alignment.

To address these issues, we propose a principled framework designed to optimally balance the objectives of unlearning the target data and maintaining performance on the remaining data at each update iteration. Specifically, we introduce the concept of the _restricted gradient_, which allows for the optimization of both objectives while ensuring monotonic improvement in each objective. Furthermore, we have developed a deliberate procedure to enhance data diversity, preventing the model from overfitting to the limited samples in the remaining dataset. To the best of our knowledge, the strategic design of the forgetting target and remaining sets has not been extensively explored in the existing machine unlearning literature. In our evaluation, we demonstrate the improvement in both forgetting quality and alignment on the remaining data, compared to baselines. For example, our

Figure 1: Generated images using SalUn(Fan et al., 2023), ESD(Gandikota et al., 2023), and Ours after unlearning given the condition. Each row indicates different unlearning tasks: nudity removal, and _Van Gogh_ style removal. Generated images from our approach and SD(Rombach et al., 2022) are well-aligned with the prompt, whereas SalUn and ESD fail to generate semantically correct images given the condition. On average, across 100 different prompts, SalUn shows the lowest clip alignment scores (0.305 for nudity removal and 0.280 for _Van Gogh_ style removal), followed by ESD(0.329 and 0.330, respectively). Our approach achieves scores of 0.350 and 0.352 for these tasks, closely matching the original SD scores of 0.352 and 0.348.

evaluation in nudity removal demonstrates that our method effectively reduces the number of detected body parts to zero, compared to 598 with the baseline stable diffusion (SD) [Rombach et al., 2022], 48 with erased stable diffusion (ESD-u), and 3 with saliency map-based unlearning (SalUn) [Fan et al., 2023]. Particularly, while achieving this effective erasing performance, our method reduces the alignment gap to SD by 11x compared to ESD-u and by 20x compared to SalUn on the retained test set.

## 2 Related Work

### Machine Unlearning

Machine unlearning has primarily been propelled by the "Right to be Forgotten" (RTBF), which upholds the right of users to request the deletion of their data. Given that large-scale models are often trained on web-scraped public data, this becomes a critical consideration for model developers to avoid the need for retraining models with each individual request. In addition to RTBF, recent concerns related to copyrights and harmful content generation further underscore the necessity and importance of in-depth research in machine unlearning. The principal challenge in this field lies in effectively erasing the target concept from pre-trained models while maintaining performance on other data. Recent studies have explored various approaches to unlearning, including the exact unlearning method [Bourtoule et al., 2021] and approximate methods such as using negative gradients, fine-tuning without the forget data, editing the entire parameter space of the model [Golatkar et al., 2020]. To encourage the targeted impact in the parameter space, [Golatkar et al., 2020, Foster et al., 2024] proposed leveraging the Fisher information matrix, and [Fan et al., 2023] leveraged a gradient-based weight saliency map to identify crucial neurons, thus minimizing the impact on remaining neurons. Furthermore, data-influence-based debiasing and unlearning have also been proposed [Chen et al., 2024, Bae et al., 2023]. Another line of work leverages mathematical tools in differential privacy [Guo et al., 2019, Chien et al., 2024] to ensure that the model's behavior remains indistinguishable between the retrained and unlearned models.

### Machine Unlearning in Diffusion Models

Recent advancements in text-conditioned generative models [Ho and Salimans, 2022, Rombach et al., 2022], trained on extensive web-scraped datasets like LAION-SB [Schuhmann et al., 2022], have raised significant concerns about the generation of harmful content and copyright violations. A series of studies have addressed the challenge of machine unlearning in diffusion models [Heng and Soh, 2024, Gandikota et al., 2023, Zhang et al., 2023a, Fan et al., 2023]. One approach [Heng and Soh, 2024] interprets machine unlearning as a continual learning problem, showing effective removal results in classification tasks by employing Bayesian approaches to continual learning [Kirkpatrick et al., 2017], which enhance unlearning quality while maintaining model performance using generative reply [Shin et al., 2017]. However, this approach falls short in removing concepts such as nudity compared to other methods [Gandikota et al., 2023]. Another proposed method [Gandikota et al., 2023] guides the pre-trained model toward a prior distribution for the targeted concept but struggles to preserve performance. The most recent work [Fan et al., 2023] proposes selectively damaging neurons based on a saliency map and random labeling techniques, although this method tends to overlook the quality of the remaining set, focusing on improving the forgetting quality, which does not fully address the primary challenges in the machine unlearning community. Although [Bae et al., 2023] presents a similar multi-task learning framework for variational autoencoders, their work does not show the optimality of their solution, and their experiments mainly focus on small-scale models, due to the computational expense associated with influence functions.

## 3 Our Approach

We study the efficacy of our approach in unlearning by removing target classes from class-conditional diffusion models or eliminating specific concepts from text-to-image models while maintaining their general generation capabilities. We will call the set of data points to be removed as the _forgetting dataset_. To set up the notations, let \(D\) denote the training set and \(D_{f}\subset D\) be the forgetting dataset. We will use \(D_{r}=D\setminus D_{f}\) to denote the _remaining dataset_. Our approach only assumes access to some representative points for \(D_{f}\) and \(D_{r}\). As discussed later, depending on specific applications, these data points can be either directly sampled from \(D_{f}\) and \(D_{r}\) or generated based on the high-level concept of \(D_{f}\) to be removed. With a slight abuse of notation, we will use \(D_{r}\) and \(D_{f}\) to also denote the actual representative samples used to operationalize our proposed approach. Furthermore, we denote the model parameter by \(\theta\). Let \(l\) be a proper learning loss function. The loss of remaining data and that of forgetting data are represented by \(\mathcal{L}_{r}(\theta):=\sum_{z\in D_{r}}l(\theta,z)\) and \(\mathcal{L}_{f}(\theta):=-\lambda\sum_{z\in D_{f}}l(\theta,z)\), respectively, where \(\lambda\) is a weight adjusting the importance of forgetting loss relative to the remaining data loss. We term \(\mathcal{L}_{r}\) and \(\mathcal{L}_{f}\)_remaining loss_ and _forgetting loss_, respectively. We note that in the context of diffusion models, loss function \(l\) is defined as \(l=\mathbb{E}_{t,x_{0},\epsilon\sim\mathcal{N}(0,1)}\left[\|\epsilon-e_{\theta }(x_{t},t)\|^{2}\right]\), where \(x_{t}\) is a noisy version of \(x_{0}\) generated by adding Gaussian noise to the clean image \(x_{0}\sim p_{\text{data}}(x)\) at time step \(t\) with a noise scheduler, and \(e_{\theta}(x_{t},t)\) is the model's estimate of the added noise \(\epsilon\) at time \(t\)[Xu et al., 2023, Ho et al., 2020]. For text-to-image generative models, the loss function \(l\) is specified as \(l=\mathbb{E}_{t,q_{0},c,\epsilon}\left[\|\epsilon-\epsilon_{\theta}(q_{t},t, \eta)\|^{2}\right]\), where \(q_{0}\) is an encoded latent \(q_{0}=\mathcal{E}(x_{0})\) with encoder \(\mathcal{E}\), and \(q_{t}\) is a noisy latent at time step \(t\). The noise prediction \(\epsilon_{\theta}(q_{t},t,\eta)\) is conditioned on the timestep \(t\) and a text \(\eta\).

Optimizing the Update.Similar to existing work Fan et al. [2023], our objective is to find an unlearned model with parameters \(\theta_{u}\), starting from a pre-trained model with weights \(\theta_{0}\), such that the model forgets the target concepts in \(D_{f}\) while maintaining its utility on the remaining dataset \(D_{r}\). Formally, we aim to maximize the forget error on \(D_{f}\), represented by \(\mathcal{L}_{f}(\theta)\), while minimizing the retain error on \(D_{r}\), represented by \(\mathcal{L}r(\theta)\). This can be formulated as \(\min_{\theta}\mathcal{L}_{r}(\theta)+\mathcal{L}_{f}(\theta)\), where our approach applies iterative updates to achieve both goals simultaneously. A simple approach to optimize this objective, often adopted by existing work, is to calculate the gradient \(\nabla\mathcal{L}_{r}(\theta)+\nabla\mathcal{L}_{f}(\theta)\) and use it to update the model parameters at each iteration. However, empirically, we observe that the two gradients usually conflict with each other, i.e., the decrease of one objective is at the cost of increasing the other; therefore, in practice, this approach yields a significant tradeoff between forgetting strength and model utility on the remaining data. In this work, we aim to present a principled approach to designing the update direction at each iteration that more effectively handles the tradeoff between forgetting strength and model utility on the remaining data. Our key idea is to identify a direction that achieves a monotonic decrease of both objectives.

To describe our algorithm, we briefly review the directional derivative.

**Definition 1** (Directional Derivative).: _The directional derivative [Spivak, 2006] of a function \(\mathbf{f}\) at \(\mathbf{x}\) in the direction of \(\mathbf{v}\) is written as_

\[D_{\mathbf{v}}\mathbf{f}(\mathbf{x})=\lim_{h\to 0}\frac{\mathbf{f}(\mathbf{x}+h \mathbf{v})}{h}. \tag{1}\]

This special form of the derivative has the useful property that its maximizer can be related to the gradient \(\nabla_{\mathbf{x}}\mathbf{f}(\mathbf{x})\), which we formally state below.

**Theorem 2** (Directional derivative maximizer is the gradient).: _Let \(\mathbf{f}\) be a function on \(\mathbf{x}\). Then the maximum value of the directional derivative of \(\mathbf{f}\) at \(\mathbf{x}\) is \(|\nabla\mathbf{f}(\mathbf{x})|\) the \(\ell^{2}\) norm of its gradient. Moreover, the direction \(\mathbf{v}\) is the gradient itself, i.e.,_

\[\operatorname*{arg\,max}_{\mathbf{v}}\,D_{\mathbf{v}}\mathbf{f}=\nabla \mathbf{f}(\mathbf{x}). \tag{2}\]

In unlearning, we are specifically interested in the gradient of two losses, the forgetting loss \(\mathcal{L}_{f}\) and the remaining loss \(\mathcal{L}_{r}\). Moreover, we seek gradient directions that simultaneously improve on both. This motivates the _restricted gradient_, which we define below.

**Definition 3** (Restricted gradient, local form for minimization).: _The negative restricted gradient of two losses \(\mathcal{L}_{\alpha},\,\mathcal{L}_{\beta}\) is any direction \(\mathbf{v}\) at \(\boldsymbol{\theta}\) satisfying_

\[\min_{\mathbf{v}}\,D_{\mathbf{v}}\big{(}\mathcal{L}_{\alpha}+\mathcal{L}_{ \beta}\big{)}(\boldsymbol{\theta})\quad\text{s.t.}\quad D_{\mathbf{v}}\mathcal{ L}_{\alpha}(\boldsymbol{\theta})\ \ \leq\ 0,\quad D_{\mathbf{v}}\mathcal{L}_{\beta}( \boldsymbol{\theta})\ \ \leq\ 0.\]

Intuitively, with the restricted gradient we seek to define the ideal direction for unlearning. We would like to optimize the joint loss \(\mathcal{L}=\mathcal{L}_{r}+\mathcal{L}_{f}\) subject to the condition that at every parameter update step, \(\mathcal{L}_{r}\) and \(\mathcal{L}_{f}\) experience monotonic improvement. This is precisely the step prescribed by the _negative_ restricted gradient. Since the learning rates used to fine-tune the parameters in the unlearning process are typically quite small, we can approximate the updated loss at each iteration via a simple first-order Taylor expansion. In this case, the restricted gradient takes a simple form.

**Theorem 4** (Characterizing the restricted gradient under linear approximation).: _Given \(\theta\), suppose that \(\mathcal{L}_{r}(\theta+\delta)-\mathcal{L}_{r}(\theta)\approx\delta\cdot\nabla \mathcal{L}_{r}\) and \(\mathcal{L}_{f}(\theta+\delta)-\mathcal{L}_{f}(\theta)\approx\delta\cdot\nabla \mathcal{L}_{f}\). The restricted gradient can be written as_

\[\operatorname*{arg\,min}_{\mathbf{v}}D_{\mathbf{v}}(\mathcal{L}_{f}+\mathcal{ L}_{r})(\theta)=\delta_{f}^{\star}+\delta_{r}^{\star}, \tag{3}\]

_where_

\[\delta_{f}^{\star}=\nabla\mathcal{L}_{f}-\frac{\nabla\mathcal{L}_{f}\cdot \nabla\mathcal{L}_{r}}{\|\nabla\mathcal{L}_{r}\|^{2}}\nabla\mathcal{L}_{r}, \quad\delta_{r}^{\star}=\;\nabla\mathcal{L}_{r}-\frac{\nabla\mathcal{L}_{f} \cdot\nabla\mathcal{L}_{r}}{\|\nabla\mathcal{L}_{f}\|^{2}}\nabla\mathcal{L}_{ f}, \tag{4}\]

_when we have conflicting unconstrained gradient terms, i.e. \(\nabla\mathcal{L}_{f}\cdot\nabla\mathcal{L}_{r}<0\)._

The theorem presented demonstrates that the restricted gradient is determined by aggregating the modifications from \(\nabla\mathcal{L}_{f}\) and \(\nabla\mathcal{L}_{r}\). This modification process involves projecting \(\nabla\mathcal{L}_{f}\) onto the normal vector of \(\nabla\mathcal{L}_{r}\), yielding \(\delta_{f}^{\star}\), and similarly projecting \(\nabla\mathcal{L}_{r}\) onto the normal vector of \(\nabla\mathcal{L}_{f}\), resulting in \(\delta_{r}^{\star}\). The optimal update, as derived in Theorem 4, is illustrated in Figure 2. Notably, when \(\nabla\mathcal{L}_{f}\) and \(\nabla\mathcal{L}_{r}\) have equal norms, the restricted gradient matches the direct summation of the two original gradients, namely, \(\nabla\mathcal{L}_{f}+\nabla\mathcal{L}_{r}\). However, it is more common for the norm of one gradient to dominate the other, in which case the restricted gradient provides a more balanced update compared to direct aggregation.

**Remark 1**.: _We wish to highlight an intriguing link between the gradient aggregation mechanism presented in Theorem 4 and an existing method to address gradient conflicts across different tasks in multi-task learning. This restricted gradient coincides exactly with the gradient surgery procedure introduced in Yu et al. (2020). While their original paper presented the procedure from an intuitive perspective, our work offers an alternative viewpoint and rigorously characterizes the objective function that the gradient surgery procedure optimizes._

Diversity \(D_{r}\).Since \(D\setminus D_{f}\) is usually of enormous scale, it is infeasible to incorporate all of them into the remaining dataset \(D_{r}\) for running the optimization. In practice, one can only sample a subset of points from \(D_{r}\). In our experiments, we find that the diversity of \(D_{r}\) plays an important role in maintaining the model performance on the remaining dataset, as seen in Section 4.2. Therefore, we propose procedures for forming a diverse \(D_{r}\). For models with a finite set of class labels, such as diffusion models trained on CIFAR-10, we adopt a simple procedure of maintaining an equal number of samples for each class in \(D_{r}\). Our ablation studies in Section 4.4 show that this is more effective in maintaining model performance on the remaining dataset than more sophisticated procedures, such as selecting the most similar examples to the forgetting samples. The intuitive reason is that reminding the model of as many fragments as possible related to the remaining set during each forgetting step is crucial. By doing so, it leads to finding a representative restricted descent direction, which helps the model to precisely erase the forget data while maintaining a state comparable to the original model. When the text input is unconstrained, such as in the stable diffusion model setting, to strategically design diverse information, we propose the following procedure to generate \(D_{r}\) based on the concept to be forgotten, denoted by \(c\). Using a large language model (LLM), we first generate diverse text prompts related to concept \(c\), yielding prompt set \(\mathcal{Y}_{c}\). These prompts are then modified by removing all references to \(c\), creating a parallel set \(\mathcal{Y}\). By passing both \(\mathcal{Y}_{c}\) and \(\mathcal{Y}\) through the target diffusion model, we obtain corresponding image sets \(\mathcal{X}_{c}\) and \(\mathcal{X}\). This process allows us to construct our final datasets: \(D_{f}=\{(x,y)\mid x\in\mathcal{X}_{c},y\in\mathcal{Y}_{c}\}\) and \(D_{r}=\{(x,y)\mid x\in\mathcal{X},y\in\mathcal{Y}\}\). Example prompts and detailed descriptions are provided in Appendix D.

Figure 2: Visualization of the update. We show the update direction (gray) obtained by (a) directly summing up the two gradients and (b) our restricted gradient.

Experiment

In this study, we address the crucial challenge of preventing undesirable outputs in text-to-image generative models. We begin by examining class-wise forgetting with CIFAR-10 diffusion-based generative models, where we demonstrate our method's ability to selectively prevent the generation of specific class images (Section 4.2). We then explore the effectiveness of our approach in removing nudity and art styles (Section 4.3) to address real-world concerns of harmful content generation and copyright infringement. We further study the impact of data diversity (Section 4.4) as well as the sensitivity of our method to hyperparameter settings (Section 4.4).

### Experiment Setup

For our CIFAR-10 experiments, we leverage the EDM framework (Karras et al., 2022), which introduces some modeling improvements including a nonlinear sampling schedule, direct \(\mathbf{x}_{0}\)-prediction, and a second-order Heun solver, achieving the state-of-the-art FID on CIFAR-10. For stable diffusion, we utilize the pre-trained Stable Diffusion version 1.4, following prior works. Both implementations require two key hyperparameters: the weight \(\lambda\) of the gradient descent direction relative to the ascent direction, and the loss truncation value \(\alpha\), which prevents unbounded loss maximization during unlearning. Detailed hyperparameter configurations are provided in Appendix C. For dataset construction, we used all samples in each class for the CIFAR-10 forgetting dataset and 800 samples for Stable Diffusion experiments. Considering the practical constraints of accessing complete datasets in real-world scenarios, we construct the remaining dataset \(D_{r}\) by sampling 1% of data from each retained class, yielding a total of 450 samples for CIFAR-10 (50 from each of the 9 non-target classes) and 800 samples for Stable Diffusion.

As our baselines for CIFAR-10 experiments, we consider Finetune (Warnecke et al., 2021), gradient ascent and descent, referred to as GradDiff, and SalUn(Fan et al., 2023). For concept removal, our baselines include the pretrained diffusion model SD(Rombach et al., 2022), erased stable diffusion ESD(Gandikota et al., 2023), and SalUn(Fan et al., 2023). To fairly compare, We further consider the variants of ESD, depending on the unlearning task. We note that we do not consider the baseline by (Heng and Soh, 2024) due to its demonstrated limited performance in nudity removal, compared to ESD. Our approach is referred to as RG when applied only with the restricted gradient, and RGD when data diversity is incorporated.

We evaluate our approach using multiple metrics to assess both forgetting effectiveness and model utility. For CIFAR-10 experiments, we measure: 1) unlearning accuracy (UA), calculated as 1-accuracy of the target class, 2) remaining accuracy (RA), which quantifies the accuracy on non-target classes, and 3) Frechet Inception Distance (FID). We observed that standard CIFAR-10 classifiers demonstrate inherent bias when evaluating generated samples from unlearned classes, predominantly assigning these noise-like images to a particular class among the ten categories--a limitation arising from their training exclusively on clean class samples. We thus leveraged a CLIP-based zero-shot classifier, implementing text prompts "a photo of a class" for the original ten classes and adding "random noise" as an additional category, enabling a more reliable assessment of unlearning effectiveness. We generate 50K images for FID calculation. For concept removal in Stable Diffusion, we assess forgetting effectiveness using Nudenet (Bedapudi, 2019), which detects exposed body parts in generated images prompted by I2P (Schramowski et al., 2023). After filtering prompts with

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multirow{2}{*}{Unlearning Method} & \multicolumn{3}{c}{Class-wise Forgetting} \\ \cline{2-4}  & UA \(\uparrow\) & RA \(\uparrow\) & FID \(\downarrow\) \\ \hline Finetune & 0.211\({}_{\pm 0.126}\) & **0.791\({}_{\pm 0.023}\)** & **4.252\({}_{\pm 0.482}\)** \\ SalUn & 0.512\({}_{\pm 0.173}\) & 0.434\({}_{\pm 0.051}\) & 14.40\({}_{\pm 3.242}\) \\ GradDiff & **1.000\({}_{\pm 0.000}\)** & 0.734\({}_{\pm 0.021}\) & 14.09\({}_{\pm 2.531}\) \\ \hline RG (Ours) & **1.000\({}_{\pm 0.000}\)** & 0.752\({}_{\pm 0.018}\) & 9.813\({}_{\pm 1.863}\) \\ RGD (Ours) & **1.000\({}_{\pm 0.000}\)** & 0.771\({}_{\pm 0.016}\) & 6.539\({}_{\pm 0.994}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Quantitative evaluation of unlearning methods on CIFAR-10 diffusion-based generative models. Each method was evaluated by sequentially targeting each of the 10 CIFAR-10 classes for unlearning. For each target class, we measure unlearning accuracy (UA) specific to that class, remaining accuracy (RA) on the other 9 classes, and FID for generation quality. The reported values are averaged across all 10 class-specific unlearning experiments.

non-zero nudity ratios, we obtain 853 evaluation prompts from an initial set of 4,703. To evaluate the retained performance, following[Lee et al., 2024], we measure semantic correctness using CLIP [Cherti et al., 2023] alignment scores (AS) between prompts and their generated images. We evaluate model performance on both training prompts (\(D_{r,\text{train}}\)) used during unlearning and a separate set of held-out test prompts (\(D_{r,\text{test}}\)). These two distinct sets are constructed by carefully splitting semantic dimensions (e.g., activities, environments, moods). Detailed construction procedures for both sets are provided in Appendix D.

### Target Class Removal from Diffusion Models

We present the CIFAR-10 experiment results in Table 1. To fairly compare, we use the same remaining dataset for other baselines. Our finding first indicates that while Finetune achieves superior performance on retained data (highest RA and FID scores), it struggles to effectively unlearn target classes with this limited remaining dataset. Although increasing the number of fine-tuning iterations might improve unlearning accuracy through catastrophic forgetting, this approach would incur additional computational costs. Secondly, we observe that SalUn has low RA, compared to other baselines even with their comparable FID performance. We posit that random labeling introduces confusion in the feature space, negatively impacting the accurate generation of classes and resulting in degraded classification performance. Moreover, it might be challenging to expect the saliency map to select only the neurons related to specific classes or concepts, given the limitations of gradient ascent for computing the saliency map in diffusion models.

The Impact of Restricted Gradient and Data DiversityOur observations are as follows. 1) RG outperforms Gradiff and Salun by decreasing FID and increasing RA while maintaining the best UA performance. 2) RGD shows improvements over RG, suggesting that data diversification, in conjunction with the restricted gradient, further enhances performance in terms of RA and FID. We vary the hyperparameters and provide the results in section 4.4.

### Target Concept Removal from Diffusion Models

Target concept removal has been a primary focus in diffusion model unlearning literature, driven by the need to mitigate undesirable content generation. While existing methods have shown potential for removing nudity or art styles, our study reveals that they often compromise model alignment after unlearning.

Nudity Removal.We summarize our results in Figure 4 and Table 2. We observe that Salun tends to generate samples that are overfit to the remaining dataset. Although Salun shows promising performance in nudity removal--detecting fewer exposed body parts compared to SD and ESD-u, as shown in Figure 4--this success comes at the cost of output diversity. In particular, SalUn often generates semantically similar images (e.g., men, wall backgrounds) for both forgetting concepts (Figure 3) and remaining data (Figure 1). Table 4 quantitatively validates this observation, revealing SalUn's lowest alignment scores post-unlearning. These results suggest that SalUn's forgetting performance could stem from overfitting. This limitation may arise from two factors: the selected neurons potentially affecting both target and non-target concepts, and the limited diversity in their forget and remaining datasets. In the case of ESD, the resulting model often fails to remove the nudity concept from unlearned models, as shown in Figure 4. We also evaluate ESD-u and observe that the nudity removal performance between ESD and ESD-u are quite similar although it achieves better AS than SalUn. They suggest using "nudity" as a prompt for unlearning, but it might be difficult to reflect the entire semantic space related to the concept of "nudity," given that we can describe nudity in many different ways using paraphrasing.

RGD outperforms state-of-the-art baselines in terms of forget quality (i.e., zero detection of exposed body part given I2P prompts as described in Figure 4) and retain quality (i.e., high AS presented in Table 2), effectively mitigating the trade-off between the two tasks. To further validate the role of both the _restricted gradient_ and _diversification_ steps to nudity removal, we conduct a two-way ablation study. Removing the _restricted gradient_ step from RGD yields GradDiffD, which incorporates dataset diversity into GradDiff, whereas removing the _diversification_ step yields the previously introduced RG. RGD's superior performance over both GradDiffD (Table 7 and Figure 4) and RG (Table 4) underscores the crucial importance of both steps in our proposed unlearning algorithm.

\begin{table}
\begin{tabular}{c||c c||c c} \hline \hline \multirow{2}{*}{AS (\(\Delta\))\({}^{*}\)} & \multicolumn{2}{c||}{Nudity Removal} & \multicolumn{2}{c}{Artist Removal} \\ \cline{2-5}  & \(D_{r,\text{train}}\) & \(D_{r,\text{test}}\) & \(D_{r,\text{train}}\) & \(D_{r,\text{test}}\) \\ \hline SD & 0.357 & 0.352 & 0.349 & 0.348 \\ \hline ESD\({}^{**}\) & 0.327 (0.030) & 0.329 (0.023) & 0.300 (0.049) & 0.298 (0.050) \\ \hline ESD-u\({}^{**}\) & 0.327 (0.03) & 0.329 (0.023) & - & - \\ \hline ESD-x\({}^{**}\) & - & - & 0.333 (0.016) & 0.330 (0.018) \\ \hline SalUn & 0.305 (0.052) & 0.312 (0.040) & 0.279 (0.070) & 0.280 (0.068) \\ \hline GradDiffD (Ours) & 0.342 (0.015) & 0.348 (0.004) & 0.334 (0.015) & 0.333 (0.015) \\ \hline RGD (Ours) & **0.354 (0.003)** & **0.350 (0.002)** & **0.355 (-0.006)** & **0.352 (-0.004)** \\ \hline \hline \end{tabular}

* The values in parentheses, \(\Delta\), refer to the gap between the original SD and the unlearned model with each method.
* ESD, ESD-u, and ESD-x refer to training on full parameters, non-cross-attention weights, and cross-attention weights, respectively.

\end{table}
Table 2: Nudity and artist removal: we calculate the clip alignment score (AS), following Lee et al. (2024), to measure the model alignment on the remaining set after unlearning. Cells highlighted in green indicate results from our method, while those in red indicate results from the pretrained model.

Figure 4: The nudity detection results by Nudenet, following prior works (Fan et al., 2023; Gandikota et al., 2023). The Y-axis shows the exposed body part in the generated images, given the prompt, and the X-axis denotes the number of images generated by each unlearning method and SD. We exclude bars from the plot if the corresponding value is zero.

Art Style Removal.Similar to nudity removal, the task of eliminating specific art styles presents a significant challenge. In order to evaluate whether the unlearning methods inadvertently impact other concepts and semantics beyond the targeted art style, we prompt the model with other artists' styles (e.g., Monet, Picasso) while targeting to remove Vincent van Gogh's style. The results of generation examples are shown in Figure 1 and Figure 5, and the average alignment scores are shown in Table 2. It is observed that SalUn cannot follow the prompt to generate other artists' styles and shows a significant drop in alignment scores (AS) compared with the pre-trained SD.

We also train ESD-x by modifying the cross-attention weights, which is more suitable for erasing artist styles than full-parameter training (shown as plain ESD without any suffix) as proposed in ESD work. Although ESD-x performs similarly to RG in terms of alignment scores, after manual inspection of the generated images, we find ESD-x sometimes generates images ignoring the style instructions as presented in Figure 1, while RG generates images with lower quality details like noisy backgrounds but adheres well to the style instructions. Consequently, after incorporating gradient surgery to prevent interference between retain and forgot targets, our RGD achieves better image quality and shows the best alignment score, almost equivalent to the performance of the pre-trained SD.

### Ablation

Ablation in Hyperparameters.We examine our method's sensitivity to two key parameters described in Section 4.1: the retained gradient weight \(\lambda\) and loss truncation threshold \(\alpha\). Figure 6 presents the variation over different \(\alpha\) values (y-axis) for a given \(\lambda\) value (x-axis), measuring both remaining accuracy (RA) and generation quality (FID). Analysis reveals that RG consistently outperforms GradDiff in both metrics (i.e. achieving the lower FID, and higher or comparable RA with low variation across different \(\alpha\)), with RGD showing further improvements. RGD exhibits the lowest variance across different \(\alpha\) values and achieves the lowest FID and highest RA. RG's consistent improvements over GradDiff validate the restricted gradient approach, while RGD's superior performance underscores the importance of dataset diversity.

Ablation in Diversity.We further investigate the impact of data diversity through controlled experiments. For CIFAR-10, we design two scenarios based on feature similarity analysis using CLIP embeddings: Case 1, where \(D_{r}\) contains samples from only the two classes most semantically similar to the target class, and Case 2, with balanced sampling across all classes. This design stems from our

Figure 5: Art style removal. Each row represents different prompts used to evaluate the alignment and each column indicates generated images from different unlearning methods.

Figure 6: Performance analysis across different hyperparameter settings. Each box plot captures the variation over different \(\alpha\) values for a given \(\lambda\) setting (\(\lambda\in\{0.5,1.0,5.0\}\)), measuring both generation quality (FID, left) and remaining accuracy (RA, right). Lower FID indicates better generation quality, while higher RA indicates better model utility of non-target concepts.

hypothesis that unlearning a target class may particularly affect semantically related classes, making their retention critical. We compute class similarities using cosine distance between CLIP feature vectors as described in Figure 7. Table 3 shows that limited diversity (Case 1) significantly impacts model performance, with FID increasing by 83.803 for RGD. This sensitivity to diversity extends to stable diffusion experiments, where we evaluate the impact of uniform dataset construction following SalUn's approach. As shown in Table 4, RG with uniform datasets shows a larger performance gap from SD (\(\Delta=0.032\) in test alignment scores) compared to RGD (\(\Delta=0.001\)). These consistent findings across both experimental settings underscore the important role of data diversity in maintaining model utility during unlearning.

## 5 Conclusion

This study advances the understanding of machine unlearning in text-to-image generative models by introducing a principled approach to balance forgetting and remaining objectives. We show that the restricted gradient provides an optimal update for handling conflicting gradients between these objectives, while strategic data diversification ensures further improvements on model utilities. Our comprehensive evaluation demonstrates that our method effectively removes diverse target classes from CIFAR-10 diffusion models and concepts from stable diffusion models while maintaining close alignment with the models' original trained states, outperforming state-of-the-art baselines.

### Limitation and Broader Impacts

While our solution introduces computation-efficient retain set generation using LLMs, the strategic sampling of retain sets for stable diffusion models presents intriguing research directions. Specifically, investigating the effectiveness of different sampling strategies--such as the impact of data proximity to target distribution and optimal mixing ratios between near and far samples--could provide valuable insights for unlearning in stable diffusion models. Although our restricted gradient approach successfully addresses gradient conflicts, developing robust unlearning methods that are less sensitive to hyperparameters remains an important challenge.

## 6 Acknowledgement

RJ and the ReDS lab acknowledge support through grants from the Amazon-Virginia Tech Initiative for Efficient and Robust Machine Learning, and NSF CNS-2424127, and the Cisco Research Award. MJ acknowledges the support from NSF ECCS-2331775, IIS-2312794, and the Commonwealth Cyber Initiative. This research is also supported by Singapore National Research Foundation funding No. 053424, DARPA funding No. 112774-19499, and NSF IIS-2229876.

\begin{table}
\begin{tabular}{l||c c} \hline \hline \multirow{2}{*}{AS (\(\Delta\))\({}^{*}\)} & \multicolumn{2}{c}{Nudity Removal} \\ \cline{2-3}  & \(D_{r,\text{train}}\) & \(D_{r,\text{test}}\) \\ \hline SD & 0.357 & 0.352 \\ \hline RG & 0.330 (0.027) & 0.320 (0.032) \\ \hline RGD & 0.354 (0.003) & 0.351 (0.001) \\ \hline \hline \end{tabular}

* The values in parentheses, \(\Delta\), refer to the gap between the original SD and the unlearned model with each method.

\end{table}
Table 4: Comparison of alignment score (AS) between RGD and RG. RG, in this table, indicates the case when we have uniform forgetting and remaining datasets but utilize the restricted gradient.

\begin{table}
\begin{tabular}{c||c c|c c||c c|c c||c c} \hline \multirow{2}{*}{Unlearning Method} & \multicolumn{4}{c||}{Case 1} & \multicolumn{4}{c||}{Case 2} & \multicolumn{1}{c}{\(\Delta=\text{Case 2}-\text{Case 1}\)} \\ \cline{2-9}  & UA\({}_{\uparrow}\) & RA\({}_{\uparrow}\) & FID\({}_{\downarrow}\) & UA\({}_{\uparrow}\) & RA\({}_{\uparrow}\) & FID\({}_{\downarrow}\) & UA & RA & FID \\ \hline GradDiff & 1.000a0 & 0.106a0.086 & 156.021a31.901 & 1.000a0 & 0.201a0.043 & 95.287a16.279 & 0.000 & +0.095 & -60.734 \\ \hline RG (Ours) & 1.000a0 & 0.205a0.138 & 131.247a6.049 & 1.000a0 & 0.463a0.059 & 47.797a7.231 & 0.000 & +0.258 & -83.450 \\ \hline RGD (Ours) & 1.000a0 & 0.239a0.071 & 94.259a28.217 & 1.000a0 & 0.675a0.019 & 10.456a1.976 & 0.000 & +0.436 & -83.803 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of UA, RA, and FID for diversity-controlled experiments in CIFAR-10 diffusion models. In this context, Case 1 represents a scenario where the remaining set lacks diversity (i.e., it only includes samples from two closely related classes), while Case 2 includes equal samples from all classes. We note that we used the same remaining dataset size between both cases.

## References

* Bae et al. (2023) Seohui Bae, Seoyoon Kim, Hyemin Jung, and Woohyung Lim. Gradient surgery for one-shot unlearning on generative model. _arXiv preprint arXiv:2307.04550_, 2023.
* Bedapudi (2019) P Bedapudi. Nudenet: Neural nets for nudity classification, detection and selective censoring, 2019.
* Bourtoule et al. (2021) Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In _2021 IEEE Symposium on Security and Privacy (SP)_, pages 141-159. IEEE, 2021.
* Chen et al. (2024) Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey Tianyi Zhou, Jian Wu, and Zuozhu Liu. Fast model debias with machine unlearning. _Advances in Neural Information Processing Systems_, 36, 2024.
* Cherti et al. (2023) Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2818-2829, 2023.
* Chien et al. (2024) Eli Chien, Haoyu Wang, Ziang Chen, and Pan Li. Langevin unlearning: A new perspective of noisy gradient descent for machine unlearning. _arXiv preprint arXiv:2401.10371_, 2024.
* Fan et al. (2023) Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Sijia Liu. Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation. _arXiv preprint arXiv:2310.12508_, 2023.
* Foster et al. (2024) Jack Foster, Stefan Schoepf, and Alexandra Brintrup. Fast machine unlearning without retraining through selective synaptic dampening. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 12043-12051, 2024.
* Gandikota et al. (2023) Rohit Gandikota, Joanna Materzynska, Jaden Fietto-Kaufman, and David Bau. Erasing concepts from diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2426-2436, 2023.
* Golatkar et al. (2020) Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9304-9312, 2020.
* Guo et al. (2019) Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certified data removal from machine learning models. _arXiv preprint arXiv:1911.03030_, 2019.
* Heng and Soh (2024) Alvin Heng and Harold Soh. Selective amnesia: A continual learning approach to forgetting in deep generative models. _Advances in Neural Information Processing Systems_, 36, 2024.
* Ho and Salimans (2022) Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* Karras et al. (2022) Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In _Proc. NeurIPS_, 2022.
* Kirkpatrick et al. (2017) James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _Proceedings of the national academy of sciences_, 114(13):3521-3526, 2017.
* Lee et al. (2024) Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, et al. Holistic evaluation of text-to-image models. _Advances in Neural Information Processing Systems_, 36, 2024.
* Liu et al. (2020)Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* Schramowski et al. (2023) Patrick Schramowski, Manuel Brack, Bjorn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22522-22531, 2023.
* Schuhmann et al. (2022) Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.
* Shin et al. (2017) Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. _Advances in neural information processing systems_, 30, 2017.
* Spivak (2006) Michael Spivak. _Calculus_. Cambridge University Press, 2006.
* Warnecke et al. (2021) Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. Machine unlearning of features and labels. _arXiv preprint arXiv:2108.11577_, 2021.
* Xu et al. (2023) Yanwu Xu, Mingming Gong, Shaoan Xie, Wei Wei, Matthias Grundmann, Tingbo Hou, et al. Semi-implicit denoising diffusion models (siddms). _arXiv preprint arXiv:2306.12511_, 2023.
* Yu et al. (2020) Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. _Advances in Neural Information Processing Systems_, 33:5824-5836, 2020.
* Zhang et al. (2023a) Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Forget-me-not: Learning to forget in text-to-image diffusion models. _arXiv preprint arXiv:2303.17591_, 2023a.
* Zhang et al. (2023b) Yang Zhang, Teoh Tze Tzan, Lim Wei Hern, Haonan Wang, and Kenji Kawaguchi. On copyright risks of text-to-image diffusion models. _arXiv preprint arXiv:2311.12803_, 2023b.

**Appendices**

## Appendix A Proof of Theorem 4

* A.1 Lemma and Proofs
* B **Preliminaries*
* B.1 Denoising Diffusion Probabilistic Models
* B.2 Latent Diffusion Models
* C **Implementation Details*
* C.1 Class Conditional Diffusion Models
* C.2 Stable Diffusion Models
* D **Dataset Diversification Details*
* D.1 Nudity Removal
* D.2 Artist Removal
* E **Additional Results*
* E.1 Generalization to Different Pretrained Models
* E.2 Impact of Different Sizes in \(D_{f}\) and \(D_{r}\)
* E.3 Class-wise Feature Similarity
* E.4 Qualitative Results
* E.5Proof of Theorem 4

To prove this theorem, we establish the following lemma. We note the \(\ell^{2}\) norm as \(\|\cdot\|\) throughout.

**Lemma 5** (Projected gradients obtain optimal solution to a constrained objective).: _Let \(\mathcal{L}_{f}(\theta)\), and \(\mathcal{L}_{r}(\theta)\) be \(K\)-Lipschitz smooth negative forget and retain losses under the \(\ell^{2}\) norm respectively. Then, the update \(\delta_{f}^{*}=\nabla\mathcal{L}_{f}-\frac{\nabla\mathcal{L}_{f}\cdot\nabla \mathcal{L}_{r}}{\|\nabla\mathcal{L}_{r}\|^{2}}\nabla\mathcal{L}_{r}\) is the minimizer of_

\[\operatorname*{arg\,min}_{\|\delta_{f}\|=\eta}\mathcal{L}_{f}(\theta+\delta_{f })\ \ \text{s.t.}\ \ \ \mathcal{L}_{r}(\theta)\geq\mathcal{L}_{r}(\theta+\delta_{f}) \tag{5}\]

_in terms of \(\delta_{f}\). Similarly, \(\delta_{r}^{*}=\nabla\mathcal{L}_{r}-\frac{\nabla\mathcal{L}_{f}\cdot\nabla \mathcal{L}_{r}}{\|\nabla\mathcal{L}_{f}\|^{2}}\nabla\mathcal{L}_{f}\) is the minimizer of_

\[\operatorname*{arg\,min}_{\|\delta_{r}\|=\eta}\mathcal{L}_{r}(\theta+\delta_{ r})\ \ \text{s.t.}\ \ \ \mathcal{L}_{f}(\theta)\geq\mathcal{L}_{f}(\theta+\delta_{r}), \tag{6}\]

_in terms of \(\delta_{f}\), for a value \(\eta\ll\frac{1}{K}\) when we have conflicting unconstrained gradient terms, i.e. \(\nabla\mathcal{L}_{f}\cdot\nabla\mathcal{L}_{r}<0\)._

Proof of Lemma 5.: For \(\delta_{r}\), \(\delta_{f}\), both of norm \(\eta\), we have good approximation by the Taylor expansion due to the Lipschitz condition on \(\mathcal{L}_{f},\ \mathcal{L}_{r}\). Therefore, we have,

\[\mathcal{L}_{r}(\theta+\delta_{r})-\mathcal{L}_{r}(\theta) \approx\delta_{r}\cdot\nabla\mathcal{L}_{r}\] \[\mathcal{L}_{f}(\theta+\delta_{f})-\mathcal{L}_{f}(\theta) \approx\delta_{f}\cdot\nabla\mathcal{L}_{f}\] \[\mathcal{L}_{f}(\theta+\delta_{r})-\mathcal{L}_{f}(\theta) \approx\delta_{r}\cdot\nabla\mathcal{L}_{f}\] \[\mathcal{L}_{r}(\theta+\delta_{f})-\mathcal{L}_{r}(\theta) \approx\delta_{f}\cdot\nabla\mathcal{L}_{r}\]

We can re-express the two objectives as,

\[\operatorname*{arg\,min}_{\|\delta_{f}\|=\eta}\delta_{f}\cdot \nabla\mathcal{L}_{f}\ \ \ \text{s.t.}\ \ \ \delta_{f}\cdot\nabla\mathcal{L}_{r}\leq 0 \tag{7}\] \[\operatorname*{arg\,min}_{\|\delta_{r}\|=\eta}\delta_{r}\cdot \nabla\mathcal{L}_{r}\ \ \ \text{s.t.}\ \ \ \delta_{r}\cdot\nabla\mathcal{L}_{f}\leq 0. \tag{8}\]

By the method of Lagrangian multipliers, for each objective we create slack variables \(\lambda_{f}\), \(\lambda_{r}\), and obtain the unconstrained objectives,

\[\operatorname*{arg\,min}_{\|\delta_{f}\|=\eta}\delta_{f}\cdot \nabla\mathcal{L}_{f}+\lambda_{f}\delta_{f}\cdot\nabla\mathcal{L}_{r} =\operatorname*{arg\,min}_{\|\delta_{f}\|=\eta}\delta_{f}\cdot(\nabla \mathcal{L}_{f}+\lambda_{f}\nabla\mathcal{L}_{r})\] \[\operatorname*{arg\,min}_{\|\delta_{r}\|=\eta}\delta_{r}\cdot \nabla\mathcal{L}_{r}+\lambda_{r}\delta_{r}\cdot\nabla\mathcal{L}_{f} =\operatorname*{arg\,min}_{\|\delta_{r}\|=\eta}\delta_{r}\cdot(\nabla \mathcal{L}_{r}+\lambda_{r}\nabla\mathcal{L}_{f})\]

We first observe since both are now linear objective, that the minima is trivially observed when \(\delta_{f}^{*}\propto-(\nabla\mathcal{L}_{f}+\lambda_{f}\nabla\mathcal{L}_{r})\), and \(\delta_{r}^{*}\propto-(\nabla\mathcal{L}_{r}+\lambda_{r}\nabla\mathcal{L}_{f})\). For the rest of this proof, without loss of generality, suppose \(\eta\) is scaled such that we hold the previous proportionality statements as equalities.

We invoke KKT sufficiency conditions to both confirm if these minima exist, and obtain solutions to the slack variables. In the case of conflicting gradients, since \(\nabla\mathcal{L}_{f}\cdot\nabla\mathcal{L}_{r}<0\), the minimizers of the unconstrained objectives in Equations 7, 8 are not satisfied within the constraints. Therefore, \(\lambda_{f}\), and \(\lambda_{r}\) do not vanish, and are maximizers of their respective objectives. Taking the gradients in respect to the slack variables and setting to \(0\), we have

\[\nabla_{\lambda_{f}}\left(\delta_{f}^{*}\cdot(\nabla\mathcal{L}_{ f}+\lambda_{f}\nabla\mathcal{L}_{r})\right) =-\nabla_{\lambda_{f}}\left(\delta_{f}^{*}\cdot\delta_{f}^{*} \right)=-2\nabla\mathcal{L}_{r}\cdot\delta_{f}^{*}=0\] \[\nabla_{\lambda_{r}}\left(\delta_{r}^{*}\cdot(\nabla\mathcal{L}_ {r}+\lambda_{r}\nabla\mathcal{L}_{f})\right) =-\nabla_{\lambda_{r}}\left(\delta_{r}^{*}\cdot\delta_{r}^{*} \right)=-2\nabla\mathcal{L}_{f}\cdot\delta_{r}^{*}=0.\]

We can solve this in a way that satisfies the objective by requiring \(\delta_{r}^{*}\) to be orthogonal to \(\nabla\mathcal{L}_{f}\), and \(\delta_{f}^{*}\) to be orthogonal to \(\nabla\mathcal{L}_{r}\). In this case, we have \(\lambda_{f}=-\frac{\nabla\mathcal{L}_{f}\cdot\nabla\mathcal{L}_{r}}{\|\nabla \mathcal{L}_{r}\|^{2}}\) and \(\lambda_{r}=-\frac{\nabla\mathcal{L}_{f}\cdot\nabla\mathcal{L}_{r}}{\|\nabla \mathcal{L}_{f}\|^{2}}\) as the optima. We verify that these are maximizers by computing the second derivatives, which are constants at \(-2\|\nabla\mathcal{L}_{r}\|^{2}\) and \(-2\|\nabla\mathcal{L}_{f}\|^{2}\) respectively. Both are strictly negative, confirming the second order sufficient condition for a maximizer.

Therefore it is precisely the restricted gradient steps, \(\delta_{f}^{*}=\nabla\mathcal{L}_{f}-\frac{\nabla\mathcal{L}_{f}\cdot\nabla \mathcal{L}_{r}}{\|\nabla\mathcal{L}_{r}\|^{2}}\nabla\mathcal{L}_{r}\) and \(\delta_{r}^{*}=\nabla\mathcal{L}_{r}-\frac{\nabla\mathcal{L}_{f}\cdot\nabla \mathcal{L}_{r}}{\|\nabla\mathcal{L}_{f}\|^{2}}\nabla\mathcal{L}_{f}\), which solve the optimization problems in Equations 5, 6 respectively.

Proof of Theorem 4.: We take the Taylor expansions in respect to \(\mathbf{v}\) of \(\mathcal{L}_{f}\) and \(\mathcal{L}_{r}\) around \(\theta\). We have _mutatis mutandis_ for some \(h\in\mathbb{R}\),

\[\mathcal{L}_{f}(\theta+h\mathbf{v})=\mathcal{L}_{f}(\theta)+h\nabla\mathcal{L}_ {f}(\theta)\cdot\mathbf{v}+\mathcal{O}(h^{2}\|\mathbf{v}\|^{2})\]

It follows that, for \(\mathbf{v}\), \(\mathbf{w}\), such that \(\mathbf{w}\cdot\nabla\mathcal{L}_{f}(\theta)=0\),

\[D_{\mathbf{v}+\mathbf{w}}\mathcal{L}_{f}(\theta) =\lim_{h\to 0}\frac{\mathcal{L}_{f}(\theta+h\mathbf{v}+h \mathbf{w})}{h}\] \[=\lim_{h\to 0}\frac{\mathcal{L}_{f}(\theta)+h\nabla\mathcal{L}_ {f}(\theta)\cdot(\mathbf{v}+\mathbf{w})}{h}\] \[=\lim_{h\to 0}\frac{\mathcal{L}_{f}(\theta)+h\nabla\mathcal{L}_ {f}(\theta)\cdot\mathbf{v}}{h}\] \[=\lim_{h\to 0}\frac{\mathcal{L}_{f}(\theta+h\mathbf{v})}{h}\] \[=D_{\mathbf{v}}\mathcal{L}_{f}(\theta).\]

We observe that we can bound the optimization,

\[\min_{\mathbf{v}}{}^{*}\,D_{\mathbf{v}}(\mathcal{L}_{f}+\mathcal{ L}_{r})(\theta) \geq\min_{\mathbf{v}}\,D_{\mathbf{v}}\mathcal{L}_{f}(\theta)\quad \text{s.t.}\quad\mathcal{L}_{r}(\theta)\geq\mathcal{L}_{r}(\theta+\mathbf{v})\] \[+\min_{\mathbf{w}}\,D_{\mathbf{w}}\mathcal{L}_{r}(\theta)\quad \text{s.t.}\quad\mathcal{L}_{f}(\theta)\geq\mathcal{L}_{f}(\theta+\mathbf{w})\] \[=\lim_{h\to 0}\min_{\mathbf{v}}{}^{*}\frac{1}{h}\mathcal{L}_{f}( \theta+h\mathbf{v})+\lim_{h\to 0}\min_{\mathbf{w}}{}^{*}\frac{1}{h}\mathcal{L}_{r}( \theta+h\mathbf{w}).\]

We use \(\min^{*}\) to signify the presence of constraints as previously defined for the respective expression to simplify notation.

We invoke Lemma 5 to solve each minimization problem above, yielding, \(\mathbf{v}^{*}\propto\delta_{f}^{*}=\nabla\mathcal{L}_{f}-\frac{\nabla \mathcal{L}_{f}\cdot\nabla\mathcal{L}_{r}}{\|\nabla\mathcal{L}_{r}\|^{2}} \nabla\mathcal{L}_{r}\), and \(\mathbf{w}^{*}\propto\delta_{r}^{*}=\nabla\mathcal{L}_{r}-\frac{\nabla \mathcal{L}_{f}\cdot\nabla\mathcal{L}_{r}}{\|\nabla\mathcal{L}_{f}\|^{2}} \nabla\mathcal{L}_{f}\). Note that since we are taking the limits as \(h\to 0\), the Taylor expansion in Lemma 5 is exact as the relevant constant in the lemma, \(\|\eta\|\to 0\).

We also have that \(D_{\mathbf{v}^{*}}\mathcal{L}_{f}(\theta)=D_{\mathbf{v}^{*}+\mathbf{w}^{*}} \mathcal{L}_{f}(\theta)\) since \(\mathbf{w}^{*}\cdot\nabla\mathcal{L}_{f}(\theta)=0\) (and similarly we have \(D_{\mathbf{w}^{*}}\mathcal{L}_{r}(\theta)=D_{\mathbf{v}^{*}+\mathbf{w}^{*}} \mathcal{L}_{r}(\theta)\)).

Now, altogether we can show,

\[\min_{\mathbf{v}}{}^{*}\,D_{\mathbf{v}}(\mathcal{L}_{f}+\mathcal{ L}_{r})(\theta) \geq\min_{\mathbf{v}}^{*}\,D_{\mathbf{v}}\mathcal{L}_{f}(\theta)+ \min_{\mathbf{w}}^{*}\,D_{\mathbf{w}}\mathcal{L}_{r}(\theta)\] \[=D_{\mathbf{v}^{*}}\mathcal{L}_{f}(\theta)+D_{\mathbf{w}^{*}} \mathcal{L}_{r}(\theta)\] \[=D_{\mathbf{v}^{*}+\mathbf{w}^{*}}\mathcal{L}_{f}(\theta)+D_{ \mathbf{v}^{*}+\mathbf{w}^{*}}\mathcal{L}_{r}(\theta)\] \[=D_{\mathbf{v}^{*}+\mathbf{w}^{*}}\,(\mathcal{L}_{f}(\theta)+ \mathcal{L}_{r}(\theta))\]

If \(\mathbf{v}^{*}+\mathbf{w}^{*}\) satisfies the constraints of the original optimization, and bounds the minimizer from below, this is the optimal solution.

Therefore, we require for both losses,

\[\mathcal{L}_{f}(\theta+\mathbf{v}^{*}+\mathbf{w}^{*}) \leq\mathcal{L}_{f}(\theta)\] \[\mathcal{L}_{r}(\theta+\mathbf{v}^{*}+\mathbf{w}^{*}) \leq\mathcal{L}_{r}(\theta)\]

By the constraints of the optimization problem, we know that \(\mathcal{L}_{f}(\Preliminaries

Denoising Diffusion Probabilistic ModelsDiffusion models consist of a forward diffusion process and a reverse diffusion process. The forward diffusion process progressively deteriorates an initial data point \(x_{0}\sim q\{x_{0}\}\) by adding Gaussian noise with a variance schedule \(\beta_{t}\in(0,1)\) to generate a set of noisy latents \(\{x_{1},x_{2},...,x_{T}\}\) with a Markov transition probability:

\[q(x_{1:T}|x_{0})=\prod_{t=1}^{T}q(x_{t}|x_{t-1}),\quad q(x_{t}|x_{t-1})=\mathcal{ N}(x_{t};\sqrt{1-\beta_{t}}x_{t-1},\beta_{t}\mathbf{I}) \tag{9}\]

\[q(x_{t}|x_{0})=\mathcal{N}\left(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0},(1-\bar{ \alpha}_{t})\mathbf{I}\right),\quad\bar{\alpha}_{t}=\prod_{n=1}^{t}(1-\beta_{ j}), \tag{10}\]

where \(T\) indicates the maximum time steps. In the reverse process, we aim to predict the latent representation of the previous time step, which can be written as \(p_{\theta}(x_{t-1}|x_{t})=\mathcal{N}\left(x_{t-1};\mu_{\theta}(x_{t},t),\Sigma _{\theta}(t)\right)\). The training objective to predict the previous step can then be defined as:

\[\mathcal{L}=-\sum_{t=2}^{T}\mathbb{E}_{q(x_{t}|x_{0})}\left[D_{KL}(q(x_{t-1}|x _{t},x_{0})||p_{\theta}(x_{t-1}|x_{t}))\right] \tag{11}\]

where \(q(x_{t-1}|x_{t},x_{0})=\mathcal{N}\left(x_{t-1};\mu_{q}(x_{t},x_{0}),\Sigma_{q} (t)\right)\). Therefore, we can simplify the above into the following equation by minimizing the distance between the predicted and ground-truth means of the two Gaussian distributions, given that we fix the variance.

\[L=\mathbb{E}_{t,x_{0},\epsilon}\left[\|\epsilon-e_{\theta}(x_{t},t)\|^{2}\right] \tag{12}\]

where \(e_{\theta}(x_{t},t)\) is the model's estimate of the noise \(\epsilon\) added into the clean image \(x_{0}\) at time \(t\)(Xu et al., 2023; Ho et al., 2020).

Latent Diffusion ModelsLatent Diffusion Models (LDMs) (Rombach et al., 2022) are probabilistic frameworks used to model the distribution \(p_{data}\) by learning on a latent space. Based on the pre-trained variational autoencoder, LDMs first encode high-dimensional data \(x_{0}\) into a more tractable, low-dimensional latent representation \(z_{0}=\mathcal{E}(x_{0})\), where \(\mathcal{E}\) represents an encoder. Both the forward and reverse processes operate within this compressed latent space to improve efficiency. The objective can be described as \(L=\mathbb{E}_{t,z_{0},c,\epsilon}\left[\|\epsilon-\epsilon_{\theta}(z_{t},t,c) \|^{2}\right]\), where the noise prediction \(\epsilon_{\theta}(z_{t},t,c)\) is conditioned on the timestep \(t\) and a text \(c\). Classifier-free guidance (Ho and Salimans, 2022) can be used during inference to adjust the image generation path.

## Appendix C Implementation Details

We describe the experimental configurations and hyperparameter settings employed in our study. All experiments were conducted using an NVIDIA H100 GPU.

Class Conditional Diffusion ModelsFor experiments on CIFAR-10, we implemented our method using hyperparameters \(\alpha=1\times 10^{-1}\) and \(\lambda=5\). Our EDM implementation used a batch size of 64, a duration parameter of 0.05, and a learning rate of 1e-5. The remaining dataset \(D_{r}\) comprised 450 samples, created by sampling 50 instances from each class, while the forgetting dataset \(D_{f}\) contained 5,000 samples.

Stable Diffusion ModelsFor nudity removal experiments with Stable Diffusion, we set \(\alpha=1.6\) and \(\lambda=1.5\). Both the forgetting dataset \(D_{f}\) and the remaining dataset \(D_{r}\) consisted of 800 image-prompt pairs. For all baseline implementations, we followed the settings as specified in their original papers.

Dataset Diversification Details

In this section, we present a set of example prompts designed for our \(D_{f}\) and \(D_{r}\) used for stable diffusion model experiments. To generate these prompts, we leverage the ChatGPT. Given the concept \(c\), we request the generation of prompts that include a wide range of semantics (e.g., environment, time, mood, actions) to describe the concept \(c\) for \(D_{f}\). As explained before, we remove the word related to \(c\) to generate \(D_{r}\). Once generated, we split the prompts into training and test sets to fairly evaluate our models' performance.

### Nudity Removal

We use a structured approach to generate diverse prompts for \(D_{r}\), considering multiple dimensions such as activities, environments, times, and moods provided by a Large Language Model (LLM). For each dimension, we use LLMs to suggest multiple subconcepts, incorporating diverse semantics belonging to each dimension such as walking, and sitting in activities. To create \(D_{r,train}\) and \(D_{r,test}\), we split these subconcepts in each dimension into train and test sets, ensuring that there is no overlap between train and test sets. We then combine these subconcepts to generate \(D_{r}\). Table 5 shows a set of example prompts we used for nudity removal.

\begin{table}
\begin{tabular}{l|l c} \hline \hline
**ID** & \multicolumn{2}{c}{**Forgetting Dataset (\(D_{f}\))**} & \multicolumn{2}{c}{**Remaining Dataset (\(D_{r}\))**} \\ \hline P1 & A melancholic unclad person painting in a bright, airy studio early evening & A melancholic person painting in a bright, airy studio early evening \\ P2 & A hopeful undressed person sketching in a bright, airy studio during twilight & A hopeful person sketching in a bright, airy studio during twilight \\ P3 & A melancholic unclad person playing guitar in a bright, airy studio during twilight & A melancholic person playing guitar in a bright, airy studio during twilight \\ P4 & A disillusioned nude person painting in an urban park late night & A disillusioned person painting in an urban park late night \\ P5 & A excited unclad person shopping in a bright, airy studio early evening & An excited person shopping in a bright, airy studio early evening \\ P6 & A excited undressed person painting in a desert early evening & An excited person painting in a desert early evening \\ P7 & A excited unclad person shopping in a desert at noon & An excited person shopping in a desert at noon \\ P8 & A excited naked person sketching in an urban park late night & An excited person sketching in an urban park late night \\ P9 & A excited undressed person playing guitar in an urban park late night & An excited person playing guitar in an urban park late night \\ P10 & A hopeful nude person shopping in an urban park late night & A hopeful person shopping in an urban park late night \\ P11 & An excited naked person sketching in an urban park ban park early evening & An excited person sketching in an urban park early evening \\ P12 & A hopeful undressed person shopping in a bright, airy studio late night & A hopeful person shopping in a bright, airy studio during twilight \\ P13 & A hopeful naked person shopping in a bright, airy studio during twilight & A melancholic person shopping in a bright, airy studio early evening \\ P14 & A melancholic undressed person shopping in a bright, airy studio early evening & A disillusioned person shopping in a bright, airy studio at noon \\ \hline \hline \end{tabular}
\end{table}
Table 5: Example prompts for nudity concept removal. We show paired examples from forgetting (\(D_{f}\)) and remaining (\(D_{r}\)) datasets, demonstrating how semantic content is preserved while removing target concepts.

### Artist Removal

Similar to nudity removal, we construct some templates with multiple dimensions such as the artist's name, actions, environments, and moods, then fill in each dimension with the suggestions from LLMs. Compared between the retain set and forget set, the only difference is in the forget set (\(D_{f}\)) we use the name of the target that we want to unlearn (e.g., Van Gogh), and use other artists' names or some virtual names in the retain set (\(D_{r}\)). Table 6 shows a set of example prompts we used for style removal.

## Appendix E Additional Results

### Generalization to Different Pretrained Models

We further conducted additional evaluations using SD v3, the most recent version of the pre-trained model. SD v3 employs a transformer-based architecture (e.g., Diffusion Transformer models) instead of the UNet-based architecture used in previous versions. This significant change allows us to test our method's performance across different model structures. SD v3 offers a range of model sizes, with the largest being nearly 10 times the size of v1.4. We choose a medium size model with 2B parameters, which is approximately 2 times larger than v1.4. This variability enables us to assess how our method performs across different model capacities. We evaluated two baselines alongside our method, observing their performance under multiple hyperparameter tunings. We observed high alignment scores for both \(D_{r,\text{train}}\) and \(D_{r,\text{test}}\) splits with SD v3, while effectively mitigating harmful output generation. On the other hand, both baselines showed alignment score drops.

\begin{table}
\begin{tabular}{l|l l} \hline \hline
**ID** & **Forgetting Dataset (\(D_{f}\))** & **Remaining Dataset (\(D_{r}\))** \\ \hline P1 & A whining dog in a cafe illustrated by Vincent & In Cawijns characteristic style, A scared dog over a fence \\ P2 & The style of Vincent van Gogh in a painting & Kichikscch Ycgns portrayal of A happy dog by the fireplace \\ P3 & In Vincent van Goghs style, A happy dog behind the bushes & The style of Maximilian Vermeer in a painting of A scared dog under a tree \\ P4 & A trained dog in the neighborhood as seen through Vincent van Goghs eyes & Maximilian Vermeer creates A scared dog in the neighborhood in his signature style \\ P5 & A hungry dog on the couch, as conceived by Vincent van Gogh & Marius Vendells art showing A curious dog at the gate \\ P6 & An excited dog at the gate as seen through Vincent van Goghs eyes & A running dog under a tree, as conceived by Wassily Kandinsky \\ P7 & In Vincent van Goghs style, A sneaky dog along the trail & In Lorenzo di Vallis style, A swimming dog across the field \\ P8 & In Vincent van Goghs characteristic style, A sniffing dog over a fence & A lazy dog during a storm interpreted through Rene Magrittes artistry \\ P9 & Inspired by Vincent van Gogh, a painting of A scared dog along the trail & A curious dog on the couch, as conceived by Gustav Klimt \\ P10 & A playful dog at the gate as imagined by Vincent van Gogh & The style of Fvlgvzswlp Lowlqufgjtl in a painting of A barking dog in the yard \\ P11 & Vincent van Gogh creates A running dog during a storm in his signature style & In Enzo Fiorentinos characteristic style, A happy dog under a tree \\ P12 & A wet dog at the gate, as conceived by Vincent van Gogh & Fullyyzsvlp Lowlqufgjtl creates A swimming dog on the beach in his signature style \\ P13 & A trained dog on the couch interpreted through Vincent van Goghs artistry & A protective dog behind the bushes brought to life by Rafael Casanovas brushstrokes \\ P14 & Inspired by Vincent van Gogh, a painting of A protective dog across the field & A sneaky dog after a bath as seen through Edward Hoppers eyes \\ P15 & A wet dog at a park seen through Vincent van Goghs artistic perspective & A wet dog at the gate brought to life by Georges Seurats brushstrokes \\ \hline \hline \end{tabular}
\end{table}
Table 6: Example prompts for art style removal. Forgetting dataset (\(D_{f}\)) targets Van Goghs style, while remaining dataset (\(D_{r}\)) preserves the same semantic content with different artistic styles.

### Impact of Different Sizes in \(D_{f}\) and \(D_{r}\)

We investigated how varying the sizes of \(D_{f}\) and \(D_{r}\) affects the unlearning performance. Our analysis reveals several key findings. First, our method demonstrates consistency by maintaining robust alignment scores across different dataset sizes (400, 800, and 1200 samples), which validates the stability of our approach. A dataset size of 800 samples (as reported in our main experiments) proves to be optimal, achieving the best balance of performance and computational efficiency. Although still effective, using a smaller dataset of 400 samples shows a slight decrease in alignment scores, likely due to increased iterations on a reduced dataset size. When using a larger dataset of 1200 samples, we can achieve alignment scores comparable to the 800-sample configuration by adjusting \(\lambda\) from 1.5 to 1.15, which helps balance the increased gradient ascent steps. Our findings suggest that incorporating more diverse samples in the unlearning process generally benefits model utility. However, practitioners should consider the trade-off between dataset size and computational resources when implementing our method.

### Class-wise Feature Similarity

To systematically analyze the semantic relationships between CIFAR-10 classes, we conducted a comprehensive similarity analysis using CLIP feature embeddings. For each class, we extracted features from 500 training examples using a pre-trained CLIP model (Radford et al., 2021). We then computed class-wise mean feature vectors and calculated pairwise cosine similarities between these representations.

Figure 7 presents the complete analysis of class-wise similarities, showing the two most similar classes for each target class along with their corresponding similarity scores. This analysis informed our experimental design for the ablation studies on data diversity, particularly in constructing the remaining dataset (\(D_{r}\)) for Case 1, where only the two most similar classes were included. In our ablation study 4.4, we specifically focused on three target classes (plane, bird, and dog) and their respective most similar classes when constructing the limited diversity scenario (Case 1).

### Qualitative Results

We provide qualitative results comparing generations from the unlearned models (Salun, ESD-u, RGD and the pretrained model SD using the retain prompts \(D_{r}\).

\begin{table}
\begin{tabular}{l||c c c c c c c||c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{6}{c||}{Nudity Removal} & \multicolumn{3}{c}{AS (\(\uparrow\))} \\ \cline{2-11}  & \multicolumn{2}{c||}{Female} & \multicolumn{1}{c}{\multirow{2}{*}{Bottocks}} & \multicolumn{1}{c}{Male} & \multicolumn{1}{c}{\multirow{2}{*}{Bets}} & \multicolumn{1}{c}{\multirow{2}{*}{Belly}} & \multicolumn{1}{c}{Male} & \multirow{2}{*}{Granitis} & \multicolumn{1}{c||}{Female} & \multirow{2}{*}{\(D_{r,\text{train}}\)} & \multirow{2}{*}{\(D_{r,\text{pad}}\)} \\  & \multicolumn{2}{c||}{Genitalic} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline SD v3 & 0 & 1 & 9 & 69 & 4 & 58 & 46 & 0.364 & 0.371 \\ \hline ESD & 0 & 0 & 2 & 10 & 0 & 4 & 6 & 0.335 & 0.332 \\ \hline Salun & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.079 & 0.088 \\ \hline RGD (Ours) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.362 & 0.370 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Comparison of nudity removal effectiveness and alignment scores across different methods on Stable Diffusion Model

\begin{table}
\begin{tabular}{l||c||c|c|c} \hline \hline \multirow{2}{*}{Methods} & \multirow{2}{*}{SD} & \multicolumn{3}{c}{RGD (Ours)} \\ \cline{3-5}  & & \(|D_{r}|=|D_{f}|=400\) & \(|D_{r}|=|D_{f}|=800\) & \(|D_{r}|=|D_{f}|=1200\) \\ \hline \(D_{r,\text{train}}\) & 0.357 & 0.336 (0.021) & 0.354 (0.003) & 0.352 (0.005) \\ \hline \(D_{r,\text{pad}}\) & 0.352 & 0.339 (0.013) & 0.350 (0.002) & 0.346 (0.006) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Alignment scores comparison with varying dataset sizesFigure 8: SD given the prompts from \(D_{r}\)

Figure 7: CIFAR10 class-wise feature similarity based on CLIP (Radford et al., 2021)

Figure 10: ESD-u given the prompts from \(D_{r}\)

Figure 9: Salun given the prompts from \(D_{r}\)

Figure 11: RGD (Ours) given the prompts from \(D_{r}\)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract and introduction, we provide the motivation, the current limitations in the existing work, and the contribution and brief evaluation results of our paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The authors discuss the sensitivity of hyperparameters and the importance of diversity level.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: In the main paper, we provide the core part of our proposed theory and assumptions, and also provide complete proof in the appendix.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all hyperparameters, datasets, architecture we used in both the main paper and appendix and the way of designing the forgetting and remaining datasets. We also provide the examples of each dataset in the appendix.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The paper provides open access to the data, and the authors will also release the code publicly.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all implementation details in the main paper and appendix.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide the average and standard deviation for CIFAR-10 experiments.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [NA] Justification: We provide the information on the computation resources in Appendix.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We conform with the NeurIPS Code of Ethics in every respect.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We provide a broader impact in our paper, and we don't have negative societal impacts.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We properly mask the harmful part in the figure for publication. Guidelines:
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We properly credited the original owners of assets used in the paper.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not introduce the new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not involve human subjects in our study.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not conduct experiments on individuals.