# Transformers are efficient hierarchical chemical graph learners

 Zihan Pengmei

Department of Chemistry

The University of Chicago

Chicago, IL 60637

&Zimu Li

Yau Mathematical Sciences Center

Tsinghua University

Beijing 100084, China

&Chih-chan Tien

Department of Computer Science

The University of Chicago

Chicago, IL 60637

&Risi Kondor

Department of Computer Science

The University of Chicago

Chicago, IL 60637

&Aaron R. Dinner

Department of Chemistry

The University of Chicago

Chicago, IL 60637

Correspondence author. Email: dinner@uchicago.edu

###### Abstract

Transformers, adapted from natural language processing, are emerging as a leading approach for graph representation learning. Contemporary graph transformers often treat nodes or edges as separate tokens. This approach leads to computational challenges for even moderately-sized graphs due to the quadratic scaling of self-attention complexity with token count. In this paper, we introduce SubFormer, a graph transformer that operates on subgraphs that aggregate information by a message-passing mechanism. This approach reduces the number of tokens and enhances learning long-range interactions. We demonstrate SubFormer on benchmarks for predicting molecular properties from chemical structures and show that it is competitive with state-of-the-art graph transformers at a fraction of the computational cost, with training times on the order of minutes on a consumer-grade graphics card. We interpret the attention weights in terms of chemical structures. We show that SubFormer exhibits limited over-smoothing and avoids over-squashing, which is prevalent in traditional graph neural networks.

## 1 Introduction

Many systems ranging from social networks to molecular structures involve interactions between discrete elements and thus can be described by graphs. It remains challenging to identify the features of these systems that best enable learning their properties from data. Manually choosing features is subjective, and the computational cost of kernel-based methods scales cubically with the size of the dataset; kernel methods are also hard to parallelize, in contrast to neural network methods [51]. Graph neural networks (GNNs), in which graph structures directly define model structures that are inherently sparse and, in turn, efficient, have emerged as attractive alternatives [16]. However, GNNs are not the only neural-network architectures that show promise for graph-structured data learning. Transformers [47] also appear to be able to learn graphs just as they are able to learn the semanticstructures of sentences by modulating the weights of a fully-connected architecture [11; 7; 56; 31; 43]. The stark contrast between these two architectures with respect to structural inductive bias calls for better understanding of how they encode graph-structured data and how they can be combined to advantage.

In the present paper, we consider the case of learning molecular properties from chemical structures. A challenge that arises in this case is that nodes (atoms or functional groups) separated by many edges (bonds) can interact owing to the delocalized nature of electronic structure and the arrangement of the atoms in space. Prevailing graph learning methods are based on message-passing (MP) neural networks (NNs) [16]. In each layer of an MPNN, nodes aggregate information from their nearest neighbors. Capturing long-range interactions requires repeated exchanges between neighbors and thus many MP layers. MPNNs with insufficient numbers of layers exhibit _under-reaching_, in which nodes remain oblivious to information associated with nodes beyond a certain number of hops [2; 46]. While this problem can be solved by stacking layers, as in relatively simple MPNNs like GCN [28] and GAT [48], this introduces two other issues: _over-squashing_ and _over-smoothing_[2; 46; 8; 29]. The former refers to the insensitivity of a feature vector at a node to variations in feature vectors at distant nodes due to excessive compression of information by repeated MP. Over-smoothing refers to feature vectors across nodes becoming increasingly uniform after numerous MP iterations.

Graph transformers, with their ability to allow tokens of atom features to interact directly through a fully-connected structure, are posited to address the over-squashing issue. However, they are not without their own set of challenges. These include a computational cost that scales quadratically with the number of nodes [47] and persistent concerns about over-smoothing [9; 44]. Furthermore, the performance of graph representation learning models is generally assessed in terms of their ability to separate non-isomorphic graphs [54; 3], and, like most efficient models [37; 38], pure graph transformers cannot separate graphs that are indistinguishable under the 1-Weisfeiler-Lehman test (1-WL) [27], an algorithm based on color refinement of graph nodes [50].

In this work, we introduce the Subgraph Transformer (SubFormer), a novel molecular graph learning architecture that combines the strengths of MPNNs and transformers to address the challenges highlighted earlier. Central to our approach is the decomposition of molecular graphs into coarse-grained representations using hierarchical clustering methods. We adopt the concept of junction trees, as proposed in [23], which can be likened to a "molecular backbone", where each node represents a subgraph or cluster derived from the original graph. SubFormer operates in two distinct phases. Initially, the graph-level features of the molecule are locally aggregated using a shallow MPNN. This approach avoids both the over-squashing and over-smoothing problems associated with deeper MPNNs. Following this, the resulting coarse-grained features, representing substructures within the molecule, are passed through a standard transformer. The transformer's direct interactions between tokens eliminate the need for numerous iterations to access long-range interactions, effectively addressing both the under-reaching problem and the over-squashing problem. By clustering nodes within the graph, we achieve a computational cost reduction for the transformer, proportional to a power of the average node count per cluster. Notably, our framework offers an expressive power that surpasses the 1-WL test.

This paper is organized as follows. Section 2 provides background on MPNNs and transformers, and we discuss pertinent models, such as the graph transformer[47; 11; 40; 27] and the junction tree variational autoencoder [23]. In Section 3, we present the update rule of the Subgraph Transformer (SubFormer) and then experimental results; we show that SubFormer performs on par with leading graph transformers on standard graph benchmarks, including the ZINC [22], long-range graph benchmarks [12], and MoleculeNet datasets [52]. We summarize in Section 4. Theoretical analysis for the expressive power of SubFormer, further details and analysis of the benchmarks, and timing information are provided in the Appendices.

## 2 Background

**Message passing graph neural networks.** A graph \(G\) consists of a collection \(V(G)\) of \(n\) nodes and a collection \(E(G)\) of edges connecting selected pairs of nodes. It is conventional to express the graph by its adjacency matrix \(A\) with elements \(a_{ij}\), where \(a_{ij}=1\) if nodes \(i\) and \(j\) are connected, and \(a_{ij}=0\), otherwise. Let \(D\) denote the diagonal matrix where \(d_{ii}\) is the number of nodes connected to \(i\). Then the normalized graph Laplacian is defined as \(L=I-\sqrt{D^{-1}}A\sqrt{D^{-1}}\)[11; 27]. Wetake its eigenvectors as the positional encoding of the transformer that we introduce later. Let \(\{x_{i}\}_{i\in V(G)},\{y_{ij}\}_{(i,j)\in E(G)}\) be initial node and edge features. Then a MPNN [16] updates features according to the following recursion formulas:

\[x_{i}^{(0)}=x_{i};\quad x_{i}^{(l+1)}=\Psi\Big{(}x_{i}^{(l)},\text{AGG}_{j\in N (i)}\Phi(x_{i}^{(l)},x_{j}^{(l)},y_{ij})\Big{)}, \tag{1}\]

where AGG is a function that aggregates node and edge features in the neighborhood \(N(i)\) of node \(i\) by summation and \(\Psi\) and \(\Phi\) are trainable functions implemented as neural-network layers.

**Graph transformers.** A transformer layer [47] is mathematically represented by a parameterized function \(f^{(l)}:\mathbb{R}^{m\times d}\rightarrow\mathbb{R}^{m\times d}\) with \(m\) tokens \(z_{i}\in\mathbb{R}^{d}\), where \(d\) is the dimension of the feature vector \(x_{i}\). The tokens \(z_{i}\) are updated through multi-head self-attention (MHSA) at the \(l\)-th layer:

\[a_{h,ij}^{(l)}=\text{softmax}\Big{(}\frac{\langle Q_{h}^{(l)}(z_{i}^{(l)}),K_ {h}^{(l)}(z_{j}^{(l)})\rangle}{\sqrt{k}}\Big{)};\quad u_{i}^{(l)}=\sum_{h=1}^{ H}W_{h}^{(l)}\sum_{j=1}^{n}a_{h,ij}^{(l)}V_{h}^{(l)}(z_{j}^{(l)}), \tag{2}\]

where \(Q_{h}^{(l)}\), \(K_{h}^{(l)}\), and \(V_{h}^{(l)}\in\mathbb{R}^{k\times d}\) are the query, key and value matrices, and \(W_{h}^{(l)}\in\mathbb{R}^{d\times k}\) is a weight matrix per head. The head number is \(H=n/k\), and the softmax function is used to produce a sequence of discrete probability distributions known as attention weights, \(a_{h,ij}\). The output of the MHSA is normalized [34; 53] and passed through a fully connected layer, the output of which is again normalized:

\[w_{i}^{(l)}=\text{LayerNorm}(z_{i}^{(l)}+u_{i}^{(l)});\quad z_{i}^{(l+1)}=\text {LayerNorm}(w_{i}^{(l)}+W_{2}\sigma(W_{1}w_{i}^{(l)})), \tag{3}\]

where \(\sigma\) is a non-linear activation function such as ReLU.

Graph transformers [11; 31; 56; 43; 27; 58] adapt the transformer architecture to studies of graphs by incorporating positional or structural encoding as a soft inductive bias. For instance, one could either add or concatenate the Laplacian eigenvectors to the initial tokens [11; 31; 49; 27], or one could use the shortest path between nodes \(i\) and \(j\) to bias the self-attention weights \(a_{h,ij}\)[56; 59]. In this paper, we add the eigenvectors of the graph Laplacian and the shortest path matrix to tokens without making any further modifications to the MHSA in a standard transformer architecture.

**Junction tree variational autoencoders.** The framework that we describe below employs both GNN and transformer architectures to study local structure of the original graph as well as long-range interactions through a hierarchically coarse-grained graph. We decompose molecular graphs based on chemically meaningful local structures like rings and bonds following the strategy in [23]. To be specific, given a molecular graph \(G\), its junction tree is crafted as follows. \((1)\) We search all rings and bonds (edges) which are not contained in a ring from the graph and record them as clusters \(C_{i}\). \((2)\) If an atom belongs to more than two clusters, we remove it from the existing clusters and make a new cluster containing only that atom. \((3)\) We draw a new graph \(G^{\prime}\) with the \(C_{i}\) as nodes. Two clusters \(C_{i}\) and \(C_{j}\) are joined by an edge if their intersection is nonzero in \(G\). \((4)\) We take a maximal spanning tree \(T\) of \(G^{\prime}\). Some examples are presented in Figs. 1, 2, 8, and 7.

## 3 Results

**SubFormer approach.** To deal with the aforementioned issues, we propose Subgraph Transformer (SubFormer). SubFormer consists of two parts. The first is characterized by the subgraph-to-token routine, which employs a few MP layers to gather local information and compress it into a coarse-grained graph using a graph decomposition scheme. We follow [15] and use the junction tree autoencoder [23] mentioned above. To be specific, suppose \(U=(u_{\mu i}),\mu,i=1,...,m\) is a matrix whose rows are eigenvectors of either Laplacian or shortest path distance matrix, where the Greek indices \(\mu\) represent eigenvalues. We first take column vectors \(a_{i}^{\text{PE}}=(u_{i\mu})\), and then let \(X\in\mathbb{R}^{n\times d}\) denote the feature matrix of the molecular graph \(G\) and let \(Z\in\mathbb{R}^{m\times d}\) denote the feature matrix of the junction tree \(T\). We initialize node features by concatenating \(x_{i}\) from \(X\) for each node \(i\) with the positional encoding \(a_{i}^{\text{PE}}\) and then applying a weight matrix \(W_{1}\):

\[x_{i}^{(0)}=W_{1}[x_{i},a_{i}^{\text{PE}}]\quad\text{for}\quad i=1,...,n. \tag{4}\]

As a caveat, \(a_{i}^{\text{PE}}\) consists of the \(i\)th components from all eigenvectors we taken from SVD to ensure the permutation equivariance. Moreover, since SVD cannot be unique due to eigenvector sign flips or repeated eigenvalues, we follow [30] by adding weight matrix \(W_{1}\) which could possibly learn proper linear combinations of eigenvectors during the training. We also apply SignNet proposed in [35], which input both \(\pm a_{i}^{\text{PE}}\) through one non-linear activation to cancel out the sign flip freedom.

We then compress the local information of \(G\) into \(T\) using the aforementioned local message-passing mechanism with a fully-connected layer as follows:

\[X^{(l)^{\prime}} =\text{MPNN}(X^{(l)});\] \[X^{(l+1)} =X^{(l)^{\prime}}+\theta_{1}\sigma(S^{T}Z^{(l)}W_{2}^{(l)}); \tag{5}\] \[Z^{(l+1)} =Z^{(l)}+\theta_{2}\sigma(SX^{(l+1)}W_{3}^{(l)}),\]

where \(\theta_{i}\) and \(W_{i}\) are trainable weights, and we denote by \(S\in\{0,1\}^{|V(T)|\times|V(G)|}=\{0,1\}^{m\times n}\) the matrix that assigns nodes from \(G\) to clusters and thus nodes of \(T\). Updated node features \(X_{\text{out}}=X^{(L)}\) are added together to obtain \(x_{\text{out}}\). Then we tokenize \(Z_{\text{out}}=Z^{(L)}\) composed of vectors \(z_{i}\) representing node features by applying a weight matrix \(W_{4}\) to its concatenation with the positional encoding \(b_{i}^{\text{PE}}\) of the coarse-grained tree analogous to Eq.(4) to preserve permutation equivariance on the tree:

\[z_{i}^{(0)}=W_{4}[z_{i},b_{i}^{\text{PE}}]\quad\text{for}\quad i=1,...,m. \tag{6}\]

It is also a common option to attach a classification token \(z_{0}^{(0)}\) of learnable parameters [10; 57] ([CLS] in Fig. 1). After training a standard Transformer of \(L\) layers to learn subgraph-level information, we read out the class token concatenated with \(x_{\text{out}}\) optionally as

\[z_{\text{out}}=[z_{0}^{(L)},x_{\text{out}}]. \tag{7}\]

The advantages of our SubFormer architecture are discussed in conjunction with our experimental results below. We show that SubFormer is more powerful than the \(1\)-WL algorithm for the graph isomorphism problem [50; 39; 26] in Appendix A.

**Predicting molecular properties.** We first learn and predict octanol-water partition coefficients using the ZINC dataset [22]. The graphs are defined by the non-hydrogen atoms (nodes) and their bonds (edges); the node features are the atom types, and the edge features are the bond types. Our results are competitive with those of a state-of-the-art graph transformer, GraphGPS, and surpass those of other graph transformers (Table 1). We also obtain results comparable to GraphGPS (as well as GT and SAN) for the Peptides-struct benchmark, a long-range graph benchmark [12] (Table 2).

The MoleculeNet datasets target predictions of molecular properties such as toxicity, solubility, and bioactivity [52]. We split the dataset and evaluate performance following [52]. SubFormer exhibits strong performance across these diverse tasks. Notably, a dual readout approach [15], which integrates information from both the original graph and the coarse-grained tree, improved prediction accuracy in most (but not all) cases (Tables 1 and 3).

Figure 1: Architectures. (A) Example molecule, its graph Laplacian, and the SubFormer architecture with dual readout from hierarchical graphs. \(\phi_{i}\) denotes the eigenvectors of the graph Laplacian or shortest path matrix. [CLS] refers to an classification token used in transformer encoders for information aggregation. (B) Illustration of the virtual node trick (left) and graph coarse-graining (right). The virtual node decreases the shortest path between any two nodes to two hops at most.

**Long-range interactions of molecular clusters.** The SubFormer architecture is designed to capture molecular properties that depend on both local and non-local interactions among atoms in a molecule. A widely adopted strategy to capture long-range interactions is to add a virtual node connected to all original nodes in the graph [16; 20]. In the example in Figure 1B, five MP layers are needed to transmit information from the rightmost node to the leftmost node in the absence of the virtual node; the introduction of the virtual node acts as a shortcut, effectively reducing the distance between any pair of nodes to two hops at most. SubFormer obviates this strategy. First, coarsening the graph into a junction tree reduces the number of hops between nodes, especially for molecules that contain multiple rings. Then the self-attention mechanism allows tokens of atom clusters to interact even when they are not directly connected in the coarse-grained tree. As indicated in Tables 1 and 3, adding a virtual node (VN) to the graph does not benefit the SubFormer approach.

**SubFormer attention weights correspond to chemically meaningful fragments.** To gain insight into how SubFormer functions, we plot the coarse-grained attention weights for representative molecules of the ZINC dataset in Figs. 2 and 7; the colored coarse-grained tree and molecular graphs

\begin{table}
\begin{tabular}{c c} \hline \hline
**Model** & **MAE** \\ \hline GCN & 0.367\(\pm\)0.011 \\ GAT & 0.384\(\pm\)0.007 \\ GIN & 0.408\(\pm\)0.008 \\ HIMP & 0.151\(\pm\)0.006 \\ AUTOBAHN & 0.106\(\pm\)0.004 \\ GT & 0.226\(\pm\)0.014 \\ SAN & 0.139\(\pm\)0.006 \\ Graphormer & 0.122\(\pm\)0.006 \\ GraphGPS & **0.070\(\pm\)0.004** \\ \hline Ours(580k) & 0.094\(\pm\)0.003 \\ Ours(dual readout,slim,200k) & 0.084\(\pm\)0.004 \\ Ours(dual readout,VN, 567k) & 0.078\(\pm\)0.003 \\ Ours(dual readout,567k) & 0.077\(\pm\)0.003 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results for the ZINC dataset. Lower values are better. Scores for published architectures are taken from [43; 45; 15; 56; 30].

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & TOX21 & TOXCAST & MUV & MOLHIV \\ \hline Num. Task & 12 & 617 & 17 & 1 \\  Metic & ROC-AUC & ROC-AUC & AP & ROC-AUC \\ \hline Random Forest & 0.769\(\pm\)0.015 & N/A & N/A & 0.781\(\pm\)0.006 \\ XGBoost & 0.794\(\pm\)0.014 & 0.640\(\pm\)0.005 & 0.086\(\pm\)0.033 & 0.756\(\pm\)0.000 \\ Kernel SVM & 0.822\(\pm\)0.006 & 0.669\(\pm\)0.014 & 0.137\(\pm\)0.033 & 0.792\(\pm\)0.000 \\ Logistic Regression & 0.794\(\pm\)0.015 & 0.605 \(\pm\) 0.003 & 0.070\(\pm\)0.009 & 0.702\(\pm\)0.018 \\ GCN & 0.840\(\pm\)0.004 & 0.735\(\pm\)0.002 & 0.114\(\pm\)0.029 & 0.761\(\pm\)0.010 \\ GIN & 0.850\(\pm\)0.009 & 0.741\(\pm\)0.004 & 0.091\(\pm\)0.033 & 0.756\(\pm\)0.014 \\ HIMP & **0.874\(\pm\)0.005** & 0.721\(\pm\)0.004 & 0.114\(\pm\)0.041 & 0.788\(\pm\)0.080 \\ AUTOBAHN & N/A & N/A & 0.119\(\pm\)0.005 & 0.780\(\pm\)0.015 \\ ChemRL-GEM & 0.849\(\pm\)0.003 & 0.742\(\pm\)0.004 & N/A & N/A \\ GraphGPS & 0.849* & 0.719* & 0.087(0.133)* & 0.788\(\pm\)0.010 \\ \hline Ours & 0.841\(\pm\)0.003 & 0.733\(\pm\)0.001 & 0.143\(\pm\)0.026 & **0.795\(\pm\)0.008** \\ Ours(dual readout,VN) & 0.844\(\pm\)0.006 & 0.744\(\pm\)0.010 & 0.160\(\pm\)0.014 & 0.781\(\pm\)0.010 \\ Ours(dual readout) & 0.851\(\pm\)0.008 & **0.752\(\pm\)0.003** & **0.182\(\pm\)0.019** & 0.756\(\pm\)0.007 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results for the Peptides-struct dataset of long-range graph benchmarks. Lower values are better. Scores for published architectures are taken from[43].

show the attention weights learned by the classification token from the last layer. There is a clear correspondence between the attention weights and the chemical structures. In particular, we see that aromatic rings generally have high attention weights. The sparsity of high attention weights for the classification token indicates the model's ability to focus on specific molecular fragments.

To ensure these results were not specific to the ZINC benchmark, we also trained SubFormer to predict the energy gap of frontier orbitals generated by density functional theory using the organic donor-acceptor molecules dataset of the computational materials repository [33]. In this dataset, the donors are typically conjugated and aromatic systems, while the acceptors are typically highly electronegative functional groups, such as ones containing fluorine. SubFormer achieves chemical accuracy of 0.07 eV on the hold-out test set, which is less than the typical error of the density functional theory calculations [36]. In this case, the SubFormer attention weights often correspond to molecular fragments participating in charge-transfer, as illustrated in Fig. 2. An additional example of SubFormer capturing long-range charge-transfer in a large molecular system is depicted in Fig. 8. Such long-range interactions are very challenging for conventional MPNNs to capture.

In Figs. 2, 7 and 8, we also present attention maps averaged across the attention heads. While the self-attention mechanism centers on particular nodes, the averaged attention maps become progressively flatter (note the varying scales of the heatmaps). This observation prompted us to further probe SubFormer's over-smoothing behavior.

**Over-smoothing and over-squashing in SubFormer: Analysis of the ZINC dataset.** Graph neural networks, while powerful, can exhibit over-smoothing, especially when deeper architectures are employed. Here, we explore the behavior of SubFormer in this regard. We specifically used SubFormer(Slim) with the dual output mechanism trained on the ZINC dataset (Table 1). With fixed feature dimension, we gradually increase the depth of SubFormer(slim) by stacking additional encoder layers. Usually, a performance increase would be expected as more parameters are introduced. However, as illustrated by Fig. 3, the accuracy clearly diminishes beyond three transformer encoder layers, which we attribute to over-smoothing based on the analysis above. To verify that this is indeed the case, we use the Dirichlet energy:

\[\mathcal{E}(X^{(l)})=\frac{1}{n}\sum_{i\in V(G)}\sum_{j\in N(i)}\|X^{(l)}_{i}- X^{(l)}_{j}\|_{2}^{2}, \tag{8}\]

where \(X^{(l)}\) represents the input to the \(l\)-th layer. Because \(\mathcal{E}(X^{(l)})\to 0\) if and only if the node features are uniform [29], this metric can be used to quantify over-smoothing.

We see in Fig. 3 that the Dirichlet energy decreases for SubFormer as the layer index increases, although the change is much more gradual than for GAT. By contrast, GraphGPS maintains a consistently high Dirichlet energy, indicating that it does not suffer from over-smoothing. GraphGPS achieves this behavior by interleaving MP and self-attention. In effect, GraphGPS uses two distinct adjacency matrices: its MP component utilizes the graph adjacency matrix to aggregate information from nearest neighbors, while its self-attention component exchanges information in a fully-connected fashion. This interplay suppresses over-smoothing. However, interleaving MP and self-attention is difficult to justify mathematically and complicates interpretation [1, 43].

As noted previously, over-squashing is another potential issue which arises in MPNNs applied to graphs with long-range interactions. To assess the significance of this issue for chemical applications, we plot the distribution of shortest path lengths to a reference node (the starting node selected by the force-directed graph drawing algorithm as implemented in the NetworkX package [18]) for several datasets in Fig. 6. We see that paths with 15 or more hops frequently arise, necessitating a comparable number of MP layers, which should lead to over-squashing.

We quantify over-squashing by computing the Jacobian \(\partial x^{(l)}_{i}/\partial x_{j}\), where \(x_{j}=x^{(0)}_{j}\) is the initial feature of a distant node \(j\) relative to \(i\). Tending to zero means the input of node \(j\) loses its influence on node \(i\), and we can attribute values close to zero to over-squashing when under-reaching is not an issue due to the number of MP layers [46, 8].

To illustrate this phenomenon, we picked a subset of molecules from the ZINC dataset and computed the Jacobian for GAT and SubFormer. For the latter, we did not read the classification token to be consistent with GAT. We furthermore did not use the dual readout to prevent the model from accessing the original graph information directly. Instead, we mapped the coarse-grained feature matrices back to the original graph. Then, we computed the Jacobian of the feature vector of the reference nodesFigure 2: Attention maps (heat maps) for representative molecules from (A) the ZINC dataset (B) the organic donor-acceptor dataset (for clarity, aromatic systems are labeled with dashed lines). Node 0 is the classification token. The molecular graphs and trees with corresponding scale bars show the attention weights learned by the classification token.

used above with respect to all input features and took the norm. We show representative results in the Fig. 4. We color the nodes with a gradient norm less than 0.05 dark gray. As described in section 2, GAT exhibits over-squashing in larger graphs with branches, while SubFormer does not.

## 4 Conclusions

Efficient graph learning paradigms have been a focal point for both the machine learning and chemistry communities. In this study, we introduced SubFormer, which combines hierarchical clustering of nodes by tree decomposition and MP with a transformer architecture for learning both local and global information in graphs. Our approach is motivated by the longstanding practice of interpreting chemical structures in terms of functional fragments. Here we showed that decomposition of graphs into fragments is not just useful for interpretation but also addresses graph representation learning challenges and reduces computational cost. SubFormer performed consistently well on standard benchmarks for predicting molecular properties from chemical structures. Visualization of the attention maps suggests that the model learns chemically pertinent features. We showed that SubFormer does not suffer from over-squashing because the attention weights obviate extensive MP to capture long-range interactions. Over-smoothing, while better controlled than in some graph transformers, remains an issue, demanding further study. Nevertheless, our results demonstrate that standard transformers, when equipped with local information aggregated via message-passing, excel as hierarchical graph learners.

Figure 4: Norm of the Jacobian with respect to the input features. Nodes with gradient norm less than 0.05 are highlighted dark gray.

Figure 3: Increasing the number of layers leads to over-smoothing. (A) Dependence of performance on the number of transformer encoder layers for SubFormer(slim) applied to the ZINC dataset. (B) Comparison of the Dirichlet energy at different encoder layers among SubFormer, GraphGPS, and GAT applied to the ZINC dataset.

## References

* [1] Mohammad Sadegh Akhondzadeh, Vijay Lingam, and Aleksandar Bojchevski. Probing graph representations. In _International Conference on Artificial Intelligence and Statistics_, pages 11630-11649. PMLR, 2023.
* [2] Uri Alon and Eran Yahav. On the Bottleneck of Graph Neural Networks and its Practical Implications. _arXiv e-prints_, art. arXiv:2006.05205, June 2020. doi: 10.48550/arXiv.2006.05205.
* [3] Waiss Azizian and Marc Lelarge. Expressive Power of Invariant and Equivariant Graph Neural Networks. _arXiv e-prints_, art. arXiv:2006.15646, June 2020. doi: 10.48550/arXiv.2006.15646.
* [4] Laszlo Babai. _Lectures on graph isomorphism_. Mimeographed lecture notes, 1977.
* [5] Laszlo Babai, Paul Erdos, and Stanley M. Selkow. Random graph isomorphism. _SIAM Journal on Computing_, 9(3):628-635, 1980. doi: 10.1137/0209047. URL [https://doi.org/10.1137/0209047](https://doi.org/10.1137/0209047).
* [6] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? _arXiv preprint arXiv:2105.14491_, 2021.
* [7] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking Attention with Performers. _arXiv e-prints_, art. arXiv:2009.14794, September 2020. doi: 10.48550/arXiv.2009.14794.
* [8] Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio', and Michael Bronstein. On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology. _arXiv e-prints_, art. arXiv:2302.02941, February 2023. doi: 10.48550/arXiv.2302.02941.
* [9] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is Not All You Need: Pure Attention Loss Rank Doubly Exponentially with Depth. _arXiv e-prints_, art. arXiv:2103.03404, March 2021. doi: 10.48550/arXiv.2103.03404.
* [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. _arXiv e-prints_, art. arXiv:2010.11929, October 2020. doi: 10.48550/arXiv.2010.11929.
* [11] Vijay Prakash Dwivedi and Xavier Bresson. A Generalization of Transformer Networks to Graphs. _arXiv e-prints_, art. arXiv:2012.09699, December 2020. doi: 10.48550/arXiv.2012.09699.
* [12] Vijay Prakash Dwivedi, Ladislav Ramgasek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini. Long range graph benchmark. _Advances in Neural Information Processing Systems_, 35:22326-22340, 2022.
* [13] Xiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, and Haifeng Wang. Geometry-enhanced molecular representation learning for property prediction. _Nature Machine Intelligence_, 4(2):127-134, 2022.
* [14] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In _ICLR Workshop on Representation Learning on Graphs and Manifolds_, 2019.
* [15] Matthias Fey, Jan-Gin Yuen, and Frank Weichert. Hierarchical Inter-Message Passing for Learning on Molecular Graphs. _arXiv e-prints_, art. arXiv:2006.12179, June 2020. doi: 10.48550/arXiv.2006.12179.
* [16] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International conference on machine learning_, pages 1263-1272. PMLR, 2017.

* Gravina et al. [2022] Alessio Gravina, Davide Bacciu, and Claudio Gallicchio. Anti-symmetric dgn: A stable architecture for deep graph networks. _arXiv preprint arXiv:2210.09789_, 2022.
* 15, Pasadena, CA USA, 2008.
* Hendrycks and Gimpel [2016] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_, 2016.
* Hu et al. [2020] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _Advances in neural information processing systems_, 33:22118-22133, 2020.
* Immerman and Lander [1990] Neil Immerman and Eric Lander. _Describing Graphs: A First-Order Approach to Graph Canonization_, pages 59-81. Springer New York, New York, NY, 1990. doi: 10.1007/978-1-4612-4478-3_5. URL [https://doi.org/10.1007/978-1-4612-4478-3_5](https://doi.org/10.1007/978-1-4612-4478-3_5).
* Irwin et al. [2012] John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc: a free tool to discover chemistry for biology. _Journal of chemical information and modeling_, 52(7):1757-1768, 2012.
* Jin et al. [2018] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction Tree Variational Autoencoder for Molecular Graph Generation. _arXiv e-prints_, art. arXiv:1802.04364, February 2018. doi: 10.48550/arXiv.1802.04364.
* Karp [1972] Richard M. Karp. _Reducibility among Combinatorial Problems_, pages 85-103. Springer US, Boston, MA, 1972. doi: 10.1007/978-1-4684-2001-2_9. URL [https://doi.org/10.1007/978-1-4684-2001-2_9](https://doi.org/10.1007/978-1-4684-2001-2_9).
* Katharopoulos et al. [2020] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In _International conference on machine learning_, pages 5156-5165. PMLR, 2020.
* Kiefer [2020] Sandra Kiefer. _Power and limits of the Weisfeiler-Leman algorithm_. PhD thesis, RWTH Aachen University, 2020. URL [https://publications.rwth-aachen.de/record/785831](https://publications.rwth-aachen.de/record/785831).
* Kim et al. [2022] Jinwoo Kim, Tien Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon Hong. Pure Transformers are Powerful Graph Learners. _arXiv e-prints_, art. arXiv:2207.02505, July 2022. doi: 10.48550/arXiv.2207.02505.
* Kipf and Welling [2016] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* Rusch et al. [2023] T. Konstantin Rusch, Michael M. Bronstein, and Siddhartha Mishra. A Survey on Oversmoothing in Graph Neural Networks. _arXiv e-prints_, art. arXiv:2303.10993, March 2023. doi: 10.48550/arXiv.2303.10993.
* Kreuzer et al. [2021] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent Letourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. _Advances in Neural Information Processing Systems_, 34:21618-21629, 2021.
* Kreuzer et al. [2021] Devin Kreuzer, Dominique Beaini, William L. Hamilton, Vincent Letourneau, and Prudencio Tossou. Rethinking Graph Transformers with Spectral Attention. _arXiv e-prints_, art. arXiv:2106.03893, June 2021. doi: 10.48550/arXiv.2106.03893.
* Kucera [1987] Ludek Kucera. Canonical labeling of regular graphs in linear average time. In _28th Annual Symposium on Foundations of Computer Science (sfcs 1987)_, pages 271-279, 1987. doi: 10.1109/SFCS.1987.11.
* Landis et al. [2012] David D Landis, Jens S Hummelshoj, Svetlozar Nestorov, Jeff Greeley, Marcin Dulak, Thomas Bligaard, Jens K Norskov, and Karsten W Jacobsen. The computational materials repository. _Computing in Science & Engineering_, 14(6):51-57, 2012.

* Ba et al. [2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. _arXiv e-prints_, art. arXiv:1607.06450, July 2016. doi: 10.48550/arXiv.1607.06450.
* Lim et al. [2022] Derek Lim, Joshua Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, and Stefanie Jegelka. Sign and basis invariant networks for spectral graph representation learning. _arXiv preprint arXiv:2202.13013_, 2022.
* Mardirossian and Head-Gordon [2017] Narbe Mardirossian and Martin Head-Gordon. Thirty years of density functional theory in computational chemistry: an overview and extensive assessment of 200 density functionals. _Molecular physics_, 115(19):2315-2372, 2017.
* Maron et al. [2018] Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and Equivariant Graph Networks. _arXiv e-prints_, art. arXiv:1812.09902, December 2018. doi: 10.48550/arXiv.1812.09902.
* Maron et al. [2019] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably Powerful Graph Networks. _arXiv e-prints_, art. arXiv:1905.11136, May 2019. doi: 10.48550/arXiv.1905.11136.
* Morgan [1965] H. L. Morgan. The Generation of a Unique Machine Description for Chemical Structures-A Technique Developed at Chemical Abstracts Service. _Journal of Chemical Documentation_, 5(2):107-113, May 1965. ISSN 0021-9576, 1541-5732. doi: 10.1021/c160017a018. URL [https://pubs.acs.org/doi/abs/10.1021/c160017a018](https://pubs.acs.org/doi/abs/10.1021/c160017a018).
* Park et al. [2022] Wonpyo Park, Woonggi Chang, Donggeon Lee, Juntae Kim, and Seung-won Hwang. Grpe: Relative positional encoding for graph transformer. _arXiv preprint arXiv:2201.12787_, 2022.
* Ramana et al. [1994] Motakuri V. Ramana, Edward R. Scheinerman, and Daniel Ullman. Fractional isomorphism of graphs. _Discrete Mathematics_, 132(1-3):247-265, September 1994. ISSN 0012365X. doi: 10.1016/0012-365X(94)90241-0. URL [https://linkinghub.elsevier.com/retrieve/pii/0012365X94902410](https://linkinghub.elsevier.com/retrieve/pii/0012365X94902410).
* Rampasek et al. [2022] Ladislav Rampasek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. _Advances in Neural Information Processing Systems_, 35:14501-14515, 2022.
* Rampasek et al. [2022] Ladislav Rampasek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a General, Powerful, Scalable Graph Transformer. _arXiv e-prints_, art. arXiv:2205.12454, May 2022. doi: 10.48550/arXiv.2205.12454.
* Shi et al. [2022] Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen M. S. Lee, and James T. Kwok. Revisiting Over-smoothing in BERT from the Perspective of Graph. _arXiv e-prints_, art. arXiv:2202.08625, February 2022. doi: 10.48550/arXiv.2202.08625.
* Thiede et al. [2021] Erik Thiede, Wenda Zhou, and Risi Kondor. Autobahn: Automorphism-based graph neural nets. _Advances in Neural Information Processing Systems_, 34:29922-29934, 2021.
* Topping et al. [2021] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M. Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. _arXiv e-prints_, art. arXiv:2111.14522, November 2021. doi: 10.48550/arXiv.2111.14522.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. _arXiv e-prints_, art. arXiv:1706.03762, June 2017. doi: 10.48550/arXiv.1706.03762.
* Velickovic et al. [2017] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. _arXiv preprint arXiv:1710.10903_, 2017.
* Wang et al. [2022] Haorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li. Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks. _arXiv e-prints_, art. arXiv:2203.00199, February 2022. doi: 10.48550/arXiv.2203.00199.
* Weisfeiler and Leman [1968] Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra which appears therein. _Nauchno-Technicheskaya Informatsiya._, 1968.

* [51] Christopher KI Williams and Carl Edward Rasmussen. _Gaussian processes for machine learning_, volume 2. MIT press Cambridge, MA, 2006.
* [52] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. _Chemical science_, 9(2):513-530, 2018.
* [53] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and Improving Layer Normalization. _arXiv e-prints_, art. arXiv:1911.07013, November 2019. doi: 10.48550/arXiv.1911.07013.
* [54] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are Graph Neural Networks? _arXiv e-prints_, art. arXiv:1810.00826, October 2018. doi: 10.48550/arXiv.1810.00826.
* [55] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _arXiv preprint arXiv:1810.00826_, 2018.
* [56] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do Transformers Really Perform Bad for Graph Representation? _arXiv e-prints_, art. arXiv:2106.05234, June 2021. doi: 10.48550/arXiv.2106.05234.
* [57] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling Vision Transformers. _arXiv e-prints_, art. arXiv:2106.04560, June 2021. doi: 10.48550/arXiv.2106.04560.
* [58] Haiteng Zhao, Shuming Ma, Dongdong Zhang, Zhi-Hong Deng, and Furu Wei. Are More Layers Beneficial to Graph Transformers? _arXiv e-prints_, art. arXiv:2303.00579, March 2023. doi: 10.48550/arXiv.2303.00579.
* [59] Wenhao Zhu, Tianyu Wen, Guojie Song, Liang Wang, and Bo Zheng. On Structural Expressive Power of Graph Transformers. _arXiv e-prints_, art. arXiv:2305.13987, May 2023. doi: 10.48550/arXiv.2305.13987.

[MISSING_PAGE_FAIL:13]

which avoids introducing more complicated tensor inputs. As a result, the separation power of our framework is limited by the method used to compress local structures of the original graph and we leave it to the future to further refine the decomposition strategy.

## Appendix B Datasets

Table 4 provides an overview of the benchmark datasets used in this study, with a primary focus on the ZINC [22] and MoleculeNet [52] datasets. For consistency, we employed a data division ratio of 8:1:1 for training, validation, and testing, as recommended by [52]. Notable exceptions are the ZINC, MOLHIV, and Peptides-struct datasets, which have predefined split specification. These predefined ratios were sourced from their implementations in the PyTorch Geometric package [14] and the Open Graph Benchmark package [20]. Additionally, we depend on these packages for loss computation and to access standard node and edge features.

## Appendix C Hyperparameters

The hyperparameters applied to all benchmark datasets in this study are detailed in Table 5. We did not perform a systematic search of hyperparameters due to limited computational resources. Notably, while the CosineAnnealingLR scheduler is frequently associated with the ZINC dataset, it was not employed in this research. Similarly, the GeLU activation function [19], often utilized for various transformers, was not incorporated. The local MP block, in its design, has the flexibility to integrate diverse MPNNs; however, this work specifically considered GINE [55] and the anti-symmetric variant [17] of GATv2 [6]. Tuning of hyperparameters could enhance performance.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline
**Dataset** & **\# Samples** & **\# Tasks** & **Task type** & **Metric** & **Split** & **\# Excluded** \\ \hline ZINC & 12,000 & 1 & Regression & MAE & Random & 0 \\ Peptides-Struct & 15,535 & 11 & Regression & MAE & Stratified & 0 \\ TOX21 & 7,831 & 12 & Classification & ROC-AUC & Random & 19 \\ TOXCAST & 8,575 & 617 & Classification & ROC-AUC & Random & 11 \\ MUV & 93,087 & 17 & Classification & AP & Random & 0 \\ MOLHIV & 41,127 & 1 & Classification & ROC-AUC & Scaffold & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Benchmark datasets used in the study. # Excluded is the number of samples that was excluded due to failure of the default tree decomposition scheme.

[MISSING_PAGE_EMPTY:15]

[MISSING_PAGE_FAIL:16]

Figure 7: Same as Fig. 2 for an additional molecule from the ZINC dataset

Figure 8: Same as Fig. 2 for an additional long-chain molecule from the organic donor-acceptor dataset