# Finding good policies in average-reward Markov Decision Processes without prior knowledge

 Adrienne Tuynman

adrienne.tuynman@inria.fr

&Remy Degenne

remy.degenne@inria.fr

Emilie Kaufmann

emilie.kaufmann@univ-lille.fr

Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189-CRIStAL, F-59000 Lille, France

###### Abstract

We revisit the identification of an \(\varepsilon\)-optimal policy in average-reward Markov Decision Processes (MDP). In such MDPs, two measures of complexity have appeared in the literature: the diameter, \(D\), and the optimal bias span, \(H\), which satisfy \(H\leq D\). Prior work have studied the complexity of \(\varepsilon\)-optimal policy identification only when a generative model is available. In this case, it is known that there exists an MDP with \(D\simeq H\) for which the sample complexity to output an \(\varepsilon\)-optimal policy is \(\Omega(SAD/\varepsilon^{2})\) where \(S\) and \(A\) are the sizes of the state and action spaces. Recently, an algorithm with a sample complexity of order \(SAH/\varepsilon^{2}\) has been proposed, but it requires the knowledge of \(H\). We first show that the sample complexity required to estimate \(H\) is not bounded by any function of \(S,A\) and \(H\), ruling out the possibility to easily make the previous algorithm agnostic to \(H\). By relying instead on a diameter estimation procedure, we propose the first algorithm for \((\varepsilon,\delta)\)-PAC policy identification that does not need any form of prior knowledge on the MDP. Its sample complexity scales in \(SAD/\varepsilon^{2}\) in the regime of small \(\varepsilon\), which is near-optimal. In the online setting, our first contribution is a lower bound which implies that a sample complexity polynomial in \(H\) cannot be achieved in this setting. Then, we propose an online algorithm with a sample complexity in \(SAD^{2}/\varepsilon^{2}\), as well as a novel approach based on a data-dependent stopping rule that we believe is promising to further reduce this bound.

## 1 Introduction

Reinforcement learning (RL) is a paradigm in which an agent interacts with its environment, modeled as a Markov Decision Process (MDP), by taking actions and observing rewards. Its goal is to learn, or to act according to, a good policy, that is a mapping from state to actions which maximizes cumulative rewards. However, there are different ways to define this notion of (expected) cumulative reward: in the _finite horizon_ setting, one should maximize the expected sum of rewards up to a certain fixed horizon; in the _discounted_ setting, each consecutive reward is \(\gamma\) times as important as the previous one, with \(0<\gamma<1\). In this paper, we focus on the _average reward_ setting, in which the value of a policy is measured by its asymptotic mean reward per time step. This setting is ideal for long-term learning, as there is no need to tune the horizon or discount parameters. However, the asymptotic nature of its optimality criterion makes the problem more complicated and highly sensitive to small changes in the MDP, which is less observable in the finite horizon or discounted settings.

Formally, a Markov Decision Processes is defined as a tuple (\(\mathcal{S}\),\(\mathcal{A}\),\(P\),\(r\)) where \(\mathcal{S}\) is the state space of finite size \(S\), \(\mathcal{A}\) is the action space of finite size \(A\). Letting \(\Sigma_{\mathcal{X}}\) denote the set of distributionover a set \(\mathcal{X}\), \(P:\mathcal{S}\times\mathcal{A}\rightarrow\Sigma_{\mathcal{S}}\) is the (assumed unknown) transition kernel, and \(r:\mathcal{S}\times\mathcal{A}\rightarrow\Sigma_{[0,1]}\) is the reward kernel. For each state-action pair \((s,a)\), we denote by \(\overline{r}_{s,a}=\mathds{E}[r(s,a)]\) the (assumed known) mean reward1. At each time step \(t\), the agent observes a state \(s_{t}\in\mathcal{S}\), takes an action \(a_{t}\), and observes a reward \(r_{t}\sim r(s_{t},a_{t})\) and a next state \(s_{t+1}\sim P(s_{t},a_{t})\). In an average-reward MDP (AR-MDP) the value of a policy \(\pi:\mathcal{S}\mapsto\Delta(\mathcal{A})\) is measured with its gain \(g_{\pi}(s)=\lim_{T\rightarrow+\infty}\frac{1}{T}\mathds{E}_{\pi}\left[\sum_{t= 0}^{T-1}r_{t}|s_{0}=s\right]\) where the expectation is taken when the agent follows policy \(\pi\) (i.e., \(s_{t}\sim\pi(a_{t})\)) from the initial state \(s\). In this paper, we consider weakly communicating MDPs (defined in Section 2) in which a policy \(\pi_{\star}\) maximizing the gain in all states exists and has a constant value, denoted by \(g_{\pi_{\star}}\).

Footnote 1: In most practical cases, the reward of the system are decided preemptively, and the uncertainty solely resides in the dynamics; moreover, estimating rewards is often easier than estimating the transition probabilities, and doing it can be done at little cost, therefore not changing our results much.

We are interested in the best policy identification problem: we want to build an algorithm that learns the MDP by taking actions and collecting observations, until it can, after some (possibly random) number of interactions \(\tau\) output a policy \(\widehat{\pi}\) that is near-optimal. More precisely, given two parameters \(\varepsilon\in(0,1)\) and \(\delta\in(0,1)\) we seek an \((\varepsilon,\delta)\)-Probably Approximately Correct (PAC) algorithm, that is an algorithm that satisfies

\[\mathbb{P}\left(\tau<\infty,\exists s\in\mathcal{S},g_{\pi_{\star}}-g_{ \widehat{\pi}}(s)>\varepsilon\right)\leq\delta\.\]

Our objective is to find such an algorithm that has minimal sample complexity, i.e., that requires the least amount of steps \(\tau\), with high probability or in expectation. Two different models can be considered for the collection of observations. In the online model, the algorithm can only choose the action \(a_{t}\) to sample at each time step, as the state \(s_{t}\) is determined by the MDP's dynamics and the previous actions. With a generative model however the agent can sample the reward and next state of any state action pair \((s_{t},a_{t})\), regardless of which state it arrived in at the previous time step.

To the best of our knowledge, prior work on \((\varepsilon,\delta)\)-PAC best policy identification in AR-MDPs has exclusively considered the generative model setting. In the online setting, the regret minimization objective has however been studied a lot (e.g., [9, 2, 3]). Existing sample complexity or regret bounds feature different notions of complexity of the MDP, besides its size \(S,A\). The diameter \(D\) and the optimal bias span \(H\), formally defined in the next section, are two such complexity notions that both feature in lower or upper bounds on the sample complexity of existing algorithms. More specifically for \((\varepsilon,\delta)\)-PAC policy identification, the work of [22] provides a worse-case lower bound showing that there exists an MDP on which any \((\varepsilon,\delta)\)-PAC algorithm using a generative model should have a sample complexity larger than \(\Omega((SAD/\varepsilon^{2})\log(1/\delta))\). The recent work of [27] provides an \((\varepsilon,\delta)\)-PAC algorithm that takes \(H\) as input and whose sample complexity is \(\widetilde{\mathcal{O}}\left((SAH/\varepsilon^{2})\log(1/\delta)\right)^{2}\). Using that \(H\leq D\), this algorithm is thus optimal, and the lower bound is also in \(\widetilde{\Omega}((SAH/\varepsilon^{2})\log(1/\delta))\). This raises the following question: can an algorithm attain the same optimal sample complexity without prior knowledge of \(H\)? More broadly, as detailed in the next section, all existing \((\varepsilon,\delta)\)-PAC algorithms require some form of prior knowledge on the MDP, and we propose the first algorithms that are agnostic to the MDP.

ContributionsIn the generative model setting, a first hope to get an algorithm agnostic to \(H\) is to plug-in a tight upper bound on this quantity in the algorithm of [27]. Our first contribution is a negative result: the number of samples necessary to estimate \(H\) within a prescribed accuracy is not polynomial in \(H\), \(S\) and \(A\). This result is proved in Section 3. On the positive side, by combining a procedure for estimating the diameter \(D\) inspired by [21] with the algorithm of [27] we propose in Section 4 Diameter Free Exploration (DFE), an algorithm for communicating MDPs that does not require any prior knowledge on the MDP and has a near-optimal \(\widetilde{\mathcal{O}}\left((SAD/\varepsilon^{2})\log(1/\delta)\right)\) sample complexity in the asymptotic regime of small \(\varepsilon\). Then in Section 5 we discuss the hardness of \((\varepsilon,\delta)\)-PAC policy identification in the online setting. We notably prove a lower bound showing that the sample complexity of \((\varepsilon,\delta)\)-PAC policy identification cannot always be polynomial in \(S\), \(A\) and \(H\), even with the knowledge of \(H\). On the algorithmic side, we propose in Section 6 an online variant of DFE whose sample complexity scales in \(SAD^{2}/\varepsilon^{2}\). As prior work, DFE hinges on a conversion between the discounted and average-reward settings [22] and uses uniform sampling. Departing from this approach, we further propose a novel data-dependent stopping rule tailored for AR-MDPs that can be used both in the generative model and the online setting. We prove that it is \((\varepsilon,\delta)\)-PAC for any (possibly adaptive) sampling rule and give preliminary sample complexity guarantees.

## 2 Related work

In order to position our work in the literature on best policy identification in average-reward MDP, we start by recalling some properties of average-reward MDPs and give a formal definition of the different complexity measures that have appeared in the literature.

First of all, it can be more or less easy to travel through the MDP and visit some states. We call an MDP weakly communicating if there exists a closed set of states that are all reachable from every other state (meaning \(\mathcal{S}^{\prime}\subset\mathcal{S}\) such that for any \(s,s^{\prime}\) in \(\mathcal{S}^{\prime}\), \(\min_{\pi:\mathcal{S}\rightarrow\mathcal{A}}\mathrm{E}[\min\{t,s_{t}=s^{ \prime}\}|s_{0}=s,\forall t^{\prime},a_{t^{\prime}}=\pi(s_{t^{\prime}})]<+\infty\)) and a possibly empty set of states which are transient (meaning the return time \(\min\{t,s_{t}=s\}\) when the chain starts with \(s_{0}=s\) is not almost surely finite) under any policy. Furthermore, when there is no such transient state, the MDP is then communicating, and it is possible to define the diameter of the MDP to quantify how fast circulating in the MDP can be:

\[D=\max_{s\neq s^{\prime}}\min_{\pi:\mathcal{S}\rightarrow\mathcal{A}}\mathrm{ E}[\min\{t>0,s_{t}=s^{\prime}\}|s_{0}=s,\forall t^{\prime},a_{t^{\prime}}=\pi(s _{t^{\prime}})]\,.\]

Finally, if the MDP satisfies for any policy \(\pi\) and states \(s,s^{\prime}\) that \(\mathrm{E}[\min\{t,s_{t}=s^{\prime}\}|s_{0}=s,\forall t^{\prime},a_{t^{\prime} }=\pi(s_{t^{\prime}})]<+\infty\), then the MDP is ergodic. If the MDP satisfies this except for a possibly empty set of transient states, the MDP is then called unichain. In the following, the MDPs are all considered at least weakly communicating, though some results shall require some stronger assumptions, which we will specify. Notably, every mention of the diameter will assume communicating MDPs.

The diameter has appeared in previous upper and lower bound on the regret [9] and in the lower bound of [22] for best policy identification. The diameter is also used in the literature on the Stochastic Shortest Path (SSP) problem, in which one seek to minimize the sum of cost obtained before some goal state is reached (some details on this alternative framework are given in Appendix B). The SSP-diameter is defined as the maximum over states of the minimum expected steps to the goal state, and it appears in the sample complexity bounds given by [21]. We will see in the following that there are multiple similarities between the SSP setting and our average-reward setting, the use of the diameter being only one of them.

For a policy \(\pi:\mathcal{S}\rightarrow\mathcal{A}\), we define the gain \(g_{\pi}(s)=\lim_{T\rightarrow+\infty}\frac{1}{T}\mathrm{E}\left[\sum_{t=0}^{T -1}r_{t}|s_{0}=s\right]\). By writing \(P_{\pi}=\left(P(s,\pi(s))\right)_{s}\), and introducing \(\overline{P_{\pi}}=\lim_{T\rightarrow+\infty}\frac{1}{T}\sum_{t=1}^{T}P_{\pi }^{t-1}\) the asymptotic distribution matrix, as well as defining \(\overline{r}_{\pi}(s)=\mathrm{E}[r(s,\pi(s))]\), we have \(g_{\pi}(s)=\overline{P_{\pi}}(s)\overline{r}_{\pi}\). Finally, we can define the bias vector \(b_{\pi}=\sum_{t=1}^{+\infty}\left(P_{\pi}^{t-1}-\overline{P_{\pi}}\right) \cdot\overline{r}_{\pi}\). Those vectors satisfy the so-called Poisson equations (see [18]):

\[\left\{\begin{array}{l}(I-P_{\pi})g_{\pi}=0\\ g_{\pi}+(I-P_{\pi})b_{\pi}=\overline{r}_{\pi}\end{array}\right. \tag{3}\]

We notice that the solution, \(b_{\pi}\), is defined up to an additive element in \(\mathrm{Ker}(I-P_{\pi})\). We can then define \(g^{\star}\) the optimal gain and (up to an additive constant) \(b^{\star}\) the optimal bias satisfying

\[g^{\star}+b^{\star}(s)=\max_{a}\left\{\overline{r}(s,a)+p_{s,a}b^{\star}\right\}, \tag{4}\]

where \(p_{s,a}\) is the (row) vector of transition probabilities from \((s,a)\). We further define

\[H=\max_{s}b^{\star}(s)-\min_{s}b^{\star}(s)\]

as the span of the optimal bias. The optimal bias span is always smaller than the diameter [2], which motivates a line of work replacing \(D\) with \(H\) in existing regret bounds [2; 7]. These improved bounds are however obtained only when \(H\) is known. Similarly existing sample complexity bounds featuring \(H\) all require this knowledge, as discussed in more detail below.

In addition to the diameter \(D\) and the optimal bias span \(H\), we define the mixing time

\[t^{\star}_{\text{mix}}=\max_{\pi}\inf\left\{t\geq 1:\max_{s}\left\|P_{\pi}^{t}(s )-\overline{P_{\pi}}(s)\right\|_{1}\leq\frac{1}{2}\right\}\]which has also appeared in previous sample complexity bounds. By default the maximum is taken over all stochastic policies \(\pi:\mathcal{S}\rightarrow\Sigma_{\mathcal{A}}\), and we denote by \(t^{\star}_{\text{mix,D}}\) the same quantity where the maximum is restricted to deterministic policies.

Multiple papers have considered the problem of finding an \(\varepsilon\)-optimal policy in average-reward with access to a generative model. We summarize existing results in Table 1. [12] and [22] derived worse case lower bounds for the problem by exhibiting MDPs on which a certain number of samples _must_ be collected to guarantee correctness of any \((\varepsilon,\delta)\)-PAC algorithm. The literature has first focused on the mixing time as a measure of the complexity of MDPs [23; 11; 12; 16], until the mixing time-scaling lower-bound for the sample complexity was matched by [24], for an algorithm taking \(t^{\star}_{\text{mix,}D}\) as input. Algorithms using the optimal bias span \(H\) were introduced more recently, by [22] and [26], and the lower bound scaling in \(H\) was matched up to logarithmic factors by [27]. This side of the literature has mostly used the links between discounted and average-reward MDPs and has used that for known \(H\) it is possible to choose a discount \(\gamma\) (with \(1-\gamma\) of order \(\varepsilon/H\)) such that \(H\)-optimal policies in the discounted MDP are \(\varepsilon\)-optimal in the average-reward MDP [22]. This idea leads to upper bounds of the sample complexity that scale with the (assumed known) upper bound on \(H\). To make these algorithms agnostic to the MDP, a natural question is therefore: can we find a tight upper bound on \(H\) from the data?

## 3 On the hardness of estimating \(H\)

In this section, we investigate the complexity of estimating the optimal bias span \(H\) and more specifically of finding upper bounds on this quantity.

**Definition 1**.: _We say an algorithm computes a \(\Delta\)-tight upper bound for the optimal bias span with probability \(1-\delta\) when, on any MDP of optimal bias \(H\), it outputs \(\hat{H}\) such that with probability higher than \(1-\delta\), \(H\leq\hat{H}\leq H+\Delta\)._

Theorem 1 shows that there exists an MDP on which the sample complexity of an algorithm finding a \(\Delta\)-tight upper bound with probability larger than \(1-\delta\) can be arbitrarily large. As a consequence, there cannot exist a bound on this sample complexity depending solely on \(S\), \(A\), \(H\), \(\delta\) and \(\Delta\).

\begin{table}
\begin{tabular}{c c c} \hline Algorithm & Bound & Setting \\ \hline
[23] & \(\widetilde{\mathcal{O}}\left(\frac{SA(t^{\star}_{\text{mix}})^{2}}{e^{2}}\log(1/ \delta)\right)\) & \(\frac{1}{\sqrt{\sigma S}}\leq\overline{P_{\pi}}(s)\leq\frac{\sqrt{\tau}}{S}\), \(\tau\) known \\  & \(t^{\star}_{\text{mix}}\) over all policies, known \\ \hline
[11] & \(\widetilde{\mathcal{O}}\left(\frac{SA(t^{\star}_{\text{mix}})^{2}}{e^{2}}\log( 1/\delta)\right)\) & \(t^{\star}_{\text{mix}}\), known \\ \hline
[12] & \(\widetilde{\mathcal{O}}\left(\frac{SAt^{\star}_{\text{mix,D}}}{e^{3}}\log(1/ \delta)\right)\) & \(t^{\star}_{\text{mix,D}}\), known \\ \hline
[16] & \(\widetilde{\mathcal{O}}\left(\frac{SA(t^{\star}_{\text{mix}})^{3}}{e^{2}}\log( 1/\delta)\right)\) & \(t^{\star}_{\text{mix}}\), known (policy gradient) \\ \hline
[24] & \(\widetilde{\mathcal{O}}\left(\frac{SAt^{\star}_{\text{mix,D}}}{e^{2}}\log(1/ \delta)\right)\) & \(t^{\star}_{\text{mix,D}}\), known \\ \hline
[22] & \(\widetilde{\mathcal{O}}\left(\frac{SAH}{e^{3}}\log(1/\delta)\right)\) & \(H\) known \\ \hline
[26] & \(\widetilde{\mathcal{O}}\left(\left[\frac{SAH^{2}}{e^{2}}+\frac{S^{2}AH}{e} \right]\log(1/\delta)\right)\) & \(H\) known \\ \hline
[27] & \(\widetilde{\mathcal{O}}\left(\frac{SAH}{e^{2}}\right)\) & \(H\) known \\ \hline \hline This paper & \(\widetilde{\mathcal{O}}\left(\left[\frac{SAD}{e^{2}}+S^{2}AD^{2}\right]\log(1/ \delta)\right)\) & no prior knowledge \\ \hline \hline
[12] & \(\Omega\left(\frac{SAt^{\star}_{\text{mix}}}{e^{2}}\right)\) & Lower bound (worse case) \\ \hline
[22] & \(\Omega\left(\frac{SAD}{e^{2}}\log(1/\delta)\right)\) & Lower bound (worse case) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Existing sample complexity bounds in the generative model setting 

**Theorem 1**.: _For any \(\delta<\frac{1}{2^{\epsilon^{4}}}\), \(T>0\), \(\Delta\), there exists an ergodic MDP \(\mathcal{M}\) with optimal bias span \(H=1/2\), \(S=3\) and \(A=2\) such that any algorithm that computes a \(\Delta\)-tight upper bound for the optimal bias span with probability \(1-\delta\) under a generative model assumption needs (in expectation) more than \(T\) samples in \(\mathcal{M}\)._

Sketch of proofTo give an idea of the proof, we first focus on the weakly-communicating setting, with the full proof and the necessary ergodicity transformation detailed in Appendix A. We use the family of MDPs \((\mathcal{M}_{R})_{0<R<1}\) displayed in Figure 1. Indeed, having \(R<1/2\) makes the full-line policy optimal; since moving from one state to the optimal final state (1) is very easy in this case, no state is drastically worse than another, hence a small optimal bias span, \(H=1/2\). However, \(R>1/2\) makes the dashed-line policy optimal; moving from state 3 to state 2 is more difficult as \(p\) is small, taking on average \(1/p\) time steps, and state 3 has a worse reward than state 2; thus state 3 is considered way worse than state 2, hence a high optimal bias span, \(H=\frac{1+p}{p}R\). Thus, to bound \(H\) tightly, one needs to know whether \(R>1/2\) or \(R<1/2\), which can require many samples if \(R\) is very close to \(1/2\).

We remark that since the hard MDP instance in our proof has a fixed optimal bias span \(1/2\), we would have the same problem if we looked for algorithms trying to find \(\hat{H}\) such that, with high probability, \(H\leq\hat{H}\leq(1+\Delta)H\). Moreover, instead of looking for upper bound we could also consider the task of estimating \(H\) within a given margin of error and the same issue would arise.

While Theorem 1 does not preclude the existence of an algorithm that is agnostic to \(H\) and reaches a \(\widetilde{\mathcal{O}}((SAH/\varepsilon^{2})\log(1/\delta))\) sample complexity, it still suggests that assuming a known upper bound on \(H\) is a lot to ask. In the next section we will see that for communicating MDPs such an assumption is not needed to attain a near-optimal sample complexity.

## 4 A near-optimal algorithm without prior knowledge

As we have seen, finding tight upper bounds on \(H\) is not feasible in finite time. In the literature on regret minimization, in which improved regret bounds featuring \(H\) have also been derived when \(H\) is known, two types of workarounds have been used. In the REGAL algorithm3, [2] propose to use a doubling trick to counter not knowing \(H\), at the expense of an additional factor of \(\sqrt{S}\) in the regret. The doubling trick method is not applicable to BPI problems, though. For communicating MDPs, the idea of plugging-in an upper bound on \(D\) which is also a (not necessarily tight) upper bound on \(H\) was first proposed by [25] and permits to match the minimax lower bound on the regret of [9] featuring \(D\). In this section, we translate this idea to the best policy identification setting.

Footnote 3: Some issues about this algorithm are detailed in [6], but the doubling trick still deserves consideration.

We propose Diameter Free Exploration (DFE), stated as Algorithm 1, which combines a diameter estimation sub-routine that follows from the work of [21, 20] with the state-of-the-art algorithm of

Figure 1: MDP \(\mathcal{M}_{R}\), the hard instance for Theorem 1. Each arrow corresponds to a state-action and next state combination, and is annotated with the mean reward of the action and the probability of the transition. Arrows with a different line style correspond to different actions.

[27] when (an upper bound on) \(H\) is known. Both components are described in details in Appendix B with their theoretical properties. More precisely, DFE first calls Algorithm 2 to output a quantity \(\widehat{D}\) which satisfies \(D\leq\widehat{D}\leq 4D\) with probability larger than \(1-\delta/2\), using a random number of samples \(N\) from each state action pair \((s,a)\) that satisfies \(N=\widetilde{\mathcal{O}}(D^{2}(\log(1/\delta)+S))\). Then Algorithm 3 (which also uses uniform sampling) takes as input \(\widehat{D}\) and is guaranteed to output a policy that is \(\varepsilon\)-optimal with probability larger than \(1-\delta/2\), using a total number of samples of order \(\widetilde{\mathcal{O}}\left(\frac{SA\widehat{D}}{\varepsilon^{2}}\log\left( \frac{1}{\delta}\right)\right)\). This gives the following theorem.

**Theorem 2**.: _Algorithm 1 is \((\varepsilon,\delta)\)-PAC and its sample complexity satisfies, with probability \(1-\delta\),_

\[\tau=\widetilde{\mathcal{O}}\left(\left[\frac{SAD}{\varepsilon^{2}}+D^{2}SA \right]\log\left(\frac{1}{\delta}\right)+D^{2}S^{2}A\right)\.\]

From Theorem 4 of [22], there exists a communicating MDP with a sample complexity in \(\Omega\left(\frac{SAD}{\varepsilon^{2}}\ln\left(1/\delta\right)\right)\). Therefore, the DFE algorithm is worse than the lower bound by a multiplicative factor \(\left(1+D\varepsilon^{2}+\frac{DS\varepsilon^{2}}{\log 1/\delta}\right)\). In particular, in the regime of small \(\varepsilon\), this algorithm is optimal up to logarithmic factors.

## 5 On the hardness of best policy identification in the online setting

In the online setting, many algorithms with regret guarantees have been designed, however to the best of our knowledge there exists no online algorithm that is guaranteed to output a policy \(\widehat{\pi}\) satisfying \(g_{\widehat{\pi}}\geq g^{*}-\varepsilon\) with probability larger than \(1-\delta\)4. In this section, we give some elements of explanation.

Footnote 4: The work of [16] provides an online policy gradient algorithm for ergodic MDPs but its guarantees are not expressed in the PAC setting considered in the paper (rather with simple regret). Moreover, this algorithm requires some prior knowledge on the MDP through some parameter \(\beta\) related to the mixing time

On the hardness of a regret to PAC conversionIn the finite-horizon setting, [10] gave a way to convert any algorithm guaranteeing sublinear regret into a BPI algorithm with finite sample complexity. However, in average-reward MDP, this conversion is not straightforward. Indeed, it hinges on the fact that selecting at random one of the policies played during the regret-minimizing algorithm should yield a good enough policy: the more time steps there are, the smaller the regret induced by the most recent policy is, thus the better the policy is. However, in our setting, we know that the empirical mean reward of a policy converges towards the gain of this policy, but the speed of this convergence is determined by the mixing time. Moreover, as shown in [22], this mixing time can be arbitrarily large. However, the regret is not _defined_ asymptotically. In [9], the regret at time \(T\) is defined as \(Tg^{*}-\mathbb{E}[\sum_{t=1}^{T}r_{t}]\). In [17], the regret is \(\mathds{E}_{\pi^{*}}[\sum_{t=1}^{T}r_{t}]-\mathds{E}[\sum_{t=1}^{T}r_{t}]\). As the mixing time can be big, it is possible that these regret measures are small for a large number of steps, but that the underlying policy is not anywhere near asymptotically optimal.

It is therefore not possible to preemptively define the number of steps necessary for the regret-minimizing algorithm to find an asymptotically good policy, and not just one that is good on the short term. For example, for any \(N\in\mathds{N}\), \(N\geq 2\), consider the MDP \(\mathcal{M}_{p}\) displayed in Figure 2 with \(p=\frac{1}{N}\) and any \(p^{\prime}<\frac{1-\varepsilon}{1+\varepsilon}p\). While the dashed-line policy is asymptotically best, the full-lined policy is better for at least \(N\) time steps, and is not even \(\varepsilon\)-optimal, as we show in Appendix C.1. An algorithm guaranteeing low regret (over a slightly modified version of the MDP to guarantee ergodicity) could therefore take the full-line action for a number of steps independent of the known parameters \(S\) and \(A\), and the knowledge of \(N\) would be required to know how many samples are necessary for best policy identification. This means that a wrapper turning any regret minimization algorithm into a best policy identification algorithm would require estimating and incorporating this measure \(N\) somewhere, which argues against the existence of a straightforward wrapper as in the episodic setting. Similar arguments in the SSP setting have been brought up for example in [21].

A hardness resultAs the following lower bound shows, it is in fact not possible at all to have an online best policy identification algorithm that has a sample complexity bound polynomial in \(S\), \(A\) and \(H\). This result is as a counterpart of the hardness result proved by [5] in SSP-MDPs.

**Theorem 3**.: _For any \(S\geq 4\), \(A\geq 4\), \(\varepsilon\in\left(0,\frac{1}{4}\right)\), \(\delta\in\left(0,\frac{1}{16}\right)\), and any \((\varepsilon,\delta)\)-PAC online algorithm, there exists a weakly communicating MDP with \(S\) states, \(A\) actions, mean rewards in \([0,1]\), \(H\leq S\) and \(D\geq A^{S-1}/16\varepsilon\) such that if the algorithm starts in a given state \(s_{1}\), \(\mathbb{E}[\tau]=\Omega\left(\frac{A^{S-1}}{\varepsilon}\right)\)._

Sketch of proofFor \(j\in(S-1)^{A}\), we define the hard instance \(\mathcal{M}_{j}\) as in Figure 3. An agent in \(s_{2}\) must execute the precise sequence of actions \(j\) to get back to \(s_{1}\), which is the only state that generates non zero rewards. Since not learning this precise sequence would mean the agent will perform badly on one of the \(\mathcal{M}_{j}\), and therefore perform a very suboptimal policy, the agent will need to have a big enough sample size to at least see \(s_{2}\) with high probability when starting in \(s_{1}\). With a small value of \(p\), it is possible to make the sample complexity exponentially big, as we detail in Appendix C.2.

This hard MDP is actually inspired from the hard SSP-MDP instance considered by [5] to prove that sample-efficient learning is impossible in the online setting of SSP. Indeed, their hard MDP is one where the cost is always the same in each state, in which case we can easily transform the SSP problem into an average reward one, as shown in Figure 5. Indeed, if the costs of the SSP-MDP are all equal and non-zero, then finding an optimal policy in it can be show to be equivalent to finding an optimal policy in the corresponding AR-MDP in which the only reward is in the initial state. 

We remark that while the hard MDP instances \(\mathcal{M}_{j}\) used in our proof have an optimal bias span \(H\) that is upper bounded by \(S\), its diameter is exponential is \(S\). Hence while Theorem 3 implies that no online algorithm can get a \(\mathcal{O}\left(\frac{SAH}{\varepsilon^{2}}\right)\) sample complexity (with or without the knowledge of \(H\)) on every instance and for every \(\varepsilon\), it does not rule out the possibility to have an online algorithm with a sample complexity scaling with \(D\). We propose such an algorithm in the next section.

Figure 3: MDP \(\mathcal{M}_{j}\), the hard instance for Theorem 3

Figure 2: MDP \(\mathcal{M}_{p,p^{\prime}}\) with high mixing time.

[MISSING_PAGE_FAIL:8]

Now to propose a concrete stopping rule, we provide expressions of \(U\) and \(L\) chosen such that the premise of Theorem 5 is satisfied in every round \(n\), with high probability. \(\mathrm{KL}(p,q)\) denotes the Kullback-Leibler divergence between the probability vectors \(p\) and \(q\) on \(\mathcal{S}\).

**Definition 2**.: _Given a sequence of \((b_{n})_{n}\), we define_

\[U^{n}_{s,a}(b_{n};\delta) = \max\left\{p^{\prime}b_{n}\bigg{|}N^{n}_{s,a}\mathrm{KL}(\hat{p}^ {n}_{s,a},p^{\prime})\leq x(\delta,N^{n}_{s,a})\right\}\] \[L^{n}_{s,a}(b_{n},\delta) = \min\left\{p^{\prime}b_{n}\bigg{|}N^{n}_{s,a}\mathrm{KL}(\hat{p}^ {n}_{s,a},p^{\prime})\leq x(\delta,N^{n}_{s,a})\right\}\]

_for the threshold function \(x(\delta,y)=\log(SA/\delta)+(S-1)\log\left(e(1+y/(S-1))\right)\) and let_

\[\tau_{\varepsilon,\delta}=\min_{n}\left\{\max_{s}\max_{a}I^{n,\sharp}_{s,a}(b _{n},\delta)-\min_{s}\max_{a}I^{n,\flat}_{s,a}(b_{n};\delta)\leq\varepsilon \right\}. \tag{6}\]

The following result is a consequence of Theorem 5 and a KL-based time uniform concentration result for the transition probabilities from [1] (Lemma 5 in Appendix). Using Pinsker's inequality (see Remark 1 in Appendix where we discuss some computational aspects), we can also justify that this theorem hold for \(U^{n}_{s,a}\), \(I^{n}_{s,a}\) replaced by

\[\widetilde{U}^{n}_{s,a}(b_{n};\delta)=\hat{p}^{n}_{s,a}b_{n}+||b_{n}||_{\infty }\sqrt{\frac{2x(\delta,N^{n}_{s,a})}{N^{n}_{s,a}}},\ \ \widetilde{L}^{n}_{s,a}(b_{n} ;\delta)=\hat{p}^{n}_{s,a}b_{n}-||b_{n}||_{\infty}\sqrt{\frac{2x(\delta,N^{n}_{ s,a})}{N^{n}_{s,a}}}. \tag{7}\]

**Theorem 6**.: _For any sampling rule \(((s_{n},a_{n}))_{n}\), and any sequence \((b_{n})_{n}\) of bias vectors, the algorithm using the stopping rule \(\tau=\tau_{\varepsilon,\delta}\) defined in (6) and recommending \(\widehat{\pi}(s)=\arg\max_{a}I^{\gamma}_{s,a}(b_{\tau};\delta)\) satisfies \(\mathbb{P}\left(\tau<\infty,g^{\star}-g_{\widehat{\pi}}>\varepsilon\right)\leq\delta\)._

Theorem 6 shows that for any sampling rule, be it in the generative model or in the online setting, the stopping rule (6) and associated recommendation rule yields an \((\varepsilon,\delta)\)-PAC algorithm, for any choice of bias vector sequence \(b_{n}\). Of course, bad choices of sampling rules and bias vectors could still yield \(\tau=\infty\) almost surely (e.g. picking always \((s_{t},a_{t})=(s_{1},a_{1})\) when a generative model is available).

Choosing a sampling ruleAs a sanity-check we analyze in Appendix D.2 an algorithm which combines the simplest possible sampling rule, uniform sampling from a generative model, with the VI stopping rule (6) where the sequence of biais vector is \(b_{n}=\hat{b}_{n}\) where \(\hat{b}_{n}\) is the optimal bias vector in the AR-MDP with rewards \(\overline{r}_{s,a}\) and transition probabilities given by \((\hat{p}^{n}_{s,a})_{s,a}\) (see Algorithm 4). We prove in Theorem 9 that in unichain MDPs, for sufficiently small \(\varepsilon\) the sample complexity of this algorithm is bounded with probability larger than \(1-\delta\) by \(\widetilde{\mathcal{O}}\left(\frac{SA(H\nabla\mathcal{M})^{2}}{\varepsilon^{2 }}\left(\log\frac{1}{\delta}+S\right)\right)\), where \(\Gamma_{\mathcal{M}}\) is some constant depending on the MDP. We refer the reader to Appendix D.2 for the definition of this constant, that has a complex expression and should be in most case larger than \(H\).

We remark that we could also use GOSPRL to turn Algorithm 4 into an online algorithm (using phases of increasing length in which we collect a uniform number of samples using GOSPRL and check our stopping rule), with a sample complexity essentially multiplied by \(D\). However the resulting sample complexity is likely to be much larger than \(SAD^{2}/\varepsilon^{2}\) in the small \(\varepsilon\) regime, making this algorithm not very interesting. We believe that to get efficient algorithms for the online setting it is important to depart from this uniform sampling + GOSPRL approach. Our stopping rule actually suggests some clever online choices that could be used to make the algorithm stop earlier. For example it could be interesting to make a greedy choice of the bias vector \(b_{n}\) that minimizes over \(b\in\mathbb{R}^{S}\) the quantity \(\max_{s}\max_{a}I^{n,\sharp}_{s,a}(b;\delta)-\min_{s}\max_{a}I^{n,\flat}_{s,a} (b;\delta)\). Assuming we could compute this vector, a possible online sampling rule could be optimistic choice \(a_{n}=\operatorname*{argmax}_{a}[\overline{r}_{s,a}+U^{n}_{s_{n},a}(b_{n}, \delta)]\). We leave the analysis of such algorithms for future work.

## 7 Conclusion and perspective

We provided several elements indicating that the optimal bias span \(H\) may not be an adequate complexity measure for \(\varepsilon\)-optimal policy identification in an average-reward MDP. In the generative model setting, as all existing algorithms with sample complexity featuring \(H\) require prior knowledge of this quantity, we investigated the question of estimating \(H\) and proved that the sample complexity of this estimation task can be arbitrarily large. Then, in the online setting, we gave a lower bound on the sample complexity indicating that no algorithm can get a \(SAH/\varepsilon^{2}\) sample complexity on every MDP, with or without the knowledge of \(H\). On the algorithmic side, we proposed the first best policy identification algorithms for the generative model setting that does not require any form of prior knowledge on the MDP. By estimating the diameter \(D\) instead of \(H\), DFE attains a near-optimal sample complexity in \(SAD/\varepsilon^{2}\) for communicating MDPs. We further proposed an online variant of DFE with a slightly larger \(SAD^{2}/\varepsilon^{2}\) sample complexity, which is a factor \(D\) away from the worse case lower bound established in the generative model setting. We leave as an open question whether there exists online algorithms with a \(SAD/\varepsilon^{2}\) sample complexity. In future work, we will investigate whether the novel adaptive stopping rule proposed in our work can lead to this reduced sample complexity, when combined with a suitable online sampling rule.

## Acknowledgments and Disclosure of Funding

The authors acknowledge the funding of the French National Research Agency under the project FATE (ANR22-CE23-0016-01) and the PEPR IA FOUNDRY project (ANR-23-PEIA-0003). The authors are members of the Inria team Scool.

## References

* [1] Aymen Al Marjani and Alexandre Proutiere. Adaptative sampling for best policy identification in Markov Decision Processes. In _38th international conference on Machine Learning_, volume 139. PMLR, 2021.
* [2] Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs. In _25th conference on uncertainty in artificial intelligence (UAI2009)_, 2009.
* [3] Hippolyte Bourel, Odalric Maillard, and Mohammad Sadegh Talebi. Tightening exploration in upper confidence reinforcement learning. In _Internation Conference on Machine Learning (ICML)_, 2020.
* [4] Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptative policies for Markov Decision Processes. _Mathematics of operations research_, 22(1):222-255, 1997.
* [5] Liyu Chen, Andrea Tirinzoni, Matteo Pirotta, and Alessandro Lazaric. Reaching goals is hard: settling the sample complexity of the stochastic shortest path. In _34th International conference on Algorithmic learning theory_, volume 201, pages 310-357. PMLR, 2023.
* [6] Ronan Fruit, Matteo Pirotta, and Alessandro Lazaric. Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes. In _32nd Conference on Neural Information Processing Systems_, 2018.
* [7] Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner. Efficient bias-span-constrained exploration-exploitation in reinforcement learning. In _Internation Conference on Machine Learning (ICML)_, 2018.
* [8] Scott Fujimoto, Meger David, Doina Precup, Ofir Nachum, and Shixiang Shane Gu. Why should i trust you, Bellman? the Bellman error is a poor replacement for value error. In _39th international conference on Machine Learning_, volume 162. PMLR, 2022.
* [9] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. _Journal of Machine Learning Research_, 11:1563-1600, 2010.
* [10] Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I. Jordan. Is Q-learning provably efficient? In _32nd international conference on on Neural Information Processing Systems (NeurIPS 2018)_, 2018.
* [11] Yujia Jin and Aaron Sidford. Efficiently solving MDPs with stochastic mirror descent. In _37th international conference on Machine Learning_, volume 119. PMLR, 2020.

* [12] Yujia Jin and Aaron Sidford. Towards tight bounds on the sample complexity of average-reward MDPs. In _38th international conference on Machine Learning_, volume 139. PMLR, 2021.
* [13] Emilie Kaufmann, Pierre Menard, Omar Darwiche Domingues, Anders Jonsson, Edouard Leurent, and Michal Valko. Adaptive reward-free exploration. In _Algorithmic Learning Theory (ALT)_, 2021.
* [14] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. _Machine Learning_, 49:209-232, 2002.
* [15] Gen Li, Laixi Shi, Yuxin Chen, Yuantao Gu, and Yuejie Chi. Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [16] Tianjiao Li, Feiyang Wu, and Guanghui Lan. Stochastic first-order methods for average-reward markov decision processes. [https://arxiv.org/abs/2205.05800](https://arxiv.org/abs/2205.05800), 2022.
* [17] Fabien Pesquerel and Odalric-Ambryn Maillard. IMED-RL: Regret optimal learning of ergodic Markov Decision Processes. In _36th conference on Neural Information Processing Systems (NeurIPS 2022)_, 2022.
* [18] Martin L. Puterman. _Markov Decision Processes_. John Wiley & sons, 1994.
* [19] Jean Tarbouriech, Evrard Garcelon, Michal Valko, Matteo Pirotta, and Alessandro Lazaric. No-regret exploration in goal-oriented reinforcement learning. In _Internation Conference on Machine Learning (ICML)_, 2020.
* [20] Jean Tarbouriech, Matteo Pirotta, Michal Valko, and Alessandro Lazaric. A provably efficient sample collection strategy for reinforcement learning. In _35th conference on neural information processing systems_, 2021.
* [21] Jean Tarbouriech, Matteo Pirotta, Michal Valko, and Alessandro Lazaric. Sample complexity bounds for stochastic shortest path with a generative model. In _32nd International conference on Algorithmic learning theory_, volume 132. PMLR, 2021.
* [22] Jinghan Wang, Mengdi Wang, and Lin F. Yang. Near sample-optimal reduction-based policy learning for average reward MDP. [https://arxiv.org/abs/2212.00603](https://arxiv.org/abs/2212.00603), 2022.
* [23] Mengdi Wang. Primal-dual \(\pi\) learning: Sample complexity and sublinear run time for ergodic Markov Decision Problems. [https://arxiv.org/abs/1710.06100](https://arxiv.org/abs/1710.06100), 2017.
* [24] Shengbo Wang, Jose Blanchet, and Peter Glynn. Optimal sample complexity for average reward markov decision processes. [https://arxiv.org/abs/2310.08833](https://arxiv.org/abs/2310.08833), 2023.
* [25] Zihan Zhang and Xiangyang Ji. Regret minimization for reinforcement learning by evaluating the optimal bias function. In _33rd conference on neural information processing systems_, 2019.
* [26] Zihan Zhang and Qiaomin Xie. Sharper model-free reinforcement learning for average-reward Markov Decision Processes. [https://arxiv.org/abs/2306.16394](https://arxiv.org/abs/2306.16394), 2023.
* [27] Matthew Zurek and Yudong Chen. Span-based optimal sample complexity for average reward MDPs. [https://arxiv.org/abs/2311.13469](https://arxiv.org/abs/2311.13469), 2023.

Proof of Theorem 1

We only prove here the result for weakly communicating MDPs, and provide the ergodic MDP necessary to prove the general theorem.

Fix \(\delta\), \(T\) and \(\Delta\). Define \(d=e^{4}\) and \(d^{\prime}=128\). Let \(\varepsilon=\min\left\{\frac{1}{10},\frac{\Delta}{2},\frac{1}{2\sqrt{\frac{32 \Gamma d^{\prime}}{-\ln 2\delta d}+1}}\right\}\), \(p=\frac{\frac{1}{2}+\varepsilon}{\Delta-\varepsilon}\). To ease the notation, we denote by \(\mathcal{M}_{\varepsilon}\) and \(\mathcal{M}_{\varepsilon}^{\prime}\) respectively the MDPs \(\mathcal{M}_{\frac{1}{2}-\varepsilon}\) and \(\mathcal{M}_{\frac{1}{2}+\varepsilon}\) in Figure 1. The rewards distribution in these MDPs are assumed to be Bernoulli variables with the means indicated in the figure.

The optimal policy in \(\mathcal{M}_{\varepsilon}\) always chooses the actions represented by a full line. Its gain is \(\frac{1}{2}\), its bias vector (up to an additive constant) is \((0,-\frac{1}{2},-\frac{1}{2})\). In \(\mathcal{M}_{\varepsilon}^{\prime}\), the optimal policy always chooses the dashed actions and its optimal gain is \(\frac{1}{2}+\varepsilon\). Using the Poisson equations, we can show that its associated bias vector (up to an additive constant) is \(\left(-\left(\frac{1}{2}+\varepsilon\right)\frac{1+p}{p},0,-\left(\frac{1}{2 }+\varepsilon\right)\frac{1}{p}\right)\). With \(H_{\varepsilon}\) the span of the optimal bias for \(\mathcal{M}_{\varepsilon}\) and \(H_{\varepsilon}^{\prime}\) that of \(\mathcal{M}_{\varepsilon}^{\prime}\), we have:

\[H_{\varepsilon}^{\prime}-H_{\varepsilon} =\frac{1}{p}\left(\left(\frac{1}{2}+\varepsilon\right)(1+p)-\frac {p}{2}\right)=\frac{1}{p}\left(\frac{1}{2}+\varepsilon\right)+\varepsilon\] \[=\frac{\Delta-\varepsilon}{\frac{1}{2}+\varepsilon}\left(\frac{1 }{2}+\varepsilon\right)+\varepsilon\] \[=\Delta\]

Consider now an algorithm that outputs \(\hat{H}\) a \(\Delta\)-tight upper bound for the optimal bias span with probability greater than \(1-\delta\) on any MDP. We denote by \(P\) and \(P^{\prime}\) the probability with regards to \(\mathcal{M}_{\varepsilon}\) and \(\mathcal{M}_{\varepsilon}^{\prime}\) respectively, and E and E\({}^{\prime}\) the associated expectation.

We denote by \(\tau\) the total number of samples used before stopping, by \(\hat{T}\) the number of samples of the dashed action taken in state \(2\) before stopping and by \(K_{t}\) the number of times the agent gets a reward of \(1\) among the first \(t\) visits of state \(2\). We introduce the three events

\[\mathcal{E}_{1} = \{\hat{H}<H_{\varepsilon}^{\prime}\}\] \[\mathcal{E}_{2} = \left\{\max_{1\leq t\leq t^{\star}}|\left(\frac{1}{2}-\varepsilon \right)t-K_{t}|\leq z\right\}\] \[\mathcal{E}_{3} = \left\{\hat{T}\leq t^{\star}\right\},\]

where we define \(z=\sqrt{2\left(\frac{1}{2}-\varepsilon\right)\left(\frac{1}{2}+\varepsilon \right)t^{\star}\ln\frac{d}{\theta}}\), with \(\theta=\exp\left(\frac{-4d^{\prime}e^{2}t^{\star}}{\left(\frac{1}{2}- \varepsilon\right)\left(\frac{1}{2}+\varepsilon\right)}\right)\) and let \(t^{\star}=\frac{\left(\frac{1}{2}-\varepsilon\right)\left(\frac{1}{2}+ \varepsilon\right)}{4d^{\prime}e^{2}}\ln\frac{1}{2d\delta}\). We let \(\mathcal{E}=\mathcal{E}_{1}\cap\mathcal{E}_{2}\cap\mathcal{E}_{3}\).

Let \(W\) be the interaction history of the learner and the generative model, \(L(w)=P(W=w)\) and \(L^{\prime}(w)=P^{\prime}(W=w)\). For \(K=K_{\hat{T}}\),

\[\frac{L^{\prime}(W)}{L(W)}\mathds{1}_{\mathcal{E}}=\frac{(1/2+\varepsilon)^{K }(1/2-\varepsilon)^{T-K}}{(1/2-\varepsilon)^{K}(1/2+\varepsilon)^{T-K}} \mathds{1}_{\mathcal{E}}\geq\frac{\theta}{d}\mathds{1}_{\mathcal{E}}\]

by the arguments within the proof for Lemma 13 of [5]. Using a change of measure, we can write

\[P^{\prime}(\mathcal{E}_{1})\geq P^{\prime}(\mathcal{E})=\mathds{E}^{\prime}[ \mathds{1}_{\mathcal{E}}(W)]=\mathds{E}\left[\frac{L^{\prime}(W)}{L(W)} \mathds{1}_{\mathcal{E}}(W)\right]\geq\frac{\theta}{d}P(\mathcal{E})=2\delta P (\mathcal{E}). \tag{8}\]

Moreover, in \(\mathcal{M}_{\varepsilon}\), \(K_{t}-(1/2-\varepsilon)\,t\) is the sum of \(t\) Bernoulli variables of expectation \((1/2-\varepsilon)\), and thus of variance \((1/2-\varepsilon)(1/2+\varepsilon)\). Kolmogorov's inequality states that

\[P(\overline{\mathcal{E}_{2}})=P\left(\max_{1\leq t\leq t^{\star}}\left|K_{t}- \left(\frac{1}{2}-\varepsilon\right)t\right|>\varepsilon\right)\leq\frac{t^{ \star}\left(\frac{1}{2}-\varepsilon\right)\left(\frac{1}{2}+\varepsilon\right) }{\varepsilon^{2}}\]

which entails \(P(\mathcal{E}_{2})\geq 7/8\). The detailed computation is given in the proof for Lemma 12 of [5].

Now, we assume towards contradiction that \(P(\mathcal{E}_{3})\geq 7/8\). Using the above, this yields

\[P\left(\overline{\mathcal{E}}\right) \leq P(\overline{\mathcal{E}_{1}})+P(\overline{\mathcal{E}_{2}})+P( \overline{\mathcal{E}_{3}})\] \[\leq \delta+\frac{2}{8}\] \[< \frac{1}{2},\]

where the first inequality uses that \(P(\mathcal{E}_{1})\geq 1-\delta\) by the correctness of the algorithm and the second inequality uses that \(\delta\leq\frac{1}{2e^{\star}}\). Hence, we have \(P(\mathcal{E})>\frac{1}{2}\) which leads to \(P^{\prime}(\mathcal{E}_{1})>\delta\) using (8). This contradicts the correctness of the algorithm. Therefore, we have \(P(\mathcal{E}_{3})<\frac{7}{8}\).

It follows that with probability larger than \(\frac{1}{8}\), \(\widehat{T}\geq t^{\star}\) on \(\mathcal{M}_{\varepsilon}\), hence

\[\mathbb{E}[\tau]\geq\mathbb{E}[\hat{T}]\geq\frac{t^{\star}}{8}\geq T,\]

which concludes the proof.

More generally, adding the transitions described in Figure 4 with \(\tau^{\prime}=\frac{\tau p}{1+p}\) can be used to show that computing a \(\frac{\Delta}{1+\tau}\)-tight upper bound for the optimal bias span with high probability would need in expectation more than \(T\) samples.

## Appendix B Complements for Section 4

In this Appendix, we provide some details on the two components of our near-optimal algorithm: a procedure to provide a high-probability upper bound on the diameter (Algorithm 2) and a near-optimal algorithm for best policy identification in an average reward MDP that requires an upper bound on \(H\) as an input (Algorithm 3). Finally, we give the proof of Theorem 2.

### Diameter Estimation Procedure

The procedure proposed by [20] for estimating the diameter hinges on the observation that the diameter can expressed in terms of optimal values in a goal-oriented Markov Decision Decision (or SSP-MDP for Stochastic Shortest Path). We recall that in a SSP-MDP the transition kernel is coupled with a goal state \(s_{g}\in\mathcal{S}\) and a cost function \(c:(s,a)\mapsto c(s,a)\). The value of a (deterministic) policy \(\pi\), that is to be minimized, is denoted by

\[V^{\pi}_{c,s_{g}}(s)=\mathbb{E}^{\pi}\left[\sum_{t=1}^{\tau^{(s_{p})}_{\pi}(s )}c(s_{t},\pi(s_{t}))\Bigg{|}s_{1}=s\right]\]where \(\tau_{s}^{(s_{g})}(s)=\inf\{t:s_{t+1}=s_{g}|s_{1}=s,\pi\}\) is the number of steps before reaching the goal when starting from state \(s\) and following policy \(\pi\). The diameter can thus be written

\[D = \max_{s}\max_{s^{\prime}\neq s}\min_{\pi}\mathbb{E}[\tau_{\pi}^{(s )}(s^{\prime})]=\max_{s}\max_{s^{\prime}\neq s}\min_{\pi}V_{\mathbf{1},s}^{\pi} (s^{\prime})\] \[= \max_{s}\max_{s^{\prime}\neq s}V_{\mathbf{1},s}^{\star}(s^{\prime})\]

where \(V_{c,s_{g}}^{\star}\) denotes the optimal (i.e. minimal) value function in the SSP-MDP with cost \(c\) and goal state \(s_{g}\) and \(\mathbf{1}\) is the cost function that is constant and equal to 1.

Prior works on SSP-MDPs [19, 21] have proposed and analyzed an Extended Value Iteration scheme for SSP, whose goal is to obtain confidence bounds on \(V_{c,s_{g}}^{\star}\), that we now recall for completeness. Given a cost function \(c\) and a goal state \(s_{g}\), \(\text{EVI-SSP}_{c,s_{g}}\) takes as input a set of plausible transitions \(\mathcal{P}=(\mathcal{P}(s,a))_{s,a}\) such that \(\mathcal{P}(s,a)\) are included in the set of probability distribution over \(\mathcal{S}\) (and the unknown vectors \(p_{s,a}\) is believed to belong to \(\mathcal{P}(s,a)\)), and a precision \(\mu_{\text{VI}}\). It introduces the extended optimal Bellman operator, defined for \(v\in\mathbb{R}^{S}\) as

\[\widetilde{L}v(s)=\min_{a\in\mathcal{A}}\left[c(s,a)+\min_{\widetilde{p}\in \mathcal{P}(s,a)}\widetilde{p}v\right]\]

for all \(s\neq s_{g}\) and \(\widetilde{L}v(g)=0\). \(\text{EVI-SSP}_{c,s_{g}}\) sets \(v_{0}=\mathbf{0}\in\mathbb{R}^{S}\) and defines \(v_{j+1}=\widetilde{L}v_{j}\) for \(j\geq 0\). It stops at iteration \(\widetilde{\jmath}=\min\left\{j:\|v_{j+1}-v_{j}\|\leq\mu_{\text{VI}}\right\}\) and outputs \(\widetilde{v}=v_{\widetilde{\jmath}}\). It also outputs an optimistic transition kernel \(\widetilde{p}=(\widetilde{p}_{s,a})_{s,a}\), defined for all \(s,a\) as

\[\widetilde{p}_{s,a}\in\underset{\widetilde{p}\in\mathcal{P}(s,a)}{\text{ argmin}}\ \widetilde{p}v\]

and the optimistic greedy policy

\[\widetilde{\pi}(s)=\underset{a\in\mathcal{A}}{\text{argmin}}\ \left[c(s,a)+ \widetilde{p}v\right]\.\]

We denote by \(\widetilde{V}_{c,s_{g}}^{\pi}(s)\) the value function of policy \(\pi\) in the SSP with cost function \(c\), goal state \(s_{g}\) and transition kernel \(\widetilde{p}\). Previous work (see e.g. Lemma 4 of [21]) have established the following properties.

**Lemma 1**.: _Assume that, for all \((s,a)\), \(p_{s,a}\in\mathcal{P}(s,a)\). Letting \((\widetilde{v},\widetilde{p},\widetilde{\pi})=\text{EVI-SSP}_{c,s_{g}}( \mathcal{P},\mu_{\text{VI}})\), the following inequalities hold (component-wise):_

1. \(\widetilde{v}\leq V_{c,s_{g}}^{\star}\) _and_ \(\widetilde{v}\leq\widetilde{V}_{c,s_{g}}^{\star}\leq\widetilde{V}_{c,s_{g}}^{\widetilde{\pi}}\)__
2. _if_ \(\mu_{\text{VI}}\leq\frac{c_{\min}}{2}\) _then_ \(\widetilde{V}_{c,s_{g}}^{\widetilde{\pi}}\leq\left(1+\frac{2\mu_{\text{VI}}}{c _{\min}}\right)\widetilde{v}\)_._

Notably, this result gives an upper bound on the optimal value in the optimistic SSP-MDP. With the additional help of a simulation lemma, proved by [20], we can further relate it to the value in the true SSP-MDP.

**Lemma 2** (Lemma 2 in [21]).: 6 _Let \(p\) and \(\widetilde{p}\) be two transition kernels such that for all \((s,a)\), \(\|p_{s,a}-\widetilde{p}_{s,a}\|_{1}\leq\eta\). Assume that \(c_{\min}>0\) and that under both \(p\) and \(\widetilde{p}\) there exists at least one policy that reaches \(s_{g}\) almost surely from any state (such a policy is called proper). Let \(\pi\) be a proper policy in \(\widetilde{p}\) such that_

Footnote 6: Compared to the original statement, we fixed a small typo in the condition (9), and also propagated the changes accordingly in the definition of Algorithm 2 and its analysis.

\[2\eta\|\widetilde{V}_{c,s_{g}}^{\pi}\|_{\infty}\leq c_{\min}. \tag{9}\]

_Then \(\pi\) is proper in \(p\) and_

\[\forall s\in\mathcal{S},V_{c,s_{g}}^{\pi}(s)\leq\left(1+\frac{2\eta\| \widetilde{V}_{c,s_{g}}^{\pi}\|_{\infty}}{c_{\min}}\right)\widetilde{V}_{c,s_{g }}^{\pi}(s)\]Finally, the last step to be able to apply Lemma 1 is to construct the sets \(\mathcal{P}(s,a)\) that contains the true transition probabilities with high probability. This can be done as in the rest of the paper by relying on Lemma 5, which gives confidence regions of the form

\[\{p\in\Sigma_{S}:\|p-\hat{p}_{s,a,n}\|_{1}\leq B(n,\delta)\}\]

where \(\hat{p}_{s,a,n}\) is the empirical estimate of the transition probability after the \(n\)-th transition has been observed from \((s,a)\) and \(B(n,\delta)=\sqrt{\frac{2\log(SA/\delta)+2(S-1)\log(e(1+n/(S-1)))}{n}}\).

The resulting algorithm for estimating the diameter is described in Algorithm 2. It is a slight variation of the procedures proposed by [20] to estimate the SSP diameter and by [21] to estimate the diameter in the online setting. In particular, our instantiation relies on simpler confidence regions.

```
Data: Accuracy \(\varepsilon>0\), confidence level \(\delta\in(0,1)\) Set \(W=\frac{1}{2}\) and \(\widetilde{v}_{\infty}=1\) while\(\widetilde{v}_{\infty}>W\)do \(W\gets 2W\)  Set accuracy \(\eta=\frac{\varepsilon}{4W}\) and compute \(N=N(\delta,\eta)=\inf\{n:B(n,\delta)\leq\eta\}\) Call the generative model until each \((s,a)\) gets \(N\) visits.  Let each \((s,a)\) let \(\mathcal{P}(s,a)=\{p\in\Sigma_{S}:\|p-\hat{p}_{s,a,N}\|_{1}\leq\eta\}\) and \(\mathcal{P}=(\mathcal{P}(s,a))_{s,a}\). for\(s=s_{1},...,s_{S}\)do  Let \((\widetilde{v}^{(s)},\widetilde{p}^{(s)},\widetilde{\pi}^{(s)})=\text{EVI-SSP} _{\mathbf{1},s}\left(\mathcal{P},\frac{1\wedge\varepsilon}{2}\right)\)  end while  Let \(\widetilde{v}_{\infty}=\max_{s}\max_{s^{\prime}\neq s}\widetilde{v}^{(s)}(s^{ \prime})\). end return\(\widehat{D}=(1+\eta\widetilde{v}_{\infty}/2)\widetilde{v}_{\infty}\)
```

**Algorithm 2**A diameter estimation procedure

**Theorem 7**.: _Let \(\varepsilon\leq 1\). With probability \(1-\delta\), Algorithm 2 run with parameters \(\varepsilon\) and \(\delta\) outputs an estimate \(\widehat{D}\) which satisfies_

\[D\leq\widehat{D}\leq\left(1+\frac{\varepsilon(1+\varepsilon)}{2}\right)(1+ \varepsilon)D\]

_using \(SA\times N(\delta,\frac{\varepsilon}{8D})\) samples where_

\[N(\delta,\eta):=\inf\left\{n>0:B(n,\delta)\leq\eta\right\}\;,\]

_leading to an overall sample complexity of_

\[\widetilde{\mathcal{O}}\left(\frac{D^{2}SA}{\varepsilon^{2}}\log\left(\frac{ 1}{\delta}\right)+\frac{D^{2}S^{2}A}{\varepsilon^{2}}\right)\]

_where the \(\widetilde{\mathcal{O}}\) ignores logarithmic factors in \(S,A,D\), \(1/\varepsilon\) and \(\log(1/\delta)\)._

Proof.: The proof closely follows that of [20]. We let \(W_{n}\) be the value of \(W\) in the \(n\)-th iteration of the while loop (starting at \(n=1\)), such that \(W_{n}=2^{n-1}\). We define \(\eta_{n}=\frac{\varepsilon}{4W_{n}}\) and \(\widetilde{v}_{n}\) the value of \(\widetilde{v}_{\infty}\) at the end of the \(n\)-th iteration. The number of iteration used by the algorithm is \(\widetilde{n}=\min\{n:\widetilde{v}_{n}\leq W_{n}\}\).

Assume the event

\[\mathcal{E}=(\forall s,a,\forall n\geq 1,\|\hat{p}_{s,a,n}-p_{s,a}\|_{1}\leq B (n,\delta))\]

holds. Using a property of the EVI scheme (first statement in Lemma 1), for all \(n\), \(\widetilde{v}_{n}^{(s)}\leq V_{1,s}^{*}\leq D\) (component-wise), which yields \(\widetilde{v}_{n}\leq D\). Hence \(\widetilde{n}\) is bounded by \(\min\{n:D\leq W_{n}\}\leq\log_{2}(D)+1\). We now bound the final accuracy \(\widetilde{\eta}:=\eta_{\widetilde{n}}\). First, as \(\widetilde{v}_{\infty}:=\widetilde{v}_{\widetilde{n}}\leq W_{\widetilde{n}}\), we have \(\widetilde{\eta}\leq\frac{\varepsilon}{4\widetilde{v}_{\infty}}\). Moreover, using that

\[D\geq\widetilde{v}_{\widetilde{n}-1}>W_{\widetilde{n}-1}=\tfrac{W_{\widetilde {n}}}{2}\]

further yields

\[\frac{\varepsilon}{8D}\leq\widetilde{\eta}\leq\frac{\varepsilon}{4\widetilde{ v}_{\infty}}.\]We let \(\widetilde{v}^{(s)},\widetilde{p}^{(s)},\widetilde{\pi}^{(s)}\) be the output of Extended Value Iteration with goal state \(s\) in the final iteration, and let \(\widetilde{V}_{1,s}\) denote the value function in the SSP MDP with costs \(\mathbf{1}\), goal state \(s\) and transition kernel \(\widetilde{p}^{(s)}\). For every \(s,s^{\prime}\), using the simulation lemma, we have

\[V_{\mathbf{1},s}^{\star}(s^{\prime}) \leq V_{\mathbf{1},s}^{\widetilde{\pi}^{(s)}}(s^{\prime})\leq(1+2 \widetilde{\eta})\|\widetilde{V}_{\mathbf{1},s}^{\widetilde{\pi}^{(s)}}\|_{ \infty}\big{\}}\widetilde{V}_{\mathbf{1},s}^{\widetilde{\pi}^{(s)}}(s^{\prime})\]

provided that the condition

\[2\widetilde{\eta}\|\widetilde{V}_{\mathbf{1},s}^{\widetilde{\pi}^{(s)}}\|_{ \infty}\leq 1\]

is satisfied. This is the case by using the second statement of Lemma 1 together with the upper bound on \(\widetilde{\eta}\) established above:

\[2\widetilde{\eta}\|\widetilde{V}_{\mathbf{1},s}^{\widetilde{\pi}^{(s)}}\|_{ \infty}\leq 2\widetilde{\eta}(1+\varepsilon)\|\widetilde{v}^{(s)}\|_{ \infty}\leq 2\widetilde{\eta}(1+\varepsilon)\widetilde{v}_{\infty}\leq\frac{ \varepsilon(1+\varepsilon)}{2}\leq 1\,\]

where the last step uses that \(\varepsilon\leq 1\). Hence, one can write

\[V_{\mathbf{1},s}^{\star}(s^{\prime}) \leq (1+2\widetilde{\eta}\|\widetilde{V}_{\mathbf{1},s}^{\widetilde{ \pi}^{(s)}}\|_{\infty})\widetilde{V}_{\mathbf{1},s}^{\widetilde{\pi}^{(s)}}(s ^{\prime})\] \[\stackrel{{(a)}}{{\leq}} (1+2\widetilde{\eta}(1+\varepsilon)\|\widetilde{v}^{(s)}\|_{ \infty})(1+\varepsilon)\widetilde{v}^{(s)}(s^{\prime})\] \[\stackrel{{(b)}}{{\leq}} (1+2\widetilde{\eta}(1+\varepsilon)\widetilde{v}_{\infty})(1+ \varepsilon)\widetilde{v}_{\infty}=\widehat{D}\] \[\stackrel{{(c)}}{{\leq}} \left(1+\frac{(1+\varepsilon)\varepsilon}{2}\right)(1+\varepsilon )D,\]

where \((a)\) uses the second statement of Lemma 1, \((b)\) uses the definition of \(\widetilde{v}_{\infty}\) and \((c)\) uses that \(\widetilde{v}_{\infty}\leq D\) and \(\widetilde{\eta}v_{\infty}\leq\varepsilon/4\). Recalling that \(D=\max_{s,s^{\prime}\neq s}V_{\mathbf{1},s}^{\star}(s^{\prime})\) yields that, on event \(\mathcal{E}\),

\[D\leq\widehat{D}\leq(1+\frac{(1+\varepsilon)\varepsilon}{2})(1+\varepsilon)D.\]

Moreover, the number of samples collected per transition is \(N(\delta,\widetilde{\eta})\leq N\left(\delta,\frac{\varepsilon}{8D}\right)\).

The conclusion follows from the fact that \(\mathcal{E}\) holds with probability larger than \(1-\delta\) (Lemma 5) and by upper bounding \(N\left(\delta,\frac{\varepsilon}{8D}\right)\) using Lemma 6 in Appendix E. Specifically, choosing the parameters \(\Delta^{2}=\eta^{2}\), \(a=2\log\left(\frac{SA}{\delta}\right)\), \(b=2(S-1)\), \(c=e\) and \(d=\frac{e}{S-1}\) we get

\[N(\delta,\eta)\leq\frac{1}{\eta^{2}}\log\left(\frac{SA}{\delta}\right)+\frac{ 2(S-1)}{\eta^{2}}\log\left(e+\frac{e}{(S-1)\eta^{4}}\log\left(\frac{SA}{ \delta}\right)+\frac{8e}{\eta^{4}}\right).\]

which leads to the approximation stated in Theorem 7. 

### Near-optimal algorithm using the knowledge of \(H\)

We recall here for completeness the algorithm of [27] to find an \(\varepsilon\)-optimal policy in an average reward MDP when an upper bound \(\overline{H}\) of the optimal bias span \(H\) is known, and its theoretical guarantees. The algorithm consists in a slight variation of the Perturbed Empirical Model-Based Planning algorithm originally given by [15] (for which a refined analysis was proposed by [27]) with the reduction from the average reward to the discounted case from [22].

**Data:** Accuracy \(\varepsilon\in(0,1]\), upper bound \(\overline{H}\) on \(H\), confidence level \(\delta\in(0,1)\)

Set discount factor \(\overline{\gamma}=1-\frac{\varepsilon}{12H}\)

Set \(\overline{n}=\frac{144C_{2}\overline{H}}{\varepsilon^{2}}\log\left(\frac{12SA }{\delta\varepsilon}\right)\) with \(C_{2}\) the constant in Theorem 1 of [27]

Collect \(\overline{n}\) samples from the transition in each \((s,a)\)

Let \(\hat{p}\) be the estimated transition kernel based on all transitions collected

Compute the randomized reward function \(\widetilde{r}(s,a)=r(s,a)+X_{s,a}\) where \(X_{s,a}\stackrel{{ i.i.d}}{{\sim}}\mathcal{U}\left(\left[0,\frac {\varepsilon}{72}\right]\right)\)

Compute \(\hat{\pi}\) the optimal policy in the discounted MDP (\(\overline{\gamma}\), \(\hat{p}\), \(\widetilde{r}\))

**return \(\hat{\pi}\)**Algorithm 3**: Algorithm 2 from [27]

**Theorem 8**.: _With probability \(1-\delta\), Algorithm 3 with parameters \(\varepsilon\in(0,1]\), \(\overline{H}\) satisfying \(H\leq\overline{H}\) and \(\delta\in(0,1)\) outputs a policy \(\hat{\pi}\) satisfying_

\[\mathbb{P}\left(\forall s\in\mathcal{S},\rho^{\star}-\rho^{\hat{\pi}}(s)\leq \varepsilon\right)\geq 1-\delta\]

_and collects a (deterministic) total number of transitions given by_

\[\frac{144C_{2}SA\overline{H}}{\varepsilon^{2}}\log\left(\frac{12SA}{\delta \varepsilon}\right),\]

_where \(C_{2}\) is the constant in Theorem 1 of [27]._

### Proof of Theorem 2

We let \(\widehat{D}\) be the output of Algorithm 2 run with parameters \(1\) and \(\delta/2\) and \(\tau_{1}\) the total number of samples it uses. From Theorem 7, it holds that

\[\mathbb{P}\left(D\leq\widehat{D}\leq 4D,\tau_{1}=\widetilde{\mathcal{O}}\left(D ^{2}SA\log(1/\delta)+D^{2}S^{2}A\right)\right)\geq 1-\delta/2.\]

We let \(\widehat{\pi}\) be the output of Algorithm 3 run with parameter \(\varepsilon\), \(\widehat{D}\) and \(\delta/2\) and \(\tau_{2}\) the total number of samples it uses. Using Theorem 8, we have

\[\mathbb{P}\left(g^{\star}-g_{\widehat{\pi}}\leq\varepsilon,\tau_{2}\leq\frac{1 44C_{2}SA\widehat{D}}{\varepsilon^{2}}\log\left(\frac{24SA}{\delta\varepsilon }\right)\right|D\leq\widehat{D}\right)\geq 1-\delta/2.\]

The total sample complexity being \(\tau=\tau_{1}+\tau_{2}\), using a union bound yields that with probability \(1-\delta\), it holds that \(g^{\star}-g_{\widehat{\pi}}\leq\varepsilon\) and

\[\tau \leq \widetilde{\mathcal{O}}\left(D^{2}SA\log(1/\delta)+D^{2}S^{2}A \right)+\frac{576C_{2}SAD}{\varepsilon^{2}}\log\left(\frac{24SA}{\delta \varepsilon}\right)\] \[= \widetilde{\mathcal{O}}\left(\left[\frac{SAD}{\varepsilon^{2}}+D ^{2}SA\right]\log\left(\frac{1}{\delta}\right)+D^{2}S^{2}A\right)\] \[= \widetilde{\mathcal{O}}\left(\left[\frac{SAD}{\varepsilon^{2}}+D ^{2}S^{2}A\right]\log\left(\frac{1}{\delta}\right)\right)\.\]

## Appendix C Complements for Section 5

### Study of the MDP \(\mathcal{M}_{p,p^{\prime}}\)

We recall the MDP introduced in Figure 2. We prove the following: for any \(N\), for any \(p<1/N\) and any \(p^{\prime}<\frac{1-\varepsilon}{1+\varepsilon}p\), the policy in full line \(\pi\) is optimal for more than \(N\) steps, but is not \(\varepsilon\)-optimal in average reward.

For that, we define \(u_{n}\) the probability of being in state \(1\) at timestep \(n\). We note that \(u_{n}\) does not depend on the policy chosen. Since we know \(\begin{cases}u_{0}=1\\ u_{n+1}=(1-p)u_{n}+p^{\prime}(1-u_{n})\end{cases}\), we have

\[u_{n}=(1-p-p^{\prime})^{n}\left(1-\frac{p^{\prime}}{p+p^{\prime}}\right)+\frac {p^{\prime}}{p+p^{\prime}}\]

Figure 2: MDP \(\mathcal{M}_{p,p^{\prime}}\) (repeated from page 7)Therefore, the (asymptotic) gain for the full-line policy \(\pi\) is \(g_{\pi}=\frac{p^{\prime}}{p+p^{\prime}}\), while that of the dashed-line policy \(\pi^{\prime}\) is \(g_{\pi^{\prime}}=1-\frac{p^{\prime}}{p+p^{\prime}}=\frac{p}{p+p^{\prime}}\). Since \(p^{\prime}<\frac{1-\varepsilon}{1+\varepsilon}p\),

\[p^{\prime}(1+\varepsilon) <p(1-\varepsilon)\] \[p^{\prime} <p-(p+p^{\prime})\varepsilon\] \[g_{\pi} <g_{\pi^{\prime}}-\varepsilon\]

and we have indeed that \(\pi\) is not \(\varepsilon\)-optimal.

Moreover, the empirical reward up to step \(n\) for policy \(\pi\) is \(r_{n}=\sum_{t=0}^{n}u_{n}\), and \(r^{\prime}_{n}=n+1-\sum_{t=0}^{n}u_{n}\) for policy \(\pi^{\prime}\). As \(\left(r_{n}/n\right)_{n}\) is decreasing and \(\left(r^{\prime}_{n}/n\right)_{n}\) is increasing, policy \(\pi^{\prime}\) becomes better than \(\pi\) when \(\sum_{t=0}^{n}u_{n}\leq\frac{n+1}{2}\), that is, when

\[(n+1)\frac{p^{\prime}}{p+p^{\prime}}+\left(1-\frac{p^{\prime}}{p+p^{\prime}} \right)\sum_{t=0}^{n}(1-p-p^{\prime})^{t} \leq\frac{n+1}{2}\]

\[\frac{p}{p+p^{\prime}}\cdot\frac{1}{n+1}\cdot\frac{1-(1-p-p^{\prime})^{n+1}}{p +p^{\prime}} \leq\frac{1}{2}-\frac{p^{\prime}}{p+p^{\prime}}\]

which implies

\[\frac{p}{p+p^{\prime}}\cdot\frac{1}{n+1}\cdot\frac{(n+1)(p+p^{\prime})-n(n+1)( p+p^{\prime})^{2}}{p+p^{\prime}}\leq\frac{1}{2}-\frac{p^{\prime}}{p+p^{\prime}}\]

by using that \((1-x)^{n+1}<1-(n+1)x+n(n+1)x^{2}/2\) for any \(0<x<1\). Finally, \(\pi^{\prime}\) being better than \(\pi\) at timestep \(n\) implies \(\frac{-n}{2}\leq\left(\frac{1}{2}-\frac{p^{\prime}}{p+p^{\prime}}\right)\frac{ 1}{p}-\frac{1}{p+p^{\prime}}\), i.e., \(n\geq\frac{1}{p}\geq N\).

### Proof of Theorem 3

Proof.: For \(S,A,\varepsilon,\delta\) satisfying the assumptions of Theorem 3, we let \(p=\frac{16\varepsilon}{A^{S-1}}\) and define the family of MDPs \(\mathcal{M}_{j}\) for any \(j\in A^{S-1}\), displayed in Figure 3. There are \(A\) actions available in each of the \(S\) states, but the figure only displays actions with distinct transitions.

* In state \(s_{1}\), for any action \(a\), a reward of 1 is incurred, and there is probability \(p\) of getting into state \(s_{2}\), and probability \(1-p\) of staying in \(s_{1}\).
* In state \(s_{n}\) for \(n\in\{2,\ldots,S\}\), taking action \(j_{n-1}\) transitions to state \(s_{n+1}\) (where \(s_{S+1}=s_{1}\)) and every other action transitions to state \(s_{2}\), with no reward.

To summarize, it is very unlikely to get into state \(s_{2}\); but, once it has been entered, only one specific deterministic policy can allow an agent to get back to the favorable state \(s_{1}\).

Let \(\pi\) be a (possibly stochastic) policy. Let us write \(p_{i}^{j}=\pi(j_{i-1}|s_{i})\) for \(i\in\{2,...S\}\), and \(x_{\pi}^{j}=\prod_{i=2}^{S}p_{i}^{j}\). With \(P_{\pi,j}\) the transition matrix under \(\pi\) in MDP \(\mathcal{M}_{j}\), and \(\overline{P}_{\pi,j}=\lim_{T\rightarrow\infty}\frac{1}{T}\sum_{t=1}^{T}P_{\pi,j}^{t-1}\), we know that \(P_{\pi,j}\overline{P}_{\pi,j}=\overline{P}_{\pi,j}\). Since the MDP \(\mathcal{M}_{j}\) is unichain, by theorem A.2 of [18], all the rows of \(\overline{P}_{\pi,j}\) are identical to a vector \((\eta_{1},...\eta_{S})\in\Sigma_{S}\).

Figure 3: MDP \(\mathcal{M}_{j}\) (repeated from page 7)

We therefore deduce the following linear system:

\[\begin{cases}\eta_{1}=(1-p)\eta_{1}+p_{S}^{j}\eta_{S}\\ \eta_{2}=p\eta_{1}+\sum_{n=2}^{S}(1-p_{n}^{j})\eta_{n}\\ \forall n\in\{3,...S\},\eta_{n}=p_{n-1}^{j}\eta_{n-1}\\ \sum_{n=1}^{S}\eta_{n}=1\end{cases}\]

Injecting the third equation into the second yields \(\eta_{2}=p\eta_{1}+\sum_{n=2}^{S-1}(\eta_{n}-\eta_{n+1})+\eta_{S}-x_{\pi}^{j} \eta_{2}\) and finally \(x_{\pi}^{j}\eta_{2}=p\eta_{1}\). Therefore,

\[\eta_{1}\left(1+\frac{p(1+p_{2}^{j}+p_{2}^{j}p_{3}^{j}+...+x_{\pi}^{j})}{x_{\pi }^{j}}\right)=1\ \implies\ \eta_{1}\leq\frac{1}{1+\frac{p}{x_{\pi}^{j}}},\]

using that \(1+p_{1}^{j}+p_{2}^{j}p_{3}^{j}+...+x_{\pi}^{j}\geq 1\). Finally,

\[g_{\pi}^{j}\leq\frac{1}{1+\frac{p}{x_{\pi}^{j}}}.\]

The optimal policy in \(\mathcal{M}_{j}\) is to play action \(j_{n-1}\) in state \(s_{n}\) for \(n\in\{2,\ldots,S\}\), so the above equation becomes \(\eta_{1}(1+p(S-1))=1\) and the optimal gain in \(\mathcal{M}_{j}\) is \(g^{j}=\frac{1}{1+p(S-1)}\).

If a policy \(\pi\) is \(\varepsilon\)-optimal in \(\mathcal{M}_{j}\), we have

\[g_{\pi}^{j} \geq g^{j}-\varepsilon\] \[\frac{1}{1+\frac{p}{x_{\pi}^{j}}} \geq\frac{1}{1+p(S-1)}-\varepsilon\] \[(S-1)p \geq\frac{p}{x_{\pi}^{j}}-\varepsilon\left(1+(S-1)p+\frac{p}{x_{ \pi}^{j}}+\frac{(S-1)p^{2}}{x_{\pi}^{j}}\right)\] \[x_{\pi}^{j} \geq p\frac{1-\varepsilon\left(1+(S-1)p\right)}{(S-1)p+\varepsilon \left(1+(S-1)p\right)}\] \[(S-1)x_{\pi}^{j} \geq\frac{1}{1+\varepsilon\frac{((S-1)p+1)^{2}}{(S-1)p(1- \varepsilon-\varepsilon(S-1)p)}}\]

Since \((S-1)p\leq\frac{1}{4}\), \((1+(S-1)p)^{2}\leq 2\). Since we also have \(\varepsilon<\frac{1}{4}\), \((1-\varepsilon-\varepsilon(S-1)p)\geq\frac{1}{2}\). Therefore, \((S-1)x_{\pi}^{j}\geq\frac{1}{1+\frac{4\varepsilon}{(S-1)p}}\). Finally, \(\frac{4\varepsilon}{(S-1)p}\geq 1\It follows that \(\sum_{j}\int_{\hat{\pi}}P(\hat{\pi}|\mathcal{E})y_{\hat{\pi}}^{j}d\hat{\pi}\leq \frac{8\varepsilon}{p}\) and that there exists \(j\) such that \(\int_{\hat{\pi}}P_{j}(\hat{\pi}|\mathcal{E})y_{\hat{\pi}}^{j}d\hat{\pi}=\int_{ \hat{\pi}}P(\hat{\pi}|\mathcal{E})y_{\hat{\pi}}^{j}d\hat{\pi}\leq\frac{8 \varepsilon}{pA^{S-1}}\). For this value of \(j\),

\[P_{j}(\mathcal{E}^{\prime}|\mathcal{E})=1-\int_{\hat{\pi}}P_{j}(\hat{\pi}| \mathcal{E})y_{\hat{\pi}}^{j}d\hat{\pi}\geq 1-\frac{8\varepsilon}{pA^{S-1}}= \frac{1}{2}\]

The overall failure probability in \(\mathcal{M}_{j}\) is thus \(P_{j}(\mathcal{E}^{\prime})\geq P_{j}(\mathcal{E})P_{j}(\mathcal{E}^{\prime}| \mathcal{E})\geq\frac{1}{16}>\delta\), which is a contradiction. Therefore, for all \((\varepsilon,\delta)\)-PAC algorithm there exists an instance \(\mathcal{M}_{j}\) for which \(P_{j}(\tau>1/p)\geq 1/8\), hence the expected sample complexity under this instance is larger than \(1/8p\).

Let us finally compute the diameter and bias span of MDP \(\mathcal{M}_{j}\). For states \(s_{1}\) and \(s_{S}\), the only (deterministic) policy with finite traveling time from \(s_{1}\) to \(s_{j}\) is the policy which plays action \(j_{n-1}\) in state \(s_{n}\) for any \(n\in\{2,...S\}\). As the bottleneck of exploration is the small probability to transition from \(s_{1}\) to \(s_{2}\), the travel time is biggest from \(s_{1}\) to \(s_{2}\). Therefore,

\[D=\min_{\pi:\mathcal{S}\rightarrow\mathcal{A}}\mathbb{E}[\min\{t>0,s_{t}=s_{1} \}|s_{0}=s_{S},\forall t^{\prime},a_{t^{\prime}}=\pi(s_{t^{\prime}})]=\frac{1} {p}+S-1=S-1+\frac{A^{S-1}}{16\varepsilon}\]

As for the bias span, by fixing \(b(s_{2})=0\), the Poisson equations (5) yield for \(i\in\{3,...S\}\) that \(b(s_{i})=(i-2)g\), and finally \(b(s_{1})=(S-1)g\leq S\).

Figure 5: An SSP-MDP and its average reward transformation

Complements for Section 6

### Correctness of the stopping rule

Proof.: _(Theorem 5)_ We first note that \(g_{\hat{\pi}_{n}}=\overline{P}_{\hat{\pi}_{n}}\overline{\tau}_{\hat{\pi}_{n}}= \overline{P}_{\hat{\pi}_{n}}\left[\overline{\tau}_{\hat{\pi}_{n}}+P_{\hat{\pi}_ {b}}b_{n}-b_{n}\right]\) where we use that \(\overline{P}_{\hat{\pi}_{n}}P_{\hat{\pi}_{n}}=\overline{P}_{\hat{\pi}_{n}}\).

Since for all \(s,a\), \(p_{s,a}b_{n}\geq L_{s,a}^{n}(b_{n};\delta)\), we have

\[g_{\hat{\pi}_{n}}\geq\overline{P}_{\hat{\pi}_{n}}\left[\overline{\tau}_{\hat{ \pi}_{n}}+\left(L_{s,\hat{\pi}_{n}(s)}^{n}(b_{n};\delta)\right)_{s}-b_{n} \right]=\overline{P}_{\hat{\pi}_{n}}\left(\max_{a}I_{s,a}^{n,\flat}(b_{n}; \delta)\right)_{s}\]

by the definition of \(\hat{\pi}_{n}\), and finally \(g_{\hat{\pi}_{n}}\geq\min_{s}\max_{a}I_{s,a}^{n,\flat}(b_{n};\delta)\).

For \(\pi\) a stationary deterministic optimal policy satisfying the optimal Poisson equation (5) (that exists according to Theorem 8.4.3 and 8.4.4 of [18]),

\[g =g_{\pi}=\overline{P}_{\pi}\overline{\tau}_{\pi}=\overline{P}_{ \pi}\left[\overline{\tau}_{\pi}+P_{\pi}b_{n}-b_{n}\right]\leq\overline{P}_{\pi }\left[\overline{\tau}_{\pi}+\left(U_{s,\hat{\pi}_{n}(s)}^{n}(b_{n};\delta) \right)_{s}-b_{n}\right]\] \[\leq\overline{P}_{\pi}\left(\max_{a}I_{s,a}^{n,\sharp}(b_{n}; \delta)\right)_{s}\]

and finally \(g\leq\max_{s}\max_{a}I_{s,a}^{n,\sharp}(b_{n};\delta)\)

Finally, since \(g\) is the gain of an optimal policy, \(g\geq g_{\hat{\pi}_{n}}\). 

Proof.: _(Theorem 6)_ By Theorem 5, we have

\[\mathds{P}\left[\tau<+\infty,g_{\widetilde{\pi}}<g^{\star}-\varepsilon\right]\] \[\qquad\leq\mathds{P}\left[\exists n,g^{\star}-g_{\widetilde{\pi} _{n}}>\varepsilon,\max_{s}\max_{a}I_{s,a}^{n,\sharp}(b_{n};\delta)-\min_{s} \max_{a}I_{s,a}^{n,\flat}(b_{n};\delta)\leq\varepsilon\right]\] \[\qquad\leq\mathds{P}\left[\exists n,\exists s,a,p_{s,a}b_{n} \notin\left[L_{s,a}(b_{n};\delta),U_{s,a}^{n}(b_{n};\delta)\right]\right]\]

Using further a union bound and the definitions of \(U\) and \(L\),

\[\mathds{P}\left[\tau<+\infty,g_{\widetilde{\pi}}<g^{\star}-\varepsilon\right]\] \[\qquad\leq\mathds{P}\left[\exists n,\exists s,a,p_{s,a}b_{n}\geq U _{s,a}^{n}(b_{n};\delta)\right)\cup\exists n,\exists s,a,p_{s,a}b_{n}\leq L_ {s,a}^{n}(b_{n};\delta))\big{]}\] \[\qquad\leq\mathds{P}\left[\exists n,\exists s,\exists a,\mathrm{ KL}(\hat{p}_{s,a}^{n},p_{s,a})>\frac{x\left(\delta,N_{s,a}^{n}\right)}{N_{s,a}^{ n}}\right]\] \[\qquad\leq\delta\,\]

where the last inequality follows from the concentration result given in Lemma 5.

### A sample complexity analysis

In this section, we present and analyze an algorithm for the generative model setting using the simplest possible sampling rule, uniform sampling, together with the adaptive stopping rule (6) for \(b_{n}=\hat{b}_{n}\) such that \(\hat{b}_{n}\) is the optimal bias function in the AR-MDP with transition probabilities given by \((\hat{p}_{s,a}^{n})_{s,a}\) (normalized so that the bias in the first state is always 0). The pseudo-code of the algorithm is given in Algorithm 4.

**Remark 1**.: _The quantities \(U_{s,a}^{n}\) and \(L_{s,a}^{n}\) in Definition 2 are solutions to complex optimization problems and can be expensive to compute. Several modifications are possible to prevent long run times. First, it is possible not to check for stopping at each time step while preserving correctness. Second, the stopping rule can be relaxed with the following looser confidence intervals which satisfy \(U_{s,a}^{n}(b_{n};\delta)\leq\widetilde{U}_{s,a}^{n}(b_{n};\delta)\) and \(L_{s,a}^{n}(b_{n};\delta)\geq\widetilde{L}_{s,a}^{n}(b_{n};\delta)\) using Pinsker's inequality:_

\[\widetilde{U}_{s,a}^{n}(b_{n};\delta)=\hat{p}_{s,a}^{n}b_{n}+||b_{n}||_{\infty }\sqrt{\frac{2x(\delta,N_{s,a}^{n})}{N_{s,a}^{n}}},\ \ \widetilde{L}_{s,a}^{n}(b_{n}; \delta)=\hat{p}_{s,a}^{n}b_{n}-||b_{n}||_{\infty}\sqrt{\frac{2x(\delta,N_{s,a}^{ n})}{N_{s,a}^{n}}}\.\]

_Theorem 6 (as well as our sample complexity analysis to follow) still applies for these alternative choices which are easier to compute but yield looser confidence intervals and thus in theory a larger sample complexity._

**Data:** Accuracy \(\varepsilon\in(0,1)\), confidence level \(\delta\in(0,1)\)

\(n=0\), \(b_{n}=0\), \(I_{s,a}^{n,\sharp}(b_{n},\delta)=1\), \(I_{s,a}^{n,\sharp}(b_{n},\delta)=0\);

**while**\(\max_{s}\max_{a}I_{s,a}^{n,\sharp}(b_{n},\delta)-\min_{s}\max_{a}I_{s,a}^{n, \flat}(b_{n};\delta)>\varepsilon\)**do**

**for**\((s,a)\in\mathcal{S}\times\mathcal{A}\)**do**

**Sample a reward and a next state of \(s,a\)**

**end**

\(n=n+SA\);

Compute \(\hat{p}_{s,a}^{n}\) for each \((s,a)\)

Compute \(b_{n}=\hat{b}_{n}\) the bias of the empirical MDP, with rewards \(\overline{r}_{s,a}\) and transitions \(\hat{p}_{s,a}^{n}\);

Compute \(I_{s,a}^{n,\sharp}(b_{n},\delta)\) and \(I_{s,a}^{n,\flat}(b_{n},\delta)\) using \(U,L\) from Definition 2 (or the relaxations (7))

**end**

**return**\(\hat{\pi}_{n}=\left(\arg\max_{a}I_{s,a}^{n,\flat}(b_{n};\delta)\right)_{s}\)

**Algorithm 4**: Uniform sampling combined with adaptive stopping

We analyze Algorithm 4 for unichain MDPs, for which we are able to derive a simulation lemma, akin to those existing in the discounted [14] or SSP [21] settings. It relates the gains and the bias in two AR-MDPs that are close enough.

A simulation lemma for AR-MDPsTo introduce this result, we need the following definitions.

**Definition 3**.: _Let \(\mathcal{D}_{M}\) be the set of policies generating a unichain Markov chain on a weakly communicating MDP \(M\). For \(M\) an MDP and \(\pi\in\mathcal{D}_{M}\),_

\[\Pi_{\pi}=\left(\begin{array}{c|ccc}1&0&...&0\\ \hline 1&\\...&I_{S-1}&\end{array}\right)-\left(\begin{array}{ccc}0&P_{\pi}(s_{1},s_{2} )&...&P_{\pi}(s_{1},s_{S})\\...&...&...\\ 0&P_{\pi}(s_{S},s_{2})&...&P_{\pi}(s_{S},s_{S})\end{array}\right)\]

_is the matrix such that the Poisson equations (3) and (4) rewrite as \(\Pi_{\pi}h_{\pi}=\overline{r}_{\pi}\) with \(\overline{r}\) the average reward vector and \(h_{\pi}=(g_{\pi},b_{\pi}(s_{2})-b_{\pi}(s_{1}),...b_{\pi}(s_{S})-b_{\pi}(s_{1}))\). We further define_

\[\delta_{1}=\min_{\pi,g_{\pi}\neq g^{\star}}|g_{\pi}-g^{\star}|,\quad\Delta_{1} =\max_{\pi\in\mathcal{D}_{M}}||\Pi_{\pi}^{-1}||\quad\text{and}\quad\Delta_{2}= \max_{\pi\in\mathcal{D}_{M}}||h_{\pi}||_{\infty}\]

_where the norm used is the operator norm associated to the infinity norm, \(||A||=\sup_{||x||_{\infty}=1}||Ax||_{\infty}\)._

We can show with results from [18] that \(\Pi_{\pi}\) is invertible for any unichain policy \(\pi\), hence \(\Delta_{1}\) is well-defined. Indeed, by Theorem A.7 and (A.4), there exist solutions to the Poisson equations. By Theorem 8.2.6, the solution is unique up to an additive vector in the kernel of \((I-P)\), which is of dimension 1 for unichain MDPs as proved in Theorem A.5. Therefore, since in the unichain setting solutions to \(\Pi_{\pi}H_{\pi}=\overline{r}_{\pi}\) are exactly solutions for (4), we can conclude.

The following two lemmas can be extracted from the proofs of Lemmas 7 and 8 of [4]. For completeness, we provide a full proof in Appendix D.3.

**Lemma 3** (Simulation lemma).: _Let \(p^{\prime}:\mathcal{S}\times\mathcal{A}\rightarrow\Sigma_{\mathcal{S}}\) be the transition probability matrix of a weakly communicating MDP \(M^{\prime}\) with the same reward function \(\overline{r}\) as \(M\). For a given policy \(\pi\) that is unichain on \(M\), if for all \((s,a)\) we have \(||p^{\prime}_{s,a}-p_{s,a}||_{1}\leq\frac{x}{\Delta_{1}(\Delta_{2}+x)}\) then \(||h_{\pi}-h_{\pi}^{M^{\prime}}||_{\infty}\leq x\)._

**Lemma 4**.: _Suppose \(M\)**unichain**. Let \(p^{\prime}:\mathcal{S}\times\mathcal{A}\rightarrow\Sigma_{\mathcal{S}}\) be the transition probability matrix of a unichain MDP \(M^{\prime}\) with the same reward function \(\overline{r}\) as \(M\). If for all \(s,a\) we have \(||p^{\prime}_{s,a}-p_{s,a}||_{1}\leq\frac{x}{\Delta_{1}(\Delta_{2}+x)}\) where \(x\leq\delta_{1}/2\), then \(||b^{\star}-b^{\star,M^{\prime}}||_{\infty}\leq x\)._

It is necessary to suppose \(M\) unichain, because without this assumption, there is no guarantee that there exists a policy \(\pi\) that is optimal in \(M^{\prime}\) and unichain in \(M\).

Sample complexity boundUsing these results together with our previous concentration result for the transitions (Lemma 5) permits to prove the following high-probability bound on the sample complexity of Algorithm 4, which features the quantity \(\Gamma_{\mathcal{M}}:=\Delta_{1}(\Delta_{2}+\delta_{1}/2)\).

**Theorem 9**.: _For \(\varepsilon\leq\delta_{1}\), with probability larger than \(1-\delta\), the sample complexity of Algorithm 4 on a unichain MDP satisfies_

\[\tau=\widetilde{\mathcal{O}}\left(\frac{SA((H+1)\vee\Gamma_{\mathcal{M}})^{2}}{ \varepsilon^{2}}\left(\log\left(\frac{1}{\delta}\right)+S\right)\right)\.\]

Just like DFE, Algorithm 4 is an \((\varepsilon,\delta)\)-PAC algorithm using a generative model that does not require any prior knowledge on the MDP, as its stopping rule is fully data-dependent. Their sample complexity in the regime of small \(\varepsilon\) and small \(\delta\) both scale in \((SAc(\mathcal{M})/\varepsilon^{2})\log(1/\delta)\) but for different complexity quantities: \(c_{1}(\mathcal{M})=D\) for Algorithm 1 while \(c_{2}(\mathcal{M})=((H+1)\vee\Gamma_{\mathcal{M}})^{2}\) for Algorithm 4. We know from the lower bound that the latter has to be larger for some MDPs, but so far we did not manage to quantify their difference (even if we suspect that \(c_{2}\) can be much larger than \(c_{1}\)). Besides this, we hope that the sample complexity of Algorithm 4 can be significantly reduced by the use of smarter (online) sampling rules.

### Simulation lemmas

Proof.: _(Lemma 3)_ First, let us fix any policy \(\pi\) and \(y<\frac{1}{||\Pi_{\pi}^{-1}||}\) such that \(\max_{s,a}||p_{s,a}-p^{\prime}_{s,a}||_{1}\leq y\). We thus have \(\Pi_{\pi}h_{\pi}=\Pi^{\prime}_{\pi}h^{M^{\prime}}_{\pi}\). \(\Pi_{\pi}-\Pi^{\prime}_{\pi}=P^{\prime}_{\pi}-P_{\pi}\), and therefore \(||\Pi_{\pi}-\Pi^{\prime}_{\pi}||<y\).

\[h_{\pi}(\Pi_{\pi}-\Pi^{\prime}_{\pi})=\Pi^{\prime}_{\pi}(h^{M^{ \prime}}_{\pi}-h_{\pi})\] \[||\Pi^{\prime-1}_{\pi}||\cdot||h_{\pi}||\cdot y>||h^{M^{\prime}}_ {\pi}-h_{\pi}||_{\infty}\]

Moreover,

\[||\Pi^{\prime-1}_{\pi}||\leq||\Pi^{-1}_{\pi}||\cdot||\Pi_{\pi}\Pi^ {\prime-1}_{\pi}||-||\Pi^{-1}_{\pi}||+||\Pi^{-1}_{\pi}||\] \[||\Pi^{\prime-1}_{\pi}||\leq||\Pi^{-1}_{\pi}||\cdot||\Pi_{\pi}\Pi ^{\prime-1}_{\pi}-I||+||\Pi^{-1}_{\pi}||\] \[||\Pi^{\prime-1}_{\pi}||\leq||\Pi^{-1}_{\pi}||\cdot||\Pi^{\prime- 1}_{\pi}||\cdot y+||\Pi^{-1}_{\pi}||\] \[||\Pi^{\prime-1}_{\pi}||\leq\frac{||\Pi^{-1}_{\pi}||}{1-||\Pi^{- 1}_{\pi}||\cdot y}\]

since \(||\Pi^{-1}_{\pi}||\cdot y<1\).

And therefore,

\[||h_{\pi}-h^{M^{\prime}}_{\pi}||<\frac{||\Pi^{-1}_{\pi}||\cdot||h_{\pi}||\cdot y }{1-||\Pi^{-1}_{\pi}||\cdot y}\leq\frac{\Delta_{1}\Delta_{2}y}{1-\Delta_{1}y}\]

Therefore, for any \(x\), taking \(y=\frac{x}{\Delta_{1}\Delta_{2}+x\Delta_{1}}\) gives the result. 

Proof.: _(Lemma 4)_ Fix \(x<\delta_{1}/2\), and let \(\pi\) be an optimal policy in \(M^{\prime}\). For any policy \(\pi^{\prime}\), using Lemma 3 successively with policy \(\pi\) and policy \(\pi^{\prime}\) yields \(g_{\pi}>g^{\prime}_{\pi}-x\geq g^{\prime}_{\pi^{\prime}}-x>g_{\pi^{\prime}}-2x\) and finally \(g_{\pi}>g_{\pi^{\prime}}-\delta_{1}\), which implies by definition of \(\delta_{1}\) that \(g_{\pi}\geq g_{\pi^{\prime}}\), and therefore \(\pi\) is optimal in \(M\). Hence applying Lemma 3 to \(\pi\) yields the result. 

### Proof of Theorem 9

Fix a unichain MDP. We prove in Theorem 10 below an explicit upper bound on the sample complexity of Algorithm 4. For \(\varepsilon\leq\delta_{1}\), we deduce from it that

\[\tau=\widetilde{\mathcal{O}}\left(\frac{SA((H+1)\vee\Gamma_{\mathcal{M}})^{2}} {\varepsilon^{2}}\left(\log\left(\frac{1}{\delta}\right)+S\right)\right)\,\]

as claimed in Theorem 9.

**Theorem 10**.: _With probability larger than \(1-\delta\), the sample complexity of Algorithm 4 is smaller than_

\[SA(S-1)\frac{y}{\widetilde{C}}\left(1+\frac{2C}{y}\log\left(\frac{y+2}{C} \right)\right)\]

_where \(y=1+\frac{\ln SA/\delta}{S-1}\) and \(C=\frac{1}{288}\frac{\min(\delta_{1},\varepsilon)^{2}}{\max(H+1,\Gamma_{ \mathcal{M}})^{2}}\)._Proof of Theorem 10We write \(N_{t}=\left\lfloor\frac{t}{3}\frac{\delta}{\delta}\right\rfloor\) to denote the number of visits in each state action pair at time step \(t\). We denote by \(b\) the optimal bias function (normalized to be equal to zero in the first state). We set \(\xi\in(0,\frac{1}{6})\) to be determined later and define the three events

\[\mathcal{E} = \left\{\forall t,\forall s,a,||\hat{p}_{s,a}^{t}-p_{s,a}||_{1} \leq\sqrt{\frac{2x(\delta,N_{t})}{N_{t}}}\right\}\] \[\mathcal{E}_{1}(\xi,t) = \left\{||\hat{b}_{t}-b||_{\infty}<\xi\varepsilon\right\}\] \[\mathcal{E}_{2}(\xi,t) = \left\{\forall s,a,||\hat{b}_{t}||_{\infty}\sqrt{2\frac{x(\delta, N_{t})}{N_{t}}}<\min\left(\left(\frac{1}{4}-\frac{3}{2}\xi\right)\varepsilon,|| \hat{b}_{t}||_{\infty}\right)\right\}\]

Using Lemma 5, we know that \(\mathbb{P}(\mathcal{E})\geq 1-\delta\).

**We first prove that, if \(\mathcal{E}\cap\mathcal{E}_{1}(\boldsymbol{\xi},\boldsymbol{t})\cap\mathcal{E }_{2}(\boldsymbol{\xi},\boldsymbol{t})\) is satisfied for a certain time step \(\boldsymbol{t}\), then the stopping condition \((\ref{eq:stopping condition})\) is satisfied at this time step.** To this end, let us assume that \(\mathcal{E}\cap\mathcal{E}_{1}(\xi,t)\cap\mathcal{E}_{2}(\xi,t)\) holds for a fixed \(t\) and \(\xi\). We recall the definition of the confidence bounds \(U\) and \(L\) from Definition 2 and their relaxations \(\widetilde{U}\) and \(\widetilde{L}\) defined in (7). For all \((s,a)\), we have

\[\overline{r}_{s,a}+U_{s,a}^{t}(\hat{b}_{t};\delta)-\hat{b}_{t}(s) \leq\overline{r}_{s,a}+\widetilde{U}_{s,a}^{t}(\hat{b}_{t};\delta) -\hat{b}_{t}(s)\] \[=\overline{r}_{s,a}+\hat{p}_{s,a}^{t}\hat{b}_{t}+\sqrt{\frac{2x( \delta,N_{s,a}^{t})}{N_{s,a}^{t}}}||\hat{b}_{t}||_{\infty}-\hat{b}_{t}(s)\] \[\leq\overline{r}_{s,a}+p_{s,a}b+(\hat{p}_{s,a}^{t}-p_{s,a})b+ \hat{p}_{s,a}^{t}(\hat{b}_{t}-b)\] \[\qquad+\sqrt{\frac{2x(\delta,N_{s,a}^{t})}{N_{s,a}^{t}}}||\hat{b} _{t}||_{\infty}-\hat{b}_{t}(s)+b(s)-b(s)\] \[\leq\overline{r}_{s,a}+p_{s,a}b-b(s)+||b-\hat{b}_{t}+\hat{b}_{t} ||_{\infty}\sqrt{\frac{2x(\delta,N_{s,a}^{t})}{N_{s,a}^{t}}}\] \[\qquad+||\hat{b}_{t}-b||_{\infty}+\sqrt{\frac{2x(\delta,N_{s,a}^{ t})}{N_{s,a}^{t}}}||\hat{b}_{t}||_{\infty}+||\hat{b}_{t}-b||_{\infty}\] \[\leq\overline{r}_{s,a}+p_{s,a}b-b(s)+\xi\varepsilon+\left(\frac{ 1}{4}-\frac{3}{2}\xi\right)\varepsilon+\xi\varepsilon+\left(\frac{1}{4}-\frac {3}{2}\xi\right)\varepsilon+\xi\varepsilon\] \[\leq\overline{r}_{s,a}+p_{s,a}b-b(s)+\frac{1}{2}\varepsilon\]

Similarly, we can prove that

\[\overline{r}_{s,a}+L_{s,a}^{t}-\hat{b}_{t}(s)\leq\overline{r}_{s,a}+p_{s,a}b- b(s)-\frac{1}{2}\varepsilon.\]

Finally, since \(b\) is a solution to the optimal Poisson equation (5), we know that

\[\max_{s}\max_{a}\left(\overline{r}_{s,a}+p_{s,a}b-b(s)\right)-\min_{s}\max_{a }\left(\overline{r}_{s,a}+p_{s,a}b-b(s)\right)=0\]

It follows that

\[\max_{s}\max_{a}\left(\overline{r}_{s,a}+U_{s,a}^{t}(\hat{b}_{t};\delta)-\hat{ b}_{t}(s)\right)-\min_{s}\max_{a}\left(\overline{r}_{s,a}+L_{s,a}^{t}(\hat{b}_{t}; \delta)-\hat{b}_{t}(s)\right)\leq\varepsilon\]

and the stopping condition is met.

**Then, we establish a sufficient condition on \(\boldsymbol{\xi}\) and \(\boldsymbol{t}\) to have \(\boldsymbol{\mathcal{E}}\subseteq\boldsymbol{\mathcal{E}_{1}(\xi,t)}\).**

Assume that the event \(\mathcal{E}\) holds. Let \(\xi^{\prime}=\min(\xi\varepsilon,\delta_{1}/2)\). From Lemma 4, if for all \(s,a\) we have \(||\hat{p}_{t}(s,a)-p(s,a)||_{1}\leq\frac{\xi^{\prime}}{\Gamma_{\mathcal{M}}}\), then \(||b-\hat{b}_{t}||_{\infty}\leq\xi^{\prime}\), which implies \(\mathcal{E}_{1}(\xi,t)\). Hence, on \(\mathcal{E}\), a sufficient condition for \(\mathcal{E}_{1}(\xi,t)\) to hold is

\[\sqrt{\frac{2x(\delta,N_{t})}{N_{t}}}<\frac{\xi^{\prime}}{\Gamma_{\mathcal{M}}}.\]Introducing \(c(\xi)=\frac{1}{2}\left(\frac{\xi^{\prime}}{\Gamma_{\mathcal{M}}}\right)^{2}\), this is equivalent to

\[\log(SA/\delta)+(S-1)\left(1+\log(1+N_{t}/(S-1))\right)\leq c(\xi)N_{t}\]

and finally to

\[c(\xi)\frac{N_{t}}{S-1}-\log\left(1+\frac{N_{t}}{S-1}\right)\geq 1+\frac{\log(SA/ \delta)}{S-1}. \tag{10}\]

**Next, we establish a sufficient condition on \(\xi\) and \(t\) to have \(\mathcal{E}_{1}(\xi,t)\subseteq\mathcal{E}_{2}(\xi,t)\).** We first remark that when \(\mathcal{E}_{1}(\xi,t)\) holds

\[\|\hat{b}_{t}\|_{\infty}\sqrt{\frac{2x(\delta,N_{t})}{N_{t}}} \leq \left(\|\hat{b}_{t}-b\|_{\infty}+\|b\|_{\infty}\right)\sqrt{\frac {2x(\delta,N_{t})}{N_{t}}}\] \[\leq \left(\xi\varepsilon+H\right)\sqrt{\frac{2x(\delta,N_{t})}{N_{t}}}\]

Therefore, a sufficient condition for \(\mathcal{E}_{2}(\xi,t)\) to hold is

\[\sqrt{\frac{2x(\delta,N_{t})}{N_{t}}}<\min\left(\frac{1-6\xi}{4(H+\xi \varepsilon)}\varepsilon,1\right)\]

Introducing \(c^{\prime}(\xi)=\min\left(\frac{1}{32}\left(\frac{1-6\xi}{H+\xi\varepsilon} \right)^{2}\varepsilon^{2},\frac{1}{2}\right)\), this is equivalent to

\[\log(SA/\delta)/(S-1)+1+\log(1+N_{t}/(S-1))<c^{\prime}(\xi)N_{t}/(S-1)\]

and finally to

\[c^{\prime}(\xi)\frac{N_{t}}{S-1}-\log\left(1+\frac{N_{t}}{S-1}\right)>1+\frac{ \log(SA/\delta)}{S-1}. \tag{11}\]

**Finally, we put things together.** The conditions (10) and (11) are of similar form. Defining \(C(\xi)=\min(c(\xi),c^{\prime}(\xi))\), for any \(\xi\in(0,1/6)\) the condition

\[C(\xi)\frac{N_{t}}{S-1}-\log\left(1+\frac{N_{t}}{S-1}\right)>1+\frac{\log(SA/ \delta)}{S-1} \tag{12}\]

is a sufficient condition on \(t\) to have \(\mathcal{E}\subseteq\mathcal{E}\cap\mathcal{E}_{1}(\xi,t)\cap\mathcal{E}_{2}( \xi,t)\). If follows that for \(t\) satisfying (12), the algorithm has stopped before time \(t\), with probability larger than \(1-\delta\).

We observe that the larger \(C(\xi)\), the less stringent the condition is on \(t\), but finding the value of \(\xi\in(0,1/6)\) that maximizes \(C(\xi)\) leads to tedious calculations. Instead we go for finding a lower bound on \(C(1/12)\), denoted by \(C\), which also provides a valid sufficient condition on \(t\):

\[C\frac{N_{t}}{S-1}-\log\left(1+\frac{N_{t}}{S-1}\right)>1+\frac{\log(SA/\delta )}{S-1}.\]

Using Lemma 6, letting \(y=1+\frac{\log(SA/\delta)}{S-1}\), this condition is satisfied for

\[\frac{N_{t}}{S-1}\geq\frac{y}{C}\left(1+\frac{2C}{y}\log\left(\frac{y+2}{C} \right)\right)\.\]

To conclude the proof,where we used twice that \(\varepsilon\leq 1\). Hence a valid lower bound on \(C(1/12)\) is

\[C=\frac{1}{288}\frac{\min(\delta_{1},\varepsilon)^{2}}{\max(H+1,\Gamma_{\mathcal{ M}})^{2}}.\]

## Appendix E Auxiliary results

In this section, we restate some useful result from the literature. The first is a time uniform concentration result on the transition probabilities that can be obtained as a consequence of Lemma 9 from [1].

**Lemma 5**.: _For all \(\delta\in(0,1)\), with \(x(\delta,y)=\log(SA/\delta)+(S-1)\log\left(e(1+y/(S-1))\right)\),_

\[\mathds{P}\left[\exists n\in\mathds{N}:N_{s,a}^{n}\mathrm{KL}(\hat{p}_{s,a}^{n },p_{s,a})>x(\delta,N_{s,a}^{n})\right]\leq\frac{\delta}{SA}\]

_Furthermore, letting \(B(n,\delta):=\sqrt{\frac{2x(\delta,n)}{n}}\), the event_

\[\mathcal{E}=\left(\forall s,a,\forall n\geq 1,\|\hat{p}_{s,a}^{n}-p_{s,a}\|_{1 }\leq B(N_{s,a}^{n},\delta)\right)\]

_holds with probability \(1-\delta\)._

Proof.: With \(\hat{p}_{s,a}(m)\) the empirical estimate of \(p_{s,a}\) once \(m\) samples have been collected in \(s,a\), arbitrarily setting \(\hat{p}_{s,a}(0)\) as the constant vector \(1/S\),

\[\mathds{P}\left[\exists n\in\mathds{N}:N_{s,a}^{n}\mathrm{KL}( \hat{p}_{s,a}^{n},p_{s,a})>x(\delta,N_{s,a}^{n},S)\right]\] \[\qquad\leq\mathds{P}\left[\exists n,m\in\mathds{N}:N_{s,a}^{n}=m,N_{s,a}^{n}\mathrm{KL}(\hat{p}_{s,a}^{n},p_{s,a})>x(\delta,N_{s,a}^{n},S)\right]\] \[\qquad\leq\mathds{P}\left[\exists m\in\mathds{N}:m\mathrm{KL}( \hat{p}_{s,a}(m),p_{s,a})>x(\delta,m,S)\right]\] \[\qquad\leq\frac{\delta}{SA}\]

by Lemma 9 from [1]. The second claim stems from a union bound over \(S,A\) and Pinsker's inequality applied to the first claim.

The second result is an inversion lemma, which allows to get explicit sample complexity bounds.

**Lemma 6** (Lemma 15 from [13]).: _Let \(n\geq 1\) and \(a,b,c,d>0\). If \(n\Delta^{2}\leq a+b\log(c+dn)\) then_

\[n\leq\frac{1}{\Delta^{2}}\left[a+b\log\left(c+\frac{d}{\Delta^{4}}(a+b(\sqrt{ c}+\sqrt{d}))^{2}\right)\right]\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our claims are that it is not possible to feasibly estimate H (shown in Theorem 1), that our new algorithm (Algorithm 1) requires no prior knowledge, is \((\varepsilon,\delta)\)-PAC, and near optimal (Theorem 2); that, in the online setting, the sample complexity of best policy identification does not scale polynomially with \(H\) (Theorem 3); that our new proposed stopping rule is \((\varepsilon,\delta)\)-PAC (Theorem 6). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of the paper are addressed in the conclusion Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Some of the more general assumptions (prior knowledge of the mean knowledge, generative model) are listed in the introduction. We have stated some proofs of past results for completeness when relevant. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: We have made no experiments Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: We have made no experiments Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: We have made no experiments Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: We have made no experiments Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). ** The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: We have made no experiments Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Our work is theoretical enough to not cause any ethical concerns Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work is theoretical enough that its societal impact is negligible Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not have any experiments, and release no models Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We use no existing assets Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We release no new assets Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We have not used crowdsourcing or research with human subjects Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We have not used crowdsourcing or research with human subjects Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. *