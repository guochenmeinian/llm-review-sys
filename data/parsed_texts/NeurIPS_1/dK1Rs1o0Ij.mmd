# Reimagining Synthetic Tabular Data Generation through Data-Centric AI:

A Comprehensive Benchmark

Lasse Hansen

A.L.2

Nabeel Seedat

Mihaela van der Schaar

Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK Faculty of Organisational Sciences, University of Belgrade, Serbia

###### Abstract

Synthetic data serves as an alternative in training machine learning models, particularly when real-world data is limited or inaccessible. However, ensuring that synthetic data mirrors the complex nuances of real-world data is a challenging task. This paper addresses this issue by exploring the potential of integrating data-centric AI techniques which profile the data to guide the synthetic data generation process. Moreover, we shed light on the often ignored consequences of neglecting these data profiles during synthetic data generation -- despite seemingly high statistical fidelity. Subsequently, we propose a novel framework to evaluate the integration of data profiles to guide the creation of more representative synthetic data. In an empirical study, we evaluate the performance of five state-of-the-art models for tabular data generation on eleven distinct tabular datasets. The findings offer critical insights into the successes and limitations of current synthetic data generation techniques. Finally, we provide practical recommendations for integrating data-centric insights into the synthetic data generation process, with a specific focus on classification performance, model selection, and feature selection. This study aims to reevaluate conventional approaches to synthetic data generation and promote the application of data-centric AI techniques in improving the quality and effectiveness of synthetic data.

## 1 Introduction

Machine learning has become an essential tool across various industries, with high-quality data representative of the real world being a crucial component for training accurate models that generalize [1, 2, 3]. In cases where data access is restricted or insufficient synthetic data has emerged as a viable alternative [4, 5]. The purpose of synthetic data is to generate training data that closely mirrors real-world data, enabling the effective use of models trained on synthetic data on real data. Moreover, synthetic data is used for a variety of different uses, including privacy (i.e. to enable data sharing, [6, 7]), competitions [8] fairness [9, 10], and improving downstream models [11, 12, 13, 14].

However, generating high-quality synthetic data that adequately captures the nuances of real-world data, remains a challenging task. Despite significant strides in synthetic data with generative models, they sometimes fall short in replicating the complex subtleties of real-world data, particularly when dealing with messy, mislabeled or biased data. For instance, regarding fairness, [15] have shown that such gaps can lead to flawed conclusions and unreliable predictions on subpopulations, thereby restricting the practical usage of synthetic data.

The ability of synthetic data to capture the subtle complexities of real-world data is crucial, particularly in contexts where these issues might surface during deployment. Inaccurate synthetic data can not only hamper predictive performance but also result in improper model selection and distorted assessments of feature importance, thereby undermining the overall analysis. These challenges underscore the need to improve the synthetic data generation process.

One might wonder, surely, assessing fidelity via statistical divergence metrics [17, 5] such as MMD or KL-divergence is sufficient? We argue that such high-level metrics tell one aspect of the story. An overlooked dimension is the characterization of data profiles. In this approach, samples are assigned to profiles that reflect their usefulness for an ML task. Specifically, samples are typically categorized as easy to learn, ambiguous, or hard, which are proxies for data issues like mislabeling, data shift, or under-represented samples. In methods such as Data-IQ [18]and Data Maps [19] this is referred to as "groups of the data", however, we use "data profiles" for clarity.

While this issue has been well-studied for supervised tasks, it has not been explored in the generative setting. We highlight the issues of overlooking such data profiling in Figure 1, where despite near-perfect statistical fidelity (inverse KLD), we show the differing proportion of 'easy' examples identified in synthetic data generated by different generative models trained on the Adult dataset [16]. On the other hand, this data profile correlates with downstream classification performance.

To address this challenge of the representativeness of synthetic data, we explore the potential of integrating data-centric AI techniques and their insights to improve synthetic data generation. Specifically, we propose characterizing individual samples in the data and subsequently using the different data profiles to guide synthetic data generation in a way that better reflects the real world. While our work is applicable across modalities, our primary focus is tabular data given the ubiquity of tabular data in real-world applications [20, 21], with approximately 79% of data scientists working with it on a daily basis, vastly surpassing other modalities [22].

**Contributions:**

1. _Conceptually_, we delve into the understanding of fundamental properties of data with respect to synthetic data generation, casting light on the impact of overlooking data characteristics and profiles when generating synthetic data.

2. _Technically_, we bring the idea of data profiles in data-centric AI to the generative setting and explore its role in guiding synthetic data generation. We introduce a comprehensive framework to facilitate this evaluation across various generative models.

3. _Empirically_, we benchmark the performance of five state-of-the-art models for tabular data generation on eleven distinct tabular datasets and investigate the practical integration of data-centric profiles to guide synthetic data generation. We provide practical recommendations for enhancing

Figure 1: Measures of data-centric profiling (A) better reflect the downstream performance of generative models (B) than measures of statistical fidelity (C). Assessed on the Adult dataset [16] using five different generative models A) Proportion easy examples in the generated datasets identified by Cleanlab, B) Supervised classification performance when training on synthetic, testing on real data, C) Inverse KL-divergence. (bn=bayesian_network)

synthetic data generation, particularly with respect to the 3 categories of synthetic data utility (i) predictive performance, (ii) model selection and (iii) feature selection.

We hope the insights of this paper spur the reconsideration of the conventional approaches to synthetic data generation and encourage experimentation on how data-centric AI could help synthetic data generation deliver on its promises.

## 2 Related work

This work engages with synthetic data generation and data characterization in data-centric AI.

**Synthetic Tabular Data Generation** uses generative models to create artificial data that mimics the structure and statistical properties of real data, and is particularly useful when real data is scarce or inaccessible [23; 24; 4]. In the following, we describe the broad classes of synthetic data generators applicable to the tabular domain. _Bayesian networks_[25] are a traditional approach for synthetic data generation, that represent probabilistic relationships using graphical models. _Conditional Tabular Generative Adversarial Network_ (CTGAN) [26] is a deep learning method for modeling tabular data. It uses a conditional GAN to capture complex non-linear relationships. _The Tabular Variational Autoencoder_ (TVAE) is a specialized Variational Autoencoder, designed for the tabular setting [26].

_Normalizing flow models_[27; 28] provide an invertible mapping between data and a known distribution, and offer a flexible approach for generative modeling. Diffusion models, which have gained recent popularity, offer a different paradigm for generative modeling. _TabDDPM_[29] is a diffusion model proposed for the tabular data domain. In this work, we evaluate these classes of generative models, considering various aspects of synthetic data evaluation.

**Evaluation of Synthetic Data** is a multifaceted task [17; 30], involving various dimensions such as data utility with respect to a downstream task, statistical fidelity, and privacy preservation [17; 30]. In this work, we focus on dimensions that impact model performance and hence, while important, we do not consider privacy aspects.

_(1) Data Utility:_ refers to how well the synthetic data can be used in place of the real data for a given task. Typically, utility is assessed by training predictive models on synthetic data and testing them on real data [4; 17; 31; 32; 5]. We posit that beyond matching predictive performance, we also desire to retain both _model ranking_ and _feature importance_ rankings. We empirically assess these aspects in Sec. 5.

_(2) Statistical Fidelity:_ measures the degree of similarity between synthetic data and the original data in terms of statistical properties, including the marginal and joint distributions of variables [17]. Statistical tests like the Kolmogorov-Smirnov test or divergence measures like Maximum Mean Discrepancy, KL-divergence or Wasserstein distance are commonly used for evaluation[17; 5].

Beyond statistical measures, the concept of data characterization and profiles of easy and hard examples has emerged in data-centric AI. These profiles serve as proxies for understanding real-world data, which is often not "perfect" due to mislabeling, noise, etc.The impact of these profiles on supervised models has been demonstrated in the data-centric literature [33; 34; 18]. In Figure 1, we show that data profiles are similarly important in the generative setting. Despite having almost perfect statistical fidelity, different generative models capture different data profiles (e.g. proportion of easy examples), leading to varying data utility as reflected in different performances. Consequently, we propose considering data profiles as an important dimension when creating synthetic data. We describe current data-centric methods that can facilitate this next.

**Data profiling** is a growing field in Data-Centric AI that aims to evaluate the characteristics of data samples for specific tasks [35; 36]. In the supervised learning setting, various methods have been developed to assign samples to groups, which we refer to as data profiles. These profiles, such as easy, ambiguous, or hard, often reveal issues such as mislabeling, data shifts, or under-represented groups [34; 18; 33; 37; 19; 38]. Various mechanisms are used in different methods for data characterization. For example, Cleanlab [34] models relationships between instances based on confidence, while Data Maps and Data-IQ [18] assess uncertainty through training dynamics. However, many existing methods are designed for neural networks and are unsuitable for non-differentiable models like XGBoost, which are commonly used in tabular data settings. Consequently, we focus on data characterization approaches such as Cleanlab, Data-IQ, and Data Maps which are more applicable to tabular data.

## 3 Framework

We propose a unified framework that enables a thorough assessment of generative models and the synthetic data they produce. The framework encompasses the evaluation of the synthetic data based on established statistical fidelity metrics as well as three distinct tasks encompassing _data utility_.

At a high level, the framework proceeds as visualized in 2. The dataset is first divided into a training set, denoted as \(\mathbb{D}_{\text{train}}\), and a testing set, denoted as \(\mathbb{D}_{\text{test}}\). A duplicate of the training set (\(\mathbb{D}_{\text{train}}\)) undergoes a data-centric preprocessing approach to produce a preprocessed version of the training set, referred to as \(\mathbb{D}_{\text{train}}^{\text{pre}}\). A generative model is then trained on \(\mathbb{D}_{\text{train}}^{\text{pre}}\). This model is used to synthesize a new dataset, denoted as \(\mathbb{D}_{\text{synth}}\). The synthetic dataset is further processed using a data-centric postprocessing method to create the final synthetic dataset, denoted as \(\mathbb{D}_{\text{synth}}^{\text{post}}\). Various classification models \(\mathcal{M}\) are then trained separately on the original training set \(\mathbb{D}_{\text{train}}\) and the synthetic dataset \(\mathbb{D}_{\text{synth}}^{\text{post}}\). These models are then applied to the testing set \(\mathbb{D}_{\text{test}}\) for evaluation. The generative and supervised models are evaluated for their statistical fidelity and data utility. The focus is on classification performance, model selection, and feature selection. Further details on each process within the framework can be found in the following subsections.

### Data profiling

Assume we have a dataset \(\mathcal{D}=\{(x^{n},y^{n})\mid n\in[N]\}\). Data profiling aims to assign a score \(S\) to samples in \(\mathcal{D}\). On the basis of the score, a threshold \(\tau\) is typically used to assign a specific profile group \(p^{n}\in\mathcal{P}\), where \(\mathcal{P}=\{Easy,Ambigious,Hard\}\) to each sample \(x^{n}\).

Our framework supports three recent data characterization methods applicable to tabular data: Cleanlab [34], Data-IQ [18], and Data Maps [33]. They primarily differ based on their scoring mechanism \(S\). For instance, Cleanlab [34] uses the predicted probabilities as \(S\) to estimate a noise matrix, Data-IQ [18] uses confidence and aleatoric uncertainty as \(S\), and Data Maps uses confidence and variability (epistemic uncertainty) as \(S\). Moreover, they differ in the categories in the data profiles derived from their scores. Data-IQ and Data Maps provide three categories of data profiles: _easy_; samples that are easy for the model to predict, _ambiguous_; samples with high uncertainty, and _hard_;

Figure 2: Illustration of the framework’s process flow. _Data partitioning_: the dataset is divided into a training set, \(\mathbb{D}_{\text{train}}\), and a testing set, \(\mathbb{D}_{\text{test}}\). _Data profiling_: a data-centric preprocessing approach is employed on a duplicate of \(\mathbb{D}_{\text{train}}\) to produce \(\mathbb{D}_{\text{train}}^{\text{pre}}\). A generative model, trained on \(\mathbb{D}_{\text{train}}^{\text{pre}}\), is then utilized to synthesize a dataset, \(\mathbb{D}_{\text{synth}}\), which is further processed using a data-centric postprocessing method to achieve the final synthetic dataset, \(\mathbb{D}_{\text{synth}}^{\text{post}}\). _Classification model training_: various classification models are separately trained on \(\mathbb{D}_{\text{train}}\) and \(\mathbb{D}_{\text{synth}}^{\text{post}}\) and applied to \(\mathbb{D}_{\text{test}}\). _Evaluation_: the generative and supervised models are appraised for their statistical fidelity and utility, focusing on classification accuracy, model selection, and feature selection.

samples that are wrongly predicted with high certainty. Cleanlab provides two profiles: _easy_ and _hard_ examples.

We create data profiles with these three data-centric methods to evaluate the value of data-centric methods to improve synthetic data generation, both _ex-ante_ and _post hoc_. We use the profiles in multiple preprocessing and postprocessing strategies applied to the original and synthetic data.

#### 3.1.1 Preprocessing

Preprocessing strategies are applied to the original data \(\mathbb{D}_{\text{train}}\) i.e., before feeding to a generative model. We investigate three preprocessing strategies: (1) baseline, which applies no processing, and simply feeds the \(\mathbb{D}_{\text{train}}\) to the generative model. (2) easy_hard: Let \(S_{c}:\mathbb{D}_{\text{train}}\rightarrow[0,1]\) denote the scoring function for data-centric method \(c\). We partition \(\mathbb{D}_{\text{train}}\) into \(\mathbb{D}_{\text{train}}^{\text{easy}}\) and \(\mathbb{D}_{\text{train}}^{\text{hard}}\) data profiles using a threshold \(\tau\), such that \(\mathbb{D}_{\text{train}}^{\text{easy}}=\{x^{n}\mid S_{c}(x^{n})\leq\tau\}\) and \(\mathbb{D}_{\text{train}}^{\text{hard}}=\{x^{n}\mid S_{c}(x^{n})>\tau\}\). (3) Analogously, easy_ambiguous_hard 2 splits the \(\mathbb{D}_{\text{train}}\) on the easy, ambiguous, and hard examples. Further details are provided in Appendix A.

Footnote 2: Only defined for data-centric methods that identify ambiguous examples, i.e. Data-IQ and Data Maps.

#### 3.1.2 Generative model

We utilize the data profiles identified in the preprocessing step to train a specific generative model for each data segment, e.g. easy and hard examples separately. Let \(G:\mathbb{D}_{\text{train}}\rightarrow\mathbb{D}_{\text{synth}}\) denote the generative model trained on a dataset \(\mathbb{D}_{\text{train}}\), which produces synthetic dataset \(\mathbb{D}_{\text{synth}}\). In our framework, for each data profile in preprocessed dataset \(\mathbb{D}_{\text{train}}^{\text{pre}}\), we train a separate generative model. We generate data using each generative model and the combined synthetic data is then \(\mathbb{D}_{\text{synth}}=G_{\text{easy}}(\mathbb{D}_{\text{train}}^{\text{easy }})\cup G_{\text{hard}}(\mathbb{D}_{\text{train}}^{\text{hard}})\), with generation preserving the ratio of the data segments, to reflect their distribution in the initial dataset.

#### 3.1.3 Postprocessing

We define postprocessing strategies as processing applied to the synthetic data after data generation but before supervised model training and task evaluation. We denote the set of postprocessing strategies as \(\mathcal{H}\). Given the synthetic dataset \(\mathbb{D}_{\text{synth}}\), each postprocessing strategy \(h\in\mathcal{H}\) maps \(\mathbb{D}_{\text{synth}}\) to a processed dataset \(\mathbb{D}_{\text{synth}}^{\text{post}}=h(\mathbb{D}_{\text{synth}})\). Two different postprocessing strategies were used: baseline: This is the identity function \(h_{\text{baseline}}(\mathbb{D}_{\text{synth}})=\mathbb{D}_{\text{synth}}\). no_hard: We remove the hard examples from the synthetic data, \(\mathbb{D}_{\text{synth}}^{\text{post}}=\mathbb{D}_{\text{synth}}\setminus\{x _{\text{synth}}^{n}\mid S_{c}(x_{\text{synth}}^{n})>\tau\}\), where \(x_{\text{synth}}^{n}\) is generated synthetic data.

### Classification model training

The training procedure of the supervised classification models \(\mathcal{M}\) comprises two steps, each minimizing a cost function \(\mathcal{L}\). (1) Train on the real data, i.e., \(\mathcal{M}_{\text{real}}=\arg\min\mathcal{L}(\mathcal{M}(\mathbb{D}_{\text {train}}))\). (2) Train on synthetic data, i.e. \(\mathcal{M}_{\text{syn}}=\arg\min\mathcal{L}(\mathcal{M}(\mathbb{D}_{\text {synth}}^{\text{post}}))\) We then compare utility of \(\mathcal{M}_{\text{real}}\) and \(\mathcal{M}_{\text{syn}}\) in the evaluation procedure. Our framework supports any machine learning model \(\mathcal{M}\) compatible with the Scikit-Learn API.

### Evaluation

Finally, the framework includes automated evaluation tools for the generated synthetic data to evaluate the effect of pre- and postprocessing strategies, across datasets, random seeds, and generative models. To thoroughly assess our framework, we establish evaluation metrics that extend beyond statistical fidelity, encapsulating data utility through the inclusion of three tasks.

#### 3.3.1 Statistical fidelity

The quality of synthetic data is commonly assessed using divergence measures between the real and synthetic data [5; 30]. Our framework allows for this assessment using widely adopted methods including inverse KL-Divergence [5], Maximum Mean Discrepancy [39], Wasserstein distance, as well as Alpha-precision and Beta-Recall [30]. However, as shown in Figure 1, such measures can only tell one aspect of the story. Indeed, despite all generative models providing near-perfect statistical fidelity based on divergence measures, the synthetic data captures the nuances of real data differently, as reflected in the varying data profiles (e.g. proportion easy examples). This motivates us to also assess the data utility and the potential implications of this variability.

#### 3.3.2 Data utility

Three specific metrics were employed to assess data utility: classification performance, model selection, and feature selection.

**Classification performance** To explore the usefulness of the generated synthetic data for model training, we use the train-on-synthetic, test-on-real paradigm to fit a set of machine learning models \(\mathcal{M}\) on the synthetic data, \(\mathbb{D}_{\text{synth}}\), and subsequently evaluate their performance on a real, held-out test dataset, \(\mathbb{D}_{\text{test}}\). By using \(\mathbb{D}_{\text{test}}\) we avoid potential issues from data leakage that might occur from an evaluation on the real training sets, \(\mathbb{D}_{\text{train}}\).

**Model selection** When using synthetic data for model selection, it is imperative that the ranking of classification models \(\mathcal{M}\) trained on synthetic data aligns closely with the ranking of classification models trained on the original data. To evaluate this, we first train a set of \(\mathcal{M}_{\text{real}}\) on \(\mathbb{D}_{\text{train}}\) and evaluate their classification performance on \(\mathbb{D}_{\text{test}}\). Next, we fit the same set of \(\mathcal{M}_{\text{synth}}\) on \(\mathbb{D}_{\text{synth}}^{\text{post}}\) and evaluate their classification performance on \(\mathbb{D}_{\text{test}}\). The rank-ordering of the \(\mathcal{M}_{\text{real}}\) in terms of a performance metric (e.g. AUROC) is compared with the ranking-order of the \(\mathcal{M}_{\text{synth}}\) using Spearman's Rank Correlation.

**Feature selection** Feature selection is a crucial task in data analysis and machine learning, aiming to identify the most relevant and informative features that contribute to a model's predictive power. To evaluate the utility of using synthetic data for feature selection, a similar approach is followed as for model selection. First, a model \(\mathcal{M}_{\text{real}}\) with inherent feature importance (e.g. random forest) is trained on \(\mathbb{D}_{train}\) and the rank-ordering of the most important features is determined. This ranking is then compared to the rank ordering of the most important features obtained from the same model type \(\mathcal{M}_{\text{synth}}\) trained on \(\mathbb{D}_{\text{synth}}^{\text{post}}\) using Spearman's Rank Correlation.

### Extending the framework

The framework presented in this paper is intentionally designed to be modular and highly adaptable, allowing for seamless integration of various generative models, pre- and postprocessing strategies, and diverse tasks. This flexibility enables researchers and practitioners to explore and evaluate e.g. different combinations of generative models alongside various pre- and post-processing strategies. Further, the framework is extensible, allowing for the incorporation of additional generative models, novel processing methods, and emerging tasks, ensuring that it remains up-to-date and capable of accommodating future advancements in the field of synthetic data generation.

## 4 Experiments

To demonstrate the framework, we conduct multiple experiments, aiming to answer the following subquestions in order to investigate: **Can data-centric ML improve synthetic data generation?**:

**Q1:** Is statistical fidelity sufficient to quantify the utility of synthetic data?

**Q2:** Can we trust results from supervised classification models trained on synthetic data to generalize to real data?

**Q3:** Can data-centric approaches be integrated with synthetic data generation to create more realistic synthetic data?

**Q4:** Does the level of label noise influence the effect of data-centric processing for synthetic data generation?

All code for running the analysis and creating tables and graphs can be found at the following links: [https://github.com/HLasse/data-centric-synthetic-data](https://github.com/HLasse/data-centric-synthetic-data) or [https://github.com/vanderschaarlab/data-centric-synthetic-data](https://github.com/vanderschaarlab/data-centric-synthetic-data).

### Data

We assess our framework on a filtered version of the Tabular Classification from Numerical features benchmark suite from [40]. To reduce computational costs, we filter the benchmark suite to only include datasets with less than 100.000 samples and less than 50 features which reduced the number of datasets from 16 to 11. The datasets span several domains and contain a highly varied number of samples and features (see B for more details.). Notably, the datasets have been preprocessed to meet a series of criteria to ensure their suitability for benchmarking tasks. For instance, the datasets have at least 5 features and 3000 samples, are not too easy to classify, have missing values removed, have balanced classes, and only contain low cardinality features.

### Generative models

To cover a representative sample of the space of generative models, we evaluate 5 different models with different architectures as reviewed in 2: bayesian networks (bayesian_network), conditional tabular generative adversarial network (ctgan), tabular variational autoencoder (tvae), normalizing flow (nflow), diffusion model for tabular data (ddpm).

### Supervised classification model training

The variety of models employed in our study includes: extreme gradient boosting (xgboost), random forest, logistic regression, decision tree, k-nearest neighbors, support vector classifier, gaussian naive bayes, and multi-layer perception. It is the ranking of these models that is evaluated for the model selection task. Given the large number of models, we restrict the classification results in the main paper to be from the xgboost model. Feature selection results are reported for xgboost models. Classification and feature selection results for the other classifiers can be found in Appendix C.

### Experimental procedure

**Main study** The experimental process followed the structure outlined in Sec. 3 and Figure 2, repeated for each of the 11 datasets, 5 generative models, 10 random seeds, and all permutations of pre- and postprocessing methods for each of the three data-centric methods (Cleanlab, Data-IQ, and Data Maps). We comprehensively evaluate the results across classification performance, model selection, feature selection, and statistical fidelity. In total, we fit more than **8000** generative models.

**Impact of label noise** To assess the impact of label noise on the effect of data-centric pre- and postprocessing, we carried out an analogous experiment to the main study, on the Covid mortality dataset [41]. Here, we introduce label noise to \(\mathbb{D}_{\text{train}}\) before applying any processing. We study the impact of adding [0, 2, 4, 6, 8, 10] percent label noise.

All results reported in the main paper use Cleanlab as the data-centric method for both pre- and postprocessing. This decision was made to ensure clarity in the reported results and because Cleanlab was found to outperform Data-IQ and Data Maps in a simulated benchmark. For the benchmark of the data-centric methods as well as results using Data-IQ and Data Maps, we refer to Appendix C.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Generative Model & Classification & Model Selection & Feature Selection & Statistical fidelity \\ \hline Real data & 0.866 (0.855, 0.877) & 1.0 & 1.0 & 1.0 \\ \hline bayesian\_network & 0.622 (0.588, 0.656) & 0.155 (0.055, 0.264) & 0.091 (-0.001, 0.188) & 0.998 (0.998, 0.999) \\ ctgan & 0.797 (0.769, 0.823) & **0.519** (0.457, 0.579) & 0.63 (0.557, 0.691) & 0.979 (0.967, 0.987) \\ ddpn & **0.813** (0.781, 0.844) & 0.508 (0.446, 0.573) & 0.635 (0.546, 0.718) & 0.846 (0.668, 0.972) \\ nflow & 0.737 (0.713, 0.761) & 0.354 (0.288, 0.427) & 0.415 (0.34, 0.485) & 0.975 (0.968, 0.981) \\ tvae & 0.792 (0.764, 0.818) & 0.506 (0.436, 0.565) & **0.675** (0.63, 0.722) & 0.966 (0.953, 0.978) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summarised performance for the baseline condition (no data-centric processing) across all datasets. Classification is measured by AUROC, model selection and feature selection by Spearman’s Rank Correlation, and statistical fidelity by inverse KL divergence. Numbers show bootstrapped mean and 95% CI. The best-performing model by task is in bold.

#### 4.4.1 Evaluation metrics

The classification performance is evaluated in terms of the area under the receiver operating characteristic curve (AUROC), model selection performance as Spearman's Rank Correlation between the ranking of the supervised classification models trained on the original data and the supervised classification models trained on the synthetic data, and feature selection performance as Spearman's Rank Correlation between the ranking of features in an xgboost model trained on the original data and an xgboost model trained on the synthetic data.

## 5 Results

**Statistical fidelity is insufficient for evaluating generative models.** Measures of statistical fidelity fail to capture variability in performance on downstream tasks, as shown in Table 1. Surprisingly, the worst performing model across all tasks (bayesian network), has the highest inverse KL-divergence of all synthetic datasets, which should indicate a strong resemblance to the original data. Conversely, the lowest inverse KL-divergence is found for ddpm which is one of the consistently best performing models.

**Practical guidance:** The benchmarking results illustrate that when selecting a generative model, even if the statistical fidelity appears similar, different generative models may perform differently on the 3 downstream tasks (classification, model selection, feature selection). Hence, beyond statistical fidelity, practitioners should understand which aspect is most crucial for their purpose to guide selection of the generative model.

**Different generative models for different tasks.** As shown in Table 1, training on synthetic data leads to a marked decline in classification performance compared to real data, as well as highly differing model and feature rankings. The effect differs largely by generative model, where CTGAN, TabDDPM, and TVAE most closely retain the characteristics of the real data. No one model is superior across all tasks. Specifically, TabDPPM achieves the highest classification performance, CTGAN performs best in model selection, and TVAE excels in feature selection. These findings indicate that one should test a range of generative models and consider the trade-offs in data utility before publishing synthetic data. Additionally, Appendix C reveals that although there are slight differences in performance based on the supervised model type, the overall pattern of results remains consistent across generative models.

**Practical guidance:** No generative model reigns supreme (highlighting the inherent challenge of synthetic tabular data). However, over tabular data sets, we show that _CTGAN_ and _TVAE_ offer the best trade-off between high statistical fidelity and strong performance on the three downstream tasks.

**Data-centric methods can improve the utility of synthetic data.** The addition of data-centric pre- and postprocessing strategies has a generally positive effect across all tasks as seen in Table 2 and Figure 3, despite resulting in lower statistical fidelity. In terms of classification performance, 13 out of 15 evaluations showed a net improvement, with gains up to 1.64% better classification

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline Generative Model & \begin{tabular}{l} Progressive \\ Strategy \\ \end{tabular} & 
\begin{tabular}{l} Polytressing \\ Strategy \\ \end{tabular} & Classification & Model Selection & Feature Selection & Statistical Fidelity \\ \hline bayesian,network & baseline & no.hard & 0.31 (4.89,35.98) & 2.78 (38.22,37.) & 5.22 (16.60,42.74) & \(\pm\)0.027 (0.054,0.046) \\ easy,hard & baseline & no.hard & 0.35 (4.98,56.00) & 2.78 (31.99,15.14) & 2.79 (10.16,12.79,26.61) & \(\pm\)0.013 (0.042,0.027) \\ no.hard & no.hard & 0.8 (4.44,63.74) & 2.72 (31.39,90.28) & 3.99 (13.22,20.17,39.9) & \(\pm\)0.023 (0.067,0.021) \\ \hline ctgan & baseline & no.hard & 1.23 (2.03,44.4) & \(\uparrow\)1.7 (1.92,42.8) & 3.66 (4.88,2.61) & \(\pm\)0.054 (1.42,4.085) \\ easy,hard & baseline & 0.78 (4.14,2.88) & 1.17 (1.34,14.7) & 1.04 (1.06,10.16,10.13) & \(\pm\)0.005 (1.06,10.80) \\ no.hard & no.hard & 0.37 (2.96,3.8) & \(\uparrow\)4.73 (2.13,26.68) & \(\uparrow\)0.18 (-100.04,9.43) & \(\uparrow\)0.119 (-1.218,0.701) \\ \hline ddpm & baseline & no.hard & 0.63 (3.55,4.24) & \(\uparrow\)6.35 (2.01,7.86) & \(\uparrow\)0.17 (1.60,13.37) & \(\pm\)0.165 (1.942,15.964) \\ easy,hard & baseline & 0.68 (2.84,4.06) & \(\uparrow\)6.70 (5.87,20.19) & \(\uparrow\)1.58 (-109.12,39.22) & 0.336 (1.66,13.83,3.89) \\ no.hard & no.hard & 1.32 (-20.46,4.68) & \(\uparrow\)9.78 (7.27,18.63) & \(\uparrow\)4.19 (-69.56,16.16) & \(\pm\)0.284 (-16.721,14.567) \\ \hline nflow & baseline & no.hard & 1.05 (2.43, 3.97) & 1.12 (16.58, 18.41) & 5.82 (1.12,12.68) & \(\pm\)0.053 (0.743,0.688) \\ easy,hard & baseline & 0.76 (2.64,3.81) & \(\uparrow\)2.37 (15.26,0.63) & \(\uparrow\)6.63 (3.01,25.65) & \(\pm\)0.022 (-0.752,0.625) \\ no.hard & no.hard & 1.64 (1.76,4.81) & 4.66 (1.367,22.84) & 7.28 (-11.54,25.01) & \(\pm\)0.052 (-0.705,0.654) \\ \hline tvae & baseline & no.hard & 1.1 (2.39,4.38) & \(\uparrow\)3.58 (1.84,11.01) & \(\downarrow\)0.13 (1.64,6.38) & \(\pm\)0.053 (-1.418,1.113) \\ easy,hard & baseline & 0.53 (3.46,3.16) & \(\downarrow\)5.83 (1.94,4.75) & \(\uparrow\)6.7 (6.03,0.87) & \(\pm\)0.24 (0.941,29.96) \\ no.hard & no.hard & 0.71 (-2.59,3.95) & \(\uparrow\)0.11 (-13.79,14.38) & \(\uparrow\)4.41 (-3.38,9.89) & \(\uparrow\)0.199 (-0.929,1.285) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Percentage increase in performance from baseline, i.e., no data-centric pre- or postprocessing (as seen in Table 1), per generative model for each pre- and postprocessing strategy, averaged across all datasets and seeds.

performance compared to not processing the data. Model selection exhibited more pronounced effects, particularly for bayesian networks, which demonstrated the greatest variability overall. While the model selection results for TVAE decreased following data-centric processing, the other generative models saw positive effects, with performance improvements ranging from 4.66% to 92%. Regarding feature selection, 12 out of 15 evaluations demonstrated a net benefit of data-centric processing, resulting in improvements of 3.41% to 9.79% in Spearman's rank correlation. The benefit of data-centric processing was found to be statistically significant for classification and feature selection (see Appendix D for details).

**Practical guidance:** Before releasing a synthetic dataset, practitioners are advised to apply the data-centric methods studied in this paper as an add-on. This will ensure enhanced utility of the synthetic data in terms of classification performance, model selection, and feature selection.

**Data-centric processing provides benefits across levels of label noise.** Data-centric pre- and postprocessing lead to consistently higher performance across tasks for all augmented datasets. As shown in Figure 4, the magnitude of the effect of data-centric processing decreases with higher levels of label noise, particularly above 8%, although this effect is not statistically significant. Even though the level of statistical fidelity decreased marginally by applying data-centric processing, data-centric processing led to statistically significant increases in performance on all three tasks.

**Practical guidance:** Fitting generative models on noisy "real-world" data can lead to sub-optimal downstream performance despite seemingly high statistical fidelity. Data-centric methods are especially useful at reasonable levels of label noise, typically below 8%. Therefore, we recommend their application when fitting generative models on real-world datasets.

### Limitations and future work

Our work delves into the performance-driven aspects of synthetic data generation, focusing primarily on data utility and statistical fidelity, particularly within the realm of tabular data. While tabular data is highly diverse and contains many intricacies, we also recognize several directions for further exploration. Our current framework, while rooted in tabular data, hints at the broader applicability to other data types such as text and images. Accommodating our framework to these modalities would require further work on modality-specific tasks. For instance, images or text do not possess a direct analog to feature selection. Such disparities underscore the need for a bespoke benchmarking methodology tailored to each specific data type.

Figure 3: Average performance across all datasets for each generative model by pre- and postprocessing method.

## 6 Conclusion

This research provides novel insights into integrating data-centric AI techniques into synthetic tabular data generation. First, we introduce a framework to evaluate the integration of data profiles for creating more representative synthetic data. Second, we confirm that statistical fidelity alone is insufficient for assessing synthetic data's utility, as it may overlook important nuances impacting downstream tasks. Third, the choice of generative model significantly influences synthetic data quality and utility. Last, incorporating data-centric methods consistently improves the utility of synthetic data across varying levels of label noise. Our study demonstrates the potential of data-centric AI techniques to enhance synthetic data's representation of real-world complexities, opening avenues for further exploration at their intersection.

## Acknowledgements

This work was partially supported by DeiC National HPC (g.a. 2022-H2-10). NS is supported by the Cystic Fibrosis Trust. LH was supported by a travel grant from A.P. Moller Fonden til Legevidenskabens Fremme and is supported by grants from the Lundbeck Foundation (grant number: R344-2020-1073), the Central Denmark Region Fund for Strengthening of Health Science (grant number: 1-36-72-4-20), and The Danish Agency for Digitisation Investment Fund for New Technologies (grant number: 2020-6720).

Figure 4: Performance of a single generative model (TabDDPM) on the Covid mortality dataset with varying levels of label noise across the pre- and postprocessing conditions.

## References

* Jain et al. [2020] Abhinav Jain, Hima Patel, Lokesh Nagalapatti, Nitin Gupta, Sameep Mehta, Shanmukha Guttula, Shashank Mujumdar, Shazia Afzal, Ruhi Sharma Mittal, and Vitobha Munigala. Overview and importance of data quality for machine learning tasks. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 3561-3562, 2020.
* Gupta et al. [2021] Nitin Gupta, Hima Patel, Shazia Afzal, Naveen Panwar, Ruhi Sharma Mittal, Shanmukha Guttula, Abhinav Jain, Lokesh Nagalapatti, Sameep Mehta, Sandeep Hans, et al. Data quality toolkit: Automatic assessment of data quality and remediation for machine learning datasets. _arXiv preprint arXiv:2108.05935_, 2021.
* Renggli et al. [2021] Cedric Renggli, Luka Rimanic, Nezihre Merve Gurel, Bojan Karlas, Wentao Wu, and Ce Zhang. A data quality-driven view of mlops. _IEEE Data Engineering Bulletin_, 2021.
* Lu et al. [2023] Yingzhou Lu, Huazheng Wang, and Wenqi Wei. Machine learning for synthetic data generation: a review. _arXiv preprint arXiv:2302.04062_, 2023.
* Qian et al. [2023] Zhaozhi Qian, Bogdan-Constantin Cebere, and Mihaela van der Schaar. Synthcity: facilitating innovative use cases of synthetic data in different data modalities. _arXiv preprint arXiv:2301.07573_, 2023.
* Jordon et al. [2018] James Jordon, Jinsung Yoon, and Mihaela Van Der Schaar. PATE-GAN: Generating synthetic data with differential privacy guarantees. In _International Conference on Learning Representations_, 2018.
* Assefa et al. [2020] Samuel A Assefa, Danial Dervovic, Mahmoud Mahfouz, Robert E Tillman, Prashant Reddy, and Manuela Veloso. Generating synthetic data in finance: opportunities, challenges and pitfalls. In _Proceedings of the First ACM International Conference on AI in Finance_, pages 1-8, 2020.
* Jordon et al. [2018] James Jordon, Jinsung Yoon, and Mihaela van der Schaar. Measuring the quality of synthetic data for use in competitions. _arXiv preprint arXiv:1806.11345_, 2018.
* Xu et al. [2019] Depeng Xu, Yongkai Wu, Shuhan Yuan, Lu Zhang, and Xintao Wu. Achieving causal fairness through generative adversarial networks. In _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence_, 2019.
* van Breugel et al. [2021] Boris van Breugel, Trent Kyono, Jeroen Berrevoets, and Mihaela van der Schaar. Decaf: Generating fair synthetic data using causally-aware generative networks. _Advances in Neural Information Processing Systems_, 34:22221-22233, 2021.
* Antoniou et al. [2017] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. _arXiv preprint arXiv:1711.04340_, 2017.
* Dina et al. [2022] Ayesha S Dina, AB Siddique, and D Manivannan. Effect of balancing data using synthetic data on the performance of machine learning classifiers for intrusion detection in computer networks. _arXiv preprint arXiv:2204.00144_, 2022.
* Das et al. [2022] Hari Prasanna Das, Ryan Tran, Japjot Singh, Xiangyu Yue, Geoffrey Tison, Alberto Sangiovanni-Vincentelli, and Costas J Spanos. Conditional synthetic data generation for robust machine learning applications with limited pandemic data. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 11792-11800, 2022.
* Bing et al. [2022] Simon Bing, Andrea Dittadi, Stefan Bauer, and Patrick Schwab. Conditional generation of medical time series for extrapolation to underrepresented populations. _PLOS Digital Health_, 1(7):1-26, 07 2022.
* Ganev et al. [2022] Georgi Ganev, Bristena Oprisanu, and Emiliano De Cristofaro. Robin hood and matthew effects: Differential privacy has disparate impact on synthetic data. In _International Conference on Machine Learning_, pages 6944-6959. PMLR, 2022.
* Becker and Kohavi [1996] Barry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI: [https://doi.org/10.24432/C5XW20](https://doi.org/10.24432/C5XW20).

* [17] Joshua Snoke, Gillian M Raab, Beata Nowok, Chris Dibben, and Aleksandra Slavkovic. General and specific utility measures for synthetic data. _Journal of the Royal Statistical Society. Series A (Statistics in Society)_, 181(3):663-688, 2018.
* [18] Nabeel Seedat, Jonathan Crabbe, Ioana Bica, and Mihaela van der Schaar. Data-iq: Characterizing subgroups with heterogeneous outcomes in tabular data. In _Advances in Neural Information Processing Systems_, 2022.
* [19] Chirag Agarwal, Daniel D'souza, and Sara Hooker. Estimating example difficulty using variance of gradients. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10368-10378, 2022.
* [20] Vadim Borisov, Tobias Leemann, Kathrin Sessler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data: A survey. _arXiv preprint arXiv:2110.01889_, 2021.
* [21] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. _Information Fusion_, 81:84-90, 2022.
* [22] Kaggle. Kaggle machine learning and data science survey, 2017.
* [23] Sam Bond-Taylor, Adam Leach, Yang Long, and Chris G Willcocks. Deep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models. _IEEE transactions on pattern analysis and machine intelligence_, 2021.
* [24] Jessamyn Dahmen and Diane Cook. Synsys: A synthetic data generation system for healthcare applications. _Sensors_, 19(5):1181, 2019.
* [25] Jim Young, Patrick Graham, and Richard Penny. Using bayesian networks to create synthetic data. _Journal of Official Statistics_, 25(4):549, 2009.
* [26] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling tabular data using conditional gan. _Advances in Neural Information Processing Systems_, 32, 2019.
* [27] Jaewoo Lee, Minjung Kim, Yonghyun Jeong, and Youngmin Ro. Differentially private normalizing flows for synthetic tabular data generation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 7345-7353, 2022.
* [28] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In _International conference on machine learning_, pages 1530-1538. PMLR, 2015.
* [29] Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. Tabddpm: Modelling tabular data with diffusion models. _arXiv preprint arXiv:2209.15421_, 2022.
* [30] Ahmed Alaa, Boris Van Breugel, Evgeny S Saveliev, and Mihaela van der Schaar. How faithful is your synthetic data? sample-level metrics for evaluating and auditing generative models. In _International Conference on Machine Learning_, pages 290-306. PMLR, 2022.
* [31] Tariq Alkhalifah, Hanchen Wang, and Oleg Ovcharenko. Mlreal: Bridging the gap between training on synthetic data and real data applications in machine learning. _Artificial Intelligence in Geosciences_, 3:101-114, 2022.
* [32] Tingwei Shen, Ganning Zhao, and Suya You. A study on improving realism of synthetic data for machine learning. _arXiv preprint arXiv:2304.12463_, 2023.
* [33] Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with training dynamics. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 9275-9293, 2020.
* [34] Curtis Northcutt, Lu Jiang, and Isaac Chuang. Confident learning: Estimating uncertainty in dataset labels. _Journal of Artificial Intelligence Research_, 70:1373-1411, 2021.

* [35] Weixin Liang, Girmaw Abebe Tadesse, Daniel Ho, L Fei-Fei, Matei Zaharia, Ce Zhang, and James Zou. Advances, challenges and opportunities in creating data for trustworthy ai. _Nature Machine Intelligence_, 4(8):669-677, 2022.
* [36] Nabeel Seedat, Fergus Imrie, and Mihaela van der Schaar. Dc-check: A data-centric ai checklist to guide the development of reliable machine learning systems. _arXiv preprint arXiv:2211.05764_, 2022.
* [37] Nabeel Seedat, Jonathan Crabbe, and Mihaela van der Schaar. Data-suite: Data-centric identification of in-distribution incongruous examples. In _International Conference on Machine Learning_, pages 19467-19496. PMLR, 2022.
* [38] Nabeel Seedat, Jonathan Crabbe, Zhaozhi Qian, and Mihaela van der Schaar. Triage: Characterizing and auditing training data for improved regression. In _Advances in Neural Information Processing Systems_, 2023.
* [39] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. _The Journal of Machine Learning Research_, 13(1):723-773, 2012.
* [40] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform deep learning on tabular data?
* [41] Pedro Baqui, Ioana Bica, Valerio Marra, Ari Ercole, and Mihaela van Der Schaar. Ethnic and regional variations in hospital mortality from covid-19 in brazil: a cross-sectional observational study. _The Lancet Global Health_, 8(8):e1018-e1026, 2020.
* [42] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework.
* [43] Kevin Bello, Bryon Aragam, and Pradeep Ravikumar. DAGMA: Learning DAGs via m-matrices and a log-determinant acyclicity characterization.

## Appendix A Additional details

This appendix provides additional details on the data-centric methods used in our study, as well as outlining limitations and future work.

### Data-centric AI methods

Our paper considers different data-centric AI methods to perform data characterization, which forms the basis of our data profiles. As discussed in the main manuscript, the primary difference between these methods is their scoring mechanism \(S\). The three approaches considered in this paper next, i.e. Cleanlab, Data-IQ and Data Maps will be described next. Importantly, these methods are applicable to tabular data models like XGBoost, unlike several other data-centric methods which are only applicable to differentiable models. Recall that the data characterization methods apply a threshold \(\tau\) to \(S\). The implementation of \(\tau\) in this work is described in Appendix C.3.

Cleanlab [34].Cleanlab estimates the joint distribution of noisy and true labels, thereby characterizing data into profiles easy and hard, where hard data points might indicate mislabeling. It operates on the output of classification models and can be applied to any modality. We focus on Cleanlab in the main manuscript, however, our framework could be applied with any of the following data-centric tools.

Data-IQ [18].Data-IQ is a training dynamics-based method, which characterizes data based on the aleatoric uncertainty (inherent data uncertainty), i.e. \(\mathbb{E}[\mathcal{P}(x,\vartheta)(1-\mathcal{P}(x,\vartheta))]\). We are able to extract three data profiles: easy, ambiguous and hard. Typically, these "ambiguous" and "hard" examples are harmful to model performance and might be mislabeled or "dirty".

Data Maps [33].Data Maps is a training dynamics-based method, which characterizes data based on the variability/epistemic uncertainty (model uncertainty), i.e. \(\mathbb{V}[\mathcal{P}(x,\vartheta)]\). We are able to extract three data profiles: easy, ambiguous and hard. Typically, these "ambiguous" and "hard" examples are harmful to model performance and might be mislabeled or "dirty".

## Appendix B Experimental details

By accident, the results reported in the main paper were based on data from 8 random seeds instead of the intended 10, as stated in the paper. To maintain consistency in the results, the results reported in the Appendix are also presented with 8 seeds.

### Computational resources

All experiments were conducted using NVIDIA T4 16GB GPUs. The total number of GPU hours spent across all experiments is approximately 1000. All experiments were performed on the UCloud platform.

### Hyperparameter tuning of generative models

In order to reduce the computational costs of running the main experiment, we conducted a hyperparameter search on the Adult dataset instead of tuning the parameters for each dataset in our benchmark. For each model (bayesian_network, ctgan, ddpm, nflow, tvae), we conducted a search over the hyperparameter space defined for each model in the synthecity [5] implementation. We used Optuna [42] to conduct 20 trials for each model. The best hyperparameters for each model are listed in Table 3.

### Classification model set

Throughout the experiments, we fit a set of models from the scikit-learn library, namely, XGBClassifier, RandomForestClassifier, LogisticRegression,DecisionTreeClassifier, KNeighborsClassifier, SVC, GaussianNB, and MLPClassifier. All hyperparameters were kept at their default value.

### Datasets

**Main experiments** As described in Sec 4.1. we used the "Tabular benchmark numerical classification" downloaded from OpenML (suite id 337), filtered to only include datasets with less than 100.000 samples and less than 50 features for our main experiments. The number of samples, features, and links to the datasets are provided in Table 4.

**Noise dataset** The dataset used to investigate the impact of label noise was the Covid mortality dataset from [41]. The data includes data on patients with Covid with a label for whether the patient will die within 14 days. The dataset contains 6882 samples and 21 features.

**Adult dataset** The adult dataset is a classic machine learning dataset, containing 48842 and 14 samples, with the task of predicting whether an individual's income exceeds $50k per year based on census data.

\begin{table}
\begin{tabular}{l|l} \hline Model & Parameters \\ \hline bayesian\_network & struct\_learning\_search method: hillclimb, \\  & struct\_learning\_score: bic \\ \hline ctgan & generator\_n\_layers\_hidden: 2, \\  & generator\_n\_units\_hidden: 50, \\  & generator\_nonlin: tanh, n\_iter: 1000, \\  & generator dropout: 0.0575, \\  & discriminator\_n\_layers\_hidden: 4, \\  & discriminator\_n\_units\_hidden: 150, \\  & discriminator\_nonlin: relu \\ \hline ddpm & lr: 0.0009375080542687667, \\  & batch\_size: 2929, \\  & num\_timesteps: 998, \\  & n\_iter: 1051, \\  & is\_classification: True \\ \hline nflow & n\_iter: 1000, \\  & n\_layers\_hidden: 10, \\  & n\_units\_hidden: 98, \\  & dropout: 0.11496088236749386, \\  & batch\_norm: True, \\  & lr: 0.0001, \\  & linear\_transform\_type: permutation, \\  & base\_transform\_type: rq-autoregressive, \\  & batch\_size: 512, \\ \hline tvae & n\_iter: 300, \\  & lr: 0.0002, \\  & decoder\_n\_layers\_hidden: 4, \\  & weight\_decay: 0.001, \\  & batch\_size: 256, \\  & n\_units\_embedding: 200, \\  & decoder\_nunits\_hidden: 300, \\  & decoder\_nonlin: elu, \\  & decoder\_dropout: 0.194325119117226, \\  & encoder\_n\_layers\_hidden: 1, \\  & encoder\_nunits\_hidden: 450, \\  & encoder\_nonlin: leaky\_relu, \\  & encoder\_dropout: 0.04288563703094718, \\ \hline \end{tabular}
\end{table}
Table 3: Hyperparameters used for the generative models.

[MISSING_PAGE_FAIL:16]

\begin{table}
\begin{tabular}{l l l l} \hline \hline Preprocessing & Postprocessing & Classification model & AUROC \\ Strategy & Strategy & & \\ \hline baseline & baseline & DecisionTreeClassifier & 0.64 (0.63, 0.64) \\  & & GaussianNB & 0.7 (0.69, 0.71) \\  & & KNeighborsClassifier & 0.65 (0.64, 0.66) \\  & & LogisticRegression & 0.72 (0.7, 0.73) \\  & & MLPClassifier & 0.7 (0.68, 0.71) \\  & & RandomForestClassifier & **0.76** (0.75, 0.78) \\  & & SVC & 0.69 (0.67, 0.7) \\  & & XGBClassifier & **0.76** (0.75, 0.77) \\ no\_hard & & DecisionTreeClassifier & 0.66 (0.65, 0.67) \\  & & GaussianNB & 0.71 (0.69, 0.72) \\  & & KNeighborsClassifier & 0.66 (0.65, 0.67) \\  & & LogisticRegression & 0.72 (0.7, 0.73) \\  & & MLPClassifier & 0.72 (0.7, 0.73) \\  & & RandomForestClassifier & **0.77** (0.75, 0.78) \\  & & SVC & 0.69 (0.68, 0.71) \\  & & XGBClassifier & **0.77** (0.75, 0.78) \\ \hline easy\_hard & baseline & DecisionTreeClassifier & 0.64 (0.63, 0.65) \\  & & GaussianNB & 0.7 (0.69, 0.72) \\  & & KNeighborsClassifier & 0.65 (0.64, 0.66) \\  & & LogisticRegression & 0.72 (0.7, 0.73) \\  & & MLPClassifier & 0.71 (0.69, 0.72) \\  & & RandomForestClassifier & **0.77** (0.75, 0.78) \\  & & SVC & 0.69 (0.68, 0.71) \\  & & XGBClassifier & 0.76 (0.75, 0.77) \\ no\_hard & & DecisionTreeClassifier & 0.67 (0.66, 0.68) \\  & & GaussianNB & 0.71 (0.69, 0.72) \\  & & KNeighborsClassifier & 0.66 (0.65, 0.67) \\  & & LogisticRegression & 0.72 (0.71, 0.73) \\  & & MLPClassifier & 0.72 (0.71, 0.73) \\  & & RandomForestClassifier & **0.77** (0.76, 0.78) \\  & & SVC & 0.7 (0.68, 0.71) \\  & & XGBClassifier & **0.77** (0.75, 0.78) \\ \hline Real data & & DecisionTreeClassifier & 0.74 (0.73, 0.75) \\  & & GaussianNB & 0.73 (0.72, 0.74) \\  & & KNeighborsClassifier & 0.73 (0.71, 0.74) \\  & & LogisticRegression & 0.78 (0.77, 0.79) \\  & & MLPClassifier & 0.77 (0.76, 0.79) \\  & & RandomForestClassifier & 0.86 (0.85, 0.87) \\  & & SVC & 0.74 (0.72, 0.75) \\  & & XGBClassifier & **0.87** (0.86, 0.88) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Average classification performance across all datasets and generative models by pre- and postprocessing strategy and classification model type, using Cleanlab as the data-centric method. Numbers in parentheses indicate 95% bootstrapped CI.

### Performance by dataset

As shown in Figures 6 to 8 the relative ranking of the five generative models remains largely consistent across datasets. Specifically, the classification performance results shown in Figure 6 indicate that ddpm produces synthetic data that best retains the ability to train classification models.

The model selection results shown in Figure 7 display higher variance across datasets and pre- and postprocessing methods. This variance can likely be attributed to the findings presented in Table 5, which demonstrate that the performance of several classification models is relatively similar. Consequently, minor performance variations caused by data-centric processing can significantly change the models' relative ranking.

As shown in Figure 8, data generated with the ddpm model tends to lead to the best ranking of important features. Consistently with the findings in classification performance and model selection, the bayesian_network model tends to perform the worst across datasets.

Figure 6: Classification performance by pre- and postprocessing strategy for XGBoost across all datasets using Cleanlab as the data-centric method.

### Identifying optimal thresholds for the data-centric methods

To identify the optimal threshold \(\tau\) for the scoring function \(S\) for each of the data-centric methods, we constructed four simulated datasets with varying levels of label noise. In particular, we generated datasets with 10 features and 10 samples, 10 features and 50 samples, 50 features and 10 samples, and 50 features and 50 samples. The correlation of each feature with the outcome label was sampled from a uniform distribution ranging from -0.7 to 0.7.

For each dataset, we randomly flipped a proportion (0, 0.02, 0.04, 0.06, 0.08, 0.1) of the outcome labels and recorded the indices of the flipped indices.

**Data-IQ**. Following the Data-IQ implementation, we explored two methods for setting \(\tau\): one based on whether the aleatoric uncertainty of the sample was below a certain percentile, and the other based on whether the aleatoric uncertainty was below a certain value. In both cases, to categorize a sample as 'hard', the average predicted probability of the correct label (throughout the model training) had to be \(\leq 0.25\) and the aleatoric uncertainty had to be below \(\tau\). 'Easy' samples were defined as samples where the average predicted probability of the correct label was \(\geq 0.75\) and the aleatoric uncertainty was below \(\tau\). Any remaining samples were categorized as 'ambiguous'. We tested setting \(\tau\) at the [20, 30, 40, 50, 60, 70, 80] percentile, and using raw values of [0.1, 0.125, 0.15, 0.175, 0.2].

**Data Maps**. The data-profiling procedure for Data Maps followed the same approach as Data-IQ, with the exception that the epistemic uncertainty was used instead of the aleatoric uncertainty. We tested the same values of \(\tau\) as used for Data-IQ.

Figure 7: Model selection performance by pre- and postprocessing strategy across all datasets using Cleanlab as the data-centric method.

**Cleanlab**. The Cleanlab package provides a 'label quality' score which was used to set \(\tau\) to categorize samples as 'easy' or 'hard'. We tested using the default score as well as the values [0.1, 0.125, 0.15, 0.175, 0.2]. For details of the theoretical basis of Cleanlab's thresholds, we refer to [34], Section 3.1.

**Experiment**. For each augmented dataset (varying levels of label noise), we applied each of the data-centric methods and tested all the specified values of \(\tau\). We recorded the number of samples identified as 'hard', \(n_{hard}\) as well as the number of samples that were correctly identified as hard (i.e., a flipped label) \(n_{hard}^{correct}\). Recall was calculated as \(\frac{n_{hard}^{correct}}{n_{flipped}}\), precision as \(\frac{n_{hard}^{correct}}{n_{hard}}\) and the F1 score as \(2\cdot\frac{\text{precision-recall}}{\text{recall+precision}}\)

**Results**. As shown in Table 6, Cleanlab achieves markedly higher F1 scores and recall compared to Data Maps and Data-IQ, albeit with slightly lower precision. Given the substantially higher F1 and recall scores, we chose to focus the main analysis on Cleanlab. We selected the threshold that maximised F1 for the main experiments, resulting in a \(\tau\) of 0.2 for all methods.

Figure 8: Feature selection performance by pre- and postprocessing strategy for XGBoost across all datasets using Cleanlab as the data-centric method.

### Development of Figure 1

The results shown in Figure 1 were obtained performing the following steps: First, the Adult dataset was split into an 80% training set and a 20% test set. Second, each of the five generative models (with the hyperparameters specified in Table 3) was trained on the training set. Third, A synthetic dataset, \(D_{synth}\), with the same dimensions as the training set was generated using each generative model. Fourth, the inverse KL divergence between \(D_{synth}\) and the training data was calculated, an XGBoost model was trained on \(D_{synth}\) and evaluated on the test set to measure the performance, and Cleanlab was applied to \(D_{synth}\) to extract the proportion of easy and hard examples from each synthetic dataset.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline Method & Type & \(\tau\) & F1 & Recall & Precision \\ \hline Cleanlab & cutoff & Default & 0.407 (0.29) & **0.633** (0.30) & 0.332 (0.28) \\  & & 0.100 & 0.384 (0.30) & 0.354 (0.30) & **0.5** (0.33) \\  & & 0.125 & 0.403 (0.30) & 0.393 (0.30) & 0.478 (0.33) \\  & & 0.150 & 0.41 (0.30) & 0.42 (0.31) & 0.45 (0.32) \\  & & 0.175 & 0.417 (0.30) & 0.45 (0.32) & 0.428 (0.31) \\  & & **0.200** & **0.42** (0.29) & 0.478 (0.32) & 0.408 (0.30) \\ \hline Data-IQ & cutoff & 0.100 & 0.083 (0.15) & 0.051 (0.10) & 0.507 (0.42) \\  & & 0.125 & 0.118 (0.19) & 0.076 (0.13) & **0.562** (0.40) \\  & & 0.150 & 0.16 (0.22) & 0.11 (0.16) & 0.547 (0.40) \\  & & 0.175 & 0.206 (0.24) & 0.153 (0.18) & 0.523 (0.40) \\  & & **0.200** & **0.221** (0.24) & **0.17** (0.19) & 0.513 (0.40) \\  & percentile & 20 & 0.033 (0.05) & 0.019 (0.03) & 0.279 (0.41) \\  & & 30 & 0.057 (0.07) & 0.038 (0.05) & 0.433 (0.43) \\  & & 40 & 0.075 (0.08) & 0.052 (0.06) & 0.371 (0.39) \\  & & 50 & 0.099 (0.10) & 0.067 (0.07) & 0.373 (0.39) \\  & & 60 & 0.138 (0.12) & 0.093 (0.08) & 0.407 (0.40) \\  & & 70 & 0.17 (0.16) & 0.12 (0.11) & 0.4 (0.40) \\  & & 80 & 0.199 (0.21) & 0.146 (0.15) & 0.438 (0.40) \\ \hline Data Maps & cutoff & 0.100 & 0.21 (0.24) & 0.16 (0.18) & 0.426 (0.39) \\  & & 0.125 & 0.216 (0.24) & 0.166 (0.19) & 0.47 (0.40) \\  & & 0.150 & 0.219 (0.24) & 0.169 (0.19) & **0.513** (0.40) \\  & & 0.175 & **0.221** (0.24) & **0.17** (0.19) & **0.513** (0.40) \\  & & **0.200** & **0.221** (0.24) & **0.17** (0.19) & **0.513** (0.40) \\  & percentile & 20 & 0.058 (0.07) & 0.033 (0.04) & 0.327 (0.37) \\  & & 30 & 0.082 (0.10) & 0.048 (0.06) & 0.331 (0.37) \\  & & 40 & 0.103 (0.12) & 0.064 (0.07) & 0.334 (0.37) \\  & & 50 & 0.124 (0.14) & 0.08 (0.09) & 0.336 (0.37) \\  & & 60 & 0.144 (0.17) & 0.097 (0.11) & 0.376 (0.38) \\  & & 70 & 0.163 (0.19) & 0.114 (0.13) & 0.419 (0.39) \\  & & 80 & 0.183 (0.21) & 0.133 (0.15) & 0.423 (0.39) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Average results across all simulated datasets and levels of noise. Numbers in parentheses indicate the standard deviation. The highest value per data-centric method and metric is highlighted in bold. The value in bold for \(\tau\) indicates our chosen threshold for the main experiments.

[MISSING_PAGE_FAIL:22]

### Performance on data with added label noise across all tasks and generative models

The performance of all generative models across all tasks (of which a subset is presented in Figure 4) is shown in Figure 11. The overall trend is consistent across generative models, although not all models are affected equally hard by varying levels of noise. For instance, tvae and ctgan seem to be more robust to noise than e.g. ddpm.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline Generative Model & \multicolumn{1}{c}{Preprocessing} & \multicolumn{1}{c}{Postprocessing} & \multicolumn{1}{c}{Classification} & \multicolumn{1}{c}{Model Selection} & \multicolumn{1}{c}{Feature Selection} \\  & \multicolumn{1}{c}{Strategy} & \multicolumn{1}{c}{Strategy} & & & & \\ \hline bayesian\_network & baseline & no\_hard & 0.14 (-5.08, 6.41\(\dagger\)) & -14.67 (-179.33, 148.67)\(\dagger\) & -70.0 (-243.76, 116.63)\(\dagger\) \\  & easy\_ambi\_hard & baseline & 0.87 (-5.18, 7.4)\(\dagger\) & 190.67 (39.33, 340.67)\(\dagger\) & 135.71 (-33.95, 301.53)\(\dagger\) \\  & & no\_hard & 1.24 (-4.86, 8.79)\(\dagger\) & 199.33 (52.67, 341.33)\(\dagger\) & 104.23 (-63.28, 227.24)\(\dagger\) \\  & easy\_hard & baseline & 2.18 (-3.58, 8.95)\(\dagger\) & 262.04 (-67.44, 33.33)\(\dagger\) & -51.71 (-22.37, 137.24)\(\dagger\) \\  & no\_hard & 2.46 (-3.46, 9.15)\(\dagger\) & 213.33 (26.00, 400.0)\(\dagger\) & -80.31 (-249.14, 118.38)\(\dagger\) \\ \hline ctgan & baseline & no\_hard & 0.17 (-2.67, 2.72)\(\dagger\) & 3.63 (-7.26, 14.19)\(\dagger\) & 0.33 (-6.6, 6.91\(\dagger\))\(\dagger\) \\  & easy\_ambi\_hard & baseline & -0.57 (-3.39, 2.17)\(\dagger\) & 7.54 (-4.08, 18.38)\(\dagger\) & -25.21 (-11.64, 5.99)\(\dagger\) \\  & easy\_hard & baseline & -0.37 (-3.11, 2.47)\(\dagger\) & 7.33 (11.34, 24.32)\(\dagger\) & -36.36 (-11.39, 5.62)\(\dagger\) \\  & easy\_hard & baseline & -0.75 (-3.8,

### Feature selection results with other models

As indicated in Table 9 and Figure 12, the feature selection results reported in the main manuscript are stable across generative models and classification models. Across all comparisons, Spearman's rank correlation is calculated between the feature rankings of the same model type trained on \(\mathbb{D}_{\text{train}}\) and \(\mathbb{D}_{\text{synth}}^{\text{post}}\). In general, random forest and decision trees models tend to display higher concordance in their feature rankings between models trained on synthetic data and models trained on real data compared to xgboost models.

\begin{table}
\begin{tabular}{l l l l} \hline \hline Preprocessing & Postprocessing & Classification Model & Spearman’s Rank \\ Strategy & Strategy & & Correlation \\ \hline baseline & baseline & DecisionTreeClassifier & 0.68 (0.65, 0.71) \\  & & RandomForestClassifier & 0.71 (0.68, 0.74) \\  & & XGBClassifier & 0.51 (0.47, 0.55) \\  & no\_hard & DecisionTreeClassifier & 0.67 (0.63, 0.69) \\  & & RandomForestClassifier & 0.69 (0.66, 0.73) \\  & & XGBClassifier & 0.51 (0.48, 0.56) \\ easy\_hard & baseline & DecisionTreeClassifier & 0.69 (0.66, 0.72) \\  & & RandomForestClassifier & 0.73 (0.7, 0.76) \\  & & XGBClassifier & 0.52 (0.49, 0.56) \\  & no\_hard & DecisionTreeClassifier & 0.67 (0.63, 0.7) \\  & & RandomForestClassifier & 0.71 (0.68, 0.74) \\  & & XGBClassifier & 0.53 (0.49, 0.56) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Feature selection performance by classification model, averaged across all datasets and generative models using Cleanlab as the data-centric method. Numbers in parentheses indicate 95% bootstrapped CI.

Figure 10: Average performance across all datasets for each generative model by pre- and postprocessing method using Data-IQ as the data-centric method.

### Causal Discovery

As a proof of concept, we investigated whether data-centric processing might be useful for causal discovery. For this purpose, we used DAGMA [43] to estimate a causal graph, \(DAG_{\text{train}}\) based on \(\mathbb{D}_{\text{train}}\). \(DAG_{\text{train}}\) was used as the reference graph, and compared to a causal graph estimated from \(\mathbb{D}_{\text{synth}}^{\text{post}}\). Results for a single dataset are shown in Figure 13 in terms of Structural Hamming Distance (SHD). The results appear promising in this particular example, however, further work is required.

Figure 11: Performance across all models and tasks on the Covid mortality data with added label noise. Using Cleanlab as the data-centric method.

Figure 12: Mean feature selection performance across all datasets by classification model and generative model, using Cleanlab as the data-centric method.

Figure 13: Causal graph discovery performance using Cleanlab as the data-centric method on a single dataset (id=361055).

### Distribution of Scores

#### c.9.1 Main Experiment

To investigate the variability of the performance across the three main tasks, we show the distribution of performance metrics across all generative models, datasets, and processing strategies. The findings for classification performance, feature selection, and model selection are shown in Figure 14, Figure 15, and Figure 16, respectively. All figures are shown using Cleanlab as the data-centric method. Overall, the difficulty of the datasets is highly variable, and the variance by generative model is heterogeneous. In general, the best performing models, such as TabDDPM, exhibit comparatively lower variability across different seeds. In contrast, models with poorer performance, like Bayesian networks and normalizing flows, display higher variability across seeds.

The variance of the performance metrics is higher for the tasks of feature selection and model selection than classification. This discrepancy can likely be partly attributed to the inherent smoothness of the performance metrics. The model ranking score is established by accurately ranking the eight supervised classification models listed in Appendix B.3. Similarly, the feature selection score is determined by correctly ranking the metrics for feature importance across all features within the dataset - a quantity that naturally varies by dataset.

In contrast, classification performance relies on AUROC values computed on the test dataset, which encompasses a relatively large number of samples (1,141 in the smallest dataset). Due to the dissimilarity in the number of samples, models, and features, the metrics for model and feature ranking are inevitably more susceptible to larger variance. This occurs because disparities in a single element can lead to more substantial effects on the performance metrics in comparison to the impact of single elements for classification metric.

Figure 14: Distribution of classification performance across the random seeds by generative model (rows) and dataset ids (columns).

Figure 15: Distribution of feature selection performance across the random seeds by generative model (rows) and dataset ids (columns).

Figure 16: Distribution of model selection performance across the random seeds by generative model (rows) and dataset ids (columns).

[MISSING_PAGE_EMPTY:31]

Figure 17: Distribution of classification performance across the random seeds by generative model (rows) and percent label noise (columns).

Figure 18: Distribution of feature selection performance across the random seeds by generative model (rows) and percent label noise (columns).

Figure 19: Distribution of model selection performance across the random seeds by generative model (rows) and percent label noise (columns).

[MISSING_PAGE_FAIL:35]

[MISSING_PAGE_FAIL:36]

### Postprocessing the real data

Tables 13 and 14 investigate the effect of postprocessing the original data (i.e. removing hard examples) on the main experimental benchmark data and on the datasets with added label noise, respectively. The performances for feature and model selection are obtained by comparing the feature and model rankings against the non-postprocessed version of the real data, in a similar manner as the performance of the generative models. The results indicate that postprocessing has a minor impact on classification: a slight negative effect on the purportedly clean benchmark datasets, and a slight positive effect on the datasets with added label noise. Spearman's Rank Correlation for feature selection remains high across both the main benchmark datasets (0.94) and the datasets with label noise (0.88), whereas the ranking of models is lower at approximately 0.7 for both types of data.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline Generative Model & Preprocessing & Postprocessing & Classification & Feature Selection & Model Selection \\  & Strategy & Strategy & & & & \\ \hline None & None & None & 0.866 (0.856, 0.876) & - & - \\  & & no\_hard & 0.859 (0.837, 0.88) & 0.937 (0.926, 0.946) & 0.697 (0.647, 0.747) \\ \hline bayesian\_network & baseline & baseline & 0.622 (0.588, 0.656) & 0.091 (-0.004, 0.188) & 0.155 (0.047, 0.263) \\  & & no\_hard & 0.624 (0.59, 0.659) & 0.043 (-0.065, 0.143) & 0.112 (0.021, 0.207) \\  & easy\_hard & baseline & 0.624 (0.592, 0.66) & 1.0 (-0.015, 0.213) & 0.297 (0.202, 0.387) \\  & no\_hard & 0.627 (0.594, 0.661) & 0.082 (-0.033, 0.195) & 0.197 (0.104, 0.291) \\ \hline ctgan & baseline & baseline & 0.797 (0.771, 0.822) & 0.63 (0.568, 0.699) & 0.519 (0.457, 0.581) \\  & & no\_hard & 0.807 (0.782, 0.829) & 0.653 (0.592, 0.714) & 0.578 (0.512, 0.648) \\  & easy\_hard & baseline & 0.791 (0.763, 0.818) & 0.63 (0.566, 0.692) & 0.513 (0.441, 0.576) \\  & & no\_hard & 0.8 (0.772, 0.828) & 0.631 (0.566, 0.691) & 0.594 (0.531, 0.655) \\ \hline ddpm & baseline & baseline & 0.813 (0.784, 0.842) & 0.635 (0.552, 0.718) & 0.508 (0.442, 0.571) \\  & easy\_hard & baseline & 0.818 (0.787, 0.849) & 0.645 (0.569, 0.726) & 0.476 (0.406, 0.543) \\  & easy\_hard & baseline & 0.818 (0.788, 0.844) & 0.645 (0.564, 0.723) & 0.544 (0.476, 0.61) \\  & no\_hard & 0.824 (0.796, 0.85) & 0.661 (0.584, 0.738) & 0.539 (0.466, 0.604) \\ \hline nflow & baseline & baseline & 0.737 (0.713, 0.761) & 0.415 (0.342, 0.486) & 0.348 (0.275, 0.415) \\  & no\_hard & 0.745 (0.718, 0.768) & 0.439 (0.372, 0.503) & 0.357 (0.292, 0.425) \\  & easy\_hard & baseline & 0.742 (0.718, 0.768) & 0.455 (0.384, 0.527) & 0.362 (0.297, 0.428) \\  & no\_hard & 0.749 (0.725, 0.772) & 0.445 (0.365, 0.511) & 0.371 (0.31, 0.436) \\ \hline tvae & baseline & baseline & 0.792 (0.766, 0.816) & 0.675 (0.623, 0.723) & 0.506 (0.438, 0.575) \\  & no\_hard & 0.8 (0.775, 0.828) & 0.675 (0.63, 0.721) & 0.488 (0.419, 0.561) \\  & easy\_hard & baseline & 0.79 (0.763, 0.814) & 0.687 (0.642, 0.73) & 0.477 (0.416, 0.538) \\  & no\_hard & 0.797 (0.771, 0.822) & 0.699 (0.653, 0.742) & 0.507 (0.437, 0.578) \\ \hline \hline \end{tabular}
\end{table}
Table 13: Summarised performance across all datasets, with an additional row showing the performance of the real data with data-centric postprocessing.

[MISSING_PAGE_FAIL:38]

[MISSING_PAGE_FAIL:39]

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline Task & Processing & Estimate & Std. Error & p-value & Significant \\ \hline Classification & preprocessing\_strategy[T.easy\_hard] & 0.017 & 0.002 & \textless{}0.000 & 1 \\  & postprocessing\_strategy[T.no\_hard] & 0.010 & 0.002 & \textless{}0.000 & 1 \\ Feature Selection & preprocessing\_strategy[T.easy\_hard] & 0.039 & 0.013 & 0.002 & 1 \\  & postprocessing\_strategy[T.no\_hard] & -0.005 & 0.013 & 0.710 & 0 \\ Model Selection & preprocessing\_strategy[T.easy\_hard] & 0.050 & 0.019 & 0.008 & 1 \\  & postprocessing\_strategy[T.no\_hard] & 0.098 & 0.019 & \textless{}0.000 & 1 \\ \hline \hline \end{tabular}
\end{table}
Table 16: Main effect of the pre- and postprocessing strategies of the label noise experiment, controlling for the effect of the proportion of label noise and generative model. The estimates represent the difference in the corresponding performance metric (AUROC or Spearman’s Rank Correlation depending on the task), compared to the baseline strategy, i.e. no processing.

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline Task & Processing & Estimate & Std. Error & p-value & Significant \\ \hline Classification & preprocessing\_strategy[T.easy\_hard] & 0.017 & 0.002 & \textless{}0.000 & 1 \\  & postprocessing\_strategy[T.no\_hard] & 0.010 & 0.002 & \textless{}0.000 & 1 \\ Feature Selection & preprocessing\_strategy[T.easy\_hard] & 0.039 & 0.013 & 0.002 & 1 \\  & postprocessing\_strategy[T.no\_hard] & -0.005 & 0.013 & 0.710 & 0 \\ Model Selection & preprocessing\_strategy[T.easy\_hard] & 0.050 & 0.019 & 0.008 & 1 \\  & postprocessing\_strategy[T.no\_hard] & 0.098 & 0.019 & \textless{}0.000 & 1 \\ \hline \hline \end{tabular}
\end{table}
Table 17: Full output of the linear model assessing the impact of data-centric processing on the classification task. The model has the following formula: auroc \(\sim\) dataset\_id \(*\) generative\_model \(+\) preprocessing\(\_\)strategy \(+\) postprocessing\(\_\)strategy.

[MISSING_PAGE_EMPTY:41]

[MISSING_PAGE_EMPTY:42]

[MISSING_PAGE_EMPTY:43]