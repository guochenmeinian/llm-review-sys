# ExID: Offline RL with Intuitive Expert Insights in Limited-Data Settings

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

With the ability to learn from static datasets, Offline Reinforcement Learning (RL) emerges as a compelling avenue for real-world applications. However, state-of-the-art offline RL algorithms perform sub-optimally when confronted with limited data confined to specific regions within the state space. The performance degradation is attributed to the inability of offline RL algorithms to learn appropriate actions for rare or unseen observations. This paper proposes a novel domain knowledge-based regularization technique and adaptively refines the initial domain knowledge to considerably boost performance in limited data with partially omitted states. The key insight is that the regularization term mitigates erroneous actions for sparse samples and unobserved states covered by domain knowledge. Empirical evaluations on standard discrete environment datasets demonstrate a substantial average performance increase compared to ensemble of domain knowledge and existing offline RL algorithms operating on limited data.

## 1 Introduction

Offline RL [9; 1], also referred to as batch RL, is a learning approach that focuses on extracting knowledge solely from static datasets. This class of algorithms has a wider range of applications being particularly appealing to real-world data sets from business [46], healthcare [25], and robotics [35]. However, offline RL poses unique challenges, including over-fitting and the need for generalization to data not present in the dataset. To surpass the behavior policy, offline RL algorithms need to query Q values of actions not in the dataset, causing extrapolation errors [21]. Most offline RL algorithms address this problem by enforcing constraints that ensure that the learned policy does not deviate too far away from the data set's state action distribution [13; 11] or is conservative towards Out-of-Distribution (OOD) actions [21; 20]. However, such approaches are designed on coherent batches [13], which do not account for OOD states.

In many domains, such as business and healthcare, available data is scarce and often confined to expert behaviors within a limited state space. _For example, a sales recommendation system, where historic data may not contain details about many active users and operator gives coupon of higher value to attract sales_. Learning on such limited data sets can curtail the generalization capabilities of state-of-the-art (SOTA) offline RL algorithms, resulting in sub-optimal performance [23]. We illustrate this limitation via Fig 1. In Fig 1a) the state action space of a simple Mountain Car environment [27] is plotted for an expert dataset [32] and a partial dataset with first 10% samples from the entire dataset. Fig 1b) shows the average reward obtained over these data sets and the average difference between the Q value of action taken by the under-performing Conservative Q Learning (CQL) [21] agent and the action in the full expert dataset for unseen states. It can be observed that the performance of the offline RL agent considerably drops. This is attributed to the critic overestimating the Q value of non-optimal actions for states that do not occur in the dataset while training.

In numerous real-world applications, expert insights regarding the general behavior of a policy are often accessible [33]. _For example, sales operators often distribute lower discount coupons to active users to maximize profit_. While these insights may not be optimal, they serve as valuable guidelines for understanding the overall behavior of the policy. A rich literature in knowledge distillation [18] has shown that teacher networks trained on domain knowledge can transfer knowledge to another network unaware of it. This work aims to leverage a teacher network mimicking simple decision tree-based domain knowledge to help offline RL generalize in limited data settings.

The paper makes the following novel contributions:

* We introduce an algorithm dubbed **ExID**, leveraging intuitive human obtainable expert insights. The domain expertise is incorporated into a teacher policy, which improves offline RL in limited-data settings through regularization.
* The teacher based on expected performance improvement of the offline policy during training, improving the teacher network beyond initial heuristics.
* We demonstrate the effectiveness of our methodology on _real sales promotion dataset_, several discrete OpenAI gym and Minigrid environments with standard offline RL data sets and show that ExID significantly exceeds the performance when faced with limited data.

## 2 Related Work

This work improves offline RL learning on batches sampled from static datasets using domain expertise. One of the major concerns in offline RL is the erroneous extrapolation of OOD actions [13]. Two techniques have been studied in the literature to prevent such errors. 1) Constraining the policy to be close to the behavior policy 2) Penalizing overly optimistic Q values [24]. We discuss a few relevant algorithms following these principles. In Batch-Constrained deep Q-learning (BCQ) [13] candidate actions sampled from an adversarial generative model are considered, aiming to balance proximity to the batch while enhancing action diversity. Algorithms like Random Ensemble Mixture Model (REM) [2], Ensemble-Diversified Actor-Critic (EDAC) [3] and Uncertainty Weighted Actor-Critic (UWAC) [42] penalize the Q value according to uncertainty by either using Q ensemble networks or directly weighting the loss with uncertainty. CQL [21] enforces regularization on Q-functions by incorporating a term that reduces Q-values for OOD actions while increasing Q-values for actions within the expected distribution. However, these algorithms do not handle OOD actions for states not in the static dataset and can have errors induced by changes in transition probability.

Integration of domain knowledge in offline RL, though an important avenue, has not yet been

Figure 1: a) Full expert, Mountain Car dataset, and reduced dataset with first 10% samples showing distribution of state (position, velocity) and action b) CQL agent converging to a sub-optimal policy for reduced dataset exhibiting high Q values for actions different from actions in the expert dataset for unseen states.

extensively explored. Domain knowledge incorporation has improved online RL with tight regret bounds [33; 4]. In offline RL, bootstrapping via blending heuristics computed using Monte-Carlo returns with rewards has shown to outperform SOTA algorithms by 9% [15]. Recent works improve offline RL by incorporating a safety expert [40] and preference query [44], contrary to our work which improves imperfect domain knowledge. The closest to our work is Domain Knowledge guided Q learning (DKQ) [46] where domain knowledge is represented in terms of action importance and the Q value is weighted according to importance. However, obtaining action importance in practical scenarios is nontrivial.

## 3 Preliminaries

A DRL setting is represented by a Markov Decision Process (MDP) formalized as \((S,A,T,r,\rho_{0},\gamma)\). Here, \(S\) denotes the state space, \(A\) signifies the action space, \(T(s^{\prime}|s,a)\) represents the transition probability distribution, \(r:S\times A\rightarrow\mathbb{R}\) is the reward function, \(\rho_{0}\) represents the initial state distribution, and \(\gamma\in(0,1]\) is the discount factor. The primary objective of any DRL algorithm is to identify an optimal policy \(\pi(a|s)\) that maximizes \(\mathbb{E}_{s_{t},a_{t}}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t},a_{t})]\) where, \(s_{0}\sim d_{0}(.),a_{t}\sim\pi(.|s_{t})\), and \(s^{\prime}\sim T(.|s_{t},a_{t})\). Deep Q networks (DQNs) [26] learn this objective by minimizing the Bellman residual \((Q_{\theta}(s,a)-B^{\pi_{\theta}}Q_{\theta}(s,a))^{2}\) where \(B^{\pi_{\theta}}Q_{\theta}(s,a)=\mathbb{E}_{s^{\prime}\sim T}[r(s,a)+\gamma \mathbb{E}_{a^{\prime}\sim\pi_{\theta}(.|s^{\prime})}[Q_{\theta^{\prime}}(s^{ \prime},a^{\prime})]]\). The policy \(\pi_{\theta}\) chooses actions that maximize the Q value \(\max_{a^{\prime}\in A}Q_{\theta}(s^{\prime},a^{\prime})\). However, in offline RL where transitions are sampled from a pre-collected dataset \(\mathcal{B}\), the chosen action \(a^{\prime}\) may exhibit a bias towards OOD actions with inaccurately high Q-values. To handle the erroneous propagation from OOD actions, CQL [22] learns conservative Q values by penalizing OOD actions. The CQL loss for discrete action space is given by

\[\mathcal{L}_{\textit{cql}}(\theta)=\min_{Q}\ \alpha\ \mathbb{E}_{s\sim \mathcal{B}}[log\sum_{a}exp(Q_{\theta}(s,a))-\] \[\mathbb{E}_{a\sim\mathcal{B}|s}[Q_{\theta}(s,a)]]+\frac{1}{2} \mathbb{E}_{s,a,s^{\prime}\sim\mathcal{B}}[Q_{\theta}-Q_{\theta^{\prime}}]^{2} \tag{1}\]

Eq. 1 encourages the policy to be close to the actions seen in the dataset. However, CQL works on the assumption of coherent batches, i.e., if \((s,a,s^{\prime})\in\mathcal{B}\), then \(s^{\prime}\in\mathcal{B}\). There is no provision for handling OOD actions for \(s\notin\mathcal{B}\), which can lead to policy failure when data is limited. In the next sections, we present ExID, a domain knowledge-based approach to improve performance in data-scarce scenarios.

## 4 Problem Setting and Methodology

In our problem setting, the RL agent learns the policy on a limited dataset with rare and unseen demonstrations. We define the characteristics of this dataset as follows:

**Definition 4.1**.: A reduced buffer \(\mathcal{B}_{r}\) is a proper subset of the full dataset \(\mathcal{B}\) i.e., \(\mathcal{B}_{r}\subset\mathcal{B}\) satisfying the following conditions:

* Some states in \(\mathcal{B}\) are not present in \(\mathcal{B}_{r}\), i.e., \(\exists s^{\prime}\in\mathcal{B}\land\forall(s,a,s^{\prime}):(s,a,s^{\prime}) \notin\mathcal{B}_{r}\)
* The number of samples \(N(s,a,s^{\prime})\) for some transitions in \(\mathcal{B}\) are less in \(\mathcal{B}_{r}\) i.e, \(\exists(s,a,s^{\prime})\in\mathcal{B}:N(s,a,s^{\prime})_{\mathcal{B}_{r}}<N(s, a,s^{\prime})_{\mathcal{B}}\)

We observe, _performing \(Q-Learning\) by sampling from a limited buffer \(\mathcal{B}_{r}\) may not converge to an optimal policy for the MDP \(M_{\mathcal{B}}\) representing the full buffer._ This can be shown as a special case of (Theorem 1,[13]) as \(p_{\mathcal{B}}(s^{\prime}|s,a)\neq p_{\mathcal{B}_{r}}(s^{\prime}|s,a)\) and no Q updates for \((s,a)\notin\mathcal{B}_{r}\) leading to sub-optimal policy. Please refer to the App. B for analysis and example.

We also assume a set of common sense rules in the form of domain knowledge is available. Domain knowledge \(\mathcal{D}\) is defined as hierarchical decision nodes capturing \(S\to A\) as represented by Eq. 2. Each decision node \(T_{\eta_{i}}\) is represented by a constraint \(\phi_{\eta_{i}}\) and Boolean indicator \(\mu_{\eta_{i}}\) function selects the branch to be traversed based on \(\phi_{\eta_{i}}\).

\[Action =\begin{cases}a_{\eta_{i}}&\text{if }\ leaf\\ \mu_{\eta_{i}}T_{\eta_{i}\swarrow}(s)+(1-\mu_{\eta_{i}})T_{\eta_{i}\searrow}(s )&\text{o/w}\end{cases}\] \[\mu_{\eta_{i}}(s) =\begin{cases}1&\text{if }s\models\phi_{\eta_{i}}\\ 0&\text{o/w}\end{cases} \tag{2}\]We assume that \(\mathcal{D}\) gives heuristically reasonable actions for \(s\models D\) and \(S_{\mathcal{D}}\cap S_{\mathcal{B}_{r}}\neq\emptyset\) where \(S_{\mathcal{D}},S_{\mathcal{B}_{r}}\) are the state coverage of \(\mathcal{D}\) and \(\mathcal{B}_{r}\).

**Training Teacher:** An overview of our methodology is depicted in Fig 2. We first construct a trainable actor network \(\pi_{t}^{\omega}\) parameterized by \(\omega\) from \(\mathcal{D}\), Fig 2 step 1. For training \(\pi_{t}^{\omega}\) synthetic data \(\hat{S}\) is generated by sampling states from a uniform random distribution over state boundaries \(B(s)\), \(\hat{S}=\mathcal{U}(B(S))\). Note that this does not represent the true state distribution and may have state combinations that will never occur. We train \(\pi_{t}^{\omega}\) using behavior cloning where state \(\hat{s}\sim\hat{S}\) is checked with root decision node in Eq. 2. A random action is chosen if \(\hat{s}\) does not satisfy decision node \(T_{\eta_{0}}\) or leaf action is absent. If \(\hat{s}\) satisfies a \(T_{\eta_{i}}\), \(T_{\eta_{i}}\) is traversed and action \(a_{\eta_{i}}\) is returned from the leaf node. This is illustrated in Fig 2 (a). We term the pre-trained actor network \(\pi_{t}^{\omega}\) as the teacher policy.

**Regularizing Critic:** We now introduce Algo 1 (App C) to train an offline RL agent on \(\mathcal{B}_{r}\). Algo 1 takes \(\mathcal{B}_{r}\) and pretrained \(\pi_{t}^{\omega}\) as input. The algorithm uses two hyper-parameters, warm start parameter \(k\) and mixing parameter \(\lambda\). A critic network \(Q_{s}^{\theta}\) with Monte-Carlo (MC) dropout and target network \(Q_{s}^{\theta^{\prime}}\) are initialized. ExID is divided into two phases. In the first phase, we aim to warm start the critic network \(Q_{s}^{\theta}\) with actions from \(\pi_{t}^{\omega}\) as shown in Fig 2b ( i). However, this must be done selectively as the teacher's policy is random around the states that do not satisfy domain knowledge. In each iteration, we first check the states sampled from a mini-batch of \(\mathcal{B}_{r}\) with \(\mathcal{D}\). For the states which satisfy \(\mathcal{D}\) we compute the teacher action \(\pi_{t}^{\omega}(s)\) and critic's action \(\operatorname*{argmax}_{a}(Q_{s}^{\theta}(s,a))\) and collect it in lists \(a_{t},a_{s}\), Algo 1 lines 4-10. Our main objective is to keep actions chosen by the critic network for \(s\models\mathcal{D}\) close to the teacher's policy. To achieve this, we introduce a regularization term:

\[\mathcal{L}_{r}(\theta)=\underbrace{\mathbb{E}_{s\sim\mathcal{B}_{r}\wedge s \models\mathcal{D}}}_{\text{states matching domain rule}}\underbrace{[Q_{s}^{ \theta}(s,a_{s})-Q_{s}^{\theta}(s,a_{t})]^{2}}_{\text{Q regularizer}} \tag{3}\]

Eq 3 incentivizes the critic to increase Q values for actions from \(\pi_{t}^{\omega}\) and decreases Q values for other actions when \(\operatorname*{argmax}_{a}(Q_{s}^{\theta}(s,a))\neq\pi_{t}^{\omega}(s)\) for states that satisfy domain knowledge. Note that Eq 3 will only be 0 when \(\operatorname*{argmax}_{a}(Q_{s}^{\theta}(s,a))=\pi_{t}^{\omega}(s)\) for \(s\models\mathcal{D}\). It is also set to 0 for \(s\not\models\mathcal{D}\). However, since \(\pi_{t}^{\omega}\) mimicking heuristic rules is sub-optimal, it is also important to incorporate learning from the data. The final loss is a combination of Eq. 1 and Eq. 3 with a mixing parameter \(\lambda\in[0,1]\):

Figure 2: Overview of the proposed methodology (a) Training a teacher policy network with domain knowledge and synthetic data (b) Updating the offline RL critic network with teacher network

\[\mathcal{L}(\theta)=\mathcal{L}_{\mathit{cql}}(\theta)+\lambda\mathbb{E}_{s\sim \mathcal{B}_{r}\wedge s\models\mathcal{D}}[Q^{\theta}_{s}(s,a_{s})-Q^{\theta}_{ s}(s,a_{t})]^{2} \tag{4}\]

The choice of \(\lambda\) and the warm start parameter \(k\) depends on the quality of \(\mathcal{D}\). In the case of perfect domain knowledge, \(\lambda\) would be set to 1, and setting \(\lambda\) to 0 would lead to the vanilla CQL loss. Mixing both the losses allows the critic to learn both from the data in \(\mathcal{B}_{r}\) and knowledge in \(\mathcal{D}\).

**Updating Teacher:** Given a reasonable warm start, the critic is expected to give higher Q values for optimal actions for \(s\in\mathcal{D}\cap\mathcal{B}_{r}\) as it learns from data. We aim to leverage this knowledge to enhance the initial teacher policy \(\pi^{\omega}_{t}\) trained on heuristic domain knowledge. For \(s\sim\mathcal{B}\) and \(s\models\mathcal{D}\), we calculate the average Q values over critic actions and teacher actions and check which one is higher in Algo 1 line 11 which refers to Cond. 6. For brevity \(\mathbb{E}_{s\sim\mathcal{B}_{r}\wedge s\models\mathcal{D}}\) is written as \(\mathbb{E}\). If \(\mathbb{E}(Q^{\theta}_{s}(s,a_{s}))>\mathbb{E}(Q^{\theta}_{s}(s,a_{t}))\) it denotes the critic expects a better return on an average over its own policy than the teacher's policy. Hence, we can use the critic's policy to update \(\pi^{\omega}_{t}\), making it better over \(\mathcal{D}\). However, only checking the critic's value can be erroneous as the critic can have high values for OOD actions. We check the average uncertainty of the predicted Q values to prevent the teacher from getting updated by OOD actions. Uncertainty has been shown to be a good metric for OOD action detection by [42, 3]. A well-established methodology to capture uncertainty is predictive variance, which takes inspiration from Bayesian formulation for the critic function and aims to maximize \(p(\theta|X,Y)=p(Y|X,\theta)p(\theta)/p(Y|X)\) where \(X=(s,a)\) and \(Y\) represents the true Q value of the states. However, \(p(Y|X)\) is generally intractable and is approximated using Monte Carlo (MC) dropout, which involves including dropout before every layer of the critic network and using it during inference [14]. Following [42], we measure the uncertainty of prediction using Eq 5.

\[Var^{T}[Q(s,a)]\approx\frac{1}{T}\sum_{t=1}^{T}[Q(s,a)-\bar{Q}(s,a)]^{2} \tag{5}\]

Eq 5 estimates the variance of Q value \(Q(s,a)\) for an action \(a\) using \(T\) forward passes on the \(Q^{\theta}_{s}(s,a)\) with dropout where \(\bar{Q}(s,a)\) represents the predictive mean. We check the average uncertainty of the Q value for action chosen by the critic and teacher policy over the states that match domain knowledge in a batch. The teacher network is updated using the critic's action only when the policy expects a higher average Q return on its action and the average uncertainty of taking this action is lower than the teacher action. \(\mathbb{E}(Var^{T}Q^{\theta}_{s}(s_{r},a_{s}))<\mathbb{E}(Var^{T}Q^{\theta}_{ s}(s_{r},a_{t}))\) indicates the actions were learned from the expert data in the buffer and are not OOD samples. The condition is summarized in cond. 6:

\[\mathbb{E}(Q^{\theta}_{s}(s_{r},a_{s}))>\mathbb{E}(Q^{\theta}_{s}(s_{r},a_{t}))\wedge\]

\[\mathbb{E}(Var^{T}Q^{\theta}_{s}(s_{r},a_{s}))<\mathbb{E}(Var^{T}Q^{\theta}_{ s}(s_{r},a_{t})) \tag{6}\]

We update the teacher with cross-entropy described in Eq 7:

\[\mathcal{L}(\omega)=-\sum_{s\models D}(\pi^{\omega}_{t}(s)log(\pi_{s}(s))) \tag{7}\]

where, \(\pi_{s}(s,a)=\frac{e^{Q(s,a)}}{\sum_{s^{\prime}}Q(s,a^{\prime})}\). When the critic's policy is better than the teacher's policy, \(\mathcal{L}_{r}(\theta)\) is set to 0 Algo 1 Lines 11 to 13. Finally, the critic network is updated using calculated loss \(\mathcal{L}(\theta)\) Algo 1 Lines 17-18. We theoretically analyse the implications of using ExID in propositions 4.2 and 4.3.

**Proposition 4.2**.: _Denote \(\hat{\pi}\) as the policy learned by ExID, \(\pi_{u}\) as any offline RL policy learned on \(\mathcal{B}_{r}\) and optimal \(Q\) function as \(Q^{*}\) and \(V\) function as \(V^{*}\). Then it holds that_

\[\eta(\hat{\pi})-\eta(\pi_{u})\geq\mathbb{E}_{s\sim O[\pi_{u}}[V^{*}(s)-Q^{*}( s,\pi_{u}(s))]-\bar{\rho}_{\hat{\pi}}\alpha\]

Where \(\alpha=\mathbb{E}_{s\sim O}[V^{*}(s)-Q^{*}(s,\hat{\pi}(s))]\), \(\bar{\rho}_{\pi}(s)=[\frac{1}{|S_{\hat{\pi}}|(1-\gamma)},\frac{1}{1-\gamma}]\) (\(|\)\(S_{\hat{\pi}}\)\(|\) is the number of different states observed by \(\hat{\pi}\)) and \(O\notin\mathcal{B}_{r}\). Here \(\alpha\) denotes the quality of regularized action for \(s\notin\mathcal{B}_{r}\). Hence, updating \(\pi^{\omega}_{t}\) is important as high divergence of action from the optimal can lead to performance degradation. In offline RL, the extrapolation error for non optimal action is usually high for states not observed in dataset (as illustrated in 1b), regularization can lead performance improvement when \(\pi^{\omega}_{t}\) is reasonable. Furthermore, in ExID coarse actions from \(\pi^{\omega}_{t}\) are updated driving them closer to the optimal actions, improving the performance lower bound. Additionally \(\pi^{\omega}_{t}\) increases \(|\)\(S_{\hat{\pi}}\)\(|\) making \(\bar{\rho}_{\pi}\ll 1\) in practice further improving the performance lower bound. _Proof is deferred to App. A_.

**Proposition 4.3**.: _ExID reduces generalization error if \(Q^{*}(s,\pi_{t}^{\omega}(s))>Q^{*}(s,\pi_{u}(s))\) for \(s\in\mathcal{D}\cap\mathcal{B}_{r}\)._

Proof is deferred to App. A.: In the next section, we discuss our empirical evaluations.

## 5 Empirical Evaluations

We investigate the following through our empirical evaluations: _1. Does ExID perform better than combining \(\mathcal{D}\) and offline RL algoes on different environments with datasets exhibiting rare and OOD states Sec 5.2? 2. Does ExID generalize to OOD states covered by \(\mathcal{D}\) Sec 5.4? 3. What is the effect of varying \(k\), \(\lambda\) and updating \(\pi_{t}^{\omega}\) Sec 5.5? 4. How does performance vary with the quality of \(\mathcal{D}\) Sec 5.6?_

### Experimental Setting

We evaluate our methodology on open-AI gym [5], MiniGrid [6] and _real sales promotion (SP) [30]_ offline data sets. All our data sets are generated using standard methodologies defined in [32; 31]_except SP which is generated by human operators_. All experiments have been conducted on a Ubuntu 22.04.2 LTS system with 1 NVIDIA K80 GPU, 4 CPUs, and 61GiB RAM. App. F notes the hyperparameter values and network architectures.

**Dataset:** We experiment on three types of data sets. _Expert Data-set_[10; 16; 22] generated using an optimal policy without any exploration with high trajectory quality but low state action coverage. _Replay Data-set_[2; 13] generated from a policy while training it online, exhibiting a mixture of multiple behavioral policies with high trajectory quality and state action coverage. _Noisy Data-set_[12; 13; 22; 16] generated using an optimal policy that also selects random actions with \(\epsilon\) greedy strategy where \(\epsilon=0.2\) having low trajectory quality and high state action coverage. Additionally we also experiment on human generated dataset for sales promotion task.

**Baselines:** We do comparative studies on 10 baselines for OpenAI gym datasets. The first baseline simply checks the conditions of \(\mathcal{D}\) and applies corresponding actions in execution. The performance of this baseline shows that \(\mathcal{D}\) is imperfect and does not achieve the optimal reward. CQL SE is from [40] where the expert is replaced by \(\mathcal{D}\). The other baselines are an ensemble of \(\mathcal{D}\) and eight algorithms popular in the Offline RL literature for discrete environments. These algorithms include Behavior Cloning (BC) [29], Behaviour Value Estimation (BVE) [16], Quantile Regression DQN (QRDQN) [7], REM, MCE, BCQ, CQL and Critic Regularized Regression Q-Learning (CRR) [41]. _For a fair comparison, we use actions from domain knowledge for states not in the buffer and actions from the trained policy for other states to obtain the final reward._ Hence, each algorithm is renamed with the suffix D in Table 5.1.

**Limiting Data:** To create limited-data settings for benchmark datasets, we first extract a small percentage of samples from the full dataset and remove some of the samples based on state conditions. This is done to ensure the reduced buffer satisfies the conditions defined in Def 4.1. We describe the specific conditions of removal in the next section. Further insights and the state visualizations for selected reduced datasets are in App H. **Note : no data reduction has been performed on SP dataset to demonstrate a real dataset exhibits characteristics of reduced buffer**.

### Performance across Different Datasets

Our results for OpenAI gym environments are summarised in Table 5.1 and Minigrid in Table 3 (App D). We observe the performance of offline RL algorithms degrades substantially when part of the data is not seen and trajectory ratios change. For these cases with only 10% partial data, ExID surpasses the performance by at least 27% in the presence of reasonable domain knowledge. The proposed method performs strongest on the replay dataset where the contribution of \(L_{r}(\theta)\) is significant due to state coverage, and the teacher learns from high-quality trajectories. Environment details are described in the App. D. All domain knowledge trees are shown in the App. D Fig 10. We describe limiting data conditions and domain knowledge specific to the environment as follows:

**Mountain Car Environment:**[27] We use simple, intuitive domain knowledge in this environment shown in the App. D Fig 10 (c), which represents taking a left action when the car is at the bottom of the valley with low velocity to gain momentum; otherwise, taking the right action to drive the car up. Fig 6 (c) shows the state action pairs this rule generates on states sampled from a uniform random distribution over the state boundaries. It can be observed that the states of \(\mathcal{D}\) cover part of the missing

[MISSING_PAGE_FAIL:7]

the sales, but the cost will also increase. The goal for the platform operator is to maximize the total profit. The horizon of the dataset is 50 days for the training and 30 days for the test. Domain knowledge ([30], App A] : Active users can be given more coupons with lower discount to maximize profit. We model this as \(order_{number}>60\wedge Avg_{fee}>0.8\implies[5,0.95]\) where action 1 is number of coupons range [0,5] and action 2 is coupon value (discount value = (1-coupon value)) range [0.6,0.95]. The dataset exhibits the properties in Def 4.1 as first 50 days of sales does not contain many active users as reported in the coverage column of Tab 2 depicting scarcity. The domain rule is imperfect as coupon value and number depend on multiple factors such as user purchase history and behavior. As illustrated in the table 2 and Fig 3 (c) the intuitive domain rule enhances performance by 10.49% in the real dataset.

### Generalization to OOD states and contribution of \(\mathcal{L}_{r}(\theta)\)

In Fig 4 (a), (b), we plot \(Q_{s}^{\theta}(s,a_{expert})-Q_{s}^{\theta}(s,a_{\theta})\) for CQL and EXID policies for different datasets of Mountain-Car environments. Action \(a_{expert}\) is obtained from the full expert dataset where position \(>-0.8\). We observe that the Q value for actions of CQL policy diverges from the expert policy actions with high values for the states not in the reduced buffer, whereas ExID stays close to the expert actions for the unseen states. This empirically shows generalization to OOD states not in the dataset but covered by domain knowledge. In Fig 4 (d), we plot the contribution by \(\mathcal{L}_{r}(\theta)\) during the training and observe the contribution is higher for replay data sets with more state coverage.

### Performance on varying \(\lambda\), \(k\), and ablation of \(\pi_{t}^{\omega}\)

We study the effect of varying \(\lambda\) on the algorithm for the given domain knowledge. We empirically observe setting a high or a low \(\lambda\) can yield sub-optimal performance, and \(\lambda=0.5\) generally gives good performance. In Fig 5 (a), we show this effect for LunarLander. Plots for other environments are in the App. G Fig 11. For \(k\) we observe setting the warm start parameter to 0 yields a sub-optimal policy, as the critic may update \(\pi_{t}^{\omega}\) without completely learning from it. The starting performance increases with an increase in \(k\) as shown in Fig 5 (b) for LunarLander. \(k=30\) works best according to empirical evaluations. Plots for other environments are in the App. G Fig 12. We show two ablations for Cart-pole in Fig 5 (c) with no teacher update after the warm start and no inclusion of \(\mathcal{L}_{r}(\theta)\) after the warm start. The warm start in this environment is set to 30 episodes. Fig 5 c) shows without teacher updated, the sub-optimal teacher drags down the performance of the policy beyond the warm start, exhibiting the necessity of \(\pi_{t}^{\omega}\) update. Also, the student converges to a sub-optimal policy if no \(\mathcal{L}_{r}(\theta)\) is included beyond the warm start.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Dataset** & \(\mathcal{D}\) & coverage \(\mathcal{D}\) & CQL + \(\mathcal{D}\) & CQLSE & EXID & Performance gain \\ \hline Sales & 654.68 & 20.32\% & 722.06 \(\pm\) 71.40 & 727.03 \(\pm\) & 802.91 & 10.49\% \\ Promotion & \(\pm\) 20.06 & & & 49.56 & \(\pm\) 41.69 & \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results on human generated Sales Promotion dataset

Figure 4: Q value difference between CQL and EXID for expert and policy action on states not present in the buffer for a) expert b) noisy in log scale c) contribution of \(\mathcal{L}_{r}(\theta)\)

### Effect of varying \(\mathcal{D}\) quality

We show the effect of choosing policies as \(\mathcal{D}\) with different average rewards for Lunar-Lander expert data in Fig 6 (a) and (b). Rule 1 is optimal and has almost the same effect as Rule 3, which is the \(\mathcal{D}\) used in our experiments exhibiting that updating a sub-optimal \(\mathcal{D}\) can lead to equivalent performance as optimal \(\mathcal{D}\). Using a rule with high uncertainty, as Rule 2, induces high uncertainty in the learned policy but performs slightly better than the baseline. Rule 4, which has a lower average reward, also causes gains on average performance with slower convergence. Finally, Rule 5, with very bad actions, affects policy performance adversely and leads to a performance lower than baseline CQL.

## 6 Conclusion and Limitation

In this paper, we study the effect of limited and partial data on offline RL and observe that the performance of SOTA offline RL algorithms is sub-optimal in such settings. The paper proposes a methodology to handle offline RL's performance degradation using domain insights. We incorporate a regularization loss in the CQL training using a teacher policy and refine the initial teacher policy while training. We show that incorporating reasonable domain knowledge in offline RL enhances performance, achieving a performance close to full data. However, this method is limited by the quality of the domain knowledge and the overlap between domain knowledge states and reduced buffer data. The study is also limited to discrete domains. In the future, the authors would like to improve on capturing domain knowledge into the policy network without dependence on data and extending the methodology to algorithms that handle continuous action space.

## 7 Broader Impact

During the trial-and-error training phase, RL agents may exhibit irrational behavior, which can be risky and costly in real-world scenarios. As a more practical alternative to online RL, offline RL

Figure 5: (a) Effect of different \(\lambda\) on the performance of ExID on Lunar Lander (b) Effect of different \(k\) on the performance of EXID on Lunar Lander (c) Performance of EXID with teacher update, no teacher update, and just warm start on Cart-pole.

Figure 6: (a) \(\mathcal{D}\) with different average rewards (b) Performance effect on Lunar-lander (c) State distribution generated for training the teacher network for mountain-carutilizes pre-existing collected data to eliminate the need for real-time interactions during training. However, a drawback of offline RL is its dependence on the quality and quantity of historical data, which, when sub-optimal, could adversely affect overall performance. Therefore, through this work, we use domain knowledge to suppress erroneous actions when available data is limited. However, this inclusion may facilitate harmful behavior in the presence of biased domain knowledge. Therefore, we advocate the use of well-regulated domain knowledge obtained from experts. Beyond this, we do not foresee any ethical impact on our work.

## References

* [1]A survey on offline reinforcement learning: Taxonomy, review, and open problems. IEEE Transactions on Neural Networks and Learning Systems2023. ISSN 21622388. doi: 10.1109/TNNLS.2023.3250269.
* Agarwal et al. [2020] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. In International Conference on Machine Learning, pp. 104-114. PMLR, 2020.
* An et al. [2021] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. Advances in neural information processing systems34:7436-7447, 2021.
* Bartlett and Tewari [2009] Peter L. Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforcement learning in weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI '09, page 35-42, Arlington, Virginia, USA, 2009. AUAI Press. ISBN 9780974903958.
* Brockman et al. [2016] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
* Chevalier-Boisvert et al. [2023] Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. CoRRabs/2306.13831, 2023.
* Dabney et al. [2018] Will Dabney, Mark Rowland, Marc Bellemare, and Remi Munos. Distributional reinforcement learning with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
* Ernst et al. [2005] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research6, 2005.
* Fu et al. [2020] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
* Fujimoto and Gu [2021] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. Advances in neural information processing systems34:20132-20145, 2021.
* Fujimoto et al. [2019] Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch deep reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019.
* Fujimoto et al. [2019] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International conference on machine learning, pages 2052-2062. PMLR, 2019.
* Gal and Ghahramani [2016] Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. Advances in neural information processing systems29, 2016.
* Geng et al. [2023] Sinong Geng, Aldo Pacchiano, Andrey Kolobov, and Ching-An Cheng. Improving offline rl by blending heuristics. arXiv preprint arXiv:2306.00321, 2023.
* Gulcehre et al. [2021] Caglar Gulcehre, Sergio Gomez Colmenarejo, Ziyu Wang, Jakub Sygnowski, Thomas Paine, Konrad Zolna, Yutian Chen, Matthew Hoffman, Razvan Pascanu, and Nando de Freitas. Regularized behavior value estimation. arXiv preprint arXiv:2103.09575, 2021.
* Hinton et al. [2015] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
* Hu et al. [2016] Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural networks with logic rules. arXiv preprint arXiv:1603.06318, 2016.

* Kakade and Langford [2002] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In _Proceedings of the Nineteenth International Conference on Machine Learning_, pages 267-274, 2002.
* Kostrikov et al. [2021] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisher divergence critic regularization. In _International Conference on Machine Learning_, pages 5774-5783. PMLR, 2021.
* Kumar et al. [2019] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. _Advances in Neural Information Processing Systems_, 32, 2019.
* Kumar et al. [2020] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 33:1179-1191, 2020.
* Levine et al. [2020] Sergey Levine, Aviral Kumar, G. Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _ArXiv_, abs/2005.01643, 2020. URL [https://api.semanticscholar.org/CorpusID:218486979](https://api.semanticscholar.org/CorpusID:218486979).
* Levine et al. [2020] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* Liu et al. [2020] Siqi Liu, Kay Choong See, Kee Yuan Ngiam, Leo Anthony Celi, Xingzhi Sun, and Mengling Feng. Reinforcement learning for clinical decision support in critical care: comprehensive review. _Journal of medical Internet research_, 22(7):e18477, 2020.
* Mnih et al. [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533, 2015.
* Moore [1990] Andrew William Moore. Efficient memory-based learning for robot control. Technical report, University of Cambridge, Computer Laboratory, 1990.
* Murphy [2005] Susan A Murphy. A generalization error for q-learning. 2005.
* Pomerleau [1991] Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. _Neural computation_, 3(1):88-97, 1991.
* Qin et al. [2022] Rong-Jun Qin, Xingyuan Zhang, Songyi Gao, Xiong-Hui Chen, Zewen Li, Weinan Zhang, and Yang Yu. Neorl: A near real-world benchmark for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 35:24753-24765, 2022.
* Schweighofer et al. [2021] Kajetan Schweighofer, Markus Hofmarcher, Marius-Constantin Dinu, Philipp Renz, Angela Bitto-Nemling, Vihang Prakash Patil, and Sepp Hochreiter. Understanding the effects of dataset characteristics on offline reinforcement learning. In _Deep RL Workshop NeurIPS 2021_, 2021. URL [https://openreview.net/forum?id=A4EWtf-T03Y](https://openreview.net/forum?id=A4EWtf-T03Y).
* Schweighofer et al. [2022] Kajetan Schweighofer, Marius-constantin Dinu, Andreas Radler, Markus Hofmarcher, Vihang Prakash Patil, Angela Bitto-Nemling, Hamid Eghbal-zadeh, and Sepp Hochreiter. A dataset perspective on offline reinforcement learning. In _Conference on Lifelong Learning Agents_, pages 470-517. PMLR, 2022.
* Silva and Gombolay [2021] Andrew Silva and Matthew Gombolay. Encoding human domain knowledge to warm start reinforcement learning. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 5042-5050, 2021.
* Silva et al. [2020] Andrew Silva, Matthew Gombolay, Taylor Killian, Ivan Jimenez, and Sung-Hyun Son. Optimization methods for interpretable differentiable decision trees applied to reinforcement learning. In _International conference on artificial intelligence and statistics_, pages 1855-1865. PMLR, 2020.
* Sinha et al. [2022] Samarth Sinha, Ajay Mandlekar, and Animesh Garg. S4rl: Surprisingly simple self-supervision for offline reinforcement learning in robotics. In Aleksandra Faust, David Hsu, and Gerhard Neumann, editors, _Proceedings of the 5th Conference on Robot Learning_, volume 164 of _Proceedings of Machine Learning Research_, pages 907-917. PMLR, 08-11 Nov 2022.
* Sohn et al. [2020] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple semi-supervised learning framework for object detection. _arXiv preprint arXiv:2005.04757_, 2020.
* Tang and Wang [2018] Jiaxi Tang and Ke Wang. Ranking distillation: Learning compact ranking models with high performance for recommender system. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 2289-2298, 2018.

* Tang et al. [2019] Raphael Tang, Yao Lu, Liqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-specific knowledge from bert into simple neural networks. _arXiv preprint arXiv:1903.12136_, 2019.
* Tseng et al. [2022] Wei-Cheng Tseng, Tsun-Hsuan Johnson Wang, Yen-Chen Lin, and Phillip Isola. Offline multi-agent reinforcement learning with knowledge distillation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 226-237. Curran Associates, Inc., 2022.
* Verma et al. [2024] Richa Verma, Durgesh Kalwar, Harshad Khadilkar, and Balaraman Ravindran. Guiding offline reinforcement learning using a safety expert. In _Proceedings of the 7th Joint International Conference on Data Science & Management of Data (11th ACM IKDD CODS and 29th COMAD)_, pages 82-90, 2024.
* Wang et al. [2020] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression. _Advances in Neural Information Processing Systems_, 33:7768-7778, 2020.
* Wu et al. [2021] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua M. Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. In _International Conference on Machine Learning_, 2021. URL [https://api.semanticscholar.org/CorpusID:234763307](https://api.semanticscholar.org/CorpusID:234763307).
* Xie et al. [2020] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10687-10698, 2020.
* Yang et al. [2023] Qisen Yang, Shenzhi Wang, Matthieu Gaetan Lin, Shiji Song, and Gao Huang. Boosting offline reinforcement learning with action preference query. In _International Conference on Machine Learning_, pages 39509-39523. PMLR, 2023.
* Yuan et al. [2020] Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation via label smoothing regularization. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3902-3910, 2020. doi: 10.1109/CVPR42600.2020.00396.
* Zhang and Yu [2021] Xiaoxuan Zhang and S Zhang Y Yu. Domain knowledge guided offline q learning. In _Second Offline Reinforcement Learning Workshop at Neurips_, volume 2021, 2021.
* IEEE Conference on Computer Communications_, pages 1-10, 2021. doi: 10.1109/INFOCOM42981.2021.9488863.

## Appendix A Theoretical Analysis

### Notations

For any deterministic policy \(\pi\) the performance return is formulated as \(\eta(\pi)=\mathbb{E}_{\tau\sim\pi}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t},a_{t})]\)

For any policy \(\pi\), \(\rho_{\pi}\) is the (unormalized) discounted visitation frequency given by \(\rho_{\pi}(s)=\sum_{t=0}^{\infty}\gamma^{t}P(s_{t}=s)\) where \(s_{0}\sim\rho^{0}(s_{0})\) and the trajectory \((s_{0},s_{1},\dots)\) is sampled from the policy \(\pi\) and \(\rho_{\pi}(s)\in[0,\frac{1}{1-\gamma}]\). \(\bar{\rho}_{\pi}(s)=sup\{\rho_{\pi}(s),s\in S\}\in[\frac{1}{|S_{\pi}|(1-\gamma )},\frac{1}{(1-\gamma)}]\)

We denote the regularized policy learned by ExID on \(\mathcal{B}_{r}\) as \(\hat{\pi}\) and the unregularized policy as \(\pi_{u}\).

**Lemmas**

We introduce the following Lemma required for our theoretical analysis.

**Lemma A.1**.: _([44]) Given two policies \(\pi_{1}\) and \(\pi_{2}\)_

\[\eta(\pi_{1})-\eta(\pi_{2})=\int_{s\in S}\rho_{\pi_{1}}(s)(Q^{*}(s,\pi_{1}(s)- V^{*}(s))ds-\int_{s\in S}\rho_{\pi_{2}}(s)(Q^{*}(s,\pi_{2}(s)-V^{*}(s))ds\]

Proof.: Please refer to Lemma A.1 Eq 17 in [44] 

**Proposition A.2**.: _(4.2) Denote \(\hat{\pi}\) as the policy learned by ExID, \(\pi_{u}\) as any offline RL policy learned on \(\mathcal{B}_{r}\) and optimal Q function as \(Q^{*}\) and \(V\) function as \(V^{*}\). Then it holds that_

\[\eta(\hat{\pi})-\eta(\pi_{u})\geq\mathbb{E}_{s\sim O|\pi_{u}}[V^{*}(s)-Q^{*}(s,\pi_{u}(s))]-\bar{\rho}_{\hat{\pi}}\alpha\]

Proof.: According to [19] performance improvement between two policies if given by

\[\eta(\pi_{1})=\eta(\pi_{2})+\mathbb{E}_{\tau\sim\pi_{1}}\left[\sum_{t=0}^{ \infty}\gamma^{t}Q_{\pi_{2}}(s_{t},a_{t})-V_{\pi_{2}}(s_{t})\right] \tag{8}\]

Replacing \(\pi_{1}\) by \(\hat{\pi}\) and \(\pi_{2}\) by \(\pi_{u}\) and by following Lemma A.1

\[\eta(\hat{\pi})-\eta(\pi_{u})=\int_{s\in S}\rho_{\pi}(s)(Q^{*}(s, \hat{\pi}(s))-V^{*}(s))ds-\int_{s\in S}\rho_{\pi_{u}}(s)(Q^{*}(s,\pi_{u}(s))- V^{*}(s))ds \tag{9}\] \[=\int_{s\in S}\rho_{\pi_{u}}(s)(V^{*}(s)-Q^{*}(s,\pi_{u}(s)))ds- \int_{s\in S}\rho_{\hat{\pi}}(s)(V^{*}(s)-Q^{*}(s,\hat{\pi}(s)))ds \tag{10}\]

Dividing the state space into in dataset domain states (I) and OOD states (O). The

\[\underbrace{\left[\int_{s\in I}\rho_{\pi_{u}}(s)(V^{*}(s)-Q^{*}( s,\pi_{u}(s)))ds-\int_{s\in I}\rho_{\hat{\pi}}(s)(V^{*}(s)-Q^{*}(s,\hat{\pi}(s)))ds \right]}_{a}+ \tag{11}\] \[\underbrace{\left[\int_{s\in O}\rho_{\pi_{u}}(s)(V^{*}(s)-Q^{*}( s,\pi_{u}(s)))ds-\int_{s\in O}\rho_{\hat{\pi}}(s)(V^{*}(s)-Q^{*}(s,\hat{\pi}(s)))ds \right]}_{b} \tag{12}\]

Since the regularization loss facilitates visitation to OOD states via knowledge distillation we assume \(\rho_{\hat{\pi}}=\rho_{\pi_{u}}-\Delta_{i}\) for \(s\in i\) and \(\rho_{\hat{\pi}}=\rho_{\pi_{u}}+\Delta_{o}\) for \(s\in o\) where \(\Delta_{i}\in[0,\rho_{\pi_{u}(s)}]\) and \(\Delta_{o}\in[0,\frac{1}{1-\gamma}-\rho_{\pi_{u}(s)}]\)

\[a=\int_{s\in I}\rho_{\pi_{u}}(s)(V^{*}(s)-Q^{*}(s,\pi_{u}(s)))ds- \int_{s\in I}(\rho_{\pi_{u}}-\Delta_{i})(s)(V^{*}(s)-Q^{*}(s,\hat{\pi}(s)))ds \tag{13}\] \[=\int_{s\in I}\rho_{\pi_{u}}(s)(Q^{*}(s,\hat{\pi}(s))-Q^{*}(s,\pi_ {u}(s)))ds+\int_{s\in I}\Delta_{i}(s)(V^{*}(s)-Q^{*}(s,\hat{\pi}(s)))ds \tag{14}\]Under assumption in distribution action can be learned from the dataset due to conservatism of offline RL (\(Q^{*}(s,\hat{\pi}(s))-Q^{*}(s,\pi_{u}(s)))\approx 0\), \(a\geq 0\)

\[b=\int_{s\in O}\rho_{\pi_{u}}(s)(V^{*}(s)-Q^{*}(s,\pi_{u}(s)))ds- \int_{s\in O}(\rho_{\pi_{u}}+\Delta_{o})(s)(V^{*}(s)-Q^{*}(s,\hat{\pi}(s)))ds \tag{15}\] \[\qquad\geq\int_{s\in O}\rho_{\pi_{u}}(s)(V^{*}(s)-Q^{*}(s,\pi_{u}( s)))ds-\int_{s\in O}\rho_{\pi}(s)(V^{*}(s)-Q^{*}(s,\hat{\pi}(s)))ds\] (16) \[\qquad\geq\mathbb{E}_{s\sim O|\pi_{u}}[V^{*}(s)-Q^{*}(s,\pi_{u}(s ))]-\mathbb{E}_{s\sim O|\hat{\pi}}[V^{*}(s)-Q^{*}(s,\hat{\pi}(s))] \tag{17}\]

Further loosening the lower bound

\[=\mathbb{E}_{s\sim O|\pi_{u}}[V^{*}(s)-Q^{*}(s,\pi_{u}(s))]-\bar{\rho}_{\hat{ \pi}}\int_{s\in O}\frac{\rho_{\hat{\pi}}}{\bar{\rho}_{\hat{\pi}}}(V^{*}(s)-Q^ {*}(s,\hat{\pi}(s)))ds \tag{18}\]

\[\geq\mathbb{E}_{s\sim O|\pi_{u}}[V^{*}(s)-Q^{*}(s,\pi_{u}(s))]-\bar{\rho}_{ \hat{\pi}}\int_{s\in O}(V^{*}(s)-Q^{*}(s,\hat{\pi}(s)))ds \tag{19}\]

Combining Eq 14, 17 and 19, and denoting \(\alpha=\mathbb{E}_{s\sim O}[V^{*}(s)-Q^{*}(s,\hat{\pi}(s))]\)

\[\eta(\hat{\pi})-\eta(\pi_{u})\geq\mathbb{E}_{s\sim O|\pi_{u}}[V^{*}(s)-Q^{*}(s,\pi_{u}(s))]-\bar{\rho}_{\hat{\pi}}\alpha \tag{20}\]

Hence, Proposition 4.2 follows **Q.E.D**

**Proposition A.3**.: _(4.3) Algo 1 reduces generalization error if \(Q^{*}(s,\pi_{t}^{\omega}(s))>Q^{*}(s,\pi(s))\) for \(s\in\mathcal{D}\cap\mathcal{B}_{r}\), where \(\pi\) is vanilla offline RL policy learnt on \(\mathcal{B}_{r}\)._

Proof.: Generalization error for any policy \(\pi\) as defined by [28] can be written as:

\[G_{\pi}=V^{*}(s_{0})-V_{\pi}(s_{0})=-\mathbb{E}_{\tau\sim\pi}[\sum_{t=0}^{T} \gamma^{t}Q^{*}(s_{t},\pi(s_{t}))-V^{*}(s_{t})] \tag{21}\]

Here, \(\mathbb{E}_{\tau\sim\pi}\) represents sampling trajectories with policy \(\pi\). Since the state space is continuous, we can represent the expectation as an integral over the state space

\[=-\sum_{t=0}^{T}\gamma^{t}\int_{s\in S}P(s_{t}=s|\pi)(Q^{*}(s_{t}, \pi(s_{t}))-V^{*}(s_{t}))ds \tag{22}\] \[=-\int_{s\in S}\sum_{t=0}^{T}\gamma^{t}P(s_{t}=s|\pi)(Q^{*}(s_{t},\pi(s_{t}))-V^{*}(s_{t}))ds \tag{23}\]

Analysing with respect to \(s\in\mathcal{D}\cap\mathcal{B}_{r}\) we can break the integration into two parts

\[=-\left[\int_{s\in S/D}\sum_{t=0}^{T}\gamma^{t}P(s_{t}=s|\pi)(Q^{*}(s_{t},\pi( s_{t}))-V^{*}(s_{t}))ds+\int_{s\in D}\sum_{t=0}^{T}\gamma^{t}P(s_{t}=s|\pi)(Q^{* }(s_{t},\pi(s_{t}))-V^{*}(s_{t}))\right] \tag{24}\]

\[=-\left[f(s|\pi)+\int_{s\in D}\sum_{t=0}^{T}\gamma^{t}P(s_{t}=s|\pi)(Q^{*}(s_{ t},\pi(s_{t}))-V^{*}(s_{t}))\right] \tag{25}\]

For a policy \(\hat{\pi}\) learnt in Algo 1 the action for \(s_{t}=s\in\mathcal{D}\) is regularized to be close to \(\pi_{t}^{\omega}\) which either follows domain knowledge or expert demonstrations. Hence, it is reasonable to assume \(Q^{*}(s_{t},\pi_{t}^{\omega}(s_{t}))>Q^{*}(s_{t},\pi(s_{t}))\).

It follows

\[\int_{s\in D}\sum_{t=0}^{T}\gamma^{t}P(s_{t}=s|\hat{\pi})(Q^{*}(s_{t},\hat{\pi} (s_{t}))-V^{*}(s_{t}))<\int_{s\in D}\sum_{t=0}^{T}\gamma^{t}P(s_{t}=s|\pi)(Q^{* }(s_{t},\pi(s_{t}))-V^{*}(s_{t})) \tag{26}\]

**Note for \(s\notin\mathcal{D}\), \(f(s|\hat{\pi})\approx f(s|\pi)\).** This is because the regularization term assigns max Q value to a different action for \(s\in\mathcal{D}\) but \(max_{a}(Q(s,a))\) remains same

\[\therefore-\left[f(s|\hat{\pi})+\int_{s\in D}\sum_{t=0}^{T}\gamma^{t }P(s_{t}=s|\hat{\pi})(Q^{*}(s_{t},\hat{\pi}(s_{t}))-V^{*}(s_{t}))\right]\] \[<-\left[f(s|\pi)+\int_{s\in D}\sum_{t=0}^{T}\gamma^{t}P(s_{t}=s| \pi)(Q^{*}(s_{t},\pi(s_{t}))-V^{*}(s_{t}))\right] \tag{27}\]

Hence, \(G_{\#}<G_{\pi}\) Proposition 2 follows **Q.E.D**Missing Examples

_Performing \(Q-Learning\) by sampling from a reduced batch \(\mathcal{B}_{r}\) may not converge to an optimal policy for the MDP \(M_{\text{S}}\) representing the full buffer._

**Example** (Theorem 1,[13]) defines MDP \(M_{\text{S}}\) of \(\mathcal{B}\) from same state action space of the original MDP \(M\) with transition probabilities \(p_{\mathcal{B}}(s^{\prime}|s,a)=\frac{N(s,a,s^{\prime})}{\sum_{\tilde{s}}N(s, a,\tilde{s})}\) where \(N(s,a,s^{\prime})\) is the number of times \((s,a,s^{\prime})\) occurs in \(\mathcal{B}\) and an terminal state \(s_{init}\). It states \(p_{\mathcal{B}}(s_{init}|s,a)=1\) when \(\sum_{\tilde{s}}N(s,a,\tilde{s})=0\). This happens when transitions of some \(s^{\prime}\) of \((s,a,s^{\prime})\) are missing from the buffer, which may occur in \(\mathcal{B}_{r}\) when \(\mathcal{B}_{r}\subset\mathcal{B}\). \(r(s_{init},s,a)\) is initialized to \(Q(s,a)\). We assume that a policy learned on reduced dataset \(\mathcal{B}_{r}\) converges to optimal value function and disprove it using the following counterexample:

A visualization is shown in Fig 8.

## Appendix C Algorithm

The pseudo code of the algorithm is described in Algo 1. We take a simple MDP illustrated in Fig 7 with 3 states and 2 actions (0,1). The reward of each action is marked along the transition. The sampled MDP is constructed the following samples (1,0,2)-2,(1,1,2)-3, (2,0,3)-3, and (2,1,3)-2 and the reduced buffer MDP with samples (1,0,2)-2 and (1,1,2)-1. The probabilities are marked along the transition. It is easy to see that the policy learned under the reduced MDP converges to a nonoptimal policy after one step of the Q table update with \(Q(s,a)=r(s,a)+p(s^{\prime}|s,a)*max_{a^{\prime}}(Q(s^{\prime},a^{\prime}))\). This happens because of transition probability shift on reducing samples \(p_{\mathcal{B}}(s^{\prime}|s,a)\neq p_{\mathcal{B}_{r}}(s^{\prime}|s,a)\) and no Q updates for \((s,a)\notin\mathcal{B}_{r}\).

Our methodology addresses these issues as follows:

* For \(s\in D\cap\mathcal{B}_{r}\) better actions are enforced through regularization using \(\pi_{*}^{\omega}\) even when the transition probabilities are low for optimal transitions.
* Incorporating regularization distills the teacher's knowledge in the critic-enhancing generalization.

Figure 8: We hypothesize the suboptimal performance of offline RL for limited data can be addressed via domain knowledge via action regularization and knowledge distillation.

Figure 7: Example MDP, sampled buffer MDP and reduced buffer with Q tables

```
1:Input: Reduced buffer \(\mathcal{B}_{r}\), Initial teacher network \(\pi_{t}^{\omega}\), Training steps \(N\), Warm-up steps \(k\), Soft update \(\tau\), hyperparameters: \(\lambda,\alpha\)
2:Initialize Critic with MC dropout and Target Critic \(Q_{s}^{\theta},Q_{s}^{\theta^{\prime}}\)
3:for\(n\gets 1\)to\(N\)do
4: Sample mini-batch \(b\) of transitions \((s,a,r,s^{\prime})\sim\mathcal{B}_{r}\)\(a_{t}=[],a_{s}=[],s_{r}=[]\)
5:for\(s\in b\)do
6:if\(s\models\mathcal{D}\) and \(\pi_{t}^{\omega}(s)\neq argmax_{a}(Q_{s}^{\theta}(s,a))\)then
7:\(a_{t}.append(\pi_{t}^{\omega}(s))\)
8:\(a_{s}.append(\arg\max_{a}(Q_{s}^{\theta}(s,a)))\)
9:\(s_{r}.append(s)\)
10:endif
11:endfor
12:if\(n>k\wedge\text{Cond. 6}\)then
13: Update \(\pi_{t}^{\omega}(s)\) using Eq 7
14:\(\mathcal{L}_{r}(\theta)=0\)
15:else
16: Calculate \(\mathcal{L}_{r}(\theta)\) using Eq 3
17:endif
18: Calculate \(\mathcal{L}(\theta)\) using Eq 4
19: Update \(Q_{s}^{\theta}\) with \(\mathcal{L}(\theta)\) and softy update \(Q_{s}^{\theta^{\prime}}\) and \(\tau\)
20:endfor
```

**Algorithm 1** Pseudo code for EXID

[MISSING_PAGE_FAIL:18]

[18; 45]. These techniques have shown to reduce overfitting and enhance generalization [38]. Knowledge distillation is also prevalent in the field of RL [47] and offline RL [39]. Contrary to prevalent teacher-student knowledge distillation techniques, our work does not enforce parameter sharing among the networks. Through experiments, we demonstrate that a simple regularization loss and expected performance-based updates can improve generalization to unobserved states covered by domain knowledge. There are also no constraints on keeping the same network structure for the teacher, paving ways for capturing the domain knowledge into more structured networks such as Differentiable Decision Trees (DDTs).

## Appendix F Network Architecture and Hyper-parameters

We follow the network architecture and hyper-parameters proposed by [31] for all our networks, including the baseline networks. The teacher BC network \(\pi_{\omega}^{t}\) and Critic network \(Q_{s}^{g}(s,a)\) consists of 3 linear layers, each having a hidden size of 256 neurons. The number of input and output neurons depends on the environment's state and action size. All layers except the last are SELU activation functions; the final layer uses linear activation. \(\pi_{\omega}^{t}\) uses a softmax activation function in the last layer for producing action probabilities. A learning rate of 0.0001 with batch size 32 and \(\alpha=0.1\) is used for all environments. MC dropout probability of 0.5 and number of stochastic passes T=10 have been used for the critic network. The uncertainty check is performed every 15 episodes after the warm start to avoid computational overhead. The hyper-parameters specific to our algorithm for OpenAI gym are reported in Table F. The hyper-parameters specific to our algorithm for Minigrid environments are reported in Table 5.

\begin{table}
\begin{tabular}{l|l l|l l|l l|l l} \hline \hline Hyperparam & \multicolumn{3}{c|}{MountainCar} & \multicolumn{3}{c|}{CartPole} & \multicolumn{3}{c|}{Lunar-} \\  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\  & Expert & Replay & Noisy & Expert & Replay & Noisy & Expert & Replay & Noisy \\ \(\lambda\) & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\ \(k\) & 30 & 30 & 30 & 30 & 30 & 30 & 30 & 30 & 30 \\ \(\pi_{\omega}^{t}\) lr & \(1e^{5}\) & \(1e^{5}\) & \(1e^{5}\) & \(1e^{2}\) & \(1e^{2}\)

## Appendix G Effect of \(k\) and \(\lambda\) and Evaluation Plots

We empirically evaluate the effect of \(\lambda\) In Fig 11 and \(k\) in Fig 12. We believe these parameters depend on the quality of \(\mathcal{D}\). For the given \(\mathcal{D}\) in the environments we empirically observe, \(\lambda=0.5\) generally performs well, except for Minigrid environments where \(\lambda=0.1\) works better. Increasing the warm start parameter \(k\) generally increases the initial performance of the policy, allowing it to learn from the teacher. Meanwhile, no warm start adversely affects policy performance as the critic may erroneously update the teacher. From empirical evaluation, we observe that \(k=30\) gives a reasonable start to the policy. All the evaluation plots are shown in Fig 13, where it can be observed that ExID performs better than baseline CQL.

## Appendix H Data reduction design and data distribution visualization of reduced dataset

In this section, we discuss the intuition behind our data-limiting choices. We also visually represent selected reduced datasets for the OpenAI gym environments.

Figure 11: Effect of \(\lambda\) on the performance of ExID for different environments expert datasets.

Figure 12: Effect of \(k\) on the performance of ExID for different environments expert datasets.

[MISSING_PAGE_EMPTY:21]

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Position\(>\)-0.5 & Position\(<\)-0.5 & Velocity\(>\)0.01 & Velocity\(<\)0.01 \\ \hline -121.89 \(\pm\) 7.69 & -151 \(\pm\) 13.6 & -128.48 \(\pm\) 11.84 & -147.80 \(\pm\) 5.01 \\ \hline \end{tabular}
\end{table}
Table 6: Performance of ExID on removing different parts of the data based on nodes of Fig 10 (c) from Mountain Car expert dataset

Figure 16: Data distribution of reduced cart pole expert dataset compared to the full dataset noisy data

Figure 15: Data distribution of reduced dataset compared to the full dataset for mountain replay and noisy dataFigure 17: Data distribution of reduced LunarLander expert dataset compared to the full dataset

NeurIPS Paper Checklist

1. **Claims**

Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?

Answer: [Yes]

Justification: The claims made in the paper have been experimented on different settings for validity and generalization. Please refer to see 5.2.

Guidelines:

* The answer NA means that the abstract and introduction do not include the claims made in the paper.
* The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
* The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
* It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations**

Question: Does the paper discuss the limitations of the work performed by the authors?

Answer: [Yes]

Justification: The paper acknowledges the dependency on reasonable domain knowledge and coverage please refer to sec 6

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**

Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?

Answer: [Yes]

Justification: Please refer to App A in the supplement material for theoretical analysis and proofs.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.

* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Yes all hyper-parameters and experimental setting have been clearly listed in the paper. Please refer to App F and sec 5.1.
4. Guidelines: * The answer NA means that the paper does not include experiments.
5. If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
6. If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
7. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
8. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
9. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 10. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
11. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is provided with the submission in a zip file with Readme for instructions. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.

* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper uses open source code to create the dataset and lists the modifications in details in the main paper and supplement material. Please refer to App F and sec 5.1. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All experiments have been run on 3 random seeds and the error bounds have been reported in Table 5.1, Table 2 and Table 3. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please refer to the Experimental setup section in the main paper sec 5.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ** The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
* **Code Of Ethics**
* Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: The authors have reviewed the code of ethics and the paper adheres to it. Guidelines:
* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
* **Broader Impacts*
* Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Please refer to the section broader impacts 7. Guidelines:
* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

* Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The algorithm proposed in this paper does not not pose any such risk of misuse. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All codes and datasets used in this paper are under MIT licence and the original owners have been cited.
13. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
14. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets have been introduced in this paper. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
15. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper did not require any crowdsourcing or human subject for experimentation. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
16. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA]
* Justification: The paper did not require any crowdsourcing or human subject for experimentation. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.