# Grammar Prompting for Domain-Specific Language Generation with Large Language Models

Bailin Wang\({}^{\diamond}\) Zi Wang\({}^{\dagger}\) Xuezhi Wang\({}^{\dagger}\) Yuan Cao\({}^{\ddagger}\) Rif A. Saurous\({}^{\dagger}\) Yoon Kim\({}^{\diamond}\)

\({}^{\diamond}\)Massachusetts Institute of Technology \({}^{\dagger}\)Google DeepMind \({}^{\ddagger}\)Google Research

{bailinw, yoonkim}@mit.edu, {wangzi, xuezhiw, yuancao, rif}@google.com

###### Abstract

Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We propose _grammar prompting_, a simple approach to enable LLMs to use external knowledge and domain-specific constraints, expressed through a grammar in Backus-Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and SMILES-based molecule generation.

## 1 Introduction

Prompting large language models (LLMs) with demonstrations optionally combined with natural language instructions has been shown to be an effective approach for surfacing their myriad capabilities acquired through pretraining [10]. This approach is however inadequate for applications where the task specifications cannot be fully delineated through just a handful of exemplars, for example in semantic parsing where an LLM must translate a natural language utterance to an executable program in a domain-specific language (DSL). DSLs often incorporate domain-specific abstractions and semantics that are difficult to characterize via just a few demonstrations. And unlike general-purpose programming languages, DSLs are by definition specialized and thus unlikely to have been encountered often enough (or at all) during pretraining for the LLM to acquire its full syntax.

How can we draw on the few-shot learning capabilities of LLMs to generate structured strings that are substantially different from those seen during pretraining? This work explores _grammar prompting_ as a simple approach for data-efficient generation of structured languages where an output string in the language can be derived through a series of symbolic manipulations. We exploit the fact that constraints over a structured output space can often be succinctly described by a context-free grammar in Backus-Naur Form (BNF), which is commonly used to define the syntax of a language. Grammar prompting augments each in-context example \((\mathbf{x},\mathbf{y})\) with a _specialized_ BNF grammar \(G[\mathbf{y}]\) that is minimally sufficient for generating \(\mathbf{y}\). Given a new input, the LLM first predicts the specialized BNF grammar and then generates the answer conditioned on the grammar.

Grammar prompting follows the recent line of work which enhances the few-shot reasoning capabilities of LLMs by interleaving intermediate "reasoning" steps between each in-context input andoutput [51; 24; 86; 80; 73]. The key difference in our approach is that the intermediate variable is in the form of a formal grammar rather than in natural language, which focuses on eliciting the symbolic manipulation capabilities of LLMs. The use of a formal grammar moreover makes it possible to impose constraints during incremental decoding such that syntactic validity is guaranteed. Finally, unlike chain-of-thought-style prompts [86] which typically require manual verbalization of the intermediate reasoning steps, in our approach the specialized grammar \(G[\mathbf{y}]\) can be derived automatically by parsing the output \(\mathbf{y}\) with the full (unspecialized) DSL grammar.

To summarize,

* [noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*]
* We propose grammar prompting as a simple approach for enabling LLMs to generate highly structured languages from just a few exemplars.
* We design a constrained LLM decoding algorithm tailored to grammar prompting, which guarantees syntactic validity while minimizing the number of LLM API calls.
* We apply grammar prompting to various domain specific languages for semantic parsing (SMCalFlow, Overnight, GeoQuery), AI planning (PDDL), and molecule generation (SMILES), and find that it can meaningfully improve upon standard prompting baselines in the few-shot setting.

## 2 Background

In this section, we define our problem and review the few-shot learning method that we build on.

### Problem Formulation: Domain-Specific Language Generation

Let \(\Sigma^{*}\) be the set of all finite strings over an alphabet \(\Sigma\), and further let \(D\subseteq\Sigma^{*}\) be a domain-specific language (DSL) for an application of interest. Given an input \(\mathbf{x}\) (e.g., a natural language command) we are interested in generating \(\mathbf{y}\in D\) (e.g., a program in a DSL fulfilling the command), as shown by the following calendar assistant example from SMCalFlow [6]:

\[\mathbf{x}:\text{Add meeting with Jean's manager on Wednesday at 3PM}.\] \[\mathbf{y}:\text{CreateEvent}(\&\text{(start\_? WednesdayNumberPM(3))} \text{(attendee\_? FindManager(Jean))})\]

DSLs are crafted by experts who use their domain-specific knowledge to incorporate higher-level abstractions than are typically found in general-purpose programming languages. We assume access to an expert-defined grammar \(G\) that fully specifies the DSL's syntax. As is the case with many DSLs, we further assume that \(G\) is a context-free grammar in Backus-Naur Form (BNF). See Figure 1 for a simple example adapted from SMCalFlow [6]. Letting \(L(G)\) be the language generated by \(G\), we have \(D\subseteq L(G)\subseteq\Sigma^{*}\) (not all syntactically valid programs are semantically valid).

### Few-shot Learning with Large Language Models

In-context learning with large language models (LLMs) has been shown to be an effective approach for few-shot learning [10]. Under this approach, a pretrained LLM is conditioned on \(N\) demonstration examples \((\mathbf{x}^{(i)},\mathbf{y}^{(i)})_{i=1}^{N}\) followed by a test example \(\mathbf{x}\), and the output is given by decoding from the prompted LLM, i.e., \(P_{\text{LLM}}(\mathbf{y}\mid\mathbf{x},(\mathbf{x}^{(i)},\mathbf{y}^{(i)})_{i=1}^{N})\). The demonstration examples can be optionally preceded by natural language instructions to further improve performance or even enable zero-shot learning [85; 62]. Recent work has additionally shown that interleaving natural language verbalizations of intermediate reasoning steps between each \(\mathbf{x}^{(i)}\) and \(\mathbf{y}^{(i)}\) can greatly improve few-shot performance on complex reasoning tasks [51; 86; 80; 73; 16].

The effectiveness of few-shot in-context learning depends both on how useful the implicit knowledge acquired through pretraining is for the task, and on how effectively the task specifications can be conveyed through the demonstrations. For DSL, the structured nature of combinatorial output space (i.e., the DSL grammar \(G\)) cannot be adequately captured through just a handful of demonstrations. Thus, few-shot generation of strings of a DSL remains challenging for LLMs.

Figure 1: A simple BNF grammar for a calendar DSL.

## 3 Grammar Prompting

Grammar prompting exploits the fact that while the actual strings of a DSL may not have been encountered frequently enough (or at all) during pretraining for the LLM to implicitly acquire its syntax, the LLM will likely have encountered many instances of _metalanguages_ (languages used to describe other languages). BNF grammars are a standard metalanguage for specifying a language's syntax, and are expected to occur in the LLM training corpus with some frequency (e.g., in computer science textbooks). We thus focus on using BNF grammars for few-shot DSL generation.

Let \(G=\bigcup_{j=1}^{M}\{r_{j}\}\) be an extended BNF grammar where each rule \(r_{j}\) is of the form

<symbol> ::= <expr1> | <expr2> |...

Here <symbol> is a nonterminal symbol and each <expr1> is a sequence of nonterminal and terminal symbols.1 A straightforward approach for incorporating a BNF grammar during in-context learning is to simply prepend the string representation of the full grammar \(G\) to the demonstration examples, along with an instruction to use the grammar. However in preliminary experiments, we found that this did not yield any improvements.2

Footnote 1: For brevity we forgo the formal tuple-based definition of \(G\) and instead define \(G\) to be equivalent to its context-free rules. We also freely go back and forth between this set definition of \(G\) and its string representation.

Footnote 2: However, when combined with specialized grammars we did observe small improvements by appending the full DSL grammar to the instructions. Hence, for all experiments where \(G\) is small enough (GeoQuery, Overnight-B, SMILES), we include \(G\) as part of the instruction. See Figure 2.

### Specialized Grammars

We propose to use _specialized grammars_ to enable better use of domain-specific knowledge and constraints. A specialized grammar \(G^{\prime}\subseteq G\) is a grammar obtained from taking a subset of the rules of the full grammar \(G\). We further define \(G[\mathbf{y}]\), a _minimal specialized grammar_ of \(\mathbf{y}\), to be a BNF

Figure 2: Example of grammar prompting for a calendar DSL. We interleave the minimal specialized grammar \(G[\mathbf{y}^{(i)}]\) between the demonstrations \(\mathbf{x}^{(i)}\) and \(\mathbf{y}^{(i)}\). During decoding, the LLM first predicts the specialized grammar \(\widehat{G}\), and then predicts the program \(\widehat{\mathbf{y}}\) conditioned on \(\widehat{G}\). The blue portion is not part of the actual prompt and only shown for illustrative purposes.

grammar with the following properties: (1) \(\mathbf{y}\in L(G[\mathbf{y}])\), and (2) \(\forall r\in G[\mathbf{y}],\ \mathbf{y}\not\in L(G[\mathbf{y}]\setminus\{r\})\).3 We can readily obtain a minimal specialized grammar by using \(G\) to parse \(\mathbf{y}\) and then taking the union of rules that were used in the derivation of \(\mathbf{y}\).

Footnote 3: Note that \(\mathbf{y}\) may have more than one minimal specialized grammar due to the potential instantiation of extended BNF rules. For instance, the expression “(attendee_?” attendee+ ")” depicted in Figure 1 implicitly defines all occurrences of attendee greater than 1. If this expression is incorporated into a program, either the concrete rule ”(attendee_?” attendee –)” or the original rule could be included in the minimal specialized grammar. In most applications we consider the rules of the minimal specialized grammar will be concrete, and thus there will only be one parse associated with \(\mathbf{y}\). See appendix A.1 for further details.

Grammar prompting feeds a sequence of \((\mathbf{x}^{(i)},G[\mathbf{y}^{(i)}],\mathbf{y}^{(i)})_{i=1}^{N}\) along with \(\mathbf{x}\) as a prompt to an LLM. For inference we first obtain the specialized grammar with an (approximate) \(\arg\max\) decoding

\[\widehat{G}=\operatorname*{arg\,max}_{G^{\prime}\subseteq G}\ P_{\text{LLM}} (G^{\prime}\,|\,\mathbf{x},(\mathbf{x}^{(i)},G[\mathbf{y}^{(i)}],\mathbf{y}^{(i)})_{i=1}^{N}).\]

We then obtain the program conditioned on \(\widehat{G}\),

\[\widehat{\mathbf{y}}=\operatorname*{arg\,max}_{\mathbf{y}\in L(\widehat{G})}\ P_{ \text{LLM}}(\mathbf{y}\,|\,\widehat{G},\mathbf{x},(\mathbf{x}^{(i)},G[\mathbf{y}^{(i)}],\mathbf{y}^ {(i)})_{i=1}^{N}).\]

We discuss how to perform constrained decoding with \(\widehat{G}\subseteq G\) and \(\widehat{\mathbf{y}}\in L(\widehat{G})\) in the next section. Grammar prompting views DSL program generation as a _grammar specialization_ process where given a natural language specification \(\mathbf{x}\), a set of production rules, \(\widehat{G}\), is selected from \(G\), and then a program \(\widehat{\mathbf{y}}\) is deduced according to the selected rules. Grammar prompting can also be viewed as an instance of chain-of-thought prompting [51, 86] where the intermediate thought is in the form of a formal grammar. However, unlike typical chain-of-thought prompting where the answer is (usually) deterministic given the intermediate reasoning steps, in our case there is still some uncertainty with respect to \(\widehat{\mathbf{y}}\) given \(\widehat{G}\) (e.g., \(L(\widehat{G})\) could still be infinite).

### Constrained Decoding

The use of a formal grammar as an intermediate variable makes it possible to enforce grammatical constraints during autoregressive LLM decoding. We first discuss how we enforce the constraint \(\mathbf{y}\in L(\widehat{G})\). One approach to constrained decoding is to use \(\widehat{G}\) to obtain a left-to-right Earley parser [18] and only decode from valid continuations at each decoding step. However this simple strategy poses several practical challenges when working with API-only LLMs. For one, a valid terminal continuation in \(\widehat{G}\) may consist of multiple BPE tokens. Moreover, while we can sample a valid continuation at each time step by disallowing invalid tokens,4 since the set of valid continuations changes at each time step, this strategy would require calling the LLM API at each time step with the full prompt and prefix along with the disallowed continuations, which is prohibitively expensive.5

Footnote 4: For example by using the logit_bias argument from OpenAI’s LLM API.

While there are many methods for grammar-constrained LM decoding [68, 64, 26], we present a simple strategy which speculatively decodes from the LLM to look ahead for multiple tokens. The pseudocode is shown in Algorithm 1. At each prediction step, we ask the LLM to speculatively decode the full program conditioned on the current prefix (lines 4-5). If the resulting continuation leads to a valid program, we return it (lines 6-7). Otherwise, we consult an Earley parser to extract the longest valid prefix from the current prediction (\(\mathbf{y}_{\text{prefix}}\)), along with a set of valid terminals that can follow the prefix (\(\Sigma[\mathbf{y}_{\text{prefix}}]\)). Finally, we rely on the LLM's probabilities to decide which terminal to use, with which a new partial program can be constructed (lines 10-11).6 Figure 3 illustrates one prediction step where the predicted program is corrected into a new valid partial program. Note that \(\mathbf{w}\) can consist of multiple BPE tokens, e.g., "FindManager(" in Figure 3. By scoring over multi-token terminals, the search procedure is implicitly augmented by looking ahead for a few tokens.

Footnote 6: In rare cases the set \(\Sigma[\mathbf{y}_{\text{prefix}}]\) was too large to feed to LLM APIs. In these cases we used Sentence-BERT [59] to compute the similarity between the encoding of \(\mathbf{y}_{\text{prefix}}\cdot\mathbf{w}\) and \(\hat{\mathbf{y}}^{(t)}\) and took the top 16 candidates as \(\Sigma[\mathbf{y}_{\text{prefix}}]\).

We use a similar procedure to operationalize the constraint \(G^{\prime}\subseteq G\), except that \(\operatorname{EarleyParse}\) (used at Algorithm 1, line 9) is constructed with a _metagrammar_ (i.e., the grammar of \(G\)) for grammar prediction. See appendix A.1 for more details. In our ablation study we find that while these constraints are helpful insofar as they guarantee syntactic validity, grammar prompting still meaningfully improves upon standard prompting with even with simple unconstrained decoding.

## 4 Experiments

We apply grammar prompting to diverse domains: DSLs for semantic parsing (SMCalFlow, Overnight, GeoQuery), an action DSL (PDDL planning), and a molecule generation DSL (SMILES). These experiments are not necessarily intended to improve upon the state-of-the-art on these benchmarks but rather intended to assess whether LLMs can improve upon standard prompting for few-shot DSL generation by learning to predict and use grammars during in-context learning.

### Semantic Parsing for Tool Usage

Software tools are typically accompanied by a collection of human-interpretable APIs which provide a platform for developers to interact programmatically with the tools. These APIs constitute a DSL, where each production rule of the grammar specifies the input and output types for a specific API call (see Figure 1 for an example). These tools demonstrate a broad spectrum in terms of DSL complexity, ranging from single-function tools such as Google(user_query), Translate(sentence, language) to more complex tools such as the entirety of Wolfram language.7 Enabling LLMs to use external tools via APIs is an important step towards enhancing their capabilities [63; 56; 53; 72; 47].

Footnote 7: [https://www.wolfram.com/language/](https://www.wolfram.com/language/)

We test our approach on standard semantic parsing benchmarks involving complex DSLs: SMCalFlow [6], which features human-generated utterances about calendar management (see Figure 2); GeoQuery [99] which features queries against a US Geography database; and Overnight-Blocks [81], which features queries about blocks in a synthetic block world. See appendix B for examples of input-output pairs along with the specialized grammars. The original benchmarks target the training of conventional semantic parsers and thus contain hundreds/thousands of training examples. Even prompting-based approaches on these benchmark rely on retrieval-based in-context learning which first retrieves \(m\) exemplars from a large training set of \(n\) examples (\(n\gg m\)) based on some similarity measure (e.g., BM-25), and then performs in-context learning with the retrieved exemplars [57; 95; 68; 46]. In contrast, we target the true few-shot setting where we only assume access to 16-32 demonstration examples.

Our baselines here include: (1) standard prompting, (2) standard prompting with constrained decoding based on the full DSL grammar \(G\)[68; 64], and (3) a derivation tree-based prompting baseline which imbues more structural information to the exemplars by feeding the linearized derivation

Figure 3: Illustration of how an predicted program is corrected in our proposed Earley-based constrained decoding. The final partial program will be subsequently fed into the LLM for continuation.

tree instead of the surface form program.8 We use Codex-davinci-002 [13] as the base LLM for these main experiments. Language models trained on code (such as Codex) have shown to be particularly effective on semantic parsing benchmarks [67]. We evaluate according to program accuracy (matching the predicted and reference programs) as well as execution accuracy (same execution in both programs) if possible.

Footnote 8: For example, the derivation tree of a subprogram (attendee_? FindManager(Jean)) is linearized to [constraint "(attendee_?" [attendee "FindManager(" [attendee "Jean" ")"], which uses square brackets to encode richer hierarchical information than just the surface form program.

Few-shot results.The main results are shown in Table 1. We find that grammar prompting can meaningfully improve upon the standard prompting baseline even without constrained decoding. Interestingly, grammar prompting outperforms derivation tree prompting which actually provides _more_ information than the minimal specialized grammar \(G[\mathbf{y}]\) (since the derivation tree explicitly shows how the rules are actually applied to obtain the program). This potentially indicates that having the LLM "plan out" the program by forcing it to predict the specialized grammar \(\widehat{G}\) first is an effective strategy. We also analyze the effect of constrained decoding on the number of LLM API calls in Table 7 of appendix A.1, where we observe that constrained decoding requires roughly three times more API calls than unconstrained decoding. However, despite the promising performance of grammar prompting, there is a large gap between using the predicted grammar versus using the oracle grammar (i.e., setting \(\widehat{G}=G[\mathbf{y}]\)), indicating opportunities for further work in this area.

Retrieval-based in-context learning.While our core target application is few-shot semantic parsing, we also apply grammar prompting for retrieval-based in-context learning to test whether it can still improve performance in the data-rich regime and also to compare against prior work on these benchmarks. Results in Table 2 (left) show that grammar prompting can improve results even in this setting, although the improvements are less pronounced than in the few-shot setting.

Out-of-distribution generalization.We experiment to see whether grammar prompting can improve compositional generalization on GeoQuery. Specifically, we test grammar prompting on the compositional splits of GeoQuery split from Shaw et al. [66]. These splits feature structural divergence between training and test examples, e.g., programs have different templates or length. Results in Table 2 (right) shows that grammar prompting can improve upon standard prompting, across all splits (Template, TMCD, Length).

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{**Retrieval-based ICL**} & \multicolumn{3}{c}{**GeoQuery Out-of-Distribution**} \\ Model & GeoQuery & SMCalFlow & Overnight-Blk & Template & TMCD & Length & NewFunc \\ (\#CL examples / retrieval set) & (32/560) & (16/128) & (32/1,436) & (32/441) & (32/440) & (32/440) & (32/453) \\ \hline Previous Work & 86.1\({}^{\spadesuit}\) & 60.7\({}^{\spadesuit}\) & 65.2\({}^{\heartsuit}\) & – & – & – & – \\ \hline Standard Prompting & 96.8 & 60.0 & 69.4 & 93.2 & 77.1 & 86.4 & 63.3 \\ Grammar Prompting & 97.9 & 62.8 & 70.2 & 95.7 & 86.6 & 88.6 & 90.8 \\ _w. oracle grammar_ & 98.6 & 88.9 & 97.2 & 97.9 & 95.0 & 95.7 & 96.2 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results on retrieval-based in-context learning (left) and compositional generalization (right) with Codex. GeoQuery and Overnight-Blk show execution accuracy while SMCalFlow shows program accuracy. The numbers with \({}^{\spadesuit}\), \({}^{\spadesuit}\) and \({}^{\heartsuit}\) are taken from Herzig and Berant [31], Ye et al. [95] and Cao et al. [11], respectively.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{2}{c}{**GeoQuery**} & \multicolumn{2}{c}{**SMCalFlow**} & \multicolumn{2}{c}{**Overnight-Blk**} \\ Approach & Prog. & Exec. & Prog. & Prog. & Exec. \\ \hline Standard Prompting (unconstrained decoding) & 60.7 & 81.5 & 46.4 & 29.3 & 54.7 \\ _w. constrained decoding (\(\widehat{\mathbf{y}}\in L(G)\))_ & 61.1 & 81.8 & 49.2 & 29.3 & 54.7 \\ Linearized Derivation Tree Prompting & 58.6 & 77.5 & 50.0 & 27.3 & 56.4 \\ \hline Grammar Prompting (unconstrained decoding) & 67.1 & 87.5 & 50.8 & 34.8 & 57.4 \\ _w. grammar constraint (\(\widehat{G}\subseteq G\))_ & 67.9 & 88.6 & 51.3 & 37.1 & 60.4 \\ _w. grammar and program constraint (\(\widehat{\mathbf{y}}\in L(\widehat{G})\))_ & 69.6 & 88.9 & 52.4 & 37.6 & 60.9 \\ _w. oracle grammar (\(\widehat{G}=G[\mathbf{y}]\))_ & 95.7 & 96.1 & 80.0 & 73.9 & 94.2 \\ _w. oracle grammar + program constraint_ & 95.7 & 96.8 & 83.6 & 74.4 & 96.5 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results on few-shot semantic parsing with Codex with various decoding strategies. GeoQuery and Overnight-Blk use 32 in-context examples, and SMCalFlow uses 16 examples. We show both program (Prog.) and execution (Exec.) accuracy when possible.

We next assess whether grammar prompting can enable LLMs to make zero-shot use of _unseen functions_ (NewFunc) that are not even part of the retrieval set. We set aside 8 functions (smallest, shortest, most, highest, sum, population_1, count, major) and remove them from the retrieval set, simulating a scenario where new functions are supported in the backend yet no NL-program paired data is available for adapting a semantic parser. Note that for GeoQuery (and Overnight-Blk), we always prepend the full DSL grammar (\(G\)--which includes the held-out functions--before the in-context exemplars. Table 2 (right-most column) shows that grammar-prompted LLMs achieve significantly better performance than standard prompting. Our results suggest that the explicit prediction of specialized grammars elicits understanding and reasoning at the grammar level, thereby enabling generalization to unseen functions. We also found that without constrained generation, LLMs were often able to guess functions that did not exist but were nonetheless sensible. An interesting direction is to explore whether LLMs can tackle DSL-open benchmarks such as LARC [1].

Different base LLMs.We finally experiment with grammar prompting across different base LLMs. Since GPT-3.5's 4K token limit is smaller than Codex's (8K) and GPT-4's (8K) limits, we use fewer exemplars in these experiments than before (24/8/16 exemplars for GeoQuery/SMCalFlow/Overnight-B respectively). Due to API cost, we limit our experiments to a smaller subset of 100 test examples instead of the full test set.

Table 3 shows that grammar prompting improves upon standard prompting in the majority of the settings. The exceptions are SMCalFlow with GPT-3.5 where both methods performed poorly, and GeoQuery with PaLM 2-L[7], where standard prompting already performed well.

### Class-Specific Molecule Generation

We next demonstrate an application of grammar prompting beyond language parsing problems with a molecule generation task. Existing methods for molecule generation typically focus on training specialized neural models using large training sets [45; 37; 15; 2; 79; 61; 19]. We instead follow Guo et al. [29] and explore a few-shot setting where the task is to generate class-specific molecules given a small number of exemplars of that class. Formally, given a small set of molecules \(\{\mathbf{y}^{(i)}_{c}\}_{i=1}^{N}\) belonging to a particular molecule class \(c\in\{\texttt{Acrylates},\ \texttt{Chain}\ \texttt{Extenders},\ \texttt{Isocyanates}\},\)our goal is to generate novel molecules \(\mathbf{y}_{c}\) of the same class that can be synthesized using existing molecules. Since the in-context examples in this case will only consist of molecules of the same class, the "input" \(\mathbf{x}^{(i)}_{c}\) is the empty string in this case. The data contains 32 Acrylates, 11 Chain Extenders, and 11 Isocyanates (see appendix G of Guo et al. [29]).

While molecules can be more faithfully represented with 3D graph structure, the SMILES string representation [87] remains popular due to its ease of use.9 The specialized grammars \(G[\mathbf{y}_{c}]\) (which are specialized from the SMILES grammar) encode various structural properties of the molecule that are specific to the molecule class. Figure 4 shows an example of a specialized grammar and the corresponding molecule in SMILES format. In this example, ring_closure ::= "1" specifies the number of rings, and branch ::= "(" smiles ")" specifies whether there is a branch.

Footnote 9: Note that SMILES does not guarantee that a generated string corresponds to a valid molecule. Using our approach on more advanced string representations such as SELFIES [44] (which guarantee validity) remains an interesting avenue for future work.

We test our approach by generating 100 molecules for each class and assessing the quality of the generated molecules. In addition to the standard prompting baseline, we also run the graph grammar baseline from Guo et al. [29] which learns a hypergraph grammar [38] from the given molecules. We use four metrics: _Validity (V)_, the percentage of chemically valid molecules; _Diversity (D)_, average pairwise Tanimoto distance over Morgan fingerprints [60]; _Retrosynthesis score (R)_, the percentage

\begin{table}
\begin{tabular}{l l c c c} \hline \hline
**Base LM** & **Method** & **GeoQuery** & **SMCalFlow** & **Overnight-Blk** \\ \hline Codex & Standard & 83 & 27 & 63 \\  & Grammar & 95 & 35 & 66 \\ \hline GPT-3.5 & Standard & 75 & 9 & 49 \\  & Grammar & 86 & 5 & 67 \\ \hline GPT-4 & Standard & 85 & 32 & 56 \\  & Grammar & 98 & 36 & 62 \\ \hline PaLM 2-L & Standard & 90 & 14 & 59 \\  & Grammar & 87 & 17 & 62 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results with different base LLMs on a subset of 100 examples sampled from the original test set. GeoQuery and Overnight-Blk show execution accuracy, while SMCalFlow shows program accuracy.

[MISSING_PAGE_FAIL:8]

domains in PDDL planning, including Blocks, Depot and Satellite. For the action space, we use either a set of primitive actions (Prim) or an augmented set with macro actions (Macro). In addition to standard prompting, we add two more baselines: (1) No LLM: planning with the entire set of actions; (2) Min Macro: where we construct a minimal set of macro actions for each domain by selecting actions from existing plans for the training tasks. The Min Macro baseline is a domain-specific method to reduce the action space. By comparing to Min Macro, we can verify the effectiveness of instance-specific v.s. domain-specific action selection. See appendix A.3 for more details.

Results.We evaluate the efficiency of planning in terms of the number of search nodes created/expanded, as well as the success rate. Table 5 shows the promising performance of LLM-guided planning via grammar prompting. In Blocks, grammar prompting significantly improves efficiency while maintaining 100% success rate. In Depot, grammar prompting with macro actions improves the success rate by 20% over the best competing baseline. In Satellite, using primitive actions yields the best performance with 100% success rate and a reduction of 57% expanded nodes comparing to the No LLM baseline. While our experiments are not intended to complete with the state-of-the-art algorithms for fast planning [20, 21, 22, 32, 25, 84], they indicate the promise of LLMs for improving existing planning algorithms.

## 5 Discussion and Limitations

We discuss several limitations of our approach including some negative results. Grammar prompting did not yield any improvements for DSLs that were likely to have been frequently encountered during pretraining (e.g., regular expressions, SQL). Moreover, constrained generation based on specialized grammars led to increased API calls, and was not always beneficial for tasks beyond semantic parsing. For instance, in molecule generation we discovered that enforcing constraints can sometimes result in lower diversity. Additionally, in PDDL planning we observed that the constraints applied to prune objects can sometimes negatively impact performance, suggesting that relevant object selection is still very challenging for LLMs. It may be interesting to explore whether finetuning of moderately-sized LLMs using specialized grammars can lead to better grammar-based models for DSL generation.

On the positive front, our work demonstrates that LLMs have the capacity to understand and generate metalanguages. Working in this "metalanguage space" can be combined with chain-of-thought-style [86] prompts by, for example, manually providing natural language comments to the rules of the specialized grammars. We found this to improve results slightly on semantic parsing (see Figure 6

Figure 5: Example of a specialized grammar for PDDL planning in the Blocks domain. Given an input \(\mathbf{x}=(\mathbf{s}_{0},\mathbf{s}_{g})\), the specialized grammar \(G[\mathbf{y}]\) only includes necessary actions for solving this task.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{**Blocks**} & \multicolumn{3}{c}{**Depot**} & \multicolumn{3}{c}{**Satellite**} \\ Approach & Created & Expanded & Success & Created & Expanded & Success & Created & Expanded & Success \\ \hline GBFS + Prim. (No LLM) & 360 & 188 & 1.0 & 18047 & 3870 & 0.4 & 8205 & 150 & 1.0 \\ \hline Standard + Prim. & 348 & 180 & 1.0 & 17597 & 4039 & 0.4 & 6686 & 78 & 1.0 \\ Grammar + Prim. & 251 & 124 & 1.0 & 15033 & 3641 & 0.4 & 5162 & 64 & 1.0 \\ \hline Standard + Macro. & 850 & 16 & 1.0 & 1460 & 56 & 0.4 & 4003 & 27 & 0.3 \\ Grammar + Macro. & 170 & 9 & 1.0 & 2917 & 127 & 0.8 & 3665 & 46 & 0.9 \\ \hline Standard + Min Macro. & 228 & 8 & 1.0 & 1903 & 65 & 0.6 & 3483 & 35 & 0.8 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results on PDDL planning. Created/Expanded refer to the number of nodes during planning (lower is better). Success refers to success rate (higher is better). Numbers are averaged over three runs using GPT-3.5.

of appendix A.1). Moreover, many scientific problems can be formally approached by representing hypotheses as DSL programs [71], and DSLs can enable easier encoding of human prior knowledge and scientific principles, providing a foundation for scientific discovery. Recent work shows that state-of-the-art LLMs can follow previously unseen formal systems [75]. Techniques like grammar prompting can widen the scope of scientific problems for which LLMs could be effectively applied by more explicitly accounting for external knowledge and constraints.

## 6 Related Work

Chain-of-thought prompting.Grammar prompting extends a recent line of work on improving reasoning capabilities by requesting explicit reasoning steps as part of the prompt [51; 24; 86; 80; 14; 94]. Our approach is closely related to concurrent work on employing symbolic variables as part of the prompt [30; 50; 33; 97; 52], though we are not aware of any existing work that uses formal grammars as the intermediate reasoning step.

LLMs for program generation and semantic parsing.Generating programs from natural language specifications, a task often referred to as semantic parsing, is a sub-problem of program synthesis; for surveys, see Kamath and Das [39] and Gulwani et al. [28]. Recent works [8; 89] have explored using LLMs for generating code in general-purpose programming languages (e.g., Python). Our work further extends this line by examining whether LLMs can generate DSL programs, which are intrinsically scarce. There has also been work on using LLMs for tool usage via further training [63] or prompting [56; 77], investigating how model scales [57] and retrievers [96; 46] affect in-context learning for semantic parsing, and constrained decoding [64; 68; 55] for program generation.

Neural grammars.Grammar prompting can also been seen as a "fully LLM" instantiation of a line of work on neural parameterizations of symbolic grammars [35; 17; 43; 42; 36; 100; 92; 91; 93]. Indeed, our approach to semantic parsing essentially uses prompt-based learning to define a quasi-synchronous grammar [70; 78] whose rules dynamically depend on the source sentence. Concretely, in contrast to recent works which embed learnable neural components within synchronous grammars [41; 23; 76], grammar prompting relies on the implicit in-context learning capabilities of LLMs for the learning component. (However unlike these works, our conditional grammar does not explicitly align its rules to the subparts of the source sentence).

Grammar-based molecule generation.Grammar-based methods have gained significant interest in the realm of molecule generation, offering advantages in interpretability, data-efficiency, and controllability. One line of research involves integrating generic SMILES grammars with neural networks to generate syntactically correct molecules [45; 15]. Another approach centers on data-driven induction of grammars for generation [29; 38]. Our work aligns with the former, viewing grammar prompting as a straightforward method for integrating grammar into an LLM without the need for additional training.

LLMs for planning.Recently, LLMs have been increasingly studied in the context of planning for autonomous agents. When given goals expressed in natural language in household environments, earlier works [3; 65; 34; 48] directly prompted LLMs to predict executable actions. However, in PDDL domains, recent works [69; 74] showed that LLMs underperform classical planners if the desired action sequences are very long. Grammar prompting represents a promising strategy for augmenting classical planners with LLMs to get the best of both worlds. Other related efforts include translating between problems and PDDL models [49] and corrective re-prompting [58]. Besides using LLMs, integrating learning and planning has been extensively studied in the past literature, e.g., learning actions [4; 82], skills [84], macro-actions [9], rules [88] and guidance strategies [90; 83; 40] for more efficient planning.

## 7 Conclusion

We propose grammar prompting as a simple approach for improving few-shot DSL generation with large language models. Experiments across a range of structured languages including DSLs for semantic parsing (SMCalFlow, GeoQuery, Overnight), PDDL planning (action DSL), and molecule generation (SMILES), show that grammar prompting can improve upon standard prompting baselines. The encouraging results in semantic parsing indicate its potential to assist LLMs with tool usage, and the promising results in other domains indicate that grammar prompting can enable application of LLMs in domains that intrinsically depend on DSLs.

## Acknowledgments

We thank Jacob Andreas, Gabriel Grand, Linlu Qiu, Tom Silver, and Hunter Lang for helpful discussion and feedback. This study was supported by funds from the Google-MIT research collaborations program and the GIST-MIT joint research program.

## References

* Acquaviva et al. [2022] Sam Acquaviva, Yewen Pu, Marta Kryven, Theodoros Sechopoulos, Catherine Wong, Gabrielle Ecanow, Maxwell Nye, Michael Tessler, and Josh Tenenbaum. Communicating natural programs to humans and machines. _Proceedings of NeurIPS_, 35:3731-3743, 2022.
* Ahmad et al. [2022] Walid Ahmad, Elana Simon, Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta-2: Towards chemical foundation models. _arXiv preprint arXiv:2209.01712_, 2022.
* Ahn et al. [2022] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as I can, not as I say: Grounding language in robotic affordances. _arXiv preprint arXiv:2204.01691_, 2022.
* Aineto et al. [2018] Diego Aineto, Sergio Jimenez, and Eva Onaindia. Learning STRIPS action models with classical planning. In _Proceedings of the International Conference on Automated Planning and Scheduling_, volume 28, pages 399-407, 2018.
* Alkhazaraji et al. [2020] Yusra Alkhazaraji, Matthias Frorath, Markus Grutzner, Malte Helmert, Thomas Liebetraut, Robert Mattmuller, Manuela Ortlieb, Jendrik Seipp, Tobias Springenberg, Philip Stahl, and Jan Wulfing. Pyperplan (v1.3), 2020. URL [https://doi.org/10.5281/zenodo.3701399](https://doi.org/10.5281/zenodo.3701399).
* Andreas et al. [2020] Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan DeLoach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lanman, Percy Liang, Christopher H. Lin, Ilya Lintsbakh, Andy McGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy, Jesse Rusak, Beth Short, Div Slomin, Ben Snyder, Stephen Striplin, Yu Su, Zachary Tellman, Sam Thomson, Andrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, and Alexander Zotov. Task-oriented dialogue as dataflow synthesis. _Transactions of the Association for Computational Linguistics_, 8:556-571, 2020. doi: 10.1162/tacl_a_00333. URL [https://aclanthology.org/2020.tacl-1.36](https://aclanthology.org/2020.tacl-1.36).
* Anil et al. [2023] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.
* Austin et al. [2021] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. _arXiv preprint arXiv:2108.07732_, 2021.
* Botea et al. [2005] Adi Botea, Markus Enzenberger, Martin Muller, and Jonathan Schaeffer. Macro-ff: Improving AI planning with automatically learned macro-operators. _Journal of Artificial Intelligence Research_, 24:581-621, 2005.
* Brown et al. [2020] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In _Proceedings of NeurIPS_, 2020.
* Cao et al. [2019] Ruisheng Cao, Su Zhu, Chen Liu, Jieyu Li, and Kai Yu. Semantic parsing with dual learning. In _Proceedings of ACL_, pages 51-64, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1007. URL [https://aclanthology.org/P19-1007](https://aclanthology.org/P19-1007).
* Chen et al. [2020] Binghong Chen, Chengtao Li, Hanjun Dai, and Le Song. Retro*: Learning retrosynthetic planning with neural guided A* search. In _Proceedings of ICML_, pages 1608-1616. PMLR, 2020.

* [13] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Happert, Foios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021.
* [14] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. _arXiv preprint arXiv:2211.12588_, 2022.
* [15] Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, and Le Song. Syntax-directed variational autoencoder for structured data. _arXiv preprint arXiv:1802.08786_, 2018.
* [16] David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, and Charles Sutton. Language Model Cascades. _arXiv:2207.10342_, 2022.
* [17] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In _Proceedings of NAACL_, 2016.
* [18] Jay Earley. An efficient context-free parsing algorithm. _Communications of the ACM_, 13(2):94-102, 1970.
* [19] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, and Heng Ji. Translation between molecules and natural language. _arXiv preprint arXiv:2204.11817_, 2022.
* [20] Richard E Fikes and Nils J Nilsson. STRIPS: A new approach to the application of theorem proving to problem solving. _Artificial intelligence_, 2(3-4):189-208, 1971.
* [21] Maria Fox and Derek Long. PDDL2. 1: An extension to PDDL for expressing temporal planning domains. _Journal of Artificial Intelligence Research_, 20:61-124, 2003.
* [22] Maria Fox and Derek Long. Modelling mixed discrete-continuous domains for planning. _Journal of Artificial Intelligence Research_, 27:235-297, 2006.
* [23] Dan Friedman, Alexander Wettig, and Danqi Chen. Finding Dataset Shortcuts with Grammar Induction. In _Proceedings of EMNLP_, 2022.
* [24] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: Program-aided Language Models. _arXiv:2211.10435_, 2022.
* [25] Caelan R. Garrett, Tomas Lozano-Perez, and Leslie P. Kaelbling. PDDLStream: Integrating symbolic planners and blackbox samplers. In _International Conference on Automated Planning and Scheduling (ICAPS)_, 2020. URL [https://arxiv.org/abs/1802.08705](https://arxiv.org/abs/1802.08705).
* [26] Saibo Geng, Martin Josifosky, Maxime Peyrard, and Robert West. Flexible Grammar-Based Constrained Decoding for Language Models. _arXiv:2305.13971_, 2023.
* the planning domain definition language. _Technical Report CVC TR98003/DCS TR1165. New Haven, CT: Yale Center for Computational Vision and Control._, 1998.
* [28] Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. Program synthesis. _Foundations and Trends(r) in Programming Languages_, 4(1-2):1-119, 2017.
* [29] Minghao Guo, Veronika Thost, Beichen Li, Payel Das, Jie Chen, and Wojciech Matusik. Data-efficient graph grammar learning for molecular generation. In _Proceedings of ICLR_, 2022.

* He-Yueya et al. [2023] Joy He-Yueya, Gabriel Poesia, Rose E Wang, and Noah D Goodman. Solving math word problems by combining language models with symbolic solvers. _arXiv preprint arXiv:2304.09102_, 2023.
* Herzig and Berant [2021] Jonathan Herzig and Jonathan Berant. Span-based semantic parsing for compositional generalization. In _Proceedings of ACL_, pages 908-921, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.74. URL [https://aclanthology.org/2021.acl-long.74](https://aclanthology.org/2021.acl-long.74).
* Hoffmann [2003] Jorg Hoffmann. The metric-ff planning system: Translating"ignoring delete lists"to numeric state variables. _Journal of Artificial Intelligence Research_, 20:291-341, 2003.
* Hu et al. [2023] Hanxu Hu, Hongyuan Lu, Huajian Zhang, Wai Lam, and Yue Zhang. Chain-of-symbol prompting elicits planning in large langauge models. _arXiv preprint arXiv:2305.10276_, 2023.
* Huang et al. [2022] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In _Proceedings of ICML_, pages 9118-9147. PMLR, 2022.
* Jiang et al. [2016] Yong Jiang, Wenjuan Han, Kewei Tu, et al. Unsupervised neural dependency parsing. Association for Computational Linguistics (ACL), 2016.
* Jin et al. [2019] Lifeng Jin, Finale Doshi-Velez, Timothy Miller, Lane Schwartz, and William Schuler. Unsupervised learning of PCFGs with normalizing flow. In _Proceedings of ACL_, Florence, Italy, July 2019. Association for Computational Linguistics.
* Jin et al. [2018] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In _Proceedings of ICML_, pages 2323-2332. PMLR, 2018.
* Kajino [2019] Hiroshi Kajino. Molecular hypergraph grammar with its application to molecular optimization. In _Proceedings of ICML_, pages 3183-3191. PMLR, 2019.
* Kamath and Das [2018] Aishwarya Kamath and Rajarshi Das. A survey on semantic parsing. _arXiv preprint arXiv:1812.00978_, 2018.
* Kim et al. [2019] Beomjoon Kim, Zi Wang, Leslie Pack Kaelbling, and Tomas Lozano-Perez. Learning to guide task and motion planning using score-space representation. _The International Journal of Robotics Research_, 38(7):793-812, 2019.
* Kim [2021] Yoon Kim. Sequence-to-sequence learning with latent neural grammars. In _Proceedings of NeurIPS_, pages 26302-26317, 2021.
* Kim et al. [2019] Yoon Kim, Chris Dyer, and Alexander Rush. Compound probabilistic context-free grammars for grammar induction. In _Proceedings of ACL_, Florence, Italy, July 2019. Association for Computational Linguistics.
* Kim et al. [2019] Yoon Kim, Alexander Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, and Gabor Melis. Unsupervised recurrent neural network grammars. In _Proceedings of NAACL_, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
* Krenn et al. [2020] Mario Krenn, Florian Hase, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (selfies): A 100% robust molecular string representation. _Machine Learning: Science and Technology_, 1(4):045024, oct 2020.
* Kusner et al. [2017] Matt J Kusner, Brooks Paige, and Jose Miguel Hernandez-Lobato. Grammar variational autoencoder. In _Proceedings of ICML_, pages 1945-1954. PMLR, 2017.
* Levy et al. [2022] Itay Levy, Ben Bogin, and Jonathan Berant. Diverse demonstrations improve in-context compositional generalization. _arXiv preprint arXiv:2212.06800_, 2022.
* Liang et al. [2023] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs. _arXiv:2303.16434_, 2023.

* [48] Bill Yuchen Lin, Chengsong Huang, Qian Liu, Wenda Gu, Sam Sommerer, and Xiang Ren. On grounded planning for embodied tasks with language models. _arXiv preprint arXiv:2209.00465_, 2022.
* [49] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with optimal planning proficiency. _arXiv preprint arXiv:2304.11477_, 2023.
* [50] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. Faithful chain-of-thought reasoning. _arXiv preprint arXiv:2301.13379_, 2023.
* [51] Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Witold Michalewski, Jacob Austin, David Bieber, David Martin Dohan, Aitor Lewkowycz, Maarten Paul Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. _arXiv:2112.00114_, 2021.
* [52] Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning. _arXiv:2305.12295_, 2023.
* [53] Bhargavi Paranjape, Scott Lundberg anbd Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. ART: Automatic multi-step reasoning and tool-use for large language models. _arXiv:2303.09014_, 2023.
* [54] Emmanouil Antonios Platanios, Adam Pauls, Subhro Roy, Yuchen Zhang, Alexander Kyte, Alan Guo, Sam Thomson, Jayant Krishnamurthy, Jason Wolfe, Jacob Andreas, and Dan Klein. Value-agnostic conversational semantic parsing. In _Proceedings of ACL_, August 2021.
* [55] Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. Synchromesh: Reliable Code Generation from Pre-trained Language Models. Proceedings of ICLR, 2022.
* [56] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. _arXiv preprint arXiv:2304.08354_, 2023.
* [57] Linlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi, Jonathan Herzig, Emily Pitler, Fei Sha, and Kristina Toutanova. Evaluating the impact of model scale for compositional generalization in semantic parsing. In _Proceedings of EMNLP_, December 2022.
* [58] Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius, and Stefanie Tellex. Planning with large language models via corrective re-prompting. _arXiv preprint arXiv:2211.09935_, 2022.
* [59] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. _arXiv preprint arXiv:1908.10084_, 2019.
* [60] David Rogers and Mathew Hahn. Extended-connectivity fingerprints. _Journal of chemical information and modeling_, 50(5):742-754, 2010.
* [61] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. _Proceedings of NeurIPS_, 33:12559-12571, 2020.
* [62] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, et al. Multitask prompted training enables zero-shot task generalization. In _Proceedings of ICLR_, 2022.
* [63] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language Models Can Teach Themselves to Use Tools. _arXiv:2302.04761_, 2023.

* [64] Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. PICARD: Parsing incrementally for constrained auto-regressive decoding from language models. In _Proceedings of EMNLP_, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
* [65] Pratyusha Sharma, Antonio Torralba, and Jacob Andreas. Skill induction and planning with latent language. _arXiv preprint arXiv:2110.01517_, 2021.
* [66] Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. Compositional generalization and natural language variation: Can a semantic parsing approach handle both? In _Proceedings of ACL_, August 2021.
* [67] Richard Shin and Benjamin Van Durme. Few-shot semantic parsing with language models trained on code. In _Proceedings of NAACL_, pages 5417-5425, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.396. URL [https://aclanthology.org/2022.naacl-main.396](https://aclanthology.org/2022.naacl-main.396).
* [68] Richard Shin, Christopher Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. Constrained language models yield few-shot semantic parsers. In _Proceedings of EMNLP_. Association for Computational Linguistics, November 2021.
* [69] Tom Silver, Varun Hariprasad, Reece S Shuttleworth, Nishanth Kumar, Tomas Lozano-Perez, and Leslie Pack Kaelbling. PDDL planning with pretrained large language models. In _NeurIPS 2022 Foundation Models for Decision Making Workshop_, 2022.
* [70] David Smith and Jason Eisner. Quasi-Synchronous Grammars: Alignment by Soft Projection of Syntactic Dependencies. In _Proceedings on the Workshop on Statistical Machine Translation_, 2006.
* [71] Jennifer J Sun, Megan Tjandrasuwita, Atharva Sehgal, Armando Solar-Lezama, Swarat Chaudhuri, Yisong Yue, and Omar Costilla-Reyes. Neurosymbolic programming for science. _arXiv preprint arXiv:2210.05050_, 2022.
* [72] Didac Suris, Sachit Menon, and Carl Vondrick. ViperGPT: Visual Inference via Python Execution for Reasoning. _arXiv:2303.08128_, 2023.
* [73] Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. _arXiv:2210.09261_, 2022.
* [74] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (a benchmark for llvms on planning and reasoning about change). _arXiv preprint arXiv:2206.10498_, 2022.
* [75] Gregor vom Scheidt. Experimental results from applying GPT-4 to an unpublished formal language. _arXiv:2305.12196_, 2023.
* [76] Bailin Wang, Ivan Titov, Jacob Andreas, and Yoon Kim. Hierarchical Phrase-based Sequence-to-Sequence Learning. In _Proceedings of EMNLP_, 2022.
* [77] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. _arXiv preprint arXiv:2305.16291_, 2023.
* [78] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA. In _Proceedings of EMNLP_, 2007.
* [79] Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In _Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics_, pages 429-436, 2019.

* [80] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In _Proceedings of ICLR_, 2023.
* [81] Yushi Wang, Jonathan Berant, and Percy Liang. Building a semantic parser overnight. In _Proceedings of ACL_, Beijing, China, July 2015.
* [82] Zi Wang, Stefanie Jegelka, Leslie Pack Kaelbling, and Tomas Lozano-Perez. Focused model-learning and planning for non-Gaussian continuous state-action systems. In _2017 IEEE International conference on robotics and automation (ICRA)_, pages 3754-3761. IEEE, 2017.
* [83] Zi Wang, Caelan Reed Garrett, Leslie Pack Kaelbling, and Tomas Lozano-Perez. Active model learning and diverse action sampling for task and motion planning. In _2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 4107-4114. IEEE, 2018.
* [84] Zi Wang, Caelan Reed Garrett, Leslie Pack Kaelbling, and Tomas Lozano-Perez. Learning compositional models of robot skills for task and motion planning. _The International Journal of Robotics Research_, 40(6-7):866-894, 2021.
* [85] Jason Wei, Maarten Paul Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Mingbo Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In _Proceedings of ICLR_, 2022.
* [86] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In _Proceedings of NeurIPS_, 2022.
* [87] David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. _Journal of chemical information and computer sciences_, 28(1):31-36, 1988.
* [88] Victoria Xia, Zi Wang, and Leslie Pack Kaelbling. Learning sparse relational transition models. In _International Conference on Learning Representations_, 2019.
* [89] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evaluation of large language models of code. In _Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming_, pages 1-10, 2022.
* [90] Ryan Yang, Tom Silver, Aidan Curtis, Tomas Lozano-Perez, and Leslie Pack Kaelbling. Pg3: Policy-guided planning for generalized policy generation. _arXiv preprint arXiv:2204.10420_, 2022.
* [91] Songlin Yang, Yanpeng Zhao, and Kewei Tu. Neural bi-lexicalized PCFG induction. In _Proceedings of ACL_, Online, August 2021. Association for Computational Linguistics.
* [92] Songlin Yang, Yanpeng Zhao, and Kewei Tu. PCFGs can do better: Inducing probabilistic context-free grammars with many symbols. In _Proceedings of NAACL_, 2021.
* [93] Songlin Yang, Roger P Levy, and Yoon Kim. Unsupervised discontinuous constituency parsing with mildly context-sensitive grammars. _arXiv preprint arXiv:2212.09140_, 2022.
* [94] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. _arXiv preprint arXiv:2305.10601_, 2023.
* [95] Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Compositional exemplars for in-context learning. _arXiv preprint arXiv:2302.05698_, 2023.
* [96] Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. Benchmarking multimodal regex synthesis with complex structures. In _Proceedings of ACL_. Association for Computational Linguistics, July 2020.
* [97] Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. Satisfiability-aided language models using declarative prompting. _arXiv preprint arXiv:2305.09656_, 2023.

* [98] Pengcheng Yin, Hao Fang, Graham Neubig, Adam Pauls, Emmanouil Antonios Platanios, Yu Su, Sam Thomson, and Jacob Andreas. Compositional generalization for neural semantic parsing via span-level supervised attention. Proceedings of ACL, 2021.
* [99] John M Zelle and Raymond J Mooney. Learning to parse database queries using inductive logic programming. In _Proceedings of the national conference on artificial intelligence_, pages 1050-1055, 1996.
* [100] Hao Zhu, Yonatan Bisk, and Graham Neubig. The return of lexical dependencies: Neural lexicalized PCFGs. _Transactions of the Association for Computational Linguistics_, 8, 2020.

## Appendix A Experiment Details

### Semantic Parsing

Statistics and Splits.We show the statistics for the splits used for the experiments in Table 6.

For GeoQuery, we utilize the standard split from Zelle and Mooney [99] in the retrieval-based setting and the Template, TMCD, and Length splits from Shaw et al. [66]. We randomly sample 32 examples from the training set of the standard split to create the few-shot split. To generate the NewFunc split, we designate examples utilizing the following eight functions as test examples: smallest, shortest, most, highest, sum, population_1, count, major; the remaining examples are incorporated into the training set.

For SMCalFlow, we adopt the 16-shot and 128-shot cross-domain settings from Yin et al. [98] as our few-shot and retrieval-based settings, respectively. It is noteworthy that the training set of the original splits contains approximately 25k in-domain training examples. Previous work [96, 57] utilized these examples as their retrieval set. However, in our experiments, we found that incorporating in-domain examples did not enhance performance. Consequently, we use 16/128 cross-domain examples as our training set in the few-shot and retrieval settings, respectively. For all experiments on SMCalFlow, we used the preprocessed version from Qiu et al. [57], which employs a more concise LISPRESS format [54] than the original version [98]. This format aligns with Ye et al. [96] for a fair comparison.

For Overnight-Blocks, we employ the standard split from Wang et al. [81] in the retrieval setting. We randomly sample 32 examples from the training set of the standard split to create the few-shot split.

Scoring Functions for Constrained Generation.For each candidate continuation \(\mathbf{w}\in\Sigma[\mathbf{y}_{\text{prefix}}]\) for correction, we first form a partial program via concatenation \(\mathbf{y}_{\text{prefix}}\cdot\mathbf{w}\) and then feed it into Codex to obtain the score for the candidate via

\[\log P_{\text{LLM}}(\mathbf{w}\,|\,\widehat{G},\mathbf{x},\mathbf{y}_{\text{prefix}},(\bm {x}^{(i)},G[\mathbf{y}^{(i)}],\mathbf{y}^{(i)})_{i=1}^{N}).\]

In the case where \(\mathbf{w}\) consists of multiple BPE tokens, e.g., FindManger( is tokenized into Find, Manager, and (, we average the token-level log-likelihood to obtain a candidate-level score. However, when the size of \(\Sigma[\mathbf{y}_{\text{prefix}}]\) exceeds 16, invoking Codex for each candidate becomes too expensive. To address this issue, we employ SentenceBERT to select 16 most plausible candidates first via a dot product,

\[(\text{SentenceBERT}(\hat{\mathbf{y}}_{t}))^{\top}(\text{SentenceBERT}(\mathbf{y}_{ \text{prefix}}\cdot\mathbf{w})),\]

where SentenceBERT yields the embeddings for the incorrect prediction \(\hat{\mathbf{y}}_{t}\) and a candidate of corrected partial program \(\mathbf{y}_{\text{prefix}}\cdot\mathbf{w}\).

\begin{table}
\begin{tabular}{l|c c c|c c c|c c c c} \hline \hline \multicolumn{2}{c}{Approach} & \multicolumn{3}{c}{**Few-Shot**} & \multicolumn{3}{c}{**Retrieval-based**} & \multicolumn{3}{c}{**GeoQuery Out-of-Dist.**} \\  & GeoQuery & SMCalflow & Overnight-Blk & GeoQuery & SMCalflow & Overnight-Blk & Template & TMCD & Length & NewFunc \\ \hline Train & 32 & 16 & 32 & 560 & 128 & 1436 & 441 & 440 & 440 & 453 \\ Test & 280 & 360 & 399 & 280 & 360 & 399 & 439 & 440 & 440 & 447 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Statistics of the splits used for experiments on semantic parsing.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Approach & **GeoQuery** & **SMCalFlow** & **Overnight-B** \\ \hline Standard Prompting (unconstrained decoding) & 81.5 (1.0) & 46.4 (1.0) & 54.7 (1.0) \\ _w. constrained decoding_ (\(\widehat{\mathbf{y}}\in L(G)\)) & 81.8 (4.3) & 49.2 (5.6) & 54.7 (1.6) \\ Linearized Derivation Tree Prompting & 77.5 (1.0) & 50.0 (1.0) & 56.4 (1.0) \\ \hline Grammar Prompting (unconstrained decoding) & 87.5 (1.0) & 50.8 (1.0) & 57.4 (1.0) \\ _w. grammar constraint_ (\(\widehat{G}\subseteq G\)) & 88.6 (3.0) & 51.3 (3.0) & 60.4 (1.4) \\ _w. grammar and program constraint_ (\(\widehat{\mathbf{y}}\in L(\widehat{G})\)) & 88.9 (3.3) & 52.4 (3.3) & 60.9 (2.8) \\ _w. oracle grammar_ (\(\widehat{G}=G[\mathbf{y}]\)) & 96.1 (1.3) & 80.0 (1.0) & 94.2 (1.0) \\ _w. oracle grammar_ + _program constraint_ & 96.8 (2.1) & 83.6 (2.6) & 96.5 (1.0) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Extended results which show the number of Codex calls per example on few-shot semantic parsing in brackets. Columns in grey show program accuracy, while white columns others indicate execution accuracy.

The functionality of obtaining the log-likelihood for a candidate continuation given a prefix is applicable via Codex APIs 11 via setting logprobs=True and echo=True. Unfortunately, subsequent models (e.g., GPT-3.5 and GPT-4) disable such functionality. As a workaround, we simply use the scoring function based on SentenceBERT to directly select the best candidate in our PDDL planning experiments.

Footnote 11: [https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference)

Cost Efficiency.We assess various decoding strategies for their cost efficiency, focusing on the number of API calls. The number of Codex calls resulting from the few-shot semantic parsing experiments is presented in Figure 7, alongside the corresponding accuracy metrics. The results indicate that standard prompting under constrained decoding leads to a significantly higher number of Codex calls. Similarly, grammar-prompting with constraints also results in an increased number of Codex calls. However, when employing both grammar and program constraints, the number of calls decreases meaningfully in comparison to standard prompting under constrained decoding. Future work might consider exploring strategies for more cost-efficient constrained decoding.

Grammar Prompting with Annotated Rules.We have additionally experimented with enhancing BNF rules by appending natural language comments. As illustrated in Figure 6, we pair each BNF rule with its corresponding natural language phrases extracted from the given query \(\mathbf{x}\). These manually annotated comments yield an explicit correspondence between natural language phrases and their corresponding BNF rules, thereby better facilitating interpretation and application of the grammars for generating programs. When employing the augmented grammar prompting, we noticed marginal improvements on SMCalFlow (+1.0%) and Overnight-Blocks (0.5%), with no observed enhancement on GeoQuery. While the gains might not appear significant, this predicted alignment could potentially contribute to improved interpretability and further constraints on generation. For instance, the phrase "someone's manager" should consistently trigger the function FindManager(. We leave the exploration of utilizing the augmented rules for future work.

### Molecule Generation

Sampling ProcedureDifferent from semantic parsing and PDDL planning, where the most probable program \(\mathbf{y}\) needs to be found via \(\arg\max\) inference, molecule generation has empty specification \(\mathbf{x}\) and requires sampling from a prompting-based distribution. The sampling procedure for grammar prompting consists of three stages: (1) we randomly sample a permutation of given molecules, denoted as \(\pi\), (2) based on a prompt formed by the permutation, we sample a specialized grammar \(\widehat{G}\) via

\[\widehat{G}\sim P_{\text{LLM}}(G^{\prime}\,|\,\mathbf{x},(G[\mathbf{y}^{(\pi[i])}], \mathbf{y}^{(\pi[i])})_{i=1}^{N}),\]

iii) we finally obtain the molecule conditioned \(\widehat{G}\),

\[\widehat{\mathbf{y}}\sim P_{\text{LLM}}(\mathbf{y}\,|\,\widehat{G},(G[\mathbf{y}^{(\pi[i]) }],\mathbf{y}^{(\pi[i])})_{i=1}^{N}).\]

We list the hyperparameters used for the sampling procedure in for (2) in Table 8 (top) and for (3) in Table 8 (bottom).

\begin{table}
\begin{tabular}{l c c c} \hline \hline Molecule Class & Temperature & Presence Penalty & Frequency Penalty \\ \hline _Sampling specialized grammars_\(\widehat{G}\) & & & \\ Acrylates & 0.6 & 0.1 & 0.1 \\ Chain Extenders & 0.6 & 0.1 & 0.1 \\ Isocyanates & 0.6 & 0.4 & 0.4 \\ \hline _Sampling molecules_\(\widehat{\mathbf{y}}\) & & & \\ Acrylates & 0.6 & 0.1 & 0.1 \\ Chain Extenders & 0.6 & 0.1 & 0.1 \\ Isocyanates & 0.3 & 0.4 & 0.4 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Hyperparameters for sampling specialized grammars \(\widehat{G}\) (top) and the molecules \(\widehat{\mathbf{y}}\) in grammar prompting for molecule generation. Standard prompting uses the same hyperparameters for \(\mathbf{y}\).

In comparison, the sampling procedure for standard prompting only consists of two stages: (1) we randomly sample a permutation of given molecules, denoted as \(\pi\), (2) based on a prompt formed by the permutation, we directly sample a molecule via

\[\widehat{\boldsymbol{y}}\sim P_{\text{LLM}}(\boldsymbol{y}\,|\,(\boldsymbol{y}^{( \pi[i])})_{i=1}^{N}).\]

The hyperparameters used for Step (2) is the same as in grammar prompting and shown in Table 8 (bottom).

While we observed that Earley-based constrained generation enhances grammar prompting in terms of improving validity, other metrics, such as the retrosynthesis score, decreased significantly. This discrepancy could be attributed to the fact that existing LLMs, due to their limited exposure to molecules represented in SMILES format, struggle with comprehending and applying the BNF grammar rules of SMILES. Overall, our current findings serve as preliminary evidence that grammar prompting can tap into the capacity of LLMs to understand and apply BNF rules. However such capacity still remains limited in text-focused LLMs.

### PDDL Planning

Restricted Action SpaceThe specialized grammar defined for PDDL planning essentially delineates a constrained action space that includes necessary actions and their associated objects. Our empirical results found that limiting the classical GBFS planner to the objects selected by a specialized grammar proved too restrictive, yielding beneficial results only within the Blocks domain. Therefore, we decided to remove this limitation, thus expanding the action space of GBFS to contain the actions predicted from the grammar with an unrestricted range of objects.

Figure 6: Example of grammar prompting where BNF grammars are additionally annotated with natural language comments (shown in green). These manually curated comments provide a detailed mapping between natural language phrases and their corresponding BNF rules, thereby better facilitating interpretation and application of the grammars for generating programs. We manually craft and add these comments to the few-shot prompts (top). The model predicts this during inference (bottom).

Prompts

Figures 7, 8, and 9 demonstrate the prompts with grammars, based on actual examples in the SMCalFlow, GeoQuery, and Overnight datasets respectively. Because the general grammar of SMCalFlow is long (around 4k tokens), we do not include it within the prompt. For GeoQuery and Overnight, the general grammar is integrated as part of the instruction within the prompt. In the context of molecule generation, the general grammar for SMILES12 is also included. Figures 10 and 11 demonstrate the prompts with action DSLs for PDDL planning.

Footnote 12: [https://metamolecular.com/cheminformatics/smiles/formal-grammar/](https://metamolecular.com/cheminformatics/smiles/formal-grammar/)

**LLM Prompt**

You are an expert programmer, and you need to write a program for the given natural language query. First, you should write a grammar that contains all the necessary BNF rules. Then, you should write programs that conform to your predicted rules.

query: I need a meeting with Elli and her team on Wednesday afternoon....................................

BNF grammar rules:

 call := "(Yield" event ")"  event := "(CreatePreflightEventWrapper" event_constraint ")"  event_constraint := "(8" event_constraint event_constraint ")"  | "(Event.start_?"datetime_constraint ")"  | "(Event.attendees_?" attendee_constraint ")"  datetime_constraint := "(DateTimeConstraint" time date ")"  time := "(Afternoon)"  date := day  day := "(NextDOW day ")" | DAY  DAY := "(Wednesday)"  attendee_constraint := "(&" attendee_constraint ")"  | "(AttendeeListHasRecipient" person_")"  person := recipient | "(FindEend" person ")"  recipient := "(Execute (refer (extensionConstraint recipient ")))"  | "(RecipientWithNameLike (PersonName.apply" string "))"  string := "Elli" ....................................

program based on the BNF grammar rules:

(Yield(CreatePreflightEventWrapper(&(Event.start_?(DateTimeConstraint(Afternoon)  (NextDOW(Wednesday)))(Event.attendees_?(&(AttendeeListHasRecipient(Execute(refer  (extensionConstraint(RecipientWithNameLike(PersonName.apply "Elli")))))))) (AttendesListHasPeople(FindTeamOf(Execute(refer(extensionConstraint  (RecipientWithNameLike(PersonName.apply "Elli")

[MISSING_PAGE_FAIL:23]

**LLM Prompt**

You are an expert programmer, and you need to write a program for the given natural language query. First, you should write grammar rules by choosing from the following BNF rules. Then, you should write programs that conform to your predicted rules.

[BEGIN RULES]

list_value := "(listValue" list_value ")"

"(filter" list_value PROPERTY ")"

"(filter" list_value PROPERTY OP list_value ")"

"(superlative" list_value AGGREGATE "

(ensureNumericProperty" PROPERTY "))"

"(countSuperlative" list_value AGGREGATE PROPERTY ")"

"(size" list_value ")"

"(aggregate" AGGREGATE list_value ")"

"(getProperty" list_value PROPERTY ")"

"(getProperty (singleton" SINGLETON_VALUE ") ltype)"

"(concat" ENTITY_VALUE ENTITY_VALUE ")"

"(concat" NUMBER_VALUE ")"

ENTITY_VALUE | NUMBER_VALUE ")"

PROPERTY := "shape" |color" |length" |is_special" |"width"

"height" |left" |right" |above" |below"

"(reverse left)" |"(reverse right)"

"(reverse above)" |"(reverse below)"

SINGLETON_VALUE := "en.block" |en.shape" |"en.color"

ENTITY_VALUE := "en.block.block" |"en.block.block2" |"en.shape.pyramid"

"en.shape.cube" |"en.color.red" |"en.color.green"

NUMBER_VALUE := "s1.inch" |"6 en.inch" |"2"

OP := "=" |">" |"<" |">" |"<=" |"|="

AGGREGATE := "sum" |"max" |"min" |"avg"
[END RULES]

query: which brick is taller than brick 1

BNF grammar rules:

list_value := "(listValue" list_value ")"

"(aggregate" AGGREGATE list_value ")"

"(getProperty" list_value PROPERTY ")"

"(getProperty (singleton" SINGLETON_VALUE ") ltype)"

AGGREGATE := "avg"

SINGLETON_VALUE := "en.block"

PROPERTY := "width"

program based on the BNF grammar rules:

(listValue(aggregate avg (getProperty(getProperty(singleton en.block) ltype) width)))

query: look right of block 1 and get the special block

**LLM Output**

BNF grammar rules:

list_value := "(listValue" list_value ")"

"(filter" list_value PROPERTY OP list_value ")"

"(filter" list_value PROPERTY ")"

"(getProperty (singleton" SINGLETON_VALUE ") ltype)"

ENTITY_VALUE := "en.block"

PROPERTY := "is_special"

"(reverse left)"

OP := "="

ENTITY_VALUE := "en.block.block1"

program based on the BNF grammar rules:

(listValue(filter(filter(getProperty(singleton en.block) ltype) is_special)

(reverse left) = en.block.block1))

Figure 9: Prompt with real examples from the Overnight dataset. Some long rules are omitted to fit the space.

**LLM Prompt**

**Q:**

(:objects a b c d e - block)

(:init (clear d) (clear c) (ontable d) (ontable a) (on c c) (on e b) (on b a) (handempty))

(:goal (on a e) (on e b) (on b d) (on d c))

**DSL:**

plan := action+

action := "(unstack" object object ")"

"(put-down" object ")"

"(pick-up-and-stack" object object ")"

"(unstack-and-stack" object object ")"

object := "c" | "e" | "d" | "b" | "a"

**A:**

(unstack c e) (put-down c) (pick-up-and-stack d c) (unstack e b) (put-down c) (unstack-and-stack b a d) (pick-up-and-stack e b) (pick-up-and-stack a e)

......................................................................

**Q:**

(:objects a b c d - block)

(:init (clear a) (clear c) (clear d) (ontable a) (ontable b) (ontable d) (on c b) (handempty))

(:goal (on a b) (on b c) (on c d))

**DSL:**

plan := action+

action := "(unstack-and-stack" object object ")"

| "(pick-up-and-stack" object object ")"

object := "c" | "b" | "d" | "a"

**A:**

(unstack-and-stack c b d) (pick-up-and-stack b c) (pick-up-and-stack a b)

......................................................................................

**Q:**

(:objects a b c d - block)

(:init (clear c) (clear a) (clear b) (clear d) (ontable c) (ontable a) (ontable b) (ontable d) (handempty))

(:goal (on d c) (on c b) (on b a))

**DSL:**

**LLM Output**

plan := action+

action := "(pick-up-and-stack" object object ")"

object := "b" | "a" | "c" | "d"

**A:**

(pick-up-and-stack b a) (pick-up-and-stack c b) (pick-up-and-stack d c)

**Figure 10: Prompt with real examples in the Blocks domain from Pyperplan. The prompt template follows [69].

[MISSING_PAGE_FAIL:26]