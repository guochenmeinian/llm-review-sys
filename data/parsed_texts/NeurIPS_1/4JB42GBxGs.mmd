Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions

Samantha Chen

Department of Computer Science Engineering

University of California - San Diego

sac003@ucsd.edu

&Yusu Wang

Halicioglu Data Science Institute

University of California - San Diego

yusuwang@ucsd.edu

###### Abstract

Learning distance functions between complex objects, such as the Wasserstein distance to compare point sets, is a common goal in machine learning applications. However, functions on such complex objects (e.g., point sets and graphs) are often required to be invariant to a wide variety of group actions e.g. permutation or rigid transformation. Therefore, continuous and symmetric _product_ functions (such as distance functions) on such complex objects must also be invariant to the _product_ of such group actions. We call these functions _symmetric and factor-wise group invariant functions_ (or _SFGI functions_ in short). In this paper, we first present a general neural network architecture for approximating SFGI functions. The main contribution of this paper combines this general neural network with a sketching idea to develop a specific and efficient neural network which can approximate the \(p\)-th Wasserstein distance between point sets. Very importantly, the required model complexity is _independent_ of the sizes of input point sets. On the theoretical front, to the best of our knowledge, this is the first result showing that there exists a neural network with the capacity to approximate Wasserstein distance with bounded model complexity. Our work provides an interesting integration of sketching ideas for geometric problems with universal approximation of symmetric functions. On the empirical front, we present a range of results showing that our newly proposed neural network architecture performs comparatively or better than other models (including a SOTA Siamese Autoencoder based approach). In particular, our neural network generalizes significantly better and trains much faster than the SOTA Siamese AE. Finally, this line of investigation could be useful in exploring effective neural network design for solving a broad range of geometric optimization problems (e.g., \(k\)-means in a metric space).

## 1 Introduction

Recently, significant interest in geometric deep learning has led to a focus on neural network architectures which learn functions on complex objects such point clouds [34; 22] and graphs [23]. Advancements in the development of neural networks for complex objects has led to progress in a variety of applications from 3D image segmentation [28] to drug discovery [3; 13]. One challenge in learning functions over such complex objects is that the desired functions are often required to be invariant to certain group actions. For instance, functions on point clouds are often permutation invariant with respect to the ordering of individual points. Indeed, developing permutation invariant or equivariant neural network architectures, as well as understanding their universal approximation properties, has attracted significant attention in the past few years; see e.g., [22; 34; 19; 20; 25; 16; 33; 29; 30; 4]However, in many geometric or graph optimization problems, our input goes beyond a single complex object, but multiple complex objects. For example, the \(p\)-Wasserstein distance \(\mathrm{W}_{p}(X,Y)\) between two point sets \(X\) and \(Y\) sampled from some metric space (e.g., \(\mathbb{R}^{d}\)) is a function over pairs of point sets. To give another example, the \(1\)-median of the collection of \(k\) point sets \(P_{1},\ldots,P_{k}\) in \(\mathbb{R}^{d}\) can be viewed as a function over \(k\) point sets.

A natural way to model such functions is to use product space. In particular, let \(\mathcal{X}\) denote the space of finite point sets from a bounded region in \(\mathbb{R}^{d}\). Then the \(p\)-Wasserstein distance can be viewed as a function \(\mathrm{W}_{p}:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}\). Similarly, 1-median for \(k\) point sets can be modeled by a function from the product of \(k\) copies of \(\mathcal{X}\) to \(\mathbb{R}\). Such functions are not only invariant to permutations of the factors of the product space (i.e. \(\mathrm{W}_{p}(X,Y)=\mathrm{W}_{p}(Y,X)\)) but are also invariant or equivariant with respect to certain group actions for each factor. For \(p\)-Wasserstein distance, \(\mathrm{W}_{p}\) is invariant to permutations of the ordering of points within both \(X\) and \(Y\). This motivates us to extend the setting of learning continuous group invariant functions to learning continuous functions over _product_ spaces which are both invariant to some product of group actions and symmetric. More precisely, we consider a type of function which we denote as an SFGI product functions.

**Definition 1.1** (SFGI product function): _Given compact metric spaces \((\mathcal{X}_{i},\mathrm{d}_{\mathcal{X}_{i}})\) where \(i\in[k]\), we define symmetric and factor-wise group invariant (SFGI) product functions as \(f:\mathcal{X}_{1}\times\mathcal{X}_{2}\times\cdots\mathcal{X}_{k}\rightarrow \mathbb{R}\) where \(f\) is (1) symmetric over the \(k\) factors, and (2) invariant to the group action \(G_{1}\times G_{2}\times\cdots\times G_{k}\) for some group \(G_{i}\) acting on \(\mathcal{X}_{i}\), for each \(i\in[1,k]\)._

Contributions.SFGI product functions can represent a wide array of geometric matching problems including computing the Wasserstein or Hausdorff distance between point sets. In this paper, we first provide a general framework for approximating SFGI product functions in Section 3.1. Our primary contribution, described in Section 3.2, is the integration of this general framework with a sketching idea in order to develop an efficient and specific SFGI neural network which can approximate the \(p\)-Wasserstein distance between point sets (sampled from a compact set in a nice metric space, such as the fixed-dimensional Euclidean space). Most importantly, the complexity of our neural network (i.e., number of parameters needed) is _independent_ of the maximum size of the input point sets, and depends on only the additive approximation error. To the best of our knowledge, this is the first architecture which provably achieves this property. This is in contrast to many existing universal approximation results on graphs or sets (e.g., for DeepSet) where the network sizes depend on the size of each graph or point set in order to achieve universality [20, 30, 4]. We also provide a range of experimental results in Section 4 showing the utility of our neural network architecture for approximating Wasserstein distances. We compare our network with both a SOTA Siamese autoencoder [15], a natural Siamese DeepSets network, and the standard Sinkhorn approximation of Wasserstein distance. Our results show that our universal neural network architecture produces Wasserstein approximations which are better than the Siamese DeepSets network, comparable to the SOTA Siamese autoencoder and generalize much better than both to input point sets of sizes which are unseen at training time. Furthermore, we show that our approximation (at inference time) is much faster than the standard Sinkhorn approximation of the \(p\)-Wasserstein distance at similar error threshholds. Moreover, our neural network trains much faster than the SOTA Siamese autoencoder. Overall, our network is able to achieve **equally accurate or better** Wasserstein approximations which **generalize better** to point sets of unseen size as compared to SOTA while **significantly reducing** training time. In Appendix B, we provide discussion of issues with other natural choices of neural network architectures one might use for estimating Wasserstein distances, including one directly based on Siamese networks, which are often used for metric learning.

All missing proofs / details can be found in the **Supplementary materials**.

Related work.Efficient approximations of Wasserstein distance via neural networks are an active area of research. One approach is to use input convex neural networks to approximate the 2-Wasserstein distance[18, 27]. However, for this approach, training is done _per_ pair of inputs and is restricted to the 2-Wasserstein distance which makes it unsuitable for a general neural network approximation of \(p\)-Wasserstein distances between discrete distributions. This neural approximation method contrasts with our goal: a general neural network that can approximate the \(p\)-Wasserstein distance between any two point sets in a compact metric space to within \(\epsilon\)-accuracy. Siamese networks are another approach for popular approach for learning Wasserstein distances. Typically, a Siamese network is composed of a single neural network which maps two input instances to Euclidean space.

The output of the network is represented then by \(\ell_{p}\)-norm between the output embeddings. In [5], the authors utilize a Siamese autoencoder which takes two histograms (images) as input. For their architecture, a single encoder network is utilized to map each histogram to an embedding in Euclidean space, while a decoder network maps each Euclidean embedding back to an output histogram. The Kullback-Liebler (KL) divergence between original histogram and the output histogram (i.e. reconstruction loss) is used during training to regularize the embeddings and the final estimate of 2-Wasserstein distance is the \(\ell_{2}\) norm between the embeddings. The idea of learning Wasserstein distances via Siamese autoencoders was extended in [15] to point cloud data with the Wasserstein point cloud embedding network (WPCE) where the original KL reconstruction loss was replaced with a differentiable Wasserstein approximation between the original point set and a fixed-size output point set from the decoder network. In our subsequent experiments, we show that our neural network trains much more efficiently and generalizes much better than WPCE to point sets of unseen size.

Moreover, the concept of group invariant networks was previously investigated in several works, including [34; 22; 19; 17; 9]. For instance, DeepSets [34] and PointNet [19] are two popular permutation invariant neural network which were shown to be universal with respect to set functions. In addition to group invariance, there have also been efforts to explore the notion of invariance with respect to combinations of groups, such as invariance to both SE(3) and permutation group [11; 21] or combining basis invariance with permutation invariance [17]. Our work differs from previous work in that we address a universal neural network which is invariant with respect to a _specific_ combination of permutation groups that corresponds to an SFGI function on point sets. In general, we can view this as a subgroup of the permutation group - encoding the invariance of each individual point set with the symmetric requirement of the product function corresponds to a specific subgroup of the permutation group. Thus, previous results regarding permutation invariant architectures such as DeepSets, PointNet or combinations of group actions (such as [11]) do not address our setting of SFGI functions or \(p\)-Wasserstein distances.

## 2 Preliminaries

We will begin with basic background on groups, universal approximation, and Wasserstein distances.

Groups and group actionsA group \(G\) is an algebraic structure that consists of a set of elements and a binary operation that satisfies a specific set of axioms: (1) the associative property, (2) the existence of an identity element, and (3) existence of inverses for each element in the set. Given a metric space \((\mathcal{X},\mathrm{d}_{\mathcal{X}})\), the action of the group \(G\) on \(\mathcal{X}\) is a function \(\alpha:G\times\mathcal{X}\rightarrow\mathcal{X}\) that transforms the elements of \(\mathcal{X}\) for each element \(\pi\in G\). For each element \(\pi\in G\), we will write \(\pi\cdot x\) to denote the action of a group element \(\pi\) on \(x\) instead of \(\alpha(\pi,x)\). For example, if \(G\) is the permutation group over \([N]:=\{1,2,\ldots,N\}\), and \(\mathcal{X}=\mathbb{R}^{N}\), then for any \(\pi\in G\), \(\pi\cdot x\) represents the permutation of elements in \(x\in\mathbb{R}^{N}\) via \(\pi\) i.e. given \(x=(x_{1},x_{2},\ldots,x_{N})\), \(\pi\cdot x=(x_{\pi(1)},x_{\pi(2)}\ldots,x_{\pi(N)})\). A function \(f:\mathcal{X}\rightarrow\mathcal{Y}\) is \(G\)-_invariant_ if for any \(X\in\mathcal{X}\) and any \(\pi\in G\), we have that \(f(X)=f(\pi\cdot X)\).

Universal Approximation.Let \(\mathcal{C}(\mathcal{X},\mathbb{R})\) denote the set of continuous functions from a metric space \((\mathcal{X},d_{\mathcal{X}})\) to \(\mathbb{R}\). Given two families of functions \(\mathcal{F}_{1}\) and \(\mathcal{F}_{2}\) where \(\mathcal{F}_{1}\subseteq\mathcal{F}_{2}\) and \(\mathcal{F}_{1},\mathcal{F}_{2}\subseteq\mathcal{C}(\mathcal{X},\mathbb{R})\), we say that \(\mathcal{F}_{1}\)_universally approximates_\(\mathcal{F}_{2}\) if for any \(\epsilon>0\) and any \(f\in\mathcal{F}_{2}\), there is a \(g\in\mathcal{F}_{1}\) such that \(\|g-f\|_{\infty}<\epsilon\). Different norms on the space of functions can be used, but we will use \(L_{\infty}\) norm in this paper, which intuitively leads to additive pointwise-error over the domain of these functions. The classic universal approximation theorem for multilayer perceptrons (MLPs) [7] states that a feedforward neural network with a single hidden layer, using certain activation functions, can approximate any continuous function to within an arbitrary _additive \(\epsilon\)-error._

Permutation invariant neural networks for point cloud dataOne of the most popular permutation invariant neural network models is the DeepSets model defined in [34]. DeepSets is designed to handle unordered input point sets by first applying a neural network to each individual element, then using sum-pooling to generate an embedding for the input data set, and finally, applying a final neural network architecture to the ouput embedding. Formally, suppose we are given a finite _multiset_\(S=\{x:x\in\mathbb{R}^{d}\}\) (meaning that an element can appear multiple times, and the number of times an element occurs in \(S\) is called its _multiplicity_). The DeepSets model is defined as

\[\mathcal{N}_{\mathrm{DeepSets}}(S)=g_{\theta_{2}}\Big{(}\sum_{x\in S}h_{\theta_{1 }}(x)\Big{)}\]

where \(h_{\theta_{1}}\) and \(g_{\theta_{2}}\) are neural network architectures. DeepSets can handle input point sets of variable sizes. It was also shown to be universal with respect to continuous _multiset functions_.

**Theorem 2.1** ([34; 1; 12]): _Assume the elements are from a compact set in \(\mathbb{R}^{k}\), and the input multiset size is fixed as \(N\). Let \(t=2kN+1\). Then any continuous multiset function, represented as \(f:\mathbb{R}^{k\times N}\rightarrow\mathbb{R}\) which is invariant with respect to permutations of the columns, can be approximated arbitrarily close in the form of \(\rho\Big{(}\sum_{x\in X}\phi(x)\Big{)}\), for continuous transformations \(\phi:\mathbb{R}^{k}\rightarrow\mathbb{R}^{t}\) and \(\rho:\mathbb{R}^{t}\rightarrow\mathbb{R}\)._

While universality for the case when \(k=1\) was shown using symmetric polynomials, the case for \(k>1\) in fact is quite subtle and the proof in [34] misses key details. For completeness, we provide a full proof in Appendix E.1 for when the output dimension of \(\phi\) is \(t=\binom{k+N}{k}\). It was recently shown in [12; 1] that the output dimension of \(\phi\) can be reduced to \(2kN+1\), which is the dimension of \(t\) which we use in Theorem 2.1 and subsequent theorems. In both the cases where the output dimension of \(\phi\) is \(t=\binom{k+N}{k}\) or \(t=2kN+1\), Theorem 2.1 implies that if we want to achieve universality, the required network size depends on input point cloud size.

Wasserstein distances and approximations.Here we will introduce Wasserstein distance for discrete measures. Let \((X,\mathrm{d}_{X})\) be a metric space. For two weighted point sets \(P=\{(x_{i},w_{i}):x_{i}\in X,\sum_{w_{i}}=1,i\in[n]\}\) and \(Q=\{(x^{\prime}_{i},w^{\prime}_{i}):x^{\prime}_{i}\in X,\sum_{w_{i}}=1,i\in[m]\}\), we define the Wasserstein distance between \(P\) and \(Q\) as

\[\mathrm{W}_{p}(P,Q)=\min_{\Pi\in\mathbb{R}^{n\times m}_{+}}\Big{\{}\Big{(} \langle\Pi,D^{p}\rangle\Big{)}^{1/p}:\Pi\mathbf{1}=[w_{1},\ldots,w_{n}],\Pi^{T }\mathbf{1}=[w^{\prime}_{1},\ldots,w^{\prime}_{m}]\Big{\}}\]

where \(D\in\mathbb{R}^{n\times m}_{+}\) is the distance matrix with \(D_{i,j}=\mathrm{d}_{X}(x_{i},x^{\prime}_{j})\). One can think of these weighted point sets as discrete probability distributions in \((X,\mathrm{d}_{X})\). When \(p=1\), \(\mathrm{W}_{1}\) is also commonly known as the Earth Mover's distance (EMD). Additionally, note that when \(p=\infty\), \(\mathrm{W}_{p}\) is the same as the Hausdorff distance between points in \(P\) and \(Q\) with non-zero weight. Computing Wasserstein distances amounts to solving a linear programming problem, which takes \(O(N^{3}\log N)\) (where \(N=\max\{n,m\}\)) time. There have been a number of methods for fast approximations of Wasserstein distances, including multi-scale and hierarchical solvers [24], and \(L_{1}\) embeddings via quadtree algorithms [2; 14]. In particular, entropic regularization of Wasserstein distance [6], also known as the Sinkhorn distance, is often used as the standard Wasserstein distance approximation for learning tasks. Unlike Wasserstein distance, the Sinkhorn approximation is differentiable and can be computed in approximately \(O(n^{2})\) time. The computation time is governed by a regularization parameter \(\epsilon\). As \(\epsilon\) approaches zero, the Sinkhorn distance approaches the true Wasserstein distance.

## 3 Learning functions between point sets

We will first present a general framework for approximating SFGI-functions and then show how this framework along with geometric sketches of our input data enables us to define neural networks which can approximate \(p\)-Wasserstein distances with complexity independent of input data size.

### A general framework for functions on product spaces

One of the key ingredients in our approach is the introduction of what we call a _sketch_ of input data to an Euclidean space whose dimension is independent of the size of the input data.

**Definition 3.1** (Sketch): _Let \(\delta>0\), \(a\in\mathbb{N}^{+}\), and \(G\) be a group which acts on \(\mathcal{X}\). A \((\delta,a,G)\)-**sketch** of a metric space \((\mathcal{X},\mathrm{d}_{\mathcal{X}})\) consists of a \(G\)-invariant continuous encoding function \(h:\mathcal{X}\rightarrow\mathbb{R}^{a}\) and a continuous decoding function \(g:\mathbb{R}^{a}\rightarrow\mathcal{X}\) such that \(\mathrm{d}_{\mathcal{X}}(g\circ h(S),S)<\delta\)._Now let \((\mathcal{X}_{1},\mathrm{d}_{\mathcal{X}_{1}}),\ldots,(\mathcal{X}_{m},\mathrm{d}_{ \mathcal{X}_{m}})\) be compact metric spaces. The product space \(\mathcal{X}_{1}\times\cdots\times\mathcal{X}_{m}\) is still a metric space equipped with the following natural metric induced from metrics of each factor:

\[\mathrm{d}_{\mathcal{X}_{1}\times\cdots\times\mathcal{X}_{m}}((A_{1},\ldots,A_{ m}),(A^{\prime}_{1},\ldots,A^{\prime}_{m}))=\mathrm{d}_{\mathcal{X}_{1}}(A_{1},A^{ \prime}_{1})+\cdots+\mathrm{d}_{\mathcal{X}_{m}}(A_{m},A^{\prime}_{m}).\]

Suppose \(G_{i}\) is a group acting on \(\mathcal{X}_{i}\), for each \(i\in[m]\). In the following result, instead of SFGI product functions, we first consider the more general case of _factor-wise group invariant functions_, namely functions \(f:\mathcal{X}_{1}\times\cdots\times\mathcal{X}_{m}\to\mathbb{R}\) such that \(f\) is uniformly continuous and invariant to the group action \(G_{1}\times\cdots\times G_{m}\).

**Lemma 3.2**: _Suppose \(f:\mathcal{X}_{1}\times\cdots\times\mathcal{X}_{m}\to\mathbb{R}\) is uniformly continuous and invariant to \(G_{1}\times\cdots\times G_{m}\). Additionally, assume that for any \(\delta>0\), \((\mathcal{X}_{i},\mathrm{d}_{\mathcal{X}_{i}})\) has a \((\delta,a_{i},G_{i})\)-sketch where \(a_{i}\) may depend on \(\delta\). Then for any \(\epsilon>0\), there is a continuous \(G_{i}\)-invariant functions \(\phi_{i}:\mathcal{X}_{i}\to\mathbb{R}^{a_{i}}\) for all \(i\in[k]\) and a continuous function \(\rho:\mathbb{R}^{a_{1}}\times\cdots\times\mathbb{R}^{a_{m}}\to\mathbb{R}\) such that_

\[|f(A_{1},A_{2},\ldots,A_{m})-\rho(\phi_{1}(A_{1}),\phi_{2}(A_{2}),\ldots,\phi_ {k}(A_{m}))|<\epsilon\]

_for any \((A_{1},\ldots,A_{m})\in\mathcal{X}_{1}\times\cdots\times\mathcal{X}_{m}\). Furthermore, if \(\mathcal{X}_{1}=\ldots\mathcal{X}_{2}=\cdots=\mathcal{X}_{m}\), then we can choose \(\phi_{1}=\phi_{2}=\ldots\phi_{m}\)._

Note that a recent result from [17] shows that a continuous factor-wise group invariant function \(f:\mathcal{X}_{1}\times\cdots\mathcal{X}_{m}\to\mathbb{R}\) can be **represented** (not approximated) by the form \(f(v_{1},\ldots,v_{m})=\rho(\phi_{1}(v_{1}),\ldots,\phi_{k}(v_{m}))\) if _there exists a topological embedding from \(\mathcal{X}_{i}/G_{i}\) to Euclidean space_. The condition that each quotient \(\mathcal{X}_{i}/G_{i}\) has a topological embedding in fixed dimensional Euclidean space is strong. A topological embedding requires injectivity, while in a sketch, one can collapse input objects as long as after decoding, we obtain an approximated object which is close to the input. Our result can be viewed as a relaxation of their result by allowing our space to have an approximate fixed-dimensional embedding (i.e., our \((\delta,a,G)\)**-sketch**).

We often consider the case where \(\mathcal{X}=\mathcal{X}_{1}=\cdots=\mathcal{X}_{m}\) i.e. \(f:\mathcal{X}\times\cdots\mathcal{X}\to\mathbb{R}\) where \(G\) is a group acting on the factor \(\mathcal{X}\). Oftentimes, we require the function to not only be invariant to the actions of a group \(G\) on each individual \(\mathcal{X}\) but also _symmetric_ with respect to the ordering of the input. By this, we mean \(f(A_{1},\ldots,A_{m})=f(A_{\pi(1)},\ldots,A_{\pi(m)})\) where \(\pi\) is a permutation on \([m]\). In other words, we now consider the _SFGI product function_\(f\) as introduced in Definition 1.1. The extra symmetry requirement adds more constraints to the form of \(f\). We show that the set of uniformly continuous SFGI product function can be universally approximated by product function with an even simpler form than Lemma 3.2 as stated in the theorem below.

**Lemma 3.3**: _Assume the same setup as Lemma 3.2 with \(\mathcal{X}=\mathcal{X}_{1}=\cdots=\mathcal{X}_{m}\) and \(G=G_{1}=\cdots=G_{m}\). Assume that \(\mathcal{X}\) has a \((\delta,a,G)\)-sketch. Additionally, suppose \(f\) is symmetric; hence \(f\) is a SFGI function. Let \(t=2am+1\). Then for any \(\epsilon>0\), there is a continuous \(G\)-invariant function \(\phi:\mathcal{X}\to\mathbb{R}^{t}\) and a continuous function \(\rho:\mathbb{R}^{t}\to\mathbb{R}\) such that_

\[|f(A_{1},\ldots,A_{m})-\rho\Big{(}\sum_{i=1}^{m}\phi(A_{i})\Big{)}|<\epsilon\]

Now suppose we want approximate an SFGI product function, \(f\), with a neural network. Lemma 3.3 implies that we can approximate \(\phi\) with any universal \(G\)-invariant neural network which embeds our original space \(\mathcal{X}\) to some Euclidean space \(\mathbb{R}^{a}\). Then the outer architecture \(\rho\) can be any universal architecture (e.g. MLP). Finding a universal \(G\)-invariant neural network to realize \(\phi\) over a single factor space \(\mathcal{X}\) is in general much easier than finding a SFGI neural network, and as we discussed at the end of Section 1, we already know how to achieve this for several settings. We will show how this idea is at work for approximating SFGI functions between point sets in the next subsection.

### Universal neural networks for functions between point sets

We are interested in learning symmetric functions between point sets (i.e. any \(p\)-Wasserstein distance) which are factor-wise _permutation_ invariant. In this section, we will show that we can find a \((\delta,a,G)\)-sketch for the space of weighted point sets. This allows us to combine Lemma 3.3 with DeepSets to define a set of neural networks which can approximate \(p\)-th Wasserstein distances to arbitrary accuracy. Furthermore, the encoding and decoding functions can be approximated with neural networks where their model complexity is _independent_ of input point set size. Thus, the resulting neural network used to approximate Wasserstein distance also has **bounded model complexity**.

Set up.Given some metric space \((\Omega,\mathrm{d}_{\Omega})\), let \(\mathcal{X}\) be the set of _weighted_ point sets with at most \(N\) elements. In other words, each \(S\in\mathcal{X}\) has the form \(S=\{(x_{i},w_{i}):w_{i}\in\mathbb{R}_{+},\sum_{i}w_{i}=1,x_{i}\in\Omega\}\) and \(|S|\leq N\). One can also consider \(\mathcal{X}\) to be the set of weighted Dirac measures over \(\Omega\). For simplicity, we also sometimes use \(S\) to refer to just the set of points \(\{x_{i}\}\) contained within it. We will consider the metric over \(\mathcal{X}\) to be the \(p\)-th Wasserstein distance, \(\mathrm{W}_{p}\). We refer to the metric space of weighted point sets over \(\Omega\) as \((\mathcal{X},\mathrm{W}_{p})\).

First, we will show that given a \(\delta\), there is a \((\delta,a,G)\)-sketch of \(\mathcal{X}\) with respect to \(\mathrm{W}_{p}\). The embedding dimension \(a\) depends on the so-called _covering number_ of the metric space \((\Omega,\mathrm{d}_{\Omega})\) from which points are sampled. Given a compact metric space \((\Omega,\mathrm{d}_{\Omega})\), the _covering number_\(\nu_{\Omega}(r)\)_w.r.t. radius \(r\) is the minimal number of radius \(r\) balls needed to cover \(\Omega\). As a simple example, consider \(\Omega=[-\Delta,\Delta]\subseteq\mathbb{R}\). Given any \(r\), we can cover \(X\) with \(\frac{2\Delta}{r}\) intervals so \(\nu_{\Omega}(r)\leq\frac{2\Delta}{r}\). The collection of the center of a set of \(r\)-balls that cover \(\Omega\) an _\(r\)-net of \(\Omega\)_. For a compact set \(\Omega\subset\mathbb{R}^{d}\) with diameter \(D\), its covering number \(\nu_{\Omega}(r)\) is a constant depending on \(D\), \(r\) and \(d\) only.

**Theorem 3.4**: _Set \(\mathrm{d}_{\mathcal{X}}\) to be \(W_{p}\) for \(1\leq p<\infty\). Let \(G\) be the permutation group. For any \(\delta>0\), let \(\delta_{0}=\frac{1}{2}\sqrt[d]{\delta/2}\) and \(a=\nu_{\Omega}(\delta_{0})\) be the covering number w.r.t. radius \(\delta_{0}\). Then there is a \((\delta,a,G)\)-sketch of \(\mathcal{X}\) with respect to \(\mathrm{W}_{p}\). Furthermore, the encoding function \(\mathbf{h}:\mathcal{X}\rightarrow\mathbb{R}^{a}\) can be expressed as the following where \(h:\Omega\rightarrow\mathbb{R}^{a}\) is continuous:_

\[\mathbf{h}(S)=\sum_{x\in S}h(x). \tag{1}\]

_Proof:_ Let \(\delta>0\) and let \(S\in\mathcal{X}\) be \(S=\{(x_{i},w_{i}):\sum w_{i}=1,x_{i}\in\Omega\}\) and \(|S|\leq N\). Given \(\delta_{0}=\frac{1}{2}\sqrt[d]{\delta/2}\) and \(a=\nu_{\Omega}(\delta_{0})\), we know \(\Omega\) has a \(\delta_{0}\)-net, \(C\), and we denote the elements of \(C\) as \(\{y_{1},\ldots,y_{a}\}\). In other words, for any \(x\in\Omega\), there is a \(y_{i}\in Cd\) such that \(\mathrm{d}_{\Omega}(x,y_{i})<\delta_{0}\).

First, we will define an encoding function \(\rho:\mathcal{X}\rightarrow\mathbb{R}^{a}\). For each \(y_{i}\), we will use a soft indicator function \(e^{-b\mathrm{d}_{\Omega}(x,B_{\delta_{0}}(y_{i}))}\) and set the constant \(b\) so that \(e^{-b\mathrm{d}_{\Omega}(x,B_{\delta_{0}}(y_{i}))}\) is "sufficiently" small if \(\mathrm{d}_{\Omega}(x,B_{\delta_{0}}(y_{i}))>\delta_{0}\). More formally, we know that \(\lim_{b\rightarrow\infty}e^{-b\delta_{0}}=0\) so there is \(\beta\in\mathbb{R}\) such that for all \(b>\beta\), \(e^{-b\delta_{0}}<\frac{\delta_{0}^{p}}{d_{max}^{p}a}\). Set \(b_{0}\) to be such that \(b_{0}>\beta\). Let \(h_{i}(x)=e^{-b_{0}\mathrm{d}_{\Omega}(x,B_{\delta_{0}}(y_{i}))}\) for each \(i\in[a]\). For a given \(x\in\Omega\), we compute \(h:\Omega\rightarrow\mathbb{R}^{a}\) as

\[h(x)=[h_{1}(x),\ldots,h_{a}(x)]\]

Then we define the encoding function \(\mathbf{h}:\mathcal{X}\rightarrow\mathbb{R}^{a}\) as

\[\mathbf{h}(S)=\sum_{i=1}^{n}w_{i}\frac{h(x_{i})}{\|h(x_{i})\|_{1}}\]

Note that \(\|\mathbf{h}(S)\|_{1}=1\) and \(\mathbf{h}\) is continuous since Wasserstein distances metrize weak convergence. Additionally, since \(\mathrm{d}_{\Omega}(x,B_{\delta_{0}}(y_{i}))\) is the distance from \(x\) to the \(\delta_{0}\)-ball around \(y_{i}\), we are guaranteed to have one \(j\) where \(h_{j}(x_{i})=1\) so \(\|h(x_{i})\|_{1}>1\).

Now, we define a decoding function \(g:\mathbb{R}^{a}\rightarrow\mathcal{X}\) as \(g(v)=\{(y_{i},\frac{v_{i}}{\|v\|_{1}}):i\in[a]\}\). In order to show that \(g\) and \(\mathbf{h}\) yields a valid \((\delta,a,G)\)-sketch of \(\mathcal{X}\), we must show that \(g\circ\mathbf{h}(S)\) is sufficiently close to \(S\) under the \(\mathrm{W}_{p}\) distance. First, we know that

\[\mathrm{W}_{p}^{p}(g\circ\mathbf{h}(S),S)\leq\sum_{i=1}^{n}\sum_{j=1}^{a}w_{1} \frac{h_{j}(x_{i})}{\|h(x_{i})\|_{1}}d(x_{i},y_{j})^{p}.\]

Let \(d_{max}\) be the diameter of \(\Omega\). For a given \(x_{i}\), we can partition \(\{h_{1}(x_{i}),\ldots,h_{a}(x_{i})\}\) into those where \(h_{j}(x_{i})\geq\frac{\delta_{0}^{p}}{d_{max}^{p}a}\) and those where \(h_{j}(x_{i})<\frac{\delta_{0}^{p}}{d_{max}^{p}a}\) i.e. \(\{h_{j_{1}}(x_{i}),\ldots,h_{j_{k}}(x_{i})\}\) and \(\{h_{j_{k+1}}(x_{i}),\ldots,h_{j_{a}}(x_{i})\}\) respectively. If \(h_{j}(x)\geq\frac{\delta_{0}^{p}}{d_{max}^{p}a}\), then

\[e^{-b_{0}\mathrm{d}_{\Omega}(x,B_{\delta_{0}}(y_{i}))}\geq\frac{\delta_{0}^{p} }{d_{max}^{p}\cdot a}>e^{-b_{0}\delta_{0}}\]so \(\mathrm{d}_{\Omega}(x,B_{\delta_{0}}(y_{i}))<\delta_{0}\). Then

\[\mathrm{W}_{p}^{p}(g\circ\mathbf{h}(S),S) \leq\sum_{i=1}^{n}\sum_{j=1}^{m}w_{i}\frac{h_{j}(x_{i})}{\|h(x_{i} )\|_{1}}\mathrm{d}_{\Omega}(x_{i},y_{j})^{p}.\] \[<\sum_{i=1}^{n}w_{i}\Big{(}\sum_{\ell=1}^{k}\frac{h_{j_{\ell}}(x_ {i})}{\|h(x_{i})\|_{1}}(2\delta_{0})^{p}+\sum_{\ell=k+1}^{a}\frac{\delta_{0}^{ p}}{d_{max}^{p}\cdot a}d_{max}^{p}\Big{)}\text{ since }\mathrm{d}_{\Omega}(x_{i},y_{j})\leq d_{max}\] \[\leq\sum_{i=1}^{n}w_{i}(2^{p}\delta_{0}^{p}+\delta_{0}^{p})\leq 2 ^{p}\Big{(}\sqrt[p]{\delta/2}\cdot\frac{1}{2}\Big{)}^{p}+\frac{1}{2^{p}}\cdot \frac{\delta}{2}<\frac{\delta}{2}+\frac{\delta}{2}=\delta\]

Thus, the encoding function \(\mathbf{h}\) and the decoding function \(g\) make up a \((\delta,a,G)\)-sketch. 

Note that the sketch outlined in Theorem 3.4 is a smooth version of a one-hot encoding. With Theorem 3.4 and Lemma 3.3, we will now give an explicit formulation of an \(\epsilon\)-approximation of \(f\) via sum-pooling of continuous functions.

**Corollary 3.5**: _Let \(\epsilon>0,\;(\Omega,d_{\Omega})\) be a compact metric space and let \(\mathcal{X}\) be the space of weighted point sets equipped with the \(p\)-Wasserstein, \(\mathrm{W}_{p}\). Suppose for any \(\delta,\;(\Omega,d_{\Omega})\) has covering number \(a(\delta)\). Then given a function \(f:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) that is uniformly continuous and permutation invariant, there is continuous functions \(h:\Omega\to\mathbb{R}^{a(\delta)}\), \(\phi:\mathbb{R}^{a(\delta)}\to\mathbb{R}^{a^{\prime}}\), and \(\rho:\mathbb{R}^{a^{\prime}}\to\mathbb{R}\), such that for any \(A,B\in\mathcal{X}\)_

\[\left|f(A,B)-\rho\Big{(}\Big{(}\sum_{(x,w_{x})\in A}w_{x}h(x)\Big{)}+\phi \Big{(}\sum_{(x,w_{x})\in B}w_{x}h(x)\Big{)}\Big{)}\right|<\epsilon\]

_where \(h,\phi\) and \(\rho\) are all continuous and \(a^{\prime}=4\cdot a(\delta)+1\)._

Due to Eqn. (1), instead of considering the function \(\mathbf{h}\) which takes a set of points \(S\in\mathcal{X}\) as input, we now only need to model the function \(h:\Omega\to\mathbb{R}^{a}\), which takes **a single point \(x\in S\)** as input. For simplicity, assume that the input metric space \((\Omega,d_{\Omega})\) is a compact set in some Euclidean space \(\mathbb{R}^{d}\). Note that in contrast to Lemma 3.3, each \(h\), \(\phi\) and \(\rho\) is simply a continuous function, and there is no further group invariance requirement. Furthermore, all the dimensions of the domain and range of these functions are bounded values that depend only on the covering number of \(\Omega\), the target additive error \(\epsilon\), and independent to the maximum size \(N\) of input points. We can use multilayer perceptrons (MLPs) \(h_{\theta_{1}}\), \(\phi_{\theta_{2}}\), and \(\rho_{\theta_{3}}\) in place of \(h\), \(\phi\) and \(\rho\) to approximate the desired function. Formally, we define the following family of neural networks:

\[\mathcal{N}_{\mathrm{ProductNet}}(A,B)=\rho_{\theta_{3}}\Big{(}\phi_{\theta_{2 }}\Big{(}\sum_{(x,w_{x})\in A}w_{x}h_{\theta_{1}}(x)\Big{)}+\phi_{\theta_{2}} \Big{(}\sum_{(x,w_{x})\in B}w_{x}h_{\theta_{1}}(x)\Big{)}\Big{)}. \tag{2}\]

In practice, we consider the input to the the neural network \(h_{\theta_{1}}\) to be a point \(x\in\Omega\) along with its weight \(w_{x}\). As per the discussions above, functions represented by \(\mathcal{N}_{\mathrm{ProductNet}}\) can universally approximate SFGI product functions on the space of point sets. See Figure 1 for an illustration of our universal architecture for approximating product functions on point sets. As \(p\)-Wasserstein distances, \(\mathrm{W}_{p}:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\), are uniformly continuous with respect to the underlying metric \(\mathrm{W}_{p}\), we can apply our framework for the problem of approximating \(p\)-Wasserstein.

Importantly, the number of parameters in \(\mathcal{N}_{\mathrm{ProductNet}}\) does not depend on the maximum size of the point set but rather only on the \(\epsilon\) additive error and by extension, the covering number of the original metric space. This is because the encoding function for our sketch is defined as the summation of single points into \(\mathbb{R}^{a}\) where \(a\) is independent of the size of the input set. Contrast this result with latent dimension in the universality statement of DeepSets (cf. Theorem 2.1), which is dependent on the input point set size. Note that in general, the model complexities of the MLPs \(\rho_{\theta_{3}}\), \(\phi_{\theta_{2}}\), and \(h_{\theta_{1}}\) depend on the dimensions of the domain and co-domain of each function they approximate (\(\rho\), \(\phi\) and \(h\)) and the desired approximation error \(\epsilon\). We assume that MLPs are Lipschitz continuous. In our case, \(\phi_{\theta_{2}}\) operates on the sum of \(h_{\theta_{1}}(x)\)s for all \(N\) number of input points in \(A\) (or in \(B\)). In general, the error made in \(h\)s may accumulate \(N\) times, which causes the precision we must achieve for each individual \(h_{\theta_{1}}(x)\) (compared to \(h(x)\)) to be \(\Theta(\epsilon/N)\). This would have caused the model complexity of \(h_{\theta_{1}}\) to depend on \(N\). Fortunately, this is not the case for our encodingfunction \(h\). In particular, because our encoding function can be written in the specific form of a normalized sum of individual points in the set \(S\); i.e, \(\phi_{\theta_{2}}\) operates on \(\sum_{(x,w_{x})\in S}w_{x}h_{\theta_{1}}(x)\) with \(\sum_{x}w_{x}=1\), the error accumulated by the normalized sum will be less than the maximum error from any single point \(h(x)\) for \(x\in S\). Thus, as both the error for each MLP and the dimension of the domain and co-domain of each approximated function \((\rho,\phi,h)\) do not depend on the size of the input point set \(N\), we get that \(\rho_{\theta_{3}}\), \(\phi_{\theta_{2}}\) and \(h_{\theta_{1}}\) each have model complexity independent of the size of the input point set. In short, because the encoding and decoding functions of our sketch can be approximated with neural networks with model complexity _independent_ of input point set size, we are able to achieve a low model complexity neural network which can approximate Wasserstein distance arbitrarily well. Note that in general, we are not guaranteed to be able to find a sketch which can be approximated by neural networks which are independent of input size. Finally, this neural network architecture can also be applied to approximating Hausdorff distance. More details regarding Hausdorff distance approximation are available in Appendix A. We summarize the above results in the following corollary.

**Corollary 3.6**: _There is a network in the form of \(\mathcal{N}_{\mathrm{ProductNet}}\) which can approximate the \(p\)-Wasserstein distance between point sets to within an additive \(\epsilon\)-error. The number of parameters in this network depends on \(\epsilon\) and the covering number of \(\Omega\) and not on the size of each point set._

_Additionally, if we replace the sum-pooling with \(\max\) in \(\mathcal{N}_{\mathrm{ProductNet}}\), there is a network of such a form where we can also approximate Hausdorff distance between point sets to additive \(\epsilon\) accuracy._

Exponential dependence on dimensionAlthough the model complexity of \(\mathcal{N}_{\mathrm{ProductNet}}\) is independent of the size of the input point set, it depends on the covering number of \(\Omega\) which, in turn, can have an exponential dependence on the dimension of \(\Omega\). In short, this means that the model complexity of \(\mathcal{N}_{\mathrm{ProductNet}}\) has an exponential dependence on the dimension of \(\Omega\). However, in practice, many machine learning tasks (e.g. 3D point processing) involve large point sets sampled from low-dimensional space (\(d=3\)). Furthermore, in general, the covering number of \(\Omega\) will depend on the _intrinsic dimension_ of \(\Omega\) rather than the _ambient dimension_. For instance, if the input point sets are sampled from a hidden manifold of dimension \(d^{\prime}\) (where \(d^{\prime}\) which is much lower than the ambient dimension \(d\)), then the covering number would depend only on \(d^{\prime}\) and the curvature bound of the manifold. In many modern machine learning applications, it is often assumed that the data is sampled from a hidden space of low dimension (the manifold hypothesis) although the ambient dimension might be very high.

Using max-pooling instead of sum-pooling.Observe that in the final step of combining the Euclidean outputs for two point sets \(A,B\in\mathcal{X}\), \(\sum_{(x,w_{x})\in A}w_{x}h_{\theta_{1}}(x)+\sum_{(x,w_{x})\in B}w_{x}h_{ \theta_{1}}(x)\), we use the sum of these two components (as in a DeepSet architecture) : \(\sum_{(x,w_{x})\in A}w_{x}h_{\theta_{1}}(x)\) and \(\sum_{(x,w_{x})\in B}w_{x}h_{\theta_{1}}(x)\), to ensure the symmetric condition of SFGI product functions. One could replace this final sum with a final max such as in PointNet. However, to show that PointNets are able to universally approximate continuous functions \(F:K\rightarrow\mathbb{R}\) where \(K\subseteq\mathbb{R}^{a\times 2}\) is compact, we need

Figure 1: Visually representing a neural network which can universally approximate uniformly continuous SFGI product functions over pairs of point sets.

[MISSING_PAGE_FAIL:9]

search with respect to the parameters of each model. Finally, we use the ModelNet40 [31] dataset which consists of point clouds in \(\mathbb{R}^{3}\) and a gene expression dataset (RNAseq) which consists of 4360 cells each represented by 2000 genes (i.e. 4360 points in \(\mathbb{R}^{2000}\)) as well as three synthetic datasets: (1) uniform, where point sets are in \(\mathbb{R}^{2}\), (2) noisy-sphere-3, where point sets are in \(\mathbb{R}^{3}\), (3) noisy-sphere-6, where point sets are in \(\mathbb{R}^{6}\). The RNAseq dataset is publicly available courtesy of the Allen institute [32]. Additional details and experiments approximating the 2-Wasserstein distance are available in Appendix D.

Approximating Wasserstein distances.Our results comparing 1-Wasserstein distance approximations are summarized in Table 1. Additionally, see Table 3 for a summary of time needed for training. For most datasets, \(\mathcal{N}_{\mathrm{ProductNet}}\) produces more accurate approximations of Wasserstein distances for both input point sets seen at training time and for input point sets unseen at training time. For the high dimensional RNAseq dataset, our approximation remains accurate in comparison with other methods, including the standard Sinkhorn approximation for input point sets seen at training time. The only exception is ModelNet-small, where \(\mathcal{N}_{\mathrm{ProductNet}}\) approximation error is slightly larger than WPCE for input point set sizes using during training (top row for each dataset in Table 1). However, for point sets where the input sizes were not used during training (bottom row for each dataset in Table 1), \(\mathcal{N}_{\mathrm{ProductNet}}\) showed significantly lower error than all other methods including WPCE. These results along with a more detailed plot in Figure 2 in Appendix D indicate that \(\mathcal{N}_{\mathrm{ProductNet}}\) generalizes better than WPCE to point sets of input sizes unseen at training time. Also, see Appendix D for additional discussion about generalization. Furthermore, one major advantage of \(\mathcal{N}_{\mathrm{ProductNet}}\) over WPCE is the dramatically reduced time needed for training (cf. Table 2). This substantial difference in training time is due to WPCE's usage of the Sinkhorn reconstruction loss as the \(O(n^{2})\) computation time for the Sinkhorn distance can be prohibitively expensive as input point set sizes grow. Thus, our results indicate that, compared to WPCE, \(\mathcal{N}_{\mathrm{ProductNet}}\) can reduce training time while still achieving comparable or better quality approximations of Wasserstein distance. Using our \(\mathcal{N}_{\mathrm{ProductNet}}\), we can produce high quality approximations of 1-Wasserstein distance while avoiding the extra cost associated with using an autoencoder architecture and Sinkhorn regularization. Finally, all models produce much faster approximations than the Sinkhorn distance (see Tables 3 and 6 in Appendix D). In summary, as compared to WPCE, our model is more accurate in approximating both 1-Wasserstein distance, generalizes better to larger input point set sizes, and is more efficient in terms of training time.

## 5 Concluding Remarks

Our work presents a general neural network framework for approximating SFGI functions which can be combined with geometric sketching ideas into a specific and efficient neural network for approximating \(p\)-Wasserstein distances. We intend to utilize \(\mathcal{N}_{\mathrm{ProductNet}}\) as an **accurate, efficient, and differentiable** approximation for Wasserstein distance in downstream machine learning tasks where Wasserstein distance is employed, such as loss functions for aligning single cell multi-omics data [8] or compressing energy profiles in high energy particle colliders [10; 26]. Beyond Wasserstein distance, we will look to apply our framework to a wide array of geometric problems that can be considered SFGI functions and are desireable to approximate via neural networks. For instance, consider the problems of computing the optimal Wasserstein distance under rigid transformation or the Gromov-Wasserstein distance, which both can be represented as an SFGI function where the factor-wise group invariances include both permutation and rigid transformations. Then our sketch must be invariant to both permutations and orthogonal group actions on the left. It remains to be seen if there is a neural network architecture which can approximate such an SFGI function to within an arbitrary additive \(\epsilon\)-error where the complexity does not depend on the maximum size of the input set.

## 6 Acknowledgements

The authors thank anonymous reviewers for their helpful comments. Furthermore, the authors thank Rohan Gala and the Allen Institute for generously providing the RNAseq dataset. Finally, Samantha Chen would like to thank Puoya Tabaghi for many helpful discussions about permutation invariant neural networks and Tristan Brugere for his implementation of the Sinkhorn algorithm. This research is supported by National Science Foundation (NSF) grants CCF-2112665 and CCF-2217058 and National Institutes of Health (NIH) grant RF1 MH125317.

## References

* [1] T. Amir, S. J. Gortler, I. Avni, R. Ravina, and N. Dym. Neural injective functions for multisets, measures and graphs via a finite witness theorem. _arXiv preprint arXiv:2306.06529_, 2023.
* [2] A. Backurs, Y. Dong, P. Indyk, I. Razenshteyn, and T. Wagner. Scalable nearest neighbor search for optimal transport. In _International Conference on machine learning_, pages 497-506. PMLR, 2020.
* [3] P. Bongini, M. Bianchini, and F. Scarselli. Molecular generative graph neural networks for drug discovery. _Neurocomputing_, 450:242-252, 2021.
* [4] C. Bueno and A. Hylton. On the representation power of set pooling networks. _Advances in Neural Information Processing Systems_, 34:17170-17182, 2021.
* [5] N. Courty, R. Flamary, and M. Ducoffe. Learning wasserstein embeddings. _arXiv preprint arXiv:1710.07457_, 2017.
* [6] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. _Advances in neural information processing systems_, 26, 2013.
* [7] G. Cybenko. Approximation by superpositions of a sigmoidal function. _Mathematics of control, signals and systems_, 2(4):303-314, 1989.
* [8] P. Demetci, R. Santorella, B. Sandstede, W. S. Noble, and R. Singh. Gromov-wasserstein optimal transport to align single-cell multi-omics data. _BioRxiv_, pages 2020-04, 2020.
* [9] C. Deng, O. Litany, Y. Duan, A. Poulenard, A. Tagliasacchi, and L. J. Guibas. Vector neurons: A general framework for so (3)-equivariant networks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 12200-12209, 2021.
* [10] G. Di Guglielmo, F. Fahim, C. Herwig, M. B. Valentin, J. Duarte, C. Gingu, P. Harris, J. Hirschauer, M. Kwok, V. Loncar, et al. A reconfigurable neural network asic for detector front-end data compression at the hl-lhc. _IEEE Transactions on Nuclear Science_, 68(8):2179-2186, 2021.
* [11] W. Du, H. Zhang, Y. Du, Q. Meng, W. Chen, N. Zheng, B. Shao, and T.-Y. Liu. Se (3) equivariant graph neural networks with complete local frames. In _International Conference on Machine Learning_, pages 5583-5608. PMLR, 2022.
* [12] N. Dym and S. J. Gortler. Low dimensional invariant embeddings for universal geometric learning. _arXiv preprint arXiv:2205.02956_, 2022.
* [13] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In _International conference on machine learning_, pages 1263-1272. PMLR, 2017.
* [14] P. Indyk and N. Thaper. Fast image retrieval via embeddings. In _3rd international workshop on statistical and computational theories of vision_, volume 2, page 5, 2003.
* [15] K. Kawano, S. Koide, and T. Kutsuna. Learning wasserstein isometric embedding for point clouds. In _2020 International Conference on 3D Vision (3DV)_, pages 473-482. IEEE, 2020.
* [16] N. Keriven and G. Peyre. Universal invariant and equivariant graph neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* [17] D. Lim, J. Robinson, L. Zhao, T. Smidt, S. Sra, H. Maron, and S. Jegelka. Sign and basis invariant networks for spectral graph representation learning. _arXiv preprint arXiv:2202.13013_, 2022.
* [18] A. Makkuva, A. Taghvaei, S. Oh, and J. Lee. Optimal transport mapping via input convex neural networks. In _International Conference on Machine Learning_, pages 6672-6681. PMLR, 2020.

* [19] H. Maron, H. Ben-Hamu, N. Shamir, and Y. Lipman. Invariant and equivariant graph networks. _arXiv preprint arXiv:1812.09902_, 2018.
* [20] H. Maron, E. Fetaya, N. Segol, and Y. Lipman. On the universality of invariant networks. In _International conference on machine learning_, pages 4363-4371. PMLR, 2019.
* [21] H. Maron, O. Litany, G. Chechik, and E. Fetaya. On learning sets of symmetric elements. In _International conference on machine learning_, pages 6734-6744. PMLR, 2020.
* [22] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 652-660, 2017.
* [23] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. _IEEE transactions on neural networks_, 20(1):61-80, 2008.
* [24] B. Schmitzer. A sparse multiscale algorithm for dense optimal transport. _Journal of Mathematical Imaging and Vision_, 56(2):238-259, 2016.
* [25] N. Segol and Y. Lipman. On universal equivariant set networks. _arXiv preprint arXiv:1910.02421_, 2019.
* [26] R. Shenoy and The CMS Collaboration Team. EMD Neural Network for HGCAL Data Compression Encoder ASIC. In _APS April Meeting Abstracts_, volume 2022 of _APS Meeting Abstracts_, page Q09.008, Jan. 2022.
* [27] A. Taghvaei and A. Jalali. 2-wasserstein approximation via restricted convex potentials with application to improved training for gans. _arXiv preprint arXiv:1902.07197_, 2019.
* [28] M. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, and S.-K. Yeung. Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1588-1597, 2019.
* [29] E. Wagstaff, F. Fuchs, M. Engelcke, I. Posner, and M. A. Osborne. On the limitations of representing functions on sets. In _International Conference on Machine Learning_, pages 6487-6494. PMLR, 2019.
* [30] E. Wagstaff, F. B. Fuchs, M. Engelcke, M. A. Osborne, and I. Posner. Universal approximation of functions on sets. _Journal of Machine Learning Research_, 23(151):1-56, 2022.
* [31] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1912-1920, 2015.
* [32] Z. Yao, C. T. van Velthoven, T. N. Nguyen, J. Goldy, A. E. Sedeno-Cortes, F. Baftizadeh, D. Bertagnolli, T. Casper, M. Chiang, K. Crichton, et al. A taxonomy of transcriptomic cell types across the isocortex and hippocampal formation. _Cell_, 184(12):3222-3241, 2021.
* [33] D. Yarotsky. Universal approximations of invariant maps by neural networks. _Constructive Approximation_, 55(1):407-474, 2022.
* [34] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola. Deep sets. _Advances in neural information processing systems_, 30, 2017.