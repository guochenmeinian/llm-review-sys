# Gaussian Process Conjoint Analysis for Adaptive Marginal Effect Estimation

Yehu Chen, Jacob Montgomery, Roman Garnett

Washington University in St Louis

chenyehu,jacob.montgomery,garnett@wustl.edu

###### Abstract

Choice-based conjoint analysis is an essential tool for learning the marginal effects of multidimensional explanatory features on preferences. However, existing marginal effect models rely either on non-parametric estimators that generalize poorly to individualized effects or on linear latent utility models that completely ignore possible higher-order interactions. We introduce Gaussian Process Conjoint Analysis (gpca) for learning marginal effects from observed choices as the first-order derivatives of unknown systems. Additionally, we propose a Gaussian mixture approximation for the predictive distributions of marginal effects, which facilitates downstream tasks such as adaptive experimentation. Our experiments show that gpca achieves more precise and efficient estimation of marginal effects.

## 1 Introduction

Understanding the relationship between targeted outcomes and features in survey experiments is fundamental in many disciplines, including social science [1, 2, 3], human-computer interaction [4, 5], and marketing research [6, 7, 8]. These associations are often captured by marginal effects, defined as the change in predicted outcomes resulting from changes in features. Depending on the type of attributes, marginal effects can either be computed as the discrete change in outcomes for categorical attributes or as infinitesimal margins for continuous attributes. In survey experiments, marginal effects are often estimated using choice-based conjoint experiments, which present a series of profile pairs with varying attribute values to compare differences in averaged outcomes [6]. For example, researchers might alternate background characteristics to study bias toward immigrants, or system designers might adjust interface setups to improve click-through rates for a new web interface. However, the multidimensional nature of conjoint experiments makes the estimation of heterogeneous marginal effects challenging and not scalable due to interactions with other attributes and small-sample biases. Existing estimators either rely on stacking multiple attributes in a difference-in-differences style [1, 2] or on linear utility theory, which overlooks possible feature expansions [8, 9, 10, 11].

We propose Gaussian Process Conjoint Analysis (gpca), which automatically learns higher-order interactions within the preference learning framework. Marginal effects are derived using the first-order derivatives of a Gaussian Process trained on observed preferences, and their distributions are approximated via Gaussian mixture models. Additionally, we leverage adaptive experimentation for data collection, balancing between exploiting attributes that are more crucial to preferences and exploring attributes where the model exhibits uncertainty, guided by a predictive belief model of the system. As demonstrated in simulated experiments, gpca achieves more precise estimation of marginal effects compared to other non-parametric and parametric methods.

Related work

**Marginal effects** are a common quantitative method for understanding transformed features in regression models [12] or examining heterogeneous associations between features and outcomes [13]. For instance, marginal effects have been extensively studied in economics to measure the responsiveness of economic variables through the concept of elasticity [14], which quantifies how a percentage change in price corresponds to a percentage change in demand quantity. Marginal effects have also been utilized to enhance the interpretability of machine learning models. Silva Filho et al. [15] proposed a feature importance method for interpreting classification models based on marginal local effects. Merz et al. [16] introduced a marginal attribution method that conditions on quantiles to analyze global gradients in deep neural networks. Scholbeck et al. [17] presented forward marginal effects as a unified, mixed-type feature interpretation method for non-linear machine learning models.

**Conjoint analysis** is an experimental design used to elicit user preferences in recommendation systems [10; 18] and has been widely adopted in quantitative research to learn marginal effects by randomizing over attribute profiles [1; 2]. Existing estimators are predominantly non-parametric and primarily focus on discrete attributes. Hainmueller et al. [1] proposed a difference-in-differences interaction effect estimator to infer preferences from multidimensional choices in survey experiments. Similarly, Egami and Imai [2] introduced a new effect estimator for factorial experiments that does not rely on baseline conditions and generalizes effectively to higher-order interaction effects. In contrast, conjoint analysis involving continuous attributes often employs parametric latent utility functions, such as generalized logistic regression [10], support vector machines [9; 8; 11], Gaussian Processes [19; 20; 21; 22], and decision trees [17]. However, these methods are primarily designed for profile recommendation rather than marginal effect estimation.

**Adaptive experimentation** leverages previously collected responses to inform experiment design or data acquisition in subsequent iterations, aiming to maximize the utility of limited data. This approach has been widely adopted by domain scientists to accelerate scientific discovery. For example, Bayesian optimization through adaptive sample selection has been successfully applied in materials science for discovering new materials [23] and in clinical trials for determining the maximum tolerated dose [24; 25]. Similarly, active search has been utilized for the iterative design of virtual screening trials in chemoinformatics [26]. In machine learning, Chen et al. [27] explored the pairwise ranking problem in a crowdsourcing setup using online learning. Biyik et al. [28] proposed an active, preference-based learning method based on information gain to optimize reward functions in robotics. However, prior adaptive designs in quantitative research have predominantly focused on treatment selection in bandit settings [29; 30; 31], with limited attention to marginal effect estimation, particularly within the gp preference learning framework.

## 3 Problem statement

We describe notations and define marginal effects in the setup of Gaussian process conjoint analysis, which are computed as the gradients of the preference probabilities w.r.t the profiles.

**Notations.** Formally, let \(\mathbf{x}\in\mathbb{R}^{d}\) represent the full profile of \(d\)-dimensional attributes, where \(\mathbf{x}l\) denotes the \(l\)th attribute and \(\mathbf{x}-l\) represents all attributes except the \(l\)th. For pairwise comparisons, let \(y_{ij}\in 0,1\) indicate whether the left-side profile \(\mathbf{x}^{(i)}\) is preferred over the right-side profile \(\mathbf{x}^{(j)}\), where \(y_{ij}=1\) if \(\mathbf{x}^{(i)}\succ\mathbf{x}^{(j)}\) and \(y_{ij}=0\) otherwise. We focus on choice-based conjoint analysis with pairwise comparisons, as scenarios involving multiple choices can be decomposed into multiple pairwise comparisons. For example, if \(\mathbf{x}^{(i)}\) is the most preferred option among \(\{\mathbf{x}^{(i)},\mathbf{x}^{(j)},\mathbf{x}^{(k)}\}\), this can be expressed as \(\mathbf{x}^{(i)}\succ\mathbf{x}^{(j)}\) and \(\mathbf{x}^{(i)}\succ\mathbf{x}^{(k)}\). Similarly, our notation accommodates score-based conjoint experiments, where \(\mathbf{x}^{(i)}\succ\mathbf{x}^{(j)}\) may indicate that \(\mathbf{x}^{(i)}\) has a higher score than \(\mathbf{x}^{(j)}\). Finally, suppose all observed preferences are collected into the dataset \(\mathcal{D}=\{(\mathbf{x}^{(i)},\mathbf{x}^{(j)}),y_{ij}\}\).

**Gaussian process conjoint analysis.** Conjoint analysis can be formulated as a preference learning problem involving a latent utility function \(u(\mathbf{x})\). The preferential relation between \(\mathbf{x}^{(i)}\) and \(\mathbf{x}^{(j)}\) is determined by comparing their utilities \(u(\mathbf{x}^{(i)})\) and \(u(\mathbf{x}^{(j)})\). Using a sigmoid probabilistic model \(\sigma(\cdot)\), the probability of observed the preference \(p(\mathbf{x}^{(i)}\succ\mathbf{x}^{(j)})\) is given by \(\sigma\big{(}u(\mathbf{x}^{(i)})-u(\mathbf{x}^{(j)})\big{)}\), allowing for potential labeling errors. Gaussian process conjoint analysis (gpca) assumes a gp prior on the latent utility \(u(\mathbf{x})\sim\mathcal{GP}(0,K)\), where \(K(x,x^{\prime})=\exp(-\|x-x^{\prime}\|^{2}/2)\) is an RBF kernel. The observation model uses a cumulative standard normal function, with \(p\big{(}\mathbf{x}^{(i)}\succ\mathbf{x}^{(j)}\mid u(\mathbf{x}^{(i)}),u(\mathbf{x}^ {(j)})\big{)}=\Phi\big{(}u(\mathbf{x}^{(i)})-u(\mathbf{x}^{(j)})\big{)}\). Although the posterior of \(u(\mathbf{x})\) is not analytical in this classification setting, it can be approximated using standard methods such as Laplace approximation or expectation propagation [32, 18]. In some cases, the predictive probability is defined directly on pairs of profiles \((\mathbf{x}^{(i)},\mathbf{x}^{(j)})\) using the preference kernel \(K_{\text{pref}}\big{(}(\mathbf{x}_{1}^{(i)},\mathbf{x}_{1}^{(j)}),(\mathbf{x}_ {2}^{(i)},\mathbf{x}_{2}^{(j)})\big{)}=K(\mathbf{x}_{1}^{(i)},\mathbf{x}_{2}^ {(i)})-K(\mathbf{x}_{1}^{(i)},\mathbf{x}_{2}^{(j)})-K(\mathbf{x}_{2}^{(i)}, \mathbf{x}_{1}^{(j)})+K(\mathbf{x}_{1}^{(j)},\mathbf{x}_{2}^{(j)})\). We adopted this preference kernel in our implementation of gpca.

**Marginal effects in gpca**. We consider the _marginal_ effects of profile pairs \((\mathbf{x}^{(i)},\mathbf{x}^{(j)})\) as the first-order gradients of the preference probability w.r.t both profiles. Leveraging the affine property of Gaussian processes, the gradient \(\pi\big{(}(\mathbf{x}^{(i)},\mathbf{x}^{(j)})\big{)}\) of the probability of target profile \(\mathbf{x}^{(i)}\) being preferred to the opponent profile \(\mathbf{x}^{(j)}\) is (see Appendix for full mathematical details)

\[\pi\big{(}(\mathbf{x}^{(i)},\mathbf{x}^{(j)})\big{)}=\mathbb{E}_{u|\mathcal{D} }\Big{[}\phi\big{(}u(\mathbf{x}^{(i)})-u(\mathbf{x}^{(j)})\big{)}\big{(}\nabla u (\mathbf{x}^{(i)}),-\nabla u(\mathbf{x}^{(j)})\big{)}\Big{]} \tag{1}\]

Intuitively, marginal effects in the outcome space are computed as the expected gradient \(\big{(}\nabla u(\mathbf{x}^{(i)}),-\nabla u(\mathbf{x}^{(j)})\big{)}\) in the latent utility space, weighted by the densities \(\phi\big{(}u(\mathbf{x}^{(i)})-u(\mathbf{x}^{(j)})\big{)}\) of a normal distribution at the latent utility difference \(u(\mathbf{x}^{(i)})-u(\mathbf{x}^{(j)})\). When projected along any unit vector \(\hat{\mathbf{e}}_{l}\), the averaged attribute-specific effect for attribute \(l\) on preferences over the profile distribution \(\mathcal{P}\) can be further expressed as \(\pi_{l}(\mathbf{x}_{l}^{(i)})=\sum_{(\mathbf{x}_{-l}^{(i)},\mathbf{x}^{(j)}) \sim\mathcal{P}}\langle\pi\big{(}(\mathbf{x}^{(i)},\mathbf{x}^{(j)})\big{)}, \hat{\mathbf{e}}_{l}\rangle\).

## 4 Our approach: GMM estimator and adaptive experimentation

In this section, we describe two key elements of our approach: 1) a Gaussian Mixture Model (gmm) approximation to compute \(\pi\big{(}(\mathbf{x}^{(i)},\mathbf{x}^{(j)})\big{)}\), and 2) an adaptive experimentation strategy that efficiently selects profile pairs for comparison based on uncertainty in the latent utility or preference prediction.

**Gaussian mixture approximation of marginal effects.** Since \(\pi\big{(}(\mathbf{x}^{(i)},\mathbf{x}^{(j)})\big{)}\) involves taking weighted averages of \(\nabla u(\mathbf{x})\) over \(u(\cdot)\), we propose the use of Gaussian mixture model (gmm) for approximation. Each component of the gmm is formed by scaling the multivariate Gaussian with the transformed values of quadrature points of the univariate Gaussian determined by Gauss-Hermite quadrature. Formally, let \(N\) be the number of points in the quadrature, \(k_{r}\) be the roots of the physicists' version of the Hermite polynomial \(H_{N}(k)\) and \(\omega_{r}=\frac{2^{N-1}N_{1}^{1}}{N^{2}[H_{N-1}(k_{r})]^{2}}\) be the weights of each component [33]. Notice that the gradient of a differentiable gp is itself a gp, so one can first derive a joint posterior of utility function \(u(\cdot)\) and its gradient \(\nabla u(\mathbf{x})\) from the induced prior \(\begin{bmatrix}u\\ \nabla u\end{bmatrix}\sim\mathcal{GP}\big{(}\begin{bmatrix}\mu_{u}\\ \nabla\mu_{u}\end{bmatrix},\begin{bmatrix}K_{u}&\nabla K_{u}^{T}\\ \nabla K_{u}&\nabla^{2}K_{u}\end{bmatrix}\big{)}\) by conditioning on the preference observations. Under this joint posterior, \(\pi\big{(}\big{(}\mathbf{x},\mathbf{x}^{(j)})\big{)}\) can then be approximated as \(\sum\limits_{r=1}^{N}\omega_{r}\phi\big{(}\bar{f}_{r}(\mathbf{x})\big{)}\circ \mathcal{N}\Big{(}\nabla\mu_{u}(\mathbf{x}),\nabla^{2}K_{u}(\mathbf{x}, \mathbf{x})\Big{)}\). Here \(\bar{f}_{r}(\mathbf{x})=\sqrt{2}[\sigma_{u}^{2}(\mathbf{x})+\sigma_{u}^{2}( \mathbf{x}^{(j)})]^{1/2}k_{r}+[\mu_{u}(\mathbf{x})-\mu_{u}(\mathbf{x}^{(j)})]\) are locations of mixture components defined on the sample point \(k_{r}\)s, and \(\circ\) denotes the Hadamard (element-wise) product. Figure 1 shows the visualization of the proposed gmm for approximating one-side marginal effect. The left-hand side shows our gmm approximation of the one-sided marginal effect using 5 sampling points, and the

Figure 1: Visualization of the proposed gmm for approximating one-side marginal effect. Left figure shows our gmm approximation of the one-side marginal effect using 5 sampling points, and right figure shows 9 possible true effects obtained by numerical sampling. Darker colors indicate components with higher weights in the gmm and numerical samples closer to the one-side marginal effect posterior mode.

right-hand side shows 9 possible true effects obtained by numerical sampling. Darker colors indicate components with higher weights in the gmm and numerical samples closer to the one-side marginal effect posterior mode. We found \(N=10\) quadrature points sufficient for our gmm.

**Adaptive experimentation in gpca.** Informed by the posterior belief on the latent utility, adaptive experimentation may efficiently explore attributes whose marginal effects on preferences are less certain. Hence, we determine the next pairs of profiles to compare by maximizing an _acquisition function_\((\mathbf{x}_{*}^{(i)},\mathbf{x}_{*}^{(j)})=\max_{(\mathbf{x}^{(i)},\mathbf{x} ^{(j)})\sim\mathbb{P}}\alpha\big{(}(\mathbf{x}^{(i)},\mathbf{x}^{(j)});\mathcal{ D}\big{)}\). For simplicity, let \(A=u(\mathbf{x}^{(i)})-u(\mathbf{x}^{(j)})\) and \(B=K_{u|\mathcal{D}}(\mathbf{x}^{(i)},\mathbf{x}^{(i)})+K_{u|\mathcal{D}}( \mathbf{x}^{(j)},\mathbf{x}^{(j)})\). We consider the following policies: (1) _Upper confident bound_ (ucb) maximizes the 95% confidence interval of preference prediction: \(\alpha\big{(}(\mathbf{x}^{(i)},\mathbf{x}^{(j)});\mathcal{D}\big{)}=\big{|}A +1.96\sqrt{B}\big{|}\); (2) _Differential entropy of the latent utility_ (de-u) maximizes the log variance of utility posterior: \(\alpha\big{(}(\mathbf{x}^{(i)},\mathbf{x}^{(j)});\mathcal{D}\big{)}=\frac{1}{ 2}\log(2\pi B)+\frac{1}{2}\); (3) _Differential entropy of the marginal effects_ (de-me) maximizes their log variance: \(\alpha\big{(}(\mathbf{x}^{(i)},\mathbf{x}^{(j)});\mathcal{D}\big{)}=\log\Big{|} \sum\limits_{k\in\{i,j\}}\sum\limits_{r=1}^{N}\omega_{r}\phi\big{(}\bar{f}_{r}( \mathbf{x}^{(k)})\big{)}\phi\big{(}\bar{f}_{r}(\mathbf{x}^{(k)})\big{)}^{T} \circ\nabla^{2}K_{u|\mathcal{D}}(\mathbf{x}^{(k)},\mathbf{x}^{(k)})\Big{|}\); (4) _Bayesian active learning by disagreement_ (bald) maximizes the mutual information between utility and predictive preferences: \(\alpha\big{(}(\mathbf{x}^{(i)},\mathbf{x}^{(j)});\mathcal{D}\big{)}=\mathbf{I}( y_{ij},u;\mathbf{x}^{(i)},\mathbf{x}^{(j)},\mathcal{D})\approx h\Big{(}\Phi\big{(} \frac{A}{\sqrt{B+1}}\big{)}\Big{)}-\frac{C}{\sqrt{B+C^{2}}}\exp\big{(}-\frac{A ^{2}}{2(B+C^{2})}\big{)}\), with entropy function \(h(p)=-p\log(p)-(1-p)\log(1-p)\) and constant \(C=\sqrt{\pi\log(2)/2}\). While ucb emphasizes _exploiting_ current belief to find the most preferred profile, de-u, de-me and bald focus on _exploring_ the profile space by reducing uncertainty on marginal effects and predictive preferences.

## 5 Experimental results

Our evaluation on gpca are based on synthetic data where the functional relations are known analytically. Experiments of two real-world datasets can be found in appendix.

**Data generating process**. Following the simulation specification in Chu and Ghahramani [18], we consider two datasets with discrete (2dplane) and continuous (friedman) attributes.1 The 2dplane dataset has \(5\) discrete attributes where \(x_{1}\in\{-1,1\}\) and \(x_{2},\ldots,x_{5}\in\{-1,0,1\}\), with a piecewise linear utility \(u(\mathbf{x})=1+2x_{2}-x_{3}\) if \(x_{1}=-1\) and \(u(\mathbf{x})=1+x_{4}-2x_{5}\) if \(x_{1}=1\). The friedman dataset has \(3\) continuous attributes where \(x_{1},\ldots,x_{3}\sim[0,1]\) with a non-linear utility \(u(\mathbf{x})=3\sin(\pi x_{1}x_{2})+6(x_{3}-0.5)^{2}\). We randomly sample \(1000\) pairs of profiles in each dataset and set \(y_{ij}=1\) with probability of \(\Phi\big{(}u(\mathbf{x}^{(i)}-u(\mathbf{x}^{(j)})\big{)}\) and \(y_{ij}=0\) otherwise. In adaptive experimentation, we initialize with the same \(25\) profile pairs, and updates model posterior every iteration. Both effects are estimated w.r.t the same target profile distribution to ensure comparability.

Footnote 1: See [https://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html](https://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html) for details.

**Evaluation metrics and baselines.** We consider three metrics: (1) the rmse of the estimated effects, (2) the correlation (cor) between the estimated effects and true effects, and (3) the log likelihood (ll) of the estimated effects. We also compare our proposed gmm approximation for marginal effects in gpca to several baselines: (1) the non-parametric diff-in-mean estimator (dim) [1], where the continuous attributes in friedman are first discretized by splitting into equally-spanned intervals, (2) the standard preference learning method with linear utility (lm-gmm) [9; 10; 8; 11], and (3) an ablated gpca method (gp-map) but with map estimation of marginal effects.

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline \multirow{2}{*}{**dataset**} & \multirow{2}{*}{**estimator**} & \multicolumn{3}{c}{Marginal effects} & \multicolumn{3}{c}{Component effects} \\ \cline{3-8}  & & & \multicolumn{1}{c}{RMSE} \(\downarrow\) & \multicolumn{1}{c}{cor} & \multicolumn{1}{c}{ll} & \multicolumn{1}{c}{rMSE} \(\downarrow\) & \multicolumn{1}{c}{cor} & \multicolumn{1}{c}{ll} \\ \hline \multirow{4}{*}{2dplane} & DIM & 0.712\(\pm\)0.022 & 0.013\(\pm\)0.003 & \(-\)2.137\(\pm\)0.115 & 0.109\(\pm\)0.005 & 0.341\(\pm\)0.029 & 0.494\(\pm\)0.117 \\  & lm-gmm & 0.213\(\pm\)0.001 & 0.340\(\pm\)0.005 & \(-\)0.238\(\pm\)0.145 & 0.069\(\pm\)0.002 & 0.475\(\pm\)0.019 & \(-\)0.778\(\pm\)0.157 \\  & gp-map & 0.175\(\pm\)0.002 & 0.732\(\pm\)0.007 & \(-\)3.993\(\pm\)0.

We then investigate adaptive experimentation in gpca for increasing efficiency of effect estimation. We consider various policies: (1) ucb popular in multi-arm bandit setting [34], (2) de-u and de-me for active learning based on differential entropy [35, 36, 37], (3) bald in Bayesian active learning for model uncertainty reduction [38] and (4) uniform design in non-parametric conjoint analysis [1, 2].

**Results.** Table 1 shows the averaged rmse, correlation and log likelihood across 25 different seeds of the proposed gp-gmm and baselines, where gp-gmm consistently has more precise effect estimation with lower rmse and higher cor/ll for both effects. Figure 2 shows box plots of averaged rmse, cor and ll of marginal effect estimation with adaptive experimentation under different acquisition policies. Sample size range from \(50\) to \(150\), and performance metrics are reported every other \(25\) acquisitions. Overall bald (blue) outperforms the rest of policies including uniform and ucb, indicating higher efficiency for effect estimation when the acquisition is designed to reduce model uncertainty. Morever, ucb (forest green) has overall the worst performance in estimating both marginal and component effects as it solely reinforces current belief on the probability of preference. Results for component effect estimation can be found in Appendix.

Besides estimation of marginal effects, we also examine the model quality of gpca by evaluating the prediction accuracy of unrevealed preferences among the not acquired profile pairs. Figure 3 shows the averaged accuracy and stds of preference prediction by various policies. With as few as \(50\) data points, gpca manages to predict at least \(80\%\) of the unrevealed preference and \(95\%\) when \(150\) data points are adaptively acquired by bald.

## 6 Conclusion

We introduce gpca, a Gaussian Process conjoint analysis model for estimating marginal effects in choice-based conjoint experiments with a Gaussian mixture approximation for their distributions that could enhance precision and efficiency in effect estimation aided by adaptive experimentation. Experiments show that gpca achieves more precise and efficient estimation of marginal effects, suggesting great potential in either machine learning interpretation or industrial applications.

Figure 3: Averaged accuracy and stds of preference prediction by various policies for simulated data.

Figure 2: Box plots of averaged rmse, cor and ll of marginal effects under different acquisition policies.

## References

* [1] Jens Hainmueller, Daniel J Hopkins, and Teppei Yamamoto. Causal Inference in Conjoint Analysis: Understanding Multidimensional Choices via Stated Preference Experiments. _Political Analysis_, 22(1):1-30, 2014.
* [2] Naoki Egami and Kosuke Imai. Causal Interaction in Factorial Experiments: Application to Conjoint Analysis. _Journal of the American Statistical Association_, 2018.
* [3] Thomas J Leeper, Sara B Hobolt, and James Tilley. Measuring Subgroup Preferences in Conjoint Experiments. _Political Analysis_, 28(2):207-221, 2020.
* [4] Dana Naous and Christine Legner. Leveraging Market Research Techniques in IS: A Review and Framework of Conjoint Analysis Studies in the IS Discipline. In _Proceedings of the 38th International Conference on Information Systems (ICIS 2017)_, 2017.
* [5] Eva-Maria Schomakers and Martina Ziefle. Privacy vs. Security: Trade-Offs in the Acceptance of Smart Technologies for Aging-in-Place. _International Journal of Human-Computer Interaction_, 39(5):1043-1058, 2023.
* [6] Paul E Green and Vithala R Rao. Conjoint Measurement for Quantifying Judgmental Data. _Journal of Marketing Research_, 8(3):355-363, 1971.
* [7] Paul E Green and Venkat Srinivasan. Conjoint Analysis in Marketing: New Developments with Implications for Research and Practice. _Journal of Marketing_, 54(4):3-19, 1990.
* [8] Theodoros Evgeniou, Constantinos Boussios, and Giorgos Zacharia. Generalized Robust Conjoint Estimation. _Marketing Science_, 24(3):415-429, 2005.
* [9] Sariel Har-Peled, Dan Roth, and Dav Zimak. Constraint Classification for Multiclass Classification and Ranking. _Advances in Neural Information Processing Systems_, 15, 2002.
* [10] Olivier Chapelle and Zaid Harchaoui. A Machine Learning Approach to Conjoint Analysis. _Advances in Neural Information Processing Systems_, 17, 2004.
* [11] Sebastian Maldonado, Ricardo Montoya, and Richard Weber. Advanced conjoint analysis using feature selection via support vector machines. _European Journal of Operational Research_, 241(2):564-574, 2015.
* [12] Daniel Ludecke. ggeffects: Tidy Data Frames of Marginal Effects from Regression Models. _Journal of Open Source Software_, 3(26):772, 2018.
* [13] Jens Hainmueller and Chad Hazlett. Kernel Regularized Least Squares: Reducing Misspecification Bias with a Flexible and Interpretable Machine Learning Approach. _Political Analysis_, 22(2):143-168, 2014.
* [14] Roy George Douglas Allen. _Mathematical Analysis For Economists_. MacMillan and Co., Ltd., 1938.
* [15] Rogerio Luiz Cardoso Silva Filho, Paulo Jorge Leitao Adeodato, and Kellyton dos Santos Brito. Interpreting Classification Models Using Feature Importance Based on Marginal Local Effects. In _Brazilian Conference on Intelligent Systems_, pages 484-497. Springer, 2021.
* [16] Michael Merz, Ronald Richman, Andreas Tsanakas, and Mario V Wuthrich. Interpreting deep learning models with marginal attribution by conditioning on quantiles. _Data Mining and Knowledge Discovery_, 36(4):1335-1370, 2022.
* [17] Christian A Scholbeck, Giuseppe Casalicchio, Christoph Molnar, Bernd Bischl, and Christian Heumann. Marginal effects for non-linear prediction functions. _Data Mining and Knowledge Discovery_, pages 1-46, 2024.
* [18] Wei Chu and Zoubin Ghahramani. Preference learning with Gaussian processes. In _Proceedings of the 22nd International Conference on Machine Learning_, pages 137-144, 2005.

* [19] Johannes Furnkranz and Eyke Hullermeier. Pairwise preference learning and ranking. In _Machine Learning: ECML 2003: 14th European Conference on Machine Learning, Cavtat-Dubrovnik, Croatia, September 22-26, 2003. Proceedings 14_, pages 145-156. Springer, 2003.
* [20] Brochu Eric, Nando Freitas, and Abhijeet Ghosh. Active Preference Learning with Discrete Choice Data. _Advances in Neural Information Processing Systems_, 20, 2007.
* [21] Shengbo Guo, Scott Sanner, and Edwin V Bonilla. Gaussian Process Preference Elicitation. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, _Advances in Neural Information Processing Systems_, volume 23. Curran Associates, Inc., 2010.
* [22] Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and Jose Hernandez-lobato. Collaborative Gaussian Processes for Preference Learning. _Advances in Neural Information Processing Systems_, 25, 2012.
* [23] Turab Lookman, Prasanna V Balachandran, Dezhen Xue, and Ruihao Yuan. Active learning in materials science with emphasis on adaptive sampling using uncertainties for targeted design. _npj Computational Materials_, 5(1):21, 2019.
* [24] Suyu Liu and Ying Yuan. Bayesian optimal interval designs for phase I clinical trials. _Journal of the Royal Statistical Society: Series C: Applied Statistics_, pages 507-523, 2015.
* [25] Jakob Richter, Tim Friede, and Jorg Rahnenfuhrer. Improving adaptive seamless designs through Bayesian optimization. _Biometrical Journal_, 64(5):948-963, 2022.
* [26] Roman Garnett, Thomas Gartner, Martin Vogt, and Jurgen Bajorath. Introducing the 'active search' method for iterative virtual screening. _Journal of Computer-Aided Molecular Design_, 29:305-314, 2015.
* [27] Xi Chen, Paul N Bennett, Kevyn Collins-Thompson, and Eric Horvitz. Pairwise ranking aggregation in a crowdsourced setting. In _Proceedings of the sixth ACM International Conference on Web Search and Data Mining_, pages 193-202, 2013.
* [28] Erdem Biyik, Nicolas Huynh, Mykel J Kochenderfer, and Dorsa Sadigh. Active Preference-Based Gaussian Process Regression for Reward Learning. _arXiv preprint arXiv:2005.02575_, 2020.
* [29] Maria Dimakopoulou, Zhengyuan Zhou, Susan Athey, and Guido Imbens. Estimation Considerations in Contextual Bandits. _arXiv preprint arXiv:1711.07077_, 2017.
* [30] Molly Offer-Westort, Alexander Coppock, and Donald P Green. Adaptive Experimental Design: Prospects and Applications in Political Science. _American Journal of Political Science_, 65(4):826-844, 2021.
* [31] Aurelien Bibaut, Maria Dimakopoulou, Nathan Kallus, Antoine Chambaz, and Mark van Der Laan. Post-Contextual-Bandit Inference. _Advances in Neural Information Processing Systems_, 34:28548-28559, 2021.
* [32] Carl Edward Rasmussen and Christopher K Williams. _Gaussian Processes for Machine Learning_. MIT Press, Cambridge, MA, 2006.
* [33] Milton Abramowitz and Irene A Stegun. _Handbook of Mathematical Functions: with Formulas, Graphs, and Mathematical Tables_, volume 55. US Government Printing Office, 1948.
* [34] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design. In _Proceedings of the 27th International Conference on International Conference on Machine Learning_, ICML'10, page 1015-1022, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077.
* [35] Shujin Sun, Ping Zhong, Huaitie Xiao, and Runsheng Wang. Active Learning With Gaussian Process Classifier for Hyperspectral Image Classification. _IEEE Transactions on Geoscience and Remote Sensing_, 53(4):1746-1760, 2014.

* [36] Jens Schreiter, Duy Nguyen-Tuong, Mona Eberts, Bastian Bischoff, Heiner Markert, and Marc Toussaint. Safe Exploration for Active Learning with Gaussian Processes. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part III 15_, pages 133-149. Springer, 2015.
* [37] Xiaowei Yue, Yuchen Wen, Jeffrey H Hunt, and Jianjun Shi. Active Learning for Gaussian Process Considering Uncertainties With Application to Shape Control of Composite Fuselage. _IEEE Transactions on Automation Science and Engineering_, 18(1):36-46, 2020.
* [38] Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and Mate Lengyel. Bayesian Active Learning for Classification and Preference Learning. _arXiv preprint arXiv:1112.5745_, 2011.
* [39] Jens Hainmueller and Daniel J Hopkins. The Hidden American Immigration Consensus: A Conjoint Analysis of Attitudes toward Immigrants. _American Journal of Political Science_, 59(3):529-548, 2015.

[MISSING_PAGE_FAIL:9]

where \(\nabla\mu_{u|\mathcal{D}}=\partial\mu_{u|\mathcal{D}}(\mathbf{x})/\partial\mathbf{x}\) is the first-order derivative of the posterior mean, \(\nabla K_{u|\mathcal{D}}=\partial K_{u|\mathcal{D}}(\mathbf{x},\mathbf{x}^{ \prime})/\partial\mathbf{x}\) is the first-order derivative of the posterior covariance and \(\nabla^{2}K_{u|\mathcal{D}}=\partial^{2}K_{u|\mathcal{D}}(\mathbf{x},\mathbf{x} ^{\prime})/\partial\mathbf{x}\partial\mathbf{x}^{\prime}\) is its second-order mixed derivatives. We use a Gaussian mixture model (gmm) to approximate \(\mathbf{g}(\mathbf{x};\mathbf{x}^{(j)},\mathcal{D})\). Each component of the gmm is formed by scaling the multivariate Gaussian with the transformed values of quadrature points of the univariate Gaussian determined by Gauss-Hermite quadrature. Let \(N\) be the number of points in the quadrature, \(k_{r}\) be the roots of the physicists' version of the Hermite polynomial \(H_{N}(k)\) and \(\omega_{r}=\frac{2^{N-1}N!}{N^{2}[H_{N-1}(k_{r})]^{2}}\) be the weights of each component [33]. We could then approximate \(\mathbf{g}(\mathbf{x};\mathbf{x}^{(j)},\mathcal{D})\) as:

\[\mathbf{g}(\mathbf{x};\mathbf{x}^{(j)},\mathcal{D}) \approx\sum_{r=1}^{N}\omega_{r}\phi\big{(}\bar{f}_{r}(\mathbf{x}) \big{)}\circ\mathcal{N}\Big{(}\nabla\mu_{u|\mathcal{D}}(\mathbf{x}),\nabla^{2 }K_{u|\mathcal{D}}(\mathbf{x},\mathbf{x})\Big{)} \tag{9}\] \[=\sum_{r=1}^{N}\omega_{r}\mathcal{N}\Big{(}\phi\big{(}\bar{f}_{r}( \mathbf{x})\big{)}\circ\nabla\mu_{u|\mathcal{D}}(\mathbf{x}),\phi\big{(}\bar{f }_{r}(\mathbf{x})\big{)}\phi\big{(}\bar{f}_{r}(\mathbf{x})\big{)}^{T}\circ \nabla^{2}K_{u|\mathcal{D}}(\mathbf{x},\mathbf{x})\Big{)} \tag{10}\]

where \(\bar{f}_{r}(\mathbf{x})=\sqrt{2}[\sigma_{u|\mathcal{D}}^{2}(\mathbf{x})+\sigma _{u|\mathcal{D}}^{2}(\mathbf{x}^{(j)})]^{1/2}k_{r}+[\mu_{u|\mathcal{D}}( \mathbf{x})-\mu_{u|\mathcal{D}}(\mathbf{x}^{(j)})]\) are locations of mixture components defined on the sample point \(k_{r}\)s, and \(\circ\) denotes the Hadamard (element-wise) product.

## Appendix C Additional experiment results

Table 2 shows the averaged accuracy and stds of preference prediction from gpca and baselines on both synthetic datasets. gpca has the best prediction for capturing the underlying preferential relations in the system.

Figure 4 shows performance of different acquisition policies for component effect estimation in the two synthetic data.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{**dataset**} & \multicolumn{4}{c}{2dplane} & \multicolumn{2}{c}{friedman} \\ \cline{2-7}  & dim & svm & gpca & dim & svm & gpca \\ \hline acc & 0.696\(\pm\)0.006 & 0.824\(\pm\)0.003 & **0.986\(\pm\)0.002** & 0.785\(\pm\)0.006 & 0.795\(\pm\)0.005 & **0.956\(\pm\)0.002** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Averaged accuracy and stds of preference prediction from gpca and baselines on both synthetic datasets. gpca has the best prediction for capturing the underlying preferential relations in the system.

Figure 4: Box plots of averaged rmse, cor and ll and their stds of component effects with adaptive experimentation.

Experimental results from two real-world applications

**Data.** We apply gpca to two real-world conjoint experiments: U.S. citizens' preferences across presidential candidates and attitudes toward immigrants containing 1733 and 6980 pairwise comparisons [1, 39]. Attributes in the candidate experiment include various aspects of candidates' personal background, demographics and issue positions, such as religion, education, profession, income and race, while attributes in the immigrant experiment include employment plans, job experience, language skills, country of origin, reasons for applying and so on.

**Results.** We run gpca using all samples in both datasets. Table 3 shows the list of attributes with estimated component effects by gpca and dim used in original studies grouped by negative, null and positive effects. Overall, component effect estimation by gpca is more reasonable. For example, in the candidate experiment gpca found negative effects of Black candidates working as high school teachers or farmers on the probability of becoming U.S. presidents and positive effects of Caucasian candidates with 5.1M or more annual income, while dim found no effects for any of these attributes. In the immigrant experiment, gpca found negative effects of Iraqi applicants with broken English on the probability of immigration approval and positive effects of applicants working as financial analysts, while dim found no effects. Figure 5 shows the averaged accuracy and stds of preference prediction by various policies for real data with sample size varying from 100 to 800, where ball has better prediction of unrevealed preferences than randomized policy.

Figure 5: Averaged accuracy and stds of preference prediction by various policies for real data, with sample size varying from 100 to 800. ball has better prediction of unrevealed preferences than randomized policy.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**dataset** & gpca & dim & msc & msc & msc & \\ \hline \multirow{4}{*}{Catalidate} & \multirow{2}{*}{NREL} & \multirow{2}{*}{Emerged pedestrian} & \multirow{2}{*}{Breasus,Catalo-like,Shock Leader} & \multirow{2}{*}{—} \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\  & & & & & \\ \hline \multirow{4}{*}{Immigrant} & \multirow{2}{*}{NREL} & \multirow{2}{*}{—} & \multirow{2}{*}{—} & \multirow{2}{*}{—} \\  & & & & & \\  & & & & & \\ \cline{1-1}  & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 3: List of attributes with estimated component effects by gpca and dim used in the original studies, grouped by negative, null and positive effects.