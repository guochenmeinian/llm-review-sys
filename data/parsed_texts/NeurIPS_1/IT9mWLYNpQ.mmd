# Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability

Jingfeng Wu

Johns Hopkins University

Baltimore, MD 21218

uuujf@jhu.edu

&Vladimir Braverman

Rice University

Houston, TX 77005

vb21@rice.edu

&Jason D. Lee

Princeton University

Princeton, NJ 08544

jasonlee@princeton.edu

###### Abstract

Recent research has observed that in machine learning optimization, gradient descent (GD) often operates at the _edge of stability_ (EoS) [20], where the stepsizes are set to be large, resulting in non-monotonic losses induced by the GD iterates. This paper studies the convergence and implicit bias of constant-stepsize GD for logistic regression on linearly separable data in the EoS regime. Despite the presence of local oscillations, we prove that the logistic loss can be minimized by GD with _any_ constant stepsize over a long time scale. Furthermore, we prove that with _any_ constant stepsize, the GD iterates tend to infinity when projected to a max-margin direction (the hard-margin SVM direction) and converge to a fixed vector that minimizes a strongly convex potential when projected to the orthogonal complement of the max-margin direction. In contrast, we also show that in the EoS regime, GD iterates may diverge catastrophically under the exponential loss, highlighting the superiority of the logistic loss. These theoretical findings are in line with numerical simulations and complement existing theories on the convergence and implicit bias of GD for logistic regression, which are only applicable when the stepsizes are sufficiently small.

## 1 Introduction

_Gradient descent_ (GD) is a foundational algorithm for machine learning optimization that motivates many popular algorithms. Theoretically, the behavior of GD is well understood when the stepsize is small. In this regard, one of the most classic results is the _descent lemma_ (see, e.g., Section 1.2.3 in [2]):

**Lemma** (Descent lemma, simplified version).: _Suppose that \(\sup_{\mathbf{w}_{\mathbf{w}}}\big{\|}\nabla^{2}L(\mathbf{w})\big{\|}_{2}\leq \beta^{1}\), then_

\[L(\mathbf{w}_{+})\leq L(\mathbf{w})-\eta\cdot(1-\eta\beta/2)\cdot\|\nabla L( \mathbf{w})\|_{2}^{2},\quad\text{where}\;\;\mathbf{w}_{+}:=\mathbf{w}-\eta \cdot\nabla L(\mathbf{w}).\]

When the targeted function is smooth (such as logistic regression) and the stepsize is _small_ (\(0<\eta<\beta/2\)), the descent lemma ensures a monotonic decrease of the function value by performing each GD step. Building upon this, a sequence of iterates produced by GD with small stepsizes provably minimizes the function value in various settings (see, e.g., Lan (2020)).

For a more modern example, a recent line of research has established the _implicit bias_ of GD with small stepsizes (see [17], Ji and Telgarsky (2018) and references thereafter). Specifically, they consider GD for optimizing logistic regression (besides other loss functions) onlinearly separable data. When the stepsizes are sufficiently small, the GD iterates are shown to decrease the risk monotonically (by a variant of the descent lemma); moreover, the GD iterates tend to align with a direction that maximizes the \(\ell_{2}\)-margin of the data (Soudry et al., 2018; Ji and Telgarsky, 2018). The margin-maximization bias of small-stepsize GD sheds important light on understanding the statistical benefits of GD, as a large margin solution often generalizes well (Bartlett et al., 2017; Neyshabur et al., 2018).

Nonetheless, in practical machine learning optimization, especially in deep learning, the empirical risk (or training loss) often varies _non-monotonically_ (while being minimized in the long run) -- the local risk oscillation is not only caused by the algorithmic randomness but is more an effect of using _large stepsizes_, as it happens for deterministic GD (with large stepsizes) as well (Wu et al., 2018; Xing et al., 2018; Lewkowycz et al., 2020; Cohen et al., 2021). This phenomenon is showcased in Figures 1(a) and 2(a), and is referred to by Cohen et al. (2021) as the _edge of stability_ (EoS). The observation sets a non-negligible gap between practical and theoretical GD setups, where in practice, GD is run with large stepsizes that lead to local risk oscillations, but in theory, GD is only considered with sufficiently small stepsizes, predicting a monotonic risk descent (with a few exceptions, which will be discussed later in Section 2). A tension remains to be resolved:

_Is the convergence of risk under local oscillation merely a "lucky" occurrence, or is it predictable based on theory?_

Contributions.In this work, we study the behaviors of GD in the EoS regime in arguably the simplest setting for machine learning optimization -- logistic regression on linearly separable data. We show that with _any_ constant stepsize, while the induced risks may oscillate locally, GD must minimize the risk in the long run at a rate of \(\mathcal{O}(1/t)\), where \(t\) is the number of iterates. In addition, we show that the direction of the GD iterates (with any constant stepsize) must align with a max-margin direction (the hard-margin SVM direction) at a rate of \(\mathcal{O}(1/\log(t))\). These results explain how GD minimizes a risk non-monotonically, and complement existing theories (Soudry et al., 2018; Ji and Telgarsky, 2018) on the convergence and implicit bias of GD, which are only applicable when the stepsizes are sufficiently small.

Some additional notable contributions are

1. We also show that, when projected to the orthogonal complement of the max-margin direction, the GD iterates (with any constant stepsize) converge to a fixed vector that minimizes a strongly convex potential at a rate of \(\mathcal{O}(1/\log(t))\). This characterization is conceptually more interpretable than an existing version (Soudry et al., 2018).
2. We show that in the EoS regime, GD can diverge catastrophically if the logistic loss is replaced by the exponential loss. This is in stark contrast to the small-stepsize regime, where the behaviors

Figure 1: The behaviors of GD for optimizing a neural network. We randomly sample \(1,000\) data from the MNIST dataset and then use GD to train a \(4\)-layer fully connected network to fit those data. We use the cross-entropy loss, i.e., the multi-class version of the logistic loss. The sub-figures (a), (b), and (c) report the training loss, test accuracy, and sharpness (see, \(\|\nabla L(\mathbf{w}_{t})\|_{2}\)) along the GD trajectories, respectively. The red curves correspond to GD with a large stepsize \(\eta=0.1\), where the training losses oscillate locally and the sharpness can exceed \(2/\eta=20\). The green curves correspond to GD with a small stepsize \(\eta=0.01\), where the training losses decrease monotonically and the sharpnesses are always below \(2/\eta=200\). Moreover, (c) suggests that large-stepsize GD achieves better test accuracy than small-stepsize GD, consistent with larger-scale deep learning experiments (Goyal et al., 2017). More details of the experiments can be found in Appendix D.

of GD are known to be similar under any exponentially-tailed losses including both the logistic and exponential losses (Soudry et al., 2018; Ji and Telgarsky, 2018). The difference in the EoS regime provides insights into why the logistic loss is preferable to the exponential loss in practice.
3. From a technical perspective, we develop a new approach for analyzing GD with large stepsizes. Our approach views the GD iterates as a coupling of two orthogonal iterates, one along a max-margin direction and the other along the orthogonal complement of the max-margin direction. The former iterates tend to infinity and the latter iterates approximate "imaginary" GD iterates that minimize a strongly convex potential with a _decaying_ stepsize scheduler, controlled by the former iterates. Our techniques for analyzing large-stepsize GD can be of independent interest.

## 2 Related Works

In this section, we discuss papers related to our work.

**Implicit bias.** We first review a set of papers on the implicit bias of GD (with small stepsizes).

Along this line, Soudry et al. (2018) are the very first to show that GD converges along a max-margin direction when minimizing the empirical risk of an exponentially-tailed loss function (such as the logistic and exponential losses), a linear model, and linearly separable data. Then, an alternative analysis is provided by Ji and Telgarsky (2018), which also deals with non-separable data. These two works directly motivate us for considering GD for logistic regression on linearly separable data. However, there are at least three notable differences between our work and theirs. Firstly, their results only apply to GD with small stepsizes, while our results apply to GD with _any_ constant stepsize. Secondly, their theories predict no difference between the logistic and exponential losses (as they are limited to the small-stepsize regime). Quite surprisingly, we prove that in the EoS regime, GD can diverge catastrophically under the exponential loss. Thirdly, from a technical viewpoint, their implicit bias analysis is built upon the risk convergence analysis, which further relies on a monotonic risk descent argument, hence only applies to small stepsizes. In comparison, we come up with a new approach that allows analyzing the implicit bias under risk oscillations; the long-term risk convergence is simply a consequence of the implicit bias results. Hence our techniques can accommodate any constant stepsize. See Section 5 for more discussions.

Subsequent works have extended the results by Soudry et al. (2018); Ji and Telgarsky (2018) to other algorithms such as momentum-based GD (Gunasekar et al., 2018; Ji et al., 2021) and SGD (Nacson et al., 2019), and homogenous but non-linear models (Gunasekar et al., 2017; Ji and Telgarsky, 2019; Gunasekar et al., 2018; Nacson et al., 2019; Lyu and Li, 2020) and non-homogenous models (Nacson et al., 2019). All these theories require the stepsizes to be small or even infinitesimal, in a regime away from our focus, the EoS regime.

It is worth noting that Nacson et al. (2019) consider GD with an increasing stepsize scheduler that achieves a faster margin-maximization rate than constant-stepsize GD. However, their stepsize at each iteration is still appropriately small, resulting in a monotonic risk descent by a variant of the descent lemma.

**Edge of stability.** The risk oscillation phenomenon has been observed in several deep learning papers (Wu et al., 2018; Xing et al., 2018; Lewkowycz et al., 2020), and the work by Cohen et al. (2021) coins the term, _edge of stability_ (EoS), that formally refers to it. In the remainder of this part, we focus on reviewing the current theoretical progress in understanding EoS.

Zhu et al. (2023) rigorously characterize EoS for a two-dimensional function \((u,v)\mapsto(u^{2}v^{2}-1)^{2}\). Chen and Bruna (2022) study EoS for a one-dimensional function \(u\mapsto(u^{2}-1)^{2}\) and for a special two-layer single-neuron network. Similar to these two works, Kreisler et al. (2023) study EoS in a 1-dimensional linear network. Ahn et al. (2022) consider functions \((u,v)\mapsto\ell(uv)\), where \(\ell\) is assumed to be convex, even, and Lipschitz; notably, they show a statistical gap between the small-stepsize regime and the EoS regime. Finally, Even et al. (2023), Andriushchenko et al. (2023) consider the regularization effect of large stepsizes in a diagonal linear network. Compared to their settings, our problem, i.e., logistic regression, is a natural machine-learning problem with fewer artifacts (if any).

EoS has also been theoretically investigated for general functions (Ma et al., 2022; Ahn et al., 2022; Damian et al., 2022; Wang et al., 2022), but these theories are often subject to subtle assumptions that are hard to interpret or verify. Specifically, Ma et al. (2022) require the function to grow "subquadratically". Ahn et al. (2022) assume the existence of a "forward invariant subset" near the set of minima of the function. Damian et al. (2022) assume a negative correlation between the gradient direction and the largest eigenvalue direction of the Hessian. Wang et al. (2022) consider a two-layer neural network but require the norm of the last layer parameter and the sharpness to change in the same direction along the GD trajectory. Indirectly connected to EoS, the work by Kong and Tao (2020) shows a chaotic behavior of GD with a non-small stepsize when optimizing a "multi-scale" loss function. In comparison, our assumptions are more natural and interpretable.

Besides, the work by Lyu et al. (2022) considers EoS induced by GD for scale-invariant loss, e.g., a network with normalization layers and weight decay, and the work by Wang et al. (2022) shows a balancing effect in matrix factorization induced by GD with a constant stepsize that is nearly \(4/\|\nabla^{2}L(\mathbf{w}_{0})\|_{2}\) and is larger than \(2/\|\nabla^{2}L(\mathbf{w}_{0})\|_{2}\). The objectives in their works are different from ours, i.e., logistic regression.

The unstable convergence has also been studied for normalized GD (Arora et al., 2022) and regularized GD (Bartlett et al., 2022). These algorithms are apart from our focus on the vanilla GD.

Finally, the work by Liu et al. (2023) considers logistic regression with non-separable data (such that the objective is strongly convex), where GD with sufficiently large stepsize diverges. In contrast, we consider logistic regression with separable data, where GD with an arbitrarily large stepsize still converges.

## 3 Preliminaries

We use \(\mathbf{x}\in\mathbb{R}^{d}\) to denote a feature vector and \(y\in\{\pm 1\}\) to denote a binary label, respectively. Let \((\mathbf{x}_{i},y_{i})_{i=1}^{n}\) be a set of training data. Throughout the paper, we assume that \((\mathbf{x}_{i},y_{i})_{i=1}^{n}\) is _linearly separable_(Soudry et al., 2018).

**Assumption 1** (Linear separability).: _Assume there is \(\mathbf{w}\in\mathbb{R}^{d}\) such that \(y_{i}\mathbf{x}_{i}^{\top}\mathbf{w}>0\) for \(i=1,\ldots,n\)._

Let \(\mathbf{w}\in\mathbb{R}^{d}\) be the parameter of a linear model. In _logistic regression_, we aim to minimize the following empirical risk

\[L(\mathbf{w}):=\sum_{i=1}^{n}\log\big{(}1+\exp(-y_{i}\cdot\langle\mathbf{x}_{i},\mathbf{w}\rangle)\big{)},\quad\mathbf{w}\in\mathbb{R}^{d}.\]

We study a sequence of iterates \((\mathbf{w}_{t})_{t\geq 0}\) produced by constant-stepsize _gradient descent_ (GD), where \(\mathbf{w}_{0}\) denotes the initialization and the remaining iterates are sequentially generated by:

\[\mathbf{w}_{t}=\mathbf{w}_{t-1}-\eta\cdot\nabla L(\mathbf{w}_{t-1}),\quad t\geq 1,\] (GD)

where \(\eta>0\) is a constant stepsize. We are especially interested in a regime where \(\eta\) is very large such that \(L(\mathbf{w}_{t})\) oscillates as a function of \(t\). For the simplicity of presentation, we will assume that \(\mathbf{w}_{0}=0\). Our results can be easily extended to allow general initialization.

The following notations are useful for presenting our results.

**Definition 1** (Margins and support vectors).: Under Assumption 1, define the following notations:

1. Let \(\gamma\) be the max-\(\ell_{2}\)-margin (or max-margin in short), i.e., \[\gamma:=\max_{\|\mathbf{w}\|_{2}=1}\min_{i\in[n]}\;y_{i}\cdot\langle\mathbf{x }_{i},\mathbf{w}\rangle.\]
2. Let \(\hat{\mathbf{w}}\) be the hard-margin support-vector-machine (SVM) solution, i.e., \[\hat{\mathbf{w}}:=\arg\min_{\mathbf{w}\in\mathbb{R}^{d}}\|\mathbf{w}\|_{2},\; \;\text{s.t.}\;\;y_{i}\cdot\langle\mathbf{x}_{i},\mathbf{w}\rangle\geq 1,\;i=1, \ldots,n.\] It is clear that \(\hat{\mathbf{w}}\) exists and is uniquely defined (see, e.g., Section 5.2 in Mohri et al. (2018)). Note that \(\|\hat{\mathbf{w}}\|_{2}=1/\gamma\) and \(\hat{\mathbf{w}}/\|\hat{\mathbf{w}}\|_{2}\) is a max-margin direction. Also note that by duality, \(\hat{\mathbf{w}}\) can be written as (see, e.g., Section 5.2 in Mohri et al. (2018)) \[\hat{\mathbf{w}}=\sum_{i\in\mathcal{S}}\alpha_{i}\cdot y_{i}\mathbf{x}_{i}, \quad\alpha_{i}\geq 0.\]* Let \(\mathcal{S}\) be the set of indexes of the support vectors, i.e., \[\mathcal{S}:=\{i\in[n]:y_{i}\cdot\langle\mathbf{x}_{i},\hat{\mathbf{w}}/\|\hat{ \mathbf{w}}\|_{2}\rangle=\gamma\}.\]
* If there exists non-support vector (\(\mathcal{S}\subsetneq[n]\)), let \(\theta\) be the second smallest margin, i.e., \[\theta:=\min_{i\notin\mathcal{S}}\;y_{i}\cdot\langle\mathbf{x}_{i},\hat{ \mathbf{w}}/\|\hat{\mathbf{w}}\|_{2}\rangle.\]

It is clear from the definitions that \(\theta>\gamma>0\). In addition, from the definitions we have

\[\sum_{i\in\mathcal{S}}\alpha_{i}=\sum_{i\in\mathcal{S}}\alpha_{i}\cdot y_{i} \mathbf{x}_{i}^{\top}\hat{\mathbf{w}}=\|\hat{\mathbf{w}}\|_{2}^{2}=\frac{1}{ \gamma^{2}}.\]

In addition to Assumption 1, we make the following two mild assumptions to facilitate our analysis.

**Assumption 2** (Regularity conditions).: _Assume that:_

* \(\|\mathbf{x}_{i}\|_{2}\leq 1,\;i=1,\ldots,n\)_._
* \(\mathrm{rank}\{\mathbf{x}_{i},\;i=1,\ldots,n\}=d\)_._

Assumption 2 is only made for the convenience of presentation. In particular, Assumption 2(A) can be made true for any dataset by scaling the data vectors with a factor of \(\max_{i}\|\mathbf{x}_{i}\|_{2}\). Without Assumption 2(B), our theorems still hold under a minor revision by replacing all the vectors of interests with their projections to \(\text{span}\{\mathbf{x}_{i},i=1,\ldots,n\}\).

**Assumption 3** (Non-degenerate data).: _In addition to Assumption 1, assume that_

* \(\mathrm{rank}\{\mathbf{x}_{i},\;i\in\mathcal{S}\}=\mathrm{rank}\{\mathbf{x}_{i },\;i=1,\ldots,n\}\)_._
* _There exist_ \(\alpha_{i}>0,i\in\mathcal{S}\) _such that_ \(\hat{\mathbf{w}}=\sum_{i\in\mathcal{S}}\alpha_{i}\cdot y_{i}\mathbf{x}_{i}\)_._

Assumption 3 has been used in Soudry et al. (2018) (see their Theorem 4), which requires that the support vectors span the dataset and are associated with strictly positive dual variables. Assumption 3(B) holds _almost surely_ for every linearly separable dataset sampled from a continuous distribution according to Appendix B in Soudry et al. (2018). Assumption 3 provides convenience to our analysis, but we conjecture it might not be necessary. Removing/relaxing Assumption 3 is left as a future work.

### Space Decomposition

Conceptually, our analysis is built on a novel space decomposition viewpoint, which relies on the following lemma.

**Lemma 3.1** (Non-separable subspace).: _Suppose that Assumptions 1, 2, and 3 hold. Then \((\mathbf{x}_{i},y_{i})_{i\in\mathcal{S}}\) is not linearly separable in the subspace orthogonal to the max-margin direction \(\hat{\mathbf{w}}/\|\hat{\mathbf{w}}\|_{2}\). That is, for every \(\mathbf{v}\) such that \(\langle\mathbf{v},\hat{\mathbf{w}}\rangle=0,\) there exist \(i,j\in\mathcal{S}\) such that \(y_{i}\cdot\langle\mathbf{x}_{i},\mathbf{v}\rangle<0,\;y_{j}\cdot\langle \mathbf{x}_{j},\mathbf{v}\rangle>0.\)_

Proof of Lemma 3.1.: By Assumption 3 and \(\langle\mathbf{v},\hat{\mathbf{w}}\rangle=0\), we have

\[0=\langle\mathbf{v},\hat{\mathbf{w}}\rangle=\sum_{i\in\mathcal{S}}\alpha_{i} \cdot y_{i}\mathbf{x}_{i}^{\top}\mathbf{v}.\]

By Assumptions 2 and 3 we have

\[\mathrm{rank}\{y_{i}\mathbf{x}_{i},\;i\in\mathcal{S}\}=\mathrm{rank}\{ \mathbf{x}_{i},\;i\in\mathcal{S}\}=\mathrm{rank}\{\mathbf{x}_{i},\;i=1,\ldots, n\}=d,\]

so there must exist \(i\in\mathcal{S}\) such that \(y_{i}\mathbf{x}_{i}^{\top}\mathbf{v}\neq 0\). Without loss of generality, assume that \(y_{i}\mathbf{x}_{i}^{\top}\mathbf{v}<0\). Then since \(\alpha_{i}>0\) for \(i\in\mathcal{S}\) by Assumption 3, there must exist \(j\in\mathcal{S}\) such that \(y_{j}\mathbf{x}_{j}^{\top}\mathbf{v}>0\). 

Lemma 3.1 shows that, although the dataset can be (linearly) separated by \(\hat{\mathbf{w}}\), it cannot be separated by _any_ vector orthogonal to \(\hat{\mathbf{w}}\). This motivates us to decompose the \(d\)-dimensional ambient space into a \(1\)-dimensional "separable" subspace and a \((d-1)\)-dimensional "non-separable" subspace. This idea is formally realized as follows.

Fix \(d-1\) orthogonal vectors \(\mathbf{f}_{1},\ldots,\mathbf{f}_{d-1}\in\mathbb{R}^{d}\) such that \(\big{(}\hat{\mathbf{w}}/\|\hat{\mathbf{w}}\|_{2},\mathbf{f}_{1},\ldots, \mathbf{f}_{d-1}\big{)}\) forms an orthogonal basis of the ambient space \(\mathbb{R}^{d}\). Then define two _projection operators_:

\[\mathcal{P}\colon\mathbb{R}^{d}\to\mathbb{R}\qquad\text{ given by}\quad \mathbf{v}\mapsto\mathbf{v}^{\top}\hat{\mathbf{w}}/\|\hat{\mathbf{w}}\|_{2},\]\[\tilde{\mathcal{P}}\colon\mathbb{R}^{d}\to\mathbb{R}^{d-1}\quad\text{given by}\quad\mathbf{v}\mapsto(\mathbf{v}^{\top}\mathbf{f}_{1},\ldots,\mathbf{v}^{\top}\mathbf{f}_{d-1}).\]

The two operators together define a natural space decomposition, i.e., \(\mathbb{R}^{d}=\mathcal{P}(\mathbb{R}^{d})\oplus\mathcal{P}(\mathbb{R}^{d})\). Moreover, \(\big{(}\mathcal{P}(\mathbf{x}_{i}),y_{i}\big{)}_{i=1}^{n}\) are linearly separable with an max-\(\ell_{2}\)-margin \(\gamma\) according to Definition 1, and \(\big{(}\tilde{\mathcal{P}}(\mathbf{x}_{i}),y_{i}\big{)}_{i\in\mathcal{S}}\) (hence \(\big{(}\tilde{\mathcal{P}}(\mathbf{x}_{i}),y_{i}\big{)}_{i=1}^{n}\)) are non-separable according to Lemma 3.1. So the decomposition of space can also be understood as the decomposition of data features into "max-margin features" and "non-separable features".

In what follows, we will call \(\mathcal{P}(\mathbb{R}^{d})\) the _max-margin subspace_ and \(\tilde{\mathcal{P}}(\mathbb{R}^{d})\) the _non-separable subspace_, respectively. In addition, we define a "margin offset" that quantifies to what extent the "non-separable features" are not separable.

**Definition 2** (Margin offset for the non-separable features).: Under Assumptions 1, 2, and 3, it holds that \(\big{(}\tilde{\mathcal{P}}(\mathbf{x}_{i}),y_{i}\big{)}_{i\in\mathcal{S}}\) is non-separable. Let \(b\) be a _margin offset_ such that

\[-b:=\max_{\bar{\mathbf{w}}\in\mathbb{R}^{d-1},\;\|\bar{\mathbf{w}}\|=1}\min_{ i\in\mathcal{S}}\;\;y_{i}\cdot\big{\langle}\tilde{\mathcal{P}}(\mathbf{x}_{i}), \;\bar{\mathbf{w}}\big{\rangle}.\]

Then \(b>0\) due to the non-separability. The definition immediately implies that:

\[\text{for every }\bar{\mathbf{v}}\in\mathbb{R}^{d-1},\;\text{there exists }i\in\mathcal{S}\;\text{such that }y_{i}\cdot\big{\langle}\tilde{\mathcal{P}}(\mathbf{x}_{i}),\;\bar{ \mathbf{v}}\big{\rangle}\leq-b\cdot\|\bar{\mathbf{v}}\|_{2}.\]

Comparison to Ji and Telgarsky (2018).The work by Ji and Telgarsky (2018) also conducts space decomposition (see their Section 2). However, our approach is completely different from theirs. Firstly, they consider a non-separable dataset but we consider a linearly separable dataset. Secondly, at a higher level, they decompose the "dataset" (into two subsets), while we decompose the "features" (into two kinds of features). More specifically, Ji and Telgarsky (2018) first group the non-separable dataset into the "maximal linearly separable subset" and the complement, non-separable subset, then decompose the ambient space according to the subspace spanned by the non-separable subset and its orthogonal complement. In comparison, we consider a linearly separable dataset and decompose the ambient space according to a max-margin direction (i.e., \(\mathcal{P}\)) and its orthogonal complement (i.e., \(\tilde{\mathcal{P}}\)).

## 4 Main Results

We are now ready to present our main results. All proofs are deferred to Appendix C. To begin with, we provide the following theorem that captures the behaviors of constant-stepsize GD for logistic regression on linearly separable data.

**Theorem 4.1** (The implicit bias of GD for logistic regression).: _Suppose that Assumptions 1, 2, and 3 hold. Consider \((\mathbf{w}_{t})_{t\geq 0}\) produced by \((\mathrm{GD})\) with initilization\({}^{2}\)\(\mathbf{w}_{0}=0\) and constant stepsize \(\eta>0\). Then there exist positive constants \(c_{1},c_{2},c_{3}>0\) that are upper bounded by a polynomial of \(\big{\{}e^{\eta},e^{n},e^{1/b},1/\eta,1/(\theta-\gamma),1/\gamma,e^{\theta/ \gamma}\big{\}}\) but are independent of \(t\), such that:_

1. _The risk is upper bounded by_ \[L(\mathbf{w}_{t})\leq c_{1}/t,\quad t\geq 3.\]
2. _In the max-margin subspace,_ \[\mathcal{P}(\mathbf{w}_{t})\geq\log(t)/\gamma+\log(\eta\gamma^{2}/2)/\gamma, \quad t\geq 1.\]
3. _In the non-separable subspace,_ \[\big{\|}\tilde{\mathcal{P}}(\mathbf{w}_{t})\big{\|}_{2}\leq c_{2},\quad t\geq 0.\]
4. _In addition, in the non-separable subspace,_ \[G\big{(}\tilde{\mathcal{P}}(\mathbf{w}_{t})\big{)}-\min G(\cdot)\leq c_{3}/ \log(t),\quad t\geq 3,\] _where_ \(G(\cdot)\) _is a strongly convex potential defined by_ \[G(\mathbf{v}):=\sum_{i\in\mathcal{S}}\exp\big{(}-y_{i}\cdot\big{\langle}\tilde {\mathcal{P}}(\mathbf{x}_{i}),\;\mathbf{v}\big{\rangle}\big{)},\quad\mathbf{v }\in\mathbb{R}^{d-1}.\]Note that Theorem 4.1 applies to GD with _any_ positive constant stepsize, therefore allowing GD to be in the EoS regime. We next discuss the implications of Theorem 4.1 in detail.

**Risk minimization.** Theorem 4.1(A) guarantees that the GD iterates minimize the logistic loss at a rate of \(\mathcal{O}(1/t)\) for any constant stepsize, even for those large stepsizes that cause local risk oscillations. This result explains the risk convergence of GD in the EoS regime, as illustrated in Figure 2, and is also consistent with the observations in neural network experiments (see Figure 1).

**Margin maximization.** Theorem 4.1(B) shows that the GD iterates, when projected to the max-margin direction, tend to infinity at a rate of \(\mathcal{O}(\log(t))\). Moreover, Theorem 4.1(C) shows that the GD iterates, when projected to the non-separable subspace, are uniformly bounded. These two results together imply that the direction of the GD iterates will tend to a max-margin direction, i.e., the hard-margin SVM direction, at a rate of \(\mathcal{O}(1/\log(t))\). Therefore, the implicit bias of GD that maximizes the \(\ell_{2}\)-margin is consistent in both the EoS regime and the small-stepsize regime (Soudry et al., 2018; Ji and Telgarsky, 2018).

**Iterate convergence in the non-separable subspace.** Theorem 4.1(D) shows that the GD iterates, when projected to the non-separable subspace, converge to the minimizer of a strongly convex potential \(G(\cdot)\). Here, \(G(\cdot)\) measures the exponential loss of a parameter on the support vectors with their non-separable features. This provides a more precise characterization of the implicit bias of GD: the direction of the GD iterates converges to the hard-margin SVM direction, moreover, the limit of the projections of the GD iterates to the orthogonal complement to the hard-margin SVM direction minimizes the exponential loss on the non-separable features of the support vectors.

**Comparison to Theorem 9 in Soudry et al. (2018).** Theorem 9, in particular, equation (18), in Soudry et al. (2018)_indirectly_ characterizes the convergence of GD iterates in the non-separable subspace. It reads in our notations that: \(\tilde{\mathbf{w}}:=\lim_{t\rightarrow\infty}\left(\mathbf{w}_{t}-\tilde{ \mathbf{w}}\log(t)\right)\) exists and satisfies

\[\text{for every }i\in\mathcal{S},\ \ \eta\cdot\exp(-y_{i}\cdot\langle\mathbf{x}_{i },\ \tilde{\mathbf{w}}\rangle)=\alpha_{i},\text{ where }\alpha_{i}\text{ is defined in Assumption \ref{Assumption:def}}. \tag{1}\]

In Appendix A, we show that Theorem 4.1(D) is equivalent to condition (1) in terms of describing \(\bar{\mathcal{P}}(\mathbf{w}_{\infty})\). Despite their equivalence, (1) is less interpretable than Theorem 4.1(D), as (1) entangles an effect of \(\mathcal{P}(\mathbf{w}_{\infty})\) with \(\bar{\mathcal{P}}(\mathbf{w}_{\infty})\), while Theorem 4.1 completely decouples \(\mathcal{P}(\mathbf{w}_{\infty})\) and \(\bar{\mathcal{P}}(\mathbf{w}_{\infty})\). In particular, (1) seems to suggest \(\bar{\mathcal{P}}(\mathbf{w}_{\infty})\) to be a function of stepsize \(\eta\) since \(\tilde{\mathbf{w}}\) depends on \(\eta\). However, this is only an illusion brought by the lack of interpretability of (1); it is clear that \(\bar{\mathcal{P}}(\mathbf{w}_{\infty})\) is independent of \(\eta\) according to Theorem 4.1(D).

**Exponential loss.** Until now, our theory for GD is consistent for large and small stepsizes. However, this is a particular benefit thanks to the design of the logistic loss, and may not hold for other losses. Our next result suggests that, in the EoS regime where the stepsizes are large, GD can diverge catastrophically under the exponential loss.

Figure 2: The behaviors of GD for logistic regression. We randomly sample \(1,000\) data with labels “\(0\)” and “\(8\)” from the MNIST dataset and then use GD to perform logistic regression on those data. The sub-figures (a) and (b) report the risk (i.e., the logistic loss) and sharpness (i.e., \(\|\nabla L(\mathbf{w}_{t})\|_{2}\)) along the GD trajectories, respectively. For GD with stepsizes \(\eta\) larger than or equal to \(0.1\), the training losses oscillate locally and the sharpnesses can exceed \(2/\eta\). For GD with a small stepsize \(\eta=0.01\), the training losses decrease monotonically and the sharpnesses are always below \(2/\eta\). More details of the experiments can be found in Appendix D.

**Theorem 4.2** (The catastrophic divergence of GD under the exponential loss).: _Consider a dataset of two samples, where_

\[\mathbf{x}_{1}=(\gamma,\;1),\quad y_{1}=1;\qquad\mathbf{x}_{2}=(\gamma,\;-1), \quad y_{2}=1.\]

_It is clear that \((\mathbf{x}_{i},y_{i})_{i=1,2}\) is linearly separable and \((1,0)\) is the max-margin direction. Consider a risk defined by the exponential loss:_

\[L(w,\bar{w}):=\exp(-y_{1}\langle\mathbf{x}_{1},\mathbf{w}\rangle)+\exp(-y_{2} \langle\mathbf{x}_{2},\mathbf{w}\rangle)=e^{-\gamma w}\cdot\big{(}e^{-\bar{w}} +e^{\bar{w}}\big{)},\quad\text{where}\;\;\mathbf{w}=(w,\bar{w}).\]

_Let \((w_{t},\bar{w}_{t})_{t\geq 0}\) be the iterates produced by GD with constant stepsize \(\eta\) for optimizing \(L(w,\bar{w})\). If_

\[0\leq w_{0}\leq 2,\quad|\bar{w}_{0}|\geq 1,\quad 0<\gamma<1/4,\quad\eta\geq 4,\]

_then:_

1. \(L(w_{t},\bar{w}_{t})\to\infty\)_._
2. \(w_{t}\to\infty\)_._
3. _For every_ \(t\geq 0\)_,_ \(|\bar{w}_{t}|\geq 2\gamma w_{t}\)_._
4. _Moreover, the sign of_ \(\bar{w}_{t}\) _flips every iteration._

_As a consequence, \((w_{t},\bar{w}_{t})_{t\geq 0}\) diverge in terms of either magnitude or direction; in particular, the direction of \((w_{t},\bar{w}_{t})_{t\geq 0}\) cannot converge to the max-margin direction (which is \((1,0)\))._

Theorem 4.2 shows that with a large constant stepsize, the GD iterates no longer minimize the risk defined by the exponential loss and no longer converge along the max-margin direction. In fact, the directions of the GD iterates flip every step, thus the direction of the GD iterates necessarily _diverges_, resulting in no meaningful implicit bias at all.

In the EoS regime, large-stepsize GD still behaves nicely under the logistic loss (Theorem 4.1) but can behave catastrophically under the exponential loss (Theorem 4.2). From a mathematical standpoint, this difference is rooted in the fact that the gradient of the logistic loss is uniformly bounded while the gradient of the exponential loss could be extremely large. From a practical standpoint, it provides insights into why the logistics loss (and its multi-class version, the cross-entropy loss) is preferable to the exponential loss in practice.

The different behaviors of large-stepsize GD under the logistic and exponential losses also sharply contrast the EoS regime with the small-stepsize regime. Because in the small-stepsize regime, the convergence and implicit bias of GD are known to be similar under any exponentially-tailed losses, including the logistic and exponential losses (Soudry et al., 2018; Ji and Telgarsky, 2018).

## 5 Techniques Overview

The proofs of Theorems 4.1 and 4.2 are deferred to Appendix C. In this section, we explain the proof ideas of Theorem 4.1 by analyzing a simple dataset considered in Theorem 4.2 (the treatment to the general datasets can be found in Appendix B). But this time we work with the logistic loss instead of the exponential loss, that is,

\[L(w,\bar{w})=\log(1+e^{-\gamma w-\bar{w}})+\log(1+e^{-\gamma w+\bar{w}}).\]

Then the GD iterates can be written as

\[w_{t+1}=w_{t}-\eta\cdot g_{t},\qquad\bar{w}_{t+1}=\bar{w}_{t}-\eta\cdot\bar{g} _{t},\]

where

\[g_{t}:=-\gamma\cdot\bigg{(}\frac{1}{1+e^{\gamma w_{t}+\bar{w}_{t}}}+\frac{1}{1 +e^{\gamma w_{t}-\bar{w}_{t}}}\bigg{)},\quad\bar{g}_{t}:=-\bigg{(}\frac{1}{1 +e^{\gamma w_{t}+\bar{w}_{t}}}-\frac{1}{1+e^{\gamma w_{t}-\bar{w}_{t}}}\bigg{)}.\]

For simplicity, assume that

\[w_{0}=0,\quad|\bar{w}_{0}|>0.\]

Different from Soudry et al. (2018); Ji and Telgarsky (2018), our approach begins with showing the implicit bias (despite that the risk may oscillate). The long-term risk convergence is then simply a consequence of the implicit bias results.

Step 1: \((\bar{w}_{t})_{t\geq 0}\) is uniformly bounded.Observe that \(\bar{g}_{t}\) and \(\bar{w}_{t}\) always share the same sign and that \(|\bar{g}_{t}|\leq 1\), so we have

\[|\bar{w}_{t+1}|=\big{|}|\bar{w}_{t}|-\eta\cdot|\bar{g}_{t}|\big{|}\leq\max\big{\{} |\bar{w}_{t}|,\;\eta\cdot|\bar{g}_{t}|\big{\}}\leq\max\big{\{}|\bar{w}_{t}|,\; \eta\big{\}}.\]

By induction, we get that \(\big{(}|\bar{w}_{t}|\big{)}_{t\geq 0}\) is uniformly bounded by \(\max\{|\bar{w}_{0}|,\;\eta\}=\Theta(1)\).

Step 2: \(w_{t}\approx\log(t)/\gamma\).We turn to study the max-margin subspace. It is clear that \(g_{t}\leq 0\) for every \(t\geq 0\). So we have \(w_{t}\geq 0\) by induction. Moreover, we have

\[-\frac{g_{t}}{\gamma}=\frac{e^{-\gamma w_{t}-\bar{w}_{t}}}{1+e^{-\gamma w_{t}- \bar{w}_{t}}}+\frac{e^{-\gamma w_{t}+\bar{w}_{t}}}{1+e^{-\gamma w_{t}+\bar{w}_ {t}}}\leq e^{-\gamma w_{t}}\cdot e^{-\bar{w}_{t}}+e^{-\gamma w_{t}}\cdot e^{ \bar{w}_{t}}\leq e^{-\gamma w_{t}}\cdot\Theta(1),\]

where the last inequality is because \(|\bar{w}_{t}|\) is uniformly bounded. We also have

\[-\frac{g_{t}}{\gamma} =\frac{e^{-\gamma w_{t}-\bar{w}_{t}}}{1+e^{-\gamma w_{t}-\bar{w}_{ t}}}+\frac{e^{-\gamma w_{t}+\bar{w}_{t}}}{1+e^{-\gamma w_{t}+\bar{w}_{t}}}\geq 0.5 \cdot\min\{1,e^{-\gamma w_{t}}e^{-\bar{w}_{t}}\}+0.5\cdot\min\{1,e^{-\gamma w_{ t}}e^{\bar{w}_{t}}\}\] \[\geq 0.5\cdot\min\{1,e^{-\gamma w_{t}}e^{-\bar{w}_{t}}+e^{-\gamma w _{t}}e^{\bar{w}_{t}}\}\geq 0.5\cdot\min\{1,e^{-\gamma w_{t}}\}=0.5\cdot e^{- \gamma w_{t}},\]

where the third inequality is because \(e^{-\bar{w}_{t}}+e^{\bar{w}_{t}}\geq 1\) and the last equality is because \(w_{t}\geq 0\). Putting these together, we have

\[g_{t}\approx-\gamma\cdot e^{-\gamma w_{t}}\cdot\Theta(1)\;\Rightarrow\;w_{t+1 }\approx w_{t}-\eta\gamma\cdot e^{-\gamma w_{t}}\cdot\Theta(1)\;\Rightarrow \;w_{t}=\log(t)/\gamma\pm\Theta(1). \tag{2}\]

Step 3: \(\bar{g}_{t}\approx\exp(-\gamma w_{t})\cdot\nabla G(\bar{w}_{t})\).We turn back to the non-separable subspace. Note that \(\bar{g}_{t}\) is an odd function of \(\bar{w}_{t}\). Without loss of generality, let us assume \(\bar{w}_{t}\geq 0\) in this part. Notice that

\[\text{for every fixed}\;a>1,\;f(t):=\frac{1}{t+1/a}-\frac{1}{t+a}\text{ is a decreasing function of}\;t\geq 0. \tag{3}\]

Then we have

\[\bar{g}_{t}=e^{-\gamma w_{t}}\cdot\left(\frac{1}{e^{-\gamma w_{t}}+e^{-\bar{w} _{t}}}-\frac{1}{e^{-\gamma w_{t}}+e^{\bar{w}_{t}}}\right)\leq e^{-\gamma w_{t} }\cdot\left(\frac{1}{e^{-\bar{w}_{t}}}-\frac{1}{e^{\bar{w}_{t}}}\right)=:e^{- \gamma w_{t}}\cdot\nabla G(\bar{w}_{t}),\]

where the inequality is by (3), and \(G(\bar{w}):=e^{\bar{w}}+e^{-\bar{w}}\) is defined as in Theorem 4.1(D). On the other hand, since \(|\bar{w}_{t}|\) is bounded and \(w_{t}\) is increasing (and tends to infinity), there must exist a time \(t_{0}\) such that \(e^{-\gamma w_{t}}\leq e^{-|\bar{w}_{t}|}\) for every \(t\geq t_{0}\). Then for \(t\geq t_{0}\) we have

\[\bar{g}_{t} =e^{-\gamma w_{t}}\cdot\left(\frac{1}{e^{-\gamma w_{t}}+e^{-\bar{ w}_{t}}}-\frac{1}{e^{-\gamma w_{t}}+e^{\bar{w}_{t}}}\right)\geq e^{-\gamma w_{t}} \cdot\left(\frac{1}{2e^{-\bar{w}_{t}}}-\frac{1}{e^{-\bar{w}_{t}}+e^{\bar{w}_{t }}}\right)\] \[=e^{-\gamma w_{t}}\cdot\frac{e^{\bar{w}_{t}}-e^{-\bar{w}_{t}}}{2 e^{-2\bar{w}_{t}}+2}\geq e^{-\gamma w_{t}}\cdot\frac{e^{\bar{w}_{t}}-e^{-\bar{w}_{t}}}{4 }=:\frac{1}{4}\cdot e^{-\gamma w_{t}}\cdot\nabla G(\bar{w}_{t}),\]

where the first inequality is by (3) and \(e^{-\gamma w_{t}}\leq e^{-\bar{w}_{t}}\), and the last inequality is because we assume \(\bar{w}_{t}\geq 0\). Putting these together, and using (2), we obtain that

\[\text{for every}\;t\geq t_{0},\;\;\bar{w}_{t+1}=\bar{w}_{t}-\eta_{t}\cdot\nabla G (\bar{w}_{t}),\;\text{where}\;\eta_{t}\approx\eta\cdot e^{-\gamma w_{t}}\cdot \Theta(1)\approx\Theta(1)/t. \tag{4}\]

Step 4: a modified descent lemma.Using (4) and Taylor's expansion, we have

\[\text{for every}\;t\geq t_{0},\;\;G(\bar{w}_{t+1})\leq G(\bar{w}_{t})-\eta_{t} \cdot\|\nabla G(\bar{w}_{t})\|^{2}+\frac{\beta}{2}\cdot\eta_{t}^{2}\cdot\| \nabla G(\bar{w}_{t})\|^{2}\leq G(\bar{w}_{t})+\frac{\Theta(1)}{t^{2}},\]

where \(\beta:=\sup_{|\bar{w}|\leq\max\{|\bar{w}_{0}|,\eta\}}\|\nabla^{2}G(\bar{v})\|_{2 }=\Theta(1)\). Taking a telescoping sum from \(t\) to \(T\), we have

\[\text{for every}\;T\geq t\geq t_{0},\quad G(\bar{w}_{T})\leq G(\bar{w}_{t})+ \Theta(1)/t. \tag{5}\]

Step 5: the convergence of \(\bar{w}_{t}\).What remains is adapted from classic convergence arguments. Choose \(\bar{w}_{*}=\arg\min G(\cdot)\), then

\[\|\bar{w}_{t+1}-\bar{w}_{*}\|_{2}^{2} =\|\bar{w}_{t}-\bar{w}_{*}\|_{2}^{2}-2\eta_{t}\cdot\langle\bar{w} _{t}-\bar{w}_{*},\nabla G(\bar{w}_{t})\rangle+\eta_{t}^{2}\cdot\|\nabla G(\bar{w }_{t})\|_{2}^{2}\] \[\leq\|\bar{w}_{t}-\bar{w}_{*}\|_{2}^{2}-2\eta_{t}\cdot(G(\bar{w}_{t })-G(\bar{w}_{*}))+\Theta(1)/t^{2},\quad t\geq t_{0},\]where the equality is by (4), and the inequality is because of the convexity of \(G(\cdot)\), \(|\bar{w}_{t}|\leq\Theta(1)\), and (4). Taking a telescoping sum, we have

\[\sum_{t=t_{0}}^{T}2\eta_{t}\cdot(G(\bar{w}_{t})-G(\bar{w}_{*}))\leq\|\bar{w}_{t_ {0}}-\bar{w}_{*}\|_{2}^{2}-\|\bar{w}_{T+1}-\bar{w}_{*}\|_{2}^{2}+\sum_{t=t_{0}} ^{T}\Theta(1)/t^{2}\leq\Theta(1).\]

Combing the above with (5) and using \(\eta_{t}\approx\Theta(1)/t\) from (4), we get

\[\sum_{t=t_{0}}^{T}\eta_{t}\cdot(G(\bar{w}_{T})-G(\bar{w}_{*}))\leq\sum_{t=t_{0} }^{T}\eta_{t}\cdot(G(\bar{w}_{t})-G(\bar{w}_{*}))+\sum_{t=t_{0}}^{T}\eta_{t} \cdot\Theta(1)/t\leq\Theta(1).\]

Finally, since \(\sum_{t=t_{0}}^{T}\eta_{t}\geq\Theta(1)\cdot(\log(T)-\log(t_{0}))\) according to (4), we get that \(G(\bar{w}_{T})-G(\bar{w}_{*})\leq\Theta(1)/(\log(T)-\log(t_{0}))\).

Step 6: risk convergence.The long-term risk convergence result can be easily established by making use of the implicit bias results we have obtained so far.

## 6 Conclusion

We consider constant-stepsize GD for logistic regression on linearly separable data. We show that with _any_ constant stepsize, GD minimizes the logistic loss; moreover, the GD iterates tend to infinity when projected to a max-margin direction and tend to a fixed minimizer of a strongly convex potential when projected to the orthogonal complement of the max-margin direction. We also show that GD with a large stepsize may diverge catastrophically if the logistic loss is replaced by the exponential loss. Our theory explains how GD minimizes a risk non-monotonically.

## Acknolwdgement

We thank the anonymous reviewers for their helpful comments and Alexander Tsigler for pointing out several typos. VB is partially supported by the Ministry of Trade, Industry and Energy(MOTIE) and Korea Institute for Advancement of Technology (KIAT) through the International Cooperative R&D program. JDL acknowledges the support of the ARO under MURI Award W911NF-11-1-0304, the Sloan Research Fellowship, NSF CCF 2002272, NSF IIS 2107304, NSF CIF 2212262, ONR Young Investigator Award, and NSF CAREER Award 2144994.

## References

* Ahn et al. (2022) Kwangjun Ahn, Sebastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang. Learning threshold neurons via the" edge of stability". _arXiv preprint arXiv:2212.07469_, 2022a.
* Ahn et al. (2022) Kwangjun Ahn, Jingzhao Zhang, and Suvrit Sra. Understanding the unstable convergence of gradient descent. In _International Conference on Machine Learning_, pages 247-257. PMLR, 2022b.
* Andriushchenko et al. (2023) Maksym Andriushchenko, Aditya Vardhan Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Sgd with large step sizes learns sparse features. In _International Conference on Machine Learning_, pages 903-925. PMLR, 2023.
* Arora et al. (2022) Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on the edge of stability in deep learning. In _International Conference on Machine Learning_, pages 948-1024. PMLR, 2022.
* Bartlett et al. (2017) Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. _Advances in neural information processing systems_, 30, 2017.
* Bartlett et al. (2022) Peter L Bartlett, Philip M Long, and Olivier Bousquet. The dynamics of sharpness-aware minimization: Bouncing across ravines and drifting towards wide minima. _arXiv preprint arXiv:2210.01513_, 2022.
* Bartlett et al. (2017)* Chen and Bruna (2022) Lei Chen and Joan Bruna. On gradient descent convergence beyond the edge of stability. _arXiv preprint arXiv:2206.04172_, 2022.
* Cohen et al. (2021) Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=jh-rTtvkGeM](https://openreview.net/forum?id=jh-rTtvkGeM).
* Damian et al. (2022) Alex Damian, Eshaan Nichani, and Jason D. Lee. Self-stabilization: The implicit bias of gradient descent at the edge of stability. In _OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)_, 2022. URL [https://openreview.net/forum?id=enoU_Kp7Dz](https://openreview.net/forum?id=enoU_Kp7Dz).
* Even et al. (2023) Mathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion. (s) gd over diagonal linear networks: Implicit regularisation, large stepsizes and edge of stability. _arXiv preprint arXiv:2302.08982_, 2023.
* Goyal et al. (2017) Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. _arXiv preprint arXiv:1706.02677_, 2017.
* Gunasekar et al. (2017) Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. _Advances in Neural Information Processing Systems_, 30, 2017.
* Gunasekar et al. (2018a) Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. In _International Conference on Machine Learning_, pages 1832-1841. PMLR, 2018a.
* Gunasekar et al. (2018b) Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. _Advances in neural information processing systems_, 31, 2018b.
* Ji and Telgarsky (2018) Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. _arXiv preprint arXiv:1803.07300_, 2018.
* Ji and Telgarsky (2019) Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In _International Conference on Learning Representations_, 2019. URL [https://openreview.net/forum?id=HJflg30qKX](https://openreview.net/forum?id=HJflg30qKX).
* Ji et al. (2021) Ziwei Ji, Nathan Srebro, and Matus Telgarsky. Fast margin maximization via dual acceleration. In _International Conference on Machine Learning_, pages 4860-4869. PMLR, 2021.
* Kong and Tao (2020) Lingkai Kong and Molei Tao. Stochasticity of deterministic gradient descent: Large learning rate for multiscale objective function. _Advances in Neural Information Processing Systems_, 33:2625-2638, 2020.
* Kreisler et al. (2023) Itai Kreisler, Mor Shpigel Nacson, Daniel Soudry, and Yair Carmon. Gradient descent monotonically decreases the sharpness of gradient flow solutions in scalar networks and beyond. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 17684-17744. PMLR, 23-29 Jul 2023.
* Lan (2020) Guanghui Lan. _First-order and stochastic optimization methods for machine learning_, volume 1. Springer, 2020.
* Lewkowycz et al. (2020) Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. _arXiv preprint arXiv:2003.02218_, 2020.
* Liu et al. (2023) Chunrui Liu, Wei Huang, and Richard Yi Da Xu. Implicit bias of deep learning in the large learning rate phase: A data separability perspective. _Applied Sciences_, 13(6):3961, 2023.
* Lyu and Li (2020) Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/forum?id=SJeLIgBKPS](https://openreview.net/forum?id=SJeLIgBKPS).
* Liu et al. (2021)Kaifeng Lyu, Zhiyuan Li, and Sanjeev Arora. Understanding the generalization benefit of normalization layers: Sharpness reduction. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL [https://openreview.net/forum?id=xp5V0BxTxZ](https://openreview.net/forum?id=xp5V0BxTxZ).
* Ma et al. (2022) Chao Ma, Daniel Kunin, Lei Wu, and Lexing Ying. Beyond the quadratic approximation: The multiscale structure of neural network loss landscapes. _Journal of Machine Learning_, 1(3):247-267, 2022. ISSN 2790-2048. doi: [https://doi.org/10.4208/jml.220404](https://doi.org/10.4208/jml.220404). URL [http://global-sci.org/intro/article_detail/jml/21028.html](http://global-sci.org/intro/article_detail/jml/21028.html).
* Mohri et al. (2018) Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of machine learning_. MIT press, 2018.
* Nacson et al. (2019a) Mor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models. In _International Conference on Machine Learning_, pages 4683-4692. PMLR, 2019a.
* Nacson et al. (2019b) Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 3420-3428. PMLR, 2019b.
* Nacson et al. (2019c) Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 3051-3059. PMLR, 2019c.
* Nesterov et al. (2018) Yurii Nesterov et al. _Lectures on convex optimization_, volume 137. Springer, 2018.
* Neyshabur et al. (2018) Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks. In _International Conference on Learning Representations_, 2018. URL [https://openreview.net/forum?id=Skz_WfbCZ](https://openreview.net/forum?id=Skz_WfbCZ).
* Soudry et al. (2018) Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. _The Journal of Machine Learning Research_, 19(1):2822-2878, 2018.
* Wang et al. (2022a) Yuqing Wang, Minshuo Chen, Tuo Zhao, and Molei Tao. Large learning rate tames homogeneity: Convergence and balancing effect. In _International Conference on Learning Representations_, 2022a. URL [https://openreview.net/forum?id=3tbDrs77LJ5](https://openreview.net/forum?id=3tbDrs77LJ5).
* Wang et al. (2022b) Zixuan Wang, Zhouzi Li, and Jian Li. Analyzing sharpness along GD trajectory: Progressive sharpening and edge of stability. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022b. URL [https://openreview.net/forum?id=thgItcOrJ4y](https://openreview.net/forum?id=thgItcOrJ4y).
* Wu et al. (2018) Lei Wu, Chao Ma, et al. How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective. _Advances in Neural Information Processing Systems_, 31, 2018.
* Xing et al. (2018) Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. A walk with sgd. _arXiv preprint arXiv:1802.08770_, 2018.
* Zhu et al. (2023) Xingyu Zhu, Zixuan Wang, Xiang Wang, Mo Zhou, and Rong Ge. Understanding edge-of-stability training dynamics with a minimalist example. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=p7EagBsMAE0](https://openreview.net/forum?id=p7EagBsMAE0).

## Appendix A On the Equivalence between Theorem 4.1(D) and (1)

Note that \(\tilde{\mathbf{w}}\) in (1) contains components in both the max-margin and non-separable subspaces, and we need to disentangle those two components.

Under the coordinate system that defines \(\mathcal{P}\) and \(\bar{\mathcal{P}}\), we can represent a vector \(\mathbf{v}\in\mathbb{R}^{d}\) as

\[\mathbf{v}:=\big{(}\mathcal{P}(\mathbf{v}),\bar{\mathcal{P}}(\mathbf{v})\big{)}.\]

Then for \(i\in\mathcal{S}\), we have

\[y_{i}\cdot\langle\mathbf{x}_{i},\tilde{\mathbf{w}}\rangle =y_{i}\cdot\big{\langle}\big{(}\mathcal{P}(\mathbf{x}_{i}),\bar{ \mathcal{P}}(\mathbf{x}_{i})\big{)},\ \big{(}\mathcal{P}(\tilde{\mathbf{w}}),\bar{ \mathcal{P}}(\tilde{\mathbf{w}})\big{)}\big{\rangle}\] \[=y_{i}\cdot\mathcal{P}(\mathbf{x}_{i})\cdot\mathcal{P}(\tilde{ \mathbf{w}})+y_{i}\cdot\big{\langle}\bar{\mathcal{P}}(\mathbf{x}_{i}),\ \bar{ \mathcal{P}}(\tilde{\mathbf{w}})\big{\rangle}\] since

\[\mathcal{P}\]

 and

\[\bar{\mathcal{P}}\]

 are orthogonal \[=\gamma\mathcal{P}(\tilde{\mathbf{w}})+y_{i}\cdot\big{\langle}\bar{ \mathcal{P}}(\mathbf{x}_{i}),\ \bar{\mathcal{P}}(\tilde{\mathbf{w}})\big{\rangle}.\] since

\[y_{i}\mathcal{P}(\mathbf{x}_{i})=\gamma\text{ for }i\in\mathcal{S}\]

So (1) is equivalent to

\[\text{for every }i\in\mathcal{S},\quad\eta\exp\big{(}-\gamma\bar{\mathcal{P}}( \tilde{\mathbf{w}})\big{)}\cdot\exp\big{(}-y_{i}\cdot\big{\langle}\bar{ \mathcal{P}}(\mathbf{x}_{i}),\ \bar{\mathcal{P}}(\tilde{\mathbf{w}})\big{)}\big{\rangle}= \alpha_{i}.\]

Recall that \(\sum_{i\in\mathcal{S}}\alpha_{i}=1/\gamma^{2}\) according to Definition 1. So focusing on \(\bar{\mathcal{P}}\), the above is equivalent to the following condition on \(\bar{\mathcal{P}}(\tilde{\mathbf{w}})\):

\[\alpha_{i}\propto\exp\big{(}-y_{i}\cdot\big{\langle}\bar{\mathcal{P}}(\mathbf{ x}_{i}),\ \bar{\mathcal{P}}(\tilde{\mathbf{w}})\big{)}\big{\rangle},\ \ i\in \mathcal{S}. \tag{6}\]

Here we ignore a shared normalization factor.

Now, recall from Assumption 3(B) that \((\alpha_{i})_{i\in\mathcal{S}}\) are such that

\[\hat{\mathbf{w}}=\sum_{i\in\mathcal{S}}\alpha_{i}\cdot y_{i}\mathbf{x}_{i}, \quad\alpha_{i}>0.\]

Note that as long as \(\sum_{i\in\mathcal{S}}\alpha_{i}=1/\gamma^{2}\), we have

\[\mathcal{P}(\hat{\mathbf{w}})=\sum_{i\in\mathcal{S}}\alpha_{i}\cdot y_{i} \mathcal{P}(\mathbf{x}_{i})=\gamma\cdot\frac{1}{\gamma^{2}}=\frac{1}{\gamma}.\]

Now consider \(\bar{\mathcal{P}}\). Note that \(\bar{\mathcal{P}}(\hat{\mathbf{w}})=0\) by the choice of \(\bar{\mathcal{P}}\), then apply \(\bar{\mathcal{P}}\) on both sides of the above equation, we get

\[0=\bar{\mathcal{P}}(\hat{\mathbf{w}})=\sum_{i\in\mathcal{S}}\alpha_{i}\cdot y _{i}\bar{\mathcal{P}}(\mathbf{x}_{i}). \tag{7}\]

Under (7), (6) is equivalent to the following condition on \(\bar{\mathcal{P}}(\tilde{\mathbf{w}})\):

\[0=\sum_{i\in\mathcal{S}}\exp\big{(}-y_{i}\cdot\big{\langle}\bar{\mathcal{P}}( \mathbf{x}_{i}),\bar{\mathcal{P}}(\tilde{\mathbf{w}})\big{)}\big{\rangle}\cdot y _{i}\bar{\mathcal{P}}(\mathbf{x}_{i}),\]

which is precisely the first-order condition of

\[\bar{\mathcal{P}}(\tilde{\mathbf{w}})\in\arg\min G(\cdot),\ \text{where}\ G(\mathbf{v}):=\sum_{i\in\mathcal{S}}\exp\big{(}-y_{i}\cdot \big{\langle}\bar{\mathcal{P}}(\mathbf{x}_{i}),\ \mathbf{v}\big{)}\big{)}.\]

Hence we have shown that: the condition that \(\tilde{\mathbf{w}}\) satisfies (1) is equivalent to the condition that \(\bar{\mathcal{P}}(\tilde{\mathbf{w}})\) minimizes the strongly convex potential \(G(\cdot)\).

## Appendix B The Behaviors of Constant-Stepsize GD

### Notation Simplifications

Without loss of generality, we assume that

\[y_{i}=1,\quad i=1,\ldots,n.\]

Otherwise, we replace \(y_{i}\) with \(1\) and \(\mathbf{x}_{i}\) with \(y_{i}\cdot\mathbf{x}_{i}\), respectively, and the following analysis does not change.

Then the risk becomes

\[L(\mathbf{w}):=\sum_{i=1}^{n}\log(1+e^{-\mathbf{w}^{\top}\mathbf{x}_{i}}).\]Rotating the hard-margin SVM solution.Note that the (GD) iterates (under linear models) are rotation equivariant. Specifically, let \(\mathbf{R}\) be an orthogonal matrix, then applying \(\mathbf{R}\) on both sides of (GD), we obtain

\[\mathbf{R}\mathbf{w}_{t+1} =\mathbf{R}\mathbf{w}_{t}+\eta\sum_{i=1}^{n}\left(1-s(\mathbf{x}_{ i}^{\top}\mathbf{w}_{t})\right)\cdot\mathbf{R}\mathbf{x}_{i}\] \[=\mathbf{R}\mathbf{w}_{t}+\eta\sum_{i=1}^{n}\left(1-s((\mathbf{R }\mathbf{x}_{i})^{\top}(\mathbf{R}\mathbf{w}_{t}))\right)\cdot\mathbf{R} \mathbf{x}_{i},\]

which is equivalent to the GD iterates under changes of variables, \(\mathbf{w}\leftarrow\mathbf{R}\mathbf{w}\) and \(\mathbf{x}\leftarrow\mathbf{R}\mathbf{x}\).

Therefore, without loss of generality, we can apply a rotation to the dataset such that \(\hat{\mathbf{w}}\parallel\mathbf{e}_{1}\). Then for \(\mathbf{v}\in\mathbb{R}^{d}\),

\[\mathcal{P}\mathbf{v}=\mathbf{v}[1]\in\mathbb{R},\quad\bar{\mathcal{P}} \mathbf{v}=\mathbf{v}[2:d]\in\mathbb{R}^{d-1}.\]

Slightly abusing notations, in what follows we will write

\[\mathbf{x}_{i}=(x_{i},\;\bar{\mathbf{x}}_{i})^{\top}\in\mathbb{R}\oplus \mathbb{R}^{d-1},\quad i=1,\ldots,n,\]

where

\[x_{i}:=\mathbf{x}_{i}[1]\in\mathbb{R},\quad\bar{\mathbf{x}}_{i}:=\mathbf{x}_{i }[2:d]\in\mathbb{R}^{d-1}.\]

Similarly, we define

\[\mathbf{w}=(w,\;\bar{\mathbf{w}})^{\top}\in\mathbb{R}\oplus\mathbb{R}^{d-1}.\]

Then we have

\[\mathbf{x}_{i}^{\top}\mathbf{w}=x_{i}w_{i}+\bar{\mathbf{x}}_{i}^{\top}\bar{ \mathbf{w}}.\]

So the loss can be written as:

\[L(w,\bar{\mathbf{w}}):=\sum_{i=1}^{n}\log(1+e^{-wx_{i}-\bar{ \mathbf{w}}^{\top}\bar{\mathbf{x}}_{i}}).\]

So (GD) can be written as:

\[\begin{split} w_{0}&=0,\quad w_{t}=w_{t-1}-\eta \cdot\nabla_{w}L(w_{t-1},\bar{\mathbf{w}}_{t-1}),\quad t\geq 1;\\ \bar{\mathbf{w}}_{0}&=0,\quad\bar{\mathbf{w}}_{t}= \bar{\mathbf{w}}_{t-1}-\eta\cdot\nabla_{\bar{\mathbf{w}}}L(w_{t-1},\bar{ \mathbf{w}}_{t-1}),\quad t\geq 1.\end{split} \tag{8}\]

The above two recursions capture the GD iterates projecting to the max-margin and non-separable subspaces, respectively.

### Boundedness of GD in the Non-Separable Subspace

We first show that \((\bar{\mathbf{w}}_{t})_{t\geq 0}\) stay bounded for every fixed stepsize \(\eta\).

**Lemma B.1** (Positiveness of \(w_{t}\)).: _Suppose that Assumption1 holds. Consider \((w_{t})_{t\geq 0}\) defined by (8) with constant stepsize \(\eta>0\). Then for every \(t\geq 0\), it holds that \(w_{t}\geq 0\)._

Proof.: Recall that

\[w_{0}=0,\quad w_{t}=w_{t-1}-\eta\cdot\nabla_{w}L(w_{t-1},\bar{ \mathbf{w}}_{t-1}),\quad t\geq 1.\]

We only need to show that \(\nabla_{w}L(w,\bar{\mathbf{w}})\leq 0\). This is because

\[\nabla_{w}L(w,\bar{\mathbf{w}}) =-\sum_{i=1}^{n}\frac{1}{1+e^{wx_{i}+\bar{\mathbf{w}}^{\top}\bar{ \mathbf{x}}_{i}}}\cdot x_{i}\] \[<0. \text{since $x_{i}\geq\gamma>0$ by Definition 1}\]

**Lemma B.2** (A recursion of \(\|\bar{\mathbf{w}}_{t}\|_{2}\)).: _Suppose that Assumptions 1, 2, and 3 hold. Consider \((\bar{\mathbf{w}}_{t})_{t\geq 0}\) defined by (8) with constant stepsize \(\eta>0\). Then for every \(t\geq 0\), there exists \(j\in[n]\) such that_

\[\|\bar{\mathbf{w}}_{t+1}\|_{2}^{2}\leq\|\bar{\mathbf{w}}_{t}\|_{2}^{2}+2\eta e^ {-w_{t}\gamma}\cdot\bigg{(}n-\frac{b\cdot\|\bar{\mathbf{w}}_{t}\|_{2}}{4} \bigg{)}+\frac{\eta}{1+e^{w_{t}x_{j}+\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x }}_{j}}}\cdot\big{(}\eta n^{2}-b\cdot\|\bar{\mathbf{w}}_{t}\|_{2}\big{)}.\]

_As a direct consequence,_

\[\|\bar{\mathbf{w}}_{t}\|_{2}\geq\max\{4n/b,\;\eta n^{2}/b\}\quad\text{implies that}\quad\|\bar{\mathbf{w}}_{t+1}\|_{2}\leq\|\bar{\mathbf{w}}_{t}\|_{2}.\]

Proof.: We first make a few useful notations. Fix a time index \(t\).

* Let \(k\) be the index of the "most negatively classified" support sample, i.e., \[k:=\arg\min_{i\in\mathcal{S}}\{\langle\bar{\mathbf{w}}_{t},\bar{\mathbf{x}}_{i }\rangle\},\] then by Definition 2 it holds that \[\langle\bar{\mathbf{w}}_{t},\bar{\mathbf{x}}_{k}\rangle\leq-b\cdot\|\bar{ \mathbf{w}}_{t}\|_{2}.\] (9)
* Let \(j\) be the index of the "most negatively classified" sample, i.e., \[j:=\arg\min_{1\leq i\leq n}\{w_{t}x_{i}+\langle\bar{\mathbf{w}}_{t},\bar{ \mathbf{x}}_{i}\rangle\}.\] Then \[w_{t}x_{j}+\langle\bar{\mathbf{w}}_{t},\bar{\mathbf{x}}_{j}\rangle\leq w_{t}x_{ i}+\langle\bar{\mathbf{w}}_{t},\bar{\mathbf{x}}_{i}\rangle\text{ for every }i\in[n].\] (10) In particular, we must have \[\langle\bar{\mathbf{w}}_{t},\bar{\mathbf{x}}_{j}\rangle\leq-b\|\bar{\mathbf{w}}_{t}\|_{2},\] (11) since \[w_{t}\gamma+\langle\bar{\mathbf{w}}_{t},\bar{\mathbf{x}}_{j}\rangle \leq w_{t}x_{j}+\langle\bar{\mathbf{w}}_{t},\bar{\mathbf{x}}_{j}\rangle \text{ by Definition 1}\] \[\leq\min_{i\in\mathcal{S}}\{w_{t}x_{i}+\langle\bar{\mathbf{w}}_{t },\bar{\mathbf{x}}_{i}\rangle\} \text{ by (\ref{eq:main_t})}\] \[=w_{t}\gamma+\min_{i\in\mathcal{S}}\{\langle\bar{\mathbf{w}}_{t}, \bar{\mathbf{x}}_{i}\rangle\} \text{ by Definition 1}\] \[\leq w_{t}\gamma-b\|\bar{\mathbf{w}}_{t}\|_{2}. \text{ by Definition 2}\] We remark that it is possible that \[k=j\].

Step 0: an iterate norm recursion.Recall that

\[\bar{\mathbf{w}}_{t+1}=\bar{\mathbf{w}}_{t}-\eta\nabla_{\bar{\mathbf{w}}}L(w_ {t},\bar{\mathbf{w}}_{t}),\quad\nabla_{\bar{\mathbf{w}}}L(w_{t},\bar{\mathbf{ w}}_{t})=-\sum_{i=1}^{n}\frac{1}{1+e^{w_{t}x_{i}+\bar{\mathbf{w}}_{t}^{\top}\bar{ \mathbf{x}}_{i}}}\cdot\bar{\mathbf{x}}_{i}.\]

Then

\[\|\bar{\mathbf{w}}_{t+1}\|_{2}^{2}=\|\bar{\mathbf{w}}_{t}\|_{2}^{2}-2\eta\cdot \langle\bar{\mathbf{w}}_{t},\nabla_{\bar{\mathbf{w}}}L(w_{t},\bar{\mathbf{w}}_ {t})\rangle+\eta^{2}\cdot\big{\|}\nabla_{\bar{\mathbf{w}}}L(w_{t},\bar{ \mathbf{w}}_{t})\big{\|}_{2}^{2}.\]

Step 1: gradient norm bounds.By definition, we have

\[\big{\|}\nabla_{\bar{\mathbf{w}}}L(w_{t},\bar{\mathbf{w}}_{t}) \big{\|}_{2} =\Big{\|}\sum_{i=1}^{n}\frac{1}{1+e^{w_{t}x_{i}+\bar{\mathbf{w}}_{ t}^{\top}\bar{\mathbf{x}}_{i}}}\cdot\bar{\mathbf{x}}_{i}\Big{\|}_{2}\] \[\leq\sum_{i=1}^{n}\frac{1}{1+e^{w_{t}x_{i}+\bar{\mathbf{w}}_{t}^{ \top}\bar{\mathbf{x}}_{i}}}\cdot\|\bar{\mathbf{x}}_{i}\|_{2}\] \[\leq\sum_{i=1}^{n}\frac{1}{1+e^{w_{t}x_{i}+\bar{\mathbf{w}}_{t}^{ \top}\bar{\mathbf{x}}_{i}}}\text{ by Assumption 2}\] \[\leq\frac{n}{1+e^{w_{t}x_{j}+\bar{\mathbf{w}}_{t}^{\top}\bar{ \mathbf{x}}_{j}}}\text{ by (\ref{eq:main_t})} \tag{12}\]\[\leq n. \tag{13}\]

Here, we use a property of logistic loss that the gradient is uniformly bounded. Therefore, we have

\[\big{\|}\nabla_{\bar{\mathbf{w}}_{t}}L(w_{t},\bar{\mathbf{w}}_{t}) \big{\|}_{2}^{2} \leq\bigg{(}\frac{n}{1+e^{w_{t}x_{j}+\bar{\mathbf{w}}_{t}^{\top} \bar{\mathbf{x}}_{j}}}\bigg{)}\cdot\big{\|}\nabla_{\bar{\mathbf{w}}_{t}}L(w_{t},\bar{\mathbf{w}}_{t})\big{\|}_{2} \text{by \eqref{eq:def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def }} \tag{12}\] \[\leq\frac{n^{2}}{1+e^{w_{t}x_{j}+\bar{\mathbf{w}}_{t}^{\top}\bar{ \mathbf{x}}_{j}}}. \tag{14}\]

Step 2: cross-term bounds.We aim to show that the negative parts in the cross-term can cancel both the positve parts in the cross-term and the squared gradient norm term.

Note that the following holds for either \(j=k\) or \(j\neq k\):

\[-\langle\bar{\mathbf{w}}_{t},\nabla_{\bar{\mathbf{w}}_{t}}L(w_{t },\bar{\mathbf{w}}_{t})\rangle \tag{15}\] \[=\sum_{i=1}^{n}\frac{1}{1+e^{w_{t}x_{i}+\bar{\mathbf{w}}_{t}^{ \top}\bar{\mathbf{x}}_{i}}}\cdot\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{i}\] \[\leq\sum_{\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{i}>0}\frac {1}{1+e^{w_{t}x_{i}+\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{i}}}\cdot \bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{i}\] \[\qquad+\frac{1}{2}\cdot\frac{1}{1+e^{w_{t}x_{j}+\bar{\mathbf{w}}_ {t}^{\top}\bar{\mathbf{x}}_{j}}}\cdot\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x }}_{j}+\frac{1}{2}\cdot\frac{1}{1+e^{w_{t}x_{k}+\bar{\mathbf{w}}_{t}^{\top} \bar{\mathbf{x}}_{k}}}\cdot\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{k}.\]

The first term in (15) can be bounded by

\[\sum_{\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{i}>0}\frac{1}{ 1+e^{w_{t}x_{i}+\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{i}}}\cdot\bar{ \mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{i}\] (16) \[=\sum_{\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{i}>0}\frac{e^ {-w_{t}x_{i}}}{1+e^{-w_{t}x_{i}-\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{i} }}\cdot e^{-\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{i}}\cdot\bar{\mathbf{ w}}_{t}^{\top}\bar{\mathbf{x}}_{i}\] \[\leq\sum_{\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{i}>0}\frac {e^{-w_{t}x_{i}}}{1+e^{-w_{t}x_{i}-\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}} _{i}}}\text{since }e^{-t}\cdot t\leq 1\] \[\leq\sum_{\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{i}>0}e^{-w_ {t}x_{i}}\] \[\leq ne^{-\gamma w_{t}}.\] since

\[x_{i}\geq\gamma\text{ for }i\in[n]\]

 (17)

The third term in (15) can be bounded by

\[\frac{1}{2}\cdot\frac{1}{1+e^{w_{t}x_{k}+\bar{\mathbf{w}}_{t}^{ \top}\bar{\mathbf{x}}_{k}}}\cdot\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_ {k} \leq\frac{1}{2}\cdot\frac{-b\cdot\|\bar{\mathbf{w}}_{t}\|_{2}}{1+ e^{w_{t}\gamma+\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{k}}} \text{by \eqref{eq:def_def_def_def_def_def_def_def_def_def_def_def_def_def_def } and the choice of }k\] (18) \[=\frac{-b\cdot\|\bar{\mathbf{w}}_{t}\|_{2}}{2}\cdot\frac{e^{-w_{t} \gamma}}{e^{-w_{t}\gamma}+e^{\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{k}}}\] \[\leq\frac{-b\cdot\|\bar{\mathbf{w}}_{t}\|_{2}}{2}\cdot\frac{e^{- w_{t}\gamma}}{2},\] since

\[e^{-w_{t}\gamma},e^{\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{k}}\leq 1 \tag{19}\]

since

\[w_{t}\gamma \geq 0, \text{by Lemma B.1}\] \[\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{k} \leq 0. \text{by the choice of }k\]

Now, bringing (16), (17), and (18) into (15), we obtain

\[-\langle\bar{\mathbf{w}}_{t},\nabla_{\bar{\mathbf{w}}_{t}}L(w_{t },\bar{\mathbf{w}}_{t})\rangle\leq e^{-w_{t}\gamma}\cdot\bigg{(}n-\frac{b \cdot\|\bar{\mathbf{w}}_{t}\|_{2}}{4}\bigg{)}-\frac{b\cdot\|\bar{\mathbf{w}}_{ t}\|_{2}}{2}\cdot\frac{1}{1+e^{w_{t}x_{j}+\bar{\mathbf{w}}_{t}^{\top}\bar{ \mathbf{x}}_{j}}}. \tag{20}\]

Here, we use a property of logistic loss that the gradients from incorrectly classified samples dominate the gradients from correctly classified samples.

Step 3: iterate norm recursion bounds.Using (14) and (19), we can obtain

\[\|\bar{\mathbf{w}}_{t+1}\|_{2}^{2} =\|\bar{\mathbf{w}}_{t}\|_{2}^{2}-2\eta\cdot\langle\bar{\mathbf{w}}_ {t},\nabla_{\bar{\mathbf{w}}}L(w_{t},\bar{\mathbf{w}}_{t})\rangle+\eta^{2}\cdot \left\|\nabla_{\bar{\mathbf{w}}}L(w_{t},\bar{\mathbf{w}}_{t})\right\|_{2}^{2}\] \[\leq\|\bar{\mathbf{w}}_{t}\|_{2}^{2}+2\eta e^{-w_{t}\gamma}\cdot \left(n-\frac{b\cdot\|\bar{\mathbf{w}}_{t}\|_{2}}{4}\right)-\eta b\cdot\|\bar{ \mathbf{w}}_{t}\|_{2}\cdot\frac{1}{1+e^{w_{t}x_{j}+\bar{\mathbf{w}}_{t}^{\top }\bar{\mathbf{x}}_{j}}}\] \[\qquad+\eta^{2}\cdot\frac{n^{2}}{1+e^{w_{t}x_{j}+\bar{\mathbf{w}} _{t}^{\top}\bar{\mathbf{x}}_{j}}}\] \[=\|\bar{\mathbf{w}}_{t}\|_{2}^{2}+2\eta e^{-w_{t}\gamma}\cdot \left(n-\frac{b\cdot\|\bar{\mathbf{w}}_{t}\|_{2}}{4}\right)+\frac{\eta}{1+e^{w _{t}x_{j}+\bar{\mathbf{w}}_{t}^{\top}\bar{\mathbf{x}}_{j}}}\cdot\left(\eta n^{ 2}-b\cdot\|\bar{\mathbf{w}}_{t}\|_{2}\right).\]

We have completed the proof. 

**Lemma B.3** (Boundedness of \(\bar{\mathbf{w}}\)).: _Suppose that Assumptions 1, 2, and 3 hold. Consider \((\bar{\mathbf{w}}_{t})_{t\geq 0}\) defined by (8) with constant stepsize \(\eta>0\). Then for every \(t\geq 0\), it holds that_

\[\|\bar{\mathbf{w}}_{t}\|_{2}\leq W_{\max}:=\max\{4n/b,\eta n^{2}/b\}+\eta n.\]

Proof.: We prove the claim by induction. Clearly, \(\|\bar{\mathbf{w}}_{0}\|_{2}=0\leq\max\{4n/b,\eta n^{2}/b\}+\eta n\). Now suppose that

\[\|\bar{\mathbf{w}}_{t}\|_{2}\leq\max\{4n/b,\eta n^{2}/b\}+\eta n,\]

and discuss the following two cases:

1. If \(\|\bar{\mathbf{w}}_{t}\|_{2}\leq\max\{4n/b,\eta n^{2}/b\}\), then \[\|\bar{\mathbf{w}}_{t+1}\|_{2} \leq\|\bar{\mathbf{w}}_{t}\|_{2}+\|\eta\cdot\nabla_{\bar{\mathbf{ w}}}L(w_{t},\bar{\mathbf{w}}_{t})\|_{2} \text{by triangle inequality}\] \[\leq\|\bar{\mathbf{w}}_{t}\|_{2}+\eta n \text{by (\ref{eq:w-1})}\] \[\leq\max\{4n/b,\eta n^{2}/b\}+\eta n.\]
2. Else, we have \[\max\{4n/b,\eta n^{2}/b\}\leq\|\bar{\mathbf{w}}_{t}\|_{2}\leq\max\{4n/b,\eta n ^{2}/b\}+\eta n,\] which implies \[\|\bar{\mathbf{w}}_{t+1}\|_{2} \leq\|\bar{\mathbf{w}}_{t}\|_{2} \text{by Lemma B.2}\] \[\leq\max\{4n/b,\eta n^{2}/b\}+\eta n.\]

This completes the induction. 

### Divergence of GD in the Max-Margin Subspace

**Definition 3** (Some loss measurements in the non-separable subspace).: Under Assumptions 1, 2, and 3, we define the following notations:

1. Define two loss functions \[G(\bar{\mathbf{w}}):=\sum_{i\in\mathcal{S}}e^{-\bar{\mathbf{w}}^{\top}\bar{ \mathbf{x}}_{i}},\qquad H(\bar{\mathbf{w}}):=\sum_{i\notin\mathcal{S}}e^{- \bar{\mathbf{w}}^{\top}\bar{\mathbf{x}}_{i}}.\] In the case where \(\mathcal{S}=[n]\), we define \(H(\bar{\mathbf{w}})=0\).
2. Define \[G_{\min}:=\min_{\bar{\mathbf{w}}\in\mathbb{R}^{d-1}}G(\bar{\mathbf{w}}),\] It is clear that \(G_{\min}\geq 1\) since \((\bar{\mathbf{x}}_{i})_{i\in\mathcal{S}}\) are non-separable by Definition 2.
3. Define \[\bar{\mathbf{w}}_{*}:=\arg\min_{\bar{\mathbf{w}}\in\mathbb{R}^{d-1}}G(\bar{ \mathbf{w}}).\] It is clear that \(G(\bar{\mathbf{w}}_{*})=G_{\min}\). Moreover, it holds that \(\|\bar{\mathbf{w}}_{*}\|_{2}\leq W_{\max}\) by Lemma B.4.

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_FAIL:19]

**Lemma B.7** (An upper bound on \(w_{t}\)).: _Suppose Assumptions 1, 2, and 3 hold. Then it holds that_

\[w_{t}\leq\frac{1}{\gamma}\cdot\log\Big{(}\big{(}e\eta\gamma^{2}G_{\max}+e\eta \gamma H_{\max}\big{)}\cdot(t+1)\Big{)},\quad t\geq 0.\]

_As a direct consequence, it holds that_

\[e^{-\gamma w_{t}}\geq\frac{1}{\big{(}e\eta\gamma^{2}G_{\max}+e\eta\gamma H_{ \max}\big{)}\cdot(t+1)},\quad t\geq 0.\]

Proof.: Observe that

\[w_{t+1}-w_{t} \leq\eta\gamma\cdot e^{-\gamma w_{t}}\cdot G(\bar{\mathbf{w}}_{t })+\eta\cdot e^{-\theta w_{t}}\cdot H(\bar{\mathbf{w}}_{t}) \text{by Lemma~{}\ref{lem:w_t}}\] \[\leq\eta\gamma\cdot e^{-\gamma w_{t}}\cdot G(\bar{\mathbf{w}}_{t })+\eta\cdot e^{-\gamma w_{t}}\cdot H(\bar{\mathbf{w}}_{t}) \text{since $\theta>\gamma$ by Definition~{}\ref{lem:w_t}}\] \[\leq\eta\cdot\big{(}\gamma G_{\max}+H_{\max}\big{)}\cdot e^{- \gamma w_{t}} \text{by Definition~{}\ref{lem:w_t}} \tag{21}\]

Let

\[t_{0}:=\min\big{\{}t:\gamma\eta\cdot\big{(}\gamma G_{\max}+H_{\max}\big{)}\cdot e ^{-\gamma w_{t}}\leq 1\big{\}}.\]

Recall that \(w_{t}\) is increasing according to (20). So we have

\[\text{for}\;\;t\leq t_{0},\qquad w_{t}\leq\frac{1}{\gamma}\cdot \log\big{(}\eta\gamma^{2}G_{\max}+\eta\gamma H_{\max}\big{)}; \tag{22}\] \[\text{for}\;\;t\geq t_{0},\qquad\gamma\eta\cdot\big{(}\gamma G_{ \max}+H_{\max}\big{)}\cdot e^{-\gamma w_{t}}\leq 1. \tag{23}\]

(21) and (23) together imply that

\[\text{for}\;\;t\geq t_{0},\qquad 0\leq\gamma\cdot\big{(}w_{t+1}-w_{t}\big{)} \leq 1. \tag{24}\]

Then for \(t\geq t_{0}\), we have

\[e^{\gamma w_{t+1}}-e^{\gamma w_{t}} =e^{\gamma w_{t}}\big{(}e^{\gamma(w_{t+1}-w_{t})}-1\big{)}\] \[\leq e^{\gamma w_{t}}\cdot e\cdot\gamma(w_{t+1}-w_{t}) \text{by (\ref{eq:w_t}) and that $e^{t}-1\leq e\cdot t$ for $0\leq t\leq 1$}\] \[\leq e\eta\gamma^{2}G_{\max}+e\eta\gamma H_{\max},\qquad\text{by (\ref{eq:w_t})}.\]

which implies

\[e^{\gamma w_{t}} \leq e^{\gamma w_{t_{0}}}+\big{(}e\eta\gamma^{2}G_{\max}+e\eta \gamma H_{\max}\big{)}\cdot(t-t_{0})\] \[\leq\eta\gamma^{2}G_{\max}+\eta\gamma H_{\max}+\big{(}e\eta\gamma ^{2}G_{\max}+e\eta\gamma H_{\max}\big{)}\cdot(t-t_{0}) \text{by (\ref{eq:w_t})}\] \[\leq\big{(}e\eta\gamma^{2}G_{\max}+e\eta\gamma H_{\max}\big{)} \cdot(t+1).\]

Therefore, for \(t\geq t_{0}\), we have

\[w_{t}\leq\frac{1}{\gamma}\cdot\log\Big{(}\big{(}e\eta\gamma^{2}G_{\max}+e\eta \gamma H_{\max}\big{)}\cdot(t+1)\Big{)}.\]

Note that the above also holds for \(0\leq t\leq t_{0}\) according to (22). We have completed the proof. 

### Convergence of GD in the Non-Separable Subspace

We show that the vanilla gradient on the non-separable subspace, \(\nabla_{\bar{\mathbf{w}}}L(w_{t},\bar{\mathbf{w}}_{t})\), can be understood as the gradient on a modified loss with a rescaling factor, \(e^{-\gamma w_{t}}\nabla G(\bar{\mathbf{w}}_{t})\), ignoring higher order errors.

**Lemma B.8** (Gradients comparison lemma).: _Suppose Assumptions 1, 2, and 3 hold. Then it holds that_

\[\big{\|}\nabla_{\bar{\mathbf{w}}}L(w_{t},\bar{\mathbf{w}}_{t})-e^{-\gamma w_{t} }\cdot\nabla G(\bar{\mathbf{w}}_{t})\big{\|}_{2}\leq e^{-2\gamma w_{t}}\cdot G _{\max}^{2}+e^{-\theta w_{t}}\cdot H_{\max},\quad t\geq 0.\]

_As a direct consequence, for every vector \(\bar{\mathbf{v}}\in\mathbb{R}^{d-1}\), it holds that_

\[\langle\bar{\mathbf{v}},\;\nabla_{\bar{\mathbf{w}}}L(w_{t},\bar{\mathbf{w}}_{t })\rangle\leq e^{-\gamma w_{t}}\cdot\langle\bar{\mathbf{v}},\;\nabla G(\bar{ \mathbf{w}}_{t})\rangle+\|\bar{\mathbf{v}}\|_{2}\cdot\big{(}e^{-2\gamma w_{t} }\cdot G_{\max}^{2}+e^{-\theta w_{t}}\cdot H_{\max}\big{)}.\]

[MISSING_PAGE_EMPTY:21]

\[\leq e^{-\gamma w_{t}}\cdot H_{\max}.\]

Bringing the bounds on the \((\spadesuit)\) and \((\heartsuit)\) into (25), we obtain

\[\big{\|}\nabla_{\bar{\mathbf{w}}}L(w_{t},\bar{\mathbf{w}}_{t})-e^{-\gamma w_{t} }\cdot\nabla G(\bar{\mathbf{w}}_{t})\big{\|}_{2}\leq e^{-2\gamma w_{t}}\cdot G _{\max}^{2}+e^{-\theta w_{t}}\cdot H_{\max},\quad t\geq 0.\]

We have shown the first conclusion. The second conclusion follows from the first conclusion: for every \(\mathbf{v}\in\mathbb{R}^{d-1}\),

\[\langle\bar{\mathbf{v}},\nabla_{\bar{\mathbf{w}}}L(w_{t},\bar{ \mathbf{w}}_{t})\rangle =e^{-\gamma w_{t}}\cdot\langle\bar{\mathbf{v}},\;\nabla G(\bar{ \mathbf{w}}_{t})\rangle+\langle\bar{\mathbf{v}},\;\nabla_{\bar{\mathbf{w}}}L( w_{t},\bar{\mathbf{w}}_{t})-e^{-\gamma w_{t}}\cdot\nabla G(\bar{\mathbf{w}}_{t})\rangle\] \[\leq e^{-\gamma w_{t}}\cdot\langle\bar{\mathbf{v}},\;\nabla G( \bar{\mathbf{w}}_{t})\rangle+\|\bar{\mathbf{v}}\|_{2}\cdot\|\nabla_{\bar{ \mathbf{w}}}L(w_{t},\bar{\mathbf{w}}_{t})-e^{-\gamma w_{t}}\cdot\nabla G(\bar {\mathbf{w}}_{t})\|_{2}\] \[\leq e^{-\gamma w_{t}}\cdot\langle\bar{\mathbf{v}},\;\nabla G( \bar{\mathbf{w}}_{t})\rangle+\|\bar{\mathbf{v}}\|_{2}\cdot\big{(}e^{-2\gamma w _{t}}\cdot G_{\max}^{2}+e^{-\theta w_{t}}\cdot H_{\max}\big{)}.\]

We have completed the proof. 

**Lemma B.9** (A gradient norm bound).: _Suppose Assumptions 1, 2, and 3 hold. Then it holds that_

\[\big{\|}\nabla_{\bar{\mathbf{w}}}L(w_{t},\bar{\mathbf{w}}_{t})\big{\|}_{2}\leq e ^{-\gamma w_{t}}\cdot(G_{\max}+H_{\max}),\quad t\geq 0.\]

Proof.: The inequality is because:

\[\big{\|}\nabla_{\bar{\mathbf{w}}_{t}}L(w_{t},\bar{\mathbf{w}}_{t}) \big{\|}_{2} =\Big{\|}\sum_{i=1}^{n}\frac{e^{-w_{t}x_{i}-\bar{\mathbf{w}}_{t}^{ \top}\bar{\mathbf{x}}_{i}}}{1+e^{-w_{t}x_{i}-\bar{\mathbf{w}}_{t}^{\top}\bar{ \mathbf{x}}_{i}}}\cdot\bar{\mathbf{x}}_{i}\Big{\|}_{2}\] \[\leq\sum_{i=1}^{n}\frac{e^{-w_{t}x_{i}-\bar{\mathbf{w}}_{t}^{ \top}\bar{\mathbf{x}}_{i}}}{1+e^{-w_{t}x_{i}-\bar{\mathbf{w}}_{t}^{\top}\bar{ \mathbf{x}}_{i}}

\[\leq\sum_{i\in\mathcal{S}}e^{-\bar{\mathbf{w}}^{\top}\bar{\mathbf{x}}_{i}} \|\mathbf{x}_{i}\|_{2}^{2}\] by triangle inequality \[\leq\sum_{i\in\mathcal{S}}e^{-\bar{\mathbf{w}}^{\top}\bar{\mathbf{x}}_{i}}\] since

\[\|\bar{\mathbf{x}}_{i}\|_{2}\leq 1\]

 by Assumption 2 \[=G(\bar{\mathbf{w}})\]

Recall that \(\|\bar{\mathbf{w}}_{t}\|_{2}\leq W_{\max}\). So we have

\[\sup_{t}\|\nabla^{2}G(\bar{\mathbf{w}}_{t})\|_{2}\leq\sup_{\|\bar{\mathbf{w}} \|_{2}\leq W_{\max}}\|\nabla^{2}G(\bar{\mathbf{w}})\|_{2}\leq\sup_{\|\bar{ \mathbf{w}}\|_{2}\leq W_{\max}}G(\bar{\mathbf{w}})=:G_{\max}. \tag{26}\]

Then we can apply Taylor's theorem to obtain that

\[G(\bar{\mathbf{w}}_{t+1}) \leq G(\bar{\mathbf{w}}_{t})+\langle\nabla G(\bar{\mathbf{w}}_{t} ),\;\bar{\mathbf{w}}_{t+1}-\bar{\mathbf{w}}_{t}\rangle+\frac{G_{\max}}{2}\cdot \|\bar{\mathbf{w}}_{t+1}-\bar{\mathbf{w}}_{t}\|_{2}^{2}\] by (26) \[=G(\bar{\mathbf{w}}_{t})-\eta\cdot\langle\nabla G(\bar{\mathbf{w} }_{t}),\;\nabla_{\bar{\mathbf{w}}}L(w_{t},\bar{\mathbf{w}}_{t})\rangle+\frac{G_ {\max}}{2}\cdot\|\nabla_{\bar{\mathbf{w}}}L(w_{t},\bar{\mathbf{w}}_{t})\|_{2}^{2}.\]

Next we use Lemma B.8 with \(\mathbf{v}=-\nabla G(\bar{\mathbf{w}}_{t})\) to get The cross-term is bounded by

\[-\langle\nabla G(\bar{\mathbf{w}}_{t}),\;\nabla_{\bar{\mathbf{w}}} L(w_{t},\bar{\mathbf{w}}_{t})\rangle\] \[\leq-e^{-\gamma w_{t}}\cdot\left\|\nabla G(\bar{\mathbf{w}}_{t}) \right\|_{2}^{2}+\|\nabla G(\bar{\mathbf{w}}_{t})\|_{2}\cdot\big{(}e^{-2\gamma w _{t}}\cdot G_{\max}^{2}+e^{-\theta w_{t}}\cdot H_{\max}\big{)}\] \[\leq-e^{-\gamma w_{t}}\cdot\left\|\nabla G(\bar{\mathbf{w}}_{t}) \right\|_{2}^{2}+G_{\max}\cdot\big{(}e^{-2\gamma w_{t}}\cdot G_{\max}^{2}+e^{- \theta w_{t}}\cdot H_{\max}\big{)}.\] by (26)

Using the above and the gradient norm bound from Lemma B.9, we get that

\[G(\bar{\mathbf{w}}_{t+1}) \leq G(\bar{\mathbf{w}}_{t})-\eta e^{-\gamma w_{t}}\cdot\left\| \nabla G(\bar{\mathbf{w}}_{t})\right\|_{2}^{2}\] \[\quad+\eta e^{-2\gamma w_{t}}\cdot G_{\max}^{3}+\eta e^{-\theta w _{t}}\cdot G_{\max}\cdot H_{\max}+\eta^{2}e^{-2\gamma w_{t}}\cdot(G_{\max}+H _{\max})^{2}\] \[\leq G(\bar{\mathbf{w}}_{t})+\eta e^{-2\gamma w_{t}}\cdot G_{\max} ^{3}+\eta e^{-\theta w_{t}}\cdot G_{\max}\cdot H_{\max}+\eta^{2}e^{-2\gamma w _{t}}\cdot(G_{\max}+H_{\max})^{2}\] \[\leq G(\bar{\mathbf{w}}_{t})+2(\eta+\eta^{2})\cdot G_{\max}\cdot \big{(}G_{\max}^{2}+H_{\max}^{2}\big{)}\cdot\big{(}e^{-2\gamma w_{t}}+e^{- \theta w_{t}}\big{)},\]

where in the last inequality we use that \(G_{\max}\geq G_{\min}\geq 1\) by Definition 3.

From the above we have

\[G(\bar{\mathbf{w}}_{t+k})\leq G(\bar{\mathbf{w}}_{t})+2(\eta+\eta^{2})\cdot G _{\max}\cdot\big{(}G_{\max}^{2}+H_{\max}^{2}\big{)}\cdot\sum_{s=t}^{s+k} \big{(}e^{-2\gamma w_{s}}+e^{-\theta w_{s}}\big{)}. \tag{27}\]

The summation is small by Lemma B.6, because

\[\sum_{s=t}^{s+k}\big{(}e^{-2\gamma w_{s}}+e^{-\theta w_{s}}\big{)}\] \[\leq\sum_{s=t}^{s+k}\bigg{(}\frac{2}{2+\eta\gamma^{2}\cdot s} \bigg{)}^{2}+\sum_{s=t}^{s+k}\bigg{(}\frac{2}{2+\eta\gamma^{2}\cdot s}\bigg{)} ^{\frac{\theta}{\gamma}}\] by Lemma B.6 \[\leq\bigg{(}\frac{2}{\eta\gamma^{2}}\bigg{)}^{2}\cdot\sum_{s=t} ^{s+k}s^{-2}+\bigg{(}\frac{2}{\eta\gamma^{2}}\bigg{)}^{\frac{\theta}{\gamma}} \cdot\sum_{s=t}^{s+k}s^{-\frac{\theta}{\gamma}}\] \[\leq\bigg{(}\frac{2}{\eta\gamma^{2}}\bigg{)}^{2}\cdot(t-1)^{-1}+ \bigg{(}\frac{2}{\eta\gamma^{2}}\bigg{)}^{\frac{\theta}{\gamma}}\cdot\frac{(t-1) ^{1-\frac{\theta}{\gamma}}}{\frac{\theta}{\gamma}-1}\] by integral inequality \[\leq\max\Bigg{\{}\bigg{(}\frac{2}{\eta\gamma^{2}}\bigg{)}^{2},\; \bigg{(}\frac{2}{\eta\gamma^{2}}\bigg{)}^{\theta/\gamma}\Bigg{\}}\cdot\frac{ \theta}{\theta-\gamma}\cdot\Big{(}(t-1)^{-1}+(t-1)^{1-\frac{\theta}{\gamma}} \Big{)}.\]

Inserting the above into (27) completes the proof. 

We now prove the convergence of the iterates projected on the non-separable subspace.

**Lemma B.11** (Convergence on the non-separable subspace).: _Suppose Assumptions 1, 2, and 3 hold. Then it holds that_

\[G(\bar{\mathbf{w}}_{T})-G(\bar{\mathbf{w}}_{*})\leq\frac{c_{1}}{\log(T)},\quad T \geq 3,\]

_where \(c_{1}>0\) is a polynomial on \(\left\{e^{\eta},e^{n},e^{1/b},\frac{1}{\eta},\frac{1}{\eta-\gamma},\frac{1}{ \gamma},e^{\theta/\gamma}\right\}\) and is independent of \(T\)._

Proof.: The proof is conducted in several steps.

Step 1: one-step function value bound.Observe that

\[\|\bar{\mathbf{w}}_{t+1}-\bar{\mathbf{w}}_{*}\|_{2}^{2} =\|\bar{\mathbf{w}}_{t}-\bar{\mathbf{w}}_{*}\|_{2}^{2}+2\cdot \langle\bar{\mathbf{w}}_{t}-\bar{\mathbf{w}}_{*},\bar{\mathbf{w}}_{t+1}-\bar{ \mathbf{w}}_{t}\rangle+\|\bar{\mathbf{w}}_{t+1}-\bar{\mathbf{w}}_{t}\|_{2}^{2}\] \[=\|\bar{\mathbf{w}}_{t}-\bar{\mathbf{w}}_{*}\|_{2}^{2}-2\eta\cdot \langle\bar{\mathbf{w}}_{t}-\bar{\mathbf{w}}_{*},\nabla_{\bar{\mathbf{w}}}L(w_ {t},\bar{\mathbf{w}}_{t})\rangle+\eta^{2}\cdot\|\nabla_{\bar{\mathbf{w}}}L(w_{ t},\bar{\mathbf{w}}_{t})\|_{2}^{2}.\]

For the cross-term, we apply Lemma B.8 with \(\mathbf{v}=-(\bar{\mathbf{w}}_{t}-\bar{\mathbf{w}}_{*})\) to obtain

\[-\langle\bar{\mathbf{w}}_{t}-\bar{\mathbf{w}}_{*},\nabla_{\bar{ \mathbf{w}}}L(w_{t},\bar{\mathbf{w}}_{t})\rangle\] \[\leq-e^{-\gamma w_{t}}\cdot\langle\bar{\mathbf{w}}_{t}-\bar{ \mathbf{w}}_{*},\nabla G(\bar{\mathbf{w}}_{t})\rangle+\|\bar{\mathbf{w}}_{t}- \bar{\mathbf{w}}_{*}\|_{2}\cdot\left(e^{-2\gamma w_{t}}\cdot G_{\max}^{2}+e^{- \theta w_{t}}\cdot H_{\max}\right)\] \[\leq-e^{-\gamma w_{t}}\cdot\langle\bar{\mathbf{w}}_{t}-\bar{ \mathbf{w}}_{*},\nabla G(\bar{\mathbf{w}}_{t})\rangle+(W_{\max}+\|{\mathbf{w}} _{*}\|_{2})\cdot\left(e^{-2\gamma w_{t}}\cdot G_{\max}^{2}+e^{-\theta w_{t}} \cdot H_{\max}\right)\] \[\leq-e^{-\gamma w_{t}}\cdot\langle\bar{\mathbf{w}}_{t}-\bar{ \mathbf{w}}_{*},\nabla G(\bar{\mathbf{w}}_{t})\rangle+2W_{\max}\cdot\left(e^{-2 \gamma w_{t}}\cdot G_{\max}^{2}+e^{-\theta w_{t}}\cdot H_{\max}\right),\]

where the second inequality is by Lemma B.3, and the last inequality is by Lemma B.4. Using the above and the gradient norm bound from Lemma B.9, we get that

\[\|\bar{\mathbf{w}}_{t+1}-\bar{\mathbf{w}}_{*}\|_{2}^{2} \leq\|\bar{\mathbf{w}}_{t}-\bar{\mathbf{w}}_{*}\|_{2}^{2}-2\eta e ^{-\gamma w_{t}}\cdot\langle\bar{\mathbf{w}}_{t}-\bar{\mathbf{w}}_{*},\nabla G (\bar{\mathbf{w}}_{t})\rangle\] \[\qquad+4\eta\cdot W_{\max}\cdot\left(e^{-2\gamma w_{t}}\cdot G_{ \max}^{2}+e^{-\theta w_{t}}\cdot H_{\max}\right) \tag{28}\] \[\quad+\eta^{2}\cdot e^{-2\gamma w_{t}}\cdot(G_{\max}+H_{\max})^{2}.\]

By the convexity of \(G(\cdot)\), we have

\[\langle\bar{\mathbf{w}}_{t}-\bar{\mathbf{w}}_{*},\nabla G(\bar{ \mathbf{w}}_{t})\rangle\geq G(\bar{\mathbf{w}}_{t})-G(\bar{\mathbf{w}}_{*}). \tag{29}\]

So we get

\[2\eta e^{-\gamma w_{t}}\cdot\left(G(\bar{\mathbf{w}}_{t})-G(\bar{ \mathbf{w}}_{*})\right) \leq 2\eta e^{-\gamma w_{t}}\cdot\langle\bar{\mathbf{w}}_{t}- \bar{\mathbf{w}}_{*},\nabla G(\bar{\mathbf{w}}_{t})\rangle\] by (29) \[\leq\|\bar{\mathbf{w}}_{t}-\bar{\mathbf{w}}_{*}\|_{2}^{2}-\|\bar{ \mathbf{w}}_{t+1}-\bar{\mathbf{w}}_{*}\|_{2}^{2}\] \[\quad+4\eta\cdot W_{\max}\cdot\left(e^{-2\gamma w_{t}}\cdot G_{ \max}^{2}+e^{-\theta w_{t}}\cdot H_{\max}\right)\] \[\quad+\eta^{2}\cdot e^{-2\gamma w_{t}}\cdot(G_{\max}+H_{\max})^{2}\] by (28) \[\leq\|\bar{\mathbf{w}}_{t}-\bar{\mathbf{w}}_{*}\|_{2}^{2}-\|\bar{ \mathbf{w}}_{t+1}-\bar{\mathbf{w}}_{*}\|_{2}^{2}\] \[\quad+6\eta\cdot W_{\max}\cdot\left(G_{\max}^{2}+H_{\max}^{2} \right)\cdot\left(e^{-2\gamma w_{t}}+e^{-\theta w_{t}}\right), \tag{30}\]

where we use \(\eta\leq W_{\max}:=\max\{4n/b,\eta n^{2}/b\}+\eta n\) in the last inequality.

Step 2: the sum of function values stays bounded.Observe that

\[\sum_{t=2}^{T}\left(e^{-2\gamma w_{t}}+e^{-\theta w_{t}}\right) \leq\sum_{t=2}^{T}\left(\frac{2}{2+\eta\gamma^{2}\cdot t}\right) ^{2}+\sum_{t=2}^{T}\left(\frac{2}{2+\eta\gamma^{2}\cdot t}\right)^{\frac{ \theta}{\gamma}}\qquad\text{by Lemma B.6}\] \[\leq\left(\frac{2}{\eta\gamma^{2}}\right)^{2}\cdot\sum_{t=2}^{T} t^{-2}+\left(\frac{2}{\eta\gamma^{2}}\right)^{\frac{\theta}{\gamma}}\cdot\sum_{t=2}^{T} t^{-\frac{\theta}{\gamma}}\] \[\leq\left(\frac{2}{\eta\gamma^{2}}\right)^{2}\cdot 1+\left(\frac{2}{ \eta\gamma^{2}}\right)^{\frac{\theta}{\gamma}}\cdot\frac{1}{\theta/\gamma-1}\] \[\leq\max\left\{\left(\frac{2}{\eta\gamma^{2}}\right)^{2},\,\left( \frac{2}{\eta\gamma^{2}}\right)^{\theta/\gamma}\right\}\cdot\frac{\theta}{\theta- \gamma}. \tag{31}\]

[MISSING_PAGE_EMPTY:25]

Putting these together, we get

\[G(\bar{\mathbf{w}}_{T})-G(\bar{\mathbf{w}}_{*})\leq\left(2W_{\max}+18W_{\max}\cdot c _{0}+\frac{8(1+\eta)c_{0}}{\gamma^{2}}\cdot\frac{3\theta}{\theta-\gamma}\right) \cdot\frac{e\eta\gamma^{2}G_{\max}+e\eta\gamma H_{\max}}{\log(T+1)-\log(3)},\]

where

\[c_{0}:=\eta\cdot\left(G_{\max}^{2}+H_{\max}^{2}\right)\cdot\frac{\theta}{\theta -\gamma}\cdot\max\left\{\left(\frac{2}{\eta\gamma^{2}}\right)^{2},\;\left(\frac {2}{\eta\gamma^{2}}\right)^{\theta/\gamma}\right\}\]

is a polynomial on \(\left\{e^{\eta},e^{n},e^{1/b},\frac{1}{\eta},\frac{1}{\theta-\gamma},\frac{1}{ \gamma},e^{\theta/\gamma}\right\}\). So for \(T\geq 3\), we have

\[G(\bar{\mathbf{w}}_{T})-G(\bar{\mathbf{w}}_{*})\leq\frac{1}{\log(T)}\cdot c_{1},\]

where \(c_{1}\) is a polynomial on \(\left\{e^{\eta},e^{n},e^{1/b},\frac{1}{\eta},\frac{1}{\theta-\gamma},\frac{1} {\gamma},e^{\theta/\gamma}\right\}\) and is independent of \(T\). 

## Appendix C Proofs Missing from the Main Paper

### Proof of Theorem 4.1

Proof of Theorem 4.1.: Theorem 4.1 is a consequence of our analysis in Appendix B.

(C) is because of Lemma B.3.

(B) is because of Lemma B.6.

(D) is because of Lemma B.6.

(A) is because of the following:

\[L(\mathbf{w}_{t}) =\sum_{i=1}^{n}\log(1+\exp(-w_{t}x_{i}-\mathbf{w}_{t}^{\top} \mathbf{x}_{i}))\] \[\leq\sum_{i=1}^{n}\exp(-w_{t}x_{i}-\mathbf{w}_{t}^{\top}\mathbf{x }_{i})\] \[\leq\exp(-w_{t}\cdot\gamma)\cdot\sum_{i=1}^{n}\exp(-\mathbf{w}_{t }^{\top}\mathbf{x}_{i})\] \[\leq c/\log(t),\]

where the last inequality is because that

\[\exp(-w_{t}\cdot\gamma)\leq\frac{2}{2+\eta\gamma^{2}\cdot t}\]

by Lemma B.6 and that \(\sum_{i=1}^{n}\exp(-\mathbf{w}_{t}^{\top}\mathbf{x}_{i})\) is uniformly bounded by a constant by Definition 3. 

### Proof of Theorem 4.2

Proof of Theorem 4.2.: The GD iterates can be written as

\[w_{t+1}=w_{t}+\eta\gamma\cdot e^{-\gamma w_{t}}\cdot\left(e^{- \bar{w}_{t}}+e^{\bar{w}_{t}}\right), \tag{33}\] \[\bar{w}_{t+1}=\bar{w}_{t}-\eta e^{-\gamma w_{t}}\cdot\left(e^{- \bar{w}_{t}}-e^{\bar{w}_{t}}\right). \tag{34}\]

We claim that: for every \(t\geq 0\),

1. \(w_{t}\geq 0\).
2. \(|\bar{w}_{t}|\geq 1\).
3. \(|\bar{w}_{t}|\geq 2\gamma w_{t}\).

[MISSING_PAGE_EMPTY:27]

To show (A), we apply \(w_{t}\to\infty\) and that \(|\bar{w}_{t}|\geq 2\gamma w_{t}\):

\[L(w_{t},\bar{w}_{t}) =e^{-\gamma w_{t}}\cdot\left(e^{-\bar{w}_{t}}+e^{\bar{w}_{t}}\right)\] \[\geq e^{-\gamma w_{t}}\cdot e^{|\bar{w}_{t}|}\] \[\geq e^{\gamma w_{t}}\to\infty.\]

We have completed all the proofs. 

## Appendix D Experimental Setups

Neural network experiments.We randomly sample \(1,000\) data from the MNIST3 dataset as the training set and use the remaining data as the test set. The feature vectors are normalized such that each feature is within \([-1,1]\).

Footnote 3: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)

We use a fully connected network with the following structure

\[784\to\texttt{ReLU}\to 500\to\texttt{ReLU}\to 500\to\texttt{ReLU}\to 500\to\texttt{ReLU}\to 10.\]

The network is initialized with Kaiming initialization. We use the cross-entropy loss.

We consider constant-stepsize GD with two types of stepsizes, \(\eta=0.1\) and \(\eta=0.01\).

The results are presented in Figure 1.

Logistic regression experiments.We randomly sample \(1,000\) data with labels "0" and "8" from the MNIST dataset as the training set. The feature vectors are normalized such that each feature is within \([-1,1]\).

We use a linear model without bias. So the number of parameters is \(784\). The model is initialized from zero. We use the binary cross-entropy loss, i.e., the logistic loss.

We consider constant-stepsize GD with various stepsizes.

The results are presented in Figure 2.

Additional experiments.We conduct additional experiments on the margin maximization effect of large stepsize GD on a homogenous neural network. Results are presented in Figure 3.

Figure 3: The behaviors of GD for optimizing a homogenous neural network. The experiment setting follows that of Figure 1 in the main paper, except that we disable all bias parameters so that the neural network is homogenous (Lyu and Li, 2020). The sub-figures (a) and (b) report the training loss and the margin along the GD trajectories, respectively. Here, we follow Lyu and Li (2020) and measure the normalized margin by \(\min_{(\mathbf{x},\mathbf{y})}\big{(}f(\mathbf{x};\mathbf{w}_{t})[y]-\max_{y^ {\prime}\neq y}f(\mathbf{x};\mathbf{w}_{t})[y^{\prime}]\big{)}/\|\mathbf{w}_{t }\|_{2}^{L}\), where \(f(\cdot;\cdot)\) refers to the homogenous neural network, \(L=4\) is the depth of the homogenous neural network, \(\mathbf{w}_{t}\) is the weight at the \(t\)-th GD step, and \(\min_{(\mathbf{x},y)}\) is taken with respect to all training data.