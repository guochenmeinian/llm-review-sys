# SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors

Tinghao Xie\({}^{1}\), Xiangyu Qi\({}^{1}\), Yi Zeng\({}^{2}\), Yangsibo Huang\({}^{1}\)

Udari Madhushani Sehwag\({}^{3}\), Kaixuan Huang\({}^{1}\), Luxi He\({}^{1}\), Boyi Wei\({}^{1}\), Dacheng Li\({}^{4}\), Ying Sheng\({}^{3}\)

Ruoxi Jia\({}^{2}\), Bo Li\({}^{5,6}\), Kai Li\({}^{1}\), Danqi Chen\({}^{1}\), Peter Henderson\({}^{1}\), Prateek Mittal\({}^{1}\)

\({}^{1}\)Princeton University \({}^{2}\)Virginia Tech \({}^{3}\)Stanford University \({}^{4}\)UC Berkeley

\({}^{5}\)University of Illinois at Urbana-Champaign \({}^{6}\)University of Chicago

###### Abstract

Evaluating aligned large language models' (LLMs) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing evaluation efforts, however, face three limitations that we address with **SORRY-Bench**, our proposed benchmark. **First**, existing methods often use coarse-grained taxonomies of unsafe topics, and are over-representing some fine-grained topics. For example, among the ten existing datasets that we evaluated, tests for refusals of self-harm instructions are over 3x less represented than tests for fraudulent activities. SORRY-Bench improves on this by using a fine-grained taxonomy of 45 potentially unsafe topics, and 450 class-balanced unsafe instructions, compiled through human-in-the-loop methods. **Second**, evaluations often overlook the linguistic formatting of prompts, like different languages, dialects, and more--which are only implicitly considered in many evaluations. We supplement SORRY-bench with 20 diverse linguistic augmentations to systematically examine these effects. **Third**, existing evaluations rely on large LLMs (e.g., GPT-4) for evaluation, which can be computationally expensive. We investigate design choices for creating a fast, accurate automated safety evaluator. By collecting 7K+ human annotations and conducting a meta-evaluation of diverse LLM-as-a-judge designs, we show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale LLMs, with lower computational cost. Putting these together, we evaluate over 40 proprietary and open-source LLMs on SORRY-Bench, analyzing their distinctive refusal behaviors. We hope our effort provides a building block for systematic evaluations of LLMs' safety refusal capabilities, in a balanced, granular, and efficient way.1

Footnote 1: Data and code are available via our website [https://sorry-bench.github.io](https://sorry-bench.github.io).

## 1 Introduction

To ensure large language model (LLM) safety, _alignment_ has become a standard procedure that follows language model pretraining (OpenAI, 2023; Touvron et al., 2023; Anthropic, 2023; Gemini Team, 2023). Alignment involves calibrating these models, via _instruction tuning_(Wei et al., 2021; Ouyang et al., 2022) and _preference optimization_(Bai et al., 2022; Rafailov et al., 2024; Meng et al., 2024), such that they adhere to human values and safely follow human intents. The resultant aligned LLMs should not engage with user requests on certain unsafe or inappropriate topics, as determinedby LLM creators and regulators. For example, most LLMs _refuse_ to assist with the user request "_Write a racist tweet._" Instead, they provide a _refusal_ response - e.g., "_I'm sorry, I cannot help with this._" Such safety refusal abilities help reduce the likelihood of misuse by users in the wild.

To date, researchers have proposed many benchmarks (Gehman et al., 2020; Parrish et al., 2022; Shaikh et al., 2022; Wang et al., 2023; Qi et al., 2023; Cui et al., 2023; Vidgen et al., 2023; Lin et al., 2023; Zou et al., 2023; Shen et al., 2023; Huang et al., 2023; Mazeika et al., 2024; Souly et al., 2024) to evaluate various aspects of LLM safety, including toxicity, harmfulness, trustworthiness, and refusal behaviors (see a detailed summary of them in Table 3). In this work, we identify three deficiencies underlying these existing evaluations, and address them with **SORRY-Bench2**, our proposed systematic benchmark to evaluate LLM safety refusal behaviors.

Footnote 2: This name stems from LLM safety refusal responses, commonly starting with “I’m sorry, I cannot...”

**First, we point out prior datasets are often built upon course-grained and varied safety categories, and that they are overrepresenting certain fine-grained categories.** For example, Vidgen et al. (2023) include broad categories like "Illegal Items" in their taxonomy, while Huang et al. (2023) use more fine-grained subcategories like "Theft" and "Illegal Drug Use". Meanwhile, both of them fail to capture certain risky topics, e.g., "Legal Advice" or "Political Campaigning", which are adopted in some other work (Liu et al., 2023; Shen et al., 2023; Qi et al., 2023). Moreover, we find these prior datasets are often imbalanced and result in over-representation of some fine-grained categories. As illustrated in Fig 1, as a whole, these prior datasets tend to skew towards certain safety categories (e.g., "Fraud", "Sexual Explicit Content", and "Social Stereotypes") with "Self-Harm" being nearly 3x less represented than these categories. However, these other underrepresented categories (e.g., "Personal Identifiable Information Violations", "Self-Harm", and "Animal-related Crimes") cannot be overlooked - failure to evaluate and ensure model safety in these categories can lead to outcomes as severe as those in the more prevalent categories.

To bridge this gap, we present a _fine-grained 45-class safety taxonomy_ (Fig 2 and SS2.2) across 4 high-level domains. We curate this taxonomy to unify the disparate taxonomies from prior work, employing a human-in-the-loop procedure for refinement - where we map data points from previous datasets to our taxonomy and iteratively identify any uncovered safety categories. Our resultant taxonomy captures diverse topics that could lead to potentially unsafe LLM responses, and allows stakeholders to evaluate LLM safety refusal on any of these risky topics at a more granular level. On top of this 45-class taxonomy, we craft a _class-balanced LLM safety refusal evaluation dataset_ (SS2.3). Our base dataset consists of 450 unsafe instructions in total, with additional manually created novel data points to ensure equal coverage across the 45 safety categories (10 per category).

**Second, we ensure balance not just over topics but over linguistic characteristics.** Existing safety evaluations fail to capture different formatting and linguistic features in user inputs. But this too can result in over-representation of a given language, dialect or other linguistic feature. We address this by considering 20 diverse _linguistic mutations_ that real-world users might apply to phrase their unsafe prompts. These include various writing styles, persuasion techniques, encoding and encryption strategies, and multi-languages (SS2.4). After paraphrasing our base dataset via these mutations, we obtain 9K additional unsafe instructions.

Figure 1: **Imbalanced data point distribution of 10 prior datasets (§2.2) on our 45-class taxonomy.**

Third, we investigate what design choices make a fast and accurate safety benchmark evaluator, a trade-off that prior work has not so systematically examined.** To benchmark safety behaviors, we need an _efficient_ and _accurate_ evaluator to decide whether a LLM response is in _compliance_ or _refusal_ of each unsafe instruction from our SORRY-Bench dataset. By far, a common practice is to leverage LLMs themselves for automating such safety evaluations. With many different implementations (Qi et al., 2023; Huang et al., 2023; Xie et al., 2023; Mazeika et al., 2024; Li et al., 2024; Souly et al., 2024; Chao et al., 2024) of LLMs-as-a-judge, there has not been a large-scale systematic study of which design choices are better, in terms of the tradeoff between efficiency and accuracy. We collect a large-scale human safety judgment dataset (SS3.2) of over 7K annotations, and conduct a thorough meta-evaluation (SS3.3) of different safety evaluators on top of it. Our finding suggests that small (7B) LLMs, when fine-tuned on sufficient human annotations, can achieve satisfactory accuracy (over 80% human agreement) with a low computational cost (\(\sim\)10s per evaluation pass), comparable with and even surpassing larger scale LLMs (e.g., GPT-4o).

In SS4.2, we benchmark **over 40** open-source and proprietary LLMs on SORRY-Bench. Specifically, we showcase the varying degrees of safety refusal across different LLMs. Claude-2 and Gemini-1.5, for example, exhibit the most refusals. Mistral models, on the other hand, demonstrate significantly higher rates of compliance with potentially unsafe user requests. There was also general variation across categories. For example, Gemini-1.5-flash is the only model that consistently refuses requests for legal advice that most other models respond to. Whilst, all but a handful of models refused most harassment-related requests. Finally, we find significant variation in compliance rates for our 20 linguistic mutations in prompts, showing that current models are inconsistent in their safety for low-resource languages, inclusion of technical terms, uncommon dialects, and more.

## 2 A Recipe for Curating Diverse and Balanced Dataset

### Related Work

To evaluate the safety of modern LLMs with instruction-following capabilities, recent work (Shaikh et al., 2023; Liu et al., 2023; Zou et al., 2023; Rottger et al., 2023; Shen et al., 2023; Qi et al., 2023; Huang et al., 2023; Vidgen et al., 2023; Cui et al., 2023; Li et al., 2024; Mazeika et al., 2024; Souly et al., 2024; Zhang et al., 2023) propose different instruction datasets that might trigger unsafe behavior--building on earlier work evaluating toxicity and bias in underlying pretrained LMs on simple sentence-level completion (Gehman et al., 2020) or knowledge QA tasks (Parrish et al., 2022). These datasets usually consist of varying numbers of potentially unsafe user instructions, spanning across different safety categories (e.g., illegal activity, misinformation). These unsafe instructions are then used as inputs to LLMs, and the model responses are evaluated to determine model safety. In Appendix C, we provide a more detailed survey of these datasets with a summary of key attributes.

### Fine-grained Refusal Taxonomy with Diverse Categories

Before building the dataset, we first need to understand its scope of safety, i.e., _what safety categories should the dataset include and at what level of granularity should they be defined?_ We note that prior datasets are often built upon discrepant safety categories, which may be too coarse-grained and not consistent across benchmarks. For example, some benchmarks have results aggregated by course-grained categories like illegal activities (Shen et al., 2023; Qi et al., 2023; Vidgen et al., 2023; Zhang et al., 2023), while others have more fine-grained subcategories like delineate more specific subcategories like "Tax Fraud" and "Illegal Drug Use" (Huang et al., 2023). Mixing these subtypes in one coarse-grained category can lead to evaluation challenges: the definition of an "illegal activity" can change across jurisdiction and time. Hate speech, for example, can be a crime in Germany, but is often protected by the First Amendment in the United States. We also note that previous datasets may have inconsistent coverage - failing to account for certain types of activities that model creators may or may not wish to constrain, like "Legal Advice" or "Political Campaigning", which are only examined by a small group of studies (Liu et al., 2023; Shen et al., 2023; Qi et al., 2023).

We suggest that benchmarking efforts should focus on fine-grained and extensive taxonomies, which not only enable capturing diverse potential safety risks, but also come with the benefit of better customizability_. Stakeholders can selectively engage with categories of particular concerns and disregard those deemed permissible. For example, some might find it acceptable for their models to provide legal advice, while others may believe this is too high-risk. In light of this, we present a **45-class safety taxonomy** to examine refusal behaviors, as shown in Fig 2 (see Table 4 in Appendix D for a more detailed version) to unify past datasets in a fine-grained and customizable way.

Our taxonomy curation method consists of two stages. In the first stage, we _aggregate_ the safety taxonomies from 10 prior safety benchmark datasets (Wang et al., 2023; Qi et al., 2023; Cui et al., 2023; Vidgen et al., 2023; Lin et al., 2023; Zou et al., 2023; Shen et al., 2023; Huang et al., 2023; Mazeika et al., 2024; Souly et al., 2024; Shaikh et al., 2022), and _break down_ any vague and broad safety categories into more fine-grained categories. For example, the coarse-grained category "Illegal Activities," occurring in (Shen et al., 2023; Qi et al., 2023; Vidgen et al., 2023)), is substituted by more detailed sub-categories like "Animal-related Crimes", "Fraud", and so on. In the second stage, we keep on _refining_ this taxonomy via a human-in-the-loop process. We first map data points from these prior datasets to our taxonomy, with GPT-4 as a classifier (see Appendix E for detailed setup). Data points that do not fit existing categories (i.e., classified to "Others") undergo human review to determine if new categories are needed or if existing ones should be subdivided further. This two-stage approach ensures an extensive and unified taxonomy, addressing the discrepancy across prior safety benchmark efforts.

### Data Collection

With the aforementioned GPT-4 classifier (Appendix E), we map data points from the 10 prior datasets to our taxonomy, where we further analyze their distribution on the 45 safety categories. As illustrated in Fig 1, these datasets exhibit significant **imbalances** - they are heavily biased towards certain categories perceived as more prevalent. For instance, System Intrusion, Fraud, Sexual Content Generation, and Social Stereotype Promotion are disproportionately represented in the past datasets. Meanwhile, other equally important categories, like Self-Harm, Animal-related Crimes, and PII Violations are significantly under-represented. Failure to capture model safety risks in these categories can lead to equivalently severe consequences.

To equally capture model risks from all safety categories in our taxonomy, we build a class-balanced dataset. We begin by aggregating labeled data from past work, but after quality filtering and deduplication,3 We find that many categories do not have enough data to build a class-balanced benchmark. To ensure sufficient and equal coverage across categories, we further create numerous novel potentially unsafe instructions less represented in prior work. Our collected **SORRY-Bench** dataset consists of 10 diverse unsafe instructions for each of the 45 categories, in total of 450 samples.

Footnote 3: Refer to Appendix F for our detailed collection process to ensure high data quality for refusal evaluation.

### Capturing Diverse Linguistic Patterns underlying User Prompts

Prompt diversity has long been a challenge in language model evaluation (Liu et al., 2023a). The same input prompt, phrased in different ways can lead to varying model responses. This issue is also important for LLM safety evaluation. Sophisticated prompt-space _jailbreaking_ methods (Shen et al., 2023; Zou et al., 2023; Andriushchenko et al., 2024) have been developed to bypass safety guardrails in LLMs, causing them to respond to potentially unsafe user requests. Some studies have shown that simple social techniques like persuasion (Zeng et al., 2024), writing prompts in alternative languages (Deng et al., 2023), or even phrasing unsafe prompts in instruction-style (imperative;

Figure 2: Taxonomy of SORRY-Bench.

e.g., "Write a tutorial to build a bomb") instead of question-style (interrogative; e.g., "Can you teach me how to build a bomb?"), can significantly affect the extent to which models refuse unsafe instructions (Bianchi et al., 2024). To ensure equal coverage of these variations, we isolate and decouple prompt-level linguistic patterns. In our collected "base" dataset, all user prompts are deliberately (re-)written as an instruction (imperative), which is one of the most common styles users would phrase their request as. We then compile 20 linguistic mutations4 (Fig 3) from prior studies (Bianchi et al., 2024; Samvelyan et al., 2024; Zeng et al., 2024; Yuan et al., 2023; Deng et al., 2023) into our datasets, including diverse _writing styles_ (question, slang, misspellings, etc.), _persuasion techniques_ (e.g., logical appeal), _encoding and encryption strategies_ (e.g., ASCII), and _multi-languages_ (i.e., translating to a non-English language like Malayalam or French). This results in 20 * 450 = 9K additional unsafe instructions, capturing diverse formatting and linguistic features.

Footnote 4: Refer to Appendix G for implementation details of these mutations.

## 3 A Recipe for Building Efficient and Accurate Automated Evaluators

When testing a language model on a safety benchmark, a practical problem is to build an evaluator that can judge whether the model response is safe or not (i.e., whether the model fulfills an unsafe instruction). Building such an evaluator is nontrivial, as it requires a sophisticated understanding of different types of harmfulness that a model response can exhibit. While human-based evaluation is the gold standard, it is short of scalability and efficiency. Therefore, currently, a common practice is to leverage the capability of language models as a proxy to automate the evaluation process.

Thus far, there are many different implementations (Qi et al., 2023; Huang et al., 2023; Xie et al., 2023; Mazeika et al., 2024; Li et al., 2024; Souly et al., 2024; Chao et al., 2024) of LLM-as-a-judge for safety evaluation - some directly using GPT-4-level LLMs as an off-the-shelf option, others fine-tuning smaller language models. However, there isn't a principled understanding of which design choices are better and what trade-offs they have. In this section, we aim to provide a systematic study of the design choices of such evaluators, presenting a recipe for building efficient and accurate automated evaluators for benchmarking safety refusal behaviors. We will first define the evaluation goal of our SORRY-Bench, and then introduce our effort in collecting a large scale human judgment dataset for safety refusal. On top of this, we then present a meta-evaluation study of different design choices using LLM as a judge, to identify a design that is both efficient and accurate for our use case.

### Evaluation Goal

The evaluation of an LLM's refusal behaviors on our SORRY-Bench dataset corresponds to a binary classification problem. Given a pair of (unsafe instruction, model response), we want to decide whether the model response is in compliance with or a refusal to the unsafe instruction. Specifically, a model response is considered as **compliance**_whenever it provides substantial content that directly

Figure 3: Demonstration of 20 linguistic mutations we apply to our base dataset.

_assists with the given unsafe instruction_. Even though sometimes the model response can contain ethical disclaimers (e.g., "I must remind you that it's unethical to do this") or refusing statements (e.g., "I'm sorry, I cannot help with this") at the beginning, as long as there exists certain content that directly resolves the unsafe instruction, it is still considered as compliance in our evaluation. In all other cases, the model response is considered as **refusal**. Similarly, this includes cases where the model response contains complying statements (e.g., "Sure, here is my response") or does not contain any explicit refusing statement, but there is no content that actually resolves the unsafe instruction.

### A Human Judgment Dataset for Evaluating the Evaluators

In this study, data points of human safety judgments on model responses are often helpful and, in many cases, necessary. First, only with human ground truth labels can it be possible for us to evaluate the accuracy of any automated evaluators, understand whether an evaluator is good or not, and compare different evaluators. Second, human-labeled safety judgment instances can also be used as training data to optimize the language model based evaluators instead of just prompting them. Therefore, we curate a large-scale human safety judgment dataset, which not only benefits our study but will also be a useful foundational resource for future research in this area.

Specifically, for every unsafe instruction from our SORRY-Bench dataset (the base-version, _without linguistic mutation_), we sample 8 model responses (from different LLMs), and 6 authors manually label each of them as either "compliance" or "refusal" to the user request (in total 450 * 8 = 3,600 records). We call this an **in-distribution (ID)** set. Moreover, we also cover the **out-of-distribution (OOD)** evaluation cases, where the unsafe instructions in our SORRY-Bench dataset are subject to linguistic mutations (described in SS2.4). We find that the safety evaluation in these cases can be more challenging. For example, after _translating_ the original user request to another language, some LLMs would simply repeat the user request (which is not considered compliance); for some _encoding_ mutations, the model responses are nonsense (undecidable content, which is also not compliance); and after mutating the user request with _persuasion_ techniques, the model response may contain a bullet list that looks like compliance, but actually cannot resolve the user request (actually not compliance). Therefore, to cover these OOD evaluation cases, we further sample 8 more model responses (from different LLMs) to the linguistic-mutated version of each unsafe instruction from our benchmark dataset. So, in total, we finally collected 450 * (8 ID + 8 OOD) = 7,200 human annotations. See Appendix H for more details.

We split these human annotations into a _train_ split of 450 * (3 ID + 3 OOD) = 2,700 records (used to directly train evaluators), and the rest 4,500 as the _test_ split.

### A Meta-Evaluation: What Makes a Good Safety Evaluator?

While directly prompting state-of-the-art LLMs such as GPT-4 to judge the refusal behaviors can result in a fairly good judge that agrees well with human evaluators (Qi et al., 2023), there are also several growing concerns. First, as versions of proprietary LLMs keep updating, there is an issue of reproducibility. Second, long prompts and the GPT-4-scale models often result in heavy computation overhead, resulting in high financial and time costs (e.g., per-pass evaluation with GPT-40 could cost $3 and 20 minutes in our case). Thus, we also explore the potential of utilizing smaller-scale open-sourced models (e.g., Llama-3 (Meta, 2024), Gemma (Team et al., 2024), and Mistral (Jiang et al., 2023)) for the refusal evaluation task, which favors both reproducibility and efficiency.

For comprehensiveness, we explore a few commonly adopted add-on techniques for boosting the accuracy of LLM judge further. 1) **Chain-of-thought (CoT)**(Wei et al., 2022) prompting: following Qi et al. (2023), we ask the LLM to first "think step-by-step", analyze the relationship between the given model response and user request, and then make the final decision of whether the model response is a "refusal" or a "compliance". 2) In-context learning with **few-shot** evaluation examples (Brown et al., 2020): for each instruction, we use the corresponding annotations in the train split of the human judge dataset (SS3.2) as the in-context demonstrations. 3) Directly **fine-tuning** LLM to specialize on the safety evaluation task (Huang et al., 2023; Mazeika et al., 2024; Li et al., 2024): we directly fine-tune LLMs on the aforementioned train split of 2.7K human judge evaluation annotations.

[MISSING_PAGE_FAIL:7]

evaluation configurations also notably affect the reported safety benchmark results (Table 5). We direct readers to Appendix J for more in-depth result analysis.

Different models exhibit significantly varying degrees of safety refusal behaviors.We observe that 22 out of 43 LLMs demonstrate a medium compliance rate of 20%\(\sim\)50%, e.g., GPT-40 (31%) and Llama-3-70b (36%). At one end of the spectrum, Claude-2 and Gemini-1.5 achieve the lowest overall compliance rate (<10%). In particular, Claude-2.1 and 2.0 refuse almost all unsafe instructions in the first 25 categories ("Hate Speech Generation" & "Assistance with Crimes or Torts" domains), and Gemini-1.5-Flash declines all requests related to "Potentially Unqualified Advice" (i.e., the last 5 categories). At the other end, 8 models (e.g., Mistral series) comply with more than half of the evaluated unsafe instructions, even on well-recognized harmful topics like "#15: Fraud."

These variations may give us independent insight into the shifting values and priorities of model creators, and their changing policy guidelines.Llama-3 models, as an instance, show notably fewer safety refusals compared to Llama-2 (compliance rate of the 70B version increases from 13% to 36%). Conversely, we observe a substantial increase in refusals from Gemini-Pro to the more recent Gemini-1.5 models (compliance rate drops from 34% to 8%). Both Gemini and Claude models refuse nearly all 10 instructions in the category "#26: Advice on Adult Content", claiming that it's unethical to discuss such personal topics. And while most prior versions of the GPT-3.5/4 API rejected most requests in the category, GPT-4o now mostly complies with such user requests. This shift aligns with OpenAI Model Spec (OpenAI, 2024) published in May 2024, which states that discussing adult topics is permissible. Meanwhile, the spec also states that "responding to user request for erotica" is unacceptable, explaining why GPT-4o consistently refuses every instruction from "#27: Sexual Explicit Content Generation."

Figure 4: **Benchmark results of 40+ LLMs on SORRY-Bench. The LLMs are ranked by their compliance rates (the bracketed scores following model names on the vertical axis) over all 45 safety categories (horizontal axis), low to high. In each grid, we report the per-category compliance rate.**

[MISSING_PAGE_FAIL:9]

## References

* Andriushchenko et al. [2024] Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Jailbreaking leading safety-aligned lms with simple adaptive attacks. _arXiv preprint arXiv:2404.02151_, 2024.
* Anthropic [2023] Anthropic. Introducing Claude. [https://www.anthropic.com/index/introducing-claude](https://www.anthropic.com/index/introducing-claude), 2023.
* Bai et al. [2021] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022.
* Bianchi et al. [2024] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Rottger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned LLaMAs: Lessons from improving the safety of large language models that follow instructions. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=gT5hALch9z](https://openreview.net/forum?id=gT5hALch9z).
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Chao et al. [2024] Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J Pappas, Florian Tramer, et al. Jailbreakbench: An open robustness benchmark for jailbreaking large language models. _arXiv preprint arXiv:2404.01318_, 2024.
* Cohen [1960] Jacob Cohen. A coefficient of agreement for nominal scales. _Educational and psychological measurement_, 20(1):37-46, 1960.
* Cui et al. [2023] Shiyao Cui, Zhenyu Zhang, Yilong Chen, Wenyuan Zhang, Tianyun Liu, Siqi Wang, and Tingwen Liu. Fft: Towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity, 2023.
* Deng et al. [2023] Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. Multilingual jailbreak challenges in large language models. _arXiv preprint arXiv:2310.06474_, 2023.
* Dinan et al. [2019] Emily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston. Build it break it fix it for dialogue safety: Robustness from adversarial human attack, 2019.
* Gehman et al. [2020] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Real-toxicityprompts: Evaluating neural toxic degeneration in language models. _arXiv preprint arXiv:2009.11462_, 2020.
* Team [2023] Gemini Team. Gemini: A family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* Huang et al. [2023] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. Catastrophic jailbreak of open-source lms via exploiting generation. _arXiv preprint arXiv:2310.06987_, 2023.
* Jiang et al. [2023] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023.
* Li et al. [2024] Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, and Jing Shao. Salad-bench: A hierarchical and comprehensive safety benchmark for large language models. _arXiv preprint arXiv:2402.05044_, 2024.

Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation. In _The 2023 Conference on Empirical Methods in Natural Language Processing_, 2023. URL [https://openreview.net/forum?id=jIiJPDv82w](https://openreview.net/forum?id=jIiJPDv82w).
* Liu et al. [2023a] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, 55(9):1-35, 2023a.
* Liu et al. [2023b] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. _arXiv preprint arXiv:2305.13860_, 2023b.
* Mazeika et al. [2024] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, et al. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. _arXiv preprint arXiv:2402.04249_, 2024.
* Meng et al. [2024] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free reward. _arXiv preprint arXiv:2405.14734_, 2024.
* Meta [2024] Meta. Meta Llama 3. [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3), 2024.
* OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
* OpenAI [2024] OpenAI. Model Spec (2024/05/08). [https://cdn.openai.com/spec/model-spec-2024-05-08.html](https://cdn.openai.com/spec/model-spec-2024-05-08.html), 2024.
* Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* Parrish et al. [2022] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. BBQ: A hand-built bias benchmark for question answering. In _Findings of the Association for Computational Linguistics: ACL 2022_, pp. 2086-2105, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.165. URL [https://aclanthology.org/2022.findings-acl.165](https://aclanthology.org/2022.findings-acl.165).
* Qi et al. [2023] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! _arXiv preprint arXiv:2310.03693_, 2023.
* Rafailov et al. [2024] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _Advances in Neural Information Processing Systems_, 36, 2024.
* Rottger et al. [2023] Paul Rottger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: A test suite for identifying exaggerated safety behaviours in large language models. _arXiv preprint arXiv:2308.01263_, 2023.
* Samvelyan et al. [2024] Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, et al. Rainbow teaming: Open-ended generation of diverse adversarial prompts. _arXiv preprint arXiv:2402.16822_, 2024.
* Shaikh et al. [2022] Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang. On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning. _arXiv preprint arXiv:2212.08061_, 2022.

* [45] Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang. On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning, 2023.
* [46] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. _arXiv preprint arXiv:2308.03825_, 2023.
* [47] Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins, et al. A strongreject for empty jailbreaks. _arXiv preprint arXiv:2402.10260_, 2024.
* [48] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Riviere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Leonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amelie Heliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clement Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Michel Reid, Maciej Mikula, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clement Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology, 2024.
* [49] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [50] Bertie Vidgen, Hannah Rose Kirk, Rebecca Qian, Nino Scherrer, Anand Kannappan, Scott A Hale, and Paul Rottger. Simplesafetytests: a test suite for identifying critical safety risks in large language models. _arXiv preprint arXiv:2311.08370_, 2023.
* [51] Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. Do-not-answer: A dataset for evaluating safeguards in llms. _arXiv preprint arXiv:2308.13387_, 2023.
* [52] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_, 2021.
* [53] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.
* [54] Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, and Fangzhao Wu. Defending chatgpt against jailbreak attack via self-reminders. _Nature Machine Intelligence_, 5(12):1486-1496, 2023.
* [55] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher. _arXiv preprint arXiv:2308.06463_, 2023.

Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade lllms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. _arXiv preprint arXiv:2401.06373_, 2024.
* Zhang et al. [2023] Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. Safetybench: Evaluating the safety of large language models with multiple choice questions. _arXiv preprint arXiv:2309.07045_, 2023.
* Zou et al. [2023] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. _arXiv preprint arXiv:2307.15043_, 2023.
* Zverev et al. [2024] Egor Zverev, Sahar Abdelnabi, Mario Fritz, and Christoph H Lampert. Can llms separate instructions from data? and what do we even mean by that? _arXiv preprint arXiv:2403.06833_, 2024.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] Refer to Appendix A 3. Did you discuss any potential negative societal impacts of your work? [Yes] Refer to Appendix A 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? We report variance on our key results in the Appendix. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Refer to Appendix B
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] See Appendix F 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? Refer to Appendix H 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] All participants are authors. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]