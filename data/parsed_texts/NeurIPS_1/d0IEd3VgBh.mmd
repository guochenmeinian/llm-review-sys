# On the Role of Randomization

in Adversarially Robust Classification

 Lucas Gnecco Heredia\({}^{1}\)1  Muni Sreenivas Pydi\({}^{1}\)  Laurent Meunier\({}^{2}\)

**Benjamin Negrevergne\({}^{1}\) Yann Chevaleyre\({}^{1}\) \({}^{1}\)** CNRS, LAMSADE, Universite Paris Dauphine - PSL \({}^{2}\) Payflows

{lucas.gnecco-heredia,muni.pydi}@dauphine.psl.eu

{benjamin.negrevergne,yann.chevaleyre}@lamsade.dauphine.fr

laurent@payflows.io

Footnote 1: Corresponding author

###### Abstract

Deep neural networks are known to be vulnerable to small adversarial perturbations in test data. To defend against adversarial attacks, probabilistic classifiers have been proposed as an alternative to deterministic ones. However, literature has conflicting findings on the effectiveness of probabilistic classifiers in comparison to deterministic ones. In this paper, we clarify the role of randomization in building adversarially robust classifiers. Given a base hypothesis set of deterministic classifiers, we show the conditions under which a randomized ensemble outperforms the hypothesis set in adversarial risk, extending previous results. Additionally, we show that for any probabilistic binary classifier (including randomized ensembles), there exists a deterministic classifier that outperforms it. Finally, we give an explicit description of the deterministic hypothesis set that contains such a deterministic classifier for many types of commonly used probabilistic classifiers, _i.e._ randomized ensembles and parametric/input noise injection.

## 1 Introduction

Modern machine learning algorithms such as neural networks are highly sensitive to small, imperceptible adversarial perturbations of inputs [3, 43]. In the last decade, there has been a back-and-forth in research progress between developing increasingly potent attacks [5, 10, 19, 25] and the creation of practical defense mechanisms through empirical design [8, 30, 32]. In particular, probabilistic classifiers (also called stochastic classifiers or randomized classifiers) have been widely used to build strong defenses which can be roughly categorized into two groups: noise injection techniques [14, 28, 34, 37, 47, 48] and randomized ensembles [31, 38]. The first group includes methods that add noise at inference, usually to the input [22, 48], an intermediate layer activation [22, 47, 51] or the weights of a parametric model [22] like a neural network. Most of this work is experimental, with some theoretical papers that try to justify the use of such methods [37]. The second group, inspired by game theory, create a mixed strategy over base classifiers as a finite mixture [31, 38]. The argument behind this kind of models is that it becomes harder for an attacker to fool multiple models at the same time [12, 36].

Intuitively, the greater flexibility of probabilistic classifiers implies that they should outperform their deterministic counterparts in adversarially robust classification. For instance, one of the earliest works using randomization to improve robustness to adversarial attacks [48] claims that _"randomization at inference time makes the network much more robust to adversarial images"_. Dhillon et al. [14] propose a pruning method that _"is stochastic and has more freedom to deceive the adversary"_.

[MISSING_PAGE_FAIL:2]

Preliminaries

### Adversarially Robust Classification

We consider a classification setting with input space \(\mathcal{X}\subseteq\mathbb{R}^{d}\) and a finite label space \(\mathcal{Y}\). The space \(\mathcal{X}\) is equipped with some norm \(\|\cdot\|\), which is commonly set to be the \(\ell_{2}\) or \(\ell_{\infty}\) norms. Let \(\rho\in\mathcal{P}(\mathcal{X}\times\mathcal{Y})\) denote the true data distribution, which can be factored as \(\rho(x,y)=\nu(y)\rho_{y}(x)\), where \(\nu\in\mathcal{P}(\mathcal{Y})\) denotes the marginal probability distribution of \(\rho\) over \(\mathcal{Y}\) and \(\rho_{y}\in\mathcal{P}(\mathcal{X})\) denotes the conditional probability distribution of the data over \(\mathcal{X}\) given class \(y\).

A _deterministic classifier_ is a function \(h:\mathcal{X}\rightarrow\mathcal{Y}\) that maps each \(x\in\mathcal{X}\) to a fixed label in \(\mathcal{Y}\). The \(0\)-\(1\) loss of \(h\) on a point \((x,y)\in\mathcal{X}\times\mathcal{Y}\) is given by, \(\ell^{0\text{-}1}((x,y),h)=\mathds{1}\{h(x)\neq y\}\).

A _probabilistic classifier_ is a function \(\mathbf{h}:\mathcal{X}\rightarrow\mathcal{P}(\mathcal{Y})\) that maps each \(x\in\mathcal{X}\) to a probability distribution over \(\mathcal{Y}\). To label \(x\in\mathcal{X}\) with \(\mathbf{h}\), one samples a random label from the distribution \(\mathbf{h}(x)\in\mathcal{P}(\mathcal{Y})\). In practice, if \(\mathcal{Y}\) consists of \(K\) classes, \(\mathcal{P}(\mathcal{Y})\) is identifiable with the \(K\)-simplex \(\Delta^{K}\) that consist of vectors \(u\in\mathbb{R}^{K}\) such that \(\sum_{i=1}^{K}u_{i}=1\). Therefore, one can think of \(\mathbf{h}(x)\) as a probability vector for every \(x\).

The \(0\)-\(1\) loss of \(\mathbf{h}\) on \((x,y)\in\mathcal{X}\times\mathcal{Y}\) is given by, \(\ell^{0\text{-}1}((x,y),\mathbf{h})=\mathbb{E}_{\hat{y}\sim\mathbf{h}(x)}[ \mathds{1}\{\hat{y}\neq y\}]=1-\mathbf{h}(x)_{y}\). Note that \(\ell^{0\text{-}1}((x,y),\mathbf{h})\in[0,1]\) is a generalization of the classical \(0\)-\(1\) loss for deterministic classifiers, which can only take values in \(\{0,1\}\).

Given \(x\in\mathcal{X}\), we consider a data perturbing adversary of budget \(\epsilon\geq 0\) that can transport \(x\) to \(x^{\prime}\in B_{\epsilon}\left(x\right)=\{x^{\prime}\in\mathcal{X}\mid d(x,x ^{\prime})\leq\epsilon\}\), where \(B_{\epsilon}(x)\) is the closed ball of radius \(\epsilon\) around \(x\). The adversarial risk of a probabilistic classifier \(\mathbf{h}\) is defined as follows.

\[\mathcal{R}_{\epsilon}(\mathbf{h})=\mathop{\mathbb{E}}_{y\sim\nu}\left[ \mathcal{R}_{\epsilon}^{y}(\mathbf{h})\right]=\mathop{\mathbb{E}}_{y\sim\nu} \mathop{\mathbb{E}}_{x\sim\mathcal{P}_{y}}\left[\sup_{x^{\prime}\in B_{ \epsilon}(x)}\ell^{0\text{-}1}((x^{\prime},y),\mathbf{h})\right]\lx@note{ footnote}{The measurability of the $0$-1$ loss under attack (inner part of (1)) is non-trivial and depends on various factors like the type of ball considered for the supremum (closed or open) [44], and the underlying $\sigma$-algebra \@@cite[cite]{[\@@bibref{}{Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,B,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,Bishop,B,Bishop,Bishop,B,Bishop,Bishop,B,Bishop,Bishop,B,Bishop,B,Bishop,B,Bishop,B,Bishop,B,Bishop,B,Bishop,B,Bishop,B,Bishop,B,Bishop,B,Bishop,B,B,Bishop,B,B,Bishop,B,B,B,B,Bishop

### Probabilistic Classifiers Built from a Base Hypothesis Set

In this paper, we study probabilistic classifiers that are constructed from a _base hypothesis set_ (BHS) of (possibly infinitely many) deterministic classifiers, denoted by \(\mathcal{H}_{b}\). We use the name _mixtures_[38] for these type of classifiers. In the following, we show that many of the probabilistic classifiers that are commonly used in practice fall under this framework.

Let \(\mathcal{H}_{b}\) be a BHS of deterministic classifiers from \(\mathcal{X}\) to \(\mathcal{Y}\), which we assume is identifiable with a Borel subset of some \(\mathbb{R}^{p}\). Let \(\mu\in\mathcal{P}(\mathcal{H}_{b})\) be a probability measure over the measure space \((\mathcal{H}_{b},\sigma(\mathcal{H}_{b}))\), where \(\sigma(\mathcal{H}_{b})\) denotes the Borel \(\sigma\)-algebra on \(\mathcal{H}_{b}\). One can construct a probabilistic classifier \(\mathbf{h}_{\mu}:\mathcal{X}\rightarrow\mathcal{P}(\mathcal{Y})\), built from \(\mathcal{H}_{b}\), that maps \(x\in\mathcal{X}\) to a probability distribution \(\mu_{x}\in\mathcal{P}(\mathcal{Y})\), where \(\mu_{x}(y)\ =\mathbb{P}_{h\sim\mu}(h(x)=y)\). We now instantiate \(\mathcal{H}_{b}\) and \(\mu\in\mathcal{P}(\mathcal{H}_{b})\) for two main types of probabilistic classifiers that are commonly used in practice: randomized ensembles and noise injection classifiers.

For a _randomized ensemble classifier_ (_REC_), \(\mu\in\mathcal{P}_{M}(\mathcal{H}_{b})\subset\mathcal{P}(\mathcal{H}_{b})\) where \(\mathcal{P}_{M}(\mathcal{H}_{b})\) is the set of all discrete measures on \(\mathcal{H}_{b}\) supported on a finite set of \(M\) deterministic classifiers, \(\{h_{1},\ldots,h_{M}\}\). In this case, \(\mathbf{h}_{\mu}(x)\) takes the value \(h_{m}(x)\) with probability \(p_{m}=\mu(h_{m})\) for \(m\in[M]\), where \(\sum_{m\in[M]}p_{m}=1\). RECs were introduced in [38] and play the role of mixed strategies in the adversarial robustness game. They are a simple randomization scheme when a finite number of classifiers are at hand, and both training and attacking them represent a challenge [12, 13].

For a _weight-noise injection classifier_ (_WNC_), \(\mathcal{H}_{b}=\{h_{w}:w\in\mathcal{W}\}\) where \(h_{w}\) is a deterministic classifier with parameter \(w\). In this case, \(\mu\) is taken to be a probability distribution over \(\mathcal{W}\) with the understanding that each \(w\in\mathcal{W}\) is associated with a unique \(h_{w}\in\mathcal{H}_{b}\). For example, \(\mathcal{H}_{b}\) can be the set of all neural network classifiers with weights in the set \(\mathcal{W}\subseteq\mathbb{R}^{p}\). Any probability distribution \(\mu\) on the space of weights \(\mathcal{W}\) results in a probabilistic classifier \(\mathbf{h}_{\mu}\). Alternatively, \(\mathcal{H}_{b}\) can be generated by injecting noise \(z\) sampled from a distribution \(\mu\) to the weights \(w_{0}\) of a fixed classifier \(h_{w_{0}}\). In this case, the probabilistic classifier \(\mathbf{h}_{\mu}\) maps \(x\in\mathcal{X}\) to a probability distribution \(\mu_{x}\in\mathcal{P}(\mathcal{Y})\), where \(\mu_{x}(y)=\mathbb{P}_{z\sim\mu}(h_{w_{0}+z}(x)=y)\). Weight noise injection has been explicitly used in [22], but there are many other approaches that implicitly define a probability distribution over the parameters of a model [14, 35] and sample one or more at inference.

For an _input-noise injection classifier_ (_INC_), \(\mathcal{H}_{b}=\{h_{\eta}:\eta\in\mathcal{X}\}\) where \(h_{\eta}(x)=h_{0}(x+\eta)\) for a fixed deterministic classifier \(h_{0}\). In this case, \(\mu\) is taken to be a probability distribution over \(\mathcal{X}\) (which is unrelated to the data generating distribution), and \(\mathbf{h}_{\mu}\) maps \(x\in\mathcal{X}\) to a probability distribution \(\mu_{x}\in\mathcal{P}(\mathcal{Y})\), where \(\mu_{x}(y)=\mathbb{P}_{\eta\sim\mu}(h(x+\eta)=y)\). Injecting noise to the input has been used for decades as a regularization method [4, 21]. INCs are studied in [22, 37, 39] as a defense against adversarial attacks, and are closely related to randomized smoothing classifiers [8, 26].

**Remark 1** (Measurability).: To ensure the well-definedness of the adversarial risk, we need to ensure that the \(0\)-\(1\) loss under attack (inner part of (1)) is measurable. The \(0\)-\(1\) loss for a fixed class \(y\in\mathcal{Y}\) can now be seen as a function from the product space \(\mathcal{X}\times\mathcal{H}_{b}\) to \(\{0,1\}\):

\[f(x,h)=\ell^{0\text{-}1}((x,y),h)=\mathds{1}\{h(x)\neq y\} \tag{2}\]

For a distribution \(\mu\in\mathcal{P}(\mathcal{H}_{b})\) and the associated probabilistic classifier \(\mathbf{h}_{\mu}\), we can rewrite the \(0\)-\(1\) loss of \(\mathbf{h}_{\mu}\) at \(x\) as

\[\ell^{0\text{-}1}((\cdot,y),\mathbf{h}_{\mu})=\mathop{\mathbb{E}}_{h\sim\mu}[ \mathds{1}\{h(\cdot)\neq y\}]=\int_{\mathcal{H}_{b}}f(\cdot,h)d\mu(h) \tag{3}\]

Note that if \(f\) is Borel measurable in \(\mathcal{X}\times\mathcal{H}_{b}\), then by Fubini-Tonelli's Theorem, the \(0\)-\(1\) loss as a function of \(x\) with an integral over \(\mathcal{H}_{b}\) shown in (3) is Borel measurable. By [16, Appendix A, Theorem 27], the \(0\)-\(1\) loss under attack is then universally measurable and therefore the adversarial risk, which requires the integral over \(\mathcal{X}\) of the loss under attack, is well-defined over this universal \(\sigma\)-algebra [44, Definition 2.7]. Recent work by Trillos et al. [44] have also shown the existence of Borel measurable solutions for the _closed ball_ formulation of adversarial risk.

In this work, we assume that the function \(f\) in (2) is Borel measurable in \(\mathcal{X}\times\mathcal{H}_{b}\) for every \(y\), so that (1) is well-defined. This assumption is already satisfied in the settings that are of interest for our work. As an example, it holds when the classifiers at hand are neural networks with continuous activation functions. More details on Appendix A. For a deeper study on the measurability and well-definedness of the adversarial risk in different threat models and settings, see [1, 16, 40, 44].

From Deterministic to Probabilistic Classifiers

Let \(\mathcal{H}_{b}\) be a class of deterministic classifiers. In this section, we compare the robustness of probabilistic classifiers built upon \(\mathcal{H}_{b}\) with the robustness of the class \(\mathcal{H}_{b}\) itself. Note that if we consider the trivial mixtures \(\mu=\delta_{h}\) for \(h\in\mathcal{H}_{b}\), we obtain the original classifiers in \(\mathcal{H}_{b}\), so it is always true that \(\inf_{\mu\in\mathcal{P}(\mathcal{H}_{b})}\mathcal{R}_{\epsilon}(\mathbf{h}_{ \mu})\leq\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h)\). We are interested in the situations in which this gap is strict, meaning that mixtures strictly outperform the base classifiers. With this in mind, we introduce the notion of _Matching Penny Gap_ to quantify the improvement of probabilistic classifiers over deterministic ones.

Theoretical Results on Robustness of Probabilistic Classifiers.We begin by presenting a theorem which shows that the adversarial risk of a probabilistic classifier is at most the average of the adversarial risk of the deterministic classifiers constituting it. The proof of this result will be a first step towards understanding the conditions that favor the mixture classifier.

**Theorem 3.1**.: For a probabilistic classifier \(\mathbf{h}_{\mu}:\mathcal{X}\rightarrow\mathcal{P}(\mathcal{Y})\) constructed from a BHS \(\mathcal{H}_{b}\) using any \(\mu\in\mathcal{P}(\mathcal{H}_{b})\), we have \(\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu})\leq\mathbb{E}_{h\sim\mu}\left[ \mathcal{R}_{\epsilon}(h)\right]\).

A natural follow-up question to is to ask _what conditions guarantee a strictly better performance of a probabilistic classifier_. We know that this gap can be strict, as can be seen in toy examples like the one shown in Figure 0(b) where \(\mathcal{H}_{b}\) is the set of all linear classifiers, and there exist two distinct classifiers \(f_{1},f_{2}\) both attaining the minimum adversarial risk among all classifiers in \(\mathcal{H}_{b}\). Any non-degenerate mixture of \(f_{1},f_{2}\) attains a strictly better adversarial risk, demonstrating a strict advantage for probabilistic classifiers.

From the proof of Theorem 3.1 it is clear that a strict gap in performance between probabilistic and deterministic classifiers is only possible when the following strict inequality holds for a non-vanishing probability mass over \((x,y)\).

\[\sup_{x^{\prime}\in B_{\epsilon}(x)}\mathbb{E}_{h\sim\mu}\left[ \mathds{1}\{h(x^{\prime})\neq y\}\right]<\mathbb{E}_{h\sim\mu}\left[\sup_{x^{ \prime}\in B_{\epsilon}(x)}\mathds{1}\{h(x^{\prime})\neq y\}\right]. \tag{4}\]

The above condition holds at \((x,y)\) if there exists a subset of vulnerable classifiers \(\mathcal{H}_{vb}\subseteq\mathcal{H}_{b}\) with \(\mu(\mathcal{H}_{vb})>0\)**any of which can be forced to individually misclassify** the point \((x,y)\) by an adversary using (possibly different) perturbations \(x^{\prime}_{h}\in B_{\epsilon}(x)\) for \(h\in\mathcal{H}_{vb}\), **but cannot be forced to misclassify all at the same time using the same perturbation \(x^{\prime}\in B_{\epsilon}(x)\)**. Such a configuration is reminiscent of the game of _matching pennies_[18, 46] (see Appendix B). If \(\mathcal{H}_{b}\) is in a _matching penny_ configuration at \((x,y)\), a mixed strategy for classification (i.e. a mixture of \(\mathcal{H}_{b}\)) achieves a decisive advantage over any pure strategy (i.e. any deterministic base classifier) because the adversary can only force a misclassification on a subset of all vulnerable classifiers. Such a configuration was first noted in [12] in the context of improved adversarial attacks on RECs, and also in [13] for computing the adversarial risk of RECs of size \(M=2\). Figure 1 illustrates such a condition on an example REC of size \(M=2\) over a single point (Figure 0(a)), and over a simple discrete distribution (Figure 0(b)). We formalize this intuition with the definition below.

**Definition 1** (Matching penny gap).: The matching penny gap of a data point \((x,y)\in(\mathcal{X}\times\mathcal{Y})\) with respect to a probabilistic classifier \(\mathbf{h}_{\mu}\) constructed from \(\mathcal{H}_{b}\) using \(\mu\in\mathcal{P}(\mathcal{H}_{b})\) is defined as,

\[\pi_{\mathbf{h}_{\mu}}(x,y)=\mu(\mathcal{H}_{vb}(x,y))-\mu^{\max}(x,y), \tag{5}\]

where \(\mathcal{H}_{vb}(x,y)\subseteq\mathcal{H}_{b}\) denotes the _valnerable subset_ and \(\mu^{\max}(x,y)\) the _maximal simultaneous vulnerability_ of base classifiers \(\mathcal{H}_{b}\), defined below.

\[\mathcal{H}_{vb}(x,y) =\{h\in\mathcal{H}_{b}:\exists x^{\prime}_{h}\in B_{\epsilon}(x) \text{ such that }h(x^{\prime}_{h})\neq y\},\] \[\mathfrak{H}_{svb}(x,y) =\{\mathcal{H}^{\prime}\subseteq\mathcal{H}_{b}:\exists x^{ \prime}\in B_{\epsilon}(x)\text{ such that }\forall h\in\mathcal{H}^{\prime},h(x^{ \prime})\neq y\},\] \[\mu^{\max}(x,y) =\sup_{\mathcal{H}^{\prime}\subseteq\mathfrak{H}_{svb}(x,y)}\mu( \mathcal{H}^{\prime}).\]

If \(\pi_{\mathbf{h}_{\mu}}(x,y)>0\), we say that \(\mathbf{h}_{\mu}\) is in _matching penny configuration_ at \((x,y)\).

For example, in Figure 0(a), \(\pi_{\mathbf{h}_{\mu}}(x_{0},y_{0})=1-\max\{\mu(f_{1}),\mu(f_{2})\}=\min\{\mu(f_ {1}),\mu(f_{2})\}\) where \(\mathcal{H}_{b}=\{f_{1},f_{2}\}\). The subset \(\mathcal{H}_{vb}(x,y)\) contains all classifiers that can be individually fooled by an optimal attacker. The collection of subsets \(\mathcal{S}_{svb}(x,y)\) contains all subsets \(\mathcal{H}^{\prime}\) of classifiers that can be _simultaneously fooled_. Then, \(\mu^{\max}(x,y)\) is the maximum mass of classifiers that can be fooled simultaneously. Thus, \(\pi_{\mathbf{h}_{\mu}}(x,y)\) measures the gap between the mass of classifiers that are individually vulnerable and the maximum mass of classifiers that can be fooled with only one perturbation.

The following theorem strengthens Theorem 3.1 by showing that \(\mathcal{R}_{\epsilon}(\mathbf{h})\) is a strictly convex function if and only if there is a non-zero expected matching penny gap.

**Theorem 3.2**.: For a probabilistic classifier \(\mathbf{h}_{\mu}:\mathcal{X}\rightarrow\mathcal{P}(\mathcal{Y})\) constructed from a BHS \(\mathcal{H}_{b}\) using any \(\mu\in\mathcal{P}(\mathcal{H}_{b})\),

\[\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu})=\mathop{\mathbb{E}}_{h\sim\mu}\left[ \mathcal{R}_{\epsilon}(h)\right]-\mathop{\mathbb{E}}_{(x,y)\sim\rho}[\pi_{ \mathbf{h}_{\mu}}(x,y)]. \tag{6}\]

Proof sketch.: By interchanging expectations over \(h\sim\mu\) and \((x,y)\sim\rho\) and using the fact that \(\sup_{x^{\prime}\in B_{\epsilon}(x)}\mathds{1}\{h(x^{\prime})\neq y\}=1\) if and only if \(h\in\mathcal{H}_{vb}(x,y)\), we first show the following.

\[\mathop{\mathbb{E}}_{h\sim\mu}\left[\mathcal{R}_{\epsilon}(h)\right]=\mathop{ \mathbb{E}}_{(x,y)\sim\rho}[\mu(\mathcal{H}_{vb}(x,y))]. \tag{7}\]

Arguing from the definition of \(\mu^{\max}(x,y)\), we then show that \(\sup_{x^{\prime}\in B_{\epsilon}(x)}\mathop{\mathbb{E}}_{h\sim\mu}[\mathds{1} \{h(x^{\prime})\neq y\}]=\mu^{\max}(x,y)\). Taking expectation with respect to \((x,y)\sim\rho\), we get,

\[\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu})=\mathop{\mathbb{E}}_{(x,y)\sim\rho} \left[\mu^{\max}(x,y)\right]. \tag{8}\]

Combining (8) and (7) yields (6). 

The following corollary shows that a lower bound on the expected matching penny gap is both necessary and sufficient for a mixture to strictly outperform any deterministic classifier in \(\mathcal{H}_{b}\).

**Corollary 3.1**.: For \(\mu^{\prime}\in\mathcal{P}(\mathcal{H}_{b})\), \(\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu^{\prime}})<\inf_{h\in\mathcal{H}_{b}} \mathcal{R}_{\epsilon}(h)\) if and only if the following condition holds.

\[\mathop{\mathbb{E}}_{(x,y)\sim\rho}[\pi_{\mathbf{h}_{\mu^{\prime}}}(x,y)]> \mathop{\mathbb{E}}_{h\sim\mu^{\prime}}[\mathcal{R}_{\epsilon}(h)]-\inf_{h\in \mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h) \tag{9}\]

Additionally, \(\inf_{\mu\in\mathcal{P}(\mathcal{H}_{b})}\mathcal{R}_{\epsilon}(\mathbf{h}_{ \mu})<\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h)\) if and only if there exists \(\mu^{\prime}\in\mathcal{P}(\mathcal{H}_{b})\) for which (9) holds.

Figure 1: Toy examples demonstrating a strict gap in adversarial risk between deterministic and probabilistic classifiers (RECs).

**Remark 2** (Multiple optimal classifiers in matching penny configuration).: For finite \(\mathcal{H}_{b}\), the assumption (9) of Corollary 3.1 holds whenever there exist two distinct optimal classifiers \(h_{1}^{*},h_{2}^{*}\in\mathcal{H}_{b}\) with a positive expected matching penny gap. In such a case, the RHS of (9) is zero for any \(\mu\) that is a mixture of \(h_{1}^{*},h_{2}^{*}\). This is indeed the case in Figure 0(b). More generally, assumption (9) holds if there is a subset of classifiers in a matching penny configuration that are "near optimal" on average. More details in Appendix C.3.

The following example depicts the scenario of Remark 2 in the extreme case in which there exist infinitely many distinct optimal classifiers in matching penny configuration, leading to the maximum possible advantage of using probabilistic classifiers.

**Example 1** (Maximum expected matching penny gap of 1).: Consider a discrete data distribution \(\rho=\delta(x=0,y=1)\) and a BHS composed of infinitely many linear classifiers, \(\mathcal{H}_{b}=\{h:h(x)=\mathds{1}\{w^{T}x<1\},\|w\|_{2}=\frac{1}{\epsilon}\}\) where \(\|\cdot\|_{2}\) is the Euclidean norm. Observe that every classifier in \(\mathcal{H}_{b}\) is vulnerable at \((x,y)=(0,1)\) and so \(\mathcal{R}_{\epsilon}(h)=1\) for all \(h\in\mathcal{H}_{b}\). However, no pair is simultaneously vulnerable. Hence, \(\pi_{\mathbf{h}_{\mu}}(x,y)=1\) for any distribution \(\mu\in\mathcal{P}(\mathcal{H}_{b})\) that is continuous over the entire support \(\mathcal{H}_{b}\). By Theorem 3.2 we get that \(\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu})=0\) for any such \(\mu\). Therefore, any such \(\mathbf{h}_{\mu}\) outperforms every \(h\in\mathcal{H}_{b}\), and we have \(0=\inf_{\mu\in\mathcal{P}(\mathcal{H}_{b})}\mathcal{R}_{\epsilon}(\mathbf{h}_ {\mu})<\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h)=1\). More details on Appendix C.1.

**Remark 3** (Probabilistic classifiers with zero matching penny gap are not useful).: Suppose the expected matching penny gap \(\mathbb{E}_{(x,y)\sim\rho}[\pi_{\mathbf{h}_{\mu^{\prime}}}(x,y)]\) is zero for some \(\mu^{\prime}\in\mathcal{P}(\mathcal{H}_{b})\). Then the left hand side of (9) is zero, whereas the right-hand side \(\mathbb{E}_{h\sim\mu^{\prime}}[\mathcal{R}_{\epsilon}(h)]-\inf_{h\in\mathcal{H }_{b}}\mathcal{R}_{\epsilon}(h)\) is always non-negative. Hence, (9) does not hold and so \(\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu^{\prime}})\geq\inf_{h\in\mathcal{H}_{b} }\mathcal{R}_{\epsilon}(h)\). Therefore, any such probabilistic classifier underperforms the best deterministic classifier in the base set \(\mathcal{H}_{b}\).

The following example illustrates a scenario described in Remark 3, where we examine classifiers with decision boundaries that are parallel sets [23].

**Example 2** (Minimum expected matching penny gap of \(0\) / Parallel decision boundaries).: Fix any binary classifier \(h:\mathcal{X}\rightarrow\{0,1\}\) with non-empty decision region \(A_{h}=\{x\in\mathcal{X}:h(x)=1\}\subset\mathcal{X}\subseteq\mathbb{R}^{d}\). Let \(\mathcal{H}_{b}\) be composed of all classifiers whose decision regions are \(r\)-parallel sets of \(A_{h}\) defined as \(A_{h}^{r}=A_{h}\oplus B_{r}(0)\) where \(\oplus\) denotes the Minkowski sum, i.e., \(\mathcal{H}_{b}=\{h:\exists r\geq 0\;s.t.\;A_{h}^{r}=\{x\in\mathcal{X}:h(x)=1\}\}\). Because of the parallel decision boundaries, whenever two classifiers in \(\mathcal{H}_{b}\) are vulnerable, they are simultaneously vulnerable, and never exhibit a matching penny configuration. Therefore, \(\mathbb{E}_{(x,y)\sim\rho}[\pi_{\mathbf{h}_{\mu}}(x,y)]=0\) for any \(\mu\in\mathcal{P}(\mathcal{H}_{b})\). More details on Appendix C.2.

Application to RECs and Links with [13].In the case of RECs where \(\mu=\sum_{m\in[M]}p_{m}\delta_{h_{m}}\) i.e., \(\mathbf{h}_{\mu}\) is a mixture of \(M\) deterministic classifiers in \(\mathcal{H}_{b}=\{h_{m}\}_{m\in[M]}\), we can instantiate Theorem 3.2 as follows. As the family \(\mathfrak{H}_{svb}(x,y)\) is finite, the supremum \(\mu^{\max}(x,y)\) is always attained by some subset of simultaneously vulnerable classifiers \(\mathcal{H}_{svb}^{max}(x,y)\). We can then write:

\[\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu})=\sum_{m\in[M]}p_{m}\mathcal{R}_{ \epsilon}(h_{m})-p_{m}\left[\mathop{\mathbb{E}}_{(x,y)\sim\rho}\mathds{1}\{h _{m}\in\mathcal{H}_{vb}(x,y)\setminus\mathcal{H}_{svb}^{max}(x,y)\}\right] \tag{10}\]

Alternatively, we can use (8) to write \(\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu})=\sum_{m\in[M]}p_{m}\mathop{\mathbb{E} }_{(x,y)\sim\rho}\mathds{1}\{h_{m}\in\mathcal{H}_{svb}^{max}(x,y)\}\). At each \((x,y)\), testing whether \(h_{m}\in\mathcal{H}_{svb}^{max}(x,y)\) reduces to solving a combinatorial optimization problem, as noted in [13]. Any \((x,y)\) falls into one of finitely many configurations, depending on which subset of classifiers are vulnerable or simultaneously vulnerable at \((x,y)\). Dbouk and Shanbhag [13] use this to derive upper and lower bounds on \(\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu})\). Specifically, [13, Proposition 1] is equivalent to (8) for the special case of RECs. Also, [13, Theorem 1] can be proved as an application of Theorem 3.2 to RECs with \(M=2\). To establish the link, one must note that \(\mathbb{E}_{(x,y)\sim\rho}[\pi_{\mathbf{h}_{\mu^{\prime}}}(x,y)]=\frac{1}{2} \rho(\{(x,y)\in R\})\) where \(R\subseteq\mathcal{X}\times\mathcal{Y}\) indicates the set of all points where \(h_{1},h_{2}\) are in matching penny configuration.

## 4 From Probabilistic to Deterministic Classifiers

In this section, we prove that for any probabilistic binary classifier \(\mathbf{h}\), there exists a deterministic classifier \(h\) in a "threshold" hypothesis set \(\mathcal{H}_{T}(\mathbf{h})\) with at least the same adversarial risk. InSection 4.1 we present the main theorem and in Section 4.2 we apply the theorem to various classes of probabilistic classifiers.

### Reducing a Probabilistic Classifier to a Deterministic One Through Threshold Classifiers

In the case of binary classification, i.e. \(\mathcal{Y}=\{0,1\}\) any distribution \(\nu\in\mathcal{P}(\mathcal{Y})\) is uniquely determined by a scalar \(\alpha=\nu(y=1)\in[0,1]\). Hence, any probabilistic binary classifier is identified with a function \(\mathbf{h}:\mathcal{X}\rightarrow[0,1]\), where \(\mathbf{h}(x)=\nu(y=1|x)\in[0,1]\). Accordingly, \(\ell^{0\text{-}1}((x,0),\mathbf{h})=\mathbf{h}(x)\) and \(\ell^{0\text{-}1}((x,1),\mathbf{h})=1-\mathbf{h}(x)\).

Given a probabilistic binary classifier \(\mathbf{h}:\mathcal{X}\rightarrow[0,1]\) and a threshold \(\alpha\in[0,1]\), the _\(\alpha\)-threshold classifier \(h^{\alpha}:\mathcal{X}\rightarrow\{0,1\}\)_ is defined as \(h^{\alpha}(x)=\mathds{1}\{\mathbf{h}(x)>\alpha\}\), and the _threshold hypothesis set (THS)_ of \(\mathbf{h}\) is given by \(\mathcal{H}_{T}(\mathbf{h})=\{h^{\alpha}\}_{\alpha\in[0,1]}\). In Theorem 4.1 we show that there exists \(h^{\alpha_{*}}\in\mathcal{H}_{T}(\mathbf{h})\) such that \(\mathcal{R}_{\epsilon}(h^{\alpha_{*}})\leq\mathcal{R}_{\epsilon}(\mathbf{h})\). The following lemma plays a crucial role in proving Theorem 4.1.

**Lemma 4.1**.: Let \(\mathbf{h}:\mathcal{X}\rightarrow[0,1]\) be any measurable function. For any \(\succ\in\{>,\geq\}\), the following inequality holds, and it becomes an equality if \(\mathbf{h}\) is continuous or takes finitely many values:

\[\mathds{1}\bigg{\{}\left(\sup_{x^{\prime}\in B_{\epsilon}(x)}\mathbf{h}(x^{ \prime})\right)\succ\alpha\bigg{\}}\geq\sup_{x^{\prime}\in B_{\epsilon}(x)} \mathds{1}\{\mathbf{h}(x^{\prime})\succ\alpha\} \tag{11}\]

Note that Lemma 4.1 is a generalization of the layer-cake representation of \(\mathbf{h}(x)\) given by,

\[\mathbf{h}(x)=\int_{0}^{1}\mathds{1}\{\mathbf{h}(x)>\alpha\}d\alpha=\int_{0}^{ 1}\mathds{1}\{\mathbf{h}(x)\geq\alpha\}d\alpha.\]

**Theorem 4.1**.: Let \(\mathbf{h}:\mathcal{X}\rightarrow[0,1]\) be any probabilistic binary classifier. Let \(h^{\alpha}\) be the \(\alpha\)-_threshold_ classifier. Then the following equation holds:

\[\mathcal{R}_{\epsilon}(\mathbf{h})\geq\int_{0}^{1}\mathcal{R}_{\epsilon}(h^{ \alpha})d\alpha\geq\inf_{\alpha}\mathcal{R}_{\epsilon}(h^{\alpha}). \tag{12}\]

Further, if \(\mathbf{h}\) is either continuous or takes finitely many values, the first inequality in (12) becomes an equality.

Note that \(\mathbf{h}\) takes finitely many values in the case of RECs, and \(\mathbf{h}\) is continuous in the case of INCs and WNCs whenever the noise distribution admits a density. Hence, \(R_{\epsilon}(\mathbf{h})=\int_{0}^{1}\mathcal{R}_{\epsilon}(h^{\alpha})d\alpha\) in all these cases.

**Remark 4**.: Theorem 4.1 says that in the binary case (K = 2), if one is able to consider complex enough hypotheses sets that contain the \(\mathcal{H}_{T}(\mathbf{h})\), then randomization is not necessary because there is a deterministic classifier with equal or better adversarial risk.

It was very recently shown with a toy example [45, Section 5.2] that Theorem 4.1 does not hold in the multi-class case \(K>2\). This example shows that even when the family of probabilistic classifiers considered is very general (all Borel measurable functions), simple data distributions can create a situation in which the optimal classifier is probabilistic, and there is no optimal deterministic classifier. In other words, there is a strict gap in adversarial risk between the best deterministic classifier and the best probabilistic one.

### Applications: Connections to Weighted Ensembles and Randomized Smoothing

In this section, we apply Theorem 4.1 to probabilistic classifiers presented in Section 2.3.

Recs.When \(\mu=\sum_{m\in[M]}p_{m}\delta_{h_{m}}\) and \(\mathcal{Y}=\{0,1\}\), the REC \(\mathbf{h}_{\mu}\) can be written as \(\mathbf{h}_{\mu}(x)=\sum_{m=1}^{M}p_{m}h_{m}(x)\). Let us introduce the constant classifier \(h_{0}(x)=1\) for all \(x\), and \(p_{0}=-\alpha\). Then \(h^{\alpha}\left(x\right)=\mathds{1}\{\sum_{m=1}^{K}p_{m}h_{m}(x)>\alpha\}= \mathds{1}\{\sum_{m=0}^{M}p_{m}h_{m}(x)>0\}\). This shows that \(h^{\alpha}\) is a _weighted ensemble_, such as those that the boosting algorithm can learn [17].

Further, a REC \(\mathbf{h}\) built with \(M\) base binary classifiers can take at most \(p\leq 2^{M}\) distinct values, corresponding to all possible partial sums of the weights \(p_{m}\). Let \(0~{}=~{}r_{1}~{}\leq~{}\ldots~{}\leq~{}r_{p}~{}=~{}1\) be the possible values. Then, for any \(\alpha\in[r_{i},r_{i+1})\), \(h^{\alpha}=h^{r_{i}}\). Applying Theorem 4.1 yields,

\[\mathcal{R}_{\epsilon}(\mathbf{h})=\int_{0}^{1}\mathcal{R}_{\epsilon}(h^{\alpha })d\alpha=\sum_{i=1}^{p-1}\int_{r_{i}}^{r_{i+1}}\mathcal{R}_{\epsilon}(h^{r_{i }})d\alpha=\sum_{i=1}^{p-1}(r_{i+1}-r_{i})\mathcal{R}_{\epsilon}(h^{r_{i}}) \tag{13}\]

Equation (13) shows that the THS \(\mathcal{H}_{T}(\mathbf{h})=\{h^{\alpha}\}_{\alpha\in[0,1]}\) is composed of at most \(p\leq 2^{M}\) distinct classifiers, each of which is in turn a weighted ensemble. In case of uniform weights i.e. \(p_{m}=\frac{1}{M}\), there are only \(M\) distinct weighted ensembles in \(\mathcal{H}_{T}(\mathbf{h})\).

INCs.Let \(h_{\eta}:\ \mathcal{X}\ \rightarrow\ \{0,1\}\) be the deterministic binary classifiers created from a base classifier \(h\), as defined in Section 2.3. The probabilistic classifier \(\mathbf{h}_{\mu}\) is defined as \(\mathbf{h}_{\mu}(x)=\mathbb{P}_{\eta\sim\mu}(h(x\ +\ \eta)\ =\ 1)\). Thus, the \(\alpha\)-threshold classifier takes a similar form to the well-known classifier obtained by _randomized smoothing_.

\[h^{\alpha}(x)=\mathds{1}\{\underset{\eta\sim\mu}{\mathbb{P}}\left(h(x+\eta)=1 \right)>\alpha\}. \tag{14}\]

Randomized smoothing was first introduced in [26] and refined in [8; 27; 42] as a way to create classifiers that are certifiably robust. Given a deterministic classifier \(h:\mathcal{X}\rightarrow\mathcal{Y}\) and a noise distribution \(\mu\in\mathcal{P}(\mathcal{X})\), the smoothed model will output a prediction according to the following rule:

\[h_{RS(\mu)}(x)=\operatorname*{argmax}_{y\in\mathcal{Y}}\mathbb{P}_{\eta\sim\mu }\left(h(x+\eta)=y\right)=\mathds{1}\left\{\mathbb{P}_{\eta\sim\mu}\left(h(x+ \eta)=1\right)>\tfrac{1}{2}\right\} \tag{15}\]

In other words, Equation (14) generalizes the randomized smoothing classifier in the binary case by replacing the \(\frac{1}{2}\) threshold by \(\alpha\). Theorem 4.1 then states that _for any INC, there exists a deterministic randomized smoothing classifier with threshold \(\alpha\) that is at least as robust_.

## 5 Families of Binary Classifiers for Which Randomization Does not Help

In Sections 3 and 4, we discussed two types of hypothesis sets for binary classification: 1) base set \(\mathcal{H}_{b}\) from which a probabilistic classifier is built using some \(\mu\in\mathcal{P}(\mathcal{H}_{b})\), and 2) threshold set \(\mathcal{H}_{T}(\mathbf{h})\) which is built from a base probabilistic classifier \(\mathbf{h}\). Recapitulating this two-step process, one may define the _completion_ of a base set \(\mathcal{H}_{b}\) of binary classifiers w.r.t. a set of probability distributions \(\mathcal{M}\subseteq\mathcal{P}(\mathcal{H}_{b})\) as, \(\overline{\mathcal{H}_{b}}(\mathcal{M})=\cup_{\mu\in\mathcal{M}}\mathcal{H}_{ T}(\mathbf{h}_{\mu})\). Observe that \(\mathcal{H}_{b}\subseteq\overline{\mathcal{H}_{b}}(\mathcal{P}(\mathcal{H}_{b}))\). In the following theorem, we show that if \(\mathcal{H}_{b}=\overline{\mathcal{H}_{b}}(\mathcal{M})\), then probabilistic binary classifiers built from \(\mathcal{H}_{b}\) using any \(\mu\in\mathcal{M}\) do not offer robustness gains compared to the deterministic classifiers in \(\mathcal{H}_{b}\).

**Theorem 5.1**.: If \(\mathcal{H}_{b}=\overline{\mathcal{H}_{b}}(\mathcal{M})\) and \(\delta_{h}\in\mathcal{M}\) for all \(h\in\mathcal{H}_{b}\), then \(\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h)=\inf_{\mu\in\mathcal{M}} \mathcal{R}_{\epsilon}(\mathbf{h}_{\mu})\).

Proof.: \[\inf_{h\in\overline{\mathcal{H}_{b}}(\mathcal{M})}\mathcal{R}_{\epsilon}(h)= \inf_{\mu\in\mathcal{M}}\inf_{h\in\mathcal{H}_{T}(\mathbf{h}_{\mu})}\mathcal{ R}_{\epsilon}(h)\leq\inf_{\mu\in\mathcal{M}}\mathcal{R}_{\epsilon}(\mathbf{h}_{ \mu})\leq\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h),\] (16)

where the first inequality follows from Theorem 4.1 and the second by considering \(\mu=\delta_{h}\) for any \(h\in\mathcal{H}_{b}\). The desired conclusion follows by observing that the first and last terms in (16) are identical from the assumption \(\mathcal{H}_{b}=\overline{\mathcal{H}_{b}}(\mathcal{M})\). 

A trivial example where the assumptions of Theorem 5.1 hold is when \(\mathcal{H}_{b}\) is the set of all measurable functions \(h:\mathcal{X}\rightarrow\{0,1\}\). Such a \(\mathcal{H}_{b}\) is commonly used in the study of optimal adversarial risk [2; 40]. In the following corollary, we show that the assumptions of Theorem 5.1 also hold in the case of RECs built using \(\mathcal{H}_{b}\) that satisfy a "closure" property.

**Corollary 5.1**.: Let \(\mathcal{H}_{b}\) be any family of deterministic binary classifiers. Let \(\mathcal{M}=\mathcal{P}_{M}(\mathcal{H}_{b})\subset\mathcal{P}(\mathcal{H}_{b})\) be the subset of probability measures over \(\mathcal{H}_{b}\) defining RECs as in Section 2.3. Let \(\mathcal{A}=\left\{h^{-1}(1):h\in\mathcal{H}_{b}\right\}\). If \(\mathcal{A}\) is closed under union and intersection, then

\[\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h)=\inf_{\mu\in\mathcal{P}_{M }(\mathcal{H}_{b})}\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu}).\]

**Remark 5** (Implications on the optimal adversarial classifier).: Pydi and Jog [40, Theorem 8] show the existence of a binary deterministic classifier attaining the minimum adversarial risk over the space of all Lebesgue measurable classifiers in \(\mathbb{R}^{d}\). Awasthi et al. [2, Theorem 1] prove a similar result over the space of universally measurable classifiers in \(\mathbb{R}^{d}\). For both these families the assumptions of Theorem 5.1 hold for \(\mathcal{M}=\mathcal{P}(\mathcal{H}_{b})\), and so probabilistic classifiers offer no additional advantage over deterministic ones. Further, suppose \(\mathcal{H}^{*}\) is a finite subset of classifiers achieving optimal adversarial risk. Then by Theorem 3.2, for any \(\mu\in\mathcal{P}(\mathcal{H}^{*})\), the expected matching penny gap for \(\mathbf{h}_{\mu}\) must be zero, otherwise we could strictly improve the adversarial risk by considering the mixture built using \(\mu\). In other words, any pair of optimally robust binary deterministic classifiers in the settings of [2, 40] can only be in a matching penny configuration on a null subset \(E\subset\mathcal{X}\times\mathcal{Y}\).

## 6 Conclusion and Future Work

In this paper, we have studied the robustness improvements brought by randomization. First, we studied the situation in which one expands a base family of classifiers by considering probability distributions over it. We showed that under some conditions on the data distribution and the configuration of the base classifiers, such a probabilistic expansion could offer gains in adversarial robustness (See Corollary 3.1), characterized by the _matching penny gap_. These results generalize previous work that focused on RECs [13]. Next, we showed that for any binary probabilistic classifier, there is always another deterministic extension with classifiers of comparable or better robustness. This result is linked with the existence results in [2, 40]. As a direct consequence of this result, we show that in the binary setting, deterministic weighted ensembles are at least as robust as randomized ensembles and randomized smoothing is at least as robust as noise injection.

There are many avenues for future research.

**Improving the Matching Penny Gap.** An interesting direction would be finding tighter bounds on the matching penny gap for particular and widely used probabilistic classifiers like RECs, INCs and WNCs. It would be interesting to establish the conditions under which each method can offer robustness gains, and to quantify those gains in terms of parameters such as the strength of the noise injected or the number of classifiers in the REC.

**Studying the Threshold Hypothesis Sets.** We have seen in Section 4.2 that different probabilistic classifiers induce different THS. In particular, we showed the THS corresponding to RECs and INCs. It would be useful to formally describe the THS induced by other popular families of probabilistic classifiers in the literature. It would also be useful to quantify the complexity gap between the initial \(\mathcal{H}_{b}\) and the THS to understand the risk-complexity trade-off we would have to incur.

**Multiclass Setting.** The toy example in [45, Section 5.2] shows that randomization might be necessary in the multi-class setting, as it is no longer true that there is always a deterministic optimal classifier, even when considering very general families of deterministic classifiers like all measurable functions. A natural road for future work is to further characterize the situations in which probabilistic classifiers strictly outperform deterministic ones in the multi-class setting.

**Training algorithms for diverse ensembles and RECs.** There are numerous works based on the idea of training diverse classifiers that are not simultaneously vulnerable to create robust ensembles [11, 24, 33, 41, 49, 50]. Most of these approaches try to induce orthogonality in decision boundaries so that gradient-based attacks do not transfer between models. This intuition is validated by Corollary 3.1, since such diversification would tend to increase the matching penny gap. It should be noted that ensembles and RECs have different inference procedures, and attacking them represents different optimization problems. One avenue of research would be to make the link between the matching penny gap and the diversity more formal. In addition, designing training algorithms explicitly optimizing the matching penny gap would be valuable, particularly because existing training algorithms for RECs [31, 38] have been shown to be inadequate [12].

## 7 Acknowledgements

This work was funded by the French National Research Agency (DELCO ANR-19-CE23-0016).

## References

* [1]P. Awasthi, N. Frank, and M. Mohri (2021) On the existence of the adversarial bayes classifier. Advances in Neural Information Processing Systems34, pp. 2978-2990. Cited by: SS1.
* [2]P. Awasthi, N. Frank, and M. Mohri (2021) On the existence of the adversarial bayes classifier (extended version). arXiv preprint arXiv:2112.01694. Cited by: SS1.
* [3]B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Srndic, P. Laskov, G. Giacinto, and F. Roli (2013) Evasion attacks against machine learning at test time. In Joint European conference on machine learning and knowledge discovery in databases, pp. 387-402. Cited by: SS1.
* [4]C. M. Bishop (1995) Training with noise is equivalent to tikhonov regularization. Neural computation7 (1), pp. 108-116. Cited by: SS1.
* [5]N. Carlini and D. Wagner (2017) Adversarial examples are not easily detected: bypassing ten detection methods. In Proceedings of the 10th ACM workshop on artificial intelligence and security, pp. 3-14. Cited by: SS1.
* [6]N. Carlini, A. Athalye, N. Papernot, W. Brendel, J. Rauber, D. Tsipras, I. Goodfellow, A. Madry, and A. Kurakin (2019) On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705. Cited by: SS1.
* [7]D. Charalambos and B. Aliprantis (2006) Infinite dimensional analysis: a hitchhiker's guide. Springer-Verlag Berlin and Heidelberg GmbH & Company KG. Cited by: SS1.
* [8]J. Cohen, E. Rosenfeld, and Z. Kolter (2019) Certified adversarial robustness via randomized smoothing. In International Conference on Machine Learning, pp. 1310-1320. Cited by: SS1.
* [9]Y. Crama and P. L. Hammer (2011) Boolean functions: theory, algorithms, and applications. Cambridge University Press. Cited by: SS1.
* [10]F. Croce and M. Hein (2020) Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206-2216. Cited by: SS1.
* [11]A. Dabouei, S. Soleymani, F. Taherkhani, J. Dawson, and N. M. Nasrabadi (2020) Exploiting joint robustness to adversarial perturbations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1122-1131. Cited by: SS1.
* [12]H. D. and N. Shanbhag (2022) Adversarial vulnerability of randomized ensembles. In International Conference on Machine Learning, pp. 4890-4917. Cited by: SS1.
* [13]H. D. and N. Shanbhag (2022) On the robustness of randomized ensembles to adversarial perturbations. In International Conference on Machine Learning, pp. 7303-7328. Cited by: SS1.
* May 3, 2018, Conference Track Proceedings, External Links: Link Cited by: SS1.
* [15]D. Diochnos, S. Mahloujifar, and M. Mahmoody (2018) Adversarial risk and robustness: general definitions and implications for the uniform distribution. Advances in Neural Information Processing Systems31. Cited by: SS1.
* [16]N. S. Frank and J. Niles-Weed (2022) Existence and minimax theorems for adversarial surrogate risks in binary classification. arXiv preprint arXiv:2206.09098. Cited by: SS1.
* [17]Y. Freund, R. Schapire, and N. Abe (1999) A short introduction to boosting. Journal-Japanese Society For Artificial Intelligence14 (771-780), pp. 1612. Cited by: SS1.
* [18]D. Fudenberg and J. Tirole (1991) Game theory. MIT press. Cited by: SS1.

* Goodfellow et al. [2015] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In Yoshua Bengio and Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015. URL [http://arxiv.org/abs/1412.6572](http://arxiv.org/abs/1412.6572).
* Gourdeau et al. [2021] Pascale Gourdeau, Varun Kanade, Marta Kwiatkowska, and James Worrell. On the hardness of robust classification. _The Journal of Machine Learning Research_, 22(1):12521-12549, 2021.
* Grandvalet et al. [1997] Yves Grandvalet, Stephane Canu, and Stephane Boucheron. Noise injection: Theoretical prospects. _Neural Computation_, 9(5):1093-1108, 1997.
* He et al. [2019] Zhezhi He, Adnan Siraj Rakin, and Deliang Fan. Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 588-597, 2019.
* Jog [2021] Varun Jog. Reverse euclidean and gaussian isoperimetric inequalities for parallel sets with applications. _IEEE Transactions on Information Theory_, 67(10):6368-6383, 2021.
* Kariyappa and Qureshi [2019] Sanjay Kariyappa and Moinuddin K. Qureshi. Improving adversarial robustness of ensembles with diversity training. _CoRR_, abs/1901.09981, 2019. URL [http://arxiv.org/abs/1901.09981](http://arxiv.org/abs/1901.09981).
* Kurakin et al. [2017] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017. URL [https://openreview.net/forum?id=BJm4T4Kgx](https://openreview.net/forum?id=BJm4T4Kgx).
* Lecuyer et al. [2019] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified robustness to adversarial examples with differential privacy. In _2019 IEEE Symposium on Security and Privacy (SP)_, pages 656-672. IEEE, 2019.
* Li et al. [2019] Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with additive noise. _Advances in neural information processing systems_, 32, 2019.
* Liu et al. [2018] Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks via random self-ensemble. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 369-385, 2018.
* Lucas et al. [2023] Keane Lucas, Matthew Jagielski, Florian Tramer, Lujo Bauer, and Nicholas Carlini. Randomness in ml defenses helps persistent attackers and hinders evaluators. _arXiv preprint arXiv:2302.13464_, 2023.
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL [https://openreview.net/forum?id=rJzIBFZAb](https://openreview.net/forum?id=rJzIBFZAb).
* Meunier et al. [2021] Laurent Meunier, Meyer Sectbon, Rafael B Pinot, Jamal Atif, and Yann Chevaleyre. Mixed nash equilibria in the adversarial examples game. In _International Conference on Machine Learning_, pages 7677-7687. PMLR, 2021.
* Moosavi-Dezfooli et al. [2019] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robustness via curvature regularization, and vice versa. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9078-9086, 2019.
* Pang et al. [2019] Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu. Improving adversarial robustness via promoting ensemble diversity. In _International Conference on Machine Learning_, pages 4970-4979. PMLR, 2019.
* Panousis et al. [2021] Konstantinos Panousis, Sotirios Chatzis, Antonios Alexos, and Sergios Theodoridis. Local competition and stochasticity for adversarial robustness in deep learning. In _International Conference on Artificial Intelligence and Statistics_, pages 3862-3870. PMLR, 2021.

* [35] Konstantinos P Panousis, Sotirios Chatzis, and Sergios Theodoridis. Stochastic local winner-takes-all networks enable profound adversarial robustness. _arXiv preprint arXiv:2112.02671_, 2021.
* [36] Juan C Perdomo and Yaron Singer. Robust attacks against multiple classifiers. _arXiv preprint arXiv:1906.02816_, 2019.
* [37] Rafael Pinot, Laurent Meunier, Alexandre Araujo, Hisashi Kashima, Florian Yger, Cedric Gouy-Pailler, and Jamal Atif. Theoretical evidence for adversarial robustness through randomization. _Advances in Neural Information Processing Systems_, 32, 2019.
* [38] Rafael Pinot, Raphael Ettedgui, Geovani Rizk, Yann Chevaleyre, and Jamal Atif. Randomization matters how to defend against strong adversarial attacks. In _International Conference on Machine Learning_, pages 7717-7727. PMLR, 2020.
* [39] Rafael Pinot, Laurent Meunier, Florian Yger, Cedric Gouy-Pailler, Yann Chevaleyre, and Jamal Atif. On the robustness of randomized classifiers to adversarial examples. _Machine Learning_, 111(9):3425-3457, 2022.
* [40] Muni Sreenivas Pydi and Varun Jog. The many faces of adversarial risk: An expanded study. _IEEE Transactions on Information Theory_, pages 1-1, 2023. doi: 10.1109/TIT.2023.3303221.
* [41] Andrew Ross, Weiwei Pan, Leo Celi, and Finale Doshi-Velez. Ensembles of locally independent prediction models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 5527-5536, 2020.
* [42] Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. _Advances in Neural Information Processing Systems_, 32, 2019.
* [43] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In Yoshua Bengio and Yann LeCun, editors, _2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings_, 2014. URL [http://arxiv.org/abs/1312.6199](http://arxiv.org/abs/1312.6199).
* [44] Nicolas Garcia Trillos, Matt Jacobs, and Jakwang Kim. On the existence of solutions to adversarial training in multiclass classification. _arXiv preprint arXiv:2305.00075_, 2023.
* [45] Nicolas Garcia Trillos, Matt Jacobs, and Jakwang Kim. The multimarginal optimal transport formulation of adversarial multiclass classification. _Journal of Machine Learning Research_, 24(45):1-56, 2023.
* [46] John Von Neumann. On the theory of games of strategy. _Contributions to the Theory of Games_, 4:13-42, 1959.
* [47] Bao Wang, Zuoqiang Shi, and Stanley Osher. Resnets ensemble via the feynman-kac formalism to improve natural and robust accuracies. _Advances in Neural Information Processing Systems_, 32, 2019.
* [48] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects through randomization. _arXiv preprint arXiv:1711.01991_, 2017.
* [49] Huanrui Yang, Jingyang Zhang, Hongliang Dong, Nathan Inkawhich, Andrew Gardner, Andrew Touchet, Wesley Wilkes, Heath Berry, and Hai Li. Dverge: diversifying vulnerabilities for enhanced robust generation of ensembles. _Advances in Neural Information Processing Systems_, 33:5505-5515, 2020.
* [50] Zhuolin Yang, Linyi Li, Xiaojun Xu, Shiliang Zuo, Qian Chen, Pan Zhou, Benjamin Rubinstein, Ce Zhang, and Bo Li. Trs: Transferability reduced ensemble via promoting gradient diversity and model smoothness. _Advances in Neural Information Processing Systems_, 34:17642-17655, 2021.

* Yu et al. [2021] Tianyuan Yu, Yongxin Yang, Da Li, Timothy Hospedales, and Tao Xiang. Simple and effective stochastic neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 3252-3260, 2021.
* Zhang et al. [2022] Dinghuai Zhang, Hongyang Zhang, Aaron Courville, Yoshua Bengio, Pradeep Ravikumar, and Arun Sai Suggala. Building robust ensembles via margin boosting. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 26669-26692. PMLR, 17-23 Jul 2022.

## Supplementary Material

### Appendix A Measurability of the \(\ell^{0\text{-}1}\) under attack

In this section, we will go over the conditions that ensure the well-definedness of the adversarial risk defined in (1). We will show that the assumptions we make are verified in the settings that are of interest to our work. We highlight that the main purpose of this paper is not to deeply study the existence of measurable solutions to the adversarial problem, as was done in previous work [1, 40, 44]. In this work, we allow ourselves to add hypothesis on the input space \(\mathcal{X}\) and the families of classifiers \(\mathcal{H}_{b}\) that we consider, as long as they are valid in practice.

#### Discussion

Recall that in our setting we have the following. The input space \(\mathcal{X}\) is some subset (almost always bounded or even compact) of \(\mathbb{R}^{d}\), \(\mathcal{Y}=[K]\) and \(\mathcal{P}(\mathcal{Y})=\Delta^{K}\). On the other hand, for all the applications of interest to our work, we consider _base hypothesis sets_\(\mathcal{H}_{b}\) that are either finite or discrete (for randomized ensembles), or identifiable with subsets of some \(\mathbb{R}^{p}\) (for weight and input noise injection). For all these sets, we assume the usual topology and the Borel \(\sigma\)-algebra that is generated by the open sets. We further assume that \(\mathcal{X}\) and \(\mathcal{H}_{b}\)**are Borel spaces**, which is not restrictive. It allows, for example, any open or closed set of \(\mathbb{R}^{n}\), which already encompasses all practical applications of interest. This hypothesis makes it simpler to work on the product measure space \(\mathcal{X}\times\mathcal{H}_{b}\).

In the adversarial attacks literature, the question of measurability is always with respect to \(\mathcal{X}\)[2, 40]. For a deterministic classifier \(h:\mathcal{X}\to\mathcal{Y}=[K]\) to be measurable, the sets \(h^{-1}(k)\) must be measurable. In other words, each classifier \(h\) creates a \(K\)-partition of \(\mathcal{X}\) into measurable subsets \(h^{-1}(1),\ldots,h^{-1}(K)\). This immediately translates into the \(\ell^{0\text{-}1}\), as a function of \(x\), being measurable, because \(\ell^{0\text{-}1}((\cdot,y),h)^{-1}(1)=\{x\in\mathcal{X}\mid h(x)\neq y\}= \mathcal{X}\setminus h^{-1}(y)\). In other words, assuming classifiers are measurable functions translates into measurability of the \(0\text{-}1\) loss, so the arguments in previous work [1, 16, 40] can be used to justify that the adversarial risk is well-defined.

We introduce a new component, the BHS \(\mathcal{H}_{b}\), and we now compute integrals over it. For this reason, we now consider more general functions of the form \(f:\mathcal{X}\times\mathcal{H}_{b}\to\{0,1\}\), \(f(x,h)=\mathds{1}\{h(x)\neq y\}\) and study their measurability. If one fixes \(h\), the function \(f^{h}(x)=\mathds{1}\{h(x)\neq y\}\) becomes the same \(0\text{-}1\) loss considered earlier, so assuming every \(h\) is a measurable function translates into the fact that \(f\) is measurable w.r.t. \(x\) for every fixed \(h\). The new part we have to deal with is the measurability of \(f\) w.r.t \(h\) for every fixed \(x\).

As a first example, let us consider the case of linear classifiers. In this case, a classifier \(h\) can be represented as a \(K\times d\) matrix \(A\) with the classification rule \(h(x)=\operatorname{argmax}_{k}(Ax)_{k}\), meaning \(h\) classifies \(x\) using the row with higher score. In this case, we can directly see the measurability of \(f\) in each component (\(f^{h}\) and \(f^{x}\)) by looking at the pre-images of \(1\), which are the sets that induce an error (recall \(y\) is fixed). For \(f^{h}\), the pre-image is the set of \(x\in\mathcal{X}\) such that \(h(x)\neq y\). This can be seen as the complement of the set \(X_{y}\) in which \(h\) predicts \(y\).

\[(f^{h})^{-1}(1)=\{x\in\mathcal{X}\mid h(x)\neq y\}=\{x\in\mathcal{X}\mid h(x) =y\}^{C}=X_{y}^{C}\]

The set \(X_{y}\) is described by \(K-1\) linear equations using the rows \(a_{j}\) of the matrix \(A\) that defines the linear classifier \(h\), as follows

\[X_{y}=\{x\in\mathcal{X}\mid a_{y}^{T}x\geq a_{j}^{T}x,\;\forall j\neq y\}\]

This set is the finite intersection of hyperplanes, so it is Borel measurable, and therefore the function \(f^{h}\) is measurable on \(\mathcal{X}\) for any \(h\) linear classifier.

For the measurability of \(f^{x}\), we have to consider the set \(H_{y}\) of classifiers that produce a prediction of \(y\) for a fixed \(x\). Let us recall that each classifier is represented by a matrix \(A\in\mathbb{R}^{K\times d}\), then

\[H_{y}=\{h\in\mathcal{H}_{b}\mid h(x)=y\}=\{A\in\mathbb{R}^{K\times d}\mid a_{y }^{T}x\geq a_{j}^{T}x,\;\forall j\neq y\}\]

\(H_{y}\) is a convex cone, because if \(A,B\in H_{y}\), then for a positive \(\alpha\), \(\alpha A\in H_{y}\) and \(A+B\in H_{y}\). This set is therefore measurable, and so we can conclude that \(f^{x}\) is measurable for every \(x\).

When considering neural networks with weights parametrized by \(w\), we can think of them as non-linear functions plus a last linear layer as the one described earlier. Then, if the non-linear function is continuous w.r.t both \(x\) and \(w\), the function \(f\) we are considering is again measurable w.r.t to \(x\) and \(w\).

In summary, all the situations that are of interest to our work verify that the function \(f\) is defined over a product space \(\mathbb{R}^{d}\times\mathbb{R}^{p}\), and it has good properties when seen as functions of \(x\) or \(h\) separately (measurable, continuous or differentiable). These _separately measurable functions_[7, Definition 4.47] might not be _jointly measurable_, _i.e._ measurable on the product space, which is one condition that would grant us the well-definedess of (1). Nevertheless, we can make use of the stronger properties these functions have and assume they are _Caratheodory functions_[7, Definition 4.50]. This will imply the joint measurability of \(f\). For completeness, we rewrite the results from [7]:

**Definition 2** (Caraheodory function [7, Definition 4.50]).: Let \((S,\Sigma)\) be a measurable space, and let \(X\) and \(Y\) be topological spaces. Let \(\mathcal{B}_{Y}\) be the Borel \(\sigma\)-algebra on \(Y\). A function \(f:S\times X\to Y\) is a **Caratheodory function** if:

* For each \(x\in X\), the function \(f^{x}=f(\cdot,x):S\to Y\) is \((\Sigma,\mathcal{B}_{Y})\)-measurable.
* For each \(s\in S\), the function \(f^{s}=f(s,\cdot):X\to Y\) is continuous.

As we have seen, the functions \(f:\mathcal{X}\times\mathcal{H}_{b}\to\mathbb{R}\) we consider can be assumed to be Cartheodory functions, as they are in general differentiable or continuous in both components. Even going away from practice, in theoretical works it is usual to consider classifiers \(h:\mathcal{X}\to\Delta^{K}\) as Borel measurable functions from \(\mathcal{X}\), and adding the assumption of continuity w.r.t \(h\) for every fixed \(x\) it not restrictive (for example neural networks with continuous activation functions [31]).

**Lemma A.1** (Caraheodory functions are jointly measurable [7, Lemma 4.51]).: Let \((S,\Sigma)\) be a measurable space, \(X\) a separable metrizable space, and \(Y\) a metrizable space. Then every Caratheodory function \(f:S\times X\to Y\) is jointly measurable

### Conclusion

If we assume the hypotheses of Lemma A.1, that is

1. We assume that for every component \(y\), the function \(f:\mathcal{X}\times\mathcal{H}_{b}\to\{0,1\}\) defined as \(f(x,h)=\mathds{1}\{h(x)\neq y\}\) is a _Caratheodory function_, which translates in our case into: * Each \(h\in\mathcal{H}_{b},h\) is a measurable function on \(\mathcal{X}\) (very common assumption). * For each \(x\), the function \(f^{x}:\mathcal{H}_{b}\to\{0,1\},f^{x}(h)=f(x,h)\) is continuous (not unrealistic, satisfied by neural networks with continuous activation functions)
2. The input space \(\mathcal{X}\) is a measurable space (satisfied by hypothesis, \(\mathcal{X}\subseteq\mathbb{R}^{d}\) is some Borel set).
3. The base hypothesis set \(\mathcal{H}_{b}\) is a separable metrizable space (valid when \(\mathcal{H}_{b}\) is a Borel subset of \(\mathbb{R}^{p}\)).
4. The output space \(\{0,1\}\) is a metrizable space (satisfied).

then, by Lemma A.1, the function \(f\) is jointly measurable.

By Fubini-Tonelli's Theorem, the function \(x\to\int_{\mathcal{H}_{b}}f(x,h)d\mu(h)\) is measurable on \(\mathcal{X}\). Then, following the same arguments as in [1, 16], the loss under attack can be defined and is measurable over the universal \(\sigma\)-algebra \(\mathcal{U}(\mathcal{X})\) on \(\mathcal{X}\). Then, considering \(\mathcal{U}(\mathcal{X})\), the adversarial risk is well-defined as it is an integral over \(\mathcal{X}\) using the completions of the class conditioned measures \(\rho_{y}\) over \(\mathcal{U}(\mathcal{X})\) (see (1)).

## Appendix B On the name _Matching penny gap_

In the original matching penny game between player 1 (attacker) and player 2 (defender), each player has a penny coin and has to secretly position its penny in either _heads_ or _tails_. Then, both coins are simultaneously revealed and the outcome of the game is decided as follows: attacker wins if both pennies match. If they do not match, then defender wins. This situation can be represented using the following matrix (attacker wants to maximize), where _heads_ and _tails_ have been replaced by \(f_{1}\) and \(f_{2}\) for reasons that will be explained later:classifier individually, there is no point in the allowed perturbation region \(B_{\epsilon}(x_{0})\) in which both are simultaneously fooled.

Consider now that the defender is using a randomized ensemble that picks either \(f_{1}\) or \(f_{2}\) at random for inference. This was introduced in [38] with the name mixtures of classifiers, related to mixed strategies in the context of game theory. In such setting, the optimal attacker that faces such mixture is now in a matching pennies game situation. At each inference pass, the attacker must choose which classifier to attack in order to craft the adversarial example \(x^{\prime}\), and if the chosen classifier matches with the one the defender used, then the attacker wins. If they do not match, the prediction made by the defender will be correct and the attacker would have lost. This is exactly the game of _matching pennies_!

What is particularly worse for defender is that, in the traditional formulation of adversarial attacks, the attacker is given the chance to play second and therefore can adapt to the classifier. In other words, attacker can craft its perturbation for the particular classifier proposed by defender, while defender has to propose a classifier that should be robust to every conceivable attack. This means that if defender uses \(f_{1}\) or \(f_{2}\) individually, then attacker would be able to craft an attack that induces an error. In this situation, and inspired by the game of _matching pennies_, defender can turn these two strategies, that have 0 robust accuracy on their own, into a strategy that guarantees an expected accuracy of \(\frac{1}{2}\) by randomizing over \(f_{1}\) and \(f_{2}\). Recall that attacker is optimal for any fixed classifier, **but it has no control over randomness**, which means that when faced against a

Figure 2: Example of classifiers in a matching penny configuration around a point \(x_{0}\).

mixture of \(f_{1}\) and \(f_{2}\), attacker can compute any perturbation \(\delta\) from them, but he has no knowledge of **which classifier will be used for each independent inference pass**. This can be interpreted as follows: for a given inference run, there is a chance of \(\frac{1}{2}\) that the perturbation proposed by attacker is indeed an adversarial attack and induces an error.

Notice that if we increase the number of choices (classifiers) in matching pennies' configuration to \(M>2\), the game becomes harder for the attacker. Indeed, for every possible choice of classifier at each inference pass, there is one out of \(M\) choices that lead to a successful attack (attacking the one that was sampled for the inference pass), and \(M-1\) that lead to a correct classification. An extreme example of such benefit to the defender is shown in Example 1.

## Appendix C Details on the examples and remarks

### Example 1: Maximum mathing penny gap

Note that for this example, each classifier \(w\) satisfies that \(w^{T}x=0\), and therefore \(\mathds{1}\{w^{T}x<1\}=1\), which means that all \(w\) predict the correct label for the clean input \(x\). Now we want to see that every \(w\) is vulnerable at \(x\).

Recall that we defined \(\mathcal{H}_{b}\) as the space of linear classifiers \(w\) such that \(\left\lVert w\right\rVert_{2}=\frac{1}{\epsilon}\). We can rewrite this norm as a dual norm \(\left\lVert w\right\rVert_{2}=\sup_{\|z\|=1}w^{T}z\). Moreover, this supremum is attained by some \(z_{w}\), so for each \(w\) we get \(z_{w}\) of norm 1, such that \(w^{T}z_{w}=\frac{1}{\epsilon}\).

Now, for each \(w\in\mathcal{H}_{b}\), consider the adversarial example \(\epsilon\cdot z_{w}\). It is a valid adversarial example because it has norm \(\epsilon\), and \(w^{T}(\epsilon\cdot z_{w})=\epsilon\cdot\frac{1}{\epsilon}=1\), meaning that the classifier predicts \(\mathds{1}\{w^{T}(\epsilon\cdot z_{w})<1\}=0\), the wrong class. The next step is to guarantee that this perturbation is unique for each \(w\), which holds because we are using the Euclidean norm.

Having that \(\epsilon\cdot z_{w}\) fools \(w\) and only \(w\), we can consider for simplicity \(\mu\) the uniform distribution over \(\mathcal{H}_{b}\). Let us compute the sets \(\mathcal{H}_{vb}(x,1)\) and \(\mu^{\max}(x,1)=0\) to be able to compute the matching penny gap for this example.

As we just saw, every \(w\) is itself vulnerable. This means that \(\mathcal{H}_{vb}(x,1)=\mathcal{H}_{b}\), and therefore \(\mu(\mathcal{H}_{vb}(x,1))=1\).

Figure 3: Visualization of Example 1. Best viewed in color. Here \(w\) is one of the linear classifiers considered in \(\mathcal{H}_{b}\). For the budget \(\epsilon\), there is exactly one point \(\epsilon\cdot z_{w}\) of intersection between the \(\epsilon\)-ball around \((x,1)\) and the hyperplane defined by the equation \(w^{T}x=1\). All the points in the half-space \(w^{T}x\geq 1\) are classified as \(0\). This includes \(\epsilon\cdot z_{w}\), and this is the only point in the \(\epsilon\)-ball that belongs to this half-space. That is why \(\epsilon\cdot z_{w}\) is the only adversarial attack that can fool \(w\), and it cannot fool any other \(w^{\prime}\neq w\).

Given the unicity of \(z_{w}\), we know that no two classifiers can be fooled by the same perturbation. Therefore the family of simultaneously vulnerable classifiers only contains singletons \(\{w\}\). As \(\mu(\{w\})=0\) for every \(w\), taking the supremum gives \(\mu^{\max}(x,1)=0\).

Finally, expected the matching penny gap for this example with only one point is

\[\mathop{\mathbb{E}}_{x,y}[\pi_{\mathbf{h}_{\mu}}(x,1)]=\pi_{\mathbf{h}_{\mu}}(x,1)=\mu(\mathcal{H}_{vb}(x,1))-\mu^{\max}(x,1)=1-0=1.\]

In other words, the mixture in this example has the best adversarial risk possible, even though it is built from classifiers with the worst adversarial risk possible individually.

### Example 2: Minimum mathing penny gap / Parallel sets

To simplify the example, consider the compact set \(A=\{0\}\subset\mathbb{R}\). Then, the family of all parallel classifiers \(h_{r}=A\oplus B_{r}(0)\) are the classifiers of the form \(h_{r}(x)=\mathds{1}\{x\in(-r,r)\}\) for \(r>0\).

Recall that a matching penny configuration arises when 1) **both classifiers are vulnerable**, but 2) **not simultaneously**. That is, each one of them must be vulnerable, but no allowed perturbation can induce an error on both at the same time. We will see that this cannot happen with this family of classifiers that are "parallel".

W.l.o.g, take any point \(x>0\) and suppose it is of class is 0. Take any two classifiers \(h_{r_{1}},h_{r_{2}}\) with \(r_{1}<r_{2}\) and fix the attacker budget to \(\epsilon\). Note that \(h_{r_{1}}\) is vulnerable at \(x\) if an only if \(x-\epsilon\leq r_{1}\). That is, the attacker must be able to move \(x\) inside \((-r,r)\) with its budget of \(\epsilon\). This also holds for \(h_{r_{2}}\).

Note that to satisfy the condition that **both classifiers are vulnerable**, we must ensure that \(x-\epsilon\leq r_{1}\) and \(x-\epsilon\leq r_{2}\). However, any \(x\) that satisfies \(x-\epsilon\leq r_{1}\) immediately satisfies \(x-\epsilon\leq r_{2}\), as \(r_{1}<r_{2}\), meaning that the same perturbation induces an error on both classifiers. In conclusion, they cannot be both **individually vulnerable** without being **simultaneously vulnerable** at \(x\).

For the case of general parallel sets \(A^{r_{1}},A^{r_{2}}\), note that if \(r_{1}<r_{2}\) then \(A^{r_{1}}\subset A^{r_{2}}\). Assuming that the classifiers predict 1 if the point is inside the set \(A^{r}\), then the condition **both classifiers are vulnerable** means that a point \(x\) of class 0 is at a distance of at most \(\epsilon\) from both \(A^{r_{1}},A^{r_{2}}\). However, for any given \(x\), being \(\epsilon\)-close to \(A^{r_{1}}\) implies that \(x\) is \(\epsilon\)-close to \(A^{r_{2}}\), so any adversarial example of \(A^{r_{1}}\) will be also adversarial for \(A^{r_{2}}\). In conclusion, they can never be in a matching penny configuration.

### Multiple optimal classifiers in matching penny configuration

In this section, we are going to explain in more detail Remark 2. First, we rewrite Equation (9) which states the condition under which a mixture outperforms the individual classifiers composing it:

\[\mathop{\mathbb{E}}_{(x,y)\sim\rho}[\pi_{\mathbf{h}_{\mu^{\prime}}}(x,y)]> \mathop{\mathbb{E}}_{h\sim\mu^{\prime}}[\mathcal{R}_{\epsilon}(h)]-\inf_{h\in \mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h)\]

For simplicity, assume \(\mathcal{H}_{b}\) consist of two different classifiers \(h_{1},h_{2}\)**with the same adversarial risk** and with a **strictly positive matching penny gap**, i.e. \(\mathop{\mathbb{E}}_{(x,y)\sim\rho}[\pi_{\mathbf{h}_{\mu}}(x,y)]>0\) for some \(\mu=(\mu(h_{1}),\mu(h_{2}))\). Then the REC \(\mathbf{h}_{\mu}\) that consists of these two classifiers will outperform each one of them. This is because the LHS of equation (9) is positive, while the RHS is exactly 0 because both classifiers have the same risk, so their average risk is equal to the best risk among them. If we add the assumption that \(h_{1}\) and \(h_{2}\) were optimal deterministic classifiers from a larger family \(\mathcal{H}_{b}\), we would have created a mixture that outperforms all classifiers in it.

If now we try to soften the assumptions on \(h_{1},h_{2}\), and say that they have very similar adversarial risks, i.e. \(\mathcal{R}_{\epsilon}(h_{2})=\mathcal{R}_{\epsilon}(h_{1})+\delta\) for some small \(\delta\), then the RHS of (9) is no longer 0 but \(\frac{\delta}{2}\). In this case, for condition (9) to hold we need the matching penny gap to be not only strictly positive, but greater than \(\frac{\delta}{2}\).

The important intuition is that mixing classifiers that have very different adversarial risks is not ideal, as one will then need a much greater expected matching penny gap in order to actually outperform the best individual classifier. Another interesting intuition is that one can create a good mixture from very bad classifiers, as was seen in Example 1, where individual classifiers had adversarial risk of 1 (the worst possible), but mixing them resulted in an adversarial risk of 0 (the best possible). Even if Example 1 is merely a toy example, it highlights the intuition that it is possible for many non-robust classifiers to gain robustness as a mixture if they interact nicely between them and the dataset (have a high expected matching penny gap).

### Example with linear classifiers: Mixtures can improve robustness (Figure 0(b))

#### c.4.1 Detailed description of the example

Here is the exact description of the example that was introduced in Figure 0(b). We will use colors to simplify the distinction between points. The points have the following coordinates: \(x_{1}=(0,1),x_{2}=(-2.7,1.1),x_{3}=(2.7,1.1),x_{4}=(0,-2)\). For this extended analysis, it will be easier to work with classes in \(\{-1,1\}\), so we have labels \(y_{1}=1\) and \(y_{2}=y_{3}=y_{4}=-1\). We set \(\epsilon=1\).

Let \(f=(w,b)\) be a generic linear classifier in \(\mathbb{R}^{2}\), where \(w=(w_{1},w_{2})\) is the normal vector of the hyperplane and \(b\) is the bias term. The classification is done using the rule \(sign(f(x))=sign(w^{T}x+b)\), as in standard binary classification with labels \(\{-1,1\}\).

In this setting, a point \((x,y)\in\mathbb{R}^{2}\times\{-1,1\}\) is correctly classified if \(y\cdot f(x)\ >\ 0\). Moreover, for linear classifiers, the optimal adversarial perturbation is known and robustness at a point can be easily studied. Note that given a perturbation \(\delta\), we can check if it induces a misclassification by simply computing \(y\cdot f(x+\delta)\). We can say \(f\) is robust at \((x,y)\) if, for every perturbation of norm at most \(\epsilon\)

Figure 5: The mixture of \(f_{1}\) and \(f_{2}\) with uniform weights has lower adversarial risk than any linear classifier, motivating the use of mixtures as a robust alternative.

we have that \(y\cdot f(x+\delta)>0\). This can be developed to get

\[y\cdot f(x+\delta)>0 \iff y(w^{T}(x+\delta)+b)>0 \tag{17}\] \[\iff y(w^{T}x+b)+y(w^{T}\delta)>0\] \[\iff y(w^{T}x+b)>-y(w^{T}\delta)\]

For \(f\) to be robust at \((x,y)\), Equation (17) must hold for all \(\delta\), in particular for the quantity maximizing the RHS. This happens for \(\delta^{*}=-y\epsilon\frac{w}{\|w\|}\). Inserting this worst case perturbation, we obtain a simple condition for robustness in the linear case:

\[y(w^{T}x+b)>\epsilon\,\|w\| \tag{18}\]

Without loss of generality, we can assume \(w\) has norm 1 so the last equation further simplifies. We can use these inequalities to show that no linear classifier can be robust at \(x_{1}\), \(x_{2}\) and \(x_{4}\) at the same time. Indeed, being robust at each point gives us the following inequalities:

\[\begin{array}{ll}(x_{1}):&w_{2}+b>1&\implies b>1-w_{2}\\ (x_{2}):&2.7w_{1}-1.1w_{2}-b>1&\implies b<2.7w_{1}-1.1w_{2}-1\\ (x_{4}):&2w_{2}-b>1&\implies b<2w_{2}-1\end{array}\]

Using the inequalities for \(x_{1}\) and \(x_{4}\), we get the condition \(w_{2}>\frac{2}{3}\). Using the inequalities for \(x_{1}\) and \(x_{2}\) gives us that \(2.7w_{1}-0.1w_{2}>2\). Together with the bound on \(w_{2}\), we get the bound \(w_{1}>\frac{62}{81}\). These two bounds make it impossible for \(w\) to have norm 1, contradicting our hypothesis. An analogous reasoning shows that no linear classifier can be robust at \(x_{1}\), \(x_{3}\) and \(x_{4}\) at the same time.

Knowing that a linear classifier can't be robust at \(x_{1}\), \(x_{2}\) and \(x_{4}\) simultaneously, we can further ask what is the best adversarial risk one can get. One can easily check that it is possible to be robust on any of the pairs \((x_{1},x_{2})\), \((x_{2},x_{4})\) or \((x_{1},x_{2})\). At each time, robustness on the two selected points comes at the expense of non-robustness on the third point. This fact discards the strategy of being robust at the pair \((x_{2},x_{4})\), because being non-robust at \(x_{1}\) implies a risk of \(\frac{1}{2}\), way higher than the risk of \(\frac{1}{6}\) one would have to pay for being non-robust at \(x_{2}\) or \(x_{4}\). With this being said, the other two solutions are optimal in terms of adversarial risk for linear classifiers, with an adversarial risk of \(\frac{1}{3}\). Nevertheless, for building a robust mixture, only one of them will be useful.

#### c.4.2 Matching pennies situation.

In order to show that the _matching pennies_ situation can exist on \(x_{4}\), the simplest thing is to propose two linear classifiers for which it happens. We propose the following linear classifiers:

* \(f_{1}=(w_{1},b_{1})\) with \(w_{1}=(0.825,0.565132728)\) and \(b_{1}=0.536876091\).
* \(f_{2}=(w_{2},b_{2})\) with \(w_{2}=(-0.825,0.565132728)\) and \(b_{2}=0.536876091\).

Using Equation (18) one can check that these classifiers match the situation that is illustrated in Figure 5, _i.e._ they are both robust at \(x_{1}\), they are robust at \(x_{3}\) and \(x_{2}\) respectively, they are non-robust at \(x_{4}\) but they can't be attacked on the same region, as their intersections with the \(1-\)ball around \(x_{4}\) are disjoint (gray circular sectors in Figure 5). Let us denote \(\mathbf{h}\) the REC \(f_{1}\) and \(f_{2}\) with probabilities \((\frac{1}{2},\frac{1}{2})\).

#### c.4.3 Adversarial risk calculation for the mixture of \(f_{1}\) and \(f_{2}\).

For \(x_{2}\), \(f_{1}\) is not robust, meaning its adversarial \(0\)-\(1\) loss on \(x_{2}\) is 1. On the other hand, \(f_{2}\) is robust on \(x_{2}\), as no perturbation of norm at most \(\epsilon\) will make it predict the wrong class. For these reasons, the adversarial \(0\)-\(1\) loss on \(x_{2}\) for \(\mathbf{h}\) is \(\frac{1}{2}\). The same goes for \(x_{3}\), and therefore they each add \(\frac{1}{12}\) to the total adversarial risk (mass of each point times the \(0\)-\(1\) loss).

At the point \(x_{4}\), given the _matching pennies_ situation, the adversarial \(0\)-\(1\) for \(\mathbf{h}\) is \(\frac{1}{2}\), for a total adversarial risk of \(\frac{1}{12}\). As both \(f_{1}\) and \(f_{2}\) are robust on \(x_{1}\), the adversarial risk on \(x_{1}\) is 0, and we can conclude that the total adversarial risk of \(\mathbf{h}\) is \(\frac{3}{12}\ =\ \frac{1}{4}\), which is less than the optimal adversarial risk when considering linear classifiers only, that is \(\frac{1}{3}\).

### On the toy example in Trillos et al. [45, Section 5.2]

The example shown in Trillos et al. [45, Section 5.2] involves three points of different classes. We are interested in Case 4-(i), in which each pair of points can be merged into one by the adversary, but the three cannot be merged because their three \(\epsilon\)-balls have an empty intersection, as depicted in Figure 6. For simplicity, we assume each point has a probability \(\omega_{i}\) of \(\frac{1}{3}\), which satisfies the condition \(\omega_{1}<\omega_{2}+\omega_{3}\) on Case 4 - (i) in [45].

Authors show that in this case, the optimal attack consist on distributing the mass of the original points into the two neighbors in such a way that, for each of the barycenters \(\bar{x}_{ij}\), the mass coming from each of the original points \(x_{i}\) and \(x_{j}\) is the same. In our simplified example, this means that each original point sends \(\frac{1}{6}\) of its mass to each neighboring barycenter, as depicted in Figure 6(a). When it comes to an optimal classifier, the one presented in Figure 6(b) is optimal and coincides with the one described in the original work of [45]. This classifier predicts the original class for each \(x_{i}\) inside the part of the ball \(B_{\epsilon}(x_{i})\) that is not intersecting any of the other balls. For each intersection \(B_{\epsilon}(x_{j})\cap B_{\epsilon}(x_{k})\), the classifier predicts class \(j\) and \(k\) with probability \(\frac{1}{2}\) each, and the third class \(i\) with probability 0. This classifier achieves an adversarial risk of \(\frac{1}{2}\), because at any point of the new adversarial distribution that is supported on \(\{\bar{x}_{12},\bar{x}_{13},\bar{x}_{23}\}\), this classifier has probability \(\frac{1}{2}\) of predicting the correct class.

From the perspective of our work, it is interesting that the optimal probabilistic classifier shown in Figure 6(b) can be built as a uniform REC of 6 deterministic classifiers \(f_{ijk}\) that predict \(i\) in \(B_{\epsilon}(x_{i})\), \(j\) in \(B_{\epsilon}(x_{j})\setminus B_{\epsilon}(x_{i})\) and \(k\) in \(B_{\epsilon}(x_{k})\setminus(B_{\epsilon}(x_{i})\cup B_{\epsilon}(x_{j}))\) (see Figure 8). Each \(f_{ijk}\) has standard accuracy of 1, and adversarial accuracy of \(\frac{1}{3}\) (risk of \(\frac{2}{3}\)) against an optimal attack, like the constant

Figure 6: Toy example in Trillos et al. [45, Section 5.2], Case 4-(i). Best viewed in color.

Figure 7: Solution for toy example in Trillos et al. [45, Section 5.2], Case 4-(i). Best viewed in color.

classifiers, because it can only be robust at one point at most. They are optimal when restricting to deterministic classifiers. Additionally, the uniform mixture of these 6 classifiers coincides with the classifier presented in Figure 6(b), which has standard accuracy of 1, and adversarial accuracy of \(\frac{1}{2}\), meaning an increase in performance of \(\frac{1}{2}-\frac{1}{3}=\frac{1}{6}\) with respect to the deterministic \(f_{ijk}\).

One can compute the matching penny gap of this mixture of 6 classifiers in this dataset as follows: at each original point \(x_{i}\), there are only two classifiers that are robust (\(f_{ijk}\) and \(f_{ikj}\)), which means \(\mu(\mathcal{H}_{vb}(x_{i},i))=\frac{4}{6}+\frac{2}{3}\). On the other hand, a single perturbation will not fool all the four vulnerable models. Starting from \(x_{i}\), the attacker might move this point towards \(x_{j}\) or \(x_{k}\). Any of these perturbations will fool three of the six classifiers. For example, the attack \(x_{1}\rightarrow\bar{x}_{12}\) will fool the subset \(\{f_{213},f_{231},f_{321}\}\), while the other three classifiers will correctly predict the class \(1\) at \(\bar{x}_{12}\). In conclusion, \(\mu^{\max}(x_{i},i)=\frac{3}{6}=\frac{1}{2}\). We conclude that at each point, the matching penny gap is exactly \(\frac{2}{3}-\frac{1}{2}=\frac{1}{6}\), for a total expected matching penny gap of \(\frac{1}{6}\) (recall every point had the same mass). The average risk is \(\frac{2}{3}\), and the risk of the mixture can be found using Equation (6) from Theorem 3.2 as \(\frac{2}{3}-\frac{1}{6}=\frac{1}{2}\).

## Appendix D Proofs

**Theorem 3.1**.: For a probabilistic classifier \(\mathbf{h}_{\mu}:\mathcal{X}\rightarrow\mathcal{P}(\mathcal{Y})\) constructed from a BHS \(\mathcal{H}_{b}\) using any \(\mu\in\mathcal{P}(\mathcal{H}_{b})\), we have \(\mathcal{R}_{e}(\mathbf{h}_{\mu})\leq\mathbb{E}_{h\sim\mu}\left[\mathcal{R}_{e }(h)\right]\).

Proof.: For any \(\mu\in\mathcal{P}(\mathcal{H}_{b})\), we have the following.

\[\mathcal{R}_{e}(\mathbf{h}_{\mu})=\mathop{\mathbb{E}}_{(x,y)\sim \rho}\left[\sup_{x^{\prime}\in B_{e}(x)}\ell^{\text{0-1}}((x^{\prime},y), \mathbf{h}_{\mu})\right] =\mathop{\mathbb{E}}_{(x,y)\sim\rho}\left[\sup_{x^{\prime}\in B_ {e}(x)}\mathop{\mathbb{E}}_{h\sim\mu}[\mathds{1}\{h(x^{\prime})\neq y\}]\right]\] \[\leq\mathop{\mathbb{E}}_{(x,y)\sim\rho}\left[\mathop{\mathbb{E}} _{h\sim\mu}\left[\sup_{x^{\prime}\in B_{e}(x)}\mathds{1}\{h(x^{\prime})\neq y \}\right]\right]\] \[=\mathop{\mathbb{E}}_{h\sim\mu}\left[\mathop{\mathbb{E}}_{(x,y) \sim\rho}\left[\sup_{x^{\prime}\in B_{e}(x)}\mathds{1}\{h(x^{\prime})\neq y \}\right]\right]{}^{2}\] \[=\mathop{\mathbb{E}}_{h\sim\mu}\left[\mathcal{R}_{e}(h)\right].\]

Taking infimum with respect to \(\mu\) on both sides of the above inequality, we get the following.

\[\inf_{\mu\in\mathcal{P}(\mathcal{H}_{b})}\mathcal{R}_{e}(\mathbf{h}_{\mu}) \leq\inf_{\mu\in\mathcal{P}(\mathcal{H}_{b})}\mathop{\mathbb{E}}_{h\sim\mu} \left[\mathcal{R}_{e}(h)\right] \tag{19}\]

Figure 8: Examples of deterministic classifiers that allow the construction of the optimal classifier as a REC

For any \(h\in\mathcal{H}_{b}\), we may choose the Dirac measure \(\mu_{h}\) that assigns probability \(1\) to \(h\) in order to obtain \(\inf_{\mu\in\mathcal{P}(\mathcal{H}_{b})}\mathbb{E}_{h\sim\mu}\left[\mathcal{R}_{ \epsilon}(h)\right]\leq\mathbb{E}_{h\sim\mu_{h}}\left[\mathcal{R}_{\epsilon}(h )\right]=\mathcal{R}_{\epsilon}(h)\). Taking infimum over \(h\in\mathcal{H}_{b}\), we get the following.

\[\inf_{\mu\in\mathcal{P}(\mathcal{H}_{b})}\mathop{\mathbb{E}}_{h\sim\mu}\left[ \mathcal{R}_{\epsilon}(h)\right]\leq\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{ \epsilon}(h). \tag{20}\]

The remaining assertion of the theorem follows by combining (19) with (20).

**Theorem 3.2**.: For a probabilistic classifier \(\mathbf{h}_{\mu}:\mathcal{X}\rightarrow\mathcal{P}(\mathcal{Y})\) constructed from a BHS \(\mathcal{H}_{b}\) using any \(\mu\in\mathcal{P}(\mathcal{H}_{b})\),

\[\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu})=\mathop{\mathbb{E}}_{h\sim\mu}\left[ \mathcal{R}_{\epsilon}(h)\right]-\mathop{\mathbb{E}}_{(x,y)\sim\rho}[\pi_{ \mathbf{h}_{\mu}}(x,y)]. \tag{6}\]

Proof.: Observe that for any \(h\in\mathcal{H}_{b}\), \(\sup_{x^{\prime}\in B_{\epsilon}(x)}\mathds{1}\{h(x^{\prime})\neq y\}=1\) if and only if \(h\in\mathcal{H}_{vb}(x,y)\). Hence,

\[\mathop{\mathbb{E}}_{h\sim\mu}\left[\sup_{x^{\prime}\in B_{\epsilon}(x)} \mathds{1}\{h(x^{\prime})\neq y\}\right]=\mathop{\mathbb{E}}_{h\sim\mu}\left[ \mathds{1}\{h\in\mathcal{H}_{vb}(x,y)\}\right]=\mu(\mathcal{H}_{vb}(x,y)).\]

Taking expectation on both sides with respect to \((x,y)\sim\rho\), we get

\[\mathop{\mathbb{E}}_{(x,y)\sim\rho}[\mu(\mathcal{H}_{vb}(x,y))]=\mathop{ \mathbb{E}}_{(x,y)\sim\rho}\mathop{\mathbb{E}}_{h\sim\mu}\left[\sup_{x^{\prime }\in B_{\epsilon}(x)}\mathds{1}\{h(x^{\prime})\neq y\}\right]=\mathop{\mathbb{ E}}_{h\sim\mu}\left[\mathcal{R}_{\epsilon}(h)\right], \tag{21}\]

where the second equality follows from switching the order of the two preceding expectations.

For any \(x^{\prime}\in B_{\epsilon}(x)\),

\[\mathop{\mathbb{E}}_{h\sim\mu}\left[\mathds{1}\{h(x^{\prime})\neq y\}\right]= \mu(\{h\in\mathcal{H}_{b}:h(x^{\prime})\neq y\})\leq\mu^{\max}(x,y),\]

where the above inequality holds because the \(\mu\) measure of any subset of \(\mathcal{H}_{b}\) that is simultaneously vulnerable at some \(x^{\prime}\in B_{\epsilon}(x)\) is at most \(\mu^{\max}(x,y)\). Taking supremum over all \(x^{\prime}\in B_{\epsilon}(x)\) in the above inequality, we get the following.

\[\sup_{x^{\prime}\in B_{\epsilon}(x)}\mathop{\mathbb{E}}_{h\sim\mu}\left[ \mathds{1}\{h(x^{\prime})\neq y\}\right]\leq\mu^{\max}(x,y).\]

We will now show that the above inequality also holds in the other direction.

Let \(\{\mathcal{H}^{k}\}_{k=1}^{\infty}\) be a sequence of sets, \(\mathcal{H}^{k}\in\mathfrak{H}_{svb}(x,y)\), such that \(\lim_{k\rightarrow\infty}\mu(\mathcal{H}^{k})=\mu^{max}(x,y)\). For each \(\mathcal{H}^{k}\), we have by definition of \(\mathfrak{H}_{svb}(x,y)\) that there exists some \(x^{k}\in B_{\epsilon}(x)\) such that all classifiers \(h\in\mathcal{H}^{k}\) are fooled by \(x^{k}\). In other words, the total mass of classifiers that are fooled by \(x^{k}\) is greater or equal to \(\mu(\mathcal{H}^{k})\). This gives

\[\sup_{x^{\prime}\in B_{\epsilon}(x)}\mathop{\mathbb{E}}_{h\sim\mu}\left[ \mathds{1}\{h(x^{\prime})\neq y\}\right]\geq\mathop{\mathbb{E}}_{h\sim\mu} \left[\mathds{1}\{h(x^{k})\neq y\}\right]\geq\mu(\mathcal{H}^{k}).\]

This is true for every \(k\), so taking \(k\rightarrow\infty\) we get that \(\sup_{x^{\prime}\in B_{\epsilon}(x)}\mathop{\mathbb{E}}_{h\sim\mu}\left[ \mathds{1}\{h(x^{\prime})\neq y\}\right]\geq\lim_{k\rightarrow\infty}\mu( \mathcal{H}^{k})=\mu^{max}(x,y)\).

Taking expectation on both sides with respect to \((x,y)\sim\rho\), we get

\[\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu})=\mathop{\mathbb{E}}_{(x,y)\sim\rho} \left[\sup_{x^{\prime}\in B_{\epsilon}(x)}\mathop{\mathbb{E}}_{h\sim\mu}\left[ \mathds{1}\{h(x^{\prime})\neq y\}\right]\right]=\mathop{\mathbb{E}}_{(x,y) \sim\rho}\left[\mu^{max}(x,y)\right]. \tag{22}\]

Combining (22) and (21) yields (6).

**Corollary 3.1**.: For \(\mu^{\prime}\in\mathcal{P}(\mathcal{H}_{b})\), \(\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu^{\prime}})<\inf_{h\in\mathcal{H}_{b}} \mathcal{R}_{\epsilon}(h)\) if and only if the following condition holds.

\[\mathop{\mathbb{E}}_{(x,y)\sim\rho}[\pi_{\mathbf{h}_{\mu^{\prime}}}(x,y)]> \mathop{\mathbb{E}}_{h\sim\mu^{\prime}}[\mathcal{R}_{\epsilon}(h)]-\inf_{h\in \mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h) \tag{9}\]

Additionally, \(\inf_{\mu\in\mathcal{P}(\mathcal{H}_{b})}\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu })<\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h)\) if and only if there exists \(\mu^{\prime}\in\mathcal{P}(\mathcal{H}_{b})\) for which (9) holds.

Proof.: Suppose (9) holds for some \(\mu^{\prime}\in\mathcal{P}(\mathcal{H}_{b})\). Then,

\[\inf_{\mu\in\mathcal{P}(\mathcal{H}_{b})}\mathcal{R}_{\epsilon}( \mathbf{h}_{\mu})\leq\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu^{\prime}}) =\mathop{\mathbb{E}}_{h\sim\mu^{\prime}}[\mathcal{R}_{\epsilon}(h)]- \mathop{\mathbb{E}}_{(x,y)\sim\rho}[\pi_{\mathbf{h}_{\mu^{\prime}}}(x,y)]\] \[<\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h)+\mathop{ \mathbb{E}}_{(x,y)\sim\rho}[\pi_{\mathbf{h}_{\mu^{\prime}}}(x,y)]-\mathop{ \mathbb{E}}_{(x,y)\sim\rho}[\pi_{\mathbf{h}_{\mu^{\prime}}}(x,y)]\] \[=\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h),\]

where the first equality follows from Theorem 3.2 and the second inequality follows from the assumption in (9). Suppose (9) does not hold for \(\mu^{\prime}\in\mathcal{P}(\mathcal{H}_{b})\). Then,

\[\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu^{\prime}}) =\mathop{\mathbb{E}}_{h\sim\mu^{\prime}}\left[\mathcal{R}_{ \epsilon}(h)\right]-\mathop{\mathbb{E}}_{(x,y)\sim\rho}[\pi_{\mathbf{h}_{\mu^ {\prime}}}(x,y)]\] \[\geq\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h)+\mathop{ \mathbb{E}}_{(x,y)\sim\rho}[\pi_{\mathbf{h}_{\mu^{\prime}}}(x,y)]-\mathop{ \mathbb{E}}_{(x,y)\sim\rho}[\pi_{\mathbf{h}_{\mu^{\prime}}}(x,y)]\] \[=\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h).\]

Suppose (9) does not hold for any \(\mu^{\prime}\in\mathcal{P}(\mathcal{H}_{b})\), then taking infimum with respect to \(\mu^{\prime}\in\mathcal{P}(\mathcal{H}_{b})\) in the above inequality, we get \(\inf_{\mu^{\prime}\in\mathcal{P}(\mathcal{H}_{b})}\mathcal{R}_{\epsilon}( \mathbf{h}_{\mu^{\prime}})\geq\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{\epsilon} (h)\). 

**Lemma 4.1**.: Let \(\mathbf{h}:\mathcal{X}\rightarrow[0,1]\) be any measurable function. For any \(\succ\in\{>,\geq\}\), the following inequality holds, and it becomes an equality if \(\mathbf{h}\) is continuous or takes finitely many values:

\[\mathds{1}\bigg{\{}\left(\sup_{x^{\prime}\in B_{\epsilon}(x)}\mathbf{h}(x^{ \prime})\right)\succ\alpha\bigg{\}}\geq\sup_{x^{\prime}\in B_{\epsilon}(x)} \mathds{1}\{\mathbf{h}(x^{\prime})\succ\alpha\} \tag{11}\]

Proof.: As both functions only take the values \(0\) and \(1\), it suffices to show that if the RHS is equal to \(1\), then so is the LHS. Suppose \(\sup_{x^{\prime}\in B_{\epsilon}(x)}\mathds{1}\{\mathbf{h}(x^{\prime})\succ \alpha\}=1\). As the function \(\mathds{1}\{\mathbf{h}(x^{\prime})\succ\alpha\}\) takes only a finite number of values, this implies that there exists some \(x^{*}\in B_{\epsilon}\left(x\right)\) such that \(\mathds{1}\{\mathbf{h}(x*)\succ\alpha\}=1\). This means that \(\mathbf{h}(x*)\succ\alpha\), and therefore \(\sup_{x^{\prime}\in B_{\epsilon}(x)}\mathbf{h}(x^{\prime})\succ\alpha\). This makes the LHS equal to \(1\).

If we further assume that \(\mathbf{h}\) is continuous or takes a finite number of values, then if the LHS is equal to one, 

**Theorem 4.1**.: Let \(\mathbf{h}:\mathcal{X}\rightarrow[0,1]\) be any probabilistic binary classifier. Let \(h^{\alpha}\) be the \(\alpha\)_-threshold_ classifier. Then the following equation holds:

\[\mathcal{R}_{\epsilon}(\mathbf{h})\geq\int_{0}^{1}\mathcal{R}_{\epsilon}(h^{ \alpha})d\alpha\geq\inf_{\alpha}\mathcal{R}_{\epsilon}(h^{\alpha}). \tag{12}\]

Further, if \(\mathbf{h}\) is either continuous or takes finitely many values, the first inequality in (12) becomes an equality.

Proof.: We begin the proof by rewriting the adversarial risk of \(\mathbf{h}\) in terms of the adversarial risk for each class. To alleviate notation, we denote \(\ell_{\epsilon}^{0\text{-}1}((x,y),\mathbf{h})=\sup_{x^{\prime}\in B_{\epsilon }(x)}\ell^{0\text{-}1}((x^{\prime},y),\mathbf{h})\). Recall from Equation (1) that

\[\mathcal{R}_{\epsilon}(\mathbf{h}) =\nu(0)\cdot\mathcal{R}_{\epsilon}^{0}(\mathbf{h})+\nu(1)\cdot \mathcal{R}_{\epsilon}^{1}(\mathbf{h}) \tag{23}\]Let us now develop the terms \(R^{y}_{\epsilon}(\mathbf{h})\). For any \(\succ\in\{>,\geq\}\) we obtain the following:

\[R^{y}_{\epsilon}(\mathbf{h}) =\underset{x\sim p_{y}}{\mathbb{E}}\left[\ell_{\epsilon}^{0\text{- 1}}((x,y),\mathbf{h})\right] \tag{24}\] \[=\underset{x\sim p_{y}}{\mathbb{E}}\left[\int_{0}^{1}\mathds{1} \{\ell_{\epsilon}^{0\text{-1}}((x,y),\mathbf{h})\succ\alpha\}d\alpha\right]\] \[=\int_{0}^{1}\underset{x\sim p_{y}}{\mathbb{E}}\left[\mathds{1} \{\ell_{\epsilon}^{0\text{-1}}((x,y),\mathbf{h})\succ\alpha\}\right]d\alpha\]

In the last equation, we were able to interchange the two integrals using Tonnelli's theorem. Indeed, the function \((x,\alpha)\mapsto\mathds{1}\{\ell_{\epsilon}^{0\text{-1}}((x,y),\mathbf{h})\succ\alpha\}\) is Lebesgue measurable in \(\mathcal{X}~{}\times~{}\mathbb{R}\) if \(\ell_{\epsilon}^{0\text{-1}}((\cdot,0),\mathbf{h})\) is Lebesgue measurable, which is the case [40, Theorem 1].

The next step is to use Lemma 4.1 to interchange the supremum operator and the indicator function. This will make the adversarial risk of the \(h^{\alpha}\) appear. For \(y=0\) and replacing the operator \(\succ\) by \(>\), we obtain the following:

\[R^{0}_{\epsilon}(\mathbf{h}) =\int_{0}^{1}\underset{x\sim p_{0}}{\mathbb{E}}\left[\mathds{1} \{\ell_{\epsilon}^{0\text{-1}}((x,0),\mathbf{h})>\alpha\}\right]d\alpha \tag{25}\] \[\geq\int_{0}^{1}\underset{x\sim p_{0}}{\mathbb{E}}\left[\sup_{x^ {\prime}\in B_{\epsilon}(x)}\mathds{1}\{\mathbf{h}(x^{\prime})>\alpha\}\right]d\alpha\] \[\geq\int_{0}^{1}\mathcal{R}^{0}_{\epsilon}(h^{\alpha})d\alpha\]

Now we do a similar development for \(y=1\) and replacing the operator \(\succ\) by \(\geq\) to obtain:

\[R^{1}_{\epsilon}(\mathbf{h}) \geq\int_{0}^{1}\underset{x\sim p_{1}}{\mathbb{E}}\left[\sup_{x^ {\prime}\in B_{\epsilon}(x)}\mathds{1}\{1-\mathbf{h}(x^{\prime})\geq\alpha\} \right]d\alpha \tag{26}\] \[\geq\int_{0}^{1}\underset{x\sim p_{1}}{\mathbb{E}}\left[\sup_{x^ {\prime}\in B_{\epsilon}(x)}\mathds{1}\{1-\alpha\geq\mathbf{h}(x^{\prime})\} \right]d\alpha\] \[\geq\int_{0}^{1}\underset{x\sim p_{1}}{\mathbb{E}}\left[\sup_{x^ {\prime}\in B_{\epsilon}(x)}1-\mathds{1}\{\mathbf{h}(x^{\prime})>1-\alpha\} \right]d\alpha\] \[\geq\int_{0}^{1}\underset{x\sim p_{1}}{\mathbb{E}}\left[\sup_{x^ {\prime}\in B_{\epsilon}(x)}1-\mathbf{h}^{1-\alpha}(x^{\prime})\right]d\alpha\] \[\geq\int_{0}^{1}\underset{x\sim p_{1}}{\mathbb{E}}\left[\sup_{x ^{\prime}\in B_{\epsilon}(x)}\ell^{0\text{-1}}((x,1),\mathbf{h}^{1-\alpha}) \right]d\alpha\] \[\geq\int_{0}^{1}\mathcal{R}^{1}_{\epsilon}(\mathbf{h}^{1-\alpha} )d\alpha=\int_{0}^{1}\mathcal{R}^{1}_{\epsilon}(\mathbf{h}^{u})du\]

In the last equation, the change of variable \(u=1-\alpha\) allows us to complete the calculations and obtain the desired result. Putting everything together, we obtain the following result about the original probabilistic classifier \(\mathbf{h}\):

\[\mathcal{R}_{\epsilon}(\mathbf{h})\geq\int_{0}^{1}\nu(0)\mathcal{R}^{0}_{ \epsilon}(h^{\alpha})+\nu(1)\mathcal{R}^{1}_{\epsilon}(h^{\alpha})d\alpha= \int_{0}^{1}\mathcal{R}_{\epsilon}(h^{\alpha})d\alpha \tag{27}\]

In particular, Equation (27) implies that \(\mathcal{R}_{\epsilon}(\mathbf{h})\geq\min_{\alpha}\mathcal{R}_{\epsilon}(h^{ \alpha})\), meaning that for any \(\mathbf{h}\) probabilistic binary classifier, there is always a deterministic \(\alpha\)-threshold classifier with better or equal adversarial risk. 

**Corollary 5.1**.: Let \(\mathcal{H}_{b}\) be any family of deterministic binary classifiers. Let \(\mathcal{M}=\mathcal{P}_{M}(\mathcal{H}_{b})\subset\mathcal{P}(\mathcal{H}_{b})\) be the subset of probability measures over \(\mathcal{H}_{b}\) defining RECs as in Section 2.3. Let \(\mathcal{A}=\{h^{-1}(1):h\in\mathcal{H}_{b}\}\). If \(\mathcal{A}\) is closed under union and intersection, then

\[\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{\epsilon}(h)=\inf_{\mu\in\mathcal{P}_{ M}(\mathcal{H}_{b})}\mathcal{R}_{\epsilon}(\mathbf{h}_{\mu}).\]Proof.: Theorem 4.1 applied to RECs (see Section 4.2) tells us that \(\overline{\mathcal{H}_{b}}(\mathcal{P}_{M}(\mathcal{H}_{b}))\) is the set of all weighted ensembles built from \(\mathcal{H}_{b}\). It is clear that for any \(h\in\mathcal{H}_{b},\ \delta_{h}\in\mathcal{P}_{M}(\mathcal{H}_{b})\), so \(\mathcal{H}_{b}\subseteq\overline{\mathcal{H}_{b}}(\mathcal{P}_{M}(\mathcal{ H}_{b}))\). Let us now show the other inclusion.

Take any \(\mu\in\mathcal{P}_{M}(\mathcal{H}_{b})\) and consider any weighted ensemble \(h^{\alpha}\) over \(\mathcal{H}_{b}\) of the form \(h^{\alpha}(x)=\mathds{1}\{\sum_{m\in[M]}p_{m}h_{m}>\alpha\}\) with \(p_{m}=\mu(h_{m})\). Define the function \(g^{\alpha}:\{0,1\}^{M}\to\{0,1\}\) as \(g^{\alpha}(z_{1}\ldots z_{M})=\mathds{1}\left\{\sum_{m=1}^{M}p_{m}z_{m}> \alpha\right\}\). Then, \(h^{\alpha}\) can be written as \(g^{\alpha}\left(h_{1}(x)\ldots h_{M}(x)\right)\). Because the \(p_{m}\) are positive, the function \(g^{\alpha}\) is a monotone boolean function. As any monotone boolean function, \(g^{\alpha}\) can be written as a disjunctive normal form (DNF) without negations [9]. Thus, the set \(A_{\mathrm{ENS}}=\{x\in\mathcal{X}:h^{\alpha}(x)=1\}\) is a union of intersections over the sets \(A_{1}\ldots A_{M}\) where \(A_{k}=h_{m}^{-1}(1)\). Because \(\mathcal{A}\) is closed under union and intersection, \(A_{\mathrm{ENS}}\in\mathcal{A}\), which means that \(h^{\alpha}\in\mathcal{H}_{b}\). Thus, \(\overline{\mathcal{H}_{b}}(\mathcal{P}_{M}(\mathcal{H}_{b}))\subseteq\mathcal{ H}_{b}\).

As all the hypothesis of Theorem 5.1 are met, we can conclude that \(\inf_{h\in\mathcal{H}_{b}}\mathcal{R}_{e}(h)=\inf_{\mu\in\mathcal{P}_{M}( \mathcal{H}_{b})}\mathcal{R}_{e}(\mathbf{h}_{\mu})\).