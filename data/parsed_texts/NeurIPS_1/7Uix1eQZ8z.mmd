# A State Representation for Diminishing Rewards

 Ted Moskovitz

Gatsby Unit, UCL

ted@gatsby.ucl.ac.uk &Samo Hromadka

Gatsby Unit, UCL

samo.hromadka.21@ucl.ac.uk &Ahmed Touati

Meta

atouati@meta.com &Diana Borsa

Google DeepMind

borsa@google.com &Maneesh Sahani

Gatsby Unit, UCL

maneesh@gatsby.ucl.ac.uk

A common setting in multitask reinforcement learning (RL) demands that an agent rapidly adapt to various stationary reward functions randomly sampled from a fixed distribution. In such situations, the successor representation (SR) is a popular framework which supports rapid policy evaluation by decoupling a policy's expected discounted, cumulative state occupancies from a specific reward function. However, in the natural world, sequential tasks are rarely independent, and instead reflect shifting priorities based on the availability and subjective perception of rewarding stimuli. Reflecting this disjunction, in this paper we study the phenomenon of diminishing marginal utility and introduce a novel state representation, the \(\lambda\) representation (\(\lambda\)R) which, surprisingly, is required for policy evaluation in this setting and which generalizes the SR as well as several other state representations from the literature. We establish the \(\lambda\)R's formal properties and examine its normative advantages in the context of machine learning, as well as its usefulness for studying natural behaviors, particularly foraging.

## 1 Introduction

The second ice cream cone rarely tastes as good as the first, and once all the most accessible brambles have been picked, the same investment of effort yields less fruit. In everyday life, the availability and our enjoyment of stimuli is sensitive to our past interactions with them. Thus, to evaluate different courses of action and act accordingly, we might expect our brains to form representations sensitive to the non-stationarity of rewards. Evidence in fields from behavioral economics [1, 2] to neuroscience [3] supports this hypothesis. Surprisingly, however, most of reinforcement learning (RL) takes place under the assumptions of the Markov Decision Process [MDP; 4], where rewards and optimal decision-making remain stationary.

In this paper, we seek to bridge this gap by studying the phenomenon of _diminishing marginal utility_[5] in the context of RL. Diminishing marginal utility (DMU) is the subjective phenomenon by which repeated exposure to a rewarding stimulus reduces the perceived utility one experiences. While DMU is thought to have its roots in the maintenance of homeostatic equilibrium (too much ice cream can result in a stomach ache), it also manifests itself in domains in which the collected rewards are abstract, such as economics ($10 vs. $0 is perceived as a bigger difference in value than $1,010 vs. $1,000), where it is closely related to risk aversion [6, 7]. While DMU is well-studied in other fields, relatively few RL studies have explored diminishing reward functions [8, 9], and, to our

Figure 1.1: Diminishing rewards.

knowledge, none contain a formal analysis of DMU within RL. Here, we seek to characterize both its importance and the challenge it poses for current RL approaches (Section 3).

Surprisingly, we find that evaluating policies under diminishing rewards requires agents to learn a novel state representation which we term the \(\lambda\)_representation_ (\(\lambda\mathrm{R}\), Section 4). The \(\lambda\mathrm{R}\) generalizes several state representations from the RL literature: the _successor representation_[SR; 10], the _first-occupancy representation_[FR; 11], and the _forward-backward representation_[FBR; 12], adapting them for non-stationary environments. Interestingly, despite the non-stationarity of the underlying reward functions, we show that the \(\lambda\mathrm{R}\) still admits a Bellman recursion, allowing for efficient computation via dynamic programming (or approximate dynamic programming) and prove its convergence. We demonstrate the scalability of the \(\lambda\mathrm{R}\) to large and continuous state spaces (Section 4.1), show that it supports policy evaluation, improvement, and composition (Section 5), and show that the behavior it induces is consistent with optimal foraging theory (Section 6).

## 2 Preliminaries

Standard RLIn standard RL, the goal of the agent is to act within its environment so as to maximize its discounted cumulative reward for some task \(T\)[13]. Typically, \(T\) is modeled as a discounted MDP, \(T=(\mathcal{S},\mathcal{A},p,r,\gamma,\mu_{0})\), where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the set of available actions, \(p:\mathcal{S}\times\mathcal{A}\mapsto\mathcal{P}(\mathcal{S})\) is the transition kernel (where \(\mathcal{P}(\mathcal{S})\) is the space of probability distributions over \(\mathcal{S}\)), \(r:\mathcal{S}\mapsto\mathbb{R}\) is the reward function, \(\gamma\in[0,1)\) is a discount factor, and \(\mu_{0}\in\mathcal{P}(\mathcal{S})\) is the distribution over initial states. Note that the reward function is also frequently defined over state-action pairs \((s,a)\) or triples \((s,a,s^{\prime})\), but to simplify notation we mostly consider state-based rewards. All of the following analysis, unless otherwise noted, can easily be extended to the \((s,a)\)- or \((s,a,s^{\prime})\)-dependent cases. The goal of the agent is to maximize its expected _return_, or discounted cumulative reward \(\sum_{t}\gamma^{t}r(s_{t})\). The role of the discount factor is twofold: from a theoretical standpoint, it ensures that this sum is finite for bounded rewards, and it induces myopia in the agent's behavior. To simplify notation, we will frequently write \(r(s_{t})\triangleq r_{t}\) and \(\mathbf{r}\in\mathbb{R}^{|\mathcal{S}|}\) as the vector of rewards for each state. The agent acts according to a stationary policy \(\pi:\mathcal{S}\mapsto\mathcal{P}(\mathcal{A})\). For finite MDPs, we can describe the expected transition probabilities under \(\pi\) using a \(|\mathcal{S}|\times|\mathcal{S}|\) matrix \(P^{\pi}\) such that \(P^{\pi}_{s,s^{\prime}}=p^{\pi}(s^{\prime}|s)\triangleq\sum_{a}p(s^{\prime}|s,a )\pi(a|s)\). Given \(\pi\) and a reward function \(r\), the expected return associated with taking action \(a\) in state \(s\) is

\[Q^{\pi}_{r}(s,a)=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^{k}r_{t+k} \Big{|}s_{t}=s,a_{t}=a\right]=\mathbb{E}_{s^{\prime}\sim p^{\pi}(\cdot\mid s)} \left[r_{t}+\gamma Q^{\pi}_{r}(s^{\prime},\pi(s^{\prime}))\right]. \tag{2.1}\]

The \(Q^{\pi}_{r}\) are called the state-action values or simply the \(Q\)-values of \(\pi\). The expectation \(\mathbb{E}_{\pi}\left[\cdot\right]\) is taken with respect to the randomness of both the policy and the transition dynamics. For simplicity of notation, from here onwards we will write expectations of the form \(\mathbb{E}_{\pi}\left[\cdot|s_{t}=s,a_{t}=a\right]\) as \(\mathbb{E}_{\pi}\left[\cdot|s_{t},a_{t}\right]\). The recursive form in Eq. (2.1) is called the _Bellman equation_, and it makes the process of estimating \(Q^{\pi}_{r}\)--termed _policy evaluation_--tractable via dynamic programming. In particular, successive applications of the _Bellman operator_\(\mathcal{T}^{\pi}Q\triangleq r+\gamma P^{\pi}Q\) are guaranteed to converge to the true value function \(Q^{\pi}\) for any initial real-valued \(|\mathcal{S}|\times|\mathcal{A}|\) matrix \(Q\). Once a policy has been evaluated, _policy improvement_ identifies a new policy \(\pi^{\prime}\) such that \(Q^{\pi}_{r}(s,a)\geq Q^{\pi^{\prime}}_{r}(s,a),\ \forall(s,a)\in Q^{\pi}_{r}(s,a)\). Helpfully, such a policy can be defined as \(\pi^{\prime}(s)\in\operatorname*{argmax}_{a}Q^{\pi}_{r}(s,a)\).

Value DecompositionOften, it can be useful for an agent to evaluate the policies it's learned on new reward functions. In order to do so efficiently, it can make use of the _successor representation_[SR; 10], which decomposes the value function as follows:

\[V^{\pi}(s)=\mathbb{E}_{\pi}\left[\sum_{k\geq 0}\gamma^{k}r_{t+k}\Big{|}s_{t} \right]=\mathbf{r}^{\mathsf{T}}\underbrace{\mathbb{E}_{\pi}\left[\sum_{k\geq 0 }\gamma^{k}\mathbf{1}(s_{t+k})\bigg{|}s_{t}\right]}_{\triangleq M^{\pi}(s)}, \tag{2.2}\]

where \(\mathbf{1}(s_{t})\) is a one-hot representation of state \(s_{t}\) and \(M^{\pi}\), the SR, is an \(|\mathcal{S}|\times|\mathcal{S}|\) matrix such that

\[M^{\pi}(s,s^{\prime})\triangleq\mathbb{E}_{\pi}\left[\sum_{k\geq 0}\gamma^{k} \mathbbm{1}(s_{t+k}=s^{\prime})\bigg{|}s_{t}\right]=\mathbb{E}_{\pi}\left[ \mathbbm{1}(s_{t}=s^{\prime})+\gamma M^{\pi}(s_{t+1},s^{\prime})\bigg{|}s_{t} \right]. \tag{2.3}\]Because the SR satisfies a Bellman recursion, it can be learned in a similar manner to the value function, with the added benefit that if the transition kernel \(p\) is fixed, it can be used to instantly evaluate a policy on a task determined by any reward vector \(\mathbf{r}\). The SR can also be conditioned on actions, \(M^{\pi}(s,a,s^{\prime})\), in which case multiplication by \(\mathbf{r}\) produces the \(Q\)-values of \(\pi\). The SR was originally motivated by the idea that a representation of state in the brain should be dependent on the similarity of future paths through the environment, and there is evidence that SR-like representations are present in the hippocampus [14]. A representation closely related to the SR is the _first-occupancy representation_[11], which modifies the SR by only counting the first visit to each state:

\[F^{\pi}(s,s^{\prime})\triangleq\mathbb{E}_{\pi}\left[\sum_{k\geq 0}\gamma^{k} \mathbbm{1}(s_{t+k}=s^{\prime},s^{\prime}\notin\{s_{t},\ldots,s_{t+k-1}\}) \right|s_{t}\right]. \tag{2.4}\]

The FR can be used in the same way as the SR, with the difference that the values it computes are predicated on ephemeral rewards--those that are consumed or vanish after the first visit to a state.

Policy CompositionIf the agent has access to a set of policies \(\Pi=\{\pi\}\) and their associated SRs (or FRs) and is faced by a new task determined by reward vector \(\mathbf{r}\), it can instantly evaluate these policies by simply multiplying: \(\{Q^{\pi}(s,a)=\mathbf{r}^{\mathsf{T}}M^{\pi}(s,a,\cdot)\}\). This process is termed _generalized policy evaluation_[GPE; 15]. Similarly, _generalized policy improvement_ (GPI) is defined as the identification of a new policy \(\pi^{\prime}\) such that \(Q^{\pi^{\prime}}(s,a)\geq\sup_{\tau\in\Pi}Q^{\pi}(s,a)\ \forall s,a\in \mathcal{S}\times\mathcal{A}\). Combining both provides a way for an agent to efficiently _compose_ its policies \(\Pi\)--that is, to combine them to produce a new policy without additional learning. This process, which we refer to as GPE+GPI, produces the following policy, which is guaranteed to perform at least as well as any policy in \(\Pi\)[16]:

\[\pi^{\prime}(s)\in\operatorname*{argmax}_{a\in\mathcal{A}}\max_{\pi\in\Pi} \mathbf{r}^{\mathsf{T}}M^{\pi}(s,a,\cdot). \tag{2.5}\]

## 3 Diminishing Marginal Utility

Problem StatementMotivated by DMU's importance in decision-making, our goal is to understand RL in the context of the following class of non-Markov reward functions:

\[r_{\lambda}(s,t)=\lambda(s)^{n(s,t)}\bar{r}(s),\quad\lambda(s)\in[0,1], \tag{3.1}\]

where \(n(s,t)\in\mathbb{N}\) is the agent's visit count at \(s\) up to time \(t\) and \(\bar{r}(s)\) describes the reward at the first visit to \(s\). \(\lambda(s)\) therefore encodes the extent to which reward diminishes after each visit to \(s\). Note that for \(\lambda(s)=\lambda=1\) we recover the usual stationary reward given by \(\bar{r}\), and so this family of rewards strictly generalizes the stationary Markovian rewards typically used in RL.

DMU is ChallengingAn immediate question when considering reward functions of this form is whether or not we can still define a Bellman equation over the resulting value function. If this is the case, standard RL approaches still apply. However, the following result shows otherwise.

**Lemma 3.1** (Impossibility; Informal).: _Given a reward function of the form Eq. (3.1), it is impossible to define a Bellman equation solely using the resulting value function and immediate reward._

We provide a more precise statement, along with proofs for all theoretical results, in Appendix B. This result means that we can't write the value function corresponding to rewards of the form Eq. (3.1) recursively only in terms of rewards and value in an analogous manner to Eq. (2.1). Nonetheless, we found that it _is_ in fact possible to derive \(a\) recursive relationship in this setting, but only by positing a novel state representation that generalizes the SR and the FR, which we term the \(\lambda\)_representation_ (\(\lambda\)R). In the following sections, we define the \(\lambda\)R, establish its formal properties, and demonstrate its necessity for RL problems with diminishing rewards.

## 4 The \(\lambda\) Representation

A Representation for DMUWe now the derive the \(\lambda\)R by decomposing the value function for rewards of the form Eq. (3.1) and show that it admits a Bellman recursion. To simplify notation, we use a single \(\lambda\) for all states, but the results below readily apply to non-uniform \(\lambda\). We have

\[V^{\pi}(s)=\mathbb{E}\left[\sum_{k=0}^{\infty}\gamma^{k}r_{\lambda}(s_{t+k},k) \big{|}s_{t}=s\right]=\bar{\mathbf{r}}^{\top}\underbrace{\mathbb{E}\left[\sum_{ k=0}^{\infty}\gamma^{k}\lambda^{n_{t}(s_{t+k},k)}\mathbf{1}(s_{t+k})\big{|}s_{t}=s \right]}_{\triangleq\Phi_{\lambda}^{\pi}(s)}, \tag{4.1}\]

where we call \(\Phi_{\lambda}^{\pi}(s)\) the \(\lambda\)_representation_ (\(\lambda\mathrm{R}\)), and \(n_{t}(s,k)\triangleq\sum_{j=0}^{k-1}\mathbb{1}(s_{t+j}=s)\), is the number of times state \(s\) is visited from time \(t\) up to--but not including--time \(t+k\). Formally:

**Definition 4.1** (\(\lambda\mathrm{R}\)).: _For an MDP with finite \(\mathcal{S}\) and \(\boldsymbol{\lambda}\in[0,1]^{|\mathcal{S}|}\), the \(\lambda\) representation is given by \(\Phi_{\lambda}^{\pi}\) such that_

\[\Phi_{\lambda}^{\pi}(s,s^{\prime})\triangleq\mathbb{E}\left[\sum_{k=0}^{ \infty}\lambda(s^{\prime})^{n_{t}(s^{\prime},k)}\gamma^{k}\mathbb{1}(s_{t+k}= s^{\prime})\big{|}s_{t}=s\right] \tag{4.2}\]

_where \(n_{t}(s,k)\triangleq\sum_{j=0}^{k-1}\mathbb{1}(s_{t+j}=s)\) is the number of times state \(s\) is visited from time \(t\) until time \(t+k-1\)._

We can immediately see that for \(\lambda=0\), the \(\lambda\mathrm{R}\) recovers the FR (we take \(0^{0}=1\)), and for \(\lambda=1\), it recovers the SR. For \(\lambda\in(0,1)\), the \(\lambda\mathrm{R}\) interpolates between the two, with higher values of \(\lambda\) reflecting greater persistence of reward in a given state or state-action pair and lower values of \(\lambda\) reflecting more ephemeral rewards.

The \(\lambda\mathrm{R}\) admits a recursive relationship:

\[\Phi_{\lambda}^{\pi}(s,s^{\prime}) =\mathbb{E}\left[\sum_{k=0}^{\infty}\lambda^{n_{t}(s^{\prime},k) }\gamma^{k}\mathbb{1}(s_{t+k}=s^{\prime})\Big{|}s_{t}=s\right]\] \[\stackrel{{(i)}}{{=}}\mathbb{E}\left[\mathbb{1}(s_{t }=s^{\prime})+\lambda^{n_{t}(s^{\prime},1)}\gamma\sum_{k=1}^{\infty}\lambda^{n _{t+1}(s^{\prime},k)}\gamma^{k-1}\mathbb{1}(s_{t+k}=s^{\prime})\Big{|}s_{t}=s\right]\] \[=\mathbb{1}(s_{t}=s^{\prime})(1+\gamma\lambda\mathbb{E}_{s_{t+1} \sim p^{\pi}}\Phi_{\lambda}^{\pi}(s_{t+1},s^{\prime}))+\gamma(1-\mathbb{1}(s_{ t}=s^{\prime}))\mathbb{E}_{s_{t+1}\sim p^{\pi}}\Phi_{\lambda}^{\pi}(s_{t+1},s^{ \prime}), \tag{4.3}\]

where \((i)\) follows from \(n_{t}(s^{\prime},k)=n_{t}(s^{\prime},1)+n_{t+1}(s^{\prime},k-1)\). A more detailed derivation is provided in Appendix A. Thus, we can define a tractable Bellman operator:

**Definition 4.2** (\(\lambda\mathrm{R}\) Operator).: _Let \(\Phi\in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}|}\) be an arbitrary real-valued matrix, and let \(\mathcal{G}_{\lambda}^{\pi}\) denote the \(\lambda\mathrm{R}\) Bellman operator for \(\pi\), such that_

\[\mathcal{G}_{\lambda}^{\pi}\Phi\triangleq I\odot\big{(}\mathbf{1}\mathbf{1}^ {\mathsf{T}}+\gamma\lambda P^{\pi}\Phi\big{)}+\gamma(\mathbf{1}\mathbf{1}^{ \mathsf{T}}-I)\odot P^{\pi}\Phi, \tag{4.4}\]

_where \(\odot\) denotes elementwise multiplication and \(I\) is the \(|\mathcal{S}|\times|\mathcal{S}|\) identity matrix. In particular, for a stationary policy \(\pi\), \(\mathcal{G}_{\lambda}^{\pi}\Phi_{\lambda}^{\pi}=\Phi_{\lambda}^{\pi}\)._

The following result establishes that successive applications of \(\mathcal{G}_{\lambda}^{\pi}\) converge to the \(\lambda\mathrm{R}\).

**Proposition 4.1** (Convergence).: _Under the conditions assumed above, set \(\Phi^{(0)}=(1-\lambda)I\). For \(k=1,2,\dots\), suppose that \(\Phi^{(k+1)}=\mathcal{G}_{\lambda}^{\pi}\Phi^{(k)}\). Then_

\[|(\Phi^{(k)}-\Phi_{\lambda}^{\pi})s_{s^{\prime}}|\leq\frac{\gamma^{k+1}}{1- \lambda\gamma}.\]

While such analysis is fairly standard in MDP theory, it is noteworthy that the analysis extends to this case despite Lemma 3.1. That is, for a ethologically relevant class of _non-Markovian_ reward functions [17], it is possible to define a Markovian Bellman operator and prove that repeated applications of it converge to the desired representation. Furthermore, unlike in the stationary reward case, where decomposing the value function in terms of the reward and the SR is "optional" to perform prediction or control, in this setting the structure of the problem _requires_ that this representation be learned. To get an intuition for the \(\lambda\mathrm{R}\) consider the simple gridworld presented in Fig. 4.1, where we visualize the \(\lambda\mathrm{R}\) for varying values of \(\lambda\). For \(\lambda=0\), the representation recovers the FR, encoding the expected discount at first occupancy of each state, while as \(\lambda\) increases, effective occupancy is accumulated accordingly at states which are revisited. We offer a more detailed visualization in Fig. F.2.

### Continuous State Spaces

When the state space is large or continuous, it becomes impossible to use a tabular representation, and we must turn to function approximation. There are several ways to approach this, each with their own advantages and drawbacks.

Feature-Based RepresentationsFor the SR and the FR, compatibility with function approximation is most commonly achieved by simply replacing the indicator functions in their definitions with a _base feature function_\(\phi:\mathcal{S}\mapsto\mathbb{R}^{D}\) to create _successor features_[SFs; 15; 16] and _first-occupancy features_[FFs; 11], respectively. The intuition in this case is that \(\phi\) for the SR and the FR is just a one-hot encoding of the state (or state-action pair), and so for cases when \(|\mathcal{S}|\) is too large, we can replace it with a compressed representation. That is, we have the following definition

**Definition 4.3** (SFs).: _Let \(\phi:\mathcal{S}\mapsto\mathbb{R}^{D}\) be a base feature function. Then, the successor feature (SF) representation \(\varphi_{1}:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}^{D}\) is defined as \(\varphi_{1}^{\pi}(s,a)\triangleq\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k}\phi(s_{t+k})\Big{|}s_{t},a_{t}\right]\) for all \(s,a\in\mathcal{S}\times\mathcal{A}\)._

One key fact to note, however, is that due to this compression, all notion of state "occupancy" is lost and these representations instead measure feature accumulation. For any feasible \(\lambda\) then, it is most natural to define these representations using their recursive forms:

**Definition 4.4** (\(\lambda\)F).: _For \(\lambda\in[0,1]\) and bounded base features \(\phi:\mathcal{S}\mapsto[0,1]^{D}\), the \(\lambda\)-feature (\(\lambda\)F) representation of state \(s\) is given by \(\varphi_{\lambda}^{\pi}\) such that_

\[\varphi_{\lambda}^{\pi}(s)\triangleq\phi(s)\odot(1+\gamma\lambda\mathbb{E}_{s^ {\prime}\sim p^{\pi}(\cdot\mid s)}\varphi_{\lambda}^{\pi}(s^{\prime}))+\gamma( 1-\phi(s))\odot\mathbb{E}_{s^{\prime}\sim p^{\pi}(\cdot\mid s)}\varphi_{\lambda }^{\pi}(s^{\prime}). \tag{4.5}\]

In order to maintain their usefulness for policy evaluation, the main requirement of the base features is that the reward should lie in their span. That is, a given feature function \(\phi\) is most useful for an associated set of reward functions \(\mathcal{R}\), given by

\[\mathcal{R}=\{r\mid\exists\mathbf{w}\in\mathbb{R}^{D}\text{ s.t. }r(s,a)=\mathbf{w}^{\mathsf{T}}\phi(s,a)\;\forall s,a\in\mathcal{S}\times \mathcal{A}\}. \tag{4.6}\]

However, Barreto et al. [18] demonstrate that good performance can still be achieved for an arbitrary reward function as long as it's sufficiently close to some \(r\in\mathcal{R}\).

Set-Theoretic FormulationsAs noted above, computing expectations of accumulated abstract features is unsatisfying because it requires that we lose the occupancy-based interpretation of these representations. It also restricts the agent to reward functions which lie within \(\mathcal{R}\). An alternative approach to extending the SR to continuous MDPs that avoids this issue is the _successor measure_[SM; 19], which treats the distribution of future states as a measure over \(\mathcal{S}\):

\[M^{\pi}(s,a,X)\triangleq\sum_{k=0}^{\infty}\gamma^{k}\mathbb{P}(s_{t+k}\in X \mid s_{t}=s,a_{t}=a,\pi)\quad\forall X\subset\mathcal{S}\text{ measurable}, \tag{4.7}\]

which can be expressed in the discrete case as \(M^{\pi}=I+\gamma P^{\pi}M^{\pi}\). In the continuous case, matrices are replaced by their corresponding measures. Note that SFs can be recovered by integrating: \(\varphi_{1}^{\pi}(s,a)=\int_{s^{\prime}}M^{\pi}(s,a,ds^{\prime})\phi(s^{ \prime})\). We can define an analogous object for the \(\lambda\mathrm{R}\) as follows

\[\Phi_{\lambda}^{\pi}(s,X)\triangleq\sum_{k=0}^{\infty}\lambda^{n_{t}(X,k)} \gamma^{k}\mathbb{P}(s_{t+k}\in X\mid s_{t}=s,\pi) \tag{4.8}\]

Figure 4.1: **The \(\lambda\mathrm{R}\) interpolates between the FR and the SR.** We visualize the \(\lambda\mathrm{R}\)s of the bottom left state for the depicted policy for \(\lambda\in\{0.0,0.5,1.0\}\).

where \(n_{t}(X,k)\triangleq\sum_{j=0}^{k-1}\delta_{s_{t+j}}(X)\). However, this is not a measure because it fails to satisfy additivity for \(\lambda<1\), i.e., for measurable disjoint sets \(A,B\subseteq\mathcal{S}\), \(\Phi_{\lambda}^{\pi}(s,A\cup B)<\Phi_{\lambda}^{\pi}(s,A)+\Phi_{\lambda}^{\pi} (s,B)\) (Lemma B.4). For this reason, we call Eq. (4.8) the \(\lambda\)_operator_ (\(\lambda\)O). We then minimize the following squared Bellman error loss for \(\Phi_{\lambda}^{\pi}\) (dropping sub/superscripts for concision):

\[\mathcal{L}(\Phi)=\mathbb{E}_{s_{t},s_{t+1}\sim\rho,s^{\prime}\sim \mu}\left[\left(\varphi(s_{t},s^{\prime})-\gamma\bar{\varphi}(s_{t+1},s^{ \prime})\right)^{2}\right]-2\mathbb{E}_{s_{t}\sim\rho}[\varphi(s_{t},s_{t})] \tag{4.9}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+2\gamma(1- \lambda)\mathbb{E}_{s_{t},s_{t+1}\sim\rho}[\mu(s_{t})\varphi(s_{t},s_{t})\bar {\varphi}(s_{t+1},s_{t})],\]

where \(\rho\) and \(\mu\) are densities over \(\mathcal{S}\) and \(\Phi_{\lambda}^{\pi}(s)\triangleq\varphi_{\lambda}^{\pi}(s)\mathrm{diag}(\mu)\) in the discrete case, with \(\varphi_{\lambda}^{\pi}\) parametrized by neural network. \(\bar{\cdot}\) indicates a stop-gradient operation, i.e., a target network. A detailed derivation and discussion are given in Appendix H. While \(\rho\) can be any training distribution of transitions we can sample from, we require an analytic expression for \(\mu\). Eq. (4.9) recovers the SM loss of Touati and Ollivier [12] when \(\lambda=1\).

## 5 Policy Evaluation, Learning, and Composition under DMU

In the following sections, we experimentally validate the formal properties of the \(\lambda\mathrm{R}\) and explore its usefulness for solving RL problems with DMU. The majority of our experiments center on navigation tasks, as we believe this is the most natural setting for studying behavior under diminishing rewards. However, in Appendix I we also explore potential for the \(\lambda\mathrm{R}\)'s use other areas, such as continuous control, even when rewards do not diminish. There is also the inherent question of whether the agent has access to \(\lambda\). In a naturalistic context, \(\lambda\) can be seen as an internal variable that the agent likely knows, especially if the agent has experienced the related stimulus before. Therefore, in subsequent experiments, treating \(\lambda\) as a "given" can be taken to imply the agent has prior experience with the relevant stimulus. Further details for all experiments can be found in Appendix E.

### Policy Evaluation

In Section 4, we showed that in order to perform policy evaluation under DMU, an agent is required to learn the \(\lambda\mathrm{R}\). In our first experimental setting, we verify this analysis empirically for the policy depicted in Fig. 4.1 with a rewarded location in the state indicated by a \(g\) in Fig. 5.1 with \(\lambda_{true}=0.5\). We then compare the performance for agents using different values of \(\lambda\) across dynamic programming (DP), tabular TD learning, and \(\lambda\mathrm{F}\) TD learning with Laplacian features. For the latter two, we use a linear function approximator with a one-hot encoding of the state as the base feature function. We then compute the \(Q\)-values using the \(\lambda\mathrm{R}\) with \(\lambda\in\{0.5,1.0\}\) (with \(\lambda=1\) corresponding to the SR) and compare the resulting value estimates to the true \(Q\)-values. Consistent with our theoretical analysis, Fig. 5.1 shows that the \(\lambda\mathrm{R}\) with \(\lambda=\lambda_{true}\) is required to produce accurate value estimates.

### Policy Learning

To demonstrate that \(\lambda\mathrm{R}\) is useful in supporting policy improvement under diminishing rewards, we implemented modified forms of \(Q\)-learning [20] (which we term \(Q_{\lambda}\)-learning) and advantage

Figure 5.1: **The \(\lambda\mathrm{R}\) is required for accurate policy evaluation.** Policy evaluation of the policy depicted in Fig. 4.1 using dynamic programming, tabular TD learning, and \(\lambda\mathrm{F}\) TD learning produces the most accurate value estimates when using the \(\lambda\mathrm{R}\) with \(\lambda=\lambda_{true}\). Results are averaged over three random seeds. Shading indicates standard error.

actor-critic [2, 21] and applied them to the TwoRooms domain from the NeuroNav benchmark task set [22] (see Fig. 5.2). For a transition \((s_{t},a_{t},s_{t+1})\), we define the following operator:

\[\mathcal{T}_{\lambda}^{\star}\Phi\triangleq\mathbf{1}(s_{t})\odot(1+\lambda \gamma\Phi(s_{t+1},a_{t+1}))+\gamma(1-\mathbf{1}(s_{t}))\odot\Phi(s_{t+1},a_{ t+1}), \tag{5.1}\]

where \(a_{t+1}=\operatorname*{argmax}_{a}Q_{\lambda}(s_{t},a)=\operatorname*{argmax}_ {a}\mathbf{r}^{\mathsf{T}}\Phi(s_{t},a)\). This is an improvement operator for \(Q_{\lambda}\). The results in Fig. 5.2 show that \(Q_{\lambda}\)-learning outperforms standard \(Q\)-learning (\(\lambda=1\)) for diminishing rewards, and that the "correct" \(\lambda\) produces the best performance. To implement A2C with a \(\lambda\mathrm{R}\) critic, we modified the standard TD target in a similar manner as follows:

\[\mathcal{T}_{\lambda}V(s_{t})=r(s_{t})+\gamma(V(s_{t+1})+(\lambda-1)\mathbf{w }^{\mathsf{T}}(\phi(s_{t})\odot\varphi_{\lambda}(s_{t+1}))), \tag{5.2}\]

where \(\phi\) were one-hot state encodings, and the policy, value function, and \(\lambda\mathrm{F}\) were output heads of a shared LSTM network [23]. Note this target is equivalent to Definition 4.4 multiplied by the reward (derivation in Appendix E). Fig. 5.2 shows again that correct value targets lead to improved performance. Videos of agent behavior can be found at lambdarepresentation.github.io.

### Policy Composition

As we've seen, DMU problems of this form have an interesting property wherein solving one task requires the computation of a representation which on its own is task agnostic. In the same way that the SR and FR facilitate generalization across reward functions, the \(\lambda\mathrm{R}\) facilitates generalization across reward functions with different \(\bar{r}\)s.The following result shows that there is a benefit to having the "correct" \(\lambda\) for a given resource.

**Theorem 5.1** (Gpi).: _Let \(\{M_{j}\}_{j=1}^{n}\subseteq\mathcal{M}\) and \(M\in\mathcal{M}\) be a set of tasks in an environment \(\mathcal{M}\) and let \(Q^{\pi_{j}^{*}}\) denote the action-value function of an optimal policy of \(M_{j}\) when executed in \(M\). Assume that the agent uses diminishing rate \(\hat{\lambda}\) that may differ from the true environment diminishing rate \(\lambda\). Given estimates \(\tilde{Q}^{\pi_{j}}\) such that \(\|Q^{\pi_{j}^{*}}-\tilde{Q}^{\pi_{j}}\|_{\infty}\leq\epsilon\) for all \(j\), define_

\[\pi(s)\in\operatorname*{argmax}_{a}\max_{j}\tilde{Q}^{\pi_{j}}(s,a).\]

_Then,_

\[Q^{\pi}(s,a)\geq\max_{j}Q^{\pi_{j}^{*}}(s,a)-\frac{1}{1-\gamma} \left(2\epsilon+|\lambda-\hat{\lambda}|\|r\|_{\infty}+\frac{\gamma(1-\lambda)r (s,a)}{1-\lambda\gamma}\right).\]

Note that for \(\lambda=1\), we recover the original GPI bound due to Barreto et al. [18] with an additional term quantifying error accrued if incorrectly assuming \(\lambda<1\).

Tabular NavigationWe can see this result reflected empirically in Fig. 5.3, where we consider the following experimental set-up in the classic FourRooms domain [24]. The agent is assumed to be given or have previously acquired four policies \(\{\pi_{0},\pi_{1},\pi_{2},\pi_{3}\}\) individually optimized to reach rewards located in each of the four rooms of the environment. There are three reward locations \(\{g_{0},g_{1},g_{2}\}\) scattered across the rooms, each with its own initial reward and all with \(\lambda=0.5\). At the

Figure 5.2: **The \(\lambda\mathrm{R}\) is required for strong performance.** We apply a tabular \(Q\)-learning-style algorithm and deep actor-critic algorithm to policy optimization in the TwoRooms domain. The blue locations indicate reward, and the black triangle shows the agentâ€™s position. Results are averaged over three random seeds. Shading indicates standard error.

beginning of each episode, an initial state \(s_{0}\) is sampled uniformly from the set of available states. An episode terminates either when the maximum reward remaining in any of the goal states is less than \(0.1\) or when the maximum number of steps \(H=40\) is reached (when \(\lambda=1\), the latter is the only applicable condition). For each of the four policies, we learn \(\lambda\)Rs with \(\lambda\in\{0.0,0.5,1.0\}\) using dynamic programming and record the returns obtained while performing GPE+GPI with each of these representations over 50 episodes. Bellman error curves for the \(\lambda\)Rs are shown in Fig. F.3, and demonstrate that convergence is faster for lower \(\lambda\). In the left panel of Fig. 5.3, we can indeed see that using the correct \(\lambda\) (0.5) nets the highest returns. Example trajectories for each agent \(\lambda\) are shown in the remaining panels.

Pixel-Based NavigationWe verified that the previous result is reflected in larger scales by repeating the experiment in a partially-observed version of FourRooms in which the agent receives \(128\times 128\) RGB egocentric observations of the environment (Fig. 5.4, left) with \(H=50\). In this case, the agent learns \(\lambda\)Fs for each policy for \(\lambda\in\{0.0,0.5,1.0\}\), where each \(\lambda\)F is parameterized by a feedforward convolutional network with the last seven previous frames stacked to account for the partial observability. The base features were Laplacian eigenfunctions normalized to \([0,1]\), which which were shown by Touati et al. [25] to perform the best of all base features for SFs across a range of environments including navigation.

## 6 Understanding Natural Behavior

Naturalistic environments often exhibit diminishing reward and give insight into animal behavior. The problem of foraging in an environment with multiple diminishing food patches (i.e., reward states) is of interest in behavioral science [26, 27, 28]. The cornerstone of foraging theory is the marginal value theorem [MVT; 28, 29], which states that the optimal time to leave a patch is when the patch's reward rate matches the average reward rate of the environment. However, the MVT does not describe which patch to move to once an agent leaves its current patch. We show that the \(\lambda\)O recovers MVT-like behavior in discrete environments and improves upon the MVT by not only predicting _when_ agents should leave rewarding patches, but also _where_ they should go. Moreover, we provide a scheme for _learning_\(\lambda\) alongside the \(\lambda\)O using feedback from the environment.

Figure 5.3: **Tabular GPI.** (Left) Average returns obtained by agents performing GPE+GPI using \(\lambda\)Rs with \(\lambda\in\{0.0,0.5,1.0\}\) over 50 episodes. Error bars indicate standard error. (Right) Sample trajectories. Agents with \(\lambda\) set too high overstay in rewarding states, and those with \(\lambda\) too low leave too early.

Figure 5.4: **Pixel-Based GPI.** Performance is strongest for agents using the correct \(\lambda=0.5\). PCA on the learned features in each underlying environment state shows that the \(\lambda\)Fs capture the value-conditioned structure of the environment.

To learn the \(\lambda\)O, we take inspiration from the FBR [12] and use the following parametrization: \(\Phi_{\lambda}^{\pi_{*}}(s,a,s^{\prime})=F(s,a,z)^{\top}B(s^{\prime})\mu(s^{ \prime})\) and \(\pi_{z}(s)=\operatorname*{argmax}_{a}F(s,a,z)^{\top}z\), where \(\mu\) is a density with full support on \(\mathcal{S}\), e.g., uniform. We then optimize using the the loss in Eq. (4.9) under this parameterization (details in Appendix E). Given a reward function \(r:\mathcal{S}\rightarrow\mathbb{R}\) at evaluation time, the agent acts according to \(\operatorname*{argmax}_{a}F(s,a,z_{R})^{\top}z_{R}\), where \(z_{R}=\mathbb{E}_{s\sim\mu}[r(s)B(s)]\). Because the environment is non-stationary, \(z_{R}\) has to be re-computed at every time step. To emulate a more realistic foraging task, the agent learns \(\lambda\) by minimizing the loss in Eq. (H.1) in parallel with the loss

\[\mathcal{L}(\lambda)=\mathbb{E}_{s_{t},s_{t+1}\sim\rho}\left[\mathbb{1}(s_{t} =s_{t+1})\left(r(s_{t+1})-\lambda r(s_{t})\right)^{2}\right],\]

which provably recovers the correct value of \(\lambda\) provided that \(\rho\) is sufficiently exploratory. In Appendix H.1 we provide experiments showing that using an incorrect value of \(\lambda\) leads to poor performance on tabular tasks. In Fig. 6.1 we show that the agent learns the correct value of \(\lambda\), increasing its performance. We illustrate the behavior of the \(\lambda\)O in an asymmetric environment that has one large reward state on one side and many small reward states (with higher total reward) on the other. Different values of \(\lambda\) lead to very different optimal foraging strategies, which the \(\lambda\)O recovers and exhibits MVT-like behavior (see Appendix H.2 for a more detailed analysis). Our hope is that the \(\lambda\)R may provide a framework for new theoretical studies of foraging behavior and possibly mechanisms for posing new hypotheses. For example, an overly large \(\lambda\) may lead to overstaying in depleted patches, a frequently observed phenomenon [30].

## 7 Conclusion

LimitationsDespite its advantages, there are several drawbacks to the representation which are a direct result of the challenge of the DMU setting. First, the \(\lambda\mathrm{R}\) is only useful for transfer across diminishing reward functions when the value of \(\lambda\) at each state is consistent across tasks. In natural settings, this is fairly reasonable, as \(\lambda\) can be thought of as encoding the type of the resource available at each state (i.e., each resource has its own associated decay rate). Second, as noted in Section 4.1, the \(\lambda\mathrm{R}\) does not admit a measure-theoretic formulation, which makes it challenging to define a principled, occupancy-based version compatible with continuous state spaces. Third, the \(\lambda\mathrm{R}\) is a prospective representation, and so while it is used to correctly evaluate a policy's future return under DMU, it is not inherently memory-based and so performs this evaluation as if the agent hasn't visited locations with diminishing reward before. Additional mechanisms (i.e., recurrence or frame-stacking) are necessary to account for previous visits. Finally, the \(\lambda\mathrm{R}\) is dependent on an episodic task setting for rewards to reset, as otherwise the agent would eventually consume all reward in the environment. An even more natural reward structure would include a mechanism for reward _replenishment_ in addition to depletion. We describe several such candidates in Appendix J, but leave a more thorough investigation of this issue to future work.

In this work, we aimed to lay the groundwork for understanding policy evaluation, learning, and composition under diminishing rewards. To solve such problems, we introduced--and showed the necessity of--the \(\lambda\)_representation_, which generalizes the SR and FR. We demonstrated its usefulness for rapid policy evaluation and by extension, composition, as well as control. We believe the \(\lambda\mathrm{R}\) represents a useful step in the development of state representations for naturalistic environments.

Figure 6.1: \(\lambda\)**O trained via FB.****a)**\(\lambda\) values of two agents in FourRooms, one which learns \(\lambda\) and one which does not. **b)** Performance of the two agents from (**a**). Learning \(\lambda\) improves performance. **c)** Reward structure and starting state of the asymmetric environment. **d)** Trajectory of an agent with \(\lambda=1\). The optimal strategy is to reach the high reward state and exploit it _ad infinitum_. **e)** Trajectory of an agent with \(\lambda=0.1\). The optimal strategy is to exploit each reward state for a few time steps before moving to the next reward state.

## References

* Kahneman and Tversky [1979] Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. _Econometrica_, 47(2):263-292, 1979.
* Rabin [2000] Matthew Rabin. Risk aversion and expected-utility theory: a calibration theorem. _Econometrica_, 68(5):1281-1292, 2000. doi: 10.2307/2999450.
* Pine et al. [2009] Alex Pine, Ben Seymour, Jonathan P Roiser, Peter Bossaerts, Karl J Friston, H Valerie Curran, and Raymond J Dolan. Encoding of marginal utility across time in the human brain. _Journal of Neuroscience_, 29(30):9575-9581, 2009.
* Puterman [2010] Martin L. Puterman. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley and Sons, 2010.
* Gossen and Blitz [1983] Hermann Heinrich Gossen and Rudolph C Blitz. _The laws of human relations and the rules of human action derived therefrom_. Mit Press Cambridge, MA, 1983.
* Arrow [1996] Kenneth J Arrow. The theory of risk-bearing: small and great risks. _Journal of risk and uncertainty_, 12:103-111, 1996.
* Pratt [1978] John W Pratt. Risk aversion in the small and in the large. In _Uncertainty in economics_, pages 59-79. Elsevier, 1978.
* Wispinski et al. [2023] Nathan Wispinski, Andrew Butcher, Kory Wallace Mathewson, Craig S Chapman, Matthew Botvinick, and Patrick M. Pilarski. Adaptive patch foraging in deep reinforcement learning agents. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL [https://openreview.net/forum?id=aOT3nOP9sB](https://openreview.net/forum?id=aOT3nOP9sB).
* Shuvaev et al. [2020] Sergey Shuvaev, Sarah Starosta, Duda Kvitsiani, Adam Kepecs, and Alexei Koulakov. R-learning in actor-critic model offers a biologically relevant mechanism for sequential decision-making. _Advances in neural information processing systems_, 33:18872-18882, 2020.
* Dayan [1993] Peter Dayan. Improving generalization for temporal difference learning: The successor representation. _Neural Computation_, 5(4):613-624, 1993. doi: 10.1162/neco.1993.5.4.613.
* Moskovitz et al. [2022] Ted Moskovitz, Spencer R Wilson, and Maneesh Sahani. A first-occupancy representation for reinforcement learning. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=JBAZe2yN6Ub](https://openreview.net/forum?id=JBAZe2yN6Ub).
* Touati and Ollivier [2021] Ahmed Touati and Yann Ollivier. Learning one representation to optimize all rewards. _Advances in Neural Information Processing Systems_, 34:13-23, 2021.
* Sutton and Barto [2018] Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. The MIT Press, second edition, 2018. URL [http://incompleteideas.net/book/the-book-2nd.html](http://incompleteideas.net/book/the-book-2nd.html).
* Stachenfeld et al. [2017] Kimberly L Stachenfeld, Matthew M Botvinick, and Samuel J Gershman. The hippocampus as a predictive map. _Nature neuroscience_, 20(11):1643-1653, 2017.
* Barreto et al. [2020] Andre Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup. Fast reinforcement learning with generalized policy updates. _Proceedings of the National Academy of Sciences_, 117(48):30079-30087, 2020. ISSN 0027-8424. doi: 10.1073/pnas.1907370117. URL [https://www.pnas.org/content/117/48/30079](https://www.pnas.org/content/117/48/30079).
* Barreto et al. [2017] Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. _Advances in neural information processing systems_, 30, 2017.
* Gaon and Brafman [2020] Maor Gaon and Ronen Brafman. Reinforcement learning with non-markovian rewards. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 3980-3987, 2020.

* Barreto et al. [2019] Andre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin Zidek, and Remi Munos. Transfer in deep reinforcement learning using successor features and generalised policy improvement, 2019.
* Blier and Ollivier [2018] Leonard Blier and Yann Ollivier. The description length of deep learning models. _Advances in Neural Information Processing Systems_, 31, 2018.
* Watkins and Dayan [1992] Christopher JCH Watkins and Peter Dayan. Q-learning. _Machine learning_, 8:279-292, 1992.
* Mnih et al. [2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In _International conference on machine learning_, pages 1928-1937. PMLR, 2016.
* Juliani et al. [2022] Arthur Juliani, Samuel Barnett, Brandon Davis, Margaret Sereno, and Ida Momennejad. Neuronav: a library for neurally-plausible reinforcement learning. _arXiv preprint arXiv:2206.03312_, 2022.
* Hochreiter and Schmidhuber [1997] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural Computation_, 9(8):1735-1780, 1997.
* Sutton et al. [1999] Richard S. Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. _Artificial Intelligence_, 112(1):181-211, 1999. URL [https://www.sciencedirect.com/science/article/pii/S0004370299000521](https://www.sciencedirect.com/science/article/pii/S0004370299000521).
* Touati et al. [2023] Ahmed Touati, Jeremy Rapin, and Yann Ollivier. Does zero-shot reinforcement learning exist? In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=MYEap_OcQI](https://openreview.net/forum?id=MYEap_OcQI).
* Hayden et al. [2011] Benjamin Y Hayden, John M Pearson, and Michael L Platt. Neuronal basis of sequential foraging decisions in a patchy environment. _Nature neuroscience_, 14(7):933-939, 2011.
* Yoon et al. [2018] Tehrim Yoon, Robert B Geary, Alaa A Ahmed, and Reza Shadmehr. Control of movement vigor and decision making during foraging. _Proceedings of the National Academy of Sciences_, 115(44):E10476-E10485, 2018.
* Gabay and Apps [2021] Anthony S Gabay and Matthew AJ Apps. Foraging optimally in social neuroscience: computations and methodological considerations. _Social Cognitive and Affective Neuroscience_, 16(8):782-794, 2021.
* Charnov [1976] Eric L. Charnov. Optimal foraging, the marginal value theorem. _Theoretical Population Biology_, 9(2):129-136, 1976. ISSN 0040-5809.
* Nonacs [2001] Peter Nonacs. State dependent behavior and the marginal value theorem. _Behavioral Ecology_, 12(1):71-83, 2001.
* Wang et al. [2020] Ruosong Wang, Hanrui Zhang, Devendra Singh Chaplot, Denis Garagic, and Ruslan Salakhutdinov. Planning with submodular objective functions. _arXiv preprint arXiv:2010.11863_, 2020.
* Prajapat et al. [2023] Manish Prajapat, Mojmir Mutny, Melanie N Zeilinger, and Andreas Krause. Submodular reinforcement learning. _arXiv preprint arXiv:2307.13372_, 2023.
* Zahavy et al. [2021] Tom Zahavy, Brendan O'Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough for convex mdps, 2021.
* Altman [2021] Eitan Altman. _Constrained Markov decision processes_. Routledge, 2021.
* Moskovitz et al. [2023] Ted Moskovitz, Brendan O'Donoghue, Vivek Veeriah, Sebastian Flennerhag, Satinder Singh, and Tom Zahavy. Reload: Reinforcement learning with optimistic ascent-descent for last-iterate convergence in constrained mdps. In _International Conference on Machine Learning_, pages 25303-25336. PMLR, 2023.

* [36] Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, et al. Behaviour suite for reinforcement learning. _arXiv preprint arXiv:1908.03568_, 2019.
* [37] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* [38] Martin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural reinforcement learning method. In _Machine Learning: ECML 2005: 16th European Conference on Machine Learning, Porto, Portugal, October 3-7, 2005. Proceedings 16_, pages 317-328. Springer, 2005.
* [39] Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. _Advances in neural information processing systems_, 5, 1992.
* [40] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, 2018_.
* [41] Ted Moskovitz, Michael Arbel, Ferenc Huszar, and Arthur Gretton. Efficient wasserstein natural gradients for reinforcement learning, 2020.
* [42] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor, 2018.
* [43] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
* [44] Ted Moskovitz, Jack Parker-Holder, Aldo Pacchiano, Michael Arbel, and Michael Jordan. Tactical optimism and pessimism for deep reinforcement learning. _Advances in Neural Information Processing Systems_, 34:12849-12863, 2021.
* [45] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, learning, and games_. Cambridge university press, 2006.

Appendix: A State Representation for Diminishing Rewards GIFs of navigation agents can be found at lambdarepresentation.github.io and in the supplementary material.

## Appendix A Derivation of \(\lambda\mathrm{R}\) Recursion

We provide a step-by-step derivation of the \(\lambda\mathrm{R}\) recursion in Eq. (4.3):

\[\Phi_{\lambda}^{\pi}(s,s^{\prime}) =\mathbb{E}\left[\sum_{k=0}^{\infty}\lambda^{n_{t}(s^{\prime},k) }\gamma^{k}\mathbb{1}\left(s_{t+k}=s^{\prime}\right)\Big{|}s_{t}=s\right]\] \[=\mathbb{E}\left[\mathbb{1}\left(s_{t}=s^{\prime}\right)+\sum_{k= 1}^{\infty}\lambda^{n_{t}(s^{\prime},k)}\gamma^{k}\mathbb{1}\left(s_{t+k}=s^{ \prime}\right)\Big{|}s_{t}=s\right]\] \[\overset{(i)}{=}\mathbb{E}\left[\mathbb{1}\left(s_{t}=s^{\prime} \right)+\lambda^{n_{t}(s^{\prime},1)}\gamma\sum_{k=1}^{\infty}\lambda^{n_{t+1 }(s^{\prime},k)}\gamma^{k-1}\mathbb{1}\left(s_{t+k}=s^{\prime}\right)\Big{|}s _{t}=s\right]\] \[\overset{(ii)}{=}\mathbb{E}\Bigg{[}\mathbb{1}(s_{t}=s^{\prime})+ \mathbb{1}\left(s_{t}=s^{\prime}\right)\lambda\gamma\sum_{k=1}^{\infty} \lambda^{n_{t+1}(s^{\prime},k)}\gamma^{k-1}\mathbb{1}\left(s_{t+k}=s^{\prime} \right)\] \[\qquad+\gamma(1-\mathbb{1}\left(s_{t}=s^{\prime}\right))\sum_{k=1 }^{\infty}\lambda^{n_{t+1}(s^{\prime},k)}\gamma^{k-1}\mathbb{1}\left(s_{t+k}=s ^{\prime}\right)\Big{|}s_{t}=s\Bigg{]}\] \[=\mathbb{1}\left(s_{t}=s^{\prime}\right)+\mathbb{1}\left(s_{t}=s^{ \prime}\right)\lambda\gamma\mathbb{E}_{s_{t+1}\sim p^{\pi}}\Phi_{\lambda}^{\pi} (s_{t+1},s^{\prime})\] \[\qquad\qquad+\gamma(1-\mathbb{1}\left(s_{t}=s^{\prime}\right)) \mathbb{E}_{s_{t+1}\sim p^{\pi}}\Phi_{\lambda}^{\pi}(s_{t+1},s^{\prime}),\] (A.1)

where \((i)\) is because \(n_{t}(s^{\prime},k)=n_{t}(s^{\prime},1)+n_{t+1}(s^{\prime},k)\) and \((ii)\) is because

\[\lambda^{n_{t}(s^{\prime},1)}=\lambda^{1\left(s_{t}=s^{\prime}\right)}= \mathbb{1}\left(s_{t}=s^{\prime}\right)\lambda+(1-\mathbb{1}(s_{t}=s^{\prime} )).\]

## Appendix B Theoretical Analysis

Here, we provide proofs for the theoretical results in the main text.

**Lemma 3.1** (Impossibility; Informal).: _Given a reward function of the form Eq. (3.1), it is impossible to define a Bellman equation solely using the resulting value function and immediate reward._

Before providing the formal statement and proof for Lemma 3.1, we introduce a definition for a Bellman operator.

**Definition B.1** (Bellman Operator).: _A Bellman operator is a contractive operator \(\mathbb{R}^{|\mathcal{S}|}\rightarrow\mathbb{R}^{|\mathcal{S}|}\) that depends solely on \(\overline{\mathbf{r}}\), one-step expectations under \(p^{\pi}\), and learning hyperparameters (in our case \(\gamma\) and \(\lambda\))._

We can now give a formal statement of Lemma 3.1:

[MISSING_PAGE_FAIL:14]

where \((i)\) comes from using \(\lambda\leq 1\) and simplifying. 

Note that we can actually get a tighter contraction factor of \(\lambda\gamma\) for \(s=s^{\prime}\). Given this contractive property, we can prove its convergence with the use of the following lemma.

**Lemma B.3** (Max \(\lambda\mathrm{R}\)).: _The maximum possible value of \(\Phi_{\lambda}^{\pi}(s,s^{\prime})\) is_

\[\frac{1(s=s^{\prime})+(1-\mathbb{1}(s=s^{\prime}))\gamma}{1-\lambda\gamma}.\]

Proof.: For \(s=s^{\prime}\),

\[\Phi_{\lambda}^{\pi}(s,s)=1+\lambda\gamma\mathbb{E}_{s_{t+1}\sim p^{\pi}(\cdot |s_{t})}\Phi_{\lambda}^{\pi}(s_{t+1},s).\]

This is just the standard SR recursion with discount factor \(\lambda\gamma\), so the maximum is

\[\sum_{k=0}^{\infty}(\lambda\gamma)^{k}=\frac{1}{1-\lambda\gamma}.\] (B.1)

For \(s\neq s^{\prime}\), \(\mathbb{1}(s_{t}=s^{\prime})=0\), so

\[\Phi_{\lambda}^{\pi}(s,s^{\prime})=\gamma\mathbb{E}_{s_{t+1}\sim p^{\pi}(\cdot |s_{t})}\Phi_{\lambda}^{\pi}(s_{t+1},s^{\prime}).\]

Observe that \(\Phi_{\lambda}^{\pi}(s,s)\geq\Phi_{\lambda}^{\pi}(s,s^{\prime})\) for \(s^{\prime}\neq s\), so the maximum is attained for \(s_{t+1}=s^{\prime}\). We can then use the result for \(s=s^{\prime}\) to get

\[\Phi_{\lambda}^{\pi}(s,s^{\prime})=\gamma\left(\frac{1}{1-\lambda\gamma}\right).\] (B.2)

Combining Eq. (B.1) and Eq. (B.2) yields the desired result. 

**Proposition 4.1** (Convergence).: _Under the conditions assumed above, set \(\Phi^{(0)}=(1-\lambda)I\). For \(k=1,2,\ldots\), suppose that \(\Phi^{(k+1)}=\mathcal{G}_{\lambda}^{\pi}\Phi^{(k)}\). Then_

\[|(\Phi^{(k)}-\Phi_{\lambda}^{\pi})_{s,s^{\prime}}|\leq\frac{\gamma^{k+1}}{1- \lambda\gamma}.\]

Proof.: Using the notation \(X_{s,s^{\prime}}=X(s,s^{\prime})\) for a matrix \(X\):

\[|(\Phi^{(k)}-\Phi_{\lambda}^{\pi})_{s,s^{\prime}}| =|(\mathcal{G}_{\lambda}^{k}\Phi^{(0)}-\mathcal{G}_{\lambda}^{k} \Phi_{\lambda}^{\pi})_{s,s^{\prime}}|\] \[=|(\mathcal{G}_{\lambda}^{k}\Phi^{(0)}-\Phi_{\lambda}^{\pi})_{s, s^{\prime}}|\] \[\stackrel{{(i)}}{{\leq}}\gamma^{k}|(\Phi^{(0)}-\Phi _{\lambda}^{\pi})_{s,s^{\prime}}|\] (B.3) \[\stackrel{{(ii)}}{{=}}\gamma^{k}\Phi_{\lambda}^{\pi} (s,s^{\prime})\] \[\stackrel{{(iii)}}{{\leq}}\frac{\gamma^{k+1}}{1- \lambda\gamma}\]

where \((i)\) is due to Lemma B.2, \((ii)\) is because \(\Phi^{(0)}(s,s^{\prime})=0\) for \(s\neq s^{\prime}\), and \((iii)\) is due to Lemma B.3. 

**Lemma B.4** (Subadditivity).: _For any \(s\in\mathcal{S}\), policy \(\pi\), \(\lambda\in[0,1)\), and disjoint measurable sets \(A,B\subseteq\mathcal{S}\),_

\[\Phi_{\lambda}^{\pi}(s,A\cup B)<\Phi_{\lambda}^{\pi}(s,A)+\Phi_{\lambda}^{\pi} (s,B).\]Proof.: Note that for disjoint sets \(A,B\), we have \(n_{t}(A\cup B,k)=n_{t}(A,k)+n_{t}(B,k)\). Hence, conditioned on some policy \(\pi\) and \(s_{t}=s\),

\[\lambda^{n_{t}(A\cup B,k)}\mathbb{P}(s_{t+k}\in A\cup B) =\lambda^{n_{t}(A,k)}\lambda^{n_{t}(B,k)}\mathbb{P}(s_{t+k}\in A) +\lambda^{n_{t}(A,k)}\lambda^{n_{t}(B,k)}\mathbb{P}(s_{t+k}\in B)\] \[\leq\lambda^{n_{t}(A,k)}\mathbb{P}(s_{t+k}\in A)+\lambda^{n_{t}(B,k)}\mathbb{P}(s_{t+k}\in B),\]

where the first line follows from \(\mathbb{P}(s_{t+k}\in A\cup B)=\mathbb{P}(s_{t+k}\in A)+\mathbb{P}(s_{t+k}\in B)\). Equality holds over all \(A,B,t,k\) if and only if \(\lambda=1\). Summing over \(k\) yields the result. 

### Proof of Theorem 5.1

We first prove two results, which rely throughout on the fact that \(\Phi_{\lambda}(s,a,s^{\prime})\leq\frac{1}{1-\lambda\gamma}\) for all \(s,a,s^{\prime}\), which follows from Lemma B.3. For simplicity, we also assume throughout that all rewards are non-negative, but this assumption can easily be dropped by taking absolute values of rewards. The proofs presented here borrow ideas from those of [16].

**Lemma B.5**.: _Let \(\{M_{j}\}_{j=1}^{n}\subseteq\mathcal{M}\) and \(M\in\mathcal{M}\) be a set of tasks in an environment \(\mathcal{M}\) with diminishing rate \(\lambda\) and let \(Q^{\pi_{j}^{*}}\) denote the action-value function of an optimal policy of \(M_{j}\) when executed in \(M\). Given estimates \(\tilde{Q}^{\pi_{j}}\) such that \(\|Q^{\pi_{j}^{*}}-\tilde{Q}^{\pi_{j}}\|_{\infty}\leq\epsilon\) for all \(j\), define_

\[\pi(s)\in\operatorname*{argmax}_{a}\max_{j}\tilde{Q}^{\pi_{j}}(s,a).\]

_Then,_

\[Q^{\pi}(s,a)\geq\max_{j}Q^{\pi_{j}^{*}}(s,a)-\frac{1}{1-\gamma} \left(2\epsilon+\frac{\gamma(1-\lambda)r(s,a)}{1-\lambda\gamma}\right),\]

_where \(r\) denotes the reward function of \(M\)._

Proof.: Define \(\tilde{Q}_{\text{max}}(s,a)\coloneqq\max_{j}\tilde{Q}^{\pi_{j}}(s,a)\) and \(Q_{\text{max}}(s,a)\coloneqq\max_{j}Q^{\pi_{j}^{*}}(s,a)\). Let \(T^{\nu}\) denote the Bellman operator of a policy \(\nu\) in task \(M\). For all \((s,a)\in\mathcal{S}\times\mathcal{A}\) and 

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_EMPTY:18]

Proof.: Let \(Q_{\lambda}\) denote a value function with respect to diminishing constant \(\lambda\). We wish to bound

\[\max_{j}Q_{\hat{\lambda}}^{\pi_{j}^{*}}(s,a)-Q_{\lambda}^{\pi}(s,a),\]

i.e., the value of the GPI policy with respect to the true \(\lambda\) compared to the maximum value of the constituent policies \(\pi_{j}^{*}\) used for GPI, which were used assuming \(\hat{\lambda}\). By the triangle inequality,

\[\max_{j}Q_{\hat{\lambda}}^{\pi_{j}^{*}}(s,a)-Q_{\lambda}^{\pi}(s,a) \leq\max_{j}Q_{\lambda}^{\pi_{j}^{*}}(s,a)-Q_{\lambda}^{\pi}(s,a) +|\max_{j}Q_{\lambda}^{\pi_{j}^{*}}(s,a)-\max_{j}Q_{\hat{\lambda}}^{\pi_{j}^{*} }(s,a)|\] \[\leq\underbrace{\max_{j}Q_{\lambda}^{\pi_{j}^{*}}(s,a)-Q_{\lambda} ^{\pi}(s,a)}_{(1)}+\max_{j}\underbrace{|Q_{\lambda}^{\pi_{j}^{*}}(s,a)-Q_{ \hat{\lambda}}^{\pi_{j}^{*}}(s,a)|}_{(2)}.\]

We bound (1) by Lemma B.5 and (2) by Lemma B.6 to get the result. 

### An Extension of Theorem 5.1

Inspired by [18], we prove an extension of Theorem 5.1:

**Theorem B.1**.: _Let \(M\in\mathcal{M}\) be a task in an environment \(\mathcal{M}\) with true diminishing constant \(\lambda\). Suppose we perform GPI assuming a diminishing constant \(\hat{\lambda}\):_

_Let_ \(\{M_{j}\}_{j=1}^{n}\) _and_ \(M_{i}\) _be tasks in_ \(\mathcal{M}\) _and let_ \(Q_{i}^{\pi_{j}^{*}}\) _denote the action-value function of an optimal policy of_ \(M_{j}\) _when executed in_ \(M_{i}\)_. Given estimates_ \(\tilde{Q}_{i}^{\pi_{j}}\) _such that_ \(\|Q_{i}^{\pi_{j}^{*}}-\tilde{Q}_{i}^{\pi_{j}}\|_{\infty}\leq\epsilon\) _for all_ \(j\)_, define_ \(\pi(s)\in\operatorname*{argmax}_{a}\max_{j}\tilde{Q}_{i}^{\pi_{j}}(s,a)\)_._

_Let \(Q_{\lambda}^{\pi}\) and \(Q_{\lambda}^{\pi^{*}}\) denote the action-value functions of \(\pi\) and the \(M\)-optimal policy \(\pi^{*}\) when executed in \(M\), respectively. Then,_

\[\|Q_{\lambda}^{\pi^{*}}-Q_{\hat{\lambda}}^{\pi}\|_{\infty}\leq\frac{2}{1- \gamma}\left(\frac{1}{2}|\lambda-\hat{\lambda}|\|r\|_{\infty}+\epsilon+\|r-r_{ i}\|_{\infty}+\min_{j}\|r_{i}-r_{j}\|_{\infty}\right)+\frac{1-\lambda}{1- \lambda\gamma}C,\]

_where \(C\) is a positive constant not depending on \(\lambda\):_

\[C=\gamma\frac{2\|r-r_{i}\|_{\infty}+2\min_{j}\|r_{i}-r_{j}\|_{\infty}+\min{(\| r\|_{\infty},\|r_{i}\|_{\infty})}+\min{(\|r_{i}\|_{\infty},\|r_{1}\|_{\infty}, \ldots,\|r_{n}\|_{\infty})}}{1-\gamma}.\]

Note that when \(\lambda=1\), we recover Proposition 1 of [18] with an additional term quantifying error incurred by \(\hat{\lambda}\neq\lambda\). The proof relies on two other technical lemmas, presented below.

**Lemma B.7**.: \[\|Q^{\pi^{*}}-Q_{i}^{\pi_{i}^{*}}\|_{\infty}\leq\frac{\|r-r_{i}\|_{\infty}}{1- \gamma}+\gamma(1-\lambda)\frac{\min{(\|r\|_{\infty},\|r_{i}\|_{\infty})}+\|r- r_{i}\|_{\infty}}{(1-\gamma)(1-\lambda\gamma)}.\]

[MISSING_PAGE_EMPTY:20]

[MISSING_PAGE_FAIL:21]

An \(n\)th Occupancy Representation

To generalize the first occupancy representation to account for reward functions of this type, it's natural to consider an \(N\)_th occupancy representation_--that is, one which accumulates value only for the first \(N\) occupancies of one state \(s^{\prime}\) starting from another state \(s\):

**Definition C.1** (NR).: _For an MDP with finite \(\mathcal{S}\), the \(N\)th-occupancy representation (NR) for a policy \(\pi\) is given by \(F^{\pi}\in[0,N]^{|\mathcal{S}|\times|\mathcal{S}|}\) such that_

\[\Phi_{(N)}^{\pi}(s,s^{\prime})\triangleq\mathbb{E}_{\pi}\left[\sum_{k=0}^{ \infty}\gamma^{t+k}\mathbbm{1}\left(s_{t+k}=s^{\prime},\#\left(\{j\mid s_{t+j} =s^{\prime},j\in[0,k-1]\}\right)<N\right)\Big{|}s_{t}\right].\] (C.1)

Intuitively, such a representation sums the first \(N\) (discounted) occupancies of \(s^{\prime}\) from time \(t\) to \(t+k\) starting from \(s_{t}=s\). We can also note that \(\Phi_{(1)}^{\pi}\) is simply the FR and \(\Phi_{(0)}(s,s^{\prime})=0\)\(\forall s,s^{\prime}\). As with the FR and the SR, we can derive a recursive relationship for the NR:

\[\Phi_{(N)}^{\pi}(s,s^{\prime})=\mathbbm{1}\left(s_{t}=s^{\prime}\right)(1+ \gamma\mathbb{E}\Phi_{(N-1)}^{\pi}(s_{t+1},s^{\prime}))+\gamma(1-\mathbbm{1} \left(s_{t}=s^{\prime}\right))\mathbb{E}\Phi_{(N)}^{\pi}(s_{t+1},s^{\prime}),\] (C.2)

where the expectation is wrt \(p^{\pi}(s_{t+1}|s_{t})\). Once again, we can confirm that this is consistent with the FR by noting that for \(N=1\), the NR recursion recovers the FR recursion. Crucially, we also recover the SR recursion in the limit as \(N\to\infty\):

\[\lim_{N\to\infty}\Phi_{(N)}^{\pi}(s,s^{\prime}) =\mathbbm{1}(s_{t}=s^{\prime})(1+\gamma\mathbb{E}\Phi_{(\infty)}^{ \pi}(s_{t+1},s^{\prime}))+\gamma(1-\mathbbm{1}\left(s_{t}=s^{\prime}\right)) \mathbb{E}\Phi_{(\infty)}^{\pi}(s_{t+1},s^{\prime})\] \[=\mathbbm{1}(s_{t}=s^{\prime})+\gamma\mathbb{E}\Phi_{(\infty)}^{ \pi}(s_{t+1},s^{\prime}).\]

This is consistent with the intuition that the SR accumulates every (discounted) state occupancy in a potentially infinite time horizon of experience. While Definition C.1 admits a recursive form which is consistent with our intuition, Eq. (C.2) reveals an inconvenient intractability: the Bellman target for \(\Phi_{(N)}^{\pi}\) requires the availability of \(\Phi_{(N-1)}^{\pi}\). This is a challenge, because it means that if we'd like to learn any NR for finite \(N>1\), the agent also must learn and store \(\Phi_{(1)}^{\pi},\ldots\Phi_{(N-1)}^{\pi}\). Given these challenges, the question of how to learn a tractable general occupancy representation remains. From a neuroscientific perspective, a fixed depletion amount is also inconsistent with both behavioral observations and neural imaging [3], which indicate instead that utility disappears at a fixed rate in proportion to the _current remaining utility_, rather than in proportion to the _original utility_. We address these theoretical and practical issues in the next section.

## Appendix D Additional Related Work

Another important and relevant sub-field of reinforcement learning is work which studies non-stationary rewards. Perhaps most relevant, the setting of DMU can be seen as a special case of submodular planning and reward structures [31; 32]. [31] focus specifically on planning and not the form of diminishment we study, while [32] is a concurrent work which focuses on the general class of submodularreward problems and introduces a policy-based, REINFORCE-like method which is necessarily on-policy. In contrast, we focus on a particular sub-class of problems especially relevant to natural behavior and introduce a family of approaches which exploit this reward structure, are value-based (and which can be used to modify the critic in policy-based methods), and are compatible with off-policy learning. Other important areas include convex [33] and constrained [34; 35] MDPs. In these cases, non-stationarity is introduced by way of a primal-dual formulation of distinct problem classes into min-max games.

## Appendix E Further Experimental Details

### Policy Evaluation

We perform policy evaluation for the policy shown in Fig. 4.1 on the \(6\times 6\) gridworld shown. The discount factor \(\gamma\) was set to 0.9 for all experiments, which were run for \(H=10\) steps per episode. The error metric was the mean squared error:

\[Q_{error}\triangleq\frac{1}{|\mathcal{S}||\mathcal{A}|}\sum_{s,a}(Q^{\pi}(s,a)- \hat{Q}(s,a))^{2},\] (E.1)

where \(Q^{\pi}\) is the ground truth \(Q\)-values and \(\hat{Q}\) is the estimate. Transitions are deterministic. For the dynamic programming result, we learned the \(\lambda\mathrm{R}\) using Eq. (4.3) for \(\lambda\in\{0.5,1.0\}\) and then measured the resulting values by multiplying the resulting \(\lambda\mathrm{R}\) by the associated reward vector \(\mathbf{r}\in\{-1,0,1\}^{36}\), which was \(-1\) in all wall states and \(+1\) at the reward state \(g\). We compared the results to the ground truth values. Dynamic programming was run until the maximum Bellman error across state-action pairs reduced below 5e-2. For the tabular TD learning result, we ran the policy for three episodes starting from every available (non-wall) state in the environment, and learned the \(\lambda\mathrm{R}\) for \(\lambda\in\{0.5,1.0\}\) as above, but using the online TD update:

\[\Phi_{\lambda}(s_{t},a_{t})\leftarrow\Phi_{\lambda}(s_{t},a_{t})+\alpha\delta _{t},\]

\[\delta_{t}=\mathbf{1}(s_{t})\odot(1+\gamma\lambda\Phi_{\lambda}(s_{t+1},a_{t+1 }))+\gamma(1-\mathbf{1}(s_{t}))\odot\Phi_{\lambda}(s_{t+1},a_{t+1})-\Phi_{ \lambda}(s_{t},a_{t}),\]

where \(a_{t+1}\sim\pi(\cdot\mid s_{t+1})\). The learned \(Q\)-values were then computed in the same way as the dynamic programming case and compared to the ground truth. For the \(\lambda\)F result, we first learned Laplacian eigenfunction base features as described in [25] from a uniform exploration policy and normalized them to the range \([0,1]\). We parameterized the base feature network as a 2-layer MLP with ReLU activations and 16 units in the hidden layer. We then used the base features to learn the \(\lambda\)Fs as in the tabular case, but with the \(\lambda\)F network parameterized as a three-layer MLP with 16 units in each of the hidden layers and ReLU activations. All networks were optimized using Adam with a learning rate of 3e-4. The tabular and neural network experiments were repeated for three random seeds, the former was run for 1,500 episodes and the latter for 2,000.

### Policy Learning

We ran the experiments for Fig. 5.2 in a version of the TwoRooms environment from the NeuroNav benchmark [22] with reward modified to decay with a specified\(\lambda_{true}=0.5\) and discount factor \(\gamma=0.95\). The initial rewards in the top right goal and the lower room goal locations were \(5\) and the top left goal had initial reward \(10\). The observations in the neural network experiment were one-hot state indicators. The tabular \(Q_{\lambda}\) experiments run the algorithm in Algorithm 1 for 500 episodes for \(\lambda\in\{0.0,0.5,1.0\}\), with \(\lambda_{true}\) set to 0.5, repeated for three random seeds. Experiments used a constant step size \(\alpha=0.1\). There were five possible actions: up, right, down, left, and stay. The recurrent A2C agents were based on the implementation from the BSuite library [36] and were run for 7,500 episodes of maximum length \(H=100\) with \(\gamma=0.99\) using the Adam optimizer with learning rate 3e-4. The experiment was repeated for three random seeds. The RNN was an LSTM with 128 hidden units and three output heads: one for the policy, one for the value function, and one for the \(\lambda\)F. The base features were one-hot representations of the current state, 121-dimensional in this case.

### Tabular GPI

The agent is assumed to be given or have previously acquired four policies \(\{\pi_{0},\pi_{1},\pi_{2},\pi_{3}\}\) individually optimized to reach rewards located in each of the four rooms of the environment. There are three reward locations \(\{g_{0},g_{1},g_{2}\}\) scattered across the rooms, each with its own initial reward \(\bar{r}=[5,10,5]\) and all with \(\lambda=0.5\). At the beginning of each episode, an initial state \(s_{0}\) is sampled uniformly from the set of available states. An episode terminates either when the maximum reward remaining in any of the goal states is less than \(0.1\) or when the maximum number of steps \(H=40\) is reached. Empty states carry a reward of \(0\), encountering a wall gives a reward of \(-1\), and the discount factor is set to \(\gamma=0.97\).

For each of the four policies, we learn \(\lambda\mathrm{R}\)s with \(\lambda\) equal to \(0\), \(0.5\), and \(1.0\) using standard dynamic programming (Bellman error curves plotted in Fig. F3), and record the returns obtained while performing GPE+GPI with each of these representations over the course of 50 episodes. Bellman error curves for the \(\lambda\mathrm{R}\)s are In the left panel of Fig. 5.3, we can indeed see that using the correct \(\lambda\) (0.5) nets the highest returns. Example trajectories for each of \(\lambda\mathrm{R}\) are shown in the remaining panels.

### Pixel-Based GPI

In this case, the base policies \(\Pi\) were identical to those used in the tabular GPI experiments. First, we collected a dataset consisting of 340 observation trajectories \((o_{0},o_{1},\ldots,o_{H-1})\in\mathcal{O}^{H}\) with \(H=19\) from each policy, totalling \(6,460\) observations. Raw observations were \(128\times 128\times 3\) and were converted to grayscale. The previous seven observations were stacked and used to train a Laplacian eigenfunction base feature network in the same way as [25]. For observations less than seven steps from the start of an episode, the remaining frames were filled in as all black observations (i.e., zeros). The network consisted of four convolutional layers with 32 \(3\times 3\) filters with strides \((2,2,2,1)\), each followed by a ReLU nonlinearity. This was then flattened and passed through a Layer Norm layer [37] and a \(\tanh\) nonlinearity before three fully fully connected layers, the first two with 64 units each and ReLU nonlinearities and the final, output layer with 50 units. The output was \(L_{2}\)-normalized as in [25]. This network \(\phi:\mathcal{O}^{7}\mapsto\mathbb{R}^{D}\) (with \(D=50\)) was trained on the stacked observations for 10 epochs using the Adam optimizer and learning rate 1e-4 with batch size \(B=64\). To perform policy evaluation, the resulting features, evaluated on the dataset of stacked observations were collected into their own dataset of \((s_{t},a_{t+1},s_{t+1},a_{t+1})\) tuples, where \(s_{t}\triangleq o_{t-6:t}\). The "states" were normalized to be between 0 and 1, and a vector **w** was fit to the actual associated rewards via linear regression on the complete dataset. The \(\lambda\)F network was then trained using a form of neural fitted Q-iteration [FQI; 38] modified for policy evaluation with \(\lambda\)Fs (Algorithm 2). The architecture for the \(\lambda\)F network was identical to the base feature network, with the exception that the hidden size of the fully connected layers was 128 and the output dimension was \(D|\mathcal{A}|=250\). FQI was run for \(K=20\) outer loop iterations, with each inner loop supervised learning setting run for \(L=100\) epochs on the current dataset. Supervised learning was done using Adam with learning rate 3e-4 and batch size \(B=64\). Given the trained networks, GPI proceeded as in the tabular case, i.e.,

\[a_{t}=\operatorname*{argmax}_{a\in\mathcal{A}}\max_{\pi\in\Pi}\textbf{w}^{ \textsf{T}}\varphi_{\theta}^{\pi}(s_{t},a). \tag{12}\]

50 episodes were run from random starting locations for \(H=50\) steps and the returns measured. Learning curves for the base features and for \(\lambda\)F fitting are shown in Fig. 1. The \(\lambda\)F curve measures the mean squared error as in Eq. (11).

The feature visualizations were created by performing PCA to reduce the average \(\lambda\)F representations for observations at each state in the environment to 2D. Each point in the scatter plot represents the reduced representation on the \(xy\) plane, and is colored according to the \(\lambda\)-conditioned value of the underlying state.

### Continuous Control

\(\lambda\)-SacSee Appendix I for details.

### Learning the \(\lambda\)O with FB

Training the \(\lambda\)O with the FB parameterization proceeds in much the same way as in [12], but adjusted for a different norm and non-Markovian environment. Wesummarize the learning procedure in Algorithm 3. The loss function \(\mathcal{L}\) is derived in Appendix H, with the addition of the following regularizer:

\[\|\mathbb{E}_{s\sim\rho}B_{\omega}(s)B_{\omega}(s)^{\top}-I\|^{2}.\]

This regularizer encourages \(B\) to be approximately orthonormal, which promotes identifiability of \(F_{\theta}\) and \(B_{\omega}\)[12].

## Appendix F Additional Results

See surrounding sections.

Figure E.1: **Learning curves for \(\lambda\)F policy evaluation. Results are averaged over three runs, with shading indicating one unit of standard error.**

```
1:Require: Probability distribution \(\nu\) over \(\mathbb{R}^{d}\), randomly initialized networks \(F_{\theta},B_{\omega}\), learning rate \(\eta\), mini-batch size \(B\), number of episodes \(E\), number of epochs \(M\), number of time steps per episode \(T\), number of gradient steps \(N\), regularization coefficient \(\beta\), Polyak coefficient \(\alpha\), initial diminishing constant \(\lambda\), discount factor \(\gamma\), exploratory policy greediness \(\epsilon\), temperature \(\tau\)
2:// Stage 1: Unsupervised learning phase
3:\(\mathcal{D}\leftarrow\varnothing\)
4:for epoch \(m=1,\ldots,M\)do
5:for episode \(i=1\ldots,E\)do
6: Sample \(z\sim\nu\)
7: Observe initial state \(s_{1}\)
8:for\(t=1,\ldots,T\)do
9: Select \(a_{t}\)\(\epsilon-\)greedy with respect to \(F_{\theta}(s_{t},a,z)^{\top}z\)
10: Observe reward \(r_{t}(s_{t})\) and next state \(s_{t+1}\)
11:\(\mathcal{D}\leftarrow\mathcal{D}\cup\{(s_{t},a_{t},r_{t}(s_{t}),s_{t+1})\}\)
12:endfor
13:endfor
14:for\(n=1,\ldots,N\)do
15: Sample a minibatch \(\{(s_{j},a_{j},r_{j}(s_{j}),s_{j+1})\}_{j\in J}\subset\mathcal{D}\) of size \(|J|=B\)
16: Sample a minibatch \(\{\tilde{s}_{j}\}_{j\in J}\subset\mathcal{D}\) of size \(|J|=B\)
17: Sample a minibatch \(\{s^{\prime}_{j}\}_{j\in J}\stackrel{{\text{iid}}}{{\mapsto}}\mu\) of size \(|J|=B\)
18: Sample a minibatch \(\{z_{j}\}_{j\in J}\stackrel{{\text{iid}}}{{\sim}}\nu\) of size \(|J|=B\)
19: For every \(j\in J\), set \(\pi_{z_{j}}(\cdot|s_{j+1})=\texttt{softmax}\left(F_{\theta^{-}}(s_{j+1},\cdot, z_{j})^{\top}z_{j}/\tau\right)\)
20: \[\mathcal{L}(\theta,\omega) \leftarrow\frac{1}{2B^{2}}\sum_{j,k\in J^{2}}\left(F_{\theta}(s_{ j},a_{j},z_{j})^{\top}B_{\omega}(s^{\prime}_{k})-\gamma\sum_{a\in\mathcal{A}}\pi_{z_ {j}}(a|s_{j+1})F_{\theta^{-}}(s_{j+1},a,z_{j})^{\top}B_{\omega^{-}}(s^{\prime }_{k})\right)^{2}\] \[\quad-\frac{1}{B}\sum_{j\in J}F_{\theta}(s_{j},a_{j},z_{j})^{\top }B_{\omega}(s_{j})\] \[\quad+\frac{\gamma(1-\lambda)}{B}\sum_{j\in J}\mu(s_{j})F_{\theta }(s_{j},a_{j},z_{j})^{\top}B_{\omega}(s_{j})\sum_{a\in\mathcal{A}}\pi_{z_{j}} (a|s_{j+1})F_{\theta^{-}}(s_{j+1},a,z_{j})^{\top}B_{\omega^{-}}(s_{j})\] \[\quad+\beta\left(\frac{1}{B^{2}}\sum_{j,k\in J^{2}}B_{\omega}(s_{ j})^{\top}\bar{B}_{\omega}(\tilde{s}_{k})\bar{B}_{\omega}(s_{j})^{\top}\bar{B}_{ \omega}(\tilde{s}_{k})-\frac{1}{B}\sum_{j\in J}B_{w}(s_{j})^{\top}\bar{B}_{ \omega}(s_{j})\right)\]
21: Update \(\theta\) and \(\omega\) via one step of Adam on \(\mathcal{L}\)
22: Sample a minibatch \(\{(s_{j},r_{j}(s_{j}),s_{j+1},r_{j+1}(s_{j+1}))\}_{j\in J}\) of size \(|J|=B\) from \(\mathcal{D}\)
23:\(\mathcal{L}_{\lambda}(\lambda)\leftarrow\frac{1}{2B}\sum_{j\in J}\mathbb{1}(s _{j+1}=s_{j})\left(r_{j+1}(s_{j+1})-\lambda r_{j}(s_{j})\right)^{2}\)
24: Update \(\lambda\) via one step of Adam on \(\mathcal{L}_{\lambda}\)
25:endfor
26:\(\theta^{-}\leftarrow\alpha\theta^{-}+(1-\alpha)\theta\)
27:\(\omega^{-}\leftarrow\alpha\omega^{-}+(1-\alpha)\omega\)
28:endfor
29:// Stage 2: Exploitation phase for a single episode with initial reward \(r_{0}(s)\)
30:\(z_{R}\leftarrow\sum_{s\in\mathcal{S}}\mu(s)r_{0}(s)B_{\omega}(s)\)
31: Observe initial state \(s_{1}\)
32:for\(t=1,\ldots,T\)do
33:\(a_{t}\leftarrow\operatorname*{argmax}_{a\in\mathcal{A}}F(s_{t},a,z_{R})^{ \top}z_{R}\)
34: Observe reward \(r_{t}(s)\) and next state \(s_{t+1}\)
35:\(z_{R}\leftarrow\sum_{s\in\mathcal{S}}\mu(s)r_{t}(s)B_{\omega}(s)\)
36:endfor
```

**Algorithm 3**\(\lambda\)O FB Learning

## Appendix G Advantage of the Correct \(\lambda\)

Importantly, for GPE using the \(\lambda\mathrm{R}\) to work in this setting, the agent must either learn or be provided with the updated reward vector \(\mathbf{r}_{\lambda}\) after each step/encounter with a rewarded state. This is because the \(\lambda\mathrm{R}\) is forward-looking in that it measures the (diminished) expected occupancies of states in the future without an explicit mechanism for remembering previous visits. For simplicity in this case, we provide this vector to the agent at each step--though if we view such a multitask agent as simply as a module carrying out the directives of a higher-level module or policy within a hierarchical framework as in, e.g., Feudal RL [39], the explicit provision of reward information is not unrealistic. Regardless, a natural question in this case is whether there is actually any value in using the \(\lambda\mathrm{R}\) with the correct value of \(\lambda\) in this

Figure F.1: **A simple grid and several policies.**

Figure F.3: **Convergence of dynamic programming on FourRooms with and without stochastic transitions.**setting: If the agent is provided with the correct reward vector, then wouldn't policy evaluation work with any \(\lambda\mathrm{R}\)?

To see that this is not the case, consider the three-state toy MDP shown in Figure Fig. G.1, where \(\bar{r}(s_{1})=10\), \(\bar{r}(s_{2})=6\), \(\bar{r}(s_{0})=0\), \(\lambda(s_{1})=0\), \(\lambda(s_{2})=1.0\), and \(\gamma=0.99\). At time \(t=0\), the agent starts in \(s_{0}\). Performing policy evaluation with \(\lambda(s_{1})=\lambda(s_{2})=1\) (i.e., with the SR) would lead the agent to go left to \(s_{1}\). However,

Figure F.2: **Visualizing the SR, the \(\lambda\mathrm{R}\) and the FR.** We can see that the \(\Phi_{1}^{\pi}\) is equivalent to the SR and \(\Phi_{0}^{\pi}\) is equivalent to the FR, with intermediate values of \(\lambda\) providing a smooth transition between the two.

the reward would then disappear, and policy evaluation on the second step would lead it to then move right to \(s_{0}\) and then \(s_{2}\), where it would stay for the remainder of the episode. In contrast, performing PI with the correct values of \(\lambda\) would lead the agent to go right to \(s_{2}\) and stay there. In the first two timesteps, the first policy nets a total reward of \(10+0=10\), while the second policy nets \(6+5.94=11.94\). (The remaining decisions are identical between the two policies.) This is a clear example of the benefit of having the correct \(\lambda\), as incorrect value estimation leads to suboptimal decisions even when the correct reward vector/function is provided at each step.

## Appendix H The \(\lambda\) Operator

To learn the \(\lambda\)O, we would like to define \(\Phi^{\pi}_{\lambda}(s_{t},ds^{\prime})\triangleq\varphi^{\pi}_{\lambda}(s_{t},s^{\prime})\mu(ds^{\prime})\) for some base policy \(\mu\). However, this would lead to a contradiction:

\[\Phi^{\pi}_{\lambda}(s,A\cup B)=\int_{A}\varphi^{\pi}_{\lambda}(s,ds^{\prime}) \mu(ds^{\prime})+\int_{B}\varphi^{\pi}_{\lambda}(s,ds^{\prime})\mu(ds^{\prime })=\Phi^{\pi}_{\lambda}(s,A)+\Phi^{\pi}_{\lambda}(s,B)\]

for all disjoint \(A,B\), contradicting Lemma B.4.

For now, we describe how to learn the \(\lambda\)O for discrete \(\mathcal{S}\), in which case we have \(\Phi^{\pi}_{\lambda}(s,s^{\prime})=\varphi^{\pi}_{\lambda}(s,s^{\prime})\mu(s ^{\prime})\), i.e., by learning \(\varphi\) we learn a weighted version of \(\Phi\). We define the following norm, inspired by Touati et al. [25]:

\[\|\Phi^{\pi}_{\lambda}\|^{2}_{\rho}\triangleq\mathop{\mathbb{E}}_{s\sim\rho \atop s^{\prime}\sim\mu}\left[\left(\frac{\Phi^{\pi}_{\lambda}(s,s^{\prime})}{ \mu(s^{\prime})}\right)^{2}\right],\]

where \(\mu\) is any density on \(\mathcal{S}\). In the case of finite \(\mathcal{S}\), we let \(\mu\) be the uniform density. We then minimize the Bellman error for \(\Phi^{\pi}_{\lambda}\) with respect to \(\|\cdot\|^{2}_{\rho,\mu}\) (dropping the sub/superscripts on \(\Phi\) and \(\varphi\) for clarity):

Figure G.1: **A 3-state toy environment**.

\[\mathcal{L}(\Phi) =\|\varphi\mu-(I\odot(\mathbf{11^{T}}+\lambda\gamma P^{\pi}\varphi\mu) +\gamma(\mathbf{11^{T}}+I)\odot P^{\pi}\varphi\mu)\|_{\rho,\mu}^{2}\] \[=\mathbb{E}_{s_{t}\sim\rho,s^{\prime}\sim\mu}\Big{[}\Big{(} \varphi(s_{t},s^{\prime})-\frac{\mathbb{1}\left(s_{t}=s^{\prime}\right)}{\mu(s ^{\prime})}\] \[\qquad\qquad+\gamma(1-\lambda)\frac{\mathbb{1}\left(s_{t}=s^{ \prime}\right)}{\mu(s^{\prime})}\mathbb{E}_{s_{t+1}\sim p^{\pi}(\cdot|s_{t})} \bar{\Phi}(s_{t+1},s^{\prime})-\gamma\mathbb{E}_{s_{t+1}\sim p^{\pi}(\cdot|s_{ t})}\bar{\varphi}(s_{t+1},s^{\prime})\Big{)}^{2}\Big{]}\] \[\overset{+c}{=}\mathbb{E}_{s_{t},s_{t+1}\sim\rho,s^{\prime}\sim \mu}\left[\left(\varphi(s_{t},s^{\prime})-\gamma\bar{\varphi}(s_{t+1},s^{ \prime})\right)^{2}\right]\] \[\qquad\qquad-2\mathbb{E}_{s_{t},s_{t+1}\sim\rho}\left[\sum_{s^{ \prime}}\mu(s^{\prime})\varphi(s_{t},s^{\prime})\frac{\mathbb{1}\left(s_{t}=s^ {\prime}\right)}{\mu(s^{\prime})}\right]\] \[\qquad\qquad+2\gamma(1-\lambda)\mathbb{E}_{s_{t},s_{t+1}\sim\rho} \left[\sum_{s^{\prime}}\mu(s^{\prime})\varphi(s_{t},s^{\prime})\bar{\varphi}(s_ {t+1},s^{\prime})\mu(s^{\prime})\frac{\mathbb{1}\left(s_{t}=s^{\prime}\right)} {\mu(s^{\prime})}\right]\] \[\overset{+c}{=}\mathbb{E}_{s_{t},s_{t+1}\sim\rho,s^{\prime}\sim \mu}\left[\left(\varphi(s_{t},s^{\prime})-\gamma\bar{\varphi}(s_{t+1},s^{ \prime})\right)^{2}\right]-2\mathbb{E}_{s_{t}\sim\rho}[\varphi(s_{t},s_{t})]\] \[\qquad\qquad+2\gamma(1-\lambda)\mathbb{E}_{s_{t},s_{t+1}\sim\rho} [\mu(s_{t})\varphi(s_{t},s_{t})\bar{\varphi}(s_{t+1},s_{t})],\]

Note that we recover the SM loss when \(\lambda=1\). Also, an interesting interpretation is that when the agent can never return to its previous state (i.e., \(\varphi(s_{t+1},s_{t})=0\)), then we also recover the SM loss, regardless of \(\lambda\). In this way, the above loss appears to "correct" for repeated state visits so that the measure only reflects the first visit.

\[\mathcal{L}(\Phi) =\mathbb{E}_{s_{t},a_{t},s_{t+1}\sim\rho,s^{\prime}\sim\mu}\left[ \left(F(s_{t},a_{t},z)^{\top}B(s^{\prime})-\gamma\bar{F}(s_{t+1},\pi_{z}(s_{t+ 1}),z)^{\top}\bar{B}(s^{\prime})\right)^{2}\right]\] \[\qquad-2\mathbb{E}_{s_{t},a_{t}\sim\rho}\left[F(s_{t},a_{t},z)^{ \top}B(s_{t})\right]\] \[\qquad+2\gamma(1-\lambda)\mathbb{E}_{s_{t},a_{t},s_{t+1}\sim\rho} \left[\mu(s_{t})F(s_{t},a_{t},z)^{\top}B(s_{t})\bar{F}(s_{t+1},\pi_{z}(s_{t+ 1}),z)^{\top}\bar{B}(s_{t})\right]\] (H.1)

Even though the \(\lambda\)O is not a measure, we can use the above loss to the continuous case, pretending as though we could take the Radon-Nikodym derivative \(\frac{\Phi(s,ds^{\prime})}{\mu(ds^{\prime})}\).

### Experimental Results with the FB Parameterization

To show that knowing the correct value of \(\lambda\) leads to improved performance, we trained \(\lambda\)O with the FB parameterization on the FourRooms task of Fig. 5.3, but with each episode initialized at a random start state and with two random goal states. Average per-epoch reward is shown in Fig. 2. We tested performance with \(\lambda_{\text{true}},\lambda_{\text{agent}}\in\{0.5,1.0\}\), where \(\lambda_{\text{true}}\) denotes the true environment diminishing rate and \(\lambda_{\text{agent}}\) denotes the diminishing rate that the agent uses. For the purpose of illustration, we do not allow the agent to learn \(\lambda\). We see in Fig. 2 that using the correct \(\lambda\) leads to significantly increased performance. In particular, the left plot shows that assuming \(\lambda=1\), i.e., using the SR, in a diminishing environment can lead to highly suboptimal performance.

Hyperparameters used are given in Table 1 (notation as in Algorithm 3).

### \(\lambda\)O and the Marginal Value Theorem

To study whether the agent's behavior is similar to behavior predicted by the MVT, we use a very simple task with constant starting state and vary the distance between rewards (see Fig. 1(a)). When an agent is in a reward state, we define an MVT-optimal leaving time as follows (similar to that of [8] but accounting for the non-stationarity of the reward).

Let \(R\) denote the average per-episode reward received by a trained agent, \(r(s_{t})\) denote the reward received at time \(t\) in a given episode, \(R_{t}=\sum_{u=0}^{t}r(s_{u})\) denote the total reward received until time \(t\) in the episode, and let \(T\) be episode length. Then, on average, the agent should leave its current reward state at time \(t\) if the next reward that it would receive by staying in \(s_{t}\), i.e., \(\lambda r(s_{t})\), is less than

\[\frac{R-R_{t}}{T}.\]

In other words, the agent should leave a reward state when its incoming reward falls below the diminished average per-step reward of the environment. We compute \(R\) by averaging reward received by a trained agent over many episodes.

Previous studies have trained agents that assume stationary reward to perform foraging tasks, even though the reward in these tasks is non-stationary. These agents can still achieve good performance and MVT-like behavior [8]. However, because

\begin{table}
\begin{tabular}{l r} \hline \hline
**Hyperparameter** & **Value** \\ \hline \(M\) & 100 \\ \(E\) & 100 \\ \(N\) & 25 \\ \(B\) & 128 \\ \(T\) & 50 \\ \(\gamma\) & 0.99 \\ \(\alpha\) & 0.95 \\ \(\eta\) & 0.001 \\ \(\tau\) & 200 \\ \(\epsilon\) & 1 \\ \hline \hline \end{tabular}
\end{table}
Table 1: \(\lambda\)**O-FB hyperparameters.**

Figure 1: **Performance of the \(\lambda\)O-FB with two values of \(\lambda\).** Results averaged over six seeds and 10 episodes per seed. Error bars indicate standard error.

they target the standard RL objective

\[\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^{k}r(s_{t+k})\Big{|}s_{t}=s\right],\]

which requires \(\gamma<1\) for convergence, optimal behavior is recovered only with respect to the _discounted MVT_, in which \(R\) (and in our case, \(R_{t}\)) weights rewards by powers of \(\gamma\)[8].

In Fig. H.1(b-c) we perform a similar analysis to that of [8] and show that, on average over multiple distances between rewards, \(\lambda\)O-FB performs similarly to the discounted MVT for \(\gamma=0.99\) and the standard MVT for \(\gamma=1.0\). An advantage of the \(\lambda\)O is that it is finite for \(\gamma=1.0\) provided that \(\lambda<1\). Hence, as opposed to previous work, we can recover the standard MVT without the need to adjust for discounting.

Hyperparameters used are given in Table 1 (notation as in Algorithm 3).

## Appendix I Sac

Mitigating Value OverestimationOne well-known challenge in deep RL is that the use of function approximation to compute values is prone to overestimation. Standard approaches to mitigate this issue typically do so by using _two_ value functions and either taking the minimum \(\min_{i\in\{1,2\}}Q_{i}^{\pi}(s,a)\) to form the Bellman target for a given \((s,a)\) pair [40] or combining them in other ways [41]. However, creating multiple networks is expensive in both computation and memory. Instead, we hypothesized that it might be possible to address this issue by using \(\lambda\)-based values. To test this idea, we modified the Soft Actor-Critic [SAC; 42] algorithm to compute \(\lambda\)Fs-based values by augmenting the soft value target \(\mathcal{T}_{soft}Q=r_{t}+\gamma\mathbb{E}V_{soft}(s_{t+1})\), where \(V_{soft}(s_{t+1})\) is given by the expression

\[\mathbb{E}_{a_{t+1}\sim\pi(\cdot|s_{t+1})}\Big{[}\bar{Q}(s_{t+1}, a_{t+1})+(\lambda-1)\textbf{w}^{\textsf{T}}(\phi(s_{t},a_{t}) \odot\varphi_{\lambda}(s_{t+1},a_{t+1}))\\ -\alpha\log\pi(a_{t+1}\mid s_{t+1})\Big{]}\]

Figure H.2: **Analysis of MVT-like behavior of \(\boldsymbol{\lambda}\)O-FB.****a**) Three environments with equal start state and structure but different distances between reward states. **b**) Difference between the agentâ€™s leave times and MVT-predicted leave times for \(\gamma=0.99\), with discounting taken into account. The agent on average behaves similar to the discounted MVT. **c**) Difference between the agentâ€™s leave times and MVT-predicted leave times for \(\gamma=1.0\), i.e., with no discounting taken into account. The agent on average behaves similar to the MVT. Results for (b) and (c) are averaged over three seeds. Error bars indicate standard error.

A derivation as well as pseudocode for the modified loss is provided in Appendix E.5. Observe that for \(\lambda=1\), we recover the standard SAC value target, corresponding to an assumed stationary reward. We apply this modified SAC algorithm, which we term \(\lambda\)-SAC to feature-based Mujoco continuous control tasks within OpenAI Gym [43]. We found that concatenating the raw state and action observations \(\hat{\phi}_{t}=[s_{t},a_{t}]\) and normalizing them to \([0,1]\) make effective regressors to the reward. That is, we compute base features as

\[\phi_{t}^{b}=\frac{\tilde{\phi}_{t}^{b}-\min_{b}\tilde{\phi}_{t}^{b}}{\max_{b} \tilde{\phi}_{t}^{b}-\min_{b}\tilde{\phi}_{t}^{b}},\]

where \(b\) indexes \((s_{t},a_{t})\) within a batch. Let \(X\in[0,1]^{B\times D}\) be the concatenated matrix of features for a batch, where \(D=\dim(\mathcal{S})+\dim(\mathcal{A})\). Then,

\[\textbf{w}_{t}=\left(X^{\mathsf{T}}X\right)^{-1}X^{\mathsf{T}}\textbf{r},\]

where here **r** denotes the vector of rewards from the batch. In addition to using a fixed \(\lambda\) value, ideally we'd like an agent to adaptively update \(\lambda\) to achieve the best balance of optimism and pessimism in its value estimates. Following [44], we frame this decision as a multi-armed bandit problem, discretizing \(\lambda\) into three possible values \(\{0,0.5,1.0\}\) representing the arms of the bandit. At the start of each episode, a random value of \(\lambda\) is sampled from these arms and used in the value function update. The probability of each arm is updated using the Exponentially Weighted Average Forecasting algorithm [45], which modulates the probabilities in proportion to a feedback score. As in [44], we use the difference in cumulative (undiscounted) reward between the current episode \(\ell\) and the previous one \(\ell-1\) as this feedback signal: \(R_{\ell}-R_{\ell-1}\). That is, the probability of selecting a given value of \(\lambda\) increases if performance is improving and decreases if it's decreasing. We use identical settings for the bandit algorithm as in [44]. We call this variant \(\lambda\)-SAC.

We plot the results for SAC with two critics (as is standard), SAC with one critic, SAC with a single critic trained with \(\lambda\)F-based values ("\(x\)-SAC" denotes SAC trained with a fixed \(\lambda=x\)), and \(\lambda\)-SAC trained on the HalfCheetah-v2 and Hopper-v2 Mujoco environments. All experiments were repeated over eight random seeds. HalfCheetah-v2 was found by [44] to support "optimistic" value estimates in that even without pessimism to reduce overestimation it was possible to perform well. Consistent with this, we found that single-critic SAC matched the performance of standard SAC, as did 1-SAC (which amounts to training a standard value function with the auxiliary task of SF prediction). Fixing lower values of \(\lambda\) performed poorly, indicating that over-pessimism is harmful in this environment. However, \(\lambda\)-SAC eventually manages to learn to set \(\lambda=1\) and matches the final performance of the best fixed algorithms. Similarly, in [44] it was observed that strong performance in Hopper-v2 was associated with pessimistic value estimates. Consistent with this, \(\lambda\)-SAC learns to select lower values of \(\lambda\), again matching the performance of SAC while only requiring one critic and significantly reducing the required FLOPS Fig. I.2. We consider these results to be very preliminary, and hope to perform more experiments on other environments. We also believe \(\lambda\)-SAC could be improved by using the difference between the current episode's total reward and the _average_ of the total rewards from previous episodes \(R_{\ell}-(\ell-1)^{-1}\sum_{i=1}^{\ell-1}R_{i}\) as a more 

[MISSING_PAGE_EMPTY:35]

[MISSING_PAGE_FAIL:36]

which satisfies the following recursion:

\[P^{\pi}(s,s^{\prime})=\mathbb{1} \,(s=s^{\prime})+\gamma(\lambda_{d}\mathbb{1}\,(s=s^{\prime})+1\] \[-\mathbb{1}\,(s=s^{\prime}))(\lambda_{r}(1-\mathbb{1}\,(s=s^{ \prime}))+\mathbb{1}\,(s=s^{\prime}))\mathbb{E}_{s_{t+1}\sim p^{\pi}(\cdot|s)}P^ {\pi}(s_{t+1},s^{\prime}).\]

While neither the reward nor the representation are guaranteed to be finite, \(P^{\pi}\) could be learned via TD updates capped at a suitable upper bound.

## Appendix K \(\lambda\) vs. \(\gamma\)

We now briefly discuss the interaction between the temporal discount factor \(\gamma\) commonly used in RL and the diminishing utility rate \(\lambda\). The key distinction between the two is that all rewards decay in value every time step with respect to \(\gamma\), regardless of whether a state is visited or not. With \(\lambda\), however, decay is specific to each state (or \((s,a)\) pair) and only occurs when the agent visits that state. Thus, \(\gamma\) decays reward in a global manner which is independent of the agent's behavior, and \(\lambda\) decays reward in a local manner which dependent on the agent's behavior. In combination, they have the beneficial effect of accelerating convergence in dynamic programming (Fig. F.3). This indicates the potential for the use of higher discount factors in practice, as paired with a decay factor \(\lambda\), similar (or faster) convergence rates could be observed even as agents are able to act with a longer effective temporal horizon.

Figure J.2: **Performance on TwoRooms with replenishing rewards.** Return is averaged over five runs, with shading indicating one unit of standard error.

Figure J.1: **Visualizing three different replenishment schemes.** For all schemes, \(\bar{r}(s)=1\) and visits to \(s\) are at \(t=2,5\). (Left) The time elapsed reward with \(\lambda=0.5\); (Middle) The eligibility trace reward with \(\lambda_{r}=\lambda_{d}=0.5\); (Right) The total time reward with \(\lambda_{d}=0.5,\lambda_{r}=0.9\).

Compute Resources

The \(\lambda\)F-based experiments shown were run on a single NVIDIA GeForce GTX 1080 GPU. The recurrent A2C experiments took roughly 30 minutes, base feature learning for policy composition took approximately 45 minutes, \(\lambda\)F learning for policy composition took approximately 10 hours, and the SAC experiments took approximately 8 hours per run. The \(\lambda\)F training required roughly 30GB of memory due to the size of the dataset. All experiments in Section 6 and Appendix H were run on a single RTX5000 GPU and each training and evaluation run took about 30 minutes. All other experiments were run on a 2020 MacBook Air laptop 1.1 GHz Quad-Core Intel Core i5 CPU and took less than one hour to train.

## Appendix M Broader Impact Statement

We consider this work to be primarily of a theoretical nature pertaining to sequential decision-making primarily in the context of natural intelligence. While it may have applications for more efficient training of artificial RL agents, it is hard to predict long-term societal impacts.