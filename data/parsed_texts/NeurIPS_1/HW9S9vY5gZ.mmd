# No-Regret Learning in Harmonic Games:

Extrapolation in the Face of Conflicting Interests

Davide Legacci

University. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG 38000 Grenoble, France

{davide.legacci,panayotis.mertikopoulos}@univ-grenoble-alpes.fr

Panayotis Mertikopoulos

University. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG 38000 Grenoble, France

{davide.legacci,panayotis.mertikopoulos}@univ-grenoble-alpes.fr

Christos Papadimitriou

Columbia University, NYC, &

Archimedes/Athena RC, Greece

christos@columbia.edu

Georgios Piliouras

Google DeepMind

London, UK

gpil@google.com

Bary Pradelski

CNRS, Maison Francaise d'Oxford

2-10 Norham Road, Oxford, OX2 6SE, United Kingdom

bary.pradelski@cnrs.fr

Corresponding author.

###### Abstract

The long-run behavior of multi-agent learning - and, in particular, _no-regret learning_ - is relatively well-understood in potential games, where players have aligned interests. By contrast, in harmonic games - the strategic counterpart of potential games, where players have _conflicting_ interests - very little is known outside the narrow subclass of 2-player zero-sum games with a fully-mixed equilibrium. Our paper seeks to partially fill this gap by focusing on the full class of (generalized) harmonic games and examining the convergence properties of follow-the-regularized-leader (FTRL), the most widely studied class of no-regret learning schemes. As a first result, we show that the continuous-time dynamics of FTRL are _Poincare recurrent_, that is, they return arbitrarily close to their starting point infinitely often, and hence fail to converge. In discrete time, the standard, "vanilla" implementation of FTRL may lead to even worse outcomes, eventually trapping the players in a perpetual cycle of best-responses. However, if FTRL is augmented with a suitable extrapolation step - which includes as special cases the optimistic and mirror-prox variants of FTRL - we show that learning converges to a Nash equilibrium from any initial condition, and all players are guaranteed at most \(\mathcal{O}(1)\) regret. These results provide an in-depth understanding of no-regret learning in harmonic games, nesting prior work on 2-player zero-sum games, and showing at a high level that harmonic games are the canonical complement of potential games, not only from a strategic, but also from a dynamic viewpoint.

## 1 Introduction

The question of "as if" rationality - that is, whether selfishly-minded, myopic agents may learn to behave "_as if_" they were fully rational - has been one of the cornerstones of non-cooperative game theory, and for good reason. Especially in modern applications of game theory to machine learning and data science - from online ad auctions to recommender systems and multi-agent reinforcementlearning - the standard postulates of rationality (knowledge of the game, capacity to compute an equilibrium, flawless execution of equilibrium strategies, common knowledge of rationality, etc.) are almost never met in practice; as a result, game-theoretic predictions that rely on these assumptions are likewise put into question. By contrast, given the ease of implementing and deploying cheap, computationally efficient learning algorithms and policies at a large scale, it is often more logical to turn to the policy being deployed as the object of interest. The aim is then to understand its long-run behavior - and, in particular, whether it ultimately leads to equilibrium.

A major obstacle in this approach is the complexity of computing a Nash equilibrium, a problem which is known to be complete for PPAD - and hence intractable - by the seminal work of Daskalakis et al. [12]. This result implies that it is not plausible to expect any algorithm to converge to Nash equilibrium in _all_ games (at least, not in a reasonable amount of time), so it dovetails naturally with the impossibility results of Hart & Mas-Colell [22, 23] who showed that there are no uncoupled learning dynamics that converge to Nash equilibrium in all games. On that account, it is natural to ask in which classes of games we can expect a learning algorithm to converge, in which classes we cannot, and under what conditions.

Perhaps the most well-behaved class of games in terms of learning is the class of _potential games_[44, 55], where players have _common_ interests - not necessarily driving them to play the same strategy, but in the sense that externalities are symmetric and aligned along a common objective (the potential of the game). In this class of games, the behavior of learning dynamics - and, in particular, no-regret learning [8, 13, 19, 27, 28, 36, 40, 43, 59] - are relatively well understood, and there is a wide range of equilibrium convergence results, from continuous to discrete time, and even with bandit, payoff-based feedback [24, 25, 55].

By contrast, in the presence of _conflicting_ interests, the situation can be quite different. In two-player zero-sum games with a fully-mixed equilibrium - such as Matching Pennies - the continuous-time dynamics of no-regret, regularized learning are recurrent in the sense of Poincare - that is, the induced trajectory of play returns arbitrarily close to where it started infinitely many times [41, 48]. In discrete time, the situation becomes more complicated: the vanilla version of follow-the-regularized-leader (FTRL) - the most widely studied family of no-regret algorithms - is no longer recurrent, but it diverges away from equilibrium in the same class of games [18, 42]. On the other hand, if players employ an optimistic / extra-gradient variant of FTRL, the induced trajectory of play converges to equilibrium [15, 42] and, under certain conditions, it is even possible to show that it converges at a geometric rate [62].

At the same time, zero-sum games may also admit a potential function, so it is not possible to predict the outcome of a learning process based on where it stands along the potential / zero-sum axis. The non-trivial intersection of these classes means that potential and zero-sum games are _not_ complementary, and this, not only from a strategic, but also from a dynamic viewpoint. Instead, the true strategic complement of potential games is the class of _harmonic games_. This class was first considered by Candogan et al. [6], who established a remarkable decomposition result: Every game in normal form can be decomposed as the sum of a potential game and a harmonic game, and this decomposition is unique up to affine transformations that do not alter the equilibrium outcomes of the game. In particular, the class of potential and harmonic games intersect trivially (up to strategic equivalence), and all two-player zero-sum games with an interior equilibrium are harmonic, thus lending credence to the fact that it is harmonic games, not zero-sum games, that correctly capture the notion of conflicting interests in this context.

This raises the question:

_What is the behavior of no-regret algorithms and dynamics in harmonic games?_

Except for a very recent paper by Legacci et al. [35] (which we discuss below), almost nothing is known on this question. With this backdrop, our contributions can be summarized as follows:

1. Starting with a continuous-time model of no-regret learning, we show that all FTRL dynamics are Poincare recurrent in all harmonic games. This generalizes and extends the recent result of Legacci et al. [35] for the replicator dynamics in uniform harmonic games (a subclass of harmonic games in which the uniform distribution is always a Nash equilibrium).22. In discrete-time models of learning, the standard implementation of FTRL cannot be expected to converge (since it fails to do so in Matching Pennies). To correct this behavior, we consider a flexible algorithmic template, inspired by Azizian et al. [3] and dubbed _extrapolated FTRL_ (FTRL+), which augments FTRL with a forward-looking, extrapolation step (including as special cases the optimistic and extra-step variants of FTRL, cf. Section4). We then establish the following results: 1. Under extrapolated FTRL, players are guaranteed constant individual regret (so, as a consequence, the players' empirical frequency of play converges to coarse correlated equilibrium at a rate of \(\mathcal{O}(1/T)\)).3 This should be contrasted with the results of [13, 14] who showed that players can achieve _polylogarithmic_ regret in any game (finite or convex). 2. The induced trajectory of play converges to Nash equilibrium from any initial condition. Footnote 3: We clarify here that “constant” refers to the horizon \(T\) of play; the dependence on the number of actions may be logarithmic or worse (depending on the specific regularized learning scheme employed by the players).

Our results aim to provide an in-depth understanding of no-regret learning in harmonic games, nesting prior work on 2-player zero-sum games - from Poincare recurrence [41, 48] to constant regret [27] and convergence under optimistic / extra-gradient schemes [11, 15, 18, 42, 62]. In partiucular, at a high level, our results show that harmonic games are the canonical complement of potential games, not only from a strategic, but also from a dynamic, learning viewpoint.

## 2 Preliminaries

**2.1**.: **Preliminaries on finite games.** Throughout the sequel, we will work with _finite games in normal form_. Formally, such games consist of (\(i\)) a finite set of _players_\(i\in\mathcal{N}\equiv\{1,\ldots,N\}\); (\(ii\)) a finite set of _actions_\(\mathcal{A}_{i}\) per player \(i\in\mathcal{N}\); and (\(iii\)) an ensemble of _payoff functions_\(u_{i}\colon\prod_{j}\mathcal{A}_{j}\to\mathds{R}\), each determining the reward \(u_{i}(\alpha)\) of player \(i\in\mathcal{N}\) in a given action profile \(\alpha=(\alpha_{1},\ldots,\alpha_{N})\). Putting everything together, we will write \(\mathcal{A}\coloneqq\prod_{i}\mathcal{A}_{i}\) for the game's _action space_ and \(\Gamma\equiv\Gamma(\mathcal{N},\mathcal{A},u)\) for the game with primitives as above.

During play, each player selects an action according to some _mixed strategy_, that is, a probability distribution \(x_{i}\) over \(\mathcal{A}_{i}\) which assigns probability \(x_{i\alpha_{i}}\) to \(\alpha_{i}\in\mathcal{A}_{i}\). In a slight abuse of notation, if \(x_{i}\) assigns all probability mass to some action \(\alpha_{i}\in\mathcal{A}_{i}\) (that is, \(x_{i\alpha_{i}}=1\)), we will identify \(x_{i}\) with \(\alpha_{i}\) and we will call it _pure_. We will also write \(\mathcal{X}_{i}\coloneqq\Delta(\mathcal{A}_{i})\subseteq\mathds{R}^{\mathcal{ A}_{i}}\) for the mixed strategy space of player \(i\), \(x=(x_{1},\ldots,x_{N})\) for the _strategy profile_ collecting the strategies of all players, and \(\mathcal{X}\coloneqq\prod_{i}\mathcal{X}_{i}\) for the game's _strategy space_.

The _mixed payoff_ of player \(i\) under a mixed strategy profile \(x\in\mathcal{X}\) may then be written as

\[u_{i}(x)=\mathds{E}_{\alpha\sim x}[u_{i}(\alpha)]=\sum_{\alpha\in\mathcal{A}}u _{i}(\alpha)\,x_{\alpha}=\sum_{\alpha_{i}\in\mathcal{A}_{i}}u_{i}(\alpha_{i}; x_{-i})\,x_{i\alpha_{i}} \tag{1}\]

where \(x_{\alpha}\coloneqq\prod_{i}x_{i\alpha_{i}}\) denotes the joint probability of \(\alpha=(\alpha_{1},\ldots,\alpha_{N})\in\mathcal{A}\) under \(x\in\mathcal{X}\), and, in standard game-theoretic notation, we write \((x_{i};x_{-i})=(x_{1},\ldots,x_{i},\ldots,x_{N})\) for the profile where player \(i\) plays \(x_{i}\in\mathcal{X}_{i}\) against the strategy \(x_{-i}\in\mathcal{X}_{-i}\coloneqq\prod_{j\neq i}\mathcal{X}_{j}\) of all other players. We also respectively define the _individual payoff field_ of player \(i\) and the _game's payoff field_ as

\[v_{i}(x)=(u_{i}(\alpha_{i};x_{-i}))_{\alpha_{i}\in\mathcal{A}_{i}}\quad\text{ and}\quad v(x)=(v_{1}(x),\ldots,v_{N}(x)) \tag{2}\]

so \(u_{i}(x)=\sum_{\alpha_{i}\in\mathcal{A}_{i}}v_{i\alpha_{i}}(x)x_{i\alpha_{i}} \equiv\langle v_{i}(x),x_{i}\rangle\), where \(\langle\cdot,\cdot\rangle\) is the standard duality pairing on \(\mathds{R}^{\mathcal{A}_{i}}\). By multilinearity, each player's individual payoff field is Lipschitz continuous on \(\mathcal{X}\), and we will write \(G_{i}\) for its Lipschitz modulus, that is

\[\|v_{i}(x^{\prime})-v_{i}(x)\|_{*}\leq G_{i}\|x^{\prime}-x\|\quad\text{for all }x,x^{\prime}\in\mathcal{X}. \tag{3}\]

_Remark_.: In the above and throughout, \(\|\cdot\|\) denotes an ambient norm on \(\mathds{R}^{\mathcal{A}_{i}}\) (usually the \(L^{1}\) norm), and \(\|\cdot\|_{*}\) is the corresponding dual norm (usually the \(L^{\infty}\) norm). To simplify notation, we will not carry the player index \(i\) in \(\|\cdot\|\), and we will instead rely on the context to resolve any ambiguities.

In terms of solution concepts, we will focus almost exclusively on the notion of a _Nash equilibrium_ (NE), i.e., a strategy profile \(x^{*}\in\mathcal{X}\) that is unilaterally stable in the sense that

\[u_{i}(x^{*})\geq u_{i}(x_{i};x^{*}_{-i})\quad\text{for all }x_{i}\in\mathcal{X}_{i},\,i\in\mathcal{N}\,.\] (NE)

Equivalently, (NE) can be expressed in terms of the game's payoff field as a variational inequality of the form

\[\langle v(x^{*}),x-x^{*}\rangle\leq 0\quad\text{for all }x\in\mathcal{X}\,.\] (VI)

Thus, writing \(\text{supp}(x^{*}_{i})=\{\alpha_{i}\in\mathcal{A}_{i}:x_{i\alpha_{i}}>0\}\) for the _support_ of \(x^{*}_{i}\), it follows that \(x^{*}\) is a Nash equilibrium if and only if \(u_{i}(\alpha_{i};x^{*}_{-i})\geq u_{i}(\beta_{i};x^{*}_{-i})\) for all \(\alpha_{i}\in\text{supp}(x^{*}_{i})\) and all \(\beta_{i}\in\mathcal{A}_{i}\), \(i\in\mathcal{N}\). We will use all this freely in the rest of our paper.

### Harmonic games

Our main focus in what follows will be the class of _harmonic games_, first introduced by Candogan et al. [6] as a game-theoretic model for strategic situations with conflicting, anti-aligned interests. Specifically, as was shown by Candogan et al. [6] - and, in a more general setting, by Abdou et al. [1] - every game in normal form can be decomposed as the sum of a potential game and a harmonic game, and this decomposition is unique up to affine transformations that do not alter the equilibrium outcomes of the game.4 In this decomposition, the potential component of a game captures multi-agent strategic interactions with _common_ interests, whereas the harmonic component covers interactions with _conflicting_ interests.5

Footnote 4: We briefly recall here that \(\Gamma\equiv\Gamma(\mathcal{N},\mathcal{A},u)\) is a potential game if it admits a _potential function_\(\phi\colon\mathcal{X}\to\mathbb{R}\) such that \(u_{i}(\beta_{i};\alpha_{-i})-u_{i}(\alpha_{i};\alpha_{-i})=\phi(\beta_{i}; \alpha_{-i})-\phi(\alpha_{i};\alpha_{-i})\) for all \(\alpha,\beta\in\mathcal{A}\) and all \(i\in\mathcal{N}\)[44].

Footnote 5: The terminology “harmonic” is due to Candogan et al. [6] and alludes to the harmonic component of the graphical Hodge decomposition [30].

Formally, adapting the more general setup by Abdou et al. [1], we have the following definition:

**Definition 1**.: A finite game \(\Gamma\equiv\Gamma(\mathcal{N},\mathcal{A},u)\) is said to be _harmonic_ when it admits a _harmonic measure_, i.e., a collection of weights \(\mu_{i,\alpha_{i}}\in(0,\infty)\), \(\alpha_{i}\in\mathcal{A}_{i},i\in\mathcal{N}\), such that

\[\sum_{i\in\mathcal{N}}\sum_{\beta_{i}\in\mathcal{A}_{i}}\mu_{\beta_{i}}\left[ u_{i}(\alpha_{i};\alpha_{-i})-u_{i}(\beta_{i};\alpha_{-i})\right]=0\quad\text{for all }\alpha\in\mathcal{A}\,.\] (HG)

In particular, if \(\Gamma\) is harmonic relative to the uniform measure \(\mu_{i\alpha_{i}}=1\), \(\alpha_{i}\in\mathcal{A}_{i},i\in\mathcal{N}\), we will say that \(\Gamma\) is a _uniform harmonic game_ (UHG). \(\diamond\)

_Remark_.: With regard to terminology, Candogan et al. [6] call "harmonic games" what we call "uniform harmonic games", and Abdou et al. [1] call "\(\mu\)-harmonic games" what we call "harmonic games".6 We use this convention because it simultaneously simplifies notation and terminology while capturing all relevant strategic features of the game; for a detailed discussion, see Appendix A. To avoid needless repetition, and unless there is a danger of confusion, when we say that \(\Gamma\) is harmonic, we will write \(\mu_{i}\) for the corresponding measure, and we will write \(m_{i}=|\mu_{i}|=\sum_{\beta_{i}\in\mathcal{A}_{i}}\mu_{i\beta_{i}}\) for the total mass of \(\mu_{i}\). \(\diamond\)

Footnote 6: To be even more precise, the definition of Abdou et al. [1] involves an additional set of weights, called a _comeasure_; however, as we explain in Appendix A, these weights do not change the preference structure of the game, so we disregard this extra degree of generality.

Broadly speaking, in harmonic games, for any player considering a deviation toward a specific pure strategy profile, there exist other players with an incentive to deviate _away_ from said profile. In this regard, harmonic games can be seen as the strategic complement of potential games, where player interests are aligned and sequences of unilateral best responses generate a finite improvement path that terminates at a pure Nash equilibrium [44]. By contrast, except for trivial cases (like the zero game) harmonic games _do not_ admit pure Nash equilibria, and they possess non-terminating best-response paths. For all these reasons, harmonic games can be considered as "orthogonal" to potential games, in a sense made precise by the decomposition results of Candogan et al. [6] and Abdou et al. [1].

It is of course natural to ask what is the relation between harmonic games and zero-sum games. Games belonging to the latter class - such as Matching Pennies and Rock-Paper-Scissors - have long been used as prototypical examples of strategic conflict; at the same time, there are zero-sum games that are also potential (and even possess strict equilibria), so the potential / zero-sum distinction does not capture the whole picture. As a matter of fact, it is not a coincidence that the textbook examples of zero-sum games admit fully-mixed Nash equilibria: as we discuss in Appendix A, two-player zero-sum games with a fully mixed Nash equilibrium are harmonic, so the existing results for such games are, in a sense, more closely attuned to their harmonic character.

Continuous-time analysis: Poincare recurrence

The most basic rationality postulate in the context of online learning is the minimization of a player's (external) regret, i.e., the difference between a player's cumulative payoff and that of the player's best possible strategy in hindsight. In more detail, assuming for the moment that play evolves in continuous time, the _regret_ of player \(i\in\mathcal{N}\) relative to a sequence of play \(x(t)\in\mathcal{X}\) is defined as

\[\text{Reg}_{i}(T)=\max_{p_{i}\in\mathcal{X}_{i}}\int_{0}^{T}\left[u_{i}(p_{i};x _{-i}(t))-u_{i}(x(t))\right]\,dt \tag{4}\]

and we say that the player has _no regret_ under \(x(t)\) if \(\text{Reg}_{i}(T)=o(T)\) as \(T\rightarrow\infty\).

The most widely used scheme for attaining no regret is the family of policies known as _follow-the-regularized-leader_ (FTRL) [57, 58]. At a high level, the idea behind FTRL is that, at all times \(t\geq 0\), each player \(i\in\mathcal{N}\) plays a mixed strategy \(x_{i}(t)\in\mathcal{X}_{i}\) that maximizes the player's cumulative payoff up to time \(t\) minus a certain regularization penalty. In our continuous-time setting, this gives rise to the FTRL dynamics

\[x_{i}(t)=\operatorname*{arg\,max}_{p_{i}\in\mathcal{X}_{i}}\left\{\int_{0}^{t }u_{i}(p_{i};x_{-i}(\tau))\;d\tau-h_{i}(p_{i})\right\}=\operatorname*{arg\, max}_{p_{i}\in\mathcal{X}_{i}}\left\{\int_{0}^{t}\langle v_{i}(x(\tau)),p_{i} \rangle\;d\tau-h_{i}(p_{i})\right\} \tag{5}\]

or, more compactly,

\[\dot{y}_{i}(t)=v_{i}(x(t))\qquad x_{i}(t)=Q_{i}(y_{i}(t))\] (FTRL-D)

where \(h_{i}\colon\mathcal{X}_{i}\rightarrow\mathbb{R}\) is a convex penalty function known as the _regularizer_ of the method, \(Q_{i}\) denotes the _regularized choice map_ of player \(i\), and \(Q=(Q_{1},\ldots,Q_{N})\) denotes the profile thereof. Formally, writing \(\mathcal{Y}_{i}\equiv\mathbb{R}^{A_{i}}\) for the _payoff space_ of player \(i\in\mathcal{N}\) - that is, the space of all possible payoff vectors \(v_{i}\) of player \(i\) - the regularized choice map \(Q_{i}\colon\mathcal{Y}_{i}\rightarrow\mathcal{X}_{i}\) is defined as

\[Q_{i}(y_{i})=\operatorname*{arg\,max}_{x_{i}\in\mathcal{X}_{i}}\left\{(y_{i}, x_{i})-h_{i}(x_{i})\right\}\quad\text{for all }y_{i}\in\mathcal{Y}_{i}\;. \tag{6}\]

In essence, \(Q_{i}\) is a "soft" version of the \(\operatorname*{arg\,max}\) correspondence \(y_{i}\mapsto\operatorname*{arg\,max}_{x_{i}\in\mathcal{X}_{i}}\langle y_{i},x _{i}\rangle\), suitably regularized by a penalty term intended to incentivize exploration. For technical reasons, we will also assume that each \(h_{i}\) is _strongly convex_, i.e.,

\[h_{i}(tx_{i}+(1-t)x_{i}^{\prime})\leq th_{i}(x_{i})+(1-t)h_{i}(x_{i}^{\prime}) -\tfrac{1}{2}K_{i}t(1-t)\|x_{i}-x_{i}^{\prime}\|^{2} \tag{7}\]

for some \(K_{i}>0\) (commonly referred to as the _strong convexity modulus_ of \(h_{i}\)), and for all \(x_{i},x_{i}^{\prime}\in\mathcal{X}\), \(t\in[0,1]\). In plain words, this simply means that \(h_{i}\) has "enough curvature" in the sense that it can be bounded from below by a (positive) quadratic function which agrees with \(h_{i}\) to first order.

The go-to example of this setup is the entropic regularizer

\[h_{i}(x_{i})=\sum_{\alpha_{i}\in\mathcal{A}_{i}}x_{i}\alpha_{i}\log x_{i} \alpha_{i} \tag{8}\]

which yields the so-called _logit choice map_

\[Q_{i}(y_{i})\equiv\Lambda_{i}(y_{i})\coloneqq\frac{(\exp(y_{i}\alpha_{i}))_{ \alpha_{i}\in\mathcal{A}_{i}}}{\sum_{\alpha_{i}\in\mathcal{A}_{i}}\exp(y_{i} \alpha_{i})}\quad\text{for all }y_{i}\in\mathcal{Y}_{i}. \tag{9}\]

By Pinsker's inequality, the entropic regularizer is \(1\)-strongly convex relative to the \(L^{1}\)-norm on \(\mathcal{X}_{i}\)[57], and by a standard calculation [37, 54], the induced sytem (FTRL-D) boils down to the replicator dynamics of Taylor & Jonker [60]. Some other standard examples of (FTRL-D) include the Euclidean projection dynamics of Friedman [17] when \(h_{i}(x_{i})=(1/2)\|x_{i}\|_{2}^{2}\), the \(q\)-replicator dynamics [21, 38], etc. To streamline our presentation, we defer a detailed discussion of these examples to Appendix C, and we proceed below to state the main regret guarantee of (FTRL-D), originally due to [33]:

**Theorem 1**.: _Under (FTRL-D), each player's regret is bounded as \(\text{Reg}_{i}(T)\leq H_{i}\coloneqq\max h_{i}-\min h_{i}\)._

Theorem 1 showcases the strong no-regret properties of (FTRL-D): it is not possible to guarantee less than constant, \(\mathcal{O}(1)\) regret, so (FTRL-D) is optimal in this regard. In turn, by standard results [47], Theorem 1 implies further that the players' (correlated) empirical frequencies \(z_{\alpha_{1},\ldots,\alpha_{N}}(t)\coloneqq(1/t)\int_{0}^{t}\prod_{i}x_{i} \alpha_{i}(\tau)\;d\tau\) converge to the game's set of coarse correlated equilibria (CCE) at a rate of \(\mathcal{O}(1/t)\).

Importantly, this result makes no assumptions about the underlying game, but it does not carry the same predictive power in all games: for one thing, a game's set of CCE may include highly non-rationalizable outcomes (such as dominated strategies and the like) [61]; for another, the time-averaging that is inherent in the definition of empirical distributions may conceal a wide range of non-convergence phenomena, from cycles to chaos [48; 56]. On that account, the day-to-day behavior of (FTRL-D) in harmonic games cannot be understood from Theorem 1 alone, and requires a closer, more in-depth look.

Our first result below provides such a lense and shows that (FTRL-D) is almost-periodic in harmonic games, a property known as _Poincare recurrence._

**Theorem 2**.: _Suppose \(\Gamma\) is harmonic. Then almost every orbit \(x(t)\) of (FTRL-D) returns arbitrarily close to its starting point infinitely often: specifically, for (Lebesgue) almost every initial condition \(x(0)=Q(y(0))\in\mathcal{X}\), there exists an increasing sequence of times \(t_{n}\uparrow\infty\) such that \(x(t_{n})\to x(0)\)._

An immediate consequence of Theorem 2 is that no-regret learning under (FTRL-D) fails to converge in _any_ harmonic game; in particular, since the orbits of (FTRL-D) eventually return to (almost) where they started, it is debatable if the players have learned anything at all, despite the fact that they incur at most constant regret. This cyclic, non-convergent landscape is the polar opposite of the long-run behavior of (FTRL-D) in _potential_ games, where the dynamics are known to converge globally [24]. Thus, in addition to the strategic viewpoint of the previous section, Theorem 2 shows that harmonic games are orthogonal to potential games also from a _dynamic_ viewpoint.

Theorem 2 also provides a far-reaching generalization of existing results on Poincare recurrence in (possibly networked) two-player zero-sum games with an interior equilibrium [41] to general-sum, \(N\)-player games. Combined with our previous remark, and given that the zero-sum property is not as meaningful for \(N\) players as it is for two,7 the class of harmonic games can be seen as the more natural \(N\)-player generalization of two-player zero-sum games from a learning viewpoint.

Footnote 7: Recall that any \(N\)-player game can be turned into an equivalent zero-sum game by adding a fictitious player.

To the best of our knowledge, the only comparable result to Theorem 2 in the literature is the very recent paper of Legacci et al. [35] who showed that the replicator dynamics - a special case of (FTRL-D) - are Poincare recurrent in _uniform_ harmonic games, that is, in harmonic games where the uniform distribution is a Nash equilibrium, cf. (A.1) and the discussion surrounding Definition 1. In this regard, Theorem 2 extends the recent results of Legacci et al. [35] along two axes: (_i_) it applies to the entire class of FTRL dynamics (not only the replicator dynamics); and (_ii_) it applies to the entire class of harmonic games (and not only _uniformly_ harmonic games).

In terms of techniques, Legacci et al. [35] obtained their result through a surprising connection between a certain Riemannian metric underlying the replicator dynamics and the defining relation of uniformly harmonic games. This relation no longer holds for different instances of (FTRL-D) or for non-uniform harmonic games, so the techniques of [35] cannot be extended - and, in fact, Legacci et al. [35] stated this generalization as an open problem. Our techniques instead rely on the fact that the orbits \(y(t)\) of (FTRL-D) comprise a volume-preserving flow in the game's payoff space \(\mathcal{Y}\equiv\prod_{i}\mathcal{Y}_{i}\) (though not necessarily on \(\mathcal{X}\)), and then deriving a suitable constant of motion. In the case of the logit map (9), this constant of motion can be written as

\[G(x)=\prod_{i\in\mathcal{N}}\prod_{\alpha_{i}\in\mathcal{A}_{i}}x^{\mu_{ \alpha_{i}}}_{\alpha_{i}}\qquad\text{for all }x\in\mathcal{X}, \tag{10}\]

where \(\mu=(\mu_{i\alpha_{i}})_{\alpha_{i}\in\mathcal{A}_{i},i\in\mathcal{N}}\) is the harmonic measure on \(\mathcal{X}\) defining \(\Gamma\). In the more general case, the construction of a constant of motion for (FTRL-D) involves a characterization of harmonic games in terms of a "strategic center", which we carry out in detail in Appendix C.

## 4 Discrete-time analysis: Convergence and constant regret via extrapolation

We now proceed to examine the regret and convergence properties of regularized learning algorithms in harmonic games. Starting with the standard, vanilla implementation of FTRL, we reproduce a well-known observation that FTRL spirals out to a non-terminating cycle of best-responses in Matching Pennies (which is a harmonic game). Subsequently, to correct this non-convergent behavior, we examine a flexible algorithmic template, which we call _extrapolated FTRL_ (FTRL+), and which includes as special cases the optimistic and extra-gradient versions of FTRL.

### Vanilla implementation of FTRL

Building on the discussion of the previous section, the standard implementation of FTRL in discrete time for \(n=1,2,\ldots\) is

\[x_{i,n+1}=\operatorname*{arg\,max}_{p_{i}\in\mathcal{X}_{i}}\bigl{\{}\sum_{k=1}^ {n}u_{i}(p_{i};x_{-i,n})-\lambda_{i}h_{i}(p_{i})\bigr{\}}=\operatorname*{arg\, max}_{p_{i}\in\mathcal{X}_{i}}\bigl{\{}\sum_{k=1}^{n}\langle v_{i}(x_{k}),p_{i} \rangle-\lambda_{i}h_{i}(p_{i})\bigr{\}} \tag{11}\]

or, in more compact, iterative notation

\[y_{i,n+1}=y_{i,n}+\eta_{i}v_{i}(x_{n})\qquad x_{i,n}=Q_{i}(y_{i,n})\] (FTRL)

where, as per (6), the map \(Q_{i}\colon\mathcal{Y}_{i}\to\mathcal{X}_{i}\) denotes the _regularized choice map_ of player \(i\in\mathcal{N}\), \(\lambda_{i}\) is a player-specific regularization weight parameter, and \(\eta_{i}=1/\lambda_{i}\) represents the _learning rate_ of player \(i\). Apart from their obvious differences - discrete vs. continuous time - a salient point that sets (FTRL) apart from (FTRL-D) is the inclusion of the parameter \(\eta_{i}\); this parameter is necessary to control the algorithm's behavior, and we will discuss it in detail in the sequel.

As mentioned in the introduction, a major shortfall of (FTRL) - and one of the main reasons for the increased popularity of optimistic / extra-gradient methods - is that it may spiral away from Nash equilibrium, even in simple \(2\times 2\) games with a unique equilibrium. The standard example of this behavior is Matching Pennies, a two-player zero-sum game with a fully-mixed equilibrium which is also uniformly harmonic, so the trajectories of (FTRL-D) are Poincare recurrent (and, in fact, periodic). In more detail, this game can be compactly represented by the payoff field \(v(x_{1},x_{2})=(4x_{2}-2,2-4x_{1})\) for \(x_{1},x_{2}\in[0,1]\), and its unique Nash equilibrium is \(x^{*}=(1/2,\,1/2)\). Thus, if we run (FTRL) with a Euclidean regularizer - that is, \(h_{i}(x_{i})=x_{i}^{2}/2\) for \(i=1,2\) - and the same learning rate \(\eta\) for both players, a straightforward calculation shows that the distance \(D_{n}=(x_{1,n}-x_{1}^{*})^{2}/2+(x_{2,n}-x_{2}^{*})^{2}/2\) between \(x_{n}\) and \(x^{*}\) evolves as

\[D_{n+1}=\tfrac{1}{2}(x_{1,n}+\eta v_{1}(x_{n})-x_{1}^{*})^{2}+\tfrac{1}{2}(x_{2,n}+\eta v_{2}(x_{n})-x_{2}^{*})^{2}=(1+16\eta^{2})D_{n} \tag{12}\]

as long as \(x_{n}+\eta v(x_{n})\in\mathcal{X}\). In other words, the distance of the iterates of (FTRL) from the game's equilibrium grows at a geometric rate until \(x_{n}\) reaches the boundary of \(\mathcal{X}\) and is ultimately trapped in a non-terminating cycle of best responses, cf. Fig. 1. In this regard, the rationality properties of (FTRL) are even worse than those of (FTRL-D) because the game's equilibrium is now _repelling_.

### Extrapolated FTRL

To mitigate this undesirable, divergent behavior of (FTRL), a standard approach in the literature is the inclusion of a forward-looking, "_extrapolation step_". Instead of updating the algorithm's "base state" \(x_{n}\) directly, players first move to an interim "leading state" \(x_{n+1/2}\) using payoff information from \(x_{n}\) (this is the extrapolation step); subsequently, players update \(x_{n}\) using payoff information from the leading state \(x_{n+1/2}\), and the process repeats. In this way, players attempt to anticipate their payoff landscape and, in so doing, to take a more informed update step at each iteration.

The seed of this idea goes back to Korpelevich [32] and Popov [49] in the context of solving monotone variational inequality problems, and it has since percolated to a wide array of "_extra-gradient_" or "_optimistic_" methods, such as the mirror-prox algorithm of Nemirovski [45], the dual extrapolation variant of Nesterov [46], the optimistic mirror descent algorithm of Chiang et al. [9] and Rakhlin & Sridharan [50], and many others. Given the different operational envelope of each of these methods, we consider below an integrated algorithmic template, which we call _extrapolated FTRL_ (FTRL+), and which is sufficiently flexible to account for a broad range of these schemes.

Formally, the proposed algorithmic blueprint unfolds in two phases as follows:

* _Extrapolation phase:_ \[y_{i,n+1/2}=y_{i,n}+\eta_{i}\hat{v}_{i,n} x_{i,n+1/2}=Q_{i}(y_{i,n+1/2})\]
* _Update phase:_ \[y_{i,n+1}=y_{i,n}+\eta_{i}\hat{v}_{i,n+1/2} x_{i,n+1}=Q_{i}(y_{i,n+1})\] (FTRL+)

In the above, \(\eta_{i}>0\) is the learning rate of player \(i\), \(x_{n}\) and \(x_{n+1/2}\) denote respectively the method's _base_ and _leading_ states at stage \(n=1,2,\ldots\), and \(\hat{v}_{i,n}\) and \(\hat{v}_{i,n+1/2}\) are sequences of "black-box" payoff models at \(x_{n}\) and \(x_{n+1/2}\) respectively.

Specifically, following Azizian et al. [3], we will assume throughout that

\[\hat{v}_{i,n+1/2}=v_{i}(x_{n+1/2})\quad\text{for all $i\in\mathcal{N}$ and all $n=1,2,\ldots$}\] (13a) i.e., players always update the base state \[x_{n}\] using payoff information from the leading state \[x_{n+1/2}\]. By contrast, the leading state \[x_{n+1/2}\] can be generated in many different ways, depending on the targeted update structure. In this regard, we will consider the linear model \[\hat{v}_{i,n}=a_{i}\,v_{i}(x_{n})+b_{i}\,v_{i}(x_{n-1/2})\quad\text{for all $i\in\mathcal{N}$ and all $n=1,2,\ldots$} \tag{13b}\]where the player-specific coefficients \(a_{i},b_{i}\geq 0\) satisfy \(a_{i}+b_{i}\leq 1\) and represent a mix of past and present payoff information. In this way, depending on the values of \(a_{i}\) and \(b_{i}\), we obtain the following prototypical regularized learning methods as special cases of (FTRL+):

* _FTRL:_ if \(a_{i}=b_{i}=0\) for all \(i\in\mathcal{N}\), players essentially forego any look-ahead efforts, so we get \[\hat{v}_{n}=0\qquad\qquad\text{for all }n=1,2,\ldots\] (14a) In turn, this gives \(x_{n+1/2}=x_{n}\), i.e., (FTRL+) regresses to (FTRL).
* _Extra-Step FTRL:_ if \(a_{i}=1\) and \(b_{i}=0\) for all \(i\in\mathcal{N}\), we have \[\hat{v}_{n}=v(x_{n})\qquad\quad\text{for all }n=1,2,\ldots\] (14b) i.e., players use payoff information from their current state to generate the leading state \(x_{n+1/2}\). This update structure requires two payoff queries per iteration and its origins can be traced back to the work of Korpelevich [32]. Specifically, depending on the choice of \(h_{i}\), it is essentially equivalent to the mirror-prox [45] and dual extrapolation [46] algorithms, it contains as a special case the forward-looking algorithm of [15, 42], etc.
* _Optimistic FTRL:_ if \(a_{i}=0\) and \(b_{i}=1\) for all \(i\in\mathcal{N}\), we have \[\hat{v}_{n}=v(x_{n-1/2})\quad\text{for all }n=1,2,\ldots\] (14c) i.e., players reuse the latest available payoff information instead of making a fresh query at \(x_{n}\) (so the algorithm only requires one payoff query per iteration). In this way, (FTRL+) recovers the optimistic algorithms of [9, 26, 50, 59], the OMW update scheme of [11, 59] when \(Q=\Lambda\), etc.

Clearly, the list above is not exhaustive: many other configurations are possible, e.g., with different players using different parameter settings for \(a_{i}\) and \(b_{i}\), depending on the information they have at hand and any other individual considerations. To avoid needlessly complicating the analysis, our only standing assumption will be that \(a_{i}+b_{i}>0\) for all \(i\in\mathcal{N}\) (since, otherwise, the benefits of the extrapolation step would vanish). In particular, by rescaling the players' learning rates if needed, we will normalize \(a_{i}\) and \(b_{i}\) to \(a_{i}+b_{i}=1\), leading to the convex model

\[\hat{v}_{i,n}=\lambda_{i}\,v_{i}(x_{n})+(1-\lambda_{i})\,v_{i}(x_{n-1/2}) \tag{15}\]

for some arbitrarily chosen ensemble of player-specific _extrapolation coefficients \(\lambda_{i}\in[0,1]\)_, \(i\in\mathcal{N}\)_.

_Remark_.: To simplify the presentation of our results, we will assume throughout the rest of our paper that (FTRL+) is initialized with \(y_{1}=y_{1/2}=0\).

### Analysis & results

With all this in hand, we are finally in a position to state our main results for (FTRL+) in harmonic games. We begin by showing that (FTRL+) achieves order-optimal regret:

**Theorem 3**.: _Suppose that each player in a harmonic game \(\Gamma\) is following (FTRL+) with learning rate \(\eta_{i}\leq m_{i}K_{i}\,[2(N+2)\max_{j}m_{j}G_{j}]^{-1}\) and payoff models as per (13a) and (15). Then the individual regret of each player \(i\in\mathcal{N}\) is bounded as_

\[\operatorname{Reg}_{i}(T)\coloneqq\max_{p_{i}\in\mathcal{N}_{i}}\sum_{n=1}^{T }[u_{i}(p_{i};x_{-i,n})-u_{i}(x_{n})]\leq\frac{H_{i}}{\eta_{i}}+\frac{2G_{i}}{ N+2}\sum_{j\in\mathcal{N}}\frac{H_{j}}{\eta_{j}G_{j}} \tag{16}\]

_where \(H_{i}=\max h_{i}-\min h_{i}\), and \(G_{i}\) is the Lipschitz modulus of \(v_{i}\)._

Even though Theorem 3 invites a natural comparison with the constant regret bound of Theorem 1, the continuous- and discrete-time settings are fundamentally different, so any conclusions drawn from such a comparison would be specious. Indeed, constant regret guarantees in the spirit of (16) are particularly rare in the context of discrete-time algorithms, and as far as we are aware, similar bounds have only been established for optimistic methods in variationally stable and two-player zero-sum games [27]; other than that - and always to the best of our knowledge - the tightest regret bounds available for general games (finite or convex) seem to be (poly)logarithmic [13, 14]. In this regard, just like the recurrence result of Theorem 2, the \(\mathcal{O}(1)\) regret bound of Theorem 3 represents a significant extension of existing results on zero-sum games (and polylogarithmic regret in general games), and suggests that, from a learning viewpoint, harmonic games are the most natural generalization of two-player zero-sum games to a general \(N\)-player context. We defer the proof of Theorem 3 to Appendix D.

As an immediate corollary of the above, we conclude that, under (FTRL+), the empirical frequencies of play \(z_{\alpha,n}\coloneqq(1/n)\sum_{k=1}^{n}x_{\alpha,k}\), \(\alpha\in\mathcal{A}\), converge to the game's set of CCE at a rate of \(\mathcal{O}(1/n)\). This rate is, again, optimal, but as we discussed in Section 3, it offers little information in games where the marginalization of CCE does not lead to Nash equilibrium - and, in general \(N\)-player harmonic games, there is little hope that it would. In addition, even when the marginalization of CCE is Nash, the actual trajectory of play may - and, in fact, often _does_ - behave very differently from the time-averaged frequency of play.

Despite these hurdles, we show below that (FTRL+) _does_ converge to Nash equilibrium. To state this result formally, we will focus on the case where each player's regularizer is _smooth_ in the sense that

\[h_{i}(x_{i}+t(x^{\prime}_{i}-x_{i}))\text{ is continuously differentiable at }t=0 \tag{17}\]

for all \(x_{i}\in\operatorname{im}Q_{i}\) and all \(x^{\prime}_{i}\in\mathcal{X}_{i}\).8 Our prototypical examples - the entropic and Euclidean regularizers - both satisfy this mild requirement, as do all regularizers of the form \(h_{i}(x_{i})=\sum_{\alpha_{i}\in\mathcal{A}}\theta_{i}(x_{i\alpha_{i}})\) for some smooth convex function \(\theta_{i}\colon[0,1]\to\mathbb{R}\). We then have the following convergence result:

Footnote 8: The restriction to \(\operatorname{im}Q_{i}\) is technical in nature and is related to the subdifferentiability of \(h_{i}\), cf. Appendix B.

**Theorem 4**.: _Suppose that each player in a harmonic game \(\Gamma\) follows (FTRL+) with learning rate \(\eta_{i}\leq m_{i}K_{i}[2(N+2)\max_{j}m_{j}G_{j}]^{-1}\) and payoff models as per (13a) and (15). Then \(x_{n}\) converges to the set of Nash equilibria of \(\Gamma\)._

To the best of our knowledge, Theorem 4 is the first result of its kind for harmonic games - and, in that regard, it is somewhat unexpected. To be sure, two-player zero-sum games with a fully-mixed equilibrium exhibit a comparable pattern: FTRL is Poincare recurrent in continuous time, its vanilla discretization is unstable, and its optimistic / forward-looking implementation is convergent. However, the convex-concave structure of min-max games which enables this analysis is completely absent in harmonic games, so it is less clear what to expect in this case (where even the set of Nash equilibria is non-convex, cf. Fig. 1). By this token, the convergence of (FTRL+) in harmonic games is a property that one could optimistically hope for, but not one that can be taken for granted.

From a technical standpoint, the proof of Theorems 3 and 4 involves two concurrent challenges:

1. Deriving a Lyapunov function with a "sufficient descent" property for all harmonic games.
2. Providing an integrated analysis for all possible update structures in (FTRL+).

With regard to the first point, our analysis hinges on the "energy function"

\[E(p,y)=\sum_{i\in\mathcal{N}}\frac{m_{i}}{\eta_{i}}F_{i}(p_{i},y_{i})\qquad p \in\mathcal{X},y\in\mathcal{Y}, \tag{18}\]

In the above, \(p\in\mathcal{X}\) is a benchmark strategy profile acting as a "reference point" for the analysis while

\[F_{i}(p_{i},y_{i})=\max_{x_{i}\in\mathcal{X}_{i}}\{\langle y_{i},x_{i}\rangle- h_{i}(x_{i})\}-[\langle y_{i},p_{i}\rangle-h_{i}(p_{i})] \tag{19}\]

Figure 1: The evolution of vanilla vs. extrapolated FTRL schemes in harmonic games. In the left figure, we consider the game of Matching Pennies (blue: FTRL+; green: FTRL; red: continuous time FTRL); in the center and to the right, two different orbits in a \(2\times 2\times 2\) harmonic game from two different viewpoints (blue: FTRL+; green/orange:FTRL; payoff profiles on vertices). In all cases, we ran the optimistic variant of FTRL+ (\(\lambda_{i}=0\) for all players), and we see that the trajectories of (FTRL) diverge away from equilibrium and the trajectories of (FTRL-D) are recurrent (actually, periodic), whereas (FTRL+) converges. We also see the highly non-convex structure of harmonic games as evidence by their equilibrium set (thick red line in center and right subfigures).

denotes the _Fenchel coupling_ associated to the regularizer \(h_{i}\) of player \(i\in\mathcal{N}\), and represents a "primal-dual" measure of divergence between \(p_{i}\in\mathcal{X}_{i}\) and \(y_{i}\in\mathcal{Y}_{i}\) (for an in-depth discussion, see Appendices B and D). Then, letting \(E_{n}=E(p,y_{n})\), the heavy lifting for our analysis is provided by the "template inequality"

\[E_{n+1}\leq E_{n} +\sum_{i\in\mathcal{N}}m_{i}\langle v_{i}(x_{n+1/2}),x_{i,n+1/2}-p _{i}\rangle\] \[+\sum_{i\in\mathcal{N}}m_{i}\langle v_{i}(x_{n+1/2})-v_{i}(x_{n}),x_{i,n+1}-x_{i,n+1/2}\rangle\] \[+\sum_{i\in\mathcal{N}}m_{i}(1-\lambda_{i})\langle v_{i}(x_{n})-v _{i}(x_{n-1/2}),x_{i,n+1}-x_{i,n+1/2}\rangle\] \[-\sum_{i\in\mathcal{N}}\frac{m_{i}K_{i}}{\eta_{i}}\big{[}\|x_{i,n +1}-x_{i,n+1/2}\|^{2}+\|x_{i,n+1/2}-x_{i,n}\|^{2}\big{]}\;. \tag{20}\]

A first important consequence of (20) is that the sequences \(A_{n}=\|x_{n+1}-x_{n+1/2}\|^{2}\) and \(B_{n}=\|x_{n+1/2}-x_{n}\|^{2}\) are both summable: this requires a repeated use of the Fenchel-Young inequality, and an instantiation of \(p\) to the strategic center \(q\) of \(\Gamma\); we detail the relevant arguments in Appendices A and D. Then, by establishing a similar template inequality for _each_ player \(i\in\mathcal{N}\), we are able to bound the players' individual regret by the same upper bound that we derived for \(\sum_{n}A_{n}\) and \(\sum_{n}B_{n}\), and which is (up to certain secondary factors) the bound (16).

For the convergence to Nash equilibrium, the summability argument above also plays a crucial role. First, by a standard result on numerical sequences, the summability of \(A_{n}\) and \(B_{n}\) coupled with the template inequality (20) implies that the energy \(E_{n}\) of the algorithm relative to the game's strategic center converges to some limit value \(E_{\infty}\). In turn, this implies that the score sequence \(y_{n}\) is bounded up to a multiple of the vector \((1,\ldots,1)\), which corresponds to a constant payoff shift in the underlying game. Then, by focusing on convergent subsequences of \(y_{n}\) and the optimality condition resulting from the definition of \(Q\), we are able to show that any limit point of \(v(x_{n})\) satisfies the variational characterization (VI) of Nash equilibria, from which our claim follows.

## 5 Concluding remarks

Our results suggest that the long-run behavior of no-regret algorithms and dynamics in harmonic games is a very rich topic, and one which opens the door to an entirely new class of games where positive convergence results can be obtained. We find this particularly appealing, not only because harmonic games comprise the strategic complement of potential games, but also because they go beyond standard problems with a convex structure - for instance, even their equilibrium set is non-convex. As such, the fact that it is possible to obtain optimal regret guarantees and positive equilibrium convergence results in this setting is very promising for future work on the topic.

In terms of open questions, it would be important to examine the rate of convergence of (FTRL+) to equilibrium. Even though (FTRL+) has order-optimal regret bounds, this only helps in establishing a convergence rate to the game's set of coarse correlated equilibria; for Nash equilibria, earlier work by Golowich et al. [19] and some more recent results by Cai et al. [5] and Gorbunov et al. [20] have shed some light on the convergence of constrained Euclidean optimistic methods, but the technology therein does not extend to non-monotone, non-Euclidean problems. Inspired by Wei et al. [62], we conjecture that the convergence rate of (FTRL+) in harmonic games is linear: this is based on the observation that any harmonic game admits a fully-mixed Nash equilibrium, and the weighted sum in the definition of a harmonic game looks formally similar to the condition needed to establish metric subregularity in [62]; however, a proof would likely require different techniques.

Another important research direction has to do with the information available to the players. A first open question here concerns the case where players do not have access to full information on their mixed payoff vectors, but can only observe their pure payoffs - either in a "what if", counterfactual manner, or in the form of bandit, payoff-based feedback. In a similar manner, the algorithms presented here are not adaptive, in the sense that the players' step-size policy has to satisfy a certain bound that depends on correctly estimating some of the game's parameters. Obtaining an adaptive version of (FTRL+) which, in the spirit of Rakhlin & Sridharan [50] and Hsieh et al. [27; 28; 29], remains convergent and attains order-optimal regret in both adversarial and game-theoretic settings without any pre-play tuning is also an ambitious question for future research.

## Acknowledgments and Disclosure of Funding

This research was supported in part by the French National Research Agency (ANR) in the framework of the PEPR IA FOUNDRY project (ANR-23-PEIA-0003), the "Investissements d'avenir program" (ANR-15-IDEX-02), the LabEx PERSYVAL (ANR-11-LABX-0025-01), MIAI@Grenoble Alpes (ANR-19-P3IA-0003), the project IRGA2024-SPICE-G7H-IRG24E90, and NSF grant CCF2212233. PM is also with the Archimedes Research Unit/Athena RC & NKUA and acknowledges financial support by project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program.

## References

* [1] Abdou, J., Pnevmatikos, N., Scarsini, M., and Venel, X. Decomposition of games: Some strategic considerations. _Mathematics of Operations Research_, 47(1):176-208, February 2022.
* [2] Arnold, V. I. _Mathematical Methods of Classical Mechanics_. Springer-Verlag, 1989.
* [3] Azizian, W., Iutzeler, F., Malick, J., and Mertikopoulos, P. The rate of convergence of Bregman proximal methods: Local geometry vs. regularity vs. sharpness. _SIAM Journal on Optimization_, 34(3):2440-2471, September 2024.
* [4] Bertsekas, D. P. _Convex optimization algorithms_. Athena Scientific, 2015.
* [5] Cai, Y., Oikonomou, A., and Zheng, W. Tight last-iterate convergence of the extragradient and the optimistic gradient descent-ascent algorithm for constrained monotone variational inequalities. 2022.
* [6] Candogan, O., Menache, I., Ozdaglar, A., and Parrilo, P. A. Flows and decompositions of games: Harmonic and potential games. _Mathematics of Operations Research_, 36(3):474-503, 2011.
* [7] Chen, G. and Teboulle, M. Convergence analysis of a proximal-like minimization algorithm using Bregman functions. _SIAM Journal on Optimization_, 3(3):538-543, August 1993.
* [8] Chen, X. and Peng, B. Hedging in games: Faster convergence of external and swap regrets. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 18990-18999. Curran Associates, Inc., 2020.
* [9] Chiang, C.-K., Yang, T., Lee, C.-J., Mahdavi, M., Lu, C.-J., Jin, R., and Zhu, S. Online optimization with gradual variations. In _COLT '12: Proceedings of the 25th Annual Conference on Learning Theory_, 2012.
* [10] Combettes, P. L. Quasi-Fejerian analysis of some optimization algorithms. In Butnariu, D., Censor, Y., and Reich, S. (eds.), _Inherently Parallel Algorithms in Feasibility and Optimization and Their Applications_, pp. 115-152. Elsevier, New York, NY, USA, 2001.
* [11] Daskalakis, C. and Panageas, I. Last-iterate convergence: Zero-sum games and constrained min-max optimization. In _ITCS '19: Proceedings of the 10th Conference on Innovations in Theoretical Computer Science_, 2019.
* [12] Daskalakis, C., Goldberg, P. W., and Papadimitriou, C. H. The complexity of computing a Nash equilibrium. _Communications of the ACM_, 52(2):89-97, 2009.
* [13] Daskalakis, C. C., Fishelson, M., and Golowich, N. Near-optimal no-regret learning in general games. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), _Advances in Neural Information Processing Systems_, 2021.
* [14] Farina, G., Anagnostides, I., Luo, H., Lee, C.-W., Kroer, C., and Sandholm, T. Near-optimal no-regret learning dynamics for general convex games. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), _Advances in Neural Information Processing Systems_, 2022.
* [15] Fasoulakis, M., Markakis, E., Pantazis, Y., and Varsos, C. Forward looking best-response multiplicative weights update methods for bilinear zero-sum games. In Camps-Valls, G., Ruiz, F. J. R., and Valera, I. (eds.), _Proceedings of The 25th International Conference on Artificial Intelligence and Statistics_, volume 151 of _Proceedings of Machine Learning Research_, pp. 11096-11117. PMLR, 28-30 Mar 2022.
* [16] Flokas, L., Vitadakis-Gkaraagkounis, E. V., Lianeas, T., Mertikopoulos, P., and Piliouras, G. No-regret learning and mixed Nash equilibria: The do not mix. In _NeurIPS '20: Proceedings of the 34th International Conference on Neural Information Processing Systems_, 2020.
* [17] Friedman, D. Evolutionary games in economics. _Econometrica_, 59(3):637-666, 1991.
* [18] Gidel, G., Berard, H., Vignoud, G., Vincent, P., and Lacoste-Julien, S. A variational inequality perspective on generative adversarial networks. In _ICLR '19: Proceedings of the 2019 International Conference on Learning Representations_, 2019.
* [19] Golowich, N., Pattathil, S., and Daskalakis, C. Tight last-iterate convergence rates for no-regret learning in multi-player games. In _NeurIPS '20: Proceedings of the 34th International Conference on Neural Information Processing Systems_, 2020.

* [20] Gorbunov, E., Taylor, A., and Gidel, G. Last-iterate convergence of optimistic gradient method for monotone variational inequalities. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), _Advances in Neural Information Processing Systems_, 2022.
* [21] Harper, M. Escort evolutionary game theory. _Physica D: Nonlinear Phenomena_, 240(18):1411-1415, September 2011.
* [22] Hart, S. and Mas-Colell, A. A simple adaptive procedure leading to correlated equilibrium. _Econometrica_, 68(5):1127-1150, September 2000.
* [23] Hart, S. and Mas-Colell, A. Stochastic uncoupled dynamics and Nash equilibrium. _Games and Economic Behavior_, 57:286-303, 2006.
* [24] Heliou, A., Cohen, J., and Mertikopoulos, P. Learning with bandit feedback in potential games. In _NIPS '17: Proceedings of the 31st International Conference on Neural Information Processing Systems_, 2017.
* [25] Hofbauer, J. and Sigmund, K. _Evolutionary Games and Population Dynamics_. Cambridge University Press, Cambridge, UK, 1998.
* [26] Hsieh, Y.-G., Iutzeler, F., Malick, J., and Mertikopoulos, P. On the convergence of single-call stochastic extra-gradient methods. In _NeurIPS '19: Proceedings of the 33rd International Conference on Neural Information Processing Systems_, pp. 6936-6946, 2019.
* [27] Hsieh, Y.-G., Antonakopoulos, K., and Mertikopoulos, P. Adaptive learning in continuous games: Optimal regret bounds and convergence to Nash equilibrium. In _COLT '21: Proceedings of the 34th Annual Conference on Learning Theory_, 2021.
* [28] Hsieh, Y.-G., Antonakopoulos, K., Cevher, V., and Mertikopoulos, P. No-regret learning in games with noisy feedback: Faster rates and adaptivity via learning rate separation. In _NeurIPS '22: Proceedings of the 36th International Conference on Neural Information Processing Systems_, 2022.
* [29] Hsieh, Y.-G., Iutzeler, F., Malick, J., and Mertikopoulos, P. Multi-agent online optimization with delays: Asynchronicity, adaptivity, and optimism. _Journal of Machine Learning Research_, 23(78):1-49, May 2022.
* [30] Jiang, X., Lim, L.-H., Yao, Y., and Ye, Y. Statistical ranking and combinatorial Hodge theory. _Mathematical Programming_, 127(1):203-244, 2011.
* [31] Juditsky, A., Nemirovski, A. S., and Tauvel, C. Solving variational inequalities with stochastic mirror-prox algorithm. _Stochastic Systems_, 1(1):17-58, 2011.
* [32] Korpelevich, G. M. The extragradient method for finding saddle points and other problems. _Ekonom. i Mat. Metody_, 12:747-756, 1976.
* [33] Kwon, J. and Mertikopoulos, P. A continuous-time approach to online optimization. _Journal of Dynamics and Games_, 4(2):125-148, April 2017.
* [34] Lee, J. M. _Introduction to Smooth Manifolds_. Number 218 in Graduate Texts in Mathematics. Springer-Verlag, New York, NY, 2 edition, 2003.
* [35] Legacci, D., Mertikopoulos, P., and Pradelski, B. S. R. A geometric decomposition of finite games: Convergence vs. recurrence under exponential weights. [https://arxiv.org/abs/2405.07224](https://arxiv.org/abs/2405.07224), 2024.
* [36] Lin, T., Zhou, Z., Mertikopoulos, P., and Jordan, M. I. Finite-time last-iterate convergence for multi-agent learning in games. In _ICML '20: Proceedings of the 37th International Conference on Machine Learning_, 2020.
* [37] Mertikopoulos, P. and Moustakas, A. L. The emergence of rational behavior in the presence of stochastic perturbations. _The Annals of Applied Probability_, 20(4):1359-1388, July 2010.
* [38] Mertikopoulos, P. and Sandholm, W. H. Learning in games via reinforcement and regularization. _Mathematics of Operations Research_, 41(4):1297-1324, November 2016.
* [39] Mertikopoulos, P. and Sandholm, W. H. Riemannian game dynamics. _Journal of Economic Theory_, 177:315-364, September 2018.
* [40] Mertikopoulos, P. and Zhou, Z. Learning in games with continuous action sets and unknown payoff functions. _Mathematical Programming_, 173(1-2):465-507, January 2019.
* [41] Mertikopoulos, P., Papadimitriou, C. H., and Piliouras, G. Cycles in adversarial regularized learning. In _SODA '18: Proceedings of the 29th annual ACM-SIAM Symposium on Discrete Algorithms_, 2018.
* [42] Mertikopoulos, P., Lecouat, B., Zenati, H., Foo, C.-S., Chandrasekhar, V., and Piliouras, G. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In _ICLR '19: Proceedings of the 2019 International Conference on Learning Representations_, 2019.
* [43] Mertikopoulos, P., Hsieh, Y.-P., and Cevher, V. A unified stochastic approximation framework for learning in games. _Mathematical Programming_, 203:559-609, January 2024.
* 143, 1996.
* [45] Nemirovski, A. S. Prox-method with rate of convergence \(O(1/t)\) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. _SIAM Journal on Optimization_, 15(1):229-251, 2004.

* [46] Nesterov, Y. Dual extrapolation and its applications to solving variational inequalities and related problems. _Mathematical Programming_, 109(2):319-344, 2007.
* [47] Nisan, N., Roughgarden, T., Tardos, E., and Vazirani, V. V. (eds.). _Algorithmic Game Theory_. Cambridge University Press, 2007.
* [48] Piliouras, G. and Shamma, J. S. Optimization despite chaos: Convex relaxations to complex limit sets via Poincare recurrence. In _SODA '14: Proceedings of the 25th annual ACM-SIAM Symposium on Discrete Algorithms_, 2014.
* [49] Popov, L. D. A modification of the Arrow-Hurwicz method for search of saddle points. _Mathematical Notes of the Academy of Sciences of the USSR_, 28(5):845-848, 1980.
* [50] Rakhlin, A. and Sridharan, K. Optimization, learning, and games with predictable sequences. In _NIPS '13: Proceedings of the 27th International Conference on Neural Information Processing Systems_, 2013.
* [51] Robinson, C. _Dynamical Systems: Stability, Symbolic Dynamics, and Chaos_. CRC Press, November 1998.
* [52] Rockafellar, R. T. _Convex Analysis_. Princeton University Press, Princeton, NJ, 1970.
* [53] Rockafellar, R. T. and Wets, R. J. B. _Variational Analysis_, volume 317 of _A Series of Comprehensive Studies in Mathematics_. Springer-Verlag, Berlin, 1998.
* [54] Rustichini, A. Optimal properties of stimulus-response learning models. _Games and Economic Behavior_, 29(1-2):244-273, 1999.
* [55] Sandholm, W. H. Potential games with continuous player sets. _Journal of Economic Theory_, 97:81-108, 2001.
* [56] Sato, Y., Akiyama, E., and Farmer, J. D. Chaos in learning a simple two-person game. _Proceedings of the National Academy of Sciences of the USA_, 99(7):4748-4751, April 2002.
* [57] Shalev-Shwartz, S. Online learning and online convex optimization. _Foundations and Trends in Machine Learning_, 4(2):107-194, 2011.
* [58] Shalev-Shwartz, S. and Singer, Y. Convex repeated games and Fenchel duality. In _NIPS' 06: Proceedings of the 19th Annual Conference on Neural Information Processing Systems_, pp. 1265-1272. MIT Press, 2006.
* [59] Syrgkanis, V., Agarwal, A., Luo, H., and Schapire, R. E. Fast convergence of regularized learning in games. In _NIPS '15: Proceedings of the 29th International Conference on Neural Information Processing Systems_, pp. 2989-2997, 2015.
* [60] Taylor, P. D. and Jonker, L. B. Evolutionary stable strategies and game dynamics. _Mathematical Biosciences_, 40(1-2):145-156, 1978.
* [61] Viossat, Y. and Zapechelnyuk, A. No-regret dynamics and fictitious play. _Journal of Economic Theory_, 148(2):825-842, March 2013.
* [62] Wei, C.-Y., Lee, C.-W., Zhang, M., and Luo, H. Linear last-iterate convergence in constrained saddle-point optimization. In _ICLR '21: Proceedings of the 2021 International Conference on Learning Representations_, 2021.

## Appendix A Harmonic Games

* Harmonic games, measures and comeasures
* Preference equivalence between harmonic games
* Mixed characterization of harmonic games
* Harmonic and zero-sum games
* Basic properties of regularizers and the induced choice maps
* Preliminary definitions
* Basic lemmas
* Continuous-time analysis
* Dynamical systems notions
* Basic properties of FTRL
* Constant of motion for harmonic games
* FTRL in the space of payoff differences
* Recurrence of FTRL in harmonic games
* Discrete-time analysis
* Lyapunov functions and template inequalities for (FTRL+)
* Proof of Theorem 3
* Proof of Theorem 4

## Appendix A Harmonic Games

The class of _uniform harmonic games_ (UHGs) introduced by Candogan et al. [6] provides a game-theoretic framework for modeling strategic situations with conflicting, anti-aligned interests.9 Broadly speaking, the characterizing property of uniform harmonic games is the following: for any player considering a deviation towards a specific pure strategy profile, there exist other players who are motivated to deviate _away_ from that profile.

Footnote 9: We include here the word “uniform” to distinguish the class of harmonic games introduced by Candogan et al. [6] from the more general class of harmonic games considered in this work, cf. Definition 1.

Given a finite game \(\Gamma=\Gamma(\mathcal{N},\mathcal{A},u)\), this is formalized by the condition that, for all \(\alpha\in\mathcal{A}\),

\[\sum_{i\in\mathcal{N}}\sum_{\beta_{i}\in\mathcal{A}_{i}}\left[u_{i}(\alpha_{i} ;\alpha_{-i})-u_{i}(\beta_{i};\alpha_{-i})\right]=0\,.\] (A.1)

From a strategic viewpoint, uniform harmonic games complement potential games: Candogan et al. [6] showed that any finite game can be uniquely decomposed into the sum of a potential game and a uniform harmonic game, up to linear transformations of the payoff functions that do not change the strategic structure of the game.

Since their introduction, harmonic games have generated a substantial body of literature; for a brief survey, we refer the reader to [35].

### Harmonic games, measures and comeasures

The class of uniform harmonic games exhibits intriguing, yet restrictive, properties. Notably, a UHG always admits the uniformly mixed strategy asa NE, and it generally possesses a _continuum_ of Nash equilibria [6]. Additionally, the framework of UHGs and the decomposition proposed by Candogan et al. [6] are incompatible with common game-theoretical transformations, such as the duplication of strategies or rescaling of payoffs [1]. To address the above limitations, Abdou et al. [1] extended the definition of harmonic games by the introduction of two parameters: a _measure_, that is a positive weight each player assigns to each of _their own_ strategy; and a _conceasure_, that is a positive weight each player assigns to each of the _other players'_ action profiles.

**Definition A.1**.: Let \(\Gamma(\mathcal{N},\mathcal{A},u)\) be a finite game. A _player measure_\(\mu_{i}\) is a function \(\mu_{i}\colon\mathcal{A}_{i}\to\mathbb{R}_{++}\); a _player co-measure_\(\gamma_{i}\) is a function \(\gamma_{i}\colon\mathcal{A}_{-i}\to\mathbb{R}_{++}\). Correspondingly, a collection \(\mu=\{\mu_{i}\}_{i\in\mathcal{N}}\) (resp. \(\gamma=\{\gamma_{i}\}_{i\in\mathcal{N}}\)) of player measures (resp. comeasures) is called _game measure_ (resp. _game comeasure_). If \(\mu_{i}\) is a player measure, we will write \(|\mu_{i}|=\sum_{\alpha_{i}}\mu_{i\alpha_{i}}\). Finally, a _probability measure_ is a game measure \(\mu\) such that \(|\mu_{i}|=1\) for all \(i\in\mathcal{N}\); a _uniform measure_ is a game measure \(\mu\) such that \(\mu_{i\alpha_{i}}=1\) for all \(i\in\mathcal{N},\alpha_{i}\in\mathcal{A}_{i}\); and a _uniform comeasure_ is a game comeasure \(\gamma\) such that \(\gamma_{i\alpha_{-i}}=1\) for all \(i\in\mathcal{N},\alpha_{-i}\in\mathcal{A}_{-i}\).

With these notions in place, Abdou et al. [1] define a finite game \(\Gamma\) to be _\((\mu,\gamma)\)-harmonic_ if there exist a game measure \(\mu\) and a game comeasure \(\gamma\) such that, for all \(\alpha\in\mathcal{A}\),

\[\sum_{i}\sum_{\beta_{i}}\mu_{i\beta_{i}}\gamma_{i\alpha_{-i}}\big{[}u_{i}( \alpha_{i};\alpha_{-i})-u_{i}(\beta_{i};\alpha_{-i})\big{]}=0\,.\] (A.2)

In this work, we focus solely on harmonic games with _uniform comeasure_. As discussed after Definition 1 in the main body of the article, this choice comes without loss of generality: the game comeasure in Eq. (A.2) can be absorbed by a payoff rescaling to give a game that is still harmonic, and _preference equivalent_ to the original game - in a sense that we make precise in the next section.

### Preference equivalence between harmonic games

The strategic structure of a game is preserved under monotonic transformations of the utility functions, since the set of pure Nash equilibria of a game is an ordinal object - it depends only on the signs of unilateral payoff differences, and not on their absolute values. For this reason, two games \(\Gamma(\mathcal{N},\mathcal{A},u)\) and \(\Gamma^{\prime}(\mathcal{N},\mathcal{A},u^{\prime})\) are called _preference-equivalent_ (PE) if for all \(\alpha,\beta\in\mathcal{A}\) and all \(i\in\mathcal{N}\), we have

\[\operatorname{sgn}\big{[}u^{\prime}_{i}(\beta_{i};\alpha_{-i})-u^{\prime}_{i}( \alpha_{i};\alpha_{-i})\big{]}=\operatorname{sgn}\big{[}u_{i}(\beta_{i};\alpha _{-i})-u_{i}(\alpha_{i};\alpha_{-i})\big{]}\,.\] (A.3)

Two games are _strategically equivalent_ (SE) - and we write \(\Gamma\sim\Gamma^{\prime}\) - if they have the same unilateral payoff differences, that is if

\[u^{\prime}_{i}(\beta_{i};\alpha_{-i})-u^{\prime}_{i}(\alpha_{i};\alpha_{-i})=u _{i}(\beta_{i};\alpha_{-i})-u_{i}(\alpha_{i};\alpha_{-i})\] (A.4)

for all \(\alpha,\beta\in\mathcal{A}\) and all \(i\in\mathcal{N}\); strategically equivalent games are clearly preference-equivalent.

**Lemma A.2**.: _Let \(\Gamma_{\mu,\gamma}=\Gamma_{\mu,\gamma}(\mathcal{N},\mathcal{A},u)\) be a harmonic game in the sense of Eq. (A.2). Then the game \((\mathcal{N},\mathcal{A},u^{\prime})\) with \(u^{\prime}_{i}(\alpha_{i};\alpha_{-i})=\gamma_{i\alpha_{-i}}u_{i}(\alpha_{i}; \alpha_{-i})\) is preference-equivalent to the game \(\Gamma_{\mu,\gamma}\), and it is harmonic in the sense of Eq. (A.2) with measure \(\mu\) and uniform comeasure._

Proof.: Let \(u^{\prime\prime}_{i}(\alpha_{i};\alpha_{-i})=\mu_{i\alpha_{i}}\gamma_{i\alpha_ {-i}}u_{i}(\alpha_{i};\alpha_{-i})\). Then replacing above, for all \(\alpha\in\mathcal{A}\),

\[0=\sum_{i\in\mathcal{N}}\sum_{\beta_{i}\in\mathcal{A}_{i}}\mu_{i\beta_{i}}\left[ \frac{u^{\prime\prime}_{i}(\alpha_{i};\alpha_{-i})}{\mu_{i\alpha_{i}}}-\frac {u^{\prime\prime}_{i}(\beta_{i};\alpha_{-i})}{\mu_{i\beta_{i}}}\right]\,.\]

Let \(u^{\prime}_{i}(\alpha_{i};\alpha_{-i})=\frac{u^{\prime\prime}_{i}(\alpha_{i}; \alpha_{-i})}{\mu_{i\alpha_{i}}}=\gamma_{i\alpha_{-i}}u_{i}(\alpha_{i};\alpha _{-i})\). The game \(u^{\prime}\) is preference-equivalent to \(u\), and

\[0=\sum_{i\in\mathcal{N}}\sum_{\beta_{i}\in\mathcal{A}_{i}}\mu_{i\beta_{i}} \big{[}u^{\prime}_{i}(\alpha_{i};\alpha_{-i})-u^{\prime}_{i}(\beta_{i};\alpha _{-i})\big{]}\] (A.5)

for all \(\alpha\in\mathcal{A}\), so \(u^{\prime}\) is harmonic in the sense of A.2 with measure \(\mu\) and uniform comeasure. 

In the proof above we perform the intermediate step \(u\to u^{\prime\prime}\) rather than defining directly \(u\to u^{\prime}\) to stress the difference between rescaling the payoffs of a game by a game measure \(\mu\) and by a game comeasure \(\gamma\). The game with payoffs \(u^{\prime}=\gamma u\) (the meaning of this notation made precise in the proof above) is preference-equivalent to the game with payoffs \(u\), i.e., rescaling the payoffs by a comeasure does not change the strategic structure of the game. On the other hand, the game with payoffs \(u^{\prime\prime}=\mu u^{\prime}\) - again, the meaning made precise in the proof - is _not_ PE to the game with payoffs \(u^{\prime}\): rescaling the payoffs by a measure can change the preferences of the players, and leads to a game with intrinsically different strategic structure.

Lemma A.2 motivates our choice to focus in this work on harmonic games with arbitrary measures and uniform comeasures, and to adopt (HG) from Definition 1 to characterize harmonic games: a _harmonic game_ (HG) \(\Gamma_{\mu}=\Gamma_{\mu}(\mathcal{N},\mathcal{A},u)\) is a finite game \((\mathcal{N},\mathcal{A},u)\) with a game measure \(\mu\) such that (HG) holds, i.e., \(\sum_{i\in\mathcal{N}}\sum_{\beta_{i}\in\mathcal{A}_{i}}\mu_{i\beta_{i}}[u_{i} (\alpha_{i};\alpha_{-i})-u_{i}(\beta_{i};\alpha_{-i})]=0\) for all \(\alpha\in\mathcal{A}\).

### Mixed characterization of harmonic games

The defining property (HG) allows for an equivalent characterization of harmonic games in terms of their mixed payoffs:

**Lemma A.3**.: _A finite game \(\Gamma=\Gamma(\mathcal{N},\mathcal{A},u)\) is harmonic with measure \(\mu\) if and only if_

\[\sum_{i\in\mathcal{N}}\lvert\mu_{i}\rvert\left\langle{{}_{t}(x),{}_{x}-\frac{ \mu_{i}}{\lvert\mu_{i}\rvert}}\right\rangle=0\quad\text{for all }x\in\mathcal{X}\,.\] (HG-mixed)

Proof.: Given a finite game \(\Gamma=\Gamma(\mathcal{N},\mathcal{A},u)\) and a game measure \(\mu\), let \(F_{i}:\mathcal{A}\to\mathbb{R}\) be defined by \(F_{i}(\alpha)=\sum_{\beta_{i}\in\mathcal{A}_{i}}\mu_{i\beta_{i}}[u_{i}(\alpha _{i};\alpha_{i-i})-u_{i}(\beta_{i};\alpha_{-i})]\). By definition, \(\Gamma\) is a \(\mu\)-harmonic game if and only if \(F(\alpha)\coloneqq\sum_{i\in\mathcal{N}}F_{i}(\alpha)=0\) for all \(\alpha\in\mathcal{A}\). Denote (with slight abuse of notation) by \(F:\mathcal{X}\to\mathbb{R}\) the multilinear extension of \(F:\mathcal{A}\to\mathbb{R}\), i.e., \(F(x)=\sum_{\alpha}x_{\alpha}F(\alpha)\), with \(x_{\alpha}\coloneqq\prod_{i}x_{i\alpha_{i}}\). Now, \(F(\alpha)=0\) for all \(\alpha\in\mathcal{A}\) if and only if \(F(x)=0\) for all \(x\in\mathcal{X}\), which is the case if and only if

\[0 =F(x)=\sum_{\alpha}x_{\alpha}\sum_{i}F_{i}(\alpha)=\sum_{i}\sum_{ \alpha_{i}}\sum_{\alpha_{-i}}x_{i\alpha_{i}}x_{-i\alpha_{-i}}\sum_{\beta_{i}} \mu_{i\beta_{i}}[u_{i}(\alpha_{i};\alpha_{-i})-u_{i}(\beta_{i};\alpha_{-i})]\] \[=\sum_{i}\sum_{\beta_{i}}\mu_{i\beta_{i}}[u_{i}(x_{i};x_{-i})-u_{ i}(\beta_{i};x_{-i})]=\sum_{i}\big{[}\lvert\mu_{i}\rvert\langle{{}_{t}(x),{}_{x} }\rangle-\langle{{}_{t}(x),\mu_{i}}\rangle\big{]}\quad\text{for all }x\in \mathcal{X},\]

from which we conclude by factoring out the \(\lvert\mu_{i}\rvert\) terms. 

_Remark_.: The first equality in the second line holds true for harmonic games with _uniform_ comeasure \(\gamma_{i\alpha_{-i}}=1\), since \(\gamma_{i\alpha_{-i}}\neq 1\) terms would couple with the corresponding \(x_{-i\alpha_{-i}}\) terms in the sum.

The above result can be reformulated as follows:

**Proposition A.4**.: _A finite game \(\Gamma=\Gamma(\mathcal{N},\mathcal{A},u)\) is harmonic if and only if it admits a strategic center \((m,q)\), viz. if there exist (\(i\)) a vector \(m\in\mathbb{R}^{N}_{++}\) and (\(ii\)) a fully mixed strategy \(q\in\mathcal{X}\) such that_

\[\sum_{i\in\mathcal{N}}m_{i}\left\langle{{}_{t}(x),{}_{x}-{}_{q}{}_{i}}\right\rangle =0\quad\text{for all }x\in\mathcal{X}\,.\] (HG-center)

This expression is intriguing: it suggest that a game is harmonic precisely if there exists a fully mixed strategy \(q\) such that, for all \(x\in\mathcal{X}\), the payoff vector \(v(x)\) is perpendicular (with respect to a \(m\)-weighted inner product) to \(x-q\); cf. Example A.1 and Fig. 2. The striking dynamical consequences of this "circular" strategic structure - hinted at in Fig. 2, showing a _periodic_ orbit of FTRL in continuous time - are captured precisely by Theorem 2 in the main text.

Proof of Proposition a.4.: Let \(\Gamma_{\mu}=\Gamma_{\mu}(\mathcal{N},\mathcal{A},u)\) be harmonic; then by Lemma A.3 that there exist a strategic center \((m,q)\) given by \(m_{i}\coloneqq\lvert\mu_{i}\rvert\) and \(q_{i}\coloneqq\mu_{i}/\lvert\mu_{i}\rvert\) with \(i\in\mathcal{N}\). Conversely let \(\Gamma=\Gamma(\mathcal{N},\mathcal{A},u)\) admit a strategic center \((m,q)\); then by the same argument \(\Gamma\) is harmonic with \(\mu_{i}\coloneqq m_{i}q_{i}\) for all \(i\in\mathcal{N}\). 

An immediate corollary is the following:

**Corollary A.5**.: _If a finite game \(\Gamma\) admits a strategic center \((m,q)\), then \(q\) is a Nash equilibrium._

Proof.: By Proposition A.4 if \(\Gamma\) admits a strategic center \((m,q)\) then it is \(\mu\)-harmonic with \(\mu_{i}=m_{i}q_{i}\) for all \(i\in\mathcal{N}\); and \((\mu_{i}/\lvert\mu_{i}\rvert)_{i\in\mathcal{N}}\) is always a NE for \(\mu\)-harmonic games [1, Theorem 1]. 

_Remark_.: The converse does not hold: a fully mixed Nash equilibrium is not necessarily a strategic center. If it were, a game would be harmonic precisely if it admitted a fully mixed NE, which is not the case - think for example of coordination or anti-coordination games, that admit a fully mixed Nash equilibrium and are not harmonic.

[MISSING_PAGE_FAIL:17]

\(\left(x_{A}-x_{N},-4x_{A}\right).\) Choosing \(\lambda=3\) the strategic center gives weights \(m=\left(6,5\right)\) and Nash equilibrium \(q=\left[\left(1/6,5/6\right),\,\left(2/5,3/5\right)\right]\). Condition (HG-center) boils down to \(6\left(v_{1},x-q_{1}\right)+5\left\langle v_{2},y-q_{2}\right\rangle=0\), which one readily verifies to hold true by replacing the expressions above and recalling that \(x_{A}+x_{N}=1=y_{D}+y_{N}\). Fig. 2 illustrates the situation: each payoff vector \(v(x)\) (black arrows) is perpendicular (with respect to a weighted inner product) to the vector \(x-q\) (dotted segment) between the evaluation point \(x\) of the payoff field and the fully mixed Nash equilibrium \(q\) (red point). \(\diamond\)

### Harmonic and zero-sum games

Candogan et al. [6]'s uniform harmonic games, defined by Eq. (A.1), are precisely the harmonic games with uniform measure, which makes uniform harmonic games a strict subset of the set of HGs. Importantly, HGs include another archetypal class of perfect-competition games: as we show in this section, _two-player zero-sum games_ (ZZSGs) with an interior NE \(x^{*}\) are harmonic with (probability) measure \(\mu=x^{*}\).

To show this, we will need the following definition and lemma:

**Definition A.6** (Non-strategic game).: A finite normal form game \(\Gamma=\Gamma(\mathcal{N},\mathcal{A},k)\) is called _non-strategic_ if the payoff of each player does not depend on their own choice, viz. if \(k_{i}(\alpha_{i},\alpha_{-i})=k_{i}(\beta_{i},\alpha_{-i})\) for all \(i\in\mathcal{N},\alpha\in\mathcal{A},\beta_{i}\in\mathcal{A}_{i}\).

**Lemma A.7**.: _Two finite games \(\Gamma(\mathcal{N},\mathcal{A},u),\Gamma^{\prime}(\mathcal{N},\mathcal{A},u^{ \prime})\) are strategically equivalent in the sense of Eq. (A.4) if and only if their difference is a non-strategic game._

Proof.: Let \(\Gamma-\Gamma^{\prime}\) be non-strategic; then \(k:=u^{\prime}-u\) fulfills the condition of Definition A.6, which shows that \(u\) and \(u^{\prime}\) fulfill Eq. (A.4). Conversely let \(\Gamma\) and \(\Gamma^{\prime}\) be strategically equivalent; set \(k:=u^{\prime}-u\) and rearrange the terms in Eq. (A.4) to immediately conclude that \(k\) is a non-strategic game. 

**Proposition A.8**.: _Let \(\Gamma_{\mu}=\Gamma_{\mu}(\mathcal{N},\mathcal{A},u)\) be a harmonic game. If the measure \(\mu\) fulfills \(|\mu_{i}|=|\mu_{j}|\) for all \(i,j\in\mathcal{N}\) then \(\Gamma_{\mu}\) is strategically equivalent to a zero-sum game._

Proof.: Recall that \(|\mu_{i}|\equiv\Sigma_{\alpha_{i}}\mu_{i\alpha_{i}}\). Under the assumption \(|\mu_{i}|=|\mu_{j}|\) for all \(i,j\in\mathcal{N}\), let \(c:=|\mu_{i}|\) for any \(i\in\mathcal{N}\). By (HG), the payoff \(u\) of \(\Gamma_{\mu}\) in this case fulfills \(\Sigma_{i\in\mathcal{N}}[u_{i}(\alpha)-k_{i}(\alpha)]=0\) for all \(\alpha\in\mathcal{A}\), with \(k_{i}(\alpha_{i};\alpha_{-i}):=c^{-1}\sum_{\beta_{i}}\mu_{i\beta_{i}}u_{i}( \beta_{i},\alpha_{-i})\). Set \(u^{\prime}_{i}:=u_{i}-k_{i}\). By definition \(u^{\prime}\) is a zero-sum game; furthermore, the difference between \(u_{i}\) and \(u^{\prime}_{i}\) is non-strategic, since \(k_{i}(\alpha_{i};\alpha_{-i})\) does not depend on \(\alpha_{i}\). Thus \(u_{i}\) and \(u^{\prime}_{i}\) are strategically equivalent by Lemma A.7. 

In particular we have the following:

**Corollary A.9**.: _Let \(\Gamma_{\mu}=\Gamma_{\mu}(\mathcal{N},\mathcal{A},u)\) be a harmonic game. If the measure \(\mu\) is a probability measure, then \(\Gamma_{\mu}\) is strategically equivalent to a zero-sum game._

The converse holds true only in the case of two-player games:

**Proposition A.10**.: _Every two-player zero-sum game with an interior Nash equilibrium \(x^{*}\) is harmonic, with (probability) measure \(\mu=x^{*}\)._

Proof.: Let \(\Gamma=\Gamma(\mathcal{N},\mathcal{A},u)\) be a two-player zero-sum game with interior Nash equilibrium \(x^{*}\). If we show that

\[\sum_{i\in\mathcal{N}}|x^{*}_{i}|\left\langle v_{i}(x),x_{i}-\frac{x^{*}_{i}} {\left|x^{*}_{i}\right|}\right\rangle=0\quad\text{for all }x\in\mathcal{X}\,,\] (A.9)

then we can conclude by Lemma A.3 that \(\Gamma\) is harmonic with measure \(x^{*}\). Eq. (A.9) holds indeed true: \(|x^{*}_{i}|=1\) for all \(i\in\mathcal{N}\), and it is well known [41, 42] that two-player zero-sum games with an interior equilibrium \(x^{*}\) fulfill \(\sum_{i\in\mathcal{N}}\langle v_{i}(x),x_{i}-x^{*}_{i}\rangle=0\) for all \(x\in\mathcal{X}\), so we are done. 

Harmonic games thus encompass and substantially generalize two prototypical classes of games with anti-aligned incentives, serving as an ideal complement to the class of potential games. This is made precise in [1]: building on the work of Candogan et al. [6], Abdou et al. [1] showed that, for any choice of game measure \(\mu\), every finite game can be uniquely decomposed into the sum of a potential and a \(\mu\)-harmonic game, up to strategic equivalence.

This establishes harmonic games as the natural complement of potential games from a strategic perspective; Theorem 2 in the main text shows that this holds true from a _dynamic_ perspective as well.

Basic properties of regularizers and the induced choice maps

In this appendix, we collect a number of properties concerning regularizers and the associated choice maps. To avoid carrying around the player index \(i\in\mathcal{N}\), we state all our results for a generic convex subset \(\mathcal{C}\) of some real vector space \(\mathcal{V}\). The desired properties for FTRL will then be obtained by specializing \(\mathcal{C}\) to \(\mathcal{X}_{i}\) or \(\mathcal{X}\) and \(\mathcal{V}\) to \(\mathbb{R}^{\mathcal{A}_{i}}\) or \(\prod_{j}\mathbb{R}^{\mathcal{A}_{j}}\), depending on the context.

#### b.1. Preliminary definitions.

To begin, let \(\mathcal{V}\) be a \(d\)-dimensional normed space with norm \(\|\cdot\|\). In what follows, we will write \(\mathcal{Y}\coloneqq\mathcal{V}^{*}\) for the dual space of \(\mathcal{V}\), \(\langle y,x\rangle\) for the canonical pairing between \(x\in\mathcal{V}\) and \(y\in\mathcal{V}^{*}\), and \(\|y\|_{*}=\max\{\langle y,x\rangle:\|x\|\leq 1\}\) for the induced dual norm on \(\mathcal{Y}\). Following standard conventions in convex analysis, functions will be allowed to take values in the extended real line \(\mathbb{R}\cup\{\infty\}\), and if \(f\colon\mathcal{V}\to\mathbb{R}\cup\{\infty\}\) is a convex function on \(\mathcal{V}\), we will denote its _effective domain_ as

\[\operatorname{dom}f\coloneqq\{x\in\mathcal{V}:f(x)<\infty\}\,.\] (B.1)

In addition, assuming \(\operatorname{dom}f\neq\varnothing\), the _subdifferential_ of \(f\) at \(x\) is defined as

\[\partial f(x)\coloneqq\{y\in\mathcal{Y}:f(x^{\prime})\geq f(x)+\langle y,x^{ \prime}-x\rangle\text{ for all }x^{\prime}\in\mathcal{V}\}\] (B.2)

and we denote the _domain of subdifferentiability_ of \(f\) as

\[\operatorname{dom}\partial f=\{x\in\mathcal{V}:\partial f(x)\neq\varnothing\}\,.\] (B.3)

Finally, to ease notation, a convex function \(f\colon\mathcal{C}\to\mathbb{R}\) will be identified with the extended-real-valued function \(\bar{f}\colon\mathcal{V}\to\mathbb{R}\cup\{\infty\}\) that agrees with \(f\) on \(\mathcal{C}\) and is identically equal to \(\infty\) on \(\mathcal{V}\setminus\mathcal{C}\).

With all this in hand, let \(\mathcal{C}\) be a closed convex subset of \(\mathcal{V}\), and let \(h\colon\mathcal{C}\to\mathbb{R}\) be a \(K\)-strongly convex _regularizer_ on \(\mathcal{C}\), that is,

\[h(tx+(1-t)x^{\prime})\leq th(x)+(1-t)h(x^{\prime})-\frac{K}{2}t(1-t)\|x^{ \prime}-x\|^{2}\,.\] (B.4)

By standard arguments in convex analysis, this readily implies that

\[h(x^{\prime})\geq h(x)+\partial h(x;x^{\prime}-x)+\frac{K}{2}\|x^{\prime}-x\| ^{2}\quad\text{for all }x,x^{\prime}\in\mathcal{X},\] (B.5)

where

\[\partial h(x;x^{\prime}-x)=\lim_{\theta\to\partial^{*}}\bigl{[}h(x+\theta(x^{ \prime}-x))-h(x)\bigr{]}/\theta\] (B.6)

denotes the one-sided directional derivative of \(h\) at \(x\) along the direction of \(x^{\prime}-x\). To proceed, we will need the following basic objects:

1. The _convex conjugate_\(h^{*}\colon\mathcal{Y}\to\mathbb{R}\) of \(h\): \[h^{*}(y)=\max_{x\in\mathcal{X}}\{\langle y,x\rangle-h(x)\}\qquad\quad\text{ for all }y\in\mathcal{Y}.\] (B.7)
2. The _regularized choice map_ - or _mirror map_\(-Q\colon\mathcal{Y}\to\mathcal{X}\) induced by \(h\): \[Q(y)=\operatorname*{arg\,max}_{x\in\mathcal{X}}\{\langle y,x\rangle-h(x)\} \qquad\text{for all }y\in\mathcal{Y}\] (B.8)
3. The associated _Fenchel coupling_\(F\colon\mathcal{X}\times\mathcal{Y}\to\mathbb{R}\) of \(h\): \[F(p,y)=h(p)+h^{*}(y)-\langle y,p\rangle\qquad\quad\text{for all }p\in\mathcal{X},y\in\mathcal{Y}.\] (B.9)

_Remark_.: The terminology "Fenchel coupling" is due to [38, 40], which we follow closely in terms of notation and conventions.

The proposition below provides some basic properties concerning the first two objects above:

**Proposition B.1**.: _Let \(h\) be a \(K\)-strongly convex regularizer on \(\mathcal{C}\). Then:_

1. \(Q\) _is single-valued on_ \(\mathcal{Y}\)_; in particular, for all_ \(x\in\operatorname{dom}\partial h\) _and all_ \(y\in\mathcal{Y}\)_, we have:_ \[x=Q(y)\quad\text{if and only if}\quad y\in\partial h(x)\,.\] (B.10)_._
2. _The image_ \(\operatorname{im}Q\) _of_ \(Q\) _satisfies_ \(\operatorname{ri}\mathcal{C}\subseteq\operatorname{im}Q=\operatorname{dom}\partial h \subseteq\mathcal{C}\)_._
3. _The convex conjugate_ \(h^{*}\colon\mathcal{Y}\to\mathbf{R}\) _of_ \(h\) _is differentiable and_ \[Q(y)=\nabla h^{*}(y)\quad\text{for all }y\in\mathcal{Y}.\] (B.11)
4. \(Q\) _is_ \((1/K)\)_-Lipschitz continuous, that is,_ \[\|Q(y^{\prime})-Q(y)\|\leq(1/K)\|y^{\prime}-y\|_{*}\quad\text{for all }y,y^{ \prime}\in\mathcal{Y}.\] (B.12)
5. _Fix some_ \(y\in\mathcal{Y}\) _and set_ \(x=Q(y)\)_. Then, for all_ \(x^{\prime}\in\mathcal{X}\) _we have:_ \[\partial h(x;x^{\prime}-x)\geq\langle y,x^{\prime}-x\rangle\,.\] (B.13) _In particular, if_ \(\partial h\) _admits a continuous selection_ \(\nabla h\colon\operatorname{dom}\partial h\to\mathcal{Y}\)_, we have_ \[\langle\nabla h(x),x^{\prime}-x\rangle\geq\langle y,x^{\prime}-x\rangle\quad \text{for all }x\in\operatorname{dom}\partial h\text{ and all }x\in\mathcal{C},\] (B.14) _or, equivalently,_ \[\partial h(x)=\nabla h(x)+\operatorname{PC}(x)\quad\text{for all }x\in \operatorname{dom}\partial h,\] (B.15) _where_ \[\operatorname{PC}(x)=\{w\in\mathcal{Y}:\langle w,x^{\prime}-x\rangle\leq 0 \text{ for all }x^{\prime}\in\mathcal{X}\}\] (B.16) _denotes the_ polar cone _to_ \(\mathcal{C}\) _at_ \(x\)_._

Proof.: These properties are fairly well known (except possibly the last one), so we only provide a quick proof or a precise pointer to the literature.

1. The maximum in (B.8) is attained for all \(y\in\mathcal{V}^{*}\) and is unique because \(h\) is strongly convex. Furthermore, \(x\) solves (B.8) if and only if \(y-\partial h(x)\geq 0\), i.e., if and only if \(y\in\partial h(x)\).
2. By (B.10), we readily get \(\operatorname{im}Q=\operatorname{dom}\partial h\). Consequently, the rest of our claim follows from standard results in convex analysis, see e.g., Rockafellar [52, Chap. 26].
3. The equality \(Q=\nabla h^{*}\) follows immediately from Danskin's theorem, see e.g., Bertsekas [4, Proposition 5.4.8, Appendix B].
4. See Rockafellar & Wets [53, Theorem 12.60(b)].
5. Since \(y\in\partial h(x)\) by (B.10), we readily get that \[h(x+\theta(x^{\prime}-x))\geq h(x)+\theta(y,x^{\prime}-x)\quad\text{for all }\theta\in[0,1]\,.\] (B.17) Hence, by rearranging and taking the limit \(\theta\to 0^{+}\),10 we conclude that Footnote 10: The existence of the limit is guaranteed by standard results, see e.g., Bertsekas [4, Appendix B]. \[\partial h(x;x^{\prime}-x)=\lim_{\theta\to 0^{+}}\frac{h(x+\theta(x^{\prime}-x))-h(x)}{\theta}\geq\langle y,x^{\prime}-x\rangle\] (B.18) as claimed. Finally, for our last assertion, let \(z=x^{\prime}-x\) and set \[\phi(\theta)=h(x+\theta z)-[h(x)+\langle y,\theta z\rangle]\quad\text{for all }\theta\in[0,1]\] (B.19) so \(\phi(\theta)\geq K\theta^{2}\|z\|^{2}/2\geq 0\) for all \(\theta\in[0,1]\). By construction, it is straightforward to verify that the function \(\psi(\theta)=\langle\nabla h(x+\theta z)-y,z\rangle\) is a selection of subgradients of \(\phi\), i.e., \[\phi(\theta^{\prime})\geq\phi(\theta)+\psi(\theta)(\theta^{\prime}-\theta) \quad\text{for all }\theta,\theta^{\prime}\in[0,1].\] (B.20) Since \(\psi\) is in addition continuous (because \(\nabla h\) is), it follows that \(\phi^{\prime}(\theta)=\psi(\theta)\) for all \(\theta\in[0,1]\) by a well-known characterization of the one-sided derivatives of convex functions, cf. Rockafellar [52, Theorem 24.2]. Hence, with \(\phi\) convex and \(\phi(\theta)\geq\phi(0)\) for all \(\theta\in[0,1]\), we conclude that \(\langle\nabla h(x)-y,z\rangle=\psi(0)=\phi^{\prime}(0)\geq 0\), and our proof is complete. 

The next proposition collects some basic properties of the Fenchel coupling.

**Proposition B.2**.: _Let \(h\) be a \(K\)-strongly convex regularizer on \(\mathcal{C}\). Then, for all \(p\in\mathcal{X}\) and all \(y,y^{\prime}\in\mathcal{Y}\), we have:_

1. \(F(p,y)\geq 0\) _with equality if and only if_ \(p=Q(y)\)_._ (B.21a)__
2. \(F(p,y)\geq\frac{1}{2}K\,\|Q(y)-p\|^{2}\)_._ (B.21b)__

Proof.: These properties are also fairly standard, but we provide a quick proof for completeness.

1. By the Fenchel-Young inequality, we have \(h(p)+h^{*}(y)\geq\langle y,p\rangle\) for all \(p\in\mathcal{X}\), \(y\in\mathcal{Y}\), with equality if and only if \(y\in\partial h(p)\). Our claim then follows from (B.10).
2. Let \(x=Q(y)\) so \(y\in\partial h(x)\) by (B.10). Then, by the definition of \(F\), we have \[F(p,y) =h(p)+h^{*}(y)-\langle y,p\rangle\] \[=h(p)+\langle y,x\rangle-h(x)-\langle y,p\rangle \%\text{ since }y\in\partial h(x)\] \[\geq h(p)-h(x)-\partial h(x;p-x) \%\text{ by Proposition B.1}\] \[\geq\frac{1}{2}K\,\|x-p\|^{2} \%\text{ by (B.4)}\] and our proof is complete. 

In view of Proposition B.2, \(F(p,y)\) can be seen a "primal-dual" measure of divergence between \(p\in\mathcal{X}\) and \(y\in\mathcal{Y}\), and the alternate expression (19) is straightforward. This observation will play a major role in the sequel.

### Basic lemmas

Moving forward, we note that the various update steps in (FTRL+) can be written as

\[y^{+}=y+w\quad\text{and}\quad x^{+}=Q(y^{+})\] (B.22)

for some \(y,w\in\mathcal{Y}\). With this in mind, we proceed below to state a series of basic lemmas for the Fenchel coupling before and after an update of the form (B.22). These results are not new, cf. [31, 40, 42] and references therein; however, the assumptions used to derive them vary significantly in the literature, so we provide detailed proofs for completeness.

All of the results that follow below are stated for a \(K\)-strongly convex regularizer on \(\mathcal{C}\). The first result is a primal-dual version of the so-called "three-point identity" for mirror descent [7]:

**Lemma B.1**.: _Fix some \(p\in\mathcal{X}\), \(y\in\mathcal{Y}\), and let \(x=Q(y)\). Then, for all \(y^{+}\in\mathcal{Y}\), we have:_

\[F(p,y^{+})=F(p,y)+F(x,y^{+})+\langle y^{+}-y,x-p\rangle.\] (B.23)

Proof.: By definition, we have:

\[F(p,y^{+}) =h(p)+h^{*}(y^{+})-\langle y^{+},p\rangle\] (B.24a) \[F(p,y) =h(p)+h^{*}(y)-\langle y,p\rangle\] (B.24b) \[F(x,y^{+}) =h(x)+h^{*}(y^{+})-\langle y^{+},x\rangle\] (B.24c)

Thus, subtracting (B.24b) and (B.24c) from (B.24a), and rearranging, we get

\[F(p,y^{+})=F(p,y)+F(x,y^{+})-h(x)-h^{*}(y)+\langle y^{+},x\rangle-\langle y^{+ }-y,p\rangle\,.\] (B.25)

Our assertion then follows by recalling that \(x=Q(y)\), so \(h(x)+h^{*}(y)=\langle y,x\rangle\). 

The next result we present concerns the Fenchel coupling before and after a direct update step; similar results exist in the literature, but we again provide a proof for completeness.

**Lemma B.2**.: _Fix some \(p\in\mathcal{X}\) and \(y,w\in\mathcal{Y}\). Then, letting \(x=Q(y)\), \(y^{+}=y+w\), and \(x^{+}=Q(y^{+})\) as per (B.22), we have:_

\[F(p,y^{+}) =F(p,y)+\langle w,x^{+}-p\rangle-F(x^{+},y)\] (B.26a) \[\leq F(p,x)+\langle w,x-p\rangle+\frac{1}{2}K\,\|w\|_{*}^{2}\,.\] (B.26b)Proof.: By the three-point identity (B.23), we have

\[F(x,y)=F(x,y^{+})+F(x^{+},x)+\langle y-y^{+},x^{+}-p\rangle\] (B.27)

so our first claim follows by rearranging. For our second claim, simply note that

\[F(p,y)+\langle w,x^{+}-p\rangle-F(x^{+},y) =F(p,y)+\langle w,x-p\rangle+\langle w,x^{+}-x\rangle-F(p,y)\] \[\leq F(p,y)+\langle w,x-p\rangle+\frac{1}{2K}\|w\|_{*}^{2}+\frac{K }{2}\|x-p\|^{2}-F(p,y)\] (B.28)

so our claim follows from Proposition B.2. 

The last result we present here is sometimes referred to as a "four-point lemma", and concerns the Fenchel coupling before and after an _extrapolation_ step:

**Lemma B.3**.: _Fix some \(p\in\mathcal{X}\) and \(y,w_{1},w_{2}\in\mathcal{Y}\). Then, letting \(x=Q(y)\), \(y_{i}^{+}=y+w_{i}\), and \(x_{i}^{+}=Q(y_{i}^{+})\), \(i=1,2\), as per (B.22), we have:_

\[F(p,y_{2}^{+}) =F(p,y)+\langle w_{2},x_{1}^{+}-p\rangle+\left[\langle w_{2},x_{2 }^{+}-x_{1}^{+}\rangle-F(x_{2}^{+},y)\right]\] (B.29a) \[=F(p,y)+\langle w_{2},x_{1}^{+}-p\rangle+\langle w_{2}-w_{1},x_{2 }^{+}-x_{1}^{+}\rangle-F(x_{2}^{+},y_{1}^{+})-F(x_{1}^{+},y)\] (B.29b) \[\leq F(p,y)+\langle w_{2},x_{1}^{+}-p\rangle+\frac{1}{2K}\|w_{2}-w _{1}\|_{*}^{2}-\frac{K}{2}\|x_{1}^{+}-x\|^{2}\,.\] (B.29c)

Proof.: By Lemma B.2, we have

\[F(p,y_{2}^{+})=F(p,y)+\langle w_{2},x_{2}^{+}-p\rangle-F(x_{2}^{+},y)\] (B.30)

so (B.29a) follows by writing \(\langle w_{2},x_{2}^{+}-p\rangle=\langle w_{2},x_{1}^{+}-p\rangle+\langle w_{ 2},x_{2}^{+}-x_{1}^{+}\rangle\), and (B.29b) follows from the three-point identity (B.23) for the Fenchel coupling. Finally, for (B.29c), the Fenchel-Young inequality in Peter-Paul form yields

\[\langle w_{2}-w_{1},x_{2}^{+}-x_{1}^{+}\rangle\leq\frac{1}{2K}\|w_{2}-w_{1}\|_ {*}^{2}+\frac{K}{2}\|x_{2}^{+}-x_{1}^{+}\|^{2}\] (B.31)

and our claim follows again by invoking Proposition B.2 to write

\[\frac{K}{2}\|x_{2}^{+}-x_{1}^{+}\|^{2}-F(x_{2}^{+},y_{1}^{+})-F(x_{1}^{+},y) \leq-F(x_{1}^{+},y)\leq-\frac{K}{2}\|x_{1}^{+}-x\|^{2}\] (B.32)

and then substituting the result in (B.31) 

Lemmas B.2 and B.3 will be responsible for most of the heavy lifting to derive a Lyapunov function for (FTRL+). We discuss the relevant details in Appendix D.

We conclude this section with a variational characterization of the abstract update (B.22) in the case where \(\partial h\) of \(h\) admits a continuous selection - or, alternatively, \(h\) is smooth in the sense of (17).

**Lemma B.4**.: _Fix some \(y,y^{+}\in\mathcal{Y}\), and let \(x^{+}=Q(y^{+})\). Then, for all \(p\in\mathcal{X}\), we have_

\[\langle y^{+}-y,p-x^{+}\rangle\leq\langle\nabla h(x^{+})-y,p-x^{+}\rangle\,.\] (B.33)

Proof.: Invoking (B.14) in Proposition B.1 with \(y\gets y^{+}\), \(x\gets x^{+}\), and \(x^{\prime}\gets p\), we get

\[\langle y^{+},p-x^{+}\rangle\leq\langle\nabla h(x^{+}),p-x^{+}\rangle\,.\] (B.34)

Our claim then follows by subtracting \(\langle y,p-x^{+}\rangle\) from both sides of the above.

Continuous-time analysis

### Dynamical systems notions

To fix notation, we recall here some basics from the theory of dynamical systems, roughly following [2, 51]. In this section, \(\mathcal{M}\) is an open subset of a Euclidean space of dimension \(d\).

We consider a system of ordinary differential equations (ODEs) of the form

\[\dot{x}(t)=X(x(t))\,,\] (DS)

where \(x(t)\) is a curve in \(\mathcal{M}\) defined on an open interval \(\mathcal{I}\subseteq\mathds{R}\) (that without loss of generality we assume to contain \(0\)), and \(X\colon\mathcal{M}\to\mathds{R}^{d}\) is a smooth function. The function \(X\) is called _vector field_ because it assigns a vector \(X(x)\) to each point \(x\) in \(\mathcal{M}\), and (DS) is called _dynamical system generated by \(X\)_.

Given \(x_{0}\in\mathcal{M}\), an _orbit with initial condition_\(x_{0}\) is a solution \(x(t)\) of (DS) with \(x(0)=x_{0}\). The _flow generated by \(X\)_ is the smooth function \(\Theta\colon\mathcal{I}\times\mathcal{M}\to\mathcal{M}\) such that \(\Theta_{0}(x_{0})=x_{0}\) for all \(x_{0}\in\mathcal{M}\) and \(\frac{d}{dt}\Theta_{t}(x)=X(\Theta_{t}(x))\) for all \(t\in\mathcal{I}\). In words, \(\Theta_{t}(x_{0})\) is the orbit \(x(t)\) with initial condition \(x_{0}\); the existence and uniqueness of this function is guaranteed by the existence and uniqueness theorem of solutions of ordinary differential equations.

A flow \(\Theta\) is called _volume-preserving_ if \(\operatorname{vol}(\Theta_{t}(\mathcal{U}))=\operatorname{vol}(\mathcal{U})\) for any (Lebesgue) measurable subset \(\mathcal{U}\subseteq\mathcal{M}\) and all \(t\in\mathcal{I}\). Liouville's theorem gives a sufficient condition for a flow to be volume-preserving based on the _divergence_ of its generating field:11

Footnote 11: Recall here that the divergence is a differential operator mapping a vector field \(X\) on \(\mathcal{M}\) to the real-valued function \(\operatorname{div}X(x):=\sum_{\alpha=1}^{d}\partial_{\alpha}X^{\alpha}(x)\), where \(\partial_{\alpha}\) is a shorthand for the partial derivative \(\partial/\partial x_{\alpha}\)

**Theorem** (Liouville).: _If \(\operatorname{div}X\equiv 0\) then the flow generated by \(X\) is volume-preserving._

Volume-preserving flows are closely related to recurrent dynamical patterns. A point \(x\in\mathcal{M}\) is said to be _recurrent_ under (DS) if, for every neighborhood \(\mathcal{U}\) of \(x\in\mathcal{M}\), there exists an increasing sequence of time \(t_{n}\uparrow\infty\) such that \(\Theta_{t_{n}}(x)\) is defined and falls in \(\mathcal{U}\) for all \(n\). Moreover, (DS) is said to be _Poincare recurrent_ if almost every point \(x\in\mathcal{M}\) is recurrent. The celebrated Poincare recurrence theorem gives a sufficient condition for a dynamical system to be Poincare recurrent:

**Theorem** (Poincare).: _Let \(X\) be a smooth vector field on \(\mathcal{M}\). If the flow induced by \(X\) is volume-preserving and all the orbits of (DS) are bounded, then (DS) is Poincare recurrent._

### Basic properties of FTRL

In this section we survey some of the properties of the follow-the-regularized-leader learning scheme in a continuous-time, multi-agent setting, in line with the presentations of [16, 38, 41]. For ease of reference we recall here some of the notions introduced in Appendix B and in Sections 2 and 3 from the main body of the paper.

Let \(\Gamma=\Gamma(\mathcal{N},\mathcal{A},u)\) be a finite normal form game, and let \(v\) denote its payoff field. The game's strategy space is \(\mathcal{X}=\prod_{j\in\mathcal{N}}\Delta(\mathcal{A}_{j})\subseteq\mathcal{ V}:=\prod_{j}\mathds{R}^{\mathcal{A}_{j}}\), and the game's payoff space is \(\mathcal{Y}:=\mathcal{Y}^{*}\). The payoff field is a map \(v\colon\mathcal{V}\to\mathcal{Y}\) that evaluated at a strategy \(x\in\mathcal{X}\) acts linearly on any \(x^{\prime}\in\mathcal{X}\) by

\[\langle v(x),x^{\prime}\rangle =\sum_{i\in\mathcal{N}}\langle v_{i}(x),x^{\prime}_{i}\rangle= \sum_{i\in\mathcal{N}}\sum_{\alpha_{i}\in\mathcal{A}_{i}}v_{i\alpha_{i}}(x)\,x ^{\prime}_{i\alpha_{i}}\] (C.1) \[=\sum_{i\in\mathcal{N}}u_{i}(x^{\prime}_{i},x_{-i})\in\mathds{R}\,.\]

Assume now that \(\Gamma\) is played continuously over time. As discussed in Section 3, the main idea behind the follow-the-regularized-leader learning scheme is that, at any given time \(t\geq 0\), each player \(i\in\mathcal{N}\) tracks their cumulative payoff up to time \(t\) and plays a "regularized" best response strategy in light of this information. Concretely, given a cumulative payoff vector \(y_{i}(t)\in\mathcal{Y}_{i}\), each player \(i\in\mathcal{N}\) selects this optimal strategy \(x_{i}(t)\in\mathcal{X}_{i}\) by means of a _regularized best response_ map \(Q_{i}\colon\mathcal{Y}_{i}\to\mathcal{X}_{i}\), a single-valued analogue of the best-response correspondence \(y_{i}\mapsto\arg\max_{x_{i}\in\mathcal{X}_{i}}\langle y_{i},x_{i}\rangle\). A standard way [57] of obtaining such map is to introduce a _regularizer function_\(h_{i}\colon\mathcal{X}_{i}\to\mathds{R}\) that is (\(i\)) continuous on \(\mathcal{X}_{i}\), (\(ii\)) smooth on \(\operatorname{ri}\mathcal{X}_{i}\), the relative interior of \(\mathcal{X}_{i}\), and (\(iii\)) strongly convex on \(\mathcal{X}_{i}\) (as per Eq. (B.4)); and to consider the induced _choice map_\(Q_{i}\colon\mathcal{Y}_{i}\to\mathcal{X}_{i}\) defined by

\[Q_{i}(y_{i})=\arg\max_{x_{i}\in\mathcal{X}_{i}}\langle\{y_{i},x_{i}\}-h_{i}(x_ {i})\rangle\quad\text{for all }y_{i}\in\mathcal{Y}_{i}\,. \tag{6}\]

By Proposition B.1, \(Q_{i}\) is well-defined and Lipschitz continuous, and it coincides with the differential \(\nabla h_{i}^{*}\) of \(h_{i}^{*}\colon\mathcal{Y}_{i}\to\mathds{R}\), the _convex conjugate_ of \(h_{i}\).

In a continuous time setting, this regularized learning scheme translates into the following implicit equations of motion, which govern the evolution of the cumulative payoff \(y(t)\in\mathcal{Y}\) and of the mixed strategy profile \(x(t)\in\mathcal{X}\) as the players attempt to maximize their payoff over time:

\[y_{i\alpha_{i}}(t)=y_{i\alpha_{i}}(0)+\int_{0}^{t}v_{i\alpha_{i}}(x(\tau))\;d \tau\quad\text{with}\quad x_{i}(t)=Q_{i}(y_{i}(t))\;,\] (C.2)

for all \(i\in\mathcal{N},\alpha_{i}\in\mathcal{A}_{i}\). A straightforward computation shows that this is equivalent to Eq. (5) from Section 3 in the main text, that governs the evolution of the mixed strategy \(x(t)\in\mathcal{X}\):

\[x_{i}(t)=\operatorname*{arg\,max}_{p_{i}\in\mathcal{X}_{i}}\left\{\int_{0}^{t }\!\!u_{i}(p_{i};x_{-i}(\tau))\;d\tau-h_{i}(p_{i})\right\}=\operatorname*{arg \,max}_{p_{i}\in\mathcal{X}_{i}}\left\{\int_{0}^{t}\langle v_{i}(x(\tau)),p_{i }\rangle\;d\tau-h_{i}(p_{i})\right\}. \tag{5}\]

Importantly, Eq. (C.2) can be cast in the form (DS) of a dynamical system in the game's payoff space. For each \(i\in\mathcal{N}\), differentiation with respect to \(t\) yields

\[\dot{y}_{i}(t)=v_{i}(x(t))\qquad x_{i}(t)=Q_{i}(y_{i}(t))\;,\] (FTRL-D)

and by aggregating the player indices we obtain the system of ODEs

\[\dot{y}=Y(y)\;,\] (C.3)

where \(Y\coloneqq v\circ Q:\mathcal{Y}\to\mathcal{Y}\) is a continuous vector field on \(\mathcal{Y}\); cf. Fig. 3.

Existence and uniqueness of a global solution \(y(t)\in\mathcal{Y}\) of Eq. (C.3) for any initial condition \(y(0)\in\mathcal{Y}\) are guaranteed by standard arguments [38, Prop. 3.1]; in line with the terminology of the previous section we will refer to such a solution as a _dual orbit_.

### Constant of motion for harmonic games

The following result shows that FTRL in harmonic games admits a constant of motion.

**Proposition C.1**.: _Let \(\Gamma=\Gamma(\mathcal{N},\mathcal{A},u)\) be a finite game and consider a vector \(m\in\mathbb{R}^{N}_{++}\) and a fully mixed strategy \(q\in\mathcal{X}\). Then the weighted Fenchel coupling \(F_{m,q}:\mathcal{Y}\to\mathbb{R}\) defined by_

\[F_{m,q}(y)\coloneqq\sum_{i}m_{i}F_{i}(q_{i},y_{i})=\sum_{i}m_{i}\left(h_{i}(q _{i})+h_{i}^{*}(y_{i})-\langle q_{i},y_{i}\rangle\right)\] (C.4)

_is a constant of motion under (FTRL-D) if and only if \(\Gamma\) is harmonic with strategic center \((m,q)\)._

Proof.: Let \(y(t)\) be a dual orbit. Then by chain rule

\[\frac{d}{dt}F_{m,q}(y(t))=\sum_{i}m_{i}\left\{\langle\nabla h_{i}^{*}(y_{i}), \dot{y}_{i}\rangle-\langle q_{i},\dot{y}_{i}\rangle\right\}=\sum_{i}m_{i} \left\langle x_{i}(t)-q_{i},v_{i}(x(t))\right\rangle\] (C.5)

where the second equality holds by (FTRL-D) and Eq. (B.11). Then, by the characterization of harmonic games in terms of a strategic center (HG-center), the time derivative of the weighted Fenchel coupling vanishes identically along a dual orbit of (FTRL-D) precisely if the underlying game is harmonic. 

The existence of this constant of motion is fundamental for proving Theorem 2, i.e., the Poincare recurrence of continuous-time FTRL in harmonic games. With this key element established, the remainder of this appendix closely follows the proof technique described by [41] for the analogous result in the context of two-player zero-sum games.

### FTRL in the space of payoff differences

For any initial condition \(y(0)\in\mathcal{Y}\), a dual orbit of (FTRL-D) induces a curve \(x(t)=Q(y(t))\) in the game's strategy space \(\mathcal{X}\) which solves Eq. (5) for all \(t\geq 0\); in the following we will refer to such curve as _trajectory of play_. Crucially, a trajectory of play is in general _not_ the global solution of a dynamical system \(\dot{x}=X(x)\) for some vector field \(X\colon\mathcal{X}\to\mathcal{X}\) in the game's strategy space. The reason for this is that the map \(Q\colon\mathcal{Y}\to\mathcal{X}\) is not necessarily invertible, so there is in general no way to identify a unique a vector field \(X\) on \(\mathcal{X}\) that is _related_ to the vector field \(Y\) on \(\mathcal{Y}\) via \(Q\).

**Related vector fields and induced dynamical systems.** The concept of vector fields related by a smooth map is standard in differential geometry (e.g., [34, p. 181]). Let \(\mathcal{M},\mathcal{M}^{\prime}\) be open subsets of Euclidean space: given a vector field \(Y\) on \(\mathcal{M}\) and a smooth map \(F\colon\mathcal{M}\to\mathcal{M}^{\prime}\), a vector field \(X\) on \(\mathcal{M}^{\prime}\) is called _\(F\)-related to \(Y\)_ if, for all \(y\in\mathcal{M}\), \((\operatorname{Jac}F)_{y}\cdot Y(y)=X(x)\), with \(x=F(y)\). Here \(\operatorname{Jac}F\) is the Jacobian matrix of \(F\), and \(\cdot\) represents matrix-vector multiplication. If \(F\) is invertible then such vector field exists always and is unique; else, it might exist and not be unique, or not exist at all.

Vector fields that are related via a smooth map are useful inasmuch as they generate "compatible" dynamical systems:

**Lemma C.2**.: _Let \(F\colon\mathcal{M}\to\mathcal{M}^{\prime}\) be a smooth map between open subsets of Euclidean spaces, and let \(\dot{y}=Y(y)\) be a dynamical system on \(\mathcal{M}\). Let \(y(t)\) be an orbit with initial condition \(y_{0}\in\mathcal{M}\), and consider the curve on \(\mathcal{M}^{\prime}\) defined by \(x(t)\coloneqq F(y(t))\). If there exists a vector field \(X\) on \(\mathcal{M}^{\prime}\) that is \(F\)-related to \(Y\), then the curve \(x(t)\) is an orbit of the dynamical system \(\dot{x}=X(x)\) with initial condition \(x_{0}=F(y_{0})\)._

Proof.: By chain rule,

\[\frac{d}{dt}x(t)=\frac{d}{dt}F(y(t))=(\operatorname{Jac}F)_{y(t)}\cdot\dot{y}( t)=(\operatorname{Jac}F)_{y(t)}\cdot Y(y(t))=X(x(t))\,,\] (C.6)

where the last equality follows by the assumption that \(X\) is \(F\)-related to \(Y\). 

In the following, if \(F\colon\mathcal{M}\to\mathcal{M}^{\prime}\) is a smooth function between open subsets of Euclidean spaces, and \(Y,X\) are vector fields fulfilling the assumptions of Lemma C.2, we say that the dynamical system \(\dot{y}=Y(y)\) on \(\mathcal{M}\)_induces the dynamical system \(\dot{x}=X(x)\) on \(\mathcal{M}^{\prime}\) via \(F\)_.

**FTRL induced in the space of payoff differences.** The choice map \(Q\colon\mathcal{Y}\to\mathcal{X}\) is in general not smooth, and neither injective nor surjective [16, Sec.3], so it generally does not allow to induce the dynamical system (C.3) from the game's payoff space \(\mathcal{Y}\) to the game's strategy space \(\mathcal{X}\). 12 In other words, the learning process (FTRL-D) in a finite game gives rise to a dynamical system in the game's payoff space \(\mathcal{Y}\), to which the theorems presented in Appendix C.1 can in principle be applied; however, it can be shown that the orbits of Eq. (C.3) in \(\mathcal{Y}\) are _not_ bounded, preventing the application of Poincare's theorem. Furthermore, the dual orbits do not convey direct information on the day-to-day behavior of the players, due to the lack of invertibility of the choice map.

Footnote 12: A detailed treatment of the conditions under which a trajectory of play \(x(t)\) actually _is_ a solution of dynamical system in the game’s strategy space \(\mathcal{X}\) is beyond the scope of this work; we refer the interested reader to [38, 39] for an in-depth treatment.

Conversely, the objects of interest from a dynamical, learning viewpoint - that is, the trajectories of play in the game's strategy space \(\mathcal{X}\) - present technical difficulties and do not easily fit the dynamical systems framework depicted in Appendix C.1. In the following we show how these difficulties can be circumvented by analyzing the dynamics induced by (FTRL-D) in yet a third space \(\mathcal{Z}\), that arises by taking the _differences_ between payoffs - rather than their absolute values - as the objects of study.

To make this precise, given the game \(\Gamma=\Gamma(\mathcal{N},\mathcal{A},u)\) fix a benchmark strategy \(\hat{\alpha}_{i}\in\mathcal{A}_{i}\) for every player \(i\in\mathcal{N}\), and consider the hyperplane \(\mathcal{Z}_{i}\coloneqq\{z_{i}\in\mathbb{R}^{A_{i}}:z_{i\dot{\alpha}_{i}}=0\} \subset\mathbb{R}^{A_{i}}\). Clearly, \(\mathcal{Z}_{i}\cong\mathbb{R}^{A_{i}-1}\). Each player's strategy space \(\mathcal{Y}_{i}=\mathbb{R}^{A_{i}}\) can be mapped onto \(\mathcal{Z}_{i}\) by the linear operator

\[\Pi_{i}\colon\mathcal{Y}_{i}\to\mathcal{Z}_{i}\quad\text{with}\quad z_{i\alpha _{i}}=y_{i\alpha_{i}}-y_{i}\hat{\alpha}_{i}\] (C.7)

for all \(\alpha_{i}\in\mathcal{A}_{i}\).

Figure 3: **FTRL diagram**. Commutative diagram of the maps discussed in Appendices C.2–C.4; note in particular that \(v\circ Q\) is a vector field on \(\mathcal{Y}\). The notation \(\mathcal{X}\hookrightarrow\mathcal{V}\) is equivalent to \(\mathcal{X}\subseteq\mathcal{V}\).

\(\Pi_{i}\) is clearly smooth, and a standard check shows that \(\Pi_{i}\) is surjective and not injective: \(\ker\Pi_{i}=\{y_{i}:y_{\alpha_{i}}=y_{i\beta_{i}}\text{ for all }\alpha_{i},\beta_{i} \in\mathcal{A}_{i}\}\) is the \(1\)-dimensional linear subspace spanned by the vector \(\mathbf{1}_{i}=(1,\ldots,1)\in\mathcal{Y}_{i}\); and \(\Pi^{-1}(z_{i})=z_{i}+\ker\Pi_{i}\) for any \(z_{i}\in\mathcal{Z}_{i}\). In particular, for all \(y_{i},y^{\prime}_{i}\in\mathcal{Y}_{i}\), we have that \(\Pi_{i}(y_{i})=\Pi_{i}(y^{\prime}_{i})\) if and only if \(y_{i}-y^{\prime}_{i}\) is proportional to \(\mathbf{1}_{i}\).

Since every \(z_{i}\in\mathcal{Z}_{i}\) is the image of some payoff \(y_{i}\) via \(\Pi_{i}\), the space \(\mathcal{Z}\coloneqq\prod_{j}\mathcal{Z}_{j}\) is called the game's _payoff-difference space_; we will denote by \(\Pi\) the product map \(\Pi\equiv\prod_{j}\Pi_{j}\), i.e., (cf. Fig. 3)

\[\Pi\colon\mathcal{Y}\to\mathcal{Z},\quad\Pi(y)\coloneqq(\Pi_{i}(y_{i}))_{i\in \mathcal{N}}\,.\] (C.8)

**Lemma C.3**.: _The choice map \(Q\colon\mathcal{Y}\to\mathcal{X}\) is invariant on the level sets of \(\Pi\)._

Proof.: Let \(y,y^{\prime}\in\mathcal{Y}\). By the discussion above, \(\Pi(y)=\Pi(y^{\prime})\) iff \(y^{\prime}_{i}-y_{i}=\lambda\mathbf{1}_{i}\) for some \(\lambda\in\mathbb{R}\). Then for each \(i\in\mathcal{N}\),

\[Q_{i}(y^{\prime}_{i})=\operatorname*{arg\,max}_{x_{i}\in\mathcal{X}_{i}}\{ \langle y^{\prime}_{i},x_{i}\rangle-h_{i}(x_{i})\}=\operatorname*{arg\,max}_{x _{i}\in\mathcal{X}_{i}}\{\langle y_{i},x_{i}\rangle+\lambda\langle\mathbf{1}_ {i},x_{i}\rangle-h_{i}(x_{i})\}=Q_{i}(y_{i})\,.\qed\]

**Proposition C.4**.: _The dynamical system (C.3) in the game's payoff space \(\mathcal{Y}\) induces a dynamical system_

\[\dot{z}=Z(z)\] (C.9)

_in the game's payoff-difference space \(\mathcal{Z}\) via the map (C.8)._

Proof.: By the discussion in the previous section (and in particular Lemma C.2), if we exhibit a vector field \(Z\) on \(\mathcal{Z}\) that is \(\Pi\)-related to \(Y\), then our proof is complete. Thus we look for a vector field \(Z\) such that, for all \(y\in\mathcal{Y}\),

\[(\operatorname{Jac}\Pi)_{y}\cdot Y(y)=Z(z),\] (C.10)

with \(z=\Pi(y)\). By Eq. (C.7), \((\operatorname{Jac}\Pi_{i})_{\alpha_{i}\beta_{i}}=\delta_{\alpha_{i}\beta_{i}} -\delta_{\dot{\alpha}_{i}\beta_{i}}\). Since \(Y=v\circ Q\), the sought-after vector field \(Z\) must fulfill, for all \(y\in\mathcal{Y}\) and all \(\alpha_{i}\in\mathcal{A}_{i}\),

\[(v_{i\alpha_{i}}-v_{i\dot{\alpha}_{i}})\circ Q_{i}(y_{i})=Z_{i\alpha_{i}}(z_{ i})\,,\] (C.11)

with \(z=\Pi(y)\). For each \(i\in\mathcal{N}\) define now (cf. Fig. 3)

\[\hat{Q}_{i}\colon\mathcal{Z}_{i}\to\mathcal{X}_{i},\quad\hat{Q}_{i}(z_{i})=Q( y_{i})\] (C.12)

for _any_\(y_{i}\in\Pi_{i}^{-1}(z_{i})\), and denote by \(\hat{Q}\colon\mathcal{Z}\to\mathcal{X}\) the induced product map. Such map exists since \(\Pi_{i}\) is surjective, and is well-defined by Lemma C.3. It follows that the vector field on \(\mathcal{Z}\) defined by

\[Z_{i\alpha_{i}}(z_{i})\coloneqq(v_{i\alpha_{i}}-v_{i\dot{\alpha}_{i}})\circ \hat{Q}_{i}(z_{i})\] (C.13)

for all \(i\in\mathcal{N},z_{i}\in\mathcal{Z}_{i},\alpha_{i}\in\mathcal{A}_{i}\) fulfills Eq. (C.11), and is hence \(\Pi\)-related to \(Y\). 

This result shows that, for every dual orbit \(y(t)\) of Eq. (C.3) with initial condition \(y_{0}\in\mathcal{Y}\), the curve \(z(t)=\Pi(y(t))\) is an orbit of the dynamical system (C.9) in \(\mathcal{Z}\) with initial condition \(\Pi(y_{0})\). To conclude this section we give a result implying that if the weighted Fenchel coupling (C.4) is a constant of motion constant then the solution orbits of (C.9) in \(\mathcal{Z}\) are bounded.

**Lemma C.5**.: _For any \(i\in\mathcal{N}\), let \(y_{i,n}\) be a sequence in \(\mathcal{Y}_{i}\), and let \(p_{i}\) be a point in the relative interior of \(\mathcal{X}_{i}\). If \(\sup_{n}\lvert h_{i}^{*}(y_{i,n})-\langle y_{i,n},p_{i}\rangle\rvert<\infty\), then also the score differences remain bounded, i.e., \(\lvert y_{i\alpha_{i},n}-y_{i\beta_{i},n}\rvert<\infty\) for all \(\alpha_{i},\beta_{i}\in\mathcal{A}_{i}\) and all \(n\)._

Proof.: See [41, Appendix D]. 

**Lemma C.6**.: _If the weighted Fenchel coupling (C.4) is a constant of motion under (FTRL-D) for some fully mixed \(p\in\mathcal{X}\) then the orbits of \(\dot{z}=Z(z)\) as in Eq. (C.9) are bounded in \(\mathcal{Z}\)._

Proof.: Assume that \(F_{m,q}(y)=\sum_{i}m_{i}F_{i}(p_{i},y_{i})=\sum_{i}m_{i}\left(h_{i}(p_{i})+h_ {i}^{*}(y_{i})-\langle p_{i},y_{i}\rangle\right)\) is a constant of motion for (FTRL-D) for some fully mixed \(p\in\mathcal{X}\) and some \(m\in\mathbb{R}^{N}_{++}\). Let \(y(t)\) be an orbit of (FTRL-D) in \(\mathcal{Y}\), and let \(y_{i,n}\coloneqq y_{i}(t_{n})\) for any sequence of time \(t_{n}\). Let furthermore \(F_{i,n}\coloneqq h_{i}^{*}(y_{i,n})-\langle p_{i},y_{i,n}\rangle\). Then \(\sup_{n}\lvert F_{i,n}\rvert<\infty\). By Lemma C.5, this implies that \(\lvert z_{i\alpha_{i},n}\rvert<\infty\) for all \(\alpha_{i}\in\mathcal{A}_{i}\), all \(i\in\mathcal{N}\), and all \(n\).

### Recurrence of FTRL in harmonic games

We now have all the ingredients to prove that almost every trajectory of play \(x(t)\) of (FTRL-D) in harmonic games returns arbitrarily close to its starting point infinitely often.

**Theorem 2**.: _Suppose \(\Gamma\) is harmonic. Then almost every orbit \(x(t)\) of (FTRL-D) returns arbitrarily close to its starting point infinitely often: specifically, for (Lebesgue) almost every initial condition \(x(0)=Q(y(0))\in\mathcal{X}\), there exists an increasing sequence of times \(t_{n}\uparrow\infty\) such that \(x(t_{n})\to x(0)\)._

Proof of Theorem 2.: The proof relies on the following steps:

1. the vector field \(Z\) defined in Eq. (C.13) has vanishing divergence, so its induced flow is volume-preserving in \(\mathcal{Z}\) by Liouville's theorem;
2. the orbits of the dynamical system \(\dot{z}=Z(z)\) of Eq. (C.9) are bounded in \(\mathcal{Z}\) since the weighted Fenchel coupling (C.4) is a constant of motion for FTRL in harmonic games;
3. the dynamical system \(\dot{z}=Z(z)\) is recurrent in \(\mathcal{Z}\) by Poincare theorem;
4. by continuity of Eq. (C.12), almost every trajectory of play \(x(t)\) of (FTRL-D) with initial condition in the image of \(\hat{Q}\) returns arbitrarily close to its starting point infinitely often.

Indeed, \(\operatorname{div}Z(z)=\sum_{i}\sum_{\alpha_{i}}\frac{\partial}{\partial z_{ \dot{\alpha}i\alpha_{i}}}((v_{i\alpha_{i}}-v_{i\dot{\alpha}_{i}})\circ\hat{Q} _{i}(z_{i}))\). For the first term, by chain rule,

\[\operatorname{div}Z(z) =\sum_{i}\sum_{\alpha_{i}}\frac{\partial v_{i\alpha_{i}}}{ \partial z_{i\alpha_{i}}}(\hat{Q}_{i}(z_{i}))=\sum_{i}\sum_{\alpha_{i}}\sum_{j }\sum_{\beta_{j}}\frac{\partial v_{i\alpha_{i}}}{\partial x_{j\beta_{j}}}(\hat {Q}(z))\frac{\partial\hat{Q}_{j\beta_{j}}}{\partial z_{i\alpha_{i}}}(z)\] \[=\sum_{i}\sum_{\alpha_{i}}\sum_{\beta_{i}}\frac{\partial v_{i \alpha_{i}}}{\partial x_{i\beta_{i}}}(\hat{Q}(z))\frac{\partial\hat{Q}_{i\beta _{i}}}{\partial z_{i\alpha_{i}}}(z)\equiv 0\]

since \(\frac{\partial v_{i\alpha_{i}}}{\partial x_{i\beta_{i}}}\equiv 0\) by multilinearity of the payoff functions. The second term yields identical result with \(\hat{\alpha}_{i}\leftarrow\alpha_{i}\), so we conclude that \(\operatorname{div}Z=0\). By Lemma C.6, the invariance of the weighted Fenchel coupling under (FTRL-D) implies that the payoff differences \(z_{i\alpha_{i}}(t)=y_{i\alpha_{i}}(t)-z_{i\dot{\alpha}_{i}}(t)\) remain bounded for all \(t\in[0,\infty)\). So, by Poincare theorem, the dynamical system \(\dot{z}=Z(z)\) is Poincare recurrent, i.e., there exists a sequence of time \(t_{n}\uparrow\infty\) such that \(\lim_{n\rightarrow\infty}z(t_{n})=z_{0}\) for almost every \(z_{0}\in\mathcal{Z}\). By continuity of (C.12), almost every trajectory of play \(x(t)=Q(y(t))=\hat{Q}(z(t))\) with \(x_{0}\in\operatorname{im}\hat{Q}\) fulfills \(\lim_{n\rightarrow\infty}x(t_{n})=x_{0}\), which concludes our proof by noting that the image of \(\hat{Q}\) is the same as the image of \(Q\). 

## Appendix D Discrete-time analysis

In this appendix, our aim is to provide the full proofs for the discrete-time guarantees of (FTRL+), as presented in Section 4. Our analysis hinges on a series of energy functions and associated template inequalities, which we introduce in the next section.

### Lyapunov functions and template inequalities for (FTRL+)

The main building block of our analysis is a suitable Lyapunov function for the discrete-time algorithmic template (FTRL+). Motivated by the continuous-time analysis of Appendix C, we begin by considering the player-specific Fenchel couplings

\[F_{i}(p_{i},y_{i})=h_{i}(p_{i})+h_{i}^{*}(y_{i})-\langle y_{i},p_{i}\rangle \quad\text{for }p_{i}\in\mathcal{X}_{i},y_{i}\in\mathcal{Y}_{i}\] (D.1)

induced by the regularizer \(h_{i}\) of player \(i\in\mathcal{N}\).

Suppose now that the game is harmonic relative to some measure \(\mu=(\mu_{1},\ldots,\mu_{N})\), let \(m_{i}=\sum_{\alpha_{i}\in A_{i}}\mu_{i\alpha_{i}}\) denote the mass of \(\mu_{i}\), and assume further that each player is running (FTRL+) with learning rate \(\eta_{i}>0\). Our analysis will hinge on the energy function

\[E(p,y)=\sum_{i\in\mathcal{N}}\frac{m_{i}}{\eta_{i}}F_{i}(p_{i},y_{i})\qquad p \in\mathcal{X},y\in\mathcal{Y}, \tag{18}\]

which, as we show below, satisfies the following template inequality:

**Proposition D.1**.: _Suppose that each player is running (FTRL+) with learning rate \(\eta_{i}>0\) in a harmonic game as above. Then, for all \(p_{i}\in\mathcal{X}_{i}\), \(i\in\mathcal{N}\), and all \(n=1,2,\dots\), the algorithm's energy \(E_{n}\coloneqq E(p,y_{n})\) enjoys the iterative bound:_

\[E_{n+1}\leq E_{n} +\sum_{i\in\mathcal{N}}m_{i}\langle v_{i}(x_{n+1/2}),x_{i,n+1/2}-p _{i}\rangle\] \[+\sum_{i\in\mathcal{N}}m_{i}\langle v_{i}(x_{n+1/2})-v_{i}(x_{n}),x_{i,n+1}-x_{i,n+1/2}\rangle\] \[+\sum_{i\in\mathcal{N}}m_{i}(1-\lambda_{i})\langle v_{i}(x_{n})-v _{i}(x_{n-1/2}),x_{i,n+1}-x_{i,n+1/2}\rangle\] \[-\sum_{i\in\mathcal{N}}\frac{m_{i}}{\eta_{i}}F_{i}(x_{i,n+1},y_{i,n+1/2})\] \[-\sum_{i\in\mathcal{N}}\frac{m_{i}}{\eta_{i}}F_{i}(x_{i,n+1/2},y_{ i,n})\.\] (D.2)

Proof.: We begin by applying the bound (B.29b) of Lemma B.3 with the array of substitutions

1. \(p\gets p_{i}\)
2. \(w_{1}\leftarrow\eta_{i}\hat{v}_{i,n}=\eta_{i}\lambda_{i}\,v_{i}(x_{n})+\eta_{ i}(1-\lambda_{i})\,v_{i}(x_{n-1/2})\)
3. \(w_{2}\leftarrow\eta_{i}\hat{v}_{i,n+1/2}=\eta_{i}v_{i}(x_{n+1/2})\)
4. \(y\gets y_{i,n}\) so \(x\gets Q_{i}(y_{i,n})=x_{i,n}\)
5. \(y_{1}^{+}\gets y_{i,n+1/2}\) so \(x_{1}^{+}\gets x_{i,n+1/2}\)
6. \(y_{2}^{+}\gets y_{i,n+1}\) so \(x_{2}^{+}\gets x_{i,n+1}\)

We then get

\[\langle w_{2}-w_{1},x_{2}^{-}-x_{1}^{+}\rangle =\eta_{i}\langle v_{i}(x_{n+1/2})-\lambda_{i}\,v_{i}(x_{n})-(1- \lambda_{i})\,v_{i}(x_{n-1/2}),x_{i,n+1}-x_{i,n+1/2}\rangle\] \[=\eta_{i}\langle v_{i}(x_{n+1/2})-v_{i}(x_{n}),x_{i,n+1}-x_{i,n+1 /2}\rangle\] \[\quad+\eta_{i}(1-\lambda_{i})\langle v_{i}(x_{n})-v_{i}(x_{n-1/2 }),x_{i,n+1}-x_{i,n+1/2}\rangle\] (D.3)

and hence, by (B.29b):

\[F_{i}(p_{i},y_{i,n+1})\leq F_{i}(p_{i},y_{i,n}) +\eta_{i}\langle v_{i}(x_{n+1/2}),x_{i,n+1/2}-p_{i}\rangle\] \[+\eta_{i}\langle v_{i}(x_{n+1/2})-v_{i}(x_{n}),x_{i,n+1}-x_{i,n+1 /2}\rangle\] \[+\eta_{i}\langle 1-\lambda_{i}\rangle\langle v_{i}(x_{n})-v_{i}(x_ {n-1/2}),x_{i,n+1}-x_{i,n+1/2}\rangle\] \[-F_{i}(x_{i,n+1/2},y_{i,n+1/2})\] \[-F_{i}(x_{i,n+1/2},y_{i,n})\.\] (D.4)

Accordingly, with \(E_{n}=E(p,y_{n})\), the bound (D.2) follows by multiplying both sides by \(m_{i}/\eta_{i}\) and summing over \(i\in\mathcal{N}\). 

Thanks to Proposition D.1, we are now in a position to state and prove the following summability guarantee for (FTRL+).

**Proposition D.2**.: _Suppose that each player in a harmonic game \(\Gamma\) with harmonic measure \(\mu\) is following (FTRL+) with learning rate \(\eta_{i}\leq m_{i}K_{i}\,[2(N+2)\max_{j}m_{j}G_{j}]^{-1}\). Then, for all \(T\), we have:_

\[\sum_{n=1}^{T}\lVert x_{n+1/2}-x_{n}\rVert^{2}+\sum_{n=2}^{T}\lVert x_{n}-x_{n -1/2}\rVert^{2}\leq\frac{2E_{1}}{(N+2)\max_{i}m_{i}G_{i}}\.\] (D.5)

_In particular, the sequences \(A_{n}\coloneqq\lVert x_{n+1/2}-x_{n}\rVert^{2}\) and \(B_{n}\coloneqq\lVert x_{n+1}-x_{n+1/2}\rVert^{2}\) are both summable._

Proof.: By reshuffling the terms of the template inequality (D.2), we get

\[\sum_{i\in\mathcal{N}}m_{i}\langle v_{i}(x_{n+1/2}),p_{i}-x_{i,n+1 /2}\rangle \leq E_{n}-E_{n+1}\] \[+\sum_{i\in\mathcal{N}}m_{i}\langle v_{i}(x_{n+1/2})-v_{i}(x_{n} ),x_{i,n+1}-x_{i,n+1/2}\rangle\] (D.6a)\[+\sum_{i\in\mathcal{N}}m_{i}(1-\lambda_{i})\langle v_{i}(x_{n})-v_{i} (x_{n-1/2}),x_{i,n+1}-x_{i,n+1/2}\rangle\] (D.6b) \[-\sum_{i\in\mathcal{N}}\frac{m_{i}}{\eta_{i}}F_{i}(x_{i,n+1},y_{i, n+1/2})-\sum_{i\in\mathcal{N}}\frac{m_{i}}{\eta_{i}}F_{i}(x_{i,n+1/2},y_{i,n})\,.\] (D.6c)

We now proceed to bound each term of (D.6) individually, paying no heed to make the resulting bounds as tight as possible.

Bounding (D.6a). By the Fenchel-Young inequality, we have:

\[\eqref{eq:Fenchel-Young-1} \leq\sum_{i\in\mathcal{N}}\frac{m_{i}}{2G_{i}}\|v_{i}(x_{n+1/2})- v_{i}(x_{n})\|_{*}^{2}+\sum_{i\in\mathcal{N}}\frac{m_{i}G_{i}}{2}\|x_{i,n+1}-x_{i,n +1/2}\|^{2}\] \[\leq\sum_{i\in\mathcal{N}}\frac{m_{i}G_{i}}{2}\|x_{n+1/2}-x_{n}\| ^{2}+\sum_{i\in\mathcal{N}}\frac{m_{i}G_{i}}{2}\|x_{i,n+1}-x_{i,n+1/2}\|^{2} \qquad\quad\text{\% $v_{i}(x)$ is $G_{i}$-Lipschitz}\] \[\leq\tfrac{1}{2}N\max_{i}m_{i}G_{i}\cdot\|x_{n+1/2}-x_{n}\|^{2}+ \tfrac{1}{2}\max_{i}m_{i}G_{i}\cdot\|x_{n+1}-x_{n+1/2}\|^{2}\] (D.7)

Bounding (D.6b). Again, by the Fenchel-Young inequality, we obtain:

\[\eqref{eq:Fenchel-Young-1} \leq\sum_{i\in\mathcal{N}}\frac{m_{i}(1-\lambda_{i})}{2G_{i}}\|v _{i}(x_{n})-v_{i}(x_{n-1/2})\|_{*}^{2}+\sum_{i\in\mathcal{N}}\frac{m_{i}(1- \lambda_{i})G_{i}}{2}\|x_{i,n+1}-x_{i,n+1/2}\|^{2}\] \[\leq\sum_{i\in\mathcal{N}}\frac{m_{i}(1-\lambda_{i})G_{i}}{2}\|x _{n}-x_{n-1/2}\|^{2}+\sum_{i\in\mathcal{N}}\frac{m_{i}(1-\lambda_{i})G_{i}}{2 }\|x_{i,n+1}-x_{i,n+1/2}\|^{2}\] \[\quad\text{\% $v_{i}(x)$ is $G_{i}$-Lipschitz}\] \[\leq\tfrac{1}{2}N\max_{i}m_{i}G_{i}\cdot\|x_{n}-x_{n-1/2}\|^{2}+ \tfrac{1}{2}\max_{i}m_{i}G_{i}\cdot\|x_{n+1}-x_{n+1/2}\|^{2}\] (D.8)

Bounding (D.6c). Finally, by the lower bound on the Fenchel coupling of Proposition B.2, we get:

\[-\sum_{i\in\mathcal{N}}\frac{m_{i}}{\eta_{i}}F_{i}(x_{i,n+1},y_{i,n+1/2})-\sum_{i\in\mathcal{N}}\frac{m_{i}}{\eta_{i}}F_{i}(x_{i,n+1/2},y_{i,n})\] \[\qquad\leq-\sum_{i\in\mathcal{N}}\frac{m_{i}K_{i}}{2\eta_{i}}\|x _{i,n+1}-x_{i,n+1/2}\|^{2}-\sum_{i\in\mathcal{N}}\frac{m_{i}K_{i}}{2\eta_{i}} \|x_{i,n+1/2}-x_{i,n}\|^{2}\qquad\quad\text{\% by \eqref{eq:Fenchel-Young-1}}\] \[\leq-\min_{i}\frac{m_{i}K_{i}}{2\eta_{i}}\cdot\llbracket\|x_{n+1} -x_{n+1/2}\|^{2}+\|x_{n+1/2}-x_{n}\|^{2}\rrbracket\] (D.9)

Thus, by folding Eqs. (D.7)-(D.9) back into (D.6), we obtain the bound

\[\sum_{i\in\mathcal{N}}m_{i}\langle v_{i}(x_{n+1/2}),p_{i}-x_{i,n+ 1/2}\rangle \leq E_{n}-E_{n+1}\] \[+\frac{1}{2}\bigg{(}N\max_{i}m_{i}G_{i}-\min_{i}\frac{m_{i}K_{i}}{ \eta_{i}}\bigg{)}\,\|x_{n+1/2}-x_{n}\|^{2}\] \[+\frac{1}{2}\bigg{(}2\max_{i}m_{i}G_{i}-\min_{i}\frac{m_{i}K_{i}}{ \eta_{i}}\bigg{)}\,\|x_{n+1}-x_{n+1/2}\|^{2}\] \[+\frac{1}{2}N\max_{i}m_{i}G_{i}\cdot\|x_{n}-x_{n-1/2}\|^{2}\,.\] (D.10)

Now, if we instantiate (D.10) to \(p\gets q\) where \(q\) is the strategic center of \(\Gamma\), its left-hand side (LHS) will vanish by (HG-center). Hence, summing over all \(n=1,2,\ldots,T\), (D.10) ultimately yields

\[0\leq E_{1} +\frac{1}{2}\bigg{(}N\max_{i}m_{i}G_{i}-\min_{i}\frac{m_{i}K_{i}}{ \eta_{i}}\bigg{)}\sum_{n=1}^{T}\|x_{n+1/2}-x_{n}\|^{2}\] \[+\frac{1}{2}\bigg{(}(N+2)\max_{i}m_{i}G_{i}-\min_{i}\frac{m_{i}K_{i }}{\eta_{i}}\bigg{)}\,\sum_{n=2}^{T}\|x_{n}-x_{n-1/2}\|^{2}\] \[+\frac{1}{2}\bigg{(}2\max_{i}m_{i}G_{i}-\min_{i}\frac{m_{i}K_{i}}{ \eta_{i}}\bigg{)}\,\|x_{T+1}-x_{T+1/2}\|^{2}\]\[\langle v_{i}(x_{n+1/2}),p_{i}-x_{i,n+1/2}\rangle \leq\frac{1}{\eta_{i}}\left[F_{i}(p_{i},y_{i,n})-F_{i}(p_{i},y_{i,n +1})\right]\] \[+\langle v_{i}(x_{n+1/2})-v_{i}(x_{n}),x_{i,n+1}-x_{i,n+1/2}\rangle\] \[+(1-\lambda_{i})\langle v_{i}(x_{n})-v_{i}(x_{n-1/2}),x_{i,n+1}-x _{i,n+1/2}\rangle\] \[-\frac{1}{\eta_{i}}F_{i}(x_{i,n+1},y_{i,n+1/2})-\frac{1}{\eta_{i} }F_{i}(x_{i,n+1/2},y_{i,n})\] (D.15)

and hence, by a repeated application of the Fenchel-Young inequality in its Peter-Paul form:

\[\langle v_{i}(x_{n+1/2}),p_{i}-x_{i,n+1/2}\rangle \leq\frac{1}{\eta_{i}}\left[F_{i}(p_{i},y_{i,n})-F_{i}(p_{i},y_{i,n+1})\right]\] \[+\frac{1}{2G_{i}}\|v_{i}(x_{n+1/2})-v_{i}(x_{n})\|_{*}^{2}+\frac{ G_{i}}{2}\|x_{i,n+1}-x_{i,n+1/2}\|^{2}\] \[+\frac{1-\lambda_{i}}{2G_{i}}\|v_{i}(x_{n})-v_{i}(x_{n-1/2})\|_{*} ^{2}+\frac{(1-\lambda_{i})G_{i}}{2}\|x_{i,n+1}-x_{i,n+1/2}\|^{2}\] \[-\frac{K_{i}}{2\eta_{i}}\left[\|x_{i,n+1}-x_{i,n+1/2}\|^{2}+\|x_{ i,n+1/2}-x_{i,n}\|^{2}\right].\] (D.16)

Hence, by using the Lipschitz continuity of \(v_{i}\), we finally get

\[\langle v_{i}(x_{n+1/2}),p_{i}-x_{i,n+1/2}\rangle \leq\frac{1}{\eta_{i}}\left[F_{i}(p_{i},y_{i,n})-F_{i}(p_{i},y_{i, n+1})\right]\] \[+\frac{G_{i}}{2}\|x_{n+1/2}-x_{n}\|^{2}+\frac{G_{i}}{2}\|x_{i,n+1 }-x_{i,n+1/2}\|^{2}\] \[+\frac{G_{i}}{2}\|x_{n}-x_{n-1/2}\|^{2}+\frac{G_{i}}{2}\|x_{i,n+1 }-x_{i,n+1/2}\|^{2}\]\[-\frac{K_{i}}{2\eta_{i}}\big{[}\|x_{i,n+1}-x_{i,n+1/2}\|^{2}+\|x_{i,n+1/2}-x_{i,n}\| ^{2}\big{]}\] (D.17)

Thus, summing over \(n=1,2,\dots,T\), and keeping in mind that our assumptions for \(\eta_{i}\) also give \(G_{i}<K_{i}/(2\eta_{i})\), we finally get

\[\sum_{n=1}^{T}\langle v_{i}(x_{n+1/2}),p_{i}-x_{i,n+1/2}\rangle\leq\frac{H_{i} }{\eta_{i}}+\frac{G_{i}}{2}\left[\sum_{n=1}^{T}\|x_{n+1/2}-x_{n}\|^{2}+\sum_{n =2}^{T}\|x_{n}-x_{n-1/2}\|^{2}\right]\] (D.18)

where we used the fact that \(F_{i}(p_{i},0)=h(p)-\min h_{i}\leq\max h_{i}-\min h_{i}\eqqcolon H_{i}\). Our result then follows by invoking (D.5) and using the fact that \(m_{i}G_{i}\leq\max_{j}m_{j}G_{j}\) for all \(i\in\mathcal{N}\). 

### Proof of Theorem 4

With all this in hand, we are finally in a position to prove our main equilibrium convergence result for (FTRL+). For convenience, we restate the relevant theorem below.

**Theorem 4**.: _Suppose that each player in a harmonic game \(\Gamma\) follows (FTRL+) with learning rate \(\eta_{i}\leq m_{i}K_{i}\big{[}2(N+2)\max_{j}m_{j}G_{j}\big{]}^{-1}\) and payoff models as per (13a) and (15). Then \(x_{n}\) converges to the set of Nash equilibria of \(\Gamma\)._

Proof.: Our proof proceeds in a series of steps, as detailed below.

_Step 1: Convergence of energy levels._ We begin by showing that the energy \(E_{n}\equiv E(q,y_{n})\) of (FTRL+) relative to the game's harmonic center converges to some finite value \(E_{\infty}\). That this is so follows from a well-known property of quasi-Fejer sequences [10, Lemma 3.1], whose proof we reproduce below for completeness.

Indeed, by Eq. (D.10) and Proposition D.2, we have

\[E_{n+1}\leq E_{n}+\varepsilon_{n}\] (D.19)

with \(\varepsilon_{n}\), \(n=1,2,\dots\) summable. Letting \(E^{\prime}_{n}=E_{n}+\sum_{k=n}^{\infty}\varepsilon_{k}\), we further get

\[E^{\prime}_{n+1}=E_{n+1}+\sum_{k=n+1}^{\infty}\varepsilon_{k}\leq E_{n}+\sum_{ k=n}^{\infty}\varepsilon_{k}=E^{\prime}_{n}\] (D.20)

by (D.19), so \(E^{\prime}_{n}\) converges. Since \(\varepsilon_{n}\) is summable, it follows that \(E_{n}\) also converges, as claimed. \(\diamond\)

_Step 2: Boundedness of score differences._ We now proceed to show that the normalized score differences \(z_{n}=\Pi(y_{n})\) where \(\Pi\) is the normalization operator (C.8) are bounded. Indeed, by the definition of \(E_{n}=E(q,y_{n})=\sum_{i\in\mathcal{N}}(m_{i}/\eta_{i})F_{i}(q_{i},y_{i,n})\), it follows that \(\sup_{n}F_{i}(q_{i},y_{i,n})<\infty\) for all \(i\in\mathcal{N}\). Thus, by Lemma C.5, we conclude that each component of \(z_{n}\) is bounded, so \(z\) is itself bounded. \(\diamond\)

_Step 3: Convergent subsequences of (FTRL+)._ We now observe that (FTRL+) enjoys the following series of properties:

1. The sequence \(z_{n}=\Pi(y_{n})\) admits a subsequence \(z_{n_{k}}\) that converges to some limit point \(z_{\infty}\in\mathcal{Z}\) (a consequence of the fact that \(z_{n}\) is bounded, see above).
2. In turn, this implies that the subsequence \(x_{n_{k}}=Q(y_{n_{k}})=\hat{Q}(z_{n_{k}})\) converges to some \(x_{\infty}\in\mathcal{X}\).
3. Since the sequences \(A_{n}=\|x_{n+1/2}-x_{n}\|^{2}\) and \(B_{n}=\|x_{n+1}-x_{n+1/2}\|^{2}\) are both summable (by Proposition D.2), we further have \(\lim_{k\to\infty}x_{n_{k}+1/2}=x_{\infty}\) and, more generally, by a straightforward induction: \[\lim_{k\to\infty}x_{n_{k}+r}=x_{\infty}\quad\text{for any (fixed) $r=1/2,1,3/2,\dots$}\] (D.21)
4. Likewise, for the sequence of payoff signals \(\hat{v}_{n}\), we have \[\hat{v}_{i,n_{k}}=\lambda_{i}\,v_{i}(x_{n_{k}})+(1-\lambda_{i})\,v_{i}(x_{n_{k} -1/2})\xrightarrow[k\to\infty]{}\lambda_{i}v_{i}(x_{\infty})+(1-\lambda_{i})v_ {i}(x_{\infty})=v_{i}(x_{\infty})\] (D.22) so \(\lim_{k\to\infty}v(x_{n_{k}})=v(x_{\infty})\). \(\diamond\)_Step 4: Variational characterization of limit points_. We now proceed to show that \(v_{\infty}\coloneqq v(x_{\infty})\) belongs to the polar cone \(\operatorname{PC}(x_{\infty})=\{w\in\mathcal{Y}:\langle w,x-x_{\infty}\rangle \leq 0\,\,\text{ for all }x\in\mathcal{X}\}\) to \(\mathcal{X}\) at \(x_{\infty}\). To do so, suppose that (FTRL+) performs \(r\) steps from \(n_{k}\) so

\[y_{n_{k}+r}=y_{n_{k}}+\eta\sum_{j=1}^{r}\hat{v}_{n_{k}+1/2}\] (D.23)

where, to ease notation, we have made the simplifying assumption that \(\eta_{i}=\eta\) for all \(i\in\mathcal{N}\).13 Then, by invoking Lemma B.4 with \(y\gets y_{n_{k}}\) and \(y^{*}\gets y_{n_{k}+r}\), we obtain

Footnote 13: This assumption does not affect the core of our arguments, but it greatly streamlines the presentation.

\[\left\{\eta\sum_{j=1}^{r}\hat{v}_{n_{k}+1/2},p-x_{n_{k}+r}\right\} \leq\langle\nabla h(x_{n_{k}+r})-y_{n_{k}},p-x_{n_{k}+r}\rangle\] \[=\langle\nabla h(x_{n_{k}+r})-z_{n_{k}},p-x_{n_{k}+r}\rangle\] (D.24)

where, in the second line, we have used the fact that \(\langle y,x^{\prime}-x\rangle=\langle\Pi(y),x^{\prime}-x\rangle\) for all \(x,x^{\prime}\in\mathcal{X}\) and all \(y\in\mathcal{Y}\). Thus, letting \(k\to\infty\), we get from Step 3 and the continuity of \(\nabla h\) that

\[\eta r\langle v(x_{\infty}),x-x_{\infty}\rangle\leq\langle\nabla h(x_{\infty}) -z_{\infty},x-x_{\infty}\rangle\] (D.25)

for all \(r=1,2,\dots\) and all \(x\in\mathcal{X}\).14 Since \(r\) can be chosen arbitrarily, we must have \(\langle v(x_{\infty}),x-x_{\infty}\rangle\leq 0\) for all \(x\in\mathcal{X}\). Hence, by the variational characterization (VI) of Nash equilibria, we conclude that \(x_{\infty}\) must be itself a Nash equilibrium of \(\Gamma\), and our proof is complete.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract and introduction follows the same logic and mostly also order as the technical results of the article. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss exact assumptions throughout. Moreover, in the concluding remarks we focus on limitations that are also natural avenues for future research. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]

Justification: We wrote the article with highest attention to rigor and mathematical detail. As such we provide all assumptions for each result and use unambigious notation and language.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our simulations are merely illustrative and as such are not critical for the main results of the paper. However, we fully disclose how they were performed. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No]

Justification: We do not release the code at this point as we don't have time to anonymize it. We describe in detail how simulations are performed and will make the code available at a later stage.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes]

Justification: We provide all details for the simulations. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.

7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All simulations were performed on standard laptops and as such do not carry any particular computational burden. Guidelines:
* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Guidelines:
* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss in the introduction the use of learning in games. Guidelines:
* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets**Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.