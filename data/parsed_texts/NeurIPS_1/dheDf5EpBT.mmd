# Unified Gradient-Based Machine Unlearning with

Remain Geometry Enhancement

 Zhehao Huang, Xinwen Cheng, JingHao Zheng, Haoran Wang, Zhengbao He, Tao Li, Xiaolin Huang

Shanghai Jiao Tong University

[xinght_H, zinwencheng, zjh20030406, haoran_whynot, lstefamie, li.tao, xiaolinhuang]@sjtu.edu.cn

###### Abstract

Machine unlearning (MU) has emerged to enhance the privacy and trustworthiness of deep neural networks. Approximate MU is a practical method for large-scale models. Our investigation into approximate MU starts with identifying the steepest descent direction, minimizing the output Kullback-Leibler divergence to exact MU inside a parameters' neighborhood. This probed direction decomposes into three components: weighted forgetting gradient ascent, fine-tuning retaining gradient descent, and a weight saliency matrix. Such decomposition derived from Euclidean metric encompasses most existing gradient-based MU methods. Nevertheless, adhering to Euclidean space may result in sub-optimal iterative trajectories due to the overlooked geometric structure of the output probability space. We suggest embedding the unlearning update into a manifold rendered by the remaining geometry, incorporating second-order Hessian from the remaining data. It helps prevent effective unlearning from interfering with the retained performance. However, computing the second-order Hessian for large-scale models is intractable. To efficiently leverage the benefits of Hessian modulation, we propose a fast-slow parameter update strategy to implicitly approximate the up-to-date salient unlearning direction. Free from specific modal constraints, our approach is adaptable across computer vision unlearning tasks, including classification and generation. Extensive experiments validate our efficacy and efficiency. Notably, our method successfully performs class-forgetting on ImageNet using DiT and forgets a class on CIFAR-10 using DDPM in just 50 steps, compared to thousands of steps required by previous methods. Code is available at Unified-Unlearning-w-Remain-Geometry.

## 1 Introduction

Machine Unlearning (MU) [1, 2, 3] aims to remove the influence of samples from a pre-trained model, ensuring the model behaves as if it has never encountered those samples. The significance of MU research has grown following data protection regulations [4, 5]. It has rapidly developed in recent years, becoming an important means to help pre-trained large-scale models adapt to various trustworthy challenges [6] in computer vision (CV). In general, MU aids in purging outdated knowledge [7, 8], mitigating biases [9, 10], and preventing large text-to-image models from generating not-safe-for-work (NSFW) images [11, 12, 13, 14].

Existing MU methods are mainly divided into two categories: _exact (or certified) MU_[15] and _approximate MU_[16]. For deep neural networks, achieving exact MU necessitates retraining on a dataset excluding forgetting samples. However, retraining is computationally prohibitive for recent large-scale deep networks. Thus, in MU for deep networks, this retrained model only serves as an aspirational standard to be approached [17]. We primarily focus on more efficient approximate MU.

Approximate MU strives to align the output distribution of unlearned models with that of retrained models. We initially explore the vanilla gradient descent of minimizing the output Kullback-Leibler(KL) divergence between the current iteration and the retrained model. Our deduction reveals that this direction consists of three parts: a weighted gradient ascent to eliminate the influence of forgetting samples, a descent gradient for fine-tuning the remaining set, and a weight saliency matrix modulating the unlearning direction. Such decomposition provides a novel perspective that unifies previous MU approaches proposed in recent years [18; 19; 20; 21; 22; 23; 24; 25], most of which only focus on one or two of these components. For example, SalUn [26] insightfully proposes saliency-based unlearning, which only optimizes the parameters important to forget, but lacks theoretical support. Our analysis fills this gap and provides new directions for further improvement.

In fact, the vanilla gradient descent for approximate MU actually pursues the steepest descent under _Euclidean distance_[27; 28; 29; 30]. However, constraining parameter updates within an Euclidean region is arbitrary, as it treats the importance of all parameters equally. Recent research indicates deep models' parameters and training processes are embedded in a low-dimensional manifold [31; 32]. Thus, there exist manifolds where changes in Euclidean between parameters are drastic, yet the output space remains unchanged. In MU, it is evident that the importance of model parameters varies for forgetting and remaining [33; 26], prompting the following question. _Can model parameters be embedded in a manifold that allows effective forgetting while efficiently maintaining remaining performance?_

To achieve this goal, we propose to discover the descent direction under the KL divergence on the remaining output distribution. Using such a manifold metric, the forgetting direction can be amended by a second-order Hessian on the remaining set to prevent forgetting loss from harming retained performance. Such iterative direction is dominated by the unlearning function, allowing the optimization process to focus on efficient forgetting. However, computing the second-order Hessian for large-scale models is computationally intensive, contradicting the need for efficiency in unlearning. Existing methods for estimating the second-order Hessian rely on initial parameters and keep them fixed thereafter [34; 35]. Therefore, we propose a fast-slow weight [36; 37] method (**Fig. 1**) to implicitly and dynamically approximate the salient forgetting direction with Hessian modulation, forming a unified MU approach for CV unlearning tasks including image classification and image generation. Key **contributions** of this paper include:

\(\bullet\) We provide a novel perspective to unify previous approaches by decomposing the vanilla gradient descent direction of approximate MU into three components: weighted forgetting gradient ascent, remaining gradient descent, and a weight saliency matrix.

\(\bullet\) We derive the steepest descent direction for approximate MU on the remain-preserved manifold.

\(\bullet\) We propose a fast-slow weight method to implicitly approximate online Hessian-modulated salient forgetting updates.

\(\bullet\) We conduct experiments on a wide range of CV unlearning tasks across multiple datasets and models of different architectures, verifying the effectiveness and efficiency of our method.

## 2 Preliminary

**Problem Setup.** MU aims to help a well-trained model eliminate the influence of specific data points, categories, or high-level concepts and patterns [1; 38]. Let \(\mathcal{D}=\left\{z_{i}\right\}_{i=1}^{N}\) represent a pretraining dataset of \(N\) data points, including \(z_{i}=(x_{i},y_{i})\) features and labels in supervised learning. The _forgetting dataset_, \(\mathcal{D}^{f}=\{z_{i}^{f}\}_{i=1}^{N^{f}}\subset\mathcal{D}\), is a subset of the pretrained dataset. Its complement, \(\mathcal{D}^{r}=\{z_{i}^{r}\}_{i=1}^{N^{r}}=\mathcal{D}\backslash\mathcal{D}^ {f}\), is the _remaining dataset_ that we wish to retain. The learner is a model parameterized by \(\theta\). \(p_{z}(\theta)=p(z;\theta)\) represents the model output probability. The pre-trained model

Figure 1: Overview of our proposal vs. previous unlearning methods on erasing concept ‘nudity’ in diffusion models [11; 12]. Conventional methods seek the steepest descent within an Euclidean ball, often compromising general capabilities. In contrast, we reach the region around retraining along a remain-preserving manifold. To address the large cost of Hessian, we implicitly approximate the up-to-date salient unlearning direction.

is obtained by empirical risk minimization, i.e., \(\theta_{0}=\arg\min_{\theta}\mathcal{L}(\theta;\mathcal{D})=\arg\min_{\theta}\sum_{ i\in\mathcal{D}}\ell(\theta;z_{i})\), as **Pretrain**. This empirical risk can be divided into two parts based on the forgetting and retaining datasets. \(\mathcal{L}(\theta;\mathcal{D})=\sum_{i\in\mathcal{D}^{f}}\varepsilon_{i}\ell( \theta;z_{i}^{f})+\sum_{j\in\mathcal{D}^{r}}\ell(\theta;z_{i}^{r})=\mathcal{L}^ {f}(\theta;\mathbf{\varepsilon})+\mathcal{L}^{r}(\theta)\), where \(\mathbf{\varepsilon}=\{\varepsilon_{i}\}_{i=1}^{N_{f}}\) weight the former part [39; 38]. For the pre-trained model, the coefficients \(\mathbf{\varepsilon}_{0}=\mathbf{1}\) are all ones. The following are the instantiations in CV unlearning tasks. In classification (Cls) problems, models output the class posterior probability \(p(z_{\text{cls}};\theta)=p(y|x;\theta)\) and the empirical risk for each sample is the cross-entropy (CE) loss \(\ell_{\text{cls}}(\theta;z)=\ell_{\text{CE}}(\theta;x,y)\). In the image generation (Gen) task of conditional diffusion models [40; 41; 42], the output is the conditional sampling probability \(p(z_{\text{Gen}};\theta)=p(x|y;\theta)\), and the average loss function for each sample is the mean squared error (MSE) loss: \(\ell_{\text{Gen}}(\theta;z)=\ell_{\text{MSE}}(\theta;x,y)=\mathbb{E}_{t,e\sim \mathcal{N}(0,1)}[\|\epsilon-\epsilon_{\theta}(x_{t}|y)\|_{2}^{2}]\), where \(t\) represents the diffusion step, \(\epsilon\) is the random noise sampled from a Gaussian distribution \(\mathcal{N}(0,1)\), \(\epsilon_{\theta}\) is the conditional denoising model, and \(x_{t}\) is the noisy version of the input image. For a more detailed introduction to image generation using conditional diffusion models and latent diffusion models, please refer to Appendix B.3.

**Exact and Approximate Machine Unlearning.**_Exact MU_[15; 43; 1] ensures that the parameter distribution of the unlearned model is identical to that of a model trained from scratch without seeing the forgetting samples. For large-scale models, exact unlearning can only be achieved by retraining (**RT**) on the remaining dataset \(\theta_{*}=\arg\min_{\theta}\mathcal{L}^{r}(\theta)\). However, the computational cost of retraining in response to every forgetting request is prohibitive. Therefore, we regard RT as _a gold standard_ to approximate rather than a competitor. A more practical approach is to guide the unlearned model output distribution to approximate the output distribution of RT, known as _approximate MU_[16]. If we use KL divergence to measure the difference in output distributions, the objective of approximate unlearning can be expressed as: \(\min_{\theta}D_{\text{KL}}(p_{z}(\theta_{*})||p_{z}(\theta))=\min_{\theta} \int p_{z}(\theta_{*})\log[p_{z}(\theta_{*})/p_{z}(\theta)]\text{d}\mathcal{D}\), starting from \(\theta_{0}\). Therefore, a straight metric to approximate unlearning is the KL divergence from the retrained model's output distribution. In addition, we also investigate metrics related to forgetting efficacy, retained performance, and privacy protection for evaluation in image classification and generation tasks. For details of evaluation, please refer to **Sec.5** and Appendix D.

**Steepest Descent.** Approximate MU methods are usually based on gradient updates to obtain the unlearned model [18; 21]. Let's first reinterpret the gradient with _steepest descent_[29; 27; 30]. The goal of steepest descent is to find the direction \(\delta\theta=\theta_{t+1}-\theta_{t}\) that drives the objective function \(F(\theta)\) descent fastest within a \(\xi\)-neighborhood of the current parameters \(\theta_{t}\). This can be formulated as the following optimization problem. (See Appendix A.1 for proof.)

\[\delta\theta:=\operatorname*{arg\,min}_{\rho(\theta_{t},\theta_{t}+\delta \theta)\leq\xi}F(\theta_{t}+\delta\theta)\Rightarrow\theta_{t+1}:=\operatorname *{arg\,min}_{\theta_{t+1}}F(\theta_{t+1})+\frac{1}{\alpha_{t}(\xi,\theta_{t})} \rho(\theta_{t},\theta_{t+1}), \tag{1}\]

where \(\rho(\cdot,\cdot)\) represents the manifold metric that renders the geometry of the parameter's neighborhood. To simplify the derivation, we rewrite it to the form on the right, where \(\alpha_{t}(\xi,\theta_{t})\) represents the learning rate required to move the distance \(\xi\). In the following, we fix a small learning rate \(\alpha_{t}\) to approximate a search within a local neighborhood. The characterization \(\rho\) of the underlying coordinate space of the neighborhood will determine update directions, optimization paths, and the flatness of the minimum. Vanilla gradient descent is obtained using the Euclidean metric, Newton's direction is measured by the second-order expansion of the objective function [44], and the KL divergence in the output space induces a natural gradient [30; 45]. Next, we will probe approximate MU through vanilla gradient descent and attempt to benefit from improved manifold metrics.

## 3 Approximate MU from Perspective of Steepest Descent

**Revisit Approximate MU Methods via Vanilla Gradient Descent.** We begin with the vanilla gradient descent direction to address approximate MU. This involves finding the steepest descent direction that minimizes the KL divergence with the retrained output within the vicinity of the current model \(\theta_{t}\). The optimization problem can be formalized as follows:

\[\theta_{t+1}=\operatorname*{arg\,min}_{\theta_{t+1}}D_{\text{KL}} \left(p_{z}(\theta_{*})||p_{z}(\theta_{t+1})\right)+\frac{1}{\alpha_{t}}\rho( \theta_{t},\theta_{t+1}) \tag{2}\] \[=\operatorname*{arg\,min}_{\theta_{t+1}}\frac{D_{\text{KL}} \left(p_{z^{f}}(\theta_{*})||p_{z^{f}}(\theta_{t+1})\right)}{(a)}p^{f}+ \underbrace{D_{\text{KL}}\left(p_{z^{r}}(\theta_{*})||p_{z^{r}}(\theta_{t+1}) \right)}_{(b)}p^{r}+\frac{1}{\alpha_{t}}\underbrace{\rho(\theta_{t},\theta_{t +1})}_{(c)},\]

where \(p^{f}=p(\mathcal{D}^{f}|\mathcal{D})\) and \(p^{r}=p(\mathcal{D}^{r}|\mathcal{D})=1-p_{f}\) denote the partition of forgetting and remaining dataset, respectively. Analyzing (2), (\(a\)) seeks to eliminate the influence of the target forgetting samples, (\(b\)) aims to maintain the performance on the remaining samples, and (\(c\)) employs the metric \(\rho\) to constrain the magnitude of each update, thereby identifying the direction of steepest descent on the manifold. To solve the optimization challenge outlined in (2), we posit that for the current model \(\theta_{t}\), there exists a set of coefficients \(\mathbf{\varepsilon}_{t}=\{\varepsilon_{t,i}\}_{i=1}^{N_{f}}\) that weights the forgetting loss, positioning \(\theta_{t}\) as the minimizer of the weighted loss for the original training set. The unlearning process necessitates adaptations in coefficients of forgetting loss. Then, we can determine the vanilla gradient descent for approximate MU by using _Euclidean distance_\(\ell_{2}\) as the manifold metric, as stated in **Prop. 1**.

**Proposition 1**.: _Under the Euclidean manifold metric, \(\rho(\theta_{t},\theta_{t+1})=\frac{1}{2}\|\theta_{t}-\theta_{t+1}\|^{2}\). Assuming that the current model \(\theta_{t}=\arg\min_{\theta}\mathcal{L}^{\prime}(\theta;\mathbf{\varepsilon}_{t})+ \mathcal{L}^{\prime}(\theta)\). Let \(H_{s}^{f}=\nabla^{2}\mathcal{L}^{\prime}(\theta_{*};\mathbf{1})\)and \(H_{*}^{r}=\nabla^{2}\mathcal{L}^{\prime}(\theta_{*})\) denote the Hessian of the retrained model on the forgetting set and the remaining set, respectively. Then, the steepest descent direction that minimizes (2) is approximately:_

\[\theta_{t+1}-\theta_{t}:=\underbrace{H_{*}^{f}(H_{*}^{r})^{-1}}_{(S)}[ \underbrace{-\nabla\mathcal{L}^{\prime}(\theta_{t};\mathbf{\varepsilon}_{t})}_{(F )}]p^{f}+\underbrace{\nabla\mathcal{L}^{\prime}(\theta_{t})}_{(R)}p^{r}]. \tag{3}\]

The proof can be found in Appendix A.2. To elucidate the effectiveness of gradient-based unlearning methods, we decompose the vanilla gradient descent direction in (3) into three components: (F), (R), and (S). (F) represents the gradient ascent direction of the weighted forgetting loss, which directs the model to discard the information of the forgetting samples. Fine-tuning (**FT**) [22, 19, 38] fails to guarantee MU due to the absence of (F). Current approximate MU methods such as Random Labeling (**RL**) [20] and BadTeacher knowledge distillation (**BT**) [23] are akin to weighted forgetting loss gradient ascent, uniformly leading to an increase in the loss on forgetting samples. The unlearning process often causes catastrophic forgetting of the retained knowledge. Thus, it is common to integrate (R) fine-tuning on the remaining set to sustain the model's general capabilities. Unlearned models via Gradient Ascent (**GA**) [20, 21] usually lose usability without (R). Furthermore, (S), ignored in most of the previous literature, involves two Hessian modulation parts. \(H_{s}^{f}\) amplifies the parameter updates crucial for forgetting, while \((H_{s}^{r})^{-1}\) dampens those important for maintaining. The notion of (S) closely mirrors the _Weight Saliency_ introduced in **SalUn**[26]. We provide theoretical support for this notion. Importantly, our framework makes no assumption regarding input modalities, allowing its flexible application across various CV unlearning tasks.

**Approximate MU in Remain-preserving Manifold.** In fact, employing Euclidean distance as the manifold metric for parameter updates is arbitrary. It treats all coordinates as equally important because the local second-order expansion is identical \(\nabla^{2}_{\theta_{t}}(\frac{1}{2}\|\theta_{t}-\theta_{t+1}\|^{2})=I\). This uniform treatment overlooks the varying parameter significance for forgetting and remaining. Moreover, certain manifolds of parameter space can exhibit substantial variations in Euclidean metric, yet the induced model output remains almost unchanged [46]. Since the retrained model performance on forgetting is unpredictable, it is pragmatic to introduce manifolds related to the remaining. Therefore, a practical objective is to constrain parameter updates during unlearning within a manifold that minimally impacts the retained performance. An empirical characterization of such a manifold could be _the KL divergence on the output distribution of the remaining set_, \(D_{\text{KL}}^{r}\). Given that the original well-trained and the retrained model output closely match the ground-truth remaining distribution, \(\nabla\mathcal{L}^{r}(\theta_{0})\approx\nabla\mathcal{L}^{r}(\theta_{*}) \approx 0\). By starting with \(\theta_{0}\) and limiting updates to this manifold, the maintained output distribution remains almost consistent throughout the unlearning iterations, \(\nabla\mathcal{L}^{r}(\theta_{t+1})\approx\nabla\mathcal{L}^{r}(\theta_{t}) \approx\nabla\mathcal{L}^{r}(\theta_{0})\approx 0\). This consistency permits a second-order Taylor expansion at \(\theta_{t}\) to terms (b) and (c) in (2), providing crucial curvature information for unlearning to prevent deviations in the model output on the remaining set, leading to **Prop. 2**.

**Proposition 2**.: _Using the model output KL divergence on the remaining set as the manifold metric, \(\rho(\theta_{t},\theta_{t+1})=D_{\text{KL}}\left(p_{z^{r}}(\theta_{t})||p_{z^{ r}}(\theta_{t+1})\right))\). Assuming that the current model \(\theta_{t}=\arg\min_{\theta}\mathcal{L}^{r}(\theta)+\mathcal{L}^{\prime}(\theta; \mathbf{\varepsilon}_{t})\). Let \(\tilde{\alpha}_{t}=\alpha_{t}p^{f}/(\alpha_{t}p^{r}+1)\), and \(H_{t}^{r}=\nabla^{2}\mathcal{L}^{r}(\theta_{t})\) represent the Hessian w.r.t. \(\theta_{t}\) on the remaining set, then the steepest descent direction that minimizes (2) is approximately:_

\[\theta_{t+1}-\theta_{t}:=-\tilde{\alpha}_{t}\underbrace{(H_{t}^{r})^{-1}}_{(R)} [\underbrace{H_{s}^{f}(H_{s}^{r})^{-1}}_{(S)}[\underbrace{-\nabla\mathcal{L} ^{\prime}(\theta_{t};\mathbf{\varepsilon}_{t})}_{(S)}]\underbrace{[-\nabla \mathcal{L}^{\prime}(\theta_{t};\mathbf{\varepsilon}_{t})]}_{(F)}]. \tag{4}\]

\begin{table}
\begin{tabular}{l|c|c|c c c|c} \hline \hline
**Approximate** & \multicolumn{2}{c|}{**Task**} & \multicolumn{2}{c|}{**MU components**} & \multicolumn{1}{c}{**Manifold**} & \multicolumn{1}{c}{**Online**} \\
**MU Methods** & Cts & Gen & (S) & (F) & (R) & **Metric** & **Hessian** \\ \hline FT [22, 19, 38] & ✓ & & & & & ✓ & \(\ell_{2}\) & \\ GA [20, 21] & ✓ & ✓ & & ✓ & ✓ & \(\ell_{2}\) & \\ BT [23] & ✓ & ✓ & ✓ & ✓ & ✓ & \(\ell_{2}\) & \\ SA [26] & ✓ & ✓ & ✓ & ✓ & ✓ & \(\ell_{2}\) & \\ SA [55] & ✓ & ✓ & ✓ & ✓ & ✓ & \(D_{\text{KL}}\) & \\ \hline SPR-on & ✓ & ✓ & ✓ & ✓ & ✓ & \(D_{\text{KL}}\) & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of approximate MU methods. We decompose the steepest descent direction into three parts: the weight saliency matrix (S), the forgetting part (F), and the remaining part (R) as in (3) and (4). Only SA and our method consider the remain-preserving manifold, and we further approximate up-to-date Hessian.

We defer the proof to Appendix A.3. The unlearning updates in (4) incorporated second-order Hessian concerning remaining to guide the optimization direction. Specifically, the large curvature direction of \(H_{t}^{r}\) corresponds with the weights that encapsulate remaining knowledge, while the small curvature direction encourages model updates for effective unlearning. Furthermore, the update direction in (4) primarily follows the weighted gradient ascent (F) modulated by weight saliency (S). Such unlearning iterative updates in remain-preserving manifold focus on diminishing the distributional discrepancy with exact MU concerning forgetting output.

**Challenges in Hessian Approximation.** To exploit the benefits of unlearning updates within the remain-preserving manifold, the key point is \((H_{t}^{r})^{-1}\). However, calculating the Hessian and its reverse for large-scale models is computationally demanding [47]. Consequently, many methods have been developed to estimate the Hessian, such as Fisher information [34], Fisher diagonals [48], and Kronecker factored Laplace approximation [49]. Regarding unlearning, Selective Amnesia (**SA**) [35] employs the initial model's remaining Fisher diagonals as the second-order Hessian constraint on parameter updates. However, the fixed Hessian in SA leads to progressively increasing estimation biases, exacerbated by cumulative errors in Taylor expansion, which harms the retained performance during unlearning. To address this issue, the subsequent **Sec. 4** introduces an fast-slow weight update method that implicitly approximates the direction adjusted by the up-to-date Hessian.

## 4 Proposed Method

**Implicit Online Hessian Approximation (R-on).** Given that computing the inverse of even well-approximated Hessian demands substantial computational resources, it is more practical to estimate the unlearning direction post-Hessian inversion modulation directly. Inspired by recent insights into the connection between Meta-Continual Learning and Hessian approximation [50], we propose a fast-slow weight [36; 37] method for implicitly approximating the desired updates. The optimization problem for fast weight updates is formulated as follows:

\[\min_{\theta_{t}^{f}}\mathcal{L}^{r}(\theta_{t}^{f})\quad\text{s.t.}\quad \theta_{t}^{f}=\theta_{t}-\beta_{t}\nabla\mathcal{L}^{u}(\theta_{t}), \tag{5}\]

where \(\mathcal{L}^{u}\) represents an arbitrary forgetting loss and \(\beta_{t}\) is its learning rate. The iterative process is depicted in **Fig. 1**. A step of forgetting is taken at the current model, resulting in \(\theta_{t}^{f}\). Several gradient descent updates on the remaining set follow to obtain the minimum point \(\theta_{t}^{r}\). This fine-tuning ensures that the updated model adheres to the remain-preserving manifold. The slow weight updates leverage the underlying connection between \(\theta_{t}\) and \(\theta_{t}^{r}\), as stated in **Prop. 3.**

**Proposition 3**.: _For implicit online Hessian approximation in (5), suppose \(\beta_{t},\delta_{t}\) is small, \(\beta_{t}<\sqrt{\delta_{t}/|\nabla\mathcal{L}^{r}(\theta_{t})-[\nabla\mathcal{L }^{r}(\theta_{t})]^{2}|}\), \(\mathcal{L}^{r}\) is \(\mu\)-smooth, i.e., \(\|\nabla\mathcal{L}^{r}(\theta)-\nabla\mathcal{L}^{r}(\theta^{\prime})\|_{2} \leq\mu\|\theta-\theta^{\prime}\|_{2}\), and there exist an \(\zeta_{t}\)-neighborhood \(\mathcal{N}(\theta_{t}^{r},\zeta_{t})\) of the optimal model parameter \(\theta_{t}^{r}=\arg\min_{\theta_{t}^{f}}\mathcal{L}^{r}(\theta_{t}^{f})\), which includes \(\theta_{t}\) and \(\theta_{t}^{f}\). Then, the iterative update term approximately is,_

\[\theta_{t}-\theta_{t}^{r}:\approx\beta_{t}^{2}\left[\nabla^{2}\mathcal{L}^{r} (\theta_{t})\right]^{-1}\nabla\mathcal{L}^{u}(\theta_{t})=\beta_{t}^{2}(H_{t} ^{r})^{-1}\nabla\mathcal{L}^{u}(\theta_{t}). \tag{6}\]

The proof is in Appendix A.4. **Prop. 3** indicates that the model \(\theta_{t}^{r}\), obtained after fine-tuning using the process described in (5), is approximately equivalent to updating the current model \(\theta_{t}\) by one step in the Hessian-adjusted unlearning direction. We use this direction to update the outer loop.

**Comparison with the joint loss (R).** We investigate the differences in the updates between our optimization in (5) (**R-on**) and the joint optimization of forgetting and remaining losses (**R**) [23; 25]. We take the checkpoint after the first step of fine-tuning the remaining set as an example and ignore the step size.

\[\mathcal{L}^{\text{R}}(\theta_{t})=\mathcal{L}^{u}(\theta_{t})+\mathcal{L}^{r} (\theta_{t}),\quad\Delta^{\text{R}}=\nabla\mathcal{L}^{u}(\theta_{t})+\nabla \mathcal{L}^{r}(\theta_{t}), \tag{7}\]

\[\Delta^{\text{R-on}}\approx\nabla\mathcal{L}^{u}(\theta_{t})+\nabla\mathcal{L} ^{r}(\theta_{t})+\nabla^{2}\mathcal{L}^{r}(\theta_{t})(\theta_{t}^{f}-\theta_{ t})=(I-H_{t}^{r})\nabla\mathcal{L}^{u}(\theta_{t})+\nabla\mathcal{L}^{r}( \theta_{t}), \tag{8}\]

Comparison of the updates in (7) and (8) reveals that the remaining gradient is the same. Our forgetting update in fast weight is adjusted by an additional term \(-H_{t}^{r}\), which is absent in joint optimization. This modification weakens the directions that significantly impact the remain, thereby mitigating the damage of forgetting loss on the retained performance. Furthermore, certain methods [24] suggest two-stage unlearning that first impairs the model and then repairs it, actually paralleling a single fast-slow weight update of our method.

[MISSING_PAGE_EMPTY:6]

## 5 Experiments

**Datasets, Models, and Settings.** In image classification, we primarily focus on the **random subset unlearning** task. Evaluations are conducted using ResNet-18 [52] on CIFAR10 [53] and Swin-T [54] on TinyImageNet [55], with additional tests on random subset and class-wise forgetting tasks involving CIFAR100 [53] and SVHN [56], detailed in Appendix F.2. In image generation, our main interest lies in **class-wise forgetting** tasks. Following [35; 26], we unlearn conditional DDPM [40] with the UNet architecture [57] on CIFAR10. Moreover, for the first time, we explore the latent diffusion model [42] equipped with Diffusion Transformer (DiT) [58] on ImageNet [59], which demonstrates superior scalability in learning large-scale data generation tasks. Finally, we perform **concept forgetting** tasks using the open-source Stable Diffusion (SD) V1.4 [42] to inhibit the generation of NSFW content, specifically by targeting the prevention of nude images. Further details on unlearning setups and training are available in Appendix E.

**Baselines and Evaluation.** We regard RT as an oracle of approximate MU and compare our proposal with eight MU methods, including six gradient-based MU approaches outlined in **Sec. 3**: **FT**[19], **GA**[21], **BT**[23], **RL**[20], **SalUn**[26], and **SA**[35]. We also consider **SCRUB**[25], an enhanced variant of BT, for image classification, and **ESD**[11] for removing concepts in SD. For the evaluation of random subset unlearning tasks, we measure the output KL divergence \(D_{\text{KL}}\) between the unlearned and retrained models, which is the direct target of approximate MU. Besides, we assess the accuracy on the forgetting set (**FA**) for unlearning efficacy, and the accuracy on the remaining (**RA**) and test (**TA**) sets for preserved generalization ability. We also consider the success rate of membership inference attack (**MIA**) [60; 38] on the forgetting set as a privacy metric. Note that the smaller the disparity in these metrics against RT, the more effective the unlearning. Run-time efficiency (**RTE**) is also reported. For class-wise forgetting tasks in image generation, we evaluate the accuracy of the unlearned model's generated images on forgetting classes (**FA**) by a pre-trained classifier. The Frechet Inception Distance (**FID**) [61] metric assesses the retained generative capability for remaining classes. For ablating the 'nudity' concept in SD, we employ the NudeNet [62] detector to identify and count nude body parts in generated NSFW images. For further introduction to the baselines and detailed evaluation metrics, please refer to Appendix E.1 and D.

To assess the unlearning effectiveness and efficiency of **SFR-on**, we perform comprehensive experiments and conduct ablation studies to address the following **four key questions**:

**Q1: How does SFR-on perform on unlearning in image classification?** We first evaluate the performance of our method, SFR-on, against existing gradient-based MU methods on the image classification random subset unlearning task. In this scenario, the forgetting set, remaining, and test sets all originate from the same distribution. Consequently, even if the model undergoes unlearning on the random subset, it may still generalize to these samples. To avoid potential biases from only using FA as an unlearning metric, we incorporate MIA to assess the privacy retention of the forgetting set, enhancing the robustness of assessments. As detailed in **Tab. 2**, for forgetting 10% random subset on CIFAR-10 and TinyImageNet, SFR-on not only most closely aligns with RT in the averaging metric disparity but also exhibits the smallest output KL divergences _w.r.t._ RT. This performance underscores our effectiveness and efficiency in achieving the objective of approximate MU. The results of the increased 50% random subset unlearning task are included in Appendix F.2.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c} \hline \multicolumn{1}{c}{\multirow{2}{*}{**Methods**}} & \multicolumn{4}{c}{**CIFAR-100**} & \multicolumn{4}{c}{**Clinical Strategies (\%)**} & \multicolumn{4}{c}{**Training**Models & **Baseline**} & \multicolumn{4}{c}{**Testing**Models & **Non-Training (\%)**} \\ \cline{2-14}  & PA & **Ex.** & **1** & **MIA** & **[**\%**] & **[**\%**]** & **[**\%**]** & **[**\%**]** & **[**\%**]** & **[**\%**]** & **[**\%**]** & **[**\%**]** & **[**\%**]** & **[**\%**]** & **[**\%**]** & **[**\%**]** \\ \hline \hline \multirow{2}{*}{**MT**} & **F** & **6.5(0.0)** & **0.0(0.0)** & **0.

[MISSING_PAGE_FAIL:8]

**Q4: How effective is SFR-on in NSFW content removal for SD?** We finally assess the efficacy of our method in removing the 'nudity' concept from the open-source SD to prevent the generation of NSFW content. Given that SD V1.4 is trained on the LAION dataset [63], purging all images depicting nudity and retraining the model would be prohibitively time-consuming and resource-intensive. In this unlearning task, we designate 'nudity' as the forgetting set and generate a set of clothed individuals as remaining to preserve SD's generalization capability across non-nudity themes. We utilize the inappropriate image prompts (I2P) [12] to query potentially NSFW content from the unlearned SD and employ NudeNet to detect exposed body parts in these images. The results in **Tab. 4** demonstrate that our method significantly prevents the generation of culturally sensitive body parts, such as breasts and genitalia, highlighting our strength to enhance the trustworthiness of machine learning applications.

## 6 Conclusion

This paper revisits gradient-based approximate MU methods from the perspective of the steepest descent. The descent direction under an Euclidean manifold metric can be divided into three integral components: weighted forgetting gradient ascent, fine-tuning remaining gradient descent, and weight saliency matrix. Our approach advances beyond the Euclidean constraints by embedding the unlearning update within a remain-preserving manifold. This novel strategy incorporates the second-order Hessian of the current model on remaining, safeguarding against detrimental impacts on retained performance. To circumvent the prohibitive computational demands of the Hessian in large-scale models, we introduce an efficient fast-slow weight update method to approximate the Hessian-adjusted direction. Furthermore, our innovative adaptive coefficient for weight forgetting loss and a forget-remain balanced weight saliency map facilitate near-retraining unlearning. Our method can be applied to popular CV unlearning tasks with empirically verified unlearning efficacy.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{3}{c|}{**Target: \(\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{ }}}}}}}}}}}}}}}\)} & \multicolumn{3}{c|}{**Non-forgetting classes**} \\  & I1 & I2 & C1 & C2 & C3 & C4 & C5 \\ \hline \hline \multirow{3}{*}{Pretrain} & \multirow{3}{*}{\begin{tabular}{} \end{tabular} } & \multirow{3}{*}{\begin{tabular}{} \end{tabular} } & \multicolumn{3}{c|}{**Non-forgetting classes**} \\  & I1 & I2 & C1 & C2 & C3 & C4 & C5 \\ \hline \hline \multirow{3}{*}{Pretrain} & \multirow{3}{*}{\begin{tabular}{} \end{tabular} } & \multirow{3}{*}{\begin{tabular}{} \end{tabular} } & \multicolumn{3}{c|}{**Non-forgetting classes**} \\  & I1 & I2 & C1 & C2 & C3 & C4 & C5 \\ \hline \hline \multirow{3}{*}{Pretrain} & \multirow{3}{*}{\begin{tabular}{} \end{tabular} } & \multirow{3}{*}{
\begin{tabular}{} \end{tabular} } & \multicolumn{3}{c|}{**Non-forgetting classes**} \\  & I1 & I2 & C1 & C2 & C3 & C4 & C5 \\ \hline \hline \multirow{3}{*}{Pretrain} & \multirow{

## Acknowledgments and Disclosure of Funding

The authors would like to thank the anonymous reviewers for their insightful comments.

The research leading to these results has received funding from National Key Research Development Project (2023YFF1104202), National Natural Science Foundation of China (62376155, Shanghai Municipal Science and Technology Research Program Major Project (2021SHZDZX0102).

## References

* [1] L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers, B. Zhang, D. Lie, and N. Papernot, "Machine unlearning," _2021 IEEE Symposium on Security and Privacy (SP)_, pp. 141-159, 2019.
* [2] T. Shaik, X. Tao, H. Xie, L. Li, X. Zhu, and Q. Li, "Exploring the landscape of machine unlearning: A survey and taxonomy," _arXiv preprint arXiv:2305.06360_, 2023.
* [3] J. Xu, Z. Wu, C. Wang, and X. Jia, "Machine unlearning: Solutions and challenges," _IEEE Transactions on Emerging Topics in Computational Intelligence_, 2024.
* [4] G. D. P. R. GDPR, "General data protection regulation," _URL: https://gdpr-info. eu/taccessed 2020-11-211_, 2018.
* [5] E. Illman and P. Temple, "California consumer privacy act," _The Business Lawyer_, vol. 75, no. 1, pp. 1637-1646, 2019.
* [6] J. Rando, D. Paleka, D. Lindner, L. Heim, and F. Tramer, "Red-teaming the stable diffusion safety filter," _arXiv preprint arXiv:2210.04610_, 2022.
* [7] N. De Cao, W. Aziz, and I. Titov, "Editing factual knowledge in language models," _Empirical Methods in Natural Language Processing (EMNLP)_, 2021.
* [8] K. Meng, D. Bau, A. Andonian, and Y. Belinkov, "Locating and editing factual associations in gpt," _Advances in Neural Information Processing Systems (NIPS)_, vol. 35, pp. 17 359-17 372, 2022.
* [9] A. Oesterling, J. Ma, F. Calmon, and H. Lakkaraju, "Fair machine unlearning: Data removal while mitigating disparities," in _International Conference on Artificial Intelligence and Statistics_. PMLR, 2024, pp. 3736-3744.
* [10] R. Chen, J. Yang, H. Xiong, J. Bai, T. Hu, J. Hao, Y. Feng, J. T. Zhou, J. Wu, and Z. Liu, "Fast model debias with machine unlearning," _Advances in Neural Information Processing Systems (NIPS)_, vol. 36, 2024.
* [11] R. Gandikota, J. Materzynska, J. Fiotto-Kaufman, and D. Bau, "Erasing concepts from diffusion models," in _IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023, pp. 2426-2436.
* [12] P. Schramowski, M. Brack, B. Deiseroth, and K. Kersting, "Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models," in _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023, pp. 22 522-22 531.
* [13] N. Kumari, B. Zhang, S.-Y. Wang, E. Shechtman, R. Zhang, and J.-Y. Zhu, "Ablating concepts in text-to-image diffusion models," in _IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023, pp. 22 691-22 702.
* [14] E. Zhang, K. Wang, X. Xu, Z. Wang, and H. Shi, "Forget-me-not: Learning to forget in text-to-image diffusion models," _arXiv preprint arXiv:2303.17591_, 2023.
* [15] C. Guo, T. Goldstein, A. Y. Hannun, and L. van der Maaten, "Certified data removal from machine learning models," _International Conference on Machine Learning (ICML)_, 2020.
* [16] Z. Izzo, M. A. Smart, K. Chaudhuri, and J. Y. Zou, "Approximate data deletion from machine learning models: Algorithms and evaluations," _ArXiv_, vol. abs/2002.10077, 2020.
* [17] A. Thudi, H. Jia, I. Shumailov, and N. Papernot, "On the necessity of auditable algorithmic definitions for machine unlearning," in _USENIX Security Symposium_, 2021.
* [18] S. Neel, A. Roth, and S. Sharifi-Malvajerdi, "Descent-to-delete: Gradient-based methods for machine unlearning," _Algorithmic Learning Theory_, 2021.
** [19] A. Warnecke, L. Pirch, C. Wressnegger, and K. Rieck, "Machine unlearning of features and labels," _Annual Network and Distributed System Security Symposium_, 2023.
* [20] L. Graves, V. Nagisetty, and V. Ganesh, "Amnesiac machine learning," in _AAAI Conference on Artificial Intelligence (AAAI)_, 2021.
* [21] A. Thudi, G. Deza, V. Chandrasekaran, and N. Papernot, "Unrolling sgd: Understanding factors influencing machine unlearning," _European Symposium on Security and Privacy (EuroS&P)_, pp. 303-319, 2022.
* [22] A. Golatkar, A. Achille, and S. Soatto, "Eternal sunshine of the spotless net: Selective forgetting in deep networks," _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 9301-9309, 2019.
* [23] V. S. Chundawat, A. K. Tarun, M. Mandal, and M. S. Kankanhalli, "Can bad teaching induce forgetting? unlearning in deep networks using an incompetent teacher," _AAAI Conference on Artificial Intelligence (AAAI)_, 2023.
* [24] A. K. Tarun, V. S. Chundawat, M. Mandal, and M. S. Kankanhalli, "Fast yet effective machine unlearning," _IEEE transactions on neural networks and learning systems_, vol. PP, 2021.
* [25] M. Kurmanji, P. Triantafillou, and E. Triantafillou, "Towards unbounded machine unlearning," _Towards Unbounded Machine Unlearning (NIPS)_, 2023.
* [26] C. Fan, J. Liu, Y. Zhang, D. Wei, E. Wong, and S. Liu, "Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation," _International Conference on Learning Representations (ICLR)_, 2023.
* [27] M. Kim, D. Li, S. X. Hu, and T. M. Hospedales, "Fisher sam: Information geometry and sharpness aware minimisation," in _International Conference on Machine Learning (ICML)_, 2022.
* [28] T.-C. Kao, K. T. Jensen, G. M. van de Ven, A. Bernacchia, and G. Hennequin, "Natural continual learning: success is a journey, not (just) a destination," _Advances in Neural Information Processing Systems (NIPS)_, 2021.
* [29] T. E. Abrudan, J. Eriksson, and V. Koivunen, "Steepest descent algorithms for optimization under unitary matrix constraint," _IEEE Transactions on Signal Processing_, vol. 56, pp. 1134-1147, 2008.
* [30] J. Martens, "New insights and perspectives on the natural gradient method," _Journal of Machine Learning Research_, vol. 21, pp. 146:1-146:76, 2014.
* [31] T. Li, L. Tan, Z. Huang, Q. Tao, Y. Liu, and X. Huang, "Low dimensional trajectory hypothesis is true: Dnns can be trained in tiny subspaces," _IEEE Transactions on Pattern Analysis and Machine Intelligence_, vol. 45, pp. 3411-3420, 2022.
* [32] J. E. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, and W. Chen, "Lora: Low-rank adaptation of large language models," _International Conference on Learning Representations (ICLR)_, 2022.
* [33] J. Foster, S. Schoepf, and A. Brintrup, "Fast machine unlearning without retraining through selective synaptic dampening," _AAAI Conference on Artificial Intelligence (AAAI)_, 2024.
* 3526, 2016.
* [35] A. Heng and H. Soh, "Selective amnesia: A continual learning approach to forgetting in deep generative models," _Advances in Neural Information Processing Systems (NIPS)_, 2023.
* [36] M. R. Zhang, J. Lucas, G. E. Hinton, and J. Ba, "Lookahead optimizer: k steps forward, 1 step back," _Advances in Neural Information Processing Systems (NIPS)_, 2019.
* [37] A. Nichol, J. Achiam, and J. Schulman, "On first-order meta-learning algorithms," _ArXiv_, vol. abs/1803.02999, 2018.
* [38] J. Jia, J. Liu, P. Ram, Y. Yao, G. Liu, Y. Liu, P. Sharma, and S. Liu, "Model sparsity can simplify machine unlearning," in _Neural Information Processing Systems (NIPS)_, 2023.
* [39] P. W. Koh and P. Liang, "Understanding black-box predictions via influence functions," in _International Conference on Machine Learning (ICML)_, 2017.

* [40] J. Ho, A. Jain, and P. Abbeel, "Denoising diffusion probabilistic models," _Advances in Neural Information Processing Systems (NIPS)_, 2020.
* [41] J. Ho, "Classifier-free diffusion guidance," _ArXiv_, vol. abs/2207.12598, 2022.
* [42] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-resolution image synthesis with latent diffusion models," _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 10 674-10 685, 2022.
* [43] A. Mahadevan and M. Mathioudakis, "Certifiable machine unlearning for linear models," _ArXiv_, vol. abs/2106.15093, 2021.
* [44] D. Kovalev, K. Mishchenko, and P. Richtarik, "Stochastic newton and cubic newton methods with simple local linear-quadratic rates," _ArXiv_, vol. abs/1912.01597, 2019.
* [45] O. Calin and C. Udriste, "Geometric modeling in probability and statistics," 2014.
* [46] T. Hoang, S. Rana, S. Gupta, and S. Venkatesh, "Learn to unlearn for deep neural networks: Minimizing unlearning interference with gradient projection," _IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, 2023.
* [47] M. Elsayed and A. R. Mahmood, "Hesscale: Scalable computation of hessian diagonals," 2022.
* [48] F. Huszar, "Note on the quadratic penalties in elastic weight consolidation," _Proceedings of the National Academy of Sciences_, vol. 115, no. 11, Feb. 2018. [Online]. Available: [http://dx.doi.org/10.1073/pnas.1717042115](http://dx.doi.org/10.1073/pnas.1717042115)
* [49] H. Ritter, A. Botev, and D. Barber, "Online structured laplace approximations for overcoming catastrophic forgetting," _Advances in Neural Information Processing Systems (NIPS)_, 2018.
* [50] Y. Wu, L.-K. Huang, R. Wang, D. Meng, and Y. Wei, "Meta continual learning revisited: Implicitly enhancing online hessian approximation via variance reduction," in _International Conference on Learning Representations (ICLR)_, 2024. [Online]. Available: [https://openreview.net/forum?id=TpD2aG1h0D](https://openreview.net/forum?id=TpD2aG1h0D)
* [51] B. Rozemberczki, L. Watson, P. Bayer, H.-T. Yang, O. Kiss, S. Nilsson, and R. Sarkar, "The shapley value in machine learning," _International Joint Conference on Artificial Intelligence (IJCAI)_, 2022.
* [52] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 770-778, 2015.
* [53] A. Krizhevsky, "Learning multiple layers of features from tiny images," 2009.
* [54] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, "Swin transformer: Hierarchical vision transformer using shifted windows," _IEEE/CVF International Conference on Computer Vision (ICCV)_, pp. 9992-10 002, 2021.
* [55] Y. Le and X. S. Yang, "Tiny imagenet visual recognition challenge," 2015.
* [56] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Ng, "Reading digits in natural images with unsupervised feature learning," 2011.
* [57] O. Ronneberger, P. Fischer, and T. Brox, "U-net: Convolutional networks for biomedical image segmentation," _Medical Image Computing and Computer-Assisted Intervention (MICCAI)_, 2015.
* [58] W. S. Peebles and S. Xie, "Scalable diffusion models with transformers," _IEEE/CVF International Conference on Computer Vision (ICCV)_, pp. 4172-4182, 2023.
* [59] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, "Imagenet: A large-scale hierarchical image database," _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 248-255, 2009.
* [60] L. Song and P. Mittal, "Systematic evaluation of privacy risks of machine learning models," in _USENIX Security Symposium_, 2020.
* [61] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, "Gans trained by a two time-scale update rule converge to a local nash equilibrium," 2018.
* [62] P. Bedapudi, "Nudenet: Neural nets for nudity classification, detection and selective censoring," 2019.
** [63] C. Schulmann, R. Vencu, R. Beaumont, R. Kaczmareczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki, "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs," _ArXiv_, vol. abs/2111.02114, 2021.
* [64] A. Sekhari, J. Acharya, G. Kamath, and A. T. Suresh, "Remember what you want to forget: Algorithms for machine unlearning," _Advances in Neural Information Processing Systems (NIPS)_, 2021.
* [65] A. A. Ginart, M. Y. Guan, G. Valiant, and J. Y. Zou, "Making ai forget you: Data deletion in machine learning," _Advances in Neural Information Processing Systems (NIPS)_, 2019.
* [66] E. Ullah, T. Mai, A. B. Rao, R. A. Rossi, and R. Arora, "Machine unlearning via algorithmic stability," in _Annual Conference Computational Learning Theory_, 2021.
* [67] R. Giordano, W. T. Stephenson, R. Liu, M. I. Jordan, and T. Broderick, "A swiss army infinitesimal jackknife," in _International Conference on Artificial Intelligence and Statistics_, 2018.
* [68] M. Chen, Z. Zhang, T. Wang, M. Backes, M. Humbert, and Y. Zhang, "When machine unlearning jeopardizes privacy," _Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security_, 2020.
* [69] Z. Li and Y. Zhang, "Membership leakage in label-only exposures," _Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security_, 2020.
* [70] L. Song, R. Shokri, and P. Mittal, "Privacy risks of securing machine learning models against adversarial examples," _Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security_, 2019.
* [71] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, "Privacy risk in machine learning: Analyzing the connection to overfitting," _2018 IEEE 31st Computer Security Foundations Symposium (CSF)_, pp. 268-282, 2017.
* [72] N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer, "Membership inference attacks from first principles," 2022.
* [73] X. Cheng, Z. Huang, and X. Huang, "Machine unlearning by suppressing sample contribution," 2024.
* 37, 2021.
* [75] M. Fredrikson, S. Jha, and T. Ristenpart, "Model inversion attacks that exploit confidence information and basic countermeasures," _Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security_, 2015.
* [76] B. Balle, G. Cherubin, and J. Hayes, "Reconstructing training data with informed adversaries," _2022 IEEE Symposium on Security and Privacy (SP)_, pp. 1138-1156, 2022.
* [77] M. McCloskey and N. J. Cohen, "Catastrophic interference in connectionist networks: The sequential learning problem," _Psychology of Learning and Motivation_, vol. 24, pp. 109-165, 1989.
* [78] R. Ratcliff, "Connectionist models of recognition memory: constraints imposed by learning and forgetting functions." _Psychological review_, vol. 97 2, pp. 285-308, 1990.
* [79] L. Wang, X. Zhang, H. Su, and J. Zhu, "A comprehensive survey of continual learning: Theory, method and application," _IEEE transactions on pattern analysis and machine intelligence_, vol. PP, 2023.
* [80] N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tramer, B. Balle, D. Ippolito, and E. Wallace, "Extracting training data from diffusion models," _USENIX Security Symposium_, 2023.
* [81] G. E. Hinton, "Deterministic boltzmann learning performs steepest descent in weight-space," _Neural Computation_, vol. 1, pp. 143-150, 1989.
* [82] L. Wu, M. Ye, Q. Lei, J. D. Lee, and Q. Liu, "Steepest descent neural architecture optimization: Escaping local optimum with signed neural splitting," 2021.
* [83] S. Amari, "Natural gradient works efficiently in learning," _Neural Computation_, vol. 10, pp. 251-276, 1998.
* [84] R. Shrestha, "Natural gradient methods: Perspectives, efficient-scalable approximations, and analysis," 2023.
** [85] T. Hartland, G. Stadler, M. Perego, K. Liegeois, and N. Petra, "Hierarchical off-diagonal low-rank approximation of hessians in inverse problems, with application to ice sheet model initialization," _Inverse Problems_, vol. 39, 2023.
* [86] J. Martens and R. B. Grosse, "Optimizing neural networks with kronecker-factored approximate curvature," in _International Conference on Machine Learning (ICML)_, 2015.
* [87] Y. Wang, W. Deng, and G. Lin, "An adaptive hessian approximated stochastic gradient mcmc method," _Journal of Computational Physics_, vol. 432, p. 110150, 2020.
* [88] L. Liu, X. Liu, C.-J. Hsieh, and D. Tao, "Stochastic optimization for non-convex problem with inexact hessian matrix, gradient, and function," _IEEE transactions on neural networks and learning systems_, vol. PP, 2023.
* [89] X.-T. Yuan and P. Li, "On convergence of distributed approximate newton methods: Globalization, sharper bounds and beyond," _Journal of Machine Learning Research_, 2019.
* [90] E. Berglund and M. Johansson, "Novel limited memory quasi-newton methods based on optimal matrix approximation," 2024.
* [91] S. J. Wright, "Convergence of projected hessian approximations in quasi-newton methods for the nonlinear programming problem," _Ima Journal of Numerical Analysis_, vol. 6, pp. 463-474, 1986.
* [92] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, "An image is worth 16x16 words: Transformers for image recognition at scale," 2021.
* [93] I. Loshchilov and F. Hutter, "Decoupled weight decay regularization," 2019.
* [94] P. Maini, Z. Feng, A. Schwarzschild, Z. C. Lipton, and J. Z. Kolter, "Tofu: A task of fictitious unlearning for llms," _arXiv preprint arXiv:2401.06121_, 2024.
* [95] Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. Lee, "Textbooks are all you need ii: phi-1.5 technical report," _arXiv preprint arXiv:2309.05463_, 2023.
* [96] B. Liu, Q. Liu, and P. Stone, "Continual learning and private unlearning," in _Conference on Lifelong Learning Agents_. PMLR, 2022, pp. 243-254.
* [97] R. Zhang, L. Lin, Y. Bai, and S. Mei, "Negative preference optimization: From catastrophic collapse to effective unlearning," _arXiv preprint arXiv:2404.05868_, 2024.

## Appendix A Detailed Proof

### Proof of Equation (1)

Proof.: We form the following optimization problem for the steepest descent of approximate MU, which is to find the direction \(\delta\theta=\theta_{t+1}-\theta_{t}\) that drives the objective function \(F(\theta)\) descent fastest within a \(\xi\)-neighborhood of the current parameters \(\theta_{t}\), and the \(\xi\)-neighborhood is rendered by the manifold metric \(\rho(\cdot,\cdot)\):

\[\delta\theta=\operatorname*{arg\,min}_{\delta\theta}F(\theta_{t}+\delta\theta) \quad\text{s.t.}\quad\rho(\theta_{t},\theta_{t}+\delta\theta)\leq\xi. \tag{10}\]

We introduce a Lagrange multiplier \(\eta\geq 0\) to construct the Lagrangian \(\tilde{\mathcal{L}}\) for this optimization problem:

\[\tilde{\mathcal{L}}(\delta\theta,\eta)=F(\theta_{t}+\delta\theta)+\eta(\rho( \theta_{t},\theta_{t}+\delta\theta)-\xi). \tag{11}\]

Using the Karush-Kuhn-Tucker (KKT) theorem, we can take the derivative of \(\tilde{\mathcal{L}}\)\(w.r.t.\)\(\delta\theta\) and set it to zero:

\[\nabla_{\delta\theta}\tilde{\mathcal{L}}(\delta\theta,\eta)=\nabla_{\delta \theta}F(\theta_{t}+\delta\theta)+\eta\nabla_{\delta\theta}\rho(\theta_{t}, \theta_{t}+\delta\theta)=0, \tag{12}\]

where \(\eta\) depends on \(\xi\) and \(\theta_{t}\) implicitly. We can rewrite (12) by variable substitution \(\theta_{t+1}=\theta_{t}+\delta\theta\) and \(\eta=1/\alpha_{t}(\xi,\theta_{t})\).

\[\nabla_{\theta_{t+1}}\tilde{\mathcal{L}}(\theta_{t+1},\eta)=\nabla_{\theta_{t+ 1}}F(\theta_{t+1})+\frac{1}{\alpha_{t}(\xi,\theta_{t})}\nabla_{\theta_{t+1}} \rho(\theta_{t},\theta_{t+1})=0. \tag{13}\]

Thus, the original problem is transformed into an unconstrained optimization problem \(w.r.t.\)\(\theta_{t+1}\), where the neighborhood size is implicitly given by \(\alpha_{t}\):

\[\theta_{t+1}=\operatorname*{arg\,min}_{\theta_{t+1}}F(\theta_{t+1})+\frac{1}{ \alpha_{t}(\xi,\theta_{t})}\rho(\theta_{t},\theta_{t+1}). \tag{14}\]

### Proof of Proposition 1

The optimization problem of **Prop. 1** is to find the steepest descent direction that minimizes the KL divergence with the retrained output within the vicinity of the current model \(\theta_{t}\):

\[\theta_{t+1}=\operatorname*{arg\,min}_{\theta_{t+1}}D_{\text{KL}} \left(p_{z}(\theta_{*})||p_{z}(\theta_{t+1})\right)+\frac{1}{\alpha_{t}}\rho( \theta_{t},\theta_{t+1}) \tag{15}\] \[=\operatorname*{arg\,min}_{\theta_{t+1}}D_{\text{KL}}\left(p_{z \tau}(\theta_{*})||p_{z\tau}(\theta_{t+1})\right)p^{f}+D_{\text{KL}}\left(p_{ z^{\tau}}(\theta_{*})||p_{z^{\tau}}(\theta_{t+1})\right)p^{r}+\frac{1}{\alpha_{t}} \rho(\theta_{t},\theta_{t+1})\]

**Proposition 1**.: _Under the Euclidean manifold metric, \(\rho(\theta_{t},\theta_{t+1})=\frac{1}{2}\|\theta_{t}-\theta_{t+1}\|^{2}\). Assuming that the current model \(\theta_{t}=\operatorname*{arg\,min}_{\theta}\mathcal{L}^{r}(\theta)+\mathcal{ L}^{f}(\theta;\mathbf{\varepsilon}_{t})\). Let \(H^{f}_{*}=\nabla^{2}\mathcal{L}^{f}(\theta_{*};\mathbf{1})\)and \(H^{r}_{*}=\nabla^{2}\mathcal{L}^{r}(\theta_{*})\) denote the Hessian matrix of the retrained model on the forgetting set and the remaining set, respectively. Then, the direction of the steepest gradient descent that minimizes the KL divergence between the output of the current model and the retrained model is approximately:_

\[\theta_{t+1}-\theta_{t}:\approx-\alpha_{t}[H^{r}_{*}(H^{r}_{*})^{-1}[-\nabla \mathcal{L}^{f}(\theta_{t};\mathbf{\varepsilon}_{t})]p^{f}+\nabla\mathcal{L}^{r }(\theta_{t})p^{r}] \tag{16}\]Proof.: We can decompose the original optimization problem into three parts: \(F(\theta_{t+1})\), \(R(\theta_{t+1})\), and \(C(\theta_{t+1})\).

\[\theta_{t+1}= \operatorname*{arg\,min}_{\theta_{t+1}}D_{\text{KL}}\left(p_{z}( \theta_{*})||p_{z}(\theta_{t+1}))\right)+\frac{1}{2\alpha_{t}}\|\theta_{t}- \theta_{t+1}\|^{2}\] \[= \operatorname*{arg\,min}_{\theta_{t+1}}\int p(z;\theta_{*})\left[ \log p(z^{f};\theta_{*})-\log p(z^{f};\theta_{t+1})\right]\mathrm{d}z^{f}p( \mathcal{D}^{f}|\mathcal{D})\] \[+\int p(z^{r};\theta_{*})\left[\log p(z^{r};\theta_{*})-\log p(z^ {r};\theta_{t+1})\right]\mathrm{d}z^{r}p(\mathcal{D}^{r}|\mathcal{D})\] \[+\frac{1}{2\alpha_{t}}\|\theta_{t}-\theta_{t+1}\|^{2}\] \[= \operatorname*{arg\,min}_{\theta_{t+1}}\mathbb{E}_{p(z^{f}; \theta_{*})}\left[\log p(z^{f};\theta_{*})-\log p(z^{f};\theta_{t+1})\right]p( \mathcal{D}^{f}|\mathcal{D})\] \[+\mathbb{E}_{p(z^{r};\theta_{*})}\left[\log p(z^{r};\theta_{*})- \log p(z^{r};\theta_{t+1})\right]p(\mathcal{D}^{r}|\mathcal{D})\] \[+\frac{1}{2\alpha_{t}}\|\theta_{t}-\theta_{t+1}\|^{2}\] \[= \operatorname*{arg\,min}_{\theta_{t+1}}\underbrace{D_{\text{KL}} \left(p_{z^{f}}(\theta_{*})||p_{z^{f}}(\theta_{t+1})\right))}_{(F(\theta_{t+1} ))}p^{f}+\underbrace{D_{\text{KL}}\left(p_{z^{r}}(\theta_{*})||p_{z^{r}}( \theta_{t+1})\right))}_{(R(\theta_{t+1}))}p^{r}\] \[+\frac{1}{2\alpha_{t}}\underbrace{\left\|\theta_{t}-\theta_{t+1} \right\|^{2}}_{(C(\theta_{t+1}))}\] (A8)

**(Part I)** First, we solve the forgetting part \(F(\theta_{t+1})\) by taking the first-order approximation at \(\theta_{t}\):

\[F(\theta_{t+1})=F(\theta_{t})+\nabla F(\theta_{t})^{\top}(\theta_{t+1}-\theta_ {t}).\] (A9)

We denote \(\nabla F(\theta_{t})=-\mathbb{E}_{p(z^{f};\theta_{*})}\left[\nabla\log p(z^{f} ;\theta_{t})\right]=G(\theta_{t})\), and then expand \(G(\theta_{t})\) at \(\theta_{*}\):

\[G(\theta_{t}) =G(\theta_{*})+\nabla G(\theta_{*})(\theta_{t}-\theta_{*})\] \[=-\mathbb{E}_{p(z^{f};\theta_{*})}\left[\nabla\log p(z^{f};\theta _{*})\right]-\mathbb{E}_{p(z^{f};\theta_{*})}\left[\nabla^{2}\log p(z^{f}; \theta_{*})\right]\Delta_{t}\] \[=0+H_{*}^{f}\Delta_{t},\] (A10)

where \(H_{*}^{f}=-\mathbb{E}_{p(z^{f};\theta_{*})}\left[\nabla^{2}\log p(z^{f}; \theta_{*})\right]\) is the Hessian \(w.r.t.\) the retrained model \(\theta_{*}\) at forgetting set, and \(\Delta_{t}=\theta_{t}-\theta_{*}\) is the difference between the current model \(\theta_{t}\) and the retrained model \(\theta_{*}\). We cannot directly obtain the parameter difference \(\Delta_{t}\) and need to estimate it. Recalling the assumption that \(\theta_{t}=\operatorname*{arg\,min}_{\theta}\mathcal{L}^{r}(\theta)+\mathcal{ L}^{f}(\theta;\mathbf{\varepsilon}_{t})\) and \(\theta_{*}=\operatorname*{arg\,min}_{\theta}\mathcal{L}^{r}(\theta)\). We can utilize the optimality of \(\theta_{t}\) on the weighted function to take the derivative \(w.r.t.\)\(\theta_{t}\) and set it to zero:

\[0 =\nabla\mathcal{L}^{r}(\theta_{t})+\nabla\mathcal{L}^{f}(\theta_ {t};\mathbf{\varepsilon}_{t})\] \[=\left[\nabla\mathcal{L}^{r}(\theta_{*})+\nabla^{2}L^{r}( \theta_{*})\Delta_{t}+o(\Delta_{t})\right]+\nabla\mathcal{L}^{f}(\theta_{t}; \mathbf{\varepsilon}_{t})\] \[\approx 0+\nabla^{2}\mathcal{L}^{r}(\theta_{*})\Delta_{t}+ \nabla\mathcal{L}^{f}(\theta_{t};\mathbf{\varepsilon}_{t}).\] (A11)

Since \(\theta_{*}\) minimizes \(\mathcal{L}^{r}\), \(\nabla\mathcal{L}^{r}(\theta_{*})=0\). By performing the Taylor expansion and dropping \(o(\Delta_{t})\) terms, we have

\[\Rightarrow\Delta_{t}\approx-\left[\nabla^{2}\mathcal{L}^{r}(\theta_{*}) \right]^{-1}\nabla\mathcal{L}^{f}(\theta_{t};\mathbf{\varepsilon}_{t})=-\left(H_ {*}^{r}\right)^{-1}\nabla\mathcal{L}^{f}(\theta_{t};\mathbf{\varepsilon}_{t}).\] (A12)

By plugging (A12) into (A10), we can get

\[\nabla F(\theta_{t})=G(\theta_{t})\approx-H_{*}^{f}\left(H_{*}^{r}\right)^{-1} \nabla\mathcal{L}^{f}(\theta_{t};\mathbf{\varepsilon}_{t}).\] (A13)

Bringing (A13) into (A9), we can get

\[F(\theta_{t+1})\approx F(\theta_{t})-\left[H_{*}^{f}\left(H_{*}^{r}\right)^{-1} \nabla\mathcal{L}^{f}(\theta_{t};\mathbf{\varepsilon}_{t})\right]^{\top}(\theta_{t +1}-\theta_{t}).\] (A14)(**Part II**) Next, we solve the remaining part \(R(\theta_{t+1})\) with similar pipeline in solving \(F(\theta_{t+1})\):

\[R(\theta_{t+1}) =R(\theta_{t})+\nabla R(\theta_{t})^{\top}(\theta_{t+1}-\theta_{t})\] \[=R(\theta_{t})-\mathbb{E}_{p(z^{\tau};\theta_{*})}\left[\nabla \log p(z^{\tau};\theta_{t})\right]^{\top}(\theta_{t+1}-\theta_{t})\] \[=R(\theta_{t})+\left[\nabla\mathcal{L}^{r}(\theta_{t})\right]^{ \top}(\theta_{t+1}-\theta_{t}).\] (A15)

(**Part III**) Finally, we derive the constraint \(C(\theta_{t+1})\) as follows,

\[C(\theta_{t+1})= C(\theta_{t})+\nabla C(\theta_{t})^{\top}(\theta_{t+1}-\theta_{t})\] \[= C(\theta_{t})+2(\theta_{t+1}-\theta_{t})^{\top}(\theta_{t+1}- \theta_{t}).\] (A16)

Substituting (A14), (A15), and (A16) to each part in (A8), and take the derivative \(w.r.t.\)\(\theta_{t+1}\) of the minimization problem to derive the optimal solution, we have

\[0 =\nabla F(\theta_{t+1})p^{f}+\nabla R(\theta_{t+1})p^{r}+\frac{1} {2\alpha_{t}}\nabla C(\theta_{t+1})\] \[\approx H_{*}^{f}\left(H_{*}^{r}\right)^{-1}\left[-\nabla\mathcal{L}^{f} (\theta_{t};\mathbf{\varepsilon}_{t})\right]p^{f}+\nabla\mathcal{L}^{r}(\theta_{t} )p^{r}+\frac{1}{\alpha_{t}}(\theta_{t+1}-\theta_{t}).\] (A17)

We thus conclude that

\[\Rightarrow\theta_{t+1}-\theta_{t}\approx-\alpha_{t}\left[H_{*}^{f}(H_{*}^{r}) ^{-1}[-\nabla\mathcal{L}^{f}(\theta_{t};\mathbf{\varepsilon}_{t})]p^{f}+\nabla \mathcal{L}^{r}(\theta_{t})p^{r}\right].\] (A18)

### Proof of Proposition 2

**Proposition 2**.: _Using the model output KL divergence on the remaining set as the manifold metric, \(\rho(\theta_{t},\theta_{t+1})=D_{\text{KL}}\left(p_{z^{\tau}}(\theta_{t})||p_{ z^{\tau}}(\theta_{t+1})\right))\). Assuming that the current model \(\theta_{t}=\arg\min_{\theta}\mathcal{L}^{r}(\theta)+\mathcal{L}^{f}(\theta;\mathbf{ \varepsilon}_{t})\). Let \(\tilde{\alpha}_{t}=\alpha_{t}p^{f}/(\alpha_{t}p^{r}+1)\), and \(H_{t}^{r}=\nabla^{2}\mathcal{L}^{r}(\theta_{t})\) represents the Hessian matrix of the current model on the remaining set, then the direction of the steepest gradient descent that minimizes the KL divergence between the output of the current model and the retrained model is approximately:_

\[\theta_{t+1}-\theta_{t}:\approx-\tilde{\alpha}_{t}(H_{t}^{r})^{-1}\left[H_{*}^{ f}(H_{*}^{r})^{-1}[-\nabla\mathcal{L}^{f}(\theta_{t};\mathbf{\varepsilon}_{t})]\right]\] (A19)

Proof.: Now, the steepest descent optimization problem for approximate MU is as follows:

\[\theta_{t+1}= \operatorname*{arg\,min}_{\theta_{t+1}}D_{\text{KL}}\left(p_{z}( \theta_{*})||p_{z}(\theta_{t+1}))\right)+\frac{1}{\alpha_{t}}D_{\text{KL}} \left(p_{z^{\tau}}(\theta_{t})||p_{z^{\tau}}(\theta_{t+1})\right))\] \[= \operatorname*{arg\,min}_{\theta_{t+1}}\underbrace{D_{\text{KL}} \left(p_{z^{f}}(\theta_{*})||p_{z^{f}}(\theta_{t+1}))\right)}_{(F(\theta_{t+1} ))}p^{f}+\underbrace{D_{\text{KL}}\left(p_{z^{\tau}}(\theta_{*})||p_{z^{\tau }}(\theta_{t+1}))\right)}_{(R(\theta_{t+1}))}p^{r}\] \[+\frac{1}{\alpha_{t}}\underbrace{D_{\text{KL}}\left(p_{z^{\tau} }(\theta_{t})||p_{z^{\tau}}(\theta_{t+1}))\right)}_{(C(\theta_{t+1}))}\] (A20)

The result of forgetting part \(F(\theta_{t+1})\) is the same as that in the Euclidean distance metric. And the remaining part \(R(\theta_{t+1})\) and the constraint \(C(\theta_{t+1})\) vary due to the output KL divergence metric \(D_{\text{KL}}^{r}\). Note that \(\nabla\mathcal{L}^{r}(\theta_{t+1})\approx\nabla\mathcal{L}^{r}(\theta_{t}) \approx\nabla\mathcal{L}^{r}(\theta_{0})\approx 0\). This enables us to take the second-order Taylor expansion at \(\theta_{t}\) for the remaining part and the constraint.

\[R(\theta_{t+1})=R(\theta_{t})+\nabla R(\theta_{t})^{\top}(\theta_{t+1}-\theta_ {t})+\frac{1}{2}(\theta_{t+1}-\theta_{t})^{\top}\nabla^{2}R(\theta_{t})(\theta_ {t+1}-\theta_{t})\] (A21)

\[\nabla R(\theta_{t})=-\mathbb{E}_{p(z^{\tau};\theta_{*})}\left[\nabla\log p(z ^{r};\theta_{t})\right]=\nabla\mathcal{L}^{r}(\theta_{t})\approx 0\] (A22)

\[\nabla^{2}R(\theta_{t})=-\mathbb{E}_{p(z^{\tau};\theta_{*})}\left[\nabla^{2} \log p(z^{r};\theta_{t})\right]=\nabla^{2}\mathcal{L}^{r}(\theta_{t})=H_{t}^ {r}\] (A23)

Substituting (A22) and (A23) into (A21), we can derive the remaining part:

\[R(\theta_{t+1})\approx R(\theta_{t})+\frac{1}{2}(\theta_{t+1}-\theta_{t})^{ \top}H_{t}^{r}(\theta_{t+1}-\theta_{t})\] (A24)As for the constraint, we have

\[C(\theta_{t+1})=C(\theta_{t})+\nabla C(\theta_{t})^{\top}(\theta_{t+ 1}-\theta_{t})+\frac{1}{2}(\theta_{t+1}-\theta_{t})^{\top}\nabla^{2}C(\theta_{t} )(\theta_{t+1}-\theta_{t})\] (A25) \[\nabla C(\theta_{t})=-\mathbb{E}_{p(z^{r};\theta_{t})}\left[\nabla \log p(z^{r};\theta_{t})\right]=0\] (A26) \[\nabla^{2}C(\theta_{t})=-\mathbb{E}_{p(z^{r};\theta_{t})}\left[ \nabla^{2}\log p(z^{r};\theta_{t})\right]=F_{t}^{r}\approx H_{t}^{r}=\nabla^{2 }\mathcal{L}^{r}(\theta_{t})\] (A27)

Substituting (A26)and (A27) into (A25), we get the constraint

\[C(\theta_{t+1})\approx C(\theta_{t})+\frac{1}{2}(\theta_{t+1}-\theta_{t})^{\top }H_{t}^{r}(\theta_{t+1}-\theta_{t}).\] (A28)

Bringing (A14), (A24), and (A28) into (A20), and take the derivative w.r.t. \(\theta_{t+1}\) of the minimization problem to derive the optimal solution, we have

\[0 =\nabla F(\theta_{t+1})p^{f}+\nabla R(\theta_{t+1})p^{r}+\frac{1}{ \alpha_{t}}\nabla C(\theta_{t+1})\] (A29) \[\approx H_{*}^{f}\left(H_{*}^{r}\right)^{-1}\left[-\nabla\mathcal{L }^{f}(\theta_{t};\mathbf{\varepsilon}_{t})\right]p^{f}+H_{t}^{r}(\theta_{t+1}- \theta_{t})p^{r}+\frac{1}{\alpha_{t}}H_{t}^{r}(\theta_{t+1}-\theta_{t}).\] (A30)

By rearranging the terms, we get

\[\Rightarrow\theta_{t+1}-\theta_{t}:\approx-\tilde{\alpha}_{t}(H_{t}^{r})^{-1} \left[H_{*}^{f}(H_{*}^{r})^{-1}[-\nabla\mathcal{L}^{f}(\theta_{t};\mathbf{ \varepsilon}_{t})]\right].\] (A31)

### Proof of Proposition 3

**Proposition 3**.: _For implicit Hessian approximation in (5), suppose **(A1)**\(\beta_{t},\delta_{t}\) is small, \(\beta_{t}<\sqrt{\delta_{t}/|\nabla\mathcal{L}^{r}(\theta_{t})-[\nabla\mathcal{L }^{r}(\theta_{t})]^{2}|}\), **(A2)**\(\mathcal{L}^{r}\) is \(\mu\)-smooth, i.e., \(\|\nabla\mathcal{L}^{r}(\theta)-\nabla\mathcal{L}^{r}(\theta^{\prime})\|_{2}\leq \mu\|\theta-\theta^{\prime}\|_{2}\), and **(A3)** there exist an \(\zeta_{t}\)-neighborhood \(\mathcal{N}(\theta_{t}^{r},\zeta_{t})\) of the optimal model parameter \(\theta_{t}^{r}=\arg\min_{\theta_{t}^{f}}\mathcal{L}^{r}(\theta_{t}^{f})\), which includes \(\theta_{t}\) and \(\theta_{t}^{f}\). Then, the iterative update term approximately is,_

\[\theta_{t}-\theta_{t}^{r}:\approx\beta_{t}^{2}\left[\nabla^{2}\mathcal{L}^{r} (\theta_{t})\right]^{-1}\nabla\mathcal{L}^{u}(\theta_{t})=\beta_{t}^{2}(H_{t}^ {r})^{-1}\nabla\mathcal{L}^{u}(\theta_{t})\] (A32)

Proof.: The objective function of implicit Hessian approximation can be formulated as:

\[\min_{\theta_{t}^{f}}\mathcal{L}^{r}(\theta_{t}^{r})\quad\text{s.t.}\quad \theta_{t}^{f}=\theta_{t}-\beta_{t}\nabla\mathcal{L}^{u}(\theta_{t}).\] (A33)

We need to get the optimal parameter \(\theta_{t}^{r}\) that minimizes (A33), which means \(0=\frac{\partial\mathcal{L}^{r}(\theta_{t}^{r})}{\partial\theta_{t}^{r}}\). We can take the Taylor expansion at \(\theta_{t}\),

\[0=\frac{\partial\mathcal{L}^{r}(\theta_{t}^{r})}{\partial\theta_{t}^{r}}= \nabla\mathcal{L}^{r}(\theta_{t}^{r})=\nabla\mathcal{L}^{r}(\theta_{t})+H_{t} ^{r}(\theta_{t}^{r}-\theta_{t})+(\theta_{t}^{r}-\theta_{t})^{\top}\otimes \mathbf{T}\otimes(\theta_{t}^{r}-\theta_{t})+o(\zeta_{t})\] (A34)

where \(H_{t}^{r}=\nabla^{2}\mathcal{L}^{r}(\theta_{t})\) and \(\mathbf{T}\) represent the Hessian matrix and the third-order symmetric tensor on the remaining set, respectively, and \(\otimes\) denotes the Kronecker product.

From (**A2**) and (**A3**), we can reduce the first-order term to \(o(\mu\zeta_{t})\),

\[\|\nabla\mathcal{L}^{r}(\theta_{t}^{r})-\nabla\mathcal{L}^{r}(\theta_{t})\|_{ 2}\leq\mu\|\theta_{t}^{r}-\theta_{t}\|_{2}\leq\mu\zeta_{t}.\] (A35)

To simplify the second-order term with (**A1**), we have

\[(\theta_{t}^{r}-\theta_{t})^{\top}\otimes\mathbf{T}\otimes(\theta _{t}^{r}-\theta_{t})\] \[= (\theta_{t}^{f}-\theta_{t})^{\top}\otimes\mathbf{T}\otimes(\theta _{t}^{f}-\theta_{t})+o(\epsilon_{t})\] \[= \mathbf{C}\odot(\theta_{t}^{f}-\theta_{t})^{2}+o(\epsilon_{t})\] \[\approx \beta^{2}\left(\nabla\mathcal{L}^{u}(\theta_{t})\right)^{2}+o( \epsilon_{t})\] \[= \beta^{2}\nabla\mathcal{L}^{u}(\theta_{t})+o(\delta_{t})+o( \epsilon_{t})\] (A36)

Bringing (A35) and (A36) into (A34), we have

\[0=\nabla\mathcal{L}^{r}(\theta_{t}^{r})\approx H_{t}^{r}(\theta_{t}^{r}-\theta_{t})+\beta^{2}\nabla \mathcal{L}^{u}(\theta_{t})+o(\delta_{t})+o(\epsilon_{t})+o(\mu\epsilon_{t})+o( \epsilon_{t}^{2})\] (A37)

Then, we can derive

\[\theta_{t}-\theta_{t}^{r}\approx\beta_{t}^{2}(H_{t}^{r})^{-1}\nabla\mathcal{L} ^{u}(\theta_{t}).\] (A38)Related Works

### Related Works on Machine Unlearning

**Data Forgetting.** MU is driven by the imperative to remove the influence of specific data from pre-trained models [2, 3, 18, 64], intrinsically linked to differential privacy [18, 64, 65, 15, 66], which aims to enhance the privacy protection of training data. Exact MU, which is approached from a parameter probability perspective, has been thoroughly explored within convex optimization problems and linear models [18, 15, 39, 67, 16]. These studies have established methods allowing models to forget data exactly while adhering to a specified privacy budget, thus significantly reducing the risk of privacy attacks [68, 69, 70, 71, 72]. However, in deep learning models, exact data forgetting typically requires retraining from scratch [1], a process whose computational intensity makes it impractical for routine application. This challenge underscores an urgent need for developing more efficient unlearning techniques that do not compromise the model's utility.

**Gradient-based Approximate Machine Unlearning.** To enhance data forgetting efficiency, research has honed in on aligning the forgetting objective with the model's output probability distribution, termed approximate MU [3, 21]. Numerous studies [22, 19, 38, 20, 21, 23, 26, 73] have developed specialized loss functions to prompt the model to expunge specific data, aiming to mitigate the risks of privacy breaches such as membership inference [74, 68, 69, 70, 71, 72] and data reconstruction attacks [75, 76]. Echoing the phenomenon of catastrophic forgetting observed in continual learning [77, 78, 79], training on forgetting data has precipitated significant declines in overall performance on remaining data. Various strategies [26, 24, 25] have emerged to uphold the model's original generalization capabilities, predominantly through fine-tuning the remaining set. Our approach provides a comprehensive analysis of iterative strategies employed in gradient-based approximate MU methods and introduces curvature information from the remaining set to better preserve the model's generalization capabilities.

**Machine Unlearning for Generative Models.** Recent advancements in text-conditional image generation models have demonstrated remarkable capability in producing images that accurately reflect textual descriptions [40, 42, 41, 58]. Despite these achievements, extensive research [6, 12, 80] has underscored significant security and privacy concerns associated with these technologies. The mechanisms underlying these issues are not yet fully understood. In response, there is a critical demand for the development of MU methods to bolster the trustworthiness of these models, facilitating their wider adoption. While pioneering studies [12, 13, 14, 11] have begun to address concept deletion within diffusion models, the dual objectives of maintaining generalization and ensuring efficient data forgetting continue to pose significant challenges.

### Related Works on Steepest Descent in Optimization

**Steepest Descent and Natural Gradient.** Steepest Descent [81, 29, 27, 30, 82] is one of the foundational algorithms in optimization, particularly in the context of machine learning and neural networks. Despite its simplicity and widespread use, Steepest Descent can suffer from slow convergence, especially in ill-conditioned problems where the objective function's curvature varies significantly across different dimensions [29]. Natural Gradient [83, 30, 84] is proposed as an enhancement of the standard gradient descent that addresses some of its limitations by considering the underlying geometry of the parameter space. Natural Gradient employs the Fisher Information Matrix to scale the gradient adaptively, leading to more efficient optimization. Natural Gradient has been shown to significantly accelerate convergence and improve optimization performance in various applications, including deep learning [30, 27] and continual learning [28]. We get inspiration from these techniques and try to improve MU update direction with preserved set curvature information.

**Hessian Approximation.** Computing the exact Hessian matrix is often impractical for large-scale problems due to its computational and memory requirements. To address this, various Hessian Approximation methods have been developed. Notable approaches include Diagonal and Low-rank Approximations [85, 86, 34], Stochastic Approximation [87, 88, 89], and Quasi-Newton Methods [90, 91]. [50] reveals that the regularization-based method is an explicit approximation of the Hessian, while the meta-learning method is an implicit estimation of it. By leveraging second-order information in a computationally feasible manner, these techniques strike a balance between accuracy and efficiency, facilitating the optimization of large-scale problems.

### Preliminary on Diffusion Model

**Conditional Diffusion Models and Classifier-free guidance.** Diffusion models [40] have gained prominence in the field of generative modeling, particularly for their effectiveness in generating high-quality images. A sample \(x_{T}\) is sampled from a Gaussian distribution and gradually denoised for \(T\) time steps, finally recovering a clean sample \(x_{0}\). Conditional Diffusion Model [40] is a variant that conditions the generation process on additional information \(c\) such as class labels, text descriptions, or other modalities. In practice, the conditional diffusion model is trained to predict the noise \(\epsilon_{\theta}(x_{t}|c)\) to form the denoising process \(p_{\theta}(x_{t-1}|x_{t},c)\), where \(x_{t}\) is a noisy version of the input \(x\). The corresponding objective of the conditional diffusion model is typically formulated as:

\[\ell_{\mathrm{MSE}}(\theta;x,c)=\mathbb{E}_{t,c\sim\mathcal{N}(0,1)}\left[\| \epsilon-\epsilon_{\theta}(x_{t}|c)\|_{2}^{2}\right],\] (A39)

with \(t\) uniformly sampled from \(\{1,\ldots,T\}\). In this setting, classifier-free guidance [41] is proposed to encourage the sampling procedure to find \(x\) with high \(\log p(c|x)\). Then diffusion process is given by \(\hat{\epsilon}_{\theta}(x_{t}|c)=(1-w)\epsilon_{\theta}(x_{t}|\emptyset)+w \epsilon_{\theta}(x_{t}|c)\), where \(\hat{\epsilon}_{\theta}(x_{t}|c)\) represents the noise estimation obtained by the conditional diffusion model given \(c\), \(w\in[0,1]\) is the guidance weight, \(\emptyset\) denotes the 'null' condition. The generation process initiates with Gaussian noise \(\hat{x}_{T}\sim\mathcal{N}(0,1)\) and repeats denoising the data by \(\hat{\epsilon}_{\theta}(\hat{x}_{t}|c)\) to obtain \(\hat{x}_{t-1}\) until \(t=0\), producing the authentic data conditioned on \(c\).

**Latent Diffusion Models.** Direct training of conditional diffusion models in high-resolution pixel space is often computationally prohibitive. Latent diffusion models (LDMs) [42, 58] address this challenge with an image compression approach. Specifically, an autoencoder is trained using perceptual loss and a patch-based adversarial objective to master the process of perceptual image compression. The input images can be embedded into smaller latent representations with the learned encoder \(E\). The trained autoencoder facilitates the transition to a low-dimensional latent space where the diffusion model is more efficiently trained on the representations \(z=E(x)\) rather than on high-resolution images \(x\). Authentic images can then be generated by sampling a representation \(z\) from the diffusion model and subsequently reconstructed into an image with the learned decoder \(x=D(z)\).

## Appendix C Algorithm

We present the algorithm of our proposed SFR-on in **Alg. A1**. In steps 3-4, we first calculate the forget-remain balanced weight saliency mask. In steps 6-8, we compute the adaptive coefficients to weight the forgetting gradient ascent, followed by one step of forgetting update within the inner loop. In steps 9-13, we fine-tune the model on the remaining set. In step 14, we perform the slow weight update for the model in the outer loop.

## Appendix D Evaluation Metrics

### Evaluation Metrics for Image Classification

\(\bullet\) Forgetting accuracy (**FA**): FA is the accuracy of the unlearned model on the forgetting dataset \(\mathcal{D}^{f}\). _A favorable approximate MU method should reduce the disparity of FA with the retrained model._

\(\bullet\) Remaining accuracy (**RA**): RA is the accuracy of the unlearned model on the remaining dataset \(\mathcal{D}^{r}\). _A favorable approximate MU method should reduce the disparity of RA with the retrained model._

\(\bullet\) Testing accuracy (**TA**): TA is the accuracy of the unlearned model on the testing dataset \(\mathcal{D}^{t}\). \(\mathcal{D}^{t}\) in random subset forgetting is sampled from the same distribution as the remaining dataset, while in class-wise forgetting \(\mathcal{D}^{t}\) excludes the samples from the forgetting class. _A favorable approximate MU method should reduce the disparity of TA with the retrained model._

\(\bullet\) Membership inference attack success rate on \(\mathcal{D}^{f}\) (**MIA**): Following [33], we use a prediction entropy-based membership inference attack to evaluate the privacy preservation of the unlearned model. We first need to train an adversarial classifier to predict whether or not a particular example was contained in the training dataset. The prediction of the remaining dataset and the testing dataset by the unlearned model is collected to compute the label-agnostic prediction entropy for the attack classifier training. Specifically, a Logistic Regression classifier is trained on the remaining prediction entropy labeled as '1' and the testing prediction entropy labeled as '0'. Then, this attack classifier isapplied to the forgetting prediction entropy to predict the membership of these forgetting samples. The success rate of membership inference attack on \(\mathcal{D}^{f}\) is quantified by the true positive rate predicted by our classifier, \(\text{MIA}=TP/N^{f}\), where \(TP\) represents the count of forgetting samples still identified as training samples and \(N^{f}\) is the size of the forgetting dataset. Due to the limitations of the membership inference attack, the attack based on a linear classifier is weak and fails to distinguish all forgetting samples predicted by the retrained model as test samples. The more advanced shadow model-based sample-wise attack is too time-consuming to evaluate the MU method. Therefore, we only regard MIA as a readout function of the forgetting effect and advocate for the development of more precise and efficient membership inference attack techniques to enhance MU method evaluation. Note that _a favorable approximate MU method also should reduce the disparity of MIA with the retrained model._

\(\bullet\) Output KL divergence with the retrained model (\(D_{\text{KL}}\)): Given that it is impractical to traverse all sample spaces, we actually calculate the empirical output KL divergence with RT \(\theta_{*}\). We first collect the predicted class probabilities from both the unlearned and retrained models across the remaining and forgetting datasets. Then, we empirically compute the output KL divergence as follows:

\[D_{\text{KL}}=\frac{1}{N^{r}+N^{f}}\left(\sum_{i\in\mathcal{D}^{r}}\sum_{c\in \mathcal{C}}p(z_{i,c}^{r};\theta_{*})\log\frac{p(z_{i,c}^{r};\theta_{*})}{p(z_ {i,c}^{r};\theta_{u})}+\sum_{j\in\mathcal{D}^{f}}\sum_{c\in\mathcal{C}}p(z_{j, c}^{f};\theta_{*})\log\frac{p(z_{j,c}^{f};\theta_{*})}{p(z_{j,c}^{f};\theta_{u})} \right), \tag{40}\]

where \(\mathcal{C}\) denotes the set of the prediction classes, \(\theta_{u}\) is the unlearned model parameter, and \(z_{i,c}^{r}\) and \(z_{j,c}^{f}\) represent the \(i\)-th remaining sample posterior of the class \(c\) and the \(j\)-th forgetting sample posterior of the class \(c\), respectively. Note that even the output KL divergence between retrained models is not zero due to the randomness of the training algorithm. _A favorable approximate MU method is hoped to display as low output KL divergence with RT as possible._

\(\bullet\) Run-time efficiency (**RTE**): This measures the computation efficiency of an MU method. We record RTE in _minutes_. _MU methods are more efficient the lower their RTE are._ Clearly, RT is not an efficient MU method.

### Evaluation Metrics for Image Generation

\(\bullet\) Forgetting accuracy (**FA**): We additionally train models to classify the images generated by the unlearned generative model. For CIFAR-10, following [35], we use ResNet-34 from torchvision pre-trained on ImageNet and fine-tune it on CIFAR-10 for 20 epochs. For ImageNet, we directly use pre-trained ViT-L [92] from torchvision as the classifier. We compute the classification accuracy of 500 images generated by the unlearned model on each forgetting category as FA. _A favorable approximate MU method is hoped to exhibit as low FA as possible._

\(\bullet\) Frechet Inception Distance (**FID**): We evaluate the remaining image fidelity of the unlearned model by assessing the standard image generation metrics FID. The model performed class-wise forgetting of CIFAR-10 generates 1000 images for each remaining class for FID. We sample 10000 images from the unlearned DiT on ImageNet by randomly selecting the remaining classes to calculate FID. _A favorable approximate MU method is expected to achieve as high FID as possible._

## Appendix E Implementation Details

### Baselines

**MU methods for image classification**:

* **FT**[22, 19, 38]: It fine-tunes the pre-trained model only on the remaining dataset to obtain the unlearned model. The code source is [https://github.com/OPTML-Group/Unlearn-Sparse](https://github.com/OPTML-Group/Unlearn-Sparse).
* **GA**[20, 21]: It conducts gradient ascent for the pre-trained model only on the forgetting dataset to obtain the unlearned model. The code source is [https://github.com/OPTML-Group/Unlearn-Sparse](https://github.com/OPTML-Group/Unlearn-Sparse).
* **RL**[20]: It replaces the forgotten set label with a random label that is not the same as the original label. Then, the modified forgetting set and the retained set are combined for fine-tuning the pre-trained model with the cross-entropy loss. The code source is [https://github.com/OPTML-Group/Unlearn-Saliency](https://github.com/OPTML-Group/Unlearn-Saliency).
* **SalUn**[26]: It first obtained the top-\(k\)% large salient weight mask sorted by the absolute value of parameter gradient to the forgetting set loss. The pre-trained model is then fine-tuned using the same pipeline as for RL, and the gradient is modified using the weight saliency mask so that only the top-\(k\)% significant parameters are optimized. The code source is [https://github.com/OPTML-Group/Unlearn-Saliency](https://github.com/OPTML-Group/Unlearn-Saliency).
* **BT**[23]: It stores pre-trained and randomly initialized models as competent and incompetent teachers, respectively. The unlearned model is acquired by minimizing the output KL divergence of the pre-trained model with the incompetent teacher on the forgetting set and with the competent teacher on the remaining set. The code source is [https://github.com/vikram2000b/bad-teaching-unlearning](https://github.com/vikram2000b/bad-teaching-unlearning).
* **SCRUB**[25]: It only saves the pre-trained model as the teacher model. Then, it optimizes the pre-trained model to minimize the KL divergence with the teacher on the remaining set and maximize the KL divergence with the teacher on the forgetting set. The cross-entropy loss of the remaining set is added to further maintain the performance. The code source is [https://github.com/meghdadk/SCRUB](https://github.com/meghdadk/SCRUB).

**MU methods for image generation**:

* **SA**[35]: It needs to generate replaying remaining samples using pre-trained models in advance and calculate fisher diagonal on the replaying remaining samples for modulating parameter regularization terms. Its unlearning loss is composed of three components: the MSE loss to make the output generated by the pre-trained model for forgetting classes or concepts close to random noise, the generative model MSE loss optimized on the replaying remaining samples, and a parameter regularization term to maintain the pre-trained model parameters with second-order curvature modulation of Fisher diagonal. The code source is [https://github.com/clear-nus/selective-ammesia](https://github.com/clear-nus/selective-ammesia).
* **ESD**[11]: It saves a frozen SD as a copy. The SD model is then optimized so as to push the condition score for the erasing concept far away from the corresponding condition score generated by the frozen SD, and align with the unconditioned score generated by the copy. The code source is [https://github.com/rohitgandikota/erasing](https://github.com/rohitgandikota/erasing).

### Training Details for Image Classification

For **CIFAR-10**, **CIFAR-100**, and **SVHN** using **ResNet-18**, all methods use the SGD optimizer with momentum of \(0.9\), weight decay of \(5\times 10^{-4}\), and batch size of \(128\). Our SFR-on train 1500 steps with the constant outer loop learning rate of \(\alpha=1.0\), inner loop iteration number \(T_{\text{in}}=5\). SFR-on search inner loop learning rate for forgetting in range \([0.1,0.5]\) and for remaining in range \([10^{-3},10^{-2}]\), temperature scalar \(\lambda\) in range \([0.0,2.0]\), and threshold \(\gamma\) in list \([0.3,1.0,3.0,10.0]\). Experiments are run on 1 RTX 4090. A summary of the hyperparameters for each method is shown in **Tab. A1**.

For **TinyImageNet**, **Swin-T** is initialized from torchvision weight pre-trained on ImageNet. All methods use the AdamW optimizer [93] with weight decay of \(0.05\) and batch size of \(128\). Our SFR-on train 500 steps with the constant outer loop learning rate \(\alpha=1.0\), inner loop iteration number \(T_{\text{in}}=1\). SFR-on search inner loop learning rate for forgetting in range \([0.001,0.1]\) and for remaining in range \([10^{-5},10^{-4}]\), temperature scalar \(\lambda\) in range \([0.0,2.0]\), and threshold \(\gamma\) in list \([0.3,1.0,3.0,10.0]\). Experiments are run on 1 RTX 4090. A summary of the hyperparameters for each method is shown in **Tab. A2**.

### Training Details for Image Generation

For **CIFAR-10**, following [35], we use **DDPM1** based on U-Net architecture with \(1000\) timesteps for linear \(\beta\) schedule. All methods use Adam optimizer with a constant learning rate of \(10^{-4}\) and batch size of \(128\). Pretrain and RT train for \(800\)K steps. SA generates \(5000\) images as the remaining set for replaying and calculating the Fisher diagonal, and trains for \(20\)K steps with \(\lambda=10\) for the regularization term. SalUn obtains top-\(50\)% salient weight mask and trains for \(1\)K steps with \(\alpha=10^{-3}\) to balance the optimization of forgetting and remaining. Our SFR-on trains for \(50\) stepswith \(T_{\text{in}}=1,\alpha=1.0,\beta^{f}=10^{-3},\beta^{r}=10^{-4},\lambda=0.5,\gamma=3\). Experiments are run on 2 RTX 4090s.

For **ImageNet**, we use pre-trained **DiT-XL/2** with \(256\times 256\) resolution. All methods use AdamW optimizer with a constant learning rate of \(10^{-4}\) and batch size of \(1\). SA calculates the Fisher diagonal on randomly sampled \(2000\) remaining data, and trains for \(10\)K steps with \(\lambda=10\) for the regularization term. SalUn obtains top-\(50\)% salient weight mask and trains for \(10\)K steps with \(\alpha=10^{-3}\) to balance the optimization of forgetting and remaining. Our SFR-on trains for \(500\) steps with \(T_{\text{in}}=1,\alpha=1.0,\beta^{f}=10^{-7},\beta^{r}=10^{-4},\gamma=3\). Since the batch size is \(1\), we ignore \(\lambda\) in adaptive coefficients. Experiments are run on 1 RTX 4090.

### Training Details for Concept Forgetting

For **concept forgetting of 'nudity'**, following [26], we use **SD V1.43** to generate \(1\)K images with the prompt 'a photo of a nude person' as the forgetting set and additional \(1\)K images with the prompt 'a photo of a person wearing clothes' as the remaining set. All methods use Adam optimizer with a constant learning rate of \(10^{-5}\) and batch size of \(1\). ESD trains for \(1\)K steps with negative guidance of \(1.0\). SalUn trains for \(1\)K steps with \(50\)% sparsity weight saliency and \(\alpha=0.1\). Our SFR-on trains for \(200\) steps with \(T_{\text{in}}=1,\alpha=1.0,\beta^{f}=10^{-6},\beta^{r}=10^{-5},\gamma=3\). Since the batch size is \(1\), we ignore \(\lambda\) in adaptive coefficients. Experiments are run on 1 RTX 4090.

Footnote 3: [https://github.com/facebookresearch/DiT](https://github.com/facebookresearch/DiT)

Footnote 3: [https://huggingface.co/CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4)

## Appendix F Additional Experimental Results

### Ablation Study on Temperature Scalar \(\lambda\) and Threshold \(\gamma\)

In our SFR-on method, we investigate the impact of two key hyperparameters, temperature scalar \(\lambda\) for adaptive coefficients and threshold \(\gamma\) for the weight saliency mask, on the unlearning performance in image classification. We vary \(\lambda\) within the range \([0,1]\) and \(\gamma\) within \([0,2]\). The results in **Fig.**A1 and **A2** indicate that increasing \(\gamma\) and \(\lambda\) enhances model retention capabilities, but it could adversely affect both the forgetting performance and privacy protection. Therefore, we select the hyperparameters for our method based on the goal of minimizing the output KL divergence from Retrain, aligning with the objectives of approximate MU.

### Additional Results for Image Classification

We conducted experiments to assess the performance of unlearning a 50% random subset of CIFAR-10 and a 10% random subset on two additional datasets, CIFAR-100 and SVHN. The results in **Tab.**A3, A4, and A5 demonstrate that our SFR-on method consistently achieves the closest average metric disparity and the smallest KL divergence relative to Retrain across all three scenarios. These findings emphasize the broad unlearning efficacy of our approach.

### Results for Natural Language Processing

The input modality of the model does not constrain our analysis or methods. Therefore, our method can seamlessly extend to other modalities beyond images, such as natural language processing using large language models (LLMs), to achieve efficient forgetting. We conduct experiments using the recently proposed benchmark of TOFU [94] fine-tuned Phi-1.5 [95] to evaluate the effectiveness of our method in the LLM unlearning task, compared with four LLM unlearning baselines: gradient descent(GA), gradient difference(GradDiff [96]), negative preference optimization(NPO [97]), and its enhanced version. The TOFU dataset comprises fictional author biographies, along with questions and answers related to the authors' attributes, which helps assess methods of data forgetting on fine-tuned LLMs.

As shown in **Tab.**A6, our method achieves superior forgetting quality, making the unlearned model almost indistinguishable from the retrained model based on the Truth Ratio distribution of the forget set. Additionally, our method efficiently preserves model utility.

### Ablation Study on SFR-on without Repairing

We conduct ablation experiments to assess the performance of our 'Sample-wise Adaptive Coefficient for Gradient Ascent (F)' and 'Forget-Remain Balanced Weight Saliency (S)' in the absence of repairing with the remaining set. The results in **Tab.**A7 affirm that these components, when used independently, can still enhance baseline performance.

### Results for Removing Influence of A Single Data Point

Our method can be directly applied to the task of forgetting a single data point without additional adaptation, as we impose no assumptions to constrain the size of the forgetting set. We conduct experiments under two setups to validate the effectiveness of our method in unlearning either a single

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{4}{c|}{**CIFAR-100 Random Subset Forgetting (10\%)**} \\  & FA & RA & TA & MIA & Avg.D ↓ \(|\)\(|\)\(|\)\(|\)\(|\)\(|\)\(|\)\(|\) RTE \\ \hline RT & 77.96\(\pm\)0.30 (0.00) & 99.85\(\pm\)0.00 (0.00) & 77.23\(\pm\)0.30 (0.00) & 41.79\(\pm\)0.01 (0.00) & 0.00 & 0.35 & 84.21 \\ \hline FT & 83.99\(\pm\)0.30 (6.04) & 98.34\(\pm\)0.26 (1.15) & 75.28\(\pm\)0.26 (1.95) & 60.92\(\pm\)0.19 (1.19) & 7.07 & 0.66 & 4.87 \\ GA & 70.57\(\pm\)0.74 (7.38) & 71.53\(\pm\)0.88 (2.45) & 51.48\(\pm\)0.83 (2.57) & 42.39\(\pm\)0.26 (0.60) & 15.55 & 1.43 & 0.73 \\ RL & 99.39\(\pm\)0.08 (21.44) & 99.96\(\pm\)0.01 (0.02) & 72.62\(\pm\)0.11 (4.61) & 44.68\(\pm\)0.04 (2.89) & 7.24 & 1.01 & 4.81 \\ SalUn & 99.40\(\pm\)0.20 (21.4) & 99.96\(\pm\)0.01 (0.02) & 73.05\(\pm\)0.20 (1.49) & 45.28\(\pm\)0.10 (3.49) & 7.29 & 0.99 & 4.90 \\ ST & 99.63\(\pm\)0.07 (21.68) & 9.97\(\pm\)0.01 (0.01) & 73.28\(\pm\)0.10 (3.95) & 45.25\(\pm\)0.04 (3.46) & 7.28 & 0.60 & 6.39 \\ SCRUB & 90.32\(\pm\)0.23 (12.36) & 99.11\(\pm\)0.14 (0.87) & 78.19\(\pm\)0.10 (0.95) & 43.38\(\pm\)0.00 (1.60) & 3.95 & 0.61 & 4.04 \\ \hline SFR-on & 78.2\(\pm\)0.78 (0.14) & 99.77\(\pm\)0.02 (0.22) & 74.76\(\pm\)0.28 (2.47) & 47.57\(\pm\)0.00 (5.78) & 2.15 & 0.60 & 5.41 \\ \hline \hline \end{tabular}
\end{table}
Table A4: MU performance for unlearning 10% random subset of CIFAR-100 using ResNet-18. The content format follows **Tab.**2.

[MISSING_PAGE_FAIL:26]

within deep models and fortifies the security of these systems against potential privacy attacks. When deployed on public networks, our approach facilitates the continuous update of knowledge, enabling models to catch new developments without losing performance. In the case of generative models, implementing SFR-on reduces the likelihood of generating improper content and limits the possibility of infringing on intellectual property rights. This helps to enhance public confidence in machine learning technologies and encourages their broader acceptance and use.

**Limitations.** We acknowledge the limitations of our study and encourage further exploration. While the theoretical framework of SFR-on accommodates various input modalities, this paper does not extend evaluations to include language models or graph neural networks. Consequently, we cannot ascertain the direct applicability of our method to these modalities without adaptation. Furthermore, the absence of an asymptotic analysis for the steepest descent in machine unlearning hinders our ability to determine the disparity between our approximation method and the optimal direction, which is crucial for refining the approach. We advocate for research to address these gaps in the future.

Figure A4: More class-wise unlearning results on classifier-free guidance DDPM on CIFAR-10. The forgetting class is marked with a red color. (More results will be shown in **Fig. A5** and **Fig. A6**)

Figure A5: More class-wise unlearning results on classifier-free guidance DDPM on CIFAR-10. The forgetting class is marked with a red color (Extended results from **Fig. A4**).

Figure A6: More class-wise unlearning results on classifier-free guidance DDPM on CIFAR-10. The forgetting class is marked with a red color (Extended results from **Fig. A4**).

Figure A8: Additional results of generated images using SFR-on. From the rows below, diagonal images represent the forgetting class, while non-diagonal images represent the remaining class. (Extended results from **Fig. A7**)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We briefly describe the scope and contributions of this paper in the abstract, detail the areas of focus in the introduction, and list the main contributions at the end of the introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our work in Appendix G. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]

Justification: All formulas appearing in this paper are reasonably numbered and cross-referenced. We all clarify the theory assumptions in detail and provide complete proof in Appendix A.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide the experimental setup, training details, and hyperparameters in Appendix E to ensure reproducibility. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is provided in supplemental material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide detailed experimental setup, training details, and hyperparameters in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: This paper provides the mean and variance for most of the experiments on image classification. However, due to the limitation of computing resources, most experiments on image generation have only been carried out once, which may not guarantee the statistically significant results of these experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ** The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The computational resources used in the experiments are indicated in Appendix E and the computational time overhead is given. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Our submission preserves anonymity. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the broader impacts in Appendix G. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We have safeguarded the potentially unsafe images appearing in the paper. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All original papers and codebases are appropriately cited in this paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.