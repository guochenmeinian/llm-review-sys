# Towards Understanding the Dynamics of Gaussian-Stein Variational Gradient Descent

 Tianle Liu

Department of Statistics

Harvard University

Cambridge, MA 02138

tianleliu@fas.harvard.edu

&Promit Ghosal

Department of Mathematics

Massachusetts Institute of Technology

Waltham, MA 02453

promit@mit.edu

&Krishnakumar Balasubramanian

Department of Statistics

University of California, Davis

Davis, CA 95616

kbala@ucdavis.edu

&Natesh S. Pillai

Department of Statistics

Harvard University

Cambridge, MA 02138

pillai@fas.harvard.edu

###### Abstract

Stein Variational Gradient Descent (SVGD) is a nonparametric particle-based deterministic sampling algorithm. Despite its wide usage, understanding the theoretical properties of SVGD has remained a challenging problem. For sampling from a Gaussian target, the SVGD dynamics with a bilinear kernel will remain Gaussian as long as the initializer is Gaussian. Inspired by this fact, we undertake a detailed theoretical study of the Gaussian-SVGD, i.e., SVGD projected to the family of Gaussian distributions via the bilinear kernel, or equivalently Gaussian variational inference (GVI) with SVGD. We present a complete picture by considering both the mean-field PDE and discrete particle systems. When the target is strongly log-concave, the mean-field Gaussian-SVGD dynamics is proven to converge linearly to the Gaussian distribution closest to the target in KL divergence. In the finite-particle setting, there is both uniform in time convergence to the mean-field limit and linear convergence in time to the equilibrium if the target is Gaussian. In the general case, we propose a density-based and a particle-based implementation of the Gaussian-SVGD, and show that several recent algorithms for GVI, proposed from different perspectives, emerge as special cases of our unifying framework. Interestingly, one of the new particle-based instance from this framework empirically outperforms existing approaches. Our results make concrete contributions towards obtaining a deeper understanding of both SVGD and GVI.

## 1 Introduction

Sampling from a given target density arises frequently in Bayesian statistics, machine learning and applied mathematics. Specifically, given a potential \(V:\mathbb{R}^{d}\to\mathbb{R}\), the target density is given by

\[\rho(x)\coloneqq Z^{-1}e^{-V(x)},\quad\text{where}\quad Z\coloneqq\int e^{-V(x )}dx\quad\text{is the normalizing constant}.\]

Traditionally-used Markov Chain Monte Carlo (MCMC) sampling algorithms are invariably not scalable to large-scale datasets [8, 61]. Variational inference and particle-based methods are two related alternatives proposed in the literature, both motivated by viewing sampling as optimization over the space of densities. We refer to [33, 39] for additional details related to this line of works.

In the literature on variational inference, recent efforts have focused on the Gaussian Variational Inference (GVI) problem. On the theoretical side, this is statistically motivated by the Bernstein-vonMises theorem, which posits that in the limit of large samples posterior distributions tend to be Gaussian distributed under certain regularity assumptions. We refer to [81, Chapter 10] for details of the classical results, and to [34, 73] for some recent non-asymptotic analysis. On the algorithmic side, efficient algorithms with both statistical and computational guarantees are developed for GVI [14, 20, 1, 35, 43, 19]. From a practical point-of-view, several works [64, 78, 80, 67] have shown superior performance of GVI, especially in the presence of large datasets.

Turning to particle-based methods, [53] proposed the Stein Variational Gradient Descent (SVGD) algorithm, a kernel-based deterministic approach for sampling. It has gained significant attention in the machine learning and applied mathematics communities due to its intriguing theoretical properties and wide applicability [23, 31, 56, 87]. Researchers have also developed variations of SVGD motivated by algorithmic and applied challenges [89, 49, 18, 28, 84, 12, 55, 71]. In its original form, SVGD could be viewed as a nonparametric variational inference method with a kernel-based practical implementation.

The flexibility offered by the _nonparametric_ aspect of SVGD also leads to unintended consequences. On one hand, from a practical perspective, the question of how to pick the right kernel for implementing the SVGD algorithm is unclear. Existing approaches are mostly ad-hoc and do not provide clear instructions on the selection of kernels. On the other hand, developing a deeper theoretical understanding of SVGD dynamics is challenging due to its nonparametric formulation. Notably [58] derived the continuous-time PDE for the evolving density that emerges as the mean-field limit of the finite-particle SVGD systems, and shows the well-posedness of the PDE solutions. In general, the following different types of convergences could be examined regarding SVGD:

1. Unified convergence of the empirical measure for \(N\) finite particles to the continuous target as time \(t\) and \(N\) jointly grow to infinity;
2. Convergence of mean-field SVGD to the target distribution over time;
3. Convergence of the empirical measure for finite particles to the mean-field distribution at any finite given time \(t\in[0,\infty)\);
4. Convergence of finite-particle SVGD to the equilibrium over time;
5. Convergence of the empirical measure for finite particles to the continuous target at time \(t=\infty\).

From a practical point of view (a) is the ideal type of result that fully characterizes the algorithmic behavior of SVGD, which could be obtained by combining either (b) and (c) or (d) and (e). Regarding (b), [51] showed the convergence of mean-field SVGD in kernel Stein discrepancy (KSD, [17, 52, 29]), which is known to imply weak convergence under appropriate assumptions. [40, 15, 70, 75, 22] sharpened the results with weaker conditions or explicit rates. [32] extended the above result to the stronger Fisher information metric and Kullback-Leibler divergence based on a regularization technique. [58, 30, 40] obtained time-dependent mean-field convergence (c) of \(N\) particles under various assumptions using techniques from the literature of _propagation of chaos_. [72] obtained even stronger results for (c) and combined (b) to get the first unified convergence (a) in terms of KSD. However, they have a rather slow rate \(1/\sqrt{\log\log N}\), resulting from the fact that their bounds for (c) still depends on the time \(t\) (sum of step sizes) double-exponentially. Moreover, there has not been any work that studies the convergence (d) and (e) for SVGD, which illustrate a new way to characterize the unified convergence (a).

In an attempt to overcome the drawbacks of the nonparametric formulation of SVGD and also taking cue from the GVI literature, in this work we study the dynamics of Gaussian-SVGD, a parametric formulation of SVGD. Our contributions in this work are three-fold:

* _Mean-field results:_ We study the dynamics of Gaussian-SVGD in the mean-field setting and establish linear convergence for both Gaussian and strongly log-concave targets. As an example of the obtained results, Table 1 shows the convergence rates of covariance for centered Gaussian families for several revelant algorithms. All of them will be introduced later in Sections 2 and 3. We also establish the well-posedness of the solutions for the mean-field PDE and discrete particle systems that govern SVGD with bilinear kernels (see Appendix C). Prior work [58] requires that the kernel be radial which rules out the important class of bilinear kernels that we consider. [32] relaxed the radial kernel assumption, however, they required boundedness assumptions which we avoid in this work for the case of bilinear kernels.
* _Finite-particle results:_ We study the finite-particle SVGD systems in both continuous and discrete time for Gaussian targets. We show that for SVGD with a bilinear kernel if the target and initializer are both Gaussian, the mean-field convergence can be uniform in time (See Theorem 3.7). To the best of our knowledge, this is the first uniform in time result for SVGD dynamics and should be contrasted with the double exponential dependency on \(t\) for nonparametric SVGD [58; 72]. Our numerical simulations suggest that similar results should hold for certain classes of non-Gaussian target as well for Gaussian-SVGD. We also study the convergence (d) by directly solving the finite-particle systems (See Theorems 3.6 and 3.8). Moreover, in Theorem 3.10, assuming centered Gaussian targets, we obtain a linear rate for covariance convergence in the finite-particles, discrete-time setting, precisely characterizing the step size choice for the practical algorithm.
* _Unifying algorithm frameworks:_ We propose two unifying algorithm frameworks for finite-particle, discrete-time implementations of the Gaussian-SVGD dynamics. The first approach assumes access to samples from Gaussian densities with the mean and covariance depending on the current time instance of the dynamics. The second is a purely particle-based approach, in that, it assumes access only to an initial set of samples from a Gaussian density. In particular, we show that three previously proposed methods from [27; 43] for GVI emerge as special cases of the proposed frameworks, by picking different bilinear kernels, thereby further strengthening the connections between GVI and the kernel choice in SVGD. Furthermore, we conduct experiments for eight algorithms that can be implied from our framework, and observe that the particle-based algorithms are invariably more stable than density-based ones. Notably one of the new particle-based algorithms emerging from our analysis outperforms existing approaches.

## 2 Preliminaries

Denote the space of probability densities on \(\mathbb{R}^{d}\) by \(\mathcal{P}(\mathbb{R}^{d}):=\big{\{}\rho\in\mathcal{F}(\mathbb{R}^{d}):\int \rho\,\mathrm{d}\mathbf{x}=1,\rho\geq 0\big{\}}\), where \(\mathcal{F}(\mathbb{R}^{d})\) is the set of smooth functions. As studied in [42], \(\mathcal{P}(\mathbb{R}^{d})\) forms a Frechet manifold called the density manifold. For any "point" \(\rho\in\mathcal{P}(\mathbb{R}^{d})\), we denote the tangent space and cotangent space at \(\rho\) by \(T_{\rho}\mathcal{P}(\mathbb{R}^{d})\) and \(T_{\rho}^{*}\mathcal{P}(\mathbb{R}^{d})\) respectively. A Riemannian metric tensor assigns to each \(\rho\in\mathcal{P}(\mathbb{R}^{d})\) a positive definite inner product \(g_{\rho}:T_{\rho}\mathcal{P}(\mathbb{R}^{d})\times T_{\rho}\mathcal{P}( \mathbb{R}^{d})\rightarrow\mathbb{R}\) and uniquely corresponds to an isomorphism \(G_{\rho}\) (called the canonical isomorphism) between the tangent and cotangent bundles [66], i.e., we have \(G_{\rho}:T_{\rho}\mathcal{P}(\mathbb{R}^{d})\to T_{\rho}^{*}\mathcal{P}( \mathbb{R}^{d})\).

**Definition 2.1** (Wasserstein metric).: _The Wasserstein metric is induced by the following canonical isomorphism \(G_{\rho}^{\mathrm{Wass}}:T_{\rho}\mathcal{P}(\mathbb{R}^{d})\to T_{ \rho}^{*}\mathcal{P}(\mathbb{R}^{d})\) such that_

\[(G_{\rho}^{\mathrm{Wass}})^{-1}\Phi=-\nabla\cdot(\rho\nabla\Phi),\quad\Phi\in T _{\rho}^{*}\mathcal{P}(\mathbb{R}^{d}).\]

The Wasserstein gradient flow (WGF) can be seen as the natural gradient flow for KL divergence on the density manifold with respect to the Wasserstein metric. In specific, the mean-field PDE is given by the linear Fokker-Planck equation [33]:

\[\dot{\rho}_{t}=-(G_{\rho_{t}}^{\mathrm{Wass}})^{-1}\tfrac{\delta}{\delta\rho_ {t}}\operatorname{KL}(\rho_{t}\parallel\rho^{*})=\nabla\cdot\Big{(}\rho_{t} \nabla\tfrac{\delta}{\delta\rho_{t}}\operatorname{KL}(\rho_{t}\parallel\rho^{*} )\Big{)}=\nabla\cdot(\nabla\rho_{t}+\rho_{t}\nabla V), \tag{1}\]

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline  & \(K_{1}\)-SVGD & \(K_{2}\)-SVGD & WGF (\(K_{3}\)-SVGD) & R-SVGD (\(K_{4}\)-SVGD) \\ \hline Centered Gaussian & \(\mathcal{O}(e^{-2t})\)[3.2] & \(\mathcal{O}(e^{-2t})\) [F.2] & \(\mathcal{O}(e^{-\frac{2t}{4}})\) [F.1] & \(\mathcal{O}(e^{-\frac{2t}{(1-\rho)\lambda+\nu}})\)[3.4] \\ \hline General Gaussian & Theorem 3.1 & \(\mathcal{O}\big{(}e^{-\frac{1}{\lambda}\wedge 2t}\big{)}\) [F.2] & \(\mathcal{O}(e^{-\frac{t}{4}})\) [F.1] & \(\mathcal{O}\big{(}e^{-\frac{1}{\lambda}\wedge\frac{2}{(1-\rho)\lambda+\nu}t} \big{)}\)[3.4] \\ \hline \hline \end{tabular}
\end{table}
Table 1: Convergence rates of SVGD with different bilinear kernels for Gaussian families.

where \(\frac{\delta}{\partial\rho_{t}}\) denotes the variational derivative with respect to \(\rho_{t}\), \(\mathrm{KL}(\rho\parallel\rho^{*})\) is the so-called energy function, and \(V(\mathbf{x})\) is the potential function that satisfies \(\rho^{*}(\mathbf{x})\propto\exp(-V(\mathbf{x}))\).

Interestingly, the mean field flow of SVGD can also be seen as a gradient flow for the KL divergence but with respect to the Stein metric, where we perform kernelization in the cotangent space before taking the divergence [51, 22].

**Definition 2.2** (Stein metric).: _The Stein metric is induced by the following canonical isomorphism \(G^{\text{Stein}}_{\rho}:T_{\rho}\mathcal{P}(\mathbb{R}^{d})\to T^{*}_{\rho} \mathcal{P}(\mathbb{R}^{d})\) such that_

\[(G^{\text{Stein}}_{\rho})^{-1}\Phi=-\nabla\cdot\big{(}\rho(\cdot)\int K(\cdot, \mathbf{y})\rho(\mathbf{y})\nabla\Phi(\mathbf{y})\,\mathrm{d}\mathbf{y}\big{)}\,,\quad\Phi\in T ^{*}_{\rho}\mathcal{P}(\mathbb{R}^{d}),\]

_where \(K:\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}\) is a positive-definite kernel._

In particular, the mean field PDE of the SVGD algorithm can be written as

\[\dot{\rho}_{t}=-(G^{\text{Stein}}_{\rho_{t}})^{-1}\tfrac{\delta}{\delta\rho_{ t}}\,\mathrm{KL}(\rho_{t}\parallel\rho^{*})=\nabla\cdot\big{(}\rho_{t}(\cdot) \int K(\cdot,\mathbf{y})\big{(}\nabla\rho_{t}(\mathbf{y})+\rho_{t}(\mathbf{y})\nabla V(\bm {y})\big{)}\,\mathrm{d}\mathbf{y}\big{)}\,. \tag{2}\]

Gaussian Families as Submanifolds.We consider the family of (multivariate) Gaussian densities \(\rho_{\theta}\in\mathcal{P}(\mathbb{R}^{d})\) where \(\theta=(\mathbf{\mu},\Sigma)\in\Theta=\mathbb{R}^{d}\times\mathrm{Sym}^{+}(d, \mathbb{R})\). Note that \(\mathrm{Sym}^{+}(d,\mathbb{R})\) is the set of (symmetric) positive definite \(d\times d\) matrices. In this way, \(\Theta\) can be identified as a Riemannian submanifold of the density manifold \(\mathcal{P}(\mathbb{R}^{d})\) with the induced Riemannian structure. If we further restrict the Gaussian family to have zero mean, it further induces the submanifold \(\Theta_{0}=\mathrm{Sym}^{+}(d,\mathbb{R})\). Notably the Wasserstein metric on the density manifold induces the Bures-Wasserstein metric for the Gaussian families [6, 82]. In our paper, however, we consider the induced Stein metric on \(\Theta\) or \(\Theta_{0}\) (we call it the **Gaussian-Stein metric**; for details see Appendix B).

**Different Bilinear Kernels and Induced Metrics.** There are several different bilinear kernels that appear in literature. [54] considers the simple bilinear kernel \(K_{1}(\mathbf{x},\mathbf{y})=\mathbf{x}^{\top}\mathbf{y}+1\) while [27, 13] suggest the use of an affine-invariant bilinear kernel \(K_{2}(\mathbf{x},\mathbf{y})=(\mathbf{x}-\mathbf{\mu})^{\top}(\mathbf{y}-\mathbf{\mu})+1\). [13] further points out that with the rescaled affine-invariant kernel \(K_{3}(\mathbf{x},\mathbf{y})=(\mathbf{x}-\mathbf{\mu})^{\top}\Sigma^{-1}(\mathbf{y}-\mathbf{\mu})+1\), the Gaussian-Stein metric magically coincides with the Bures-Wasserstein metric on \(\Theta\) (not true on the whole density manifold). Note that here \(\mathbf{\mu}\) and \(\Sigma\) are the mean and covariance of the current mean-field Gaussian distribution which could change with time. Moreover, [32] proposed a regularized version of SVGD (R-SVGD) that interpolates the dynamics between WGF and SVGD. Interestingly R-SVGD with the affine-invariant kernel \(K_{2}\) for Gaussian families can be reformulated as Gaussian-SVGD with a new kernel \(K_{4}(\mathbf{x},\mathbf{y})=(\mathbf{x}-\mathbf{\mu})^{\top}((1-\nu)\Sigma+\nu I)^{-1}(\mathbf{y}- \mathbf{\mu})\), which interpolates between \(K_{2}\) and \(K_{3}\) (see Theorem 3.4). For clarity we present the results for \(K_{1}\) in the main article while leave the analogues for \(K_{2}\)-\(K_{4}\) in the appendix. We also point out that \(K_{1}\) and \(K_{2}\) are the same on \(\Theta_{0}\).

**Gaussian-Stein Variational Gradient Descent.** With a bilinear kernel and Gaussian targets, we will prove in the next subsection that the SVGD dynamics would remain Gaussian as long as the initializer is Gaussian. However, this is not true in more general situations especially when the target is non-Gaussian. Fortunately for Gaussian variational inference we can still consider the gradient flow restricted to the Gaussian submanifold. In general, we denote by \(G^{\text{Stein}}_{\theta}\) the canonical isomorphism on \(\Theta\) induced by \(G^{\text{Stein}}_{\rho}\), and define the Gaussian-Stein variational gradient descent as

\[\dot{\theta}_{t}=-(G^{\text{Stein}}_{\theta_{t}})^{-1}\nabla_{\theta_{t}}\, \mathrm{KL}(\rho_{\theta_{t}}\parallel\rho^{*}),\]

where \(\rho^{*}\) might not be a Gaussian density. Notably Gaussian-SVGD solves the following optimization problem

\[\min_{\theta\in\Theta}\mathrm{KL}(\rho_{\theta}\parallel\rho^{*}),\quad\text{ where }\rho_{\theta}\text{ is the density of }\mathcal{N}(\mathbf{\mu},\Sigma),\]

via gradient descent under the Gaussian-Stein metric.

## 3 Dynamics of Gaussian-SVGD for Gaussian Targets

### Mean-Field Analysis from WGF to SVGD

The Wasserstein gradient flow (WGF) restricted to the general Gaussian family \(\Theta\) is known as the Bures-Wasserstein gradient flow [43, 19]. For consistency in this subsection we always set the initializer to be \(\mathcal{N}(\mathbf{\mu}_{0},\Sigma_{0})\) with density \(\rho_{0}\) and target \(\mathcal{N}(\mathbf{b},Q)\) with density \(\rho^{*}\) for the general Gaussian family. Then the WGF at any time \(t\) remains Gaussian, and can be fully characterized by the following dynamics of the mean \(\mathbf{\mu}_{t}\) and covariance matrix \(\Sigma_{t}\):

\[\dot{\mathbf{\mu}}_{t}=-Q^{-1}(\mathbf{\mu}_{t}-\mathbf{b}),\qquad\qquad\dot{\Sigma}_{t}=2I -\Sigma_{t}Q^{-1}-Q^{-1}\Sigma_{t}. \tag{3}\]

For SVGD with bilinear kernels we have similar results:

**Theorem 3.1** (Svgd).: _For any \(t\geq 0\) the solution \(\rho_{t}\) of SVGD (2) with the bilinear kernel \(K_{1}\) remains a Gaussian density with mean \(\mathbf{\mu}_{t}\) and covariance matrix \(\Sigma_{t}\) given by_

\[\begin{cases}\dot{\mathbf{\mu}}_{t}=(I-Q^{-1}\Sigma_{t})\mathbf{\mu}_{t}-(1+\mathbf{\mu}_{ t}^{\top}\mathbf{\mu}_{t})Q^{-1}(\mathbf{\mu}_{t}-\mathbf{b})\\ \dot{\Sigma}_{t}=2\Sigma_{t}-\Sigma_{t}\left(\Sigma_{t}+\mathbf{\mu}_{t}(\mathbf{\mu}_ {t}-\mathbf{b})^{\top}\right)Q^{-1}-Q^{-1}\left(\Sigma_{t}+(\mathbf{\mu}_{t}-\mathbf{b}) \mathbf{\mu}_{t}^{\top}\right)\Sigma_{t}\end{cases}, \tag{4}\]

_which has a unique global solution on \([0,\infty)\) given any \(\mathbf{\mu}_{0}\in\mathbb{R}^{d}\) and \(\Sigma_{0}\in\mathrm{Sym}^{+}(d,\mathbb{R})\). And \(\rho_{t}\) converges weakly to \(\rho^{*}\) as \(t\to\infty\) at the following rates_

\[\|\mathbf{\mu}_{t}-\mathbf{b}\|=\mathcal{O}(e^{-2(\gamma-\epsilon)t}),\quad\|\Sigma_{t} -Q\|=\mathcal{O}(e^{-2(\gamma-\epsilon)t}),\quad\forall\epsilon>0,\]

_where \(\gamma\) is the smallest eigenvalue of the matrix_

\[\begin{bmatrix}I_{d^{2}}&\frac{1}{\sqrt{2}}\mathbf{b}\otimes Q^{-1/2}\\ \frac{1}{\sqrt{2}}\mathbf{b}^{\top}\otimes Q^{-1/2}&\frac{1}{2}(1+\mathbf{b}^{\top}\mathbf{ b})Q^{-1}\end{bmatrix}\text{ with a lower bound }\gamma>\frac{1}{1+\mathbf{b}^{\top}\mathbf{b}+2\lambda},\]

_where \(\lambda\) is the largest eigenvalue of \(Q\)._

Note that for any vector \(\mathbf{x}\), \(\|\mathbf{x}\|\) denotes its Euclidean norm and for any matrix \(A\) we use \(\|A\|\) for its spectral norm, \(\|A\|_{*}\) for the nuclear norm and \(\|A\|_{F}\) for the Frobenius norm. All matrix convergence are considered under the spectral norm in default for technical simplicity (though all matrix norms are equivalent in finite dimensions).

If we restrict to the centered Gaussian family where both the initializer and target have zero mean (setting \(\mathbf{\mu}_{0}=\mathbf{b}=\mathbf{0}\)), the dynamics can further be simplified.

**Theorem 3.2** (Svgd for centered Gaussian).: _Let \(\rho_{0}\) and \(\rho^{*}\) be two centered Gaussian densities. Then for any \(t\geq 0\) the solution \(\rho_{t}\) of SVGD (2) with the bilinear kernel \(K_{1}\) or \(K_{2}\) remains a centered Gaussian density with the covariance matrix \(\Sigma_{t}\) given by the following Riccati type equation:_

\[\dot{\Sigma}_{t}=2\Sigma_{t}-\Sigma_{t}^{2}Q^{-1}-Q^{-1}\Sigma_{t}^{2}, \tag{5}\]

_which has a unique global solution on \([0,\infty)\) given any \(\Sigma_{0},Q\in\mathrm{Sym}^{+}(d,\mathbb{R})\). If \(\Sigma_{0}Q=Q\Sigma_{0}\), we have the closed-form solution:_

\[\Sigma_{t}^{-1}=e^{-2t}\Sigma_{0}^{-1}+(1-e^{-2t})Q^{-1}. \tag{6}\]

_In particular, if we let \(\Sigma_{0}=I\) and \(Q=I+\eta\mathbf{v}\mathbf{v}^{\top}\) for some \(\eta>0\) and \(\mathbf{v}\in\mathbb{R}^{d}\) such that \(\mathbf{v}^{\top}\mathbf{v}=1\), then \(\Sigma_{t}\) can be rewritten as_

\[\Sigma_{t}=I+\tfrac{\eta(1-e^{-2t})}{1+\eta e^{-2t}}\mathbf{v}\mathbf{v}^{\top}. \tag{7}\]

[13] shows that for WGF if \(\Sigma_{0}Q=Q\Sigma_{0}\), then we have \(\|\mathbf{\mu}_{t}-\mathbf{b}\|\)\(=\mathcal{O}(e^{-t/\lambda})\) and \(\|\Sigma_{t}-Q\|=\mathcal{O}(e^{-2t/\lambda})\). For the centered Gaussian family SVGD converges faster if \(\lambda>1\). For the general Gaussian family WGF and SVGD have rather comparable rates (e.g., take \(\lambda\gg\|\mathbf{b}\|\) then the lower bound here is roughly \(\mathcal{O}(e^{-t/\lambda})\)). Another observation is that the WGF rates depend on \(Q\) alone but the SVGD rates here sometimes also depend on \(\mathbf{b}\), which breaks the affine invariance of the system. This is a problem originated from the choice of kernels as addressed in [13], where they propose to use \(K_{2}\) instead of \(K_{1}\). Such approach has both advantages and disadvantages. The convention in SVGD is that the kernel should not depend on the mean-field density because the density is usually unknown and changes with time. But for GVI the affine-invariant bilinear kernel \(K_{2}\) only requires estimating the means from Gaussian distributions and is not a big issue.

**Regularized Stein Variational Gradient Descent.** In Section 2 we show that SVGD can be regarded as WGF kernelized in the cotangent space \(T_{\rho}^{*}\mathcal{P}(\mathbb{R}^{d})\). The regularized Stein variational gradient descent (R-SVGD) [32] interpolates WGF and SVGD by pulling back part of the kernelized gradient of the cotangent vector \(\Phi\), which is also seen as gradient flows under the regularized Stein metric:

**Definition 3.3** (Regularized Stein metric).: _The regularized Stein metric is induced by the following canonical map_

\[(G_{\rho}^{\text{RS}})^{-1}\Phi:=-\nabla\cdot\left(\rho\,\left((1-\nu)\mathcal{T} _{K,\rho}+\nu I\right)^{-1}\mathcal{T}_{K,\rho}\nabla\Phi\right),\]

_where \(\mathcal{T}_{K,\rho}\) is the kernelization operator given by \(\mathcal{T}_{K,\rho}f:=\int K(\cdot,\mathbf{y})f(\mathbf{y})\rho(\mathbf{y})\,\mathrm{d}\bm {y}\)._

The R-SVGD is defined as

\[\dot{\rho}_{t}=-(G_{\rho_{t}}^{\text{RS}})^{-1}\tfrac{\delta}{\delta\rho_{t}} \operatorname{KL}(\rho_{t}\parallel\rho^{*})=\nabla\cdot\left(\rho_{t}\left((1 -\nu)\mathcal{T}_{K,\rho_{t}}+\nu I\right)^{-1}\mathcal{T}_{K,\rho_{t}}\nabla \log\tfrac{\rho_{t}}{\rho^{*}}\right). \tag{8}\]

**Theorem 3.4** (R-SVGD).: _Let \(\rho_{0}\) and \(\rho^{*}\) be two Gaussian densities. Then the solution \(\rho_{t}\) of R-SVGD (8) with the bilinear kernel \(K_{2}\) converges to \(\rho^{*}\) as \(t\to\infty\), and \(\rho_{t}\) is the density of \(\mathcal{N}(\mathbf{b},\Sigma_{t})\) with_

\[\begin{cases}\dot{\mathbf{\mu}}_{t}=-Q^{-1}(\mathbf{\mu}_{t}-\mathbf{b})\\ \dot{\Sigma}_{t}=2((1-\nu)\Sigma_{t}+\nu I)^{-1}\Sigma_{t}-((1-\nu)\Sigma_{t}+ \nu I)^{-1}\Sigma_{t}^{2}Q^{-1}-Q^{-1}((1-\nu)\Sigma_{t}+\nu I)^{-1}\Sigma_{t} ^{2}\end{cases}. \tag{9}\]

_If \(\Sigma_{0}Q=Q\Sigma_{0}\), we have \(\|\Sigma_{t}-Q\|=\mathcal{O}\left(e^{-2t/((1-\nu)\lambda+\nu)}\right)\), where \(\lambda\) is the largest eigenvalue of \(Q\)._

From this theorem we see that R-SVGD can take the advantage of both regimes by choosing \(\nu\) wisely. Another interesting connection is that on \(\Theta\) the induced regularized Stein metric coincides with the Stein metric with a different kernel \(K_{4}(\mathbf{x},\mathbf{y})=(\mathbf{x}-\mathbf{\mu})^{\top}((1-\nu)\Sigma+\nu I)^{-1}(\mathbf{y}- \mathbf{\mu})+1\) (see Theorem B.7).

**Stein AIG Flow.** Accelerating methods are widely used in first-order optimization algorithms and have attracted considerable interest in particle-based variational inference [50]. [76, 86] study the accelerated information gradient (AIG) flows as the analogue of Nesterov's accelerated gradient method [63] on the density manifold. Given a probability space \(\mathcal{P}(\mathbb{R}^{d})\) with a metric tensor \(g_{\rho}(\cdot,\cdot)\), let \(G_{\rho}:T_{\rho}\mathcal{P}(\mathbb{R}^{d})\to T_{\rho}^{*}\mathcal{P}( \mathbb{R}^{d})\) be the corresponding isomorphism. The Hamiltonian flow in probability space [16] follows from

\[\partial_{t}\begin{bmatrix}\rho_{t}\\ \Phi_{t}\end{bmatrix}=\begin{bmatrix}0&1\\ -1&0\end{bmatrix}\begin{bmatrix}\tfrac{\delta}{\delta\rho_{t}}\mathcal{H}\left( \rho_{t},\Phi_{t}\right)\\ \tfrac{\delta}{\delta\Phi_{t}}\mathcal{H}\left(\rho_{t},\Phi_{t}\right)\end{bmatrix},\text{ where }\mathcal{H}(\rho_{t},\Phi_{t}):=\tfrac{1}{2}\int\Phi_{t}G_{ \rho_{t}}^{-1}\Phi_{t}\,\mathrm{d}\mathbf{x}+\operatorname{KL}(\rho\parallel\rho^ {*})\]

is called the Hamiltonian function, which consists of a kinetic energy \(\tfrac{1}{2}\int\Phi G_{\rho}^{-1}\Phi\,\mathrm{d}\mathbf{x}\) and a potential energy \(\operatorname{KL}(\rho\parallel\rho^{*})\). Following [86] we introduce the accelerated information gradient flow in probability space. Let \(\alpha_{t}\geq 0\) be a scalar function of time \(t\). We add a damping term \(\alpha_{t}\Phi_{t}\) to the Hamiltonian flow:

\[\partial_{t}\begin{bmatrix}\rho_{t}\\ \Phi_{t}\end{bmatrix}=-\begin{bmatrix}0\\ \alpha_{t}\Phi_{t}\end{bmatrix}+\begin{bmatrix}0&1\\ -1&0\end{bmatrix}\begin{bmatrix}\tfrac{\delta}{\delta\rho_{t}}\mathcal{H} \left(\rho_{t},\Phi_{t}\right)\\ \tfrac{\delta}{\delta\Phi_{t}}\mathcal{H}\left(\rho_{t},\Phi_{t}\right)\end{bmatrix}, \tag{10}\]

By adopting the Stein metric we obtain the Stein AIG flow (S-AIGF):

\[\begin{cases}\dot{\rho}_{t}=-\nabla\cdot\left(\rho_{t}(\cdot)\int K(\cdot,\mathbf{y })\rho_{t}(\mathbf{y})\nabla\Phi_{t}(\mathbf{y})\,\mathrm{d}\mathbf{y}\right)\\ \dot{\Phi}_{t}=-\alpha_{t}\Phi_{t}-\int\nabla\Phi_{t}(\cdot)^{\top}\nabla\Phi_{t }(\mathbf{y})K(\cdot,\mathbf{y})\rho_{t}(\mathbf{y})\,\mathrm{d}\mathbf{y}-\tfrac{\delta}{ \delta\rho_{t}}\operatorname{KL}(\rho_{t}\parallel\rho^{*})\end{cases}. \tag{11}\]

Again we characterize the dynamics of S-AIGF with the linear kernel for the Gaussian family:

**Theorem 3.5** (S-Aigf).: _Let \(\rho_{0}\) and \(\rho^{*}\) be two centered Gaussian densities. Then the solution \(\rho_{t}\) of S-AIGF (11) with the bilinear kernel \(K_{1}\) or \(K_{2}\) is the density of \(\mathcal{N}(\mathbf{0},\Sigma_{t})\) where \(\Sigma_{t}\) satisfies_

\[\begin{cases}\dot{\Sigma}_{t}=2(S_{t}\Sigma_{t}^{2}+\Sigma_{t}^{2}S_{t})\\ \dot{S}_{t}=-\alpha_{t}S_{t}-2(S_{t}^{2}\Sigma_{t}+\Sigma_{t}S_{t}^{2})+ \tfrac{1}{2}(\Sigma_{t}^{-1}-Q^{-1})\end{cases}, \tag{12}\]

_where \(S_{t}\in\operatorname{Sym}(d,\mathbb{R})\) with initial value \(S_{0}=0\)._

Note that the convergence properties of S-AIGF still remains open in contrast to the Wasserstein AIG flow (W-AIGF) as [86] shows that the W-AIGF for the centered Gaussian family is

\[\dot{\Sigma}_{t}=2(S_{t}\Sigma_{t}+\Sigma_{t}S_{t}),\qquad\qquad\dot{S}_{t}=- \alpha_{t}S_{t}-2S_{t}^{2}+\tfrac{1}{2}(\Sigma_{t}^{-1}-Q^{-1}),\]

and that if \(\alpha_{t}\) is well-chosen, the KL divergence in W-AIGF converges at the rate of \(\mathcal{O}(e^{-t/\sqrt{\lambda}})\). Thus, when \(\lambda\) is large it converges faster than WGF. It is also interesting to point out that the acceleration effect of Nesterov's scheme also comes from time discretization of the ODE system (see [74]) as it moves roughly \(\sqrt{\epsilon}\) rather than \(\epsilon\) along the gradient path when the step size is \(\epsilon\).

### Finite-Particle Systems

In this subsection, we consider the case where \(N<\infty\) particles evolve in time \(t\). We set a Gaussian target \(\mathcal{N}(\mathbf{b},Q)\) (i.e., the potential is \(V(\mathbf{x})=\frac{1}{2}(\mathbf{x}-\mathbf{b})^{\top}Q^{-1}(\mathbf{x}-\mathbf{b})\)) and run the SVGD algorithm with a bilinear kernel, and obtain the dynamics of \(\mathbf{x}_{1}^{(t)},\cdots,\mathbf{x}_{N}^{(t)}\). The continuous-time particle-based SVGD corresponds to the following deterministic interactive system in \(\mathbb{R}^{d}\):

\[\dot{\mathbf{x}}_{i}^{(t)}=\tfrac{1}{N}\sum_{j=1}^{N}\nabla_{\mathbf{x}_{j}}K\left(\bm {x}_{i}^{(t)},\mathbf{x}_{j}^{(t)}\right)-\tfrac{1}{N}\sum_{j=1}^{N}K\left(\mathbf{x}_ {i}^{(t)},\mathbf{x}_{j}^{(t)}\right)\nabla V\left(\mathbf{x}_{j}^{(t)}\right) \tag{13}\]

with initial particles given by \(\mathbf{x}_{i}^{(0)}\) (\(i=1,\cdots,N\)). Now denoting the sample mean and covariance matrix at time \(t\) by \(\mathbf{\mu}_{t}:=\frac{1}{N}\sum_{j=1}^{N}\mathbf{x}_{j}^{(t)}\) and \(C_{t}:=\frac{1}{N}\sum_{j=1}^{N}\mathbf{x}_{j}^{(t)}\mathbf{x}_{j}^{(t)\top}-\mathbf{\mu}_ {t}\mathbf{\mu}_{t}^{\top}\), we have the following theorem.

**Theorem 3.6** (Svgd).: _Suppose the initial particles satisfy that \(C_{0}\) is non-singular. Then SVGD (13) with the bilinear kernel \(K_{1}\) and Gaussian potential \(V\) has a unique solution given by_

\[\mathbf{x}_{i}^{(t)}=A_{t}(\mathbf{x}_{i}^{(0)}-\mathbf{\mu}_{0})+\mathbf{\mu}_{t}, \tag{14}\]

_where \(A_{t}\) is the unique (matrix) solution of the linear system_

\[\dot{A}_{t}=\left(I-Q^{-1}(C_{t}+\mathbf{\mu}_{t}\mathbf{\mu}_{t}^{\top})+Q^{-1}\mathbf{b} \mathbf{\mu}_{t}^{\top}\right)A_{t},\quad A_{0}=I, \tag{15}\]

_and \(\mathbf{\mu}_{t}\) and \(C_{t}\) are the unique solution of the ODE system_

\[\begin{cases}\dot{\mathbf{\mu}}_{t}=(I-Q^{-1}C_{t})\mathbf{\mu}_{t}-(1+\mathbf{\mu}_{t}^{ \top}\mathbf{\mu}_{t})Q^{-1}(\mathbf{\mu}_{t}-\mathbf{b})\\ \dot{C}_{t}=2C_{t}-C_{t}\left(C_{t}+\mathbf{\mu}_{t}(\mathbf{\mu}_{t}-\mathbf{b})^{\top} \right)Q^{-1}-Q^{-1}\left(C_{t}+(\mathbf{\mu}_{t}-\mathbf{b})\mathbf{\mu}_{t}^{\top}\right) C_{t}\end{cases}. \tag{16}\]

The ODE system (16) is exactly the same as that in the density flow (4). Thus, we have the the same convergence rates as in Theorem 3.1. Theorem 3.6 can be interpreted as: At each time \(t\) the particle positions are a linear transformation of the initialization. On one hand, if we initialize _i.i.d._ from Gaussian, there is uniform in time convergence as shown in the theorem below.

On the other hand, if we initialize _i.i.d._ from a non-Gaussian distribution \(\rho_{0}\). At each time \(t\) the mean field limit \(\rho_{t}\) should be a linear transformation of \(\rho_{0}\) and cannot converge to the Gaussian target \(\rho^{*}\) as \(t\to\infty\). Note that in general SVGD with the bilinear kernel might not always converge to the target distribution for nonparametric sampling but for GVI there is no such issue. This will be discussed in detail in Appendix C together with general results of well-posedness and mean-field convergence of SVGD with the bilinear kernel, which has not yet been studied in literature.

**Theorem 3.7** (Uniform in time convergence).: _Given the same setting as Theorem 3.6, further suppose the initial particles are drawn i.i.d. from \(\mathcal{N}(\mathbf{\mu}_{0},\Sigma_{0})\). Then there exists a constant \(C_{d,Q,\mathbf{b},\Sigma_{0},\mathbf{\mu}_{0}}\) such that for all \(t\in[0,\infty]\), for all \(N\geq 2\), with the empirical measure \(\zeta_{N}^{(t)}=\frac{1}{N}\sum_{i=1}^{N}\delta_{\mathbf{x}_{i}^{(t)}}\), the second moment of Wasserstein-\(2\) distance between \(\zeta_{N}^{(t)}\) and \(\rho_{t}\) converges:_

\[\mathbb{E}\left[\mathcal{W}_{2}^{2}\left(\zeta_{N}^{(t)},\rho_{t}\right)\right] \leq C_{d,Q,\mathbf{b},\Sigma_{0},\mathbf{\mu}_{0}}\times\left\{\begin{array}{cl}N^ {-1}\log\log N&\text{ if }d=1\\ N^{-1}(\log N)^{2}&\text{ if }d=2\\ N^{-2/d}&\text{ if }d\geq 3\end{array}\right.. \tag{17}\]

Similar to Theorem 3.2, we also provide the finite-particle result for a centered Gaussian target.

**Theorem 3.8** (SVGD for centered Gaussian).: _Suppose the SVGD particle system (13) with the bilinear kernel \(K_{1}\) or \(K_{2}\) is targeting a centered Gaussian distribution and initialized by \(\left(\mathbf{x}_{i}^{(0)}\right)_{i=1}^{N}\) such that \(\mathbf{\mu}_{0}=\mathbf{0}\) and \(C_{0}Q=QC_{0}\). Then we have the following closed-form solution_

\[\mathbf{x}_{i}^{(t)}=\left(e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0}\right)^{-1/2}\mathbf{x}_{i}^ {(0)}. \tag{18}\]

**Analogous Result for R-SVGD.** Next we consider the particle dynamics of R-SVGD. As shown in [32], the finite-particle system of R-SVGD is

\[\dot{X}_{t}=-\left((1-\nu)\tfrac{K_{t}}{N}+\nu I\right)^{-1}\left(\tfrac{K_{t} }{N}\mathcal{L}_{t}\nabla V-\tfrac{1}{N}\sum_{j=1}^{N}\mathcal{L}_{t}\nabla K \left(\mathbf{x}_{j}^{(t)},\,\cdot\,\right)\right),\]

where \(X_{t}:=\left(\mathbf{x}_{1}^{(t)},\cdots,\mathbf{x}_{N}^{(t)}\right)^{\top}\), \(\mathcal{L}_{t}f:=\left(f(\mathbf{x}_{i}^{(t)}),\cdots,f(\mathbf{x}_{N}^{(t)})\right)^{\top}\) for all \(f:\mathbb{R}^{d}\to\mathbb{R}^{d}\) and \((K_{t})_{ij}:=K\left(\mathbf{x}_{i}^{(t)},\mathbf{x}_{j}^{(t)}\right)\) for all \(1\leq i,j\leq N\). Similar to Theorem 3.6, we have the following result:

**Theorem 3.9** (R-Svgd).: _Suppose the R-SVGD system (13) with \(K_{1}\) or \(K_{2}\) is targeting a centered Gaussian distribution and initialized by \(\big{(}\mathbf{x}_{i}^{(0)}\big{)}_{i=1}^{N}\) such that \(\mathbf{\mu}_{0}=\mathbf{0}\). Then we have \(\mathbf{x}_{i}^{(t)}=A_{t}\mathbf{x}_{i}^{(0)}\) where \(A_{t}\) is the unique solution of the linear system_

\[\dot{A}_{t}=(I-Q^{-1}C_{t})((1-\nu)C_{t}+\nu I)^{-1}A_{t},\quad A_{0}=I, \tag{19}\]

_and the sample covariance matrix \(C_{t}\) is given by_

\[\dot{C}_{t}=2((1-\nu)C_{t}+\nu I)^{-1}C_{t}-((1-\nu)C_{t}+\nu I)^{-1}C_{t}^{2} Q^{-1}-Q^{-1}((1-\nu)C_{t}+\nu I)^{-1}C_{t}^{2}. \tag{20}\]

Again we observe that the particles at time \(t\) is a time-changing linear transformation of the initializers.

**Discrete-time Analysis for Finite Particles.** Next we consider the algorithm in discrete time \(t\). The SVGD updates according to the following equation:

\[\mathbf{x}_{i}^{(t+1)}=\mathbf{x}_{i}^{(t)}+\tfrac{\epsilon}{N}\bigg{(} \Sigma_{j=1}^{N}\nabla_{\mathbf{x}_{j}^{(t)}}K\big{(}\mathbf{x}_{i}^{(t)},\mathbf{x}_{j}^{ (t)}\big{)}-\sum_{j=1}^{N}K\big{(}\mathbf{x}_{i}^{(t)},\mathbf{x}_{j}^{(t)}\big{)} \nabla_{\mathbf{x}_{j}^{(t)}}V\big{(}\mathbf{x}_{j}^{(t)}\big{)}\bigg{)}. \tag{21}\]

For simplicity, we only consider the case where both the target and initializers are centered, i.e., \(\mathbf{b}=\mathbf{\mu}_{0}=\mathbf{0}\) and show the convergence:

**Theorem 3.10** (Discrete-time convergence).: _For a centered Gaussian target, suppose the particle system (21) with \(K_{1}\) or \(K_{2}\) is initialized by \(\big{(}\mathbf{x}_{i}^{(0)}\big{)}_{i=1}^{N}\) such that \(\mathbf{\mu}_{0}=\mathbf{0}\) and \(C_{0}Q=QC_{0}\). For \(0<\epsilon<0.5\), we have \(\mathbf{\mu}_{t}=\mathbf{0}\) and \(\|C_{t}-Q\|\to 0\) as long as all the eigenvalues of \(Q^{-1}C_{0}\) lie in the interval \((0,1+1/\epsilon)\)._

_Furthermore, if we set \(u_{\epsilon}\) to be the smaller root of the equation \(f_{\epsilon}^{\prime}(u)=1-\epsilon\) (it has \(2\) distinct roots) where \(f_{\epsilon}(x):=(1+\epsilon(1-x))^{2}x\), then we have linear convergence, i.e.,_

\[\|C_{t}-Q\|\leq(1-\epsilon)^{t}\|C_{0}-Q\|\leq e^{-\epsilon t}\|C_{0}-Q\|\]

_as long as all the eigenvalues of \(Q^{-1}C_{0}\) lie in the interval \([u_{\epsilon},1/3+1/(3\epsilon)]\)._

The above result illustrates that firstly the step sizes required are restricted by the largest eigenvalue of \(Q^{-1}C_{0}\). In particular if \(C_{0}=I_{d}\) then we need smaller step size if the smallest eigenvalue of \(Q\) is smaller, which corresponds to the \(\beta\)-log-smoothness condition of the target distribution. Secondly we can potentially have faster convergence over iteration given larger step sizes. We believe that the commutativity assumption in Theorem 3.10 can be relaxed and similar results can be obtained for general targets. Detailed examinations are left as future work.

## 4 Beyond Gaussian Targets

In this section we consider the Gaussian-SVGD with a general target and have the following dynamics.

**Theorem 4.1**.: _Let \(\rho^{*}\) be the density of the target distribution with the potential function \(V(\mathbf{x})\) that satsifies Assumption C.1 and \(\rho_{0}\) be the density of \(\mathcal{N}(\mathbf{\mu}_{0},\Sigma_{0})\). The Gaussian-SVGD with \(K_{1}\) produces a Gaussian density \(\rho_{t}\) with mean \(\mathbf{\mu}_{t}\) and covariance matrix \(\Sigma_{t}\) given by_

\[\begin{cases}\dot{\mathbf{\mu}}_{t}=(I-\Gamma_{t}\Sigma_{t})\,\mathbf{\mu}_{t}-(1+\mathbf{ \mu}_{t}^{\top}\mathbf{\mu}_{t})\mathbf{m}_{t}\\ \dot{\Sigma}_{t}=2\Sigma_{t}-\Sigma_{t}\left(\Sigma_{t}\Gamma_{t}+\mathbf{\mu}_{t }\mathbf{m}_{t}^{\top}\right)-\left(\Gamma_{t}\Sigma_{t}+\mathbf{m}_{t}\mathbf{\mu}_{t}^{ \top}\right)\Sigma_{t}\end{cases}, \tag{22}\]

_where \(\Gamma_{t}=\mathbb{E}_{\mathbf{x}\sim\rho_{t}}[\nabla^{2}V(\mathbf{x})]\) and \(\mathbf{m}_{t}=\mathbb{E}_{\mathbf{x}\sim\rho_{t}}[\nabla V(\mathbf{x})]\)._

_Furthermore, suppose that \(\theta^{*}\) is the unique solution of the following optimization problem_

\[\min_{\theta=(\mathbf{\mu},\Sigma)}\mathrm{KL}(\rho_{\theta}\parallel\rho^{*}),\; \text{where}\;\rho_{\theta}\;\text{is the Gaussian measure}\;\mathcal{N}(\mathbf{\mu},\Sigma).\]

_Then we have \(\rho_{t}\rightarrow\rho_{\theta^{*}}\sim\mathcal{N}(\mathbf{\mu}^{*},\Sigma^{*})\) as \(t\rightarrow\infty\)._

In particular, if the target is strongly log-concave, it gives rise to the following linear convergence:

**Theorem 4.2**.: _Assume that the target \(\rho^{*}\) is \(\alpha\)-strongly log-concave and \(\beta\)-log-smooth, i.e., \(\alpha I\preceq\nabla^{2}V(\mathbf{x})\preceq\beta I\). Then \(\rho_{t}\) of Theorem 4.1 converges to \(\rho_{\theta^{*}}\) at the following rate_

\[\|\mathbf{\mu}_{t}-\mathbf{\mu}^{*}\|=\mathcal{O}(e^{-(\gamma-\epsilon)t}),\quad\| \Sigma_{t}-\Sigma^{*}\|=\mathcal{O}(e^{-(\gamma-\epsilon)t}),\quad\forall\epsilon >0,\]

_where \(\gamma/\alpha\) is the smallest eigenvalue of the matrix_

\[\begin{bmatrix}I_{d}\otimes\Sigma^{*}&\mathbf{\mu}^{*}\otimes(\Sigma^{*})^{1/2}\\ \mathbf{\mu}^{*\top}\otimes(\Sigma^{*})^{1/2}&(1+\mathbf{\mu}^{*\top}\mathbf{\mu}^{*})I_{d }\end{bmatrix}\text{ with a lower bound }\gamma>\frac{\alpha}{\beta(1+\mathbf{\mu}^{* \top}\mathbf{\mu}^{*})+1}.\]

Typically the \(\beta\)-log-smoothness condition is not required for continuous-time analyses. However, it is required in the above statement, as our proof technique is based on comparing the decay of the energy function of the flow to that for WGF, following [13]. Relaxing this condition is interesting and we leave it as future work.

**Unifying Algorithms.** For general targets, we propose two unifying algorithm frameworks where we can choose any bilinear kernel (e.g., \(K_{1}\)-\(K_{4}\)) to solve GVI with SVGD. The first framework is density-based where we update \(\mathbf{\mu}_{t}\) and \(\Sigma_{t}\) according to the mean-field dynamics. It requires the closed-form of the ODE system

\[\begin{cases}\hat{\mathbf{\mu}}_{t}=F\left(\mathbf{\mu}_{t},\Sigma_{t},\mathbf{m}_{t}, \Gamma_{t}\right)\\ \hat{\Sigma}_{t}=\Sigma_{t}\ G\left(\mathbf{\mu}_{t},\Sigma_{t},\mathbf{m}_{t},\Gamma_{ t}\right)+G\left(\mathbf{\mu}_{t},\Sigma_{t},\mathbf{m}_{t},\Gamma_{t}\right)^{\top}\ \Sigma_{t}\end{cases}, \tag{23}\]

where \(\mathbf{m}_{t}=\mathbb{E}_{\mathbf{x}\sim\rho_{t}}[\nabla V(\mathbf{x})]\) and \(\Gamma_{t}=\mathbb{E}_{\mathbf{x}\sim\rho_{t}}[\nabla^{2}V(\mathbf{x})]\), and \(F\) and \(G\) are some closed-form functions. For example \(F\left(\mathbf{\mu}_{t},\Sigma_{t},\mathbf{m}_{t},\Gamma_{t}\right)=(I-\Gamma_{t} \Sigma_{t})\ \mathbf{\mu}_{t}-(1+\mathbf{\mu}_{t}^{\top}\mathbf{\mu}_{t})\mathbf{m}_{t}\) and \(G\left(\mathbf{\mu}_{t},\Sigma_{t},\mathbf{m}_{t},\Gamma_{t}\right)=I-\Sigma_{t}\Gamma _{t}-\mathbf{\mu}_{t}\mathbf{m}_{t}^{\top}\) for \(K_{1}\) as shown in (22). Note that \(\mathbf{m}_{t}\) and \(\Gamma_{t}\) can be estimated from samples using

\[\widehat{\mathbf{m}}_{t}=\tfrac{1}{N}\sum_{k=1}^{N}\nabla V(\mathbf{x}_{k}^{(t)}), \quad\widehat{\Gamma}_{t}=\tfrac{1}{N}\sum_{k=1}^{N}\nabla^{2}V(\mathbf{x}_{k}^{(t )}), \tag{24}\]

or using the first-order estimator

\[\widehat{\Gamma}_{t}=\tfrac{1}{N}\sum_{k=1}^{N}\nabla V(\mathbf{x}_{k}^{(t)})(\mathbf{ x}_{k}^{(t)}-\mathbf{\mu}_{t})^{\top}\Sigma_{t}^{-1}. \tag{25}\]

The second framework is particle-based and does not need the closed-form ODE of the mean and covariance, making it more flexible than Algorithm 1. Here we initially draw \(N\) particles and keep updating them over time using

\[\mathbf{x}_{i}^{(t+1)}=\mathbf{x}_{i}^{(t)}+\tfrac{\epsilon}{N}\bigg{(}\sum_{j=1}^{N} \nabla_{\mathbf{x}_{j}^{(t)}}K\big{(}\mathbf{x}_{i}^{(t)},\mathbf{x}_{j}^{(t)}\big{)}-\sum_ {j=1}^{N}K\big{(}\mathbf{x}_{i}^{(t)},\mathbf{x}_{j}^{(t)}\big{)}\widehat{\nabla V} \big{(}\mathbf{x}_{j}^{(t)}\big{)}\bigg{)}, \tag{26}\]

where \(\widehat{\nabla V}\) is a (time-dependent) linear approximation of \(\nabla V\) defined as \(\widehat{\nabla V}(\mathbf{x})=\widehat{\Gamma}_{t}\ (\mathbf{x}-\mathbf{\mu}_{t})+\widehat{\mathbf{m}}_{t}\). Intuitively this is used instead of \(\nabla V\) to ensure the Gaussianity of the particle system.

```  Draw \((\mathbf{x}_{i}^{(0)})_{i=1}^{N}\) from \(\mathcal{N}(\mathbf{\mu}_{0},\Sigma_{0})\) for\(t\) in \(0:T\)do \(\mathbf{\mu}_{t}\leftarrow\tfrac{1}{N}\sum_{k=1}^{N}\mathbf{x}_{k}^{(t)}\) \(\Sigma_{t}\leftarrow\tfrac{1}{N}\sum_{k=1}^{N}\mathbf{x}_{k}^{(t)}\mathbf{x}_{k}^{(t) \top}-\mathbf{\mu}_{t}\mathbf{

## 5 Simulations

In this section, we conduct simulations to compare Gaussian-SVGD dynamics with different kernels and the performance of the algorithms mentioned in the previous section. We consider three settings, **Bayesian logistic regression**, and **Gaussian and Gaussian mixture targets**. Here we present the results for Bayesian logistic regression as it involves a non-Gaussian but unimodal target and is one of the typical setups such that GVI is preferred in practice. For sake of space we leave the simulations for the other two settings along with further discussions to Appendix E.

**Bayesian Logistic Regression.** The following generative model is considered: Given a parameter \(\mathbf{\xi}\in\mathbb{R}^{d}\), draw samples \(\{(X_{i},Y_{i})\}_{i=1}^{n}\in(\mathbb{R}^{d}\times\{0,1\})^{n}\) such that \(X_{i}\overset{\text{i.i.d.}}{\sim}\mathcal{N}(\mathbf{0},I_{d})\) and \(Y_{i}\mid X_{i}\sim\mathrm{Bern}(\sigma(\mathbf{\zeta},X_{i})))\) where \(\sigma(\cdot)\) is the logistic function. Given the samples \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) and a uniform (improper) prior on \(\mathbf{\xi}\), the potential function of the posterior \(\rho^{*}\) on \(\mathbf{\xi}\) is given by \(V(\mathbf{\xi})=\sum_{i=1}^{n}\left(\log(1+\exp((\mathbf{\xi},X_{i}))-Y_{i}(\mathbf{\xi},X _{i}))\right)\). We run both Algorithms 1 and 2 initialized at \(\rho_{0}=\mathcal{N}(\mathbf{0},I_{d})\) to find the \(\rho_{\theta^{*}}\) that minimizes \(\mathrm{KL}(\rho_{\theta}\parallel\rho^{*})\). In Figure 1, **SBGD** (Simple Bilinear Gradient Descent), **GF** (Gaussian Flow [27]), **BVGD** (Bures-Wasserstein Gradient Descent [43]), and **RGF** (Regularized Gaussian Flow) are density-based algorithms with the bilinear kernels \(K_{1}\), \(K_{2}\), \(K_{3}\), and \(K_{4}\) (\(\nu=0.5\)) respectively; **SBPF** (Simple Bilinear Particle Flow), **GPF** (Gaussian Particle Flow [27]), **BWPF** (Bures-Wasserstein Particle Flow), and **RGPF** (Regularized Gaussian Particle Flow) are particle-based algorithms with the bilinear kernels \(K_{1}\), \(K_{2}\), \(K_{3}\), and \(K_{4}\) (\(\nu=0.5\)) respectively. We use the sample estimator of \(\mathcal{F}(\rho_{\theta})=\int(\log\rho_{\theta}+V)\,\mathrm{d}\rho_{\theta}= \mathrm{KL}(\rho_{\theta}\parallel\rho^{*})+C\) (\(C\) is some constant) to evaluate the learned parameters \(\theta=(\mathbf{\mu},\Sigma)\). For a fair comparison in Figure 0(b), we draw \(2000\) particles for particle-based algorithms and run all algorithms for \(2000\) iterations so that a total of \(2000\) samples are drawn for the density-based algorithms. The largest safe step sizes are \(0.02,0.1,2,0.8,0.02,0.2,4,4\).

Figure 1 shows the decay of \(\widehat{\mathcal{F}}(\rho_{\theta})\) over time or iterations. For Figure 0(a) the same step size \(0.01\) is specified for all algorithms while for Figure 0(b) we choose the largest safe step size for each algorithm. In other words, Figure 0(a) provides the continuous-time flow of the dynamical system in each algorithm while Figure 0(b) emphasizes more on the discrete-time algorithmic behaviors. In Figure 0(a) there are roughly four distinct curves indicating four different kernels. \(K_{1}\) has the most rapid descent, followed by \(K_{2}\), \(K_{4}\), and \(K_{3}\).

From Figure 0(b) we observe that BWPF and RGPF are the better choices for practical use. The difference in the largest step sizes shows that in terms of stability \(K_{3}\) is the best, \(K_{4}\) is almost as stable, but \(K_{2}\) and \(K_{1}\) are much worse. We also observe that particle-based algorithms are consistently more stable than density-based counterparts (which are essentially stochastic gradient based). The superiority of particle-based algorithms are even more evident in the Gaussian mixture experiment where the target is multi-modal. We further remark that another recently proposed density-based algorithm, the FB-GVI [19] shows comparable performance to BWPF and RGPF with large step sizes. We conduct a comparison of these three algorithms in Appendix E but do not include it here for clarity. It would also be really interesting to study the particle-based analogue of FB-GVI as future works.

Figure 1: Convergence of Algorithms 1 and 2 with bilinear kernels in Bayesian logistic regression.

## Acknowledgements

We thank Lester Mackey and Jiaxin Shi for several clarifications about their work, [72], and for helpful discussions regarding the larger literature on SVGD. Promit Ghosal was supported in part by NSF grant DMS-2153661. Krishnakumar Balasubramanian was supported in part by NSF grant DMS-2053918. Natesh S. Pillai was supported by ONR grant N00014-21-1-2664.

## References

* [1] Pierre Alquier and James Ridgway, _Concentration of tempered posteriors and of their variational approximations_, The Annals of Statistics **48** (2020), no. 3, 1475-1497.
* [2] David Alvarez-Melis, Yair Schiff, and Youssef Mroueh, _Optimizing functionals on the space of probabilities with input convex neural networks_, Transactions on Machine Learning Research (2022).
* [3] Michael Arbel, Anna Korba, Adil Salim, and Arthur Gretton, _Maximum mean discrepancy gradient flow_, Advances in Neural Information Processing Systems, vol. 32, 2019.
* [4] Jimmy Ba, Murat A Erdogdu, Marzyeh Ghassemi, Shengyang Sun, Taiji Suzuki, Denny Wu, and Tianzong Zhang, _Understanding the variance collapse of svgd in high dimensions_, International Conference on Learning Representations, 2021.
* [5] Julio Backhoff-Veraguas, Joaquin Fontbona, Gonzalo Rios, and Felipe Tobar, _Stochastic gradient descent in Wasserstein space_, arXiv preprint arXiv:2201.04232 (2022).
* [6] Rajendra Bhatia, Tanvi Jain, and Yongdo Lim, _On the Bures-Wasserstein distance between positive definite matrices_, Expositiones Mathematicae **37** (2019), no. 2, 165-191.
* [7] Rajendra Bhatia and Peter Rosenthal, _How and why to solve the operator equation \(AX-XB=Y\)_, Bulletin of the London Mathematical Society **29** (1997), no. 1, 1-21.
* [8] David M Blei, Alp Kucukelbir, and Jon D McAuliffe, _Variational inference: A review for statisticians_, Journal of the American statistical Association **112** (2017), no. 518, 859-877.
* [9] S Bobkov and M Ledoux, _One-dimensional empirical measures, order statistics, and Kantorovich transport distances_, Memoirs Amer. Math. Soc (2019).
* [10] Jose Antonio Carrillo, Katy Craig, and Francesco S Patacchini, _A blob method for diffusion_, Calculus of Variations and Partial Differential Equations **58** (2019), 1-53.
* [11] Anthony Caterini, Rob Cornish, Dino Sejdinovic, and Arnaud Doucet, _Variational inference with continuously-indexed normalizing flows_, Uncertainty in Artificial Intelligence, PMLR, 2021, pp. 44-53.
* [12] Peng Chen and Omar Ghattas, _Projected Stein variational gradient descent_, Advances in Neural Information Processing Systems **33** (2020), 1947-1958.
* [13] Yifan Chen, Daniel Zhengyu Huang, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart, _Gradient flows for sampling: Mean-field models, Gaussian approximations and affine invariance_, arXiv preprint arXiv:2302.11024 (2023).
* [14] Badr-Eddine Cherief-Abdellatif, Pierre Alquier, and Mohammad Emtiyaz Khan, _A generalization bound for online variational inference_, Asian Conference on Machine Learning, PMLR, 2019, pp. 662-677.
* [15] Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, and Philippe Rigollet, _SVGD as a kernelized Wasserstein gradient flow of the chi-squared divergence_, Advances in Neural Information Processing Systems **33** (2020), 2098-2109.
* [16] Shui-Nee Chow, Wuchen Li, and Haomin Zhou, _Wasserstein Hamiltonian flows_, Journal of Differential Equations **268** (2020), no. 3, 1205-1219.
* [17] Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton, _A kernel test of goodness of fit_, International conference on machine learning, PMLR, 2016, pp. 2606-2615.
* [18] Gianluca Detommaso, Tiangang Cui, Youssef Marzouk, Alessio Spantini, and Robert Scheichl, _A Stein variational Newton method_, Advances in Neural Information Processing Systems **31** (2018).

* [19] Michael Diao, Krishnakumar Balasubramanian, Sinho Chewi, and Adil Salim, _Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein space_, arXiv preprint arXiv:2304.05398 (2023).
* [20] Justin Domke, _Provable smoothness guarantees for black-box variational inference_, International Conference on Machine Learning, PMLR, 2020, pp. 2587-2596.
* [21] Justin Domke and Daniel R Sheldon, _Importance weighting and variational inference_, Advances in neural information processing systems **31** (2018).
* [22] Andrew Duncan, Nikolas Nusken, and Lukasz Szpruch, _On the geometry of Stein variational gradient descent_, Journal of Machine Learning Research **24** (2023), 1-39.
* [23] Yihao Feng, Dilin Wang, and Qiang Liu, _Learning to draw samples with amortized Stein variational gradient descent_, Conference on Uncertainty in Artificial Intelligence, 2017.
* [24] Nicolas Fournier, _Convergence of the empirical measure in expected Wasserstein distance: Non asymptotic explicit bounds in \(\mathbb{R}^{d}\)_, arXiv preprint arXiv:2209.00923 (2022).
* [25] Nicolas Fournier and Arnaud Guillin, _On the rate of convergence in Wasserstein distance of the empirical measure_, Probability theory and related fields **162** (2015), no. 3-4, 707-738.
* [26] Oded Galor, _Discrete dynamical systems_, Springer Science & Business Media, 2007.
* [27] Theo Galy-Fajou, Valerio Perrone, and Manfred Opper, _Flexible and efficient inference with particles for the variational Gaussian approximation_, Entropy **23** (2021), no. 8, 990.
* [28] Chengyue Gong, Jian Peng, and Qiang Liu, _Quantile Stein variational gradient descent for batch Bayesian optimization_, International Conference on Machine Learning, PMLR, 2019, pp. 2347-2356.
* [29] Jackson Gorham and Lester Mackey, _Measuring sample quality with kernels_, International Conference on Machine Learning, PMLR, 2017, pp. 1292-1301.
* [30] Jackson Gorham, Anant Raj, and Lester Mackey, _Stochastic Stein discrepancies_, Advances in Neural Information Processing Systems **33** (2020), 17931-17942.
* [31] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine, _Reinforcement learning with deep energy-based policies_, International conference on machine learning, PMLR, 2017, pp. 1352-1361.
* [32] Ye He, Krishnakumar Balasubramanian, Bharath K Sriperumbudur, and Jianfeng Lu, _Regularized Stein variational gradient flow_, arXiv preprint arXiv:2211.07861 (2022).
* [33] Richard Jordan, David Kinderlehrer, and Felix Otto, _The variational formulation of the Fokker-Planck equation_, SIAM journal on mathematical analysis **29** (1998), no. 1, 1-17.
* [34] Mikolaj J Kasprzak, Ryan Giordano, and Tamara Broderick, _How good is your Gaussian approximation of the posterior? Finite-sample computable error bounds for a variety of useful divergences_, arXiv preprint arXiv:2209.14992 (2022).
* [35] Anya Katsevich and Philippe Rigollet, _On the approximation accuracy of Gaussian variational inference_, arXiv preprint arXiv:2301.02168 (2023).
* [36] Gabriel Khan and Jun Zhang, _When optimal transport meets information geometry_, Information Geometry **5** (2022), no. 1, 47-78.
* [37] Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling, _Improved variational inference with inverse autoregressive flow_, Advances in Neural Information Processing Systems **29** (2016).
* [38] Anna Korba, Pierre-Cyril Aubin-Frankowski, Szymon Majewski, and Pierre Ablin, _Kernel Stein discrepancy descent_, International Conference on Machine Learning, PMLR, 2021, pp. 5719-5730.
* [39] Anna Korba and Adil Salim, _Sampling as first-order optimization over a space of probability measures_, Tutorial at the International Conference of Machine Learning, 2022, [https://akorba.github.io/resources/Baltimore_July2022_ICMLtutorial.pdf](https://akorba.github.io/resources/Baltimore_July2022_ICMLtutorial.pdf).
* [40] Anna Korba, Adil Salim, Michael Arbel, Giulia Luise, and Arthur Gretton, _A non-asymptotic analysis for Stein variational gradient descent_, Advances in Neural Information Processing Systems, vol. 33, 2020, pp. 4672-4682.

* [41] Andreas Kriegl and Peter W Michor, _The convenient setting of global analysis_, vol. 53, American Mathematical Soc., 1997.
* [42] John D Lafferty, _The density manifold and configuration space quantization_, Transactions of the American Mathematical Society **305** (1988), no. 2, 699-741.
* [43] Marc Lambert, Sinho Chewi, Francis Bach, Silvere Bonnabel, and Philippe Rigollet, _Variational inference via Wasserstein gradient flows_, Advances in Neural Information Processing Systems, vol. 35, 2022.
* [44] Thomas Laurent, _Local and global existence for an aggregation equation_, Communications in Partial Differential Equations **32** (2007), no. 12, 1941-1964.
* [45] Michel Ledoux, _On optimal matching of Gaussian samples_, Journal of Mathematical Sciences **238** (2019), 495-522.
* [46] Michel Ledoux and Jie-Xiang Zhu, _On optimal matching of Gaussian samples III_, Probability and Mathematical Statistics **41** (2021).
* [47] Jing Lei, _Convergence and concentration of empirical measures under Wasserstein distance in unbounded functional spaces_, Bernoulli **26** (2020), no. 1, 767-798.
* [48] Wu Lin, Mohammad Emtiyaz Khan, and Mark Schmidt, _Fast and simple natural-gradient variational inference with mixture of exponential-family approximations_, International Conference on Machine Learning, PMLR, 2019, pp. 3992-4002.
* [49] Chang Liu and Jun Zhu, _Riemannian Stein variational gradient descent for Bayesian inference_, Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, 2018.
* [50] Chang Liu, Jingwei Zhuo, Pengyu Cheng, Ruiyi Zhang, and Jun Zhu, _Understanding and accelerating particle-based variational inference_, International Conference on Machine Learning, PMLR, 2019, pp. 4082-4092.
* [51] Qiang Liu, _Stein variational gradient descent as gradient flow_, Advances in neural information processing systems, vol. 30, 2017.
* [52] Qiang Liu, Jason Lee, and Michael Jordan, _A kernelized Stein discrepancy for goodness-of-fit tests_, International conference on machine learning, PMLR, 2016, pp. 276-284.
* [53] Qiang Liu and Dilin Wang, _Stein variational gradient descent: A general purpose Bayesian inference algorithm_, Advances in neural information processing systems, vol. 29, 2016.
* [54], _Stein variational gradient descent as moment matching_, Advances in Neural Information Processing Systems, vol. 31, 2018.
* [55] Xing Liu, Harrison Zhu, Jean-Francois Ton, George Wynne, and Andrew Duncan, _Grassmann Stein variational gradient descent_, International Conference on Artificial Intelligence and Statistics, PMLR, 2022, pp. 2002-2021.
* [56] Xingchao Liu, Xin Tong, and Qiang Liu, _Sampling with trusthworthy constraints: A variational gradient framework_, Advances in Neural Information Processing Systems **34** (2021), 23557-23568.
* [57] John Lott, _Some geometric calculations on Wasserstein space_, Communications in Mathematical Physics **277** (2008), no. 2, 423-437.
* [58] Jianfeng Lu, Yulong Lu, and James Nolen, _Scaling limit of the Stein variational gradient descent: The mean field regime_, SIAM Journal on Mathematical Analysis **51** (2019), no. 2, 648-671.
* [59] Jan R. Magnus and Heinz Neudecker, _Matrix differential calculus with applications in statistics and econometrics_, Wiley Series in Probability and Statistics, John Wiley, 1988.
* [60] Luigi Malago, Luigi Montrucchio, and Giovanni Pistone, _Wasserstein Riemannian geometry of Gaussian densities_, Information Geometry **1** (2018), 137-179.
* [61] Gael M Martin, David T Frazier, and Christian P Robert, _Approximating Bayes in the 21st century_, Statistical Science **1** (2023), no. 1, 1-26.
* [62] Boris Muzellec and Marco Cuturi, _Generalizing point embeddings using the wasserstein space of elliptical distributions_, Advances in Neural Information Processing Systems **31** (2018).
* [63] Yurii Evgenevich Nesterov, _A method of solving a convex programming problem with convergence rate \(O(k^{2})\)_, Doklady Akademii Nauk **269** (1983), no. 3, 543-547.

* [64] Manfred Opper and Cedric Archambeau, _The variational Gaussian approximation revisited_, Neural Computation **21** (2009), no. 3, 786-792.
* [65] Felix Otto, _The geometry of dissipative evolution equations: the porous medium equation_, Comm. Partial Differential Equations **26** (2001), 101-174.
* [66] Peter Petersen, _Riemannian geometry_, vol. 171, Springer, 2006.
* [67] Matias Quiroz, David J Nott, and Robert Kohn, _Gaussian variational approximations for high-dimensional state space models_, Bayesian Analysis **1** (2022), no. 1, 1-28.
* [68] Danilo Rezende and Shakir Mohamed, _Variational inference with normalizing flows_, International Conference on Machine Learning, PMLR, 2015, pp. 1530-1538.
* [69] Adil Salim, Anna Korba, and Giulia Luise, _The Wasserstein proximal gradient algorithm_, Advances in Neural Information Processing Systems (H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, eds.), vol. 33, Curran Associates, Inc., 2020, pp. 12356-12366.
* [70] Adil Salim, Lukang Sun, and Peter Richtarik, _A convergence theory for SVGD in the population limit under Talagrand's inequality T1_, International Conference on Machine Learning, PMLR, 2022, pp. 19139-19152.
* [71] Jiaxin Shi, Chang Liu, and Lester Mackey, _Sampling with mirrored Stein operators_, International Conference on Learning Representations, 2022.
* [72] Jiaxin Shi and Lester Mackey, _A Finite-Particle Convergence Rate for Stein Variational Gradient Descent_, arXiv preprint arXiv:2211.09721 (2022).
* [73] Vladimir Spokoiny, _Dimension free non-asymptotic bounds on the accuracy of high dimensional Laplace approximation_, arXiv preprint arXiv:2204.11038 (2022).
* [74] Weijie Su, Stephen Boyd, and Emmanuel Candes, _A differential equation for modeling Nesterov's accelerated gradient method: theory and insights_, Advances in neural information processing systems, vol. 27, 2014.
* [75] Lukang Sun, Avetik Karagulyan, and Peter Richtarik, _Convergence of Stein variational gradient descent under a weaker smoothness condition_, International Conference on Artificial Intelligence and Statistics, PMLR, 2023, pp. 3693-3717.
* [76] Amirhossein Taghvaei and Prashant Mehta, _Accelerated flow for probability distributions_, International Conference on Machine Learning, PMLR, 2019, pp. 6076-6085.
* [77] Asuka Takatsu, _Wasserstein geometry of Gaussian measures_, Osaka Journal of Mathematics **48** (2011), no. 4, 1005-1026.
* [78] Linda SL Tan and David J Nott, _Gaussian variational approximation with sparse precision matrices_, Statistics and Computing **28** (2018), 259-275.
* [79] Gerald Teschl, _Ordinary differential equations and dynamical systems_, Graduate Studies in Mathematics **140** (2000), 08854-8019.
* [80] Marcin Tomczak, Siddharth Swaroop, and Richard Turner, _Efficient low rank Gaussian variational inference for neural networks_, Advances in Neural Information Processing Systems **33** (2020), 4610-4622.
* [81] Aad W van der Vaart, _Asymptotic statistics_, vol. 3, Cambridge University Press, 2000.
* [82] Jesse van Oostrum, _Bures-Wasserstein geometry for positive-definite Hermitian matrices and their trace-one subset_, Information Geometry **5** (2022), no. 2, 405-425.
* [83] Cedric Villani, _Topics in optimal transportation_, Graduate Studies in Mathematics 58, American Mathematical Society, 2003.
* [84] Dilin Wang, Ziyang Tang, Chandrajit Bajaj, and Qiang Liu, _Stein variational gradient descent with matrix-valued kernels_, Advances in neural information processing systems **32** (2019).
* [85] Yifei Wang, Peng Chen, and Wuchen Li, _Projected Wasserstein gradient descent for high-dimensional Bayesian inference_, SIAM/ASA Journal on Uncertainty Quantification **10** (2022), no. 4, 1513-1532.
* [86] Yifei Wang and Wuchen Li, _Accelerated information gradient flow_, Journal of Scientific Computing **90** (2022), 1-47.

* [87] Lantian Xu, Anna Korba, and Dejan Slepcev, _Accurate quantization of measures via interacting particle-based optimization_, International Conference on Machine Learning, PMLR, 2022, pp. 24576-24595.
* [88] Rentian Yao and Yun Yang, _Mean field variational inference via Wasserstein gradient flow_, arXiv preprint arXiv:2207.08074 (2022).
* [89] Jingwei Zhuo, Chang Liu, Jiaxin Shi, Jun Zhu, Ning Chen, and Bo Zhang, _Message passing Stein variational gradient descent_, International Conference on Machine Learning, PMLR, 2018, pp. 6018-6027.

Further Discussion

**Other Related Works.** There exist several other approaches for minimizing KL-divergence over the Wasserstein space (or over appropriately restricted subsets). For example, normalizing flows [68, 37, 11], the blob method [10], variants of gradient descent in Wasserstein spaces [69, 85, 5], natural-gradient based variational inference techniques [48], mean-field methods [88], neural networks based approaches [2] and MCMC methods. This is certainly not a comprehensive list and summarizing the large literature on this topic is impossible in the limited space available. However, we emphasize that a majority of the above methods are nonparametric and do not come with strong theoretical guarantees. Our main focus in this work is on theoretically understanding (Gaussian)-SVGD and its connection to GVI.

**Beyond Gaussian VI.** While we present a comprehensive study of Gaussian-SVGD, it is currently unclear how such relation between bilinear kernels and Gaussian family could be generalized. For example, it would be interesting to ask if there is a class (submanifold) \(\mathcal{C}\) of probability densities such that for any initialization-target pair \(\rho_{0},\rho^{*}\in\mathcal{C}\), the dynamics of SVGD with a radius-based kernel function (RBF-SVGD) would always remain in \(\mathcal{C}\). If such \(\mathcal{C}\) is identified, we could similarly carry out the convergence analysis of RBF-SVGD and perform variational inference with respect to the class \(\mathcal{C}\). It remains an open question whether the uniform in time propagation of chaos as shown in Theorem 3.7 would hold for general kernels. Moreover, there has been increasing interest on variational inference with respect to other special density classes. Remarkably [43] considered Gaussian mixtures in additional to GVI, and [62] analyzed the family of elliptical distributions, which has applications for performing variational inference under heavy-tails [21].

**SVGD in High Dimensions.** It is well-known in literature [89, 4] that RBF-SVGD suffers from severe particle degeneracy in high dimensions and cannot guarantee good covariance estimation. However, such issue has not been observed for Gaussian-SVGD in our simulations. It would be interesting future works to study whether this undesired phenomenon is tied to specific choices of kernels and to carry out theoretical analysis of (Gaussian)-SVGD in high dimensions.

## Appendix B Details on Gaussian-Stein Metrics

For any region \(\Omega\subseteq\mathbb{R}^{d}\), denote the set of probability densities on \(\Omega\) by

\[\mathcal{P}(\Omega):=\left\{\rho\in\mathcal{F}(\Omega):\int_{\Omega}\rho\, \mathrm{d}\mathbf{x}=1,\rho\geq 0\right\},\]

where \(\mathcal{F}(\Omega)\) is the set of \(\mathcal{C}^{\infty}\)-smooth functions on \(\Omega\). As studied in [42], \(\mathcal{P}(\Omega)\) forms a Frechet manifold called the density manifold. For any "point" \(\rho\in\mathcal{P}(\Omega)\), the tangent space is given by

\[T_{\rho}\mathcal{P}(\Omega)=\left\{\sigma\in\mathcal{F}(\Omega):\int\sigma \,\mathrm{d}\mathbf{x}=0\right\}.\]

And the cotangent space at \(\rho\), \(T_{\rho}^{*}\mathcal{P}(\Omega)\) consists of equivalent classes of \(\mathcal{F}(\Omega)\), each containing functions that differ by a constant. A Riemannian metric (tensor) assigns to each \(\rho\in\mathcal{P}(\Omega)\) a positive definite inner product \(g_{\rho}:T_{\rho}\mathcal{P}(\Omega)\times T_{\rho}\mathcal{P}(\Omega)\to \mathbb{R}\). If we define the pairing between \(T_{\rho}^{*}\mathcal{P}(\Omega)\) and \(T_{\rho}\mathcal{P}(\Omega)\) by

\[\langle\Phi,\sigma\rangle:=\int\Phi\cdot\sigma\,\mathrm{d}\mathbf{x},\]

where \(\Phi\in T_{\rho}^{*}\mathcal{P}(\Omega)\) and \(\sigma\in T_{\rho}\mathcal{P}(\Omega)\). Any Riemannian metric uniquely corresponds to an isomorphism (called the **canonical isomorphism**) between the tangent and cotangent bundles [66], i.e., we have \(G_{\rho}:T_{\rho}\mathcal{P}(\Omega)\to T_{\rho}^{*}\mathcal{P}(\Omega)\) through

\[g_{\rho}(\sigma_{1},\sigma_{2})=\langle G_{\rho}\sigma_{1},\sigma_{2}\rangle =\langle G_{\rho}\sigma_{2},\sigma_{1}\rangle,\quad\sigma_{1},\sigma_{2}\in T _{\rho}\mathcal{P}(\Omega).\]

**Definition B.1** (Wasserstein metric).: _The Wasserstein metric is induced by the following canonical isomorphism \(G_{\rho}^{\mathrm{Wass}}:T_{\rho}\mathcal{P}(\Omega)\to T_{\rho}^{*} \mathcal{P}(\Omega)\) such that_

\[(G_{\rho}^{\mathrm{Wass}})^{-1}\Phi=-\nabla\cdot(\rho\nabla\Phi),\quad\Phi\in T _{\rho}^{*}\mathcal{P}(\Omega).\]

Note that the Wasserstein metric we define here corresponds exactly to the Wasserstein-\(2\) distance studied in the literature of optimal transport [83]. For details of such connection, please refer to [36]. Also we need to clarify that the concepts we introduce follow from the convention in Riemannian geometry, where the manifold strictly speaking is required to be finite-dimensional. For a mathematically formal formulation of infinite-dimensional calculus on the density manifold, please refer to [42, 41, 65, 57].

The Wasserstein gradient flow can be seen as the natural gradient flow on the density manifold with respect to the Wasserstein metric. In specific, consider any energy functional \(E(\rho)\), e.g., the KL divergence from \(\rho\) to some fixed target \(\rho^{*}\). The Wasserstein gradient flow for \(E(\rho)\) is provided by

\[\dot{\rho}_{t}=-(G^{\text{Wass}}_{\rho_{t}})^{-1}\frac{\delta E}{\delta\rho_{t} }=\nabla\cdot\left(\rho_{t}\nabla\frac{\delta E}{\delta\rho_{t}}\right), \tag{27}\]

where \(\frac{\delta E}{\delta\rho_{t}}\) is the variational derivative of the functional \(E\) with respect to \(\rho_{t}\). If \(E(\rho)\) is the KL divergence mentioned above, (27) gives the linear Fokker-Planck equation [33]

\[\dot{\rho}_{t}=\nabla\cdot(\nabla\rho_{t}+\rho_{t}\nabla V),\]

where \(V(\mathbf{x})=-\log\rho^{*}(\mathbf{x})\) is called the potential function. If \(E(\rho)\) is the Wasserstein metric between \(\rho\) and \(\rho^{*}\), (1) gives the geodesic flow on the density manifold. If \(E(\rho)\) is the maximum mean discrepancy (MMD) or kernelized Stein discrepancy (KSD) between \(\rho\) and \(\rho^{*}\), it leads to the MMD descent and KSD descent algorithms [3, 38].

Interestingly, the mean field flow of SVGD can also be seen as the gradient flows of the KL divergence on the density manifold but with respect to the Stein metric, where we perform kernelization in the cotangent space before taking the divergence [51, 22].

**Definition B.2** (Stein metric).: _The Stein metric is induced by the following canonical isomorphism \(G^{\text{Wass}}_{\rho}:T_{\rho}\mathcal{P}(\Omega)\to T^{*}_{\rho}\mathcal{P}(\Omega)\) such that_

\[(G^{\text{Stein}}_{\rho})^{-1}\Phi=-\nabla\cdot\left(\rho(\cdot)\int K(\cdot, \mathbf{y})\rho(\mathbf{y})\nabla\Phi(\mathbf{y})\,\mathrm{d}\mathbf{y}\right),\quad\Phi\in T^{ *}_{\rho}\mathcal{P}(\Omega).\]

In particular, the mean field PDE of the SVGD algorithm can be written as

\[\dot{\rho}_{t}(\mathbf{x}) =-(G^{\text{Stein}}_{\rho})^{-1}\frac{\delta\operatorname{KL}( \rho_{t}\parallel\rho^{*})}{\delta\rho_{t}}(\mathbf{x}) \tag{28}\] \[=\nabla\cdot\left(\rho_{t}(\mathbf{x})\int K(\mathbf{x},\mathbf{y})\rho_{t}( \mathbf{y})\nabla\frac{\delta\operatorname{KL}(\rho_{t}\parallel\rho^{*})}{\delta \rho_{t}}(\mathbf{y})\,\mathrm{d}\mathbf{y}\right)\] \[=\nabla\cdot\left(\rho_{t}(\mathbf{x})\int K(\mathbf{x},\mathbf{y})\rho_{t}( \mathbf{y})\,\nabla\big{(}\log\rho_{t}(\mathbf{y})+V+1\big{)}\,\mathrm{d}\mathbf{y}\right)\] \[=\nabla\cdot\left(\rho_{t}(\mathbf{x})\int K(\mathbf{x},\mathbf{y})\big{(} \nabla\rho_{t}(\mathbf{y})+\rho_{t}(\mathbf{y})\nabla V(\mathbf{y})\big{)}\,\mathrm{d}\bm {y}\right),\]

where \(V(\mathbf{x})=-\log\rho^{*}(\mathbf{x})\).

We set \(\Omega=\mathbb{R}^{d}\) and consider the multivariate Gaussian densities \(\mathcal{N}(\mathbf{\mu},\Sigma)\) in \(\mathbb{R}^{d}\) :

\[\rho(\mathbf{x},\theta)=\tfrac{1}{\sqrt{\det(2\pi\Sigma)}}\exp\left(-\tfrac{1}{2}( \mathbf{x}-\mathbf{\mu})^{\top}\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})\right),\]

where \(\theta:=(\mathbf{\mu},\Sigma)\in\Theta\) and \(\Theta:=\mathbb{R}^{d}\times\operatorname{Sym}^{+}(d,\mathbb{R})\). Here \(\operatorname{Sym}^{+}(d,\mathbb{R})\) is the set of (symmetric) positive definite \(d\times d\) matrices, which is an open subset of the \(d\times d\) symmetric matrix space \(\operatorname{Sym}(d,\mathbb{R})\). In this way, \(\Theta\) can be identified as a Riemannian submanifold of the density manifold \(\mathcal{P}(\mathbb{R}^{d})\) with the induced Riemannian structure.

We first look into the Stein metric with the bilinear kernel \(K_{1}(\mathbf{x},\mathbf{y})=\mathbf{x}^{\top}\mathbf{y}+1\), and derive the closed form of the induced metric tensor on \(\Theta\), which plays an essential role in characterizing the SVGD dynamics.

**Theorem B.3** (Gaussian-Stein metric with the simple bilinear kernel).: _Given \(\theta=(\mathbf{\mu},\Sigma)\in\Theta\), let \(g_{\theta}(\cdot,\cdot)\) denote the Gaussian-Stein metric tensor for the multivariate Gaussian model with the bilinear kernel \(K_{1}(\mathbf{x},\mathbf{y})=\mathbf{x}^{\top}\mathbf{y}+1\), and \(G_{\theta}\) be the corresponding canonical isomorphism from \(T_{\theta}\Theta\simeq\mathbb{R}^{d}\times\operatorname{Sym}(d,\mathbb{R})\) to \(T^{*}_{\theta}\Theta\simeq\mathbb{R}^{d}\times\operatorname{Sym}(d,\mathbb{R})\)._

_For any \(\mathbf{\nu}\in\mathbb{R}^{n}\), and \(S\in\operatorname{Sym}(d,\mathbb{R})\), the inverse map \(G^{-1}_{\theta}\) is given by the following automorphism on \(\mathbb{R}^{d}\times\operatorname{Sym}(d,\mathbb{R})\):_

\[G^{-1}_{\theta}(\mathbf{\nu},S)=\left(2S\Sigma\mathbf{\mu}+(1+\mathbf{\mu}^{\top}\mathbf{\mu}) \mathbf{\nu},\ \big{(}\Sigma(2\Sigma S+\mathbf{\mu}\mathbf{\nu}^{\top})+(2S\Sigma+\mathbf{\nu}\mathbf{ \mu}^{\top})\Sigma\big{)}\right). \tag{29}\]_And for any \(\xi,\eta\in T_{\theta}\Theta\) the Stein metric tensor can be written as_

\[g_{\theta}(\xi,\eta)=\operatorname{tr}(S_{1}S_{2}\Sigma^{2})+(\mathbf{b}_{1}^{\top}S _{2}+\mathbf{b}_{2}^{\top}S_{1})\Sigma\mathbf{\mu}+(1+\mathbf{\mu}^{\top}\mathbf{\mu})\mathbf{b}_{1} ^{\top}\mathbf{b}_{2}. \tag{30}\]

_Here \(\xi=\left(\widetilde{\mathbf{\mu}}_{1},\widetilde{\Sigma}_{1}\right)\) and \(\eta=\left(\widetilde{\mathbf{\mu}}_{2},\widetilde{\Sigma}_{2}\right)\), in which \(\widetilde{\mathbf{\mu}}_{1},\widetilde{\mathbf{\mu}}_{2}\in\mathbb{R}^{d},\widetilde{ \Sigma}_{1},\widetilde{\Sigma}_{2}\in\operatorname{Sym}(d,\mathbb{R})\). And \(\mathbf{b}_{i},S_{i}\)'s (\(i=1,2\)) are defined as_

\[\left(\mathbf{b}_{i},\frac{1}{2}S_{i}\right)=G_{\theta}(\widetilde{\mathbf{\mu}}_{i}, \widetilde{\Sigma}_{i}).\]

Then for the Gaussian-Stein metric with kernel \(K_{2}(\mathbf{x},\mathbf{y})=(\mathbf{x}-\mathbf{\mu})^{\top}(\mathbf{y}-\mathbf{\mu})+1\) we have the following result:

**Theorem B.4** (Gaussian-Stein metric with the affine-invariant bilinear kernel).: _Given \(\theta=(\mathbf{\mu},\Sigma)\in\Theta\), let \(g_{\theta}(\cdot,\cdot)\) denote the affine-invariant Gaussian-Stein metric tensor for the multivariate Gaussian model with the affine-invariant bilinear kernel \(K_{2}\), and \(G_{\theta}\) be the corresponding canonical isomorphism from \(T_{\theta}\Theta\simeq\mathbb{R}^{d}\times\operatorname{Sym}(d,\mathbb{R})\) to \(T_{\theta}^{*}\Theta\simeq\mathbb{R}^{d}\times\operatorname{Sym}(d,\mathbb{R})\)._

_For any \(\mathbf{\nu}\in\mathbb{R}^{n}\), and \(S\in\operatorname{Sym}(d,\mathbb{R})\), the inverse map \(G_{\theta}^{-1}\) is given by the following automorphism on \(\mathbb{R}^{d}\times\operatorname{Sym}(d,\mathbb{R})\):_

\[G_{\theta}^{-1}(\mathbf{\nu},S)=\left(\mathbf{\nu},\;2\left(\Sigma^{2}S+S\Sigma^{2} \right)\right). \tag{31}\]

_And for any \(\xi,\eta\in T_{\theta}\Theta\) the Stein metric tensor can be written as_

\[g_{\theta}(\xi,\eta)=\operatorname{tr}(S_{1}S_{2}\Sigma^{2})+\widetilde{\mathbf{ \mu}}_{1}^{\top}\widetilde{\mathbf{\mu}}_{2}. \tag{32}\]

_Here \(\xi=\left(\widetilde{\mathbf{\mu}}_{1},\widetilde{\Sigma}_{1}\right)\) and \(\eta=\left(\widetilde{\mathbf{\mu}}_{2},\widetilde{\Sigma}_{2}\right)\), in which \(\widetilde{\mathbf{\mu}}_{1},\widetilde{\mathbf{\mu}}_{2}\in\mathbb{R}^{d},\widetilde{ \Sigma}_{1},\widetilde{\Sigma}_{2}\in\operatorname{Sym}(d,\mathbb{R})\). And \(S_{i}\)'s (\(i=1,2\)) are defined as the symmetric solution of_

\[\widetilde{\Sigma}_{i}=\Sigma^{2}S_{i}+S_{i}\Sigma^{2}.\]

Now if we further restrict the Gaussian family to have zero mean, it induces the submanifold \(\Theta_{0}=\operatorname{Sym}^{+}(d,\mathbb{R})\). We have the following result for \(K_{1}\) or \(K_{2}\) as a direct corollary of Theorem B.3 or Theorem B.4.

**Corollary B.5**.: _Given \(\Sigma\in\Theta_{0}\), let \(g_{\Sigma}(\cdot,\cdot)\) denote the Gaussian-Stein metric tensor for the centered Gaussian family with the bilinear kernel \(K(\mathbf{x},\mathbf{y})=\mathbf{x}^{\top}\mathbf{y}+1\), and \(G_{\Sigma}\) be the corresponding canonical isomorphism from \(T_{\Sigma}\Theta_{0}\simeq\operatorname{Sym}(d,\mathbb{R})\) to \(T_{\Sigma}^{*}\Theta_{0}\simeq\operatorname{Sym}(d,\mathbb{R})\)._

_For any \(S\in\operatorname{Sym}(d,\mathbb{R})\), the inverse map \(G_{\Sigma}^{-1}\) is given by the following automorphism on \(\operatorname{Sym}(d,\mathbb{R})\):_

\[G_{\Sigma}^{-1}(S)=2(\Sigma^{2}S+S\Sigma^{2}). \tag{33}\]

_And for any \(\widetilde{\Sigma}_{1},\widetilde{\Sigma}_{2}\in T_{\Sigma}\Theta_{0}\) the Stein metric tensor can be written as_

\[g_{\Sigma}(\widetilde{\Sigma}_{1},\widetilde{\Sigma}_{2})=\operatorname{tr}(S _{1}S_{2}\Sigma^{2}), \tag{34}\]

_where for \(i=1,2\), \(S_{i}\) is the unique solution in \(\operatorname{Sym}(d,\mathbb{R})\) that satisfies the Lyapunov equation_

\[\widetilde{\Sigma}_{i}=\Sigma^{2}S_{i}+S_{i}\Sigma^{2}.\]

Next we consider the Bures-Wasserstein metric for Gaussian families, which is defined as the Wasserstein metric restricted to the Gaussian family. It has the following elegant expressions from [77, 60]:

**Theorem B.6** (Bures-Wasserstein metric).: _Given \(\theta=(\mathbf{\mu},\Sigma)\in\Theta\), let \(g_{\theta}(\cdot,\cdot)\) denote the Bures-Wasserstein metric tensor for the multivariate Gaussian model (or equivalently Gaussian-Stein metric with the kernel \(K_{3}\)), and \(G_{\theta}\) be the corresponding isomorphism from \(T_{\theta}\Theta\simeq\mathbb{R}^{d}\times\operatorname{Sym}(d,\mathbb{R})\) to \(T_{\theta}^{*}\Theta\simeq\mathbb{R}^{d}\times\operatorname{Sym}(d,\mathbb{R})\). For any \(\mathbf{\nu}\in\mathbb{R}^{n}\), and \(S\in\operatorname{Sym}(d,\mathbb{R})\), the inverse map \(G_{\theta}^{-1}\) is given by the following automorphism on \(\mathbb{R}^{d}\times\operatorname{Sym}(d,\mathbb{R})\):_

\[(G_{\theta}^{\text{\rm{Wass}}})^{-1}(\mathbf{\nu},S)=\left(\mathbf{\nu},\;2(\Sigma S+S \Sigma)\right). \tag{35}\]

_And for any \(\xi,\eta\in T_{\theta}\Theta\) the Bures-Wasserstein metric tensor can be written as_

\[g_{\theta}(\xi,\eta)=\operatorname{tr}(S_{1}S_{2}\Sigma)+\widetilde{\mathbf{\mu}} _{1}^{\top}\widetilde{\mathbf{\mu}}_{2}. \tag{36}\]_Here \(\xi=\left(\widetilde{\mathbf{\mu}}_{1},\widetilde{\Sigma}_{1}\right)\) and \(\eta=\left(\widetilde{\mathbf{\mu}}_{2},\widetilde{\Sigma}_{2}\right)\), in which \(\widetilde{\mathbf{\mu}}_{1},\widetilde{\mathbf{\mu}}_{2}\in\mathbb{R}^{d},\widetilde{ \Sigma}_{1},\widetilde{\Sigma}_{2}\in\operatorname{Sym}(d,\mathbb{R})\). And \(S_{i}\)'s (\(i=1,2\)) are defined as the symmetric solution of_

\[\widetilde{\Sigma}_{i}=\Sigma S_{i}+S_{i}\Sigma.\]

_Notably this metric coincides with Gaussian-Stein metric with the kernel \(K_{3}(\mathbf{x},\mathbf{y})=(\mathbf{x}-\mathbf{\mu})^{\top}\Sigma^{-1}(\mathbf{y}-\mathbf{\mu})+1\)._

Note that the only difference between (32) and (36) is the power of \(\Sigma\).

**Theorem B.7** (Regularized Gaussian-Stein metric with the affine-invariant bilinear kernel).: _Given \(\theta=(\mathbf{\mu},\Sigma)\in\Theta\), let \(g_{\theta}(\cdot,\cdot)\) denote the affine-invariant regularized Gaussian-Stein metric tensor for Gaussian families, and \(G_{\theta}\) be the corresponding isomorphism from \(T_{b}\Theta\simeq\mathbb{R}^{d}\times\operatorname{Sym}(d,\mathbb{R})\) to \(T_{\theta}^{*}\Theta\simeq\mathbb{R}^{d}\operatorname{Sym}(d,\mathbb{R})\). For any \(\mathbf{\nu}\in\mathbb{R}^{d}\) and \(S\in\operatorname{Sym}(d,\mathbb{R})\), the inverse map \(G_{\theta}^{-1}\) is given by the following automorphism on \(\mathbb{R}^{d}\times\operatorname{Sym}(d,\mathbb{R})\):_

\[(G_{\theta}^{\text{RS}})^{-1}(\mathbf{\nu},S)=\left(\mathbf{\nu},2\left(((1-\nu)\Sigma +\nu I)^{-1}\Sigma^{2}S+S((1-\nu)\Sigma+\nu I)^{-1}\Sigma^{2}\right)\right). \tag{37}\]

_Notably this metric coincides with Gaussian-Stein metric with the kernel \(K_{4}(\mathbf{x},\mathbf{y})=(\mathbf{x}-\mathbf{\mu})^{\top}((1-\nu)\Sigma+\nu I)^{-1}(\mathbf{y} -\mathbf{\mu})+1\)._

## Appendix C Properties of SVGD Solutions with the Simple Bilinear Kernel

[58] showed a few nice properties of the mean field PDE (2) for SVGD with radius-based kernels that can be written as \(K(\mathbf{x}-\mathbf{y})\) which is symmetric and positive definite, meaning that

\[\sum_{i=1}^{m}\sum_{j=1}^{m}K(\mathbf{x}_{i}-\mathbf{x}_{j})\xi_{i}\xi_{j}\geq 0, \quad\forall\mathbf{x}_{i}\in\mathbb{R}^{d},\;\xi_{i}\in\mathbb{R},\;m\in\mathbb{ N}.\]

Although their results covered a large class of kernels commonly used in practice e.g., Gaussian kernels, they do not apply to the bilinear kernel \(K(\mathbf{x},\mathbf{y})=\mathbf{x}^{\top}\mathbf{y}+1\). In this subsection we establish similar results for the bilinear kernel. In fact, some of their results for radius-based kernels still hold here while some do not.

Before showing the properties of the mean field PDE, we need the following assumption on the potential function \(V\).

**Assumption C.1**.: _The potential function \(V:\mathbb{R}^{d}\to\mathbb{R}\) satisfies the conditions below:_

1. \(V\in\mathcal{C}^{\infty}(\mathbb{R}^{d})\)_,_ \(V\geq 0\)_, and_ \(V(\mathbf{x})\to\infty\) _as_ \(\|\mathbf{x}\|\to\infty\)_._
2. _For any_ \(\alpha,\beta>0\)_, there exists a constant_ \(C_{\alpha,\beta}>0\) _such that if_ \(\|\mathbf{y}\|\leq\alpha\|\mathbf{x}\|+\beta\)_, then the following inequality always holds that_ \[(1+\|\mathbf{x}\|)\|\nabla V(\mathbf{y})\|+(1+\|\mathbf{x}\|)^{2}\|\nabla^{2}V(\mathbf{y})\| \leq C_{\alpha,\beta}(1+V(\mathbf{x})).\]

To make things precise although all vector norms in \(\mathbb{R}^{d}\) and all matrix norms are equivalent, we always choose the Euclidean norm for vectors and the spectral norm for matrices unless otherwise specified. Note that our assumption here is a little different from Assumption 2.1 in [58]. We do not require their second formula but our second piece of assumption is slightly stricter than their third one. It is straightforward to check that any positive definite quadratic form satisfies all the assumptions above, corresponding to the case where the target is a non-degenerate Gaussian distribution.

We use \(\mathcal{P}_{V}(\mathbb{R}^{d})\) and \(\mathcal{P}^{p}(\mathbb{R}^{d})\) (\(p=1,2,\cdots\)) to denote the set of probability measure \(\mu\) on \(\mathbb{R}^{d}\) satisfying

\[\|\mu\|_{\mathcal{P}_{V}}:=\int_{\mathbb{R}^{d}}(1+V(\mathbf{x}))\,\mathrm{d}\mu( \mathbf{x})<\infty \tag{38}\]

and

\[\|\mu\|_{\mathcal{P}^{p}}:=\int_{\mathbb{R}^{d}}\|\mathbf{x}\|^{p}\,\mathrm{d}\mu( \mathbf{x})<\infty \tag{39}\]

respectively.

**Theorem C.2** (Well-posedness and regularity of the mean field solution).: _Let \(V\) satisfy Assumption C.1. For any \(\nu\in\mathcal{P}_{V}(\mathbb{R}^{d})\), there is a unique \(\rho_{t}\in\mathcal{C}\big{(}[0,\infty),\mathcal{P}_{V}(\mathbb{R}^{d})\big{)}\) which is a weak solution to (2) with initial condition \(\rho_{0}=\nu\). Moreover, there exists \(C_{1}>0\) depending on \(V\) such that_

\[\|\rho_{t}\|_{\mathcal{P}_{V}}\leq e^{C_{1}t}\|\nu\|_{\mathcal{P}_{V}}\quad t \geq 0. \tag{40}\]

_If \(\nu\in\mathcal{P}^{p}(\mathbb{R}^{d})\cap\mathcal{P}_{V}(\mathbb{R}^{d})\), then for any \(t\in[0,\infty)\), we have that \(\rho_{t}\in\mathcal{P}^{p}(\mathbb{R}^{d})\cap\mathcal{P}_{V}(\mathbb{R}^{d})\) and there exists \(C_{2}>0\) depending on \(V\) such that_

\[\|\rho_{t}\|_{\mathcal{P}^{p}}\leq e^{C_{2}t}\|\nu\|_{\mathcal{P}^{p}}\quad t \geq 0. \tag{41}\]

_If \(\nu\) has a density \(\rho_{0}(\mathbf{x})\geq 0\), then \(\rho_{t}\) also has a density. Furthermore, if \(\rho_{0}\in\mathcal{H}^{k}(\mathbb{R}^{d})\) for some \(k\), then we have \(\rho_{t}\in\mathcal{H}^{k}(\mathbb{R}^{d})\). Here_

\[\mathcal{H}^{k}(\mathbb{R}^{d})=\mathcal{W}^{k,2}(\mathbb{R}^{d}):=\left\{u\in \mathcal{L}^{p}(\mathbb{R}^{d}):D^{\alpha}u\in\mathcal{L}^{p}(\mathbb{R}^{d}) \;\forall|\alpha|\leq k\right\},\quad k\geq 1\]

_denotes the Sobolev (Hilbert) space of order \(k\)._

**Theorem C.3** (Well-posedness of the finite-particle solution).: _Let \(V\) satisfy Assumption C.1. Then for any initial condition \(X_{0}=\left(\mathbf{x}_{1}^{(0)},\cdots,\mathbf{x}_{N}^{(0)}\right)^{\top}\in\mathbb{R }^{dN}\), the system (13) has a unique solution_

\[X_{t}=\left(\mathbf{x}_{1}^{(t)},\cdots,\mathbf{x}_{N}^{(t)}\right)^{\top}\in\mathcal{ C}^{1}\left([0,\infty),\mathbb{R}^{dN}\right),\]

_and the measure \(\mu_{t}^{N}=\frac{1}{N}\sum_{i=1}^{N}\delta_{\mathbf{x}_{i}^{(t)}}\) is a weak solution to the PDE (2)._

Finally, we show that if two initial probability measures are close to each other, the solutions of (2) up to time \(T\) are also close. We need to further impose the following assumption on \(V\).

**Assumption C.4**.: _There exists a constant \(C_{V}>0\) and some index \(q>1\) such that_

\[\|\nabla V(\mathbf{x})\|^{q}\leq C_{V}(1+V(\mathbf{x}))\quad\text{ for every }\mathbf{x}\in \mathbb{R}^{d} \tag{42}\]

_and that_

\[\sup_{\theta\in[0,1]}\|\nabla^{2}V\left(\theta\mathbf{x}+(1-\theta)\mathbf{y}\right) \|^{q}\leq C_{V}\left(1+\frac{V(\mathbf{x})+V(\mathbf{y})}{(\|\mathbf{x}\|+\|\mathbf{y}\|)^{q }}\right). \tag{43}\]

Remarkably a Gaussian distribution satisfies both Assumptions C.1 and C.4. Secondly an important observation is that (42) implies that there is \(C_{0}\) such that

\[V(\mathbf{x})\leq C_{0}(1+\|\mathbf{x}\|^{q^{\prime}})\quad\forall\mathbf{x}\in\mathbb{R} ^{d} \tag{44}\]

where \(q^{\prime}=q/(q-1)\). Indeed, we note that

\[\partial_{t}\left(1+V\left(t\frac{\mathbf{x}}{\|\mathbf{x}\|}\right)\right)^{\frac{q-1 }{q}}=\frac{q-1}{q}\cdot\frac{\frac{\mathbf{x}}{\|\mathbf{x}\|}\cdot\nabla V\left(t \frac{\mathbf{x}}{\|\mathbf{x}\|}\right)}{\left(1+V\left(t\frac{\mathbf{x}}{\|\mathbf{x}\|} \right)\right)^{1/q}}\leq\left(1-\frac{1}{q}\right)C_{V}^{1/q}.\]

Integrating from \(t=0\) to \(t=\|\mathbf{x}\|\), we get (44), which shows that \(\mathcal{P}^{p}\subset\mathcal{P}_{V}\) for any \(p\geq q^{\prime}=q/(q-1)\).

**Theorem C.5**.: _Let \(V\) satisfy Assumptions C.1 and C.4. Let \(R>0\). Assume that \(\nu_{1},\nu_{2}\) are two initial probability measures in \(\mathcal{P}^{p}(\mathbb{R}^{d})\) satisfying \(\|\nu_{i}\|_{\mathcal{P}^{p}}\leq R\;(i=1,2)\). Let \(\mu_{1,t}\) and \(\mu_{2,t}\) be the associated weak solutions to (2). Then given any \(T>0\), there exists a constant \(C_{T}>0\) depending on \(V,R\) and \(T\) such that_

\[\sup_{t\in[0,T]}\mathcal{W}_{p}\left(\mu_{1,t},\mu_{2,t}\right)\leq C_{T} \mathcal{W}_{p}\left(\nu_{1},\nu_{2}\right).\]

**Remark**.:
1. _Theorem C.5 implies the convergence of empirical measure to the mean field limit at time_ \(t\in[0,T]\)_. In fact, if we set_ \(\nu_{1,n}\) _to be an empirical measure, which converges to_ \(\nu_{2}\) _as_ \(n\) _grows to infinity, then_ \(\mu_{1,n,t}\)_, the solution of (_2_) at time_ \(t\) _with initializaiton_ \(\nu_{1,n}\)_, will also converge to_ \(\mu_{2,t}\) _for any_ \(t\in[0,T]\)_._
2. _In general there is no guarantee that the density_ \(\rho_{t}\) _will converge to the target density_ \(\rho^{*}\) _in (_2_) with the bilinear kernel. Some counterexamples will be elaborated in the remarks following Theorem_ 3.6_._
3. _We show in Theorem_ 3.1 _that for Gaussian families_ \(\rho_{t}\) _always converges to the target density_ \(\rho^{*}\) _as_ \(t\to\infty\) _and the convergence rate is linear in KL divergence. We also establish a uniform in time convergence result (Theorem_ 3.7_) for the empirical measure in this case._Details of the Gaussian-SVGD Algorithms

**Different Ways of Estimating \(\Gamma_{t}\).** The first-order estimator of \(\Gamma_{t}\) arises from

\[\Gamma_{t} =\mathbb{E}_{\rho_{\theta}}[\nabla^{2}V(\mathbf{x})]=\int\rho_{\theta} (\mathbf{x})\nabla^{2}V(\mathbf{x})\,\mathrm{d}\mathbf{x}=-\int\nabla\rho_{\theta}(\mathbf{x}) \nabla V(\mathbf{x})^{\top}\,\mathrm{d}\mathbf{x}\] \[=\int\rho_{\theta}(\mathbf{x})\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})\nabla V( \mathbf{x})^{\top}\,\mathrm{d}\mathbf{x}=\Sigma^{-1}\mathbb{E}_{\rho_{\theta}}[(\mathbf{x} -\mathbf{\mu})\nabla V(\mathbf{x})^{\top}].\]

Since \(\Gamma_{t}\) is symmetric we also have

\[\Gamma_{t}=\mathbb{E}_{\rho_{\theta}}[\nabla V(\mathbf{x})(\mathbf{x}-\mathbf{\mu})^{\top }]\Sigma^{-1}.\]

Note that using the first-order estimator also comes at a cost as the inverse of \(\Sigma\) is needed. However, for density-based Gaussian-SVGD this \(\Sigma^{-1}\) might cancel with \(\Sigma\) in computing.

**Previous Algorithms Under the Proposed Frameworks.** The use of \(K_{1}\) for SVGD in variational inference dates back to [54]. Our Algorithm 2 is slightly different from [54] in the sense that \(\nabla V\) is replaced by a linear function to ensure Gaussianity. Moreover, Algorithms 1 and 2 with the kernel \(K_{2}\) correspond precisely to the GF and GPF algorithms in [27]. If \(K_{3}\) (Bures-Wasserstein metric) is chosen, Algorithm 1 reproduces the BW-SGD algorithm in [43] (with \(N=1\)). [19] also uses \(K_{3}\) (Bures-Wasserstein metric) but their energy function for gradient flow is different from others. Instead of directly performing the gradient descent to minimize KL divergence, they separate the KL divergence into two parts and perform the proximal gradient descent.

**Variants of Gradient Descent in Density-Based Gaussian-SVGD.** For the density-based SVGD, we draw new samples at each step. It is interesting to study the behaviorial difference between drawing one sample (stochastic) and efficiently many samples (almost deterministic). For example, in [43] only one sample is drawn at each time step and they study the stochastic properties arising from this design. [19] considers both settings. In general, they do not differ much in convergence rates but there could be huge gaps in the constants of the bounds and will actually impact practical performance. Another choice is to only draw \(N\) samples at time \(0\) and we use a linear transformation of the same \(N\) points to serve as new samples at time \(t\), which becomes similar to the particle-based Gaussian-SVGD. Moreover, the vanilla gradient descent could also be replaced by accelerated ones or gradient descent with adaptive learning rate, e.g., AdaGrad, RMSProp.

**Resampling Scheme and Particle-Level Convergence.** As presented in the main text the Gaussian-SVGD for a general target is given by

\[\mathbf{x}_{i}^{(t+1)}=\mathbf{x}_{i}^{(t)}+\frac{\epsilon}{N}\bigg{(}\sum_{j=1}^{N} \nabla_{\mathbf{x}_{j}^{(t)}}K\big{(}\mathbf{x}_{i}^{(t)},\mathbf{x}_{j}^{(t)}\big{)}- \sum_{j=1}^{N}K\big{(}\mathbf{x}_{i}^{(t)},\mathbf{x}_{j}^{(t)}\big{)}\widehat{\nabla V }\big{(}\mathbf{x}_{j}^{(t)}\big{)}\bigg{)}. \tag{45}\]

where \(\widehat{\nabla V}(\mathbf{x})=\widehat{\Gamma}_{t}(\mathbf{x}-\mathbf{\mu}_{t})+\widehat {\mathbf{m}}_{t}\).

The updating rules of Gaussian-SVGD given above is totally deterministic, meaning that at each time step \(\mathbf{x}_{i}^{(t)}\) is updated only using deterministic quantities and \(\big{(}\mathbf{x}_{k}^{(t)}\big{)}_{k=1}^{N}\) without any external randomness. This is computationally more efficient but imposes difficulty in the analysis. On the other hand, we could also consider slightly modifying the updating rules by applying a resampling scheme to get a better estimation of \(\mathbf{m}_{t}\) and \(\Gamma_{t}\). In other words, we resample \((\mathbf{y}_{k})_{k=1}^{M}\)_i.i.d._ from \(\mathcal{N}(\mathbf{\mu}_{t},\Sigma_{t})\) and use the following estimators

\[\widehat{\mathbf{m}}_{t}=\frac{1}{M}\sum_{k=1}^{M}\nabla V(\mathbf{y}_{k}^{(t)}),\quad \widehat{\Gamma}_{t}=\frac{1}{M}\sum_{k=1}^{M}\nabla^{2}V(\mathbf{y}_{k}^{(t)}), \tag{46}\]

or first-order estimators

\[\widehat{\Gamma}_{t}=\frac{1}{M}\sum_{k=1}^{M}\nabla V(\mathbf{y}_{k}^{(t)})(\mathbf{ y}_{k}^{(t)}-\mathbf{\mu}_{t})^{\top}\Sigma_{t}^{-1}=\frac{1}{M}\sum_{k=1}^{M} \Sigma_{t}^{-1}(\mathbf{y}_{k}^{(t)}-\mathbf{\mu}_{t})\nabla V(\mathbf{y}_{k}^{(t)})^{\top}. \tag{47}\]

In this way, when \(M\) is large enough \(\widehat{\nabla V}(\mathbf{x})\) will be sufficiently close to \(\Gamma_{t}(\mathbf{x}-\mathbf{\mu}_{t})+\mathbf{m}_{t}\).

Now we replace \(\widehat{\mathbf{m}}_{t}\) and \(\widehat{\Gamma}_{t}\) by \(\mathbf{m}_{t}\) and \(\Gamma_{t}\) and consider the continuous-time dynamics

\[\dot{\mathbf{x}}_{i}^{(t)}=\frac{1}{N}\bigg{(}\sum_{j=1}^{N}\nabla_{\mathbf{x}_{j}^{(t)} }K\big{(}\mathbf{x}_{i}^{(t)},\mathbf{x}_{j}^{(t)}\big{)}-\sum_{j=1}^{N}K\big{(}\mathbf{x }_{i}^{(t)},\mathbf{x}_{j}^{(t)}\big{)}\big{(}\Gamma_{t}(\mathbf{x}_{j}^{(t)}-\mathbf{\mu} _{t})+\mathbf{m}_{t}\big{)}\bigg{)}, \tag{48}\]

where \(\mathbf{\mu}_{t}\) and \(\Sigma_{t}\) are sample mean and variance and

\[\mathbf{m}_{t}:=\mathbb{E}_{\mathbf{x}\sim\mathcal{N}(\mathbf{\mu}_{t},\Sigma_{t})}[\nabla V (\mathbf{x})],\quad\Gamma_{t}:=\mathbb{E}_{\mathbf{x}\sim\mathcal{N}(\mathbf{\mu}_{t}, \Sigma_{t})}[\nabla^{2}V(\mathbf{x})].\]

**Theorem D.1** (Equivalence of density-based and particle-based algorithms).: _The solution of the finite-particle system (48) with \(K_{1}\) is given by \(\mathbf{x}_{i}^{(t)}=A_{t}(\mathbf{x}_{i}^{(0)}-\mathbf{\mu}_{0})+\mathbf{\mu}_{t}\) where \(A_{t}\) is the unique solution of_

\[\dot{A}_{t}=(I-\Gamma_{t}C_{t}-\mathbf{m}_{t}\mathbf{\mu}_{t}^{\top})A_{t},\quad A_{0 }=I,\]

_where \(\mathbf{\mu}_{t}\) and \(\Sigma_{t}\) are the unique solution of the ODE system_

\[\begin{cases}\dot{\mathbf{\mu}}_{t}=(I-\Gamma_{t}\Sigma_{t})\,\mathbf{\mu}_{t}-(1+\mathbf{ \mu}_{t}^{\top}\mathbf{\mu}_{t})\mathbf{m}_{t}\\ \dot{\Sigma}_{t}=2\Sigma_{t}-\Sigma_{t}\left(\Sigma_{t}\Gamma_{t}+\mathbf{\mu}_{t} \mathbf{m}_{t}^{\top}\right)-\left(\Gamma_{t}\Sigma_{t}+\mathbf{m}_{t}\mathbf{\mu}_{t}^{ \top}\right)\Sigma_{t}\end{cases}. \tag{49}\]

This can be proved using exactly the same technique as in the proof of Theorem 3.6. Here (49) is the same as (22), and hence by Theorem 4.1\(\mathbf{\mu}_{t}\) and \(\Sigma_{t}\) converges to \(\mathbf{\mu}^{*}\) and \(\Sigma^{*}\) that solves the GVI and the convergence rate is given in Theorem 4.2 when the target is strongly log-concave. We also conjecture that there is still uniform in time convergence to the mean-field limit for this particle system and leave it to future works..

## Appendix E Details of the Simulations

**Gaussian Targets.** Following [19], we consider a scenario where the target is Gaussian \(\mathcal{N}(\mathbf{\mu},\Sigma)\) where \(\mathbf{\mu}\sim\mathrm{Unif}([0,1]^{10})\) and \(\Sigma^{-1}=U\operatorname{diag}\{\lambda_{1},\lambda_{2},\cdots,\lambda_{10 }\}U^{\top}\) with \(U\in\mathbb{R}^{10\times 10}\) drawn from the Haar measure of the orthogonal matrices \(O(10)\) and \(\lambda_{1},\cdots,\lambda_{10}\) being a geometric sequence such that \(\lambda_{1}=0.01\) and \(\lambda_{10}=1\). We run the eight different algorithms as introduced in Section 5 and show the decay of \(\log\mathrm{KL}(\rho_{\theta}\parallel\rho^{*})\) over time in Figure 2. Clearly the algorithms with \(K_{1}\) show a faster rate over time compared to the others while the other algorithms eventually all converge at the same rate. This is actually confirmed from our theoretical analysis as in all the other dynamics except SBGD and SBPF, the mean converges at the rate of \(\mathcal{O}(e^{-t/\lambda})\) while the covariance converges at a faster rate, resulting in the fact that the KL divergence converges at \(\mathcal{O}(e^{-2t/\lambda})\). But for \(K_{1}\) the rate is different and given in Theorem 3.1.

**Gaussian Mixture Targets.** Next we consider the \(1\)-dimensional Gaussian mixture targets given by \(w_{1}\mathcal{N}(\mu_{1},\sigma_{1}^{2})+(1-w_{1})\mathcal{N}(\mu_{2},\sigma_{ 2}^{2})\). We run the aforementioned eight algorithms with initial

Figure 2: Convergence of Algorithms 1 and 2 with bilinear kernels for a Gaussian target.

\(\mu=0\) and \(\sigma=1\) or particles drawn from \(\mathcal{N}(0,1)\). Here again we plot the decay of \(\widehat{\mathcal{F}}(\rho_{\theta})\) over time or iteration as shown in Figure 3. In particular, the plots correspond to the specific setting of \(\mu_{1}=5,\mu_{2}=10,\sigma_{1}=5,\sigma_{2}=2\) and \(\rho^{*}(x)\propto 0.3\exp(-(x-5)^{2}/50)+0.7\exp(-(x-10)^{2}/8)\). These parameters are arbitrarily chosen. For the decay of \(\widehat{\mathcal{F}}(\rho_{\theta})\) over time, we fix the step size to be \(0.1\) and run \(1000\) iterations. For \(\widehat{\mathcal{F}}(\rho_{\theta})\) over time, we draw \(500\) particles for particle-based algorithms and run all algorithms for \(500\) iterations so that a total of \(500\) samples are drawn for the density-based algorithms. The step sizes are chosen to be the largest ones that still allow convergence. Specifically for these eight algorithms the step sizes are \(0.02,0.1,1,1,0.2,0.8,8,8\). Consistent with the results of Bayesian logistic regression, the particle-based ones are more stable and allow larger step sizes, with BWP and RGPF clearly outperforming all the others. In fact, the constrast between particle-based and density-based algorithms is particularly significant in this problem probably because of the non-log-concave target.

**More on the Bayesian Logistic Regression.** We compare three so-far best performed algorithms, BWP, RGPF, and FB-GVI [19] for the same problem with different step sizes. From Figure 4 we see that BWP outperforms the other two. FB-GVI is better than RGPF with a larger learning rate but fluctuate a bit more when \(\eta=2\). This is probably attributed to the stochastic gradients. Furthermore, it is interesting to compare to ordinary gradient descent (OGD) on the variational parameters (mean and covariance) and SVGD with a radius-based kernel function (RBF-SVGD) \(K_{h}(\mathbf{x},\mathbf{y})=\exp(-\frac{\|\mathbf{x}-\mathbf{y}\|^{2}}{2h^{2}})\). We show this comparison in Figure 5 with the same step size \(\eta=2\). Firstly, OGD does not converge as fast as BWPF and it is not as stable. Secondly, RBF-SVGD is quite sensitive to the choice of bandwidth and it does not converge as fast as BWPF in general. We also notice that RBF-SVGD is significantly slower in computation compared to Gaussian-SVGD. However, as a particle-based algorithm, RBF-SVGD does have the advantage of being stable even when the step size is large.

## Appendix F Analogous Results for the Affine-Invariant Bilinear Kernels

For the Bures-Wasserstein metric (Gaussian-Stein metric with \(K_{3}\)), the dynamics of natural gradient descent has already been studied in literature. See [60, 86, 13] for proofs for the following theorem.

**Theorem F.1** (Wasserstein gradient flow for the Gaussian family).: _Let \(\rho_{0}\sim\mathcal{N}(\mathbf{\mu}_{0},\Sigma_{0})\) and \(\rho^{*}\sim\mathcal{N}(\mathbf{b},Q)\) be two Gaussian measures. Then the solution of (1) converges to \(\rho^{*}\) as \(t\to\infty\). In particular, \(\rho_{t}\) is the density of \(\mathcal{N}(\mathbf{\mu}_{t},\Sigma_{t})\) where the mean \(\mathbf{\mu}_{t}\) and covariance matrix \(\Sigma_{t}\) satisfies_

\[\begin{cases}\dot{\mathbf{\mu}}_{t}=-Q^{-1}(\mathbf{\mu}_{t}-\mathbf{b})\\ \dot{\Sigma}_{t}=2I-\Sigma_{t}Q^{-1}-Q^{-1}\Sigma_{t}\end{cases}. \tag{50}\]

_If \(\Sigma_{0}Q=Q\Sigma_{0}\), we have \(\|\mathbf{\mu}_{t}-\mathbf{b}\|{=\mathcal{O}(e^{-t/\lambda})}\) and \(\|\Sigma_{t}-Q\|=\mathcal{O}(e^{-2t/\lambda})\), where \(\lambda\) is the largest eigenvalue of \(Q\)._

Now we focus on the results for Gaussian-SVGD with the kernel \(K_{2}\). First we remark that for \(K_{2}\) the previous results on the well-posedness of mean-field PDE and finite-particle solutions in Appendix C still hold and can be proved using similar techniques to Appendix H but for simplicity they are

Figure 3: Convergence of Algorithms 1 and 2 with bilinear kernels for a Gaussian mixture target.

omitted. The proofs of the following theorems will also be omitted unless a different proof technique from the analogous results needs to be applied.

**Theorem F.2** (Analogue of Theorem 3.1).: _For any \(t\geq 0\) the solution \(\rho_{t}\) of SVGD (2) with the bilinear kernel remains a Gaussian density with mean \(\boldsymbol{\mu}_{t}\) and covariance matrix \(\Sigma_{t}\) given by_

\[\begin{cases}\dot{\boldsymbol{\mu}}_{t}=-Q^{-1}(\boldsymbol{\mu}_{t}- \boldsymbol{b})\\ \dot{\Sigma}_{t}=2\Sigma_{t}-\Sigma_{t}^{2}Q^{-1}-Q^{-1}\Sigma_{t}^{2}\end{cases}, \tag{51}\]

_which has a unique global solution on \([0,\infty)\) given any \(\boldsymbol{\mu}_{0}\in\mathbb{R}^{d}\) and \(\Sigma_{0}\in\mathrm{Sym}^{+}(d,\mathbb{R})\). And \(\rho_{t}\) converges weakly to \(\rho^{*}\) as \(t\to\infty\). If \(\Sigma_{0}Q=Q\Sigma_{0}\) then we have the following rates_

\[\|\boldsymbol{\mu}_{t}-\boldsymbol{b}\|=\mathcal{O}(e^{-t/\lambda}),\quad\| \Sigma_{t}-Q\|=\mathcal{O}(e^{-2t}),\quad\forall\epsilon>0,\]

_where \(\lambda\) is the largest eigenvalue of \(Q\)._

The proof of the dynamics is similar to that of Theorem 3.1. The rate \(\mathcal{O}(e^{-t/\lambda})\) is trivially true from the theory of linear ODEs and the rate \(\mathcal{O}(e^{-2t})\) is given by Theorem 3.2.

**Theorem F.3** (Analogue of Theorem 3.6).: _Suppose the initial particles satisfy that \(C_{0}\) is non-singular. There exists a unique solution of the finite particle system (13) given by_

\[\boldsymbol{x}_{i}^{(t)}=A_{t}(\boldsymbol{x}_{i}^{(0)}-\boldsymbol{\mu}_{0})+ \boldsymbol{\mu}_{t}, \tag{52}\]

Figure 4: Performance of BWPF, RGPF, and FB-GVI with different step sizes \(\eta\).

Figure 5: Performance of BWPF, OGD, and RBF-SVGD with bandwidth \(h\).

_where \(A_{t}\) is the unique (matrix) solution of the linear system_

\[\dot{A}_{t}=\big{(}I-Q^{-1}C_{t}\big{)}\,A_{t},\quad A_{0}=I, \tag{53}\]

_and \(\mathbf{\mu}_{t}\) and \(C_{t}\) are the unique solution of the ODE system_

\[\begin{cases}\dot{\mathbf{\mu}}_{t}=-Q^{-1}(\mathbf{\mu}_{t}-\mathbf{b})\\ \dot{C}_{t}=2C_{t}-C_{t}^{2}Q^{-1}-Q^{-1}C_{t}^{2}\end{cases}. \tag{54}\]

The proof is similar to that of Theorem 3.6.

**Theorem F.4** (Analogue of Theorem 3.7).: _Given the same setting as Theorem F.3, further suppose the initial particles are drawn i.i.d. from \(\mathcal{N}(\mathbf{\mu}_{0},\Sigma_{0})\). Then there exists a constant \(C_{d,Q,\mathbf{b},\Sigma_{0},\mathbf{\mu}_{0}}\) such that for all \(t\), for all \(N\geq 2\), with the empirical measure \(\zeta_{N}^{(t)}=\frac{1}{N}\sum_{i=1}^{N}\delta_{\mathbf{x}_{i}^{(t)}}\), the second moment of Wasserstein-\(2\) distance between \(\zeta_{N}^{(t)}\) and \(\rho_{t}\) converges:_

\[\mathbb{E}\left[\mathcal{W}_{2}^{2}\left(\zeta_{N}^{(t)},\rho_{t}\right)\right] \leq C_{d,Q,\mathbf{b},\Sigma_{0},\mathbf{\mu}_{0}}\times\left\{\begin{array}{cl}N^ {-1}\log\log N&\text{if }d=1\\ N^{-1}(\log N)^{2}&\text{if }d=2\\ N^{-2/d}&\text{if }d\geq 3\end{array}\right.. \tag{55}\]

The proof is similar to that of Theorem 3.7. But for sake of completeness we provide more proof details in Appendix L.

**Theorem F.5** (Analogue of Theorem 4.1).: _Let \(\rho^{*}\) be the density of a target distribution with the potential function \(V(\mathbf{x})=-\log\rho^{*}(\mathbf{x})\) and \(\rho_{0}\) be the density of \(\mathcal{N}(\mathbf{\mu}_{0},\Sigma_{0})\). Then for any \(t\geq 0\) the Gaussian-SVGD produces a Gaussian density \(\rho_{t}\) with mean \(\mathbf{\mu}_{t}\) and covariance matrix \(\Sigma_{t}\) given by the following ODE system:_

\[\begin{cases}\dot{\mathbf{\mu}}_{t}=-\mathbb{E}_{\mathbf{x}\sim\rho_{t}}[\nabla V(\mathbf{x })]\\ \dot{\Sigma}_{t}=2\Sigma_{t}-\Sigma_{t}^{2}\mathbb{E}_{\mathbf{x}\sim\rho_{t}}[ \nabla^{2}V(\mathbf{x})]-\mathbb{E}_{\mathbf{x}\sim\rho_{t}}[\nabla^{2}V(\mathbf{x})] \Sigma_{t}^{2}\end{cases}. \tag{56}\]

_Furthermore, suppose that \(\theta^{*}\) is the unique solution of the following optimization problem_

\[\min_{\theta=(\mathbf{\mu},\Sigma)}\mathrm{KL}(\rho_{\theta}\parallel\rho^{*}), \text{ where }\rho_{\theta}\text{ is the Gaussian measure }\mathcal{N}(\mathbf{\mu},\Sigma).\]

_Then we have \(\rho_{t}\to\rho_{\theta^{*}}\sim\mathcal{N}(\mathbf{\mu}^{*},\Sigma^{*})\) as \(t\to\infty\)._

The proof is similar to that of Theorem 4.1. In particular, if the target is strongly log-concave, it gives rise to the following linear convergence rate.

**Theorem F.6** (Analogue of Theorem 4.2).: _Assume that the target \(\rho^{*}\) is \(\alpha\)-strongly log-concave and \(\beta\)-log-smooth, i.e., \(\alpha I\preceq\nabla^{2}V(\mathbf{x})\preceq\beta I\). Then \(\rho_{t}\) converges to \(\rho_{\theta^{*}}\) at the following rate:_

\[\|\mathbf{\mu}_{t}-\mathbf{\mu}^{*}\|=\mathcal{O}(e^{-\alpha t/\max\{\beta,1\}}), \quad\|\Sigma_{t}-\Sigma^{*}\|=\mathcal{O}(e^{-\alpha t/\max\{\beta,1\}}).\]

The proof is similar to that of Theorem 4.2.

**Theorem F.7** (Analogue of Theorem D.1).: _The solution of the finite-particle system (48) with \(K_{2}\) is given by \(\mathbf{x}_{i}^{(t)}=A_{t}(\mathbf{x}_{i}^{(0)}-\mathbf{\mu}_{0})+\mathbf{\mu}_{t}\) where \(A_{t}\) is the unique solution of_

\[\dot{A}_{t}=(I-\Gamma_{t}C_{t})A_{t},\quad A_{0}=I,\]

_where \(\mathbf{\mu}_{t}\) and \(\Sigma_{t}\) are the unique solution of the ODE system_

\[\begin{cases}\dot{\mathbf{\mu}}_{t}=-\mathbf{m}_{t}\\ \dot{\Sigma}_{t}=2\Sigma_{t}-\Sigma_{t}^{2}\Gamma_{t}-\Gamma_{t}\Sigma_{t}^{2 }\end{cases}.\]

The proof is similar to that of Theorem 3.6 or Theorem D.1.

Proofs for Section B

**Lemma G.1**.: _Let \(\rho(\mathbf{x})=(2\pi)^{-d/2}\big{(}\det(\Sigma)\big{)}^{-1/2}\exp\left(-\frac{1}{2} \mathbf{x}^{\top}\Sigma^{-1}\mathbf{x}\right)\) be the density of a \(d\)-dimensional normal random vector where \(\Sigma\) is a positive definite matrix. Then for any \(d\times d\) real matrix \(A\) and \(d\)-dimensional real vector \(\mathbf{b}\), we have_

\[\int\mathbf{x}^{\top}A\mathbf{x}\rho(\mathbf{x})\,\mathrm{d}\mathbf{x}=\operatorname{tr}(A \Sigma),\quad\int\mathbf{b}^{\top}\mathbf{x}A\mathbf{x}\rho(\mathbf{x})\,\mathrm{d}\mathbf{x}=A \Sigma\mathbf{b}.\]

Proof.: Since \(\Sigma^{-1}\) is a positive definite matrix, we can find its positive definite root \(\Sigma^{-1/2}\). Let \(\mathbf{y}=\Sigma^{-1/2}\mathbf{x}\) and \(\rho_{0}(\mathbf{x})=(2\pi)^{-d/2}\exp\left(-\frac{1}{2}\mathbf{x}^{\top}\mathbf{x}\right)\). Then we have

\[\int\mathbf{x}^{\top}A\mathbf{x}\rho(\mathbf{x})\,\mathrm{d}\mathbf{x}\] \[= \int\mathbf{y}^{\top}\Sigma^{1/2}A\Sigma^{1/2}\mathbf{y}\left(\det(\Sigma )\right)^{-1/2}\rho_{0}(\mathbf{y})\left(\det(\Sigma)\right)^{1/2}\mathrm{d}\mathbf{y}\] \[= \int\left(\sum_{j=1}^{d}(\Sigma^{1/2}A\Sigma^{1/2})_{jj}y_{j}^{2 }\right)\rho_{0}(\mathbf{y})\,\mathrm{d}\mathbf{y}\] \[= \sum_{j=1}^{d}(\Sigma^{1/2}A\Sigma^{1/2})_{jj}\] \[= \operatorname{tr}(\Sigma^{1/2}A\Sigma^{1/2})=\operatorname{tr}(A \Sigma).\]

The second equation is given by

\[\int\mathbf{b}^{\top}\mathbf{x}A\mathbf{x}\rho(\mathbf{x})\,\mathrm{d}\mathbf{x}=\int A \mathbf{x}\mathbf{x}^{\top}\mathbf{b}\rho(\mathbf{x})\,\mathrm{d}\mathbf{x}\] \[= A\left(\int\mathbf{x}\mathbf{x}^{\top}\rho(\mathbf{x})\,\mathrm{d}\mathbf{x} \right)\mathbf{b}=A\Sigma\mathbf{b}.\]

**Lemma G.2** (Lyapunov equation).: _The Lyapunov equation_

\[PX+XP=Q\]

_has a unique solution. If \(P,Q\in\operatorname{Sym}(d,\mathbb{R})\), then the solution \(X\in\operatorname{Sym}(d,\mathbb{R})\)._

Proof.: By Sylvester-Rosenblum theorem in control theory [7], \(PX+XP=Q\) has a unique solution \(X\). Note that if \(P,Q\in\operatorname{Sym}(d,\mathbb{R})\) and \(X\) is a solution, then \(X^{\top}\) is also a solution. Thus, we have \(X=X^{\top}\) which implies that \(X\in\operatorname{Sym}(d,\mathbb{R})\). 

Proof of Theorem b.3.: Note that the tangent space at each \(\theta\in\Theta=\mathbb{R}^{d}\times\operatorname{Sym}^{+}(d,\mathbb{R})\) is \(T_{\theta}\Theta\simeq\mathbb{R}^{d}\times\operatorname{Sym}(d,\mathbb{R})\). If we define the inner product (pairing) on \(T_{\theta}\Theta\) by

\[\langle\xi,\eta\rangle:=\operatorname{tr}(\Sigma_{1}\Sigma_{2})+\mathbf{\mu}_{1}^ {\top}\mathbf{\mu}_{2}, \tag{57}\]

for any \(\xi,\eta\in T_{\theta}\Theta\), where \(\xi=\big{(}\widetilde{\mathbf{\mu}}_{1},\widetilde{\Sigma}_{1}\big{)}\) and \(\eta=\big{(}\widetilde{\mathbf{\mu}}_{2},\widetilde{\Sigma}_{2}\big{)}\), then the tangent bundle \(T\Theta\) is trivial. Since

\[\begin{array}{cccc}\phi:&\Theta&\to&\mathcal{P}(\mathbb{R}^{d})\\ &\theta&\mapsto&\rho(\cdot,\theta)\end{array}\]

provides an immersion from \(\Theta\) to \(\mathcal{P}(\mathbb{R}^{d})\), we consider its pushforward \(\mathrm{d}\phi_{\theta}\) given by

\[\begin{array}{cccc}\mathrm{d}\phi_{\theta}:&T_{\theta}\Theta&\to&T_{\rho} \mathcal{P}(\mathbb{R}^{d})\\ &\xi&\mapsto&\langle\nabla_{\theta}\rho(\cdot,\theta),\xi\rangle.\end{array}\]On the other hand, for any \(\Phi\in T^{*}_{\rho}\mathcal{P}(\mathbb{R}^{d})\), the inverse canonical isomorphism of Stein metric maps it to

\[G_{\rho}^{-1}\Phi=-\nabla\cdot\rho(\cdot,\theta)\int K(\cdot,\mathbf{y})\rho(\mathbf{y},\theta)\nabla\Phi(\mathbf{y})\,\mathrm{d}\mathbf{y}\in T_{\rho}\mathcal{P}(\mathbb{R}^ {d}).\]

Thus, we obtain that

\[\langle\nabla_{\theta}\rho(\mathbf{x},\theta),\xi\rangle=-\nabla_{\mathbf{x}}\cdot\rho( \mathbf{x},\theta)\int K(\mathbf{x},\mathbf{y})\rho(\mathbf{y},\theta)\nabla\Phi(\mathbf{y})\, \mathrm{d}\mathbf{y} \tag{58}\]

for any \(\mathbf{x}\in\mathbb{R}^{d}\).

Now we try to find a suitable function \(\nabla\Phi_{\xi}\) that satisfies the equation above. We compute that

\[\langle\nabla_{\theta}\rho(\mathbf{x},\theta),\xi\rangle\] \[= \operatorname{tr}\bigl{(}\nabla_{\Sigma}\rho(\mathbf{x},\theta) \widetilde{\Sigma}_{1}\bigr{)}+\nabla_{\mathbf{\mu}}\rho(\mathbf{x},\theta)^{\top} \widetilde{\mathbf{\mu}}_{1}\] \[= \biggl{(}-\frac{1}{2}\left(\operatorname{tr}\left(\Sigma^{-1} \widetilde{\Sigma}_{1}\right)-(\mathbf{x}-\mathbf{\mu})^{T}\Sigma^{-1}\widetilde{ \Sigma}_{1}\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})\right)+\widetilde{\mathbf{\mu}}_{1}^{\top }\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})\biggr{)}\rho(\mathbf{x},\theta).\]

Letting \(\Psi(\mathbf{x})=\int K(\mathbf{x},\mathbf{y})\rho(\mathbf{y},\theta)\nabla\Phi(\mathbf{y})\, \mathrm{d}\mathbf{y}\), we get

\[-\nabla_{\mathbf{x}}\cdot\rho(\mathbf{x},\theta)\int K(\mathbf{x},\mathbf{y})\rho( \mathbf{y},\theta)\nabla\Phi(\mathbf{y})\,\mathrm{d}\mathbf{y}\] \[= -\nabla_{\mathbf{x}}\cdot\rho(\mathbf{x},\theta)\Psi(\mathbf{x})\] \[= \Bigl{(}\Psi(\mathbf{x})^{\top}\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})-\nabla \cdot\Psi(\mathbf{x})\Bigr{)}\rho(\mathbf{x},\theta).\]

We choose \(\nabla\Phi_{\xi}(\mathbf{x})=S_{1}(\mathbf{x}-\mathbf{\mu})+\mathbf{b}_{1}\), where \(S_{1}\in\mathrm{Sym}(d,\mathbb{R})\) and \(\mathbf{b}_{1}\in\mathbb{R}^{d}\) will be determined later. Note that \(S_{1}\) needs to be symmetric because the gradient field is curl-free. We derive that

\[\Psi(\mathbf{x})= \int(\mathbf{x}^{\top}\mathbf{y}+1)\rho(\mathbf{y},\theta)\nabla\Phi_{\xi}( \mathbf{y})\,\mathrm{d}\mathbf{y}\] \[= \int(\mathbf{x}^{\top}\mathbf{y}+1)\rho(\mathbf{y},\theta)(S_{1}(\mathbf{y}-\mathbf{ \mu})+\mathbf{b}_{1})\,\mathrm{d}\mathbf{y}\] \[= \int\bigl{(}\mathbf{x}^{\top}(\mathbf{y}+\mathbf{\mu})+1\bigr{)}\rho(\mathbf{y}+ \mathbf{\mu},\theta)\bigl{(}S_{1}\mathbf{y}+\mathbf{b}_{1}\bigr{)}\,\mathrm{d}\mathbf{y}\] \[= S_{1}\Sigma\mathbf{x}+(\mathbf{x}^{\top}\mathbf{\mu}+1)\mathbf{b}_{1}\] \[= (S_{1}\Sigma+\mathbf{b}_{1}\mathbf{\mu}^{\top})\mathbf{x}+\mathbf{b}_{1}.\]

By comparison of the coefficients, we need

\[\begin{cases}(S_{1}\Sigma+\mathbf{b}_{1}\mathbf{\mu}^{\top})^{\top}\Sigma^{-1}+\Sigma^ {-1}(S_{1}\Sigma+\mathbf{b}_{1}\mathbf{\mu}^{\top})=\Sigma^{-1}\widetilde{\Sigma}_{1 }\Sigma^{-1},\\ \operatorname{tr}(S_{1}\Sigma+\mathbf{b}_{1}\mathbf{\mu}^{\top})=\frac{1}{2} \operatorname{tr}(\Sigma^{-1}\widetilde{\Sigma}_{1}),\\ S_{1}\Sigma\mathbf{\mu}+(1+\mathbf{\mu}^{\top}\mathbf{\mu})\mathbf{b}_{1}=\widetilde{\mathbf{\mu}} _{1}.\end{cases} \tag{59}\]

Note that the Lyapunov equation

\[PX+XP=Q,\]

where

\[P =\Sigma\left(I-\frac{1}{1+\mathbf{\mu}^{\top}\mathbf{\mu}}\mathbf{\mu}\mathbf{\mu}^ {\top}\right)\Sigma,\] \[Q =\widetilde{\Sigma}_{1}-\frac{1}{1+\mathbf{\mu}^{\top}\mathbf{\mu}}( \Sigma\mathbf{\mu}\widetilde{\mathbf{\mu}}_{1}^{\top}+\widetilde{\mathbf{\mu}}_{1}\mathbf{\mu} ^{\top}\Sigma),\]

has a unique solution \(X=S_{1}\in\mathrm{Sym}(d,\mathbb{R})\). Together with

\[\mathbf{b}_{1}=\frac{1}{1+\mathbf{\mu}^{\top}\mathbf{\mu}}(\widetilde{\mathbf{\mu}}_{1}-S_{1} \Sigma\mathbf{\mu}),\]we find that (59) holds with the unique solution described above. Now from the calculations above it is straightforward to check that (59) is equivalent to the following equation

\[G_{\theta}^{-1}\Big{(}\mathbf{b}_{1},\frac{1}{2}S_{1}\Big{)}=\left(\widetilde{\mathbf{ \mu}}_{1},\widetilde{\Sigma}_{1}\right),\]

where \(G_{\theta}^{-1}\) is the map defined in (29). Thus, the existence and uniqueness of the solution indicates that \(G_{\theta}\) is an isomorphism.

Similarly, we let

\[\nabla\Phi_{\eta}(\mathbf{x})=S_{2}(\mathbf{x}-\mathbf{\mu})+\mathbf{b}_{2},\]

where \(X=S_{2}\in\mathrm{Sym}(d,\mathbb{R})\) is the unique solution of the Lyapunov equation

\[PX+XP=Q,\]

where

\[P=\Sigma\left(I-\frac{1}{1+\mathbf{\mu}^{\top}\mathbf{\mu}}\mathbf{\mu}\mathbf{\mu}^{\top} \right)\Sigma,\]

\[Q=\widetilde{\Sigma}_{2}-\frac{1}{1+\mathbf{\mu}^{\top}\mathbf{\mu}}(\Sigma\mathbf{\mu} \widetilde{\mathbf{\mu}}_{2}^{\top}+\widetilde{\mathbf{\mu}}_{2}\mathbf{\mu}^{\top}\Sigma),\]

and

\[\mathbf{b}_{2}=\frac{1}{1+\mathbf{\mu}^{\top}\mathbf{\mu}}(\widetilde{\mathbf{\mu}}_{2}-S_{2} \Sigma\mathbf{\mu}).\]

Now we compute the Riemannian tensor

\[g_{\theta}(\xi,\eta)=g_{\rho}(\xi,\eta)=\int\Phi_{\xi}(\mathbf{x})G_{ \rho}^{-1}\Phi_{\eta}(\mathbf{x})\,\mathrm{d}\mathbf{x}\] \[= \int(\nabla\Phi_{\xi}(\mathbf{x}))^{\top}\rho(\mathbf{x},\theta)\int(\mathbf{ x}^{\top}\mathbf{y}+1)\rho(\mathbf{y},\theta)\nabla\Phi_{\eta}(\mathbf{y})\,\mathrm{d}\mathbf{y} \,\mathrm{d}\mathbf{x}\] \[= \int\rho(\mathbf{x},\theta)\left(S_{1}(\mathbf{x}-\mathbf{\mu})+\mathbf{b}_{1} \right)^{\top}\left((S_{2}\Sigma+\mathbf{b}_{2}\mathbf{\mu}^{\top})\mathbf{x}+\mathbf{b}_{2} \right)\mathrm{d}\mathbf{x}\] \[= \operatorname{tr}\left(S_{1}(S_{2}\Sigma+\mathbf{b}_{2}\mathbf{\mu}^{ \top})\Sigma\right)+\mathbf{b}_{1}^{\top}(S_{2}\Sigma\mathbf{\mu}+(1+\mathbf{\mu}^{\top} \mathbf{\mu})\mathbf{b}_{2})\] \[= \operatorname{tr}(S_{1}S_{2}\Sigma^{2})+(\mathbf{b}_{1}^{\top}S_{2}+ \mathbf{b}_{2}^{\top}S_{1})\Sigma\mathbf{\mu}+(1+\mathbf{\mu}^{\top}\mathbf{\mu})\mathbf{b}_{1}^{ \top}\mathbf{b}_{2}.\]

Finally, we show that \(G_{\theta}\) is indeed the canonical isomorphism corresponding to \(g_{\theta}(\cdot,\cdot)\). We check that

\[g_{\theta}(\xi,\eta)= \operatorname{tr}\left(S_{1}(S_{2}\Sigma+\mathbf{b}_{2}\mathbf{\mu}^{\top })\Sigma\right)+\mathbf{b}_{1}^{\top}(S_{2}\Sigma\mathbf{\mu}+(1+\mathbf{\mu}^{\top}\mathbf{ \mu})\mathbf{b}_{2})\] \[= \frac{1}{2}\operatorname{tr}(S_{1}\widetilde{\Sigma}_{2})+\mathbf{b} _{1}^{\top}\widetilde{\mathbf{\mu}}_{2}=\langle G_{\theta}(\xi),\eta\rangle.\]

Proof of Theorem b.7.: Similar to the proof of Theorem B.3, we define the inner product on \(T_{\theta}\Theta\) by

\[\langle\xi,\eta\rangle:=\operatorname{tr}(\Sigma_{1}\Sigma_{2})+\mathbf{\mu}_{1} ^{\top}\mathbf{\mu}_{2},\]

for any \(\xi,\eta\in T_{\theta}\Theta\), where \(\xi=\left(\widetilde{\mathbf{\mu}}_{1},\widetilde{\Sigma}_{1}\right)\) and \(\eta=\left(\widetilde{\mathbf{\mu}}_{2},\widetilde{\Sigma}_{2}\right)\). The map

\[\begin{array}{ccc}\phi:&\Theta\quad\rightarrow&\mathcal{P}(\mathbb{R}^{d}) \\ \theta&\mapsto\quad\rho(\cdot,\theta),\end{array}\]

where \(\rho(\cdot,\Sigma)\) denotes the density of \(\mathcal{N}(\mathbf{\mu},\Sigma)\), provides an immersion from \(\Theta\) to \(\mathcal{P}(\mathbb{R}^{d})\). We consider its pushforward \(\mathrm{d}\phi_{\theta}\) given by

\[\begin{array}{ccc}\mathrm{d}\phi_{\theta}:&T_{\theta}\Theta\quad\to &T_{\rho}\mathcal{P}(\mathbb{R}^{d})\\ \xi&\mapsto\quad\langle\nabla_{\theta}\rho(\cdot,\theta),\xi\rangle.\end{array}\]

On the other hand, for any \(\Phi\in T_{\rho}^{*}\mathcal{P}(\mathbb{R}^{d})\), the inverse canonical isomorphism of regularized Stein metric maps it to

\[G_{\rho}^{-1}\Phi=-\nabla\cdot\left(\rho((1-\nu)\mathcal{T}_{K,\rho}+\nu I)^{-1} \mathcal{T}_{K,\rho}\left(\nabla\Phi\right)\right)\in T_{\rho}\mathcal{P}( \mathbb{R}^{d}).\]Thus, we obtain that

\[\langle\nabla_{\theta}\rho(\mathbf{x},\theta),\xi\rangle=-\nabla_{\mathbf{x}}\cdot\big{(} \rho(\mathbf{x},\theta)((1-\nu)\mathcal{T}_{K,\rho}+\nu I)^{-1}\mathcal{T}_{K,\rho} \left(\nabla\Phi(\mathbf{x})\right)\big{)} \tag{60}\]

for any \(\mathbf{x}\in\mathbb{R}^{d}\).

Now we try to find a suitable function \(\nabla\Phi_{\xi}\) that satisfies the equation above. We compute that

\[\langle\nabla_{\theta}\rho(\mathbf{x},\theta),\xi\rangle\] \[= \operatorname{tr}\bigl{(}\nabla_{\Sigma}\rho(\mathbf{x},\theta)\widetilde {\Sigma}_{1}\bigr{)}+\nabla_{\mathbf{\mu}}\rho(\mathbf{x},\theta)^{\top}\widetilde{\bm {\mu}}_{1}\] \[= \biggl{(}-\frac{1}{2}\left(\operatorname{tr}\left(\Sigma^{-1} \widetilde{\Sigma}_{1}\right)-(\mathbf{x}-\mathbf{\mu})^{T}\Sigma^{-1}\widetilde{\Sigma }_{1}\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})\right)+\widetilde{\mathbf{\mu}}_{1}^{\top}\Sigma ^{-1}(\mathbf{x}-\mathbf{\mu})\biggr{)}\rho(\mathbf{x},\theta).\]

Letting \(\Psi(\mathbf{x})=((1-\nu)\mathcal{T}_{K,\rho}+\nu I)^{-1}\mathcal{T}_{K,\rho}\left( \nabla\Phi(\mathbf{x})\right)\), we get that the RHS of (60) is equal to

\[-\nabla_{\mathbf{x}}\cdot\rho(\mathbf{x},\Sigma)\Psi(\mathbf{x})=\Bigl{(}\Psi(\mathbf{x})^{\top }\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})-\nabla\cdot\Psi(\mathbf{x})\Bigr{)}\rho(\mathbf{x},\Sigma).\]

We choose \(\nabla\Phi_{\xi}(\mathbf{x})=S_{1}(\mathbf{x}-\mathbf{\mu})+\mathbf{b}_{1}\), where \(S_{1}\in\operatorname{Sym}(d,\mathbb{R})\) and \(\mathbf{b}_{1}\in\mathbb{R}^{d}\) will be determined later. Note that \(S_{1}\) needs to be symmetric because the gradient field is curl-free. We derive that

\[\mathcal{T}_{K,\rho}\left(\nabla\Phi_{\xi}(\mathbf{x})\right) =\] \[= \int((\mathbf{x}-\mathbf{\mu})^{\top}(\mathbf{y}-\mathbf{\mu})+1)\rho(\mathbf{y}, \theta)(S_{1}(\mathbf{y}-\mathbf{\mu})+\mathbf{b}_{1})\operatorname{d}\!\mathbf{y}\] \[= S_{1}\Sigma(\mathbf{x}-\mathbf{\mu})+\mathbf{b}_{1}.\]

Thus, we know \(\Psi(\mathbf{x})=S_{1}\Sigma\left((1-\nu)\Sigma+\nu I\right)^{-1}\mathbf{x}+\mathbf{b}_{1}\). By comparison of the coefficients, we need \(\mathbf{b}_{1}=\widetilde{\mathbf{\mu}}_{1}\) and

\[\begin{cases}\Sigma\left((1-\nu)\Sigma+\nu I\right)^{-1}S_{1}\Sigma^{-1}+ \Sigma^{-1}S_{1}\Sigma\left((1-\nu)\Sigma+\nu I\right)^{-1}=\Sigma^{-1} \widetilde{\Sigma}_{1}\Sigma^{-1},\\ \operatorname{tr}\left(S_{1}\Sigma\left((1-\nu)\Sigma+\nu I\right)^{-1}\right) =\frac{1}{2}\operatorname{tr}\left(\Sigma^{-1}\widetilde{\Sigma}_{1}\right). \end{cases} \tag{61}\]

Note that the first equation is equivalent to the following Lyapunov equation

\[\Sigma^{2}\left((1-\nu)\Sigma+\nu I\right)^{-1}X+X\Sigma^{2}\left((1-\nu) \Sigma+\nu I\right)^{-1}=\widetilde{\Sigma}_{1},\]

which has a unique solution \(X=S_{1}\in\operatorname{Sym}(d,\mathbb{R})\), and the second equation is automatically satisfied once we have the first one. Now from the calculations above it is straightforward to see that

\[G_{\theta}^{-1}(\mathbf{b}_{1},S_{1})=\left(\mathbf{b}_{1},2\left(((1-\nu)\Sigma+\nu I )^{-1}\Sigma^{2}S_{1}+S_{1}((1-\nu)\Sigma+\nu I)^{-1}\Sigma^{2}\right)\right).\]

The proofs of Theorems B.4 and B.6 are similar to that of Theorems B.3 and B.7. In particular, other proofs of Theorem B.6 can also be found in literature, for example, see [77, 60]. Finally, Corollary B.5 is the direct corollary of Theorem B.3 or Theorem B.4.

## Appendix H Proofs for Section C

Given a probability measure \(\mu\) and a Borel-measurable map \(f\), we denote by \(f_{\#}\mu\) the pushforward of the measure \(\mu\) under the map \(f\).

**Definition H.1** (Mean field characteristic flow).: _Given a probability measure \(\nu\), we say that the map_

\[X(t,\mathbf{x},\nu):[0,\infty)\times\mathbb{R}^{d}\to\mathbb{R}^{d}\]

_is a mean field characteristic flow associated to the particle system (13) or to the mean field PDE (2) if \(X\) is \(\mathcal{C}^{1}\) in time and solves the following problem_

\[\begin{split}&\dot{X}(t,\mathbf{x},\nu)=-\left(\nabla K*\mu_{t}\right) \left(X(t,\mathbf{x},\nu)\right)-\left(K*\left(\mu_{t}\nabla V\right)\right)\left( X(t,\mathbf{x},\nu)\right)\\ &\mu_{t}=X(t,\cdot,\nu)_{\#}\nu\\ & X(0,\mathbf{x},\nu)=\mathbf{x}\end{split}. \tag{62}\]

[MISSING_PAGE_EMPTY:30]

[MISSING_PAGE_EMPTY:31]

[MISSING_PAGE_FAIL:32]

[MISSING_PAGE_FAIL:33]

We now explain that the uniqueness of the weak solution follows from the uniqueness of the mean field characteristic flow. Suppose \(q\in\mathcal{C}\left([0,T],\mathcal{P}_{V}(\mathbb{R}^{d})\right)\) is another weak solution to (2). By definition of the weak solution, the vector field \((t,\mathbf{x})\mapsto U\left[q_{t}\right](\mathbf{x})\) is bounded over \([0,T]\times\mathbb{R}^{d}\), continuous in \(t\) and Lipschitz continuous in \(\mathbf{x}\). Then we can define a continuous family of maps \(\widetilde{X}(t,\cdot,\nu)\) by

\[\dot{\widetilde{X}}=U\left[q_{t}\right](\widetilde{X})\] \[\widetilde{X}(0,\mathbf{x},\nu)=\mathbf{x}\]

And the measure \(\widetilde{q}_{t}=\widetilde{X}(t,\cdot,\nu)_{\#}\nu\) is a weak solution to the transport equation

\[\dot{\widetilde{q}}_{t}+\nabla\cdot(\widetilde{q}_{t}U[q_{t}](\mathbf{x}))=0\]

with initial condition \(\widetilde{q}_{0}=\nu=q_{0}\). Uniqueness of the solution to this linear equation implies that \(\widetilde{q}_{t}=q_{t}\). Thus, we have \(\widetilde{X}(t,\cdot,\nu)_{\#}\nu=q_{t}\). In other words, \(\widetilde{X}(t,\mathbf{x},\nu)\) is the mean field characteristic flow for \(\nu\). Uniqueness of the characteristic flow implies that \(\widetilde{X}=X\), and hence \(q_{t}=\rho_{t}\). Thus, we conclude that the weak solution is unique.

Lastly, we show the regularity result: If \(\nu\) has a density \(\rho_{0}(\mathbf{x})\geq 0\), then \(\rho_{t}\) also has a density. Furthermore, if \(\rho_{0}\in\mathcal{H}^{k}(\mathbb{R}^{d})\) for some \(k\), then we have \(\rho_{t}\in\mathcal{H}^{k}(\mathbb{R}^{d})\).

Note that we have already proven that \(\rho_{t}\in\mathcal{C}\left([0,T],\mathcal{P}_{V}\right)\) and

\[\|\rho_{t}\|_{\mathcal{P}_{V}}\leq e^{Ct}\|\rho_{0}\|_{\mathcal{P}_{V}},\quad t \geq 0.\]

Noting that

\[U[\rho](\mathbf{x}) =-\mathbf{x}-\int_{\mathbb{R}^{d}}\nabla V(\mathbf{y})\mathbf{y}^{\top}\mathbf{x} \,\mathrm{d}\mu(\mathbf{y})\] \[=-\mathbf{x}+\int_{\mathbb{R}^{d}}V(\mathbf{y})\mathbf{x}\,\mathrm{d}\mu(\mathbf{ y})\] \[=\int_{\mathbb{R}^{d}}(V(\mathbf{y})-1)\,\mathrm{d}\mu(\mathbf{y})\,\mathbf{ x},\]

we have

\[|U[\rho_{t}](\mathbf{x})|\leq e^{Ct}\|\rho_{0}\|_{\mathcal{P}_{V}}\| \mathbf{x}\|,\] \[\|\nabla U[\rho_{t}](\mathbf{x})\|\leq e^{Ct}\|\rho_{0}\|_{\mathcal{P }_{V}},\] \[D^{j}_{\mathbf{x}}U[\rho_{t}](\mathbf{x})=0\text{ for }j=2,\cdots,k+1.\]

Thus, \(U(t,\mathbf{x}):=U[\rho_{t}](\mathbf{x})\in\mathcal{C}\left([0,T],\mathcal{C}^{k+1}_{ b}(\mathbb{R}^{d})\right)\) where \(\mathcal{C}^{k+1}_{b}(\mathbb{R}^{d})\) is the space of continuous functions with bounded \((k+1)\)-th order derivatives. Let \(\Phi_{t}(\mathbf{x})=X(t,\mathbf{x},\nu)\) denote the characteristic flow. Since \(\Phi_{t}\) satisfies the ODE system

\[\partial_{t}\Phi_{t}(\mathbf{x})=U[\rho_{t}](\Phi_{t}(\mathbf{x})),\]

from the regularity theory of ODE systems (see Chapter 2 of [79]) we know that both the map \(\mathbf{x}\mapsto\Phi_{t}\) and its inverse \(\Phi_{t}^{-1}\) are \(\mathcal{C}^{k}\). Therefore, if \(\rho_{0}\) has a density, then \(\rho_{t}\) also has a density and it is given by

\[\rho_{t}(\mathbf{x})=(\Phi_{t})_{\#}\rho_{0}=\rho_{0}(\Phi_{t}^{-1}(\mathbf{x}))\exp \left(-\int_{0}^{t}(\nabla_{\mathbf{x}}\cdot U[\rho_{s}])(\Phi_{s}\cdot\Phi_{t}^{ -1}(\mathbf{x}))\,\mathrm{d}s\right).\]

Moreover, since \(\rho\) satisfies

\[\dot{\rho}_{t}=-\nabla\cdot(\rho_{t}U[\rho_{t}])\]

with the vector field \(U(t,\mathbf{x})\in\mathcal{C}\left([0,T],\mathcal{C}^{k+1}_{b}(\mathbb{R}^{d})\right)\), it follows from Lemma 2.8 of [44] that

\[\rho\in\mathcal{C}\left([0,T],\mathcal{H}^{k}(\mathbb{R}^{d})\right)\]

for any \(T\geq 0\).

Proof of Theorem c.3.: We show that the particle system (13) is well-posed and that the empirical measure is a weak solution to the mean field PDE. We introduce the function

\[H_{N}(X_{t})=\frac{1}{N}\sum_{i=1}^{N}V\left(\mathbf{x}_{i}^{(t)}\right)+1.\]

Since \(V\) is \(\mathcal{C}^{1}\) hence locally Lipschitz, by Picard-Lindelof theorem the problem (13) has a unique solution up to some time \(T_{0}>0\). Intuitively we only need to show that the solution does not blow up at any finite time. We claim that for some constant \(C\),

\[H_{N}(X_{t})\leq H_{N}(X_{0})\cdot e^{Ct}. \tag{68}\]

To establish this, we first differentiate \(V\left(x_{i}(t)\right)\) with respect to \(t\) and sum over \(i\):

\[\partial_{t}\left(\frac{1}{N}\sum_{i=1}^{N}V\left(\mathbf{x}_{i}^{(t) }\right)\right)\] \[= -\frac{1}{N}\sum_{i=1}^{N}\nabla V\left(\mathbf{x}_{i}^{(t)}\right)^{ \top}\mathbf{x}_{i}^{(t)}-\frac{1}{N^{2}}\sum_{i,j=1}^{N}{\mathbf{x}_{i}^{(t)}}^{\top }\mathbf{x}_{j}^{(t)}\nabla V\left(\mathbf{x}_{i}^{(t)}\right)^{\top}\nabla V\left(\bm {x}_{j}^{(t)}\right)\] \[\leq C_{1,0}\left(\frac{1}{N}\sum_{i=1}^{N}V\left(\mathbf{x}_{i}^{(t)} \right)+1\right).\]

Note that here we have used Assumption C.1. By Gronwall's inequality, (68) holds.

Now to be rigorous, once again we define

\[\tau:=\sup\left\{t\in\mathbb{R}^{+}\cup\left\{\infty\right\}:\text{ \eqref{eq:H1} has a (unique) solution on }\left[0,t\right)\right\}.\]

If \(\tau<\infty\), we define

\[\mathbf{x}_{i}^{(\tau)}:=\lim_{t\nearrow\tau^{-}}\mathbf{x}_{i}^{(t)}.\]

Then (13) has a unique solution on \([0,\tau]\). Again by Picard-Lindelof theorem, there exists some \(\epsilon>0\) such that (13) has a unique solution on \([\tau,\tau+\epsilon]\), which contradicts the definition of \(\tau\). Thus, we conclude that \(\tau=\infty\), which means that there is a global unique solution to (13).

Having established the well-posedness of the finite particle system, it now follows from the definition of the characteristic flow \(X\left(t,\mathbf{x},\mu_{0}^{N}\right)\) that

\[\mathbf{x}_{i}^{(t)}=X\left(t,\mathbf{x}_{i}^{(t)},\mu_{0}^{N}\right)\]

and

\[\mu_{t}^{N}(\mathrm{d}\mathbf{x})=\left(X\left(t,\cdot,\mu_{0}^{N}\right)\right)_ {\#}\mu_{0}^{N}.\]

Similar to the proof of Theorem C.2, we conclude that \(\mu_{t}^{N}\) is a weak solution to the mean field PDE (2). 

Finally, we show Theorem C.5.

Proof of Theorem c.5.: Recall that \(p=q^{\prime}=q/(q-1)\). By assumption \(\|\nu_{i}\|_{\mathcal{P}^{p}}\leq R<\infty\) and the fact that \(\mathcal{P}^{p}(\mathbb{R}^{d})\subset\mathcal{P}_{V}(\mathbb{R}^{d})\), we know that there exists \(C>0\) depending on \(R\) such that

\[\|\nu_{i}\|_{\mathcal{P}_{V}}\leq C<\infty.\]

From the proof of Theorem C.2 and Definition H.1, we know that the weak solutions \(\mu_{i,t}\) take the form

\[\mu_{i,t}=(X(t,\cdot,\nu_{i}))_{\#},\quad i=1,2.\]

Now we bound \(\mathcal{W}_{p}^{p}(\mu_{1,t},\mu_{2,t})\) using \(\mathcal{W}_{p}^{p}(\nu_{1},\nu_{2})\). Let \(\pi^{0}\) be a coupling between \(\nu_{1}\) and \(\nu_{2}\). For \(\delta>0\) define \(\phi_{\delta}(\mathbf{x}):=\frac{1}{p}(\|\mathbf{x}\|+\delta)^{p/2}\) to be an approximation to \(\frac{1}{p}\|\mathbf{x}\|^{p}\). Given any two points \(\mathbf{x}_{1},\mathbf{x}_{2}\in\mathbb{R}^{d}\), we have that \[\partial_{t}\phi_{\delta}\left(X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2})\right)\] \[= -\nabla\phi_{\delta}\left(X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2}, \nu_{2})\right)^{\top}\] \[\left\{(X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2}))\right.\] \[\left.+\left(\int_{\mathbb{R}^{2d}}\nabla V(X(t,\mathbf{x}_{1}^{\prime },\nu_{1}))X(t,\mathbf{x}_{1}^{\prime},\nu_{1})^{\top}X(t,\mathbf{x}_{1},\nu_{1})\nu_{ 1}(\mathrm{d}\mathbf{x}_{1}^{\prime})\right.\right.\] \[\left.\left.-\int_{\mathbb{R}^{2d}}\nabla V(X(t,\mathbf{x}_{2}^{ \prime},\nu_{2}))X(t,\mathbf{x}_{2}^{\prime},\nu_{2})^{\top}X(t,\mathbf{x}_{2}^{\prime },\nu_{2})\nu_{2}(\mathrm{d}\mathbf{x}_{2}^{\prime})\right)\right\}\] \[= -\nabla\phi_{\delta}\left(X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2}, \nu_{2})\right)^{\top}\] \[\left\{(X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2}))\right.\] \[\left.+\int_{\mathbb{R}^{2d}}\nabla V(X(t,\mathbf{x}_{1}^{\prime},\nu _{1}))X(t,\mathbf{x}_{1}^{\prime},\nu_{1})^{\top}(X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{ x}_{2},\nu_{2}))\pi^{0}(\mathrm{d}\mathbf{x}_{1}^{\prime}\,\mathrm{d}\mathbf{x}_{2}^{ \prime})\right.\right.\] \[\left.+\int_{\mathbb{R}^{2d}}\nabla V(X(t,\mathbf{x}_{1}^{\prime},\nu _{1}))(X(t,\mathbf{x}_{1}^{\prime},\nu_{1})-X(t,\mathbf{x}_{2}^{\prime},\nu_{2}))^{ \top}X(t,\mathbf{x}_{2},\nu_{2})\pi^{0}(\mathrm{d}\mathbf{x}_{1}^{\prime}\,\mathrm{d} \mathbf{x}_{2}^{\prime})\right.\] \[\left.+\int_{\mathbb{R}^{2d}}\left(\nabla V(X(t,\mathbf{x}_{1}^{ \prime},\nu_{1}))-\nabla V(X(t,\mathbf{x}_{2}^{\prime},\nu_{2}))\right)X(t,\mathbf{x}_ {2}^{\prime},\nu_{2})^{\top}X(t,\mathbf{x}_{2},\nu_{2})\pi^{0}(\mathrm{d}\mathbf{x}_{1 }^{\prime}\,\mathrm{d}\mathbf{x}_{2}^{\prime})\right\}\] \[= :I_{1}+I_{2}+I_{3}+I_{4}.\]

Below we bound \(I_{i}\) individually. First, noticing that

\[\left\|\nabla\phi_{\delta}(\mathbf{x})\right\|=\left\|(\left\|\mathbf{x} \right\|^{2}+\delta)^{p/2-1}\mathbf{x}\right\|\leq\left\|\mathbf{x}\right\|^{p-1}, \tag{69}\]

we obtain

\[I_{1}\leq\left\|X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2}) \right\|^{p}. \tag{70}\]

Next we bound \(I_{2}\):

\[I_{2} \leq\left\|X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2})\right\| ^{p}\left\|\int_{\mathbb{R}^{2d}}\nabla V(X(t,\mathbf{x}_{1}^{\prime},\nu_{1}))X(t,\mathbf{x}_{1}^{\prime},\nu_{1})^{\top}\nu_{1}(\mathrm{d}\mathbf{x}_{1}^{\prime})\right\|\] \[\overset{a}{\leq}\left\|X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2}, \nu_{2})\right\|^{p}. \tag{71}\] \[\quad\left(\int_{\mathbb{R}^{2d}}\lVert\nabla V(X(t,\mathbf{x}_{1}^{ \prime},\nu_{1}))\rVert^{q}\nu_{1}(\mathrm{d}\mathbf{x}_{1}^{\prime})\right)^{1/q} \left(\int_{\mathbb{R}^{2d}}\lVert X(t,\mathbf{x}_{1}^{\prime},\nu_{1})\rVert^{p} \nu_{1}(\mathrm{d}\mathbf{x}_{1}^{\prime})\right)^{1/p}\] \[\overset{b}{\leq}\left\|X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2}, \nu_{2})\right\|^{p}C_{V}^{1/q}\lVert\mu_{1,t}\rVert_{\mathcal{P}_{V}}^{1/q} \cdot\lVert\mu_{1,t}\rVert_{\mathcal{P}^{p}}\] \[\overset{c}{\leq}C_{V}^{1/q}e^{(C_{1}/q+C_{2})t}\lVert\nu_{1} \rVert_{\mathcal{P}_{V}}^{1/q}\cdot\lVert\nu_{1}\rVert_{\mathcal{P}^{p}}\left\|X (t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2})\right\|^{p}. \tag{72}\]

Here we have applied Holder's inequality in \(a\) and Theorem C.2 in \(c\). The inequality \(b\) is due to Assumption C.4 and the definitions of the \(\mathcal{P}_{V}\)-norm and \(\mathcal{P}^{p}\)-norm.

Similarly for \(I_{3}\) we use Holder's inequality again and get

\[I_{3} \leq\|X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2})\|^{p-1}\cdot \|X(t,\mathbf{x}_{2},\nu_{2})\|\,.\] \[\quad\int_{\mathbb{R}^{2d}}\|\nabla V(X(t,\mathbf{x}_{1}^{\prime},\nu _{1}))\|\cdot\|X(t,\mathbf{x}_{1}^{\prime},\nu_{1})-X(t,\mathbf{x}_{2}^{\prime},\nu_{2} )\|\,\pi^{0}(\mathrm{d}\mathbf{x}_{1}^{\prime}\,\mathrm{d}\mathbf{x}_{2}^{\prime})\] \[\leq\|X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2})\|^{p-1}\cdot \|X(t,\mathbf{x}_{2},\nu_{2})\|\,.\] \[\quad\left(\int_{\mathbb{R}^{2d}}\|\nabla V(X(t,\mathbf{x}_{1}^{ \prime},\nu_{1}))\|^{q}\,\nu_{1}(\mathrm{d}\mathbf{x}_{1}^{\prime})\right)^{1/q} \left(\int_{\mathbb{R}^{2d}}\|X(t,\mathbf{x}_{1}^{\prime},\nu_{1})-X(t,\mathbf{x}_{2}^ {\prime},\nu_{2})\|^{p}\,\pi^{0}(\mathrm{d}\mathbf{x}_{1}^{\prime}\,\mathrm{d}\mathbf{ x}_{2}^{\prime})\right)^{1/p}\] \[\leq C_{V}^{1/q}e^{(C_{1}/q+C_{2})t}\|\nu_{1}\|_{\mathcal{P}_{V} ^{\prime}}^{1/q}\,\|X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2})\|^{p-1}\, \|X(t,\mathbf{x}_{2},\nu_{2})\|\,.\] \[\quad\left(\int_{\mathbb{R}^{2d}}\|X(t,\mathbf{x}_{1}^{\prime},\nu_{1} )-X(t,\mathbf{x}_{2}^{\prime},\nu_{2})\|^{p}\,\pi^{0}(\mathrm{d}\mathbf{x}_{1}^{\prime }\,\mathrm{d}\mathbf{x}_{2}^{\prime})\right)^{1/p}. \tag{73}\]

Finally we proceed to bound \(I_{4}\). An application of the intermediate value theorem to the difference of \(\nabla V\) yields that

\[I_{4} \leq\|X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2})\|^{p-1} \cdot\|X(t,\mathbf{x}_{2},\nu_{2})\|\,.\] \[\quad\int_{\mathbb{R}^{2d}}\sup_{\theta\in[0,1]}\bigl{\|}\nabla^{2 }V(\theta X(t,\mathbf{x}_{1}^{\prime},\nu_{1})+(1-\theta)X(t,\mathbf{x}_{2}^{\prime}, \nu_{2}))\bigr{\|}\,.\] \[\quad\quad\|X(t,\mathbf{x}_{1}^{\prime},\nu_{1})-X(t,\mathbf{x}_{2}^{ \prime},\nu_{2})\|\cdot\|X(t,\mathbf{x}_{2}^{\prime},\nu_{2})\|\,\pi^{0}(\mathrm{d }\mathbf{x}_{1}^{\prime}\,\mathrm{d}\mathbf{x}_{2}^{\prime})\] \[\leq\|X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2})\|^{p-1}\, \|X(t,\mathbf{x}_{2},\nu_{2})\|\,.\] \[\quad\left(\int_{\mathbb{R}^{2d}}\sup_{\theta\in[0,1]}\bigl{\|} \nabla^{2}V(\theta X(t,\mathbf{x}_{1}^{\prime},\nu_{1})+(1-\theta)X(t,\mathbf{x}_{2}^{ \prime},\nu_{2}))\bigr{\|}^{q}\,\|X(t,\mathbf{x}_{2}^{\prime},\nu_{2})\|^{q}\,\pi ^{0}(\mathrm{d}\mathbf{x}_{1}^{\prime}\,\mathrm{d}\mathbf{x}_{2}^{\prime})\right)^{1/q}\cdot\] \[\quad\quad\left(\int_{\mathbb{R}^{2d}}\|X(t,\mathbf{x}_{1}^{\prime}, \nu_{1})-X(t,\mathbf{x}_{2}^{\prime},\nu_{2})\|^{p}\,\pi^{0}(\mathrm{d}\mathbf{x}_{1}^ {\prime}\,\mathrm{d}\mathbf{x}_{2}^{\prime})\right)^{1/p}\] \[\leq C_{V}^{1/q}\,\|X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2 })\|^{p-1}\,\|X(t,\mathbf{x}_{2},\nu_{2})\|\,.\] \[\quad\left(\int_{\mathbb{R}^{2d}}\|X(t,\mathbf{x}_{1}^{\prime},\nu_{1 })\|+\|V(X(t,\mathbf{x}_{2}^{\prime},\nu_{2}))\|+\|X(t,\mathbf{x}_{2}^{\prime},\nu_{2} )\|^{q}\right)\pi^{0}(\mathrm{d}\mathbf{x}_{1}^{\prime}\,\mathrm{d}\mathbf{x}_{2}^{ \prime})\right)^{1/q}\cdot\] \[\quad\quad\left(\int_{\mathbb{R}^{2d}}\|X(t,\mathbf{x}_{1}^{\prime},\nu_{1})-X(t,\mathbf{x}_{2}^{\prime},\nu_{2})\|^{p}\,\pi^{0}(\mathrm{d}\mathbf{x}_{1}^ {\prime}\,\mathrm{d}\mathbf{x}_{2}^{\prime})\right)^{1/p}\] \[\leq C_{V}^{1/q}\,\Bigl{(}\|\mu_{1,t}\|_{\mathcal{P}_{V}}^{1/q}+ \|\mu_{2,t}\|_{\mathcal{P}_{V}}^{1/q}+\|\mu_{2,t}\|_{\mathcal{P}_{V}}\Bigr{)}\, \|X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2})\|^{p-1}\,\|X(t,\mathbf{x}_{2}, \nu_{2})\|\,.\] \[\quad\quad\left(\int_{\mathbb{R}^{2d}}\|X(t,\mathbf{x}_{1}^{\prime},\nu _{1})-X(t,\mathbf{x}_{2}^{\prime},\nu_{2})\|^{p}\,\pi^{0}(\mathrm{d}\mathbf{x}_{1}^{ \prime}\,\mathrm{d}\mathbf{x}_{2}^{\prime})\right)^{1/p}\] \[\stackrel{{ b}}{{\leq}}C_{V}^{1/q}\,\Bigl{(}e^{C _{1}t/q}\|\nu_{1}\|_{\mathcal{P}_{V}}^{1/q}+e^{C_{1}t/q}\|\nu_{2}\|_{\mathcal{P}_ {V}}^{1/q}+e^{C_{2}t}\|\nu_{2}\|_{\mathcal{P}_{P}}\Bigr{)}\,\|X(t,\mathbf{x}_{1}, \nu_{1})-X(t,\mathbf{x}_{2},\nu_{2})\|^{p-1}\cdot\] \[\quad\quad\|X(t,\mathbf{x}_{2},\nu_{2})\|\left(\int_{\mathbb{R}^{2d}} \|X(t,\mathbf{x}_{1}^{\prime},\nu_{1})-X(t,\mathbf{x}_{2}^{\prime},\nu_{2})\|^{p}\,\pi ^{0}(\mathrm{d}\mathbf{x}_{1}^{\prime}\,\mathrm{d}\mathbf{x}_{2}^{\prime})\right)^{1/p} \tag{74}\]

Note that to get \(a\) we have applied (43) of Assumption C.4 and \(b\) is implied by Theorem C.2.

If we define

\[D_{p}(\pi)(s):=\left(\int_{\mathbb{R}^{2d}}\|X(s,\mathbf{x}_{1}^{\prime},\nu_{1})-X(s,\mathbf{x}_{2}^{\prime},\nu_{2})\|^{p}\,\pi(\mathrm{d}\mathbf{x}_{1}^{\prime}\, \mathrm{d}\mathbf{x}_{2}^{\prime})\right)^{1/p},\]

combining (70), (71), (73) and (74) we obtain that

\[\partial_{t}\phi_{\delta}\left(X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_ {2},\nu_{2})\right)\] \[\leq 4C_{V}^{1/q}e^{(C_{1}/q+C_{2})t}R^{(q+1)/q}\|X(t,\mathbf{x}_{1},\nu_{1} )-X(t,\mathbf{x}_{2},\nu_{2})\|^{p-1}\cdot\] \[\left(\|X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2})\|+D_{p}( \pi^{0})(t)\|X(t,\mathbf{x}_{2},\nu_{2})\|\right).\]Now integrating the inequality above with respect to the coupling \(\pi^{0}(\mathrm{d}\mathbf{x}_{1},\mathrm{d}\mathbf{x}_{2})\) using the fact that

\[\int_{\mathbb{R}^{2d}}\|X(s,\mathbf{x}_{1},\nu_{1})-X(s,\mathbf{x}_{2},\nu_ {2})\|^{p-1}\,\|X(s,\mathbf{x}_{2},\nu_{2})\|\pi^{0}(\mathrm{d}\mathbf{x}_{1}\,\mathrm{ d}\mathbf{x}_{2})\] \[\leq \left(\int_{\mathbb{R}^{2d}}\|X(s,\mathbf{x}_{1},\nu_{1})-X(s,\mathbf{x}_ {2},\nu_{2})\|^{p}\,\pi^{0}(\mathrm{d}\mathbf{x}_{1}\,\mathrm{d}\mathbf{x}_{2})\right)^ {(p-1)/p}\left(\int_{\mathbb{R}^{2d}}\|X(s,\mathbf{x}_{2},\nu_{2})\|^{p}\nu_{2}( \mathbf{x}_{2})\right)^{1/p}\] \[\leq e^{C_{2}t}RD_{p}^{p-1}(\pi^{0})(s).\]

Thus, we obtain that

\[\partial_{t}\phi_{\delta}\left(X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2} )\right)\leq 8C_{V}^{1/q}e^{(C_{1}/q+2C_{2})t}R^{(2q+1)/q}D_{p}^{p}(\pi^{0})(t) \leq C_{V,R}e^{CT}D_{p}^{p}(\pi^{0})(t).\]

Integrating \(t\) we get

\[\phi_{\delta}\left(X(t,\mathbf{x}_{1},\nu_{1})-X(t,\mathbf{x}_{2},\nu_{2})\right)\leq \phi_{\delta}(\mathbf{x}_{1}-\mathbf{x}_{2})+C_{V,R}e^{CT}\int_{0}^{t}D_{p}^{p}(\pi^{0} )(s)\,\mathrm{d}s.\]

Finally letting \(\delta\to 0\) yields

\[D_{p}^{p}(\pi^{0})(t)\leq D_{p}^{p}(\pi^{0})(0)+C_{V,R}e^{CT}\int_{0}^{t}D_{p}^ {p}(\pi^{0})(s)\,\mathrm{d}s.\]

By Gronwall's inequality, we obtain that

\[D_{p}^{p}(\pi^{0})(t)\leq D_{p}^{p}(0)\exp\left(C_{V,R}e^{CT}t\right).\]

Now since \(\pi^{0}\in\Gamma(\nu_{1},\nu_{2})\) and \(\mu_{i,t}=(X(t,\cdot,\nu_{i}))_{\#}\nu_{i}\), the mapping

\[\Xi_{t}:(\mathbf{x}_{1},\mathbf{x}_{2})\in\mathbb{R}^{2d}\mapsto(X(t,\mathbf{x}_{1},\nu_{1 }),X(t,\mathbf{x}_{2},\nu_{2}))\in\mathbb{R}^{2d}\]

satisfies that \((\Xi_{t})_{\#}\pi^{0}\in\Gamma(\mu_{1,t},\mu_{2,t})\). As a consequence we have that

\[\mathcal{W}_{p}^{p}(\mu_{1,t},\mu_{2,t}) =\inf_{\pi\in\Gamma(\mu_{1,t},\mu_{2,t})}\int_{\mathbb{R}^{2d}}\| \mathbf{x}_{1}-\mathbf{x}_{2}\|^{p}\pi(\mathrm{d}\mathbf{x}_{1}\,\mathrm{d}\mathbf{x}_{2})\] \[\leq\inf_{\pi^{0}\in\Gamma(\nu_{1},\nu_{2})}D_{p}^{p}(\pi^{0})(t)\] \[\leq\exp\left(C_{V,R}e^{CT}T\right)\inf_{\pi^{0}\in\Gamma(\nu_{1},\nu_{2})}D_{p}^{p}(\pi^{0})(0)\] \[=\exp\left(C_{V,R}e^{CT}T\right)\cdot\mathcal{W}_{p}^{p}(\nu_{1}, \nu_{2}).\]

## Appendix I Proofs for Section 3.1

Proof of Theorem 3.1.: For any \(\theta=(\mathbf{\mu},\Sigma)\in\Theta\), define \(\widetilde{E}(\mathbf{\mu},\Sigma):=E(\rho)\). Then we have

\[\widetilde{E}(\mathbf{\mu},\Sigma)=\mathrm{KL}(\rho\parallel\rho^{*})=\frac{1}{2} \left(\mathrm{tr}(Q^{-1}\Sigma)-\log\det(Q^{-1}\Sigma)-d+(\mathbf{\mu}-\mathbf{b})^{ \top}Q^{-1}(\mathbf{\mu}-\mathbf{b})\right),\]

where \(\rho\) is the density of \(\mathcal{N}(\mathbf{\mu},\Sigma)\) and \(\rho^{*}\) is the density of \(\mathcal{N}(\mathbf{b},Q)\).

Now we consider the gradient flow on the submanifold \(\Theta\)

\[\dot{\theta}_{t}=-G_{\theta_{t}}^{-1}(\nabla_{\theta_{t}}\widetilde{E}(\theta_ {t})).\]

For clarity note that here

\[\nabla_{\theta_{t}}\widetilde{E}(\theta_{t}):=\left(\nabla_{\mathbf{\mu}_{t}} \widetilde{E}(\mathbf{\mu}_{t},\Sigma_{t}),\ \nabla_{\Sigma_{t}}\widetilde{E}(\mathbf{\mu}_{t}, \Sigma_{t})\right),\]

where \(\nabla_{\Sigma}\widetilde{E}(\mathbf{\mu},\Sigma)\) denotes the standard matrix derivative (not the covariant derivative or affine connection in the contexts of Riemannian geometry). We calculate that

\[\nabla_{\mathbf{\mu}}\widetilde{E}(\mathbf{\mu},\Sigma)=Q^{-1}(\mathbf{\mu}-\mathbf{b}),\quad \nabla_{\Sigma}\widetilde{E}(\mathbf{\mu},\Sigma)=\frac{1}{2}(Q^{-1}-\Sigma^{-1}).\]

[MISSING_PAGE_FAIL:39]

is exactly a quadratic form. Thus,

\[\frac{\delta E}{\delta\widetilde{\rho}_{t}}\in\operatorname{Im}\psi_{\theta_{t}}= \operatorname{Im}\psi_{\theta_{t}}\,{}^{\sim}\,G_{\theta_{t}}=\operatorname{Im}G _{\widetilde{\rho}_{t}}{}^{\sim}\,\mathrm{d}\phi_{\theta}.\]

Next since \(\mathrm{d}\phi_{\theta_{t}}\) maps the tangent vector \(\frac{\partial}{\partial\theta_{t}}\in T_{\theta_{t}}\Theta\) to \(\frac{\delta}{\delta\widetilde{\rho}_{t}}\in T_{\widetilde{\rho}_{t}}\mathcal{ P}(\mathbb{R}^{d})\), we have that

\[\phi^{*}\frac{\delta E}{\delta\widetilde{\rho}_{t}}=\nabla_{\theta_{t}} \widetilde{E}.\]

Combining this with the fact that \(\frac{\delta E}{\delta\widetilde{\rho}_{t}}\in\operatorname{Im}G_{\widetilde{ \rho}_{t}}{}^{\sim}\,\mathrm{d}\phi_{\theta}\), we get

\[\frac{\delta E}{\delta\widetilde{\rho}_{t}}=\psi_{\theta}(\nabla_{\theta_{t}} \widetilde{E}).\]

Thus, we have

\[G_{\widetilde{\rho}_{t}}^{\sim}\,\frac{\delta E}{\delta\widetilde{\rho}_{t}}=G _{\widetilde{\rho}_{t}}^{-1}\psi_{\theta}(\nabla_{\theta_{t}}\widetilde{E})= \mathrm{d}\phi_{\theta_{t}}G_{\theta_{t}}^{-1}(\nabla_{\theta_{t}}\widetilde{ E}).\]

And we conclude

\[\dot{\widetilde{\rho}}_{t}=\mathrm{d}\phi_{\theta_{t}}\dot{\theta_{t}}=-\, \mathrm{d}\phi_{\theta_{t}}G_{\theta_{t}}^{-1}(\nabla_{\theta_{t}}\widetilde{ E})=-G_{\widetilde{\rho}_{t}}^{-1}\frac{\delta E}{\delta\widetilde{\rho}_{t}}.\]

The claim is proven.

Now back to the original problem. Suppose \(s<\infty\). Since we know that the mean field PDE (76) has a unique solution on \([0,\infty)\), in particular, it exists on \([0,s]\). Note that the weak limit of Gaussian distributions is Gaussian and since \(\rho_{s}\in\mathcal{P}(\mathbb{R}^{d})\) it does not degenerate. By definition of \(\phi\) we have that \(\phi^{-1}(\rho_{s})\in\Theta\). By letting \((\boldsymbol{\mu}_{s},\Sigma_{s})=\gamma(s):=\phi^{-1}(\rho_{s})\), we obtain the solution of (75) on \([0,s]\). Again by Picard-Lindelof theorem there exists a small neighborhood \([s,s+\epsilon^{\prime})\) such that (75) has a unique solution. This together with the solution on \([0,s]\) contradicts the definition of \(s\). Therefore, we conclude that \(s=\infty\). (75) has a unique global solution corresponding to the mean and covariance matrix of \(\rho_{t}\).

Next, we prove that \(\rho_{t}\) converges weakly to \(\rho^{*}\) as \(t\to\infty\). We calculate the quantity \(\dot{\widetilde{E}}(\boldsymbol{\mu}_{t},\Sigma_{t})\). By Jacobi's formula in matrix calculus (Theorem 8.1 in [59]), we have

\[\partial_{t}\det\Sigma_{t}=\det\Sigma_{t}\operatorname{tr}(\Sigma_{t}^{-1} \dot{\Sigma}_{t}).\]

Thus, we derive that

\[\dot{\widetilde{E}}(\boldsymbol{\mu}_{t},\Sigma_{t})=\frac{1}{2} \operatorname{tr}((Q^{-1}-\Sigma_{t}^{-1})\dot{\Sigma}_{t})+(\boldsymbol{\mu }_{t}-\boldsymbol{b})^{\top}Q^{-1}\dot{\boldsymbol{\mu}}_{t}\] \[= -\operatorname{tr}\left((Q^{-1}-\Sigma_{t}^{-1})^{2}\Sigma_{t}^{ 2}\right)-2\operatorname{tr}\left((Q^{-1}-\Sigma_{t}^{-1})\Sigma_{t} \boldsymbol{\mu}_{t}(\boldsymbol{\mu}_{t}-\boldsymbol{b})^{\top}Q^{-1}\right)\] \[\qquad-(1+\boldsymbol{\mu}_{t}^{\top}\boldsymbol{\mu}_{t})( \boldsymbol{\mu}_{t}-\boldsymbol{b})^{\top}Q^{-2}(\boldsymbol{\mu}_{t}- \boldsymbol{b})\] \[= -\operatorname{tr}\left(\left((Q^{-1}-\Sigma_{t}^{-1})\Sigma_{t}+ Q^{-1}(\boldsymbol{\mu}

If we set \(\boldsymbol{\eta}_{t}=Q^{-1}(\boldsymbol{\mu}_{t}-\boldsymbol{b})\) and \(S_{t}=(Q^{-1}-\Sigma_{t}^{-1})\Sigma_{t}\), then

\[-\dot{\widetilde{E}}(\boldsymbol{\mu}_{t},\Sigma_{t})\] \[= \operatorname{tr}\left(\left((Q^{-1}-\Sigma_{t}^{-1})\Sigma_{t}+Q ^{-1}(\boldsymbol{\mu}_{t}-\boldsymbol{b})\boldsymbol{\mu}_{t}^{\top}\right)^{ \top}\left((Q^{-1}-\Sigma_{t}^{-1})\Sigma_{t}+Q^{-1}(\boldsymbol{\mu}_{t}- \boldsymbol{b})\boldsymbol{\mu}_{t}^{\top}\right)\right)\] \[+(\boldsymbol{\mu}_{t}-\boldsymbol{b})^{\top}Q^{-2}(\boldsymbol{ \mu}_{t}-\boldsymbol{b})\] \[= \operatorname{tr}\left((S_{t}+\boldsymbol{\eta}_{t}\boldsymbol{ \mu}_{t}^{\top})^{\top}(S_{t}+\boldsymbol{\eta}_{t}\boldsymbol{\mu}_{t}^{\top })\right)+\boldsymbol{\eta}_{t}^{\top}\boldsymbol{\eta}_{t}\] \[= \begin{bmatrix}\operatorname{vec}^{\top}(S_{t})&\boldsymbol{\eta} _{t}{}^{\top}\end{bmatrix}\begin{bmatrix}I_{d^{2}}&\boldsymbol{\mu}_{t}\otimes I _{d}\\ \boldsymbol{\mu}_{t}^{\top}\otimes I_{d}&(1+\boldsymbol{\mu}_{t}^{\top} \boldsymbol{\mu}_{t})I_{d}\end{bmatrix}\begin{bmatrix}\operatorname{vec}(S_{t })\\ \boldsymbol{\eta}_{t}\end{bmatrix}\] \[= \begin{bmatrix}\operatorname{vec}^{\top}(S_{t})&\boldsymbol{\eta} _{t}{}^{\top}\end{bmatrix}\begin{bmatrix}I_{d^{2}}&\boldsymbol{b}_{t}\otimes I _{d}\\ \boldsymbol{b}_{t}^{\top}\otimes I_{d}&(1+\boldsymbol{b}_{t}^{\top} \boldsymbol{b}_{t})I_{d}\end{bmatrix}\begin{bmatrix}\operatorname{vec}(S_{t}) \\ \boldsymbol{\eta}_{t}\end{bmatrix}+o(\|S_{t}\|+\|\boldsymbol{\eta}_{t}\|).\]

On the other hand, \(\widetilde{E}(\boldsymbol{\mu}_{t},\Sigma_{t})\) can be written as

\[\widetilde{E}(\boldsymbol{\mu}_{t},\Sigma_{t})= \frac{1}{2}\left(\operatorname{tr}(Q^{-1}\Sigma)-\log\det(Q^{-1} \Sigma)-d+(\boldsymbol{\mu}-\boldsymbol{b})^{\top}Q^{-1}(\boldsymbol{\mu}- \boldsymbol{b})\right)\] \[= \frac{1}{2}\left(\operatorname{tr}(S_{t})-\log\det(I_{d}+S_{t})+ \boldsymbol{\eta}_{t}^{\top}Q\boldsymbol{\eta}_{t}\right)\] \[= \frac{1}{4}\operatorname{tr}(S_{t}^{\top}S_{t})+\frac{1}{2} \boldsymbol{\eta}_{t}^{\top}Q\boldsymbol{\eta}_{t}+o(\|S_{t}\|^{2})\] \[= \frac{1}{4}\begin{bmatrix}\operatorname{vec}^{\top}(S_{t})& \boldsymbol{\eta}_{t}{}^{\top}\end{bmatrix}\begin{bmatrix}I_{d^{2}}&\\ &2Q\end{bmatrix}\begin{bmatrix}\operatorname{vec}(S_{t})\\ \boldsymbol{\eta}_{t}\end{bmatrix}+o(\|S_{t}\|^{2})\]

Now we prove that \(\forall\epsilon>0\) there exists \(T>0\) such that \(-\dot{\widetilde{E}}(\boldsymbol{\mu}_{t},\Sigma_{t})\geq 4(\gamma-\epsilon) \widetilde{E}(\boldsymbol{\mu}_{t},\Sigma_{t})\) for \(t\geq T\). It suffices to show that

\[\begin{bmatrix}I_{d^{2}}&\boldsymbol{b}_{t}\otimes I_{d}\\ \boldsymbol{b}_{t}^{\top}\otimes I_{d}&(1+\boldsymbol{b}_{t}^{\top} \boldsymbol{b}_{t})I_{d}\end{bmatrix}\succeq\gamma\begin{bmatrix}I^{d^{2}}& 2Q\end{bmatrix},\]

which is equivalent to

\[\begin{bmatrix}I_{d^{2}}&\frac{1}{\sqrt{2}}\boldsymbol{b}\otimes Q^{-1/2}\\ \frac{1}{\sqrt{2}}\boldsymbol{b}^{\top}\otimes Q^{-1/2}&\frac{1}{2}(1+ \boldsymbol{b}^{\top}\boldsymbol{b})Q^{-1}\end{bmatrix}\succeq\gamma I_{d^{2 }+d}.\]

This is true because by definition \(\gamma\) is the smallest eigenvalue of the matrix.

By Gronwall's inequality, we know \(\widetilde{E}(\boldsymbol{\mu}_{t},\Sigma_{t})=\mathcal{O}(e^{-4(\gamma- \epsilon)t})\). Thus, we conclude

\[\|\boldsymbol{\mu}_{t}-\boldsymbol{b}\|=\mathcal{O}(e^{-2(\gamma-\epsilon)t} ),\quad\|\Sigma_{t}-Q\|=\mathcal{O}(e^{-2(\gamma-\epsilon)t}),\quad\forall \epsilon>0.\]

Finally, we provide a lower bound on \(\gamma\). Note that for any \(u>0\) if \(\boldsymbol{b}\neq\boldsymbol{0}\) we have

\[\begin{bmatrix}\frac{1}{1+u}I_{d^{2}}&\frac{1}{\sqrt{2}}\boldsymbol{b}\otimes Q ^{-1/2}\\ \frac{1}{\sqrt{2}}\boldsymbol{b}^{\top}\otimes Q^{-1/2}&\frac{1+u}{2} \boldsymbol{b}^{\top}\boldsymbol{b}Q^{-1}\end{bmatrix}\succeq 0.\]

Thus,

\[\begin{bmatrix}I_{d^{2}}&\frac{1}{\sqrt{2}}\boldsymbol{b}\otimes Q^{-1/2}\\ \frac{1}{\sqrt{2}}\boldsymbol{b}^{\top}\otimes Q^{-1/2}&\frac{1}{2}(1+ \boldsymbol{b}^{\top}\boldsymbol{b})Q^{-1}\end{bmatrix}\succeq\begin{bmatrix} \frac{u}{1+u}I_{d^{2}}&\\ &\frac{1}{2}(1-u\boldsymbol{b}^{\top}\boldsymbol{b})Q^{-1}\end{bmatrix}=: \Omega_{u}.\]

Since \(\lambda_{\max}\) is the largest eigenvalue of \(Q\), we know the smallest eigenvalue of \(\Omega_{u}\) is given by

\[\min\left\{\frac{u}{1+u},\frac{1-u\boldsymbol{b}^{\top}\boldsymbol{b}}{2\lambda _{\max}}\right\},\text{ where }u>0.\]We find \(u\) such that this quantity is maximized and get

\[\gamma\geq \max_{u>0}\min\left\{\frac{u}{1+u},\frac{1-ub^{\top}\mathbf{b}}{2\lambda _{\max}}\right\}=\frac{2}{1+\mathbf{b}^{\top}\mathbf{b}+2\lambda_{\max}+\sqrt{(1+\mathbf{b} ^{\top}\mathbf{b}+2\lambda_{\max})^{2}-8\lambda_{\max}}}\] \[> \frac{1}{1+\mathbf{b}^{\top}\mathbf{b}+2\lambda_{\max}}.\]

If \(\mathbf{b}=0\), then the smallest eigenvalue is given by

\[\gamma=\min\left\{1,\frac{1}{2\lambda_{\max}}\right\}>\frac{1}{1+2\lambda_{ \max}}.\]

Proof of Theorem 3.2.: Equation (5) is a direct corollary of Theorem 3.1.

By Theorem C.2 there is a unique global solution. Thus, we only need to check that the \(\Sigma_{t}\) given by (6) and (7) satisfy the algebraic Riccati equation (5).

For

\[\Sigma_{t}^{-1}=e^{-2t}\Sigma_{0}^{-1}+(1-e^{-2t})Q^{-1}, \tag{77}\]

we take the derivative with respect to \(t\) and get

\[-\Sigma_{t}^{-1}\dot{\Sigma}_{t}\Sigma_{t}^{-1}=2e^{-2t}(Q^{-1}-\Sigma_{0}^{- 1}). \tag{78}\]

Substituting (78) into (77), we get

\[2\Sigma_{t}^{-1}=2Q^{-1}+\Sigma_{t}^{-1}\dot{\Sigma}_{t}\Sigma_{t}^{-1}.\]

Multiplying by \(\Sigma_{t}^{2}\) and using the fact that \(\Sigma_{t}\) and \(Q\) commute, we can see that the algebraic Riccati equation (5) holds.

For

\[\Sigma_{t}=I+\frac{\eta(1-e^{-2t})}{1+\eta e^{-2t}}\mathbf{v}\mathbf{v}^{\top},\]

we apply the Sherman-Morrison formula:

\[(\Sigma_{t})^{-1}=I-\frac{\frac{\eta(1-e^{-2t})}{1+\eta e^{-2t}}}{1+\frac{\eta (1-e^{-2t})}{1+\eta e^{-2t}}}\mathbf{v}\mathbf{v}^{\top}=I-\frac{\eta(1-e^{-2t})}{1+ \eta}\mathbf{v}\mathbf{v}^{\top}.\]

Thus,

\[2\Sigma_{t}^{-1}\big{(}Q^{-1}-\Sigma_{t}^{-1}\big{)}\Sigma_{t} =2\bigg{(}I-\frac{\eta(1-e^{-2t})}{1+\eta}\mathbf{v}\mathbf{v}^{\top} \bigg{)}\bigg{(}-\frac{\eta e^{-2t}}{1+\eta}\mathbf{v}\mathbf{v}^{\top}\bigg{)}\bigg{(} I+\frac{\eta(1-e^{-2t})}{1+\eta e^{-2t}}\mathbf{v}\mathbf{v}^{\top}\bigg{)}\] \[=-\frac{2\eta e^{-2t}}{1+\eta}\mathbf{v}\mathbf{v}^{\top}=\partial_{t} \big{(}\Sigma_{t}^{-1}\big{)}.\]

Moreover,

\[\big{(}\Sigma_{t}^{-1}-Q^{-1}\big{)}\Sigma_{t}=\frac{\eta e^{-2t}}{1+\eta}\bm {v}\mathbf{v}^{\top}\bigg{(}I+\frac{\eta(1-e^{-2t})}{1+\eta e^{-2t}}\mathbf{v}\mathbf{v}^{ \top}\bigg{)}=\frac{\eta e^{-2t}}{1+\eta e^{-2t}}\mathbf{v}\mathbf{v}^{\top}.\]

Thus,

\[2\operatorname{tr}\bigl{(}\big{(}\Sigma_{t}^{-1}-Q^{-1}\big{)}\Sigma_{t} \big{)}=\frac{2\eta e^{-2t}}{1+\eta e^{-2t}}\operatorname{tr}(\mathbf{v}\mathbf{v}^{ \top})=\frac{\eta e^{-2t}}{1+\eta e^{-2t}}\operatorname{tr}(\mathbf{v}^{\top}\mathbf{ v})=\frac{\eta e^{-2t}}{1+\eta e^{-2t}}.\]

On the other hand,

\[\det(\Sigma_{t})=1+\frac{\eta(1-e^{-2t})}{1+\eta e^{-2t}}\mathbf{v}^{\top}\mathbf{v}= \frac{1+\eta}{1+\eta e^{-2t}}.\]

Thus,

\[\partial_{t}\log\det(\Sigma_{t})=-\frac{-2\eta e^{-2t}}{1+\eta e^{-2t}}=\frac{ 2\eta e^{-2t}}{1+\eta e^{-2t}}.\]Therefore,

\[\rho_{t}=(2\pi)^{-d/2}\big{(}\det(\Sigma_{t})\big{)}^{-1/2}\exp\left(-\frac{1}{2} \mathbf{x}^{\top}\Sigma_{t}^{-1}\mathbf{x}\right)\]

with

\[\Sigma_{t}=I+\frac{\eta(1-e^{-2t})}{1+\eta e^{-2t}}\mathbf{v}\mathbf{v}^{\top},\]

is a solution with the initial condition \(\Sigma_{0}=I_{d}\). The theorem follows by the uniqueness of the solution of the mean field PDE (Theorem C.2). 

Proof of Theorem 3.4.: Similar to the proof of Theorem 3.1, we have

\[\dot{\mathbf{\mu}}_{t}=-\nabla_{\mathbf{\mu}_{t}}\widetilde{E}(\mathbf{\mu}_{t},\Sigma_{t}) =-Q^{-1}(\mathbf{\mu}_{t}-\mathbf{b})\]

and

\[\dot{\Sigma}_{t} =-2\Sigma_{t}^{2}((1-\nu)\Sigma_{t}+\nu I)^{-1}\nabla_{\Sigma_{t }}\widetilde{E}(\Sigma_{t})-2\nabla_{\Sigma_{t}}\widetilde{E}(\Sigma_{t}) \Sigma_{t}^{2}((1-\nu)\Sigma_{t}+\nu I)^{-1}\] \[\Leftrightarrow \dot{\Sigma}_{t} =2((1-\nu)\Sigma_{t}+\nu I)^{-1}\Sigma_{t}-((1-\nu)\Sigma_{t}+ \nu I)^{-1}\Sigma_{t}^{2}Q^{-1}-Q^{-1}((1-\nu)\Sigma_{t}+\nu I)^{-1}\Sigma_{t }^{2}.\]

Following the arguments similar to the proof of Theorem 3.1, we can show

* (8) has a unique global solution,
* \(\rho_{t}\) is the density of \(\mathcal{N}(\mathbf{\mu}_{t},\Sigma_{t})\) given by (8),
* \(\dot{\widetilde{E}}(\theta_{t})\leq 0\) and \(\dot{\widetilde{E}}(\mathbf{\mu}_{t},\Sigma_{t})\to 0\) as \(t\to\infty\),
* \(\rho_{t}\) converges weakly to \(\rho^{*}\).

Finally suppose \(\Sigma_{0}Q=Q\Sigma_{0}\), then \(\Sigma_{t}\) also commutes with \(Q\) since \(0\) is a solution of the ODE satisfied by \(\Sigma_{t}Q-Q\Sigma_{t}\) and the solution is unique. Thus, we can diagonalize \(\Sigma_{t}\) and \(Q\) simutaneously. Then there exists orthogonal matrix \(P\) such that

\[\Sigma_{t}=P^{\top}\operatorname{diag}\left\{\sigma_{1}^{(t)},\cdots,\sigma_{ d}^{(t)}\right\}P,\quad Q=P^{\top}\operatorname{diag}\left\{\lambda_{1}, \cdots,\lambda_{d}\right\}P.\]

And (9) reduces to

\[\dot{\sigma}_{i}^{(t)}=\frac{2\sigma_{i}^{(t)}(\lambda_{i}-\sigma_{i}^{(t)})}{ \lambda_{i}\left((1-\nu)\sigma_{i}^{(t)}+\nu\right)}.\]

Solving this ODE, we get

\[\frac{(\sigma_{i}^{(t)}-\lambda_{i})^{(1-\nu)\lambda_{i}+\nu}}{(\sigma_{i}^{(t )})^{\nu}}=\frac{(\sigma_{i}^{(0)}-\lambda_{i})^{(1-\nu)\lambda_{i}+\nu}}{( \sigma_{i}^{(0)})^{\nu}}e^{-2t}.\]

Thus, we have \(\sigma_{i}^{(t)}\to\lambda_{i}\) as \(t\to\infty\) and

\[\left|\sigma_{i}^{(t)}-\lambda_{i}\right|=\mathcal{O}\left(e^{-2t/((1-\nu) \lambda_{i}+\nu)}\right).\]

In particular, we conclude \(\|\Sigma_{t}-Q\|=\mathcal{O}\left(e^{-2t/((1-\nu)\lambda+\nu)}\right)\) where \(\lambda\) is the largest eigenvalue of \(Q\). 

Proof of Theorem 3.5.: First, we define the Hamiltonian on the centered Gaussian submanifold by

\[\mathcal{H}(\Sigma_{t},S_{t}):=\frac{1}{2}\operatorname{tr}\left(S_{t}(G_{ \Sigma_{t}}^{-1}S_{t})\right)+E(\Sigma_{t}).\]

By Corollary B.5 we have

\[G_{\Sigma}^{-1}S=2(\Sigma^{2}S+S\Sigma^{2}).\]

Thus, the Hamiltonian is reduced to

\[\mathcal{H}(\Sigma_{t},S_{t})=2\operatorname{tr}\left(\Sigma_{t}^{2}S_{t}^{2} \right)+\frac{1}{2}\left(\operatorname{tr}(Q^{-1}\Sigma_{t})-\log\det(Q^{-1} \Sigma_{t})-d\right).\]

[MISSING_PAGE_FAIL:44]

We remark that the fact the Stein AIG flow remains Gaussian is highly non-trivial. In fact it requires \(\frac{\delta\mathcal{H}}{\delta\rho_{t}}\) to lie in the cotangent space of the Gaussian submanifold. One sufficient condition is that the variational derivatives of both the kinetic and potential energies lie in this same space, and the former could be interpreted as: The Gaussian submanifold is totally geodesic under the given metric, meaning that any geodesic flow with an initial position and velocity chosen from the Gaussian submanifold remains Gaussian. Fortunately both the Wasserstein metric and the Stein metric satisfy this property.

## Appendix J Proofs for Section 3.2

Proof of Theorem 3.6.: From the proof of Theorem 3.1 we know that (16) has a unique solution that is continuous in \(t\) and bounded for any \(t\in[0,T]\) with \(T<\infty\). Thus, the linear system (15) also has a unique solution.

Now we check that the sample mean \(\mathbf{\mu}_{t}\) and covariance matrix \(C_{t}\) satisfy (16). We simplify the right-han-side of (13) using \(\mathbf{\mu}_{t}\) and \(C_{t}\).

\[RHS =\mathbf{x}_{i}^{(t)}-\frac{1}{N}\sum_{j=1}^{N}\bigl{(}(\mathbf{x}_{i}^{(t )})^{\top}\mathbf{x}_{j}^{(t)}+1\bigr{)}Q^{-1}(\mathbf{x}_{j}^{(t)}-\mathbf{b})\] \[=\mathbf{x}_{i}^{(t)}-\frac{1}{N}\sum_{j=1}^{N}Q^{-1}(\mathbf{x}_{j}^{(t) }-\mathbf{b})\bigl{(}(\mathbf{x}_{i}^{(t)})^{\top}\mathbf{x}_{j}^{(t)}+1\bigr{)}\] \[=\mathbf{x}_{i}^{(t)}-Q^{-1}\frac{1}{N}\sum_{j=1}^{N}\mathbf{x}_{j}^{(t)} \bigl{(}(\mathbf{x}_{j}^{(t)})^{\top}\mathbf{x}_{i}^{(t)}+1\bigr{)}+Q^{-1}\mathbf{b}\frac{1 }{N}\sum_{j=1}^{N}\bigl{(}(\mathbf{x}_{j}^{(t)})^{\top}\mathbf{x}_{i}^{(t)}+1\bigr{)}\] \[=(I-Q^{-1}(C_{t}+\mathbf{\mu}_{t}\mathbf{\mu}_{t}^{\top})+Q^{-1}\mathbf{b}\bm {\mu}_{t}^{\top})\mathbf{x}_{i}^{(t)}-Q^{-1}\mathbf{\mu}_{t}+Q^{-1}\mathbf{b}. \tag{79}\]

Let \(X_{t}=(\mathbf{x}_{1}^{(t)},\cdots,\mathbf{x}_{N}^{(t)})^{\top}\). Then we have that

\[\mathbf{\mu}_{t}=\frac{X_{t}^{\top}\mathbf{1}}{N},\quad C_{t}=\frac{X_{t}^{\top}X_{t}}{ N}-\mathbf{\mu}_{t}\mathbf{\mu}_{t}^{\top}.\]

Then (79) can be written in the matrix form as

\[\dot{X}_{t}=X_{t}(I-(C_{t}+\mathbf{\mu}_{t}\mathbf{\mu}_{t}^{\top})Q^{-1}+\mathbf{\mu}_{t} \mathbf{b}^{\top}Q^{-1})-\mathbf{1}\mathbf{\mu}_{t}^{\top}Q^{-1}+\mathbf{1}\mathbf{b}^{\top}Q^{-1}. \tag{80}\]

Multiplying by \(\mathbf{1}^{\top}/N\) on the left, we get

\[\dot{\mathbf{\mu}}_{t}^{\top}=\mathbf{\mu}_{t}^{\top}-\mathbf{\mu}_{t}^{\top}(C_{t}+\mathbf{ \mu}_{t}\mathbf{\mu}_{t}^{\top})Q^{-1}+\mathbf{\mu}_{t}^{\top}\mathbf{\mu}_{t}\mathbf{b}^{\top }Q^{-1}-(\mathbf{\mu}_{t}-\mathbf{b})^{\top}Q^{-1}.\]

Thus, we have

\[\dot{\mathbf{\mu}}_{t}=(I-Q^{-1}C_{t})\mathbf{\mu}_{t}-(1+\mathbf{\mu}_{t}^{\top}\mathbf{\mu} _{t})Q^{-1}(\mathbf{\mu}_{t}-\mathbf{b}).\]

Note that \(\dot{C}_{t}=(\dot{X}_{t}^{\top}X_{t}+X_{t}^{\top}\dot{X}_{t})/N-\mathbf{\mu}_{t} \dot{\mathbf{\mu}}_{t}-\dot{\mathbf{\mu}}_{t}\mathbf{\mu}_{t}^{\top}\). Substituting (80) we obtain

\[\dot{C}_{t}=2C_{t}-C_{t}\left(C_{t}+\mathbf{\mu}_{t}(\mathbf{\mu}_{t}-\mathbf{b})^{\top} \right)Q^{-1}-Q^{-1}\left(C_{t}+(\mathbf{\mu}_{t}-\mathbf{b})\mathbf{\mu}_{t}^{\top}\right) C_{t}.\]

Next we show that (14) and (15) satisfies (13).

\[RHS =(I-Q^{-1}(C_{t}+\mathbf{\mu}_{t}\mathbf{\mu}_{t}^{\top})+Q^{-1}\mathbf{b}\bm {\mu}_{t}^{\top})\mathbf{x}_{i}^{(t)}-Q^{-1}\mathbf{\mu}_{t}+Q^{-1}\mathbf{b}\] \[=(I-Q^{-1}(C_{t}+\mathbf{\mu}_{t}\mathbf{\mu}_{t}^{\top})+Q^{-1}\mathbf{b}\bm {\mu}_{t}^{\top})\left(\mathbf{x}_{i}^{(t)}-\mathbf{\mu}_{t}\right)+\] \[\qquad(I-Q^{-1}C_{t})\mathbf{\mu}_{t}-(1+\mathbf{\mu}_{t}^{\top}\mathbf{\mu} _{t})Q^{-1}(\mathbf{\mu}_{t}-\mathbf{b})\] \[=(I-Q^{-1}(C_{t}+\mathbf{\mu}_{t}\mathbf{\mu}_{t}^{\top})+Q^{-1}\mathbf{b}\bm {\mu}_{t}^{\top})A_{t}(\mathbf{x}_{i}^{(0)}-\mathbf{\mu}_{0})+\dot{\mathbf{\mu}}_{t}\] \[=\dot{A}_{t}(\mathbf{x}_{i}^{(0)}-\mathbf{\mu}_{0})+\dot{\mathbf{\mu}}_{t}= \dot{\mathbf{x}}_{i}^{(t)}.\]

By Theorem C.3 (13) has a unique solution. Thus, the solution of (13) is given by (14)-(16). 

The proof of Theorem 3.7 is quite long and tedious so we defer it to Appendix L.

Proof of Theorem 3.8.: If \(C_{0}\) is non-singular, this is a direct corollary of Theorem 3.6 and Theorem 3.2. To also accommodate for the singular case, we provide a direct proof by calculation. Since \(C_{0}Q=QC_{0}\), we know that \(C_{0}\) and \(Q\) are simultaneously diagonalizable. There exists some orthogonal matrix \(P_{0}\) such that we have the spectral decompositions

\[C_{0}=P_{0}^{\top}D_{0}P_{0},\quad Q=P_{0}^{\top}Q_{0}P_{0},\]

where \(D_{0}=\operatorname{diag}(\lambda_{1}^{(0)},\cdots,\lambda_{d}^{(0)})\) and \(Q_{0}=\operatorname{diag}(q_{1},\cdots,q_{d})\) are diagonal matrices. Let \(D_{t}:=P_{0}C_{t}P_{0}^{\top}\). (18) can be rewritten as

\[\mathbf{x}_{i}^{(t)}=P_{0}^{\top}\operatorname{diag}\bigl{(}(e^{-2t}+(1-e^{-2t}) \lambda_{1}^{(0)}/q_{1})^{-1/2},\cdots,(e^{-2t}+(1-e^{-2t})\lambda_{d}^{(0)}/q _{d})^{-1/2}\bigr{)}P_{0}\mathbf{x}_{i}^{(0)}.\]

Thus, by taking the derivative with respect to \(t\), we obtain

\[\dot{\mathbf{x}}_{i}^{(t)} =e^{-2t}P_{0}^{\top}\operatorname{diag}\Biggl{(}\frac{1-\lambda_{ 1}^{(0)}/q_{1}}{(e^{-2t}+(1-e^{-2t})\lambda_{1}^{(0)}/q_{1})^{3/2}},\cdots, \frac{1-\lambda_{d}^{(0)}/q_{d}}{(e^{-2t}+(1-e^{-2t})\lambda_{d}^{(0)}/q_{d})^ {3/2}}\Biggr{)}P_{0}\mathbf{x}_{i}^{(0)}\] \[=U\bigl{(}e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0}\bigr{)}^{-3/2}\mathbf{x}_{i }^{(0)},\]

where

\[U=e^{-2t}P_{0}^{\top}\operatorname{diag}\bigl{(}1-\lambda_{1}^{(0)}/q_{1}, \cdots,1-\lambda_{d}^{(0)}/q_{d}\bigr{)}P_{0}=e^{-2t}(I-Q^{-1}C_{0}).\]

On the other hand, we check that

\[\mathbf{x}_{i}^{(t)}-\frac{1}{N}\sum_{j=1}^{N}\bigl{(}(\mathbf{x}_{i}^{( t)})^{\top}\mathbf{x}_{j}^{(t)}+1\bigr{)}Q^{-1}\mathbf{x}_{j}^{(t)}\] \[= \bigl{(}e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0}\bigr{)}^{-1/2}\mathbf{x}_{i}^ {(0)}-\frac{1}{N}\sum_{j=1}^{N}\bigl{(}(\mathbf{x}_{i}^{(0)})^{\top}\bigl{(}e^{-2t }I+(1-e^{-2t})Q^{-1}C_{0}\bigr{)}^{-1}\mathbf{x}_{j}^{(0)}+1\bigr{)}\cdot\] \[\qquad Q^{-1}\bigl{(}e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0}\bigr{)}^{-1/ 2}\mathbf{x}_{j}^{(0)}\] \[= \bigl{(}e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0}\bigr{)}^{-1/2}\mathbf{x}_{i}^ {(0)}-\frac{1}{N}\sum_{j=1}^{N}Q^{-1}\bigl{(}e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0} \bigr{)}^{-1/2}\mathbf{x}_{j}^{(0)}\] \[\qquad-\frac{1}{N}\sum_{j=1}^{N}(\mathbf{x}_{j}^{(0)})^{\top}\bigl{(} e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0}\bigr{)}^{-1}\mathbf{x}_{i}^{(0)}\cdot\] \[\qquad Q^{-1}\bigl{(}e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0}\bigr{)}^{-1/ 2}\mathbf{x}_{j}^{(0)}\] \[= \bigl{(}e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0}\bigr{)}^{-1/2}\mathbf{x}_{i}^ {(0)}\] \[= \bigl{(}e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0}\bigr{)}^{-1/2}\mathbf{x}_{i}^ {(0)}\] \[= \bigl{(}e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0}\bigr{)}^{-1/2}C_{0}\bigl{(} e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0}\bigr{)}^{-1}\mathbf{x}_{i}^{(0)}\] \[= e^{-2t}(I-Q^{-1}C_{0})\bigl{(}e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0} \bigr{)}^{-3/2}\mathbf{x}_{i}^{(0)}\] \[= U\bigl{(}e^{-2t}I+(1-e^{-2t})Q^{-1}C_{0}\bigr{)}^{-3/2}\mathbf{x}_{i} ^{(0)}.\]

Thus, we conclude that (18) is a solution of (13). By Theorem C.3, the solution for (13) is unique and hence the theorem follows. 

Proof of Theorem 3.9.: For any particle system of the R-SVGF, we can derive that

\[\dot{X}_{t}= \left((1-\nu)\biggl{(}\frac{X_{t}X_{t}^{\top}}{N}+\frac{\mathbf{1} \mathbf{1}^{\top}}{N}\biggr{)}+\nu I_{d}\right)^{-1}\left(X_{t}-\frac{1}{N}(X_{t}X _{t}^{\top}+\mathbf{1}\mathbf{1}^{\top})X_{t}Q^{-1}\right)\] \[= \left((1-\nu)\Bigl{(}\frac{X_{t}X_{t}^{\top}}{N}+\frac{\mathbf{1}\mathbf{1 }^{\top}}{N}\Bigr{)}+\nu I_{d}\right)^{-1}X_{t}(I-C_{t}Q^{-1}).\]By Sherman-Morrison formula we have

\[\bigg{(}\nu I_{d}+(1-\nu)\frac{\mathbf{1}\mathbf{1}^{\top}}{N}\bigg{)}^{-1}=\frac{ 1}{\nu}I_{d}-\frac{1-\nu}{\nu}\frac{\mathbf{1}\mathbf{1}^{\top}}{N}.\]

By Woodbury matrix identity we derive

\[\bigg{(}(1-\nu)\Big{(}\frac{X_{t}X_{t}^{\top}}{N}+\frac{\mathbf{1} \mathbf{1}^{\top}}{N}\Big{)}+\nu I_{d}\bigg{)}^{-1}\] \[= \bigg{(}\nu I_{d}+(1-\nu)\frac{\mathbf{1}\mathbf{1}^{\top}}{N} \bigg{)}^{-1}-\bigg{(}\nu I_{d}+(1-\nu)\frac{\mathbf{1}\mathbf{1}^{\top}}{N} \bigg{)}^{-1}X_{t}.\] \[\quad\frac{1}{N}\left(\frac{1}{1-\nu}I_{d}+X_{t}^{\top}\bigg{(} \nu I_{d}+(1-\nu)\frac{\mathbf{1}\mathbf{1}^{\top}}{N}\bigg{)}^{-1}X_{t}\right) ^{-1}X_{t}^{\top}\bigg{(}\nu I_{d}+(1-\nu)\frac{\mathbf{1}\mathbf{1}^{\top}}{N }\bigg{)}^{-1}\] \[= \bigg{(}\frac{1}{\nu}I_{d}-\frac{1-\nu}{\nu}\frac{\mathbf{1} \mathbf{1}^{\top}}{N}\bigg{)}-\frac{1}{N\nu^{2}}X_{t}\bigg{(}\frac{1}{1-\nu}I_{ d}+\frac{1}{\nu}C_{t}\bigg{)}^{-1}X_{t}^{\top}.\]

Substituting this into (81), we have

\[\dot{X}_{t}= \bigg{(}\frac{1}{\nu}I_{d}-\frac{1-\nu}{\nu}\frac{\mathbf{1} \mathbf{1}^{\top}}{N}\bigg{)}X_{t}(I-C_{t}Q^{-1})-\frac{1}{N\nu^{2}}X_{t} \bigg{(}\frac{1}{1-\nu}I_{d}+\frac{1}{\nu}C_{t}\bigg{)}^{-1}X_{t}^{\top}X_{t}(I -C_{t}Q^{-1})\] \[= \frac{1}{\nu}X_{t}-\frac{1}{\nu}X_{t}C_{t}Q^{-1}-\frac{1}{\nu}X_{t }\bigg{(}\frac{1}{1-\nu}I_{d}+\frac{1}{\nu}C_{t}\bigg{)}^{-1}\bigg{(}\frac{1}{ \nu}C_{t}-\frac{1}{\nu}C_{t}^{2}Q^{-1}\bigg{)}\] \[= \frac{1}{\nu}X_{t}\left(I_{d}-\bigg{(}\frac{1}{1-\nu}I_{d}+\frac{ 1}{\nu}C_{t}\bigg{)}^{-1}\frac{1}{\nu}C_{t}\right)(I-C_{t}Q^{-1})\] \[= X_{t}(\nu I_{d}+(1-\nu)C_{t})^{-1}(I-C_{t}Q^{-1}) \tag{81}\]

Multiplying by \(X_{t}^{\top}\) on the left we get

\[\frac{X_{t}^{\top}\dot{X}_{t}}{N}=(\nu I_{d}+(1-\nu)C_{t})^{-1}C_{t}(I-C_{t}Q^ {-1}).\]

Thus, the derivative of covariance matrix \(C_{t}\) is given by

\[\dot{C}_{t}=\frac{X_{t}^{\top}\dot{X}_{t}}{N}+\frac{\dot{X}_{t}^ {\top}X_{t}}{N}\] \[= (\nu I_{d}+(1-\nu)C_{t})^{-1}C_{t}(I-C_{t}Q^{-1})+(I-Q^{-1}C_{t}) (\nu I_{d}+(1-\nu)C_{t})^{-1}C_{t}\] \[= 2(\nu I_{d}+(1-\nu)C_{t})^{-1}C_{t}-(\nu I_{d}+(1-\nu)C_{t})^{-1} C_{t}^{2}Q^{-1}-Q^{-1}(\nu I_{d}+(1-\nu)C_{t})^{-1}C_{t}^{2}.\]

Next we show that (19) and (20) satisfies (81).

\[RHS=X_{0}A_{t}^{\top}\left(\nu I_{d}+(1-\nu)C_{t}\right)^{-1}(I-C_{t}Q^{-1})= X_{0}\dot{A}_{t}^{\top}=\dot{X}_{t}.\]

Similar to the proof of Theorem 3.4, it could be shown that the R-SVGF also has a unique solution and the proof is complete. 

Next we show Theorem 3.10.

**Lemma J.1**.: _The covariance matrix \(C_{t}\) (\(t=1,2,\cdots\)) of the discrete-time finite particle system satisfies the following equation_

\[C_{t+1}=\big{(}I+\epsilon(I-Q^{-1}C_{t})\big{)}C_{t}\big{(}I+\epsilon(I-Q^{-1} C_{t})\big{)}^{\top}.\]

Proof.: Let \(X=\left(\mathbf{x}_{1},\cdots,\mathbf{x}_{N}\right)^{\top}\). Then (21) can be written as

\[X_{t+1}=X_{t}+\epsilon X_{t}(I-C_{t}Q^{-1}).\]

Thus, we have

\[C_{t+1}=\frac{X_{t}^{\top}X_{t}}{N}=\big{(}I+\epsilon(I-Q^{-1}C_{t})\big{)}C_{ t}\big{(}I+\epsilon(I-Q^{-1}C_{t})\big{)}^{\top}.\]Proof of Theorem 3.10.: First since \(C_{0}Q=QC_{0}\) they are simultaneously diagonalizable. By Theorem 3.8 any \(C_{t}\) and \(Q\) are simultaneously diagonalizable. Thus, without loss of generality we assume all \(C_{t}\) and \(Q\) are diagonal matrices. Then by Lemma J.1 we know

\[Q^{-1}C_{t+1}=(I+\epsilon(I-Q^{-1}C_{t}))^{2}Q^{-1}C_{t}.\]

If we define \(f_{\epsilon}(x)=(1+\epsilon(1-x))^{2}x\) then every entry (i.e., eigenvalue) of \(Q^{-1}C_{t}\) follows the \(f_{\epsilon}\)-iteration trajectory of the corresponding entry of \(Q^{-1}C_{0}\).

The fixed points of \(f_{\epsilon}(x)=(1+\epsilon(1-x))^{2}x\) are \(\{0,1,2/\epsilon+1\}\). If \(0<\epsilon<1\) then \(|f^{\prime}(0)|>1\), \(|f^{\prime}(2/\epsilon+1)|>1\), and \(|f^{\prime}(1)|<1\). By Proposition 1.9 of [26], \(1\) is an attracting fixed point while \(0\) and \(2/\epsilon+1\) are repelling fixed points. By definition there exists an interval around \(1\) such that for all initial points in that interval, the trajectory of any eigenvalue of \(Q^{-1}C_{t}\) converges to \(1\). We now quantify that result further.

Note that \(f^{\prime}_{\epsilon}(x)=(1+\epsilon-\epsilon x)(1+\epsilon-3\epsilon x)\) is an upward-opening parabola whose zeros are \(1/3+1/(3\epsilon)\) and \(1+\epsilon\). Thus, \(f^{\prime}_{\epsilon}\) is monotone decreasing on \([0,1/3+1/(3\epsilon)]\). In particular, both equation \(f^{\prime}_{\epsilon}(x)=1\) and \(f^{\prime}_{\epsilon}(x)=1-\epsilon\) have two distinct roots, the smaller ones lying on \((0,1/3+1/(3\epsilon))\). Define \(w_{\epsilon}\) to be the smaller root of \(f^{\prime}_{\epsilon}(x)=1\) and \(u_{\epsilon}\), the smaller root of \(f^{\prime}_{\epsilon}(x)=1-\epsilon\). Since \(f^{\prime}_{\epsilon}\) is monotone descreasing and \(f^{\prime}_{\epsilon}(1)=1-2\epsilon\), we have \(0<w_{\epsilon}<u_{\epsilon}<1\).

Thus, for any \(x\in[w_{\epsilon},1/3+1/(3\epsilon)]\) we have \(0\leq f^{\prime}_{\epsilon}(x)\leq 1\) and \(0\leq f_{\epsilon}(x)\leq f_{\epsilon}(1/3+1/(3\epsilon))<1/3+1/(3\epsilon)\) (here we have used the condition \(\epsilon<0.5\)). On the other hand, since \(f^{\prime}_{\epsilon}(0)\geq 1\) and \(0\) and \(1\) are two fixed points, it holds that \(f_{\epsilon}(x)\geq x\) for any \(x\in[0,1]\), which implies that for any \(x\in[w_{\epsilon},1/3+1/(3\epsilon)]\) we have \(f_{\epsilon}(x)\geq f_{\epsilon}(w_{\epsilon})\geq w_{\epsilon}\). Hence we know that \(f^{\prime}_{\epsilon}\) is a contraction map and \(f_{\epsilon}([w_{\epsilon},1/3+1/(3\epsilon)])\subset[w_{\epsilon},1/3+1/(3 \epsilon)]\), which implies that there is a unique fixed point (i.e., \(1\)) such that the \(f_{\epsilon}\)-iteration trajectory converges to it.

Next we prove that for any \(x\in(0,1+1/\epsilon)\) the trajectory falls into the interval \([w_{\epsilon},1/3+1/(3\epsilon)]\) after finite iterations. If \(x\in(1/3+1/(3\epsilon),1+1/\epsilon)\) then after one iteration we get \(f_{\epsilon}(x)\in(0,f_{\epsilon}(1/3+1/(3\epsilon)))\subset(0,1/3+1/(3 \epsilon))\). We claim that if \(x\in(0,w_{\epsilon})\) then after \(t_{0}:=\left\lceil\frac{\log w_{\epsilon}/x}{2\log(1+\epsilon(1-w_{ \epsilon}))}\right\rceil\) we have \(f^{(t_{0})}_{\epsilon}(x)\in[w_{\epsilon},1]\). Firstly \(f^{(t)}_{\epsilon}(x)\leq 1\) for any \(t\). It suffices to prove \(f^{(t_{0})}_{\epsilon}(x)\geq w_{\epsilon}\). Suppose \(f^{(t_{0})}_{\epsilon}(x)<w_{\epsilon}\). Then for any \(0\leq t\leq t_{0}\) we have \(f^{(t)}_{\epsilon}(x)<w_{\epsilon}\). By definition of \(t_{0}\) we know

\[f^{(t_{0})}_{\epsilon}(x)>(1+\epsilon(1-w_{\epsilon}))^{2t_{0}}x\geq w_{ \epsilon}.\]

This is a contradiction. Thus, the claim holds and in conclusion for any \(x\in(0,1+1/\epsilon)\) the \(f_{\epsilon}\)-iteration trajectory converges to the fixed point \(1\).

Finally we consider the case when \(x\in[u_{\epsilon},1/3+1/(3\epsilon)]\). Since \(f^{\prime}_{\epsilon}\) is monotone descreasing here we have \(f^{\prime}_{\epsilon}(x)\in[0,1-\epsilon]\). And by similar argument we know \(f_{\epsilon}([u_{\epsilon},1/3+1/(3\epsilon)])\subset[u_{\epsilon},1/3+1/(3 \epsilon)]\). Thus, we conclude

\[|f^{(t)}_{\epsilon}(x)-1|\leq(1-\epsilon)|f^{(t-1)}_{\epsilon}(x)-1|\leq \cdots\leq(1-\epsilon)^{t}|x-1|\leq e^{-\epsilon t}|x-1|.\]

Figure 6: Plot of the function \(f_{\epsilon}(x)=(1+\epsilon(1-x))^{2}x\).

Proofs for Section 4

The following result was derived in [43]. We provide the proof below for completeness.

**Lemma K.1**.: _Let \(\rho^{*}\) be a probability measure and \(\rho_{\theta}\) be a Gaussian measure with parameters \(\theta=(\mathbf{\mu},\Sigma)\). Then we have the following expressions:_

\[\nabla_{\mathbf{\mu}}\operatorname{KL}(\rho_{\theta}\parallel\rho^{*})=\mathbb{E}_{ \mathbf{x}\sim\rho_{\theta}}[\nabla V(\mathbf{x})],\quad\nabla_{\Sigma}\operatorname{ KL}(\rho_{\theta}\parallel\rho^{*})=\frac{1}{2}\left(\mathbb{E}_{\mathbf{x}\sim\rho_{ \theta}}[\nabla^{2}V(\mathbf{x})]-\Sigma^{-1}\right). \tag{82}\]

Proof.: We compute that

\[\nabla_{\mathbf{\mu}}\operatorname{KL}(\rho_{\theta}\parallel\rho^{*}) =\nabla_{\mathbf{\mu}}\int\log\frac{\rho_{\theta}(\mathbf{x})}{\rho^{*}( \mathbf{x})}\rho_{\theta}(\mathbf{x})\operatorname{d}\!\mathbf{x}\] \[=\int\frac{\nabla_{\mathbf{\mu}}\rho_{\theta}(\mathbf{x})}{\rho_{\theta}( \mathbf{x})}\rho_{\theta}(\mathbf{x})\operatorname{d}\!\mathbf{x}+\int\log\frac{\rho_{ \theta}(\mathbf{x})}{\rho^{*}(\mathbf{x})}\nabla_{\mathbf{\mu}}\rho_{\theta}(\mathbf{x}) \operatorname{d}\!\mathbf{x}\] \[\stackrel{{*}}{{=}}\nabla_{\mathbf{\mu}}\int\rho_{\theta }(\mathbf{x})\operatorname{d}\!\mathbf{x}-\int\log\frac{\rho_{\theta}(\mathbf{x})}{\rho^{* }(\mathbf{x})}\nabla_{\mathbf{\alpha}}\rho_{\theta}(\mathbf{x})\operatorname{d}\!\mathbf{x}\] \[=\int\nabla_{\mathbf{x}}\log\frac{\rho_{\theta}(\mathbf{x})}{\rho^{*}(\mathbf{ x})}\rho_{\theta}(\mathbf{x})\operatorname{d}\!\mathbf{x}\] \[=\int\nabla_{\mathbf{x}}\rho_{\theta}(\mathbf{x})\operatorname{d}\!\mathbf{x }-\int\nabla_{\mathbf{x}}\log\rho^{*}(\mathbf{x})\cdot\rho_{\theta}(\mathbf{x}) \operatorname{d}\!\mathbf{x}\] \[=\mathbb{E}_{\mathbf{x}\sim\rho_{\theta}}[\nabla V(\mathbf{x})],\]

where we have used the fact that \(\rho_{\theta}\) is a Gaussian density in \(*\). Similarly, we have

\[\nabla_{\Sigma}\operatorname{KL}(\rho_{\theta}\parallel\rho^{*}) =\nabla_{\Sigma}\int\log\frac{\rho_{\theta}(\mathbf{x})}{\rho^{*}(\bm {x})}\rho_{\theta}(\mathbf{x})\operatorname{d}\!\mathbf{x}\] \[=\int\frac{\nabla_{\Sigma}\rho_{\theta}(\mathbf{x})}{\rho_{\theta}( \mathbf{x})}\rho_{\theta}(\mathbf{x})\operatorname{d}\!\mathbf{x}+\int\log\frac{\rho_{ \theta}(\mathbf{x})}{\rho^{*}(\mathbf{x})}\nabla_{\Sigma}\rho_{\theta}(\mathbf{x}) \operatorname{d}\!\mathbf{x}\] \[\stackrel{{(a)}}{{=}}\int\log\frac{\rho_{\theta}(\bm {x})}{\rho^{*}(\mathbf{x})}\left(-\frac{1}{2}\Sigma^{-1}+\frac{1}{2}\Sigma^{-1}( \mathbf{x}-\mathbf{\mu})(\mathbf{x}-\mathbf{\mu})^{\top}\Sigma^{-1}\right)\rho_{\theta}(\mathbf{x} )\operatorname{d}\!\mathbf{x}\] \[\stackrel{{(b)}}{{=}}\frac{1}{2}\int\log\frac{\rho_ {\theta}(\mathbf{x})}{\rho^{*}(\mathbf{x})}\nabla_{\mathbf{x}}^{2}\rho_{\theta}(\mathbf{x}) \operatorname{d}\!\mathbf{x}\] \[=\frac{1}{2}\int\nabla_{\mathbf{x}}^{2}\log\frac{\rho_{\theta}(\mathbf{x })}{\rho^{*}(\mathbf{x})}\cdot\rho_{\theta}(\mathbf{x})\operatorname{d}\!\mathbf{x}\] \[=\frac{1}{2}\left(\mathbb{E}_{\mathbf{x}\sim\rho_{\theta}}[\nabla^{2} V(\mathbf{x})]-\Sigma^{-1}\right).\]

Here again in \((a)\) and \((b)\) we have used the closed-form expression of Gaussian densities. Thus,

\[\nabla_{\mathbf{\mu}}E(\mathbf{\mu},\Sigma)=\mathbb{E}_{\mathbf{x}\sim\rho_{\theta}}[ \nabla V(\mathbf{x})],\quad\nabla_{\Sigma}E(\mathbf{\mu},\Sigma)=\frac{1}{2}\left( \mathbb{E}_{\mathbf{x}\sim\rho_{\theta}}[\nabla^{2}V(\mathbf{x})]-\Sigma^{-1}\right).\]

Proof of Theorem 4.1.: By definition of Gaussian approximate gradient descent, we have that

\[\dot{\theta}_{t}=-G_{\theta_{t}}^{-1}(\nabla_{\theta_{t}}E(\theta_{t})),\ \text{where}\ E(\theta_{t})=\operatorname{KL}(\rho_{\theta_{t}}\parallel\rho^{*})\ \text{and}\ \theta=(\mathbf{\mu},\Sigma).\]

Applying Theorem B.3, we obtain that \(\dot{\theta}_{t}=-G_{\theta_{t}}^{-1}(\nabla_{\theta_{t}}E(\theta_{t}))\) is given by

\[\begin{cases}\dot{\mathbf{\mu}}_{t}=-\left(2\nabla_{\Sigma_{t}}E(\mathbf{\mu}_{t}, \Sigma_{t})\Sigma_{t}\mathbf{\mu}_{t}+(1+\mathbf{\mu}_{t}^{\top}\mathbf{\mu}_{t})\nabla_{ \mathbf{\mu}_{t}}E(\mathbf{\mu}_{t},\Sigma_{t})\right)\\ \dot{\Sigma}_{t}=-\Sigma_{t}\left(2\Sigma_{t}\nabla_{\Sigma_{t}}E(\mathbf{\mu}_{t},\Sigma_{t})+\mathbf{\mu}_{t}\nabla_{\mathbf{\mu}_{t}}^{\top}E(\mathbf{\mu}_{t},\Sigma_{t}) \right)-\left(2\nabla_{\Sigma_{t}}E(\mathbf{\mu}_{t},\Sigma_{t})\Sigma_{t}+\nabla_{ \mathbf{\mu}_{t}}E(\mathbf{\mu}_{t},\Sigma_{t})\mathbf{\mu}_{t}^{\top}\right)\Sigma_{t}\.\end{cases} \tag{83}\]Substituting (82) into (83), we get

\[\begin{cases}\dot{\mathbf{\mu}}_{t}=\left(I-\mathbb{E}_{\mathbf{x}\sim\rho_{t}}[\nabla^{2}V (\mathbf{x})]\Sigma_{t}\right)\mathbf{\mu}_{t}-(1+\mathbf{\mu}_{t}^{\top}\mathbf{\mu}_{t}) \mathbb{E}_{\mathbf{x}\sim\rho_{t}}[\nabla V(\mathbf{x})]\\ \dot{\Sigma}_{t}=2\Sigma_{t}-\Sigma_{t}\left(\Sigma_{t}\mathbb{E}_{\mathbf{x}\sim \rho_{t}}[\nabla^{2}V(\mathbf{x})]+\mathbf{\mu}_{t}\mathbb{E}_{\mathbf{x}\sim\rho_{t}}[ \nabla^{\top}V(\mathbf{x})]\right)\\ \qquad-\left(\mathbb{E}_{\mathbf{x}\sim\rho_{t}}[\nabla^{2}V(\mathbf{x})]\Sigma_{t}+ \mathbb{E}_{\mathbf{x}\sim\rho_{t}}[\nabla^{\top}V(\mathbf{x})]\mathbf{\mu}_{t}\right) \Sigma_{t}\end{cases}.\]

Next we prove the convergence.

\[\dot{E}(\mathbf{\mu}_{t},\Sigma_{t}) =\operatorname{tr}(\nabla_{\Sigma_{t}}E(\mathbf{\mu}_{t},\Sigma_{t}) ^{\top}\dot{\Sigma}_{t})+\nabla_{\mathbf{\mu}_{t}}E(\mathbf{\mu}_{t},\Sigma_{t})^{\top }\dot{\mathbf{\mu}}_{t}\] \[=-\operatorname{tr}\left((2\nabla_{\Sigma}E(\mathbf{\mu},\Sigma) \Sigma+\nabla_{\mathbf{\mu}}E(\mathbf{\mu},\Sigma)\mathbf{\mu}^{\top})^{\top}(2\nabla_{ \Sigma}E(\mathbf{\mu},\Sigma)\Sigma+\nabla_{\mathbf{\mu}}E(\mathbf{\mu},\Sigma)\mathbf{\mu}^{ \top})\right)\] \[\quad-\nabla_{\mathbf{\mu}}^{\top}E(\mathbf{\mu},\Sigma)\nabla_{\mathbf{\mu}}E (\mathbf{\mu},\Sigma)\leq 0.\]

Noticing that

\[0\leq-\int_{0}^{t}\dot{E}(\mathbf{\mu}_{s},\Sigma_{s})\,\mathrm{d}s=E(\mathbf{\mu}_{0 },\Sigma_{0})-E(\mathbf{\mu}_{t},\Sigma_{t})<\infty,\]

we obtain that \(\dot{E}(\mathbf{\mu}_{t},\Sigma_{t})\to 0\) as \(t\to\infty\), which means that there exists \(\mathbf{\mu}_{\infty},\Sigma_{\infty}\) such that \(\rho_{t}\) converges to \(\rho_{\infty}\), \(\rho_{\infty}\) is the density of \(\mathcal{N}(\mathbf{\mu}_{\infty},\Sigma_{\infty})\). (Since \(E(\mathbf{\mu}_{t},\Sigma_{t})\) is given by the KL divergence between \(\rho_{t}\) and \(\rho^{*}\), by Lemma K.1 it will diverge if \(\mathbf{\mu}_{t}\) or \(\Sigma_{t}\) diverges.) In particular, it satisfies that

\[\begin{cases}2\nabla_{\Sigma}E(\mathbf{\mu}_{\infty},\Sigma_{\infty})+\nabla_{\bm {\mu}}E(\mathbf{\mu}_{\infty},\Sigma_{\infty})\mathbf{\mu}_{\infty}^{\top}=0\\ \nabla_{\mathbf{\mu}}E(\mathbf{\mu}_{\infty},\Sigma_{\infty})=\mathbf{0}\end{cases},\]

which is equivalent to

\[\begin{cases}\nabla_{\Sigma}E(\mathbf{\mu}_{\infty},\Sigma_{\infty})=0\\ \nabla_{\mathbf{\mu}}E(\mathbf{\mu}_{\infty},\Sigma_{\infty})=\mathbf{0}\end{cases},\]

and implies that \(\rho_{\infty}=\rho_{\theta^{*}}\). 

**Lemma K.2**.: _Given \(\alpha\)-strongly convex measure \(\rho^{*}\), we define \(\theta^{*}\) as the unique minimizer of \(\mathrm{KL}(\rho_{\theta}\parallel\rho^{*})\) where \(\rho_{\theta}\) denotes a Gaussian measure with parameters \(\theta\). Then it holds that_

\[\left\lVert\mathbb{E}_{\rho_{\theta}}[\nabla V(\mathbf{x})]\right\rVert _{2}^{2}+\operatorname{tr}\left((\mathbb{E}_{\rho_{\theta}}[\nabla^{2}V(\mathbf{x })]-\Sigma^{-1})\Sigma(\mathbb{E}_{\rho_{\theta}}[\nabla^{2}V(\mathbf{x})]- \Sigma^{-1})\right)\] \[\qquad\geq 2\alpha\left(\mathrm{KL}(\rho_{\theta}\parallel\rho^{*} )-\mathrm{KL}(\rho_{\theta^{*}}\parallel\rho^{*})\right).\]

This is proven in the Appendix D of [43]. The proof idea is to consider the Gaussian approximate Wasserstein gradient flow from \(\rho_{\theta}\) with the target \(\rho^{*}\).

**Lemma K.3**.: _Given \(\alpha\)-strongly convex measure \(\rho^{*}\), we define \(\theta^{*}\) as the unique minimizer of \(\mathrm{KL}(\rho_{\theta}\parallel\rho^{*})\) where \(\rho_{\theta}\) denotes a Gaussian measure with parameters \(\theta\). Then the Wasserstein-\(2\) distance between \(\rho_{\theta}\) and \(\rho_{\theta^{*}}\) satisfies that_

\[\alpha\mathcal{W}_{2}^{2}(\rho_{\theta},\rho_{\theta^{*}})\leq\mathrm{KL}(\rho _{\theta}\parallel\rho^{*})-\mathrm{KL}(\rho_{\theta^{*}}\parallel\rho^{*}).\]

This is Lemma E.2 of [13].

Proof of Theorem 4.2.: From Lemma K.1 and the proof of Theorem 4.1 we know that

\[\dot{E}(\mathbf{\mu}_{t},\Sigma_{t}) =-\operatorname{tr}\left((2\nabla_{\Sigma}E(\mathbf{\mu},\Sigma) \Sigma+\nabla_{\mathbf{\mu}}E(\mathbf{\mu},\Sigma)\mathbf{\mu}^{\top})^{\top}(2\nabla_{ \Sigma}E(\mathbf{\mu},\Sigma)\Sigma+\nabla_{\mathbf{\mu}}E(\mathbf{\mu},\Sigma)\mathbf{\mu}^{ \top})\right)\] \[\quad-\nabla_{\mathbf{\mu}}^{\top}E(\mathbf{\mu},\Sigma)\nabla_{\mathbf{\mu} }E(\mathbf{\mu},\Sigma)\] \[=-\operatorname{tr}\left((\mathbb{E}_{\rho_{\theta}}[\nabla^{2}V (\mathbf{x})]\Sigma-I+\mathbb{E}_{\rho_{\theta}}[\nabla V(\mathbf{x})]\mathbf{\mu}^{\top}) ^{\top}(\mathbb{E}_{\rho_{\theta}}[\nabla^{2}V(\mathbf{x})]\Sigma-I+\mathbb{E}_{ \rho_{\theta}}[\nabla V(\mathbf{x})]\mathbf{\mu}^{\top})\right)\] \[\quad-\left\lVert\mathbb{E}_{\rho_{\theta}}[\nabla V(\mathbf{x})] \right\rVert_{2}^{2}.\]

For convenience we let \(\mathbf{\eta}_{t}=\mathbb{E}_{\rho_{\theta}}[\nabla V(\mathbf{x})]\) and \(S_{t}=(\mathbb{E}_{\rho_{\theta}}[\nabla^{2}V(\mathbf{x})]-\Sigma^{-1})\Sigma^{1/2}\). Then we get

\[-\dot{E}(\mathbf{\mu}_{t},\Sigma_{t}) =\begin{bmatrix}\operatorname{vec}^{\top}(S_{t})&\mathbf{\eta}_{t} \end{bmatrix}\begin{bmatrix}I_{d}\otimes\Sigma_{t}&\mathbf{\mu}_{t}\otimes\Sigma_{t }^{1/2}\\ \mathbf{\mu}_{t}^{\top}\otimes\Sigma_{t}^{1/2}&(1+\mathbf{\mu}_{t}^{\top}\mathbf{\mu}_{t})I_{ d}\end{bmatrix}\begin{bmatrix}\operatorname{vec}(S_{t})\\ \mathbf{\eta}_{t}\end{bmatrix}\] \[=:\begin{bmatrix}\operatorname{vec}^{\top}(S_{t})&\mathbf{\eta}_{t} \end{bmatrix}M_{t}\begin{bmatrix}\operatorname{vec}(S_{t})\\ \mathbf{\eta}_{t}\end{bmatrix}.\]Noting that \(M_{t}\to M_{\infty}\), for any \(\epsilon>0\) there exists \(T>0\) such that the smallest eigenvalue \(\gamma_{t}/\alpha\) of \(M_{t}\) satisfies \(\gamma_{t}\geq\gamma-\epsilon\) for any \(t>T\), where \(\gamma/\alpha\) is the smallest eigenvalue of \(M_{\infty}\).

Moreover, by Lemma K.2 we have

\[\begin{bmatrix}\operatorname{vec}^{\top}(S_{t})&\boldsymbol{\eta}_{t}\end{bmatrix} \begin{bmatrix}\operatorname{vec}(S_{t})\\ \boldsymbol{\eta}_{t}\end{bmatrix}\geq 2\alpha(\operatorname{KL}(\rho_{ \theta}\parallel\rho^{*})-\operatorname{KL}(\rho_{\infty}\parallel\rho^{*})).\]

Thus, for \(t>T\) the derivative of KL divergence is controlled, i.e.,

\[-\partial_{t}\operatorname{KL}(\rho_{t}\parallel\rho^{*})\geq 2(\gamma- \epsilon)(\operatorname{KL}(\rho_{\theta}\parallel\rho^{*})-\operatorname{KL}( \rho_{\infty}\parallel\rho^{*})).\]

By Gronwall's inequality we know

\[\operatorname{KL}(\rho_{\theta}\parallel\rho^{*})-\operatorname{KL}(\rho_{ \infty}\parallel\rho^{*})=\mathcal{O}(e^{-2(\gamma-\epsilon)t}).\]

By Lemma K.3 this implies

\[\mathcal{W}_{2}^{2}(\rho_{\theta},\rho_{\theta^{*}})=\mathcal{O}(e^{-2(\gamma- \epsilon)t}).\]

Noting that

\[\mathcal{W}_{2}^{2}(\rho_{\theta},\rho_{\theta^{*}})=\left\lVert\boldsymbol{ \mu}-\boldsymbol{\mu}^{*}\right\rVert_{2}^{2}+\operatorname{tr}\left((\Sigma^{1 /2}-(\Sigma^{*})^{1/2})^{2}\right),\]

we conclude

\[\left\lVert\boldsymbol{\mu}_{t}-\boldsymbol{\mu}^{*}\right\rVert=\mathcal{O}( e^{-(\gamma-\epsilon)t}),\quad\left\lVert\Sigma_{t}-\Sigma^{*}\right\rVert= \mathcal{O}(e^{-(\gamma-\epsilon)t}),\quad\forall\epsilon>0.\]

Finally, we provide a lower bound on \(\gamma\). Note that for any \(u>0\) if \(\boldsymbol{\mu}^{*}\neq\boldsymbol{0}\) we have

\[\begin{bmatrix}\frac{1}{1+u}I_{d}\otimes\Sigma^{*}&\boldsymbol{\mu}^{*} \otimes(\Sigma^{*})^{1/2}\\ \boldsymbol{\mu}^{*\top}\otimes(\Sigma^{*})^{1/2}&(1+u)\boldsymbol{\mu}^{* \top}\boldsymbol{\mu}^{*}I_{d}\end{bmatrix}\succeq 0.\]

Thus,

\[\begin{bmatrix}I_{d}\otimes\Sigma^{*}&\boldsymbol{\mu}^{*}\otimes(\Sigma^{*}) ^{1/2}\\ \boldsymbol{\mu}^{*\top}\otimes(\Sigma^{*})^{1/2}&(1+\boldsymbol{\mu}^{*\top }\boldsymbol{\mu}^{*})I_{d}\end{bmatrix}\succeq\begin{bmatrix}\frac{u}{1+u}I_ {d}\otimes\Sigma^{*}\\ (1-u\boldsymbol{\mu}^{*\top}\boldsymbol{\mu}^{*})I_{d}\end{bmatrix}=:\Omega_{u}.\]

Since \(\Sigma^{*}\) satisfies that

\[\mathbb{E}_{\rho_{\theta^{*}}}[\nabla^{2}V(\boldsymbol{x})]-(\Sigma^{*})^{-1 }=0,\]

and \(\nabla^{2}V(\boldsymbol{x})\preceq\beta I_{d}\), we know the smallest eigenvalue of \(\Sigma^{*}\) is at least \(1/\beta\). Thus, the smallest eigenvalue of \(\Omega_{u}\) is

\[\min\left\{\frac{u}{\beta(1+u)},1-ur\right\},\text{ where }u>0,r=\boldsymbol{ \mu}^{*\top}\boldsymbol{\mu}^{*}.\]

We find \(u\) such that this quantity is maximized and get

\[\frac{\gamma}{\alpha}\geq\max_{u>0}\min\left\{\frac{u}{\beta(1+u)},1-ur\right\} =\frac{2}{1+\beta(1+r)+\sqrt{(1+\beta(1+r))^{2}-4\beta}}>\frac{1}{\beta(1+r)+ 1}.\]

If \(\boldsymbol{\mu}^{*}=0\), we still have

\[\frac{\gamma}{\alpha}=\min\left\{\frac{1}{\beta},1\right\}>\frac{1}{\beta+1}.\]

## Appendix L Proofs of Uniform in Time Convergence

To show Theorem 3.7 we need a lemma on the convergence of empirical measures in the _i.i.d._ setting. There are results for general measures on this given by [25, 47, 24] but for our purpose we always have a Gaussian distributions as the limit and there are tight results with faster convergence rates as shown in [9, 45, 46]:

**Lemma L.1** (Convergence of empirical measures for Gaussian distributions).: _Fix the dimension \(d\geq 1\). There exists a constant \(C_{d}\) such that for all \(N\geq 1\), with \(\mu_{N}=\frac{1}{N}\sum_{k=1}^{N}\delta_{X_{k}}\) where \(\{X_{k}\}\) is i.i.d. sequence drawn from \(\mu\sim\mathcal{N}(\mathbf{0},I_{d})\), we have_

\[\mathbb{E}\left[\mathcal{W}_{2}^{2}\left(\mu_{N},\mu\right)\right]\leq C_{d} \times\left\{\begin{array}{cl}N^{-1}\log\log N&\text{ if }d=1\\ N^{-1}(\log N)^{2}&\text{ if }d=2\\ N^{-2/d}&\text{ if }d\geq 3\end{array}\right..\]

Proof of Theorem 3.7.: Suppose the sample mean and covariance at time \(t\) is \(\mathbf{m}_{t}\) and \(C_{t}\), and that the mean and covariance of the mean-field limit is \(\mathbf{\mu}_{t}\) and \(\Sigma_{t}\). We bound \(\mathbb{E}[\mathcal{W}_{2}^{2}(\zeta_{N}^{(t)},\rho_{t})]\) using Theorems 3.1 and 3.6 and Lemma L.1 in six steps.

**Step I.** We prove that \((\mathbf{x}_{i}^{(t)})\) has the same distribution as \(\left(\widetilde{\mathbf{x}}_{i}^{(t)}\right)\) where \(\widetilde{\mathbf{x}}_{i}^{(t)}=C_{t}^{1/2}C_{0}^{-1/2}(\mathbf{x}_{i}^{(0)}-\mathbf{m}_{ 0})+\mathbf{m}_{t}\). Note that according to Theorem 3.6, where we have \(\mathbf{x}_{i}^{(t)}=A_{t}(\mathbf{x}_{i}^{(0)}-\mathbf{m}_{0})+\mathbf{m}_{t}\). Here \(A_{t}\) is the unique (matrix) solution of the linear system

\[\dot{A}_{t}=\left(I-Q^{-1}(C_{t}+\mathbf{\mu}_{t}\mathbf{\mu}_{t}^{\top})+Q^{-1}\mathbf{b} \mathbf{\mu}_{t}^{\top}\right)A_{t},\quad A_{0}=I, \tag{84}\]

and \(\mathbf{m}_{t}\) and \(C_{t}\) are the unique solution of the ODE system

\[\begin{cases}\dot{\mathbf{m}}_{t}=(I-Q^{-1}C_{t})\mathbf{m}_{t}-(1+\mathbf{m}_{t}^{\top} \mathbf{m}_{t})Q^{-1}(\mathbf{m}_{t}-\mathbf{b})\\ \dot{C}_{t}=2C_{t}-C_{t}\left(C_{t}+\mathbf{m}_{t}(\mathbf{m}_{t}-\mathbf{b})^{\top}\right) Q^{-1}-Q^{-1}\left(C_{t}+(\mathbf{m}_{t}-\mathbf{b})\mathbf{m}_{t}^{\top}\right)C_{t}\end{cases}. \tag{85}\]

Since \(C_{t}\) is the sample covariance, we have

\[A_{t}C_{0}A_{t}^{\top}=C_{t},\]

which simplies that

\[(C_{t}^{-1/2}A_{t}C_{0}^{1/2})(C_{t}^{-1/2}A_{t}C_{0}^{1/2})^{\top}=I.\]

Thus, \(P_{t}=C_{t}^{-1/2}A_{t}C_{0}^{1/2}\) is an orthogonal matrix, and we have

\[A_{t}=C_{t}^{1/2}P_{t}C_{0}^{-1/2}.\]

Since the multivariate Gaussian distribution \(\mathcal{N}(\mathbf{0},I_{d})\) is invariant under orthogonal transformation, the joint distribution of \(\left(C_{0}^{-1/2}(\mathbf{x}_{i}^{(0)}-\mathbf{m}_{0})\right)_{i=1}^{N}\) is also invariant under \(P_{t}\). Thus, we have \((\mathbf{x}_{i}^{(t)}-\mathbf{m}_{t})\) has the same distribution as \(\left(\widetilde{\mathbf{x}}_{i}^{(t)}-\mathbf{m}_{t}\right)\) and Step I is proven.

**Step II.** Establish uniform decay rates for \(\left\|C_{t}-Q\right\|_{F}\) and \(\left\|\mathbf{m}_{t}-\mathbf{b}\right\|\). We begin by checking the energy function

\[0\leq E(\mathbf{m}_{t},C_{t})=\frac{1}{2}\left(\operatorname{tr}(Q^{-1}C_{t})-\log \det(Q^{-1}C_{t})-d+(\mathbf{m}_{t}-\mathbf{b})^{\top}Q^{-1}(\mathbf{m}_{t}-\mathbf{b})\right).\]

As shown in the proof of Theorem 3.1, we have

\[\dot{E}(\mathbf{m}_{t},C_{t})=-\left\|C_{t}Q^{-1}-I+\mathbf{m}_{t}(\mathbf{m}_{t}-\mathbf{b})^ {\top}Q^{-1}\right\|_{F}^{2}-\left\|Q^{-1}(\mathbf{m}_{t}-\mathbf{b})\right\|^{2}\leq 0.\]

Thus, \(E(\mathbf{m}_{t},C_{t})\leq E(\mathbf{m}_{0},C_{0})\) for any \(t\geq 0\). Furthermore, similar to the proof of Theorem 3.1 we check that

\[-\dot{E}(\mathbf{m}_{t},C_{t})\] \[= \left[\operatorname{vec}^{\top}\left(Q^{-1}C_{t}-I\right)\quad( \mathbf{m}_{t}-\mathbf{b})^{\top}Q^{-1}\right]\begin{bmatrix}I_{d^{2}}&\mathbf{m}_{t} \otimes I_{d}\\ \mathbf{m}_{t}^{\top}\otimes I_{d}&(1+\mathbf{m}_{t}^{\top}\mathbf{m}_{t})I_{d}\end{bmatrix} \begin{bmatrix}\operatorname{vec}\left(Q^{-1}C_{t}^{-1}-I\right)\\ Q^{-1}(\mathbf{m}_{t}-\mathbf{b})\end{bmatrix}\] \[\geq \gamma_{t}\left[\operatorname{vec}^{\top}\left(Q^{-1}C_{t}-I \right)\quad(\mathbf{m}_{t}-\mathbf{b})^{\top}Q^{-1}\right]\begin{bmatrix}I_{d^{2}}&2Q \end{bmatrix}\begin{bmatrix}\operatorname{vec}\left(Q^{-1}C_{t}-I\right)\\ Q^{-1}(\mathbf{m}_{t}-\mathbf{b})\end{bmatrix}\] \[= \gamma_{t}\operatorname{tr}\left((Q^{-1}C_{t}-I)^{\top}(Q^{-1}C_{ t}-I)\right)+2\gamma_{t}(\mathbf{m}_{t}-\mathbf{b})^{\top}Q(\mathbf{m}_{t}-\mathbf{b})\] \[\geq 2\gamma_{t}\cdot\left(\operatorname{tr}(Q^{-1}C_{t}-I)-\log\det(Q^ {-1}C_{t})+(\mathbf{m}_{t}-\mathbf{b})^{\top}Q(\mathbf{m}_{t}-\mathbf{b})\right)\] \[= 4\gamma_{t}E(\mathbf{m}_{t},C_{t}).\]where \(\gamma_{t}\) is the smallest eigenvalue of

\[\begin{bmatrix}I_{d^{2}}&\frac{1}{\sqrt{2}}\mathbf{m}_{t}\otimes Q^{-1/2}\\ \frac{1}{\sqrt{2}}\mathbf{m}_{t}^{\top}\otimes Q^{-1/2}&\frac{1}{2}(1+\mathbf{m}_{t}^{ \top}\mathbf{m}_{t})Q^{-1}\end{bmatrix},\]

and as shown in the proof of Theorem 3.1 it has a lower bound

\[\gamma_{t}>\frac{1}{1+\mathbf{m}_{t}^{\top}\mathbf{m}_{t}+q_{\max}},\]

where \(q_{\max}\) is the largest eigenvalue of \(Q\). Now since

\[\frac{1}{2}(\mathbf{m}_{t}-\mathbf{b})^{\top}Q^{-1}(\mathbf{m}-\mathbf{b})\leq E(\mathbf{m}_{t},C_ {t})\leq E(\mathbf{m}_{0},C_{0}),\]

we know \((\mathbf{m}_{t}-\mathbf{b})^{\top}(\mathbf{m}_{t}-\mathbf{b})\leq 2q_{\max}E(\mathbf{m}_{0},C_{0})\). Thus, \(\|\mathbf{m}_{t}\|\) is upper bounded by some quantity \(F_{1}=F_{1;Q,\mathbf{b},C_{0},\mathbf{m}_{0}}\). Hence \(\gamma_{t}\) is uniformly lower bounded by

\[\gamma^{*}:=\inf_{t\geq 0}\gamma_{t}\geq\frac{1}{1+F_{1}^{2}+q_{\max}}.\]

Thus, by Gronwall's inequality we have \(E(\mathbf{m}_{t},C_{t})\leq e^{-4\gamma^{*}t}E(\mathbf{m}_{0},C_{0})\). There exists \(F_{2}=F_{2;Q,\mathbf{b},C_{0},\mathbf{m}_{0}}\) such that \(\|\mathbf{m}_{t}-\mathbf{b}\|\leq e^{-2\gamma^{*}t}F_{2}\).

Now similarly

\[0\leq\frac{1}{2}\left(\operatorname{tr}(Q^{-1}C_{t}-1)-\log\det(Q^{-1}C_{t}) \right)\leq E(\mathbf{m}_{t},C_{t})\leq e^{-4\gamma^{*}t}E(\mathbf{m}_{0},C_{0})\]

also renders an upper bound for \(\|C_{t}-Q\|_{F}\) with exponential decay noting that \(\operatorname{tr}(A)-\log\det(I+A)\) is quadratic in \(\|A\|_{F}\) when \(\|A\|_{F}\) is small, i.e., there exists \(F_{3}=F_{3;Q,\mathbf{b},C_{0},\mathbf{m}_{0}}\) such that

\[\|C_{t}-Q\|_{F}\leq e^{-2\gamma^{*}t}F_{3}.\]

By Theorem 3.1 we know that \(\mathbf{\mu}_{t},\Sigma_{t}\) satisfy the same ODEs as \(\mathbf{m}_{t},C_{t}\):

\[\begin{cases}\dot{\mathbf{\mu}}_{t}=(I-Q^{-1}\Sigma_{t})\mathbf{\mu}_{t}-(1+\mathbf{\mu}_{ t}^{\top}\mathbf{\mu}_{t})Q^{-1}(\mathbf{\mu}_{t}-\mathbf{b})\\ \dot{\Sigma}_{t}=2\Sigma_{t}-\Sigma_{t}\left(\Sigma_{t}+\mathbf{\mu}_{t}(\mathbf{\mu}_ {t}-\mathbf{b})^{\top}\right)Q^{-1}-Q^{-1}\left(\Sigma_{t}+(\mathbf{\mu}_{t}-\mathbf{b}) \mathbf{\mu}_{t}^{\top}\right)\Sigma_{t}\end{cases}. \tag{86}\]

Thus, similarly we have

\[\|\mathbf{\mu}_{t}-\mathbf{b}\|\leq e^{-2\gamma^{*}t}F_{2}^{\prime},\quad\|\Sigma_{t} -Q\|_{F}\leq e^{-2\gamma^{*}t}F_{3}^{\prime}.\]

**Step III.** Show that \(\|C_{t}-\Sigma_{t}\|_{F}\) and \(\|\mathbf{m}_{t}-\mathbf{\mu}_{t}\|\) can be controlled after sufficient time.

For any \(\epsilon>0\) we define

\[\Theta_{\epsilon}:=\left\{(\mathbf{v},S)\in\mathbb{R}^{d}\times\operatorname{Sym} ^{+}(d,\mathbb{R}):\|\mathbf{v}-\mathbf{b}\|\leq\epsilon,\text{ and }(1-\epsilon)Q \preceq S\preceq(1+\epsilon)Q\right\},\]

and consider the relative energy function

\[E(\mathbf{m}_{t},C_{t},\mathbf{\mu}_{t},\Sigma_{t})=\frac{1}{2}\left( \operatorname{tr}\left(C_{t}^{-1}\Sigma_{t}-I\right)-\log\det(C_{t}^{-1} \Sigma_{t})+(\mathbf{m}_{t}-\mathbf{\mu}_{t})^{\top}Q^{-1}(\mathbf{m}_{t}-\mathbf{\mu}_{t}) \right).\]

We show that for \(\epsilon\) small enough if \((\mathbf{m}_{t},C_{t}),(\mathbf{\mu}_{t},\Sigma_{t})\in\Theta_{\epsilon}\), then

\[\dot{E}(\mathbf{m}_{t},C_{t},\mathbf{\mu}_{t},\Sigma_{t})\leq 0.\]

We derive that

\[\dot{E}(\mathbf{m}_{t},C_{t},\mathbf{\mu}_{t},\Sigma_{t})\] \[= \frac{1}{2}\operatorname{tr}\left(C_{t}^{-1}(\Sigma_{t}-C_{t}) \left(\Sigma_{t}^{-1}\dot{\Sigma}_{t}-C_{t}^{-1}\dot{C}_{t}\right)\right)+( \mathbf{m}_{t}-\mathbf{\mu}_{t})^{\top}Q^{-1}(\dot{\mathbf{m}}_{t}-\dot{\mathbf{\mu}}_{t}).\]

[MISSING_PAGE_EMPTY:54]

Given \(Q,\mathbf{b},C_{0},\mathbf{m}_{t},\Sigma_{0},\mathbf{\mu}_{0}\) suppose at time \(t_{0}\) we have \((\mathbf{m}_{t_{0}},C_{t_{0}})\in\Theta_{\epsilon_{0}}\) and \((\mathbf{\mu}_{t_{0}},\Sigma_{t_{0}})\in\Theta_{\epsilon_{0}}\). Then for any \(t>t_{0}\) we know

\[E(\mathbf{m}_{t},C_{t},\mathbf{\mu}_{t},\Sigma_{t})\leq E(\mathbf{m}_{t_{0}}, C_{t_{0}},\mathbf{\mu}_{t_{0}},\Sigma_{t_{0}})\] \[\leq \frac{1}{4}\operatorname{tr}((C_{t_{0}}-\Sigma_{t_{0}})^{2}C_{t_ {0}}^{-2})+(\mathbf{m}_{t_{0}}-\mathbf{\mu}_{t_{0}})^{\top}Q^{-1}(\mathbf{m}_{t_{0}}-\mathbf{ \mu}_{t_{0}})\] \[\leq \frac{1}{4(1-\epsilon_{0})^{2}q_{\min}^{2}}\|C_{t_{0}}-\Sigma_{t _{0}}\|_{F}^{2}+\frac{1}{q_{\min}}\|\mathbf{m}_{t_{0}}-\mathbf{\mu}_{t_{0}}\|^{2}.\]

Note that

\[\lim_{\|A\|\neq 0}\frac{\operatorname{tr}(A)-\log\det(I+A)}{\operatorname{ tr}(A^{\top}A)}=\frac{1}{2}.\]

Fixing any \(\delta>0\) as long as \(\epsilon_{0}\) is small enough we have

\[E(\mathbf{m}_{t},C_{t},\mathbf{\mu}_{t},\Sigma_{t})\geq \frac{1}{4+\delta}\operatorname{tr}((C_{t}-\Sigma_{t})^{2}C_{t}^ {-2})+(\mathbf{m}_{t}-\mathbf{\mu}_{t})^{\top}Q^{-1}(\mathbf{m}_{t}-\mathbf{\mu}_{t})\] \[\geq \frac{1}{(4+\delta)(1+\epsilon_{0})^{2}q_{\max}^{2}}\|C_{t}- \Sigma_{t}\|_{F}^{2}+\frac{1}{q_{\max}}\|\mathbf{m}_{t}-\mathbf{\mu}_{t}\|^{2}.\]

Therefore, we conclude that for any given \(Q\), \(\mathbf{b}\), \(C_{0}\), \(\mathbf{m}_{0}\), \(\Sigma_{0}\) and \(\mathbf{\mu}_{0}\) there exists \(\epsilon_{0}\) as stated above and \(F_{4}=F_{4;Q,\mathbf{b},C_{0},\mathbf{m}_{0},\Sigma_{0},\mathbf{\mu}_{0}}>0\) such that as long as \((\mathbf{m}_{t_{0}},C_{t_{0}})\in\Theta_{\epsilon_{0}}\) and \((\mathbf{\mu}_{t_{0}},\Sigma_{t_{0}})\in\Theta_{\epsilon_{0}}\) then for any \(t>t_{0}\)

\[\|C_{t}-\Sigma_{t}\|_{F}^{2} \leq F_{4}\left(\|C_{t_{0}}-\Sigma_{t_{0}}\|_{F}^{2}+\|\mathbf{m}_{t_ {0}}-\mathbf{\mu}_{t_{0}}\|^{2}\right),\] \[\|\mathbf{m}_{t}-\mathbf{\mu}_{t}\|^{2} \leq F_{4}\left(\|C_{t_{0}}-\Sigma_{t_{0}}\|_{F}^{2}+\|\mathbf{m}_{t_ {0}}-\mathbf{\mu}_{t_{0}}\|^{2}\right).\]

**Step IV.** Uniformly bound \(\|C_{t}-\Sigma_{t}\|_{F}^{2}\) and \(\|\mathbf{m}_{t}-\mathbf{\mu}_{t}\|^{2}\) using \(\|C_{0}-\Sigma_{0}\|_{F}^{2}+\|\mathbf{m}_{0}-\mathbf{\mu}_{0}\|^{2}\).

Note that by definition of the Frobenius norm (or any matrix norm), given \(\epsilon_{0}>0\) there exists \(\epsilon_{1}\in(0,\epsilon_{0})\) such that for any \(S\in\operatorname{Sym}(d,\mathbb{R})\) as long as the norm is small enough, i.e., \(\|S-Q\|_{F}\leq\epsilon_{1}\), then we have \((1-\epsilon_{0})Q\leq S\leq(1+\epsilon_{0})Q\). Then by Step II we know that if we set \(F_{5}=\max\{F_{2},F_{3},F_{2}^{\prime},F_{3}^{\prime}\}\) and \(t_{0}>-\frac{1}{2\gamma^{\gamma}}\log\frac{\epsilon_{1}}{F_{5}}\) then the following bounds hold:

\[\|\mathbf{m}_{t}-\mathbf{b}\|<\epsilon_{1},\quad\|C_{t}-Q\|_{F}<\epsilon_{1},\quad\| \mathbf{\mu}_{t}-\mathbf{b}\|<\epsilon_{1},\quad\|\Sigma_{t}-Q\|_{F}<\epsilon_{1}.\]

Now it is straight forward to check from (85) and (86) and the results in Step II that there exists \(F_{6}=F_{6;Q,\mathbf{b},C_{0},\mathbf{m}_{0},\Sigma_{0},\mathbf{\mu}_{t}}\) such that for any \(t\geq 0\)

\[\frac{\mathrm{d}}{\mathrm{d}t}\|\mathbf{m}_{t}-\mathbf{\mu}_{t}\|^{2}\leq F_{6}\|\mathbf{m} _{t}-\mathbf{\mu}_{t}\|,\quad\frac{\mathrm{d}}{\mathrm{d}t}\|C_{t}-\Sigma_{t}\|_{F }^{2}\leq F_{6}\|C_{t}-\Sigma_{t}\|_{F}^{2}.\]

Thus, by Gronwall's inequality we have

\[\|\mathbf{m}_{t}-\mathbf{\mu}_{t}\|^{2}\leq e^{F_{6}t}\|\mathbf{m}_{0}-\mathbf{\mu}_{0}\|^{2}, \quad\|C_{t}-\Sigma_{t}\|_{F}^{2}\leq e^{F_{6}t}\|C_{0}-\Sigma_{0}\|_{F}^{2}.\]

Combining this with Step III, we know for any \(t\geq 0\), there exists \(F_{7}=e^{F_{6}t_{0}}F_{4}\) (only depending on \(Q,\mathbf{b},C_{0},\mathbf{m}_{0},\Sigma_{0},\mathbf{\mu}_{0}\)) such that

\[\|C_{t}-\Sigma_{t}\|_{F}^{2} \leq F_{7}\left(\|C_{0}-\Sigma_{0}\|_{F}^{2}+\|\mathbf{m}_{0}-\mathbf{\mu} _{0}\|^{2}\right),\] \[\|\mathbf{m}_{t}-\mathbf{\mu}_{t}\|^{2} \leq F_{7}\left(\|C_{0}-\Sigma_{0}\|_{F}^{2}+\|\mathbf{m}_{0}-\mathbf{\mu} _{0}\|^{2}\right).\]

**Step V.** Let \(\mathbf{y}_{i}^{(t)}:=\Sigma_{t}^{1/2}\Sigma_{0}^{-1/2}(\mathbf{x}_{i}^{(0)}-\mathbf{\mu}_{0 })+\mathbf{\mu}_{t}\). Define \(\xi_{N}^{(t)}=\frac{1}{N}\sum_{i=1}^{N}\delta_{\mathbf{y}_{i}^{(t)}}\) and \(\widehat{\zeta}_{N}^{(t)}=\frac{1}{N}\sum_{i=1}^{N}\delta_{\widehat{\mathbf{x}}_{i}^ {(t)}}\). We show that

\[\mathbb{E}\left[\mathcal{W}_{2}^{2}\left(\xi_{N}^{(t)},\widehat{\zeta}_{N}^{(t)} \right)\right]\stackrel{{\circ}}{{\leq}}\mathbb{E}\left[\frac{1} {N}\sum_{i=1}^{N}\left\|\widehat{\mathbf{x}}_{N}^{(t)}-\mathbf{y}_{N}^{(t)}\right\|^{2} \right]\stackrel{{\circ}}{{=}}o\left(\frac{\log\log N}{N}\right).\]Since \(\circ\) is trivial by the definition of the Wasserstein metric, we only need to check \(\diamond\). In fact,

\[\frac{1}{N}\sum_{i=1}^{N}\left\|\widetilde{\mathbf{x}}_{N}^{(t)}-\mathbf{y }_{N}^{(t)}\right\|^{2}\] \[\leq \frac{3}{N}\sum_{i=1}^{N}\left\|\left(\Sigma_{t}^{1/2}\Sigma_{0}^ {-1/2}-C_{t}^{1/2}C_{0}^{-1/2}\right)\left(\mathbf{x}_{i}^{(0)}-\mathbf{\mu}_{0} \right)\right\|^{2}\] \[+3\left\|C_{t}^{1/2}C_{0}^{-1/2}(\mathbf{m}_{0}-\mathbf{\mu}_{0})\right\|^ {2}+3\|\mathbf{\mu}_{t}-\mathbf{m}_{t}\|^{2}\] \[= 3\left\|\Sigma_{t}^{1/2}\Sigma_{0}^{-1/2}C_{0}^{1/2}-C_{t}^{1/2} \right\|_{F}^{2}+3\left\|C_{t}^{1/2}C_{0}^{-1/2}(\mathbf{m}_{0}-\mathbf{\mu}_{0}) \right\|^{2}+3\|\mathbf{\mu}_{t}-\mathbf{m}_{t}\|^{2}\] \[=:I_{5}+I_{6}+I_{7}.\]

Note that

\[I_{6}\leq 3\|C_{t}\|_{F}\ \|C_{0}^{-1}\|_{F}\ \|\mathbf{m}_{0}- \mathbf{\mu}_{0}\|^{2},\] \[I_{7}\leq 3F_{7}\left(\|C_{0}-\Sigma_{0}\|_{F}^{2}+\|\mathbf{m}_{0}- \mathbf{\mu}_{0}\|^{2}\right).\]

By the central limit theorem \(\sqrt{N}(\mathbf{m}_{0}-\mathbf{\mu}_{0})\) converges to \(\mathcal{N}(\mathbf{0},\Sigma_{0})\) in distribution. Thus, \(\sqrt{\frac{N}{\log\log N}}(\mathbf{m}_{0}-\mathbf{\mu}_{0})\) converges to \(\mathbf{0}\) in distribution and hence also in probability. (There could even be almost sure results using the law of iterated logarithm but converge in probability is good enough.)

Similarly by CLT every entries of \(\sqrt{N}(C_{0}-\Sigma_{0})\) converges to a Gaussian distribution. Thus, \(\sqrt{\frac{N}{\log\log N}}(C_{0}-\Sigma_{0})\) converges to zero matrix in probability. Therefore, we have \(\frac{N}{\log\log N}\|C_{t}-\Sigma_{t}\|_{F}^{2}\to 0\) in probability.

By Step II, we have \(\|C_{t}\|_{F}\leq\|Q\|_{F}+F_{3}\). All these constants here (\(F_{3}\), \(F_{7}\), \(\|C_{0}^{-1}\|_{F}\)) can be seen or chosen as a continuous function of \(Q,\mathbf{b},C_{0},\mathbf{m}_{0},\Sigma_{0},\mathbf{\mu}_{0}\) and by continuous mapping theorem they converge to the values of the same function with \(C_{0}=\Sigma_{0}\) and \(\mathbf{m}_{0}=\mathbf{\mu}_{0}\). Thus, we conclude that \(\frac{N}{\log\log N}(I_{6}+I_{7})\to 0\) in probability.

Now we derive that

\[I_{5}\leq \left\|(\Sigma_{t}^{1/2}-C_{t}^{1/2})\Sigma_{0}^{-1/2}C_{0}^{1/2} +C_{t}^{1/2}\Sigma_{0}^{-1/2}(C_{0}^{1/2}-\Sigma_{0}^{1/2})\right\|_{F}^{2}\] \[\leq 2\|\Sigma_{t}^{1/2}-C_{t}^{1/2}\|_{F}^{2}\|\Sigma_{0}^{-1}\|_{F} \|C_{0}\|_{F}+\|C_{t}\|_{F}\|\Sigma_{0}^{-1}\|_{F}\|C_{0}^{1/2}-\Sigma_{0}^{1/ 2}\|_{F}^{2}.\]

Now we show a lemma: Suppose \(A,B\in\mathrm{Sym}^{+}(d,\mathbb{R})\) are two positive definite matrices. Then we have \(\|A^{1/2}-B^{1/2}\|\leq\frac{1}{2\sqrt{\lambda}}\|A-B\|\) where \(\lambda\) is the smallest eigenvalue of \(A\) and \(B\). Note that we are using the spectral norm here.

In fact, denote the largest eigenvector of \(A^{1/2}-B^{1/2}\) by \(\eta\), and let \(\mathbf{x}\in\mathbb{R}^{d}\) be the corresponding eigenvector such that \(\mathbf{x}^{\top}\mathbf{x}=1\). We have

\[\|A-B\|\geq\mathbf{x}^{\top}(A-B)\mathbf{x}\] \[= \mathbf{x}^{\top}A^{1/2}(A^{1/2}-B^{1/2})\mathbf{x}+\mathbf{x}^{\top}(A^{1/2} -B^{1/2})B^{1/2}\mathbf{x}\] \[= \eta\mathbf{x}^{\top}(A^{1/2}+B^{1/2})\mathbf{x}\geq 2\eta\sqrt{\lambda}.\]

Thus, we have that \(\|A^{1/2}-B^{1/2}\|=\eta\leq\frac{1}{2\sqrt{\lambda}}\|A-B\|\). Moreover, the Frobenius norm is bounded by

\[\|A^{1/2}-B^{1/2}\|_{F}\leq\sqrt{d}\|A^{1/2}-B^{1/2}\|\leq\frac{\sqrt{d}}{2 \sqrt{\lambda}}\|A-B\|\leq\frac{\sqrt{d}}{2\sqrt{\lambda}}\|A-B\|_{F}.\]

Applying this lemma, we know

\[\left\|C_{0}^{1/2}-\Sigma_{0}^{1/2}\right\|_{F}\leq\frac{\sqrt{d}}{2\sqrt{ \lambda}}\|\Sigma_{0}-C_{0}\|_{F},\]where \(\lambda\) is the smallest eigenvalue of \(C_{0}\) and \(\Sigma_{0}\), which converges to the smallest eigenvalue of \(\Sigma_{0}\) as \(N\) goes to infinity.

Next we need to show that the smallest eigenvalue of \(C_{t}\) and \(\Sigma_{t}\) are uniformly (in time) lower bounded by some \(\lambda^{\prime}>0\) (depending on \(Q,\boldsymbol{b},C_{0},\boldsymbol{m}_{0},\Sigma_{0},\boldsymbol{\mu}_{0}\)). We revisit Step II, where we show that \(E(\boldsymbol{m}_{t},C_{t})\leq E(\boldsymbol{m}_{0},C_{t})\). Then

\[\operatorname{tr}(Q^{-1}C_{t}-I)-\log\det(Q^{-1}C_{t})\leq 2E(\boldsymbol{m}_{0 },C_{0})\]

leads to a uniform lower bound of the smallest eigenvalue of \(C_{t}\) since \(\operatorname{tr}(Q^{-1}C_{t}-I)-\log\det(Q^{-1}C_{t})\to\infty\) as the smallest eigenvalue of \(C_{t}\) goes down to zero. Thus, we have

\[\left\|C_{t}^{1/2}-\Sigma_{t}^{1/2}\right\|_{F}^{2}\leq\frac{d}{4\lambda^{ \prime}}\|\Sigma_{t}-C_{t}\|_{F}^{2}\leq\frac{dF_{7}}{4\lambda^{\prime}}(\|C _{0}-\Sigma_{0}\|_{F}^{2}+\|\boldsymbol{m}_{0}-\boldsymbol{\mu}_{0}\|^{2}).\]

Following similar arguments we conclude that \(\frac{N}{\log\log N}I_{5}\to 0\) in probability as \(N\to\infty\). Therefore, in probability

\[\frac{N}{\log\log N}\frac{1}{N}\sum_{i=1}^{N}\left\|\widetilde{\boldsymbol{x}} _{i}^{(t)}-\boldsymbol{y}_{i}^{(t)}\right\|\leq\frac{N}{\log\log N}(I_{5}+I_{ 6}+I_{7})\to 0,\]

which implies that

\[\mathbb{E}\left[\mathcal{W}_{2}^{2}\left(\xi_{N}^{(t)},\widetilde{\zeta}_{N}^{ (t)}\right)\right]=o\left(\frac{\log\log N}{N}\right).\]

**Step VI.** Apply Lemma L.1 to get the final result. Note that \(\boldsymbol{x}_{i}^{(t)}\) are _i.i.d._ from \(\mathcal{N}(\boldsymbol{0},I)\) and \(\boldsymbol{y}_{i}^{(t)}\) is a linear function of \(\boldsymbol{x}_{t}^{(t)}\) (unlike \(\boldsymbol{m}_{t}\) and \(C_{t}\) which are random, \(\boldsymbol{\mu}_{t}\) and \(\Sigma_{t}\) are deterministic). By Lemma L.1 and Step II (\(\boldsymbol{\mu}_{t}\) and \(\Sigma_{t}\) are uniformly bounded) we have

\[\mathbb{E}\left[\mathcal{W}_{2}^{2}\left(\xi_{N}^{(t)},\rho_{t}\right)\right] \leq C_{Q,\boldsymbol{b},\Sigma_{0},\boldsymbol{\mu}_{0}}\times\left\{ \begin{array}{cl}N^{-1}\log\log N&\text{ if }d=1\\ N^{-1}(\log N)^{2}&\text{ if }d=2\\ N^{-2/d}&\text{ if }d\geq 3\end{array}\right.. \tag{87}\]

Thus, we derive that

\[\mathbb{E}\left[\mathcal{W}_{2}^{2}(\zeta_{N}^{(t)},\rho_{t}) \right]\overset{(*)}{=}\mathbb{E}\left[\mathcal{W}_{2}^{2}(\widetilde{\zeta}_ {N}^{(t)},\rho_{t})\right]\] \[\overset{(**)}{\leq}\mathbb{E}\left[\left(\mathcal{W}_{2}( \widetilde{\zeta}_{N}^{(t)},\xi_{N}^{(t)})+\mathcal{W}_{2}(\xi_{N}^{(t)},\rho _{t})\right)^{2}\right]\] \[\leq 2\mathbb{E}\left[\mathcal{W}_{2}^{2}\left(\xi_{N}^{(t)}, \widetilde{\zeta}_{N}^{(t)}\right)\right]+2\mathbb{E}\left[\mathcal{W}_{2}^{2 }\left(\xi_{N}^{(t)},\rho_{t}\right)\right]\] \[\overset{(***)}{\leq}C_{Q,\boldsymbol{b},\Sigma_{0},\boldsymbol {\mu}_{0}}\times\left\{\begin{array}{cl}N^{-1}\log\log N&\text{ if }d=1\\ N^{-1}(\log N)^{2}&\text{ if }d=2\\ N^{-2/d}&\text{ if }d\geq 3\end{array}\right..\]

Note that we have used Step I in \((*)\), the triangle inequality in \((**)\), and Step V along with (87) in \((***)\).

Proof of Theorem F.4.: The proof is roughly similar to that of Theorem 3.7. But Step III is a little different (can be strengthened and simplified at the same time).

We show the global contraction of \(\|\boldsymbol{m}_{t}-\boldsymbol{\mu}_{t}\|\) and also the contraction of \(\|C_{t}-\Sigma_{t}\|_{F}\) after sufficient time. Note here \(\|\cdot\|_{F}\) is the Frobenius norm.

First we check the derivative of squared Euclidean norm of \(\boldsymbol{m}_{t}-\boldsymbol{\mu}_{t}\).

\[\frac{1}{2}\frac{\mathrm{d}}{\mathrm{d}t}\|\boldsymbol{m}_{t}- \boldsymbol{\mu}_{t}\|^{2}=(\boldsymbol{m}_{t}-\boldsymbol{\mu}_{t})^{\top}( \dot{\boldsymbol{m}}_{t}-\dot{\boldsymbol{\mu}}_{t})\] \[= -(\boldsymbol{m}_{t}-\boldsymbol{\mu}_{t})^{\top}Q^{-1}( \boldsymbol{m}_{t}-\boldsymbol{\mu}_{t})\leq-\frac{1}{q_{\max}}\|\boldsymbol{m }_{t}-\boldsymbol{\mu}_{t}\|^{2}.\]Thus, \(\|\mathbf{m}_{t}-\mathbf{\mu}_{t}\|\leq e^{-\frac{\epsilon}{\mathrm{tr}}}\|\mathbf{m}_{0}-\mathbf{ \mu}_{0}\|\).

To bound \(\|C_{t}-\Sigma_{t}\|_{F}\) we define

\[\mathcal{S}_{\epsilon}:=\left\{S\in\mathrm{Sym}^{+}(d,\mathbb{R}):(1-\epsilon) Q\preceq S\preceq(1+\epsilon)Q\right\}.\]

This time we do not need the relative energy but can directly check the derivative of the squared Frobenius norm:

\[\frac{1}{2}\frac{\mathrm{d}}{\mathrm{d}t}\|C_{t}-\Sigma_{t}\|_{F}^ {2}=\frac{1}{2}\frac{\mathrm{d}}{\mathrm{d}t}\operatorname{tr}((C_{t}-\Sigma_{ t})^{2})\overset{\text{tr}}{=}(C_{t}-\Sigma_{t})(\dot{C}_{t}-\dot{\Sigma}_{t})\] \[\overset{\text{tr}}{=}2(C_{t}-\Sigma_{t})^{2}-2(C_{t}-\Sigma_{t})( C_{t}^{2}-\Sigma_{t}^{2})Q^{-1}\] \[\overset{\text{tr}}{=}2(C_{t}-\Sigma_{t})^{2}-(C_{t}-\Sigma_{t})( C_{t}+\Sigma_{t})(C_{t}-\Sigma_{t})Q^{-1}-(C_{t}-\Sigma_{t})^{2}(C_{t}+\Sigma_{t})Q ^{-1}\] \[\overset{\text{tr}}{=}2(C_{t}-\Sigma_{t})^{2}-(C_{t}-\Sigma_{t})^ {2}\left(C_{t}+\Sigma_{t}\right)Q^{-1}-(C_{t}-\Sigma_{t})(C_{t}+\Sigma_{t})(C_{ t}-\Sigma_{t})Q^{-1}. \tag{88}\]

where \(\overset{\text{tr}}{=}\) denotes equal in trace. Now if \(C_{t},\Sigma_{t}\in S_{\epsilon}\) then

\[\operatorname{tr}\left((C_{t}-\Sigma_{t})(C_{t}+\Sigma_{t})(C_{t} -\Sigma_{t})Q^{-1}\right)=\operatorname{tr}\left(Q^{-1/2}(C_{t}-\Sigma_{t})(C_ {t}+\Sigma_{t})(C_{t}-\Sigma_{t})Q^{-1/2}\right)\] \[\geq \operatorname{tr}\left(2(1-\epsilon)Q^{-1/2}(C_{t}-\Sigma_{t})Q(C _{t}-\Sigma_{t})Q^{-1/2}\right)\overset{\diamond}{\geq}2(1-\epsilon) \operatorname{tr}\left((C_{t}-\Sigma_{t})^{2}\right), \tag{89}\]

and

\[\operatorname{tr}\left((C_{t}-\Sigma_{t})^{2}(C_{t}+\Sigma_{t})Q^ {-1}\right)-2(1-\epsilon)\operatorname{tr}\left((C_{t}-\Sigma_{t})^{2}\right)\] \[= \operatorname{tr}\left((C_{t}-\Sigma_{t})^{2}(C_{t}+\Sigma_{t}-2( 1-\epsilon)Q)Q^{-1}\right)\] \[= \frac{1}{2}\operatorname{tr}\left((C_{t}-\Sigma_{t})\left((C_{t}+ \Sigma_{t}-2(1-\epsilon)Q)Q^{-1}+Q^{-1}(C_{t}+\Sigma_{t}-2(1-\epsilon)Q) \right)(C_{t}-\Sigma_{t})\right)\geq 0. \tag{90}\]

Note that \(\diamond\) is not trivially true. We show it as a lemma: Suppose \(A\) is a symmetric matrix and \(B\) is a positive definite symmetric matrix. Then \(\operatorname{tr}(ABAB^{-1})\geq\operatorname{tr}(A^{2})\).

In fact, we write \(B=P\Lambda P^{\top}\) where \(P\) is an orthogonal matrix and \(\Lambda=\operatorname{diag}\{\lambda_{2},\cdots,\lambda_{d}\}\) is a diagonal matrix. Then

\[\operatorname{tr}(ABAB^{-1})=\operatorname{tr}(AP\Lambda P^{\top}AP\Lambda^{- 1}P^{\top})=\left\|\Lambda^{1/2}P^{\top}AP\Lambda^{-1/2}\right\|_{F}^{2}.\]

Denoting \(P^{\top}AP=(a_{ij})\) we get

\[\left\|\Lambda^{1/2}P^{\top}AP\Lambda^{-1/2}\right\|_{F}^{2}=\sum _{i,j=1}^{d}\left(\frac{\sqrt{\lambda_{i}}}{\sqrt{\lambda_{j}}}a_{ij}\right)^{2}\] \[= \frac{1}{2}\sum_{i,j=1}^{d}\left(\frac{\lambda_{i}}{\lambda_{j}}a_ {ij}^{2}+\frac{\lambda_{j}}{\lambda_{i}}a_{ji}^{2}\right)\geq\sum_{i,j=1}^{d} a_{ij}^{2}=\operatorname{tr}(A^{2}).\]

Thus, substituting (90) and (89) into (88) we get

\[\frac{\mathrm{d}}{\mathrm{d}t}\|C_{t}-\Sigma_{t}\|_{F}^{2}=-2 \operatorname{tr}\left((C_{t}-\Sigma_{t})(\dot{C}_{t}-\dot{\Sigma}_{t})\right) \leq-4(1-2\epsilon)\|C_{t}-\Sigma_{t}\|_{F}^{2}\leq 0.\]

Suppose at time \(t_{0}\) both \(C_{t_{0}}\) and \(\Sigma_{t_{0}}\) lie in \(\mathcal{S}_{\epsilon}\). Then for any \(t\geq t_{0}\) we have

\[\|C_{t}-\Sigma_{t}\|_{F}\leq e^{-2(1-2\epsilon)(t-t_{0})}\|C_{t_{0}}-\Sigma_{t_{0 }}\|_{F}.\]