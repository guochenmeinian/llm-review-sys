# Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Bayesian methodologies for handling count-valued time series have gained prominence due to their ability to infer interpretable latent structures and to estimate uncertainties, and thus are especially suitable for dealing with _noisy_ and _incomplete_ count data. Among these Bayesian models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to be effective in capturing the evolving dynamics underlying observed count sequences. However, the state-of-the-art PGDS still falls short in capturing the _time-varying_ transition dynamics that are commonly observed in real-world count time series. To mitigate this limitation, a non-stationary PGDS is proposed to allow the underlying transition matrices to evolve over time, and the evolving transition matrices are modeled by the specifically-designed Dirichlet Markov chains. Leveraging Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed to perform posterior simulation. Experiments show that, in comparison with related models, the proposed non-stationary PGDS achieves improved predictive performance due to its capacity to learn non-stationary dependency structure captured by the time-evolving transition matrices.

## 1 Introduction

In recent years, there has been an increasing interest in modeling count time series. For instance, some previous works [1, 2, 3] are concerned with how to learn the evolving topics behind text corpus (frequencies of words) over time. Some works [4, 5, 6, 7] try to predict global immigrant trends underlying international population movements. Count time series are often _overdispersed_, _sparse_, _high-dimensional_, and thus can not be well modeled by widely used dynamic models such as linear dynamical systems [8, 9]. Recently, many works [10, 11, 12, 13, 14, 15, 16] prefer to choose distributions of the gamma-Poisson family to build their hierarchical Bayesian models. In particular, these models enjoy strong explainability and can estimate uncertainty especially when the observations are _noisy_ and _incomplete_. Among these works, Poisson-Gamma Dynamical Systems (PGDSs) [13] received a lot of attention because PGDS can learn how the latent dimensions excite each other to capture complicated dynamics in observed count series. For instance, a very inspiring research paper may motivate other researchers to publish papers on related topics [17]. The outbreak of COVID-19 in one state, may lead to the rapid rising of COVID-19 cases in the nearby states and vice versa [18]. In particular, PGDS can be efficiently learned with a tractable Gibbs sampling scheme via Poisson-Logarithmic data augmentation and marginalization technique [11]. Due to its strong flexibility, PGDS achieves better performance in predicting missing entities and future observations, compared with related models [9, 15].

Despite these advantages, PGDS still can not capture the time-varying transition dynamics underlying observed count sequences, which are commonly observed in real-world scenarios [19]. For instance,during the initial stage of the COVID-19 pandemic, the worldwide counts of infectious patients were significantly affected by various local policies, government interventions, and emergent events [20; 21; 22]. The cross transition dynamics among the different monitoring areas were also evolving as the corresponding policies and interventions changed over time. Hence, PGDS unavoidably makes a certain amount of approximation error in capturing the aforementioned non-stationary count time series, using a _time-invariant_ transition kernel.

To mitigate this limitation, Non-Stationary Poisson-Gamma Dynamical Systems (NS-PGDSs), a novel kind of Poisson-gamma dynamical systems with non-stationary transition dynamics are developed. More specifically, NS-PGDS captures the evolving transition dynamics by the specifically-designed Dirichlet Markov chains. Via the Dirichlet-Multinomial-Beta data augmentation strategy, the Non-Stationary Poisson-Gamma Dynamical Systems can be inferred with a conjugate-yet-efficient Gibbs sampler. Our contributions are summarized as follows:

* We propose a Non-Stationary Poisson-Gamma Dynamical System (NS-PGDS), a novel Poisson-gamma dynamical system with time-evolving transition matrices that can well capture non-stationary transition dynamics underlying observed count series.
* Three Dirichlet Markov chains are dedicated to improving the flexibility and expressiveness of NS-PGDSs, for capturing the complex transition dynamics behind sequential count data.
* Fully-conjugate-yet-efficient Gibbs samplers are developed via Dirichlet-Multinomial-Beta augmentation techniques to perform posterior simulation for the proposed Dirichlet Markov chains.
* Extensive experiments are conducted on four real-world datasets, to evaluate the performance of the proposed NS-PGDS in predicting missing and future unseen observations. We also provide exploratory analysis to demonstrate the explainable latent structure inferred by the proposed NS-PGDS.

## 2 Preliminaries

Let \(\mathbf{y}^{(t)}=\left[y_{1}^{(t)},\cdots,y_{V}^{(t)}\right]^{\mathrm{T}}\in \mathbb{N}^{V}\) be a vector of nonnegative count valued observations at time \(t\). To capture the latent dynamics underlying count sequences, some previous works [23; 24] model the observations as

\[\mathbf{y}^{(t)}=p\left(\mathbf{z}^{(t)}\right),\ \ \mathbf{z}^{(t)}=f^{-1}\left(\mathbf{x}^{(t) }\right),\]

where \(p\left(\cdot\right)\) is the observation likelihood function, and \(f\left(\cdot\right)\) is an invertible link function that maps the parameters of observation component to continuous-valued latent variables \(\mathbf{x}^{(t)}\in\mathbb{R}^{K}\). The latent factor \(\mathbf{x}^{(t)}\) evolves over time according to a linear dynamical system (LDS) given by \(\mathbf{x}^{(t)}\sim\mathcal{N}(\mathbf{A}\mathbf{x}^{(t-1)},\mathbf{\Lambda}^{-1})\), where \(\mathbf{A}\) is the state transition matrix of size \(K\times K\), and \(\mathbf{\Lambda}=\mathrm{diag}\left(\lambda_{1},\cdots,\lambda_{K}\right)\) is the inverse covariance matrix with \(\lambda_{k}^{-1}\) determining the variance of \(k\)-th latent dimension. Han et al. [23] adopted the Extended Rank likelihood function to model count observations using LDS with time complexity \(\mathcal{O}((K+V)^{3})\), which prevents it from practical applications for analyzing large-scale count data.

Recently, Acharya et al. [15] and Schein et al. [13; 16] developed Poisson-gamma family models for sequential count observations. Gamma Process Dynamic Poisson Factor Analysis (GP-DPFA) [15] models count data as \(y_{v}^{(t)}\sim\mathrm{Pois}(\sum_{k=1}^{K}\lambda_{k}\phi_{vk}\theta_{k}^{(t)})\), where \(\theta_{k}^{(t)}\) represents the strength of \(k\)-th latent factor at time \(t\), and \(\phi_{vk}\) captures the involvement degree of \(k\)-th factor to \(v\)-th observed dimension. To ensure the model identifiability, we can impose a restriction as \(\sum_{v}\phi_{vk}=1\), and thus place a Dirichlet prior over \(\mathbf{\phi_{k}}=\left[\phi_{1k},\cdots,\phi_{Vk}\right]^{T}\) as \(\mathbf{\phi_{k}}\sim\mathrm{Dir}\left(\epsilon_{0},\cdots,\epsilon_{0}\right)\).

To capture the underlying dynamics, the latent factor \(\theta_{k}^{(t)}\) evolves over time according to a gamma Markov chain as \(\theta_{k}^{(t)}\sim\mathrm{Gam}(\theta_{k}^{(t-1)},c_{t})\), where \(c_{t}\) is the rate parameter of the gamma distribution to control the variance of the gamma Markov chains. Although GP-DPFA can well fit one-dimensional count sequences, it fails to learn how the latent dimensions interact with each other.

To address this concern, Schein et al. [13] developed Poisson-gamma dynamical systems to capture the underlying transition dynamics. In particular, \(\theta_{k}^{(t)}\) evolves over time as \(\mathrm{Gam}(\tau_{0}\sum_{k_{2}=1}^{K}\pi_{kk_{2}}\theta_{k_{2}}^{(t-1)},\tau_{0})\), where \(\pi_{kk_{2}}\) represents how \(k_{2}\)-th latent factor excites the \(k\)-th latent factor at next time step, and \(\sum_{k=1}^{K}\pi_{kk_{2}}=1\).

## 3 Non-Stationary Poisson-Gamma Dynamical Systems

Real-world count time sequences are often _non-stationary_ because the external interventional environments are always changing over time. The stationary PGDS with a time-invariant transition kernel fails to capture such time-varying transition dynamics. For instance, the transition dynamics behind COVID-19 infectious processes are time-varying, and highly affected by various interventional policies. Hence, to mitigate this limitation, we model the count sequences as

\[y_{v}^{(t)}\sim\mathrm{Pois}\left(\delta^{(t)}\sum_{k=1}^{K}\phi_{vk}\theta_{k }^{(t)}\right),\]

in which, the latent factors are specified by

\[\theta_{k}^{(t)}\sim\mathrm{Gam}\left(\tau_{0}\sum_{k_{2}=1}^{K}\pi_{kk_{2}}^{ (t-1)}\theta_{k_{2}}^{(t-1)},\tau_{0}\right), \tag{1}\]

where the multiplicative term \(\delta^{(t)}\sim\mathrm{Gam}\left(\epsilon_{0},\epsilon_{0}\right)\) and the transition matrices are time-varying as \(\mathbf{\Pi}^{(t)}\equiv\left[\pi_{kk_{2}}^{(t)}\right]_{k,k_{2}=1}^{K}\). As shown in Figure 2, to model the time-varying transition dynamics, we assume the whole time interval can be divided into \(I\) equally-spaced sub-intervals. The transition kernel behind complicated dynamic counts is assumed to be _static_ within each sub-interval, while evolving over sub-intervals, to capture non-stationary behaviours. In another word, the proposed model allows the latent factors to evolve over time steps while the transition matrices change over sub-intervals but assumed to be stationary within each sub-interval, as shown in Figure 1. In particular, we let each sub-interval contains \(M\) time steps, and the \(i\)-th interval contains time steps \(\left\{t\mid t=\left(i-1\right)M+1,\cdots,iM\right\}\). We define \(i\left(t\right)\) as the function that maps time step \(t\) to its corresponding sub-interval.

**Dirichlet-Dirichlet Markov processes.** To capture how the underlying transition kernel smoothly evolves over sub-intervals, we first propose the Dirichlet-Dirichlet (Dir-Dir) Markov chain as

\[\mathbf{\pi}_{k}^{(i)}\mid\mathbf{\pi}_{k}^{(i-1)}\sim\mathrm{Dir}\left(\eta K \pi_{1k}^{(i-1)},\cdots,\eta K\pi_{Kk}^{(i-1)}\right), \tag{2}\]

where \(\mathbf{\pi}_{k}^{(i)}\) represents the \(k\)-th column of \(\mathbf{\Pi}^{(i)}\), and the prior of the scaling parameter \(\eta\) is given by \(\eta\sim\mathrm{Gam}\left(\epsilon_{0},f_{0}\right)\).

Figure 1: The graphical representation of the NS-PGDS. The time interval is divided into equally-spaced sub-intervals. Each sub-interval contains \(M\) time steps. The transition dynamics is stationary within a sub-interval. In particular, the transition matrices evolve over sub-intervals via Dirichlet Markov processes while latent factors evolve over time steps via Eq.(1).

Figure 2: An example illustrates the Poisson-gamma dynamical systems with non-stationary transition kernels. The three gamma dynamic processes independently evolve over time during the (\(i-1\))-th interval. During \(i\)-th interval, \(\theta_{1}^{(t)}\) and \(\theta_{2}^{(t)}\) gradually starts to interact with each other while \(\theta_{3}^{(t)}\) remains independent to the other two dimensions. During (\(i+1\))-th interval all the three latent components start to interact with each other.

The initial states are defined as \(\theta_{k}^{(1)}\sim\operatorname{Gam}\left(\tau_{0}\nu_{k},\tau_{0}\right)\). The prior for the transition kernel of the first sub-interval is given by \(\mathbf{\pi}_{k}^{(1)}\sim\operatorname{Dir}\left(\nu_{1}\nu_{k},\cdots,\xi\nu_{k}, \cdots,\nu_{K}\nu_{k}\right)\), where \(\nu_{k}\sim\operatorname{Gam}(\frac{\eta}{\mathcal{R}},\beta)\) and \(\xi,\beta\sim\operatorname{Gam}\left(\epsilon_{0},\epsilon_{0}\right)\). Note that the expectation and variance of the transition kernel at \(i\)-th sub-interval can be calculated as

\[\mathsf{E}\left[\mathbf{\pi}_{k}^{(i)}\mid\mathbf{\pi}_{k}^{(i-1)}\right]=\mathbf{\pi}_{k}^ {(i-1)},\quad\mathsf{Var}\left[\pi_{k_{1}k}^{(i)}\mid\mathbf{\pi}_{k}^{(i-1)} \right]=\frac{\pi_{k_{1}k}^{(i-1)}\left(1-\pi_{k_{1}k}^{(i-1)}\right)}{\eta K+1},\]

respectively. The transition dynamics of \(i\)-th sub-interval inherits the information of the previous sub-interval, and also adapts to the data observed in the current sub-interval. The scaling parameter \(\eta\) controls the variance of the transition matrices.

The prior specification defined in Eq.(2) by rescaling the transition matrix at the previous sub-interval allows the transition dynamics to change smoothly, and thus might be insufficient to capture the rapid changes observed in complicated dynamics. To further improve the flexibility of the transition structure, two modified Dirichlet Markov chains are studied to capture the correlation structure between the dimensions of the transition matrices over time. **Dirichlet-Gamma-Dirichlet Markov processes.** We first introduce the Dirichlet-Gamma-Dirichlet (Dir-Gam-Dir) Markov chain to model the evolving transition matrices as

\[\mathbf{\pi}_{k}^{(i)} \sim\operatorname{Dir}\left(\alpha_{1k}^{(i)},\cdots,\alpha_{Kk}^ {(i)}\right),\] \[\alpha_{k_{1}k}^{(i)} \sim\operatorname{Gam}\left(\gamma_{k}^{(i-1)}\sum_{k_{2}=1}^{K} \psi_{kk_{1}k_{2}}^{(i-1)}\pi_{k_{2}k}^{(i-1)},c_{k}^{(i)}\right), \tag{3}\]

where we use \(\psi_{kk_{1}k_{2}}^{(i-1)}\) to capture the mutation between two consecutive sub-intervals, and its prior is given by

\[\left(\psi_{k1k_{2}}^{(i-1)},\cdots,\psi_{kkK_{2}}^{(i-1)}\right)\sim \operatorname{Dir}\left(\epsilon_{0},\cdots,\epsilon_{0}\right),\]

and \(\gamma_{k}^{(i)},c_{k}^{(i)}\sim\operatorname{Gam}\left(\epsilon_{0},\epsilon _{0}\right)\). Compared with the construction defined by Eq.(2), the expectation of Dirichlet-Gamma-Dirichlet Markov chain is

\[\mathsf{E}\left[\mathbf{\pi}_{k}^{(i)}\mid\mathbf{\pi}_{k}^{(i-1)}\right]=\mathbf{\Psi}_{ k}^{(i-1)}\mathbf{\pi}_{k}^{(i-1)}.\]

This construction takes interactions among components of columns into account. Hence it will dramatically improve the flexibility of our model and thus better fit more complicated dynamics, compared with Dir-Dir Markov chains that only yield smoothing transition dynamics.

**Poisson-randomized-gamma-Dirichlet Markov processes.** By leveraging the Poisson-randomized gamma distribution [25], we introduce another type of time-varying transition kernels, which also model the interactions among components like Dir-Gam-Dir construction but may induce different properties such as sparsity. The Poisson-randomized-gamma-Dirichlet (PR-Gam-Dir) Markov chain can be formulated as

\[\mathbf{\pi}_{k}^{(i)}\sim\operatorname{Dir}\left(\alpha_{1k}^{(i)},\cdots,\alpha _{Kk}^{(i)}\right),\;\alpha_{k_{1}k}^{(i)}\sim\operatorname{RG1}\left(\epsilon ^{\alpha},\gamma_{k}^{(i-1)}\sum_{k=2}^{K}\psi_{kk_{1}k_{2}}^{(i-1)}\pi_{k_{2} k}^{(i-1)},c_{k}^{(i)}\right), \tag{4}\]

where \(\operatorname{RG1}\left(\cdot\right)\) denotes the randomized gamma distribution of the first type. Similarly, for \(\psi_{kk_{1}k_{2}}^{(i-1)}\), \(\gamma_{k}^{(i)}\), and \(c_{k}^{(i)}\), the priors are given by

\[\left(\psi_{k1k_{2}}^{(i-1)},\cdots,\psi_{kkK_{2}}^{(i-1)}\right)\sim \operatorname{Dir}\left(\epsilon_{0},\cdots,\epsilon_{0}\right),\;\gamma_{k}^ {(i)},c_{k}^{(i)}\sim\operatorname{Gam}\left(\epsilon_{0},\epsilon_{0}\right), \text{ respectively}.\]

The diagrams of three Dirichlet Markov constructions are shown in Figure 3.

## 4 Markov Chain Monte Carlo Inference

In this section, we present the Gibbs sampler for the proposed NS-PGDS. We only illustrate the key points of the derivation and the details can be found in the appendix.

Figure 3: Diagrams of the proposed Dirichlet Markov constructions. (a) is the Dir-Dir construction. (b) is the Dir-Gam-Dir construction which takes mutation into account. (c) illustrates the PR-Gam-Dir construction which adopts Poisson randomized gamma distribution and can be equivalently represented as Eq.(5).

**Lemma 1**: _If \(y\sim\mathrm{NB}\left(a,g\left(\zeta\right)\right)\) and \(l\sim\mathrm{CRT}\left(y,a\right)\), where \(\mathrm{NB}\left(\cdot\right)\) refers to negative-binomial distribution, \(\mathrm{CRT}\left(\cdot\right)\) represents Chinese restaurant table distribution [26], and \(g\left(z\right)=1-\exp\left(-z\right)\). Then the joint distribution of \(y\) and \(l\) can be equivalently distributed as \(y\sim\mathrm{SumLog}\left(l,g\left(\zeta\right)\right)\) and \(l\sim\mathrm{Pois}\left(a\zeta\right)\)[11], i.e._

\[\mathrm{NB}\left(y;a,g\left(\zeta\right)\right)\mathrm{CRT}\left(l;y,a\right)= \mathrm{SumLog}\left(y;l,g\left(\zeta\right)\right)\mathrm{Pois}\left(l;a\zeta \right),\]

_where \(\mathrm{SumLog}\left(l,g\left(\zeta\right)\right)=\sum_{i=1}^{l}x_{i}\) and \(x_{i}\sim\mathrm{Log}\left(g\left(\zeta\right)\right)\) are independently and identically logarithmic distributed random variables [27]._

**Lemma 2**: _Suppose \(\mathbf{n}=\left(n_{1},\cdots,n_{K}\right)\) and_

\[\mathbf{n}\mid n\sim\mathrm{DirMult}\left(n,r_{1},\cdots,r_{K}\right),\]

_where \(\mathrm{DirMult}\left(\cdot\right)\) refers to Dirichlet-multinomial distribution. We sample the augmented variable \(q\mid n\sim\mathrm{Beta}\left(n,r\right)\), where \(r.=\sum_{k=1}^{K}r_{k}\). According to [28], conditioning on \(q\), we have \(n_{k}\sim\mathrm{NB}\left(r_{k},q\right)\)._

**Sampling \(y_{vk}^{\left(t\right)}\) :** Use the relationship between Poisson and multinomial distributions, we sample

\[\left(\left(y_{vk}^{\left(t\right)}\right)_{k=1}^{K}\mid-\right)\sim\mathrm{ Mult}\left(y_{v}^{\left(t\right)},\left(\frac{\phi_{vk}\theta_{k}^{\left(t \right)}}{\sum_{k=1}^{K}\phi_{vk}\theta_{k}^{\left(t\right)}}\right)_{k=1}^{K }\right).\]

**Sampling \(\boldsymbol{\phi_{k}}\) :** Via Dirichlet-multinomial conjugacy, the posterior of \(\boldsymbol{\phi_{k}}\) is

\[\left(\boldsymbol{\phi_{k}}\mid-\right)\sim\mathrm{Dir}\left(\epsilon_{0}+ \sum_{t=1}^{T}y_{1k}^{\left(t\right)},\cdots,\epsilon_{0}+\sum_{t=1}^{T}y_{Vk }^{\left(t\right)}\right).\]

**Sampling \(\theta_{k}^{\left(t\right)}\) :** To sample from the posterior of \(\theta_{k}^{\left(t\right)}\), we first sample the auxiliary variables. Setting \(l_{\cdot k}^{\left(T+1\right)}=0\) and \(\zeta^{\left(T+1\right)}=0\), we sample the augmented variables backwards from \(t=T,\cdots,2\),

\[\left(l_{k\cdot}^{\left(t\right)}\mid-\right)\sim\mathrm{CRT}\left(y_{\cdot k} ^{\left(t\right)}+l_{\cdot k}^{\left(t+1\right)},\tau_{0}\sum_{k_{2}=1}^{K} \pi_{kk_{2}}^{i\left(t-1\right)}\theta_{k_{2}}^{\left(t-1\right)}\right),\]

\[\left(l_{k1}^{\left(t\right)},\cdots,l_{kK}^{\left(t\right)}\mid-\right)\sim \mathrm{Mult}\left(l_{k\cdot}^{\left(t\right)},\left(\frac{\pi_{k1}^{i\left(t -1\right)}\theta_{1}^{\left(t-1\right)}}{\sum_{k_{2}=1}^{K}\pi_{kk_{2}}^{i \left(t-1\right)}\theta_{k_{2}}^{\left(t-1\right)}},\cdots,\frac{\pi_{kK}^{i \left(t-1\right)}\theta_{K}^{\left(t-1\right)}}{\sum_{k_{2}=1}^{K}\pi_{kk_{2}} ^{i\left(t-1\right)}\theta_{k_{2}}^{\left(t-1\right)}}\right)\right).\]

Let us define \(l_{\cdot k}^{\left(t\right)}=\sum_{k_{1}=1}^{K}l_{k_{1}k}^{\left(t\right)}\) and \(\zeta^{\left(t\right)}=\ln(1+\frac{\delta^{\left(t\right)}}{\tau_{0}}+\zeta^{ \left(t+1\right)})\). After sampling the auxiliary variables, then for \(t=1,\cdots,T\), by Poisson-gamma conjugacy, we obtain

\[\left(\theta_{k}^{\left(t\right)}\mid-\right)\sim\mathrm{Gam}\left(y_{\cdot k} ^{\left(t\right)}+l_{\cdot k}^{\left(t+1\right)}+\tau_{0}\sum_{k_{2}=1}^{K}\pi _{kk_{2}}^{i\left(t-1\right)}\theta_{k_{2}}^{\left(t-1\right)},\tau_{0}+\delta ^{\left(t\right)}+\zeta^{\left(t+1\right)}\tau_{0}\right).\]

**Sampling \(\mathbf{\Pi}^{\left(i\right)}\) :** We only illustrate Gibbs sampling algorithm for PR-Gam-Dir construction, sampling algorithms for other constructions can be found in the appendix. We define \(M\) as the length of each sub-interval, and \(I\) as the number of intervals. For \(i=I,\cdots,2\), because \((l_{1k}^{\left(i\right)},\cdots,l_{Kk}^{\left(i\right)})\) and \((g_{\cdot 1k}^{\left(i+1\right)},\cdots,g_{\cdot Kk}^{\left(i+1\right)})\) are multinomially distributed, where \(l_{k_{1}k}^{\left(i\right)}=\sum_{\left(i-1\right)M+1}^{iM}l_{k_{1}k}^{\left(t \right)}\) refers to the summation of \(l_{k_{1}k}^{\left(t\right)}\) over \(i\)-th sub-interval and same notation for other variables. By the definition of Dirichlet-multinomial distribution and Lemma 2, defining \(g_{k_{1}k}^{\left(1+1\right)}=0\), we sample the auxiliary variables as \((q_{k}^{\left(i\right)}\mid-)\sim\mathrm{Beta}(l_{\cdot k}^{\left(i\right)}+g_{ \cdot k}^{\left(i+1\right)},\alpha_{k}^{\left(i\right)})\), then we have \((l_{k_{1}k}^{\left(i\right)}+g_{\cdot k_{1}k}^{\left(i+1\right)})\sim\mathrm{ NB}(\alpha_{k_{1}k}^{\left(i\right)},q_{k}^{\left(i\right)})\). Then we further sample \((h_{k_{1}k}^{\left(i\right)}\mid-)\sim\mathrm{CRT}(l_{k_{1}k}^{\left(i \right)}+g_{\cdot k_{1}k}^{\left(i+1\right)},\alpha_{k_{1}k}^{\left(i\right)})\). Via Lemma 1, we obtain \(h_{k_{1}k}^{\left(i\right)}\sim\mathrm{Pois}(-\alpha_{k_{1}k}^{\left(i\right)} \mathrm{ln}(1-q_{k}^{\left(i\right)}))\). For Dirichlet-Randomized-Gamma-Dirichlet Markov construction defined by Eq.(4), we can equivalently represent it as

\[\alpha_{k_{1}k}^{\left(i\right)}\sim\mathrm{Gam}\left(g_{k_{1}k}^{\left(i\right)} +\epsilon^{\alpha},c_{k}^{\left(i\right)}\right),\ g_{k_{1}k}^{\left(i\right)}= \mathrm{Pois}\left(\gamma^{\left(i-1\right)}\sum_{k=2}^{K}\psi_{kk_{1}k_{2}}^{ \left(i-1\right)}\pi_{k_{2}k}^{\left(i-1\right)}\right). \tag{5}\]

We define \(\lambda_{k_{1}k}^{\left(i-1\right)}\triangleq\gamma_{k}^{\left(i-1\right)} \sum_{k=2}^{K}\psi_{kk_{1}k_{2}}^{\left(i-1\right)}\pi_{k_{2}k}^{\left(i-1 \right)}\) for notation conciseness. By Poisson-gamma conjugacy, we have \((\alpha_{k_{1}k}^{\left(i\right)}\mid-)\sim\mathrm{Gam}(g_{k_{1}k}^{\left(i \right)}+\epsilon^{\alpha}+h_{k_{1}k}^{\left(i\right)},c_{k}^{\left(i\right)}- \ln(1-q_{k}^{\left(i\right)}))\). If \(\epsilon^{\alpha}>0\), we cansample the posterior of \(g^{(i)}_{k_{1}k}\) via \((g^{(i)}_{k_{1}k}\mid-)\sim\text{Bessel}(\epsilon^{\alpha}-1,2\sqrt{\alpha^{(i)}_{ k_{1}k}c^{(i)}_{k}\lambda^{(i-1)}_{k_{1}k}})\), where Bessel \((\cdot)\) denotes Bessel distribution. If \(\epsilon^{\alpha}=0\), we sample \(g^{(i)}_{k_{1}k}\) via

\[\left(g^{(i)}_{k_{1}k}\mid-\right)\sim\left\{\begin{array}{ll}\text{Pois} \left(\frac{c^{(i)}_{k_{1}k}\lambda^{(i-1)}_{k_{1}k}}{c^{(i)}_{k}-\ln\left(-g^ {(i)}_{k_{1}k}\right)}\right)&\text{if }h^{(i)}_{k_{1}k}=0\\ \text{SCH}\left(h^{(i)}_{k_{1}k},\frac{c^{(i)}_{k}\lambda^{(i-i)}_{k_{1}k}}{c^{ (i)}_{k}-\ln\left(-g^{(i)}_{k}\right)}\right)&\text{otherwise},\end{array}\right.\]

where \(\text{SCH}\left(\cdot\right)\) denotes the shifted confluent hypergeometric distribution [16]. Defining \(g^{(i)}_{k_{1}k}=g^{(i)}_{k_{1}k}=\sum_{k=2}^{K}g^{(i)}_{k_{1}k_{2}k}\), we first augment

\[\left(g^{(i)}_{k_{1}1k},\cdots,g^{(i)}_{k_{1}Kk}\right)\sim\text{Mult}\left(g^ {(i)}_{k_{1}k},\left(\psi^{(i-1)}_{kk_{1}k_{2}}\pi^{(i-1)}_{k_{2}k}\right)^{K}_ {k_{2}=1}\right),\]

then we obtain \(g^{(i)}_{k_{1}k_{2}k}\sim\text{Pois}(\gamma^{(i-1)}\psi^{(i-1)}_{kk_{1}k_{2}} \pi^{(i-1)}_{k_{2}k})\). By Dirichlet-multinomial conjugacy, we have

\[\left(\left(\psi^{(i-1)}_{k1k_{2}},\cdots,\psi^{(i-1)}_{kKK_{2}} \right)\mid-\right)\sim\text{Dir}\left(\epsilon_{0}+g^{(i)}_{1k_{2}k},\cdots, \epsilon_{0}+g^{(i)}_{Kk_{2}k}\right),\text{ and }\] \[\left(\mathbf{\pi}^{(i-1)}_{k}\mid-\right)\sim\text{Dir}\left(\alpha^ {(i-1)}_{1k}+l^{(i-1)}_{1k}+g^{(i)}_{1,k},\cdots,\alpha^{(i-1)}_{Kk}+l^{(i-1)} _{Kk}+g^{(i)}_{\cdot Kk}\right).\]

Specifically, we have \(\alpha^{(1)}_{k_{1}k}=\nu_{k_{1}}\nu_{k}\), if \(k_{1}\neq k\), and \(\alpha^{(1)}_{k_{1}k}=\xi\nu_{k}\), if \(k_{1}=k\).

## 5 Related Work

Modeling count time sequences has been receiving increasing attentions in statistical and machine learning communities. Han et al. [23] adopted linear dynamical systems to capture the underlying dynamics of the data and leveraged Extended Rank likelihood function to model count observations. Some Poisson-gamma models assume that the count vector at each time step is modeled by Poisson factor analysis (PFA) [11] and leverage special stochastic processes to model the temporal dependencies of latent factors. For example, gamma process dynamic Poisson factor analysis (GP-DPFA) [15] adopts gamma Markov chains which assumes the latent factor of the next time step is drawn from a gamma distribution with the shape parameter be the latent factor of the current time step. Schein et al. [13] proposed Poisson-gamma dynamical systems (PGDSs), which take the interactions among latent dimensions into account and use a transition matrix to capture the interactions. Deep dynamic Poisson factor analysis (DDPFA) [29] adopts recurrent neural networks (RNNs) to capture the complex long-term dependencies of latent factors. Yang and Koeppl [30] applied Poisson-gamma count model to analyze relational data arising from longitudinal networks, which can capture the evolution of individual node-group memberships over time. Many modifications of PGDS have been proposed in recent years. Guo et al. [31] proposed deep Poisson-gamma dynamical systems which aim to capture the long-range temporal dependencies. Schein et al. [16] employed Poisson-randomized gamma distribution to build a new transition process of latent factors. Chen et al. [32] proposed Switching Poisson-gamma dynamical systems (SPGDS), allowing PGDS to select from several transition matrices, and thus can better adapt to nonlinear dynamics. In contrast to SPGDS, the number of transition matrices of the proposed NS-PGDS is not limited and thus can be adopted to analyze various complicated non-stationary count sequences. Filstroff et al. [33] extensively analyzed many gamma Markov chains for non-negative matrix factorization and introduced new gamma Markov chains with well-defined stationary distribution (BGAR).

## 6 Experiments

We conducted experiments for both predictive and exploratory analysis to demonstrate the ability of the proposed model in capturing non-stationary count time sequences. The baseline models included in the experiments are: \(1)\) Gamma process dynamic Poisson factor analysis (GP-DPFA) [15]. GP-DPFA models the evolution of latent components as \(\theta^{(t)}_{k}\sim\text{Gam}(\theta^{(t-1)}_{k},c_{t})\), in which each component evolves independently of the other components. \(2)\) Gamma Markov chains on the rate parameter of gamma distribution (GMC-RATE) [33]. GMC-RATE adopts gamma Markov chains defined via the rate parameter of the gamma distribution to model the evolution of as \(\theta_{k}^{(t)}\sim\mathrm{Gam}(\alpha,\beta/\theta_{k}^{(t-1)})\). 3) Gamma Markov chains on the rate parameter with hierarchical auxiliary variable (GMC-HIER) [33]. GMC-HIER models the evolution of latent components with an auxiliary variables as \(z_{k}^{(t)}\sim\mathrm{Gam}(\alpha_{s},\beta_{s}\theta_{k}^{(t-1)})\) and \(\theta_{k}^{(t)}\sim\mathrm{Gam}(a_{\theta},\beta_{\theta}z_{k}^{(t)})\). 4) Autogressive beta-gamma process (BGAR) [34, 33]. BGAR is also a gamma Markov model. In contrast to the above models, there is a well-defined stationary distribution for BGAR. 5) Poisson-gamma dynamical system (PGDS) [13] takes interactions among latent dimensions into account, and models the evolution of \(\theta_{k}^{(t)}\) as \(\theta_{k}^{(t)}\sim\mathrm{Gam}(\tau_{0}\sum_{k_{2}=1}^{K}\pi_{kk_{2}}\theta _{k_{2}}^{(t-1)},\tau_{0})\).

The real-world datasets used in the experiments are: \(1)\)**Integrated Crisis Early Warning System (ICEWS)**: ICEWS is an international relations event dataset, comprising interaction events between countries extracted from news corpora. For ICEWS dataset, we have \(T=365\) time steps and \(V=6197\) dimensions, and we set \(M=30\). \(2)\)**NIPS**: NIPS dataset contains the papers published in the NeurIPS conference from 1987 to 2015. We have \(T=28\) time steps and \(V=2000\) dimensions for NIPS dataset and we set \(M=5\). \(3)\)**U.S. Earthquake Intensity (USEI)**: USEI contains a collection of damage and felt reports for U.S. (and a few other countries) earthquakes. We use the monthly reports from 1957-1986 and have \(T=348,V=64\) and set \(M=34\). \(4)\)**COVID-19**: This dataset contains daily death cases data for states in the United States, spanning from March 2020 to June 2020. For this dataset, we have \(V=51\) dimensions and \(T=90\) time steps and set \(M=20\).

### Predictive Analysis

To compare the predictive performance of the proposed model with the baselines, we considered two standard tasks: data smoothing and forecasting. For data smoothing task, our objective is to predict \(\mathbf{y}^{(t)}\) given the remaining data observation \(\mathbf{Y}\backslash\mathbf{y}^{(t)}\). To this end, we randomly masked 10 percents of the observed data over non-adjacent time steps, and predicted the masked values. For forecasting task, we held out data of the last \(S\) time steps, and predicted \(\mathbf{y}^{(T+1)},\cdots,\mathbf{y}^{(T+S)}\) given \(\mathbf{y}^{(1)},\cdots,\mathbf{y}^{(T)}\). In this experiment we set \(S=2\). We ran the baseline models including GP-DPFA, PGLS, GMC-RATE, GMC-HIER, BGAR, using their default settings as provided in [15, 13, 33]. For the NS-PGDS, we set \(K=100\) for ICEWS, \(K=10\) for other datasets, and set \(\tau_{0}=1,\gamma_{0}=50,\epsilon_{0}=0.1\). We performed 4000 Gibbs sampling iterations. In the experiments, we found that the Gibbs sampler started to converge after 1000 iterations, and thus we set the burn-in time be 2000 iterations. We retained every hundredth sample, and averaged the predictions over the samples. Mean relative error (MRE) and mean absolute error (MAE) are adopted to evaluate the model's predictive capability, which are defined as \(\mathrm{MRE}=\frac{1}{TV}\sum_{t}\sum_{v}\frac{|y_{v}^{(t)}-\hat{y}_{v}^{(t)}|} {1+y_{v}^{(t)}}\) and \(\mathrm{MAE}=\frac{1}{TV}\sum_{t}\sum_{v}|y_{v}^{(t)}-\hat{y}_{v}^{(t)}|\) respectively, where \(y_{v}^{(t)}\) indicates the true count and \(\hat{y}_{v}^{(t)}\) is the prediction.

As the experiment results shown in Table 1, the NS-PGDS exhibits improved performance in both data smoothing and forecasting tasks. We attribute this enhanced capability to the time-varying transition kernels, which effectively adapt to the non-stationary environment, and thus achieve improved predictive performance. For some datasets (e.g. ICEWS) and tasks, the effectiveness of the Dir-Gam-Dir and Pr-Gam-Dir constructions does not be exhibited in the numerical results. However, these two constructions indeed induce more informative patterns compared with Dir-Dir construction, as shown in the exploratory analysis.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline  & & GP-DPFA & GMC-RATE & GMC-HIER & BGAR & PGDS & 
\begin{tabular}{c} NS-PGDS \\ (Dir-Dir) \\ \end{tabular} & NS-PGDS & NS-PGDS \\  & & & & & & & & (Dir-Dir) & (Pr-Gam-Dir) \\ \hline ICEWS & MAE S & 0.259 ±0.005 & 0.258 ±0.005 & 0.256 ±0.006 & 0.264 ±0.006 & 0.215 ±0.007 & 0.215 ±0.008 & **0.214** ±0.008 & 0.215 ±0.008 \\  & F & 0.176 ±0.005 & 0.187 ±0.003 & 0.185 ±0.016 & 0.222 ±0.043 & 0.185 ±0.003 & **0.167** ±0.009 & 0.169 ±0.006 & 0.169 ±0.009 \\  & MRE S & 0.125 ±0.0

### Exploratory Analysis

We used ICEWS and NIPS datasets for exploratory analysis, and chose the NS-PGDS with Dirichlet-Dirichlet Markov chains for illustration. Figure 4(a) and Figure 4(b) demonstrate the top 2 latent factors inferred by NS-PGDS from ICEWS dataset. From Figure 4(a) we can see that the main labels are "Iraq (IRQ)-United States (USA)", "Iraq (IRQ)-United Kingdom (UK)", "Russia (RUS)-United States (USA)", and so on. This latent factor probably corresponds to the topic about Iraq war. Besides, in Figure 4(a), there is a peak around March, 2003, and we know that the Iraq war broke out exactly on 20 March, 2003. In addition, the most dominant labels shown in Figure 4(b) are "Japan (JPN)-United States (USA)", "China (CHN)-United States (USA)", "North Korea (PRK)-United States (USA)", "South Korea (KOR)-United States (USA)", and so on. We can infer that this latent factor corresponds to "Six-Party Talks" and other accidents about it.

Figure 4(c) demonstrates the evolving trends of the top 5 latent factors inferred by the NS-PGDS from NIPS dataset, and the legend indicates the representative words of the corresponding latent factors. Clearly, the green and blue lines correspond to the latent factors of neural network research which started to decline from the 1990s. From the 1990s we see that the latent factors about statistical and probabilistic methods began to dominate the NeurIPS conference. In addition, the NS-PGDS also captured the revival of neural networks (blue line) from the 2010s. The above observations from the latent structure inferred by the NS-PGDS match our prior knowledge.

Next, we explored the time-varying transition matrices inferred by the NS-PGDS. We chose NIPS dataset for illustratiuon, and set \(K=10\) and the interval length \(M\) to be 5. The time-varying transition matrices are shown from Figure 5(b) to Figure 5(f). At the beginning, matrices shown in Figure 5(b) and Figure 5(c) are close to identity matrices. Then the transition matrices tend to become block diagonal matrices with 2 blocks, as shown in Figure 5(d)-5(f). The representative words for latent factors in the first block are "state-linear-classification", "network-neural-networks", "kernel-image-space", "network-neural-networks", "neural-networks-state". The representative words for latent factors in the second topics about neural networks. The second block reflects that, from the 1990s, statistical learning and Bayesian methods began to dominate, and these topics are highly correlated. Figure 5(a) illustrates the transition matrix inferred

Figure 4: The latent factors inferred by the NS-PGDS. (a) and (b) illustrate the top 2 latent factors inferred from ICEWS dataset, (a) corresponds to Iraq war and (b) corresponds to the Six-Party Talks. (c) illustrates the evolving trends of the top 5 latent factors inferred from NIPS dataset.

Figure 5: Transition matrices inferred from NIPS dataset. (a) illustrates the transition matrix inferred by the PGDS. (b)-(f) illustrate the time-varying transition matrices inferred by the NS-PGDS.

by the PGDS, which is averaged over all time steps. Compared with the NS-PGDS, the PGDS can not capture the informative time-varying transition dynamics. We also analyzed the features of the proposed Dirichlet Markov chains. The left column of Figure 6 demonstrates transition matrices of the first four sub-intervals of ICEWS dataset inferred by the NS-PGDS (Dir-Dir). Because of the Dir-Dir construction, the consecutive transition matrices smoothly change over time and thus the NS-PGDS may lack sufficient flexibility to capture rapid dynamics. The middle column of Figure 6 illustrates the transition matrices inferred by the NS-PGDS (Dir-Gam-Dir), which takes mutations among latent components into account and captured more complicated patterns. Transition matrices inferred by the PR-Gam-Dir construction are shown in the right column of Figure 6, these matrices not only exhibited sufficient flexibility but also captured sparser patterns compared with the Dir-Gam-Dir construction.

## 7 Conclusion

The Poisson-gamma dynamical systems with time-varying transition matrices, have been proposed to capture complicated dynamics observed in _non-stationary_ count sequences. In particular, Dirichlet Markov chains are constructed to allow the underlying transition matrices to evolve over time. Although the Dirichlet Markov processes lack conjugacy, we have developed tractable-but-efficient Gibbs sampling algorithms to perform posterior simulation. The experiment results demonstrate the improved performance of the proposed NS-PGDS in data smoothing and forecasting tasks, compared with the PGDS with a stationary transition kernel. Moreover, the experimental results on several real-world data sets show the explainable structures inferred by the proposed NS-PGDS. For the future work, we plan to design a method that can find the point of change and thus the length of each sub-interval can be determined automatically instead of a constant. We also consider to generalize Dirichlet belief networks by incorporating the proposed Dirichlet Markov chain constructions, which allow the hierarchical topics to mutate across layers, and thus can generate more rich text information. And we also consider to capture non-stationary interaction dynamics among individuals over online social networks in the future research.

Figure 6: From top to bottom are the first four transition matrices inferred by different Dirichlet Markov chains from ICEWS dataset. Top row: Matrices inferred by the Dir-Dir construction. Middle row: Matrices inferred by the Dir-Gam-Dir construction. Bottom row: Matrices inferred by the PR-Gam-Dir construction.

## References

* Blei and Lafferty [2006] David M Blei and John D Lafferty. Dynamic topic models. In _Proceedings of the 23rd international conference on Machine learning_, pages 113-120, 2006.
* Wang and McCallum [2006] Xuerui Wang and Andrew McCallum. Topics over time: a non-markov continuous-time model of topical trends. In _Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 424-433, 2006.
* Jahnichen et al. [2018] Patrick Jahnichen, Florian Wenzel, Marius Kloft, and Stephan Mandt. Scalable generalized dynamic topic models. In _International Conference on Artificial Intelligence and Statistics_, pages 1427-1435, 2018.
* Sheldon and Dietterich [2011] Daniel Sheldon and Thomas G Dietterich. Collective graphical models. In _Proceedings of the 24th International Conference on Neural Information Processing Systems_, pages 1161-1169, 2011.
* Raymer et al. [2013] James Raymer, Arkadiusz Wisniowski, Jonathan J Forster, Peter WF Smith, and Jakub Bijak. Integrated modeling of european migration. _Journal of the American Statistical Association_, 108(503):801-819, 2013.
* Wilson [2017] Tom Wilson. Methods for estimating sub-state international migration: The case of australia. _Spatial Demography_, 5(3):171-192, 2017.
* Wanner [2021] Philippe Wanner. How well can we estimate immigration trends using google data? _Quality & Quantity_, 55(4):1181-1202, 2021.
* Kalman [1960] R. E. Kalman. A New Approach to Linear Filtering and Prediction Problems. _Journal of Basic Engineering_, 82(1):35-45, 1960.
* Ghahramani and Roweis [1998] Zoubin Ghahramani and Sam T Roweis. Learning nonlinear dynamical systems using an em algorithm. In _Proceedings of the 11th International Conference on Neural Information Processing Systems_, pages 431-437, 1998.
* Zhou and Carin [2012] M Zhou and L Carin. Augment-and-conquer negative binomial processes. _Advances in Neural Information Processing Systems_, 4:2546-2554, 2012.
* Zhou and Carin [2015] Mingyuan Zhou and Lawrence Carin. Negative binomial process count and mixture modeling. _IEEE Transactions on Pattern Analysis & Machine Intelligence_, 37(02):307-320, 2015.
* Schein et al. [2015] Aaron Schein, John Paisley, David M Blei, and Hanna Wallach. Bayesian poisson tensor factorization for inferring multilateral relations from sparse dyadic event counts. In _Proceedings of the 21th ACM SIGKDD International conference on knowledge discovery and data mining_, pages 1045-1054, 2015.
* Schein et al. [2016] Aaron Schein, Mingyuan Zhou, and Hanna Wallach. Poisson-gamma dynamical systems. In _Proceedings of the 30th International Conference on Neural Information Processing Systems_, pages 5012-5020, 2016.
* Schein et al. [2016] Aaron Schein, Mingyuan Zhou, David Blei, and Hanna Wallach. Bayesian poisson tucker decomposition for learning the structure of international relations. In _International Conference on Machine Learning_, pages 2810-2819, 2016.
* Acharya et al. [2015] Ayan Acharya, Joydeep Ghosh, and Mingyuan Zhou. Nonparametric bayesian factor analysis for dynamic count matrices. In _Artificial Intelligence and Statistics_, pages 1-9, 2015.
* Schein et al. [2019] Aaron Schein, Scott W Linderman, Mingyuan Zhou, David M Blei, and Hanna Wallach. Poisson-randomized gamma dynamical systems. In _Proceedings of the 33rd International Conference on Neural Information Processing Systems_, pages 782-793, 2019.
* Chang and Blei [2009] Jonathan Chang and David Blei. Relational topic models for document networks. In _Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics_, pages 81-88, 2009.

* [18] H Juliette T Unwin, Swapnil Mishra, Valerie C Bradley, Axel Gandy, Thomas A Mellan, Helen Coupland, Jonathan Ish-Horowicz, Michaela AC Vollmer, Charles Whittaker, Sarah L Filippi, et al. State-level tracking of covid-19 in the united states. _Nature Communications_, 11(1):1-9, 2020.
* [19] Rainer Winkelmann. _Econometric Analysis of Count Data_. Springer Publishing Company, Incorporated, 5th edition, 2008.
* [20] Guy Grossman, Soojong Kim, Jonah M Rexer, and Harsha Thirumurthy. Political partisanship influences behavioral responses to governors' recommendations for covid-19 prevention in the united states. _Proceedings of the National Academy of Sciences_, 117(39):24144-24153, 2020.
* [21] IHME COVID-19 Forecasting Team. Modeling covid-19 scenarios for the united states. _Nature medicine_, 27(1):94-105, 2021.
* [22] Luzhao Feng, Ting Zhang, Qing Wang, Yiran Xie, Zhibin Peng, Jiandong Zheng, Ying Qin, Muli Zhang, Shengjie Lai, Dayan Wang, et al. Impact of covid-19 outbreaks and interventions on influenza in china and the united states. _Nature communications_, 12(1):3249, 2021.
* [23] Shaobo Han, Lin Du, Esther Salazar, and Lawrence Carin. Dynamic rank factor model for text streams. In _Proceedings of the 27th International Conference on Neural Information Processing Systems-Volume 2_, pages 2663-2671, 2014.
* [24] Rahi Kalantari and Mingyuan Zhou. Graph gamma process generalized linear dynamical systems. _arXiv preprint arXiv:2007.12852_, 2020.
* [25] Lin Yuan and John D Kalbfleisch. On the bessel distribution and related problems. _Annals of the Institute of Statistical Mathematics_, 52:438-447, 2000.
* [26] Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. Hierarchical dirichlet processes. _Journal of the American Statistical Association_, 101(476):1566-1581, 2006.
* [27] Norman L Johnson, Adrienne W Kemp, and Samuel Kotz. _Univariate discrete distributions_, volume 444. John Wiley & Sons, 2005.
* [28] Mingyuan Zhou. Nonparametric bayesian negative binomial factor analysis. _Bayesian Analysis_, 13(4):1065-1093, 2018.
* [29] Chengyue Gong and Win-bin Huang. Deep dynamic poisson factorization model. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, pages 1665-1673, 2017.
* [30] Sikun Yang and Heinz Koeppl. Dependent relational gamma process models for longitudinal networks. In _International Conference on Machine Learning_, pages 5551-5560, 2018.
* [31] Dandan Guo, Bo Chen, Hao Zhang, and Mingyuan Zhou. Deep poisson gamma dynamical systems. In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_, pages 8451-8461, 2018.
* [32] Wenchao Chen, Bo Chen, Yicheng Liu, Qianru Zhao, and Mingyuan Zhou. Switching poisson gamma dynamical systems. In _Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence_, pages 2029-2036, 2021.
* [33] Louis Filstroff, Olivier Gouvert, Cedric Fevotte, and Olivier Cappe. A comparative study of gamma markov chains for temporal non-negative matrix factorization. _IEEE Transactions on Signal Processing_, 69:1614-1626, 2021.
* [34] Peter AW Lewis, Edward McKenzie, and David Kennedy Hugus. Gamma processes. _Stochastic Models_, 5(1):1-30, 1989.
* [35] John Frank Charles Kingman. _Poisson processes_, volume 3. Clarendon Press, 1992.

MCMC Inference

**Notation.** When expressing the full conditionals for Gibbs sampling, we use the shorthand "-" to denote all other variables. We use "-" as an index summation shorthand, e.g., \(x_{.j}=\sum_{i}x_{ij}\).

In this section, we present a fully-conjugate and efficient Gibbs sampler for the proposed NS-PGDS. The sampling algorithms depend on several key technical results, which we will repeatedly exploit, thus we list them below.

**Negative-binomial Distribution**. Let \(y\sim\operatorname{Pois}\left(c\lambda\right)\), and \(\lambda\sim\operatorname{Gam}(a,b)\). If we marginalize over \(\lambda\), then \(y\sim\operatorname{NB}\left(a,\frac{c}{b+c}\right)\) is a negative-binomial distributed random variable. We can further parameterize it as \(y\sim\operatorname{NB}\left(a,g\left(\zeta\right)\right)\), where \(g\left(z\right)=1-\exp\left(-z\right)\) and \(\zeta=\ln\left(1+\frac{c}{6}\right)\).

**Lemma 1**.: If \(y\sim\operatorname{NB}\left(a,g\left(\zeta\right)\right)\) and \(l\sim\operatorname{CRT}\left(y,a\right)\), where \(\operatorname{CRT}\left(\cdot\right)\) represents Chinese restaurant table distribution [26], then the joint distribution of \(y\) and \(l\) can be equivalently distributed as \(y\sim\operatorname{SumLog}\left(l,g\left(\zeta\right)\right)\) and \(l\sim\operatorname{Pois}\left(a\zeta\right)\)[11], i.e.

\[\operatorname{NB}\left(y;a,g\left(\zeta\right)\right)\operatorname{CRT}\left( l;y,a\right)=\operatorname{SumLog}\left(y;l,g\left(\zeta\right)\right) \operatorname{Pois}\left(l;a\zeta\right),\]

where \(\operatorname{SumLog}\left(l,g\left(\zeta\right)\right)=\sum_{i=1}^{l}x_{i}\) and \(x_{i}\sim\operatorname{Log}\left(g\left(\zeta\right)\right)\) are independently and identically logarithmic distributed random variables [27].

**Lemma 2**.: Suppose \(\mathbf{n}=(n_{1},\cdots,n_{K})\) and \(\mathbf{n}\mid n\sim\operatorname{DirMult}\left(n,r_{1},\cdots,r_{K}\right),\) where \(\operatorname{DirMult}\left(\cdot\right)\) refers to Dirichlet-multinomial distribution. We sample the augmented variable \(q\mid n\sim\operatorname{Beta}\left(n,r_{.}\right)\), where \(r_{.}=\sum_{k=1}^{K}r_{k}\). According to [28], conditioning on \(q\), we have \(n_{k}\sim\operatorname{NB}\left(r_{k},q\right)\).

**Lemma 3**.: If \(y_{.}=\sum_{s=1}^{S}y_{s}\), and \(y_{s}\overset{\text{i.i.d.}}{\sim}\operatorname{Pois}(\lambda_{s}),s=1,\cdots,S\). Then \(y_{.}\sim\operatorname{Pois}(\sum_{s=1}^{S}\lambda_{s})\) and \((y_{1},\cdots,y_{S})\sim\operatorname{Mult}(y_{.},(\frac{\lambda_{1}}{\sum_{s =1}^{1}\lambda_{s}},\cdots,\frac{\lambda_{S}}{\sum_{s=1}^{1}\lambda_{s}}))\), where \(\operatorname{Mult}\left(\cdot\right)\) represents multinomial distribution [35].

**Sampling \(y_{vk}^{(t)}\):** Use the relationship between Poisson and multinomial distributions as described by Lemma 3, given observed counts and latent parameters, we sample

\[\left(\left(y_{vk}^{(t)}\right)_{k=1}^{K}\mid-\right)\sim\operatorname{Mult} \left(y_{v}^{(t)},\left(\frac{\phi_{vk}\theta_{k}^{(t)}}{\sum_{k=1}^{K}\phi_{ vk}\theta_{k}^{(t)}}\right)_{k=1}^{K}\right). \tag{6}\]

Then the distribution of \(y_{vk}^{(t)}\) is \(y_{vk}^{(t)}\sim\operatorname{Pois}(\delta^{(t)}\phi_{vk}\theta_{k}^{(t)})\).

**Sampling \(\phi_{\mathbf{k}}\):** Via Dirichlet-multinomial conjugacy, the posterior of \(\phi_{\mathbf{k}}\) is

\[\left(\phi_{\mathbf{k}}\mid-\right)\sim\operatorname{Dir}\left(\epsilon_{0}+\sum_ {t=1}^{T}y_{1k}^{(t)},\cdots,\epsilon_{0}+\sum_{t=1}^{T}y_{Vk}^{(t)}\right). \tag{7}\]

**Marginalizing over \(\theta_{\mathbf{k}}^{(t)}\):** Note that \(y_{v}^{(t)}=y_{v\cdot}^{(t)}=\sum_{k=1}^{K}y_{vk}^{(t)}\) and \(y_{vk}^{(t)}\sim\operatorname{Pois}(\delta^{(t)}\phi_{vk}\theta_{k}^{(t)})\). Then we define \(y_{.k}^{(t)}=\sum_{v=1}^{V}y_{vk}^{(t)}\). Because \(\sum_{v=1}^{V}\phi_{vk}=1\), we obtain \(y_{.k}^{(t)}\sim\operatorname{Pois}(\delta^{(t)}\theta_{k}^{(t)})\).

We start by marginalizing over \(\theta_{k}^{(T)}\), using the definition of negative-binomial distribution, we obtain

\[y_{.k}^{(T)}\sim\operatorname{NB}\left(\tau_{0}\sum_{k_{2}=1}^{K}\pi_{kk_{2}}^ {i(T-1)}\theta_{k_{2}}^{(T-1)},g\left(\zeta^{(T)}\right)\right),\]

where \(\zeta^{(T)}=\ln(1+\frac{\delta^{(T)}}{\tau_{0}})\). Next, we further marginalize over \(\theta_{k}^{(T-1)}\). To this end, we first sample auxiliary variables

\[l_{k}^{(T)}\sim\operatorname{CRT}\left(y_{.k}^{(T)},\tau_{0}\sum_{k_{2}=1}^{K} \pi_{kk_{2}}^{i(T-1)}\theta_{k_{2}}^{(T-1)}\right).\]

By Lemma 1, the joint distribution of \(y_{.k}^{(T)}\) and \(l_{k}^{(T)}\) can be expressed as

\[y_{.k}^{(T)}\sim\operatorname{SumLog}\left(l_{k}^{(T)},g\left(\zeta^{(T)} \right)\right)\text{ and }l_{k}^{(T)}\sim\operatorname{Pois}\left(\zeta^{(T)}\tau_{0}\sum_{k_{2}=1}^ {K}\pi_{kk_{2}}^{i(T-1)}\theta_{k_{2}}^{(T-1)}\right).\]* [441] Via Lemma 3, we re-express the auxiliary variables as \[l_{k}^{(T)}=l_{k\cdot}^{(T)}=\sum_{k_{2}=1}^{K}l_{kk_{2}}^{(T)},\text{ and obtain }l_{kk_{2}}^{(T)}\sim\text{Pois}\left(\zeta^{(T)}\tau_{0}\pi_{kk_{2}}^{i(T-1)} \theta_{k_{2}}^{(T-1)}\right).\] Then we define \(l_{\cdot k}^{(T)}=\sum_{k_{1}=1}^{K}l_{k_{1}k}^{(T)}.\) Leveraging Lemma 3 and \(\sum_{k_{1}=1}^{K}\pi_{k_{1}k}^{i(T-1)}=1,\) we obtain \[l_{\cdot k}^{(T)}\sim\text{Pois}\left(\zeta^{(T)}\tau_{0}\theta_{k}^{(T-1)} \right)\text{ and }\left(l_{1k}^{(T)},\cdots,l_{Kk}^{(T)}\right)\sim\text{Mult} \left(l_{\cdot k}^{(T)},\left(\pi_{1k}^{i(T-1)},\cdots,\pi_{Kk}^{i(T-1)} \right)\right).\] Next, note that \(y_{\cdot k}^{(T-1)}\sim\text{Pois}(\delta^{(T-1)}\theta_{k}^{(T-1)}),\) if we introduce \(m_{k}^{(T-1)}=y_{\cdot k}^{(T-1)}+l_{\cdot k}^{(T)},\) then we have \[m_{k}^{(T-1)}\sim\text{Pois}\left(\theta_{k}^{(T-1)}\left(\delta^{(T-1)}+ \zeta^{(T)}\tau_{0}\right)\right).\] Because the prior of \(\theta_{k}^{(T-1)}\) is gamma distributed, by the definition of negative-binomial distribution, we can again marginalize over \(\theta_{k}^{(T-1)}\) to obtain \[m_{k}^{(T-1)}\sim\text{NB}\left(\tau_{0}\sum_{k_{2}=1}^{K}\pi_{kk_{2}}^{i(T-2) }\theta_{k_{2}}^{(T-2)},g\left(\zeta^{(T-1)}\right)\right),\] where \(\zeta^{(T-1)}=\ln(1+\frac{\delta^{(T-1)}}{\tau_{0}}+\zeta^{(T)}).\) Then we introduce auxiliary variables \[l_{k}^{(T-1)}\sim\text{CRT}\left(m_{k}^{(T-1)},\tau_{0}\sum_{k_{2}=1}^{K}\pi_{ kk_{2}}^{i(T-2)}\theta_{k_{2}}^{(T-2)}\right).\] And similar to the case for \(t=T\), we can obtain \[l_{\cdot k}^{(T-1)}\sim\text{Pois}\left(\zeta^{(T-1)}\tau_{0}\theta_{k}^{(T-2) }\right)\text{ and }m_{k}^{(T-2)}\sim\text{NB}\left(\tau_{0}\sum_{k_{2}=1}^{K}\pi_{kk_{2}}^{i(T- 3)}\theta_{k_{2}}^{(T-3)},g\left(\zeta^{(T-2)}\right)\right).\] Thus we have marginalized over \(\theta_{k}^{(T-2)}\). Note that we can repeat this marginalization process recursively until \(t=1\) with \(\zeta^{(t)}=\ln(1+\frac{\delta^{(t)}}{\tau_{0}}+\zeta^{(t+1)})\) and \(m_{k}^{(T)}=y_{\cdot k}^{(T)}\) to maginalize over all the \(\theta_{k}^{(t)}\).

**Sampling \(\theta_{k}^{(t)}\) :** Via the above marginalization process, to sample from the posterior of \(\theta_{k}^{(t)}\), we first sample the auxiliary variables. Setting \(l_{\cdot k}^{(T+1)}=0\) and \(\zeta^{(T+1)}=0\), we sample the augmented variables backwards from \(t=T,\cdots,2\),

\[\left(l_{k\cdot}^{(t)}\mid-\right)\sim\text{CRT}\left(y_{\cdot k}^{(t)}+l_{ \cdot k}^{(t+1)},\tau_{0}\sum_{k_{2}=1}^{K}\pi_{kk_{2}}^{i(t-1)}\theta_{k_{2}} ^{(t-1)}\right), \tag{8}\]

\[\left(l_{k1}^{(t)},\cdots,l_{kK}^{(t)}\mid-\right)\sim\text{Mult}\left(l_{k \cdot}^{(t)},\left(\frac{\pi_{k1}^{i(t-1)}\theta_{k}^{(t-1)}}{\sum_{k_{2}=1}^{ K}\pi_{kk_{2}}^{i(t-1)}\theta_{k_{2}}^{(t-1)}},\cdots,\frac{\pi_{kkK}^{i(t-1)} \theta_{K}^{(t-1)}}{\sum_{k_{2}=1}^{K}\pi_{kk_{2}}^{i(t-1)}\theta_{k_{2}}^{(t-1 )}}\right)\right). \tag{9}\]

And via Lemma 3, we obtain

\[\left(l_{1k}^{(t)},\cdots,l_{Kk}^{(t)}\right)\sim\text{Mult}\left(l_{\cdot k}^{ (t)},\pi_{1k}^{i(t-1)},\cdots,\pi_{Kk}^{i(t-1)}\right) \tag{10}\]

We compute \(\zeta^{(t)}\) recursively via

\[\zeta^{(t)}=\ln\left(1+\frac{\delta^{(t)}}{\tau_{0}}+\zeta^{(t+1)}\right). \tag{11}\]

After sampling the auxiliary variables, then for \(t=1,\cdots,T\), by Poisson-gamma conjugacy, we obtain

\[\left(\theta_{k}^{(1)}\mid-\right)\sim\text{Gam}\left(y_{\cdot k}^{(1)}+l_{ \cdot k}^{(2)}+\tau_{0}\nu_{k},\tau_{0}+\delta^{(1)}+\zeta^{(2)}\tau_{0}\right), \tag{12}\]

\[\left(\theta_{k}^{(t)}\mid-\right)\sim\text{Gam}\left(y_{\cdot k}^{(t)}+l_{ \cdot k}^{(t+1)}+\tau_{0}\sum_{k_{2}=1}^{K}\pi_{kk_{2}}^{i(t-1)}\theta_{k_{2}}^{(t -1)},\tau_{0}+\delta^{(t)}+\zeta^{(t+1)}\tau_{0}\right). \tag{13}\]

**Sampling \(\Pi^{(i)}\) :** We define \(M\) as the length of each sub-interval, and \(I\) as the number of intervals. For \(i=I\), by Eq.(10), \((l_{1k}^{(I)},\cdots,l_{Kk}^{(I)})\) is multinomial distributed. Thus by multinomial-Dirichlet conjugacy, we obtain

\[\left(\mathbf{\pi}_{k}^{(I)}\mid-\right)\sim\mathrm{Dir}\left(\alpha_{1k}^{(I)}+l_{ 1k}^{(I)},\cdots,\alpha_{Kk}^{(I)}+l_{Kk}^{(I)}\right), \tag{14}\]

where \(l_{k_{1}k}^{(I)}\) indicates the summation of \(l_{k_{1}k}^{(t)}\) over \(I\)-th sub-interval, i.e. \(l_{k_{1}k}^{(I)}=\sum_{t=(I-1)M+1}^{T}l_{k_{1}k}^{(t)}\).

**Inference for Dirichlet-Dirichlet Markov chains.** For Dirichlet-Dirichlet Markov chains, \(\alpha_{k_{1}k}^{(i)}=\eta K\pi_{k_{1}k}^{(i-1)}\). By Eq.(10), \((l_{1k}^{(i)},\cdots,l_{Kk}^{(i)})\) is multinomial distributed. If we marginalize \((\pi_{1k}^{(i)},\cdots,\pi_{Kk}^{(i)})\), \((l_{1k}^{(i)},\cdots,l_{Kk}^{(i)})\) will be Dirichlet-multinomial distributed. Thus by Lemma 2, for \(i=I\), we first sample the auxiliary variables as

\[\left(q_{k}^{(I)}\mid-\right)\sim\mathrm{Beta}\left(l_{\cdot k}^{(I)},\eta K \right)\ \mathrm{and}\ \left(h_{k_{1}k}^{(I)}\mid-\right)\sim\mathrm{CRT} \left(l_{k_{1}k}^{(I)},\eta K\pi_{k_{1}k}^{(I-1)}\right). \tag{15}\]

Similarly, by Eq.(18), \((h_{1k}^{(i)},\cdots,h_{Kk}^{(i)})\) is also Dirichlet-multinomial distributed. Thus for \(i=I-1,\cdots,2\), we sample the auxiliary variables as

\[\left(q_{k}^{(i)}\mid-\right)\sim\mathrm{Beta}\left(l_{\cdot k}^{(i)}+h_{\cdot k }^{(i+1)},\eta K\right)\ \mathrm{and}\ \left(h_{k_{1}k}^{(i)}\mid-\right)\sim \mathrm{CRT}\left(l_{k_{1}k}^{(i)}+h_{k_{1}k}^{(i+1)},\eta K\pi_{k_{1}k}^{(i-1 )}\right), \tag{16}\]

where \(l_{k_{1}k}^{(i)}=\sum_{(i-1)M+1}^{iM}l_{k_{1}k}^{(t)}\) refers to the summation of \(l_{k_{1}k}^{(t)}\) over \(i\)-th interval. Via Lemma 2, conditioning on \(q_{k}^{(i)}\), we have

\[\left(l_{k_{1}k}^{(i)}+h_{k_{1}k}^{(i+1)}\right)\sim\mathrm{NB}\left(\eta K\pi _{k_{1}k}^{(i-1)},q_{k}^{(i)}\right).\]

Then via Lemma 1, we obtain

\[h_{k_{1}k}^{(i)}\sim\mathrm{Pois}\left(-\eta K\pi_{k_{1}k}^{(i-1)}\mathrm{ln} \left(1-q_{k}^{(i)}\right)\right). \tag{17}\]

Note that by Eq.(17), \(h_{k_{1}k}^{(i)}\) is Poisson distributed and by Lemma 3, we obtain

\[\left(h_{1k}^{(i)},\cdots,h_{Kk}^{(i)}\right)\sim\mathrm{Mult}\left(h_{\cdot k }^{(i)},\left(\pi_{1k}^{(i-1)},\cdots,\pi_{Kk}^{(i-1)}\right)\right). \tag{18}\]

In addition, note that

\[\left(l_{1k}^{(i-1)},\cdots,l_{Kk}^{(i-1)}\right)\sim\mathrm{Mult}\left(l_{ \cdot k}^{(i-1)},\left(\pi_{1k}^{(i-1)},\cdots,\pi_{Kk}^{(i-1)}\right)\right),\]

via Dirichlet-multinomial conjugacy, for \(i=I-1,\cdots,2\), we obtain

\[\left(\mathbf{\pi}_{k}^{(i)}\mid-\right)\sim\mathrm{Dir}\left(\eta K\pi_{1k}^{(i-1 )}+l_{1k}^{(i)}+h_{1k}^{(i+1)},\cdots,\eta K\pi_{Kk}^{(i-1)}+l_{Kk}^{(i)}+h_{Kk }^{(i+1)}\right). \tag{19}\]

Specifically, for \(i=1\), we have

\[\left(\mathbf{\pi}_{k}^{(1)}\mid-\right)\sim\mathrm{Dir}\left(\nu_{1}\nu_{k}+l_{1k }^{(1)}+h_{1k}^{(2)},\cdots,\xi\nu_{k}+l_{kk}^{(1)}+h_{kk}^{(2)},\cdots,\nu_{ K}\nu_{k}+l_{Kk}^{(1)}+h_{Kk}^{(2)}\right). \tag{20}\]

For sampling \(\eta\), note that \((h_{k_{1}k}^{(i)}\mid-)\sim\mathrm{Pois}(-\eta K\pi_{k_{1}k}^{(i-1)}\mathrm{ln} \left(1-q_{k}^{(i)}\right))\), \(i=I,\cdots,2\). Given the prior \(\eta\sim\mathrm{Gam}\left(e_{0},f_{0}\right)\), via Poisson-gamma conjugacy, we obtain

\[\left(\eta\mid-\right)\sim\mathrm{Gam}\left(e_{0}+\sum_{i=2}^{I}\sum_{k_{1}=1 }^{K}\sum_{k_{2}=1}^{K}h_{k_{1}k_{2}}^{(i)},f_{0}-K\sum_{i=2}^{I}\sum_{k=1}^{ K}\mathrm{ln}\left(1-q_{k}^{(i)}\right)\right). \tag{21}\]

**Inference for Dirichlet-Gamma-Dirichlet Markov chains.** For Dirichlet-Gamma-Dirichlet Markov chains

\[\alpha_{k_{1}k}^{(i)}\sim\mathrm{Gam}\left(\gamma_{k}^{(i-1)}\sum_{k_{2}=1}^{K} \psi_{kk_{1}k_{2}}^{(i-1)}\pi_{k_{2}k}^{(i-1)},c_{k}^{(i)}\right).\]By Eq.(10), \((l_{1k}^{(i)},\cdots,l_{KK}^{(i)})\) is multinomial distributed. If we marginalize \((\pi_{1k}^{(i)},\cdots,\pi_{KK}^{(i)})\), \((l_{1k}^{(i)},\cdots,l_{KKk}^{(i)})\) will be Dirichlet-multinomial distributed. Thus by Lemma 2, for \(i=I\), we first sample the auxiliary variables as

\[\left(q_{k}^{(I)}\mid-\right)\sim\mathrm{Beta}\left(l_{\cdot k}^{(I)},\alpha_{k }^{(I)}\right)\;\mathrm{and}\;\left(h_{k_{1}k}^{(I)}\mid-\right)\sim\mathrm{CRT }\left(l_{k_{1}k}^{(I)},\alpha_{k_{1}k}^{(I)}\right). \tag{22}\]

Similarly, by Eq.(27), \((g_{1:k}^{(i)},\cdots,g_{.Kk}^{(i)})\) is also Dirichlet-multinomial distributed. Thus for \(i=I-1,\cdots,2\), we sample the auxiliary variables as

\[\left(q_{k}^{(i)}\mid-\right)\sim\mathrm{Beta}\left(l_{\cdot k}^{(i)}+g_{.k}^ {(i+1)},\alpha_{k}^{(i)}\right)\;\mathrm{and}\;\left(h_{k_{1}k}^{(i)}\mid- \right)\sim\mathrm{CRT}\left(l_{k_{1}k}^{(i)}+g_{\cdot k_{1}k}^{(i+1)},\alpha_ {k_{1}k}^{(i)}\right). \tag{23}\]

Via Lemma 2, conditioning on \(q_{k}^{(i)}\), we have

\[\left(l_{k_{1}k}^{(i)}+g_{\cdot k_{1}k}^{(i+1)}\right)\sim\mathrm{NB}\left( \alpha_{k_{1}k}^{(i)},q_{k}^{(i)}\right).\]

Then via Lemma 1, we obtain

\[h_{k_{1}k}^{(i)}\sim\mathrm{Pois}\left(-\alpha_{k_{1}k}^{(i)}\mathrm{ln}\left( 1-q_{k}^{(i)}\right)\right).\]

Thus via Poisson-gamma conjugacy, we obtain

\[\left(\alpha_{k_{1}k}^{(i)}\mid-\right)\sim\mathrm{Gam}\left(\gamma_{k}^{(i-1 )}\sum_{k2=1}^{K}\psi_{kk_{1}k_{2}}^{(i-1)}\pi_{k_{2}k}^{(i-1)}+h_{k_{1}k}^{(i )},c_{k}^{(i)}-\mathrm{ln}\left(1-q_{k}^{(i)}\right)\right). \tag{24}\]

Marginalizing over \(\alpha_{k_{1}k}^{(i)}\), and via the definition of negative-binomial distribution, we have

\[h_{k_{1}k}^{(i)}\sim\mathrm{NB}\left(\gamma_{k}^{(i-1)}\sum_{k2=1}^{K}\psi_{ kk_{1}k_{2}}^{(i-1)}\pi_{k_{2}k}^{(i-1)},\frac{-\mathrm{ln}\left(1-q_{k}^{(i)} \right)}{c_{k}^{(i)}-\mathrm{ln}\left(1-q_{k}^{(i)}\right)}\right).\]

Then using Lemma 1, we sample

\[\left(g_{k_{1}k}^{(i)}\mid-\right)\sim\mathrm{CRT}\left(h_{k_{1}k}^{(i)},\gamma _{k}^{(i-1)}\sum_{k2=1}^{K}\psi_{kk_{1}k_{2}}^{(i-1)}\pi_{k_{2}k}^{(i-1)} \right), \tag{25}\]

and obtain

\[g_{k_{1}k}^{(i)}\sim\mathrm{Pois}\left(\gamma_{k}^{(i-1)}\sum_{k2=1}^{K}\psi_ {kk_{1}k_{2}}^{(i-1)}\pi_{k_{2}k}^{(i-1)}\mathrm{ln}\left(1-\mathrm{ln}\left( 1-q_{k}^{(i)}\right)/c_{k}^{(i)}\right)\right).\]

If we define \(g_{k_{1}k}^{(i)}=g_{k_{1}\cdot k}^{(i)}=\sum_{k2=1}^{K}g_{k_{1}k_{2}k}^{(i)}\), and augment

\[\left(g_{k_{1}1k}^{(i)},\cdots,g_{k_{1}Kk}^{(i)}\right)\sim\mathrm{Mult}\left( g_{k_{1}k}^{(i)},\left(\psi_{kk_{1}k_{2}}^{(i-1)}\pi_{k_{2}k}^{(i-1)}\right)_{k_{2} =1}^{K}\right). \tag{26}\]

By Lemma 3, we have

\[g_{k_{1}k_{2}k}^{(i)}\sim\mathrm{Pois}\left(\gamma^{(i-1)}\psi_{kk_{1}k_{2}}^{( i-1)}\pi_{k_{2}k}^{(i-1)}\mathrm{ln}\left(1-\mathrm{ln}\left(1-q_{k}^{(i)} \right)/c_{k}^{(i)}\right)\right).\]

Using Lemma 3 and \(\sum_{k_{1}}^{K}\psi_{kk_{1}k_{2}}^{(i-1)}=1\), we have,

\[\left(g_{\cdot 1k}^{(i)},\cdots,g_{\cdot Kk}^{(i)}\right)\sim\mathrm{Mult}\left( g_{\cdot k}^{(i)},\left(\pi_{k_{1}k}^{(i-1)}\right)_{k_{1}=1}^{K}\right), \tag{27}\]

\[\left(g_{1k_{2}k}^{(i)},\cdots,g_{Kk_{2}k}^{(i)}\right)\sim\mathrm{Mult}\left( g_{\cdot k_{2}k}^{(i)},\left(\psi_{kk_{1}k_{2}}^{(i-1)}\right)_{k_{1}=1}^{K} \right).\]

Thus by Dirichlet-multinomial conjugacy, for \(i=I,\cdots,2\), we can obtain

\[\left(\left(\psi_{k1k_{2}}^{(i-1)},\cdots,\psi_{kk_{2}}^{(i-1)} \right)\mid-\right) \sim\mathrm{Dir}\left(\epsilon_{0}+g_{1k_{2}k}^{(i)},\cdots, \epsilon_{0}+g_{Kk_{2}k}^{(i)}\right), \tag{28}\] \[\left(\pi_{k}^{(i-1)}\mid-\right) \sim\mathrm{Dir}\left(\alpha_{1k}^{(i-1)}+l_{1k}^{(i-1)}+g_{\cdot 1k}^{(i)}, \cdots,\alpha_{KK}^{(i-1)}+l_{Kk}^{(i-1)}+g_{\cdot Kk}^{(i)}\right). \tag{29}\]For sampling \(\gamma_{k}^{(i-1)}\), note that by Eq.(26) and \(\sum_{k_{1}}^{K}\psi_{kk_{1}k_{2}}^{(i-1)}=1\), we have

\[g_{.k}^{(i)}=\sum_{k_{1}=1}^{K}g_{k_{1}k}^{(i)}\;\mathrm{and}\;g_{.k}^{(i)}\sim \mathrm{Pois}\left(\gamma_{k}^{(i-1)}\mathrm{ln}\left(1-\mathrm{ln}\left(1-q_{k }^{(i)}\right)/c_{k}^{(i)}\right)\right). \tag{30}\]

Thus via Poisson-gamma conjugacy, we obtain

\[\left(\gamma_{k}^{(i-1)}\mid-\right)\sim\mathrm{Gam}\left(\epsilon_{0}+g_{.k}^ {(i)},\epsilon_{0}+\mathrm{ln}\left(1-\mathrm{ln}\left(1-q_{k}^{(i)}\right) \right)\right). \tag{31}\]

By gamma-gamma conjugacy, we have

\[\left(c_{k}^{(i)}\mid-\right)\sim\mathrm{Gam}\left(\epsilon_{0}+\gamma_{k}^{( i-1)},\epsilon_{0}+\sum_{k_{1}=1}^{K}\alpha_{k_{1}k}^{(i)}\right). \tag{32}\]

**Inference for Dirichlet-Randomized-Gamma-Dirichlet Markov chains.** For Dirichlet-Randomized-Gamma-Dirichlet Markov chains,

\[\alpha_{k_{1}k}^{(i)}\sim\mathrm{RG1}\left(\epsilon^{\alpha},\gamma^{(i-1)} \sum_{k2=1}^{K}\psi_{kk_{1}k_{2}}^{(i-1)}\pi_{k_{2}k}^{(i-1)},c_{k}^{(i)} \right),\]

which can be equivalently represented as

\[\alpha_{k_{1}k}^{(i)}\sim\mathrm{Gam}\left(g_{k_{1}k}^{(i)}+\epsilon^{\alpha}, c_{k}^{(i)}\right),\;\mathrm{and}\;g_{k_{1}k}^{(i)}=\mathrm{Pois}\left( \gamma^{(i-1)}\sum_{k2=1}^{K}\psi_{kk_{1}k_{2}}^{(i-1)}\pi_{k_{2}k}^{(i-1)} \right).\]

By Eq.(10), \((l_{1k}^{(i)},\cdots,l_{Kk}^{(i)})\) is multinomial distributed. If we marginalize \((\pi_{1k}^{(i)},\cdots,\pi_{Kk}^{(i)})\), \((l_{1k}^{(i)},\cdots,l_{Kk}^{(i)})\) will be Dirichlet-multinomial distributed. Thus by Lemma 2, for \(i=I\), we first sample the auxiliary variables as

\[\left(q_{k}^{(I)}\mid-\right)\sim\mathrm{Beta}\left(l_{.k}^{(I)},\alpha_{.k}^{ (I)}\right)\;\mathrm{and}\;\left(h_{k_{1}k}^{(I)}\mid-\right)\sim\mathrm{CRT} \left(l_{k_{1}k}^{(I)},\alpha_{k_{1}k}^{(I)}\right). \tag{33}\]

Similarly, by Eq.(39), \((g_{1k}^{(i)},\cdots,g_{.Kk}^{(i)})\) is also Dirichlet-multinomial distributed. Thus for \(i=I-1,\cdots,2\), we sample the auxiliary variables as

\[\left(q_{k}^{(i)}\mid-\right)\sim\mathrm{Beta}\left(l_{.k}^{(i)}+g_{.k}^{(i+1)},\alpha_{.k}^{(i)}\right)\;\mathrm{and}\;\left(h_{k_{1}k}^{(i)}\mid-\right) \sim\mathrm{CRT}\left(l_{k_{1}k}^{(i)}++g_{.k_{1}k_{1}}^{(i+1)},\alpha_{k_{1}k }^{(i)}\right). \tag{34}\]

Via Lemma 2, conditioning on \(q_{k}^{(i)}\), we have

\[\left(l_{k_{1}k}^{(i)}+g_{.k_{1}k}^{(i+1)}\right)\sim\mathrm{NB}\left(\alpha_{ k_{1}k}^{(i)},q_{k}^{(i)}\right).\]

Then via Lemma 1, we obtain

\[h_{k_{1}k}^{(i)}\sim\mathrm{Pois}\left(-\alpha_{k_{1}k}^{(i)}\mathrm{ln}\left( 1-q_{k}^{(i)}\right)\right).\]

Via Poisson-gamma conjugacy, we first sample

\[\left(\alpha_{k_{1}k}^{(i)}\mid-\right)\sim\mathrm{Gam}\left(g_{k_{1}k}^{(i)} +\epsilon^{\alpha}+h_{k_{1}k}^{(i)},c_{k}^{(i)}-\mathrm{ln}\left(1-q_{k}^{(i) }\right)\right). \tag{35}\]

If \(\epsilon^{\alpha}>0\), we can sample the posterior of \(g_{k_{1}k}^{(i)}\) via

\[\left(g_{k_{1}k}^{(i)}\mid-\right)\sim\mathrm{Bessel}\left(\epsilon^{\alpha}-1,2\sqrt{\alpha_{k_{1}k}^{(i)}c_{k}^{(i)}\gamma_{k}^{(i-1)}\sum_{k2=1}^{K}\psi_{ kk_{1}k_{2}}^{(i-1)}\pi_{k_{2}k}^{(i-1)}}\right), \tag{36}\]

where \(\mathrm{Bessel}\left(\cdot\right)\) denotes Bessel distribution. If \(\epsilon^{\alpha}=0\), we sample \(g_{k_{1}k}^{(i)}\) via

\[\left(g_{k_{1}k}^{(i)}\mid-\right)\sim\left\{\begin{array}{cc}\mathrm{Pois} \left(\frac{c_{k_{1}}^{(i)}\gamma_{k}^{(i-1)}\sum_{k2=1}^{K}\psi_{kk_{1}k_{2}}^ {(i-1)}\pi_{k_{2}k}^{(i-1)}}{c_{k}^{(i)}-\mathrm{ln}\left(1-q_{k}^{(i)}\right) }\right)&\mathrm{if}\;h_{k_{1}k}^{(i)}=0\\ \mathrm{SCH}\left(h_{k_{1}k}^{(i)},\frac{c_{k_{1}}^{(i)}\gamma_{k}^{(i-1)}\sum _{k2=1}^{K}\psi_{kk_{1}k_{2}}^{(i-1)}\pi_{k_{2}k}^{(i-1)}}{c_{k}^{(i)}-\mathrm{ ln}\left(1-q_{k}^{(i)}\right)}\right)&\mathrm{otherwise},\end{array}\right. \tag{37}\]where \(\mathrm{SCH}\left(\cdot\right)\) denotes the shifted confluent hypergeometric distribution [16].

Defining \(g_{k_{1}k}^{(i)}=g_{k_{1}\cdot k}^{(i)}=\sum_{k=21}^{K}g_{k_{1}k_{2}k}^{(i)}\), we first augment

\[\left(g_{k_{1}1k}^{(i)},\cdots,g_{k_{1}Kk}^{(i)}\right)\sim\mathrm{Mult}\left(g _{k_{1}k}^{(i)},\left(\psi_{kk_{1}k_{2}}^{(i-1)}\pi_{k_{2}k}^{(i-1)}\right)_{k _{2}=1}^{K}\right). \tag{38}\]

By Lemma 3, we have

\[g_{k_{1}k_{2}k}^{(i)}\sim\mathrm{Pois}\left(\gamma^{(i-1)}\psi_{kk_{1}k_{2}}^{(i -1)}\pi_{k_{2}k}^{(i-1)}\right),\]

and because \(\sum_{k_{1}}^{K}\psi_{kk_{1}k_{2}}^{(i-1)}=1\), we have

\[\left(g_{\cdot 1k}^{(i)},\cdots,g_{\cdot K}^{(i)}\right) \sim\mathrm{Mult}\left(g_{\cdot k}^{(i)},\left(\pi_{k_{1}k}^{(i-1 )}\right)_{k_{1}=1}^{K}\right),\text{ and } \tag{39}\] \[\left(g_{1k_{2}k}^{(i)},\cdots,g_{Kk_{2}k}^{(i)}\right) \sim\mathrm{Mult}\left(g_{\cdot k_{2}k}^{(i)},\left(\psi_{kk_{1}k_ {2}}^{(i-1)}\right)_{k1=1}^{K}\right).\]

Thus by Dirichlet-multinomial conjugacy, for \(i=I,\cdots,2\), we have

\[\left(\left(\psi_{k1k_{2}}^{(i-1)},\cdots,\psi_{kKk_{2}}^{(i-1)}\right)\mid- \right)\sim\mathrm{Dir}\left(\epsilon_{0}+g_{1k_{2}k}^{(i)},\cdots,\epsilon_{ 0}+g_{Kk_{2}k}^{(i)}\right), \tag{40}\]

\[\left(\mathbf{\pi}_{k}^{(i-1)}\mid-\right)\sim\mathrm{Dir}\left(\alpha_{1k}^{(i-1 )}+l_{1k}^{(i-1)}+g_{\cdot 1k}^{(i)},\cdots,\alpha_{Kk}^{(i-1)}+l_{Kk}^{(i-1)}+g_{ \cdot Kk}^{(i)}\right). \tag{41}\]

Via Poisson-gamma conjugacy, we obtain

\[\left(\gamma_{k}^{(i-1)}\mid-\right)\sim\mathrm{Gam}\left(\epsilon_{0}+g_{ \cdot k}^{(i)},\epsilon_{0}+1\right). \tag{42}\]

By gamma-gamma conjugacy, we have

\[\left(c_{k}^{(i)}\mid-\right)\sim\mathrm{Gam}\left(\epsilon_{0}+\gamma_{k}^{( i-1)},\epsilon_{0}+\sum_{k_{1}=1}^{K}\alpha_{k_{1}k}^{(i)}\right). \tag{43}\]

Specifically, for \(i=1\), we have \(\alpha_{k_{1}k}^{(1)}=\nu_{k_{1}}\nu_{k}\), if \(k_{1}\neq k\). And \(\alpha_{k_{1}k}^{(1)}=\xi\nu_{k}\), if \(k_{1}=k\).

**Sampling \(\nu_{k}\) and \(\xi\) :** As we sample \(\mathbf{\Pi}^{(i)}\), by the definition of Dirichlet-multinomial distribution, we obtain

\[\left(l_{1k}^{(1)}+g_{\cdot 1k}^{(2)},\cdots,l_{Kk}^{(1)}+g_{\cdot Kk}^{(2)} \right)\sim\mathrm{DirMult}\left(\nu_{1}\nu_{K},\cdots,\xi\nu_{k},\cdots,\nu_{ K}\nu_{k}\right),\]

where \(l_{k_{1}k}^{(1)}=\sum_{t=1}^{M}l_{k_{1}k}^{(t)}\). In particular, with a little abuse of notation here, for Dir-Dir construction, we take \(g_{\cdot k_{1}k}^{(2)}=h_{k_{1}k}^{(2)}\). We first sample

\[\left(h_{k_{1}k}^{(1)}\mid-\right)\sim\left\{\begin{array}{cc}\mathrm{CRT} \left(l_{k_{1}k}^{(1)}+g_{\cdot k_{1}k}^{(2)},\nu_{k_{1}}\nu_{k}\right)&k_{1} \neq k\\ \mathrm{CRT}\left(l_{k_{1}k}^{(1)}+g_{\cdot k_{1}k}^{(2)},\xi\nu_{k}\right)&k_ {1}=k.\end{array}\right. \tag{44}\]

Then we sample

\[q_{k}^{(1)}\sim\mathrm{Beta}\left(l_{k}^{(1)}+g_{\cdot k}^{(2)},\nu_{k}\left( \sum_{k_{1}\neq k}\nu_{k1}+\xi\right)\right). \tag{45}\]

We further introduce

\[n_{k}= h_{kk}^{(1)}+\sum_{k_{1}\neq k}h_{k_{1}k}^{(1)}+\sum_{k_{2}\neq k}h_{ kk_{2}}^{(1)}+l_{k}^{(1)},\text{ and }\] \[\rho_{k}= \tau_{0}\zeta^{(1)}-\ln\left(1-q_{k}^{(1)}\right)\left(\xi+\sum_ {k_{1}\neq k}\nu_{k_{1}}\right)-\sum_{k_{2}\neq k}\ln\left(1-q_{k_{2}}^{(1)} \right)\nu_{k_{2}}.\]Via Poisson-gamma conjugacy, we have

\[(\xi\mid-) \sim\operatorname{Gam}\left(\frac{\gamma_{0}}{K}+\sum_{k}h_{kk}^{(1)},\beta-\sum_{k}\nu_{k}\ln\left(1-q_{k}^{(1)}\right)\right), \tag{46}\] \[(\nu_{k}\mid-) \sim\operatorname{Gam}\left(\frac{\gamma_{0}}{K}+n_{k},\beta+\rho _{k}\right). \tag{47}\]

**Sampling \(\delta^{(t)}\) and \(\beta\) :** Via Poisson-gamma conjugacy

\[\left(\delta^{(t)}\mid-\right)\sim\operatorname{Gam}\left(\epsilon_{0}+\sum_{ v=1}^{V}y_{v}^{(t)},\epsilon_{0}+\sum_{k=1}^{K}\theta_{k}^{(t)}\right). \tag{48}\]

And by gamma-gamma conjugacy, we obtain

\[(\beta\mid-)\sim\operatorname{Gam}\left(\epsilon_{0}+\gamma_{0},\epsilon_{0}+ \sum_{k=1}^{K}\nu_{k}\right). \tag{49}\]

The full procedure of our Gibbs sampling algorithms are summarized in Algorithm 1, Algorithm 2 and Algorithm 3.

```
Input: observed count sequence \(\{\boldsymbol{y}^{(t)}\}_{t=1}^{T}\), iterations \(\mathcal{J}\). Initialize the model's rank \(K\), hyperparameters \(\gamma_{0},\epsilon_{0},e_{0},f_{0}\). for\(iter=1\) to \(\mathcal{J}\)do  Sample \(\{y_{kk}^{(t)}\}_{v,k}\) via Eq.(6).  Sample \(\{\boldsymbol{\phi}_{k}\}_{k}\) via Eq.(7).  Sample \(\{\delta^{(t)}\}_{t}\) via Eq.(48). Update \(\zeta^{(t)}\) as \(\zeta^{(T+1)}=0\), \(\zeta^{(t)}=\ln\left(1+\frac{\delta^{(t)}}{\tau_{0}}+\zeta^{(t+1)}\right),\ t =T,\cdots,1\).  Set \(l_{k}^{(T+1)}=0\). for\(t=T\) to \(\boldsymbol{2}\)do  Sample \(\{l_{k}^{(t)}\}_{k}\) and \(\{l_{kk_{k}}^{(t)}\}_{k,k_{2}}\) via Eq.(8) and Eq.(9) respectively. endfor for\(t=1\) to \(T\)do  Sample \(\{\theta_{k}^{(t)}\}_{k}\) via Eq.(12) and Eq.(13). endfor for\(i=1\) to \(I\)do  Sample \(\{q_{k}^{(i)}\}_{k}\) and \(\{h_{k_{k}}^{(i)}\}_{k_{1},k}\) via Eq.(16), Eq.(44) and Eq.(45).  Sample \(\{\boldsymbol{\pi}_{k}^{(i)}\}_{k}\) via Eq.(14) and Eq.(19).  Sample \(\eta\) via Eq.(21). endfor  Sample \(\xi\), \(\{\nu_{k}\}_{k}\), \(\beta\) via Eq.(46), Eq.(47) and Eq.(49) respectively. endfor Output posterior means:\(\{\theta_{k}^{(1:T)}\}_{k}\),\(\{\boldsymbol{\phi}_{k}\}_{k}\), \(\{\boldsymbol{\pi}_{k}^{(i)}\}_{k}\), \(\delta^{(1:T)}\), \(\xi\), \(\{\nu_{k}\}_{k}\), \(\beta\).
```

**Algorithm 1** Gibbs sampling algorithm for NS-PGDS (Dir-Dir Markov construction)

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_EMPTY:20]

Justification: As we discussed in the conclusion part, the length of each sub-interval is a constant and is treated as a hyper-parameter of the model. In the future work, we plan to design a method that can find the point of change and thus the length of each sub-interval can be determined automatically.

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The derivation of the Gibbs sampler is the main theoretical part of this paper which can be found in sec.4 and the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]Justification: We carefully described the proposed model in sec.2 and sec.3 and the experiment details can be found in sec.6.1.

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: The authors will release the data and code as soon as possible if this paper could be accepted. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.

* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experiment details can be found in sec.6.1. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Table 1 reports the predictive performance of the proposed model and the corresponding standard deviation. The results are computed by running the Gibbs sampling several times from different initialization. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]Justification: The experiments are conducted on a server with an Intel(R) Xeon(R) CPU E5-2699CV4 @ 2.20GHz and 64G RAM.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).

9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We have checked the NeurIPS Code of Ethics and make sure this work is with the NeurIPS Code of Ethics.

Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: For positive societal impacts, we have discussed in conclusion section for the potential application for textual analysis and social networks. And the authors think this work does not have potential negative societal impacts.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This work poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used.

* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.