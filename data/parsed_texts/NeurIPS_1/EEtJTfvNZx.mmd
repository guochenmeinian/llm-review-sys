# Harnessing the Power of Choices in Decision Tree Learning

 Guy Blanc

Stanford

gblanc@stanford.edu

Authors ordered alphabetically.

Jane Lange1

MIT

jlange@mit.edu

Chirag Pabbaraju1

Stanford

cpabbara@stanford.edu

Colin Sullivan1

Stanford

colins26@stanford.edu

Li-Yang Tan1

Stanford

lytan@stanford.edu

Mo Tiwari1

Stanford

motiwari@stanford.edu

Footnote 1: footnotemark:

###### Abstract

We propose a simple generalization of standard and empirically successful decision tree learning algorithms such as ID3, C4.5, and CART. These algorithms, which have been central to machine learning for decades, are greedy in nature: they grow a decision tree by iteratively splitting on the best attribute. Our algorithm, Top-\(k\), considers the \(k\) best attributes as possible splits instead of just the single best attribute.We demonstrate, theoretically and empirically, the power of this simple generalization. We first prove a _greediness hierarchy theorem_ showing that for every \(k\in\mathds{N}\), \(\text{Top-}(k+1)\) can be dramatically more powerful than Top-\(k\): there are data distributions for which the former achieves accuracy \(1-\varepsilon\), whereas the latter only achieves accuracy \(\frac{1}{2}+\varepsilon\). We then show, through extensive experiments, that Top-\(k\) outperforms the two main approaches to decision tree learning: classic greedy algorithms and more recent "optimal decision tree" algorithms. On one hand, Top-\(k\) consistently enjoys significant accuracy gains over greedy algorithms across a wide range of benchmarks. On the other hand, Top-\(k\) is markedly more scalable than optimal decision tree algorithms and is able to handle dataset and feature set sizes that remain far beyond the reach of these algorithms. The code to reproduce our results is available at: [https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

## 1 Introduction

Decision trees are a fundamental workhorse in machine learning. Their logical and hierarchical structure makes them easy to understand and their predictions easy to explain. Decision trees are therefore the most canonical example of an interpretable model: in his influential survey [1], Breiman writes "On interpretability, trees rate an A+"; much more recently, the survey [14] lists decision tree optimization as the very first of 10 grand challenges for the field of interpretable machine learning. Decision trees are also central to modern ensemble methods such as random forests [1] and XGBoost [15], which achieve state-of-the-art accuracy for a wide range of tasks.

Greedy algorithms such as ID3 [17], C4.5 [16], and CART [1] have long been the standard approach to decision tree learning. These algorithms build a decision tree from labeled data in a top-down manner, growing the tree by iteratively splitting on the "best" attribute as measured with respect to a certain heuristic function (e.g., information gain). Owing to their simplicity, thesealgorithms are highly efficient and scale gracefully to handle massive datasets and feature set sizes, and they continue to be widely employed in practice and enjoy significant empirical success. For the same reasons, these algorithms are also part of the standard curriculum in introductory machine learning and data science courses.

The trees produced by these greedy algorithms are often reasonably accurate, but can nevertheless be suboptimal. There has therefore been a separate line of work, which we review in Section 2, on algorithms that optimize for accuracy and seek to produce optimally accurate decision trees. These algorithms employ a variety of optimization techniques (including dynamic programming, integer programming, and SAT solvers) and are completely different from the simple greedy algorithms discussed above. Since the problem of finding an optimal decision tree has long been known to be NP-hard [16], any algorithm must suffer from the inherent combinatorial explosion when the instance size becomes sufficiently large (unless P=NP). Therefore, while this line of work has made great strides in improving the scalability of algorithms for optimal decision trees, dataset and feature set sizes in the high hundreds and thousands remain out of reach.

This state of affairs raises a natural question:

* Can we design decision tree learning algorithms that improve significantly on the accuracy of classic greedy algorithms and yet inherit their simplicity and scalability?

In this work, we propose a new approach and make a case that provides a strong affirmative answer to the question above. Our work also opens up several new avenues for exploration in both the theory and practice of decision tree learning.

### Our contributions

#### 1.1.1 Top-\(k\): a simple and effective generalization of classic greedy decision tree algorithms

We introduce an easily interpretable greediness parameter to the class of all greedy decision tree algorithms, a broad class that encompasses ID3, C4.5, and CART. This parameter, \(k\), represents the number of features that the algorithm considers as candidate splits at each step. Setting \(k=1\) recovers the fully greedy classical approaches, and increasing \(k\) allows the practitioner to produce more accurate trees at the cost of only a mild training slowdown. The focus of our work is on the regime where \(k\) is a small constant--preserving the efficiency and scalability of greedy algorithms is a primary objective of our work--although we mention here that by setting \(k\) to be the dimension \(d\), our algorithm produces an optimal tree. Our overall framework can thus be viewed as interpolating between greedy algorithms at one extreme and "optimal decision tree" algorithms at the other, precisely the two main and previously disparate approaches to decision tree learning discussed above.

We will now describe our framework. A feature scoring function \(\mathcal{H}\) takes as input a dataset over \(d\) binary features and a specific feature \(i\in[d]\), and returns a value quantifying the "desirability" of this feature as the root of the tree. The greedy algorithm corresponding to \(\mathcal{H}\) selects as the root of the tree the feature that has the largest score under \(\mathcal{H}\); our generalization will instead consider the \(k\) features with the \(k\) highest scores.

**Definition 1** (Feature scoring function).: _A feature scoring function \(\mathcal{H}\) takes as input a labeled dataset \(S\) over a \(d\)-dimensional feature space, a feature \(i\in[d]\), and returns a score \(\nu_{i}\in[0,1]\)._

See Section 3.1 for a discussion of the feature scoring functions that correspond to standard greedy algorithms ID3, C4.5, and CART. Pseudocode for Top-\(k\) is provided in Figure 1. We note that from the perspective of interpretability, the trained model looks the same regardless of what \(k\) is. During training, the algorithm considers more splits, but only one split is eventually used at each node.

#### 1.1.2 Theoretical results on the power of Top-\(k\)

The search space of Top-\((k+1)\) is larger than that of Top-\(k\), and therefore its training accuracy is certainly at least as high. The first question we consider is: is the test accuracy of Top-\((k+1)\) only marginally better than that of Top-\(k\), or are there examples of data distributions for which even a single additional choice provably leads to huge gains in test accuracy? Our first main theoretical result is a sharp greediness hierarchy theorem, showing that this parameter can have dramatic impacts on accuracy, thereby illustrating its power:

**Theorem 1** (Greediness hierarchy theorem).: _For every \(\varepsilon>0\), \(k,h\in\mathds{N}\), there is a data distribution \(\mathcal{D}\) and sample size \(n\) for which, with high probability over a random sample \(\mathbf{S}\sim\mathcal{D}^{n}\),_ Top-\((k+1)\) _achieves at least \(1-\varepsilon\) accuracy with a depth budget of \(h\), but_ Top-\(k\) _achieves at most \(\frac{1}{2}+\varepsilon\) accuracy with a depth budget of \(h\)._

All of our theoretical results, Theorems 1 to 3, hold whenever the scoring function is an _impurity-based heuristic_. This broad class includes the most popular scoring functions (see Section 3.1 for more details). Theorem 1 is a special case of a more general result that we show: for all \(k<K\), there are data distributions on which Top-\(K\) achieves maximal accuracy gains over Top-\(k\), even if Top-\(k\) is allowed a larger depth budget:

**Theorem 2** (Generalization of Theorem 1).: _For every \(\varepsilon>0\), \(k,K,h\in\mathds{N}\) where \(k<K\), there is a data distribution \(\mathcal{D}\) and sample size \(n\) for which, with high probability over a random sample \(\mathbf{S}\sim\mathcal{D}^{n}\),_ Top-\(K\) _achieves at least \(1-\varepsilon\) accuracy with a depth budget of \(h\), but_ Top-\(k\) _achieves at most \(\frac{1}{2}+\varepsilon\) accuracy even with a depth budget of \(h+(K-k-1)\)._

The proof of Theorem 2 is simple and highlights the theoretical power of choices. One downside, though, is that it is based on data distributions that are admittedly somewhat unnatural: the labeling function has embedded within it a function that is the XOR of certain features, and real-world datasets are unlikely to exhibit such adversarial structure. To address this, we further prove that the power of choices is evident even for monotone data distributions. We defer the definition of monotone data distributions to Section 4.2.

**Theorem 3** (Greediness hierarchy theorem for monotone data distributions).: _For every \(\varepsilon>0\), depth budget \(h\), \(K\) between \(\tilde{\Omega}(h)\) and \(\tilde{O}(h^{2})\) and \(k\leq K-h\), there is a monotone data distribution \(\mathcal{D}\) and sample size \(n\) for which, with high probability over a random sample \(\mathbf{S}\sim\mathcal{D}^{n}\), Top-\(K\) achieves at least \(1-\varepsilon\) accuracy with a depth budget of \(h\), but Top-\(k\) achieves at most \(\frac{1}{2}+\varepsilon\) accuracy with a depth budget of \(h\)._

Many real-world data distributions are monotone in nature, and relatedly, they are a common assumption and the subject of intensive study in learning theory. Most relevant to this paper, recent theoretical work has identified monotone data distributions as a broad and natural class for which classical greedy decision tree algorithms (i.e., Top-\(1\)) provably succeed [1, 1]. Theorem 3 shows that even within this class, increasing the greediness parameter can lead to dramatic gains in accuracy. Compared to Theorem 2, the proof of Theorem 3 is more technical and involves the use of concepts from the Fourier analysis of boolean functions [13].

Figure 1: The Top-\(k\) algorithm. It can be instantiated with any feature scoring function \(\mathcal{H}\), and when \(k=1\), recovers standard greedy algorithms such as ID3, C4.5, and CART.

[MISSING_PAGE_FAIL:4]

soft), to understand the classification of a test point, it is sufficient to look at only one root-to-leaf path, as opposed to a weighted combination across many.

## 3 The Top-\(k\) algorithm

### Background and context: Impurity-based algorithms

Greedy decision tree learning algorithms like ID3, C4.5 and CART are all instantiations of Top-\(k\) in Figure 1 with \(k=1\) and an appropriate choice of the feature-scoring function \(\mathcal{H}\). Those three algorithms all used impurity-based heuristics as their feature-scoring function:

**Definition 2** (Impurity-based heuristic).: _An impurity function\(\mathcal{G}:[0,1]\rightarrow[0,1]\) is a function that is concave, symmetric about \(0.5\), and satisfies \(\mathcal{G}(0)=\mathcal{G}(1)=0\) and \(\mathcal{G}(0.5)=1\). A feature-scoring function \(\mathcal{H}\) is an impurity-based heuristic, if there is some impurity function \(\mathcal{G}\) for which:_

\[\mathcal{H}(S,i) =\mathcal{G}\left(\mathop{\mathbb{E}}_{\mathbf{x},\mathbf{y}\sim\mathcal{ S}}[\mathbf{y}]\right)-\Pr_{\mathbf{x},\mathbf{y}\sim\mathcal{S}}[\mathbf{x}_{i}=0]\cdot\mathcal{ G}\left(\mathop{\mathbb{E}}_{\mathbf{x},\mathbf{y}\sim\mathcal{S}}[\mathbf{y}\mid\mathbf{x}_{i}=0]\right)\] \[-\Pr_{\mathbf{x},\mathbf{y}\sim\mathcal{S}}[\mathbf{x}_{i}=1]\cdot\mathcal{ G}\left(\mathop{\mathbb{E}}_{\mathbf{x},\mathbf{y}\sim\mathcal{S}}[\mathbf{y}\mid\mathbf{x}_{i}=1]\right)\]

_where in each of the above, \((\mathbf{x},\mathbf{y})\) are a uniformly random point from within \(S\)._

Common examples for the impurity function include the binary entropy function \(\mathcal{G}(p)=-p\log_{2}(p)-(1-p)\log_{2}(1-p)\) (used by ID3 and C4.5), the Gini index \(\mathcal{G}(p)=4p(1-p)\) (used by CART), and the function \(\mathcal{G}(p)=2\sqrt{p(1-p)}\) (proposed and analyzed in [14]). We refer the reader to [14] for a theoretical comparison, and [1] for an experimental comparison, of these impurity-based heuristics.

Our experiments focus on binary entropy being the impurity measure, but our theoretical results apply to Top-\(k\) instantiated with _any_ impurity-based heuristic.

### Basic theoretical properties of the Top-\(k\) algorithm

Running time.The key behavioral aspect in which Top-\(k\) differs from greedy algorithms is that it is less greedy when trying to determine which coordinate to query. This naturally increases the running time of Top-\(k\), but that increase is fairly mild. More concretely, suppose Top-\(k\) is run on a dataset \(S\) with \(n\) points. We can then easily derive the following bound on the running time of Top-\(k\), where \(\mathcal{H}(S,i)\) is assumed to take \(O(n)\) time to evaluate (as it does for all impurity-based heuristics).

**Claim 3.1**.: _The running time of Top-\(k(\mathcal{H},S,h)\) is \(O((2k)^{h}\cdot nd)\)._

Proof.: Let \(T_{h}\) be the number of recursive calls made by Top-\(k(\mathcal{H},S,h)\). Then, we have the simple recurrence relation \(T_{h}=2kT_{h-1}\), where \(T_{0}=1\). Solving this recurrence gives \(T_{h}=(2k)^{h}\). Each recursive call takes \(O(nd)\) time, where the bottleneck is scoring each of the \(d\) features. 

We note that any decision tree algorithm, including fast greedy algorithms such as ID3, C4.5, and CART, has runtime that scales exponentially with the depth \(h\). The size of a depth-\(h\) tree can be \(2^{h}\), and this is of course a lower bound on the runtime as the algorithm needs to output such a tree. In contrast with greedy algorithms (for which \(k=1\)), Top-\(k\) incurs an additional \(k^{h}\) cost in running time. As mentioned earlier, in practice, we are primarily concerned with fitting small decision trees (e.g., \(h=5\)) to the data, as this allows for explainable predictions. In this setting, the additional \(k^{h}\) cost (for small constant \(k\)) is inexpensive, as confirmed by our experiments.

The search space of Top-\(k\):We state and prove a simple claim that Top-\(k\) returns the _best_ tree within its search space.

**Definition 3** (Search space of Top-\(k\)).: _Given a sample \(S\) and integers \(h,k\), we use \(\mathcal{T}_{k,h,S}\) to refer to all trees in the search space of Top-\(k\). Specifically, if \(h=0\), this contains all trees with a height of zero (the constant \(0\) and constant \(1\) trees). For \(h\geq 1\), and \(\mathcal{I}\subseteq[d]\) being the \(k\) coordinates with maximal score, this contains all trees with a root of \(x_{i}\), left subtree in \(\mathcal{T}_{k,h-1,S_{x_{i}=0}}\) and right subtree in \(\mathcal{T}_{k,h-1,S_{x_{i}=1}}\) for some \(i\in\mathcal{I}\)._

**Lemma 3.2** (Top-\(k\) chooses the most accurate tree in its search space).: _For any sample \(S\) and integers \(h,k\), let \(T\) be the output of Top-\(k\) with a depth budget of \(h\) on \(S\). Then_

\[\Pr_{\mathbf{x},\mathbf{y}\sim S}[T(\mathbf{x})=\mathbf{y}]=\max_{T^{\prime}\in\mathcal{T}_{k,h,S}}\left(\Pr_{\mathbf{x},\mathbf{y}\sim S}[T^{\prime}(\mathbf{x})=\mathbf{y}]\right).\]

We refer the reader to Appendix A for the proof of this lemma.

## 4 Theoretical bounds on the power of choices

We refer the reader to the Appendix B for most of the setup and notation. For now, we briefly mention a small amount of notation relevant to this section: we use **bold font** (e.g. \(\mathbf{x}\)) to denote random variables. We also use bold font to indicate _stochastic functions_ which output a random variable. For example,

\[\mathbf{f}(x)\coloneqq\begin{cases}x&\text{with probability }\frac{1}{2}\\ -x&\text{with probability }\frac{1}{2}\end{cases}\]

is the stochastic function that returns either the identity or its negation with equal probability. To define the data distributions of Theorems 2 and 3, we will give a distribution over the domain, \(X\) and the stochastic function that provides the label given an element of the domain.

Intuition for proof of greediness hierarchy theoremTo construct a distribution which Top-\(k\) fits poorly and Top-\((k+1)\) fits well, we will partition features into two groups: one group consisting of features with medium correlation to the labels and another group consisting of features with high correlation when taken all together but low correlation otherwise. Since the correlation of features in the former group is larger than that of the latter group unless all features from the latter group are considered, both algorithms will prioritize features from the former group. However, if the groups are sized correctly, then Top-\((k+1)\) will consider splitting on all features from the latter group, whereas Top-\(k\) will not. As a result, Top-\((k+1)\) will output a decision tree with higher accuracy.

### Proof of Theorem 2

For each depth budget \(h\) and search branching factor \(K\), we will define a hard distribution \(\mathcal{D}_{h,K}\) that is learnable to high accuracy by Top-\(K\) with a depth of \(h\), but not by Top-\(k\) with a depth of \(h^{\prime}\) for any \(h^{\prime}<h+K-k\). This distribution will be over \(\{0,1\}^{d}\times\{0,1\}\), where \(d=h+K-1\). The marginal distribution over \(\{0,1\}^{d}\) is uniform, and the distribution over \(\{0,1\}\) conditioned on a setting of the \(d\) features is given by the stochastic function \(\mathbf{f}_{h,K}(x)\). All of the results of this section (Theorems 2 and 3) hold when the feature scoring function is _any_ impurity-based heuristic.

Description of \(\mathbf{f}_{h,K}(x)\).Partition \(x\) into two sets of variables, \(x^{(1)}\) of size \(h\) and \(x^{(2)}\) of size \(K-1\). Let \(\mathbf{f}_{h,K}(x)\) be the randomized function defined as follows:

\[\mathbf{f}_{h,K}(x)=\begin{cases}\mathrm{Par}_{h}(x^{(1)})&\text{with probability }1-\varepsilon\\ x_{i}^{(2)}\sim\mathrm{Unif}[x^{(2)}]&\text{with probability }\varepsilon,\end{cases}\]

where \(\mathrm{Unif}[x^{(2)}]\) denotes the uniform distribution on \(x^{(2)}\). \(\mathrm{Par}_{h}(x^{(1)})\) is the parity function, whose formal definition can be found in Appendix B.

The proof of Theorem 2 is divided into two parts. First, we prove that when the data distribution is \(\mathcal{D}_{h,K}\), Top-\(K\) succeeds in building a high accuracy tree with a depth budget of \(h\). Then, we show that Top-\(k\) fails and builds a tree with low accuracy, even given a depth budget of \(h+(K-k-1)\).

**Lemma 4.1** (Top-\(K\) succeeds).: _The accuracy of Top-\(K\) with a depth of \(h\) on \(\mathcal{D}_{h,K}\) is at least \(1-\varepsilon\)._

**Lemma 4.2** (Top-\(k\) fails).: _The accuracy of Top-\(k\) with a depth of \(h^{\prime}\) on \(\mathcal{D}_{h,K}\) is at most \((1/2+\varepsilon)\) for any \(h^{\prime}<h+K-k\)._

Proofs of both these lemmas are deferred to Appendix B. Theorem 2 then follows directly from these two lemmas.

### Proof of Theorem 3

In this section, we overview the proof Theorem 3. Some of the proofs are deferred to Appendix B.2.

Before proving Theorem 3, we formalize the concept of monotonicity. For simplicity, we assume the domain is the Boolean cube, \(\{0,1\}^{d}\), and use the partial ordering \(x\preceq x^{\prime}\) iff \(x_{i}\leq x^{\prime}_{i}\) for each \(i\in[d]\); however, the below definition easily extends to the domain being any partially ordered set.

**Definition 4** (Monotone).: _A stochastic function, \(\mathbf{f}:\{0,1\}^{d}\to\{0,1\}\), is monotone if, for any \(x,x^{\prime}\in\{0,1\}^{d}\) where \(x\preceq x^{\prime}\), \(\operatorname{E}[\mathbf{f}(x)]\leq\operatorname{E}[\mathbf{f}(x^{\prime})]\). A data distribution, \(\mathcal{D}\) over \(\{0,1\}^{d}\times\{0,1\}\) is said to be monotone if the corresponding stochastic function, \(\mathbf{f}(x)\) returning \((\mathbf{y}\mid\mathbf{x}=x)\) where \((\mathbf{x},\mathbf{y})\sim\mathcal{D}\), is monotone._

To construct the data distribution of Theorem 3, we will combine monotone functions, Majority and Tribes, commonly used in the analysis of Boolean functions due to their extremal properties. See Appendix B.2 for their definitions and useful properties. Let \(d=h+K-1\), and the distribution over the domain be uniform over \(\{0,1\}^{d}\). Given some \(x\in\{0,1\}^{d}\), we use \(x^{(1)}\) to refer to the first \(h\) coordinates of \(x\) and \(x^{(2)}\) the other \(K-1\) coordinates. This data distribution is labeled by the stochastic function \(\mathbf{f}\) given below.

\[\mathbf{f}(x)\coloneqq\begin{cases}\text{Tribes}_{h}(x^{(1)})&\text{with probability }1-\varepsilon\\ \text{Maj}_{K-1}(x^{(2)})&\text{with probability }\varepsilon.\end{cases}\]

Clearly \(\mathbf{f}\) is monotone as it is the mixture of two monotone functions. Throughout this subsection, we'll use \(\mathcal{D}_{h,K}\) to refer to the data distribution over \(\{0,1\}^{d}\times\{0,1\}\) where to sample \((\mathbf{x},\mathbf{y})\sim\mathcal{D}\), we first draw \(\mathbf{x}\sim\{0,1\}^{d}\) uniformly and then \(\mathbf{y}\) from \(\mathbf{f}(\mathbf{x})\). The proof of Theorem 3 is a direct consequence of the following two Lemmas, both of which we prove in Appendix B.2.

**Lemma 4.3** (Top-\(K\) succeeds).: _On the data distribution \(\mathcal{D}_{h,K}\), Top-\(K\) with a depth budget of \(h\) achieves at least \(1-\varepsilon\) accuracy._

**Lemma 4.4** (Top-\(k\) fails).: _On the data distribution \(\mathcal{D}_{h,K}\), Top-\(k\) with a depth budget of \(h\) achieves at most \(\frac{1}{2}+\varepsilon\) accuracy._

## 5 Experiments

Setup for experiments.At all places, the Top-\(1\) tree that we compare to is that given by scikit-learn [25], which according to their documentation2, is an optimized version of CART. We run experiments on a variety of datasets from the UCI Machine Learning Repository [1] (numerical as well as categorical features) having a size in the thousands and having \(\approx 50-300\) features after binarization. There were \(\approx 100\) datasets meeting these criteria, and we took a random subset of \(20\) such datasets. We binarize all the datasets - for categorical datasets, we convert every categorical feature that can take on (say) \(\ell\) values into \(\ell\) binary features. For numerical datasets, we sort and compute thresholds for each numerical attribute, so that the total number of binary features is \(\approx 100\). A detailed description of the datasets is given in Appendix C.

Footnote 2: [https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart)

We build decision trees corresponding to binary entropy as the impurity measure \(\mathcal{H}\). In order to leverage existing engineering optimizations from state-of-the-art optimal decision tree implementations, we implement the Top-\(k\) algorithm given in Figure 1 via simple modifications to the PyDL8.5 [2, 2] codebase3. Details about this are provided in Appendix D. Our implementation of the Top-\(k\) algorithm and other technical details for the experiments are available at [https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

Footnote 3: [https://github.com/aia-uclouvain/pydl8.5](https://github.com/aia-uclouvain/pydl8.5)

[https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

[https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

[https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

[https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

[https://github.com/SullivanC19/pydl8.5](https://github.com/SullivanC19/pydl8.5)

[https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

[https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

[https://github.com/SullivanC19/pydl8.5-topk](https://github.com/SullivanC19/pydl8.5-topk).

### Key experimental findings

Small increments of \(k\) yield significant accuracy gains.Since the search space of Top-\(k\) is a superset of that of Top-1 for any \(k>1\), the training accuracy of Top-\(k\) is guaranteed to be larger. The primary objective in this experiment is to show that Top-\(k\) can outperform Top-\(1\) in terms of test accuracy as well. Figure 2 shows the results for Top-1 versus Top-\(k\) for \(k=2,3,4,8,12,16,d\). Eachplot is a different dataset, where on the x-axis, we plot the depth of the learned decision tree, and on the y-axis, we plot the test accuracy. Note that \(k=d\) corresponds to the DL8.5 optimal decision tree. We can clearly observe that the test accuracy increases as \(k\) increases--in some cases, the gain is \(>5\%\) (absolute). Furthermore, for (smaller) datasets like nursery, for which we were able to run \(k=d\), the accuracy of Top-\(8/16\) is already very close to that of the optimal tree.

Lastly, since Top-\(k\) invests more computation towards fitting a better tree on the training set, its training time is naturally longer than Top-1. However, Figure 6 in Appendix E, which plots the training time, shows that the slowdown is mild.

Top-\(k\) scales much better than optimal decision tree algorithms.Optimal decision tree algorithms suffer from poor runtime scaling. We empirically demonstrate that, in comparison, Top-\(k\) has a significantly better scaling in training time. Our experiments are identical to those in Figures 14 and 15 in the GOSDT paper [14], where two notions of scalability are considered. In the first experiment, we fix the number of samples and gradually increase the number of features to train the decision tree. In the second experiment, we include all the features, but gradually increase the number of training samples. The dataset we use is the FICO [16] dataset, which has a total of 1000 samples with 1407 binary features. We plot the training time (in seconds) versus number of features/samples for optimal decision tree algorithms (MurTree, GOSDT) and Top-\(k\) in Figure 3. We do this for depth \(=4,5,6\) (for GOSDT, the regularization coefficient \(\lambda\) is set to \(2^{-\text{depth}}\)). We observe that the training time for both MurTree and GOSDT increases dramatically compared to Top-\(k\), in both experiments. In particular, for depth \(=5\), both MurTree and GOSDT were unable to build a tree on 300 features within the time limit of 10 minutes, while Top-\(16\) completed execution even with all 1407 features. Similarly, in the latter experiment, GOSDT/MurTree were unable to build a depth-5 tree on 150 samples within the time limit, while Top-\(16\) comfortably finished execution even on 1000 samples. These experiments demonstrates the scalability issues with optimal tree algorithms. Coupled with the accuracy gains seen in the previous experiment, Top-\(k\) can thus be seen as achieving a more favorable tradeoff between training time and accuracy.

We note, however, that various optimization have been proposed to allow optimal decision tree algorithms to scale to larger datasets. For example, a more recent version of GOSDT has integrated a guessing strategy using reference ensembles which guides the binning of continuous features, tree

Figure 2: Test accuracy comparison between Top-\(k\) for various values of \(k\). We can see that Top-\((k+1)\) generally obtains higher accuracy than Top-\(k\), and in some cases (e.g., nursery), Top-\(8/16\)â€™s accuracy is even comparable to the optimal tree (Top-\(d\)). Missing points in the plots correspond to settings that did not terminate within a sufficiently large time limit. All plots are averaged over 10 random train-test splits (except avila and ml-prove that have pre-specified splits) with confidence intervals plotted for 2 standard deviations.

size, and search [MZA\({}^{+}\)]. Many of these optimizations are generally applicable across optimal tree algorithms and could be combined with Top-\(k\) for further improvement in performance.

Increasing \(k\) beyond a point does not improve test accuracy.In our experiments above, we ran Top-\(k\) only till \(k=16\): in Figure 4, we show that increasing \(k\) to very large values, which increases runtime, often does not improve test accuracy, and in some cases, may even _hurt_ due to overfitting. For 3 datasets - car, hayes-roth and tic-tac-toe - we plot train and test error as a function of \(k\). Naturally, the train accuracy monotonically increases with \(k\) in each plot. However, for both car and hayes-roth, we can observe that the test accuracy first increases and then plateaus. Interestingly, for tic-tac-toe, the test accuracy first increases and then _decreases_ as we increase \(k\). These experiments demonstrate that selecting too large of a \(k\), as optimal decision tree algorithms do, is a waste of computational resources and can even hurt test accuracy via overfitting.

## 6 Conclusion

We have shown how popular and empirically successful greedy decision tree learning algorithms can be improved with the power of choices: our generalization, Top-\(k\), considers the \(k\) best features as candidate splits instead of just the single best one. As our theoretical and empirical results demonstrate, this simple generalization is powerful and enables significant accuracy gains while preserving the efficiency and scalability of standard greedy algorithms. Indeed, we find it surprising that such a simple generalization has not been considered before.

There is much more to be explored and understood, both theoretically and empirically; we list here a few concrete directions that we find particularly exciting and promising. First, we suspect that power

Figure 4: Test accuracy plateaus for large \(k\). All runs averaged over 10 random train-test splits with maximum depth fixed to 3.

Figure 3: Training time comparison between Top-\(k\) and optimal tree algorithms. As the number of features/samples increases, both GOSDT and MurTree scale poorly compared to Top-\(k\), and beyond a threshold, do not complete execution within the time limit.

of choices affords more advantages over greedy algorithms than just accuracy gains. For example, an avenue for future work is to show that the trees grown by Top-\(k\) are more robust to noise. Second, are there principled approaches to the automatic selection of the greediness parameter \(k\)? Can the optimal choice be inferred from a few examples or learned over time? This opens up the possibility of new connections to machine-learned advice and algorithms with predictions [14], an area that has seen a surge of interest in recent years. Finally, as mentioned in the introduction, standard greedy decision tree algorithms are at the very heart of modern tree-based ensemble methods such as XGBoost and random forests. A natural next step is to combine these algorithms with Top-\(k\) and further extend the power of choices to these settings.

## Acknowledgements

We thank the NeurIPS reviewers and AC for their detailed and helpful feedback.

Guy and Li-Yang are supported by NSF awards 1942123, 2211237, 2224246 and a Google Research Scholar award. Jane is supported by NSF Graduate Research Fellowship under Grant No. 2141064, NSF Awards CCF-2006664, DMS-2022448, and Microsoft. Mo is supported by a Stanford Interdisciplinary Graduate Fellowship and a Stanford Data Science Scholarship. Chirag is supported by Moses Charikar and Greg Valiant's Simons Investigator Awards.

## References

* [AAV19] Sina Aghaei, Mohammad Javad Azizi, and Phebe Vayanos. Learning optimal and fair decision trees for non-discriminative decision-making. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 1418-1426, 2019.
* [ABF\({}^{+}\)09] Misha Alekhnovich, Mark Braverman, Vitaly Feldman, Adam Klivans, and Toniann Pitassi. The complexity of properly learning simple concept classes. _Journal of Computer & System Sciences_, 74(1):16-34, 2009.
* [AH08] Micah Adler and Brent Heeringa. Approximating optimal binary decision trees. In _Approximation, Randomization and Combinatorial Optimization. Algorithms and Techniques_, pages 1-9. Springer, 2008.
* [ANS20] Gael Aglin, Siegfried Nijssen, and Pierre Schaus. Learning optimal decision trees using caching branch-and-bound search. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 3146-3153, 2020.
* [ANS21] Gael Aglin, Siegfried Nijssen, and Pierre Schaus. Pydl8. 5: a library for learning optimal decision trees. In _Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence_, pages 5222-5224, 2021.
* [Ave20] Florent Avellaneda. Efficient inference of optimal decision trees. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 3195-3202, 2020.
* [BD17] Dimitris Bertsimas and Jack Dunn. Optimal classification trees. _Machine Learning_, 106(7):1039-1082, 2017.
* [BDM19] Alon Brutzkus, Amit Daniely, and Eran Malach. On the Optimality of Trees Generated by ID3. _ArXiv_, abs/1907.05444, 2019.
* [BDM20] Alon Brutzkus, Amit Daniely, and Eran Malach. ID3 learns juntas for smoothed product distributions. In _Proceedings of the 33rd Annual Conference on Learning Theory (COLT)_, pages 902-915, 2020.
* [BFS08] Leo Breiman, Jerome Friedman, Charles Stone, and Richard Olshen. _Classification and regression trees_. Wadsworth International Group, 1984.
* [BLQT21a] Guy Blanc, Jane Lange, Mingda Qiao, and Li-Yang Tan. Decision tree heuristics can fail, even in the smoothed setting. In Mary Wootters and Laura Sanita, editors, _Proceedings of the 25th International Conference on Randomization and Computation (RANDOM)_, volume 207, pages 45:1-45:16, 2021.

* [BLQT21b] Guy Blanc, Jane Lange, Mingda Qiao, and Li-Yang Tan. Properly learning decision trees in almost polynomial time. In _Proceedings of the 62nd IEEE Annual Symposium on Foundations of Computer Science (FOCS)_, 2021.
* [BLT20a] Guy Blanc, Jane Lange, and Li-Yang Tan. Provable guarantees for decision tree induction: the agnostic setting. In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, 2020.
* [BLT20b] Guy Blanc, Jane Lange, and Li-Yang Tan. Top-down induction of decision trees: rigorous guarantees and inherent limitations. In _Proceedings of the 11th Innovations in Theoretical Computer Science Conference (ITCS)_, volume 151, pages 1-44, 2020.
* [Bre01a] Leo Breiman. Random forests. _Machine learning_, 45(1):5-32, 2001.
* 231, 2001.
* [CG16] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)_, pages 785-794, 2016.
* [DG17] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
* [DKM96] Tom Dietterich, Michael Kearns, and Yishay Mansour. Applying the weak learning framework to understand and improve C4.5. In _Proceedings of the 13th International Conference on Machine Learning (ICML)_, pages 96-104, 1996.
* [DLH\({}^{+}\)22] Emir Demirovic, Anna Lukina, Emmanuel Hebrard, Jeffrey Chan, James Bailey, Christopher Leckie, Kotagiri Ramamohanarao, and Peter J Stuckey. Murtree: Optimal decision trees via dynamic programming and search. _Journal of Machine Learning Research_, 23(26):1-47, 2022.
* [FGI\({}^{+}\)18] FICO, Google, Imperial College London, MIT, University of Oxford, UC Irvine, and UC Berkeley. Explainable Machine Learning Challenge. [https://community.fico.com/s/explainable-machine-learning-challenge](https://community.fico.com/s/explainable-machine-learning-challenge), 2018.
* [HR76] Laurent Hyafil and Ronald L. Rivest. Constructing optimal binary decision trees is np-complete. _Information Processing Letters_, 5(1):15-17, 1976.
* [HRS19] Xiyang Hu, Cynthia Rudin, and Margo Seltzer. Optimal sparse decision trees. _Advances in Neural Information Processing Systems_, 32, 2019.
* [IYA12] Ozan Irsoy, Olcay Taner Yildiz, and Ethem Alpaydin. Soft decision trees. In _Proceedings of the 21st international conference on pattern recognition (ICPR2012)_, pages 1819-1822. IEEE, 2012.
* [JM20] Mikolas Janota and Antonio Morgado. SAT-based encodings for optimal decision trees with explicit paths. In _International Conference on Theory and Applications of Satisfiability Testing_, pages 501-518. Springer, 2020.
* [Kea96] Michael Kearns. Boosting theory towards practice: recent developments in decision tree induction and the weak learning framework (invited talk). In _Proceedings of the 13th National Conference on Artificial intelligence (AAAI)_, pages 1337-1339, 1996.
* [Kea98] Michael Kearns. Efficient noise-tolerant learning from statistical queries. _Journal of the ACM (JACM)_, 45(6):983-1006, 1998.
* [KKL88] Jeff Kahn, Gil Kalai, and Nathan Linial. The influence of variables on boolean functions. In _Proceedings of the 29th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 68-80, 1988.

* [KM96] Michael Kearns and Yishay Mansour. On the boosting ability of top-down decision tree learning algorithms. In _Proceedings of the 28th Annual Symposium on the Theory of Computing (STOC)_, pages 459-468, 1996.
* [KM99] Michael Kearns and Yishay Mansour. On the boosting ability of top-down decision tree learning algorithms. _Journal of Computer and System Sciences_, 58(1):109-128, 1999.
* [LZH\({}^{+}\)20] Jimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, and Margo Seltzer. Generalized and scalable optimal sparse decision trees. In _International Conference on Machine Learning_, pages 6150-6160. PMLR, 2020.
* [MV20] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. _arXiv preprint arXiv:2006.09123_, 2020.
* [MZA\({}^{+}\)] Hayden McTavish, Chudi Zhong, Reto Achermann, Ilias Karimalis, Jacques Chen, Cynthia Rudin, and Margo Seltzer. Fast sparse decision tree optimization via reference ensembles. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(9).
* [NF07] Siegfried Nijssen and Elisa Fromont. Mining optimal decision trees from itemset lattices. In _Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 530-539, 2007.
* [NF10] Siegfried Nijssen and Elisa Fromont. Optimal constraint-based decision tree induction from itemset lattices. _Data Mining and Knowledge Discovery_, 21(1):9-51, 2010.
* [NIPMS18] Nina Narodytska, Alexey Ignatiev, Filipe Pereira, and Joao Marques-Silva. Learning optimal decision trees with sat. In _Ijcai_, pages 1362-1368, 2018.
* [O'D14] Ryan O'Donnell. _Analysis of Boolean Functions_. Cambridge University Press, 2014.
* [PVG\({}^{+}\)11] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* [Qui86] Ross Quinlan. Induction of decision trees. _Machine learning_, 1(1):81-106, 1986.
* [Qui93] Ross Quinlan. _C4.5: Programs for Machine Learning_. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1993.
* [RCC\({}^{+}\)22] Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable machine learning: Fundamental principles and 10 grand challenges. _Statistics Surveys_, 16:1-85, 2022.
* [Sie08] Detlef Sieling. Minimization of decision trees is hard to approximate. _Journal of Computer and System Sciences_, 74(3):394-403, 2008. Computational Complexity 2003.
* [TAA\({}^{+}\)19] Ryutaro Tanno, Kai Arulkumaran, Daniel Alexander, Antonio Criminisi, and Aditya Nori. Adaptive neural trees. In _International Conference on Machine Learning_, pages 6166-6175. PMLR, 2019.
* [VNP\({}^{+}\)20] Helene Verhaeghe, Siegfried Nijssen, Gilles Pesant, Claude-Guy Quimper, and Pierre Schaus. Learning optimal decision trees using constraint programming. _Constraints_, 25(3):226-250, 2020.
* [VZ17] Sicco Verwer and Yingqian Zhang. Learning decision trees with flexible constraints and objectives using integer optimization. In _International Conference on AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems_, pages 94-103. Springer, 2017.

* [VZ19] Sicco Verwer and Yingqian Zhang. Learning optimal classification trees using a binary linear program formulation. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 1625-1632, 2019.
* [ZMP\({}^{+}\)20] Haoran Zhu, Pavankumar Murali, Dzung Phan, Lam Nguyen, and Jayant Kalagnanam. A scalable MIP-based method for learning optimal multivariate decision trees. _Advances in Neural Information Processing Systems_, 33:1771-1781, 2020.