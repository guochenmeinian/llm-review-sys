# Graph Contrastive Learning with Stable and Scalable Spectral Encoding

Deyu Bo\({}^{1}\)1, Yuan Fang\({}^{2}\), Yang Liu\({}^{1}\), Chuan Shi\({}^{1}\)2

\({}^{1}\)Beijing University of Posts and Telecommunications, China

\({}^{2}\)Singapore Management University, Singapore

{bodeyu, liuyangjanet, shichuan}@bupt.edu.cn, yfang@smu.edu.sg

Footnote 1: This work was done when the first author was a visiting student at Singapore Management University

Footnote 2: Corresponding author

###### Abstract

Graph contrastive learning (GCL) aims to learn representations by capturing the agreements between different graph views. Traditional GCL methods generate views in the spatial domain, but it has been recently discovered that the spectral domain also plays a vital role in complementing spatial views. However, existing spectral-based graph views either ignore the eigenvectors that encode valuable positional information, or suffer from high complexity when trying to address the instability of spectral features. To tackle these challenges, we first design an informative, stable, and scalable spectral encoder, termed EigenMLP, to learn effective representations from the spectral features. Theoretically, EigenMLP is invariant to the rotation and reflection transformations on eigenvectors and robust against perturbations. Then, we propose a spatial-spectral contrastive framework (Sp\({}^{2}\)GCL) to capture the consistency between the spatial information encoded by graph neural networks and the spectral information learned by EigenMLP, thus effectively fusing these two graph views. Experiments on the node- and graph-level datasets show that our method not only learns effective graph representations but also achieves a 2-10x speedup over other spectral-based methods.

## 1 Introduction

Graph neural networks (GNNs) have become the _de facto_ framework to encode graph-structured data [14, 35, 39]. However, training high-quality GNNs usually requires a large number of domain-specific labels, which is not feasible in many real-world applications. Therefore, as a paradigm of self-supervised learning, graph contrastive learning (GCL) is proposed to learn node or graph representations without using labels [20, 19, 12, 38].

Typically, GCL methods first generate different views of a graph, and then contrast the positive views against the negative ones. By minimizing a contrastive loss, GCL methods can learn invariant information from different views for various downstream tasks. Therefore, how to generate ideal graph views is crucial to GCL. Most graph views are obtained by augmenting graphs in the spatial domain, such as dropping nodes and edges, heuristically [49, 44, 8] or adversarially [27, 40, 41]. Nevertheless, recent studies [18, 17] argue that spatial perturbations ignore the structural properties, and propose to perturb graph spectrum in the spectral domain.

Generally, the spatial and spectral views represent different information of the graph. The spatial domain captures the feature information and learns local graph representations by propagating the node features along local topology, _i.e._, \(k\)-hop subgraphs. In contrast, the spectral domain covers the global structural information. The eigenvalues and eigenvectors encode the global shapes [13] andnode absolute positions [25]. Leveraging the agreements between the spatial and spectral views can significantly improve the expressive power and generalization ability of GNNs [37; 47]. However, several reasons hinder the study of spectral view: First, the spectral features are unstable. Previous work [24] shows that randomly flipping the signs or rotating the coordinates of eigenvectors also satisfies the eigenvalue decomposition, _a.k.a._, the sign and basis ambiguity issues, implying that spectral features are not unique and hard to transfer [15; 37]. Besides, perturbing spectral features is time-consuming. Existing spectral-based methods [18; 17] need to decompose and reconstruct the adjacency matrix, in which the complexity is cubic and quadratic in the number of nodes, respectively.

Therefore, to realize the spatial-spectral contrast, it is natural to ask: _How to encode the spectral view of a graph effectively and efficiently?_ To answer this question, we first propose a spectral encoder, named EigenMLP, which not only inherits the scalability advantage of multilayer perceptrons (MLP), but also adheres to two key design principles. First, EigenMLP resolves the sign ambiguity issue by taking both positive and negative eigenvectors as input. Second, to address the basis ambiguity issue, the weights of EigenMLP are generated by a learnable mapping of the eigenvalues, making them equivariant to the coordinates of eigenvectors. To integrate the representations learned from the spatial and spectral views, we propose a Spatial-Spectral GCL framework (Sp\({}^{2}\)GCL) to maximize the agreements between views and learn effective representations for downstream tasks.

The contributions of our paper are as follows. (1) We propose EigenMLP, a novel spectral encoder to effectively and efficiently learn sign- and basis-invariant representations from spectral features. (2) We theoretically prove that EigenMLP is permutation-equivariant, rotation- and reflection-invariant, and can learn more stable representations against structural perturbations. (3) We propose Sp\({}^{2}\)GCL, a spatial-spectral GCL framework that utilizes the cross-domain contrasts to effectively fuse the feature and structural information learned by spatial GNNs and EigenMLP. (4) Extensive experiments on both node-level and graph-level tasks demonstrate the effectiveness of the proposed framework Sp\({}^{2}\)GCL, and verify the scalability and stability aspects of EigenMLP.

## 2 Related Work

Graph Contrastive Learning.Most GCL methods aim to learn invariant information by maximizing the mutual information between different graph views [1; 36]. There are many ways to generate graph views and we broadly categorize them into the spatial and spectral approaches. In the spatial domain, graph views are usually generated by augmenting the original graphs. For example, GRACE [48], GCA [49] and CCA-SSG [44] propose to augment graphs by randomly dropping edges and nodes, AD-GCL [27] leverages adversarial training to filter unimportant edges, and JOAO [41] combines different augmentation strategies automatically. While in the spectral domain, the views are generated by perturbing graph spectrum. For example, MVGRL [8] heuristically uses a graph diffusion as augmentation, which acts as a low-pass filter, SpCo [18] proposes to preserve low-frequency components and perturbs high-frequency ones, and SPAN [17] generates augmentations by maximizing the spectral change. Besides, SFA [46] analyzes the spectrum of node representations, which is out of the scope of this work. In general, spectral views of graphs may have better performance and interpretability than spatial views but also suffer from high complexity.

Spectral Encoder.Perturbing the graph spectrum is time-consuming. Another approach is to encode the eigenvalues and eigenvectors instead of the eigenspaces. However, eigenvectors suffer from sign and basis ambiguity issues, and using these features directly will affect the stability of the model. Therefore, some spectral encoders are proposed to learn invariant representations from the non-unique spectral features. For example, SAN [15] uses a Transformer-based [34] encoder, which is invariant to the order of the bases. BasisNet [16] uses IGN [21] to learn permutation-invariant representations from the eigenspaces. PEG [37] leverages the distance between eigenvectors to reweigh the graph structure and avoids sign and basis ambiguity. However, the complexity of SAN and BasisNet is quadratic, which is difficult to scale to large graphs.

## 3 Preliminaries

Assume that \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) is a graph, where \(\mathcal{V}\) is the node set with \(|\mathcal{V}|=N\) and \(\mathcal{E}\) is the edge set with \(|\mathcal{E}|=E\). Let \(\mathbf{A}\in\{0,1\}^{N\times N}\) be the adjacency matrix, and \(\mathbf{X}\in\mathbb{R}^{N\times d}\) be the node feature matrix on \(\mathcal{G}\). The normalized graph Laplacian \(\mathbf{L}\) of \(\mathcal{G}\) is defined as \(\mathbf{L}=\mathbf{I}_{n}-\mathbf{D}^{-\frac{1}{2}}\mathbf{AD}^{-\frac{1}{2}}\), where \(\mathbf{I}_{n}\) is the \(N\times N\) identity matrix and \(\mathbf{D}\) is the degree matrix with \(D_{ii}=\sum_{j}A_{ij}\) for \(i\in\mathcal{V}\) and \(D_{ij}=0\) for \(i\neq j\). The eigenvalue decomposition (EVD) of graph Laplacian is defined as \(\mathbf{L}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{\top}\), where \(\mathbf{\Lambda}\)is a diagonal matrix whose diagonal entries \(0\leq\lambda_{1}\leq\cdots\leq\lambda_{N}\leq 2\) are the eigenvalues of \(\mathbf{L}\), and \(\mathbf{U}=[\mathbf{u}_{1},\cdots,\mathbf{u}_{N}]\) are the corresponding eigenvectors.

It is worth noting that in some cases, randomly flipping the signs and rotating the coordinates of eigenvectors may also satisfy EVD [24], which we refer to as sign and basis ambiguity.

Sign ambiguity.Given a pair of eigenvalue and eigenvector \((\lambda_{i},\mathbf{u}_{i})\), it satisfies \(\mathbf{Lu}_{i}=\lambda_{i}\mathbf{u}_{i}\), and \(\lambda_{i}=\mathbf{u}_{i}^{\top}\mathbf{Lu}_{i}=\sum_{(v,v^{\prime})\in \mathcal{E}}(u_{iv}-u_{iv^{\prime}})^{2}\). Therefore, if \(\mathbf{u}_{i}\) is an eigenvector of \(\mathbf{L}\), then \(-\mathbf{u}_{i}\) also satisfies EVD, _i.e._, \(\mathbf{u}_{i}^{\top}\mathbf{Lu}_{i}=(-\mathbf{u}_{i})^{\top}\mathbf{L}(- \mathbf{u}_{i})\).

Basis ambiguity.If there is high multiplicity in the eigenvalues, _i.e._, \(\lambda_{p+1}=\cdots=\lambda_{p+q}\) for some \(q>1\), then the corresponding eigenvectors \([\mathbf{u}_{p+1},\cdots,\mathbf{u}_{p+q}]\) lie in an orthogonal group O(\(q\)) \(=\{\mathbf{Q}\in\mathbb{R}^{q\times q}|\mathbf{Q}^{\top}\mathbf{Q}=\mathbf{QQ}^ {\top}=\mathbf{I}_{q}\}\). Therefore, for any \(\mathbf{Q}\in\) O(\(q\)), replacing \([\mathbf{u}_{p+1},\cdots,\mathbf{u}_{p+q}]\) with \([\mathbf{u}_{p+1},\cdots,\mathbf{u}_{p+q}]\mathbf{Q}\) also satisfies EVD, _i.e._, \(\mathbf{Lu}_{i}=\lambda_{j}\mathbf{u}_{i},p+1\leq i,j\leq p+q\).

The above two facts state that the eigenvectors of graph Laplacian are not unique. Therefore, the model should consider how to learn sign- and basis-invariant representations from spectral features for better stability and generalization [37; 16; 15].

## 4 Proposed Framework: Sp\({}^{2}\)GCL

In this section, we present the proposed spatial-spectral GCL framework called Sp\({}^{2}\)GCL. We first give a high-level overview on how to represent and contrast the spatial and spectral views of a graph. We then introduce the proposed invariant and equivariant spectral view encoder in detail. Finally, we briefly describe the preprocessing, training, and inference processes.

### Overview of Sp\({}^{2}\)GCL

Views refer to different perspectives of the same data [30]. Unlike previous GCL methods that generate graph views in a single domain, we propose to model the spatial and spectral views separately and further utilize cross-domain contrasts to capture invariant information. Here we first describe how to represent the spatial and spectral views of graphs.

**Spatial View** represents the explicit connectivity of nodes, which can be denoted as \(\mathbb{V}_{a}=(\mathbf{A},\mathbf{X})\). Through propagating node features along graph substructures, the spatial view can naturally fuse the topology and content information and learn _local smooth representations_ of a graph.

**Spectral View** indicates the implicit relationships between nodes, which is expressed as \(\mathbb{V}_{e}=(\mathbf{\Lambda},\mathbf{U})\). The eigenvalues and eigenvectors encode the geometric information and node positions of the graph topology, which can be seen as _global structural information_ of a graph.

Since graphs are non-Euclidean data, it is difficult to directly contrast the spatial and spectral views. Therefore, we need to design suitable encoders to learn different view representations for GCL:

\[\mathbf{H}_{a}=f(\mathbf{A},\mathbf{X}),\quad\mathbf{H}_{e}=g(\mathbf{\Lambda },\mathbf{U}), \tag{1}\]

where \(f\) and \(g\) are the encoders of the spatial and spectral views, respectively, and \(\mathbf{H}_{a},\mathbf{H}_{e}\in\mathbb{R}^{N\times d}\) are the spatial and spectral representation matrices, respectively. These representations can then be used in contrastive learning to learn invariant information across both domains.

The fundamental idea of contrastive learning is to define the positive and negative pairs, from which the model can capture the self-supervised signals. In our framework, we define the spatial and spectral representations of the same node or graph as positive pairs, and those of different nodes or graphs as negative pairs. For graph-level contrasts, we additionally use a readout function to learn the graph-level representations. Then, two projection heads \(\varphi_{a},\varphi_{e}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) are used to transform the representations into the contrastive space:

\[\mathbf{Z}_{a}=\varphi_{a}(\mathbf{H}_{a}),\quad\mathbf{Z}_{e}=\varphi_{e}( \mathbf{H}_{e}). \tag{2}\]

Subsequently, we employ InfoNCE [33], a classical contrastive objective function, to maximize the agreements between spatial and spectral representations:

\[\mathcal{L}=-\frac{1}{2}\sum_{i=1}^{N}\left(\log\frac{e^{\langle\mathbf{z}_{a }^{i},\mathbf{z}_{e}^{i}\rangle}}{e^{\langle\mathbf{z}_{a}^{i},\mathbf{z}_{e}^ {i}\rangle}+\sum_{j\neq i}e^{\langle\mathbf{z}_{a}^{i},\mathbf{z}_{e}^{i} \rangle}}+\log\frac{e^{\langle\mathbf{z}_{e}^{i},\mathbf{z}_{e}^{i}\rangle}}{e^ {\langle\mathbf{z}_{e}^{i},\mathbf{z}_{e}^{i}\rangle}+\sum_{j\neq i}e^{ \langle\mathbf{z}_{e}^{i},\mathbf{z}_{e}^{i}\rangle}}\right), \tag{3}\]where \(\langle\cdot,\cdot\rangle\) represents the cosine similarity and \(i\) is the index of nodes or graphs.

Finally, to materialize our contrastive framework \(\text{Sp}^{2}\text{GCL}\), we must choose or design the encoders \(f\) and \(g\) for the two views. For the spatial encoder \(f\), we directly employ a standard message-passing neural network (MPNN), _e.g._, GraphSAGE [7] or GIN [39], which is widely adopted in previous GCL methods. For the spectral encoder \(g\), which is the emphasis of this work, we propose EigenMLP in the next part (Section 4.2) based on several key properties. Note that, on the one hand, although MPNNs can learn useful representations, their expressive power is bounded by the 1-Weisfeiler-Lehman (WL) test [39]. On the other hand, the spectral view encodes the global information, which can help overcome the limitation of the 1-WL test [5]. Hence, from the perspective of expressive power, the spectral view is also complementary to the spatial view.

### Proposed Spectral Encoder: EigenMLP

The spectral encoder is designed to learn stable representations from spectral features. A desirable spectral encoder should have three properties: (1) It can encode both the information of eigenvalues and eigenvectors, which represent different structural information [15]; (2) It is free of the sign and basis ambiguity issues in spectral features, toward learning stable representations [37]; (3) It is scalable to large graphs and has linear or sublinear time complexity. These three properties pose great challenges to the design of spectral encoders. Here we propose EigenMLP, which adopts an MLP-based architecture for scalability and addresses the sign and basis ambiguity problems for stability. Table 1 summarizes the differences between the proposed EigenMLP and existing spectral encoders. Specifically, EigenMLP satisfies all of the three properties above, as we elaborate below.

Scalability.In general, MLPs have demonstrated good scalability and have been widely used in learning graph representations [45, 11]. Since EigenMLP employs an MLP-based architecture, it inherits the low complexity and is linear to the number of nodes \(N\).

Stability.MLPs are sensitive to input data, making the outputs vary with respect to the flipping of signs and rotation of coordinates of eigenvectors. This motivates us to increase the stability of MLP while maintaining high efficiency. We adopt two design principles that enable MLPs to learn sign- and basis-invariant representations. Figure 1 illustrates the difference between EigenMLP and MLP.

To address the sign ambiguity issue, we follow the design of SignNet [16], taking both positive and negative eigenvectors as inputs:

\[\tilde{\mathbf{U}}=[\psi(\phi(\mathbf{u}_{i})+\phi(-\mathbf{u}_{i}))]_{i=1}^{ N}, \tag{4}\]

where \(\psi\) and \(\phi\) are neural networks, \([\cdot]\) is the concatenation operator, and \(\tilde{\mathbf{U}}\) is the sign-invariant eigenvectors. In practice, the sign-invariant neural networks may slow down model convergence. In this case, we can resort to some heuristics to determine the sign of the eigenvectors [47].

To solve the basis ambiguity issue, we first observe that each eigenvector has a corresponding eigenvalue, and when the coordinates of the eigenvectors are rotated, the positions of the eigenvalues are also displaced. Therefore, if we replace the weights of MLP with eigenvalues \(\mathbf{\lambda}=[\lambda_{1},\lambda_{2},\cdots,\lambda_{N}]\), the model will be invariant to the rotation of eigenvectors, _i.e._, \(\mathbf{UQ}(\mathbf{\lambda}\mathbf{Q})^{\top}=\mathbf{UQQ}^{\top}\mathbf{\lambda}^{ \top}=\mathbf{U}\mathbf{\lambda}^{\top}\). However, directly replacing the learnable weights with fixed eigenvalues is trivial and will greatly

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{2}{c}{Information} & \multicolumn{2}{c}{Stability} & \multicolumn{2}{c}{Scalability} \\ \cline{2-7}  & EigVal & EigVec & Sign & Basis & Inductive & Complexity \\ \hline MLP [3, 4] & & & & & & \(\checkmark\) & \(\mathcal{O}(Nkd)\) \\ SAN [15] & & & & & & \(\checkmark\) & \(\mathcal{O}(Nk^{2}d+Nkd)\) \\ BasisNet [16] & & & & & \(\checkmark\) & \(\checkmark\) & \(\mathcal{O}(Nk^{2}d+Nkd)\) \\ PEG [37] & & & & & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\mathcal{O}(k^{2}d)\) \\ \hline EigenMLP & & & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison between different spectral encoders.

limit the expressive power of the model. Therefore, we first extend the scalar eigenvalues to their high-dimensional Fourier features [28]. The weights of EigenMLP are then decoded from the Fourier features using a learnable matrix:

\[\rho(\lambda)=[\sin(\lambda),\cos(\lambda),\sin(2\lambda),\cos(2\lambda),\cdots \sin(T\lambda),\cos(T\lambda)]\cdot\mathbf{W}_{\rho}, \tag{5}\]

where \(T\) is the period and \(\mathbf{W}_{\rho}\in\mathbb{R}^{2T\times d}\) is a learnable matrix. Here \(\rho(\lambda)\) can be seen as a graph filter and the filtered eigenvalues are equivariant to the coordinates of eigenvectors [2], which can be used to learn powerful and basis-invariant spectral representations:

\[g(\mathbf{\Lambda},\mathbf{U})=\tilde{\mathbf{U}}\cdot\rho(\mathbf{\lambda}), \tag{6}\]

where \(\rho(\mathbf{\lambda})=[\rho(\lambda_{1}),\rho(\lambda_{2}),\cdots,\rho(\lambda_{ N})]^{\top}\in\mathbb{R}^{N\times d}\) represents all the filtered eigenvalues. The detailed matrix form of Equation (6) is provided in Appendix C. Note that the learnable matrix \(\mathbf{W}_{\rho}\) is shared between different eigenvalues. Therefore, the size of \(\mathbf{W}_{\rho}\) is independent of the number of eigenvalues but depends on the period \(T\), which reduces the number of parameters in EigenMLP. More discussions can be found in Section 5.2.

Information.Notably, the invariant layer, _i.e._, Equation (6), in EigenMLP incorporates both eigenvalues and eigenvectors, which can capture both geometric and positional information.

### Preprocessing, Training, and Inference

It is worth noting that running EVD for full eigenvectors has the complexity \(\mathcal{O}(N^{3})\), which is unacceptable for large graphs. Therefore, in the preprocessing, we use the eigenvectors with smallest-\(k\) eigenvalues as a substitute and reduce the complexity to \(\mathcal{O}(N^{2}k)\). Besides, we can pre-calculate the rotation-invariant spectral features \(\mathbf{V}\cdot\rho(\mathbf{\lambda}_{k})\). Therefore, in the training and inference, EigenMLP has the implementation of MLP. Note that the decomposition is only computed once in the training. Therefore, the overhead of the preprocessing should be amortized by each training epoch.

In training, traditional GCL methods need to compute the message-passing twice for different graph views, whose complexity is \(\mathcal{O}(E)\). While in our framework, only the spatial view needs to calculate the message-passing, and the complexity of spectral view is \(\tilde{\mathcal{O}}(N)\). Therefore, our framework is faster than others in the training processing. In inference, because we need to calculate the spatial and spectral representations separately, the inference speed is slightly lower than traditional methods. The overheads of training and inference are provided in Section 6.6.

## 5 Deeper Insights

In this section, we provide deeper insights into EigenMLP to understand its effectiveness. Specifically, we prove that EigenMLP is invariant, equivariant, and stable, and can generalize existing spectral augmentations. Besides, we also comment on the connections to previous work.

### Theoretical Results

**Theorem 1**.: _EigenMLP is equivariant to permutation, and invariant to rotation, and reflection._

Proof.: (Permutation) Assume that there are two matrices \(\mathbf{L}^{(1)},\mathbf{L}^{(2)}\in\mathbb{R}^{N\times N}\), and \(\mathbf{L}^{(1)}=\mathbf{PL}^{(2)}\mathbf{P}^{\top}\). We have \(\mathbf{L}^{(1)}=\mathbf{PL}^{(2)}\mathbf{P}^{\top}=(\mathbf{PU}^{(2)})\mathbf{ \Lambda}(\mathbf{PU}^{(2)})^{\top}\) such that \(\mathbf{U}^{(1)}\rho(\mathbf{\lambda})=\mathbf{PU}^{(2)}\rho(\mathbf{\lambda})\).

(Rotation) For any rotation matrix \(\mathbf{Q}\in\mathbf{O}(N)\), the eigenvectors are rotated as \(\mathbf{UQ}\), and the corresponding Fourier matrix are permuted as \(\mathbf{Q}^{\top}\rho(\mathbf{\lambda})\). Therefore, we have \(\mathbf{UQ}\cdot\mathbf{Q}^{\top}\rho(\mathbf{\lambda})=\mathbf{U}\rho(\mathbf{\lambda})\).

(Reflection) Following Proposition 1 in [16], a continuous function is sign-invariant iff it satisfies \(f(v)=h(v)+h(-v)\). Hence, EigenMLP is invariant to reflection on the signs of eigenvectors. 

**Lemma 1**.: _(Lemma 3.4 in [37]) For any positive semidefinite \(\mathbf{L}\) without multiple eigenvalues, set \(\mathbf{V}\) as the eigenvectors corresponding to the smallest \(k\) eigenvalues and sorted as \(0<\lambda_{1}<\cdots<\lambda_{k}\). For any sufficiently small \(\epsilon>0\), there exists a perturbation \(\Delta\mathbf{L}\), \(\|\Delta\mathbf{L}\|_{F}<\epsilon\) such that_

\[\min_{\mathbf{Q}\in\mathbf{O}(k)}\|(\mathbf{V}+\Delta\mathbf{V})-\mathbf{VQ} \|_{F}\geq 0.99\max_{1\leq i\leq k}|\lambda_{i+1}-\lambda_{i}|^{-1}||\Delta \mathbf{L}||_{F}+o(\epsilon). \tag{7}\]

**Theorem 2**.: _For any sufficiently small \(\epsilon>0\), there exists a perturbation \(\Delta\mathbf{L}\), \(\|\Delta\mathbf{L}\|_{F}<\epsilon\) such that_

\[\forall_{\mathbf{Q}\in\mathbf{O}(k)}\,||(\mathbf{V}+\Delta\mathbf{V})\rho( \boldsymbol{\lambda}_{k})-\mathbf{V}\rho(\boldsymbol{\lambda}_{k})||_{F}\leq 0.99T \max_{1\leq i\leq k}|\lambda_{i+1}-\lambda_{i}|^{-1}||\Delta\mathbf{L}||_{F}+o( \epsilon). \tag{8}\]

Lemma 1 states that a small structural perturbation will produce changes unbounded from above in non-equivariant spectral features \(\mathbf{V}\) if there is a small spectral gap. Theorem 2 shows that there is a finite upper bound on the changes of equivariant spectral features \(\mathbf{V}\rho(\boldsymbol{\lambda}_{k})\). Therefore, EigenMLP can learn more stable representations against structural perturbations. We present the proof of Theorem 2 in Appendix A. Experiments in Section 6.5 further give an empirical justification of Theorem 2.

**Proposition 1**.: _EigenMLP generalizes existing spectral augmentations._

Proof.: Existing spectral augmentations, such as SpCo [18] and SPAN [17], generate graph views by perturbing the graph spectrum. We assume that there is a continuous univariate function \(\delta(\cdot)\) between the original eigenvalues \(\boldsymbol{\lambda}\) and the perturbed eigenvalues \(\boldsymbol{\lambda}^{\prime}\), such that \(\lambda^{\prime}_{i}=\delta(\lambda_{i})\), and the perturbed eigenvalues are still non-negative. Then these spectral augmentations can be represented as:

\[\mathbf{A}^{\prime}=\mathbf{U}\delta(\boldsymbol{\Lambda})\mathbf{U}^{\top}=( \mathbf{U}\delta(\boldsymbol{\Lambda})^{\frac{1}{2}})(\mathbf{U}\delta( \boldsymbol{\Lambda})^{\frac{1}{2}})^{\top}. \tag{9}\]

Note that Equation (5) is a Fourier series and can approximate any continuous functions. Therefore, the function \(\delta(\cdot)\) is a special case of \(\rho(\cdot)\), and EigenMLP can approximate the simplex geometry [32] of the augmentations, _i.e._, \(\mathbf{U}\delta(\boldsymbol{\Lambda})^{\frac{1}{2}}\), thus generalizing existing spectral augmentations. 

### Connections to Existing Work

**Hypernetworks**[6] are a class of neural networks used to generate parameters for another network. EigenMLP can be seen as a special case of Hypernetworks, which takes eigenvalues as input and generates equivariant parameters for spectral features. Because Hypernetworks can generate a set of non-shared weights from shared parameters, it can greatly reduce the number of trainable parameters. EigenMLP inherits this advantage, and its number of parameters is only related to the period of the Fourier series, rather than the number of eigenvectors. Therefore, EigenMLP is more efficient than other spectral encoders, including vanilla MLP.

**Spectral GNNs** aim to combine the eigenspaces with filtered eigenvalues, _i.e._, \(\sum_{i=1}^{N}\delta(\lambda_{i})\mathbf{u}_{i}\mathbf{u}_{i}^{\top}\), and EigenMLP is used to combine the eigenvectors with filtered eigenvalues, _i.e._, \(\sum_{i=1}^{N}\rho(\lambda_{i})\mathbf{u}_{i}\). Therefore, both methods choose to use eigenvalue mappings as weights to guarantee the equivariance. However, the calculation of eigenspaces has the complexity of \(\mathcal{O}(N^{2})\), which is not suitable for large graphs, whereas EigenMLP is more scalable.

## 6 Experiments

In this section, we conduct three types of experiments, including unsupervised node classification, unsupervised graph prediction, and transfer learning, to verify the effectiveness of Sp\({}^{2}\)GCL. Besides, we also test the stability and time overhead of the proposed method.

### Unsupervised Node Classification

**Datasets.** In the node classification task, we consider using graphs with different scales to evaluate both the effectiveness and scalability of GCL methods. Specifically, for the small graphs (< 50,000), we use Pubmed [14], Wiki-CS [22], and Facebook [23] datasets. For the large graphs (> 50,000), we use Flickr [43], arXiv [9], and PPI [7] datasets. Additional statistics are provided in Appendix B.

**Baselines and Setting.** We compare our model against a wide range of baselines, including semi-supervised GNNs, _e.g._, GCN [14] and GAT [35], graph self-supervised learning methods, _e.g._, DGI [36] and BGRL [29], GCL with spatial augmentations, _e.g._, MVGRL [8], GRACE [48], and CCA-SSG [44], and GCL with spectral augmentations, _e.g._, SpCo [18] and SPAN [17]. For the Facebook dataset, we randomly split the nodes into train/validation/test data with a ratio of 1:1:8. For other datasets, we use the public splits for a fair comparison. We use a two-layer GCN as the encoder forall datasets and set the hidden dimension \(d=512\) for all methods. For our model, the spatial encoder is the same as baselines, and we additionally use EigenMLP to learn the spectral representation. In the evaluation, we use a linear classifier to evaluate the performance of all methods, as suggested by [27]. We run all the models 10 times and report the mean accuracy and standard deviation. More details, _e.g._, optimizers, and hyperparameters, are provided in Appendix B.

Results.From Table 2, we can find that Sp\({}^{2}\)GCL consistently outperforms state-of-the-art baselines on 5 out of 6 datasets, which validates the effectiveness of the proposed spatial-spectral contrastive framework. Meanwhile, spectral-based methods are proven to be more effective than spatial-based methods, suggesting that integrating spectral information into GCL can help models learn better representations. However, it is worth noting that neither SpCo nor SPAN works for large graphs, implying that perturbing graph spectrum cannot be scalable to large-scale datasets. Therefore, the application scenarios of these two graph augmentations are limited. On the contrary, our method can be used for large graphs and can be easily trained in a mini-batch manner, which is more scalable than other spectral-based methods. Additionally, We find that Sp\({}^{2}\)GCL does not perform well in the Wiki-CS dataset. The reason is that the node features dominate the classification results while the graph structure contributes less. Therefore, the spectral view cannot complement the spatial view.

### Unsupervised Graph Prediction

Setup.We benchmark our model on the OGB graph property prediction task [9], which contains three regression datasets and five classification datasets. We choose a series of competitive GCL methods as baselines, including InfoGraph [26], GraphCL [42], MVGRL [8], JOAO [41], AD-GCL [27], and SPAN [17]. It is worth noting that SpCo [18] is not designed for graph-level contrastive learning, so we do not compare with it. We use a five-layer GIN [39] with a graph pooling layer as the encoder for all methods. Similarly, we additionally use EigenMLP to encode the spectral view for Sp\({}^{2}\)GCL. We use a linear downstream classifier, _e.g._, logistic regression model, to evaluate the performance of different GCL methods, as suggested by [27].

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Data} & \multicolumn{3}{c}{Small Graphs (Full-Batch)} & \multicolumn{3}{c}{Large Graphs (Mini-Batch)} \\ \cline{3-8}  & & PubMed & Wiki-CS & Facebook & arXiv & Flickr & PPI \\ \hline GCN & \(\mathbf{A},\mathbf{X},\mathbf{Y}\) & 79.0 & 77.19±0.12 & 90.65±0.16 & 71.74±0.29 & 49.20±0.31 & 82.28±0.24 \\ GAT & \(\mathbf{A},\mathbf{X},\mathbf{Y}\) & 79.0±0.3 & 77.65±0.11 & 90.47±0.15 & 71.82±0.23 & 54.48±0.21 & 98.85±0.05 \\ \hline DGI & \(\mathbf{A},\mathbf{X}\) & 76.8±0.6 & 75.35±0.14 & 84.42±0.43 & 70.32±0.25 & 50.59±0.28 & 63.80±0.20 \\ BGRL & \(\mathbf{A},\mathbf{X}\) & 79.6±0.5 & 79.98±0.13 & 89.71±0.35 & 71.54±0.17 & 51.87±0.15 & 73.63±0.16 \\ MVGRL & \(\mathbf{A},\mathbf{X}\) & 80.1±0.7 & 77.52±0.08 & 87.29±0.28 & - & - & 71.45±0.14 \\ GRACE & \(\mathbf{A},\mathbf{X}\) & 80.6±0.4 & 80.14±0.48 & 89.32±0.40 & - & - & 69.71±0.17 \\ CCA-SSG & \(\mathbf{A},\mathbf{X}\) & 81.0±0.4 & 78.85±0.32 & 89.45±0.60 & 71.21±0.20 & 51.66±0.10 & 73.34±0.17 \\ \hline SpCo & \(\mathbf{A},\mathbf{X},\mathbf{\Lambda}\) & 81.5±0.4 & 79.16±0.27 & 89.98±0.45 & - & - & - \\ SPAN & \(\mathbf{A},\mathbf{X},\mathbf{\Lambda}\) & 81.5±0.2 & **82.13±0.15** & - & - & - & - \\ \hline Sp\({}^{2}\)GCL & \(\mathbf{A},\mathbf{X},\mathbf{\Lambda},\mathbf{U}\) & **82.3±0.3** & 79.42±0.19 & **90.43±0.13** & **71.83±0.19** & **52.05±0.33** & **74.28±0.22** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Node classification on transductive and inductive graphs. Mean accuracy (%) \(\pm\) standard deviation. Bold indicates the best performance and “-” means out-of-memory or cannot be reproduced.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline Task & \multicolumn{3}{c}{Regression (Metric: RMSE \(\downarrow\))} & \multicolumn{3}{c}{Classification (Metric: ROC-AUC\% \(\uparrow\))} \\ \cline{2-9} Dataset & molesol & mollipo & molfreesolv & molluce & mollobpp & molclintox & molcox21 & molsider \\ \hline Supervised & 1.173±0.057 & 0.757±0.018 & 2.755±0.349 & 72.97±4.00 & 68.17±1.48 & 88.14±2.51 & 74.91±0.51 & 57.60±1.40 \\ \hline InfoGraph & 1.34±0.178 & 1.005±0.023 & 1.000±0.819 & 74.74±3.64 & 66.33±2.79 & 64.50±3.52 & 69.74±0.57 & 60.54±0.90 \\ Grapbell. & 1.272±0.089 & 0.910±0.016 & 7.697±2.48 & 74.32±2.70 & 68.22±1.89 & 74.92±4.47 & 72.40±1.01 & 61.76±1.11 \\ MVGRL & 1.433±0.145 & 0.962±0.036 & 9.042±1.982 & 74.20±2.31 & 67.24±1.39 & 73.84±2.45 & 70.48±0.83 & 61.94±0.94 \\ JOAO & 1.285±0.121 & 0.865±0.032 & 5.131±0.722 & 74.43±1.94 & 67.62±1.29 & 78.72±1.42 & 71.83±0.92 & 62.73±0.92 \\ AD-GCL & **1.271±0.087** & 0.842±0.028 & 5.150±0.624 & 76.37±2.03 & 68.21±4.47 & 80.77±3.92 & 71.42±0.73 & 63.91±0.95 \\ SPAN & 1.218±0.052 & **0.802±0.019** & 4.531±0.463 & 76.74±2.02 & **69.59±1.34** & 80.28±2.42 & 72.83±0.62 & **64.87±0.88** \\ \hline Sp\({}^{2}\)GCL & 1.235±0.119 & 0.835±0.026 & **4.144±0.573** & **78.76±1.43** & 68.72±1.53 & **80.88±3.36** & **73.06±0.75** & 64.23±0.96 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results on the graph-level tasks. The best and runner-up results are highlighted with **bold** and underline, respectively. \(\downarrow\) means lower the better, and \(\uparrow\) means higher the better.

[MISSING_PAGE_FAIL:8]

GRACE. Nevertheless, the performance is still inferior to that of Sp\({}^{2}\)GCL, thus providing further evidence of the effectiveness of spatial-spectral contrast.

### Stability of EigenMLP and MLP

We conduct stability experiments to evaluate whether EigenMLP and MLP can learn stable representations against perturbations. We consider two types of perturbations: 1) Synthetic perturbation, which applies random reflection \(\mathbf{\Pi}\) and rotations \(\mathbf{Q}\) on the eigenvectors. 2) Practical perturbation, which decomposes the Laplacian matrix with different tolerances (\(10^{-3},10^{-4},10^{-5}\)). The corresponding eigenvectors are expressed as \(\mathbf{U}_{3}\), \(\mathbf{U}_{4}\), and \(\mathbf{U}_{5}\). Note that synthetic perturbation only changes the signs and coordinates of eigenvectors while practical perturbation is more challenging because it perturbs the values. For clearer results, we only evaluate the performance of spectral representations.

For each type of perturbation, we construct three instances, _i.e._, (\(\mathbf{U}\), \(\mathbf{U}\)**II**, \(\mathbf{U}\)**Q**) or (\(\mathbf{U}_{3}\), \(\mathbf{U}_{4}\), \(\mathbf{U}_{5}\)). The models are trained on one instance and tested on the other two instances on PubMed. We report the results of EigenMLP and show the performance change after switching to MLP in the brackets. From Table 7, we can find that the synthetic perturbation can hardly change the representations learned by EigenMLP but has a great influence on MLP. Additionally, Table 8 shows that practical perturbations can affect both EigenMLP and MLP. Nevertheless, EigenMLP still consistently outperforms MLP across different tolerances, which verifies the stability of EigenMLP.

### Time Overhead

To assess the efficiency, we compare the time overhead of Sp\({}^{2}\)GCL with other GCL methods as well as compare EigenMLP with different spectral encoders. The time costs associated with preprocessing, training, and inference are shown in Table 9. Previous spectrum-based methods have significant time costs in the preprocessing stage, _e.g._, graph diffusion (GD) or Sinkhorn's Iteration (SI), whereas Sp\({}^{2}\)GCL exhibits minimal overhead. In the training stage, Sp\({}^{2}\)GCL outperforms other methods due to the efficient spectral encoding, but it also introduces an additional burden during inference. Table 10 illustrates the overhead imposed by various spectral encoders within the Sp\({}^{2}\)GCL framework. Both MLP and EigenMLP exhibit remarkable efficiency. Conversely, SAN and BasisNet entail excessive time costs due to their quadratic time complexity.

## 7 Conclusion

In this study, we introduce Sp\({}^{2}\)GCL, a novel spatial-spectral GCL framework that learns the consistency between the spatial and spectral views of graphs. To effectively and efficiently learn the spectral view information, we propose EigenMLP, a scalable spectral encoder to learn stable spectral representations from the non-unique spectral features. Extensive experiments on various graph-related tasks demonstrate the effectiveness, efficiency, and stability of the proposed method.

\begin{table}
\begin{tabular}{l c|r r r} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{Test} \\ \cline{2-5}  & & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \cline{3-5}  & & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline \multirow{2}{*}{Method} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\  & & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\  & & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \cline{3-5}  & & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline \hline \end{tabular}
\end{table}
Table 8: Practical perturbations: Eigenvectors with different EVD tolerances.

\begin{table}
\begin{tabular}{l|r r r} \hline \hline \multirow{2}{*}{PubMed} & \multicolumn{4}{c}{Test} \\ \cline{2-4}  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \cline{3-5}  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline \multirow{2}{*}{**GCL**} & **U** & 78.8 (40.1) & 78.3 (-5.5) & 78.8 (-6.4) \\  & **UII** & 78.5 (-7.8) & 78.9 (-0.4) & 79.0 (-5.2) \\  & **UQ** & 78.7 (-5.1) & 78.8 (-9.3) & 78.9 (+0.0) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Synthetic perturbations: Eigenvectors with random reflection \(\mathbf{\Pi}\) and rotation \(\mathbf{Q}\).

\begin{table}
\begin{tabular}{l r r r} \hline \hline \multirow{2}{*}{PubMed} & \multicolumn{4}{c}{Test} \\ \cline{2-4}  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \cline{3-5}  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline \multirow{2}{*}{**GCL**} & **U** & **V** & **78.6 (+0.3) & 73.9 (-1.5) & 73.7 (-7.8) \\  & **U4** & 75.1 (-3.2) & 78.2 (+0.3) & 72.8 (-6.8) \\  & **U5** & 73.7 (-6.9) & 72.0 (-5.0) & 79.7 (-0.5) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Time overheads (s) of different GCL methods.

Limitation and Broader ImpactCurrently, we focus on encoding spectral features while ignoring node features. A promising future direction is to unify these two kinds of information to learn effective graph representations. Our work reveals the superiority of integrating spectral information into GCL and may inspire the community to pay more attention to the spectral view of graphs.

## Acknowledgments and Disclosure of Funding

This work is supported in part by the National Natural Science Foundation of China (No. U20B2045, 62192784, U22B2038, 62002029, 62172052). This work is also partially supported by the Ministry of Education, Singapore, under its Academic Research Fund Tier 2 (Proposal ID: T2EP20122-0041). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of the Ministry of Education, Singapore.

## References

* [1] Philip Bachman, R. Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In _NeurIPS_, pages 15509-15519, 2019.
* [2] Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In _NeurIPS_, pages 3837-3845, 2016.
* [3] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. _CoRR_, abs/2012.09699, 2020.
* [4] Vijay Prakash Dwivedi, Chaitanya K. Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. _CoRR_, abs/2003.00982, 2020.
* [5] Or Feldman, Amit Boyarski, Shai Feldman, Dani Kogan, Avi Mendelson, and Chaim Baskin. Weisfeiler and leman go infinite: Spectral and combinatorial pre-colorings. _CoRR_, abs/2201.13410, 2022.
* [6] David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In _ICLR (Poster)_. OpenReview.net, 2017.
* [7] William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _NIPS_, pages 1024-1034, 2017.
* [8] Kaveh Hassani and Amir Hosein Khas Ahmadi. Contrastive multi-view representation learning on graphs. In _ICML_, volume 119 of _Proceedings of Machine Learning Research_, pages 4116-4126. PMLR, 2020.
* [9] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In _NeurIPS_, 2020.
* [10] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In _ICLR_. OpenReview.net, 2020.
* [11] Yang Hu, Haoxuan You, Zhecan Wang, Zhicheng Wang, Erjin Zhou, and Yue Gao. Graph-mlp: Node classification without message passing in graph. _CoRR_, abs/2106.04051, 2021.
* [12] Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, and Jiliang Tang. Self-supervised learning on graphs: Deep insights and new direction. _CoRR_, abs/2006.10141, 2020.
* [13] Mark Kac. Can one hear the shape of a drum? _The american mathematical monthly_, 73(4P2):1-23, 1966.
* [14] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _ICLR_, 2017.

* [15] Devin Kreuzer, Dominique Beaini, William L. Hamilton, Vincent Letourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. In _NeurIPS_, pages 21618-21629, 2021.
* [16] Derek Lim, Joshua Robinson, Lingxiao Zhao, Tess E. Smidt, Suvrit Sra, Haggai Maron, and Stefanie Jegelka. Sign and basis invariant networks for spectral graph representation learning. _CoRR_, abs/2202.13013, 2022.
* [17] Lu Lin, Jinghui Chen, and Hongning Wang. Spectral augmentation for self-supervised learning on graphs. In _ICLR_, 2023.
* [18] Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian Pei. Revisiting graph contrastive learning from the perspective of graph spectrum. In _NeurIPS_, 2022.
* [19] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self-supervised learning: Generative or contrastive. _IEEE Trans. Knowl. Data Eng._, 35(1):857-876, 2023.
* [20] Yixin Liu, Shirui Pan, Ming Jin, Chuan Zhou, Feng Xia, and Philip S. Yu. Graph self-supervised learning: A survey. _CoRR_, abs/2103.00111, 2021.
* [21] Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. In _ICLR (Poster)_. OpenReview.net, 2019.
* [22] Peter Mernyei and Catalina Cangea. Wiki-cs: A wikipedia-based benchmark for graph neural networks. _CoRR_, abs/2007.02901, 2020.
* [23] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. _J. Complex Networks_, 9(2), 2021.
* [24] Daniel A. Spielman. Spectral graph theory and its applications. In _FOCS_, pages 29-38. IEEE Computer Society, 2007.
* [25] Balasubramaniam Srinivasan and Bruno Ribeiro. On the equivalence between positional node embeddings and structural graph representations. In _ICLR_. OpenReview.net, 2020.
* [26] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. In _ICLR_. OpenReview.net, 2020.
* [27] Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve graph contrastive learning. In _NeurIPS_, pages 15920-15933, 2021.
* [28] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In _NeurIPS_, 2020.
* [29] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou, Eva L. Dyer, Remi Munos, Petar Velickovic, and Michal Valko. Large-scale representation learning on graphs via bootstrapping. In _ICLR_. OpenReview.net, 2022.
* [30] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In _ECCV_, pages 776-794. Springer, 2020.
* [31] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? In _NeurIPS_, 2020.
* [32] Leo Torres, Kevin S. Chan, and Tina Eliassi-Rad. GLEE: geometric laplacian eigenmap embedding. _J. Complex Networks_, 8(2), 2020.
* [33] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _CoRR_, abs/1807.03748, 2018.

* [34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NIPS_, pages 5998-6008, 2017.
* [35] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _ICLR_, 2018.
* [36] Petar Velickovic, William Fedus, William L. Hamilton, Pietro Lio, Yoshua Bengio, and R. Devon Hjelm. Deep graph infomax. In _ICLR (Poster)_. OpenReview.net, 2019.
* [37] Haorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li. Equivariant and stable positional encoding for more powerful graph neural networks. In _ICLR_. OpenReview.net, 2022.
* [38] Lirong Wu, Haitao Lin, Cheng Tan, Zhangyang Gao, and Stan Z. Li. Self-supervised learning on graphs: Contrastive, generative, or predictive. _IEEE Transactions on Knowledge and Data Engineering_, 2021.
* [39] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _ICLR_. OpenReview.net, 2019.
* [40] Longqi Yang, Liangliang Zhang, and Wenjing Yang. Graph adversarial self-supervised learning. In _NeurIPS_, pages 14887-14899, 2021.
* [41] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In _ICML_, volume 139 of _Proceedings of Machine Learning Research_, pages 12121-12132. PMLR, 2021.
* [42] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. In _NeurIPS_, 2020.
* [43] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. Graphsaint: Graph sampling based inductive learning method. In _ICLR_. OpenReview.net, 2020.
* [44] Hengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S. Yu. From canonical correlation analysis to self-supervised graph neural networks. In _NeurIPS_, pages 76-89, 2021.
* [45] Shichang Zhang, Yozen Liu, Yizhou Sun, and Neil Shah. Graph-less neural networks: Teaching old mlps new tricks via distillation. In _ICLR_. OpenReview.net, 2022.
* [46] Yifei Zhang, Hao Zhu, Zixing Song, Piotr Koniusz, and Irwin King. Spectral feature augmentation for graph contrastive learning and beyond. In _AAAI_, 2023.
* [47] Ziwei Zhang, Peng Cui, Jian Pei, Xin Wang, and Wenwu Zhu. Eigen-gnn: A graph structure preserving plug-in for gnns. _IEEE Trans. Knowl. Data Eng._, 35(3):2544-2555, 2023.
* [48] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive representation learning. _CoRR_, abs/2006.04131, 2020.
* [49] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In _WWW_, pages 2069-2080. ACM / IW3C2, 2021.

## Appendix A Proof of Theorem 2

We use \(i\) to indicate the index of \(\lambda^{-1}=\max_{i\in N}|\lambda_{i+1}-\lambda_{i}|^{-1}\) and perturb graph Laplacian \(\mathbf{L}\) by perturbing the eigenvectors. Specifically, we set

\[\mathbf{u}_{i}^{\prime}= \sqrt{1-\epsilon^{2}}\mathbf{u}_{i}+\epsilon\mathbf{u}_{i+1}, \tag{1}\] \[\mathbf{u}_{i+1}^{\prime}= -\epsilon\mathbf{u}_{i}+\sqrt{1-\epsilon^{2}}\mathbf{u}_{i+1}.\]

Note that \(||\mathbf{u}_{i}^{\prime}||=||\mathbf{u}_{i+1}^{\prime}||=1\) and \(\mathbf{u}_{i}^{\prime\top}\mathbf{u}_{i+1}^{\prime}=0\). Therefore, replacing \(\mathbf{u}_{i}\) and \(\mathbf{u}_{i+1}\) with \(\mathbf{u}_{i}^{\prime}\) and \(\mathbf{u}_{i+1}^{\prime}\) still satisfies EVD. We denote \(\mathbf{V}^{\prime}=[\mathbf{u}_{1},\cdots,\mathbf{u}_{i}^{\prime},\mathbf{u} _{i+1}^{\prime},\cdots,\mathbf{u}_{k}]\). Then the perturbation can be represented as \(\Delta\mathbf{L}=\mathbf{V}^{\prime}\Lambda\mathbf{V}^{\prime\top}-\mathbf{V} \Lambda\mathbf{V}^{\top}=\lambda_{i}(\mathbf{u}_{i}^{\prime}\mathbf{u}_{i}^{ \prime\top}-\mathbf{u}_{i}\mathbf{u}_{i}^{\top})+\lambda_{i+1}(\mathbf{u}_{i+ 1}^{\prime}\mathbf{u}_{i+1}^{\prime}{}^{\top}-\mathbf{u}_{i+1}\mathbf{u}_{i+ 1}^{\top})\).

For sufficient small \(\epsilon>0\), we have:

\[\begin{split}&\min_{\mathbf{Q}\in\mathbf{O}(k)}||(\mathbf{V}+ \Delta\mathbf{V})-\mathbf{V}\mathbf{Q}||_{F}\\ =&||\mathbf{u}_{i}^{\prime}-\mathbf{u}_{i},\mathbf{u} _{i+1}^{\prime}-\mathbf{u}_{i+1}||_{F}\\ =&||(\sqrt{1-\epsilon^{2}}-1)\mathbf{u}_{i}+\epsilon \mathbf{u}_{i+1}||_{F}+||(\sqrt{1-\epsilon^{2}}-1)\mathbf{u}_{i+1}-\epsilon \mathbf{u}_{i}||_{F}\\ =& 4(1-\sqrt{1-\epsilon^{2}})\\ =& 2\epsilon^{2}+o(\epsilon^{2}).\end{split} \tag{2}\]

For Fourier features with period \(\frac{T}{2}\), we have:

\[\begin{split}&\quad\forall_{\mathbf{Q}\in\mathbf{O}(k)}\left||( \mathbf{V}+\Delta\mathbf{V})\rho(\boldsymbol{\lambda}_{k})-\mathbf{V}\rho( \boldsymbol{\lambda}_{k})||_{F}\right.\\ =&||\left[\mathbf{u}_{i}^{\prime}-\mathbf{u}_{i}, \mathbf{u}_{i+1}^{\prime}-\mathbf{u}_{i+1}\right][\rho(\lambda_{i}),\rho( \lambda_{i+1})]^{\top}\left||_{F}\right.\\ =&\sum_{t=1}^{T/2}||\sin(\lambda_{i})(\mathbf{u}_{i }^{\prime}-\mathbf{u}_{i})+\sin(\lambda_{i+1})(\mathbf{u}_{i+1}^{\prime}- \mathbf{u}_{i+1})||_{F}\\ &+\sum_{t=1}^{T/2}||\cos(\lambda_{i})(\mathbf{u}_{i}^{\prime}- \mathbf{u}_{i})+\cos(\lambda_{i+1})(\mathbf{u}_{i+1}^{\prime}-\mathbf{u}_{i+ 1})||_{F}\\ \leq& T\left(||\mathbf{u}_{i}^{\prime}-\mathbf{u}_{i }||_{F}+||\mathbf{u}_{i+1}^{\prime}-\mathbf{u}_{i+1}||_{F}\right)\\ \leq& T(2\epsilon^{2}+o(\epsilon^{2})),\end{split} \tag{3}\]

Next, we characterize \(||\Delta\mathbf{L}||_{F}\):

\[\begin{split}&||\Delta\mathbf{L}||_{F}\\ =&\left\|\lambda_{i}\left(\mathbf{u}_{i}^{\prime} \mathbf{u}_{i}^{\prime\top}-\mathbf{u}_{i}\mathbf{u}_{i}^{\top}\right)+ \lambda_{i+1}\left(\mathbf{u}_{i+1}^{\prime}\mathbf{u}_{i+1}^{\prime}{}^{\top} -\mathbf{u}_{i+1}\mathbf{u}_{i+1}^{\top}\right)\right\|_{F}\\ =&\left\|(\lambda_{i+1}-\lambda_{i})\left[-\epsilon ^{2}\left(\mathbf{u}_{i}\mathbf{u}_{i}^{\top}-\mathbf{u}_{i+1}\mathbf{u}_{i+1 }^{\top}\right)+\epsilon\sqrt{1-\epsilon^{2}}\left(\mathbf{u}_{i}\mathbf{u}_{ i+1}^{\top}+\mathbf{u}_{i+1}\mathbf{u}_{i}^{\top}\right)\right]\right\|_{F}^{2}\\ =&\left(\lambda_{k+1}-\lambda_{k}\right)^{2}\left( \epsilon^{2}\left\|\mathbf{u}_{k}\mathbf{u}_{k+1}^{\top}+\mathbf{u}_{k+1} \mathbf{u}_{k}^{\top}\right\|_{F}^{2}+o\left(\epsilon^{2}\right)\right)\\ =& 2\left(\lambda_{k+1}-\lambda_{k}\right)^{2}\left( \epsilon^{2}+o\left(\epsilon^{2}\right)\right)\end{split} \tag{4}\]

Combining Equations (2) and (4), we have the lower bound of the changes of non-equivariant spectral features under small perturbations:

\[\min_{\mathbf{Q}\in\mathbf{O}(k)}||(\mathbf{V}+\Delta\mathbf{V})-\mathbf{V} \mathbf{Q}||_{F}\geq 0.99\max_{1\leq i\leq k}|\lambda_{i+1}-\lambda_{i}|^{-1}|| \Delta\mathbf{L}||_{F}+o(\epsilon), \tag{5}\]

which concludes the Lemma 1, _i.e._, Lemma 3.4 in [37].

Combining Equations (3) and (4), we have the upper bound of the changes of equivariant spectral features under small perturbations:

\[\forall_{\mathbf{Q}\in\mathbf{O}(k)}\left||(\mathbf{V}+\Delta\mathbf{V})\rho( \boldsymbol{\lambda}_{k})-\mathbf{V}\rho(\boldsymbol{\lambda}_{k})||_{F}\leq 0.99T\max_{1 \leq i\leq k}|\lambda_{i+1}-\lambda_{i}|^{-1}||\Delta\mathbf{L}||_{F}+o( \epsilon), \tag{6}\]

which concludes the Theorem 2.

Detailed Experimental Setup

In this section, we report the details of our experiments. Specifically, we first introduce some general settings in all experiments. Then we introduce the detailed setup of each experiment one by one.

### General Settings

Optimizer.For all experiments, we use the Adam optimizer.

Environment.The environment in which we run experiments is:

* Linux version: 5.19.0-38-generic
* Operating system: Ubuntu 22.04.2
* CPU information: AMD EPYC 7313P 16-Core Processor
* GPU information: GeForce RTX 3090 (24 GB)

Resources.The addresses and licenses of all datasets are as follows:

* PubMed: [https://github.com/tkipf/pygcn](https://github.com/tkipf/pygcn) (MIT License)
* Wiki-CS: [https://github.com/pmernyei/wiki-cs-dataset](https://github.com/pmernyei/wiki-cs-dataset) (MIT License)
* Facebook: [https://github.com/benedekrozemberczki/MUSAE](https://github.com/benedekrozemberczki/MUSAE) (GPL-3.0 license)
* arXiv: [https://github.com/snap-stanford/ogb](https://github.com/snap-stanford/ogb) (MIT license)
* Flickr: [https://github.com/GraphSAINT/GraphSAINT](https://github.com/GraphSAINT/GraphSAINT) (MIT license)
* PPI: [https://github.com/mims-harvard/ohmnet](https://github.com/mims-harvard/ohmnet) (MIT license)
* OGB-graph: [https://github.com/snap-stanford/ogb](https://github.com/snap-stanford/ogb) (MIT license)
* ZINC-2M: [https://github.com/snap-stanford/pretrain-gnns](https://github.com/snap-stanford/pretrain-gnns) (MIT license)

Reproducibility.Our code is attached in the supplementary material.

### Unsupervised Node Classification

Evaluation protocol.In the unsupervised node classification task, all methods are first trained with the corresponding self-supervised learning objectives. Then the learned representations are evaluated with a Logistic classifier with \(l_{2}\) normalization. We evaluate the method every 10 epochs and the maximum epoch is set to 1000. For the mini-batch training, we set the batch size to 1024. The detailed statistics are shown in Table 1 and the hyperparameters are shown in Table 2.

### Unsupervised Graph Prediction

Evaluation protocol.In the unsupervised graph prediction task, we use the stand encoder, provided by OGB 4, as the spatial encoder of Sp2GCL, which is a 5-layer GIN with hidden dimension \(d=300\). We use add pooling to learn graph-level representations and set the batch size to 32. For the spectral

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline  & **Graphs** & **Nodes** & **Edges** & **Features** & **Classes** \\ \hline PubMed & 1 & 19,717 & 88,648 & 500 & 3 \\ Wiki-CS & 1 & 11,701 & 216,123 & 300 & 10 \\ Facebook & 1 & 22,470 & 342,004 & 128 & 4 \\ arXiv & 1 & 169,343 & 1,116,243 & 128 & 40 \\ Flickr & 1 & 89,250 & 899,756 & 500 & 7 \\ PPI & 24 & 56,928 & 1,226,368 & 50 & 121 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Statistics of unsupervised node classification datasets.

encoder, due to the relatively small sizes of the molecular graphs, we use all eigenvectors as the spectral features. We set the learning rate to 0.001 and the period to 10 for all datasets, and the number of training epochs is chosen among {20, 50, 80, 100, 150} using the validation set, as suggested by AD-GCL [27]. For the downstream evaluator, we use a Riger regressor for the regression tasks and a Logistic classifier for the binary classification tasks. The strength of \(l_{2}\) normalization is grid searched among {0.001, 0.01, 0.1, 1, 10, 100, 1000} on the validation set for each dataset. The detailed statistics of the datasets are shown in Table 3.

### Transfer Learning

Evaluation protocol.For the transfer learning task, we use the same GIN encoder as [10]. In the pre-training stage, the learning rate is set to 0.001 and the number of training epochs is chosen from {20, 50, 80, 100} based on the validation set. Similarly, we use all eigenvalues and eigenvectors as the spectral features, and the period is set to 10. In the fine-tuning stage, we remove the self-supervised learning objective, and an additional linear projection layer is used on the output of the encoder for classification. The hyperparameters are the same as in the pre-training stage. The detailed statistics of the datasets are shown in Table 4.

\begin{table}
\begin{tabular}{l l r r r} \hline \hline  & **Graphs** & **Utilization** & **Avg. Nodes** & **Avg. Edges** \\ \hline ZINC-2M & Pre-Training & 2,000,000 & 26.62 & 57.72 \\ BBBP & Finetuning & 2,039 & 24.06 & 51.90 \\ Tox21 & Finetuning & 7,831 & 18.57 & 38.58 \\ SIDER & Finetuning & 1,427 & 33.64 & 70.71 \\ ClinTox & Finetuning & 1,477 & 26.15 & 55.76 \\ BACE & Finetuning & 1,513 & 34.08 & 73.71 \\ HIV & Finetuning & 41,127 & 25.51 & 54.93 \\ MUV & Finetuning & 93,087 & 24.23 & 52.55 \\ ToxCast & Finetuning & 8,576 & 18.78 & 38.52 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Statistics of transfer learning datasets.

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline  & **Graphs** & **Avg. Nodes** & **Avg. Edges** & **Classes** & **Task** & **Metric** \\ \hline ogbg-molesol & 1,128 & 13.3 & 13.7 & 1 & Regression & RMSE \\ ogbg-mollipo & 4,200 & 27.0 & 29.5 & 1 & Regression & RMSE \\ ogbg-molfreesolv & 642 & 8.7 & 8.4 & 1 & Regression & RMSE \\ ogbg-molbbee & 1,513 & 34.1 & 36.9 & 1 & Binary Class. & ROC-AUC \\ ogbg-mollbbbp & 2,039 & 24.1 & 26.0 & 1 & Binary Class. & ROC-AUC \\ ogbg-mollcintox & 1,477 & 26.2 & 27.9 & 2 & Binary Class. & ROC-AUC \\ ogbg-moltox21 & 7,831 & 18.6 & 19.3 & 12 & Binary Class. & ROC-AUC \\ ogbg-molsider & 1,427 & 33.6 & 35.4 & 27 & Binary Class. & ROC-AUC \\ \hline \hline \end{tabular}
\end{table}
Table 3: Statistics of unsupervised graph prediction datasets.

[MISSING_PAGE_FAIL:16]

* #k:numberofEigenvectors #d:hiddendimension #T:period self.phi=nn.Sequential(nn.Linear(1,d),nn.ReLU(),nn.Linear(d,d)) self.psi=nn.Sequential(nn.Linear(d,d),nn.ReLU(),nn.Linear(d,1)) self.mlp=nn.Sequential(nn.Linear(2*T,d),nn.ReLU(),nn.Linear(d,d))

 defforward(e,u): u=u.unsqueeze(-1) u=self.psi(self.phi(u)+self.phi(-u)).squeeze(-1)#[N,k] T_term=torch.arange(0,T).float() T_e=e.unsqueeze(1)*T F_e=torch.cat([torch.sin(T_e),torch.cos(T_e)],dim=-1)#[k,2T] returnself.mlp(torch.mm(u,F_e))

Figure 4: Pseudo Algorithm of EigenMLP

Figure 5: Pseudo Algorithm of Sp\({}^{2}\)GCL