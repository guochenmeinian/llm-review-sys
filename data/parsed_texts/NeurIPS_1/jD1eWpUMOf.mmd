[MISSING_PAGE_FAIL:1]

like negative guidance [] or altering the conditional distribution towards another anchor/surrogate concept [].

Despite notable advancements in the field of concept erasing, fine-tuned diffusion models often exhibit a _lack of robustness_. In particular, recent studies [], [] have shown that concepts trained to be erased can easily be regenerated through meticulously designed prompts, referred to as adversarial prompts. Consider the example shown in the first row of Fig. 1, although the model has been fine-tuned to exclude "nudity" from its outputs, it inadvertently reproduces nude images when faced with slightly modified, adversarial prompts. This reveals a fundamental weakness in current concept erasing methods: the embedded knowledge of the concept within the models could be hidden rather than forgotten. This vulnerability poses a significant risk when considering the deployment of diffusion models in real-world scenarios and calls for new solutions. However, how to improve the robustness performance has been a challenging problem yet to be solved.

With the above problem in mind, we first examine why fine-tuned diffusion models fail to be robust against adversarial prompts. We analyze the behavior of hidden states within these models. By tracing the activation of concept-related feature representations in neural networks, we have found that current fine-tuning techniques merely deactivate the generation of concept-related hidden states rather than eliminating them entirely. This deactivation is fragile, as input perturbations can reactivate these hidden states, allowing for the regeneration of supposedly erased content. This implies that the internal pathways for generating concept-related hidden states stay intact, even though they are _temporarily inactive_. To enhance robustness, we propose a simple solution: pruning specific parameters to sever these generation pathways completely. If we can strategically _zero out certain parameters_, we may _cut off the routes_ that lead to the reactivation of concept-related hidden states, even in the face of adversarial prompts.

To achieve the above goal, we develop a differentiable pruning strategy for robust concept erasing. Specifically, we parameterize a mask for each parameter and define the training objective with a standard concept-erasing objective, such as ESD [] and AC []. We then employ back-propagation to optimize the mask, allowing the concept erasing loss to determine which parameters should be pruned. That is, we integrate the erasing and pruning into a single objective. In this way, we can achieve two goals _simultaneously_: 1) minimizing the loss associated with concept erasing, effectively eliminating the concept knowledge within models, and 2) severing the pathways that could potentially reactivate the concept-related hidden states, ensuring robustness against adversarial prompts.

The enhanced robustness of our proposed method, compared to previous approaches, has been empirically validated across three widely-used test environments: the erasure of nudity, style, and objects, as detailed in Section 4. We find that our method achieves comparable or even superior performance in the concept erasing rate on normal prompts and significantly improves the robustness performance on adversarial prompts, crafted by attack methods including UnlearnDiff [], [] and P4D []. A summarized comparison between the SOTA fine-tuning based method ESD and our method P-ESD is reported in Fig. 2. Notably, we also empirically find that the sparsity of pruning is well controlled under to a small portion (e.g., less than 0.01%) of parameters and does not sacrifice generation quality on other concepts.

Figure 1: Left panel: semantic illustration of prior concept erasing methods (the top row) and our method (the bottom row). Right panel: concrete examples illustrate the vulnerability of prior concept-erasing methods and the robustness of our method.

We summarize our contributions as follows:

* We analyze why fine-tuning-based erasing is vulnerable to adversarial attacks, offering insights to improve the robustness of concept erasing in diffusion models.
* We develop a new concept-erasing paradigm based on pruning to enhance robustness. This approach integrates erasing and pruning into a single objective and can be easily applied to existing concept-erasing objectives.
* Experiments demonstrate that our method significantly improves the robustness of diffusion models across three test beds while maintaining the ability to generate standard concepts.

## 2 Related Work

### Concept erasing in diffusion models

The task of concept erasing, or generally the removal of undesirable image generation, is introduced in [24][23][27][10]. There are two kinds of methods: inference-based and training-based. For the former, there is no need to update the model's parameters. In this vein, [27] proposed designing a safety guidance to steer the generation in the opposite direction for unsafe prompts. [24] proposed applying an NSFW safety filter to detect sensitive prompts before generation. On the other hand, training-based approaches are believed to be safer as they aim to make the model forget undesirable knowledge within the parameters. To name a few, [2] explored the use of negative guidance in text-to-image diffusion models to reduce the conditional generation probability. [11][10][11][12] showed that modifying the conditional distribution of the target concept to that of another anchor/surrogate concept also performs well. Note that closed-form solutions are available for [10][11] since they merely update the linear projection layer in the cross-attention module.

Concept erasing in text-to-image diffusion models is similar to the concept of machine unlearning, which aims to remove the impact of certain data subsets from a trained model, as outlined in [29][14][13][14]. While both processes share the goal of mitigating undesired influences, they differ in focus. Concept erasing specifically targets the modification of content in generated images, as highlighted in [2].

### Neural network pruning

Pruning [16] is a compression technique commonly used to remove redundant components (e.g., weights or neurons) in neural networks. It is effective in reducing the number of neural network parameters, thereby improving computational efficiency on edge devices [13]. Typically, pruning strategies are designed to prune "less important" parameters while preserving the acquired abilities [8][9][10]. Different from them, our framework requires to prune critical parameters associated the concept for removal. We are motivated by previous studies [23][27][28][13] that pruned neural networks are sparse, which can reduce the correlation among dominant features and thereby enhance robustness. They demonstrated that pruning is beneficial for adversarial robustness in machine _learning_, particularly in _classification_ tasks. In contrast, our focus is on the robustness of concept _erasing_ in _generative_ models.

Figure 2: Concept erasure rates of the fine-tuning-based ESD method [2] and our proposed pruning-based P-ESD method, with both methods applying the same erasing objective. Higher values indicate better performance. The results show that our method significantly outperforms the fine-tuning based approach, especially when facing adversarial prompts.

## 3 Robust Concept Erasing

### Preliminary

Diffusion models, trained on vast amounts of unfiltered Internet data [23], often acquire the capability to generate content that may include offensive images and copyrighted artworks. To mitigate these unintended consequences, the framework of **concept erasing** has been introduced in [21]. In particular, this framework aims to fine-tune the diffusion model to disable its generation ability for concepts deemed undesirable or inappropriate. Concretely, existing methods update model parameter \(\theta\) to override the prediction of the text prompt \(c\) (associated with the erased concept) to a new target \(y\):

\[\min_{\theta}\mathcal{L}_{\mathrm{erase}}(\theta)=\mathbb{E}_{x_{t},c,t}\left[ \left\|\epsilon_{\theta}(x_{t},c,t)-y\right\|_{2}^{2}\right]. \tag{1}\]

where \(\epsilon_{\theta}\) is the denoising network, and \(x_{t}\) is the noisy image input at time step \(t\). In this way, the probability of generating undesirable concepts are reduced in the denoising process. We explain how existing methods can be substantiated in the above framework.

* For the ESD (Erasing Stable Diffusion) [21], it uses the target value \[y=\epsilon_{\theta^{*}}(x_{t},c_{\mathrm{null}},t)-\eta[\epsilon_{\theta^{*}} (x_{t},c,t)-\epsilon_{\theta^{*}}(x_{t},c_{\mathrm{null}},t)],\] (2) where \(c_{\mathrm{null}}\) is the null text for unconditioned generation and \(\theta^{*}\) is the parameter for an non-erased diffusion model. Using the terminology from classifier-free guidance generation, this target value guides the generation in the opposite direction of the erased concept.
* Another famous method is AC (Ablating Concept) [17], which uses the target value from the prediction of text prompt \(c^{*}\) for an anchor concept: \[y=\mathrm{stop\_gradient}(\epsilon_{\theta}(x_{t},c^{*},t)).\] (3) This anchor concept is semantically similar to the erased concept but is removed with the target concept. For example, to erase "Grumpy Cat", \(c\) could be "A cute little Grumpy Cat" and \(c^{*}\) is "A cute little cat".

### Vulnerability of Concept Erasing

Although existing concept erasing methods are effective on normal prompts, they are vulnerable to adversarial prompts [24][6]. We provide such examples in Fig. [1] and Fig. [8] A critical question arises: why do these fine-tuning-based erasing methods fail to be robust when faced adversarial prompts? In this section, we explore the underlying reasons for this weakness by analyzing the model's internal hidden states, specifically focusing on how concepts emerge and dissipate within the diffusion model.

We believe that concept generation in the produced images is primarily controlled by certain parameters in the denoising network that interact with the inputs to yield concept-related feature representations and final images. We realize that it is challenging to provide a complete depiction of this process, but it is possible to identify such concept-related feature representations and trace their behaviors to get some insights. For a denoising network consists of many ResNet blocks (e.g., 22 in SD-v1.4), we trace the outputs of blocks (post-activation). Provided text prompts \(c\) containing the concept to be erased, we measure the change of hidden states by:

\[\rho_{\ell,i}=\mathbb{E}_{x_{t},c}\left[\left\|z_{\ell,i}^{*}(x_{t},c,t)-z_{ \ell,i}(x_{t},c,t)\right\|_{1}\right], \tag{4}\]

Figure 3: Visualization of intrinsic vulnerability of fine-tuned models. More visualization examples are provided in Appendix A.1where \(z^{\prime}_{\ell,i}\) and \(z_{\ell,i}\) denote the outputs of the \(\ell\)-th block and \(i\)-th channel in the original model and erased model (e.g., by ESD) respectively. A large value of \(\rho\) indicates that such a channel is modified a lot by the erasing method and more correlated with concept generation. For each block, we identify and focus on the channels with the most largest values of \(\rho\) as concept-related hidden states. An example is provided in Fig. 2, where we demonstrate the erasing of the concept "tench". More examples are provided in Appendix [A.1]

Fig. 2 reveals the following mechanism: concept-erasing methods like ESD can effectively deactivate the hidden states related to concept generation, successfully removing the undesired concept from the generated image under normal prompts. However, when an adversarial perturbation is introduced to the input prompt, the model's generation pathways for these hidden states are reactivated, causing the undesired concept to reemerge. This observation implies that the internal pathways for generating concept-related hidden states stay intact, even though they are _temporarily inactive_. We believe this drawback is inherent to _fine-tuning methods, which merely update parameters to change the denoising network's output, but do not sever internal pathways of concept-related hidden states_.

Our observation also leads to an interesting question: could we directly remove the identified channels (set the value of channels to zero) to prevent the reactivation of concept-related hidden states? We have experimented with this and found that while this approach is effective for some adversarial prompts, it simultaneously impairs the model's ability to generate non-targeted concepts. Detailed results of this approach are provided in Appendix [A.1] We believe this failure stems, in part, from the _polysemantic nature of channels_[11, 22]. As output units, they may be responsible for a mixture of multiple concepts, not just the one we aim to remove. This prompts us to explore a more principled strategy: selectively pruning weights to disrupt the generation pathways of concept-related hidden states, as discussed in the following section.

### Pruning for Robust Concept Erasing

In this section, we introduce a parameter-pruning-based strategy to achieve robust concept erasing. Previous studies on neural network pruning typically target the removal of "less important" connections, often identified through their minimal impact on overall model performance. Different from them, our work innovatively integrates the erasing and pruning into a unified objective, and prune critical parameters associated the concept for removal.

Here, let \(\theta^{*}\in\mathbb{R}^{p}\) denote the parameter of the original diffusion model. We introduce hard masks \(M_{\rm hard}\in\{0,1\}^{p}\), which has the same dimension as \(\theta^{*}\). The training objective remains to minimize a concept erasing loss function for the denoising network, but the optimization variables are now the masks:

\[\min_{M_{\rm hard}\in\{0,1\}^{p}}\mathcal{L}_{\rm erase}=\mathbb{E}_{x_{t},c,t}\left[\left\|\epsilon_{\theta^{*}\odot M_{\rm hard}}(x_{t},c,t)-y\right\|_ {2}^{2}\right], \tag{5}\]

where \(\odot\) means element-wise multiplication. The masks are applied to parameters (weights and biases) in convolution and linear layers to selectively enable or disable the connections within these layers. For parameters with special roles, such as those in layer normalization, masks are not applied. The design of the target variable \(y\) is flexible and can be adapted to various existing methods, such as ESD and AC.

By solving Eq. 5, we can achieve two goals _simultaneously_: 1) minimizing the loss associated with concept erasing, effectively eliminating the concept knowledge within models, and 2) cutting off the pathways that could potentially reactivate the concept-related hidden states, ensuring robustness against adversarial prompts.

**Practical Algorithm.** Despite good properties of Eq. 5, the optimization problem involves discrete optimization and is hard to solve. To address this challenge, we propose convert it to a continuous optimization problem and employ gradient-based optimization algorithms such as AdamW [20]. In particular, We parameterize the hard mask to be soft via the sigmoid function:

\[M_{\rm soft}(m)=\frac{1}{1+\exp(-\eta\cdot m)}\in[0,1]^{p}, \tag{6}\]

where \(\eta>0\) is a fixed temperature coefficient (usually \(\eta=10\)) controlling the slope of the sigmoid function, and \(m\in\mathbb{R}^{p}\) is the trainable parameter to be optimized with same dimension as \(\theta^{*}\). Other parameterization techniques may also be applicable, but we find that the sigmoid transformation works well in our experiments. Then we solve the following continuous optimization problem 

[MISSING_PAGE_FAIL:6]

the criterion called _Concept Erasure Rate (CER)_, which indicates the rate at which the diffusion model successfully erases a specified concept from its generated images. A higher rates means better performance in achieving concept erasure.

**Attack methods:** In all three scenarios, we implement two recently proposed attack methods: P4D [1] and UnlearnDiff [24], which use a local search method to find an adversarial prompt for concept regeneration. The prepended prompt perturbation is set as 5 tokens for erasing nudity, and 3 tokens for erasing style and object. For each prompt, we conduct 10 attacks on samples drawn from 10 timesteps, selected at intervals of 5 steps across 50 diffusion steps. Details of attack configuration is provided in Appendix A.3

### Erasing Nudity

We evaluate models on erasing nudity using the same test prompts as [24], derived from the "sexual" category of the I2P dataset [27] with nudity scores above 0.75. NudeNet [2] is then used to detect nudity in the generated images.

We report the average concept erase rate over these test prompts in Tab. [1]. Quite interestingly, we find that our method not only improves the concept erasing rate on normal test prompts but also the adversarial prompts. Note that the concept erasing on adversarial prompts are challenging: the performance of all methods we tested dropped on adversarial prompts compared to that with normal test prompts. Nevertheless, we find that our P-ESD is still robust among baselines. Specifically, the concept erasure rates on adversarial prompts improves by over 30% compared to existing methods. These results demonstrate that our proposed method serves as an effective strategy for enhancing the robustness of concept erasing in the nudity task.

### Erasing Style

In this section, we consider to remove the artist style, a more abstract concept. Following [24], we choose to examine the effectiveness of various methods in erasing the "Van Gogh" style from diffusion model. There are 50 test prompts. The success of concept erasing is evaluated using a style classifier to check if the "Van Gogh" style is among the top-3 predictions for images generated by the model after concept erasing has been applied.

We report the results in Tab. [2]. Among the fine-tuning-based methods, ESD emerges as the most effective aimed at erasing style. However, it is still inferior to our proposed P-ESD method, which outperforms ESD by over 30% when tested against adversarial prompts.

### Erasing Objects

In this section, we focus on removing various objects, including "tench," "church," and "garbage truck". For each object class, we use 50 test prompts from [24], generated by ChatGPT.

The results are presented in Tab. [3]. Among baselines, UCE outperforms both FMN and ESD. However, by integrating pruning into ESD, P-ESD exhibits enhanced performance over ESD on adversarial

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline  & UCE & RACE & SPM & AC & P-AC & ESD & P-ESD \\ \hline _Normal Prompts_ & 0.80 & 0.83 & 0.47 & 0.60 & 0.63 & 0.80 & **0.95** \\ \hline _Adversarial Prompts:_ & & & & & & & \\ UnlearnDiff & 0.14 & 0.49 & 0.08 & 0.17 & 0.36 & 0.40 & **0.86** \\ P4D & 0.13 & 0.50 & 0.08 & 0.26 & 0.42 & 0.39 & **0.82** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Concept erasure rate for erasing nudity. A larger number means a better erasing.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline  & UCE & RACE & SPM & AC & P-AC & ESD & P-ESD \\ \hline _Normal Prompts_ & 0.28 & 0.56 & 0.36 & 0.82 & 0.80 & 0.84 & **1.00** \\ \hline _Adversarial Prompts:_ & & & & & & & \\ UnlearnDiff & 0.04 & 0.20 & 0.12 & 0.42 & 0.62 & 0.52 & **0.90** \\ P4D & 0.06 & 0.18 & 0.10 & 0.46 & 0.62 & 0.56 & **0.86** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Concept erasure rate for erasing style.

prompts and competes favorably with UCE. This suggests that our pruning-based approach offers greater robustness than fine-tuning when optimizing the same erasing objective.

### Analysis of the Proposed Method

In addition to evaluating the concept erasing rate on adversarial prompts, we aim to explore the model's internal robustness. To do this, we assess the sensitivity score of concept-related hidden states identified using Eq. 4. This score is based on the magnitude of activation value changes when exposed to normal prompts, \(c\), versus adversarial prompts, \(c_{\mathrm{adv}}\). Intuitively, we expect the concept-related hidden states to remain stable with a low sensitivity score under adversarial attacks, such that they would not easily reactivate. Specifically, for each feature \(\varepsilon_{\ell,i}\) located at the \(\ell\)-th layer and \(i\)-th channel in the erased model, we define its sensitivity score at denoising timestep \(t\) as:

\[\delta_{\ell,i}=\mathbb{E}_{x_{t},c}\left[\left\|z_{\ell,i}(x_{t},c,t)-z_{\ell,i}(x_{t},c_{\mathrm{adv}},t)\right\|_{1}\right], \tag{8}\]

A large value of \(\delta\) implies that the activation of the hidden state is significantly affected by the prompt change, thus indicating its vulnerability to input variations.

In Fig. 4 we compare the sensitivity scores of fine-tuning-based and pruning-based erasing methods, namely ESD and P-ESD. These scores are computed over five timesteps, selected at intervals of 10 steps across a total of 50 diffusion steps. We find that P-ESD consistently reduces the sensitivity score throughout the denoising process, suggesting greater internal robustness compared to ESD.

## 5 Conclusion

In this paper, we develop a new pruning strategy to address the robustness issue in existing concept erasing frameworks. Our method selectively prunes parameters critical to targeted concepts, demonstrating superior performance over existing approaches. This work aims to mitigate risks associated with deploying diffusion models in real-world scenarios where adversarial prompts may be encountered. Future research will explore extending these techniques to improve robustness in other model types beyond diffusion models.

\begin{table}
\begin{tabular}{l|c c c c c|c c c c c|c c c c c c} \hline \hline  & \multicolumn{4}{c|}{Tench} & \multicolumn{4}{c|}{Church} & \multicolumn{4}{c}{Garbage Truck} \\ \cline{2-13}  & FMN & UCE & SPM & ESD & P-ESD & FMN & UCE & SPM & ESD & P-ESD & FMN & UCE & SPM & ESD & P-ESD \\ \hline _Normal Prompts_ & 0.64 & **1.00** & 0.94 & **1.00** & **1.00** & 0.48 & **0.94** & 0.54 & 0.86 & 0.88 & 0.54 & 0.98 & 0.92 & 0.98 & **1.00** \\ \hline _Adversarial Prompts_: & & & & & & & & & & & & & & & & & \\ \hline \multirow{4}{*}{
\begin{tabular}{} \end{tabular} } & \multirow{4}{*}{0.12} & **0.96** & 0.42 & 0.78 & 0.92 & 0.12 & **0.74** & 0.08 & 0.58 & 0.64 & 0.08 & 0.84 & 0.64 & **0.90** & 0.86 \\  & & 0.14 & 0.92 & 0.56 & 0.86 & **0.98** & 0.16 & 0.64 & 0.10 & 0.64 & **0.68** & 0.04 & 0.88 & 0.56 & 0.82 & **0.94** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Concept erasing rate for erasing objects.

Figure 4: Sensitivity score comparison between ESD and P-ESD. The sensitivity scores are averaged on concept-related hidden states from each layer.

## References

* [1] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and Antonio Torralba. Understanding the role of individual units in a deep neural network. _Proceedings of the National Academy of Sciences_, 117(48):30071-30078, 2020.
* [2] P Bedapudi. Nudenet: Neural nets for nudity classification, detection and selective censoring, 2019.
* [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. _Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf_, 2(3):8, 2023.
* [4] Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of neural network pruning? _Proceedings of machine learning and systems_, 2:129-146, 2020.
* [5] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In _2021 IEEE Symposium on Security and Privacy (SP)_, pages 141-159. IEEE, 2021.
* [6] Zhi-Yi Chin, Chieh-Ming Jiang, Ching-Chun Huang, Pin-Yu Chen, and Wei-Chen Chiu. Prompting4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts. In _International Conference on Machine Learning (ICML)_, 2024.
* [7] Jack Foster, Stefan Schoepf, and Alexandra Brintrup. Fast machine unlearning without retraining through selective synaptic dampening. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 12043-12051, 2024.
* [8] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. _arXiv preprint arXiv:1803.03635_, 2018.
* [9] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. _arXiv preprint arXiv:2303.07345_, 2023.
* [10] Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzynska, and David Bau. Unified concept editing in diffusion models. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 5111-5120, 2024.
* [11] Chao Gong, Kai Chen, Zhipeng Wei, Jingjing Chen, and Yu-Gang Jiang. Reliable and efficient concept erasure of text-to-image diffusion models. In _Proceedings of the European Conference on Computer Vision_, 2024.
* [12] Shupeng Gui, Haotao Wang, Haichuan Yang, Chen Yu, Zhangyang Wang, and Ji Liu. Model compression with adversarial robustness: A unified optimization framework. _Advances in Neural Information Processing Systems_, 32, 2019.
* [13] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. _arXiv preprint arXiv:1510.00149_, 2015.
* [14] Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, and Sijia Liu. Model sparsity can simplify machine unlearning. In _Annual Conference on Neural Information Processing Systems_, 2023.
* [15] Artur Jordao and Helio Pedrini. On the effect of pruning on adversarial robustness. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1-11, 2021.
* [16] Ehud D Karnin. A simple procedure for pruning back-propagation trained neural networks. _IEEE transactions on neural networks_, 1(2):239-242, 1990.
* [17] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 22691-22702, 2023.

* [18] Guihong Li, Hsiang Hsu, Radu Marculescu, et al. Machine unlearning for image-to-image generative models. _arXiv preprint arXiv:2402.00351_, 2024.
* [19] Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, PRANAY SHARMA, Sijia Liu, et al. Model sparsity can simplify machine unlearning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [20] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [21] Mengyao Lyu, Yuhong Yang, Haiwen Hong, Hui Chen, Xuan Jin, Yuan He, Hui Xue, Jungong Han, and Guiguang Ding. One-dimensional adapter to rule them all: Concepts diffusion models and erasing applications. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7559-7568, 2024.
* [22] Simon C Marshall and Jan H Kirchner. Understanding polysemanticity in neural networks through coding theory. _arXiv preprint arXiv:2401.17975_, 2024.
* [23] Pamela Mishkin, Lama Ahmad, Miles Brundage, Gretchen Krueger, and Girish Sastry. Dall' e 2 preview-risks and limitations. _Noudetnu_, 28:2022, 2022.
* [24] Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tramer. Red-teaming the stable diffusion safety filter. _arXiv preprint arXiv:2210.04610_, 2022.
* [25] Reddit. Tutorial: How to remove the safety filter in 5 seconds, 2023.
* [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* [27] Patrick Schramowski, Manuel Brack, Bjorn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22522-22531, 2023.
* [28] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.
* [29] Ayush K Tarun, Vikram S Chundawat, Murari Mandal, and Mohan Kankanhalli. Fast yet effective machine unlearning. _IEEE Transactions on Neural Networks and Learning Systems_, 2023.
* [30] Yulong Wang, Xiaolu Zhang, Lingxi Xie, Jun Zhou, Hang Su, Bo Zhang, and Xiaolin Hu. Pruning from scratch. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 12273-12280, 2020.
* [31] Dongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. _Advances in Neural Information Processing Systems_, 34:16913-16925, 2021.
* [32] Shaokai Ye, Kaidi Xu, Sijia Liu, Hao Cheng, Jan-Henrik Lambrechts, Huan Zhang, Aojun Zhou, Kaisheng Ma, Yanzhi Wang, and Xue Lin. Adversarial robustness vs. model compression, or both? In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 111-120, 2019.
* [33] Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Forget-me-not: Learning to forget in text-to-image diffusion models. _arXiv preprint arXiv:2303.17591_, 2023.
* [34] Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, and Sijia Liu. To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now. In _Proceedings of the European conference on computer vision_, 2024.

Appendix

### Vulnerability of Concept Erasing

**Concept-related channels visualization.** We provide additional visualization results of concept-related channels in Fig. 5, Fig. 6, Fig. 6, and Fig. 6, which are deactivated in the fine-tuned model (by ESD) but reactivated by adversarial prompts. The results indicate the internal pathways for concept-related hidden states are temporarily inactive in fine-tuned models. In those figures, the identified channels are rescaled to the same size as the generated images and overlap with them. Redder regions in the figure indicate higher activation.

**Results on zeroing out concept-related channels.** We also evaluate the erasing performance by directly zeroing out the identified concept-related channels. As indicated in Fig. 5, the fine-tuned model (by ESD) is vulnerable to adversarial prompts and regenerate the target concepts (the third column). However, after we directly zeroing out the identified concept-related channels, the adversarial prompts no longer succeed (the fourth column, ESD + ZC). Despite the improvement in erasing robustness, this method could compromise the generation quality, as evidenced by the noticeable degradation in generation quality. This effect might arise from the polysemantic nature of channels [11, 22], where the identified channels might contribute to a mixture of multiple concepts rather than being exclusively tied to a single target concept. Therefore, we provide a safer and more automatic approach, to prune within the parameter space. This allows us to achieve both a high rate of concept erasure and good generation quality.

### Additional Experiment Results

**On the generation quality on non-concept content.** To assess the impact on image generation quality on non-concept contents, we evaluated the fidelity (FID) score using 30K prompts from the COCO dataset. We compared the FID scores between ESD, and P-ESD with the results presented in Tab. 3. Notably, P-ESD maintains generation quality comparable to, or even better than, the ESD method. The results indicate that compared with fine-tuning-based erasing, pruning-based method does not compromise quality while enhancing erasure performance.

**Comparison of different pruning stages.** We analyze which pruning approach best enhances concept erasing, comparing three methods for removing nudity:

Figure 5: (a-c) The vulnerability of edited SD by fine-tuning when erasing “church” (the first row) and “tench” (the second row). (d) The results from directly zeroing out concept-related channels.

* Pre-Prune: Following [19], we globally prune 10% of pre-trained weights by magnitude before erasing. During erasing, pruned weights are fixed while remaining weights are fine-tuned using ESD.
* P-ESD (Our method): Pruning occurs during the erasing process, optimizing the model for the erasing objective. The final pruning ratio is 0.0012%.
* Post-Prune: Standard ESD erasing followed by global magnitude-based pruning of 10% of the model.

In Fig. 6 we compare three pruning strategies against ESD without pruning. All methods improve on test and adversarial prompts, highlighting the role of neural network sparsity in robust concept erasing. P-ESD stands out as the most effective strategy, with the least pruned weights. This could be due to the fact that pruning aware of the erasing objective could achieve localized robustness for the erased concept, while generic pruning aimed at merely increasing the network's sparsity may lead to a widespread reduction in neuron sensitivity.

**Hyper-parameter analysis.** The temperature \(\eta\) determines the steepness of the sigmoid function used in discrete optimization. In Tab. 5 we analyze the impact of \(\eta\) on concept erasing rate under UnlearnDiff attack and generation quality (FID). As the table indicates, \(\eta\)=10 gives a good trade-off between erasing effectiveness and generation quality. When \(\eta\) is larger, such as 15, the sigmoid function becomes steeper, which may make optimization more challenging and negatively affecting generation quality. Conversely, a smaller \(\eta\) value, such as 5, leads to slower convergence and less effective erasing. At \(\eta=10\), the soft masks tend to concentrate around 0 and 1, which reduces the need for fine-tuning the threshold \(\sigma\), allowing us to empirically set it at 0.5.

**Analysis on pruned weights.** To analysis which parameters are mostly pruned. In Fig. 1 we illustrate the percentage of pruned weight of relative to the total pruned weights in each layer, when pruning the unconditional layers for erasing tend (the left figure) and the conditional layers for erasing style (the right figure). It is observed that when pruning the unconditional layers, the majority of pruned weights are in the attention layers, including the input/output projection layers and feedforward layer.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & Nudity & Style & Tench & Church & Garbage Truck \\ \hline ESD & 14.32 & 15.01 & 13.72 & 16.07 & 17.75 \\ P-ESD & 13.60 & 15.08 & 13.23 & 16.72 & 14.09 \\ \hline \hline \end{tabular}
\end{table}
Table 4: FID comparison on COCO-30k prompts between ESD and P-ESD. The original stable diffusion model’s FID score is 14.64.

Figure 6: Comparison of different pruning strategies. The pruning ratio is also shown in the bar. Even though its pruning ratio is smaller, P-ESD is more effective than Post-Prune and Pre-Prune.

When pruning conditional layers, the mostly pruned weights are found in the cross-attention value matrix, this is because value matrix of cross attention layer plays a crucial role in determining which parts of the texture information are been leveraged to generate the visual content.

**Case visualization.** In Fig. 8, we present concrete examples of attack results which our method remains robust to the attack. We can observe that our method is robust towards adversarial attacks while maintaining generation quality.

Figure 8: Visualization examples. The black boxes in the first two rows are added by the authors to hide NSFW content for publication. The symbol represents successful concept erasure, and indicates a failure in concept erasure.

Figure 7: Percentage of pruned weights for each type of layer.

### Additional Experiment Details

Our experiments are conducted on four V100-32G GPUs. The machine is equipped with 48 Intel-Xeon-Gold-6226 CPUs. For experiments on P-ESD and P-AC, we run the experiments using random seed as 42.

For the evaluation of adversarial attacks, both methods utilize prepended prompt perturbations. Specifically, 5 tokens are used for erasing nudity, and 3 tokens are used for erasing style and object. For each prompt, we perform 10 attacks on samples drawn from 10 timesteps, chosen at 5-step intervals across the 50 diffusion steps. The prepended prompt perturbations are optimized over 40 iterations using the Adam optimizer, with a learning rate of 0.01 and a weight decay of 0.1 at each step. For the evaluation of the concept erasing rate, we generated images using the LMSDiscreteScheduler as the sampling scheduler. The parameters for this scheduler include a beta start of 0.00085, a beta end of 0.012, and a beta schedule set to "scaled_linear." The sampling was performed with 50 steps. The FID scores are calculated using _clean-fid_[1]

Figure 9: Visualization of concept-related hidden states in the original stable diffusion (SD) and the edited SD when erasing church. They are from the 1017th channel output by the first group of layers (Resnet-0) in the second upsampling block (UpBlock-1) of the network.

Figure 10: Visualization of concept-related hidden states in the original stable diffusion (SD) and the edited SD when erasing Van Gogh. They are from the 1134th channel output by the first group of layers (Resnet-0) in the fourth downsampling block (DownBlock-3) of the network.

Figure 11: Visualization of concept-related hidden states in the original stable diffusion (SD) and the edited SD when erasing nudity. They are from the 607th channel output by the first group of layers (Resnet-0) in the middle block of the network.

Figure 12: Visualization of concept-related hidden states in the original stable diffusion (SD) and the edited SD when erasing tench. They are from the 298th channel output by the first group of layers (Resnet-0) in the second upsampling block (UpBlock-1) of the network.