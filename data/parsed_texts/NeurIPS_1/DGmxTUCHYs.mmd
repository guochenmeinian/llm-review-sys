# Zero-One Laws of Graph Neural Networks

Sam Adam-Day

Department of Mathematics

University of Oxford

Oxford, UK

sam.adam-day@cs.ox.ac.uk

&Theodor-Mihai Iliant

Department of Computer Science

University of Oxford

Oxford, UK

theodor-mihai.iliant@lmh.ox.ac.uk

&Ismail Ilkan Ceylan

Department of Computer Science

University of Oxford

Oxford, UK

ismail.ceylan@cs.ox.ac.uk

###### Abstract

Graph neural networks (GNNs) are the de facto standard deep learning architectures for machine learning on graphs. This has led to a large body of work analyzing the capabilities and limitations of these models, particularly pertaining to their representation and extrapolation capacity. We offer a novel theoretical perspective on the representation and extrapolation capacity of GNNs, by answering the question: how do GNNs behave as the number of graph nodes become very large? Under mild assumptions, we show that when we draw graphs of increasing size from the Erdos-Renyi model, the probability that such graphs are mapped to a particular output by a class of GNN classifiers tends to either _zero_ or to _one_. This class includes the popular graph convolutional network architecture. The result establishes 'zero-one laws' for these GNNs, and analogously to other convergence laws, entails theoretical limitations on their capacity. We empirically verify our results, observing that the theoretical asymptotic limits are evident already on relatively small graphs.

## 1 Introduction

Graphs are common structures for representing relational data in a wide range of domains, including physical [35], chemical [7, 18], and biological [42, 10] systems, which sparked interest in machine learning over graphs. Graph neural networks (GNNs) [33, 14] have become prominent models for graph machine learning for a wide range of tasks, owing to their capacity to explicitly encode desirable relational inductive biases [5]. One important virtue of these architectures is that every GNN model can be applied to _arbitrarily large_ graphs, since in principle the model parameters are independent of the graph size. This raises the question: how do GNNs behave as the number of nodes becomes very large? When acting as binary classifiers, GNNs can be thought of as parametrizing Boolean properties of (labelled) graphs. A classical method of specifying such properties is through first-order formulas, which allow for precise definitions using a formal language [9]. The celebrated 'zero-one law' for first-order logic [13, 8] provides a crisp answer to the question of the asymptotic behaviour of such properties: as graphs of increasing size are drawn from the Erdos-Renyi distribution, the probability that a _first-order_ property holds either tends to _zero_ or to _one_.

In this paper, we show an analogous result for binary classification GNNs: under mild assumptions on the model architecture, several GNN architectures including graph convolutional networks [21] satisfy a zero-one law over Erdos-Renyi graphs with random node features. The principal import of this result is that it establishes a novel upper-bound on the expressive power of GNNs: any property of graphs which can be _uniformly_ expressed by a GNN must obey a zero-one law. An example of a simple property which does _not_ asymptotically tend to zero or one is that of having an even number of nodes. Note however that our result, combined with the manifest success of GNNs in practice, suggests that zero-one laws must be abundant in nature: if a property we cared about did not satisfy a zero-one law, none of the GNN architectures we consider would be able to express it.

Our main results flexibly apply both to the case where we consider the GNN as a classifier applied to randomly sampled graphs and node features, and where we consider the random node features as part of the model. For the latter, it is known that incorporating randomized features into the model significantly increases its expressive power on graphs with _bounded number of nodes_[32, 1]. Our results yield the first upper bound on the expressive power of these architectures which is _uniform_ in the number of nodes. We complement this with a corresponding lower bound, showing that these architectures can universally approximate any property which satisfies a certain zero-one law.

A key strength of the results is that they apply equally well to randomly initialized networks, trained networks, and anything in between. In this sense, our asymptotic analysis is orthogonal to the question of optimization, and holds regardless of the choice of training method. Another interesting aspect of these results is that they unite analysis of expressive power with extrapolation capacity. Our zero-one laws simultaneously provide limits on the ability of GNNs to extrapolate from smaller Erdos-Renyi graphs to larger ones: eventually any GNN must classify all large graphs the same way.

To validate our theoretical findings, we conduct a series of experiments: since zero-one laws are of asymptotic nature, we may need to consider very large graphs to observe clear empirical evidence for the phenomenon. Surprisingly however, GNNs already exhibit clear evidence of a zero-one law even on small graphs. Importantly, this is true for networks with very few layers (even a single-layer), which is reassuring, as it precludes confounding factors, such as the effect of over-smoothing due to increased number of layers [23]. We provide further experimental results in the appendix of this paper, where all proofs of technical statements can also be found. We make the code for our experiments available online at [https://github.com/SamAdamDay/Zero-One-Laws-of-Graph-Neural-Networks](https://github.com/SamAdamDay/Zero-One-Laws-of-Graph-Neural-Networks).

## 2 Preliminaries

**Random graphs and matrices.** The focus of our study is on classes of random graphs with random features, for which we introduce some notation. We write \(\mathbf{x}\in\mathbb{R}^{d}\) to represent a vector, and \(\mathbf{X}\in\mathbb{R}^{d\times n}\) to represent a matrix. Analogously, we write \(\mathbf{x}\) to denote a _random_ vector, and \(\mathbf{X}\) to denote a _random_ matrix, whose entries are (real) random variables. We write \(\mathbb{G}(n,r)\) to denote the class of simple, undirected Erdos-Renyi (ER) graphs with \(n\) nodes and edge probability \(r\) and let \(\mathbb{D}(d)\) denote some distribution of feature vectors over \(\mathbb{R}^{d}\). We define an Erdos-Renyi graph equipped with random node features as a pair \(\mathcal{G}=(\mathbf{A},\mathbf{X})\), where \(\mathbf{A}\sim\mathbb{G}(n,r)\) is the random graph adjacency matrix of the graph \(G=(V,E)\) and \(\mathbf{X}\in\mathbb{R}^{d\times n}\) is a corresponding random feature matrix, independent of \(G\), which contains, for each node \(v\in V\), an initial random node feature \(\mathbf{x}_{v}\sim\mathbb{D}(d)\) as the corresponding columns of \(\mathbf{X}\).2

Footnote 2: We define a \(d\times|V|\) dimensional (random) feature matrix as opposed to the more common \(|V|\times d\). This is for ease of presentation, since we aim to work on the (random) column vectors of such matrices.

**Message passing neural networks.** The focus of this work is on _message-passing neural networks (MPNNs)_[12, 17] which encapsulate the vast majority of GNNs. The fundamental idea in MPNNs is to update the initial (random) state vector \(\mathbf{x}_{v}^{(0)}=\mathbf{x}_{v}\) of each node \(v\) for \(T\in\mathbb{N}\) iterations, based on its own state and the state of its neighbors \(\mathcal{N}(v)\) as:

\[\mathbf{x}_{v}^{(t+1)}=\phi^{(t)}\Big{(}\mathbf{x}_{v}^{(t)},\psi^{(t)}\big{(} \mathbf{x}_{v}^{(t)},\{\!\!\{\mathbf{x}_{u}^{(t)}|\,u\in\mathcal{N}(v)\}\!\} \big{)}\Big{)},\]

where \(\{\!\!\{\cdot\}\!\}\) denotes a multiset, and \(\phi^{(t)}\) and \(\psi^{(t)}\) are differentiable _combination_, and _aggregation_ functions, respectively. Each layer's node representations can have different dimensions: we denote by \(d(t)\) the dimension of the node embeddings at iteration \(t\) and typically write \(d\) in place of \(d(0)\).

The final node representations can then be used for node-level predictions. For graph-level predictions, the final node embeddings are _pooled_ to form a graph embedding vector to predict properties of entire graphs. The pooling often takes the form of simple averaging, summing or component-wise maximum. For Boolean node (resp., graph) classification, we further assume a classifier \(\mathcal{E}:\mathbb{R}^{d(T)}\rightarrow\mathbb{B}\) which acts on the final node representations (resp., on the final graph representation).

There exist more general message passing paradigms [5] such as _MPNNs with global readout_ which additionally aggregate over all node features at every layer, and are known to be more expressive [4]. Some model architectures considered in this paper include a global readout component and we consider different choices for the combine (\(\phi^{(t)}\)) and aggregate (\(\psi^{(t)}\)) functions, as we introduce next.

**GCN.** The primary GNN architecture we consider is _graph convolutional networks_ (GCN) [21]. These are instances of MPNNs with self-loops, which aggregate over the extended neighborhood of a node \(\mathcal{N}^{+}(v):=\mathcal{N}(v)\cup\{v\}\). GCNs iteratively update the node representations as \(\mathbf{x}_{v}^{(t)}=\sigma\left(\mathbf{y}_{v}^{(t)}\right)\), where the preactivations are given by:

\[\mathbf{y}_{v}^{(t)}=\mathbf{W}_{n}^{(t)}\sum_{u\in\mathcal{N}^{+}(v)}\frac{1}{ \sqrt{|\mathcal{N}(u)||\mathcal{N}(v)|}}\mathbf{x}_{u}^{(t-1)}+\mathbf{b}^{(t)}\]

We apply the linear transformation \(\mathbf{W}_{n}^{(t)}\in\mathbb{R}^{d(t)\times d(t-1)}\) to a normalized sum of the activations for the previous layers of the neighbors of the node under consideration, together with its own activation. Adding a bias term \(\mathbf{b}^{(t)}\) yields the preactivation \(\mathbf{y}_{v}^{(t)}\), to which we apply the non-linearity \(\sigma\).

**MeanGNN.** We also consider the MeanGNN\({}^{+}\) architecture which is a _self-loop GNN with mean aggregation and global readout_[17], and updates the node representation as \(\mathbf{x}_{v}^{(t)}=\sigma\left(\mathbf{y}_{v}^{(t)}\right)\), where:

\[\mathbf{y}_{v}^{(t)}=\frac{1}{|\mathcal{N}^{+}(v)|}\mathbf{W}_{n}^{(t)}\sum_{u\in \mathcal{N}^{+}(v)}\mathbf{x}_{u}^{(t-1)}+\frac{1}{n}\mathbf{W}_{r}^{(t)}\sum_{u\in V }\mathbf{x}_{u}^{(t-1)}+\mathbf{b}^{(t)}\]

MeanGNN\({}^{+}\) models additionally apply a linear transformation \(\mathbf{W}_{r}^{(t)}\in\mathbb{R}^{d(t)\times d(t-1)}\) to the mean of all previous node representations. We refer to MeanGNN as the special case of this architecture which does _not_ include a global readout term (obtained by dropping the second term in the equation).

**SumGNN.** Finally, we consider the SumGNN\({}^{+}\) architecture which is a _GNN with sum aggregation and global readout_[12], and updates the node representations as \(\mathbf{x}_{u}^{(t)}=\sigma\left(\mathbf{y}_{u}^{(t)}\right)\), where:

\[\mathbf{y}_{v}^{(t)}=\mathbf{W}_{s}^{(t)}\mathbf{x}_{v}^{(t-1)}+\mathbf{W}_{n}^{(t)} \sum_{u\in\mathcal{N}(v)}\mathbf{x}_{u}^{(t-1)}+\mathbf{W}_{r}^{(t)}\sum_{u\in V} \mathbf{x}_{u}^{(t-1)}+\mathbf{b}^{(t)}\]

This time, we separate out the contribution from the preactivation of the previous activation for the node itself. This yields three linear transformations \(\mathbf{W}_{s}^{(t)},\mathbf{W}_{n}^{(t)},\mathbf{W}_{r}^{(t)}\in\mathbb{R}^{d(t)\times d(t -1)}\). The corresponding architecture without the global readout term is called SumGNN.

## 3 Related work

Graph neural networks are flexible models which can be applied to graphs of any size following training. This makes an asymptotic analysis in the size of the input graphs very appealing, since such a study could lead to a better understanding of the extrapolation capabilities of GNNs, which is widely studied in the literature [41; 40]. Previous studies of the asymptotic behaviour of GNNs have focused on convergence to theoretical limit networks [20; 31] and their stability under the perturbation of large graphs [11; 22].

Zero-one laws have a rich history in first-order logic and random graph theory [13; 8; 25; 34; 6]. Being the first of its kind in the graph machine learning literature, our study establishes new links between graph representation learning, probability theory, and logic, while also presenting a new and interesting way to analyze the expressive power of GNNs. It is well-known that the expressive power of MPNNs is upper bounded by the _1-dimensional Weisfeiler Leman graph isomorphism test (1-WL)_[39; 29] and architectures such as SumGNN\({}^{+}\)[29] can match this. Barcelo et al. [4] further gives a logical characterization for a class of MPNNs showing SumGNN\({}^{+}\) can capture any function which can be expressed in the logic \(\mathsf{C}^{2}\), which is an extension of the two-variable fragment of first-order logic with counting quantifiers. Several works study the expressive power of these models under the assummption that there are _unique node identifiers_[26], or define _higher-order_ GNN models [29; 27; 28; 19] to obtain more expressive architectures.

Our work has direct implications on GNNs using random node features [32; 1], which are universal in the bounded graph domain. Specifically, we derive a zero-one law for GNNs using random node features which puts an upper bound on the expressive power of such models in a uniform sense: what class of functions on graphs can be captured by a _single_ GNN with random node features? Abboud et al. [1] prove a universality result for these models, but it is not uniform, since the construction depends on the graph sizes, and yields a different model parametrization depending on the choice of the graph sizes. Moreover, the construction of Abboud et al. [1] is of size exponential in the worst case. Grohe [15] recently improved upon this result, by proving that the functions that can be computed by a _polynomial-size bounded-depth_ family of GNNs using random node features are exactly the functions computed by bounded depth Boolean circuits with threshold gates. This establishes an upper bound on the power of GNNs with random node features, by requiring the class of models to be of bounded depth (fixed layers) and of size polynomial. However, this result is still _not_ uniform, since it allows the target function to be captured by different model parametrizations. There is no known upper bound for the expressive power of GNNs with random node features in the uniform setting, and our result establishes this.

Other limitations of MPNNs include _over-smoothing_[23; 30] and _over-squashing_[2] which are related to information propagation, and are linked to using more message passing layers. The problem of over-smoothing has also been studied from an asymptotic perspective [23; 30], where the idea is to see how the node features evolve as we increase the number of layers in the network. Our study can be seen as orthogonal to this work: we conduct an asymptotic analysis in the size of the graphs rather than in the number of layers.

## 4 Zero-one laws of graph neural networks

### Problem statement

We first define graph invariants following Grohe [16].

**Definition 4.1**.: A _graph invariant_\(\xi\) is a function over graphs, such that for any pair of graphs \(G_{1}\), \(G_{2}\), and, for any isomorphism \(f\) from \(G_{1}\) to \(G_{2}\) it holds that \(\xi(G_{1})=\xi(f(G_{2}))\). Graph invariants for graphs with node features are defined analogously.

Consider any GNN model \(\mathcal{M}\) used for binary graph classification. It is immediate from the definition that \(\mathcal{M}\) is invariant under isomorphisms of the graphs on which it acts. Hence \(\mathcal{M}\), considered as function from graphs to \(\mathbb{B}=\{0,1\}\), is a graph invariant. In this paper, we study the asymptotic behavior of \(\mathcal{M}\) as the number of nodes increases.

One remarkable and influential result from finite model theory is the 'zero-one law' for first-order logic. A (Boolean) graph invariant \(\xi\) satisfies a _zero-one law_ if when we draw graphs \(G\) from the ER distribution \(\mathbb{G}(n,r)\), as \(n\) tends to infinity the probability that \(\xi(G)=1\) either tends to \(0\) or tends to \(1\). The result, due to Glebskii et al. [13] and Fagin [8], states that any graph invariant which can be expressed by a first-order formula satisfies a zero-one law. Inspired by this asymptotic analysis of first-order properties, we ask whether GNNs satisfy a zero-one law. As the input of a GNN is a graph with node features, we need to reformulate the statement of the law to incorporate these features.

**Definition 4.2**.: Let \(\mathcal{G}=(\mathbf{A},\mathbf{X})\) be a graph with node features, where \(\mathbf{A}\sim\mathbb{G}(n,r)\) is a graph adjacency matrix and, independently, \(\mathbf{X}\) is a matrix of node embeddings, where \(\mathbf{x}_{v}\sim\mathbb{D}(d)\) for every node \(v\). A graph invariant \(\xi\) for graphs with node features satisfies a _zero-one law_ with respect to \(\mathbb{G}(n,r)\) and \(\mathbb{D}(d)\) if, as \(n\) tends to infinity, the probability that \(\xi(\mathcal{G})=1\) tends to either \(0\) or \(1\).

Studying the asymptotic behavior of GNNs helps to shed light on their capabilities and limitations. A zero-one law establishes a limit on the ability of such models to extrapolate to larger graphs: any GNN fitted to a finite set of datapoints will tend towards outputting a constant value on larger and larger graphs drawn from the distribution described above. A zero-one law in this setting also transfers to a corresponding zero-one law for GNNs with random features. This establishes an upper-bound on the uniform expressive power of such models.

### Graph convolutional networks obey a zero-one law

Our main result in this subsection is that (Boolean) GCN classifiers obey a zero-one law. To achieve our result, we place some mild conditions on the model and initial node embeddings.

First, our study covers sub-Gaussian random vectors, which in particular include all _bounded random vectors_, and all _multivariate normal random vectors_. We note that in every practical setup all node features have bounded values (determined by the bit length of the storage medium), and are thus sub-Gaussian.

**Definition 4.3**.: A random vector \(\mathbf{x}\in\mathbb{R}^{d}\) is _sub-Gaussian_ if there is \(C>0\) such that for every unit vector \(\boldsymbol{y}\in\mathbb{R}^{d}\) the random variable \(\mathbf{x}\cdot\boldsymbol{y}\) satisfies the _sub-Gaussian property_; that is, for every \(t>0\):

\[\mathbb{P}(|\mathbf{x}\cdot\boldsymbol{y}|\geq t)\leq 2\exp\left(-\frac{t^{2}}{ C^{2}}\right)\]

Second, we require that the non-linearity \(\sigma\) be Lipschitz continuous. This is again a mild assumption, because all non-linearities used in practice are Lipschitz continuous, including \(\mathrm{ReLU}\), clipped \(\mathrm{ReLU}\), \(\mathrm{sigmoid}\), linearized \(\mathrm{sigmoid}\) and \(\tanh\).

**Definition 4.4**.: A function \(f\colon\mathbb{R}\to\mathbb{R}\) is _Lipschitz continuous_ if there is \(C>0\) such that for any \(x,y\in\mathbb{R}\) it holds that \(|f(x)-f(y)|\leq C|x-y|\).

Third, we place a condition on the GCN weights with respect to the classifier function \(\mathfrak{C}\colon\mathbb{R}^{d(T)}\to\mathbb{B}\), which intuitively excludes a specific weight configuration.

**Definition 4.5**.: Consider a distribution \(\mathbb{D}(d)\) with mean \(\boldsymbol{\mu}\). Let \(\mathcal{M}\) be a GCN used for binary graph classification. Define the sequence \(\boldsymbol{\mu}_{0},\ldots,\boldsymbol{\mu}_{T}\) of vectors inductively by \(\boldsymbol{\mu}_{0}:=\boldsymbol{\mu}\) and \(\boldsymbol{\mu}_{t}:=\sigma(\boldsymbol{W}_{n}^{(t)}\boldsymbol{\mu}_{t-1}+ \boldsymbol{b}^{(t)})\). The classifier classifier \(\mathfrak{C}:\mathbb{R}^{d(T)}\to\mathbb{B}\) is _non-splitting_ for \(\mathcal{M}\) if the vector \(\boldsymbol{\mu}_{T}\) does not lie on a decision boundary for \(\mathfrak{C}\).

For all reasonable choices of \(\mathfrak{C}\), the decision boundary has dimension lower than the \(d(T)\), and is therefore a set of zero-measure. This means that in practice essentially all classifiers are non-splitting.

Given these conditions, we are ready to state our main theorem:

**Theorem 4.6**.: _Let \(\mathcal{M}\) be a GCN used for binary graph classification and take \(r\in[0,1]\). Then, \(\mathcal{M}\) satisfies a zero-one law with respect to graph distribution \(\mathbb{G}(n,r)\) and feature distribution \(\mathbb{D}(d)\) assuming the following conditions hold: (i) the distribution \(\mathbb{D}(d)\) is sub-Gaussian, (ii) the non-linearity \(\sigma\) is Lipschitz continuous, (iii) the graph-level representation uses average pooling, and (iv) the classifier \(\mathfrak{C}\) is non-splitting._

The proof hinges on a probabilistic analysis of the preactivations in each layer. We use a sub-Gaussian concentration inequality to show that the deviation of each of the first-layer preactivations \(\mathbf{y}_{v}^{(1)}\) from its expected value becomes less and less as the number of node \(n\) tends to infinity. Using this and the fact that \(\sigma\) is Lipschitz continuous we show then that each activation \(\mathbf{x}_{v}^{(1)}\) tends towards a fixed value. Iterating this analysis through all the layers of the network yields the following key lemma, which is the heart of the argument.

**Lemma 4.7**.: _Let \(\mathcal{M}\) and \(\mathbb{D}(d)\) satisfy the conditions in Theorem 4.6. Then, for every layer \(t\), there is \(\boldsymbol{z}_{t}\in\mathbb{R}^{d(t)}\) such that when sampling a graph with node features from \(\mathbb{G}(n,r)\) and \(\mathbb{D}(d)\), for every \(i\in\{1,\ldots,d(t)\}\) and for every \(\epsilon>0\) we have that:_

\[\mathbb{P}\left(\forall v\colon\following intuitive argument for why the rate of convergence should decrease as the embedding dimensionality increases: if we fix a node \(v\) and a layer \(t\) then each of the components of its preactivation can be viewed as the weighted sum of \(d(t-1)\) random variables, each of which is the aggregation of activations in the previous layer. Intuitively, as \(d(t-1)\) increases, the variance of this sum also increases. This increased variance propagates through the network, resulting in a higher variance for the final node representations and thus a slower convergence.

Using analogous assumptions and techniques to those presented in this section, we also establish a zero-one law for MeanGNN\({}^{+}\), which we report in detail in Appendix B. Abstracting away from technicalities, the overall structure of the proofs for MeanGNN\({}^{+}\) and GCN is very similar, except that for the former case we additionally need to take care of the global readout component.

### Graph neural networks with sum aggregation obey a zero-one law

The other variant of GNNs we consider are those with sum aggregation. The proof in the case works rather differently, and we place different conditions on the model.

**Definition 4.8**.: A function \(\sigma\colon\mathbb{R}\to\mathbb{R}\) is _eventually constant in both directions_ if there are \(x_{-\infty},x_{\infty}\in\mathbb{R}\) such that \(\sigma(y)\) is constant for \(y<x_{-\infty}\) and \(\sigma(y)\) is constant for \(y>x_{\infty}\). We write \(\sigma_{-\infty}\) to denote the minimum and \(\sigma_{\infty}\) to denote the maximum value of an eventually constant function \(\sigma\).

This means that there is a threshold (\(x_{-\infty}\)) below which sigma is constant, and another threshold (\(x_{\infty}\)) above which sigma is constant. Both the linearized \(\mathrm{sigmoid}\) and clipped \(\mathrm{ReLU}\) are eventually constant in both directions. Moreover, when working with finite precision any function with vanishing gradient in both directions (such as \(\mathrm{sigmoid}\)) can be regarded as eventually constant in both directions.

We also place the following condition on the weights of the model with respect to the mean of \(\mathbb{D}(d)\) and the edge-probability \(r\).

**Definition 4.9**.: Let \(\mathcal{M}\) be any SumGNN\({}^{+}\) for binary graph classification with a non-linearity \(\sigma\) which is eventually constant in both directions. Let \(\mathbb{D}(d)\) be any distribution with mean \(\mathbf{\mu}\), and let \(r\in[0,1]\). Then, the model \(\mathcal{M}\) is _synchronously saturating_ for \(\mathbb{G}(n,r)\) and \(\mathbb{D}(d)\) if the following conditions hold:

1. For each \(1\leq i\leq d(1)\): \[\left[(r\mathbf{W}_{n}^{(1)}+\mathbf{W}_{g}^{(1)})\mathbf{\mu}\right]_{i}\neq 0\]
2. For every layer \(1<t\leq T\), for each \(1\leq i\leq d(t)\) and for each \(\mathbf{z}\in\{\sigma_{-\infty},\sigma_{\infty}\}^{d(t-1)}\): \[\left[(r\mathbf{W}_{n}^{(t)}+\mathbf{W}_{g}^{(t)})\mathbf{z}\right]_{i}\neq 0\]

Analysis of our proof of the zero-one law for SumGNN\({}^{+}\) models (Theorem 4.10 below) reveals that the asymptotic behaviour is determined by the matrices \(\mathbf{Q}_{t}\coloneqq r\mathbf{W}_{n}^{(t)}+\mathbf{W}_{g}^{(t)}\), where the asymptotic final layer embeddings are \(\sigma(\mathbf{Q}_{T}(\sigma(\mathbf{Q}_{T-1}\cdots\sigma(\mathbf{Q}_{0}\mathbf{\mu})\cdots)))\).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).)).).).).).).).)).).).).).).)).).).).)).).).)).).).)).).).)).).)).).).)).).)).).).).)).).)).).).)).)).).)).).

The proof works differently than the GCN and MeanGNN\({}^{+}\) cases, but still rests on a probabilistic analysis of the preactivations in each layer. Assuming that \(\mathcal{M}\) is synchronously saturating for \(\mathbb{G}(n,r)\) and \(\mathbb{D}(d)\), we can show that the expected absolute value of each preactivation tends to infinity as the number of nodes increases, and that moreover the probability that it lies below any fixed value tends to \(0\) exponentially. Hence, the probability that all node embeddings after the first layer are the same and have components which are all \(\sigma_{-\infty}\) or \(\sigma_{\infty}\) tends to \(1\). We then extend this analysis to further layers, using the fact that \(\mathcal{M}\) is synchronously saturating, which yields inductively that all node embeddings are the same with probability tending to \(1\), resulting in the following key lemma.

**Lemma 4.11**.: _Let \(\mathcal{M}\), \(\mathbb{D}(d)\) and \(r\) be as in Theorem 4.10. Let \(\sigma_{-\infty}\) and \(\sigma_{\infty}\) be the extremal values taken by the non-linearity. Then, for every layer \(t\), there is \(\mathbf{z}_{t}\in\{\sigma_{-\infty},\sigma_{\infty}\}^{d(t)}\) such that when we sample graphs with node features from \(\mathbb{G}(n,r)\) and \(\mathbb{D}(d)\) the probability that \(\mathbf{\mathrm{x}}_{v}^{(t)}=\mathbf{z}_{t}\) for every node \(u\) tends to \(1\) as \(n\) tends to infinity._

The final classification output must therefore be the same asymptotically, since its input consists of node embeddings which always take the same value.

## 5 Graph neural networks with random node features

Up to this point we have been considering the graph plus node features as the (random) input to GNNs. In this section, we make a change in perspective and regard the initial node features as part of the model, so that its input consists solely of the graph without features. We focus in this section on SumGNN\({}^{+}\). Adding random initial features to GNNs is known to increase their power [32].

Note that Theorem 4.10 immediately yields a zero-one law for these models. This places restrictions on what can be expressed by SumGNN\({}^{+}\) models with random features subject to the conditions of Theorem 4.10. For example, it is not possible to express that the number of graph nodes is even, since the property of being even does not satisfy a zero-one law with respect to any \(r\in[0,1]\).

It is natural to wonder how tight these restrictions are: what precisely is the class of functions which can be approximated by these models? Let us first formalize the notion of approximation.

**Definition 5.1**.: Let \(f\) be a Boolean function on graphs, and let \(\zeta\) be a random function on graphs. Take \(\delta>0\). Then \(\zeta\)_uniformly \(\delta\)-approximates \(f\)_ if:

\[\forall n\in\mathbb{N}\colon\ \mathbb{P}(\zeta(G)=f(G)\mid|G|=n)\geq 1-\delta\]

when we sample \(G\sim\mathbb{G}(n,1/2)\).

The reason for sampling graphs from \(\mathbb{G}(n,1/2)\) is that under this distribution all graphs on \(n\) nodes are equally likely. Therefore, the requirement is the same as that for every \(n\in\mathbb{N}\) the proportion of \(n\)-node graphs on which \(\zeta(G)=f(G)\) is at least \(1-\delta\).

Building on results due to Abboud et al. [1], we show a partial converse to Theorem 4.10: if a graph invariant satisfies a zero-one law for \(\mathbb{G}(n,1/2)\) then it can be universally approximated by a SumGNN\({}^{+}\) with random node features.

**Theorem 5.2**.: _Let \(\xi\) be any graph invariant which satisfies a zero-one law with respect to \(\mathbb{G}(n,1/2)\). Then, for every \(\delta>0\) there is a SumGNN\({}^{+}\) with random node features \(\mathcal{M}\) which uniformly \(\delta\)-approximates \(\xi\)._

The basis of the proof is a result due to Abboud et al. [1] which states that a SumGNN\({}^{+}\) with random node features can approximate any graph invariant _on graphs of bounded size_. When the graph invariant satisfies a zero-one law, we can use the global readout to count the number of nodes. Below a certain threshold, we use the techniques of Abboud et al. [1] to approximate the invariant, and above the threshold we follow its asymptotic behavior. We emphasise that the combination of these techniques yields a model which provides an approximation which is uniform across all graph sizes.

## 6 Experimental evaluation

We empirically verify our theoretical findings on a carefully designed synthetic experiment using ER graphs with random features. The goal of these experiments is to answer the following questions for each model under consideration:

**Q1.** Do we empirically observe a zero-one law?

**Q2.** What is the rate of convergence like empirically?

**Q3.** What is the impact of the number of layers on the convergence?

### Experimental setup

We report experiments for GCN, MeanGNN, and SumGNN. The following setup is carefully designed to eliminate confounding factors:

* We consider \(10\) GNN models of the same architecture each with _randomly initialized weights_, where each weight is sampled independently from \(U(-1,1)\). The non-linearity is eventually constant in both directions: identity between \([-1,1]\), and truncated to \(-1\) if the input is smaller than \(-1\), and \(1\) if the input is greater than \(1\). In the appendix we include additional experiments in which test other choices of non-linearity (see Appendix E.2). We apply mean pooling to yield a final representation \(\mathbf{z}_{G}\in\mathbb{R}^{d}\) of the input graph.
* For every model, we apply a final classifier \(\sigma(f):\mathbb{R}^{d}\rightarrow\mathbb{B}\) where \(f\) is a 2-layer MLP with _random weights_ and with \(\tanh\) activation, which outputs a real value, and \(\sigma\) is the sigmoid function. Graphs are classified as \(1\) if the output of the sigmoid is greater than \(0.5\), and \(0\) otherwise.
* The input graphs are drawn from \(\mathbb{G}(n,1/2)\) with corresponding node features independently drawn from \(U(0,1)\).
* We conduct these experiments with three choices of layers: \(10\) models with \(T=1\) layer, \(10\) models with \(T=2\) layers, and \(10\) models with \(T=3\) layers.

The goal of these experiments is to understand the behavior of the respective GNN graph classifiers with mean-pooling, as we draw larger and larger ER graphs. Specifically, each model classifies graphs of varying sizes, and we are interested in knowing _how the proportion of the graphs which are classified as \(1\) evolves, as we increase the graph sizes_.

We independently sample 10 models to ensure this is not a model-specific behavior, aiming to observe the same phenomenon across the models. If there is a zero-one law, then for each model, we should only see two types of curves: either tending to \(0\) or tending to \(1\), as graph sizes increase. Whether it will tend to \(0\) or \(1\) depends on the final classifier: since each of these are independent MLPs with random weights the specific outcome is essentially random.

We consider models with up to \(3\) layers to ensure that the node features do not become alike because of the orthogonal over-smoothing issue [24], which surfaces with increasing number of layers. A key feature of our theoretical results is that they do not depend on the number of layers, and this is an aspect which we wish to validate empirically. Using models with random weights is a neutral setup, and random GNNs are widely used in the literature as baseline models [36], as they define valid graph convolutions and tend to perform reasonably well.

### Empirical results

We report all results in Figure 1 for all models considered and discuss them below. Each plot in this figure depicts the curves corresponding to the behavior of independent models with random weights.

**GCN.** For this experiment, we use an embedding dimensionality of \(128\) for each GCN model and draw graphs of sizes up \(5000\), where we take \(32\) samples of each size. The key insight of Theorem 4.6 is that the final mean-pooled embedding vector \(\mathbf{z}_{G}\) tends to a constant vector as we draw larger graphs. Applying an MLP followed by a sigmoid function will therefore map \(\mathbf{z}_{G}\) to either \(0\) or \(1\), showing a zero-one law. It is evident from Figure 1 (top row) that all curves tend to either \(0\) or \(1\), confirming our expectation regarding the outcome of these experiments for GCNs. Moreover, this holds regardless of the number of layers considered. Since the convergence occurs quickly, already around graphs of size of \(1000\), we did not experiment with larger graph sizes in this experiment.

**MeanGNN.** Given that the key insight behind this result is essentially similar to that of Theorem 4.6, we follow the exact same configuration for these models as for GCNs. The proof structure is the same in both cases: we show that the preactivations and activations become closer and closer tosome fixed values as the number of nodes increases. Moreover, comparing the summations in the definitions of GCN and MeanGNN, on a typical ER graph drawn from \(\mathbb{G}(n,1/2)\) we would expect each corresponding summand to have a similar value, since \(\sqrt{|\mathcal{N}(u)||\mathcal{N}(v)|}\) should be close to \(|\mathcal{N}^{+}(v)|\). Figure 1(mid row) illustrates the results for MeanGNN and the trends are reassuringly similar to those of GCNs: all models converge quickly to either \(0\) and \(1\) with all choices of layers. Interestingly, the plots for GCN and MeanGNN models are almost identical. We used the same seed when drawing each of the model weights, and the number of parameters is the same between the two. Hence, the GCN models were parameterized with the same values as the MeanGNN models. The fact that each pair of models preforms near identically confirms the expectation that the two architectures work in similar ways on ER graphs.

**SumGNN.** Theorem 4.10 shows that, as the number of nodes grow, the embedding vector \(\mathbf{z}_{v}\) of each _node_\(v\) will converge to a constant vector with high probability. Hence, when we do mean-pooling at the end, we expect to get the same vector for different graphs of the same size. The mechanism by which a zero-one law is arrived at is quite different compared with the GCN and MeanGNN case. In particular, in order for the embedding vectors to begin to converge, there must be sufficiently many nodes so that the preactivations surpass the thresholds of the non-linearity. For this experiment, we use a smaller embedding dimensionality of \(64\) for each SumGNN model and draw graphs of sizes up to \(100000\), where we take \(32\) samples of each size. Figure 1 shows the results for SumGNN. Note that we observe a slower convergence than with GCN or MeanGNN.

Figure 1: Each plot shows the proportion of graphs of certain size which are classified as \(1\) by a set of ten GCNs (top row), MeanGNNs (middle), and SumGNNs (bottom row). Each curve (color-coded) shows the behavior of a model, as we draw increasingly larger graphs. The phenomenon is observed for 1-layer models (left column), 2-layer models (mid column), and 3-layer models (last column). GCNs and MeanGNNs behave very similarly with all models converging quickly to \(0\) or to \(1\). SumGNNs shows slightly slower convergence, but all models perfectly converge in all layers.

Limitations, discussion, and outlook

The principal limitations of our work come from the assumptions placed on the main theorems. In our formal analysis, we focus on graphs drawn from the ER distribution. From the perspective of characterizing the _expressiveness_ of GNNs this is unproblematic, and accords with the classical analysis of first-order properties of graphs. However, when considering the _extrapolation capacity_ of GNNs, other choices of distributions may be more realistic. In Appendices E.4 and E.5 we report experiments in which zero-one laws are observed empirically for sparse ER graphs and Barabasi-Albert graphs [3], suggesting that formal results may be obtainable. While we empirically observe that GNNs converge to their asymptotic behaviour very quickly, we leave it as future work to rigorously examine the rate at which this convergence occurs.

In this work we show that GNNs with random features can at most capture properties which follow a zero-one law. We complement this with an _almost_ matching lower bound: Theorem 5.2 currently requires a graph invariant \(\xi\) which obeys a zero-one law with respect to a specific value of \(r\) (i.e., \(1/2\)), and if this assumption could be relaxed, it would yield a complete characterization of the expressive power of these models.

## 8 Acknowledgments

We would like to thank the anonymous reviewers for their feedback, which lead to several improvements in the presentation of the paper. The first author was supported by an EPSRC studentship with project reference _2271793_.

## References

* Abboud et al. [2021] Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The surprising power of graph neural networks with random node initialization. In _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI_, pages 2112-2118, 2021.
* Alon and Yahav [2021] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In _Proceedings of the Ninth International Conference on Learning Representations, ICLR_, 2021.
* Barabasi and Albert [1999] Albert-Laszlo Barabasi and Reka Albert. Emergence of scaling in random networks. _Science_, 286(5439):509-512, 1999.
* Barcelo et al. [2020] Pablo Barcelo, Egor V. Kostylev, Mikael Monet, Jorge Perez, Juan L. Reutter, and Juan Pablo Silva. The logical expressiveness of graph neural networks. In _Proceedings of the Eighth International Conference on Learning Representations, ICLR_, 2020.
* Battaglia et al. [2018] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks, 2018.
* Bollobas [2001] Bela Bollobas. _Random Graphs_. Cambridge Studies in Advanced Mathematics. Cambridge University Press, second edition, 2001.
* Duvenaud et al. [2015] David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gomez-Bombarelli, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In _Proceedings of the Twenty-Eighth Annual Conference on Advances in Neural Information Processing Systems, NIPS_, pages 2224-2232, 2015.
* Fagin [1976] Ronald Fagin. Probabilities on finite models. _The Journal of Symbolic Logic, JSL_, 41(1):50-58, 1976. ISSN 00224812.
* Flach [2010] Peter A. Flach. First-order logic. In Claude Sammut and Geoffrey I. Webb, editors, _Encyclopedia of Machine Learning_, pages 410-415. Springer US, Boston, MA, 2010. ISBN 978-0-387-30164-8. doi: 10.1007/978-0-387-30164-8_311.

* Fout et al. [2017] Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph convolutional networks. In _Proceedings of the Thirtieth Annual Conference on Advances in Neural Information Processing Systems, NIPS_, pages 6530-6539, 2017.
* Gama et al. [2020] Fernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability properties of graph neural networks. _IRE Transactions on Audio_, 68:5680-5695, 2020. ISSN 1053-587X.
* Gilmer et al. [2017] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In _Proceedings of the Thirty-Fourth International Conference on Machine Learning, ICML_, pages 1263-1272, 2017.
* Glebskii et al. [1969] Yu V Glebskii, DI Kogan, MI Liogonkii, and VA Talanov. Volume and fraction of satisfiability of formulas of the lower predicate calculus. _Kibernetika_, 2:17-27, 1969.
* Gori et al. [2005] Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In _Proceedings of the 2005 IEEE International Joint Conference on Neural Networks, IJCNN_, volume 2, pages 729-734, 2005.
* Grohe [2023] M. Grohe. The descriptive complexity of graph neural networks. In _2023 38th Annual ACM/IEEE Symposium on Logic in Computer Science, LICS_, pages 1-14, Los Alamitos, CA, USA, jun 2023. IEEE Computer Society. doi: 10.1109/LICS56636.2023.10175735.
* Grohe [2021] Martin Grohe. The logic of graph neural networks. In _Proceedings of the 36th Annual ACM/IEEE Symposium on Logic in Computer Science, LICS_, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781665448956.
* Hamilton et al. [2017] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _Proceedings of the Thirty-First Annual Conference on Advances in Neural Information Processing Systems, NIPS_. Curran Associates, Inc., 2017.
* Kearnes et al. [2016] Steven M. Kearnes, Kevin McCloskey, Marc Berndl, Vijay S. Pande, and Patrick Riley. Molecular graph convolutions: moving beyond fingerprints. _Journal of Computer Aided Molecular Design_, 30(8):595-608, 2016.
* Keriven and Peyre [2019] Nicolas Keriven and Gabriel Peyre. Universal invariant and equivariant graph neural networks. In _Proceedings of the Thirty-Second Annual Conference on Advances in Neural Information Processing Systems, NeurIPS_, pages 7090-7099, 2019.
* Keriven et al. [2020] Nicolas Keriven, Alberto Bietti, and Samuel Vaiter. Convergence and stability of graph convolutional networks on large random graphs. In _Proceedings of the Thirty-Fourth Annual Conference on Advances in Neural Information Processing Systems, NeurIPS_, pages 21512-21523. Curran Associates, Inc., 2020.
* Kipf and Welling [2017] Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _Proceedings of the Fifth International Conference on Learning Representations, ICLR_, 2017.
* Levie et al. [2021] Ron Levie, Wei Huang, Lorenzo Bucci, Michael Bronstein, and Gitta Kutyniok. Transferability of spectral graph convolutional neural networks. _Journal of Machine Learning Research_, 22(1), 2021. ISSN 1532-4435.
* Li et al. [2018] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI_, pages 3538-3545, 2018.
* Li et al. [2016] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. In _Proceedings of the Fourth International Conference on Learning Representations, ICLR_, 2016.
* Libkin [2004] Leonid Libkin. _Zero-One Laws_, pages 235-248. Springer Berlin Heidelberg, Berlin, Heidelberg, 2004. ISBN 978-3-662-07003-1.
* Loukas [2020] Andreas Loukas. What graph neural networks cannot learn: depth vs width. In _Proceedings of the Eighth International Conference on Learning Representations, ICLR_, 2020.

* Maron et al. [2019] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. In _Proceedings of the Thirty-Second Annual Conference on Advances in Neural Information Processing Systems, NeurIPS_, pages 2153-2164, 2019.
* Maron et al. [2019] Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant networks. In _Proceedings of the Thirty-Sixth International Conference on Machine Learning, ICML_, pages 4363-4371, 2019.
* Morris et al. [2019] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and Leman go neural: Higher-order graph neural networks. In _Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence, AAAI_, pages 4602-4609, 2019.
* Oono and Suzuki [2020] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. In _Proceedings of the Eighth International Conference on Learning Representations, ICLR_, 2020.
* Ruiz et al. [2020] Luana Ruiz, Luiz Chamon, and Alejandro Ribeiro. Graphon neural networks and the transferability of graph neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Proceedings of the Thirty-Fourth Annual Conference on Advances in Neural Information Processing Systems, NeurIPS_, pages 1702-1712. Curran Associates, Inc., 2020.
* Sato et al. [2021] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural networks. In _Proceedings of the 2021 SIAM International Conference on Data Mining, SDM_, pages 333-341, 2021.
* Scarselli et al. [2009] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. _IEEE Transactions on Neural Networks_, 20(1):61-80, 2009.
* Shelah and Spencer [1988] Saharon Shelah and Joel Spencer. Zero-one laws for sparse random graphs. _Journal of the American Mathematical Society_, 1(1):97-115, 1988. ISSN 08940347, 10886834.
* Shlomi et al. [2021] Jonathan Shlomi, Peter Battaglia, and Jean-Roch Vlimant. Graph neural networks in particle physics. _Machine Learning: Science and Technology_, 2(2):021001, 2021.
* Thompson et al. [2022] Rylee Thompson, Boris Knyazev, Elahe Ghalebi, Jungtaek Kim, and Graham W. Taylor. On evaluation metrics for graph generative models. In _Proceedings of the Tenth International Conference on Learning Representations, ICLR_, 2022.
* Velickovic et al. [2018] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _Proceedings of the Sixth International Conference on Learning Representations, ICLR_, 2018.
* Vershynin [2018] Roman Vershynin. _High-dimensional probability_. Number 47 in Cambridge series on statistical and probabilistic mathematics. Cambridge University Press, Cambridge, 2018. ISBN 9781108415194.
* Xu et al. [2019] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _Proceedings of the Seventh Annual Conference on Learning Representations, ICLR_, 2019.
* Xu et al. [2021] Keyulu Xu, Mozhi Zhang, Jingling Li, Simon Shaolei Du, Ken-Ichi Kawarabayashi, and Stefanie Jegelka. How neural networks extrapolate: From feedforward to graph neural networks. In _Proceedings of the Ninth International Conference on Learning Representations, ICLR_, 2021.
* Yehudai et al. [2020] Gilad Yehudai, Ethan Fetaya, Eli A. Meirom, Gal Chechik, and Haggai Maron. From local structures to size generalization in graph neural networks. In _International Conference on Machine Learning, ICML_, 2020.
* Zitnik et al. [2018] Marinka Zitnik, Monica Agrawal, and Jure Leskovec. Modeling polypharmacy side effects with graph convolutional networks. _Bioinformatics_, 34(13):i457-i466, 2018.

Proof of the zero-one law for GCN

The proof of Lemma 4.7 and subsequently Theorem 4.6 relies on an asymptotic analysis of the distributions of the node embeddings at each layer. The following famous concentration inequality for sub-Gaussian random variables allows us to put bounds on the deviation of a sum of random variables from its expected value.

**Theorem A.1** (Hoeffding Inequality for sub-Gaussian random variables).: _There is a universal constant \(c\) such that the following holds. Let \(\mathrm{z}_{1},\ldots,\mathrm{z}_{N}\) be independent sub-Gaussian scalar random variables with mean \(0\). Assume that the constants \(C\) from Definition 4.3 for each \(\mathrm{z}_{i}\) can be bounded by \(K\). Then for all \(t>0\):_

\[\mathbb{P}\left(\left|\sum_{i=1}^{N}\mathrm{z}_{i}\right|\geq t\right)\leq \exp\left(-\frac{ct^{2}}{K^{2}N}\right)\]

Proof.: See Theorem 2.6.2 in [38]. 

We also make use of the following three basic facts about sub-Gaussian random variables.

**Lemma A.2**.: _If \(\mathbf{z}\) is a sub-Gaussian random vector and \(\mathbf{q}\) is any vector of the same dimension then \(\mathbf{q}\cdot\mathbf{z}\) is sub-Gaussian._

Proof.: This follows directly from Definition 4.3. 

**Lemma A.3**.: _If \(\mathbf{z}\) is a sub-Gaussian scalar random variable then so is \(\mathbf{z}-\mathbb{E}[\mathrm{z}]\)._

Proof.: See Lemma 2.6.8 in [38]. 

**Lemma A.4**.: _If \(\mathrm{z}\) is a sub-Gaussian scalar random variable and \(\mathrm{a}\) is an independent Bernoulli random variable then \(\mathrm{z}\) is sub-Gaussian._

Proof.: Let \(C\) be the constant given by Definition 4.3 for \(\mathrm{z}\). Let a take values \(\alpha\) and \(\beta\). Using the Law of Total Probability:

\[\mathbb{P}(|\mathrm{z}\mathrm{a}|\geq t) =\mathbb{P}(|\mathrm{z}\mathrm{a}|\geq t\mid\mathrm{a}=\alpha) \,\mathbb{P}(\mathrm{a}=\alpha)+\mathbb{P}(|\mathrm{z}\mathrm{a}|\geq t\mid \mathrm{a}=\beta)\,\mathbb{P}(\mathrm{a}=\beta)\] \[=\mathbb{P}(|\mathrm{z}|\geq t/|\alpha|)\,\mathbb{P}(\mathrm{a}= \alpha)+\mathbb{P}(|\mathrm{z}|\geq t/|\beta|)\,\mathbb{P}(\mathrm{a}=\beta)\] \[\leq 2\exp\left(-\frac{t^{2}}{|\alpha|^{2}C^{2}}\right)\mathbb{P }(\mathrm{a}=\alpha)+2\exp\left(-\frac{t^{2}}{|\beta|^{2}C^{2}}\right)\mathbb{ P}(\mathrm{a}=\beta)\] \[\leq 2\exp\left(-\frac{t^{2}}{\max\{|\alpha|,|\beta|\}^{2}C^{2}}\right)\]

Therefore \(\mathrm{z}\mathrm{a}\) is sub-Gaussian. 

We first prove the key lemma regarding the node embeddings.

Proof of Lemma 4.7.: Let \(C\) be the Lipschitz constant for \(\sigma\). Start by considering the first layer preactivations \(\mathbf{y}_{v}^{(1)}\) and drop superscript \((1)\)'s for notational clarity. We have that:

\[\mathbf{y}_{v}=\sum_{v\in\mathcal{N}^{+}(v)}\frac{1}{\sqrt{|\mathcal{N}(v)|| \mathcal{N}(u)|}}\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}+\mathbf{b}\]

Fix \(i\in\{1,\ldots,d(1)\}\). The deviation from the expected value in the \(i\)th component is as follows:

\[|[\mathbf{y}_{v}-\mathbb{E}[\mathbf{y}_{v}]]_{i}|=\left|\sum_{u\in\mathcal{N}^ {+}(v)}\frac{1}{\sqrt{|\mathcal{N}(v)||\mathcal{N}(u)|}}\left[\mathbf{W}_{n} \mathbf{x}_{u}^{(0)}-\mathbb{E}\left[\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}\right] \right]_{i}\right|\]

Now every \(|\mathcal{N}(u)|\) is a sum of \(n\) independent \(0\)-\(1\) Bernoulli random variables with success probability \(r\) (since the graph is sampled from an Erdos-Renyi distribution). Since Bernoulli random variables are sub-Gaussian (Lemma A.4) we can use Hoeffding's Inequality to bound the deviation of \(|\mathcal{N}(u)|+1\) from its expected value \(nr+1\). By Theorem A.1 there is a constant \(K\) such that for every \(\gamma\in(0,1)\) and node \(u\):

\[\mathbb{P}(|\mathcal{N}^{+}(u)|\leq\gamma nr) \leq\mathbb{P}(||\mathcal{N}^{+}(u)|-nr|\geq(1-\gamma)nr)\] \[\leq\mathbb{P}(||\mathcal{N}(u)|-nr|\geq(1-\gamma)nr-1)\] \[\leq 2\exp\left(-\frac{K((1-\gamma)nr-1)^{2}}{n}\right)\]

This means that, taking a union bound:

\[\mathbb{P}(\forall u\in V\colon|\mathcal{N}^{+}(u)|\geq\gamma nr)\geq 1-2n\exp \left(-\frac{K_{0}((1-\gamma)nr-1)^{2}}{n}\right)\]

Fix \(i\in\{1,\ldots,d(1)\}\). In the case where \(\forall u\in V\colon|\mathcal{N}^{+}(u)|\geq\gamma nr\) we have that:

\[|[\mathbf{y}_{v}-\mathbb{E}[\mathbf{y}_{v}]]_{i}|\leq\frac{1}{\sqrt{|\mathcal{ N}(v)|\gamma nr}}\Bigg{|}\sum_{u\in\mathcal{N}^{+}(v)}\left[\mathbf{W}_{n}\mathbf{x} _{u}^{(0)}-\mathbb{E}\left[\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}\right]\right]_{i} \Bigg{|}\]

Now, by Lemma A.2 and Lemma A.3 each \(\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}-\mathbb{E}\left[\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}\right]\) is sub-Gaussian. We can thus apply Hoeffding's Inequality (Theorem A.1) to obtain a constant \(K\) such that for every \(t>0\) we have:

\[\mathbb{P}(||\mathbf{y}_{v}-\mathbb{E}[\mathbf{y}_{v}]|_{i}|\geq t) \leq\mathbb{P}\left(\left|\sum_{u\in\mathcal{N}^{+}(v)}\left[\mathbf{ W}_{n}\mathbf{x}_{u}^{(0)}-\mathbb{E}\left[\mathbf{W}_{n}\mathbf{x}_{u}^{(0)} \right]\right]_{i}\right|\right|\geq t\sqrt{|\mathcal{N}(v)|\gamma nr}\right)\] \[\leq 2\exp\left(-\frac{Kt^{2}|\mathcal{N}(v)|\gamma nr}{|\mathcal{ N}^{+}(v)|}\right)\] \[\leq 2\exp\left(-\frac{Kt^{2}\gamma nr}{2}\right)\]

Now using the Law of Total Probability, partitioning depending on whether \(\forall u\in V\colon|\mathcal{N}^{+}(u)|\geq\gamma nr\), we get a bound as follows:

\[\mathbb{P}(||\mathbf{y}_{v}-\mathbb{E}[\mathbf{y}_{v}]|_{i}|\geq t)\leq 2\exp \left(-\frac{Kt^{2}\gamma nr}{2}\right)+2n\exp\left(-\frac{K_{0}((1-\gamma)nr -1)^{2}}{n}\right)\]

From now on fix any \(\gamma\in(0,1)\).

Let \(\mathbf{z}_{1}:=\sigma(\mathbb{E}[\mathbf{y}_{v}])\) for any \(v\) (this is the same for every \(v\)). Applying the bound with \(t=C\epsilon\) we can bound the deviation of \(\mathbf{x}_{v}\) from \(\mathbf{z}_{1}\) as follows, using the Lipschitz constant \(C\).

\[\mathbb{P}(||[\mathbf{x}_{v}-\mathbf{z}_{1}]_{i}|\geq\epsilon) =\mathbb{P}(||[\sigma(\mathbf{y}_{v})-\sigma(\mathbb{E}[\mathbf{y }_{v}])]_{i}|\geq\epsilon)\] \[\leq\mathbb{P}(||\mathbf{y}_{v}-\mathbb{E}[\mathbf{y}_{v}]|_{i}| \geq C\epsilon)\] \[\leq 2\exp\left(-\frac{KC^{2}\epsilon^{2}\gamma nr}{2}\right)+2n \exp\left(-\frac{K_{0}((1-\gamma)nr-1)^{2}}{n}\right)\]

Taking a union bound, the probability that \(|[\mathbf{x}_{v}-\mathbf{z}_{1}]_{i}|<\epsilon\) for every node \(v\) and every \(i\in\{1,\ldots,d(1)\}\) is at least:

\[1-nd(i)\,\mathbb{P}(||[\mathbf{x}_{v}-\mathbf{z}_{1}]_{i}|\geq\epsilon)\]

This tends to \(1\) as \(n\) tends to infinity, which yields the result for the first layer.

Now consider the preactivations for the second layer:

\[\mathbf{y}_{v}^{(2)}=\sum_{u\in\mathcal{N}^{+}(v)}\frac{1}{\sqrt{|\mathcal{N} (v)|\mathcal{N}(u)|}}\mathbf{W}_{n}^{(2)}\mathbf{x}_{u}^{(1)}+\mathbf{b}^{(2)}\]

As in the single layer case, we can bound the probability that any \(|\mathcal{N}(u)|\) is less than some \(\gamma nr\). Condition on the event that \(\forall u\in V\colon|\mathcal{N}^{+}(u)|\geq\gamma nr\).

By applying the result for the first layer to \(\epsilon^{\prime}=\epsilon\sqrt{\gamma r}/(2C\|\mathbf{W}_{n}^{(2)}\|_{\infty})\), we have that for each \(i\in\{1,\ldots,d(2)\}\):

\[\mathbb{P}\left(\forall v\colon\Big{|}\Big{[}\mathbf{x}_{v}^{(1)}-\mathbf{z}_{1} \Big{]}_{i}\Big{|}<\epsilon^{\prime}\right)\to 1\quad\text{as }n\to\infty\]

Condition additionally on the event that \(|[\mathbf{x}_{v}^{(1)}-\mathbf{z}_{1}]_{i}|<\epsilon^{\prime}\) for every node \(v\) and every \(i\in\{1,\ldots,d(1)\}\).

Now define:

\[\mathbf{a}_{2}:=\sum_{v\in\mathcal{N}^{+}(v)}\frac{1}{\sqrt{|\mathcal{N}(v)|| \mathcal{N}(u)|}}\mathbf{W}_{n}^{(2)}\mathbf{z}_{1}+\mathbf{b}^{(2)}\]

Then we have that for every \(i\in\{1,\ldots,d(1)\}\):

\[|[\mathbf{y}_{v}^{(2)}-\mathbf{a}_{2}]_{i}| \leq\left|\sum_{u\in\mathcal{N}^{+}(v)}\frac{1}{\sqrt{|\mathcal{N} (u)||\mathcal{N}(v)|}}\left[\mathbf{W}_{n}^{(2)}(\mathbf{x}_{u}^{(1)}-\mathbf{z}_{1}) \right]_{i}\right|\] \[\leq\frac{1}{\sqrt{|\mathcal{N}(v)|\gamma nr}}\Bigg{|}\sum_{u\in \mathcal{N}^{+}(v)}\left[\mathbf{W}_{n}^{(2)}(\mathbf{x}_{u}^{(1)}-\mathbf{z}_{1}) \right]_{i}\Bigg{|}\] \[\leq\frac{1}{\sqrt{|\mathcal{N}(v)|\gamma nr}}\Big{\|}\mathbf{W}_{n} ^{(2)}\Big{\|}_{\infty}\sum_{u\in\mathcal{N}^{+}(v)}\Big{\|}\mathbf{x}_{u}^{(1 )}-\mathbf{z}_{1}\Big{\|}_{\infty}\] \[\leq\frac{\epsilon|\mathcal{N}^{+}(v)|}{2C\sqrt{|\mathcal{N}(v)|n}}\] \[\leq\frac{\epsilon(n+1)}{2Cn}\] \[\leq\frac{\epsilon}{C}\]

Now let \(\mathbf{z}_{2}:=\sigma(\mathbf{a}_{2})\). As in the single-layer case we can use the bound on \(|[\mathbf{y}_{v}^{(2)}-\mathbf{a}_{2}]_{i}|\) and the fact that \(\sigma\) is Lipschitz to find that, for every node \(v\) and \(i\in\{1,\ldots,d(2)\}\):

\[|[\mathbf{x}_{v}^{(2)}-\mathbf{z}_{2}]_{i}|<\epsilon\]

Since the probability that the two events on which we conditioned tends to \(1\), the result follows for the second layer.

Finally, we apply the argument inductively through the layers to obtain the ultimate result. 

With the key lemma established we can prove the main result.

Proof of Theorem 4.6.: By Lemma 4.7 the final node embeddings \(\mathbf{x}_{v}^{(T)}\) deviate less and less from \(\mathbf{z}_{T}\) as the number of nodes \(n\) increases. Therefore, the average-pooled graph-level representation also deviates less and less from \(\mathbf{z}_{T}\). By inspecting the proof, we can see that this \(\mathbf{z}_{T}\) is exactly the vector \(\mathbf{\mu}_{T}\) in the definition of non-splitting (Definition 4.5). This means that \(\mathbf{z}_{T}\) cannot lie on a decision boundary for the classifier \(\mathfrak{C}\). Hence, there is \(\epsilon>0\) such that \(\mathfrak{C}\) is constant on:

\[\{\mathbf{x}\in\mathbb{R}^{d(T)}\mid\forall i\in\{1,\ldots,d(T)\}\colon[\mathbf{z}_{T }-\mathbf{x}]_{i}<\epsilon\}\]

Therefore, the probability that the output of \(\mathcal{M}\) is \(\mathfrak{C}(\mathbf{z}_{T})\) tends to \(1\) as \(n\) tends to infinity. 

## Appendix B Proof of the zero-one law for MeanSnn\({}^{\star}\)

Let us turn now to establishing a zero-one law for GNNs using mean aggregation. We place the same conditions as with Theorem 4.6. This time the notion of 'non-splitting' is as follows.

**Definition B.1**.: Consider a distribution \(\mathbb{D}(d)\) with mean \(\mathbf{\mu}\). Let \(\mathcal{M}\) be a MeanGNN\({}^{+}\) used for binary graph classification. Define the sequence \(\mathbf{\mu}_{0},\ldots,\mathbf{\mu}_{T}\) of vectors inductively by \(\mathbf{\mu}_{0}:=\mathbf{\mu}\) and \(\mathbf{\mu}_{t}:=\sigma((\mathbf{W}_{n}^{(t)}+\mathbf{W}_{r}^{(t)})\mathbf{\mu}_{t-1}+\mathbf{b}^ {(t)})\). The classifier \(\mathfrak{C}:\mathbb{R}^{d(T)}\to\mathbb{B}\) is _non-splitting_ for \(\mathcal{M}\) if the vector \(\mathbf{\mu}_{T}\) does not lie on a decision boundary for \(\mathfrak{C}\).

Again, in practice essentially all classifiers are non-splitting.

**Theorem B.2**.: _Let \(\mathcal{M}\) be a MeanGNN\({}^{+}\) used for binary graph classification and take \(r\in[0,1]\). Then, \(\mathcal{M}\) satisfies a zero-one law with respect to graph distribution \(\mathbb{G}(n,r)\) and feature distribution \(\mathbb{D}(d)\) assuming the following conditions hold: (i) the distribution \(\mathbb{D}(d)\) is sub-Gaussian, (ii) the non-linearity \(\sigma\) is Lipschitz continuous, (iii) the graph-level representation uses average pooling, (iv) the classifier \(\mathfrak{C}\) is non-splitting._

Note that the result immediately applies to the MeanGNN architecture, since it is a special case of MeanGNN\({}^{+}\).

The overall structure of the proof is the same as for GCN. In particular, we prove the following key lemma stating that all node embeddings tend to fixed values.

**Lemma B.3**.: _Let \(\mathcal{M}\) and \(\mathbb{D}(d)\) satisfy the conditions in Theorem B.2. Then, for every layer \(t\), there is \(\mathbf{z}_{t}\in\mathbb{R}^{d(t)}\) such when sampling a graph with node features from \(\mathbb{G}(n,r)\) and \(\mathbb{D}(d)\), for every \(i\in\{1,\ldots,d(t)\}\) and for every \(\epsilon>0\) we have that:_

\[\mathbb{P}\left(\forall v\colon\left|\left[\mathbf{x}_{v}^{(t)}-\mathbf{z}_{t} \right]_{i}\right|<\epsilon\right)\to 1\quad\text{as }n\to\infty\]

We are ready to present the proofs of the statements. The proof of the key lemma works in a similar way to the GCN case.

Proof of Lemma b.3.: Let \(C\) be the Lipschitz constant for \(\sigma\). Start by considering the first layer preactivations \(\mathbf{y}_{v}^{(1)}\) and drop superscript \((1)\)'s for notational for clarity. We have that:

\[\mathbf{y}_{v}=\frac{1}{|\mathcal{N}^{+}(v)|}\mathbf{W}_{n}\sum_{v\in\mathcal{N}^{ +}(u)}\mathbf{x}_{v}^{(0)}+\frac{1}{n}\mathbf{W}_{r}\sum_{u\in V}\mathbf{x}_{u}^{( 0)}+\mathbf{b}\]

Fix \(i\in\{1,\ldots,d(1)\}\). We can bound the deviation from the expected value as follows:

\[|[\mathbf{y}_{v}-\mathbb{E}[\mathbf{y}_{v}]]_{i}|\leq\frac{1}{| \mathcal{N}^{+}(v)|}\Bigg{|}\sum_{u\in\mathcal{N}^{+}(v)}\left[\mathbf{W}_{n} \mathbf{x}_{u}^{(0)}-\mathbb{E}\left[\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}\right] \right]_{i}\Bigg{|}+\frac{1}{n}\Bigg{|}\sum_{u\in V}\left[\mathbf{W}_{n}\mathbf{x} _{u}^{(0)}-\mathbb{E}\left[\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}\right]\right]_{i} \Bigg{|}\]

By Lemma A.2 and Lemma A.3 both each \(\left[\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}-\mathbb{E}\left[\mathbf{W}_{n}\mathbf{x}_{u}^ {(0)}\right]\right]_{i}\) and each \(\left[\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}-\mathbb{E}\left[\mathbf{W}_{n}\mathbf{x}_{u} ^{(0)}\right]\right]_{i}\) are sub-Gaussian. We can therefore apply Hoeffding's Inequality to their sums. First, by Theorem A.1 there is a constant \(K_{\mathrm{g}}\) such that for any \(t>0\):

\[\mathbb{P}\left(\frac{1}{n}\Bigg{|}\sum_{u\in V}\left[\mathbf{W}_{n} \mathbf{x}_{u}^{(0)}-\mathbb{E}\left[\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}\right] \right]_{i}\right|\leq t\right) =\mathbb{P}\left(\left|\sum_{u\in V}\left[\mathbf{W}_{n}\mathbf{x}_{u }^{(0)}-\mathbb{E}\left[\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}\right]\right]_{i}\right| \leq tn\right)\] \[\leq 2\exp\left(-\frac{K_{\mathrm{g}}t^{2}n^{2}}{n}\right)\] \[=2\exp\left(-K_{\mathrm{g}}t^{2}n\right)\]

Second, applying Theorem A.1 again there is a constant \(K_{\mathrm{n}}\) such that for any \(t>0\):

\[\mathbb{P}\left(\frac{1}{|\mathcal{N}^{+}(v)|}\Bigg{|}\sum_{u\in \mathcal{N}^{+}(v)}\left[\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}-\mathbb{E}\left[\mathbf{W} _{n}\mathbf{x}_{u}^{(0)}\right]\right]_{i}\right|\leq t\right)\] \[=\mathbb{P}\left(\left|\sum_{u\in\mathcal{N}^{+}(v)}\left[\mathbf{W} _{n}\mathbf{x}_{u}^{(0)}-\mathbb{E}\left[\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}\right] \right]_{i}\right|\leq t|\mathcal{N}^{+}(v)|\right)\] \[\leq 2\exp\left(-\frac{K_{\mathrm{n}}t^{2}|\mathcal{N}^{+}(v)|^{2 }}{n}\right)\]Now \(|\mathcal{N}(v)|\) is a the sum of \(n\) independent \(0\)-\(1\) Bernoulli random variables with success probability \(r\). Hence, as in the proof of Lemma 4.7, by Hoeffding's Inequality (Theorem A.1) there is a constant \(K_{0}\) such that for every \(\gamma\in(0,1)\):

\[\mathbb{P}(|\mathcal{N}^{+}(v)|\geq\gamma nr)\leq 2\exp\left(-\frac{K_{0}((1- \gamma)nr-1)^{2}}{n}\right)\]

We can then use the Law of Total Probability, partitioning on whether \(|\mathcal{N}^{+}(v)|\geq\gamma nr\), to get a bound as follows:

\[\mathbb{P}\left(\frac{1}{|\mathcal{N}^{+}(v)|}\Bigg{|}\sum_{u\in \mathcal{N}^{+}(v)}\Big{(}\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}-\mathbb{E}\left[\mathbf{W }_{n}\mathbf{x}_{u}^{(0)}\right]\Big{)}\Bigg{|}\leq t\right)\] \[\qquad\leq 2\exp\left(-\frac{K_{n}t^{2}(\gamma nr)^{2}}{n}\right)+2 \exp\left(-\frac{K_{0}((1-\gamma)nr-1)^{2}}{n}\right)\]

From now on fix any \(\gamma\in(0,1)\).

Finally let \(\mathbf{z}_{1}:=\sigma(\mathbb{E}[\mathbf{y}_{v}])\) for any \(v\) (this is the same for every \(v\)). Applying the two bounds with \(t=C\epsilon/2\) we can bound the deviation of \(\mathbf{x}_{v}\) from \(\mathbf{z}_{1}\) as follows, using the Lipschitz constant \(C\).

\[\mathbb{P}(|[\mathbf{x}_{v}-\mathbf{z}_{1}]_{i}|\geq\epsilon) =\mathbb{P}(|[\sigma(\mathbf{y}_{v})-\sigma(\mathbb{E}[\mathbf{y}_ {v}])]_{i}|\geq\epsilon)\] \[\leq\mathbb{P}(|[\mathbf{y}_{v}-\mathbb{E}[\mathbf{y}_{v}]]_{i}| \geq C\epsilon)\] \[\leq\left(\begin{array}{c}\mathbb{P}\left(\frac{1}{|\mathcal{N }^{+}(v)|}\Big{[}\sum_{u\in\mathcal{N}^{+}(v)}\Big{[}\mathbf{W}_{n}\mathbf{x}_{u}^ {(0)}-\mathbb{E}\left[\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}\right]\Big{]}_{i}\Big{|} \leq\frac{C\epsilon}{2}\right)\\ +\mathbb{P}\left(\frac{1}{n}\Big{|}\sum_{u\in V}\Big{[}\mathbf{W}_{n}\mathbf{x}_{u }^{(0)}-\mathbb{E}\left[\mathbf{W}_{n}\mathbf{x}_{u}^{(0)}\right]\Big{]}_{i}\Big{|} \leq\frac{C\epsilon}{2}\right)\end{array}\right)\] \[\leq\left(\begin{array}{c}2\exp\left(-\frac{K_{0}((C\epsilon \gamma)r)^{2}n}{4}\right)\\ +2\exp\left(-\frac{K_{0}((1-\gamma)nr-1)^{2}}{n}\right)\\ +2\exp\left(-K_{\mathrm{g}}\frac{(C\epsilon)^{2}n}{4}\right)\end{array}\right)\]

Taking a union bound, the probability that \(|[\mathbf{x}_{v}-\mathbf{z}_{1}]_{i}|<\epsilon\) for every node \(v\) and every \(i\in\{1,\ldots,d(1)\}\) is at least:

\[1-nd(i)\,\mathbb{P}(|[\mathbf{x}_{v}-\mathbf{z}_{1}]_{i}|\geq\epsilon)\]

This tends to \(1\) as \(n\) tends to infinity.

Let us turn now to the second layer. By applying the above result for the first layer to \(\epsilon^{\prime}=\epsilon/(C\max\{\|\mathbf{W}_{n}^{(2)}\|_{\infty}),\|\mathbf{W}_{r} ^{(2)}\|_{\infty}\}\), we have that for each \(i\in\{1,\ldots,d(2)\}\):

\[\mathbb{P}\left(\forall v\colon\Big{|}\Big{[}\mathbf{x}_{v}^{(1)}-\mathbf{z}_{1} \Big{]}_{i}\Big{|}<\epsilon^{\prime}\right)\to 1\quad\text{as }n\to\infty\]

Condition on the event that \(|[\mathbf{x}_{v}^{(1)}-\mathbf{z}_{1}]_{i}|<\epsilon^{\prime}\) for every node \(v\) and every \(i\in\{1,\ldots,d(1)\}\).

Fix \(v\) and consider the second-layer preactivations:

\[\mathbf{y}_{v}^{(2)}=\frac{1}{|\mathcal{N}^{+}(v)|}\mathbf{W}_{n}^{(2)}\sum_{u\in \mathcal{N}^{+}(v)}\mathbf{x}_{u}^{(1)}+\frac{1}{n}\mathbf{W}_{r}^{(2)}\sum_{u\in V }\mathbf{x}_{u}^{(1)}+\mathbf{b}^{(2)}\]

Define:

\[\mathbf{a}_{2}:=\frac{1}{|\mathcal{N}^{+}(v)|}\mathbf{W}_{n}^{(2)}\sum_{u\in\mathcal{N }^{+}(v)}\mathbf{z}_{1}+\frac{1}{n}\mathbf{W}_{r}^{(2)}\sum_{u\in V}\mathbf{z}_{1}+\mathbf{b}^ {(2)}\]

Fix \(i\in\{1,\ldots,d(2)\}\). Then:

\[\Big{|}[\mathbf{y}_{v}^{(2)}-\mathbf{a}_{2}]_{i}\Big{|} =\left|\frac{1}{|\mathcal{N}^{+}(v)|}\mathbf{W}_{n}^{(2)}\sum_{u\in \mathcal{N}^{+}(v)}(\mathbf{x}_{u}^{(1)}-\mathbf{z}_{1})+\frac{1}{n}\mathbf{W}_{r}^{(2 )}\sum_{u\in V}(\mathbf{x}_{u}^{(1)}-\mathbf{z}_{1})\right|\] \[\leq\frac{1}{|\mathcal{N}^{+}(v)|}\Big{\|}\mathbf{W}_{n}^{(2)}\Big{\|} \sum_{u\in\mathcal{N}^{+}(v)}\Big{\|}\mathbf{x}_{u}^{(1)}-\mathbf{z}_{1}\Big{\|}_{ \infty}+\frac{1}{n}\Big{\|}\mathbf{W}_{r}^{(2)}\Big{\|}_{\infty}\sum_{u\in V}\Big{\|} \mathbf{x}_{u}^{(1)}-\mathbf{z}_{1}\Big{\|}_{\infty}\] \[\leq\frac{\epsilon}{C}\]Let \(\mathbf{z}_{2}:=\sigma(\mathbf{a}_{2})\). Then we can use the Lipschitz continuity of \(\sigma\) to bound the deviation of the activation from \(\mathbf{z}_{2}\) as follows.

\[\Big{|}[\mathbf{x}_{v}^{(2)}-\mathbf{z}_{2}]_{i}\Big{|}=\Big{|}[\sigma(\mathbf{y}_{v }^{(2)})-\sigma(\mathbf{a}_{2})]_{i}\Big{|}\leq C\Big{|}[\mathbf{y}_{v}^{(2)}-\mathbf{a }_{2}]_{i}\Big{|}\leq\epsilon\]

Since the probability that \(|[\mathbf{x}_{v}^{(1)}-\mathbf{z}_{1}]_{i}|<\epsilon^{\prime}\) for every node \(v\) and every \(i\in\{1,\ldots,d(1)\}\) tends to \(1\), we get that the probability that \(|[\mathbf{x}_{v}^{(2)}-\mathbf{z}_{2}]_{i}|<\epsilon\) for every node \(v\) and every \(i\in\{1,\ldots,d(2)\}\) also tends to \(1\).

Finally we apply the above argument inductively through all layers to get the desired result. 

The proof of the main result now proceeds as in the proof of Theorem 4.6.

Proof of Theorem b.2.: By Lemma B.3 the final node embeddings \(\mathbf{x}_{v}^{(T)}\) deviate less and less from \(\mathbf{z}_{T}\) as the number of nodes \(n\) increases. Therefore, the average-pooled graph-level representation also deviates less an less from \(\mathbf{z}_{T}\). By inspecting the proof, we can see that this \(\mathbf{z}_{T}\) is exactly the vector \(\mathbf{\mu}_{T}\) in the definition of non-splitting (Definition B.1). This means that \(\mathbf{z}_{T}\) cannot lie on a decision boundary for the classifier \(\mathfrak{C}\). Hence, there is \(\epsilon>0\) such that \(\mathfrak{C}\) is constant on:

\[\{\mathbf{x}\in\mathbb{R}^{d(T)}\mid\forall i\in\{1,\ldots,d(T)\}\colon[\mathbf{z}_{T} -\mathbf{x}]_{i}<\epsilon\}\]

Therefore, the probability that the output of \(\mathcal{M}\) is \(\mathfrak{C}(\mathbf{z}_{T})\) tends to \(1\) as \(n\) tends to infinity. 

## Appendix C Proof of the zero-one law for SumGNN\({}^{+}\)

The proof of the key lemma works rather differently to the GCN and MeanGNN\({}^{+}\) case, but we still make important use of Hoeffding's Inequality.

Proof of Lemma 4.11.: Consider the first layer preactivations \(\mathbf{y}_{v}^{(1)}\) and drop superscript \((1)\)'s for notational clarity. We can rearrange the expression as follows:

\[\mathbf{y}_{v}=(\mathbf{W}_{s}+\mathbf{W}_{g})\mathbf{x}_{v}^{(0)}+(\mathbf{W}_{n}+\mathbf{W}_ {g})\sum_{u\in\mathcal{N}(v)}\mathbf{x}_{u}^{(0)}+\mathbf{W}_{g}\sum_{u\in V \setminus\mathcal{N}^{+}(v)}\mathbf{x}_{u}^{(0)}+\mathbf{b}\]

For \(u,v\leq n\) define:

\[\mathbf{w}_{u,v}=(\mathbf{A}_{uv}\mathbf{W}_{n}+\mathbf{W}_{g})\mathbf{x}_{u}^{(0)}1_ {u\neq v}+(\mathbf{W}_{s}+\mathbf{W}_{g})\mathbf{x}_{u}^{(0)}1_{u=v}\]

Using this, we can rewrite:

\[\mathbf{y}_{v}=\sum_{u=1}^{n}\mathbf{w}_{u,v}+\mathbf{b}\]

By assumption on the distribution from which we draw graphs with node features, the \(\mathbf{w}_{u,v}\)'s are independent for any fixed \(v\).

Now fix \(i\in\{1,\ldots,d(1)\}\). By Lemma A.2 and Lemma A.4 we have that each \([\mathbf{w}_{u,v}]_{i}\) is sub-Gaussian. We therefore apply Hoeffding's Inequality to the sum. Note that \(\mathbf{w}_{u,v}\) can have one of two (sub-Gaussian) distributions, depending on whether \(u=v\). Therefore, by Theorem A.1 and Lemma A.3, there are constants \(c\) and \(K\) such that, no matter how many nodes \(n\) there are, we have that:

\[\mathbb{P}(|[\mathbf{y}_{v}]_{i}-\mathbb{E}[\mathbf{y}_{v}]_{i}|\geq t)=\mathbb{ P}\left(\left|\sum_{u=1}^{n}([\mathbf{w}_{u,v}]_{i}-\mathbb{E}[\mathbf{w}_{u,v}]_{i })\right|\geq t\right)\leq 2\exp\left(-\frac{ct^{2}}{K^{2}n}\right)\]

Let's now compute \(\mathbb{E}[\mathbf{y}_{v}]\), by first computing \(\mathbb{E}[\mathbf{w}_{u,v}]\). When \(u=v\) we have that:

\[\mathbb{E}[\mathbf{w}_{v,v}] =\mathbb{E}[(\mathbf{W}_{s}+\mathbf{W}_{g})\mathbf{x}_{v}^{(0)}]\] \[=(\mathbf{W}_{s}+\mathbf{W}_{g})\,\mathbb{E}[\mathbf{x}_{v}^{(0)}]\] \[=(\mathbf{W}_{s}+\mathbf{W}_{g})\mathbf{\mu}\]When \(u\neq v\) we have, using the independence of \(\mathbf{A}_{uv}\) and \(\mathbf{x}_{v}\):

\[\mathbb{E}[\mathbf{w}_{u,v}] =\mathbb{E}([\mathbf{A}_{uv}\mathbf{W}_{n}+\mathbf{W}_{g})\mathbf{x}_{u}^{ (0)}]\] \[=(\mathbb{E}[\mathbf{A}_{uv}]\mathbf{W}_{n}+\mathbf{W}_{g})\,\mathbb{E}[ \mathbf{x}_{v}^{(0)}]\] \[=(r\mathbf{W}_{n}+\mathbf{W}_{g})\mathbf{\mu}\]

Therefore (separating \(\mathbf{w}_{v,v}\) from \(\mathbf{w}_{u,v}\) for \(u\neq v\)):

\[\mathbb{E}[\mathbf{y}_{v}]=\sum_{u=1}^{n}\mathbb{E}[\mathbf{w}_{u,v}]+\mathbf{b}=(n -1)(r\mathbf{W}_{n}+\mathbf{W}_{g})\mathbf{\mu}+(\mathbf{W}_{s}+\mathbf{W}_{g})\mathbf{\mu}+\mathbf{b}\]

Since \(\mathcal{M}\) is synchronously saturating for \(\mathbb{G}(n,r)\) and \(\mathbb{D}(d)\), we know that \([(r\mathbf{W}_{n}+\mathbf{W}_{g})\mathbf{\mu}]_{i}\neq 0\). Assume without loss of generality that \([(r\mathbf{W}_{n}+\mathbf{W}_{g})\mathbf{\mu}]_{i}>0\). Then the expected value of \([\mathbf{y}_{v}]_{i}\) increases as \(n\) tends to infinity; moreover we have a bound on how much \([\mathbf{y}_{v}]_{i}\) can vary around its expected value.

Recall that the non-linearity \(\sigma\) is eventually constant in both directions. In particular, it is constant with value \(\sigma_{\infty}\) above some \(x_{\infty}\). When \(\mathbb{E}[\mathbf{y}_{v}]_{i}>x_{\infty}\) the probability that \([\mathbf{y}_{v}]_{i}\) doesn't surpass this threshold is:

\[\mathbb{P}([\mathbf{y}_{v}]_{i}<x_{\infty}) \leq\mathbb{P}(|[\mathbf{y}_{v}]_{i}-\mathbb{E}[\mathbf{y}_{v}]_{i }|>|x_{\infty}-\mathbb{E}[\mathbf{y}_{v}]_{i}|)\] \[\leq 2\exp\left(-\frac{c|x_{\infty}-\mathbb{E}[\mathbf{y}_{v}]_{i} |^{2}}{K^{2}n}\right)\]

There is a constant \(\rho\) such that \(|x_{\infty}-\mathbb{E}[\mathbf{y}_{v}]_{i}|\geq\rho n\). Hence for sufficiently large \(n\) (i.e. such that \(\mathbb{E}[\mathbf{y}_{v}]_{i}>x_{\infty}\)):

\[\mathbb{P}([\mathbf{y}_{v}]_{i}<x_{\infty})\leq 2\exp\left(-\frac{c\rho^{2}n^{2} }{K^{2}n}\right)=2\exp\left(-\frac{c\rho^{2}n}{K^{2}}\right)\]

Since the activation \([\mathbf{x}_{v}]_{i}=\sigma([\mathbf{y}_{v}]_{i})\), the probability that \([\mathbf{x}_{v}]_{i}\) takes value \(\sigma_{\infty}\) is at least \(1-2\exp\left(-c\rho^{2}n/K^{2}\right)\). Now, for each node \(v\) and each \(i\in\{1,\ldots,d(1)\}\), the activation \([\mathbf{x}_{v}]_{i}\) is either \(\sigma_{\infty}\) with high probability or \(\sigma_{-\infty}\) with high probability. By taking a union bound, for sufficiently large \(n\) the probability that every \([\mathbf{x}_{v}]_{i}\) takes its corresponding value is at least:

\[1-2nd(1)\exp\left(-\frac{c\rho^{2}n}{K^{2}}\right)\]

This tends to \(1\) as \(n\) tends to infinity. In other words, there is \(\mathbf{z}_{1}\in\{\sigma_{-\infty},\sigma_{\infty}\}^{d(1)}\) such that \(\mathbf{x}_{v}^{(1)}=\mathbf{z}_{1}\) for every \(v\) asymptotically.

We now proceed to the second layer, and condition on the event that \(\mathbf{x}_{v}^{(1)}=\mathbf{z}_{1}\) for every \(v\). In this case, we have that the second layer preactivation for node \(v\) is as follows.

\[\mathbf{y}_{v}^{(2)}=(\mathbf{W}_{s}^{(2)}+|\mathcal{N}(v)|\mathbf{W}_{n}^{(2)}+n\mathbf{W} _{g}^{(2)})\mathbf{z}_{1}+\mathbf{b}^{(2)}\]

Since we're in the situation where every \(\mathbf{x}_{u}^{(1)}=\mathbf{z}_{1}\), the degree \(|\mathcal{N}(v)|\) is simply binomially distributed \(\mathrm{Bin}(n,r)\). The preactivation \(\mathbf{y}_{v}^{(2)}\) then has expected value:

\[\mathbb{E}[\mathbf{y}_{v}^{(2)}]=n(r\mathbf{W}_{n}^{(2)}+\mathbf{W}_{g}^{(2)})\mathbf{z}_{1 }+\mathbf{W}_{s}^{(2)}\mathbf{z}_{1}+\mathbf{b}^{(2)}\]

Fix \(i\in\{1,\ldots,d(2)\}\). Since \(\mathcal{M}\) is synchronously saturating for \(\mathbb{G}(n,r)\) and \(\mathbb{D}(d)\), we have that \([(r\mathbf{W}_{n}^{(2)}+\mathbf{W}_{g}^{(2)})\mathbf{z}_{1}]_{i}\neq 0\). Assume without loss of generality that \([(r\mathbf{W}_{n}^{(2)}+\mathbf{W}_{g}^{(2)})\mathbf{z}_{1}]_{i}>0\). Then \(\mathbb{E}[\mathbf{y}_{v}^{(2)}]_{i}\) tends to infinity as \(n\) increases.

Furthermore, we can view \([n(r\mathbf{W}_{n}^{(2)}+\mathbf{W}_{g}^{(2)})\mathbf{z}_{1}]_{i}\) as the sum of \(n\) Bernoulli random variables (which take values \([(\mathbf{W}_{n}^{(2)}+\mathbf{W}_{g}^{(2)})\mathbf{z}_{1}]_{i}\) and \([\mathbf{W}_{g}^{(2)}\mathbf{z}_{1}]_{i}\)). Since by Lemma A.4 Bernoulli random variables are sub-Gaussian, as in the first-layer case we can apply Hoeffding's Inequality to bound the probability that \([\mathbf{y}_{v}^{(2)}]_{i}\) is less than \(x_{\infty}\). We get that, for sufficiently large \(n\), there is a constant \(K\) such that this probability is bounded by \(2\exp\left(-Kn\right)\).

Then, as before, we find \(\mathbf{z}_{2}\in\{\sigma_{-\infty},\sigma_{\infty}\}^{d(2)}\) such that, for sufficiently large \(n\), every \(\mathbf{x}_{v}^{(2)}=\mathbf{z}_{2}\) with probability at least \(1-2nd\exp{(-Kn)}\).

Finally, this argument is applied inductively through all layers. As the number of layers remains constant (since \(\mathcal{M}\) is fixed), we find that the node embeddings throughout the model are asymptotically constant. 

With the key lemma in place, we can now prove the main theorem.

Proof of Theorem 4.10.: Applying Lemma 4.11 to the final layer, we find \(\mathbf{z}_{T}\in\{\sigma_{-\infty},\sigma_{\infty}\}^{d(T)}\) such that every \(\mathbf{x}_{v}^{(T)}=\mathbf{z}_{T}\) with probability tending to \(1\). Since we use either average or component-wise maximum pooling, then means that the final graph-level representation is asymptotically constant, and thus the output of the classifier must be asymptotically constant. 

## Appendix D Proof of the uniform expressive power of SumGNN\({}^{+}\) with random features

We make use of a result due to Abboud et al. [1] which shows that SumGNN\({}^{+}\) models with random features can approximate any graph invariant on graphs with a fixed number of nodes.

**Definition D.1**.: Let \(f\) be a function on graphs, and let \(\zeta\) be a random function on graphs. Take \(\delta>0\) and \(N\in\mathbb{N}\). Then \(\zeta\)\(\delta\)-_approximates \(f\) up to \(N\) if:

\[\forall n\leq N\colon\,\mathbb{P}(\zeta(G)=f(G)\mid|G|=n)\geq 1-\delta\]

For completeness, we state the definition of the linearized sigmoid here.

**Definition D.2**.: The _linearized_ sigmoid is the function \(\mathbb{R}\to\mathbb{R}\) defined as follows:

\[x\mapsto\left\{\begin{array}{ll}-1&\text{if }x\in(-\infty,-1),\\ x&\text{if }x\in[-1,1),\\ 1&\text{otherwise}.\end{array}\right.\]

**Theorem D.3**.: _Let \(\xi\) be any graph invariant. For every \(N\in\mathbb{N}\) and \(\delta>0\) there is a SumGNN\({}^{+}\) with random features \(\mathcal{M}\) which \(\delta\)-approximates \(\xi\) up to \(N\). Moreover, \(\mathcal{M}\) uses the linearized sigmoid as the non-linearity and the distribution of the initial node embeddings consists of \(d\) iid \(U[0,1]\) random variables._

Proof.: See [1, Theorem 1]. 

With this result we can now prove the uniform expressivity result.

Proof of Theorem 5.2.: First, \(\xi\) satisfies a zero-one law for \(\mathbb{G}(n,1/2)\). Without loss of generality assume that \(\xi\) is asymptotically \(1\). There is \(N\in\mathbb{N}\) such that for every \(n>N\) we have:

\[\mathbb{P}(\xi(G)=1\mid G\sim\mathbb{G}(n,1/2))\geq 1-\delta\]

Note that this \(N\) depends on both \(\xi\) and \(\delta\).

Second, by Theorem D.3 there is a SumGNN\({}^{+}\) with random features \(\mathcal{M}\) which \(\delta\)-approximates \(\xi\) up to \(N\). Moreover, \(\mathcal{M}^{\prime}\) uses the linearized sigmoid as the non-linearity and the distribution of the initial node embeddings consists of \(d\) iid \(U[0,1]\) random variables.

Using the global readout and the linearized sigmoid, we can condition the model behavior on the number of nodes. We give a rough description of the model as follows. Define a SumGNN\({}^{+}\) with random features \(\mathcal{M}\) by extending \(\mathcal{M}^{\prime}\) as follows.

* Increase the number of layers to at least three.
* Increase each embedding dimension by \(1\). For convenience call this the \(0\)th component of each embedding.
* Use the bias term in the first layer to ensure that the \(0\)th component of the activation \(\mathbf{x}_{v}^{(1)}\) for each node \(v\) is \(1\).

* Use the global readout to threshold the number of nodes on \(N\). The \(0\)th row of the matrix \(\mathbf{W}_{g}^{(2)}\) should have a \(2\) in the \(0\)th position and \(0\)'s elsewhere. The \(0\)th component of the bias vector \(\mathbf{b}^{(2)}\) should be \(2N-1\). This ensures that the \(0\)th component of every activation \(\mathbf{x}_{v}^{(2)}\) is \(1\) if \(n>N\) and \(-1\) otherwise.
* Propagate this value through the \(0\)th component of each layer embedding.
* In the final layer, use this value to decide whether to output what \(\mathcal{M}^{\prime}\) would output, or simply to output \(1\).

For any \(n\leq N\) the model \(\mathcal{M}\) behaves like \(\mathcal{M}^{\prime}\). Therefore:

\[\mathbb{P}(\xi(G)=\mathcal{M}(G)\mid|G|=n)\geq 1-\delta\]

On the other hand, for \(n>N\) the model \(\mathcal{M}\) simply outputs \(1\) and so:

\[\mathbb{P}(\xi(G)=\mathcal{M}(G)\mid|G|=n)=\mathbb{P}(\xi(G)=1\mid|G|=n)\geq 1-\delta\]

Thus \(\mathcal{M}\) uniformly \(\delta\)-approximates \(\xi\). 

## Appendix E Further Experiments

In this section, we focus on GCNs and provide further experiments regarding our results. In particular, we pose the following questions:

1. Our theoretical results entail a zero-one law for a large class of distributions: do we empirically observe a zero-one law when node features are instead drawn from a normal distribution (Appendix E.1)?
2. Our theoretical results state a zero-one law for a large class of non-linearities: do we empirically observe a zero-one law when considering other common non-linearities (Appendix E.2)?
3. Does a zero-one law also manifest itself empirically for GAT models (Appendix E.3)?
4. Do we empirically observe a zero-one law if we were to consider sparse Erdos-Renyi graphs (Appendix E.4)?
5. Is there empirical evidence for our results to apply to other random graph models, such as the Barabasi-Albert model (Appendix E.5)?

### Experiments with initial node features drawn from a normal distribution

Here we consider using a normal distribution to draw our initial node features. Note that normal distributions are sub-Gaussian, and hence our theoretical findings (Theorem 4.6) confer a zero-one law in this case. Figure 2 demonstrates the results for GCN models. We observe the expected asymptotic behavior in most cases, however in the two and three layer cases a few models have not converged by the end of the experiment.

Figure 2: Normally distributed random node features with GCN models. Each plot shows the proportion of graphs of certain size which are classified as \(1\) by a set of ten GCN models. Each curve (color-coded) shows the behavior of a model, as we draw increasingly larger graphs. The phenomenon is observed for 1-layer models (left column), 2-layer models (mid column), and 3-layer models (last column). We draw the initial features randomly from a normal distribution with mean 0.5 and standard deviation \(1\).

### Experiments with other non-linearities

In this subsection we test the effect of using different non-linearities in the layers of our GCN models. Theorem 4.6 applies in all of these cases, so we do expect to see a zero-one law. Figures 3 to 5 present the results for \(\mathrm{ReLU}\), \(\tanh\) and \(\mathrm{sigmoid}\), respectively. We see the expected behavior in all cases. Note however that, in contrast with other non-linearities, when we use \(\mathrm{sigmoid}\) we observe that the rate of convergence actually increases as the number of layers increases. This suggests a complex relationship between the rate of convergence, the non-linearity and the number of layers.

Figure 4: GCN models with \(\tanh\) non-linearity. Each plot shows the proportion of graphs of certain size which are classified as \(1\) by a set of ten GCN models. Each curve (color-coded) shows the behavior of a model, as we draw increasingly larger graphs. The phenomenon is observed for 1-layer models (left column), 2-layer models (mid column), and 3-layer models (last column). We use \(\tanh\) as an activation function for the GNN layers, and keep everything else the same.

Figure 5: GCN models with sigmoid non-linearity. Each plot shows the proportion of graphs of certain size which are classified as \(1\) by a set of ten GCN models. Each curve (color-coded) shows the behavior of a model, as we draw increasingly larger graphs. The phenomenon is observed for 1-layer models (left column), 2-layer models (mid column), and 3-layer models (last column). We use the \(\mathrm{sigmoid}\) activation function for the GNN layers, and keep everything else the same.

Figure 3: GCN models with \(\mathrm{ReLU}\) non-linearity. Each plot shows the proportion of graphs of certain size which are classified as \(1\) by a set of ten GCN models. Each curve (color-coded) shows the behavior of a model, as we draw increasingly larger graphs. The phenomenon is observed for 1-layer models (left column), 2-layer models (mid column), and 3-layer models (last column). This time we choose the \(\mathrm{ReLU}\) activation function for the GNN layers. Apart from this, the setup is the same as in the main body of the paper.

### Experiments with GAT

Here we investigate the asymptotic behaviour of a GNN architecture not considered in the main body: the Graph Attention Network [37]. Cast as an MPNN, this architecture uses an attention mechanism as the aggregate function \(\phi\) in the message passing step. The techniques used in this paper to establish a zero-one law for other GNN architectures do not easily extend to GAT. However, our experiments demonstrate a very quick convergence to \(0\) or \(1\).

### Experiments on sparse Erdos-Renyi graphs

In these experiments, we consider GCN models on a variation of the Erdos-Renyi distribution in which the edge probability \(r\) is allowed to vary as a function of \(n\). Specifically, we set \(r=\log(n)/n\), which yields sparser graphs than in the standard distribution. Our experiments provide evidence for a zero-one law also in this case (Figure 7).

### Experiments on the Barabasi-Albert random graph model

In this subsection, we consider another alternative graph distribution: the Barabasi-Albert model [3]. This model aims to better capture the degree distributions commonly found in real-world networks. We can again observe a zero-one law for GCN models under this distribution (Figure 8).

Figure 6: Ten GAT models with number of layers 1, 2 and 3 are run on graphs of increasing size, with the proportion of nodes classified as 1 recorded. We observe a convergence to zero-one law very quickly.

Figure 7: Sparse Erds-Rnyi graphs with GCN models. Each plot shows the proportion of graphs of certain size which are classified as \(1\) by a set of ten GCN models. Each curve (color-coded) shows the behavior of a model, as we draw increasingly larger graphs. The phenomenon is observed for 1-layer models (left column), 2-layer models (mid column), and 3-layer models (last column). We let the probability \(r\) of an edge appearing be \(\frac{\log(n)}{n}\). All the other parameters are the same as in the experiments of the main body of the paper.

Figure 8: Barabsi-Albert graphs with GCN models. Each plot shows the proportion of graphs of certain size which are classified as \(1\) by a set of ten GCN models. Each curve (color-coded) shows the behavior of a model, as we draw increasingly larger graphs. The phenomenon is observed for 1-layer models (left column), 2-layer models (mid column), and 3-layer models (last column). We generate the graphs using the Barabsi-Albert model; apart from this the setup is the main as in the experiments in the main body of the paper.