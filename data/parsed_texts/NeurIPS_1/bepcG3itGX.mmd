# LibAMM: Empirical Insights into Approximate Computing for Accelerating Matrix Multiplication

Xianzhi Zeng\({}^{1,2}\)1  Wenchao Jiang\({}^{3}\)  Shuhao Zhang\({}^{1}\)

\({}^{1}\) National Engineering Research Center for Big DataTechnology and System

Services Computing Technology and System Lab

Cluster and Grid Computing Lab

School of Computer Science and Technology

Huazhong University of Science and Technology, Wuhan, 430074, China

\({}^{2}\) Nanyang Technological University

\({}^{3}\) Singapore University of Technology and Design

shuhao_zhang@hust.edu.cn

Footnote 1: Work is done while visiting Shuhao Zhangâ€™s Lab.

###### Abstract

Matrix multiplication (MM) is pivotal in fields from deep learning to scientific computing, driving the quest for improved computational efficiency. Accelerating MM encompasses strategies like complexity reduction, parallel and distributed computing, hardware acceleration, and approximate computing techniques, namely AMM algorithms. Amidst growing concerns over the resource demands of large language models (LLMs), AMM has garnered renewed focus. However, understanding the nuances that govern AMM's effectiveness remains incomplete. This study delves into AMM by examining algorithmic strategies, operational specifics, dataset characteristics, and their application in real-world tasks. Through comprehensive testing across diverse datasets and scenarios, we analyze how these factors affect AMM's performance, uncovering that the selection of AMM approaches significantly influences the balance between efficiency and accuracy, with factors like memory access playing a pivotal role. Additionally, dataset attributes are shown to be vital for the success of AMM in applications. Our results advocate for tailored algorithmic approaches and careful strategy selection to enhance AMM's effectiveness. To aid in the practical application and ongoing research of AMM, we introduce LibAMM --a toolkit offering a wide range of AMM algorithms, benchmarks, and tools for experiment management. LibAMM aims to facilitate research and application in AMM, guiding future developments towards more adaptive and context-aware computational solutions.

## 1 Introduction

Matrix multiplication (MM) is essential across computational domains, particularly in machine learning and scientific simulations. While efforts to improve MM's performance and scalability [24, 38] underscore its importance, MM can dominate up to 90% of processing time in some applications, posing a significant bottleneck. Approximate matrix multiplication (AMM) [17, 12, 34] offers a solution by trading exact accuracy for increased efficiency in contexts where absolute precision is not critical, such as machine learning inference from VGG-like models [6] to GPT-3 LLMs [12]. One may further combine AMM with advances in algorithmic design [32], parallel and distributed computing [4, 26, 37], and hardware technology [11, 18] to mitigate the computational demands of traditional MM, highlighting its potential to revolutionize MM-intensive applications.

The advancement of AMM methods has brought forth a range of techniques designed to meet various computational challenges. These techniques strategically utilize approximations to enhance matrix operation efficiency [13; 16]. Central to AMM is the concept of simplifying computations by modulating calculation detail, prioritizing matrix elements vital to the outcome and downplaying lesser ones [34; 2]. This approach embodies a crucial trade-off: it lowers computational load in exchange for reduced accuracy, a compromise finely adjusted through parameter \(\omega\). AMM encompasses three principal approximation strategies: pruning-based methods that cut superfluous calculations for better efficiency [21; 13; 17], extraction-based techniques that identify and leverage key elements or patterns to streamline computations [36; 29; 2; 23; 6; 25], and hybrid approaches that merge these methods [22; 34; 9; 31].

Despite the progress in AMM, a comprehensive and impartial comparison remains elusive, often leading to confusing and sometimes contradictory advice for algorithm selection and task-specific modifications. This confusion stems from several factors: First, the appeal of AMM across various downstream tasks, each favoring a different optimal approach, results in varied conclusions. For example, Adelman et al. [1] favor pruning-based AMM for machine learning training, whereas Blalock et al. [6] recommend extraction-based AMM for inference tasks. Second, the evaluation of some AMM algorithms lacks breadth over representative workloads. Mroueh et al.'s [25] assertion that extraction-based AMM yields only minor inaccuracies is primarily based on synthetic datasets, which may not reflect real-world distribution complexities. Third, inconsistencies in AMM implementation and the standards for baselines raise further issues. The mixture of just-in-time (JIT) and static compilation methods [1; 6; 29] complicates fair performance evaluations among AMM implementations. Additionally, the use of manually coded nested loop MM as a baseline by some studies [36; 25; 16] diverges from modern data science practices, which often employ optimizations like cache-aware data loading and SIMD instructions [28].

To address these gaps, our study delves into a comprehensive empirical analysis of AMM techniques across key algorithmic dimensions. We structure our investigation around three main axes: (a) We conduct a unified static compilation analysis of twelve AMM algorithms alongside two benchmark MM baselines to measure their performance and efficiency. (b) We leverage eight real-world datasets from a spectrum of disciplines to test the versatility and robustness of AMM strategies. (c) We examine four statistics and machine learning applications to assess AMM's practical utility in various downstream tasks. Our methodology intentionally avoids optimizations that cater to specific algorithms or hardware setups, focusing instead on principles with broad applicability [25; 16]. This approach aims to provide a wide-ranging and insightful examination of the AMM domain, highlighting the diverse factors critical for progress in the field. In our study, we discovered insights crucial for advancing MM acceleration via approximation. Key takeaways from our experiments include:

* Among three approximation strategies, **pruning-based and hybrid AMM is notably more beneficial** than extraction-based AMM (Section 3.1).
* For all evaluated AMM, **minimizing memory overhead is the key** to practical performance benefits, especially when further leveraging hardware accelerations like GPUs (Section 3.2).
* For all evaluated AMM, the **accuracy greatly depends on dataset attributes** like value skewness and non-zero distribution, requiring improvements of the error bound (Section 3.3).
* In downstream tasks where approximate computing is allowed, **pruning-based and hybrid AMM significantly outperform extraction-based** AMM. Specifically, they are superior in both reducing processing latency and conducting the task-aligned approximation with less error (Section 3.4).

To support and inspire ongoing and future research, we introduce LibAMM, a framework within the PyTorch ecosystem. LibAMM is offered as an open-source tool at [https://github.com/intellistream/LibAMM](https://github.com/intellistream/LibAMM). It aggregates prevalent AMM algorithms, benchmark datasets, and scripts to easily reproduce our experimental results.

## 2 Preliminary

### Problem Formulation

Let \(A\in\mathbb{R}^{M\times K}\) and \(B\in\mathbb{R}^{K\times N}\) denote two matrices intended for multiplication (denoted as \(MM(\cdot)\)), aiming to calculate the product \(C=MM(A,B)\in\mathbb{R}^{M\times N}\). With AMM, the objective shifts towards computing an approximation \(\tilde{C}\in\mathbb{R}^{M\times N}\), which seeks to balance computational efficiency with the fidelity of the approximation to \(C\). Efforts in AMM focus on developing an algorithm \(AMM(\cdot)\)that takes \(A\) and \(B\) as inputs and applies approximation techniques to produce \(\tilde{C}\) with reduced computational demand. The result of this process is expressed as \(\tilde{C}=AMM(A,B)\in\mathbb{R}^{M\times N}\), with each element \(\tilde{c}_{ij}\) signifying the approximated value corresponding to the element \(c_{ij}\) in the traditional product \(C\). The critical performance metrics for \(\mathtt{AMM}\) are _Processing Latency (\(\mathbf{1}\), \(\mathtt{AMM}\) Error (\(\epsilon\))_, and _Approximation Impact Factor_ (\(\Delta E\)). \(l\) indicates the time efficiency of \(\mathtt{AMM}\), by measuring the time from \(A\) and \(B\) are presented to \(\tilde{C}\) is eventually produced. \(\epsilon\), calculated by \(\frac{\|\tilde{C}-C\|_{F}}{\|C\|_{F}}\), measures the _frobenius normalized accuracy_[6, 34] of \(\mathtt{AMM}\) to standard \(\mathtt{MM}\). The \(\Delta E\), calculated through \(E_{MM}-E_{AMM}\), contrasts the downstream application error when employing \(\mathtt{MM}\) (\(E_{MM}\)) against that when using \(\mathtt{AMM}\) (\(E_{AMM}\)). This metric effectively measures the impact of \(\mathtt{AMM}\) on prediction accuracy within downstream tasks, capturing both the potential benefits and drawbacks.

### Existing \(\mathtt{AMM}\) Revisit

Based on how to handle the matrix information, we broadly classify \(\mathtt{AMM}\) algorithms into three categories: pruning-based, extraction-based, and hybrid. We summarize representative \(\mathtt{AMM}\) algorithms and the \(\mathtt{MM}\) baselines in Table 1, more related work is discussed in Section 4.

#### 2.2.1 Pruning-based \(\mathtt{AMM}\)

Pruning-based \(\mathtt{AMM}\) emphasizes the selective removal of redundant information from matrices. This approach can be implemented at two levels of granularity: bit-wise pruning and element-wise pruning. Bit-wise pruning involves compressing each matrix element by using fewer binary bits. A notable example is \(\mathrm{INT}8\), which quantizes 32-bit floating-point elements into 8-bit signed integers. Conversely, element-wise pruning keeps the entire binary representation for certain elements and completely discards others. Techniques for achieving this include CRS (column row sampling) and count sketching mechanisms used in CS (count sketch-based \(\mathtt{AMM}\)). Pruning-based \(\mathtt{AMM}\) algorithms are lightweight, but also reliant on the nature of the underlying data.

#### 2.2.2 Extraction-based \(\mathtt{AMM}\)

Extraction-based \(\mathtt{AMM}\) focuses on identifying and utilizing higher-level characteristics from matrix elements. It preserves these characteristics in intermediate structures, which are then used to facilitate \(\mathtt{AMM}\) computations. This approach is favored because operations on these intermediate structures are significantly faster than traditional \(\mathtt{MM}\) processes. Various matrix attributes serve to accomplish this goal. Frequent Direction-based methods, such as CoOFD, iteratively isolate significant singular values directly from the input matrices. In contrast, the BlockLRA algorithm applies singular value decomposition to separate blocks within the matrices, offering a balance between precision and computational efficiency compared to comprehensive singular value decomposition as in CoOFD. FastJLT employs a different tactic by extracting Johnson-Lindenstrauss (JL) embedding properties of the matrices through Walsh-Hadamard transformations. Beyond leveraging intermediate matrix structures, incorporating a codebook that catalogues K-nearest neighbor (KNN) centroids of matrix rows or columns also facilitates effective information extraction for \(\mathtt{AMM}\). This technique is implemented in VQ (vector quantization) and PQ (product quantization), with PQ typically

\begin{table}
\begin{tabular}{|l|l|l|} \hline _Category_ & _Algorithm Name_ & _Descriptions_ \\ \hline \multirow{3}{*}{\begin{tabular}{l} Purning- \\ based \(\mathtt{AMM}\) \\ \end{tabular} } & INTR [21] & Purning 32-bit into 8-bit \\ \cline{2-3}  & CRS [13] & Purning elements by sampling \\ \cline{2-3}  & CS [17] & Purning elements by sketching \\ \hline \multirow{6}{*}{\begin{tabular}{l} Extraction- \\ based \(\mathtt{AMM}\) \\ \end{tabular} } & CoOFD [36] & Extracting singular value, for entire matrices \\ \cline{2-3}  & BlockLRA [29] & Extracting singular value, for blocks \\ \cline{2-3}  & FastJLT [2] & Extracting JL embeddings \\ \cline{2-3}  & VQ [23] & Extracting KNN centroids \\ \cline{2-3}  & PQ [6] & Similar to VQ, more efficient codebook \\ \cline{2-3}  & RHP [22] & Randomized JL embeddings extraction \\ \cline{2-3}  & SMP-PCA [34] & Similar to RIP, scaling values for higher accuracy \\ \cline{2-3}  & WeightedCR [9] & Extract the weight information during sampling \\ \hline \multirow{2}{*}{
\begin{tabular}{l} Baseline \(\mathtt{MM}\) \\ \end{tabular} } & ToGoFW [31] & Extract the median and select the optimal after sketching \\ \cline{2-3}  & NLMM [36] & The manual, brute-force, nested loop implementation of \(\mathtt{MM}\) \\ \cline{2-3}  & LTMM [28] & LibTorchâ€™s optimized implementation of \(\mathtt{MM}\) \\ \hline \end{tabular}
\end{table}
Table 1: \(\mathtt{AMM}\) algorithms and \(\mathtt{MM}\) baselines investigatedachieving faster processing times for larger matrices by optimizing the number of KNN centroids through the Cartesian product of subspaces. While extraction-based methods theoretically maintain higher accuracy by preserving essential information during feature extraction, the computational and memory demands of this extraction process may surpass those of standard MM.

#### 2.2.3 Hybrid AMM

Hybrid AMM methods combine pruning and extraction techniques to balance processing latency with accuracy. These methods aim to accelerate feature extraction while allowing for selective information pruning to enhance efficiency. For instance, RIP and SMP-PCA introduce randomness to the Johnson-Lindenstrauss (JL) embeddings extraction process, which is inherently deterministic in approaches like FastJLT, achieving a speed boost. SMP-PCA distinguishes itself by incorporating an additional value scaling step post-randomly pruned JL transform, which theoretically yields higher accuracy compared to RIP. Other hybrid AMM strategies focus on refining element-wise pruning through targeted feature extraction. For example, WeightedCR leverages extracted weight information for more informed sampling, presenting a significant improvement over the indiscriminate sampling seen in CRS. Similarly, the TugOfWar method identifies the optimal sketch from a set of random sketches, prioritizing the retention of information over the more arbitrary selection found in CS. By leveraging the strengths of both pruning and extraction, hybrid AMM methods stand to deliver superior performance, striking an optimal balance between speed and precision.

### Tuning Knob \(\omega\) of AMM Algorithms

The tuning parameter \(\omega\) plays a crucial role in managing the trade-off between computational efficiency (\(l\)) and accuracy (\(\epsilon\))in AMM algorithms. This section elucidates the influence of \(\omega\) across a spectrum of AMM methodologies, noting that \(\omega\) has no bearing on INT8, LTMM, and NLMM. The latter two algorithms, being precise MM implementations, and INT8, although an AMM algorithm, do not utilize this adjustable parameter.

VQ and PQ showcase an innovative approach by adjusting the number of K-nearest neighbor (KNN) centroids relative to the row count of the input matrix \(A\) through \(\omega\). Unlike other AMM methods that rely on intermediate matrices, these algorithms employ codebooks for feature encapsulation, linking each KNN centroid to a unique code within the codebook. This adjustment effectively moderates the codebook's capacity and computational overhead, striking a balance between latency and precision.

For feature extraction-focused algorithms like CoOFD and FastJLT, \(\omega\) is instrumental in determining the dimensionality of the feature space extracted in relation to input matrix \(A\)'s column volume. By converting singular values or JL embeddings of \(A,B\) into intermediate matrices, the size of which is configurable through \(\omega\), these algorithms propose a trade-off: smaller intermediate structures facilitate quicker computations at the expense of accuracy, while larger configurations promise enhanced accuracy at the cost of increased computational time. This principle is similarly applicable to hybrid AMM approaches such as RIP and SMP-PCA, which incorporate randomized optimizations in the feature extraction phase to improve efficiency.

In the context of BlockLRA, \(\omega\) significantly impacts the sizing of the feature extraction matrix in proportion to the eigenspace of \(A,B\), diverging from the full matrix consideration to a block-wise feature extraction perspective. This nuanced application of \(\omega\) alters the dimensionality of intermediate matrices, thereby modifying computational dynamics.

Lastly, for CRS, CS, WeightedCR, and TugOfWar, the parameter \(\omega\) governs the proportion of matrix elements preserved after pruning, employing sampling or sketching techniques to forgo computations on specific elements within \(A,B\). The allocation determined by \(\omega\) serves to represent the characteristics of the majority pruned, leveraging statistical properties such as mean or variance to approximate the impact of excluded elements.

## 3 Empirical Studies

In the following section, we first introduce experimental configurations and then present the results.

**Evaluation Configurations.** Our experimental framework is meticulously designed to ensure a comprehensive and equitable assessment of AMM algorithms across a spectrum of real-world and synthetic datasets. To this end, we delineate our evaluation methodology under two principal components: the datasets employed and the detailed implementation nuances of AMM algorithms. In the evaluation, we set \(\omega\) as \(10\%\) if not otherwise specified.

**Datasets.** The core of our benchmark suite is derived from MatrixMarket [27], encapsulating a diverse array of **real-world workloads** including but not limited to _ECO_, _DWAVE_, _AST_, _UTM_, _RDB_, _ZENOS_, _QCD_, and _BUS_, detailed in Table 2. These datasets are preprocessed to normalize matrix elements within the range of \(-1\) to \(+1\), covering a wide breadth of applications from economic modeling (_ECO_) to power flow analysis (_BUS_). Specifically, we linearly align the maximum value to 1 and minimum value to -1, and we leave the more complicated and application-specific normalization such as L2 Normalization [28] for future works. Complementing these real-world datasets, we also generate **synthetic workloads** using LibTorch functionalities like _torch::rand_.

**Downstream Tasks.** We examine a suite of downstream tasks where the integration of AMM is particularly appealing. These tasks include _Principal Component Analysis_ (PCA) [33, 30], _Machine Learning Training_[1] and _Inference_[6] phases, and _Unitary Transformation_[20, 15]. Detailed descriptions of those applications are presented in Appendix A. Note that, while some pruning-based AMM methods under element quantization like INT8 can seamlessly integrate into complex models, such as transformer-based large language models [35, 12], incorporating other AMM strategies (e.g., CRS or SMP-PCA) into these advanced models poses a significant challenge. As such, we follow [1] for training and [6] for inference, and leave more intricate models in future research.

Table 3 presents an analysis of the datasets utilized, highlighting the proportion of latency associated with AMM-replaceable MM operations within these downstream tasks. By integrating these tasks into the LibTorch ecosystem, we capitalize on its LTMM functionality for MM operations. The machine learning training and inference procedures, in particular, engage the PyTorch frontend. We bind PyTorch calls to static compilation, thus isolating the impacts of JIT on the execution of AMM or MM.

**Implementation.** We unify the implementation of the examined AMM algorithms into one C++ codebase, using static compilation to guarantee consistency across experiments. We use the IEEE 754 32-bit floating-point (FP32) format for representing matrix elements, and take the advantage of LibTorch C++ API [28], thereby inheriting AVX-512 instructions of FP32 from LibTorch. While certain AMM algorithms might benefit from algorithm-specific optimizations--like Bernoulli sampling probabilities in CRS[1] or the _MADDNESS_ hash function in PQ[6]--we exclude these from our primary evaluation. This exclusion is to avoid bias introduced by optimizations that rely on assumptions not universally applicable, aiming for evaluations that are as inclusive and applicable as possible. We focus on in-memory MM and AMM, and leave out-of-memory case [38] or disk-memory corporation [3] for future works.

**Deployment.** Our evaluation primarily unfolds on a Silver 4310 processor, with both MM and AMM seamlessly adapting to parallel and distributed computing through a straightforward block partition approach [6]. This directs our focus towards single-threaded evaluation. Notably, we include an experiment to explore AMM's performance in a parallelized context, utilizing an I7-13700K CPU and an RTX A6000 GPU, to assess its potential on parallel hardware architectures.

### Algorithmic Strategies

We first investigate how different algorithmic strategies affect the effectiveness of AMM, concerning both processing latency and accuracy, as summarized in Table 4.

\begin{table}
\begin{tabular}{|l|l|l|} \hline
**Name** & **Application Field** & **Size** \\ \hline _ECO_ & Economics & \(2077\times 200\) \\ \hline _DWAVE_ & Integrated Circuit & \(512\times 512\) \\ \hline _AST_ & Astrophysics & \(765\times 765\) \\ \hline _UTM_ & Plasma Physics & \(1700\times 1700\) \\ \hline _RDB_ & Chemical Engineering & \(2948\times 2948\) \\ \hline _ZENOS_ & Air Traffic & \(2873\times 2873\) \\ \hline _QCD_ & Quantum Physics & \(3072\times 3072\) \\ \hline _BUS_ & Land Traffic & \(4929\times 10595\) \\ \hline \end{tabular}
\end{table}
Table 2: Real-world Workloads for Comparing AMM Algorithms

[MISSING_PAGE_FAIL:6]

**Our results highlighted a complex performance trade-off landscape.** Generally, increasing \(\omega\) leads to lower \(\epsilon\) for most algorithms, except CS, but at the expense of increased \(l\). This trade-off between minimizing error and managing latency varies significantly among algorithms. For instance, CRS can reduce \(l\) by up to \(60\%\) by tolerating an \(\epsilon\) rise from \(0.02\) to \(0.28\). In contrast, BlockLRA consistently keeps \(\epsilon\) below \(0.01\) and indicates a narrower trade-off scope. Interestingly, CS deviates from the expected trend of reduced \(\epsilon\) with higher \(\omega\), attributed to its relatively loose error bound.

### Operational Specifics

We examined the impact of operational specifics on AMM performance, categorizing processor operations into _memory operations_ and _computing operations_, using PAPI [8] to trace processor cycles during AMM. Our analysis on a Silver 4310 processor identified cycles affected by memory operations as 1) _Mem Stall_, 2) _L1D Stall_, 3) _L2 Stall_, and 4) _L3 Stall_. Additionally, we noted 5) _Computing Stall_, caused by pending computing operations, and 6) _Useful_ cycles, where the processor efficiently executes without memory or computing stalls. Importantly, stalls are only recorded when operations exceed their expected completion time [19], with on-time operations classified as _Useful_.

Observation 4. Cache and memory stalls significantly impact all AMM algorithms, particularly in larger datasets, with memory stalls been a key issue.

In Table 5, we delve into the scalability of AMM algorithms across a spectrum of dataset sizes, including _ECO_, _UTM_, and _BUS_. A key finding is the pronounced impact of cache and memory stalls on all AMM algorithms and MM baselines, particularly in larger datasets, where memory stalls emerge as a critical bottleneck. For instance, FastJLT sees nearly all its processor cycles consumed by memory stalls in the _BUS_ dataset, underscoring the heavy data demands of matrix operations.

**Memory access and the resulting cache performance issues scale with dataset size**, evident in the stark increase from \(21.91\%\) of affected cycles in _ECO_ to \(99.99\%\) in _BUS_ for SMP-PCA. Memory stalls notably surpass cache stalls in magnitude across most algorithms, such as FastJLT on _BUS_, due to LibTorch's cache optimizations aligning well with algorithms that utilize contiguous data structures. Conversely, algorithms like CS and VQ face greater challenges due to their access patterns to disjoint data structures. Furthermore, computational challenges compound for algorithms like CoOFD and PQ in large datasets, with a significant portion of their processing time hampered by computational stalls--evident in the substantial delays faced by these algorithms on _BUS_. These observations suggest that while memory stalls are a universal bottleneck, especially in larger datasets, computational efficiency remains a critical consideration for certain AMM algorithms.

Figure 3 further reveals **the escalation of memory stall cycles with data size**, aligning with increases in processing latency (\(l\)) as demonstrated in Figure 1(a). The surge in memory stalls, particularly observed in LTMM with over \(71\times\) growth from \(1000\) to \(2500\) rows, highlights the critical point of memory bandwidth utilization. Pruning-based and hybrid AMM algorithms, notably SMP-PCA and RIP, excel in mitigating this memory stall growth, outperforming even LTMM by maintaining stall cycles within manageable limits up to \(50000\) rows. This contrasts with extraction-based algorithms, which exhibit higher memory stall cycles due to their more intensive memory usage.

Observation 5. Adapting AMM to GPUs shows promise but is limited by data transfer costs, highlighting the need for optimized hardware-software integration.

\begin{table}
\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|} \hline \multirow{2}{*}{Algorithms} & \multicolumn{3}{c|}{
\begin{tabular}{c} Algorithm \\ \end{tabular} } & \multicol

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

feature extraction does not align with the task's principal data characteristics, leading to elevated \(\Delta E\). For example, these algorithms significantly increased \(\Delta E\) in PCA due to mismatches in feature representation. The impact of AmM on machine learning training and inference highlights the importance of semantic preservation, which is often overlooked by current approximation strategies. In training, CRS's \(\Delta E\) escalated from \(0.05\) at a 500-D hidden layer to \(0.08\) at 2000-D, indicating a greater risk of information loss with larger matrices. Similarly, in inference, extraction-based methods (CoOFD, BlockLRA, VQ, and PQ) failed to preserve semantic details, with CoOFD's \(\Delta E\) reaching \(0.68\). These findings underscore that AmM's numerical approximation focus can lead to semantic information loss, particularly in consecutive applications or nuanced interpretation tasks. However, SMP-PCA's resilience to error amplification in unitary transformations signals a potential for AmM strategies to achieve numerical and semantic fidelity.

## 4 Related Work

While AmM has been theoretically well-explored [21; 25; 13; 1; 9; 31; 29; 2; 22; 7; 17; 34; 36; 16; 6; 23], there exists a gap in benchmarks that evaluate these algorithms' performance in diverse real-world applications, particularly emphasizing accuracy alongside computational efficiency. Prior research has illustrated AmM's potential in specific tasks; however, these analyses often lack a holistic evaluation that considers various performance metrics [14; 34]. Our contribution aims to bridge this gap by offering an in-depth comparison of AmM algorithms across a range of practical tasks, thereby providing insights into their applicability and efficiency in real-world scenarios. Our approach sets a new precedent in the study of AmM by amalgamating an extensive array of algorithms and applications, moving beyond traditional theoretical assessments to explore their practical utility. By adopting a comprehensive benchmarking strategy, we illuminate the strengths and limitations of AmM, facilitating a richer understanding of its potential for enhancing computational processes. This endeavor not only aids in theoretical exploration but also enhances practical applications, marking a significant step forward in the evolution of AmM research.

## 5 Conclusion

This study sheds light on the impactful nuances of AmM algorithms within real-world applications. Our takeaways and inspirations for future works can be summarized as follows.

**Summary of Observations.** There are three major findings throughout this study. First, pruning-based (e.g., INT8, CRS) and hybrid (e.g., RIP, SMP-PCA) AmM outperform extraction-based AmM, especially in various downstream tasks **(O1, O2, O8)**. Second, memory overhead is a common bottleneck, while some AmM (e.g., CRS, SMP-PCA) do succeed in optimizing memory access, the high cost of data transfer still limits border applications of AmM **(O4, O5)**. Third, the tradeoff space between accuracy and efficiency is wide for some AmM (e.g., CRS, SMP-PCA) yet narrow for others (e.g., BlockLRA). It is further challenged by data distributions, and the existing error bound of AmM is not strong enough under severe skewness or bias of data distribution **(O3, O6, O7)**.

**Impacts and Future Directions.** Our empirical insights offer a more comprehensive understanding of the strengths and limitations in current AmM to data science communities. Future work on AmM could further reduce the memory overhead while strengthening the error bound guarantees. We also envision a versatile and robust software-hardware co-design to better incorporate AmM with orthogonal optimizations, i.e., algorithmic design [32], parallel and distributed computing [26], and hardware technology [18], to better cater to the evolving demands of diverse computational landscapes.

## Acknowledgments and Disclosure of Funding

This project received partial support from the National Research Foundation, Singapore, and the Infocomm Media Development Authority under its Future Communications Research & Development Programme (FCP-SUTD-RG-2022-005), Ministry of Education AcRF Tier 2 grant (MOE-T2EP20122-0010, MOE-T2EP20221-0017), and a Nanyang Technological University startup grant (023452-00001). The perspectives, conclusions, or recommendations put forward in this material are exclusively those of the authors and do not mirror the views of the Ministry of Education, Singapore. Corresponding author is Shuhao Zhang.

## References

* [1] M. Adelman, K. Levy, I. Hakimi, and M. Silberstein. Faster neural network training with approximate tensor operations. _Advances in Neural Information Processing Systems_, 34:27877-27889, 2021.
* [2] N. Ailon and B. Chazelle. The fast johnson-lindenstrauss transform and approximate nearest neighbors. _SIAM Journal on computing_, 39(1):302-322, 2009.
* [3] J. Alafate and Y. S. Freund. Faster boosting with smaller memory. _Advances in Neural Information Processing Systems_, 32, 2019.
* [4] A. R. Benson and G. Ballard. A framework for practical parallel fast matrix multiplication. _ACM SIGPLAN Notices_, 50(8):42-53, 2015.
* [5] R. A. Bertlmann, G. Launer, and E. De Rafael. Gaussian sum rules in quantum chromodynamics and local duality. _Nuclear Physics B_, 250(1-4):61-108, 1985.
* [6] D. Blalock and J. Guttag. Multiplying matrices without multiplying. In _International Conference on Machine Learning_, pages 992-1004. PMLR, 2021.
* [7] C. Boutsidis and A. Gittens. Improved matrix algorithms via the subsampled randomized hadamard transform. _SIAM Journal on Matrix Analysis and Applications_, 34(3):1301-1340, 2013.
* [8] S. Browne, J. Dongarra, N. Garner, G. Ho, and P. Mucci. A portable programming interface for performance evaluation on modern processors. _The international journal of high performance computing applications_, 14(3):189-204, 2000.
* [9] N. Charalambides, M. Pilanci, and A. O. Hero. Approximate weighted cr coded matrix multiplication. In _ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 5095-5099. IEEE, 2021.
* [10] X. Chen, Z.-C. Gu, and X.-G. Wen. Local unitary transformation, long-range quantum entanglement, wave function renormalization, and topological order. _Physical review b_, 82(15):155138, 2010.
* [11] H. Cheng, W. Li, Y. Lu, and W. Liu. Haspgemm: Heterogeneity-aware sparse general matrix-matrix multiplication on modern asymmetric multicore processors. In _Proceedings of the 52nd International Conference on Parallel Processing_, pages 807-817, 2023.
* [12] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. _Advances in Neural Information Processing Systems_, 35:30318-30332, 2022.
* [13] P. Drineas, R. Kannan, and M. W. Mahoney. Fast monte carlo algorithms for matrices i: Approximating matrix multiplication. _SIAM Journal on Computing_, 36(1):132-157, 2006.
* [14] P. Drineas, R. Kannan, and M. W. Mahoney. Fast monte carlo algorithms for matrices ii: Computing a low-rank approximation to a matrix. _SIAM Journal on computing_, 36(1):158-183, 2006.
* [15] L. Feng, O. Katz, C. Haack, M. Maghrebi, A. V. Gorshkov, Z. Gong, M. Cetina, and C. Monroe. Continuous symmetry breaking in a trapped-ion spin chain. _Nature_, pages 1-5, 2023.
* [16] D. P. Francis and K. Raimond. A practical streaming approximate matrix multiplication algorithm. _Journal of King Saud University-Computer and Information Sciences_, 34(1):1455-1465, 2022.
* [17] V. Gupta, S. Wang, T. Courtade, and K. Ramchandran. Oversketch: Approximate matrix multiplication for the cloud. In _2018 IEEE International Conference on Big Data (Big Data)_, pages 298-304. IEEE, 2018.
* [18] R. Hu, S. L. Chau, D. Sejdinovic, and J. Glaunes. Giga-scale kernel matrix-vector multiplication on gpu. _Advances in Neural Information Processing Systems_, 35:9045-9057, 2022.

* Intel [2019] Intel. Intel 64 and ia-32 architectures optimization reference manual, [https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf](https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf), 2019. Last Accessed: 2023-10-12.
* Jain et al. [2022] R. Jain, G. Piliouras, and R. Sim. Matrix multiplicative weights updates in quantum zero-sum games: Conservation laws & recurrence. _Advances in Neural Information Processing Systems_, 35:4123-4135, 2022.
* Khudia et al. [2021] D. Khudia, J. Huang, P. Basu, S. Deng, H. Liu, J. Park, and M. Smelyanskiy. Fbgemm: Enabling high-performance low-precision deep learning inference. _arXiv preprint arXiv:2101.05615_, 2021.
* Krahmer and Ward [2011] F. Krahmer and R. Ward. New and improved johnson-lindenstrauss embeddings via the restricted isometry property. _SIAM Journal on Mathematical Analysis_, 43(3):1269-1281, 2011.
* Matsui and Satah [2018] Y. Matsui and S. Satah. Revisiting column-wise vector quantization for memory-efficient matrix multiplication. In _2018 25th IEEE International Conference on Image Processing (ICIP)_, pages 1158-1162. IEEE, 2018.
* Matsumoto et al. [2012] K. Matsumoto, N. Nakasato, and S. G. Sedukhin. Performance tuning of matrix multiplication in opencl on different gpus and cpus. In _2012 SC Companion: High Performance Computing, Networking Storage and Analysis_, pages 396-405. IEEE, 2012.
* Mroueh et al. [2017] Y. Mroueh, E. Marcheret, and V. Goel. Co-occurring directions sketching for approximate matrix multiply. In _Artificial Intelligence and Statistics_, pages 567-575. PMLR, 2017.
* Muralee Krishnan et al. [2020] N. K. Muralee Krishnan, S. Hosseini, and A. Khisti. Coded sequential matrix multiplication for straggler mitigation. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 16060-16069. Curran Associates, Inc., 2020.
* [27] NA. Matrix market, http://[https://math.nist.gov/MatrixMarket//](https://math.nist.gov/MatrixMarket//), 2007. Last Accessed: 2023-08-09.
* [28] NA. Pytorch homepage, [https://pytorch.org/](https://pytorch.org/), 2023.
* Osawa et al. [2017] K. Osawa, A. Sekiya, H. Naganuma, and R. Yokota. Accelerating matrix multiplication in deep learning by using low-rank approximation. In _2017 International Conference on High Performance Computing & Simulation (HPCS)_, pages 186-192. IEEE, 2017.
* Santo [2012] R. d. E. Santo. Principal component analysis applied to digital image compression. _Einstein (Sao Paulo)_, 10:135-139, 2012.
* Sarlos [2006] T. Sarlos. Improved approximation algorithms for large matrices via random projections. In _2006 47th annual IEEE symposium on foundations of computer science (FOCS'06)_, pages 143-152. IEEE, 2006.
* Strassen et al. [1969] V. Strassen et al. Gaussian elimination is not optimal. _Numerische mathematik_, 13(4):354-356, 1969.
* Wold et al. [1987] S. Wold, K. Esbensen, and P. Geladi. Principal component analysis. _Chemometrics and intelligent laboratory systems_, 2(1-3):37-52, 1987.
* Wu et al. [2016] S. Wu, S. Bhojanapalli, S. Sanghavi, and A. G. Dimakis. Single pass pca of matrix products. _Advances in Neural Information Processing Systems_, 29, 2016.
* Xiao et al. [2023] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In _International Conference on Machine Learning_, pages 38087-38099. PMLR, 2023.
* Ye et al. [2016] Q. Ye, L. Luo, and Z. Zhang. Frequent direction algorithms for approximate matrix multiplication with applications in cca. _computational complexity_, 1(m3):2, 2016.

* Yu et al. [2017] Q. Yu, M. Maddah-Ali, and S. Avestimehr. Polynomial codes: an optimal design for high-dimensional coded matrix multiplication. _Advances in Neural Information Processing Systems_, 30, 2017.
* Yue et al. [2023] B. Yue et al. _Optimizing Out-Of-Memory Sparse-Dense Matrix Multiplication_. PhD thesis, Massachusetts Institute of Technology, 2023.

## Appendix A Details of Downstream Tasks

We discuss the details of four selected downstream tasks in the following. The application errors (\(E\)) when applying AMM or MM on those downstream tasks are summarized in Table 8.

**Principal Component Analysis (PCA).** PCA is a popular statistical function for dimensionality reduction [33, 30]. It computes the rank-r approximation of a matrix \(A\) as \(\hat{A}_{r}\), and the application error \(E\) is defined as \(E=||A-\hat{A}_{r}||/||A||\). PCA is required to compute the covariance matrix, and the involved MM can be replaced by AMM. We conducted the PCA task on the _SIFT10K_ dataset following the methodology outlined in [34]. Because the number of rows is exceptionally small (128) in comparison to the substantial column count (10000), the tuning parameter \(\omega\) is configured at \(10\%\) for PQ and VQ, as their adjustment is row-relevant (Section 2.3). For the remaining AMM, we set \(\omega\) to \(1\%\).

**Machine Learning Training.** We implement the methodology outlined in prior work [1], incorporating AMM techniques into three fully connected layers of an MLP model during the machine learning training. We use different configurations of hidden layer dimensions in our evaluation, i.e., 500-D and 2000-D, which involve relatively smaller and larger weight matrices, respectively. There are thousands of Stochastic Gradient Descent (SGD) iterations in training, and each SGD iteration utilizes AMM to forward the training loss before randomly updating the weight matrices. We report the average \(\epsilon\) of first 10 SGD iterations during training, as they are most meaningful for the training task. The \(E\) is referred to as the _classification error_ in validating neural networks [1]. We exclude VQ and PQ, because they necessitate costly rebuilding of the entire codebook from scratch for each new version of the weight matrix, and it requires \(10^{7}\) ms \(l\) for building codebook 1000 times even in 500-D hidden layer case.

**Machine Learning Inference.** For inference, we apply AMM to the final dense layer in the pre-trained model discussed in [6] and set \(\omega\) to \(2.56\%\). The machine learning model also works as classifiers in [6], and the meaning of \(E\) is the same as that in the case of machine learning training. We employ _CIFAR100_ datasets as the illustration example.

**Unitary Transformation.** Unitary transformation is one of the key operations in the scientific computing of quantum physics [15, 10], and it is also the building block of more sophisticated quantum transformations, such as the canonical transformation in Quantum Zero-Sum Games [20]. Specifically, it transforms the quantum state matrix \(q\) into \(Q\) by two consecutive MM with a unitary gate. All of these MM are possible to be replaced by AMM, which transforms \(q\) into \(\hat{Q}\) instead of \(Q\). We report the average \(\epsilon\) of these two multiplications. \(Q^{2}\) is proportional to the probability of collapsing

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|} \hline Algorithms & & PCA & MLT(500D) & MLT(2000D) & MLT & UT \\ \hline \multirow{3}{*}{Pruning-based} & INTS & **0.1323** & **0.0156** & **0.0166** & **0.2953** & **0.3968** \\ \cline{2-6}  & CRS & **0.2403** & **0.0658** & **0.1022** & **0.5510** & **0.0048** \\ \cline{2-6}  & CS & 0.2002 & 0.0752 & 0.0962 & 0.5900 & 0.0213 \\ \hline \multirow{4}{*}{Extraction-based} & CooFD & 0.1323 & 0.0207 & 0.0275 & 0.9751 & 0.0191 \\ \cline{2-6}  & BLOCKLRA & 0.2905 & 0.0184 & 0.0165 & 0.9298 & 0.0052 \\ \cline{2-6}  & FastJIL & 0.2538 & 0.0783 & 0.1046 & 0.6535 & 0.2390 \\ \cline{2-6}  & VQ & 0.4829 & 7.8A. & N.A. & 0.9821 & 1.0000 \\ \cline{2-6}  & PQ & 0.9994 & N.A. & N.A. & 0.9900 & 1.0000 \\ \hline \multirow{4}{*}{Hybrid} & RIP & **0.2509** & **0.0733** & **0.0780** & **0.6142** & **0.0265** \\ \cline{2-6}  & SMP-PCA & **0.2112** & **0.0764** & **0.0891** & **0.4959** & **0.0003** \\ \cline{1-1} \cline{2-6}  & WRIGHTback & 0.2286 & 0.0798 & 0.0519 & 0.5835 & 0.0411 \\ \cline{1-1} \cline{2-6}  & TwoMM & 0.2075 & 0.0769 & 0.1098 & 0.6645 & 0.5335 \\ \hline \multirow{2}{*}{Baselines} & NLMM & 0.1233 & 0.0186 & 0.0166 & 0.2952 & 0.0000 \\ \cline{2-6}  & TLMM & **0.1232** & **0.0187** & **0.0184** & **0.2952** & **0.0000** \\ \hline \end{tabular}
\end{table}
Table 8: Application errors \(E\) of AMM or MM. _MLT_, _MLT_ and _UT_ are abbreviations for _Machine Learning Training_, _Machine Learning Inference_ and _Unitary Transformation_, respectively. VQ and PQ are excluded in MLT due to the \(\geq 10^{7}ms\) overhead of codebook rebuilding.

into specific classic states when measured, and \(E\) is hence formulated as \(E=||Q^{2}-\hat{Q}^{2}||/||Q^{2}||\). For illustration purposes, we instantiate \(q\) as one of the \(QCD\) matrices, let the unitary gate exhibit Gaussian distributions [5], and ignore the normalization constants.