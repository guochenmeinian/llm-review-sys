# PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs

 Zhongkai Hao\({}^{1}\), Jiachen Yao\({}^{1*}\), Chang Su\({}^{1*}\), Hang Su\({}^{1}\), Ziao Wang\({}^{1}\),

Fanzhi Lu\({}^{1}\), Zeyu Xia\({}^{1}\), Yichi Zhang\({}^{1}\), Songming Liu\({}^{1}\), Lu Lu\({}^{2}\), Jun Zhu\({}^{1}\)

\({}^{1}\)Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, THBI Lab,

Tsinghua-Bosch Joint ML Center, Tsinghua University

\({}^{2}\) Department of Statistics and Data Science, Yale University

hzj21@mails.tsinghua.edu.cn; dcszj@tsinghua.edu.cn

Equal contribution. \({}^{\dagger}\)The corresponding author.

###### Abstract

While significant progress has been made on Physics-Informed Neural Networks (PINNs), a comprehensive comparison of these methods across a wide range of Partial Differential Equations (PDEs) is still lacking. This study introduces PINNacle, a benchmarking tool designed to fill this gap. PINNacle provides a diverse dataset, comprising over 20 distinct PDEs from various domains, including heat conduction, fluid dynamics, biology, and electromagnetics. These PDEs encapsulate key challenges inherent to real-world problems, such as complex geometry, multi-scale phenomena, nonlinearity, and high dimensionality. PINNacle also offers a user-friendly toolbox, incorporating about 10 state-of-the-art PINN methods for systematic evaluation and comparison. We have conducted extensive experiments with these methods, offering insights into their strengths and weaknesses. In addition to providing a standardized means of assessing performance, PINNacle also offers an in-depth analysis to guide future research such as domain decomposition methods and loss reweighting for handling multi-scale problems. To the best of our knowledge, it is the largest benchmark with a diverse and comprehensive evaluation that will undoubtedly foster further research in PINNs.

## 1 Introduction

Partial Differential Equations (PDEs) are of paramount importance in science and engineering, as they often underpin our understanding of intricate physical systems such as fluid flow, heat transfer, and stress distribution [37]. The computational simulation of PDE systems has been a focal point of research for an extensive period, leading to the development of numerical methods such as finite difference [6], finite element [45], and finite volume methods [14].

Recent advancements have led to the use of deep neural networks to solve forward and inverse problems involving PDEs [44, 62, 11, 54]. Among these, Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative to traditional numerical methods in solving such problems [44, 25]. PINNs leverage the underlying physical laws and available data to effectively handle various scientific and engineering applications. The growing interest in this field has spurred the development of numerous PINN variants, each tailored to overcome specific challenges or to enhance the performance of the original framework.

While PINN methods have achieved remarkable progress, a comprehensive comparison of these methods across diverse types of PDEs is currently lacking. Establishing such a benchmark is crucial as it could enable researchers to more thoroughly understand existing methods and pinpoint potential challenges. Despite the availability of several studies comparing sampling methods [60] and reweighting methods [2], there has been no concerted effort to develop a rigorous benchmark using challenging datasets from real-world problems. The sheer variety and inherent complexity of PDEs make it difficult to conduct a comprehensive analysis. Moreover, different mathematical properties and application scenarios further complicate the task, requiring the benchmark to be adaptable and exhaustive.

To resolve these challenges, we propose PINNacle, a comprehensive benchmark for evaluating and understanding the performance of PINNs. As shown in Fig. 1, PINNacle consists of three major components -- a diverse dataset, a toolbox, and evaluation modules. The dataset comprises tasks from over 20 different PDEs from various domains, including heat conduction, fluid dynamics, biology, and electromagnetics. Each task brings its own set of challenges, such as complex geometry, multi-scale phenomena, nonlinearity, and high dimensionality, thus providing a rich testing ground for PINNs. The toolbox incorporates more than 10 state-of-the-art (SOTA) PINN methods, enabling a systematic comparison of different strategies, including loss reweighting, variational formulation, adaptive activations, and domain decomposition. These methods can be flexibly applied to the tasks in the dataset, offering researchers a convenient way to evaluate the performance of PINNs which is also user-friendly for secondary development. The evaluation modules provide a standardized means of assessing the performance of different PINN methods across all tasks, ensuring consistency in comparison and facilitating the identification of strengths and weaknesses in various methods.

PINNacle provides a robust, diverse, and comprehensive benchmark suite for PINNs, contributing significantly to the field's understanding and application. It represents a major step forward in the evolution of PINNs which could foster more innovative research and development in this exciting field. Code and data are publicly available at [https://github.com/i207M/PINNacle](https://github.com/i207M/PINNacle).

In a nutshell, our contributions can be summarized as follows:

* We design a dataset encompassing over 20 challenging PDE problems. These problems encapsulate several critical challenges faced by PINNs, including handling complex geometries, multi-scale phenomena, nonlinearity, and high-dimensional problems.
* We systematically evaluate more than 10 carefully selected representative variants of PINNs. We conducted thorough experiments and ablation studies to evaluate their performance. To the best of our knowledge, this is the largest benchmark comparing different PINN variants.

Figure 1: Architecture of PINNacle. It contains a dataset covering more than 20 PDEs, a toolbox that implements about 10 SOTA methods, and an evaluation module. These methods have a wide range of application scenarios like fluid mechanics, electromagnetism, heat conduction, geophysics, and so on.

* We provide an in-depth analysis to guide future research. We show using loss reweighting and domain decomposition methods could improve the performance on multi-scale and complex geometry problems. Variational formulation achieves better performance on inverse problems. However, few methods can adequately address nonlinear problems, indicating a future direction for exploration and advancement.

## 2 Related Work

### Benchmarks and datasets in scientific machine learning

The growing trend of AI in scientific research has stimulated the development of various benchmarks and datasets, which differ greatly in data formats, sizes, and governing principles. For instance, [33] presents a benchmark for comparing neural operators, while [3; 41] benchmarks methods for learning latent Newtonian mechanics. Furthermore, domain-specific datasets and benchmarks exist in fluid mechanics [20], climate science [42; 5], quantum chemistry [1], and biology [4].

Beyond these domain-specific datasets and benchmarks, physics-informed machine learning has received considerable attention [18; 9] since the advent of Physics-Informed Neural Networks (PINNs) [44]. These methods successfully incorporate physical laws into model training, demonstrating immense potential across a variety of scientific and engineering domains. Various papers have compared different components within the PINN framework; for instance, [10] and [60] investigate the sampling methods of collocation points in PINNs, and [2] compare reweighting techniques for different loss components. PDEBench [52] and PDEArena [17] design multiple tasks to compare different methods in scientific machine learning such as PINNs, FNO, and U-Net. Nevertheless, a comprehensive comparison of various PINN approaches remains absent in the literature.

### Softwares and Toolboxes

A plethora of software solutions have been developed for solving PDEs with neural networks. These include SimNet [19], NeuralPDE [43], TorchDiffEq [8], and PyDEs [29]. More recently, DeepXDE [34] has been introduced as a fundamental library for implementing PINNs across different backends. However, there remains a void for a toolbox that provides a unified implementation for advanced PINN variants. Our PINNacle fills this gap by offering a flexible interface that facilitates the implementation and evaluation of diverse PINN variants. We furnish clear and concise code for researchers to execute benchmarks across all problems and methods.

### Variants of Physics-informed neural networks

The PINNs have received much attention due to their remarkable performance in solving both forward and inverse PDE problems. However, vanilla PINNs have many limitations. Researchers have proposed numerous PINN variants to address challenges associated with high-dimensionality, non-linearity, multi-scale issues, and complex geometries [18; 9; 25; 30]. Broadly speaking, these variants can be categorized into: loss reweighting/resampling [57; 58; 53; 60; 40], innovative optimizers [61], novel loss functions such as variational formulations [62; 26; 27; 28] or regularization terms [63; 50], and novel architectures like domain decomposition [21; 31; 38; 24] and adaptive activations [23; 22]. These variants have enhanced PINN's performance across various problems. Here we select representative methods from each category and conduct a comprehensive analysis using our benchmark dataset to evaluate these variants.

## 3 PINNacle: A Hierarchical Benchmark for PINNs

In this section, we first introduce the preliminaries of PINNs. Then we introduce the details of datasets (tasks), PINN methods, the toolbox framework, and the evaluation metrics.

### Preliminaries of Physics-informed Neural Networks

Physics-informed neural networks are neural network-based methods for solving PDEs as well as inverse problems of PDEs, which have received much attention recently. Specifically, let's consider a general Partial Differential Equation (PDE) system defined on \(\Omega\), which can be represented as:

\[\mathcal{F}(u(x);x) = 0,\quad x\in\Omega, \tag{1}\] \[\mathcal{B}(u(x);x) = 0,\quad x\in\partial\Omega. \tag{2}\]

where \(\mathcal{F}\) is a differential operator and \(\mathcal{B}\) is the boundary/initial condition. PINN uses a neural network \(u_{\theta}(x)\) with parameters \(\theta\) to approximate \(u(x)\). The objective of PINN is to minimize the following loss function:

\[\mathcal{L}(\theta)=\frac{w_{c}}{N_{c}}\sum_{i=1}^{N_{c}}||\mathcal{F}(u_{ \theta}(x_{c}^{i});x_{c}^{i})||^{2}+\frac{w_{b}}{N_{b}}\sum_{i=1}^{N_{b}}|| \mathcal{B}(u_{\theta}(x_{b}^{i});x_{b}^{i})||^{2}+\frac{w_{d}}{N_{d}}\sum_{i =1}^{N_{d}}||u_{\theta}(x_{d}^{i})-u(x_{d}^{i})||^{2}. \tag{3}\]

where \(w_{c},w_{b},w_{d}\) are weights. The first two terms enforce the PDE constraints on \(\{x_{c}^{i}\}_{1\ldots N_{c}}\) and boundary conditions on \(\{x_{b}^{i}\}_{1\ldots N_{b}}\). The last term is data loss, which is optional when there is data available. However, PINNs have several inherent drawbacks. First, PINNs optimize a mixture of imbalance loss terms which might hinder its convergence as illustrated in [57]. Second, nonlinear or stiff PDEs might lead to unstable optimization [58]. Third, the vanilla MLPs might have difficulty in representing multi-scale or high-dimensional functions. For example, [30] shows that vanilla PINNs only work for a small parameter range, even in a simple convection problem. To resolve these challenges, numerous variants of PINNs are proposed. However, a comprehensive comparison of these methods is lacking, and thus it is imperative to develop a benchmark.

### Datasets

To effectively compare PINN variants, we've curated a set of PDE problems (datasets) representing a wide range of challenges. We chose PDEs from diverse domains, reflecting their importance in science and engineering. Our dataset includes 22 unique cases, with further details in Appendix B.

* The **Burgers' Equation**, fundamental to fluid mechanics, considering both one and two-dimensional problems.
* The **Poisson's Equation**, widely used in math and physics, with four different cases.
* The **Heat Equation**, a time-dependent PDE that describes diffusion or heat conduction, demonstrated in four unique cases.
* The **Navier-Stokes Equation**, describing the motion of viscous fluid substances, showcased in three scenarios: a lid-driven flow (NS2d-C), a geometrically complex backward step flow (NS2d-CG), and a time-dependent problem (NS2d-LT).
* The **Wave Equation**, modeling wave behavior, exhibited in three cases.
* **Chaotic PDEs**, featuring two popular examples: the Gray-Scott (GS) and Kuramoto-Sivashinsky (KS) equations.
* **High Dimensional PDEs**, including the high-dimensional Poisson equation (PNd) and the high-dimensional diffusion or heat equation (HNd).
* **Inverse Problems**, focusing on the reconstruction of the coefficient field from noisy data for the Poisson equation (PIInv) and the diffusion equation (HInv).

It is important to note that we have chosen PDEs encompassing a wide range of mathematical properties. This ensures that the benchmarks do not favor a specific type of PDE. The selected PDE problems introduce several core challenges, which include:

* **Complex Geometry**: Many PDE problems involve complex or irregular geometry, such as heat conduction or wave propagation around obstacles. These complexities pose significant challenges for PINNs in terms of accurate boundary behavior representation.
* **Multi-Scale Phenomena**: Multi-scale phenomena, where the solution varies significantly over different scales, are prevalent in situations such as turbulent fluid flow. Achieving a balanced representation across all scales is a challenge for PINNs in multi-scale scenarios.
* **Nonlinear Behavior**: Many PDEs exhibit nonlinear or even chaotic behavior, where minor variations in initial conditions can lead to substantial divergence in outcomes. The optimization of PINNs becomes intriguing on nonlinear PDEs.

* **High Dimensionality**: High-dimensional PDE problems, frequently encountered in quantum mechanics, present significant challenges for PINNs due to the "curse of dimensionality". This term refers to the increase in computational complexity with the addition of each dimension, accompanied by statistical issues like data sparsity in high-dimensional space.

These challenges are selected due to their frequent occurrence in numerous real-world applications. As such, a method's performance in addressing these challenges serves as a reliable indicator of its overall practical utility. Table 1 presents a detailed overview of the dataset, the PDEs, and the challenges associated with these problems. We generate data using FEM solver provided by COMSOL 6.0 [39] for problems with complex geometry and spectral method provided by Chebfun [12] for chaotic problems. More details can be found in Appendix B.

### Methods and Toolbox

After conducting an extensive literature review, we present an overview of diverse PINNs approaches for comparison. Then we present the high-level structure of our PINNacle.

#### 3.3.1 Methods

As mentioned above, variants of PINNs are mainly based on loss functions, architecture, and optimizer [18]. The modifications to loss functions can be divided into reweighting existing losses and developing novel loss functions like regularization and variational formulation. Variants of architectures include using domain decomposition and adaptive activations.

The methods discussed are directly correlated with the challenges highlighted in Table 1. For example, domain decomposition methods are particularly effective for problems involving complex geometries and multi-scale phenomena. Meanwhile, loss reweighting strategies are adept at addressing imbalances in problems with multiple losses. We have chosen variants from these categories based on their significant contributions to the field.

Here, we list the primary categories and representative methods as summarized in Table 2:

* **Loss reweighting/Resampling (2\(\sim\)4)**: PINNs are trained with a mixed loss of PDE residuals, boundary conditions, and available data losses shown in Eq 3. Various methods [57, 59, 2, 35, 47] propose different strategies to adjust these weights \(w_{c},w_{b}\) and \(w_{d}\) at different epochs or resample collocation points \(\{x^{i}_{c}\}\) and \(\{x^{i}_{b}\}\) in Eq 3, which indirectly adjust the weights [60, 40]. We choose three famous examples, i.e., reweighting using gradient norms (PINN-LRA) [57], using neural tangent kernel (PINN-NTK) [59], and residual-based resampling (RAR)[34, 60].
* **Novel optimizer (5)**: To handle the problem of multi-scale objectives, some new optimizers [32, 61] are proposed. We chose MultiAdam, which is resistant to domain scale changes.
* **Novel loss functions (6\(\sim\)7)**: Some works introduce novel loss functions like variational formulation [49, 28, 27] and regularization terms to improve training. We choose hp-VPINN [27] and gPINN [63, 50], which are representative examples from these two categories.

\begin{table}
\begin{tabular}{c|c c c c} \hline Dataset & Complex geometry & Multi-scale & Nonlinearity & High dim \\ \hline Burgers\({}^{1\sim 2}\) & \(\times\) & \(\times\) & \(\surd\) & \(\times\) \\ Poisson\({}^{3\sim 6}\) & \(\times/\surd\) & \(\times/\surd\) & \(\times\) & \(\times\) \\ Heat\({}^{7\sim 10}\) & \(\times/\surd\) & \(\times/\surd\) & \(\times\) & \(\times\) \\ NS\({}^{11\sim 13}\) & \(\times/\surd\) & \(\times/\surd\) & \(\surd\) & \(\times\) \\ Wave\({}^{14\sim 16}\) & \(\times/\surd\) & \(\times/\surd\) & \(\times\) & \(\times\) \\ Chaotic\({}^{17\sim 18}\) & \(\times\) & \(\surd\) & \(\surd\) & \(\times\) \\ High dim\({}^{19\sim 20}\) & \(\times\) & \(\times\) & \(\times\) & \(\surd\) \\ Inverse \({}^{21\sim 22}\) & \(\times\) & \(\times\) & \(\surd\) & \(\times\) \\ \hline \end{tabular}
\end{table}
Table 1: Overview of our datasets along with their challenges. We chose 22 cases in total to evaluate the methods of PINNs. The left picture shows the visualization of cases with these four challenges, i.e., complex geometry, multi-scale, nonlinearity, and high dimension.

* **Novel activation architectures (8\(\sim\)10)**: Some works propose various network architectures, such as using CNN and LSTM [64; 15; 46], custom activation functions [22; 23], and domain decomposition [21; 48; 24; 38]. Among adaptive activations for PINNs, we choose LAAF [22] and GAAF [23]. Domain decomposition is a method that divides the whole domain into multiple subdomains and trains subnetworks on these subdomains. It is helpful for solving multi-scale problems, but multiple subnetworks increase the difficulty of training. XPINNs, cPINNs, and FBPINNs [21; 24; 38] are three representative examples. We choose FBPINNs which is the state-of-the-art domain decomposition that applies domain-specific normalization to stabilize training.

#### 3.3.2 Structure of Toolbox

We provide a user-friendly and concise toolbox for implementing, training, and evaluating diverse PINN variants. Specifically, our codebase is based on DeepXDE and provides a series of encapsulated classes and functions to facilitate high-level training and custom PDEs. These utilities allow for a standardized and streamlined approach to the implementation of various PINN variants and PDEs. Moreover, we provided many auxiliary functions, including computing different metrics, visualizing predictions, and recording results.

Despite the unified implementation of diverse PINNs, we also design an adaptive multi-GPU parallel training framework to enhance the efficiency of systematic evaluations of PINN methods. It addresses the parallelization phase of training on multiple tasks, effectively balancing the computational loads of multiple GPUs. It allows for the execution of larger and more complex tasks. In a nutshell, we provide an example code for training and evaluating PINNs on two Poisson equations using our PINNacle framework in Appendix D.

### Evaluation

To comprehensively analyze the discrepancy between the PINN solutions and the true solutions, we adopt multiple metrics to evaluate the performance of the PINN variants. Generally, we choose several metrics that are commonly used in literature that apply to all methods and problems. We suppose that \(\mathbf{y}=(y_{i})_{i=1}^{n}\) is the prediction and \(\mathbf{y}^{\prime}=(y^{\prime}_{i})_{i=1}^{n}\) to is ground truth, where \(n\) is the number of testing examples. Specifically, we use \(\ell_{2}\) relative error (\(\mathrm{L2RE}\)), and \(\ell_{1}\) relative error (\(\mathrm{L1RE}\)) which are two most commonly used metrics to measure the global quality of the solution,

\[\mathrm{L2RE}=\sqrt{\frac{\sum_{i=1}^{n}(y_{i}-y^{\prime}_{i})^{2}}{\sum_{i=1}^ {n}{y^{\prime}_{i}}^{2}}},\,\mathrm{L1RE}=\frac{\sum_{i=1}^{n}|y_{i}-y^{\prime }_{i}|}{\sum_{i=1}^{n}|y^{\prime}_{i}|}. \tag{4}\]

We also compute max error (mERR in short), mean square error (MSE), and Fourier error (fMSE) for a detailed analysis of the prediction. These three metrics are computed as follows:

\[\mathrm{MSE}=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-y^{\prime}_{i})^{2},\,\,\mathrm{ mERR}=\max_{i}|y_{i}-y^{\prime}_{i}|,\,\mathrm{fMSE}=\frac{\sqrt{\sum_{k_{\min }}^{k_{\max}}|\mathcal{F}(\mathbf{y})-\mathcal{F}(\mathbf{y^{\prime}})|^{2}}}{k_{\max}- k_{\min}+1}, \tag{5}\]

where \(\mathcal{F}\) denotes Fourier transform of \(\mathbf{y}\) and \(k_{\min}\), \(k_{\max}\) are chosen similar to PDEBench [52]. Besides, for time-dependent problems, investigating the quality of the solution with time is important. Therefore we compute the L2RE error varying with time in Appendix E.2.

We assess the performance of PINNs against the reference from numerical solvers. Experimental results utilizing the \(\ell_{2}\) relative error (L2RE) metric are incorporated within the main text, while a more exhaustive set of results, based on the aforementioned metrics, is available in the Appendix E.1.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline  & Complex Geometry & Multi-scale & Nonlinearity & High dim \\ \hline Vanilla PINN [1] & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ Reweighting/Resampling\({}^{2\sim 4}\) & \(\surd\) & \(\surd\) & \(\times\) & \(\times\) \\ Novel Optimizer\({}^{7}\) & \(\times\) & \(\surd\) & \(\times\) & \(\times\) \\ Novel Loss Functions\({}^{5\sim 6}\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\ Novel Architecture\({}^{8\sim 10}\) & \(\surd\) & \(\surd\) & \(\times\) & \(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Overview of methods in our PINNacle. \(\surd\) denotes the method is potentially designed to solve or show empirical improvements for problems encountering the challenge and vice versa.

[MISSING_PAGE_FAIL:7]

geometries and multi-scale systems. For example, FBPINN achieves the smallest error on the chaotic GS equation (\(7.99\%\)), while LAAF delivers the best fitting result on Heat-2d-CG (\(2.39\%\)).

**Discussion.** For challenges related to complex geometries and multi-scale phenomena, some methods can mitigate these issues by implementing mechanisms like loss reweighting, novel optimizers, and better capacity through adaptive activation. This holds true for the 2D cases of Heat and Poisson equations, which are classic linear equations. However, when systems have higher dimensions (Poisson3d-CG) or longer time spans (Heat2d-LT), all methods fail to solve, highlighting the difficulties associated with complex geometries and multi-scale systems.

In contrast, nonlinear, long-time PDEs like 2D Burgers, NS, and KS pose challenges for most methods. These equations are sensitive to initial conditions, resulting in complicated solution spaces and more local minima for PINNs [51]. The Wave equation, featuring a second-order time derivative and periodic behavior, is particularly hard for PINNs, which often become unstable and may violate conservation laws [24; 55]. Although all methods perform well on Poisson-Nd, only PINN with LBFGS solves Heat-Nd, indicating the potential of a second-order optimizer for solving high dimensional PDEs[53].

### Parameterized PDE Experiments

To investigate whether PINNs could handle a class of PDEs, we design this experiment to solve the same PDEs with different parameters. We choose 6 PDEs, i.e., Burgers2d-C, Poisson2d-C, Heat2d-MS, NS-C, Wave1d-C, and Heat-Nd (HNd), with each case containing five parameterized examples. Details of the parametrized PDEs are shown in Appendix B. Here we report the average L2RE metric on these parameterized PDEs for every case, and results are shown in the following Table 4. First, we see that compared with the corresponding cases in Table E.1, the mean L2RE of parameterized PDEs is usually higher. We suggest that this is because there are some difficult cases under certain parameters for these PDEs with very high errors. Secondly, we find that PINN-NTK works well on parameterized PDE tasks which achieve three best results among all six experiments. We speculate that solving PDEs with different parameters requires different weights for loss terms, and PINN-NTK is a powerful method for automatically balancing these weights.

### Hyperparameter Analysis

The performance of PINNs is strongly affected by hyperparameters, with each variant introducing its own unique set. The results are shown in Figure 2. We focus on a set of problems, i.e., Burgers1d, GS, Heat2d-CG, and Poisson2d-C. Detailed results and additional findings are in Appendix E.2.

**Batch size and training epochs.** Figure 19 presents the effects of varying batch sizes and training epochs. Larger batch sizes generally yield better outcomes due to more accurate gradient estimations, though saturation is observed beyond a batch size of 2048 for the GS and Poisson2d-C problems. Similarly, increasing the number of training epochs reduces the L2RE, indicating an improvement in model accuracy. However, this benefit plateaus around 20k to 80k epochs, where further increases in epochs do not significantly reduce the error.

**Learning Rates.** The performance of standard PINNs under various learning rates and learning rate schedules is shown in Figure 3. We observe that the influence of the learning rate on performance is intricate, with optimal learning rates varying across problems. Furthermore, PINN training tends to be unstable. High learning rates, such as \(10^{-2}\), often lead to error spikes, while low learning rates,

\begin{table}
\begin{tabular}{c c c|c c c|c c c|c c} \hline \hline \multirow{2}{*}{L2RE} & Name & Vanilla & \multicolumn{3}{c|}{Loss Reweighting/Sampling} & \multicolumn{3}{c|}{Optimizer} & \multicolumn{3}{c|}{Loss functions} & \multicolumn{3}{c}{Architecture} \\ \cline{2-13}  & – & PINN & LRA & NTK & RAR & Multi-Adam & gPINN & vPINN & LAAF & GAAF & FBPINN \\ \hline Burgers-P & 2d-C & 4.74E-01 & 4.36E-01 & **4.13E-01** & 4.71E-01 & 4.93E-01 & 4.91E-01 & 2.82E+04 & 4.37E-01 & 4.34E-01 & - \\ Poisson-P & 2d-C & 2.24E-01 & 7.07E-02 & **1.66E-02** & 2.33E-01 & 8.24E-02 & 4.03E-01 & 5.51E-1 & 1.84E-01 & 2.97E-01 & 2.87E-2 \\ Heat-P & 2d-MS & 1.73E-01 & 1.23E-01 & 1.50E-01 & 1.53E-01 & 4.00E-01 & 4.99E-01 & 5.12E-1 & 6.27E-02 & 1.89E-01 & 2.46E-1 \\ NS-P & 2d-C & 3.98E-01 & – & 4.52E-01 & 3.91E-01 & 9.33E-01 & 7.19E-01 & 3.76E-01 & 3.43E-01 & 4.85E-01 & 3.99E-1 \\ Waw-P & 1d-C & 5.22E-01 & 3.44E-01 & **2.66E-01** & 5.08E-01 & 6.89E-01 & 7.66E-01 & 3.58E-1 & 4.03E-01 & 9.00E-01 & 1.15E-01 \\ High dim-P & HNd & 7.66E-03 & 6.53E-03 & 9.04E-03 & 8.07E-03 & **2.22E-03** & 7.87E-03 & - & 6.97E-03 & 1.94E-01 & - \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results of different PINN variants on parametric PDEs. We report average L2RE on all examples within a class of PDE. We **bold** the best results across all methods.

like \(10^{-5}\), result in slow convergence. Our findings suggest that a moderate learning rate, such as \(10^{-3}\) or \(10^{-4}\), or a step decay learning rate schedule, tends to yield more stable performance.

## 5 Limitations

First, real-world problems are often more complex, with giant geometric domains or chaotic behaviors. Good performance on PINNacle does not guarantee it solves practical problems. We could explore larger-scale PINN training methods or efficient domain decomposition methods[7]. Second, the issues of safety in the PINN methods pose potential roadblocks. Developing theoretical convergence for PINN like stability and convergence analysis [13] could help resolve these limitations.

## 6 Conclusion

In this work, we introduced PINNacle, a comprehensive benchmark offering a user-friendly toolbox that encompasses over 20 PDE problems and 10 PINN methods with extensive experiments and ablation studies. Looking forward, we plan to expand the benchmark by integrating additional state-of-the-art methods and incorporating more practical problem scenarios. Our analysis of the experimental results yields several key insights. First, domain decomposition is beneficial for addressing problems characterized by complex geometries, and PINN-NTK is a strong method for balancing loss weights as experiments show. Second, selecting appropriate hyperparameters is crucial to the performance of PINNs. However, the best hyperparameters usually vary with PDEs. Third, we identify high-dimensional and nonlinear problems as a pressing challenge. The overall performance of PINNs is not yet on par with traditional numerical methods [16]. Fourth, there are only a few attempts exploring of PINNs' loss landscape [30]. Finally, integrating the strengths of neural networks with numerical methods like preconditioning, weak formulation, and multigrid may present a promising avenue toward overcoming the challenges[36].

Figure 3: Convergence curve of PINNs with different learning rate schedules on Burgers1d, Heat2d-CG, and Poisson2d-C.

Figure 2: Performance of vanilla PINNs under different batch sizes (number of collocation points), which is shown in the left figure; and number of training epochs, which is shown in the right figure.

## Acknowledgement

This work was supported by NSFC Projects (Nos. 62350080, 92370124, 92248303, 62276149, U2341228, 62061136001, 62076147), BNRist (BNR2022RC01006), Tsinghua Institute for Guo Qiang, and the High Performance Computing Center, Tsinghua University. J. Zhu was also supported by the XPIorer Prize.

## References

* [1] Simon Axelrod and Rafael Gomez-Bombarelli. Geom, energy-annotated molecular conformations for property prediction and molecular generation. _Scientific Data_, 9(1):185, 2022.
* [2] Rafael Bischof and Michael Kraus. Multi-objective loss balancing for physics-informed deep learning. _arXiv preprint arXiv:2110.09813_, 2021.
* [3] Aleksandar Botev, Andrew Jaegle, Peter Wirnsberger, Daniel Hennes, and Irina Higgins. Which priors matter? benchmarking models for learning latent dynamics. _arXiv preprint arXiv:2111.05458_, 2021.
* [4] Emmanuel Boutet, Damien Lieberherr, Michael Tognolli, Michel Schneider, Parit Bansal, Alan J Bridge, Sylvain Poux, Lydie Bougueleret, and Ioannis Xenarios. Uniprotkb/swiss-prot, the manually annotated section of the uniprot knowledgebase: how to use the entry view. _Plant bioinformatics: methods and protocols_, pages 23-54, 2016.
* [5] Salva Ruhling Cachay, Venkatesh Ramesh, Jason NS Cole, Howard Barker, and David Rolnick. Climart: A benchmark dataset for emulating atmospheric radiative transfer in weather and climate models. _arXiv preprint arXiv:2111.14671_, 2021.
* [6] DM Causon and CG Mingham. _Introductory finite difference methods for PDEs_. Bookboon, 2010.
* [7] Nithin Chalapathi, Yiheng Du, and Aditi Krishnapriyan. Scaling physics-informed hard constraints with mixture-of-experts. _arXiv preprint arXiv:2402.13412_, 2024.
* [8] Feiyu Chen, David Sondak, Pavlos Protopapas, Marios Mattheakis, Shuheng Liu, Devansh Agarwal, and Marco Di Giovanni. Neurodiffeq: A python package for solving differential equations with neural networks. _Journal of Open Source Software_, 5(46):1931, 2020.
* [9] Salvatore Cuomo, Vincenzo Schiano Di Cola, Fabio Giampaolo, Gianluigi Rozza, Maziar Raissi, and Francesco Piccialli. Scientific machine learning through physics-informed neural networks: where we are and what's next. _Journal of Scientific Computing_, 92(3):88, 2022.
* [10] Sourav Das and Solomon Tesfamariam. State-of-the-art review of design of experiments for physics-informed deep learning. _arXiv preprint arXiv:2202.06416_, 2022.
* [11] Arka Daw, Anuj Karpatne, William Watkins, Jordan Read, and Vipin Kumar. Physics-guided neural networks (pgnn): An application in lake temperature modeling. _arXiv preprint arXiv:1710.11431_, 2017.
* [12] Tobin A Driscoll, Nicholas Hale, and Lloyd N Trefethen. Chebfun guide, 2014.
* [13] Francisco Eiras, Adel Bibi, Rudy Bunel, Krishnamurthy Dj Dvijotham, Philip Torr, and M Pawan Kumar. Provably correct physics-informed neural networks. _arXiv preprint arXiv:2305.10157_, 2023.
* [14] Robert Eymard, Thierry Gallouet, and Raphaele Herbin. Finite volume methods. _Handbook of numerical analysis_, 7:713-1018, 2000.
* [15] Han Gao, Luning Sun, and Jian-Xun Wang. Phygeonet: Physics-informed geometry-adaptive convolutional neural networks for solving parameterized steady-state pdes on irregular domain. _Journal of Computational Physics_, 428:110079, 2021.

* [16] Tamara G Grossmann, Urszula Julia Komorowska, Jonas Latz, and Carola-Bibiane Schonlieb. Can physics-informed neural networks beat the finite element method? _arXiv preprint arXiv:2302.04107_, 2023.
* [17] Jayesh K Gupta and Johannes Brandstetter. Towards multi-spatiotemporal-scale generalized pde modeling. _arXiv preprint arXiv:2209.15616_, 2022.
* [18] Zhongkai Hao, Songming Liu, Yichi Zhang, Chengyang Ying, Yao Feng, Hang Su, and Jun Zhu. Physics-informed machine learning: A survey on problems, methods and applications. _arXiv preprint arXiv:2211.08064_, 2022.
* [19] Oliver Hennigh, Susheela Narasimhan, Mohammad Amin Nabian, Akshay Subramaniam, Kaustubh Tangsali, Zhiwei Fang, Max Rietmann, Wonmin Byeon, and Sanjay Choudhry. Nvidia simnet(tm): An ai-accelerated multi-physics simulation framework. In _International Conference on Computational Science_, pages 447-461. Springer, 2021.
* [20] Zizhou Huang, Teseo Schneider, Minchen Li, Chenfanfu Jiang, Denis Zorin, and Daniele Panozzo. A large-scale benchmark for the incompressible navier-stokes equations. _arXiv preprint arXiv:2112.05309_, 2021.
* [21] Ameya D Jagtap and George E Karniadakis. Extended physics-informed neural networks (xpinns): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations. In _AAAI Spring Symposium: MLPS_, 2021.
* [22] Ameya D Jagtap, Kenji Kawaguchi, and George Em Karniadakis. Locally adaptive activation functions with slope recovery for deep and physics-informed neural networks. _Proceedings of the Royal Society A_, 476(2239):20200334, 2020.
* [23] Ameya D Jagtap, Kenji Kawaguchi, and George Em Karniadakis. Adaptive activation functions accelerate convergence in deep and physics-informed neural networks. _Journal of Computational Physics_, 404:109136, 2020.
* [24] Ameya D Jagtap, Ehsan Kharazmi, and George Em Karniadakis. Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems. _Computer Methods in Applied Mechanics and Engineering_, 365:113028, 2020.
* [25] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. _Nature Reviews Physics_, 3(6):422-440, 2021.
* [26] Ehsan Kharazmi, Zhongqiang Zhang, and George Em Karniadakis. Variational physics-informed neural networks for solving partial differential equations. _arXiv preprint arXiv:1912.00873_, 2019.
* [27] Ehsan Kharazmi, Zhongqiang Zhang, and George Em Karniadakis. hp-vpinns: Variational physics-informed neural networks with domain decomposition. _Computer Methods in Applied Mechanics and Engineering_, 374:113547, 2021.
* [28] Reza Khodayi-Mehr and Michael Zavlanos. Varnet: Variational neural networks for the solution of partial differential equations. In _Learning for Dynamics and Control_, pages 298-307. PMLR, 2020.
* [29] R Khudorozhkov, S Tsimfer, and A Koryagin. Pydens framework for solving differential equations with deep learning. _arXiv preprint arXiv:1909.11544_, 2019.
* [30] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks. _Advances in Neural Information Processing Systems_, 34:26548-26560, 2021.
* [31] Ke Li, Kejun Tang, Tianfan Wu, and Qifeng Liao. D3m: A deep domain decomposition method for partial differential equations. _IEEE Access_, 8:5283-5294, 2019.
* [32] Yuhao Liu, Xiaojian Li, and Zhengxian Liu. An improved physics-informed neural network based on a new adaptive gradient descent algorithm for solving partial differential equations of nonlinear systems. 2022.

* [33] Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George Em Karniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data. _Computer Methods in Applied Mechanics and Engineering_, 393:114778, 2022.
* [34] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: A deep learning library for solving differential equations. _SIAM review_, 63(1):208-228, 2021.
* [35] Suryanarayana Maddu, Dominik Sturm, Christian L Muller, and Ivo F Sbalzarini. Inverse dirichlet weighting enables reliable training of physics informed neural networks. _Machine Learning: Science and Technology_, 3(1):015026, 2022.
* [36] Stefano Markidis. The old and the new: Can physics-informed deep-learning replace traditional linear solvers? _Frontiers in big Data_, 4:669097, 2021.
* [37] Keith W Morton and David Francis Mayers. _Numerical solution of partial differential equations: an introduction_. Cambridge university press, 2005.
* [38] Ben Moseley, Andrew Markham, and Tarje Nissen-Meyer. Finite basis physics-informed neural networks (fbpins): a scalable domain decomposition approach for solving differential equations. _arXiv preprint arXiv:2107.07871_, 2021.
* [39] COMSOL Multiphysics. Introduction to comsol multiphysics(r). _COMSOL Multiphysics, Burlington, MA, accessed Feb_, 9:2018, 1998.
* [40] Mohammad Amin Nabian, Rini Jasmine Gladstone, and Hadi Meidani. Efficient training of physics-informed neural networks via importance sampling. _Computer-Aided Civil and Infrastructure Engineering_, 36(8):962-977, 2021.
* [41] Karl Ottess, Arvi Gjoka, Joan Bruna, Daniele Panozzo, Benjamin Peherstorfer, Teseo Schneider, and Denis Zorin. An extensible benchmark suite for learning to simulate physical systems. _arXiv preprint arXiv:2108.07799_, 2021.
* [42] Evan Racah, Christopher Beckham, Tegan Maharaj, Samira Ebrahimi Kahou, Mr Prabhat, and Chris Pal. Extremeweather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events. _Advances in neural information processing systems_, 30, 2017.
* [43] Christopher Rackauckas and Qing Nie. Differentialequations. jl-a performant and feature-rich ecosystem for solving differential equations in julia. _Journal of open research software_, 5(1), 2017.
* [44] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. _Journal of Computational physics_, 378:686-707, 2019.
* [45] Junuthula Narasimha Reddy. _Introduction to the finite element method_. McGraw-Hill Education, 2019.
* [46] Pu Ren, Chengping Rao, Yang Liu, Jian-Xun Wang, and Hao Sun. Phycrnet: Physics-informed convolutional-recurrent network for solving spatiotemporal pdes. _Computer Methods in Applied Mechanics and Engineering_, 389:114399, 2022.
* [47] Franz M Rohrhofer, Stefan Posch, and Bernhard C Geiger. On the pareto front of physics-informed neural networks. _arXiv preprint arXiv:2105.00862_, 2021.
* [48] Khemraj Shukla, Ameya D Jagtap, and George Em Karniadakis. Parallel physics-informed neural networks via domain decomposition. _Journal of Computational Physics_, 447:110683, 2021.
* [49] Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial differential equations. _Journal of computational physics_, 375:1339-1364, 2018.
* [50] Hwijae Son, Jin Woo Jang, Woo Jin Han, and Hyung Ju Hwang. Sobolev training for the neural network solutions of pdes. 2021.

* [51] Sophie Steger, Franz M Rohrhofer, and Bernhard C Geiger. How pinns cheat: Predicting chaotic motion of a double pendulum. In _The Symbiosis of Deep Learning and Differential Equations II_, 2022.
* [52] Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Daniel MacKinlay, Francesco Alesiani, Dirk Pfluger, and Mathias Niepert. Pdebench: An extensive benchmark for scientific machine learning. _Advances in Neural Information Processing Systems_, 35:1596-1611, 2022.
* [53] Kejun Tang, Xiaoliang Wan, and Chao Yang. Das: A deep adaptive sampling method for solving partial differential equations. _arXiv preprint arXiv:2112.14038_, 2021.
* [54] Nanzhe Wang, Dongxiao Zhang, Haibin Chang, and Heng Li. Deep learning of subsurface flow via theory-guided neural network. _Journal of Hydrology_, 584:124700, 2020.
* [55] Sifan Wang, Shyam Sankaran, and Paris Perdikaris. Respecting causality is all you need for training physics-informed neural networks. _arXiv preprint arXiv:2203.07404_, 2022.
* [56] Sifan Wang, Shyam Sankaran, Hanwen Wang, and Paris Perdikaris. An expert's guide to training physics-informed neural networks. _arXiv preprint arXiv:2308.08468_, 2023.
* [57] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient flow pathologies in physics-informed neural networks. _SIAM Journal on Scientific Computing_, 43(5):A3055-A3081, 2021.
* [58] Sifan Wang, Hanwen Wang, and Paris Perdikaris. On the eigenvector bias of fourier feature networks: From regression to solving multi-scale pdes with physics-informed neural networks. _Computer Methods in Applied Mechanics and Engineering_, 384:113938, 2021.
* [59] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel perspective. _Journal of Computational Physics_, 449:110768, 2022.
* [60] Chenxi Wu, Min Zhu, Qinyang Tan, Yadhu Kartha, and Lu Lu. A comprehensive study of non-adaptive and residual-based adaptive sampling for physics-informed neural networks. _Computer Methods in Applied Mechanics and Engineering_, 403:115671, 2023.
* [61] Jiachen Yao, Chang Su, Zhongkai Hao, Songming Liu, Hang Su, and Jun Zhu. Multiadam: Parameter-wise scale-invariant optimizer for multiscale training of physics-informed neural networks. _arXiv preprint arXiv:2306.02816_, 2023.
* [62] Bing Yu et al. The deep ritz method: a deep learning-based numerical algorithm for solving variational problems. _Communications in Mathematics and Statistics_, 6(1):1-12, 2018.
* [63] Jeremy Yu, Lu Lu, Xuhui Meng, and George Em Karniadakis. Gradient-enhanced physics-informed neural networks for forward and inverse pde problems. _Computer Methods in Applied Mechanics and Engineering_, 393:114823, 2022.
* [64] Ruiyang Zhang, Yang Liu, and Hao Sun. Physics-informed multi-lstm networks for metamodeling of nonlinear structures. _Computer Methods in Applied Mechanics and Engineering_, 369:113226, 2020.
* [65] Zahari Zlatev, Ivan Dimov, Istvan Farago, and Agnes Havasi. _Richardson extrapolation: Practical aspects and applications_, volume 2. Walter de Gruyter GmbH & Co KG, 2017.

## Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [**TODO**] to [Yes], [No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [Yes] See Section 1.

Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] 3. Did you discuss any potential negative societal impacts of your work? [Yes] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]Overview of Appendices

We provide supplementary details about problems and experiments for the main text in the Appendix. In Appendix B, we provide mathematical descriptions and visualization for all PDEs in this paper. In Appendix C, we list the detailed hyperparameters and training/testing settings. In Appendix D, we provide a high-level overview of the codebase of the toolbox. In Appendix E, the results for the main experiments, i.e., the performance of L2RE, L1RE, MSE, and runtime for all methods on all PDEs are displayed. In Appendix F, we show the visualization results for several methods on some problems.

## Appendix B Details of PDEs and Methods

Here provide details of PDE tasks used for evaluating different variants of PINNs. Denote \(u\) to be the function to solve and \(x,t\) to be spatial and temporal variables.

### Definitions for PDEs in main experiments

#### 1. One-dimensional Burgers Equation (Burgers1d)

The Burgers 1D equation is given by

\[u_{t}+uu_{x}=\nu u_{xx}. \tag{6}\]

The domain is defined as

\[(x,t)\in\Omega=[-1,1]\times[0,1]. \tag{7}\]

The initial and boundary conditions are

\[u(x,0) = -\sin\pi x, \tag{8}\] \[u(-1,t)=u(1,t) = 0. \tag{9}\]

The parameter is

\[\nu=\frac{0.01}{\pi}. \tag{10}\]

#### 2. 2D Coupled Burgers equation (Burgers 2d)

The 2D Coupled Burgers equation is given by

\[\boldsymbol{u}_{t}+\boldsymbol{u}\cdot\nabla\boldsymbol{u}-\nu \Delta\boldsymbol{u}=0, \tag{11}\] \[\boldsymbol{u}(0,y,t)=\boldsymbol{u}(L,y,t),\quad\boldsymbol{u}( x,0,t)=\boldsymbol{u}(x,L,t),\] (12) \[\{x,y\}\in[0,L],\quad t\in[0,T], \tag{13}\]

Figure 4: Reference solution of Burgers1d using FEM solver.

The domain is defined as

\[(x,y,t)\in\Omega=[0,L]^{2}\times[0,1]. \tag{14}\]

The initial conditions are given by

\[\mathbf{w}(x,y)=\sum_{i=-L}^{L}\sum_{j=-L}^{L}\mathbf{a}_{ij}\sin(2\pi(ix+jy))+\mathbf{b}_{ij }\cos(2\pi(ix+jy)), \tag{15}\]

\[\mathbf{u}(x,y,0)=2\mathbf{w}(x,y)+\mathbf{c} \tag{16}\]

where \(a,b,c\sim N(0,1)\). The parameters are

\[L=4,\quad T=1,\quad\nu=0.001. \tag{17}\]

3. **Poisson 2D Classic (Poisson2d-C)**

The Poisson 2D equation is given by

\[-\Delta u=0. \tag{18}\]

The domain is a rectangle minus four circles \(\Omega=\Omega_{rec}\setminus R_{i}\) where \(\Omega_{rec}=[-0.5,0.5]^{2}\) is the rectangle and \(R_{i}\) denotes four circle areas:

\[R_{1} = \{(x,y):(x-0.3)^{2}+(y-0.3)^{2}\leq 0.1^{2}\}, \tag{19}\] \[R_{2} = \{(x,y):(x+0.3)^{2}+(y-0.3)^{2}\leq 0.1^{2}\},\] (20) \[R_{3} = \{(x,y):(x-0.3)^{2}+(y+0.3)^{2}\leq 0.1^{2}\},\] (21) \[R_{4} = \{(x,y):(x+0.3)^{2}+(y+0.3)^{2}\leq 0.1^{2}\}. \tag{22}\]

The boundary condition is

\[u=0,x\in\partial R_{i}, \tag{23}\] \[u=1,x\in\partial\Omega_{rec}. \tag{24}\]

Figure 5: Reference solution of Burgers2d at timesteps \(t=0,0.2,0.4,1.0\) using FEM solver.

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_EMPTY:18]

## 7 2D Heat with Varying Coefficients (Heat2d-VC)

The 2D heat equation with a varying source is given by

\[\frac{\partial u}{\partial t}-\nabla(a(x)\nabla u)=f(x,t). \tag{47}\]

The domain is \(\Omega\times T=[0,1]^{2}\times[0,5]\). The function \(a(x)\) is chosen similarly to Darcy flow but with an exponential GRF. The function \(f(x,t)\) is defined as

\[f(x,t)=A\sin(m_{1}\pi x)\sin(m_{2}\pi y)\sin(m_{3}\pi t). \tag{48}\]

with \(A=200,m_{1}=1,m_{2}=5,m_{3}=1\). The initial and boundary conditions are

\[u(x,y,0) = 0,x\in\Omega \tag{49}\] \[u(x,y,t) = 0,x\in\partial\Omega. \tag{50}\]

## 8 2D Heat Multi-Scale (Heat2d-MS)

The 2D heat multi-scale equation is given by

\[\frac{\partial u}{\partial t}-\frac{1}{(500\pi)^{2}}u_{xx}-\frac{1}{\pi^{2}}u _{yy}=0, \tag{51}\]

with domain \(\Omega\times T=[0,1]^{2}\times[0,5]\).

The initial and boundary conditions are

\[u(x,y,0) = \sin(20\pi x)\sin(\pi y),\quad x\in\Omega, \tag{52}\] \[u(x,y,t) = 0,\quad x\in\partial\Omega. \tag{53}\]

Figure 8: Reference solution of Poisson2d-MS by FEM solver.

Figure 9: Reference solution of Heat2d-VC by FEM solver at timesteps \(t=0,0.5,2.0,3.5\).

## 9 2D Heat Complex Geometry

(Heat Exchanger, Heat2d-CG)

The 2D heat equation for a complex geometry is given by

\[\frac{\partial u}{\partial t}-\Delta u=0. \tag{54}\]

The domain is defined as \(\Omega\times T=([-8,8]\times[-12,12]\setminus\cup_{i}R_{i})\times[0,3]\).

The boundary condition is

\[-n\cdot(-c\nabla u)=g-qu. \tag{55}\]

Here we choose \(c=1\). The positions of large circles are

\[(\pm 4,\pm 3),\quad(\pm 4,\pm 9),\quad(0,0),\quad(0,\pm 6),\quad r=1 \tag{56}\]

with \(g=5\) and \(q=1\). The positions of small circles are

\[(\pm 3.2,\pm 6),\quad(\pm 3.2,0),\quad r=0.4 \tag{57}\]

with \(g=1\) and \(q=1\). For the rectangular boundary conditions, \(g=0.1\) and \(q=1\).

**10. 2D Heat Long Time (Heat2d-LT)**

The governing PDE is

\[\frac{\partial u}{\partial t}=0.001\Delta u+5\sin(ku^{2})\left(1+2\sin\left( \frac{\pi t}{4}\right)\right)\sin(m_{1}\pi x)\sin(m_{2}\pi y) \tag{58}\]

with domain \(\Omega\times T=[0,1]^{2}\times[0,100]\), \(m_{1}=4\), \(m_{2}=2\), and \(k=1\).

The initial and boundary conditions are given by

\[u(x,y,0) = \sin(4\pi x)\sin(3\pi y),x\in\Omega \tag{59}\] \[u(x,y,t) = 0,\quad x\in\partial\Omega. \tag{60}\]

**11. 2D NS lid-driven flow (NS2d-C)**.

Figure 11: reference solution of Heat2d-CG by FEM solver at timesteps \(t=0.5,2.0,2,5,3.0\).

Figure 10: Reference solution of Heat2d-MS by FEM solver.

The PDE is given by

\[\boldsymbol{u}\cdot\nabla\boldsymbol{u}+\nabla p-\frac{1}{Re}\Delta \boldsymbol{u} = 0,x\in\Omega \tag{61}\] \[\nabla\cdot\boldsymbol{u} = 0,x\in\Omega \tag{62}\]

The domain is \(\Omega=[0,1]^{2}\), the top boundary is \(\Gamma_{1}\), the left, right and bottom boundary is \(\Gamma_{2}\).

The boundary conditions are

\[\boldsymbol{u}(\boldsymbol{x}) = (4x(1-x),0),x\in\Gamma_{1} \tag{63}\] \[\boldsymbol{u}(\boldsymbol{x}) = (0,0),x\in\Gamma_{2}\] (64) \[p = 0,x=(0,0). \tag{65}\]

The Reynolds number \(\text{Re}=100\).

**12. 2D Back Step Flow (NS-CG)**

The equations and boundary conditions are given by

\[u\cdot\nabla u+\nabla p-\frac{1}{\text{Re}}\Delta u = 0, \tag{66}\] \[\nabla\cdot u = 0. \tag{67}\]

The domain is defined as \(\Omega=[0,4]\times[0,2]\setminus([0,2]\times[1,2]\bigcup R_{i})\) (excluding the top-left quarter).

Figure 12: reference solution of Heat2d-LT by FEM solver at timesteps \(t=0,20,50,80,100\).

Figure 13: Reference solution of NS2d-Ld by FEM solver.

he inlet velocity is given by \(u_{\text{in}}=4y(1-y)\), the outlet pressure is \(p=0\), and the boundary condition is no-slip: \(\mathbf{u}=0\). The Reynolds number of \(\text{Re}=100\).

### 2D NS Long Time (NS2d-LT)

The PDE of this case is given by

\[\frac{\partial u}{\partial t}+u\cdot\nabla u+\nabla p-\frac{1}{ \text{Re}}\Delta u =f(x,y,t), \tag{68}\] \[\nabla\cdot u =0. \tag{69}\]

The domain is \(\Omega\times T=([0,2]\times[0,1])\times[0,5]\), and the forcing term \(f(x,y,t)\) can be given as

\[f(x,y,t)=(0,-\sin(\pi x)\sin(\pi y)\sin(\pi t)). \tag{70}\]

The boundary conditions are similar to case 12, and the left inlet initial condition can be given as an oscillatory form:

\[u(0,y,t)=\sin(\pi y)(A_{1}\sin(\pi t)+A_{2}\sin(3\pi t)+A_{3}\sin(5\pi t)). \tag{71}\]

where \(A_{1}=1,A_{2}=1,A_{3}=1\).

The initial condition in the domain is

\[u(x,y,0)=0. \tag{72}\]

### Basic 1D Wave Equation (Wave1d-C)

The governing PDE is

\[u_{tt}-4u_{xx}=0 \tag{73}\]

The domain is \(\Omega\times T=[0,1]\times[0,1]\). The boundary conditions are

\[u(0,t)=u(1,t)=0 \tag{74}\]

The initial condition:

\[u(x,0) = \sin(\pi x)+\frac{1}{2}\sin(4\pi x) \tag{75}\] \[u_{t}(x,0) = 0 \tag{76}\]

The analytical solution of this problem is

\[u(x,t)=\sin(\pi x)\cos(2\pi t)+\frac{1}{2}\sin(4\pi x)\cos(8\pi t). \tag{77}\]

### 2D Wave Equation in Heterogeneous Medium (Wave2d-CG)

The governing PDE is given by

\[\left[\nabla^{2}-\frac{1}{c(x)}\frac{\partial^{2}}{\partial t^{2}}\right]u(x, t)=0 \tag{78}\]

Figure 14: Reference fields \(u,v,p\) from top to bottom of NS2d-LT by FEM solver at timesteps \(t=0.5,1.0,2.5,4.0,5.0\).

The Domain is \(\Omega=[-1,1]\times[-1,1]\) and the initial condition is

\[u(x,0) = \exp\left(-\frac{\|x-\mu\|^{2}}{2\sigma^{2}}\right),x\in\Omega \tag{79}\] \[\frac{\partial u}{\partial t}(x,0) = 0,x\in\Omega \tag{80}\]

The boundary conditions are

\[\frac{\partial u}{\partial n}=0,x\in\partial\Omega \tag{81}\]

The parameters are

\[\mu=(-0.5,0),\sigma=0.3, \tag{82}\]

and \(c(x)\) are generated by a Gaussian random field.

## 16 2D Multi-Scale Long Time Wave Equation (Wave2d-MS)

The governing PDE is

\[u_{tt}-(u_{xx}+a^{2}u_{yy})=0 \tag{83}\]

The domain is defined as \(\Omega=[0,1]^{2}\times[0,100]\) and the boundary and initial conditions are

\[u(x,y,t)=c_{1}\sinh(m_{1}\pi x)\sinh(n_{1}\pi y)\cos(p_{1}\pi t),(x,y)\in \partial\Omega. \tag{84}\]

\[\frac{\partial u}{\partial t}(x,y,0)=0 \tag{85}\]

The exact solution to this problem is

\[u(x,y,t)=c_{1}\sinh(m_{1}\pi x)\sinh(n_{1}\pi y)\cos(p_{1}\pi t), \tag{86}\]

where \(a=\sqrt{2},m_{1}=1,n_{1}=1,p_{1}=\sqrt{3}\) and \(c_{1}=1\).

## 17 2D Diffusion-Reaction Gray-Scott Model (GS)

The governing PDE is

\[u_{t} = \varepsilon_{1}\Delta u+b(1-u)-uv^{2} \tag{87}\] \[v_{t} = \varepsilon_{2}\Delta v-dv+uv^{2} \tag{88}\]

The domain is \(\Omega\times T=[-1,1]^{2}\times[0,200]\) and parameters are

\[b=0.04,d=0.1,\varepsilon_{1}=1\times 10^{-5},\varepsilon_{2}=5\times 10^{-6} \tag{89}\]

The initial conditions are

\[u(x,y,0) = 1-\exp(-80((x+0.05)^{2}+(y+0.02)^{2}))\] (90) \[v(x,y,0) = \exp(-80((x-0.05)^{2}+(y-0.

The governing PDE is

\[u_{t}+\alpha uu_{x}+\beta u_{xx}+\gamma u_{xxxx}=0 \tag{92}\]

The domain is \(\Omega\times T=[0,2\pi]\times[0,1]\). (Note: Error may increase rapidly in chaotic problems.)

\[\alpha=\frac{100}{16},\beta=\frac{100}{16^{2}},\gamma=\frac{100}{16^{4}} \tag{93}\]

The initial condition is

\[u(x,0)=\cos(x)(1+\sin(x)) \tag{94}\]

The reference solution of KS equation is shown in Figure B.1.

**19. N-Dimensional Poisson equation (PNd)**

The governing PDE is

\[-\Delta u=\frac{\pi^{2}}{4}\sum_{i=1}^{n}\sin\left(\frac{\pi}{2}x_{i}\right) \tag{95}\]

The domain is defined by \(\Omega=[0,1]^{n}\). The exact solution is

\[u=\sum_{i=1}^{n}\sin\left(\frac{\pi}{2}x_{i}\right) \tag{96}\]

Figure 16: Reference solution of GS equation at timestep \(t=0.0,2.5,5.0,7.5,10.0\).

Figure 17: Reference solution of KS equation.

We choose \(n=5\) in our code.

**20. N-Dimensional Heat Equation (HNd)**

The governing PDE is

\[\frac{\partial u}{\partial t} = k\Delta u+f(x,t),x\in\Omega\times[0,1] \tag{97}\] \[\boldsymbol{n}\cdot\nabla u = g(x,t),x\in\partial\Omega\times[0,1]\] (98) \[u(x,0) = g(x,0),x\in\Omega \tag{99}\]

The geometric domain \(\Omega=\{x:|x|_{2}\leqslant 1\}\) is a unit sphere in \(d\)-dimensional space. We choose dimension \(d=5\).

\[k=\frac{1}{d} \tag{100}\]

The two functions are

\[f(x,t) = -\frac{1}{d}|x|_{2}^{2}\exp\left(\frac{1}{2}|x|_{2}^{2}+t\right) \tag{101}\] \[g(x,t) = \exp\left(\frac{1}{2}|x|_{2}^{2}+t\right) \tag{102}\]

We can see that the exact solution of the equation is \(g(x,t)\).

**21. Poisson inverse problem (PInv)**

The governing PDE is

\[-\nabla(a\nabla u)=f \tag{103}\]

The geometric domain is \(\Omega=[0,1]^{2}\), and

\[u=\sin\pi x\sin\pi y. \tag{104}\]

The source term \(f\) is

\[f=\frac{2\pi^{2}\sin\pi x\sin\pi y}{1+x^{2}+y^{2}+(x-1)^{2}+(y-1)^{2}}+\frac{ 2\pi((2x-1)\cos\pi x\sin\pi y+(2y-1)\sin\pi x\cos\pi y)}{(1+x^{2}+y^{2}+(x-1)^ {2}+(y-1)^{2})^{2}}. \tag{105}\]

To ensure the uniqueness of the solution, we impose a boundary condition of \(a(x,y)\), i.e.,

\[a(x,y)=\frac{1}{1+x^{2}+y^{2}+(x-1)^{2}+(y-1)^{2}},x\in\partial\Omega \tag{106}\]

We sample data of \(u(x,y)\) with 2500 uniformly distributed \(50\times 50\) points and add Gaussian noise \(\mathcal{N}(0,0.1)\) to it. The goal is to reconstruct the diffusion coefficients. We see that the ground truth of \(a(x,y)\) is

\[a(x,y)=\frac{1}{1+x^{2}+y^{2}+(x-1)^{2}+(y-1)^{2}},x\in\Omega. \tag{107}\]

**22. Heat (Diffusion) inverse problem (HInv)**

The governing PDE of this inverse problem is

\[u_{t}-\nabla(a\nabla u)=f \tag{108}\]

The geometric domain is \(\Omega\times T=[-1,1]^{2}\times[0,1]\), and

\[u=e^{-t}\sin\pi x\sin\pi y \tag{109}\]

Similarly, we impose a boundary condition for the diffusion coefficient field:

\[a(x,y)=2,\partial x\in\Omega. \tag{110}\]

Then the source function \(f\) is

\[f=((4\pi^{2}-1)\sin\pi x\sin\pi y+\pi^{2}(2\sin^{2}\pi x\sin^{2}\pi y-\cos^{2 }\pi x\sin^{2}\pi y-\sin^{2}\pi x\cos^{2}\pi y))e^{-t} \tag{111}\]

We sample data of \(u(x,y,t)\) randomly with 2500 points from the temporal domain \(\Omega\times T\) and add Gaussian noise \(\mathcal{N}(0,0.1)\) to it. The goal is to reconstruct the diffusion coefficients. We see that the ground truth is

\[a(x,y)=2+\sin\pi x\sin\pi y,x\in\Omega. \tag{112}\]

### Definitions and design choices for parametric PDEs

We design a set of parametric PDEs and evaluate the average performance of PINN variants on cases with different parameters. We choose Burgers2d-C, Poisson2d-C, Heat2d-MS, NS2d-C, Wave2d-C, and Heat-Nd to design these parametric cases.

**1. 2D Coupled Burgers equation (Burgers2d-C) with different initial values.**

The initial values of this case are shown in Eq 16 where \(\mathbf{a}\) and \(\mathbf{b}\) are sampled from Gaussian Random Field. Here the initial values are used as parameters and we sample 5 different \(\mathbf{a}\) and \(\mathbf{b}\) from GRF and test the performance of PINN variants on all 5 cases. Each parametrized PDE is solved using COMSOL. In PDEBench, the authors similarly tested the average effect of PDEs sampled multiple times from the GRF with the same equation. Since the GRF has not changed, there is not much variation in the magnitude and frequency of the initial flow velocity, but there may be significant differences in their spatial distribution. This can also lead to differences in difficulty when solving with the PINN method. From the 4, we see that the error of the best method increased from 26% to 41%, indicating a significant influence of the flow distribution on the solution.

**2. Poisson 2d Classic (Poisson2d-C)**

This PDE is defined on \(\Omega=[-L,L]^{2}\). We parametrize this case by using different domain scales \(L\) from \(\{1,2,4,8,16\}\). Since this PDE is linear, we could compute the ground truth solution by linearly scaling the original PDE where \(L=0.5\). Some papers [61] pointed out that the effect of PINN is influenced by the size of the domain. This is because scaling the domain directly to \([0,1]^{d}\) may be suboptimal and can lead to an imbalanced ratio of PDE loss to boundary loss. This is because PINNs are sensitive to initialization, so different domain scales might lead to different results. Here the real solution of this linear PDE can be obtained through a linear transformation from a solution of another domain scale \(L\). The condition number does not differ when we change \(L\), making it suitable to study the influence of domain scale on PINN's performance. We observed from the results that some methods (PINN-NTK, MultiAdam, FBPINN) are relatively robust to domain scale.

**3. 2D Heat Multi-Scale (Heat2d-MS)**

We parameterize this case using different initial conditions in Eq 53,

\[u(x,y,0)=\sin(a\pi x)\sin(\pi y). \tag{113}\]

Here we choose \((a,b)\) from \(\{(20,1),(1,20),(10,2),(2,10),(5,4)\}\). The reference solutions for different parameters are solved using COMSOL. Changes in the frequency of the initial condition will lead to changes in the frequency of the solution, which allows us to study the influence of the initial condition frequency on PINN. Comparing the results of several experiments, we found that the loss reweighting strategy of PINN-NTK and the adaptive activation function of LAAF perform well for multi-scale problems overall. However, when the frequency variation range is more significant, both their performances decline, suggesting room for improvement.

**4. 2D NS lid-driven flow (NS2d-C)**

We parametrize NS2d-C by setting different speeds at the top boundary in Eq 65,

\[\mathbf{u}(\mathbf{x})=(ax(1-x),0),x\in\Gamma_{1}, \tag{114}\]

where \(a\) is chosen from \(\{2,4,8,16,32\}\). The reference solutions for different parameters are solved using COMSOL. Different flow rates imply different Reynolds numbers, thus altering the difficulty of solving the equation. As the Reynolds number increases, the condition number of the equation will also increase. Generally, the higher the Reynolds number, the more likely turbulence or some small-scale complex flow states will occur. Testing different Reynolds numbers is a natural idea. Specifically, we chose a velocity \(u=ax(1-x)\), where \(a\) ranges between 2 and 32. Compared to the main experiment with \(a=4\), the Reynolds number increased eightfold when \(a=32\).

**5. 1D Wave Equation**

We parametrize this case with different initial conditions in Eq 76,

\[u(x,0)=\sin(\pi x)+\frac{1}{2}\sin(a\pi x), \tag{115}\]

where \(a\) is chosen from \(\{2,4,6,8,10\}\). The ground truth solution is given by,

\[u(x,t)=\sin(\pi x)\cos(2\pi t)+\frac{1}{2}\sin(\pi ax)\cos(2a\pi t). \tag{116}\]

## 6 N-Dimensional Heat Equation

We parametrize this case by choosing a different number of dimensions \(n\) from \(\{4,5,6,8,10\}\). The solutions are given by Eq 102. Although neural networks are theoretically universal function approximators, the ability to fit the solution of high-dimensional PDEs still needs to be studied. So, we chose heat equations of different dimensions to compare the effects of various PINN methods. We observed that for high-dimensional heat equations, the improved optimizer MultiAdam is very helpful in solving high-dimensional problems.

### Discussion about the reference solutions and Mesh Convergence Study.

Given that our benchmark includes various types of PDEs, we generated the reference data using different types of numerical solvers, including the FEM solvers in COMSOL [39], Chebfun [12], among others. For different PDEs, selecting the appropriate numerical solver allows for higher precision results. The types of solvers used, mesh sizes, parameters, and convergence accuracies are detailed in Table 5. For these problems, highly optimized numerical solvers can achieve solutions with very high accuracy and theoretical guarantees. However, the choice of mesh discretization and solver type is highly dependent on the PDE type and parameters. Although PINNs have limitations in terms of accuracy and efficiency, their flexibility and ability to generalize by incorporating data are advantages over traditional numerical solvers, which is one of the motivations behind the development of PINNs.

We conducted a mesh convergence study for the Poisson3d and NS2d-CG equations using grids with varying spacing. To estimate the error bound, we employed Richardson extrapolation [65], a technique that leverages the solutions from different grid sizes to predict the solution's behavior as the grid is further refined. The principle behind Richardson extrapolation is that the error in the numerical solution decreases predictably with grid refinement. If the error reduces as a power of the grid size \(h\), the extrapolated solution \(u_{\text{extrapolated}}\) can be calculated as:

\[u_{\text{extrapolated}}=\frac{h_{2}^{p}u_{h_{1}}-h_{1}^{p}u_{h_{2}}}{h_{2}^{p} -h_{1}^{p}}, \tag{117}\]

where \(u_{h_{1}}\) and \(u_{h_{2}}\) are the solutions obtained on grids with sizes \(h_{1}\) and \(h_{2}\) respectively, and \(p\) is the theoretical convergence rate. The error bound can then be estimated by comparing the extrapolated solution with the finer grid solution \(u_{h_{2}}\) as follows:

\[\text{Error Bound}=\frac{||u_{\text{extrapolated}}-u_{h_{2}}||}{||u_{\text{ extrapolated}}||}. \tag{118}\]

We obtained the following Figure 18 showing the error bound as a function of grid size. As we refined the grid, the differences between the solutions decreased, and the error bound rapidly decreased with smaller grid sizes. The error for the reference data we used was below 0.1%, indicating that it is highly reliable and can serve as a valid reference for the PINN solutions.

### Relationship with existing PDE benchmarks

\begin{table}
\begin{tabular}{c c|c c c} \hline PDE type & & Software & Solver & \#Mesh \\ \hline Burgers & 1d-C & Comsol & BDF,MUMPS & 1000 \\  & 2d-C & Comsol & BDF,MUMPS & 24912 \\ \hline Poisson & 2d-C & Comsol & MUMPS & 40000 \\  & 2d-CG & Comsol & MUMPS & 22420 \\  & 3d-CG & Comsol & MUMPS & 371024 \\  & 2d-MS & Comsol & MUMPS & 24912 \\ \hline Heat & all & Comsol & BDF,MUMPS & 24912 \\ \hline Naiver-Stokes & 2d-C & Comsol & PARDISO & 10000 \\  & 2d-CG & Comsol & PARDISO & 39294 \\  & 2d-LT & Comsol & BDF,PARDISO & 43250 \\ \hline Wave & 1d-C & & Analytical & \\  & 2d-CG & Comsol & Generalized alpha, MUMPS & 24912 \\  & 2d-MS & Comsol & Generalized alpha, MUMPS & 25140 \\ \hline Chaotic & GS & Chebfun & ETDRK4 & 1000 \\  & KS & Chebfun & ETDRK4 & 1000 \\ \hline High dim & all & & Analytical & \\ \hline Inverse problems & all & & Analytical & \\ \hline \end{tabular}
\end{table}
Table 5: Details of the solver for reference data.

Here we compare the PDEs we used with PDEs in PDEBench [52], PDErena [17], and Wang et. al[56]. We list the number of different PDEs used in the experiments in Table 6. The selection of PDEs for our study was carefully curated to align with the objectives of comparing PINN methods, which differs from the approach taken in PDEBench or PDEArena. While PDEBench and PDEArena are oriented towards time-dependent PDEs, such as the compressible Naiver-Stokes and Diffusion Reaction equations, and provide extensive datasets for neural operator research, our focus was distinct. For [56], we compared more PINN variants and selected a wider range of equations We chose a range of PDEs. specifically for their relevance to PINN research, where datasets are not typically provided, emphasizing the direct application of PINNs to the PDEs themselves. We select a diverse range of PDE types and complexities from existing PINN literature. Among these, we included widely applicable and representative PDEs like the incompressible Naiver-Stokes equation and the Poisson equation (Darcy flow), which are fundamental to a multitude of disciplines. Our choice thus facilitates a more targeted and appropriate comparison of PINN methodologies, underscoring the unique aspects of our research approach.

### Overview of methods

The baselines we selected could be roughly divided into several categories, i.e., loss reweighting/resampling, novel optimizer, novel loss functions, and novel activation/architectures. As shown in Eq 119, the general formulation of PINNs is to optimize a mixture of PDE residual loss, boundary loss, and available data loss,

\[\mathcal{L}(\theta)=\frac{w_{c}}{N_{c}}\sum_{i=1}^{N_{c}}||\mathcal{F}(u_{ \theta}(x_{c}^{i});x_{c}^{i})||^{2}+\frac{w_{b}}{N_{b}}\sum_{i=1}^{N_{b}}|| \mathcal{B}(u_{\theta}(x_{b}^{i});x_{b}^{i})||^{2}+\frac{w_{d}}{N_{d}}\sum_{i =1}^{N_{d}}||u_{\theta}(x_{d}^{i})-u(x_{d}^{i})||^{2}. \tag{119}\]

Under this formulation, we could explain different variants of PINNs.

\begin{table}
\begin{tabular}{c|c c c c} \hline PDE type/Number of PDEs & PINNacle & PDEBench & PDEArena & Wang et. al[56] \\ \hline Burgers & 2 & 1 & 0 & 0 \\ Poisson & 4 & 1 & 0 & 0 \\ Convection-Diffusion & 4 & 3 & 0 & 2 \\ Shallow water & 0 & 1 & 1 & 0 \\ Naiver-Stokes & 3 & 4 & 2 & 4 \\ Wave & 2 & 0 & 0 & 0 \\ Chaotic & 2 & 0 & 0 & 1 \\ High dim & 2 & 0 & 0 & 0 \\ Inverse problems & 2 & 0 & 0 & 0 \\ \hline \end{tabular}
\end{table}
Table 6: A comparison between our work and several existing PDE benchmarks. We list the PDEs used in the experiments for each paper.

Figure 18: Richardson extrapolation and error bound analysis for Poisson3d-CG(left) and NS2d-CG(right).

* Loss reweighting methods dynamically modify the weights \(w_{c},w_{b},w_{d}\) to enable a better convergence rate. Resampling methods allocate new collocation points \(x_{c},x_{b}\) or adjust their sampling probability. These methods alleviate the imbalance between PINN optimization. Results show that they achieve remarkable results on many cases of Poisson, Heat, and Wave equations.
* Novel loss functions. It modifies the form of \(\mathcal{L}(\theta)\) or adds new regularization terms for higher convergence accuracy. Results show that vPINNs are excellent at solving inverse problems.
* Novel optimizer. An example of novel optimizer is Multi-Adam which is more suitable for dealing with multiple conflict loss terms especially when they have a different scale. Results show that it works for several problems with multi-scale problems.
* Novel activations/architectures. It modifies the form of surrogate neural networks \(u_{\theta}\) for better model capacity. We see that these modifications are effective for some problems with complex geometries and nonlinear NS equations.

## Appendix C Model Configuration and Hyperparameters

### Model architecture

Our research employs a specific model structure: a Multilayer Perceptron (MLP) with 5 layers, each of which has a width of 100 neurons.

The model was trained for a total of 20,000 iterations or epochs. This number of training rounds was found to be sufficient for the model to learn the underlying patterns in the data, while also avoiding potential overfitting that might occur with too many epochs.

As for the number of collocation points, for 2-dimensional problems, we used 8192 points. These collocation points provide dense coverage of the problem space while it does not consume too much GPU memory. In addition to these, we utilized 2048 boundary/initial points.

For 3-dimensional problems, the number of collocation points and boundary/initial points were increased to 32768 and 8192, respectively. This increase corresponds to the added complexity of 3-dimensional problems, requiring a more comprehensive representation of the problem space to achieve reliable and accurate results.

### Optimization hyperparameters

In our primary experiment, we use Adam optimizer with momentum \((0.9,0.999)\). We set the learning rate at 1e-3. This learning rate was selected after carefully considering the trade-off between the speed of convergence and the stability of learning, which we discussed previously. We found that this learning rate provides a good balance, enabling robust learning without the issues associated with excessively high or low rates. For vanilla PINNs, the loss weights are set to 1.

In summary, our model structure and parameters were carefully selected to balance the need for accuracy and computational efficiency, providing a fair and effective comparison in our study. Detailed ablation studies about these hyperparameters are reported in Appendix E.

### Other method-specific hyperparameters

Here we present the hyperparameters of the methods we tested.

* **PINN.** There are no special hyperparameters for the baseline PINN. Please refer to the section above for the network structure and optimization hyperparameters.
* **PINN-w.** We assign larger weights to boundary conditions for PINN-w. Specifically, the weight for PDE loss is set at 1, while those for initial and boundary conditions are increased to \(100\). These losses are then aggregated as the target loss.
* **PINN-LRA.** We set \(\alpha=0.1\) for updating loss weights, which is the recommended value in the original paper.

* **PINN-NTK.** No special hyperparameter is needed for this method.
* **RAR.** For residual-based adaptive refinement, we add new points where the residual is greatest into the training set every 2000 epochs.
* **MultiAdam.** Although there is no manual weighting for MultiAdam, the loss grouping criteria can affect its performance. Due to time constraints, we only tuned the grouping criteria for the Wave1d-C case, where losses were divided into Dirichlet boundary losses and non-Dirichlet losses and trained for 10,000 epochs. For all other cases, we simply categorize the losses into PDE and boundary losses.
* **gPINN.** For simplicity, we assign a weight of \(0.01\) to the gradient terms and a weight of \(1\) to all others. However, these weights are delicate and require further fine-tuning.

## Appendix D High-level Structure of Toolbox

In Figure 19, we provide a high-level overview of the usage and modules of the benchmark. We provide several encapsulated classes upon DeepXDE. Specifically, we have a PDE class for building PDE problems conveniently. Then we warp the model class by passing neural network architecture, optimizer, and custom callbacks. After that, the model is compiled by DeepXDE. Finally, we invoke the multi-GPU parallel training and evaluation framework to allocate the training tasks to different GPUs. We support convenient one-button parallel training and testing on all PDE cases using all methods. An example code snippet is shown here.

Figure 19: A high-level illustration of PINNacle code structure.

Detailed Experimental Results

### Detailed results of main experiments.

The detailed results of the main experiments in listed in the subsection. In Table 9, we provide the mean and std of L2RE for all baselines on all PDEs. In Table 8, we provide the mean and std of L1RE for all baselines on all PDEs. In Table 11, Table 12, and Table 13, we provide the low-frequency, medium-frequency, and high-frequency Fourier errors, respectively. In Table 10, we provide the mean and std of MSE for all baselines on all PDEs. In Table 14, we provide the average runtime (seconds) for all baselines trained with 20000 epochs on all PDEs averaged by three runs. In Table E.2, we show the results of all baselines on parametric PDEs.

Here we provide an analysis of these results. Since the results of the main experiments have been described in the main text, we won't go over them again. For different metrics of the same PDE, the best-performing methods often differ. This is because different errors reflect different mismatches between the predicted solution and the true solution.

* From the results, we can see that for most cases, methods that perform well in L2RE error also perform well in L1RE. This shows that L1RE and L2RE are generally similar. Although the absolute values differ, they can mostly be used interchangeably, or one can be chosen for calculation.
* Max error measures the worst-case error, significantly different from the average loss measured by L1RE/L2RE. From the results, we can see that hp-VPINN performs very well on this metric, followed by the adaptive activation function LAAF. PINN-LRA and PINN-NTK are optimal for some equations, but their effects are not as stable.
* Fourier error allows for the convergence of different frequency components, so it's an essential reference indicator. Since functions defined in irregular geometric areas are not suitable for calculating Fourier error, we ignored these equations. Looking at **Table 9, Table 10, and Table 11** comprehensively, for mid-low frequency functions, FBPINN is the best performing in most instances. Loss reweighting methods like PINN-LRA and ordinary PINN are better for low and high-frequency components, respectively. We speculate that reweighting the loss to some extent changes the convergence order of different function components.
* Regarding the runtime metric, hp-VPINN is the fastest in most problems. This might be due to the optimization inherent in hp-VPINN's implementation and its fewer required differentiations than vanilla PINN. All other methods introduced varying degrees of additional computational overhead compared to vanilla PINN, with some methods like gPINN even requiring about twice the computational time. We list all training and inference Flops in Table 15 and Table 18. The flops metric also shows that vanilla PINNs and hp-VPINNs are the most efficient PINN variants.

[MISSING_PAGE_FAIL:33]

[MISSING_PAGE_FAIL:34]

[MISSING_PAGE_FAIL:35]

[MISSING_PAGE_FAIL:36]

[MISSING_PAGE_FAIL:37]

[MISSING_PAGE_EMPTY:38]

[MISSING_PAGE_FAIL:39]

[MISSING_PAGE_FAIL:40]

\begin{tabular}{c c|c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Training Flops} & \multirow{2}{*}{Name} & \multicolumn{3}{c|}{Vanilla} & \multicolumn{3}{c|}{Loss Reweighting/Sampling} & Optimizer & \multicolumn{3}{c|}{Loss functions} & \multicolumn{3}{c}{Architecture} \\ \cline{3-14}  & & & PINN & PINN-w & L-BFGS & LRA & NTK & MultiAdam & gPNN & vPINN & LAAF & GAAF & FBPNN \\ \hline \multirow{2}{*}{Burgers} & 1d-C & **1.87E+11** & 1.87E+11 & 2.32E+12 & 5.12E+11 & 4.29E+11 & 3.39E+11 & 4.11E+11 & 1.81E+11 & 2.22E+11 & 2.29E+11 & 7.34E+11 \\  & 2d-C & 2.72E+12 & 2.72E+12 & 4.17E+13 & 1.23E+13 & 2.61E+12 & 1.82E+12 & 2.79E+12 & **6.32E+11** & 2.53E+12 & 2.73E+12 & – \\ \hline \multirow{4}{*}{Poisson} & 2d-C & 2.55E+11 & 2.55E+11 & 1.34E+12 & 6.04E+11 & 5.32E+11 & 4.15E+11 & 5.03E+11 & **2.21E+11** & 3.85E+11 & 2.81E+11 & 2.46E+12 \\  & 2d-C & 2.27E+11 & 2.37E+11 & 2.08E+13 & 6.71E+11 & 5.82E+11 & 4.4E+11 & 5.29E+11 & **2.38E+11** & 4.05E+11 & 2.91E+11 & 2.79E+12 \\  & 3d-Cq & **9.03E+11** & 9.03E+11 & 5.32E+13 & 7.22E+12 & 2.95E+12 & 1.61E+12 & 3.56E+12 & 1.35E+12 & 1.13E+12 & 1.46E+12 \\  & 2d-MS & **2.75E+11** & 2.75E+11 & 5.02E+11 & 1.43E+13 & 5.66E+11 & 4.47E+11 & 5.31E+11 & 1.21E+12 & 4.44E+11 & 3.06E+11 & 2.83E+12 \\ \hline \multirow{4}{*}{Heat} & 2d-VC & **1.01E+11** & 7.01E+11 & 4.71E+13 & 2.26E+12 & 1.03E+12 & 1.28E+12 & 8.98E+11 & 2.03E+12 & 1.02E+12 & 2.03E+12 & 2.63E+12 \\  & 2d-MS & **1.75E+11** & 7.15E+11 & 2.23E+12 & 3.35E+13 & 1.01E+12 & 1.27E+12 & 8.77E+11 & 1.13E+12 & 1.05E+12 & 3.71E+12 \\  & 2d-Cq & **6.91E+11** & 6.91E+11 & 7.52E+12 & 3.43E+12 & 1.07E+12 & 1.27E+12 & 8.78E+11 & 2.04E+12 & 1.08E+12 & 1.01E+12 & 3.54E+12 \\  & 2d-LT & **7.62E+11** & 7.62E+11 & 3.84E+12 & 2.26E+12 & 1.11E+12 & 1.27E+12 & 8.84E+11 & 1.42E+12 & 1.01E+12 & 1.01E+12 & 2.63E+12 \\ \hline \multirow{2}{*}{NS} & 2d-C & 5.05E+11 & 5.05E+11 & 9.82E+11 & 1.39E+12 & 1.23E+12 & 8.38E+11 & 1.36E+12 & **3.81E+11** & 6.36E+11 & 6.32E+11 & 4.85E+12 \\  & 2d-Cq & 4.85E+11 & 4.85E+11 & 1.80E+12 & 1.24E+12 & 1.23E+12 & 8.38E+11 & 1.36E+12 & **4.55E+11** & 6.26E+11 & 4.66E+12 & 4.81E+12 & 3.67E+12 \\  & 2d-LT & 1.87E+12 & 8.77E+12 & 2.57E+13 & 1.51E+13 & 2.87E+12 & 2.55E+12 & 2.96E+12 & **9.25E+11** & 2.77E+12 & 2.67E+12 & 2.75E+12 \\ \hline Wave & 1d-C & 2.15E+11 & 2.15E+11 & 1.44E+13 & 7.51E+11 & 5.63E+11 & 1.82E+11 & 5.19E+11 & **1.49E+11** & 4.13E+11 & 2.92E+11 & 2.07E+12 \\  & 2d-CG & 7.11E+11 & 7.11E+11 & 1.75E+13 & 3.02E+12 & 1.19E+12 & 1.35E+12 & 8.51E+11 & **4.01E+11** & 1.75E+12 & 1.08E+12 & 2.02E+12 \\  & 2d-MS & 1.47E+12 & 4.71E+12 & 4.71E+12 & 8.77E+12 & 4.53E+12 & 3.36E+12 & 2.76E+12 & 4.14E+12 & **1.44E+12** & 1.74E+12** & 1.51E+12 & 2.46E+12 \\ \hline \multirow{2}{*}{Chaotic} & GS & 1.68E+12 & 1.68E+12 & 1.79E+12 & 5.07E+12 & 2.12E+12 & 2.83E+12 & 1.74E+12 & **4.07E+11** & 2.16E+12 & 2.16E+12 & 3.66E+12 \\  & KS & 9.12E+11 & 9.19E+12 & 1.96E+12 & 2.12E+12 & 2.41E+12 & 1.53E+12 & 2.57E+12 & **4.78E+11** & 1.09E+12 & 1.09E+12 & 5.92E+12 \\ \hline \multirow{2}{*}{High dim} & PNd & **1.19E+12** & 1.19E+12 & 1.40E+12 & 2.88E+12 & 3.18E+12 & 2.02E+12 & 1.28E+12 & 2.38E+12 & 1.56E+12 & – \\  & INd & **1.257E+12** & 1.57E+12 & 4.97E+12 & 2.02E+12 & 4.21E+12 & 2.68E+12 & 1.84E+12 & – & – & 2.07E+12 & 2.06E+12 & – \\ \hline Inverse & Plav & **3.04E+11** & 3.27E+11 & 1.68E+12 & 8.38E+11 & 1.15E+12 & 5.24E+11 & 1.01E+12 & 3.28E+11 & 3.85E+11 & 3.94E+11 & 2.43E+12 \\  & HInv & **7.34E+11** & 7.34E+11 & 1.81E+12 & 2.27E+12 & 1.13E+12 & 1.19E+12 & 1.05E+12 & 1.25E+12 & 9.65E+11 & 9.65E+11 & 2.63E+12 \\ \hline \hline \end{tabular}

*Note: 10 Average Drop only for more experiments, we set \(\beta\) values from three times.

### Ablation Experiments

Influence of learning rates.To understand the impact of learning rates We selected three methods, i.e., vanilla Physics-Informed Neural Networks (PINN), PINN-NTK, and PINN-LRA. We conduct experiments on four PDE problems, i.e., Burgers1d-C, GS, Heat2d-CG, and Poisson2d-C. The comparative analysis involved evaluating the performance of these methods using learning rates of 1e-5, 1e-4, 1e-3, and 1e-2, along with a step learning rate decay strategy implemented every 1000 epochs with a decay factor of 0.75. The results are shown in Table E.2. As stated in the main text, a moderate learning rate like 1e-3, 1e-4, or using a decay strategy is a good choice.

Influence of batch size (Collocation points).To further understand the impact of the number of collocation points on our model's performance, we conducted an ablation study. We used four different numbers of collocation points, specifically 512, 2048, 8192, and 32768. The cases tested in this study were burgers1d, GS, Heat2d-CG, and Poisson2d, which is the same as the ablation study on learning rates. We utilized two variants of Physics-Informed Neural Networks: the vanilla PINN and the PINN-LRA. We found that using more batch size leads to a continual improvement in performance. For some cases, 8192 is a enough large batch size and the performance saturates. The conclusions and plots of this experiment are shown in the main text.

Influence of training epochs.In this ablation study, we examine the impact of varying the number of training epochs on our model's performance. We selected four different values, specifically 5k, 20k, 80k, and 160k epochs. Similar to the previous study, the cases chosen for testing were burgers1d, GS, Heat2d-CG, and Poisson2d. The trend is that training more epochs leads to better performance. However, it is easier to saturate than a larger batch size.

Influence of Adam hyperparameters.Here we examine the impact of varying the momentum hyperparameters in the Adam optimizer. Despite the learning rate, Adam contains two momentum hyperparameters, i.e., \((\beta_{1},\beta_{2})\) for storing the approximate first and second-order momentum. In

\begin{table}
\begin{tabular}{r r|c c c c} \hline \hline \multicolumn{2}{c|}{L2RE} & \multicolumn{2}{c}{Burgers1d} & GS & Heat2d-CG & Poisson2d-C \\ \hline \multirow{4}{*}{PINN} & 1e-5 & 2.35E-2(1.90E-3) & 9.39E-2(3.60E-4) & 1.20E-1(2.40E-3) & 1.08E+(1.08E-1) \\  & 1e-4 & 1.99E-2(4.30E-3) & 1.79E-1(1.20E-1) & 1.35E-1(2.00E-2) & 2.81E-2(2.44E-3) \\  & 1e-3 & 1.93E-2(4.00E-3) & **9.35E-2(2.30E-4)** & **8.51E-2(9.50E-3)** & **2.32E-2(1.52E-3)** \\  & 1e-2 & 3.79E-1(1.40E-1) & 1.91E-1(1.30E-1) & 1.73E-1(7.10E-2) & 3.26E-2(1.51E-3) \\  & decay & **1.69E-2(4.10E-3)** & 1.81E-1(1.20E-1) & 1.59E-1(2.00E-2) & 2.41E-2(9.33E-4) \\ \hline PINN-LRA & 1e-5 & 3.44E-2(1.40E-2) & 1.79E-1(1.20E-1) & 1.18E-1(7.60E-4) & 2.91E-2(3.19E-3) \\  & 1e-4 & 2.12E-2(5.30E-3) & **9.36E-2(4.50E-4)** & 1.37E-1(8.50E-3) & 2.49E-2(3.88E-3) \\  & 1e-3 & 1.49E-2(9.60E-4) & 9.37E-2(3.63E-5) & 1.31E-1(9.60E-3) & **2.26E-2(1.93E-3)** \\  & 1e-2 & 6.23E-1(7.40E-2) & 1.29E-1(5.10E-2) & **8.99E-2(7.00E-3)** & 1.00E+(0.562E-7) \\  & decay & **1.37E-2(5.00E-4)** & 1.81E-1(1.20E-1) & 1.19E-1(1.30E-2) & 2.61E-2(7.64E-4) \\ \hline PINN-NTK & 1e-5 & 1.08E-1(2.70E-2) & 4.09E-1(1.20E-3) & **1.21E-1(2.80E-3)** & 1.86E-3(1.26E-4) \\  & 1e-4 & 4.72E-2(8.70E-3) & **1.96E-1(1.40E-1)** & 1.27E-1(5.30E-3) & 2.30E-3(9.48E-4) \\  & 1e-3 & 2.91E-2(7.40E-3) & 2.99E-1(1.50E-1) & 1.21E-1(9.50E-3) & 5.34E-3(1.22E-4) \\  & 1e-2 & NaN & 1.90E+0(1.63E+0) & NaN & 2.39E-1(2.00E-1) \\  & decay & **1.74E-2(2.30E-3)** & 3.06E-1(1.50E-1) & 1.48E-1(9.60E-3) & **8.24E-4(1.32E-4)** \\ \hline \hline \end{tabular}
\end{table}
Table 16: Results of PINN, PINN-NTK, PINN-LRA under different learning rates or learning rate schedules.

\begin{table}
\begin{tabular}{r r|c c c c} \hline \hline \multicolumn{2}{c|}{L2RE} & \multicolumn{2}{c}{Burgers1d} & GS & Heat2d-CG & Poisson2d-C \\ \hline \multirow{4}{*}{PINN} & 512 & 4.59E-1(8.36E-2) & 2.46E-1(1.09E-1) & 4.31E-1(6.57E-2) & 3.15E-2(4.04E-3) \\  & 2048 & 2.60E-1(2.43E-1) & 9.37E-2(2.60E-4) & 2.02E-1(1.92E-2) & 2.62E-2(2.31E-3) \\  & 8192 & 2.14E-2(1.76E-3) & 9.41E-2(6.05E-4) & 1.35E-1(1.71E-2) & **2.58E-2(6.51E-4)** \\  & 32768 & **1.44E-2(4.91E-4)** & **9.37E-2(3.89E-5)** & **3.73E-2(3.23E-3)** & 2.63E-2(2.32E-3) \\ \hline \multirow{4}{*}{PINN-LRA} & 512 & 2.80E-1(2.02E-1) & 9.39E-2(1.66E-4) & 3.66E-1(3.86E-2) & 3.00E-2(3.16E-3) \\  & 2048 & 1.82E-1(1.85E-1) & 1.33E-1(5.57E-2) & 2.07E-1(4.96E-3) & 2.57E-2(1.78E-3) \\ \cline{1-1}  & 8192 & 1.88E-2(9.45E-4) & **9.36E-2(2.14E-4)** & 1.01E-1(2.18E-2) & 2.82E-2(8.12E-4) \\ \cline{1-1}  & 32768 & **1.49E-2(1.51E-3)** & 1.17E-1(3.25E-2) & **4.44E-2(1.05E-2)** & **2.49E-2(6.32E-4)** \\ \hline \hline \end{tabular}
\end{table}
Table 17: Comparison of PINN and PINN-LRA’s performance under different batch sizes (number of collocation points).

[MISSING_PAGE_EMPTY:43]

Influence of momentum parameters for loss reweighting.Here we choose the momentum update \(\alpha\) from \(\{0.01,0.05,0.2,0.4,0.7\}\). We see that the optimal value of \(\alpha\) is problem-dependent. However, we observe that relatively small \(\alpha\) achieves better performance.

Influence of weight for gPINNs.Here we choose the weight of gPINNs \(w\) from \(\{0.001,0.01,0.1,1\}\). We see that the optimal value of \(w\) is also problem-dependent and the property is intriguing. We observe that the performance of gPINNs is bad on Poisson2d-C for all values of \(w\). We suggest that adding higher-order PDE residuals might harm the training process in some situations.

Influence of number of grids for hp-VPINNs.The number of points to compute integral within a domain \(Q\) and number of grids \(N_{\mathrm{grid}}\) are two critical hyperparameters for hp-VPINN. Here we choose \(Q\) from \(\{5,10,15,20\}\) for 2-dimensional problems and \(\{6,8,10,12\}\) for 3-dimensional problems to investigate their influence. We also take \(N_{\mathrm{grid}}\) into consideration, which varies in \(\{4,8,16,32\}\) for 2-dimensional problems and 3-dimensional problems. Different parameter selection is applied due to the limit of the VRAM. We can observe a consistent trend that as the \(Q\) value rises, the accuracy of the model's predictions also enhanced. This is attributed to the fact that the \(Q\) value dictates the number of integration points; hence, a higher value leads to more precise integration. However, for certain scenarios where hp-VPINN might not be the best fit, a surge in the \(Q\) value doesn't significantly bolster the prediction accuracy. On the other hand, the choice of \(N_{\mathrm{grid}}\) exhibits a complex influence on accuracy. Generally, as the value of \(N_{\mathrm{grid}}\) increases, precision tends to improve. However, in regions where the solution has large gradients or discontinuities, a denser grid might amplify these anomalies, leading to larger errors during model training.

Influence of the number of subdomains and overlap factors for FBPINNs.The number of subdomains for domain decomposition and the overlap ratio \(\alpha\) are two important hyperparameters for FBPINNs. The overlap ratio is chosen from \(\{0.2,0.4,0.6,0.8\}\).

Results on different domain scaleHere we study the influence of domain scales. While numerical methods are usually resistant to domain scales, PINN methods are not invariant to domain scale changes. Moreover, normalizing the domain to \([0,1]\) might be suboptimal for PINNs. Here we take the domain scale \(L\) of Poisson2d-C as an example to study the performance under different

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline weight \(w\) & Burgers1d & GS & Heat2d-CG & Poisson2d-C \\ \hline
0.001 & **6.12E-2(1.36E-2)** & 1.66E-1(1.01E-1) & **4.97E-2(7.10E-4)** & **6.74E-1(1.71E-2)** \\
0.01 & 1.95E-1(2.47E-2) & 1.79E-1(1.21E-1) & 7.78E-2(1.47E-2) & 6.89E-1(2.47E-2) \\
0.1 & 4.93E-1(1.59E-2) & 4.61E-1(1.99E-1) & 1.34E-1(1.37E-3) & 6.92E-1(7.72E-3) \\
1 & 5.53E-1(7.49E-2) & **9.38E-2(1.79E-5)** & 2.19E-1(9.90E-2) & 6.96E-1(4.39E-3) \\ \hline \end{tabular}
\end{table}
Table 22: Performance comparison of gPINN with different weights.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline Q & Burgers1d & Q & GS & Heat2d-CG & Q & Poisson2d-C \\ \hline
5 & 3.19E-01(2.91E-02) & 6 & 3.88E-01(9.73E-02) & 6 & 7.14E-01(7.14E-01) & 5 & 2.46E-01(1.62E-01) \\
10 & 2.88E-01(6.03E-03) & 8 & 4.25E-01(1.51E-01) & 8 & 7.19E-01(4.89E-02) & 10 & 2.43E-01(1.57E-01) \\
15 & 1.85E-01(6.97E-02) & 10 & 3.68E-01(2.04E-01) & 10 & 7.19E-01(4.75E-02) & 15 & 2.45E-01(1.61E-01) \\
20 & 1.85E-01(4.65E-02) & 12 & 3.58E-01(2.06E-01) & 12 & 7.21E-01(4.95E-02) & 20 & 2.46E-01(2.46E-01) \\ \hline \end{tabular}
\end{table}
Table 23: Performance comparison of hp-VPINN with different \(Q\).

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline Q & Burgers1d & Q & GS & Heat2d-CG & Q & Poisson2d-C \\ \hline
5 & 3.19E-01(2.91E-02) & 6 & 3.88E-01(9.73E-02) & 6 & 7.14E-01(7.14E-01) & 5 & 2.46E-01(1.62E-01) \\
10 & 2.88E-01(6.03E-03) & 8 & 4.25E-01(1.51E-01) & 8 & 7.19E-01(4.89E-02) & 10 & 2.43E-01(1.57E-01) \\
15 & 1.85E-01(6.97E-02) & 10 & 3.68E-01(2.04E-01) & 10 & 7.19E-01(4.75E-02) & 15 & 2.45E-01(1.61E-01) \\
20 & 1.85E-01(4.65E-02) & 12 & 3.58E-01(2.06E-01) & 12 & 7.21E-01(4.95E-02) & 20 & 2.46E-01(2.46E-01) \\ \hline \end{tabular}
\end{table}
Table 23: Performance comparison of hp-VPINN with different \(Q\).

settings. We see that Multi-Adam is the most stable under domain scale changes and achieves the best performance when \(L\) is small.

Comparison between MultiAdam and L-BFGS.Here we compare the new MUltiAdam optimizer for PINNs with L-BFGS, which is a frequently used optimizer in PINN variants. The L2Re result is listed in the Table E.2. We see that L-BFGS does not converge in many cases as it is unstable while MultiAdam has a better convergence property. However, L-BFGS achieves better accuracy on some of the problems like high dimensional PDEs.

Temporal error analysisFor time-dependent problems, an important metric is the generalization ability along the time dimension. We selected Heat2d-CG, Heat2d-MS, and Wave1d-C with two different parameters (domain scale is 2 and 8) to observe how the error evolves over time. We found that the error accumulation over time varies depending on the specific PDE problem. For instance, in the case of Heat2d-CG, its final state is a relatively easy steady state, which results in a gradual reduction of error over time. On the other hand, for Heat2d-MS, the solution continuously oscillates, leading to an increasing error as time progresses. In the case of Wave1d-C, due to the periodic nature of the wave equation and the presence of a ground truth solution that is entirely zero, we observed the L2 Relative Error (L2RE) also increases with fluctuations. In summary, error accumulation in time-dependent problems remains challenging for PINNs, necessitating deeper analysis and improved optimization methods in future research.

Runtime analysisThe runtime results for different methods are shown in Table 14. We have analyzed the results in the previous section.

## Appendix F Other visualization results and analysis

Here we list some visualization results of these experiments. We see that Burgers1d, Poisson2d-C, Poisson2d-CG, and NS2d-C could be solved with a relatively low error. Other problems are difficult to learn, even the approximate shape of the solution. Here we only visualize two-dimensional cases,

\begin{table}
\begin{tabular}{c|c c c c} \hline \(\alpha\) & Burgers1d & GS & Heat2d-CG & Poisson2d-C \\ \hline
0.2 & 9.88E-2(1.75E-2) & 8.57E-2(3.14E-3) & 1.05E+01(6.8E-1) & 5.81E-1(1.01E-3) \\
0.4 & 9.01E-2(1.43E-2) & 8.09E-2(7.63E-4) & 7.36E-1(7.23E-2) & 2.85E-1(9.30E-2) \\
0.6 & 1.75E-1(7.97E-2) & 7.95E-2(6.30E-4) & 6.79E-1(1.17E-1) & 5.54E-2(1.23E-3) \\
0.8 & 1.61E-1(1.08E-1) & 8.04E-2(1.03E-3) & 6.96E-1(1.50E-1) & 4.19E-2(4.71E-3) \\ \hline \end{tabular}
\end{table}
Table 26: Performance (L2RE) comparison of FBPINN with different overlap ratios \(\alpha\).

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline \(N_{\mathrm{grid}}\) & Burgers1d & \(N_{\mathrm{grid}}\) & GS & \(N_{\mathrm{grid}}\) & Heat2d-CG & \(N_{\mathrm{grid}}\) & Poisson2d-C \\ \hline
4 & 3.67E-01(1.28E-02) & 3 & 1.93E-01(2.06E-02) & 3 & 6.91E-01(2.44E-02) & 4 & 4.95E-01(8.46E-02) \\
8 & 2.43E-01(2.39E-03) & 4 & 3.68E-01(2.04E-01) & 4 & 7.19E-01(4.75E-02) & 8 & 4.95E-01(8.63E-02) \\
16 & 3.66E-01(3.67E-02) & 5 & 3.59E-01(1.34E-01) & 5 & 7.22E-01(5.14E-02) & 16 & 2.86E-01(1.94E-02) \\
32 & 4.59E-01(1.34E-02) & 6 & 2.81E-01(1.96E-01) & 6 & 7.23E-01(5.19E-02) & 32 & 2.43E-01(1.57E-01) \\ \hline \end{tabular}
\end{table}
Table 24: Performance comparison of hp-VPINN with different number of grids \(N_{\mathrm{grid}}\).

\begin{table}
\begin{tabular}{c c c} \hline  & Burgers1d & \multicolumn{2}{c}{GS} \\ \hline (1,1) & 2.12E-1(1.19E-1) & (1,1,1) & 7.98E-2(3.59E-3) \\ (2,1) & 1.75E-1(7.97E-2) & (1,1,3) & 8.15E-2(1.73E-3) \\ (3,1) & 1.61E-1(9.77E-2) & (1,1,5) & 7.90E-2(1.28E-3) \\ (1,2) & 1.98E-1(7.34E-2) & (2,2,1) & 8.15E-2(3.56E-3) \\ \hline \multicolumn{3}{c}{Heat2d-CG} & \multicolumn{2}{c}{Poisson2d-C} \\ \hline (1,1,1) & 3.30E-1(1.04E-1) & (1,1) & 5.01E-2(2.80E-3) \\ (1,1,3) & 6.80E-1(1.18E-1) & (1,2) & 3.51E-1(1.26E-1) \\ (1,1,5) & 7.48E-1(3.39E-2) & (2,1) & 4.38E-1(5.30E-2) \\ (2,2,1) & 2.89E-1(2.30E-2) & (2,2) & 5.54E-2(1.23E-3) \\ \hline \end{tabular}
\end{table}
Table 25: Performance (L2RE) comparison of FBPINN with different domain decomposition types.

\begin{table}
\begin{tabular}{c|c|c c} \hline \hline \multicolumn{2}{c|}{L2RE} & \multicolumn{2}{c}{MultiAdam} & \multicolumn{2}{c}{L-BFGS} \\ \hline Burgers & 1d-C & 4.85E-(1.61E-2) & **1.33E-2(5.30E-5)** \\
2d-C & **3.33E-1(8.65E-3)** & 4.65E-1(4.69E-3) \\ \hline  & 2d-C & **2.63E-2(6.57E-3)** & NaN \\ Poisson & 2d-CG & **2.76E-1(1.03E-1)** & 2.96E-1(4.77E-1) \\  & 3d-CG & 3.64E+0(2.74E-2) & 3.51E+0(0.33E-2) \\  & 2d-MS & **5.90E-1(4.06E-2)** & 1.45E+0(4.75E-3) \\ \hline Heat & 2d-VC & 4.75E-1(8.44E-2) & **2.32E-1(5.29E-3)** \\  & 2d-MS & 2.18E-1(9.26E-2) & **1.73E-2(4.74E-3)** \\  & 2d-CG & **7.12E-2(1.30E-2)** & 8.57E-1(6.69E-4) \\  & 2d-LT & 1.00E+0(3.85E-5) & 1.00E+0(6.69E-5) \\ \hline NS & 2d-C & 7.27E-1(1.95E-1) & **2.14E-1(1.07E-3)** \\  & 2d-CG & **4.31E-1(6.95E-2)** & NaN \\  & 2d-LT & 1.00E+0(2.19E-4) & 9.70E-1(3.66E-4) \\ \hline Wave & 1d-C & **1.21E-1(1.76E-2)** & NaN \\  & 2d-CG & 1.09E+0(1.24E-1) & 1.33E+0(2.34E-1) \\  & 2d-MS & 9.33E-1(1.26E-2) & NaN \\ \hline Chaotic & GS & **9.37E-2(1.21E-5)** & NaN \\  & KS & 9.61E-1(4.77E-3) & NaN \\ \hline High dim & PNd & 3.98E-3(1.11E-3) & **4.67E-4(7.12E-5)** \\  & HNd & 3.02E-1(4.07E-2) & **1.19E-4(4.01E-6)** \\ \hline \end{tabular}
\end{table}
Table 28: Mean L2RE comparison between MultiAdam and L-BFGS.

\begin{table}
\begin{tabular}{c|c|c c c} \hline \hline Scale \(L\) & Adam & MultiAdam & LRA & GePinn \\ \hline
0.5 & 6.94E-1(1.76E-2) & **5.71E-1(6.11E-2)** & 6.93E-1(1.48E-2) & 7.06E-1(2.94E-3) \\
1 & 6.92E-1(1.79E-2) & **3.56E-2(1.25E-2)** & 3.88E-1(2.61E-1) & 6.89E-1(1.41E-2) \\
2 & 4.41E-10(5.97E-2) & **3.81E-2(9.38E-3)** & 1.68E-1(6.78E-2) & 6.76E-1(3.86E-2) \\
4 & **1.77E-2(4.66E-3)** & 3.38E-2(9.71E-3) & 1.11E-1(1.43E-1) & 3.13E-2(2.85E-3) \\
8 & 2.39E-2(7.26E-3) & 4.40E-2(3.07E-2) & 1.41E-1(1.70E-2) & **1.95E-2(6.42E-3)** \\
16 & 1.83E-2(8.19E-3) & 3.62E-2(1.10E-2) & 9.45E-2(2.05E-2) & **1.59E-2(6.03E-3)** \\ \hline \hline \end{tabular}
\end{table}
Table 27: Performance comparison of vanilla PINNs, Multi-Adam, PINN-LRA, and gPINN on Poisson2d-C different domain scales.

Figure 20: L2RE (mean/std) of different methods on heart2d-CG, Heat2d-MS.

which are easier to display in the paper. Note that we also support different forms of three-dimensional plot functionals in our code.

Figure 21: L2RE varying with time for PINNs on Wave1d-C-scale2 and Wave1d-C-Scale8.

Figure 22: Visualization of Burgers1d. The left pictures are the prediction of PINN methods. The right pictures show the error between the prediction and the ground truth.

Figure 23: Visualization of Poisson2d-C. The left pictures are the prediction of PINN methods. The right pictures show the error between the prediction and the ground truth.

Figure 24: Visualization of Poisson2d-CG. The left pictures are the prediction of PINN methods. The right pictures show the error between the prediction and the ground truth.

Figure 25: Visualization of Poisson2d-MS. The left pictures are the prediction of PINN methods. The right pictures show the error between the prediction and the ground truth.

Figure 26: Visualization of NS2d-C. The left pictures are the prediction of PINN methods. The right pictures show the error between the prediction and the ground truth. Note that PINN-LRA diverged in this case.

Figure 27: Visualization of NS2d-CG. The left pictures are the prediction of PINN methods. The right pictures show the error between the prediction and the ground truth.

Figure 28: Visualization of KS. The left pictures are the prediction of PINN methods. The right pictures show the error between the prediction and the ground truth.