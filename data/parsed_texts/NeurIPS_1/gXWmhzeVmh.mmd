# Rough Transformers: Lightweight and Continuous Time Series Modelling through Signature Patching

Fernando Moreno-Pino\({}^{1,}\)1 Alvaro Arroyo\({}^{1,2,}\)1 Harrison Waldon\({}^{1,*}\)

**Xiaowen Dong\({}^{1,2}\) Alvaro Cartea\({}^{1,3}\)**

\({}^{1}\) Oxford-Man Institute, University of Oxford

\({}^{2}\) Machine Learning Research Group, University of Oxford

\({}^{3}\) Mathematical Institute, University of Oxford

Equal contribution.

Footnote 1: footnotemark:

Footnote 1: footnotemark:

Footnote 1: footnotemark:

Footnote 1: footnotemark:

Footnote 1: footnotemark:

###### Abstract

Time-series data in real-world settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In these settings, traditional sequence-based recurrent models struggle. To overcome this, researchers often replace recurrent architectures with Neural ODE-based models to account for irregularly sampled data and use Transformer-based architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of even moderate length. To address this challenge, we introduce the Rough Transformer, a variation of the Transformer model that operates on continuous-time representations of input sequences and incurs significantly lower computational costs. In particular, we propose _multi-view signature attention_, which uses path signatures to augment vanilla attention and to capture both local and global (multi-scale) dependencies in the input data, while remaining robust to changes in the sequence length and sampling frequency and yielding improved spatial processing. We find that, on a variety of time-series-related tasks, Rough Transformers consistently outperform their vanilla attention counterparts while obtaining the representational benefits of Neural ODE-based models, all at a fraction of the computational time and memory resources.

## 1 Introduction

Real-world sequential data in areas such as healthcare [65], finance [36], and biology [28] often are irregularly sampled, of variable length, and exhibit long-range dependencies. Furthermore, these data, which may be drawn from financial limit order books [8] or EEG readings [85], are often sampled at high frequency, yielding long sequences of data. Hence, many popular machine learning models struggle to model real-world sequential data, due to input dimension inflexibility, memory constraints, and computational bottlenecks. Rather than treating these data as _discrete_ sequences, effective theoretical models often assume data are generated from some underlying _continuous-time_ process [53, 66]. Hence, there is an increased interest in developing machine learning methods that use _continuous-time_ representations to analyze sequential data.

One recent approach to modelling continuous-time data involves the development of continuous-time analogues of standard deep learning models, such as Neural ODEs [12] and Neural CDEs [45], which extend ResNets [37] and RNNs [30], respectively, to continuous-time settings. Instead of processing discrete data directly, these models operate on a latent continuous-time representationof input sequences. This approach is successful in continuous-time modelling tasks where standard deep recurrent models fail. In particular, extensions of vanilla Neural ODEs to the time-series setting [70, 45] succeed in various domains such as adaptive uncertainty quantification [59], counterfactual inference [79], or generative modelling [7].

In many practical settings, such as financial market volatility [20, 54] or heart rate fluctuations [35], continuous-time data also exhibit long-range dependencies. That is, data from the distant past may impact the system's current behavior. Deep recurrent models struggle in this setting due to vanishing gradients, whereas continuous-time analogues of these models have been shown to address this difficulty [46]. Several recent works [52, 58] also successfully extract long-range dependencies from sequential data with Transformers [86], which learn temporal dependencies of a tokenized representation of input sequences. Extracting such temporal dependencies requires a positional encoding of input data, because the attention mechanism is permutation invariant, which projects data into some latent space. The parallelizable nature of the Transformer allows for rapid training and evaluation on sequences of moderate length and it contributes to its success in fields such as natural language processing (NLP).

While the above approaches succeed in certain settings, several limitations hinder their wider applications. On the one hand, Neural ODEs and their analogues [45, 70] bear substantial computational costs when modelling long sequences of high dimension; see [57]. On the other hand, Transformers operate on discrete-time representations of input sequences, whose relative ordering is represented by the positional encoding. This representation may inhibit their expressivity in continuous-time data modelling tasks [91]. Moreover, Transformer-based models suffer from a number of difficulties, including (i) input sequences must be sampled at the same times, (ii) the sequence length must be fixed, and (iii) the computational cost scales quadratically in the length of the input sequence. These difficulties severely limit the application of Transformers to continuous-time data modelling.

**Contributions 1)** We introduce _Rough Transformers_, a variant of the Transformer architecture amenable to the processing of continuous-time signals, which can be easily integrated into existing code-bases. The Rough Transformer is built upon the path signature from Rough Path Theory [51]. We define a novel, multi-scale transformation which projects discrete input data to a continuous-time path and compresses the input data with minimal information loss. Moreover, this transformation is an efficient feature representation of continuous-time paths, because linear functionals of path signatures approximate continuous functions of paths arbitrarily well (see Theorem A.2 in Appendix A).

**2)** We introduce the _multi-view attention mechanism_ to extract both local and global dependencies of very long time-series efficiently. This mechanism operates directly on continuous-time representations of data without the need for expensive numerical solvers or constraints on the smoothness of the data stream. Moreover, the multi-view attention mechanism is provably robust to irregularly sampled data.

**3)** We carry out extensive experimentation on long and irregularly sampled time-series data. In particular, we show that Rough Transformers (i) improve the learning dynamics of the Transformer, making it more sample-efficient and allowing it to achieve better out-of-sample results, (ii) reduce the training cost by a factor of up to \(25\times\) when compared with vanilla Transformers and more when compared with Neural ODE based architectures, (iii) maintain similar performance when data are irregularly sampled, where traditional recurrent-based models suffer a substantial decrease in

Figure 1: A representation of the multi-view signature. The continuous-time path is irregularly sampled at points marked with a red \(x\). The local and global signatures of a linear interpolation of these points are computed and concatenated to form the multi-view signature. The multi-view signature transform consists of \(\overline{L}\) multi-view signatures.

performance [70], and (iv) yield improved spatial processing, accounting for relationships between different temporal channels without having to pre-define a specific inter-channel relation structure.

## 2 Background and Methodology

Problem Formulation.In many real-world scenarios, sequential data are time-series sampled from some underlying continuous-time process, so datasets consist of long, irregularly sampled sequences of varied lengths. In these settings, the problem of sequence modelling is described as follows. Let \(C(\mathbb{R}^{+};\mathbb{R}^{d})=\{g:\mathbb{R}^{+}\rightarrow\mathbb{R}^{d} \mid g\text{ continuous}\}\), and consider \(\widehat{X}\in C(\mathbb{R}^{+};\mathbb{R}^{d})\) which we call a continuous-time _path_. A time-series of length \(L\) with sampling times \(\mathcal{T}_{\mathbf{X}}=\{t_{i}\}_{i=1}^{L}\subset\mathbb{R}^{+}\) is defined as \(\mathbf{X}=((t_{1},X_{1}),...,(t_{L},X_{L}))\), where \(X_{i}=\widehat{X}(t_{i})\in\mathbb{R}^{d}\). Now, define a continuous function on paths \(f:C(\mathbb{R}^{+};\mathbb{R}^{d})\rightarrow\mathbb{R}^{k}\). Next define a dataset \(\mathcal{D}=\left\{(\mathbf{X}^{i},f(\widehat{X}^{i}))_{i=1}^{N}\right\}\). We seek to approximate the function \(f\) from the set \(\mathcal{D}\) for some downstream task. Importantly, we do not assume that \(\mathcal{T}_{\mathbf{X}}=\mathcal{T}_{\mathbf{Y}}\) for all \(\mathbf{X},\mathbf{Y}\in\mathcal{D}\), so that \(\mathcal{D}\) may be irregularly sampled.

Sequence Modelling with Transformers.Transformers are used extensively as a baseline architecture to approximate functions of discrete-time sequential data and are successfully applied to settings when input sequences are fixed in length, relatively short, and sampled at regular intervals. First, the Transformer projects input time series \(\mathbf{X}\in\mathbb{R}^{L\times d}\) to a high-dimensional space \(\mathbf{X}\mapsto T(\mathbf{X})\in\mathbb{R}^{L\times d^{\prime}}\) for \(d^{\prime}>>d\) using some linear positional encoding \(T:\mathbb{R}^{L\times d}\rightarrow\mathbb{R}^{L\times d^{\prime}}\). Next, a latent representation of the encoded sequence is learned by a multi-headed self-attention mechanism which splits \(T(\mathbf{X})\) into \(H\) distinct query, key, and value sequences: \(Q_{h}=T(\mathbf{X})W_{h}^{Q}\), \(K_{h}=T(\mathbf{X})W_{h}^{K}\), \(V_{h}=T(\mathbf{X})W_{h}^{V}\), respectively, with \(h=1,...,H\) and weight matrices \(W_{h}^{Q},W_{h}^{K},W_{h}^{V}\in\mathbb{R}^{d^{\prime}\times d^{\prime}}\). The multi-head self-attention calculation for each head is given by

\[O_{h}=\mathrm{softmax}\left(\frac{Q_{h}K_{h}^{\intercal}}{\sqrt{d_{k}}}\right) V_{h}\,, \tag{1}\]

and the latent representation is projected to the output space \(\mathbb{R}^{k}\) using a multi-layer perceptron (MLP).

The input length \(L\) of the MLP and the Transformer is fixed by assumption. To evaluate the Transformer on a time-series \(\mathbf{X}\) with \(|\mathcal{T}_{\mathbf{X}}|\neq n\), one must perform some transformation (interpolation, extrapolation, etc.) which may degrade the performance of the model. Furthermore, the memory and time complexity of the Transformer is of order \(O(L^{2}d)\), which presents a substantial difficulty in modelling long sequences.

Rough Path Signatures.Broadly, the difficulties faced by the Transformer in modelling time-series stem from time-series being sampled from underlying _continuous-time_ objects, while the attention mechanism underpinning the Transformer is designed to model discrete sequences. To address these difficulties, Rough Transformers augment standard Transformers by lifting the input time-series to the space of continuous-time functions and performing the self-attention calculation in this infinite-dimensional space. To achieve this, we use the path signature from Rough Path Theory.

For a continuous-time path \(\widehat{X}\in C_{b}^{1}(\mathbb{R}^{+};\mathbb{R}^{d})\) and times \(s,t\in\mathbb{R}^{+}\), the path signature of \(\widehat{X}\) from \(s\) to \(t\), denoted \(S(\widehat{X})_{s,t}\), is defined as follows. First, let

\[\mathcal{I}_{d}=\{(i_{1},...,i_{p}):i_{j}\in\{1,...,d\}\,\forall\,j\text{ and }p\in\mathbb{N}\} \tag{2}\]

denote the set of all \(d\)-multi-indices and \(\mathcal{I}_{d}^{n}=\{I\in\mathcal{I}_{d}:|I|=n\}\). Next, set \(S(\widehat{X})_{s,t}^{0}:=1\) and for any \(I\in\mathcal{I}_{d}\), define

\[S(\widehat{X})_{s,t}^{I}=\int_{s<u_{1}<\ldots<u_{p}<t}\hat{\hat{X}}^{{}^{1_{1} }}(u_{1})\cdots\hat{\hat{X}}^{{}^{p}}(u_{p})\,du_{1}\ldots du_{p}\,, \tag{3}\]

where \(\hat{\hat{X}}^{j}=d\widehat{X}^{j}/dt\). Abusing notation, define level \(n\) of the signature as

\[S^{n}(\widehat{X})_{s,t}=\left\{S(\widehat{X})_{s,t}^{I}:I\in\mathcal{I}_{d}^{n }\right\}\,. \tag{4}\]and define the signature as the infinite sequence

\[S(\widehat{X})^{n}_{s,t}=(S(\widehat{X})^{0}_{s,t},S(\widehat{X})^{1}_{s,t},...,S( \widehat{X})^{n}_{s,t},...)\,. \tag{5}\]

Finally, define the truncation of the signature \(S(\widehat{X})^{\leq n}_{s,t}=(S(\widehat{X})^{0}_{s,t},...,S(\widehat{X})^{n} _{s,t})\), where \(S(\widehat{X})^{n}_{s,t}\) can be interpreted as an element of the _extended tensor algebra_ of \(\mathbb{R}^{d}\):

\[T((\mathbb{R}^{d}))=\left\{(a_{0},...,a_{n},...):a_{n}\in\mathbb{R}^{d\otimes n }\right\}\,. \tag{6}\]

Analogously, we say that \(S(\widehat{X})^{\leq n}_{s,t}\in T((\mathbb{R}^{d}))_{\leq n}\). A central property of the signature is that is invariant with respect to time-reparameterization [51]. That is, let \(\gamma:[0,T]\rightarrow[0,T]\) be surjective, continuous, and non-decreasing. Then we have

\[S(\widehat{X})_{0,T}=S(\widehat{X}\circ\gamma)_{0,T}\,, \tag{7}\]

which will be crucial to demonstrate the Rough Transformer's robustness to irregularly sampled data.

In contrast to wavelets or Fourier transforms, which parameterize paths on a functional basis, the signature provides a basis for functions of continuous paths. Hence, the path signature is well-suited to sequence modelling tasks in which one seeks to learn a function of the underlying functional. For a more rigorous presentation of signatures and a description of additional properties, see Appendix A and Lyons et al. [51].

## 3 Rough Transformers

Now, we construct the Rough Transformer, a Transformer-based architecture that operates on continuous-time sequential data by means of the path signature.

Let \(\mathcal{D}\) be a dataset of irregularly sampled time-series. To project a discretized time-series \(\mathbf{X}\in\mathcal{D}\) to a continuous-time object, let \(\tilde{X}\) denote the piecewise-linear interpolation of \(\mathbf{X}\).2 Next, for \(t_{k}\in\mathcal{T}\), define the _multi-view signature_

Footnote 2: Any continuous-time interpolation of \(\mathbf{X}\) can be used, e.g., splines. However, the signature computation of piecewise-linear paths is particularly fast; see Appendix A.

\[M(\mathbf{X})_{k}:=\left(S(\tilde{X})_{0,t_{k}},S(\tilde{X})_{t_{k-1},t_{k}} \right)\,. \tag{8}\]

In what follows, we refer to the components \(\left(S(\tilde{X})_{0,t_{k}},S(\tilde{X})_{t_{k-1},t_{k}}\right)\) as _global_ and _local_, respectively; see Figure 1. Intuitively, one can interpret the global component as an efficient representation of long-term information (see Theorem A.2 in Appendix A), and the local component as a type of convolutional filter that is invariant to the sampling rate of the signal. Now, define the _multi-view signature transform_\(M(\mathbf{X})=\left(M(\mathbf{X})_{1},...,M(\mathbf{X})_{\tilde{L}}\right),\) and denote by \(M(\mathbf{X})^{\leq n}\) the truncated signature for a truncation level \(n\). Next, define the _multi-view attention mechanism_, which uses the multi-view signature transform to extend the standard attention mechanism to the space of continuous functions [51]. First, fix a truncation level \(n\in\mathbb{N}\), and let \(\bar{d}\in\mathbb{N}\) be such that \(M(\mathbf{X})^{\leq n}_{k}\in\mathbb{R}^{\bar{d}}\). For \(h=1,...,H\) let \(W^{\bar{Q},\tilde{K},\tilde{V}}_{h}\in\mathbb{R}^{d\times\mathcal{A}^{\prime}}\) for some \(\bar{d}^{\prime}\in\mathbb{N}\), and let

\[\tilde{Q}_{h}=M(\mathbf{X})^{\leq n}W^{\bar{Q}}_{h},\quad\tilde{K}_{h}=M( \mathbf{X})^{\leq n}W^{\bar{K}}_{h},\quad\tilde{V}_{h}=M(\mathbf{X})^{\leq n} W^{\bar{V}}_{h}\,. \tag{9}\]

Then, the attention calculation is given by

\[O_{h}=\mathrm{softmax}\,\left(\frac{\tilde{Q}_{h}\tilde{K}^{\star}_{h}}{\sqrt {\bar{d}^{\prime}}}\right)\tilde{V}_{h}\,. \tag{10}\]

Notice that the attention calculation is similar to (1), however, we stress that the multi-view attention is built on _continuous-time_ objects, the signatures, while the standard attention mechanism acts on discrete objects. The multi-view signature provides a compressed representation of the time series, minimizing the computational costs associated to quadratic scaling without excessive loss of representational capacity, see Appendix F.

### Advantages of Rough Transformers

**Computational Efficiency.** As demonstrated in Section 4, multi-view attention mechanism can substantially reduce the computational cost of vanilla Transformers. In particular, the attention calculation decreases from \(O(L^{2}\,d)\) in the vanilla case to \(O(\overline{L}^{2}\,d)\), where \(\overline{L}<<L\) with Rough Transformers. This enables both faster wall-clock training time and the ability to process long input sequences which would otherwise yield out-of-memory errors for the vanilla Transformer, see Figure 2. Moreover, the multi-view attention mechanism does not require backpropagation through the signature calculation, which can be computed _offline_. This is significantly more computationally efficient compared with the complexity of computing signatures batch-wise in every training step. Finally, the signature of piecewise-linear paths can be computed explicitly, see Appendix A, and there are a number of Python packages devoted to optimized signature calculation [43, 68].

**Variable Length and Irregular Sampling.** The multi-view signature transform underpinning Rough Transformers is evaluated by constructing a continuous-time interpolation of input data and computing a series of iterated integrals of this interpolation. The bounds of these integrals are a fixed set of time points, meaning that the sequence length of the multi-view attention mechanism is fixed and independent of the sequence length of input samples. Furthermore, the following proposition shows that the output of the Rough Transformer for two (possibly irregular) samplings of the same path is similar.

**Proposition 3.1**.: _Let \(\mathbb{T}\) be a Rough Transformer. Suppose \(\widehat{X}:[0,T]\rightarrow\mathbb{R}^{d}\) is a continuous-time process, and let \(\gamma:[0,T]\rightarrow[0,T]\) denote a time-reparameterization. Suppose \(\mathbf{X}\) and \(\mathbf{X}^{\prime}\) are samplings of \(\widehat{X}\) and \(\widehat{X}\circ\gamma\), respectively. Then \(\mathbb{T}(\mathbf{X})\approx\mathbb{T}(\mathbf{X}^{\prime})\)._

Proof.: By (7), \(S(\widehat{X})_{s,t}=S(\widehat{X}\circ\gamma)_{s,t}\) for all \(s,t\in[0,T]\). Hence, one has \(M(X^{1})\approx M(X^{2})\). Finally, \(\mathbb{T}(X^{1})\approx\mathbb{T}(X^{2})\) because the attention mechanism and final MLP are both continuous. 

Hence, the Rough Transformer is robust to irregular sampling. In many tasks, the sampling times convey important information about the time-series. In these settings, one may augment the input time-series with its sampling times, that is, write \(X=((t_{0},X_{0}),...,(t_{L},X_{L}))\).

**Spatial Processing.** While an interpolation of input data could be sampled to make vanilla Transformers independent of the length of the input sequence, important locality information could be lost, see Appendix F.2. Instead, Rough Transformers summarize spatial interactions between channels by means of the multi-view signature transform. One may notice that in (5), the dimension of the signature grows exponentially in the level of the signature \(n\). In particular, when \(X_{i}\in\mathbb{R}^{d}\), \(|S(\tilde{X})_{0,t}^{\otimes n}|=\frac{d(d^{n}-1)}{d-1}=O(d^{n})\), so the multi-view attention calculation is of order \(O(\bar{L}^{2}d^{n})\). In many practical time-series modelling problems, however, the value of \(d\) is not very large. The signature terms also decay factorially in the signature level \(n\) (see Proposition A.3 in Appendix A), so in practice, one may take the value of \(n\) to be small without sacrificing performance. The majority of computational savings result from the reduction of the sequence length to \(\bar{L}\), and in practice, we take \(\bar{L}<<L\).

When the dimension \(d\) is large, there are three possible remedies to maintain computational efficiency. First, instead of computing the signature in \(M(X)_{k}=(S(X)_{0,t_{k}},S(\tilde{X}_{t_{k-1},t_{k}})\), one may compute

Figure 2: Seconds per epoch for growing input length and for different model types on the sinusoidal dataset. **Left:** Log Scale. **Middle:** Regular Scale. **Right:** Log-log scale. When a line stops, it indicates an OOM error.

the _log-signature_, which is a compressed version of the signature [67]. When the dimension is large enough such that the log-signature is computationally infeasible, one may instead compute the _univariate_ signatures of features coupled with the time channel. That is, consider \(\widehat{X}\in C([0,T];\mathbb{R}^{d})\), with \(\widehat{X}(t)=(\widehat{X}_{1}(t),...,\widehat{X}_{d}(t))\). Denote the time-added function \(\overline{X}_{i}(t):=(t,\widehat{X}_{i}(t))\). Then we define the _univariate multi-view signature_

\[\widehat{M}(\widehat{X})_{k}=\left(M(\overline{X}_{1})_{k},...,M(\overline{X} _{d})_{k}\right)\,. \tag{11}\]

The attention mechanism in this case is constructed as before. Fixing the maximum signature depth to be some value \(n^{*}\), one sees that the number of features in the univariate multi-view signature is approximately \(2^{n^{*}}d\). In practice we find that \(n^{*}\leq 5\) provides sufficient performance, so the order of the attention calculation is \(O(C\,\bar{L}^{2}\,d)\) for \(C\leq 2^{n^{*}}\). Finally, one may use randomized signatures to reduce dimension by using a Johnson-Lindenstrauss-type projection to a low-dimensional latent space and computing the signature in this space, as in [21; 19].

## 4 Experiments

In this section, we present empirical results for the effectiveness of the Rough Transformer, hereafter denoted RFormer, on a variety of time-series-related tasks. Experimental and hyperparameter details regarding the implementation of the method are in Appendices C and D. We consider long multivariate time-series as our main experimental setting because we expect signatures to perform best in this scenario. Additional experimentation on long-range reasoning tasks on image-based datasets is left for future work, as these would likely require additional inductive biases.

To benchmark RFormer, we consider both discrete-time and continuous-time models. In particular, we include as main baselines traditional RNN models (GRU [15]), ODE-based methods designed for sequential data (Neural-CDE [45]), as well as ODE-based methods explicitly designed for long time-series (Neural-RDE [57]).3 Furthermore, we compare against a vanilla Transformer [86] which is the RFormer backbone. Finally, we present comparisons with a recent continuous-time Transformer model, ContiFormer [13], to highlight the computational efficiency gap between RFormer and similar continuous-time models. We note that the first two tasks focus on evaluating the performance improvement of RFormer over the Transformer baseline. For other long-range tasks, we include comparisons to recent state-space models [31; 62; 81]. In the irregular sampling regime, we benchmark against state-of-the-art models tailored to that setting [61; 78]. See Appendix B for additional discussion on related models and more details about our experimental choices.

Footnote 3: We only benchmark Neural-CDE models in settings where time series are of relatively short length, due to the computational demands of this model for longer sequences.

### Time Series Processing

Frequency Classification.Our first experiment is based on a set of synthetically generated time series from continuous paths of the form

\[\widehat{X}(t)=g(t)\sin(\omega\,t+\nu)+\eta(t)\,, \tag{12}\]

where \(g(t)\) is a non-linear trend component, \(\nu\) and \(\eta\) are two noise terms, and \(\omega\) is the frequency. Here, the task of the model is to classify the time-series according to its frequency \(\omega\). We consider \(1000\) samples in \(100\) classes with \(\omega\) evenly distributed from \(10\) to \(500\). Each time-series is regularly sampled with \(2000\) times-steps on the interval \([0,1]\).

This synthetic experiment is similar to others in recent work on time-series modelling [49; 89; 55]. We include an additional experiment in which we alter the signal in (12) so its frequency is \(\omega_{0}\) for \(t<t_{0}\) and \(\omega_{1}\) afterward, where the task is to classify the sinusoid based on the first frequency. We call this dataset the "long sinusoidal" dataset. This extension of the original experiment aims to test the ability of the model to perform long-range reasoning effectively. Note that for this task, we also add ODE-RNN[70] to the previously mentioned baselines.

Figure 3: Test accuracy per epoch for the frequency classification task across three random seeds. **Left:** Sinusoidal dataset. **Right:** Long Sinusoidal dataset.

[MISSING_PAGE_FAIL:7]

### Training Efficiency

Here, we focus on the computational gains of the model when compared with vanilla Transformers and methods that require numerical ODE solvers.

Attention-based architectures are highly parallelizable on modern GPUs, as opposed to traditional RNN models which require sequential updating. However, vanilla attention experiences a bottleneck in memory and time complexity as the sequence length \(L\) grows. As covered above in Section 3, variations of the signature transform allow the model to operate on a reduced sequence length \(\bar{L}\) without increasing the dimensionality in a way that would become problematic for the model. This allows us to bypass the quadratic complexity of the model without resorting to sparsity techniques commonly used in the literature [26, 49].

Tables 1-3 show that RFormer is competitive when modelling datasets with extremely long sequences without an explosion in the memory requirements. RFormer exploits the parallelism of the attention mechanism to significantly accelerate training time, as the length of the input sequence is decreased substantially. In particular, we observe speedups of \(\mathbf{1.4}\times\) to \(\mathbf{26.11}\times\) with respect to standard attention, and higher when compared with all methods requiring numerical solutions to ODEs. The computational efficiency gains of RFormer are attained due to the signature transform reducing the length of the time-series with minimal information loss. The effectiveness of this transformation can be seen from the ablation study carried out in Appendix F. This contrasts with NRDEs [57], which augment NCDEs with local signatures of input data, and find that smaller windows often perform better. Furthermore, NRDEs do not experience the same computational gains as RFormer because they must perform many costly ODE integration steps.

In Figure 2, we showcase the improvements in computational efficiency of RFormer compared to vanilla Transformers [86], continuous-time Transformers [13], and other continuous-time RNNs [45, 57] when processing sequences from \(L=100\) samples up to \(L=10\)K. As seen, RFormer is significantly more efficient than its continuous-time and vanilla counterparts, even when performing the signature computation online, which involves computing the signatures for each batch during training, resulting in significant redundant computation. When signatures are precomputed just once before training, the computational time of each epoch remains _constant_ across input all sequence lengths including \(L=10\)K (see the exact signature computation times for different datasets in Table 4). We also stress the fact that RFormer also scales gracefully for extremely long sequences (up to \(L=250\)K) with both online and offline computation of the signatures, as shown in Appendix G. Finally, we highlight that ContiFormer has a sample complexity of \(\mathcal{O}(L^{2}d^{2}S)\), where \(S\) represents the normalized number of function evaluations of the numerical ODE solver, which makes ContiFormer orders of magnitude more computationally intensive when compared to RFormer and prevents the model from running on sequences longer than 500 points without running out of memory (see device details in Appendix C).

### Irregular Time Series Classification

So far, we mainly focused on the efficiency and inductive bias afforded to the model through the use of signatures. However, a key element of RFormer is that it can naturally deal with irregularly sampled sequences without expensive numerical ODE solvers. This property follows from the fact that signatures are _invariant to time reparameterization_, see Proposition 3.1. In this subsection, we empirically test this property by training the model on the same datasets but randomly dropping a percentage of the data points. This test intends to find if the model is able to learn continuous-time representations of the original input time-series. The results can be found in Table 5. We find that

\begin{table}
\begin{tabular}{l c c c} \hline \hline Dataset & **Train** & **Val** & **Test** \\ \hline Eigenworms & 1.11 s. & 0.19 s. & 0.19 s. \\ HR & 4.23 s. & 0.84 s. & 0.85 s. \\ Sine (1k) & 0.39 s. & 0.39 s. & 0.39 s. \\ Sine (5k) & 0.51 s. & 0.51 s. & 0.51 s. \\ Sine (20k) & 1.64 s. & 1.64 s. & 1.64 s. \\ Sine (100k) & 5.74 s. & 5.74 s. & 5.74 s. \\ \hline \hline \end{tabular}
\end{table}
Table 4: Dataset processing times for training, validation, and testing phases.

\begin{table}
\begin{tabular}{l c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{3}{c}{Sec. / Epoch} \\ \cline{2-4}  & **Sine** & **EW** & **HR** \\ \hline GRU & **0.12** & 0.25 & 1.07 \\ ODE-RNN & 5.39 & 48.59 & 50.71 \\ Neural-CDE & 9.83 & - & - \\ Neural-RDE & 0.85 & 5.23 & 9.52 \\ Transformer & 0.77 & OOM & 11.71 \\
**RFormer** & 0.55 & **0.11** & **0.45** \\
**Speedup** & \(1.4\times\) & - & \(26.11\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Seconds per epoch for all models considered.

RFcorner consistently results in the best performance, with a small performance drop when compared to the full dataset. Importantly, this property is achieved in conjunction with the efficiency gains afforded to the model and without the use of expensive numerical ODE solvers.5 Finally, we perform an additional set of experiments on the 15 univariate classification datasets from the UEA time series classification archive and compare our model with recent state-of-the-art models for irregular time series [61, 78]. Across the board, we find that our model is both faster and more accurate than the continuous-time benchmark _despite having a discrete-time Transformer backbone_, as shown in Figure 4, which introduces Continuous Recurrent Units (CRU) [78] as an extra baseline. For more details and more exhaustive experimentation on random data drops, see Appendix G.

Footnote 5: We use our own reproduction to test the performance of all models in irregularly sampled datasets. Random dropping requires window sizes larger than 2 because signatures cannot be computed over a single point. The best step size was chosen in accordance with performance on the validation dataset, see Appendix D.

## 5 Reasons for improved model performance

In this final section, we provide explanations for the superior inductive bias of the RFcorner model compared to its vanilla Transformer counterpart, despite its lower computational cost.

### Spatial Processing

First, we highlight that a key reason the model achieves significant compression benefits in the tasks considered is its ability to _jointly_ account for temporal and spatial interactions through the self-attention mechanism and signature terms, respectively. In particular, we believe that for certain datasets, the relationships between different channels of the time series may hold more importance than the temporal information itself, which can often be redundant. This is exemplified in the Eigenworms dataset, which experiences a 20% performance drop when employing univariate signatures, but is able to achieve state-of-the-art performance with a 600\(\times\) compression rate in the temporal dimension when signatures are applied across all channels, as shown in Figure 6. To this end, we draw parallels between the use of signatures and the field of temporal graph processing, where the use of the signature over all channels can be seen as a fully connected graph, capturing information from all channels, and the univariate signature would correspond to a graph with only self-connections between the nodes, as depicted in Figure 5. In our view, this hints towards the idea of using sparse graph learning techniques [17, 23] to reduce the explosion in signature terms while retaining the ability to perform effective spatial processing.

To empirically test these claims, we design a synthetic experiment using a 2-channel time series. Each channel contains a signal of the form \(\sin(\omega_{i}t+\nu_{i}),i=1,2\), where \(\omega_{i}\) and \(\nu_{i}\) are randomly sampled from the interval \([0,2\pi]\). For half of the dataset, the last 1% of temporal samples in the second channel are set to match the frequency of the first channel. The task is to classify whether the samples in this final interval are of the same frequency. As shown in Figure 5, RFcorner demonstrates greater sample efficiency and achieves higher test accuracy compared to its vanilla Transformer counterpart, highlighting the effectiveness of signatures in spatial processing.

Figure 4: Average performance of all models on the 15 univariate datasets from the UEA Time Series archive under different degrees of data drop.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{4}{c}{**50\% Drop Performance**} \\ \cline{2-5}  & EW (\%) \(\uparrow\) & HR \(\downarrow\) & Sine (\%) \(\uparrow\) & Sine Long (\%) \(\uparrow\) \\ \hline GRU & 35.90 & 13.06 & 0.96 & 1.16 \\ ODE-RNN & 37.61 & 13.06 & 1.06 & 1.23 \\ Neural-RDE & 60.68 & 4.67 & 0.94 & 0.87 \\ Transformer & OOM & 12.73 & 7.37 & 20.23 \\
**RFcorner** & **87.69** & **2.96** & **59.57** & **93.17** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Performance of all models under a random 50% drop in datapoints per epoch.

### Sequence Coarsening as an Inductive Bias for Transformers

In addition to the benefits of higher-order signature terms, we empirically observe that even using level-one signature terms resulted in performance improvements when compared to processing sequences without any transformation. We believe that the reduction in input signal length, achieved without significant information loss through the signature transform is another important factor in the improved inductive bias of RFormer. This finding aligns with the concurrent work of [4], which highlights some of the drawbacks of decoder-only Transformers for long sequences in terms of both _oversquashing_ and _representational collapse_.

To measure the degree of coarsening in the sequence, we find that interpreting the temporal sequence as a path graph and using ideas from the oversmoothing literature [75] serves as a good way to measure the similarity of the representations being fed to the Transformer. In particular, we compute the Dirichlet Energy [74], defined in this case as \(E(\mathbf{X})=\frac{1}{N}\sum_{i=1}^{N}||\mathbf{X}_{i}-\mathbf{X}_{i-1}||_{2}\) of the temporal sequence resulting from taking increasing window sizes of the global signature. An example of this is shown in Figure 6 for the Eigenworms dataset, where we compared different numbers of windows (from 2 to 18k). Interestingly, we found that the "elbow" of the Dirichlet energy corresponded to 30 windows in this dataset, which we found empirically to be one of the most performant settings. This hints at the idea of the Dirichlet energy being used for signature hyperparameter tuning as well.

## 6 Conclusion

We introduced the Rough Transformer, a variant of the original Transformer that allows the processing of discrete-time series as continuous-time signals through the use of multi-view signature attention. Empirical comparisons showed that Rough Transformers outperform vanilla Transformers and continuous-time models on a variety of time-series tasks and are robust to the sampling rate of the signal. Finally, we showed that RFormer provides significant speedups in training time compared to regular attention and ODE-based methods, without the need for major architectural modifications or sparsity constraints.

## Impact Statement

This work is unlikely to result in any harmful societal repercussions. Its primary potential lies in its ability to enhance and advance existing data modelling and machine learning methods.

## Acknowledgements

We thank Christopher Salvi, Antonio Orvieto, Yannick Limmer, and Benjamin Walker for discussions at different stages of the project. AA acknowledges support from the Rafael Del Pino Foundation and a G-Research travel grant. XD acknowledges support from the Oxford-Man Institute of Quantitative Finance.

Figure 5: **Left: Graph connectivity structures for multivariate, univariate and sparse signature. Middle: Example samples for synthetic task. Right: Performance on spatial synthetic experiment.**

Figure 6: **Left: Dirichlet energy as a function of window size for the Eigenworms dataset. Right: Original and hidden representation after signature layer for two examples in the EW dataset.**

## References

* Arjovsky et al. [2016] M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution recurrent neural networks. In _International conference on machine learning_, pages 1120-1128. PMLR, 2016.
* Arribas [2018] I. P. Arribas. Derivatives pricing using signature payoffs. _arXiv preprint arXiv:1809.09466_, 2018.
* Bagnall et al. [2018] A. Bagnall, H. A. Dau, J. Lines, M. Flynn, J. Large, A. Bostrom, P. Southam, and E. Keogh. The uea multivariate time series classification archive, 2018. _arXiv preprint arXiv:1811.00075_, 2018.
* Barbero et al. [2024] F. Barbero, A. Banino, S. Kapturowski, D. Kumaran, J. G. Araujo, A. Vitvitskyi, R. Pascanu, and P. Velickovic. Transformers need glasses! information over-squashing in language tasks. _arXiv preprint arXiv:2406.04267_, 2024.
* Beltagy et al. [2020] I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_, 2020.
* Bilos et al. [2021] M. Bilos, J. Sommer, S. S. Rangapuram, T. Januschowski, and S. Gunnemann. Neural flows: Efficient alternative to neural odes. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 21325-21337. Curran Associates, Inc., 2021. URL [https://proceedings.neurips.cc/paper_files/paper/2021/file/b21f9f98829dea9a48ffd8aaddc1f159d-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/b21f9f98829dea9a48ffd8aaddc1f159d-Paper.pdf).
* Calvo-Ordonez et al. [2023] S. Calvo-Ordonez, J. Huang, L. Zhang, G. Yang, C.-B. Schonlieb, and A. I. Aviles-Rivero. Beyond u: Making diffusion models faster & lighter. _arXiv preprint arXiv:2310.20092_, 2023.
* Cartea et al. [2015] A. Cartea, S. Jaimungal, and J. Penalva. _Algorithmic and high-frequency trading_. Cambridge University Press, 2015.
* Cartea et al. [2023] A. Cartea, G. Duran-Martin, and L. Sanchez-Betancourt. Detecting toxic flow. _arXiv preprint arXiv:2312.05827_, 2023.
* Chang et al. [2018] B. Chang, M. Chen, E. Haber, and E. H. Chi. Antisymmetricrnn: A dynamical system view on recurrent neural networks. In _International Conference on Learning Representations_, 2018.
* Chang et al. [2023] P. Chang, G. Duran-Martin, A. Y. Shestopaloff, M. Jones, and K. Murphy. Low-rank extended kalman filtering for online learning of neural networks from streaming data. _arXiv preprint arXiv:2305.19535_, 2023.
* Chen et al. [2018] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. _Advances in neural information processing systems_, 31, 2018.
* Chen et al. [2023] Y. Chen, K. Ren, Y. Wang, Y. Fang, W. Sun, and D. Li. Contiformer: Continuous-time transformer for irregular time series modeling. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Child et al. [2019] R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers. _arXiv preprint arXiv:1904.10509_, 2019.
* Cho et al. [2014] K. Cho, B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. _arXiv preprint arXiv:1406.1078_, 2014.
* Choromanski et al. [2020] K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. In _International Conference on Learning Representations_, 2020.
* Cini et al. [2024] A. Cini, I. Marisca, D. Zambon, and C. Alippi. Taming local effects in graph-based spatiotemporal forecasting. _Advances in Neural Information Processing Systems_, 36, 2024.
* Cirone et al. [2024] N. M. Cirone, A. Orvieto, B. Walker, C. Salvi, and T. Lyons. Theoretical foundations of deep selective state-space models. _arXiv preprint arXiv:2402.19047_, 2024.

* [19] E. M. Compagnoni, A. Scampicchio, L. Biggio, A. Orvieto, T. Hofmann, and J. Teichmann. On the effectiveness of randomized signatures as reservoir for learning rough dynamics. In _2023 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2023.
* [20] F. Corsi. A simple approximate long-memory model of realized volatility. _Journal of Financial Econometrics_, 7(2):174-196, 2009.
* [21] C. Cuchiero, L. Gonon, L. Grigoryeva, J.-P. Ortega, and J. Teichmann. Discrete-time signatures and randomness in reservoir computing. _IEEE Transactions on Neural Networks and Learning Systems_, 33(11):6321-6330, 2021.
* [22] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. _Advances in Neural Information Processing Systems_, 35:16344-16359, 2022.
* [23] H. S. de Ocariz Borde, A. Arroyo, and I. Posner. Projections of model spaces for latent graph inference. In _ICLR 2023 Workshop on Physics for Machine Learning_, 2023.
* [24] E. Dupont, A. Doucet, and Y. W. Teh. Augmented neural odes. _Advances in neural information processing systems_, 32, 2019.
* [25] N. B. Erichson, O. Azencot, A. Queiruga, L. Hodgkinson, and M. W. Mahoney. Lipschitz recurrent neural networks. In _International Conference on Learning Representations_, 2020.
* [26] A. Feng, I. Li, Y. Jiang, and R. Ying. Diffuser: efficient transformers with multi-hop attention diffusion for long sequences. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 12772-12780, 2023.
* [27] A. Fermanian. Embedding and learning with signatures. _Computational Statistics & Data Analysis_, 157:107148, 2021.
* [28] C. Fleming, D. Sheldon, W. Fagan, P. Leimgruber, T. Mueller, D. Nandintsetseg, M. Noonan, K. Olson, E. Setyawan, A. Sianipar, et al. Correcting for missing and irregular data in home-range estimation. _Ecological Applications_, 28(4):1003-1010, 2018.
* [29] E. Fons, A. Sztrajman, Y. El-Laham, A. Iosifidis, and S. Vyetrenko. Hypertime: Implicit neural representation for time series. _arXiv preprint arXiv:2208.05836_, 2022.
* [30] K.-i. Funahashi and Y. Nakamura. Approximation of dynamical systems by continuous time recurrent neural networks. _Neural networks_, 6(6):801-806, 1993.
* [31] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv preprint arXiv:2312.00752_, 2023.
* [32] A. Gu, K. Goel, and C. Re. Efficiently modeling long sequences with structured state spaces. In _International Conference on Learning Representations_, 2021.
* [33] B. Hambly and T. Lyons. Uniqueness for the signature of a path of bounded variation and the reduced path group. _Annals of Mathematics_, pages 109-167, 2010.
* [34] R. Hasani, M. Lechner, A. Amini, D. Rus, and R. Grosu. Liquid time-constant networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 7657-7666, 2021.
* [35] J. M. Hausdorff and C.-K. Peng. Multiscaled randomness: A possible source of 1/f noise in biology. _Physical review E_, 54(2):2154, 1996.
* [36] N. Hautsch. _Modelling irregularly spaced financial data: theory and practice of dynamic duration models_. Springer Science & Business Media, 2004.
* [37] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.

* Henaff et al. [2016] M. Henaff, A. Szlam, and Y. LeCun. Recurrent orthogonal networks and long-memory tasks. In _International Conference on Machine Learning_, pages 2034-2042. PMLR, 2016.
* Hoglund et al. [2023] M. Hoglund, E. Ferrucci, C. Hernandez, A. M. Gonzalez, C. Salvi, L. Sanchez-Betancourt, and Y. Zhang. A neural rde approach for continuous-time non-markovian stochastic control problems. In _ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems_, 2023.
* Holt et al. [2022] S. I. Holt, Z. Qian, and M. van der Schaar. Neural laplace: Learning diverse classes of differential equations in the laplace domain. In _International Conference on Machine Learning_, pages 8811-8832. PMLR, 2022.
* Katharopoulos et al. [2020] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In _International conference on machine learning_, pages 5156-5165. PMLR, 2020.
* Keller et al. [2023] T. A. Keller, L. Muller, T. Sejnowski, and M. Welling. Traveling waves encode the recent past and enhance sequence learning. In _The Twelfth International Conference on Learning Representations_, 2023.
* Kidger and Lyons [2020] P. Kidger and T. Lyons. Signatory: differentiable computations of the signature and logsignature transforms, on both cpu and gpu. _arXiv preprint arXiv:2001.00706_, 2020.
* Kidger et al. [2019] P. Kidger, P. Bonnier, I. Perez Arribas, C. Salvi, and T. Lyons. Deep signature transforms. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper_files/paper/2019/file/d2cdf047a6674cef251d56544a3cf029-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/d2cdf047a6674cef251d56544a3cf029-Paper.pdf).
* Kidger et al. [2020] P. Kidger, J. Morrill, J. Foster, and T. Lyons. Neural controlled differential equations for irregular time series. _Advances in Neural Information Processing Systems_, 33:6696-6707, 2020.
* Lechner and Hasani [2020] M. Lechner and R. Hasani. Learning long-term dependencies in irregularly-sampled time series. _arXiv preprint arXiv:2006.04418_, 2020.
* Lemercier et al. [2021] M. Lemercier, C. Salvi, T. Cass, E. V. Bonilla, T. Damoulas, and T. J. Lyons. Siggpde: Scaling sparse gaussian processes on sequential data. In _International Conference on Machine Learning_, pages 6233-6242. PMLR, 2021.
* Lezcano-Casado and Martinez-Rubio [2019] M. Lezcano-Casado and D. Martinez-Rubio. Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group. In _International Conference on Machine Learning_, pages 3794-3803. PMLR, 2019.
* Li et al. [2019] S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, and X. Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. _Advances in neural information processing systems_, 32, 2019.
* Li et al. [2020] Z. Li, N. B. Kovachki, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, A. Anandkumar, et al. Fourier neural operator for parametric partial differential equations. In _International Conference on Learning Representations_, 2020.
* Lyons et al. [2007] T. J. Lyons, M. Caruana, and T. Levy. _Differential equations driven by rough paths_. Springer, 2007.
* Melnychuk et al. [2022] V. Melnychuk, D. Frauen, and S. Feuerriegel. Causal transformer for estimating counterfactual outcomes. In _International Conference on Machine Learning_, pages 15293-15329. PMLR, 2022.
* Morariu-Patrichi and Pakkanen [2022] M. Morariu-Patrichi and M. S. Pakkanen. State-dependent hawkes processes and their application to limit order book modelling. _Quantitative Finance_, 22(3):563-583, 2022.
* Moreno-Pino and Zohren [2022] F. Moreno-Pino and S. Zohren. Deepvol: Volatility forecasting from high-frequency data with dilated causal convolutions. _arXiv preprint arXiv:2210.04797_, 2022.

* [55] F. Moreno-Pino, P. M. Olmos, and A. Artes-Rodriguez. Deep autoregressive models with spectral attention. _Pattern Recognition_, 133:109014, 2023.
* [56] F. Moreno-Pino, A. Arroyo, H. Waldon, X. Dong, and A. Cartea. Rough transformers for continuous and efficient time-series modelling. _arXiv preprint arXiv:2403.10288_, 2024.
* [57] J. Morrill, C. Salvi, P. Kidger, and J. Foster. Neural rough differential equations for long time series. In _International Conference on Machine Learning_, pages 7829-7838. PMLR, 2021.
* [58] T. Nguyen and A. Grover. Transformer neural processes: Uncertainty-aware meta learning via sequence modeling. In _International Conference on Machine Learning_, pages 16569-16594. PMLR, 2022.
* [59] A. Norcliffe, C. Bodnar, B. Day, J. Moss, and P. Lio. Neural ode processes. In _International Conference on Learning Representations_, 2020.
* [60] A. Norcliffe, C. Bodnar, B. Day, N. Simidjievski, and P. Lio. On second order behaviour in augmented neural odes. _Advances in neural information processing systems_, 33:5911-5921, 2020.
* [61] Y. Oh, D. Lim, and S. Kim. Stable neural stochastic differential equations in analyzing irregular time series data. In _The Twelfth International Conference on Learning Representations_.
* [62] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De. Resurrecting recurrent neural networks for long sequences. In _International Conference on Machine Learning_, pages 26670-26698. PMLR, 2023.
* [63] Y. Park, J. Choi, C. Yoon, M. Kang, et al. Learning pde solution operator for continuous modeling of time-series. _arXiv preprint arXiv:2302.00854_, 2023.
* [64] I. Perez Arribas, G. M. Goodwin, J. R. Geddes, T. Lyons, and K. E. Saunders. A signature-based machine learning model for distinguishing bipolar disorder and borderline personality disorder. _Translational psychiatry_, 8(1):274, 2018.
* [65] S. Perveen, M. Shahbaz, T. Saba, K. Keshayjee, A. Rehman, and A. Guergachi. Handling irregularly sampled longitudinal data and prognostic modeling of diabetes using machine learning technique. _IEEE Access_, 8:21875-21885, 2020.
* [66] R. Ratcliff, P. L. Smith, S. D. Brown, and G. McKoon. Diffusion decision model: Current issues and history. _Trends in cognitive sciences_, 20(4):260-281, 2016.
* [67] J. Reizenstein. Calculation of iterated-integral signatures and log signatures. _arXiv preprint arXiv:1712.02757_, 2017.
* [68] J. Reizenstein and B. Graham. The isignature library: efficient calculation of iterated-integral signatures and log signatures. _arXiv preprint arXiv:1802.08252_, 2018.
* [69] D. W. Romero, A. Kuzina, E. J. Bekkers, J. M. Tomczak, and M. Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data. In _International Conference on Learning Representations_.
* [70] Y. Rubanova, R. T. Chen, and D. K. Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. _Advances in neural information processing systems_, 32, 2019.
* [71] T. K. Rusch and S. Mishra. Coupled oscillatory recurrent neural network (corm): An accurate and (gradient) stable architecture for learning long time dependencies. In _International Conference on Learning Representations_, 2020.
* [72] T. K. Rusch and S. Mishra. Unicormn: A recurrent model for learning very long time dependencies. In _International Conference on Machine Learning_, pages 9168-9178. PMLR, 2021.
* [73] T. K. Rusch, S. Mishra, N. B. Erichson, and M. W. Mahoney. Long expressive memory for sequence modeling. In _International Conference on Learning Representations_, 2021.

* [74] T. K. Rusch, B. Chamberlain, J. Rowbottom, S. Mishra, and M. Bronstein. Graph-coupled oscillator networks. In _International Conference on Machine Learning_, pages 18888-18909. PMLR, 2022.
* [75] T. K. Rusch, M. M. Bronstein, and S. Mishra. A survey on oversmoothing in graph neural networks. _arXiv preprint arXiv:2303.10993_, 2023.
* [76] H. Saez de Ocariz Borde, A. Arroyo, I. Morales, I. Posner, and X. Dong. Neural latent geometry search: product manifold inference via gromov-hausdorff-informed bayesian optimization. _Advances in Neural Information Processing Systems_, 36, 2024.
* [77] C. Salvi, M. Lemercier, C. Liu, B. Horvath, T. Damoulas, and T. Lyons. Higher order kernel mean embeddings to capture filtrations of stochastic processes. _Advances in Neural Information Processing Systems_, 34:16635-16647, 2021.
* [78] M. Schirmer, M. Eltayeb, S. Lessmann, and M. Rudolph. Modeling irregular time series with continuous recurrent units. In _International conference on machine learning_, pages 19388-19405. PMLR, 2022.
* [79] N. Seedat, F. Imrie, A. Bellot, Z. Qian, and M. van der Schaar. Continuous-time modeling of counterfactual outcomes using neural controlled differential equations. In _International Conference on Machine Learning_, pages 19497-19521. PMLR, 2022.
* [80] V. Sitzmann, J. Martel, A. Bergman, D. Lindell, and G. Wetzstein. Implicit neural representations with periodic activation functions. _Advances in neural information processing systems_, 33:7462-7473, 2020.
* [81] J. T. Smith, A. Warrington, and S. Linderman. Simplified state space layers for sequence modeling. In _The Eleventh International Conference on Learning Representations_.
* [82] C. Tallec and Y. Ollivier. Can recurrent neural networks warp time? _arXiv preprint arXiv:1804.11188_, 2018.
* [83] C. W. Tan, C. Bergmeir, F. Petitjean, and G. I. Webb. Monash university, uea, ucr time series extrinsic regression archive. _arXiv preprint arXiv:2006.10996_, 2020.
* [84] A. Tong, T. Nguyen-Tang, D. Lee, T. M. Tran, and J. Choi. Sigformer: Signature transformers for deep hedging. In _Proceedings of the Fourth ACM International Conference on AI in Finance_, pages 124-132, 2023.
* [85] A. Vahid, M. Muckschel, S. Stober, A.-K. Stock, and C. Beste. Applying deep learning to single-trial eeg data provides evidence for complementary theories on action control. _Communications biology_, 3(1):112, 2020.
* [86] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [87] B. Walker, A. D. McLeod, T. Qin, Y. Cheng, H. Li, and T. Lyons. Log neural controlled differential equations: The lie brackets make a difference. _arXiv preprint arXiv:2402.18512_, 2024.
* [88] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.
* [89] J. Yoon, D. Jarrett, and M. Van der Schaar. Time-series generative adversarial networks. _Advances in neural information processing systems_, 32, 2019.
* [90] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. _Advances in neural information processing systems_, 33:17283-17297, 2020.
* [91] A. Zeng, M. Chen, L. Zhang, and Q. Xu. Are transformers effective for time series forecasting? In _Proceedings of the AAAI conference on artificial intelligence_, volume 37, pages 11121-11128, 2023.

Properties of Path Signatures

First, we recall that the path is uniquely determined by its signature, which motivates its use as a feature map.

**Proposition A.1**.: _Given a path \(\widehat{X}:[0,T]\to\mathbb{R}^{d}\), then the map \(P:[0,T]\to\mathbb{R}^{1+d}\) where \(P(t)=(t,\widehat{X}(t))\) is uniquely determined by it's signature \(S(P)_{0,T}\)._

The proof can be found in Hambly and Lyons [33].

For Rough Transformers, several features of path signatures are important. First, linear functionals on path signatures possess universal approximation properties for continuous functionals.

**Theorem A.2**.: _Fix \(T>0\), and let \(K\subset C^{1}_{b}([0,T];\mathbb{R}^{d})\). Let \(f:K\to\mathbb{R}\) be continuous with respect to the sup-norm topology on \(C^{1}_{b}([0,T];\mathbb{R}^{d})\). Then for any \(\epsilon>0\), there exists a linear functional \(\ell\) such that_

\[|f(\overline{X})-\langle\ell,S(\overline{X})_{0,T}\rangle|\leq\epsilon\,, \tag{13}\]

_for any \(\widehat{X}\in K\), where \(\overline{X}\) denotes the time-added augmentation of \(\widehat{X}\)._

For a proof of A.2, see Arribas [2]. Even though Theorem A.2 guarantees that _linear_ functionals are sufficient for universal approximation, linear models are not always sufficient in practice. This motivates the development of nonlinear models built upon the path signature which efficiently extract path behavior.

The second feature is that the terms of the path signature decay factorially, as described by the following proposition.

**Proposition A.3**.: _Given \(\widehat{X}\in C^{1}_{b}([0,T];\mathbb{R}^{d})\), for any \(s,t\in[0,T]\), we have that for any \(I\in\mathcal{I}^{n}_{d}\)_

\[|S(\widehat{X})^{I}_{0,T}|=O\left(1/n!\right)\,. \tag{14}\]

For a proof of Proposition A.3, see [51]. Hence, the number of terms in the signature grows exponentially in the level of the signature, but the tail of the signature is well-behaved, so only a few levels in a truncated signature are necessary to adequately approximate continuous functionals.

### Signatures of Piecewise Linear Paths.

In the Rough Transformer, we use linear interpolation of input time-series to get a continuous-time representation of the data. As mentioned in Section 3, the signature computation in this case is particularly simple.

Suppose \(\widehat{X}_{k}:[t_{k},t_{k+1}]\to\mathbb{R}^{d}\) is a linear interpolation between two points \(X_{k},X_{k+1}\in\mathbb{R}^{d}\). That is,

\[\widehat{X}_{k}(t)=X_{k}+\frac{t-t_{k}}{t_{k+1}-t_{k}}\left(X_{k+1}-X_{k} \right)\,. \tag{15}\]

Then the signature of \(\widehat{X}_{k}\) is given explicitly by

\[S(\widehat{X}_{k})_{t_{k},t_{k+1}}=\left(1,X_{k+1}-X_{k},\frac{1}{2}(X_{k+1}-X _{k})^{\otimes 2},\frac{1}{3!}(X_{k+1}-X_{k})^{\otimes 3},...,\frac{1}{n!}(X_{k+ 1}-X_{k})^{\otimes n},...\right)\,, \tag{16}\]

where \(\otimes\) denotes the tensor product. Let \(\widehat{X}_{k}*\widehat{X}_{k+1}\) denote the _concatenation_ of \(\widehat{X}_{k}\) and \(\widehat{X}_{k+1}\). That is, \(\widehat{X}_{k}*\widehat{X}_{k+1}:[t_{k},t_{k+2}]\to\mathbb{R}^{d}\) is given by

\[\widehat{X}_{k}*\widehat{X}_{k+1}(t)=\begin{cases}\widehat{X}_{k}(t)&t\in[t_{ k},t_{k+1}]\\ \widehat{X}_{k+1}(t)&t\in(t_{2},t_{k+2}]\,.\end{cases} \tag{17}\]

The signature of the concatenation \(\widehat{X}_{k}*\widehat{X}_{k+1}\) is given by _Chen's relation_, whose proof is in [51]. To state this result, we first note that \(S(\widehat{X})^{n}_{s,t}\) can be interpreted as an element of the _extended tensor algebra_ of \(\mathbb{R}^{d}\):

\[T((\mathbb{R}^{d}))=\left\{(a_{0},...,a_{n},...):a_{n}\in\mathbb{R}^{d\otimes n }\right\}\,. \tag{18}\]

**Proposition A.4** (Chen's Relation).: _The following identity holds:_

\[S(\widehat{X}_{k}*\widehat{X}_{k+1})_{t_{k},t_{k+2}}=S(\widehat{X}_{k})_{t_{k},t_{ k+1}}\otimes S(\widehat{X}_{k+1})_{t_{k+1},t_{k+2}}\,, \tag{19}\]

_where for elements \(A,B\in T((\mathbb{R}^{d}))\) with \(A=(A_{0},A_{1},A_{2},...)\) and \(B=(B_{0},B_{1},B_{2},...)\) the tensor product \(\otimes\) is defined_

\[A\otimes B=\left(\sum_{j=0}^{k}A_{j}\otimes B_{k-j}\right)_{k\geq 0}\,. \tag{20}\]

Let \(\mathbf{X}=(X_{0},...,X_{L})\) be a time-series. Then the linear interpolation \(\tilde{X}:[0,T]\rightarrow\mathbb{R}^{d}\) can be represented as the concatenation of a finite number of linear paths:

\[\tilde{X}=\widehat{X}_{0}*\cdots*\widehat{X}_{L-1}\,. \tag{21}\]

Hence, the signature is

\[S(\tilde{X})_{0,T}=S(\widehat{X}_{0})_{0,t_{1}}\otimes\cdots\otimes S(\widehat {X}_{L-1})_{t_{L-1},T}\,. \tag{22}\]

## Appendix B Related Work, Experimental Choices, and Impact Statement

Continuous-time models.Since their introduction in [12], Neural ODEs were extended in various ways to facilitate modelling continuous time-series data [70, 60, 34, 40, 79]. While Neural ODEs and their extensions are successful in certain tasks they are burdened with a high computational cost, which makes them scale very poorly to long sequences in the time-series setting. Various authors propose methods and augmentations to vanilla Neural ODEs to decrease their computational overhead [24, 6]. Other approaches to augmenting deep learning methods to modelling continuous data include implicit neural representations [80, 29], continuous kernel convolutions [69], or Fourier neural operators [50, 63].

Transformers.First proposed in [86], the Transformer has been exceptionally successful in discrete sequence modelling tasks such as natural language processing (NLP). Key to the success of the Transformer in NLP is the attention mechanism, which extracts long-range dependencies. There are a number of extensions to improve efficiency and decrease the computation cost of the attention mechanism [49, 88, 22, 41, 16].

Signatures in machine learning.The path signature originates from theoretical stochastic analysis [51] and has since become a popular tool in machine learning. Path signatures are regarded as effective feature transformations for sequential data [64, 27, 44]. Additionally, signatures help mitigate the computational cost of Neural CDEs in long time-series [57] and non-Markovian stochastic control problems [39]. Other more recent works in this direction include [18, 87]. Approaches such as randomized signatures [21, 19] and the signature kernel [47, 77] have been developed to mitigate the curse of dimensionality inherent in path signature computations. Rough Transformers provide a first step towards incorporating path signatures for continuous-time sequence modelling using Transformers. 6

Footnote 6: For a preliminary version of this paper, we also direct the reader to [56].

We also note that contemporary work [84] employs a Transformer architecture with signature features for the task of deep hedging. However, our work differs in several key aspects. First, we introduce the multi-view attention mechanism, which uses signatures to extract both global and local information, which we found to be necessary in our experimentation, as Transformers are known to struggle in extracting local information (see Figure 7), whereas their work just uses a global signature. Moreover, their work computes the signature at every time step, strictly dilating input data. This is particularly problematic for long, multi-variate sequences, for reasons discussed above, and can actually negatively impact performance. Our work, however, _compresses_ data using the multi-view signature transform, and we find that this compressed representation can actually improve performance. Finally, their work relies on the assumption that data is regularly sampled, as the signature is computed at every time step, in contrast to our work which is robust to irregular sampling.

Long-Range Sequence modelling.A highly relevant line of research related to enhancing recurrent neural networks' capability to capture long-term dependencies involves the development of various models. These include Unitary RNNs [1], Orthogonal RNNs [38], expRNNs [48], chronoLSTM [82], antisymmetric RNNs [10], Lipschitz RNNs [25], coRNNs [71], unicorRNN [72], LEMs [73], waveRNN [42], Linear Recurrent Units [62], and Structured State Space Models [32, 31]. While we utilize many benchmarks and synthetic tasks from these works to test our model, it is important to note that our work is not intended to compete with the state-of-the-art in these tasks. Therefore, we do not directly compare our model with the models mentioned above. Instead, this paper seeks to show that the baseline Transformer architecture can benefit from the use of signatures by (i) becoming more computationally efficient, (ii) being invariant to the sampling rate of the signal, and (iii) having a good inductive bias for temporal and spatial processing. Furthermore, we highlight that RFormer brings alternative benefits, such as the ability to perform spatial processing effectively, which is a setting in which long-range sequence models typically struggle.

Efficient Attention Variants.There are several efficient self-attention variants that have emerged over the years, including Sparse Transformer [14], Longformer [5], Linear Transformers [41], BigBird [90], Performer [16], or Diffuser [26]. In our setting, we highlight that a central part of this paper is to showcase how signatures significantly reduce the computational requirements of vanilla attention and empirically demonstrate that this also results in improved learning dynamics and invariance to the sampling frequency of the signal. Given the large efficiency gains that we observed with this approach when employed on vanilla attention, we did not consider that further experimentation on other forms of "approximate" attention was needed. Since most variants of attention seek to make the operation more efficient through several approximations (e.g., linearization or sparsification techniques), we believe that a first attempt at showcasing the power of multi-view signatures on vanilla attention is already significant. However, other variants of attention (such as the ones outlined before) could be added on top of the signature representations to obtain even better efficiency gains.

Limitations and Future Work.While we found RFormer to be very performant in our experiments, much of this performance gain relies on heavy hyperparameter tuning, especially when it comes to the choice of window sizes and signature level. However, this could be handled using Neural Architecture Search (NAS) techniques, such as those employed in [76]. Furthermore, despite the computational gains we achieve for low-dimensional sequences, additional work would be required to scale this method to larger dimensions. We should also note that the experiments and results presented in this paper are constrained by the relatively small scale of the models studied.

## Appendix C Experimental Details

All experiments are conducted on an NVIDIA GeForce RTX 3090 GPU with 24,564 MiB of memory, utilizing CUDA version 12.3. Hyperparameters used to produce the results in Table 2 are reported in Tables 6. The timings presented in all tables are obtained by executing each model independently for each dataset and averaging the resulting times across 100 epochs.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & SCP1 & SCP2 & MI & EW & ETC & HB \\ \hline Batch Size & 20 & 10 & 50 & 5 & 10 & 20 \\ Embedded Dim & 10 & 5 & 20 & 20 & 20 & 5 \\ Multi-View Terms & [G] & [G] & [L] & [L] & [G] & [G, L] \\ Learning Rate & 4.08e-3 & 1.38e-3 & 4.08e-3 & 6.73e-3 & 1.00e-3 & 7.72e-3 \\ Num. Heads & 3 & 3 & 3 & 1 & 1 & 3 \\ Num. Layers & 2 & 3 & 3 & 2 & 1 & 3 \\ Num. Sig Windows & 100 & 50 & 200 & 10 & 400 & 30 \\ Sig Level & 2 & 3 & 2 & 2 & 1 & 2 \\ Univariate & true & true & true & false & false & true \\ Num. Epoch & 110 & 10 & 26 & 39 & 200 & 16 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Hyperparameters used for Table 2, where G and L refer to the Global and Local signature components, respectively.

To prevent excessive growth in signature terms, we use the univariate signature in LOB datasets. As an alternative, one could employ randomized signatures [19] or low-rank approximations [9; 11].

## Appendix D Baselines Validation

This section collects the validation of Step and Depth for the Neural-RDE model. Optimal values are selected for evaluation on test-set. Early-stopping is used with the same criteria as [57].

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Dataset** & **Learning Rate** & **Number of Windows** & **Sig. Depth** & **Sig. Type** & **Univariate/Multivariate Sig.** \\ \hline Sinusoidal & \(1\times 10^{-3}\) & 75 & 6 & Multi-View & - \\ HR & \(1\times 10^{-3}\) & 75 & 4 & Local & Multivariate \\ \hline \hline \end{tabular}
\end{table}
Table 7: Hyperparameters validation on remaining datasets.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Acc. Val** & **Step** & **Depth** & **Memory Usage (Mb)** & **Elapsed Time (s)** \\ \hline
17.26 & 2 & 2 & 778.9 & 6912.7 \\
12.21 & 2 & 3 & 770.3 & 1194.43 \\
16.35 & 4 & 2 & 382.2 & 2702.48 \\
19.27 & 4 & 3 & 386.16 & 574.97 \\
20.99 & 8 & 2 & 193 & 1321.36 \\
**24.02** & **8** & **3** & **194.17** & **332.17** \\
17.15 & 16 & 2 & 97.13 & 136.43 \\
21.59 & 16 & 3 & 98.17 & 156.93 \\
17.46 & 24 & 2 & 65.96 & 105.94 \\
20.59 & 24 & 3 & 66.68 & 98.97 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Validation accuracy on the sinusoidal dataset.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Acc. Val** & **Step** & **Depth** & **Memory Usage (Mb)** & **Elapsed Time (s)** \\ \hline
11.10 & 2 & 2 & 4017.22 & 2961.98 \\
9.59 & 2 & 3 & 4008.33 & 2779.52 \\
10.39 & 4 & 2 & 2001.76 & 1677.78 \\
10.19 & 4 & 3 & 2006.80 & 1615.64 \\
14.03 & 8 & 2 & 1004.07 & 665.55 \\
**15.34** & **8** & **3** & **1005.72** & **723.41** \\
1.61 & 16 & 2 & 503.66 & 125.85 \\
1.92 & 16 & 3 & 505.28 & 120.63 \\
1.51 & 24 & 2 & 339.80 & 58.87 \\
2.12 & 24 & 3 & 341.90 & 69.35 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Validation accuracy on the long sinusoidal dataset.

## Appendix E Long Temporal Datasets Details

Table 13 summarises the long temporal modeling datasets from the UEA time series classification archive [3] used in Section 4.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Acc. Val** & **Step** & **Depth** & **Memory Usage (Mb)** & **Elapsed Time (s)** \\ \hline
84.62 & 2 & 2 & 5799.40 & 21289.99 \\
**87.18** & **2** & **3** & **6484.93** & **25925.80** \\
79.49 & 4 & 2 & 2891.61 & 11449.14 \\
82.05 & 4 & 3 & 3240.99 & 9055.12 \\
82.05 & 8 & 2 & 1446.94 & 4143.26 \\
76.92 & 8 & 3 & 1624.73 & 3616.43 \\
82.05 & 16 & 2 & 724.35 & 1909.69 \\
76.92 & 16 & 3 & 817.04 & 1924.27 \\
79.49 & 24 & 2 & 483.92 & 1098.21 \\
74.36 & 24 & 3 & 543.78 & 987.02 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Validation accuracy on the EW dataset.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Acc. Val** & **Step** & **Depth** & **Memory Usage (Mb)** & **Elapsed Time (s)** \\ \hline
**2.44** & **2** & **2** & **5044.44** & **56492.33** \\
3.03 & 2 & 3 & 5059.28 & 39855.19 \\
3.67 & 4 & 2 & 2515.40 & 10765.58 \\
16.04 & 4 & 3 & 2531.44 & 7157.20 \\
5.35 & 8 & 2 & 1259.30 & 3723.94 \\
2.70 & 8 & 3 & 1268.60 & 18682.82 \\
3.58 & 16 & 2 & 632.08 & 3518.96 \\
3.64 & 16 & 3 & 636.64 & 7922.96 \\
3.86 & 24 & 2 & 422.74 & 3710.95 \\
3.55 & 24 & 3 & 426.83 & 6567.02 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Validation loss on the HR dataset.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Val Loss** & **Step** & **Depth** & **Memory Usage (Mb)** & **Elapsed Time (s)** \\ \hline
**0.58** & **2** & **2** & **1253.55** & **180.79** \\
1.74 & 2 & 3 & 1447.57 & 308.52 \\
1.58 & 4 & 2 & 623.87 & 71.18 \\
32.90 & 4 & 3 & 754.05 & 87.81 \\
2.94 & 8 & 2 & 317.40 & 61.27 \\
4.84 & 8 & 3 & 406.88 & 62.71 \\
2.24 & 16 & 2 & 164.70 & 18.67 \\
6.26 & 16 & 3 & 234.92 & 24.20 \\
3.82 & 24 & 2 & 112.80 & 12.69 \\
15.35 & 24 & 3 & 176.68 & 14.92 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Validation loss on the LOB dataset (1K), included as an additional experiment in Appendix??.

## Appendix F Ablation Studies

### Global and Local Signature Components

In this section, we ablate the use of the multi-view signature transform over both global and local transformations of the input signal. The results for the sinusoidal datasets are shown in Figure 7. In most cases, the use of both local and global components improves the performance of RFormer. This choice, however, can be seen as a hyperparameter and will be dataset-dependent.

### Signature Level and Naive Downsampling

One of the main points of the paper is that the shorter representation of the time-series endowed by the signatures helps to significantly reduce the computational cost of the self-attention operation with minimal information loss (and with improved performance in many of the experiments). By equation (16), one sees that the first level of the signature of a linear function is the difference between its endpoints. Hence, using multi-view attention with signature level one operates on the increments of piecewise-linear interpolated data, which corresponds to naive downsampling. To test that higher levels of the signature provide improvements in performance, we compare the result of using the signature on the datasets tested in Table 14 below.

There is a significant performance gain in considering higher levels of the signature because one can capture the higher-order interactions between the different time-series.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Dataset** & **\#Sequences** & **Length** & **\#Classes** & **\#Dimensions** \\ \hline SelfRegulationSCP1 (SCP1) & 561 & 896 & 2 & 6 \\ SelfRegulationSCP2 (SCP2) & 380 & 1152 & 2 & 7 \\ MotorImagery (MI) & 378 & 3000 & 2 & 64 \\ EigenWorms (EW) & 259 & 17984 & 5 & 6 \\ EthanolConcentration (ETC) & 524 & 1751 & 4 & 3 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Summary of datasets used in the long time-series classification task.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Dataset & Linear-Interpolation + Vanilla & Rough Transformer with sig level (n) & Improvement \\ \hline EigenWorms & 64.10\% & 90.24\% (2) & 40.77\% \\ HR & 10.56 & 2.66 (4) & 74.81\% \\ \hline \hline \end{tabular}
\end{table}
Table 14: Comparative performance of different methods on datasets.

Figure 7: Ablation of local and local components of the multi-view signature for the sinusoidal datasets. **Left:** Sinusoidal dataset. **Right:** Long Sinusoidal dataset.

[MISSING_PAGE_FAIL:22]

Finally, Table 19 compares CRU and RFormer in an irregularly sampled synthetic data setting, featuring shorter sinusoids and fewer classes than the experiments in Section 4.1. Additionally, Table 20 presents the hyperparameter validation for CRU (see Table 21 for training time analysis). These experiments demonstrate that recurrent models perform well with short sequences. Note that despite RFormer's superior performance, our model is significantly faster than other continuous-time models, as shown in Appendix G.2, particularly in Table 21.

### Additional Efficiency Experiments and Discussion

We conduct additional experiments to compare the runtime of Rough Transformers with other models. In this experiment, we use the synthetic sinusoidal dataset considered in our paper and compute the runtime per epoch for varying sequence lengths. We demonstrate results for two variants of RFormer: "online", which corresponds to computing the signatures of each batch during training (resulting in significant redundant computation), and "offline", which corresponds to computing the signatures in one go at the beginning of training. We include a recent RNN-based model as a basis for comparison with high-performing RNN baselines. In addition to the models discussed in Section 4, we introduce Continuous Recurrent Units (CRU) [78] as a new baseline. See Table 21 for a summary of the results.

\begin{table}
\begin{tabular}{l c c c} \hline \hline LSD & Num. basis & Bandwidth & Acc (30 Epochs) \\ \hline \multirow{3}{*}{10} & 15 & 3 & 78\% \\  & 15 & 10 & - \\  & 20 & 3 & 100\% \\  & 20 & 10 & - \\ \hline \multirow{3}{*}{20} & 15 & 3 & 81.30\% \\  & 15 & 10 & 91.70\% \\  & 20 & 3 & 100\% \\  & 20 & 10 & 99.90\% \\ \hline \multirow{3}{*}{40} & 15 & 3 & 99.90\% \\  & 15 & 10 & 97.50\% \\ \cline{1-1}  & 20 & 3 & 100\% \\ \cline{1-1}  & 20 & 10 & 100\% \\ \hline \hline \end{tabular}
\end{table}
Table 20: CRU’s hyperparameters (\(L=100\)) (latent state dimension (LSD), number of basis matrices (Num.basis), and their bandwidth).

\begin{table}
\begin{tabular}{l c c c c} \hline \hline L & Random Drop & RFormer & CRU (LSD=10) & CRU (LSD=20) \\ \hline \multirow{3}{*}{100} & 0\% & 100.00\% & 100\% & 100.00\% \\  & 30\% & 98.60\% & 65.90\% & 99.60\% \\  & 50\% & 97.80\% & 34.40\% & 94.70\% \\  & 70\% & 96.10\% & 43.00\% & 78.60\% \\  & 85\% & 85.50\% & 32.30\% & 57.30\% \\ \hline \multirow{3}{*}{250} & 0\% & 100.00\% & 100.00\% & 100\% \\  & 30\% & 99.90\% & 42.95\% & 94.90\% \\  & 50\% & 99.40\% & 43.65\% & 77.30\% \\  & 70\% & 98.30\% & 45.40\% & 94.40\% \\  & 85\% & 86.20\% & 38.80\% & 83.60\% \\ \hline \multirow{3}{*}{500} & 0\% & 100.00\% & 100.00\% & OOM \\  & 30\% & 99.90\% & 47.15\% & OOM \\ \cline{1-1}  & 50\% & 99.70\% & 48.80\% & OOM \\ \cline{1-1}  & 70\% & 99.30\% & 55.15\% & OOM \\ \cline{1-1}  & 85\% & 87.70\% & 46.50\% & OOM \\ \hline \hline \end{tabular}
\end{table}
Table 19: Comparison of RFormer and CRU (two best and simplest performing instances [Num.basis/Bandwidth\(=20/3\)]) at different random drop percentages.

We remark that previous running times are obtained with a batch size of 10. Further, the ContiFormer model could be run for \(L=1000\) if decreasing the batch size to 2 (which significantly affects the parallelization process), avoiding OOM issues and resulting in 4025 seconds/epoch, which is several orders of magnitude larger than RFormer. As an additional experiment, we tested the epoch time (S/E) of RFormer for extremely oversampled sinusoidal time series. We show our results in the table below.

Thus, the time needed to compute the signature is inconsequential when compared with the time required to train standard models on the full or even downsampled datasets, since this step has to be carried out only once. To put this into context with an example, we note that it takes 4s to compute the signature representations for the HR dataset (which is about half the time it takes for the Vanilla Transformer to go through one epoch) and results in a 26\(\times\) increase in computational speed for RFormer when compared to the vanilla Transformer.

To showcase that this is the case for not only sequences of moderate length but also extremely long sequences, we also carry out the following experiment where we compute the signature representation for the sine dataset, with a progressively increasing number of datapoints. As seen in Table 23, this does not cause an explosion in computational time.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{6}{c}{S/E for Varying Context Length \(\downarrow\)} \\ \cline{2-11}  & **L=25k** & **L=50k** & **L=100k** & **L=250k** & **L=250k** \\ \hline RFormer (Online) & 5.39 & 9.06 & 19.95 & 45.20 \\ RFormer (Offline) & 0.60 & 0.61 & 0.60 & 0.63 \\ \hline \hline \end{tabular}
\end{table}
Table 22: Seconds per epoch for very large input length.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{6}{c}{S/E for Varying Context Length \(\downarrow\)} \\ \cline{2-11}  & **L=100** & **L=250** & **L=500** & **L=1000** & **L=2500** & **L=5000** & **L=7.5k** & **L=10k** \\ \hline NRDE & 5.87 & 11.67 & 20.27 & 44.01 & 103.11 & 201.21 & 312.31 & 467.47 \\ NCDE & 42.59 & 121.82 & 225.14 & 458.09 & 1126.77 & 2813.42 & 4199.50 & 5345.39 \\ GRU & 1.56 & 1.55 & 1.65 & 1.63 & 1.78 & 2.37 & 3.65 & 4.79 \\ CRU & 59.22 & 199.15 & 789.28 & OOM & OOM & OOM & OOM & OOM & OOM \\ ContiFormer & 61.36 & 248.31 & 1165.02 & OOM & OOM & OOM & OOM & OOM & OOM \\ Transformer & 0.75 & 0.79 & 0.82 & 0.95 & 1.36 & 5.31 & 9.32 & 16.32 \\ RFormer (Online) & 0.75 & 0.88 & 0.94 & 1.03 & 1.28 & 1.55 & 1.83 & 2.35 \\ RFormer (Offline) & 0.67 & 0.64 & 0.63 & 0.65 & 0.60 & 0.59 & 0.62 & 0.60 \\ \hline \hline \end{tabular}
\end{table}
Table 21: Seconds per epoch for growing input length and for different model types on the sinusoidal dataset.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{6}{c}{S/E for Varying Context Length \(\downarrow\)} \\ \cline{2-11}  & **L=25k** & **L=50k** & **L=100k** & **L=250k** \\ \hline RFormer (Online) & 5.39 & 9.06 & 19.95 & 45.20 \\ RFormer (Offline) & 0.60 & 0.61 & 0.60 & 0.63 \\ \hline \hline \end{tabular}
\end{table}
Table 23: Processing times for different sizes on the sinusoidal dataset.

### Additional ContiFormer Comparisons

Also, to provide some context of the performance of ContiFormer compared with our method (and not only results on complexity and training times), we run the model on the sinusoidal classification task for signals of length \(L=100\) and \(L=250\). Due to the slow running time of the ContiFormer model, we did not consider sequence lengths of \(L>250\). We evaluate the ContiFormer model using one head. However, given the subpar results we obtain, we also test it with four heads, using the hyperparameters originally used in the paper for their irregularly sampled time series classification experiments. By contrast, all variations of RFormer tested in this paper for this experiment employ only one head, but reported significantly better results.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Model & Epoch 100 & Epoch 250 & Epoch 500 \\ \hline ContiFormer (1 Head) & 2.3\% & 2.8\% & 3.1\% \\ ContiFormer (4 Heads) & 8.5\% & 17.3\% & 20.0\% \\ Transformer (1 Head) & 13.7\% & 40.1\% & 82.8\% \\ RFormer (1 Head) & 38.7\% & 81.1\% & 92.3\% \\ \hline \hline \end{tabular}
\end{table}
Table 24: Model performance for \(L=100\).

Figure 9: Seconds per epoch for growing input length and for different model types on the sinusoidal dataset for extremely long lengths (up to 250k) **Left:** Log Scale. **Middle:** Regular Scale. **Right:** Log-log scale. When a line stops, it indicates an OOM error.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Yes, they are an accurate reflection of the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, the limitations ae discussed in Appendix B. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The paper contains only one theoretical result, and a complete and correct proof is provided. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All experimental details are in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

[MISSING_PAGE_FAIL:28]

* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All experiments were conducted on an NVIDIA GeForce RTX 3090 GPU with 24,564 MiB of memory, as outlined in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We conform with the NeurIPS code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The societal impact statement is included in Appendix B. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper does not involve the release of data or models that have a high risk for misuse. However, as mentioned in our impact statement included in Appendix B, we acknowledge potential misuses of our advancements in time series analysis and advocate for ethical application and regulatory oversight. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The owners of the datasets and packages used are acknowledged in the code and in the manuscript. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. ** For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not introduce new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not include any experiments involving crowdsourcing or human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not include any research involving human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.