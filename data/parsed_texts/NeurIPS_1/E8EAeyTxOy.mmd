# InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models

 Linyi Li

Simon Fraser University

linyi_li@sfu.ca

&Shijie Geng

ByteDance Inc & Rutgers University

sg1309@rutgers.edu

 Zhenwen Li &Yibo He &Hao Yu &Ziyue Hua &Guanghan Ning

ByteDance Inc & Siwei Wang

ByteDance Inc & Tao Xie

Key Lab of HCST (PKU), MOE

taoxie@pku.edu.cn

Equal contribution.

###### Abstract

Large Language Models for code (code LLMs) have witnessed tremendous progress in recent years. With the rapid development of code LLMs, many popular evaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to measure the performance of code LLMs with a particular focus on code generation tasks. However, they are insufficient to cover the full range of expected capabilities of code LLMs, which span beyond code generation to answering diverse coding-related questions. To fill this gap, we propose **InfiBench**, the **first large-scale freeform question-answering (QA) benchmark for code** to our knowledge, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages. InfiBench uses four types of model-free automatic metrics to evaluate response correctness where domain experts carefully concretize the criterion for each question. We conduct a systematic evaluation for over 100 latest code LLMs on InfiBench, leading to a series of novel and insightful findings. Our detailed analyses showcase potential directions for further advancement of code LLMs. InfiBench is fully open source at [https://infi-coder.github.io/infibench](https://infi-coder.github.io/infibench) and continuously expanding to foster more scientific and systematic practices for code LLM evaluation.

## 1 Introduction

In recent years, Large Language Models (LLMs) have been revolutionizing the software development landscape [17, 12], demonstrating exceedingly strong and comprehensive capabilities in comprehending, generating, debugging, and summarizing code [9, 24]. For example, code LLM-powered products like GitHub Copilot [14] reached millions of active users within just one year of their launch.

Alongside the huge success of proprietary LLMs such as GPT-3.5 / GPT-4 [36] and Gemini [13], the development of open-source code LLMs2[35, 43, 30, 31] has been advancing at an unprecedented fast pace. As of June 2024, the Hugging Face Open LLM Leaderboard [4] has cataloged over 3,300 submissions of such models.

can still fail as exemplified in Figure 2. (4) **Common benchmarks may be contaminated.** Some LLMs have unconventional high performance in common benchmarks and are suspected to have memorized benchmark-related data [10, 46, 32], obscuring the evaluation results. _Can we systematically and comprehensively evaluate code LLMs' abilities in challenging real-world usage scenarios?_

\begin{table}
\begin{tabular}{c c c c c c} \hline Benchmark & Domain & \# Question & Evaluation & Data Source & Highest LLM Score \\ \hline HumanEval [9] & Python Programming & 164 & Test Cases & Hand-Winten & 90.2\% \\ MMPP [3] & Python Programming & 974 & Test Cases & Hand-Winten & 81.1\% \\ APPS [16] & Python Programming & 10,000 & Test Cases & Conceptions & (or no report yet) \\ IDS-D00 [21] & Python Programming & 1,000 & Test Cases & Surface Form Constraints & StackOverflow & / no report yet) \\ HumanEval++ [26] & Python Programming & 164 & Augmented Test Cases & HumanEval & 86.6\% \\ HumanEvalPredN [36] & Repair, Evolution, Generation to 6 Languages & 2,952 & Test Cases & HumanEval & 47.8\% \\ NLP [32] & Python Programming & 164 & Test Cases & Hand-Winten & 64.6\% \\ SVE-Bench [9] & Python Engineering & 2,294 & Test Cases & Github & 22.0\% \\ SWE-Bench Verified [38] & Python Engineering / Repair & 500 & Test Cases & SWE-bench & 45.20\% \\ \hline InfBench & \multicolumn{3}{c}{Free-Train Test Detection} & \multicolumn{3}{c}{234} & \multicolumn{3}{c}{Expected Field Testing} & \multicolumn{3}{c}{Shelf} & \multicolumn{1}{c}{70.64\%} \\ \multicolumn{3}{c}{} & \multicolumn{3}{c}{Answering in 15 Languages} & \multicolumn{3}{c}{Test Cases & Test Similarity} & \multicolumn{3}{c}{Overflow} \\ \hline \end{tabular}
\end{table}
Table 1: **Comparison between InfiBench and common existing benchmarks.** Existing benchmarks weigh heavily on code generation, unit-test-based evaluation, and major programming languages. InfiBench processes a much higher diversity to reflect real-world code LLMs’ usage scenarios. More discussion in Section 2.6.

Figure 1: **InfiBench overview**. We construct the InfiBench benchmark by filtering high-quality and diverse question posts from Stack Overflow and annotating question-level evaluation criteria with domain experts. With an model-free automatic evaluation framework, we evaluate over 100 latest code LLMs (one of the most extensive evaluations for code LLMs to the best of our knowledge), leading to several insightful findings.

Figure 2: A challenging question paraphrased from Stack Overflow where GPT-4 fails to answer.

To answer the question, we introduce InfiBench, a systematic benchmark for evaluating the free-form question-answering capabilities of code LLMs. As the first benchmark of its kind, the core principle of InfiBench aims to accurately represent how developers interact with and utilize such models in real-world scenarios. To achieve this, InfiBench comprises 234 questions that are carefully selected and proportionally filtered from the natural high-quality question distribution of Stack Overflow, without any constraints on topics, programming languages, question types, or answer forms. As a result, the curated 234 questions span 15 programming languages and 5 major areas: _front-end_, _back-end_, _DS&ML_ (_data science and machine learning_), _mobile and desktop_, and _ITOps_ (_information technology operations_).

Question diversity comes with evaluation challenges for two reasons. (1) Lack of metric. Unlike code generation or multiple-choice benchmarks, which can be evaluated through standardized methods like unit testing, there is no universal metric for response correctness for free-form questions. (2) Challenges with model-based evaluation. Model-based evaluations such as those involving GPT-4 are not only costly but also raise concerns about privacy and bias.

To mitigate the evaluation challenges, InfiBench includes an automatic evaluation framework that integrates four types of _model-free_ metrics: keyword matching, blank filling, unit testing, and dialogue similarity. For each question, we invite industry domain experts to paraphrase the prompt, select the most appropriate metric, and write down the concrete criteria using domain-specific knowledge, with highly-voted answers from Stack Overflow as a reference. These questions and evaluation criteria are then cross-validated to ensure correctness and objectiveness and further calibrated to improve consistency across languages. Human experiments show that InfiBench evaluation aligns with humans better than LLM-based evaluation, achieving 85.1% agreement rate compared to 77.8% achieved by GPT-4o-based evaluation.

As a novel and systematic benchmark disjoint with existing ones in terms of both forms and data sources, we believe that InfiBench is an ideal tool to measure existing code LLMs objectively. Hence, we conduct a systematic evaluation for **over 100 code LLMs** spanning both proprietary and open-source worlds using the InfiBench framework -- the latest and most extensive evaluation for code LLMs to the best of our knowledge. Our evaluation leads to several insightful findings: (1) On InfiBench, GPT-4 achieves a score of \(70.64\%\), being far from perfect but still far exceeding the most capable open-source models as of June 2024. On the other hand, GPT3.5 is surpassed by a few open-source models. (2) At similar model sizes, coding LLMs are usually visibly stronger than general LLMs; finetuning LLMs are usually visibly stronger than base LLMs. (3) The performance differences between different model families are huge, where one model could surpass another with less than 1/10 parameters, highlighting the importance of training data quality and techniques. (4) The scaling law is empirically verified for open-source models with fewer than 40B parameters, but not for those with more, where a turning point emerges. InfiBench is fully open source under CC BY-SA 4.0 license and continuously expanding3, including both the benchmark and Hugging-Face-compatible evaluation tools. All resources are available at [https://infi-coder.github.io/infibench](https://infi-coder.github.io/infibench).

Footnote 3: In other words, **inf**initely expanding, after which the **benchmark** is named.

## 2 Benchmark Creation

InfiBench is created from a high-quality subset of Stack Overflow questions up until June 14, 2023. In this section, we describe the data curation process and the evaluation framework in detail.

### Data Curation

Stack Overflow is a question-and-answer website for developers with more than 24 million registered users as of June 2024 [41]. Since the website is a large collection of natural and diverse coding questions from real-world developers, we believe that questions from Stack Overflow can effectively evaluate code LLM's capabilities in real-world usage scenarios.

The full Stack Overflow dataset contains 23.54 million question posts and 34.68 million answer posts. Each question post has a total view count. Each answer post is attached to a question and has a vote count. The question creator can choose one answer as officially accepted.

As we aim to create a benchmark where the correctness evaluation criteria are clear, we view the positively voted answers as an important reference source. Hence, we choose to keep only the questions that have at least three positively voted answers and an officially accepted answer, which turn out to be 1,090,238 questions. For these one million questions, we choose to keep questions that are frequently viewed and relatively new. To fulfill this criterion, we draw a scatter plot of these \(\approx\) 1 million questions, plotting the number of days since their creation until June 14, 2023 (data collection end-date) on the \(x\)-axis against the logarithm of their view counts on the \(y\)-axis. As shown in Figure 3, we empirically determine to keep questions that lie above the line connecting \((0,5)\) and \((3000,15.5)\), resulting in a subset of 17,402 questions.

Utilizing the mandatory question tags of these questions, we then manually construct a tag tree that covers the 200 most frequent tags, enabling us to identify the top programming languages and areas for 14,330 out of these 17,402 questions. These questions are from 24 programming languages, with each language being categorized into one primary area among the five (front-end, back-end, DS&ML, mobile and desktop, and ITOps). Lastly, we exclude 6 programming languages that either describe data or are domain-specific: JSON, regex, Markdown, YAML, CSV, and SQL. As a result, we compile 13,854 questions that serve as the _initial seed set_.

### Sampling

Based on a user study of developers' demand from our organization, we allocate the tentative area quota to be 25%, 25%, 25%, 15%, and 10% for front-end, back-end, DS&ML, mobile and desktop, and IT Ops, respectively. Inspired by HumanEval size and considering the labelling labor cost, we set 200 questions as the target benchmark size. Hence, the tentative size quotas by area are 50, 50, 50, 30, and 20 respectively. We then proportionally distribute the area quotas to language quotas based on the frequency of each language in the initial seed set. However, we observe that following this rule, certain languages such as CSS and C/C++ end up with fewer than 10 questions, which may yield unreliable language-level sub-score, so, for these languages, we set their quotas to 10.

As a result, we derive the _tentative_ question quota for each language as shown in Table 2, which sums up to 270 questions. After determining the tentative question quota, we uniformly sample from the initial seed set a roughly two times larger pool for the domain expects to select and annotate.

\begin{table}
\begin{tabular}{c|c|c c|c|c c c c c} \hline \hline \multirow{2}{*}{Area} & \multirow{2}{*}{Language} & \multicolumn{2}{c|}{Initial Seed Set} & \multicolumn{2}{c}{Tentative} & \multicolumn{4}{c}{Final InfiBench Benchmark} \\ \cline{4-10}  & & \# Questions & \% Area & \# Questions & \# Questions & \% Questions & \% Questions & \# Area & \% Area \\  & & & & Quota & Quota & Quota & Quota & Quota & Quota \\ \hline \multirow{2}{*}{Front-End} & Javascript & 4912 & \multirow{2}{*}{40.41\%} & 44 & 44 & 18.80\% & \multirow{2}{*}{63} & \multirow{2}{*}{26.92\%} \\  & CSS & 87 & & & & & & & \\  & HTML & 600 & & 10 & 9 & 3.85\% & \multirow{2}{*}{77} & \multirow{2}{*}{32.91\%} \\ \hline \multirow{4}{*}{Back-End} & Java & 930 & & 18 & 17 & 7.26\% & \multirow{4}{*}{63} & \multirow{4}{*}{26.92\%} \\  & CM & 629 & & 12 & 12 & 5.13\% & \\  & PHP & 462 & & 10 & 9 & 3.85\% & \multirow{4}{*}{77} & \multirow{2}{*}{32.91\%} \\  & Go & 117 & 18.71\% & 10 & 9 & 3.85\% & \multirow{4}{*}{77} & \multirow{2}{*}{32.91\%} \\  & Ruby & 71 & & 10 & 10 & 4.27\% & \\  & Rust & 96 & & 10 & 10 & 4.27\% & \\  & C/C++ & 287 & & 10 & 10 & 4.27\% & \\ \hline DS \& ML & Python & 2779 & & 47 & 47 & 2.00\% & \multirow{4}{*}{56} & \multirow{4}{*}{23.93\%} \\  & R & 184 & & 10 & 9 & 3.85\% & & \\ \hline \multirow{3}{*}{Mobile \& Desktop} & Dart & 1562 & & 19 & 19 & 8.12\% & 19 & 8.12\% \\  & Kotlin & 383 & & 10 & & & & \\  & Swift & 551 & & 10 & & & & \\  & VBA & 16 & & 9 & & & & \\ \hline IT Ops. & Bash & 188 & 1.36\% & 21 & 19 & 8.12\% & 19 & 8.12\% \\ \hline \hline Total & & 13854 & 100.0\% & 270 & 234 & 100.00 \% & 234 & 100.00\% \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Infibench data statistics by area and language**. We uniformly sample a subset from the initial seed set (see Section 2.1) according to the area quota (see Section 2.2) for domain experts to select questions and annotate the correctness criterion to construct the benchmark.

Figure 3: Scatter plot of filtered Stack Overflow questions. Questions above the orange line kept.

### Human Annotation

We recruited five domain experts inside our company to create the benchmark, each in charge of one area. The annotation process is composed of three steps:

* **Step 1: Question Selection and Type Annotation.** Domain experts select high-quality questions from the inspecting set and annotate the question type to be one of the four: code completion, code debugging, config and environment debugging, and knowledge question-answering.
* **Step 2: Prompt Paraphrasing.** Domain experts paraphrase and simplify the original question body into succinct and explicit instructions. We include this step for two main purposes: (1) Reduce domain gap. From user-shared conversations collected from ShareGPT, we observe that when interacting with code LLMs, users tend to provide short and direct instructions like "Fix problem..." and "Debug code...". However, when posting Stack Overflow questions, users tend to be lengthy with courtesy words. We ask the domain experts to paraphrase the question to code LLM user's style without changing the semantics. (2) Reduce the impact of memorization and data contamination. Some code LLMs may be trained or finetuned with Stack Overflow data. Paraphrasing the questions can help to mitigate the result advantages of these models. Benchmark results in Table 4 reveal the effectiveness of this step where copying Stack Overflow answers only achieves a 65.18% score. We defer further discussion in Section 2.5.
* **Step 3: Correctness Criterion Annotation.** Domain experts choose one or multiple evaluation metrics from our supported ones (see Section 2.4) and annotate the concrete criterion following a YAML schema. External files can be attached if needed, e.g., unit tests and reference answers.

**Calibration and Post-Filtering.** To improve annotation consistency and objectiveness, we introduce a few checkpoints for domain experts to read others' annotated cases, discuss them, and reach consensus for controversial cases. After the 270 tentative questions were annotated, we then ran an initial evaluation of all these questions on over 30 code LLMs. This initial evaluation helps us to identify questions whose criteria are incorrect or out of distribution. We filter out these questions and then remove all questions from Kotlin, Swift, and VBA languages since the questions in these languages are too few after filtering. After this calibration and post-filtering process, the final benchmark includes 234 questions spanning over 15 languages. Their statistics are shown in Table 2. As we can observe, compared to the population area distribution of high-quality Stack Overview questions (see "% Area Quota" column under "Initial Seed Set"), the area distribution of final benchmark questions (see "% Area Quota" column under "Final InfiBench Benchmark") is more balanced and less biased towards front-end, mobile, and desktop topics.

### Evaluation Criteria and Evaluation Framework

In response to the diversified questions, InfiBench evaluation framework integrates four types of model-free and automatic metrics as below. Domain experts choose one or multiple metric types along with their weights and concretize.

* **Keywords Matching.** Though the responses can be in diverse forms, for a significant portion of benchmark questions, we find that the existence of some keywords strongly determines the quality of the response. Domain experts can write rules that match keywords and regular expressions or construct recursive logical expressions on top of keyword-matching results. When multiple keywords exist, each matching result can have its weight in the final score.
* **Blank Filling.** For some questions, it is challenging to measure the correctness given the response uncertainty. In this case, domain experts can instruct the model to answer the question by following a given template and filling in the blanks in the template. The blanks can correspond to either natural language or code snippet. Then, similar to keywords matching, each blank can match potential keywords, regular expressions, or recursive logic expressions built upon matching results. This metric type tests not only the model's QA ability but also its instruction-following ability.
* **Unit Testing.** For code-intensive questions, we can follow existing benchmarks to evaluate response correctness by unit tests. For this type, domain experts may add more specifications in the prompt to allow for unit-test-based evaluation, such as specifications on function name, input arguments, and output format. Domain experts can further import the context setup and cleanup script.
* **Dialogue Similarity.** For natural-language-intensive questions, domain experts can extract and shorten the reference answers from Stack Overflow, and then use the ROUGE score [25] to evaluate the response similarity with reference answers. The ROUGE score was initially proposed andwidely used in evaluating the quality of text summarization and machine translation. To map the ROUGE score back to our benchmark scale, we allow domain experts to tune the mapping interval and scores within the interval are then linearly mapped to our score scale.

The example questions and corresponding criteria are illustrated in Figure 1. Detail statistics of metric type ratios, question type ratios, and prompt length are shown in Table 3.

Score Computation.We treat each question equally with one point each. Given 234 questions in the benchmark, the full score is 234, and we by default report the percentage score (achieved score divided by 234) unless otherwise noted. The one point for each question can be further decomposed into a few scoring points within each question. For example, a question may contain four keywords with weights 2, 1, 1, and 1 each. Then, matching each keyword can contribute to 0.4, 0.2, 0.2, and 0.2 points respectively to the final score.

Implementation.We have implemented an automated evaluation framework with Python, publicly available at [https://infi-coder.github.io/infibench](https://infi-coder.github.io/infibench). Specifically, for blank-filling evaluation, we use the longest common subsequence matching via dynamic programming to capture the filled blanks in the response. For unit-testing evaluation, we construct a runtime environment that supports the test execution for nine languages. We plan to integrate the framework into the Hugging Face Open LLM Leaderboard [4] to further ease the evaluation burden.

How does InfiBench Evaluation Align with Human?To evaluate the alignment between InfiBench evaluation and human expert evaluation, we randomly sample 100 questions without replacement from the benchmark and select three strong LLMs to generate responses: GPT-4-0613, GPT-3.5-turbo, and Mistral Codestral-22b. For each question, we randomly choose two out of these three model responses to construct response pairs, resulting in 100 response pairs \(\mathcal{R}=\{(A_{i},B_{i}):1\leq i\leq 100\}\). For each response pair \((A,B)\in\mathcal{R}\), we use InfiBench, GPT-4o, and human expert to evaluate into four outcomes: \(A\) is more correct than \(B\) (\(A>B\)); \(B\) is more correct then \(A\) (\(B>A\)); both \(A\) and \(B\) are correct (\(A\approx B\uparrow\)); both \(A\) and \(B\) are incorrect (\(A\approx B\downarrow\)). _Our purpose is to evaluate how InfiBench evaluation aligns with humans, specifically when compared to the widely-used LLM-as-a-judge (i.e., model-based evaluation) [47]._ The concrete grading criteria is as below:

* InfiBench gives a score between \([0,1]\) for each response in the pair. If the score difference in the pair is larger than 0.2, we label the outcome to be \(A>B\) or \(B>A\) respectively; otherwise, if the maximum score among the two is larger than 0.5, we label the outcome to be \(A\approx B\uparrow\); otherwise, we label the outcome to be \(A\approx B\downarrow\).
* For GPT-4o evaluation, we deploy the prompting template from LLM-Blender [18, Appendix E] and trigger GPT-4o for grading the four outcomes. We enhance the reliability of the comparison by switching \(A\) and \(B\) and prompting GPT-4o twice. We record the preference only when a consistent preference exists.
* For human evaluation, we recruit human annotators who came up with the criteria to label the comparison preference since they are familiar with the questions and have strong expertise. Annotators have no access to the evaluation results of InfiBench and GPT-4o, nor which source model generates the response. Annotators were instructed to directly label each pair with the four outcomes.

We defer the consensus matrices between InfiBench/GPT-4o and human annotators along with more findings in Appendix C. If we only count the cases where both InfiBench/GPT-4o and humans have clear preferences, the agreement rate between InfiBench and humans is 85.1%, and the agreement rate between GPT-4o and humans is 77.8%. Hence, _the InfiBench evaluation aligns with human experts better than the GPT-4o evaluation (with >80% confidence)_. We observe that the advantage of InfiBench comes from the ability to detect deceptive answers. some model responses pretend to be helpful with lengthy wording and hallucinations. GPT-4o is more likely to be cheated than InfiBench, which looks for key concepts that should exist in a helpful answer.

\begin{table}

\end{table}
Table 3: InfiBench statistics.

### Mitigations on Memorization and Data Contamination

InfiBench is created from the publicly available Stack Overflow corpus to reflect real-world scenarios, and this corpus may already exist in the training set of some code LLMs (e.g., DeepSeek Coder [15] and StarCoder 2 [28]). Hence, some code LLMs may achieve a high score simply due to memorization. To mitigate this, we asked the domain experts to paraphrase every question as an essential step (see Section 2.3). Hence, copying either the highly voted answers or officially accepted answers of the original questions only achieves 65.18%, being far from perfect and inferior to GPT-4's 70.64%. Furthermore, code LLMs that use Stack Overflow data do not demonstrate significant advantages over those without. Hence, we deem the effect of contamination as small.

On the other hand, we release the post IDs of the source question posts of InfiBench. Hence, future LLM training could consider this benchmark to conduct deduplication and ablation studies on data contamination. Another usage of our benchmark is to evaluate retrieval-augmented (RAG) code LLMs where perfect retrieval from Stack Overflow and moderate adaptation should solve these questions, which we leave as future work.

### Comparison with Existing Benchmarks

In Table 1, we compare InfiBench with several existing benchmarks for code LLMs. As reflected in the table, InfiBench strongly complements existing benchmarks for code LLMs by (1) extending them beyond code generation to a wide range of real-world tasks, (2) diversifying them since InfiBench does not share the same source as existing ones, and (3) increasing the differentiation as an unsaturated benchmark. Related benchmarks are further illustrated in Section 5. On the other hand, the benchmark is limited in size due to the high cost of correctness criteria labelling, and we are continuously expanding the benchmark.

## 3 Evaluation and Leaderboard

We systematically evaluated over 100 code LLMs spanning both proprietary and open-source worlds on InfiBench. To the best of our knowledge, this is the most extensive evaluation for code LLMs.

Evaluation Protocol.We adopt best@10 as the main evaluation metric: 10 responses are sampled and evaluated for each question, then the best score per question is recorded and summed up. Throughout the evaluation, we set sampling temperature \(T=0.2\) and top \(p=0.9\).

Furthermore, we swept sampling parameters with GPT-4 and the detailed results are in Appendix G. In a nutshell, for maximizing the performance under best@10, the best parameters are \(T=1.0\) and \(p=0.9\), leading to a score of \(76.15\%\pm 0.21\%\) (in comparison to \(70.64\%\pm 0.82\%\) in our main setting \(T=0.2,p=0.9\)). In particular, the temperature \(T\) affects much and the effect of top \(p\) is minor. We decided to stick to the original parameters \(T=0.2\) and \(p=0.9\) in the main evaluation since this setting is more akin to the real-world scenario where user generates once with low temperature.

We design two system prompts (shown in Appendix H), one for normal questions and the other for open-ended questions with an additional sentence to encourage succinct responses. For generic models, we generate the prompt with "{system prompt}\n{content prompt}" format; for instruction-finetuned or chat models, we generate the prompt with their prompt templates.

For proprietary models, we evaluate the latest models from OpenAI (GPT-4, GPT-4o, etc), Anthropic (Claude 3), and Mistral AI (Mistral Small/Medium/Large) with API calling. When budget permits, we repeat each evaluation three times and report standard deviation. For open-source models, we download models from Hugging Face and evaluate them on an 8xA100 server with bigcode-evaluation-harness [5]. When the model size is within 30B parameters, we repeat each evaluation three times and report the standard deviation. All raw model responses are available at [https://figshare.com/articles/dataset/InfiBench_Detail_Evaluation_Data/26104864](https://figshare.com/articles/dataset/InfiBench_Detail_Evaluation_Data/26104864). More details on the evaluation protocol are in Appendix E.

Leaderboard.In Table 4, we present aggregated InfiBench leaderboards by model family, model type, and model size. The full leaderboard is deferred to Appendix E due to space limit. The table includes scores from using the original Stack Overflow answer posts as reference. Results are also presented as a scatter plot in Figure 4, where normal models are shown as scatters with error bars,MoE models are shown as horizontal segments with error ranges connecting the activated parameters during inference and total parameters, and strong proprietary models are shown as horizontal lines.

In both tables and the figure, we classify LLMs by general/code and base/finetuned. The general LLMs are claimed to have strong capabilities beyond code, e.g., in various natural language tasks, while the code LLMs are exclusively optimized for the code domain. The base LLMs only went through the pretraining phase, while the finetuned LLMs are claimed to have instruction-following capabilities or are finetuned on instruction or human preference datasets.

## 4 Analysis and Discussion

**The best model so far, GPT-4, is still far from perfect, and open-source models are competitive but still far from GPT-4.** GPT-4 achieves the highest score 70.64% (interestingly, achieved by GPT-4-0613 instead of the more recent GPT-4o), then Claude 3 Opus with a score 63.89%, and then Codestral-22b [1] with a score 62.98% and deepseek-coder-33b-instruct [15] with a score 62.96%. The result implies that: (1) Noting that the full score is 100%, even the powerful GPT-4 is still far from perfect, which is in contrast to its \(\approx\)90% HumanEval score. We inspect the score breakdown. For the two most frequent metric types, keywords matching and unit testing, GPT-4 achieves similar scores 66.61% and 76.00% respectively. For blank filling, the score is relatively lower at 58.08%. These scores imply that GPT-4 may still lack generic ability in answering diversified real-world questions related to code. When instructed to follow a given template to answer (blank filling), due to the more strict requirement and narrower solution space, its lack of capability is more pronounced. (2) There is still a visible gap between open-source models and GPT-4. The gap between the most powerful open-source model, Codestral-22b, and GPT-4 is roughly 8 points. On the other hand, noticing that GPT-3.5-turbo achieves 56.47%, the open-source model, Codestral-22b, is now reliably better than GPT-3.5-turbo with merely 22B parameters which is promising.

**Among open-source models, different models have various performances.** Figure 4 systematically visualizes the performance of different open-source models at diverse scales. Although there is a general tendency that larger models achieve higher scores, the scores among different models at a similar scale differ largely. For example, on scale 7B, the best-performing model is at around

\begin{table}

\end{table}
Table 4: **Aggregated InfiBench leaderboards (best viewed zoomed in and in color). “Size” column records number of parameters. For MoE models, “total params. / params. activated during inference” is recorded. Bar colors stand for General Base, General Finetuned, Code Base, and Code Finetuned models respectively. Icon “\(\bar{\texttt{i}}\)” stands for proprietary models otherwise open-source. Full leaderboard in Appendix E.**55%, pretty close to GPT-3.5, while the low-performing model stays at around 15%. Moreover, deepseek-coder-1.3b-instruct achieves 41.32% at 1.3B and surpasses a few models at scale 70B or 100B. Hence, though scaling matters, the training techniques and training data are equally important or even more, helping to reduce the required scale for achieving a certain score by more than 10\(\times\).

**Hard problems generalize their difficulties.** We rate the benchmark problem difficulty with five levels by how well GPT-4 and GPT-3.5-turbo answer them, as detailed in Appendix D. Example questions from each level are shown in Appendix I. We present the detail result table including the sub-score for each difficulty level in Appendix E. Interestingly, the trend is _highly consistent that sub-scores decrease along with the increase of problem level_. Specifically, hard problems for the most powerful model yet, GPT-4, are also generally hard for open-source models. These hard problems usually correspond to code generation with long and domain-specific context or challenging blank-filling questions since blank-filling is a specific task that rarely appears in training data before.

**Instruction finetuning is important for QA.** Among models of similar scales and the same family, we find that the best-performing ones almost always include an instruction-finetuning phase, such as deepseek-llm-67b-chat, deepseek-coder-33b-instruct, CodeLlama-34B-Instruct, and Qwen-18B-Chat. In contrast, the pretraining models, such as davinci-002 and phi models, usually perform poorly despite their strong performances in code generation benchmarks. Instruction-finetuning is also critical for other code domain tasks such as code generation. As shown in Appendix F.1 where we plot model scores in QA (measured by InfiBench) and code generation (measured by HumanEval pass@1), instruction-tuning generally improves both QA and code generation, but the improvement is usually more significantly on code generation but more moderately on QA. As a result, we suggest generalizing the instruction-finetuning data beyond simple coding problems to improve code LLMs. Indeed, our preliminary experiments show that, after fine-tuning with the decontaminated and sanitized Stack Overflow data, we improved InfiBench scores for Codellama-13b-Instruct from 46.37% to 60.74% and for mixtral-8x7B-Instruct from 55.55% to 62.61%.

**Some models may focus too much on code generation, especially the small ones.** As detailed in Appendix F.1, we observe that for large models (\(>\)30B) and top entries, InfiBench and HumanEval pass@1 scores coincide well. However, for smaller models, the score tendencies start to diverge, where some models are relatively stronger on InfiBench (Mixtral-8x7B-Instruct) and more are relatively stronger on HumanEval (Phi1, Phi2, gamma-7b,...). This phenomenon implies that a few models may be optimized too heavily on code generation benchmarks while ignoring the performance in generic code scenarios as represented by InfiBench, which in turn highlights the significance of free-form QA benchmarks like InfiBench in detecting capability imbalance in code LLMs.

Figure 4: **Scatter plot for all evaluated LLMs on InfiBench. \(x\)-axis is the model size in terms of number of parameters and \(y\)-axis is InfiBench score. Projected empirical scaling laws for both general and code models are drawn. Detail discussion in Section 4.**

**Code LLama models have unique characteristics.** We evaluated all Code Llama models [39]. As shown in Table 5, we found finetuning on Python data improves on HumanEval but hurts InfiBench scores, while instruction finetuning usually improves InfiBench scores but may hurt HumanEval. As a side product, we found CodeLlama-70B may be overly safeguarded and denies answering some safe questions in InfiBench. More model-specific findings are presented in Appendix F.

**Code models and general models may exhibit different scaling laws, and open-source models scale well only within 40B yet.** In Figure 4, we use the top-performing code and general models at each scale respectively to regress and extrapolate model performance at larger scales. As shown, code models tend to have higher capabilities compared to general models of the same scale, though the gap shrinks for larger models. Hence, when the compute budget is heavily limited, training exclusively in the code domain could be more efficient for building strong code LLMs.

In Figure 4, both predicting curves are split into two segments, steep in the first segment and much flat in the second. Following the first segment, open-source models catch up with GPT-4 at around 50B scale. However, following the second segment, they may need to be at >300B scale to catch up. The finding contradicts the common scaling law [20, 34, 7] where a strong linear relationship between model scale and capability exists. The contradiction implies that very large open-source models (>40B) may fail to achieve the expected performance at their scales, or there is some non-trivial barrier when scaling the model beyond 40B, or the scaling law may change at such a large scale. We leave further investigation as the future work. Notably, after the release of InfiBench, Deepseek-coder-v2 [49] was released as the largest code LLM to our knowledge in an MoE architecture with 236B total and 21B active parameters. On InfiBench, Deepseek-coder-v2 achieves 65.49%, setting the new baseline for open-source LLMs but still being inferior to GPT-4. More importantly, the score is within the predicted range of our empirical scaling law.

We defer dataset card and data accessibility details, discussion on limitations and societal impact, full leaderboard, additional findings, ablation studies, and data examples in appendices.

## 5 Related Work

Large language models [44, 11, 8] are transforming people's lives. In the coding domain, LLMs [9, 24] are shown to be capable of completing a wide range of tasks such as code generation, debugging, and question-answering. Recently, code LLMs are booming. New models, including both proprietary [14, 36] and open-source ones [4, 35, 42, 43, 22, 30, 39, 49], emerge almost every month.

Benchmarks for code LLMs are developing, though at a relatively slower pace. Common benchmarks, e.g., APPS [16], MBPP [3], and HumanEval [9], focus on code generation and unit-test-based evaluation. Some efforts augment these benchmarks by language translation (e.g., Multilingual HumanEval [2], HumanEval-X [48]), test augmentation (e.g., HumanEval+ [26]), task generalization (e.g., HumanEvalPack [33]), and human rewriting (e.g., LBPP [32]). To systematically evaluate real-world problem solving, recently, SWE-bench [19], its filtered version SWE-bench Verified [38], and RepoBench [27] are proposed but they still primarily focus on code generation. Some general-purpose benchmarks, e.g., Arena-Hard [23], contain code-related questions, but rely on LLM to judge and do not provide domain-specific scores. CodeXGLUE [29] considers multiple coding capabilities beyond code generation, but replies on existing data sources. In contrast to these benchmarks, InfiBench benchmark is built for evaluating free-form question-answering ability in the code domain beyond code generation in an automated and model-independent way.

## 6 Conclusion

We proposed InfiBench, a systematic benchmark for evaluating the question-answering ability of code LLMs in real-world scenarios, to facilitate development and scientific evaluation of LLMs. InfiBench comprises 234 high-quality questions from Stack Overflow and supports automatic model-free evaluation. A comprehensive evaluation of over 100 code LLMs reveals several findings and takeaways. The benchmark is publicly available and continuously expanding.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline \hline  & Benchmark & Base & Python & Instrut \\ \hline \multirow{2}{*}{7B} & HumanEval & 33.53\% & 34.54\% (+-1.95) & 34.85\% (+-1.25) \\  & InfiBench & 37.62\%, acc. & 28.95\%, acc. (+-4.75) & 33.15\%, acc. (+-2.47) \\ \hline \multirow{2}{*}{13B} & HumanEval & 36.07\% & 44.25\%, acc. (+-7.33) & 42.78\% (+-7.07) \\  & InfiBench & 41.66\%, acc. & 41.31\%, acc. (+-0.35) & 46.37\%, acc. (+-1.71) \\ \hline \multirow{2}{*}{34B} & HumanEval & 45.83\% & 53.74\% (+-1.95) & 44.12\% (+-1.73) \\  & InfiBench & 47.36\% & 43.13\% (+-2.35) & 50.45\% (+-3.07) \\ \hline \multirow{2}{*}{70B} & HumanEval & 53.03\% & 57.97\% (+-1.95) & 69.83\% (+-1.85) \\  & InfiBench & 40.60\% & 40.29\% (-0.31) & 42.82\% (+-2.22) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Evaluation on eight models from the Code Llama [39] family showcases intense Python finetuning may hurt free-form QA ability, despite achieving higher HumanEval scores.

## Acknowledgement

We thank ByteDance Inc. for the support on computing resources, anonymous reviewers for their constructive feedback, and Kaixin Li (National University of Singapore) for contributing the Docker image after the initial release of InfiBench. This work was partially supported by National Natural Science Foundation of China under Grant No. 62161146003, and the Tencent Foundation/XPLORER PRIZE. Tao Xie is also affiliated with the School of Computer Science, Peking University, China. The corresponding authors are Linyi Li and Tao Xie.

## References

* [1]M. AI. C.: Hello, world! | mistral ai | frontier ai in your hands. [https://mistral.ai/news/codestral/](https://mistral.ai/news/codestral/), 2024.
* [2] Ben Athiwaratkun, S. Grouda, Z. Wang, X. Li, Y. Tian, M. Tan, W. Uddin Ahmad, S. Wang, Q. Sun, M. Shang, S. Kumar Gonugondla, H. Ding, V. Kumar, N. Fulton, A. Farahani, S. Jain, R. Giaquinto, H. Qian, M. Krishna Ramanathan, R. Nallapati, B. Ray, P. Bhatia, S. Sengupta, D. Roth, and B. Xiang (2023) Multi-lingual evaluation of code generation models. In Eleventh International Conference on Learning Representations, External Links: Link Cited by: SS1, SS2.
* [3]J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. (2021) Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Cited by: SS1, SS2.
* [4]E. Beeching, C. Fourrier, N. Habib, S. Han, N. Lambert, N. Rajani, O. Sanseviero, L. Tunstall, and T. Wolf (2023) Open llm leaderboard. [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). Cited by: SS1, SS2.
* [5]L. Ben Allal, N. Muennighoff, L. Kumar Umapathi, B. Lipkin, and L. von Werra (2022) A framework for the evaluation of code generation models. Note: [https://github.com/bigcode-project/bigcode-evaluation-harness](https://github.com/bigcode-project/bigcode-evaluation-harness) Cited by: SS1, SS2.
* [6]M. Bhatt, S. Chennabasappa, Y. Li, C. Nikolaidis, D. Song, S. Wan, F. Ahmad, C. Aschermann, Y. Chen, D. Kapil, et al. (2024) Cyberseceval 2: a wide-ranging cybersecurity evaluation suite for large language models. arXiv preprint arXiv:2404.13161. Cited by: SS1, SS2.
* [7]X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al. (2024) DeepSeek llm: scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954. Cited by: SS1, SS2.
* [8]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. In Thirty-fourth International Conference on Neural Information Processing Systems, Cited by: SS1, SS2.
* [9]M. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. (2021) Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Cited by: SS1, SS2.
* [10]J. Dekoninck, M. Niklas Muller, and M. Vechev (2024) Constat: performance-based contamination detection in large language models. arXiv preprint arXiv:2405.16281. Cited by: SS1, SS2.
* [11]J. Devlin, M. Chang, K. Lee, and K. Toutanova (2018) Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Cited by: SS1, SS2.
* [12]A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo, and J. M. Zhang (2023) Large language models for software engineering: survey and open problems. arXiv preprint arXiv:2310.03533. Cited by: SS1, SS2.

* [13] Google Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* your ai pair programmer. [https://github.com/features/copilot](https://github.com/features/copilot), 2023.
* [15] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming-the rise of code intelligence. _arXiv preprint arXiv:2401.14196_, 2024.
* [16] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.
* [17] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. Large language models for software engineering: A systematic literature review. _arXiv preprint arXiv:2308.10620_, 2023.
* [18] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. In _Sixty-first Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2023.
* [19] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. Swe-bench: Can language models resolve real-world github issues? In _Twelfth International Conference on Learning Representations_, 2024.
* [20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.
* [21] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for data science code generation. In _Fortieth International Conference on Machine Learning_, 2023.
* [22] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Koectkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! _arXiv preprint arXiv:2305.06161_, 2023.
* [23] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. _arXiv preprint arXiv:2406.11939_, 2024.
* [24] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. _Science_, 378(6624):1092-1097, 2022.
* [25] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, 2004.
* [26] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In _Thirty-seventh International Conference on Neural Information Processing Systems_, 2023.
* [27] Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto-completion systems. _arXiv preprint arXiv:2306.03091_, 2023.
* [28] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. _arXiv preprint arXiv:2402.19173_, 2024.

* [29] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. Codexglue: A machine learning benchmark dataset for code understanding and generation. In _Thirty-fifth International Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)_, 2021.
* [30] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. _arXiv preprint arXiv:2306.08568_, 2023.
* [31] Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schoelkopf, Mrinmaya Sachan, and Taylor Berg-Kirkpatrick. Membership inference attacks against language models via neighbourhood comparison. In _Findings of the Association for Computational Linguistics: ACL 2023_, 2023.
* [32] Alexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone, Milad Alizadeh, Jingyi He, Raymond Ma, Maxime Voisin, Ellen Gilsenan-McMahon, and Matthias Galle. On leakage of code generation evaluation datasets. _arXiv preprint arXiv:2407.07565_, 2024.
* [33] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. _arXiv preprint arXiv:2308.07124_, 2023.
* [34] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin A Raffel. Scaling data-constrained language models. In _Thirty-seventh International Conference on Neural Information Processing Systems_, 2024.
* [35] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. In _Eleventh International Conference on Learning Representations_, 2023.
* [36] OpenAI. Gpt-4 technical report. _OpenAI_, 2023.
* [37] OpenAI. Hello gpt-4o | openai. [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/), 2024.
* [38] OpenAI. Introducing swe-bench verified | openai. [https://openai.com/index/introducing-swe-bench-verified/](https://openai.com/index/introducing-swe-bench-verified/), 2024.
* [39] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_, 2023.
* [40] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In _Thirty-eight IEEE Symposium on Security and Privacy_, 2017.
* [41] StackExchange. All sites -- stackexchange. [https://stackexchange.com/sites?view=list#users](https://stackexchange.com/sites?view=list#users), 2024.
* [42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [43] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Thirty-first International Conference on Neural Information Processing Systems_, 2017.
* [45] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. In _Thirty-seventh International Conference on Neural Information Processing Systems_, 2024.

* [46] Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking benchmark leakage in large language models. _arXiv preprint arXiv:2404.18824_, 2024.
* [47] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In _Thirty-seventh International Conference on Neural Information Processing Systems_, 2024.
* [48] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. In _Twenty-ninth ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, 2023.
* [49] Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. _arXiv preprint arXiv:2406.11931_, 2024.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] Limitations are discussed throughout Section 2 and specifically in Appendix B. 3. Did you discuss any potential negative societal impacts of your work? [Yes] Societal impacts are discussed in Appendix B. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] All code and data are publicly available at [https://infi-coder.github.io/infibench](https://infi-coder.github.io/infibench) along with instructions needed to reproduce. The accessibility information is also available in detail in Appendix A. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A] This work does not involve model training. The inference hyperparameters are listed in Section 3 and ablation studies are presented in Appendix G. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] All experiments are repeated three times whenever budget and computing resource permit. Error bars and standard deviations are reported. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] The information is included in Section 3.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? The main assets are from Stack Overflow which is open source under CC BY-SA 4.0 license. We inherit this license to release. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] New assets (the benchmark and evaluation tool) is accessible through [https://infi-coder.github.io/infibench](https://infi-coder.github.io/infibench).

* Did you discuss whether and how consent was obtained from people whose data you're using/curating? We release the new asset inheriting the CC BY-SA 4.0 license as described in Section 1 and Appendix A.
* Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? Domain experts are required to remove such information by paraphrasing when constructing the benchmark.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]

## Appendices

In appendices, we present dataset card and data accessibility details in Appendix A, discussion on limitations and societal impact in Appendix B, agreement statistics between InfiBench/GPT-4o and human in Appendix C, question grouping by difficulty in Appendix D, full leaderboard in Appendix E, additional findings in Appendix F, study of sampling hyperparameters in Appendix G, prompts in Appendix H, and benchmark data examples in Appendix I.

## Appendix A Dataset Card and Accessibility Details

```
DatasetCard * Name: InfiBench * Description: Evaluation Dataset for the Question-Answering Capabilities of Code Large Language Models * URL: [https://infi-coder.github.io/infibench](https://infi-coder.github.io/infibench) (all resources) / [https://huggingface.co/datasets/llylly001/InfiBench](https://huggingface.co/datasets/llylly001/InfiBench) (data part) * Version: 2.1 License: Creative Commons Attribution Share Alike 4.0 * Citation: @misc{infibench,  title={InfiBench: Evaluating the Question-Answering Capabilities  of Code Large Language Models},  howpublished = "url{[https://infi-coder.github.io/infibench](https://infi-coder.github.io/infibench)}",  author={InfiBench},  year={2024} }
```
* DOI: doi:10.57967/hf/2474
* Responsible AI -- Data Collection: Data source is downloaded from the publicly available StackExchange archive ([https://archive.org/download/stackexchange](https://archive.org/download/stackexchange), [https://ia904700.us.archive.org/view_archive.php?archive=/6/items/stackexchange/stackoverflow.com-Posts.7z](https://ia904700.us.archive.org/view_archive.php?archive=/6/items/stackexchange/stackoverflow.com-Posts.7z)). Especially, we use the preprocessed version from [https://huggingface.co/datasets/mikex86/stackoverflow-posts](https://huggingface.co/datasets/mikex86/stackoverflow-posts) where all posts are formatted in Markdown text. We choose to keep only the questions with at least three positively voted answers and an officially accepted answer, which turn out to be 1,090,238 questions. For these one million questions, we choose to keep frequently viewed and relatively new questions. Utilizing the mandatory question tags of these questions, we then manually construct a tag tree that covers the 200 most frequent tags, enabling us to identify the top programming languages and areas for 14,330105 out of these 17,402 questions. We exclude 6 programming languages that either describe data or are domain-specific: JSON, regex, Markdown, YAML, CSV, and SQL. As a result, we compile 13,854 questions that serve as the initial seed set. We randomly sample from the initial seed set. Then we recruited five domain experts inside our company to create the benchmark from the sampled initial seed set, each in charge of one area. The annotation process is composed of three steps: (1) Question Selection and Type Annotation; (2) Prompt Paraphrasing. (3) Correctness Criterion Annotation.
* Responsible AI -- Data Biases: The data essentially serves as an evaluation benchmark. We foresee data biases in the following aspects: (1) Non-standard evaluation. Alongside the data is a comprehensive benchmark of existing code LLMs. The benchmark scores are evaluated under a specific set of hyperparameters(e.g, temperature 0.2, top probability 0.9, best@10 at question level). Data usage under different evaluation conditions may result in misleading comparison results and conclusions. (2) Usage misinterpretation. The benchmark focuses on evaluating the response correctness of code LLMs for a set of real-world developers' questions. Our evaluation standard does not specifically take other aspects (naturalness, conciseness, fairness, politeness, etc) into consideration. Hence, this is risk of overinterpreting the evaluation results. When evaluating a code LLM, we recommend combining this benchmark score with other evaluations to be a more comprehensive evaluation.

(3) Potential data contamination. Though we have made our efforts to reduce the impact of data contamination, future code LLMs may train or fine-tune on this benchmark dataset to improve the score on InfiBench. This could be challenging to prevent as a cost of being fully public. On the other hand, as responsible LLM developers, we hope future practitioners would report how they use the benchmark data if beyond the original scope (for evaluation use).
* **Responsible AI -- Personal Sensitive Information**: During the data construction process, our domain experts paraphrased the question prompts to remove personal and sensitive information (PII) and a cross validation stage was introduced to further ensure the PII removal.

**Croissant Dataset Description:**[https://huggingface.co/datasets/llylly001/InfiBench/blob/main/croissant-infibench.json](https://huggingface.co/datasets/llylly001/InfiBench/blob/main/croissant-infibench.json). Note that the Croissant format is mainly designed for machine learning dataset description. However, InfiBench is more than a dataset; it is an evaluation benchmark including response evaluation standards, tools, and an accompanying leaderboard. Hence, the Croissant script records only the CSV file and covers question prompts and evaluation standards; whereas the open-source evaluation tool and leaderboard are not recorded which can be separately downloaded from [https://infi-coder.github.io/infibench](https://infi-coder.github.io/infibench).

**Data Accessibility.** As briefly mentioned in the main text, all materials are made publicly available and accessible at the website: [https://infi-coder.github.io/infibench](https://infi-coder.github.io/infibench) without personal request. The materials include three parts: (1) Benchmark questions and evaluation metrics -- this part is additionally uploaded to Hugging Face (URL and DOI are in the above dataset card). (2) Automatic evaluation tool -- this part is uploaded and maintained in a dedicated GitHub repo [https://github.com/infi-coder/infibench-evaluator](https://github.com/infi-coder/infibench-evaluator). In addition, we uploaded our extension of bigcode-evaluation-harness[5], namely infibench-evaluation-harness to a dedicated GitHub repo [https://github.com/infi-coder/infibench-evaluation-harness](https://github.com/infi-coder/infibench-evaluation-harness). The extension includes the inference code on InfiBench for all evaluated LLMs. (3) Evaluation raw data and leaderboard -- the leaderboard is displayed on the website [https://infi-coder.github.io/infibench](https://infi-coder.github.io/infibench) and the raw model responses are stored in the website repo [https://github.com/infi-coder/infibench](https://github.com/infi-coder/infibench). All materials are under the Creative Commons Attribution-Share Alike 4.0 license. In the above dataset card and Appendix B, we anticipate potential inappropriate usage of the benchmark and we encourage the practitioners to document their usage of the benchmark if beyond model evaluation. In the future, we will continue the maintenance and expansion of the benchmark. Furthermore, we are developing an adaptor for automatic evaluation on Hugging Face so that InfiBench can be integrated into the Hugging Face Open LLM Leaderboard [4] to further ease the evaluation burden.

## Appendix B Limitations, Societal Impacts, and Future Work

In this appendix, we expand our discussion of limitations, potential societal impacts, and future work.

**Evaluation Metric.** In InfiBench, the expert-annotated evaluation metric is designed to mainly focus on response correctness, more specifically, whether the response contains key information that solves the given question. Concretely, the metric may evaluate whether the response passes a given set of unit tests, whether it suggests the right API or concept, whether it follows the instruction to provide relevant information, etc. Hence, the score comes with two limitations: (1) The score is subjective since the metric is annotated by human experts without an explicit and universal standard. Note that we did not aim to provide an objective metric since the developers' views of response correctness intrinsically vary and diverge for these diverse questions. On the other hand, we introduce a cross-validation and calibration stage to improve the metric representativeness of most developers' standards. We leave it as a future work to further quantitatively measure and improve the metric representativeness. (2) The score focuses mainly on correctness. Several other aspects define a model's usability, such as language naturalness (including conciseness, politeness, etc), trustworthiness (refusal of risky questions, fairness, unbiasedness, privacy, etc), and system-level metrics (latency, throughput, parallelism-friendliness, etc). Model evaluators and practitioners may keep in mind that InfiBench score is not a comprehensive usability measurement of code LLMs, and we strongly encourage them to combine InfiBench score with benchmarks on these other aspects (c.f. [6, 45]) to comprehensively evaluate LLMs.

Data Contamination.The limitations and mitigations on data contamination are discussed in Section 2.5. In addition, as a side effect of open source, future code LLMs may leverage the benchmark data to deliberately introduce data contamination to achieve a high score in InfiBench. To partly detect such data contamination, our evaluation of using the original stack Overflow answers might be a proxy. According to Table 4(a), even gold extraction from human answers cannot saturate the benchmark while strong LLMs like GPT-4 surpassed human answers. Hence, if a future model achieves scores close to human answers (between 50% and 65%) but cannot further improve beyond human along with scaling, data contamination may potentially happen. Detecting data contamination is itself a research topic where research on member inference attacks [40, 31] is involved. We did not integrate a detection module in the current release of InfiBench but we are planning to inspect this topic in the future.

Labelling Cost.InfiBench construction involves human labelling cost, where domain experts paraphrase the source question post and label the evaluation metric. Such a cost prevents the InfiBench from scaling up in terms of size, and the questions for less popular programming languages, such as Rust and Ruby, are relatively few. In an attempt to mitigate this limitation, we explored a few alternative evaluation metrics, such as dialogue similarity with officially accepted answers. However, these alternatives either require a language model which may induce bias and heavy computing cost, or deviate away from domain experts' correctness judgment. We leave the exploration of more scalable metrics and annotation procedures as future work and make the benchmark fully open source so community involvement may boost the expansion.

## Appendix C Agreement Statistics between InfiBench/GPT-4o Evaluation and Human

In Section 2.4, we evaluated the alignment between InfiBench/GPT-4o evaluation and human evaluation by generating 100 response pairs for InfiBench questions and let InfiBench, GPT-4o, and human annotators to grade into four outcomes.

Table 6 shows the confusion matrices between InfiBench/GPT-4o and human, where each cell corresponds to the frequency of each combination of outcomes among 100 pairs. The implication of each outcome is introduced in Section 2.4.

Learned from Table 6, if we only count the cases where both human and InfiBench have clear preferences, their agreement rate is \(\frac{40}{47}=85.1\%\); if we only count the cases where both human and GPT-4o have clear preferences, their agreement rate is \(\frac{35}{45}=77.8\%\). Hence, the InfiBench evaluation aligns with human experts better than the GPT-4o evaluation (with >80% confidence). Furthermore,

\begin{table}

\end{table}
Table 6: Confusion matrices between InfiBench/GPT-4o and human. Details in Appendix C. Bolded cells correspond to when both methods have clear preferences on one response.

we observe that GPT-40 has a stronger opinion and tends to choose one response more often, so it falls short when \(A\) and \(B\) are both bad responses, labelling none of them as "both bad". We also observe that InfiBench evaluation could be too strict due to pattern matching and fixed post-processing leading to over-differentiation--when a human believes \(A\) and \(B\) are both good responses, with only a 29.4% chance InfiBench labels them as "both good".

## Appendix D Difficulty Grouping

We systematically evaluated GPT-4 and GPT-3.5-turbo on the benchmark following the evaluation protocol in Section 3, based on which we classify the benchmark questions into five disjoint difficulty groups.

* Level 1 (93 questions, 39.7%): GPT-3.5-turbo can achieve a mean score \(\geq\) 0.5.
* Level 2 (55 questions, 23.5%): Among the rest questions, those where GPT-4's mean score \(\geq\) 0.5.
* Level 3 (44 questions, 18.8%): Among the rest questions, those where GPT-4 with sampling temperature 1.0 can achieve a maximum score \(\geq\) 0.5 among 10 trials.
* Level 4 (18 questions, 7.7%): Among the rest questions, those GPT-4 with sampling temperature 0.2 can achieve a positive score among 100 trials.
* Level 5 (24 questions, 10.3%): The remaining questions, i.e., GPT-4 cannot get score among 100 trials.

Appendix E shows each code LLM's score in each difficulty group. The mean scores strictly decrease for higher difficulty levels, highlighting that the question difficulty is in general consistent across different code LLMs and our group assignment is reasonable. We hope that the grouping can help better reveal the strengths and weaknesses of a code LLM for different questions.

Question examples by difficulty groups are in Appendix I.

## Appendix E Evaluation Details and Full Benchmark Results

Evaluation Details of Code LLMs.For proprietary model evaluation, we did not specify the max tokens to generate and found out that the longest response generated by GPT-4 has 662 tokens with Code Llama tokenizer.

For open-source model evaluation, for models with over 30B parameters, due to the GPU memory limit and efficiency concerns, we impose the longest context constraint of 4,096 tokens and experiment just once. Since there is only one question whose GPT-4 context (prompt + GPT-4 response) can exceed 4,096 tokens, we think this context constraint has little effect, reducing the score by 0.37% at most. For models within 30B parameters, since GPT-4 response has at most 662 tokens, we set the max number of tokens to generate to be \(\min\{1024,\text{context length - prompt length}\}\), providing some wiggle room. Meanwhile, we repeat the evaluation three times for models within 30B parameters.

Evaluation Details of Original Stack Overflow Answers.As listed in Table 4(a) and Table 7, besides evaluating LLM responses, we evaluated the score of human-written original Stack Overflow answers since the question prompts are paraphrased from Stack Overflow. We consider three settings: (1) evaluating the officially-accepted answer post (note that we select only the Stack Overflow questions with an officially-accepted answer into the benchmark); (2) evaluating the highest-voted answer post (note that any registered user can equally vote for or against an answer); and (3) evaluating the highest-voted answer posts up to 10 and recording the highest score achieved by any post. For the last setting, we chose the number 10 because the main evaluation metric of model response is best@10. Moreover, we observe that all officially accepted answers for InfiBench questions are among the top 10 highest-voted answer posts. Note that there is no randomness of scores from Stack Overflow answers, so we do not repeat the evaluation nor report the standard deviation.

As expected, the last setting achieves the highest score 65.18% among the three settings. Due to its consistency with models' evaluation metric best@10, we deem this score most comparable with scores from LLMs. Interestingly, when considering only one answer post, the second setting, selecting the highest-voted answer, is better than the first setting, selecting the officially accepted answer.

Full Benchmark Results.We present the full leaderboard in Table 7 (by descending order of InfiBench scores) and Table 8 (by alphabetical order of model family names). These tables are expanded from the aggregated Table 4. In these tables, we show model properties including size and context length. We also present HumanEval [3] scores since HumanEval is one of the most widely used benchmarks for evaluating code LLMs (further discussion in Appendix F). Furthermore, we represent the score breakdown by difficulty levels, problem types, and evaluation metric types. The proportion of each difficulty level can be found in Appendix D, and the proportion of each problem type and evaluation metric type is shown in Table 3(a,b). InfiBench score can be computed by the weighted sum of breakdown subscores by proportions. We present the score of human-written original Stack Overflow answers in the last three rows.

In tables, the mean scores are computed from scores of all 106 code LLMs. We observe that the mean overall score, 37.82%, is still much inferior to human answers (which achieves over 50% even with just one attempt). The model performance is monotonically decreasing for higher difficulty levels; relatively equivalent across different problem types; and weaker under blank-filling and dialogue-similarity metrics than keyword-matching and unit-testing metrics.

## Appendix F Additional Findings and Discussion

In this appendix, we present additional findings and discussion that are omitted from Section 3.

### Correlations between InfiBench and HumanEval Scores

We study the correlation between InfiBench and HumanEval pass@1 scores for different LLMs. In Figure 5, we plot LLMs with both InfiBench and HumanEval scores, in total 66 LLMs, in Table 7 as a scatter plot. The figure shows that scores on the two benchmarks are generally positively correlated, with a Pearson correlation coefficient \(r=0.8058\). If conducting a linear regression, we would observe that different model types (i.e., general/code model, base/finetuned model) share almost the same linear relationship, indicating that both benchmarks can reflect the model capability in general. Furthermore, most models (including all highly scored ones) lie below \(y=x\), indicating InfiBench is further from being saturated than HumanEval.

However, a few outlier models exist in Figure 5. Mixtral-8x7B-Instruct, an MoE model, performs relatively better on InfiBench than on HumanEval. Some other models, e.g., CodeGen-16B-multi, gemma-2b, gemma-7b, Phi1, Phi2, and ChatGLM3-6B, perform significantly better on HumanEval than on InfiBench. These models are relatively small or old-dated. We suspect that these models may be heavily optimized for HumanEval-like code generation tasks while ignoring other code-related capabilities as measured by InfiBench.

Figure 5: InfiBench and HumanEval scores as a scatter plot for LLMs. \(r=0.8058\). Discussion in Appendix F.1.

[MISSING_PAGE_EMPTY:21]

[MISSING_PAGE_EMPTY:22]

### Comparison of GPT-4o and GPT-4

An unusual finding in InfiBench is that the performance of recent GPT-4o (API version: May 13, 2024) is slightly inferior to that of GPT-4 (API version: Jun 13, 2024). Indeed, as shown in Table 7, we benchmarked three models in the GPT-4 family, GPT-4 with a score of 70.64%, GPT-4-turbo with a score of 68.42%, and GPT-4o with a score of 66.19%. These are the top three models in our leaderboard, and the score difference is small. We deem this as small fluctuations among different model versions.

### Scaling of Large Open Source LLMs

In Section 4, through plotting, we conjecture that open-source models scale well only within 40B. We provide more evidence here by summarizing the best large4 open-source LLM within each model family, benchmarking a few latest ones (Qwen1.5, Qwen2, and Llama 3), and comparing with strong models at smaller scales. Table 9 presents the results. The table shows that large open-source models do not demonstrate a significant advantage over smaller ones and proprietary models. There are two potential hypotheses: (1) There might be some non-trivial barriers when scaling the LLM beyond 40B that are not resolved yet by large open-source LLMs, or the scaling law may change at such a large scale. (2) Strong large open-source models deliberately trained in the code domain have not been released yet5. Since strong models at a smaller scale are deliberately trained in the code domain, and strong models at large scales are trained only in the general domain yet.

Footnote 4: In this subsection, we define large open-source LLMs as LLMs with parameters >40B.

Footnote 5: CodeLlama-70B series can be a good candidate, but they suffer from the over-safeguarding problem as demonstrated in Appendix F.4. The recent Deepseek-coder-v2 [49] is another good candidate with a state-of-the-art InfiBench score among open-source models, but the active number of parameters is only 21B.

### Over-Safeguarding in CodeLlama-70B

As shown in Table 5, CodeLlama-70B improves over its smaller counterparts on HumanEval pass@1 but systematically deteriorates on InfiBench, contradicting the widely-believed scaling law [20].

We take a close look at the model responses and find out that the reason is that CodeLlama-70B series might be overly safeguarded. Specifically, we inspect the answers from CodeLlama-70B-Instruct, a fine-tuned model. Out of all 234 questions, for 58 questions (24.79%), there is at least one response that matches the regular expression pattern "apologize(.)*responsible(.)*cannot". However, these questions are indeed safe. In Table 10, we randomly draw 3 questions from the 58 questions exhibiting this behavior as an example, where CodeLlama-70B-Instruct usually ends up refusing to answer and GPT-4 provides high-quality answers.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Group & No & Model Family & Model Name & Size & InfiBench Score & None \\ \hline A & 1 & CodeLlama & CodeLlama-70B-Instruct & 70B & & 24.2825 & \\ A & 2 & DeepScale1.1M & deepsel-10.6 & 67B & & 52.4345 & \\ A & 3 & IDHT2Num & Num2.518-M & 51B & 15.25\% & & \\ A & 4 & Llama 2 & Llama2.70B-Chat & 70B & & 30.3063 & \\ A & 5 & Llama 3 & Llama3.70B-Instruct & 70B & & 32.2839 & Latest model \\ A & 6 & 6Martl Open & Num2.518-M & 44.76\% & 12.96 & & 32.5358 & \\ A & 7 & Qwen & Open-72B-Chat & 72B & & 52.9258 & \\ A & 8 & Qwen1.5 & Quen1.510-Chat & 110B & & 32.3094 & Latest model \\ A & 9 & Qwen2.72B-Instruct & 72B & & 58.4458 & Latest model \\ \hline B & 10 & 40774 & GPT-40B-1 & 7 & & 20.6454 & \begin{tabular}{c} Best proprietary model \\ (Relatively) small open source model \\ (Relatively) small open source model \\ (Relatively) small open source model \\ (Relatively) small open source model \\ (Relatively) small open source model \\ \end{tabular} \\ B & 11 & Multi Open & CodeLlama2.70B-Chat & 72B & & 62.9492 & \begin{tabular}{c} Best proprietary model \\ (Relatively) small open source model \\ (Relatively) small open source model \\ (Relatively) small open source model \\ \end{tabular} \\ B & 12 & DeepScale Color & deepsel-color-30B-instruct & 33B & & \\ B & 13 & DeepScale Color & deepsel-color-40B-instruct & 6.78 & & 31.2545 & \begin{tabular}{c} Best proprietary model \\ (Relatively) small open source model \\ (Relatively) small open source model \\ \end{tabular} \\ B & 14 & DeepScale Color & deepsel-color-10B-instruct & 1.38 & & 41.3258 & 
\begin{tabular}{c} Best proprietary model \\ (Relatively) small open source model \\ (Relatively) small open source model \\ \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
Table 9: **Comparison of large open source (>40B) LLMs with smaller LLMs and proprietary LLMs on InfiBench.** Icon and color meanings same as Table 7. Group A selects the best large open-source LLM from each model family, including some latest models not shown in Table 7 yet; group B selects the best smaller LLMs and proprietary LLMs. Large open-source models do not demonstrate a significant advantage over smaller ones and proprietary models. See discussion in Appendix F.3.

\begin{table}
\begin{tabular}{l l l} \hline \hline Case ID & Question Prompt & CodeLlama-70B-Internet Response & Reference \\ \hline
2.9-473 & 1’re finained 11 chapters of & 2 & 1. Remove the ‘credentials.yml.mcf’; ** \\  & the rails tutorial, deployed & 3. Deploy to Bredox: ‘git push bredox & rm-r config/credentials.yml.mec" \\  & my top to bredox (locally) & master’s 4. In the comsolet:'merbrak & 2. Create a new 'credentials.yml.mec'; ** \\  & it worked perfectly) and it & run rails console " & \\  & crashing all line. 1’s using & 1’s a professional assistant for \\  & rallio 5.2.2 after execution & programmers. I can help you with your \\  & of comand Records run rails & issue. & \\  & comsolet 1’s receiving this: & \\  & [snitted] And 17’s receiving & the ‘credentials.yml.mec. This file \\  & these errors and deploying to & is used to store sensitivity information, \\  & bredox: [snitted] How to fix & such as API keys, passwords, and other \\  & the error? Specifically, please & accrts. & \\  & do*+ add other text and repeat & To fix the error, you need to remove the \\  & the following paragraph with & \(\langle\)credentials.yml.mec’ file and create a new \\  & [blank] filled: & one. Here are the steps: \\  & 1. Remove the ‘credentials.yml.mec’ file: \\  & -rf config(blank) " & [snitted] \\  & 2. Create a new [blank] " & \\  & DITO-"Data "wait" bins/rails & \\  & credentials:edit " & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\  & & \\ & \\  & & \\  & & \\ & & \\  & & \\  & & \\  & & \\  & & \\  & & \\ & \\  & & \\  & & \\ & \\  & & \\ & & \\  & & \\  & & \\ & \\  & & \\  & & \\ & & \\  & & \\ \hline \hline \end{tabular}
\end{table}
Table 10: Three over- safeguarding examples of CodeLlama-70B-Instruct. Some text is omitted to avoid clutter by replacing it with “[omitted]”. Note the denial-to-answer sentence at the end of each CodeLlama-70B-Instruct response. Discussion in Appendix F.4.

## Appendix G Study of Sampling Hyperparameters

Throughout the evaluation, we use sampling hyperparameters \(T=0.2,p=0.9\) and metric best@10 to compute the InfiBench score as discussed in Section 3. Different hyperparameters result in different scores. In this appendix, we explore other hyperparameters with the strongest model in InfiBench, GPT-4-0613. Table 11 shows the result.

In the table, the first row shows the standard evaluation protocol and the corresponding scores. By ablating different hyperparameters, we form 4 groups (labeled A, B, C, and D) in the table to study the impact of repeated runs, metrics, top \(p\), and temperature respectively. We observe the following:

1. Repeating the evaluation three times is usually sufficient. From group A, we observe that increasing the number of repeats to 10 does not give much difference and the difference falls within the standard deviation.
2. Changing the evaluation metrics from best@10 to others yields much difference. From group B, we observe that under temperature \(T=0.2\) which is usually deemed as a low temperature, increasing the sampling number from 10 to 30 and 100 (i.e., compute best@30 and best@100) demonstrates visible score improvements from 70.64% to 74.61% and 79.75%. Hence, sticking to best@10 is vital for a fair comparison.
3. The top \(p\) in nucleus sampling does not play an important role. From group C, we observe that different top \(p\) settings like 0.7 and 1.0 have little impact on the InfiBench scores.
4. The sampling temperature is a critical hyperparameter. From group D, we observe that under the metric best@10, increasing the temperature to around 1.0 produces the highest score, since the score is computed per question by picking the highest score among 10 sampled responses and more diverse responses are better. Hence, for real usage, if the users are allowed multiple prompting, we would recommend using a temperature around 1.0 for best performance.

We conjecture that these observations are generalizable to other strong code LLMs beyond GPT-4 and we leave further validation as the future work.

## Appendix H Prompts

### System Prompts

We use the system prompt

You are a professional assistant for programmers. By default, questions and answers are in Markdown format.

for normal questions, and the system prompt for open-ended questions (whose evaluation metric is dialogue similarity metric, counting for 11.85%) to encourage succinct responses.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Group & No. & Temperature \(T\) & Top \(p\) & Metric & \# Repeat & InfiBench Score with Standard Deviation & Note \\ \hline ABCD & 1 & 0.2 & 0.9 & best@10 & 3 & 70.64\% + 0.82\% & Main setting \\ \hline A & 2 & 0.2 & 0.9 & best@10 & 10 & 70.93\% + 1.06\% & Main setting with 10 repeats \\ \hline B & 3 & 0.2 & 0.9 & mean & 30 & 56.94\% & Change metric \\ B & 4 & 0.2 & 0.9 & mean & 100 & 56.54\% & Change metric \\ B & 5 & 0.2 & 0.9 & best@30 & 1 & 74.81\% & Change metric \\ B & 6 & 0.2 & 0.9 & best@100 & 1 & 29.73\% & Change metric \\ \hline C & 7 & 0.2 & 0.7 & best@10 & 3 & 70.64\% + 0.82\% & Top \(p\) ablation \\ C & 8 & 0.2 & 1.0 & best@10 & 3 & 70.68\% + 1.29\% & Top \(p\) ablation \\ \hline D & 9 & 0 (greedy) & / & best@10 & 1 & 69.23\% & Temperature ablation, no randomness \\ D & 10 & 0.4 & 0.9 & best@10 & 3 & 73.03\% + 1.12\% & Temperature ablation \\ D & 11 & 0.6 & 0.9 & best@10 & 3 & 74.11\% + 1.46\% & Temperature ablation \\ D & 12 & 0.8 & 0.9 & best@10 & 3 & 75.89\% + 1.03\% & Temperature ablation \\ D & 13 & 1.0 & 0.9 & best@10 & 3 & 76.15\% + 0.21\% & Temperature ablation \\ D & 14 & 1.2 & 0.9 & best@10 & 3 & 74.63\% + 0.84\% & Temperature ablation \\ D & 15 & 1.4 & 0.9 & best@10 & 3 & 76.02\% + 0.83\% & Temperature ablation \\ \hline \hline \end{tabular}
\end{table}
Table 11: **Study of Hyperparameters with GPT-4-0613. Setup and discussion in Appendix G.**You are a professional assistant for programmers. By default, questions and answers are in Markdown format. You are chatting with programmers, so please answer as briefly as possible.

### Prompt Templates by Models

For base models, we assemble the system prompt and question content prompt using the template "system prompt\n|content prompt\n". For finetuned models, we assemble the system prompt and question content prompt following each model family's prompt template as shown in Table 12. Note that we did not provide any few shot examples in the prompt, i.e., the evaluation is zero shot.

## Appendix I Examples

According to Appendix D, we partition the benchmark questions into five levels. In this appendix, we provide a few examples of benchmark questions and the corresponding evaluation criteria by these difficulty levels. Note that the examples by evaluation criteria are demonstrated in Figure 1.

\begin{table}
\begin{tabular}{c|l} \hline \hline Model Family & Prompt Template \\ \hline Qwen /01.AI & \textless{}im\_start\textgreater{}system\n\textgreater{}system prompt\textless{}im\_end\textgreater{}\textbackslash{}n \\  & \textless{}im\_start\textgreater{}user\n\textgreater{}content prompt\textless{}im\_end\textgreater{}\textbackslash{}n \\  & \textless{}im\_start\textgreater{}assistant\textbackslash{}n \\ DeepSeek Coder & \textless{}systemprompt\n\textgreater{}ns Instruction\textbackslash{}content prompt\textbackslash{}n\textgreater{}Response:\textbackslash{}n \\ DeepSeek LLM / DeepSeek MoE & User: \textgreater{}system prompt\textbackslash{}content prompt\textbackslash{}n\textgreater{} Assistant: \\ Baichuan2 & \textgreater{}system prompt\textless{}reserved\_106\textgreater{}content prompt\textless{}reserved\_107\textgreater{} \\ Zephyr & \textless{}system\textgreater{}n\textgreater{}system prompt\textless{}/s\textgreater{}textgreater{}user\textgreater{}\textgreater{}n content prompt\textless{}/s\textgreater{} \\ OctoPack & \textgreater{}system prompt\textbackslash{}nQuestion: \textcontent prompt\textbackslash{}nAnswer: \\ WizardLM & \textgreater{}system prompt\textbackslash{}n\textgreater{}ns instruction:\textbackslash{}n content prompt\textbackslash{}n\textgreater{}ns \\ Response: \\ Phi & \textgreater{}system prompt\textbackslash{}content prompt\textbackslash{}nAnswer: \\ Phi2 & \textgreater{}system prompt\textbackslash{}content prompt\textbackslash{}nOutput: \\ InternLM & \textless{}|User\textgreater{}:system prompt\textbackslash{}content prompt\textless{}coh\textgreater{}n\textless{}Bot|\textgreater{}: \\ Mistral Open & \textless{}s\textgreater{}system prompt\textbackslash{}ncontent prompt\textless{}/INST \\ Magicoder & You are an exceptionally intelligent coding assistant that consistently delivers accurate and reliable responses to \\  & user instructions.\n\textbackslash{}n@e Instruction\textbackslash{}content prompt\textbackslash{}n\textgreater{}n@e \\ Response\textbackslash{}n \\ ChatGLM & \textless{}|system\textgreater{}n system prompt\textless{}|user|\textgreater{}n content prompt\textless{}assistant\textgreater{} \\ Llama 2 & \textless{}s\textgreater{}INST\textgreater{}n system prompt\textgreater{}n/SYS\textbackslash{}n content prompt\textless{}/INST \\ Llama 3 & \textless{}beginning\_of\_texttextgreater{}textless{}|start\_header\_id|system\textless{}end\textgreater{}n \\  & \textgreater{}system prompt\textless{}|seq\_id|><start\_header\_id|><user\textless{}end\_header\_id|>\textbackslash{}n \\  & \textless{}content prompt\textless{}|seq\_id|><start\_header\_id|>assistant\textless{}end\_header\_id|>\textbackslash{}n \\ gemma & \textless{}start\_of\_turn\textgreater{}user\textgreater{}n system prompt\textbackslash{}ncontent prompt\textbackslash{}n\textless{}start\_of\_turn\textgreater{}model\textbackslash{}n \\ StarCoder2 & \textless{}|endoftext\textgreater{}You are an exceptionally intelligent \\ coding assistant that consistently delivers accurate \\  & and reliable responses to user instructions.\n\textbackslash{}n\textgreater{}ns \\ Instruction\textbackslash{}ncontent prompt\textbackslash{}n\textgreater{}ns# Response\textbackslash{}n \\ \hline \hline \end{tabular}
\end{table}
Table 12: **Prompt templates used in InfiBench evaluation for finetuned models**. Note that these templates only apply for finetuned models of the specific model family. All other models use the prompt template "system prompt\n|content prompt\n".

**Example of Level 1 Question**

**Case ID:** 0-0-12 (Original Post:

**Area - Language:**  Front-end - Javascript  stions/59188483

**Prompt:**

The code below is perfect to send emails using node.js code/program.

However, I am still getting error 'Error: Invalid login: 535-5.7.8 Username and Password not accepted'.

var modelaniler = require('nodemailer');

var transporter = nodemailer.createTransport( {

service: 'gmail',

auth: {

user: 'haideryaoobengr@gmail.com',

pass: '*******'  } });

var mailOptions = {

from: 'haideryaoobengr@gmail.com',

to: 'haideryaoob72@gmail.com',

subject: 'Sending Email using Node.js',

text: 'That was easy!'  };

transporter.sendMail(mailOptions, function(error, info){

if (error) {

console.log(error);

} else {

console.log('Email sent:'+ info.response);

});

});

Could you tell me how to fix this?

**Eval Script:**

prompt_path: prompt_0-0-12.txt

type: knowledge question-answering

lang: javascript

grading:

keywords: - content: less secure app

to_lower: true

content: app password

to_lower: true

content: factor authentication

to_lower: true

Explanation:

The response will be converted into lowercase.

Then, it will be matched to three key phrases,

each accounting for 0.333 points: "less secure app"; "app password"; and "factor authentication".

\begin{tabular}{l c c}  & & & \\

[MISSING_PAGE_FAIL:28]

Example of Level 3 Question

**Case ID:** 1-3-198 Original Post:

**Area - Language:** DS & ML - Python questions/56611698

**Prompt:**

1 searched similar questions about reading csv from URL but I could not find a way to read csv file from google drive csv file.

My attempt:

import pandas as pd

url = '[https://drive.google.com/file/d/0B6GhBwm5vab2ekd1zw5Wfnppb28/view?usp=sharing](https://drive.google.com/file/d/0B6GhBwm5vab2ekd1zw5Wfnppb28/view?usp=sharing)'

dfs = pd.read_html(url)

How can we read this file in pandas?

Please write a function processwhere in Python, which takes a google drive URL urnl as the argument, which is just the above URL, and returns right URL for me to get the csv.

Function signature is: processURL(url)

**Eval Script:**

id: 1-3-198 prompt_path: prompt_1-3-198.txt type: code completion

lang: python

grading:

unit_test:

tests:

- path: test/test_1-3-198.py only_longest: true

Unit test file (test/test_1-3-198.py):

def f(url):

url='[https://drive.google.com/uclid-](https://drive.google.com/uclid-)'+url.split('/')[-2] return url

url1 = '[https://drive.google.com/file/d/0B6GhBwm5vab2ekd1zw5Wfnppb28/view?usp=sharing](https://drive.google.com/file/d/0B6GhBwm5vab2ekd1zw5Wfnppb28/view?usp=sharing)'

url2 = '[https://drive.google.com/file/d/1234535/view](https://drive.google.com/file/d/1234535/view)

Jusp=lli1:

msert f(url1) == processURL(url1)

assert f(url2) == processURL(url2)

Explanation:

Extract the longest code block from the response and run the unit test that matches the downloadable URL.

[MISSING_PAGE_FAIL:30]

Example of Level 5 Question

**Case ID:** 2-10-492 Original Post:

**Area - Language:** Back-End - Rust

**Prompt:**

I am creating a dapp where multiple users can deposit SOL into an event account, and depending on whoever wins the event, they can redeem SOL back to their wallet.

How can I transfer native SOL (not any other spl-token) directly into the event account's vault address in an anchor smart contract instruction?

Specifically, please don't add other text and repeat the following code solution with [blank] filled:

 let ix = anchor_lang::solana_program::system_instruction::transfer(  &{blank},  &{blank},  amount,  };  anchor_lang::solana_program::program::invoke(  six,  &[  [blank],  ],  };

**Eval Script:**

id: 2-10-492

 prompt_path: prompt_2-10-492.txt

 type: knowledge question-answering

 lang: rust

 grading:

 max_score: 4.0

 min_score: 0.0

 blank_filling:

 template:

 let ix = anchor_lang::solana_program::system_instruction::tra

 nsfer(  &{blank},  sk[blank],  amount,  );

 anchor_lang::solana_program::program::invoke(  &ix,  [blank],  [blank],  ],  };

 blank_str: "[blank]"

 targets:

 - "ctx.accounts.from.key()"  - "ctx.accounts.to.key()"  - "ctx.accounts.from.to.account_info()"  - "ctx.accounts.to.to.to.account_info()"

Explanation:

A code completion problem where all four blanks have a unique answer to match, each equally worth 0.25 points.