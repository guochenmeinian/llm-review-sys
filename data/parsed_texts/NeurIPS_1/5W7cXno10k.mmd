# Characteristic Circuits

 Zhongjie Yu

TU Darmstadt

Darmstadt, Germany &Martin Trapp

Aalto University

Espoo, Finland &Kristian Kersting

TU Darmstadt/Hessian.AI/DFKI

Darmstadt, Germany

{yu,kersting}@cs.tu-darmstadt.de, martin.trapp@aalto.fi

###### Abstract

In many real-world scenarios, it is crucial to be able to _reliably_ and _efficiently_ reason under uncertainty while capturing complex relationships in data. Probabilistic circuits (PCs), a prominent family of _tractable_ probabilistic models, offer a remedy to this challenge by composing simple, tractable distributions into a high-dimensional probability distribution. However, learning PCs on heterogeneous data is challenging and densities of some parametric distributions are not available in closed form, limiting their potential use. We introduce characteristic circuits (CCs), a family of tractable probabilistic models providing a unified formalization of distributions over heterogeneous data in the spectral domain. The one-to-one relationship between characteristic functions and probability measures enables us to learn high-dimensional distributions on heterogeneous data domains and facilitates efficient probabilistic inference even when no closed-form density function is available. We show that the structure and parameters of CCs can be learned efficiently from the data and find that CCs outperform state-of-the-art density estimators for heterogeneous data domains on common benchmark data sets.

## 1 Introduction

Probabilistic circuits (PCs [2]) have gained increasing attention in the machine learning community as a promising modelling family that renders many probabilistic inferences tractable with little compromise in their expressivity. Their beneficial properties have prompted many successful applications in density estimation [_e.g._, 20, 21, 22, 23] and in areas where probabilistic reasoning is key, for example, neuro-symbolic reasoning [1], certified fairness [23], or causality [24]. Moreover, recent works have explored ways of specifying PCs for more complex modelling scenarios, such as time series [20, 21, 22] or tractable representation of graphs [21].

However, while density estimation is at the very core of many machine learning techniques (_e.g._, approximate Bayesian inference [22]) and a fundamental tool in statistics to identify characteristics of the data such as \(k^{\text{th}}\) order moments or multimodality [23], even in the case of parametric families, densities are sometimes

Figure 1: Characteristic circuits provide a unified, tractable specification of joint continuous and discrete distributions in the spectral domain of probability measures.

not available in closed-form. For example, only special cases of \(\alpha\)-stable distributions provide closed-form densities (Nolan, 2013). Fortunately, there exists a one-to-one correspondence between probability measures and characteristic functions (Sasvari, 2013), which can be understood as the Fourier-Stieltjes transform of the probability measures, enabling the characterisation of any probability measure through its characteristic function. Henceforth, the characteristic function of probability measures has found wide applicability in statistics, ranging from its use as a non-parametric estimator through the empirical characteristic function (Feuerverger and Mureika, 1977) to estimate heavy-tailed data, _e.g._, through the family of \(\alpha\)-stable distributions (Nolan, 2013). However, even though the characteristic function has many beneficial properties, its application to encode high-dimensional data distributions and efficient computation of densities can be quite challenging (Nolan, 2013).

In this work, we bridge between the characteristic function of probability measures and PCs. We do so by examining PCs from a more general perspective, similar in spirit to their specifications as a summation over functions on a commutative semiring (Friesen and Domingos, 2016) or as a convex combination of product measures on product probability spaces (Trapp et al., 2020). Instead of defining the circuit over density functions, we propose to form the circuit over the _characteristic function_ of the respective probability measures, illustrated in Fig. 1. The resulting _characteristic circuits_ (CCs) are related to recent works, which define a circuit over probability generating polynomials to represent discrete probability distributions (Zhang et al., 2021), in that both approaches can be understood as transformation methods. The benefits of using the spectral domain are manifold: (i) _characteristic functions_ as the base enable a _unified view_ for discrete and continuous random variables, (ii) directly representing the _characteristic function_ allows learning distributions that _do not_ have closed-form expressions for their density, and (iii) the moment can be obtained efficiently by differentiating the circuit. When modelling heterogeneous data, standard PCs do not naturally lend themselves to a unified view of mixed data but treat discrete and continuous random variables (RVs) conceptually differently. The difference arises as PCs model heterogeneous data domains _after_ integration w.r.t. the base measure which, in the case of mixed domains, differs between discrete and continuous RVs. RVs distributed according to a singular distribution can typically not be represented in PCs at all. This dependence on the base measure is subtly embedded within PCs, resulting in challenges when it comes to learning these models in heterogeneous domains. In contrast, CCs provide a unified view compared to PCs by moving away from the dependence on the base measure, achieved by representing the distribution through its characteristic function, which is independent of the base measure.

In summary, our contributions are: (1) We propose characteristic circuits, a novel deep probabilistic model class representing the joint of discrete and continuous random variables through a unifying view in the spectral domain. (2) We show that characteristic circuits retain the tractability of PCs despite the change of domain and enable efficient computation of densities, marginals, and conditionals. (3) We derive parameter and structure learning for characteristic circuits and find that characteristic circuits outperform SOTA density estimators in the majority of tested benchmarks.1

Footnote 1: Source code is available at [https://github.com/ml-research/CharacteristicCircuits](https://github.com/ml-research/CharacteristicCircuits)

We proceed as follows. We start by discussing related work and review preliminaries on probabilistic circuits and characteristic functions. Consequently, we define our model _characteristic circuits_, discuss theoretical properties, and show how to learn the circuits' parameters and structure. We conclude by presenting an empirical evaluation and discussion of the new model class.

## 2 Related Work

**Characteristic functions** (CFs) were originally proposed as a tool in the study of limit theorems and afterwards developed with independent mathematical interest (Lukacs, 1972). The uniqueness between CFs and probability measures is recovered with Levy's inversion theorem (Sasvari, 2013). A popular application of the CF is in statistical tests (_e.g._, Eriksson and Koivunen, 2003; Su and White, 2007; Wang and Hong, 2018; Ke and Yin, 2019). In practice, the CF of a distribution is in most cases not easy to estimate, and in turn, the empirical characteristic function (ECF) is employed as an approximation to the CF (Feuerverger and Mureika, 1977). The ECF has been successfully applied in sequential data analysis tasks (Knight and Yu, 2002; Yu, 2004; Davis et al., 2021). When handling high dimensional data, multivariate CFs, and ECFs were proposed for modelling _e.g._ multivariate time series (Lee et al., 2022) and images (Ansari et al., 2020). Although a mass of work has been developed for the applications of CF, less attention has been paid to estimating the model quality of CF itself. Therefore, modelling the multivariate CF remains to be challenging.

**Probabilistic circuits** (PCs) are a unifying framework for tractable probabilistic models (Choi et al., 2020) that recently show their power in _e.g._ probabilistic density estimation (Dang et al., 2020; Di Mauro et al., 2021; Zhang et al., 2021), flexible inference (Shao et al., 2022), variational inference (Shih and Ermon, 2020), and sample generating (Peharz et al., 2020). When it comes to data containing both discrete and continuous values, a mixture of discrete and continuous random variables is employed in PCs. Molina et al. (2018) propose to model mixed data based on the Sum-Product Network (SPN) structure, casting the randomized dependency coefficient (Lopez-Paz et al., 2013) for independence test for hybrid domains and piece-wise polynomial as leaf distributions, resulting in Mixed Sum-Product Networks (MSPN). Furthermore, statistical data type and likelihood discovery have been made available with Automatic Bayesian Density Analysis (ABDA) (Vergari et al., 2019), which is a suitable tool for the analysis of mixed discrete and continuous tabular data. Moreover, Bayesian SPNs (Trapp et al., 2019) use a well-principled Bayesian framework for SPN structure learning, achieving competitive results in density estimation on heterogeneous data sets. The above-mentioned models try to handle the probability measure with either parametric density/mass functions or histograms, but yet could not offer a unified view of heterogeneous data. PCs will also fail to model leaves with distributions that do not have closed-form density expressions.

## 3 Preliminaries on Probabilistic Circuits and Characteristic Functions

Before introducing characteristic circuits, we recap probabilistic circuits and characteristic functions.

### Probabilistic Circuits (PCs)

PCs are tractable probabilistic models, structured as rooted directed acyclic graphs, where each _leaf_ node L represents a probability distribution over a univariate RV, each _sum_ node S models a mixture of its children, and each _product_ node P models a product distribution (assuming independence) of their children. A PC over a set of RVs \(\mathbf{X}\) can be viewed as a computational graph \(\mathcal{G}\) representing a tractable probability distribution over \(\mathbf{X}\), and the value obtained at the root node is the probability computed by the circuit. We refer to Choi et al. (2020) for more details.

Each node in \(\mathcal{G}\) is associated with a subset of \(\mathbf{X}\) called the scope of a node N and is denoted as \(\psi(\textsf{N})\). The scope of an inner node is the union of the scope of its children. Sum nodes compute a weighted sum of their children \(\textsf{S}=\sum_{\textsf{N}\in\text{ch}(\textsf{S})}w_{\textsf{S},\textsf{N}} \textsf{N}\), and product nodes compute the product of their children \(\textsf{P}=\prod_{\textsf{N}\in\text{ch}(\textsf{P})}\textsf{N}\), where \(\text{ch}(\cdot)\) denotes the children of a node. The weights \(w_{\textsf{S},\textsf{N}}\) are generally assumed to be non-negative and normalized (sum up to one) at each sum node. We also assume the PC to be smooth (complete) and decomposable (Darwiche, 2003), where smooth requires all children of a sum node having the same scope, and decomposable means all children of a product node having pairwise disjoint scopes.

### Characteristic Functions (CFs)

Characteristic functions provide a _unified view_ for discrete and continuous random variables through the Fourier-Stieltjes transform of their probability measures. Let \(\mathbf{X}\) be a \(d\)-dimensional random vector, the CF of \(\mathbf{X}\) for \(\mathbf{t}\in\mathbb{R}^{d}\) is given as:

\[\varphi_{\mathbf{X}}(\mathbf{t})=\mathbb{E}[\exp(\mathrm{i}\,\mathbf{t}^{\top}\,\mathbf{X})]= \int_{\mathbf{x}\in\mathbb{R}^{d}}\exp(\mathrm{i}\,\mathbf{t}^{\top}\,\mathbf{x})\,\mu_{ \mathbf{X}}(\mathrm{d}\mathbf{x}), \tag{1}\]

where \(\mu_{\mathbf{X}}\) is the distribution/probability measure of \(\mathbf{X}\). CFs have certain useful properties. We will briefly review those that are relevant for the remaining discussion: (i) \(\varphi_{X}(0)=1\) and \(|\varphi_{X}(t)|\leq 1\); (ii) for any two RVs \(X_{1}\), \(X_{2}\), both have the same distribution iff \(\varphi_{X_{1}}=\varphi_{X_{2}}\); (iii) if \(X\) has \(k\) moments, then \(\varphi_{X}\) is \(k\)-times differentiable; and (iv) two RVs \(X_{1},X_{2}\) are independent iff \(\varphi_{X_{1},X_{2}}(s,t)=\varphi_{X_{1}}(s)\varphi_{X_{2}}(t)\). We refer to Sasvari (2013) for a more detailed discussion of CFs and their properties.

**Theorem 3.1** (Levy's inversion theorem (Sasvari, 2013)).: _Let \(X\) be a real-valued random variable, \(\mu_{X}\) its probability measure, and \(\varphi_{X}\colon\mathbb{R}\to\mathbb{C}\) its characteristic function. Then for any \(a,b\in\mathbb{R}\)\(a<b\), we have that_

\[\lim_{T\to\infty}\frac{1}{2\pi}\int_{-T}^{T}\frac{\exp(-\mathrm{i}ta)-\exp(- \mathrm{i}tb)}{\mathrm{i}t}\varphi_{X}(t)\mathrm{d}t=\mu_{X}[(a,b)]+\frac{1}{2}( \mu_{X}(a)+\mu_{X}(b)) \tag{2}\]

_and, hence, \(\varphi_{X}\) uniquely determines \(\mu_{X}\)._

**Corollary**.: _If \(\int_{\mathbb{R}}|\varphi_{X}(t)|\mathrm{d}t<\infty\), then \(X\) has a continuous probability density function \(f_{x}\) given by_

\[f_{X}(x)=\frac{1}{2\pi}\int_{\mathbb{R}}\exp(-\mathrm{i}tx)\varphi_{X}(t) \mathrm{d}t. \tag{3}\]

Note that not every probability measure admits an analytical solution to Eq. (3), _e.g._, only special cases of the family of \(\alpha\)-stable distributions have a closed-form density function (Nolan, 2013), and numerical integration might be needed.

**Empirical Characteristic Function (ECF).** In many cases, a parametric form of the data distribution is not available and one needs to use a non-parametric estimator. The ECF (Feuerverger and Mureika, 1977; Cramer, 1999) is an unbiased and consistent non-parametric estimator of the population characteristic function. Given data \(\{\mathbf{x}_{j}\}_{j=1}^{n}\) the ECF is given by

\[\hat{\varphi}_{\mathbb{P}}(\mathbf{t})=\frac{1}{n}\sum_{j=1}^{n}\exp(\mathrm{i}\, \mathbf{t}^{\top}\,\mathbf{x}_{j}). \tag{4}\]

**Evaluation Metric.** To measure the distance between two distributions represented by their characteristic functions, the squared characteristic function distance (CFD) can be employed. The CFD between two distributions \(\mathbb{P}\) and \(\mathbb{Q}\) is defined as:

\[\text{CFD}^{2}_{\omega}(\mathbb{P},\mathbb{Q})=\int_{\mathbb{R}^{d}}\left| \varphi_{\mathbb{P}}(\mathbf{t})-\varphi_{\mathbb{Q}}(\mathbf{t})\right|^{2}\omega( \mathbf{t};\eta)\mathrm{d}\mathbf{t}, \tag{5}\]

where \(\omega(\mathbf{t};\eta)\) is a weighting function parameterized by \(\eta\) and guarantees the integral in Eq. (5) converge. When \(\omega(\mathbf{t};\eta)\) is a probability density function, Eq. (5) can be rewritten as:

\[\text{CFD}^{2}_{\omega}(\mathbb{P},\mathbb{Q})=\mathbb{E}_{\mathbf{t}\sim\omega( \mathbf{t};\eta)}\left[\left|\varphi_{\mathbb{P}}(\mathbf{t})-\varphi_{\mathbb{Q}}( \mathbf{t})\right|^{2}\right]. \tag{6}\]

Actually, using the uniqueness theorem of CFs, we have \(\text{CFD}_{\omega}(\mathbb{P},\mathbb{Q})=0\) iff \(\mathbb{P}=\mathbb{Q}\)(Sriperumbudur et al., 2010). Computing Eq. (6) is generally intractable, therefore, we use Monte-Carlo integration to approximate the expectation, resulting in \(\text{CFD}^{2}_{\omega}(\mathbb{P},\mathbb{Q})\approx\frac{1}{k}\sum_{j=1}^{k }\left|\varphi_{\mathbb{P}}(t_{j})-\varphi_{\mathbb{Q}}(t_{j})\right|^{2}\), where \(\{t_{1},\cdots,t_{k}\}\stackrel{{\mathrm{i}id.}}{{\sim}}\omega( \mathbf{t};\eta)\). We refer to Ansari et al. (2020) for a detailed discussion.

## 4 Characteristic Circuits

Now we have everything at hand to introduce characteristic circuits. We first give a recursive definition of CC, followed by devising each type of node in a CC. We then show CCs feature efficient computation of densities, and in the end, introduce how to learn a CC from data.

**Definition 4.1** (Characteristic Circuit).: _Let \(\mathbf{X}=\{X_{1},\ldots,X_{d}\}\) be a set of random variables. A characteristic circuit denoted as \(\mathcal{C}\) is a tuple consisting of a rooted directed acyclic graph \(\mathcal{G}\), a scope function \(\psi\colon\,\mathbb{V}(\mathcal{G})\to\mathcal{P}(\mathbf{X})\), parameterized by a set of graph parameters \(\theta_{\mathcal{G}}\). Nodes in \(\mathcal{G}\) are either sum (\(\mathsf{S}\)), product (\(\mathsf{P}\)), or leaf (\(\mathsf{L}\)) nodes. With this, a characteristic circuit is defined recursively as follows:_

1. _a characteristic function for a scalar random variable is a characteristic circuit._
2. _a product of characteristic circuits is a characteristic circuit._
3. _a convex combination of characteristic circuits is a characteristic circuit._

Let us now provide some more details. To this end, we denote with \(\varphi_{\mathcal{C}}(\mathbf{t})\) the output of \(\mathcal{C}\) computed at the root of \(\mathcal{C}\), which represents the estimation of characteristic function given argument of the characteristic function \(\mathbf{t}\in\mathbb{R}^{d}\). Further, we denote the number of RVs in the scope of \(\mathsf{N}\) as \(|\psi(\mathbf{N})|\) and use \(\varphi_{\mathbf{N}}(\mathbf{t})\) for the characteristic function of a node. Throughout the paper, we assume the CC to be smooth and decomposable.

**Product Nodes.** A product node in a CC encodes the independence of its children. Let \(X\) and \(Y\) be two RVs. Following property (iv) of characteristic functions, the characteristic function of \(X,Y\) is given as \(\varphi_{X,Y}(t,s)=\varphi_{X}(t)\varphi_{Y}(s)\), if and only if \(X\) and \(Y\) are independent. Therefore, by definition, the characteristic function of product nodes is given as:

\[\varphi_{\mathsf{P}}(\mathbf{t})=\prod\nolimits_{\mathbf{N}\in\operatorname{ch}( \mathsf{P})}\varphi_{\mathbf{N}}(\mathbf{t}_{\psi(\mathbf{N})}), \tag{7}\]

where \(\mathbf{t}=\bigcup\nolimits_{\mathbf{N}\in\operatorname{ch}(\mathsf{P})}\mathbf{t}_{ \psi(\mathbf{N})}\).

**Sum Nodes.** A sum node in a CC encodes the mixture of its children. Let the parameters of \(\mathsf{S}\) be given as \(\sum\nolimits_{\mathbf{N}\in\operatorname{ch}(\mathsf{S})}w_{\mathsf{S}, \mathbf{N}}=1\) and \(w_{\mathsf{S},\mathbf{N}}\geq 0,\forall\mathsf{S},\mathbf{N}\). Then the sum node in a CC is given as: \(\varphi_{\mathsf{S}}(\mathbf{t})=\int_{\mathbf{x}\in\mathbb{R}^{d}}\exp(\mathrm{i}\mathbf{ t}^{\top}\mathbf{x})\left[\sum\nolimits_{\mathbf{N}\in\operatorname{ch}( \mathsf{S})}w_{\mathsf{S},\mathbf{N}}\,\mu_{\mathbf{N}}(\mathrm{d}\mathbf{x}) \right]=\sum\nolimits_{\mathbf{N}\in\operatorname{ch}(\mathsf{S})}w_{\mathsf{S },\mathbf{N}}\underbrace{\int_{\mathbf{x}\in\mathbb{R}^{\mathsf{PS}}}\exp(\mathrm{ i}\mathbf{t}^{\top}\mathbf{x})\,\mu_{\mathbf{N}}(\mathrm{d}\mathbf{x})}_{=\varphi_{ \mathbf{N}}(\mathbf{t})}.\) (8)

**Leaf Nodes.** A leaf node of a CC models the characteristic function of a univariate RV. To handle various data types, we propose the following variants of leaf nodes.

_ECF leaf._ The most straightforward way for modelling the leaf node is to directly employ the empirical characteristic function for the local data at each leaf, defined as \(\varphi_{\mathsf{L}_{\text{ECF}}}(t)=\frac{1}{n}\sum_{j=1}^{n}\exp(\mathrm{i} \,t\,x_{j}),\) where \(n\) is the number of instances at the leaf \(\mathsf{L}\), and \(x_{j}\) is the \(j^{th}\) instance. The ECF leaf is non-parametric and is determined by the \(n\) instances \(x_{j}\) at the leaf.

_Parametric leaf for continuous RVs._ Motivated by existing SPN literature, we can assume that the RV at a leaf node follows a parametric continuous distribution _e.g._ normal distribution. With this, the leaf node is equipped with the CF of normal distribution \(\varphi_{\mathsf{L}_{\text{Normal}}}(t)=\exp(\mathrm{i}\,t\,\mu-\frac{1}{2} \sigma^{2}t^{2}),\) where parameters \(\mu\) and \(\sigma^{2}\) are the mean and variance.

_Parametric leaf for discrete RVs._ For discrete RVs, if it is assumed to follow categorical distribution (\(P(X=j)=p_{j}\)), then the CF at the leaf node can be defined as \(\varphi_{\mathsf{L}_{\text{categorical}}}(t)=\mathbb{E}[\exp(\mathrm{i}\,t\,x)] =\sum_{j=1}^{k}p_{j}\exp(\mathrm{i}\,t\,j).\) Other discrete distributions which are widely used in probabilistic circuits can also be employed as leaf nodes in CCs, _e.g._, Bernoulli, Poisson, and geometric distributions.

\(\alpha\)_-stable leaf._ In the case of financial data or data distributed with heavy tails, the \(\alpha\)-stable distribution is frequently employed. \(\alpha\)-stable distributions are more flexible in modelling _e.g._ data with skewed centered distributions. The characteristic function of an \(\alpha\)-stable distribution is \(\varphi_{\mathsf{L}_{\alpha\text{-stable}}}(t)=\exp(\mathrm{i}\,t\,\mu-|ct|^{ \alpha}\,(1-\mathrm{i}\beta\text{sgn}(t)\Phi)),\) where \(\text{sgn}(t)\) takes the sign of \(t\) and \(\Phi=\left\{\begin{array}{cc}\tan(\pi\alpha/2)&\alpha\neq 1\\ -2/\pi\log|t|&\alpha=1\end{array}\right.\). The parameters in \(\alpha\)-stable distributions are the stability parameter \(\alpha\), the skewness parameter \(\beta\), the scale parameter \(c\), and the location parameter \(\mu\). Despite its modelling power, \(\alpha\)-stable distribution is never employed in PCs, as it is represented analytically by its CF and in most cases does not have a closed-form probability density function.

### Theoretic Properties of Characteristic Circuits

With the CC defined above, we can now derive the densities, marginals, and moments from it.

#### 4.1.1 Efficient computation of densities

Through their recursive nature, CCs enable efficient computation of densities in high-dimensional settings even if the density function is not available in closed form. For this, we present an extension of Theorem 3.1 for CCs, formulated using the notion of induced trees \(\mathcal{T}\)(Zhao et al., 2016). A detailed definition of induced trees can be found in Appendix A.2.

**Lemma 4.2** (Inversion).: _Let \(\mathcal{C}=\langle\mathcal{G},\psi,\theta_{\mathcal{G}}\rangle\) be a characteristic circuit on RVs \(\mathbf{X}=\{X_{j}\}_{j=1}^{d}\) with univariate leave nodes. If \(\int_{\mathbb{R}}|\varphi_{\mathsf{L}}(t)|\mathrm{d}t<\infty\) for every \(\mathsf{L}\in V(\mathcal{G})\), then \(\mathbf{X}\) has a continuous probability density function \(f_{\mathbf{x}}\) given by \(f_{\mathbf{X}}(\mathbf{x})=\)_

\[\frac{1}{(2\pi)^{d}}\sum_{i=1}^{\tau}\prod_{(\mathsf{S},\mathsf{N})\in\mathsf{E}( \mathcal{T}_{i})}\!\!\!w_{\mathsf{S},\mathsf{N}}\prod_{\mathsf{L}\in\mathsf{V}( \mathcal{T}_{i})}\int_{\mathbb{R}}\exp(-\mathrm{i}tx_{\psi(\mathsf{L})})\varphi _{\mathsf{L}}(t)\,\mathrm{d}t, \tag{9}\]

_and can be computed efficiently through analytic or numerical integration at the leaves._

Proof.: Let \(\mathcal{C}=\langle\mathcal{G},\psi,\theta_{\mathcal{G}}\rangle\) be a characteristic circuit on RVs \(\mathbf{X}=\{X_{j}\}_{j=1}^{d}\) with univariate leave nodes and \(p_{\mathsf{N}}\) the number of RVs in the scope of \(\mathsf{N}\). In order to calculate the density function of \(\mathcal{C}\), we need to integrate over the \(d\)-dimensional real space \(\mathbb{R}^{d}\), _i.e._,

\[f_{\mathbf{X}}(\mathbf{x})=\frac{1}{(2\pi)^{d}}\underbrace{\int_{\mathbf{t}\in\mathbb{R}^ {d}}\exp(-\mathrm{i}\,\mathbf{t}^{\top}\,\mathbf{x})\,\varphi_{\mathcal{C}}(\mathbf{t})\, \lambda_{d}(\mathrm{d}\mathbf{t})}_{=\hat{f}_{\mathcal{C}}(\mathbf{x})}, \tag{10}\]

where \(\varphi_{\mathcal{C}}(\mathbf{t})\) denotes the CF defined by the root of the characteristic circuit and \(\lambda_{d}\) is the Lebesque measure on \((\mathbb{R}^{d},\mathcal{B}(\mathbb{R}^{d}))\). We can examine the computation of Eq. (10) recursively for every node.

**Leaf Nodes.** If \(\mathsf{N}\) is a leaf node \(\mathsf{L}\), we obtain \(\hat{f}_{\mathsf{N}}(\cdot)\) by calculating:

\[\hat{f}_{\mathsf{L}}(x)=2\pi f_{\mathsf{L}}(x)=\int_{\mathbb{R}}\exp(- \mathrm{i}tx)\varphi_{X}(t)\lambda(\mathrm{d}t), \tag{11}\]

which follows from Theorem 3.1.

**Sum Nodes.** If \(\mathsf{N}\) is a sum node \(\mathsf{S}\), then:

\[\hat{f}_{\mathsf{S}}(\mathbf{x})=\int_{\mathbf{t}\in\mathbb{R}^{p}}\!\!\!\exp(- \mathrm{i}\,\mathbf{t}^{\top}\,\mathbf{x})\,\varphi_{\mathsf{S}}(\mathbf{t})\,\lambda_{p} (\mathrm{d}\mathbf{t})=\sum_{\mathsf{N}\in\mathsf{ch}(\mathsf{S})}w_{\mathsf{S}, \mathsf{N}}\underbrace{\int_{\mathbf{t}\in\mathbb{R}^{p_{\mathsf{S}}}}\!\!\!\exp( -\mathrm{i}\,\mathbf{t}^{\top}\,\mathbf{x})\,\varphi_{\mathsf{N}}(\mathbf{t})\,\lambda_{p_ {\mathsf{S}}}(\mathrm{d}\mathbf{t})}_{=\hat{f}_{\mathsf{N}}(\mathbf{x})}. \tag{12}\]

Therefore, computing the inverse for \(\mathsf{S}\) reduces to inversion at its children.

**Product Nodes.** If \(\mathsf{N}\) is a product node \(\mathsf{P}\), then:

\[\hat{f}_{\mathsf{P}}(\mathbf{x})=\int_{t\in\mathbb{R}^{p_{\mathsf{N}}}}\!\!\!\exp( -\mathrm{i}\,\mathbf{t}^{\top}\,\mathbf{x})\,\varphi_{\mathsf{P}}(t)\,\lambda_{p_{ \mathsf{P}}}(\mathrm{d}\mathbf{t})=\prod_{\mathsf{N}\in\mathsf{ch}(\mathsf{P})} \underbrace{\int_{\mathbf{s}\in\mathbb{R}^{p_{\mathsf{N}}}}\!\!\!\exp(-\mathrm{i} \,\mathbf{s}^{\top}\,\mathbf{x}_{[\psi(\mathsf{N})]})\,\varphi_{\mathsf{N}}(\mathbf{s})\, \lambda_{p_{\mathsf{N}}}(\mathrm{d}\mathbf{s})}_{=\hat{f}_{\mathsf{N}}(\mathbf{x}_{[ \psi(\mathsf{N})]})}, \tag{13}\]

where we used that \(\lambda_{p_{\mathsf{P}}}=\otimes_{\mathsf{N}\in\mathsf{ch}(\mathsf{P})}\, \lambda_{p_{\mathsf{N}}}\) is a product measure on a product space, applied Fubini's theorem (Fubini, 1907), and used the additivity property of exponential functions. Consequently, computing the inverse for \(\mathsf{P}\) reduces to inversion at its children.

Through the recursive application of Eq. (12) and Eq. (13), we obtain that Eq. (10) reduces to integration at the leaves and, therefore, can be solved either analytically or efficiently through one-dimensional numerical integration. 

#### 4.1.2 Efficient computation of marginals

Similar to PCs over distribution functions, CCs allow efficient computation of arbitrary marginals. Given a CC on RVs \(\mathbf{Z}=\mathbf{X}\cup\mathbf{Y}\), we can obtain the marginal CC of \(\mathbf{X}\) as follows. Let \(n=|\mathbf{X}|\), \(m=|\mathbf{Y}|\) and let the characteristic function of the circuit be given by

\[\varphi_{\mathcal{C}}(t_{1},\ldots,t_{n},t_{n+1},\ldots,t_{n+m})=\int_{\mathbf{z} \in\mathbb{R}^{n+m}}\!\!\!\exp(\mathrm{i}\mathbf{t}^{\top}\mathbf{z})\mu_{\mathsf{S}} (\mathrm{d}\mathbf{z}), \tag{14}\]

where \(\mu_{\mathsf{S}}\) denotes the distribution of the root. Then the marginal CC of \(\mathbf{X}\) is given by setting \(t_{j}=0\), \(n<j\leq n+m\). The proof of marginal computation is provided in Appendix B.1.

#### 4.1.3 Efficiently computing moments via differentiation

Characteristic circuits also allow efficient computation of moments of distributions. Let \(k\in\mathbb{N}^{+}\) be such that the partial derivative \(\frac{\partial^{dk}\varphi_{\mathcal{C}}(\mathbf{t})}{\partial t_{1}^{k}\cdots \partial t_{d}^{k}}\) exists, then the moment \(\mathbb{M}_{k}\) exists and can be computed efficiently through the derivative at the leaves

\[\mathbb{M}_{k}=\mathrm{i}^{-dk}\frac{\partial^{dk}\varphi_{\mathcal{C}}( \mathbf{t})}{\partial t_{1}^{k}\cdots\partial t_{d}^{k}}\bigg{|}_{t_{1}=0, \ldots,t_{d}=0}=\mathrm{i}^{-dk}\sum_{i=1}^{\tau}\prod_{(\mathsf{S},\mathsf{N}) \in\mathsf{E}(\mathcal{T}_{i})}w_{\mathsf{S},\mathsf{N}}\prod_{\mathsf{L}\in \mathsf{V}(\mathcal{T}_{i})}\frac{\mathrm{d}^{k}\varphi_{\mathsf{L}}(t_{\psi( \mathsf{L})})}{\mathrm{d}t_{\psi(\mathsf{L})}^{k}}\bigg{|}_{t_{\psi(\mathsf{L}) }=0}. \tag{15}\]

A detailed proof can be found in Appendix B.2.

### Learning Characteristic Circuits from Data

To learn a characteristic circuit from data there are several options. The first option is _parameter learning using a random circuit structure_. The random structure is initialized by recursively creating mixtures with random weights for sum nodes and randomly splitting the scopes for product nodes. A leaf node is created with randomly initialized parameters when there is only one scope in a node. Maximising the likelihood at the root of a CC requires one to apply the inversion theorem to the CC for each training data. When a leaf node does not have a closed-form density function, numerical integration could be used to obtain the density value given data, which makes the maximum likelihood estimation (MLE) at the root not guaranteed to be tractable.

As discussed in prior works, see _e.g._Yu (2004), minimising a distance function to the ECF is most related to moment-matching approaches, but can result in more accurate fitting results. Therefore, minimising the CFD to the ECF can be beneficial if no tractable form of the likelihood exists but evaluating the characteristic function is tractable. In this case, instead of maximising the likelihood from CC, which is not guaranteed to be tractable, we take the ECF from data as an anchor and minimise the CFD between the CC and ECF:

\[\frac{1}{k}\sum\nolimits_{j=1}^{k}\left|\frac{1}{n}\sum\nolimits_{i=1}^{n} \exp(\mathrm{i}\mathbf{t}_{j}^{\top}\mathbf{x}_{i})-\varphi_{\mathcal{C}}(\mathbf{t}_ {j})\right|^{2}. \tag{16}\]

Applying Sedrakyan's inequality Sedrakyan (1997) to Eq. (16), parameter learning can be operated batch-wise:

\[\frac{1}{k}\sum\limits_{j=1}^{k}\left|\frac{1}{b}\sum\limits_{l=1}^{b}\frac{1 }{n_{b}}\sum\limits_{i=1}^{n_{b}}\exp(\mathrm{i}\mathbf{t}_{j}^{\top}\mathbf{x}_{ i})-\varphi_{\mathcal{C}}(\mathbf{t}_{j})\right|^{2}\leq\frac{1}{k}\sum\limits_{j=1}^{k} \frac{1}{b}\sum\limits_{l=1}^{b}\left|\frac{1}{n_{b}}\sum\limits_{i=1}^{n_{b}} \exp(\mathrm{i}\mathbf{t}_{j}^{\top}\mathbf{x}_{i})-\varphi_{\mathcal{C}}(\mathbf{t}_{ j})\right|^{2},\]

where \(b\) is the number of batches and \(n_{b}\) the batch size. This way, parameter learning of \(\mathbf{C}\) is similar to training a neural network. Furthermore, if two CCs are compatible, as similarly defined for PCs Vergari et al. (2021), the CFD between the two CCs can be calculated analytically, see Appendix D for more details.

Figure 2: Illustration of the recursive structure learning algorithm. Sum nodes are the result of clustering, having weighted children that are product nodes. Product nodes are the result of independence test, enforcing independence assumptions of their children. Leaf nodes are univariate characteristic functions modelling local data.

However, relying on randomly initialized structures (_e.g._, due to the fixed split of scopes) may also limit the performance of parameter learning of CC. To overcome this, we derive now a _structure learning_ algorithm to learn the structure of the CC. Inspired by Gens and Domingos (2013), this structure learning recursively splits the data slice and creates sum and product nodes of the CC as summarized in Algorithm 1 and depicted in Fig. 2. To create a sum node \(\mathcal{S}\), clustering algorithms, _e.g._, K-means clustering, is employed to split data instances in the slice into \(k_{\text{S}}\) subsets. The weights of the sum node are then determined by the portion of data instances in each subset. To create a product node \(\mathsf{P}\), some independence tests--_e.g._, G-test of independence or random dependency coefficient (RDC) based splitting (Molina et al., 2018)--are used to decide on splitting the random variables in the data slice into \(k_{\text{P}}\) sub-groups. The sum and product nodes are created recursively until any of the following conditions fulfils: (1) There is only one scope in the data slice, and then a leaf node with the corresponding scope is created. (2) The number of data instances in the data slice is smaller than a pre-defined threshold \(min\_k\). In the latter case, a naive factorization is applied to the scopes in the data slice to create a product node, and then create leaves for each scope as children for this product node. When creating a leaf node, the leaf parameters are estimated by MLE if the closed-form density function is available. In the case of ECF leaves, the leaf nodes are created from local data following the definition of ECF in Eq. (4). When there is no closed-form density at a leaf, the parameters of an \(\alpha\)-stable distribution are estimated using the algorithm in McCulloch (1986).

## 5 Experimental Evaluation

Our intention here is to evaluate the performance of characteristic circuit on synthetic data sets and UCI data sets, consisting of heterogeneous data. The likelihoods were computed based on the inversion theorem. For discrete and Gaussian leaves, the likelihoods were computed analytically. For \(\alpha\)-stable leaves, the likelihoods were computed via numerical integration using the Gauss-Hermit quadrature of degree 50.

**Can characteristic circuits approximate known distributions well?** We begin by describing and evaluating the performance of CC on two synthetic data sets. The first data set consisted of data generated from a mixture of multivariate distributions (denoted as MM): \(p(\mathbf{x})=\sum_{i=1}^{K}w_{i}p(\mathbf{x}_{1}\mid\mu_{i},\sigma_{i}^{2})p(\mathbf{x}_ {2}\mid\mathbf{p}_{i})\), where \(p(\mathbf{x}\mid\mu,\sigma^{2})\) is the univariate normal distribution with mean \(\mu\) and variance \(\sigma^{2}\), and \(p(\mathbf{x}\mid\mathbf{p})\) is the univariate categorical distribution with \(\mathbf{p}\) the vector of probability of seeing each element. In our experiments we set \(K=2\) and \(w_{1}=0.3\), \(w_{2}=0.7\). For each univariate distribution we set \(\mu_{1}=0\), \(\sigma_{1}^{2}=1\), \(\mu_{2}=5\), \(\sigma_{2}^{2}=1\), \(\mathbf{p}_{1}=[0.6,0.4,0.0]\) and \(\mathbf{p}_{2}=[0.1,0.2,0.7]\). The second data set consisted of data generated from a Bayesian network with 5 nodes (denoted as BN), to test the modelling power of characteristic circuits with more RVs and more complex correlations among each RV. The details of the BN are depicted in Fig. 3. Here, \(\mathbf{X}_{1}\), \(\mathbf{X}_{2}\), \(\mathbf{X}_{3}\) and \(\mathbf{X}_{5}\) are binary random variables parameterized by \(\mathbf{p}_{i}\), and \(\mathbf{X}_{4}\) is a continuous random variable conditioned on \(\mathbf{X}_{3}\). For both data sets MM and BN, 800 instances were generated for training and \(800\) for testing.

We first employed parameter learning and evaluated the log-likelihoods from the random structure and after parameter learning. A detailed setting of parameter learning is illustrated in Appendix C.1. The increase of log-likelihoods after parameter learning (columns 2 and 3 in Table 1) implies that

\begin{table}
\begin{tabular}{l|c c|c c c} \hline \hline Data Set & \begin{tabular}{c} Random \\ Structure \\ \end{tabular} & \begin{tabular}{c} Random Structure \\ \end{tabular} & \begin{tabular}{c} Structure \\ Learning \\ \end{tabular} & \begin{tabular}{c} Structure Learning \\ \end{tabular} & \begin{tabular}{c} Structure Learning \\ \end{tabular} & 
\begin{tabular}{c} Structure Learning (random \(\mathbf{w}\)) \\ \end{tabular} \\ \hline MM & -4.93 & -3.50 & -2.87 & -2.86 & -3.34 \\ BN & -6.30 & -4.12 & -3.27 & -3.27 & -3.93 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Average test log-likelihoods from CC after parameter learning by minimising the CFD on synthetic data sets. The CC structure is either generated using Random Structure or learned using the Structure Learning algorithm.

Figure 3: The Bayesian network used for BN.

minimising the CFD pushes CC to better approximate the true distribution of data. We then learnt CCs using structure learning with \(min\_k=100\), and \(k_{\text{S}}=k_{\text{P}}=2\). Various leaf types were evaluated: CC with ECF as leaves (CC-E), CC with normal distribution for continuous RVs and categorical distributions for discrete RVs, _i.e._, parametric leaves (CC-P), and CC with normal distribution for all leaf nodes (CC-N). The trained CCs were evaluated with the CFD between the CC and the ground truth CF. For data set BN, the ground truth CF was derived via the algorithm for generating arithmetic circuits that compute network polynomials (Darwiche, 2003). Following Chwialkowski et al. (2015) and Ansari et al. (2020), we illustrate both the CFD with varying scale \(\eta\) in \(\omega(\mathbf{t};\eta)\) and also optimising \(\eta\) for the largest CFD, shown in Fig. 4. We report average CFD values and standard deviations obtained from five runs. It can be seen from Fig. 4 that both CC-E and CC-P have almost equally lower CFD values and also lower maximum CFD values compared to the ECF, which indicates the characteristic circuit structure better encodes the data distribution than the ECF. The smaller standard deviation values from characteristic circuits compared with ECF also imply that characteristic circuits offer a more stable estimate of the characteristic function from data. For data set MM, the maximum CFD of CC-N is \(0.0270\) when \(\log(\sigma)=0.6735\), which is far more than \(0.0006\) of CC-P, and thus not visualized in Fig. 4 (Left). This also happens to data set BN, as can be seen in Fig. 4 (Right) that CC-N gives higher CFD than CC-P and CC-E, which implies that assuming a discrete RV as Normal distributed is not a suitable choice for CC. In addition, parameter learning on CCs from structure learning and structure learning with randomized parameters (last 2 columns in Table 1) provides higher log-likelihoods than random structures, which implies a well-initialized structure improves parameter learning. To conclude, CC estimates the data distribution better than ECF, which is justified by the smaller CFD from CC-E compared with ECF.

**Can characteristic circuits be better density estimators on heterogeneous data?** Real-world tabular data usually contain both discrete and real-valued elements, and thus are in most cases heterogeneous. Therefore, we also conducted density estimation experiments on real-world heterogeneous data sets and compared to state-of-the-art probabilistic circuit methods, including Mixed SPNs (MSPN) (Molina et al., 2018), Automatic Bayesian Density Analysis (ABDA) (Vergari et al., 2019) and Bayesian SPNs (BSPN) (Trapp et al., 2019). We employed the heterogeneous data from the UCI data sets, see Molina et al. (2018) and Vergari et al. (2019) for more details on the data sets. Similar to the setup in Trapp et al. (2019), a random variable was treated as categorical if less than 20 unique states of that RV were in the training set. All the rest RVs were modelled with either normal distributions (CC-P) or \(\alpha\)-stable distributions (CC-A). Again, we first employed parameter learning with a (fixed) random structure using \(\alpha\)-stable distribution for continuous RVs and report the log-likelihoods (Parameter Learning in Table 2). Note that \(\alpha\)-stable distributions can not be represented with closed-form densities, thus maximising the likelihood from it can not be solved exactly and efficiently. As a comparison, structure learning was also employed with \(min\_k=100\), \(k_{\text{S}}=k_{\text{P}}=2\) for G-test based splitting (CC-P & CC-A), and with \(min\_k=100\), \(k_{\text{S}}=2\) for RDC based splitting (CC-A\({}^{\text{RDC}}\)). A detailed description of the experimental settings can be found in Appendix C.2. The test log-likelihoods are presented in Table 2. As one can see, parameter learning performs worse than CC-A but still outperforms some of the baselines. CC-P does not win on all the data sets but is competitive with MSPN and ABDA on most of the data sets. CC-A outperforms the baselines on 8 out of 12 data sets, and CC-A\({}^{\text{RDC}}\) outperforms all the other methods

Figure 4: Characteristic circuits approximate the true distributions better than the ECF by providing a smaller CFD. We visualize the CFD for CC with parametric leaves (CC-P ), ECF as leaves (CC-E ), normal distribution as leaves (CC-N ) and a single empirical characteristic function (ECF ) learned from synthetic heterogeneous data (Left: MM, Right: BN). Best viewed in color.

on 9 out of 12 data sets. This implies that characteristic circuit, especially with structure learning, is a competitive density estimator compared with SOTA PCs. Actually, \(\alpha\)-stable leaf distributions are a more suitable choice for characteristic circuits on heterogeneous tabular data.

## 6 Conclusion

We introduced characteristic circuits (CCs), a novel circuit-based characteristic function estimator that leverages an arithmetic circuit with univariate characteristic function leaves for modelling the joint of heterogeneous data distributions. Compared to existing PCs, characteristic circuits model the characteristic function of data distribution in the continuous spectral domain, providing a unified view for discrete and continuous random variables, and can further model distributions that do not have closed-form probability density functions. We showed that both joint and marginal probability densities can be computed exactly and efficiently using characteristic circuits. Finally, we empirically showed that characteristic circuits approximate the data distribution better than ECF, measured by the squared characteristic function distance, and that characteristic circuits can also be competitive density estimators as they win on 9 out of 12 heterogeneous data sets compared to SOTA models.

There are several avenues for future work. For instance, sampling from characteristic functions and, in turn, characteristic circuits is not straightforward. One should explore existing literature discussing sampling from CFs (Devroye, 1986; Ridout, 2009; Walker, 2017), and adapt them to sampling from CCs. The circuit structure of characteristic circuits generated by structure learning has a high impact on the performance of the characteristic circuit, and therefore an inappropriate structure can limit the modelling power of characteristic circuits. Therefore, one should explore parameter learning of characteristic circuits on more advanced circuit structures (Peharz et al., 2020) and, in particular, using normalizing flows, resulting in what could be called characteristic flows.

**Broader Impact.** Our contributions are broadly aimed at improving probabilistic modelling. CCs could be used to develop more scalable and more accurate probabilistic models, in particular over mixed domains as common in economics, social science, or medicine. Scaling to even bigger mixed models can open up even more potential applications, but also may require careful design to handle overconfidence and failures of CC.

## Acknowledgments and Disclosure of Funding

This work was supported by the Federal Ministry of Education and Research (BMBF) Competence Center for AI and Labour ("KompAKI", FKZ 02L19C150). It benefited from the Hessian Ministry of Higher Education, Research, Science and the Arts (HMWK; projects "The Third Wave of AI" and "The Adaptive Mind"), and the Hessian research priority program LOEWE within the project "WhiteBox". MT acknowledges funding from the Academy of Finland (grant number 347279).

\begin{table}
\begin{tabular}{l|r|r r r|r r r} \hline \multirow{2}{*}{Data Set} & Parameter & \multirow{2}{*}{MSPN} & \multirow{2}{*}{ABDA} & \multirow{2}{*}{BSPN} & \multicolumn{3}{c}{Structure Learning} \\ \cline{4-9}  & & & & & CC-P & CC-A & CC-A\({}^{\text{RHC}}\) \\ \hline Abalone & 3.06 & 9.73 & 2.22 & 3.92 & 4.27 & 15.10 & **17.75** \\ Adult & -14.47 & -44.07 & -5.91 & -4.62 & -31.37 & -7.76 & **-1.43** \\ Australian & -5.59 & -36.14 & -16.44 & -21.51 & -30.29 & -3.26 & **-2.94** \\ Autism & -27.80 & -39.20 & -27.93 & **-0.47** & -34.71 & -17.52 & -15.5 \\ Breast & -20.39 & -28.01 & -25.48 & -25.02 & -54.75 & -13.41 & **-12.36** \\ Chess & -13.33 & -13.01 & -12.30 & **-11.54** & -13.04 & -13.04 & -12.40 \\ Crx & -6.82 & -36.26 & -12.82 & -19.38 & -32.63 & -4.72 & **-3.19** \\ Dermatology & -45.54 & -27.71 & -24.98 & -23.95 & -30.34 & -24.92 & **-23.58** \\ Diabetes & -1.49 & -31.22 & -17.48 & -21.21 & -23.01 & **0.63** & 0.27 \\ German & -19.54 & -26.05 & -25.83 & -26.76 & -27.29 & -15.24 & **-15.02** \\ Student & -33.13 & -30.18 & -28.73 & -29.51 & -31.59 & -27.92 & **-26.99** \\ Wine & 0.32 & -0.13 & -10.12 & -8.62 & -6.92 & 13.34 & **13.36** \\ \hline \# best & 0 & 0 & 0 & 2 & 0 & 1 & 9 \\ \hline \end{tabular}
\end{table}
Table 2: Average test log-likelihoods from CC and SOTA algorithms on heterogeneous data.

## References

* Ahmed et al. (2022) Kareem Ahmed, Stefano Teso, Kai-Wei Chang, Guy Van den Broeck, and Antonio Vergari. Semantic probabilistic layers for neuro-symbolic learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 35, pages 29944-29959, 2022.
* Ansari et al. (2020) Abdul Fatir Ansari, Jonathan Scarlett, and Harold Soh. A characteristic function approach to deep implicit generative modeling. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 7478-7487, 2020.
* Choi et al. (2020) YooJung Choi, Antonio Vergari, and Guy Van den Broeck. Probabilistic circuits: A unifying framework for tractable probabilistic models. Technical report, UCLA, 2020.
* Chwialkowski et al. (2015) Kacper P Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, and Arthur Gretton. Fast two-sample testing with analytic representations of probability measures. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 28, 2015.
* Correia et al. (2023) Alvaro HC Correia, Gennaro Gala, Erik Quaeghebeur, Cassio de Campos, and Robert Peharz. Continuous mixtures of tractable probabilistic models. In _AAAI Conference on Artificial Intelligence_, volume 37, pages 7244-7252, 2023.
* Cramer (1999) Harald Cramer. _Mathematical methods of statistics_, volume 26. Princeton university press, 1999.
* Dang et al. (2020) Meihua Dang, Antonio Vergari, and Guy Broeck. Strudel: Learning structured-decomposable probabilistic circuits. In _International Conference on Probabilistic Graphical Models (PGM)_, pages 137-148, 2020.
* Dang et al. (2022) Meihua Dang, Antonio Vergari, and Guy Van den Broeck. Strudel: A fast and accurate learner of structured-decomposable probabilistic circuits. _International Journal of Approximate Reasoning_, 140:92-115, 2022.
* Darwiche (2003) Adnan Darwiche. A differential approach to inference in bayesian networks. _Journal of the ACM (JACM)_, 50(3):280-305, 2003.
* Davis et al. (2021) Richard A Davis, Thiago do Rego Sousa, and Claudia Kluppelberg. Indirect inference for time series using the empirical characteristic function and control variates. _Journal of Time Series Analysis_, 42(5-6):653-684, 2021.
* Devroye (1986) Luc Devroye. An automatic method for generating random variates with a given characteristic function. _SIAM journal on applied mathematics_, 46(4):698-719, 1986.
* Mauro et al. (2021) Nicola Di Mauro, Gennaro Gala, Marco Iannotta, and Teresa MA Basile. Random probabilistic circuits. In _Uncertainty in Artificial Intelligence (UAI)_, pages 1682-1691, 2021.
* Eriksson and Koivunen (2003) Jan Eriksson and Visa Koivunen. Characteristic-function-based independent component analysis. _Signal Processing_, 83(10):2195-2208, 2003.
* Feuerverger and Mureika (1977) Andrey Feuerverger and Roman A Mureika. The empirical characteristic function and its applications. _The annals of Statistics_, pages 88-97, 1977.
* Friesen and Domingos (2016) Abram Friesen and Pedro Domingos. The sum-product theorem: A foundation for learning tractable models. In _International Conference on Machine Learning (ICML)_, pages 1909-1918, 2016.
* Fubini (1907) Guido Fubini. Sugli integrali multipli. In _Rendiconti_, volume 16, pages 608-614. Accademia Nazionale dei Lincei, 1907.
* Gens and Domingos (2013) Robert Gens and Pedro Domingos. Learning the structure of sum-product networks. In _International Conference on Machine Learning (ICML)_, pages 873-880, 2013.
* Ke and Yin (2019) Chenlu Ke and Xiangrong Yin. Expected conditional characteristic function-based measures for testing independence. _Journal of the American Statistical Association_, 2019.
* Knight and Yu (2002) John L Knight and Jun Yu. Empirical characteristic function in time series estimation. _Econometric Theory_, 18(3):691-721, 2002.
* Knight et al. (2018)Sangyeol Lee, Simos G Meintanis, and Charl Pretorius. Monitoring procedures for strict stationarity based on the multivariate characteristic function. _Journal of Multivariate Analysis_, 189:104892, 2022.
* Lopez-Paz et al. (2013) David Lopez-Paz, Philipp Hennig, and Bernhard Scholkopf. The randomized dependence coefficient. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 26, 2013.
* Lukacs (1972) Eugene Lukacs. A survey of the theory of characteristic functions. _Advances in Applied Probability_, 4(1):1-37, 1972.
* McCulloch (1986) J Huston McCulloch. Simple consistent estimators of stable distribution parameters. _Communications in statistics-simulation and computation_, 15(4):1109-1136, 1986.
* Molina et al. (2018) Alejandro Molina, Antonio Vergari, Nicola Di Mauro, Sriraam Natarajan, Floriana Esposito, and Kristian Kersting. Mixed sum-product networks: A deep architecture for hybrid domains. In _AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* Murphy (2012) Kevin P Murphy. _Machine learning: a probabilistic perspective_. MIT press, 2012.
* Nolan (2013) John P Nolan. Multivariate elliptically contoured stable distributions: theory and estimation. _Computational statistics_, 28:2067-2089, 2013.
* Peharz et al. (2020) Robert Peharz, Steven Lang, Antonio Vergari, Karl Stelzner, Alejandro Molina, Martin Trapp, Guy Van den Broeck, Kristian Kersting, and Zoubin Ghahramani. Einsum networks: Fast and scalable learning of tractable probabilistic circuits. In _International Conference on Machine Learning (ICML)_, pages 7563-7574, 2020.
* Ridout (2009) Martin S Ridout. Generating random numbers from a distribution specified by its laplace transform. _Statistics and Computing_, 19:439-450, 2009.
* Sasvari (2013) Zoltan Sasvari. _Multivariate characteristic and correlation functions_, volume 50. Walter de Gruyter, 2013.
* Sedrakyan (1997) Nairi Sedrakyan. About the applications of one useful inequality. _Kvant Journal_, 97(2):42-44, 1997.
* Selvam et al. (2023) Nikil Roashan Selvam, Guy Van den Broeck, and YooJung Choi. Certifying fairness of probabilistic circuits. In _AAAI Conference on Artificial Intelligence_, volume 37, pages 12278-12286, 2023.
* Shao et al. (2022) Xiaoting Shao, Alejandro Molina, Antonio Vergari, Karl Stelzner, Robert Peharz, Thomas Liebig, and Kristian Kersting. Conditional sum-product networks: Modular probabilistic circuits via gate functions. _International Journal of Approximate Reasoning_, 140:298-313, 2022.
* Shih and Ermon (2020) Andy Shih and Stefano Ermon. Probabilistic circuits for variational inference in discrete graphical models. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 33, pages 4635-4646, 2020.
* Silverman (2018) Bernard W Silverman. _Density estimation for statistics and data analysis_. Routledge, 2018.
* Sriperumbudur et al. (2010) Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Scholkopf, and Gert RG Lanckriet. Hilbert space embeddings and metrics on probability measures. _The Journal of Machine Learning Research_, 11:1517-1561, 2010.
* Su and White (2007) Liangjun Su and Halbert White. A consistent characteristic function-based test for conditional independence. _Journal of Econometrics_, 141(2):807-834, 2007.
* Trapp et al. (2019) Martin Trapp, Robert Peharz, Hong Ge, Franz Pernkopf, and Zoubin Ghahramani. Bayesian learning of sum-product networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 32, 2019.
* Trapp et al. (2020) Martin Trapp, Robert Peharz, Franz Pernkopf, and Carl Edward Rasmussen. Deep structured mixtures of gaussian processes. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 2251-2261, 2020.
* Trapp et al. (2019)Antonio Vergari, Alejandro Molina, Robert Peharz, Zoubin Ghahramani, Kristian Kersting, and Isabel Valera. Automatic bayesian density analysis. In _AAAI Conference on Artificial Intelligence_, volume 33, pages 5207-5215, 2019.
* Vergari et al. (2021) Antonio Vergari, YooJung Choi, Anji Liu, Stefano Teso, and Guy Van den Broeck. A compositional atlas of tractable circuit operations for probabilistic inference. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 34, pages 13189-13201, 2021.
* Walker (2017) Stephen G Walker. A laplace transform inversion method for probability distribution functions. _Statistics and Computing_, 27(2):439-448, 2017.
* Wang et al. (2022) Benjie Wang, Matthew R Wicker, and Marta Kwiatkowska. Tractable uncertainty for structure learning. In _International Conference on Machine Learning (ICML)_, pages 23131-23150, 2022.
* Wang & Hong (2018) Xia Wang and Yongmiao Hong. Characteristic function based testing for conditional independence: A nonparametric regression approach. _Econometric Theory_, 34(4):815-849, 2018.
* Yu (2004) Jun Yu. Empirical characteristic function estimation and its applications. _Econometric reviews_, 2004.
* Yu et al. (2021a) Zhongjie Yu, Fabrizio Ventola, and Kristian Kersting. Whittle networks: A deep likelihood model for time series. In _International Conference on Machine Learning (ICML)_, pages 12177-12186, 2021a.
* Yu et al. (2021b) Zhongjie Yu, Mingye Zhu, Martin Trapp, Arseny Skryagin, and Kristian Kersting. Leveraging probabilistic circuits for nonparametric multi-output regression. In _Uncertainty in Artificial Intelligence (UAI)_, pages 2008-2018, 2021b.
* Zecevic et al. (2021) Matej Zecevic, Devendra Singh Dhami, Athresh Karanam, Sriraam Natarajan, and Kristian Kersting. Interventional sum-product networks: Causal inference with tractable probabilistic models. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 34, pages 15019-15031, 2021.
* Zhang et al. (2021) Honghua Zhang, Brendan Juba, and Guy Van den Broeck. Probabilistic generating circuits. In _International Conference on Machine Learning (ICML)_, pages 12447-12457, 2021.
* Zhao et al. (2016) Han Zhao, Tameem Adel, Geoff Gordon, and Brandon Amos. Collapsed variational inference for sum-product networks. In _International Conference on Machine Learning (ICML)_, pages 1310-1318, 2016.