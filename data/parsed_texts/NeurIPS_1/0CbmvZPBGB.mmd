# Partial Label Learning with Dissimilarity Propagation guided Candidate Label Shrinkage

 Yuheng Jia\({}^{1,3}\), **Fuchao Yang\({}^{2}\), Yongqiang Dong\({}^{1}\)**

\({}^{1}\) School of Computer Science and Engineering, Southeast University, Nanjing 210096, China

\({}^{2}\) College of Software Engineering, Southeast University Nanjing 210096, China

\({}^{3}\) Key Laboratory of New Generation Artificial Intelligence Technology and Its

Interdisciplinary Applications (Southeast University), Ministry of Education, China

yhjia@seu.edu.cn, yangfc@seu.edu.cn, dongyq@seu.edu.cn

Corresponding author

###### Abstract

In partial label learning (PLL), each sample is associated with a group of candidate labels, among which only one label is correct. The key of PLL is to disambiguate the candidate label set to find the ground-truth label. To this end, we first construct a constrained regression model to capture the confidence of the candidate labels, and multiply the label confidence matrix by its transpose to build a second-order similarity matrix, whose elements indicate the pairwise similarity relationships of samples globally. Then we develop a semantic dissimilarity matrix by considering the complement of the intersection of the candidate label set, and further propagate the initial dissimilarity relationships to the whole data set by leveraging the local geometric structure of samples. The similarity and dissimilarity matrices form an _adversarial_ relationship, which is further utilized to shrink the solution space of the label confidence matrix and promote the dissimilarity matrix. We finally extend the proposed model to a kernel version to exploit the non-linear structure of samples and solve the proposed model by the inexact augmented Lagrange multiplier method. By exploiting the adversarial prior, the proposed method can significantly outperform state-of-the-art PLL algorithms when evaluated on 10 artificial and 7 real-world partial label data sets. We also prove the effectiveness of our method with some theoretical guarantees. The code is publicly available at [https://github.com/Yangfc-ML/DPCLS](https://github.com/Yangfc-ML/DPCLS).

## 1 Introduction

Partial label learning (PLL) [14, 15, 17] is an emerging weakly supervised learning framework. In PLL, each sample is associated with a set of candidate labels, among which only one is the ground-truth label. Different from conventional supervised learning, PLL avoids precisely annotating label on each sample, which greatly reduces the labeling cost. Accordingly, PLL has been applied to many real-world scenarios such as automatic image annotation [1, 20], web mining [9], and ecoinformatics [8].

Formally speaking, let \(\mathcal{X}=\mathbb{R}^{d}\) be the \(d\)-dimensional feature space and \(\mathcal{Y}=\{1,2,...,q\}\) be the label space with \(q\) labels. Given the partial label training set \(\mathcal{D}=\{(x_{i},S_{i})\mid 1\leq i\leq m\}\), where \(x_{i}\in\mathcal{X}\) is a \(d\)-dimensional feature vector to represent the \(i\)-th sample and \(S_{i}\subseteq\mathcal{Y}\) is the associated candidate label set, among which only one label is correct. PLL aims to induce a multi-class classifier \(f:\mathcal{X}\rightarrow\mathcal{Y}\) from \(\mathcal{D}\), which is very challenging as the ground-truth label of a sample is concealed in the candidate label set.

The key to solving PLL is label disambiguation, i.e., identifying the ground-truth label of a sample from its candidate label set. For example, [6; 12] considered the ground-truth label as a latent variable and identified it through an iterative refining procedure. [3] narrowed the candidate label set through a sparsity-based self-training procedure. Some works [19; 22; 23] leveraged the similarity relationship of samples in the feature space to disambiguation, i.e., samples are similar to each other in the feature space are likely to share the same ground-truth label. Recently, some researches exploited the label space to achieve label disambiguation [2; 7; 13]. For example, SDIM [2] first built a pairwise dissimilarity matrix through the candidate label sets, and then maximized the difference of the label confidence between two samples if their pairwise dissimilarity between them is large according to the constructed dissimilarity matrix. However, the dissimilarity matrix constructed by SDIM is predefined and relatively sparse, which depresses its effectiveness.

Realizing the effectiveness of the dissimilarity relationship in PLL, we propose a novel PLL method named DPCLS (partial label learning with Dissimilarity Propagation guided Candidate Label Shrinkage). Specifically, we first construct a semantic dissimilarity matrix by considering the complement of the intersection of the candidate label set, i.e., if two samples do not share any common candidate labels, they must belong to different classes, and their semantic dissimilarity is large. The above constructed dissimilarity relationships are still sparse and fixed, we therefore propose to propagate the initial dissimilarity relationships to the whole data set by leveraging the local geometric structure of samples, i.e., if two samples are similar to each other in feature space, they are expected to share the similar dissimilarity codings. Second, to include the enhanced semantic dissimilarity matrix in label disambiguation, we design a second-order similarity matrix by multiplying the label confidence matrix with its transpose. Under the ideal condition, the enhanced semantic dissimilarity matrix and the similarity matrix naturally form an adversarial relationship, i.e., a larger (resp. smaller) dissimilarity between two samples means a smaller (resp. larger) similarity between them. By exploiting this adversarial prior, the enhanced dissimilarity matrix can shrink the solution space of the label confidence matrix, and meanwhile, the similarity matrix induced from the label confidence matrix also contributes to build a better dissimilarity matrix. We theoretically confirm the above statement under some general conditions. Furthermore, we extend our method to a kernel version to model the non-linear structure of samples. The proposed model is finally formulated as a constrained regression problem with adversarial learning and graph regularization, which is optimized by inexact augmented Lagrange multiplier (IALM). Extensive experiments on artificial and real-world partial label data sets demonstrate the effectiveness of the proposed PLL method.

## 2 The Proposed Method

**Basic Model**

Let \(\mathbf{X}=[x_{1},x_{2},\cdots,x_{m}]^{\mathsf{T}}\in\mathbb{R}^{m\times d}\) denote the feature matrix, where \(m\) and \(d\) represent the number of samples and the dimension of features. \(\mathbf{Y}=[y_{1},y_{2},\cdots,y_{m}]^{\mathsf{T}}\in\{0,1\}^{m\times q}\) represents the partial label matrix, where \(q\) is the number of classes. Moreover, \(y_{ij}=1\) indicates that the \(j\)-th label is one of the candidate labels of the sample \(x_{i}\). Note that in the candidate label set, only one label is correct.

To fulfill PLL, we first build the following constrained regression model

\[\begin{split}&\min_{\mathbf{W},\mathbf{F}}\|\mathbf{X}\mathbf{W}- \mathbf{F}\|_{F}^{2}+\lambda\|\mathbf{W}\|_{F}^{2}\\ &\text{s.t. }\mathbf{F}\mathbf{1}_{q}=\mathbf{1}_{m},\mathbf{0}_{m \times q}\leq\mathbf{F}\leq\mathbf{Y},\end{split} \tag{1}\]

where \(\mathbf{F}\in\mathbb{R}^{m\times q}\) is the label confidence matrix with \(\mathbf{F}_{ij}\) representing the probability of the \(j\)-th label being the ground-truth label for the \(i\)-th sample. \(\mathbf{W}\in\mathbb{R}^{d\times q}\) is the coefficient matrix that maps the feature matrix to the label confidence matrix \(\mathbf{F}\). To avoid over-fitting, we impose the widely-used squared Frobenius norm on \(\mathbf{W}\) as the regularization term, which is introduced by \(\lambda>0\). \(\mathbf{1}_{q}\in\mathbb{R}^{q}\) and \(\mathbf{1}_{m}\in\mathbb{R}^{m}\) are two all ones vectors, \(\mathbf{0}_{m\times q}\in\mathbb{R}^{m\times q}\) is an all zeroes matrix. The first constraint \(\mathbf{F}\mathbf{1}_{q}=\mathbf{1}_{m}\) normalizes the label confidence vector for all samples. The second constraint \(\mathbf{0}_{m\times q}\leq\mathbf{F}\leq\mathbf{Y}\) means each element of \(\mathbf{F}\) is no less than 0, and no more than the corresponding element of \(\mathbf{Y}\), which implies the ground-truth label of each sample must reside in the candidate label set and the label confidence of the non-candidate labels must be 0. We initialize the label confidence matrix as \(\mathbf{F}_{ij}=1/\sum_{j}y_{ij}\) if \(y_{ij}\)=1, otherwise \(\mathbf{F}_{ij}=0\).

By minimizing Eq. (1), we construct a linear regression model that maps the feature space to the label confidence \(\mathbf{F}\). We assume that the mapping from the features to the ground-truth label may be easier, while that to the false-positive label residing in the candidate label set is relatively harder. Accordingly, optimizing Eq. (1) will help disambiguate the candidate labels and produce a preliminary label confidence matrix by exploring the useful information in the feature space.

**Dissimilarity Propagation guided Candidate Label Shrinkage**

To further exploit the valuable information in the label space, we first use candidate labels to construct a dissimilarity matrix \(\mathbf{D}_{0}\in\mathbb{R}^{m\times m}\), i.e.,

\[\mathbf{D}_{0ij}=\begin{cases}1,\text{ if }y_{i}y_{j}^{\mathsf{T}}=0\\ 0,\text{ otherwise.}\end{cases} \tag{2}\]

If \(y_{i}y_{j}^{\mathsf{T}}=0\), the \(i\)-th sample \(x_{i}\) and the \(j\)-th sample \(x_{j}\) do not share any common candidate labels, which means they must belong to the different classes. Otherwise, \(x_{i}\) and \(x_{j}\) have a chance to belong to the same class. Therefore, \(\mathbf{D}_{0}\) indicates the semantic dissimilarity of samples. We then multiply the label confidence matrix with its transpose to create a similarity matrix termed as \(\mathbf{FF}^{\mathsf{T}}\), whose \((i,j)\)-th element indicates the similarity between \(x_{i}\) and \(x_{j}\). _As the semantic dissimilarity matrix \(\mathbf{D}_{0}\) and similarity matrix \(\mathbf{FF}^{\mathsf{T}}\) form an adversarial relationship_, i.e., a larger (resp. smaller) element in \(\mathbf{D}_{0}\) implies a smaller (resp. larger) element in \(\mathbf{FF}^{\mathsf{T}}\), we use this adversarial prior to shrink the solution space of \(\mathbf{F}\) by:

\[\min_{\mathbf{F}}\left\|\mathbf{D}_{0}\odot\mathbf{FF}^{\mathsf{T}}\right\|_{ 1}, \tag{3}\]

where \(\odot\) denotes the element-wise product of two matrices and \(||\cdot||_{1}\) represents the \(l_{1}\) norm (i.e., the sum of absolute values of all elements in a matrix). Minimizing Eq. (3) ensues that the positive elements of \(\mathbf{D}_{0}\) and \(\mathbf{FF}^{\mathsf{T}}\) will lie in the different locations. Unfortunately, directly minimizing Eq. (3) cannot help produce a better label confidence matrix \(\mathbf{F}\), because \(\mathbf{D}_{0}\) is inferred from the candidate label set, and the \((i,j)\)-th element of \(\mathbf{D}_{0}\) is positive only when \(y_{i}y_{j}^{\mathsf{T}}=0\), while \(\mathbf{F}\) is upper bounded by \(\mathbf{Y}\), the \((i,j)\)-th element of \(\mathbf{FF}^{\mathsf{T}}\) is positive only when \(y_{i}y_{j}^{\mathsf{T}}\neq 0\). That is the locations of the positive elements of \(\mathbf{D}_{0}\) and \(\mathbf{FF}^{\mathsf{T}}\) are complementary.

To solve this problem, we propose to promote the initial dissimilarity matrix \(\mathbf{D}_{0}\) to produce a denser dissimilarity matrix \(\mathbf{D}\in\mathbb{R}^{m\times m}\) by a novel dissimilarity propagation method. Specifically, we leverage the local geometric structure of samples to enhance \(\mathbf{D}_{0}\). Note that each column of \(\mathbf{D}\) (e.g., \(\mathbf{D}_{.i}\)) can represent the dissimilarity relationships between a sample (e.g., \(x_{i}\)) and the other samples. If two samples \(x_{i}\) and \(x_{j}\) are close to each other in the feature space, their dissimilarity relationships (\(\mathbf{D}_{.i}\) and \(\mathbf{D}_{.j}\)) should also be similar. To capture the feature similarity, we build a local geometric matrix \(\mathbf{S}\in\mathbb{R}^{m\times m}\) using a radial basis function (RBF) kernel:

\[\mathbf{S}_{ij}=\begin{cases}\text{exp}(-||x_{i}-x_{j}||_{2}^{2}/\sigma^{2}), \text{ if }j\in\mathcal{N}_{i}\\ 0,\text{ otherwise,}\end{cases} \tag{4}\]

where \(j\in\mathcal{N}_{i}\) indicates that \(x_{j}\) is a \(k\)-nearest neighbor of \(x_{i}\), and \(\sigma\) is a hyper-parameter controlling the bandwidth of the RBF kernel. Based on \(\mathbf{S}\), the dissimilarity propagation guided candidate label shrinkage module becomes

\[\begin{split}&\min_{\mathbf{F},\mathbf{D}}\left\|\mathbf{D} \odot\mathbf{FF}^{\mathsf{T}}\right\|_{1}+\sum_{i,j=1}^{m}\mathbf{S}_{ij}\left\| \mathbf{D}_{.i}-\mathbf{D}_{.j}\right\|_{2}^{2}\\ &\text{s.t. }\mathbf{0}_{m\times m}\leq\mathbf{D}\leq\mathbf{1}_{m \times m},\mathbf{D}_{ij}=\mathbf{D}_{0ij},\text{if }\mathbf{D}_{0ij}=1.\end{split} \tag{5}\]

The second term makes the highly similar samples in the feature samples share the similar dissimilarity codings. Moreover, \(\mathbf{D}_{ij}=\mathbf{D}_{0ij}\) if \(\mathbf{D}_{0ij}=1\) means the reliable semantic relationship should be retained in \(\mathbf{D}\). In order to make \(\mathbf{D}\) a well-defined dissimilarity matrix, each element in \(\mathbf{D}\) should lie in the range of [0,1]. Note that in Eq. (5), both \(\mathbf{F}\) and \(\mathbf{D}\) are optimization variables. By minimizing Eq. (5), the enhanced semantic dissimilarity matrix will shrink the solution space of \(\mathbf{F}\) and at the same time \(\mathbf{F}\) will also help promote the quality of \(\mathbf{D}\) by the adversarial term.

**Model Formulation**

Taking all the above considerations into account, the proposed model finally becomes:

\[\begin{split}&\min_{\mathbf{W},\mathbf{F},\mathbf{D}}\left\|\mathbf{XW }-\mathbf{F}\right\|_{F}^{2}+\lambda\left\|\mathbf{W}\right\|_{F}^{2}+\alpha \left\|\mathbf{D}\odot\mathbf{FF}^{\mathsf{T}}\right\|_{1}+\beta\text{Tr}( \mathbf{DLD}^{\mathsf{T}})\\ &\text{s.t. }\mathbf{F1}_{q}=\mathbf{1}_{m},\mathbf{0}_{m\times q}\leq \mathbf{F}\leq\mathbf{Y},\mathbf{0}_{m\times m}\leq\mathbf{D}\leq\mathbf{1}_{ m\times m},\mathbf{D}_{ij}=\mathbf{D}_{0ij},\text{if }\mathbf{D}_{0ij}=1,\end{split} \tag{6}\]

where \(\mathbf{L}\in\mathbb{R}^{m\times m}=\mathbf{D_{S}}-\mathbf{S}\) is a graph Laplacian matrix, and \(\mathbf{D_{S}}\in\mathbb{R}^{m\times m}\) is a diagonal matrix with the \(i\)-th diagonal element \(\mathbf{D_{S}}_{ii}=\sum_{j=1}^{m}\mathbf{S}_{ij}\). Tr(\(\cdot\)) returns the trace of a matrix. \(\alpha\), \(\beta\geq 0\) are two hyper-parameters to balance different terms. By solving Eq. (6), the semantic dissimilarity is enhanced by a dissimilarity propagation process, and further utilized to shrink the solution space of the label confidence matrix by a novel adversarial prior.

Optimization and Setting

We adopt IALM to solve the problem in Eq. (6). To simplify Eq. (6), we introduce an auxiliary matrix \(\mathbf{A}=\mathbf{D}\in\mathbb{R}^{m\times m}\) and equivalently rewrite it as

\[\begin{split}&\min_{\mathbf{W},\mathbf{F},\mathbf{D},\mathbf{A}}\left\| \mathbf{XW}-\mathbf{F}\right\|_{F}^{2}+\lambda\left\|\mathbf{W}\right\|_{F}^{2 }+\alpha\left\|\mathbf{A}\odot\mathbf{F}\mathbf{F}^{\mathsf{T}}\right\|_{1}+ \beta\mathrm{Tr}(\mathbf{D}\mathbf{L}\mathbf{D}^{\mathsf{T}})\\ &\text{s.t. }\mathbf{F}\mathbf{1}_{q}=\mathbf{1}_{m},\mathbf{0}_{m \times q}\leq\mathbf{F}\leq\mathbf{Y},\mathbf{D}=\mathbf{A},\mathbf{0}_{m \times m}\leq\mathbf{A}\leq\mathbf{1}_{m\times m},\mathbf{A}_{ij}=\mathbf{D}_ {0ij},\text{if }\mathbf{D}_{0ij}=1.\end{split} \tag{7}\]

The solution of Eq. (7) can be obtained by solving the following augmented Lagrange equation:

\[\begin{split}&\min_{\mathbf{W},\mathbf{F},\mathbf{D},\mathbf{A}} \left\|\mathbf{XW}-\mathbf{F}\right\|_{F}^{2}+\lambda\left\|\mathbf{W}\right\|_ {F}^{2}+\alpha\left\|\mathbf{A}\odot\mathbf{F}\mathbf{F}^{\mathsf{T}}\right\|_ {1}+\beta\mathrm{Tr}(\mathbf{D}\mathbf{L}\mathbf{D}^{\mathsf{T}})+\left\langle \mathbf{\Phi},\mathbf{D}-\mathbf{A}\right\rangle+\frac{\mu}{2}\left\|\mathbf{D} -\mathbf{A}\right\|_{F}^{2}\\ &\text{s.t. }\mathbf{F}\mathbf{1}_{q}=\mathbf{1}_{m},\mathbf{0}_{m \times q}\leq\mathbf{F}\leq\mathbf{Y},\mathbf{0}_{m\times m}\leq\mathbf{A}\leq \mathbf{1}_{m\times m},\mathbf{A}_{ij}=\mathbf{D}_{0ij},\text{if }\mathbf{D}_{0ij}=1,\end{split} \tag{8}\]

where \(\mathbf{\Phi}\in\mathbb{R}^{m\times m}\) is the Lagrange multiplier matrix, \(\mu\geq 0\) is a penalty parameter, and \(\left\langle\cdot,\cdot\right\rangle\) returns the inner product of two matrices. We can optimize Eq. (8) by solving the following subproblems alternatively and iteratively.

_I)_**W subproblem is formulated as

\[\min_{\mathbf{W}}\left\|\mathbf{XW}-\mathbf{F}\right\|_{F}^{2}+\lambda\left\| \mathbf{W}\right\|_{F}^{2}, \tag{9}\]

Kernel Extension

The linear mapping in Eq. (9) may fail to model the nonlinear relationship. Therefore, we extend the above model to a kernel-based non-linear version. Let \(\phi(\cdot):\mathbb{R}^{d}\rightarrow\mathbb{R}^{h}\) denote the feature transformation that maps the origin feature space \(\mathbf{X}\) to a higher dimensional Hilbert space \(\phi(\mathbf{X})\). According to the Representer Theorem, \(\mathbf{W}\) can be expressed as a linear combination of the input features, i.e., \(\mathbf{W}=\phi(\mathbf{X})^{\mathsf{T}}\mathbf{H}\), where \(\mathbf{H}\in\mathbb{R}^{m\times q}\) stores the combination weights. Then, we have \(\phi(\mathbf{X})\mathbf{W}=\mathbf{K}\mathbf{H}\), where \(\mathbf{K}=\phi(\mathbf{X})\phi(\mathbf{X})^{\mathsf{T}}\in\mathbb{R}^{m\times m}\) is the kernel matrix and each element \(\mathbf{K}_{ij}=\mathcal{K}(x_{i},x_{j})\). Finally, the nonlinear version is represented as:

\[\min_{\mathbf{H},\mathbf{b}}\left\|\mathbf{K}\mathbf{H}+\mathbf{1}_{m}\mathbf{ b}^{\mathsf{T}}-\mathbf{F}\right\|_{F}^{2}+\lambda\mathrm{Tr}(\mathbf{H}^{ \mathsf{T}}\mathbf{K}\mathbf{H}), \tag{10}\]

where \(\mathbf{b}\in\mathbb{R}^{q}\) is the bias term. When the first derivatives of \(\mathbf{H}\) and \(\mathbf{b}\) reach 0, Eq. (10) is solved, i.e.,

\[\mathbf{H}=\left(\mathbf{K}+\lambda\mathbf{I}_{m\times m}-\frac{\mathbf{1}_{m }\mathbf{1}_{m}^{\mathsf{T}}\mathbf{K}}{m}\right)^{-1}\left(\mathbf{F}-\frac {\mathbf{1}_{m}\mathbf{1}_{m}^{\mathsf{T}}\mathbf{F}}{m}\right),\mathbf{b}= \frac{1}{m}\left(\mathbf{F}^{\mathsf{T}}\mathbf{1}_{m}-\mathbf{H}^{\mathsf{T }}\mathbf{K}^{\mathsf{T}}\mathbf{1}_{m}\right), \tag{11}\]

where \(\mathbf{I}_{m\times m}\in\mathbb{R}^{m\times m}\) denotes an identity matrix. In the experiments, we use the RBF kernel as the kernel function, i.e., \(\mathcal{K}(x_{i},x_{j})=\text{exp}(-||x_{i}-x_{j}||_{2}^{2}/\sigma^{2})\), for our method and the compared ones.

_2)_**F subproblem is written as

\[\begin{split}&\min_{\mathbf{F}}\left\|\mathbf{F}-\mathbf{P}\right\|_ {F}^{2}+\alpha\left\|\mathbf{A}\odot\mathbf{F}\mathbf{F}^{\mathsf{T}}\right\| _{1}\\ &\text{s.t. }\mathbf{F}\mathbf{1}_{q}=\mathbf{1}_{m},\mathbf{0}_{m \times q}\leq\mathbf{F}\leq\mathbf{Y},\end{split} \tag{12}\]

where \(\mathbf{P}=\mathbf{K}\mathbf{H}+\mathbf{1}_{m}\mathbf{b}^{\mathsf{T}}\in \mathbb{R}^{m\times q}\) is the output matrix of the model. Eq. (12) can be formulated as a standard quadratic programming (QP) problem, and solved by any QP tools. The detailed derivation process can be found in **Section A** of the supplementary file.

_3)_**D subproblem is represented as

\[\min_{\mathbf{D}}\beta\mathrm{Tr}(\mathbf{D}\mathbf{L}\mathbf{D}^{\mathsf{T}} )+\frac{\mu}{2}\left\|\mathbf{D}-\mathbf{A}+\frac{\mathbf{\Phi}}{\mu}\right\| _{F}^{2}. \tag{13}\]

Eq. (13) reaches the minimum when its first-order derivative with respect to \(\mathbf{D}\) vanishes, leading to

\[\mathbf{D}=(\mu\mathbf{A}-\mathbf{\Phi})(2\beta\mathbf{L}+\mu\mathbf{I}_{m \times m})^{-1}. \tag{14}\]

_4)_**A** subproblem is expressed as

\[\begin{split}&\min_{\mathbf{A}}\alpha\left\|\mathbf{A}\odot\mathbf{F} \mathbf{F}^{\mathsf{T}}\right\|_{1}+\frac{\mu}{2}\left\|\mathbf{D}-\mathbf{A}+ \frac{\mathbf{\Phi}}{\mu}\right\|_{F}^{2}\\ &\text{s.t. }\mathbf{0}_{m\times m}\leq\mathbf{A}\leq\mathbf{1}_{m \times m},\mathbf{A}_{ij}=\mathbf{D}_{0ij},\text{if }\mathbf{D}_{0ij}=1.\end{split} \tag{15}\]

Eq. (15) can solved element-wisely, i.e.,

\[\mathbf{A}=\mathcal{T}\left(\mathcal{T}_{1}\left(\mathcal{T}_{0}\left(\frac{ \mu\mathbf{D}+\mathbf{\Phi}-\alpha\mathbf{F}\mathbf{F}^{\mathsf{T}}}{\mu} \right)\right)\right), \tag{16}\]where \(\mathcal{T}\), \(\mathcal{T}_{1}\), \(\mathcal{T}_{0}\) are three thresholding operators in elementwise, i.e., \(\mathcal{T}(\mathbf{C}_{ij})=1,\)if \(\mathbf{D}_{0ij}=1\), \(\mathcal{T}_{1}(\mathbf{C}_{ij}):=\text{min}(1,\mathbf{C}_{ij})\), \(\mathcal{T}_{0}(\mathbf{C}_{ij}):=\text{max}(0,\mathbf{C}_{ij})\).

Finally, the Lagrangian multiplier matrix and \(\mu\) are updated by

\[\begin{cases}\mathbf{\Phi}\leftarrow\mathbf{\Phi}+\mu(\mathbf{D}-\mathbf{A}) \\ \mu\ \leftarrow\text{min}(1.1\mu,\mu_{\text{max}}),\end{cases} \tag{17}\]

where \(\mu_{\text{max}}\)=\(10^{10}\) is a predefined upper bound for \(\mu\).

**Model Prediction**

Given an unseen test example \(\widehat{x}\), our method predicts its label by

\[\widehat{y}=\operatorname*{arg\,max}_{k}\sum_{i=1}^{m}\mathbf{H}_{ik}\mathcal{ K}(\widehat{x},x_{i})+\mathbf{b}_{k}. \tag{18}\]

The overall pseudo code of our method is summarized in Algorithm 1, where it stops when \(\|\mathbf{D}-\mathbf{A}\|_{\infty}<10^{-8}\), where \(\|\cdot\|_{\infty}\) denotes the infinity norm of a matrix.

**Hyper-parameter Settings of Our Method**

Parameter \(\lambda\) is used to control the model complexity. A too large (resp. small) value of \(\lambda\) will lead to under-fitting (resp. over-fitting). Therefore, we set \(\lambda\)=0.05 for our method. Parameters \(\alpha\) and \(\beta\) are used to control the importance of the adversarial term and dissimilarity propagation term respectively. According to a number of experiments, we fix \(\beta=0.001\) and select \(\alpha\) from \(\{0.001,0.01\}\). Parameter \(k\) controls the number of \(k\)-nearest neighbors. Following the previous works [16; 22], we set \(k\)=10.

**Computational Complexity Analysis**

The computational complexity of Algorithm 1 is mainly determined by steps 4-7. Specifically, steps 4 and 6 involve the inversion of \(m\times m\) matrices with the complexity of \(\mathcal{O}(m^{3})\). Step 5 solves a QP problem, leading to the complexity of \(\mathcal{O}(m^{3}q^{3})\). Step 7 can be efficiently solved by linear thresholding operations with the complexity of \(\mathcal{O}(m^{2})\). Therefore, the overall computational complexity of Algorithm 1 in each iteration is \(\mathcal{O}(2m^{3}+m^{2}+m^{3}q^{3})\). More analysis about computational complexity can be found in **Section A** of the supplementary file.

```
0:\(\mathcal{D}\): the partial label training set; \(\lambda,\alpha,\beta,k\): the parameters of model; \(\widehat{x}\): an unseen test sample
0:\(\widehat{y}\): the predicted label for sample \(\widehat{x}\)
1: Construct the dissimilarity matrix \(\mathbf{D}_{0}\) according to Eq. (2) and the kernel matrix \(\mathbf{K}=[\mathcal{K}(x_{i},x_{j})]_{m\times m}\)
2: Initialize \(\mathbf{D}=\mathbf{A}=\mathbf{\Phi}=\mathbf{0}_{m\times m},\mu=10^{-4}\)
3:while not converged do
4: Update \(\mathbf{H}\) and b by Eq. (11)
5: Update \(\mathbf{F}\) by solving Eq. (12)
6: Update \(\mathbf{D}\) by Eq. (14)
7: Update \(\mathbf{A}\) by Eq. (16)
8: Update \(\mathbf{\Phi},\mu\) by Eq. (17)
9: Check the convergence condition \(\|\mathbf{D}-\mathbf{A}\|_{\infty}<10^{-8}\)
10:endwhile
11: Return the predicted label \(\widehat{y}\) according to Eq. (18).
```

**Algorithm 1** The Pseudo Code of the Proposed Method

## 4 Theoretical Analysis

**Theorem 1**.: _The square loss function \(\ell\) of DPCLS can be rewritten as \(\|\mathbf{F}_{\mathbf{G}}+\mathbf{N}-\mathbf{X}\mathbf{W}\|_{F}^{2}\), where \(\mathbf{F}_{\mathbf{G}}\in\mathbb{R}^{m\times q}\) and \(\mathbf{N}\in\mathbb{R}^{m\times q}\) are ground-truth label matrix and false-positive label matrix respectively. Let \(\mathcal{H}=\mathbf{W}\times\mathbf{N}\) represent the family of functions for DPCLS, where the linear function \((\mathbf{W},\mathbf{N})\in\mathcal{H}\). Suppose the complexity of \(\mathbf{W}\) and the sparsity of \(\mathbf{N}\) are upper bounded by \(\epsilon_{1}\) and \(\epsilon_{2}\) respectively, i.e., \(\|\mathbf{W}\|_{F}\leq\epsilon_{1}\) and \(\|\mathbf{N}\|_{1}\leq\epsilon_{2}\). The Rademacher complexity of DPCLS with square loss \(\ell\) is upper bounded as follow_

\[\hat{\mathcal{R}}_{S}(\ell\circ\mathcal{H})\leq\frac{2\sqrt{2}q(\sqrt{mq} \epsilon_{1}+\epsilon_{2})}{m}. \tag{19}\]

**Lemma 1**.: _[_11_]_ _Denote \(\mathcal{H}\) be a family of functions and \(S=\{x_{1},x_{2},...,x_{m}\}\) is a set of fixed samples. Loss function \(\ell\) is upper bounded by \(\Theta\geq 0\), then for any \(\delta>0\), with probability at least \(1-\delta\), for all \(h\in\mathcal{H}\) we have_

\[\mathcal{L}(h)\leq\mathcal{L}_{S}(h)+\hat{\mathcal{R}}_{S}(\ell\circ\mathcal{H })+36\sqrt{\frac{log(2/\delta)}{2m}}, \tag{20}\]_where \(\mathcal{L}(h)\) and \(\mathcal{L}_{\mathcal{S}}(h)\) are generalization error and empirical error to \(h\) respectively._

The detailed proof of **Theorem 1** is given in the **Section B** of the supplementary file. According to **Lemma 1** and **Theorem 1**, we have

\[\mathcal{L}(h)\leq\mathcal{L}_{\mathcal{S}}(h)+\frac{2\sqrt{2}q(\sqrt{mq} \epsilon_{1}+\epsilon_{2})}{m}+3\Theta\sqrt{\frac{log(2/\delta)}{2m}}. \tag{21}\]

The Rademacher complexity is bounded by the sum of the complexity of classifier \(\mathbf{W}\) and the sparsity of false-positive label matrix \(\mathbf{N}\). When the number of false-positive labels is small, which leads to a better generalization performance. Moreover, more training samples (a larger \(m\)) will also promote the generalization performance.

**Theorem 2**.: _Denote \(\mathbf{F}\in\{0,1\}^{m\times q}\) and \(\mathbf{D}\in\{0,1\}^{m\times m}\) the partial label matrix and the to be optimized semantic dissimilarity matrix. Let \(\mathbf{F}_{\mathbf{G}}\) and \(\hat{\mathbf{D}}\) be the ground-truth label matrix and the ground-truth dissimilarity matrix. Suppose the smallest eigenvalue of \(\hat{\mathbf{D}}\) and \(\mathbf{L}\) are \(\lambda_{\hat{\mathbf{D}}}\) and \(\lambda_{\mathbf{L}}\) respectively (\(\lambda_{\hat{\mathbf{D}}}\geq 0,\lambda_{\mathbf{L}}\geq 0\)). Let \(\left\|\mathbf{\bar{\Delta}}_{\mathbf{F}}\right\|_{F}\) be the average distance of each sample between \(\mathbf{F}_{\mathbf{G}}\) and \(\mathbf{F}\) (i.e., \(\left\|\mathbf{\bar{\Delta}}_{\mathbf{F}}\right\|_{F}=\frac{1}{m}\left\| \mathbf{F}_{\mathbf{G}}-\mathbf{F}\right\|_{F}\)) and \(\left\|\mathbf{\bar{\Delta}}_{\mathbf{D}}\right\|_{F}\) be the average distance of each corresponding position between \(\hat{\mathbf{D}}\) and \(\mathbf{D}\) (i.e., \(\left\|\mathbf{\bar{\Delta}}_{\mathbf{D}}\right\|_{F}=\frac{1}{m^{2}}\left\| \mathbf{\hat{D}}-\mathbf{D}\right\|_{F}\)). Then we have_

\[\left\|\mathbf{\bar{\Delta}}_{\mathbf{F}}\right\|_{F}\leq\frac{q} {\lambda_{\hat{\mathbf{D}}}}\left\|\mathbf{D}-\mathbf{\hat{D}}\right\|_{F}+ \frac{2\sqrt{q}}{\lambda_{\hat{\mathbf{D}}}\sqrt{m}}\left\|\mathbf{\hat{D}} \right\|_{F}, \tag{22}\] \[\left\|\mathbf{\bar{\Delta}}_{\mathbf{D}}\right\|_{F}\leq\frac{1 }{\lambda_{\mathbf{L}}m}\left\|\mathbf{F}\mathbf{F}^{\mathsf{T}}-\mathbf{F}_{ \mathbf{G}}\mathbf{F}_{\mathbf{G}}\right.^{\mathsf{T}}\right\|_{F}+\frac{2}{ \lambda_{\mathbf{L}}m}\left\|\mathbf{L}\right\|_{F}+\frac{1}{\lambda_{\mathbf{ L}}m}.\]

The proof can be found in **Section B** of the supplementary file. From **Theorem 2**, we can find that as the number of samples \(m\) increases, the upper bound of \(\left\|\mathbf{\bar{\Delta}}_{\mathbf{F}}\right\|_{F}\) decreases, which indicates that more training samples will push the partial label matrix to be close to the ground-truth one and achieve better PLL performance. Moreover, a smaller error between \(\mathbf{D}\) and \(\mathbf{\hat{D}}\) implies a smaller upper bound of \(\left\|\mathbf{\bar{\Delta}}_{\mathbf{F}}\right\|_{F}\), which indicates a better dissimilarity matrix can help achieve a better label matrix. Similarly, a larger number of training samples will reduce the distance between \(\mathbf{D}\) and \(\mathbf{\hat{D}}\), and a smaller error between \(\mathbf{F}\) and \(\mathbf{F}_{\mathbf{G}}\) implies a smaller upper bound of \(\left\|\mathbf{\bar{\Delta}}_{\mathbf{D}}\right\|_{F}\), suggesting a better dissimilarity matrix. _As a summary, we prove that, under some general assumptions, a better dissimilarity matrix will help produce a better label matrix, and vice versa. Therefore, the rationality of the proposed adversarial prior is theoretically proved._

## 5 Experiment and Analysis

To demonstrate the effectiveness of the proposed model, we compared DPCLS with eight shallow PLL algorithms, which were configured by the suggested parameters in the literature, i.e., CLPL [1], PL-SVM [12], PL-KNN [5], PL-DA [18], IPAL [22], AGGD [16], PL-CLA [13], SDIM [2]. Those methods were evaluated on 10 synthetic data sets and 7 real-world data sets, whose details can be found in **Section C** of the supplementary file. Three deep learning PLL methods RC [4], PRODEN [10], and CAVL [21] were also evaluated as comparison on the real-world data sets. Ten runs of \(50\%/50\%\) random train/test splits were performed on each data set, and the average classification accuracy and the standard deviation were recorded.

### Performance on Synthetic Data Sets

Following the widely-used partial label data generation protocol [1], 10 synthetic data sets were used to generate artificial partial label data sets. Specifically, three parameters \(r,p,\epsilon\) control the generation process, i.e., \(p\) controls the proportion of partial label examples, \(r\) controls the number of false-positive labels, and \(\epsilon\) controls the probability of a specific false-positive label co-occurs with the ground-truth label.

Fig. 1 illustrates the classification accuracy of six data sets (Glass, Steel, Ecoli, Yeast, Optdigits and Usps) as the co-occurring probability \(\epsilon\) varies from 0.1 to 0.7 with step-size 0.1 (\(p\)=1, \(r\)=1). In general, the proposed model clearly exceeds the compared methods with different \(\epsilon\), and achieves the highest accuracy in 33 out of 36 cases. Moreover, as \(\epsilon\) increases, it is more difficult to distinguish the 

[MISSING_PAGE_FAIL:7]

Different from them, the proposed method produces excellent performance on all the evaluated data sets, suggesting its robustness to different data sets.

Table 3 (row (I)) reports win/tie/loss counts between DPCLS and eight comparing algorithms on the synthetic data sets according to the pairwise t-test at the significance level of 0.05, where we can find that DPCLS statistically outperforms other algorithms in 79.6% cases (331 out of 416) and none of the algorithms can beat DPCLS significantly.

### Performance on Real-world Data Sets

Real-world data sets were collected from various tasks and domains. As the average size of the candidate label set of FG-NET is large, which will cause low classification accuracy on the test set. Following [2, 16] we employed the mean absolute error (MAE) to further calculate two extra evaluation indicators MAE3 and MAE5 on FG-NET, i.e., the test examples are considered to be correctly classified if the difference between the predicted age and the ground-truth age is no more than 3/5 years.

Table 2 demonstrates the classification accuracies of different methods on the real-world data sets. It is obvious that our method ranks first in all cases when compared with shallow PLL algorithms. Moreover, according to the pairwise t-test, our method statistically outperforms PL-SVM, PL-KNN and PL-DA on all real-world data sets and statistically surpasses AGGD, PL-CLA and SDIM on six data sets except FG-NET. Furthermore, compared with the best comparisons, our method improves the classification accuracy from 0.494 to 0.557 on MSRCv2 and from 0.747 to 0.770 on Lost.

We also compare the classification accuracies of our method with three deep learning PLL methods RC, PRODEN, CAVL on the real-world data sets. Our method ranks first on six the data sets except Yahoo! News. Specifically, according to the pairwise t-test, our method statistically outperforms RC, PRODEN and CAVL on three real-world data sets (Lost, MSRCv2 and BirdSong). Furthermore, compared with the best comparisons of deep learning PLL methods, our method improves the classification accuracy from 0.446 to 0.557 on MSRCv2 and from 0.715 to 0.751 on BirdSong. Therefore, we can conclude that our method shows competitive performance compared with the deep learning PLL methods.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline Type & Method & FG-NET & FG-NET3 & FG-NET3 & Lost & MSRCv2 & BirdSong & Malagy & Soccer Player & Yahoo! News \\ \hline \multirow{8}{*}{5} & DPCLS & **.077\(\pm\).009** & **436\(\pm\)017** & **.586\(\pm\)011** & **.770\(\pm\)024** & **.557\(\pm\)014** & **.751\(\pm\)009** & **.676\(\pm\)004** & **.532\(\pm\)002** & **.626\(\pm\)003** \\  & CLPL &.058\(\pm\).009\(\pm\) & 383\(\pm\)016** &.538\(\pm\)017** & **.665\(\pm\)019** &.371\(\pm\)010** & **.610\(\pm\)012** & **.675\(\pm\)016** &.497\(\pm\)002** & **.544\(\pm\)**.004\(\pm\)** \\  & PL-SVM &.052\(\pm\).010\(\pm\) &.357\(\pm\)022** & **.511\(\pm\)026** & **.578\(\pm\)078** & **.310\(\pm\)060** & **.682\(\pm\)023** & **.564\(\pm\)**.061** & **.500\(\pm\)**.002** & **.546\(\pm\)**.006\(\pm\)** \\  & PL-KNN &.038\(\pm\).005\(\pm\) &.287\(\pm\)022** & **.433\(\pm\)019** & **.334\(\pm\)021** & **.391\(\pm\)023** & **.657\(\pm\)014** & **.573\(\pm\)**.007** & **.493\(\pm\)**.002** & **.383\(\pm\)**.003\(\pm\)** \\  & PL-DA &.042\(\pm\).004\(\pm\) &.166\(\pm\)050** & **.255\(\pm\)**070** & **.309\(\pm\)069** & **.416\(\pm\)022** & **.690\(\pm\)**.013** & **.606\(\pm\)**.008** & **.495\(\pm\)**.003\(\pm\)** & **.397\(\pm\)**.004\(\pm\)** \\  & IPAI &.052\(\pm\).006\(\pm\) &.347\(\pm\)015** & **.510\(\pm\)**016** & **.610\(\pm\)**020** & **.494\(\pm\)**.024** & **.722\(\pm\)006** & **.621\(\pm\)**.017** & **.530\(\pm\)**.005** & **.618\(\pm\)**.007\(\pm\)** \\  & AGGD &.075\(\pm\).010 &.423\(\pm\)016 &.568\(\pm\)018 & **.702\(\pm\)**024** & **.477\(\pm\)019** & **.722\(\pm\)**014** & **.593\(\pm\)**.050** & **.527\(\pm\)003** & **.616\(\pm\)**.004\(\pm\)** \\  & PL-CLA &.074\(\pm\).011 &.424\(\pm\)020 &.571\(\pm\)015** & **.696\(\pm\)012** & **.470\(\pm\)**016** & **.722\(\pm\)**012** & **.654\(\pm\)**.005** & **.525\(\pm\)**.003** & **.606\(\pm\)**.004\(\pm\)** \\  & SDIM &.073\(\pm\).009 &.423\(\pm\)022 &.568\(\pm\)019** & **.726\(\pm\)023** & **.447\(\pm\)016** & **.724\(\pm\)012** & **.543\(\pm\)**007** & **.542\(\pm\)**013** & **.607\(\pm\)**.004\(\pm\)** \\  & RC &.072\(\pm\).009 &.391\(\pm\)012** & **.488\(\pm\)**020** & **.740\(\pm\)**026** & **.446\(\pm\)019** & **.715\(\pm\)**007** & **.664\(\pm\)**.004** & **.532\(\pm\)**.004** & **.620\(\pm\)**.003\(\pm\)** \\  & PRODEN &.071\(\pm\).009 &.415\(\pm\)016** & **.567\(\pm\)025** & **.712\(\pm\)**032** & **.430\(\pm\)**019** & **.704\(\pm\)**013** & **.665\(\pm\)**.017** & **.528\(\pm\)**.004** & **.620\(\pm\)**.003\(\pm\)** \\  & CAVL &.071\(\pm\).006 &.365\(\pm\)020** & **.488\(\pm\)**021** & **.747\(\pm\)060** & **.444\(\pm\)**013** & **.695\(\pm\)**017** & **.668\(\pm\)**.039** & **510\(\pm\)**.004** & **.628\(\pm\)**.004** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Classification accuracy (mean\(\pm\)std) of each algorithm on real-world partial label data sets. \(\bullet\)/\(\circ\) indicates whether the accuracy of DPCLS is statistically superior/inferior to the compared algorithm according to the pairwise t-test at 0.05 significance level. “S” and “D” indicate shallow and deep PLL methods respectively.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline  & CLPL & PL-SVM & PL-KNN & PL-DA & IPAI & AGGD & PL-CLA & SDIM & D-PLL \\ \hline (I) & 44/8/0 & 51/1/0 & 52/0/0 & 47/5/0 & 46/6/0 & 23/29/0 & 38/14/0 & 30/22/0 & – \\ (II) & 8/1/0 & 9/0/0 & 9/0/0 & 9/0/0 & 8/1/0 & 7/2/0 & 7/2/0 & 7/2/0 & 21/6/0 \\ Total & 52/9/0 & 60/1/0 & 61/0/0 & 56/5/0 & 54/7/0 & 30/31/0 & 45/16/0 & 37/2Table 3 (row (II)) presents the win/tie/loss counts of the proposed method against the compared ones on real-world data sets according to the pairwise t-test at significance level of 0.05. On all the real-world data sets, DPCLS achieves significantly superior performance in 85.9% cases, and gets comparable performance in the remaining cases against the comparing algorithms. Taking both the synthetic data sets and real-world data sets into account, our method accomplishes significantly better performance in 80.8% cases, which indicates the excellent classification ability of our method.

### Further Analysis

**Visualization** Fig. 2 visually compares the ideal, the original and the enhanced similarity matrix and semantic dissimilarity matrices on data set Ecoli (\(p\)=1, \(r\)=1, \(\epsilon\)=0.8). Compared with the ideal similarity matrix, the value of each element in the original similarity matrix \(\mathbf{FF}^{\mathsf{T}}\) is relatively small with many incorrect connections. Compared with the ideal dissimilarity matrix, the original dissimilarity matrix \(\mathbf{D}_{0}\) is relatively sparse, and many positive elements are missed in \(\mathbf{D}_{0}\). On the contrary, the similarity matrix and the dissimilarity matrix produced by DPCLS become denser, which are quite close to the ideal ones. Especially, when checking the areas highlighted by the red rectangle boxes, DPCLS can effectively recover the similarity relationship between samples in \(\mathbf{FF}^{\mathsf{T}}\), and many positive elements missed by \(\mathbf{D}_{0}\) have been recovered by our method. The visual comparison illustrates that our method can produce much higher-quality similarity and dissimilarity matrices, which is useful to find a high-quality label confidence matrix \(\mathbf{F}\) and promote the performance of PLL.

**Ablation Study** In Table 4, we conduct an ablation study on the real-world data sets to check the necessity of the involved terms of our method. Specifically, DPCLS-LM denotes DPCLS without Dissimilarity Propagation and Kernel Extension, and DPCLS-KE and DPCLS-DP indicate DPCLS without Kernel Extension and without Dissimilarity Propagation respectively. From Table 4, we can find that both the kernel extension and the dissimilarity propagation are helpful in improving classification accuracy and taking both of them into account is the best choice.

## 6 Conclusion

In this paper, we have presented a novel PLL method. Specifically, we first construct a similarity matrix based on the multiplication of the label confidence matrix and its transpose. Then, we develop a dissimilarity matrix by exploiting the label space, and further utilize the local geometric structure of the samples to enhance the dissimilarity matrix, i.e., propagating the initial semantic dissimilarity relationships to the whole data set. The similarity and dissimilarity matrices form an adversarial relation, and the proposed model takes advantage of this novel adversarial prior to shrink the solution space of the label confidence matrix, which contributes to find the correct label in the candidate label set. Extensive experiments and comparisons on artificial and real-world partial label data sets have validated the effectiveness of our approach.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & FG-NET & Lost & MSRCV2 & BirdSong & Malays & Soccer Player & Yahoo! News \\ \hline DPCLS & **.077\(\pm\).009** & **.770\(\pm\).024** & **.557\(\pm\).014** & **.751\(\pm\).009** & **.676\(\pm\).004** & **.532\(\pm\).002** & **.626\(\pm\).003** \\ DPCLS-LM & 0.067\(\pm\).009\(\bullet\) & 652\(\pm\).023\(\bullet\) & 357\(\pm\).009\(\bullet\) & 577\(\pm\).012\(\bullet\) & 587\(\pm\).014\(\bullet\) & 492\(\pm\).002\(\bullet\) & 447\(\pm\).00\(\bullet\) \\ DPCLS-KE &.068\(\pm\).009\(\bullet\) &.701\(\pm\).023\(\bullet\) & 388\(\pm\).014\(\bullet\) &.595\(\pm\).014\(\bullet\) &.674\(\pm\).009 &.495\(\pm\).002\(\bullet\) &.463\(\pm\).004\(\bullet\) \\ DPCLS-DP &.073\(\pm\).010 &.687\(\pm\).027\(\bullet\) &.466\(\pm\).018\(\bullet\) &.721\(\pm\).014\(\bullet\) &.612\(\pm\).011\(\bullet\) &.524\(\pm\).003\(\bullet\) &.604\(\pm\).004\(\bullet\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablation study of our method on the real-world partial label data sets.

Figure 2: Visual comparison of the similarity matrix and the semantic dissimilarity matrix on data set Ecoli (\(p\)=1, \(r\)=1, \(\epsilon\)=0.8). (a), (d) Ideal similarity matrix and semantic dissimilarity matrix; (b), (e) Initial similarity matrix \(\mathbf{FF}^{\mathsf{T}}\) and semantic dissimilarity matrix \(\mathbf{D}_{0}\); (c), (f) The similarity matrix \(\mathbf{FF}^{\mathsf{T}}\) and the dissimilarity matrix \(\mathbf{D}\) produced by DPCLS. The brighter color indicates a larger value.

#### Acknowledgments

This work was supported in part by the National Natural Science Foundation of China under Grant 62106044 and 62072100, in part by the Natural Science Foundation of Jiangsu Province under Grant BK20210221, and in part by the ZhiShan Youth Scholar Program from Southeast University under Grant 2242022R40015.

## References

* [1] Timothee Cour, Benjamin Sapp, and Ben Taskar. Learning from partial labels. _Journal of Machine Learning Research_, 12:1501-1536, 2011.
* [2] Lei Feng and Bo An. Partial label learning by semantic difference maximization. In _International Joint Conference on Artificial Intelligence_, pages 2294-2300, 2019.
* [3] Lei Feng and Bo An. Partial label learning with self-guided retraining. In _AAAI Conference on Artificial Intelligence_, pages 3542-3549, 2019.
* [4] Lei Feng, Jiaqi Lv, Bo Han, Miao Xu, Gang Niu, Xin Geng, Bo An, and Masashi Sugiyama. Provably consistent partial-label learning. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems_, 2020.
* [5] Eyke Hullermeier and Jurgen Beringer. Learning from ambiguously labeled examples. _Intelligent Data Analysis_, 10(5):419-439, 2006.
* [6] Rong Jin and Zoubin Ghahramani. Learning with multiple labels. In _Advances in Neural Information Processing Systems 15_, pages 897-904, 2002.
* [7] Changchun Li, Ximing Li, and Jihong Ouyang. Learning with noisy partial labels by simultaneously leveraging global and local consistencies. In _ACM International Conference on Information and Knowledge Management_, pages 725-734, 2020.
* [8] Li-Ping Liu and Thomas G. Dietterich. A conditional multinomial mixture model for superset label learning. In _Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems_, pages 557-565, 2012.
* [9] Jie Luo and Francesco Orabona. Learning from candidate labeling sets. In _Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems_, pages 1504-1512, 2010.
* [10] Jiaqi Lv, Miao Xu, Lei Feng, Gang Niu, Xin Geng, and Masashi Sugiyama. Progressive identification of true labels for partial-label learning. In _International Conference on Machine Learning_, volume 119, pages 6500-6510, 2020.
* [11] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of Machine Learning_. The MIT Press, 2nd edition, 2018.
* [12] Nam Nguyen and Rich Caruana. Classification with partial labels. In _ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 551-559, 2008.
* [13] Peng Ni, Suyun Zhao, Zhi-Gang Dai, Hong Chen, and Cui-Ping Li. Partial label learning via conditional-label-aware disambiguation. _J. Comput. Sci. Technol._, 36(3):590-605, 2021.
* [14] Cai-Zhi Tang and Min-Ling Zhang. Confidence-rated discriminative partial label learning. In Satinder Singh and Shaul Markovitch, editors, _AAAI Conference on Artificial Intelligence_, pages 2611-2617, 2017.
* [15] Yingjie Tian, Xiaotong Yu, and Saiji Fu. Partial label learning: Taxonomy, analysis and outlook. _Neural Networks_, 161:708-734, 2023.
* [16] Deng-Bao Wang, Min-Ling Zhang, and Li Li. Adaptive graph guided disambiguation for partial label learning. _IEEE Trans. Pattern Anal. Mach. Intell._, 44(12):8796-8811, 2022.

* [17] Haoobo Wang, Ruixuan Xiao, Yixuan Li, Lei Feng, Gang Niu, Gang Chen, and Junbo Zhao. Pico: Contrastive label disambiguation for partial label learning. In _International Conference on Learning Representations_, 2022.
* [18] Wei Wang and Min-Ling Zhang. Partial label learning with discrimination augmentation. In _ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1920-1928, 2022.
* [19] Ning Xu, Jiaqi Lv, and Xin Geng. Partial label learning via label enhancement. In _AAAI Conference on Artificial Intelligence_, pages 5557-5564, 2019.
* [20] Zinan Zeng, Shijie Xiao, Kui Jia, Tsung-Han Chan, Shenghua Gao, Dong Xu, and Yi Ma. Learning by associating ambiguously labeled images. In _IEEE Conference on Computer Vision and Pattern Recognition_, pages 708-715, 2013.
* [21] Fei Zhang, Lei Feng, Bo Han, Tongliang Liu, Gang Niu, Tao Qin, and Masashi Sugiyama. Exploiting class activation value for partial-label learning. In _International Conference on Learning Representations_, 2022.
* [22] Min-Ling Zhang and Fei Yu. Solving the partial label learning problem: An instance-based approach. In _International Joint Conference on Artificial Intelligence_, pages 4048-4054, 2015.
* [23] Min-Ling Zhang, Bin-Bin Zhou, and Xu-Ying Liu. Partial label learning via feature-aware disambiguation. In _ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1335-1344, 2016.