# MMD-FUSE: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting

 Felix Biggs

Centre for Artificial Intelligence

Department of Computer Science

University College London & Inria London

contact@felixbiggs.com

&Antonin Schrab

Centre for Artificial Intelligence

Gatsby Computational Neuroscience Unit

University College London & Inria London

a.schrab@ucl.ac.uk

Equal contribution.

&Arthur Gretton

Gatsby Computational Neuroscience Unit

University College London

arthur.gretton@gmail.com

###### Abstract

We propose novel statistics which maximise the power of a two-sample test based on the Maximum Mean Discrepancy (MMD), by adapting over the set of kernels used in defining it. For finite sets, this reduces to combining (normalised) MMD values under each of these kernels via a weighted soft maximum. Exponential concentration bounds are proved for our proposed statistics under the null and alternative. We further show how these kernels can be chosen in a data-dependent but permutation-independent way, in a well-calibrated test, avoiding data splitting. This technique applies more broadly to general permutation-based MMD testing, and includes the use of deep kernels with features learnt using unsupervised models such as auto-encoders. We highlight the applicability of our MMD-FUSE test on both synthetic low-dimensional and real-world high-dimensional data, and compare its performance in terms of power against current state-of-the-art kernel tests.

## 1 Introduction

The fundamental problem of non-parametric two-sample testing consists in detecting the difference between any two distributions having access only to samples from these. Kernel-based tests relying on the Maximum Mean Discrepancy (MMD; Gretton et al., 2012a) as a measure of distance on distributions are well-suited for this framework as they can identify complex non-linear features in the data, and benefit from both strong theoretical guarantees and ease of implementation. This explains their popularity among practitioners and justifies their wide use for real-world applications.

However, the performance of these tests is crucially impacted by the choice of kernel. This is commonly tackled by either: choosing the kernel by some weakly data-dependent heuristic (Gretton et al., 2012a); or splitting off a hold-out set of data for kernel selection, with the other half used for the actual test (Gretton et al., 2012b; Sutherland et al., 2017). This latter method includes training feature extractors such as deep kernels on the first selection half. Both of these methods can incur a significant loss in test power, since heuristics may lead to poor kernel choices, and data splitting reduces the number of data points for the actual test.

Our contribution is to present MMD-based tests which can strongly adapt to the data _without_ data splitting. This comes in two parallel parts: firstly we show how the kernel can be chosen in anunsupervised fashion using the entire dataset, and secondly we show how multiple such kernels can be adaptively weighted in a single test statistic, optimising test power.

**Data Splitting.** The data splitting approach selects test parameters on a held-out half of the dataset, and applies the test to the other half. Commonly, this involves optimising a kernel on held-out data in a supervised fashion to distinguish which sample originated from which distribution, either by learning a deep kernel directly (Sutherland et al., 2017; Liu et al., 2020, 2021), or indirectly through the associated witness function (Kubler et al., 2022, 2022). Jitkrittum et al. (2016) propose tests which select witness function features (in either spatial or frequency space) on the held-out data, running the analytic representation test of Chwialkowski et al. (2015) on the remaining data.

Our first contribution is to show that it is possible to learn a feature extractor for our test (_e.g._, a deep kernel) on the _entirety_ of the data in an _unsupervised_ manner while retaining the desired non-asymptotic test level. Specifically, any method can be used that is ignorant of which distribution generated which samples. We can thus leverage many feature extraction methods, from auto-encoders (Hinton and Salakhutdinov, 2006) to recent powerful developments in self-supervised learning (He et al., 2020; Chen et al., 2020, 2020; Chen and He, 2021; Grill et al., 2020; Caron et al., 2020; Zbontar et al., 2021; Li et al., 2021). Remarkably, our method applies to _any_ permutation-based two-sample test, even non-kernel tests, provided the parameters are chosen in this unsupervised fashion. This includes a very wide array of MMD-based tests, and finally provides a formal justification for the commonly-used median heuristic (Gretton et al., 2012; Ramdas et al., 2015; Reddi et al., 2015).

**Adaptive Kernel Selection.** A newer approach originating in Schrab et al. (2023) performs adaptive kernel selection through multiple testing. Aggregating several MMD-based tests with different kernels, each on the whole dataset, results in an overall adaptive test with optimal power guarantees in terms of minimax separation rate over Sobolev balls. Variants of this kernel adaptivity through aggregation have been proposed with linear-time estimators (Schrab et al., 2022), spectral regularisation (Hagrass et al., 2022), kernel thinning compression (Domingo-Enrich et al., 2023), and in the asymptotic regime (Chatterjee and Bhattacharya, 2023). Another adaptive approach using the entire dataset for both kernel selection and testing is given by Kubler et al. (2020); this leverages the Post Selection Inference framework, but the resulting test suffers from low power in practice.

While our unsupervised feature extraction method is an extremely general technique and potentially powerful, particularly for high-dimensional structured data like images, it is not always sufficient for data where such feature extraction is difficult. This motivates our second contribution, a method for combining whole-dataset MMD estimates under multiple different kernels into single test statistics, for which we prove exponential concentration. Kernel parameters such as bandwidth can then be chosen in a non-heuristic manner to optimise power, even on data with less structure and varying length scales. Using a single statistic also ensures that a single _test_ can be used, instead of the multiple testing approach outlined above, reducing computational expense.

**MMD-FUSE.** By combining these contributions we construct two closely related MMD-FUSE tests. Each chooses a set of kernels based on the whole dataset in an unsupervised fashion, and then adaptively weights and _fuses_ this (potentially infinite) set of kernels through our new statistics; both parts of this procedure are done using the entire dataset without splitting. On the finite sets of kernels we use in practice, the weighting procedure is done in closed form via a weighted soft maximum. We show these new tests to be well-calibrated and give sufficient power conditions which achieve the minimax optimal rate in terms of MMD. In empirical comparisons, our test compares favourably to the state-of-the-art aggregated tests in terms of power and computational cost.

**Outline and Summary of Contributions.** In Section 2 we outline our setting, crucial results underlying our work, and alternative existing approaches. Section 3 covers the construction of permutation tests and discusses how we can choose the parameters for any such test in any unsupervised fashion including with deep kernels and by those methods mentioned above. Section 4 introduces and motivates our two proposed tests. Section 5 discusses sufficient conditions for test power (at a minimax optimal rate in MMD), and the exponential concentration of our statistics. Finally, we show that our test compares favourably with a wide variety of competitors in Section 6 and discuss in Section 7.

## 2 Background

**Two-Sample Testing.** The two-sample testing problem is to determine whether two distributions \(p\) and \(q\) are equal or not. In order to test this hypothesis, we are given access to two samples, \(\mathbf{X}\coloneqq(X_{1},\ldots,X_{n})\stackrel{{\text{iid}}}{{\sim}}p\) and \(\mathbf{Y}\coloneqq(Y_{1},\ldots,Y_{m})\stackrel{{\text{iid}}}{{\sim}}q\) as tuples of data points with sizes \(n\) and \(m\). We write the combined (ordered) sample as \(\mathbf{Z}\coloneqq(Z_{1},\ldots,Z_{n+m})=(\mathbf{X},\mathbf{Y})=(X_{1},\ldots,X_{n},Y_{1 },\ldots,Y_{m})\).

We define the null hypothesis \(H_{0}\) as \(p=q\) and the alternative hypothesis \(H_{1}\) as \(p\neq q\), usually with a requirement \(\mathbb{D}(p,q)>\epsilon\) for a distance \(\mathbb{D}\) (such as the MMD) and some \(\epsilon>0\). A hypothesis test \(\Delta\) is a \(\{0,1\}\)-valued function of \(\mathbf{Z}\), which rejects the null hypothesis if \(\Delta(\mathbf{Z})=1\) and fails to reject it otherwise. It will usually be formulated to control the probability of a type I error at some level \(\alpha\in(0,1)\), so that \(\mathbb{P}_{p\times p}(\Delta(\mathbf{Z})=1)\leq\alpha\), while simultaneously minimising the probability of a type II error, \(\mathbb{P}_{p\times q}(\Delta(\mathbf{Z})=0)\). In the above we have used the notation \(\mathbb{P}_{p\times p}\) and \(\mathbb{P}_{p\times q}\) to indicate that the sample \(\mathbf{Z}\) is either drawn from the null, \(p=q\), or the alternative \(p\neq q\). Similar notation will be used for expectations and variances. When a bound \(\beta\in(0,1)\) on the probability of a type II error is given (which may depend on the precise formulation of the alternative), we say the test has power \(1-\beta\).

**Maximum Mean Discrepancy.** The Maximum Mean Discrepancy (MMD) is a kernel-based measure of distance between two distributions \(p\) and \(q\), which is often used for two-sample testing. The MMD quantifies the dissimilarity between these distributions by comparing their mean embeddings in a reproducing kernel Hilbert space (RKHS; Aronszajn, 1950) with kernel \(k(\cdot,\cdot)\). Formally, if \(\mathcal{H}_{k}\) is the RKHS associated with kernel function \(k\), the MMD between distributions \(p\) and \(q\) is the integral probability metric defined by:

\[\mathrm{MMD}_{k}(p,q)\coloneqq\sup_{f\in\mathcal{H}_{k}:\|f\|_{\mathcal{H}_{k} }\leq 1}\left(\mathbb{E}_{X\sim p}[f(X)]-\mathbb{E}_{Y\sim q}[f(Y)]\right).\]

The minimum variance unbiased estimate of \(\mathrm{MMD}_{k}^{2}\), is given by the sum of two U-statistics and a sample average as:2

Footnote 2: Kim et al. (2022) note that \(\widehat{\mathrm{MMD}}_{k}^{2}\) can equivalently be written as a two-sample U-statistic with kernel \(h_{k}(x,x^{\prime};y,y^{\prime})=k(x,x^{\prime})+k(y,y^{\prime})-k(x,y^{ \prime})-k(x^{\prime},y)\), which is useful for analysis.

\[\widehat{\mathrm{MMD}}_{k}^{2}(\mathbf{Z})\coloneqq\frac{1}{n(n-1)}\underset{(i,i ^{\prime})\in[n]_{2}}{\sum\hskip-56.905512ptk(X_{i},X_{i^{\prime}})+\frac{1}{ m(m-1)}\underset{(j,j^{\prime})\in[m]_{2}}{\sum}k(Y_{j},Y_{j^{\prime}})-\frac{2}{mn} \sum_{i=1}^{n}\sum_{j=1}^{m}k(X_{i},Y_{j})},\]

where we introduced the notation \([n]_{2}=\{(i,i^{\prime})\in[n]^{2}:i\neq i^{\prime}\}\) for the set of all pairs of distinct indices in \([n]=\{1,\ldots,n\}\). Tests based on the MMD usually reject the null when \(\widehat{\mathrm{MMD}}_{k}^{2}\) exceeds some critical threshold, with the resulting power being greatly affected by the kernel choice. For _characteristic_ kernel functions (Sriperumbudur et al., 2011), it can be shown that \(\mathrm{MMD}_{k}(p,q)=0\) if and only if \(p=q\), leading to consistency results. However, on finite sample sizes, convergence rates of MMD estimates typically have strong dependence on the data dimension, so there are settings in which kernels ignoring redundant or unimportant features (_i.e._ non-characteristic kernels) will give higher test power in practice than characteristic kernels (which can over-weight redundant features).

**Distributions Over Kernels.** In our test statistic, we will consider the case where the kernel \(k\in\mathcal{K}\) is drawn from a distribution \(\rho\in\mathcal{M}_{+}^{\star}(\mathcal{K})\) (with the latter notation denoting a probability measure; _c.f._ Benton et al., 2019 in the Gaussian Process literature). This distribution will be adaptively chosen based on the data subject to a regularisation term based on a "prior" \(\pi\in\mathcal{M}_{+}^{\star}(\mathcal{K})\) and defined through the Kullback-Liebler divergence: \(\mathrm{KL}(\rho\|\pi)\coloneqq\mathbb{E}_{\rho}[\log(\mathrm{d}\rho/\mathrm{ d}\pi)]\) for \(\rho\ll\pi\) and \(\mathrm{KL}(\rho\|\pi)\coloneqq\infty\) otherwise. When constructing these statistics the Donsker-Varadhan equality (Donsker and Varadhan, 1975), holding for any measurable \(g:\mathcal{K}\to\mathbb{R}\), will be useful:

\[\sup_{\rho\in\mathcal{M}_{+}^{\star}(\mathcal{A})}\mathbb{E}_{\rho}[g]-\mathrm{ KL}(\rho\|\pi)=\log\mathbb{E}_{\pi}[\exp\circ g]. \tag{1}\]

This can be further related to the notion of soft maxima. If \(\pi\) is a uniform distribution on finite \(\mathcal{K}\) with \(|\mathcal{K}|=r\), then \(\mathrm{KL}(\rho\|\pi)\leq\log(r)\). Setting \(g=tf\) for some \(t>0\), Equation (1) relaxes to

\[\max_{k}f(k)-\frac{\log(r)}{t}\leq\frac{1}{t}\log\left(\frac{1}{r}\sum_{k}e^{tf (k)}\right)\leq\max_{k}f(k), \tag{2}\]which approximates the maximum with error controlled by \(t\). Our approach in considering these soft maxima is reminiscent of the PAC-Bayesian (McAllester, 1998; Seeger et al., 2001; Maurer, 2004; Catoni, 2007; see Guedj, 2019 or Alquier, 2021 for a survey) approach to capacity control and generalisation. This framework has raised particular attention recently as it has been used to provide the only non-vacuous generalisation bounds for deep neural networks (Dziugaite and Roy, 2017, 2018; Dziugaite et al., 2021; Zhou et al., 2019; Perez-Ortiz et al., 2021; Biggs and Guedj, 2021, 2022a); it has also been fruitfully applied to other varied problems from ensemble methods (Lacasse et al., 2006, 2010; Masegosa et al., 2020; Wu et al., 2021; Zantedeschi et al., 2021; Biggs and Guedj, 2022b; Biggs et al., 2022) to online learning (Haddouche and Guedj, 2022). Our proofs draw on techniques in that literature for U-Statistics (Lever et al., 2013) and martingales (Seldin et al., 2012; Biggs and Guedj, 2023; Haddouche and Guedj, 2023; Chugg et al., 2023). By considering these soft maxima, we gain a major advantage over the standard approaches, as we can obtain concentration and power results for our statistics without incurring Bonferroni-type multiple testing penalties.

**Permutation Tests.** The tests we discuss in this paper use permutations of the data \(\mathbf{Z}\) to approximate the null distribution. We begin our discussion in a fairly general form to include other close settings such as independence testing (Albert et al., 2022; Rindt et al., 2021) or wild bootstrap-based two-sample tests (Fromont et al., 2012, 2013; Chwialkowski et al., 2014; Schrab et al., 2023). Let \(\mathcal{G}\) be a group of transformations on \(\mathbf{Z}\); in our setting \(\mathcal{G}=\mathfrak{S}_{n+m}\), denoting the permutation (or symmetric) group of \([N]\) by \(\mathfrak{S}_{N}\) and its elements by \(\sigma\in\mathfrak{S}_{N},\sigma:[N]\to[N]\). We write \(g\mathbf{Z}\) for the action of \(g\in\mathcal{G}\) on \(\mathbf{Z}\); _e.g._ defining \(\sigma\mathbf{Z}=(Z_{\sigma(1)},\ldots,Z_{\sigma(n+m)})\) for the group elements \(\sigma\in\mathfrak{S}_{n+m}\). We will suppose \(\mathbf{Z}\) is invariant under the action of \(\mathcal{G}\) when the sample is drawn from the _null_ distribution, _i.e._ if the null is true than \(g\mathbf{Z}=^{d}\mathbf{Z}\) (by which we notate equality of distribution) for all \(g\in\mathcal{G}\). This is clearly the case for \(\mathcal{G}=\mathfrak{S}_{n+m}\) in two-sample testing, since under the null \(\mathbf{Z}=(Z_{1},\ldots,Z_{m+n})\stackrel{{\text{iid}}}{{\sim}}p\) with \(p=q\), while under the alternative, the permuted sample \(\sigma\mathbf{Z}\) for randomised \(\sigma\sim\operatorname{Uniform}(\mathfrak{S}_{n+m})\) simulates the null distribution.

We can use permutations to construct an approximate cumulative distribution (CDF) of our test statistic under the null, and choose an appropriate quantile of this CDF as our test threshold, which must be exceeded under the null with probability less that level \(\alpha\). For this we introduce a quantile operator (analogous to the \(\max\) and \(\min\) operators) for a finite set \(\{f(a)\in\mathbb{R}:a\in\mathcal{A}\}\):

\[\operatorname*{quantile}_{q,\,a\in\mathcal{A}}f(a)\coloneqq\inf\left\{r\in \mathbb{R}:\frac{1}{|\mathcal{A}|}\sum_{a\in\mathcal{A}}\mathbf{1}\{f(a)\leq r\} \geq q\right\}. \tag{3}\]

Various different results can be used to choose the threshold giving a correct level; we will here highlight a very general and easy-to-use theorem for constructing permutation tests, and in Section 3 will discuss previously unconsidered implications for using unsupervised feature extraction methods as a part of our tests. Although this result is not new, we believe that its usefulness has been under-appreciated in the kernel testing community, and can be more conveniently applied than Romano and Wolf (2005, Lemma 1), which requires exchangeable and is commonly used, _e.g._ in Albert et al. (2022, Proposition 1) and Schrab et al. (2023, Proposition 1).

**Theorem 1** (Hemerik and Goeman, 2018, Theorem 2.).: _Let \(G\) be a vector of elements from \(\mathcal{G}\), \(G=(g_{1},g_{2},\ldots,g_{B+1})\), with \(g_{B+1}=\operatorname{id}\) (the identity permutation) for any \(B\geq 1\). The elements \(g_{1},\ldots,g_{B}\) are drawn uniformly from \(\mathcal{G}\) either i.i.d. or without replacement (which includes the possibility of \(G=\mathcal{G}\)). If \(\tau(\mathbf{Z})\) is a statistic of \(\mathbf{Z}\) and \(\mathbf{Z}=^{d}g\mathbf{Z}\) for all \(g\in\mathcal{G}\) under the null then_

\[\mathbb{P}_{p\times p,G}\left(\tau(\mathbf{Z})>\operatorname*{quantile}_{1-\alpha,g \in G}\tau(g\mathbf{Z})\right)\leq\alpha.\]

In other words, if we compare \(\tau(\mathbf{Z})\) with the empirical quantile of \(\tau(g\mathbf{Z})\) as a test threshold, the type I error rate is no more than \(\alpha\).3 The potentially complex task of constructing a permutation test reduces to the trivial task of choosing the statistic \(\tau(\mathbf{Z})\). This result is also true for _randomised_ permutations of any number \(B\geq 1\)_without_ approximation, so an exact and computationally efficient test can be straightforwardly constructed this way.

Footnote 3: This test is near-exact. Specifically, if the statistic realisations are distinct (\(\tau(g\mathbf{Z})\neq\tau(g^{\prime}\mathbf{Z})\) for \(g\neq g^{\prime}\)) as is common for continuous data, the level is exactly \(\lfloor(B+1)\alpha\rfloor/(B+1)\).

We finally mention the related approach of the MMD Aggregated test (MMDAgg; Schrab et al., 2023), which combines multiple MMD-based permutation tests with different kernels, and rejects if any_ of these reject, using distinct quantiles for each kernel. To ensure the overall aggregated test is well-calibrated, these quantiles must be adjusted using a _second level_ of permutations. This incurs additional computational cost, a pitfall avoided by our fused single statistic.

**Faster Sub-Optimal Tests.** While the main focus of this paper revolves around the kernel selection problem for optimal, quadratic MMD testing, we also highlight the existence of a rich literature on efficient kernel tests which run in linear (or near-linear) time. These speed improvements are achieved using various tools such as: incomplete U-statistics (Gretton et al., 2012; Yamada et al., 2019; Lim et al., 2020; Kubler et al., 2020; Schrab et al., 2022b), block U-statistics (Zaremba et al., 2013; Deka and Sutherland, 2022), eigenspectrum approximations (Gretton et al., 2009), Nystrom approximations (Chefarioui et al., 2022), random Fourier features (Zhao and Meng, 2015), analytic representations (Chwialkowski et al., 2015; Jitkrittum et al., 2016), deep linear kernels (Kirchler et al., 2020), kernel thinning (Dwivedi and Mackey, 2021; Domingo-Enrich et al., 2023), _etc._

The efficiency of these tests usually entail weaker theoretical power guarantees compared to their quadratic-time counterparts, which are minimax optimal4(Kim et al., 2022; Li and Yuan, 2019; Fromont et al., 2013; Schrab et al., 2023; Chatterjee and Bhattacharya, 2023). These optimal quadratic tests are either permutation-based non-asymptotic tests (Kim et al., 2022; Schrab et al., 2023) or studentised asymptotic tests (Kim and Ramdas, 2023; Shekhar et al., 2022; Li and Yuan, 2019; Gao and Shao, 2022; Florian et al., 2023). We emphasise that the parameters of any of these permutation-based two-sample tests can be chosen in the unsupervised way we outline in Section 3. We choose to focus in this work on optimal quadratic-time results, but note that our general approach could be extended to sub-optimal faster tests as well.

Footnote 4: With the exception of the near-linear test of Domingo-Enrich et al. (2023) which achieves the same MMD separation rate as the quadratic test but under stronger assumptions on the data distributions.

## 3 Learning Statistics for Permutation Tests

Here we discuss how we can improve permutation-based tests by learning parameters for them in an unsupervised manner. The reason that we highlighted Theorem 1 in the previous section is that it holds for any \(\tau\). For example, we could use the MMD with a kernel chosen based on the data, \(\tau(\mathbf{Z})=\widehat{\mathrm{MMD}}^{2}_{k=k(\mathbf{Z})}(\mathbf{Z})\); however, for each permutation \(\sigma\) we would need to re-compute \(\tau(\sigma\mathbf{Z})=\widehat{\mathrm{MMD}}^{2}_{k=k(\sigma\mathbf{Z})}(\sigma\mathbf{Z})\), so the kernel being used would be different for each permutation. This has two major disadvantages: firstly, it might be computationally expensive to re-compute \(k\) for each permutation, especially for a deep kernel5. Secondly, the _scale_ of the resulting MMD could be dramatically different for each permutation, so the empirical quantile might not lead to a powerful test. This second problem is related to the problem of combining multiple different MMD values into a single test which our MMD-FUSE statistics are designed to combat (Section 4; _c.f._ also MMDAgg, Schrab et al., 2023).

Footnote 5: A possibility envisaged by _e.g._Liu et al. (2020) and dismissed due to the computational infeasability.

**Our Proposal.** In two-sample testing we propose to use a statistic \(\tau_{\theta}\) parameterised by some \(\theta\), where \(\theta\) is fixed for all permutations, but depends on the data in an _unsupervised_ or _permutation-invariant_ way. Specifically, we allow such a parameter to depend on \(\langle\mathbf{Z}\rangle\coloneqq\{Z_{1},\ldots,Z_{n+m}\}\), the _unordered_ combined sample. Since our tests will not depend on the internal ordering of \(\mathbf{X}\) and \(\mathbf{Y}\) (which are assumed i.i.d. under both hypotheses), the only additional information contained in \(\mathbf{Z}\) over \(\langle\mathbf{Z}\rangle\) is the _label_ assigning \(Z_{i}\) to its initial sample. This is justified since \(\langle\mathbf{Z}\rangle=\langle\sigma\mathbf{Z}\rangle\) for all \(\sigma\in\mathfrak{S}_{n+m}\), so setting \(\tau(\mathbf{Z})=\tau_{\theta(\langle\mathbf{Z}\rangle)}(\mathbf{Z})\) gives a fixed \(\theta\) and statistic for all permutations to use in Theorem 1. The information in \(\langle\mathbf{Z}\rangle\) can be used to fine-tune test parameters for any test fitting this setup. This solves both the computation and scaling issues mentioned above.

The above provides a first formal justification for the use of the median heuristic in Gretton et al. (2012a), since it is a permutation-invariant function of the data. However, a far richer set of possibilities are available even when restricting ourselves to these permutation-free functions of \(\mathbf{Z}\). For example, we can use any unsupervised or self-supervised learning method to learn representations to use as the input to an MMD-based test statistic, while paying no cost in terms of calibration and needing to train such methods only once. Given the wide variety of methods dedicated to feature extraction and dimensionality reduction, this opens up a huge range of possibilities for the design of new and principled two-sample tests. The simplicity and generality of our proposal might lead one to expect that this idea has been used before, but it has not to our best knowledge, underlined by the fact that the median heuristic has been widely used without such justification when one follows from this method. This potentially powerful and widely-applicable possibility represents one of our most practical contributions.

## 4 MMD-FUSE: Fusing U-Statistics by Exponentiation

Say we have computed several MMD values \(\widehat{\mathrm{MMD}}_{k}^{2}\) under different kernels \(k\in\mathcal{K}\). How might we combine these? One possibility is to perform multiple testing as in the case of MMDAgg. An even simpler approach would simply take the maximum of those values \(\max_{k}\widehat{\mathrm{MMD}}_{k}^{2}\) since Theorem 1 shows that this will not prevent us from controlling the level of our test by \(\alpha\); note though that for each permutation we would take this maximum separately. Indeed, Carcamo et al. (2022) show that for certain kernel choices the supremum of the MMD values with respect to the bandwidth is a valid integral probability metric (Muller, 1997). There are two main problems with this approach: a capacity control issue and a normalisation issue.

Firstly, if the class over which we are maximising is sufficiently rich (for example a complex neural network), then the maximum may be able to effectively memorise the entire sample for each possible permutation, saturating the statistic for every permutation and limiting test power. Any convergence results would need to hold _simultaneously_ for every \(k\) in \(\mathcal{K}\), and so power results suffer: for finite \(|\mathcal{K}|\) we would incur a sub-optimal Bonferroni correction (see Section 5); while for infinite classes, results would need to involve capacity control quantities like Rademacher complexity. Further, only information from a single maximising "base" kernel can be considered at a time.

Therefore, in both our statistics, we prefer a "soft" maximum, which considers information from every kernel simultaneously and when more than one of the kernels is well-suited (Section 5) it therefore avoids the Bonferroni correction arising from uniform bounds. From Equation (1), our approach is equivalent to using a KL complexity penalty, and is strongly reminiscent of _PAC-Bayesian_ (Section 2) capacity control. We note that other soft maxima could be considered, but our choice makes obtaining exponential concentration inequalities relatively easy (Appendix C), and the dual formulation (Equation (2)) allows us to derive power results in terms of MMD directly.

The second issue is that the MMD estimates might have different scales or variances per kernel which need to be accounted for. In order to be able to meaningfully compare MMD values between each other, these need to be normalised somehow before taking a maximum. We use the common approach of dividing through by a variance-like term (as in "studentised" tests, see Section 2). The specific normaliser is permutation invariant and gives our statistic tight sub-Gaussian null concentration (for well-chosen regularisation parameter \(\lambda\); see Appendix C).

Based on the above we introduce the FUSE-1 statistic which uses a log-sum-exp soft maximum, and the FUSE-N which combines this with normalisation. Both statistics use a "prior" distribution on \(\mathcal{K}\), denoted \(\pi(\langle\mathbf{Z}\rangle)\), which is either fixed independently of the data, or is a function of the data which is invariant under permutation (as discussed in Section 3).

**Definition 1**.: _We define the un-normalised (subscript 1 for normaliser of \(1\)) and normalised (subscript N) test statistics with parameter \(\lambda>0\), respectively, as_

\[\widehat{\mathrm{FUSE}}_{1}(\mathbf{Z}) \coloneqq\frac{1}{\lambda}\log\left(\mathbb{E}_{k\sim\pi(\langle \mathbf{Z}\rangle)}\left[\exp\left(\lambda\widehat{\mathrm{MMD}}_{k}^{2}(\mathbf{X}, \mathbf{Y})\right)\right]\right),\] \[\widehat{\mathrm{FUSE}}_{N}(\mathbf{Z}) \coloneqq\frac{1}{\lambda}\log\left(\mathbb{E}_{k\sim\pi(\langle \mathbf{Z}\rangle)}\left[\exp\left(\lambda\frac{\widehat{\mathrm{MMD}}_{k}^{2}(\mathbf{X },\mathbf{Y})}{\sqrt{\widehat{N}_{k}(\mathbf{Z})}}\right)\right]\right),\]

_where \(\widehat{N}_{k}(\mathbf{Z})\coloneqq\frac{1}{n(n-1)}\sum_{(i,j)\in[n+m]_{2}}k(Z_{i },Z_{j})^{2}\) is permutation invariant._

Although these statistics appear complex, we note that in the case where \(\pi\) has finite support, their calculation reduces to a log-sum-exp of MMD estimates normalised by \(\widehat{N}_{k}\). This is even clearer when \(\pi\) is also uniform on its support, as we consider experimentally; then Equation (2) shows that our statistics reduce to soft maxima, with \(\lambda\) controlling the smoothness or "temperature".

From this, we define the FUSE-1 test (with the FUSE-N test \(\Delta_{\mathrm{FUSE}_{N}}\) defined analogously) as

\[\Delta_{\mathrm{FUSE}_{1}}(\mathbf{Z}):=\mathbf{1}\bigg{\{}\widehat{\mathrm{FUSE}}_{1}( \mathbf{Z})\,>\,\operatorname*{quantile}_{1-\alpha,\sigma\in S}\widehat{\mathrm{FUSE }}_{1}(\sigma\mathbf{Z})\bigg{\}} \tag{4}\]

for sampled set \(S\) of permutations as described above. It compares the test statistic \(\widehat{\mathrm{FUSE}}_{1}\) with its quantiles under permutation, and rejects if the overall value exceeds a quantile controlled by \(\alpha\). Note that since \(\widehat{N}_{k}(\mathbf{Z})\) is permutation invariant, it only needs to be calculated once per kernel in \(\Delta_{\mathrm{FUSE}_{N}}\) (and not separately for each permutation) as per Section 3. See Appendix A.5 for its time complexity.

**Comparison with MMDAgg.** MMDAgg is a different way to think about combining multiple kernels and MMD values, but it relies on a framework based on multiple testing. This can be problematic in the case where the number of kernels considered is large, since as this number increases in the multiple testing approach MMDAgg, the level needs to be corrected differently, so its theretical power behaviour becomes unclear (though empirically MMDAgg retains its power in this setting). By contrast, our proposed FUSE methods avoid multiple testing as a single statistic and quantile are used, avoiding such issues. The FUSE statistics are even defined in the limit of an infinite number of kernels (continuous/uncountable collection of kernels) by considering distributions on the space of kernels, which is not the case for MMDAgg. Moreover, while the MMDAgg approach is only useful for hypothesis testing, having a quantity like FUSE combining multiple kernel-based measures of distance with exponential concentration bounds could be of interest in a wider range of applications.

**FUSE-1 and the Mean Kernel.** Although the un-normalised test has worse performance in practice than our normalised test, the FUSE-1 statistic is interesting in its own right because of various theoretical properties, as we discuss below. Firstly, we introduce the _mean_ kernel \(K_{\rho}(x,y)=\mathbb{E}_{k\sim\rho}k(x,y)\) under a "posterior" \(\rho\in\mathcal{M}^{1}_{+}(\mathcal{K})\), which is indeed a reproducing kernel in its own right. In the finite case, this is simply a weighted sum of "base" kernels.

Note that the linearity of \(\widehat{\mathrm{MMD}}^{2}_{k}\) and \(\mathrm{MMD}^{2}_{k}\) in the kernel \(k\) implies that \(\mathbb{E}_{k\sim\rho}\,\mathrm{MMD}^{2}_{k}(p,q)=\mathrm{MMD}^{2}_{K_{\rho}}( p,q)\), and similarly for \(\widehat{\mathrm{MMD}}^{2}_{k}\), with these terms appearing in our power results (Section 5).

Combining this linearity with the dual formulation of \(\widehat{\mathrm{FUSE}}_{1}\) via Equation (1) gives

\[\widehat{\mathrm{FUSE}}_{1}(\mathbf{Z})=\sup_{\rho\in\mathcal{M}^{1}_{+}(\mathcal{ K})}\widehat{\mathrm{MMD}}^{2}_{K_{\rho}}(\mathbf{X},\mathbf{Y})-\frac{\mathrm{KL}(\rho, \pi)}{\lambda}.\]

This re-states \(\widehat{\mathrm{FUSE}}_{1}\) in terms of "posterior" \(\rho\), and makes the interpretation of our statistic as a KL-regularised kernel-learning method clear. In the finite case, our test simply optimises the weightings of the different kernels in a constrained way. We note that for certain _infinite_ kernel sets and choices of prior it is be possible to express the mean kernel in closed form. This happens because, _e.g._ the expectation of a Gaussian kernel with respect to a Gamma prior over the bandwidth is simply a (closed form) rational quadratic kernel. We discuss this point further in Appendix B.

## 5 Theoretical Power of MMD-Fuse

In this section we outline possible sufficient conditions for our tests to obtain power at least \(1-\beta\) at given level \(\alpha\). The conditions will depend on a fixed data-independent "prior" \(\pi\in\mathcal{M}^{1}_{+}(\mathcal{K})\) and thus hold even without unsupervised parameter optimisation. They are stated as requirements for the _existence_ of a "posterior" \(\rho\in\mathcal{M}^{1}_{+}(\mathcal{K})\) with corresponding mean kernel \(K_{\rho}\) as defined in Section 4. In the finite case, \(K_{\rho}\) is simply a weighted sum of kernels, so these requirements are also satisfied under the same conditions for any single kernel, corresponding to the case where the posterior puts all its weight on a single kernel.

Technically, these results require that there is some constant \(\kappa\) upper bounding all of the kernels, and that \(n\) and \(m\) are within a constant multiple (_i.e._\(n\leq m\leq cn\) for some \(c\geq 1\), notated \(n\asymp m\)). They hold when using randomised permutations provided \(B>c^{\prime}\alpha^{-2}\log(\beta^{-1})\) for small constant \(c^{\prime}>0\).

**Theorem 2** (Fuse-1 Power).: _Fix prior \(\pi\) independently of the data. For the un-normalised test FUSE-1 with \(\lambda\asymp n/\kappa\) and \(n\asymp m\), there exists a universal constant \(C>0\) such that_

\[\exists\rho\in\mathcal{M}^{1}_{+}(\mathcal{K})\ :\ \mathrm{MMD}^{2}_{K_{\rho}}(p,q) >\frac{C\kappa}{n}\left(\frac{1}{\beta}+\log\frac{1}{\alpha}+\mathrm{KL}(\rho, \pi)\right).\]

_is sufficient for FUSE-1 to achieve power at least \(1-\beta\) at level \(\alpha\)._A similar result is obtained for FUSE-N under an assumption that the normalising term is well behaved (see assumption in Theorem 3). This requirement will be satisfied for kernels (including most common ones) that tend to zero only in the limit of the data being infinitely far apart.

**Theorem 3** (FUSE-N Power).: _Fix prior \(\pi\) independently of the data such that for all \(k\in\operatorname{supp}(\pi)\) the expectation \(\mathbb{E}_{\mathbb{Z}\sim p\times q}[\widehat{N}_{k}(\mathbb{Z})^{-1}]<c/\kappa\) is bounded for some \(c<\infty\). For the normalised test FUSE-N with \(\lambda\asymp n\asymp m\), there exists a universal constant \(C>0\) such that_

\[\exists\rho\in\mathcal{M}^{1}_{+}(\mathcal{K})\ :\ \operatorname{MMD}^{2}_{K \rho}(p,q)>\frac{C\kappa}{n}\left(\frac{1}{\beta^{2}}+\log\frac{1}{\alpha}+ \operatorname{KL}(\rho,\pi)\right).\]

_is sufficient for FUSE-N to achieve power at least \(1-\beta\) at level \(\alpha\)._

**Discussion.** The conditions in Theorems 2 and 3 give the optimal \(\operatorname{MMD}^{2}\) separation rate in \(n\)(Domingo-Enrich et al., 2023, Proposition 2). These results also imply consistency if the prior \(\pi\) assigns non-zero weight to characteristic kernels.6 Applying these results to uniform priors supported on \(r\) points, the KL penalty can be upper bounded as \(\operatorname{KL}(\rho,\pi)\leq\log(r)\). Thus in the _worst_ case, where only a single kernel achieves large \(\operatorname{MMD}^{2}_{k}(p,q)\), the price paid for adaptivity is only \(\log(r)\). In many cases, _most_ of the kernels will give large \(\operatorname{MMD}^{2}_{k}(p,q)\). The posterior will then mirror the prior, and this KL penalty will be even smaller. Thus very large numbers of kernels could be considered, and if all give large MMD values the power would not be greatly affected.

Footnote 6: Under this condition \(\operatorname{KL}(\nu,\pi){<}\infty\) with \(\nu\) the restriction of \(\pi\) to characteristic kernels. \(K_{\nu}\) is characteristic so \(\operatorname{MMD}_{K_{\nu}}(p,q){>}0\iff p\neq q\), and our condition lower bound tends to zero with \(n\to\infty\).

**Additional Technical Results.** Aside from the presentation of our new statistics and tests, we make a number of technical contributions on the way to proving Theorems 2 and 3, as well as proving some additional results. In particular, we give exponential concentration bounds for our statistics under permutations and the null, which do not require bounded kernels. This refined analysis requires the construction of a coupled Rademacher chaos and concentration thereof. We obtain intermediate results using variances from the proofs of Theorems 2 and 3 that could be used in future work to obtain power guarantees under alternative assumptions such as Sobolev spaces (Schrab et al., 2023). Finally, we prove exponential concentration for \(\widetilde{\operatorname{FUSE}}_{1}\) under the alternative and bounded kernels, requiring the proof of a "PAC-Bayesian" bounded differences-type concentration inequality. See the appendix for a more detailed overview.

## 6 Experiments

We compare the test power of MMD-FUSE-N (\(\lambda=\sqrt{n(n-1)}\)) against various MMD-based kernel selective tests (see Section 1 for details) using: the median heuristic (MMD-Median; Gretton et al., 2012), data splitting (MMD-Split; Sutherland et al., 2017), analytic Mean Embeddings and Smooth Characteristic Functions (ME & SCF; Jitkrittum et al., 2016), the MMD Deep kernel (MMD-D; Liu et al., 2020), Automated Machine Learning (AutoML; Kubler et al., 2022), kernel thinning to (Aggregate) Compress Then Test (CTT & ACTT; Domingo-Enrich et al., 2023), and MMD Aggregated (Incomplete) tests (MMDAgg & MMDAggInc; Schrab et al., 2023, 2022). Additional details and code link for experimental reproducibility are provided in Appendix A.

**Distribution on Kernels.** We choose our kernel prior distribution \(\pi\) as uniform over a collection of Gaussian, \(k_{\gamma}^{q}(x,y)=\exp(-\left\lVert x-y\right\rVert_{2}^{2}/2\gamma^{2})\), and Laplace, \(k_{\gamma}^{\ell}(x,y)=\exp(-\sqrt{2}\left\lVert x-y\right\rVert_{1}/\gamma)\), kernels with various bandwidths \(\gamma>0\). These bandwidths are chosen as the uniform discretisation of the interval between half the 5% and twice the 95% quantiles (for robustness) of \(\{\left\lVert z-z^{\prime}\right\rVert_{r}:z,z^{\prime}\in\mathbb{Z}\}\), with \(r\in\{1,2\}\), respectively. This choice is similar to that of Schrab et al. (2023, Section 5.2), who empirically show that ten points for the discretisation is sufficient (Schrab et al., 2023, Figure 6), which we verify in Appendix A.4. This set of distances is permutation-invariant, so Theorem 1 guarantees a well-calibrated test even though the kernels are data-dependent.

**Mixture of Gaussians.** Our first experiments (Figure 1) consider multimodal distributions \(p\) and \(q\), each a \(2\)-dimensional mixture of four Gaussians with means \((\pm\mu,\pm\mu)\) with \(\mu=20\) and diagonal covariances. For \(p\), the four components all have unit variances, while for \(q\) we vary the standard deviation \(\sigma\) of _one_ of the Gaussians, \(\sigma=1\) corresponds to the null hypothesis \(p=q\). Intuitively, an appropriate kernel bandwidth to distinguish \(p\) from \(q\) would correspond to that separating Gaussianswith standard deviations \(1\) and \(\sigma\). This is significantly smaller than the _median_ bandwidth which scales with the distance \(\mu\) between modes.

**Perturbed Uniform.** In Figure 1, we report test power for detecting perturbations on uniform distributions in one and two dimensions. We vary the amplitude \(a\) of two perturbations from \(a=0\) (null) to \(a=1\) (maximum value for the density to remain non-negative). A similar benchmark was first proposed by Schrab et al. (2023, Section 5.5) and considered in several other works (Schrab et al., 2022b; Hagrass et al., 2022; Chatterjee and Bhattacharya, 2023). Different bandwidths are required to detect different amplitudes of the perturbations.

**Galaxy MNIST.** We examine performance on real-world data in Figure 1, through galaxy images (Walmsley et al., 2022) in dimension \(d=3\times 64\times 64=12288\) captured by a ground-based telescope. These consist of four classes:'smooth and cigar-shaped', 'edge-on-disk', 'unbarred spiral', and'smooth and round'. One distribution uniformly samples images from the first three categories, while the other does the same with probability \(1-c\) and uniformly samples a'smooth and round' galaxy image with probability of corruption \(c\in[0,1]\). The null hypothesis corresponds to the case \(c=0\).

**CIFAR 10 vs 10.1.** The aim of this experiment is to detect the difference between images from the CIFAR-10 (Krizhevsky, 2009) and CIFAR-10.1 (Recht et al., 2019) test sets. This is a challenging problem as CIFAR-10.1 was specifically created to consist of new samples from the CIFAR-10 distribution so that it can be used as an alternative test set for models trained on CIFAR-10. Samples from the two distributions are presented in Figure 6 in Appendix A.3(Liu et al., 2020, Figure 5). This benchmark was proposed by Liu et al. (2020, Table 3) who introduced the deep MMD test MMD-D and the MMD-Split test (here referred to as MMD-O to point out that their implementation has been used rather than ours). They also compare to ME and SCF, as well as to C2ST-L and C2ST-S (Lopez-Paz and Oquab, 2017) which correspond to Classifier Two-Sample T

\begin{table}
\begin{tabular}{r c} \hline \hline Tests & Power \\ \hline MMD-FUSE & **0.937** \\ MMDAgg & 0.883 \\ MMD-D & 0.744 \\ CTT & 0.711 \\ MMD-Median & 0.678 \\ ACTT & 0.652 \\ ME & 0.588 \\ AutoML & 0.544 \\ C2ST-L & 0.529 \\ C2ST-S & 0.452 \\ MMD-O & 0.316 \\ MMDAggInc & 0.281 \\ SCF & 0.171 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Test power for detecting the difference between CIFAR-10 and CIFAR-10.1 images with test level \(\alpha=0.05\). The averaged numbers of rejections over \(1000\) repetitions are reported.

Figure 1: Power experiments. The four columns correspond to different settings: Mixture of Gaussians in two dimensions (null \(\sigma=1\)), Perturbed Uniform for \(d\in\{1,2\}\) (null \(a=0\)), and Galaxy MNIST in dimension \(12288\) (null \(c=0\)). In the first row, the deviations away from the null are varied for fixed sample size \(m=n=500\). In the second row, the sample size varies while the deviations are fixed as \(\sigma=1.3\), \(a=0.2\), \(a=0.4\), and \(c=0.15\), for the four respective problems. The plots correspond to the rejections of the null averaged over \(200\) repetitions.

Linear kernels. For the tests slitting the data, \(1000\) images from both datasets are used for parameter selection and/or model training, and \(1021\) other images from each distributions are used for testing. Consequently, tests avoiding data splitting are given the full \(2021\) images from CIFAR-10.1 and \(2021\) images sampled from CIFAR-10.

**Experimental Results of Figure 1.** We observe similar trends in all eight experiments in Figure 1: MMD-FUSE _matches the power_ of state-of-the-art MMDAgg while being _computationally faster_ (both theoretically and practically). These two tests consistently obtain the highest power in every experiment, except when increasing the number of Galaxy MNIST images where MMD-D obtains higher power. However, we observe in Table 1 that MMD-FUSE outperforms MMD-D on another image data problem, also with large sample size. On synthetic data, the deep kernel test MMD-D surprisingly only obtains power similar to MMD-Split in most experiments (even lower in the 2-dimensional perturbed uniform experiment). The two near-linear aggregated variants ACTT and MMDAggInc trade-off a small portion of test power for computational efficiency, with the former outperforming the latter for large sample sizes. The importance of kernel selection is emphasised by the fact that the two tests using the median bandwidth (MMD-Median and CTT) achieve very low power. While the linear-time SCF test, based in the frequency domain, attains high power in the Mixture of Gaussians experiments, it has low power in the three other experiments. Its spatial domain variant ME performs better on Perturbed Uniform \(d\in\{1,2\}\) experiments but in general still has reduced power compared to both linear and quadratic time alternatives. Finally, the AutoML test performs well for fixed sample size \(m=n=500\) (first row of Figure 1), but its power compared to other tests considerably deteriorates as the sample size increases (second row of Figure 1). Overall, MMD-FUSE achieves _state-of-the-art performance_ across all experiments on both low-dimensional synthetic data and high-dimensional real-world data.

**Experimental Results of Table 1.** We report in Table 1 the power achieved by each test on the CIFAR 10 vs 10.1 experiment, which is averaged over \(1000\) repetitions. We observe that MMD-FUSE performs the best and obtains power \(0.937\), which means that out of \(1000\) repetitions, it was 937 times able to distinguish between samples from CIFAR-10 and from CIFAR-10.1. This demonstrates that the images in CIFAR-10.1 do not come from the same distribution as those in CIFAR-10.

**Experimental Results of Figure 7.** We observe in Figure 7 of Appendix A.4 that MMD-FUSE can achieve higher power than MMDAgg in some additional perturbed uniform experiment. We also note that using a relatively small number of kernels (_e.g._, \(10\)) is enough to capture all the required information, and that the test power is retained when further increasing the number of kernels.

## 7 Conclusions

In this work, we propose MMD-FUSE, an MMD-based test which fuses kernels through a soft maximum and a method for learning general two-sample testing parameters in an unsupervised fashion. We demonstrate the empirical performance of MMD-FUSE and show that it achieves the optimal MMD separation rate guaranteeing high test power. This optimality holds with respect to the sample size and likely also for the logarithmic dependence in \(\alpha\), but we believe the dependence on \(\beta\) could be improved in future work; a general question is whether lower bounds in terms of \(\alpha\) and \(\beta\) can be proved. Obtaining separation rates in terms of the \(L^{2}\)-norm between the densities (Schrab et al., 2023) may also be possible but challenging since this distance is independent of the kernel.

An open question is in explaining the significant empirical power advantage of the normalised test over its un-normalised variant, which is currently not reflected in the derived rates. The importance of this normalisation is clear when considering kernels with different bandwidths, leading to vastly different scaling in the un-normalised permutation distributions. Work here could begin with finite-sample concentration guarantees for our normalised statistic or other "studentised" variants, some of which might obtain better performance.

Future work could also examine computationally efficient variants of MMD-FUSE, by either relying on incomplete \(U\)-statistics (Schrab et al., 2022) and leading to suboptimal rates, or by relying on recent ideas of kernel thinning (Domingo-Enrich et al., 2023) which can lead to the same optimal rate under stronger assumptions on the data distributions, or by considering the permutation-free approach of Shekhar et al. (2022). Finally, our two-sample MMD fusing approach could be extended to the HSIC independence framework (Greton et al., 2005, 2008; Albert et al., 2022) and to the KSD goodness-of-fit setting (Chwialkowski et al., 2016; Liu et al., 2016; Schrab et al., 2022).

## Acknowledgements

We would like to thank the anonymous reviewers and area chair for their thorough reading of our work, for their constructive feedback, and for engaging in the discussions, all of which have helped to improve the paper. Felix Biggs and Antonin Schrab both gratefully acknowledge the support from the U.K. Research and Innovation under the EPSRC grant EP/S021566/1. Arthur Gretton acknowledges support from the Gatsby Charitable Foundation.

## References

* Albert et al. (2022) Melisande Albert, Beatrice Laurent, Amandine Marrel, and Anouar Meynaoui. Adaptive test of independence based on HSIC measures. _The Annals of Statistics_, 50(2):858-879, 2022. _(Cited on pages 4 and 10.)_
* Alquier (2021) Pierre Alquier. User-friendly introduction to PAC-Bayes bounds. _CoRR_, abs/2110.11216, 2021. URL [https://arxiv.org/abs/2110.11216](https://arxiv.org/abs/2110.11216). _(Cited on page 4.)_
* Aronszajn (1950) Nachman Aronszajn. Theory of reproducing kernels. _Transactions of the American Mathematical Society_, 68(3):337-404, 1950. _(Cited on page 3.)_
* Benton et al. (2019) Gregory Benton, Wesley J Maddox, Jayson Salkey, Julio Albinati, and Andrew Gordon Wilson. Function-space distributions over kernels. In H. Wallach, H. Larochelle, A. Beygelzimer, F. Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper_files/paper/2019/file/39d929972619274cc9066307f707d002-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/39d929972619274cc9066307f707d002-Paper.pdf). _(Cited on page 3.)_
* Biggs and Guedj (2021) Felix Biggs and Benjamin Guedj. Differentiable PAC-Bayes objectives with partially aggregated neural networks. _Entropy_, 23(10):1280, 2021. doi: 10.3390/e23101280. URL [https://doi.org/10.3390/e23101280](https://doi.org/10.3390/e23101280). _(Cited on page 4.)_
* Biggs and Guedj (2022) Felix Biggs and Benjamin Guedj. Non-vacuous generalisation bounds for shallow neural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 1963-1981. PMLR, 2022a. URL [https://proceedings.mlr.press/v162/biggs22a.html](https://proceedings.mlr.press/v162/biggs22a.html). _(Cited on page 4.)_
* Biggs and Guedj (2022) Felix Biggs and Benjamin Guedj. On margins and derandomisation in PAC-Bayes. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, _Proceedings of The 25th International Conference on Artificial Intelligence and Statistics_, volume 151 of _Proceedings of Machine Learning Research_, pages 3709-3731. PMLR, 28-30 Mar 2022b. URL [https://proceedings.mlr.press/v151/biggs22a.html](https://proceedings.mlr.press/v151/biggs22a.html). _(Cited on page 4.)_
* Biggs and Guedj (2023) Felix Biggs and Benjamin Guedj. Tighter pac-bayes generalisation bounds by leveraging example difficulty. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pages 8165-8182. PMLR, 25-27 Apr 2023. URL [https://proceedings.mlr.press/v206/biggs23a.html](https://proceedings.mlr.press/v206/biggs23a.html). _(Cited on page 4.)_
* Biggs et al. (2022) Felix Biggs, Valentina Zantedeschi, and Benjamin Guedj. On margins and generalisation for voting classifiers. In _NeurIPS_, 2022. doi: 10.48550/arXiv.2206.04607. URL [https://doi.org/10.48550/arXiv.2206.04607](https://doi.org/10.48550/arXiv.2206.04607). _(Cited on page 4.)_
* Boucheron et al. (2013) Stephane Boucheron, Gabor Lugosi, and Pascal Massart. _Concentration inequalities: A nonasymptotic theory of independence_. Oxford university press, 2013. _(Cited on page 31.)_
* Bradbury et al. (2018) James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL [http://github.com/google/jax](http://github.com/google/jax). _(Cited on page 18.)_
* Carcamo et al. (2022) Javier Carcamo, Antonio Cuevas, and Luis-Alberto Rodriguez. A uniform kernel trick for high-dimensional two-sample problems. _arXiv preprint arXiv:2210.02171_, 2022. _(Cited on page 6.)_
* Caron et al. (2018) Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In Hugo Larochelle,Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Catoni [2007] Olivier Catoni. _PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning_. Institute of Mathematical Statistics lecture notes-monograph series. Institute of Mathematical Statistics, 2007. ISBN 9780940600720. URL [https://books.google.fr/books?id=acnaAAAAAAAAJ](https://books.google.fr/books?id=acnaAAAAAAAAJ).
* Chatterjee and Bhattacharya [2023] Anirban Chatterjee and Bhaswar B. Bhattacharya. Boosting the power of kernel two-sample tests. _arXiv preprint arXiv:2302.10687_, 2023.
* Chen et al. [2020a] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 1597-1607. PMLR, 2020a.
* Chen et al. [2020b] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E. Hinton. Big self-supervised models are strong semi-supervised learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020b.
* Chen and He [2021] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021_, pages 15750-15758. Computer Vision Foundation / IEEE, 2021.
* Chen et al. [2020c] Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. _CoRR_, abs/2003.04297, 2020c. URL [https://arxiv.org/abs/2003.04297](https://arxiv.org/abs/2003.04297).
* Cherfaoui et al. [2022] Farah Cherfaoui, Hachem Kadri, Sandrine Anthoine, and Liva Ralaivola. A discrete RKHS standpoint for Nystrom MMD. _HAL preprint hal-03651849_, 2022.
* Chugg et al. [2023] Ben Chugg, Hongjian Wang, and Aaditya Ramdas. A unified recipe for deriving (time-uniform) pac-bayes bounds. _CoRR_, abs/2302.03421, 2023. doi: 10.48550/arXiv.2302.03421. URL [https://doi.org/10.48550/arXiv.2302.03421](https://doi.org/10.48550/arXiv.2302.03421).
* Chwialkowski et al. [2014] Kacper Chwialkowski, Dino Sejdinovic, and Arthur Gretton. A wild bootstrap for degenerate kernel tests. In _Advances in neural information processing systems_, pages 3608-3616, 2014.
* Chwialkowski et al. [2016] Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of fit. In _International Conference on Machine Learning_, pages 2606-2615. PMLR, 2016.
* Chwialkowski et al. [2015] Kacper P Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, and Arthur Gretton. Fast two-sample testing with analytic representations of probability measures. In _Advances in Neural Information Processing Systems_, volume 28, pages 1981-1989, 2015.
* Deka and Sutherland [2022] Namrata Deka and Danica J. Sutherland. Mmd-b-fair: Learning fair representations with statistical testing. _CoRR_, abs/2211.07907, 2022. doi: 10.48550/arXiv.2211.07907. URL [https://doi.org/10.48550/arXiv.2211.07907](https://doi.org/10.48550/arXiv.2211.07907).
* Domingo-Enrich et al. [2023] Carles Domingo-Enrich, Raaz Dwivedi, and Lester Mackey. Compress then test: Powerful kernel testing in near-linear time. _The 26th International Conference on Artificial Intelligence and Statistics, AISTATS 2023_, 2023.
* Donsker and Varadhan [1975] M. D. Donsker and S. R. S. Varadhan. Asymptotic evaluation of certain markov process expectations for large time, i. _Communications on Pure and Applied Mathematics_, 28(1):1-47, 1975. doi: [https://doi.org/10.1002/cpa.3160280102](https://doi.org/10.1002/cpa.3160280102). URL [https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.3160280102](https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.3160280102).
* 669, 1956. doi: 10.1214/aoms/1177728174. URL [https://doi.org/10.1214/aoms/1177728174](https://doi.org/10.1214/aoms/1177728174).
* Dwivedi and Mackey [2021] Raaz Dwivedi and Lester Mackey. Kernel thinning. In Mikhail Belkin and Samory Kpotufe, editors, _Conference on Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado,USA_, volume 134 of _Proceedings of Machine Learning Research_, page 1753. PMLR, 2021. URL [http://proceedings.mlr.press/v134/dwivedi21a.html](http://proceedings.mlr.press/v134/dwivedi21a.html). _(Cited on page 5.)_
* Dziugaite and Roy (2017) Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. _Conference on Uncertainty in Artificial Intelligence 33._, 2017. _(Cited on page 4.)_
* Dziugaite and Roy (2018) Gintare Karolina Dziugaite and Daniel M. Roy. Data-dependent PAC-Bayes priors via differential privacy. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 8440-8450, 2018. URL [https://proceedings.neurips.cc/paper/2018/hash/9a0ee0a9e7a42d2d69b8f86b3a0756b1-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/9a0ee0a9e7a42d2d69b8f86b3a0756b1-Abstract.html). _(Cited on page 4.)_
* Dziugaite et al. (2021) Gintare Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino, and Daniel Roy. On the role of data in PAC-Bayes. In Arindam Banerjee and Kenji Fukumizu, editors, _The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event_, volume 130 of _Proceedings of Machine Learning Research_, pages 604-612. PMLR, 2021. URL [http://proceedings.mlr.press/v130/karolina-dziugaite21a.html](http://proceedings.mlr.press/v130/karolina-dziugaite21a.html). _(Cited on page 4.)_
* Florian et al. (2023) Bruck Florian, Jean-David Fermanian, and Aleksey Min. Distribution free mmd tests for model selection with estimated parameters. _arXiv preprint arXiv:2305.07549_, 2023. _(Cited on page 5.)_
* Fromont et al. (2012) Magalie Fromont, Beatrice Laurent, Matthieu Lerasle, and Patricia Reynaud-Bouret. Kernels based tests with non-asymptotic bootstrap approaches for two-sample problems. In _Conference on Learning Theory_, volume 23 of _Journal of Machine Learning Research Proceedings_, 2012. _(Cited on page 4.)_
* Fromont et al. (2013) Magalie Fromont, Beatrice Laurent, and Patricia Reynaud-Bouret. The two-sample problem for Poisson processes: Adaptive tests with a nonasymptotic wild bootstrap approach. _The Annals of Statistics_, 41(3):1431-1461, 2013.
* Gao and Shao (2022) Hanjia Gao and Xiaofeng Shao. Two sample testing in high dimension via maximum mean discrepancy. _arXiv preprint arXiv:2109.14913_, 2022. _(Cited on page 5.)_
* Gretton et al. (2005) Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Scholkopf. Measuring statistical dependence with Hilbert-Schmidt norms. In _International Conference on Algorithmic Learning Theory_. Springer, 2005. _(Cited on page 10.)_
* Gretton et al. (2008) Arthur Gretton, Kenji Fukumizu, Choon H Teo, Le Song, Bernhard Scholkopf, and Alex J Smola. A kernel statistical test of independence. In _Advances in Neural Information Processing Systems_, volume 1, pages 585-592, 2008. _(Cited on page 10.)_
* Gretton et al. (2009) Arthur Gretton, Kenji Fukumizu, Zaid Harchaoui, and Bharath K Sriperumbudur. A fast, consistent kernel two-sample test. _Advances in Neural Information Processing Systems_, 22, 2009. _(Cited on page 5.)_
* Gretton et al. (2012a) Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander J. Smola. A kernel two-sample test. _J. Mach. Learn. Res._, 13:723-773, 2012a. doi: 10.5555/2503308.2188410. URL [https://dl.acm.org/doi/10.5555/2503308.2188410](https://dl.acm.org/doi/10.5555/2503308.2188410). _(Cited on pages 1, 5, 8, and 18.)_
* Gretton et al. (2012b) Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil, Kenji Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for large-scale two-sample tests. In _Advances in Neural Information Processing Systems_, volume 1, pages 1205-1213, 2012b. _(Cited on pages 1, 2, and 5.)_
* A new approach to self-supervised learning. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. _(Cited on page 2.)_
Benjamin Guedj. A primer on PAC-Bayesian learning. In _Proceedings of the second congress of the French Mathematical Society_, volume 33, 2019. URL [https://arxiv.org/abs/1901.05353](https://arxiv.org/abs/1901.05353).
* Haddouche and Guedj (2022) Maxime Haddouche and Benjamin Guedj. Online pac-bayes learning. In _NeurIPS_, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/a4d991d581accd2955a1e1928f4e6965-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/a4d991d581accd2955a1e1928f4e6965-Abstract-Conference.html). _(Cited on page 4.)_
* Haddouche and Guedj (2023) Maxime Haddouche and Benjamin Guedj. Pac-bayes generalisation bounds for heavy-tailed losses through supermartingales. _Trans. Mach. Learn. Res._, 2023, 2023. URL [https://openreview.net/forum?id=qxwt6F3sf](https://openreview.net/forum?id=qxwt6F3sf). _(Cited on page 4.)_
* Hagrass et al. (2022) Omar Hagrass, Bharath K. Sriperumbudur, and Bing Li. Spectral regularized kernel two-sample tests. _arXiv preprint arXiv:2212.09201_, 2022. _(Cited on pages 2 and 9.)_
* He et al. (2020) Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020_, pages 9726-9735. Computer Vision Foundation / IEEE, 2020. _(Cited on page 2.)_
* Hemerik and Goeman (2018) Jesse Hemerik and Jelle Goeman. Exact testing with random permutations. _TEST_, 27(4):811-825, Dec 2018. ISSN 1863-8260. doi: 10.1007/s11749-017-0571-1. URL [https://doi.org/10.1007/s11749-017-0571-1](https://doi.org/10.1007/s11749-017-0571-1). _(Cited on page 4.)_
* Hinton and Salakhutdinov (2006) Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. _science_, 313(5786):504-507, 2006. _(Cited on page 2.)_
* Jitkrittum et al. (2016) Wittawat Jitkrittum, Zoltan Szabo, Kacper P Chwialkowski, and Arthur Gretton. Interpretable distribution features with maximum testing power. In _Advances in Neural Information Processing Systems_, volume 29, pages 181-189, 2016. _(Cited on pages 2, 8, 8, and 18.)_
* Kim and Ramdas (2023) Ilmun Kim and Aaditya Ramdas. Dimension-agnostic inference using cross u-statistics. _2023. _(Cited on page 5.)_
* Kim et al. (2022) Ilmun Kim, Sivaraman Balakrishnan, and Larry Wasserman. Minimax optimality of permutation tests. _Annals of Statistics_, 50(1):225-251, February 2022. ISSN 0090-5364. doi: 10.1214/21-AOS2103. Funding Information: Funding. This work was partially supported by the NSF Grant DMS-17130003 and EP-SRC Grant EP/N031938/1. Publisher Copyright: \(\copyright\) Institute of Mathematical Statistics, 2022. _(Cited on pages 3, 5, 27, 34, and 35.)_
* Kirchler et al. (2020) Matthias Kirchler, Shahryar Khorasani, Marius Kloft, and Christoph Lippert. Two-sample testing using deep learning. In Silvia Chiappa and Roberto Calandra, editors, _The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy]_, volume 108 of _Proceedings of Machine Learning Research_, pages 1387-1398. PMLR, 2020. URL [http://proceedings.mlr.press/v108/kirchler20a.html](http://proceedings.mlr.press/v108/kirchler20a.html). _(Cited on page 5.)_
* Krizhevsky (2009) Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. _(Cited on pages 9 and 21.)_
* Kubler et al. (2020) J. M. Kubler, W. Jitkrittum, B. Scholkopf, and K. Muandet. Learning kernel tests without data splitting. In _Advances in Neural Information Processing Systems 33_, pages 6245-6255. Curran Associates, Inc., 2020. _(Cited on pages 2 and 5.)_
* Kubler et al. (2022) Jonas M Kubler, Wittawat Jitkrittum, Bernhard Scholkopf, and Krikamol Muandet. A witness two-sample test. In _International Conference on Artificial Intelligence and Statistics_, pages 1403-1419. PMLR, 2022a. _(Cited on page 2.)_
* Kubler et al. (2020) Jonas M Kubler, Vincent Stimper, Simon Buchholz, Krikamol Muandet, and Bernhard Scholkopf. AutoML two-sample test. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022b. _(Cited on pages 2, 8, and 18.)_
* Lacasse et al. (2006) Alexandre Lacasse, Francois Laviolette, Mario Marchand, Pascal Germain, and Nicolas Usunier. PAC-Bayes bounds for the risk of the majority vote and the variance of the Gibbs classifier. In Bernhard Scholkopf, John C. Platt, and Thomas Hofmann, editors, _Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006_, pages 769-776. MIT Press, 2006. URL [https://proceedings.neurips.cc/paper/2006/hash/779efbd24d5a7e37ce8dc93e7c04d572-Abstract.html](https://proceedings.neurips.cc/paper/2006/hash/779efbd24d5a7e37ce8dc93e7c04d572-Abstract.html). _(Cited on page 4.)_Alexandre Lacasse, Francois Laviolette, Mario Marchand, and Francis Turgeon-Boutin. Learning with randomized majority votes. In Jose L. Balcazar, Francesco Bonchi, Aristides Gionis, and Michele Sebag, editors, _Machine Learning and Knowledge Discovery in Databases, European Conference, ECML PKDD 2010, Barcelona, Spain, September 20-24, 2010, Proceedings, Part II_, volume 6322 of _Lecture Notes in Computer Science_, pages 162-177. Springer, 2010. doi: 10.1007/978-3-642-15883-4_11. URL [https://doi.org/10.1007/978-3-642-15883-4_11](https://doi.org/10.1007/978-3-642-15883-4_11). _(Cited on page 4.)_
* Lee [1990] Justin Lee. \(U\)_-statistics: Theory and Practice_. Citeseer, 1990.
* Lever et al. [2013] Guy Lever, Francois Laviolette, and John Shawe-Taylor. Tighter PAC-Bayes bounds through distribution-dependent priors. _Theoretical Computer Science_, 473:4-28, February 2013. ISSN 0304-3975. doi: 10.1016/j.tcs.2012.10.013. URL [https://linkinghub.elsevier.com/retrieve/pii/S0304397512009346](https://linkinghub.elsevier.com/retrieve/pii/S0304397512009346). _(Cited on page 4.)_
* Li and Yuan [2019] Tong Li and Ming Yuan. On the optimality of gaussian kernel based nonparametric tests against smooth alternatives. _arXiv preprint arXiv:1909.03302_, 2019.
* Li et al. [2021] Yazhe Li, Roman Pogodin, Danica J. Sutherland, and Arthur Gretton. Self-supervised learning with kernel dependence maximization. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 15543-15556, 2021. URL [https://proceedings.neurips.cc/paper/2021/hash/83004190b1793d7aa15f8d0d49a13eba-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/83004190b1793d7aa15f8d0d49a13eba-Abstract.html). _(Cited on page 2.)_
* Lim et al. [2020] Jen Ning Lim, Makoto Yamada, Wittawat Jitkrittum, Yoshikazu Terada, Shigeyuki Matsui, and Hidetoshi Shimodaira. More powerful selective kernel tests for feature selection. In _International Conference on Artificial Intelligence and Statistics_, pages 820-830. PMLR, 2020.
* Liu et al. [2020] Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, Arthur Gretton, and Dougal J Sutherland. Learning deep kernels for non-parametric two-sample tests. In _International Conference on Machine Learning_, 2020.
* Liu et al. [2021] Feng Liu, Wenkai Xu, Jie Lu, and Danica J. Sutherland. Meta two-sample testing: Learning kernels for testing with limited data. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 5848-5860, 2021. URL [https://proceedings.neurips.cc/paper/2021/hash/2e6d9c6052e99fcdfa61d9b9da273ca2-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/2e6d9c6052e99fcdfa61d9b9da273ca2-Abstract.html). _(Cited on page 2.)_
* Liu et al. [2016] Qiang Liu, Jason Lee, and Michael Jordan. A kernelized Stein discrepancy for goodness-of-fit tests. In _International Conference on Machine Learning_, pages 276-284. PMLR, 2016.
* Lopez-Paz and Oquab [2017] David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017. URL [https://openreview.net/forum?id=SJKfE5xx](https://openreview.net/forum?id=SJKfE5xx). _(Cited on page 9.)_
* Masegosa et al. [2020] Andres R. Masegosa, Stephan Sloth Lorenzen, Christian Igel, and Yevgeny Seldin. Second order PAC-Bayesian bounds for the weighted majority vote. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/386854131f58a5634a6056f03626e00-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/386854131f58a5634a6056f03626e00-Abstract.html). _(Cited on page 4.)_
* 1283, 1990. doi: 10.1214/aop/1176990746. URL [https://doi.org/10.1214/aop/1176990746](https://doi.org/10.1214/aop/1176990746). _(Cited on page 34.)_
* Maurer [2004] Andreas Maurer. A note on the PAC-Bayesian theorem. _CoRR_, cs.LG/0411099, 2004. URL [http://arxiv.org/abs/cs.LG/0411099](http://arxiv.org/abs/cs.LG/0411099). _(Cited on page 4.)_
* McAllester [1998] David A McAllester. Some PAC-Bayesian theorems. In _Proceedings of the eleventh annual conference on Computational Learning Theory_, pages 230-234. ACM, 1998. _(Cited on page 4.)_Alfred Muller. Integral probability metrics and their generating classes of functions. _Advances in Applied Probability_, 1:429-443, 1997.
* Perez-Ortiz et al. [2021] Maria Perez-Ortiz, Omar Rivasplata, John Shawe-Taylor, and Csaba Szepesvari. Tighter risk certificates for neural networks. _Journal of Machine Learning Research_, 22(227):1-40, 2021. URL [http://jmlr.org/papers/v22/20-879.html](http://jmlr.org/papers/v22/20-879.html).
* Ramdas et al. [2015] Aaditya Ramdas, Sashank Jakkam Reddi, Barnabas Poczos, Aarti Singh, and Larry Wasserman. On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 29, 2015. _(Cited on page 2.)_
* Recht et al. [2019] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 5389-5400. PMLR, 09-15 Jun 2019. URL [https://proceedings.mlr.press/v97/recht19a.html](https://proceedings.mlr.press/v97/recht19a.html).
* Reddi et al. [2015] Sashank Reddi, Aaditya Ramdas, Barnabas Poczos, Aarti Singh, and Larry Wasserman. On the high dimensional power of a linear-time two sample test under mean-shift alternatives. In _Artificial Intelligence and Statistics_, pages 772-780. PMLR, 2015.
* Rindt et al. [2021] David Rindt, Dino Sejdinovic, and David Steinsaltz. Consistency of permutation tests of independence using distance covariance, hsic and dhsic. _Stat_, 10(1):e364, 2021. doi: [https://doi.org/10.1002/sta4.364](https://doi.org/10.1002/sta4.364). URL [https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.364](https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.364). e364.
* Romano and Wolf [2005] Joseph P Romano and Michael Wolf. Exact and approximate stepdown methods for multiple hypothesis testing. _Journal of the American Statistical Association_, 100(469):94-108, 2005.
* 9, 2013. doi: 10.1214/ECP.v18-2865. URL [https://doi.org/10.1214/ECP.v18-2865](https://doi.org/10.1214/ECP.v18-2865).
* Schrab et al. [2022a] Antonin Schrab, Benjamin Guedj, and Arthur Gretton. KSD aggregated goodness-of-fit test. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022_, 2022a.
* Schrab et al. [2022b] Antonin Schrab, Ilmun Kim, Benjamin Guedj, and Arthur Gretton. Efficient aggregated kernel tests using incomplete \(U\)-statistics. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022_, 2022b.
* Schrab et al. [2023] Antonin Schrab, Ilmun Kim, Melisande Albert, Beatrice Laurent, Benjamin Guedj, and Arthur Gretton. MMD aggregated two-sample test. _Journal of Machine Learning Research_, 24(194):1-81, 2023. URL [http://jmlr.org/papers/v24/21-1289.html](http://jmlr.org/papers/v24/21-1289.html).
* Seeger et al. [2001] Matthias Seeger, John Langford, and Nimrod Megiddo. An improved predictive accuracy bound for averaging classifiers. In _Proceedings of the 18th International Conference on Machine Learning_, number CONF, pages 290-297, 2001.
* Seldin et al. [2012] Yevgeny Seldin, Francois Laviolette, Nicolo Cesa-Bianchi, John Shawe-Taylor, and Peter Auer. PAC-Bayesian inequalities for martingales. In Nando de Freitas and Kevin P. Murphy, editors, _Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, Catalina Island, CA, USA, August 14-18, 2012_, page 12. AUAI Press, 2012. URL [https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=2341&proceeding_id=28](https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=2341&proceeding_id=28).
* Shekhar et al. [2022] Shubhanshu Shekhar, Ilmun Kim, and Aaditya Ramdas. A permutation-free kernel two-sample test. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022.

Bharath K Sriperumbudur, Kenji Fukumizu, and Gert RG Lanckriet. Universality, characteristic kernels and RKHS embedding of measures. _Journal of Machine Learning Research_, 12(7), 2011.
* Sutherland et al. (2017) Danica J Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy. In _International Conference on Learning Representations_, 2017. _(Cited on pages 1, 2, 8, and 18.)_
* Wainwright (2019) Martin J. Wainwright. _High-Dimensional Statistics: A Non-Asymptotic Viewpoint_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, Cambridge, 2019. doi: 10.1017/9781108627771. _(Cited on page 32.)_
* Walmsley et al. (2021) Mike Walmsley, Chris J. Lintott, Tobias Geron, Sandor Kruk, Coleman Krawczyk, Kyle W. Willett, Steven Bamford, William Keel, Lee S. Kelvin, Lucy Fortson, Karen L. Masters, Vihang Mehta, Brooke D. Simmons, Rebecca J. Smethurst, Elisabeth M. Baeten, and Christine Macmillan. Galaxy zoo decals: Detailed visual morphology measurements from volunteers and deep learning for 314, 000 galaxies. _Monthly Notices of the Royal Astronomical Society_, 509:3966-3988, 2022. _(Cited on pages 9 and 20.)_
* Wu et al. (2021) Yi-Shan Wu, Andres R. Masegosa, Stephan Sloth Lorenzen, Christian Igel, and Yevgeny Seldin. Chebyshev-Cantelli PAC-Bayes-Bennett inequality for the weighted majority vote. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 12625-12636, 2021. URL [https://proceedings.neurips.cc/paper/2021/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html). _(Cited on page 4.)_
* Yamada et al. (2019) Makoto Yamada, Denny Wu, Yao-Hung Hubert Tsai, Hirofumi Ohta, Ruslan Salakhutdinov, Ichiro Takeuchi, and Kenji Fukumizu. Post selection inference with incomplete maximum mean discrepancy estimator. In _International Conference on Learning Representations_, 2019. _(Cited on page 5.)_
* Zantedeschi et al. (2021) Valentina Zantedeschi, Paul Viallard, Emilie Morvant, Remi Emonet, Amaury Habrard, Pascal Germain, and Benjamin Guedj. Learning stochastic majority votes by minimizing a PAC-Bayes generalization bound. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 455-467, 2021. URL [https://proceedings.neurips.cc/paper/2021/hash/0415740eaa4d9decbc8da001d3fd805f-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/0415740eaa4d9decbc8da001d3fd805f-Abstract.html). _(Cited on page 4.)_
* Zaremba et al. (2013) Wojciech Zaremba, Arthur Gretton, and Matthew Blaschko. B-test: A non-parametric, low variance kernel two-sample test. _Advances in neural information processing systems_, 26, 2013. _(Cited on page 5.)_
* Zbontar et al. (2021) Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 12310-12320. PMLR, 2021. URL [http://proceedings.mlr.press/v139/zbontar21a.html](http://proceedings.mlr.press/v139/zbontar21a.html). _(Cited on page 2.)_
* Zhao and Meng (2015) Ji Zhao and Deyu Meng. FastMMD: Ensemble of circular discrepancy for efficient two-sample test. _Neural computation_, 27(6):1345-1372, 2015. _(Cited on page 5.)_
* Zhou et al. (2019) Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P. Adams, and Peter Orbanz. Non-vacuous generalization bounds at the ImageNet scale: A PAC-Bayesian compression approach. In _International Conference on Learning Representations_, 2019. URL [https://openreview.net/forum?id=BJgqqsAct7](https://openreview.net/forum?id=BJgqqsAct7). _(Cited on page 4.)_

## Overview of Appendices

* We give further experimental details, discuss time complexity of our results while graphing run-times.
* We discuss how \(\widehat{\mathrm{FUSE}}_{1}\) can be expressed in a simple form for certain uncountable priors.
* We prove exponential concentration bounds for both our statistics under the null and permutations.
* We prove exponential concentration bounds for the \(\widehat{\mathrm{FUSE}}_{1}\) statistic under the alternative hypothesis.
* We prove the power results stated in Section 5 and other interesting intermediate results.

## Appendix A Additional Experimental Details

### Code and licenses

Our implementation of MMD-FUSE in Jax (Bradbury et al., 2018), as well as the code for the reproducibility of the experiments, are made publicly available at:

[https://github.com/antoninschrab/mmdfuse-paper](https://github.com/antoninschrab/mmdfuse-paper)

Our code is under the MIT License, and we also implement ourselves the MMD-Median (Gretton et al., 2012a) and MMD-Split (Sutherland et al., 2017) tests. For ME & SCF (Jitkrittum et al., 2016), MMD-D (Liu et al., 2020), AutoML (Kubler et al., 2022b), CIT & ACTT (Domingo-Enrich et al., 2023), MMDAgg & MMDAggInc (Schrab et al., 2023, 2022b), we use the implementations of the respective authors, which are all under the MIT license.

The experiments were run on an AMD Ryzen Threadripper 3960X 24 Cores 128Gb RAM CPU at 3.8GHz and on an NVIDIA RTX A5000 24Gb Graphics Card, with a compute time of a couple of hours.

### Test parameters

In general, we use the default test parameters recommended by the authors of the tests (listed above). For the ME and SCF tests, ten test locations are chosen on half of the data. AutoML is run with the recommended training time limit of one minute.

**Kernels.** As explained in Section 6 (SSDistribution on Kernels), for MMD-Fuse, we use Gaussian and Laplace kernels with bandwidths in \(\{q_{5\%}^{r}+i(q_{95\%}^{r}-q_{55\%}^{r})/9:i=0,\ldots,9\}\) where \(q_{55\%}^{r}\) is half the \(5\%\) quantile of all the inter-sample distances \(\{\|z-z^{\prime}\|_{r}:z,z^{\prime}\in\mathbf{Z}\}\) with \(r=1\) and \(r=2\) for Laplace and Gaussian kernels, respectively. Similarly, \(q_{95\%}^{r}\) is twice the \(95\%\) quantile.

MMD-Split selects a Gaussian kernel on half of the data with bandwidth in \(\{q_{5\%}^{r}+i(q_{95\%}^{r}-q_{5\%}^{r})/99:i=0,\ldots,99\}\) by maximizing a proxy for asymptotic power which is the ratio of the estimated MMD with its estimated standard deviation under the alternative (Liu et al., 2020, Equation 3). The MMD test is then run on the other half with the selected kernel.

CTT and MMD-Median both use a Gaussian kernel with bandwidth the median of \(\{\|z-z^{\prime}\|_{2}:z,z^{\prime}\in\mathbf{Z}\}\), while ACTT is run with Gaussian kernels with bandwidths in \(\{q_{5\%}^{2}+i(q_{95\%}^{2}-q_{5\%}^{2})/9:i=0,\ldots,9\}\).

The MMDAgg and MMDAggInc tests are run with their default implementations, which similarly use collections of 20 kernels split equally between Gaussian and Laplace kernels with ten bandwidths each, but they use a different (non-uniform) discretisation of the intervals \([q_{95\%}^{r},q_{5\%}^{r}]\).

**Permutations.** For MMD-Median, MMD-Split, and MMD-FUSE, we use \(2000\) permutations to estimate the quantiles. MMDAgg and MMDAggInc use \(2000+2000\) and \(500+500\) permutations, respectively, to approximate the quantiles and the multiple testing correction. The CTT and ACTT tests are run with \(39\) and \(299+200\) permutations, respectively. AutoML uses \(10000\) permutations

[MISSING_PAGE_FAIL:19]

Figure 4: Perturbed Uniform \(d=2\). Two-dimensional uniform densities with two perturbations per dimension are plotted for various amplitudes \(a\).

Figure 5: Galaxy MNIST (Walmsley et al., 2022) images in dimension \(3\times 64\times 64\) across four categories: ‘smooth cigar’ (_first row_), ‘edge on disk’ (_second row_), ‘unbarred spiral’ (_third row_), and ‘smooth round’ (_fourth row_).

Figure 6: Images from the CIFAR-10 (Krizhevsky, 2009) and CIFAR-10.1 (Recht et al., 2019) tests sets. This figure corresponds to Figure 5 of Liu et al. (2020).

### Experiment varying the number of kernels

In Figure 7, we provide an additional perturbed uniform experiment where we vary the number of kernels for MMD-FUSE and for MMDAgg. We consider the task of detecting six one-dimensional perturbations with amplitude \(a=0.5\) for sample sizes \(m=n=500\) while varying the number of Gaussian and Laplace kernels from \(10\) to \(1000\).

Firstly, we observe that MMD-FUSE achieves higher power than MMDAgg in this setting. Secondly, both MMD-FUSE and MMDAgg retain their power when increasing the number of bandwidths. This matches the empirical observation of Schrab et al. (2023, Section 5.7) that a discretisation of \(10\) bandwidths is enough to capture all the information in certain two-sample problems and that using a finer discretisation (which is computationally more expensive) does not improve the test power.

### Time complexity and runtimes

The time complexity of MMD-FUSE is

\[\mathcal{O}\Big{(}K\,B\,\big{(}m^{2}+n^{2}\big{)}\Big{)} \tag{5}\]

where \(m\) and \(n\) are the sizes of the two samples, \(K\) is the number of kernels fused, and \(B\) is the number of permutations to estimate the quantile. Note that this is an improvement over the time complexity of MMDAgg (Schrab et al., 2023)

\[\mathcal{O}\Big{(}K\,\big{(}B+B^{\prime}\big{)}\big{(}m^{2}+n^{2}\big{)}\Big{)} \tag{6}\]

where the extra parameter \(B^{\prime}\) corresponds to the number of permutations used to estimate the multiple testing correction, often set as \(B^{\prime}=B\) in practice (Schrab et al., 2023, Section 5.2). We indeed observe in Figure 8 that MMD-FUSE runs twice as fast as MMDAgg.7

Footnote 7: The two constants hidden in the \(\mathcal{O}\)-notations of Equations (5) and (6) are exactly the same, which is indeed verified by our empirical observations.

While the runtimes of most tests should not depend on the type of data (only on the sample size and on the dimension), we note that the runtimes of tests relying on optimisation (_e.g._ ME & SCF) can be affected. In the experiment of Figure 8, we consider samples from multivariate Gaussians centred at zero with covariance matrices \(I_{d}\) and \(\sigma I_{d}\) with \(\sigma=1.1\). We vary both the sample sizes and the dimensions.

Figure 7: Power experiment with perturbed uniform samples while varying the number of kernels.

Figure 8: Test runtimes plotted using logarithmic scales. In the LHS figure, we vary the sample size for fixed dimension \(10\). In the RHS figure, we vary the dimension for fixed sample size \(500\). The mean and standard deviations of the test runtimes over ten repetitions are reported. Recall that the tests are run with their default parameters with different numbers of permutations, and that AutoML has a training time parameter which is set to \(60\) seconds by default, as described in Appendix A.2.

Closed Form Mean Kernel

We have observed in the main text that in some cases where the prior has non-finite support, \(\widehat{\mathrm{FUSE}}_{1}\) still gives a straightforwardly expressed statistic. This happens because for certain kernel choices, the expectation of the kernel with respect to some parameter is still a closed form kernel. We give one example of such a statistic here.

**Theorem 4**.: _For any fixed \(\alpha>0\), the following is an example of a \(\widehat{\mathrm{FUSE}}_{1}\) statistic:_

\[\widehat{\mathrm{FUSE}}_{1}^{rq}(\mathbf{Z})=\sup_{R>0}\,\widehat{\mathrm{MMD}}_{k _{rq(\alpha,\sqrt{R}_{0})}}^{2}(\mathbf{Z})-\alpha\cdot\frac{\log R+1/R-1}{\lambda}\]

_where \(k_{rq(\alpha,\eta)}=(1+\|x-y\|^{2}/2\eta^{2})^{-\alpha}\) is a rational quadratic kernel, and \(\eta_{0}\) is the "prior" bandwidth (which can essentially be absorbed into the data scaling)._

Proof of Theorem 4.: Firstly, we define a general rational quadratic kernel

\[k_{rq(\alpha,\eta)}(x,y)=\left(1+\frac{\|x-y\|_{2}^{2}}{2\eta^{2}}\right)^{- \alpha}.\]

Note that \(k_{g}(r)=e^{-\tau r^{2}/2}\) with \(r=\|x-y\|_{2}\) is a bounded kernel with parameter \(\tau\). If we take the expectation of \(\tau\) with respect to a Gamma distribution,

\[\mathbb{E}_{\tau\sim\Gamma(\alpha,\beta)}e^{-\tau r^{2}/2} =\frac{\beta^{\alpha}}{\Gamma(\alpha)}\int_{-\infty}^{\infty}\tau ^{\alpha-1}\exp(-\tau\beta)\exp(-\tau r^{2}/2)\,\mathrm{d}\tau\] \[=\frac{\beta^{\alpha}}{\Gamma(\alpha)}\,\Gamma(\alpha)\left(\beta +\frac{r^{2}}{2}\right)^{-\alpha}\] \[=\left(1+\frac{r^{2}}{2\beta}\right)^{-\alpha}=k_{rq(\alpha,\sqrt {\beta})}(r)\]

where \(\Gamma\) denotes the gamma function. The KL divergence between Gamma distributions is

\[\mathrm{KL}(\Gamma(\alpha,\beta),\Gamma(\alpha_{0},\beta_{0}))=(\alpha-\alpha_ {0})\psi(\alpha)+\log\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha)}+\alpha_{0}\log R +\alpha\left(\frac{1}{R}-1\right)\]

where \(R=\beta/\beta_{0}\) and \(\psi(\alpha)\coloneqq\Gamma^{\prime}(\alpha)/\Gamma(\alpha)\) denotes the digamma function. Using the dual form of \(\widehat{\mathrm{FUSE}}_{1}\) we find that

\[\widehat{\mathrm{FUSE}}_{1}=\sup_{\alpha>0,\beta>0}\widehat{\mathrm{MMD}}_{k _{rq(\alpha,\sqrt{\beta})}}^{2}(\mathbf{Z})-\frac{\mathrm{KL}(\Gamma(\alpha,\beta ),\Gamma(\alpha_{0},\beta_{0}))}{\lambda}.\]

Restricting \(\alpha=\alpha_{0}\), prior \(\beta_{0}=\eta_{0}^{2}\) and setting \(\beta=R\eta_{0}^{2}\) gives the result.

Null and Permutation Concentration Results

In this section we prove the following concentration results under the null (or equivalently, under the permutation distribution). These results show that for appropriate choices of \(\lambda\), both statistics converge to zero at rate at least \(\mathcal{O}(1/n)\) under the null or permutations. We recall that without loss of generality \(n\leq m\).

**Theorem 5**.: _For bounded kernels \(k\leq\kappa<\infty\) and parameter \(\lambda\), with probability at least \(1-\delta\) over a sample \(\mathbf{Z}\) from the null,_

\[\widehat{\mathrm{FUSE}}_{1}(\mathbf{Z})\leq\frac{4\kappa^{2}\lambda}{n(n-1)}+\frac {\log\frac{1}{\delta}}{\lambda}\]

_provided \(0<\lambda<\sqrt{n(n-1)}/8\sqrt{2}\kappa\), and_

\[-\widehat{\mathrm{FUSE}}_{1}(\mathbf{Z})\leq\frac{1+32\log\frac{1}{\delta}}{\sqrt{2 n(n-1)}}\cdot\frac{\kappa}{2}.\]

_For potentially unbounded kernels and parameter \(\lambda\), with probability at least \(1-\delta\) over a sample \(\mathbf{Z}\) from the null,_

\[\widehat{\mathrm{FUSE}}_{N}(\mathbf{Z})\leq\frac{16\lambda}{n(n-1)}+\frac{\log \frac{1}{\delta}}{\lambda},\]

_provided \(0<\lambda<\sqrt{n(n-1)}/16\sqrt{2}\), and_

\[-\widehat{\mathrm{FUSE}}_{N}(\mathbf{Z})\leq\frac{1+32\log\frac{1}{\delta}}{\sqrt{ 2n(n-1)}}.\]

_The above bounds are also valid for \(\widehat{\mathrm{FUSE}}_{1}(\sigma\mathbf{Z}),\widehat{\mathrm{FUSE}}_{N}(\sigma \mathbf{Z})\) and any fixed \(\mathbf{Z}\) (potentially non-null) under permutation by \(\sigma\sim\mathrm{Uniform}(\mathfrak{S}_{n+m})\)._

While the upper bounds depend critically upon our choice of \(\lambda\), the lower bounds instead hold regardless of \(\lambda\). Choosing \(\lambda\asymp\sqrt{n(n-1)}\asymp n\) gives the desired upper bound rates \(\mathcal{O}(1/n)\).

### Sub-Gaussian Chaos Theorem

We provided constants for Theorem 5, even though they are not strictly necessary, as we hope they could be useful when adapting FUSE statistics to other settings. In particular, the constants help understand for which values of \(\lambda\) the bounds hold, and to understand the relation between the two different right hand side terms which can be matched by tuning \(\lambda\). In provide these constants, in this subsection we replicate the proof of a sub-result from Rudelson and Vershynin (2013), but unlike in that paper we explicitly keep track of the constants. To be clear, Rudelson and Vershynin (2013) does not give numerical constants at all (only their existence is proved), and we need then for our statement of Theorem 5. We do not claim this as a contribution, the proof is only included so that we are not stating the numerical constants without justification.

**Theorem 6** (Sub-Gaussian Chaos; adapted from Rudelson and Vershynin, 2013).: _Let \(X_{i}\) be mean-zero \(1\)-sub-Gaussian variables, such that \(\log\mathbb{E}_{X}\exp(tX)\leq\frac{1}{2}t^{2}\) for every \(t\in\mathbb{R}\). Let \(A\in\mathbb{R}^{n\times n}\) be a real symmetric matrix with zeros on the diagonal and we define the sub-Gaussian chaos_

\[W\coloneqq\sum_{i=1}^{n}\sum_{j=1}^{n}A_{ij}X_{i}X_{j}.\]

_For all \(|t|\leq(4\sqrt{2}\|A\|)^{-1}\),_

\[\mathbb{E}_{X}\exp(tW)\leq\exp\big{(}16t^{2}\|A\|_{F}^{2}\big{)}.\]

**Proof.** This proof is closely adapted from Rudelson and Vershynin (2013) with some modifications to explicitly track numerical constants.

Let \(X_{i}\) be mean-zero \(1\)-sub-Gaussian variables, such that \(\log\mathbb{E}_{X}\exp(tX)\leq\frac{1}{2}t^{2}\) for every \(t\in\mathbb{R}\). We are considering

\[W:=\sum_{i,j}A_{ij}X_{i}X_{j}\]where \(A_{ij}\) has zeros on the diagonal. We introduce independent Bernoulli random variables \(\delta_{i}\in\{0,1\}\) with \(\mathbb{E}\delta_{i}=1/2\) and define the matrix \(A^{\delta}\) with entries \((A^{\delta})_{ij}=\delta_{i}(1-\delta_{j})A_{ij}\). Note that \(\mathbb{E}_{\delta}A^{\delta}=\frac{1}{4}A\), since \(\mathbb{E}\delta_{i}(1-\delta_{j})\) equals \(1/4\) for \(i\neq j\).

Writing the set of indices \(\Lambda_{\delta}=\{i\in[n]:\,\delta_{i}=1\}\), we introduce

\[W_{\delta}:=\sum_{i,j}A_{ij}^{\delta}X_{i}X_{j}=\sum_{i\in\Lambda_{\delta},\,j \in\Lambda_{\delta}^{c}}A_{ij}X_{i}X_{j}=\sum_{j\in\Lambda_{\delta}^{c}}X_{j} \Big{(}\sum_{i\in\Lambda_{\delta}}A_{ij}X_{i}\Big{)}.\]

By Jensen's inequality,

\[\mathbb{E}_{X}\exp(tW)\leq\mathbb{E}_{X,\delta}\exp(4tW_{\delta}).\]

Conditioned on \(\delta\) and \((X_{i})_{i\in\Lambda_{\delta}}\), \(W_{\delta}\) is a linear combination of the mean-zero sub-gaussian random variables \(X_{j}\), \(j\in\Lambda_{\delta}^{c}\). It follows that this conditional distribution of \(W_{\delta}\) is sub-gaussian, and so

\[\mathbb{E}_{(X_{j})_{j\in\Lambda_{\delta}^{c}}}\exp(4tW_{\delta})\leq\exp \left(\frac{1}{2}(4t)^{2}\sum_{j\in\Lambda_{\delta}^{c}}\Big{(}\sum_{i\in \Lambda_{\delta}}A_{ij}X_{i}\Big{)}^{2}\right).\]

Now introduce \(g=(g_{1},\ldots,g_{n})\sim\mathcal{N}(0,1)\) i.i.d. draws from a normal and note that the above can also arise directly as the moment generating function of these Normal variables, so we make the further equality (for fixed \(X\) and \(\delta\)),

\[\exp\left(\frac{1}{2}(4t)^{2}\sum_{j\in\Lambda_{\delta}^{c}}\Big{(}\sum_{i\in \Lambda_{\delta}}A_{ij}X_{i}\Big{)}^{2}\right)=\mathbb{E}_{g}\exp\left(4t\sum_ {j\in\Lambda_{\delta}^{c}}g_{j}\Big{(}\sum_{i\in\Lambda_{\delta}}A_{ij}X_{i} \Big{)}^{2}\right).\]

Rearranging the terms, and using that the resulting term is a linear combination of the sub-Gaussian \(X_{i}\), \(i\in\Lambda_{\delta}\), we find that

\[\mathbb{E}_{X,g}\exp\left(4t\sum_{j\in\Lambda_{\delta}^{c}}g_{j} \Big{(}\sum_{i\in\Lambda_{\delta}}A_{ij}X_{i}\Big{)}^{2}\right) =\mathbb{E}_{X,g}\exp\left(4t\sum_{i\in\Lambda_{\delta}}X_{i} \Big{(}\sum_{j\in\Lambda_{\delta}^{c}}A_{ij}g_{j}\Big{)}\right)\] \[\leq\mathbb{E}_{X,g}\exp\left(\frac{1}{2}(4t)^{2}\sum_{i\in\Lambda _{\delta}}\Big{(}\sum_{j\in\Lambda_{\delta}^{c}}A_{ij}g_{j}\Big{)}^{2}\right)\] \[\leq\mathbb{E}_{X,g}\exp\left(8t^{2}\sum_{i}\Big{(}\sum_{j}\delta _{i}(1-\delta_{j})A_{ij}g_{j}\Big{)}^{2}\right)\] \[\leq\mathbb{E}_{X,g}\exp\left(8t^{2}\|A^{\delta}g\|_{2}^{2}\right).\]

Now, by the rotation invariance of the distribution of \(g\), the random variable \(\|A^{\delta}g\|_{2}^{2}\) is distributed identically with \(\sum_{i}s_{i}^{2}g_{i}^{2}\) where \(s_{i}\) denote the singular values of \(A^{\delta}\), so by independence

\[\mathbb{E}_{X,g}\exp\left(8t^{2}\|A^{\delta}g\|_{2}^{2}\right) =\mathbb{E}_{g}\exp\left(8t^{2}\sum_{i}s_{i}^{2}g_{i}^{2}\right)\] \[=\prod_{i}\mathbb{E}_{g}\exp\left(8t^{2}s_{i}^{2}g_{i}^{2}\right)\] \[\leq\prod_{i}\exp\left(16t^{2}s_{i}^{2}\right)\]

where the last inequality holds if \(8t^{2}\max_{i}s_{i}^{2}\leq\frac{1}{4}\) and arises since the MGF of a Chi-squared variable, \(\mathbb{E}e^{tg^{2}}\leq e^{2t}\) for \(0\leq t\leq\frac{1}{4}\).

Since \(\max_{i}s_{i}=\|A_{\delta}\|\leq\|A\|\) and \(\sum_{i}s_{i}^{2}=\|A_{\delta}\|_{F}^{2}\leq\|A\|_{F}\), we combine the above steps to find that

\[\mathbb{E}_{X}\exp(tW)\leq\exp\left(16t^{2}\|A\|_{F}^{2}\right)\quad\text{for }|t|\leq(4\sqrt{2}\|A\|)^{-1}.\]

### Permutation Bounds for Two-Sample U-Statistics

The main technical result we use for the null bounds is the following. Here we assume the permutation-invariance property of the U statistic kernel that \(h(x_{1},x_{2};y_{1},y_{2})=h(x_{2},x_{1};y_{1},y_{2})=h(x_{1},x_{2};y_{2},y_{1})\). This can always be ensured by symmetrizing the kernel, and is satisfied by the kernel used by \(\widehat{\mathrm{MMD}}^{2}\) which we consider in this paper.

**Theorem 7**.: _Fix combined sample \(\mathbf{Z}\) of size \(n+m\) with \(n\leq m\) and let \(h(x_{1},x_{2};y_{1},y_{2})\) be a two-sample U-statistic kernel as described above. Define_

\[U_{k}(\mathbf{Z}):=\frac{1}{n(n-1)m(m-1)}\sum_{(i,i^{\prime})\in[n]_{2}}\sum_{(j,j ^{\prime})\in[m]_{2}}h(Z_{i},Z_{i^{\prime}};Z_{n+j},Z_{n+j^{\prime}}).\]

_and_

\[\bar{U}_{k}(\mathbf{Z})=\frac{1}{n(n-1)m(m-1)}\sup_{\sigma\in\mathfrak{S}_{n+m}} \sqrt{\sum_{(i,i^{\prime})\in[n]_{2}}\left(\sum_{(j,j^{\prime})\in[m]_{2}}h(Z_ {\sigma(i)},Z_{\sigma(i^{\prime})};Z_{\sigma(n+j)},Z_{\sigma(n+j^{\prime})}) \right)^{2}}.\]

_If \(\sigma\in\mathrm{Uniform}(\mathfrak{S}_{n+m})\) is a random permutation of the dataset, then for all \(|t|<(4\sqrt{2}\bar{U}_{k}(\mathbf{Z}))^{-1}\),_

\[\mathbb{E}_{\sigma}\exp(tU_{k}(\sigma\mathbf{Z}))\leq\exp\left(t^{2}\bar{U}_{k}^{2 }(\mathbf{Z})\right).\]

**Theorem 8** (Normaliser Bound).: _If \(h(x,x^{\prime};y,y^{\prime})=k(x,x^{\prime})+k(y,y^{\prime})-k(x,y^{\prime})-k (x^{\prime},y)\) then_

\[\bar{U}_{k}(\mathbf{Z})^{2}\leq\frac{16}{n(n-1)}\widehat{\mathbb{N}}_{k}(\mathbf{Z})\]

_where \(n\leq m\) and_

\[\widehat{\mathbb{N}}_{k}(\mathbf{Z})=\frac{1}{n(n-1)}\sum_{(i,j)\in[n+m]_{2}}k(Z_{ i},Z_{j})^{2}.\]

_Additionally, if \(0\leq k(x,x^{\prime})\leq\kappa\) for all \(x,x^{\prime}\), then_

\[\bar{U}_{k}^{2}(\mathbf{Z})\leq\frac{4\kappa^{2}}{n(n-1)}.\]

Proof of Theorem 7.: Firstly, we make the following definitions based on the work of Kim et al. (2022): let \(L=\{l_{1},\ldots,l_{n}\}\) be an \(n\)-tuple drawn uniformly without replacement from \([m]\). Then for _any_ fixed \(\mathbf{Z}\)

\[\mathbb{E}_{L}[\tilde{U}_{k}^{L}(\mathbf{Z})]=U_{k}(\mathbf{Z}) \tag{7}\]

where we define

\[\tilde{U}_{k}^{L}(\mathbf{Z}):=\frac{1}{n(n-1)}\sum_{(i,j)\in[n]_{2}}h(Z_{i},Z_{j };Z_{n+l_{i}},Z_{n+l_{j}}).\]

Note that in the above we have used the invariance \(h(x_{1},x_{2};y_{1},y_{2})=h(x_{1},x_{2};y_{2},y_{1})\).

Now additionally define \(\zeta_{i}\) as i.i.d. Rademacher variables, and let \(\sigma\sim\mathrm{Uniform}(\mathfrak{S}_{n+m})\). For any fixed \(\mathbf{Z}\)

\[\tilde{U}_{k}^{L}(\sigma\mathbf{Z})=^{d}\tilde{U}_{k}^{L,\zeta}(\sigma\mathbf{Z}) \tag{8}\]

where we have defined

\[\tilde{U}_{k}^{L,\zeta}(\mathbf{Z}):=\frac{1}{n(n-1)}\sum_{(i,j)\in[n]_{2}}\zeta_ {i}\zeta_{j}h(Z_{i},Z_{j};Z_{n+l_{i}},Z_{n+l_{j}}).\]

This works because we can first define \(\tilde{Z}_{i}=Z_{i}\) or \(\tilde{Z}_{i}=Z_{n+l_{i}}\), each with probability \(\frac{1}{2}\). The distribution of \(\tilde{U}_{k}^{L}(\sigma\tilde{\mathbf{Z}})=^{d}\tilde{U}_{k}^{L}(\sigma\mathbf{Z})\), and then using the symmetry of \(k(x,y)\) in its arguments gives the equivalence Equation (8) (_c.f._ eq. 28, Kim et al., 2022).

Now for fixed \(\mathbf{Z}\), combining Equations (7) and (8) and Jensen's inequality we gives

\[\mathbb{E}_{\sigma}\exp(tU_{k}(\sigma\mathbf{Z}))=\mathbb{E}_{\sigma}\exp(t\mathbb{ E}_{L}[\tilde{U}_{k}^{L}(\sigma\mathbf{Z})|\sigma])\]\[=\mathbb{E}_{\sigma}\exp(t\mathbb{E}_{L,\zeta}[\bar{U}_{L,\zeta}^{L, \zeta}(\sigma\mathbf{Z})|\sigma])\] \[\leq\mathbb{E}_{\sigma,\zeta}\exp\left(t\mathbb{E}_{L}\left.\left[ \frac{1}{n(n-1)}\sum_{(i,j)\in[n]_{2}}\zeta_{i}\zeta_{j}h(Z_{\sigma(i)},Z_{ \sigma(j)};Z_{\sigma(n+l_{i})},Z_{\sigma(n+l_{j})})\Bigg{|}\sigma\right]\right)\right.\] \[=\mathbb{E}_{\sigma,\zeta}\exp\left(t\frac{1}{n(n-1)}\sum_{(i,j) \in[n]_{2}}\zeta_{i}\zeta_{j}\mathbb{E}_{L}\left.\left[h(Z_{\sigma(i)},Z_{ \sigma(j)};Z_{\sigma(n+l_{i})},Z_{\sigma(n+l_{j})})\right|\sigma\right]\right)\] \[=:\mathbb{E}_{\sigma,\zeta}\exp\left(t\sum_{i=1}^{n}\sum_{j=1}^{n }\zeta_{i}\zeta_{j}A_{ij}^{\sigma}\right).\]

In the final step we have defined the matrix \(A^{\sigma}\), with entries \(A_{ij}^{\sigma}=(n(n-1))^{-1}\mathbb{E}_{L}\left[h(Z_{\sigma(i)},Z_{\sigma(j)} ;Z_{\sigma(n+l_{i})},Z_{\sigma(n+l_{j})})\big{|}\sigma\right]\) for \(i\neq j\) and \(A_{ii}=0\) for all \(i\).

We note that \(\zeta\) is independent of \(\sigma\) and satisfies the conditions of Theorem 6, so applying this theorem we obtain that

\[\mathbb{E}_{\sigma,\zeta} \exp\left(t\sum_{i=1}^{n}\sum_{j=1}^{n}\zeta_{i}\zeta_{j}A_{ij}^{ \sigma}\right)\] \[\leq\mathbb{E}_{\sigma}\exp\left(16t^{2}\|A^{\sigma}\|_{F}^{2}\right) \text{for}\,|t|<(4\sqrt{2}\|A^{\sigma}\|)^{-1}\] \[\leq\mathbb{E}_{\sigma}\exp\left(16t^{2}\|A^{\sigma}\|_{F}^{2}\right) \text{for}\,|t|<(4\sqrt{2}\|A^{\sigma}\|_{F})^{-1}\] \[\leq\exp\left(16t^{2}\sup_{\sigma\in\mathfrak{S}_{n+m}}\|A^{ \sigma}\|_{F}^{2}\right) \text{for}\,|t|<(4\sqrt{2}\sup_{\sigma\in\mathfrak{S}_{n+m}}\|A^{ \sigma}\|_{F})^{-1}\]

where in the second line we have used that \(\|M\|\leq\|M\|_{F}\) for any matrix \(M\).

Now note that

\[\|A^{\sigma}\|_{F}^{2} =\sum_{i=1}^{n}\sum_{\nu=1}^{n}(A_{ii^{\prime}}^{\sigma})^{2}\] \[=\frac{1}{n^{2}(n-1)^{2}}\sum_{(i,i^{\prime})\in[n]_{2}}(\mathbb{ E}_{L}\left.\left[h(Z_{\sigma(i)},Z_{\sigma(i^{\prime})};Z_{\sigma(n+l_{i})},Z_{ \sigma(n+l_{i^{\prime}})})\big{|}\sigma\right])^{2}\right.\] \[=\frac{1}{n^{2}(n-1)^{2}}\sum_{(i,i^{\prime})\in[n]_{2}}\left( \frac{1}{m(m-1)}\sum_{(j,j^{\prime})\in[m]_{2}}h(Z_{\sigma(i)},Z_{\sigma(i^{ \prime})};Z_{\sigma(n+j)},Z_{\sigma(n+j^{\prime})})\right)^{2}.\]

We complete the proof by noting that \(\bar{U}_{k}(\mathbf{Z})^{2}=\sup_{\sigma\in\mathfrak{S}_{n+m}}\|A^{\sigma}\|_{F}^ {2}\). 

**Proof of Theorem 8.** We have

\[\bar{U}_{k}^{2}(\mathbf{Z}) =\] \[\leq \frac{1}{n^{2}(n-1)^{2}m(m-1)}\sum_{(i,i^{\prime})\in[n]_{2}}\sum _{(j,j^{\prime})\in[m]_{2}}h(Z_{\sigma(i)},Z_{\sigma(i^{\prime})};Z_{\sigma(n +j)},Z_{\sigma(n+j^{\prime})})^{2}\] \[\leq \frac{1}{n^{2}(n-1)^{2}m(m-1)}\sum_{(i,i^{\prime})\in[n]_{2}}\sum _{(j,j^{\prime})\in[m]_{2}}h(Z_{\sigma(i)},Z_{\sigma(i^{\prime})};Z_{\sigma(n +j)},Z_{\sigma(n+j^{\prime})})^{2}\] \[\leq \frac{4}{n^{2}(n-1)^{2}m(m-1)}\sum_{(i,i^{\prime})\in[n]_{2}}\sum _{(j,j^{\prime})\in[m]_{2}}\left(k(Z_{\sigma(i)},Z_{\sigma(i^{\prime})})^{2}+k(Z _{\sigma(n+j)},Z_{\sigma(n+j^{\prime})})^{2}\right.\] \[\quad+\left.k(Z_{\sigma(i)},Z_{\sigma(n+j^{\prime})})^{2}+k(Z_{ \sigma(i^{\prime})},Z_{\sigma(n+j^{\prime})})^{2}\right)\] \[= \frac{4}{n^{2}(n-1)^{2}m(m-1)}\Bigg{(}m(m-1)\sum_{(i,i^{\prime}) \in[n]_{2}}k(Z_{\sigma(i)},Z_{\sigma(i^{\prime})})^{2}\]\[\leq\log\mathbb{E}_{\mathbf{Z}}\mathbb{E}_{k\sim\pi(\langle\mathbf{Z}\rangle)} \mathbb{E}_{\sigma}\exp\left(\lambda\widehat{\mathrm{MMD}}^{2}_{k}(\mathbf{X},\mathbf{Y })\right)+\log\frac{1}{\delta}\] Markov \[=\log\mathbb{E}_{\sigma}\mathbb{E}_{\mathbf{Z}}\mathbb{E}_{k\sim\pi( \langle\mathbf{Z}\rangle)}\exp\left(\lambda\widehat{\mathrm{MMD}}^{2}_{k}(\sigma \mathbf{Z})\right)+\log\frac{1}{\delta}\] Null Permutation-Free \[=\log\mathbb{E}_{\mathbf{Z}}\mathbb{E}_{k\sim\pi(\langle\mathbf{Z}\rangle)} \mathbb{E}_{\sigma}\exp\left(\lambda\widehat{\mathrm{MMD}}^{2}_{k}(\sigma\mathbf{Z })\right)+\log\frac{1}{\delta}\] Prior Permutation-Free \[\leq\log\mathbb{E}_{\mathbf{Z}}\mathbb{E}_{k\sim\pi(\langle\mathbf{Z} \rangle)}\mathbb{E}_{\sigma}\exp\left(\lambda^{2}\bar{U}^{2}_{k}\right)+\log \frac{1}{\delta}\] \[\leq\frac{4\kappa^{2}\lambda^{2}}{n(n-1)}+\log\frac{1}{\delta}, \tag{9}\]

where the penultimate result holds using Theorem 7 provided \(|\lambda|<(4\sqrt{2}\sup_{k}\bar{U}_{k}(\mathbf{Z}))^{-1}\). Using the bound on \(\bar{U}_{k}\) from above gives the \(|\lambda|<\sqrt{n(n-1)}/8\sqrt{2}\kappa\) in the theorem statement.

By dividing \(\widehat{\mathrm{MMD}}^{2}\) by the permutation invariant \(\widehat{N}_{k}\) in every one of the above, we also find that

\[\log\mathbb{E}_{k\sim\pi(\langle\mathbf{Z}\rangle)}\exp\left(\lambda \frac{\widehat{\mathrm{MMD}}^{2}_{k}(\mathbf{X},\mathbf{Y})}{\sqrt{\widehat{N}_{k}( \langle\mathbf{Z}\rangle)}}\right)\leq_{1-\delta}\log\mathbb{E}_{\mathbf{Z}}\mathbb{E}_ {k\sim\pi(\langle\mathbf{Z}\rangle)}\mathbb{E}_{\sigma}\exp\left(\frac{\lambda^{2} \bar{U}^{2}_{k}}{\widehat{N}_{k}(\langle\mathbf{Z}\rangle)}\right)+\log\frac{1}{\delta}\]\[\leq\frac{16\lambda^{2}}{n(n-1)}+\log\frac{1}{\delta} \tag{10}\]

the first inequality using that \(\widehat{N}_{k}\) is permutation-invariant and holding for \(|\lambda|<\sqrt{\sup_{k}\widehat{N}_{k}}(4\sqrt{2}\sup_{k}\bar{U}_{k}(\mathbf{Z}))^{ -1}\). The final step uses that \(\bar{U}_{k}^{2}\leq 16\widehat{N}_{k}/n(n-1)\) from Theorem 8 which also shows that \(|\lambda|<\sqrt{n(n-1)}/16\sqrt{2}\) is sufficient.

The upper tail bounds for \(\widehat{\operatorname{FUSE}}_{N}\) and \(\widehat{\operatorname{FUSE}}_{1}\) immediately follow from the above by dividing through \(\lambda>0\) and from their definitions.

Lower Bounds.For the lower tails,

\[-\widehat{\operatorname{FUSE}}_{1}(\mathbf{Z}) =\frac{1}{\lambda}\log\mathbb{E}_{k\sim\pi((\mathbf{Z}))}\exp(\lambda \widehat{\operatorname{MMD}}_{k}^{2}(\mathbf{Z}))\] \[\leq-\mathbb{E}_{k\sim\pi((\mathbf{Z}))}[\widehat{\operatorname{MMD} }_{k}^{2}(\mathbf{Z})]\] Jensen \[=\frac{1}{s}\log\,\circ\,\exp(\mathbb{E}_{k\sim\pi((\mathbf{Z}))}[-s \widehat{\operatorname{MMD}}_{k}^{2}(\mathbf{Z})])\] introduce dummy \[s>0\] \[\leq\frac{1}{s}\log\,\circ\,\exp(\mathbb{E}_{k\sim\pi((\mathbf{Z}))}- s\widehat{\operatorname{MMD}}_{k}^{2}(\mathbf{Z}))\] Jensen on exp \[\leq_{1-\delta}\frac{4\kappa^{2}s}{n(n-1)}+\frac{1}{s}\log\frac{ 1}{\delta},\]

where the last line follows from Equation (9) replacing \(\lambda\) by dummy variable \(-s\) for \(0<s<\sqrt{n(n-1)}/8\sqrt{2}\kappa\); we importantly note that this bound held for negative \(\lambda\) by Theorem 7.

By following the same process for \(\widehat{\operatorname{FUSE}}_{N}\) and instead using Equation (10), also holding for potentially negative \(\lambda\), we obtain

\[-\widehat{\operatorname{FUSE}}_{N}(\mathbf{Z})\leq_{1-\delta}\frac{16s}{n(n-1)}+ \frac{1}{s}\log\frac{1}{\delta}\]

for \(0<s<\sqrt{n(n-1)}/16\sqrt{2}\).

For the small \(\delta\) we generally use, these are tightest when \(s\) is at its maximum permissible value, so we substitute these values for the theorem statement.

Under Permutation.To prove the equivalent result for \(\sigma\mathbf{Z}\) under permutations, we replace \(\mathbf{Z}\) by \(\sigma\mathbf{Z}\) and our application of Markov's inequality introducing an expectation over \(\mathbf{Z}\) with one over \(\sigma\). This changes nothing else in the derivations.

Concentration under the alternative

We give the exponential convergence bounds for \(\widehat{\mathrm{FUSE}}_{1}\) under the alternative and relate it to its mean. In the following we will assume that \(\mathcal{K}\) is a class of kernels bounded by \(0<\kappa<\infty\), so that \(\widehat{\mathrm{FUSE}}_{1}\in[-2\kappa,2\kappa]\). We also introduce the following quantity (for fixed, data-free prior \(\pi\)) which is closely related to the expectation of \(\widehat{\mathrm{FUSE}}_{1}\),

\[\mathrm{FUSE}_{1}:=\sup_{\rho\in\mathcal{M}^{1}_{+}(\mathcal{K})}\mathrm{MMD}^ {2}_{K_{\rho}}(p,q)-\frac{\mathrm{KL}(\rho,\pi)}{\lambda}.\]

**Theorem 9**.: \(\mathrm{FUSE}_{1}\) _is bounded in the following ways:_

\[\mathrm{FUSE}_{1}\leq\mathbb{E}_{\mathbf{Z}}\widehat{\mathrm{FUSE}}_{1}(\mathbf{Z}) \leq\mathrm{FUSE}_{1}+8\kappa^{2}\lambda\left(\frac{1}{n}+\frac{1}{m}\right),\]

_and_

\[\mathrm{MMD}^{2}_{K_{\pi}}(p,q)\leq\mathrm{FUSE}_{1}\leq\sup_{\rho\in \mathcal{M}^{1}_{+}(\mathcal{K}):\mathrm{KL}(\rho,\pi)<\infty}\mathrm{MMD}^{2 }_{K_{\rho}}(p,q)\leq\sup_{k\in\mathrm{supp}(\pi)}\mathrm{MMD}^{2}_{k}(p,q).\]

_Under the null hypothesis \(\mathrm{FUSE}_{1}=0\)._

We can now state concentration results for \(\widehat{\mathrm{FUSE}}_{1}\) in terms of \(\mathrm{FUSE}_{1}\).

**Theorem 10**.: _With probability at least \(1-\delta\) over the sample_

\[\widehat{\mathrm{FUSE}}_{1}(\mathbf{Z})-\mathrm{FUSE}_{1}\leq 8\kappa^{2}\lambda \left(\frac{1}{n}+\frac{1}{m}\right)+\frac{\log\delta^{-1}}{\lambda}\]

_and with the same probability,_

\[\mathrm{FUSE}_{1}-\widehat{\mathrm{FUSE}}_{1}(\mathbf{Z})\leq 2\kappa\sqrt{8 \left(\frac{1}{n}+\frac{1}{m}\right)\log\delta^{-1}}.\]

### Proofs

Note on the proofs.The "bounded difference lemma" (Theorem 11) we give below is not the same as the bounded difference inequality, though it is closely related. In fact, our result could be used as an intermediate step in proving the latter, but we note that this would not be the usual method (_e.g._ Boucheron et al., 2013, use an "entropy" method instead), and the converse is not true. The fact that we have to prove a variant form of an existing concentration inequality to get concentration for our log-sum-exp statistics is similar to how the same is required in PAC-Bayesian proofs, where _e.g._ PAC-Bayes Bernstein inequalities also require modified proof techniques that mirror those used to prove the usual Bernstein inequalities.

The usual bounded difference inequality cannot be used to prove Theorem 9, since it is not a concentration bound. It also does not give the concentration we need in Theorem 10, since the FUSE statistics do not have the bounded difference property, only \(\widehat{\mathrm{MMD}}^{2}_{k}\) does; this is why our proofs cannot use a "plug-in" concentration bound.

**Theorem 11** (Bounded Difference Lemma).: _A function \(f\) has the bounded difference property there exist constants \(L_{\ell}<\infty,\ell\in[n]\) such that_

\[|f(z_{1},\ldots,z_{k},\ldots,z_{n})-f(z_{1},\ldots,z_{\ell},\ldots,z_{n})|\leq L _{\ell}\]

_for any choices of \(z_{1},\ldots,z_{n}\), \(z_{\ell}^{\prime}\), and \(\ell\in[n]\)._

_For such a function and any independent random variables \(Z_{1},\ldots,Z_{n}\) and \(t\in\mathbb{R}\) (subject to appropriate measurability restrictions)_

\[\mathbb{E}\exp(t(f(Z_{1},\ldots,Z_{n})-\mathbb{E}f(Z_{1},\ldots,Z_{n})))\leq \exp\left(\frac{1}{8}t^{2}\sum_{\ell=1}^{n}L_{\ell}^{2}\right).\]Proof of Theorem 11.: We introduce the Doob construction, defining

\[D_{\ell}=\mathbb{E}[f(Z_{1},\ldots,Z_{n})|Z_{1},\ldots,Z_{\ell}]-\mathbb{E}[f(Z_{1 },\ldots,Z_{n})|Z_{1},\ldots,Z_{\ell-1}]\]

This is a martingale difference sequence with

\[\sum_{\ell=1}^{n}D_{\ell}=f(Z_{1},\ldots,Z_{n})-\mathbb{E}f(Z_{1},\ldots,Z_{n}).\]

It is shown by (for example) Wainwright (2019, Ch. 2.2, p.37) that \(D_{\ell}\) lies in an interval of length at most \(L_{\ell}\) by the bounded differences assumption. Thus, applying iterated expectation and Hoeffding's lemma for the MGF of bounded random variables,

\[\mathbb{E}\exp(t(f(Z_{1},\ldots,Z_{n})-\mathbb{E}f(Z_{1},\ldots,Z_ {n}))) =\mathbb{E}\exp\left(\lambda\sum_{\ell=1}^{n}D_{\ell}\right)\] \[=\mathbb{E}\left[\exp\left(\lambda\sum_{\ell=1}^{n-1}D_{\ell} \right)\mathbb{E}\left[\exp\left(\lambda D_{n}\right)|Z_{1},\ldots,Z_{n-1} \right]\right]\] \[\leq\mathbb{E}\left[\exp\left(\lambda\sum_{\ell=1}^{n-1}D_{\ell} \right)\right]\exp\left(\frac{1}{8}\lambda^{2}L_{n}^{2}D_{n}\right)\] \[\leq\exp\left(\frac{1}{8}t^{2}\sum_{\ell=1}^{n}L_{\ell}^{2}\right).\]

The MGF of \(\widehat{\mathrm{MMD}}^{2}\) can then be bounded using the following result, proved via the above.

**Theorem 12**.: _For bounded kernel \(k\leq\kappa\), \(t\in\mathbb{R}\) and sample sizes \(m,n\),_

\[\mathbb{E}\exp(t(\widehat{\mathrm{MMD}}^{2}_{k}(\boldsymbol{X},\boldsymbol{Y})- \mathrm{MMD}^{2}_{k}(p,q)))\leq\exp\left(8t^{2}\kappa^{2}\left(\frac{1}{m}+ \frac{1}{n}\right)\right).\]

Proof of Theorem 12.: We show that \(\widehat{\mathrm{MMD}}^{2}_{k}(x,y)\) has the bounded differences property and then apply Theorem 11. Denote by \(x^{\setminus\ell}\) for \(\ell\in[n]\) that the \(\ell\)-th example in the \(x\) sample is changed. Then

\[\widehat{\mathrm{MMD}}^{2}_{k}(x,y)- \widehat{\mathrm{MMD}}^{2}_{k}(x^{\setminus\ell},y)|\] \[=\left|\frac{2}{n(n-1)}\sum_{i\in[n]\setminus\{\ell\}}(k(x_{\ell},x_{i})-k(x_{\ell}^{\prime},x_{i}))-\frac{2}{mn}\sum_{j=1}^{m}(k(x_{\ell},y_{j })-k(x_{\ell}^{\prime},y_{j}))\right|\] \[\leq\frac{2}{n(n-1)}\sum_{i\in[n]\setminus\{\ell\}}|k(x_{\ell},x_ {i})-k(x_{\ell}^{\prime},x_{i})|+\frac{2}{mn}\sum_{j=1}^{m}|k(x_{\ell},y_{j})- k(x_{\ell}^{\prime},y_{j})|\] \[\leq\frac{2}{n(n-1)}(n-1)\cdot 2\kappa+\frac{2}{mn}m\cdot 2\kappa\] \[=\frac{8\kappa}{n}.\]

A similar process for the \(y\) sample gives bounds of \(8\kappa/m\), so that \(\widehat{\mathrm{MMD}}^{2}\) has the bounded differences property with

\[\sum_{\ell=1}^{n+m}L_{\ell}^{2}\leq n\cdot\left(\frac{8\kappa}{n}\right)^{2}+m \cdot\left(\frac{8\kappa}{m}\right)^{2}=64\kappa^{2}\left(\frac{1}{n}+\frac{1} {m}\right).\]

Proof of Theorem 9.: For the first lower bound, note

\[\mathbb{E}_{\boldsymbol{Z}}\widehat{\mathrm{FUSE}}_{1}(\boldsymbol{Z})= \mathbb{E}_{\boldsymbol{Z}}\sup_{\rho}\mathbb{E}_{\rho}[\widehat{\mathrm{MMD} }^{2}]-\frac{\mathrm{KL}}{\lambda}\geq\sup_{\rho}\mathbb{E}_{\boldsymbol{Z}, \rho}[\widehat{\mathrm{MMD}}^{2}]-\frac{\mathrm{KL}}{\lambda}=\mathrm{FUSE}_{1}\,.\]For the upper bound, note that

\[\mathbb{E}_{\mathbf{Z}}\widehat{\mathrm{FUSE}}_{1}(\mathbf{Z}) \leq\left[\frac{1}{\lambda}\log\left(\mathbb{E}_{\mathbf{Z}}\mathbb{E}_ {k\sim\pi}\left[e^{\lambda\widehat{\mathrm{MMD}}_{k}^{2}}\right]\right)\right]\] Jensen \[\leq\left[\frac{1}{\lambda}\log\left(\mathbb{E}_{k\sim\pi}\mathbb{ E}_{\mathbf{Z}}\left[e^{\lambda\widehat{\mathrm{MMD}}_{k}^{2}}\right]\right)\right]\] Independence of

\[\pi\]

 from

\[\mathbf{Z}\]

 KL-regularised version. Under the null hypothesis, \(\mathrm{MMD}_{k}(p,q)=0\) for every kernel (regardless of them being characteristic or not), so \(\mathrm{FUSE}_{1}=0\). 

Proof of Theorem 10.: For the upper bound,

\[\widehat{\mathrm{FUSE}}_{1}(\mathbf{Z}) =\left[\frac{1}{\lambda}\log\left(\mathbb{E}_{k\sim\pi}\left[e^{ \lambda\widehat{\mathrm{MMD}}_{k}^{2}}\right]\right)\right]\] \[\leq_{1-\delta}\left[\frac{1}{\lambda}\log\left(\mathbb{E}_{\mathbf{Z }}\mathbb{E}_{k\sim\pi}\left[e^{\lambda\widehat{\mathrm{MMD}}_{k}^{2}}\right] \right)\right]+\frac{\log\delta^{-1}}{\lambda}\] Markov \[\leq\left[\frac{1}{\lambda}\log\left(\mathbb{E}_{k\sim\pi}\mathbb{ E}_{\mathbf{Z}}\left[e^{\lambda\widehat{\mathrm{MMD}}_{k}^{2}}\right]\right)\right]+\frac{ \log\delta^{-1}}{\lambda}\] Independence of

\[\pi\]

 from

\[\mathbf{Z}\]

 (which we note is independent of the sample), so that

\[\mathrm{FUSE}_{1}-\widehat{\mathrm{FUSE}}_{1}(\mathbf{Z}) =\sup_{\rho}\left\{\mathbb{E}_{\rho^{*}}[\mathrm{MMD}_{k}^{2}]- \frac{\mathrm{KL}(\rho,\pi)}{\lambda}\right\}-\sup_{\rho}\left\{\mathbb{E}_{ \rho}[\widehat{\mathrm{MMD}}_{k}^{2}]-\frac{\mathrm{KL}(\rho,\pi)}{\lambda}\right\}\] \[=\mathbb{E}_{\rho^{*}}[\mathrm{MMD}_{k}^{2}]-\frac{\mathrm{KL}( \rho^{*},\pi)}{\lambda}-\sup_{\rho}\left\{\mathbb{E}_{\rho}[\widehat{\mathrm{ MMD}}_{k}^{2}]-\frac{\mathrm{KL}(\rho,\pi)}{\lambda}\right\}\] \[\leq\mathbb{E}_{\rho^{*}}[\mathrm{MMD}_{k}^{2}]-\frac{\mathrm{KL }(\rho^{*},\pi)}{\lambda}-\left(\mathbb{E}_{\rho^{*}}[\widehat{\mathrm{MMD}}_ {k}^{2}]-\frac{\mathrm{KL}(\rho^{*},\pi)}{\lambda}\right)\] \[=\mathbb{E}_{\rho^{*}}[\mathrm{MMD}_{k}^{2}-\widehat{\mathrm{MMD }}_{k}^{2}]\] \[=\mathrm{MMD}_{K_{\rho^{*}}}^{2}(p,q)-\widehat{\mathrm{MMD}}_{K_{ \rho^{*}}}^{2}(\mathbf{Z}).\]

Now the above is for fixed kernel \(K_{\rho^{*}}\) independent of the data, so by Markov's inequality and Theorem 12, for any \(s>0\) we find

\[\mathrm{MMD}_{K_{\rho^{*}}}^{2}(p,q)-\widehat{\mathrm{MMD}}_{K_{ \rho^{*}}}^{2}(\mathbf{Z})\] \[\leq_{1-\delta}s^{-1}\log\mathbb{E}_{Z}\exp\left(s\left(\mathrm{ MMD}_{K_{\rho^{*}}}^{2}(p,q)-\widehat{\mathrm{MMD}}_{K_{\rho^{*}}}^{2}(\mathbf{Z}) \right)\right)+s^{-1}\log\delta^{-1}\] \[\leq 8s\kappa^{2}\left(\frac{1}{m}+\frac{1}{n}\right)+s^{-1}\log \delta^{-1}.\]

Optimising for \(s\) yields \(s=\kappa^{-1}\sqrt{\log(1/\delta)}/\sqrt{8(1/m+1/n)}\) which gives the desired result.

Power Analysis

### General Recipe for Power Analysis of Permutation Tests

The general outline for power analysis consists of the following: First we start with the Type II error probability. Then we attempt to upper bound it by iteratively applying the following simple lemma to the different terms.

Monotonicity in High Probability.Let \(X,Y\) be r.v.s, such that \(X\leq Y\) w.p. \(\geq 1-\delta\). Then \(\mathbb{P}(X\geq c)\leq\mathbb{P}(Y\geq c)+\delta\). This result can be applied both when \(X\leq Y\) a.s. giving \(\delta=0\), or if \(Y=a\) is constant.

Iteratively applying this lemma gives \(\mathbb{P}(\text{Type II})\leq N\delta\), and then we set \(N\delta=\beta\).

As a first step in the above process we will use the following useful result (adapted from Kim et al., 2022) to convert from the random permutations we use in practice to the full set of permutation. This result shows that when \(B\) is taken sufficiently large (roughly \(\Omega(\alpha^{-2}\log(\beta^{-1}))\)), the only changes to the final power results will be in constants multiplying \(\alpha\) and \(\beta\).

**Theorem 13** (From Randomised to Deterministic Permutations).: _Suppose \(G=(g_{1},\ldots,g_{B+1})\) consists of \(B\) uniformly drawn permutations from \(\mathcal{G}\), plus the identity permutation as \(g_{B+1}\). Then_

\[\mathbb{P}\left(\tau(\mathbf{Z})\leq\operatorname*{quantile}_{1-\alpha,G}\tau(g\bm {Z})\right)\leq\,\mathbb{P}\left(\tau(\mathbf{Z})\leq\operatorname*{quantile}_{1- \alpha_{B},\mathcal{G}}\tau(g\mathbf{Z})\right)+\delta\]

_where \(1-\alpha_{B}=\frac{B+1}{B}(1-\alpha)+\sqrt{\frac{\log(2/\delta)}{2B}}\)._

_We note that provided \(B\geq 8\alpha^{-2}\log(2/\delta)\), then \(1-\alpha_{B}\leq 1-\alpha/2\)._

**Proof.** We note the Dvoretzky-Kiefer-Wolfowitz inequality Dvoretzky et al. (1956); Massart (1990) for empirical CDF \(F_{n}\) of \(n\) samples from original CDF \(F\):

\[\mathbb{P}\left(\sup_{x}|F_{n}(x)-F(x)|\geq t\right)\leq 2e^{-2nt^{2}} \tag{12}\]

for every \(t>0\).

The permutation CDF for a group \(\mathcal{G}\) take the form

\[F_{\mathcal{G}}(x)=\frac{1}{|\mathcal{G}|}\sum_{g\in\mathcal{G}}\mathbf{1}\{ \tau(g\mathbf{Z})\leq x\}\]

and given sample \(\widetilde{G}=(g_{1},\ldots,g_{B})\) i.i.d. uniformly from \(\mathcal{G}\) (this excludes the identity permutation added to \(G\)), the empirical CDF is

\[\widehat{F}_{\widetilde{G}}(x)=\frac{1}{B}\sum_{g\in\widetilde{G}}\mathbf{1}\{ \tau(g\mathbf{Z})\leq x\}\]

We can also write \(\widehat{F}_{G}(x)\) including the identity permutation as \(g_{B+1}\), and note that \(\widehat{F}_{G}(x)=\) Define the good event

\[\mathcal{A}=\left\{\sup_{x}|F_{\widetilde{G}}(x)-F_{\mathcal{G}}(x)|\leq\sqrt{ \frac{\log(2/\delta)}{2B}}\right\}\]

which holds with probability at least \(1-\delta\) by Equation (12). Given \(\mathcal{A}\), we have

\[q_{G}:=\operatorname*{quantile}_{1-\alpha,g\in G}\tau(g\mathbf{Z}) =\inf\{r\in\mathbb{R}:\frac{1}{B+1}\sum_{g\in G}\mathbf{1}\{\tau (g\mathbf{Z})\leq r\}\geq 1-\alpha\}\] \[\leq\inf\{r\in\mathbb{R}:\frac{1}{B+1}\sum_{g\in G}\mathbf{1}\{ \tau(g\mathbf{Z})\leq r\}\geq 1-\alpha\}\]\[=\inf\{r\in\mathbb{R}:\widehat{F}_{\widetilde{G}}(r)\geq\frac{B+1}{B}( 1-\alpha)\}\] \[\leq\inf\left\{r\in\mathbb{R}:\widehat{F}_{\widetilde{G}}(r)\geq \frac{B+1}{B}(1-\alpha)+\sqrt{\frac{\log(2/\delta)}{2B}}\right\}\] \[=\underset{1\to\alpha_{B},g\in\mathcal{G}}{\text{ quantile }}\tau(g\mathbf{Z})\eqq\]

where we have defined \(\alpha_{B}\) as above.

Overall we find \(\mathbb{P}(\tau\leq q_{G})\leq\mathbb{P}(\tau\leq q_{G}|\mathcal{A})+\mathbb{P }(\mathcal{A}^{c})\leq\mathbb{P}(\tau\leq q)+\delta\). 

### Variance of \(\widehat{\mathrm{MMD}}_{k}^{2}\)

In proving our power results it is necessary to upper bound the variance of \(\widehat{\mathrm{MMD}}_{k}^{2}\).

**Theorem 14**.: _For any kernel \(k\) upper bounded by \(\kappa\), if \(n\leq m\leq cn\) for \(c\geq 1\), there exists universal constant \(C>0\) depending only on \(c\), such that_

\[\mathbb{V}[\widehat{\mathrm{MMD}}_{k}^{2}]\leq C\left(\frac{4\kappa\,\mathrm{ MMD}_{k}^{2}}{n}+\frac{\kappa^{2}}{n^{2}}\right)\]

**Proof.** We define

\[\sigma_{10}^{2} =\mathbb{V}_{X}(\mathbb{E}_{X^{\prime},Y,Y^{\prime}}[h(X,X^{ \prime},Y,Y^{\prime})])\] \[\sigma_{01}^{2} =\mathbb{V}_{Y}(\mathbb{E}_{X,X^{\prime},Y^{\prime}}[h(X,X^{ \prime},Y,Y^{\prime})])\] \[\sigma_{11}^{2} =\max\{\mathbb{E}[k^{2}(X,X^{\prime})],\mathbb{E}[k^{2}(X,Y)], \mathbb{E}[k^{2}(Y,Y^{\prime})]\}.\]

From a well-known bound (based on Lee, 1990, Equation 2, p.38; see also Kim et al., 2022, Appendix F, Equation 59 or Schrab et al., 2023, Proposition 3),

\[\mathbb{V}[\widehat{\mathrm{MMD}}_{k}^{2}] \leq C\left(\frac{\sigma_{10}^{2}}{m}+\frac{\sigma_{01}^{2}}{n} +\sigma_{11}^{2}\left(\frac{1}{m}+\frac{1}{n}\right)^{2}\right)\] \[\leq C\left(\frac{\sigma_{10}^{2}}{n}+\frac{\sigma_{01}^{2}}{n}+ \frac{\sigma_{11}^{2}}{n^{2}}\right)\]

where we used that \(n\leq m\leq cn\). and the result in red above, and the boundedness of the kernel for the final term. This gives the further bound

\[\mathbb{V}[\widehat{\mathrm{MMD}}_{k}^{2}]\leq C\left(\frac{4\kappa\,\mathrm{ MMD}_{k}^{2}}{n}+\frac{\kappa^{2}}{n^{2}}\right)\]

since

\[\sigma_{10}^{2} =\mathrm{var}_{X}(\mathbb{E}_{X^{\prime},Y,Y^{\prime}}[h(X,X^{ \prime},Y,Y^{\prime})])\] \[=\mathbb{E}_{X}\Big{[}\big{(}\mathbb{E}_{X^{\prime},Y,Y^{\prime}} [\langle\phi(X)-\phi(Y),\phi(X^{\prime})-\phi(Y^{\prime})\rangle]\big{)}^{2} \Big{]}\] \[=\mathbb{E}_{X}\Big{[}\langle\phi(X)-\mu_{Q},\mu_{P}-\mu_{Q} \rangle^{2}\Big{]}\] \[\leq\Big{(}\mathbb{E}_{X}\Big{[}\|\phi(X)-\mu_{Q}\|^{2}\Big{)} \Big{)}\|\mu_{P}-\mu_{Q}\|^{2}\] \[\leq 2\kappa\|\mu_{P}-\mu_{Q}\|^{2}\] \[=2\kappa\,\mathrm{MMD}_{k}^{2},\]

a similar result for \(\sigma_{01}\), and the simple bound \(\sigma_{11}^{2}\leq\kappa^{2}\).

### Proof of Theorem 2

**Proof.** Define \(\alpha_{B}\) as in Theorem 13, noting that \(1-\alpha_{B}<1-\alpha/2\) under the assumption \(B\geq 8\alpha^{-2}\log(4/\beta)\). From Theorem 13 we can consider the full permutation set as

\[\mathbb{P}_{p\times q,G}\left(\widehat{\mathrm{FUSE}}_{1}(\mathbf{Z}) \leq\underset{1-\alpha,G}{\mathrm{quantile}}\,\widehat{\mathrm{FUSE}}_{1}(\sigma \mathbf{Z})\right)\] \[\leq\mathbb{P}_{p\times q}\left(\widehat{\mathrm{FUSE}}_{1}(\mathbf{Z })\leq\underset{1-\alpha_{B},\mathcal{G}}{\mathrm{quantile}}\,\widehat{ \mathrm{FUSE}}_{1}(\sigma\mathbf{Z})\right)+\beta/2.\]

We also recall that from Theorem 5 when \(\lambda=cn/\kappa\),

\[\underset{1-\alpha_{B},\mathcal{G}}{\mathrm{quantile}}\,\widehat{\mathrm{FUSE }}_{1}(\sigma\mathbf{Z})\leq\frac{C_{1}\kappa(1+\log\alpha^{-1})}{n},\]

so that

\[\mathbb{P}_{p\times q}\left(\widehat{\mathrm{FUSE}}_{1}(Z)\leq\underset{1- \alpha_{B},\mathcal{G}}{\mathrm{quantile}}\,\widehat{\mathrm{FUSE}}_{1}(\sigma \mathbf{Z})\right)\leq\mathbb{P}_{p\times q}\left(\widehat{\mathrm{FUSE}}_{1}(\bm {Z})\leq\frac{C_{1}\kappa(1+\log\alpha^{-1})}{n}\right).\]

For any \(\rho\), we define

\[S_{\rho}=\mathrm{MMD}^{2}_{K_{\rho}}(p,q)-\frac{\mathrm{KL}(\rho,\pi)}{\lambda }-\frac{C_{1}\kappa(1+\log\alpha^{-1})}{n},\]

which we substitute into

\[\mathbb{P}_{p\times q}\left(\widehat{\mathrm{FUSE}}_{1}(\mathbf{Z}) \leq\frac{C_{1}\kappa(1+\log\alpha^{-1})}{n}\right)\] \[=\mathbb{P}\left(\mathrm{MMD}^{2}_{K_{\rho}}(p,q)-\frac{1}{ \lambda}\,\mathrm{KL}(\rho,\pi)-\widehat{\mathrm{FUSE}}_{1}(\mathbf{Z})\geq S_{ \rho}\right)\] \[=\mathbb{P}\left(\mathrm{MMD}^{2}_{K_{\rho}}(p,q)-\frac{1}{ \lambda}\,\mathrm{KL}(\rho,\pi)-\sup_{\rho^{\prime}}\left(\widehat{\mathrm{MMD }}^{2}_{K_{\rho^{\prime}}}(\mathbf{Z})-\frac{1}{\lambda}\,\mathrm{KL}(\rho^{\prime },\pi)\right)\geq S_{\rho}\right)\] \[\leq\mathbb{P}\left(\mathrm{MMD}^{2}_{K_{\rho}}(p,q)-\widehat{ \mathrm{MMD}}^{2}_{K_{\rho}}(\mathbf{Z})\geq S_{\rho}\right)\] \[=\frac{1}{S_{\rho}^{2}}\mathbb{V}_{p\times q}\left[\widehat{ \mathrm{MMD}}^{2}_{K_{\rho}}(\mathbf{Z})\right]\] \[\leq\frac{C_{2}}{S_{\rho}^{2}}\left(\frac{4\kappa\,\mathrm{MMD}^{ 2}}{n}+\frac{\kappa^{2}}{n^{2}}\right).\]

After substituting \(S_{\rho}\), we used the dual form of \(\widehat{\mathrm{FUSE}}_{1}\) and the inequalities \(\sup f(\rho^{\prime})\geq f(\rho)\), Chebyshev's, and Theorem 14. This term is upper bounded by \(\beta/2\) if we set

\[S_{\rho}^{2}>\frac{2C_{2}}{\beta}\left(\frac{4\kappa\,\mathrm{MMD}^{2}}{n}+ \frac{\kappa^{2}}{n^{2}}\right).\]

We also note that for \(a,b,x\) all non-negative, if \(x^{2}>a^{2}+2b\), then \(x^{2}>ax+b\). This works because \(x^{2}>ax+b\) is equivalent to \(x^{2}>\left(\frac{a}{2}+\sqrt{\frac{a^{2}}{4}+b}\right)^{2}\) by taking the positive root, and

\[\left(\frac{a}{2}+\sqrt{\frac{a^{2}}{4}+b}\right)^{2}=\frac{a^{2}}{2}+b+2 \frac{a}{2}\sqrt{\frac{a^{2}}{4}+b}\leq a^{2}+2b\]

using Young's inequality \(2AB\leq A^{2}+B^{2}\).

Combining the above the Type II error rate is controlled by \(\beta\) provided any of the following statements are true for any \(\rho\) (with each new result implying the former):

\[\mathrm{MMD}^{2}_{K_{\rho}}(p,q)>\frac{\mathrm{KL}(\rho,\pi)}{\lambda}+\frac{ C_{1}\kappa(1+\log\alpha^{-1})}{n}+\sqrt{\frac{2C_{2}}{\beta}\left(\frac{4 \kappa\,\mathrm{MMD}^{2}}{n}+\frac{\kappa^{2}}{n^{2}}\right)}\]\[\operatorname{MMD}^{2}_{K_{\rho}}(p,q) >\frac{\operatorname{KL}(\rho,\pi)}{\lambda}+\frac{C_{1}\kappa(1+ \log\alpha^{-1})}{n}+\sqrt{\frac{8C_{2}\kappa}{n\beta}}\operatorname{MMD}+ \frac{\sqrt{2C_{2}\kappa}}{n\sqrt{\beta}}\] \[\operatorname{MMD}^{2}_{K_{\rho}}(p,q) >\frac{2\operatorname{KL}(\rho,\pi)}{\lambda}+\frac{2C_{1}\kappa( 1+\log\alpha^{-1})}{n}+\frac{8C_{2}\kappa}{n\beta}+\frac{2\sqrt{2C_{2}\kappa}} {n\sqrt{\beta}}\] \[\operatorname{MMD}^{2}_{K_{\rho}}(p,q) >\frac{C_{3}\kappa}{n}\left(\frac{1}{\beta}+\log\frac{1}{\alpha} +\operatorname{KL}(\rho,\pi)\right)\]

where we used that \(\sqrt{x+y}\leq\sqrt{x}+\sqrt{y}\), the result above, and \(\lambda=cn/\kappa\). 

### Proof of Theorem 3

The proof of this statement proceeds similarly to the proof of Theorem 2. Using Theorem 5 with \(\lambda=cn\) and Theorem 13 we find that

\[\mathbb{P}_{p\times q,G}\left(\widetilde{\operatorname{FUSE}}_{N }(\mathbf{Z})\leq\operatorname*{quantile}_{1-\alpha,G}\widetilde{\operatorname{ FUSE}}_{N}(\sigma\mathbf{Z})\right)\] \[\qquad\leq\mathbb{P}_{p\times q}\left(\widetilde{\operatorname{ FUSE}}_{N}(\mathbf{Z})\leq\frac{C_{1}(1+\log\alpha^{-1})}{n}\right)+\beta/2.\]

For any \(\rho\) such that \(\operatorname{KL}(\rho,\pi)<\infty\) we define

\[T_{\rho} =\frac{C_{1}}{\kappa}\cdot\operatorname{MMD}^{2}_{K_{\rho}}\] \[S_{\rho} =T_{\rho}-\frac{1}{\lambda}\operatorname{KL}(\rho,\pi)-\frac{C_{1 }\kappa(1+\log\alpha^{-1})}{n}.\]

We assumed that \(n\leq m\leq cn\) for some \(c\geq 1\). Based on this, note that \(N_{k}\leq\kappa^{2}/C_{1}^{2}\) for \(C_{1}\) depending only on \(m/n\in[1,c]\), and (since \(\operatorname{MMD}^{2}\geq 0\) is strictly non-negative, unlike \(\widehat{\operatorname{MMD}}^{2}\) which can be negative),

\[-\mathbb{E}_{\rho}\left[\frac{\operatorname{MMD}^{2}_{k}}{\sqrt{\widehat{N}_{ k}(\mathbf{Z})}}\right]\leq-\frac{C_{1}}{\kappa}\mathbb{E}_{k\sim\rho}\left[ \operatorname{MMD}^{2}_{k}\right]=-T_{\rho}. \tag{13}\]

Now we introduce these definitions to bound

\[\mathbb{P}\left(\widetilde{\operatorname{FUSE}}_{N}(\mathbf{Z})\leq \frac{C_{1}(1+\log\alpha^{-1})}{n}\right)\] \[=\mathbb{P}\left(T_{\rho}-\frac{1}{\lambda}\operatorname{KL}( \rho,\pi)-\widetilde{\operatorname{FUSE}}_{N}(\mathbf{Z})\geq S_{\rho}\right)\] \[=\mathbb{P}\left(T_{\rho}-\frac{1}{\lambda}\operatorname{KL}( \rho,\pi)-\sup_{\rho^{\prime}}\left(\mathbb{E}_{k\sim\rho^{\prime}}\left[\frac {\widehat{\operatorname{MMD}}^{2}_{k}(\mathbf{Z})}{\sqrt{\widehat{N}_{k}(\mathbf{Z})} }\right]-\frac{1}{\lambda}\operatorname{KL}(\rho^{\prime},\pi)\right)\geq S_{ \rho}\right)\] \[\leq\mathbb{P}\left(T_{\rho}-\mathbb{E}_{k\sim\rho}\left[\frac{ \widehat{\operatorname{MMD}}^{2}_{k}(\mathbf{Z})}{\sqrt{\widehat{N}_{k}(\mathbf{Z})} }\right]\geq S_{\rho}\right)\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\[\leq\mathbb{P}\left(\mathbb{E}_{k\sim\rho}\left[\frac{\mathrm{MMD}_{k }^{2}-\widehat{\mathrm{MMD}}_{k}^{2}(\mathbf{Z})}{\sqrt{\widehat{N}_{k}(\mathbf{Z})}} \right]\geq S_{\rho}\right)\] by Equation (13) \[\leq\mathbb{P}\left(\left|\mathbb{E}_{k\sim\rho}\left[\frac{ \mathrm{MMD}_{k}^{2}-\widehat{\mathrm{MMD}}_{k}^{2}(\mathbf{Z})}{\sqrt{\widehat{N} _{k}(\mathbf{Z})}}\right]\right|\geq S_{\rho}\right) x\leq|x|\] \[\leq\mathbb{P}\left(\mathbb{E}_{k\sim\rho}\left[\left|\frac{ \mathrm{MMD}_{k}^{2}-\widehat{\mathrm{MMD}}_{k}^{2}(\mathbf{Z})}{\sqrt{\widehat{N} _{k}(\mathbf{Z})}}\right|\right]\geq S_{\rho}\right) |x|\] convex, Jensen \[=\mathbb{P}\left(\mathbb{E}_{k\sim\rho}\left[\frac{|\mathrm{MMD}_{ k}^{2}-\widehat{\mathrm{MMD}}_{k}^{2}(\mathbf{Z})|}{\sqrt{\widehat{N}_{k}(\mathbf{Z})}} \right]\geq S_{\rho}\right) \widehat{N}_{k}\text{ positive}\] \[\leq\frac{1}{S_{\rho}}\mathbb{E}_{\mathbf{Z}}\mathbb{E}_{k\sim\rho} \left[\frac{|\mathrm{MMD}_{k}^{2}-\widehat{\mathrm{MMD}}_{k}^{2}(\mathbf{Z})|}{ \sqrt{\widehat{N}_{k}(\mathbf{Z})}}\right] \widehat{N}_{k}\text{ positive, Markov}\] \[\leq\frac{1}{S_{\rho}}\sqrt{\mathbb{E}_{\mathbf{Z}}\mathbb{E}_{k\sim \rho}\left[|\mathrm{MMD}_{k}^{2}-\widehat{\mathrm{MMD}}_{k}^{2}(\mathbf{Z})|^{2} \right]\mathbb{E}_{\mathbf{Z}}\mathbb{E}_{k\sim\rho}\left[\frac{1}{\widehat{N}_{k}( \mathbf{Z})}\right]\] Cauchy-Schwarz.

Thus the type II error will be controlled by \(\beta\) if for any \(\rho\)

\[S_{\rho}>\frac{2}{\beta}\sqrt{\mathbb{E}_{\mathbf{Z}}\mathbb{E}_{k\sim\rho}\left[| \mathrm{MMD}_{k}^{2}-\widehat{\mathrm{MMD}}_{k}^{2}(\mathbf{Z})|^{2}\right]\mathbb{ E}_{\mathbf{Z}}\mathbb{E}_{k\sim\rho}\left[\frac{1}{\widehat{N}_{k}(\mathbf{Z})}\right]}.\]

Provided there is a \(c>0\) with

\[\mathbb{E}_{p\times q}\left[\frac{1}{\widehat{N}_{k}(\mathbf{Z})}\right]\leq c\]

for all \(k\), this reduces to the condition

\[\mathrm{MMD}_{K_{\rho}}^{2}(p,q)\,>\,C_{2}\kappa\left(\frac{\log\alpha^{-1}}{ n}+\frac{1}{\beta}\sqrt{\mathbb{E}_{\rho}\mathbb{V}_{p\times q}\left[\widehat{ \mathrm{MMD}}_{k}^{2}(\mathbf{Z})\right]}+\frac{\mathrm{KL}(\rho,\pi)}{n}\right).\]

Applying Theorem 14, the proof is completed in essentially the same way as in the result for \(\widehat{\mathrm{FUSE}}_{1}\) (with slightly different \(\beta\) dependence).