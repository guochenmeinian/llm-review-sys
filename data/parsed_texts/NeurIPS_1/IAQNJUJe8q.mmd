# Full-Atom Peptide Design

with Geometric Latent Diffusion

 Xiangzhe Kong\({}^{1,2}\) Yinjun Jia\({}^{3}\) Wenbing Huang\({}^{4}\) Yang Liu\({}^{1,2,5}\)

\({}^{1}\)Dept. of Comp. Sci. & Tech., Tsinghua University

\({}^{2}\)Institute for AIR, Tsinghua University

\({}^{3}\)School of Life Sciences, Tsinghua University

\({}^{4}\)Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{5}\)Shanghai Artificial Intelligence Laboratory, Shanghai, China

Correspondence to Wenbing Huang <hwenbing@126.com>, Yang Liu <liuyang2011@tsinghua.edu.cn>

###### Abstract

Peptide design plays a pivotal role in therapeutics, allowing brand new possibility to leverage target binding sites that are previously undruggable. Most existing methods are either inefficient or only concerned with the target-agnostic design of 1D sequences. In this paper, we propose a generative model for full-atom **Peptide** design with **G**eometric **LA**tent **D**iffusion (PepGLAD) given the binding site. We first establish a benchmark consisting of both 1D sequences and 3D structures from Protein Data Bank (PDB) and literature for systematic evaluation. We then identify two major challenges of leveraging current diffusion-based models for peptide design: the full-atom geometry and the variable binding geometry. To tackle the first challenge, PepGLAD derives a variational autoencoder that first encodes full-atom residues of variable size into fixed-dimensional latent representations, and then decodes back to the residue space after conducting the diffusion process in the latent space. For the second issue, PepGLAD explores a receptor-specific affine transformation to convert the 3D coordinates into a shared standard space, enabling better generalization ability across different binding shapes. Experimental Results show that our method not only improves diversity and binding affinity significantly in the task of sequence-structure co-design, but also excels at recovering reference structures for binding conformation generation.

## 1 Introduction

Peptides are short chains of amino acids and acts as vital mediators of many protein-protein interactions in human cells. Designing functional peptides has attracted increasing attention in biological research and therapeutics, since the highly-flexible conformation space of peptides allows brand new possibility to target binding sites previously undruggable with antibodies or small molecules [17, 35]. The key of peptide design is to generate peptides that interact compactly with target proteins (see Figure 1), since they mostly exhibit flexible conformations [20] unless bound to these receptors [60].

Conventional simulation or searching algorithms rely on frequent calculations of physical energy functions [6, 9], which are inefficient and prone to poor local optimum. Recent advances illuminate the remarkable success of exploiting geometric deep generative models, particularly the equivariant diffusion models [45, 72], for molecule design [21, 39], antibody design [29, 44, 34] and protein design [64, 28], as well as latent diffusion models further enhancing the performance [71, 18]. Inspired by these successes, a natural idea is leveraging diffusion models for peptide design as well, which, yet, is challenging in two aspects. From the dataset aspect, existing databases (PepBDB [65], Propedia [46]) merely collect data from Protein Data Bank (PDB) [5], neither performing filters according to practical relevance [49] and redundancy, nor providing an adequate split for evaluation.

Therefore, this paper first curates a benchmark from PDB [5] and the literature [59], and then systematically evaluates the generative models in terms of diversity, consistency, and binding affinity.

From the methodology aspect, it is nontrivial to adopt latent diffusion models to characterize the geometry of protein-peptide interactions. The first nontriviality stems from the _full-atom geometry_, which determines the comprehensive protein-peptide interactions in the atomic level, yet difficult to preserve. Throughout the generation process, the type of each amino acid always changes and thus requires us to generate different number of atoms, which is unfriendly to diffusion models that prefer fixed-size generation. Current latent diffusion models on molecules [71] or protein backbones [18] still tackle tasks with fixed number of atoms, thus leaving this challenge untouched. The second nontriviality lies in the _variable binding geometry_. Diffusion models are typically implemented directly in the data space, which might be suitable for regular data (_e.g._ images with fixed value range), yet ill-suited for our case on 3D coordinates where the value range is not fixed and even cursed with high variances due to the rich diversity in protein-peptide interactions. These variances define divergent target distributions of Gaussian with disparate expectation and covariance, which hinders the transferability of the diffusion process across different binding sites and thereby yields unsatisfactory generalization capability. Unfortunately, this point is seldom investigated previously.

To address the above problems, we propose a powerful model for full-atom **Pep**tide design with **Geometric LA**tent **D**iffusion (PepGLAD) with the following contributions:

* We construct a new benchmark from PDB and literature based on practical relevance and non-redundancy, then systematically evaluate available sequence-structure co-design models on the task of target-specific peptide design.
* To capture the _full-atom geometry_, we first learn a Variational AutoEncoder (VAE) to obtain a fixed-size latent representation (including a 3D coordinate and a hidden feature) for each residue of the input peptide, and then conduct the diffusion process in this latent space, both of which are conditioned on the binding site to better model protein-peptide interactions. Notably, the proposed design enables our model to accommodate full-atom input and output.
* Regarding the _variable binding geometry_, we derive a shared standard space from the binding sites by proposing a novel skill--receptor-specific affine transformation. Such affine transformation is computed by the center offset and covariance Cholesky decomposition of the binding site coordinates, serving as a mapping from the binding site distribution to standard Gaussian. With the affine transformation applied to both the binding sites and the peptides, we are able to project the shape of all complexes into approximately standard Gaussian distribution, which facilitates generalization to diverse binding sites.

Favorably, all the aforementioned models and processes meet the desired symmetry, _i.e._, E(3)-equivariance, as proved by us. Experiments on sequence-structure co-design and complex conformation generation demonstrate the superiority of PepGLAD over the existing generative models.

Figure 1: **Left**: Peptide design requires generating peptides that form compact interactions with the binding site on the receptor. The intricacy of protein-peptide interactions demands efficient exploration in the vast space for sequence-structure co-design. **Right**: Different binding sites (a, b, c, d) adopt disparate center offsets and geometric shapes, approximating variable 3D Gaussian distributions that deviate from \(\mathcal{N}(\mathbf{0},\boldsymbol{I})\). We propose to convert the geometry into a standard space approximating standard Gaussian, via an affine transformation derived from the binding site (ยง3.3).

Related Work

**Peptide Design** Conventional methods directly sample residues [6] or building blocks from libraries containing small fragments of proteins [26; 57; 9; 8], with guidance from delicate physical energy functions [2]. These methods are time-consuming and easy to be trapped by local optimum. Recent advances with deep generative models mainly focuses on target-agnostic 1D language models [48], antimicrobial peptides [13; 63], or a subtype of peptides with \(\alpha\)-helix [68; 69]. While geometric deep generative models are exhibiting notable potential in other domains of target-specific binder design (_e.g._ antibodies), their capability of target-specific peptide design remains unclear, which is the first problem we answer in this paper. Other contemporary work includes peptide design algorithms with flow matching frameworks [38; 40].

**Geometric Protein/Antibody Design** Protein design primarily aims to generate stable secondary or tertiary structures [27], where diffusion models demonstrate inspiring performance [66; 58; 3; 72]. In particular, RFDiffusion [64] first generates backbones via diffusion, and then designs the sequences through cycles of inverse folding and structure refining with empirical force fields. Chroma [28] adopts a similar strategy, but further explores controllable generative process with custom energy functions. Antibody design, encompassing a special family of proteins in the immune system to capture antigens, mainly focuses on inpainting complementarity-determining regions (CDRs) at the interface between the antigen and the framework [33; 34; 61], where the geometric diffusion models exhibit promising potential [44; 45] in co-designing sequence and structure. Unlike antibodies which are constrained by framework regions, peptides exhibit a more irregular binding pattern and greater flexibility, adapting to binding sites upon interaction [35]. Thus the target distributions are remarkably divergent on different binding sites, posing an urgent need for more robust generative modeling.

**Geometric Latent Diffusion Models** Diffusion models learn a denoising trajectory to generate desired data distribution from a prior distribution, commonly standard Gaussian [54; 55; 25]. Recent literature extends diffusion to 3D small molecules satisfying the E(3)-equivariance [70], which triggers subsequent advances in geometric design of macro molecules (_e.g._ antibody, protein) as aforementioned. Further efforts are made to latent diffusion models [53; 71; 18], which implement the generative process in the compressed latent space of pretrained auto-encoders, to improve the performance. Compared to the literature that either encodes atom-wise representation in the latent space for small molecule generation [71], or compress a fixed number of atoms into one latent node for protein backbone generation [18], we explore compression of the full-atom geometry by directly generating different residues with variable number of atoms in the latent space. Moreover, we propose a novel technique, namely the data-specific affine transformations, to enhance the generalization ability of diffusion models, which is barely explored before.

## 3 Our Method: PepGLAD

We first define the notations in the paper and formalize peptide design in SS3.1. The overall workflow of our PepGLAD is presented in Figure 2, which consists of three modules: (1) An autoencoder that defines the joint latent space for sequences and structures conditioned on the full-atom context of the binding site (SS3.2); (2) An affine transformation derived from the binding site to project the 3D geometry into a standard space approximating standard Gaussian distribution (SS3.3); (3) A latent diffusion model trained on the standard latent space (SS3.4). Finally, we summarize the training and the sampling procedures in SS3.5.

### Definitions and Notations

We represent binding sites and peptides as geometric graphs \(\mathcal{G}=\{(x_{i},\vec{\mathbf{X}}_{i})\}\), where each node \(i\) is a residue with its amino acid type \(x_{i}\) and the coordinates of all its \(c_{i}\) atoms \(\vec{\mathbf{X}}_{i}\in\mathbb{R}^{c_{i}\times 3}\). In later sections, we use the simplified notations \(i\in\mathcal{G}\) to denote that a node \(i\) is in the geometric graph \(\mathcal{G}\), and \(|\mathcal{G}|\) to denote the total number of nodes in \(\mathcal{G}\). We use \(\mathcal{G}_{p}\) and \(\mathcal{G}_{b}\) to represent the geometric graph of the peptide and the binding site, respectively. In this work, the binding site incorporates residues on the target protein within 10A distances to the peptide residues based on \(C_{\beta}\) atoms which alleviates leakage of the side-chain interactions. Note that the threshold (10A) is chosen to be large to better reduce leakage of the peptide geometry.

Task DefinitionGiven the binding site \(\mathcal{G}_{b}\), we aim to obtain a generative model \(p_{\theta}\) conforming to the distribution of binding peptides \(q(\mathcal{G}_{p}|\mathcal{G}_{b})\).

### Variational AutoEncoder

The autoencoder [62] consists of an encoder \(\mathcal{E}_{\phi}\) that encodes the peptide \(\mathcal{G}_{p}\) in the presence of the binding site \(\mathcal{G}_{b}\) into a latent state \(\mathcal{G}_{z}\), and a decoder \(\mathcal{D}_{\xi}\) that reconstructs the peptide from the latent state to obtain \(\mathcal{G}^{\prime}_{p}=\{(x^{\prime}_{i},\vec{\mathbf{X}}^{\prime}_{i})\}\). To encourage \(\mathcal{E}_{\phi}\) to learn contextual representations of residues, we corrupt 25% of the residues in \(\mathcal{G}_{p}\) with a \([\mathrm{MASK}]\) type to obtain \(\tilde{\mathcal{G}}_{p}\) as the input:

\[\mathcal{G}_{z}=\mathcal{E}_{\phi}(\tilde{\mathcal{G}}_{p},\mathcal{G}_{b}), \qquad\mathcal{G}^{\prime}_{p}=\mathcal{D}_{\xi}(\mathcal{G}_{z},\mathcal{G}_ {b}), \tag{1}\]

where \(\mathcal{G}_{z}=\{(\mathbf{z}_{i},\vec{\mathbf{z}}_{i})|i\in\mathcal{G}_{p}\}\) contains the latent states \(\mathbf{z}_{i}\in\mathbb{R}^{h}\) (\(h=8\) in this paper) and \(\vec{\mathbf{z}}_{i}\in\mathbb{R}^{3}\) sampled from the encoded distribution \(\mathcal{N}(\mathbf{z}_{i};\mathbf{\mu}_{i},\mathbf{\sigma}_{i})\) and \(\mathcal{N}(\vec{\mathbf{z}}_{i};\vec{\mathbf{\mu}}_{i},\vec{\mathbf{\sigma}}_{i})\) using the reparameterization trick [32]. We borrow the adaptive multi-channel equivariant encoder in dyMEAN [34] for both \(\mathcal{E}_{\phi}\) and \(\mathcal{D}_{\xi}\) to capture the full-atom geometry. In the decoder \(\mathcal{D}_{\xi}\), we factorize the joint distribution of sequences and structures as follows:

\[p_{\xi}(x^{\prime}_{i},\vec{\mathbf{X}}^{\prime}_{i}|\mathcal{G}_{z},\mathcal{G}_ {b})=p_{\xi_{1}}(x^{\prime}_{i}|\mathcal{G}_{z},\mathcal{G}_{b})p_{\xi_{2}}( \vec{\mathbf{X}}^{\prime}_{i}|x^{\prime}_{i},\mathcal{G}_{z},\mathcal{G}_{b}), \tag{2}\]

where the sequence is first decoded and then the all-atom geometry, initialized with replications of \(\vec{\mathbf{z}}_{i}\), is reconstructed. The training objective of the autoencoder consists of the reconstruction loss \(\mathcal{L}_{recom}\) and the KL divergence \(\mathcal{L}_{KL}\) to constrain the latent space. The reconstruction loss includes cross entropy on the residue types, mean square error (MSE) on the full-atom structures, and an auxilary loss \(\mathcal{L}_{aux}\) on bond lengths and angles [31]:

\[\mathcal{L}_{recom}(i)=H(p(x_{i}),p(x^{\prime}_{i}))+\mathrm{MSE}(\vec{\mathbf{X} }_{i},\vec{\mathbf{X}}^{\prime}_{i})+\mathcal{L}_{aux}(i), \tag{3}\]

where \(H\) denotes cross entropy. We include details of \(\mathcal{L}_{aux}\) in Appendix A. The KL divergence constrains \(\mathbf{z}_{i}\) and \(\vec{\mathbf{z}}_{i}\) with the prior \(\mathcal{N}(\mathbf{0},\mathbf{I})\) and \(\mathcal{N}(\vec{\mathbf{r}}_{i},\mathbf{I})\), respectively, where \(\vec{\mathbf{r}}_{i}\) denotes the coordinate of the alpha carbon (\(\mathtt{c}_{\alpha}\)) in node \(i\):

\[\mathcal{L}_{KL}(i)=\lambda_{1}\cdot D_{\mathrm{KL}}(\mathcal{N}(\mathbf{0},\mathbf{I })\|\mathcal{N}(\mathbf{\mu}_{i},\mathrm{diag}(\mathbf{\sigma}_{i})))+\lambda_{2} \cdot D_{\mathrm{KL}}(\mathcal{N}(\vec{\mathbf{r}}_{i},\mathbf{I})\|\mathcal{N}(\vec {\mathbf{\mu}}_{i},\mathrm{diag}(\vec{\mathbf{\sigma}}_{i}))), \tag{4}\]

where \(D_{\mathrm{KL}}\) denotes the KL divergence, \(\lambda_{1}\) and \(\lambda_{2}\) reweight the contraints on the sequence and the structure, respectively. \(\mathcal{L}_{KL}\) prevents the scale of \(\mathbf{z}_{i}\) from exploding and constrains \(\vec{\mathbf{z}}_{i}\) around \(\mathtt{C}_{\alpha}\) to retain necessary geometric information. Such regularization also helps ensure consistent scales between the peptide latent coordinates and the pocket, mitigating potential issues arising from their different levels of abstraction. Then we have the overall training objective of the variational autoencoder as follows:

\[\mathcal{L}_{AE}=\sum\nolimits_{i\in\mathcal{G}_{p}}(\mathcal{L}_{recom}(i)+ \mathcal{L}_{KL}(i))/|\mathcal{G}_{p}|. \tag{5}\]

Figure 2: Overall architecture of PepGLAD. **(A)** Variational AutoEncoder (ยง3.2): compressing the sequence and the structure \(\{(x_{i},\vec{\mathbf{X}}_{i})\}\) of the peptide into the latent space \(\{(\mathbf{z}_{i},\vec{\mathbf{z}}_{i})\}\) with the encoder \(\mathcal{E}_{\phi}\), and decoding the sequence and full-atom geometry from the latent states with the decoder \(\mathcal{D}_{\xi}\). **(B)** Affine Transformation \(F\) (ยง3.3): projecting the geometry to approximately \(\mathcal{N}(\mathbf{0},\mathbf{I})\) via the receptor-specific affine transformation derived from the binding site, and recovering the data geometry with the inverse of \(F\) after the diffusion generative process. **(C)** Latent Diffusion (ยง3.4): jointly generating \(\mathbf{z}_{i}\) and \(\vec{\mathbf{z}}_{i}\) in the standard latent space.

We have explored \(\mathrm{E}(3)\)-invariant latent space, which appears to have difficulties in reconstructing the full-atom structures since it lacks information of geometric interactions with the pocket atoms (Appendix F).

### Receptor-Specific Affine Transformation

With the latent space given by the autoencoder, we further exploit a standard space obtained from receptor-specific affine transformations, which enhances the transferability of diffusions on disparate binding sites (see Figure 1). Most peptides fold into complementary shape upon binding on the receptor [60, 41]. Thus, the target distribution is inherently characterized by the shape of the binding site. Given the wide disparity in binding geometries, directly implementing diffusion in the data space yields minimal transferability among different binding sites. To address this deficiency, we propose to implement the diffusion process on a shared standard space converted via an affine transformation derived from the binding site. Formally, denoting the \(\mathcal{C}_{\alpha}\) coordinates of the residues in a given binding site \(\mathcal{G}_{b}\) as \(\vec{\mathbf{R}}\in\mathbb{R}^{3\times|\mathcal{G}_{b}|}\), we can derive their center \(\vec{\mathbf{\mu}}=\mathbb{E}[\vec{\mathbf{R}}]\in\mathbb{R}^{3}\) and covariance \(\vec{\mathbf{\Sigma}}=\mathrm{Cov}(\vec{\mathbf{R}},\vec{\mathbf{R}})\in\mathbb{R}^{3\times 3}\), so that these coordinates can be regarded as sampled from the distribution \(\mathcal{N}(\vec{\mathbf{\mu}},\vec{\mathbf{\Sigma}})\). We then calculate the Cholesky decomposition [19] of \(\vec{\mathbf{\Sigma}}\):

\[\vec{\mathbf{\Sigma}}=\vec{\mathbf{L}}\vec{\mathbf{L}}^{\top},\vec{\mathbf{L}}\in\mathbb{R}^{3 \times 3}, \tag{6}\]

where \(\vec{\mathbf{L}}\) is a lower triangular matrix. \(\vec{\mathbf{L}}\) is unique [19] and invertible since the covariance matrix is a real-valued symmetric positive-definite matrix2. Then we can define the affine transformation \(F:\mathbb{R}^{3}\rightarrow\mathbb{R}^{3}\), which enables the projection of the geometry into the standard space approximating standard Gaussian \(F(\vec{\mathbf{R}})\sim\mathcal{N}(\mathbf{0},\mathbf{I})\). Further, we can easily obtain the inverse of \(F\) as:

Footnote 2: The binding site has at least 3 nonoverlapping nodes, namely \(\mathrm{rank}(\vec{\mathbf{R}})=3\), thus we can ignore the corner case of semi-positive definite matrices.

\[F(\vec{\mathbf{x}})=\vec{\mathbf{L}}^{-1}(\vec{\mathbf{x}}-\vec{\mathbf{\mu}}),\qquad F^{-1}( \vec{\mathbf{x}})=\vec{\mathbf{L}}\vec{\mathbf{x}}+\vec{\mathbf{\mu}}. \tag{7}\]

With the above definitions, for each given binding site \(\mathcal{G}_{b}\), we transform the geometry via the derived \(F\) to obtain the standard space, where the diffusion model is implemented, and recover the original geometry with \(F^{-1}\) (see Figure 2) after generation. Notably, we have the following proposition to ensure that the equivariance is maintained under the proposed affine transformation with scalarization-based equivariant GNNs [23, 16]:

**Proposition 3.1**.: _Denote the invariant and equivariant outputs from a scalarization-based E(3)-equivariant GNN as \(f(\{\mathbf{h}_{i},\vec{\mathbf{x}}_{i}\})\) and \(\vec{f}(\{\mathbf{h}_{i},\vec{\mathbf{x}}_{i}\})\), respectively. With the definition of \(F\) in Eq. 7, \(\forall g\in E(3)\), we have \(f(\{\mathbf{h}_{i},F(\vec{\mathbf{x}}_{i})\})=f(\{\mathbf{h}_{i},F_{g}(g\cdot\vec{\mathbf{x}} _{i})\})\) and \(g\cdot F^{-1}(\vec{f}(\{\mathbf{h}_{i},F(\vec{\mathbf{x}}_{i})\}))=F_{g}^{-1}(\vec{f}( \{\mathbf{h}_{i},F_{g}(g\cdot\vec{\mathbf{x}}_{i})\}))\), where \(F_{g}\) is derived on the coordinates transformed by \(g\). Namely, the E(3)-equivariance is preserved if we implement the GNN on the standard space and recover the original geometry from the outputs._

The proof is in Appendix B. This is vital since it indicates the Markov kernel is E(3)-equivariant, and thus ensures the E(3)-invariance of the probability density in the diffusion process [70]. Note that our variational autoencoder (SS 3.2) and latent diffusion model (SS 3.4) are already designed to be equivariant even without the proposed affine transformation here. The purpose of defining such component is to encourage better generalization of the diffusion processes. Indeed, it is nontrivial to analyze whether such an implementation will break the equivariance of our workflow. Luckily, Proposition 3.1 manages to prove that scalarization-based equivariant networks [23], which is used in our autoencoder and diffusion model, are seamlessly compatible with such affine transformation, naturally preserving equivariance without any requirements of adaption.

### Geometric Latent Diffusion Model

With the aforementioned preparations, the discrete residue types are encoded as continuous latent representations \(\{\mathbf{z}_{i}\}\), and the full-atom geometry is also compressed and standardized into 3D vectors \(\{\vec{\mathbf{z}}_{i}\}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\). Therefore, we are ready to implement a diffusion model on the standard latent space to generate \(\mathbf{z}_{i}\) and \(\vec{\mathbf{z}}_{i}\). The forward diffusion process gradually adds noise to the data from \(t=0\) to \(t=T\), resulting in the prior distribution \(\mathcal{N}(\mathbf{0},\mathbf{I})\). The reverse diffusion process generates data distribution by iteratively denosing the distribution from \(t=T\) to \(t=0\). We denote \(\vec{\mathbf{u}}_{i}^{t}=[\vec{x}_{i}^{t},\vec{\mathbf{z}}_{i}^{t}]\) and \(\mathcal{G}_{z}^{t}=\{(\mathbf{z}_{i}^{t},\vec{\mathbf{z}}_{i}^{t})\}\) as the intermediate state for node \(i\) and the entire peptide at time step \(t\), respectively. For simplicity, we assume both \(\mathcal{G}_{z}^{t}\) and the binding site \(\mathcal{G}_{b}\) are already standardized via the transformation \(F_{b}\) in Eq. 7. Then we have the forward process as:

\[q(\vec{\mathbf{u}}_{i}^{t}|\vec{\mathbf{u}}_{i}^{t-1}) =\mathcal{N}(\vec{\mathbf{u}}_{i}^{t};\sqrt{1-\beta^{t}}\cdot\vec{\mathbf{ u}}_{i}^{t-1},\beta^{t}\mathbf{I}), \tag{8}\] \[q(\vec{\mathbf{u}}_{i}^{t}|\vec{\mathbf{u}}_{i}^{t}) =\mathcal{N}(\vec{\mathbf{u}}_{i}^{t};\sqrt{\bar{\mathbf{\alpha}}^{t}} \cdot\vec{\mathbf{u}}_{i}^{0},(1-\bar{\alpha}^{t})\mathbf{I}), \tag{9}\]

where \(\beta^{t}\) is the noise scale increasing with the timestep from \(0\) to \(1\) conforming to the cosine schedule [51], and \(\bar{\alpha}^{t}=\prod_{s=1}^{s=t}(1-\beta^{s})\). Then the state at timestep \(t\) can be sampled as:

\[\vec{\mathbf{u}}_{i}^{t}=\sqrt{\bar{\alpha}^{t}}\vec{\mathbf{u}}_{i}^{t}+(1-\bar{ \alpha}^{t})\mathbf{\epsilon}_{i}, \tag{10}\]

where \(\mathbf{\epsilon}_{i}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\). Following Ho et al. [25], the reverse process can be defined with the reparameterization trick as:

\[p_{\theta}(\vec{\mathbf{u}}_{i}^{t-1}|\mathcal{G}_{z}^{t},\mathcal{G}_{b})= \mathcal{N}(\vec{\mathbf{u}}_{i}^{t-1};\vec{\mathbf{\mu}}_{\theta}(\mathcal{G}_{z}^{t},\mathcal{G}_{b}),\beta^{t}\mathbf{I}), \tag{11}\]

\[\vec{\mathbf{\mu}}_{\theta}(\mathcal{G}_{z}^{t},\mathcal{G}_{b})=\frac{1}{\sqrt{ \alpha^{t}}}(\vec{\mathbf{u}}_{i}^{t}-\frac{\beta^{t}}{\sqrt{1-\bar{\alpha}^{t}}} \mathbf{\epsilon}_{\theta}(\mathcal{G}_{z}^{t},\mathcal{G}_{b},t)[i]), \tag{12}\]

where \(\alpha^{t}=1-\beta^{t}\), and \(\mathbf{\epsilon}_{\theta}\) is the denoising network also implemented with the equivariant adaptive multi-channel equivariant encoder in dyMEAN [34] to retain full-atom context of the binding site during generation and preserve the equivariance under affine transformations (Proposition 3.1). Finally, we have the objective at time step \(t\) as MSE between the predicted noise and the added noise in Eq. 10, as well as the overall training objective \(\mathcal{L}_{\textit{LDM}}\) as the expectation with respect to \(t\):

\[\mathcal{L}_{\textit{LDM}}=\mathbb{E}_{t\sim\mathrm{Uniform}(1\dots T)}[\sum _{i}\|\mathbf{\epsilon}_{i}-\mathbf{\epsilon}_{\theta}(\mathcal{G}_{z}^{t},\mathcal{G }_{b},t)[i]\|^{2}/|\mathcal{G}_{z}^{t}|]. \tag{13}\]

### Training and Sampling

TrainingThe training of our PepGLAD can be divided into two phases where a variational autoencoder is first trained and then a diffusion model is trained on the standard latent space. We provide the overall training procedure in Algorithm 1 (see Appendix D). Note that a smooth and informative latent space is necessary for the consecutive training of the diffusion model, thus we resort to unsupervised data from protein fragments apart from the limited protein-peptide complexes for training the autoencoder, which we describe in Appendix E.

**Sampling in Ordered Subspace** The sampling procedure includes generative diffusion process on the standard latent states, recovering the original geometry with the inverse of \(F\) in Eq. 7, and decoding the sequence as well as the full-atom structure of the peptide (see Algorithm 2 in Appendix D). A problem here is that the unordered nature of graphs is not compatible with the sequential nature of peptides, thus the generated residues may have arbitrary permutation on the sequence order. Inspired by the concept of classifier-guided sampling [15], we first assign an arbitrary permutation \(\mathcal{P}\) on the sequence order to the nodes. Then we steer the sampling procedure towards the desired subspace conforming to \(\mathcal{P}\) with the following empirical classifier \(p(1|\{\vec{\mathbf{z}}_{i}^{t}\})\), which estimates the probability of the current coordinates belonging to the desired subspace:

\[p(1|\{\vec{\mathbf{z}}_{i}^{t}\}) =\exp(-\sum\nolimits_{\mathcal{P}(i)-\mathcal{P}(j)=1}E(\|\vec{ \mathbf{z}}_{i}^{t}-\vec{\mathbf{z}}_{j}^{t}\|)), \tag{14}\] \[E(d) =\left\{\begin{array}{ll}d-(\mu_{d}+3\sigma_{d}),&d>\mu_{d}+3 \sigma_{d},\\ (\mu_{d}-3\sigma_{d})-d,&d<\mu_{d}-3\sigma_{d},\\ 0,&\text{otherwise},\end{array}\right. \tag{15}\]

where \(\mu_{d}\) and \(\sigma_{d}\) are the mean and variance of the distances of adjacent residues in the latent space measured from the training set. Intuitively, this classifier gives higher confidence if the adjacent (defined by \(\mathcal{P}\)) residues are within reasonable distances aligning with the statistics from the training set. Nevertheless, the effect of the guidance is relatively minor, which is only a technical trick to enhance the robustness. We provide more details in Appendix G.

## 4 Experiments

### Setup

**Task** We evaluate our PepGLAD and baselines on the following tasks: (1) **sequence-structure co-design** (SS4.2) aims to generate both the sequence and the structure of the peptide given the specific binding site on the receptor (_i.e._ protein). (2) **Binding Conformation Generation** (SS4.3) requires to generate the binding state of the peptide given its sequence and the binding site of interest.

**Dataset** We first extract all dimers from the Protein Data Bank (PDB) [5] and select the complexes with a receptor longer than 30 residues and a ligand between 4 to 25 residues [59]. Then we remove the duplicated complexes with the criterion that both the receptor and the peptide has a sequence identity over 90% [56], after which 6105 non-redundant complexes are obtained. To achieve the cross-target generalization test, we utilize the large non-redundant dataset (LNR) from Tsaban et al. [59] as the test set, which contains 93 protein-peptide complexes with canonical amino acids curated by domain experts. We then cluster the data by receptor with a sequence identity threshold of over 40%, and remove the complexes sharing the same clusters with those from the test set. Finally, the remaining data are randomly split based on clustering results into training and validation sets, yielding a new benchmark calling **PepBench**. Further, we exploit 70k unsupervised data from protein fragments (**ProtFrag**) to facilitate training of the variational autoencoder. We also implement a split on **PepBDB**[65] based on clustering results for evaluation. We show details and statistics of these datasets in Appendix E.

**Baselines** We first borrow three baselines from the antibody design domain. **HSRN**[30] autoregressively decodes the sequence while keeps refining the structure hierarchically, from the \(\mathsf{C}_{\alpha}\) to other atoms. **dyMEAN**[34] is equipped with an full-atom geometric encoder and exploits iterative non-autoregressive generation. **DiffAb**[44] jointly diffuses on the categorical residue type, the coordinate of \(\mathsf{C}_{\alpha}\) as well as the orientation of each residue. Next, we explore two baselines from the general protein design. **RFDiffusion**[64] exploits a pipeline that first generates the backbone via diffusion and then alternates between inverse folding [14] and structure refining based on a physical energy function [2]. **AlphaFold 2**[31] is the well-known model for protein folding, which also shows certain abilities on peptide conformation prediction [59]. We also include two traditional methods. **AnchorExtension**[26] designs peptides by first docking an existing scaffold to the binding site, and then optimizing the peptide with cycles of mutations guided by energy functions. **FlexPepDock**[42] is designed for flexible peptide docking via optimization in the landscape of a physical energy function [2]. Implementation details are provided in Appendix I.

### Sequence-Structure Co-Design

**Metrics** A favorable generative model should produce diverse candidates while maintaining fidelity to the desired distribution. To comprehensively evaluate the models, we generate 40 candidates for each receptor and employ the following metrics: (1) **Diversity**. Inspired by [72], we measure the diversity via unique clusters of sequences and structures. Specifically, we hierarchically cluster the structures based on pair-wise root mean square deviation (RMSD) of \(\mathsf{C}_{\alpha}\). The diversity of structures \(\text{Div}_{struct}\) is defined as the number of clusters versus the number of candidates. A similar procedure can be applied to the sequences to obtain \(\text{Div}_{seq}\), utilizing the similarity [36] derived from alignment [24]. Then the co-design diversity is \(\sqrt{\text{Div}_{seq}\text{Div}_{struct}}\). (2) **Consistency**. We measure how well the models learn the 1D&3D joint distribution by the sequence-structure consistency, quantified via Cramer's V [12] association between the clustering labels (as in Diversity) of the sequences and the structures. High consistency indicates that candidates with similar sequences also have similar structures, implying that the generative model effectively captures the dependency between 1D and 3D. (3) \(\mathbf{\Delta G}\). Aligned with the literature [34; 44], we employ the binding energy (kcal/mol) provided by Rosetta [2], a widely-used suite for biomolecular modeling with physical energy functions, to evaluate the binding affinity of the generated candidates. Lower \(\Delta G\) indicates stronger binding between the peptide and the target. (4) **Success**. We report the proportion of successful designs (_i.e._\(\Delta G<0\), indicating no severe atomic clashes or twisted conformations) among all the candidates.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{4}{c}{PepBench} & \multicolumn{4}{c}{PepBDB} \\ \cline{2-9}  & Div.(\(\uparrow\)) & Con.(\(\uparrow\)) & \(\Delta G(\downarrow\)) & Success & Div.(\(\uparrow\)) & Con.(\(\uparrow\)) & \(\Delta G(\downarrow\)) & Success \\ \hline Test Set & - & - & -35.25 & 95.70\% & - & - & -35.96 & 95.79\% \\ \hline HSRN\({}^{3}\) & 0.158 & 0.0 & \(\geq 0\) & 10.46\% & 0.111 & 0.0 & \(\geq 0\) & 10.86\% \\ dyMEAN & 0.150 & 0.0 & -2.26 & 14.60\% & 0.150 & 0.0 & -1.92 & 6.26\% \\ DiffAb & 0.427 & 0.670 & -21.20 & 49.87\% & 0.269 & 0.463 & -18.40 & 41.45\% \\ PepGLAD (ours) & **0.506** & **0.789** & **-21.94** & **55.97\%** & **0.692** & **0.923** & **-21.53** & **48.47\%** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Evaluation on sequence-structure co-design. On each target, 40 candidates are generated for evaluation. Div. and Con. are abbreviations for diversity and consistency, respectively.

For all metrics except \(\Delta G\), we first compute values for each receptor individually and then average the results across different receptors. For \(\Delta G\), we identify the best candidate on each receptor as the outputs and report the median value across different receptors. Details about the metrics are provided in Appendix H, including discussion on AAR (Appendix H.1) and consistency (Appendix H.2).

**Results** Table 1 illustrates that our PepGLAD generates significantly more diversified and consistent peptides with better binding energy and success rates compared to the baselines. When benchmarking HSRN, dyMEAN, and DiffAb, which perform well on antibody CDR design, we observe a notable performance gap between non-diffusion baselines (_i.e._ HSRN, dyMEAN) and the diffusion-based baseline (_i.e._ DiffAb), suggesting the higher complexity in peptide design and the need for stronger modeling capabilities. Compared to DiffAb, which operates on categorical residue types, \(\mathsf{C}_{\alpha}\) coordinates and orientations, our PepGLAD (1) better captures the dependency between sequence and structure, as indicated by higher diversity and consistency, since diffusion is implemented on the latent space where the representation of sequence and structure are nicely correlated by the autoencoder; (2) more effectively captures the intricate protein-peptide interactions, demonstrated by better \(\Delta G\) and success rates, since we leverage the full-atom context of the binding site and enhances generalization capability by converting the geometry into a standard space. We showcase two candidates designed by our PepGLAD with favorable binding energy given by Rosetta in Figure 3. Furthermore, the diversity within successful designs is 0.632, which is higher than that of all designs (0.506), indicating the high structural flexibility of peptides upon successful binding.

We also evaluate our PepGLAD against two sophisticated pipeline systems in Table 2. The traditional method (_i.e._ AnchorExtension) is limited by low efficiency, thus we can only afford outputting 10 candidates for each receptor. For a relatively fair comparison with RFDiffusion, we refine the structure of the generated candidates using the empirical force field in RFDiffusion. However, the comparison may still disadvantage our PepGLAD, given that RFDiffusion is finetuned from a model pretrained on a large-scale dataset [64]. Nevertheless, as demonstrated in Table 2, our model still exhibits marvelous superiority on diversity, consistency, and success rate, while achieving competitive binding energy \(\Delta G\), with obviously higher efficiency.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Model & Div.(\(\uparrow\)) & Con.(\(\uparrow\)) & \(\Delta G(\downarrow)\) & Success & Time \\ \hline AnchorExtension & 0.245 & 0.423 & -26.80 & 84.30\% & 735s \\ RFDiffusion & 0.259 & 0.696 & **-33.82** & 79.68\% & 61s \\ PepGLAD (ours) & **0.506** & **0.789** & -29.36 & **92.82\%** & **3s** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Evaluation on sequence-structure co-design with two well-established systems. Time cost is measured as the total time spent divided by the number of designed candiates.

Figure 3: **Top**: A generated candidate confined within the binding site (PDB=4cu4, \(\Delta G\)=-34.21). **Bottom**: A generated candidate with complementary shape to the binding site (PDB=3pkn, \(\Delta G\)=-33.32). Both candidates form compact interactions at the interface.

### Binding Conformation Generation

**Metrics** For each receptor, we generate 10 candidates and report the median value of the following metrics across different receptors to measure how well the generated distribution can recover the reference conformation: (1) \(\mathbf{RMSD}_{\mathbb{C}_{\alpha}}\) : Root mean square deviation on the coordinates of \(\mathbb{C}_{\alpha}\) between a candidate and a reference structure with the unit A. (2) \(\mathbf{RMSD}_{\text{atom}}\): RMSD on all atoms to measure the quality of the full-atom geometry. (3) \(\mathbf{DockQ}\)[4]. A comprehensive metric evaluating the full-atom similarity on the interface between a candidate and a reference complex. It ranges from 0 to 1, with values above 0.23 and 0.49 considered as acceptable and medium quality, respectively.

**Results** As shown in Table 3, our PepGLAD surpasses all the baselines in terms of both \(\text{RMSD}_{\mathbb{C}_{\alpha}}\) and \(\text{DockQ}\) by a large margin, highlighting the superiority of incorporating the full-atom context and the binding-site shape into the latent diffusion process. Additionally, we present the distribution of the best \(\text{RMSD}_{\mathbb{C}_{\alpha}}\) on different test receptors using box plots and showcase a generated conformation highly resembling the reference in Figure 4. The distribution reveals that our model achieves favorable performance on \(\text{RMSD}_{\mathbb{C}_{\alpha}}\) with lower variance on the test set compared to other baselines, exhibiting robust generalization ability across disparate binding sites.

## 5 Analysis

We conduct the following ablations: the full-atom geometry (**Full-Atom**); the affine transformation (**Affine**); the unsupervised data from protein fragments (**ProtFrag**) and the mask policy (**Mask**) when training the autoencoder. Note that generative performance is assessed from various aspects, and improvement in one aspect at the disproportionate expense of others might be meaningless. Thus, we additionally compute the average of all the metrics to evaluate the comprehensive effect of each module, where \(\Delta G\) is normalized by the statistics on the test set. Table 4 demonstrates the following observations: (1) Discarding the full-atom context results in a significant degradation on all metrics, especially the success rate, implying the necessity of the full-atom context in capturing the intricate protein-peptide interactions; (2) Implementing the diffusion directly on the data space without the proposed affine transformation incurs a notably adverse impact on all metrics, indicating the remarkable enhancement on the generalization capability made by the affine transformation; (3) Training without the unsupervised data leads to a less informative latent space, exerting a negative effect on the binding energy and success rate; (4) Removal of the mask policy reduces the correlation between sequence and structure in the latent space, thus harms the consistency.

Figure 4: The distribution of \(\text{RMSD}_{\mathbb{C}_{\alpha}}\) on the test set of PepBench and a visualized sample.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{3}{c}{PepBench} & \multicolumn{3}{c}{PepBDB} \\ \cline{2-7}  & \(\text{RMSD}_{\mathbb{C}_{\alpha}}(\downarrow)\) & \(\text{RMSD}_{\text{atom}}(\downarrow)\) & \(\text{DockQ}(\uparrow)\) & \(\text{RMSD}_{\text{atom}}(\downarrow)\) & \(\text{DockQ}(\uparrow)\) \\ \hline FlexPepDock & 6.43 & 7.52 & 0.393 & - & - & - \\ AlphaFold 2 & 8.49 & 9.20 & 0.355 & - & - & - \\ dyMEAN & 7.96 & 8.35 & 0.374 & 17.64 & 17.56 & 0.142 \\ HSRN & 6.02 & 7.59 & 0.508 & 9.28 & 9.72 & 0.394 \\ DiffAb & 4.23 & 7.60 & 0.586 & 13.96 & 13.12 & 0.236 \\ \hline PepGLAD (ours) & **4.09** & **5.30** & **0.592** & **8.87** & **8.62** & **0.403** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Evaluation on binding conformation generation. On each target, 10 candidates are generated to calculate the optimal recall of the reference conformation.

## 6 Limitations

Despite the promising results, we acknowledge several limitations which might be addressed by future work. First, the binding affinity assessment relies on the Rosetta scoring function as a proxy for wetlab experiments. There may be discrepancies between the predicted and actual binding energies. The ultimate test of a peptide utility is its performance _in vivo_, which is too costly for large-scale evaluation. Nevertheless, this is a problem confronting the entire community, and we hope future research might propose more reliable _in silico_ proxies to bridge the gap. Second, while this paper addresses peptide design from the aspect of proteins, it might also be reasonable to think from the aspect of small molecules if the peptides are short enough. Under such circumstances, it is also beneficial to further explore counterparts of methods for small molecule design [43; 52; 39], which we leave for future work.

## 7 Conclusion

In this paper, we first assemble a dataset from Protein Data Bank (PDB) and literature to benchmark generative models on target-specific peptide design in terms of diversity, consistency, and binding energy. Subsequently, we propose PepGLAD, a powerful diffusion-based model for full-atom peptide design. In particular, we explore diffusion on the latent space where the sequence and the full-atom structure are jointly encoded by a variational autoencoder. We further propose a receptor-specific affine transformation technique to project variable geometries in the data space into a standard space, which enhances the transferability of diffusion processes on disparate binding sites. Our PepGLAD outperforms the existing models on sequence-structure co-design and binding conformation generation, exhibiting high generalization across diverse binding sites. Our work represents a pioneering effort in the exploration of deep generative models for simultaneous design of 1D sequences and 3D structures of peptides, which could inspire future research in this field.

## Software and Data

The curated PepBench and ProtFrag are available at [https://zenodo.org/records/13373108](https://zenodo.org/records/13373108). The codes for our PepGLAD are open-sourced at [https://github.com/THUNLP-MT/PepGLAD](https://github.com/THUNLP-MT/PepGLAD).

## Impact Statements

This paper aims to advance the field of peptide design through the construction of a benchmark and the development of a novel latent diffusion model, PepGLAD, which addresses key limitations in current methods. Our work represents a step forward in computational peptide design, with the potential to impact both scientific research and practical applications in various domains. For instance, more precise peptide design could lead to enhanced drugs in the pharmaceutical industry, and could facilitate the creation of new biomaterials, sensors, and other innovative technologies in biology and materials science. We wish our paper could inspire future research in this field.

## Acknowledgments

This work is jointly supported by the National Key R&D Program of China (No.2022ZD0160502), the National Natural Science Foundation of China (No. 61925601, No. 62376276, No. 62276152), and Beijing Nova Program (20230484278).

\begin{table}
\begin{tabular}{c c c c c|c} \hline \hline Ablations & Div.(\(\uparrow\)) & Con.(\(\uparrow\)) & \(\Delta G(\downarrow\)) & Success & Avg. \\ \hline PepGLAD & 0.506 & 0.789 & -21.94 & 55.97\% & **0.619** \\ \hline w/o Full-Atom & 0.441 & 0.751 & -20.87 & 51.18\% & 0.574 \\ w/o Affine & 0.450 & 0.740 & -19.08 & 52.39\% & 0.564 \\ w/o ProtFrag & 0.535 & 0.760 & -20.16 & 52.15\% & 0.597 \\ w/o Mask & 0.422 & 0.741 & -20.45 & 57.44\% & 0.579 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablations on different components. Avg. computes the average of all metrics, where \(\Delta G\) is first normalized by the median value of the references on test set.

## References

* [1] J. Adolf-Bryfogle, O. Kalyuzhniy, M. Kubitz, B. D. Weitzner, X. Hu, Y. Adachi, W. R. Schief, and R. L. Dunbrack Jr. Rosettaanibodydesign (rabd): A general framework for computational antibody design. _PLoS computational biology_, 14(4):e1006112, 2018.
* [2] R. F. Alford, A. Leaver-Fay, J. R. Jeliazkov, M. J. O'Meara, F. P. DiMaio, H. Park, M. V. Shapovalov, P. D. Renfrew, V. K. Mulligan, K. Kappel, et al. The rosetta all-atom energy function for macromolecular modeling and design. _Journal of chemical theory and computation_, 13(6):3031-3048, 2017.
* [3] N. Anand and T. Achim. Protein structure and sequence generation with equivariant denoising diffusion probabilistic models. _arXiv preprint arXiv:2205.15019_, 2022.
* [4] S. Basu and B. Wallner. Dockq: a quality measure for protein-protein docking models. _PloS one_, 11(8):e0161879, 2016.
* [5] H. M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T. N. Bhat, H. Weissig, I. N. Shindyalov, and P. E. Bourne. The protein data bank. _Nucleic acids research_, 28(1):235-242, 2000.
* [6] G. Bhardwaj, V. K. Mulligan, C. D. Bahl, J. M. Gilmore, P. J. Harvey, O. Cheneval, G. W. Buchko, S. V. Pulavarti, Q. Kaas, A. Eletsky, et al. Accurate de novo design of hyperstable constrained peptides. _Nature_, 538(7625):329-335, 2016.
* [7] J. Bose, T. Akhound-Sadegh, K. FATRAS, G. Huguet, J. Rector-Brooks, C.-H. Liu, A. C. Nica, M. Korablyov, M. M. Bronstein, and A. Tong. Se (3)-stochastic flow matching for protein backbone generation. In _The Twelfth International Conference on Learning Representations_, 2023.
* [8] P. Bryant and A. Elofsson. Evobind: in silico directed evolution of peptide binders with alphafold. _bioRxiv_, pages 2022-07, 2022.
* [9] L. Cao, B. Coventry, I. Goreshnik, B. Huang, W. Sheffler, J. S. Park, K. M. Jude, I. Markovic, R. U. Kadam, K. H. Verschueren, et al. Design of protein-binding proteins from the target structure alone. _Nature_, 605(7910):551-560, 2022.
* [10] C. Chothia and J. Janin. Principles of protein-protein recognition. _Nature_, 256(5520):705-708, 1975.
* [11] P. J. Cock, T. Antao, J. T. Chang, B. A. Chapman, C. J. Cox, A. Dalke, I. Friedberg, T. Hamelryck, F. Kauff, B. Wilczynski, et al. Biopython: freely available python tools for computational molecular biology and bioinformatics. _Bioinformatics_, 25(11):1422, 2009.
* [12] H. Cramer. _Mathematical methods of statistics_, volume 26. Princeton university press, 1999.
* [13] P. Das, T. Sercu, K. Wadhawan, I. Padhi, S. Gehrmann, F. Cipcigan, V. Chenthamarakshan, H. Strobelt, C. Dos Santos, P.-Y. Chen, et al. Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations. _Nature Biomedical Engineering_, 5(6):613-623, 2021.
* [14] J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. Wicky, A. Courbet, R. J. de Haas, N. Bethel, et al. Robust deep learning-based protein sequence design using proteinmpnn. _Science_, 378(6615):49-56, 2022.
* [15] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.
* [16] A. Duval, S. V. Mathis, C. K. Joshi, V. Schmidt, S. Miret, F. D. Malliaros, T. Cohen, P. Lio, Y. Bengio, and M. Bronstein. A hitchhiker's guide to geometric gnns for 3d atomic systems. _arXiv preprint arXiv:2312.07511_, 2023.
* [17] K. Fosgerau and T. Hoffmann. Peptide therapeutics: current status and future directions. _Drug discovery today_, 20(1):122-128, 2015.

* [18] C. Fu, K. Yan, L. Wang, W. Y. Au, M. C. McThrow, T. Komikado, K. Maruhashi, K. Uchino, X. Qian, and S. Ji. A latent diffusion model for protein structure generation. In _Learning on Graphs Conference_, pages 29-1. PMLR, 2024.
* [19] G. H. Golub and C. F. Van Loan. _Matrix computations_. JHU press, 2013.
* [20] C. Grathwohl and K. Wuthrich. The x-pro peptide bond as an nmr probe for conformational studies of flexible linear peptides. _Biopolymers: Original Research on Biomolecules_, 15(10): 2025-2041, 1976.
* [21] J. Guan, W. W. Qian, X. Peng, Y. Su, J. Peng, and J. Ma. 3d equivariant diffusion for target-aware molecule generation and affinity prediction. In _The Eleventh International Conference on Learning Representations_, 2022.
* [22] K. Guruprasad, B. B. Reddy, and M. W. Pandit. Correlation between stability of a protein and its dipeptide composition: a novel approach for predicting in vivo stability of a protein from its primary sequence. _Protein Engineering, Design and Selection_, 4(2):155-161, 1990.
* [23] J. Han, Y. Rong, T. Xu, and W. Huang. Geometrically equivariant graph neural networks: A survey. _arXiv preprint arXiv:2202.07230_, 2022.
* [24] S. Henikoff and J. G. Henikoff. Amino acid substitution matrices from protein blocks. _Proceedings of the National Academy of Sciences_, 89(22):10915-10919, 1992.
* [25] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* [26] P. Hosseinzadeh, P. R. Watson, T. W. Craven, X. Li, S. Rettie, F. Pardo-Avila, A. K. Bera, V. K. Mulligan, P. Lu, A. S. Ford, et al. Anchor extension: a structure-guided approach to design cyclic peptides targeting enzyme active sites. _Nature Communications_, 12(1):3384, 2021.
* [27] J. Ingraham, V. Garg, R. Barzilay, and T. Jaakkola. Generative models for graph-based protein design. _Advances in neural information processing systems_, 32, 2019.
* [28] J. B. Ingraham, M. Baranov, Z. Costello, K. W. Barber, W. Wang, A. Ismail, V. Frappier, D. M. Lord, C. Ng-Thow-Hing, E. R. Van Vlack, et al. Illuminating protein space with a programmable generative model. _Nature_, pages 1-9, 2023.
* [29] W. Jin, J. Wohlwend, R. Barzilay, and T. Jaakkola. Iterative refinement graph neural network for antibody sequence-structure co-design. _arXiv preprint arXiv:2110.04624_, 2021.
* [30] W. Jin, R. Barzilay, and T. Jaakkola. Antibody-antigen docking and design via hierarchical equivariant refinement. _arXiv preprint arXiv:2207.06616_, 2022.
* [31] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Zidek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* [32] D. P. Kingma and M. Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [33] X. Kong, W. Huang, and Y. Liu. Conditional antibody design as 3d equivariant graph translation. _arXiv preprint arXiv:2208.06073_, 2022.
* [34] X. Kong, W. Huang, and Y. Liu. End-to-end full-atom antibody design. _arXiv preprint arXiv:2302.00203_, 2023.
* [35] A. C.-L. Lee, J. L. Harris, K. K. Khanna, and J.-H. Hong. A comprehensive review on current advances in peptide drug development and design. _International journal of molecular sciences_, 20(10):2383, 2019.
* [36] Y. Lei, S. Li, Z. Liu, F. Wan, T. Tian, S. Li, D. Zhao, and J. Zeng. A deep-learning framework for multi-level peptide-protein interaction prediction. _Nature communications_, 12(1):5465, 2021.

* [37] J. K. Leman, B. D. Weitzner, S. M. Lewis, J. Adolf-Bryfogle, N. Alam, R. F. Alford, M. Aprahamian, D. Baker, K. A. Barlow, P. Barth, et al. Macromolecular modeling and design in rosetta: recent methods and frameworks. _Nature methods_, 17(7):665-680, 2020.
* [38] J. Li, C. Cheng, Z. Wu, R. Guo, S. Luo, Z. Ren, J. Peng, and J. Ma. Full-atom peptide design based on multi-modal flow matching. In _Forty-first International Conference on Machine Learning_, 2024.
* [39] H. Lin, Y. Huang, O. Zhang, Y. Liu, L. Wu, S. Li, Z. Chen, and S. Z. Li. Functional-group-based diffusion for pocket-specific molecule generation and elaboration. _Advances in Neural Information Processing Systems_, 36, 2024.
* [40] H. Lin, O. Zhang, H. Zhao, D. Jiang, L. Wu, Z. Liu, Y. Huang, and S. Z. Li. Ppflow: Target-aware peptide design with torsional flow matching. In _Forty-first International Conference on Machine Learning_, 2024.
* [41] N. London, D. Movshovitz-Attias, and O. Schueler-Furman. The structural basis of peptide-protein binding strategies. _Structure_, 18(2):188-199, 2010.
* [42] N. London, B. Raveh, E. Cohen, G. Fathi, and O. Schueler-Furman. Rosetta flexpepdock web server--high resolution modeling of peptide-protein interactions. _Nucleic acids research_, 39 (suppl_2):W249-W253, 2011.
* [43] S. Luo, J. Guan, J. Ma, and J. Peng. A 3d generative model for structure-based drug design. _Advances in Neural Information Processing Systems_, 34:6229-6239, 2021.
* [44] S. Luo, Y. Su, X. Peng, S. Wang, J. Peng, and J. Ma. Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures. _Advances in Neural Information Processing Systems_, 35:9754-9767, 2022.
* [45] K. Martinkus, J. Ludwiczak, K. Cho, W.-C. Lian, J. Lafrance-Vanasse, I. Hotzel, A. Rajpal, Y. Wu, R. Bonneau, V. Gligorijevic, et al. Abdiffuser: Full-atom generation of in-vitro functioning antibodies. _arXiv preprint arXiv:2308.05027_, 2023.
* [46] P. M. Martins, L. H. Santos, D. Mariano, F. C. Queiroz, L. L. Bastos, I. d. S. Gomes, P. H. Fischer, R. E. Rocha, S. A. Silveira, L. H. de Lima, et al. Propedia: a database for protein-peptide identification based on a hybrid clustering algorithm. _BMC bioinformatics_, 22:1-20, 2021.
* [47] S. Mitternacht. Freesasa: An open source c library for solvent accessible surface area calculations. _F1000Research_, 5, 2016.
* [48] A. T. Muller, J. A. Hiss, and G. Schneider. Recurrent neural network model for constructive peptide design. _Journal of chemical information and modeling_, 58(2):472-479, 2018.
* [49] M. Muttenthaler, G. F. King, D. J. Adams, and P. F. Alewood. Trends in peptide drug discovery. _Nature reviews Drug discovery_, 20(4):309-325, 2021.
* [50] S. B. Needleman and C. D. Wunsch. A general method applicable to the search for similarities in the amino acid sequence of two proteins. _Journal of molecular biology_, 48(3):443-453, 1970.
* [51] A. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pages 8162-8171. PMLR, 2021.
* [52] X. Peng, S. Luo, J. Guan, Q. Xie, J. Peng, and J. Ma. Pocket2mol: Efficient molecular sampling based on 3d protein pockets. In _International Conference on Machine Learning_, pages 17644-17655. PMLR, 2022.
* [53] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* [54] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pages 2256-2265. PMLR, 2015.

* [55] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.
* [56] M. Steinegger and J. Soding. Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. _Nature biotechnology_, 35(11):1026-1028, 2017.
* [57] S. Swanson, V. Sivaraman, G. Grigoryan, and A. E. Keating. Tertiary motifs as building blocks for the design of protein-binding peptides. _Protein Science_, 31(6):e4322, 2022.
* [58] B. L. Trippe, J. Yim, D. Tischer, T. Broderick, D. Baker, R. Barzilay, and T. Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem. _arXiv preprint arXiv:2206.04119_, 2022.
* [59] T. Tsaban, J. K. Varga, O. Avraham, Z. Ben-Aharon, A. Khramushin, and O. Schueler-Furman. Harnessing protein folding neural networks for peptide-protein docking. _Nature communications_, 13(1):176, 2022.
* [60] P. Vanhee, A. M. van der Sloot, E. Verschueren, L. Serrano, F. Rousseau, and J. Schymkowitz. Computational design of peptide ligands. _Trends in biotechnology_, 29(5):231-239, 2011.
* [61] Y. Verma, M. Heinonen, and V. Garg. Abode: Ab initio antibody design using conjoined odes. _arXiv preprint arXiv:2306.01005_, 2023.
* [62] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, P.-A. Manzagol, and L. Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. _Journal of machine learning research_, 11(12), 2010.
* [63] D. Wang, Z. Wen, F. Ye, H. Zhou, and L. Li. Accelerating antimicrobial peptide discovery with latent sequence-structure model. _arXiv preprint arXiv:2212.09450_, 2022.
* [64] J. L. Watson, D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst, R. J. Ragotte, L. F. Milles, et al. De novo design of protein structure and function with rfidiffusion. _Nature_, 620(7976):1089-1100, 2023.
* [65] Z. Wen, J. He, H. Tao, and S.-Y. Huang. Pepbdb: a comprehensive structural database of biological peptide-protein interactions. _Bioinformatics_, 35(1):175-177, 2019.
* [66] K. E. Wu, K. K. Yang, R. van den Berg, S. Alamdari, J. Y. Zou, A. X. Lu, and A. P. Amini. Protein structure generation via folding diffusion. _Nature communications_, 15(1):1059, 2024.
* [67] H. Xia, J. McMichael, M. Becker-Hapak, O. C. Onyeador, R. Buchli, E. McClain, P. Pence, S. Supabphol, M. M. Richters, A. Basu, et al. Computational prediction of mhc anchor locations guides neoantigen identification and prioritization. _Science immunology_, 8(82):eabg2200, 2023.
* [68] X. Xie, P. A. Valiente, and P. M. Kim. Helixgan a deep-learning methodology for conditional de novo design of \(\alpha\)-helix structures. _Bioinformatics_, 39(1):bta036, 2023.
* [69] X. Xie, P. A. Valiente, J. Kim, and P. M. Kim. Helixdiff, a score-based diffusion model for generating all-atom \(\alpha\)-helical structures. _ACS Central Science_, 10(5):1001-1011, 2024.
* [70] M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang. Geodiff: A geometric diffusion model for molecular conformation generation. _arXiv preprint arXiv:2203.02923_, 2022.
* [71] M. Xu, A. S. Powers, R. O. Dror, S. Ermon, and J. Leskovec. Geometric latent diffusion models for 3d molecule generation. In _International Conference on Machine Learning_, pages 38592-38610. PMLR, 2023.
* [72] J. Yim, B. L. Trippe, V. De Bortoli, E. Mathieu, A. Doucet, R. Barzilay, and T. Jaakkola. Se (3) diffusion model with application to protein backbone generation. _arXiv preprint arXiv:2302.02277_, 2023.
* [73] Y. Zhang, Z. Zhang, B. Zhong, S. Misra, and J. Tang. Diffpack: A torsional diffusion model for autoregressive protein side-chain packing. _arXiv preprint arXiv:2306.01794_, 2023.

Reconstruction of Full-Atom Geometry

### Auxiliary Loss for Training the AutoEncoder

To better recover the all-atom geometry, we employ an auxilary structural loss similar to the violation loss in Jumper et al. [31] including the supervision on \(\mathcal{C}_{\alpha}\) coordinates, bond lengths, and side-chain dihedral angles. First, since the \(\mathcal{C}_{\alpha}\) is critical in deciding the global geometry of the peptide, we exert additional loss on its coordinates to distinguish it from other atoms:

\[\mathcal{L}_{\mathcal{CA}}(i)=\mathrm{MSE}(\vec{\mathbf{r}}_{i},\vec{\mathbf{r}}_{i}^{ \prime}), \tag{16}\]

where \(\vec{\mathbf{r}}_{i}^{\prime}\) and \(\vec{\mathbf{r}}_{i}\) are the reconstructed and the ground truth coordinates of \(\mathcal{C}_{\alpha}\) in node \(i\). Next, we implement L1 loss on the bond lengths:

\[\mathcal{L}_{bond}(i)=\sum\nolimits_{b\in\mathcal{B}(i)}|b-b^{\prime}|/| \mathcal{B}(i)|, \tag{17}\]

where \(\mathcal{B}(i)\) includes all chemical bonds in node \(i\), and \(b^{\prime}\) denotes the reconstructed bond length. For simplicity, he bonds between residues are included into the bonds of the former residue. Finally, we supervise on the \(\chi_{1}\) to \(\chi_{4}\) side-chain dihedral angles [73]:

\[\mathcal{L}_{angle}(i)=\sum\nolimits_{\chi\in\mathcal{A}(i)}|\chi-\chi^{ \prime}|/|\mathcal{A}(i)|, \tag{18}\]

where \(\mathcal{A}(i)\) includes all side-chain dihedral angles in node \(i\), \(\chi^{\prime}\) and \(\chi\) denotes the reconstructed and the ground truth angles, respectively. The overall auxilary loss for node \(i\) is then given by:

\[\mathcal{L}_{\textit{aux}}(i)=\lambda_{\mathcal{CA}}\mathcal{L}_{\mathcal{CA}} (i)+\lambda_{bond}\mathcal{L}_{bond}(i)+\lambda_{angle}\mathcal{L}_{angle}(i), \tag{19}\]

where we set \(\lambda_{\mathcal{CA}}=1.0\), \(\lambda_{bond}=1.0\), \(\lambda_{angle}=0.5\) in our experiments. We find that it is necessary to set \(\lambda_{angle}\) with a relatively small value to make the training process stable.

### Idealization of Local Geometry

Preserving atom instances enhances the modeling of side chain interactions but introduces challenges due to potential twisting in local geometry. While in sequence-structure co-design, samples undergo fast relax by physical force field to ensure a valid local geometry, in binding conformation generation, we use an alignment technique to place the idealized side chains on the generated atom instances. Specifically, the idealized side chain can be represented as at most 4 dihedral angles (\(\chi\)-angles), treating fragments like phenyl group as rigid bodies. Suppose there are \(n_{i}\)\(\chi\)-angles and \(c_{i}\) atoms in node \(i\), we can define a function to map from the \(\chi\)-angles with the backbone coordinates \(\vec{\mathbf{B}}_{i}\in\mathbb{R}^{4\times 3}\) to atom instances as \(M_{i}:\mathbb{R}^{n_{i}}\times\mathbb{R}^{4\times 3}\rightarrow\mathbb{R}^{c_{i} \times 3}\), which is luckily differentiable [28]. Thus we can optimize \(\chi\)-angles via gradient descent to minimize the MSE between the coordinates constructed from \(\chi\)-angles and those generated by the model:

\[\mathbf{\chi}_{i}^{*}=\arg\min\nolimits_{\mathbf{\chi}\in[0,2\pi]^{n_{i}}}\|M_{i}(\mathbf{ \chi},\vec{\mathbf{B}}_{i})-\vec{\mathbf{X}}_{i}\|^{2}, \tag{20}\]

where \(\vec{\mathbf{X}}_{i}\in\mathbb{R}^{c_{i}\times 3}\) denotes the coordinates of \(c_{i}\) atom instances in node \(i\) generated by the model. Now it should be easy to construct an idealized side chain maintaining fidelity to the generated atom instances by \(M(\mathbf{\chi}_{i}^{*},\vec{\mathbf{B}}_{i})\). In the experiments of conformation generation on PepBench, such idealization gives slightly better DockQ and RMSD\({}_{\text{atom}}\) than Rosetta side-chain packing algorithm but with higher efficiency.

## Appendix B Proof of Proposition 3.1

**Proposition 3.1**.: _Denote the invariant and equivariant outputs from a scalarization-based E(3)-equivariant GNN as \(f(\{\mathbf{h}_{i},\vec{\mathbf{x}}_{i}\})\) and \(\vec{f}(\{\mathbf{h}_{i},\vec{\mathbf{x}}_{i}\})\), respectively. With the definition of \(F\) in Eq. 7, \(\forall g\in E(3)\), we have \(f(\{\mathbf{h}_{i},F(\vec{\mathbf{x}}_{i})\})=f(\{\mathbf{h}_{i},F_{g}(g\cdot\vec{\mathbf{x} }_{i})\})\) and \(g\cdot F^{-1}(\vec{f}(\{\mathbf{h}_{i},F(\vec{\mathbf{x}}_{i})\}))=F_{g}^{-1}(\vec{f}( \{\mathbf{h}_{i},F_{g}(g\cdot\vec{\mathbf{x}}_{i})\}))\), where \(F_{g}\) is derived on the coordinates transformed by \(g\). Namely, the E(3)-equivariance is preserved if we implement the GNN on the standard space and recover the original geometry from the outputs._For simplicity, given \(F\) derived from a set of coordinates \(\{\vec{\mathbf{x}}_{i}\}\) following Eq. 7, we use \(F_{g}\) to indicate the affine transformation derived from \(\{g\cdot\vec{\mathbf{x}}_{i}\}\), where \(g\in E(3)\). Additionally, we keep the terminology "standard space" for describing the space after the data-specific affine transformation \(F\). We begin by proving a key lemma, that is, the E(3)-invariance of distances between two nodes in the standard space converted by \(F\):

**Lemma C.1**.: _Given two nodes \(i\) and \(j\) in the geometric graph \(\mathcal{G}\), denoting their coordinates as \(\vec{\mathbf{x}}_{i}\) and \(\vec{\mathbf{x}}_{j}\), their distance in the standard space is E(3)-invariant. Namely, \(\forall g\in E(3),\|F(\vec{\mathbf{x}}_{i})-F(\vec{\mathbf{x}}_{j})\|=\|F_{g}(g\cdot \vec{\mathbf{x}}_{i})-F_{g}(g\cdot\vec{\mathbf{x}}_{j})\|\)._

Proof.: \(\forall g\in E(3)\), \(g\) can be instantiated as an orthogonal matrix \(\mathbf{Q}\in O(3)\) (including rotation and reflection), and a translation vector \(\vec{\mathbf{t}}\in\mathbb{R}^{3}\). Denoting all coordinates in the geometric graph as \(\vec{\mathbf{X}}\in\mathbb{R}^{3\times|\mathcal{G}|}\), and the number of nodes in \(\mathcal{G}\) as \(n\), we can derive the E(3)-equivariance of the expectation of the coordinates:

\[\mathbb{E}[g\cdot\vec{\mathbf{X}}] =\frac{1}{n}\sum_{i}g\cdot\vec{\mathbf{x}}_{i}=\frac{1}{n}\sum_{i}( \mathbf{Q}\vec{\mathbf{x}}_{i}+\vec{\mathbf{t}}) \tag{21}\] \[=\frac{1}{n}\mathbf{Q}(\sum_{i}\vec{\mathbf{x}}_{i})+\vec{\mathbf{t}}=\mathbf{Q} \mathbb{E}[\vec{\mathbf{X}}]+\vec{\mathbf{t}}\] (22) \[=g\cdot\mathbb{E}[\vec{\mathbf{X}}]. \tag{23}\]

With the E(3)-equivariance of the expectation, it is easy to derive the following equation on the covariance matrix:

\[\mathrm{Cov}(g\cdot\vec{\mathbf{X}},g\cdot\vec{\mathbf{X}}) =\frac{1}{n-1}(g\cdot\vec{\mathbf{X}}-\mathbb{E}[g\cdot\vec{\mathbf{X}}]) (g\cdot\vec{\mathbf{X}}-\mathbb{E}[g\cdot\vec{\mathbf{X}}])^{\top} \tag{24}\] \[=\frac{1}{n-1}(g\cdot\vec{\mathbf{X}}-g\cdot\mathbb{E}[\vec{\mathbf{X}}]) (g\cdot\vec{\mathbf{X}}-g\cdot\mathbb{E}[\vec{\mathbf{X}}])^{\top}\] (25) \[=\frac{1}{n-1}(\mathbf{Q}(\vec{\mathbf{X}}-\mathbb{E}[\vec{\mathbf{X}}]))(\bm {Q}(\vec{\mathbf{X}}-\mathbb{E}[\vec{\mathbf{X}}]))^{\top}\] (26) \[=\frac{1}{n-1}\mathbf{Q}(\vec{\mathbf{X}}-\mathbb{E}[\vec{\mathbf{X}}])(\vec{ \mathbf{X}}-\mathbb{E}[\vec{\mathbf{X}}])^{\top}\mathbf{Q}^{\top}\] (27) \[=\mathbf{Q}\mathrm{Cov}(\vec{\mathbf{X}},\vec{\mathbf{X}})\mathbf{Q}^{\top}. \tag{28}\]

Based on the Cholesky decomposition used in the derivation of \(F\), we denote \(\mathrm{Cov}(\vec{\mathbf{X}},\vec{\mathbf{X}})=\vec{\mathbf{L}}\vec{\mathbf{L}}^{\top}\) and \(\mathrm{Cov}(g\cdot\vec{\mathbf{X}},g\cdot\vec{\mathbf{X}})=\vec{\mathbf{L}}_{g}\vec{\mathbf{ L}}_{g}^{\top}\), with which we can immediately derive:

\[\vec{\mathbf{L}}_{g}\vec{\mathbf{L}}_{g}^{\top}=\mathbf{Q}\vec{\mathbf{L}}\vec{ \mathbf{L}}^{\top}\mathbf{Q}^{\top}. \tag{29}\]

Considering \(\mathbf{Q}^{-1}=\mathbf{Q}^{\top}\), we further have the following equation:

\[\vec{\mathbf{L}}_{g}^{\top}\vec{\mathbf{L}}_{g}^{\top}=\mathbf{Q}\vec{\mathbf{L}} ^{-\top}\vec{\mathbf{L}}^{-1}\mathbf{Q}^{\top}. \tag{30}\]

Given that \(F(\vec{\mathbf{x}})=\vec{\mathbf{L}}^{-1}(\vec{\mathbf{x}}-\mathbb{E}[\vec{\mathbf{X}}])\), we are now ready to prove Lemma C.1 as follows:

\[\|F_{g}(g\cdot\vec{\mathbf{x}}_{i})-F_{g}(g\cdot\vec{\mathbf{x}}_{j})\| =\|\vec{\mathbf{L}}_{g}^{-1}(g\cdot\vec{\mathbf{x}}_{i}-\mathbb{E}[g\cdot \vec{\mathbf{X}}])-\vec{\mathbf{L}}_{g}^{-1}(g\cdot\vec{\mathbf{x}}_{j}-\mathbb{E}[g\cdot \vec{\mathbf{X}}])\| \tag{31}\] \[=\|\vec{\mathbf{L}}_{g}^{-1}(g\cdot\vec{\mathbf{x}}_{i}-g\cdot\vec{\mathbf{x }}_{j})\|=\|\vec{\mathbf{L}}_{g}^{-1}\mathbf{Q}(\vec{\mathbf{x}}_{i}-\vec{\mathbf{x}}_{j})\|\] (32) \[=\sqrt{(\vec{\mathbf{L}}_{g}^{-1}\mathbf{Q}(\vec{\mathbf{x}}_{i}-\vec{\mathbf{x }}_{j}))^{\top}(\vec{\mathbf{L}}_{g}^{-1}\mathbf{Q}(\vec{\mathbf{x}}_{i}-\vec{\mathbf{x}}_{j}))}\] (33) \[=\sqrt{(\vec{\mathbf{x}}_{i}-\vec{\mathbf{x}}_{j})^{\top}\mathbf{Q}^{\top} \vec{\mathbf{L}}_{g}^{-\top}\vec{\mathbf{L}}_{g}^{-1}\mathbf{Q}(\vec{\mathbf{x}}_{i}-\vec{\bm {x}}_{j})}\] (34) \[=\sqrt{(\vec{\mathbf{x}}_{i}-\vec{\mathbf{x}}_{j})^{\top}\vec{\mathbf{L}}^{- \top}\vec{\mathbf{L}}^{-1}(\vec{\mathbf{x}}_{i}-\vec{\mathbf{x}}_{j})}\] (35) \[=\sqrt{(\vec{\mathbf{L}}^{-1}(\vec{\mathbf{x}}_{i}-\vec{\mathbf{x}}_{j}))^{ \top}(\vec{\mathbf{L}}^{-1}(\vec{\mathbf{x}}_{i}-\vec{\mathbf{x}}_{j}))}=\|\vec{\mathbf{L}}^{ -1}(\vec{\mathbf{x}}_{i}-\vec{\mathbf{x}}_{j})\|\] (36) \[=\|\vec{\mathbf{L}}^{-1}(\vec{\mathbf{x}}_{i}-\mathbb{E}[\vec{\mathbf{X}}])- \vec{\mathbf{L}}^{-1}(\vec{\mathbf{x}}_{j}-\mathbb{E}[\vec{\mathbf{X}}])\|=\|F(\vec{\mathbf{x }}_{i})-F(\vec{\mathbf{x}}_{j})\| \tag{37}\]With Lemma C.1, we are able to give the proof of Proposition 3.1 as follows.

Proof.: A first observation is that to prove Proposition 3.1, we only need to prove the equivariance in the 1-layer case, since the multi-layer case can be decomposed into the 1-layer case by inserting \(I=F\circ F^{-1}\) between layers, where \(I\) is the identical mapping. Generally, each layer in scalarization-based E(3)-equivariant GNN has the following paradigm:

\[\mathbf{m}_{ij} =\phi_{m}(\mathbf{h}_{i},\mathbf{h}_{j},\|\mathbf{\vec{x}}_{i}-\mathbf{\vec{x}}_{j }\|^{2},\mathbf{e}_{ij}), \tag{38}\] \[\mathbf{\vec{x}}^{\prime}_{i} =\mathbf{\vec{x}}_{i}+\sum\nolimits_{j\in\mathcal{N}(i)}(\mathbf{\vec{x}} _{i}-\mathbf{\vec{x}}_{j})\phi_{x}(\mathbf{m}_{ij}),\] (39) \[\mathbf{\vec{h}}^{\prime}_{i} =\phi_{h}(\mathbf{h}_{i},\sum\nolimits_{j\in\mathcal{N}(i)}\mathbf{m}_{ij }), \tag{40}\]

where \(\mathcal{N}(i)\) denotes the neighborhood of node \(i\), and \(\phi_{x}\) outputs a scalar. Therefore, for the invariant part, we have:

\[f_{i}(\{\mathbf{h}_{i},F_{g}(g\cdot\mathbf{\vec{x}}_{i})\}) =\phi_{h}(\mathbf{h}_{i},\sum\nolimits_{j\in\mathcal{N}(i)}\mathbf{m}_{ij,F_{g}}) \tag{41}\] \[=\phi_{h}(\mathbf{h}_{i},\sum\nolimits_{j\in\mathcal{N}(i)}\phi_{m}( \mathbf{h}_{i},\mathbf{h}_{j},\|F_{g}(g\cdot\mathbf{\vec{x}}_{i})-F_{g}(g\cdot\mathbf{\vec{x}} _{j})\|^{2},\mathbf{e}_{ij}))\] (42) \[=\phi_{h}(\mathbf{h}_{i},\sum\nolimits_{j\in\mathcal{N}(i)}\phi_{m}( \mathbf{h}_{i},\mathbf{h}_{j},\|F(\mathbf{\vec{x}}_{i})-F(\mathbf{\vec{x}}_{j})\|,\mathbf{e}_{ij}))\] (43) \[=\phi_{h}(\mathbf{h}_{i},\sum\nolimits_{j\in\mathcal{N}(i)}\mathbf{m}_{ij,F})\] (44) \[=f_{i}(\{\mathbf{h}_{i},F(\mathbf{\vec{x}}_{i})\}) \tag{45}\]

For the equivariant features, recalling \(F^{-1}(\mathbf{\vec{x}})=\mathbf{\vec{L}}\mathbf{\vec{x}}+\mathbb{E}[\mathbf{\vec{X}}]\), we have:

\[F^{-1}_{g}\vec{f}_{i}(\{\mathbf{h}_{i},F_{g}(g\cdot\mathbf{\vec{x}}_{i})\}) \tag{46}\] \[=F^{-1}_{g}(F_{g}(g\cdot\mathbf{\vec{x}}_{i})+\sum\nolimits_{j\in \mathcal{N}(i)}(F_{g}(g\cdot\mathbf{\vec{x}}_{i})-F_{g}(g\cdot\mathbf{\vec{x}}_{j})) \phi_{x}(\mathbf{m}_{ij,F_{g}}))\] (47) \[=F^{-1}_{g}(F_{g}(g\cdot\mathbf{\vec{x}}_{i})+\sum\nolimits_{j\in \mathcal{N}(i)}\mathbf{\vec{L}}^{-1}_{g}\mathbf{Q}(\mathbf{\vec{x}}_{i}-\mathbf{\vec{x}}_{j}) \phi_{x}(\mathbf{m}_{ij,F}))\] (48) \[=F^{-1}_{g}(\mathbf{\vec{L}}^{-1}_{g}(\mathbf{Q}\mathbf{\vec{x}}_{i}+\mathbf{ \vec{t}}-\mathbb{E}[g\cdot\mathbf{\vec{X}}])+\sum\nolimits_{j\in\mathcal{N}(i)}\bm {\vec{L}}^{-1}_{g}\mathbf{Q}(\mathbf{\vec{x}}_{i}-\mathbf{\vec{x}}_{j})\phi_{x}(\mathbf{m}_{ij,F}))\] (49) \[=F^{-1}_{g}(\mathbf{\vec{L}}^{-1}_{g}(\mathbf{Q}\mathbf{\vec{x}}_{i}+\mathbf{ \vec{t}}-g\cdot\mathbb{E}[\mathbf{\vec{X}}])+\sum\nolimits_{j\in\mathcal{N}(i)}\bm {\vec{L}}^{-1}_{g}\mathbf{Q}(\mathbf{\vec{x}}_{i}-\mathbf{\vec{x}}_{j})\phi_{x}(\mathbf{m}_{ij,F}))\] (50) \[=F^{-1}_{g}(\mathbf{\vec{L}}^{-1}_{g}(\mathbf{Q}\mathbf{\vec{x}}_{i}-\mathbf{Q} \mathbb{E}[\mathbf{\vec{X}}])+\sum\nolimits_{j\in\mathcal{N}(i)}\mathbf{\vec{L}}^{-1}_ {g}\mathbf{Q}(\mathbf{\vec{x}}_{i}-\mathbf{\vec{x}}_{j})\phi_{x}(\mathbf{m}_{ij,F}))\] (51) \[=F^{-1}_{g}(\mathbf{\vec{L}}^{-1}_{g}\mathbf{Q}(\mathbf{\vec{x}}_{i}-\mathbb{ E}[\mathbf{\vec{X}}])+\sum\nolimits_{j\in\mathcal{N}(i)}\mathbf{\vec{L}}^{-1}_{g}\mathbf{Q}( \mathbf{\vec{x}}_{i}-\mathbf{\vec{x}}_{j})\phi_{x}(\mathbf{m}_{ij,F}))\] (52) \[=F^{-1}_{g}(\mathbf{\vec{L}}^{-1}_{g}\mathbf{Q}(\mathbf{\vec{x}}_{i}-\mathbb{ E}[\mathbf{\vec{X}}]+\sum\nolimits_{j\in\mathcal{N}(i)}(\mathbf{\vec{x}}_{i}-\mathbf{\vec{x}}_{j}) \phi_{x}(\mathbf{m}_{ij,F})))\] (53) \[=\mathbf{Q}(\mathbf{\vec{x}}_{i}-\mathbb{E}[\mathbf{\vec{X}}]+\sum\nolimits_{ j\in\mathcal{N}(i)}(\mathbf{\vec{x}}_{i}-\mathbf{\vec{x}}_{j})\phi_{x}(\mathbf{m}_{ij,F}))+ \mathbb{E}[g\cdot\mathbf{\vec{X}}]\] (54) \[=\mathbf{Q}(\mathbf{\vec{x}}_{i}+\sum\nolimits_{j\in\mathcal{N}(i)}(\mathbf{ \vec{x}}_{i}-\mathbf{\vec{x}}_{j})\phi_{x}(\mathbf{m}_{ij,F}))-\mathbf{Q}\mathbb{E}[\mathbf{ \vec{X}}]+g\cdot\mathbb{E}[\mathbf{\vec{X}}]\] (55) \[=\mathbf{Q}(\mathbf{\vec{x}}_{i}+\sum\nolimits_{j\in\mathcal{N}(i)}(\mathbf{ \vec{x}}_{i}-\mathbf{\vec{x}}_{j})\phi_{x}(\mathbf{m}_{ij,F}))+\mathbf{\vec{t}}\] (56) \[=g\cdot(\mathbf{\vec{x}}_{i}+\sum\nolimits_{j\in\mathcal{N}(i)}(\mathbf{ \vec{x}}_{i}-\mathbf{\vec{x}}_{j})\phi_{x}(\mathbf{m}_{ij,F})) \tag{57}\]

By replacing \(g\) with identical element \(I\) in \(E(3)\), since \(F=F_{I}\), we can immediately derive:

\[F^{-1}\vec{f}_{i}((\{\mathbf{h}_{i},F(\mathbf{\vec{x}}_{i})\}) =(\mathbf{\vec{x}}_{i}+\sum\nolimits_{j\in\mathcal{N}(i)}(\mathbf{\vec{x} }_{i}-\mathbf{\vec{x}}_{j})\phi_{x}(\mathbf{m}_{ij,F})), \tag{58}\] \[g\cdot F^{-1}\vec{f}_{i}((\{\mathbf{h}_{i},F(\mathbf{\vec{x}}_{i})\}) =F^{-1}_{g}\vec{f}_{i}(\{\mathbf{h}_{i},F_{g}(g\cdot\mathbf{\vec{x}}_{i})\}), \tag{59}\]

which concludes Proposition 3.1.

Algorithm for Training and Sampling

We present the pseudo codes for training in Algorithm 1 amd sampling in Algorithm 2.

```
0: geometric data of protein-peptide complexes \(\mathcal{S}\)
0: encoder \(\mathcal{E}_{\phi}\), decoder \(\mathcal{D}_{\xi}\), denoising network \(\mathbf{\epsilon}_{\theta}\)
1:functionTrainAutoEncoder(\(\mathcal{S}\))
2: Initialize \(\mathcal{E}_{\phi}\), \(\mathcal{D}_{\xi}\)
3:while\(\phi,\xi\) have not converged do
4: Sample \((\mathcal{G}_{p},\mathcal{G}_{b})\sim\mathcal{S}\)
5:\(\tilde{\mathcal{G}}_{p}\leftarrow\mathrm{mask}(\mathcal{G}_{p})\){Mask 25% Residues}
6:\(\{\mathbf{(\mu}_{i},\mathbf{\sigma}_{i},\mathbf{\tilde{\mu}}_{i},\mathbf{\tilde{\sigma}}_{i}) \}\leftarrow\mathcal{E}_{\phi}(\tilde{\mathcal{G}}_{p},\mathcal{G}_{b})\){Encoding}
7:\(\{\mathbf{(\epsilon}_{i},\mathbf{\tilde{\epsilon}}_{i})\}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\){Reparameterization}
8:\(\mathcal{G}_{z}=\{(\mathbf{\mu}_{i}+\mathbf{\epsilon}_{i}\odot\mathbf{\sigma}_{i},\mathbf{ \tilde{\mu}}_{i}+\mathbf{\tilde{\epsilon}}_{i}\odot\mathbf{\tilde{\sigma}}_{i})\}\){Decoding}
9:\(\mathcal{G}_{p}^{\prime}\leftarrow\mathcal{D}_{\xi}(\mathcal{G}_{z},\mathcal{ G}_{b})\){Decoding}
10:\(\mathcal{L}_{AE}=\sum_{i\in\mathcal{G}_{p}}(\mathcal{L}_{recon}(i)+\mathcal{L}_{ KL}(i))/|\mathcal{G}_{p}|\)
11:\(\phi,\xi\leftarrow\mathrm{optimizer}(\mathcal{L}_{AE};\phi,\xi)\)
12:endwhile
13:return\(\mathcal{E}_{\phi}\), \(\mathcal{D}_{\xi}\)
14:endfunction
15:
16:functionTrainLatentDiffusion(\(\mathcal{E}_{\phi},\mathcal{D}_{\xi},\mathcal{S}\))
17: Initialize \(\mathbf{\epsilon}_{\theta}\)
18:while\(\theta\) have not converged do
19: Sample \((\mathcal{G}_{p},\mathcal{G}_{b})\sim\mathcal{S}\)
20:\(\{(\mathbf{\mu}_{i},\mathbf{\sigma}_{i},\mathbf{\tilde{\mu}}_{i},\mathbf{\tilde{\sigma}}_{i}) \}\leftarrow\mathcal{E}_{\phi}(\mathcal{G}_{p},\mathcal{G}_{b})\){Encoding}
21:\(\mathcal{G}_{p}^{0}\leftarrow\{(\mathbf{\mu}_{i},\mathbf{\tilde{\mu}}_{i})\}\){Affine Transformation}
22:\(F\leftarrow\mathrm{affine}(\mathcal{G}_{b})\){Standard Geometry}
23:\(\mathcal{G}_{z},\mathcal{G}_{b}\leftarrow(F(\mathcal{G}_{z}),F(\mathcal{G}_{b})\){Standard Geometry}
24:\(\{(\mathbf{\tilde{\mu}}_{i},\mathbf{\tilde{\sigma}}_{i}^{0})\}\leftarrow\mathcal{G}_ {z}^{0}\)
25:\(t\sim\mathbf{U}(\mathbf{1},\mathbf{T})\), \(\{(\mathbf{\epsilon}_{i},\mathbf{\tilde{\epsilon}}_{i})\}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\)
26:\(\mathcal{G}_{z}^{t}\leftarrow\{\sqrt{\tilde{\alpha}^{t}}[\mathbf{z}_{i}^{0},\mathbf{ \tilde{z}}_{i}^{0}]+(1-\tilde{\alpha}^{t})[\mathbf{\epsilon}_{i},\mathbf{\tilde{ \epsilon}}_{i}]\}\)
27:\(\mathcal{L}_{\textit{DM}}^{t}=\sum_{i}\big{\|}[\mathbf{\epsilon}_{i},\mathbf{\tilde{ \epsilon}}_{i}]-\mathbf{\epsilon}_{\theta}(\mathcal{G}_{z}^{t},\mathcal{G}_{1},t )[i]\big{\|}^{2}/|\mathcal{G}_{z}^{t}|\)
28:\(\theta\leftarrow\mathrm{optimizer}(\mathcal{L}_{\textit{DM}}^{t};\theta)\)
29:endwhile
30:return\(\mathbf{\epsilon}_{\theta}\)
31:endfunction
32:
33:\(\mathcal{E}_{\phi},\mathcal{D}_{\xi}\leftarrow\mathrm{TrainAutoEncoder}( \mathcal{S})\)
34:Fix parameters \(\phi\) and \(\xi\)
35:\(\mathbf{\epsilon}_{\theta}\leftarrow\mathrm{TrainLatentDiffusion}(\mathcal{E}_{ \phi},\mathcal{D}_{\xi},\mathcal{S})\)
36:return\(\mathcal{E}_{\phi}\), \(\mathcal{D}_{\xi}\), \(\mathbf{\epsilon}_{\theta}\)
```

**Algorithm 1** Training Algorithm of PepGLAD

## Appendix E Data Preparation

We show details for constructing the datasets used in our paper here. Further statistics are presented in Table 5 and distribution of peptide lengths in Figure 5.

### Unsupervised Data from Protein Fragments (ProtFrag)

We exploit unsupervised data from monomer proteins to enrich the training of the autoencoder. Specifically, we first extract all single chains from the Protein Data Bank (PDB) before December 8th, 2023, and remove the duplicated chains on a sequence identity threshold of over 90%. Then, for each chain, we extract fragments satisfying the following criteria:1. **Length**: The fragment should consist of 4 to 25 residues.
2. **Balanced constitution**: No single amino acid should constitute more than 25% of the fragment; Hydrophobic amino acids should comprise less than 45% of the fragment, with charged amino acids accounting for 25% to 45%.
3. **Isolated Stability**: Instability [22] should be below 40; Considering the surrounding amino acids as interaction partners, the fragment should have a buried surface area (BSA) above \(400\text{\AA}^{2}\)[10], with a relative BSA above 20%.

We use FreeSASA [47] to calculate the surface area of fragments. Let \(\text{SA}_{bound}\) represent the surface area of the isolated fragment when considering surrounding amino acids, and \(\text{SA}_{unbound}\) represent the surface area when not considering them. The buried surface area is then calculated as \(\text{BSA}=\text{SA}_{unbound}-\text{SA}_{bound}\), and the relative BSA is calculated as \(\text{BSA}_{rel}=\text{BSA}/\text{SA}_{unbound}\). In total, we obtain 70,645 fragments meeting these criteria.

### Construction of Our PepBench

Here we illustrate the details for constructing the supervised dataset (PepBench) used in our paper. Similar to literature [65], we also exploits available data from the Protein Data Bank (PDB) [5]. We first extract all dimers in PDB deposited before December 8th, 2023, and filter out the complexes with a receptor longer than 30 residues and a ligand between 4 to 25 residues, which aligns with Tsaban et al. [59]. Peptides with lengths in this range are more relevant to practical applications such as drug discovery, as they exhibit favorable biochemical properties [49]. Then we remove the duplicated complexes with the criterion that both the receptor and the peptide has a sequence identity over 90% [56], after which 6105 non-redundant complexes are obtained. We use MMseqs2 for clustering based on sequence identity:

```
#createdatabasefromthesequences
#messqscreateseq.fastsatabase
#clusteringwithsequenceidentityabove90%
#messqsclusterdatabasedatabase.clustersresults-min-seq-id0.9-c0.95-cov-mode1
```

**Algorithm 2** Sampling Algorithm of PepGLAD

To achieve the cross-target generalization test, we utilize the large non-redundant dataset (LNR) introduced by Tsaban et al. [59] as the test set, which is curated by domain experts. LNR originally includes 96 protein-peptide complexes. We obtain 93 complexes after excluding the ones with non-canonical amino acids. We then cluster the LNR along with the PDB data by receptor with a sequence identity threshold of over 40%. Subsequently, we remove the complexes sharing the same clusters with those from the test set and those including non-canonical amino acids in the peptides. Finally, the remaining data are randomly split based on clustering results into training and validation sets. The characteristics of different splits are presented in Table 5. In addition, the binding site contains residues on the receptor within 10A distances to the peptide, where the distance between two residues is measured by the distance between their C\({}_{\beta}\) coordinates.

### Split of PepBDB

Similar to the split of PepBench, we use MMseqs2 for clustering and randomly split the data into training, validation, and test sets based on the clustering results. For the test set, we randomly select one protein-peptide complex in each cluster to avoid redundancy.

## Appendix F E(3)-Invariant Latent Space

We found that the E(3)-equivariant latent vectors were significant, which can convey sufficient geometric information. If we use E(3)-invariant latent space without these geometric latent vectors, the reconstruction ability (RMSD, DockQ) of the VAE deteriorates significantly (Table 6). It is reasonable since E(3)-invariant latent space lacks geometric interactions with the pocket atoms, leading to difficulties in reconstructing the full-atom structures on the binding site.

## Appendix G Guidance on Sequence Orders for Sampling

Peptides consist of linearly connected amino acids, which exert constraints on the 3D geometry. Specifically, residues adjacent in the sequence should also be close in the structure, since they are connected by a peptide bond. However, 3D graphs are unordered and do not incorporate such induct bias, which means the generated nodes might have arbitrary permutation on sequence orders. To tackle this problem, we take inspiration from classifier-guided diffusion [15], which adds the gradient

\begin{table}
\begin{tabular}{c c c c} \hline \hline Latent Space & AAR\(\uparrow\) & RMSD \(\downarrow\) & DockQ\(\uparrow\) \\ \hline E(3)-equivariant & 95.1\% & 0.79 & 0.898 \\ E(3)-invariant & 93.4\% & 1.75 & 0.823 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison of reconstruction ability of variational autoencoders with E(3)-equivariant latent space and E(3)-invariant latent space.

Figure 5: Peptide (or protein fragment) length distribution of three datasets.

\begin{table}
\begin{tabular}{l r r l} \hline \hline Split & \#entry & \#cluster & source \\ \hline PepBench (Training) & 4,157 & 952 & PDB [5] \\ PepBench (Validation) & 114 & 50 & PDB [5] \\ PepBench (Test) & 93 & 93 & LNR [59] \\ PepBDB (Training) & 8,434 & 1,617 & PepBDB [65] \\ PepBDB (Validation) & 370 & 95 & PepBDB [65] \\ PepBDB (Test) & 190 & 190 & PepBDB [65] \\ ProfFrag & 70,645 & - & monomers in PDB \\ \hline \hline \end{tabular}
\end{table}
Table 5: Statistics of the constructed datasets.

of a classifier to the denoising outputs to guide the generative diffusion process towards the subspace where the classifier gives high confidence. We utilize an empirical classifier \(p(1|\{\vec{\mathbf{z}}_{i}^{t}\})\) defined as follows:

\[p(1|\{\vec{\mathbf{z}}_{i}^{t}\}) =\exp(-\sum\nolimits_{\mathcal{P}(i)-\mathcal{P}(j)=1}E(\|\vec{\bm {z}}_{i}^{t}-\vec{\mathbf{z}}_{j}^{t}\|)), \tag{60}\] \[E(d) =\left\{\begin{array}{rl}d-(\mu_{d}+3\sigma_{d}),&d>\mu_{d}+3 \sigma_{d},\\ (\mu_{d}-3\sigma_{d})-d,&d<\mu_{d}-3\sigma_{d},\\ 0,&\text{otherwise},\end{array}\right. \tag{61}\]

where \(\mu_{d}\) and \(\sigma_{d}\) are the mean and variance of the distances of adjacent residues in the latent space measured from the training set. With \(p(1|\{\vec{\mathbf{z}}_{i}^{t}\})\), we are able to assign an arbitrary permutation on sequence orders to the nodes, and steer the sampling procedure towards the desired subspace conforming to \(\mathcal{P}\), since this classifier gives higher confidence if the adjacent (defined by \(\mathcal{P}\)) residues are within reasonable distances aligning with the statistics from the training set. In particular, the coordinate denoising outputs are refined as follows:

\[\vec{\mathbf{\epsilon}}_{i}^{t}=\vec{\mathbf{\epsilon}}_{\theta}(\mathcal{G}_{z}^{t}, \mathcal{G}_{b},t)[i]-\lambda\sqrt{1-\bar{\alpha}^{t}}\nabla_{\vec{\mathbf{z}}_{i} ^{t}}\log p(1|\{\vec{\mathbf{z}}_{i}^{t}\}), \tag{62}\]

where \(\lambda\) adjusts the weight of the guidance. Besides the constraints on the distance between adjacent residues, we can also include guidance on avoiding clashes between non-adjacent residues by defining the following energy term:

\[C(d)=\left\{\begin{array}{rl}\mu_{d}-d,&d<\mu_{d},\\ 0,&\text{otherwise},\end{array}\right. \tag{63}\]

Subsequently, we just need to revise the empirical classifier as:

\[p(1|\{\vec{\mathbf{z}}_{i}^{t}\})=\exp(-\sum\limits_{\mathcal{P}(i)- \mathcal{P}(j)=1}E(\|\vec{\mathbf{z}}_{i}^{t}-\vec{\mathbf{z}}_{j}^{t}\|)-\sum\limits _{\mathcal{P}(i)-\mathcal{P}(j)\neq 1}C(\|\vec{\mathbf{z}}_{i}^{t}-\vec{\mathbf{z}}_{j }^{t}\|)-\sum\limits_{i\in\mathcal{G}_{z},j\in\mathcal{G}_{b}}C(\|\vec{\mathbf{z} }_{i}^{t}-\vec{\mathbf{r}}_{j}\|), \tag{64}\]

where \(\vec{\mathbf{r}}_{j}\) is the \(\mathcal{C}_{\alpha}\) coordinate of node \(j\) in the binding site. We observe a slight improvement upon including the clash energy term. Nevertheless, the guidance is only a small technical trick with minor enhancement on the performance as shown in Table 7.

## Appendix H Metrics for Evaluating Sequence-Structure Co-Design

### Why not Amino Acid Recovery (AAR)

\begin{table}
\begin{tabular}{c c c c} \hline Guidance & RMSD\({}_{C_{\alpha}}\downarrow\) & RMSD\({}_{\text{atom}}\downarrow\) & DockQ\(\uparrow\) \\ \hline w/ & 4.09 & 5.30 & 0.592 \\ w/o & 4.10 & 5.34 & 0.582 \\ \hline \end{tabular}
\end{table}
Table 7: Results on binding conformation generation with and without guidance on sequence orders.

Figure 6: Best amino acid recovery (AAR) on samples constructed according to specified positive ratios. Each point contains the averaged result across 102 receptors.

While the amino acid recovery (AAR) is widely used in antibody design [33, 34, 44], we find it not informative enough for evaluating generative models on learning the distribution of binding peptides, due to the vast and highly diverse solution space. To elucidate this, we conducted an analysis of AAR using a sequence dataset from Xia et al. [67] including 328 receptors and 600k peptides with binary binding labels, from which we filter out 102 receptors with well-explored solution space (_i.e._ with at least 500 binders). For each receptor, we randomly select one binder as the reference and sample \(N\) candidates according to a specified positive ratio \(r\), resembling the scenario of evaluating generative models for peptide design. For example, setting \(r=0.1\) and \(N=100\) involves sampling 10 binders and 90 non-binders to construct the candidates. Then we compute the best AAR of these candidates with respect to the reference as the result. This process is repeated for 10 times per receptor, and the results are averaged across different receptors, which can be interpreted as the evaluation score of AAR on a generative model that can generates candidates with a positive ratio of \(r\). We select \(N=10,40,100\) and enumerate the choices of \(r\) to derive the relation plot in Figure 6, where the results of a random sequence generator is also included for comparison. It can be derived that the gap of AAR between the worst model (\(r=0.0\)) and the best model (\(r=1.0\)) is insignificant. Furthermore, all models, including the best model (\(r=1.0\)) which always produces positive samples, exhibit performance akin to the random sequence generator, with consistent trends regardless of the choice of \(N\). We attribute this to the vast solution space of peptide design, where evaluating with dozens of candidates relative to a single reference is unreliable. In other words, **achieving a high AAR is improbable since the model would need to fortuitously explore the subspace around the reference, which is arbitrary, on every test receptor; Conversely, a low AAR does not necessarily denote a poor generative model, as the model may be exploring distinct solutions from the single reference.** Moreover, we calculate the Spearman correlation between the receptor-level best AAR and the positive ratio \(r\) of the candidates, yielding \(0.23\), \(0.22\) and \(0.24\) for \(N=10\), 40, and \(100\), respectively, indicating very weak correlation. Based on this analysis, we assert that AAR is unsuitable for evaluating target-specific peptide design. The comparison of AAR and success rates on PepBench (Table 8) also indicates that models with similar AAR can have distinct success rates.

### "Unsupervised" Consistency

While it is common to evaluate consistency by comparing generated structures with AF2-predicted structures [7], such a "supervision-based" method suffers from severe limitations in the situation where AF2 [31] fails to achieve an acceptable performance. As highlighted not only in Tsaban et al. [59] but also demonstrated by our experiments, even state-of-the-art models like AF2 struggles to consistently produce high-quality structures of protein-peptide complexes. Our findings reveal that only 36% of test samples could be accurately predicted within a 5A RMSD (considered as near-native conformation) by AF2, indicating even the ground truth can only achieve 36% success rates if we use such supervison-based consistency for evaluation, making such evaluation not reliable in the domain of peptide design.

In light of this limitation, we propose an "unsupervised" evaluation framework to assess consistency, that is, the statistical association between the clustering results of sequences and structures. Theoretically, this serves the necessary condition for true consistency. Namely, if a model truly captures the consistency between sequence and structure, it will necessarily achieve a high score on the proposed metric. Conversely, if a model fails to attain a high score on the proposed metric, it is not possible to capture true consistency.

In our experiments, we found that our proposed consistency metric effectively distinguishes non-consistent modeling methods, such as HSRN, which tend to produce disparate sequences while sharing identical structures.

\begin{table}
\begin{tabular}{c c c} \hline \hline Model & AAR & Success (\(\Delta G<0\)) \\ \hline HSRN & 35.8\% & 10.46\% \\ DiffAb & 37.1\% & 49.87\% \\ PepGLAD & 36.7\% & 55.97\% \\ \hline \hline \end{tabular}
\end{table}
Table 8: Amino acid recovery and success rates of different models on PepBench.

### Implementation and Elaboration on Metrics

Indeed, evaluating generative models comprehensively is crucial, requiring assessment from multiple perspectives. Generally, these evaluations can be categorized into two main aspects: diversity and fidelity to the desired distribution. Regarding diversity, we take inspiration from Yim et al. [72] and quantify it with the number of unique clusters relative to the number of candidates. This metric provides insight into the variety and richness of the generated samples. For fidelity, the primary focus should be the binding affinity, for which we adopt the physical energy from Rosetta [2] since it is widely used in various domains and exhibit robust generalization capability [1; 37; 64]. Further, considering the dependency between sequence and structure, we propose the consistency metric, which is critical for distinguishing whether the generative model is capturing the 1D&3D joint distribution and thereby truly facilitating "co-design". We have also discussed the reason for implementing the consistency metric with the "unsupervised" fashion in the above section. Below, we outline the implementation of these metrics.

DiversityWe hierarchically cluster the sequences and the structures with aligning score [36] and RMSD of \(\texttt{C}_{\alpha}\), respectively. In particular, we implement the aligning score using Biopython [11] with BLOSUM62 matrix [24] and Needleman-Wunsch algorithm [50]. The thresholds for clustering is similarity above 0.6 and RMSD below 4.0 for sequence and structure, respectively. We provide the python codes below for clearer presentation:

```
frommathimportsqrt fromtypingimportList fromBio.Alignimportsubstitution_matrices,PairwiseAligner importnumpyasnp defalign_score(sequence_A,sequence_B):
#loadmatrix sub_matrices=substitution_matrices.load('BLOSUM62') aligner=PairwiseAligner() aligner.substitution_matrix=sub_matrices #align alms=aligner.align(sequence_A,sequence_B) best_lan=alms[0] aligned.A,aligned_B=best_alm
#normalize base_A=aligner.score(sequence_A,sequence_A) base_B=aligner.score(sequence_B,sequence_B) base=sqrt(base_A*base_B) similarity=aligner.score(sequence_A,sequence_B)/base returnsimilarity defseq_diversity(seg:List[ntr],th:float=0.4)->float: ''' th:sequencedistance(1-similarity) ''' distrs=[] fori,sequence_Ainenumerate(seg): distrs.append([]) forj,sequence_Binenumerate(seg): distrs[i].append(1-align_score(sequence_A,sequence_B)) distrs=np.array(distrs) Z=linkage(sameform(distrs),'single') cluster=cluster(Z,'tth,criterion='distance') returnlen(np.unique(cluster))/len(seg) defstruct_diversity(structs:np.ndarray,th:float=4.0)->float: ''' structrs:N*L*3,alphacarboncoordinates ;; th:thresholdforclustering(distance<th) ;; ca_dists=np.sum((structs[:,None]-structs[None,:])**2,axis=-1) rmsd=np.sqrt(np.mean(ca_dists,axis=-1)) Z=linkage(sameform(rmsd),'single') cluster=cluster(Z,'tth,criterion='distance') returnlen(np.unique(cluster))/structs.shape[0]
```

Denoting the diversity of the sequences and the structures as \(\text{Div}_{seq}\) and \(\text{Div}_{struct}\), respectively, we calculate the co-design diversity as \(\sqrt{\text{Div}_{seq}\text{Div}_{struct}}\).

ConsistencyThe clustering process in the calculation of diversity will assign each sequence and each structure with a clustering label. The labels on the sequences and those on the structures can be regarded as two nominal variables. Since similar sequences should produce similar structures, these two variables should be highly correlated if the model really learns the joint distribution. Naturally, we quantify the consistency via Cramer's V [12] association between these two variables, with 1.0 indicating perfect association, and 0.0 means no association.

\(\Delta\)GWe use Rosetta [2] to calculate the binding energy with the "ref2015" score function. Both the candidates and the references first endure the fast relax protocol in Rosetta to correct atomic clashes at the interface before the computation of binding energy. We use the implementation in pyRosetta (_i.e._ the python version of Rosetta), which is borrowed from Luo et al. [44]:

```
importpyrosetta frompyrosetta.rosettaimportprotocols frompyrosetta.rosetta.protocols.relaximportFastRelax frompyrosetta.rosetta.core.pack.taskimportTaskFactory frompyrosetta.rosetta.core.pack.taskimportoperation frompyrosetta.rosetta.core.selectimportresidue_selectors asselections frompyrosetta.rosetta.core.select.movemap importMoveMapFactory,move_map_action frompyrosetta.rosetta.core.scoring importScoreType frompyrosetta.rosetta.protocols.analysisimportInterfaceAnalyzerMover pyrosetta.init(' '.join( '-mute', 'all', '-usp_input_sc', '-ignore_unrecognized_res', '-ignore_zero_occupancy', 'false', '-load_PDB_components', 'false', '-relax:default_repeats', '2', '-no_fconfig', '-use_terminal_residues', 'true', '-in:in1_in1_struct_type', 'binary' }))
``` classRelaxRegion(object):#FastRelax def__init__(self,max_iter=1000): super()__init__() self.scorefram=pyrosetta.create_score_function('ref2015') self.fast_relax_FastRelax() self.fast_relax.set_scorefram(self.scorefram) self.fast_relax.max_iter(max_iter) def__call__(self,pdb_path,peptide_chain): pose=pyrosetta.pose_from_pdb(pdb_path) tf=TaskFactory() tf.push_back(operation.InitializeFromCommandline()) tf.push_back(operation.RestrictToRepacking())#Onlyallowresiduestopreack. Nodesignatanyposition. gen_selector=selections.ChainSelector(chain) nbr_selector=selections.NeighbourhoodEsidueSelector() nbr_selector.net_focus_selector(gen_selector) nbr_selector prevent_repacking_rlt=operation.PreventRepackingRLT() prevent_subset_repacking=operation.OperateOnResidueSubset( prevent_repacking_rlt, subset_selector, flip_subset=True, } tf.push_back(prevent_subset_repacking) fr=self.fast_relax pose=original_pose.clone() mmf=MoveMapFactory() mmf.add_bb_action(move_map_action.mm_enable,gen_selector) mmf.add_chi_action(move_map_action.mm_enable,subset_selector) mm=mf.create_movemap_from_pose(pose) fr.set_movemap(mm) fr.set_task_factor(tf) fr.apply(pose)return pose defpyrosetta_interface_energy(pdb_path,receptor_chains,ligand_chains):#binding energy pose=pyrosetta.pose_from_pdb(pdb_path) interface=":join(ligmal.chains)+":join(receptor_chains) mover=InterfaceAnalyzerMover(interface) mover.set_pack_separated(True) mover.apply(pose) returnpose_scores['d6_separated'] ```

SuccessSince a negative \(\Delta G\) value typically indicates a potential for binding, we report the ratio of all generated candidates that satisfy this threshold. Moreover, candidates with \(\Delta G<0\) usually are at least physically valid (_i.e._ without obvious atomic clash), thus it is meaningful to use this success rate to evaluation the generative ability of the models.

## Appendix I Experiment Details

### Implementation of PepGLAD

We train PepGLAD on a GPU with 24G memory with AdamW optimizer. For the autoencoder, we train for 60 epochs with dynamic batches, ensuring that the total number of edges (proportional to the square of the number of nodes) remains below 60,000. The initial learning rate is \(10^{-4}\) and decays by 0.8 if the loss on the validation set has not decreased for 3 consecutive epochs. Regarding the diffusion model, we train for 500 epochs with the same batching strategy as for the autoencoder. The learning rate is \(10^{-4}\) and decay by 0.6 if the loss has not decreased for 3 consecutive validations, where the validation is conducted every 10 epochs. In the experiment of binding conformation generation, we only use the supervised dataset, thus we extend the training epochs for the autoencoder and the diffusion model to 500 and 1000, respectively. Consequently, the patience of learning rate decay is extended to 15 epochs for training the autoencoder. We keep other settings unchanged. The hyperparameters of PepGLAD used in our experiments are provided in Table 9.

### Implementation of the Baselines

For **HSRN**[30], **dyMEAN**[34], and **DiffAb**[44], we directly integrate their official implementation into the same training framework as our PepGLAD, and adjust the batch size, learning rate, and training epochs to obtain the optimal performance, which we present in Table 10.

We outline the implementation of other baselines below:

\begin{table}
\begin{tabular}{c c c l} \hline \hline \multirow{2}{*}{Name} & \multicolumn{2}{c}{Value} & \multirow{2}{*}{Description} \\ \cline{2-2} \cline{4-4}  & codesign & & \multicolumn{1}{l}{\(\text{conformation}\)} & \\  & & & \multicolumn{2}{c}{Variational AutoEncoder} \\ \hline embed size & 128 & 128 & Size of embeddings for residue types. \\ hidden size & 128 & 128 & Size of hidden layers. \\ \(h\) & 8 & - & Size of the latent variable for residue types in the sequences. \\ layers & 3 & 3 & Number of layers. \\ \(\lambda_{1}\) & 0.1 & 0.0 & The weight of KL divergence on the sequence. \\ \(\lambda_{2}\) & 0.5 & 0.5 & The weight of KL divergence on the structure. \\ \(\lambda_{CA}\) & 1.0 & 1.0 & The weight of \(\mathsf{C}_{\alpha}\) loss in \(\mathcal{L}_{aux}\). \\ \(\lambda_{bond}\) & 1.0 & 1.0 & The weight of bond loss in \(\mathcal{L}_{aux}\). \\ \(\lambda_

**RFDiffusion**[64] We follow the official instruction to randomly select 20% of residues on the binding site as "hotspots", and generate the backbone via diffusion followed by cycles of inverse folding with ProteinMPNN [14] and full-atom structural refining with the officially provided Rosetta protocol.

**AnchorExtension**[26] It is implemented with the Rosetta suite, thus we resort to the official release of the pipeline protocols for docking and optimizing. We use the default parameters and generate 10 candidates for each receptor due to its limitation of efficiency. We randomly pick one peptide in the training set with the same length as the reference peptide as the initial motif for docking.

**FlexPepDock**[42] We follow its official tutorial to implement this baseline in the C++ version of Rosetta.

**AlphaFold2**[31] We borrow the results from [59], which explores two strategy to use AlphaFold2 on peptide conformation generation, including modeling the receptor and the peptide as separate chains or link them together with long loops. The results contain a total of 10 candidates for each receptor, with 5 from the separate strategy and 5 from the linked strategy.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Model & Batch Size & Learning Rate & Epoch \\ \hline HSRN & 8 & 1.0e-4 & 50 \\ dyMEAN & 32 & 1.0e-4 & 100 \\ DiffAb & 32 & 1.0e-4 & 50 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Hyperparameters for training the baselines.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: SS 3 and SS 4 illustrate the claims well from the methodology and the empirical aspects. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have an independent section (SS 6) to discuss limitations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: See Appendix B. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: In addition to the methodology section (SS 3), we provide implementation details in Appendix I. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes]

Justification: We provide codes and data for our model and the experiments.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See SS 4.1, Appendix E and Appendix I. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The size of the dataset is large, and we report results on two different datasets for more solid evaluation. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix I. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We ensure the research to conform to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See SS 7. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not pose such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: They are properly credited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The README.md in the provided codes illustrate how to construct the benchmark from publicly available resources. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve such a topic. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.