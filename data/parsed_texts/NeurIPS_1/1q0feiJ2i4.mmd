[MISSING_PAGE_FAIL:1]

Introduction

**Visual reasoning** is a crucial task that demands models to not only comprehend and interpret visual information but also to apply high-level cognition to derive logical solutions [40; 120; 60]. The field has received significant attention from the machine learning community because of its potential to enable a wide range of intelligent applications, such as intelligent tutoring systems [5; 94; 69], automated image captioning [103], and virtual assistants [88; 50]. To perform visual reasoning effectively, a model must possess both visual perception capabilities and strong logic reasoning abilities.

While classic visual reasoners typically rely on complex architectures [117; 61; 116] or are unable to generalize beyond the training dataset [121; 72], recent advancements in large pretrained models have shown that vision-language models (VLMs) can achieve impressive performance on visual reasoning tasks even under zero-shot settings [104; 52; 51; 49; 48; 3]. Meanwhile, large language models (LLMs) have also demonstrated robust zero-shot commonsense reasoning abilities on the natural language processing (NLP) applications [109; 8; 15]. Several recent studies have attempted to combine such complementary VLMs and LLMs for visual reasoning. For example, PICA [115] utilizes image captioning models to generate textual prompts for GPT-3 [8], and adapts GPT-3 to solve the visual question answering (VQA) tasks in an in-context few-shot learning manner. Socratic Models [123] allow VLMs and LLMs to communicate through prompt engineering to unlock zero-shot multimodal reasoning capabilities.

On the premise that current studies have focused on the interactions among heterogeneous models (specifically, among VLM and LLMs), in this work, we shift to examine how to reconcile **homogeneous expert models** (_e.g._, multiple VLMs) with an LLM in a _coordinative_ paradigm. Inspired by the findings in CICERO [65] that LLMs capture strong strategic planning and negotiation abilities in coordinating multiple agents, we propose \(\overline{\textbf{\#}}\)**Cola**, a novel **model ensemble** approach that utilizes an LLM as the coordinator in between multiple VLMs. A key finding of this study is that _given multiple VLMs with different preferred patterns in describing the visual context and predicting plausible answers in natural languages, an LLM can coordinate and integrate their respective strengths efficiently and effectively_. We present two variants of Cola, namely \(\overline{\textbf{\#}}\)**Cola-FT** and \(\overline{\textbf{\#}}\)**Cola-Zero**, where FT corresponds to an instruction finetuning approach and Zero stands for an in-context learning approach to adapt the coordinator LLM for visual reasoning. Figure 1 provides an overview of Cola and conventional model ensemble approaches.

Existing work on model ensembles usually focuses on manipulating model weights [36] or predictions [111; 22], while remaining cumbersome, if possible, to implement on prevalent end-to-end black box model APIs, like GPT-4 [70], Google Bard, Anthropic Claude, etc. To address this issue, prompt ensembles [107; 106; 73] sample model outputs (_e.g._, rationales) in natural languages to boost chain-of-thought reasoning [109]. Recent studies on augmented LLM such as [86; 93; 58] have delved into developing a comprehensive strategy that enables LLMs to utilize external tools. These tools comprise multiple off-the-shelf models, web search engines, Python functions [97], and rule-based modules, which are instrumental in performing complex tasks. Despite these efforts, the power of prompt ensembles to aggregate multiple models remains untouched. In contrast, we show that Cola leverages language prompts generated from multiple expert models to make model ensembles.

Systematic experiments demonstrate that Cola performs at the pinnacle of ability on VQA, outside knowledge VQA, visual entailment, and visual spatial reasoning tasks. Specifically, Cola-FT achieves state-of-the-art performance on A-OKVQA [89], OK-VQA [63], e-SNLI-VE [21], and VSR datasets [56], even when compared with methods that adopt larger models or require more training computations. Cola-FT also demonstrates competitive capabilities on VQA v2 [27], and compositional reasoning tasks (GQA [35] and CLEVR [40]). Perhaps surprisingly, we find that Cola-Zero demonstrates comparable performance without finetuning, as an emerging ability of larger language models. Compared to a single VLM and ensemble modeling, both Cola-FT and Cola-Zero improve the performance substantially across most datasets. They are even effective with the recent large multimodal models like InstructBLIP [18] which embeds a pretrained LLM inside itself. Besides, we conduct a thorough analysis of perturbed VLM caption or plausible answer labels and saliency visualization to investigate how Cola recognizes each VLM's individual functionalities and then performs coordination behaviors. We conjecture that, in principle, any language-expressing reasoning task can be usefully augmented with coordinative language models that learn to aggregate multiple expert models, even via in-context learning.

In summary, our contributions are as follows:

* **Cola**: a novel paradigm that utilizes a language model as a coordinator between multiple vision-language models to integrate their respective strengths for visual reasoning (SS2).
* **State-of-the-art performance**: Cola achieves the pinnacles on a challenging suite of diverse visual reasoning tasks and datasets(SS3.2).
* **Systematic analysis**: our experiments reveal how Cola comprehends the instruction prompts, then coordinates them to capture impressive visual reasoning capabilities (SS3.3, SS3.4, SS3.6).

## 2 Cola

We formulate various visual reasoning tasks as a multi-class classification problem for simplicity. Given an image \(v\in\mathcal{V}\) and a question-like prompt \(q\in\mathcal{Q}\), the reasoner is required to select an answer \(a\) from the candidate set \(\mathcal{A}=\{a\}\). In the case that the reasoner outputs a text sequence \(s_{v,q}\), we map \(s\) to a prediction \(P(v,q)=\operatorname{sim}(T(s_{v,q}),T(\{a\}))\) where \(T\) transforms text sequences into text embeddings (we use a all-mpnet-base-v2 model [81] here), and \(\operatorname{sim}\) denotes cosine similarity.

Ensemble Modelingaggregates multiple models' predictions in order to improve the overall performance (Figure 0(a)). For instance, one common practice is averaging over \(n\) models:

\[P(v,q)=\frac{1}{n}\sum_{i=1}^{n}P_{i}(v,q), \tag{1}\]

where \(P_{i}(v,q)\) denotes the prediction of the \(i^{th}\) model on input \((v,q)\).

### 2.1 Cola & Templates

An overview of Cola is shown in Figure 0(c). We use OFA [104] and BLIP [52] as the VLMs. LLMs include encoder-decoder (FLAN-T5 [16]) and decoder-only (Vicuna-1.5 [125], Mistral [39]) transformers. We first prompt each VLM to output captions and plausible answers independently. We then concatenate the instruction prompt, the question with choices, captions, and plausible answers to fuse all contexts for the LLM to reason, coordinate, and answer.

Image captioninggives important visual context to reason from. We first employ \(i^{th}\) VLM to describe each image respectively to get visual descriptions \(c_{i}(v)\). We use ofa-large for OFA and blip-image-captioning-large for BLIP, both implemented by the Hugging Face Transformers library [110].

Plausible answersby the VLMs to the question provide clues and patterns of VLMs for the LM to consider and coordinate. Similar to captioning, we prompt each \(i^{th}\) VLM using the image-question pair to get a plausible answer \(\hat{a}_{i}(v,q)\). We use ofa-large for OFA and blip-vqa-base for BLIP. Following OFA, our prompt template varies by task category. For the VQA tasks, we leave the original question unchanged. For the visual entailment and visual spatial reasoning tasks, our prompt template is _"does the image describe "<text premise>"?"_.

Prompt templateis shown in Table 1. First, we designed an instruction prompt for LM to understand the requirement to coordinate VLMs to answer the visual reasoning question. We then concatenate the captions from each VLM model, with the VLM identification labels in natural languages (referred to as VLM caption labels in Figure 3), such as _"OFA's description: <OFA caption>"_. Next, the question and its plausible answers provided by VLMs (with similar identification labels referred to as VLM answer labels in Figure 3) are concatenated. We follow [16] to

\begin{table}
\begin{tabular}{l} \hline \hline
**General Prompt Template** \\ \hline Answer the following multiple-choice question by OFA and BLIP’s description and their answers to the visual question. OFA and BLIP are two different vision-language models to provide clues. \\ OFA’s description: \textless{}OFA caption\$ BLIP’s description: \textless{}BLIP caption\$ Q: \textless{}Question\$ Q: \textless{}Question\$ QFA’s answer: \textless{}OFA answers\$ BLIP’s answer: \textless{}BLIP answers\$ Choices: \textless{}Choices to the questions\$ A: \\ \hline \hline \end{tabular}
\end{table}
Table 1: **LM prompt template.** The LM is instructed to coordinate VLMs. Each question set defines _visual context_, _question (and choices)_, and _plausible answers_.

include the choices of question (for multiple-choice questions in A-OKVQA, e-SNLI-VE, and VSR) and "_A:"_ to prompt for answers. Overall, the prompt for LLM input is given by:

\[\mathrm{Prompt}(v,q)=\mathrm{Template}(\{(c_{i}(v),\hat{a}_{i}(v,q))\mid i=1, \cdots,n\}). \tag{2}\]

More specific prompt templates on each dataset are provided in Appendix A.7.

### Cola-FT

Instruction Tuningof Cola is initialized with pretrained checkpoints. Given the question \(q\) based on the image \(v\), the LM predicts the answer in the form of sequence

\[s_{v,q}=\mathrm{LLM}(\mathrm{Prompt}(v,q)). \tag{3}\]

To optimize the LLM, we use the language modeling loss for next-token prediction, with the teacher forcing strategy. We only finetune the LLM (while not the VLMs) to follow the common paradigm of ensemble modeling and simplify the method (Figure 1).

Inferencedeploys the same prompt as Table 1 to align with instruction tuning. We resort to the greedy decoding strategy for conditional sequence generation at both instruction tuning and inference.

### Cola-Zero

In-context learningis an emerging ability of the LLM models pretrained on documents of long-range coherence. By learning input and output format from demonstration, in-context learners learn to perform a downstream task simply by conditioning on a prompt consisting of input-output examples [114]. The coordinator LLM, finetuned on instruction prompts with examples, is capable of in-context few-shot learning and zero-shot learning (see Figures 6 and 7).

Cola-Zerois the in-context few-shot/zero-shot learning variant of Cola, without instruction tuning. For in-context \(k\)-shot learning, we modify the prompt (Table 1) to include \(k\) input-output examples sampled from the training set. For zero-shot learning, the prompt remains the same as Table 1.

## 3 Experiments

First, the experimental setups and basic methods are described in this section. The main quantitative results are then presented in Table 2. Next, we analyze qualitative visualizations and scaling to verify the effectiveness of the Cola paradigm in different settings. Further details on datasets, training, evaluation, and experimental analysis can be found in Appendix A.

### Baseline Methods

State-of-the-art Methodsare in two broad categories, VLM alone, and VLM combined with LLM. In Table 2, for a fair comparison, we detail the techniques (whether finetuning or in-context learning is required) used for training VLMs and LLMs, and the number of training epochs.

Ensemble Modelingcan be considered the most basic baseline for aggregating VLMs. It represents the base performance that the combination of VLMs can achieve on the target task when not trained. We implement an averaging ensemble (Equation (1)) of cosine similarity between VLM output and each choice of a question as our ensemble baseline.

### Overall Performance

In Table 2, we first observe that Cola-FT achieves state-of-the-art (SOTA) performance on four datasets (A-OKVQA, OK-VQA, e-SNLI-VE, VSR), with merely 1 epoch of instruction tuning and a medium-sized language model. In contrast, many previous SOTA methods require finetuning more epochs than Cola-FT (_e.g._, VLC-BERT, PromptCap on A-OKVQA). Some also use much larger language models, such as GPT-3 (175B) [8] and OPT (175B) [124]. Cola-FT outperforms OFA-X on e-SNLI-VE, although the latter is finetuned on much more related tasks and data (c.f.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c|}{**Vision-language Model**} & \multicolumn{3}{c|}{**Large Language Model**} & \multirow{2}{*}{**Accuracy \(\uparrow\)**} \\ \cline{2-2} \cline{4-6}  & **Model Spec.** & **FT \(\downarrow\)** & & **Model Spec.** & **ICL \(\downarrow\)** & **FT \(\downarrow\)** \\ \hline \hline \multicolumn{6}{c}{**Visual Question Answering (VQA v2)**} \\ \hline MetaLM [31] & Pretrained Encoder & 350k steps & MetaLM (1.3B) & - & 350k steps & 41.1 \\ PNP-VQA [99] & BLIP-Caption (446M) & - & UnifiedQAv2 [43] (11B) & 0-shot & - & 63.3 \\ BLIP-2 [51] & CLIP [76] (1.2B trainable) & 5 epochs & FLAN-TS (3B) & - & - & 81.6 \\ BLIP-2 [51] & CLIP [76] (1.2B trainable) & 5 epochs & OPT [12] (6.7B) & - & - & 82.2 \\ \hline Ensemble & \multirow{2}{*}{BLIP+OFA (384M+472M)} & - & - & - & - & - & 68.0 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 2-shot & - & 69.1 \\
**Cola-FT** & & - & FLAN-TS (11B) & - & 1 epoch & **83.7** \\ \hline \multicolumn{6}{c}{**Outside Knowledge Visual Question Answering, Multiple Choice (A-OKVQA)**} \\ \hline PromptCap [32] & OFA (472M) & 2 epochs & GPT-3 (175B) & 0-shot & - & - 73.2 \\ Img2Prompt [29] & BLIP (384M) & - & OPT (175B) & 0-shot & - & 42.9 / 40.7 \\ Prophet-MC [90] & MCAN-large [118] (56M) & 6 epochs & GPT-3 (175B) & 16-shot & - & 76.4 / 73.6 \\ \hline Ensemble & \multirow{2}{*}{BLIP+OFA (384M+472M)} & - & - & - & - & 56.6 / 54.9 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 0-shot & - & 65.4 / 61.6 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 2-shot & - & 70.4 / 66.5 \\
**Cola-FT** & & - & FLAN-TS (11B) & - & 1 epoch & 77.7 / 74.0 \\ \hline
**Cola-Zero** & & - & FLAN-TS (11B) & 0-shot & - & 68.0 / 66.5 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 2-shot & - & 72.3 / 72.3 \\
**Cola-FT** & & - & FLAN-TS (11B) & - & 1 epoch & **78.1 / 76.7** \\
**Cola-Zero** & XL+XXL & - & Vicuna (7B) & 2-shot & - & 63.9 / 63.0 \\
**Cola-FT** & (3B+11B) & - & Vicuna (7B) & - & 1 epoch & 68.6 / 66.9 \\
**Cola-Zero** & & - & Mistral (7B) & 2-shot & - & 69.3 / 66.2 \\
**Cola-FT** & & - & Mistral (7B) & - & 1 epoch & 74.3 / 71.8 \\ \hline \multicolumn{6}{c}{**Outside Knowledge Visual Question Answering, Direct Answer (OK-VQA)**} \\ \hline PromptCap [32] & OFA (472M) & 2 epochs & GPT-3 (175B) & 0-shot & - & 58.8 \\ Prophet [90] & MCAN-large [118] (56M) & 6 epochs & GPT-3 (175B) & 16-shot & - & 61.1 \\ \hline Ensemble & \multirow{2}{*}{BLIP+OFA (384M+472M)} & - & - & - & - & 39.2 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 0-shot & - & 39.4 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 2-shot & - & 39.4 \\
**Cola-FT** & & - & FLAN-TS (11B) & - & 1 epoch & **62.4** \\ \hline \hline \multicolumn{6}{c}{**Visual Entailment (e-SNL-VE)**} \\ \hline e-UG [42] & UNITE (86M) & 400 epochs & GPT-2 (117M) & - & 400 epochs & 79.5 \\ OFA-X [74] & OFA (472M) & 10 epochs & - & - & 80.9 \\ \hline Ensemble & \multirow{2}{*}{BLIP+OFA (384M+472M)} & - & - & - & - & 48.8 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 0-shot & - & 56.2 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 2-shot & - & 57.8 \\
**Cola-FT** & & - & FLAN-TS (11B) & - & 1 epoch & **81.6** \\ \hline \multicolumn{6}{c}{**Visual Spatial Reasoning (VSR)**} \\ \hline VisualBERT [53] & VisualBERT (110M) & 100 epochs & - & - & - & 54.0 \\ LXMERT [101] & LXMERT (110M) & 100 epochs & - & - & - & 63.2 \\ ViLT [44] & ViLT (88M) & 30 epochs & - & - & - & 62.4 \\ \hline Ensemble & \multirow{2}{*}{BLIP+OFA (384M+472M)} & - & - & - & - & 51.4 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 0-shot & - & 55.8 \\
**Cola-Zero** & & - & FLAN-TS (11B) & 2-shot & - & 54.9 \\
**Cola-FT** & & - & FLAN-TS (11B) & - & 1 epoch & **67.0** \\ \hline \multicolumn{6}{c}{**Compositional Question Answering, Real Images (GQA)**} \\ \hline BLIP [52] & BLIP (384M) & - & - & - & - & 41.7 \\ OFA [104] & OFA (472M) & - & - & - & - & 58.0 \\ VisProg [30] & ViLT (88M) & - & GPT-3 (175B) & 8-shot & - & 50.5 \\ \hline
**Cola-FT** & BLIP+OFA (384M+472M) & - & FLAN-TS (11B) & - & 1 epoch & **60.3** \\ \hline \multicolumn{6}{c}{**Compositional Question Answering, Synthetic Images (CLEVR)**} \\ \hline InstructBLIP [18] & XL (3B) & - & - & - & - & 33.7 \\ XXL (11B) & - & - & - & - & 16.6 \\ \hline
**Cola-Zero** & InstructBLIP & - & FLAN-TS (11B) & 2-shot & - & 34.4 \\
**Cola-FT** & XL+XXL (3B+11B) & - & FLAN-TS (11B) & - & 1 epoch & **54.3** \\ \hline \end{tabular}
\end{table}
Table 2: **Overall performance.** Model Spec. denotes specification where we summarize the detailed VLMs and LMs adopted in each method and their parameters. FT and ICT denote finetuning and in-context learning, respectively. Downward arrows indicate that fewer FT and ICT are more efficient. The accuracy metric varies slightly in different datasets. In A-OKVQA, we report both val/test accuracies, and val accuracy in VQA v2, OK-VQA, e-SNLI-VE, GQA, and CLEVR; test (zero-shot split) accuracy in VSR. Upward arrows indicate higher accuracy is better. We mark the best performance on each dataset with **bold font** and second-best with underlines.

Cola-FT is trained on each one dataset only in Table 2). In addition, the lighter variant Cola-Zero also achieves comparable performance to most baseline methods through in-context few-shot and zero-shot learning, without training any model parameter. To evaluate the performance of Cola with large multimodal models that embed large language models inside, we also assembled InstructBLIP [18] models based on FLAN-T5 XL and XXL and tested on A-OKVQA dataset. In Table 2, we report the multiple-choice accuracies on A-OKVQA. See Appendix A.4 for direct answer results.

### Qualitative Examples

In Figure 2, we exhibit several qualitative examples. The language coordinator determines the correctness of VLM plausible answers implicitly, given their captions and the caption and answer labels. The leftmost example (a tennis player playing) demonstrates a case when captions are not informative to guide the LLM for predictions. Between OFA and BLIP's plausible answers, the LLM follows the answer of BLIP. In contrast, in the left example (an oven next to a fridge), again with trivial captions, the LLM follows OFA's plausible answer instead.

It's all plausible answers, captions, VLM answer/caption labels, and the world knowledge the LLM encodes in itself, that contribute to the final decision of the language coordinator. The rightmost example presents the scenario of inconsistency between captions and answers. OFA describes the image as _"an elephant is loaded onto a truck in yangon."_ Though, it agrees that _"the truck is away from the elephant"_. With Cola-FT, The LLM coordinates OFA's correct caption and BLIP's correct answer to make a reasonable prediction.

Notably, we observe a scenario in which captions can be more informative than plausible answers to guide LLM. The right example (a puppy running) presents an uninformative image. Though neither OFA nor BLIP succeeds in answering the question, the LLM chooses to answer with "maybe" based on the given visual context. See Appendix A.5 and Appendix A.6 for more analysis on qualitative examples and failure cases.

Figure 2: **Qualitative examples. The correct choices are underlined. Leftmost: a commonsense question example of A-OKVQA; LLM follows the answer of BLIP. Left: a visual question example of A-OKVQA; LLM follows the answer of OFA. Right: an example of e-SNLI-VE; LLM chooses another option after coordination. Rightmost: an example of VSR; LLM predicts based on the caption of OFA and the answer of BLIP. Cola-Zero answers are referenced in zero-shot settings. The bottom row, Cola-FT (swapped VLM answer labels), indicates that the LLM follows the answer of certain VLMs based on their separate functionalities. LLM answers are associated with the distribution of VLM answer labels.**

### Coordination Analysis

Overall, Figure 3 validates the efficacy of Cola to coordinate VLMs. All the experiments use the same prompt template as in Table 1 unless otherwise stated. On A-OKVQA validation set, the performance of a single VLM (w/o FLAN-T5) is 50.83% for BLIP, or 54.75% for OFA. To validate the effectiveness of multi-VLM collaboration, we first ablate single-VLM variants of Cola-FT, shown as #1 (OFA only, without BLIP) and #2 (OFA only, without OFA) from the top. As expected, both fall behind Cola-FT (#8) by a large margin. With both VLMs, we ablate VLMs' captions (#3) and VLMs' plausible answers (#4), which reveal that plausible answers are much more significant in helping the LLM answer visual reasoning questions. Next, we perturb caption labels by swapping the VLM caption labels at instruction tuning and evaluation (#5), specifically _"OFA's description: "_ and _"BLIP's description: "_, by a chance of 50%. Under such settings, the LLM fails to acquire the preferred patterns of VLM for captioning, though the overall visual context is preserved. The results underperform Cola-FT, which verifies that VLM caption labels improve Cola-FT performance. Notably, the VLM (plausible) answer labels are more important to the LLM's decision: a considerable gap exists between (#6) and Cola-FT. In #6, the LLM fails to learn the separate functionalities of VLM when answer labels are perturbed. This highlights that the performance gained from the _coordination_ between BLIP and OFA, but not the strong reasoning capabilities of the LLM, FLAN-T5.

Naturally, we ask _what if the LLM can learn the patterns each VLM answers, but they cannot apply it at inference?_ We input correct VLM answer labels at instruction tuning and swap labels at evaluation (#7). Consequently, #7 falls behind Cola-FT with a smaller but still considerable margin. The results suggest that learning and applying the separate functionalities of VLMs is important for the coordinator LLM to make predictions. See Appendix A.9 for more ablation studies.

### Scaling Cola with More VLMs.

By decoding the top-k (k=5) results from three identical (OFA-base) models on the A-OKVQA validation set, both the answers and captions may exhibit slight variations. Cola demonstrated significant performance improvements compared to a single VLM or ensemble, as shown in Table 3. Furthermore, the performance gap between the ensemble baselines and Cola based on three different models (OFA-tiny/medium/base) is even more substantial, as depicted in Table 4.

\begin{table}
\begin{tabular}{l|c|c} \hline \hline
**Methods** & **A-OKVQA** & **e-SNLI-VE** \\ \hline OFA-base (1) & 45.76 & 52.60 \\ OFA-base (2) & 46.07 & 51.70 \\ OFA-base (3) & 45.73 & 52.33 \\ \hline Ensemble (majority voting) & 44.79 & 52.71 \\ Ensemble (average) & 46.04 & 52.25 \\ \hline
**Cola-Zero (2-shot)** & 47.71 & 54.42 \\
**Cola-FT** & 48.85 & 56.92 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Performance of ensemble methods based on three identical models.**

Figure 3: **Ablation study results** using a single VLM (#1, #2 from top), VLMs’ plausible answers only (w/o captions, #3), VLMs’ captions (w/o plausible answers, #4), perturbed VLM caption/answer labels at instruction tuning (#5, #6), and swapped answer labels at evaluation (#7). In #6, the coordination prior cannot be learned by the LLM. In #7, the coordination prior can be learned by the LLM, but cannot be properly applied at evaluation. FT: instruction tuning; Eval: evaluation.

\begin{table}
\begin{tabular}{l|c|c} \hline \hline
**Methods** & **A-OKVQA** & **e-SNLI-VE** \\ \hline OFA-base (1) & 45.76 & 52.60 \\ OFA-base (2) & 46.07 & 51.70 \\ OFA-base (3) & 45.73 & 52.33 \\ \hline Ensemble (majority voting) & 44.79 & 52.71 \\ Ensemble (average) & 46.04 & 52.25 \\ \hline
**Cola-Zero (2-shot)** & 47.71 & 54.42 \\
**Cola-FT** & 48.85 & 56.92 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Performance of ensemble methods based on three different models.**We further scale the number of OFA-base and find that Cola-FT saturates at 5 VLMs (49.77%) and Cola-Zero (2-shot in-context learning) sutures at 3 VLMs (47.71%). We observe that long input harms the performance of Cola-Zero, which is negative for scaling with more VLMs (Figure 4). LMs with a larger context window size [12, 68, 9] are promising to further improve the performance of Cola, which we leave for future works.

Scaling Model Size.We conduct experiments on scaling the coordinator LLM size to see if there are ramifications when operating at a larger scale. Figure 6 reveals that Cola-FT performance increases as the LLM (FLAN-T5) model size increases. Notably, Cola-FT/small, with only 80M parameters, could achieve 65% MC accuracy on A-OKVQA validation set, which is far beyond our baseline methods (55%). Cola-Zero, under the in-context learning paradigm, achieves competitive performance when the model grows to a billion-parameter scale. This observation on Cola-Zero can be regarded as a proof-of-concept that potentially reveals Cola-Zero's emerging abilities (inherited from FLAN-T5 [16]) on visual reasoning tasks at a relatively large scale. Cola-FT is effective with small models, but Cola-Zero is an emerging ability on larger models only.

In-context Learning & Low-data Instruction Tuning.We conduct experiments on different data scales to verify Cola's performance varying from zero-shot to full-shot under in-context learning and full-finetune paradigm. As shown in Figure 6, with Cola-Zero, few-shot exemplars substantially improve performance compared to zero-shot learning. As [16, 108] revealed, exemplars potentially help the model better understand the output format and understand the instructions in Table 1. Cola-Zero for in-context few-shot learning outperforms zero-shot learning by a large margin, being on par with low-data Cola-FT without instruction tuning. We also observe Cola-FT's substantial performance gain when finetuning shots increase to 1000 and beyond.

### Saliency Visualization

As shown in Figure 7, we visualize the importance of the input prompt tokens by input-gradient saliency feature attribution [19], implementing with Ecco [2]. The input tokens that are more relevant to predict the output token "grass" are highlighted in darker colors. In the given example, both Cola-FT and Cola-Zero predict the correct answer and find the relevant clues from visual context and plausible answers. Figure 7(b) shows that Cola-Zero attributes the output more to the instructions in the prompt template. This explains Cola-Zero's competitive performance, a consequence of FLAN instruction tuning [108]. After instruction tuning, Cola-FT focuses more on the most informative parts of input: the question, choices, as well as VLMs' plausible answers.

### Can \(\overline{\rule{0.0pt}{1.0pt}\rule{1.0pt}{1.0pt}}\) Cola-FT Explain its Answers?

We modify the prompt template of Cola so that the model would output the logical conduction process, allowing us to observe the specific behaviors of the LLM during its coordination of VLMs.

Figure 4: **Scaling of # Models.**

Figure 5: **Cola performances versus the LLM (FLAN-T5) sizes.** Figure 6: **Low-data Cola-FT and Cola-Zero performances.**\(x\)-axis is distorted for optimal display.

We finetune Cola-FT to output rationales before answers, by A-OKVQA ground truth rationales. In the modified prompt, we ask the model to provide rationales. For the leftmost example of Figure 8, Cola-FT outputs _"Rationale: People might sit here to rest. The umbrellas are on the riverwalk. The answer is:rest "._ OFA gives a reasonable answer (but out of choices) to the question while BLIP gives an irrelevant answer. In this case, both answers are wrong. However, either Cola-Zero or Cola-FT is able to infer from captions and plausible answers to give the correct answer "to rest". The rationale suggests that the LLM understands the scene that the umbrellas are on the riverwalk and guesses that people might sit here to rest based on commonsense. The final answer is correct. For the leftmost example of Figure 11, the rationale output is _"The bike is parked in a no parking zone. The bike is parked next to a pedestrian crossing sign. The answer is:no parking"_. Both VLMs are wrong in their plausible answers. OFA's answer "boating" is semantically correct as the correct answer is "kayaking", though it's not the correct answer because this is a multiple-choice question. Cola-Zero gives a wrong answer "OFA" which is obviously wrong because "OFA" is the name of one of the VLMs given in the prompt and it's out of the choices too. However, Cola-FT gives the correct answer "kayaking", recognizing the correct choice based on prompts of captions and plausible answers after being finetuned. Even though the OFA and BLIP captions fail to identify that the people in the water are on a canoe. The LLM identifies that the people in the water are associated with the canoe. The rationale is valid and helpful, though repetitive. The final answer is correct.

To force the LLM to output rationale does not improve the reasoning performance of Cola (w/t rationale 74.3% vs. w/o rationale 77.7%, on A-OKVQA val set). This might be attributed to the low-quality ground truth rationales provided by the A-OKVQA dataset that we use to train the LLM. Such rationales are just short and objective descriptions of the scene, without suggesting the underlying outside knowledge to answer the question. Therefore, training the LLM to output rationale is harmful, though it derives insights into the LLM's behaviors during reasoning.

### Does \(\boldsymbol{\sharp}\) Cola-FT Transfer across Tasks?

We examine Cola-FT's generalization ability across tasks. From Table 5, we observe the zero-shot performances on target datasets after instruction tuning on a certain source dataset. Although each dataset varies in question types and prompt templates (see detailed comparisons in Appendix A.7), we find that Cola-FT maintains competitive performance when zero-shot transferred to a new task, outperforming Cola-Zero in-context 2-shot learning and ensemble baselines (see also Table 2).

## 4 Related Work

**Visual Reasoning.** Beyond unimodal reasoning tasks such as question answering (QA) [100; 14; 119; 7], visual reasoning extends high-level cognition to visual domains, requiring an intelligent agent to derive rational solutions [40; 35; 84; 120; 38; 112]. Several tasks have been introduced to address

Figure 7: **Visualization of input token saliency. We visualize the relevancy between input tokens and the output token "grass" by feature attribution [19]. The more salient tokens are highlighted in darker boxes. Cola-FT focuses on the question, choices, and VLMs’ plausible answers in (a). While as shown in (b), Cola-Zero pays extra attention to instructions and VLM labels, as a consequence of FLAN-T5 instruction tuning [16].**

visual reasoning, such as VQA [1], in which models are expected to provide answers to questions related to an image, and visual entailment [113], where the model is required to determine if a text description is consistent with the visual content provided.

Classic visual reasoning methods employ an image encoder along with a reasoning block that utilizes attention mechanisms [102, 121, 122, 105], neuro-symbolic methods [101, 117, 61], or external knowledge [62, 28, 13].

Recent progress in large pretrained models has led to the development of LLMs that capture exceptional commonsense reasoning capabilities [78, 16, 15]. These LLMs can potentially replace the reasoning module in visual reasoning tasks, and LLMs' lack of perception can be compensated by incorporating multiple VLMs trained on different domains [76, 104, 52]. However, there is still a lack of research on how to harness the collective power of these separate VLMs for visual reasoning tasks. More related works are in Appendix B.

**Model Ensemble.** Model ensemble is a powerful machine learning technique that combines the predictions of multiple models to improve the overall performance of a given task [20]. The variance and bias of the final predictions decrease, resulting in a more robust and accurate model [83]. To this end, common methods include averaging [11], voting [37], interpolation [36], weighting the predictions based on model performance [22], or stacking the models [10].

Ensemble methods have been challenging for _generative_ tasks like visual reasoning, where a simple combination is not applicable to heterogeneous models due to their enormous and varying input/output token spaces. To address the issue, Socratic Models (SMs) [123] use prompt engineering to guide the heterogeneous pretrained multimodal models through natural language discussions. With a similar goal, [54] proposes a closed-loop iterative consensus optimization method to utilize the strengths of individual models. However, previous methods do not fully adapt to the intrinsic patterns of different models, particularly in the visual reasoning scenario. Recent studies, such as CICERO [65], have shown that LLMs possess strong social intelligence in coordinating multiple agents, which inspires us to reorganize pretrained mixed-modal models with a focus on adapting LLMs. More recently, Toolformer [86] and HuggingGPT [93] further demonstrate LLMs' abilities to leverage, coordinate, and incorporate the results from external sources such as other models or even APIs to solve complex tasks. While the external tools are called in sequential order in existing work, we study coordinating multiple tools (specifically, expert models) in parallel in this work.

## 5 Discussion

**Question Format.** Datasets like VQA v2 and OK-VQA contain open-ended questions, while A-OKVQA, e-SNLI-VE, and VSR use multiple-choice. Converting VQA v2 and OK-VQA to classification introduces complexities for traditional ensemble methods, as evident in Table 2. Classic methods struggle with generative models like API-based GPT-4, underscoring Cola's value as an end-to-end ensemble strategy for extensive (vision-)language models. Moreover, Cola-Zero's efficiency also relies on the question format - it's easier for LLMs to answer when given choices like in A-OKVQA. Conversely, Cola-FT finetunes LLMs to discern answer formats (Figure 7).

**Limitations.** Visual reasoning is a diverse topic. This work demonstrates the first step toward applying end-to-end language models for visual reasoning. While the LLMs perform well on the discussed datasets, there is a large body of visual reasoning tasks to evaluate in future works, such as intention prediction and rationale explanation.

**Future Works.** First, exploring the use of non-parametric tools for visual reasoning would be useful to enhance Cola's performance. Second, Cola's use can be extended to other reasoning and planning tasks, such as image generation and action planning, by coordinating multiple models in parallel. Third, by improving inter-model communications, Cola can be more interpretable and safe for high-stakes applications.

**Conclusion.** In this paper, we have proposed a novel paradigm for visual reasoning that harnesses the power of multiple VLMs by utilizing a coordination mechanism, where an LLM acts as a coordinator who communicates with VLMs to integrate their respective strengths. Experiments show that reasoning performance is substantially improved by LLM finetuning or in-context learning. Our results provide a promising step towards building multi-component intelligent systems that capture multimodal reasoning capabilities in a human-like way.

## Acknowledgements

This research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2022-01-029). Besides, this project is supported by NTU NAP, MOE AcRF Tier 2 (MOE-T2EP2021-0012), and under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as advise from the industry partner(s).

## References

* [1]A. Agrawal, J. Lu, S. Antol, M. Mitchell, C. Lawrence Zitnick, Devi Parikh, and D. Batra (2015) Vqa: visual question answering. International Journal of Computer Vision123, pp. 4-31. Cited by: SS1.
* [2]J. Alammar (2021-01) Ecco: an open source library for the explainability of transformer language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, Online, pp. 249-257. External Links: Link, Document Cited by: SS1.
* [3]J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. (2022) Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems35, pp. 23716-23736. Cited by: SS1.
* [4]S. Amizadeh, H. Palangi, A. Polozov, Y. Huang, and K. Koishida (2020) Neuro-symbolic visual reasoning: disentangling. In International Conference on Machine Learning, pp. 279-290. Cited by: SS1.
* [5]J. R. Anderson, C. Franklin Boyle, and B. J. Reiser (1985) Intelligent tutoring systems. Science228 (4698), pp. 456-462. Cited by: SS1.
* [6]R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. (2021) On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. Cited by: SS1.
* [7]A. Bondarenko, M. Wolska, S. Heindorf, L. Blubaum, A. Ngonga Ngomo, B. Stein, P. Braslavski, M. Hagen, and M. Potthast (2022) Causalqa: a benchmark for causal question answering. In ACL, Cited by: SS1.
* [8]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.
* [9]A. Bulatov, Y. Kuratov, and M. Burtsev (2022) Recurrent memory transformer. Advances in Neural Information Processing Systems35, pp. 11079-11091. Cited by: SS1.
* [10]A. Chatzimparmpas, R. M. Martins, K. Kucher, and A. Kerren (2020) Stack-genvis: alignment of data, algorithms, and models for stacking ensemble learning using performance metrics. IEEE Transactions on Visualization and Computer Graphics27 (2), pp. 1547-1557. Cited by: SS1.
* [11]M. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. (2021) Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Cited by: SS1.
* [12]S. Chen, S. Wong, L. Chen, and Y. Tian (2023) Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595. Cited by: SS1.
* [13]Z. Chen, Y. Huang, J. Chen, Y. Geng, Y. Fang, J. Z. Pan, N. Zhang, and W. Zhang (2022) LAKO: knowledge-driven visual question answering via late knowledge-to-text injection. ArXivabs/2207.12888. Cited by: SS1.
* [14]E. Choi, D. Hewlett, J. Uszkoreit, I. Polosukhin, A. Lacoste, and J. Berant (2017) Coarse-to-fine question answering for long documents. In ACL, Cited by: SS1.

* [15] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* [16] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.
* [17] Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. Tydi qa: A benchmark for information-seeking question answering in typologically diverse languages. _Transactions of the Association for Computational Linguistics_, 2020.
* [18] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. _ArXiv_, abs/2305.06500, 2023.
* [19] Misha Denil, Alban Demiraj, and Nando De Freitas. Extraction of salient sentences from labelled documents. _arXiv preprint arXiv:1412.6815_, 2014.
* [20] Thomas G Dietterich. Ensemble methods in machine learning. In _International workshop on multiple classifier systems_, pages 1-15. Springer, 2000.
* [21] Virginie Do, Oana-Maria Camburu, Zeynep Akata, and Thomas Lukasiewicz. e-snli-ve: Corrected visual-textual entailment with natural language explanations. _arXiv preprint arXiv:2004.03744_, 2020.
* [22] Alican Dogan and Derya Birant. A weighted majority voting ensemble approach for classification. In _2019 4th International Conference on Computer Science and Engineering (UBMK)_, pages 1-6, 2019.
* [23] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5: Long form question answering. _arXiv preprint arXiv:1907.09190_, 2019.
* [24] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. _arXiv preprint arXiv:2012.15723_, 2020.
* [25] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. _Transactions of the Association for Computational Linguistics_, 2021.
* [26] Dan Goldwasser and Dan Roth. Learning from natural instructions. _Machine learning_, 94(2):205-232, 2014.
* [27] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6904-6913, 2017.
* [28] Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, and Jianfeng Gao. Kat: A knowledge augmented transformer for vision-and-language. _arXiv preprint arXiv:2112.08614_, 2021.
* [29] Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, and Steven CH Hoi. From images to textual prompts: Zero-shot vqa with frozen large language models. _arXiv preprint arXiv:2212.10846_, 2022.
* [30] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. _ArXiv_, abs/2211.11559, 2022.
* [31] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei. Language models are general-purpose interfaces. _arXiv preprint arXiv:2206.06336_, 2022.
* [32] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo. Promptcap: Prompt-guided task-aware image captioning. _arXiv preprint arXiv:2211.09699_, 2022.
* [33] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. _arXiv preprint arXiv:2302.14045_, 2023.
** [34] Ziqi Huang, Hongyuan Zhu, Ying Sun, Dongkyu Choi, Cheston Tan, and Joo-Hwee Lim. A diagnostic study of visual question answering with analogical reasoning. In _2021 IEEE International Conference on Image Processing (ICIP)_, pages 2463-2467. IEEE, 2021.
* [35] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6700-6709, 2019.
* [36] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. _arXiv preprint arXiv:2208.05592_, 2022.
* [37] Anmol Jain, Aishwary Kumar, and Seba Susan. Evaluating deep neural network ensembles by majority voting cum meta-learning scheme. In _Soft Computing and Signal Processing: Proceedings of 3rd ICSCSP 2020, Volume 2_, pages 29-37. Springer, 2022.
* [38] Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert D Hawkins, and Yoav Artzi. Abstract visual reasoning with tangram shapes. In _EMNLP_, 2022.
* [39] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* [40] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2901-2910, 2017.
* [41] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In _ACL_, 2017.
* [42] Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do, Zeynep Akata, and Thomas Lukasiewicz. e-vil: A dataset and benchmark for natural language explanations in vision-language tasks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1244-1254, 2021.
* [43] Daniel Khashabi, Yeganeh Kordi, and Hannaneh Hajishirzi. Unifiedqa-v2: Stronger generalization via broader cross-format training. _arXiv preprint arXiv:2202.12359_, 2022.
* [44] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In _International Conference on Machine Learning_, pages 5583-5594. PMLR, 2021.
* [45] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In _International Conference on Machine Learning_, pages 6265-6274. PMLR, 2021.
* [46] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. _arXiv preprint arXiv:1910.13461_, 2019.
* [47] Bo Li, Yifei Shen, Jingkang Yang, Yezhen Wang, Jiawei Ren, Tong Che, Jun Zhang, and Ziwei Liu. Sparse mixture-of-experts are domain generalizable learners. _arXiv preprint arXiv:2206.04046_, 2022.
* [48] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning. _arXiv preprint arXiv:2306.05425_, 2023.
* [49] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning, 2023.
* [50] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. _arXiv preprint arXiv:2309.10020_, 2023.
* [51] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.

* [52] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. _arXiv preprint arXiv:2201.12086_, 2022.
* [53] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. _arXiv preprint arXiv:1908.03557_, 2019.
* [54] Shuang Li, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, and Igor Mordatch. Composing ensembles of pre-trained models via iterative consensus. _ArXiv_, abs/2210.11522, 2022.
* [55] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _European conference on computer vision_, pages 740-755. Springer, 2014.
* [56] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. _arXiv preprint arXiv:2205.00363_, 2022.
* [57] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. _arXiv preprint arXiv:2205.05638_, 2022.
* [58] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. _arXiv preprint arXiv:2304.09842_, 2023.
* [59] Mikolaj Makinski and Jacek Mandziuk. Deep learning methods for abstract visual reasoning: A survey on raven's progressive matrices. _arXiv preprint arXiv:2201.12382_, 2022.
* [60] Mikolaj Makinski and Jacek Mandziuk. A review of emerging research directions in abstract visual reasoning. _arXiv preprint arXiv:2202.10284_, 2022.
* [61] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. The neuro-symbolic concept learner: Interpreting scenes words and sentences from natural supervision. _ArXiv_, abs/1904.12584, 2019.
* [62] Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach. Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based vqa. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14111-14121, 2021.
* [63] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In _Proceedings of the IEEE/cvf conference on computer vision and pattern recognition_, pages 3195-3204, 2019.
* [64] John McCarthy et al. _Programs with common sense_. RLE and MIT computation center Cambridge, MA, USA, 1960.
* [65] Fundamental AI Research Diplomacy Team Meta, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. _Science_, 2022.
* [66] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context. _arXiv preprint arXiv:2110.15943_, 2021.
* [67] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. _arXiv preprint arXiv:2104.08773_, 2021.
* [68] Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens. _arXiv preprint arXiv:2304.08467_, 2023.
* [69] Hyacinth S Nwana. Intelligent tutoring systems: an overview. _Artificial Intelligence Review_, 4(4):251-277, 1990.
* [70] OpenAI. Gpt-4 technical report. 2023.
* [71] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _arXiv preprint arXiv:2203.02155_, 2022.

* [72] Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, and Yejin Choi. Visualacomet: Reasoning about the dynamic context of a still image. In _European Conference on Computer Vision_, 2020.
* [73] Silviu Pitis, Michael R Zhang, Andrew Wang, and Jimmy Ba. Boosted prompt ensembles for large language models. _arXiv preprint arXiv:2304.05970_, 2023.
* [74] Bjorn Pluster, Jakob Ambsdorf, Lukas Braach, Jae Hee Lee, and Stefan Wermter. Harnessing the power of multi-task pretraining for ground-truth level natural language explanations. _arXiv preprint arXiv:2212.04231_, 2022.
* [75] Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W Bruce Croft, and Mohit Iyyer. Open-retrieval conversational question answering. In _ACM SIGIR_, 2020.
* [76] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* [77] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. _arXiv preprint arXiv:2112.11446_, 2021.
* [78] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21(140):1-67, 2020.
* [79] Nazneen Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. In _ACL_, 2019.
* [80] Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering challenge. _Transactions of the Association for Computational Linguistics_, 2019.
* [81] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics, 11 2019.
* [82] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. _Advances in Neural Information Processing Systems_, 34:8583-8595, 2021.
* [83] Omer Sagi and Lior Rokach. Ensemble learning: A survey. _Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery_, 8(4):e1249, 2018.
* [84] Shailaja Keyur Sampat, Maitreya Patel, Subhasish Das, Yezhou Yang, and Chitta Baral. Reasoning about actions over visual and linguistic modalities: A survey. _arXiv preprint arXiv:2207.07568_, 2022.
* [85] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. _arXiv preprint arXiv:2110.08207_, 2021.
* [86] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. _arXiv preprint arXiv:2302.04761_, 2023.
* [87] Timo Schick and Hinrich Schutze. Exploiting cloze questions for few shot text classification and natural language inference. _arXiv preprint arXiv:2001.07676_, 2020.
* [88] Benedikt Schmidt, Reuben Borrison, Andrew Cohen, Marcel Dix, Marco Gartler, Martin Hollender, Benjamin Klopper, Sylvia Maczey, and Shumuuga Siddharthan. Industrial virtual assistants: Challenges and opportunities. In _Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers_, pages 794-801, 2018.
* [89] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. _arXiv preprint arXiv:2206.01718_, 2022.

* [90] Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large language models with answer heuristics for knowledge-based visual question answering. _arXiv preprint arXiv:2303.01903_, 2023.
* [91] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. _arXiv preprint arXiv:1701.06538_, 2017.
* [92] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In _International Conference on Machine Learning_, pages 4596-4604. PMLR, 2018.
* [93] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. _arXiv preprint arXiv:2303.17580_, 2023.
* [94] Derek Sleeman and John Seely Brown. _Intelligent tutoring systems_. London: Academic Press, 1982.
* [95] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.
* [96] Md Arafat Sultan, Shubham Chandel, Ramon Fernandez Astudillo, and Vittorio Castelli. On the importance of diversity in question generation for qa. In _ACL_, 2020.
* [97] Didac Suris, Sachit Menon, and Carl Vondrick. Vibergpt: Visual inference via python execution for reasoning. _arXiv preprint arXiv:2303.08128_, 2023.
* [98] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. _arXiv preprint arXiv:1908.07490_, 2019.
* [99] Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, and Steven CH Hoi. Plug-and-play vqa: Zero-shot vqa by conjoining large pretrained models with zero training. _arXiv preprint arXiv:2210.08773_, 2022.
* [100] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. Newsqa: A machine comprehension dataset. In _Rep4NLP@ACL_, 2016.
* [101] Aisha Urooj, Hilde Kuehne, Bo Wu, Kim Chheu, Walid Bousselham, Chuang Gan, Niels Lobo, and Mubarak Shah. Learning situation hyper-graphs for video question answering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14879-14889, 2023.
* [102] Mohit Vaishnav and Thomas Serre. Gamr: A guided attention model for (visual) reasoning. In _The Eleventh International Conference on Learning Representations_, 2022.
* [103] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: Lessons learned from the 2015 mscoco image captioning challenge. _IEEE transactions on pattern analysis and machine intelligence_, 39(4):652-663, 2016.
* [104] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In _International Conference on Machine Learning_, pages 23318-23340. PMLR, 2022.
* [105] Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. VLmo: Unified vision-language pre-training with mixture-of-modality-experts. _ArXiv_, abs/2111.02358, 2021.
* [106] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented ensembles in language models. _arXiv preprint arXiv:2207.00747_, 2022.
* [107] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. _arXiv preprint arXiv:2203.11171_, 2022.
* [108] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_, 2021.

* [109] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022.
* [110] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Marianna Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online, October 2020. Association for Computational Linguistics.
* [111] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 23965-23998. PMLR, 17-23 Jul 2022.
* [112] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. Star: A benchmark for situated reasoning in real-world videos. In _Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)_, 2021.
* [113] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: A novel task for fine-grained image understanding. _ArXiv_, abs/1901.06706, 2019.
* [114] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. _arXiv preprint arXiv:2111.02080_, 2021.
* [115] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. In _AAAI_, 2022.
* [116] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B. Tenenbaum. Clever: Collision events for video representation and reasoning. _ArXiv_, abs/1910.01442, 2019.
* [117] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B. Tenenbaum. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. _ArXiv_, abs/1810.02338, 2018.
* [118] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6281-6290, 2019.
* [119] Munazza Zaib, Wei Emma Zhang, Quan Z Sheng, Adnan Mahmood, and Yang Zhang. Conversational question answering: A survey. _Knowledge and Information Systems_, 2022.
* [120] Rufai Yusuf Zakari, Jim Wilson Owusu, Hailin Wang, Ke Qin, Zaharaddeen Karami Lawal, and Yuezhou Dong. Vqa and visual reasoning: An overview of recent datasets, methods and challenges. _arXiv preprint arXiv:2212.13296_, 2022.
* [121] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6713-6724, 2018.
* [122] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. In _Neural Information Processing Systems_, 2021.
* [123] Andy Zeng, Adrian S. Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael S. Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Peter R. Florence. Socratic models: Composing zero-shot multimodal reasoning with language. _ArXiv_, abs/2204.00598, 2022.
* [124] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.

* [125] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _arXiv preprint arXiv:2306.05685_, 2023.
* [126] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. _Advances in Neural Information Processing Systems_, 35:7103-7114, 2022.
* [127] Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. Retrieving and reading: A comprehensive survey on open-domain question answering. _arXiv preprint arXiv:2101.00774_, 2021.

Experimental Details

In this section, we elaborate on our training and evaluation details, prompt templates, and more qualitative examples for analysis.

### Datasets

Our experiments are conducted on a challenging suite of three diverse visual reasoning tasks, including outside knowledge VQA, visual entailment, and visual spatial reasoning. For each task, we select the following dataset respectively.

Visual Question Answering v2[27] (VQA v2) is a large-scale benchmark containing over 1 million images from the COCO dataset and more than 250,000 human-generated question-answer pairs. The dataset is designed to test the ability of machine learning models to understand both the visual content of an image and the meaning behind natural language questions. The questions in VQA v2 cover a wide range of topics and are often open-ended, requiring models to reason and generalize about the world. VQA v2 has been widely used to evaluate the performance of state-of-the-art models in the field of computer vision and natural language processing.

Augmented Outside Knowledge VQA[89] (A-OKVQA) contains about 25k questions paired with both multiple choice (MC) answer options. Unlike most existing VQA datasets, the questions in A-OKVQA cannot often be answered by querying the knowledge base, but rather involve some type of commonsense reasoning and outside knowledge about the situation portrayed in the image.

Outside Knowledge VQA[63] (OK-VQA) includes more than 14,000 questions that require external knowledge to answer. The answers are provided in free-text direct answer form. Both A-OKVQA and OK-VQA sample images from the COCO dataset, with no overlapping.

e-SNLI-VE[21] dataset is an extended version of SNLI-VE dataset [113], which contains about 190k question pairs and human-annotated natural language explanations for the ground-truth labels. The text premise provides a statement about the contents of the image. The task is to determine whether the statement is true or false based on the image content.

Visual Spatial Reasoning[56] (VSR) consists of 65 spatial relations (_e.g._, under, in front of, facing, _etc._) of instances in images. VSR has more than 10k question pairs, associated with 6940 images from MS COCO [55].

Gqa[35] dataset consists of 22M questions about various day-to-day images. The questions are about compositional question answering based on scene graphs. In our evaluation, we only use the text of the question as model input, but not the scene graphs.

Compositional Language and Elementary Visual Reasoning[40] (CLEVR) is a synthetic dataset with questions that test various aspects of visual reasoning including attribute identification, counting, comparison, spatial relationships, and logical operations. The dataset contains 700k questions in the training set and 150k in the validation set.

### Instruction Tuning Details

We adopt pretrained BLIP [52]1 and OFA [104]2 as VLMs unless specified otherwise, and freeze their parameters without updating. The instruction tuning only happens on the language model part. The training set of each dataset is used for finetuning. We use the whole training set unless otherwise specified in the low-data instruction tuning discussion.

Footnote 1: BLIP: [https://github.com/salesforce/BLIP](https://github.com/salesforce/BLIP)

Footnote 2: OFA: [https://huggingface.co/OFA-Sys](https://huggingface.co/OFA-Sys)

We use an AdaFactor optimizer [92] at the learning rate of 1e-4 for all Cola-FT experiments. The batch size is by default set to 16, though we find Cola-FT insensitive to batch size. We finetune and evaluate the models on NVIDIA V100 or A100 GPUs. The finetuning time is shown in Table 6.

Following the common experiment protocols, we employ a teacher forcing and greedy decoding strategy for fine-tuning.

### Evaluation Details

As specified, we use the validation or test set multiple choice accuracy as the evaluation metric. In A-OKVQA, we report val/test accuracy, and val accuracy in e-SNLI-VE, test (zero-shot split) accuracy in VSR. For simplicity and consistency, we evaluate ablation experiments on A-OKVQA validation set. Following the common experiment protocols [32; 74], we report the single run results for performance comparison.

The exemplars at the inference of Cola-Zero are randomly sampled from the training set, i.e., supposedly help the LLM learn the input data distribution and output format but do not leak relevant information to the evaluation question.

### A-OKVQA Direct Answer Results

In addition to MC accuracy, we present the direct answer (DA) accuracy of models on the A-OKVQA validation set in Tables 7 and 8.

### Qualitative Examples

In this section, we provide more qualitative examples on A-OKVQA (Figure 8), e-SNLI-VE (Figure 9), and VSR (Figure 10) datasets.

Due to the large span of the three figures, for better visibility, we put the detailed description directly in each figure's caption part. We illustrate how Cola-FT and Cola-Zero process the VLMs answers in each example. Overall, in these examples, we can observe that even if BLIP and OFA provide wrong answers, Cola can still present the correct answer based on the captions provided by OFA and BLIP, as well as the choice set. This may illustrate how Cola amazingly accomplishes visual reasoning tasks via coordinating BLIP and OFA.

### Failure Cases

In Figure 11, we provide a few failed cases to analyze the specific behavior of Cola.

The leftmost example's correct answer is _kayaking_, but there are no hints from OFA and BLIP's answers and captions. Therefore Cola-Zero incorrectly provides the answer _OFA_ without sufficient information as hints, while surprisingly Cola-FT answered correctly from OFA's _boating_ answer.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline V100 hours & A-OKVQA & e-SNLI-VE & VSR & GQA & VQA v2 & OK-VQA & CLEVR \\ \hline Cola-FT & 12 & 8 & 8 & 24 & 80 & 12 & 24 \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Cola-FT training time of FLAN-T5-XXL for each dataset.** We finetune a subset of GQA.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & FLAN-T5-Small & FLAN-T5-Base & FLAN-T5-XL & FLAN-T5-XXL \\ \hline Cola-FT & 56.5 & 60.6 & 64.1 & 65.4 \\ Cola-Zero (2-shot) & 30.3 & 34.6 & 57.6 & 61.0 \\ Cola-Zero (0-shot) & 28.6 & 36.0 & 55.0 & 59.3 \\ \hline \hline \end{tabular}
\end{table}
Table 7: **A-OKVQA validation set DA performance.** Extension of Figure 5.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & 1-shot & 2-shot & 3-shot & 4-shot \\ \hline Cola-Zero & 60.2 & 61.0 & 60.7 & 59.2 \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Cola-Zero in-context few-shot learning DA performance on A-OKVQA validation set.** Extension of Figure 6.

The left example again has insufficient information from captions. While BLIP answers _no_ and OFA answers _yes_, Cola-FT chooses to answer _maybe_, which looks natural but unfortunately picks the wrong choice.

The right example's captions contain enough information this time. But both Cola-FT and Cola-Zero are misled by BLIP's wrong answer _no parking_.

The rightmost example also has insufficient information from captions. In this situation, Cola has no choice but to believe either BLIP or OFA's answer, but it mistakenly prefers BLIP's wrong answer.

### Prompt Templates

Across three datasets, the prompt template is roughly the same, with minor differences mainly in the format of the questions and choices. We list the prompt templates adopted in A-OKVQA and e-SNLI-VE/VSR in Table 9 and Table 10, respectively.

### Parameter-efficient Finetuning

To further reduce the computation cost in model adaptation, we explored parameter-efficient finetuning (PEFT) techniques to reduce finetuning parameter counts. Specifically, we use (IA)3[57], which finetunes an overhead of 1 million parameters, equivalent to 0.01% of the full parameters of FLAN-T5-XXL.

Compared to full finetuning, (IA)3 requires more iterations to converge. The performance of a (IA)3 finetuned FLAN-T5-XXL model is on par with a fully finetuned FLAN-T5-Small (80 million parameters) counterpart (Figure 5). Notably, the former is associated with more computation and memory footprint as a consequence of more parameters in the forward pass.

Figure 8: **A-OKVQA qualitative examples.** Leftmost: LLM doesn’t use BLIP and OFA’s answers, but may observe from captions to derive the correct final answer. Left: As shown on the left, LLM does not follow the wrong answers from OFA and BLIP but gets the correct answers from captions. Right: With both OFA and BLIP answering incorrectly, LLM derives the correct one from both VLMs’ captions and answers. Rightmost: After assessing the questions, answers, and captions, LLM goes with OFA’s answer and rewrites it to match the expression in the choices. The correct choices are underlined. Cola-Zero answers are given in zero-shot settings.

### Extended Ablation Studies

**Do caption labels offer useful information to LLM? How would more prompt variations affect the performance of Cola?** We tested Cola-Zero with and without caption labels on A-OKVQA validation set, observing a slight decrease in performance when without them (70.39% w/t vs. 69.97% w/o). More ablative experiments showed that removing the VLM's answer labels led to a substantial drop in performance (70.39% w/t vs. 67.62% w/o). Removing the model characteristic descriptions also led to a decrease (70.39% w/t vs. 68.37% w/o).

**Do longer image captions improve reasoning performance?** On A-OKVQA validation set, we tested longer image descriptions (>50 tokens) but found no gain compared to Cola or single VLMs. Longer captions decreased FLAN-T5+OFA's accuracy by 0.61% and FLAN-T5 with BLIP by 0.69% on the A-OKVQA validation set. Cola (captions <30 tokens) reached 77.73%, outperforming individual VLMs. Longer captions lacked meaningful visual context, possibly due to short text and image pairs in their training datasets. This experiment reaffirms Cola's effectiveness in aggregating individual VLM functionalities.

## Appendix B Extended Related Works

### Finetuning Large Language Models

Large language models [8, 71, 6] pretrained on massive amounts of unstructured data have gradually demonstrated great performance by finetuning on additional task-specific instances. Finetuning a large language model can be considerably more sample efficient than re-training from scratch, although acceptable performance may still require a considerable quantity of data [95]. Recent

Figure 9: **e-SNLI-VE qualitative examples.** Leftmost: As the connection to _daredevil_ is not obvious in BLIP and OFA’s captions, although Cola-Zero is misled, Cola-FT correctly answers _maybe_. Left: Similar to the left example, Cola-FT answer correctly as no obvious connections are seen from the captions to this question. Right: Similar to the left example, the fact of _catch catfish_ is not reasonable from the captions, Cola-FT picks the correct answer _maybe_. Rightmost: As _girl gets hit_ is not obvious in BLIP and OFA’s captions and answers, Cola-Zero and Cola-FT both follow BLIP to choose the correct answer _no_. The correct choices are underlined. Cola-Zero answers are given in zero-shot settings.

works have finetuned task-specific models that demonstrate amazing capabilities in many real-world applications, such as Copilot for program synthesis [11].

Figure 11: **Failed cases. The correct choices are underlined. Cola-Zero answers are given in zero-shot settings.**

Figure 10: **VSR qualitative examples. Leftmost: As OFA caption mentioned _elephant being transported_ and OFA provides the correct answer, Cola-FT follows OFA’s choice. Left: As OFA and BLIP provide the same answer, Cola-Zero and Cola-FT follow the choice. Right: As the captions do not provide obvious information, even BLIP and OFA provide the same answer, Cola-Zero and Cola-FT are not misled to the wrong choice. Rightmost: As the captions provide strong clue _bananas in a bowl_, although BLIP’s answer is incorrect, Cola-Zero and Cola-FT still choose the correct answer. The correct choices are underlined. Cola-Zero answers are given in zero-shot settings.**

### Instruction-based Learning

Recent advances in the capabilities of language models have piqued researchers' curiosity in the field of instruction-based learning [26, 64, 87, 24]. The core of instruction-based learning is to explore the knowledge of the language model itself. In contrast to prompt learning to stimulate the language model's ability to complete blanks, instruction tuning more focuses on activating the language model's comprehension by giving obvious instructions to models and expecting correct feedback. Earlier work [67] finetune BART [46] using instructions and few-shot exemplars in question answering, text classification, and text modification. Their findings suggest that few-shot instruction tuning improves performance on unseen tasks. [66] finetunes GPT-2 Large and also observes that few-shot exemplar instruction tuning could improve performance. [85] finetunes T5-11B with more diverse instruction templates and observe similar improvements in zero-shot learning. More recent work [108] performs large-scale experiments with a 137B FLAN-T5 model and instruction-tune it on over 60 datasets verbalized via instruction templates. They observe FLAN-T5 substantially improves over zero-shot GPT-3 (175B) on 20 of 25 evaluation datasets. OpenAI also releases InstructGPT [71] based on GPT-3 [8], it makes use of human annotations to steer desired model behavior through both instruction

\begin{table}
\begin{tabular}{l} \hline \hline
**vQA Prompt Template** \\ \hline Answer the following multiple-choice question by OFA and BLIP’s description and their answers to the visual question. OFA and BLIP are two different vision-language models to provide clues. \\ OFA’s description: \(\lessapprox\)OFA captions\(\lessapprox\)BLIP captions\(\lessapprox\)Q: does the image describe \(\lessapprox\)hypothesis\(\lessapprox\)? \\ OFA’s answer: \(\lessapprox\)BAIP answers\(\lessapprox\)e-SNLI-VE Choices: [yes, no, maybe] \\ VSR Choices: [yes, no] \\ A: \\ \hline \hline \end{tabular}
\end{table}
Table 10: **e-SNLI-VE/VSR prompt template for the LLM.** The LLM is instructed to coordinate VLMs. Each question set defines _visual context_, _hypothesis_, and _plausible answers_.

\begin{table}
\begin{tabular}{l} \hline \hline
**vQA Prompt Template** \\ \hline Answer the following multiple-choice question by OFA and BLIP’s description and their answers to the visual question. OFA and BLIP are two different vision-language models to provide clues. \\ OFA’s description: \(\lessapprox\)OFA captions\(\lessapprox\)BLIP’s description: \(\lessapprox\)BLIP captions\(\lessapprox\)Q: \(\lessapprox\)Question\(\lessapprox\)Q: \(\lessapprox\)Question\(\lessapprox\)OFA answers\(\lessapprox\)BLIP answers\(\lessapprox\) \\ Choices: \(\lessapprox\)Choices to the questions\(\lessapprox\)A: \\ A: \\ \hline \hline \end{tabular}
\end{table}
Table 9: **VQA prompt template for the LLM, for VQA v2 / OK-VQA / A-OKVQA.** The LLM is instructed to coordinate VLMs. Each question set defines _visual context_, _question with choices_, and _plausible answers_.

tuning and reinforcement learning of human feedback. They discover that InstructGPT is favored by humans over unmodified GPT-3.

### Visual Reasoning

Beyond the uni-modal reasoning tasks such as question answering (QA) [100, 41, 14, 80, 79, 23, 75, 17, 96, 25, 127, 119, 7], visual reasoning requires models to not only understand and interpret visual information but also to apply high-level cognition to derive rational solutions [40, 35, 4, 59, 60, 84, 120, 34]. Several tasks have been introduced to address visual reasoning, such as visual question answering (VQA) [1], in which models are expected to provide answers to questions related to an image and visual entailment (VE) [113], where the model is required to determine the similarity or relationship between a given image and a description. Classic visual reasoning methods have employed an image encoder and a text encoder, along with a reasoning block that utilizes attention mechanisms [121, 72, 105, 122], neuro-symbolic methods [116, 117, 61], or external knowledge [62, 28, 13] to perform reasoning.

Recent progress in large pre-trained models has led to the development of language models (LLMs) that possess exceptional commonsense reasoning capabilities [78, 16, 15, 77]. These models can potentially replace the reasoning block in visual reasoning tasks, and LLMs' lack of perception can be compensated by incorporating multiple vision-language models (VLMs) trained on different domains [104, 52, 76]. For example, PICa [115] converts the image into captions that GPT-3 [8] can understand, and adapts GPT-3 to solve the VQA task in a few-shot manner by providing a few in-context VQA examples. However, there is still a lack of research on how to harness the collective power of these complementary VLMs for visual reasoning tasks.

### Model Ensembling

Model ensembling is a powerful machine learning technique that combines the predictions of multiple models to improve the overall performance of a given task [20]. Classic model ensembling methods include simple averaging, weighting the predictions based on model performance, and stacking the models. By combining the predictions of multiple models, ensembling can reduce the variance and bias of the final predictions, resulting in a more robust and accurate model [83]. Ensemble methods have been shown to perform well in a wide range of tasks, including image classification, natural language processing, and time series forecasting. However, when it turns to multimodal tasks such as visual reasoning, a simple combination is not applicable to heterogeneous models as their inputs and outputs vary.

The Mixture-of-Experts (MoE) [91, 82, 126, 45, 47] can be conceptualized as a model ensemble strategy implemented at the level of network architecture. MoE-based multi-modal models [33] excel in leveraging the specific strengths of each expert, thereby delivering the performance that often outstrips that of any individual expert. In these networks, the credibility of each expert's output is dynamically weighted, facilitating a comprehensive and nuanced response to multimodal tasks.

However, even within this sophisticated framework, challenges can arise, particularly when managing heterogeneous pre-trained multimodal models. To address this problem, an innovative approach known as Socratic Models (SMs) [123] has been proposed. SMs employ prompt engineering to guide these diverse models through multimodal discussions, effectively combining their varied knowledge. This method promotes a more harmonious and effective integration of different models, enhancing the ensemble's ability to handle complex tasks.

With a similar goal, [5-4] proposes a closed-loop iterative consensus optimization method to utilize the strengths of individual models. However, previous methods do not fully explore the potential of a centralized solution or adapt to the separate functionalities of different models, particularly in the

\begin{table}
\begin{tabular}{c|c c} \hline \hline  & **Accuracy** & **\# Finetuning Params** \\ \hline Finetuning & 77.73 & 11B (100\%) \\ PEFT, (IA)\({}^{3}\) & 63.76 & 1M (0.01\%) \\ \hline \hline \end{tabular}
\end{table}
Table 11: (\(\mathbf{IA}\))\({}^{3}\)[57] **parameter-efficient tuning (PEFT) performance.** We finetune a FLAN-T5-XXL model on the A-OKVQA training set and evaluate it on the A-OKVQA validation set.

visual reasoning scenario. Recent studies, such as CICERO [65], have shown that language models possess strong capabilities in coordinating multiple agents, which inspires us to reorganize pre-trained multimodal models with a focus on the language models.

## Broader Impact

This study inherits ethical risks of biases from pretrained VLMs and LLMs, depending on their training data. We suggest the users consider the possible biases in reasoning and prompt the model to interpret its predictions in natural languages when necessary.