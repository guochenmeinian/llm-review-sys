# FERERO: A Flexible Framework for

Preference-Guided Multi-Objective Learning

 Lisha Chen\({}^{1}\), AFM Saif\({}^{1}\), Yanning Shen\({}^{2}\), Tianyi Chen\({}^{1}\)

\({}^{1}\)Rensselaer Polytechnic Institute, \({}^{2}\)University of California, Irvine

###### Abstract

Finding specific preference-guided Pareto solutions that represent different trade-offs among multiple objectives is critical yet challenging in multi-objective problems. Existing methods are restrictive in preference definitions and/or their theoretical guarantees. In this work, we introduce a Flexible framEwork for pREFeRence-guided multi-Objective learning (**FERERO**) by casting it as a constrained vector optimization problem. Specifically, two types of preferences are incorporated into this formulation - the _relative preference_ defined by the partial ordering induced by a polyhedral cone, and the _absolute preference_ defined by constraints that are linear functions of the objectives. To solve this problem, convergent algorithms are developed with both single-loop and stochastic variants. Notably, this is the _first single-loop primal algorithm_ for constrained optimization to our knowledge. The proposed algorithms adaptively adjust to both constraint and objective values, eliminating the need to solve different subproblems at different stages of constraint satisfaction. Experiments on multiple benchmarks demonstrate the proposed method is very competitive in finding preference-guided optimal solutions. Code is available at [https://github.com/lisha-chen/FERERO/](https://github.com/lisha-chen/FERERO/).

## 1 Introduction

Many machine learning tasks inherently involve multiple objectives, which can be different performance metrics such as accuracy, fairness, and privacy; or, the same metrics defined on different data [52, 42]. To tackle such multi-objective problems, it is common to learn a shared model that simultaneously performs well on all the objectives. Compared to learning one model for each objective, learning a shared model has the benefit of reducing both the model size and the inference time. This can be achieved through multi-objective optimization [52, 59, 35], which is to learn a model that minimizes the vector-valued objective. In practical applications, it is of interest to learn solutions with controlled trade-offs or preferences. To further illustrate, we give two examples below.

In fairness-aware machine learning, a trade-off exists between the fairness \(f_{\mathrm{fair}}(\theta)\) and accuracy \(f_{\mathrm{acc}}(\theta)\)[42, 37], see also Figure 0(a). With \(\theta\) denoting the model parameter, and \(C\) denoting the partial order cone, to find the optimal models that consider different trade-offs, one can solve the following problem with different thresholds \(\epsilon\)[9]

\[\text{maximize}_{C}\ \left(f_{\mathrm{acc}}(\theta),f_{\mathrm{fair}}( \theta)\right)^{\top}\ \mathrm{s.t.}\ f_{\mathrm{fair}}(\theta)\geq\epsilon. \tag{1.1}\]

Another example is in drug or molecule design, where the goal is to design drugs or molecules with multiple desired properties \(f_{1}(\theta),f_{2}(\theta),\ldots,f_{M}(\theta)\). Aiming to align the values of the properties \(F(\theta)\) with a predefined preference vector \(v\) as in Figure 0(b), one can solve the following problem [40, 1, 61]

\[\text{maximize}_{C}\ \ F(\theta)\coloneqq\left(f_{1}(\theta),\ldots,f_{M}( \theta)\right)^{\top}\ \mathrm{s.t.}\ BF(\theta)=Bv,\ Bv=0 \tag{1.2}\]

where \(B\in\mathbb{R}^{(M-1)\times M}\) is full row rank.

Then a natural question arises:

_Can we develop a principled framework to capture flexible preferences and admit provably convergent deterministic and stochastic algorithms?_

Our answer to this question is affirmative. Recognizing that all the aforementioned applications can be addressed within a unified framework, we formulate preference-guided multi-objective learning (PMOL) as a constrained vector optimization problem. Specifically, given a model \(\theta\in\mathbb{R}^{q}\), and the objectives \(f_{m}:\mathbb{R}^{q}\rightarrow\mathbb{R},\,m=1,\ldots,M\), we define the constrained vector optimization problem as

\[\min_{\theta\in\mathbb{R}^{q}}F(\theta)\coloneqq\big{(}f_{1}(\theta),\ldots,f_ {M}(\theta)\big{)}^{\top},\ \ \ \mathrm{s.t.}\ \ G(\theta)\leq 0,\,H(\theta)=0\] (PMOL)

where \(G(\theta)\) and \(H(\theta)\) are the vector-valued preference constraints such as the examples in (1.1) and (1.2). Here "\(\leq\)" and "\(=\)" are element-wise relations on the vectors, with each row representing one constraint. In these examples, the preferences are directly defined in the objective space, as intersections of half-spaces defined by the hyperplanes; see Figure 1. Thus, \(G(\cdot)\) and \(H(\cdot)\) in (PMOL) can be expressed as linear functions of \(F(\theta)\), given by

\[G(\theta)=B_{g}F(\theta)+b_{g},\ \ H(\theta)=B_{h}F(\theta)+b_{h} \tag{1.3}\]

where \(B_{g}\in\mathbb{R}^{M_{g}\times M},B_{h}\in\mathbb{R}^{M_{h}\times M}\), and \(b_{g}\in\mathbb{R}^{M_{g}},b_{h}\in\mathbb{R}^{M_{h}}\). Different \(B_{g},B_{h},b_{g},b_{h}\) correspond to different preferences, and thus different trade-offs among the objectives.

A comparison of our methods to existing methods is summarized in Table 1. Specifically, our contributions are listed as follows:

* We cast the PMOL problem as a constrained vector optimization problem, and develop the FERERO framework to capture flexible preferences.
* Under the FERERO framework, we develop a meta primal algorithm with a unified subprogram adaptive to both objectives and constraints to meet flexible preferences, eliminating the need for multiple subprograms under different active constraints.
* Under the FERERO framework, we develop a practical single-loop algorithm with non-asymptotic convergence guarantees. To our best knowledge, this is the _first single-loop primal algorithm_ in constrained vector optimization with convergence guarantees.
* We apply the proposed algorithms to various synthetic and real-world image and speech datasets to demonstrate its ability to find flexible preference-guided optimal models.

In our theoretical analysis, we address the following technical challenges.

* The commonly used constraint qualification assumptions do not generally hold for the PMOL problem. We overcome this challenge by leveraging the specific structure that the constraints are linear functions of \(F\) to prove the calmness condition holds for PMOL. See more details in Lemma 2.
* The convergence of the single-loop algorithm is slower with the commonly-used merit functions. We provide a sharper analysis by introducing a different merit/Lyapunov function and exploiting the algorithm properties under additional assumptions. See Theorem 3.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline Method & Preference & \multirow{2}{*}{Exactness} & Controlled & Single & Convergence & \multirow{2}{*}{Stoch.} \\  & Flexibility & Exactness & ascent & loop & Deter. & \multicolumn{1}{c}{} \\ \hline Linear Scalarization & weight & - & ✗ & ✓ & \(T^{-1}\) & \(T^{-\frac{1}{2}}\) \\ (Smooth) Tchebycheff [32] & weight & - & ✗ & ✓ & non-asymptotic & ✗ \\ PMTL [33] & inequalities (absolute) & ✗ & ✗ & ✗ & asymptotic & ✗ \\ EPO [41] & \(r^{-1}\) ray (ratio, absolute) & ✓ & ✓ & ✗ & asymptotic & ✗ \\ (X)WC-MGDA [44] & shifted ray (absolute) & ✓ & ✗ & ✗ & ✗ & ✗ \\ FERERO (ours) & relative \& absolute & ✓ & ✓ & ✓ & \(T^{-1}\) & \(T^{-\frac{1}{2}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison to existing methods. “Flexibility” represents preference modeling, such as by using weights, preference vectors (rays), or constraints. “Exactness” represents the ability to align with a preference vector exactly. “Deter.”, “Stoch.” represent deterministic and stochastic, respectively. “✗” means not provided in the corresponding work, and “-” means not relevant.

Figure 1: Illustration of preferences in different examples. The solid red curves represent the Pareto front, dashed lines represent preference constraints.

* The convergence analysis often relies on assumptions on bounded functions or bounded constraints. We remove such assumptions by applying similar techniques in [7] with proper choice of Lyapunov functions, and exploiting algorithm properties. See Theorems 2 and 3.

## 2 Problem Setup and A Meta Algorithm

To characterize the optimality conditions of PMOL, we introduce the generalized notion of dominance and the related concept of optimality. We then present a meta-algorithm to solve PMOL.

### Problem setup and preliminaries

We first introduce optimality definitions for PMOL that go beyond the standard definitions of Pareto optimality [15; 11; 36]. Given two vectors \(v\) and \(w\), we use \(v<w\) and \(v\leq w\) to denote \(v_{i}<w_{i}\) for all \(i\), and \(v_{i}\leq w_{i}\) for all \(i\), respectively. We use \(v\leq w\) to denote \(v\leq w\) and \(v\neq w\), and define \(>,\geq,\geq\) analogously.

**Definition 1** (\(C_{a}\)-dominance [12; 27]).: _Given \(v,w\in\mathbb{R}^{M}\), \(A\in\mathbb{R}^{M\times M}\), and \(C_{A}:=\{y\in\mathbb{R}^{M}\mid Ay\geq 0\}\neq\emptyset\), we say \(v\) strictly dominates \(w\) based on \(C_{A}\) if and only if \(A(v-w)<0\)._

The generalized dominance defines a partial order on \(\mathbb{R}^{M}\), i.e., the relation between two vectors. Illustrations of different partial orders are given in Figure 2. Figure 1(a) shows the dominance relation under the widely used non-negative orthant cone with \(C_{A}=\mathbb{R}^{M}_{+}\), corresponding to Pareto optimality. However, as illustrated by the figure, given the initial green reference point, a descent method such as MGDA [15] cannot find points on the Pareto front but outside of the gray shaded region. This poses a critical challenge for applications where specific preference-guided solutions on the Pareto front are needed. Nevertheless, this issue can be addressed by substituting \(\mathbb{R}^{M}_{+}\) with a more general definition of \(C_{A}\) as displayed in Figure 1(b). Under this partial order, a general descent method is able to reach any points on the Pareto front starting from the green reference point.

Based on the partial order, one can then find the minimum or optimal elements in the vector-valued objective space, whose formal definition is provided below.

**Definition 2** (\(C_{a}\)-optimal).: _A point \(\theta\in\mathbb{R}^{q}\) is \(C_{A}\)-optimal if there is no \(\theta^{\prime}\neq\theta\) such that, \(AF(\theta^{\prime})\leq AF(\theta)\). A point \(\theta\) is weakly \(C_{A}\)-optimal if there is no \(\theta^{\prime}\neq\theta\) such that, \(AF(\theta^{\prime})<AF(\theta)\)._

Note that, \(C_{A}\) is a polyhedral cone, or the intersection of half-spaces defined by the rows of the inequality \(Ay\geq 0\). When \(A=I_{M}\), an \(M\times M\) identity matrix, \(C_{A}=\mathbb{R}^{M}_{+}\coloneqq\{y\in\mathbb{R}^{M}\mid y_{m}\geq 0\ \ \forall m \in[M]\}\), then Definition 1 reduces to the commonly used notion of dominance associated with Pareto optimality. The cone \(C_{A}\) can be interpreted as a _relative preference_ that defines the objectives' improvement directions, which generalizes the relative preference defined by \(\mathbb{R}^{M}_{+}\). In contrast, the preference defined by constraints in (1.3) can be interpreted as an _absolute preference_ that defines the feasible or preferred set of objective function values. In practice, \(C_{A}\) can be chosen based on the requirements of specific applications. For example, when the controlled ascent of objectives is needed [41], we can choose \(C_{A}\) such that the controlled ascent direction belongs to \(-C_{A}\). We defer the detailed implementation to Section 3.2. The \(C_{A}\)-optimal set, denoted as \(\mathcal{P}_{A}\), contains all the \(C_{A}\)-optimal models. When \(A=I_{M}\), \(\mathcal{P}_{A}\) is the Pareto optimal set \(\mathcal{P}\). The Pareto front is the set of function values evaluated at Pareto optimal models, i.e., \(\mathcal{F}=\{F(\theta)\mid\theta\in\mathcal{P}\}\).

We make the following standard assumptions throughout the paper [15; 25; 7].

**Assumption 1**.: _1. (Non-negative objectives) \(AF(\theta)\geq 0\), and \(\mathbf{1}^{\top}AF(\theta)\geq c_{AF}>0\) for all \(\theta\in\mathbb{R}^{q}\). 2. (Differentiable objectives) \(F\) is twice continuously differentiable. 3. (Ordering cone with non-empty interior) \(C_{A}\) has a non-empty interior._

### Find the preference-guided direction

In this section, we proceed to discuss an adaptive method to solve (PMOL). At iteration \(t\), the algorithm finds an update direction \(d_{t}\) and performs the iterative update \(\theta_{t+1}=\theta_{t}+\alpha_{t}d_{t}\) with a step size \(\alpha_{t}\). Ideally, the update direction \(d_{t}\) is chosen to improve the objective \(F(\theta)\) and to satisfy

Figure 2: Illustration of \(C_{A}\)-dominance. The solid red curves are the Pareto fronts, green dots are the reference points, gray shaded regions are the set of objectives dominating the reference points, under different \(C_{A}\) in (a) and (b).

the preference constraints. It is desirable that when the constraints are not satisfied, \(d_{t}\) decreases the violation of constraints and improves the objectives in the general partial ordering sense; when the constraints are satisfied, \(d_{t}\) improves the objectives and ensures the constraints are satisfied. To achieve this, we find a direction \(d^{*}(\theta)\) that solves following subprogram

\[\psi(\theta)\coloneqq \min_{(d,c)\in\mathbb{R}^{q}\times\mathbb{R}}c+\frac{1}{2}\|d\|^{2} \mathrm{s.t.} A\nabla F(\theta)^{\top}d\leq\frac{c}{\mathbf{1}^{\top}AF(\theta)}AF(\theta) \tag{2.1}\] \[\nabla G(\theta)^{\top}d+c_{g}G(\theta)\leq 0,\;\nabla H(\theta)^{ \top}d+c_{h}H(\theta)=0\]

where \(\|\cdot\|\) denotes the \(\ell_{2}\)-norm, \(c_{g}\) and \(c_{h}\) are pre-defined positive constants. Larger \(c_{g}\) and \(c_{h}\) put more emphasis on constraint satisfaction than objective improvement. We call this subprogram _adaptive_ since it deals with constraints in an adaptive way, which does not require the initial model to be feasible, nor \(\theta_{t}\) to be feasible at each iteration. But rather, it finds an update direction that decreases the constraint violation. Because of this, it neither requires solving different subprograms at different stages nor requires different treatment of the active set of inequalities as in existing works [33, 41, 44].

We then show in Lemma 1 that the desired properties can be satisfied.

**Lemma 1**.: _For the subprogram (2.1), the following holds: If \(\theta\) is a local optimal solution with \(AF(\theta)>0\), then \(d^{*}(\theta)=0\), \(\psi(\theta)=0\). Otherwise, if \(\theta\) is not a local optimal solution, then \(d^{*}(\theta)\neq 0\), \(\psi(\theta)<0\), and when \(\theta\) is feasible,_

\[2\psi(\theta)\leq-\|d^{*}(\theta)\|^{2}<0. \tag{2.2}\]

_Let \(\theta\) be a weak \(C_{A}\)-optimal solution, with \((AF(\theta))_{m}=0\) for some \(m\in[M]\). If there exists feasible and non-strictly improving directions at \(\theta\) with \(A\nabla F(\theta)^{\top}d\leq 0\), then \(d^{*}(\theta)\neq 0\), \(\psi(\theta)<0\). Otherwise, \(d^{*}(\theta)=0\), \(\psi(\theta)=0\)._

By Lemma 1, \(\|d^{*}(\theta)\|=0\) is a stationary condition for PMOL. Recall the feasibility condition requires \([G(\theta)]_{+}=0\) and \(|H(\theta)|_{\mathrm{ab}}=0\), where \([\cdot]_{+}\) and \(|\cdot|_{\mathrm{ab}}\) are entry-wise ReLU and absolute functions, respectively. And the complementary slackness condition requires \(\lambda_{g}^{*\top}[-G(\theta)]_{+}=0\). Thus \(\|d^{*}(\theta)\|^{2}+\lambda_{g}^{*\top}[-G(\theta)]_{+}+\|[G(\theta)]_{+}\|_ {1}+\|H(\theta)\|_{1}\) achieves zero if and only if the model \(\theta\) satisfies the first-order KKT condition. Besides the properties in Lemma 1, it has an additional scale-invariant property that is deferred to Lemma 6 due to space limit.

By the Lagrangian of (2.1), the optimal update direction can be expressed in a simple form as a weighted combination of the gradients, i.e. \(d^{*}(\theta)=-\nabla F(\theta)A_{ag}^{\top}\lambda^{\star}\), with \(A_{ag}\coloneqq[A;B_{g};B_{h}]\), and

\[\lambda^{*}\in\operatorname*{arg\,min}_{\lambda\in\Omega_{\lambda}(\theta)}\; \varphi(\lambda;\theta)\coloneqq\frac{1}{2}\|\nabla F(\theta)A_{ag}^{\top} \lambda\|^{2}-c_{g}\lambda_{g}^{\top}G(\theta)-c_{h}\lambda_{h}^{\top}H(\theta) \tag{2.3}\]

where \(\lambda=[\lambda_{f};\lambda_{g};\lambda_{h}]\), \(\Omega_{\lambda}(\theta)\) is the domain of the Lagrangian multipliers, given by 1

Footnote 1: Note that, our formulation and analysis cover the constrained MOO problem with a simplified subprogram, where \(\Omega_{\lambda_{f}}=\Delta^{M}\), which is detailed in Remark 4 in Appendix D.1.

\[\Omega_{\lambda}(\theta)\coloneqq\Omega_{\lambda_{f}}(\theta)\times\mathbb{R }_{+}^{M_{g}}\times\mathbb{R}^{M_{h}},\;\;\text{with}\;\;\Omega_{\lambda_{f}} (\theta)\coloneqq\{\lambda\in\mathbb{R}_{+}^{M}\;|\;\lambda^{\top}AF(\theta)= \mathbf{1}_{M}^{\top}AF(\theta)\}. \tag{2.4}\]

Our goal is to design an algorithm that converges to a KKT solution based on (2.1). However, the KKT condition is not necessary unless certain constraint qualifications (CQs) hold. Prior works [20, 33] assume certain CQs hold, e.g., the Linear Independence Constraint Qualification (LICQ). However, the LICQ assumption (c.f., [20, Section 3.1, (A2)]) does not generally hold at a local optimal solution for problem (PMOL), c.f., Example 1 in Appendix D.3.2. Though some commonly used CQs do not hold generally, in our case, leveraging the specific structure that the constraints are linear functions of \(F\), we can justify the calmness CQ in Definition 10 tailored for our problem in Lemma 2, thus the KKT condition is a necessary optimality condition. The proof is deferred to Appendix D.3.2.

**Lemma 2**.: _Let \(\bar{\theta}\in\mathbb{R}^{q}\) be a global solution to (PMOL). Define \(\Sigma(p,q)\coloneqq\{y\in\mathbb{R}^{M}\mid B_{g}y+b_{g}\leq p,B_{h}y+b_{h}=q\}\). If \(\Sigma(p,q)\) is a line, the PMOL calmness condition in Definition 10 is satisfied for (PMOL) at \(\bar{\theta}\) if \(A\in\mathbb{R}^{M\times M}\) is full rank, \(H(\theta),G(\theta)\) defined by (1.3) satisfy \([B_{h}^{\top},B_{g}^{\top}]\neq 0\), and \(B_{h},B_{g}\) are full row rank. Consequently, the KKT condition is a necessary optimality condition._

Lemma 2 provides a sufficient condition for the KKT condition to be a necessary optimality condition without relying on unjustified assumptions. The requirement that the constraint set is a line in the objective space is common for applications such as alignment to a preference vector.

We then discuss a generic preference-guided multi-objective algorithm based on the subprogram.

### A meta algorithm for preference-guided multi-objective learning

Given the model \(\theta_{t}\) at iteration \(t\), one can then solve (2.3) to obtain \(\lambda_{t}\). The direction \(d_{t}=-\nabla F(\theta_{t})A_{qg}^{\top}\lambda_{t}\) is used to update the model \(\theta_{t}\) by \(\theta_{t+1}=\theta_{t}+\alpha_{t}d_{t}\) iteratively until convergence. The full procedure of this meta algorithm is summarized in Algorithm 1, where Step 4 is a generic step and can be customized in Section 3.

To establish the non-asymptotic convergence rate, we use the following standard smoothness assumption that has been commonly used in prior works for multi-objective learning [7; 36].

**Assumption 2** (Smooth objectives).: _For all \(m\in[M]\), \(\nabla f_{m}(\theta)\) is \(\ell_{f,1}\)-Lipschitz continuous._

We then state the convergence result for Algorithm 1 in Theorem 1.

**Theorem 1** (Convergence of the generic FERERO algorithm).: _Suppose Assumptions 1, 2 hold. Let \(\{\theta_{t}\}\) be the sequences produced by Algorithm 1, with \(d_{t}\) being an \(\epsilon\)-optimal solution to the subprogram (2.1). If \(\|\lambda^{*}(\theta_{t})\|_{1}\leq c_{\lambda}\), \(\alpha_{t}\leq\min\{\frac{1}{c_{\lambda}\ell_{f,1}\|\lambda_{alg}\|_{\infty,1} },c_{g}^{-1},c_{h}^{-1}\}\), and \(\alpha_{t}=\Theta(1)\), then_

\[\frac{1}{T}\sum_{t=0}^{T-1}\underbrace{\|\nabla F(\theta_{t})A_{ ag}^{\top}\lambda^{*}(\theta_{t})\|^{2}}_{\text{equilibrium}}+ \underbrace{\lambda_{g}^{*}(\theta_{t})^{\top}[-G(\theta_{t})]_{+}}_{\text{ equilibrium}}+\underbrace{\|[G(\theta_{t})]_{+}\|_{1}+\|H(\theta_{t})\|_{1}}_{\text{Equallicity}}=\mathcal{O} \big{(}T^{-1}+\epsilon\big{)}. \tag{2.5}\]

Theorem 1 guarantees the non-asymptotic convergence for the generic FERERO algorithm. In Algorithm 1, \(\lambda_{t}\) can be solved through projected gradient descent or Frank Wolfe algorithm iteratively within an inner loop. In practice, we usually do not need to solve the subprogram exactly. Next, we discuss the efficient single-loop approximate algorithm based on Algorithm 1.

## 3 Efficient Single-loop Algorithms

In this section, we first discuss algorithm development with the approximate single-loop update and practical choice of preferences. We focus on (PMOL) with _equality constraints only_, i.e., \(M_{g}=0\). Building upon this, we then discuss the stochastic variants of the algorithms that can be applied to large-scale learning problems.

### Single-loop approximate algorithm

In practice, if one only requires the converging solutions generated by the algorithm to be feasible, but not all the iterates, then further approximations can be made to the subprogram (2.3). At iteration \(t\), to obtain an approximate direction \(d_{t}\), we adopt the following update

\[\lambda_{t+1}=\Pi_{\Omega_{\lambda}}\big{(}\lambda_{t}-\gamma_{t }\nabla_{\lambda}\varphi(\lambda_{t};\theta_{t})\big{)}. \tag{3.1}\]

The single-loop algorithm with the approximate solution is summarized in Algorithm 2. We name it FERERO with Single-loop Approximate update (FERERO-SA) algorithm.

```
1:Initialize \(t=0\), \(\theta_{0}\), \(\lambda_{0}\), step sizes \(\{\alpha_{t},\gamma_{t}\}\); define \(A\), number of iterations \(T\).
2:for\(t=0,\dots,T-1\)do
3: Compute gradient \(\nabla F(\theta_{t})\);
4: Compute direction \(d_{t}=-\nabla F(\theta_{t})A_{ag}^{\top}\lambda_{t}\);
5: Update \(\theta_{t}\) by \(\theta_{t+1}=\theta_{t}+\alpha_{t}d_{t}\);
6: Update \(\lambda_{t}\) by (3.1);
7:endfor
```

**Algorithm 2** FERERO-SA

We make the following additional assumption of Lipschitz objectives to prove the convergence of Algorithm 2, which is standard in optimization literature.

**Assumption 3** (Lipschitz objectives).: _For all \(m\in[M]\), \(f_{m}(\theta)\) is \(\ell_{f}\)-Lipschitz continuous._To prove the convergence of Algorithm 2, we can use the same merit function with \(\ell_{1}\)-norm of \(H(\theta_{t})\), which leads to a slow convergence rate of \(\mathcal{O}\big{(}T^{-\frac{1}{6}}\big{)}\). See Theorem 2 below and its proof in Appendix F.2, where the proof follows similar ideas of the proofs of Theorem 3 and Theorem 5 in [7].

**Theorem 2** (Convergence of the FERERO-SA algorithm).: _Suppose Assumptions 1, 2, 3 hold, and \(M_{g}=0\). Let \(\{\theta_{t}\},\{\lambda_{t}\}\) be the sequences produced by Algorithm 2 with \(A=I\) and \(\Omega_{\lambda_{f}}(\theta)=\Delta^{M}\) (c.f. Remark 4). Assume \(\lambda_{t},\lambda^{*}(\theta_{t})\), and \(\lambda^{*}_{\rho}(\theta_{t})\coloneqq\arg\min_{\lambda\in\Omega_{\lambda}} \varphi(\lambda;\theta_{t})+\frac{\rho}{2}\|\lambda\|^{2}\) are bounded. With properly chosen step sizes \(\alpha=\Theta(T^{-\frac{1}{6}})\), \(\gamma=\Theta(T^{-\frac{1}{6}})\), and hyperparameters, it holds that_

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla F(\theta_{t})A_{ag}^{\top}\lambda_{t}\|^{2 }+\|H(\theta_{t})\|_{1}=\mathcal{O}\Big{(}T^{-\frac{1}{6}}\Big{)}. \tag{3.2}\]

To obtain a sharper convergence rate, we consider a different merit function with \(\ell_{2}\)-norm of the constraint \(H(\theta_{t})\), and under additional assumptions listed below.

**Definition 3** (Proximal PL inequality).: _Define \(D_{\varphi,\gamma}(\lambda;\theta)\coloneqq-\frac{2}{7}\min_{\lambda^{\prime} \in\Omega_{\lambda}}\big{\{}\langle\nabla_{\lambda}\varphi(\lambda;\theta), \lambda^{\prime}-\lambda\rangle+\frac{1}{2\gamma}\|\lambda^{\prime}-\lambda\|^{2} \big{\}}\). We say \(\varphi(\lambda;\theta)\) satisfies the \(\mu_{\varphi}\)-proximal PL inequality on the point \((\lambda,\theta)\), if there exists some constant \(\mu_{\varphi}>0\) such that \(D_{\varphi,\gamma}(\lambda;\theta)\geq\mu_{\varphi}\big{(}\varphi(\lambda; \theta)-\varphi(\lambda^{*}(\theta);\theta)\big{)}\)._

**Assumption 4**.: _For \(\theta\in\{\theta_{t}\},\lambda\in\{\lambda_{t}\}\) on the trajectory of Algorithm 2, the following hold: 1. \(\varphi(\cdot;\theta)\) is \(\mu_{\varphi}\)-proximal PL in Definition 3; 2. For all \(m\in[M]\), \(\nabla^{2}f_{m}(\theta)\) is \(\ell_{f,2}\)-Lipschitz continuous._

Assumption 4-1 essentially requires some regularity conditions of \(\varphi(\cdot;\theta)\) on the trajectory of Algorithm 2. Leveraging the fact that \(\varphi(\cdot;\theta)\) is convex, it has been discussed in e.g., [28, Appendix B] that if the smallest non-zero singular value of the Hessian is bounded away from zero, then Assumption 4-1 holds. This could be satisfied when the gradients \(\nabla F(\theta_{t})A_{ag}^{\top}\) have lower-bounded non-zero singular values on the trajectory. A more detailed analysis of the sufficient conditions for Assumption 4-1 to hold is left for further work.

We then provide a sharper convergence analysis in Theorem 3. The detailed proof and choices of step sizes and hyperparameters are deferred to Appendix F.3.

**Theorem 3** (Sharper convergence of the FERERO-SA algorithm).: _Suppose Assumptions 1, 2, 3, 4 hold, and \(M_{g}=0\). Let \(\{\theta_{t}\},\{\lambda_{t}\}\) be the sequences produced by Algorithm 2 with \(A=I\) and \(\Omega_{\lambda_{f}}(\theta)=\Delta^{M}\) (c.f. Remark 4). With properly chosen step sizes \(\alpha_{t}=\Theta(1)\), \(\gamma_{t}=\Theta(1)\), and hyperparameters, it holds that_

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla F(\theta_{t})A_{ag}^{\top}\lambda_{t}\|^{2 }+\|H(\theta_{t})\|^{2}=\mathcal{O}\Big{(}T^{-1}\Big{)}. \tag{3.3}\]

Theorem 3 states that \(\{\theta_{t}\}\) produced by Algorithm 2 converges to a KKT solution of the PMOL problem in the general nonconvex case. Moreover, both \(\|d_{t}\|^{2}\) and \(\|H(\theta_{t})\|^{2}\) converge to zero at a rate of \(\mathcal{O}(T^{-1})\), implying the convergence of both the objective values and the preference constraints. Note that, the convergence in terms of \(\|H(\theta_{t})\|^{2}\) at a rate of \(\mathcal{O}(T^{-1})\) is weaker compared to the one with \(\|H(\theta_{t})\|_{1}\) at the same rate for Algorithm 1. This is reasonable since Algorithm 2 only uses a one-step approximate update of \(\lambda_{t}\) instead of exactly solving the subprogram.

**The stochastic variant.** We employ a stochastic variant of Algorithm 2 based on the double sampling techniques developed in the recent work [7]. The update is given by

\[\theta_{t+1}= \theta_{t}+\nabla F_{\xi_{t,1}}(\theta_{t})A_{ag}^{\top}\lambda_{t} \tag{3.4a}\] \[\lambda_{t+1}= \Pi_{\Omega_{\lambda}}\big{(}\lambda_{t}-\gamma_{t}\tilde{\nabla }_{\lambda}\varphi(\lambda_{t};\theta_{t})\big{)}\] (3.4b) \[\tilde{\nabla}_{\lambda}\varphi(\lambda_{t};\theta_{t})= A_{ag}\nabla F_{\xi_{t,2}}(\theta_{t})^{\top}\nabla F_{\xi_{t,1}}(\theta_{t})A_{ag}^{ \top}\lambda_{t}-[0^{\top},c_{h}H_{\xi_{t,1}}(\theta_{t})^{\top}]^{\top} \tag{3.4c}\]

where \(\tilde{\nabla}\) is the unbiased stochastic estimate of the gradient, and \(\xi_{t,1}\) and \(\xi_{t,2}\) are two independent stochastic samples obtained at iteration \(t\).

The full description of the stochastic algorithm and its convergence guarantee are deferred to Appendix G. We provide a converegnce rate guarantee that matches the rate of SGD under additional assumptions on the bounded variance of the stochastic gradients.

### Choice of relative preferences

As briefly discussed in Section 2.1, the ordering cone and the corresponding matrix \(A\) can be specified according to practical needs. We first discuss how to obtain matrix \(A\) for the relative preference given the set of improvement directions. Then we discuss how to choose the relative preference to allow controlled ascent update, which is useful for touring the Pareto front [41].

**Ordering cone generation.** In practice, to obtain the polyhedral cone that defines the partial order, one can usually first define the extreme rays of the polyhedral cone. We then show how to convert the extreme ray description of the cone to the half-space description given by matrix \(A\), i.e., \(C_{A}=\{y\in\mathbb{R}^{M}\mid Ay\geq 0\}\), by showing how to compute \(A\) from the extreme rays.

Let \(Y=[y_{1}\cdots y_{M}]\in\mathbb{R}^{M\times M}\) be a matrix that contains all the extreme rays of \(C_{A}\) as its column vectors, then \(C_{A}=\{Y\lambda\mid\lambda\geq 0\}\). Let \(a_{m}^{\top}\in\mathbb{R}^{1\times M}\) denote the row vectors of \(A\) for all \(m\in[M]\). Then all \(a_{m}\) can be found by \(a\) that solves the following linear feasibility program

\[\underset{a\neq 0,\lambda\geq 0}{\text{find}}\ \ \text{s.t.}\ Y\lambda=c,\ \ c^{\top}a=0,\ \ Y^{\top}a\geq 0. \tag{3.5}\]

**Choice of \(C_{A}\) for controlled ascent.** If \(C_{A}\) is not pre-specified, and the decision maker wants to choose \(C_{A}\) to allow controlled ascent, it can be achieved with the following procedure. Let \(F_{0}=F(\theta_{0})\) be the objective of the initial iterate of the algorithm, and \(F_{tg}\) be the target function value along the controlled ascent direction. To ensure \(F_{tg}-F_{0}\in-C_{A}\) for controlled ascent, we include \((F_{0}-F_{tg})/\|F_{0}-F_{tg}\|\) in the set of extreme rays, then take the extreme rays of the convex hull of the new set to form the columns of \(Y\). Finally, we obtain \(C_{A}\) by solving (3.5).

## 4 Related Works

To put our work in context, we review the most relevant literature in (preference-guided) multi-objective optimization, constrained optimization, with a focus on gradient-based approaches.

**Multi-objective optimization (MOO).** A straightforward approach of MOO is to use scalarization to transform MOO into a single-objective optimization problem [43]. Another popular approach focuses on finding update directions which avoid conflicts with the gradients of the objectives [52; 59; 35]. A foundational algorithm in this domain is the Multiple Gradient Descent Algorithm (MGDA) [15; 17; 11; 36], which dynamically weights gradients to find a steepest common descent direction for all objectives. Later on, variants of MGDA are developed, which are discussed in detail in Appendix B.1 and [7]. However, solutions based on MGDA usually cannot capture pre-defined user preferences that represent various trade-offs on the Pareto front. This motivates the development of _preference-guided multi-objective optimization_ methods.

Preferences can be modeled through weights or thresholds assigned to different objectives [43]. For example, scalarization-based methods use the \(\ell_{p}\)-norm of the weighted vector-valued objective to convert the vector-valued objective into a scalar-valued objective, e.g., Linear scalarization (LS), Tchebycheff scalarization; see e.g., [32]. Then the problem can be solved by single-objective optimization on the scalar objective. The \(\epsilon\)-constraint methods enforce threshold constraints on different objectives, then solve the problem by constrained optimization; see e.g., [9]. More recently, preferences have been modeled by preference vectors defined in the objective space. Then the problem can be formulated as finding Pareto optimal solutions satisfying the constraints defined by the preference vectors [33], or optimizing the distance to the preference vectors [41; 44]. The key difference between FERERO and these works is that FERERO can capture more flexible preferences based on a general partial order, and general inequality/equality constraints. Moreover, we provide convergence rate guarantees for the proposed algorithms. A detailed comparison is summarized in Table 1 in Section 1 and Table 5 in Appendix B.2.

**Constrained optimization.** Constrained optimization methods include primal methods, penalty and barrier methods, and primal-dual methods [4; 39]. Our proposed method is related to the primal method that finds an update direction to ensure the models are feasible and improving along the optimization trajectory. To address the limitation that it usually requires a stage-one procedure to ensure the initialization is feasible, we use an adaptive approach to ensure the constraint violation is decreasing and converging to zero. This idea can also be found in sequential quadratic programming (SQP). SQP has been widely applied to solve constrained single-objective optimization [21; 6]. Later on, it has also been applied to constrained MOO [16]. Compared to SQP, we use an identity matrix to approximate the Hessian of each objective, and we propose an adaptive variant that automatically adjust the descent amount of objectives. Furthermore, existing SQP algorithms typically require an inner loop to solve the optimal Lagrangian multiplier, resulting in double-loop algorithms. In contrast, we develop a single-loop algorithm which can be more efficient.

**Vector optimization.** Vector optimization [12; 27] generalizes multi-objective optimization by substituting the commonly used component-wise partial order with a more general partial order, such as a general convex-cone induced partial order used in this paper. In the unconstrained setting, the MGDA method is extended to a steepest cone descent method in the vector optimization setting in [25]. In the constrained setting, the first-order optimality conditions are studied in [23; 57]. Algorithms based on projected gradient [24; 18; 19] or conditional gradient [8] are developed to solve vector optimization with parameters in a constraint set, to name a few. Besides gradient-based vector optimization, another line of works focus on black-box vector optimization with discrete design space; see e.g. [3; 2]. To our best knowledge, we are the first to design gradient-based single-loop (stochastic) primal algorithms for constrained vector optimization with convergence rate guarantees.

## 5 Experiments

In this section, we conduct experiments to verify our theory and show the applicability of the algorithms to preference-guided multi-task learning, and multi-objective finetuning of large multi-lingual speech recognition models. We use Linear scalarization (LS), MGDA [52], PMTL [33], EPO [41], XWC-MGDA [44] as baselines for comparison.

**Metrics.**_Objective loss and accuracy._ We report the objective losses and accuracies in classification. _Relative loss profile._ We use the element-wise product of the preference vector and the objective values as a measure of the relative loss profile. _Hypervolume._ Let \(F^{\prime}\in\mathbb{R}^{M}\) denote a reference point, and \(\mathcal{S}\) denote a set of objective function values of the obtained models. Hypervolume measures the size of the dominated space of \(\mathcal{S}\) relative to \(F^{\prime}\), which can be computed by \(H(\mathcal{S})=\Lambda(\{q\in\mathbb{R}^{M}\mid\exists F\in\mathcal{S}:F\leq q \leq F^{\prime}\})\), where \(\Lambda(\cdot)\) denotes the Lebesgue measure. For a fair comparison, we use the Nadir point, i.e., the worst performance on single-task baselines, as the reference point \(F^{\prime}\).

**Additional details.** The implementation and additional experiments can be found in Appendix H.

### Synthetic data

Following [33; 41; 44], the first objective we consider is

\[F(\theta)=\big{(}1-e^{-\|\theta-\frac{1}{\sqrt{q}}1\|_{2}^{2}},\ \ 1-e^{-\| \theta+\frac{1}{\sqrt{q}}1\|_{2}^{2}}\big{)}. \tag{5.1}\]

The objective has a nonconvex Pareto front (PF). See the results of different methods in Figure 3. With uniformly generated weights from a simplex, LS only finds extreme points on the PF with one objective minimized. MGDA can only find points close to the center of the PF. PMTL can find points

Figure 4: Outputs (colored markers) and optimization trajectories (colored lines) of different methods when initial objectives are near the Pareto front. Different colors represent different preferences.

Figure 3: Converging solutions (blue dots) and optimization trajectories (blue lines) on the objective space of different methods on synthetic objectives given in (5.1). Dashed arrows represent pre-specified preference vectors. The green dots represent initial objective values.

in the subregions but not aligned well with the exact preference vectors. Similar to EPO, in Figure 2(e), our method finds points that align well with the exact preferences; and in Figure 2(f), our method can handle different definitions of preferences.

We conduct another experiment in a more difficult setting where the initial objectives are close to the PF. In Figures 3(a)-3(c), we consider a relatively easier case where the initial model is not too close to the Pareto optimal. For our method, by solving (3.5), \(a_{1}=[\frac{1}{\sqrt{5}};\frac{2}{\sqrt{5}}],a_{2}=[\frac{2}{\sqrt{5}};\frac{1 }{\sqrt{5}}]\). The corresponding matrix \(A\) is given by \(A=[a_{1},a_{2}]^{\top}\). In this setting, all methods converge to the PF, and our method takes the least number of iterations (PMTL takes 100, EPO Search takes 60, and our method takes only 10 iterations). PMTL does not align exactly with the preference vectors, while EPO and our method do. In Figures 3(d)-3(f), PMTL and our method take \(200\) iterations, EPO Search takes 80 iterations. Results show that for the green and yellow preferences, PMTL moves further away from the PF in the first stage, and does not perform any update in the second stage. It converges to the PF only in 2 out of 4 cases. In contrast, with controlled ascent updates, EPO and our method can converge to the PF and trace the PF until the objectives align exactly with the preferences.

### Real data

**Multi-patch image classification.** Following [33; 41; 44], we consider three datasets for image classification, including Multi-MNIST, Multi-Fashion, and Multi-Fashion+MNIST. The two tasks or objectives in all three datasets are to classify the top-left and the bottom-right images, respectively. For a fair comparison, we use LeNet as the backbone neural network. The training losses and accuracies of different methods given different preference vectors are plotted in Figure 5. Experiments for our method are repeated 5 times. Hypervolumes with means and standard deviations are reported in Table 2. The results for other methods in Table 2 are referenced from [44].

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Datasets & LS & PMTL [33] & EPO [41] & XWC-MGDA [44] & FERERO \\ \hline Multi-MNIST loss & 1.68 & 1.41 & 1.35 & 1.42 & **1.97\(\pm\)0.21** \\ Multi-Fashion loss & 6.75 & 5.90 & 6.02 & 6.77 & **7.76\(\pm\)0.18** \\ Multi-F+M loss & 3.63 & 3.03 & 3.76 & **3.89** & 3.82\(\pm\)0.21 \\ Multi-MNIST accuracy & 0.19 & 0.15 & 0.15 & 0.16 & **0.24\(\pm\)0.04** \\ Multi-Fashion accuracy & 0.99 & 0.87 & 0.87 & 0.99 & **1.17\(\pm\)0.07** \\ Multi-F+M accuracy & 0.48 & 0.40 & 0.50 & 0.52 & **0.53\(\pm\)0.04** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Hypervolumes of different methods (\(\times 10^{-2}\))

Figure 5: Training losses and accuracies of various methods with different preferences across three image datasets. The horizontal and vertical axes represent results for objective 1 and objective 2, respectively. Different colored dashed arrows indicate various preference vectors. Different markers denote the solutions obtained by different methods, with marker colors matching the preferences.

One limitation of EPO is that the preference is defined as a ray from the origin in the objective space, whose corresponding objectives can be unattainable, e.g., the yellow preferences in Figure 5. As a result, the losses of all methods are far away from the preference vectors. In this case, a more flexible choice of preferences is helpful to ensure preference satisfaction. To demonstrate this, we conduct experiments with more flexible preferences; see the results in Figure 6, where the obtained solutions align better with the preference lines compared to those in Figure 5. Moreover, it can perform controlled ascent updates during optimization, which cannot be achieved by PMTL or XWC-MGDA.

Multi-lingual speech recognition.We further apply the proposed method to the multi-objective finetuning of pre-trained multi-lingual speech models. We use the Librispeech (100 hours) [47], and AISHELL v1 [5] datasets for multi-lingual speech recognition. A conformer with 8 blocks is used as the model architecture. The total number of parameters is around 64.5M with 58.4M encoder layer parameters and the rest being the classification layer parameters. We consider the objectives associated with the speech recognition Connectionist Temporal Classification (CTC) losses in Chinese and English, denoted as \(f_{t}^{\mathrm{ch}}\) and \(f_{t}^{\mathrm{en}}\), respectively. We also use the self-supervised Contrastive Predictive Coding (CPC) loss \(f_{p}\) for representation learning; that is

\[\min_{\theta}\ \ \ F(\theta):=\left(f_{p}(\theta),f_{t}^{\mathrm{ch}}(\theta),f_{t} ^{\mathrm{en}}(\theta)\right)^{\top}\ \ \mathrm{s.t.}\ \ f_{p}(\theta)\leq\epsilon_{1},\ f_{t}^{\mathrm{ch}}(\theta)-f_{t}^{\mathrm{en}}(\theta)=\epsilon_{2} \tag{5.2}\]

where the first constraint ensures to learn a good representation with \(\epsilon_{1}=1.2\), and the second constraint avoids one language loss dominates the other with \(\epsilon_{2}=0.5\); see more details in Appendix H.1.

Results on the word error rate (WER) are reported in Table 3. The baselines include the state-of-the-art result from Komatsu et al. [29] without an additional large language model, our own implementation of training using only the sum of supervised CTC losses (w/o CPC), the initial pre-trained M2ASR model [51] (init.), linear scalarization of all three objectives for finetuning a pre-trained model with the CPC loss (LS-FT). Results show that considering CPC loss besides the supervised CTC loss improves the average WER by 4.2%, and this can be further improved by 0.3% by finetuning with linear scalarization. However, the LS-FT model has a much better performance in Chinese compared to English. With our proposed approach, the performance gap between different languages is reduced, and the average WER is further improved by 1.3%.

## 6 Conclusions

In this work, we frame preference-guided multi-objective learning as a constrained vector optimization problem. Specifically, we introduce constraints and partial order to capture the absolute and relative preferences. Under this framework, we develop algorithms to solve the constrained vector optimization problem. Our proposed algorithms use a unified formulation without solving different subprograms at different stages. And they enjoy the benefit of allowing controlled ascent and escaping weak optimal solutions. Theoretical guarantees on the non-asymptotic convergence of the deterministic algorithms and their stochastic variants are provided. Experiments on benchmark datasets demonstrate the broad applicability of the proposed algorithms.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline Method & English & Chinese & Average \\ \hline Komatsu et al. [29] & 7.11 & - & - \\ w/o CPC [51] & 11.8 & 10.2 & 11.0 \\ Init. (M2ASR) [51] & 7.3 & 6.2 & 6.7 \\ LS-FT & 6.8 & 5.9 & 6.4 \\ FERERO-FT & **5.4** & **4.9** & **5.1** \\ \hline \hline \end{tabular}
\end{table}
Table 3: WERs (%) on Librispeech and AISHELL v1.

Figure 6: Losses and preferences of FERERO when the initial objective is close to the Pareto front.

## Broader Impacts and Limitations

This paper casts the preference-guided multi-objective learning as a constrained vector optimization problem and proposes an algorithm with single-loop and stochastic variants to solve the problem, which have non-asymptotic convergence guarantees. The proposed method is applied to image classification and speech recognition. The positive impact is that it is a principled method with efficient implementations that has broad applications across various domains. There is no negative social impact.

The proposed algorithm is able to model flexible preferences but at a cost of higher per-iteration complexity compared to scalarization methods. The theoretical guarantees make standard assumptions that the objectives are lower bounded, Lipschitz continuous and smooth. These are common assumptions in the optimization literature, and can be satisfied for neural networks with smooth activation functions.

## Acknowledgements

The work of L. Chen, AFM Saif, and T. Chen was supported by the National Science Foundation (NSF) projects 2401297, 2412486, the RPI-IBM Artificial Intelligence Research Collaboration (AIRC), the Cisco Research Award, and the IEEE Signal Processing Society scholarship. The work of Y. Shen was supported by NSF ECCS-2412484. We also thank Quan Xiao, Prof. Luis Nunes Vicente, Prof. Rongjie Lai for inspiring and helpful discussions, and the anonymous reviewers for their constructive feedback to improve our paper.

## References

* [1] Jaqueline S Angelo, Isabella A Guedes, Helio JC Barbosa, and Laurent E Dardenne. Multi-and many-objective optimization: present and future in de novo drug design. _Frontiers in Chemistry_, 11, 2023.
* [2] Cagin Ararat and Cem Tekin. Vector optimization with stochastic bandit feedback. In _Proc. International Conference on Artificial Intelligence and Statistics_, pages 2165-2190, Valencia, Spain, 2023.
* [3] Peter Auer, Chao-Kai Chiang, Ronald Ortner, and Madalina Drugan. Pareto front identification from stochastic bandit feedback. In _Proc. International Conference on Artificial Intelligence and Statistics_, pages 939-947, Cadiz, Spain, 2016.
* [4] Dimitri Bertsekas. _Constrained Optimization and Lagrange Multiplier Methods (Optimization and Neural Computation Series)_. Athena Scientific, 1996.
* [5] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline. In _Conference of the oriental chapter of the international coordinating committee on speech databases and speech I/O systems and assessment_, pages 1-5, 2017.
* [6] Richard H Byrd, Frank E Curtis, and Jorge Nocedal. An inexact sqp method for equality constrained optimization. _SIAM Journal on Optimization_, 19(1):351-369, 2008.
* [7] Lisha Chen, Heshan Fernando, Yiming Ying, and Tianyi Chen. Three-way trade-off in multi-objective learning: Optimization, generalization and conflict-avoidance. _Journal of Machine Learning Research_, 2024.
* [8] Wang Chen, Xinmin Yang, and Yong Zhao. Conditional gradient method for vector optimization. _Computational Optimization and Applications_, 85(3):857-896, July 2023.
* [9] Frank E Curtis, Suyun Liu, and Daniel P Robinson. Fair machine learning through constrained stochastic optimization and an epsilon-constraint method. _Optimization Letters_, pages 1-17, 2023.
* [10] Dmitriy Drusvyatskiy and Adrian S Lewis. Error bounds, quadratic growth, and linear convergence of proximal methods. _Mathematics of Operations Research_, 43(3):919-948, 2018.

* [11] Jean-Antoine Desideri. Multiple-gradient Descent Algorithm (MGDA) for Multi-objective Optimization. _Comptes Rendus Mathematique_, 350(5-6), 2012.
* [12] Matthias Ehrgott. _Multicriteria optimization_. Springer, Berlin; New York, 2nd ed edition, 2005.
* [13] Heshan Fernando, Lisha Chen, Songtao Lu, Pin-Yu Chen, Miao Liu, Subhajit Chaudhury, Keerthiram Murugesan, Gaowen Liu, Meng Wang, and Tianyi Chen. Variance reduction can improve trade-off in multi-objective learning. In _Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing_, pages 6975-6979, 2024.
* [14] Heshan Fernando, Han Shen, Miao Liu, Subhajit Chaudhury, Keerthiram Murugesan, and Tianyi Chen. Mitigating gradient bias in multi-objective learning: A provably convergent stochastic approach. In _Proc. International Conference on Learning Representations_, Kigali, Rwanda, May 2023.
* [15] Jorg Fliege and Benar Fux Svaiter. Steepest descent methods for multicriteria optimization. _Mathematical methods of operations research_, 51:479-494, 2000.
* [16] Jorg Fliege and A. Ismael F. Vaz. A method for constrained multiobjective optimization based on sqp techniques. _SIAM Journal on Optimization_, 26(4):2091-2119, 2016.
* [17] Jorg Fliege, A Ismael F Vaz, and Luis Nunes Vicente. Complexity of Gradient Descent for Multi-objective Optimization. _Optimization Methods and Software_, 34(5):949-959, 2019.
* [18] Ellen H. Fukuda and L. M. Grana Drummond. On the convergence of the projected gradient method for vector optimization. _Optimization_, 60(8-9):1009-1021, 2011.
* [19] Ellen H. Fukuda and L. M. Grana Drummond. Inexact projected gradient method for vector optimization. _Computational Optimization and Applications_, 54:473-493, 2013.
* [20] Bennet Gebken, Sebastian Peitz, and Michael Dellnitz. A descent method for equality and inequality constrained multiobjective optimization problems. In _Numerical and Evolutionary Optimization_, pages 29-61. Springer, 2019.
* [21] Philip E Gill, Walter Murray, and Michael A Saunders. Snopt: An sqp algorithm for large-scale constrained optimization. _SIAM review_, 47(1):99-131, 2005.
* [22] Chengyue Gong, Xingchao Liu, and Qiang Liu. Automatic and harmless regularization with constrained and lexicographic optimization: A dynamic barrier approach. In _Proc. Advances in Neural Information Processing Systems_, volume 34, pages 29630-29642, virtual, 2021.
* [23] L. M. Grana Drummond, A. N. Iusem, and B. F. Svaiter. On first order optimality conditions for vector optimization. _Acta Mathematicae Applicatae Sinica, English Series_, 19(3), September 2003.
* [24] L. M. Grana Drummond and A.N. Iusem. A projected gradient method for vector optimization problems. _Computational Optimization and Applications_, 28:5-29, April 2004.
* [25] L. M. Grana Drummond and B.F. Svaiter. A steepest descent method for vector optimization. _Journal of Computational and Applied Mathematics_, 175(2):395-414, March 2005.
* [26] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for speech recognition. _arXiv preprint arXiv:2005.08100_, 2020.
* [27] Johannes Jahn. _Vector Optimization: Theory, Applications, and Extensions_. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.
* [28] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-lojasiewicz condition. _arXiv preprint arXiv:1608.04636_, 2016.
* [29] Tatsuya Komatsu, Yusuke Fujita, Jaesong Lee, Lukas Lee, Shinji Watanabe, and Yusuke Kida. Better intermediates improve CTC inference. _arXiv preprint arXiv:2204.00176_, 2022.

* [30] Panagiotis Kyriakis, Jyotirmoy Deshmukh, and Paul Bogdan. Pareto policy adaptation. In _Proc. International Conference on Learning Representations_, virtual, 2021.
* [31] Xi Lin, Zhiyuan Yang, Xiaoyuan Zhang, and Qingfu Zhang. Pareto set learning for expensive multi-objective optimization. In _Proc. Advances in Neural Information Processing Systems_, volume 35, New Orleans, LA, December 2022.
* [32] Xi Lin, Xiaoyuan Zhang, Zhiyuan Yang, Fei Liu, Zhenkun Wang, and Qingfu Zhang. Smooth tchebycheff scalarization for multi-objective optimization. _arXiv preprint arXiv:2402.19078_, 2024.
* [33] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task learning. In _Proc. Advances in Neural Information Processing Systems_, Vancouver, Canada, December 2019.
* [34] Bo Liu, Yihao Feng, Peter Stone, and Qiang Liu. Famo: Fast adaptive multitask optimization. In _Proc. Advances in Neural Information Processing Systems_, volume 36, New Orleans, LA, 2023.
* [35] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-Averse Gradient Descent for Multi-task Learning. In _Proc. Advances in Neural Information Processing Systems_, virtual, December 2021.
* [36] Suyun Liu and Luis Nunes Vicente. The Stochastic Multi-gradient Algorithm for Multi-objective Optimization and its Application to Supervised Machine Learning. _Annals of Operations Research_, pages 1-30, 2021.
* [37] Suyun Liu and Luis Nunes Vicente. Accuracy and fairness trade-offs in machine learning: A stochastic multi-objective approach. _Computational Management Science_, 19(3):513-537, 2022.
* [38] Xingchao Liu, Xin Tong, and Qiang Liu. Profiling Pareto Front With Multi-Objective Stein Variational Gradient Descent. In _Proc. Advances in Neural Information Processing Systems_, virtual, December 2021.
* [39] David G. Luenberger and Yinyu Ye. _Linear and Nonlinear Programming_, volume 116 of _International Series in Operations Research & Management Science_. Springer US, New York, NY, 2008.
* [40] Sohvi Luukkonen, Helle W. van den Maagdenberg, Michael T.M. Emmerich, and Gerard J.P. van Westen. Artificial intelligence in multi-objective drug design. _Current Opinion in Structural Biology_, 79:102537, 2023.
* [41] Debabrata Mahapatra and Vaibhav Rajan. Multi-task learning with user preferences: Gradient descent with controlled ascent in pareto optimization. In _Proc. International Conference on Machine Learning_, virtual, 2020.
* [42] Natalia Martinez, Martin Bertran, and Guillermo Sapiro. Minimax pareto fairness: A multi objective perspective. In _Proc. International Conference on Machine Learning_, pages 6755-6764, virtual, 2020.
* [43] Kaisa Miettinen. _Nonlinear Multiobjective Optimization_, volume 12. Springer US, Boston, MA, 1998.
* [44] Michinari Momma, Chaosheng Dong, and Jia Liu. A multi-objective/multi-task learning framework induced by pareto stationarity. In _Proc. International Conference on Machine Learning_, Baltimore, MD, 2022.
* [45] Aviv Navon, Aviv Shamsian, Ethan Fetaya, and Gal Chechik. Learning the pareto front with hypernetworks. In _Proc. International Conference on Learning Representations_, virtual, April 2020.
* [46] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.

* [47] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an ASR corpus based on public domain audio books. In _Proc. International Conference on Acoustics, Speech and Signal Processing_, pages 5206-5210, 2015.
* [48] Javier Pena, Juan C. Vera, and Luis F. Zuluaga. New characterizations of hoffman constants for systems of linear constraints. _Mathematical Programming_, 187(1):79-109, 2021.
* [49] Hoang Phan, Ngoc Tran, Trung Le, Toan Tran, Nhat Ho, and Dinh Phung. Stochastic multiple target sampling gradient descent. In _Proc. Advances in Neural Information Processing Systems_, New Orleans, LA, December 2022.
* [50] Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alex Smola. Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization. In _Proc. Advances in Neural Information Processing Systems_, volume 29, 2016.
* [51] A F M Saif, Lisha Chen, Xiaodong Cui, Songtao Lu, Brian Kingsbury, and Tianyi Chen. M2ASR: Multilingual multi-task automatic speech recognition via multi-objective optimization. In _Interspeech 2024_, pages 1240-1244, 2024.
* [52] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. In _Proc. Advances in Neural Information Processing Systems_, Montreal, Canada, December 2018.
* [53] Han Shen, Quan Xiao, and Tianyi Chen. On penalty-based bilevel gradient descent method. _arXiv preprint arXiv:2302.05185_, 2023.
* [54] Hiroki Tanabe, Ellen H. Fukuda, and Nobuo Yamashita. Proximal gradient methods for multi-objective optimization and their applications. _Computational Optimization and Applications_, 72(2):339-361, 2019.
* [55] Peiyao Xiao, Hao Ban, and Kaiyi Ji. Direction-oriented multi-objective learning: Simple and provable stochastic algorithms. In _Proc. Advances in Neural Information Processing Systems_, New Orleans, LA, 2023.
* [56] Yijun Yang, Jing Jiang, Tianyi Zhou, Jie Ma, and Yuhui Shi. Pareto policy pool for model-based offline reinforcement learning. In _Proc. International Conference on Learning Representations_, virtual, 2021.
* [57] Jane J Ye and Qiji J Zhu. Multiobjective optimization problem with variational inequality constraints. _Mathematical Programming_, 96(1):139-160, 2003.
* [58] Yiming Ying and Ding-Xuan Zhou. Unregularized online learning algorithms with general loss functions. _Applied and Computational Harmonic Analysis_, 42(2):224-244, 2017.
* [59] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. In _Proc. Advances in Neural Information Processing Systems_, virtual, December 2020.
* [60] Shiji Zhou, Wenpeng Zhang, Jiyan Jiang, Wenliang Zhong, Jinjie Gu, and Wenwu Zhu. On the convergence of stochastic multi-objective gradient manipulation and beyond. In _Proc. Advances in Neural Information Processing Systems_, volume 35, pages 38103-38115, New Orleans, LA, December 2022.
* [61] Yiheng Zhu, Jialu Wu, Chaowen Hu, Jiahuan Yan, Tingjun Hou, Jian Wu, et al. Sample-efficient multi-objective molecular optimization with gflownets. In _Proc. Advances in Neural Information Processing Systems_, volume 36, New Orleans, LA, 2023.

**Appendix for "FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning "**

## Appendix A Notations

A summary of notations used in this work is listed in Table 4 for ease of reference.

Recall that given vectors \(v,w\), we use \(v<w\) and \(v\leq w\) to denote \(v_{i}<w_{i}\) for all \(i\), and \(v_{i}\leq w_{i}\) for all \(i\), respectively. We use \(v\leq w\) to denote \(v\leq w\) and \(v\neq w\), and define \(>,\geq,\geq\) analogously. In the proof, we use \(\|\cdot\|\) to denote the \(\ell_{2}\)-norm, and \(\|\cdot\|_{1}\) to denote the \(\ell_{1}\)-norm. We use \(|\cdot|_{\mathrm{ab}}\) to denote the operator that takes element-wise absolute value of a matrix. We use \(\mathbf{1}\) and \(0\) to denote the all-one and all-zero vectors, respectively. Their dimensions are specified only when they are not clear in the context. We use \([v,w]\) to represent column concatenation of matrices or vectors, and use \([v;w]\) to represent row concatenation of matrices or vectors.

## Appendix B Related Works and Comparison

In this section, we provide a detailed review and comparison of additional related works in multi-task/objective learning, vector optimization, and Pareto front approximation.

[MISSING_PAGE_FAIL:16]

models. The learned neural network is able to generate different models with different input user preferences [45, 56, 30, 31]. Although we do not focus on Pareto front approximation in this work, our algorithm can be applied to generate different models based on different diverse preferences to approximate the Pareto front, as in [33].

### A detailed comparison with existing works

Preferences as linear constraints of objectives.Different constraints \(S\) partition the objectives into sub-regions, as shown in Figure 1. Many preferences can be modeled by linear equality or inequality constraints [33, 41, 44]. For example, below we list different choices of \(C\) for different methods in Figure 1.

1. \(B_{g}=[0,I_{2:M}]^{\top}\in\mathbb{R}^{M\times M},b=-[0,\epsilon_{2},\ldots, \epsilon_{M}]^{\top}\);
2. \(B_{h}\in\mathbb{R}^{(M-1)\times M},b=0\);

In Figure 0(a), the preferences are based on the function values of \(f_{1}\) controlled by different thresholds, corresponding to the inequality constraints defined by (a). In Figure 0(b), the constraints are that the objectives \(F(\theta)\) should lie on one of the preference vectors \(v\), therefore should satisfy the equality constraint \(B_{h}F(\theta)=0\).

Detailed comparison with the most relevant works.Below we provide a fine-grained comparison with some existing works in Table 5, as an extension of Table 1.

In terms of preference modeling, the scalarization-based methods such as Linear Scalarization and Smooth Tchebycheff scalarization use weight of different objectives to model preferences. They are not flexible enough to capture preferences illustrated in Figure 1. PMTL uses a constrained multi-objective optimization formulation, with preferences modeled by inequalities. EPO models the preference by an \(r^{-1}\) ray, same as the example given in Figure 0(b). (X)WC-MGDA uses a shifted ray not necessarily from the origin to model the preferences. In all of these works, they only model the absolute preferences that define the preferred objective values. In contrast, we also consider the relative preference that define the relative improvement directions of objectives.

In addition to the comparison in Table 1, our framework enjoys additional benefits including the ability to escape weak optimal solutions and to maintain scale-invariance. These abilities are attributed to the subprogram that is adaptive to the objective values, as detailed in Lemma 6.

Below, we further summarize the reasons behind the benefits of our proposed method. We use "\(\rightarrow\)" to indicate the reasons on the left and the corresponding benefits on the right.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline Method & \begin{tabular}{c} Handle \\ nonconvex PF \\ \end{tabular} & \begin{tabular}{c} General \\ partial order \\ \end{tabular} & \begin{tabular}{c} Single subprogram \\ w/o computing active index \\ \end{tabular} & \begin{tabular}{c} Scale \\ invariance \\ \end{tabular} & \begin{tabular}{c} Escape weak \\ optimal \\ \end{tabular} & 
\begin{tabular}{c} Provable \\ CQ \\ \end{tabular} \\ \hline Linear Scalarization & ✗ & ✗ & ✓ & ✗ & ✗ & - \\ (Smooth Tchebycheff [32]) & ✓ & ✗ & ✓ & ✗ & ✗ & \(\rightarrow\) \\ PMTL [3] & ✓ & ✗ & ✗ & ✗ & ✗ & assume LICQ \\ EPO [41] & ✓ & ✗ & ✗ & ✓ & ✗ \\ (OVKC-MGDA) [44] & ✓ & ✗ & ✗ & ✗ & ✓ & ✗ \\ FERERO (ours) & ✓ & ✓ & ✓ & ✓ & ✓ & prove calmness \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison to existing PMOL methods, extension of Table 1.

Preliminaries

In this section we introduce preliminaries on the general cone-induced partial ordering and the corresponding optimality conditions for completeness since we use these concepts in our proofs. Then we discuss the relation between the Pareto optimality and the optimality induced by a general polyhedral cone.

### General cone-induced partial ordering

In this section, we introduce basic definitions, lemmas, propositions, and theorems in vector optimization, including the cone-induced partial ordering, the minimum and weakly minimum associated with the partial ordering in real linear space, and necessary conditions for minimum. These concepts are defined in [27]. We restate them following our notations for completeness. We denote \(Z\) as a real linear space, \(C,S\) as subsets in \(Z\), and \(w,x,y,z\) as points or elements in \(Z\), \(0_{Z}\) as the zero vector in the space \(Z\).

**Definition 4** (Cone).: _Let \(C\) be a nonempty subset of a real linear space \(Z\). The set \(C\) is called a cone, if \(y\in C,\lambda\geq 0\Longrightarrow\lambda y\in C\)._

**Lemma 3** (Convex cone).: _A cone \(C\) in a real linear space is convex if and only if \(C+C\subset C\)._

**Definition 5** (Partially ordered linear space).: _A real linear space equipped with a partial ordering is a partially ordered linear space._

**Proposition 1**.: _(a) If \(\leq\) is a partial ordering on \(Z\), then the set \(C:=\{z\in Z\mid 0_{Z}\leq z\}\) is a convex cone. If, in addition, \(\leq\) is antisymmetric, then \(C\) is pointed._

_(b) If \(C\) is a convex cone in \(Z\), then the binary relation \(\leq_{C}:=\{(x,y)\in Z\times Z\mid y-x\in C\}\) is a partial ordering on \(Z\). If, in addition, \(C\) is pointed, then \(\leq_{C}\) is antisymmetric._

**Definition 6** (Ordering cone).: _A convex cone characterizing a partial ordering in a real linear space is an ordering cone._

**Definition 7** (Cone-induced partial ordering).: _Let \(C\) be a closed pointed convex cone of \(\mathbb{R}^{M}\), with nonempty interior. The partial order in \(\mathbb{R}^{M}\) induced by \(C,\leq_{C}\) is defined by_

\[u\leq_{C}v,\ \ \text{if}\ \ v-u\in C.\] (C.1)

_The relation induced by \(\operatorname{int}(C)\) in \(\mathbb{R}^{M}\), \(<_{C}\) is defined by_

\[u<_{C}v,\ \ \text{if}\ \ v-u\in\operatorname{int}(C).\] (C.2)

**Definition 8** (\(C\)-minimum and \(C\)-weakly minimum).: _Let \(S\) be a nonempty subset of a partially ordered linear space with an ordering cone \(C\), then_

_(a) an element_ \(z\in S\) _is called a_ \(C\)_-minimum of the set_ \(S\)_, if_ \((\{z\}-C)\cap S\subset\{z\}+C\)_, in other words, there exists no other_ \(z^{\prime}\in S\) _with_ \(z^{\prime}\leq_{C}z\) _and_ \(z^{\prime}\neq z\)_;_

_(b) an element_ \(z\in S\) _is called a_ \(C\)_-weakly minimum of the set_ \(S\)_, if_ \((\{z\}-\operatorname{int}(C))\cap S=\emptyset\)_, where_ \(\operatorname{int}(C)\neq\emptyset\) _is the algebraic interior of_ \(C\)_, in other words, there exists no other_ \(z^{\prime}\in S\) _with_ \(z^{\prime}<_{C}z\) _and_ \(z^{\prime}\neq z\)_._

**Definition 9** (\(C\)-stationary).: _A point \(\theta\in\mathbb{R}^{q}\) is \(C\)-stationary if there is no first-order common descent direction \(d\in\mathbb{R}^{q}\) that \(\nabla F(\theta)^{\top}d\in-\operatorname{int}(C)\), i.e., \(\operatorname{range}(\nabla F(\theta)^{\top})\cap(-\operatorname{int}(C))=\emptyset\)._

### Necessary and sufficient conditions for \(C\)-optimality

Note that, when \(C=\mathbb{R}^{M}_{+}:=\{z\in\mathbb{R}^{M}\mid z_{m}\geq 0\text{ for all }m\in[M]\}\), \(C\)-minimum and \(C\)-weakly minimum in Definition 8 are Pareto minimum and weakly Pareto minimum, respectively. Recall that \(F:\mathbb{R}^{q}\rightarrow\mathbb{R}^{M}\) is a continuously differentiable function. The problem we consider is to find the unconstrained \(C\)-minimizers of \(F\), denoted as \(\min_{C}F(\theta)\) with \(\theta\in\mathbb{R}^{q}\). We then proceed to introduce the relation between \(C\)-stationarity and Pareto stationarity in this section.

**Proposition 2**.: _Let \(C\) be a closed convex pointed cone. 1) Suppose \(C\subseteq\mathbb{R}^{M}_{+}\). If \(\theta\) is Pareto stationary, \(\theta\) is \(C\)-stationary. In other words, \(C\)-stationarity is a necessary condition for Pareto stationarity. 2) Suppose \(\mathbb{R}^{M}_{+}\subseteq C\). if \(\theta\) is \(C\)-stationary, \(\theta\) is Pareto stationary. In other words, \(C\)-stationarity is a sufficient condition for Pareto stationarity._Proof of Proposition 2.: 1) By definition, if \(\theta\) is Pareto stationary, then \(\operatorname{range}(\nabla F(\theta)^{\top})\cap(-\mathrm{int}(\mathbb{R}_{+}^{M} ))=\emptyset\). Since \(C\subseteq\mathbb{R}_{+}^{M}\), then \(-\mathrm{int}(C)\subseteq-\mathrm{int}(\mathbb{R}_{+}^{M})\), and we have

\[\operatorname{range}(\nabla F(\theta)^{\top})\cap(-\mathrm{int}(C))\subseteq \operatorname{range}(\nabla F(\theta)^{\top})\cap(-\mathrm{int}(\mathbb{R}_{+} ^{M}))=\emptyset.\] (C.3)

Therefore, \(\theta\) is \(C\)-stationary.

Following similar arguments, 2) can also be proved. 

## Appendix D Proof of Auxiliary Lemmas

In this section, we provide proof of the main theoretical results in this paper.

### Lagrangian of the subprogram

Proof of subprogram reformulation.: Define the Lagrangian function

\[L(c,d,\lambda_{f},\lambda_{g},\lambda_{h})\coloneqq c+\frac{1}{2}\|d\|^{2}+\lambda_{f}^{\top}\big{(}A\nabla F(\theta)^{ \top}d-c(\mathbf{1}^{\top}AF(\theta))^{-1}AF(\theta)\big{)}\] \[+\lambda_{g}^{\top}\big{(}B_{g}\nabla F(\theta)^{\top}d+c_{g}G( \theta)\big{)}+\lambda_{h}^{\top}\big{(}B_{h}\nabla F(\theta)^{\top}d+c_{h}H( \theta)\big{)}\] (D.1)

where \(\lambda_{f}\in\mathbb{R}_{+}^{M}\), \(\lambda_{g}\in\mathbb{R}_{+}^{M_{g}}\), \(\lambda_{h}\in\mathbb{R}^{M_{h}}\). By the first-order optimality condition w.r.t. \(d\) and \(c\), we can obtain that

\[d^{*}+\nabla F(\theta)(A^{\top}\lambda_{f}^{*}+B_{g}^{\top}\lambda _{g}^{*}+B_{h}^{\top}\lambda_{h}^{*}) =0;\] (D.2) \[\mathbf{1}^{\top}AF(\theta)-\lambda_{f}^{\top}AF(\theta) =0.\] (D.3)

Combining the last equation with \(\lambda_{f}\in\mathbb{R}_{+}^{M}\), we obtain \(\lambda_{f}^{*}\in\Omega_{\lambda_{f}}(\theta)\). Plugging the above results into the Lagrangian function gives

\[[\lambda_{f}^{*};\lambda_{g}^{*};\lambda_{h}^{*}]\in\operatorname {arg\,min}_{\{\lambda_{f};\lambda_{g};\lambda_{h}\}\in\Omega_{\lambda}(\theta) }\frac{1}{2}\|\nabla F(\theta)(A^{\top}\lambda_{f}+B_{g}^{\top} \lambda_{g}+B_{h}^{\top}\lambda_{h})\|^{2}\] \[-c_{g}\lambda_{g}^{\top}G(\theta)-c_{h}\lambda_{h}^{\top}H(\theta)\] (D.4)

which leads to the dual form in (2.3). Since (2.1) is a constrained convex optimization problem where the Slater's condition holds, therefore, the duality gap is zero. 

**Remark 4**.: _Note that we can also have a simplified subprogram with \(A=I\), and without adaptation to the objective values, as defined below_

\[\psi(\theta)\coloneqq \min_{(d,c)\in\mathbb{R}^{q}\times\mathbb{R}}c+\frac{1}{2}\|d\|^{2} \ \ \mathrm{s.t.}\ \ \nabla F(\theta)^{\top}d\leq c\mathbf{1}\] (D.5) \[\nabla G(\theta)^{\top}d+c_{g}G(\theta)\leq 0,\ \nabla H(\theta)^{ \top}d+c_{h}H(\theta)=0.\]

_This formulation corresponds to the SQP method applied to the constrained MOO problem [16]. Then the corresponding Lagrangian function becomes_

\[L(c,d,\lambda_{f},\lambda_{g},\lambda_{h})\coloneqq c+\frac{1}{2}\|d\|^{2}+\lambda_{f}^{\top}\big{(}\nabla F(\theta)^{\top}d-c \mathbf{1}\big{)}\] \[+\lambda_{g}^{\top}\big{(}B_{g}\nabla F(\theta)^{\top}d+c_{g}G( \theta)\big{)}+\lambda_{h}^{\top}\big{(}B_{h}\nabla F(\theta)^{\top}d+c_{h}H( \theta)\big{)}.\] (D.6)

_By the first-order optimality condition w.r.t. \(c\), (D.3) can be replaced by_

\[1-\lambda_{f}^{*\top}\mathbf{1}=0.\] (D.7)

_And the rest results remain the same, i.e., (D.2) and (D.4) still hold, while \(\Omega_{\lambda_{f}}=\Delta^{M}\)._

### First-order necessary optimality conditions

We then discuss the first-order necessary optimality conditions for problem (PMOL). We begin the discussion with the geometric notions of improving and feasible directions.

Improving directions.The improvement directions are defined as generalized common descent directions so that the iterates strictly improve or dominate the previous iterates based on \(C_{A}\), i.e., \(F(\theta_{t})-F(\theta_{t+1})\in\operatorname{int}(C_{A})\). Denote \(d_{t}\in\mathbb{R}^{q}\) as an update direction at iteration \(t\), and \(\alpha_{t}>0\) as the step size at the \(t\)-th iteration. The general update equation given update direction \(d_{t}\) is \(\theta_{t+1}=\theta_{t}+\alpha_{t}d_{t}\). Based on first-order Taylor expansion, the amount of improvement at iteration \(t\) can be approximately expressed as \(F(\theta_{t})-F(\theta_{t+1})\approx-\alpha_{t}\nabla F(\theta_{t})^{\top}d_{t} \in\operatorname{int}(C_{A})\). We term such directions the general \(C_{A}\)-improving directions. The cone of \(C_{A}\)-improving directions at \(x\) is

\[D_{C_{A}}=\{d\in\mathbb{R}^{q}\mid\nabla F(\theta)^{\top}d\in- \operatorname{int}(C_{A})\}.\] (D.8)

When \(A=I_{M}\), they are common descent directions.

Feasible directions.Similar to the concept in constrained single objective optimization, the feasible directions are those that ensure \(F(\theta_{t}+\alpha_{t}d_{t})\in S\). We rewrite problem (PMOL) with explicit \(C_{A}\)-induced partial ordering as

\[\min\,_{C_{A}}\ F(\theta)\ \operatorname{s.t.}\ G(\theta)\leq 0,\ H(\theta)=0.\] PMOL

where \(G:\mathbb{R}^{q}\rightarrow\mathbb{R}^{M_{g}},H:\mathbb{R}^{q}\rightarrow \mathbb{R}^{M_{h}}\) are linear functions of \(F\), and are differentiable. Let \(I=\{i\mid G_{i}(\theta)=0\}\) be the index set of the active inequality constraints in \(G(\theta)\), and \(G_{I}(\theta)=[\cdots,G_{i}(\theta),\cdots]^{\top}\) for \(i\in I\). A subset of the feasible directions described by the gradients of the equality and active inequality constraints at \(\theta\) is given by

\[D_{g}=\{d\in\mathbb{R}^{q}\mid\nabla G_{I}(\theta)^{\top}d<0\}, \quad D_{H}=\{d\in\mathbb{R}^{q}\mid\nabla H(\theta)^{\top}d=0\}.\] (D.9)

A necessary optimality condition is that there exists no feasible and improving directions at \(\theta\), i.e., \(D_{C_{A}}\cap D_{g}\cap D_{h}=\emptyset\). An algebraic description of the necessary optimality conditions for (PMOL) is summarized below.

**Proposition 3** (First-order necessary optimality conditions for (PMOL)).: _Let \(C_{A}\coloneqq\{y\in\mathbb{R}^{M}\mid Ay\geq 0\}\) that satisfies \(\operatorname{int}(C_{A})\neq\emptyset\). If \(\bar{\theta}\) solves (PMOL) locally, then there exists \(\lambda_{f}\in\mathbb{R}^{M}_{+}\), \(\lambda_{g}\in\mathbb{R}^{M_{g}}_{+}\), \([\lambda_{f};\lambda_{g}]\neq 0\), and \(\lambda_{h}\in\mathbb{R}^{M_{h}}\) that_

\[\nabla F(\bar{\theta})A^{\top}\lambda_{f}+\nabla G(\bar{\theta}) \lambda_{g}+\nabla H(\bar{\theta})\lambda_{h}=0,\ \text{ and }\ \lambda_{g}^{ \top}[-G(\bar{\theta})]_{+}=0\] (D.10)

Proof of Proposition 3.: The geometric description \(D_{C_{A}}\cap D_{g}\cap D_{h}=\emptyset\) is equivalent to that the linear system below w.r.t. \(d\) is inconsistent

\[\begin{bmatrix}A\nabla F(\bar{\theta})^{\top}\\ \nabla G_{I}(\bar{\theta})^{\top}\end{bmatrix}d<0\ \text{ and }\ \nabla H(\bar{ \theta})^{\top}d=0.\] (D.11)

By the Motzkin's transposition theorem, system (D.11) being inconsistent is equivalent to that the following linear system w.r.t. \(p\), \(\lambda_{h}\) has a solution with \(p\geq 0\)

\[\begin{bmatrix}\nabla F(\bar{\theta})A^{\top}&\nabla G_{I}(\bar{ \theta})\end{bmatrix}p+\nabla H(\bar{\theta})\lambda_{h}=0.\] (D.12)

Letting \(p=[\lambda_{f};\lambda_{g,I}]\), where \(\lambda_{g,I}=[\cdots;\lambda_{g,i};\cdots],i\in I\), and \(\lambda_{g,i^{\prime}}=0\), for all \(i^{\prime}\notin I\) completes the proof. 

**Remark 5**.: _Notice that, Proposition 3 provides a Fritz John (FJ)-type first-order necessary optimality condition, which has been discussed in prior works such as [57, Theorem 1.2] with additional variational inequality constraints, and [23, Section 3, (2)-(5)] with inequality constraints only. We provide the derivation for our problem here for completeness. In the FJ-type necessary optimality condition, the multiplier \(\lambda_{f}\) associated with the objective \(F(\theta)\) can be zero if \(|I|\geq 1\), which is undesirable. We need additional constraint qualifications to ensure the condition in (D.10) with \(\lambda_{f}\neq 0\), i.e., the KKT condition, is also a necessary optimality condition. This is equivalent to \(\mu_{0}=1\), and without considering the variational inequality constraints in [57, Theorem 1.2]. The constraint qualification is discussed in detail in Appendix D.3.2._

### Properties of PMOL

In this section, we discuss the properties of PMOL and their proofs. These include the properties of the subprogram in Lemma 1, and the calmness CQ of PMOL in Lemma 2.

[MISSING_PAGE_FAIL:21]

where by the feasibility and optimality conditions,

\[\lambda_{h}^{*\top}\big{(}B_{h}\nabla F(\theta)^{\top}d^{*}(\theta)+c _{h}H(\theta)\big{)}= 0,\] (D.20a) \[\lambda_{g}^{*\top}\big{(}B_{g}\nabla F(\theta)^{\top}d^{*}(\theta) +c_{g}G(\theta)\big{)}= 0,\] (D.20b) \[\lambda_{f}^{*\top}\big{(}A\nabla F(\theta)^{\top}d^{*}(\theta)-c ^{*}(\mathbf{1}_{M}^{\top}AF(\theta))^{-1}AF(\theta)\big{)}= 0.\] (D.20c)

Combining the above with (D.19), we have

\[\|d^{*}(\theta)\|^{2}= -d^{*}(\theta)^{\top}\nabla F(\theta)\Big{(}A^{\top}\lambda_{f}^ {*}+B_{g}^{\top}\lambda_{g}^{*}+B_{h}^{\top}\lambda_{h}^{*}\Big{)}\] \[= -d^{*}(\theta)^{\top}\nabla F(\theta)A^{\top}\lambda_{f}^{*}+c_{ h}\lambda_{h}^{*\top}H(\theta)+c_{g}{\lambda_{g}^{*}}^{\top}G(\theta)\] \[\leq -c^{*}(\theta)(\mathbf{1}^{\top}AF(\theta))^{-1}{\lambda_{f}^{*} }^{\top}AF(\theta)=-c^{*}(\theta)\] (D.21)

where the last inequality uses the fact that \(\theta\) is feasible, and \(G(\theta)\leq 0\), \(H(\theta)=0\).

Then it holds that

\[2\psi(\theta)=2c^{*}(\theta)+\|d^{*}(\theta)\|^{2}\leq-\|d^{*}( \theta)\|^{2}<0.\] (D.22)

Therefore, Property-2 holds.

For **Property-3**, let \(I\subseteq[M]\) be the set such that \((AF(\theta))_{m}=0\) for all \(m\in I\), then (2.1) is equivalent to

\[\psi(\theta)= \min_{(d,c)\in\mathbb{R}^{n}\times\mathbb{R}}c+\frac{1}{2}\|d\|^{2}\] \[\mathrm{s.t.} (A\nabla F(\theta)^{\top}d)_{m}-c(\mathbf{1}^{\top}AF(\theta))^{ -1}(AF(\theta))_{m}\leq 0,\;\;\text{for all }m\in[M]\setminus I\] \[(A\nabla F(\theta)^{\top}d)_{m}\leq 0,\;\;\text{for all }m\in I\] \[B_{g}\nabla F(\theta)^{\top}d+c_{g}G(\theta)\leq 0\] \[B_{h}\nabla F(\theta)^{\top}d+c_{h}H(\theta)=0\]

In the first case, if there exists feasible and non-strictly improving directions at \(\theta\) with \(A\nabla F(\theta)^{\top}d\leq 0\), then such \(d\neq 0\), \(d\in\Omega_{d}\). Following similar arguments as (D.18) by taking \(\sigma=-\max_{m\in[M]\setminus I}(A\nabla F(\theta)^{\top}d)_{m}(\mathbf{1}^{ \top}AF(\theta))/\big{(}(AF(\theta))_{m}\|d\|^{2}\big{)}\), and \(d_{\sigma}=\sigma d\), then

\[\psi(\theta)\coloneqq\min_{(d,c)\in\Omega_{d}(\theta)\times \mathbb{R}}c+\frac{1}{2}\|dwith \(\psi(\theta)=0\) if and only if \(d=0\in\Omega_{d}(\theta)\).

Combining the above arguments, Property-3 is proved.

For **Property-4**, let \(d^{*}(\theta)\) be the solution to the original problem (2.1) without inequality constraints. Using the fact that \(H(\theta)=0\), and letting \(\lambda=A^{\top}\lambda_{f}+B_{h}^{\top}\lambda_{h}=\lambda_{f}+B_{h}^{\top} \lambda_{h}\), then the original dual problem can be written as

\[d^{*}(\theta)=-\nabla F(\theta)\lambda^{*}\] \[\mathrm{s.t.}\ \lambda^{*}\in\operatorname*{arg\,min}_{\lambda\in \Omega_{\tilde{\lambda}}(\theta)}\varphi(\lambda;\theta)\coloneqq\frac{1}{2}\| \nabla F(\theta)\lambda\|^{2}\] (D.27)

where \(\Omega_{\tilde{\lambda}}(\theta)=\big{(}\Omega_{\lambda_{f}}(\theta)\big{)}+B_ {h}^{\top}\big{(}\mathbb{R}^{M_{h}}\big{)}\), and \(\Omega_{\lambda_{f}}(\theta)=\{\lambda_{f}\in\mathbb{R}_{+}^{M}\mid{\lambda_{ f}}^{\top}F(\theta)=\mathbf{1}^{\top}F(\theta)\}\).

Suppose the objective is scaled by a positive diagonal matrix \(\Lambda\in\mathbb{R}^{M\times M}\), then the scaled subprogram has a dual given by

\[d^{*}(\theta)=-\nabla F(\theta)\Lambda\lambda^{*}\] \[\mathrm{s.t.}\ \lambda^{*}\in\operatorname*{arg\,min}_{\lambda\in \Omega_{\tilde{\lambda}}(\theta;\Lambda)}\varphi(\lambda;\theta)\coloneqq\frac {1}{2}\|\nabla F(\theta)\Lambda\lambda\|^{2}\] (D.28)

where \(\Omega_{\tilde{\lambda}}(\theta;\Lambda)=\Lambda\big{(}\Omega_{\lambda_{f}}( \theta;\Lambda)\big{)}+\Lambda B_{h}^{\prime\top}\big{(}\mathbb{R}^{M_{h}} \big{)}\). The set \(\Lambda\big{(}\Omega_{\lambda_{f}}(\theta;\Lambda)\big{)}\) can be written as

\[\Lambda\big{(}\Omega_{\lambda_{f}}(\theta;\Lambda)\big{)}= \{\Lambda\lambda_{f}\mid\lambda_{f}\in\mathbb{R}_{+}^{M},{\lambda_ {f}}^{\top}\Lambda F(\theta)=\mathbf{1}^{\top}\Lambda F(\theta)\}\] \[= \{\lambda_{f}^{\prime}\in\mathbb{R}_{+}^{M}\mid F(\theta)^{\top} \lambda_{f}^{\prime}=\mathbf{1}^{\top}\Lambda F(\theta)\}.\] (D.30)

Notice that,

\[F(\theta)^{\top}{\lambda_{f}}^{\prime}=\mathbf{1}^{\top}\Lambda F(\theta)= \mathbf{1}^{\top}F(\theta)c_{s}\] (D.31)

where \(c_{s}=\mathbf{1}^{\top}\Lambda F(\theta)/(\mathbf{1}^{\top}F(\theta))\). Therefore, \(\Lambda\big{(}\Omega_{\lambda_{f}}(\theta;\Lambda)\big{)}=c_{s}\big{(}\Omega_ {\lambda_{f}}(\theta)\big{)}\).

Also note that, \(B_{h}\in\mathbb{R}^{(M-1)\times M}\) is full row rank, and is selected based on \(F(\theta)\), which satisfies

\[B_{h}(F(\theta_{1})-F(\theta_{2}))=0\] (D.32)

where \(F(\theta_{1}),F(\theta_{2})\) are two reference points which fully defines the kernel of \(B_{h}\). Similarly, when \(F(\theta)\) is scaled by \(\Lambda\), the corresponding \(B_{h}^{\prime}\) satisfies

\[B_{h}^{\prime}\Lambda(F(\theta_{1})-F(\theta_{2}))=0.\] (D.33)

This further implies

\[\Lambda{B_{h}^{\prime}}^{\top}(\mathbb{R}^{M_{h}})=\operatorname*{range}( \Lambda{B_{h}^{\prime}}^{\top})=\ker(B_{h}^{\prime}\Lambda)^{\perp}=\ker(B_ {h})^{\perp}=B_{h}(\mathbb{R}^{M_{h}})=c_{s}B_{h}(\mathbb{R}^{M_{h}}).\] (D.34)

Combining with \(\Lambda\big{(}\Omega_{\lambda_{f}}(\theta;\Lambda)\big{)}=c_{s}\big{(}\Omega_ {\lambda_{f}}(\theta)\big{)}\), it holds that

\[\Omega_{\tilde{\lambda}_{f}}(\theta;\Lambda)=c_{s}\Omega_{\tilde{\lambda}}( \theta).\] (D.35)

Therefore, the solution of \(\tilde{\lambda}\) and \(\lambda^{\prime}\) is only subject to a scaling factor, which does not change the direction of \(d^{*}(\theta)\). This proves Property-4, the scale invariance. 

**Remark 7**.: _Note that, Property 3, the ability to escape weak optimal solutions, and Property 4, the scale invariance, come from the subprogram design that is adaptive to the objectives. For the simplified subprogram that is not adaptive to the objectives, these two properties no longer hold, but Properties 1 and 2 still hold._

#### d.3.2 Proof of Lemma 2: calmness of PMOL

**Example 1**.: _Let \(F:\mathbb{R}^{q}\to\mathbb{R}^{2}\). Consider the problem below as a special case of (PMOL), given by_

\[\min_{\mathbb{R}^{2}_{+}}\,F(\theta)\,\,\,\mathrm{s.t.}\,\,\,f_{2}(\theta)=\min f _{2}(\theta).\] (D.36)

_For \(\bar{\theta}=\arg\min_{\theta\in\mathbb{R}^{q}}f_{2}(\theta)\), we have \(\nabla f_{2}(\bar{\theta})=0\), and \(\bar{\theta}\) satisfies (D.10) with \(\lambda=[0,1]^{\top}\neq 0\) and \(\lambda_{h}=1\). However, \(\nabla H(\bar{\theta})=\nabla f_{2}(\bar{\theta})=0\) violates the LICQ, the Slater's CQ, and the MFCQ._

Below we restate the definition of the Calmness condition for PMOL [57], which generalizes the calmness condition in single-objective optimization.

**Definition 10** (Calmness condition for PMOL [57, Restatement of Definition 4.5]).: _Let \(\bar{\theta}\) be a local solution to (PMOL). We say the PMOL problem satisfies the calmness condition at \(\bar{\theta}\) provided that there exists \(\epsilon>0\) and a Lipschitz function \(\phi:\mathbb{R}^{M_{g}+M_{h}}\to\mathbb{R}^{M}\) satisfying \(\phi(0,0)=0\) such that there exists no \((\theta,p,q)\in[(\bar{\theta},0,0)+\epsilon\mathcal{B}|/\{(\bar{\theta},0,0)\}\) satisfying_

\[G(\theta)+p\leq 0,\] (D.37a) \[H(\theta)+q=0,\] (D.37b) \[F(\theta)-F(\bar{\theta})+\phi(p,q)\in-\mathrm{int}(C_{A}).\] (D.37c)

Our proof relies on the following relative form of Hoffman error bound, which bounds the distance of a point to a nonempty solution set defined by constraints by a measure of the constraint violation of the point.

**Lemma 8** (Relative form of Hoffman error bound [48, Proposition 5]).: _Given \(B_{h}\in\mathbb{R}^{k_{H}\times M},b_{h}\in\mathbb{R}^{k_{H}}\), \(B_{g}\in\mathbb{R}^{k_{G}\times M},b_{g}\in\mathbb{R}^{k_{G}}\), define \(\Sigma(p,q)\coloneqq\{y\in\mathbb{R}^{M}\mid B_{g}y+b_{g}\leq p,B_{h}y+b_{h}=q\}\), and \(\mathrm{dom}\,\Sigma\coloneqq\{(p,q)\mid\Sigma(p,q)\neq\emptyset\}\). Let \(\Omega_{R}\subseteq\mathbb{R}^{M}\) be a reference polyhedron (e.g., one defined by the intersection of half-spaces). Then for all \(u\in\Omega_{R}\), and \((p,q)\in\mathrm{dom}\,\Sigma\), there exists a relative Hoffman constant \(c_{\mathrm{hof}}\) depending only on \(B_{g},B_{h},\Omega_{R}\) such that_

\[\mathrm{dist}(u,\Sigma(p,q)\cap\Omega_{R})\leq c_{\mathrm{hof}}(B_{g},B_{h}\mid\Omega_{R})\left\|\begin{bmatrix}(B_{ g}u+b_{g}-p)+\\ B_{h}u+b_{h}-q\end{bmatrix}\right\|\] (D.38)

_where \((B_{g}u+b_{g}-p)_{+}\coloneqq\max\{0,B_{g}u+b_{g}-p\}\) which replaces each negative component of \(B_{g}u+b_{g}-p\) by zero, and \(\mathrm{dist}(u,\Omega)\coloneqq\inf_{u^{\prime}\in\Omega}\|u-u^{\prime}\|\)._

Proof of Lemma 2.: We first construct \(\phi(p,q)=\overline{c_{\mathrm{hof}}}\|p^{\top},q^{\top}\|^{\top}\|A^{-1} \mathbf{1}_{M}\), where \(\overline{c_{\mathrm{hof}}}\) is the Hoffman constant upper bound in Lemma 8. Then \(\phi(0,0)=0\), and \(\phi(p,q)\) is Lipschitz because

\[\|\phi(p,q)-\phi(p^{\prime},q^{\prime})\|\leq \overline{c_{\mathrm{hof}}}M\|A^{-1}\|\left\|\begin{bmatrix}p\\ q\end{bmatrix}\right\|-\left\|\begin{bmatrix}p^{\prime}\\ q^{\prime}\end{bmatrix}\right\|\] \[\leq \overline{c_{\mathrm{hof}}}M\|A^{-1}\|\left\|\begin{bmatrix}p-p^ {\prime}\\ q-q^{\prime}\end{bmatrix}\right\|.\] (D.39)

Next we prove the PMOL calmness condition holds by contradiction. Suppose for every \(\epsilon>0\), there exists \((\hat{\theta},p,q)\in[(\bar{\theta},0,0)+\epsilon\mathcal{B}]/\{(\bar{\theta},0,0)\}\) satisfying (D.37).

Define \(\Omega_{F_{1}}\coloneqq\{F(\theta)\in\Sigma(0,0)\mid\theta\in\mathbb{R}^{q}\}\neq\emptyset\), there exists \(\tilde{\theta}\in\mathbb{R}^{q}\) such that \(F(\tilde{\theta})\in\Omega_{F_{1}}\) and \(\|F(\tilde{\theta})\|<\infty\). We then consider the following two cases:

_Case 1:_\(F(\hat{\theta})\in\Sigma(0,0)\). In this case, \((\hat{\theta},p,q)=(\hat{\theta},0,0)\neq(\bar{\theta},0,0)\), thus \(\hat{\theta}\neq\bar{\theta}\). Take \(\tilde{\theta}=\hat{\theta}\neq\bar{\theta}\).

_Case 2:_\(F(\hat{\theta})\notin\Sigma(0,0)\). Take \(\tilde{\theta}\) such that \(F(\tilde{\theta})\in\Omega_{F_{1}}\), then \(F(\tilde{\theta})\neq F(\hat{\theta})\).

In both cases, let \(\Omega_{R}\) be the convex hull of \(\{F(\hat{\theta}),F(\tilde{\theta})\}\), i.e., \(\Omega_{R}=\mathrm{conv}(\{F(\tilde{\theta}),F(\hat{\theta})\})\). Then \(\Omega_{R}\) is a line segment (or reduces to a point in _case 1_), thus a polyhedron. Since \(\Sigma(0,0)\) is a line, \(F(\tilde{\theta})\in\Sigma(0,0)\cap\Omega_{R}\), thus \(\Sigma(0,0)\cap\Omega_{R}=\Omega_{R}=\{F(\tilde{\theta})\}\) in _case 1_, and \(\Sigma(0,0)\cap\Omega_{R}=\{F(\tilde{\theta})\}\) in _case 2_. Therefore, in both cases,

\[\|F(\tilde{\theta})-F(\hat{\theta})\|=\mathrm{dist}(F(\hat{\theta}),\Sigma(0,0) \cap\Omega_{R})\] (D.40)

where \(\mathrm{dist}(F,\Omega)\coloneqq\inf_{F^{\prime}\in\Omega}\|F-F^{\prime}\|\).

We also have

\[\mathrm{dist}(F(\hat{\theta}),\Sigma(0,0)\cap\Omega_{R})\stackrel{{ (a)}}{{\leq}} c_{\mathrm{hof}}(\Omega_{R})\left\|\begin{bmatrix}(B_{g}F(\hat{ \theta})+b_{g})_{+}\\ B_{h}F(\hat{\theta})+b_{h}\end{bmatrix}\right\|\] \[\stackrel{{(b)}}{{\leq}} \overline{c_{\mathrm{hof}}}\left\|\begin{bmatrix}(-p)_{+}\\ -q\end{bmatrix}\right\|\leq\overline{c_{\mathrm{hof}}}\left\|\begin{bmatrix}p \\ q\end{bmatrix}\right\|\] (D.41)

where \((a)\) follows from Lemma 8; \((b)\) follows from (D.37) that \(0\leq(B_{g}F(\hat{\theta})+b_{g})_{+}\leq(-p)_{+}\), \(B_{h}F(\hat{\theta})+b_{h}=-q\), and that \(c_{\mathrm{hof}}(\Omega_{R})\leq\overline{c_{\mathrm{hof}}}\) for different bounded \(\Omega_{R}\). Multiplying \(\|A\|\mathbf{1}_{M}\) on both sides of the above inequality yields

\[\|A\|\mathrm{dist}(F(\hat{\theta}),\Sigma(0,0)\cap\Omega_{R}) \mathbf{1}_{M}\leq A\phi(p,q).\] (D.42)

It can then be derived that

\[AF(\tilde{\theta})-AF(\hat{\theta})\leq \|AF(\tilde{\theta})-AF(\hat{\theta})\|\mathbf{1}_{M}\leq\|A\| \|F(\tilde{\theta})-F(\hat{\theta})\|\mathbf{1}_{M}\] \[\leq \|A\|\mathrm{dist}(F(\hat{\theta}),\Sigma(0,0)\cap\Omega_{R}) \mathbf{1}_{M}\leq A\phi(p,q).\] (D.43)

By rearranging the above inequality and applying (D.37c), we have that

\[AF(\tilde{\theta})\leq AF(\hat{\theta})+A\phi(p,q)<AF(\bar{\theta})\] (D.44)

which contradicts to that \(\bar{\theta}\) is a global solution to (PMMOL).

Therefore, the PMOL calmness condition in Definition 10 is satisfied. 

## Appendix E Proof of Theorem 1: convergence of Algorithm 1

Recall that, we let \(\lambda=[\lambda_{f};\lambda_{g};\lambda_{h}]\in\mathbb{R}^{M+M_{g}+M_{h}}\), \(A_{ag}=[A;B_{g};B_{h}]\in\mathbb{R}^{(M+M_{g}+M_{h})\times M}\), and use the following concise notation

\[d^{*}(\theta)=-\nabla F(\theta)A_{ag}^{\top}\lambda^{*}(\theta)\] \[\mathrm{s.t.}\ \lambda^{*}(\theta)\in\operatorname*{arg\,min}_{ \lambda\in\Omega_{\lambda}(\theta)}\varphi(\lambda;\theta)\coloneqq\frac{1}{2 }\|\nabla F(\theta)A_{ag}^{\top}\lambda\|^{2}-c_{g}\lambda_{g}^{\top}G(\theta) -c_{h}\lambda_{h}^{\top}H(\theta)\] (E.1)

where \(\Omega_{\lambda}(\theta)=\Omega_{\lambda_{f}}(\theta)\times\mathbb{R}_{+}^{M_ {g}}\times\mathbb{R}^{M_{h}}\), and \(\Omega_{\lambda_{f}}(\theta)=\{\lambda_{f}\in\mathbb{R}_{+}^{M}\mid{\lambda_{f }}^{\top}AF(\theta)=\mathbf{1}^{\top}AF(\theta)\}\).

In the following discussion in this section, we first present the supporting lemmas and their proofs, then provide the proof of Theorem 1.

### Auxiliary lemmas

Lemma 9 is a result from the smoothness of \(F(\theta)\), and thus the smoothness of \(G(\theta)\) and \(H(\theta)\), whose smoothness constants depend on \(B_{g}\) and \(B_{h}\), respectively.

**Lemma 9**.: _Suppose Assumptions 1, 2 hold. Then for all \(\theta,\theta^{\prime}\in\mathbb{R}^{q}\), and all \(\lambda_{f}\in\mathbb{R}^{M}\), we have_

\[\lambda_{f}^{\top}AF(\theta_{t+1})-\lambda_{f}^{\top}AF(\theta_{t})\leq \alpha_{t}\lambda_{f}^{\top}A\nabla F(\theta_{t})^{\top}d_{t}+ \frac{\ell_{f,1}\|A^{\top}\lambda_{f}\|_{1}}{2}\alpha_{t}^{2}\|d_{t}\|^{2}\] (E.2) \[G(\theta_{t+1})-G(\theta_{t})\leq \alpha_{t}\nabla G(\theta_{t})^{\top}d_{t}+\frac{\ell_{f,1}}{2} \alpha_{t}^{2}\|B_{g}^{\top}\|_{\infty,1}\|d_{t}\|^{2}\mathbf{1}\] (E.3) \[H(\theta_{t+1})-H(\theta_{t})\leq \alpha_{t}\nabla H(\theta_{t})^{\top}d_{t}+\frac{\ell_{f,1}}{2} \alpha_{t}^{2}\|B_{h}^{\top}\|_{\infty,1}\|d_{t}\|^{2}\mathbf{1}.\] (E.4)

Proof.: By Assumption 2, it holds that \(\lambda_{f}^{\top}AF(\theta)\) is \(\|A^{\top}\lambda_{f}\|_{1}\ell_{f,1}\)-smooth. By the definition of smoothness, we have

\[\lambda_{f}^{\top}AF(\theta_{t+1})\leq\lambda_{f}^{\top}AF(\theta_{t})+\alpha_ {t}\lambda_{f}^{\top}A\nabla F(\theta_{t})^{\top}d_{t}+\frac{\ell_{f,1}\|A^{ \top}\lambda_{f}\|_{1}}{2}\alpha_{t}^{2}\|d_{t}\|^{2}.\] (E.5)Let \(B_{g,m}\) and \(B_{h,m}\) be the \(m\)-th row of \(B_{g}\) and \(B_{h}\), respectively, then by the \(\ell_{f,1}\)-smoothness of \(F(\theta)\), \(B_{g,m}F(\theta)\) is \(\ell_{f,1}\|B_{g,m}\|_{1}\)-smooth for all \(m\in[M_{g}]\). Also because \(\|B_{g,m}\|_{1}\leq\|B_{g}^{\top}\|_{\infty,1}\) where \(\|B_{g}^{\top}\|_{\infty,1}=\max_{m\in M_{g}}\||B_{g,m}\|_{1}\|\), \(g_{m}(\theta)\) is \(\ell_{f,1}\|B_{g}^{\top}\|_{\infty,1}\)-smooth for all \(m\in[M_{g}]\). By the definition of smoothness, it holds that

\[G(\theta_{t+1})-G(\theta_{t})\leq \alpha_{t}\nabla G(\theta_{t})^{\top}d_{t}+\frac{\ell_{f,1}}{2} \alpha_{t}^{2}\|B_{g}^{\top}\|_{\infty,1}\|d_{t}\|^{2}\mathbf{1}.\] (E.6)

Following similar arguments as the above for \(G(\theta)\), (E.4) can be proved. 

**Lemma 10**.: _For the subprogram (2.3) or equivalently (E.1), it holds that for any \(\lambda\in\Omega_{\lambda}(\theta)\),_

\[\langle\nabla F(\theta)A_{ag}^{\top}\lambda,\nabla F(\theta)A_{ag}^{\top} \lambda^{*}(\theta)\rangle-[0^{\top},c_{g}G(\theta)^{\top},c_{h}H(\theta)^{\top }](\lambda-\lambda^{*}(\theta))\geq\|\nabla F(\theta)A_{ag}^{\top}\lambda^{*}( \theta)\|^{2}.\] (E.7)

Proof of Lemma 10.: Since \(\varphi(\lambda;\theta)\) is a convex function w.r.t. \(\lambda\), by the first order optimality condition, it holds that for all \(\lambda\in\Omega_{\lambda}(\theta)\)

\[\langle\nabla_{\lambda}\varphi(\lambda^{*}(\theta);\theta),\lambda-\lambda^{*} (\theta)\rangle\geq 0\] (E.8)

which can be further written as

\[\lambda^{\top}A_{ag}\nabla F(\theta)^{\top}\nabla F(\theta)A_{ag}^{\top} \lambda^{*}(\theta)-[0^{\top},c_{g}G(\theta)^{\top},c_{h}H(\theta)^{\top}]( \lambda-\lambda^{*}(\theta))\geq\|\nabla F(\theta)A_{ag}^{\top}\lambda^{*}( \theta)\|^{2}.\] (E.9)

This completes the proof. 

We next prove Lemma 11, which can be viewed as a descent lemma for \([G(\theta)]_{+}\) and \(|H(\theta)|_{\rm ab}\) based on the smoothness of \(G(\theta)\) and \(H(\theta)\), as well as proper hyperparameter choices. This is crucial for proving the convergence result in Theorem 1. One key technical challenge in proving the lemma is that even though \(G(\theta)\) and \(H(\theta)\) are smooth, \([G(\theta)]_{+}\) and \(|H(\theta)|_{\rm ab}\) are not. We address this challenge by exploiting the fact that \(\nabla G(\theta_{t})^{\top}d^{*}(\theta_{t})\leq-c_{g}G(\theta_{t})\) and \(\nabla H(\theta_{t})^{\top}d^{*}(\theta_{t})=-c_{g}H(\theta_{t})\), as well as choosing \(\alpha_{t}\) properly depending on \(c_{g}\) and \(c_{h}\).

**Lemma 11**.: _Let \(\epsilon\geq 0\) be a constant. Define \([y]_{+}\coloneqq\max\{y,0\}\) which replaces each negative component of \(y\) by zero, and \(|y|_{\rm ab}\) replaces each component of \(y\) by its absolute value. Let \(\{\theta_{t}\}\) be the sequence produced by Algorithm 1 with the update \(\theta_{t+1}=\theta_{t}+\alpha_{t}d_{t}\), where \(d_{t}\) satisfies the constraints of the subprogram (2.1) up to an error of \(\epsilon\), i.e.,_

\[[\nabla G(\theta_{t})^{\top}d_{t}+c_{g}G(\theta_{t})]_{+}\leq \epsilon\mathbf{1},\] (E.10) \[[\nabla H(\theta_{t})^{\top}d_{t}+c_{h}H(\theta_{t})|_{\rm ab} \leq\epsilon\mathbf{1}.\] (E.11)

_If \(\alpha_{t}\leq\min\{c_{g}^{-1},c_{h}^{-1}\}\), then it holds that_

\[[G(\theta_{t+1})]_{+}-[G(\theta_{t})]_{+}\leq -\alpha_{t}c_{g}[G(\theta_{t})]_{+}+\frac{\ell_{f,1}}{2}\alpha_{t} ^{2}\|B_{g}^{\top}\|_{\infty,1}\|d_{t}\|^{2}\mathbf{1}+\epsilon\mathbf{1}\] (E.12) \[|H(\theta_{t+1})|_{\rm ab}-|H(\theta_{t})|_{\rm ab}\leq -\alpha_{t}c_{h}|H(\theta_{t})|_{\rm ab}+\frac{\ell_{f,1}}{2} \alpha_{t}^{2}\|B_{h}^{\top}\|_{\infty,1}\|d_{t}\|^{2}\mathbf{1}+\epsilon \mathbf{1}.\] (E.13)

Proof.: By the smoothness of \(G(\theta)\) in Lemma 9 and \(\nabla G(\theta)^{\top}d+c_{g}G(\theta)\leq[\nabla G(\theta)^{\top}d+c_{g}G( \theta)]_{+}\leq\epsilon\mathbf{1}\), it holds that

\[G(\theta_{t+1})-G(\theta_{t})\leq \alpha_{t}\nabla G(\theta_{t})^{\top}d_{t}+\frac{\ell_{f,1}}{2} \alpha_{t}^{2}\|B_{g}^{\top}\|_{\infty,1}\|d_{t}\|^{2}\mathbf{1}+\epsilon \mathbf{1}\] \[\leq -\alpha_{t}c_{g}G(\theta_{t})+\frac{\ell_{f,1}}{2}\alpha_{t}^{2}\| B_{g}^{\top}\|_{\infty,1}\|d_{t}\|^{2}\mathbf{1}+\epsilon\mathbf{1}.\] (E.14)

For all \(m\in[M_{g}]\), since \(G(\theta_{t})\leq[G(\theta_{t})]_{+}\), it holds that

\[g_{m}(\theta_{t+1})-[g_{m}(\theta_{t})]_{+}\leq g_{m}(\theta_{t})-[g_{m}(\theta_{t})]_{+}-\alpha_{t}c_{g}g_{m}(\theta_{t})+ \frac{\ell_{f,1}}{2}\alpha_{t}^{2}\|B_{g}^{\top}\|_{\infty,1}\|d_{t}\|^{2}+\epsilon\] (E.15) \[\leq -[-g_{m}(\theta_{t})]_{+}-\alpha_{t}c_{g}g_{m}(\theta_{t})+\frac{ \ell_{f,1}}{2}\alpha_{t}^{2}\|B_{g}^{\top}\|_{\infty,1}\|d_{t}\|^{2}+\epsilon.\] (E.16)It can be further derived that

\[-[-g_{m}(\theta_{t})]_{+}-\alpha_{t}c_{g}g_{m}(\theta_{t})= \left\{\begin{aligned} -\alpha_{t}c_{g}g_{m}(\theta_{t}),g_{m}( \theta_{t})\geq 0\\ (1-\alpha_{t}c_{g})g_{m}(\theta_{t}),g_{m}(\theta_{t})<0\end{aligned}\right.\] \[\leq -\alpha_{t}c_{g}[g_{m}(\theta_{t})]_{+}\] (E.17)

where the last inequality holds since \(1-\alpha_{t}c_{g}\geq 0\). Plugging this inequality back into (E.16), yields that when \(g_{m}(\theta_{t+1})\geq 0\),

\[[g_{m}(\theta_{t+1})]_{+}-[g_{m}(\theta_{t})]_{+} \leq-\alpha_{t}c_{g}[g_{m}(\theta_{t})]_{+}+\frac{\ell_{f,1}}{2} \alpha_{t}^{2}\|B_{g}^{\top}\|_{\infty,1}\|d_{t}\|^{2}+\epsilon.\] (E.18)

When \(g_{m}(\theta_{t+1})<0\), we have

\[[g_{m}(\theta_{t+1})]_{+}-[g_{m}(\theta_{t})]_{+} \leq-[g_{m}(\theta_{t})]_{+}\leq-\alpha_{t}c_{g}[g_{m}(\theta_{t} )]_{+}\] \[\leq-\alpha_{t}c_{g}[g_{m}(\theta_{t})]_{+}+\frac{\ell_{f,1}}{2} \alpha_{t}^{2}\|B_{g}^{\top}\|_{\infty,1}\|d_{t}\|^{2}+\epsilon.\] (E.19)

Combining (E.18) and (E.19) proves (E.12).

By the smoothness of \(H(\theta)\) and \(|\nabla H(\theta)^{\top}d+c_{h}H(\theta)|_{\mathrm{ab}}\leq\epsilon\mathbf{1}\), we have

\[|H(\theta_{t+1})|_{\mathrm{ab}}\leq |H(\theta_{t})-\alpha_{t}c_{h}H(\theta_{t})|_{\mathrm{ab}}+\frac{ \ell_{f,1}}{2}\alpha_{t}^{2}\|B_{h}^{\top}\|_{\infty,1}\|d_{t}\|^{2}\mathbf{1} +\epsilon\mathbf{1}\] \[= (1-\alpha_{t}c_{h})|H(\theta_{t})|_{\mathrm{ab}}+\frac{\ell_{f,1}} {2}\alpha_{t}^{2}\|B_{h}^{\top}\|_{\infty,1}\|d_{t}\|^{2}\mathbf{1}+\epsilon \mathbf{1}\] (E.20)

where the last equality holds because \(1-\alpha_{t}c_{h}\geq 0\), which proves (E.13). 

### Proof of Theorem 1

In this section, we prove Theorem 1. Similar to the proof techniques used in [7], we use \(\lambda_{f}^{\top}AF(\theta_{t})\) with a fixed \(\lambda_{f}\in\Omega_{\lambda_{f}}(\theta)\) as a part of the Lyapunov function, instead of using the dynamically changing \(\lambda_{f,t}\). This eliminates the need to assume the objective values are bounded above in our theorem.

Proof of Theorem 1.: To consider both objective function minimization and constraint satisfaction, we define a Lyapunov function below with a constant vector \(\lambda=(\lambda_{f},\lambda_{g},\lambda_{h})\in\Omega_{\lambda}(\theta)\), where \(\lambda_{f}=\mathbf{1}\), \(\lambda_{g}\in\mathbb{R}_{+}^{M_{g}}\), \(\lambda_{h}\in\mathbb{R}^{M_{h}}\), and \(\lambda_{g}>\lambda_{g}^{*}(\theta_{t})\), \(\lambda_{h}>\lambda_{h}^{*}(\theta_{t})\) for all \(t\in[T]\).

\[\mathbb{V}_{t}\coloneqq\underbrace{\lambda_{f}^{\top}AF(\theta_{t})}_{\mathbb{ V}_{f,t}}+\underbrace{\lambda_{g}^{\top}[G(\theta_{t})]_{+}}_{\mathbb{V}_{g,t} ^{*}}+\underbrace{\lambda_{h}^{\top}[H(\theta_{t})]_{\mathrm{ab}}}_{\mathbb{ V}_{h,t}}.\] (E.21)

Note that \(\mathbb{V}_{t}\geq 0\) for all \(t\) since \(AF(\theta)\geq 0,\lambda_{f}\geq 0\).

For notation simplicity, we let \(d_{t}^{*}=d^{*}(\theta_{t})\). From Assumption 2, the smoothness of the objectives, and Lemma 9, based on the update \(\theta_{t+1}=\theta_{t}+\alpha_{t}d_{t}\), it holds that

\[\mathbb{V}_{f,t+1}-\mathbb{V}_{f,t}\overset{(a)}{\leq}\alpha_{t} \lambda_{f}^{\top}A\nabla F(\theta_{t})^{\top}d_{t}+\frac{\ell_{f,1}}{2} \alpha_{t}^{2}\|A^{\top}\|_{\infty,1}\|d_{t}\|^{2}\lambda_{f}^{\top}\mathbf{1}\] \[\overset{(b)}{\leq} \alpha_{t}\lambda_{f}^{\top}A\nabla F(\theta_{t})^{\top}d_{t}^{ *}+\frac{\ell_{f,1}}{2}\alpha_{t}^{2}\|A^{\top}\|_{\infty,1}\|d_{t}^{*}\|^{2} \lambda_{f}^{\top}\mathbf{1}+\epsilon\mathbf{1}\] \[\overset{(c)}{\leq} -\alpha_{t}\|d_{t}^{*}\|^{2}+\alpha_{t}\big{(}c_{g}\lambda_{g}^{* }(\theta_{t})^{\top}G(\theta_{t})+c_{h}\lambda_{h}^{*}(\theta_{t})^{\top}H( \theta_{t})\big{)}+\frac{\ell_{f,1}}{2}\alpha_{t}^{2}\|A^{\top}\mathbf{1}\|_{ 1}\|d_{t}^{*}\|^{2}+\epsilon\mathbf{1}\] (E.22)

where \((a)\) follows Lemma 9; \((b)\) follows from that \(d_{t}\) is an \(\epsilon\)-optimal solution to the subprogram; \((c)\) follows from Lemma 10 with \(\lambda=[\lambda_{f};0;0]\in\Omega_{\lambda}(\theta)\) therein.

From Lemma 11, for \(\alpha_{t}\leq\min\{c_{g}^{-1},c_{h}^{-1}\}\), it holds that

\[\mathbb{V}_{g,t+1}-\mathbb{V}_{g,t}\leq -\alpha_{t}c_{g}\lambda_{g}^{\top}[G(\theta_{t})]_{+}+\frac{\ell _{f,1}}{2}\alpha_{t}^{2}\|B_{g}^{\top}\|_{\infty,1}\|d_{t}\|^{2}\lambda_{g}^{ \top}\mathbf{1}+\epsilon\lambda_{g}^{\top}\mathbf{1}\] (E.23)\[\mathbb{V}_{h,t+1}-\mathbb{V}_{h,t}\leq -\alpha_{t}c_{h}\lambda_{h}^{\top}|H(\theta_{t})|_{\mathrm{ab}}+ \frac{\ell_{f,1}}{2}\alpha_{t}^{2}\|B_{h}^{\top}\|_{\infty,1}\|d_{t}\|^{2} \lambda_{h}^{\top}\mathbf{1}+\epsilon\lambda_{h}^{\top}\mathbf{1}.\] (E.24)

Combining the above inequalities for \(\mathbb{V}_{f,t},\mathbb{V}_{g,t},\mathbb{V}_{h,t}\), we have

\[\mathbb{V}_{t+1}-\mathbb{V}_{t}\leq -\alpha_{t}\|d_{t}^{*}\|^{2}+\alpha_{t}\big{(}c_{g}\lambda_{g}^{* }(\theta_{t})^{\top}G(\theta_{t})+c_{h}\lambda_{h}^{*}(\theta_{t})^{\top}H( \theta_{t})\big{)}\] \[+\frac{\ell_{f,1}}{2}\alpha_{t}^{2}\|A_{ag}^{\top}\lambda\|_{1}\|d _{t}^{*}\|^{2}-\alpha_{t}c_{g}\lambda_{g}^{\top}[G(\theta_{t})]_{+}-\alpha_{t} c_{h}\lambda_{h}^{\top}|H(\theta_{t})|_{\mathrm{ab}}+\epsilon\lambda^{\top} \mathbf{1}\] \[\leq -\alpha_{t}\|d_{t}^{*}\|^{2}-\alpha_{t}c_{g}(\lambda_{g}-\lambda_ {g}^{*}(\theta_{t}))^{\top}[G(\theta_{t})]_{+}-\alpha_{t}c_{g}\lambda_{g}^{*}( \theta_{t})^{\top}[-G(\theta_{t})]_{+}\] \[-\alpha_{t}c_{h}(\lambda_{h}-\lambda_{h}^{*}(\theta_{t}))^{\top}|H (\theta_{t})|_{\mathrm{ab}}+\frac{\ell_{f,1}}{2}\alpha_{t}^{2}\|A_{ag}^{\top} \lambda\|_{1}\|d_{t}^{*}\|^{2}+\epsilon\lambda^{\top}\mathbf{1}\] (E.25)

where the last inequality holds because \(\lambda_{g}^{*}(\theta_{t})^{\top}G(\theta_{t})=\lambda_{g}^{*}(\theta_{t})^{ \top}[G(\theta_{t})]_{+}-\lambda_{g}^{*}(\theta_{t})^{\top}[-G(\theta_{t})]_{+}\).

Taking telescoping sum of the above inequality from \(t=0,\dots,T-1\) and rearranging, we have

\[\sum_{t=0}^{T-1}\alpha_{t}\Big{(}1-\frac{1}{2}\|A_{ag}^{\top}\lambda\|_{1} \ell_{f,1}\alpha_{t}\Big{)}\|d_{t}^{*}\|^{2}+\alpha_{t}c_{g}(\lambda_{g}- \lambda_{g}^{*}(\theta_{t}))^{\top}[G(\theta_{t})]_{+}+\alpha_{t}c_{g}\lambda_ {g}^{*}(\theta_{t})^{\top}[-G(\theta_{t})]_{+}\] \[+\alpha_{t}c_{h}(\lambda_{h}-\lambda_{h}^{*}(\theta_{t}))^{\top}|H (\theta_{t})|_{\mathrm{ab}}\leq\mathbb{V}_{0}-\mathbb{V}_{T}+T\epsilon\lambda ^{\top}\mathbf{1}\leq\mathbb{V}_{0}+T\epsilon\|\lambda\|_{1}.\] (E.26)

Recall that \(\alpha_{t}\leq 1/(\ell_{f,1}\|A_{ag}^{\top}\lambda\|_{1})\). Plugging this into the above inequality yields

\[\sum_{t=0}^{T-1}\frac{1}{2}\alpha_{t}\|d_{t}^{*}\|^{2}+\alpha_{t} c_{g}(\lambda_{g}-\lambda_{g}^{*}(\theta_{t}))^{\top}[G(\theta_{t})]_{+}+ \alpha_{t}c_{g}\lambda_{g}^{*}(\theta_{t})^{\top}[-G(\theta_{t})]_{+}\] \[+\alpha_{t}c_{h}(\lambda_{h}-\lambda_{h}^{*}(\theta_{t}))^{\top}|H (\theta_{t})|_{\mathrm{ab}}\leq\mathbb{V}_{0}+T\epsilon\|\lambda\|_{1}.\] (E.27)

Taking \(\alpha_{t}=\Theta(1)\), then

\[\frac{1}{T}\sum_{t=0}^{T-1}\frac{1}{2}\|d^{*}(\theta_{t})\|^{2}+c _{g}(\lambda_{g}-\lambda_{g}^{*}(\theta_{t}))^{\top}[G(\theta_{t})]_{+}+c_{g} \lambda_{g}^{*}(\theta_{t})^{\top}[-G(\theta_{t})]_{+}\] \[+c_{h}(\lambda_{h}-\lambda_{h}^{*}(\theta_{t}))^{\top}|H(\theta_{t })|_{\mathrm{ab}}=\mathcal{O}\Big{(}\frac{1}{T}+\epsilon\Big{)}.\] (E.28)

The proof is complete. 

Next we show that the subprogram converges with a projected gradient descent (PGD) algorithm on \(\lambda\) with \(K\) iterations.

**Lemma 12** (Convergence of the subprogram with projected gradient descent).: _At the \(t\)-th iteration, given \(\theta_{t}\), let \(\{\lambda_{t,k}\}_{k}\) be the sequence generated by the projected gradient descent algorithm to solve the subprogram \(\min_{\lambda\in\Omega_{\lambda}(\theta_{t})}\varphi(\lambda;\theta_{t})\), then_

\[\varphi(\lambda_{t,K};\theta_{t})-\min_{\lambda\in\Omega_{\lambda}(\theta_{t})} \varphi(\lambda;\theta_{t})\leq\frac{\|\lambda_{t,0}-\lambda^{*}(\theta_{t}) \|^{2}}{2\gamma K}.\] (E.29)

Proof.: The result follows from the convergence result of projected gradient descent for convex objective functions. Note that at each iteration \(t\), given \(\theta_{t}\), \(\Omega_{\lambda}(\theta_{t})\) is fixed. 

**Lemma 13**.: _Suppose Assumption 3 holds. Due to the \(\ell_{\varphi,\lambda,1}\)-smoothness and the convexity of the subprogram, it holds for all \(\lambda\in\Omega_{\lambda}(\theta)\) that_

\[\|\nabla_{\lambda}\varphi(\lambda;\theta)-\nabla_{\lambda}\varphi(\lambda^{*}( \theta);\theta)\|^{2}\leq 2\ell_{\varphi\lambda,1}\big{(}\varphi(\lambda;\theta)- \varphi(\lambda^{*}(\theta);\theta)\big{)}.\] (E.30)

Proof.: Since the objectives \(f_{m}(\theta)\) are Lipschitz continuous for all \(m\in[M]\), the subprogram objective \(\varphi(\lambda;\theta)\) is \(\ell_{\varphi\lambda,1}\)-smooth w.r.t. \(\lambda\). By Proposition 1 (b) in [58], it holds that

\[\frac{1}{2\ell_{\varphi_{\lambda,1}}}\|\nabla_{\lambda}\varphi(\lambda;\theta)- \nabla_{\lambda}\varphi(\lambda^{*}(\theta);\theta)\|^{2}+\langle\nabla_{ \lambda}\varphi(\lambda^{*}(\theta);\theta),\lambda-\lambda^{*}(\theta) \rangle\leq\varphi(\lambda;\theta)-\varphi(\lambda^{*}(\theta);\theta).\] (E.31)By the convexity of \(\varphi(\lambda;\theta)\) w.r.t. \(\lambda\), for all \(\lambda\in\Omega_{\lambda}(\theta)\),

\[\langle\nabla_{\lambda}\varphi(\lambda^{*}(\theta);\theta),\lambda-\lambda^{*}( \theta)\rangle\geq 0.\] (E.32)

Combining the above two inequalities proves the result. 

**Corollary 14** (Convergence of Algorithm 1 with \(K\)-iteration PGD for the subprogram).: _Suppose Assumptions 1, 2 hold. Let \(\{\theta_{t}\}\) be the sequence produced by Algorithm 1 with the update \(\theta_{t+1}=\theta_{t}+\alpha_{t}d_{t}\), where \(d_{t}\) is the \(\epsilon\)-optimal solution to the subprogram (2.1) obtained by \(K\)-iteration PGD for the subprogram on \(\lambda\). Define \(\lambda:=(\lambda_{f},\lambda_{g},\lambda_{h})\in\Omega_{\lambda}(\theta)\) with \(\lambda_{g}\geq\lambda_{g}^{*}(\theta)+\mathbf{1}\), \(\lambda_{h}\geq\lambda_{h}^{*}(\theta)+\mathbf{1}\) for all \(\theta\in\mathbb{R}^{q}\). If the step size \(\alpha_{t}\leq 1/(\ell_{f,1}\|A_{ag}^{\top}\lambda\|_{1})\) and \(\alpha_{t}=\Theta(1)\), then_

\[\sum_{t=0}^{T-1} \frac{1}{2}\|d_{t}^{*}\|^{2}+c_{g}(\lambda_{g}-\lambda_{g}^{*}( \theta_{t}))^{\top}[G(\theta_{t})]_{+}+c_{g}\lambda_{g}^{*}(\theta_{t})^{\top} [-G(\theta_{t})]_{+}\] \[+c_{h}(\lambda_{h}-\lambda_{h}^{*}(\theta_{t}))^{\top}|H(\theta_{t })|_{\mathrm{ab}}=\mathcal{O}\big{(}1\big{)}.\] (E.33)

Proof.: For \(t=0,\ldots,T-1\), we take \(K=T^{2}\), applying Lemma 12, we have

\[\varphi(\lambda_{t,K};\theta_{t})-\min_{\lambda\in\Omega_{\lambda}(\theta_{t}) }\varphi(\lambda;\theta_{t})\leq\frac{\|\lambda_{t,0}-\lambda^{*}(\theta_{t}) \|^{2}}{2\gamma T^{2}}.\] (E.34)

From Lemma 13, the above inequality implies

\[\|\nabla\varphi(\lambda_{t};\theta_{t})-\nabla\varphi(\lambda^{*}(\theta_{t}); \theta_{t})\|^{2}\leq 2\ell_{\varphi_{\lambda,1}}\big{(}\varphi(\lambda_{t}; \theta_{t})-\varphi(\lambda^{*}(\theta_{t});\theta_{t})\big{)}\leq\frac{\ell_ {\varphi_{\lambda,1}}\|\lambda_{t-1}-\lambda^{*}(\theta_{t})\|^{2}}{\gamma T^{ 2}}.\] (E.35)

Plugging in the gradient \(\nabla\varphi(\lambda_{t};\theta_{t})\), we have

\[\|A\nabla F(\theta_{t})^{\top}(d_{t}-d_{t}^{*})\|^{2}+\|\nabla G (\theta_{t})^{\top}(d_{t}-d_{t}^{*})\|^{2}+\|\nabla G(\theta_{t})^{\top}(d_{t }-d_{t}^{*})\|^{2}\] \[\leq \frac{\ell_{\varphi_{\lambda,1}}\|\lambda_{t-1}-\lambda^{*}( \theta_{t})\|^{2}}{\gamma T^{2}}\leq\frac{4\ell_{\varphi_{\lambda,1}}c_{ \lambda}^{2}}{\gamma T^{2}}.\] (E.36)

Let \(\epsilon=\frac{4\ell_{\varphi_{\lambda,1}}c_{\lambda}^{2}}{\gamma T^{2}}\), from Theorem 1, it holds that

\[\mathbb{V}_{t+1} -\mathbb{V}_{t}\leq-\alpha_{t}\|d_{t}^{*}\|^{2}+\alpha_{t}c_{g} \big{(}\lambda_{g}^{*}(\theta_{t})-\lambda_{g}\big{)}^{\top}[G(\theta_{t})]_ {+}+\alpha_{t}c_{g}\lambda_{g}^{*}(\theta_{t})^{\top}[-G(\theta_{t})]_{+}\] \[+\alpha_{t}c_{h}\big{(}\lambda_{h}^{*}(\theta_{t})-\lambda_{h} \big{)}^{\top}|H(\theta_{t})|_{\mathrm{ab}}+\epsilon^{\frac{1}{2}}+\frac{1}{ 2}\gamma\alpha_{t}\|\nabla_{\lambda}\varphi(\lambda_{t};\theta_{t})\|^{2}+ \frac{\ell_{f,1}}{2}\alpha_{t}^{2}\|A_{ag}^{\top}\lambda\|_{1}\|d_{t}^{*}\|^ {2}.\] (E.37)

Taking telescoping sum of the above inequality from \(t=0,\ldots,T-1\), rearranging, and letting \(\alpha_{t}\leq 1/(\|\lambda\|_{1}\ell_{f,1}\|A_{ag}^{\top}\|_{\infty,1})\), we have

\[\sum_{t=T}^{T-1} \frac{1}{2}\alpha_{t}\|d_{t}^{*}\|^{2}+\alpha_{t}c_{g}(\lambda_{ g}-\lambda_{g}^{*}(\theta_{t}))^{\top}[G(\theta_{t})]_{+}+\alpha_{t}c_{g} \lambda_{g}^{*}(\theta_{t})^{\top}[-G(\theta_{t})]_{+}\] \[+\alpha_{t}c_{h}(\lambda_{h}-\lambda_{h}^{*}(\theta_{t}))^{\top}|H (\theta_{t})|_{\mathrm{ab}}\leq\mathbb{V}_{T}+T\epsilon^{\frac{1}{2}}.\] (E.38)

Letting \(\alpha_{t}=\Theta(1),\gamma=\Theta(1)\) yields

\[\sum_{t=T}^{T-1} \frac{1}{2}\|d_{t}^{*}\|^{2}+c_{g}(\lambda_{g}-\lambda_{g}^{*}( \theta_{t}))^{\top}[G(\theta_{t})]_{+}+c_{g}\lambda_{g}^{*}(\theta_{t})^{\top }[-G(\theta_{t})]_{+}\] \[+c_{h}(\lambda_{h}-\lambda_{h}^{*}(\theta_{t}))^{\top}|H(\theta_{ t})|_{\mathrm{ab}}=\mathcal{O}\big{(}1\big{)}.\] (E.39)

The proof is complete.

Proof of Theorems 2 and 3: convergence of Algorithm 2

In this section, we prove the convergence of Algorithm 2 with single-loop updates. We focus on the problem with equality constraints only, i.e., \(M_{g}=0\). Furthermore, we consider the simplified subprogram without adaptivity to the objectives, thus \(\Omega_{\lambda_{f}}(\theta)=\Delta^{M}\).

We provide two theoretical results in Theorems 2 and 3, respectively. Specifically, Theorem 2 uses the same merit function as Theorem 1, but provides a slower convergence rate. Theorem 3 uses a different merit function, and provides a faster convergence rate than Theorem 1 under additional assumptions.

### Auxiliary lemmas

**Lemma 15** (Smoothness of \(\varphi\) w.r.t. \(\lambda\)).: _Suppose Assumptions 1 and 3 hold. \(\varphi(\lambda;\theta)\) is \(\ell_{\varphi_{\lambda},1}\)-smooth w.r.t. \(\lambda\), with \(\ell_{\varphi_{\lambda},1}=M\|A_{ag}\|^{2}\ell_{f}^{2}\)._

Proof.: The Hessian of \(\varphi(\lambda;\theta)\) w.r.t. \(\lambda\) can be computed by

\[\nabla_{\lambda}^{2}\varphi(\lambda;\theta)=A_{ag}\nabla F(\theta)^{\top}\nabla F (\theta)A_{ag}^{\top}.\]

By Assumption 3, the Lipschitz continuity of \(F\), it holds that

\[\|\nabla_{\lambda}^{2}\varphi(\lambda;\theta)\|\leq\|A_{ag}\nabla F(\theta)^{ \top}\nabla F(\theta)A_{ag}^{\top}\|\leq\|\nabla F(\theta)A_{ag}^{\top}\|^{2} \leq M\|A_{ag}\|^{2}\ell_{f}^{2}.\]

The result is proved. 

**Lemma 16** (\(\|\nabla_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})\|\) is bounded by \(\|d_{t}\|\)).: _Suppose Assumptions 1 and 3 hold. For \(\{\theta_{t}\}\) produced by Algorithm 2, we have_

\[\|\nabla_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})\|\leq\|A^{\top}\|_{\infty,1}\ell_{f}\|d_{t}\|.\] (F.1)

Proof.: The gradient of \(\varphi(\lambda_{t};\theta_{t})\) w.r.t. \(\lambda_{f}\) can be computed by

\[\nabla_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})=A\nabla F(\theta_{t})^{\top }\nabla F(\theta_{t})A_{ag}^{\top}\lambda_{t}=-A\nabla F(\theta_{t})^{\top}d_{ t}.\] (F.2)

By Assumption 3, it holds that

\[\|\nabla_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})\|\leq\|A^{\top}\|_{ \infty,1}\ell_{f}\|d_{t}\|.\] (F.3)

The proof is complete. 

**Lemma 17**.: _Let \(\lambda_{t}=[\lambda_{f,t};\lambda_{h,t}]\). Consider the sequence \(\{\lambda_{t}\}_{t=1}^{T}\) generated by the update (3.1). Then for all \(\lambda\in\Omega_{\lambda}(\theta_{t})\) with \(\lambda=(\lambda_{f},\lambda_{h})\), it holds that_

\[2\gamma_{t}\langle\lambda_{f,t}-\lambda_{f},\nabla_{\lambda_{f}} \varphi(\lambda_{t};\theta_{t})\rangle\leq\|\lambda_{f,t}-\lambda_{f}\|^{2}-\| \lambda_{f,t+1}-\lambda_{f}\|^{2}+\gamma_{t}^{2}\|\nabla_{\lambda_{f}}\varphi (\lambda_{t};\theta_{t})\|^{2};\] \[2\gamma_{t}\langle\lambda_{h,t}-\lambda_{h},\nabla_{\lambda_{h}} \varphi(\lambda_{t};\theta_{t})\rangle=\|\lambda_{h,t}-\lambda_{h}\|^{2}-\| \lambda_{h,t+1}-\lambda_{h}\|^{2}+\gamma_{t}^{2}\|\nabla_{\lambda_{h}}\varphi (\lambda_{t};\theta_{t})\|^{2}.\] (F.4)

Proof.: By the update of \(\lambda_{f,t}\), and the non-expansiveness of projection, for all \(\lambda_{f}\in\Delta^{M}\), we have

\[\|\lambda_{f,t+1}-\lambda_{f}\|^{2}\leq\|\lambda_{f,t}-\gamma_{t} \nabla_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})-\lambda_{f}\|^{2}\] \[= \|\lambda_{f,t}-\lambda_{f}\|^{2}-2\gamma_{t}\langle\lambda_{f,t}- \lambda_{f},\nabla_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})\rangle+\gamma _{t}^{2}\|\nabla_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})\|^{2}.\] (F.5)

Rearranging the above inequality proves the first inequality.

By the update of \(\lambda_{h,t}\), for all constant \(\lambda_{h}\in\mathbb{R}^{M_{h}}\), we have

\[\|\lambda_{h,t+1}-\lambda_{h}\|^{2}=\|(\lambda_{h,t}-\gamma_{t} \nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t}))-\lambda_{h}\|^{2}\] \[= \|\lambda_{h,t}-\lambda_{h}\|^{2}+\gamma_{t}^{2}\|\nabla_{\lambda _{h}}\varphi(\lambda_{t};\theta_{t})\|^{2}-2\gamma_{t}\langle\lambda_{h,t}- \lambda_{h},\nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})\rangle.\] (F.6)

Rearranging the above inequality proves the second inequality. 

**Corollary 18**.: _Let \(\lambda_{t}=[\lambda_{f,t};\lambda_{h,t}]\). Consider the sequence \(\{\lambda_{t}\}_{t=1}^{T}\) generated by the update (3.1). Then for all \(\lambda\in\Omega_{\lambda}\) with \(\lambda=(\lambda_{f},\lambda_{h})\), it holds that_

\[2\gamma_{t}\big{(}\varphi(\lambda_{t};\theta_{t})-\varphi(\lambda;\theta_{t}) \big{)}\leq\|\lambda_{t}-\lambda\|^{2}-\|\lambda_{t+1}-\lambda\|^{2}+\gamma_{t} ^{2}\|\nabla_{\lambda}\varphi(\lambda_{t};\theta_{t})\|^{2}.\] (F.7)

Proof of Corollary 18.: The result follows from combining the two inequalities in Lemma 17, and applying the convexity property of \(\varphi\) w.r.t. \(\lambda\)

### Analysis with the same merit function: proof of Theorem 2

In this section, we provide analysis with the same merit function as Theorem 1. The proof follows similar ideas of the proofs of Theorem 3 (for convergence of the subprogram with the approximate single-loop update) and Theorem 5 (for convergence of the main program) in [7]. We follow the proofs in [7], as they provide, to the best of our knowledge, the fastest convergence rate guarantees for single-loop MOO algorithms under minimal assumptions.

Similar to [7], we first define the following auxiliary functions to assist our analysis. Note that the functions are only used for analysis but not for the algorithm update.

\[\varphi_{\rho}(\lambda;\theta)\coloneqq\varphi(\lambda;\theta)+ \frac{\rho}{2}\|\lambda\|^{2},\ \ \lambda_{\rho}^{*}(\theta)\coloneqq\operatorname*{arg \,min}_{\lambda\in\Omega_{\lambda}}\varphi_{\rho}(\lambda;\theta).\] (F.8)

We then present the following Lemmas that are useful for the proof of convergence of Algorithm 2.

**Lemma 19**.: _Suppose Assumption 3 holds, and \(\lambda^{*}(\theta)\) and \(\lambda_{\rho}^{*}(\theta)\) are bounded for \(\theta\in\{\theta_{t}\}_{t=0}^{T-1}\) produced by Algorithm 2, i.e., \(\|\lambda^{*}(\theta)\|\leq c_{\overline{\lambda}^{*}}\), \(\|\lambda_{\rho}^{*}(\theta)\|\leq c_{\overline{\lambda}}\). Then on the trajectory of Algorithm 2, with \(\theta\in\{\theta_{t}\}_{t=0}^{T-1}\), we have_

\[\varphi(\lambda_{\rho}^{*}(\theta);\theta)-\varphi(\lambda^{*}( \theta);\theta)\leq\frac{\rho}{2}c_{\overline{\lambda}}.\] (F.9)

Proof of Lemma 19.: The proof follows the proof of [7, Lemma 13]. 

**Corollary 20**.: _Suppose Assumption 3 holds, and \(\lambda^{*}(\theta)\) and \(\lambda_{\rho}^{*}(\theta)\) are bounded for \(\theta\in\{\theta_{t}\}_{t=0}^{T-1}\) produced by Algorithm 2, i.e., \(\|\lambda^{*}(\theta)\|\leq c_{\overline{\lambda}^{*}}\)\(\|\lambda_{\rho}^{*}(\theta)\|\leq c_{\overline{\lambda}}\). Then on the trajectory of Algorithm 2, with \(\theta\in\{\theta_{t}\}_{t=0}^{T-1}\), we have_

\[\|\nabla_{\lambda_{\lambda}}\varphi(\lambda;\theta)\|^{2}\leq 2 \ell_{\varphi_{\lambda},1}\big{(}\varphi(\lambda;\theta)-\varphi(\lambda_{ \rho}^{*}(\theta);\theta)\big{)}+\ell_{\varphi_{\lambda},1}\rho c_{\overline{ \lambda}}.\] (F.10)

Proof of Corollary 20.: By applying Lemma 13, and that \(\nabla_{\lambda_{h}}\varphi(\lambda^{*}(\theta);\theta)=0\), we have

\[\|\nabla_{\lambda_{h}}\varphi(\lambda;\theta)\|^{2}= \|\nabla_{\lambda_{h}}\varphi(\lambda;\theta)-\nabla_{\lambda_{h} }\varphi(\lambda^{*}(\theta);\theta)\|^{2}\leq\|\nabla_{\lambda}\varphi( \lambda;\theta)-\nabla_{\lambda}\varphi(\lambda^{*}(\theta);\theta)\|^{2}\] \[\stackrel{{\text{Lemma \ref{lem:main_lemma}}}}{{\leq}}2 \ell_{\varphi_{\lambda},1}\big{(}\varphi(\lambda;\theta)-\min_{\lambda\in \Omega_{\lambda}(\theta)}\varphi(\lambda;\theta)\big{)}.\] (F.11)

Applying Lemma 19, we can further derive

\[\varphi(\lambda;\theta)-\min_{\lambda\in\Omega_{\lambda}(\theta)} \varphi(\lambda;\theta)= \varphi(\lambda;\theta)-\varphi(\lambda^{*}(\theta);\theta)+ \varphi(\lambda_{\rho}^{*}(\theta);\theta)-\varphi(\lambda_{\rho}^{*}(\theta) ;\theta)\] \[\stackrel{{\text{Lemma \ref{lem:main_lemma}}}}{{\leq}} \varphi(\lambda;\theta)-\varphi(\lambda_{\rho}^{*}(\theta);\theta)+ \frac{\rho}{2}c_{\overline{\lambda}}.\] (F.12)

Combining (F.11) and (F.12) yields the result. 

**Lemma 21** (Continuity of \(\lambda_{\rho}^{*}(\theta)\)).: _For \(\lambda_{\rho}^{*}(\theta)\) defined in (F.8), and \(\Omega_{\lambda}(\theta)=\Omega_{\lambda}\), the following holds_

\[\|\lambda_{\rho}^{*}(\theta)-\lambda_{\rho}^{*}(\theta^{\prime})\|\leq \rho^{-1}\|\nabla_{\lambda}^{2}\varphi(\lambda_{\rho}^{*}(\theta); \theta)-\nabla_{\lambda}^{2}\varphi(\lambda_{\rho}^{*}(\theta^{\prime});\theta ^{\prime})\|\] \[\leq 2\rho^{-1}\ell_{f,1}\ell_{f}\|A_{ag}^{\top}\|_{\infty,1}^{2}\| \theta-\theta^{\prime}\|.\] (F.13)

Proof of Lemma 21.: The proof follows the proof of [7, Lemma 12]. 

**Lemma 22**.: _Suppose Assumptions 1, 2, 3 hold. Let \(\{\theta_{t}\},\{\lambda_{t}\}\) be the sequences produced by Algorithm 2 with step sizes \(\alpha_{t}=\alpha>0\), \(\gamma_{t}=\gamma>0\). Assume \(\|\lambda^{*}(\theta_{t})\|,\|\lambda_{\rho}^{*}(\theta_{t})\|,\|\lambda_{t}\| \leq c_{\overline{\lambda}}\). Then for any \(\rho>0\), it holds that_

\[\frac{1}{T}\sum_{t=0}^{T-1}\varphi(\lambda_{t};\theta_{t})-\varphi( \lambda_{\rho}^{*}(\theta_{t});\theta_{t})\leq \frac{2c_{\overline{\lambda}}^{2}}{\gamma T}(1+2\rho^{-1} \alpha T\ell_{f,1}\ell_{f}^{2}\|A_{ag}^{\top}\|_{\infty,1}^{3})+\frac{\gamma}{ 2T}\sum_{t=0}^{T-1}\|\nabla_{\lambda}\varphi(\lambda_{t};\theta_{t})\|^{2}.\] (F.14)Proof of Lemma 22.: The proof follows the proof techniques of [7, Lemma 15].

First, applying Corollary 18 and \(\gamma_{t}=\gamma\) yields

\[2\gamma\big{(}\varphi(\lambda_{t};\theta_{t})-\varphi(\lambda_{ \rho}^{*}(\theta_{t});\theta_{t})\big{)}\leq\|\lambda_{t}-\lambda_{\rho}^{*}( \theta_{t})\|^{2}-\|\lambda_{t+1}-\lambda_{\rho}^{*}(\theta_{t})\|^{2}+\gamma^ {2}\|\nabla_{\lambda}\varphi(\lambda_{t};\theta_{t})\|^{2}.\] (F.15)

Taking telescoping sum of the above inequality and rearranging, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\varphi(\lambda_{t};\theta_{t})- \varphi(\lambda_{\rho}^{*}(\theta_{t});\theta_{t})\leq \frac{1}{2\gamma T}\Big{(}\underbrace{\sum_{t=0}^{T-1}\|\lambda_ {t}-\lambda_{\rho}^{*}(\theta_{t})\|^{2}-\|\lambda_{t+1}-\lambda_{\rho}^{*}( \theta_{t})\|^{2}}_{J_{1}}\Big{)}\] \[+\frac{\gamma}{2T}\sum_{t=0}^{T-1}\|\nabla_{\lambda}\varphi( \lambda_{t};\theta_{t})\|^{2}\] (F.16)

where \(J_{1}\) can be further bounded by

\[J_{1}\leq \|\lambda_{0}-\lambda_{\rho}^{*}(\theta_{0})\|^{2}-\|\lambda_{T}- \lambda_{\rho}^{*}(\theta_{T-1})\|^{2}+\sum_{t=0}^{T-2}\|2\lambda_{t+1}- \lambda_{\rho}^{*}(\theta_{t+1})-\lambda_{\rho}^{*}(\theta_{t})\|\|\lambda_{ \rho}^{*}(\theta_{t+1})-\lambda_{\rho}^{*}(\theta_{t})\|\] \[\leq 4c_{\overline{\lambda}}^{2}+4c_{\overline{\lambda}}\sum_{t=0}^{T- 2}\|\lambda_{\rho}^{*}(\theta_{t+1})-\lambda_{\rho}^{*}(\theta_{t})\|\leq 4c_{ \overline{\lambda}}^{2}+8c_{\overline{\lambda}}\sum_{t=0}^{T-2}\rho^{-1}\alpha_ {t}\ell_{f,1}\ell_{f}\|\mathcal{A}_{ag}^{T}\|_{\infty,1}^{2}\|d_{t}\|\]

where the last inequality follows from Lemma 21 and the update of \(\theta_{t}\).

Finally, taking \(\alpha_{t}=\alpha\), plugging the above bound for \(J_{1}\) back into (F.16), and bounding \(\|d_{t}\|\) by Assumption 3 and that \(\|\lambda_{t}\|\leq c_{\overline{\lambda}}\) prove the result. 

Proof of Theorem 2.: We consider the following Lyapunov function with a constant vector \(\lambda=[\lambda_{f};\lambda_{h}]\in\Omega_{\lambda}\), where \(\lambda_{f}\in\Delta^{M}\), \(\lambda_{h}\in\mathbb{R}^{M_{h}}\).

\[\mathbb{V}_{t}\coloneqq\underbrace{\lambda_{f}^{\top}AF(\theta_{ t})}_{\mathbb{V}_{f,t}}+\underbrace{\frac{\alpha_{0}}{2\gamma_{0}}\|\lambda_{f,t}- \lambda_{f}\|^{2}}_{\mathbb{V}_{\lambda_{f,t}}}+\underbrace{\frac{\alpha_{0}} {2\gamma_{0}}\|\lambda_{h,t}-\lambda_{h}\|^{2}}_{\mathbb{V}_{\lambda_{h,t}}}+ \underbrace{\frac{\alpha_{0}}{2\gamma_{0}}\|\lambda_{h,t}-\lambda_{h}\|^{2}}_ {\mathbb{V}_{h,1,t}}+\underbrace{c_{V_{h}}\|H(\theta_{t})\|_{1}}_{\mathbb{V}_ {h,4,t}}.\] (F.17)

Recall that \(\lambda_{t}=[\lambda_{f,t};\lambda_{h,t}]\), and the algorithm takes the update \(\theta_{t+1}=\theta_{t}+\alpha_{t}d_{t}\) with \(d_{t}=\nabla F(\theta_{t})A_{ag}^{\top}\lambda_{t}\). From Assumption 2, the smoothness of the objectives, and Lemma 9, the function \(\lambda_{f}^{\top}AF(\theta)\) is smooth, thus

\[\mathbb{V}_{f,t+1}-\mathbb{V}_{f,t}\leq \langle\nabla F(\theta_{t})A^{\top}\lambda_{f},\theta_{t+1}-\theta _{t}\rangle+\frac{\ell_{f,1}}{2}\|A^{\top}\lambda_{f}\|_{1}\|\theta_{t+1}- \theta_{t}\|^{2}\] \[= \alpha_{t}\langle\nabla F(\theta_{t})A^{\top}\lambda_{f},d_{t} \rangle+\frac{\ell_{f,1}}{2}\alpha_{t}^{2}\|A^{\top}\lambda_{f}\|_{1}\|d_{t}\|^ {2}.\] (F.18)

By Lemma 17, taking \(\gamma_{t}>0\) and rearranging, we have

\[\langle\nabla F(\theta_{t})A^{\top}\lambda_{f},d_{t}\rangle\leq \frac{1}{2\gamma_{t}}\big{(}\|\lambda_{f,t}-\lambda_{f}\|^{2}-\| \lambda_{f,t+1}-\lambda_{f}\|^{2}\big{)}\] \[+\frac{1}{2}\gamma_{t}\|\nabla_{\lambda_{f}}\varphi(\lambda_{t}; \theta_{t})\|^{2}-\langle\lambda_{f,t},\nabla_{\lambda_{f}}\varphi(\lambda_{t };\theta_{t})\rangle.\] (F.19)

Combining (F.18) and (F.19), and choosing \(\frac{\alpha_{t}}{\gamma_{t}}=\frac{\alpha_{0}}{\gamma_{0}}\) for all \(t\in[T]\), we have

\[\mathbb{V}_{f,t+1}-\mathbb{V}_{f,t}+\mathbb{V}_{\lambda_{f},t+1}- \mathbb{V}_{\lambda_{f},t}\] \[\leq \frac{\ell_{f,1}}{2}\alpha_{t}^{2}\|A^{\top}\lambda_{f}\|_{1}\|d_ {t}\|^{2}+\frac{1}{2}\alpha_{t}\gamma_{t}\|\nabla_{\lambda_{f}}\varphi(\lambda_{ t};\theta_{t})\|^{2}-\alpha_{t}\langle\lambda_{f,t},\nabla_{\lambda_{f}} \varphi(\lambda_{t};\theta_{t})\rangle.\] (F.20)

By the smoothness of \(\lambda_{h}^{\top}H(\theta)\), and \(\nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})=-\nabla H(\theta_{t})^{ \top}d_{t}-c_{h}H(\theta_{t})\), it holds that

\[\mathbb{V}_{h,1,t+1}-\mathbb{V}_{h,1,t}\leq \alpha_{t}\lambda_{h}^{\top}\nabla H(\theta_{t})^{\top}d_{t}+\frac{ \ell_{f,1}}{2}\alpha_{t}^{2}\|B_{h}^{\top}\lambda_{h}\|_{1}\|d_{t}\|^{2}\]\[= -\alpha_{t}c_{h}\lambda_{h}^{\top}H(\theta_{t})+\frac{\ell_{f,1}}{2} \alpha_{t}^{2}\|B_{h}^{\top}\lambda_{h}\|_{1}\|d_{t}\|^{2}\] \[-\alpha_{t}\langle\lambda_{h},\nabla_{\lambda_{h}}\varphi(\lambda_ {t};\theta_{t})\rangle.\] (F.21)

Bounding the last term in the above inequality by Lemma 17, and taking \(\gamma_{t}>0\), we have

\[\mathbb{V}_{h,1,t+1}-\mathbb{V}_{h,1,t}\leq -\alpha_{t}c_{h}\lambda_{h}^{\top}H(\theta_{t})+\frac{\ell_{f,1}}{ 2}\alpha_{t}^{2}\|B_{h}^{\top}\lambda_{h}\|_{1}\|d_{t}\|^{2}+\frac{1}{2}\alpha_ {t}\gamma_{t}\|\nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})\|^{2}\] \[-\alpha_{t}\langle\lambda_{h,t},\nabla_{\lambda_{h}}\varphi( \lambda_{t};\theta_{t})\rangle+\frac{\alpha_{t}}{2\gamma_{t}}\big{(}\|\lambda_ {h,t}-\lambda_{h}\|^{2}-\|\lambda_{h,t+1}-\lambda_{h}\|^{2}\big{)}.\] (F.22)

Adding up (F.20) and (F.22) yields

\[\mathbb{V}_{f,t+1}-\mathbb{V}_{f,t}+\mathbb{V}_{\lambda,t+1}- \mathbb{V}_{\lambda,t}+\mathbb{V}_{h,1,t+1}-\mathbb{V}_{h,1,t}\] \[\leq -\alpha_{t}(\lambda_{t},\nabla_{\lambda}\varphi(\lambda_{t}; \theta_{t}))+\frac{1}{2}\gamma_{t}\alpha_{t}\|\nabla_{\lambda}\varphi(\lambda_ {t};\theta_{t})\|^{2}-\alpha_{t}c_{h}\lambda_{h}^{\top}H(\theta_{t})+\frac{\ell _{f,1}}{2}\alpha_{t}^{2}\|A_{ag}^{\top}\lambda\|_{1}\|d_{t}\|^{2}\] \[\leq -\alpha_{t}\|d_{t}\|^{2}+\alpha_{t}c_{h}(\lambda_{h,t}-\lambda_{h })^{\top}H(\theta_{t})+\frac{1}{2}\gamma_{t}\alpha_{t}\|\nabla_{\lambda}\varphi (\lambda_{t};\theta_{t})\|^{2}+\frac{\ell_{f,1}}{2}\alpha_{t}^{2}\|A_{ag}^{\top }\lambda\|_{1}\|d_{t}\|^{2}\] (F.23)

where the last inequality uses the fact that \(\langle\lambda_{t},\nabla_{\lambda}\varphi(\lambda_{t};\theta_{t})\rangle=\|d _{t}\|^{2}-c_{h}\lambda_{h,t}^{\top}H(\theta_{t})\).

Using the fact that \(\nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})=-\nabla H(\theta_{t})^{ \top}d_{t}-c_{h}H(\theta_{t})\), and with similar arguments as (E.20) in Lemma 11, we can further derive that

\[|H(\theta_{t+1})|_{\rm ab}\leq |H(\theta_{t})-\alpha_{t}c_{h}H(\theta_{t})-\alpha_{t}\nabla_{ \lambda_{h}}\varphi(\lambda_{t};\theta_{t})|_{\rm ab}+\frac{\ell_{f,1}}{2} \alpha_{t}^{2}\|B_{h}^{\top}\|_{\infty,1}\|d_{t}\|^{2}\mathbf{1}\] \[\leq (1-\alpha_{t}c_{h})|H(\theta_{t})|_{\rm ab}+\frac{\ell_{f,1}}{2} \alpha_{t}^{2}\|B_{h}^{\top}\|_{\infty,1}\|d_{t}\|^{2}\mathbf{1}+\alpha_{t}| \nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})|_{\rm ab}.\] (F.24)

Therefore,

\[\mathbb{V}_{h,2,t+1}-\mathbb{V}_{h,3,t}\leq-\alpha_{t}c_{h}c_{V_{h}}\|H(\theta_ {t})\|_{1}+\frac{\ell_{f,1}}{2}c_{V_{h}}M_{h}\alpha_{t}^{2}\|B_{h}^{\top}\|_{ \infty,1}\|d_{t}\|^{2}+\alpha_{t}c_{V_{h}}\|\nabla_{\lambda_{h}}\varphi(\lambda _{t};\theta_{t})\|_{1}.\] (F.25)

Combining (F.23) and (F.25), and by choosing step sizes \(\alpha_{t}\), \(\gamma_{t}\), parameter \(c_{V_{h}}\) such that

\[\frac{\ell_{f,1}}{2}c_{V_{h}}M_{h}\alpha_{t}^{2}\|B_{h}^{\top}\|_{\infty,1}+ \frac{\ell_{f,1}}{2}\alpha_{t}^{2}\|A_{ag}^{\top}\lambda\|_{1}\leq\frac{1}{2},\] (F.26)

we have

\[\mathbb{V}_{t+1}-\mathbb{V}_{t}\leq -\frac{1}{2}\alpha_{t}\|d_{t}\|^{2}-\alpha_{t}c_{h}(c_{V_{h}}-\| \lambda_{h}-\lambda_{h,t}\|_{1})\|H(\theta_{t})\|_{1}+\frac{1}{2}\gamma_{t} \alpha_{t}\ell_{\varphi}^{2}+\alpha_{t}c_{V_{h}}\|\nabla_{\lambda_{h}}\varphi( \lambda_{t};\theta_{t})\|_{1}.\] (F.27)

Taking telescoping sum of the above inequality over \(t=0,\dots,T-1\), and applying that \(\|\nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})\|_{1}\leq\sqrt{M_{h}}\| \nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})\|\), we have

\[\sum_{t=0}^{T-1}\mathbb{V}_{t+1}-\mathbb{V}_{t}\leq \sum_{t=0}^{T-1}- \frac{1}{2}\alpha_{t}\|d_{t}\|^{2}-\alpha_{t}c_{h}(c_{V_{h}}-\| \lambda_{h}-\lambda_{h,t}\|_{1})\|H(\theta_{t})\|_{1}+\frac{1}{2}\gamma_{t} \alpha_{t}\ell_{\varphi}^{2}\] \[+\alpha_{t}c_{V_{h}}\sqrt{M_{h}}\|\nabla_{\lambda_{h}}\varphi( \lambda_{t};\theta_{t})\|\] (F.28)

where \(\sum_{t=0}^{T-1}\|\nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})\|\) can be further bounded by applying Lemma 22 and Corollary 20 along with Jensen's inequality as follows

\[\Big{(}\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla_{\lambda_{h}}\varphi( \lambda_{t};\theta_{t})\|\Big{)}^{2}\leq\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla_{ \lambda_{h}}\varphi(\lambda_{t};\theta_{t})\|^{2}\] \[\stackrel{{\text{Corollary \ref{cor\[\|H(\theta_{t+1})\|^{2}-\|H(\theta_{t})\|^{2}\leq\alpha_{t}2H(\theta_{t})^{\top} \nabla H(\theta_{t})^{\top}d_{t}+\frac{1}{2}\alpha_{t}^{2}\|\nabla^{2}(H(\tilde{ \theta}_{t})^{\top}H(\tilde{\theta}_{t}))\|\|d_{t}\|^{2}.\] (F.37)

The term \(\|\nabla^{2}(H(\tilde{\theta}_{t})^{\top}H(\tilde{\theta}_{t}))\|\) can be upper bounded by

\[\|\nabla^{2}(H(\tilde{\theta}_{t})^{\top}H(\tilde{\theta}_{t}))\|\leq 2\|\nabla H(\tilde{\theta}_{t})\nabla H(\tilde{\theta}_{t})^{\top}\|+2\| \nabla^{2}H(\tilde{\theta}_{t})\|\|H(\tilde{\theta}_{t})\|\]\[\|\nabla_{\theta}\varphi(\lambda;\theta)-\nabla_{\theta}\varphi( \lambda;\theta^{\prime})\|\leq\|\nabla F(\theta)-\nabla F(\theta^{\prime})\|\|v\| \|\nabla^{2}F(\theta)v\|\] \[\qquad+\|\nabla F(\theta^{\prime})v\|\|\nabla^{2}F(\theta)-\nabla^ {2}F(\theta^{\prime})\|\|v\|+c_{h}\|\lambda_{h}\|\|\nabla H(\theta)-\nabla H( \theta^{\prime})\|\] \[\leq \big{(}M\ell_{f,1}^{2}\|v\|^{2}+M\ell_{f}\ell_{f,2}\|v\|^{2}+c_{ h}\|\lambda_{h}\|\|B_{h}\|\ell_{f,1})\|\theta-\theta^{\prime}\|\] (F.42)

where the last inequality follows from Assumptions 2 and 3. Using the fact that \(\lambda_{f}\in\Delta^{M}\), and \(\|\lambda_{h,t}\|\leq c_{\lambda_{h}}\) on the trajectory of Algorithm 2, for all \(\lambda\in\{\lambda_{t}\}\) on the trajectory of Algorithm 2, \(\|v\|=\|A_{ag}^{\top}\lambda\|\) can be further bounded by

\[\|A_{ag}^{\top}\lambda\|\leq\|A^{\top}\lambda_{f}\|+\|B_{h}^{\top} \lambda_{h}\|\leq\|A\|+\|B_{h}\|c_{\lambda_{h}}.\] (F.43)

Plugging the above inequality back into (F.42) completes the proof. 

**Lemma 26** ([53, Lemma 4]).: _Let \(\Omega\subseteq\mathbb{R}^{M}\) be a closed convex set, and let \(\Pi_{\Omega}\) denote Euclidean projection to \(\Omega\). Given any \(\lambda\in\Omega,d\in\mathbb{R}^{M}\) and \(\gamma>0\), it holds that_

\[\Pi_{\Omega}(\lambda-\gamma d)=\operatorname*{arg\,min}_{\lambda^{ \prime}\in\Omega}\langle d,\lambda^{\prime}\rangle+\frac{1}{2\gamma}\|\lambda- \lambda^{\prime}\|^{2}.\] (F.44)

**Lemma 27** (Proximal PL inequality implies proximal error bound and quadratic growth).: _Suppose Assumptions 1, 3, and 4-1 hold. Then for \(\lambda,\theta\) on the trajectory of Algorithm 2, \(\varphi(\lambda;\theta)+g(\lambda)\), with \(g(\lambda)\) being an indicator function defined on the set \(\Omega_{\lambda}\), satisfies the \(\frac{1}{\mu_{\varphi}}\)-proximal error bound (EB) and the \(\frac{1}{\mu_{\varphi}}\)-quadratic growth (QG) w.r.t. \(\lambda\) for some \(\bar{\mu}_{\varphi},\mu_{\varphi}^{\prime}>0\) depending on \(\mu_{\varphi}\), as defined below_

\[\frac{1}{\bar{\mu}_{\varphi}}\mathrm{dist}(\lambda,S_{\varphi}( \theta))\leq\frac{1}{\gamma}\|\lambda-\Pi_{\Omega_{\lambda}}(\lambda-\gamma \nabla_{\lambda}\varphi(\lambda;\theta))\|\quad\text{(proximal EB)}\] (F.45) \[\frac{1}{\mu_{\varphi}^{\prime}}\mathrm{dist}^{2}(\lambda,S_{ \varphi}(\theta))\leq\varphi(\lambda;\theta)-\varphi(\lambda^{*}(\theta); \theta)\quad\text{(QG)}\] (F.46)

_where \(S_{\varphi}(\theta)\coloneqq\{\lambda\in\Omega_{\lambda}\mid\varphi(\lambda; \theta)=\varphi(\lambda^{*}(\theta);\theta)\}\)._

Proof of Lemma 27.: By Lemma 15, \(\varphi(\lambda;\theta)\) is smooth w.r.t. \(\lambda\). Furthermore, by [28, Appendix G], and combined with Assumption 4-1, the proximal PL inequality, it implies that \(\varphi(\lambda;\theta)+g(\lambda)\) satisfies the proximal error bound. From [10, Corollary 3.6], the proximal error bound further implies the quadratic growth, which proves the result.

**Lemma 28** (Lipschitz continuity of \(\lambda^{*}(\theta)\), [53, Lemma 5]).: _Suppose Assumption 3 holds. If given \(\lambda\in\Omega_{\lambda}\), and \(\theta^{\prime}\in\mathbb{R}^{q}\), \(\varphi(\lambda;\theta^{\prime})\) satisfies the \(\frac{1}{\mu_{\varphi}}\)-proximal error bound w.r.t. \(\lambda\). Then given \(\theta\in\mathbb{R}^{q}\), for any \(\lambda^{*}(\theta)\in\arg\min_{\lambda\in\Omega_{\lambda}}\varphi(\lambda;\theta)\), there exists \(\lambda^{*}(\theta^{\prime})\in\arg\min_{\lambda\in\Omega_{\lambda}}\varphi( \lambda;\theta^{\prime})\) such that_

\[\|\lambda^{*}(\theta)-\lambda^{*}(\theta^{\prime})\|\leq\ell_{\lambda^{*}}\| \theta-\theta^{\prime}\|\]

_with \(\ell_{\lambda^{*}}=\ell_{\varphi_{\lambda},1}\bar{\mu}_{\varphi}\), and \(\ell_{\varphi_{\lambda},1}\) defined in Lemma 15._

**Lemma 29** (Danskin-type Lemma for proximal PL functions [53, Proposition 6]).: _Suppose Assumptions 1, 2, 3, 4 hold, then \(\varphi(\lambda^{*}(\theta);\theta)\) is differentiable with the gradient computed by_

\[\nabla\varphi(\lambda^{*}(\theta);\theta)=\nabla_{\theta}\varphi(\lambda; \theta),\quad\forall\lambda\in\operatorname*{arg\,min}_{\lambda\in\Omega_{ \lambda}}\varphi(\lambda;\theta).\] (F.47)

_Moreover, \(\varphi(\lambda^{*}(\theta);\theta)\) is \(\ell_{\varphi^{*},1}\)-smooth with \(\ell_{\varphi^{*},1}:=\ell_{\varphi,1}(1+\ell_{\lambda^{*}})\)._

Below, Lemma 30 establishes the approximate descent or contraction of the subprogram after taking one-step update on \(\lambda_{t}\). This is crucial for a sharper analysis of convergence of Algorithm 2.

**Lemma 30** (Error of subprogram).: _Suppose Assumptions 1, 2, 3, 4 hold, \(M_{g}=0\), and \(\Omega_{\lambda_{f}}(\theta)=\Delta^{M}\). Let \(\{\theta_{t}\},\{\lambda_{t}\}\) be the sequences produced by Algorithm 2 with step size \(\gamma_{t}\leq\ell_{\varphi_{\lambda},1}^{-1}\). Then for any \(c_{\varphi,d}>0\), the following hold_

\[\varphi(\lambda_{t+1};\theta_{t})-\varphi(\lambda^{*}(\theta_{t}); \theta_{t})\leq(1-\gamma_{t}\mu_{\varphi})\big{(}\varphi(\lambda_{t};\theta_{t })-\varphi(\lambda^{*}(\theta_{t});\theta_{t})\big{)}\] (F.48a) \[\varphi(\lambda_{t+1};\theta_{t+1})-\varphi(\lambda^{*}(\theta_{t +1});\theta_{t+1})\leq\Big{(}1+\alpha_{t}c_{\varphi,d}\ell_{\varphi_{\theta},1 }^{2}\mu_{\varphi}^{\prime}\Big{)}\Big{(}\varphi(\lambda_{t+1};\theta_{t})- \varphi(\lambda^{*}(\theta_{t});\theta_{t})\Big{)}\] \[\qquad\qquad\qquad\qquad+\Big{(}\frac{\alpha_{t}}{2c_{\varphi,d} }+\frac{\ell_{\varphi_{\theta},1}+\ell_{\varphi^{*},1}}{2}\alpha_{t}^{2}\Big{)} \|d_{t}\|^{2}.\] (F.48b)

Proof of Lemma 30.: We first prove (F.48a). Recall the definition of \(D_{\varphi,\gamma}(\lambda;\theta)\) in Definition 3. By the \(\ell_{\varphi_{\lambda},1}\)-smoothness of \(\varphi\) w.r.t. \(\lambda\) and the update on \(\lambda_{t}\), and that \(\gamma_{t}\leq\ell_{\varphi_{\lambda},1}^{-1}\), we have

\[\varphi(\lambda_{t+1};\theta_{t})\leq\varphi(\lambda_{t};\theta_{ t})+\langle\nabla_{\lambda}\varphi(\lambda_{t};\theta_{t}),\lambda_{t+1}- \lambda_{t}\rangle+\frac{1}{2\gamma_{t}}\|\lambda_{t+1}-\lambda_{t}\|^{2}\] \[\leq \varphi(\lambda_{t};\theta_{t})-\frac{\gamma_{t}}{2}D_{\varphi, \gamma_{t}}(\lambda_{t};\theta_{t})\quad\text{by Lemma \ref{lem:main-step-update-\[\forall_{t}\coloneqq\underbrace{\lambda_{f}^{\top}AF(\theta_{t})}_{ \forall_{f,t}}+\underbrace{\frac{\alpha_{0}}{2\gamma_{0}}\|\lambda_{f,t}-\lambda_ {f}\|^{2}}_{\forall_{\lambda_{f,t}}}+\underbrace{\underbrace{\lambda_{h,t}^{\top }H(\theta_{t})}_{\forall_{h,0,t}}+\underbrace{\frac{1}{2}\|H(\theta_{t})\|^{2} }_{\forall_{h,3,t}}}_{\forall_{h,t}}+\underbrace{\underbrace{\varphi(\lambda_{t };\theta_{t})-\varphi(\lambda^{*}(\theta_{t});\theta_{t})}_{\forall_{\varphi,t}}}_ {\forall_{h,t}}.\] (F.57)

Following the same arguments from (F.18)-(F.20), and by choosing \(\frac{\alpha_{t}}{\gamma_{t}}=\frac{\alpha_{0}}{\gamma_{0}}=\frac{1}{c_{ \gamma,\alpha}}\) for all \(t\in[T]\), we have

\[\forall_{f,t+1}-\mathbb{V}_{f,t}+\mathbb{V}_{\lambda_{f},t+1}- \mathbb{V}_{\lambda_{f},t}\] \[\leq \frac{\ell_{f,1}}{2}\alpha_{t}^{2}\|A^{\top}\lambda_{f}\|_{1}\|d_ {t}\|^{2}+\frac{1}{2}\alpha_{t}\gamma_{t}\|\nabla_{\lambda_{f}}\varphi(\lambda_ {t};\theta_{t})\|^{2}-\alpha_{t}\langle\lambda_{f,t},\nabla_{\lambda_{f}} \varphi(\lambda_{t};\theta_{t})\rangle.\] (F.58)Similarly, we can derive that

\[\mathbb{V}_{h,0,t+1}-\mathbb{V}_{h,0,t}\leq -\alpha_{t}c_{h}\lambda_{h,t}^{\top}H(\theta_{t})+\frac{\ell_{f,1}}{ 2}\alpha_{t}^{2}\|B_{h}^{\top}\lambda_{h,t}\|_{1}\|d_{t}\|^{2}-\alpha_{t} \langle\lambda_{h,t},\nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})\rangle\] \[+(\lambda_{h,t+1}-\lambda_{h,t})^{\top}H(\theta_{t}).\] (F.59)

Combining (F.58) and (F.59) yields

\[\mathbb{V}_{f,t+1}-\mathbb{V}_{f,t}+\mathbb{V}_{\lambda_{f},t+1}- \mathbb{V}_{\lambda_{f},t}+\mathbb{V}_{h,0,t+1}-\mathbb{V}_{h,0,t}\leq-\alpha_ {t}\|d_{t}\|^{2}\] \[\quad+\frac{1}{2}\gamma_{t}\alpha_{t}\|\nabla_{\lambda_{f}} \varphi(\lambda_{t};\theta_{t})\|^{2}+\frac{\ell_{f,1}}{2}\alpha_{t}^{2}(\|A^{ \top}\lambda_{f}\|_{1}+\|B_{h}^{\top}\lambda_{h,t}\|_{1})\|d_{t}\|^{2}-\gamma_ {t}\nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})^{\top}H(\theta_{t}).\] (F.60)

Next we proceed to bound \(\mathbb{V}_{h,3,t+1}-\mathbb{V}_{h,3,t}\). By Lemma 24, it holds that

\[\mathbb{V}_{h,3,t+1}-\mathbb{V}_{h,3,t}\leq\alpha_{t}H(\theta_{t})^{\top}\nabla H (\theta_{t})^{\top}d_{t}+\frac{1}{4}\alpha_{t}^{2}\ell_{H^{2},1,t}\|d_{t}\|^{2}\] (F.61)

where \(\ell_{H^{2},1,t}=2M\ell_{f}^{2}+2(\alpha_{t}^{-1}c_{\alpha,h}+\ell_{H}c_{d}) \sqrt{M}\ell_{f,1}\). Because \(\nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})=-\nabla H(\theta_{t})^{\top }d_{t}-c_{h}H(\theta_{t})\), the term \(H(\theta_{t})^{\top}\nabla H(\theta_{t})^{\top}d_{t}\) can be further written as

\[H(\theta_{t})^{\top}\nabla H(\theta_{t})^{\top}d_{t}= -H(\theta_{t})^{\top}\big{(}\nabla_{\lambda_{h}}\varphi(\lambda_{ t};\theta_{t})+c_{h}H(\theta_{t})\big{)}\] \[= -c_{h}\|H(\theta_{t})\|^{2}-H(\theta_{t})^{\top}\nabla_{\lambda_{ h}}\varphi(\lambda_{t};\theta_{t}).\] (F.62)

Plugging (F.62) into (F.61) yields

\[\frac{1}{2}\|H(\theta_{t+1})\|^{2}-\frac{1}{2}\|H(\theta_{t})\|^{2}\leq-\alpha _{t}c_{h}\|H(\theta_{t})\|^{2}-\alpha_{t}H(\theta_{t})^{\top}\nabla_{\lambda_{ h}}\varphi(\lambda_{t};\theta_{t})+\frac{1}{4}\alpha_{t}^{2}\ell_{H^{2},1,t}\|d_{t}\|^{2}.\] (F.63)

Letting \(\ell_{FH,1}=\ell_{f,1}(\|A^{\top}\|_{1}+\|B_{h}^{\top}\|_{1}c_{\lambda_{h}})\), and adding up (F.60) and (F.63), we have

\[\mathbb{V}_{f,t+1}-\mathbb{V}_{f,t}+\mathbb{V}_{\lambda_{f},t+1}- \mathbb{V}_{\lambda_{f},t}+\mathbb{V}_{h,t+1}-\mathbb{V}_{h,t}\leq-\alpha_{t} \|d_{t}\|^{2}-\alpha_{t}c_{h}\|H(\theta_{t})\|^{2}\] \[\quad-(\alpha_{t}+\gamma_{t})\nabla_{\lambda_{h}}\varphi(\lambda_{ t};\theta_{t})^{\top}H(\theta_{t})+\frac{1}{2}\gamma_{t}\alpha_{t}\|\nabla_{ \lambda_{f}}\varphi(\lambda_{t};\theta_{t})\|^{2}+\frac{1}{4}\alpha_{t}^{2}(2 \ell_{FH,1}+\ell_{H^{2},1,t})\|d_{t}\|^{2}\] (F.64)

where \(\|\nabla_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})\|^{2}\leq\|\nabla_{ \lambda}\varphi(\lambda_{t};\theta_{t})\|^{2}\) is further bounded by Lemma 23, (F.32) as

\[\|\nabla_{\lambda}\varphi(\lambda_{t};\theta_{t})\|^{2}\leq 2\|A_{ag}\|^{2}M\ell_{f}^{2}\|d_{t}\|^{2}+2c_{h}^{2}\|H(\theta_{t})\|^{2}.\] (F.65)

Plugging (F.65) back into (F.64) yields

\[\mathbb{V}_{f,t+1}-\mathbb{V}_{f,t}+\mathbb{V}_{\lambda_{f},t+1}- \mathbb{V}_{\lambda_{f},t}+\mathbb{V}_{h,t+1}-\mathbb{V}_{h,t}\leq-\alpha_{t} \|d_{t}\|^{2}-\alpha_{t}c_{h}\|H(\theta_{t})\|^{2}\] \[\quad-(\alpha_{t}+\gamma_{t})\nabla_{\lambda_{h}}\varphi(\lambda_{ t};\theta_{t})^{\top}H(\theta_{t})+\underbrace{\frac{1}{4}\alpha_{t}^{2}(2\ell_{FH,1}+ \ell_{H^{2},1,t})\|d_{t}\|^{2}}_{\mathcal{J}_{1}}\] \[\quad+\underbrace{\gamma_{t}\alpha_{t}\|A_{ag}\|^{2}M\ell_{f}^{2} \|d_{t}\|^{2}}_{\mathcal{J}_{2}}+\underbrace{\gamma_{t}\alpha_{t}c_{h}^{2}\|H( \theta_{t})\|^{2}}_{\mathcal{J}_{3}}\] (F.66)

where by choosing the step sizes \(\alpha_{t}\leq\frac{1}{2\ell_{FH,1}+\ell_{H^{2},1,t}}\), \(\gamma_{t}\leq\min\left\{\frac{1}{4\|A_{ag}\|^{2}M\ell_{f}^{2}},\frac{1}{2c_{h }}\right\}\), it holds that

\[J_{1}\leq\frac{1}{4}\alpha_{t}\|d_{t}\|^{2},\;\;J_{2}\leq\frac{1}{4}\alpha_{t} \|d_{t}\|^{2},\;\;J_{3}\leq\frac{1}{2}\alpha_{t}c_{h}\|H(\theta_{t})\|^{2}.\] (F.67)

Plugging (F.67) into (F.66), and rearranging, we have

\[\mathbb{V}_{f,t+1}-\mathbb{V}_{f,t}+\mathbb{V}_{\lambda_{f},t+1}- \mathbb{V}_{\lambda_{f},t}+\mathbb{V}_{h,t+1}-\mathbb{V}_{h,t}\] \[\leq -\frac{1}{2}\alpha_{t}\|d_{t}\|^{2}-\frac{1}{2}\alpha_{t}c_{h}\| H(\theta_{t})\|^{2}-(\alpha_{t}+\gamma_{t})\nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})^{ \top}H(\theta_{t})\] \[\leq -\frac{1}{2}\alpha_{t}\|d_{t}\|^{2}-\frac{1}{4}\alpha_{t}c_{h}\| H(\theta_{t})\|^{2}+\frac{(1+c_{\gamma,\alpha})^{2}}{c_{h}}\alpha_{t}\|\nabla_{\lambda_{h}} \varphi(\lambda_{t};\theta_{t})\|^{2}\] (F.68)

where the last inequality follows from Cauchy-Schwarz inequality and that \(\gamma_{t}=c_{\gamma,\alpha}\alpha_{t}\).

By applying Corollary 31 with \(\gamma_{t}\leq\ell_{\varphi_{\lambda,1}}^{-1}\), \(\alpha_{t}\leq\frac{1}{(\ell_{\varphi_{\rho,1}+1}\ell_{\varphi_{*},1})c_{\varphi,d}}\) and \(c_{\gamma,\alpha}\geq\frac{2c_{\varphi,d}\ell_{\varphi_{\rho,1}}\mu_{\varphi}^{ \prime}}{\mu_{\varphi}}\), we further have that

\[\mathbb{V}_{\varphi,t+1}-\mathbb{V}_{\varphi,t}\leq-\frac{1}{2}\mu_{\varphi} \gamma_{t}\big{(}\varphi(\lambda_{t};\theta_{t})-\varphi(\lambda^{*}(\theta_{t} );\theta_{t})\big{)}+\frac{\alpha_{t}}{c_{\varphi,d}}\|d_{t}\|^{2}.\] (F.69)

Then note that from (F.11) we have \(\|\nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})\|^{2}\leq 2\ell_{\varphi_{ \lambda,1}}\big{(}\varphi(\lambda_{t};\theta_{t})-\varphi(\lambda^{*}(\theta_{t });\theta_{t})\big{)}\). Adding up (F.69) and (F.68) with properly chosen hyperparameters \(c_{h}\geq(1+c_{\gamma,\alpha})^{2}\), \(c_{\gamma,\alpha}\geq\frac{4\ell_{\varphi_{\lambda,1}}}{\mu_{\varphi}}\), and \(c_{\varphi,d}=4\) yields

\[\mathbb{V}_{t+1}-\mathbb{V}_{t}\leq-\frac{1}{4}\alpha_{t}\|d_{t}\|^{2}-\frac{ 1}{4}\alpha_{t}c_{h}\|H(\theta_{t})\|^{2}.\]

Taking telescoping sum of the above inequality over \(t=0,\ldots,T-1\) yields

\[\sum_{t=0}^{T-1}\alpha_{t}\Big{(}\|d_{t}\|^{2}+c_{h}\|H(\theta_{t })\|^{2}\Big{)}\leq 4(\mathbb{V}_{0}-\mathbb{V}_{T})\] \[\leq 4\mathbb{V}_{f,0}+4\big{(}\lambda_{h,0}^{\top}H(\theta_{0})- \lambda_{h,T}^{\top}H(\theta_{T})\big{)}+2\|H(\theta_{0})\|^{2}+4\mathbb{V}_{ \varphi,0}\leq 2c_{0}+8c_{\lambda_{h}}c_{H}\] (F.70)

where the second last inequality follows from \(\mathbb{V}_{f,t}\geq 0\), choosing \(\lambda_{f}=\lambda_{f,0}\), and \(\mathbb{V}_{\varphi,t}\geq 0\), the last inequality follows from choosing \(\theta_{0},\lambda_{0}\) such that \(\lambda_{f}^{\top}AF(\theta_{0}),H(\theta_{0}),\varphi(\lambda_{0};\theta_{0}) -\varphi(\lambda^{*}(\theta_{0});\theta_{0})\) are bounded, thus \(2\mathbb{V}_{f,0}+\|H(\theta_{0})\|^{2}+2\mathbb{V}_{\varphi,0}\leq c_{0}<\infty\), \(\lambda_{h,t}\) are bounded on the trajectory, and \(\|H(\theta_{0})\|_{1},\|H(\theta_{T})\|_{1}\leq c_{H}\), thus \(4\big{(}\lambda_{h,0}^{\top}H(\theta_{0})-\lambda_{h,T}^{\top}H(\theta_{T}) \big{)}\leq 8c_{\lambda_{h}}c_{H}\).

We then summarize the best possible choices for \(\alpha_{t},\gamma_{t}\). Recall that we require \(\alpha_{t}\leq\frac{1}{2\ell_{FH,1}+\ell_{H^{2},1,t}}\). Rearranging this inequality with \(\ell_{H^{2},1,t}=2M\|B_{h}\|^{2}\ell_{f}^{2}+2(\alpha_{t}^{-1}+\ell_{H})c_{ \alpha,h}\sqrt{M}\ell_{f,1}\|B_{h}\|\), and choosing \(c_{\alpha,h}=\frac{1}{4\sqrt{M}\ell_{f,1}\|B_{h}\|}\) yield

\[\alpha_{t}(2\ell_{FH,1}+\ell_{H^{2},1,t})= 2\alpha_{t}\ell_{FH,1}+2\alpha_{t}M\|B_{h}\|^{2}\ell_{f}^{2}+ \frac{1}{2}(1+\alpha_{t}\ell_{H})\leq 1.\] (F.71)

Then we can choose the following to ensure the above inequality holds

\[\alpha_{t}\leq\frac{1}{4\big{(}\ell_{FH,1}+M\|B_{h}\|^{2}\ell_{f}^{2}\big{)}+ \ell_{H}}.\] (F.72)

To summarize, we can choose the following hyperparameters and step sizes

\[c_{\gamma,\alpha}\geq\max\Big{\{}\frac{8\ell_{\varphi_{\theta,1}} ^{2}\mu_{\varphi}^{\prime}}{\mu_{\varphi}},\frac{4\ell_{\varphi_{\lambda,1}}}{ \mu_{\varphi}}\Big{\}},\ c_{h}=(1+c_{\gamma,\alpha})^{2},\ \ c_{\alpha,h}=\frac{1}{4\sqrt{M} \ell_{f,1}\|B_{h}\|}\] (F.73a) \[\gamma_{t}=c_{\gamma,\alpha}\alpha_{t},\ \text{ and }\alpha_{t}=\min \Big{\{}\frac{c_{\alpha,h}}{\max\{\|H(\theta_{t})\|,\ell_{f,1}\|A_{ ag}^{\top}\|_{1}(1+c_{\lambda_{h}})\}},\frac{1}{4(\ell_{\varphi_{\theta,1}}+\ell_{ \varphi^{*},1})},\] \[\frac{1}{c_{\gamma,\alpha}\ell_{\varphi_{\lambda,1}}},\frac{1}{4c_ {\gamma,\alpha}\|A_{ag}\|^{2}M\ell_{f}^{2}},\frac{1}{2c_{\gamma,\alpha}c_{h}}, \frac{1}{4\big{(}\ell_{FH,1}+M\|B_{h}\|^{2}\ell_{f}^{2}\big{)}+\ell_{H}}\Big{\}},\] (F.73b)

where \(\ell_{FH,1}=\ell_{f,1}(\|A^{\top}\|_{1}+\|B_{h}^{\top}\|_{1}c_{\lambda_{h}})\), and \(\ell_{H}=\|B_{h}\|\sqrt{M}\ell_{f}\). Then it holds that

\[\sum_{t=0}^{T-1}\alpha_{t}\Big{(}\|d_{t}\|^{2}+c_{h}\|H(\theta_{t})\|^{2}\Big{)} =\mathcal{O}(1).\] (F.74)

Therefore, \(\alpha_{t}c_{h}\|H(\theta_{t})\|^{2}\) are bounded for all \(t=0,\ldots,T\). Combining with Lemma 32, we have \(\|H(\theta_{t})\|\) are bounded for all \(t=0,\ldots,T\), thus we can choose \(\alpha_{t}=\Omega(1)\), i.e., \(\alpha_{t}\) is lower bounded by a constant.

Collecting the results above, we have proved that we can choose \(\alpha_{t}=\Theta(1)\), \(\gamma_{t}=\Theta(1)\) such that

\[\frac{1}{T}\sum_{t=0}^{T-1}\Big{(}\|d_{t}\|^{2}+\|H(\theta_{t})\|^{2}\Big{)}= \mathcal{O}\Big{(}\frac{1}{T}\Big{)}.\] (F.75)

The proof is complete.

Stochastic Algorithms

In this section, we discuss the single-loop stochastic algorithm and its convergence guarantees. Note that, the extension of the analysis of the double-loop algorithm, i.e., Algorithm 1 and the extension of the single-loop algorithm analysis in Theorem 2 to their stochastic variants with double sampling as used in [7], are rather straightforward, thus we ommit the discussion in this paper, and only focus on the _single-loop stochastic_ algorithm with equality constraints only, i.e., \(M_{g}=0\), and with a sharper analysis as an extension of Theorem 3.

Let \(\xi\) and \(\xi^{\prime}\) be i.i.d. random variables. The stochastic constrained vector optimization problem is defined as

\[\min_{\theta\in\mathbb{R}^{q}}F(\theta)\coloneqq\mathbb{E}[F_{\xi}(\theta)],\ \ \text{s.t.}\ \ \ H(\theta)\coloneqq\mathbb{E}[H_{\xi^{\prime}}(\theta)]=0,\ \ \text{with}\ \ H_{\xi^{\prime}}(\theta)=B_{h}F_{\xi^{\prime}}(\theta)+b_{h}.\] (G.1)

### Algorithm summary

The stochastic algorithm is summarized in Algorithm 3. Note that, instead of computing \(\nabla F_{\xi_{t,1}}(\theta_{t}),\nabla F_{\xi_{t,2}}(\theta_{t})\), which requires \(2M\) gradient computation at each iteration, we compute \(\nabla F_{\xi_{t,1}}(\theta_{t}),\nabla\big{(}F_{\xi_{t,2}}(\theta_{t})A_{ag}^{\top}\lambda_{t}\big{)}\), which requires \(M+1\) gradient computation per iteration. This saves nearly half of the per-iteration complexity compared to the most relevant existing stochastic algorithm for multi-objective optimization [7]. Furthermore, with the gradient-based single-loop update for \(\lambda_{t}\), the approximation approach proposed in [34, Section 3.2] can be further applied to largely reduce the per-iteration complexity, which we leave for future work.

```
1:Initialize \(t=0\), \(\theta_{0}\), \(\lambda_{0}\), step sizes \(\alpha_{t}\), \(\gamma_{t}\);
2:for\(t=0,\dots,T-1\)do
3: Compute the stochastic gradients \(\nabla F_{\xi_{t,2}}(\theta_{t}),\nabla F_{\xi_{t,1}}(\theta_{t})A_{ag}^{\top} \lambda_{t}\);
4: Compute the stochastic estimate of the constraint \(H_{\xi_{t,1}}(\theta_{t})\);
5: Compute an update direction \(d_{t}=\nabla F_{\xi_{t,1}}(\theta_{t})A_{ag}^{\top}\lambda_{t}\);
6: Choose the step size \(\alpha_{t}\) by a predefined schedule;
7: Update \(\theta_{t}\) by \(\theta_{t+1}=\theta_{t}+\alpha_{t}d_{t}\);
8: Update \(\lambda_{t}\) by (3.4);
9:endfor
```

**Algorithm 3** Stochastic FERERO-SA

### Proof of Theorem 4: convergence of Algorithm 3

We first introduce the supporting lemmas, and then present the main proofs. Denote \(\mathcal{F}_{t}\) as the \(\sigma\)-algebra generated by \(\nabla F_{\xi_{0}}(\theta_{0}),\nabla F_{\xi_{1}}(\theta_{1}),\dots,\nabla F_{ \xi_{t}}(\theta_{t})\), where \(\xi_{t}=\{\xi_{t,1},\xi_{t,2}\}\). For brevity, we let \(\mathbb{E}_{t}[\cdot]\coloneqq\mathbb{E}[\cdot\mid\mathcal{F}_{t-1}]\). Also recall that \(\tilde{\nabla}\) is the unbiased stochastic estimate of the gradient.

We make the following additional assumptions for proof of convergence.

**Assumption 5**.: _For \(\{\theta_{t},\lambda_{t}\}_{t=0}^{T-1}\) on the trajectory of Algorithm 3, it holds that 1. The variance of \(\nabla F_{\xi_{t}}(\theta_{t})\) is bounded by \(\sigma^{2}\). 2. The variance of \(\tilde{\nabla}_{\lambda}\varphi(\lambda_{t};\theta_{t})\) is bounded by \(\gamma_{t}\sigma^{2}\). 3. The function \(\|H(\theta_{t})\|\) is bounded by \(c_{H}\)._

Note that the bounded variance assumption is common in optimization literature. However, for sharp analysis here, we additionally require \(\tilde{\nabla}_{\lambda}\varphi(\lambda_{t};\theta_{t})\) has reduced variance in the order of \(\mathcal{O}(\gamma_{t})\), which can be achieved using a large batch size. Note that, even without assuming reduced variance, i.e., Assumption 5-2, the stochastic algorithm still converges, which can be proved by extending Theorem 2 to the stochastic case using the same techniques in [7] for MoDo, a double-sampling-based single-loop stochastic variant of MGDA. However, the convergence rate will be slower than \(\mathcal{O}(T^{-\frac{1}{2}})\). Here we use this additional assumption to achieve a faster convergence rate.

The following Lemma 34 extends Lemma 17 to the stochastic case.

**Lemma 34**.: _Let \(\lambda_{t}=[\lambda_{f,t};\lambda_{h,t}]\). Consider the stochastic sequence \(\{\lambda_{t}\}_{t=0}^{T}\) produced by Algorithm 3. Then for all \(\lambda=[\lambda_{f};\lambda_{h}]\in\Omega_{\lambda}\), it holds that_

\[2\gamma_{t}\mathbb{E}_{t}[\langle\lambda_{f,t}-\lambda_{f}, \nabla_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})\rangle]\leq\mathbb{E}_{t}[ \|\lambda_{f,t}-\lambda_{f}\|^{2}-\|\lambda_{f,t+1}-\lambda_{f}\|^{2}+\gamma_{ t}^{2}\|\tilde{\nabla}_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})\|^{2}];\] \[2\gamma_{t}\mathbb{E}_{t}[\langle\lambda_{h,t}-\lambda_{h}, \nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})\rangle]\leq\mathbb{E}_{t}[ \|\lambda_{h,t}-\lambda_{h}\|^{2}-\|\lambda_{h,t+1}-\lambda_{h}\|^{2}+\gamma_ {t}^{2}\|\tilde{\nabla}_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})\|^{2}].\] (G.2)

Proof.: By the update of \(\lambda\), it holds that

\[\|\lambda_{f,t+1}-\lambda_{f}\|^{2}\leq\|\lambda_{f,t}-\gamma_{t }\tilde{\nabla}_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})-\lambda_{f}\|^{2}\] \[= \|\lambda_{f,t}-\lambda_{f}\|^{2}-2\gamma_{t}\langle\lambda_{f,t}- \lambda_{f},\tilde{\nabla}_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})\rangle+ \gamma_{t}^{2}\|\tilde{\nabla}_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})\|^ {2}.\] (G.3)

Taking expectation over the stochastic samples and rearranging the above inequality, we have

\[2\gamma_{t}\mathbb{E}_{t}[\langle\lambda_{f,t}-\lambda_{f}, \nabla_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})\rangle]=2\gamma_{t}\mathbb{ E}_{t}[\langle\lambda_{f,t}-\lambda_{f},\nabla_{\lambda_{f}}\varphi(\lambda_{t}; \theta_{t})\rangle]\] \[\leq \mathbb{E}_{t}[\|\lambda_{f,t}-\lambda_{f}\|^{2}-\|\lambda_{f,t+1} -\lambda_{f}\|^{2}+\gamma_{t}^{2}\|\tilde{\nabla}_{\lambda_{f}}\varphi( \lambda_{t};\theta_{t})\|^{2}].\] (G.4)

Following similar arguments, it holds that

\[2\gamma_{t}\mathbb{E}_{t}[\langle\lambda_{h,t}-\lambda_{h}, \nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})\rangle]\] \[\leq \mathbb{E}_{t}[\|\lambda_{h,t}-\lambda_{h}\|^{2}-\|\lambda_{h,t+1 }-\lambda_{h}\|^{2}+\gamma_{t}^{2}\|\tilde{\nabla}_{\lambda_{h}}\varphi( \lambda_{t};\theta_{t})\|^{2}].\] (G.5)

The proof is complete. 

**Lemma 35** (Restatement of [50, Lemma 2]).: _Let \(\bar{\varphi}(x)=\varphi(x)+h(x)\), where \(\varphi:\mathbb{R}^{q}\to\mathbb{R}\) is \(L\)-smooth, and \(h:\mathbb{R}^{q}\to\mathbb{R}\) is nonsmooth but convex and relatively simple. Define \(y=\operatorname{prox}_{\gamma h}(x-\gamma d^{\prime})\) for some \(d^{\prime}\in\mathbb{R}^{q}\). Then for \(y\), the following inequality holds for all \(z\in\mathbb{R}^{q}\):_

\[\bar{\varphi}(y)\leq\bar{\varphi}(z)+ \langle y-z,\nabla\varphi(x)-d^{\prime}\rangle\] \[+(\frac{L}{2}-\frac{1}{2\gamma})\|y-x\|^{2}+(\frac{L}{2}+\frac{1}{ 2\gamma})\|z-x\|^{2}-\frac{1}{2\gamma}\|y-z\|^{2}.\] (G.6)

The following Lemma 36 extends Lemma 30 to the stochastic case.

**Lemma 36** (Error of subprogram in the stochastic setting).: _Suppose Assumptions 1, 2, 3, 4, 5 hold, and \(M_{g}=0\). Let \(\{\theta_{t}\},\{\lambda_{t}\}\) be the sequences produced by Algorithm 3 with step size \(\gamma_{t}\leq\ell_{\varphi_{\lambda},1}^{-1}\). Then for any \(c_{\varphi,d}>0\), the following hold_

\[\mathbb{E}[\varphi(\lambda_{t+1};\theta_{t})-\varphi(\lambda^{*}( \theta_{t});\theta_{t})]\leq(1-\gamma_{t}\mu_{\varphi})\mathbb{E}\big{[} \varphi(\lambda_{t};\theta_{t})-\varphi(\lambda^{*}(\theta_{t});\theta_{t}) \big{]}+\gamma_{t}^{2}\sigma^{2}\] (G.7a) \[\mathbb{E}[\varphi(\lambda_{t+1};\theta_{t+1})-\varphi(\lambda^{*} (\theta_{t+1});\theta_{t+1})]\leq\Big{(}1+\alpha_{t}c_{\varphi,d}\ell_{\varphi _{\theta},1}^{2}\mu_{\varphi}^{\prime}\Big{)}\mathbb{E}\big{[}\varphi(\lambda _{t+1};\theta_{t})-\varphi(\lambda^{*}(\theta_{t});\theta_{t})\Big{]}\] \[+\Big{(}\frac{\alpha_{t}}{2c_{\varphi,d}}+\frac{\ell_{\varphi_{ \theta},1}+\ell_{\varphi^{*},1}}{2}\alpha_{t}^{2}\Big{)}\mathbb{E}[\|\nabla F( \theta_{t})A_{ag}^{\top}\lambda_{t}\|^{2}]+\frac{\ell_{\varphi_{\theta},1}+ \ell_{\varphi^{*},1}}{2}\alpha_{t}^{2}\sigma^{2}.\] (G.7b)

Proof of Lemma 36.: The proof follows most of that of Lemma 30. We highlight the difference.

First we define \(\lambda_{t+1}^{\prime}=\Pi_{\Omega_{\lambda}}(\lambda_{t}-\nabla_{\lambda} \varphi(\lambda_{t};\theta_{t}))\) as an auxiliary variable. By the \(\ell_{\varphi_{\lambda},1}\)-smoothness of \(\varphi(\cdot;\theta)\), we have

\[\mathbb{E}[\varphi(\lambda_{t+1}^{\prime};\theta_{t})]\leq\mathbb{E}[\varphi( \lambda_{t};\theta_{t})+(\frac{\ell_{\varphi_{\lambda},1}}{2}-\frac{1}{ \gamma_{t}})\|\lambda_{t+1}^{\prime}-\lambda_{t}\|^{2}].\] (G.8)

Applying Lemma 35 with \(y=\lambda_{t+1},z=\lambda_{t+1}^{\prime}\), \(x=\lambda_{t}\), and that \(\lambda_{t},\lambda_{t+1},\lambda_{t+1}^{\prime}\in\Omega_{\lambda}\) yields

\[\mathbb{E}[\varphi(\lambda_{t+1};\theta_{t})]\leq\mathbb{E}\Big{[} \varphi(\lambda_{t+1}^{\prime};\theta_{t})+\langle\lambda_{t+1}-\lambda_{t+1}^{ \prime},\nabla_{\lambda}\varphi(\lambda_{t};\theta_{t})-\tilde{\nabla}_{ \lambda}\varphi(\lambda_{t};\theta_{t})\rangle\] \[+(\frac{\ell_{\varphi_{\lambda},1}}{2}-\frac{1}{2\gamma_{t}})\| \lambda_{t+1}-\lambda_{t}\|^{2}+(\frac{\ell_{\varphi_{\lambda},1}}{2}+\frac{1}{ 2\gamma_{t}})\|\lambda_{t+1}^{\prime}-\lambda_{t}\|^{2}-\frac{1}{2\gamma_{t}} \|\lambda_{t+1}-\lambda_{t+1}^{\prime}\|^{2}\Big{]}.\] (G.9)Furthermore, following similar arguments as (F.49), by Assumption 4-1, and taking total expectation, we have

\[\mathbb{E}[\varphi(\lambda^{\prime}_{t+1};\theta_{t})]{\leq}\mathbb{E}\Big{[} \varphi(\lambda_{t};\theta_{t})-\gamma_{t}\mu_{\varphi}\Big{(}\varphi(\lambda_{t };\theta_{t})-\varphi(\lambda^{*}(\theta_{t});\theta_{t})\Big{)}\Big{]}.\] (G.10)

Adding up \(\frac{2}{3}\times\) (G.8), \(1\times\) (G.9), and \(\frac{1}{3}\times\) (G.10) yields

\[\mathbb{E}[\varphi(\lambda_{t+1};\theta_{t})]\leq \mathbb{E}\Big{[}\varphi(\lambda_{t};\theta_{t})+\big{(}\frac{5 \ell_{\varphi_{\lambda,1}}}{6}-\frac{1}{6\gamma_{t}}\big{)}\|\lambda^{\prime}_ {t+1}-\lambda_{t}\|^{2}+\big{(}\frac{\ell_{\varphi_{\lambda,1}}}{2}-\frac{1}{2 \gamma_{t}}\big{)}\|\lambda_{t+1}-\lambda_{t}\|^{2}\] \[-\frac{\mu_{\varphi}\gamma_{t}}{3}\big{(}\varphi(\lambda_{t}; \theta_{t})-\varphi(\lambda^{*}(\theta_{t});\theta_{t})\big{)}-\frac{1}{2 \gamma_{t}}\|\lambda_{t+1}-\lambda^{\prime}_{t+1}\|^{2}\] \[+\langle\lambda_{t+1}-\lambda^{\prime}_{t+1},\nabla_{\lambda} \varphi(\lambda_{t};\theta_{t})-\tilde{\nabla}_{\lambda}\varphi(\lambda_{t}; \theta_{t})\rangle\Big{]}.\] (G.11)

Choosing \(\gamma_{t}\geq\frac{1}{\ell_{\varphi_{\lambda,1}}}\geq\frac{1}{5\ell_{\varphi_ {\lambda,1}}}\) and applying Cauchy-Schwarz and Young's inequality, we have

\[\mathbb{E}[\varphi(\lambda_{t+1};\theta_{t})]\leq \mathbb{E}\Big{[}\varphi(\lambda_{t};\theta_{t})-\frac{\mu_{ \varphi}\gamma_{t}}{3}\big{(}\varphi(\lambda_{t};\theta_{t})-\varphi(\lambda^{ *}(\theta_{t});\theta_{t})\big{)}\] \[+\frac{\gamma_{t}}{2}\|\nabla_{\lambda}\varphi(\lambda_{t};\theta _{t})-\tilde{\nabla}_{\lambda}\varphi(\lambda_{t};\theta_{t})\|^{2}\Big{]}.\] (G.12)

The first inequality is proved. We then prove the second inequality. Note that (F.50) still holds here. Following similar arguments in (F.51), \(\mathbb{E}[J_{1}]\) in (F.50) can be further bounded by

\[\mathbb{E}[J_{1}] \leq\mathbb{E}\Big{[}-\alpha_{t}\langle\nabla_{\theta}\varphi( \lambda_{t+1};\theta_{t})-\nabla\varphi(\lambda^{*}(\theta_{t});\theta_{t}),d_ {t}\rangle+\frac{\ell_{\varphi_{\theta},1}+\ell_{\varphi^{*},1}}{2}\alpha_{t} ^{2}\|d_{t}\|^{2}\Big{]}\] \[\leq\mathbb{E}\Big{[}-\alpha_{t}\langle\nabla_{\theta}\varphi( \lambda_{t+1};\theta_{t})-\nabla\varphi(\lambda^{*}(\theta_{t});\theta_{t}), \nabla F(\theta_{t})A_{ag}^{\top}\lambda_{t}\rangle+\frac{\ell_{\varphi_{ \theta},1}+\ell_{\varphi^{*},1}}{2}\alpha_{t}^{2}\|d_{t}\|^{2}\Big{]}\] \[\leq\mathbb{E}\Big{[}\alpha_{t}\|\nabla_{\theta}\varphi(\lambda _{t+1};\theta_{t})-\nabla\varphi(\lambda^{*}(\theta_{t});\theta_{t})\|\|\nabla F (\theta_{t})A_{ag}^{\top}\lambda_{t}\|+\frac{\ell_{\varphi_{\theta},1}+\ell_{ \varphi^{*},1}}{2}\alpha_{t}^{2}\|d_{t}\|^{2}\Big{]}.\] (G.13)

Then following similar arguments in (F.52) and (F.53), we have

\[\mathbb{E}[J_{1}]{\leq} \mathbb{E}\Big{[}\alpha_{t}c_{\varphi,d}\ell_{\varphi_{\theta}}^{ 2},\mu_{\varphi}^{\prime}\big{(}\varphi(\lambda_{t+1};\theta_{t})-\varphi( \lambda^{*}(\theta_{t});\theta_{t})\big{)}\] \[+\frac{\alpha_{t}}{2c_{\varphi,d}}\|\nabla F(\theta_{t})A_{ag}^{ \top}\lambda_{t}\|^{2}+\frac{\ell_{\varphi_{\theta},1}+\ell_{\varphi^{*},1}}{ 2}\alpha_{t}^{2}\|d_{t}\|^{2}\Big{]}.\] (G.14)

Plugging (G.14) back into (F.50) with total expectation completes the proof of the second inequality. 

Next we proceed to state and prove Theorem 4, which generalizes Theorem 3 to its stochastic variants, with a matching convergence rate to the unconstrained stochastic MOO algorithms and stochastic gradient descent. This allows us to apply the algorithm to large-scale machine learning problems, which we detail in Section 5. Its proof also extends that of Theorem 3. We ommit the similar derivations and only highlight the difference.

**Theorem 4** (Convergence of the single-loop stochastic FERERO algorithm).: _Suppose Assumptions 1, 2, 3, 4, 5 hold, and \(M_{g}=0\). Let \(\{\theta_{t}\},\{\lambda_{t}\}\) be the sequences produced by Algorithm 3 with \(A=I\) and \(\Omega_{\lambda_{f}}(\theta)=\Delta^{M}\) (c.f. Remark 4). With properly chosen step sizes \(\alpha_{t}=\alpha=\Theta(T^{-\frac{1}{2}})\), \(\gamma_{t}=\gamma=\Theta(T^{-\frac{1}{2}})\), it holds that_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\Big{[}\|\nabla F(\theta_{t})A_{ag}^{\top }\lambda_{t}\|^{2}+\|H(\theta_{t})\|^{2}\Big{]}=\mathcal{O}\Big{(}T^{-\frac{1} {2}}\Big{)}.\] (G.15)

Proof of Theorem 4.: Reuse the Lyapunov functions defined in (F.57). Let \(\lambda_{t}=[\lambda_{f,t};\lambda_{h,t}]\). The algorithm takes the update \(\theta_{t+1}=\theta_{t}+\alpha_{t}d_{t}\) with \(d_{t}=\nabla F_{\xi_{t,1}}(\theta_{t})A_{ag}^{\top}\lambda_{t}\). From Lemma 9, the function \(\lambda_{f}^{\top}AF(\theta)\) is \(\ell_{f,1}\|A^{\top}\|_{1}\)-smooth. Then following similar arguments from (F.18)-(F.20),choosing \(\frac{\alpha_{t}}{\gamma_{t}}=\frac{\alpha_{0}}{\gamma_{0}}=\frac{1}{c_{\gamma,\alpha}}\) for all \(t\in[T]\), and taking total expectation, we have the stochastic version of (F.58) below

\[\mathbb{E}[\mathbb{V}_{f,t+1}-\mathbb{V}_{f,t}+\mathbb{V}_{\lambda _{f},t+1}-\mathbb{V}_{\lambda_{f},t}]\] \[\leq \frac{\ell_{f,1}\|A^{\top}\|_{1}}{2}\alpha_{t}^{2}\mathbb{E}[\|d_ {t}\|^{2}]+\frac{1}{2}\alpha_{t}\gamma_{t}\mathbb{E}[\|\tilde{\nabla}_{\lambda _{f}}\varphi(\lambda_{t};\theta_{t})\|^{2}]-\alpha_{t}\mathbb{E}[\langle\lambda _{f,t},\nabla_{\lambda_{f}}\varphi(\lambda_{t};\theta_{t})\rangle].\] (G.16)

The stochastic version of (F.59) is

\[\mathbb{E}[\mathbb{V}_{h,0,t+1}-\mathbb{V}_{h,0,t}]\leq -\alpha_{t}c_{h}\mathbb{E}[\lambda_{h,t}^{\top}H(\theta_{t})]+ \frac{\ell_{f,1}}{2}\alpha_{t}^{2}\|B_{h}^{\top}\|_{1}c_{\lambda_{h}}\mathbb{E }[\|d_{t}\|^{2}]\] \[-\alpha_{t}\mathbb{E}[\langle\lambda_{h,t},\nabla_{\lambda_{h}} \varphi(\lambda_{t};\theta_{t})\rangle]-\gamma_{t}\mathbb{E}[\tilde{\nabla}_{ \lambda_{h}}\varphi(\lambda_{t};\theta_{t})^{\top}H(\theta_{t})].\] (G.17)

By Lemma 24, and that \(\nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_{t})=-\nabla H(\theta_{t})^{ \top}\nabla F(\theta_{t})A_{ag}^{\top}\lambda_{t}-c_{h}H(\theta_{t})\), the stochastic version of (F.63) is

\[\mathbb{E}[\mathbb{V}_{h,3,t+1}-\mathbb{V}_{h,3,t}]\leq\alpha_{t} \mathbb{E}[H(\theta_{t})^{\top}\nabla H(\theta_{t})^{\top}\nabla F_{\xi_{t,1}}( \theta_{t})A_{ag}^{\top}\lambda_{t}]+\frac{1}{4}\alpha_{t}^{2}\mathbb{E}[\ell_ {H^{2},1,t}\|d_{t}\|^{2}]\] \[\leq -\alpha_{t}c_{h}\mathbb{E}[\|H(\theta_{t})\|^{2}]-\alpha_{t} \mathbb{E}[H(\theta_{t})^{\top}\nabla_{\lambda_{h}}\varphi(\lambda_{t};\theta_ {t})]+\frac{1}{4}\alpha_{t}^{2}\mathbb{E}[\ell_{H^{2},1,t}\|d_{t}\|^{2}]\] (G.18)

where \(\ell_{H^{2},1,t}=\ell_{H^{2},1}=2M\|B_{h}\|^{2}\ell_{f}^{2}+2(c_{t}+\ell_{H}c_{ \alpha,h})\sqrt{M}\ell_{f,1}\|B_{h}\|\) by Assumption 5-3, and \(\ell_{H}=\|B_{h}\|\sqrt{M}\ell_{f}\). Let \(\ell_{FH,1}=\ell_{f,1}(\|A^{\top}\|_{1}+\|B_{h}^{\top}\|_{1}c_{\lambda_{h}})\). Adding up (G.16), (G.17), and (G.18) yields that

\[\mathbb{E}[\mathbb{V}_{f,t+1}-\mathbb{V}_{f,t}+\mathbb{V}_{\lambda _{f},t+1}-\mathbb{V}_{\lambda_{f},t}+\mathbb{V}_{h,t+1}-\mathbb{V}_{h,t}]\] \[\leq \frac{1}{4}\alpha_{t}^{2}(2\ell_{FH,1}+\ell_{H^{2},1})\mathbb{E}[ \|d_{t}\|^{2}]-\alpha_{t}\mathbb{E}[\langle\lambda_{t},\nabla_{\lambda} \varphi(\lambda_{t};\theta_{t})\rangle]-\alpha_{t}c_{h}\mathbb{E}[\lambda_{h,t} ^{\top}H(\theta_{t})]\] \[-\alpha_{t}c_{h}\mathbb{E}[\|H(\theta_{t})\|^{2}]-(\alpha_{t}+ \gamma_{t})\mathbb{E}[H(\theta_{t})^{\top}\nabla_{\lambda_{h}}\varphi(\lambda _{t};\theta_{t})]+\frac{1}{2}\alpha_{t}\gamma_{t}\mathbb{E}[\|\tilde{\nabla}_{ \lambda_{h}}\varphi(\lambda_{t};\theta_{t})\|^{2}].\] (G.19)

Further rearranging the above inequality, applying (F.65), invoking that \(\gamma_{t}=c_{\gamma,\alpha}\alpha_{t}\), and choosing \(\alpha_{t}\leq\min\left\{\frac{1}{2\ell_{FH,1}+\ell_{H^{2},1}},\frac{1}{4\|A_ {ag}\|^{2}M\ell_{c}^{2}\gamma_{\gamma,\alpha}},\frac{1}{2c_{hc_{\gamma,\alpha }}}\right\}\), we have

\[\mathbb{E}[\mathbb{V}_{f,t+1}-\mathbb{V}_{f,t}+\mathbb{V}_{\lambda _{f},t+1}-\mathbb{V}_{\lambda_{f},t}+\mathbb{V}_{h,t+1}-\mathbb{V}_{h,t}]\] \[\leq -\frac{1}{2}\alpha_{t}\mathbb{E}[\|\nabla F(\theta_{t})A_{ag}^{ \top}\lambda_{t}\|^{2}]-\frac{1}{4}\alpha_{t}c_{h}\mathbb{E}[\|H(\theta_{t}) \|^{2}]+\frac{(1+c_{\gamma,\alpha})^{2}}{c_{h}}\alpha_{t}\mathbb{E}[\|\nabla_{ \lambda_{h}}\varphi(\lambda_{t};\theta_{t})\|^{2}]\] \[+\alpha_{t}^{3}c_{\gamma,\alpha}^{2}\sigma^{2}+\frac{1}{4}(2\ell_{ FH,1}+\ell_{H^{2},1})\alpha_{t}^{2}\sigma^{2}.\] (G.20)

By applying Lemma 36, and choosing \(\alpha_{t}\leq\min\left\{\frac{1}{\ell_{\varphi,\lambda_{1}}c_{\gamma,\alpha}}, \frac{1}{(\ell_{\varphi_{0},1}+\ell_{\varphi^{*},1})c_{\varphi,d}},\frac{\mu_{ \varphi}}{c_{\varphi,d}c_{\varphi,d}^{2}\sigma_{\theta,1}}\right\}\) and \(c_{\gamma,\alpha}\geq\frac{2c_{\varphi,d}c_{\varphi_{0},1}^{2}\mu_{\varphi}^{ \prime}}{\mu_{\varphi}}\), the stochastic version of (F.69) is

\[\mathbb{E}[\mathbb{V}_{\varphi,t+1}-\mathbb{V}_{\varphi,t}]\leq -\frac{1}{2}\mu_{\varphi}\gamma_{t}\mathbb{E}\big{[}\varphi( \lambda_{t};\theta_{t})-\varphi(\lambda^{*}(\theta_{t});\theta_{t})\big{]}\] \[+\frac{\alpha_{t}}{c_{\varphi,d}}\mathbb{E}[\|\nabla F(\theta_{t}) A_{ag}^{\top}\lambda_{t}\|^{2}]+(1+2c_{\gamma,\alpha}^{2})\alpha_{t}^{2}\sigma^{2}.\] (G.21)

Adding up (G.20) and (G.21) with properly chosen hyperparameters \(c_{h}\geq(1+c_{\gamma,\alpha})^{2}\), \(c_{\gamma,\alpha}\geq\frac{4\ell_{\varphi,\lambda_{1}}}{\mu_{\varphi}}\), and \(c_{\varphi,d}=4\), we have

\[\mathbb{E}[\mathbb{V}_{t+1}-\mathbb{V}_{t}]\leq -\frac{1}{4}\alpha_{t}\mathbb{E}[\|\nabla F(\theta_{t})A_{ag}^{ \top}\lambda_{t}\|^{2}]-\frac{1}{4}\alpha_{t}c_{h}\mathbb{E}[\|H(\theta_{t})\|^{2}]\] \[+\Big{(}1+2c_{\gamma,\alpha}^{2}+\alpha_{t}c_{\gamma,\alpha}^{2}+ \frac{1}{4}(2\ell_{FH,1}+\ell_{H^{2},1})\Big{)}\alpha_{t}^{2}\sigma^{2}.\] (G.22)

With the same hyperparameters and step sizes summarized in (F.73), one can choose \(\alpha=\Theta(T^{-\frac{1}{2}})\), \(\gamma=\Theta(T^{-\frac{1}{2}})\) to obtain

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\Big{[}\|\nabla F(\theta_{t})A_{ag}^{\top} \lambda_{t}\|^{2}+\|H(\theta_{t})\|^{2}\Big{]}=\mathcal{O}\Big{(}T^{-\frac{1}{2}} \Big{)}.\] (G.23)

The proof is complete.

Implementation Details and Additional Experiment Results

In this section, we report the additional implementation details omitted from the main text in Appendix H.1 and the additional experimental results in Appendix H.2.

### Implementation details

Computation.All experiments were conducted on a server with an Intel i9-7920X CPU, two NVIDIA A5000 GPUs and two NVIDIA A4500 GPUs.

For all the experiments reported in the main text except for the multi-lingual speech recognition experiment, we exactly follow the settings from [41]. The implementations of the baselines including LS, PMTL, and EPO are from the official code of the EPO paper in [https://github.com/dbmptr/EPOSearch](https://github.com/dbmptr/EPOSearch) with their default hyperparameters. The results of XWC-MGDA are directly referenced from the paper due to lack of official implementation.

Synthetic data.For the results in both Figure 3 and Figure 4, the model parameter \(\theta\) has dimension \(q=20\), the number of objectives is \(M=2\). The angles between the preference vectors and the horizontal axis are generated between \([\frac{1}{20}\pi,\frac{9}{20}\pi]\) with equal angular distance. This experiment does not involve stochastic optimization. For our method, we solve the subprogram using PGD with a step size \(0.1\) up to an error of \(10^{-5}\) or with a maximum of \(250\) iterations. In the experiments, we set the parameter \(c_{h}=1\) for the subprogram if not otherwise specified.

In Figure 3, for all preferences and all methods, the initial model parameter \(\theta_{0}\) is randomly generated from a Gaussian distribution \(\mathcal{N}(0,1)\) for each dimension. In Table 6, we provide a summary of the hyperparameters for the baselines and our methods for the experiments in Figure 3.

In Figures 3(a)-3(c), the initial model parameters are randomly generated from a uniform distribution between \([-0.3,0.3]\) for each dimension. In Figures 3(d)-3(f), the initial model parameters are randomly generated from a uniform distribution between \([-0.5,-0.15]\) or \([0.15,0.5]\) for each dimension. Table 7 summarizes the hyperparameters for the experiments in Figure 4.

Multi-patch image classification.For a fair comparison, we follow the same data splitting and processing procedures as [41] using their official code. In each of the three datasets, there are 120k samples for training and 20k samples for testing. There are two tasks on each dataset: 1) classifying the top-left image, and 2) classifying the bottom-right image.

For all methods, we use the SGD optimizer with batch size 256. Note that, for our stochastic method, we use batch size 128 for each batch in the double sampling. Thus the total number of samples taken at each iteration is also 256. The hyperparameters are summarized in Table 8. The results of XWC-MGDA are directly referenced from the paper.

We use the Pymoo 0.6.1 library to compute the hypervolume. The Nadir points, i.e., the worst performance on single task baselines, used for the hypervolume computation are given in Table 9. For a fair comparison, the Nadir points we use are the same with [44] inferred from Figure 4 in the paper.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline Hyperparameters & LS & MGDA & PMTL & EPO & Ours Figure 3(e) & Ours Figure 3(f) \\ \hline step size \(\alpha_{t}\) & 0.1 & 0.2 & 0.2 & 0.1 & 0.05 & 0.05 \\ max iterations & 150 & 150 & 150 & 100 & 100 & 100 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Summary of hyper-parameters for the synthetic data experiments in Figure 3.

\begin{table}
\begin{tabular}{c|c c c|c c c} \hline \hline Hyperparameters & \multicolumn{3}{c|}{Figures 3(a)-3(c)} & \multicolumn{3}{c}{Figures 3(d)-3(f)} \\ \cline{2-7}  & PMTL & EPO & Ours & PMTL & EPO & Ours \\ \hline step size \(\alpha_{t}\) & 0.25 & 0.10 & 0.60 & 0.50 & 0.20 & 0.60 \\ max iterations & 100 & 60 & 10 & 200 & 120 & 200 \\ \(c_{h}\) & - & - & 1 & - & - & 0.01 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Summary of hyper-parameters for the synthetic data experiments in Figure 4.

Multi-lingual speech recognition.We use two datasets, Librispeech and AISHELL v1. Librispeech is an English speech dataset that consists of 960 hours of labeled audio data. For our experiments, we use the "train-clean-100" subset of the Librispeech dataset for supervised training, which contains 100 hours of clean training data. Additionally, we use the full 960 hours of data for self-supervised training. AISHELL v1 is a 178-hour Mandarin speech corpus designed for various speech and speaker processing tasks. We use the full AISHELL v1 dataset for both self-supervised and supervised training. We combine these two datasets for our multi-lingual speech recognition experiments.

We use the conformer [26] model with 8 conformer blocks as the encoder. Each block contains 512 hidden units and 8 attention heads. Each attention head has dimension 64. The convolutional kernel size is 31. Two classification heads are used. They contain two linear layers, one with 1000 output size for English, and another with 5000 output size for Chinese.

The loss functions we use include the Contrastive Predictive Coding (CPC) loss, and the Connectionist Temporal Classification (CTC) loss. The _CPC loss_[46] is a self-supervised loss to learn robust representations from unlabeled speech data. The CPC loss is designed to maximize the probability of a future sample given a contextual representation generated from the current speech sequence. The _CTC loss_ is defined as the negative log-likelihood of the model parameter given the input sequence and the label sequence.

For all methods including the baselines, we use the step sizes \(\alpha_{t,1}=5\times 10^{-4}\) for training backbone conformer parameters and \(\alpha_{t,2}=5\times 10^{-5}\) for training classification head parameters. The step size \(\gamma_{t}=0.1\) and the parameter \(c_{h}=0.5\).

### Additional experiment results

Synthetic data.We conduct several additional experiments on the synthetic objectives to further verify our theory. First, we conduct all the experiments on the synthetic objectives reported in the main text, using the single-loop approximate algorithm described in Algorithm 2. The results are plotted in Figure 8. The hyperparameters are the same unless otherwise specified.

From Figure 7(a), we can see that Algorithm 2 with a one-step approximate update of \(\lambda_{t}\) also leads to convergence and preference alignment. However, different from the results obtained by exactly solving for \(\lambda^{*}(\theta_{t})\) at each iteration, the models on the optimization trajectories do not align exactly with the preference. Similar observations can be found in Figure 7(b). In Figure 7(c), which is a difficult case due to the initialization, \(A=I_{M}\) does not work since it does not incorporate more general relative preference to allow controlled ascent update. This is addressed in Figure 7(d), where a general \(A\) (the same as in prior experiments) is used. Compared with exactly solving for \(\lambda^{*}(\theta_{t})\) at each

\begin{table}
\begin{tabular}{c|c c c c|c c c c|c c c c} \hline \hline Hyperparameters & \multicolumn{4}{c}{Multi-MNIST} & \multicolumn{4}{c}{Multi-Fashion+MNIST} \\ \cline{2-13}  & LS & PMTL & EPO & Ours & LS & PMTL & EPO & Ours & LS & PMTL & EPO & Ours \\ \hline step size \(\alpha_{t}\) & 1E-3 & 1E-3 & 1E-3 & 1E-3 & 1E-3 & 1E-3 & 1E-3 & 1E-3 & 1E-3 & 1E-3 & 1E-3 & 1E-3 \\ step size \(\gamma_{t}\) & - & - & - & 1E-4 & - & - & - & 1E-4 & - & - & - & - & 1E-4 \\ epochs & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 \\ \(c_{h}\) & - & - & - & 0.5 & - & - & - & 0.5 & - & - & - & 0.5 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Summary of hyper-parameter choices for multi-patch image classification experiments.

Figure 7: Scale invariance verification.

iteration, the approximate algorithm takes more iterations to converge, but has smaller per-iteration complexity, and smaller total time complexity.

We conduct another experiment to verify that the scale invariance can be preserved. We use the same objective as above, but scale the second one by \(2\). We use a fixed initialization \(\theta_{0}=0.3\cdot[\mathbf{1}_{q/2};-\mathbf{1}_{q/2}]\) for this experiment. The other hyperparameters are the same as the default. We use both \(F(\theta_{0})\) and \(F(0)\) as the reference points and choose \(B_{h}\) such that \(B_{h}(F(\theta_{0})-F(0))=0\). Results in Figure 7 show that for different scales, the trajectory and the converging solution are the same.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Hyperparameters & Figure 8a & Figure 8b & Figure 8c & Figure 8d \\ \hline step size \(\alpha_{t}\) & 0.10 & 0.06 & 0.15 & 0.15 \\ max iterations & 100 & 100 & 250 & 250 \\ \(c_{h}\) & 6 & 6 & 0.1 & 0.1 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Summary of hyper-parameters for the synthetic data experiments in Figure 8.

Figure 8: Synthetic experiment results with Algorithm 2.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline Datasets & Metrics & LS & PMTL & EPO & FERERO \\ \hline Synthetic, Figures 3(a-c) & Iterations & 100 & 100 & 60 & 10 \\  & Per-iteration run time & 3.50E-4s & 7.67E-4s & 4.93E-3s & 7.50E-4s \\  & Total run time & 0.035s & 0.0767s & 0.296s & 0.0075s \\ \hline Synthetic, Figures 3(d-f) & Iterations & 100 & 200 & 80 & 200 \\  & Per-iteration run time & 3.10E-4s & 7.65E-4s & 4.93E-3s & 7.30E-4s \\  & Total run time & 0.031s & 0.153s & 0.394s & 0.146s \\ \hline Multi-MNIST/Fashion/F+M & Epochs & 100 & 100 & 100 & 100 \\  & Per-epoch run time & 3.54s & 11.88s & 9.66s & 7.02s \\  & Total run time & 5.9m & 19.8m & 16.1m & 11.7m \\ \hline \hline \end{tabular}
\end{table}
Table 11: Summary of average run time in seconds (s) or minutes (m) and number of iterations or epochs of different methods on different datasets. We use Algorithm 1 for the synthetic experiments, and Algorithm 3 for the other two experiments.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See Section 1, introduction.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See the Broader impacts and limitations section.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: See Assumptions 1, 2, 3, 4 for the assumptions, and the Appendix D, and G for the proof.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Section 5 and Appendix H.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code is available at [https://github.com/lisha-chen/FERERO/](https://github.com/lisha-chen/FERERO/).
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 5 and Appendix H.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Section 5. We use the standard deviations as the error bars for all experiments except the speech recognition experiments since the speech recognition experiments take much longer time to run.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Section 5 and Appendix H.

9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We preserve anonymity.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See the end of the main paper in the Broader impacts and limitations section.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: the paper poses no such risks.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: See Section 5 and Appendix H.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: the paper does not release new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects