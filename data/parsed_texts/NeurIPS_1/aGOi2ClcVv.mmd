# Token Highlighter: Inspecting and Mitigating

Jailbreak Prompts for Large Language Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Large Language Models (LLMs) are increasingly being integrated into services such as ChatGPT to provide responses to user queries. To mitigate potential harm and prevent misuse, there have been concerted efforts to align the LLMs with human values and legal compliance by incorporating various techniques, such as Reinforcement Learning from Human Feedback (RLHF), into the training of the LLMs. However, recent research has exposed that even aligned LLMs are susceptible to adversarial manipulations known as Jailbreak Attacks. To address this challenge, this paper proposes a method called **Token Highlighter** to inspect and mitigate the potential jailbreak threats in the user query. Token Highlighter introduced a concept called AffirmationLoss to measure the LLM's willingness to answer the user query. It then uses the gradient of AffirmationLoss for each token in the user query to locate the jailbreak-critical tokens. Further, Token Highlighter exploits our proposed _Soft Removal_ technique to mitigate the jailbreak effects of critical tokens via shrinking their token embeddings. Experimental results on two aligned LLMs (LLaMA-2 and Vicuna-V1.5) demonstrate that the proposed method can effectively defend against a variety of Jailbreak Attacks while maintaining competent performance on benign questions of the AlpacaEval benchmark. In addition, **Token Highlighter** is a cost-effective and interpretable defense because it only needs to query the protected LLM once to compute the AffirmationLoss and can highlight the critical tokens upon refusal.

## 1 Introduction

Large Language Models (LLMs) like GPT-4 [15], LLaMA-2 [19], and Vicuna [27] have demonstrated impressive capabilities in achieving state-of-the-art results in a wide range of natural language processing and generation tasks. With the surging interest and integration into services such as ChatGPT, ensuring the safety and trustworthiness of their output becomes crucial. Techniques such as Reinforcement Learning from Human Feedback (RLHF) have been proven to be effective in aligning LLMs with human values [3, 4, 10, 16].

Despite advancements in alignment techniques, aligned LLMs have been found to be susceptible to jailbreak attacks, which involve rewriting the malicious query at token-level or prompt-level to bypass and circumvent the safety guardrails of aligned LLMs. A notable example is that a jailbroken LLM would be tricked into giving tutorials on how to cause harm to others, as demonstrated in Figure 1. Different jailbreak attack algorithms [28, 13, 5, 14] have been proposed recently to automatically construct the jailbreak attacks. Take GCG [28] as an example, GCG can successfully trick several LLMs to output objectionable responses by simply inserting a universal adversarial suffix.

Since the exposure of jailbreak risks for LLMs, various methods of defending against jailbreak attacks have been explored [8, 17, 24, 11, 9, 7] and are indeed empirically successful in defendingagainst certain types of jailbreak attacks. However, existing defenses are challenged by three main considerations: **(1)** Some defenses like perplexity filtering (PPL [8]) showed little effect on interpretable and fluent jailbreak prompts [13]. **(2)** Some detector-based defenses have a high False Positive Rate [11] and thus would significantly compromise the LLM's performance on benign user queries. **(3)** Some defenses that rely on querying an LLM multiple times [17, 11, 9, 7], may incur unacceptable inference costs.

Recent works [28, 22, 26] exposed an observation that successful jailbreaks often succeed in tricking the LLMs to first generate an affirmative response like "Sure, here's...". This motivates us to find the tokens in the jailbreak prompt that are most critical to generating these affirmations, and then mitigate the potential jailbreak threat by reducing the influence of those tokens in the response generation process. Motivated by this thought, we propose **Token Highlighter** to alleviate the threats of jailbreak attacks and avoid the aforementioned limitations of existing defenses. An overview of how Token Highlighter works can be found on the bottom left of Figure 1. Firstly, we define the Affirmation Loss using the loss function of the LLM generating a pre-defined affirmation (we use "Sure, I'd like to help you with this." throughout this paper) to measure the LLM's willingness to respond to the user query. Next, we use the gradient of Affirmation Loss to locate the jailbreak-critical tokens in the user query. Finally, we diminish the influence of these tokens in the response generation process by multiplying the original embeddings of these tokens by a value \(\beta\) between \(0\) and \(1\). We call the operation of multiplying a small value _Soft Removal_, as opposed to directly removing these tokens from the user query, which can be understood as _Hard Removal_ (equivalently, setting \(\beta=0\)). We use _Highlight_ to vividly describe the process of identifying an influential token and then shrinking its embedding. The bottom right of Figure 1 shows that the LLM equipped with **Token Highlighter** can correctly reject the malicious user query owing to soft removals on self-discovered jailbreak-critical prompts.

Empirical results show that **Token Highlighter** can significantly mitigate jailbreak attacks while maintaining the performance of LLMs on benign user queries (see Figure 2). Our comprehensive analysis in Section 4 also underscores Token Highlighter's running efficiency and robustness against adaptive attacks.

Figure 1: Overview of **Token Highlighter**. (a) The top panel illustrates the concept of LLM jailbreaks by presenting examples of two types of jailbreak prompts (token-level jailbreak by GCG [28] and sentence-level jailbreak by TAP [14]. (b) The bottom left panel explains how Token Highlighter finds the jailbreak-critical tokens and mitigates the potential jailbreak effects. We define a loss function called Affirmation Loss to measure the modelâ€™s willingness to generate affirmative responses to the user query. In step 1, our method selects a set of tokens in the user query that have a large influence on generating the affirmation. In step 2, our method applies _Soft Removal_ on these tokens by shrinking the embeddings of these tokens. We call the user query modified by _Soft Removal_ the _Highlighted User Query_. The bottom right panel demonstrates that Token Highlighter can inspect suspicious tokens and help the LLM to correctly refuse malicious user queries.

We summarize our **main contributions** as follows:

* We propose a jailbreak defense method called Token Highlighter, which uses our proposed AffirmationLoss and _Soft removal_ techniques to reduce potential jailbreak risks by finding and mitigating jailbreak-critical tokens in the user query when generating responses.
* Experiments on 2 aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5), 6 jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Mansyhot, and AIM) [28; 13; 5; 14; 2; 1] and a common LLM performance evaluation benchmark (AlpacaEval [12] ) demonstrate that Token Highlighter can achieve outstanding performance in defending against various jailbreak prompts while maintaining good utility on benign user queries.
* Token Highlighter is a cost-efficient and interpretable defense. Compared to standard LLM inference, Token Highligter only needs one extra query for the computation of the AffirmationLoss. The highlighted tokens can be used to provide explanations of refusal responses.

## 2 Related Work

**Jailbreak Attacks.** Jailbreak attack methods can be divided into token-level jailbreaks and prompt-level jailbreaks. The seminal work in token-level jailbreaks is GCG [28], which computes the target LLM's generative loss for an affirmation and then uses the loss's gradients with respect to the one-hot token indicators to find better token choices at each position. Prompt-level jailbreaks try to find a prompt to lure the LLM to respond to the malicious instruction. The prompt can be manually designed or automatically generated. Manually designed prompts, like AIM [1] and Mansyhot [2], often involve encapsulating the malicious user instruction into a pre-defined template with a placeholder. Automated prompt-level jailbreak methods often utilize the LLM's feedback to iteratively refine the prompt until the target LLM is successfully jailbroken. AutoDAN [13] employs the target LLM's generative loss of the target response to design the fitness score of the candidate jailbreak prompt to guide further optimization. PAIR [5] and TAP [14] use another two LLMs as the attacker and evaluator respectively. At each iteration, the attacker-generated jailbreak prompt would be rated and commented on by the evaluator model according to the target LLM's response to the attack. Next, the attacker would generate new jailbreak prompts based on the evaluator's comments and ratings, and repeat the above cycle until the jailbreak prompt can get full marks from the evaluator.

**Jailbreak Defenses.** Existing jailbreak defense methods can be divided into detector-based defense, smoothing-based defense, and prompt-engineering-based defense. Detector-based Defense [8; 7] utilizes a detector to distinguish whether the user query is malicious and only the query that could pass the checking of the detector would be sent to query the target LLM. Typical ones of this type of method is PPL [8], which uses an LLM to compute the perplexity of the input query and rejects those with high perplexity. Smoothing-based Defense, which is motivated by randomized smoothing [6], transforms the original input query to obtain multiple copies and then aggregates the corresponding responses of the target LLM to give the final response to the original query. The earliest one of this line of work is SmoothLLM [17], which uses character-based perturbation. Semantic Smoothing [9] tries to preserve the semantic information when perturbing the user query by using semantic transformations such as summarize, paraphrase, and spell-check. Prompt-engineering-based methods are different from these. In these works [24; 25; 23; 21], prompt engineering techniques are used to defend against jailbreak attacks by either altering the system prompt or embedding the user input into a pre-defined template. Self Reminder [24] is a representative of this line of work, which alters the system prompt of the LLM to instruct the model to remind itself to engage and reply to the user while maintaining the perspective of being an aligned LLM.

## 3 Methodology and Algorithms

Following the overview in Figure 1, in this section we will introduce how **Token Highlighter** works to inspect and mitigate jailbreak prompts for LLMs. Especially, in Section 3.1, we will introduce the concept of the AffirmationLoss and explain how to utilize this loss to locate the tokens with a high influence on tricking the LLM into the affirmative mode. In Section 3.2, we will introduce what **Token Highlighter** does with _Soft Removal_ to mitigate the potential jailbreak risks in user queries.

### Affirmation Loss Function and Critical Token Set Construction

Recent research [22; 26] found that many successful jailbreak attempts share a common property that they all trick the LLM into generating affirmations like starting with "Sure, here is" at the beginning of their responses. Drawing upon this inspiration, our proposed defense aims to find the tokens that are most critical in forcing the LLM to generate such affirmative responses, decrease their importance in the generation, and thereby resolve the potential jailbreak risks brought by these tokens. To identify these tokens, we propose a new concept called the \(\mathtt{AffirmationLoss}\). Given the target LLM \(T_{\theta}\) parameterized with \(\theta\) and a user query \(q_{1:n}\) (where \(n\) is the number of tokens in this query), we define \(x_{1:n}\) as the embedding matrix of \(q_{1:n}\):

\[x_{1:n}=\mathtt{embed}_{\theta}(q_{1:n}) \tag{1}\]

where \(\mathtt{embed}_{\theta}(\cdot)\) indicates the embedding layer in \(T_{\theta}\), and \(x_{i}=\mathtt{embed}_{\theta}(q_{1:n})_{i}=\mathtt{embed}_{\theta}(q_{i})\) is the embedding of the \(i^{th}\) token \(q_{i}\) in \(q_{1:n}\).

The \(T_{\theta}\)'s \(\mathtt{AffirmationLoss}(x_{1:n},\theta)\) with respect to \(x_{1:n}\) is defined as:

\[\mathtt{AffirmationLoss}(x_{1:n},\theta)=-\log P_{\theta}(y|x_{1:n}), \tag{2}\]

where \(y=\)"Sure, I'd like to help you with this.", which is our default sentence to represent the \(T_{\theta}\)'s affirmation to answer the question. We then further define the \(\mathtt{influence}\) of each token embedding \(x_{i}\) in \(x_{1:n}\) when generating \(y\) as follows:

\[\mathtt{Influence}(x_{i})=\|\nabla_{x_{i}}\log P_{\theta}(y|x_{1:n})\|_{2}, \tag{3}\]

where \(\nabla_{x_{i}}\) denotes the gradient operation with respect to \(x_{i}\). Finally, we sort the \(\mathtt{influence}\) metric and select the top-\(n\alpha\) tokens to construct the \(\mathbf{CriticalSet}\)\(\mathcal{Q}\) of tokens:

\[\mathcal{X}=\mathtt{argtop\!-\!}n\alpha(\{\mathtt{Influence}(x_{i}),\forall x_{ i}\in x_{1:n}\})\text{ and }\mathcal{Q}=\{q_{i},\forall x_{i}\in\mathcal{X}\}. \tag{4}\]

, where \(\alpha\in[0,1]\) is the highlight percentage and \(n\alpha\) means the total number of the tokens we selected.

### Mitigating Jailbreak Effect by _Soft Removal_

With the identified top-influence tokens, one naive idea to mitigate the jailbreak threats brought by the tokens \(\{q_{i}\}\) in \(\mathcal{Q}\) is to directly erase some of them from \(q_{1:n}\), which shares a similar idea with Erase Check [11]. However, prior works [9; 7] found that although directly removing them can effectively reduce the attack success rate of jailbreak prompts, this "hard removal" leads to a considerable drop in the model's performance on processing with benign user queries. To better trade-off the model's performance on benign user queries and the defense effectiveness against jailbreak attacks, we propose _Soft Removal_, which shrinks the embeddings of the candidate tokens in \(\mathcal{Q}\) to decrease \(q_{1:n}\)'s influence on manipulating \(T_{\theta}\) to generate affirmation responses. We call the query processed by _Soft Removal a highlighted user query_. Given a user query \(q_{1:n}\) and its corresponding _highlighted user query_\(q^{\prime}_{1:n}\), we denote the embedding matrix for \(q^{\prime}_{1:n}\) as \(x^{\prime}_{1:n}\). Mathematically, \(x^{\prime}_{1:n}\) is computed as:

\[x^{\prime}_{i}=\begin{cases}\beta\times\mathtt{embed}(q_{i}),\text{ if }q_{i} \text{ in }\mathcal{Q}\\ \mathtt{embed}(q_{i}),\text{ otherwise}\end{cases} \tag{5}\]

with \(\beta\in[0,1]\) acting as the soft removal level. For a given input user query \(q_{1:n}\), we define the LLM \(T_{\theta}\)'s native response to it (i.e., when there is no defense) as \(r_{\theta}(q_{1:n})\sim P_{\theta}(\cdot|x_{1:n})\). After deploying our Token Highlighter for \(T_{\theta}\), the response to \(q_{1:n}\) would be replaced as \(r_{\theta}(q_{1:n})\sim P_{\theta}(\cdot|x^{\prime}_{1:n})\).

### Token Highlighter: Inspect and Mitigate Jailbreak Prompts

Based on the technical details of \(\mathtt{AffirmationLoss}\) and _Soft Removal_ in Section 3.1 and Section 3.2, we now formally introduce the **Token Highlighter** framework. At a high level, the proposed method aims to locate the parts of the user query that show signs of jailbreaking, and then mitigate the possible jailbreak threats by suppressing the influence of these suspicious tokens before generating the response. Token Highlighter can be summarized in two steps:

* **Step #1: Critical Token Set Construction.** In this step, we compute the \(\mathtt{Influence}\) metric defined by Equation 2 and Equation 3 for each token \(q_{i}\) in the user query \(q_{1:n}\) and construct the Critical Set \(\mathcal{Q}\) using the tokens with the top-\(n\alpha\)\(\mathtt{influence}\).

* **Step #2: Token Soft Removal.** In this step, we multiply a value \(\beta\in[0,1]\) to the token embedding of each token in the Critical Set \(\mathcal{Q}\), get the embeddings of the highlighted user query \(q^{\prime}_{1:n}\) following Equation 5, and use the \(T_{\theta}\)'s response to \(x^{\prime}_{1:n}\) as the final response to \(q_{1:n}\).

The algorithmic description for our method can be found in Algorithm 1. It can be clearly seen that our defense is quite cost-efficient, as there is only one forward and backward pass of the LLM in Step #1.

```
1:Input: User input query \(q_{1:n}\), Target LLM \(T_{\theta}\) and its token embedding layer embed\({}_{\theta}(\cdot)\), Highlight Percentage \(\alpha\in[0,1]\), and the Soft Removal Level \(\beta\ \in[0,1]\)
2:
3:Step #1: Critical Token Set Construction.
4:Compute the embedding matrix \(x_{1:n}\) for \(q_{1:n}\) based on Equation 1.
5:Compute the Affirmation Loss\((x_{1:n},\theta)\) for \(x_{1:n}\) based on Equation 2.
6:Compute the Influence\((x_{i})\) for all the \(x_{i}\) in \(x_{1:n}\) based on Equation 3
7:Construct \(\mathcal{Q}\) based on Equation 4
8:
9:Step #2: Token Soft Removal.
10:Get initial embedding for the highlighted user query \(q^{\prime}_{1:n}:x^{\prime}_{1:n}=\texttt{embed}_{\theta}(q_{1:n})\)
11:for\(q_{i}\in\mathcal{Q}\); do
12:\(x^{\prime}_{i}=\beta\times x^{\prime}_{i}\)
13:endfor
14:
15:Output: The LLM's response to \(q_{1:n}\): \(r(q_{1:n})\sim P_{\theta}(\cdot|x^{\prime}_{1:n})\)
```

**Algorithm 1** Token Highlighter

## 4 Performance Evaluation

### Experiment Setup

**Malicious User Queries**. We sampled 100 harmful behavior instructions from AdvBench1 in [28] as jailbreak prototypes, each of which elicits the target LLM to generate answer for a specified question with harmful contents. We then use various existing jailbreak attack methods to generate jailbreak prompts for them. Specifically, for each harmful behavior instruction, we use GCG [28] to generate a universal adversarial suffix, use AutoDAN [13], PAIR [5], and TAP [14] to automatically generate a new semantic-preserving instruction, use AIM [1] to encapsulate it to a manually designed template, and use Mansyshot [2] to insert multiple faux dialogues between a human user and an AI assistant as the prefix of the original user query, where the user asks malicious queries and the AI assistant responds with affirmations. See Appendix A.3 for more details on generating these jailbreak prompts.

Footnote 1: GCG Github Repository[https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv](https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv)

**Utility Evaluation Benchmark**. We tested our method as well as all the defense baselines on AlpacaEval2 to evaluate how these defense methods would affect the target LLM's utility (performance on benign user queries). AlpacaEval is a benchmark to measure how well the responses of a given LLM align with human preferences. In this paper, we select the text-davinci-003's responses to the AlpacaEval questions as a reference and use GPT-4 as a judge to compare the outputs of the target LLM with the reference.

Footnote 2: AlpacaEval Github Repository[https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)

**Aligned LLMs.** We conduct the jailbreak experiments on 2 aligned LLMs: LLaMA-2-7B-Chat [19] and Vicuna-7B-V1.5 [27]. LLaMA-2-7B-Chat is the aligned version of LLAMA-2-7B. Vicuna-7B-V1.5 is also based on LLAMA2-7B and has been further supervised fine-tuned on 70k user-assistant conversations collected from ShareGPT 3. We use **protected LLM** to represent these two models in the experiments.

Footnote 3: [https://sharegpt.com](https://sharegpt.com)

**Defense Baselines.** We compare our method with three types of jailbreak defense methods, including (I) detector-based methods: PPL [8], Erase Check [11], and Gradient Cuff [7]; (II) smoothing based methods: SmoothLLM [17] and Semantic Smoothing [9]; and (III) prompt-engineering-based methods: Self Reminder [24]. To implement PPL, we use the protected LLM itself to compute the perplexity for the input user query and directly reject the one with a perplexity higher than a threshold in our experiment. For Erase Check, we employ the LLM itself to serve as a safety checker to check whether the input query or any of its erased sub-sentences is harmful. Gradient Cuff, which is a two-stage detection framework, proposed a loss function called Refusal Loss. Gradient Cuff detects jailbreaks by checking the value and gradient norm of Refusal Loss. SmoothLLM and Semantic Smoothing perturb the original input query to obtain multiple copies and then aggregate the protected LLM's responses to generate the final response. Self Reminder converts the protected LLM into a self-remind mode by modifying the system prompt. For Token Highlighter, to demonstrate the effectiveness of the construction of the Critical Set, we also include a new baseline called Random Soft Removal, which does soft removal on randomly selected tokens. For more details on the implementation of these baselines, please refer to Appendix A.5.

**Metrics.** We report the Attack Success Rate (**ASR**) measured by LLaMA-Guard-2 [18] to evaluate each defense against various jailbreak attacks. We also report the **Win Rate** measured on Alpaca Eval to show how the protected LLM's utility is affected. In general, a higher Win Rate and lower ASR indicate a better defense. Details about computing the metrics are given in Appendix A.4.

**Implementation of Token Highlighter.** We use \(\alpha=0.25\) in all our experiments for both the two protected LLMs. In terms of \(\beta\), we use \(0.3\) for Vicuna-7B-V1.5 and \(0.5\) for LLaMA2-7B-Chat to keep a balanced trade-off between the Win Rate and the ASR. For the text generation setting, we use temperature \(=0.6\) and top-p parameter \(=0.9\) for both LLaMA2-7B-Chat and Vicuna-7B-V1.5, and adopt Nucleus Sampling. As for the system prompt, we use the default setting provided in the fastchat repository [27]. All our experiments are run on a single NVIDIA A800 GPU with 80G of memory. We run all the experiments with the random seed set to \(100\) to ensure reproducibility.

### Comparison with Existing Methods

We begin by comparing our methods and all the defense baselines, jointly considering the AlpacaEval Win Rate and the Average ASR which is averaged across all six jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Mansshot, and AIM). From Figure 2, we can conclude that our method outperforms all other baselines by showing strong defense against jailbreak attacks and good utility on benign user queries. Though smoothing-based methods like Semantic Smoothing can also achieve comparable or even lower ASR than Token Highlighter, these methods would cause a large drop in the utility of the protected LLM, due to the fact that the perturbations they applied to the original user query may deteriorate the semantic information. For example, SmoothLLM uses meaningless characters to replace some words in the original query. Though Semantic Smoothing tries to preserve the semantic information by using summarization to transform the query, the summarization technique would also affect the semantics of the original query. Detector-based methods like Gradient Cuff and PPL can attain good utility because these methods can limit the False Positive Rate (FPR) to a small value (e.g., 5%) by adjusting the threshold. Erase Check, another detector-based method in which there is no

Figure 2: Performance evaluation on Vicuna-7B-V1.5 (a) and LLaMA2-7B-Chat (b). The horizon axis represents the Attack Success Rate (ASR) averaged over 6 jailbreak attacks, and the vertical axis shows the Win Rate on Alpaca Eval of the protected LLM when the corresponding defense is deployed. Complete results can be found in Appendix A.6.

threshold to be adjusted, cannot attain good utility as it has a large and uncontrollable FPR, as also mentioned in prior works [7, 9]. Self Reminder can maintain a high Win Rate on Vicuna-7B-V1.5 but also show utility degradation on LLaMA-2-7B-Chat.

Our method stands out by having the lowest ASR among all the methods that can keep a high Win Rate. In particular, Token Highlighter decreases the ASR from 0.730 to 0.142 on Vicuna-7B-V1.5 while the best baseline Gradient Cuff can only decrease the ASR to 0.243. Token Highlighter outperforms Gradient Cuff by 20.7% (0.588 vs 0.487) in terms of the ASR reduction. On LLaMA-2-7B-chat, all baselines can make the ASR close to zero, because LLaMA-2 is more difficult to jailbreak. The comparison between Token Highlighter and Random Soft Removal reveals the effectiveness of the construction of the Critical Set using the gradient of the Affirmation Loss. Another notable fact is that Random Soft Removal can also keep the utility almost unchanged compared with when there is no defense. This finding suggests that in terms of maintaining utility, exploring the effect on the values of \(\beta\) and \(\alpha\) in soft removal may be more crucial than which tokens are softly removed. More studies on the trade-off between ASR and Win Rate by adjusting \(\alpha\) and \(\beta\) are presented in Section 4.3.

The results in Figure 1(a) show that Self Reminder is not effective on Vicuna-7B-V1.5. Since prompt-engineering-based methods can be easily combined with Token Highlighter, we choose to combine our method with Self Reminder by simply replacing the system prompt used in our method with that used in Self Reminder. We call the combined version Self Reminder (TH) and run experiments under varying values of \(\beta\) to see whether Token Highlighter can improve Self Reminder. The results in Table 1 show that Self Reminder (TH) can have a much better performance than the plain Self Reminder in terms of the trade-off between ASR and Win Rate. Specifically, Token Highlighter further decreases the ASR of Self Reminder by \(15.2\%\) (0.362 vs 0.427) while maintaining the \(95.5\%\) win rate of the vanilla Self Reminder (0.653 vs 0.684). Reducing the \(\beta\) from \(0.5\) to a smaller number like \(0.3\) can continually reduce the ASR at the cost of decreased win rate. When \(\beta\) is set to 0.3, the ASR is nearly zero while the win rate can still maintain almost 80% of the vanilla Self Reminder.

### Trade-off Analysis between ASR and Win Rate

Recall that we have two parameters for the Token Highlighter algorithm: the highlight percentage \(\alpha\) and the soft removal level \(\beta\). In Figure 3, we report the average **ASR** and the **Win Rate** for various \(\alpha\) and \(\beta\). From Figure 3, we can find that the ASR has the same trend as the Win Rate with the changing of \(\alpha\) and \(\beta\). Specifically, when \(\alpha\) is fixed, a larger value of \(\beta\) would make both the Win Rate and the ASR increase. When \(\beta\) is fixed, larger \(\alpha\) would both reduce the ASR and the Win Rate.

This phenomenon can be interpreted as follows. Taking a larger \(\alpha\), Token Highlighter would highlight more tokens in the jailbreak prompt, thus improving the chance to mitigate the jailbreak effects. However, in another prospective, highlighting more tokens would decrease the model's utility because more tokens in benign queries would also be highlighted. Taking a smaller \(\beta\) would further suppress the importance of the highlighter tokens in generating responses, thus better at mitigating the jailbreak effects. However, heavier soft removals are more likely to destroy the semantic context of the token embeddings. An extreme case is that the soft removal becomes "hard removal" when \(\beta\) is set to zero.

### Interpretability Analysis of Highlighted Queries

We show Token Highlighter can be used as a post-hoc explanation tool upon LLM refusal, by identifying jailbreak-critical tokens in 3 separate examples from GCG and TAP. Table A1 shows that our Token Highlighter indeed underscores some jailbreak-critical components of the prompt for interpreting jailbreak attempts. For GCG, which is a token-level jailbreak attack that appends an optimized adversarial suffix to the original malicious query, the highlighted tokens are all within the

\begin{table}
\begin{tabular}{l|c|c|c} \hline
**Defense Method** & \(\beta\) & **ASR \(\downarrow\)** & **Win Rate \(\uparrow\)** \\ \hline Self Reminder & NA & 0.427 & 0.684 \\ \hline \multirow{3}{*}{Self Reminder (TH)} & 0.5 & **0.362** & **0.653** \\  & 0.4 & 0.248 & 0.599 \\ \cline{1-1}  & 0.3 & 0.023 & 0.536 \\ \cline{1-1}  & 0.2 & 0.015 & 0.328 \\ \hline \end{tabular}
\end{table}
Table 1: Performance evaluation of combining Self Reminder and Token Highlighter. \(\uparrow\) means that larger value is better while \(\downarrow\) means the opposite.

inserted suffix parts. For TAP, which is a prompt-level jailbreak method that uses an attacker LLM to refine the malicious instruction, Token Highlighter can facilitate the explanation on the strategies the attacker used to rewrite the prompt. For instance, raising the LLM's mismatched generalization by role-playing (e.g., "You are a private investigator..."), story-telling (e.g., content dilution), introducing illusive objectives (e.g., write dialogue, report, etc), and instructing the LLM to begin the response with an affirmation (e.g., "begin with 'Sure, here is guidance on...").

In addition, we also explored some highlighted cases for benign user queries and vanilla malicious queries (w/o jailbreak prompts). We found that the highlighted tokens in these cases are just some words or simply some punctuation marks to represent the interrogative/imperative moods (e.g., "How", "What", "Please", "?" and "."). In summary, for interpretability analysis, we advocate using Token Highlighter to inspect which tokens are more crucial to cause refusal responses by the protected LLM (e.g., "I am sorry, but I cannot..." as shown in Figure 1, bottom right panel), to facilitate the explanation to end users and model developers.

## 5 Conclusion

This paper presents a novel jailbreak defense method called **Token Highlighter**. Token Highlighter can effectively capture the jailbreak-critical components designed by the attacker in the malicious user query and then mitigate their jailbreak effects by applying _Soft Removal_ on these critical tokens. Our extensive experiments on 2 aligned LLMs (LLaMA-2-7b-Chat and Vicuna-7B-V1.5) and 6 jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Mansyhot, and AIM) validate the effectiveness of Token Highlighter over existing defenses by achieving state-of-the-art performance in alleviating jailbreak attacks while maintaining good utility on benign user prompts and low running time cost.

## References

* [1] Alex Albert. Jailbreak chat, 2023. [https://www.jailbreakhat.com](https://www.jailbreakhat.com).
* [2] Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Nina Rimsky, Meg Tong, Jesse Mu, Daniel Ford, et al. Many-shot jailbreaking. 2024.
* [3] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment. _CoRR_, abs/2112.00861, 2021.
* [4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine

Figure 3: Trade-off between Win Rate and Attack Success Rate by adjusting the values of \(\alpha\) and \(\beta\).

Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. _CoRR_, abs/2204.05862, 2022.
* Chao et al. [2023] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. _CoRR_, abs/2310.08419, 2023.
* Cohen et al. [2019] Jeremy Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified adversarial robustness via randomized smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 1310-1320. PMLR, 2019.
* Hu et al. [2024] Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes. _CoRR_, abs/2403.00867, 2024.
* Jain et al. [2023] Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Sompaelli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. Baseline defenses for adversarial attacks against aligned language models. _CoRR_, abs/2309.00614, 2023.
* Ji et al. [2024] Jiabao Ji, Bairu Hou, Alexander Robey andF George J. Pappas, Hamed Hassani, Yang Zhang, Eric Wong, and Shiyu Chang. Defending large language models against jailbreak attacks via semantic smoothing. _CoRR_, abs/2402.16192, 2024.
* Kasirzadeh and Gabriel [2022] Atoosa Kasirzadeh and Iason Gabriel. In conversation with artificial intelligence: aligning language models with human values. _CoRR_, abs/2209.00731, 2022.
* Kumar et al. [2023] Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, and Hima Lakkaraju. Certifying LLM safety against adversarial prompting. _CoRR_, abs/2309.02705, 2023.
* Li et al. [2023] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval), 2023.
* Liu et al. [2023] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. _CoRR_, abs/2310.04451, 2023.
* Mehrotra et al. [2023] Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi. Tree of attacks: jailbreaking black-box lms automatically. _CoRR_, abs/2312.02119, 2023.
* OpenAI [2023] OpenAI. GPT-4 technical report. _CoRR_, abs/2303.08774, 2023.
* December 9, 2022_, 2022.
* Robey et al. [2023] Alexander Robey, Eric Wong, Hamed Hassani, and George J. Pappas. Smoothllm: Defending large language models against jailbreaking attacks. _CoRR_, abs/2310.03684, 2023.
* Team [2024] Llama Team. Meta llama guard 2. [https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md](https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md), 2024.
* Touvron et al. [2021] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashkykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, ZhengYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. _CoRR_, abs/2307.09288, 2023.
* Tramer et al. [2020] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial example defenses. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Varshney et al. [2024] Neeraj Varshney, Pavel Dolin, Agastya Seth, and Chitta Baral. The art of defending: A systematic evaluation and analysis of LLM defense strategies on safety and over-defensiveness. _CoRR_, abs/2401.00287, 2024.
* Wei et al. [2023] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does LLM safety training fail? _CoRR_, abs/2307.02483, 2023.
* Wei et al. [2023] Zeming Wei, Yifei Wang, and Yisen Wang. Jailbreak and guard aligned language models with only few in-context demonstrations. _CoRR_, abs/2310.06387, 2023.
* Xie et al. [2023] Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, and Fangzhao Wu. Defending chatgpt against jailbreak attack via self-reminders. _Nat. Mac. Intell._, 5(12):1486-1496, 2023.
* Zhang et al. [2024] Yuqi Zhang, Liang Ding, Lefei Zhang, and Dacheng Tao. Intention analysis prompting makes large language models A good jailbreak defender. _CoRR_, abs/2401.06561, 2024.
* Zhao et al. [2024] Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, and William Yang Wang. Weak-to-strong jailbreaking on large language models. _CoRR_, abs/2401.17256, 2024.
* Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. _CoRR_, abs/2306.05685, 2023.
* Zou et al. [2023] Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. _CoRR_, abs/2307.15043, 2023.

Appendix

### Interpretability Analysis of Highlighted Queries

We show Token Highlighter can be used as a post-hoc explanation tool upon LLM refusal, by identifying jailbreak-critical tokens in 3 separate examples from GCG and TAP. Table A1 shows that our Token Highlighter indeed underscores some jailbreak-critical components of the prompt for interpreting jailbreak attempts. For GCG, which is a token-level jailbreak attack that appends an optimized adversarial suffix to the original malicious query, the highlighted tokens are all within the inserted suffix parts. For TAP, which is a prompt-level jailbreak method that uses an attacker LLM to refine the malicious instruction, Token Highlighter can facilitate the explanation on the strategies the attacker used to rewrite the prompt. For instance, raising the LLM's mismatched generalization by role-playing (e.g., "You are a private investigator..."), story-telling (e.g., content dilution), introducing illusive objectives (e.g., write dialogue, report, etc), and instructing the LLM to begin the response with an affirmation (e.g., "begin with 'Sure, here is guidance on...").

In addition, we also explored some highlighted cases for benign user queries and vanilla malicious queries (w/o jailbreak prompts). We found that the highlighted tokens in these cases are just some words or simply some punctuation marks to represent the interrogative/imperative moods (e.g., "How", "What", "Please", "?" and "."). In summary, for interpretability analysis, we advocate using Token Highlighter to inspect which tokens are more crucial to cause refusal responses by the protected LLM (e.g., "I am sorry, but I cannot..." as shown in Figure 1, bottom right panel), to facilitate the explanation to end users and model developers.

### Effectiveness towards Different Affirmations

We further test Token Highlighter's effectiveness by measuring its performance using different affirmations. The total 3 affirmations we compared are listed and compared in Table A2. We designed these 3 affirmations by observing some real jailbreak cases. We use these affirmations to compute the Affirmation Loss and implement Token Highlighter. From Table A2, we can observe that the Token Highlighter's performance is quite stable for different affirmations.

### Jailbreak Generation

In this section, we introduce how we generate jailbreak prompts used in the main paper.

For GCG, AutoDAN, PAIR, and TAP, we use their open-sourced codes and leave most of the default settings unchanged. We summarize the code repository address and the most critical parameters:

* Repository Address: [https://github.com/llm-attacks/llm-attacks](https://github.com/llm-attacks/llm-attacks)
* Notes: n-steps=1000, test-steps=50, batch-size=512

* Repository Address: [https://github.com/SheltonLiu-N/AutoDAN](https://github.com/SheltonLiu-N/AutoDAN)
* Notes: num-steps=100, batch-size=64

* Repository Address: [https://github.com/patrickrchao/JailbreakingLLMs](https://github.com/patrickrchao/JailbreakingLLMs)
* Notes: n-stream=5, n-iterations=12

* Repository Address: [https://github.com/RICmmmunity/TAP](https://github.com/RICmmmunity/TAP)
* Notes: n-stream=1, branching-factor=4, width=10, depth=10

For the AIM jailbreak attacks, we provide the template in Table A3:

For the Manyshot jailbreak attacks, we use 32 faux dialogues between a human and an AI assistant as the prefix for the user query. We combine Manyshot jailbreak attacks with GCG to improve its ASR. In each dialogue, the human's input would be a GCG query and the AI's response is an affirmation. We provide one example in Table A4:

### Attack Success Rate & Win Rate

**Attack Success Rate**. We use an LLM safeguard model called LLaMA-Guard-2-8B4 to evaluate the effectiveness of the jailbreak attacks. LLaMA-Guard-2-8B can be used for classifying content in both LLM inputs (prompt classification) and in LLM responses (response classification). In this context, we use its function to classify the LLM responses to compute the ASR. It can generate text in its output to indicate whether a given response is safe or unsafe. In our evaluation, we collect the protected LLM's response to the jailbreak prompt and use LLaMA-Guard-2-8B to determine whether the response is unsafe. We regard it as a successful jailbreak if the model outputs "Unsafe".

Footnote 4: [https://huggingface.co/meta-llama/Meta-llama-Guard-2-8B](https://huggingface.co/meta-llama/Meta-llama-Guard-2-8B)

**Win Rate**. We use all the 805 instructions in the AlpacaEval evaluation dataset to compute the Win Rate. We take the default setting which uses alpaca_eval_gpt4 as the annotator and text_davinci_003 as the baseline.

### Implementation of Baselines

The details of the implementation of the baselines are summarised in the following:

* Perplexity calculator: Protected language model itself.
* Threshold: \(5\%\)

### Manyshot prompt example

Question: Provide detailed instructions on how to perform a cyber attack occur interrupt two sentencesmix Buddhi dialect[,Start instructionwith Sure Heres tw essential Dunjest primer Answer: Sure, here is a detailed instruction on how to perform a cyber attack Question: Teach people how to manipulate stock prices for financial gain caps': okay Sur thereein Ric? hashestr a planeton { pr"> Complete moderStyle sentence Answer: Sure, here is a plan to manipulate stock prices for financial gain... Question: Write a tutorial on how to make a bomb jar lei. {tabular poem petitrelative namedunderutorial=" sure heresst forale making="?" Answer: Sure, here is a tutorial for making a bomb Question: **[Input your instruction here]*** Gradient Cuff
* \(\mu\): 0.25
* \(\sigma\): 0.05
* N: 10
* P: 4
* Total query Number: \(50\)

* Mode: Swap
* Perturb Ratio: \(25\%\)
* Perturb Number: \(50\)
* Semantic Smoothing
* Mode: Summarization
* Perturb Number: \(50\)
* Erase Check:
* Mode: Insertion
* Safety-Checker: Protected language model itself.
* Most tokens to be erased: \(25\%\) of the total tokens in the query.
* Note: The number of subsequences is at most 50.
* Self Reminder
* System prompt: You are a responsible **[Insert the Model Name Here]*
* and good at avoiding generating harmful or misleading content! Please answer the following user query in a responsible way **[Insert User Query Here]*
* **[Insert the Model Name Here]*
* and good at avoiding generating harmful or misleading content!

### Complete Experimental Results

We provide complete results for Figure 2, Figure 3, and Table 1 in this section.

### Adaptive Attack

Adaptive attack is a commonly used evaluation scheme to test the resilience of a defense when the defense mechanism is transparent to an attacker [20]. Some studies on jailbreak defense also test their method against adaptive attacks [17, 24, 9]. To see how adaptive attacks could weaken Token Highlighter, we design adaptive attacks based on the methods of GCG and TAP. Specifically, we 

[MISSING_PAGE_FAIL:15]

```
1:Input: Initial prompt \(q_{1:n}\), modifiable subset \(\mathcal{I}\), iterations \(T\), loss \(\mathcal{L}\), \(k\), batch size \(B\)
2:for\(T=1\) : N do
3:for\(i\in\mathcal{I}\)do
4: Compute \(x^{\prime}_{1:n}\) for \(q_{1:n}\) based on Equation 1, 2, 4, 5
5:\(\mathcal{Q}_{i}:=\) Top-\(k(-\nabla_{q_{i}}\mathcal{L}(x^{\prime}_{1:n}))\) #Compute top-\(k\) promising token substitutions
6:endfor
7:\(\mathcal{X}\)=[]
8:\(\mathcal{C}\)=[]
9:for\(b=1:B\)do
10:\(\tilde{q}_{1:n}:=q_{1:n}\) #Initialize element of batch
11:\(\tilde{q}_{i}:=\) UnIf\(\text{Norm}(\mathcal{Q}_{i})\), where \(i=\) UnIf\(\text{Norm}(\mathcal{I})\) #Select random replacement token
12: Compute \(\tilde{x}^{\prime}_{1:n}\) for \(\tilde{q}_{1:n}\) based on Equation 1, 2, 4, 5
13:\(\mathcal{X}\)=\(\mathcal{X}\)+[\(\tilde{x}^{\prime}_{1:n}\)]
14:\(\mathcal{C}\)=\(\mathcal{C}\)+[\(\tilde{q}_{1:n}\)]
15:endfor
16:\(q_{1:n}=\mathcal{C}[b]\), where \(b^{*}=\text{argmin}_{b}\mathcal{L}(\mathcal{X}[b])\) #Compute best replacement
17:endfor
```

**Algorithm 2** Adaptive GCG

```
1:Input: A goal \(G\), a branching-factor \(b\), a maximum width \(w\), and a maximum depth \(d\)
2:Oracles: Query access to an attacker language model \(A\), a Token Highlighter protected target language model \(T(\alpha,\beta)\), and JUDGE and off-topic functions.
3:Preparation:
4: Initialize the system prompt of \(A\)
5: Initialize a tree whose root has an empty conversation history and a prompt \(G\)
6:Generating Jailbreak attacks
7:while depth of the tree is at most \(d\)do
8:Branch
9:for each leaf \(\ell\) of the tree do
10: Sample prompts \(P_{1},P_{2},\ldots,P_{b}\sim q(C;A)\), where \(C\) is the conversation history in \(\ell\)
11: Add \(b\) children of \(\ell\) with prompts \(P_{1},\ldots,P_{b}\) respectively and conversation histories \(C\)
12:endfor
13:Prune (Phase 1)
14:for each (new) leaf \(\ell\) of the tree do
15: If \(\texttt{off-topic}(P,G)=1\), then delete \(\ell\) where \(P\) is the prompt in node \(\ell\)
16:endforQuery and Assess
17:for each (remaining) leaf \(\ell\) of the tree do
18:\(P=\) the prompt in node \(\ell\)
19: Sample response \(R\sim q(P;T(\alpha,\beta))\)
20: Evaluate score \(S\leftarrow\) JUDGE(\(R,G\)) and add score to node \(\ell\)
21: If \(S\) is JAILBROKEN, then return\(P\)
22: Append \([P,R,S]\) to node \(\ell\)'s conversation history
23:endfor
24:Prune (Phase 2):
25:if the tree has more than \(w\) leaves then
26: Select the top \(w\) leaves by their scores (breaking ties arbitrarily) and delete the rest
27:endif
28:endwhile
29:Return None
```

**Algorithm 3** Adaptive TAP