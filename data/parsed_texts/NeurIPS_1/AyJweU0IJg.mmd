# Mode Collapse in Variational Deep Gaussian Processes

Francisco Javier Saez-Maldonado

Universidad de Granada

fjaviersaezm@ugr.es

Juan Maronas

CUNEF Universidad

Universidad Autonoma de Madrid

juan.maronas@cunef.edu

Daniel Hernandez-Lobato

Universidad Autonoma de Madrid

daniel.hernandez@uam.es

###### Abstract

Gaussian Processes (gps)[2] are flexible non-parametric models that have shown promising results across many applications, such as molecule optimization [3, 4] or uncertainty estimation in dnns[5, 6]. They are used as prior distributions over some target function \(f\), _i.e._, \(f\sim\mathcal{GP}(\mu(\cdot),K_{\phi}(\cdot,\cdot))\), where \(\mu(\cdot)\) and \(K_{\phi}(\cdot,\cdot)\) are, respectively, the mean and covariance functions of the gp. Deep Gaussian Processes (dgps) [7] concatenate several gps across a layered network, defining a hierarchical structure that leads to a more flexible probabilistic model. This concatenation enlarges the class of functions that can be modeled, allowing to capture complex patterns within the data.

Bayesian inference in dgps is intractable. Thus, approximations using variational inducing points such as Double Stochastic Variational Inference (dsvi) [8] must be employed. To alleviate optimization difficulties, two main tools are often used: (i) a whitened representation of the process values at the inducing points [9] to enhance the numerical stability of the model [10, 11, 12], and (ii) an identity mean function in the inner layers of the model [8]. In [8], it is claimed that the identity mean function is needed to avoid the pathologies outlined in [13]. However, the reality is that, in practice, this mean function is used even in \(2\)-layer dgps, which are far from being similar models to those in [13].

Here, we show, via experiments in a toy dataset and a dataset from the UCI repository, that the need for the identity mean function in dgps is linked to the initial variational parameters and the usage of the whitened representation. Specifically, we found that a zero mean dgp, with the variational initialization used in [8, 10, 11, 12], can lead to mode collapse during the optimization process, which is undesirable. Namely, the algorithm sets the variational mean and covariance to those of the gp prior (a standard Gaussian in the whitened case). Then, a huge noise variance is set in the observation model to explain the observed data, which is considered pure noise. This is not a desirable behavior.

Our main contributions are: 1) we explain why the whitened representation, beyond being designed to enhance mixing in MCMC algorithms [9], provides numerical stability in the Variational's Inference optimization process; 2) we highlight why the identity mean function in dgps avoids the mode collapse effect that occurs when the zero mean function is used; 3) we provide a theoretical explanation of the effect of whitening in the optimization process; and, 4) we propose a new initialization of the variational parameters of the zero mean dgp that alleviates the mode collapse problem.

## 2 Mode collapse in variational dgps

Consider the observed data \(\mathcal{D}=\{(\mathbf{x}_{i},y_{i})\;\mid\;\mathbf{x}_{i}\in\mathbb{R}^{D},\; y_{i}\in\mathbb{R},\;i=1,\ldots,N\}\) which is grouped into a matrix \(\mathbf{X}=(\mathbf{x}_{1},\cdots,\mathbf{x}_{N})\) and a vector \(\mathbf{y}=(y_{1},\cdots,y_{N})^{\mathsf{T}}\). Consider a single gp. The process values at the training points are denoted by \(\mathbf{f}=(f(\mathbf{x}_{1}),\cdots,f(\mathbf{x}_{N}))^{\mathsf{T}}\). The set of \(M\)inducing locations \(\mathbf{Z}=(\mathbf{z}_{1},\cdots,\mathbf{z}_{M})\), with \(\mathbf{z}_{i}\in\mathbb{R}^{D}\), have their corresponding inducing values \(\mathbf{u}=f(\mathbf{Z})=(\mathbf{u}^{1},\cdots,\mathbf{u}^{M})^{\text{T}}\). The posterior distribution is approximated using a variational distribution \(q(\mathbf{f},\mathbf{u})=p(\mathbf{f}\mid\mathbf{u})q(\mathbf{u})\), with \(q(\mathbf{u})=\mathcal{N}(\mathbf{u}\mid\mathbf{m},\mathbf{S})\) where \(\mathbf{m}\) and \(\mathbf{S}\) are the _variational parameters_[13]. The prior over \(\mathbf{u}\) is \(\mathcal{N}(\mu_{\mathbf{Z}},K_{\mathbf{Z}\mathbf{Z}})\). In the whitened representation, we rewrite \(\mathbf{u}=\mathbf{L}_{\mathbf{Z}\mathbf{Z}}\mathbf{v}+\mu_{\mathbf{Z}}\) where \(K_{\mathbf{Z}\mathbf{Z}}=\mathbf{L}_{\mathbf{Z}\mathbf{Z}}\mathbf{L}_{\mathbf{ Z}\mathbf{Z}}^{\text{T}}\) and \(p(\mathbf{v})=\mathcal{N}(\mathbf{0},\mathbf{I})\). Then, we make inference about \(\mathbf{v}\) instead of \(\mathbf{u}\) and learn \(q(\mathbf{v})=\mathcal{N}(\mathbf{m}_{\mathbf{v}},\mathbf{S}_{\mathbf{v}})\). Variational parameters are initialized as \(\mathbf{m}=\mathbf{m}_{\mathbf{v}}=0\), \(\mathbf{S}=\mathbf{S}_{\mathbf{v}}=10^{-5}\mathbf{I}\).

DGPs concatenate layers of GPs as observed in Fig. 1. Adding the superscript \(l\) to denote the \(l\)-th layer, the training objective for the dsvi algorithm, described in [7], is the elbo:

\[\mathcal{L}=\sum_{n=1}^{N}\mathbb{E}_{q(\mathbf{f}^{L}_{n})}\left[\log p(y_{n} \mid\mathbf{f}^{L}_{n})\right]-\sum_{l=1}^{L}\text{KL}\left(q(\mathbf{u}^{l}) \parallel p(\mathbf{u}^{l};\mathbf{Z}^{l})\right)\,. \tag{1}\]

Non-whitened GPs:In classic svgps, the mean of the marginal variational distribution \(q(\mathbf{f}^{l=1})\) is1:

Footnote 1: When \(l>2\), \(\mathbf{X}\) is replaced by points \(\mathbf{f}^{l-1}\) sampled from \(q(\mathbf{f}^{l-1})\) at the previous layer.

\[\mu_{qf}=K_{\mathbf{X}\mathbf{Z}}K_{\mathbf{Z}\mathbf{Z}}^{-1}\mathbf{m}+\mu _{\mathbf{X}}-K_{\mathbf{X}\mathbf{Z}}K_{\mathbf{Z}\mathbf{Z}}^{-1}\mu_{ \mathbf{Z}}\,. \tag{2}\]

The kl between prior and posterior is given by:

\[\text{KL}\left(q(\mathbf{u})\parallel p(\mathbf{u})\right)=\frac{1}{2}\left[ \log(|K_{\mathbf{Z}\mathbf{Z}}||\mathbf{S}|^{-1})-M+\text{Tr}\left(K_{\mathbf{ Z}\mathbf{Z}}^{-1}(\mathbf{S}+(\mathbf{m}-\mu_{\mathbf{Z}})(\mathbf{m}-\mu_{ \mathbf{Z}})^{T})\right)\right]. \tag{3}\]

Whitened GPs:In the whitened representation we have:

\[\mu_{qf}^{w} =K_{\mathbf{X}\mathbf{Z}}[L_{\mathbf{Z}\mathbf{Z}}^{\text{T}}]^{ -1}\mathbf{m}_{\mathbf{v}}+\mu_{\mathbf{X}}\,, \tag{4}\] \[\text{KL}\left(q(\mathbf{v})\parallel p(\mathbf{v})\right) =\frac{1}{2}\left[-\log|\mathbf{S}_{\mathbf{v}}|-M+\mathbf{m}_{ \mathbf{v}}^{T}\mathbf{m}_{\mathbf{v}}+\text{Tr}(\mathbf{S}_{\mathbf{v}}) \right]\,. \tag{5}\]

KL at initialization and mode collapse:In the whitened case, at initialization, the marginal distribution's variational mean for a zero mean dgp is zero and the kl is very close to zero. Thus, the minimization of the objective will likely force \(\mathbf{m}_{\mathbf{v}}=0\) and \(\mathbf{S}_{\mathbf{v}}=\mathbf{I}\) (since it is the kl minimizer), and will minimize the ell, _i.e._, the data-dependent term in (1), by learning a huge observation noise. However, when using an identity mean function it is simpler to optimize the ell, since the posterior mean at initialization is \(\mathbf{x}_{i}\), the optimization forces \(\mathbf{m}_{\mathbf{v}}\) to move away from its initial value \(0\) to learn a map from \(\mathbf{x}_{i}\) to the outputs \(\mathbf{y}\). This often avoids mode collapse. For the non-whitened case, at initialization, the variational mean is also \(0\) in the zero mean dgp. However, we should expect the model not to be so prone to mode collapse, since at initialization the kl depends on \(K_{\mathbf{Z}\mathbf{Z}}\) via the prior, which can differ from the identity matrix (depending on the length-scale value). The contrary may also be true, _i.e._, one may also expect mode collapse, since now we can freely adapt both the non-whitened gp prior kernel hyper-parameters and \(\mathbf{Z}\), and the variational parameters \(\mathbf{m}\) and \(\mathbf{S}\) to minimize kl. Therefore, in the non-whitened case, mode collapse depends not only on the initialization of \(q\), but on the initial kernel's hyper-parameter as well. In any case, this model usually suffers from optimization difficulties, so even though mode collapse can be controlled, the model is not useful in practice as it is complicated to optimize.

Optimization difficulties:There is a different impact on the kl divergence depending on whether the whitened representation is used or not. In the whitened representation one often observes that: 1) the kl at initialization is close to zero, providing a small learning signal to the objective; and

Figure 1: Example of a three layer dgp, with three units per layer.

[MISSING_PAGE_FAIL:3]

layer, and that we initialize \(\mathbf{m}^{{}^{\prime}}=\mathbf{0}\). This is the typical setup considered in the literature for DGPs [7]. The output of the inner layers of this model model at \(\mathbf{Z}\), is \(\mathbf{Z}\), _i.e._, the identity, as specified by the prior mean \(\mu_{\mathbf{Z}}\). See Eq. (6). However, since the identity mean function is not used in the last layer [7], the predictive mean at the inducing points \(\mathbf{Z}\) will be zero, unlike in the proposed method, described in the previous paragraph. Something similar will happen in the case of the whitened representation, when \(\mathbf{m}_{\mathbf{v}}=\mathbf{0}\). See Eq. (4). That is, the initial DGP predictive mean, without training the model, will be zero for every input.

**Inducing points:** The initial inducing points may impact the initial solution. Selecting them at random from the training set may lead to regions of the input space being unrepresented, leading to a poor initial solution in those areas. See Fig. 2. To ensure a good initialization, we use a two-step algorithm:

1. We compute \(M\) centroids \(\mathcal{C}=\{\mathbf{c}_{1},\cdots,\mathbf{c}_{M}\}\) of \(\mathbf{X}_{\text{train}}\) using _k-means_.
2. We select the inducing locations \(\mathbf{z}_{j}\) from \(\mathbf{X}_{\text{train}}\) using the cosine distance: \[\mathbf{z}_{j}=\arg\min_{\mathbf{x}_{j}\in\mathbf{X}_{\text{train}}}d( \mathbf{c}_{j},\mathbf{x}_{j})=\arg\min_{\mathbf{x}_{j}\in\mathbf{X}_{\text{ train}}}\mathbf{c}_{j}\cdot\mathbf{x}_{j}/(\|\mathbf{c}_{j}\|\|\mathbf{x}_{j}\|), \quad\forall\mathbf{c}_{j}\in\mathcal{C}.\] (8)

Fig. 2 compares the predictive distribution (without training) of the standard initialization of a dgp with that of the proposed initialization with a random selection of the inducing points among the training points, and when selecting the inducing locations as described above. We observe that the proposed initialization already explains quite well the observed data with no training of the objective.

**Optimization difficulties:** As mentioned, with the standard initialization, the kl in the whitened case provides a more stable learning signal (Sec. 2). In the proposed initialization, in the whitened case we have \(p(\mathbf{v})=\mathcal{N}(0,\mathbf{I})\) and \(q(\mathbf{v})=\mathcal{N}(\mathbf{L}_{\mathbf{Z}\mathbf{Z}}^{-1}\mathbf{Z},10^{ -5}\cdot\mathbf{I})\). In the non-whitened (reparameterized version) we have \(p(\mathbf{u})=\mathcal{N}(0,K_{\mathbf{Z}\mathbf{Z}})\) and \(q(\mathbf{u})=\mathcal{N}(\mathbf{Z},10^{-5}\cdot\mathbf{I})\). Therefore, the whitened parameterization is expected to lead to a more stable optimization process since, although both kl are different from \(0\) at initialization, the gradient of the kl for the non-whitened case depends on both the model parameters (_e.g._, length-scales and inducing points) and the variational parameters.

Figure 3: **Left and middle**: Ell and kl, respectively, during train of the \(5\) layer, zero mean function dgp in the _Yacht_ dataset. The mean and standard deviation across 20 splits are plotted. **Right**: Log likelihood results (right is better) of the \(L=2\) and \(L=5\) layer dgps in _Yacht_. The model with \(5\) layers and zero mean suffers from mode collapse, which is solved by the proposed initialization.

Figure 2: Initialization predictions of whitened 2 layer dgps. Yellow dots indicate the inducing locations \(\mathbf{Z}\) and red dots indicate predictive mean \(\mathbf{m}_{\mathbf{v}}\) at the inducing locations.

## 4 Experiments

To validate the proposed initialization, we compare it with the standard initialization. We perform an extensive qualitative evaluation on the toy dataset presented in Fig. 2 and on the _Yatch_ dataset. The results show that the proposed initialization has two benefits: a) it leads to a faster convergence than the standard initialized DGP; and b) the proposed initialization avoids mode collapse when using a 5-layer DGP. Fig. 5 in Appendix B.1 supports point a). The next paragraph supports point b).

We perform an extensive analysis of 8 different UCI datasets. See Appendix B.2. The most interesting case is the _Yacht_ dataset analyzed below. The data-dependent term of the objective (ELL), as a function of the training epochs, is shown in Fig. 3 (left). The kl is also reported (center). We observe a clear example of the mode collapse in the \(5\)-layer dgp when using the zero mean function with the standard initialization. Specifically, the test log-likelihood results displayed in Fig. 3 (right) are significantly worse for the zero mean function with the standard initialization dgps. We observe the ell and the kl (left and center of the figure). In the standard initialization, when using the zero mean function, the kl quickly falls to zero when whitening is not used. Besides this, we also observe that the kl has a high variance when the whitened representation is used. This indicates that there are some data splits where mode collapse happens, even when using whitening. By contrast, the proposed initialization solves the mode collapse problem, preventing the kl from falling to zero by choosing a good initial solution. It also leads to an overall higher final kl. The ell chart (left) shows that, in the whitened case, our proposal exhibits a fast convergence and the final results pair up with the dgp with identity mean function. The non-whitened case has a slower convergence because the initial solution has a low ell, caused by the initial value of the kernel's length-scale4. We observe this behavior in other datasets. Appendix B.2 has a figure with all the results of ll and rmse.

Footnote 4: In the UCI datasets we have not selected a good initial kernel length-scale for each dataset. Due to this, the faster convergence is not appreciated in Fig. 3.

### Kernel invertibility dependency

As a limitation of the proposed initialization, we remark that the correct inversion of the kernel evaluated at the inducing points highly influences the initial solution. The parameters of the kernel (in our case, the length-scale \(\ell\) of the rbf kernel) must be properly selected so that the covariance matrix is not ill-conditioned, making the proposed initialization properly predict the corresponding inducing value at the selected inducing locations. An example of this can be observed in Fig. 4. However, this limitation can be surpassed by performing an initial evaluation of the model using different values of the kernel length-scale and selecting the one with higher ell or lower rmse.

## 5 Conclusions

DGPs may suffer from mode collapse (_i.e._, convergence to the prior) as the number of layers grows. Here, we have presented an initialization of svgps and dgps that uses the training data. This initialization fixes the inducing points and the variational parameters to predict an initial good solution of the targets. The results obtained, when using this initialization, show that it has benefits both in the convergence speed dgps and in avoiding the mode collapse problem. A limitation is that it requires solving a linear system that depends on the initial kernel parameters, which may be problem-dependent.

Figure 4: Predictive distribution of the proposed initialization when the length-scale varies. Proper kernel initialization is key for the success of this initialization.

## Acknowledgments and Disclosure of Funding

The authors acknowledge financial support from project PID2022-139856NB-I00, funded by MCIN and from the Autonomous Community of Madrid (ELLIS Unit Madrid). They also acknowledge the use of the facilities of Centro de Computacion Cientifica, UAM. This work was also supported by grant PID2022-140189OB-C22 funded by MCIN/AEI/10.13039/501100011033 and by "ERDF A way of making Europe", by the European Union.

## References

* [1] Carl Edward Rasmussen and Christopher K. I. Williams. _Gaussian Processes for Machine Learning_. The MIT Press, 2005.
* [2] Rafael Gomez-Bombarelli, Jennifer N Wei, David Duvenaud, Jose Miguel Hernandez-Lobato, Benjamin Sanchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alan Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. _ACS central science_, 4(2):268-276, 2018.
* [3] Austin Tripp, Sergio Bacallado, Sukriti Singh, and Jose Miguel Hernandez-Lobato. Tanimoto random features for scalable molecular machine learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [4] Luis A Ortega, Simon Rodriguez Santana, and Daniel Hernandez-Lobato. Variational linearized laplace approximation for bayesian deep learning. In _Forty-first International Conference on Machine Learning_, 2024.
* [5] Jeremiah Zhe Liu, Shreyas Padhy, Jie Ren, Zi Lin, Yeming Wen, Ghassen Jerfel, Zachary Nado, Jasper Snoek, Dustin Tran, and Balaji Lakshminarayanan. A simple approach to improve single-model deep uncertainty via distance-awareness. _Journal of Machine Learning Research_, 24(42):1-63, 2023.
* [6] Andreas Damianou and Neil D. Lawrence. Deep Gaussian processes. In _Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics_, 2013.
* [7] Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep gaussian processes. In _Advances in Neural Information Processing Systems_, 2017.
* [8] James Hensman, Alexander G Matthews, Maurizio Filippone, and Zoubin Ghahramani. Mcmc for variationally sparse gaussian processes. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015.
* [9] Alexander G. de G. Matthews, Mark van der Wilk, Tom Nickson, Keisuke. Fujii, Alexis Boukouvalas, Pablo Leon-Villagor, Zoubin Ghahramani, and James Hensman. GPflow: A Gaussian process library using TensorFlow. _Journal of Machine Learning Research_, 18(40):1-6, apr 2017.
* [10] Vincent Dutordoir, Hugh Salimbeni, Eric Hambro, John McLeod, Felix Leibfried, Artem Artemev, Mark van der Wilk, Marc P. Deisenroth, James Hensman, and ST John. GPflux: A library for deep gaussian processes. _arXiv:2104.05674_, 2021.
* [11] Jacob R Gardner, Geoff Pleiss, David Bindel, Kilian Q Weinberger, and Andrew Gordon Wilson. Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In _Advances in Neural Information Processing Systems_, 2018.
* [12] David Duvenaud, Oren Rippel, Ryan Adams, and Zoubin Ghahramani. Avoiding pathologies in very deep networks. In Samuel Kaski and Jukka Corander, editors, _Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics_, volume 33 of _Proceedings of Machine Learning Research_, pages 202-210, Reykjavik, Iceland, 22-25 Apr 2014. PMLR.
* [13] Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In _Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics_, 2009.
* [14] Iain Murray and Ryan P Adams. Slice sampling covariance hyperparameters of latent gaussian models. _Advances in neural information processing systems_, 23, 2010.

Background

We include an extended background section for a clearer understanding of the problem statement and notation.

We tackle the standard regression problem, where the goal is to infer an unknown function \(f:\mathbb{R}^{D}\rightarrow\mathbb{R}\), using the observed data \(\mathcal{D}=\{(\mathbf{x}_{i},y_{i})\ \mid\ \mathbf{x}_{i}\in\mathbb{R}^{D},\ y_{i} \in\mathbb{R},\ i=1,\ldots,N\}\). We denote \(\mathbf{X}=(\mathbf{x}_{1},\cdots,\mathbf{x}_{N})\) and \(\mathbf{y}=(y_{1},\cdots,y_{N})\). Gaussian Processes [1] place a multivariate Gaussian prior distribution over \(f\), that is, \(\mathbf{f}=(f(\mathbf{x}_{1}),\cdots,f(\mathbf{x}_{N}))\sim\mathcal{N}(\mu( \mathbf{X}),K_{\phi}(\mathbf{X},\mathbf{X}))\), where \(\mu:\mathbb{R}^{D}\rightarrow\mathbb{R}\) is the mean function, and \(K_{\phi}(\mathbf{X},\mathbf{X})\) is the covariance matrix determined by a covariance function \(K_{\phi}:\mathbb{R}^{D}\times\mathbb{R}^{D}\rightarrow\mathbb{R}\). We write \(\mu_{\mathbf{x}}=\mu(\mathbf{x})\) and \(K_{\mathbf{x}\mathbf{x}}=K_{\phi}(\mathbf{x},\mathbf{x})\) to shorten notation. The exact posterior distribution \(p(\mathbf{f}\mid\mathcal{D})\) is intractable since it scales cubically with \(N\), so we use sparse variational Gaussian Processes (svGps) [13] to scale to large datasets.

SVGPs use a set of inducing locations \(\mathbf{Z}=(\mathbf{z}_{1},\cdots,\mathbf{z}_{M})\), with \(\mathbf{z}_{i}\in\mathbb{R}^{D}\), and \(M\ll N\) and their corresponding inducing values \(\mathbf{u}=f(\mathbf{Z})=(\mathbf{u}^{1},\cdots,\mathbf{u}^{M})\), where \(p(\mathbf{u})=\mathcal{N}(\mathbf{u}\mid\mu(\mathbf{Z}),K_{\phi}(\mathbf{Z}, \mathbf{Z}))\). The posterior distribution is approximated using a variational distribution \(q(\mathbf{f},\mathbf{u})=p(\mathbf{f}\mid\mathbf{u})q(\mathbf{u})\), where \(q(\mathbf{u})=\mathcal{N}(\mathbf{u}\mid\mathbf{m},\mathbf{S})\) where \(\mathbf{m}\) and \(\mathbf{S}\) are the _variational parameters_. Using this variational distribution, the posterior distribution is Gaussian \(q(\mathbf{f})=\mathcal{N}(\mathbf{f}\mid\mu_{qf},K_{qf})\), where:

\[\mu_{qf} =K_{\mathbf{X}\mathbf{Z}}K_{\mathbf{Z}\mathbf{Z}}^{-1}\mathbf{m}+ \mu_{\mathbf{X}\mathbf{Z}}-K_{\mathbf{X}\mathbf{Z}}K_{\mathbf{Z}\mathbf{Z}}^{- 1}\mu_{\mathbf{Z}} \tag{9}\] \[K_{qf} =K_{\mathbf{X}\mathbf{X}}-K_{\mathbf{X}\mathbf{Z}}K_{\mathbf{Z} \mathbf{Z}}^{-1}K_{\mathbf{Z}\mathbf{X}}+[K_{\mathbf{X}\mathbf{Z}}K_{\mathbf{Z }\mathbf{Z}}^{-1}]\mathbf{S}[K_{\mathbf{X}\mathbf{Z}}K_{\mathbf{Z}\mathbf{Z}} ^{-1}]^{T} \tag{10}\]

The presented formulation is sometimes referred to as the _unwhitened_ representation of a gp. Another common form is the _whitened prior_ gp representation [8; 14]. This consists of expressing \(\mathbf{u}=\mathbf{L}_{\mathbf{Z}\mathbf{Z}}\mathbf{v}\) with \(\mathbf{L}_{\mathbf{Z}\mathbf{Z}}\mathbf{L}_{\mathbf{Z}\mathbf{Z}}^{T}=K_{ \mathbf{Z}\mathbf{Z}}\) and \(\mathbf{v}=\mathcal{N}(\mathbf{v}\mid\mathbf{0},\mathbf{I})\). The original variational parameters are recovered as \(\mathbf{m}=\mu_{\mathbf{Z}}+\mathbf{L}_{\mathbf{Z}\mathbf{Z}}\mathbf{m}_{ \mathbf{v}}\) and \(\mathbf{S}=\mathbf{L}_{\mathbf{Z}\mathbf{Z}}\mathbf{S}_{\mathbf{v}}\mathbf{L} _{\mathbf{Z}\mathbf{Z}}^{T}\), so the parameters to estimate are now \(\mathbf{m}_{\mathbf{v}},\mathbf{S}_{\mathbf{v}}\). Using this representation, the posterior \(q^{w}(\mathbf{f})=\mathcal{N}(\mathbf{f}\mid\mu_{qf}^{w},K_{qf}^{w})\) is computed as:

\[\mu_{qf}^{w} =K_{\mathbf{X}\mathbf{Z}}[\mathbf{L}_{\mathbf{Z}\mathbf{Z}}^{T}]^ {-1}\mathbf{m}_{\mathbf{v}}+\mu_{\mathbf{X}}, \tag{11}\] \[K_{qf}^{w} =K_{\mathbf{X}\mathbf{X}}-K_{\mathbf{X}\mathbf{X}}K_{\mathbf{Z} \mathbf{Z}}^{-1}K_{\mathbf{Z}\mathbf{X}}+K_{\mathbf{X}\mathbf{Z}}\mathbf{L}_{ \mathbf{Z}\mathbf{Z}}^{T}-1\mathbf{S}_{\mathbf{v}}\mathbf{L}^{-1}K_{\mathbf{Z }\mathbf{X}}. \tag{12}\]

Whitening the gp prior leads to a much faster convergence in many cases [14].

#### Deep Gaussian Processes

Following the fashion of deep neural networks, dgps concatenate layers of gps to generalize them and extend their flexibility. Formally, the output of the gp in layer \(l-1\) is used as the input of layer \(l\), defining a prior recursively on each layer \(F^{1},\cdots,F^{L}\). Each layer \(l\) is defined by its inducing locations and values \((\mathbf{Z}^{l},\mathbf{u}^{l})\), and the corresponding variational parameters \((\mathbf{m}^{l},\mathbf{S}^{l})\). The joint distribution of the dgp factorizes as:

\[p(\mathbf{y}\mid\{\mathbf{F}^{l},\mathbf{u}^{l}\}_{l=1}^{L})=\underbrace{ \prod_{i=1}^{N}p(y_{i}\mid\mathbf{f}_{i}^{L})\prod_{l=1}^{L}p(\mathbf{F}^{l} \mid\mathbf{u}^{l};\ \mathbf{F}^{l-1},\mathbf{Z}^{l-1})p(\mathbf{u}^{l};\mathbf{Z}^{l-1})}_{ \text{dgp Prior}}. \tag{13}\]

The exact posterior remains intractable in dgps, so it is approximated using a variational distribution that mimics the one in svgps and assumes layer independence:

\[q(\{\mathbf{F}^{l},\mathbf{u}^{l}\}_{l=1}^{L})=\prod_{l=1}^{L}p(\mathbf{F}^{l} \mid\mathbf{u}^{l};\mathbf{F}^{l-1},\mathbf{Z}^{l-1})q(\mathbf{u}^{l}). \tag{14}\]

With these considerations, we obtain an elbo that can be optimized using two sources of stochasticity [7]:

\[\mathcal{L}=\sum_{n=1}^{N}\mathbb{E}_{q(\mathbf{f}_{n}^{L})}\left[\log p(y_{n} \mid\mathbf{f}_{n}^{L})\right]-\sum_{l=1}^{L}\text{KL}\left(q(\mathbf{u}^{l}) \parallel p(\mathbf{u}^{l};\mathbf{Z}^{l-1}\right) \tag{15}\]Further Experiments

Implementation detailsIn the toy experiment, we generate \(300\) datapoints train the models using a learning rate of \(\lambda=10^{-3}\). For the UCI experiments, we use 20 different train-test splits. We use a learning rate of \(\lambda=10^{-2}\) and train for \(20K\) iterations. In both cases, we run all our experiments in cpu. For the implementation, we use gpflow_2.1.3_, which builds upon _Tensorflow_. The used kernel is the standard rbf kernel:

\[K(\mathbf{x},\mathbf{x}^{\prime})=\exp\left(-\frac{\|\mathbf{x}-\mathbf{x}^{ \prime}\|^{2}}{2\ell^{2}}\right), \tag{16}\]

where \(\ell\) is the kernel _lengthscale_.

### Toy Dataset

During the experimentation in the toy dataset, we have found other significant results that further show the benefits or limitations of using the proposed initialization. We present them in this section.

Convergence speedDue to the initial good solution, we found out that our initialization speeds up the convergence of dgps. This comes from the fact that, when the kernel parameters are properly chosen, the initial predictions pose an already good solution, with a lower rmse. An example of this is shown in Fig. 5.

Whitening provides stabilityWe have observed that for any initialization or mean function, using the whitening representation of the dgp is key to optimization smoothness. Even in the case of the Identity mean function, where the convergence is always faster than the Zero mean dgp, we have observed in this toy dataset that not using the whitening representation may lead to a poor training of the model. An example of this is shown in Fig. 6, where the kl of the \(5\) layer, Identity mean dgp presents oscillations that hinder the optimization task and do not appear in the whitened case.

### UCI Datasets

We compare our initialization with the standard one, using both whitened and non-whitened representations of the variational distribution. We also include the Identity mean dgp for further comparison. We use \(L\in\{2,3,4,5\}\) for each model.

The Log Likelihood results are shown in Fig. 7. The results show that dgps with standard initialization suffers from mode collapse when the number of layers is increased (which can be seen in _Kin8nm, Power, Redwine or Yatch_). However, the proposed initialization prevents that mode collapse, obtaining

Figure 5: Top row: Initialization of the dgp. Standard initialization for the Zero mean and identity mean, and proposed initialization at the right. Bottom row: predictions of each of the models after 500 epochs.

results which are at least paired to the results obtained using the identity mean function in the inner layers of the DGP.

In Fig. 8 we represent the results using the rmse. The conclusions are similar to the ones obtained from the ll results. Again, the poor solutions in terms of rmse can be clearly seen in the case of a zero mean DGP with standard initialization, with those cases being solved with our initialization.

Figure 6: Loss function values of the \(5-\) layer DGP in the toy dataset. The convergence stability that the whitened version of the model provides is clearly appreciated. Both the ell and kl are much harder to optimize in the non-whitened case.

Figure 7: Log likelihood (right is better) results in the 8 UCI dataset, for 4 different number of layers.

Figure 8: rmse (left is better) results in the 8 UCI dataset, for 4 different number of layers.