# A Quadratic Synchronization Rule for

Distributed Deep Learning

 Xinran Gu\({}^{1}\)1 Kaifeng Lyu\({}^{3}\)1 Sanjeev Arora\({}^{3}\)2 Jingzhao Zhang\({}^{1,2}\)\({}^{\dagger}\) Longbo Huang\({}^{1}\)\({}^{1}\)

\({}^{1}\)Institute for Interdisciplinary Information Sciences, Tsinghua University

\({}^{2}\)Shanghai Qizhi Institute

\({}^{3}\)Department of Computer Science & Princeton Language and Intelligence, Princeton University

gxr21@mails.tsinghua.edu.cn

{klyu,arora}@cs.princeton.edu

{jingzhaoz,longbohuang}@tsinghua.edu.cn

Equal contributionCorresponding authors

Footnote 1: Code available at [https://github.com/hmgxr128/QSR](https://github.com/hmgxr128/QSR)

###### Abstract

In distributed deep learning with data parallelism, synchronizing gradients at each training step can cause a huge communication overhead, especially when many nodes work together to train large models. Local gradient methods, such as Local SGD, address this issue by allowing workers to compute locally for \(H\) steps without synchronizing with others, hence reducing communication frequency. While \(H\) has been viewed as a hyperparameter to trade optimization efficiency for communication cost, recent research indicates that setting a proper \(H\) value can lead to generalization improvement. Yet, selecting a proper \(H\) is elusive. This work proposes a theory-grounded method for determining \(H\), named the Quadratic Synchronization Rule (QSR), which recommends dynamically setting \(H\) in proportion to \(\frac{1}{\eta^{2}}\) as the learning rate \(\eta\) decays over time. Extensive ImageNet experiments on ResNet and ViT show that local gradient methods with QSR consistently improve the test accuracy over other synchronization strategies. Compared with the standard data parallel training, QSR enables Local AdamW on ViT-B to cut the training time on 16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the same time, achieves \(1.12\%\) or \(0.84\%\) higher top-1 validation accuracy.

## 1 Introduction

The growing scale of deep learning necessitates distributed training to reduce the wall-clock time. Data parallel training is a foundational technique that distributes the workload of gradient computation to \(K\) workers, also serving as a key building block of more advanced parallel strategies. At each step of this method, each worker first computes gradients on their own local batches of data. Then, they take an average over local gradients, which typically involves a costly All-Reduce operation. Finally, they update the model parameter with the averaged gradient and a gradient-based optimizer OPT, e.g., SGD, AdamW. In this paper, we term the data parallel implementation of optimizer OPT as "Parallel OPT". See Algorithm 1 for the pseudocode. The cost for this data parallelism is obvious. Frequent gradient synchronization can induce huge communication overhead as the number of workers and model size grow, severely hindering the scalability of distributed training (Tang et al., 2021; Li et al., 2022; Xu et al., 2023).

One approach to reducing this communication overhead is Local SGD (Stich, 2018; Zhou & Cong, 2018; Woodworth et al., 2020). Rather than synchronizing gradients at every step, Local SGD allows workers to independently train their local replicas using their own local batches with SGDupdates. It is only after completing \(H>1\) local steps that these workers synchronize, where the model parameters get averaged over all replicas. Notably, while we mention SGD, this approach can be readily adapted to other popular optimizers. In this paper, if a gradient-based optimizer OPT is used for local updates, we term the variant as "Local OPT" (e.g., Local SGD, Local AdamW), and collectively refer to this class of approaches as _local gradient methods_. We provide a pseudocode for local gradient methods in Algorithm 2.

The main focus of this paper is to study the best strategies to set the synchronization period \(H\) (i.e., the number of local steps per communication round) in local gradient methods. While setting \(H\) to a larger value reduces communication, a very large \(H\) can hinder the training loss from decreasing at normal speed, since the local replicas may significantly diverge from each other before averaging. Indeed, it has been observed empirically that larger \(H\) leads to higher training loss after the same number of steps (Wang and Joshi, 2021; Ortiz et al., 2021), and efforts to analyze the convergence of local gradient methods in theory usually end up with loss bounds increasing with \(H\)(Khaled et al., 2020; Stich, 2018; Haddadpour et al., 2019; Yu et al., 2019). To better trade-off between communication cost and optimization speed, Kamp et al. (2014); Wang and Joshi (2019); Haddadpour et al. (2019); Shen et al. (2021) proposed adaptive synchronization schemes, such as linearly increasing \(H\) as the iteration goes on (Haddadpour et al., 2019), or adjusting \(H\) based on the variance in model parameters (Kamp et al., 2014). Nonetheless, their effectiveness has only been validated on linear models or small-scale datasets, e.g., CIFAR-10/100.

All these strategies are developed to avoid sacrificing too much training loss, but training loss is _never_ the final evaluation metric that one cares about in deep learning. Due to the overparameterized nature of modern neural networks, reaching the same training loss does not correspond to the same performance on test data. It has also been long known that the choice of optimizers or hyperparameters can change not only the optimization speed of the training loss but also their _implicit bias_ towards solutions with different test accuracies.

The presence of this implicit bias indeed complicates the picture of setting \(H\) in local gradient methods. Though a large \(H\) might be harmful for training loss, it has been observed empirically that setting \(H\) properly can sometimes improve rather than hurt the final test accuracy. Lin et al. (2020) are the first to report this phenomenon. Comparing with running just the standard data parallel SGD (equivalent to \(H=1\)), they observed that switching from SGD to Local SGD (\(H>1\)) halfway through consistently leads to higher final test accuracy. Local SGD with this specific schedule of \(H\) is designated as _Post-local SGD_. Lin et al. (2020)'s work opens up a new angle in setting \(H\) in local gradient methods, yet, the proposed schedule in Post-local SGD, referred to as the post-local schedule in this paper, is suboptimal in improving test accuracy. It was later reported by Ortiz et al. (2021) that Post-local SGD does not improve much on ImageNet. For both stepwise decay and cosine decay learning rate schedules, the test accuracy improvement of Post-local SGD diminishes as learning rate decreases. Further, it remains unclear whether the generalization benefit continues to appear when the optimizer is changed from SGD to adaptive gradient methods such as Adam/AdamW, which are now indispensable for training large models.

Figure 1: When training ResNet-152 and ViT-B on ImageNet with cosine learning rate decay, Local SGD/AdamW with QSR consistently outperforms data parallel methods or Local SGD/AdamW with other synchronization strategies in terms of top-1 validation accuracy, while only requiring 20.1% and 10.4% of the communication volume used by data parallel methods, respectively. With QSR, Local SGD on ResNet or Local AdamW on ViT cuts the training time from 20.7 to 18 hours or 26.7 to 20.2 hours on 16 GPUs, when compared with data parallel methods. See Appendix C for training details.

Our Contributions.In this paper, we aim to propose a general and effective \(H\) schedule that can be readily applied to various optimizers and neural network models. Specifically, we introduce a simple yet effective strategy, called _Quadratic Synchronization Rule_ (QSR), for dynamically adjusting the synchronization period according to the learning rate: given a learning rate schedule, we set \(H\) proportional to \(\eta^{-2}\) as the learning rate \(\eta\) decays. This rule is largely inspired by a previous theoretical work (Gu et al., 2023), which shows that the generalization benefits arise only if \(H=\Omega(\frac{1}{\eta})\) when \(\eta\to 0\), but did not make any recommendation on how to set \(H\).

Our main contributions are:

1. We propose the Quadratic Synchronization Rule (QSR) to simultaneously reduce the wall-clock time and improve the final test accuracy of local gradient methods. Based on the theoretical insights in Theorem 3.1, we provide a theoretical separation among data parallel SGD, Local SGD with \(H\sim\eta^{-1}\), and Local SGD with QSR in terms of SDE approximations. We show that QSR can help reduce sharpness faster and hence improve generalization.
2. We demonstrate with ImageNet experiments that QSR can consistently improve the final test accuracy of ResNet-152 and ViT-B over other synchronization strategies, including constant-period and post-local schedules, and also \(H\sim\eta^{-1}\) which one will expect to be optimal from the optimization perspective (Figure 1).
3. We thoroughly validate the efficacy of QSR not only for Local SGD but also for Local AdamW, which is arguably more suitable for training large models. We also validate its efficacy for cosine, linear and step decay learning rate schedules that are commonly used in practice.
4. We evaluate the communication efficiency of QSR on a 64-GPU NVIDIA GeForce RTX 3090 cluster. As an illustrative example, the standard data parallel AdamW takes 8.6 hours to train ViT-B for 300 epochs. With our QSR, Local AdamW cuts the training time down to 5.5 hours with even higher test accuracy.

## 2 Our Method: Quadratic Synchronization Rule

In this section, we first formulate the local gradient methods and then present our Quadratic Synchronization Rule (QSR) in detail.

Local Gradient Methods.Given any gradient-based optimizer OPT, the corresponding local gradient method consists of multiple communication rounds. At the \(s\)-th round, each of the \(K\) workers (say the \(k\)-th) gets a local copy of the global iterate \(\bar{\mathbf{\theta}}^{(s)}\), i.e., \(\mathbf{\theta}^{(s)}_{k,0}\leftarrow\bar{\mathbf{\theta}}^{(s)}\), and then performs \(H\) steps of local updates. At the \(h\)-th local step of the \(s\)-th round, which corresponds to the \((sH+h)\)-th iteration globally, each worker gets a batch of \(B_{\mathrm{loc}}\) samples \((\xi^{(s)}_{k,h,1},\ldots,\xi^{(s)}_{k,h,B_{\mathrm{loc}}})\) from a globally shared dataset \(\tilde{D}\), computes the gradient on that batch, and updates the model with optimizer OPT and learning rate \(\eta_{sH+h}\):

\[\mathbf{\theta}^{(s)}_{k,h+1}\leftarrow\mathrm{OPT}(\mathbf{\theta}^{(s)}_{k,h},\eta_{ sH+h},\mathbf{g}^{(s)}_{k,h})\quad\text{where}\quad\mathbf{g}^{(s)}_{k,h}=\frac{1}{B_{ \mathrm{loc}}}\sum_{i=1}^{B_{\mathrm{loc}}}\nabla\ell(\mathbf{\theta}^{(s)}_{k,h;i} ;\xi^{(s)}_{k,h,i}). \tag{1}\]

After finishing \(H\) steps of local updates, all workers average their local models to generate the next global iterate: \(\bar{\mathbf{\theta}}^{(s+1)}\leftarrow\frac{1}{K}\sum_{k=1}^{K}\mathbf{\theta}^{(s)} _{k,H}\). Note that conventional local gradient methods set the synchronization period as a constant, denoted as \(H\), throughout training. See also Algorithm 2.

Quadratic Synchronization Rule.Given a learning rate schedule \(\eta_{t},t\in\{0,\cdots,T-1\}\) that decays with time, instead of keeping \(H\) constant, we propose to dynamically increase the synchronization period \(H^{(s)}\) at each round \(s\) as the learning rate decreases. More specifically, if at the global iteration \(t\) we need to start a new communication round, then we set

\[H^{(s)}:=\max\left\{H_{\mathrm{base}},\left\lfloor\left(\frac{\alpha}{\eta_{t} }\right)^{2}\right\rfloor\right\}. \tag{2}\]

Here \(H_{\mathrm{base}}\) is a constant indicating the minimum number of local steps one would like to use for each round, which should be set according to the relative cost of computation and communication. The coefficient \(\alpha\), termed the "growth coefficient" henceforth, is a hyperparameter controlling how fast \(H^{(s)}\) increases as \(\eta_{t}\) decreases.

As suggested by our later theorem 3.1, \(\alpha\) should be set as a small constant. In our experiments, we tune \(\alpha\) properly between \(0.01\) and \(0.5\) and test the effectiveness of our proposed method with \(H_{\rm base}=2,4,8\). Note that the last communication round may not finish exactly at the last iteration of the learning rate schedule. If this is the case, we force a synchronization at the last step by setting \(H^{(s)}:=T-t\).

A surprising part of our method is that we use the power \(2\) in the above formula (2). This choice of power \(2\) is inspired by the analysis in Gu et al. (2023), which suggests that setting \(H=\Omega(\frac{1}{\eta})\) is beneficial for reducing the sharpness of the local landscape. Indeed, \(H^{(s)}\) could have been set to \(H^{(s)}:=\max\left\{H_{\rm base},\left|\left(\frac{\alpha}{\eta_{t}}\right)^{ \gamma}\right|\right\}\) for any \(\gamma\). However, using \(\gamma=2\) is crucial for the success of our method, and we will provide a theoretical justification of this choice in Section 3, together with empirical evidence. We also visualize the \(H\) schedule for QSR in Figure 5 in the appendix.

Dealing with Learning Rate Warmup.Many learning rate schedules use a warmup phase where the learning rate increases linearly from \(0\) to \(\eta_{\rm max}\), and then decays monotonically. This warmup phase is often used to avoid the instability caused by the initial large learning rate (Goyal et al., 2017). Our rule is not directly compatible with the warmup phase, since it is designed for learning rate decay, but the learning rate increases rather than decreases in this phase. Practically, we recommend setting \(H^{(s)}\) as the value to be used in the communication round right after the warmup.

## 3 Theoretical Motivations of Quadratic Synchronization Rule

To justify our choice of power 2, we build on the same theoretical setup as Gu et al. (2023) to analyze the Stochastic Differential Equation (SDE) approximation of SGD and Local SGD using different scalings of \(H\) with respect to \(\eta\). Though the learning rate continuously decays over time in most of our experiments, it does not usually change much within a couple of epochs. Inspired by this, we take a quasistatic viewpoint: consider a significant period of time where the learning rate is relatively constant, and directly treat the learning rate as a real constant \(\eta\). First, we recap Gu et al. (2023)'s theory that applies to Local SGD with \(H\sim\eta^{-1}\), then we show how to generalize the result to our rule where \(H\sim\eta^{-2}\), leading to a stronger implicit bias towards flatter minima.

Setup.Consider optimizing the loss function \(\mathcal{L}(\mathbf{\theta}):=\mathbb{E}_{\xi\sim\tilde{\mathcal{D}}}[\ell(\mathbf{ \theta};\xi)]\), where \(\mathbf{\theta}\in\mathbb{R}^{d}\) is the parameter vector and \(\ell(\mathbf{\theta};\xi)\) is the loss function for a single data sample \(\xi\) drawn from a training set/training distribution \(\tilde{\mathcal{D}}\). We use \(\mathbf{\Sigma}(\mathbf{\theta}):=\mathrm{Cov}_{\xi\sim\tilde{\mathcal{D}}}[\nabla \ell(\mathbf{\theta};\xi)]\) to denote the covariance matrix of the stochastic gradient \(\nabla\ell(\mathbf{\theta};\xi)\) at \(\mathbf{\theta}\). Following Gu et al. (2023), we make regularity assumptions on \(\mathcal{L}(\mathbf{\theta}),\mathbf{\Sigma}(\mathbf{\theta})\) and \(\|\nabla\ell(\mathbf{\theta};\xi)\|_{2}\) in Assumption E.1, and we assume that \(\mathcal{L}\) has a manifold \(\Gamma\) of minimizers in Assumption E.2. Our analysis is based on SDE approximations near \(\Gamma\), providing a clean view of how different choices of \(H\) affect the selection of minimizers by Local SGD.

SDE approximations of SGD and Local SGD.SDE is a powerful tool to precisely characterize the effect of noise in SGD, leading to many applications such as Linear Scaling Rule (Goyal et al., 2017). The SDE \(\mathrm{d}\mathbf{\theta}(t)=-\nabla\mathcal{L}(\mathbf{\theta}(t))\mathrm{d}t+\frac{ 1}{\sqrt{B}}\mathbf{\Sigma}(\mathbf{\theta}(t))^{1/2}\mathrm{d}\mathbf{W}_{t}\) is conventionally used in the literature (Jastrzebski et al., 2017; Smith et al., 2020; Li et al., 2021b), where \(\mathbf{W}_{t}\) is the standard Wiener process. In this SDE, each discrete step corresponds to a continuous time interval of length \(\eta\), and the expected gradient and gradient noise become a deterministic drift term and a stochastic diffusion term, respectively. When the training proceeds to a point \(\mathbf{\theta}(t)\) near a minimizer \(\mathbf{\zeta}_{0}\) on the manifold \(\Gamma\), the gradient \(\nabla\mathcal{L}(\mathbf{\theta}(t))\) is almost zero but the gradient noise \(\frac{1}{\sqrt{B}}\mathbf{\Sigma}(\mathbf{\theta}(t))^{1/2}\mathrm{d}\mathbf{W}_{t}\) drives the parameter to diffuse locally. This can be captured by a careful first-order approximation of the dynamics, leading to an Ornstein-Uhlenbeck process (Zhu et al., 2019; Li et al., 2019; Izmailov et al., 2018). However, these rough approximations only hold for about \(\mathcal{O}(\eta^{-1})\) steps, whereas neural networks in practice are usually trained for much longer.

Recently, a series of works (Blanc et al., 2020; Damian et al., 2021; Li et al., 2021c) study the dynamics of SGD on a _longer_ horizon. They show that higher-order terms can accumulate over time and drive this local diffusion to gradually move on the manifold \(\Gamma\). Among them, Li et al. (2021c) precisely characterized this with an SDE tracking the _gradient flow projection_ of \(\mathbf{\theta}(t)\) on \(\Gamma\), denoted as \(\Phi(\mathbf{\theta}(t))\) (see Definition E.1). Here, \(\Phi(\mathbf{\theta}(t))\) can be thought of as a natural "center" of the local diffusion. This SDE, termed as Slow SDE, tracks the dynamics of SGD over \(\mathcal{O}(\eta^{-2})\) steps, which is much longer than the \(\mathcal{O}(\eta^{-1})\) horizon for conventional SDEs.

To provide a theoretical understanding of why Local SGD generalizes better than SGD, Gu et al. (2023) derived the Slow SDEs for Local SGD using the scaling \(H\sim\eta^{-1}\). By comparing the Slow SDEs, they argued that Local SGD drifts faster to flatter minima than SGD. However, their analysis does not encompass the more aggressive scaling \(H\sim\eta^{-2}\) recommended by our QSR. Recognizing this gap, we derive the Slow SDE for this scaling, enriching the theoretical framework for the generalization behavior of Local SGD. Below, we first present the Slow SDEs for SGD and Local SGD with \(H\sim\eta^{-1}\) and \(H\sim\eta^{-2}\), then we interpret why \(H\sim\eta^{-2}\) may generalize better.

**Definition 3.1** (Slow SDE for SGD, informal, (Li et al., 2021; Gu et al., 2023)).: _Given \(\zeta_{0}\in\Gamma\), define \(\mathbf{\zeta}(t)\) as the solution to the following SDE with initial condition \(\mathbf{\zeta}(0)=\mathbf{\zeta}_{0}\):_

\[\mathrm{d}\mathbf{\zeta}(t)=P_{\mathbf{\zeta}}\Big{(}\underbrace{\frac{1}{\sqrt{B}} \mathbf{\Sigma}_{\|}^{\sfrac{1}{2}}(\mathbf{\zeta})\mathrm{d}\mathbf{W}_{t}}_{\text{(a) diffusion on $\Gamma$}}-\underbrace{\frac{1}{2B}\nabla^{3}\mathcal{L}(\mathbf{\zeta})[\widehat{ \mathbf{\Sigma}}_{\Diamond}(\mathbf{\zeta})]\mathrm{d}t}_{\text{(b) drift on $\Gamma$}}\Big{)}. \tag{3}\]

_Here, \(P_{\mathbf{\zeta}}\) is a projection operator of differential forms to ensure that taking an infinitesimal step from \(\mathbf{\zeta}\in\Gamma\) remains on the manifold \(\Gamma\). \(B\) is the total batch size. \(\mathbf{\Sigma}_{\|}(\mathbf{\zeta})\) and \(\widehat{\mathbf{\Sigma}}_{\Diamond}(\mathbf{\zeta})\) are certain PSD matrices related to gradient noise and Hessian. See Definition E.2 for the full definition._

**Definition 3.2** (Slow SDE for Local SGD with \(H\sim\eta^{-1}\), informal (Gu et al., 2023)).: _Consider the scaling \(H=\beta/\eta\) for some constant \(\beta\). Given \(\zeta_{0}\in\Gamma\), define \(\mathbf{\zeta}(t)\) as the solution to the following SDE with initial condition \(\mathbf{\zeta}(0)=\mathbf{\zeta}_{0}\):_

\[\mathrm{d}\mathbf{\zeta}(t)=P_{\mathbf{\zeta}}\Big{(}\underbrace{\frac{1}{\sqrt{B}}\bm {\Sigma}_{\|}^{\sfrac{1}{2}}(\mathbf{\zeta})\mathrm{d}\mathbf{W}_{t}}_{\text{(a) diffusion on $\Gamma$}}-\underbrace{\frac{1}{2B}\nabla^{3}\mathcal{L}(\mathbf{\zeta})[ \widehat{\mathbf{\Sigma}}_{\Diamond}(\mathbf{\zeta})]\mathrm{d}t}_{\text{(b) drift on $\Gamma$, same as SGD}}-\underbrace{\frac{K}{2B}\nabla^{3}\mathcal{L}(\mathbf{\zeta})[ \widehat{\mathbf{\Psi}}(\mathbf{\zeta};H\eta)]\mathrm{d}t}_{\text{(c) an extra drift term on $\Gamma$}}\Big{)}, \tag{4}\]

_where \(K\) is the number of workers, \(B,\mathbf{\Sigma}_{\|}(\mathbf{\zeta})\) and \(\widehat{\mathbf{\Sigma}}_{\Diamond}(\mathbf{\zeta})\) are the same as in Definition 3.1. Here, \(\widehat{\mathbf{\Psi}}(\mathbf{\zeta};\beta)\) is a PSD matrix depending on gradient noise and Hessian. It scales with \(\beta\) as \(\lim_{\beta\to 0}\widehat{\mathbf{\Psi}}(\mathbf{\zeta};\beta)=\mathbf{0}\), \(\lim_{\beta\to+\infty}\widehat{\mathbf{\Psi}}(\mathbf{\zeta};\beta)=\widehat{\mathbf{\Sigma}}_{\Diamond}(\mathbf{\zeta})\). See Definition E.3 for the full definition._

**Definition 3.3** (Slow SDE for Local SGD with QSR).: _Given \(\mathbf{\zeta}_{0}\in\Gamma\), define \(\mathbf{\zeta}(t)\) as the solution to the following SDE with initial condition \(\mathbf{\zeta}(0)=\mathbf{\zeta}_{0}\):_

\[\mathrm{d}\mathbf{\zeta}(t)=P_{\mathbf{\zeta}}\Big{(}\underbrace{\frac{1}{\sqrt{B}} \mathbf{\Sigma}_{\|}^{\sfrac{1}{2}}(\mathbf{\zeta})\mathrm{d}\mathbf{W}_{t}}_{\text{(a) diffusion on $\Gamma$}}-\underbrace{\frac{K}{2B}\nabla^{3}\mathcal{L}(\mathbf{\zeta})[ \widehat{\mathbf{\Sigma}}_{\Diamond}(\mathbf{\zeta})]\mathrm{d}t}_{\text{(b) drift on $\Gamma$, $K$ times larger}}\Big{)}, \tag{5}\]

_where \(K,B,\mathbf{\Sigma}_{\|}(\mathbf{\zeta})\) and \(\widehat{\mathbf{\Sigma}}_{\Diamond}(\mathbf{\zeta})\) are defined in Definitions 3.1 and 3.2._

The following approximation theorem indicates that when the learning rate \(\eta\) and the growth coefficient \(\alpha\) for QSR are small, the above Slow SDEs closely track their discrete counterparts. The approximation theorem for QSR is new, and we defer the proof to Appendix E.2.

**Theorem 3.1** (Weak Approximations).: _Let \(T>0\) be a constant and \(\mathbf{\zeta}(t)\) be the solution to one of the above Slow SDEs with the initial condition \(\mathbf{\zeta}(0)=\Phi(\mathbf{\theta}^{(0)})\in\Gamma\). Let \(g(\mathbf{\theta})\) be any \(\mathcal{C}^{4}\)-smooth function._

1. _(Gu et al.,_ 2023_) For SGD, let_ \(\mathbf{\zeta}(t)\) _be the solution to (_3_). Then,_ \(\max_{0\leq s\leq\frac{T}{\eta^{2}}}|\mathbb{E}[g(\Phi(\mathbf{\theta}_{s}))]- \mathbb{E}[g(\mathbf{\zeta}(s\eta^{2}))]|=\tilde{\mathcal{O}}(\eta^{0.25})\)_._
2. _(Gu et al.,_ 2023_) For Local SGD with_ \(H=\beta/\eta\) _for some constant_ \(\beta\)_, let_ \(\mathbf{\zeta}(t)\) _be the solution to (_4_). Then,_ \(\max_{0\leq s\leq\frac{T}{H\eta^{2}}}|\mathbb{E}[g(\Phi(\mathbf{\theta}^{(s)}))]- \mathbb{E}[g(\mathbf{\zeta}(sH\eta^{2}))]|=\tilde{\mathcal{O}}(\eta^{0.25})\)_._
3. _For Local SGD with_ \(H=(\frac{\alpha}{\eta})^{2}\)_, where the positive constant_ \(\alpha\) _is small but larger than_ \(\Omega(\eta^{\gamma})\) _for all_ \(\gamma>0\)_, let_ \(\mathbf{\zeta}(t)\) _be the solution to (_5_). Then,_ \(\max_{0\leq s\leq\frac{T}{H\eta^{2}}}|\mathbb{E}[g(\Phi(\mathbf{\theta}^{(s)}))]- \mathbb{E}[g(\mathbf{\zeta}(sH\eta^{2}))]|=\mathcal{O}(\alpha^{2})\)_._

_Here, \(\mathcal{O}(\,\cdot\,)\) and \(\tilde{\mathcal{O}}(\cdot)\) hide constants that are independent of \(\alpha\) and \(\eta\) but can depend on \(g\) and \(T\). \(\tilde{\mathcal{O}}(\cdot)\) also hides log terms._

By comparing the Slow SDEs, we can predict the generalization order for different scaling as \(\mathbf{QSR}\)\(>\{\mathbf{H}\sim\mathbf{\eta}^{-1}\}>\mathbf{\mathbf{constant}}\), which we explain in detail below.

Interpretation of the Slow SDEs.We first focus on the Slow SDE for SGD (3). The key component of this Slow SDE is the drift term (b), which comes from higher-order approximations of the aforementioned local diffusion that happens in \(\mathcal{O}(\eta^{-1})\) steps. Viewing \(\nabla^{3}\mathcal{L}(\zeta)[\widehat{\mathbf{\Sigma}}_{\zeta}(\zeta)]\) as a semi-gradient of \(\langle\nabla^{2}\mathcal{L}(\mathbf{\zeta}),\widehat{\mathbf{\Sigma}}_{\zeta}(\zeta)\rangle\) that discards the dependence of \(\mathbf{\theta}\) in \(\widehat{\mathbf{\Sigma}}_{\zeta}(\zeta)\), we can interpret the Slow SDE as a continuous version of a semi-gradient method for reducing \(\langle\nabla^{2}\mathcal{L}(\zeta),\widehat{\mathbf{\Sigma}}_{\zeta}(\zeta)\rangle\) on \(\Gamma\). Since the Hessian matrix \(\nabla^{2}\mathcal{L}(\zeta)\) determines the local curvature of the loss landscape, we can conclude from the Slow SDE that SGD tends to reduce sharpness and move towards flatter minimizers in \(\mathcal{O}(\eta^{-2})\) steps. Reduced sharpness has been shown to yield better sample complexity bounds in specific theoretical settings. For details, we refer readers to Li et al. (2021).

Now, we turn to the Slow SDE for QSR. Compared with the SDE for SGD, it possesses a \(K\) times larger drift term, leading to much faster sharpness reduction than SGD. An intuitive explanation for why this extra drift arises is as follows. Since the local batch size is \(K\) times smaller than the global one, this local diffusion at each worker is much more significant than that in parallel SGD, thereby leading to an extra drift term in Slow SDE accumulated from higher-order terms.

The case of Local SGD with \(H=\beta/\eta\) is somewhere in between QSR and SGD. Compared with the SDE for SGD, it has an extra drift term (c), where \(\beta\) serves as the knob to control the magnitude of the drift term. For small \(\beta\), \(\widehat{\mathbf{\Psi}}(\mathbf{\zeta})\) diminishes to zero, yielding the same SDE as SGD. By contrast, as \(\beta\) goes to infinity, \(\widehat{\mathbf{\Psi}}(\mathbf{\zeta})\) approximates \(\widehat{\mathbf{\Sigma}}_{\zeta}(\mathbf{\zeta})\), leading to the Slow SDE for QSR.

Comparison of different scalings.Based on the interpretation, keeping \(H\) constant as \(\eta\) diminishes is equivalent to setting a small \(\beta\) for \(H=\beta/\eta\), making the extra drift term negligible and thus yielding nearly no generalization benefit over SGD. Conversely, the SDE for \(H=\beta/\eta\) converges to the SDE of QSR in the limit \(\beta\to\infty\), maximizing the drift term. But in practice, \(\beta\) cannot be arbitrarily large. In Theorem 3.3 of Gu et al. (2023), the distance between the iterate and \(\Gamma\) blows up as \(\widehat{\mathcal{O}}(\sqrt{\beta\eta})\), suggesting that setting a very large \(\beta\) for a not-so-small \(\eta\) can blow up the loss. Therefore, the generalization performance of \(H\)\(\sim\)\(\eta^{-1}\) is expected to be worse than QSR. In summary, the order of generalization performance predicted by our theory is QSR \(>\{H\sim\eta^{-1}\}>\) [constant \(H\)].

Experimental results in Figure 2 validate that this order of generalization performance for different scalings holds not only for Local SGD but also for Local AdamW. For Local SGD we additionally have {constant \(H\)} \(\approx\) [parallel SGD] since parallel SGD is mathematically equivalent to Local SGD with \(H=1\). Apart from \(H\sim\eta^{-1}\) and \(H\sim\eta^{-2}\), we have also tried a more aggressive scaling, \(H\sim\eta^{-3}\), but it does not provide consistent improvements over QSR. See Appendix G for more discussion.

## 4 Experiments

In this section, we empirically demonstrate that QSR not only improves the test accuracy of local gradient methods but also reduces the wall-clock time of standard data parallel training, with a focus on the ImageNet classification task (Russakovsky et al., 2015). Our experiments include Local SGD on ResNet-152 (He et al., 2016), and Local AdamW on ViT-B with patch size 16x16 (Dosovitskiy et al., 2021). We briefly outline our training configuration below. See Appendix C for full details.

Baselines.For QSR with base synchronization period \(H_{\mathrm{base}}\), we benchmark their performance against two baselines running the same number of epochs: \(\mathcal{O}\) Local SGD/AdamW with constant synchronization period \(H=H_{\mathrm{base}}\), and \(\mathcal{Q}\) parallel SGD/AdamW. When comparing with these baselines, we mainly focus on validating that (a) QSR maintains or sometimes outperforms the communication efficiency of \(\mathcal{O}\), thus communicating much less than \(\mathcal{Q}\), and (b) QSR improves the generalization performance of \(\mathcal{O}\), even surpassing \(\mathcal{Q}\) in test accuracy.

Figure 2: Empirical results on Local SGD and Local AdamW validate the generalization performance order predicted by our theory: QSR \(>\{H\sim\eta^{-1}\}>\) [constant \(H\)]. For SGD, we additionally have [constant \(H\)] \(\approx\) [parallel SGD] since the latter is equivalent to Local SGD with \(H=1\). Here, \(\alpha\) and \(\beta\) are tuned to maximize the test accuracy of QSR and \(H\sim\eta^{-1}\), respectively.

Comparison with other synchronization strategies.Besides the above two baselines, other potential baselines include 3 Post-local SGD, 4 the scaling of \(H\sim\eta^{-1}\), and 5 large batch training with batch size \(H\times B\), which we discuss below. 3 is proposed for the same purpose as QSR: to improve communication efficiency and generalization together. However, it is less communication efficient than our QSR because it starts with parallel SGD and sustains this for a significant fraction of the training duration, leading to a limited reduction in communication. Also, as shown by our comparison in Figure 1(a) (also observed in Ortiz et al. 2021), its generalization benefits over SGD appear shortly after switching and diminish in the end. 4 is inspired by Gu et al. (2023) and may also improve generalization while reducing communication, but we have conducted a thorough comparison between QSR and 4 in Figure 2, demonstrating the superiority of QSR. 5 has the same communication efficiency as Local SGD with the same constant \(H\) (4), but it has been observed to have worse test accuracy than parallel SGD/AdamW without scaling up the batch size (4), which we also observe in Table 2. For the above reasons, we mainly compare with baselines 1 and 2.

Hardware.We conduct the experiments on Tencent Cloud, where each machine is equipped with 8 NVIDIA GeForce RTX 3090 GPUs. The machines are interconnected by a 25Gbps network. Since intra-machine communication speed is not substantially faster than inter-machine speed on our specific hardware, we treat _each GPU_ as an independent worker and set the batch size on each GPU as \(B_{\mathrm{loc}}=256\). In this paper, we use _a_x_b GPUs to denote \(a\) machines with \(b\) GPUs each.

Training Setup.Our experiments on ResNet-152 follow the 200-epoch recipe in Foret et al. (2021b) except that we use 5 epochs of linear learning rate warmup. For experiments on ViT-B, we follow the simple and effective 300-epoch recipe proposed in Beyer et al. (2022) with RandAugment and Mixup. We use the cosine decay unless otherwise stated. The hyperparameters (primarily learning rate and weight decay) are optimally tuned for all baselines. We explore \(H_{\mathrm{base}}=2,4\) for ResNet-152 and \(H_{\mathrm{base}}=4,8\) for ViT-B. This choice stems from the observation that the communication overhead for ResNet-152 is smaller than ViT-B (see Table 4). To tune the growth coefficient \(\alpha\) for QSR, we first fix the learning rate schedule and then search among a few values of \(\alpha\). The \(\alpha\) values we explore typically allow the training to start with \(H_{\mathrm{base}}\), maintain \(H=H_{\mathrm{base}}\) for an initial period to optimize the training loss, and gradually increase \(H\) as \(\eta\) decays in the late phase.

### QSR Improves Generalization

Through experiments spanning various batch sizes and learning rate schedules, in this subsection, we illustrate that QSR consistently enhances the generalization of gradient methods, even outperforming the communication-intensive data parallel approach.

Main results.We first present our main results for batch size \(B=4096\) on 2x8 GPUs, covering Local SGD on ResNet-152 and Local AdamW on ViT-B. As shown in Table 1, QSR significantly improves the validation accuracy of local gradient methods by up to \(0.8\%\) on ResNet-152 and \(1.7\%\) on ViT-B, despite inducing higher training loss. The results support the thesis that the improvement in generalization is due to the implicit regularization of local gradient noise instead of better optimization. Noticeably, QSR surpasses the data parallel approach in validation accuracy by \(0.7\%\) on ResNet-152 and by \(1.1\%\) on ViT-B while cutting the communication volume to less than \(25\%\). As an added benefit of increasing the synchronization interval in line with the decaying learning rate, QSR further reduces communication overhead, even halving the communication volume compared to Local AdamW with a fixed synchronization period on ViT-B.

The advantages of QSR are more pronounced for ViT-B compared to ResNet-152. This is probably because vision transformers are general-purpose architectures with less image-specific inductive bias than CNNs (Dosovitskiy et al., 2021; Chen et al., 2021). As a result, they may benefit more from external regularization effects, such as those induced by adding local steps.

\begin{table}

\end{table}
Table 1: QSR enhances the test accuracy of local gradient methods, even outperforming the communication-intensive data parallel approach. The experiments below use batch size 4096. We report the validation accuracy and train loss averaged over 3 runs, along with the standard deviation.

Scaling up the batch size.In Table 2, when scaling the training up to 8x8 GPUs with total batch size \(B=16384\), we observe a drop in test accuracy for both data parallel approach and local gradient methods. This generalization degradation for large batch training, which has been widely observed in the literature (Shallue et al., 2019; Jastrzebski et al., 2017; You et al., 2018), probably arises from a reduced level of gradient noise associated with increased batch size (Keskar et al., 2017; Smith et al., 2021). While the Linear Scaling Rule for SGD (Krizhevsky, 2014; Goyal et al., 2017) and the Square Root Scaling Rule (Malladi et al., 2022; Granziol et al., 2022) for adaptive gradient methods - which increase the learning rate in proportion to the total batch size or its square root - can mitigate this degradation, they cannot fully bridge the gap. In Table 2, the test accuracy drop persists even when we tune the learning rate for all baselines. Applying QSR to local gradient methods can help reduce this generalization gap. It improves the validation accuracy of local gradient methods by up to \(0.6\%\) on ResNet-152 and \(1.5\%\) on ViT-B. This enables local gradient methods to achieve comparable validation accuracy as the data parallel approach on ResNet or outperform it by \(0.8\%\) on ViT while communicating considerably less.

Other learning rate schedules.So far, our experiments are conducted with the cosine learning rate schedule, which is a common choice for training modern deep neural nets (Liu et al., 2021, 2022; Brown et al., 2020). To further validate the efficacy of QSR, we now investigate other popular learning rate schedules, including linear (Li et al., 2020; Izsak et al., 2021; Leclerc et al., 2023) and step decay (He et al., 2016; Huang et al., 2017; Ma et al., 2019). See Figure 4 for a visualization of these schedules. Figure 3 presents the results for Local AdamW on ViT-B with linear decay, where the peak learning rates for baselines are tuned optimally. QSR improves the test accuracy of Local AdamW by a significant margin of \(1.4\%\), even outperforming parallel AdamW by \(0.6\%\) while cutting the communication volume to only \(9.3\%\). The step decay scheduler divides the learning rate by factors such as \(2\) or \(10\) at some specified epochs. Given the absence of standard recipes to determine the decay points in our training setup, we derive a step decay schedule from the cosine decay by rounding its learning rate to powers of \(2\), which is defined as \(\eta_{\text{step}}(t):=2^{\text{round}(\log_{2}\eta_{\text{conv}}(t))}\). As shown in Table 3, QSR exhibits strong generalization performance with this decay schedule, enhancing the test accuracy of local gradient methods by up to \(0.8\%\) on ResNet-152 and \(1.5\%\) on ViT-B. It even surpasses the communication-intensive parallel SGD by \(0.7\%\) on ResNet and parallel AdamW by \(1\%\) on ViT.

### QSR Reduces Wall-clock Time

In addition to improving generalization, our original motivation for adopting local steps is to reduce communication overhead and hence reduce the wall-clock time. In this section, we confirm this

\begin{table}
\begin{tabular}{c c c} \hline \hline Method & Val. Acc.(\%) & Comm. (\%) \\ \hline \hline Parallel SGD & 79.20 & 100 \\ \hline Local SGD (\(H\)=2) & 78.67 & 50 \\ + QSR (\(H_{\mathrm{base}}=2\)) & **79.27** & **42.8** \\ \hline Local SGD (\(H\)=4) & 78.34 & 25 \\ + QSR (\(H_{\mathrm{base}}=4\)) & **78.65** & **21.9** \\ \hline \hline \end{tabular}
\end{table}
Table 2: QSR mitigates the generalization degradation in large-batch training. Here the batch size is \(16384\).

(a) Local SGD on ResNet-152

\begin{table}
\begin{tabular}{c c c} \hline \hline Method & Val. Acc.(\%) & Comm. (\%) \\ \hline \hline Parallel SGD & 79.68 & 100 \\ \hline \hline Local SGD (\(H\)=2) & 79.58 & 50 \\ +QSR (\(H_{\mathrm{base}}=2\)) & **80.40** & **40.3** \\ \hline \hline Local SGD (\(H\)=4) & 79.53 & 25 \\ +QSR (\(H_{\mathrm{base}}=4\)) & **80.11** & **20.5** \\ \hline \hline \end{tabular}
\end{table}
Table 3: QSR also exhibits strong generalization performance on the step-decay learning rate schedule.

(a) Local SGD on ResNet-152.

Figure 3: For linear decay, QSR improves the test accuracy of Local AdamW on ViT-B, even outperforming the communication-intensive parallel AdamW.

for training with 2x8 and 8x8 GPUs, as shown in Table 4. See also Appendix F for our method of measuring the communication time. In our setup, scaling the training from 2x8 to 8x8 GPUs increases the communication overhead for both models. Notably, on 8x8 GPUs, communication accounts for almost half of the total training time for ViT-B. Since communication makes up a larger portion of the total time for ViT-B compared to ResNet-152, the speedup from QSR is more significant on ViT-B: the time is cut from 26.7 to 20.2 hours on 2x8 GPUs, and 8.6 to 5.5 hours on 8x8 GPUs. As discussed in Section 4.1, compared to the constant period local gradient method, QSR further reduces the communication cost by increasing the synchronization period in the late phase. For example, applying QSR to Local AdamW with \(H=4\) further reduces the time by 1 hour for ViT training on 2x8 GPUs.

Discussion on the choice of \(H_{\rm base}\).As elaborated in Section 2, \(H_{\rm base}\) indicates the minimum synchronization period and should be determined based on the communication overhead. For ResNet-152, given that communication only accounts for 3.3 out of 20.7 hours on 2x8 GPUs and 1.3 out of 5.7 hours on 8x8 GPUs, setting \(H_{\rm base}\) as \(2\) or \(4\) suffices to reduce the communication time to an inconsequential amount. By contrast, the communication overhead for ViT-B is more prominent, motivating us to consider larger values of \(H_{\rm base}\), such as 4 and 8. As shown in Tables 1 and 2, \(H_{\rm base}\) introduces a tradeoff between communication efficiency and final test accuracy. For instance, when training ResNet-152 with batch size 16384, one can either choose \(H_{\rm base}=2\) to achieve comparable test accuracy as parallel SGD, or \(H_{\rm base}=4\) to further halve the communication volume at the expense of a \(0.6\%\) drop in test accuracy. One probable explanation for this accuracy drop for larger \(H_{\rm base}\) can be worse optimization in the early training phase, where the learning rate is large.

## 5 Discussions and Future Directions

This paper primarily focuses on relatively large models trained with long horizons, and proposes the Quadratic Synchronization Rule (QSR). As validated by our experiments, QSR effectively improves test accuracy and communication efficiency simultaneously for training large vision models (ResNet-152 and ViT-B) with quite a few hundred epochs. However, on the downside, for smaller models trained with shorter horizons, QSR may not consistently deliver noticeable generalization improvements (see Table 5). Nonetheless, training in this regime is not costly, either, making it less of a critical concern. Another limitation of our work is that the effectiveness of QSR relies on the implicit regularization effects of noise, but when training large models with unsupervised learning on massive data, regularization techniques might not be necessary to bridge the gap between the training and population loss (Vyas et al., 2023). Still, recent work (Liu et al., 2023) has found that the same pretraining loss can lead to different internal representations and thus different downstream performances. We leave it to future work to explore and design communication-efficient methods for unsupervised learning, particularly language model pretraining, that improve models' transferability to downstream tasks.

\begin{table}

\end{table}
Table 4: QSR reduces the wall-clock time of data parallel training. The following tables present wall-clock time for the entire training process on 2x8 GPUs and 8x8 GPUs, with batch sizes 4096 and 16384, respectively. We highlight the wall-clock time of QSR when it matches or outperforms the data parallel baseline in test accuracy. “Ratio” represents communication time divided by total time, reflecting the communication overhead. We also include local gradient methods with a constant synchronization period for reference. (a) ResNet-152 (200 epochs) on 2x8 GPUs (b) ViT-B (300 epochs) on 2x8 GPUs

## References

* Arora et al. (2019) Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d' Alche-Buc, E. Fox, and R. Garnett (eds.), _Advances in Neural Information Processing Systems 32_, pp. 7411-7422. Curran Associates, Inc., 2019.
* Arora et al. (2022) Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on the edge of stability in deep learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pp. 948-1024. PMLR, 17-23 Jul 2022.
* Basu et al. (2019) Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-SGD: Distributed SGD with quantization, sparsification and local computations. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Beyer et al. (2022) Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Better plain vit baselines for imagenet-1k. _arXiv preprint arXiv:2205.01580_, 2022.
* Blanc et al. (2020) Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process. In Jacob Abernethy and Shivani Agarwal (eds.), _Proceedings of Thirty Third Conference on Learning Theory_, volume 125 of _Proceedings of Machine Learning Research_, pp. 483-513. PMLR, 09-12 Jul 2020.
* Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. _CoRR_, abs/2005.14165, 2020. URL [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165).
* Chen and Huo (2016) Kai Chen and Qiang Huo. Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering. In _2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pp. 5880-5884, 2016. doi: 10.1109/ICASSP.2016.7472805.
* Chen et al. (2021) Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets without pre-training or strong data augmentations. In _International Conference on Learning Representations_, 2021.
* Chizat and Bach (2020) Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In Jacob Abernethy and Shivani Agarwal (eds.), _Proceedings of Thirty Third Conference on Learning Theory_, volume 125 of _Proceedings of Machine Learning Research_, pp. 1305-1338. PMLR, 09-12 Jul 2020.
* Cohen et al. (2020) Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In _International Conference on Learning Representations_, 2020.
* Cowsik et al. (2022) Aditya Cowsik, Tankut Can, and Paolo Glorioso. Flatter, faster: scaling momentum for optimal speedup of sgd. _arXiv preprint arXiv:2210.16400_, 2022.
* Damian et al. (2021) Alex Damian, Tengyu Ma, and Jason D Lee. Label noise SGD provably prefers flat global minimizers. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems_, volume 34, pp. 27449-27461. Curran Associates, Inc., 2021.
* Damian et al. (2023) Alex Damian, Eshaan Nichani, and Jason D. Lee. Self-stabilization: The implicit bias of gradient descent at the edge of stability. In _The Eleventh International Conference on Learning Representations_, 2023.
* Damian et al. (2021)Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021.
* Draxler et al. (2018) Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In _International conference on machine learning_, pp. 1309-1318. PMLR, 2018.
* Foret et al. (2021a) Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In _International Conference on Learning Representations_, 2021a.
* Foret et al. (2021b) Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In _International Conference on Learning Representations_, 2021b.
* Frankle et al. (2020) Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In _International Conference on Machine Learning_, pp. 3259-3269. PMLR, 2020.
* Garipov et al. (2018) Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. _Advances in neural information processing systems_, 31, 2018.
* Ge et al. (2021) Rong Ge, Yunwei Ren, Xiang Wang, and Mo Zhou. Understanding deflation process in over-parametrized tensor decomposition. _Advances in Neural Information Processing Systems_, 34, 2021.
* Goyal et al. (2017) Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training imagenet in 1 hour. _arXiv preprint arXiv:1706.02677_, 2017.
* Granziol et al. (2022) Diego Granziol, Stefan Zohren, and Stephen Roberts. Learning rates as a function of batch size: A random matrix theory approach to neural network training. _The Journal of Machine Learning Research_, 23(1):7795-7859, 2022.
* Gu et al. (2023) Xinran Gu, Kaifeng Lyu, Longbo Huang, and Sanjeev Arora. Why (and when) does local SGD generalize better than SGD? In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=svCCui6Dr1](https://openreview.net/forum?id=svCCui6Dr1).
* Gupta et al. (2020) Vipul Gupta, Santiago Akle Serrano, and Dennis DeCoste. Stochastic weight averaging in parallel: Large-batch training that generalizes well. In _International Conference on Learning Representations_, 2020.
* Haddadpour et al. (2019) Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Local SGD with periodic averaging: Tighter analysis and adaptive synchronization. _Advances in Neural Information Processing Systems_, 32, 2019.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 770-778, 2016.
* Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. _Neural computation_, 9(1):1-42, 1997.
* Hu et al. (2017) Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of nonconvex stochastic gradient descent. _arXiv preprint arXiv:1705.07562_, 2017.
* Huang et al. (2017) Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 4700-4708, 2017.
* Ibayashi and Imaizumi (2021) Hikaru Ibayashi and Masaaki Imaizumi. Exponential escape efficiency of SGD from sharp minima in non-stationary regime. _arXiv preprint arXiv:2111.04004_, 2021.
* Ibayashi et al. (2021)P Izmailov, AG Wilson, D Podoprikhin, D Vetrov, and T Garipov. Averaging weights leads to wider optima and better generalization. In _34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018_, pp. 876-885, 2018.
* Izsak et al. (2021) Peter Izsak, Moshe Berchansky, and Omer Levy. How to train bert with an academic budget. _arXiv preprint arXiv:2104.07705_, 2021.
* Jastrzebski et al. (2017) Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in SGD. _arXiv preprint arXiv:1711.04623_, 2017.
* Ji and Telgarsky (2020) Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 17176-17186. Curran Associates, Inc., 2020.
* Jiang et al. (2020) Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In _International Conference on Learning Representations_, 2020.
* Jin et al. (2023) Jikai Jin, Zhiyuan Li, Kaifeng Lyu, Simon Shaolei Du, and Jason D. Lee. Understanding incremental learning of gradient descent: A fine-grained analysis of matrix sensing. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 15200-15238. PMLR, 23-29 Jul 2023.
* Kairouz et al. (2021) Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* Kamp et al. (2014) Michael Kamp, Mario Boley, Daniel Keren, Assaf Schuster, and Izchak Sharfman. Communication-efficient distributed online prediction by dynamic model synchronization. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part I 14_, pp. 623-639. Springer, 2014.
* Karimireddy et al. (2020) Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_, pp. 5132-5143. PMLR, 2020.
* Keskar et al. (2017a) Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations_, 2017a.
* Keskar et al. (2017b) Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations_, 2017b.
* Khaled et al. (2020) Ahmed Khaled, Konstantin Mishchenko, and Peter Richtarik. Tighter theory for local SGD on identical and heterogeneous data. In _International Conference on Artificial Intelligence and Statistics_, pp. 4519-4529. PMLR, 2020.
* Kleinberg et al. (2018) Bobby Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does SGD escape local minima? In Jennifer Dy and Andreas Krause (eds.), _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pp. 2698-2707. PMLR, 10-15 Jul 2018.
* Konecny et al. (2016) Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. _arXiv preprint arXiv:1610.05492_, 2016.
* Krizhevsky (2014) Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. _arXiv preprint arXiv:1404.5997_, 2014.
* Krizhevsky et al. (2014)Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander Madry. ffcv. [https://github.com/libffcv/ffcv/](https://github.com/libffcv/ffcv/), 2022.
* Leclerc et al. (2023) Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander Madry. Ffcv: Accelerating training by removing data bottlenecks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 12011-12020, June 2023.
* Li et al. (2022) Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, and Yuxiong He. 1-bit lamb: communication efficient large-scale large-batch training with lamb's convergence speed. In _2022 IEEE 29th International Conference on High Performance Computing, Data, and Analytics (HiPC)_, pp. 272-281. IEEE, 2022.
* Li et al. (2020a) Mengtian Li, Ersin Yumer, and Deva Ramanan. Budgeted training: Rethinking deep neural network training under resource constraints. In _International Conference on Learning Representations_, 2020a.
* Li et al. (2019a) Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations. _Journal of Machine Learning Research_, 20(40):1-47, 2019a.
* Li et al. (2020b) Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. _Proceedings of Machine learning and systems_, 2:429-450, 2020b.
* Li et al. (2019b) Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. In _International Conference on Learning Representations_, 2019b.
* Li et al. (2018) Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), _Proceedings of the 31st Conference On Learning Theory_, volume 75 of _Proceedings of Machine Learning Research_, pp. 2-47. PMLR, 06-09 Jul 2018.
* Li et al. (2021a) Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning. In _International Conference on Learning Representations_, 2021a.
* Li et al. (2021b) Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling SGD with stochastic differential equations (sdes). _Advances in Neural Information Processing Systems_, 34:12712-12725, 2021b.
* Li et al. (2021c) Zhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after SGD reaches zero loss?-a mathematical framework. In _International Conference on Learning Representations_, 2021c.
* Lin et al. (2020) Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, and Martin Jaggi. Don't use large mini-batches, use Local SGD. In _International Conference on Learning Representations_, 2020.
* Liu et al. (2023) Hong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. Same pre-training loss, better downstream: Implicit bias matters for language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 22188-22214. PMLR, 23-29 Jul 2023.
* Liu et al. (2021) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pp. 10012-10022, 2021.
* Liu et al. (2022) Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 11976-11986, 2022.
* Lyu and Li (2020) Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In _International Conference on Learning Representations_, 2020.
* Li et al. (2021)Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on two-layer nets: Margin maximization and simplicity bias. _Advances in Neural Information Processing Systems_, 34, 2021.
* Lyu et al. (2022) Kaifeng Lyu, Zhiyuan Li, and Sanjeev Arora. Understanding the generalization benefit of normalization layers: Sharpness reduction, 2022.
* Ma & Ying (2021) Chao Ma and Lexing Ying. On linear stability of SGD and input-smoothness of neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems_, volume 34, pp. 16805-16817. Curran Associates, Inc., 2021.
* Ma et al. (2022) Chao Ma, Daniel Kunin, Lei Wu, and Lexing Ying. Beyond the quadratic approximation: The multiscale structure of neural network loss landscapes. _Journal of Machine Learning_, 1(3):247-267, 2022. ISSN 2790-2048.
* Ma et al. (2019) Wei-Chiu Ma, Shenlong Wang, Rui Hu, Yuwen Xiong, and Raquel Urtasun. Deep rigid instance scene flow. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 3614-3622, 2019.
* Malladi et al. (2022) Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora. On the SDEs and scaling rules for adaptive gradient algorithms. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), _Advances in Neural Information Processing Systems_, 2022.
* Mann et al. (2009) Gideon Mann, Ryan T. McDonald, Mehryar Mohri, Nathan Silberman, and Dan Walker. Efficient large-scale distributed training of conditional maximum entropy models. In _Advances in Neural Information Processing Systems 22_, pp. 1231-1239, 2009.
* McMahan et al. (2017) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pp. 1273-1282. PMLR, 2017.
* Nacson et al. (2019) Mor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pp. 4683-4692, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
* Nadiradze et al. (2021) Giorgi Nadiradze, Amirmojtaba Sabour, Peter Davies, Shigang Li, and Dan Alistarh. Asynchronous decentralized sgd with quantized and local updates. _Advances in Neural Information Processing Systems_, 34:6829-6842, 2021.
* Neyshabur et al. (2017) Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring generalization in deep learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* Ortiz et al. (2021) Jose Javier Gonzalez Ortiz, Jonathan Frankle, Mike Rabbat, Ari Morcos, and Nicolas Ballas. Tradeoffs of Local SGD at scale: An empirical study. _arXiv preprint arXiv:2110.08133_, 2021.
* Povey et al. (2014) Daniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur. Parallel training of dnns with natural gradient and parameter averaging. _arXiv preprint arXiv:1410.7455_, 2014.
* Razin & Cohen (2020) Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 21174-21187. Curran Associates, Inc., 2020.
* Razin et al. (2022) Noam Razin, Asaf Maman, and Nadav Cohen. Implicit regularization in hierarchical tensor factorization and deep convolutional neural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pp. 18422-18462. PMLR, 17-23 Jul 2022.
* Razin et al. (2021)Sashank J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In _International Conference on Learning Representations_, 2020.
* Russakovsky et al. (2015) Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. _International Journal of Computer Vision (IJCV)_, 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
* Shallue et al. (2019) Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E. Dahl. Measuring the effects of data parallelism on neural network training. _Journal of Machine Learning Research_, 20(112):1-49, 2019.
* Shen et al. (2021) Shuheng Shen, Yifei Cheng, Jingchang Liu, and Linli Xu. Stl-sgd: Speeding up local sgd with stage-wise communication period. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pp. 9576-9584, 2021.
* Smith et al. (2020) Samuel Smith, Erich Elsen, and Soham De. On the generalization benefit of noise in stochastic gradient descent. In Hal Daume III and Aarti Singh (eds.), _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pp. 9058-9067. PMLR, 13-18 Jul 2020.
* Smith et al. (2021) Samuel L Smith, Benoit Dherin, David Barrett, and Soham De. On the origin of implicit regularization in stochastic gradient descent. In _International Conference on Learning Representations_, 2021.
* Soudry et al. (2018a) Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. _Journal of Machine Learning Research_, 19(70):1-57, 2018a.
* Soudry et al. (2018b) Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. In _International Conference on Learning Representations_, 2018b.
* Stich (2018) Sebastian U Stich. Local SGD converges fast and communicates little. In _International Conference on Learning Representations_, 2018.
* Stoger and Soltanolkotabi (2021) Dominik Stoger and Mahdi Soltanolkotabi. Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction. _Advances in Neural Information Processing Systems_, 34, 2021.
* Su and Chen (2015) Hang Su and Haoyu Chen. Experiments on parallel training of deep neural network using model averaging. _arXiv preprint arXiv:1507.01239_, 2015.
* Tang et al. (2021) Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He. 1-bit adam: Communication efficient large-scale training with adam's convergence speed. In _Proceedings of the 38th International Conference on Machine Learning_, 2021.
* Vyas et al. (2023) Nikhil Vyas, Depen Morwani, Rosie Zhao, Gal Kaplun, Sham Kakade, and Boaz Barak. Beyond implicit bias: The insignificance of sgd noise in online learning. _arXiv preprint arXiv:2306.08590_, 2023.
* Wang and Joshi (2019) Jianyu Wang and Gauri Joshi. Adaptive communication strategies to achieve the best error-runtime trade-off in local-update SGD. _Proceedings of Machine Learning and Systems_, 1:212-229, 2019.
* Wang and Joshi (2021) Jianyu Wang and Gauri Joshi. Cooperative SGD: A unified framework for the design and analysis of local-update SGD algorithms. _Journal of Machine Learning Research_, 22(213):1-50, 2021.
* Wang et al. (2019) Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat. Slowmo: Improving communication-efficient distributed SGD with slow momentum. In _International Conference on Learning Representations_, 2019.
* Wang et al. (2023) Runzhe Wang, Sadhika Malladi, Tianhao Wang, Kaifeng Lyu, and Zhiyuan Li. The marginal value of momentum for small learning rate sgd. _arXiv preprint arXiv:2307.15196_, 2023.
* Wang et al. (2019)Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan, Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? In _International Conference on Machine Learning_, pp. 10334-10343. PMLR, 2020.
* Wortsman et al. (2022) Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In _International Conference on Machine Learning_, pp. 23965-23998. PMLR, 2022.
* Wortsman et al. (2023) Mitchell Wortsman, Suchin Gururangan, Shen Li, Ali Farhadi, Ludwig Schmidt, Michael Rabbat, and Ari S. Morcos. lo-fi: distributed fine-tuning without communication. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.
* Wu et al. (2018) Lei Wu, Chao Ma, and Weinan E. How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* Xie et al. (2021) Zeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima. In _International Conference on Learning Representations_, 2021.
* Xu et al. (2023) Hang Xu, Wenxuan Zhang, Jiawei Fei, Yuzhe Wu, Tingwen Xie, Jun Huang, Yuchen Xie, Mohamed Elhoseiny, and Panos Kalnis. SLAMB: Accelerated large batch training with sparse communication. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* You et al. (2018) Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. Imagenet training in minutes. In _Proceedings of the 47th International Conference on Parallel Processing_, pp. 1-10, 2018.
* You et al. (2020) Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training BERT in 76 minutes. In _International Conference on Learning Representations_, 2020.
* Yu et al. (2019) Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pp. 5693-5700, 2019.
* Zhang et al. (2017) Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In _International Conference on Learning Representations_, 2017.
* Zhang et al. (2014) Xiaohui Zhang, Jan Trmal, Daniel Povey, and Sanjeev Khudanpur. Improving deep neural network acoustic models using generalized maxout networks. In _2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pp. 215-219, 2014. doi: 10.1109/ICASSP.2014.6853589.
* Zhou and Cong (2018) Fan Zhou and Guojing Cong. On the convergence properties of a k-step averaging stochastic gradient descent algorithm for nonconvex optimization. In _Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18_, pp. 3219-3227. International Joint Conferences on Artificial Intelligence Organization, 7 2018. doi: 10.24963/ijcai.2018/447. URL [https://doi.org/10.24963/ijcai.2018/447](https://doi.org/10.24963/ijcai.2018/447).
* Zhu et al. (2023) Tongtian Zhu, Fengxiang He, Kaixuan Chen, Mingli Song, and Dacheng Tao. Decentralized SGD and average-direction SAM are asymptotically equivalent. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 43005-43036. PMLR, 23-29 Jul 2023.
* Zhu et al. (2018)Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pp. 7654-7663. PMLR, 09-15 Jun 2019.
* Zinkevich et al. (2010) Martin Zinkevich, Markus Weimer, Lihong Li, and Alex Smola. Parallelized stochastic gradient descent. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta (eds.), _Advances in Neural Information Processing Systems_, volume 23. Curran Associates, Inc., 2010.

###### Contents

* 1 Introduction
* 2 Our Method: Quadratic Synchronization Rule
* 3 Theoretical Motivations of Quadratic Synchronization Rule
* 4 Experiments
	* 4.1 QSR Improves Generalization
	* 4.2 QSR Reduces Wall-clock Time
* 5 Discussions and Future Directions
* A Additional Related Works
* B PseudoCode
* C Experimental Details
* C.1 Training Details for ResNet-152
* C.2 Training Details For ViT-B
* D Additional Experiments on ResNet-50
* E Supplementary Materials for Section 3
* E.1 Missing Definitions and Assumptions
* E.2 Proof for Theorem 3.1
* F Details for Communication Time Measurement
* G Discussion on More Aggressive Scalings
* H Comparison with Local SGD/AdamW + SWAP

Additional Related Works

Advances in local gradient methods.Local gradient methods are a class of communication-efficient algorithms for distributed training. In this approach, workers update their models locally and average the model parameters every time they finish \(H\) steps of updates. Dating back to Mann et al. (2009) and Zinkevich et al. (2010), local gradient methods have been widely used to improve communication efficiency in both datacenter distributed training Zhang et al. (2014); Povey et al. (2014); Su & Chen (2015); Chen & Huo (2016) and Federated Learning (Kairouz et al., 2021; McMahan et al., 2017; Li et al., 2019; Konecny et al., 2016). Many variants have been proposed to facilitate the convergence speed. Examples include using control variates (Karimireddy et al., 2020), adding proximal terms to local loss functions (Li et al., 2020), and applying adaptivity on top of each communication round (Wang et al., 2019; Reddi et al., 2020). Local gradient methods can also be readily combined with orthogonal approaches like communication compression (Basu et al., 2019) and asynchronous updates (Nadiradze et al., 2021) for further communication cost reduction.

Optimization perspectives on selecting \(H\).Extensive prior research has been devoted to optimizing the selection of the synchronization period \(H\) from an optimization perspective. The conventional approach sets \(H\) as a constant throughout training. In this setup, a series of studies (e.g.,Khaled et al. (2020); Stich (2018); Haddadpour et al. (2019); Yu et al. (2019)) established convergence bounds for the training loss, which typically degrade as \(H\) gets larger. leading to a trade-off between communication efficiency and model accuracy. Drawing upon these theoretical results, \(H\) should be set as the smallest value that reduces the communication cost to an acceptable level to minimize the negative impact on optimization. To better trade-off between optimization and generalization, researchers introduced various adaptive communication strategies. Kamp et al. (2014) designed a synchronization protocol controlled by the variance in model parameters. Haddadpour et al. (2019) suggested linearly increasing \(H\) as the iteration goes on. Shen et al. (2021) introduced a stagewise communication scheme that halves the learning rate \(\eta\) while doubles \(H\) every time the training has finished a predefined stage. Aimed at optimizing the convergence of training loss with respect to wall-clock time, Wang & Joshi (2019) proposed a strategy that starts with infrequent communication and gradually decreases \(H\) as training progresses. Nonetheless, the effectiveness of these adaptive communication strategies has only been empirically validated on linear models or small-scale datasets like CIFAR-10/100.

Generalization perspectives on selecting \(H\).While a larger \(H\) usually hurts optimization, it can sometimes improve generalization. Apart from Lin et al. (2020) that has been discussed in detail in Section 1, similar observations have been reported by Gupta et al. (2020) and Wortsman et al. (2023). Specifically, Gupta et al. (2020) introduced the Stochastic Weight Averaging in Parallel (SWAP) algorithm, which runs parallel SGD until a target training accuracy, then lets workers perform local updates with a final model averaging. Their empirical results validate SWAP's superior generalization performance over parallel SGD. When using LAMB (You et al., 2020) as the optimizer, Wortsman et al. (2023) find that complete local fine-tuning, followed by a single model averaging in the end (equivalent to setting \(H\) as the total number of iterations), outperforms the standard parallel LAMB in test accuracy under distribution shifts. Another relevant method is the "model soup" (Wortsman et al., 2022), which averages multiple models fine-tuned with different hyperparameters and turns out to beat the single model in test accuracy. Our paper focuses on designing the synchronization scheme best for generalization.

Implicit bias of optimizers.The success of deep learning lies in its remarkable ability to generalize to unseen data, though it possesses the capacity to fit randomly labeled data (Zhang et al., 2017). A significant contributing factor to this success is the implicit bias inherent in popular optimizers like Gradient Descent (GD) and Stochastic Gradient Descent (SGD). Specifically, these optimizers favor minima that exhibit good generalization, without explicitly encoding such bias into the training loss. A lot of studies have been devoted to characterizing this implicit bias, some through the lens of margin maximization (Soudry et al., 2018, 2018; Lyu & Li, 2020; Ji & Telgarsky, 2020; Chizat & Bach, 2020; Nacson et al., 2019), and some others focus on the simplicity bias from small initialization (Li et al., 2018; Razin & Cohen, 2020; Arora et al., 2019; Li et al., 2021; Lyu et al., 2021; Razin et al., 2022; Stoger & Soltanolkotabi, 2021; Ge et al., 2021; Jin et al., 2023). The line of work most closely related to our paper interprets the implicit bias via sharpness reduction. The connection between flatter minima and better generalization is a commonly held belief that has been investigated both theoretically (Hochreiter & Schmidhuber, 1997; Neyshabur et al., 2017) and empirically (Keskar et al., 2017; Jiang et al., 2020). Drawing on this insight, Foret et al. (2021) introduced SAM optimizer, which delivers superior generalization performance by explicitly penalizing sharpness. Recent theoretical studies (Arora et al., 2022; Lyu et al., 2022; Damian et al., 2023; Ma et al., 2022) elucidate that GD inherently biases towards flatter regions on the loss landscape. Specifically, under some regularity conditions, they show that GD will eventually enter the "Edge of Stability"(Cohen et al., 2020), where the maximum eigenvalue of the loss Hessian stays around \(2\)/learning rate, and then constantly moves towards flatter minima. Going beyond GD, another line of work studies how gradient noise in SGD helps reduce sharpness. Wu et al. (2018); Hu et al. (2017); Ma and Ying (2021) showed that gradient noise can cause training instability around sharp minima, and hence, the iterate can only settle around flat minima. Kleinberg et al. (2018); Zhu et al. (2019); Xie et al. (2021); Ibayashi and Imaizumi (2021) analyzed the escaping behavior of SGD from sharp minima. Motivated by recent empirical observations that low-loss solutions on the loss landscape are path-connected (Garipov et al., 2018; Draxler et al., 2018; Frankle et al., 2020) rather than isolated, Blanc et al. (2020); Damian et al. (2021); Li et al. (2021c) assume the existence of a minimizer manifold and show that gradient noise provably drives the iterate towards flatter minima on this manifold. Cowsik et al. (2022); Wang et al. (2023) discuss how momentum preserves or strengthens this effect. Also through the lens of sharpness reduction, the recent work by Gu et al. (2023) explains the generalization benefit of Local SGD, as discussed in Section 3. Zhu et al. (2023) elucidate that a similar implicit bias also manifests in decentralized training by making connections to certain variants of SAM.

PseudoCode

We present the pseudocodes for standard data parallel methods and local gradient methods below.

```
1Input: loss function \(\ell(\mathbf{\theta};\xi)\), initial parameter \(\mathbf{\theta}^{(0)}\)
2Hyperparameters: total number of iterations \(T\)
3Hyperparameters: learning rate schedule \(\eta_{t}\), \(t\in\{0,\cdots,T\}\), local batch size \(B_{\text{loc}}\)
4\(t\gets 0\) ; // initialize the global iteration number
5for\(t=0,\ldots,R-1\)do
6foreach worker \(k\)doin parallel
7\((\xi^{(s)}_{k,t,1},\ldots,\xi^{(s)}_{k,t,B_{\text{loc}}})\leftarrow\texttt{Sample ()}\) ; // sample a local batch
8\(\mathbf{g}^{(t)}_{k}\leftarrow\frac{1}{B_{\text{loc}}}\sum_{i=1}^{B_{\text{loc}}} \nabla\ell(\mathbf{\theta}^{(t)};\xi^{(t)}_{k,i})\) ; // computing the local gradient
9
10 end for
11\(\mathbf{g}^{(t)}\leftarrow\frac{1}{K}\sum_{k=1}^{K}\mathbf{g}^{(t)}_{k}\) ; // All-Reduce aggregation of local gradients
12\(\mathbf{\theta}^{(t+1)}\leftarrow\texttt{OPT}(\mathbf{\theta}^{(t)},\eta_{t},\mathbf{g}^ {(t)})\) ; // update the model with optimizer OPT
13
14 end for
```

**Algorithm 1**Parallel OPT: Data Parallel Methods on \(K\) Workers

```
1Input: loss function \(\ell(\mathbf{\theta};\xi)\), initial parameter \(\mathbf{\theta}^{(0)}\)
2Hyperparameters: total number of rounds \(R\)
3Hyperparameters: learning rate schedule \(\eta_{t}\), \(t\in\{0,\cdots,T\}\), local batch size \(B_{\text{loc}}\)
4\(t\gets 0\) ; // initialize the global iteration number
5for\(s=0,\ldots,R-1\)do
6\(H^{(s)}\leftarrow\texttt{GetH}(s)\) ; // get synchronization period for the current round
7foreach worker \(k\)doin parallel
8\(\mathbf{\theta}^{(s)}_{k,0}\leftarrow\bar{\mathbf{\theta}}^{(0)}\) ; // maintain a local copy of the global model parameter
9for\(h=0,\ldots,H^{(s)}-1\)do
10\((\xi^{(s)}_{k,h,1},\ldots,\xi^{(s)}_{k,h,B_{\text{loc}}})\leftarrow\texttt{Sample ()}\) ; // sample a local batch
11\(g^{(s)}_{k,h}\leftarrow\frac{1}{B_{\text{loc}}}\sum_{i=1}^{B_{\text{loc}}} \nabla\ell(\mathbf{\theta}^{(s)}_{k,h};\xi^{(s)}_{k,h,i})\) ; // computing the local gradient
12\(\mathbf{\theta}^{(s)}_{k,h+1}\leftarrow\texttt{OPT}(\mathbf{\theta}^{(s)}_{k,h},\eta_{t+ h},\mathbf{g}^{(s)}_{k,h})\) ; // update the local model with optimizer OPT
13
14 end for
15\(\bar{\mathbf{\theta}}^{(s+1)}\leftarrow\frac{1}{K}\sum_{k=1}^{K}\mathbf{\theta}^{(s)}_{k, H^{(s)}}\) ; // All-Reduce aggregation of local model parameters
16\(t\gets t+H^{(s)}\) ; // update the global iteration number
17
18 end for
```

**Algorithm 2**Local OPT: Local Gradient Methods on \(K\) Workers

Sampling local batches.In Algorithms 1 and 2, \(\texttt{Sample()}\) returns a local batch for each worker. In our experiments, local batches are sampled without replacement at each epoch, which is standard for distributed training (Goyal et al., 2017; Lin et al., 2020; Ortiz et al., 2021). More specifically, at the beginning of each epoch, all the workers use the same random seed to draw a shared random permutation of train data points, and partition the data points evenly among the \(K\) workers. Then at each local step of each worker, \(\texttt{Sample()}\) sequentially takes samples from its own partition. Once there are too few remaining samples to form a complete batch, a new permutation is sampled and a new epoch starts. For our theoretical analysis, following Gu et al. (2023), we assume \(\texttt{Sample()}\) takes samples with replacement, i.e., the \(K\) workers are taking i.i.d. samples from the globally shared dataset/distribution. See Appendix B in Gu et al. (2023) for pseudocodes of sampling with and without replacement.

Setting synchronization periods.In Algorithm 2, \(\texttt{GetH}(s)\) is a function that returns the synchronization period \(H^{(s)}\) for the current round. Conventionally, \(H^{(s)}\) is chosen as a fixed value, so \(\texttt{GetH}(s)\) always returns a constant. In this paper, we study how \(H^{(s)}\) should change as training goes on, e.g., in QSR, \(\texttt{GetH}(s)\) works as specified in Section 2.

## Appendix C Experimental Details

This section lists the additional experimental details omitted in the main text.

**Software and platform.** We use Pytorch Distributed with NCCL backend to support multinode distributed training and use FFCV Leclerc et al. (2022) to accelerate data loading of ImageNet.

**Sampling scheme.** We employ the "sampling without replacement" scheme, as described in Appendix B.

### Training Details for ResNet-152

We generally follow the recipe in Foret et al. (2021) to train ResNet-152. Specifically, we set the momentum as \(0.9\) and the weight decay \(\lambda\) as \(0.0001\). For data augmentation, we employ random resized crop and random horizontal flip. We additionally use label smoothing \(0.1\). We adopt a local batch size \(B_{\mathrm{loc}}=256\) through \(8\) gradient accumulations. Therefore, the batch size for BatchNorm is \(32\). This choice stems from our observation that a smaller batch size for BatchNorm enhances the test accuracy of parallel SGD. Since the BatchNorm statistics on each worker are estimated on the local model parameter, we pass 100 batches, each of size 32, to estimate the BatchNorm statistics on the global parameter before evaluation.

**Training details for batch size 4096.** We search the optimal peak learning rate \(\eta_{\max}\) of the cosine learning rate schedule among \(\{0.4,0.8,1.6\}\) for all baseline algorithms, i.e., parallel SGD and Local SGD with constant synchronization period \(H=2\) and \(H=4\). The learning rate yielding the highest final test accuracy is selected. We find that \(\eta_{\max}=0.8\) is optimal for all the baseline algorithms. For QSR with \(H_{\mathrm{base}}=2\) and \(H_{\mathrm{base}}=4\), we directly set \(\eta_{\max}=0.8\). We search \(\alpha\) among \(\{0.2,0.25,0.3\}\), and choose \(\alpha=0.2\) and \(0.25\) for QSR with \(H_{\mathrm{base}}=2\) and \(4\) respectively. Regarding other communication strategies in Figure 1(a), we set the switching point at epoch 100 and employ \(H=8\) for Post-local SGD. For \(H=\beta/\eta\), we search \(\beta\) among \(\{0.6,0.8,1,1.2\}\), finally selecting \(\beta=1\).

**Training details for batch size 16384.** The hyperparameter tuning procedure for \(B=16384\) is similar to that of \(B=4096\). We search \(\eta_{\max}\) among \(\{0.8,1.6,3.2\}\) for all baseline algorithms, including SGD and Local SGD with constant synchronization period \(H_{\mathrm{base}}=2\) and \(H_{\mathrm{base}}=4\). We find that \(\eta_{\max}=3.2\) yields the highest final test accuracy for all of them. However, for QSR, we find that peak learning rate \(\eta_{\max}=3.2\) is excessively large, causing the dynamic scheduling to be triggered too late in the training process. This late triggering leaves insufficient training time for the training to fully leverage the generalization benefits introduced by local steps. Consequently, we set \(\eta_{\max}=1.6\) for QSR with \(H_{\mathrm{base}}=2\) and \(4\). We search \(\alpha\) among \(\{0.2,0.25,0.3\}\), and choose \(\alpha=0.2\) for both QSR with \(H_{\mathrm{base}}=2\) and \(4\).

**Training details for the step decay scheduler.** In our experiments with step decay, we employ a batch size of 4096. Given that our step decay scheduler is derived from the cosine decay, we only need to specify the weight decay \(\lambda\), and peak learning rate \(\eta_{\max}\). These are set identically to the values used in our cosine decay experiments. For QSR, we search the growth coefficient \(\alpha\) among \(\{0.2,0.3\}\) and choose \(0.2\) for both \(H_{\mathrm{base}}=2\) and \(4\).

**Training details for experiments in Appendix H.** For Local SGD + SWAP experiments in Appendix H, we use the cosine learning rate schedule with peak learning rate \(\eta_{\max}=0.8\). We start with Local SGD with a constant synchronization period \(H=4\) and explore the switching point \(t_{0}\) from \(\{175,180,185,190\}\).

Figure 4: A visualization of the learning rate schedules we investigate.

### Training Details For ViT-B

For training ViT-B, we primarily follow the 300-epoch recipe proposed by Beyer et al. (2022). Specifically, we replace the [cls] token of the original ViT token with global average pooling and use fixed 2D sin-cos position rather than learned positional embeddings. Our implementation of the model architecture follows the high-starred repository 3 by Phil Wang. Apart from random resized crop and random horizontal flip, we employ RandAugment with parameters (2, 10) and MixUp with a coefficient of 0.2 for data augmentation. Different from Beyer et al. (2022), we use a larger batch size (\(B=4096\) or \(16384\) as opposed to their \(1024\)) and use AdamW instead of Adam.

Footnote 3: [https://github.com/lucidrains/vit-pytorch](https://github.com/lucidrains/vit-pytorch)

As for gradient clipping, we set it as \(1\) for standard AdamW following Beyer et al. (2022); Dosovitskiy et al. (2021) and Chen et al. (2021). However, for Local AdamW, the smaller batch size locally leads to larger gradient noise and, hence larger gradient norm for local updates. This calls for an increase in the gradient clipping threshold. We find that the training process remains stable even when we remove gradient clipping (equivalent to setting the clipping threshold to \(+\infty\)) for most of the hyperparameter configurations we tested. For ease of tuning, we choose to turn off gradient clipping for Local AdamW unless otherwise stated.

Training details for batch size 4096.We use 10k iterations for learning rate warmup following (Beyer et al., 2022; Dosovitskiy et al., 2021; Chen et al., 2021). For parallel AdamW and Local AdamW (\(H=4\)), we explore combinations of \(\eta_{\max}\) and weight decay \(\lambda\) from the grid \(\{0.05,0.1\}\times\{0.004,0.008,0.016\}\). To optimize the final test accuracy, we select \(\eta_{\max}=0.008,\lambda=0.1\) for parallel AdamW and \(\eta_{\max}=0.008,\lambda=0.05\) for Local AdamW (\(H=4\)). For Local AdamW (\(H=8\)), keeping \(\lambda=0.05\), we conduct a grid search for \(\eta_{\max}\) among \(\{0.004,0.008,0.016\}\) and choose \(\eta_{\max}=0.008\). For QSR with \(H_{\rm base}=4\) and \(8\), we directly use \(\eta_{\max}=0.008\) and \(\lambda=0.05\). To optimize \(\alpha\), we search among \(\{0.015,0.0175,0.02\}\) and find \(\alpha=0.0175\) works best for both QSR with \(H_{\rm base}=4\) and \(8\). Regarding the communication strategy of \(H=\beta/\eta\) in Figure 1(b), we explore \(\beta\) among \(\{0.025,0.03,0.035,0.04\}\), settling on \(\beta=0.03\). In Figure 5, we also visualize the \(H\) schedule for Local AdamW with a constant synchronization period and with QSR.

Training details for batch size 16384.To keep the same portion of the total budget for learning rate warmup as \(B=4096\), we set the warmup iterations to 2.5k. We set \(\lambda\) as \(0.1\) and \(0.05\) for parallel AdamW and Local AdamW, respectively. We search for the optimal \(\eta_{\max}\) among \(\{0.004,0.008,0.016\}\) and select \(\eta_{\max}=0.004\) for parallel AdamW, \(\eta_{\max}=0.016\) for Local AdamW with \(H=4\) and \(8\). We adopt the same \(\lambda\) and \(\eta_{\max}\) as Local AdamW for QSR. For QSR with \(H_{\rm base}=4\), we search for the optimal \(\alpha\) among \(\{0.015,0.0175,0.02\}\) and choose \(\alpha=0.0175\). For QSR with \(H_{\rm base}=8\), we search for the optimal \(\alpha\) among \(\{0.01,0.0175\}\), finally picking \(\alpha=0.01\).

Training details for linear and step decay schedulers.For both step and linear decay schedulers, we employ a batch size of 4096. For the step decay scheduler, the peak learning rate \(\eta_{\max}\) and weight decay \(\lambda\) are set identically to the values used in our cosine decay experiments. We search the growth coefficient \(\alpha\) for QSR among \(\{0.015,0.0175\}\) and choose \(0.015\) for both \(H_{\rm base}=4\) and \(8\). For linear decay, we use the same weight decay as our cosine decay experiments. We explore \(\eta_{\max}\) values from \(\{0.004,0.008,0.016\}\) for baselines, finally picking \(\eta_{\max}=0.008\) for parallel AdamW and \(\eta_{\max}=0.016\) for Local AdamW. For QSR, we adopt the same \(\eta_{\max}\) and \(\alpha\) as in our cosine decay experiments. Additionally, we add a gradient clipping threshold of \(4\) for Local AdamW with a constant synchronization period to stabilize training.

Training details for experiments in Appendix G.For the experiments in Table 6, we employ the same weight decay \(\lambda\) and peak learning rate \(\eta_{\max}\) as used in the cosine schedule. Specifically,

Figure 5: A visualization of the \(H\) schedule for Local AdamW with a constant synchronization period \(H=4\) and with QSR \(H_{\rm base}=4,\alpha=0.0175\). The corresponding learning rate schedule is cosine decay with a peak learning rate of \(0.008\). Adopting QSR improves the top-1 validation accuracy of Local AdamW on ViT-B from \(79.32\%\) to \(80.98\%\)..

[MISSING_PAGE_FAIL:24]

Supplementary Materials for Section 3

### Missing Definitions and Assumptions

For a function \(F:\mathbb{R}^{d}\to\mathbb{R}^{d}\), we use \(\partial F(\mathbf{\theta})\) to denote its Jacobian at \(\mathbf{\theta}\) and use \(\partial^{2}F(\mathbf{\theta})\) to denote the second order derivative at \(\mathbf{\theta}\). For any matrix \(\mathbf{M}\in\mathbb{R}^{d\times d}\), \(\partial^{2}F(\mathbf{\theta})[\mathbf{M}]=\sum_{i\in[d]}(\frac{\partial^{2}F_{i}}{ \partial\mathbf{\theta}^{2}},\mathbf{M})\mathbf{e}_{i}\) where \(\mathbf{e}_{i}\) is the \(i\)-th vector of the standard basis. For convenience, we write \(\partial^{2}(\nabla\mathcal{L})(\mathbf{\theta})[\mathbf{M}]\) as \(\nabla^{3}\mathcal{L}(\mathbf{\theta})[\mathbf{M}]\).

**Assumption E.1**.: _Following Gu et al. (2023), we assume that \(\mathcal{L}(\mathbf{\theta})\) and \(\mathbf{\Sigma}(\mathbf{\theta})^{1/2}\) are \(\mathcal{C}^{\infty}\)-smooth on \(\mathbb{R}^{d}\). We also assume that \(\|\nabla\ell(\mathbf{\theta};\xi)\|_{2}\) is uniformly bounded for all \(\mathbf{\theta}\) and \(\xi\)._

**Assumption E.2**.: \(\Gamma\) _is a \(\mathcal{C}^{\infty}\)-smooth, \((d-m)\)-dimensional compact submanifold of \(\mathbb{R}^{d}\) such that any \(\mathbf{\zeta}\in\Gamma\) is a local minimizer of \(\mathcal{L}\) and \(\mathrm{rank}(\nabla^{2}\mathcal{L}(\mathbf{\zeta}))=m\). Additionally, there exists an open neighborhood \(U\) of \(\Gamma\) such that \(\Gamma=\mathrm{argmin}_{\mathbf{\theta}\in U}\mathcal{L}(\mathbf{\theta})\)._

Assumption E.2 is motivated by recent empirical observations that low-loss solutions on the loss landscape are not isolated but path-connected (Garipov et al., 2018; Draxler et al., 2018; Frankle et al., 2020). It is also adopted by Li et al. (2021); Lyu et al. (2022); Gu et al. (2023).

**Definition E.1** (Gradient Flow Projection).: _Fix \(\mathbf{\theta}_{\mathrm{null}}\notin\Gamma\). For \(\mathbf{x}\in\mathbb{R}^{d}\), the gradient flow starting from \(\mathbf{x}\) is the solution to \(\frac{\mathrm{d}\mathbf{x}(t)}{\mathrm{d}t}=-\nabla\mathcal{L}(\mathbf{x}(t))\) with the initial condition\(\mathbf{x}(0)=\mathbf{x}\). The gradient flow projection of \(\mathbf{x}\) is defined as \(\Phi(\mathbf{x}):=\lim_{t\to+\infty}\mathbf{x}(t)\) if the limit exists and belongs to \(\Gamma\). Otherwise, \(\Phi(\mathbf{x}):=\mathbf{\theta}_{\mathrm{null}}\)._

**Definition E.2** (Slow SDE for SGD, formal).: _Given \(\mathbf{\zeta}_{0}\in\Gamma\), define \(\mathbf{\zeta}(t)\) as the solution to the following SDE with initial condition \(\mathbf{\zeta}(0)=\mathbf{\zeta}_{0}\):_

\[\mathrm{d}\mathbf{\zeta}(t)=P_{\mathbf{\zeta}}\Big{(}\underbrace{\frac{1}{\sqrt{B}} \mathbf{\Sigma}_{\parallel}^{1/2}(\mathbf{\zeta})\mathrm{d}\mathbf{W}_{t}}_{\text{(a) diffusion on $\Gamma$}}\underbrace{-\frac{1}{2B}\nabla^{3}\mathcal{L}(\mathbf{\zeta})[ \widehat{\mathbf{\Sigma}}_{\Diamond}(\mathbf{\zeta})]\mathrm{d}t}_{\text{(b) drift on $\Gamma$}}\Big{)}. \tag{6}\]

_Here, for any \(\mathbf{\zeta}\in\Gamma\), \(P_{\mathbf{\zeta}}\) is a projection operator that maps any differential form \(\mathbf{A}\mathrm{d}\mathbf{W}_{t}+\mathbf{b}\mathrm{d}t\) in Ito calculus to \(\partial\Phi(\mathbf{\zeta})\mathbf{A}\mathrm{d}\mathbf{W}_{t}+\big{(}\partial\Phi(\mathbf{ \zeta})\mathbf{b}+\frac{1}{2}\partial^{2}\Phi(\mathbf{\zeta})[\mathbf{A}\mathbf{A}^{\top}] \big{)}\), which guarantees \(\mathbf{\zeta}\) to remain on the manifold after taking such an infinitesimal step. \(B\) is the total batch size. \(\mathbf{\Sigma}_{\parallel}(\mathbf{\zeta}):=\partial\Phi(\mathbf{\zeta})\mathbf{\Sigma}(\mathbf{ \zeta})\partial\Phi(\mathbf{\zeta})\) is the covariance matrix of gradient noise projected onto the tangent space of \(\mathbf{\zeta}\) at \(\Gamma\), and \(\widehat{\mathbf{\Sigma}}_{\Diamond}(\mathbf{\zeta})\) is the noise covariance in the rest, with coordinates rescaled in the eigenbasis \(\{(\lambda_{i},\mathbf{v}_{i})\}_{i=1}^{d}\) of \(\nabla^{2}\mathcal{L}(\mathbf{\zeta})\):_

\[\widehat{\mathbf{\Sigma}}_{\Diamond}(\mathbf{\zeta}):=\sum_{i,j:(\lambda_{i}\neq 0) \vee(\lambda_{j}\neq 0)}\frac{1}{\lambda_{i}+\lambda_{j}}\left<\mathbf{\Sigma}(\mathbf{ \zeta})-\mathbf{\Sigma}_{\parallel}(\mathbf{\zeta}),\mathbf{v}_{i}\mathbf{v}_{j}^{\top}\right> \mathbf{v}_{i}\mathbf{v}_{j}^{\top}.\]

**Definition E.3** (Slow SDE for Local SGD with \(H\sim\eta^{-1}\), formal).: _Consider the scaling \(H=\beta/\eta\) for some constant \(\beta\). Given \(\mathbf{\zeta}_{0}\in\Gamma\), define \(\mathbf{\zeta}(t)\) as the solution to the following SDE with initial condition \(\mathbf{\zeta}(0)=\mathbf{\zeta}_{0}\):_

\[\mathrm{d}\mathbf{\zeta}(t)=P_{\mathbf{\zeta}}\Big{(}\underbrace{\frac{1}{\sqrt{B}} \mathbf{\Sigma}_{\parallel}^{1/2}(\mathbf{\zeta})\mathrm{d}\mathbf{W}_{t}}_{\text{(a) diffusion on $\Gamma$}}\underbrace{-\frac{1}{2B}\nabla^{3}\mathcal{L}(\mathbf{\zeta})[ \widehat{\mathbf{\Sigma}}_{\Diamond}(\mathbf{\zeta})]\mathrm{d}t}_{\text{(b) drift on $\Gamma$, same as SGD}}\underbrace{-\frac{K-1}{2B}\nabla^{3}\mathcal{L}(\mathbf{\zeta})[ \widehat{\mathbf{\Psi}}(\mathbf{\zeta};H\eta)]\mathrm{d}t}_{\text{(c) an extra drift term on $\Gamma$}}\Big{)}, \tag{7}\]

_where \(K\) is the number of workers, \(B,\mathbf{\Sigma}_{\parallel}(\mathbf{\zeta})\) and \(\widehat{\mathbf{\Sigma}}_{\Diamond}(\mathbf{\zeta})\) are the same as in Definition 3.1. Here, \(\widehat{\mathbf{\Psi}}(\mathbf{\zeta};\beta)\) is a PSD matrix depending on gradient noise and Hessian defined as follows:_

\[\widehat{\mathbf{\Psi}}(\mathbf{\zeta}):=\sum_{i,j:(\lambda_{i}\neq 0)\vee(\lambda_{j}\neq 0)} \frac{\psi(\eta H\cdot(\lambda_{i}+\lambda_{j}))}{\lambda_{i}+\lambda_{j}}\left< \mathbf{\Sigma}_{\Diamond}(\mathbf{\zeta}),\mathbf{v}_{i}\mathbf{v}_{j}^{\top}\right>\mathbf{v}_{i} \mathbf{v}_{j}^{\top}, \tag{8}\]

_where \(\{\mathbf{v}_{i}\}_{i=1}^{d}\) is a set of eigenvectors of \(\nabla^{2}\mathcal{L}(\mathbf{\zeta})\) that forms an orthonormal eigenbasis, and \(\lambda_{1},\ldots,\lambda_{d}\) are the corresponding eigenvalues. Additionally, \(\psi(x):=\frac{e^{-x}-1+x}{x}\) for \(x\neq 0\) and \(\psi(0)=0\)._

Notice that \(\psi(x)\) monotonically increases in \(x\) and has the limit \(\lim_{x\to 0}\psi(x)=0\) and \(\lim_{x\to\infty}\psi(x)=1\). Therefore, given \(\mathbf{\zeta}\), \(\widehat{\mathbf{\Psi}}(\mathbf{\zeta};\beta)\) is a monotonically increasing function of \(\beta\) in the eigenspace of the Hessian matrix \(\nabla^{2}\mathcal{L}(\mathbf{\zeta})\).

### Proof for Theorem 3.1

We consider the asymptotics that \(\eta\to 0,\alpha\to 0\) and \(\alpha=\Omega(\eta^{\gamma})\) for all \(\gamma>0\). We use big-\(\mathcal{O}\) notation to hide constants independent of \(\eta,\alpha\), and use big-\(\tilde{\mathcal{O}}\) notations to hides constants independent of \(\eta,\alpha\) and also polylog factors of \(\eta,\alpha\). We define \(\mathbf{\phi}^{(s)}:=\Phi(\mathbf{\hat{\theta}}^{(s)})\) and let \(R_{\mathrm{tot}}:=\lfloor\frac{T}{H\eta^{\gamma}}\rfloor=\lfloor\frac{T}{ \alpha^{2}}\rfloor\) be the total number of rounds.

Proof outline.: The general framework of our proof follows (Li et al., 2019a), which demonstrates the close tracking between SGD iterates and the conventional SDE by examining the moments of parameter changes over a small observation interval \(\eta\). However, their analysis is not directly applicable to our case. Their SDE approximation is only valid for \(\mathcal{O}(\eta^{-1})\) steps while our QSR involves multiple communication rounds, each with \(\mathcal{O}(\eta^{-2})\) steps. To tackle this challenge, we treat each round as a continuous-time observation interval of length \(\alpha^{2}\), and then establish that the moments of changes in the manifold projection of Local SGD and the corresponding slow SDE (5), specifically the moments of \(\mathbf{\phi}^{(s+1)}-\mathbf{\phi}^{(s)}\) and \(\mathbf{\zeta}((s+1)\alpha^{2})-\mathbf{\zeta}(s\alpha^{2})\), are closely aligned.

Notably, though the results in (Gu et al., 2023) serve as a building block to compute the moments of \(\mathbf{\phi}^{(s+1)}-\mathbf{\phi}^{(s)}\) in Lemmas E.1 to E.3, their analysis is not trivially extendable to QSR. This is because their analysis depends on the condition \(H\eta=\mathcal{O}(1)\), and many bounds therein explode as \(H\eta\to\infty\), e.g., Theorem 3.3, Lemmas I.14 and I.16. In the context of QSR, where \(H\eta=\frac{\alpha^{2}}{\eta}\) goes to infinity as \(\eta\) approaches \(0\), the condition \(H\eta=\mathcal{O}(1)\) is violated, rendering the analysis in Gu et al. (2023) ineffective for QSR.

In the following lemma, we present equivalent forms of (3), (4) and (5) that are less intuitive but more friendly to mathematical analysis.

**Theorem E.1**.: _Equations (3), (4), (5) can be rewritten as the following SDEs, respectively:_

\[\mathrm{d}\mathbf{\zeta}(t) =\frac{1}{\sqrt{B}}\partial\Phi(\mathbf{\zeta})\mathbf{\Sigma}(\mathbf{\zeta} )^{1/2}\mathrm{d}\mathbf{W}_{t}+\frac{1}{2B}\partial^{2}\Phi(\mathbf{\zeta})[\mathbf{ \Sigma}(\mathbf{\zeta})]\mathrm{d}t, \tag{9}\] \[\mathrm{d}\mathbf{\zeta}(t) =\frac{1}{\sqrt{B}}\partial\Phi(\mathbf{\zeta})\mathbf{\Sigma}(\mathbf{\zeta} )^{1/2}\mathrm{d}\mathbf{W}_{t}+\frac{1}{2B}\partial^{2}\Phi(\mathbf{\zeta})[\mathbf{ \Sigma}(\mathbf{\zeta})+(K-1)\mathbf{\Psi}(\mathbf{\zeta})]\mathrm{d}t,\] (10) \[\mathrm{d}\mathbf{\zeta}(t) =\frac{1}{\sqrt{B}}\partial\Phi(\mathbf{\zeta})\mathbf{\Sigma}(\mathbf{\zeta} )^{1/2}\mathrm{d}\mathbf{W}_{t}+\frac{K}{2B}\partial^{2}\Phi(\mathbf{\zeta})[\mathbf{ \Sigma}(\mathbf{\zeta})]\mathrm{d}t. \tag{11}\]

Proof.: Directly apply Lemmas I.1 to I.5 of Gu et al. (2023), and we have this theorem. 

Based on Gu et al. (2023)'s analysis, below we compute the moments of \(\mathbf{\phi}^{(s+1)}-\mathbf{\phi}^{(s)}\) through a series of lemmas. Then, we follow Gu et al. (2023)'s method of moments to derive the SDE approximation.

**Lemma E.1**.: _For any round \(s\leq R_{\mathrm{tot}}\) and any worker \(k\in[K]\), if \(\mathbf{\phi}^{(s)}\in\Gamma\), then it holds with probability at least \(1-\delta\), where \(\delta=\mathcal{O}(\mathrm{poly}(\eta))\), that \(\Phi(\mathbf{\theta}^{(s)}_{k,H})\in\Gamma\) and \(\|\mathbf{\theta}^{(s)}_{k,H}-\Phi(\mathbf{\theta}^{(s)}_{k,H})\|_{2}=\mathcal{O}( \sqrt{\eta\log\frac{1}{\eta\delta}})\)._

Proof.: The key insight is that the dynamics of each worker before averaging in each round is just the standard SGD with a smaller batch size, \(B_{\mathrm{loc}}\). Since the distance bound to \(\Gamma\), Theorem 3.3 in Gu et al. (2023), also applies to SGD by taking \(K^{\prime}=1\) and \(H^{\prime}=\frac{1}{\eta}\), we can apply this result to obtain that \(\|\mathbf{\theta}^{(s)}_{k,H}-\Phi(\mathbf{\theta}^{(s)}_{k,H})\|_{2}=\mathcal{O}( \sqrt{\eta\log\frac{1}{\eta\delta}})\). 

Before computing the moments of the change in manifold projection for each worker \(\Phi(\mathbf{\theta}^{(s)}_{k,H})-\mathbf{\phi}^{(s)}\), we introduce Preliminary Lemmas E.1 and E.2. Specifically, the Ito-Taylor expansion lemma E.1 is a straightforward application of Lemma B.7 of (Malladi et al., 2022) on a bounded set. Preliminary Lemma E.2 is adapted from Lemma 26 of (Li et al., 2019a).

Let \(\mathbf{X}(t)\) be the solution to the SDE \(\mathrm{d}\mathbf{X}(t)=\mathbf{b}(\mathbf{X}(t))\mathrm{d}t+\mathbf{\sigma}(\mathbf{X}(t))\mathrm{d} \mathbf{W}_{t}\), where \(\mathbf{b}(\cdot):\mathbb{R}^{d}\to\mathbb{R}^{d}\) is the drift function and \(\mathbf{\sigma}(\cdot):\mathbb{R}^{d}\to\mathbb{R}^{d\times d}\) is the diffusion matrix. Both \(\mathbf{b}(\cdot)\) and \(\mathbf{\sigma}(\cdot)\) belong to \(\mathcal{C}^{4}\). Let \(\mathcal{S}\) be a bounded invariant set of the SDE. That is, if \(\mathbf{X}(0)\in\mathcal{S}\), for any \(t\geq 0\), \(\mathbf{X}(t)\in\mathcal{S}\) almost surely. Let \(\eta_{\mathrm{e}}\) be the "effective learning rate", which can be viewed as the length of the continuous-time observation interval for \(\mathbf{X}(t)\). Then we have the following lemma.

**Preliminary Lemma E.1** (Ito-Taylor expansion).: _Let \(g:\mathbb{R}^{d}\to\mathbb{R}\) be any \(\mathcal{C}^{4}\)-smooth function. Define_

\[\mathcal{A}g(\mathbf{x}):=\sum_{i\in[D]}b_{i}(\mathbf{x})\partial_{i}g(\mathbf{x})+\frac{1 }{2}\sum_{i,j\in[D]}\left(\sum_{t\in[D]}\sigma_{i,l}(\mathbf{x})\sigma_{l,j}(\mathbf{x })\right)\partial_{i,j}^{2}g(\mathbf{x}). \tag{12}\]

_Given \(\mathbf{X}(t)=\mathbf{x}\in\mathcal{S}\), there exists a constant \(C\) independent of \(\eta_{\mathrm{e}}\) such that_

\[|\mathbb{E}[g(\mathbf{X}(t+\eta_{\mathrm{e}}))-g(\mathbf{x})-\eta_{\mathrm{e}}\mathcal{ A}g(\mathbf{x})]|\leq C\eta_{\mathrm{e}}^{2}\]

Proof.: WLOG, we prove the case for \(t=0\). Due to the Markovian property of Ito processes, the same proof can be done for any \(t>0\) by a time shift. Give \(\mathbf{X}(0)=\mathbf{x}\in\mathcal{S}\), by Ito's lemma,

\[g(\mathbf{X}(\eta_{\mathrm{e}}))=g(\mathbf{x})+\int_{0}^{\eta_{\mathrm{e}}}\mathcal{A} g(\mathbf{X}(s))\mathrm{d}s+\int_{0}^{\eta_{\mathrm{e}}}\left\langle\Lambda g(\mathbf{X} (s)),\mathrm{d}\mathbf{W}_{s}\right\rangle,\]

where \(\Lambda(\mathbf{x}):=\mathbf{\sigma}(\mathbf{x})^{\top}\nabla g(\mathbf{x})\). 

Further apply Ito's lemma to \(\mathcal{A}g(\mathbf{X}(s))\) and we have

\[g(\mathbf{X}(\eta_{\mathrm{e}})) =g(\mathbf{x})+\int_{0}^{\eta_{\mathrm{e}}}\left(\mathcal{A}g(\mathbf{x} )+\int_{0}^{s}\mathcal{A}^{2}g(\mathbf{X}(r))\mathrm{d}r+\int_{0}^{s}\left\langle \Lambda\mathcal{A}g(\mathbf{X}(r)),\mathrm{d}\mathbf{W}_{r}\right\rangle\right)\mathrm{ d}s\] \[\quad+\int_{0}^{\eta_{\mathrm{e}}}\left\langle\Lambda g(\mathbf{X}(s )),\mathrm{d}\mathbf{W}_{s}\right\rangle\] \[=g(\mathbf{x})+\eta_{\mathrm{e}}\mathcal{A}g(\mathbf{x})+\int_{0}^{\eta_ {\mathrm{e}}}\int_{0}^{s}\mathcal{A}^{2}g(\mathbf{X}(r))\mathrm{d}r\mathrm{d}s\] \[\quad+\int_{0}^{\eta_{\mathrm{e}}}\int_{0}^{s}\left\langle \Lambda\mathcal{A}g(\mathbf{X}(r)),\mathrm{d}\mathbf{W}_{r}\right\rangle\mathrm{d}s+ \int_{0}^{\eta_{\mathrm{e}}}\left\langle\Lambda g(\mathbf{X}(s)),\mathrm{d}\mathbf{W}_ {s}\right\rangle.\]

Take expectation on both sides, and the last two terms become zero:

\[\mathbb{E}g(\mathbf{X}(\eta_{\mathrm{e}}))=g(\mathbf{x})+\eta_{\mathrm{e}}\mathcal{A} g(\mathbf{x})+\int_{0}^{\eta_{\mathrm{e}}}\int_{0}^{s}\mathcal{A}^{2}g(\mathbf{X}(r)) \mathrm{d}r\mathrm{d}s.\]

Since \(\mathbf{X}(s)\) belongs to the bounded set \(\mathcal{S}\), there exists a constant \(C\) independent of \(\eta_{\mathrm{e}}\) such that \(|\mathcal{A}^{2}g(\mathbf{y})|\leq C\) for all \(\mathbf{y}\in\mathcal{S}\). Therefore,

\[|\mathbb{E}[g(\mathbf{X}(\eta_{\mathrm{e}}))-g(\mathbf{x})-\eta_{\mathrm{e}}\mathcal{A} g(\mathbf{x})]|\leq C\eta_{\mathrm{e}}^{2}.\]

**Preliminary Lemma E.2** (Adaptation of Lemma 26 in [11]).: _Given \(\mathbf{X}(t)=\mathbf{x}\in\mathcal{S}\), denote the change in \(\mathbf{X}(s)\) over time interval \(\eta_{\mathrm{e}}\) as \(\tilde{\mathbf{\Delta}}(\mathbf{x},t,\eta_{\mathrm{e}}):=\mathbf{X}(t+\eta_{\mathrm{e}})- \mathbf{x}\). Then, for all \(\mathbf{x}\in\mathcal{S}\) and \(t\geq 0\), there exists a constant \(C^{\prime}\) independent of \(\eta_{\mathrm{e}}\) such that_

\[\mathbb{E}[\prod_{j=1}^{n+1}\left|\tilde{\Delta}_{i_{j}}(\mathbf{x},t,\eta_{ \mathrm{e}})\right|]\leq C^{\prime}\eta_{\mathrm{e}}^{\frac{n+1}{2}},\quad \forall 1\leq i_{1},\cdots,i_{n+1}\leq d,\]

_where \(n\geq 1\)._

Proof.: WLOG, we prove the case for \(t=0\). Due to the Markovian property of Ito processes, the same proof can be done for any \(t>0\) by a time shift. Denote \(\tilde{\mathbf{\Delta}}(\mathbf{x}):=\tilde{\mathbf{\Delta}}(\mathbf{x},0,\eta_{\mathrm{e}})\) for brevity. By definition,

\[\tilde{\mathbf{\Delta}}(\mathbf{x})=\int_{0}^{\eta_{\mathrm{e}}}\mathbf{b}(\mathbf{X}(s)) \mathrm{d}s+\int_{0}^{\eta_{\mathrm{e}}}\mathbf{\sigma}(\mathbf{X}(s))\mathrm{d}\mathbf{W} _{s}.\]By triangle inequality, for all \(i\in[d]\),

\[\left|\tilde{\Delta}_{i}(\mathbf{x})\right|\leq\left\|\int_{0}^{n_{\text{e}}}\mathbf{b}( \mathbf{X}(s))\mathrm{d}s\right\|_{2}+\left\|\int_{0}^{n_{\text{e}}}\mathbf{\sigma}(\mathbf{ X}(s))\mathrm{d}\mathbf{W}_{s}\right\|_{2}.\]

Therefore,

\[\mathbb{E}[\prod_{j=1}^{n+1}\left|\tilde{\Delta}_{i_{j}}(\mathbf{x}) \right|] \leq\left(\mathbb{E}\left\|\int_{0}^{n_{\text{e}}}\mathbf{b}(\mathbf{X}(s ))\mathrm{d}s\right\|_{2}+\mathbb{E}\left\|\int_{0}^{n_{\text{e}}}\mathbf{\sigma}( \mathbf{X}(s))\mathrm{d}\mathbf{W}_{s}\right\|_{2}\right)^{n+1}\] \[\leq 2^{n}\left(\underbrace{\mathbb{E}\left\|\int_{0}^{n_{\text{e }}}\mathbf{b}(\mathbf{X}(s))\mathrm{d}s\right\|_{2}}_{\mathcal{T}_{1}}\right)^{n+1}+2^{ n}\left(\underbrace{\mathbb{E}\left\|\int_{0}^{n_{\text{e}}}\mathbf{\sigma}(\mathbf{X}(s)) \mathrm{d}\mathbf{W}_{s}\right\|_{2}}_{\mathcal{T}_{2}}\right)^{n+1}.\]

By triangle inequality,

\[\mathcal{T}_{1}\leq\mathbb{E}\left[\int_{0}^{n_{\text{e}}}\|\mathbf{b}(\mathbf{X}(s)\| _{2}\mathrm{d}s\right].\]

By Cauchy-Schwarz inequality and Ito's isometry,

\[\mathcal{T}_{2}\leq\sqrt{\mathbb{E}\left\|\int_{0}^{n_{\text{e}}}\mathbf{\sigma}( \mathbf{X}(s))\mathrm{d}\mathbf{W}_{s}\right\|_{2}^{2}}=\sqrt{\mathbb{E}\int_{0}^{n_{ \text{e}}}\mathrm{tr}[\mathbf{\sigma}(\mathbf{X}(s)^{\top}\mathbf{\sigma}(\mathbf{X}(s))] \mathrm{d}s}.\]

Since \(\mathbf{X}(s)\in\mathcal{S}\) almost surely and \(\mathcal{S}\) is a bounded set, there exists constants \(C_{1}\) and \(C_{2}\) such that \(\mathcal{T}_{1}\leq C_{1}\eta_{\text{e}},\mathcal{T}_{2}\leq C_{2}\eta_{\text{ e}}^{0.5}\). Substituting the bounds for \(\mathcal{T}_{1}\) and \(\mathcal{T}_{2}\) back, we have the lemma. 

**Lemma E.2**.: _For any round \(s\leq R_{\mathrm{tot}}\) and any worker \(k\in[K]\), given \(\mathbf{\phi}^{(s)}\in\Gamma\), then_

\[\mathbb{E}[\Phi(\mathbf{\theta}^{(s)}_{k,H})-\mathbf{\phi}^{(s)}\mid\mathbf{ \phi}^{(s)}]=\frac{\alpha^{2}}{2B_{\mathrm{loc}}}\partial^{2}\Phi(\mathbf{\phi}^{( s)})[\mathbf{\Sigma}(\mathbf{\phi}^{(s)})]+\mathcal{O}(\alpha^{4}), \tag{13}\] \[\mathbb{E}\left[(\Phi(\mathbf{\theta}^{(s)}_{k,H})-\mathbf{\phi}^{(s)})( \Phi(\mathbf{\theta}^{(s)}_{k,H})-\mathbf{\phi}^{(s)})^{\top}\mid\mathbf{\phi}^{(s)} \right]=\frac{\alpha^{2}}{B_{\mathrm{loc}}}\mathbf{\Sigma}_{\parallel}(\mathbf{\phi}^{( s)})+\mathcal{O}(\alpha^{4}),\] (14) \[\mathbb{E}\left[(\Phi(\mathbf{\theta}^{(s)}_{k,H})-\mathbf{\phi}^{(s)})^{ \otimes 3}\mid\mathbf{\phi}^{(s)}\right]=\mathcal{O}(\alpha^{4}),\] (15) \[\mathbb{E}\left[\|\Phi(\mathbf{\theta}^{(s)}_{k,H})-\mathbf{\phi}^{(s)}\| _{2}^{6}\mid\mathbf{\phi}^{(s)}\right]=\mathcal{O}(\alpha^{6}). \tag{16}\]

Proof.: Again, the key insight is that the dynamics of each worker before averaging in each round is just the standard SGD with a smaller batch size, \(B_{\mathrm{loc}}\). Since the SDE approximation theorem for Local SGD, Theorem 3.2 in Gu et al. (2023), also applies to SGD by taking \(K^{\prime}=1\) and \(H^{\prime}=\frac{1}{\eta}\), we can apply this result to obtain that, for any \(\mathcal{C}^{4}\)-smooth function \(g(\mathbf{\theta})\), it holds for \(\mathbf{\zeta}\) defined in (9) with the initial condition \(\mathbf{\zeta}(0)=\mathbf{\phi}^{(s)}\) that

\[|\mathbb{E}[g(\Phi(\mathbf{\theta}^{(s)}_{k,H}))]-\mathbb{E}[g(\mathbf{\zeta}(T^{ \prime}))]|=\tilde{\mathcal{O}}(\eta^{0.25}), \tag{17}\]

where \(T^{\prime}=\alpha^{2}\) is the continuous-time observation interval.

To establish a connection between the moments of \(\Phi(\mathbf{\theta}^{(s)}_{k,t})-\mathbf{\phi}^{(s)}\) and those of \(\mathbf{\zeta}(T^{\prime})-\mathbf{\phi}^{(s)}\), we can let the function \(g(\mathbf{\theta})\) to take specific forms, each returning a single coordinate of \(\mathbf{\theta}-\mathbf{\phi}^{(s)}\), \((\mathbf{\theta}-\mathbf{\phi}^{(s)})(\mathbf{\theta}-\mathbf{\phi}^{(s)})^{\top}\), \((\mathbf{\theta}-\mathbf{\phi}^{(s)})^{\otimes 3}\) and \(\|\mathbf{\theta}-\mathbf{\phi}^{(s)}\|_{2}^{6}\). For example, to relate \(\Phi(\mathbf{\theta}^{(s)}_{k,H})-\mathbf{\phi}^{(s)}\) to \(\mathbf{\zeta}(T^{\prime})-\mathbf{\phi}^{(s)}\), let \(g(\mathbf{\theta})=\langle\mathbf{e}_{1},\mathbf{\theta}\rangle\) where \(\mathbf{e}_{1}=(1,0,\cdots,0)^{\top}\). Substitute \(g\) into (17), and we get \(|\left\langle\mathbf{e}_{1},\mathbb{E}[\Phi(\mathbf{\theta}^{(s)}_{k,H})-\mathbf{\phi}^{(s )}]-\mathbb{E}[\Phi(\mathbf{\zeta}(T^{\prime}))-\mathbf{\phi}^{(s)}]\right\rangle|= \tilde{\mathcal{O}}(\eta^{0.25})\). We can obtain the same results for all coordinates by letting \(g(\mathbf{\theta})=\langle\mathbf{e}_{i},\mathbf{\theta}\rangle\) for all \(i\in[D]\). Therefore, \(|\mathbb{E}[\Phi(\mathbf{\theta}^{(s)}_{k,H})-\mathbf{\phi}^{(s)}]-\mathbb{E}[\mathbf{\zeta} (T^{\prime})-\mathbf{\phi}^{(s)}]|=\tilde{\mathcal{O}}(\eta^{0.25})\). Similarly, we can show that the LHS of (14) to (16) are only changed by \(\tilde{\mathcal{O}}(\eta^{0.25})=o(\mathrm{poly}(\alpha))\) when replacing \(\Phi(\mathbf{\theta}^{(s)}_{k,H})\) with \(\mathbf{\zeta}(T^{\prime})\).

Then, it suffices to compute the moments for \(\mathbf{\zeta}(T^{\prime})\) and verify that they match the RHS of (13) to (16). Since \(\Gamma\) is compact and invariant for the SDE (11) (Lemma I.39 in (Gu et al., 2023)), we can apply the Ito-Taylor expansion in Preliminary Lemma E.1 with \(\eta_{\mathrm{e}}=\alpha^{2}\), \(\mathbf{X}(t)=\mathbf{\zeta}(t)\), \(\mathbf{b}(\mathbf{\zeta})=\frac{K}{2B}\partial^{2}\Phi(\mathbf{\zeta})[\mathbf{\Sigma}(\mathbf{ \zeta})]\) and \(\mathbf{\sigma}(\mathbf{\zeta})=\sqrt{\frac{1}{2B}}\partial\Phi(\mathbf{\zeta})\mathbf{\Sigma}^ {1/2}(\mathbf{\zeta})\).

To obtain the first moment (13), let \(g(\mathbf{\zeta})=\left\langle\mathbf{e}_{1},\mathbf{\zeta}-\mathbf{\phi}^{(s)}\right\rangle\) and substitute it into (12). By Preliminary Lemma E.1, we have

\[\left|\mathbb{E}[\zeta_{1}(T^{\prime})]-b_{1}(\mathbf{\phi}^{(s)})\right|=\mathcal{ O}(T^{\prime 2})=\mathcal{O}(\alpha^{4}).\]

We can repeat this process for all coordinates of \(\mathbf{\zeta}(T^{\prime})\) to obtain

\[\mathbb{E}[\mathbf{\zeta}(T^{\prime})-\mathbf{\phi}^{(s)}\mid\mathbf{\phi}^{(s)}]=\frac{ \alpha^{2}}{2B_{\mathrm{loc}}}\partial^{2}\Phi(\mathbf{\phi}^{(s)})[\mathbf{\Sigma}( \mathbf{\phi}^{(s)})]+\mathcal{O}(\alpha^{4}), \tag{18}\]

and thus (13).

For the second moment (14), define \(g^{(i,j)}(\mathbf{\zeta})=\left\langle\mathbf{M}_{i,j},(\mathbf{\zeta}-\mathbf{\phi}^{(s)})( \mathbf{\zeta}-\mathbf{\phi}^{(s)})^{\top}\right\rangle\), where \(M_{i^{\prime},j^{\prime}}=\begin{cases}1,(i^{\prime},j^{\prime})=(i,j),\\ 0,\mathrm{otherwise}\end{cases}\). Since \(\partial_{i^{\prime}}g^{(i,j)}(\mathbf{\zeta})=0\) for all \(i^{\prime}\), the first term of \(\mathcal{A}g^{(i,j)}(\mathbf{\zeta})\) vanishes. It suffices to compute the second term. When \(i=j\), \(\partial_{i^{\prime},j^{\prime}}^{2}g^{(i,i)}(\mathbf{\zeta})=\begin{cases}2,(i^{ \prime},j^{\prime})=(i,i)\\ 0,\mathrm{otherwise}\end{cases}\). Therefore,

\[\mathcal{A}g^{(i,j)}(\mathbf{\zeta})=\sum_{l\in[D]}\sigma_{i,l}(\mathbf{\zeta})\sigma_ {l,i}(\mathbf{\zeta}),\quad\forall i\in[D]. \tag{19}\]

When \(i\neq j\), \(\partial_{i^{\prime},j^{\prime}}^{2}g^{(i,j)}(\mathbf{\zeta})=\begin{cases}1,(i^{ \prime},j^{\prime})\in\{(i,j),(j,i)\}\\ 0,\mathrm{otherwise}\end{cases}\). Therefore,

\[\mathcal{A}g^{(i,j)}(\mathbf{\zeta})=\sum_{l\in[D]}\sigma_{i,l}(\mathbf{\zeta})\sigma_ {l,j}(\mathbf{\zeta}),\quad i\neq j. \tag{20}\]

Combining (19) and (20) and noticing that \(g^{(i,j)}(\mathbf{\phi}^{(s)})=0\) for all \(i,j\), we have

\[\mathbb{E}[(\mathbf{\zeta}(T^{\prime})-\mathbf{\phi}^{(s)})(\mathbf{\zeta}(T^{\prime})-\bm {\phi}^{(s)})^{\top}\mid\mathbf{\phi}^{(s)}]=\frac{\alpha^{2}}{B}\mathbf{\Sigma}_{ \parallel}(\mathbf{\phi}^{(s)})+\mathcal{O}(\alpha^{4}), \tag{21}\]

and thus (14).

For the third moment (15), define \(g^{(i,j,l)}(\mathbf{\zeta})=\left\langle\mathbf{e}_{i}\otimes\mathbf{e}_{j}\otimes\mathbf{e}_ {l},(\Phi(\mathbf{\theta}^{(s)}_{k,H})-\mathbf{\phi}^{(s)})^{\otimes 3}\right\rangle\). Noticing that \(\partial_{i^{\prime}}g^{(i,j,l)}(\mathbf{\phi}^{(s)})=0\) for all \(i^{\prime}\) and \(\partial_{i^{\prime},j^{\prime}}^{2}g^{(i,j,l)}(\mathbf{\phi}^{(s)})=0\) for all \((i^{\prime},j^{\prime})\), we have

\[\mathbb{E}\left[(\mathbf{\zeta}(T^{\prime})-\mathbf{\phi}^{(s)})^{\otimes 3}\mid\mathbf{ \phi}^{(s)}\right]=\mathcal{O}(\alpha^{4}), \tag{22}\]

and thus (15).

Finally, by directly applying Preliminary Lemma E.2, we have

\[\mathbb{E}\left[\left\|\mathbf{\zeta}(T^{\prime})-\mathbf{\phi}^{(s)}\right\|_{2}^{6} \mid\mathbf{\phi}^{(s)}\right]=\mathcal{O}(\alpha^{6}) \tag{23}\]

and thus (16). 

Now we are ready to compute the moments for \(\mathbf{\phi}^{(s+1)}-\mathbf{\phi}^{(s)}\) at each round:

**Lemma E.3**.: _For any round \(s\leq R_{\mathrm{tot}}\), given \(\mathbf{\phi}^{(s)}\in\Gamma\), then_

\[\mathbb{E}[\mathbf{\phi}^{(s+1)}-\mathbf{\phi}^{(s)}\mid\mathbf{\phi}^{(s)}]=\frac{\alpha^{ 2}}{2B_{\mathrm{loc}}}\partial^{2}\Phi(\mathbf{\phi}^{(s)})[\mathbf{\Sigma}(\mathbf{\phi}^{ (s)})]+\mathcal{O}(\alpha^{4}), \tag{24}\]

\[\mathbb{E}\left[(\mathbf{\phi}^{(s+1)}-\mathbf{\phi}^{(s)})(\mathbf{\phi}^{(s+1)}-\mathbf{ \phi}^{(s)})^{\top}\mid\mathbf{\phi}^{(s)}\right]=\frac{\alpha^{2}}{B}\mathbf{\Sigma}_{ \parallel}(\mathbf{\phi}^{(s)})+\mathcal{O}(\alpha^{4}), \tag{25}\]

\[\mathbb{E}\left[\left\|\mathbf{\phi}^{(s+1)}-\mathbf{\phi}^{(s)}\right\|_{2}^{6}\mid \mathbf{\phi}^{(s)}\right]=\mathcal{O}(\alpha^{6}). \tag{26}\]Proof.: Let \(\mathbf{\Delta}_{1}:=\frac{1}{K}\sum_{k=1}^{K}\Phi(\mathbf{\theta}^{(s)}_{k,H})-\mathbf{\phi}^{ (s)}\). By Lemma E.2,

\[\mathbb{E}[\mathbf{\Delta}_{1}] =\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}[\Phi(\mathbf{\theta}^{(s)}_{k,H}) -\mathbf{\phi}^{(s)}]\] \[=\frac{\alpha^{2}}{2B_{\mathrm{loc}}}\partial^{2}\Phi(\mathbf{\phi}^{ (s)})[\mathbf{\Sigma}(\mathbf{\phi}^{(s)})]+\mathcal{O}(\alpha^{4}),\] \[\mathbb{E}[\mathbf{\Delta}_{1}\mathbf{\Delta}_{1}^{\top}] =\frac{1}{K^{2}}\sum_{j=1}^{K}\sum_{k=1}^{K}\mathbb{E}\left[(\Phi (\mathbf{\theta}^{(s)}_{j,H})-\mathbf{\phi}^{(s)})(\Phi(\mathbf{\theta}^{(s)}_{k,H})-\mathbf{ \phi}^{(s)})^{\top}\right]\] \[=K\cdot\frac{\alpha^{2}}{B}\mathbf{\Sigma}_{\parallel}(\mathbf{\phi}^{(s) })+\mathcal{O}(\alpha^{4})+K(K-1)\cdot\mathcal{O}(\alpha^{4})\] \[=\frac{\alpha^{2}}{B_{\mathrm{loc}}}\mathbf{\Sigma}_{\parallel}(\mathbf{ \phi}^{(s)})+\mathcal{O}(\alpha^{4}).\]

Let \(\mathbf{\Delta}_{2}:=\frac{1}{K}\sum_{k=1}^{K}\mathbf{\theta}^{(s)}_{k,H}-\mathbf{\phi}^{ (s)}\). Then \(\mathbf{\Delta}_{2}=\mathbf{\Delta}_{1}+\frac{1}{K}\sum_{k=1}^{K}(\mathbf{\theta}^{(s)}_{k, H}-\Phi(\mathbf{\theta}^{(s)}_{k,H}))\). Finally, let \(\mathbf{\Delta}_{3}:=\Phi(\frac{1}{K}\sum_{k=1}^{K}\mathbf{\theta}^{(s)}_{k,H})-\mathbf{ \phi}^{(s)}\). By Lemma E.1 it holds with probability at least \(1-\delta\) that \(\|\mathbf{\theta}^{(s)}_{k,H}-\Phi(\mathbf{\theta}^{(s)}_{k,H})\|_{2}=\mathcal{O}(\sqrt {\eta\log\frac{1}{\eta}})\) and thus \(\left\|\mathbf{\Delta}_{2}-\mathbf{\Delta}_{1}\right\|_{2}=\mathcal{O}(\sqrt{\eta\log \frac{1}{\eta}}).\) Let \(\delta=\eta^{100}\). Since \(\|\partial\Phi(\,\cdot\,)\|_{2}\) is always bounded by \(\mathcal{O}(1)\), we can always add an error of \(\mathcal{O}(\delta)\) to our bounds for the moments and ignore the possibility that this event does not happen. To prove (24), we do Taylor expansion of \(\Phi\) at \(\mathbf{\phi}^{(s)}\), then

\[\mathbb{E}[\mathbf{\Delta}_{3}] =\mathbb{E}[\Phi(\mathbf{\phi}^{(s)}+\mathbf{\Delta}_{2})-\mathbf{\phi}^{(s)}]\] \[=\mathbb{E}[\Phi(\mathbf{\phi}^{(s)}+\mathbf{\Delta}_{1})-\mathbf{\phi}^{(s)} ]+\mathcal{O}\Big{(}\sqrt{\eta\log\frac{1}{\eta}}+\delta\Big{)}\] \[=\frac{\alpha^{2}}{2B_{\mathrm{loc}}}\partial^{2}\Phi(\mathbf{\phi}^{ (s)})[\mathbf{\Sigma}(\mathbf{\phi}^{(s)})]+\mathcal{O}(\alpha^{4}).\]

The last equation uses the fact that \(\partial\Phi(\mathbf{\phi})\), for \(\mathbf{\phi}\in\Gamma\), is a projection matrix onto the tangent space of \(\Gamma\) at \(\mathbf{\theta}\) (Lemma 4.3 of (Li et al., 2021c)).

To prove (25), again we do Taylor expansion of \(\Phi\) at \(\mathbf{\phi}^{(s)}\) to connect \(\Phi(\mathbf{\phi}^{(s)}+\mathbf{\Delta}_{2})\) with \(\Phi(\mathbf{\phi}^{(s)}+\mathbf{\Delta}_{1})\) and obtain:

\[\mathbb{E}[\mathbf{\Delta}_{3}\mathbf{\Delta}_{3}^{\top}] =\mathbb{E}\left[(\Phi(\mathbf{\phi}^{(s)}+\mathbf{\Delta}_{2})-\mathbf{\phi }^{(s)})(\Phi(\mathbf{\phi}^{(s)}+\mathbf{\Delta}_{2})-\mathbf{\phi}^{(s)})^{\top}\right]\] \[=\mathbb{E}\left[(\Phi(\mathbf{\phi}^{(s)}+\mathbf{\Delta}_{1})-\mathbf{\phi }^{(s)})(\Phi(\mathbf{\phi}^{(s)}+\mathbf{\Delta}_{1})-\mathbf{\phi}^{(s)})^{\top}\right] +\mathcal{O}\Big{(}\sqrt{\eta\log\frac{1}{\eta}}+\delta\Big{)}\,.\]

Applying the second-order Taylor expansion gives

\[\mathbb{E}[\mathbf{\Delta}_{3}\mathbf{\Delta}_{3}^{\top}] =\mathbb{E}\bigg{[}\Big{(}\partial\Phi(\mathbf{\phi}^{(s)})\mathbf{ \Delta}_{1}\Big{)}\left(\partial\Phi(\mathbf{\phi}^{(s)})\mathbf{\Delta}_{1}\right)^{\top}\] \[\qquad+\Big{(}\partial^{2}\Phi(\mathbf{\phi}^{(s)})[\mathbf{\Delta}_{1},\mathbf{\Delta}_{1}]\partial\Phi(\mathbf{\phi}^{(s)})\mathbf{\Delta}_{1}+\mathbf{\Delta}_{1}^{ \top}\partial\Phi(\mathbf{\phi}^{(s)})\partial^{2}\Phi(\mathbf{\phi}^{(s)})[\mathbf{ \Delta}_{1},\mathbf{\Delta}_{1}]^{\top}\Big{)}\] \[\qquad+\mathcal{O}(\left\|\mathbf{\Delta}_{1}\right\|_{2}^{4})\bigg{]} +\mathcal{O}\Big{(}\sqrt{\eta\log\frac{1}{\eta}}+\delta\Big{)}\,.\]

By (15) and the fact that \(\|\partial^{2}\Phi(\mathbf{\phi}^{(s)})\|_{2}\) is bounded, the above equation can be simplified to

\[\mathbb{E}[\mathbf{\Delta}_{3}\mathbf{\Delta}_{3}^{\top}] =\mathbb{E}\left[\partial\Phi(\mathbf{\phi}^{(s)})\mathbf{\Delta}_{1}\bm {\Delta}_{1}^{\top}\partial\Phi(\mathbf{\phi}^{(s)})\right]+\mathcal{O}(\alpha^{4})+ \mathcal{O}(\alpha^{4})+\mathcal{O}\Big{(}\sqrt{\eta\log\frac{1}{\eta}}+\delta \Big{)}\] \[=\frac{\alpha^{2}}{B}\mathbf{\Sigma}_{\parallel}(\mathbf{\phi}^{(s)})+ \mathcal{O}(\alpha^{4}).\]

Finally, for (26), we can repeat the above process to bound \(\mathbb{E}[\|\mathbf{\Delta}_{1}\|_{2}^{3}]\), and then conclude that \(\mathbb{E}[\|\Delta_{3}\|_{2}^{3}]=\mathcal{O}(\alpha^{6})\)Now we are ready to prove our main theorem.

Proof for Theorem 3.1.: Let \(\mathbf{\zeta}(t)\) be the solution of (11). Let \(r\) be some integer greater than \(s\). If \(\mathbf{\phi}^{(s)}\in\Gamma\), define \(\hat{\mathbf{\zeta}}_{s,r}\) as the random variable sampled from the distribution of \(\mathbf{\zeta}(\alpha^{2}r)\) conditioned on \(\mathbf{\zeta}(\alpha^{2}s)=\mathbf{\phi}^{(s)}\). If \(\mathbf{\phi}^{(s)}=\mathbf{\theta}_{\mathrm{null}}\notin\Gamma\), define \(\hat{\mathbf{\zeta}}_{s,r}=\mathbf{0}\).

If \(\mathbf{\theta}\in\Gamma\), define \(u(\mathbf{\theta},t_{1},t_{2})\) the expected value of \(g(\mathbf{\zeta}(t_{2}))\) conditioned on \(\mathbf{\zeta}(t_{1})=\mathbf{\theta}\). If \(\mathbf{\theta}=\mathbf{\theta}_{\mathrm{null}}\), define \(u(\mathbf{\theta},t_{1},t_{2})=\mathbf{0}\). That is,

\[u(\mathbf{\theta},t_{1},t_{2}):=\begin{cases}\mathbb{E}[g(\mathbf{\zeta}(t_{2}))\mid \mathbf{\zeta}(t_{1})=\mathbf{\theta}],&\mathbf{\theta}\in\Gamma,\\ \mathbf{0},&\mathbf{\theta}\notin\Gamma.\end{cases}\]

For all \(n\leq R_{\mathrm{tot}}\), we have

\[|\mathbb{E}[g(\mathbf{\phi}^{(n)})]-\mathbb{E}[g(\mathbf{\zeta}(n\alpha^ {2}))]| =|\mathbb{E}[g(\hat{\mathbf{\zeta}}_{n,n})]-\mathbb{E}[g(\hat{\mathbf{\zeta }}_{0,n})]|\] \[\leq\sum_{s=0}^{n-1}|\mathbb{E}[g(\hat{\mathbf{\zeta}}_{s+1,n})]- \mathbb{E}[g(\hat{\mathbf{\zeta}}_{s,n})]|\] \[=\sum_{s=0}^{n-1}\Bigg{|}\mathbb{E}\left[\mathbb{E}[g(\hat{\mathbf{ \zeta}}_{s+1,n})\mid\mathbf{\phi}^{(s+1)}]\right]-\underbrace{\mathbb{E}\left[ \mathbb{E}[g(\hat{\mathbf{\zeta}}_{s,n})\mid\mathbf{\phi}^{(s)}]\right]}_{\mathcal{T}_ {s}}\Bigg{|}.\]

By the law of total expectation and the Markovian property of Ito process,

\[\mathcal{T}_{s} =\mathbb{E}\left[\mathbb{E}[g(\mathbf{\zeta}(n\alpha^{2}))\mid\mathbf{ \zeta}(s\alpha^{2})=\mathbf{\phi}^{(s)}]\right]\] \[=\mathbb{E}\left\{\mathbb{E}\left[\mathbb{E}[g(\mathbf{\zeta}(n \alpha^{2}))\mid\mathbf{\zeta}((s+1)\alpha^{2})]\Big{|}\mathbf{\zeta}(s\alpha^{2})=\bm {\phi}^{(s)}\right]\right\}\] \[=\mathbb{E}[u(\hat{\mathbf{\zeta}}_{s,s+1},(s+1)\alpha^{2},n\alpha^{ 2})].\]

Therefore,

\[|\mathbb{E}[g(\mathbf{\phi}^{(n)})]-\mathbb{E}[g(\mathbf{\zeta}(n\alpha^{2}))]|=\sum_{ s=0}^{n-1}\Bigg{|}\underbrace{\mathbb{E}[u(\mathbf{\phi}^{(s+1)},(s+1)\alpha^{2},n \alpha^{2})]-\mathbb{E}[u(\hat{\mathbf{\zeta}}_{s,s+1},(s+1)\alpha^{2},n\alpha^{2} )]}_{\mathcal{T}_{s}^{\prime}}\Bigg{|}.\]

By the law of total expectation,

\[|\mathcal{T}_{s}^{\prime}| \leq\underbrace{|\mathbb{E}[u(\mathbf{\phi}^{(s+1)},(s+1)\alpha^{2}, n\alpha^{2})-u(\hat{\mathbf{\zeta}}_{s,s+1},(s+1)\alpha^{2},n\alpha^{2})\mid\mathbf{ \phi}^{(s)},\mathbf{\phi}^{(s+1)}\in\Gamma]|}_{\mathcal{A}_{s}}\] \[+|u(\mathbf{0})|\mathbb{P}(\mathbf{\phi}^{(s)}\notin\Gamma\text{ or } \mathbf{\phi}^{(s+1)}\notin\Gamma),\]

where the latter term comes from the definition of \(u(\mathbf{\theta},t_{1},t_{2})\) and \(\hat{\mathbf{\zeta}}_{s,r}\). By Lemma I.11 in (Gu et al., 2023), there exists a constant \(\epsilon\) such that if \(\min_{\mathbf{\phi}\in\Gamma}\|\mathbf{\theta}-\mathbf{\phi}\|_{2}\leq\epsilon\), then \(\Phi(\mathbf{\theta})\in\Gamma\). Therefore, substituting \(\delta=\eta^{100}\) into Lemma E.1, we can conclude that the latter term is at most \(\mathcal{O}(\eta^{100})\).

For \(\mathcal{A}_{s}\), notice that the two terms differ only in the first position. By Lemma E.3 and (18) to (23), the moments of \(\mathbf{\phi}^{(s+1)}-\mathbf{\phi}^{(s)}\) and \(\hat{\mathbf{\zeta}}_{s,s+1}-\mathbf{\phi}^{(s)}\) are close to each other. Therefore, it suffices to discuss the smoothness of \(u\) and perform Taylor expansion. By Proposition 25 of (Li et al., 2019), since \(g\in\mathcal{C}^{4}\), \(u(\mathbf{\phi},t_{1},t_{2})\) satisfies the compatibility condition for the Whitney Extension Theorem for \(\mathbf{\phi}\in\Gamma\). Therefore, there exists a function \(\tilde{u}(\mathbf{\phi},t_{1},t_{2})\) that is \(\mathcal{C}^{4}\) in \(\mathbf{\phi}\) for \(\mathbf{\phi}\in\mathbb{R}^{d}\) and satisfies \(\tilde{u}(\mathbf{\phi},t_{1},t_{2})=u(\mathbf{\phi},t_{1},t_{2})\) for all \(\mathbf{\phi}\in\Gamma\). Denote \(\tilde{u}(\mathbf{\phi},s\alpha^{2},n\alpha^{2})\) as \(\tilde{u}_{s,n}(\mathbf{\phi})\) for brevity. Now, we can safely substitute \(u\) in \(\mathcal{A}_{s}\) with \(\tilde{u}\) and perform Taylor expansion:

\[\mathcal{A}_{s} =\underbrace{\mathbb{E}[\tilde{u}(\mathbf{\phi}^{(s)}+(\mathbf{\phi}^{ (s+1)}-\mathbf{\phi}^{(s)})),(s+1)\alpha^{2},n\alpha^{2})\mid\mathbf{\phi}^{(s)},\mathbf{ \phi}^{(s+1)}\in\Gamma]}_{\mathcal{A}_{s}^{\prime}}\] \[\quad-\underbrace{\mathbb{E}[\tilde{u}(\mathbf{\phi}^{(s)}+(\hat{ \mathbf{\zeta}}_{s,s+1}-\mathbf{\phi}^{(s)})),(s+1)\alpha^{2},n\alpha^{2})\mid\mathbf{\phi }^{(s)}\in\Gamma]}_{\mathcal{A}_{s}^{\prime\prime}}.\]\[\mathcal{A}^{\prime}_{s} =\tilde{u}(\mathbf{\phi}^{(s)})+\left\langle\partial\tilde{u}_{s+1,n}( \mathbf{\phi}^{(s)}),\mathbb{E}[\mathbf{\phi}^{(s+1)}-\mathbf{\phi}^{(s)}\mid\mathbf{\phi}^{(s)},\mathbf{\phi}^{(s+1)}\in\Gamma]\right\rangle\] \[\quad+\frac{1}{2}\left\langle\partial^{2}\tilde{u}_{s+1,n}(\mathbf{ \phi}^{(s)}),\mathbb{E}[(\mathbf{\phi}^{(s+1)}-\mathbf{\phi}^{(s)})(\mathbf{\phi}^{(s+1)}- \mathbf{\phi}^{(s)})^{\top}\mid\mathbf{\phi}^{(s)},\mathbf{\phi}^{(s+1)}\in\Gamma]\right\rangle\] \[\quad+\mathcal{O}(\mathbb{E}[\|\mathbf{\phi}^{(s+1)}-\mathbf{\phi}^{(s)}\| _{2}^{3}\mid\mathbf{\phi}^{(s)},\mathbf{\phi}^{(s+1)}\in\Gamma]).\]

\[\mathcal{A}^{\prime\prime}_{s} =\tilde{u}(\mathbf{\phi}^{(s)})+\left\langle\partial\tilde{u}_{s+1,n} (\mathbf{\phi}^{(s)}),\mathbb{E}[\hat{\xi}_{s,s+1}-\mathbf{\phi}^{(s)}\mid\mathbf{\phi}^{( s)}\in\Gamma]\right\rangle\] \[\quad+\frac{1}{2}\left\langle\partial^{2}\tilde{u}_{s+1,n}(\mathbf{ \phi}^{(s)}),\mathbb{E}[(\hat{\xi}_{s,s+1}-\mathbf{\phi}^{(s)})(\hat{\xi}_{s,s+1}- \mathbf{\phi}^{(s)})^{\top}\mid\mathbf{\phi}^{(s)}\in\Gamma]\right\rangle\] \[\quad+\mathcal{O}(\mathbb{E}[\|\hat{\xi}_{s,s+1}-\mathbf{\phi}^{(s)}\| _{2}^{3}\mid\mathbf{\phi}^{(s)}\in\Gamma])\]

Substituting in \(\delta=\eta^{100}\) into Lemma E.1, we can conclude that, given \(\mathbf{\phi}^{(s)}\in\Gamma\), the event \(\{\mathbf{\phi}^{(s+1)}\in\Gamma\}\) happens with probability at least \(1-\eta^{100}\). We can replace the condition \(\mathbf{\phi}^{(s)},\mathbf{\phi}^{(s+1)}\in\Gamma\) with \(\mathbf{\phi}^{(s)}\in\Gamma\) in \(\mathcal{A}^{\prime}_{s}\) with an error of only \(\mathcal{O}(\eta^{100})\). Therefore,

\[\mathcal{A}_{s} =\left\langle\partial\tilde{u}_{s+1,n}(\mathbf{\phi}^{(s)}),\mathbb{E} [(\mathbf{\phi}^{(s+1)}-\mathbf{\phi}^{(s)})-(\hat{\xi}_{s,s+1}-\mathbf{\phi}^{(s)})\mid \mathbf{\phi}^{(s)}\in\Gamma]\right\rangle\] \[\quad+\frac{1}{2}\bigg{\langle}\partial^{2}\tilde{u}_{s+1,n}(\mathbf{ \phi}^{(s)}),\mathbb{E}[((\mathbf{\phi}^{(s)}-\mathbf{\phi}^{(s)})(\hat{\xi}_{s,s+1}- \mathbf{\phi}^{(s)})^{\top}\] \[\quad-(\hat{\xi}_{s,s+1}-\mathbf{\phi}^{(s)})(\hat{\xi}_{s,s+1}-\mathbf{ \phi}^{(s)})^{\top}\mid\mathbf{\phi}^{(s)}\in\Gamma]\bigg{\rangle}\] \[\quad+\mathcal{O}(\mathbb{E}[\|\mathbf{\phi}^{(s+1)}-\mathbf{\phi}^{(s)} \|_{2}^{3}\mid\mathbf{\phi}^{(s)}\in\Gamma])+\mathcal{O}(\mathbb{E}[\|\hat{\xi}_{s,s+1}-\mathbf{\phi}^{(s)}\|_{2}^{3}\mid\mathbf{\phi}^{(s)}\in\Gamma])+\mathcal{O}(\eta ^{100}).\]

Since \(\mathbf{\phi}^{(s)}\in\Gamma\) where \(\Gamma\) is a compact set, both \(\|\partial\tilde{u}_{s+1,n}(\mathbf{\phi}^{(s)})\|_{2}\) and \(\|\partial^{2}\tilde{u}_{s+1,n}(\mathbf{\phi}^{(s)})\|_{2}\) are bounded. Substituting Lemma E.3 and (18) to (23) to the expression of \(\mathcal{A}_{s}\), we have \(\mathcal{A}_{s}=\mathcal{O}(\alpha^{4})\) and thus \(|\mathcal{T}^{\prime}_{s}|=\mathcal{O}(\alpha^{4})\). Summing \(|\mathcal{T}^{\prime}_{s}|\) up, we have \(|\mathbb{E}[g(\mathbf{\phi}^{(n)})]-\mathbb{E}[g(\mathbf{\zeta}(n\alpha^{2}))]|\leq \mathcal{O}(n\alpha^{4})\leq\mathcal{O}(\alpha^{2})\), which completes the proof. 

## Appendix F Details for Communication Time Measurement

It is straightforward to measure the time duration for the entire training, but it is hard to directly measure the communication time due to the asynchronous nature of CUDA computation. Hence, in our experiments, we derive the communication time from the difference in total training time across runs with various communication frequencies.

Specifically, let \(T^{\text{tot}}_{\text{para}},T^{\text{tot}}_{H_{1}}\) be the total time durations of data parallel approaches and local gradient methods with \(H=H_{1}\), respectively. Also let \(T^{\text{comm}}_{\text{para}},T^{\text{comm}}_{H_{1}}\) be their communication times, and \(T^{\text{comp}}_{\text{para}},T^{\text{comp}}_{H_{1}}\) be their computation time. Ideally, setting the synchronization period to \(H_{1}\) reduces the communication volume exactly by a factor of \(\frac{1}{H_{1}}\), so these variables satisfy the following relationships:

\[T^{\text{comp}}_{H_{1}} =T^{\text{comp}}_{\text{para}},\] \[T^{\text{comm}}_{H_{1}} =\frac{1}{H_{1}}T^{\text{comm}}_{\text{para}},\] \[T^{\text{comm}}_{H_{1}} +T^{\text{comp}}_{H_{1}} =T^{\text{tot}}_{H_{1}},\] \[T^{\text{comm}}_{\text{para}} +T^{\text{comp}}_{\text{para}} =T^{\text{tot}}_{\text{para}}.\]

Then we can express the communication and computation times in terms of the total time duration \(T^{\text{tot}}_{\text{para}}\) and \(T^{\text{tot}}_{H_{1}}\):

\[T^{\text{comm}}_{\text{para}} =H_{1}T^{\text{comm}}_{H_{1}}=\frac{H_{1}}{H_{1}-1}(T^{\text{tot}} _{\text{para}}-T^{\text{tot}}_{H_{1}}),\] \[T^{\text{comp}}_{\text{para}} =T^{\text{comp}}_{H_{1}}=T^{\text{tot}}_{\text{para}}-T^{\text{ comm}}_{\text{para}}=\frac{H_{1}}{H_{1}-1}T^{\text{tot}}_{H_{1}}-\frac{1}{H_{1}-1}T^{ \text{tot}}_{\text{para}}.\]Therefore, we empirically measure the total time duration \(\tilde{T}^{\rm tot}_{\rm para}\) and \(\tilde{T}^{\rm tot}_{H_{1}}\) for some \(H_{1}\), then use the following formulas to obtain estimates of the communication and computation times:

\[\tilde{T}^{\rm comm}_{\rm para} =\frac{H_{1}}{H_{1}-1}(\tilde{T}^{\rm tot}_{\rm para}-\tilde{T}^{ \rm tot}_{H_{1}}), \tag{27}\] \[\tilde{T}^{\rm comp}_{\rm para} =\frac{H_{1}}{H_{1}-1}\tilde{T}^{\rm tot}_{H_{1}}-\frac{1}{H_{1} -1}\tilde{T}^{\rm tot}_{\rm para}. \tag{28}\]

These estimates are very predictive for the total time duration of local gradient methods with a different \(H\). For example, when \(H=H_{2}\), we can predict the total time duration \(T^{\rm tot}_{H_{2}}\) as follows:

\[T^{\rm comm}_{H_{2}} \approx\frac{1}{H_{2}}\tilde{T}^{\rm comm}_{\rm para}, \tag{29}\] \[T^{\rm tot}_{H_{2}} \approx\frac{1}{H_{2}}\tilde{T}^{\rm comm}_{\rm para}+\tilde{T}^ {\rm comp}_{\rm para}. \tag{30}\]

We find that the relative error \(\frac{|\tilde{T}^{\rm tot}_{H_{2}}-T^{\rm tot}_{H_{2}}|}{\tilde{T}^{\rm tot}_ {H_{2}}}\times 100\%\), where \(T^{\rm tot}_{H_{2}}\) denotes the measured total time, is only \(\sim 1\%\) across all configurations in Table 4, where we set \(H_{1}=2,H_{2}=4\) for ResNet-152 and \(H_{1}=4,H_{2}=8\) for ViT-B. The small relative error suggests that our method offers a close approximation to the actual time. For this reason, in Table 4, we report the communication time estimated by (27) and (29) for data-parallel approaches and local gradient methods with a constant synchronization period.

For QSR, since its communication volume relative to data parallel approaches, denoted as \(f_{\rm QSR}\), can be easily computed given the learning rate schedule, the growth coefficient \(\alpha\) and the base synchronization period \(H_{\rm base}\), we can estimate its communication time \(T^{\rm comm}_{\rm QSR}\) in a similar vein to (27) and (29):

\[T^{\rm comm}_{\rm QSR}\approx f_{\rm QSR}\tilde{T}^{\rm comm}_{\rm para}. \tag{31}\]

We report the communication time estimated by (31) in Table 4 for QSR.

## Appendix G Discussion on More Aggressive Scalings

Apart from the scalings discussed in Section 3, one can consider more aggressive scalings, e.g., \(H=\lfloor(\rho/\eta)^{-3}\rfloor\). Compared with QSR \(H=\lfloor(\alpha/\eta)^{-2}\rfloor\) that uses the same amount of communication, this cubic synchronization rule communicates more frequently at earlier stages but much less at later stages. Our theory in Theorem 3.1 suggests that taking \(H\sim\eta^{-3}\) blows up the approximation error, but as shown in Figure 6, this cubic rule with a properly tuned \(\rho\) can either outperform or underperform the QSR in test accuracy, depending on the training scenarios.

We argue that this is because our quasistatic view may break in the very late phase of cosine decay, where the learning rate decays so fast that \(\eta_{t}\) sees significant decay within a single communication round. As an example where the cubic rule performs better, we plot in Figure 8 the test accuracy

Figure 6: The cubic rule \(H=\lfloor(\frac{\rho}{\eta})^{3}\rfloor\) with a properly tuned \(\rho\) can either outperform or underperform the QSR in test accuracy, depending on the training scenarios.

curves of the QSR and cubic rule for training ViT-B with Local AdamW and batch size 4096. Same as our experiment setup in Section 4, the learning rate peaks at the value \(0.008\) and then decays to nearly zero (\(10^{-6}\)) following a cosine decay schedule. Setting \(H=\lfloor(0.0075/\eta)^{-3}\rfloor\) results in consistently worse test accuracy than QSR (with the same communication volume) before epoch 265. However, during the final communication round, which spans from epoch 265 to 300, the cubic rule catches up with QSR. During this period, the learning rate dramatically decreases from \(3.5\times 10^{-4}\) to nearly zero, but our quasistatic view assumes that the learning rate \(\eta_{t}\) should remain relatively constant for at least one communication round.

Based on the above observation, we argue that the cubic rule offers benefits over QSR only for certain schedules that have a rapid tail of learning rate decay near the end of training. To validate this view, we replace the cosine decay schedule with a variant of the step decay schedule in Smith et al. (2020). In our step decay schedule, given a total of 300 epochs, the learning rate remains at its peak until epoch 150, after which it is divided by 2 every 30 epochs. See Figure 7 for an illustration. Unlike the cosine schedule, this step decay schedule maintains a constant learning rate for a significant amount of time. As shown in Table 6(a), the cubic rule yields inferior generalization performance compared with our QSR, even after careful tuning of \(\rho\). See Appendix C.2 for training details.

Another way to corroborate our view is to run both scalings with a modified cosine learning rate schedule, which ceases to decay after a specific epoch \(t^{\prime\prime}\) and remains constant until training ends. See Figure 7 for an illustration of this modified cosine schedule. As shown in Table 6(b), QSR consistently outperforms the cubic rule across various choices of \(t^{\prime\prime}\). Further training details can be found in Appendix C.2. The probable reason is that when the learning rate is held constant, the cubic rule results in an excessively large \(H\), negatively impacting optimization.

Given these failure cases of the cubic rule, we generally recommend using the QSR and leave it to future work to design a better rule to deal with schedules that have a rapid tail of learning rate decay.

\begin{table}

\end{table}
Table 6: We validate that the higher test accuracy achieved by \(H\sim\eta^{-3}\) relies on the rapid decaying learning rate within a synchronization period via ablation studies on ViT-B. In Table 6(a), we replace the cosine decay schedule with a variant of the step decay schedule in Smith et al. (2020). In Table 6(b), we run both scalings with a modified cosine decay schedule that ceases to decay at some epoch \(t^{\prime}\). QSR consistently outperforms \(H\sim\eta^{-3}\) in both cases.

Figure 7: An illustration of the learning rate schedules.

## Appendix H Comparison with Local SGD/AdamW + SWAP

In this section, we compare QSR with the modified Stochastic Weight Averaging in Parallel (SWAP) algorithm, termed "Local SGD/AdamW + SWAP". Specifically, the original SWAP proposed by (Gupta et al., 2020) uses SGD for the majority of the training process and only switches to local updates at some \(t_{0}\) near the end, thus saving less communication than QSR. To compare SWAP with QSR at a similar level of communication volume, we experiment with the modified SWAP, which starts with Local SGD/AdamW using a constant communication period \(H_{\mathrm{base}}\) and, after some time \(t_{0}\), lets workers perform local updates with a final model averaging. As shown in Figure 9, QSR outperforms Local SGD/AdamW SWAP though we have tuned \(t_{0}\) carefully for the latter.

Figure 8: Test accuracy curves for QSR (\(\alpha=0.01\)) and the cubic rule (\(\rho=0.0075\)). The cubic rule results in consistently worse test accuracy than QSR (with the same communication volume) before the last communication round.

Figure 9: QSR outperforms Local SGD/AdamW + SWAP on both models. See Appendix C for training details.