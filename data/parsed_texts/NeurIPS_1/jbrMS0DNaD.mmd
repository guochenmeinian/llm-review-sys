[MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:2]

Our evaluation of multimodal retrieval methods demonstrates that Inquire poses a significant challenge, necessitating the development of models able to perform expert-level retrieval within large image collections. A key finding from our experiments is that reranking, a technique typically used in text retrieval [54][33][34], offers a promising avenue for improvement in image retrieval. We hope that Inquire will inspire the community to build next-generation image retrieval methods towards the ultimate goal of accelerating scientific discovery. We make Inquire, the iNat24 dataset, pre-computed outputs from state-of-the-art models, and code for evaluation available at [https://inquire-benchmark.github.io/](https://inquire-benchmark.github.io/)

## 2 Related Work

**Vision-Language Models (VLMs).** Large web-sourced datasets containing paired text and images have enabled recent advances in powerful VLMs [13][85]. Contrastive methods such as CLIP [60] and ALIGN [52], among others, learn an embedding space where the data from the two modalities can be encoded jointly. The ability to reason using natural language and images together has yielded impressive results in a variety of text-based visual tasks such as zero-shot classification [60][33], image captioning [39][40][41][42][43], and text-to-image generation [53][7][61][64][8]. However, the effectiveness of these contrastive VLMs for more complex compositional reasoning is bottlenecked by the information loss induced by their text encoders [33].

There also exists a family of more computationally expensive VLMs that connect the outputs of visual encoders directly into language models. Models like LLaVA [43][44], BLIP [39][40][41], and GPT-4o [35] have demonstrated impressive vision-language understanding. However, despite their potential for answering complex vision-language queries, these models are not suitable for processing large sets of images at interactive rates, which is essential for retrieval, due to their large computational requirements during inference. In this paper, we do not introduce new VLMs, but aim to better understand the capabilities and shortfalls of existing methods for text-to-image retrieval.

**Image Retrieval.** Effective feature representations are essential for achieving strong image retrieval performance. Earlier approaches from image-to-image used hand-crafted features [49][12] but these have largely been replaced with deep learning-based alternatives [56][63][11]. More recently, in the context of text-to-image retrieval, we have seen the adoption of contrastive VLMs [60][52] trained on web-sourced paired text and image datasets. These models enable zero-shot text-based retrieval and have been demonstrated to exhibit desirable scaling properties as training sets become larger [26][24]. However, despite the potential of VLMs for image retrieval, their evaluation has been mostly limited to small datasets adapted from existing image captioning benchmarks, such as Flickr30k [79] and COCO [42], which contain just 1,000 and 5,000 images, respectively. Furthermore, recent models are saturating performance on these less challenging datasets, e.g., BLIP-2 [40] scores 98.9 on Flickr30K and 92.6 on COCO top-10 text-to-image retrieval. As most text-to-image benchmarks have been derived from image captioning datasets, each query is a descriptive caption that matches exactly one image. In contrast, real-world retrievals often involve multiple images relevant to a single query, and the query itself typically does not describe every aspect of the images as thoroughly as a caption does. We compare Inquire to common text-to-image retrieval datasets in Table [1]

More recent datasets have been purpose-built to probe specific weaknesses of retrieval systems, such as compositionality [50][53][62], object relationships [82], negation [72][66], and semantic

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Dataset & Images & Queries & MpQ & Expert \\ \hline Flickr30k [79] & 1,000 & 5k & 1 & ✗ \\ COCO [42] & 5,000 & 25k & 1 & ✗ \\ \hline
**Inquire** & 5,000,000 & 250 & 1–1.5k & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison to common datasets used to evaluate text-to-image retrieval [26]. Unlike other datasets, Inquire has significantly more images and many matches per query rather than exactly one. _MpQ: Matches per query_

Figure 3: Proportion of queries in Inquire associated with each iconic group of species.

granularity [16]. [25] created a retrieval dataset for camera trap images, but use image captions that were automatically generated from a small set of discrete image attributes, limiting their utility beyond this set. The problem of fine-grained retrieval, where there may only be subtle visual differences between concepts of interest, has also been explored extensively [73]. However, typically these datasets convert existing classification datasets to the retrieval setting, resulting in small image pools and limited query diversity. Inquire addresses these shortcomings with a considerably larger image set and fine-grained queries that require advanced image understanding and domain expertise.

**Reranking.** In text retrieval, a common workflow is to first efficiently obtain an initial ranking of documents using pre-computed text embeddings and then _rerank_ the top retrievals with a more costly but sophisticated model [54]. While VLMs like CLIP [60] enable efficient image retrieval and more expensive models such as GPT-4o [55] could perform more complex ranking, this workflow has not been extensively explored in text-to-image applications primarily due to a lack of evaluation datasets. To this end, Inquire introduces a reranking challenge to drive further progress on this task.

**Expert-level Benchmarks.** Visual classification benchmarks have evolved from simply containing common everyday categories [13]. [42] to having more "expert-level" concepts [61]. Challenging datasets like the iNaturalist benchmarks [62]. [70], contain large class imbalances and fine-grained concepts that require expert-level knowledge to identify. The NeWT benchmark from [70] is similar in spirit to Inquire in that it proposes a collection of natural world questions. However, NeWT is a set of binary classification challenges, and while there is a variety of tasks, the majority of them are standard species classification. Further, NeWT uses a small (200-400) fixed set of positive and negative labeled images for each task, so it is not suitable for evaluating retrieval.

In general, evaluation benchmarks have struggled to keep pace with the growing capabilities of recent large models which perform very well on them [5]. For language, specific datasets have been developed to challenge common sense reasoning abilities [52]. Multimodal datasets have also been proposed to assess vision-language capabilities [73]. Nevertheless, these benchmarks test general skills in tasks that are not particularly challenging for humans and thus, are not testing a models' abilities in scenarios where expert-level knowledge is required.

To address the need for more difficult benchmarks, recent expert-level benchmarks have been devised for LLMs [54] and multimodal models [51]. For instance, MMMU [51] features questions that cover a range of college-level disciplines while Encyclopedic-VQA [51] comprises visual questions related to fine-grained entities which demand encyclopedic knowledge. The relatively low performance on these benchmarks, compared to human performance, highlights current limitations in multimodal models. However, there is no equivalent expert-level dataset for fine-grained text-to-image retrieval. Inquire fills this gap by providing a set of challenging and visually fine-grained retrieval questions focused on real-world tasks in retrieval from natural world image collections.

## 3 The Inquire Benchmark

Here we describe Inquire, our novel benchmark for assessing expert-level image retrieval for fine-grained queries on natural world image collections. Inquire consists of a collection of 250 queries, where each query is represented as a brief text description of the concept of interest (e.g., _"Alligator lizards mating" [52])_, and contains its relevant image matches comprehensively labeled over a dataset of five million natural world images. These queries represent real scientific use cases collected to cover diverse, expert sources including discussions with scientists across environmental and ecological

Figure 4: The Inquire benchmark consists of a full-dataset ranking task and a reranking task targeting different aspects of the image retrieval problem.

disciplines. Several examples of our queries are illustrated in Figure 1 with more in Appendix 2. Our queries challenge retrieval methods to demonstrate fine-grained detail recognition, compositional reasoning, character recognition, scene understanding, or natural world domain knowledge. While queries can require expert-level knowledge, the information needed to solve them is publicly available online and thus feasible for large web-trained models to learn. In this section, we detail the data sources utilized for the construction of Inquire, describe the data collection process, and introduce two image retrieval tasks -- Inquire-Fullrank and Inquire-Rerank -- that address different aspects of real-world text-to-image retrieval.

### The iNaturalist 2024 Dataset

As part of the Inquire benchmark, we create a new image dataset, which we refer to as iNaturalist 2024 (iNat24). This dataset contains five million images spanning 10,000 species classes collected and annotated by community scientists from 2021-2024 on the iNaturalist platform 2. iNat24 forms one of the largest publicly available natural world image repositories, with twice as many images as in iNat21 70. To ensure cross-compatibility for researchers interested in using both datasets, iNat24 and iNat21 have the same classes but do not contain the same images, freeing iNat21 to be used as a training set. The sampling and collection process of iNat24 is in Appendix 2.

### Query and Image Collection Process

Query Collection.To ensure that Inquire comprises text queries that are relevant to scientists, we conducted interviews with individuals across different ecological and environmental domains - including experts in ornithology, marine biology, entomology, and forestry. Further queries were sourced from reviews of academic literature in ecology 57. Representative queries and statistics can be seen in Figures 1, 2, and 3. We retained only queries that (1) could be discerned from images alone, (2) were feasible to comprehensively label over the entire iNat24 dataset, and (3) were of interest to domain experts.

Image Annotation.All image annotations were performed by a small set of individuals whose interest and familiarity with wildlife image collections enabled them to provide accurate labels for challenging queries. Annotators were instructed to label all candidate images as either _relevant_ (i.e., positive match) or _not relevant_ (i.e., negative match) to a query, and to mark an image as not relevant if there was reasonable doubt. To allow for comprehensive labeling, where applicable, iNat24 species labels were used to narrow down the search to a sufficiently small size to label all relevant images for the query of interest. For queries in which species labels could not be used, labeling was performed over the top CLIP ViT-H-14 [24] retrievals alone. In this case, the resulting annotations were only kept if we were certain that this labeling captured the vast majority of positives, including labeling until at least 100 consecutive retrievals were not relevant (see Appendix 3). Queries that were deemed too easy, not comprehensively labeled, or otherwise not possible to label were excluded from our benchmark. In total, this process resulted in 250 queries which involved labeling 194,334 images, of which 32,696 were relevant to their query. Further details are in Appendix 3.

Query Categories.Each query belongs to one of four supercategories (appearance, behavior, context, or species), and further into one of sixteen fine-grained categories (e.g., Animal Structures and Habitats). Figure 2 shows the distribution of query categories, and Figure 3 shows the distribution of iconic groups of the species represented by each query (e.g., Mammals, Birds). We also note queries that use scientific terminology, words typically used only within scientific contexts (e.g., "A godwit performing distal rhynchokinesis").

Data Split.We divide all queries into 50 validation and 200 test queries using a random split, stratified by category.

### Retrieval Tasks

We introduce two tasks to address different aspects of the text-to-image retrieval problem. Real-world retrieval implementations often consist of two stages: an initial top-k retrieval with a more computationally efficient method (e.g., CLIP zero-shot using pre-computed image embeddings), followed by a reranking of the top-k retrievals with a more expensive model. To enable researchers to explore both stages, while ensuring that those with more limited computational resources can participate, we follow previous large-scale reranking challenges like TREC [19, 20] by offering both a full dataset retrieval task and a reranking task (see Figure 3).

**Inquire-Fullrank.** The goal of this task is end-to-end retrieval, starting from the entire five million image iNat24 dataset. Progress on the full retrieval task can be made with better and more efficient ways to organize, process, filter, and search large image datasets. Although performance will increase with improvements to either of the two stages in a typical retrieval pipeline, we hope this task also encourages the development of retrieval systems beyond the two-stage approach.

**Inquire-Rerank.** This task evaluates reranking performance from a fixed initial ranking of 100 images. We believe that significant progress in retrieval will come from developing better reranking methods that re-order an initial retrieved subset. Thus, fixing the starting images for each query provides a consistent evaluation of reranking methods. This task also lowers the barrier to entry by giving researchers a considerably smaller set of top retrievals to work with, rather than requiring them to implement an end-to-end retrieval system. The top 100 ranked images for each query are retrieved using CLIP ViT-H-14 zero-shot retrieval on the entire iNat24 dataset. Consistent with previous large-scale reranking challenges [129][20][27], we retain only queries for which at least one positive image is among the top 100 retrieved images and no more than 50% of these top images are relevant. This ensures that the reranking evaluation remains meaningful and discriminative. This filtering process yields a task subset of 200 queries (reduced from our original 250 queries), split into 40 validation and 160 test queries according to the original validation/test split, with and 4,000 and 16,000 corresponding images, respectively.

## 4 Retrieval Methods

The goal of text-to-image retrieval is to rank images from a potentially large image collection according to their relevance to an input text query. Here, we describe the retrieval and reranking methods that we evaluate, covering current state-of-the-art approaches.

**Embedding Similarity.** Models such as CLIP [60] are well suited for the text-to-image retrieval setting as they operate on a joint vision and language embedding space. In this setting, similarity between an image and text query is simply determined by their cosine similarity. The key advantage of embedding models is that the embedding for each image can be pre-computed once offline as they do not change over time. At inference time, only the embedding of the text query needs to be computed and then compared to the cached image embeddings for retrieval. This is helpful as the number of images we wish to search over can be on the order of millions, or even billions [63]. Thus to speed up retrieval, the image embeddings can be pre-computed and indexed using approximate nearest neighbor methods [23], allowing for near-instantaneous retrievals on large collections. This is beneficial both for end-to-end retrieval and as the first step for a multi-stage retrieval approach. We also benchmark recent models such as WildCLIP [23] and BioCLIP [62] which are adapted versions of CLIP that explicitly target natural world use cases.

**Reranking with Multimodal Models.** Reranking is a common paradigm in text retrieval, where a rapid search through pre-computed document indexes for potential matches is followed by a more expensive reranking of the top retrievals [54][53][54]. In the image domain, reranking has been

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline
**Training dataset** & **Method** & **Params (M)** & **mAP@50** & **nDCG@50** & **MRR** \\ \hline WildCLIP [23] & CLIP ViT-B-16 & 150 & 7.4 & 16.1 & 0.33 \\ \hline BioCLIP [62] & CLIP ViT-B-16 & 150 & 5.0 & 8.6 & 0.17 \\ \hline \multirow{4}{*}{OpenAI [60]} & CLIP RN50 & 102 & 6.8 & 15.1 & 0.29 \\  & CLIP RN50x16 & 291 & 13.6 & 25.5 & 0.48 \\  & CLIP ViT-B-32 & 151 & 7.5 & 16.8 & 0.30 \\  & CLIP ViT-B-16 & 150 & 10.4 & 20.9 & 0.40 \\  & CLIP ViT-L-14 & 428 & 14.4 & 27.1 & 0.46 \\ \hline \multirow{3}{*}{DFN [24]} & CLIP ViT-B-16 & 150 & 15.1 & 28.1 & 0.48 \\  & CLIP ViT-L-14 & 428 & 23.1 & 37.3 & 0.54 \\  & CLIP ViT-H-14@378 & 987 & 33.3 & 48.8 & **0.69** \\ \hline WebL1 [83] & SigLIP ViT-L-16@384 & 652 & 31.1 & 46.6 & 0.68 \\  & SigLIP SQ400m-14@384 & 878 & **34.2** & **49.1** & **0.69** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Inquire-Fullrank retrieval performance for selected CLIP-style models. Larger models, trained on higher quality datasets, tend to achieve better performance.

[MISSING_PAGE_FAIL:7]

performance (see Figure 5), these results suggest that just scaling might not be enough, so future research should seek methods to better incorporate domain knowledge.

**Small models struggle to answer many queries.** In Table 2 we can see that CLIP RN50 and CLIP ViT-B-32 score an mAP@50 of just 7.6 and 8.2 respectively, demonstrating that these smaller models are unable to provide accurate retrievals for nearly all queries. Since the largest models get comparatively much higher scores, the queries are not impossible but rather difficult for smaller models. DFN ViT-B-16, trained with curated data, outperforms the larger OpenAI ViT-L-14, emphasizing the opportunity to improve the performance of efficient models via better data or training methods.

**High-quality training data is crucial for expert-level queries**. In Figure 5 left we show the retrieval performance on different supercategories for CLIP ViT-B/16 models that are trained on different datasets: BioCLIP [62], WildCLIP [25], OpenAI [60], and DFN [24]. The DFN model, trained on two billion filtered image-text pairs, is the best generalist model on OpenCLIP's benchmarks [21] and also outperforms all the others here, demonstrating the effectiveness of high quality pretraining. Conversely, models specifically trained on natural world data demonstrate degraded performance: BioCLIP was trained primarily on taxonomic captions and images, including iNat21, yet fails significantly on non-species queries, while WildCLIP has degraded performance in all supercategories. This performance emphasizes the need for better natural world models and fine-tuning strategies that can gain domain-specific expertise while preserving generalist capabilities.

**Reranking offers a valuable opportunity for improving retrieval.** Table 2 shows that reranking with larger models like VILA-40B and GPT-4o gives a significant performance boost in mAP@50 of 7 and 12 points, respectively. Still, even GPT-4o performs significantly worse than the best possible rerank of its initial CLIP ViT-H-14 ranking. Increasing the size of the initial retrieval set from 50 to 100 can further improve performance by surfacing more relevant images, but only higher-performing models benefit: The mAP@50 for GPT-4o increases by 5 points, while lower-performing models

Figure 5: **Left: CLIP zero-shot retrieval performance across supercategories using an identical backbone (ViT-B/16) trained or fine-tuned on different datasets. We see how training datasets have a significant effect on final performance, e.g., BioCLIP is tuned on natural world data at the expense of forgetting other categories. Right: CLIP retrieval performance of models trained on DFN [24].**

Figure 6: Retrieval performance for selected methods on Inquire query categories, ordered by difficulty. Some categories like Life Cycle and Development are exceptionally hard for current models. GPT-4o reranking improves performance in every category over its initial ViT-H-14 ranking.

like LLaVA-v1.6-7B see decreased performance. Further results for varying the initial ranking set size are in AppendixE. Figure 6 visualizes how GPT-4o reranking improves performance on every category compared to its initial ViT-H-14 ranking.

**Different query types present challenges of varying difficulties to existing models.** Figure 6 illustrates the difference in performance across query categories. We see that Appearance queries, which often require both domain knowledge of an organism's appearance and the fine-grained visual reasoning to recognize them, are the most difficult for existing models. Indeed, the Life Cycle and Development set (e.g., _"Immature bald eagle", "A cicada in the process of shedding its exoskeleton"_) are by far the most difficult. Conversely, Context queries such those in the Human Impact set (e.g., _"leopard on a road"_, _"bird caught in a net"_), for which less expertise and comparatively coarser image understanding are needed, are easier for existing models.

### Rerank Retrieval Task Results

The results for the INQUIRE-Rerank task are presented in Table 7 where we evaluate reranking performance of both CLIP-style models like ViT-B-32 and larger vision-language models such as GPT-4o. Since the total number of images for each query is small (i.e., 100), we also show the expected results of a random reranking for baseline comparison. In Table 8 we further break down INQUIRE-Rerank results by queries containing scientific terminology and by query supercategory.

**Current models struggle with expert-level text-to-image retrieval on Inquire.** In Table 7 we observe that the highest average precision score of 59.6, achieved by GPT-4o, is far below the perfect score of 100, showing substantial room for improvement. Smaller models like CLIP ViT-B-32 only slightly outperform random chance. Since the top retrieved are often visually or semantically similar, lower-performing models may be confused into promoting irrelevant images, leading to poorer ranking.

**Queries with scientific terminology are significantly more challenging, showing that models might not understand domains-specific language**. For example, the query "_Axanthism in a green frog"_--referring to a mutation limiting yellow pigment production, resulting in a blue appearance--uses specialized terminology that a model may not understand. As a result, a model may incorrectly rank typical green frogs higher than axanthic green frogs, leading to worse-than-random performance. We show the performance of reranking models on queries with scientific terminology in Table 5. Interestingly, GPT-4o appears to be closing this gap, with an average difference of 7 points between queries with and without scientific terminology (AP scores of 53 and 60, respectively), compared to a 16-point difference for the next best model, VIIA-40B (AP of 39 and 55). Nevertheless, this gap remains. Future work should explore methods to improve models' comprehension of domain-specific language, which is critical for accurate retrieval in scientific contexts.

**Reranking effectiveness varies widely by the query type.** Table 8 shows that Context queries, often requiring general visual understanding, benefit substantially from reranking. Conversely, Species queries, requiring fine-grained visual understanding, see minimal improvement, with the

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Method** & **AP** & **nDCG** & **MRR** \\ \hline _Random_ & 22.1 & 52.6 & 0.35 \\ \hline _Embedding models_ & & & \\ CLIP ViT-B-32 & 30.2 & 59.1 & 0.47 \\ CLIP ViT-L-14 & 36.8 & 64.2 & 0.57 \\ CLIP ViT-H-14 & 42.6 & 68.7 & 0.66 \\ SigLIP SQA00m-14 & **53.1** & **50.1** & **73.5** & **0.72** \\ \hline _Proprietary multimodal models_ & & & \\ GPT-4V & 47.8 & 71.9 & 0.70 \\ GPT-4o & **59.6** & **78.9** & **0.78** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results for the Inquire-Rerank task on various embedding and multimodal models. For each task, a fixed set of the top-100 images is provided, which we then rerank using different methods. Evaluation metrics are calculated based solely on this fixed set, disregarding any potential positives outside of the top-100 images. Therefore, a perfect score is achievable within this context.

specialized BioCLIP beating out even GPT-4o. These trends suggest that while recent models have better generalized vision capabilities, they continue to struggle with fine-grained visual understanding.

## 6 Limitations and Societal Impact

While the species labels for each image in iNat24 are generated via consensus from multiple citizen scientists, there may still be errors in the labels which our evaluation will inherit. However, this error rate is estimated to be low [47]. Inquire contains natural world images, which while diverse, may hinder the relevance of some of our insights to other visual domains. In spite of this, we believe that due to the wide range of visual queries contained within, progress on Inquire will likely be indicative of multimodal model performance on other challenging domains.

There could be unintended negative consequences if conservation assessments were made based on the predictions from biased or inaccurate models evaluated in this paper. Where relevant, we have attempted to flag these performance deficiencies. While we have filtered out personally identifiable information from our images, the retrieval paradigm allows for free-form text search and thus care should be taken to ensure that appropriate text filters are in-place to prevent inaccurate or hurtful associations being made between user queries and images of wildlife.

## 7 Conclusion

We introduced Inquire, a challenging new text-to-image retrieval benchmark which consists of expert-level text queries that have been exhaustively annotated across a large pool of five million natural world images called iNat24. This benchmark aims to emulate real world image retrieval and analysis problems faced by scientists working with these types of large-scale image collections. Our hope is that progress on Inquire will drive advancements in the real scientific utility of AI systems. Our evaluation of existing methods reveals that Inquire poses a significant challenge even for the current largest state-of-the-art multimodal models, showing there is significant room for innovations to develop accurate retrieval systems for complex visual domains.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{**By Lingo**} & \multicolumn{3}{c}{**By Supercategory**} \\ \cline{2-7} Model & Lingo & No Lingo & Appearance & Behavior & Context & Species \\ \hline WildCLIP & 20.8 & 32.0 & 31.8 & 29.8 & 35.7 & 36.0 \\ BioCLIP & 17.2 & 30.3 & 27.8 & 25.6 & 31.3 & **44.8** \\ CLIP ViT-B-32 & 22.8 & 31.6 & 29.5 & 31.0 & 36.0 & 35.3 \\ CLIP ViT-L-14 & 29.2 & 37.4 & 37.2 & 36.2 & 40.2 & 38.2 \\ CLIP ViT-H-14 & 32.1 & 44.0 & 38.1 & 50.5 & 45.5 & 31.2 \\ SigLIP SO400m-14 & **38.3** & **51.7** & **51.7** & **53.6** & **54.1** & 44.6 \\ \hline LLaVA-v1.6-34B & 28.2 & 49.5 & 41.0 & 48.2 & 53.8 & **37.5** \\ VILA-13B & 37.2 & 48.2 & 39.3 & 47.1 & 58.6 & 34.8 \\ VILA-40B & **38.6** & **54.5** & **46.9** & **54.9** & **63.1** & 37.0 \\ \hline GPT-4V & 35.9 & 49.3 & 40.3 & 50.2 & 54.2 & 39.0 \\ GPT-4o & **53.3** & **60.4** & **51.9** & **61.4** & **75.4** & **44.3** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Evaluation of INQUIRE-Rerank with queries grouped into different query types. First, we group queries containing scientific lingo and no scientific lingo. Next, we group queries by their supercategory (Appearance, Behavior, Context, Species). Queries with lingo tend to be more difficult, especially for large models with good generalist understanding but lacking domain expertise. All results are reported in AP.

### Acknowledgments and Disclosure of Funding

We wish to thank the many iNaturalist participants for continuing to share their data and also the numerous individuals who provided suggestions for search queries. Special thanks to Kayleigh Neil, Benat Yanez Iturbe-Ormaeche, Filip Dorm, and Patricia Mrazek for data annotation. Funding for annotation was provided by the Generative AI Laboratory (GAIL) at the University of Edinburgh. EV and SB were supported in part by the Global Center on AI and Biodiversity Change (NSF OISE-2330423 and NSERC 585136). OMA was in part supported by a Royal Society Research Grant. OP and KJ were supported by the Biome Health Project funded by WWF-UK.

## References

* [1] iNaturalist Challenge Datasets. [https://github.com/visipedia/inat_comp](https://github.com/visipedia/inat_comp). accessed June 5, 2024.
* [2] iNaturalist. [https://www.inaturalist.org](https://www.inaturalist.org). accessed June 5, 2024.
* [3] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Al-tenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. _arXiv:2303.08774_, 2023.
* [4] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. _NeurIPS_, 2022.
* [5] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. Vqa: Visual question answering. In _ICCV_, 2015.
* [6] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic. Netvlad: Cnn architecture for weakly supervised place recognition. In _CVPR_, 2016.
* [7] O. Avrahami, D. Lischinski, and O. Fried. Blended diffusion for text-driven editing of natural images. In _CVPR_, 2022.
* [8] O. Avrahami, T. Hayes, O. Gafni, S. Gupta, Y. Taigman, D. Parikh, D. Lischinski, O. Fried, and X. Yin. Spatext: Spatio-textual representation for controllable image generation. In _CVPR_, 2023.
* [9] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky. Neural codes for image retrieval. In _ECCV_, 2014.
* [10] R. Baeza-Yates, B. Ribeiro-Neto, et al. _Modern information retrieval_. 1999.
* [11] V. Balntas, E. Riba, D. Ponsa, and K. Mikolajczyk. Learning local feature descriptors with triplets and shallow convolutional neural networks. In _BMVC_, 2016.
* [12] H. Bay, T. Tuytelaars, and L. Van Gool. Surf: Speeded up robust features. In _ECCV_, 2006.
* [13] L. Beyer*, A. Steiner*, A. S. Pinto*, A. Kolesnikov*, X. Wang*, D. Salz, M. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello, et al. PaliGEMma: A versatile 3B VLM for transfer. _arXiv preprint arXiv:2407.07726_, 2024.
* [14] M. Boratko, X. L. Li, R. Das, T. O'Gorman, D. Le, and A. McCallum. Protoqa: A question answering dataset for prototypical common-sense reasoning. In _EMNLP_, 2020.
* [15] F. Bordes, R. Y. Pang, A. Ajay, A. C. Li, A. Bardes, S. Petryk, O. Manas, Z. Lin, A. Mahmoud, B. Jayaraman, et al. An introduction to vision-language modeling. _arXiv:2405.17247_, 2024.
* [16] M. Chandler, L. See, K. Copas, A. M. Bonde, B. C. Lopez, F. Danielsen, J. K. Legind, S. Masinde, A. J. Miller-Rushing, G. Newman, et al. Contribution of citizen science towards international biodiversity monitoring. _Biological Conservation_, 2017.
* [17] M. Cherti, R. Beaumont, R. Wightman, M. Wortsman, G. Ilharco, C. Gordon, C. Schuhmann, L. Schmidt, and J. Jitsev. Reproducible scaling laws for contrastive language-image learning. In _CVPR_, 2023.
* [18] N. Craswell and S. Robertson. _Average Precision at \(n\)_. 2009.

* [19] N. Craswell, B. Mitra, E. Yilmaz, D. Campos, and E. M. Voorhees. Overview of the trec 2019 deep learning track. _arXiv:2003.07820_, 2020.
* [20] N. Craswell, B. Mitra, E. Yilmaz, and D. Campos. Overview of the trec 2020 deep learning track. _arXiv:2102.07662_, 2021.
* [21] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. _NeurIPS_, 2023.
* [22] J. Deng, J. Guo, E. Ververas, I. Kotsia, and S. Zafeiriou. Retinaface: Single-shot multi-level face localisation in the wild. In _CVPR_, 2020.
* [23] M. Douze, A. Guzhva, C. Deng, J. Johnson, G. Szilvasy, P.-E. Mazare, M. Lomeli, L. Hosseini, and H. Jegou. The Faiss library. _arXiv:2401.08281_, 2024.
* [24] A. Fang, A. M. Jose, A. Jain, L. Schmidt, A. Toshev, and V. Shankar. Data filtering networks. In _ICLR_, 2024.
* [25] V. Gabeff, M. Russwurm, D. Tuia, and A. Mathis. Wildclip: Scene and animal attribute retrieval from camera trap data with domain-adapted vision-language models. _IJCV_, 2024.
* [26] S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. _NeurIPS_, 2023.
* [27] D. Hawking, N. Craswell, P. Bailey, and K. Griffiths. Measuring search engine quality. _Information retrieval_, 2001.
* [28] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In _ICLR_, 2021.
* [29] C.-Y. Hsieh, J. Zhang, Z. Ma, A. Kembhavi, and R. Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. _NeurIPS_, 2023.
* [30] X. Hu, Z. Gan, J. Wang, Z. Yang, Z. Liu, Y. Lu, and L. Wang. Scaling up vision-language pre-training for image captioning. In _CVPR_, 2022.
* [31] G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave, V. Shankar, H. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and L. Schmidt. Openclip, 2021.
* [32] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _ICML_, 2021.
* [33] A. Kamath, J. Hessel, and K.-W. Chang. Text encoders bottleneck compositionality in contrastive vision-language models. In _EMNLP_, 2023.
* [34] V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. In _EMNLP_, 2020.
* [35] O. Khattab and M. Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In _SIGIR Conference on Research and Development in Information Retrieval_, 2020.
* [36] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. _NeurIPS_, 2012.
* [37] D. Lawrie, S. MacAvaney, J. Mayfield, P. McNamee, D. W. Oard, L. Soldaini, and E. Yang. Overview of the TREC 2023 NeuCLIR Track. _arXiv:2404.08071_, 2024.
* [38] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. In _CVPR_, 2024.
* [39] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _ICML_, 2022.

* Li et al. [2023] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _ICML_, 2023.
* Lin et al. [2024] J. Lin, H. Yin, W. Ping, P. Molchanov, M. Shoeybi, and S. Han. Vila: On pre-training for visual language models. In _CVPR_, 2024.
* Lin et al. [2014] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft coco: Common objects in context. In _ECCV_, 2014.
* Liu et al. [2023] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In _NeurIPS_, 2023.
* Liu et al. [2024] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. In _CVPR_, 2024.
* Liu et al. [2024] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge, 2024. URL [https://llava-vl.github.io/blog/2024-01-30-llava-next](https://llava-vl.github.io/blog/2024-01-30-llava-next)
* Liu et al. [2024] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mmbench: Is your multi-modal model an all-around player? In _ECCV_, 2024.
* Loarie [2024]_S. Loarie. iNaturalist Observation Accuracy Experiment (0.3), 2024. URL [https://www.inaturalist.org/observation_accuracy_experiments/4](https://www.inaturalist.org/observation_accuracy_experiments/4)
* Lohan [2024] T. Lohan. From observation to action: How inaturalist spurs conservation, Jan 2024. URL [https://therevelator.org/inaturalist-conservation/](https://therevelator.org/inaturalist-conservation/)
* Lowe [2004] D. G. Lowe. Distinctive image features from scale-invariant keypoints. _IJCV_, 2004.
* Ma et al. [2023] Z. Ma, J. Hong, M. O. Gul, M. Gandhi, I. Gao, and R. Krishna. Crepe: Can vision-language foundation models reason compositionally? In _CVPR_, 2023.
* Mensink et al. [2023] T. Mensink, J. Uijlings, L. Castrejon, A. Goel, F. Cadar, H. Zhou, F. Sha, A. Araujo, and V. Ferrari. Encyclopedic vqa: Visual questions about detailed properties of fine-grained categories. In _ICCV_, 2023.
* Mialon et al. [2023] G. Mialon, C. Fourrier, T. Wolf, Y. LeCun, and T. Scialom. Gaia: a benchmark for general ai assistants. In _ICLR_, 2023.
* Nichol et al. [2022] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever, and M. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In _ICML_, 2022.
* Nogueira and Cho [2019] R. Nogueira and K. Cho. Passage re-ranking with bert. _arXiv:1901.04085_, 2019.
* OpenAI [2024] OpenAI. GPT-4o, 2024. URL [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)

Accessed June 5, 2024.
* Pauly [2018]_G. Pauly. We want your photos of alligator lizard sex, 2018_. URL [https://nhm.org/stories/we-want-your-photos-alligator-lizard-sex](https://nhm.org/stories/we-want-your-photos-alligator-lizard-sex)

* Pernat et al. [2024] N. Pernat, S. Canavan, M. Golivets, J. Hillaert, Y. Itescu, I. Jaric, H. M. Mann, P. Pipek, C. Preda, D. M. Richardson, et al. Overcoming biodiversity blindness: Secondary data in primary citizen science observations. _Ecological Solutions and Evidence_, 2024.
* Philbin et al. [2007] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman. Object retrieval with large vocabularies and fast spatial matching. In _CVPR_, 2007.
* Philbin et al. [2008] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman. Lost in quantization: Improving particular object retrieval in large scale image databases. In _CVPR_, 2008.
* Radford et al. [2021] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.

* [61] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. _arXiv:2204.06125_, 2022.
* [62] A. Ray, F. Radenovic, A. Dubey, B. Plummer, R. Krishna, and K. Saenko. cola: A benchmark for compositional text-to-image retrieval. _NeurIPS_, 2023.
* [63] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. _IJCV_, 2015.
* [64] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _NeurIPS_, 2022.
* [65] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _NeurIPS_, 2022.
* [66] J. Singh, I. Shrivastava, M. Vatsa, R. Singh, and A. Bharati. Learn" no" to say" yes" better: Improving vision-language models via negations. _arXiv:2403.20312_, 2024.
* [67] S. Stevens, J. Wu, M. J. Thompson, E. G. Campolongo, C. H. Song, D. E. Carlyn, L. Dong, W. M. Dahdul, C. Stewart, T. Berger-Wolf, W.-L. Chao, and Y. Su. BioCLIP: A vision foundation model for the tree of life. In _CVPR_, 2024.
* [68] G. Van Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeirotis, P. Perona, and S. Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In _CVPR_, 2015.
* [69] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam, P. Perona, and S. Belongie. The inaturalist species classification and detection dataset. In _CVPR_, 2018.
* [70] G. Van Horn, E. Cole, S. Beery, K. Wilber, S. Belongie, and O. Mac Aodha. Benchmarking representation learning for natural world image collections. In _CVPR_, 2021.
* [71] E. M. Voorhees, D. K. Harman, et al. _TREC: Experiment and evaluation in information retrieval_. 2005.
* [72] H. Wang, Y. Li, H. Yao, and X. Li. Clipn for zero-shot ood detection: Teaching clip to say no. In _ICCV_, 2023.
* [73] X.-S. Wei, Y.-Z. Song, O. Mac Aodha, J. Wu, Y. Peng, J. Tang, J. Yang, and S. Belongie. Fine-grained image analysis with deep learning: A survey. _PAMI_, 2021.
* [74] T. Weyand, A. Araujo, B. Cao, and J. Sim. Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval. In _CVPR_, 2020.
* [75] P. Xu, W. Shao, K. Zhang, P. Gao, S. Liu, M. Lei, F. Meng, S. Huang, Y. Qiao, and P. Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. _arXiv:2306.09265_, 2023.
* [76] Z. Xu, Y. Zhu, S. Deng, A. Mittal, Y. Chen, M. Wang, P. Favaro, J. Tighe, and D. Modolo. Benchmarking zero-shot recognition with vision-language models: Challenges on granularity and specificity. In _CVPR_, 2024.
* [77] Z. Yin, J. Wang, J. Cao, Z. Shi, D. Liu, M. Li, X. Huang, Z. Wang, L. Sheng, L. Bai, et al. Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark. _NeurIPS_, 2023.
* [78] N. E. Young, M. Fairchild, T. Belcher, P. Evangelista, C. J. Verdone, and T. J. Stohlgren. Finding the needle in the haystack: iterative sampling and modeling for rare taxa. _Journal of Insect Conservation_, 2019.
* [79] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _TACL_, 2014.

* [80] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastive captioners are image-text foundation models. In _TMLR_, 2022.
* [81] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In _CVPR_, 2024.
* [82] M. Yuksekgonul, F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In _ICLR_, 2022.
* [83] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training. In _ICCV_, 2023.
* [84] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. Agieval: A human-centric benchmark for evaluating foundation models. _arXiv:2304.06364_, 2023.
* [85] Y. Zong, O. Mac Aodha, and T. Hospedales. Self-supervised multimodal learning: A survey. _PAMI_, 2024.