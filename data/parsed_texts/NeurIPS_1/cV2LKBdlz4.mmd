# On Statistical Rates and Provably Efficient Criteria

of Latent Diffusion Transformers (DiTs)

 Jerry Yao-Chieh Hu1,2 Weimin Wu1,2 Zhuoru Li3

Sophia Pi3 Zhao Song4 Han Liu2

2Center for Foundation Models and Generative AI, 3Department of Computer Science, 3Department of Statistics

and Data Science, Northwestern University, Evanston, IL 60208, USA

4Simons Institute for the Theory of Computing, UC Berkeley, Berkeley, CA 94720, USA

{jhu,wum}@u.northwestern.edu;

magic.linuukde@gmail.com; hanliu@northwestern.edu

Equal contribution. Version: January 3, 2025. Future updates are on arXiv.

Footnote 1: footnotemark:

###### Abstract

We investigate the statistical and computational limits of latent Diffusion Transformers (DiTs) under the low-dimensional linear latent space assumption. Statistically, we study the universal approximation and sample complexity of the DiTs score function, as well as the distribution recovery property of the initial data. Specifically, under mild data assumptions, we derive an approximation error bound for the score network of latent DiTs, which is sub-linear in the latent space dimension. Additionally, we derive the corresponding sample complexity bound and show that the data distribution generated from the estimated score function converges toward a proximate area of the original one. Computationally, we characterize the hardness of both forward inference and backward computation of latent DiTs, assuming the Strong Exponential Time Hypothesis (SETH). For forward inference, we identify efficient criteria for all possible latent DiTs inference algorithms and showcase our theory by pushing the efficiency toward almost-linear time inference. For backward computation, we leverage the low-rank structure within the gradient computation of DiTs training for possible algorithmic speedup. Specifically, we show that such speedup achieves almost-linear time latent DiTs training by casting the DiTs gradient as a series of chained low-rank approximations with bounded error. Under the low-dimensional assumption, we show that the statistical rates and the computational efficiency are all dominated by the dimension of the subspace, suggesting that latent DiTs have the potential to bypass the challenges associated with the high dimensionality of initial data.

## 1 Introduction

We investigate the statistical and computational limits of latent diffusion transformers (DiTs), assuming the data is supported on an unknown low-dimensional linear subspace. This analysis is not only practical but also timely. On one hand, DiTs have demonstrated revolutionary success in generative AI and digital creation by using Transformers as score networks [Eser et al., 2024, Ma et al., 2024, Chen et al., 2024a, Mo et al., 2023, Peebles and Xie, 2023]. On the other hand, they require significant computational resources [Liu et al., 2024], making them challenging to train outside of specialized industrial labs. Therefore, it is natural to ask whether it is possible to make them lighter and faster without sacrificing performance. Answering these questions requires a fundamental understanding of the DiT architecture. This work provides a timely theoretical analysis of the fundamental limits of DiT architecture, aided by the analytical feasibility provided by the low-dimensional data assumption.

Empirically, Latent Diffusion is a go-to design for effectiveness and computational efficiency (Rombach et al., 2022; Liu et al., 2021; Pope et al., 2021; Su and Wu, 2018). Theoretically, it is capable of hosting the assumption of low-dimensional data structure (see Assumption 2.1 for formal definition) for detailed analytical characterization (Chen et al., 2023; Bortoli, 2022). In essence, diffusion models with low-dimensional data structures manifest a natural lower-dimensional diffusion process through an encoder/decoder within a robust and informative latent representation feature space (Rombach et al., 2022; Pope et al., 2021). Such lower-dimensional diffusion improves computational efficiency by reducing data complexity without sacrificing essential information (Liu et al., 2021). With this assumption, Chen et al. (2023) decompose the score function of U-Net based diffusion models into on-support and orthogonal components. This decomposition allows for the characterization of the distinct behaviors of the two components: the on-support component facilitates latent distribution learning, while the orthogonal component facilitates subspace recovery.

In our work, we utilize low-dimensional data structure assumption to explore statistical and computational limits of latent DiTs. Our analysis includes the characterizations of statistical rates and provably efficient criteria. Statistically, we pose two questions and provide a theory to characterize the statistical rates of latent DiT under the assumption of a low-dimensional data:

**Question 1**.: What is the approximation limit of using transformers to approximate the DiT score function, particularly in the low-dimensional data subspace?

**Question 2**.: How accurate is the estimation limit for such a score estimator in practical training scenarios? With the score estimator, how well can diffusion transformers recover the data distribution?

Computationally, the primary challenge of DiT lies in the transformer blocks' quadratic complexity. This computational burden applies to both inference and training, even with latent diffusion. Thus, it is essential to design algorithms and methods to circumvent this \(\Omega(L^{2})\) where \(L\) is the latent DiT sequence length. However, there are no formal results to support and characterize such algorithms. To address this gap, we pose the following questions and provide a fundamental theory to fully characterize the complexity of latent DiT under the low-dimensional linear subspace data assumption:

**Question 3**.: Is it possible to improve the \(\Omega(L^{2})\) time complexity with a bounded approximation error for both forward and backward passes? What is the computational limit for such an improvement?

Contributions.We study the fundamental limits of latent DiT. Our contributions are threefold:

* **Score Approximation.** We address Question 1 by characterizing the approximation limit of matching the DiT score function with a transformer-based score estimator. Specifically, under mild data assumptions, we derive an approximation error bound for the score network, sub-linear in the latent space dimension (Theorem 3.1). These results not only explain the expressiveness of latent DiT (under mild assumptions) but also provide guidance for the structural configuration of the score network for practical implementations (Theorem 3.1).
* **Score and Distribution Estimation.** We address Question 2 by exploring the limitations of score and distribution estimations of latent DiTs in practical training scenarios. Specifically, we provide a sample complexity bound for score estimation (Theorem 3.2), using norm-based covering number bound of transformer architecture. Additionally, we show that the learned score estimator is able to recover the initial data distribution (Corollary 3.2.1).
* **Provably Efficient Criteria and Existence of Almost Linear Time Algorithms.** We address Question 3 by providing provably efficient criteria for latent DiTs in both forward inference and backward computation/training. For forward inference, we characterize all possible efficient DiT algorithms using a norm-based efficiency threshold for both conditional and unconditional generation (Proposition 4.1). Efficient algorithms, including almost-linear time algorithms (Proposition 4.2), are possible only below this threshold. For backward computation, we prove the existence of almost-linear time DiT training algorithms (Theorem 4.1) by utilizing the inherent low-rank structure in DiT gradients through a chained low-rank approximation.

Interestingly, both our statistical and computational results are dominated by the subspace dimension under the low-dimensional assumption, suggesting that latent DiT can potentially bypass the challenges associated with the high dimensionality of initial data.

Organization.Section 2 includes background on score decomposition and Transformer-based score networks. Section 3 includes DiTs' statistical rates. Section 4 includes DiTs' provably efficient criteria. Section 5 includes concluding remarks. We defer discussions of related works to Appendix C.

Notations.We use lower case letters to denote vectors, e.g., \(z\in\mathbb{R}^{D}\). \(\left\lVert z\right\rVert_{2}\) and \(\left\lVert z\right\rVert_{\infty}\) denote its Euclidean norm and Infinite norm respectively. We use upper case letters to denote matrix, e.g., \(Z\in\mathbb{R}^{d\times L}\). \(\left\lVert Z\right\rVert_{2}\), \(\left\lVert Z\right\rVert_{\mathrm{op}}\), and \(\left\lVert Z\right\rVert_{F}\) denote the \(2\)-norm, operator norm and Frobenius norm respectively. \(\left\lVert Z\right\rVert_{p,q}\) denotes the \(p,q\)-norm where the \(p\)-norm is over columns and \(q\)-norm is over rows. Given a function \(f\), let \(\left\lVert f(x)\right\rVert_{L^{2}}\coloneqq(\int\left\lVert f(x)\right\rVert _{2}^{2}\mathrm{d}x)^{1/2}\), and \(\left\lVert f(\cdot)\right\rVert_{Lip}=\sup_{x\neq y}(\left\lVert f(x)-f(y) \right\rVert_{2}/\left\lVert x-y\right\rVert_{2})\). With a distribution \(P\), we denote \(\left\lVert f\right\rVert_{L^{2}(P)}=(\int_{p}\left\lVert f(x)\right\rVert_{2} ^{2}\mathrm{d}x)^{1/2}\) as the \(L^{2}(P)\) norm. Let \(f_{\sharp}P\) be a pushforward measure, i.e., for any measurable \(\Omega\), \((f_{\sharp}P)(\Omega)=P(f^{-1}(\Omega))\). We use \(\psi\) for (conditional) Gaussian density functions.

## 2 Background

This section reviews the ideas we built on, including an overview of diffusion models (Section 2.1), the score decomposition under the linear latent space assumption (Section 2.2), and the transformer backbone in DiT (Section 2.3).

### Score-Matching Denoising Diffusion Models

We briefly review forward process, backward process and score matching in diffusion models.

Forward and Backward Process.In the **forward** process, Diffusion models gradually add noise to the original data \(x_{0}\in\mathbb{R}^{D}\), and \(x_{0}\sim P_{0}\). Let \(x_{t}\) denote the noisy data at the timestamp \(t\), with marginal distribution and destiny as \(P_{t}\) and \(p_{t}\). The conditional distribution \(P(x_{t}|x_{0})\) follows \(N(\beta(t)x_{0},\sigma(t)I_{D})\), where \(\beta(t)=\exp(-\int_{0}^{t}w(s)\mathrm{d}s/2)\), \(\sigma(t)=1-\beta^{2}(t)\), and \(w(t)>0\) is a nondecreasing weighting function. In practice, the forward process terminates at a large enough \(T\) such that \(P_{T}\) is close to \(N(0,I_{D})\). In the **backward** process, we obtain \(y_{t}\) by reversing the forward process. The generation of \(y_{t}\) depends on the score function \(\nabla\log p_{t}(\cdot)\). However, this is unknown in practice, we use a score estimator \(s_{W}(\cdot,t)\) to replace \(\nabla\log p_{t}(\cdot)\), where \(s_{W}(\cdot,t)\) is usually a neural network with parameters \(W\). See Appendix D.1 for the details.

Score Matching.To estimate the score function, we use the following loss

\[\min_{W}\int_{T_{0}}^{T}\gamma(t)\mathbb{E}_{x_{t}\sim P_{t}}\left[\left\lVert s _{W}(x_{t},t)-\nabla\log p_{t}(x_{t})\right\rVert_{2}^{2}\right]\mathrm{d}t,\]

where \(\gamma(t)\) is the weight function, and \(T_{0}\) is a small value to stabilize training and prevent score function from blowing up (Vahdat et al., 2021). However, it is hard to compute \(\nabla\log p_{t}(\cdot)\) with available data samples. Therefore, we minimize the equivalent denoising score matching objective

\[\min_{W}\int_{T_{0}}^{T}\gamma(t)\mathbb{E}_{x_{0}\sim P_{0}}\left[\mathbb{E}_ {x_{t}|x_{0}}\left[\left\lVert s_{W}(x_{t},t)-\nabla_{x_{t}}\log\psi_{t}(x_{t }\mid x_{0})\right\rVert_{2}^{2}\right]\right]\mathrm{d}t, \tag{2.1}\]

where \(\psi_{t}(x_{t}|x_{0})\) is the transition kernel, then \(\nabla_{x_{t}}\log\psi_{t}(x_{t}|x_{0})=(\beta(t)x_{0}-x_{t})\left/\sigma(t)\).

To train the parameters \(W\) in the score estimator \(s_{W}(\cdot,t)\), we use the empirical version of (2.1). We select \(n\) i.i.d. data samples \(\{x_{0,i}\}_{i=1}^{n}\sim P_{0}\), and sample time \(t_{i}\)\((1\leq i\leq n)\) uniformly from interval \([T_{0},T]\). Given \(x_{0,i}\), we sample \(x_{t_{i}}\) from \(N(\beta(t_{i})x_{0,i},\sigma(t_{i})I_{D})\). The empirical loss is

\[\widehat{\mathcal{L}}(W)=\frac{1}{n}\sum_{i=1}^{n}\left\lVert s_{W}(x_{t_{i}},t_{i})-x_{0,i}\right\rVert_{2}^{2}. \tag{2.2}\]

For convenience of notation, we denote population loss \(\mathcal{L}(W)=\mathbb{E}_{P_{0}}[\widehat{\mathcal{L}}(W)]\).

### Score Decomposition in Linear Latent Space

In this part, we review the score decomposition in (Chen et al., 2023). We consider that the \(D\)-dimensional input data \(x\) supported on a \(d_{0}\)-dimensional subspace, where \(d_{0}\leq D\).

**Assumption 2.1** (Low-Dimensional Linear Latent Space).: Let \(x\) denote the initial data at \(t=0\). \(x\) has a latent representation via \(x=Bh\), where \(B\in\mathbb{R}^{D\times d_{0}}\) is an unknown matrix with orthonormal columns. The latent variable \(h\in\mathbb{R}^{d_{0}}\) follows the distribution \(P_{h}\) with a density function \(p_{h}\).

**Remark 2.1**.: By "linear latent space," we mean that each entry of a given latent vector is a linear combination of the corresponding input, i.e., \(x=Bh\). This is also known as the "low-dimensional data" assumption in literature (Chen et al., 2023).

Let \(\overline{x}\) and \(\overline{h}\) denote the perturbed data and its associated latent variable at \(t>0\), respectively. Based on the low-dimensional data structure assumption, we have the following score decomposition theory: on-support score \(s_{+}(B^{\top}\overline{x},t)\) and orthogonal score \(s_{-}(\overline{x},t)\).

**Lemma 2.1** (Score Decomposition, Lemma 1 of [Chen et al., 2023]).: Let data \(x=Bh\) follow Assumption 2.1. The decomposition of score function \(\nabla\log p_{t}(\overline{x})\) is

\[\nabla\log p_{t}(\overline{x})=\underbrace{B\nabla\log p_{t}^{h}(\overline{h} )}_{s_{+}(\overline{h},t)}\underbrace{-\left(I_{D}-BB^{\top}\right)\overline{ x}/\sigma(t)}_{s_{-}(\overline{x},t)},\quad\overline{h}=B^{\top}\overline{x}, \tag{2.3}\]

where \(p_{t}^{h}(\overline{h})\coloneqq\int\psi_{t}(\overline{h}|h)p_{h}(h)\mathrm{d }h\), \(\psi_{t}(\cdot|h)\) is the Gaussian density function of \(N(\beta(t)h,\sigma(t)I_{d_{0}})\), \(\beta(t)=e^{-t/2}\) and \(\sigma(t)=1-e^{-t}\). We restate the proof in Appendix D.2 for completeness.

Additionally, our theoretical analysis is based on two following assumptions as in [Chen et al., 2023].

**Assumption 2.2** (Tail Behavior of \(P_{h}\)).: The density function \(p_{h}>0\) is twice continuously differentiable. Moreover, there exist positive constants \(A_{0}\), \(A_{1},A_{2}\) such that when \(\left\lVert h\right\rVert_{2}\geq A_{0}\), the density function \(p_{h}(h)\leq(2\pi)^{-d_{0}/2}A_{1}\exp(-A_{2}\|h\|_{2}^{2}/2)\).

**Assumption 2.3** (\(L_{s_{+}}\)-Lipschitz of \(s_{+}(\overline{h},t)\)).: The on-support score function \(s_{+}(\overline{h},t)\) is \(L_{s_{+}}\)-Lipschitz in \(\overline{h}\in\mathbb{R}^{d_{0}}\) for any \(t\in[0,T]\).

### Score Network and Transformers

In this part, we introduce the score network architecture and Transformers. Transformers are the backbone of the score network in DiT. By Assumption 2.1, \(\overline{h}=B^{\top}\overline{x}\in\mathbb{R}^{d_{0}}\) with \(d_{0}<D\).

(Latent) Score Network.Following [Chen et al., 2023], we rearrange (2.3) into

\[\nabla\log p_{t}(\overline{x})=B\underbrace{(\sigma(t)\nabla\log p_{t}^{h}( \overline{T})\overline{x})+B^{\top}\overline{x})/\sigma(t)-\overline{x}/\sigma (t)}_{:=q(B^{\top}\overline{x},t):\mathbb{R}^{d_{0}}\times[T_{0},T]\to \mathbb{R}^{d_{0}}} \tag{2.4}\]

We use \(W_{B}\in\mathbb{R}^{D\times d_{0}}\) to approximate \(B\in\mathbb{R}^{D\times d_{0}}\), and a neural network \(f(W_{B}^{\top}\overline{x},t)\) to approximate \(q(B^{\top}\overline{x},t)\). We adopt the following score network class for diffusion in latent space (i.e., in \(\overline{h}\in\mathbb{R}^{d_{0}}\))

\[\mathcal{S}=\left\{s_{W}(\overline{x},t)=W_{B}f(W_{B}^{\top}\overline{x},t)/ \sigma(t)-\overline{x}/\sigma(t),\;W=\left\{W_{B},f\right\}\right\}, \tag{2.5}\]

where the columns in \(W_{B}\) are orthogonal, \(f:\mathbb{R}^{d_{0}}\times[T_{0},T]\to\mathbb{R}^{d_{0}}\) is a neural network. In this work, we focus on the diffusion transformers (DiTs), i.e., using Transformer for \(f\)[Peebles and Xie, 2023].

Transformers.A Transformer block consists of a self-attention layer and a feed-forward layer, with both layers having skip connection. We use \(\tau^{r,m,l}:\mathbb{R}^{d\times L}\to\mathbb{R}^{d\times L}\) to denote a Transformer block. Here \(r\) and \(m\) are the number of heads and head size in self-attention layer, and \(l\) is the hidden dimension in feed-forward layer. Let \(X\in\mathbb{R}^{d\times L}\) be the model input, then we have the model output

\[\mathrm{Attn}(X)=X+\sum\nolimits_{i=1}^{r}W_{O}^{i}W_{V}^{i}X\cdot\mathrm{ Softmax}\left(\left(W_{K}^{i}X\right)^{\top}W_{Q}^{i}X\right), \tag{2.6}\]

\[\mathrm{FF}\circ\mathrm{Attn}(X)=\mathrm{Attn}(X)+W_{2}\cdot\mathrm{ReLU}(W_{ 1}\cdot\mathrm{Attn}(X)+b_{1}\mathds{1}_{L}^{\mathsf{T}})+b_{2}\mathds{1}_{L}^ {\mathsf{T}}, \tag{2.7}\]

where \(W_{K}^{i},W_{Q}^{i},W_{V}^{i}\in\mathbb{R}^{m\times d},W_{O}^{i}\in\mathbb{R}^{ d\times m},W_{1}\in\mathbb{R}^{l\times d},W_{2}\in\mathbb{R}^{d\times l},b_{1}\in \mathbb{R}^{l},b_{2}\in\mathbb{R}^{d}\).

In our work, we use Transformer networks with positional encoding \(E\in\mathbb{R}^{d\times L}\). We define the Transformer networks as the composition of Transformer blocks

\[\mathcal{T}_{P}^{r,m,l}=\{f_{\mathcal{T}}:\mathbb{R}^{d\times L}\to\mathbb{R}^ {d\times L}\mid f_{\mathcal{T}}\text{ is a composition of blocks }\tau^{r,m,l}\text{s}\}.\]

For example, the following is a Transformer network consisting \(K\) blocks and positional encoding

\[f_{\mathcal{T}}(X)=\mathrm{FF}^{(K)}\circ\mathrm{Attn}^{(K)}\circ\cdots\mathrm{ FF}^{(1)}\circ\mathrm{Attn}^{(1)}(X+E). \tag{2.8}\]

## 3 Statistical Rates of Latent DiTs with Subspace Data Assumption

In this section, we analyze the statistical rates of latent DiTs. Section 3.1 introduces the class of latent DiT score networks. In Section 3.2, we prove the approximation limit of matching the DiT score function with the score network class, and characterize the structural configuration of the score network when a specified approximation error is required. Following this, in Section 3.3, utilizing the characterized structural configuration, we prove the score and distribution estimation for latent DiTs.

### DiT Score Network Class

Here, we provide the details about DiT score network class used in our analysis. In (2.5), \(f\) is a network with Transformer as the backbone, and \((\overline{h},t)\in\mathbb{R}^{d_{0}}\times[T_{0},T]\) denotes the input data. Following [5], DiT uses time point \(t\) to calculate the scale and shift value in the Transformer backbone, and it transforms an input picture into a sequential version. To achieve the transformation, we introduce a reshape layer.

**Definition 3.1** (DiT Reshape Layer \(R(\cdot)\)).: Let \(R(\cdot):\mathbb{R}^{d_{0}}\rightarrow\mathbb{R}^{d\times L}\) be a reshape layer that transforms the \(d_{0}\)-dimensional input into a \(d\times L\) matrix. Specifically, for any \(d_{0}=i\times i\) image input, \(R(\cdot)\) converts it into a sequence representation with feature dimension \(d\coloneqq p^{2}\) (where \(p\geq 2\)) and sequence length \(L\coloneqq(i/p)^{2}\). Besides, we define the corresponding reverse reshape (flatten) layer \(R^{-1}(\cdot):\mathbb{R}^{d\times L}\rightarrow\mathbb{R}^{d_{0}}\) as the inverse of \(R(\cdot)\). By \(d_{0}=dL\), \(R,R^{-1}\) are associative w.r.t. their input.

To simplify the self-attention block in (2.6), let \(W_{OV}^{i}=W_{O}^{i}W_{V}^{i}\) and \(W_{KQ}^{i}=(W_{K}^{i})^{\mathsf{T}}W_{Q}^{i}\).

**Definition 3.2** (Transformer Network Class \(\mathcal{T}_{p}^{r,m,l}\)).: We define the Transformer network class as

\(\mathcal{T}_{p}^{r,m,l}(K,C_{\mathcal{T}},C_{OV}^{2,\infty},C_{OV},C_{KQ}^{2, \infty},C_{KQ},C_{F}^{2,\infty},C_{F},C_{E},L_{\mathcal{T}})\), satisfying the constraints

* Model architecture with \(K\) blocks: \(f_{\mathcal{T}}(X)=\operatorname{FF}^{(K)}\circ\operatorname{Attn}(K)\circ \cdots\operatorname{FF}^{(1)}\circ\operatorname{Attn}(^{1})(X)\);
* Model output bound: \(\sup_{X}\left\lVert f_{\mathcal{T}}(X)\right\rVert_{2}\leq C_{\mathcal{T}}\);
* Parameter bound in \(\operatorname{Attn}(^{i})\): \(\left\lVert(W_{OV}^{i})^{\top}\right\rVert_{2,\infty}\leq C_{OV}^{2,\infty}\), \(\left\lVert(W_{OV}^{i})^{\top}\right\rVert_{2}\leq C_{OV}\), \(\left\lVert W_{KQ}^{i}\right\rVert_{2,\infty}\leq C_{KQ}^{2,\infty}\), \(\left\lVert W_{KQ}^{i}\right\rVert_{2}\leq C_{KQ}\), \(\left\lVert E^{\top}\right\rVert_{2,\infty}\leq C_{E},\forall i\in[K]\);
* Parameter bound in \(\operatorname{FF}^{(i)}\): \(\left\lVert W_{g}^{i}\right\rVert_{2,\infty}\leq C_{F}^{2,\infty}\), \(\left\lVert W_{g}^{i}\right\rVert_{2}\leq C_{F},\forall j\in[2],i\in[K]\);
* Lipschitz of \(f_{\mathcal{T}}\): \(\left\lVert f_{\mathcal{T}}(X_{1})-f_{\mathcal{T}}(X_{2})\right\rVert_{F}\leq L _{\mathcal{T}}\|X_{1}-X_{2}\|_{F},\forall X_{1},X_{2}\in\mathbb{R}^{d\times L}\).

**Definition 3.3** (DiT Score Network Class \(\mathcal{S}_{\mathcal{T}_{p}^{r,m,l}}\) (Figure 1)).: We denote \(\mathcal{S}_{\mathcal{T}_{p}^{r,m,l}}\) as the DiT score network class in (2.5), replacing \(f\) with \(R^{-1}\circ f_{\mathcal{T}}\circ R\), and \(f_{\mathcal{T}}\) is from the Transformer class \(\mathcal{T}_{p}^{r,m,l}\).

### Score Approximation of DiT

Here, we explore the approximation limit of latent DiT score network class \(\mathcal{S}_{\mathcal{T}_{p}^{r,m,l}}\) under linear latent space assumption. Recall that \(P_{t}\) is the distribution of \(x_{t}\), \(\sigma(t)\) is the variance of \(P(x_{t}|x_{0})\), \(d_{0}\) is the dimension of latent space, \(L\) is the sequence length of transformer input, \(T\) is the stopping time in forward process, \(T_{0}\) is the early stopping time in backward process, and \(L_{s_{+}}\) is the Lipschitz coefficient of on-support score function. Then we have the following Theorem3.1.

**Theorem 3.1** (Score Approximation of DiT).: For any approximation error \(\epsilon>0\) and any data distribution \(P_{0}\) under Assumptions2.1 to 2.3, there exists a DiT score network \(s_{\widehat{W}}\) from \(\mathcal{S}_{\mathcal{T}_{p}^{2,1,4}}\) (defined in Definition3.2), where \(\widehat{W}=\{\widehat{W}_{B},\widehat{f}_{\mathcal{T}}\}\), such that for any \(t\in[T_{0},T]\), we have:

\[\left\lVert s_{\widehat{W}}(\cdot,t)-\nabla\log p_{t}(\cdot)\right\rVert_{L^{2 }(P_{t})}\leq\epsilon\cdot\sqrt{d_{0}}/\sigma(t),\]

where \(\sigma(t)=1-e^{-t}\), and the upper bound of hyperparameters in \(\mathcal{S}_{\mathcal{T}_{p}^{2,1,4}}\) are

\[K=\mathcal{O}(\epsilon^{-2L}),\;C_{\mathcal{T}}=\mathcal{O}\left(d_{0}L_{s_{+} }\sqrt{d_{0}\log(d_{0}/T_{0})+\log(1/\epsilon)}\right),\]

Figure 1: **Overview of DiT Score Network Architecture \(s_{W}(\cdot,t)\). \(W_{B}^{T}\) denotes the linear layer from the input data space to the linear latent space. \(f(\cdot)=R^{-1}\circ f_{\mathcal{T}}\circ R(\cdot)\) denotes the transformer network \(f_{\mathcal{T}}(\cdot)\) with reshaping layer \(R(\cdot)\), where \(f_{\mathcal{T}}(\cdot)\in\mathcal{T}_{p}^{r,m,l}\). \(W_{B}\) denotes the linear layer from the linear latent space to the input data space. \(\sigma(t)\) denote the variance of the conditional distribution \(P(x_{t}\mid x_{0})\).**\[C_{OV}^{2,\infty}=(1/\epsilon)^{\mathcal{O}(1)},\ C_{OV}=(1/ \epsilon)^{\mathcal{O}(1)},\ C_{KQ}^{2,\infty}=(1/\epsilon)^{\mathcal{O}(1)},\ C_{KQ}=(1/\epsilon)^{\mathcal{O}(1)},\] \[C_{E}=\mathcal{O}(L^{3/2}),\,C_{F}^{2,\infty}=(1/\epsilon)^{ \mathcal{O}(1)},\,C_{F}=(1/\epsilon)^{\mathcal{O}(1)},\,L_{\mathcal{T}}= \mathcal{O}\left(d_{0}L_{s+}\right).\]

Proof Sketch.: Our proof is built on the key observation that there is a tail behavior of the low-dimensional latent variable distribution \(P_{h}\) (Assumption 2.2). Recall that \(\nabla\log p_{t}(\overline{x})=Bq(\overline{h},t)/\sigma(t)-\overline{x}/ \sigma(t)\), where \(\overline{h}=B^{\top}\overline{x}\) (defined in (2.4)). By taking \(\widehat{W}_{B}=B\), our aim reduces to construct a transformer network to approximate \(q(\overline{h},t)\). To achieve this, we firstly approximate \(q(\overline{h},t)\) with a compact-supported continuous function, based on the tail behavior of \(P_{h}\). Then we construct a transformer to approximate the compact-supported continuous function using the universal approximation capacity of transformer (Yun et al., 2020). See Appendix F.1 for a detailed proof. 

Intuitively, Theorem 3.1 indicates the capability of the transformer-based score network to approximate the score function with precise guarantees. Furthermore, Theorem 3.1 provides empirical guidance for the design choices of the score network when a specified approximation error is required.

**Remark 3.1** (Comparing with Existing Works).: Theoretical analysis of DiTs is limited. Previous works that do not specify the model architecture assume that the score estimator is well-approximated (Benton et al., 2024; Wibisono et al., 2024). To the best of our knowledge, this work is the first to present an approximation theory for DiTs, offering the estimation theory in Theorem 3.2 and Corollary 3.2.1 based on the estimated score network, rather than a perfectly trained one.

**Remark 3.2** (Latent Dimension Dependency).: Theorem 3.1 suggests that the approximation capacity and Transformer network size primarily depend on the latent variable dimension \(d_{0}=d\times L\). This indicates that DiTs can potentially bypass the challenges associated with the high dimensionality of initial data by transforming input data into a low-dimensional latent variable.

### Score Estimation and Distribution Estimation

Besides score approximation capability, Theorem 3.1 also characterizes the structural configuration of the score network for any specific precision, e.g., \(K,C_{E},C_{F}\), etc. This characterization enables further analysis of the performance of score network in practical scenarios. In Theorem 3.2, we provide a sample complexity bound for score estimation. In Corollary 3.2.1, show that the learned score estimator is able to recover the initial data distribution.

Score Estimation.To derive a sample complexity for score estimation using \(\mathcal{S}_{\mathcal{T}_{p}^{2,1,4}}\), we rewrite the score matching objective in (2.2) as \(\widehat{W}\in\operatorname*{argmin}_{s_{W}\in\mathcal{S}_{\mathcal{T}_{p}^{2, 1,4}}}\widehat{\mathcal{L}}(s_{W}),\ \widehat{W}=\{\widehat{W}_{B},\widehat{f}_{ \mathcal{T}}\}\).

Theorem 3.2 shows that as sample size \(n\to\infty\), \(s_{W}(\cdot,t)\) convergences to \(\nabla\log p_{t}(\cdot)\).

**Theorem 3.2** (Score Estimation of DiT).: Under Assumptions 2.1 to 2.3, we choose \(\mathcal{S}_{\mathcal{T}_{p}^{2,1,4}}\) as in Theorem 3.1 using \(\epsilon\in(0,1)\) and \(L>1\), With probability \(1-1/\mathrm{poly}(n)\), we have

\[\frac{1}{T-T_{0}}\int_{T_{0}}^{T}\left\|s_{\widehat{W}}(\cdot,t)- \nabla\log p_{t}(\cdot)\right\|_{L^{2}(P_{t})}\mathrm{d}t=\widetilde{\mathcal{ O}}\left(\frac{1}{n^{1/3}T_{0}T}\cdot 2^{(1/\epsilon)^{2L}}+\frac{1}{n^{1/3}T_{0}T}+ \frac{1}{T_{0}T}\epsilon^{2}\right), \tag{3.1}\]

where \(\widetilde{\mathcal{O}}\) hides the factors related to \(D,d_{0},d,L_{s+}\), and \(\log n\).

Proof.: See Appendix F.2 for a detailed proof. 

Intuitively, Theorem 3.2 shows a sample complexity bound for score estimation in practice.

**Remark 3.3** (Comparing with Existing Works).: (Zhu et al., 2023) provides a sample complexity for simple ReLU-based diffusion models under the assumption of an accurate score estimator. To the best of our knowledge, we are the first to provide a sample complexity for DiTs, based on the learned score network in Theorem 3.1 and the quantization (piece-wise approximation) approach for transformer universality (Yun et al., 2020). Furthermore, our first term shows a convergence rate of \(1/T\), outperforming (Chen et al., 2023), in which the first term is independent of \(T\).

**Remark 3.4** (Double Exponential Factor and Inconsistent Convergence).: Theorem3.2 reports an explicit result on sample complexity bounds for score estimation of latent DiTs: a double exponential factor \(2^{(1/\epsilon)^{2L}}\) in the first term. We remark that this arises from the required depth \(K\) is \(\mathcal{O}(\epsilon^{-2L})\), and the norm of required weight parameters is \((1/\epsilon)^{\mathcal{O}(1)}\) as shown in Theorem3.1, assuming the universality of transformers requires dense layers (Yun et al., 2020). This double exponential factor causes inconsistent convergence with respect to sample size \(n\), as its large value prevents setting \(\epsilon\) as a function of \(n\) to balance the first and second terms in (3.1). This motivates us to rethink transformer universality and explore new proof techniques for DiTs, which we leave for future work.

**Definition 3.4**.: For later convenience, we define \(\xi(n,\epsilon,L):=\frac{1}{n!^{3/3}}\cdot 2^{(1/\epsilon)^{2L}}+\frac{1}{n!^{ 3/3}}+\epsilon^{2}\).

Distribution Estimation.In practice, DiTs generate data using the discretized version with step size \(\mu\), see AppendixD.1 for details. Let \(\widehat{P}_{T_{0}}\) be the distribution generated by \(s_{\widehat{W}}\) using the discretized backward process in Theorem3.2. Let \(P^{h}_{T_{0}}\) and \(p^{h}_{T_{0}}\) be the distribution and density function of on-support latent variable \(\overline{h}\) at \(T_{0}\). We have the following results for distribution estimation.

**Corollary 3.2.1** (Distribution Estimation of DiT, Modified From Theorem 3 of (Chen et al., 2023)).: Let \(T=\mathcal{O}(\log n),T_{0}=\mathcal{O}(\min\{c_{0},1/L_{s_{+}}\})\), where \(c_{0}\) is the minimum eigenvalue of \(\mathbb{E}_{P_{h}}[hh^{\top}]\). With the estimated DiT score network \(s_{\widehat{W}}\) in Theorem3.2, we have the following with probability \(1-1/\mathrm{poly}(n)\).

1. The accuracy to recover the subspace \(B\) is \(\left\|W_{B}W_{B}^{\top}-BB^{\top}\right\|_{F}^{2}=\widetilde{\mathcal{O}}\left( \xi(n,\epsilon,L)/c_{0}\right)\).
2. With the conditions \(\mathsf{KL}(P_{h}||N(0,I_{d_{0}}))<\infty\), there exists an orthogonal matrix \(U\in\mathbb{R}^{d\times d}\) such that we have the following upper bound for the total variation distance \[\mathsf{TV}(P^{h}_{T_{0}},(W_{B}U)_{\sharp}^{\top}\widehat{P}_{T_{0}})= \widetilde{\mathcal{O}}(\sqrt{\xi(n,\epsilon,L)\cdot\log n}),\] (3.2) where \(\widetilde{\mathcal{O}}\) hides the factor about \(D,d_{0},d,L_{s_{+}},\log n\), and \(T-T_{0}\). and \((W_{B}U)_{\sharp}^{\top}\widehat{P}_{T_{0}}\) denotes the pushforward distribution.
3. For the generated data distribution \(\widehat{P}_{T_{0}}\), the orthogonal pushforward \((I-W_{B}W_{B}^{\top})_{\sharp}\widehat{P}_{T_{0}}\) is \(N(0,\Sigma)\), where \(\Sigma\unlhd aT_{0}I\) for a constant \(a>0\).

Proof.: See AppendixF.3 for a detailed proof. 

Intuitively, Corollary3.2.1 shows the estimation results in 3 parts: (i) the accuracy of recovering the subspace \(B\); (ii) the estimation error between \(\widehat{P}_{T_{0}}\) and \(P^{h}_{T_{0}}\); and (iii) the vanishing behavior of \(\widehat{P}_{T_{0}}\) in the orthogonal space. These indicate that the learned score estimator is capable of recovering the initial data distribution. Notably, Corollary3.2.1 is agnostic to the specifics of \(\xi(n,\epsilon,L)\).

**Remark 3.5** (Comparing with Existing Works).: Oko et al. (2023) analyze the distribution estimation under the assumption that the initial density is supported on \([-1,1]^{D}\) and smooth in the boundary. Our Assumption2.2 demonstrates greater practical relevance. This suggests that our method of distribution estimation aligns more closely with empirical realities.

**Remark 3.6** (Subspace Recovery Accuracy).: (i) of Corollary3.2.1 confirms that the subspace is learned by DiTs. The error is proportional to the sample complexity for score estimation and depends on the minimum eigenvalue of the covariance of \(P_{h}\).

## 4 Provably Efficient Criteria

Here, we analyze the computational limits of latent DiTs under low-dimensional linear subspace data assumption (i.e., Assumption2.1). The hardness of DiT models ties to both forward and backward passes of the score network in Definition3.3. We characterize them separately.

### Computational Limits of Backward Computation

Following Section2, suppose we have \(n\) i.i.d. data samples \(\{x_{0,i}\}_{i=1}^{n}\sim P_{d}\), and time \(t_{i_{0}}\)\((1\leq i\leq n)\) uniformly sampled from \([T_{0},T]\). For each data \(x_{0,i}\in\mathbb{R}^{D}\), we sample \(x_{t_{i_{0}}}\in\mathbb{R}^{D}\) from \(N(\beta(t_{i_{0}})x_{0,i},\sigma(t_{i_{0}})I_{D})\). Let \((W_{A}R^{-1}(\cdot))^{\dagger}\) be the inverse transformation of \(W_{A}R^{-1}(\cdot)\), and denote\(Y_{0,i}\coloneqq(W_{A}R^{-1})^{\dagger}(x_{0,i})\in\mathbb{R}^{d\times L}\). We rewrite the empirical denoising score-matching loss (2.2) as

\[\frac{1}{n}\sum_{i=1}^{n}\Big{\|}W_{A}R^{-1}(f_{\mathcal{T}}(R( \underbrace{W_{A}^{\top}x_{t_{0}})}_{d_{0}\times 1}))-x_{0,i}\Big{\|}_{F}^{2}= \frac{1}{n}\sum_{i=1}^{n}\Big{\|}\underbrace{W_{A}}_{D\times d_{0}} \underbrace{R^{-1}(\overbrace{f_{\mathcal{T}}(R(W_{A}^{\top}x_{t_{0}})}^{d \times L})}_{d_{0}\times 1})-Y_{0,i})\Big{\|}_{F}^{2}. \tag{4.1}\]

For efficiency, it suffices to focus on just transformer attention heads of the DiT score network due to their dominating quadratic time complexity in both passes. Thus, we consider only a single layer attention for \(f_{\mathcal{T}}\), to simplify our analysis. Further, we consider the following simplifications:

1. To prove the hardness of (4.1) for both full gradient descent and stochastic mini-batch gradient descent methods, it suffices to consider training on a single data point.
2. For the convenience of our analysis, we consider the following expression for attention mechanism. Let \(X,Y\in\mathbb{R}^{d\times L}\). Let \(W_{K},W_{Q},W_{V}\in\mathbb{R}^{s\times d}\) be attention weights such that \(Q=W_{Q}X\in\mathbb{R}^{d\times L}\), \(K=W_{K}X\in\mathbb{R}^{s\times L}\) and \(V=W_{V}X\in\mathbb{R}^{s\times L}\). We write attention mechanism of hidden size \(s\) and sequence length \(L\) as \[\operatorname{Att}(X)=\underbrace{(W_{O}W_{V}X)}_{V\text{ multiplication}}\underbrace{D^{-1}\exp\bigl{(}X^{\mathsf{T}}W_{K}^{\mathsf{T}}W_{Q}X \bigr{)}}_{K\text{-}Q\text{ multiplication}}\in\mathbb{R}^{d\times L},\] (4.2) with \(D\coloneqq\operatorname{diag}(\exp\bigl{(}XW_{Q}W_{K}^{\mathsf{T}}X^{\mathsf{ T}}\bigr{)}\mathds{1}_{L})\). Here, \(\exp(\cdot)\) is entry-wise exponential function, i.e., \(\exp(A)_{i,j}=\exp(A_{i,j})\) for any matrix \(A\), \(\operatorname{diag}(\cdot)\) converts a vector into a diagonal matrix with the vector's entries on the diagonal, and \(\mathds{1}_{L}\) is the length-\(L\) all ones vector.
3. Since \(V\) multiplication is linear in weight while \(K\)-\(Q\) multiplication is exponential in weights, we only need to focus on the gradient update of \(K\)-\(Q\) multiplication. Therefore, for efficiency analysis of gradient, it is equivalent to analyzing a reduced problem with fixed \(W_{O}W_{V}X=\text{const}\)..
4. To focus on the DiT, we consider the low-dimensional linear encoder \(W_{A}\) to be pretrained and to not participate in gradient computation. This aligns with common practice [Rombach et al., 2022] and is justified by the trivial computation cost due to the linearity of \(W_{A}\)2. Footnote 2: The gradient computation is linear in \(W_{A}\), and hence the computation w.r.t. \(W_{A}\) is cheap and upper-bounded by \(L\cdot\operatorname{poly}(d)\) time in a straightforward way.
5. To further simplify, we introduce \(A_{1},A_{2},A_{3}\in\mathbb{R}^{s\times L}\) and \(W\in\mathbb{R}^{d\times d}\) via \[\Big{\|}W_{A}R^{-1}\bigl{(}f_{\mathcal{T}}(\underbrace{R(W_{A}^{ \top}x_{t_{0}})}_{\coloneqq X\in\mathbb{R}^{d\times L}})-\underbrace{Y_{0,i}} _{\coloneqq Y\in\mathbb{R}^{d\times L}}\Big{)}\Big{\|}_{F}^{2}\] (By (S0), (S1) and (S2)) \[=\Big{\|}W_{A}R^{-1}\bigl{(}\underbrace{W_{O}W_{V}}_{=W_{OV}\in \mathbb{R}^{d\times d}}\underbrace{X}_{=A_{3}\in\mathbb{R}^{d\times L}}D^{-1} \exp\big{(}\underbrace{X^{\mathsf{T}}}_{\coloneqq A_{1}^{\top}\in\mathbb{R}^ {L\times d}}\underbrace{W_{K}^{\mathsf{T}}W_{Q}}_{=W\in\mathbb{R}^{d\times d }}\underbrace{X}_{=A_{2}\in\mathbb{R}^{d\times L}}\big{)}-Y\big{)}\Big{\|}_{F}^ {2}.\] (4.3) Notably, \(A_{1},A_{2},A_{3},X,Y\) are constants w.r.t. training above loss with gradient updates.

Therefore, we simplify the objective of training DiT into

**Definition 4.1** (Training Generic DiT Loss).: Given \(A_{1},A_{2},A_{3},Y\in\mathbb{R}^{d\times L}\) and \(W_{OV},W\in\mathbb{R}^{d\times d}\) following (S4), Training a DiT with \(\ell_{2}\) loss on a single data point \(X,Y\in\mathbb{R}^{d\times L}\) is formulated as

\[\min_{W}\ \mathcal{L}_{0}(W)=\min_{W}\ \frac{1}{2}\Big{\|}W_{A}R^{-1}\bigl{(}W_{OV}A _{3}D^{-1}\exp\bigl{(}A_{1}^{\top}WA_{2}\bigr{)}-Y\bigr{)}\Big{\|}_{F}^{2}. \tag{4.4}\]

Here \(D\coloneqq\operatorname{diag}(\exp\bigl{(}A_{1}^{\mathsf{T}}WA_{2}\bigr{)} \mathds{1}_{n})\in\mathbb{R}^{L\times L}\).

**Remark 4.1** (Conditional and Unconditional Generation).: \(\mathcal{L}_{0}\) is generic. If \(A_{1}\neq A_{2}\in\mathbb{R}^{d\times L}\), Definition 4.1 reduces to cross-attention in DiT score net (for conditional generation). If \(A_{1}=A_{2}\in\mathbb{R}^{d\times L}\), Definition 4.1 reduces to self-attention in DiT score net (for unconditional vanilla generation).

We introduce the next problem to characterize all possible gradient computations of optimizing (4.4).

**Problem 1** (Approximate DiT Gradient Computation (\(\text{ADiTGC}(L,d,\Gamma,\epsilon)\))).: Given \(A_{1},A_{2},A_{3},Y\in\mathbb{R}^{d\times L}\). Let \(\epsilon>0\). Assume all numerical values are in \(\mathcal{O}(\log(L))\)-bits encoding. Let loss function \(\mathcal{L}_{0}\) follow Definition4.1. The problem of approximating gradient computation of optimizing empirical DiT loss (4.4) is to find an approximated gradient matrix \(\widetilde{G}^{(W)}\in\mathbb{R}^{d\times d}\) such that \(\left\|\widetilde{G}^{(W)}-\frac{\partial\mathcal{L}}{\partial\underline{W}} \right\|_{\max}\leq 1/\mathrm{poly}(L)\). Here, \(\left\|A\right\|_{\max}\coloneqq\max_{i,j}\left|A_{ij}\right|\) for any matrix \(A\).

In this work, we aim to investigate the computational limits of all possible efficient algorithms of ADiTGC with \(\epsilon=1/\mathrm{poly}(L)\). Yet, the explicit gradient of DiT denoising score matching loss (4.4) is too complicated to characterize ADiTGC. To combat this, we make the following observations.

1. Let \(g_{1}(\cdot)\coloneqq W_{A}R^{-1}(\cdot):\mathbb{R}^{d\times L}\to\mathbb{R}^ {d_{0}}\), \(g_{2}(\cdot)\coloneqq\mathrm{Att}(\cdot):\mathbb{R}^{d\times L}\to\mathbb{R}^ {d\times L}\), and \(g_{3}(\cdot)\coloneqq R(W_{A}^{\top}):\mathbb{R}^{D}\to\mathbb{R}^{d\times L}\) such that \(g_{3}(x)=X\) for \(x\in\mathbb{R}^{D}\) (with \(D>d_{0}=dL\)).
2. **Vectorization of \(f_{\mathcal{T}}\).** For the ease of presentation, we use notation flexibly that \(f_{\mathcal{T}}\) to denote both a matrix in \(\mathbb{R}^{d\times L}\) and a vector in \(\mathbb{R}^{dL}\) in the following analysis. This practice does not affect correctness. The context in which \(f_{\mathcal{T}}\) is used should clarify whether it refers to a matrix or a vector. Explicit vectorization follows DefinitionD.1.
3. **Linearity of \(g_{1}\).** By linearity of \(W_{A}R^{-1}(\cdot)\), we treat \(g_{1}\) as a matrix in \(\mathbb{R}^{d_{0}\times dL}\) acting on vector \(f_{\mathcal{T}}(\cdot)\in\mathbb{R}^{dL}\).

Therefore, we have \(\mathcal{L}_{0}=\left\|g_{1}\cdot\left[g_{2}(g_{3})-Y\right]\right\|_{2}^{2}\), such that its gradient involves \(\frac{\mathrm{d}\mathcal{L}_{0}}{\mathrm{d}W}=g_{1}\frac{\mathrm{d}g_{2}}{ \mathrm{d}W}\). From above, we only need to focus on proving the computation time and error control of term \(\frac{\mathrm{d}g_{2}}{\mathrm{d}W}\) for gradient w.r.t \(W\). Luckily, with tools from fine-grained complexity theory [10, 20, 21] and tensor trick (see AppendixD.3), we prove the existence of almost-linear time algorithms for Problem1 in the next theorem. Let \(\mathrm{vec}(W)\coloneqq\underline{W}\) for any matrix \(W\) following DefinitionD.1.

**Theorem 4.1** (Existence of Almost-Linear Time Algorithms for ADiTGC).: Suppose all numerical values are in \(\mathcal{O}(\log L)\)-bits encoding. Let \(\max(\left\|W_{OV}A_{3}\right\|_{\max},\left\|W_{K}A_{1}\right\|_{\max},\left\| W_{Q}A_{2}\right\|_{\max})\leq\Gamma\). There exists a \(L^{1+o(1)}\) time algorithm to solve \(\text{ADiTGC}(L_{p},L,d=\mathcal{O}(\log L),\Gamma=o(\sqrt{\log L}))\) (i.e., Problem1) with loss \(\mathcal{L}_{0}\) from Definition4.1 up to \(1/\mathrm{poly}(L)\) accuracy. In particular, this algorithm outputs gradient matrices \(\widetilde{G}^{(W)}\in\mathbb{R}^{d\times d}\) such that \(\left\|\widetilde{G}^{(W)}-\frac{\partial\mathcal{L}}{\partial\underline{W}} \right\|_{\max}\leq 1/\mathrm{poly}(L)\).

Proof Sketch.: Our proof is built on the key observation that there exist low-rank structures within the DiT training gradients. Using the tensor trick [20, 21] and computational hardness results of attention [13, 10], we approximate DiT training gradients with a series of low-rank approximations and carefully match the multiplication dimensions so that the computation of \(\frac{\mathrm{d}g_{2}}{\mathrm{d}W}\) forms a chained low-rank approximation. We complete the proof by demonstrating that this approximation is bounded by a \(1/\mathrm{poly}(L)\) error and requires only almost-linear time. See AppendixG.2 for a detailed proof. 

**Remark 4.2**.: We remark that Theorem4.1 is dominated by the relation between \(L\) and \(d\), hence by the subspace dimension3\(d_{0}=dL\). A smaller \(d_{0}\) makes Theorem4.1 more likely to hold.

Footnote 3: See Assumption2.1.

### Computational Limits of Forward Inference

Since the inference of score-matching diffusion models is a forward pass of the trained score estimator \(s_{W}\), the computational hardness of DiT ties to the transformer-based score network,

\[s_{W}(A_{1},A_{2},A_{3})=W_{A}R^{-1}\big{(}\underbrace{W_{OV}A_{3}}_{d\times L }\underbrace{D^{-1}}_{L\times L}\mathrm{exp}\big{(}\underbrace{A_{1}^{\top} W_{K}^{\top}}_{L\times s}\underbrace{W_{Q}A_{2}}_{s\times L}\big{)}\big{)}, \tag{4.5}\]

following notation in Definition4.1. For inference, we study the following approximation problem. Notably, by Remark4.1, (4.5) subsumes both conditional and unconditional DiT inferences.

**Problem 2** (Approximate DiT Inference \(\text{ADiT1}(d,L,\Gamma,\delta_{F})\)).: Let \(\delta_{F}>0\) and \(B>0\). Given \(A_{1},A_{2},A_{3}\in\mathbb{R}^{d\times L}\), and \(W_{OV},W_{K},W_{Q}\in\mathbb{R}^{d\times d}\) with guarantees that \(\left\|W_{OV}A_{3}\right\|_{\infty}\leq B\), \(\left\|W_{K}A_{1}\right\|_{\infty}\leq B\) and \(\left\|W_{Q}A_{2}\right\|_{\infty}\leq B\), we aim to study an approximation problemADiTI(\(d,L,B,\delta_{F}\)), that approximates \(s_{W}(A_{1},A_{2},A_{3})\) with a vector \(\widetilde{z}\in\mathbb{R}^{d_{0}}\) (with \(d_{0}=d\cdot L\)) such that \(\left\|\widetilde{z}-W_{A}R^{-1}\left(W_{OV}A_{3}D^{-1}\exp\bigl{(}A_{1}^{\top} W_{K}^{\top}W_{Q}A_{2}\bigr{)}\right)\right\|_{\max}\leq\delta_{F}\). Here, \(\left\|A\right\|_{\max}\coloneqq\max_{i,j}\left|A_{ij}\right|\) for any matrix \(A\).

By (O2) and (O3), we make an observation that Problem 2 is just a special case of (Alman and Song, 2023). Hence, we characterize the all possible efficient algorithms for ADiTI with next proposition.

**Proposition 4.1** (Norm-Based Efficiency Phase Transition).: Let \(\left\|W_{Q}A_{2}\right\|_{\infty}\leq B\), \(\left\|W_{K}A_{1}\right\|_{\infty}\leq B\) and \(\left\|W_{OV}A_{3}\right\|_{\infty}\leq B\) with \(B=\mathcal{O}(\sqrt{\log L})\). Assuming SETH (Hypothesis 1), for every \(q>0\), there are constants \(C,C_{a},C_{b}>0\) such that: there is no \(O(n^{2-q})\)-time (sub-quadratic) algorithm for the problem ADiTI(\(L,d=C\log L,B=C_{b}\sqrt{\log L},\delta_{F}=L^{-C_{a}}\)).

**Remark 4.3**.: Proposition 4.1 suggests an efficiency threshold for the upper bound of \(\left\|W_{K}A_{1}\right\|_{\infty}\), \(\left\|W_{Q}A_{2}\right\|_{\infty}\), \(\left\|W_{OV}A_{3}\right\|_{\infty}\). Only below this threshold are efficient algorithms for Problem 2 possible.

Moreover, there exist almost-linear DiT inference algorithms following (Alman and Song, 2023).

**Proposition 4.2** (Almost-Linear Time DiT Inference).: Assuming SETH, the DiT inference problem ADiTI(\(L,d=\mathcal{O}(\log L),B=o(\sqrt{\log L}),\delta_{F}=1/\mathrm{poly}(L)\)) can be solved in \(L^{1+o(1)}\) time.

**Remark 4.4**.: Proposition 4.2 is a special case of Proposition 4.1 under the efficiency threshold.

**Remark 4.5**.: Propositions 4.1 and 4.2 are dominated by the relation between \(L\) and \(d\), hence by the subspace dimension \(d_{0}=dL\). A smaller \(d_{0}\) makes Propositions 4.1 and 4.2 more likely to hold.

## 5 Discussion and Concluding Remarks

We explore the fundamental limits of latent DiTs with 3 key contributions. First, we prove that transformers are universal approximators for the score functions in DiTs (Theorem 3.1), with approximation capacity and model size dependent only on the latent dimension, suggesting DiTs can handle high-dimensional data challenges. Second, we show that Transformer-based score estimators converge to the true score function (Theorem 3.2), ensuring the generated data distribution closely approximates the original (Corollary 3.2.1). Third, we provide provably efficient criteria (Proposition 4.1) and prove the existence of almost-linear time algorithms for forward inference (Proposition 4.2) and backward computation (Theorem 4.1). Our computational results hold for both unconditional and conditional generation of DiTs (Remark 4.1). These results highlight the potential of latent DiTs to achieve both computational efficiency and robust performance in practical scenarios.

Practical Guidance from Computational Results.Section 4 analyzes the computational feasibility and identifies all possible efficient DiT algorithms/methods for both forward inference and backward training. These results provide practical guidance for designing efficient methods:

* The latent dimension should be sufficiently small: \(d=\mathcal{O}(\log L)\) (Theorem 4.1, Propositions 4.1 and 4.2).
* Normalization of \(K\), \(Q\), and \(V\) in DiT attention heads enhances performance and efficiency:
* For efficient inference: \(\max\left\{\left\|W_{K}A_{1}\right\|,\left\|W_{Q}A_{2}\right\|,\left\|W_{OV}A_ {3}\right\|\right\}\leq B\) with \(B=o(\sqrt{\log L})\) (Proposition 4.2) and \(A_{1},A_{2},A_{3}\) being the input data associated with \(K,Q,V\).
* For efficient training: \(\max\left\{\left\|W_{K}A_{1}\right\|,\left\|W_{Q}A_{2}\right\|,\left\|W_{OV}A_ {3}\right\|\right\}\leq\Gamma\) with \(\Gamma=o(\sqrt{\log L})\) (Theorem 4.1).

We remark that these conditions are necessary but not sufficient; sufficient conditions depend on the specific design of the methods used. This is due to the best- or worst-case nature of hardness results.

Limitations and Future Direction.As discussed in Remark 3.4, the double exponential factor in our explicit sample complexity bound (Theorem 3.2) suggests a possible gap in our understanding of transformer universality and its interplay with DiT architecture. This motivates us to rethink transformer universality and explore new proof techniques for DiTs, which we leave for future work. Besides, due to its formal nature, this work does not provide immediate practical implementations. However, we expect that our findings provide valuable insights for future diffusion generative models.

Post-Acceptance Note (October, 29, 2024).A follow-up work by Hu et al. (2024) alleviates the double exponential factor and achieves minimax optimal statistical rates for DiTs under Holder smoothness data assumptions.

### Broader Impact

This theoretical work aims to shed light on the foundations of diffusion generative models and is not anticipated to have negative social impacts.

JH would like to thank to Minshuo Chen, Sophia Pi, Yi-Chen Lee, Yu-Chao Huang, Yibo Wen, Damien Jian, Jialong Li, Zijia Li, Tim Tsz-Kit Lau, Chenwei Xu, Dino Feng and Andrew Chen for enlightening discussions on related topics; Ting-Chun Liu for pointing out typos; and the Red Maple Family for support. The authors would like to thank the anonymous reviewers and program chairs for constructive comments. JH is partially supported by the Walter P. Murphy Fellowship. HL is partially supported by NIH R01LM1372201, AbbVie and Dolby. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies.

## References

* Achilli et al. (2024) Beatrice Achilli, Enrico Ventura, Gianluigi Silvestri, Bao Pham, Gabriel Raya, Dmitry Krotov, Carlo Lucibello, and Luca Ambrogioni. Losing dimensions: Geometric memorization in generative diffusion. _arXiv preprint arXiv:2410.08727_, 2024.
* Alman and Song (2023) Josh Alman and Zhao Song. Fast attention requires bounded entries. _Advances in Neural Information Processing Systems (NeurIPS)_, 36, 2023.
* Alman and Song (2024a) Josh Alman and Zhao Song. How to capture higher-order correlations? generalizing matrix softmax attention to kronecker computation. In _The Twelfth International Conference on Learning Representations (ICLR)_, 2024a.
* Alman and Song (2024b) Josh Alman and Zhao Song. The fine-grained complexity of gradient computation for training large language models. In _NeurIPS_. arXiv preprint arXiv:2402.04497, 2024b.
* Alman and Song (2024c) Josh Alman and Zhao Song. Fast rope attention: Combining the polynomial method and fast fourier transform. In _manuscript_, 2024c.
* Ambrogioni (2023) Luca Ambrogioni. In search of dispersed memories: Generative diffusion models are associative memory networks. _arXiv preprint arXiv:2309.17290_, 2023.
* Bao et al. (2022) Fan Bao, Chongxuan Li, Yue Cao, and Jun Zhu. All are worth words: a vit backbone for score-based diffusion models. In _NeurIPS 2022 Workshop on Score-Based Methods_, 2022.
* Benton et al. (2024) Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Nearly d-linear convergence bounds for diffusion models via stochastic localization. In _The Twelfth International Conference on Learning Representations (ICLR)_, 2024.
* De Bortoli (2022) Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. _Transactions on Machine Learning Research_, 2022. ISSN 2835-8856.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Chen et al. (2024a) Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-SalphaS: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In _The Twelfth International Conference on Learning Representations (ICLR)_, 2024a.
* Chen et al. (2020a) Minshuo Chen, Xingguo Li, and Tuo Zhao. On generalization bounds of a family of recurrent neural networks. In _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 108, pages 1233-1243, 2020a.
* Chen et al. (2020b) Minshuo Chen, Wenjing Liao, Hongyuan Zha, and Tuo Zhao. Distribution approximation and statistical estimation guarantees of generative adversarial networks. _arXiv preprint arXiv:2002.03938_, 2020b.
* Chen et al. (2020c)Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. In _International Conference on Machine Learning (ICML)_, pages 4672-4712. PMLR, 2023.
* Chen et al. (2024b) Minshuo Chen, Song Mei, Jianqing Fan, and Mengdi Wang. An overview of diffusion models: Applications, guided generation, statistical rates and optimization. _arXiv preprint arXiv:2404.07771_, 2024b.
* Cygan et al. (2016) Marek Cygan, Holger Dell, Daniel Lokshtanov, Daniel Marx, Jesper Nederlof, Yoshio Okamoto, Ramamohan Paturi, Saket Saurabh, and Magnus Wahlstrom. On problems as hard as cnf-sat. _ACM Transactions on Algorithms (TALG)_, 12(3):1-24, 2016.
* Diao et al. (2018) Huaian Diao, Zhao Song, Wen Sun, and David Woodruff. Sketching for kronecker product regression and p-splines. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 1299-1308. PMLR, 2018.
* Diao et al. (2019) Huaian Diao, Rajesh Jayaram, Zhao Song, Wen Sun, and David Woodruff. Optimal sketching for kronecker product regression and low rank approximation. _Advances in neural information processing systems (NeurIPS)_, 32, 2019.
* Edelman et al. (2022) Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning (ICML)_, pages 5793-5831. PMLR, 2022.
* Esser et al. (2024) Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. _arXiv preprint arXiv:2403.03206_, 2024.
* Floridi and Chiriatti (2020) Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. _Minds and Machines_, 30:681-694, 2020.
* Gao et al. (2023a) Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 23164-23173, 2023a.
* Gao et al. (2023b) Yeqi Gao, Zhao Song, Weixin Wang, and Junze Yin. A fast optimization view: Reformulating single layer attention in llm based on tensor and svm trick, and solving it in matrix multiplication time. _arXiv preprint arXiv:2309.07418_, 2023b.
* Gao et al. (2023c) Yeqi Gao, Zhao Song, and Shenghao Xie. In-context learning for attention scheme: from single softmax regression to multiple softmax regression via a tensor trick. _arXiv preprint arXiv:2307.02419_, 2023c.
* Gu et al. (2024) Jiuxiang Gu, Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Tensor attention training: Provably efficient learning of higher-order transformers. _arXiv preprint arXiv:2405.16411_, 2024.
* Guan et al. (2024) Jiaqi Guan, Xiangxin Zhou, Yuwei Yang, Yu Bao, Jian Peng, Jianzhu Ma, Qiang Liu, Liang Wang, and Quanquan Gu. Decompdiff: diffusion models with decomposed priors for structure-based drug design. _arXiv preprint arXiv:2403.07902_, 2024.
* Havrilla and Liao (2024) Alexander Havrilla and Wenjing Liao. Understanding scaling laws with statistical and approximation theory for transformer neural networks on intrinsically low-dimensional data. In _The Thirty-eighth Annual Conference on Neural Information Processing Systems_, 2024.
* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* Hoover et al. (2023) Benjamin Hoover, Hendrik Strobelt, Dmitry Krotov, Judy Hoffman, Zsolt Kira, and Duen Horng Chau. Memory in plain sight: A survey of the uncanny resemblances between diffusion models and associative memories. _arXiv preprint arXiv:2309.16750_, 2023.
* Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* Huang et al. (2021)Jerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, and Han Liu. On sparse modern hopfield model. In _Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)_, 2023.
* Hu et al. (2024) Jerry Yao-Chieh Hu, Pei-Hsuan Chang, Haozheng Luo, Hong-Yu Chen, Weijian Li, Wei-Po Wang, and Han Liu. Outlier-efficient hopfield layers for large transformer-based models. In _Forty-first International Conference on Machine Learning (ICML)_, 2024a.
* Hu et al. (2024) Jerry Yao-Chieh Hu, Thomas Lin, Zhao Song, and Han Liu. On computational limits of modern hopfield models: A fine-grained complexity analysis. In _Forty-first International Conference on Machine Learning (ICML)_, 2024b.
* Hu et al. (2024) Jerry Yao-Chieh Hu, Maojiang Su, En-Jui Kuo, Zhao Song, and Han Liu. Computational limits of low-rank adaptation (lora) for transformer-based models. _arXiv preprint arXiv:2406.03136_, 2024c.
* Hu et al. (2024) Jerry Yao-Chieh Hu, Wei-Po Wang, Ammar Gilani, Chenyang Li, Zhao Song, and Han Liu. Fundamental limits of prompt tuning transformers: Universality, capacity and efficiency. _arXiv preprint arXiv:2411.16525_, 2024d.
* Hu et al. (2024) Jerry Yao-Chieh Hu, Dennis Wu, and Han Liu. Provably optimal memory capacity for modern hopfield models: Transformer-compatible dense associative memories as spherical codes. In _Thirty-eighth Conference on Neural Information Processing Systems (NeurIPS)_, volume 38, 2024e.
* Hu et al. (2024) Jerry Yao-Chieh Hu, Weimin Wu, Yi-Chen Lee, Yu-Chao Huang, Minshuo Chen, and Han Liu. On statistical rates of conditional diffusion transformers: Approximation, estimation and minimax optimality. _arXiv preprint arXiv:2411.17522_, 2024f.
* Impagliazzo and Paturi (2001) Russell Impagliazzo and Ramamohan Paturi. On the complexity of k-sat. _Journal of Computer and System Sciences_, 62(2):367-375, 2001.
* Ji et al. (2021) Yanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri. Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome. _Bioinformatics_, 37(15):2112-2120, 2021.
* Jiang and Li (2023) Haotian Jiang and Qianxiao Li. Approximation theory of transformer networks for sequence modeling. _arXiv preprint arXiv:2305.18475_, 2023.
* Kajitsuka and Sato (2023) Tokio Kajitsuka and Issei Sato. Are transformers with one layer self-attention using low-rank weight matrices universal approximators? _arXiv preprint arXiv:2307.14023_, 2023.
* Kim et al. (2022) Junghwan Kim, Michelle Kim, and Barzan Mozafari. Provable memorization capacity of transformers. In _The Eleventh International Conference on Learning Representations (ICLR)_, 2022.
* Liang et al. (2024a) Yingyu Liang, Heshan Liu, Zhenmei Shi, Zhao Song, Zhuoyan Xu, and Junze Yin. Conv-basis: A new paradigm for efficient attention inference and gradient computation in transformers. _arXiv preprint arXiv:2405.05219_, 2024a.
* Liang et al. (2024b) Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, and Yufa Zhou. Beyond linear approximations: A novel pruning approach for attention matrix. _arXiv preprint arXiv:2410.11261_, 2024b.
* Liang et al. (2024c) Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, and Yufa Zhou. Multi-layer transformers gradient can be approximated in almost linear time. _arXiv preprint arXiv:2408.13233_, 2024c.
* Liang et al. (2024d) Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Tensor attention training: Provably efficient learning of higher-order transformers. _arXiv preprint arXiv:2405.16411_, 2024d.
* Liu et al. (2024) Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao Sun. Sora: A review on background, technology, limitations, and opportunities of large vision models, 2024.
* Liu et al. (2021) Zhonghua Liu, Yue Lu, Zhihui Lai, Weihua Ou, and Kaibing Zhang. Robust sparse low-rank embedding for image dimension reduction. _Applied Soft Computing_, 113:107907, 2021.
* Liu et al. (2020)Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. In _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10209-10218. IEEE, 2023.
* Ma et al. (2024) Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. _arXiv preprint arXiv:2401.08740_, 2024.
* Mahdavi et al. (2023) Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis. Memorization capacity of multi-head attention in transformers. _arXiv preprint arXiv:2306.02010_, 2023.
* Mo et al. (2023) Shentong Mo, Enze Xie, Ruihang Chu, Lanqing Hong, Matthias Niessner, and Zhenguo Li. Dit-3d: Exploring plain diffusion transformers for 3d shape generation. _Advances in Neural Information Processing Systems (NeurIPS)_, 36, 2023.
* Nichol et al. (2021) Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.
* Oko et al. (2023) Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution estimators. In _International Conference on Machine Learning (ICML)_, pages 26517-26582. PMLR, 2023.
* OpenAI (2024) OpenAI. Sora: A video generative model based on transformer diffusion. _OpenAI Research_, 2024. Accessed: 08/16/2024.
* Peebles & Xie (2023) William Peebles and Saining Xie. Scalable diffusion models with transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 4195-4205, 2023.
* Pope et al. (2021) Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. _arXiv preprint arXiv:2104.08894_, 2021.
* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.
* Ramsauer et al. (2020) Hubert Ramsauer, Bernhard Schafl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic, Geir Kjetil Sandve, et al. Hopfield networks is all you need. _arXiv preprint arXiv:2008.02217_, 2020.
* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)_, pages 10684-10695, 2022.
* Song & Ermon (2019) Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems (NeurIPS)_, 32, 2019.
* Su & Wu (2018) Bing Su and Ying Wu. Learning low-dimensional temporal representations. In _International Conference on Machine Learning (ICML)_, pages 4761-4770. PMLR, 2018.
* Tang & Zhao (2024) Wenpin Tang and Hanyang Zhao. Score-based diffusion models via stochastic differential equations-a technical tutorial. _arXiv preprint arXiv:2402.07487_, 2024.
* Vahdat et al. (2021) Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 34, pages 11287-11302, 2021.
* Wang et al. (2024) Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, and Quanquan Gu. Diffusion language models are versatile protein learners. _arXiv preprint arXiv:2402.18567_, 2024a.
* Wang et al. (2020)Yan Wang, Lihao Wang, Yuning Shen, Yiqun Wang, Huizhuo Yuan, Yue Wu, and Quanquan Gu. Protein conformation generation via force-guided se (3) diffusion models. _arXiv preprint arXiv:2403.14088_, 2024b.
* Wang et al. (2023) Yihan Wang, Jatin Chauhan, Wei Wang, and Cho-Jui Hsieh. Universality and limitations of prompt tuning. _Advances in Neural Information Processing Systems (NeurIPS)_, 36, 2023.
* Wibisono et al. (2024) Andre Wibisono, Yihong Wu, and Kaylee Yingxi Yang. Optimal score estimation via empirical bayes smoothing. _arXiv preprint arXiv:2402.07747_, 2024.
* Vassilevska Williams (2018) Virginia Vassilevska Williams. On some fine-grained questions in algorithms and complexity. In _Proceedings of the international congress of mathematicians: Rio de Janeiro 2018_, pages 3447-3487. World Scientific, 2018.
* Wu et al. (2024a) Dennis Wu, Jerry Yao-Chieh Hu, Teng-Yun Hsiao, and Han Liu. Uniform memory retrieval with larger capacity for modern hopfield models. In _Forty-first International Conference on Machine Learning (ICML)_, 2024a.
* Wu et al. (2024b) Dennis Wu, Jerry Yao-Chieh Hu, Weijian Li, Bo-Yu Chen, and Han Liu. STanhop: Sparse tandem hopfield model for memory-enhanced time series prediction. In _The Twelfth International Conference on Learning Representations (ICLR)_, 2024b.
* Yun et al. (2020) Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In _International Conference on Learning Representations (ICLR)_, 2020.
* Zheng et al. (2023) Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. _arXiv preprint arXiv:2306.09305_, 2023.
* Zhou et al. (2024a) Xiangxin Zhou, Xiwei Cheng, Yuwei Yang, Yu Bao, Liang Wang, and Quanquan Gu. Decompopt: Controllable and decomposed diffusion models for structure-based molecular optimization. _arXiv preprint arXiv:2403.13829_, 2024a.
* Zhou et al. (2024b) Xiangxin Zhou, Dongyu Xue, Ruizhe Chen, Zaixiang Zheng, Liang Wang, and Quanquan Gu. Antigen-specific antibody design via direct energy-based preference optimization. _arXiv preprint arXiv:2403.16576_, 2024b.
* Zhou et al. (2023) Zhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. Dnabert-2: Efficient foundation model and benchmark for multi-species genome. _arXiv preprint arXiv:2306.15006_, 2023.
* Zhou et al. (2024c) Zhihan Zhou, Weimin Wu, Harrison Ho, Jiayi Wang, Lizhen Shi, Ramana V Davuluri, Zhong Wang, and Han Liu. Dnabert-s: Learning species-aware dna embedding with genome foundation models. _ArXiv_, 2024c.
* Zhu et al. (2023) Zhenyu Zhu, Francesco Locatello, and Volkan Cevher. Sample complexity bounds for score-matching: Causal discovery and generative modeling. _Advances in Neural Information Processing Systems (NeurIPS)_, 36, 2023.

## Appendix

* A More Discussions on Low-Dimensional Linear Latent Space
* B Notation Table
* C Related Works
* D Supplementary Theoretical Background
* D.1 Diffusion Models
* D.2 Proof of Lemma 2.1
* D.3 Preliminaries: Strong Exponential Time Hypothesis (SETH) and Tensor Trick
* E More Background and Auxiliary Lemmas: Universal Approximation of Transformers via Piecewise Approximation
* E.1 Piecewise-Constant Function Approximates Compact-Supported Continuous Function
* E.2 Modified Transformer Approximates Piecewise-constant Function
* E.2.1 Quantization by Modified Feed-forward Layers
* E.2.2 Contextual Mapping by Modified Self-attention Layers
* E.2.3 Map to the Desired Output by Modified Feed-forward Layers
* E.3 Standard Transformers Approximate Modified Transformers
* E.4 All Together: Standard Transformers Approximate Compact-supported Continuous Functions
* E.5 Supplementary Proofs
* E.5.1 Preliminaries
* E.5.2 Proof of Lemma E.2
* E.5.3 Proof of Lemma E.4
* E.5.4 Proof of Lemma E.5
* E.5.5 Proof of Lemma E.7
* F Proofs of Section 3
* F.1 Proof of Theorem 3.1
* F.1.1 Auxiliary Lemmas for Theorem 3.1
* F.1.2 Main Proof of Theorem 3.1
* F.2 Proof of Theorem 3.2
* F.2.1 Auxiliary Lemmas for Theorem 3.2
* F.2.2 Proof of Theorem 3.2
* F.3 Proof of Corollary 3.2.1
* F.3.1 Auxiliary Lemmas
* F.3.2 Main Proof of Corollary 3.2.1
* G Proofs of Section 4
* G.1 Auxiliary Theoretical Results for Theorem 4.1
* G.1.1 Low-Rank Decomposition of DiT Gradients
* G.1.2 Low-Rank Approximations of Building Blocks Part I: \(f(\cdot),q(\cdot)\), and \(c(\cdot)\)
* G.1.3 Low-Rank Approximations of Building Blocks Part II: \(p(\cdot)\)
* G.2 Proof of Theorem 4.1
More Discussions on Low-Dimensional Linear Latent Space

Our analysis is based on the low-dimensional linear latent space assumption (Assumption 2.1). Here we further discuss this in light of our theoretical results

Our results are more general and extend beyond Assumption 2.1. In addition to the case where \(d_{0}<D\), our theoretical results apply to two other settings: \(d_{0}=D\) and \(d_{0}>D\). Especially, for \(d_{0}=D\), our results still hold by setting \(B\) as the identity matrix \(I_{D}\). Namely, our results hold after removing the linear subspace assumption.

* Statistically, for score approximation, score estimation, and distribution estimation, the upper bounds depend on the dimension of the latent variable \(d_{0}\), other than \(d\). A smaller \(d_{0}\) allows for a reduced model size to achieve a specified approximation error compared to a larger one (Theorem 3.1). Additionally, with a smaller \(d_{0}\), both score and distribution estimation errors are reduced relative to scenarios with larger ones (Theorem 3.2 and Corollary 3.2.1).
* Computationally, smaller \(d_{0}\) benefits the provably efficient criteria (Proposition 4.1, almost-linear time algorithms for forward inference (Proposition 4.2) and backward computation (Theorem 4.1).

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_FAIL:19]

estimation under the assumption that the initial density is supported on \([-1,1]^{D}\) and smooth in the boundary.

Among these works, our work is built on and closest to [Chen et al., 2023], as both assume the data has a low-dimensional structure4. However, our work differs in three key aspects. First, beyond the simple ReLU networks considered in [Chen et al., 2023], we provide the first score approximation analysis for DiTs with a transformer-based score estimator. Second, our work is the first to provide the statistical rates of DiTs (score and distribution estimation) based on transformer universality [Yun et al., 2020] and norm-based converging number bound [Edelman et al., 2022], supporting the practical success of DiTs [Esser et al., 2024, Ma et al., 2024]. Lastly, our work provides the first comprehensive analysis of the computational limits and all possible efficient DiT algorithms/methods for both forward inference and backward training. This offers timely insights into the empirical computational inefficiency of DiTs [Liu et al., 2024] and guidance for future DiT architectures.

Footnote 4: Recent work by Havrilla and Liao [2024] examines the generalization and approximation of transformers under Hölder smoothness and low-dimensional subspace assumptions.

Transformers in Foundation Models: Transformer-Based Pretrained Models.Transformers-based pretrained models utilize attention mechanisms to process sequential data, enabling the learning of contextual relationships for tasks like natural language understanding and generation. These models encompass three types: encoder-based, decoder-based, and diffusion transformers. Encoder-based transformers, such as DNABERT [Zhou et al., 2024, 2023, Ji et al., 2021], employ bidirectional attention to extract feature representations DNABERT shows great potential to capture complex patterns of genome sequences and improve tasks such as gene prediction. Decoder-based transformers generate output sequences from encoded information using unidirectional attention, such as ChatGPT [Radford et al., 2019, Floridi and Chiriatti, 2020, Brown et al., 2020] for natural language. The diffusion transformers generate a sequence toward a target distribution, such as SoRA [Liu et al., 2024] and Videofusion [Luo et al., 2023] for video generation and DecompDiff [Guan et al., 2024] for drug design. In our paper, we present an early exploration of the statistical and computational limits of diffusion transformer models.

Supplementary Theoretical Background

In this section, we provide some further background. We show the details about the forward and backward process in Diffusion Models in Appendix D.1. Besides, we give the details of the proof about the score decomposition in Appendix D.2.

### Diffusion Models

Forward Process.Diffusion models gradually add noise to the original data in the forward process. We describe the forward process as the following SDE

\[\mathrm{d}x_{t}=-\frac{1}{2}w(t)x_{t}\mathrm{d}t+\sqrt{w(t)}\mathrm{d}W_{t},\; x_{t}\in\mathbb{R}^{D},\] (D.1)

where \(x_{0}\sim P_{0}\), \((W_{t})_{t\geq 0}\) is a standard Brownian motion, and \(w(t)>0\) is a nondecreasing weighting function. Let \(P_{t}\) and \(p_{t}\) denote the marginal distribution and destiny of \(x_{t}\). The conditional distribution \(P(x_{t}|x_{0})\) follows \(N(\beta(t)x_{0},\sigma(t)I_{D})\), where \(\beta(t)=\exp\!\left(-\int_{0}^{t}w(s)\mathrm{d}s/2\right)\) and \(\sigma(t)=1-\beta^{2}(t)\). In practice, (D.1) terminates at a large enough \(T\) such that \(P_{T}\) is close to \(N(0,I_{D})\).

Backward Process.We obtain the backward process \(y_{t}:=x_{T-t}\) by reversing (D.1). The backward process satisfies

\[\mathrm{d}y_{t}=\left[\frac{1}{2}w(T-t)y_{t}+w(T-t)\nabla\log p_{T-t}(y_{t}) \right]\mathrm{d}t+\sqrt{w(T-t)}\mathrm{d}\overline{W}_{t},\] (D.2)

where the score function \(\nabla\log p_{t}(\cdot)\) is the gradient of log probability density function of \(x_{t}\), and \(\overline{W}_{t}\) is a reversed Brownian motion. However, \(\nabla\log p_{t}(\cdot)\) and \(P_{T}\) are both unknown in (D.2). To resolve this, we use a score estimator \(s_{W}(\cdot,t)\) to replace \(\nabla\log p_{t}(\cdot)\), where \(s_{W}(\cdot,t)\) is usually a neural network with parameters \(W\). Secondly, we replace \(P_{T}\) by the standard Gaussian distribution. Consequently, we obtain the following SDE

\[\mathrm{d}\widetilde{y}_{t}=\left[\frac{1}{2}w(T-t)\widetilde{y}_{t}+w(T-t)s_{ W}(\widetilde{y}_{t},T-t)\right]\mathrm{d}t+\sqrt{w(T-t)}\mathrm{d}\overline{W }_{t},\;\widetilde{y}_{0}\sim N(0,I_{D}).\] (D.3)

In practice, we use discrete schemes of (D.3) to generate data, following (Song and Ermon, 2019). We use \(\mu>0\) to denote the discretization step size. For \(t\in[k\mu,(k+1)\mu]\), we have

\[\mathrm{d}\widetilde{y}_{t}^{\mu}=\left[\frac{1}{2}w(T-t)\widetilde{y}_{k\mu} ^{\mu}+w(T-t)s_{W}(\widetilde{y}_{k\mu}^{\mu},T-k\mu)\right]\mathrm{d}t+\sqrt{ w(T-t)}\mathrm{d}\overline{W}_{t}.\] (D.4)

### Proof of Lemma 2.1

Here we restate the proof of (Chen et al., 2023, Lemma 1) for completeness.

Proof.: Recall \(x=Bh\) by Assumption 2.1 with \(x\in\mathbb{R}^{D}\), \(B\in\mathbb{R}^{D\times d_{0}}\) and \(h\in\mathbb{R}^{d_{0}}\).

By the forward process (D.1), we have

\[p_{t}(\overline{x})=\int\psi_{t}(\overline{x}\mid Bh)p_{h}(h) \mathrm{d}h,\] (D.5)

where

\[\psi_{t}(\overline{x}\mid Bh)=\left[2\pi h(t)\right]^{-D/2}\exp\left(-\frac{ \left\|\beta(t)Bh-\overline{x}\right\|_{2}^{2}}{2\sigma(t)}\right),\] (D.6)

is the Gaussian transition kernel.

Then we write the score function as

\[\nabla\log p_{t}(\overline{x}) =\frac{\nabla p_{t}(\overline{x})}{p_{t}(\overline{x})}\] (D.7) \[=\frac{\nabla\int\psi_{t}(\overline{x}\mid Bh)p_{h}(h)\mathrm{d}h} {\int\psi_{t}(\overline{x}\mid Bh)p_{h}(h)\mathrm{d}h} \text{(By pluging in }p_{t}(\overline{x})\text{)}\] \[=\frac{\int\nabla\psi_{t}(\overline{x}\mid Bh)p_{h}(h)\mathrm{d}h} {\int\psi_{t}(\overline{x}\mid Bh)p_{h}(h)\mathrm{d}h}, \text{(By interchanging }\int\text{ with }\nabla\text{)}\]

where the last equality holds since \(\psi_{t}(\overline{x}\mid Bh)\) is continuously differentiable in \(\overline{x}\).

Plugging (D.6) into (D.7), we have

\[\nabla\log p_{t}(\overline{x})\] \[=\frac{[2\pi h(t)]^{-D/2}}{\int\psi_{t}(\overline{x}\mid Bh)p_{h} (h)\mathrm{d}h}\int\frac{1}{\sigma(t)}\left(\beta(t)Bh-\overline{x}\right) \exp\left(-\frac{\left\|\beta(t)Bh-\overline{x}\right\|_{2}^{2}}{2\sigma(t)} \right)p_{h}(h)\mathrm{d}h.\]

We then decompose above score function by projecting of \(\overline{x}\) into \(\mathrm{Span}(B)\), i.e., replacing \(-\overline{x}\) with \(-BB^{\top}\overline{x}-(I_{D}-BB^{\top})\overline{x}\):

\[\nabla\log p_{t}(\overline{x})\] \[=\frac{[2\pi h(t)]^{-D/2}}{\int\psi_{t}(\overline{x}\mid Bh)p_{h} (h)\mathrm{d}h}\] \[\quad\cdot\int\frac{1}{\sigma(t)}\Bigg{[}\left(\beta(t)Bh-BB^{ \top}\overline{x}\right)-\left(I_{D}-BB^{\top}\right)\overline{x}\Bigg{]}\exp \left(-\frac{\left\|\beta(t)Bh-\overline{x}\right\|_{2}^{2}}{2\sigma(t)} \right)p_{h}(h)\mathrm{d}h.\]

Absorbing the factor of \([2\pi h(t)]^{-D/2}\) into the Gaussian kernel \(\psi_{t}(\overline{x}\mid Bh)\), we have

\[\nabla\log p_{t}(\overline{x})\] \[=\frac{[2\pi h(t)]^{-D/2}}{\int\psi_{t}(\overline{x}\mid Bh)p_{h} (h)\mathrm{d}h}\int\frac{1}{\sigma(t)}\left(\beta(t)Bh-BB^{\top}\overline{x} \right)\exp\left(-\frac{\left\|\beta(t)Bh-\overline{x}\right\|_{2}^{2}}{2 \sigma(t)}\right)p_{h}(h)\mathrm{d}h\] \[\quad-\frac{1}{\int\psi_{t}(\overline{x}|Bh)p_{h}(h)\mathrm{d}h} \left(\frac{1}{\sigma(t)}\left(I_{D}-BB^{\top}\right)\overline{x}\right)\int \psi_{t}(\overline{x}\mid Bh)p_{h}(h)\mathrm{d}h\] \[=\underbrace{\frac{1}{\int\psi_{t}(\overline{x}\mid Bh)p_{h}(h) \mathrm{d}h}\int\frac{1}{\sigma(t)}\left(\beta(t)Bh-BB^{\top}\overline{x} \right)\psi_{t}(\overline{x}\mid Bh)p_{h}(h)\mathrm{d}h}_{:=s_{+}}\underbrace{ \frac{1}{\sigma(t)}\left(I_{D}-BB^{\top}\right)\overline{x}}_{:=s_{-}}.\]

To further simplify \(s_{+}\), we decompose \(\psi_{t}(\overline{x}\mid Bh)\) as

\[\psi_{t}(\overline{x}\mid Bh)\] \[=[2\pi h(t)]^{-D/2}\exp\left(-\frac{1}{2\sigma(t)}\big{\|}\beta(t )Bh-\overline{x}\big{\|}_{2}^{2}\right)\] \[=[2\pi h(t)]^{-D/2}\exp\left(-\frac{1}{2\sigma(t)}\big{\|}\beta(t )Bh-BB^{\top}\overline{x}-\left(I_{D}-BB^{\top}\right)\overline{x}\big{\|}_{2} ^{2}\right)\] \[=[2\pi h(t)]^{-D/2}\exp\Bigg{(}-\frac{1}{2\sigma(t)}\Big{(}\big{\|} \beta(t)Bh-BB^{\top}\overline{x}\big{\|}_{2}^{2}+\big{\|}\big{(}I_{D}-BB^{ \top}\big{)}\overline{x}\big{\|}_{2}^{2}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad-2(B(\beta(t)h-B^{\top} \overline{x}))^{\top}(I_{D}-BB^{\top})\overline{x}\Big{)}\Bigg{)}\]\[=[2\pi h(t)]^{-D/2}\exp\left(-\frac{1}{2\sigma(t)}\left(\left\| \beta(t)Bh-BB^{\top}\overline{x}\right\|_{2}^{2}+\left\|\left(I_{D}-BB^{\top} \right)\overline{x}\right\|_{2}^{2}\right)\right)\] \[=\underbrace{[2\pi h(t)]^{-d_{0}/2}\exp\left(-\frac{\left\|\beta(t )h-B^{\top}\overline{x}\right\|_{2}^{2}}{2\sigma(t)}\right)}_{\right:=\psi_{t} (B^{\top}\overline{x}|h)}\cdot\underbrace{[2\pi h(t)]^{-(D-d_{0})/2}\exp\left( -\frac{\left\|\left(I_{D}-BB^{\top}\right)\overline{x}\right\|_{2}^{2}}{2 \sigma(t)}\right)}_{\right:=\psi_{t}(\left(I_{D}-BB^{\top}\right)\overline{x}) \left(\text{since $B$ has orthonormal columns}\right)},\]

where both \(\psi_{t}\left(B^{\top}\overline{x}\mid h\right)\) and \(\psi_{t}\left((I_{D}-BB^{\top})\overline{x}\right)\) are Gaussian.

Plugging \(\psi_{t}(\overline{x}\mid Bh)=\psi_{t}\left(B^{\top}\overline{x}\mid h\right) \psi_{t}\left((I_{D}-BB^{\top})\overline{x}\right)\) into \(s_{+}\), we obtain

\[s_{+}(\overline{x},t) =C\int\frac{1}{\sigma(t)}\left(\beta(t)Bh-BB^{\top}\overline{x} \right)\psi_{t}(B^{\top}\overline{x}\mid h)\psi_{t}((I_{D}-BB^{\top}) \overline{x})p_{h}(h)\mathrm{d}h\] \[=C\psi_{t}((I_{D}-BB^{\top})\overline{x})\int\frac{1}{\sigma(t)} \left(\beta(t)Bh-BB^{\top}\overline{x}\right)\psi_{t}(B^{\top}\overline{x}\mid h )p_{h}(h)\mathrm{d}h\] \[=\frac{1}{\int\psi_{t}(B^{\top}\overline{x}\mid h)p_{h}(h) \mathrm{d}h}\int\frac{1}{\sigma(t)}\left(\beta(t)Bh-BB^{\top}\overline{x} \right)\psi_{t}(B^{\top}\overline{x}\mid h)p_{h}(h)\mathrm{d}h,\]

where \(C\coloneqq[\psi_{t}((I_{D}-BB^{\top})\overline{x})\int\psi_{t}(B^{\top} \overline{x}\mid h)p_{h}(h)\mathrm{d}h]^{-1}\).

Notably, \(s_{+}\) depends only on the projected data \(B^{\top}\overline{x}\). Therefore, we are able to replace \(s_{+}(\overline{x},t)\) with \(s_{+}(B^{\top}\overline{x},t)\). The benefit is that the dimension \(d_{0}\) of the first input in \(s_{+}(B^{\top}\overline{x},t)\) is much smaller.

Lastly, by denoting \(\overline{h}=B^{\top}\overline{x}\) such that \(\nabla_{\overline{h}}\psi_{t}(\overline{h}\mid h)=(\beta(t)h-\overline{h})\psi _{t}(\overline{h}\mid h)/\sigma(t)\), we arrive at

\[s_{+}(B^{\top}\overline{x},t) =B\int\frac{\nabla_{\overline{h}}\psi_{t}(\overline{h}\mid h)p_{ h}(h)}{\int\psi_{t}(\overline{h}\mid h)p_{h}(h)\mathrm{d}h}\mathrm{d}h\] \[=B\nabla\log p_{t}^{h}(B^{\top}x). \left(p_{t}^{h}(\overline{h})\coloneqq\int\psi_{t}(\overline{h} |h)p_{h}(h)\mathrm{d}h\right)\]

This completes the proof. 

### Preliminaries: Strong Exponential Time Hypothesis (SETH) and Tensor Trick

Here we present the ideas we built upon for Section 4.

Strong Exponential Time Hypothesis (SETH).Impagliazzo and Paturi (2001) introduce the Strong Exponential Time Hypothesis (SETH) as a stronger form of the \(\mathtt{P}\neq\mathtt{NP}\) conjecture. It suggests that our current best SAT algorithms are optimal and is a popular conjecture for proving fine-grained lower bounds for a wide variety of algorithmic problems (Cygan et al., 2016; Williams, 2018).

**Hypothesis 1** (SETH).: For every \(\epsilon>0\), there is a positive integer \(k\geq 3\) such that \(k\)-SAT on formulas with \(n\) variables cannot be solved in \(\mathcal{O}(2^{(1-\epsilon)n})\) time, even by a randomized algorithm.

Tensor Trick for Computing Gradients.The tensor trick (Diao et al., 2019, 2018) is an instrument to compute complicated gradients in a clean and tractable fashion. We start with some definitions.

**Definition D.1** (Vectorization).: For any matrix \(X\in\mathbb{R}^{L\times d}\), we define \(\underline{X}\coloneqq\operatorname{vec}(X)\in\mathbb{R}^{Ld}\) such that \(X_{i,j}=\underline{X}_{(i-1)d+j}\) for all \(i\in[L]\) and \(j\in[d]\).

**Definition D.2** (Matrixization).: For any vector \(\underline{X}\in\mathbb{R}^{Ld}\), we define \(\operatorname{mat}(\underline{X})=X\) such that \(X_{i,j}=\operatorname{mat}(\underline{X})\coloneqq\underline{X}_{(i-1)d+j}\) for all \(i\in[L]\) and \(j\in[d]\), namely \(\operatorname{mat}(\cdot)=\operatorname{vec}^{-1}(\cdot)\).

**Definition D.3** (Kronecker Product).: Let \(A\in\mathbb{R}^{L_{a}\times d_{a}}\) and \(B\in\mathbb{R}^{L_{b}\times d_{b}}\). We define the Kronecker product of \(A\) and \(B\) as \(A\otimes B\in\mathbb{R}^{L_{a}L_{b}\times d_{a}d_{b}}\) such that \((A\otimes B)_{(i_{a}-1)L_{b}+i_{b},(J_{a}-1)d_{b}+j_{b}}\), is equal to \(A_{i_{a},j_{a}}B_{i_{b},j_{b}}\) with \(i_{a}\in[L_{a}],j_{a}\in[d_{a}],i_{b}\in[L_{b}],j_{b}\in[d_{b}]\).

**Definition D.4** (Sub-Block of a Tensor).: For any \(A\in\mathbb{R}^{L_{a}\times d_{a}}\) and \(B\in\mathbb{R}^{L_{b}\times d_{b}}\), let \(\mathsf{A}\coloneqq A\otimes B\in\mathbb{R}^{L_{a}\,L_{b}\times d_{a}d_{b}}\). For any \(\underline{j}\in[L_{a}]\), we define \(\mathsf{A}\underline{j}\in\mathbb{R}^{L_{b}\times d_{a}d_{b}}\) be the \(\underline{j}\)-th \(L_{b}\times d_{a}d_{b}\) sub-block of \(\mathsf{A}\).

**Lemma D.1** (Tensor Trick [20, 218]).: For any \(A\in\mathbb{R}^{L_{a}\times d_{a}}\), \(B\in\mathbb{R}^{L_{b}\times d_{b}}\) and \(X\in\mathbb{R}^{d_{a}\times d_{b}}\), it holds \(\operatorname{vec}\left(A^{\top}XB\right)=(A^{\top}\otimes B^{\top})\underline {X}\in\mathbb{R}^{L_{a}L_{b}}\).

To showcase the tensor trick, let's consider a (single data point) attention following [14, 20]. Setting \(D\coloneqq\operatorname{diag}\left(\exp\left(X^{\top}W_{K}^{\top}W_{Q}X\right) \mathds{1}_{L}\right)\) and \(W\coloneqq W_{K}W_{Q}^{\top}\in\mathbb{R}^{d\times d}\), we have

\[\mathcal{L}_{0}\coloneqq\big{\|}\,\underset{d\times d}{W_{V}}\,\underset{\in \mathbb{R}^{d\times L}}{X}\,\underset{\in\mathbb{R}^{d\times L}}{D^{-1}}\, \underset{\in\mathbb{R}^{d\times L}}{\exp\{X^{\top}WX\}}-\underset{\in\mathbb{ R}^{d\times L}}{Y}\,\big{\|}_{2}^{2}.\] (D.8)

**Proposition D.1** (Definition 4.7 of [14]).: By Definition D.3 and Definition D.4, we identify \(D_{\underline{j},\underline{j}}\coloneqq\left\langle\exp\left(\mathsf{A} \underline{j}\,\underline{W}\right),\mathds{1}_{L}\right\rangle\in\mathbb{R}\) for all \(\underline{j}\in[L]\), with \(\mathsf{A}\coloneqq X\otimes X\in\mathbb{R}^{L^{2}\times d^{2}}\) and \(\underline{W}\in\mathbb{R}^{d^{2}}\). Therefore, for each \(\underline{j}\in[L]\) and \(\underline{i}\in[d]\), it holds \(\mathcal{L}_{0}=\sum_{\underline{j}=1}^{L}\sum_{\underline{i}=1}^{d}\frac{1}{ 2}\left(\left\langle D^{-1}_{\underline{j},\underline{j}}\exp\left(\mathsf{A }\underline{j}\,\underline{W}\right),XW_{V}[\cdot,\underline{i}]\right\rangle -Y_{\underline{j},\underline{i}}\right)^{2}\).

The elegance of Proposition D.1 emerges when we vectorize the weights into vectors \(\underline{W}\), \(\underline{W}_{V}\), making the gradient computations (e.g., \(\nicefrac{{\mathcal{L}_{0}}}{{\underline{W}}}\) and \(\nicefrac{{\mathcal{L}_{0}}}{{\underline{W}_{V}}}\)) more tractable by avoiding complex matrix or tensor derivatives. This approach systematically simplifies the handling of chain-rule terms in the gradient computation of losses like \(\mathcal{L}_{0}\).

Fine-Grained Complexity for Transformer.Many recent works also utilize similar techniques from fine-grained complexity to analyze transformer architectures. Alman and Song [20, 21], Liang et al. [20], Alman and Song [20] explore the computational feasibility of inference and training for standard softmax and tensor attention. Liang et al. [20] extend the single-layer training results from [13] to deep transformer models. [11] extend [13] to provide a fast attention gradient approximation based on Fourier transform. [11] extend [13] to sparse attention matrix. Hu et al. [20] study the computational limits of inference and training in prompt-tuning for pretrained transformers. Hu et al. [20] study the computational limits of LoRA [15] in transformers, identifying norm-bound conditions for efficient LoRA training and proving the existence of nearly linear-time LoRA algorithms.

Our work is closest to [13]. Our forward inference computational results build on [13]. Our backward training computational results are related to [13] but include additional analysis on reshaping and latent embedding.

More Background and Auxiliary Lemmas: Universal Approximation of Transformers via Piecewise Approximation

Here, we review the universal approximation of transformers following (Yun et al., 2020).

Our goal is to reproduce the results of (Yun et al., 2020) and use or modify them as auxiliary lemmas for proofs of Section3 (i.e., AppendixF.)

We start with their central result and prove it in the rest of the section.

**Lemma E.1** (Universal Approximation of Transformers, Theorem3 of (Yun et al., 2020)).: Let \(\epsilon>0\). For any given compact-supported continuous function \(f:\mathbb{R}^{d\times L}\to\mathbb{R}^{d\times L}\), there exists a transformer network \(f_{\mathcal{T}}\in\mathcal{T}_{p}^{2,1,4}\), such that

\[\left(\int\|f_{\mathcal{T}}(X)-f(X)\|_{F}^{2}\mathrm{d}X\right)^{1/2}\leq\epsilon.\]

Proof Overview.We use the following proof strategy:

* **Step 1.** We show that the piecewise-constant function is able to approximate compact-supported continuous function in AppendixE.1.
* **Step 2.** We define modified self-attention and feed-forward layers to construct the modified transformer. We show that the modified transformer is able to approximate piecewise-constant function in AppendixE.2.
* **Step 3.** We show that the standard transformer in AppendixE.3 is able to approximate the modified transformer.

We provide details of **Step 1.** in AppendixE.1, **Step 2.** in AppendixE.2, and **Step 3.** in AppendixE.3. Then we summarize our results in AppendixE.4.

### Piecewise-Constant Function Approximates Compact-Supported Continuous Function

In this subsection, we show that the piecewise-constant function is able to approximate compact-supported continuous function.

We start with the definition of the compact-supported continuous functions of interest.

**Assumption E.1**.: Without loss of generality, we assume that the target function in discussion is supported on \([0,1]^{d\times L}\). We denote the set of \([0,1]^{d\times L}\)-supported continuous functions as \(\mathcal{F}\).

We introduce the notion of grid and cube for the compact support \([0,1]^{d\times L}\).

**Definition E.1** (Grid and Cube with Width\(\delta\)).: Given a grid width \(\delta\), let \(\mathcal{G}_{\delta}\coloneqq\{0,\delta,\dots,1-\delta\}^{d\times L}\) denote the set of grids within \([0,1]^{d\times L}\). For a grid point \(G=(G_{j\in[d],k\in[L]})\in\mathcal{G}_{\delta}\), we denote its associated cube as

\[\mathcal{S}_{G}:=\otimes_{j=1}^{d}\otimes_{k=1}^{L}\left[G_{j,k},G_{j,k}+ \delta\right)\subset[0,1]^{d\times L}.\]

Each cube \(\mathcal{S}_{G}\) represents a hyper rectangular in the multi-dimensional space \([0,1]^{d\times L}\), constructed to discretize the space into smaller subspaces.

We introduce the notion of piecewise-constant fucntion class w.r.t. the \([0,1]^{d\times L}\)-supported continuous function class \(\mathcal{F}\).

**Definition E.2** (Piecewise-Constant Function Class).: Let \(f_{\delta}\) denote the piesewise constant function of grid width \(\delta\), and \(\mathds{1}\{\cdot\}\) denote the indicator function. For each \(G\in\mathcal{G}_{\delta}\), and any matrix \(A_{G}\in\mathbb{R}^{d\times L}\), we define the piecewise-constant function class as

\[\mathcal{F}(\delta)\coloneqq\left\{f_{\delta}:X\to\sum\nolimits_{G\in\mathcal{ G}_{\delta}}A_{G}\cdot\mathds{1}\{X\in\mathcal{S}_{G}\},A_{G}\in\mathbb{R}^{d \times L}\right\}.\] (E.1)We recall that for a given sequence-to-sequence function \(f\),

\[\|f\|_{L^{2}}:=\bigg{(}\int\|f(X)\|_{F}^{2}\mathrm{d}X\bigg{)}^{1/2}.\]

We approximate the compact-supported function with a piecewise-constant function in the next lemma.

**Lemma E.2**.: (Lemma 8 of [25]) For any given \(f\in\mathcal{F}\) and \(\epsilon/3>0\), we can find a \(\delta^{\star}>0\), such that there exists a \(f_{\delta^{\star}}\in\mathcal{F}(\delta^{\star})\) satisfying \(\|f-f_{\delta^{\star}}\|_{L^{2}}\leq\epsilon/3\).

Proof.: See Appendix E.5.2 for a detailed proof. 

### Modified Transformer Approximates Piecewise-constant Function

In this subsection, we define modified self-attention and feed-forward layers to construct the modified transformers. We use the modified transformers to approximate the piecewise-constant function.

**Definition E.3** (Modified Transformer Networks).: The modified transformer network \(\overline{\mathcal{T}}_{p}^{r,m,l}\) includes two modifications to the standard transformer network \(\mathcal{T}_{p}^{r,m,l}\):

* Modified attention layer: Replace \(\mathrm{Softmax}\) operator with \(\mathrm{Hardmax}\) operator \(\sigma_{H}(\cdot)\).
* Modified feed-forward layer: Replace \(\mathrm{ReLU}(\cdot)\) with an activation function \(\zeta\in\Psi\). Here, \(\Psi\) denotes the set of all piecewise linear functions with at most three pieces and at least one constant.

We approximate \(\mathcal{F}(\delta)\) with this modified transformer networks \(\overline{\mathcal{T}}_{p}^{r,m,l}\).

**Lemma E.3** (Modified from Proposition 4 of [25]).: For each \(f_{\delta}\in\mathcal{F}(\delta)\), there exists a \(f_{\mathcal{T},c}\in\overline{\mathcal{T}}_{p}^{2,1,1}\) such that \(\left\|f_{\delta}-f_{\mathcal{T},c}\right\|_{L^{2}}=\mathcal{O}(\delta^{d/2})\).

Proof Sketch.: Given \(\delta\), and for any grid \(G\in\mathcal{G}_{\delta}\), we have a grid set \(\mathcal{G}_{\delta}\) and the cube \(\mathcal{S}_{G}\).

Our proof follows two steps:

* For all \(X\in\mathbb{R}^{d\times L}\), we quantize it to a finite set:
* If \(X\in\mathcal{S}_{G}\subset[0,1]^{d\times L}\), we quantize it to the element \(G\in\mathcal{G}_{\delta}\).
* If \(X\notin[0,1]^{d\times L}\), we quantize it to an element out of \(\mathcal{G}_{\delta}\).
* **Mapping.** For any \(G\in\mathcal{G}_{\delta}\), we map it to the desired output \(A_{G}\).

For **Quantization**, we achieve this by a series of modified feed-forward layers. We show this in Appendix E.2.1.

For **Mapping**, we follow two steps:

* For any \(G\neq G^{\prime}\in\mathcal{G}_{\delta}\), we use a "contextual mapping" \(q_{c}(\cdot)\) (defined as Definition E.4). The mapping maps all the elements in \(q_{c}(G)\) and \(q_{c}(G^{\prime})\) to different values. Then, we use a series of modified self-attention layers to achieve "contextual mapping". We show this in Appendix E.2.2.

**Definition E.4** (Contextual Mapping).: Consider a finite set \(\mathcal{G}_{\delta}\in\mathbb{R}^{d\times L}\). A map \(q_{c}:\mathcal{G}_{\delta}\rightarrow\mathbb{R}^{1\times L}\) defines a contextual mapping if the map satisfies the following:

* For any \(G\in\mathcal{G}_{\delta}\), the entries in \(q_{c}(G)\) are all distinct.
* For any \(G\neq G^{\prime}\in\mathcal{G}_{\delta}\), all entries of \(q_{c}(G)\) and \(q_{c}(G^{\prime})\) are distinct.
* For any \(G\in\mathcal{G}_{\delta}\), we use a series of modified feed-forward layers to map \(q_{c}(G)\) to \(A_{G}\). We show this in Appendix E.2.3.

**Remark E.1**.: Our proof differs from (Yun et al., 2020) in one aspect: Although (Yun et al., 2020, Proposition 4) outlines a proof for transformer networks without positional encoding and sketches the proof for networks with it, we provide a detailed proof for the latter to support our proof.

#### e.2.1 Quantization by Modified Feed-forward Layers

We use a series of modified feed-forward layers in \(\overline{\mathcal{T}}_{p}^{r,m,l}\) to quantize an input \(X\in\mathbb{R}^{d\times L}\) to an element \(G\) of the following grid:

\[\{-J,0,\delta,\ldots,1-\delta\}^{d\times L},\]

where \(J>L>0\) is a large number to be determined later. We achieve this via two steps.

* **Step 1: Map the element out of \([0,1)\) to \(-J\).** We use \(e_{i}\) to represent the standard unit vector where the \(i\)-th element is \(1\). For the \(i\)-th row of \(X\), we define the following feed-forward layer to achieve our aim. [leftmargin=*]
* **Definition E.5** (Feed-forward Layer 1).: The vector \(e_{i}\) acts as the weight parameters, and \(\zeta_{1}(\cdot)\) acts as the activation function in the feed-forward layer \[X\to X+e_{i}\zeta_{1}(e_{i}^{\top}X),\ \ \zeta_{1}(t)=\begin{cases}-t-J,&\text{for }t<0\text{ or }t\geq 1,\\ 0,&\text{otherwise}.\end{cases}\] (E.2) We take \(i=1\) as an example to give the specific calculation. Let \(X=(x_{i,j})_{d\times L}\), then we have \[\operatorname{FF}(X) =X+\begin{pmatrix}1\\ 0\\ \vdots\\ 0\end{pmatrix}(\zeta_{1}(x_{1,1})\quad\zeta_{1}(x_{1,2})\quad\cdots\quad\zeta _{1}(x_{1,L}))\] \[=X+\begin{pmatrix}\zeta_{1}(x_{1,1})\quad\zeta_{1}(x_{1,2})\quad \cdots\quad\zeta_{1}(x_{1,L})\\ 0\quad 0\quad\cdots\quad 0\\ \vdots\quad\vdots\quad\vdots\quad\vdots\\ 0\quad 0\quad\cdots\quad 0\end{pmatrix}.\] In the first row of \(X\), the above layer transforms the element that is out of \([0,1)\) to \(-J\). We stack the above layers together for \(i=1,2,\ldots,d\). If the element of \(X\) is out of \([0,1)\), the series of layers maps it to \(J\).
* **Step 2: Map the element in \([0,1)\) to \(\{0,\delta,2\delta,\ldots,1-\delta\}\).** For the \(i\)-th row of \(X\), we take \(k=0,1,\ldots,1/\delta-1\) respectively. We define the following layer. [leftmargin=*]
* **Definition E.6** (Feed-forward Layer 2).: The vector \(e_{i}\) acts as the weight parameters and \(\zeta_{2}(\cdot)\) acts as the activation function in the feed-forward layer \[X\to X+e_{i}\zeta_{2}(e_{i}^{\top}X-k\delta\mathds{1}_{n}^{\top}),\ \ \zeta_{2}(t)=\begin{cases}0,&t<0\text{ or }t\geq\delta,\\ -t,&0\leq t<\delta.\end{cases}\] (E.3) We take \(i=1\) and \(k=1\) as an example. We give the following specific calculation \[\operatorname{FF}(X) =X+\begin{pmatrix}1\\ 0\\ \vdots\\ 0\end{pmatrix}(\zeta_{2}(x_{1,1}-\delta)\quad\zeta_{2}(x_{1,2}-\delta)\quad \cdots\quad\zeta_{2}(x_{1,L}-\delta))\] \[=X+\begin{pmatrix}\zeta_{2}(x_{1,1}-\delta)\quad\zeta_{2}(x_{1,2} -\delta)\quad\cdots\quad\zeta_{2}(x_{1,L}-\delta)\\ 0\quad 0\quad\cdots\quad 0\\ \vdots\quad\vdots\quad\vdots\quad\vdots\\ 0\quad\cdots\quad 0\end{pmatrix}.\]In the first row of \(X\), the above layer transforms the element in \([\delta,2\delta]\) to \(\delta\).

We stack the above layers together for \(i=1,2,\ldots,d\) and \(k=0,1,\ldots,1/\delta-1\). If the element of \(X\) is in \([k\delta,(k+1)\delta]\), the series layers maps it to \(k\delta\).

Combining the above two parts, we achieve our goal with \(d/\delta+d\) feed-forward layers. We denote the \(d/\delta+d\) series layers as \(f_{\mathcal{T},c1}\).

#### e.2.2 Contextual Mapping by Modified Self-attention Layers

In our attention layers, we use the following positional encoding \(E\in\mathbb{R}^{d\times L}\)

\[E=\begin{pmatrix}0&1&2&\cdots&L-1\\ 0&1&2&\cdots&L-1\\ \vdots&\vdots&\vdots&&\vdots\\ 0&1&2&\cdots&L-1\end{pmatrix}.\] (E.4)

According to Appendix E.2.1, the output of \(f_{\mathcal{T},c1}\) is in the grid \(\{-J,0,\delta,\ldots,1-\delta\}^{d\times L}\). For any \(X\) in this grid, the first column of \(X+E\) is in

\[\{-J,0,\delta,\ldots,1-\delta\}^{d},\]

and the second column is in

\[\{-J+1,1,1+\delta,\ldots,2-\delta\}^{d}.\]

The results are similar in the other columns.

For \(i=0,1,\ldots,L-1\), we use the following notation:

\[[i:\delta:i+1-\delta]_{J}\coloneqq\{i-J,i,i+\delta,\ldots,i+1-\delta\}.\]

Then, we define the grid \(\mathcal{G}_{\delta}^{+}\) as the following.

**Definition E.7** (Grid \(\mathcal{G}_{\delta}^{+}\)).: We add \(E\) to all the grid points in \(\mathcal{G}_{\delta}\) to generate the modified grid \(\mathcal{G}_{\delta}^{+}\), defined as follows:

\[\mathcal{G}_{\delta}^{+}\coloneqq[0:\delta:1-\delta]_{J}^{d}\times[1:\delta:2- \delta]_{J}^{d}\times\cdots\times[L-1:\delta:L-\delta]_{J}^{d}.\]

Next, we show that the modified attention layer computes contextual mapping (Definition E.4) for \(\mathcal{G}_{\delta}^{+}\). For \(i=1,2,\ldots,L-1\), we use the following notation:

\[[i:\delta:i+1-\delta]\coloneqq\{i,i+\delta,i+2\delta,\ldots,i+1-\delta\}.\]

**Lemma E.4** (Modified from Lemma 6 of (Yun et al., 2020)).: We consider the following subset of \(\mathcal{G}_{\delta}^{+}\):

\[\widetilde{\mathcal{G}}_{\delta}:=\underbrace{[0:\delta:1-\delta]^{d}\times[1 :\delta:2-\delta]^{d}\times\cdots\times[L-1:\delta:L-\delta]^{d}}_{L}.\]

Assume that \(L\geq 2\) and \(\delta^{-1}\geq 2\). Then, there exist a function \(f_{\mathcal{T},c2}:\mathbb{R}^{d\times L}\rightarrow\mathbb{R}^{d\times L}\) composed of \(\delta^{-d}+1\) modified attention layers (Definition E.3), a vector \(u\in\mathbb{R}^{d}\), and two constants \(t_{l},t_{r}\in\mathbb{R}\) (\(0<t_{l}<t_{r}\)), such that \(q_{c}(G)\coloneqq u^{\top}f_{\mathcal{T},c2}(G),G\in\mathcal{G}_{\delta}^{+}\) satisfies the following properties:

1. For any \(G\in\widetilde{\mathcal{G}}_{\delta}\), all the entries of \(q_{c}(G)\) are distinct.
2. For any different \(G,G^{\prime}\!\in\!\widetilde{\mathcal{G}}_{\delta}\), all the entries of \(q_{c}(G)\), \(q_{c}(G^{\prime})\) are distinct.
3. For any \(G\in\widetilde{\mathcal{G}}_{\delta}\), all the entries of \(q_{c}(G)\) are in \([t_{l},t_{r}]\).
4. For any \(G\in\mathcal{G}_{\delta}^{+}\setminus\widetilde{\mathcal{G}}_{\delta}\), all the entries of \(q_{c}(G)\) are outside \([t_{l},t_{r}]\).

Proof.: See Appendix E.5.3 for a detailed proof. 

**Remark E.2**.: Our proof differs from (Yun et al., 2020) in one aspect: The original (Yun et al., 2020, Lemma 6) does not include positional encoding (E.4). Although Yun et al. (2020) sketches the proof for networks with (E.4) in the attention layer input, we detail the proof.

#### e.2.3 Map to the Desired Output by Modified Feed-forward Layers

Next, we show that a series of feed-forward layers map the output of modified attention layers \(f_{\mathcal{T},c2}\) to the desired output of function \(f_{\delta^{*}}\).

**Lemma E.5** (Lemma 7 of (Yun et al., 2020)).: There exists a function \(f_{\mathcal{T},c3}:\mathbb{R}^{d\times L}\to\mathbb{R}^{d\times L}\) composed of \(\mathcal{O}(L(1/\delta)^{dL}/L!)\) modified feed-forward layers, such that

\[f_{\mathcal{T},c3}\circ f_{\mathcal{T},c2}(G)=\begin{cases}A_{G}&\text{if }G \in\widetilde{\mathcal{G}}_{\delta},\\ \mathbf{0}_{d\times L}&\text{if }G\in\mathcal{G}_{\delta}^{+}\setminus \widetilde{\mathcal{G}}_{\delta}.\end{cases}\]

Proof.: See Appendix E.5.4 for a detailed proof. 

In conclusion, we have the following lemma for the required number of layers in the modified transformer.

**Lemma E.6** (Total Number of Layers).: From the proof of Lemma E.3, if we want to achieve a approximation error \(\mathcal{O}(\delta^{d/2})\) by the modified transformer, we need \(\mathcal{O}(\delta^{-1})\) modified feed-forward layers in \(f_{\mathcal{T},c1}\), \(\mathcal{O}(\delta^{-d})\) modified self-attention layers in \(f_{\mathcal{T},c2}\), and \(\mathcal{O}(\delta^{-dL})\) modified feed-forward layers in \(f_{\mathcal{T},c3}\).

Proof.: By the proof of Lemma E.3, we complete the proof. 

### Standard Transformers Approximate Modified Transformers

In this subsection, we show that standard neural network layers are able to approximate the modified self-attention layers and the modified feed-forward layers (Definition E.3). We have the following Lemma E.7.

**Lemma E.7** (Lemma 9 of (Yun et al., 2020)).: For each \(f_{\mathcal{T},c}\in\overline{\mathcal{T}}_{p}^{2,1,1}\) and any \(\epsilon>0\), there exists \(f_{\mathcal{T}}\in\mathcal{T}_{p}^{2,1,4}\) such that \(\left\lVert f_{\mathcal{T}}-f_{\mathcal{T},c}\right\rVert_{L^{2}}\leq\epsilon/3\).

Proof.: See Appendix E.5.5 for a detailed proof. 

### All Together: Standard Transformers Approximate Compact-supported Continuous Functions

We summarize the results of Lemmas E.2, E.3 and E.7. Then we prove Lemma E.1.

Furthermore, to achieve the \(\epsilon\) approximation error in Lemma E.1, we take \(\delta=\mathcal{O}(\epsilon^{2/d})\) in Lemma E.3.

### Supplementary Proofs

We first present two preliminary concepts: selective shift operation and bijective column ID mapping in Appendix E.5.1.

Then we show

* Proof of Lemma E.2 in Appendix E.5.2
* Proof of Lemma E.4 in Appendix E.5.3
* Proof of Lemma E.5 in Appendix E.5.4
* Proof of Lemma E.7 in Appendix E.5.5

#### e.5.1 Preliminaries

Here, we give the definition of two preliminary concepts: selective shift operation and bijective column ID mapping.

Selective Shift Operation.This operation refers to shifting certain entries of the input selectively. To achieve this, we consider the following function \(\xi(\cdot;\cdot):\mathbb{R}^{d\times L}\rightarrow\mathbb{R}^{d\times L}\)

\[\xi(X;b_{Q})=e_{1}u^{\top}X\sigma_{H}\left[(u^{\top}X)^{\top}(u^{\top}X-b_{Q} \mathds{1}_{n}^{\top})\right],\] (E.5)

where \(X\in\mathbb{R}^{d\times L}\), \(e_{1}=(1,0,0,\cdots,0)^{\top}\in\mathbb{R}^{d}\), and \(b_{Q}\in\mathbb{R}\). \(u\in\mathbb{R}^{d}\) is a vector to be determined.

To see the output, we consider the \(j\)-th column of \(u^{\top}X\sigma_{H}\left[(u^{\top}X)^{\top}(u^{\top}X-b_{Q}\mathds{1}_{n}^{ \top})\right]\):

* If \(u^{\top}X_{:,j}>b_{Q}\), it calculates \(\operatorname{argmax}\) of \(u^{\top}X\);
* If \(u^{\top}X_{:,j}<b_{Q}\), it calculates \(\operatorname{argmin}\) of \(u^{\top}X\).

All rows of \(\xi(X;b_{Q})\) except the first row are zero. We consider the \(j\)-th entry of the first row in \(\xi(X;b_{Q})\), which is denoted as \(\xi(X;b_{Q})_{1,j}\). Then for all \(j\in[L]\), we have

\[\xi(X;b_{Q})_{1,j}=u^{\top}X\sigma_{H}\left[(u^{\top}X)^{\top}(u^{\top}X_{:,j} -b_{Q})\right]=\begin{cases}\max_{k}u^{\top}X_{:,k}&\text{ if }u^{\top}X_{:,j}>b_{Q},\\ \min_{k}u^{\top}X_{:,k}&\text{ if }u^{\top}X_{:,j}<b_{Q}.\end{cases}\]

From this observation, we define a function parametrized by \(b_{Q}\) and \(b^{\prime}_{Q}\) (with \(b_{Q}<b^{\prime}_{Q}\))

\[\xi(X;b_{Q},b^{\prime}_{Q}):=\xi(X;b_{Q})-\xi(X;b^{\prime}_{Q}).\] (E.6)

Then we have

\[\xi(X;b_{Q},b^{\prime}_{Q})_{1,j}=\begin{cases}\max_{k}u^{\top}X_{:,k}-\min_{ k}u^{\top}X_{:,k},&\text{ if }b_{Q}<u^{\top}X_{:,j}<b^{\prime}_{Q},\\ 0,&\text{ others}.\end{cases}\]

We define an attention layer of the form \(X\to X+\xi(X;b_{Q},b^{\prime}_{Q})\). For any column \(X_{:,j}\), if \(b_{Q}<u^{\top}X_{:,j}<b^{\prime}_{Q}\), its first coordinate \(X_{1,j}\) is shifted up by \(\max_{k}u^{\top}X_{:,k}-\min_{k}u^{\top}X_{:,k}\), while all the other coordinates stay untouched. We call this the selective shift operation because we can choose \(b_{Q}\) and \(b^{\prime}_{Q}\) to shift certain entries of the input selectively.

Bijective Column ID Mapping.We consider the input \(G\in\mathcal{G}_{\delta}^{+}\) (Definition E.7). We use

\[J=L+3L\delta^{-dL},\text{ and }u=(1,\delta^{-1},\delta^{-2},\ldots,\delta^{-d+ 1}).\] (E.7)

For any \(j\in[L]\), we have the following two conclusions:* If \(G_{i,j}\geq 0\) for all \(i\in[d]\), i.e., \(G_{:,j}\in[j-1:\delta:j-\delta]^{d}\), then we have \[u^{\top}G_{:,j}\in\left[\delta_{j}:\delta:\delta_{j}+\delta^{-d+1}-\delta\right],\text{ where }\delta_{j}=(j-1)\cdot\left(\frac{\delta-\delta^{-d+1}}{\delta-1 }\right).\] (E.8) The mapping \(G_{:,j}\to u^{\top}G_{:,j}\) maps the elements in \([j-1:\delta:j-\delta]^{d}\) to \(\left[\delta_{j}:\delta:\delta_{j}+\delta^{-d+1}-\delta\right]\). This is a bijection.
* If there exists \(i\in[d]\) such that \(G_{i,j}=-J+j\), then \[u^{\top}G_{:,j}\leq-3L\delta^{-dL}+(j-1)\cdot\left(\frac{\delta^{-d+1}-\delta} {1-\delta}\right)+\delta^{-d+1}<0.\] (E.9) We say that \(u^{\top}G_{:,j}\) gives the "column ID" for each possible value of \(G_{:,j}\in[j-1:\delta:j-\delta]^{d}\).

**Remark E.3** (Illustration of Bijection Property).: For the bijection property, we give the following illustration. Let \(G_{:,j}=(g_{1j},g_{2j},\cdots,g_{dj})^{\top}\) and \(\overline{G}_{:,j}=(\overline{g}_{1j},\overline{g}_{2j},\cdots,\overline{g}_{ dj})^{\top}\). If \(u^{\top}G_{:,j}=u^{\top}\overline{G}_{:,j}\) and \(G_{:,j}\neq\overline{G}_{:,j}\), we deduce

\[(g_{1j}-\overline{g}_{1j})+\delta^{-1}(g_{2j}-\overline{g}_{2j})+\cdots+ \delta^{-d+1}(g_{dj}-\overline{g}_{dj})=0.\] (E.10)

Because \(G_{:,j}\neq\overline{G}_{:,j}\), then there exists a \(k\) (\(k<d\)), such that \(g_{kj}\neq\overline{g}_{kj}\) and \(g_{ij}=\overline{g}_{ij}(i>k)\). We have

\[\left|\delta^{-k+1}(g_{kj}-\overline{g}_{kj})\right|\geq\delta^{-k+2}.\]

However,

\[\left|(g_{1j}-\overline{g}_{1j})+\cdots+\delta^{-k+2}(g_{k-1,j}- \overline{g}_{k-1,j})\right|\] \[\leq\left|g_{1j}-\overline{g}_{1j}\right|+\cdots+\left|\delta^{-k +2}(g_{k-1,j}-\overline{g}_{k-1,j})\right|\] \[\leq(1-\delta)+\cdots+\delta^{-k+2}(1-\delta)\] \[<\delta^{-k+2}.\]

This contradicts with (E.10). Thus we prove the property of bijection.

#### e.5.2 Proof of Lemma e.2

Proof of Lemma e.2.: We restate the proof from [24] for completeness.

By the nature of the compact-supported continuous function, \(f\) is uniformly continuous.

Because \(\left\lVert\cdot\right\rVert_{\infty}\) is equivalent to \(\left\lVert\cdot\right\rVert_{F}\) when the number of entries are finite, we have the following by the definition of uniform continuity.

For any \(\epsilon/3>0\), there exists a \(\delta^{*}>0\), such that for any \(X,Y\in\mathbb{R}^{d\times L}\), and \(\left\lVert X-Y\right\rVert_{\infty}<\delta^{*}\), we have \(\left\lVert f(X)-f(Y)\right\rVert_{F}<\epsilon/3\).

Then we perform the following steps following Definitions E.1 and E.2:

* We create a grid \(\mathcal{G}_{\delta^{*}}\) by choosing grid width \(\delta^{*}\). We also create cube \(\mathcal{S}_{G}\) with respect to \(G\in\mathcal{G}_{\delta^{*}}\).
* For any grid point \(G\in\mathcal{G}_{\delta^{*}}\), we define \(C_{G}\in\mathcal{S}_{G}\) as the center point of the cube \(\mathcal{S}_{G}\).
* We define a piecewise-constant function \(f_{\delta^{*}}(X)=\sum_{L\in\mathcal{G}_{\delta^{*}}}f(C_{G})\mathds{1}\{X\in \mathcal{S}_{G}\}\).

For any \(X\in\mathcal{S}_{G}\), we have \(\left\lVert X-C_{G}\right\rVert_{\infty}<\delta^{*}\). According to the uniform continuity, we drive

\[\left\lVert f(X)-f_{\delta^{*}}(X)\right\rVert_{F}=\left\lVert f(X)-f(C_{G}) \right\rVert_{F}<\epsilon/3.\]

This implies that \(\left\lVert f-f_{\delta^{*}}\right\rVert_{L^{2}}<\epsilon/3\) and completes the proof. 

#### e.5.3 Proof of Lemma e.4

We give the proof of Lemma E.4 by constructing the network to satisfy the requirements.

Proof of Lemma e.4.: Recall the selective shift operation in Appendix E.5.1. The overall idea of the construction includes two steps:

* **Step 1:** For each \(j\in[L]\), we stack \(\delta^{-d}\) attention layers. For \(g\in[\delta_{j}:\delta:\delta_{j}+\delta^{-d+1}-\delta]\) (E.8) in the increasing order, we use the attention layer as \[\delta^{-d}\xi(\cdot;g-\delta/2,g+\delta/2).\] (E.11) The total number of layers is \(L\delta^{-d}\). These layers cast \(G\in\widetilde{\mathcal{G}}_{\delta}\) to \(L\) different entries required by Property 1 of Lemma E.4.
* **Step 2:** We add an extra single-head attention layer with the following attention part \[L\delta^{-(L+1)d-1}\xi(\cdot;0).\] (E.12) This layer achieves a global shifting and casts different \(G\in\widetilde{\mathcal{G}}_{\delta}\) to unique elements required by the Property 2 of Lemma E.4.

The two operations map \(\widetilde{\mathcal{G}}_{\delta}\) and \(\mathcal{G}_{\delta}^{+}\setminus\widetilde{\mathcal{G}}_{\delta}\) to different sets, as required by Property 3 and Property 4 of Lemma E.4. The bounds \(t_{l}\) and \(t_{r}\) are calculated then.

Then, we give a detailed proof by showing the impact of the two steps and verifying the four properties of Lemma E.4. We achieve this by making a category division of \(\mathcal{G}_{\delta}^{+}\):

* **Category 1:**\(G\in\widetilde{\mathcal{G}}_{\delta}\), all entries in the point \(G\) are between \(0\) and \(L-\delta\).
* **Category 2:**\(G\in\mathcal{G}_{\delta}^{+}\setminus\widetilde{\mathcal{G}}_{\delta}\), the point \(G\) has at least one entry that equals to \(-J\).

Let \(u=(1,\delta^{-1},\delta^{-2},\ldots,\delta^{-d+1})\). Recall that \(\delta_{j}=(j-1)(\delta-\delta^{-d+1})/(\delta-1)\) for any \(j\in[L]\) in (E.8).

[MISSING_PAGE_FAIL:33]

We have \[\widetilde{g}_{2} \coloneqq u^{\top}\widetilde{G}_{:,2}\] \[=g_{2}+\delta^{-d}(g_{1}-g_{2})+\delta^{-2d}(g_{L}-g_{1}).\] Then we deduce \(\widetilde{g}_{1}<\widetilde{g}_{2}\), because \[g_{1}+\delta^{-d}(g_{L}-g_{1})<g_{2}+\delta^{-d}(g_{1}-g_{2})+ \delta^{-2d}(g_{L}-g_{1})\] \[\iff(\delta^{-d}-1)(g_{2}-g_{1})<\delta^{-d}(\delta^{-d}-1)(g_{L} -g_{1}).\qquad\qquad(\text{By }\delta^{-d}>1\text{ and }g_{L}>g_{2})\] Thus, after updating, we have \[\max u^{\top}\left(\widetilde{G}_{:,1}\quad\widetilde{G}_{:,2} \quad\cdots\quad G_{:,L}\right)=\max\{\widetilde{g}_{1},\widetilde{g}_{2}, \ldots,g_{L}\}=\widetilde{g}_{2},\] and the new minimum is \(g_{3}\).
* **Repeating the Process.** By repeating this process, we show that the \(j\)-th shift operation shifts \(G_{1,j}\) by \(\delta^{-d}(\widetilde{g}_{j-1}-g_{j})\). Then we have \[\widetilde{g}_{j} \coloneqq u^{\top}\widetilde{G}_{:,j}\] \[=g_{j}+\sum_{k=1}^{j-1}\delta^{-kd}(g_{j-k}-g_{j-k+1})+\delta^{- jd}(g_{L}-g_{1}).\] We deduce \(\widetilde{g}_{j-1}<\widetilde{g}_{j}\) holds for all \(2\leq j\leq L\), because \[\widetilde{g}_{j-1}<\widetilde{g}_{j}\] \[\iff g_{j-1}+\sum_{k=2}^{j-1}\delta^{-kd+d}(g_{j-k}-g_{j-k+1})+\delta^{- (j-1)d}(g_{L}-g_{1})\] \[<g_{j}+\sum_{k=1}^{j-1}\delta^{-kd}(g_{j-k}-g_{j-k+1})+\delta^{- jd}(g_{L}-g_{1})\] \[\iff \sum_{k=1}^{j-1}\delta^{-kd+d}(\delta^{-d}-1)(g_{j-k+1}-g_{j-k})< \delta^{-(j-1)d}(\delta^{-d}-1)(g_{L}-g_{1}),\] where the last inequality holds because \[\sum_{k=1}^{j-1}\delta^{-kd+d}(g_{j-k+1}-g_{j-k})\] \[<\delta^{-(j-1)d}\sum_{k=1}^{j-1}(g_{j-k+1}-g_{j-k})\] \[<\delta^{-(j-1)d}(g_{L}-g_{1}).\] Therefore, after the \(j\)-th selective shift operation, \(\widetilde{g}_{j}\) is the new maximum among \(\{\widetilde{g}_{1},\ldots,\widetilde{g}_{j},g_{j+1},\ldots,g_{L}\}\) and \(g_{j+1}\) is the new minimum.
* **After \(L\) Shift Operations.** After the whole \(L\) shift operations, the input \(G\) is mapped to a new point \(\widetilde{G}\), where \(u^{\top}\widetilde{G}=(\widetilde{g}_{1}\quad\widetilde{g}_{2}\quad\cdots\quad \widetilde{g}_{L})\) and \(\widetilde{g}_{1}<\widetilde{g}_{2}<\cdots<\widetilde{g}_{L}\). For the lower and upper bound of \(\widetilde{g}_{L}\), we have the following lemma. [Lemma 10 of [20]] \(\widetilde{g}_{L}=u^{\top}\widetilde{G}_{:,L}\) satisfies the following bounds: \[\delta^{-(L-1)d+1}(\delta^{-d}-1)\leq\widetilde{g}_{L}\leq L\delta^{-(L+1)d}.\]Also, the mapping from \((g_{1}\quad g_{2}\quad\cdots\quad g_{L})\) to \(\widetilde{g}_{L}\) is one-to-one mapping.
* **Global Shifting by the Last Layer.** We note that after the above \(L\) shift operations, there is another attention layer with attention part \(L\delta^{-(L+1)d-1}\xi(\cdot;0)\). Since \(0<\widetilde{g}_{1}<\cdots<\widetilde{g}_{L}\), it adds the following to each entry in the first row of \(\widetilde{G}\): \[L\delta^{-(L+1)d-1}\max_{k}u^{\top}\widetilde{G}_{\cdot,k}=L\delta^{-(L+1)d-1 }\widetilde{g}_{L}.\] The output of this layer is defined to be the function \(f_{\mathcal{T},e2}(G)\).

In summary, for any \(G\in\widetilde{\mathcal{G}}_{\delta}\), \(i\in[d]\), and \(j\in[L]\), we have

\[f_{\mathcal{T},e2}(G)_{i,j}=\begin{cases}G_{1,j}+\delta_{j}^{+}&\text{ if }i=1,\\ G_{i,j}&\text{ if }2\leq i\leq d,\end{cases}\]

where \(\delta_{j}^{+}=\sum_{k=1}^{j-1}\delta^{-kd}(g_{j-k}-g_{j-k+1})+\delta^{-jd}(g _{L}-g_{1})+L\delta^{-(L+1)d-1}\widetilde{g}_{L}\).

For any \(G\in\widetilde{\mathcal{G}}_{\delta}\) and \(j\in[L]\),

\[u^{\top}f_{\mathcal{T},e2}(G)_{\cdot,j}=\widetilde{g}_{j}+L\delta^{-(L+1)d-1} \widetilde{g}_{L}.\]

Next, we check the Property 1, Property 2 and Property 3 of Lemma E.4.

* **Checking Property 1 of Lemma E.4.** Given any \(G\in\widetilde{\mathcal{G}}_{\delta}\), we already prove that \[\widetilde{g}_{1}<\widetilde{g}_{2}<\cdots<\widetilde{g}_{L},\] All of them are distinct.
* **Checking Property 2 of Lemma E.4.** Note that the upper bound on \(\widetilde{g}_{L}\) from Lemma E.8 also holds for other \(\widetilde{g}_{j}\) (\(j\in[L-1]\)). For all \(j\in[L]\), we have \[L\delta^{-(L+1)d-1}\widetilde{g}_{L}\leq u^{\top}f_{\mathcal{T},e2}(G)_{\cdot,j}<L\delta^{-(L+1)d-1}\widetilde{g}_{L}+L\delta^{-(L+1)d}.\] By Lemma E.8, two different \(G,G^{\prime}\in\widetilde{\mathcal{G}}_{\delta}\) are mapped to different \(\widetilde{g}_{L}\) and \(\widetilde{g}_{L}^{\prime}\), and they differ at least by \(\delta\). This means that the following two intervals are guaranteed to be disjoint: \[[L\delta^{-(L+1)d-1}\widetilde{g}_{L},L\delta^{-(L+1)d-1} \widetilde{g}_{L}+L\delta^{-(L+1)d}),\] \[[L\delta^{-(L+1)d-1}\widetilde{g}_{L}^{\prime},L\delta^{-(L+1)d-1 }\widetilde{g}_{L}^{\prime}+L\delta^{-(L+1)d}).\] Thus, the entries of \(u^{\top}f_{\mathcal{T},e2}(G)\) and \(u^{\top}f_{\mathcal{T},e2}(G^{\prime})\) are all distinct. Now, we finish showing that the mapping \(f_{\mathcal{T},e2}(\cdot)\) uses \((1/\delta)^{d}+1\) attention layers to implement a contextual mapping on \(\widetilde{\mathcal{G}}_{\delta}\).
* **Checking Property 3 of Lemma E.4.** Given Lemma E.8 and \(u^{\top}f_{\mathcal{T},e2}(G)_{\cdot,j}\in[L\delta^{-(L+1)d-1}\widetilde{g}_{L },L\delta^{-(L+1)d-1}\widetilde{g}_{L}+L\delta^{-(L+1)d})\), for any \(G\in\widetilde{\mathcal{G}}_{\delta}\), we have \[u^{\top}f_{\mathcal{T},e2}(G)_{\cdot,j}\geq L\delta^{-2(L+1)d}( \delta^{-d}-1),\] \[u^{\top}f_{\mathcal{T},e2}(G)_{\cdot,j}<L^{2}\delta^{-2(L+1)d-1}+ L\delta^{-(L+1)d}.\] This proves that all \(u^{\top}f_{\mathcal{T},e2}(L)_{\cdot,j}\) are between \(t_{l}\) and \(t_{r}\), where \[t_{l}=L\delta^{-2(L+1)d}(\delta^{-d}-1),\]\[t_{r}=L^{2}\delta^{-2(L+1)d-1}+L\delta^{-(L+1)d}.\]

Category 2.Now we check the Property 4 of Lemma E.4. For the input points \(G\in\mathcal{G}_{\delta}^{+}\setminus\widetilde{\mathcal{G}}_{\delta}\), note that the point \(G\) has at least one entry that equals to \(-J+k,k\in[L-1]\). Let \(g_{j}\coloneqq u^{\top}G_{\cdot,j}\). Recall that whenever a column \(G_{\cdot,j}\) has an entry that equals to \(-J+k,k\in[L-1]\), we have \(g_{j}<0\). Without loss of generality, assume that \(g_{1}<0\).

Because the selective shift operation is applied to each element of \([0:\delta:\delta_{L}+\delta^{-d+1}-\delta]\) and is not applied to negative values, thus we have \(\min_{k}u^{\top}G_{\cdot,k}=g_{1}<0\). \(g_{1}\) never gets shifted upwards and remains the minimum for the whole time.

* **All \(g_{j}\)'s are Negative.** When all \(g_{j}\)'s are negative, selective shift operation never shifts the input \(G\). Thus \(\widetilde{G}=G\). Recall that \(u^{\top}\widetilde{G}_{\cdot,j}<0\) for all \(j\in[L]\). The last layer with attention part \(L\delta^{-(L+1)d-1}\xi(\cdot;0)\) adds \(L\delta^{-(L+1)d-1}\min_{k}u^{\top}\widetilde{G}_{\cdot,k}<0\) to each entry in the first row of \(\widetilde{G}\). This makes \(\widetilde{G}\) remain negative. Therefore, \(f_{\mathcal{T},c2}(G)\) satisfies \(u^{\top}f_{\mathcal{T},c2}(G)_{\cdot,j}<0<t_{l}\) for all \(j\in[L]\).
* **Not All \(g_{j}\)'s are Negative.** Now consider the case where at least one \(g_{j}\) is positive. Suppose that there are \(k\) positive elements and they satisfy \(g_{i_{1}}<g_{i_{2}}<\cdots<g_{i_{k}}\). Thus selective shift operation does not affect \(g_{i}\), where \(i\in[L]\setminus\{i_{1},\ldots,i_{k}\}\). It shifts \(g_{i_{1}}\) by \[\delta^{-d}(\max_{k}u^{\top}G_{\cdot,k}-\min_{k}u^{\top}G_{\cdot,k})\] \[\geq\delta^{-d}(2L\delta^{-dL}-(L-1)\frac{\delta^{-d+1}-\delta}{1- \delta}-\delta^{-d+1}+(i_{k}-1)\frac{\delta^{-d+1}-\delta}{1-\delta})\] (By (E.9)) \[=\delta^{-d}(3L\delta^{-dL}-\delta^{-d+1}-(L-i_{k})\frac{\delta^{ -d+1}-\delta}{1-\delta})\] \[\geq\delta^{-d}\cdot 2L\delta^{-dL} (\text{By }\delta^{-1}\geq 2)\] \[=2L\delta^{-(L+1)d}.\] The next shift operations shift \(g_{i_{2}},\ldots,g_{i_{k}}\) by an even larger amount. Therefore, at the end of the first \(L(1/\delta)^{d}\) layers, we have \(L\delta^{-(L+1)d}\leq\widetilde{g}_{i_{1}}\leq\cdots\leq\widetilde{g}_{i_{k}}\), and \(\widetilde{g}_{j}<0\) for all \(j\in[L]\setminus\{i_{1},\ldots,i_{k}\}\). Then, we shift \(G\) by the last layer. The last layer with attention part \(L\delta^{-(L+1)d-1}\xi(\cdot;0)\) acts differently for negative and positive \(\widetilde{g}_{j}\)'s. (i). For negative \(\widetilde{g}_{j}\)'s, it adds the following to \(\widetilde{g}_{j},j\in[L]\setminus\{i_{1},\ldots,i_{k}\}\): \[L\delta^{-(L+1)d-1}\min_{k}u^{\top}\widetilde{G}_{\cdot,k}=L\delta^{-(L+1)d-1}g _{1}<0.\] This term pushes them further to the negative side. (ii). For positive \(\widetilde{g}_{i}\)'s, it adds \[L\delta^{-(L+1)d-1}\max_{k}u^{\top}\widetilde{G}_{k}=L\delta^{-(L+1)d-1} \widetilde{g}_{i_{k}}\geq 2L^{2}\delta^{-2(L+1)d-1}.\] Thus they are all greater than or equal to \(2L^{2}\delta^{-2(L+1)d+1}\). Note that \[2L^{2}\delta^{-2(L+1)d-1}>t_{r},\text{ where }t_{r}=L^{2}\delta^{-2(L+1)d-1}+L \delta^{-(L+1)d}.\] Then the final output \(f_{\mathcal{T},c2}(G)\) satisfies \(u^{\top}f_{\mathcal{T},c2}(G)_{\cdot,j}\notin[t_{l},t_{r}]\), for all \(j\in[L]\). This completes the verification of Property 4 of Lemma E.4.

In conclusion, we need \(\mathcal{O}(L\delta^{-d})\) layers of modified self-attention layer to obtain our approximation. This completes the proof.

#### e.5.4 Proof of Lemma e.5

Proof of Lemma e.5.: We restate the proof from [256] for completeness.

Note that \(|\mathcal{G}_{\delta}^{+}|=(1/\delta+1)^{dL}<\infty\), so the output of \(f_{\mathcal{T},c2}(\mathcal{G}_{\delta}^{+})\) has finite number of distinct real values. Let \(M\) be the upper bound of all these possible values. By the construction of \(f_{\mathcal{T},c2}\), \(M>0\).

Construct the Layers:\(f_{\mathcal{T},c3}(f_{\mathcal{T},c2}(G))=\mathbf{0}_{d\times L}\) if \(G\in\mathcal{G}_{\delta}^{+}\setminus\widetilde{\mathcal{G}}_{\delta}\).According to Lemma E.4, for all \(j\in[L]\), we have \(u^{\top}f_{\mathcal{T},c2}(G)_{\cdot,j}\in[t_{l},t_{r}]\) if \(G\in\widetilde{\mathcal{G}}_{\delta}\), and \(u^{\top}f_{\mathcal{T},c2}(G)_{\cdot,j}\notin[t_{l},t_{r}]\) if \(G\in\mathcal{G}_{\delta}^{+}\setminus\widetilde{\mathcal{G}}_{\delta}\). Due to this property, we add the following feed-forward layer.

**Definition E.8** (Feed-forward Layer 3).: The vectors \(u\) and \(\mathds{1}_{L}\) act as the weight parameters, and \(\zeta_{3}(\cdot)\) acts as the activation function in the feed-forward layer.

\[X\to X-(M+1)\mathds{1}_{L}\zeta_{3}(u^{\top}X),\ \ \zeta_{3}(t)=\begin{cases}0& \text{if }t\in[t_{l},t_{r}]\\ 1&\text{if }t\notin[t_{l},t_{r}].\end{cases}\] (E.13)

* **Case for \(G\in\mathcal{G}_{\delta}^{+}\setminus\widetilde{\mathcal{G}}_{\delta}\).** We have \(\zeta_{3}(u^{\top}f_{\mathcal{T},c2}(G))=\mathds{1}_{L}^{\top}\). Thus, all the entries of the input are shifted by \(-M-1\) and become strictly negative.
* **Case for \(G\in\widetilde{\mathcal{G}}_{\delta}\).** We have \(\zeta_{3}(u^{\top}f_{\mathcal{T},c2}(G))=\mathbf{0}_{L}^{\top}\), so the output stays the same as the \(f_{\mathcal{T},c2}(G)\).

With the input \(f_{\mathcal{T},c2}(G)\), if \(G\in\widetilde{\mathcal{G}}_{\delta}\), then \(\zeta_{3}(u^{\top}f_{\mathcal{T},c2}(G))=\mathbf{0}_{L}^{\top}\). Thus, the output stays the same as the input. If \(G\in\mathcal{G}_{\delta}^{+}\setminus\widetilde{\mathcal{G}}_{\delta}\), then \(\zeta_{3}(u^{\top}f_{\mathcal{T},c2}(G))=\mathds{1}_{L}^{\top}\). Thus, all the entries of the input are shifted by \(-M-1\) and become strictly negative.

Next, we map those negative entries to zero. For \(i=1,2,\cdots,d\), we add the following layer:

**Definition E.9** (Feed-forward Layer 4).: The vectors \(u\) and \(e_{i}\) act as the weight parameters and \(\zeta_{4}(\cdot)\) acts as the activation function in the feed-forward layer.

\[X\to X+e_{i}\zeta_{4}((e_{i})^{\top}X),\ \ \zeta_{4}(t)=\begin{cases}-t& \text{if }t<0\\ 0&\text{if }t\geq 0.\end{cases}\] (E.14)

After these \(d\) layers, the output for \(G\in\mathcal{G}_{\delta}^{+}\setminus\widetilde{\mathcal{G}}_{\delta}\) is a zero matrix, while the output for \(G\in\widetilde{\mathcal{G}}_{\delta}\) remains \(f_{\mathcal{T},c2}(G)\).

Construct the Layers:\(f_{\mathcal{T},c3}(f_{\mathcal{T},c2}(G))=A_{G}\) if \(G\in\widetilde{\mathcal{G}}_{\delta}\).Each different \(G\) is mapped to \(L\) unique numbers \(u^{\top}f_{\mathcal{T},c2}(G)\), which are at least \(\delta\) apart from each other. We map each unique number to the corresponding output column as follows. We choose one \(\widetilde{G}\in\widetilde{\mathcal{G}}_{\delta}\). For each \(u^{\top}f_{\mathcal{T},c2}(\widetilde{G})_{\cdot,j}\), \(j\in[L]\), we add the following feed-forward layer.

**Definition E.10** (Feed-forward Layer 5).: The vectors \(u\) and \(e_{i}\) act as the weight parameters, and \(\zeta_{4}(\cdot)\) acts as the activation function in the feed-forward layer.

\[X\rightarrow X+\left((A_{\widetilde{G}})_{\cdot,j}-f_{\mathcal{T},c2}( \widetilde{G})_{\cdot,j}\right)\zeta_{5}(u^{\top}X-u^{\top}f_{\mathcal{T},c2} (\widetilde{G})_{\cdot,j}\mathds{1}_{L}^{\top}),\] (E.15) \[\zeta_{5}(t)=\begin{cases}1&-\delta/2\leq t<\delta/2,\\ 0&\text{others}.\end{cases}\] (E.16)

* **Case for \(G\in\mathcal{G}_{\delta}^{+}\setminus\widetilde{\mathcal{G}}_{\delta}\).** Recall that the input \(X\) of this layer is \(f_{\mathcal{T},c2}(G)\). If \(X\) is a zero matrix, which is the case for \(G\in\mathcal{G}_{\delta}^{+}\setminus\widetilde{\mathcal{G}}_{\delta}\), we have \(u^{\top}X=\mathbf{0}_{L}^{\top}\). Then \(u^{\top}X-u^{\top}f_{\mathcal{T},c2}(\widetilde{G})_{\cdot,j}\mathds{1}_{L} ^{\top}<-t_{l}\mathds{1}_{L}\). Since \(t_{l}>\delta/2\), the output remains the same as \(X\).

* **Case for \(G\in\widetilde{\mathcal{G}}_{\delta}\).** Let the input \(X\) be \(f_{\mathcal{T},c2}(G)\), where \(G\in\widetilde{\mathcal{G}}_{\delta}\) is not equal to \(\overline{G}\). According to the Property 2 of Lemma E.4 and given a \(j\in[L]\), \(u^{\top}f_{\mathcal{T},c2}(G)_{:,k},(k\in[L])\) differs from \(u^{\top}f_{\mathcal{T},c2}(\overline{G})_{:,j}\) by at least \(\delta\). Then we have \[\zeta_{5}(u^{\top}f_{\mathcal{T},c2}(G)-u^{\top}f_{\mathcal{T},c2}(\overline{G })_{:,j}\mathds{1}_{L}^{\top})=\mathbf{0}_{L}^{\top}.\] Thus the input is left untouched. If \(G=\overline{G}\), then \[\zeta_{5}(u^{\top}f_{\mathcal{T},c2}(G)-u^{\top}f_{\mathcal{T},c2}(\overline{ G})_{:,j}\mathds{1}_{L}^{\top})=(e_{j})^{\top}.\] Thus we shift the \(j\)-th column of \(f_{\mathcal{T},c2}(G)\) to \[f_{\mathcal{T},c2}(G)_{:,j}+((A_{\overline{G}})_{:,j}-f_{\mathcal{T},c2}( \overline{G})_{:,j})=f_{\mathcal{T},c2}(G)_{:,j}+((A_{G})_{:,j}-f_{\mathcal{T },c2}(G)_{:,j})=(A_{G})_{:,j}.\] In other word, this layer maps the column \(f_{\mathcal{T},c2}(G)_{:,j}\) to \((A_{G})_{:,j}\), without affecting any other columns. For each \(G\in\widetilde{\mathcal{G}}_{\delta}\), we defer that we need one layer for each unique value of \(u^{\top}f_{\mathcal{T},c2}(G)_{:,j}\). Note that there are \(\mathcal{O}(\delta^{-dL})\) such numbers, so we use \(\mathcal{O}(\delta^{-dL})\) layers to finish our construction. This completes the proof.

#### e.5.5 Proof of Lemma e.7

Proof of Lemma e.7.: We restate the proof from [23] for completeness.

The proof follows two steps: (i) Approximate the modified self-attention layers. (ii) Approximate the modified feed-forward layers.

* **Step 1: Approximate the Modified Self-Attention Layers.** We achieve this by approximating the \(\operatorname{Softmax}\) operator \(\sigma_{S}\) with the \(\operatorname{Hardmax}\) operator \(\sigma_{H}\). Given a matrix \(X\in\mathbb{R}^{d\times L}\), we have \[\sigma_{S}(\lambda X)\rightarrow\sigma_{H}(X),\quad\text{as}\quad\lambda \rightarrow\infty.\] The operator is the only difference between the normal and the modified self-attention layers. We approximate the modified self-attention layer in \(\overline{\mathcal{T}}_{p}^{r,m,l}\) by the normal self-attention layer with the same number of heads \(r\) and head size \(m\).
* **Step2: Approximate the Modified Feed-Forward Layers.** We achieve this by approximating the activation function in \(\Psi\) with four \(\operatorname{ReLU}\) functions. From Definition E.3, we recall that \(\Psi\) denotes three-piecewise functions with at least a constant piece. We consider the following \(\zeta\in\Psi\): \[\zeta(x)=\begin{cases}b_{1}&\text{if }x<c_{1},\\ a_{2}x+b_{2}&\text{if }c_{1}\leq x<c_{2},\\ a_{3}x+b_{3}&\text{if }c_{2}\leq x,\end{cases}\] where \(a_{2},a_{3},b_{1},b_{2},b_{3},c_{1},c_{2}\in\mathbb{R}\), and \(c_{1}<c_{2}\). We approximate \(\zeta(x)\) by \(\widetilde{\zeta}(x)\) composed of four \(\operatorname{ReLU}\) functions: \[\widetilde{\zeta}(x)= b_{1}+\frac{a_{2}c_{1}+b_{2}-b_{1}}{\epsilon}\operatorname{ ReLU}(\operatorname{x}-c_{1}+\epsilon)+\left(\operatorname{a}_{2}-\frac{ \operatorname{a}_{2}c_{1}+\operatorname{b}_{2}-\operatorname{b}_{1}}{\epsilon }\right)\operatorname{ReLU}(\operatorname{x}-c_{1})\] \[+\left(\frac{a_{3}c_{2}+b_{3}-a_{2}(c_{2}-\epsilon)-b_{2}}{ \epsilon}-a_{2}\right)\operatorname{ReLU}(\operatorname{x}-c_{2}+\epsilon)\] \[+\left(a_{3}-\frac{a_{3}c_{2}+b_{3}-a_{2}(c_{2}-\epsilon)-b_{2}} {\epsilon}\right)\operatorname{ReLU}(\operatorname{x}-c_{2})\] \[= \begin{cases}b_{1}&\text{if }x<c_{1}-\epsilon,\\ (a_{2}c_{1}+b_{2}-b_{1})(x-c_{1})/\epsilon+a_{2}c_{1}+b_{2}&\text{if }c_{1}- \epsilon\leq x<c_{1},\\ a_{2}x+b_{2}&\text{if }c_{1}\leq x<c_{2}-\epsilon,\\ (a_{3}c_{2}+b_{3}-a_{2}(c_{2}-\epsilon)-b_{2})(x-c_{2})/\epsilon+a_{3}c_{2}+b _{3}&\text{if }c_{2}-\epsilon\leq x<c_{2},\\ a_{3}x+b_{3}&\text{if }c_{2}\leq x.\end{cases}\] As \(\epsilon\to 0\), we approximate \(\zeta(x)\) by \(\widetilde{\zeta}(x)\). The activation function is the only difference between the normal and modified feed-forward layers. We approximate the modified feed-forward layer in \(\overline{\mathcal{T}}_{p}^{r,m,l}\) by the normal one. Thus, for any \(f_{\mathcal{T},c}\in\overline{\mathcal{T}}_{p}^{2,1,1}\), there exists a function \(f_{\mathcal{T}}\in\mathcal{T}_{p}^{2,1,4}\) to approximate \(f_{\mathcal{T},c}\). This completes the proof.

Proofs of Section 3

Our proofs are motivated by the approximation and estimation theory of U-Net-based diffusion models in [Chen et al., 2023]. We use transformer networks' universal approximation theory in Appendix E and the covering number to proceed with our proof. Specifically, we derive the approximation error bound in Appendix F.1 and the corresponding sample complexity bound in Appendix F.2. Then we show that the data distribution generated from the estimated score function converges toward a proximate area of the original one in Appendix F.3.

### Proof of Theorem 3.1

Here we present some auxiliary theoretical results in Appendix F.1.1 to prepare for our main proof of Theorem 3.1. Then we derive the approximation error bound of DiTs (i.e., the proof of Theorem 3.1) in Appendix F.1.2.

#### f.1.1 Auxiliary Lemmas for Theorem 3.1.

We restate some auxiliary lemmas and their proofs from [Chen et al., 2023] for later convenience.

**Lemma F.1** (Lemma 16 of [Chen et al., 2023]).: Consider a probability density function \(p_{h}(h)=\exp\bigl{(}-C\|h\|_{2}^{2}/2\bigr{)}\) for \(h\in\mathbb{R}^{d_{0}}\) and constant \(C>0\). Let \(r_{h}>0\) be a fixed radius. Then it holds

\[\int_{\|h\|_{2}>r_{h}}p_{h}(h)\mathrm{d}h\leq\frac{2d_{0}\pi^{d_{0 }/2}}{C\Gamma(d_{0}/2+1)}r_{h}^{d_{0}-2}\exp\bigl{(}-Cr_{h}^{2}/2\bigr{)},\] \[\int_{\|h\|_{2}>r_{h}}\|h\|_{2}^{2}p_{h}(h)\mathrm{d}h\leq\frac{2 d_{0}\pi^{d_{0}/2}}{C\Gamma(d_{0}/2+1)}r_{h}^{d_{0}}\exp\bigl{(}-Cr_{h}^{2}/2 \bigr{)}.\]

**Lemma F.2** (Lemma 2 of [Chen et al., 2023]).: Suppose Assumption 2.2 holds and \(g\) is defined as:

\[q(\overline{h},t)=\int\frac{h\psi_{t}(\overline{h}|h)p_{h}(h)}{\int\psi_{t}( \overline{h}|h)p_{h}(h)\mathrm{d}h}\mathrm{d}h,\quad\overline{h}=B^{\top} \overline{x}.\]

Given \(\epsilon>0\), with \(r_{h}=c\left(\sqrt{d_{0}\log(d_{0}/T_{0})+\log(1/\epsilon)}\right)\) for an absolute constant \(c\), it holds

\[\big{\|}q(\overline{h},t)\mathds{1}\{\big{\|}\overline{h}\big{\|}_{2}\geq r_{h }\big{\}}\big{\|}_{L^{2}(P_{t})}\leq\epsilon,\text{ for }t\in[T_{0},T].\]

**Lemma F.3** (Theorem 1 of [Chen et al., 2023]).: We denote

\[\tau(r_{h})=\sup_{t\in[T_{0},T]}\sup_{\overline{h}\in[0,r_{h}]^{d}}\left\| \frac{\partial}{\partial t}q(\overline{h},t)\right\|_{2}.\]

With \(q(\overline{h},t)=\int h\psi_{t}(\overline{h}|h)p_{h}(h)/(\int\psi_{t}( \overline{h}|h)p_{h}(h)\mathrm{d}h)\mathrm{d}h\) and \(p_{h}\) satisfies Assumption 2.2, we have a coarse upper bound for \(\tau(r_{h})\):

\[\tau(r_{h})=\mathcal{O}\left(\frac{1+\beta^{2}(t)}{\beta(t)}\left(L_{s_{+}}+ \frac{1}{\sigma(t)}\right)\sqrt{d_{0}}r_{h}\right)=\mathcal{O}\left(e^{7/2}L_ {s_{+}}r_{h}\sqrt{d_{0}}\right).\]

**Lemma F.4** (Lemma 10 of [Chen et al., 2020]).: For any given \(\epsilon>0\), and \(L\)-Lipschitz function \(g\) defined on \([0,1]^{d_{0}}\), there exists a continuous function \(\overline{f}\) constructed by trapezoid function, such that

\[\big{\|}g-\overline{f}\big{\|}_{\infty}\leq\epsilon.\]Moreover, the Lipschitz continuity of \(\overline{f}\) is bounded:

\[\big{|}\overline{f}(x)-\overline{f}(y)\big{|}\leq 10d_{0}L\|x-y\|_{2}\quad\text{ for any}\quad x,y\in[0,1]^{d_{0}}.\]

#### f.1.2 Main Proof of Theorem 3.1

Proof of Theorem 3.1.: With \(\nabla\log p_{t}^{h}\left(\overline{h}\right)=B^{\top}s_{+}(\overline{h},t)\), we have the following in (2.4)

\[q(\overline{h},t)=\sigma(t)\nabla\log p_{t}^{h}\left(\overline{h}\right)+B^{ \top}\overline{x}=\sigma(t)B^{\top}(s_{+}(\overline{h},t)+\overline{x}).\] (F.1)

We proceed as follows:

* **Step 1.** Approximate \(q(\overline{h},t)\) with a compact-supported continuous function \(\overline{f}(\overline{h},t)\).
* **Step 2.** Approximate \(\overline{f}(\overline{h},t)\) with a transformer network.

Step 1. Approximate \(q(\overline{h},t)\) with a Compact-supported Continuous Function \(\overline{f}(\overline{h},t)\).We partition \(\mathbb{R}^{d_{0}}\) into a compact subset \(H_{1}:=\{\overline{h}\big{|}\big{\|}\overline{h}\big{\|}_{2}\leq r_{h}\}\) and its complement \(H_{2}\), where \(r_{h}\) is to be determined later. We approximate \(q(\overline{h},t)\) on the two subsets respectively and then prove \(\overline{f}\)'s continuity. Such a step achieves an estimation error of \(\sqrt{d_{0}}\epsilon\) between \(q(\overline{h},t)\) and \(\overline{f}(\overline{h},t)\). We show the main proof here.

* **Approximation on \(H_{2}\times[T_{0},T]\).** For any \(\epsilon>0\), we take \(r_{h}=c(\sqrt{d_{0}\log(d_{0}/T_{0})-\log\epsilon})\). From Lemma F.2, we have \[\big{\|}q(\overline{h},t)\mathds{1}\{\big{\|}\overline{h}\big{\|}_{2}\geq r_{h }\}\big{\|}_{L^{2}(P_{t})}\leq\epsilon\quad\text{for}\quad t\in[T_{0},T].\] So we set \(\overline{f}(\overline{h},t)=0\) on \(H_{2}\times[T_{0},T]\).
* **Approximation on \(H_{1}\times[T_{0},T]\).** On \(H_{1}\times[T_{0},T]\), we approximate \(q(\overline{h},t)\) by approximating each coordinate \(q_{k}(\overline{h},t)\) respectively, where \(q(\overline{h},t)=[q_{1}(\overline{h},t),q_{2}(\overline{h},t),\cdots,q_{d_{0 }}(\overline{h},t)]\). We rescale the input by \(y^{\prime}=(\overline{h}+r_{h}\mathds{1})/2r_{h}\) and \(t^{\prime}=t/T\). Then the transformed input space is \([0,1]^{d_{0}}\times[T_{0}/T,1]\). We implement such a transformation by a single feed-forward layer. By Assumption 2.3, on-support score \(s_{+}(\overline{h},t)\) is \(L_{s_{+}}\)-Lipschitz in \(\overline{h}\). This implies \(q(\overline{h},t)\) is \((1+L_{s_{+}})\)-Lipschitz in \(\overline{h}\). When taking the transformed inputs, \(g(y^{\prime},t^{\prime})=q(2r_{h}y^{\prime}-r_{h}\mathds{1},Tt^{\prime})\) becomes \(2r_{h}(1+L_{s_{+}})\)-Lipschitz in \(y^{\prime}\). Similarly, each coordinate \(g_{k}(y^{\prime},t)\) is also \(2r_{h}(1+L_{s_{+}})\)-Lipschitz in \(y^{\prime}\). Here we take \(L_{h}=1+L_{s_{+}}\). Besides, \(g(y^{\prime},t^{\prime})\) is \(T\tau(r_{h})\)-Lipschitz with respect to \(t\), where \[\tau(r_{h})=\sup_{t\in[T_{0},T]}\sup_{\overline{h}\in[0,r_{h}]^{d_{0}}}\bigg{\|} \frac{\partial}{\partial t}q(\overline{h},t)\bigg{\|}_{2}.\] We have a coarse upper bound for \(\tau(r_{h})\) in Lemma F.3. We restate it here for convenience \[\tau(r_{h})=\mathcal{O}\left(\frac{1+\beta^{2}(t)}{\beta(t)}\left(L_{s_{+}}+ \frac{1}{\sigma(t)}\right)\sqrt{d_{0}}r_{h}\right)=\mathcal{O}\left(e^{T/2}L_{ s_{+}}r_{h}\sqrt{d_{0}}\right).\] In conclusion, each \(g_{k}(y^{\prime},t)\) is Lipschitz continuous. So we can apply Lemma F.4 to determine \(\overline{f}_{k}(y^{\prime},t)\) for approximating each coordinate. We concatenate \(\overline{f}_{i}\)'s together and construct \(\overline{f}=[\overline{f}_{1},\ldots,\overline{f}_{d_{0}}]^{\top}\). According to the construction in Lemma F.4 and for any given \(\epsilon\), we achieve \[\sup_{y^{\prime},t^{\prime}\in[0,1]^{d}\times[T_{0}/T,1]}\big{\|}\overline{f }(y^{\prime},t^{\prime})-g(y^{\prime},t^{\prime})\big{\|}_{\infty}\leq\epsilon,\]Considering the input rescaling (i.e., \(\overline{h}\to y^{\prime}\) and \(t\to t^{\prime}\)), we obtain: * The constructed function is Lipschitz continuous in \(\overline{h}\). For any \(\overline{h}_{1},\overline{h}_{2}\in H_{1}\) and \(t\in[T_{0},T]\), it holds \[\big{\|}\overline{f}(\overline{h}_{1},t)-\overline{f}(\overline{h}_{2},t) \big{\|}_{\infty}\leq 10d_{0}L_{h}\big{\|}\overline{h}_{1}-\overline{h}_{2} \big{\|}_{2}.\] (F.2) * The function is also Lipschitz in \(t\). For any \(t_{1},t_{2}\in[T_{0},T]\) and \(\big{\|}\overline{h}\big{\|}_{2}\leq r_{h}\), it holds \[\big{\|}\overline{f}(\overline{h},t_{1})-\overline{f}(\overline{h},t_{2}) \big{\|}_{\infty}\leq 10\tau(r_{h})\big{\|}t_{1}-t_{2}\big{\|}_{2}.\] Due to the fact that the construction of \(\overline{f}(\overline{h},t)\) is based on trapezoid function, we have \(\overline{f}(\overline{h},t)=0\) for \(\big{\|}\overline{h}\big{\|}_{2}=r_{h}\) and any \(t\in[T_{0},T]\). Thus, the two parts of \(\overline{f}(\overline{h},t)\) can be joined together. To be more specific, the above Lipschitz continuity in \(\overline{h}\) extends to the whole \(\mathbb{R}^{d_{0}}\).
* **Approximation Error Analysis under \(L^{2}\) Norm.** The \(L^{2}\) approximation error of \(\overline{f}\) can be decomposed into two terms: \[\big{\|}q(\overline{h},t)-\overline{f}(\overline{h},t)\big{\|}_{L ^{2}(P^{h}_{t})}\] \[=\big{\|}(q(\overline{h},t)-\overline{f}(\overline{h},t))\mathds{1 }\{\big{\|}\overline{h}\big{\|}_{2}<r_{h}\}\big{\|}_{L^{2}(P^{h}_{t})}+\big{\|} q(\overline{h},t)\mathds{1}\{\big{\|}\overline{h}\big{\|}_{2}>r_{h}\}\big{\|}_{L ^{2}(P^{h}_{t})}.\] The second term in the RHS above has already been bounded with the selection of \(r_{h}\): \[\big{\|}g(\overline{h},t)\mathds{1}\{\big{\|}\overline{h}\big{\|}_{2}>r_{h}\} \big{\|}_{L^{2}(P^{h}_{t})}\leq\epsilon.\] The first term is bounded by: \[\big{\|}(q(\overline{h},t)-\overline{f}(\overline{h},t))\mathds{1 }\{\big{\|}\overline{h}\big{\|}_{2}<r_{h}\}\big{\|}_{L^{2}(P^{h}_{t})}\] \[\leq\sqrt{d_{0}}\sup_{y^{\prime},t^{\prime}\in[0,1]^{d}\times[T_{ 0}/T,1]}\big{\|}\overline{f}(y^{\prime},t^{\prime})-g(y^{\prime},t^{\prime}) \big{\|}_{\infty}\] \[\leq\sqrt{d_{0}}\epsilon.\] Then we obtain \[\big{\|}q(\overline{h},t)-\overline{f}(\overline{h},t)\big{\|}_{L ^{2}(P^{h}_{t})}\leq(\sqrt{d_{0}}+1)\epsilon.\] If we substitute \(\epsilon\) with \(\epsilon/2\), we obtain that the approximation error of \(\overline{f}(\overline{h},t)\) is \(\sqrt{d_{0}}\epsilon\).

**Step 2. Approximate \(\overline{f}(\overline{h},t)\) by a Transformer.** This step is based on the universal approximation of transformers for the compact-supported continuous function in Lemma E.1. DiT uses time point \(t\) to calculate the scale and shift value in the transformer backbone [23]. It also transforms an input picture into a sequential version. We ignore time point \(t\) in the notation of the transformer network in DiT. Recall the reshape layer \(R(\cdot)\) in Definition 3.1, we consider using \(f(\cdot):=R^{-1}\circ f_{\mathcal{T}}\circ R(\cdot)\) to approximate \(\overline{f}_{t}(\cdot):=\overline{f}(\cdot,t)\), where \(f_{\mathcal{T}}\in\mathcal{T}_{p}^{2,1,4}\).

* **Overall Approximation Error.** With Lemma E.1, we approximate \(\overline{f}_{t}(\cdot)\) with \(\widehat{f}(\cdot):=R^{-1}\circ\widehat{f}_{\mathcal{T}}\circ R(\cdot)\). We denote \[H=R(\overline{h}).\] We have \[\Big{\|}\overline{f}_{t}(\overline{h})-\widehat{f}(\overline{h})\Big{\|}_{L ^{2}(P^{h}_{t})}=\left(\int_{P^{h}_{t}}\Big{\|}\overline{f}_{t}(\overline{h})- \widehat{f}(\overline{h})\Big{\|}_{2}^{2}\mathrm{d}h\right)^{1/2}\]\[=\left(\int_{P_{t}^{h}}\left\|R\circ\overline{f}_{t}\circ R^{-1}(H)-R \circ\widehat{f}\circ R^{-1}(H)\right\|_{F}^{2}\mathrm{d}h\right)^{1/2}\] \[=\left(\int_{P_{t}^{h}}\left\|R\circ\overline{f}_{t}\circ R^{-1}(H )-\widehat{f}_{\mathcal{T}}(H)\right\|_{F}^{2}\mathrm{d}h\right)^{1/2}\] \[\leq\epsilon.\] (F.3) Along with Step 1, we obtain \[\left\|q(\overline{h},t)-\widehat{f}(\overline{h})\right\|_{L^{2}(P_{t}^{h})} \leq\left\|q(\overline{h},t)-\overline{f}(\overline{h},t)\right\|_{L^{2}(P_{t }^{h})}+\left\|\overline{f}(\overline{h},t)-\widehat{f}(\overline{h})\right\| _{L^{2}(P_{t}^{h})}\leq(1+\sqrt{d_{0}})\epsilon.\] The constructed approximator to \(\nabla\log p_{t}(x)\) is \(s_{\widehat{W}}=(B\widehat{f}(B^{\top}x,t)-x)/\sigma(t)\), and the approximation error is \[\left\|\nabla\log p_{t}(\cdot)-s_{\widehat{W}}(\cdot,t)\right\|_{L^{2}(P_{t})} \leq\frac{1+\sqrt{d_{0}}}{\sigma(t)}\epsilon\quad\text{for any}\quad t\in[T_{0 },T].\]
* **Settling-down of Hyperparameters.** We settle down the hyperparameters to configure our network here. We refer to Appendix E.2 for some of the following calculations.
1. **Model Architecture Depth \(K\).** From Lemma E.6, we have \(K=\mathcal{O}((1/\delta)^{dL})\). To achieve \(\epsilon\)-error approximation, we set \(\delta=\mathcal{O}\left(\epsilon^{2/d}\right)\) according to Lemma E.3. Thus we obtain \[K=\mathcal{O}\left(\epsilon^{-2L}\right).\] (F.4)
2. **Lipchitz Upperbound for Transformer: \(L_{\mathcal{T}}\).** We denote \(\overline{f}_{t,R}(\cdot)=R\circ\overline{f}_{t}\circ R^{-1}(\cdot)\). We get the Lipshitz upper bound for \(\widehat{f}_{\mathcal{T}}\in\mathcal{T}_{p}^{2.1.4}\) in the following way \[\left\|\widehat{f}_{\mathcal{T}}\left(H_{1}\right)-\widehat{f}_{ \mathcal{T}}\left(H_{2}\right)\right\|_{F} \leq\left\|\widehat{f}_{\mathcal{T}}\left(H_{1}\right)-\overline{ f}_{t,R}\left(H_{1}\right)\right\|_{F}+\left\|\overline{f}_{t,R}\left(H_{1} \right)-\overline{f}_{t,R}\left(H_{2}\right)\right\|_{F}\] \[\quad+\left\|\overline{f}_{t,R}\left(H_{2}\right)-\widehat{f}_{ \mathcal{T}}\left(H_{2}\right)\right\|_{F}\] \[\leq 2\epsilon+\left\|\overline{f}_{t,R}\left(H_{1}\right)-\overline {f}_{t,R}\left(H_{2}\right)\right\|_{F}\] (By (F.3)) \[\leq 2\epsilon+10d_{0}L_{s_{+}}\|H_{1}-H_{2}\|_{F}.\] (By (F.2)) Then we get \[L_{\mathcal{T}}=\mathcal{O}\left(d_{0}L_{s_{+}}\right).\] (F.5)
3. **Model Output Bound for \(\mathcal{S}_{\mathcal{T}_{p}^{2,1,4}}\).** For the output of the constructed transformer \(\widehat{f}_{\mathcal{T}}(\cdot)\), according to Lemma E.5, we have \(\widehat{f}_{\mathcal{T}}(O)=O\), where \(O=\mathbf{0}_{d\times L}\). Thus, with the Lipschitz upperbound \(\mathcal{O}(d_{0}L_{s_{+}})\), we have \(\|\widehat{f}_{\mathcal{T}}(H)\|_{F}=\mathcal{O}(d_{0}L_{s_{+}}r_{h})\), where \(\left\|H\right\|_{F}\leq r_{h}\). With \(r_{h}=c(\sqrt{d_{0}\log(d_{0}/T_{0})+\log(1/\epsilon)})\), we obtain \[C_{\mathcal{T}}=\mathcal{O}\left(d_{0}L_{s_{+}}\cdot\sqrt{d_{0}\log(d_{0}/T_{0} )+\log(1/\epsilon)}\right).\] (F.6)
4. **Model Parameters Bound: \(C_{OV}^{2,\infty},C_{OV},C_{KQ}^{2,\infty},C_{KQ},C_{E}\).** By definition, we have: \[\left\|(W_{OV}^{i})^{\top}\right\|_{2,\infty}\leq C_{OV}^{2,\infty},\,\left\|( W_{OV}^{i})^{\top}\right\|_{2}\leq C_{OV},\,\left\|W_{KQ}^{i}\right\|_{2, \infty}\leq C_{KQ}^{2,\infty},\,\left\|W_{KQ}^{i}\right\|_{2}\leq C_{KQ},\]where \(i=1,2\). For simplicity, we omit \(i\) hereafter, which does not affect our discussion.

Recall that \(\left\lVert Z\right\rVert_{2,\infty}\) denotes the \(2,\infty\)-norm, where the \(2\)-norm is over columns and \(\infty\)-norm is over rows. By the construction of modified attention layers (E.11) and (E.12) in Appendix E.5.3, we consider \(W_{OV}\) to have the largest norm, i.e.,

\[W_{OV}=L\delta^{-(L+1)d-1}\cdot\left(\begin{array}{cccc}1&\delta^{-1}&\cdots &\delta^{-d+1}\\ 0&0&\cdots&0\\ \vdots&\vdots&\cdots&\vdots\\ 0&0&\cdots&0\end{array}\right).\]

We give the following upper bounds

\[\left\lVert W_{OV}^{\top}\right\rVert_{2,\infty} =Ld\delta^{-(L+2)d}=\mathcal{O}\left(\delta^{-Ld}\right),\] (F.7) \[\left\lVert W_{OV}^{\top}\right\rVert_{2} =\sup_{\left\lVert x\right\rVert_{2}=1}\left\lVert W_{OV}^{\top}x \right\rVert_{2}=L\delta^{-(L+1)d-1}\cdot\sqrt{\sum_{i=0}^{d-1}\delta^{-2i}}= \mathcal{O}\left(\delta^{-Ld}\right).\] (F.8)

By (E.11) and (E.12) in Appendix E.5.3, and the self-attention layers in Appendix E.5.5, we consider \(W_{KQ}\) to have the largest norm, i.e.,

\[W_{KQ} :=\left(\begin{array}{c}1\\ \delta^{-1}\\ \vdots\\ \delta^{-d+1}\end{array}\right)\left(1,\delta^{-1},\cdots,\delta^{-d+1} \right)=\left(\begin{array}{cccc}1&\delta^{-1}&\cdots&\delta^{-d+1}\\ \delta^{-1}&\delta^{-2}&\cdots&\delta^{-d}\\ \vdots&\vdots&\cdots&\vdots\\ \delta^{-d+1}&\delta^{-d}&\cdots&\delta^{-2d+2}\end{array}\right).\]

Then we have

\[\left\lVert W_{KQ}\right\rVert_{2,\infty} =\sqrt{\sum_{i=0}^{d-1}\delta^{-2i-2d+2}}=\mathcal{O}(\delta^{-2d }),\] (F.9) \[\left\lVert W_{KQ}\right\rVert_{2} =\sup_{\left\lVert x\right\rVert_{2}=1}\left\lVert W_{KQ}x\right\rVert _{2}=\delta^{-2d+2}=\mathcal{O}(\delta^{-2d}).\] (F.10)

We substitute \(\delta\) with \(\mathcal{O}\left(\epsilon^{2/d}\right)\) (according to Appendix E.4) and get:

\[C_{OV}^{2,\infty} =(1/\epsilon)^{\mathcal{O}(1)},\] \[C_{OV} =(1/\epsilon)^{\mathcal{O}(1)},\] \[C_{KQ}^{2,\infty} =(1/\epsilon)^{\mathcal{O}(1)},\] \[C_{KQ} =(1/\epsilon)^{\mathcal{O}(1)}.\]

From the construction of positional encoder (E.4) in Appendix E.2, we have

\[E=\left(\begin{array}{cccc}0&1&\cdots&L-1\\ 0&1&\cdots&L-1\\ \vdots&\vdots&\vdots&\vdots\\ \vdots&\vdots&\vdots\\ 0&1&\cdots&L-1\end{array}\right).\]

We deduce

\[\left\lVert E^{\top}\right\rVert_{2,\infty} =\sqrt{L}(L-1)=\mathcal{O}(L^{3/2}).\]Thus we have \[C_{E}=\mathcal{O}(L^{3/2}).\] (F.11)
5. **Parameters Bound in Feed Forward Layers:**\(C_{F}^{2,\infty},C_{F}\)**.** Recall the construction of modified feed-forward layers in the proof of Lemma E.4, which includes Definitions E.5, E.6 and E.8 to E.10. With the approximation by normal feed-forward layers in Appendix E.5.5, we consider the weight parameters with the largest norm in the feed-forward layers, i.e., \[W_{1}:=\left(\begin{array}{c}1\\ 1\\ 1\\ 1\end{array}\right)\left(1,\delta^{-1},\cdots,\delta^{-d+1}\right)=\left( \begin{array}{cccc}1&\delta^{-1}&\cdots&\delta^{-d+1}\\ 1&\delta^{-1}&\cdots&\delta^{-d+1}\\ 1&\delta^{-1}&\cdots&\delta^{-d+1}\\ \end{array}\right)\in\mathbb{R}^{4\times d}.\] Then we have \[C_{F}^{2,\infty} =\mathcal{O}\left(\sqrt{\sum_{i=0}^{d-1}\delta^{-2i}}\right)= \mathcal{O}\left(\delta^{-d}\right)\] (F.12) \[=(1/\epsilon)^{\mathcal{O}(1)}.\qquad\qquad\qquad\left(\text{By setting }\delta=\mathcal{O}(\epsilon^{2/d})\text{ according to Appendix\leavevmode\nobreak\ E.4}\right)\] and \[C_{F} =\sup_{\left\|x\right\|_{2}=1}\left\|W_{1}x\right\|_{2}=\mathcal{O}\left( \delta^{-d}\right)\] (F.13) \[=(1/\epsilon)^{\mathcal{O}(1)}.\qquad\qquad\qquad\left(\text{By setting }\delta=\mathcal{O}(\epsilon^{2/d})\text{ according to Appendix\leavevmode\nobreak\ E.4}\right)\] This completes the proof.

### Proof of Theorem 3.2

Here we present the auxiliary theoretical results about the covering number of transformer networks in Appendix F.2.1. The results are based on [22, Theorem A.17]. Then we derive the sample complexity bound of DiTs (i.e., the proof of Theorem 3.2) in Appendix F.2.

#### f.2.1 Auxiliary Lemmas for Theorem 3.2

**Lemma F.5** (Lemma 15 of [22]).: Let \(\mathcal{G}\) be a bounded function class. Then there exists a constant \(b\) such that the output of any \(g\in\mathcal{G}:\mathbb{R}^{d_{0}}\mapsto[0,b]\) is bounded by \(b\). Let \(z_{1},z_{2},\cdots,z_{n}\in\mathbb{R}^{d_{0}}\) be i.i.d. random variables. For any \(\delta\in(0,1),a\leq 1\), and \(c>0\), we have

\[P\left(\sup_{g\in\mathcal{G}}\frac{1}{n}\sum_{i=1}^{n}g(z_{i})-( 1+a)\mathbb{E}\left[g(z)\right]>\frac{(1+3/a)B}{3n}\log\frac{\mathcal{N}(c, \mathcal{G},\left\|\cdot\right\|_{\infty})}{\delta}+(2+a)c\right)\leq\delta,\] \[P\left(\sup_{g\in\mathcal{G}}\mathbb{E}\left[g(z)\right]-\frac{1+ a}{n}\sum_{i=1}^{n}g(z_{i})>\frac{(1+6/a)B}{3n}\log\frac{\mathcal{N}(c, \mathcal{G},\left\|\cdot\right\|_{\infty})}{\delta}+(2+a)c\right)\leq\delta.\]

Now, we give the definition of the covering number as follows.

**Definition F.1** (Covering Number).: Given a function class \(\mathcal{F}\) and a data distribution \(P\). Sample n data points \(\{X_{i}\}_{i=1}^{n}\) from \(P\). For any \(\epsilon>0\), the covering number \(\mathcal{N}(\epsilon,\mathcal{F},\{X_{i}\}_{i=1}^{n},\left\|\cdot\right\|)\) is the smallest size of a collection (a cover) \(\mathcal{C}\in\mathcal{F}\), such that for any \(f\in\mathcal{F}\), there exists a \(\widehat{f}\in\mathcal{C}\) satisfying

\[\max_{i}\left\|f(X_{i})-\widehat{f}(X_{i})\right\|\leq\epsilon.\]

Furthermore, we define the covering number with respect to the data distribution as

\[\mathcal{N}(\epsilon,\mathcal{F},\left\|\cdot\right\|)=\sup_{\{X_{i}\}_{i=1}^{ n}\sim P}\mathcal{N}(\epsilon,\mathcal{F},\{X_{i}\}_{i=1}^{n},\left\|\cdot\right\|).\]

Then we give the covering number of the transformer networks.

**Lemma F.6** (Modified from Theorem A.17 of [22]).: Let \(\mathcal{T}_{p}^{r,m,l}(K,C_{\mathcal{T}},C_{OV}^{2,\infty},C_{OV},C_{KQ}^{2, \infty},C_{KQ},C_{F}^{2,\infty},C_{F},C_{E},L_{\mathcal{T}})\) represent the class of functions of \(K\)-layer transformer blocks satisfying the norm bound for matrix and Lipschitz property for feed-forward layers. Then for all data point \(\left\|X\right\|_{2,\infty}\leq C_{X}\), we have

\[\log\mathcal{N}(\epsilon_{c},\mathcal{T}_{p}^{r,m,l}(K,C_{\mathcal{ T}},C_{OV}^{2,\infty},C_{OV},C_{KQ}^{2,\infty},C_{KQ},C_{F}^{2,\infty},C_{F},C _{E},L_{\mathcal{T}}),\left\|\cdot\right\|_{2})\] \[\leq \frac{\log(nL)}{\epsilon_{c}^{2}}\cdot\left(\sum_{i=1}^{K}\alpha ^{\frac{2}{3}}\left(d^{\frac{2}{3}}\left(C_{F}^{2,\infty}\right)^{\frac{2}{3}} +d^{\frac{2}{3}}\left(2(C_{F})^{2}C_{OV}C_{KQ}^{2,\infty}\right)^{\frac{2}{3} }+\tau m^{\frac{2}{3}}\left((C_{F})^{2}C_{OV}^{2,\infty}\right)^{\frac{2}{3}} \right)\right)^{3},\]

where \(\alpha\coloneqq\prod_{j<i}(C_{F})^{2}C_{OV}(1+4C_{KQ})(C_{X}+C_{E})\).

**Remark F.1**.: We modify [22, Theorem A.17] in seven aspects:

1. We do not consider the last linear layer in the model, which converts each column vector of the transformer output to a scalar. Therefore, we ignore the item related to the last linear layer in [22, Theorem A.17].
2. We do not consider the normalization layer in our model. Because the normalization layer \(\prod_{\mathrm{norm}}(\cdot)\) in the original proof only ensures that \(\left\|\prod_{\mathrm{norm}}(X_{1})-\prod_{\mathrm{norm}}(X_{2})\right\|_{2, \infty}\leq\left\|X_{1}-X_{2}\right\|_{2,\infty}\), ignoring this layer does not change the result.
3. Our activation function is \(\mathrm{ReLU}\). Thus, we replace the Lipschitz upperbound of activate function by 1.

4. We consider the positional encoding (E.4). Then we need to replace the upperbound \(C_{X}\) for the inputs with the upperbound \(C_{X}+C_{E}\). Besides, for multi-layer transformer, the original conclusion in (Edelman et al., 2022, Theorem A.17) uses 1 as the upperbound for the \(2,\infty\)-norm of inputs. We incorporate the upperbound for the inputs into the result stated in Lemma F.6.
5. We use (2.7) as the feed-forward layer, including two linear layers and a residual layer. Thus, we replace the original upperbound for the norm of weight matrix with the upperbound for the norm of \(I_{d}+W_{2}W_{1}\) in Lemma F.6. In the following, we use \(\mathcal{O}\) to estimate the log-covering number, thus we ignore the item for \(I_{d}\) here for convergence. This is the same for the self-attention layer.
6. We use multi-head attention, and incorporate the number of heads \(\tau\) into our result, which is similar to (Edelman et al., 2022, Theorem A.12).
7. In our work, we use the transformer \(\mathcal{T}_{p}^{2,1,4}\), i.e., \(\tau=2,m=1\).

#### f.2.2 Proof of Theorem 3.2

Proof of Theorem 3.2.: Our proof is built on (Chen et al., 2023, Appendix B.2). For one data sample, we define the empirical score matching loss objective (2.1) as follows

\[\ell(x;s_{\widehat{W}})=\frac{1}{T-T_{0}}\int_{T_{0}}^{T}\mathbb{ E}_{x_{t}\mid x_{0}=x}\big{[}\big{\|}\nabla_{x_{t}}\log\psi_{t}(x_{t}|x_{0})-s_ {\widehat{W}}(x_{t},t)\big{\|}_{2}^{2}\big{]}\mathrm{d}t.\]

Then we define \(\mathcal{L}(s_{\widehat{W}})=\mathbb{E}_{x\sim P_{0}}\left[\ell(x;s_{\widehat{ W}})\right]\).

Following (Chen et al., 2023, Appendix B.2), for any \(a\in(0,1)\), we have

\[\mathcal{L}(s_{\widehat{W}})\] \[\leq\underbrace{\mathcal{L}^{\mathrm{trunc}}(s_{\widehat{W}})-(1+ a)\widehat{\mathcal{L}}^{\mathrm{trunc}}(s_{\widehat{W}})}_{(I)}+\underbrace{ \mathcal{L}(s_{\widehat{W}})-\mathcal{L}^{\mathrm{trunc}}(s_{\widehat{W}})}_{(II )}+(1+a)\underbrace{\inf_{s_{W}\in S_{\mathrm{NN}}}\widehat{\mathcal{L}}(s_{W })}_{(III)},\]

where

\[\mathcal{L}^{\mathrm{trunc}}(s_{\widehat{W}})\coloneqq\mathbb{E}_{x\sim P_{0}} \left[\ell^{\mathrm{trunc}}(x;s_{\widehat{W}})\right]=\mathbb{E}_{x\sim P_{0}} \left[\ell(x;s_{\widehat{W}})\mathds{1}\{\left\|x\right\|_{2}\leq r_{x}\} \right],\;r_{x}>B.\]

We denote

\[\eta \coloneqq 4C_{\mathcal{T}}(C_{\mathcal{T}}+r_{x})(r_{x}/D)^{D-2} \exp\bigl{(}-r_{x}^{2}/\sigma(t)\bigr{)}/(T_{0}(T-T_{0})),\] \[r_{x} \coloneqq\mathcal{O}\left(\sqrt{d_{0}\log d_{0}+\log C_{\mathcal{ T}}+\log\bigl{(}n/\overline{\delta}\bigr{)}}\right).\]

Then we have

\[\eta\leq\frac{1}{nT_{0}(T-T_{0})}.\] (F.14)

For any \(\overline{\delta}>0\), according to Lemma F.5, the following holds for term \((I)\) with probability \(1-\overline{\delta}\),

\[(I)=\mathcal{O}\left(\frac{(1+6/a)(C_{\mathcal{T}}^{2}+r_{x}^{2})}{nT_{0}(T-T _{0})}\log\frac{\mathcal{N}\left(\frac{(T-T_{0})(t-\eta)}{(C_{\mathcal{T}}+r_{ x})\log(T/T_{0})},\mathcal{S}_{\mathcal{T}_{p}^{2,1,4}},\left\|\cdot\right\|_{2} \right)}{\overline{\delta}}+(2+a)\iota\right),\]

where \(c\leq 0\) is a constant, and \(\iota>0\) will be determined later.

We set

\[\iota:=\frac{2}{n^{b}T_{0}(T-T_{0})},\]

where \(0<b\leq 1\) is a constant to be determined later.

**Remark F.2** (Selection Criteria of \(\tau\)).: We have two criteria:

* Recall that the covering number used in our setting is \(\mathcal{N}\left(\frac{(T-T_{0})(\iota-\eta)}{(C_{\mathcal{T}}+r_{x})\log(T/T_ {0})},\mathcal{S}_{\mathcal{T}_{p}^{2,1,4}},\left\|\cdot\right\|_{2}\right)\). Thus, we must ensure \(\iota\geq\eta\). According to (F.14), we consider \(\iota\) satisfying the condition \(\iota\geq(nT_{0}(T-T_{0}))^{-1}\). Therefore, we consider \(0<b\leq 1\).
* For the exponent of \((T-T_{0})\), although selecting a value smaller than 1 is possible, we find that the convergence rate with respect to \(T\) is dominated by the \(1/T\) term appearing later in the second term of (F.18). Therefore, we continue to consider the exponent to be 1.

Then we have

\[(I)=\mathcal{O}\left(\frac{(1+6/a)\left(C_{\mathcal{T}}^{2}+r_{x}^{2}\right)}{ nT_{0}(T-T_{0})}\log\frac{\mathcal{N}\left((n^{b}(C_{\mathcal{T}}+r_{x})T_{0} \log(T/T_{0}))^{-1},\mathcal{S}_{\mathcal{T}_{p}^{2,1,4}},\left\|\cdot\right\| _{2}\right)}{\delta}+\frac{4+2a}{n^{b}T_{0}(T-T_{0})}\right),\]

with probability \(1-\overline{\delta}\).

Following the proof structure of term \((II)\) in [Chen et al., 2023, Appendix B.2], we have

\[(II)=\mathcal{O}\left(\frac{1}{T_{0}}C_{\mathcal{T}}^{2}r_{x}^{2}\exp\{-A_{2} r_{x}^{2}/2\}\right).\]

For any \(\epsilon>0\), let \(s_{\overline{W}}\) be the transformer network approximator to the score function in Theorem 3.1.

For the term \((III)\), we have

\[(III)\leq\underbrace{\widehat{\mathcal{L}}(s_{\overline{W}})-(1+a)\mathcal{L} ^{\mathrm{trunc}}(s_{\overline{W}})}_{(II)_{1}}+(1+a)\underbrace{\mathcal{L}^ {\mathrm{trunc}}(s_{\overline{W}})}_{(III)_{2}}.\]

For any \(\overline{\delta}>0\), according to Lemma F.5 and given that \(s_{\overline{W}}\) is a fixed function, the following holds for term \((III)_{1}\) with probability \(1-\overline{\delta}\),

\[(III)_{1}=\mathcal{O}\left(\frac{(1+3/a)\left(C_{\mathcal{T}}^{2}+r_{x}^{2} \right)}{nT_{0}(T-T_{0})}\log\frac{1}{\overline{\delta}}\right).\]

Following the proof structure of term \((III)_{2}\) in [Chen et al., 2023, Appendix B.2], we have

\[(III)_{2}=\mathcal{O}\left(\frac{d\epsilon^{2}}{T_{0}(T-T_{0})} \right)+C_{3},\]

where \(C_{3}\) is a constant.

Putting \((I)\), \((II)\), and \((III)\) together and setting \(a=\epsilon^{2}\), then we have

\[\frac{1}{T-T_{0}}\int_{T_{0}}^{T}\left\|s_{\widehat{W}}(\cdot,t)- \nabla\log p_{t}(\cdot)\right\|_{L^{2}(P_{t})}^{2}\mathrm{d}t\] \[=\mathcal{O}\left(\frac{\left(C_{\mathcal{T}}^{2}+r_{x}^{2}\right) }{\epsilon^{2}nT_{0}(T-T_{0})}\log\frac{\mathcal{N}\left((n^{b}(C_{\mathcal{T }}+r_{x})T_{0}\log(T/T_{0}))^{-1},\mathcal{S}_{\mathcal{T}_{p}^{2,1,4}},\left\| \cdot\right\|_{2}\right)}{\overline{\delta}}+\frac{n^{-b}+d_{0}\epsilon^{2}}{ T_{0}(T-T_{0})}\right),\] (F.15)

with probability \(1-3\overline{\delta}\).

Covering Number of \(\mathcal{S}_{T^{2,1,4}}\).The next step is to calculate the covering number of \(\mathcal{S}_{T^{2,1,4}}\). \(\mathcal{S}_{T^{2,1,4}}\) consists of two components: (i) Matrix \(W_{B}\) with orthonormal columns; (ii) Network function \(f_{\mathcal{T}}\).

Suppose we have \(W_{B1},W_{B2}\) and \(f_{1},f_{2}\), such that \(\left\|W_{B1}-W_{B2}\right\|_{F}\leq\delta_{1}\) and \(\sup_{\left\|x\right\|_{2}\leq 3r_{x}+\sqrt{D\log D},t\in[T_{0},T]}\left\|f_{1}(x,t)-f_ {2}(x,t)\right\|_{2}\leq\delta_{2}\), where \(f_{1}=R^{-1}\circ f_{\mathcal{T}1}\circ R,f_{2}=R^{-1}\circ f_{\mathcal{T}2}\circ R\). Then we have

\[\sup_{\left\|x\right\|_{2}\leq 3r_{x}+\sqrt{D\log D},t\in[T_{0},T ]}\left\|s_{W_{B1},f_{\mathcal{T}1}}(\overline{x},t)-s_{W_{B2},f_{\mathcal{T}2 }}(\overline{x},t)\right\|_{2}\] \[=\frac{1}{\sigma(t)}\sup_{\left\|x\right\|_{2}\leq 3r_{x}+\sqrt{D \log D},t\in[T_{0},T]}\left\|W_{B1}f_{1}(W_{B1}^{\top}\overline{x},t)-W_{B2}f_ {2}(W_{B2}^{\top}\overline{x},t)\right\|_{2}\] \[\leq\frac{1}{\sigma(t)}\sup_{\left\|x\right\|_{2}\leq 3r_{x}+\sqrt{D \log D},t\in[T_{0},T]}\left(\left\|W_{B1}f_{1}(W_{B1}^{\top}\overline{x},t)-W_ {B1}f_{1}(W_{B2}^{\top}\overline{x},t)\right\|_{2}\right.\] \[\quad+\left\|W_{B1}f_{1}(W_{B2}^{\top}\overline{x},t)-W_{B1}f_{2} (W_{B2}^{\top}\overline{x},t)\right\|_{2}+\left\|W_{B1}f_{2}(W_{B2}^{\top} \overline{x},t)-W_{B2}f_{2}(W_{B2}^{\top}\overline{x},t)\right\|_{2}\right)\] \[\leq\frac{1}{\sigma(t)}\left(L_{\mathcal{T}}\delta_{1}\sqrt{d_{0 }}(3r_{x}+\sqrt{D\log D})+\delta_{2}+\delta_{1}K\right),\] (F.16)

where \(L_{\mathcal{T}}\) upper bounds the Lipschitz constant of \(f_{\mathcal{T}}\).

For the set \(\{W_{B}\in\mathbb{R}^{D\times d_{0}}:\left\|W_{B}\right\|_{2}\leq 1\}\), its \(\delta_{1}\)-covering number is \(\left(1+2\sqrt{d_{0}}/\delta_{1}\right)^{Dd_{0}}\) [Chen et al., 2020a, Lemma 8]. The \(\delta_{2}\)-covering number of \(f\) needs further discussion as there is a reshaping process in our network. The input is reshaped from \(\overline{h}\in\mathbb{R}^{d_{0}}\) to \(H\in\mathbb{R}^{d\times L}\), and

\[\left\|\overline{h}\right\|_{2}\leq r_{x}\Longleftrightarrow\left\|H\right\|_ {F}\leq r_{x}.\]

Thus we have

\[\sup_{\left\|\overline{h}\right\|_{2}\leq 3r_{x}+\sqrt{D\log D},t\in[T_{0},T]}\left\|f_{1}(\overline{h},t)-f_{2}(\overline{h},t)\right\|_{2}\leq\delta _{2}\] \[\Longleftrightarrow \sup_{\left\|H\right\|_{F}\leq 3r_{x}+\sqrt{D\log D},t\in[T_{0},T]} \left\|f_{\mathcal{T}1}(H)-f_{\mathcal{T}2}(H)\right\|_{2}\leq\delta_{2}.\]

Then we follow the covering number of sequence-to-sequence transformer \(\mathcal{T}^{2,1,4}_{p}\) in Lemma F.6. We get the following \(\delta_{2}\)-covering number

\[\frac{\log(nL)}{\delta_{2}^{2}}\cdot\left(\sum_{i=1}^{K}\alpha_{i}^{\frac{2}{3 }}\left(d^{\frac{2}{3}}\left(C_{F}^{2,\infty}\right)^{\frac{4}{3}}+d^{\frac{2} {3}}\left(2(C_{F})^{2}C_{OV}C_{KQ}^{2,\infty}\right)^{\frac{2}{3}}+\tau m^{ \frac{3}{3}}\left((C_{F})^{2}C_{OV}^{2,\infty}\right)^{\frac{2}{3}}\right) \right)^{3},\]

where

\[\alpha_{i}\coloneqq\prod_{j<i}(C_{F})^{2}C_{OV}(1+4C_{KQ})(C_{X}+C_{E}).\]

According to the (F.4), (F.5), (F.7), (F.8), (F.9), (F.10), (F.12), (F.13), (F.11) and (F.6) in Appendix F.1.2, we derive the following with \(\delta=\mathcal{O}(\epsilon^{2/d})\) (Appendix E.4) and \(d=4\) (Theorem 3.1):

\[K=\mathcal{O}\left(\epsilon^{-2L}\right),L_{\mathcal{T}}= \mathcal{O}\left(d_{0}L_{s_{+}}\right),\;C_{OV}^{2,\infty}=\mathcal{O}(d \epsilon^{-4L}),\;C_{OV}=\mathcal{O}(\epsilon^{-4L}),\] \[C_{KQ}^{2,\infty}=\mathcal{O}(\epsilon^{-4}),\;C_{KQ}=\mathcal{O }(\epsilon^{-4}),\;C_{F}^{2,\infty}=\mathcal{O}(\epsilon^{-4}),\;C_{F}= \mathcal{O}(\epsilon^{-2}),\;C_{E}=\mathcal{O}(L^{3/2}),\] (F.17) \[C_{\mathcal{T}}=\mathcal{O}\left(d_{0}L_{s_{+}}\cdot\sqrt{d_{0} \log(d_{0}/T_{0})+\log(1/\epsilon)}\right),\;r_{x}=\mathcal{O}\left(\sqrt{d_{0} \log d_{0}+\log C_{\mathcal{T}}+\log\bigl{(}n/\overline{\delta}\bigr{)}} \right).\]

Each element of the input data is within \([0,1]\), as shown in Appendix E.

For any \(\delta_{3}>0\), we get the log-covering number of \(\mathcal{T}_{p}^{2,1,4}\),

\[\log\mathcal{N}\left(\delta_{3},\mathcal{T}_{p}^{2,1,4},\|\cdot\|_{ 2}\right) =\mathcal{O}\left(\frac{\epsilon^{-8K}\cdot L^{K}d^{2}\log(nL)}{ \delta_{3}}\right)\] \[=\mathcal{O}(1)\cdot\left(\frac{2^{8K\log(L/\epsilon)}d^{2}\log( nL)}{\delta_{3}}\right).\]

According to (F.15), we adopt the following value for \(\delta_{3}\) in our setting

\[\delta_{3}=\frac{1}{n^{b}(C_{\mathcal{T}}+r_{x})T_{0}\log(T/T_{0})}.\]

According to [Chen et al., 2023, Appendix B.2], the log-covering number of \(\mathcal{S}_{\mathcal{T}_{p}^{2,1,4}}\) is

\[\log\mathcal{N}\left(\delta_{3},\mathcal{S}_{\mathcal{T}_{p}^{2, 1,4}},\|\cdot\|_{2}\right)\] \[=\mathcal{O}\left(2Dd_{0}\cdot\log\left(1+\frac{6C_{\mathcal{T}} L_{\mathcal{T}}\sqrt{d_{0}}(3r_{x}+\sqrt{D\log D})}{T_{0}\delta_{3}}\right)+ \frac{2^{8K\log(L/\epsilon)}d^{2}\log(nL)}{T_{0}^{2}\delta_{3}^{2}}\right)\] (By (F.16)) \[=\mathcal{O}\left(n^{2b}2^{8(1/\epsilon)^{L}\log(L/\epsilon)}Dd^{ 2}d_{0}^{6}L_{s_{+}}^{2}\cdot\log(nL)\right)\] (By (F.17)) \[=\mathcal{O}\left(n^{2b}2^{(1/\epsilon)^{2L}}Dd^{2}d_{0}^{6}L_{s_ {+}}^{2}\cdot\log(nL)\right)\] (By ( \[1/\epsilon\] ) \[=\widetilde{\mathcal{O}}\left(n^{2b}2^{(1/\epsilon)^{2L}}Dd^{2}d_{0 }^{6}L_{s_{+}}^{2}\right)\] (By ignoring the log factors) \[=\widetilde{\mathcal{O}}\left(n^{2b}2^{(1/\epsilon)^{2L}}Dd^{2}d_{0 }^{6}L_{s_{+}}^{2}\right).\]

Substituting the log-covering number into (F.15), we have

\[\frac{1}{T-T_{0}}\int_{T_{0}}^{T}\left\|s_{\widehat{\mathcal{W}} }(\cdot,t)-\nabla\log p_{t}(\cdot)\right\|_{L^{2}(P_{t})}^{2}\mathrm{d}t\] \[=\mathcal{O}\Big{(}\frac{C_{\mathcal{T}}^{2}+r_{x}^{2}}{\epsilon ^{2}nT_{0}T(T-T_{0})}(\log\mathcal{N}(\delta_{3},\mathcal{S}_{\mathcal{T}_{p} ^{2,1,4}},\|\cdot\|_{2})+\log\bigl{(}1/\widetilde{\delta}\bigr{)})+\frac{1}{n^ {b}T_{0}(T-T_{0})}+\frac{d_{0}}{T_{0}(T-T_{0})}\epsilon^{2}\Big{)}\] \[=\mathcal{O}\Big{(}\underbrace{\frac{C_{\mathcal{T}}^{2}+r_{x}^{ 2}}{\epsilon^{2}nT_{0}T}(\log\mathcal{N}(\delta_{3},\mathcal{S}_{\mathcal{T}_{ p}^{2,1,4}},\|\cdot\|_{2})+\log\bigl{(}1/\widetilde{\delta}\bigr{)})}_{1 \text{st term}}+\frac{1}{n^{b}T_{0}T}+\underbrace{\frac{d_{0}}{T_{0}T}\epsilon ^{2}}_{\text{2nd term}}\Big{)}.\] (F.18)

Recall the following parameters:

* \(C_{\mathcal{T}}^{2}=\mathcal{O}(d_{0}^{2}L_{s_{+}}^{2}d_{0}\log(d_{0}/T_{0})+ \log(1/\epsilon))\),
* \(r_{x}^{2}=\mathcal{O}(d_{0}\log d_{0}+\log C_{\mathcal{T}}+\log\bigl{(}n/ \widetilde{\delta}\bigr{)})\),
* \(\widetilde{\delta}\): probability error,
* \(\epsilon\): approximation error,
* \(n\): sample size,
* \(T_{0}<T/2\),
* \(D,d,d_{0}>1\): feature dimension,
* \(L>1\): sequence length,* \(d_{0}=L\cdot d\),
* \(L_{s_{+}}\): Lipschitz coefficient.

Ignoring the \(\log\) factors and \(\mathrm{poly}(D,d,d_{0},L_{S_{+}})\), the first term in (F.18) becomes

\[\frac{1}{n^{1-2b}}\cdot\frac{1}{T_{0}T}\cdot 2^{(1/\epsilon)^{2L}}.\]

The second term is simplified to

\[\frac{1}{T_{0}T}\epsilon^{2}.\]

Thus, the final bound is

\[\widetilde{O}\Bigg{(}\frac{1}{n^{1-2b}}\cdot\frac{1}{T_{0}T}\cdot 2^{(1/ \epsilon)^{2L}}+\frac{1}{n^{b}T_{0}T}+\frac{1}{T_{0}T}\epsilon^{2}\Bigg{)}.\]

To balance the first and second terms with respect to \(n\), we select \(b=1/3\). Therefore, we give the final bound as

\[\widetilde{O}\Bigg{(}\frac{1}{n^{1/3}}\cdot\frac{1}{T_{0}T}\cdot 2^{(1/ \epsilon)^{2L}}+\frac{1}{n^{1/3}T_{0}T}+\frac{1}{T_{0}T}\epsilon^{2}\Bigg{)}.\]

This completes the proof.

### Proof of Corollary 3.2.1

Our proof is built on [Chen et al., 2023, Appendix C]. The main difference between our work and [Chen et al., 2023] is our score estimation error in Theorem 3.2. Consequently, only the subspace error and the total variation distance differ from [Chen et al., 2023, Theorem 3].

First, we introduce the ground truth backward SDE and the learned backward SDE of the latent variable. Recall from (D.2), \(y_{t}\) denotes the backward process. We denote the backward latent variable by \(h_{t}^{\leftarrow}=B^{\top}y_{t}\). Since we write the time index explicitly, we drop the \(\overline{y},\overline{h}\) notation for \(t>0\).

Following [Chen et al., 2023, Appendix C.2], we have the following ground truth backward process

\[\mathrm{d}h_{t}^{\leftarrow}=\left[\frac{1}{2}h_{t}^{\leftarrow}+ \nabla\log p_{T-t}^{h}(h_{t}^{\leftarrow})\right]\mathrm{d}t+\mathrm{d}\big{(} B^{\top}\overline{W}_{t}\big{)}\,,\]

where \(\overline{W}_{t}\) denotes the reversed Wiener process (standard Brownian motion) at time \(t\) (see Section 2 for more details).

We define \(P_{T_{0}}^{h}\) as the _ground truth_ marginal distribution of \(h_{T_{0}}^{\leftarrow}\).

For the learned process \(\widetilde{y}_{t}\), we consider \(\widetilde{h}_{t}^{\leftarrow}=W_{B}^{\top}\widetilde{y}_{t}\). For any orthogonal matrix \(U\in\mathbb{R}^{d_{0}\times d_{0}}\), we define the \(U\) transformed version of \(\widetilde{h}_{t}^{\leftarrow}\) as \(\widetilde{h}_{t}^{\leftarrow,U}=U^{\top}\widetilde{h}_{t}^{\leftarrow}\). Then the backward SDE for \(\widetilde{h}_{t}^{\leftarrow,U}\) is

\[\mathrm{d}\widetilde{h}_{t}^{\leftarrow,U}=\left[\widetilde{h}_{ t}^{\leftarrow,U}+\widehat{s}_{U,f}^{h}(\widetilde{h}_{t}^{\leftarrow,U},T-t) \right]\mathrm{d}t+\mathrm{d}\big{(}U^{\top}W_{B}^{\top}\overline{W}_{t}\big{)}\,,\]

where

\[\widehat{s}_{U,f}^{h}\left(\widetilde{h}_{t}^{\leftarrow,U},t\right)\coloneqq \frac{1}{\sigma(t)}[-\widetilde{h}_{t}^{\leftarrow,U}+U^{\top}f(U\widetilde{h} _{t}^{\leftarrow,U},t)].\]

We define \(\widehat{P}_{T_{0}}^{h}\) as the _estimated_ marginal distribution of \(\widetilde{h}_{T_{0}}^{\leftarrow,U}\) from above continuous SDE.

The discretized backward SDE of \(\widetilde{h}_{T_{0}}^{\leftarrow,U}\) is

\[\mathrm{d}\widetilde{h}_{t}^{\leftarrow,U}=\left[\widetilde{h}_{ k\mu}^{\leftarrow,U}+\widehat{s}_{U,f}^{h}(\widetilde{h}_{k\mu}^{\leftarrow,U},T- k\mu)\right]\mathrm{d}t+\mathrm{d}\big{(}U^{\top}W_{B}^{\top}\overline{W}_{t} \big{)}\,,t\in[k\mu,(k+1)\mu).\]

We define \(\widehat{P}_{T_{0}}^{h,\mathrm{dis}}\) as the _estimated_ marginal distribution of \(\widetilde{h}_{T_{0}}^{\leftarrow,U}\) from above discrete SDE.

Next, we present the auxiliary theoretical results in Appendix F.3.1 to prepare our main proof of Corollary 3.2.1. Then we give a detailed proof of Corollary 3.2.1 in Appendix F.3.2.

#### f.3.1 Auxiliary Lemmas

Here we include a few auxiliary lemmas from [Chen et al., 2023] without proofs. Recall the definition of Lipschitz norm: for a given function \(f\), \(\left\|f(\cdot)\right\|_{Lip}=\sup_{x\neq y}(\left\|f(x)-f(y)\right\|_{2}/ \left\|x-y\right\|_{2})\).

**Lemma F.7** (Lemma 3 of [Chen et al., 2023]).: Assume that the following holds

\[\mathbb{E}_{h\sim P_{h}}\left\|\nabla\log p_{h}(h)\right\|_{2}^{2} \leq C_{sh},\quad\lambda_{\min}\mathbb{E}_{h\sim P_{h}}[hh^{\top}]\geq c_{0}, \quad\mathbb{E}_{h\sim P_{h}}\left\|h\right\|_{2}^{2}\leq C_{h},\]

where \(\lambda_{\min}\) denotes the smallest eigenvalue. We denote

\[\overline{\mathbb{E}}[\phi(\overline{h},t)]=\int_{T_{0}}^{T} \frac{1}{\sigma^{2}(t)}\mathbb{E}_{\overline{x}\sim P_{t}}[\phi(B^{\top} \overline{x},t)]dt.\] (F.19)

Let \(T_{0}\leq\min\{2\log(d_{0}/C_{sh}),1,2\log(c_{0}),c_{0}\}\) and \(T\geq\max\{2\log(C_{h}/d_{0}),1\}\). Suppose we have

\[\overline{\mathbb{E}}\left\|W_{B}f(W_{B}^{\top}\overline{x},t)- Bq(B^{\top}\overline{x},t)\right\|_{2}^{2}\leq\epsilon.\] (F.20)Then we have

\[\left\|W_{B}W_{B}^{\top}-BB^{\top}\right\|_{\mathrm{F}}^{2}=\mathcal{O}(\epsilon T_{ 0}/c_{0}),\]

and there exists an orthogonal matrix \(U\in\mathbb{R}^{d_{0}\times d_{0}}\), such that:

\[\mathbb{E}\big{\|}U^{\top}f(U\overline{h},t)-q(\overline{h},t) \big{\|}_{2}^{2}\] \[=\epsilon\cdot\mathcal{O}\left(1+\frac{T_{0}}{c_{0}}\left[(T-\log T _{0})d_{0}\cdot\max_{t}\left\|f(\cdot,t)\right\|_{\mathrm{Lip}}^{2}+C_{sh} \right]+\frac{\max_{t}\left\|f(\cdot,t)\right\|_{\mathrm{Lip}}^{2}\cdot C_{h} }{c_{0}}\right).\]

**Lemma F.8** (Lemma 4 of [Chen et al., 2023]).: Assume that \(P_{h}\) is sub-Gaussian and that \(f(\overline{h},t)\) and \(\nabla\log p_{t}^{h}(\overline{h})\) are Lipschitz continuous with respect to \(\overline{h}\) and \(t\). For any orthogonal matrix \(U\in\mathbb{R}^{d_{0}\times d_{0}}\), we define

\[\widehat{s}_{U,f}^{h}\left(\overline{h},t\right)\coloneqq\frac{1}{\sigma(t)}[ -\overline{h}+U^{\top}f(U\overline{h},t)].\]

Assume that we have the latent score matching error-bound

\[\int_{T_{0}}^{T}\mathbb{E}_{\overline{h}\sim P_{t}^{h}}\left\| \widehat{s}_{U,f}^{h}\left(\overline{h},t\right)-\nabla\log p_{t}^{h}\left( \overline{h}\right)\right\|_{2}^{2}\,\mathrm{d}t\leq\epsilon_{\mathrm{latent} }\left(T-T_{0}\right),\]

where \(\epsilon_{\mathrm{latent}}>0\). Then we have the following latent distribution estimation error for the continuous backward SDE:

\[\mathrm{TV}\left(P_{T_{0}}^{h},\widehat{P}_{T_{0}}^{h}\right) \lesssim\sqrt{\epsilon_{\mathrm{latent}}\left(T-T_{0}\right)}+\sqrt{\mathrm{ KL}\left(P_{h}\|N\left(0,I_{d_{0}}\right)\right)}\cdot\exp(-T),\]

where \(\widehat{P}_{T_{0}}^{h}\) is the marginal distribution of the generated \(h_{T_{0}}\) using the continuous backward SDE.

Furthermore, let \(\widehat{P}_{T_{0}}^{h,\mathrm{dis}}\) denote the marginal distribution of the generated \(h_{T_{0}}\) using the discretized backward SDE. Then we have the following latent distribution estimation error for the discretized backward SDE

\[\mathrm{TV}\left(P_{T_{0}}^{h},\widehat{P}_{T_{0}}^{h,\mathrm{dis}}\right) \lesssim\sqrt{\epsilon_{\mathrm{latent}}(T-T_{0})}+\sqrt{\mathrm{KL}\left(P_{ h}\|N\left(0,I_{d_{0}}\right)\right)}\cdot\exp(-T)+\sqrt{\epsilon_{\mathrm{dis}}(T-T_{0})},\]

where

\[\epsilon_{\mathrm{dis}}= \left(\frac{\max_{\overline{h}}\left\|f(\overline{h},\cdot) \right\|_{\mathrm{Lip}}}{\sigma\left(T_{0}\right)}+\frac{\max_{\overline{h},t}\left\|f(\overline{h},t)\right\|_{2}}{T_{0}^{2}}\right)^{2}\eta^{2}\] \[+\left(\frac{\max_{t}\left\|f(\cdot,t)\right\|_{\mathrm{Lip}}}{ \sigma\left(T_{0}\right)}\right)^{2}\eta^{2}\max\left\{\mathbb{E}\left\|h_{0} \right\|^{2},d_{0}\right\}+\eta d_{0},\]

and \(\eta\) is the step size in the backward process.

**Lemma F.9** (Lemma 6 of [Chen et al., 2023]).: Consider the following discretized SDE with step size \(\mu\) satisfying \(T-T_{0}=K_{T}\mu\) for some \(K_{T}\in\mathbb{N}_{+}\),

\[\mathrm{d}y_{t}=\left[\frac{1}{2}-\frac{1}{\sigma(T-k\mu)}\right]y_{k\mu} \mathrm{d}t+\mathrm{d}B_{t},\quad\text{for}\quad t\in[k\mu,(k+1)\mu),\]

where \(y_{0}\sim\mathrm{N}(0,I)\). Then, for \(T>1\) and \(T_{0}+\mu\leq 1\), we have \(y_{T-T_{0}}\sim\mathrm{N}\left(0,\sigma^{2}I\right)\) with \(\sigma^{2}\leq e\left(T_{0}+\mu\right)\).

**Lemma F.10** (Lemma 10 in [Chen et al., 2023]).: Assume that \(\nabla\log p_{h}(h)\) is \(L_{h}\)-Lipschitz. Then we have \(\mathbb{E}_{h\sim P_{h}}\left\|\nabla\log p_{h}(h)\right\|_{2}^{2}\leq d_{0}L_{h}\).

#### f.3.2 Main Proof of Corollary 3.2.1

Proof.: Recall the estimation error in Theorem 3.2 is \(\xi(n,\epsilon,L)/(TT_{0})\), where

\[\xi(n,\epsilon,L):=\frac{1}{n^{1/3}}\cdot 2^{(1/\epsilon)^{2L}}+\frac{1}{n^{1/3 }}+\epsilon^{2}.\]

* **Proof of (i).** By the definition of (F.19) and the estimation error in Theorem 3.2, the error bound in (F.20) equals to \(\xi(n,\epsilon,L)(T-T_{0})/(TT_{0})\) in Lemma F.7. By Lemma F.10, we set \(C_{sh}=d_{0}L_{h}\). Then, we have \[\left\|W_{B}W_{B}^{\top}-BB^{\top}\right\|_{F}^{2}=\mathcal{O} \Bigg{(}\frac{\xi(n,\epsilon,L)}{c_{0}}\Bigg{)}.\] By substituting the value of \(\xi(n,\epsilon,L)\) and \(T=\mathcal{O}(\log n)\) into the bound above, we deduce \[\left\|W_{B}W_{B}^{\top}-BB^{\top}\right\|_{F}^{2}=\mathcal{O} \left(\frac{1}{c_{0}n^{1/3}}2^{(1/\epsilon)^{2L}}+\frac{1}{c_{0}n^{1/3}}+ \frac{\epsilon^{2}}{c_{0}}\right).\]
* **Proof of (ii).** Recall that \(\max_{t}\left\|f(\cdot,t)\right\|_{\mathrm{Lip}}\leq L_{\mathcal{T}}\). Furthermore, according to Lemma F.7 and Lemma F.10, we have \[\mathbb{E}\big{\|}U^{\top}f(U\overline{h},t)-q(\overline{h},t) \big{\|}_{2}^{2}=\mathcal{O}(\epsilon_{\mathrm{latent}}(T-T_{0})),\] where \[\epsilon_{\mathrm{latent}}=\frac{\xi(n,\epsilon,L)}{TT_{0}}\cdot \mathcal{O}\left(\frac{T_{0}}{c_{0}}\left[(T-\log T_{0})d_{0}\cdot L_{\mathcal{ T}}^{2}+d_{0}L_{h}\right]+\frac{L_{\mathcal{T}}^{2}\cdot C_{h}}{c_{0}}\right).\] Following the proof structure in [Chen et al., 2023, Appendix C.4], we get \[\mathbb{E}\big{\|}U^{\top}f(U\overline{h},t)-q(\overline{h},t) \big{\|}_{2}^{2} =\int_{T_{0}}^{T}\mathbb{E}_{\overline{h}\sim P_{h}^{h}}\bigg{\|} \frac{U^{\top}f(U\overline{h},t)-\overline{h}}{\sigma(t)}-\nabla\log p_{t}^{h} (\overline{h})\bigg{\|}_{2}^{2}\mathrm{d}t\] \[\leq\epsilon_{\mathrm{latent}}(T-T_{0}).\] Following the proof structure in [Chen et al., 2023, Appendix C.4] and setting \(T=\mathcal{O}(\log n)\), we obtain \[\mathsf{TV}(P_{T_{0}}^{h},\widehat{P}_{T_{0}}^{h,\mathrm{dis}}) =\widetilde{\mathcal{O}}\left(\sqrt{\epsilon_{\mathrm{latent}}(T- T_{0})}\right)\] \[=\widetilde{\mathcal{O}}\left(\sqrt{\left(\frac{1}{n^{1/3}}2^{(1/ \epsilon)^{2L}}+\frac{1}{n^{1/3}}+\epsilon^{2}\right)\cdot\log n}\right),\] where \(\widetilde{\mathcal{O}}\) hides the factor about \(D,d_{0},d,L_{s_{+}},\log n\), and \(T-T_{0}\) By definition, \(\widehat{P}_{T_{0}}^{h,\mathrm{dis}}=(UW_{B})_{\sharp}^{\top}\widehat{P}_{T_{0}}\), where \(\widehat{P}_{T_{0}}\) is the distribution generated by \(s_{\widehat{W}}\) using the discretized backward process. This completes the proof of the total variation distance.
* **Proof of (iii).** We apply Lemma F.9 due to our score decomposition. With the marginal distribution at time \(T-T_{0}\) and observing \(\mu\ll T_{0}\), we obtain the last property.

This completes the proof.

Proofs of Section 4

Our proofs are motivated by the observation of low-rank gradient decomposition in transformer-like models [Alman and Song, 2024b, Gu et al., 2024]. With our simplifications and observations made in Section 4, we utilize the fine-grained complexity results of transformer and attention [Hu et al., 2024b, Alman and Song, 2024a,b] and tensor trick (Lemma D.1 and [Diao et al., 2019, 2018]) to proceed our proofs. Specifically, we approximate DiT training gradients with a series of low-rank approximations in Appendices G.1.1 to G.1.3, and carefully match the multiplication dimensions so that the computation of \(\frac{\mathrm{d}q_{2}}{\mathrm{d}W}\) forms a chained low-rank approximation in Appendix G.2.

### Auxiliary Theoretical Results for Theorem 4.1

Here we present some auxiliary theoretical results to prepare our main proof of the Existence of almost-linear Time Algorithms for ADITGC Theorem 4.1.

#### g.1.1 Low-Rank Decomposition of DiT Gradients

We start by some definitions. Recall that \(W\in\mathbb{R}^{d\times d}\) and \(\underline{W}\in\mathbb{R}^{d^{2}}\) denotes the vectorization of \(W\in\mathbb{R}^{d\times d}\) following Definition D.1.

**Definition G.1**.: Let \(A_{1},A_{2}\in\mathbb{R}^{d\times L}\) be two matrices. Suppose \(\mathsf{A}=A_{1}^{\top}\otimes A_{2}^{\top}\in\mathbb{R}^{L^{2}\times d^{2}}\). Define \(\mathsf{A}_{j_{0}}\in\mathbb{R}^{L\times d^{2}}\) as an \(L\times d^{2}\) sub-block of \(\mathsf{A}\). There are \(L\) such sub-blocks in total. For each \(j_{0}\in[L]\), define the function \(u(\underline{W})_{j_{0}}:\mathbb{R}^{d^{2}}\to\mathbb{R}^{L}\) by \(u(\underline{W})_{j_{0}}:=\exp(\mathsf{A}_{j_{0}}\,\underline{W})\in\mathbb{R }^{L}\).

**Definition G.2**.: Let \(A_{1},A_{2}\in\mathbb{R}^{d\times L}\) be two matrices. Suppose \(\mathsf{A}=A_{1}^{\top}\otimes A_{2}^{\top}\in\mathbb{R}^{L^{2}\times d^{2}}\). Define \(\mathsf{A}_{j_{0}}\in\mathbb{R}^{L\times d^{2}}\) as an \(L\times d^{2}\) sub-block of \(\mathsf{A}\). There are \(L\) such sub-blocks in total. For every index \(j_{0}\in[L]\), consider the function \(\alpha(\underline{W})_{j_{0}}:\mathbb{R}^{d^{2}}\to\mathbb{R}\) defined by \(\alpha(\underline{W})_{j_{0}}:=\langle\underbrace{\exp(\mathsf{A}_{j_{0}}\, \underline{W})}_{L\times 1},\underbrace{\mathds{1}_{L}}_{L\times 1}\rangle\).

**Definition G.3**.: Suppose that \(\alpha(\underline{W})_{j_{0}}\in\mathbb{R}\) and \(u(\underline{W})_{j_{0}}\in\mathbb{R}^{L}\) are defined as in Definitions G.1 and G.2, respectively. For a fixed \(j_{0}\in[L]\), consider the function \(f(\underline{W})_{j_{0}}:\mathbb{R}^{d^{2}}\to\mathbb{R}^{L}\) defined by

\[f(\underline{W})_{j_{0}}:=\underbrace{\alpha(\underline{W})_{j_{0}}^{-1}}_{ \text{scalar}}\underbrace{u(\underline{W})_{j_{0}}}_{L\times 1}.\]

Define \(f(\underline{W})\in\mathbb{R}^{L\times L}\) as the matrix where the \(j_{0}\)-th row is \((f(\underline{W})_{j_{0}})^{\top}\).

**Definition G.4**.: For every \(i_{0}\in[d]\), define the function \(h(\underline{W}_{OV})_{i_{0}}:\mathbb{R}^{d^{2}}\to\mathbb{R}^{L}\) by

\[h(\underline{W}_{OV})_{i_{0}}:=\underbrace{A_{1}^{\top}}_{L\times d}( \underbrace{W_{OV}^{\top}}_{d\times 1})_{*,i_{0}}.\]

Here, \(W_{OV}\in\mathbb{R}^{d\times d}\) denotes the matrix representation of \(\underline{W}_{OV}\in\mathbb{R}^{d^{2}}\), and \((W_{OV})_{*,i_{0}}^{\top}\) represents the \(i_{0}\)-th column of \(W_{OV}^{\top}\). Define \(h(\underline{W}_{OV})\in\mathbb{R}^{L\times d}\) as the matrix where the \(i_{0}\)-th column is \(h(\underline{W}_{OV})_{i_{0}}\).

**Definition G.5**.: For each \(j_{0}\in[L]\), we denote \(f(\underline{W})_{j_{0}}\in\mathbb{R}^{L}\) as the normalized vector defined by Definition G.3. For each \(i_{0}\in[d]\), \(h(\underline{W}_{OV})_{i_{0}}\) is defined as per Definition G.4. For every pair \((j_{0},i_{0})\in[L]\times[d]\), define the function \(c(\underline{W})_{j_{0},i_{0}}:\mathbb{R}^{d^{2}}\times\mathbb{R}^{d^{2}}\to \mathbb{R}\) by

\[c(\underline{W})_{j_{0},i_{0}}:=\langle f(\underline{W})_{j_{0}},h(\underline{ W}_{OV})_{i_{0}}\rangle-Y_{j_{0},i_{0}}^{\top},\]

[MISSING_PAGE_EMPTY:56]

[MISSING_PAGE_EMPTY:57]

[MISSING_PAGE_EMPTY:58]

Proof of Lemma g.4.: 

**Lemma G.5** (Approximate \(q(\cdot)\)).: Let \(k_{2}=L^{o(1)}\), \(c(\cdot)\in\mathbb{R}^{L\times d}\) follow Definition G.5 and let \(q(W):=c(W)h(W_{OV})^{\mathsf{T}}\in\mathbb{R}^{L\times L}\) (follow Definition G.6). There exist two matrices \(U_{2},V_{2}\in\mathbb{R}^{L\times k_{2}}\) such that \(\left\|U_{2}V_{2}^{\mathsf{T}}-q(\underline{W})\right\|_{\max}\leq\epsilon/ \mathrm{poly}(L)\). In addition, it takes \(L^{1+o(1)}\) time to construct \(U_{2},V_{2}\).

Proof of Lemma g.5.: Our proof is built on [1, Lemma D.3].

Let \(\widetilde{q}(\cdot)\) denote an approximation to \(q(\cdot)\).

By Lemma G.4, \(U_{1}V_{1}^{\mathsf{T}}h(W_{OV})-Y\) approximates \(c(\underline{W})\) up to accuracy \(\epsilon=1/\mathrm{poly}(L)\).

Thus, by setting \(\widetilde{q}(\underline{W})=h(W_{OV})\left(U_{1}V_{1}^{\mathsf{T}}h(W_{OV}) -Y\right)^{\top}\), we find a low-rank form for \(\widetilde{q}(\cdot)\):

\[\widetilde{q}(\underline{W})=h(W_{OV})\left(h(W_{OV})\right)^{\top}V_{1}U_{1} ^{\mathsf{T}}-h(W_{OV})Y^{\mathsf{T}},\]

such that

\[\left\|\widetilde{q}(\underline{W})-q(\underline{W})\right\|_{ \max} =\left\|h(W_{OV})\left(U_{1}V_{1}^{\mathsf{T}}h(W_{OV})-Y\right)^{ \mathsf{T}}-h(W_{OV})Y^{\mathsf{T}}\right\|_{\max}\] \[\leq d\left\|h(W_{OV})\right\|_{\max}\left\|U_{1}V_{1}^{\mathsf{T }}h(W_{OV})-Y-c(\underline{W})\right\|_{\max}\] \[\leq\epsilon/\mathrm{poly}(L).\]

By \(k_{1},d=L^{o(1)}\), compute \(\underbrace{\left(h(W_{OV})\right)^{\mathsf{T}}}_{d\times L}\underbrace{V_{1 }}_{L\times k_{1}}\underbrace{U_{1}^{\mathsf{T}}}_{k_{1}\times L}\) takes only \(L^{1+o(1)}\) time. This completes the proof.

#### g.1.3 Low-Rank Approximations of Building Blocks Part II: \(p(\cdot)\)

Now, we use the low-rank approximations of \(f,q,c\) to construct low-rank approximations for \(p_{1}(\cdot),p_{2}(\cdot),p(\cdot)\).

**Lemma G.6** (Approximate \(p_{1}(\cdot)\)).: Let \(k_{1},k_{2}=L^{o(1)}\). Suppose \(U_{1},V_{1}\in\mathbb{R}^{L\times k_{1}}\) approximates \(f(\underline{W})\in\mathbb{R}^{L\times L}\) such that \(\left\|U_{1}V_{1}^{\mathsf{T}}-f(\underline{W})\right\|_{\max}\leq\epsilon/ \mathrm{poly}(L)\), and \(U_{2},V_{2}\in\mathbb{R}^{L\times k_{2}}\) approximates the \(q(\underline{W})\in\mathbb{R}^{L\times L}\) such that \(\left\|U_{2}V_{2}^{\mathsf{T}}-q(\underline{W})\right\|_{\max}\leq\epsilon/ \mathrm{poly}(L)\). Then there exist two matrices \(U_{3},V_{3}\in\mathbb{R}^{L\times k_{3}}\) such that \(\left\|U_{3}V_{3}^{\mathsf{T}}-p_{1}(\underline{W})\right\|_{\max}\leq\epsilon/ \mathrm{poly}(L)\). In addition, it takes \(L^{1+o(1)}\) time to construct \(U_{3},V_{3}\).

Proof of Lemma g.6.: By tensor trick, we construct \(U_{3}\), \(V_{3}\) as tensor products of \(U_{1},V_{1}\) and \(U_{2},V_{2}\), respectively, while preserving their low-rank structures. Then, we show the low-rank approximation of \(p_{1}(\cdot)\) with bounded error by Lemma G.3 and Lemma G.5.

Let \(\oslash\) be _column-wise_ Kronecker product such that \(A\oslash B\coloneqq[A[\cdot,1]\otimes B[\cdot,1]\mid\dots\mid A[\cdot,k_{1}] \otimes B[\cdot,k_{1}]]\in\mathbb{R}^{L\times k_{1}k_{2}}\) for \(A\in\mathbb{R}^{L\times k_{1}},B\in\mathbb{R}^{L\times k_{2}}\).

Let \(\widetilde{f}(\underline{W})\coloneqq U_{1}V_{1}^{\mathsf{T}}\) and \(\widetilde{q}(\underline{W})\coloneqq U_{2}V_{2}^{\mathsf{T}}\) denote matrix-multiplication approximations to \(f(\underline{W})\) and \(q(\underline{W})\), respectively.

For the case of presentation, let \(U_{3}=\overbrace{U_{1}}^{L\times k_{1}}\oslash\overbrace{U_{2}}^{L\times k_{2}}\) and \(V_{3}=\overbrace{V_{1}}^{L\times k_{1}}\oslash\overbrace{V_{2}}^{L\times k_{2}}\). It holds

\[\big{\|}U_{3}V_{3}^{\top}-p_{1}(\underline{W})\big{\|}_{\max}\] \[=\big{\|}U_{3}V_{3}^{\top}-f(\underline{W})\odot q(\underline{W}) \big{\|}_{\max}\] \[=\big{\|}\big{(}U_{1}\oslash U_{2})\left(V_{1}\oslash V_{2}\right)^ {\top}-f(\underline{W})\odot q(\underline{W})\big{\|}_{\max}\] \[=\big{\|}\big{(}U_{1}V_{1}^{\top}\big{)}\odot\big{(}U_{2}V_{2}^{ \top}\big{)}-f(\underline{W})\odot q(\underline{W})\big{\|}_{\max}\] \[=\big{\|}\widetilde{f}(\underline{W})\odot\widetilde{q}(\underline {W})-f(\underline{W})\odot q(\underline{W})\big{\|}_{\max}\] \[\leq\underbrace{\big{\|}\widetilde{f}(\underline{W})\odot \widetilde{q}(\underline{W})-\widetilde{f}(\underline{W})\odot q(\underline{W })\big{\|}_{\max}}_{\leq\epsilon/\mathrm{poly}(L)}+\underbrace{\big{\|} \widetilde{f}(\underline{W})\odot q(\underline{W})-f(\underline{W})\odot q( \underline{W})\big{\|}_{\max}}_{\leq\epsilon/\mathrm{poly}(L)}\] \[\leq\epsilon/\mathrm{poly}(L).\] (By Lemma G.3 and Lemma G.5 )

Computationally, by \(k_{1},k_{2}=L^{o(1)}\), computing \(U_{3}\) and \(V_{3}\) takes \(L^{1+o(1)}\) time. This completes the proof. 

**Lemma G.7** (Approximate \(p_{2}(\cdot)\)).: Let \(k_{1},k_{2},k_{4}=L^{o(1)}\). Let \(p_{2}(\underline{W})\in\mathbb{R}^{L\times L}\) follow Definition G.7 such that its \(j_{0}\)-th column is \(p_{2}(\underline{W})_{j_{0}}=f(\underline{W})_{j_{0}}f(\underline{W})_{j_{0}}^ {\top}q(\underline{W})_{j_{0}}\) for each \(j_{0}\in[L]\). Suppose \(U_{1},V_{1}\in\mathbb{R}^{L\times k_{1}}\) approximates the \(\overline{\mathrm{f}}(\underline{X})\) such that \(\big{\|}U_{1}V_{1}^{\top}-f(\underline{W})\big{\|}_{\max}\leq\epsilon/\mathrm{ poly}(L)\), and \(U_{2},V_{2}\in\mathbb{R}^{L\times k_{2}}\) approximates the \(q(\underline{W})\in\mathbb{R}^{L\times L}\) such that \(\big{\|}U_{2}V_{2}^{\top}-q(\underline{W})\big{\|}_{\max}\leq\epsilon/\mathrm{ poly}(L)\). Then there exist matrices \(U_{4},V_{4}\in\mathbb{R}^{L\times k_{4}}\) such that \(\big{\|}U_{4}V_{4}^{\top}-p_{2}(\big{)}\big{\|}_{\max}\leq\epsilon/\mathrm{ poly}(L)\). In addition, it takes \(L^{1+o(1)}\) time to construct \(U_{4},V_{4}\).

Proof of Lemma g.7.: From Definition G.7,

\[p_{2}(\underline{W})_{j_{0}}\coloneqq\overbrace{f(\underline{W})_{j_{0}}}^{ \left(\underline{H}\right)}\underbrace{\big{(}\underline{H}\big{)}_{j_{0}}^ {\top}q(\underline{W})_{j_{0}}}_{(I)}.\]

For (I), we show its low-rank approximation by observing the low-rank-preserving property of the multiplication between \(f(\cdot)\) and \(q(\cdot)\) (from Lemma G.3 and Lemma G.5). For (II), we show its low-rank approximation by the low-rank structure of \(f(\cdot)\) and (I).

Part (I).We define a function \(r(\underline{W}):\mathbb{R}^{d^{2}}\to\mathbb{R}^{L}\) such that the \(j_{0}\)-th component \(r(\underline{W})_{j_{0}}\coloneqq\left(f(\underline{W})_{j_{0}}\right)^{\top}q (\underline{W})_{j_{0}}\) for all \(j_{0}\in[L]\). Let \(\widetilde{r}(\underline{W})\) denote the approximation of \(r(\underline{W})\) via decomposing into \(f(\cdot)\) and \(q(\cdot)\):

\[\widetilde{r}(\underline{W})_{j_{0}} \coloneqq\left\langle\widetilde{f}(\underline{W})_{j_{0}}, \widetilde{q}(\underline{W})_{j_{0}}\right\rangle=\left(U_{1}V_{1}^{\top} \right)\left[j_{0},\cdot\right]\cdot\left[\left(U_{2}V_{2}^{\top}\right) \left[j_{0},\cdot\right]\right]^{\top}\] \[=U_{1}[j_{0},\cdot]\underbrace{V_{1}^{\top}}_{k_{1}\times L} \underbrace{V_{2}}_{L\times k_{2}}\left(U_{2}[j_{0},\cdot]\right)^{\top},\] (G.6)

for all \(j_{0}\in[L]\). This allows us to write \(p_{2}(\underline{W})=f(\underline{W})\operatorname{diag}(r(\underline{W}))\) with \(\operatorname{diag}(\widetilde{r}(\underline{W}))\) denoting a diagonal matrix with diagonal entries being components of \(\widetilde{r}(\underline{W})\).

Part (II).With \(r(\cdot)\), we approximate \(p_{2}(\cdot)\) with \(\widetilde{p}_{2}(\underline{W})=\widetilde{f}(\underline{W})\operatorname{diag} (\widetilde{r}(\underline{W}))\) as follows.

Since \(\widetilde{f}(\underline{W})\) has low rank representation, and \(\operatorname{diag}(\widetilde{r}(\underline{W}))\) is a diagonal matrix, \(\widetilde{p}_{2}(\cdot)\) has low-rank representation by definition. Thus, we set \(\widetilde{p}_{2}(\underline{W})=U_{4}V_{4}^{\top}\) with \(U_{4}=U_{1}\) and \(V_{4}=\operatorname{diag}(\widetilde{r}(\underline{W}))V_{1}\). Then, we bound the approximation error

\[\big{\|}U_{4}V_{4}^{\top}-p_{2}(\underline{W})\big{\|}_{\max}\]

[MISSING_PAGE_EMPTY:61]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our contributions and scope in Section 3 and Section 4 are reflected by the claims in abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations in Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Yes. We include our proofs in the appendix and have made every effort to ensure the correctness of our theoretical results.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: This is a formal theory work without experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: This is a formal theory work without experiments. Guidelines:

* The answer NA means that paper does not include experiments requiring code.

* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: This is a formal theory work without experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.

7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: This is a formal theory work without experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: This is a formal theory work without experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Yes. We follow the code of ethics in this work. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: This theoretical work aims to shed light on the foundations of diffusion generative models and is not anticipated to have negative social impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ** If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This is a formal theory work without experiments. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This is a formal theory work without experiments. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This is a formal theory work without experiments. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used.

* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This is a formal theory work without experiments. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This is a formal theory work without experiments. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.