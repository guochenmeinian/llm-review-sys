# Reverse Transition Kernel: A Flexible Framework to Accelerate Diffusion Inference

Xunpeng Huang

HKUST

xhuangck@connect.ust.hk

&Difan Zou

HKU

dzou@cs.hku.hk

&Hanze Dong

Salesforce AI Research

hanze.dong@salesforce.com

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yian Ma

UC San Diego

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yian Ma

UC San Diego

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yian Ma

UC San Diego

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yian Ma

UC San Diego

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yian Ma

UC San Diego

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yian Ma

UC San Diego

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

UXi Zhang

yianma@ucsd.edu

&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

UXi Zhang

UXi Zhang

HKU

yianma@ucsd.edu
&Tong Zhang

UIUC

tongzhang@tongzhang-ml.org

&Yi Zhang

HKU

yizhang101@connect.hku.hk

&Yi Zhang

[MISSING_PAGE_POST]

a series of Gaussian transitions, DDPM successfully generates high-quality samples that follow the data distribution. The empirical success of DDPM has immediately triggered various follow-up work [33, 25], aiming to accelerate the inference process and improve the generation quality. Alongside rapid empirical research on diffusion models and DDPM-like sampling algorithms [19, 39], theoretical studies have emerged to analyze the convergence and sampling error of DDPM. In particular, [21, 24, 8, 7, 3, 9] have established polynomial convergence bounds, in terms of dimension \(d\) and target sampling error \(\epsilon\), for the generation process under various assumptions. Previous work usually assume the score estimation error to construct the sampling analysis [8, 15, 3, 19]. A typical bound under minimal data assumptions on the score of the data distribution is provided by [8, 3], which establishes an \(\tilde{\mathcal{O}}(d\epsilon^{-2})\) score estimation guarantees to sample from data distribution within \(\epsilon\)-sampling error in the Total Variation (TV) distance.

In essence, the denoising diffusion process can be approached through various decompositions of sampling subproblems, where the overall complexity depends on the number of these subproblems multiplied by the complexity of solving each one. Within this framework, DDPM can be regarded as a specific solver for the denoising diffusion process that heavily prioritizes the simplicity of subproblems over their quantity. In particular, it adopts simple one-step Gaussian approximations for the subproblems, with \(\mathcal{O}(1)\) computation complexity, but needs to deal with a relatively large number--approximately \(O(d\epsilon^{-2})\)--of target subproblems to ensure the cumulative sampling error is bounded by \(\epsilon\) in TV distance. This imbalance raises the question of whether the DDPM-like approaches stand as the most efficient algorithm, considering the extensive potential subproblem decompositions of the denoising diffusion process. We therefore aim to:

_accelerate the inference of diffusion models via a more balanced subproblem decomposition in the denoising process._

In this work, we propose a novel framework called reverse transition kernel (RTK) to achieve exactly that. Our approach considers a generalized subproblem decomposition of the denoising process, where the difficulty of each sampling subproblem and the total number of subproblems are determined by the step size parameter \(\eta\). Unlike DDPM, which requires setting \(\eta=\epsilon^{2}\), resulting in approximately \(\tilde{\mathcal{O}}(1/\eta)=\tilde{\mathcal{O}}(1/\epsilon^{2})\) subproblems, our framework allows \(\eta\) to be feasible in a broader range. Furthermore, we demonstrate that a more balanced subproblem decomposition can be attained by carefully selecting \(\eta=\Theta(1)\) as a constant, resulting in approximately \(\tilde{\mathcal{O}}(1)\) sampling subproblems, with each target distribution being strongly log-concave. This nice property further enables us to efficiently solve the sampling subproblems using well-established acceleration techniques, such as Metropolis Hasting step and underdamped discretization, without encountering many subproblems. Consequently, our proposed framework facilitates the design of provably faster algorithms than DDPM for performing diffusion inference. Our contributions can be summarized as follows.

* We propose a flexible framework that enhances the efficiency of diffusion inference by balancing the quantity and hardness of RTK sampling subproblems used to segment the entire denoising diffusion process. Specifically, we demonstrate that with a carefully designed decomposition, the number of sampling subproblems can be reduced to approximately \(\tilde{\mathcal{O}}(1)\), while ensuring that all RTK targets exhibit strong log-concavity. This capability allows us to seamlessly integrate a range of well-established sampling acceleration techniques, thereby enabling highly efficient algorithms for diffusion inference.
* Building upon the developed framework, we implement the RTK using the Metropolis-Adjusted Langevin Algorithm (MALA), making it the first attempt to adapt this highly accurate sampler for diffusion inference. Under slightly stricter assumptions on the estimation errors of the energy difference and score function, we demonstrate that RTK-MALA can achieve linear convergence with respect to the sampling error \(\epsilon\), specifically \(\mathcal{O}(\log(1/\epsilon))\), which significantly outperforms the \(\tilde{\mathcal{O}}(1/\epsilon^{2})\) convergence rate of DDPM [8, 3]. Additionally, we consider the practical diffusion model where only the score function is accessible and develop a score-only RTK-MALA algorithm. We further prove that the score-only RTK-MALA algorithm can achieve an error \(\epsilon\) with a complexity of \(\tilde{\mathcal{O}}(\epsilon^{-2/(u-1)}\cdot 2^{u})\), where \(u\) can be an arbitrarily large constant, provided the energy function satisfies the \(u\)-th order smoothness condition.
* We further implement Underdamped Langevin Dynamics (ULD) within the RTK framework. The resulting RTK-ULD algorithm achieves a state-of-the-art complexity of \(\tilde{\mathcal{O}}(d^{1/2}\epsilon^{-1})\) for both and \(\epsilon\) dependence under some mild assumptions as [13]. Compared with the \(\tilde{\mathcal{O}}(d\epsilon^{-2})\) complexity guarantee for DDPM, it improves the complexity with an \(\tilde{\mathcal{O}}(d^{1/2}\epsilon^{-1})\) factor. This result also matches the state-of-the-art convergence rate of the ODE-based methods [9], though those methods require Lipschitz conditions for both the ground truth score function and the score neural network.

## 2 Preliminaries

In this section, we first introduce the notations used in subsequent sections. Then, we present several distinct Markov processes to demonstrate the procedures for adding noise to existing data and generating new data. Besides, we specify the assumptions required for the target distribution in our algorithms and analysis.

**Notations.** We say a complexity \(h\colon\mathbb{R}\to\mathbb{R}\) to be \(h(n)=\mathcal{O}(n^{k})\) or \(h(n)=\tilde{\mathcal{O}}(n^{k})\) if the complexity satisfies \(h(n)\leq c\cdot n^{k}\) or \(h(n)\leq c\cdot n^{k}[\log(n)]^{k^{\prime}}\) for absolute contant \(c,k\) and \(k^{\prime}\). We use the lowercase bold symbol \(\mathbf{x}\) to denote a random vector, and the lowercase italicized bold symbol \(\mathbf{x}\) represents a fixed vector. The standard Euclidean norm is denoted by \(\|\cdot\|\). The data distribution is presented as \(p_{*}\propto\exp(-f_{*})\). Besides, we define two Markov processes \(\mathbb{R}^{d}\), i.e.,

\[\left\{\mathbf{x}_{t}\right\}_{t\in[0,T]},\quad\left\{\mathbf{x}_{k\eta}^{ \leftarrow}\right\}_{k\in\{0,1,\ldots,K\}},\quad\mathrm{where}\quad T=K\eta.\]

In the above notations, \(T\) presents the mixing time required for the data distribution to converge to specific priors, \(K\) denotes the iteration number of the generation process, and \(\eta\) signifies the corresponding step size. Further details of the two processes are provided below.

**Adding noise to data with the forward process.** The first Markov process \(\{\mathbf{x}_{t}\}\) corresponds to generating progressively noised data from \(p_{*}\). In most denoising diffusion models, \(\{\mathbf{x}_{t}\}\) is an Ornstein-Uhlenbeck (OU) process shown as follows

\[\mathrm{d}\mathbf{x}_{t}=-\mathbf{x}_{t}\mathrm{d}t+\sqrt{2}\mathrm{d}\mathbf{B}_ {t}\quad\mathrm{where}\quad\mathbf{x}_{0}\sim p_{*}\propto\exp(-f_{*}). \tag{1}\]

If we denote underlying distribution of \(\mathbf{x}_{t}\) as \(p_{t}\propto\exp(-f_{t})\) meaning \(f_{0}=f_{*}\), the forward OU process provides an analytic form of the transition kernel, i.e.,

\[p_{t^{\prime}|t}(\mathbf{x}^{\prime}|\mathbf{x})=\frac{p_{t^{\prime},t}(\mathbf{x}^{ \prime},\mathbf{x})}{p_{t}(\mathbf{x})}=\left(2\pi\left(1-e^{-2(t^{\prime}-t)}\right) \right)^{-d/2}\cdot\exp\left[\frac{-\left\|\mathbf{x}^{\prime}-e^{-(t^{\prime}-t )}\mathbf{x}\right\|^{2}}{2\left(1-e^{-2(t^{\prime}-t)}\right)}\right] \tag{2}\]

for any \(t^{\prime}\geq t\), where \(p_{t^{\prime},t}\) denotes the joint distribution of \((\mathbf{x}_{t^{\prime}},\mathbf{x}_{t})\). According to the Fokker-Planck equation, we know the stationary distribution for SDE. (1) is the standard Gaussian distribution.

**Denoising generation with a reverse SDE.** Various theoretical works [21; 24; 8; 7; 3] based on DDPM [17] consider the generation process of diffusion models as the reverse process of SDE. (1) denoted as \(\{\mathbf{x}_{t}^{\leftarrow}\}\). According to the Doob's \(h\)-Transform, the reverse SDE, i.e., \(\{\mathbf{x}_{t}^{\leftarrow}\}\), follows from

\[\mathrm{d}\mathbf{x}_{t}^{\leftarrow}=(\mathbf{x}_{t}^{\leftarrow}+2\nabla \ln p_{T-t}(\mathbf{x}_{t}^{\leftarrow}))\,\mathrm{d}t+\sqrt{2}\mathrm{d} \mathbf{B}_{t}, \tag{3}\]

whose underlying distribution \(p_{t}^{\leftarrow}\) satisfies \(p_{T-t}=p_{t}^{\leftarrow}\). Similar to the definition of transition kernel shown in Eq. (2), we define \(p_{t^{\prime}|t}^{\leftarrow}(\mathbf{x}^{\prime}|\mathbf{x})=p_{t^{\prime},t}^{ \leftarrow}(\mathbf{x}^{\prime},\mathbf{x})/p_{t}^{\leftarrow}(\mathbf{x})\) for any \(t^{\prime}\geq t\geq 0\) and name it as reverse transition kernel (RTK).

To implement SDE. (3), diffusion models approximate the score function \(\nabla\ln p_{t}\) with a parameterized neural network model, denoted by \(\mathbf{s}_{\mathbf{\theta},t}\), where \(\mathbf{\theta}\) denotes the network parameters. Then, SDE. (3) can be practically implemented by

\[\mathrm{d}\overline{\mathbf{x}}_{t}=(\overline{\mathbf{x}}_{t}+2\mathbf{s}_{\theta,T-k\eta}(\overline{\mathbf{x}}_{k\eta}))\,\mathrm{d}t+\sqrt{2}\mathrm{d}\mathbf{B }_{t}\quad\mathrm{for}\quad t\in[k\eta,(k+1)\eta) \tag{4}\]

with a standard Gaussian initialization, \(\overline{\mathbf{x}}_{0}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\). Eq. (4) has the following closed solution

\[\overline{\mathbf{x}}_{(k+1)\eta}=e^{\eta}\cdot\overline{\mathbf{x}}_{k\eta}-2( 1-e^{\eta})\mathbf{s}_{\theta,T-k\eta}(\overline{\mathbf{x}}_{k\eta})+\sqrt{e^{2 \eta}-1}\cdot\xi\quad\mathrm{and}\quad\xi\sim\mathcal{N}(\mathbf{0},\mathbf{I}), \tag{5}\]

which is exactly the DDPM algorithm.

[MISSING_PAGE_FAIL:4]

generation process of diffusion models with underlying distributions \(\{\hat{p}_{k\eta}\}\), we require \(\hat{p}_{0}=\mathcal{N}(\mathbf{0},\mathbf{I})\) and \(\hat{p}_{K\eta}\approx p_{*}\), which is similar to the Markov process \(\{\mathbf{x}_{k\eta}^{\leftarrow}\}\). In order to make the underlying distribution of output particles close to the data distribution, we can generate \(\hat{\mathbf{x}}_{k\eta}\) with Alg. 1, which is equivalent to the following steps:

* Initialize \(\hat{\mathbf{x}}_{0}\) with an easy-to-sample distribution, e.g., \(\mathcal{N}(\mathbf{0},\mathbf{I})\), which is close to \(p_{K\eta}\).
* Update particles by drawing samples from \(\hat{p}_{(k+1)\eta|k\eta}(\cdot|\hat{\mathbf{x}}_{k\eta})\), which satisfies \[\hat{p}_{(k+1)\eta|k\eta}(\cdot|\hat{\mathbf{x}}_{k\eta})\approx p_{(k+1)\eta| k\eta}^{\leftarrow}(\cdot|\hat{\mathbf{x}}_{k\eta}).\] Under these conditions, if \(\hat{p}_{k\eta}(\mathbf{z})\approx p_{(K-k)\eta}(\mathbf{z})\), then we have \[\hat{p}_{(k+1)\eta}(\mathbf{z})=\left\langle\hat{p}_{(k+1)\eta|k\eta}(\mathbf{z}|\cdot ),\hat{p}_{k\eta}(\cdot)\right\rangle\approx\left\langle p_{(k+1)\eta|k\eta}^{ \leftarrow}(\mathbf{z}|\cdot),p_{k\eta}^{\leftarrow}(\cdot)\right\rangle=p_{(k+1) \eta}(\mathbf{z})\] for any \(k\in\{0,1,\ldots,K\}\). This means we can implement the generation of diffusion models by solving a series of sampling subproblems with target distributions \(p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot|\hat{\mathbf{x}}_{k\eta})\).

```
1:Input: Initial particle \(\hat{\mathbf{x}}_{0}\) sampled from the standard Gaussian distribution, Iteration number \(K\), Step size \(\eta\), required convergence accuracy \(\epsilon\);
2:for\(k=0\) to \(K-1\)do
3: Draw sample \(\hat{\mathbf{x}}_{(k+1)\eta}\) with MCMCs from \(\hat{p}_{(k+1)\eta|k\eta}(\cdot|\hat{\mathbf{x}}_{k\eta})\) which approximates the ground-truth reverse transition kernel, i.e., \[p_{(k+1)\eta|k\eta}^{\leftarrow}(\mathbf{z}|\hat{\mathbf{x}}_{k\eta})\propto\exp \left(-g(\mathbf{z})\right)\coloneqq\exp\left(-f_{(K-k-1)\eta}(\mathbf{z})-\frac{\| \hat{\mathbf{x}}_{k\eta}-\mathbf{z}\cdot e^{-\eta}\|^{2}}{2(1-e^{-2\eta})}\right).\]
4:return\(\hat{\mathbf{x}}_{K}\).
```

**Algorithm 1** Inference with Reverse Transition Kernel (RTK)

The close form of reverse transition kernels. To implement Alg. 1, the most critical problem is determining the analytic form of RTK \(p_{t^{\prime}|t}^{\leftarrow}(\mathbf{x}^{\prime}|\mathbf{x})\) for and \(t^{\prime}\geq t\geq 0\) which is shown in the following lemma whose proof is deferred to Appendix B.

**Lemma 3.1**.: _Suppose a Markov process \(\{\mathbf{x}_{t}\}\) with SDE. 1, then for any \(t^{\prime}>t\), we have_

\[p_{T-t|T-t^{\prime}}^{\leftarrow}(\mathbf{x}|\mathbf{x}^{\prime})=p_{t|t^{\prime}}(\mathbf{ x}|\mathbf{x}^{\prime})\propto\exp\left(-f_{t}(\mathbf{x})-\frac{\left\|\mathbf{x}^{ \prime}-\mathbf{x}\cdot e^{-(t^{\prime}-t)}\right\|^{2}}{2(1-e^{-2(t^{\prime}-t) })}\right).\]

The first critical property shown in this Lemma is that RTK \(p_{t|t^{\prime}}\) is a perturbation of \(p_{t}\) with a \(l_{2}\) regularization. This means if the score of \(p_{t}\), i.e., \(\nabla f_{t}\), can be well-estimated, the score of RTK, i.e., \(\nabla\log p_{t|t^{\prime}}\) can also be approximated with high accuracy. Moreover, in the diffusion model, \(\nabla f_{t}=\nabla\log p_{t}\) is exactly the score function at time \(t\), which is approximated by the score network function \(\mathbf{s}_{\theta,t}(\mathbf{x})\), then

\[-\nabla\log p_{t|t^{\prime}}(\mathbf{x}|\mathbf{x}^{\prime})=\nabla f_{t}(\mathbf{x})+ \frac{e^{-2(t^{\prime}-t)}\mathbf{x}-e^{-(t^{\prime}-t)}\mathbf{x}^{\prime}}{1-e^{-2(t ^{\prime}-t)}}\approx\mathbf{s}_{\theta,t}(\mathbf{x})+\frac{e^{-2(t^{\prime}-t)}\mathbf{x }-e^{-(t^{\prime}-t)}\mathbf{x}^{\prime}}{1-e^{-2(t^{\prime}-t)}},\]

which can be directly calculated with a single query of \(\mathbf{s}_{\theta,t}(\mathbf{x})\). The second critical property of RTK is that we can control the spectral information of its score by tuning the gap between \(t^{\prime}\) and \(t\). Specifically, considering the target distribution, i.e., \(p_{(K-k-1)\eta|(K-k)\eta}\) for the \(k\)-th transition, the Hessian matrix of its energy function satisfies

\[-\nabla^{2}\log p_{(K-k-1)\eta|(K-k)\eta}=\nabla^{2}f_{(K-k-1)\eta}(\mathbf{x})+ \frac{e^{-2\eta}}{1-e^{-2\eta}}\cdot\mathbf{I}.\]

According to Assumption **[A1]**, the Hessian \(\nabla^{2}f_{(K-k-1)\eta}(\mathbf{x})=-\nabla^{2}\log p_{(K-k-1)\eta}\) can be lower bounded by \(-L\mathbf{I}\), which implies that RTK \(p_{(K-k-1)\eta|(K-k)\eta}\) will be \(L\)-strongly log-concave and \(3L\)-smooth when the step size is set \(\eta=1/2\cdot\log(1+1/2L)\). This further implies that the targetsof all subsampling problems in Alg. 1 will be strongly log-concave, which can be sampled very efficiently by various posterior sampling algorithms.

**Sufficient conditions for the convergence.** According to Pinsker's inequality and Eq. (7), we can obtain the following lemma that establishes the general error decomposition for Alg. 1.

**Lemma 3.2**.: _For Alg 1, we have_

\[\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq \sqrt{(1+L^{2})d+\left\|\nabla f_{*}(\mathbf{0})\right\|^{2}}\cdot \exp(-K\eta)\] \[+\sqrt{\frac{1}{2}\sum_{k=0}^{K-1}\mathbb{E}_{\hat{\mathbf{x}} \sim\hat{p}_{k\eta}}\left[\mathrm{KL}\left(\hat{p}_{(k+1)\eta|k\eta}(\cdot| \hat{\mathbf{x}})\right)\right\|p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot|\hat{ \mathbf{x}})\right)}\]

_for any \(K\in\mathbb{N}_{+}\) and \(\eta\in\mathbb{R}_{+}\)._

It is worth noting that the choice of \(\eta\) represents a trade-off between the number of subproblems divided throughout the entire process and the difficulty of solving these subproblems. By considering the choice \(\eta=1/2\cdot\log(1+1/2L)\), we can observe two points: (1) the sampling subproblems in Alg. 1 tend to be simple, as all RTK targets, presented in Lemma 3.1, can be provably strongly log-concave; (2) the total number of subproblems is \(K=T/\eta=\tilde{\mathcal{O}}(1)\), which is not large. Conversely, when considering a larger \(\eta\) that satisfies \(\eta\gg\log(1+1/L)\), the RTK target will no longer be guaranteed to be log-concave, resulting in high computational complexity, potentially even exponential in \(d\), when solving the corresponding sampling subproblems. On the other hand, if a much smaller step size \(\eta=o(1)\) is considered, the target distribution of the sampling subproblems can be easily solved, even with a one-step Gaussian transition. However, this will increase the total number of sampling subproblems, potentially leading to higher computational complexity.

Therefore, we will consider the setup \(\eta=1/2\cdot\log(1+1/2L)\) in the remaining part of this paper. Now, the remaining task, which will be discussed in the next section, would be designing and analyzing the sampling algorithms for implementing all iterations of Alg. 1, i.e., solving the subproblems of RTK.

## 4 Implementation of RTK inner loops

In this section, we outline the implementation of Step 3 in the RTK algorithm, which aims to solve the sampling subproblems with strong log-concave targets, i.e., \(p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot|\hat{\mathbf{x}}_{k\eta})\propto\exp(-g)\). Specifically, we employ two MCMC algorithms, i.e., the Metropolis-adjusted Langevin algorithm (MALA) and underdamped Langevin dynamics (ULD). For each algorithm, we will first introduce the detailed implementation, combined with some explanation about notations and settings to describe the inner sampling process. After that, we will provide general convergence results and discuss them in several theoretical or practical settings. Besides, we will also compare our complexity results with the previous ones when achieving the convergence of TV distance to show that the RTK framework indeed obtains a better complexity by balancing the number and complexity of sampling subproblems.

**RTK-MALA.** Alg. 2 presents a solution employing MALA for the inner loop. When it is used to solve the \(k\)-th sampling subproblem of Alg. 1, \(\mathbf{x}_{0}\) is equal to \(\hat{\mathbf{x}}_{k\eta}\) defined in Section 3 and used to initialize particles iterating in Alg. 2. In Alg. 2, we consider the process \(\{\mathbf{z}_{s}\}_{s=0}^{S}\) whose underlying distribution is denoted as \(\{\mu_{s}\}_{s=0}^{S}\). Although we expect \(\mu_{S}\) to be close to the target distribution \(p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot|\mathbf{x}_{0})\), in real practice, the output particles \(\mathbf{z}_{S}\) can only approximately follow \(p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot|\mathbf{x}_{0})\) due to inevitable errors. Therefore, these errors should be explained in order to conduct a meaningful complexity analysis of the implementable algorithm. Specifically, Alg. 2 introduces two intrinsic errors:

* Estimation error of the score function: we assume a score estimator, e.g., a well-trained diffusion model, \(s_{\theta}\), which can approximate the score function with an \(\epsilon_{\mathrm{score}}\) error, i.e., \(\|s_{\theta_{t}}(\mathbf{z})-\nabla\log p_{t}(\mathbf{z})\|\leq\epsilon_{\mathrm{score}}\) for all \(\mathbf{z}\in\mathbb{R}^{d}\) and \(t\in[0,T]\).
* Estimation error of the energy function difference: we assume an energy difference estimator \(r\) which can approximate energy difference with an \(\epsilon_{\mathrm{energy}}\) error, i.e., \(|r_{t}(\mathbf{z}^{\prime},\mathbf{z})+\log p_{t}(\mathbf{z}^{\prime})-\log p_{t}(\mathbf{z})| \leq\epsilon_{\mathrm{energy}}\) for all \(\mathbf{z},\mathbf{z}^{\prime}\in\mathbb{R}^{d}\).

Under these settings, we provide a general convergence theorem for Alg. 2. To clearly convey the convergence properties, we only show an informal version.

**Theorem 4.1** (Informal version of Theorem C.17).: _Under Assumption [4, 1, 2], for Alg. 1, we choose_

\[\eta=\frac{1}{2}\log\frac{2L+1}{2L}\quad\mathrm{and}\quad K=4L\cdot\log\frac{(1+L ^{2})d+\|\nabla f_{*}(\mathbf{0})\|^{2}}{\epsilon^{2}}\]

_and implement Step 3 of Alg. 1 with Alg. 2. Suppose the score **[E1]**, energy **[E2]** estimation errors and the inner step size \(\tau\) satisfy_

\[\epsilon_{\mathrm{score}}=\mathcal{O}(\rho d^{-1/2}),\quad\epsilon_{\mathrm{ energy}}=\mathcal{O}(\rho\tau^{1/2}),\quad\mathrm{and}\quad\tau=\tilde{\mathcal{O}} \left(L^{-2}\cdot(d+m_{2}^{2}+Z^{2})^{-1}\right),\]

_and the hyperparameters, i.e., \(R\) and \(r\), are chosen properly. We have_

\[\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq\tilde{\mathcal{O}}(\epsilon )+\exp\left(\mathcal{O}(L(d+m_{2}^{2}))\right)\cdot\left(1-\frac{\rho^{2}}{4} \cdot\tau\right)^{S}+\tilde{\mathcal{O}}\left(\frac{L\epsilon_{\mathrm{score} }}{\rho}\right)+\tilde{\mathcal{O}}\left(\frac{L\epsilon_{\mathrm{energy}}}{ \rho\tau^{1/2}}\right) \tag{9}\]

_where \(\rho\) is the Cheeger constant of a truncated inner target distribution \(\exp(-g(\mathbf{z}))\mathbf{1}[\mathbf{z}\in\mathcal{B}(\mathbf{0},R)]\) and \(Z\) denotes the maximal \(l_{2}\) norm of particles appearing in outer loops (Alg. 1)._

It should be noted that the choice of \(\eta\) choice ensures the \(L\) strong log-concavity of target distribution \(\exp(-g(\mathbf{z}))\), which means its Cheeger constant is also \(L\). Although the Cheeger constant \(\rho\) in the second term of Eq. 9 corresponding to truncated \(\exp(-g(\mathbf{z}))\) should also be near \(L\) intuitively, current techniques can only provide a loose lower bound at an \(\mathcal{O}(\sqrt{L/d})\)-level (proven in Corollary C.8). While in both cases above, the Cheeger constant is independent with \(\epsilon\). Combining this fact with an \(\epsilon\)-independent choice of inner step sizes \(\tau\), the second term of Eq. 9 will converge linearly with respect to \(\epsilon\). As for the diameter \(Z\) of particles used to upper bound \(\tau\), though it may be unbounded in the standard implementation of Alg. 2, Lemma C.18 can upper bound it with \(\tilde{\mathcal{O}}\left(L^{3/2}(d+m_{2}^{2})\rho^{-1}\right)\) under the projected version of Alg. 2.

Additionally, to require the final sampling error to satisfy \(\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq\tilde{\mathcal{O}}(\epsilon)\), Eq. 9 shows that the score and energy difference estimation errors should be \(\epsilon\)-dependent and sufficiently small, where \(\epsilon_{\mathrm{score}}\) corresponding to the training loss can be well-controlled. However, obtaining a highly accurate energy difference estimation (requiring a small \(\epsilon_{\mathrm{energy}}\)) is hard with only diffusion models. To solve this problem, we can introduce a neural network energy estimator similar to [38] to construct \(r(\mathbf{z}^{\prime},\mathbf{z},t)\), which induces the following complexity describing the calls of the score estimation.

**Corollary 4.2** (Informal version of Corollary C.19).: _Suppose the estimation errors of score and energy difference satisfy_

\[\epsilon_{\mathrm{score}}\leq\frac{\rho\epsilon}{Ld^{1/2}}\quad\mathrm{and} \quad\epsilon_{\mathrm{energy}}\leq\frac{\rho\epsilon}{L^{2}\cdot(d^{1/2}+m_{2} +Z)},\]_If we implement Alg. 1 with the projected version of Alg. 2 with the same hyperparameter settings as Theorem 4.1, it has \(\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq\tilde{\mathcal{O}}(\epsilon)\) with an \(\mathcal{O}(L^{4}\rho^{-2}\cdot\left(d+m_{2}^{2}\right)^{2}Z^{2}\cdot\log(d/ \epsilon))\) complexity._

Considering the loose bound for both \(\rho\) and \(Z\), the complexity will be at most \(\tilde{\mathcal{O}}(L^{5}(d+m_{2}^{2})^{6})\) which is the first linear convergence w.r.t. \(\epsilon\) result for the diffusion inference process.

**Score-only RTK-MALA.** However, the parametric energy function may not always exist in real practice. We consider a more practical case where only the score estimation is accessible. In this case, we will make use of estimated score functions to approximate the energy difference, leading to the score-only RTK-MALA algorithm. In particular, recall that the energy difference function takes the following form:

\[g(\mathbf{z}^{\prime})-g(\mathbf{z})=-\log p_{(K-k-1)\eta}(\mathbf{z}^{\prime})+\frac{ \left\lVert\mathbf{x}_{0}-\mathbf{z}^{\prime}\cdot e^{-\eta}\right\rVert^{2}}{2(1-e^{ -2\eta})}+\log p_{(K-k-1)\eta}(\mathbf{z})-\frac{\left\lVert\mathbf{x}_{0}-\mathbf{z}\cdot e ^{-\eta}\right\rVert^{2}}{2(1-e^{-2\eta})}.\]

Since the quadratic term can be obtained exactly, we only need to estimate the energy difference. Then let \(f(\mathbf{z})=-\log p_{(K-k-1)\eta}(\mathbf{z})\) and denote \(h(t)=f\left((\mathbf{z}^{\prime}-\mathbf{z})\cdot t+\mathbf{z}\right)\), the energy difference \(g(\mathbf{z}^{\prime})-g(\mathbf{z})\) can be reformulated as

\[h(1)-h(0)=\sum_{i=1}\frac{h^{(i)}(0)}{i!}\quad\mathrm{and}\quad h^{(i)}(t) \coloneqq\frac{\mathrm{d}^{i}h(t)}{(\mathrm{d}t)^{i}},\]

where we perform the standard Taylor expansion at the point \(t=0\). Then, we only need the derives of \(h^{i}(0)\), which can be estimated using only the score function. For instance, the \(h^{(1)}(t)\) can be estimated with score estimations:

\[h^{(1)}(t)=\nabla f((\mathbf{z}^{\prime}-\mathbf{z})\cdot t+\mathbf{z})\cdot(\mathbf{z}^{ \prime}-\mathbf{z})\approx\tilde{h}^{(1)}(t)\coloneqq s_{\theta}((\mathbf{z}^{\prime} -\mathbf{z})\cdot t+\mathbf{z})\cdot(\mathbf{z}^{\prime}-\mathbf{z}).\]

Moreover, regarding the high-order derivatives, we can recursively perform the approximation: \(\tilde{h}^{(i+1)}(0)=(\tilde{h}^{(i)}(\Delta t)-\tilde{h}^{(i)}(0))/\Delta t\). Consider performing the approximation up to \(u\)-order derivatives, we can get the approximation of the energy difference:

\[r_{(K-k-1)\eta}(\mathbf{z}^{\prime},\mathbf{z})\coloneqq\sum_{i=1}^{u}\frac{\tilde{h} ^{(i)}(0)}{i!}.\]

Then, the following corollary states the complexity of the score-only RTK-MALA algorithm.

**Corollary 4.3**.: _Suppose the estimation errors of the score satisfies \(\epsilon_{\mathrm{score}}\ll\rho\epsilon/(Ld^{1/2})\), and the log-likelihood function of \(p_{t}\) has a bounded \(u\)-order derivative, e.g., \(\left\lVert\nabla^{(u)}f(\mathbf{z})\right\rVert\leq L\), we have a non-parametric estimation for log-likelihood to make we have \(\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq\tilde{\mathcal{O}}(\epsilon)\) with a complexity shown as follows_

\[\tilde{\mathcal{O}}\left(L^{4}\rho^{-3}\cdot\left(d+m_{2}^{2}\right)^{2}Z^{3} \cdot\epsilon^{-2/(u-1)}\cdot 2^{u}\right).\]

This result implies that if the energy function is infinite-order Lipschitz, we can nearly achieve any polynomial order convergence w.r.t. \(\epsilon\) with the non-parametric energy difference estimation.

**RTK-ULD.** Alg. 3 presents a solution employing ULD for the inner loop, which can accelerate the convergence of the inner loop due to the better discretization of the ULD algorithm. When it is used to solve the \(k\)-th sampling subproblem of Alg. 1, \(\mathbf{x}_{0}\) is equal to \(\hat{\mathbf{x}}_{k\eta}\) defined in Section 3 and used to initialize particles iterating in Alg. 2. Besides, the underlying distribution of noise sample pair is

\[(\xi_{s}^{z},\xi_{s}^{z})\sim\mathcal{N}\left(\mathbf{0},\begin{bmatrix}\frac{2}{ \gamma}\left(\tau-\frac{2}{\gamma}\left(1-e^{-\gamma\tau}\right)\right)+\frac{ 1}{2\gamma}\left(1-e^{-2\gamma\tau}\right)&\frac{1}{\gamma}\left(1-2e^{-\gamma \tau}+e^{-2\gamma\tau}\right)\\ \frac{1}{\gamma}\left(1-2e^{-\gamma\tau}+e^{-2\gamma\tau}\right)&1-e^{-2\gamma \tau}\end{bmatrix}\right).\]

In Alg. 3, we consider the process \(\{(\hat{\mathbf{z}}_{s},\hat{\mathbf{v}}_{s})\}_{s=0}^{S}\) whose underlying distribution is denoted as \(\{\hat{\pi}_{s}\}_{s=0}^{S}\). We expect the \(\mathbf{z}\)-marginal distribution of \(\hat{\pi}_{S}\) to be close to the target distribution presented in Eq. 8. Then, the complexity of RTK-ULD to achieve the convergence of TV distance is provided as follows, and the detailed proof is deferred to Theorem D.6. Besides, we compare our theoretical results with the previous in Table 1.

**Theorem 4.4**.: _Under Assumptions **[A1]-[A2]** and **[E1]**, for Alg. 1, we choose_

\[\eta=1/2\cdot\log[(2L+1)/2L]\quad\mathrm{and}\quad K=4L\cdot\log[((1+L^{2})d+ \|\nabla f_{*}(\mathbf{0})\|^{2})^{2}\cdot\epsilon^{-2}]\]

_and implement Step 3 of Alg. 1 with projected Alg. 3. For the \(k\)-th run of Alg. 3, we require Gaussian-type initialization and high-accurate score estimation, i.e.,_

\[\hat{\pi}_{0}=\mathcal{N}(\mathbf{x}_{0},e^{2\eta}-1)\otimes\mathcal{N}(\mathbf{0},\bm {I})\quad\mathrm{and}\quad\epsilon_{\mathrm{score}}=\tilde{\mathcal{O}}( \epsilon/\sqrt{L}).\]

_If we set the hyperparameters of inner loops as follows. the step size and the iteration number as_

\[\tau =\tilde{\Theta}\left(\epsilon d^{-1/2}L^{-1/2}\cdot\left(\log \left[\frac{L(d+m_{2}^{2}+\|\mathbf{x}_{0}\|^{2})}{\epsilon^{2}}\right]\right)^{- 1/2}\right)\] \[S =\tilde{\Theta}\left(\epsilon^{-1}d^{1/2}\cdot\left(\log\left[ \frac{L(d+m_{2}^{2}+\|\mathbf{x}_{0}\|^{2})}{\epsilon^{2}}\right]\right)^{1/2} \right).\]

_It can achieve \(\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\lesssim\epsilon\) with an \(\tilde{\mathcal{O}}\left(L^{2}d^{1/2}\epsilon^{-1}\right)\) gradient complexity._

## 5 Conclusion and Limitation

This paper presents an analysis of a modified version of diffusion models. Instead of focusing on the discretization of the reverse SDE, we propose a general RTK framework that can produce a large class of algorithms for diffusion inference, which is formulated as solving a sequence of RTK sampling subproblems. Given this framework, we develop two algorithms called RTK-MALA and RTK-ULD, which leverage MALA and ULD to solve the RTK sampling subproblems. We develop theoretical guarantees for these two algorithms under certain conditions on the score estimation, and demonstrate their faster convergence rate than prior works. Numerical experiments support our theory.

We would also like to point out several limitations and future work. One potential limitation of this work is the lack of large-scale experiments. The main focus of this paper is the theoretical

\begin{table}
\begin{tabular}{c c c c} \hline \hline Results & Algorithm & Assumptions & Complexity \\ \hline Chen et al. [8] & DDPM (SDE-based) & **[A1],[A2],[E1]** & \(\tilde{\mathcal{O}}(L^{2}d\epsilon^{-2})\) \\ \hline Chen et al. [9] & DPOM (ODE-based) & **[A1],[A2],[E1]**, and \(s_{\theta}\) smoothness & \(\tilde{\mathcal{O}}(L^{3}d\epsilon^{-2})\) \\ \hline Chen et al. [9] & DPUM (ODE-based) & **[A1],[A2],[E1]**, and \(s_{\theta}\) smoothness & \(\tilde{\mathcal{O}}(L^{2}d^{1/2}\epsilon^{-1})\) \\ \hline Li et al. [24] & ODE-based sampler & **[E1]** and estimation error of energy Hessian & \(\tilde{\mathcal{O}}(d^{3}\epsilon^{-1})\) \\ \hline Corollary 4.2 & RTK-MALA & **[A1],[A2],[E1]**, and **[E2]** & \(\mathcal{O}(L^{4}d^{2}\log(d/\epsilon))\) \\ \hline Theorem 4.4 & RTK-ULD (ours) & **[A1],[A2],[E1]** & \(\tilde{\mathcal{O}}(L^{2}d^{1/2}\epsilon^{-1})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with prior works for RTK-based methods. The complexity denotes the number of calls for the score estimation to achieve \(\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq\tilde{\mathcal{O}}(\epsilon)\). \(d\) and \(\epsilon\) mean the dimension and error tolerance. Compared with the state-of-the-art result, RTK-ULD achieves the best dependence for both \(d\) and \(\epsilon\). Though RTK-MALA requires slightly stricter assumptions and worse dimension dependence, a linear convergence w.r.t. \(\epsilon\) makes it suit high-accuracy sampling tasks.

understanding and rigorous analysis of the diffusion process. Implementing large-scale experiments requires GPU resources and practitioner support, which can be an interesting direction for future work. Besides, though we provided a score-only RTK-MALA algorithm, the \(\tilde{\mathcal{O}}(1/\epsilon)\) convergence rate can only be achieved by the RTK-MALA algorithm (Alg. 2). However, this faster algorithm requires a direct approximation of the energy difference, which is not accessible in the existing pretrained diffusion model. Developing practical energy difference approximation algorithms and incorporating them with Alg. 2 for diffusion inference are also very interesting future directions.

## Acknowledgement

The research is partially supported by the NSF awards: SCALE MoDL-2134209, CCF-2112665 (TILOS). It is also supported, DARPA AIE program, the U.S. Department of Energy, Office of Science, the Facebook Research Award, as well as CDC-RFA-FT-23-0069 from the CDC's Center for Forecasting and Outbreak Analytics. Difan Zou is supported in part by Guangdong NSF 2024A151501244, NSFC 62306252, and the central fund from HKU IDS.

## References

* Altschuler and Chewi [2023] Jason M Altschuler and Sinho Chewi. Faster high-accuracy log-concave sampling via algorithmic warm starts. In _2023 IEEE 64th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 2169-2176. IEEE, 2023.
* Austin et al. [2021] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_, 34:17981-17993, 2021.
* Benton et al. [2024] Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Nearly \(d\)-linear convergence bounds for diffusion models via stochastic localization. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=r5njV3BsuD](https://openreview.net/forum?id=r5njV3BsuD).
* Block et al. [2020] Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and Langevin sampling. _arXiv preprint arXiv:2002.00107_, 2020.
* Boffi and Vanden-Eijnden [2023] Nicholas M. Boffi and Eric Vanden-Eijnden. Probability flow solution of the Fokker-Planck equation, 2023.
* Buser [1982] Peter Buser. A note on the isoperimetric constant. In _Annales scientifiques de l'Ecole normale superieure_, volume 15, pages 213-230, 1982.
* Chen et al. [2023] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In _International Conference on Machine Learning_, pages 4735-4763. PMLR, 2023.
* Chen et al. [2023] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In _International Conference on Learning Representations_, 2023.
* Chen et al. [2024] Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ODE is provably fast. _Advances in Neural Information Processing Systems_, 36, 2024.
* Chen et al. [2022] Yongxin Chen, Sinho Chewi, Adil Salim, and Andre Wibisono. Improved analysis for a proximal algorithm for sampling. In _Conference on Learning Theory_, pages 2984-3014. PMLR, 2022.
* Cheng and Bartlett [2018] Xiang Cheng and Peter Bartlett. Convergence of langevin mcmc in kl-divergence. In _Algorithmic Learning Theory_, pages 186-211. PMLR, 2018.
* Chewi [2024] Sinho Chewi. _Log-Concave Sampling_. 2024.

* De Bortoli et al. [2021] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. _Advances in Neural Information Processing Systems_, 34:17695-17709, 2021.
* Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.
* Dong et al. [2023] Hanze Dong, Xi Wang, LIN Yong, and Tong Zhang. Particle-based variational inference with preconditioned functional gradient flow. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=60phWAE3cS](https://openreview.net/forum?id=60phWAE3cS).
* Dwivedi et al. [2019] Raaz Dwivedi, Yuansi Chen, Martin J Wainwright, and Bin Yu. Log-concave sampling: Metropolis-Hastings algorithms are fast. _Journal of Machine Learning Research_, 20(183):1-42, 2019.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* Huang et al. [2023] Xunpeng Huang, Hanze Dong, Yifan Hao, Yian Ma, and Tong Zhang. Monte Carlo sampling without isoperimetry: A reverse diffusion approach, 2023.
* Huang et al. [2024] Xunpeng Huang, Hanze Dong, Yifan HAO, Yian Ma, and Tong Zhang. Reverse diffusion monte carlo. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=kIPEyMSdFV](https://openreview.net/forum?id=kIPEyMSdFV).
* Lee et al. [2018] Holden Lee, Andrej Risteski, and Rong Ge. Beyond log-concavity: Provable guarantees for sampling multi-modal distributions using simulated tempering Langevin Monte Carlo. _Advances in neural information processing systems_, 31, 2018.
* Lee et al. [2022] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. _arXiv preprint arXiv:2206.06227_, 2022.
* Lee and Vempala [2018] Yin Tat Lee and Santosh S Vempala. Convergence rate of Riemannian Hamiltonian Monte Carlo and faster polytope volume computation. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1115-1121, 2018.
* Lee and Vempala [2017] Yin Tat Lee and Santosh Srinivas Vempala. Eldan's stochastic localization and the KLS hyperplane conjecture: an improved lower bound for expansion. In _2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 998-1007. IEEE, 2017.
* Li et al. [2023] Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards non-asymptotic convergence for diffusion-based generative models. In _The Twelfth International Conference on Learning Representations_, 2023.
* Lu et al. [2022] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _Advances in Neural Information Processing Systems_, 35:5775-5787, 2022.
* Ma et al. [2021] Yi-An Ma, Niladri S Chatterji, Xiang Cheng, Nicolas Flammarion, Peter L Bartlett, and Michael I Jordan. Is there an analog of Nesterov acceleration for gradient-based MCMC? _Bernoulli_, 27(3), 2021.
* Popov et al. [2021] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts: A diffusion probabilistic model for text-to-speech. In _International Conference on Machine Learning_, pages 8599-8608. PMLR, 2021.
* Ramesh et al. [2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arxiv 2022. _arXiv preprint arXiv:2204.06125_, 2022.
* Saharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in neural information processing systems_, 35:36479-36494, 2022.

* Shamir [2011] Ohad Shamir. A variant of azuma's inequality for martingales with subgaussian tails. _arXiv preprint arXiv:1110.2392_, 2011.
* Song et al. [2020] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.
* Song and Ermon [2019] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.
* Song et al. [2020] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2020.
* Trippe et al. [2023] Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi S. Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=6TxBxqNME1Y](https://openreview.net/forum?id=6TxBxqNME1Y).
* Vempala and Wibisono [2019] Santosh Vempala and Andre Wibisono. Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices. _Advances in neural information processing systems_, 32, 2019.
* Vincent [2011] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural computation_, 23(7):1661-1674, 2011.
* Watson et al. [2023] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rdfdiffusion. _Nature_, 620(7976):1089-1100, 2023.
* Xu and Chi [2024] Xingyu Xu and Yuejie Chi. Provably robust score-based diffusion posterior sampling for plug-and-play image reconstruction. _arXiv preprint arXiv:2403.17042_, 2024.
* Zhang et al. [2024] Dinghuai Zhang, Ricky T. Q. Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=OIsaab1UVC](https://openreview.net/forum?id=OIsaab1UVC).
* Zhang et al. [2023] Shunshi Zhang, Sinho Chewi, Mufan Li, Krishna Balasubramanian, and Murat A Erdogdu. Improved discretization analysis for underdamped Langevin Monte Carlo. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 36-71. PMLR, 2023.
* Zou et al. [2021] Difan Zou, Pan Xu, and Quanquan Gu. Faster convergence of stochastic gradient langevin dynamics for non-log-concave sampling. In _Uncertainty in Artificial Intelligence_, pages 1152-1162. PMLR, 2021.

###### Contents

* 1 Introduction
* 2 Preliminaries
* 3 General Framework of Reverse Transition Kernel
* 4 Implementation of RTK inner loops
* 5 Conclusion and Limitation
* A Numerical Experiments
* B Inference process with reverse transition kernel framework
* C Implement RTK inference with MALA
* C.1 Control the error from the projected transition kernel
* C.2 Control the error from the approximation of score and energy
* C.3 Control the error from Inner MALA to its stationary
* C.3.1 The Cheeger isoperimetric inequality of \(\tilde{\mu}_{*}\)
* C.3.2 The conductance properties of \(\tilde{\mathcal{T}}_{*}\)
* C.4 Main Theorems of InnerMALA implementation
* C.5 Control the error from Energy Estimation
* D Implement RTK inference with ULD
* E Auxiliary Lemmas

[MISSING_PAGE_FAIL:14]

Fig. 1 (a) shows the marginal accuracies of our RTK sampling algorithms and DDPM along NFE. We observe that all algorithms using RTK converge quickly. Among all RTK algorithms, RTK-MALA achieves the highest marginal accuracy. Score-only RTK-MALA is worse than RTK-MALA since the estimated energy contains errors, yet it is still slightly better than RTK-ULD. Along all RTK algorithms, RTK-ULA demonstrates the lowest performance in terms of marginal accuracy, but it still outperforms DDPM with a large margin especially when NFE is small.

Fig. 1 (b-f) shows the histograms of sampled MoG by DDPM and RTK-based methods. We observe that DDPM cannot reconstruct the local structure of MoG. ULA can roughly reconstruct the MoG structure, but it is still weak in complex regions, specifically around the peaks and valleys. In contrast, RTK-ULD, score-only RTK-MALA, and RTK-MALA can reconstruct more fine-grained structures in complex regions.

Fig. 2 (a-e) shows the clusters sampled by DDPM and RTK-based methods. We observe that DDPM fails to accurately reconstruct the ground truth distribution. In contrast, all methods based on RTK can generate distributions that closely approximate the ground truth. Additionally, RTK-MALA shows superior performance in accurately reconstructing the distribution in regions of low probability.

We perform other experiments on various MoG settings.Figure 3 and 4 shows the experiments on a spiral-shaped and chessboard-shaped MoG as the ground truth distribution, respectively. In these experiments, we also compared Annealed Langevin Dynamics (ALD) [] with our RTK-based method. In these figures, we observe that compared to DDPM and ALD, our RTK-based methods achieve significantly better marginal accuracy when NFE is small. Besides, we find that our RTK-based methods have a much better estimation on low-probability region. Furthermore, in Figure 5, we evaluated the methods using the Wasserstein Distance metric, which corresponds to Frechet Inception Distance. Our results indicate that our RTK-based methods have a lower Wasserstein Distance compared to DDPM and ALD, especially when NFE is small.

Furthermore, we conducted experiments on the MNIST dataset, as shown in Figure 6. We first trained a score model following the typical variance-preserving noise schedule and then compared different sampling methods using the Frechet Inception Distance (FID) evaluation criterion. Figure 6 (a) shows that compared with DDPM, our RTK-based methods achieve better FID scores than DDPM,

Figure 1: (a) Marginal accuracy of the sampled MoG by different algorithms along NFE. (b-f) The histograms along a certain direction of sampled MoG by different algorithms. The plots labeled by ‘ULA’, ‘ULD’, ‘MALA’, ‘MALA_ES’ correspond to RTK-ULA, RTK-ULD, RTK-MALA, score-only RTK-MALA, respectively. The histogram is oriented along the second dimension when the first dimension is constrained within (0.75, 1.25).

particularly when NFE is small. Figure 6 (b) shows that our RTK-based methods generate images of higher quality than DDPM when NFE is small.

Overall, these numerical experiments demonstrate the benefit of the RTK framework for developing faster algorithms than DDPM in diffusion inference. Besides, experimental results also well support our theory, showing that RTK-MALA achieves faster convergence than RTK-ULA and RTK-ULD, even with estimated energy difference via score functions.

Figure 3: (a) Ground truth clusters sampled by a spiral-shaped MoG distribution. (b-g) Clusters sampled using 205 NFE by DDPM, ALD, ULA, ULD, MALA_ES, and MALA, respectively. (h) Marginal accuracy of the sampled MoG by different algorithms along NFE.

Figure 2: (a-e) Clusters sampled by DDPM, RTK-ULA, RTK-ULD, score-only RTK-MALA, and RTK-MALA, respectively. (f) Clusters sampled by the ground truth distribution. These \(2D\) clusters represent the projection of the original \(10D\) data onto the first two dimensions.

## Appendix B Inference process with reverse transition kernel framework

Proof of Lemma 3.1.: According to Bayes theorem, the following equation should be validated for any \(\mathbf{x}\in\mathbb{R}^{d}\) and \(t^{\prime}>t\),

\[p_{t}(\mathbf{x})=\int p_{t|t^{\prime}}(\mathbf{x}|\mathbf{x}^{\prime})\cdot p_{t^{\prime}}( \mathbf{x}^{\prime})\mathrm{d}\mathbf{x}^{\prime}. \tag{10}\]

To simplify the notation, we suppose the normalizing constant of \(p_{t}\), i.e.,

\[Z_{t}\coloneqq\int\exp(-f_{t}(\mathbf{x}))\mathrm{d}\mathbf{x}.\]

Besides, the forward OU process, i.e., SDE. 1, has a closed transition kernel, i.e.,

\[p_{t^{\prime}|t}(\mathbf{x}^{\prime}|\mathbf{x})=\left(2\pi\left(1-e^{-2(t^{\prime}-t) }\right)\right)^{-d/2}\cdot\exp\left[\frac{-\left\|\mathbf{x}^{\prime}-e^{-(t^{ \prime}-t)}\mathbf{x}\right\|^{2}}{2\left(1-e^{-2(t^{\prime}-t)}\right)}\right]\]

Then, we have

\[p_{t^{\prime}}(\mathbf{x}^{\prime})= \int p_{t}(\mathbf{y})p_{t^{\prime}|t}(\mathbf{x}^{\prime}|\mathbf{y}) \mathrm{d}\mathbf{y}\] \[= \int Z_{t}^{-1}\cdot\exp(-f_{t}(\mathbf{y}))\cdot\left(2\pi\left(1-e^ {-2(t^{\prime}-t)}\right)\right)^{-d/2}\cdot\exp\left[\frac{-\left\|\mathbf{x}^{ \prime}-e^{-(t^{\prime}-t)}\mathbf{y}\right\|^{2}}{2\left(1-e^{-2(t^{\prime}-t)} \right)}\right]\mathrm{d}\mathbf{y}.\]

Figure 4: (a) Ground truth clusters sampled by a chessboard-shaped MoG distribution. (b-g) Clusters sampled using 205 NFE by DDPM, ALD, ULA, ULD, MALA_ES, and MALA, respectively. (h) Marginal accuracy of the sampled MoG by different algorithms along NFE.

Figure 5: Wasserstein distance of the sampled MoG by different algorithms along NFE. (a) and (b) are the Wasserstein distance plot for spiral-shaped and chessboard shaped MoG distribution, respectively.

Plugging this equation into Eq. 10, and we have

RHS of Eq. 10 \[= \int p_{t|t^{\prime}}(\mathbf{x}|\mathbf{x}^{\prime})\cdot p_{t^{\prime}}( \mathbf{x}^{\prime})\mathrm{d}\mathbf{x}^{\prime}\] \[= \int p_{t|t^{\prime}}(\mathbf{x}|\mathbf{x}^{\prime})\cdot\int Z_{t}^{-1} \cdot\exp(-f_{t}(\mathbf{y}))\cdot\left(2\pi\left(1-e^{-2(t^{\prime}-t)}\right) \right)^{-d/2}\cdot\exp\left[\frac{-\left\|\mathbf{x}^{\prime}-e^{-(t^{\prime}-t)} \mathbf{y}\right\|^{2}}{2\left(1-e^{-2(t^{\prime}-t)}\right)}\right]\mathrm{d}\mathbf{ y}\mathrm{d}\mathbf{x}^{\prime}.\]

Moreover, when we plug the reverse transition kernel

\[p_{t|t^{\prime}}(\mathbf{x}|\mathbf{x}^{\prime})\propto\exp\left(-f_{t}(\mathbf{x})-\frac{ \left\|\mathbf{x}^{\prime}-\mathbf{x}\cdot e^{-(t^{\prime}-t)}\right\|^{2}}{2(1-e^{-2 (t^{\prime}-t)})}\right)\]

into the previous equation and have

RHS of Eq. 10 \[= \int\frac{\exp\left(-f_{t}(\mathbf{x})-\frac{\left\|\mathbf{x}^{\prime}- \mathbf{x}\cdot e^{-(t^{\prime}-t)}\right\|^{2}}{2(1-e^{-2(t^{\prime}-t)})}\right) }{\int\exp\left(-f_{t}(\mathbf{x})-\frac{\left\|\mathbf{x}^{\prime}-\mathbf{x}\cdot e^{-(t^ {\prime}-t)}\right\|^{2}}{2(1-e^{-2(t^{\prime}-t)})}\right)\mathrm{d}\mathbf{x}}.\] \[\int Z_{t}^{-1}\cdot\exp(-f_{t}(\mathbf{y}))\cdot\left(2\pi\left(1-e^{ -2(t^{\prime}-t)}\right)\right)^{-d/2}\cdot\exp\left[\frac{-\left\|\mathbf{x}^{ \prime}-e^{-(t^{\prime}-t)}\mathbf{y}\right\|^{2}}{2\left(1-e^{-2(t^{\prime}-t)} \right)}\right]\mathrm{d}\mathbf{y}\mathrm{d}\mathbf{x}^{\prime}\] \[=Z_{t}^{-1}\cdot\exp(-f_{t}(\mathbf{x}))\cdot\int\exp\left(-\frac{ \left\|\mathbf{x}^{\prime}-\mathbf{x}\cdot e^{-(t^{\prime}-t)}\right\|^{2}}{2(1-e^{-2( t^{\prime}-t)})}\right)\cdot\left(2\pi\left(1-e^{-2(t^{\prime}-t)}\right) \right)^{-d/2}.\] \[\left[\int\frac{\exp\left(-f_{t}(\mathbf{y})-\frac{\left\|\mathbf{x}^{ \prime}-e^{-(t^{\prime}-t)}\right\|^{2}}{2(1-e^{-2(t^{\prime}-t)})}\right)}{ \int\exp\left(-f_{t}(\mathbf{x})-\frac{\left\|\mathbf{x}^{\prime}-\mathbf{x}\cdot e^{-(t^ {\prime}-t)}\right\|}{2(1-e^{-2(t^{\prime}-t)})}\right)\mathrm{d}\mathbf{x}}\right] \mathrm{d}\mathbf{x}^{\prime}\] \[=p_{t}(\mathbf{x})=\text{LHS of Eq.~{}\ref{eq:LHS}}.\]

Hence, the proof is completed. 

**Lemma B.1** (Chain rule of TV).: _Consider four random variables, \(\mathbf{x},\mathbf{z},\tilde{\mathbf{x}},\tilde{\mathbf{z}}\), whose underlying distributions are denoted as \(p_{x},p_{z},q_{x},q_{z}\). Suppose \(p_{x,z}\) and \(q_{x,z}\) denotes the densities of joint distributions of \((\mathbf{x},\mathbf{z})\) and \((\tilde{\mathbf{x}},\tilde{\mathbf{z}})\), which we write in terms of the conditionals and marginals as_

\[p_{x,z}(\mathbf{x},\mathbf{z}) =p_{x|z}(\mathbf{x}|\mathbf{z})\cdot p_{z}(\mathbf{z})=p_{z|x}(\mathbf{z}|\mathbf{x}) \cdot p_{x}(\mathbf{x})\] \[q_{x,z}(\mathbf{x},\mathbf{z}) =q_{x|z}(\mathbf{x}|\mathbf{z})\cdot q_{z}(\mathbf{z})=q_{z|x}(\mathbf{z}|\mathbf{x}) \cdot q_{x}(\mathbf{x}).\]

Figure 6: (a) FID of the sampled MNIST data by different algorithms along NFE. (b-d) Sampled MNIST data by ULD, score-only RTK-MALA, and DDPM respectively, when NFE is 20.

_then we have_

\[\mathrm{TV}\left(p_{x,z},q_{x,z}\right)\leq\min\left\{\mathrm{TV}\left(p_{z},q_{z} \right)+\mathbb{E}_{\mathbf{z}\sim p_{z}}\left[\mathrm{TV}\left(p_{x|z}(\cdot|\mathbf{z }),q_{x|z}(\cdot|\mathbf{z})\right)\right],\right.\]

_Besides, we have_

\[\left.\mathrm{TV}\left(p_{x},q_{x}\right)\leq\mathrm{TV}\left(p_{x,z},q_{x,z} \right).\right.\]

Proof.: According to the definition of the total variation distance, we have

\[\mathrm{TV}\left(p_{x,z},q_{x,z}\right)= \frac{1}{2}\int\int\left|p_{x,z}(\mathbf{x},\mathbf{z})-q_{x,z}(\mathbf{x}, \mathbf{z})\right|\mathrm{d}\mathbf{z}\mathrm{d}\mathbf{x}\] \[= \frac{1}{2}\int\int\left[p_{z}(\mathbf{z})p_{x|z}(\mathbf{x}|\mathbf{z})-p_{z} (\mathbf{z})q_{x|z}(\mathbf{x}|\mathbf{z})+p_{z}(\mathbf{z})q_{x|z}(\mathbf{x}|\mathbf{z})-q_{z}(\mathbf{z} )q_{x|z}(\mathbf{x}|\mathbf{z})\right]\mathrm{d}\mathbf{z}\mathrm{d}\mathbf{x}\] \[\leq \frac{1}{2}\int p_{z}(\mathbf{z})\int\left|p_{x|z}(\mathbf{x}|\mathbf{z})-q_{ x|z}(\mathbf{x}|\mathbf{z})\right|\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{z}+\frac{1}{2}\int\left|p_ {z}(\mathbf{z})-q_{z}(\mathbf{z})\right|\int q_{x|z}(\mathbf{x}|\mathbf{z})\mathrm{d}\mathbf{x} \mathrm{d}\mathbf{z}\] \[= \mathbb{E}_{\mathbf{z}\sim p_{z}}\left[\mathrm{TV}\left(p_{x|z}(\cdot |\mathbf{z}),q_{x|z}(\cdot|\mathbf{z})\right)\right]+\mathrm{TV}\left(p_{z},q_{z}\right).\]

With a similar technique, we have

\[\mathrm{TV}\left(p_{x,z},q_{x,z}\right)\leq\mathrm{TV}\left(p_{x},q_{x}\right)+ \mathbb{E}_{\mathbf{x}\sim p_{x}}\left[\mathrm{TV}\left(p_{z|x}(\cdot|\mathbf{x}),q_{x |z}(\cdot|\mathbf{x})\right)\right].\]

Hence, the first inequality of this Lemma is proved. Then, for the second inequality, we have

\[\mathrm{TV}\left(p_{x},q_{x}\right)= \frac{1}{2}\int\left|p_{x}(\mathbf{x})-q_{x}(\mathbf{x})\right|\mathrm{d} \mathbf{x}\] \[= \frac{1}{2}\int\left|\int p_{x,z}(\mathbf{x},\mathbf{z})\mathrm{d}\mathbf{z}- \int q_{x,z}(\mathbf{x},\mathbf{z})\mathrm{d}\mathbf{z}\right|\mathrm{d}\mathbf{x}\] \[\leq \frac{1}{2}\int\int\left|p_{x,z}(\mathbf{x},\mathbf{z})-q_{x,z}(\mathbf{x}, \mathbf{z})\right|\mathrm{d}\mathbf{z}\mathrm{d}\mathbf{x}\,=\mathrm{TV}\left(p_{x,z},q_{x,z}\right).\]

Hence, the proof is completed. 

**Lemma B.2**.: _For Alg 1, we have_

\[\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq \sqrt{(1+L^{2})d+\left\|\nabla f_{*}(\mathbf{0})\right\|^{2}}\cdot \exp(-K\eta)\] \[+\sum_{k=0}^{K-1}\mathbb{E}_{\hat{\mathbf{x}}\sim\hat{p}_{k\eta}} \left[\mathrm{TV}\left(\hat{p}_{(k+1)\eta|k\eta}(\cdot|\hat{\mathbf{x}}),p_{(k+1) \eta|k\eta}^{\leftarrow}(\cdot|\hat{\mathbf{x}})\right)\right]\]

_for any \(K\in\mathbb{N}_{+}\) and \(\eta\in\mathbb{R}_{+}\)._

Proof.: For any \(k\in\{0,1,\ldots,K-1\}\), let \(\hat{p}_{(k+1)\eta,k\eta}\) and \(p_{(k+1)\eta,k\eta}^{\leftarrow}\) denote the joint distribution of \((\hat{\mathbf{x}}_{(k+1)\eta},\hat{\mathbf{x}}_{k\eta})\) and \((\mathbf{x}_{(k+1)\eta}^{\leftarrow},\mathbf{x}_{k\eta}^{\leftarrow})\), which we write in term of the conditionals and marginals as

\[\hat{p}_{(k+1)\eta,k\eta}(\mathbf{x}^{\prime},\mathbf{x})=\hat{p}_{(k+1) \eta|k\eta}(\mathbf{x}^{\prime}|\mathbf{x})\cdot\hat{p}_{k\eta}(\mathbf{x})=\hat{p}_{k\eta| (k+1)\eta}(\mathbf{x}|\mathbf{x}^{\prime})\cdot\hat{p}_{(k+1)\eta}(\mathbf{x}^{\prime})\] \[p_{(k+1)\eta,k\eta}^{\leftarrow}(\mathbf{x}^{\prime},\mathbf{x})=p_{(k+1) \eta|k\eta}^{\leftarrow}(\mathbf{x}^{\prime}|\mathbf{x})\cdot p_{k\eta}^{\leftarrow}( \mathbf{x})=p_{k\eta|(k+1)\eta}^{\leftarrow}(\mathbf{x}|\mathbf{x}^{\prime})\cdot p_{(k+1 )\eta}^{\leftarrow}(\mathbf{x}^{\prime}).\]

Under this condition, we have

\[\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)=\mathrm{TV}\left( \hat{p}_{K\eta},p_{K\eta}^{\leftarrow}\right) \leq\mathrm{TV}\left(\hat{p}_{K\eta,(K-1)\eta},p_{K\eta,(K-1)\eta}^ {\leftarrow}\right)\] \[\leq\mathrm{TV}\left(\hat{p}_{(K-1)\eta},p_{(K-1)\eta}^{ \leftarrow}\right)+\mathbb{E}_{\hat{\mathbf{x}}\sim\hat{p}_{(K-1)\eta}}\left[ \mathrm{TV}\left(\hat{p}_{K\eta|(K-1)\eta}(\cdot|\hat{\mathbf{x}}),p_{K\eta|(K-1 )\eta}^{\leftarrow}(\cdot|\hat{\mathbf{x}})\right)\right]\]

where the inequalities follow from Lemma B.1. By using the inequality recursively, we have

\[\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq \mathrm{TV}\left(\hat{p}_{0},p_{0}^{\leftarrow}\right)+\sum_{k=0}^ {K-1}\mathbb{E}_{\hat{\mathbf{x}}\sim\hat{p}_{k\eta}}\left[\mathrm{TV}\left(\hat{p }_{(k+1)\eta|k\eta}(\cdot|\hat{\mathbf{x}}),p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot| \hat{\mathbf{x}})\right)\right] \tag{11}\] \[= \underbrace{\mathrm{TV}\left(p_{\infty},p_{K\eta}\right)}_{\text{ Term}\;1}+\sum_{k=0}^{K-1}\mathbb{E}_{\hat{\mathbf{x}}\sim\hat{p}_{k\eta}}\left[\mathrm{TV}\left(\hat{p }_{(k+1)\eta|k\eta}(\cdot|\hat{\mathbf{x}}),p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot| \hat{\mathbf{x}})\right)\right]\]

where \(p_{\infty}\) denotes the stationary distribution of the forward process. In this analysis, \(p_{\infty}\) is the standard since the forward SDE. 1, whose negative log density is \(1\)-strongly convex and also satisfies LSI with constant \(1\) due to Lemma E.9.

For Term 1.we have

\[\mathrm{TV}\left(p_{\infty},p_{K\eta}\right)\leq \sqrt{\frac{1}{2}\mathrm{KL}\left(p_{K\eta}\big{\|}p_{\infty}\right) }\leq\sqrt{\frac{1}{2}\cdot\exp\left(-2K\eta\right)\cdot\mathrm{KL}\left(p_{0} \big{\|}p_{\infty}\right)}\] \[\leq \sqrt{\left(1+L^{2}\right)d+\left\|\nabla f_{*}(\mathbf{0}) \right\|^{2}}\cdot\exp(-K\eta)\]

where the first inequality follows from Pinsker's inequality, the second one follows from Lemma E.1, and the last one follows from Lemma E.2. It should be noted that the smoothness of \(p_{0}\) required in Lemma E.2 is given by **[A1]**.

Plugging this inequality into Eq. 11, we have

\[\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq \sqrt{\left(1+L^{2}\right)d+\left\|\nabla f_{*}(\mathbf{0}) \right\|^{2}}\cdot\exp(-K\eta)\] \[+\sum_{k=0}^{K-1}\mathbb{E}_{\hat{\mathbf{x}}\sim\hat{p}_{k\eta}} \left[\mathrm{TV}\left(\hat{p}_{(k+1)\eta|k\eta}(\cdot|\hat{\mathbf{x}}),p_{(k +1)\eta|k\eta}^{\leftarrow}(\cdot|\hat{\mathbf{x}})\right)\right]\]

Hence, the proof is completed. 

**Corollary B.3**.: _For Alg 1, if we set_

\[\eta=\frac{1}{2}\cdot\log\frac{2L+1}{2L},\qquad K=4L\cdot\log\frac{\left(1+L^{ 2}\right)d+\left\|\nabla f_{*}(\mathbf{0})\right\|^{2}}{\epsilon^{2}}\]

_and_

\[\mathrm{TV}\left(\hat{p}_{(k+1)\eta|k\eta}(\cdot|\hat{\mathbf{x}}),p_{(k+1)\eta |k\eta}^{\leftarrow}(\cdot|\hat{\mathbf{x}})\right)\leq\frac{\epsilon}{K}=\frac {\epsilon}{4L}\cdot\left[\log\frac{\left(1+L^{2}\right)d+\left\|\nabla f_{*}( \mathbf{0})\right\|^{2}}{\epsilon^{2}}\right]^{-1},\]

_we have the total variation distance between the underlying distribution of Alg 1 output and the data distribution \(p_{*}\) will satisfy \(\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq 2\epsilon\)._

Proof.: According to Lemma B.2, we have

\[\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq \sqrt{\left(1+L^{2}\right)d+\left\|\nabla f_{*}(\mathbf{0}) \right\|^{2}}\cdot\exp(-K\eta)\] \[+\underbrace{\sum_{k=0}^{K-1}\mathbb{E}_{\hat{\mathbf{x}}\sim\hat {p}_{k\eta}}\left[\mathrm{TV}\left(\hat{p}_{(k+1)\eta|k\eta}(\cdot|\hat{ \mathbf{x}}),p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot|\hat{\mathbf{x}})\right) \right]}_{\mathrm{Term}\;2}\]

for any \(K\in\mathbb{N}_{+}\) and \(\eta\in\mathbb{R}_{+}\). To achieve the upper bound \(\mathrm{TV}\left(p_{\infty},p_{K\eta}\right)\leq\epsilon\), we only require

\[T=K\eta\geq\frac{1}{2}\log\frac{\left(1+L^{2}\right)d+\left\|\nabla f_{*}( \mathbf{0})\right\|^{2}}{\epsilon^{2}}. \tag{12}\]

For Term 2.For any \(\mathbf{x}\in\mathbb{R}^{d}\), the formulation of \(p_{k+1|k}^{\leftarrow}(\cdot|\hat{\mathbf{x}})\) is

\[p_{(k+1)\eta|k\eta}^{\leftarrow}(\mathbf{x}|\hat{\mathbf{x}})=p_{(K-k-1)\eta|(K-1)\eta }(\mathbf{x}|\hat{\mathbf{x}})\propto\exp\left(-f_{(K-k-1)\eta}(\mathbf{x})-\frac{\left\| \hat{\mathbf{x}}-\mathbf{x}\cdot e^{-\eta}\right\|^{2}}{2(1-e^{-2\eta})}\right),\]

whose negative log Hessian satisfies

\[-\nabla_{\mathbf{x}}^{2}\log p_{(k+1)\eta|k\eta}^{\leftarrow}(\mathbf{x}|\hat{\mathbf{x}}) =\nabla^{2}f_{(K-k-1)\eta}(\mathbf{x})+\frac{e^{-2\eta}}{1-e^{-2\eta}}\cdot\mathbf{I} \succeq\left(\frac{e^{-2\eta}}{1-e^{-2\eta}}-L\right)\cdot\mathbf{I}.\]

Note that the last inequality follows from **[A1]**. In this condition, if we require

\[\left(\frac{e^{-2\eta}}{1-e^{-2\eta}}-L\right)\geq L\quad\Leftrightarrow\quad \eta\leq\frac{1}{2}\log\frac{2L+1}{2L},\]then we have

\[\frac{e^{-2\eta}}{2(1-e^{-2\eta})}\cdot\mathbf{I}\preceq-\nabla_{\mathbf{x}}^{2}\log p_{(k+ 1)\eta|k\eta}^{\leftarrow}(\mathbf{x}|\hat{\mathbf{x}})\preceq\frac{3e^{-2\eta}}{2(1-e^{ -2\eta})}\cdot\mathbf{I}.\]

To simplify the following analysis, we choose \(\eta\) to its upper bound, and we know for all \(k\in\{0,1,\ldots,K-1\}\), the conditional density \(p_{k+1|k}^{\leftarrow}(\mathbf{x}|\hat{\mathbf{x}})\) is strongly-log concave, and its score is \(3L\)-Lipschitz. Besides, combining Eq. 12 and the choice of \(\eta\), we require

\[K=T/\eta\geq\log\frac{(1+L^{2})d+\left\lVert\nabla f_{*}(\mathbf{0})\right\rVert^{ 2}}{\epsilon^{2}}\Big{/}\log\frac{2L+1}{2L}\]

which can be achieved by

\[K\coloneqq 4L\cdot\log\frac{(1+L^{2})d+\left\lVert\nabla f_{*}(\mathbf{0})\right\rVert ^{2}}{\epsilon^{2}}\]

when we suppose \(L\geq 1\) without loss of generality. In this condition, if there is a uniform upper bound for all conditional probability approximation, i.e.,

\[\mathrm{TV}\left(\hat{p}_{(k+1)\eta|k\eta}(\cdot|\hat{\mathbf{x}}),p_{(k+1)\eta|k \eta}^{\leftarrow}(\cdot|\hat{\mathbf{x}})\right)\leq\frac{\epsilon}{K}=\frac{ \epsilon}{4L}\cdot\left[\log\frac{(1+L^{2})d+\left\lVert\nabla f_{*}(\mathbf{0}) \right\rVert^{2}}{\epsilon^{2}}\right]^{-1},\]

then we can find Term 2 in Eq. 11 will be upper bounded by \(\epsilon\). Hence, the proof is completed. 

**Lemma B.4** (Chain rule of KL).: _Consider four random variables, \(\mathbf{x},\mathbf{z},\tilde{\mathbf{x}},\tilde{\mathbf{z}}\), whose underlying distributions are denoted as \(p_{x},p_{z},q_{x},q_{z}\). Suppose \(p_{x,z}\) and \(q_{x,z}\) denotes the densities of joint distributions of \((\mathbf{x},\mathbf{z})\) and \((\tilde{\mathbf{x}},\tilde{\mathbf{z}})\), which we write in terms of the conditionals and marginals as_

\[p_{x,z}(\mathbf{x},\mathbf{z}) =p_{x|z}(\mathbf{x}|\mathbf{z})\cdot p_{z}(\mathbf{z})=p_{z|x}(\mathbf{z}|\mathbf{x} )\cdot p_{x}(\mathbf{x})\] \[q_{x,z}(\mathbf{x},\mathbf{z}) =q_{x|z}(\mathbf{x}|\mathbf{z})\cdot q_{z}(\mathbf{z})=q_{z|x}(\mathbf{z}|\mathbf{x}) \cdot q_{x}(\mathbf{x}).\]

_then we have_

\[\mathrm{KL}\left(p_{x,z}\middle\lVert q_{x,z}\right)= \mathrm{KL}\left(p_{z}\middle\lVert q_{z}\right)+\mathbb{E}_{ \mathbf{z}\sim p_{z}}\left[\mathrm{KL}\left(p_{x|z}(\cdot|\mathbf{z})\middle\lVert q _{x|z}(\cdot|\mathbf{z})\right)\right]\] \[= \mathrm{KL}\left(p_{x}\middle\lVert q_{x}\right)+\mathbb{E}_{\bm {x}\sim p_{x}}\left[\mathrm{KL}\left(p_{z|x}(\cdot|\mathbf{x})\middle\lVert q _{z|x}(\cdot|\mathbf{x})\right)\right]\]

_where the latter equation implies_

\[\mathrm{KL}\left(p_{x}\middle\lVert q_{x}\right)\leq\mathrm{KL}\left(p_{x,z} \middle\lVert q_{x,z}\right).\]

Proof.: According to the formulation of KL divergence, we have

\[\mathrm{KL}\left(p_{x,z}\middle\lVert q_{x,z}\right)= \int p_{x,z}(\mathbf{x},\mathbf{z})\log\frac{p_{x,z}(\mathbf{x},\mathbf{z})}{q_{x,z}(\mathbf{x},\mathbf{x})}\mathrm{d}(\mathbf{x},\mathbf{z})\] \[= \int p_{x,z}(\mathbf{x},\mathbf{z})\left(\log\frac{p_{x}(\mathbf{x})}{q_{x}( \mathbf{x})}+\log\frac{p_{z|x}(\mathbf{z}|\mathbf{x})}{q_{z|x}(\mathbf{z}|\mathbf{x})}\right) \mathrm{d}(\mathbf{x},\mathbf{z})\] \[= \int p_{x,z}(\mathbf{x},\mathbf{z})\log\frac{p_{x}(\mathbf{x})}{q_{x}(\mathbf{x}) }\mathrm{d}(\mathbf{x},\mathbf{z})+\int p_{x}(\mathbf{x})\int p_{z|x}(\mathbf{z}|\mathbf{x})\log \frac{p_{z|x}(\mathbf{z}|\mathbf{x})}{q_{z|x}(\mathbf{z}|\mathbf{x})}\mathrm{d}\mathbf{z}\mathrm{ d}\mathbf{x}\] \[= \mathrm{KL}\left(p_{x}\middle\lVert q_{x}\right)+\mathbb{E}_{\bm {x}\sim p_{x}}\left[\mathrm{KL}\left(p_{z|x}(\cdot|\mathbf{x})\middle\lVert q _{z|x}(\cdot|\mathbf{x})\right)\right]\geq\mathrm{KL}\left(p_{x}\middle\lVert q _{x}\right),\]

where the last inequality follows from the fact

\[\mathrm{KL}\left(p_{z|x}(\cdot|\mathbf{z})\middle\lVert\tilde{p}_{z|x}(\cdot|\mathbf{x} )\right)\geq 0\quad\forall\,\mathbf{x}.\]

With a similar technique, it can be obtained that

\[\mathrm{KL}\left(p_{x,z}\middle\lVert q_{x,z}\right)= \int p_{x,z}(\mathbf{x},\mathbf{z})\log\frac{p_{x,z}(\mathbf{x},\mathbf{z})}{q_{x,z}(\mathbf{x},\mathbf{z})}\mathrm{d}(\mathbf{x},\mathbf{z})\] \[= \int p_{x,z}(\mathbf{x},\mathbf{z})\left(\log\frac{p_{z}(\mathbf{z})}{q_{z}( \mathbf{z})}+\log\frac{p_{x|z}(\mathbf{x}|\mathbf{z})}{q_{x|z}(\mathbf{x}|\mathbf{z})}\right) \mathrm{d}(\mathbf{x},\mathbf{z})\] \[= \int p_{x,z}(\mathbf{x},\mathbf{z})\log\frac{p_{z}(\mathbf{z})}{q_{z}(\mathbf{z}) }\mathrm{d}(\mathbf{x},\mathbf{z})+\int p_{z}(\mathbf{z})\int p_{x|z}(\mathbf{x}|\mathbf{z})\log \frac{p_{x|z}(\mathbf{x}|\mathbf{z})}{q_{x|z}(\mathbf{x}|\mathbf{z})}\mathrm{d}\mathbf{z}\mathrm{ d}\mathbf{x}\] \[= \mathrm{KL}\left(p_{z}\middle\lVert q_{z}\right)+\mathbb{E}_{\bm {x}\sim p_{z}}\left[\mathrm{KL}\left(p_{x|z}(\cdot|\mathbf{z})\middle\lVert \tilde{p}_{x|z}(\cdot|\mathbf{z})\right)\right].\]

Hence, the proof is completed.

Proof of Lemma 3.2.: This Lemma uses nearly the same techniques as those in Lemma B.2, while it may have a better smoothness dependency in convergence since the chain rule of KL divergence. Hence, we will omit several steps overlapped in Lemma B.2.

For any \(k\in\{0,1,\ldots,K-1\}\), let \(\hat{p}_{(k+1)\eta,k\eta}\) and \(p_{(k+1)\eta,k\eta}^{\leftarrow}\) denote the joint distribution of \((\hat{\mathbf{x}}_{(k+1)\eta},\hat{\mathbf{x}}_{k\eta})\) and \((\mathbf{x}_{(k+1)\eta}^{\leftarrow},\mathbf{x}_{k\eta}^{\leftarrow})\), which we write in term of the conditionals and marginals as

\[\hat{p}_{(k+1)\eta,k\eta}(\boldsymbol{x}^{\prime},\boldsymbol{x}) =\hat{p}_{(k+1)\eta|k\eta}(\boldsymbol{x}^{\prime}|\boldsymbol{x}) \cdot\hat{p}_{k\eta}(\boldsymbol{x})=\hat{p}_{k\eta|(k+1)\eta}(\boldsymbol{x}| \boldsymbol{x}^{\prime})\cdot\hat{p}_{(k+1)\eta}(\boldsymbol{x}^{\prime})\] \[p_{(k+1)\eta,k\eta}^{\leftarrow}(\boldsymbol{x}^{\prime}, \boldsymbol{x})=p_{(k+1)\eta|k\eta}^{\leftarrow}(\boldsymbol{x}^{\prime}| \boldsymbol{x})\cdot p_{k\eta}^{\leftarrow}(\boldsymbol{x})=p_{k\eta|(k+1)\eta }^{\leftarrow}(\boldsymbol{x}|\boldsymbol{x}^{\prime})\cdot p_{(k+1)\eta}^{ \leftarrow}(\boldsymbol{x}^{\prime}).\]

Besides, we consider a reference Markov process \(\{\tilde{\mathbf{x}}_{k}\}\) whose initial marginal distribution and transition kernels satisfy

\[\tilde{\mathbf{x}}_{0}\sim\tilde{p}_{0}=\hat{p}_{0}\quad\text{and}\quad\tilde{ p}_{(k+1)\eta|k\eta}(\boldsymbol{x}^{\prime}|\boldsymbol{x})=p_{(k+1)\eta|k\eta} ^{\leftarrow}(\boldsymbol{x}^{\prime}|\boldsymbol{x}).\]

Under these conditions, we have

\[\operatorname{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq\operatorname{TV}\left( \hat{p}_{K\eta},\tilde{p}_{K\eta}\right)+\operatorname{TV}\left(\tilde{p}_{K \eta},p_{*}\right).\]

Since \(\{\tilde{\mathbf{x}}_{k}\}\) and \(\{\mathbf{x}_{k}^{\leftarrow}\}\) share the same transition kernel, then we have

\[\begin{split}&\operatorname{TV}\left(\tilde{p}_{K\eta},p_{*}\right) \leq\operatorname{TV}\left(\tilde{p}_{0},p_{0}^{\leftarrow}\right)\leq\sqrt{ \frac{1}{2}\mathrm{KL}\left(p_{0}^{\leftarrow}\left\|\tilde{p}_{0}^{ \leftarrow}\right)}\right.\\ &=\sqrt{\frac{1}{2}\mathrm{KL}\left(p_{K\eta}\left\|p_{\infty} \right)}\leq\sqrt{(1+L^{2})d+\left\|\nabla f_{*}(\boldsymbol{0})\right\|^{2} \cdot\exp(-K\eta)},\end{split} \tag{13}\]

where the first inequality follows from the chain rule of TV distance, the second inequality follows from Pinsker's inequality, and the last inequality follows from Lemma E.2. Besides, we have

\[\begin{split}&\operatorname{TV}\left(\hat{p}_{K\eta},\tilde{p}_{ K\eta}\right)\leq\sqrt{\frac{1}{2}\mathrm{KL}\left(\hat{p}_{K\eta}\left\| \tilde{p}_{K\eta}\right)}\leq\sqrt{\frac{1}{2}\mathrm{KL}\left(\hat{p}_{K\eta,(K-1)\eta}\left\|\tilde{p}_{K\eta,(K-1)\eta}\right)}\right.\\ &\leq\sqrt{\frac{1}{2}\mathrm{KL}\left(\hat{p}_{(K-1)\eta}\left\| \tilde{p}_{(K-1)\eta}\right)+\frac{1}{2}\mathbb{E}_{\hat{\mathbf{x}}\sim\hat{p }_{(K-1)\eta}}\left[\mathrm{KL}\left(\hat{p}_{K\eta|(K-1)\eta}(\cdot|\hat{ \mathbf{x}})\right)\right]\right.}\end{split}\]

where the first inequality follows from Pinsker's inequality, the second and the third inequalities follow from Lemma B.4. By using this inequality recursively, we have

\[\begin{split}\operatorname{TV}\left(\hat{p}_{K\eta},\tilde{p}_ {K\eta}\right)\leq&\sqrt{\frac{1}{2}\mathrm{KL}\left(\hat{p}_{0 }\left\|\tilde{p}_{0}\right)+\frac{1}{2}\sum_{k=0}^{K-1}\mathbb{E}_{\hat{ \mathbf{x}}\sim\hat{p}_{k\eta}}\left[\mathrm{KL}\left(\hat{p}_{(k+1)\eta|k \eta}(\cdot|\hat{\mathbf{x}})\right\|p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot| \hat{\mathbf{x}})\right)\right]}\\ =&\sqrt{\frac{1}{2}\sum_{k=0}^{K-1}\mathbb{E}_{\hat{ \mathbf{x}}\sim\hat{p}_{k\eta}}\left[\mathrm{KL}\left(\hat{p}_{(k+1)\eta|k \eta}(\cdot|\hat{\mathbf{x}})\right\|p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot| \hat{\mathbf{x}})\right)\right]}\end{split} \tag{14}\]

where the equation follows from the definition of process \(\{\tilde{\mathbf{x}}_{k}\}\). Therefore, combining Eq. 14 with Eq. 13, the proof is completed. 

**Corollary B.5**.: _For Alg 1, if we set_

\[\eta=\frac{1}{2}\cdot\log\frac{2L+1}{2L},\qquad K=4L\cdot\log\frac{(1+L^{2})d+ \left\|\nabla f_{*}(\boldsymbol{0})\right\|^{2}}{\epsilon^{2}}\]

_and_

\[\mathrm{KL}\left(\hat{p}_{(k+1)\eta|k\eta}(\cdot|\hat{\mathbf{x}})\right\|p_{( K-k-1)\eta|(K-k)\eta}(\cdot|\hat{\mathbf{x}})\right)\leq\frac{\epsilon^{2}}{4L} \cdot\left[\log\frac{(1+L^{2})d+\left\|\nabla f_{*}(\boldsymbol{0})\right\|^{2 }}{\epsilon^{2}}\right]^{-1},\]

_we have the total variation distance between the underlying distribution of Alg 1 output and the data distribution \(p_{*}\) will satisfy \(\operatorname{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq 2\epsilon\)._Proof.: According to Lemma 3.2, we have

\[\begin{split}\operatorname{TV}\left(\hat{p}_{K\eta},p_{*}\right)& \leq\underbrace{\sqrt{(1+L^{2})d+\left\|\nabla f_{*}(\mathbf{0}) \right\|^{2}}\cdot\exp(-K\eta)}_{\text{Term 1}}\\ &\quad+\underbrace{\sqrt{\frac{1}{2}\sum_{k=0}^{K-1}\mathbb{E}_{ \hat{\mathbf{x}}\sim\hat{p}_{k\eta}}\left[\operatorname{KL}\left(\hat{p}_{(k+1) \eta|k\eta}(\cdot|\hat{\mathbf{x}})\right)\right\|p_{(k+1)\eta|k\eta}^{\leftarrow} (\cdot|\hat{\mathbf{x}})\right]}}_{\text{Term 2}}\end{split} \tag{15}\]

To achieve the upper bound Term \(1\leq\epsilon\), we only require

\[T=K\eta\geq\frac{1}{2}\log\frac{(1+L^{2})d+\left\|\nabla f_{*}(\mathbf{0}) \right\|^{2}}{\epsilon^{2}}. \tag{16}\]

For Term 2, by choosing

\[\eta=\frac{1}{2}\log\frac{2L+1}{2L},\]

we know for all \(k\in\{0,1,\dots,K-1\}\), the conditional density \(p_{k+1|k}^{\leftarrow}(\mathbf{x}|\hat{\mathbf{x}})\) is strongly-log concave, and its score is \(3L\)-Lipschitz. In this condition, we require

\[K\coloneqq 4L\cdot\log\frac{(1+L^{2})d+\left\|\nabla f_{*}(\mathbf{0})\right\| ^{2}}{\epsilon^{2}}\]

when we suppose \(L\geq 1\) without loss of generality. Then, to achieve Term 2\(\leq\epsilon\), the sufficient condition is to require a uniform upper bound for all conditional probability approximation, i.e.,

\[\operatorname{KL}\left(\hat{p}_{(k+1)\eta|k\eta}(\cdot|\hat{\mathbf{x}})\right) \left\|p_{k+1|k}^{\leftarrow}(\cdot|\hat{\mathbf{x}})\right)\leq\frac{\epsilon^{2 }}{K}=\frac{\epsilon^{2}}{4L}\cdot\left[\log\frac{(1+L^{2})d+\left\|\nabla f_{* }(\mathbf{0})\right\|^{2}}{\epsilon^{2}}\right]^{-1}.\]

Hence, the proof is completed. 

**Remark 1**.: _To achieve the TV error tolerance shown in Corollary B.3,.i.e.,_

\[\operatorname{TV}\left(\hat{p}_{(k+1)\eta|k\eta}(\cdot|\hat{\mathbf{x}}),p_{(k+1) \eta|k\eta}^{\leftarrow}(\cdot|\hat{\mathbf{x}})\right)\leq\frac{\epsilon}{4L} \cdot\left[\log\frac{(1+L^{2})d+\left\|\nabla f_{*}(\mathbf{0})\right\|^{2}}{ \epsilon^{2}}\right]^{-1},\]

_it requires the KL divergence error to satisfy_

\[\begin{split}\operatorname{TV}\left(\hat{p}_{(k+1)\eta|k\eta}( \cdot|\hat{\mathbf{x}}),p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot|\hat{\mathbf{x}})\right) &\leq\sqrt{\frac{1}{2}\operatorname{KL}\left(\hat{p}_{(k+1) \eta|k\eta}(\cdot|\hat{\mathbf{x}})\right\|p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot |\hat{\mathbf{x}})\right)}\\ &\leq\frac{\epsilon^{2}}{16L^{2}}\cdot\left[\log\frac{(1+L^{2})d+ \left\|\nabla f_{*}(\mathbf{0})\right\|^{2}}{\epsilon^{2}}\right]^{-2}.\end{split}\]

_Compared with the results shown in Corollary B.5, this result requires a higher accuracy with an \(\mathcal{O}(L)\) factor, which is not acceptable sometimes._

**Lemma B.6**.: _Suppose Assumption **[A1]-[A2]** hold, the choice of \(\eta\) keeps the same as that in Corollary B.5, and the second moment of the underlying distribution of \(\hat{\mathbf{x}}_{k\eta}\) is \(M_{k}\), then we have_

\[M_{k+1}\leq\frac{2\delta_{k}}{L}+16(d+m_{2}^{2})+24M_{k}.\]

Proof.: Considering the second moment of \(\hat{\mathbf{x}}_{(k+1)\eta}\), we have

\[\begin{split}\mathbb{E}_{\hat{p}_{(k+1)\eta}}\left[\left\|\hat{ \mathbf{x}}_{(k+1)\eta}\right\|^{2}\right]=&\int\hat{p}_{(k+1) \eta}(\mathbf{x})\cdot\|\mathbf{x}\|^{2}\mathrm{d}\mathbf{x}\\ =&\int\left(\int\hat{p}_{k\eta}(\mathbf{y})\cdot\hat{p}_ {(k+1)\eta|k\eta}(\mathbf{x}|\mathbf{y})\mathrm{d}\mathbf{y}\right)\cdot\|\mathbf{x}\|^{2} \mathrm{d}\mathbf{x}\\ =&\int\hat{p}_{k\eta}(\mathbf{y})\cdot\int\hat{p}_{(k+1 )\eta|k\eta}(\mathbf{x}|\mathbf{y})\cdot\|\mathbf{x}\|^{2}\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{y}. \end{split} \tag{17}\]Then, we focus on the innermost integration, suppose \(\hat{\gamma}_{\mathbf{y}}(\cdot,\cdot)\) as the optimal coupling between \(\hat{p}_{(k+1)\eta|k\eta}(\cdot\left\|\mathbf{y}\right)\) and \(p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot\left\|\mathbf{y}\right)\). Then, we have

\[\begin{split}&\int\hat{p}_{(k+1)\eta|k\eta}(\mathbf{x}|\mathbf{y})\left\| \mathbf{x}\right\|^{2}\mathrm{d}\mathbf{x}-2\int p_{(k+1)\eta|k\eta}^{\leftarrow}(\mathbf{x }|\mathbf{y})\left\|\mathbf{x}\right\|^{2}\mathrm{d}\mathbf{x}\\ &\leq\int\hat{\gamma}_{\mathbf{y}}(\hat{\mathbf{x}},\mathbf{x})\left(\left\| \hat{\mathbf{x}}\right\|^{2}-2\left\|\mathbf{x}\right\|^{2}\right)\mathrm{d}(\hat{\bm {x}},\mathbf{x})\leq\int\hat{\gamma}_{\mathbf{y}}(\hat{\mathbf{x}},\mathbf{x})\left\|\hat{\mathbf{ x}}-\mathbf{x}\right\|^{2}\mathrm{d}(\hat{\mathbf{x}},\mathbf{x})\\ &=W_{2}^{2}\left(\hat{p}_{(k+1)\eta|k\eta},p_{(k+1)\eta|k\eta}^{ \leftarrow}\right).\end{split} \tag{18}\]

Since \(p_{(k+1)\eta|k\eta}^{\leftarrow}\) is strongly log-concave, i.e.,

\[-\nabla_{\mathbf{x}}^{2}\log p_{(k+1)\eta|k\eta}^{\leftarrow}(\mathbf{x}|\hat{\mathbf{x}} )=\nabla^{2}f_{(K-k-1)\eta}(\mathbf{x})+\frac{e^{-2\eta}}{1-e^{-2\eta}}\cdot\mathbf{I} \succeq L\mathbf{I},\]

the distribution \(p_{(k+1)\eta|k\eta}^{\leftarrow}\) also satisfies \(1/L\) log-Sobolev inequality due to Lemma E.9. By Talagrand's inequality, we have

\[W_{2}^{2}\left(\hat{p}_{(k+1)\eta|k\eta},p_{(k+1)\eta|k\eta}^{\leftarrow}\right) \leq\frac{2}{L}\cdot\mathrm{KL}\left(\hat{p}_{k+1|k+\frac{1}{2},b}\|p_{k+1|k+ \frac{1}{2},b}\right)\coloneqq\frac{2\delta_{k}}{L}. \tag{19}\]

Plugging Eq 18 and Eq 19 into Eq 17, we have

\[\mathbb{E}\left[\left\|\hat{\mathbf{x}}_{(k+1)\eta}\right\|^{2}\right]\leq\int\hat{ p}_{k\eta}(\mathbf{y})\cdot\left(\frac{2\delta_{k}}{L}+2\int p_{(k+1)\eta|k\eta}(\mathbf{x}| \mathbf{y})\left\|\mathbf{x}\right\|^{2}\mathrm{d}\mathbf{x}\right)\mathrm{d}\mathbf{y}. \tag{20}\]

To upper bound the innermost integration, we suppose the optimal coupling between \(p_{(K-k-1)\eta}\) and \(p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot\left\|\mathbf{y}\right)\) is \(\gamma_{\mathbf{y}}(\cdot,\cdot)\). Then it has

\[\begin{split}&\int p_{(k+1)\eta|k\eta}^{\leftarrow}(\mathbf{x}|\mathbf{y}) \left\|\mathbf{x}\right\|^{2}\mathrm{d}\mathbf{x}-2\int p_{(K-k-1)\eta}(\mathbf{x})\left\| \mathbf{x}\right\|^{2}\mathrm{d}\mathbf{x}\\ &\leq\int\gamma_{\mathbf{y}}(\mathbf{x}^{\prime},\mathbf{x})\left(\left\|\mathbf{x }^{\prime}\right\|^{2}-2\left\|\mathbf{x}\right\|^{2}\right)\mathrm{d}(\mathbf{x}^{ \prime},\mathbf{x})\leq\int\gamma_{\mathbf{y}}(\mathbf{x}^{\prime},\mathbf{x})\left\|\mathbf{x}^{ \prime}-\mathbf{x}\right\|^{2}\mathrm{d}(\mathbf{x}^{\prime},\mathbf{x})\\ &=W_{2}^{2}(p_{(K-k-1)\eta},p_{(k+1)\eta|k\eta}^{\leftarrow}) \end{split} \tag{21}\]

Since \(p_{(k+1)\eta|k\eta}^{\leftarrow}\) satisfies LSI with constant \(1/L\). By Talagrand's inequality and LSI, we have

\[\begin{split}& W_{2}^{2}(p_{(K-k-1)\eta},p_{(k+1)\eta|k\eta}^{ \leftarrow})\leq\frac{2}{L}\cdot\mathrm{KL}\left(p_{(K-k-1)\eta}\|p_{(k+1) \eta|k\eta}\right)\\ &\leq\frac{4}{L^{2}}\cdot\int p_{(K-k-1)\eta}(\mathbf{x})\cdot\left\| \nabla\log\frac{p_{(K-k-1)\eta}(\mathbf{x})}{p_{(k+1)\eta|k\eta}^{\leftarrow}(\mathbf{ x}|\mathbf{y})}\right\|^{2}\mathrm{d}\mathbf{x}\\ &=\frac{4}{L^{2}}\cdot\int p_{(K-k-1)\eta}(\mathbf{x})\cdot\left\| \frac{e^{-\eta}\mathbf{y}-e^{-2\eta}\mathbf{x}}{1-e^{-2\eta}}\right\|^{2}\mathrm{d}\bm {x}\\ &\leq 12\left\|\mathbf{y}\right\|^{2}+8\int p_{(K-k-1)\eta}(\mathbf{x})\| \mathbf{x}\|^{2}\mathrm{d}\mathbf{x}\\ &\leq 12\|\mathbf{y}\|^{2}+8(d+m_{2}^{2}).\end{split}\]

where the last inequality follows from the choice of \(\eta=1/2\cdot\log(2L+1)/2L\) and the fact \(E_{p_{(K-k-1)\eta}}[\|\mathbf{x}\|^{2}]\leq(d+m_{2}^{2})\) obtained by Lemma E.7. Plugging this results into Eq. 20, we have

\[\mathbb{E}\left[\left\|\hat{\mathbf{x}}_{(k+1)\eta}\right\|^{2}\right]\leq\frac{2 \delta_{k}}{L}+16(d+m_{2}^{2})+24\cdot\mathbb{E}\left[\left\|\hat{\mathbf{x}}_{k \eta}\right\|^{2}\right].\]

## Appendix C Implement RTK inference with MALA

In this section, we consider introducing a MALA variant to sample from \(p_{k+1|k}^{\leftarrow}(\mathbf{z}|\mathbf{x}_{0})\). To simplify the notation, we set

\[g(\mathbf{z})\coloneqq f_{(K-k-1)\eta}(\mathbf{z})+\frac{\left\|\mathbf{x}_{0}-\mathbf{z}\cdot e ^{-\eta}\right\|^{2}}{2(1-e^{-2\eta})} \tag{22}\]and consider \(k\) and \(\mathbf{x}_{0}\) to be fixed. Besides, we set

\[p^{\leftarrow}(\mathbf{z}|\mathbf{x}_{0})\coloneqq p^{\leftarrow}_{k+1|k}(\mathbf{z}|\mathbf{x}_ {0})\propto\exp(-g(\mathbf{z}))\]

According to Corollary B.5 and Corollary B.3, when we choose

\[\eta=\frac{1}{2}\log\frac{2L+1}{2L},\]

the log density \(g\) will be \(L\)-strongly log-concave and \(3L\)-smooth. With the following two approximations,

\[\mathbf{s}_{\mathbf{\theta}}(\mathbf{z})\approx\nabla g(\mathbf{z})\quad\text{and}\quad r_{ \mathbf{\theta}^{\prime}}(\mathbf{z},\mathbf{z}^{\prime})\approx g(\mathbf{z})-g(\mathbf{z}^{\prime }), \tag{23}\]

We left the approximation level here and determined when we needed the detailed analysis. we can use the following Algorithm to replace Line 3 of Alg. 1.

In this section, we introduce several notations about three transition kernels presenting the standard, the projected, and the ideally projected implementation of Alg. 2.

Standard implementation of Alg. 2.According to Step 4, the transition distribution satisfies

\[Q_{\mathbf{z}_{s}}=\mathcal{N}\left(\mathbf{z}_{s}-\tau\cdot s_{\theta}(\mathbf{z}_{s}),2 \tau\right) \tag{24}\]

with a density function

\[q(\tilde{\mathbf{z}}_{s}|\mathbf{z}_{s})=\varphi_{2\tau}\left(\tilde{\mathbf{z}}_{s}-(\bm {z}_{s}-\tau\cdot s_{\theta}(\mathbf{z}_{s}))\right). \tag{25}\]

Considering a \(1/2\)-lazy version of the update, we set

\[\mathcal{T}^{\prime}_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime})=\frac{1}{2}\cdot \delta_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime})+\frac{1}{2}\cdot Q_{\mathbf{z}_{s}} (\mathrm{d}\mathbf{z}^{\prime}). \tag{26}\]

Then, with the following Metropolis-Hastings filter,

\[a_{\mathbf{z}_{s}}(\mathbf{z}^{\prime})=\min\left\{1,\frac{q(\mathbf{z}_{s}|\mathbf{z}^{ \prime})}{q(\mathbf{z}^{\prime}|\mathbf{z}_{s})}\cdot\exp\left(-r_{\theta}(\mathbf{z}^{ \prime},\mathbf{z}_{s})\right)\right\}\quad\text{where}\quad a_{\mathbf{z}_{s}}(\mathbf{z }^{\prime})=a(\mathbf{z}^{\prime}-(\mathbf{z}_{s}-\tau\cdot s_{\theta}(\mathbf{z}_{s})),\bm {z}_{s}), \tag{27}\]

the transition kernel for the standard implementation of Alg, 2 will be

\[\mathcal{T}_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}_{s+1})=\mathcal{T}^{\prime}_{\mathbf{z} _{s}}(\mathrm{d}\mathbf{z}_{s+1})\cdot a_{\mathbf{z}_{s}}(\mathbf{z}_{s+1})+\left(1-\int a _{\mathbf{z}_{s}}(\mathbf{z}^{\prime})\mathcal{T}^{\prime}_{\mathbf{z}_{s}}(\mathrm{d}\bm {z}^{\prime})\right)\cdot\delta_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}_{s+1}). \tag{28}\]

Projected implementation of Alg. 2.According to Step 4, the transition distribution satisfies

\[\tilde{Q}_{\mathbf{z}_{s}}=\mathcal{N}\left(\mathbf{z}_{s}-\tau\cdot s_{\theta}(\mathbf{z} _{s}),2\tau\right)\]

with a density function

\[\tilde{q}(\tilde{\mathbf{z}}_{s}|\mathbf{z}_{s})=\varphi_{2\tau}\left(\tilde{\mathbf{z}}_ {s}-(\mathbf{z}_{s}-\tau\cdot\nabla s_{\theta}(\mathbf{z}_{s}))\right).\]

Considering the projection operation, i.e., Step 5 in Alg 1, if we suppose the feasible set

\[\Omega=\mathcal{B}(\mathbf{0},R)\quad\text{and}\quad\Omega_{\mathbf{z}}=\mathcal{B}( \mathbf{z},r)\cap\mathcal{B}(\mathbf{0},R)\]

the transition distribution becomes

\[\tilde{Q}^{\prime}_{\mathbf{z}_{s}}(\mathcal{A})=\int_{\mathcal{A}\cap\Omega_{\bm {z}_{s}}}\tilde{Q}_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime})+\int_{\mathcal{A} -\Omega_{\mathbf{z}_{s}}}\tilde{Q}_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime})\cdot \delta_{\mathbf{z}_{s}}(\mathcal{A}).\]

Hence, a \(1/2\)-lazy version of the transition distribution becomes

\[\tilde{\mathcal{T}}^{\prime}_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime})=\frac{1}{ 2}\cdot\delta_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime})+\frac{1}{2}\cdot\tilde{ Q}^{\prime}_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime}).\]

Then, with the following Metropolis-Hastings filter,

\[\tilde{a}_{\mathbf{z}_{s}}(\mathbf{z}^{\prime})=\min\left\{1,\frac{\tilde{q}(\mathbf{z}_{ s}|\mathbf{z}^{\prime})}{\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z}_{s})}\cdot\exp\left(-r_{ \theta}(\mathbf{z}^{\prime},\mathbf{z}_{s})\right)\right\}\quad\text{where}\quad \tilde{a}_{\mathbf{z}_{s}}(\mathbf{z}^{\prime})=a(\mathbf{z}^{\prime}-(\mathbf{z}_{s}-\tau \cdot s_{\theta}(\mathbf{z}_{s})),\mathbf{z}_{s}),\]

the transition kernel for the projected implementation of Alg, 2 will be

\[\tilde{\mathcal{T}}_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}_{s+1})=\tilde{\mathcal{T}}^{ \prime}_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}_{s+1})\cdot\tilde{a}_{\mathbf{z}_{s}}(\mathbf{z} _{s+1})+\left(1-\int_{\Omega}\tilde{a}_{\mathbf{z}_{s}}(\mathbf{z}^{\prime})\tilde{ \mathcal{T}}^{\prime}_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime})\right)\cdot\delta_{ \mathbf{z}_{s}}(\mathrm{d}\mathbf{z}_{s+1}).\]Ideally projected implementation of Alg. 2.In this condition, we know the accurate \(g(\mathbf{z})-g(\mathbf{z}^{\prime})\) and \(\nabla g(\mathbf{z})\). In this condition, the ULA step will provide

\[\tilde{Q}_{*,\mathbf{z}_{s}}=\mathcal{N}\left(\mathbf{z}_{s}-\tau\cdot\nabla g(\mathbf{z}_{s }),2\tau\right) \tag{29}\]

with a density function

\[\tilde{q}_{*}(\tilde{\mathbf{z}}_{s}|\mathbf{z}_{s})=\varphi_{2\tau}\left(\tilde{\mathbf{z} }_{s}-(\tau\cdot\nabla g(\mathbf{z}_{s}))\right).\]

Considering the projection operation, i.e., Step 5 in Alg 1, the transition distribution becomes

\[\tilde{Q}^{\prime}_{*,\mathbf{z}_{s}}(\mathcal{A})=\int_{\mathcal{A}\cap\Omega_{ \mathbf{z}_{s}}}\tilde{Q}_{*,\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime})+\int_{ \mathcal{A}-\Omega_{\mathbf{z}_{s}}}\tilde{Q}_{*,\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{ \prime})\cdot\delta_{\mathbf{z}_{s}}(\mathcal{A}). \tag{30}\]

Hence, a \(1/2\)-lazy version of the transition distribution becomes

\[\tilde{\mathcal{T}}^{\prime}_{*,\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime})=\frac{ 1}{2}\cdot\delta_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime})+\frac{1}{2}\cdot \tilde{Q}^{\prime}_{*,\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime}). \tag{31}\]

Then, with the following Metropolis-Hastings filter,

\[\tilde{a}_{*,\mathbf{z}_{s}}(\mathbf{z}^{\prime})=\min\left\{1,\frac{\tilde{q}_{*}(\bm {z}_{s}|\mathbf{z}^{\prime})}{\tilde{q}_{*}(\mathbf{z}^{\prime}|\mathbf{z}_{s})}\cdot\exp \left(-\left(g(\mathbf{z}^{\prime})-g(\mathbf{z}_{s})\right)\right)\right\}, \tag{32}\]

the transition kernel for the accurate projected update will be

\[\tilde{\mathcal{T}}_{*,\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}_{s+1})=\tilde{\mathcal{T}} ^{\prime}_{*,\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}_{s+1})\cdot\tilde{a}_{*,\mathbf{z}_{s}}( \mathbf{z}_{s+1})+\left(1-\int_{\Omega}\tilde{a}_{*,\mathbf{z}_{s}}(\mathbf{z}^{\prime}) \tilde{\mathcal{T}}^{\prime}_{*,\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime})\right) \cdot\delta_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}_{s+1}). \tag{33}\]

**Lemma C.1**.: _Suppose we have_

\[\eta=\frac{1}{2}\log\frac{2L+1}{2L},\]

_then the target distribution of the Inner MALA, i.e., \(p^{\leftarrow}(\mathbf{z}|\mathbf{x}_{0})\) will be \(L\)-strongly log-concave and \(3L\)-smooth for any given \(\mathbf{x}_{0}\)._

Proof.: Consider the energy function \(g(\mathbf{z})\) of \(p^{\leftarrow}(\mathbf{z}|\mathbf{x}_{0})\), we have

\[g(\mathbf{z})=f_{(K-k-1)\eta}(\mathbf{z})+\frac{\left\|\mathbf{x}_{0}-\mathbf{z}\cdot e^{-\eta }\right\|^{2}}{2(1-e^{-2\eta})}\]

whose Hessian matrix satisfies

\[\left(\frac{e^{-2\eta}}{(1-e^{-2\eta})}+L\right)\cdot\mathbf{I}\succeq\nabla^{2}g( \mathbf{z})=\nabla^{2}f_{(K-k-1)}(\mathbf{z})+\frac{e^{-2\eta}}{(1-e^{-2\eta})}\cdot \mathbf{I}\succeq\left(\frac{e^{-2\eta}}{(1-e^{-2\eta})}-L\right)\cdot\mathbf{I}.\]

Under these conditions, if we have

\[\eta\leq\frac{1}{2}\log\frac{2L+1}{2L}\quad\Leftrightarrow\quad\frac{e^{-2 \eta}}{1-e^{-2\eta}}\geq 2L,\]

which means

\[\frac{3e^{-2\eta}}{2(1-e^{-2\eta})}\succeq\nabla^{2}g(\mathbf{z})\succeq\frac{e^{- 2\eta}}{2(1-e^{-2\eta})}.\]

For the analysis convenience, we set

\[\eta=\frac{1}{2}\log\frac{2L+1}{2L},\]

that is to say \(g(\mathbf{z})\) is \(L\)-strongly convex and \(3L\)-smooth.

### Control the error from the projected transition kernel

Here, we consider the marginal distribution of \(\{\mathbf{z}_{s}\}\) and \(\{\tilde{\mathbf{z}}_{s}\}\) to be the random process when Alg. 2 is implemented by the standard and projected version, respectively. The underlying distributions of these two processes are denoted as \(\mathbf{z}_{s}\sim\mu_{s}\) and \(\tilde{\mathbf{z}}_{s}\sim\tilde{\mu}_{s}\), and we would like to upper bound \(\operatorname{TV}\left(\mu_{S},\tilde{\mu}_{S}\right)\) for any given \(\mathbf{x}_{0}\).

Rewrite the formulation of \(\mathbf{z}_{S}\), we have

\[\mathbf{z}_{S}=\tilde{\mathbf{z}}_{S}\cdot\mathbf{1}\left(\mathbf{z}_{S}= \tilde{\mathbf{z}}_{S}\right)+\mathbf{z}_{S}\cdot\mathbf{1}\left(\mathbf{z}_{ S}\neq\tilde{\mathbf{z}}_{S}\right)\]

where \(\mathbf{1}(\cdot)\) is the indicator function. In this condition, for any set \(\mathcal{A}\), we have

\[\mathbf{1}\left(\mathbf{z}_{S}\in\mathcal{A}\right)= \mathbf{1}\left(\tilde{\mathbf{z}}_{S}\in\mathcal{A}\right)\cdot \mathbf{1}\left(\mathbf{z}_{S}=\tilde{\mathbf{z}}_{S}\right)+\mathbf{1} \left(\mathbf{z}_{S}\in\mathcal{A}\right)\cdot\mathbf{1}\left(\mathbf{z}_{S} \neq\tilde{\mathbf{z}}_{S}\right)\] \[= \mathbf{1}\left(\tilde{\mathbf{z}}_{S}\in\mathcal{A}\right)- \mathbf{1}\left(\tilde{\mathbf{z}}_{S}\in\mathcal{A}\right)\cdot\mathbf{1} \left(\mathbf{z}_{S}\neq\tilde{\mathbf{z}}_{S}\right)+\mathbf{1}\left(\mathbf{z }_{S}\in\mathcal{A}\right)\cdot\mathbf{1}\left(\mathbf{z}_{S}\neq\tilde{ \mathbf{z}}_{S}\right),\]

which means

\[-\mathbf{1}\left(\tilde{\mathbf{z}}_{S}\in\mathcal{A}\right)\cdot\mathbf{1} \left(\mathbf{z}_{S}\neq\tilde{\mathbf{z}}_{S}\right)\leq\mathbf{1}\left( \mathbf{z}_{S}\in\mathcal{A}\right)-\mathbf{1}(\tilde{\mathbf{z}}_{S}\in \mathcal{A})\leq\mathbf{1}\left(\mathbf{z}_{S}\in\mathcal{A}\right)\cdot \mathbf{1}\left(\mathbf{z}_{S}\neq\tilde{\mathbf{z}}_{S}\right).\]

Therefore, the total variation distance between \(\mu_{S}\) and \(\tilde{\mu}_{S}\) can be upper bounded with

\[\operatorname{TV}\left(\mu_{S},\tilde{\mu}_{S}\right)\leq\sup_{\mathcal{A} \subseteq\mathbb{R}^{d}}\left|\mu_{S}(\mathcal{A})-\tilde{\mu}_{S}(\mathcal{A} )\right|\leq\mathbf{1}\left(\mathbf{z}_{S}\neq\tilde{\mathbf{z}}_{S}\right).\]

Hence, to require \(\operatorname{TV}\left(\mu_{S},\tilde{\mu}_{S}\right)\leq\epsilon/4\) a sufficient condition is to consider \(\Pr[\mathbf{z}_{S}\neq\tilde{\mathbf{z}}_{S}]\). The next step is to show that, in Alg. 2, the projected version generates the same outputs as that of the standard version with probability at least \(1-\epsilon/4\). It suffices to show that with probability at least \(1-\epsilon/4\), projected MALA will accept all \(S\) iterates. In this condition, let \(\{\mathbf{z}_{1},\mathbf{z}_{2},\ldots,\mathbf{z}_{S}\}\) be the iterates generated by the standard MALA (without the projection step), our goal is to prove that with probability at least \(1-\epsilon/4\) all \(\mathbf{z}_{s}\) stay inside the region \(\mathcal{B}(\mathbf{0},R)\) and \(\|\mathbf{z}_{s}-\mathbf{z}_{s-1}\|\leq r\) for all \(s\leq S\). That means we need to prove the following two facts

1. With probability at least \(1-\epsilon/8\), all iterates stay inside the region \(\mathcal{B}(\mathbf{0},R)\).
2. With probability at least \(1-\epsilon/8\), \(\|\mathbf{x}_{s}-\mathbf{x}_{s-1}\|\leq r\) for all \(s\leq S\).

**Lemma C.2**.: _Let \(\mu_{S}\) and \(\tilde{\mu}_{S}\) be distributions of the outputs of standard and projected implementation of Alg. 2. For any \(\epsilon\in(0,1)\), we set_

\[R\geq\max\left\{8\cdot\sqrt{\frac{\|\nabla g(\mathbf{0})\|^{2}}{L^{2}}+\frac{d }{L}},63\cdot\sqrt{\frac{d}{L}\log\frac{16S}{\epsilon}}\right\},\quad r\geq( \sqrt{2}+1)\cdot\sqrt{\tau d}+2\sqrt{\tau\log\frac{8S}{\epsilon}}\]

_where \(\mathbf{z}_{*}\) is denoted as the global optimum of the energy function, i.e., \(g\), defined in Eq. 22. Suppose \(\operatorname{P}(\|\mathbf{z}_{0}\|\geq R/2)\leq\epsilon/4\) and set_

\[\tau\leq\min\left\{\frac{d}{(3LR+\|\nabla g(\mathbf{0})\|+\epsilon_{\mathrm{ score}})^{2}},\frac{16d}{L^{2}R^{2}}\right\}=\frac{d}{(3LR+\|\nabla g(\mathbf{0})\|+ \epsilon_{\mathrm{score}})^{2}},\]

_then we have_

\[\operatorname{TV}\left(\mu_{S},\tilde{\mu}_{S}\right)\leq\frac{\epsilon}{4}.\]

Proof.: We borrow the proof techniques provided in Lemma 6.1 of [41] to control the TVD gap between the standard and the projected implementation of Alg. 2.

Particles stay inside \(\mathcal{B}(\mathbf{0},R)\).We first consider the expectation of \(\left\|\mathbf{z}_{s+1}\right\|^{2}\) when \(\mathbf{z}_{s}\) is given, and have

\[\begin{split}&\mathbb{E}\left[\left\|\mathbf{z}_{s+1}\right\|^{2} \left|\mathbf{z}_{s}\right|\right]=\int\left\|\mathbf{z}^{\prime}\right\|^{2}\mathcal{ T}_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime})\\ &=\int\left\|\mathbf{z}^{\prime}\right\|^{2}\cdot\left[\mathcal{T}_{ \mathbf{z}_{s}}^{\prime}(\mathrm{d}\mathbf{z}^{\prime})\cdot a_{\mathbf{z}_{s}}(\mathbf{z}^{ \prime})+\left(1-\int a_{\mathbf{z}_{s}}(\bar{\mathbf{z}})\mathcal{T}_{\mathbf{z}_{s}}^{ \prime}(\mathrm{d}\bar{\mathbf{z}})\right)\delta_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{ \prime})\right]\\ &=\left\|\mathbf{z}_{s}\right\|^{2}+\int\left(\left\|\mathbf{z}^{\prime} \right\|^{2}-\left\|\mathbf{z}_{s}\right\|^{2}\right)\cdot a_{\mathbf{z}_{s}}(\mathbf{z}^{ \prime})\mathcal{T}_{\mathbf{z}_{s}}^{\prime}(\mathrm{d}\mathbf{z}^{\prime})\cdot\\ &=\left\|\mathbf{z}_{s}\right\|^{2}+\int\left(\left\|\mathbf{z}^{\prime} \right\|^{2}-\left\|\mathbf{z}_{s}\right\|^{2}\right)\cdot a_{\mathbf{z}_{s}}(\mathbf{z}^{ \prime})\cdot\left(\frac{1}{2}\cdot\delta_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{ \prime})+\frac{1}{2}\cdot Q_{\mathbf{z}_{s}}(\mathrm{d}\mathbf{z}^{\prime})\right)\\ &=\left\|\mathbf{z}_{s}\right\|^{2}+\frac{1}{2}\int\left(\left\|\mathbf{z }^{\prime}\right\|^{2}-\left\|\mathbf{z}_{s}\right\|^{2}\right)\cdot\min\left\{q( \mathbf{z}^{\prime}|\mathbf{z}_{s}),q(\mathbf{z}_{s}|\mathbf{z}^{\prime})\cdot\exp\left(-r_{ \theta}(\mathbf{z}^{\prime},\mathbf{z}_{s})\right)\right\}\mathrm{d}\mathbf{z}^{\prime}\\ &\leq\frac{1}{2}\left\|\mathbf{z}_{s}\right\|^{2}+\frac{1}{2}\int \left\|\mathbf{z}^{\prime}\right\|^{2}\cdot q(\mathbf{z}^{\prime}|\mathbf{z}_{s})\mathrm{d }\mathbf{z}^{\prime},\end{split} \tag{34}\]

where the second equation follows from Eq. 28, the forth equation follows from Eq. 26 and the fifth equation follows from Eq. 27 and Eq. 25. Note that \(q(\mathbf{z}^{\prime}|\mathbf{z}_{s})\) is a Gaussian-type distribution whose mean and variance are \(\mathbf{z}_{s}-\tau\cdot s_{\theta}(\mathbf{z}_{s})\) and \(2\tau\) respectively. It means

\[\int\left\|\mathbf{z}^{\prime}\right\|^{2}\cdot q(\mathbf{z}^{\prime}|\mathbf{z}_{s}) \mathrm{d}\mathbf{z}^{\prime}=\left\|\mathbf{z}_{s}-\tau\cdot s_{\theta}(\mathbf{z}_{s}) \right\|^{2}+2\tau d. \tag{35}\]

Suppose \(\mathbf{z}_{s}\) is the global optimum of the function \(g\) due to Lemma C.1, we have

\[\begin{split}&\left\|\mathbf{z}_{s}-\tau\cdot s_{\theta}(\mathbf{z}_{s}) \right\|^{2}=\left\|\mathbf{z}_{s}\right\|^{2}-2\tau\cdot\mathbf{z}_{s}^{\top}s_{ \theta}(\mathbf{z}_{s})+\tau^{2}\cdot\left\|s_{\theta}(\mathbf{z}_{s})\right\|^{2}\\ &=\left\|\mathbf{z}_{s}\right\|^{2}-2\tau\cdot\mathbf{z}_{s}^{\top}\nabla g (\mathbf{z}_{s})+2\tau\cdot\mathbf{z}_{s}^{\top}\left(s_{\theta}(\mathbf{z}_{s})-\nabla g (\mathbf{z}_{s})\right)+\tau^{2}\cdot\left\|s_{\theta}(\mathbf{z}_{s})-\nabla g(\mathbf{z} _{s})+\nabla g(\mathbf{z}_{s})\right\|^{2}\\ &\leq\left\|\mathbf{z}_{s}\right\|^{2}-2\tau\cdot\left(\frac{L\left\| \mathbf{z}_{s}\right\|^{2}}{2}-\frac{\left\|\nabla g(\mathbf{0})\right\|^{2}}{2L} \right)+\tau^{2}\cdot\left\|\mathbf{z}_{s}\right\|^{2}+\left\|s_{\theta}(\mathbf{z}_{ s})-\nabla g(\mathbf{z}_{s})\right\|^{2}\\ &\quad+2\tau^{2}\cdot\left\|\nabla g(\mathbf{z}_{s})\right\|^{2}+2\tau ^{2}\cdot\left\|s_{\theta}(\mathbf{z}_{s})-\nabla g(\mathbf{z}_{s})\right\|^{2}\\ &=\left(1-L\tau+\tau^{2}\right)\cdot\left\|\mathbf{z}_{s}\right\|^{2} +\tau\cdot\left\|\nabla g(\mathbf{0})\right\|^{2}/L+(1+2\tau^{2})\epsilon_{\mathrm{ score}}^{2}+2\tau^{2}\cdot\left\|\nabla g(\mathbf{z}_{s})\right\|^{2}\\ &\leq\left(1-L\tau+(1+36L^{2})\cdot\tau^{2}\right)\cdot\left\| \mathbf{z}_{s}\right\|^{2}+\tau\cdot\left\|\nabla g(\mathbf{0})\right\|^{2}/L+4\tau^{ 2}\cdot\left\|\nabla g(\mathbf{0})\right\|^{2}+(1+2\tau^{2})\epsilon_{\mathrm{ score}}^{2},\end{split} \tag{36}\]

where the first inequality follows from the combination of \(L\)-strong convexity of \(g\) and Lemma E.3, the second inequality follows from the \(3L\)-smoothness of \(g\) The strong convexity and the smoothness of \(g\) follow from Lemma C.1.

Combining Eq. 34, Eq. 35 and Eq. 36, we have

\[\begin{split}\mathbb{E}\left[\left\|\mathbf{z}_{s+1}\right\|^{2} \left|\mathbf{z}_{s}\right|\right]\leq&\left(1-\frac{L\tau}{2}+\frac{1 +36L^{2}}{2}\cdot\tau^{2}\right)\cdot\left\|\mathbf{z}_{s}\right\|^{2}\\ &\quad+\left(\frac{\tau}{2L}+2\tau^{2}\right)\cdot\left\|\nabla g( \mathbf{0})\right\|^{2}+\frac{(1+2\tau^{2})\epsilon_{\mathrm{score}}^{2}}{2}+\tau d.\end{split}\]

By requiring \(\epsilon_{\mathrm{score}}\leq\tau\leq L/(2+72L^{2})<1\), we have

\[\mathbb{E}\left[\left\|\mathbf{z}_{s+1}\right\|^{2}\left|\mathbf{z}_{s}\right|\leq \left(1-\frac{L\tau}{4}\right)\cdot\left\|\mathbf{z}_{s}\right\|^{2}+\frac{\tau}{L} \cdot\left\|\nabla g(\mathbf{0})\right\|^{2}+(2+d)\tau.\]

Suppose a radio \(R\) satisfies

\[R\geq 8\cdot\sqrt{\frac{\left\|\nabla g(\mathbf{0})\right\|^{2}}{L^{2}}+\frac{d}{L}}. \tag{37}\]Then, if \(\|\mathbf{z}_{s}\|\geq R/2\geq 4\sqrt{\|\nabla g(\mathbf{0})\|^{2}/L^{2}+d/L}\), it has

\[\|\mathbf{z}_{s}\|^{2}\geq 16\cdot\left(\frac{\|\nabla g(\mathbf{0})\|^{2}}{ L^{2}}+\frac{d}{L}\right)\geq\frac{8\|\nabla g(\mathbf{0})\|^{2}}{L^{2}}+\frac{8 \cdot(2+d)}{L}\] \[\Leftrightarrow \frac{L\tau\left\|\mathbf{z}_{s}\right\|^{2}}{8}\geq\frac{\tau}{L} \cdot\left\|\nabla g(\mathbf{0})\right\|^{2}+(2+d)\tau\] \[\Leftrightarrow \mathbb{E}\left[\left\|\mathbf{z}_{s+1}\right\|^{2}\left|\mathbf{z}_{s} \right|\leq\left(1-\frac{L\tau}{8}\right)\cdot\left\|\mathbf{z}_{s}\right\|^{2}.\]

To prove \(\|\mathbf{z}_{s}\|\leq R\) for all \(s\leq S\), we only need to consider \(\mathbf{z}_{s}\) satisfying \(\|\mathbf{z}_{s}\|\geq 4\sqrt{\|\nabla g(\mathbf{0})\|^{2}/L^{2}+d/L}\), otherwise \(\|\mathbf{z}_{s}\|\leq R/2\leq R\) naturally holds. Then, by the concavity of the function \(\log(\cdot)\), for any \(\|\mathbf{z}_{s}\|\geq R/2\), we have

\[\mathbb{E}\left[\log(\|\mathbf{z}_{s+1}\|^{2})|\mathbf{z}_{s}\right]\leq\log\mathbb{E} \left[\|\mathbf{z}_{s+1}\|^{2}|\mathbf{z}_{s}\right]\leq\log(1-\frac{L\tau}{4})+\log( \|\mathbf{z}_{s}\|^{2})\leq\log(\|\mathbf{z}_{s}\|^{2})-\frac{L\tau}{4}. \tag{38}\]

Consider the random variable

\[\bar{\mathbf{z}}_{s}:=\mathbf{z}_{s}-\tau\cdot s_{\theta}(\mathbf{z}_{s})+\sqrt{2\tau} \cdot\xi\quad\mathrm{where}\quad\xi\sim\mathcal{N}(\mathbf{0},\mathbf{I})\]

obtained by the transition kernel Eq. 24, Note that \(\|\xi\|\) is the square root of a \(\chi(d)\) random variable, which is subgaussian and satisfies

\[\mathbb{P}\left[\|\xi\|\geq\sqrt{d}+\sqrt{2}t\right]\leq e^{-t^{2}}\]

for any \(t\geq 0\). Under these conditions, requiring

\[\tau\leq(3LR+G+\epsilon_{\mathrm{score}})^{-2}\cdot d\quad\mathrm{where}\quad G \coloneqq\left\|\nabla g(\mathbf{0})\right\|, \tag{39}\]

we have

\[\mathbb{P}\left[\|\mathbf{z}_{s+1}\|-\|\mathbf{z}_{s}\|\geq 3\sqrt{ \tau d}+2\sqrt{\tau}t\right]\leq\mathbb{P}\left[\|\bar{\mathbf{z}}_{s}\|-\|\mathbf{z} _{s}\|\geq 3\sqrt{\tau d}+2\sqrt{\tau}t\right] \tag{40}\] \[\leq\mathbb{P}\left[\tau\left\|s_{\theta}(\mathbf{z}_{s})\right\|+ \sqrt{2\tau}\left\|\xi\right\|\geq 3\sqrt{\tau d}+2\sqrt{\tau}t\right]\leq\mathbb{P} \left[\sqrt{2\tau}\|\xi\|\geq\sqrt{2\tau d}+2\sqrt{\tau}t\right]\leq e^{-t^{2}}.\]

In Eq. 40, the first inequality follows from the definition of transition kernel \(\mathcal{T}_{\mathbf{z}_{s}}\) shown in Eq. 28 and the second inequality follows from

\[\|\bar{\mathbf{z}}_{s}\|-\|\mathbf{z}_{s}\|\leq\tau\left\|s_{\theta}(\mathbf{z}_{s})\right\| +\sqrt{2\tau}\left\|\xi\right\|.\]

According to the fact

\[\tau\left\|s_{\theta}(\mathbf{z}_{s})\right\|\leq \tau\left\|\nabla g(\mathbf{z}_{s})\right\|+\tau\epsilon_{\mathrm{score }}\leq\tau\cdot\left(\|\nabla g(\mathbf{z}_{s})-\nabla g(\mathbf{0})\|+\|\nabla g( \mathbf{0})\|+\epsilon_{\mathrm{score}}\right) \tag{41}\] \[\leq \tau\cdot(3L\cdot\|\mathbf{z}_{s}\|+\|\nabla g(\mathbf{0})\|+\epsilon_{ \mathrm{score}})\leq\sqrt{\tau d}\]

where the second inequality follows from the smoothness of \(g\), and the last inequality follows from Eq. 39 and \(\|\mathbf{z}_{s}\|\leq R\), we have

\[3\sqrt{\tau d}+2\sqrt{\tau}t-\tau\|s_{\theta}(\mathbf{z}_{s})\|\geq\sqrt{2\tau d}+ 2\sqrt{\tau}t,\]

which implies the last inequality of Eq. 40 for all \(t\geq 0\). Furthermore, suppose \(\|\mathbf{z}_{s}\|\geq R/2\), it follows that

\[\log(\|\mathbf{z}_{s+1}\|^{2})-\log(\|\mathbf{z}_{s}\|^{2})=2\log(\|\mathbf{z}_{s+1}\|/\| \mathbf{z}_{s}\|)\leq\|\mathbf{z}_{s+1}\|/\|\mathbf{z}_{s}\|-1\leq\frac{2\|\mathbf{z}_{s+1}\|- 2\|\mathbf{z}_{s}\|}{R}.\]

Therefore, we have \(\log(\|\mathbf{z}_{s+1}\|^{2})-\log(\|\mathbf{z}_{s}\|^{2})\) is also a sub-Gaussian random variable and satisfies

\[\mathbb{P}\left[\log(\|\mathbf{z}_{s+1}\|^{2})-\log(\|\mathbf{z}_{s}\|^{2})\geq 6R^{-1} \sqrt{\tau d}+4R^{-1}t\sqrt{\tau}\right]\leq\exp(-t^{2}). \tag{42}\]

We consider any subsequence among \(\{\mathbf{z}_{k}\}_{k=1}^{S}\), with all iterates, except the first one, staying outside the region \(\mathcal{B}(\mathbf{0},R/2)\). Denote such subsequence by \(\{\mathbf{y}_{s}\}_{s=0}^{S^{\prime}}\) where \(\|\mathbf{y}_{0}\|\leq R/2\) and \(S^{\prime}\leq S\). Then, we know \(\mathbf{y}_{s}\) and \(\mathbf{y}_{s+1}\) satisfy Eq. 38 and Eq. 42 for all \(s\geq 1\). Under these conditions, by requiring\(\|\mathbf{z}_{0}\|\leq R/2\) with a probability at least \(1-\epsilon/16\), we only need to prove all points in \(\{\mathbf{y}_{s}\}_{s=0}^{S^{\prime}}\) will stay inside the region \(\mathcal{B}(\mathbf{0},R)\) with probability at least \(1-\epsilon/16\).

Then, set \(\mathcal{E}_{s}\) to be the event that

\[\mathcal{E}_{s}=\left\{\|\mathbf{y}_{s^{\prime}}\|\leq R,\forall s^{\prime} \leq s\right\},\]

which satisfies \(\mathcal{E}_{s-1}\subseteq\mathcal{E}_{s}\). Besides, suppose the filtration \(\mathcal{F}_{s}=\{\mathbf{y}_{0},\mathbf{y}_{1},\ldots,\mathbf{y}_{s}\}\), the sequence

\[\left\{\mathbf{1}(\mathcal{E}_{s-1})\cdot\left(\log(\|\mathbf{y}_{s}\|^{2}+Ls \tau/4)\right)\right\}_{s=1,2,\ldots,S}\]

is a super-martingale, and the martingale difference has a subgaussian tail, i.e., for any \(t\geq 0\),

\[\mathbb{P}\left[\log(\|\mathbf{y}_{s+1}\|^{2})+\frac{L(s+1)\tau} {4}-\log(\|\mathbf{y}_{s}\|^{2})-\frac{Ls\tau}{4}\geq 7R^{-1}\sqrt{\tau d}+4R^{ -1}t\sqrt{\tau d}\right]\] \[\leq\mathbb{P}\left[\log(\|\mathbf{y}_{s+1}\|^{2})+\frac{L(s+1) \tau}{4}-\log(\|\mathbf{y}_{s}\|^{2})-\frac{Ls\tau}{4}\geq 6R^{-1}\sqrt{\tau d} +4R^{-1}t\sqrt{\tau}+\frac{L\tau}{4}\right]\] \[=\mathbb{P}\left[\log(\|\mathbf{z}_{s+1}\|^{2})-\log(\|\mathbf{z} _{s}\|^{2})\geq 6R^{-1}\sqrt{\tau d}+4R^{-1}t\sqrt{\tau}\right]\leq\exp(-t^{2}),\]

where the first inequality is established when

\[\frac{L\tau}{4}\leq\frac{\sqrt{\tau d}}{R}\quad\Leftrightarrow\quad\tau\leq \frac{16d}{L^{2}R^{2}}\quad\text{and}\quad d\geq 1. \tag{43}\]

Under these conditions, suppose

\[u=\frac{6\sqrt{\tau d}}{R}+\frac{4t\sqrt{\tau d}}{R}\quad\Leftrightarrow\quad t =\frac{uR}{4\sqrt{\tau d}}-\frac{3}{2},\]

it implies

\[t^{2}\geq\frac{R^{2}u^{2}}{64\tau d}-1\]

which follows from the fact \((a-b)^{2}\geq a^{2}/4-b^{2}/3\) for all \(a,b\in\mathbb{R}\). Then, for any \(u\geq 0\), we have

\[\mathbb{P}\left[\log(\|\mathbf{y}_{s+1}\|^{2})+\frac{L(s+1)\tau}{4}-\log(\| \mathbf{y}_{s}\|^{2})-\frac{Ls\tau}{4}\geq u\right]\leq\exp\left(-\frac{R^{2}u ^{2}}{64\tau d}+1\right)\leq 3\exp\left(-\frac{R^{2}u^{2}}{64\tau d}\right),\]

which implies that the martingale difference is subgaussian. Then by Theorem 2 in [30], for any \(s\), we have

\[\log(\|\mathbf{y}_{s}\|^{2})+\frac{Ls\tau}{4}\leq\log(\|\mathbf{y}_{0}\|^{2}) +\frac{74}{R}\cdot\sqrt{s\tau d\log(1/\epsilon^{\prime})}\]

with the probability at least \(1-\epsilon^{\prime}\) conditioned on \(\mathcal{E}_{s-1}\). Taking the union bound over all \(s=1,2,\ldots,S^{\prime}\) (\(S^{\prime}\leq S\)) and set \(\epsilon=16\epsilon^{\prime}S^{\prime}\), we have with probability at least \(1-\epsilon/16\), for all \(s=1,2,\ldots,S^{\prime}\), it holds

\[\log(\|\mathbf{y}_{s}\|^{2})\leq 2\log(R/2)+\frac{74}{R}\cdot\sqrt{s\tau d\log(16S/\epsilon)}- \frac{Ls\tau}{4}\] \[\leq 2\log(R/2)+\frac{74^{2}\cdot d\log(16S/\epsilon)}{R^{2}L}.\]

By requiring

\[R\geq 63\cdot\sqrt{\frac{d}{L}\log\frac{16S}{\epsilon}}\quad\Rightarrow\quad \frac{74^{2}\cdot d\log(16S/\epsilon)}{R^{2}L}\leq 2\log 2, \tag{44}\]

we have \(\log(\|\mathbf{y}_{s}\|^{2})\leq\log(R^{2})\), which is equivalent to \(\|\mathbf{y}_{s}\|\leq R\). Combining with the fact that with probability at least \(1-\epsilon/16\) the initial point \(\mathbf{y}_{0}\) stays inside \(\mathcal{B}(\mathbf{0},R/2)\), we can conclude that with probability at least \(1-\epsilon/8\) all iterates stay inside the region \(\mathcal{B}(\mathbf{0},R)\).

The difference between \(\mathbf{z}_{s+1}\) and \(\mathbf{z}_{s}\) is smaller than \(r\).In this paragraph, we aim to prove \(\|\mathbf{z}_{s+1}-\mathbf{z}_{s}\|\leq r\) for all \(s\leq S\). Similar to the previous techniques, we consider

\[\tilde{\mathbf{z}}_{s}\coloneqq\mathbf{z}_{s}-\tau\cdot s_{\theta}(\mathbf{z}_{ s})+\sqrt{2\tau}\cdot\xi\quad\mathrm{where}\quad\xi\sim\mathcal{N}(\mathbf{0}, \boldsymbol{I}).\]

According to the transition kernel Eq. 28, it has

\[\begin{split}\mathbb{P}\left[\|\mathbf{z}_{s+1}-\mathbf{z}_{s}\| \geq r\right]\leq&\mathbb{P}\left[\|\tilde{\mathbf{z}}_{s}- \mathbf{z}_{s}\|\geq r\right]\leq\mathbb{P}\left[\tau\|s_{\theta}(\mathbf{z}_ {s})\|+\sqrt{2\tau}\|\xi\|\geq r\right]\\ =&\mathbb{P}\left[\|\xi\|\geq\frac{r-\tau\|s_{\theta }(\mathbf{z}_{s})\|}{\sqrt{2\tau}}\right]\leq\mathbb{P}\left[\|\xi\|\geq\frac{ r-\sqrt{\tau d}\|}{\sqrt{2\tau}}\right]\end{split} \tag{45}\]

where the second inequality follows from the triangle inequality, and the last inequality follows from Eq. 41 when the choice of \(\tau\) satisfies Eq. 39. Under these conditions, by choosing

\[r\geq(\sqrt{2}+1)\cdot\sqrt{\tau d}+2\sqrt{\tau\log(8S/\epsilon)}\quad\Leftrightarrow \quad\frac{r-\sqrt{\tau d}}{\sqrt{2\tau}}\geq\sqrt{d}+\sqrt{2}\cdot\sqrt{\log(8 S/\epsilon)},\]

Eq. 45 becomes

\[\mathbb{P}\left[\|\mathbf{z}_{s+1}-\mathbf{z}_{s}\|\geq r\right]\leq\mathbb{P} \left[\|\xi\|\geq\frac{r-\sqrt{\tau d}\|}{\sqrt{2\tau}}\right]\leq\mathbb{P} \left[\|\xi\|\geq\sqrt{d}+\sqrt{2}\cdot\sqrt{\log(8S/\epsilon)}\right]\leq \frac{\epsilon}{8S},\]

which means

\[\mathbb{P}\left[\|\mathbf{z}_{s+1}-\mathbf{z}_{s}\|\leq r\right]\geq 1-\frac{ \epsilon}{8S}.\]

Taking union bound over all iterates, we know all particles satisfy the local condition, i.e., \(\|\mathbf{z}_{s+1}-\mathbf{z}_{s}\|\leq r\) with the probability at least \(1-\epsilon/8\). Hence, the proof is completed. 

### Control the error from the approximation of score and energy

**Lemma C.3**.: _Under Assumption [41, 42, 42], we set_

\[\eta=\frac{1}{2}\log\frac{2L+1}{2L}\quad\mathrm{and}\quad G\coloneqq\|\nabla g (\mathbf{0})\|\,.\]

_For any \(\epsilon\in(0,1)\), we set_

\[R\geq\max\left\{8\cdot\sqrt{\frac{\|\nabla g(\mathbf{0})\|^{2}}{L^{2}}+\frac{d }{L}},63\cdot\sqrt{\frac{d}{L}\log\frac{16S}{\epsilon}}\right\},\quad r=3\cdot \sqrt{\tau d\log\frac{8S}{\epsilon}}.\]

_Suppose it has_

\[\frac{\delta}{16}\coloneqq\frac{3\epsilon_{\mathrm{score}}}{2}\cdot\sqrt{\tau d \log\frac{8S}{\epsilon}}+\frac{\tau\epsilon_{\mathrm{score}}^{2}}{4}+\frac{ \tau(3LR+G)\epsilon_{\mathrm{score}}}{2}\leq\frac{1}{32}\quad\mathrm{and}\quad \epsilon_{\mathrm{energy}}\leq\frac{1}{10},\]

_we have_

\[(1-\delta-5\epsilon_{\mathrm{energy}})\cdot\tilde{\mathcal{T}}_{*,\mathbf{z}}( \Omega_{\mathbf{z}}^{\prime})\leq\tilde{\mathcal{T}}_{\mathbf{z}}(\Omega_{ \mathbf{z}}^{\prime})\leq(1+\delta+5\epsilon_{\mathrm{energy}})\cdot\tilde{ \mathcal{T}}_{*,\mathbf{z}}(\Omega_{\mathbf{z}}^{\prime}).\]

_for any set \(\mathcal{A}\subseteq\mathcal{B}(0,R)\) and point \(\mathbf{z}\in\mathcal{B}(0,R)\)._

Proof.: Note that the Markov process defined by \(\tilde{\mathcal{T}}_{\mathbf{z}}(\cdot)\) and \(\tilde{\mathcal{T}}_{*,\mathbf{z}}(\cdot)\) are \(1/2\)-lazy. We prove the lemma by considering two cases: \(\mathbf{z}\not\in\mathcal{A}\) and \(\mathbf{z}\in\mathcal{A}\).

When \(\mathbf{z}\not\in\mathcal{A}\),we have

\[\tilde{\mathcal{T}}_{\mathbf{z}}(\mathcal{A}) =\int_{\mathcal{A}}\tilde{a}_{\mathbf{z}}(\mathbf{z}^{\prime}) \tilde{\mathcal{T}}_{\mathbf{z}}^{\prime}(\mathrm{d}\mathbf{z}^{\prime})= \frac{1}{2}\int_{\mathcal{A}}\tilde{a}_{\mathbf{z}}(\mathbf{z}^{\prime}) \tilde{Q}_{\mathbf{z}}^{\prime}(\mathrm{d}\mathbf{z}^{\prime})\] \[=\frac{1}{2}\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}\tilde{a}_{ \mathbf{z}}(\mathbf{z}^{\prime})\tilde{Q}_{\mathbf{z}}(\mathrm{d}\mathbf{z}^{ \prime})=\frac{1}{2}\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}\tilde{a}_{ \mathbf{z}}(\mathbf{z}^{\prime})\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z}) \mathrm{d}\mathbf{z}^{\prime}.\]Similarly, we have

\[\tilde{\mathcal{T}}_{*,\mathbf{z}}(\mathcal{A})=\frac{1}{2}\int_{\mathcal{A}\cap\Omega _{\mathbf{z}}}\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\tilde{q}_{*}(\mathbf{z}^{\prime}| \mathbf{z})\mathrm{d}\mathbf{z}^{\prime}.\]

In this condition, we consider

\[2\tilde{\mathcal{T}}_{\mathbf{z}}(\mathcal{A})-2\tilde{\mathcal{T}}_{ *,\mathbf{z}}(\mathcal{A})= -\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}\tilde{a}_{*,\mathbf{z}}(\mathbf{z} ^{\prime})\tilde{q}_{*}(\mathbf{z}^{\prime}|\mathbf{z})\mathrm{d}\mathbf{z}^{\prime}+\int_ {\mathcal{A}\cap\Omega_{\mathbf{z}}}\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\tilde{q} (\mathbf{z}^{\prime}|\mathbf{z})\mathrm{d}\mathbf{z}^{\prime}\] \[-\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}\tilde{a}_{*,\mathbf{z}}(\mathbf{z} ^{\prime})\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})\mathrm{d}\mathbf{z}^{\prime}+\int_{ \mathcal{A}\cap\Omega_{\mathbf{z}}}\tilde{a}_{\mathbf{z}}(\mathbf{z}^{\prime})\tilde{q}( \mathbf{z}^{\prime}|\mathbf{z})\mathrm{d}\mathbf{z}^{\prime},\]

which means

\[\frac{\tilde{\mathcal{T}}_{\mathbf{z}}(\mathcal{A})-\tilde{\mathcal{T }}_{*,\mathbf{z}}(\mathcal{A})}{\tilde{\mathcal{T}}_{*,\mathbf{z}}(\mathcal{A})} =\underbrace{\frac{\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}\tilde{a}_ {*,\mathbf{z}}(\mathbf{z}^{\prime})\cdot\left(\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})-\tilde{ q}_{*}(\mathbf{z}^{\prime}|\mathbf{z})\right)\mathrm{d}\mathbf{z}^{\prime}}{\int_{ \mathcal{A}\cap\Omega_{\mathbf{z}}}\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\tilde{q}_{ *}(\mathbf{z}^{\prime}|\mathbf{z})\mathrm{d}\mathbf{z}^{\prime}}}_{\text{Term 1}} \tag{46}\] \[\quad+\underbrace{\frac{\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}( \tilde{a}_{\mathbf{z}}(\mathbf{z}^{\prime})-\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})) \,\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})\mathrm{d}\mathbf{z}^{\prime}}{\int_{\mathcal{A }\cap\Omega_{\mathbf{z}}}\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\tilde{q}_{*}(\mathbf{z} ^{\prime}|\mathbf{z})\mathrm{d}\mathbf{z}^{\prime}}}_{\text{Term 2}}.\]

First, we try to control Term 1, which can be achieved by investigating \(\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})/\tilde{q}_{*}(\mathbf{z}^{\prime}|\mathbf{z})\) as follows.

\[\frac{\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})}{\tilde{q}_{*}(\mathbf{z}^{\prime}|\mathbf{z}) }=\exp\left(-\frac{\|\mathbf{z}^{\prime}-(\mathbf{z}-\tau\cdot s_{\theta}(\mathbf{z}))\|^{ 2}}{4\tau}+\frac{\|\mathbf{z}^{\prime}-(\mathbf{z}-\tau\cdot\nabla g(\mathbf{z}))\|^{2}}{ 4\tau}\right),\]

In this condition, we have

\[\frac{\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})}{\tilde{q}_{*}(\mathbf{z}^{ \prime}|\mathbf{z})}= \exp\left((4\tau)^{-1}\cdot\left(-\left\|\mathbf{z}^{\prime}-\mathbf{z} \right\|^{2}-2\tau\cdot(\mathbf{z}^{\prime}-\mathbf{z})^{\top}s_{\theta}(\mathbf{z})-\tau ^{2}\cdot\left\|s_{\theta}(\mathbf{z})\right\|^{2}\right.\right. \tag{47}\] \[\quad+\left\|\mathbf{z}^{\prime}-\mathbf{z}\right\|^{2}+2\tau\cdot\left( \mathbf{z}^{\prime}-\mathbf{z}\right)^{\top}\nabla g(\mathbf{z})+\tau^{2}\cdot\left\| \nabla g(\mathbf{z})\right\|^{2}\right)\Bigr{)}\] \[= \exp\left(\frac{1}{2}(\mathbf{z}^{\prime}-\mathbf{z})^{\top}\left(-s_{ \theta}(\mathbf{z})+\nabla g(\mathbf{z})\right)+\frac{\tau}{4}\left(-\left\|s_{\theta }(\mathbf{z})\right\|^{2}+\left\|\nabla g(\mathbf{z})\right\|^{2}\right)\right).\]

It means

\[\left|\ln\frac{\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})}{\tilde{q}_{*}(\bm {z}^{\prime}|\mathbf{z})}\right|= \left|\frac{1}{2}(\mathbf{z}^{\prime}-\mathbf{z})^{\top}\left(-s_{\theta }(\mathbf{z})+\nabla g(\mathbf{z})\right)+\frac{\tau}{4}\left(-\left\|s_{\theta}(\bm {z})\right\|^{2}+\left\|\nabla g(\mathbf{z})\right\|^{2}\right)\right|\] \[\leq \frac{1}{2}\left\|\mathbf{z}^{\prime}-\mathbf{z}\right\|\cdot\left\|s_{ \theta}(\mathbf{z})-\nabla g(\mathbf{z})\right\|+\frac{\tau}{4}\cdot\left[\left\|s_{ \theta}(\mathbf{z})+\nabla g(\mathbf{z})\right\|\cdot\left\|s_{\theta}(\mathbf{z})-\nabla g (\mathbf{z})\right\|\right]\] \[\leq \frac{1}{2}\left\|\mathbf{z}^{\prime}-\mathbf{z}\right\|\cdot\left\|s_{ \theta}(\mathbf{z})-\nabla g(\mathbf{z})\right\|+\frac{\tau}{4}\cdot\left\|s_{\theta }(\mathbf{z})-\nabla g(\mathbf{z})\right\|^{2}+\frac{\tau}{2}\cdot\left\|\nabla g(\mathbf{ z})\right\|\cdot\left\|s_{\theta}(\mathbf{z})-\nabla g(\mathbf{z})\right\|\] \[\leq \frac{r\epsilon_{\text{score}}}{2}+\frac{\tau\epsilon_{\text{score} }^{2}}{4}+\frac{\tau(3LR+G)\epsilon_{\text{score}}}{2}\]

where the last inequality follows from the fact \(\mathbf{z}^{\prime}\in\mathcal{B}(\mathbf{z},r)\cap\mathcal{B}(\mathbf{0},R)/\{\mathbf{z}\}\), \(\mathbf{z}\in\mathcal{B}(\mathbf{0},R)\) and

\[\left\|\nabla g(\mathbf{z})\right\|=\left\|\nabla g(\mathbf{z})-\nabla g(\mathbf{0})+ \nabla g(\mathbf{0})\right\|\leq 3L\cdot\|\mathbf{z}\|+G\leq 3LR+G.\]

According to the definition of \(R\) and r shown in Lemma C.2, we choose

\[r\coloneqq 3\cdot\sqrt{\tau}\cdot\sqrt{d\log\frac{8S}{\epsilon}}\geq(\sqrt{2}+1 )\cdot\sqrt{\tau d}+2\sqrt{\tau\log\frac{8S}{\epsilon}}\]

Under this condition, we require

\[\frac{\delta}{16}\coloneqq\frac{3\epsilon_{\text{score}}}{2}\cdot\sqrt{\tau d \log\frac{8S}{\epsilon}}+\frac{\tau\epsilon_{\text{score}}^{2}}{4}+\frac{ \tau(3LR+G)\epsilon_{\text{score}}}{2}\leq\frac{1}{32}, \tag{48}\]

then we have

\[\ln\left(1-\frac{\delta}{8}\right)\leq\ln\frac{\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z} )}{\tilde{q}_{*}(\mathbf{z}^{\prime}|\mathbf{z})}\leq\ln\left(1+\frac{\delta}{8} \right)\quad\Leftrightarrow\quad 1-\frac{\delta}{8}<\frac{\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})}{ \tilde{q}_{*}(\mathbf{z}^{\prime}|\mathbf{z})}\leq 1+\frac{\delta}{8},\]and

\[-\frac{\delta}{8}\leq\min_{\mathbf{z}^{\prime}\in\mathcal{A}\cap\Omega_{\mathbf{z}}}\frac{ \tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})}{\tilde{q}_{*}(\mathbf{z}^{\prime}|\mathbf{z})}-1\leq \text{Term }1\leq\max_{\mathbf{z}^{\prime}\in\mathcal{A}\cap\Omega_{\mathbf{z}}}\frac{ \tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})}{\tilde{q}_{*}(\mathbf{z}^{\prime}|\mathbf{z})}-1\leq \frac{\delta}{8}. \tag{49}\]

with the definition of \(\text{Term }1\) shown in Eq. 46.

Then, we try to control Term \(2\) of Eq. 46 and have

\[\frac{\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}\left(\tilde{a}_{\mathbf{z}}(\mathbf{z}^{ \prime})-\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\right)\tilde{q}(\mathbf{z}^{\prime }|\mathbf{z})\text{d}\mathbf{z}^{\prime}}{\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}\tilde{ a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\tilde{q}_{*}(\mathbf{z}^{\prime}|\mathbf{z})\text{d} \mathbf{z}^{\prime}}=\frac{\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}\left(\tilde{a}_{ \mathbf{z}}(\mathbf{z}^{\prime})-\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\right)\tilde{q}( \mathbf{z}^{\prime}|\mathbf{z})\text{d}\mathbf{z}^{\prime}}{\int_{\mathcal{A}\cap\Omega_{ \mathbf{z}}}\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z}) \text{d}\mathbf{z}^{\prime}}\cdot\frac{\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}\tilde{ a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})\text{d}\mathbf{z}^{ \prime}}{\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{ \prime})\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})\text{d}\mathbf{z}^{\prime}}. \tag{50}\]

According to Eq. 49, it has

\[1-\frac{\delta}{8}\leq\min_{\mathbf{z}^{\prime}\in\mathcal{A}\cap\Omega_{\mathbf{z}}} \frac{\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})}{\tilde{q}_{*}(\mathbf{z}^{\prime}|\mathbf{z}) }\leq\frac{\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{ \prime})\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})\text{d}\mathbf{z}^{\prime}}{\int_{ \mathcal{A}\cap\Omega_{\mathbf{z}}}\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\tilde{q}_ {*}(\mathbf{z}^{\prime}|\mathbf{z})\text{d}\mathbf{z}^{\prime}}\leq\max_{\mathbf{z}^{\prime} \in\mathcal{A}\cap\Omega_{\mathbf{z}}}\frac{\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})}{ \tilde{q}_{*}(\mathbf{z}^{\prime}|\mathbf{z})}\leq 1+\frac{\delta}{8}, \tag{51}\]

then we can upper and lower bounding \(\text{Term }2\) by investigating \(\tilde{a}_{\mathbf{z}}(\mathbf{z}^{\prime})/\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\) as follows

\[\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})= \min\left\{1,\exp\left(-(g(\mathbf{z}^{\prime})-g(\mathbf{z}))\right)\cdot \frac{\tilde{q}_{*}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}_{*}(\mathbf{z}^{\prime}|\bm {z})}\right\},\] \[\tilde{a}_{\mathbf{z}}(\mathbf{z}^{\prime})= \min\left\{1,\exp\left(-r_{\theta}(\mathbf{z}^{\prime},\mathbf{z})\right) \cdot\frac{\tilde{q}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z} )}\right\}.\]

In this condition, for any \(0<\delta\leq 1\), we first consider two cases. When

\[\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})=1\leq\exp\left(-(g(\mathbf{z}^{\prime})-g( \mathbf{z}))\right)\cdot\frac{\tilde{q}_{*}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}_{*} (\mathbf{z}^{\prime}|\mathbf{z})}\quad\text{and}\quad\tilde{a}_{\mathbf{z}}(\mathbf{z}^{ \prime})=\exp\left(-r_{\theta}(\mathbf{z}^{\prime},\mathbf{z})\right)\cdot\frac{\tilde{ q}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})}\leq 1,\]

we have

\[\underbrace{\frac{\exp\left(-r_{\theta}(\mathbf{z}^{\prime},\mathbf{z})\right)\cdot \frac{\tilde{q}(\mathbf{z}|\mathbf{z}^{\prime})}{\exp\left(-(g(\mathbf{z}^{\prime})-g( \mathbf{z}))\right)\cdot\frac{\tilde{q}_{*}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}_{*} (\mathbf{z}^{\prime}|\mathbf{z})}}}_{\text{Term }2.1}}\leq\frac{\tilde{a}_{\mathbf{z}}(\mathbf{z}^{\prime})}{ \tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})}=\exp\left(-r_{\theta}(\mathbf{z}^{\prime}, \mathbf{z})\right)\cdot\frac{\tilde{q}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}(\mathbf{z}^{ \prime}|\mathbf{z})}\leq 1. \tag{52}\]

Besides, when

\[\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})=\exp\left(-(g(\mathbf{z}^{\prime})-g(\mathbf{z})) \right)\cdot\frac{\tilde{q}_{*}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}_{*}(\mathbf{z}^{ \prime}|\mathbf{z})}\leq 1\quad\text{and}\quad\tilde{a}_{\mathbf{z}}(\mathbf{z}^{\prime})=1\leq\exp\left(-r_{ \theta}(\mathbf{z}^{\prime},\mathbf{z})\right)\frac{\tilde{q}(\mathbf{z}|\mathbf{z}^{\prime})}{ \tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})},\]

we have

\[1\leq\frac{\tilde{a}_{\mathbf{z}}(\mathbf{z}^{\prime})}{\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{ \prime})}=\frac{1}{\exp\left(-(g(\mathbf{z}^{\prime})-g(\mathbf{z}))\right)\cdot\frac{ \tilde{q}_{*}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}_{*}(\mathbf{z}^{\prime}|\mathbf{z})} }\leq\underbrace{\frac{\exp\left(-r_{\theta}(\mathbf{z}^{\prime},\mathbf{z})\right)\cdot \frac{\tilde{q}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})}}{ \underbrace{\exp\left(-(g(\mathbf{z}^{\prime})-g(\mathbf{z}))\right)\cdot\frac{ \tilde{q}_{*}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}_{*}(\mathbf{z}^{\prime}|\mathbf{z})}}_{ \text{Term }2.1}}. \tag{53}\]

Then, we start to consider finding the range of \(\ln(\text{Term }2.1)\) as follows

\[\begin{split}|\ln\left(\text{Term }2.1\right)|=& \left|\left(-r_{\theta}(\mathbf{z}^{\prime},\mathbf{z})+(g(\mathbf{z}^{\prime})-g(\mathbf{z})) \right)+\ln\frac{\tilde{q}_{*}(\mathbf{z}^{\prime}|\mathbf{z})}{\tilde{q}(\mathbf{z}^{ \prime}|\mathbf{z})}+\ln\frac{\tilde{q}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}_{*}(\mathbf{z }|\mathbf{z}^{\prime})}\right|\\ \leq&\epsilon_{\text{energy}}+\left|\ln\frac{\tilde{q }_{*}(\mathbf{z}^{\prime}|\mathbf{z})}{\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})}\right|+ \left|\ln\frac{\tilde{q}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}_{*}(\mathbf{z}|\mathbf{z}^ {\prime})}\right|\leq\epsilon_{\text{energy}}+\frac{\delta}{16}+\left|\ln\frac{ \tilde{q}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}_{*}(\mathbf{z}|\mathbf{z}^{\prime})}\right|, \end{split} \tag{54}\]

where the last inequality follows from Eq. 48. Besides, similar to Eq. 47, we have

\[\begin{split}\frac{\tilde{q}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}_{* }(\mathbf{z}|\mathbf{z}^{\prime})}=&\exp\left(\left(4\tau\right)^{-1} \cdot\left(-\left\|\mathbf{z}-\mathbf{z}^{\prime}\right\|^{2}-2\tau\cdot(\mathbf{z}-\mathbf{z}^{ \prime})^{\top}s_{\theta}(\mathbf{z}^{\prime})-\tau^{2}\cdot\left\|s_{\theta}(\mathbf{z}^{ \prime})\right\|^{2}\right.\right.\\ &\left.\left.\left.\qquad+\left\|\mathbf{z}-\mathbf{z}^{\prime}\right\|^{2}+2 \tau\cdot(\mathbf{z}-\mathbf{z}^{\prime})^{\top}\nabla g(\mathbf{z}^{\prime})+\tau^{2} \cdot\left\|\nabla g(\mathbf{z}^{\prime})\right\|^{2}\right)\right),\end{split}\]which means

\[\left|\ln\frac{\tilde{q}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}_{*}(\mathbf{z }|\mathbf{z}^{\prime})}\right|= \left|\frac{1}{2}(\mathbf{z}-\mathbf{z}^{\prime})^{\top}(-s_{\theta}(\mathbf{z }^{\prime})+\nabla g(\mathbf{z}^{\prime}))+\frac{\tau}{4}\left(-\left\|s_{\theta}( \mathbf{z}^{\prime})\right\|^{2}+\left\|\nabla g(\mathbf{z}^{\prime})\right\|^{2} \right)\right|\] \[\leq \frac{1}{2}\left\|\mathbf{z}-\mathbf{z}^{\prime}\right\|\cdot\left\|s_{ \theta}(\mathbf{z}^{\prime})-\nabla g(\mathbf{z}^{\prime})\right\|+\frac{\tau}{4}\cdot \left\|s_{\theta}(\mathbf{z}^{\prime})+\nabla g(\mathbf{z}^{\prime})\right\|\cdot\left\| s_{\theta}(\mathbf{z}^{\prime})-\nabla g(\mathbf{z}^{\prime})\right\|\] \[\leq \frac{1}{2}\left\|\mathbf{z}-\mathbf{z}^{\prime}\right\|\cdot\left\|s_{ \theta}(\mathbf{z}^{\prime})-\nabla g(\mathbf{z}^{\prime})\right\|+\frac{\tau}{4}\cdot \left\|s_{\theta}(\mathbf{z}^{\prime})-\nabla g(\mathbf{z}^{\prime})\right\|^{2}+\frac {\tau}{2}\left\|\nabla g(\mathbf{z}^{\prime})\right\|\cdot\left\|s_{\theta}(\mathbf{z} ^{\prime})-\nabla g(\mathbf{z}^{\prime})\right\|\] \[\leq \frac{r\epsilon_{\rm score}}{2}+\frac{\tau\epsilon_{\rm score}^{2} }{4}+\frac{\tau(3LR+G)\epsilon_{\rm score}}{2},\]

where the last inequality follows from the fact \(\mathbf{z}^{\prime}\in\mathcal{B}(\mathbf{z},r)\cap\mathcal{B}(\mathbf{0},R)/\{\mathbf{z}\}\) and

\[\left\|\nabla g(\mathbf{z}^{\prime})\right\|=\left\|\nabla g(\mathbf{z}^{\prime})- \nabla g(\mathbf{0})+\nabla g(\mathbf{0})\right\|\leq 3L\cdot\left\|\mathbf{z}^{\prime} \right\|+G\leq 3LR+G.\]

Combining this result with Eq. 48, we have

\[\left|\ln\frac{\tilde{q}(\mathbf{z}|\mathbf{z}^{\prime})}{\tilde{q}_{*}(\mathbf{z}|\mathbf{z} ^{\prime})}\right|\leq\frac{\delta}{16}\quad\Leftrightarrow\quad\frac{\delta }{16}=\frac{3\epsilon_{\rm score}}{2}\cdot\sqrt{\tau d\log\frac{8S}{\epsilon} }+\frac{\tau\epsilon_{\rm score}^{2}}{4}+\frac{\tau(3LR+G)\epsilon_{\rm score} }{2}.\]

Plugging this result into Eq. 54, it has

\[\left|\ln\left(\text{Term }2.1\right)\right|\leq\frac{\delta}{8}+\epsilon_{\rm energy}.\]

By requiring \(\epsilon_{\rm energy}\leq 0.1\), we have

\[\ln\left(1-\frac{\delta}{4}-2\epsilon_{\rm energy}\right)\leq\ln \left(\text{Term }2.1\right)\leq\ln\left(1+\frac{\delta}{4}+2\epsilon_{\rm energy}\right)\] \[\Leftrightarrow\quad 1-\frac{\delta}{4}-2\epsilon_{\rm energy}\leq \text{Term }2.1\leq 1+\frac{\delta}{4}+2\epsilon_{\rm energy}.\]

Combining this result with Eq. 52 and Eq. 53, we have

\[1-\frac{\delta}{4}-2\epsilon_{\rm energy}\leq\frac{a_{\mathbf{z}}(\mathbf{z}^{\prime}) }{a_{*,\mathbf{z}}(\mathbf{z}^{\prime})}\leq 1+\frac{\delta}{4}+2\epsilon_{\rm energy},\]

which implies

\[\frac{\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}\left(\tilde{a}_{\mathbf{z}}(\mathbf{z}^{ \prime})-\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\right)\tilde{q}(\mathbf{z}^{\prime }|\mathbf{z})\text{d}\mathbf{z}^{\prime}}{\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}\tilde{ a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})\text{d}\mathbf{z}^{ \prime}}\geq\min_{z^{\prime}\in\mathcal{A}}\frac{\tilde{a}_{\mathbf{z}}(\mathbf{z}^{ \prime})}{\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})}-1\geq-\frac{\delta}{4}-2 \epsilon_{\rm energy} \tag{55}\] \[\frac{\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}}\left(\tilde{a}_{\mathbf{z }}(\mathbf{z}^{\prime})-\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\right)\tilde{q}(\mathbf{z }^{\prime}|\mathbf{z})\text{d}\mathbf{z}^{\prime}}{\int_{\mathcal{A}\cap\Omega_{\mathbf{z}}} \tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})\tilde{q}(\mathbf{z}^{\prime}|\mathbf{z})\text{d} \mathbf{z}^{\prime}}\leq\max_{z^{\prime}\in\mathcal{A}}\frac{\tilde{a}_{\mathbf{z}}(\mathbf{z }^{\prime})}{\tilde{a}_{*,\mathbf{z}}(\mathbf{z}^{\prime})}-1\leq\frac{\delta}{4}+2 \epsilon_{\rm energy}.\]

Plugging Eq. 55 and Eq. 51 into Eq. 50, we have

\[-\frac{\delta}{3}-\frac{5\epsilon_{\rm energy}}{2}\leq\left(-\frac{\delta}{4}-2 \epsilon_{\rm energy}\right)\cdot\left(1+\frac{\delta}{8}\right)\leq\text{ Term }2\leq\left(\frac{\delta}{4}+2\epsilon_{\rm energy}\right)\cdot\left(1+\frac{\delta}{8} \right)\leq\frac{\delta}{3}+\frac{5\epsilon_{\rm energy}}{2}. \tag{56}\]

In this condition, combining Eq. 56, Eq. 49 with Eq. 46, we have

\[-\frac{\delta+5\epsilon_{\rm energy}}{2}\leq\frac{\tilde{\mathcal{T }}_{\mathbf{z}}(\mathcal{A})-\tilde{\mathcal{T}}_{*,\mathbf{z}}(\mathcal{A})}{\tilde{ \mathcal{T}}_{*,\mathbf{z}}(\mathcal{A})}\leq\frac{\delta+5\epsilon_{\rm energy}}{2} \tag{57}\] \[\Leftrightarrow\quad\left(1-\frac{\delta+5\epsilon_{\rm energy}}{2} \right)\cdot\tilde{\mathcal{T}}_{*,\mathbf{z}}(\mathcal{A})\leq\tilde{\mathcal{T}}_{ \mathbf{z}}(\mathcal{A})\leq\left(1+\frac{\delta+5\epsilon_{\rm energy}}{2} \right)\cdot\tilde{\mathcal{T}}_{*,\mathbf{z}}(\mathcal{A}).\]

Hence, we complete the proof for \(\mathbf{z}\not\in\mathcal{A}\).

When \(\mathbf{z}\in\mathcal{A}\), suppose there exist some \(r^{\prime}\) satisfying

\[\Omega^{\prime}_{\mathbf{z}}:=\mathcal{B}(\mathbf{z},r^{\prime})\subseteq\mathcal{A}.\]

We can split \(\mathcal{A}\) into \(\mathcal{A}-\Omega^{\prime}_{\mathbf{z}}\) and \(\Omega^{\prime}_{\mathbf{z}}\). Note that by our results in the first case, we have

\[\left(1-\frac{\delta+5\epsilon_{\mathrm{energy}}}{2}\right)\cdot\tilde{ \mathcal{T}}_{*,\mathbf{z}}(\mathcal{A}-\Omega^{\prime}_{\mathbf{z}})\leq\tilde{ \mathcal{T}}_{\mathbf{z}}(\mathcal{A}-\Omega^{\prime}_{\mathbf{z}})\leq\left(1+\frac{ \delta+5\epsilon_{\mathrm{energy}}}{2}\right)\cdot\tilde{\mathcal{T}}_{*,\mathbf{z} }(\mathcal{A}-\Omega^{\prime}_{\mathbf{z}}).\]

Then for the set \(\Omega^{\prime}_{\mathbf{z}}\), we have

\[\left|\frac{\tilde{\mathcal{T}}_{\mathbf{z}}(\Omega^{\prime}_{\mathbf{z} })-\tilde{\mathcal{T}}_{*,\mathbf{z}}(\Omega^{\prime}_{\mathbf{z}})}{\tilde{\mathcal{ T}}_{*,\mathbf{z}}(\Omega^{\prime}_{\mathbf{z}})}\right|=\left|\frac{\left(1-\tilde{ \mathcal{T}}_{\mathbf{z}}(\Omega-\Omega^{\prime}_{\mathbf{z}})\right)-\left(1-\tilde{ \mathcal{T}}_{*,\mathbf{z}}(\Omega-\Omega^{\prime}_{\mathbf{z}})\right)}{\tilde{ \mathcal{T}}_{*,\mathbf{z}}(\Omega^{\prime}_{\mathbf{z}})}\right|\] \[=\left|\frac{\tilde{\mathcal{T}}_{*,\mathbf{z}}(\Omega-\Omega^{\prime}_ {\mathbf{z}})-\tilde{\mathcal{T}}_{\mathbf{z}}(\Omega-\Omega^{\prime}_{\mathbf{z}})}{\tilde {\mathcal{T}}_{*,\mathbf{z}}(\Omega^{\prime}_{\mathbf{z}})}\right|\leq\left|\frac{ \tilde{\mathcal{T}}_{*,\mathbf{z}}(\Omega-\Omega^{\prime}_{\mathbf{z}})-\tilde{ \mathcal{T}}_{\mathbf{z}}(\Omega-\Omega^{\prime}_{\mathbf{z}})}{\tilde{\mathcal{T}}_{*,\mathbf{z}}(\Omega-\Omega^{\prime}_{\mathbf{z}})}\right|.\left|\frac{\tilde{\mathcal{ T}}_{*,\mathbf{z}}(\Omega-\Omega^{\prime}_{\mathbf{z}})}{\tilde{\mathcal{T}}_{*,\mathbf{z}}( \Omega^{\prime}_{\mathbf{z}})}\right|\] \[\leq\frac{\delta+5\epsilon_{\mathrm{energy}}}{2}\cdot 2=\delta+5 \epsilon_{\mathrm{energy}},\]

where the last inequality follows from Eq. 57 and the property of \(1/2\) lazy, i.e.,

\[\tilde{\mathcal{T}}_{*,\mathbf{z}}(\Omega-\Omega^{\prime}_{\mathbf{z}})\leq 1\quad \mathrm{and}\quad\tilde{\mathcal{T}}_{*,\mathbf{z}}(\Omega^{\prime}_{\mathbf{z}})\geq \frac{1}{2}.\]

In this condition, we have

\[(1-\delta-5\epsilon_{\mathrm{energy}})\cdot\tilde{\mathcal{T}}_{*,\mathbf{z}}( \Omega^{\prime}_{\mathbf{z}})\leq\tilde{\mathcal{T}}_{\mathbf{z}}(\Omega^{\prime}_{ \mathbf{z}})\leq(1+\delta+5\epsilon_{\mathrm{energy}})\cdot\tilde{\mathcal{T}}_{*, \mathbf{z}}(\Omega^{\prime}_{\mathbf{z}}).\]

Hence, we complete the proof for \(\mathbf{z}\in\mathcal{A}\). 

**Corollary C.4**.: _Under the same conditions as shown in Lemma C.3, if we require_

\[\epsilon_{\mathrm{energy}}\leq\delta/5,\]

_then we have_

\[(1-2\delta)\cdot\tilde{\mathcal{T}}_{*,\mathbf{z}}(\mathcal{A})\leq\tilde{ \mathcal{T}}_{\mathbf{z}}(\mathcal{A})\leq(1+2\delta)\cdot\tilde{\mathcal{T}}_{*,\mathbf{z}}(\mathcal{A}),\]

_for any set \(\mathcal{A}\subseteq\mathcal{B}(0,R)\) and point \(\mathbf{z}\in\mathcal{B}(0,R)\)._

### Control the error from Inner MALA to its stationary

In this section, we denote the ideally projected implementation of Alg. 2 whose Markov process, transition kernel, and particles' underlying distributions are denoted as \(\{\tilde{\mathbf{z}}_{*,s}\}_{s=0}^{S}\), Eq. 33, and \(\tilde{\mu}_{*,s}\) respectively. According to [41], we know the stationary distribution of the time-reversible process \(\{\tilde{\mathbf{z}}_{*,s}\}_{s=0}^{S}\) is

\[\tilde{\mu}_{*}(\mathrm{d}\mathbf{z})=\begin{cases}\frac{e^{-g(\mathbf{z})}}{\int_{ \Omega}e^{-g(\mathbf{z}^{\prime})}\mathrm{d}\mathbf{z}^{\prime}}\mathrm{d}\mathbf{z}&\mathbf{ x}\in\Omega;\\ 0&\text{otherwise}.\end{cases} \tag{58}\]

Here, we denote \(\Omega=\mathcal{B}(\mathbf{0},R)\) and

\[\Omega_{\mathbf{z}}=\mathcal{B}(\mathbf{0},R)\cap\mathcal{B}(\mathbf{z},r).\]

In the following analysis, we default

\[\eta=\frac{1}{2}\log\frac{2L+1}{2L}.\]

Under this condition, the smoothness of \(g\) is \(3L\) and the strong convexity constant is \(L\).

we aim to build the connection between the underlying distribution of the output particles obtained by projected Alg 2, i.e., \(\tilde{\mu}_{S}\), and the stationary distribution \(\tilde{\mu}_{*}\) though the process \(\{\tilde{\mathbf{z}}_{*,s}\}_{s=0}^{S}\). Since the ideally projected implementation of Alg. 2 is similar to standard MALA except for the projection, we prove its convergence through its conductance properties, which can be deduced by the Cheeger isoperimetric inequality of \(\tilde{\mu}_{*}\).

Under these conditions, we organize this subsection in the following three steps:

1. Find the Cheeger isoperimetric inequality of \(\tilde{\mu}_{*}\).
2. Find the conductance properties of \(\tilde{\mathcal{T}}_{*}\).
3. Build the connection between \(\tilde{\mu}_{S}\) and \(\tilde{\mu}_{*}\) through the process \(\{\tilde{\mathbf{z}}_{*,s}\}_{s=0}^{S}\).

#### c.3.1 The Cheeger isoperimetric inequality of \(\tilde{\mu}_{*}\)

**Definition 1** (Definition 2.5.9 in [12]).: _A probability measure \(\mu\) defined on a Polish space \((\mathcal{X},\mathrm{dis})\) satisfies a Cheeger isoperimetric inequality with constant \(\rho>0\) if for all Borel set \(A\subseteq\mathcal{X}\), it has_

\[\lim\inf_{\epsilon\to 0}\frac{\mu(A^{e})-\mu(A)}{\epsilon}\geq\frac{1}{\rho} \mu(A)\mu(A^{e}).\]

**Lemma C.5** (Theorem 2.5.14 in [12]).: _Let \(\mu\in\mathcal{P}_{1}(\mathcal{X})\) and let \(\mathrm{Ch}>0\). The following are equivalent._

1. \(\mu\) _satisfies a Cheeger isoperimetric inequality with constant_ \(\mathrm{Ch}\)_._
2. _For all Lipschitz_ \(f\colon\mathcal{X}\to\mathbb{R}\)_, it holds that_ \[\mathbb{E}_{\mu}\left|f-\mathbb{E}_{\mu}f\right|\leq 2\rho\cdot\mathbb{E}_{ \mu}\left\|\nabla f\right\|\] (59)

**Remark 2**.: _For a general non-log-concave distribution, a tight bound on the Cheeger constant can hardly be provided. However, considering the Cheeger isoperimetric inequality is stronger than the Poincare inequality, [6] lower bound the Cheeger constant \(\rho\) with \(\Omega(d^{1/2}c_{P})\) where \(c_{P}\) is the Poincare constant of \(\tilde{\mu}_{*}\). The lower bound of \(c_{P}\) can be generally obtained by the Bakry-Emery criterion and achieve \(\exp(-\tilde{\mathcal{O}}(d))\). While for target distributions with better properties, \(\rho\) can usually be much better. When the target distribution is a mixture of strongly log-concave distributions, the lower bound of \(\rho\) can achieve \(1/\mathrm{poly}(d)\) by [20]. For log-concave distributions, [23] proved that \(\rho=\Omega(1/(\mathrm{Tr}(\Sigma^{2}))^{1/4})\), where \(\Sigma\) is the covariance matrix of the distribution \(\tilde{\mu}_{*}\). When the target distribution is \(m\)-strongly log-concave, based on [16], \(\rho\) can even achieve \(\Omega(\sqrt{L})\). In the following, we will prove that the Cheeger constant can be independent of \(x_{0}\)._

**Lemma C.6**.: _Suppose \(\mu_{*}\) and \(\tilde{\mu}_{*}\) are defined as Eq. 22 and Eq. 58, respectively, where \(R\) in \(\tilde{\mu}_{*}\) is chosen as that in Lemma C.2. For any \(\epsilon\in(0,1)\), we have_

\[\frac{1}{2}\leq\frac{\int_{\Omega}\tilde{\mu}_{*}(\mathrm{d}\mathbf{z})}{\int_{ \mathbb{R}^{d}}\mu_{*}(\mathrm{d}\mathbf{z})}\leq 1.\]

Proof.: Suppose \(\mu_{*}\propto\exp(-g)\) and \(\tilde{\mu}_{*}\) are the original and truncated target distributions of the inner loops. Following from Lemma C.13, it has

\[\mathrm{TV}\left(\mu_{*},\tilde{\mu}_{*}\right)\leq\frac{\epsilon}{4}\]

when \(\tilde{\mu}_{*}\) is deduced by the \(R\) shown in Lemma C.2. Under these conditions, supposing \(\Omega=\mathcal{B}(\mathbf{0},R)\), then we have

\[\mathrm{TV}\left(\tilde{\mu}_{*},\mu\right) =\int_{\mathbb{R}^{d}}\left|\mu_{*}(\mathrm{d}\mathbf{z})-\tilde{\mu} _{*}(\mathrm{d}\mathbf{z})\right|=\int_{\Omega}\left|\mu_{*}(\mathrm{d}\mathbf{z})- \tilde{\mu}_{*}(\mathrm{d}\mathbf{z})\right|+\int_{\mathbb{R}^{d}-\Omega}\mu_{*}( \mathrm{d}\mathbf{z})\] \[=\int_{\Omega}\left|\frac{\exp\left(-g(\mathbf{z})\right)}{\int_{ \mathbb{R}^{d}}\exp\left(-g(\mathbf{z}^{\prime})\right)\mathrm{d}\mathbf{z}^{\prime}} -\frac{\exp\left(-g(\mathbf{z})\right)}{\int_{\Omega}\exp\left(-g(\mathbf{z}^{\prime} )\right)\mathrm{d}\mathbf{z}^{\prime}}\right|\mathrm{d}\mathbf{z}+\int_{\mathbb{R}^{d }-\Omega}\frac{\exp\left(-g(\mathbf{z})\right)}{\int_{\mathbb{R}^{d}}\exp\left(-g (\mathbf{z}^{\prime})\right)\mathrm{d}\mathbf{z}^{\prime}}\mathrm{d}\mathbf{z}. \tag{60}\]

Suppose

\[Z=\int_{\mathbb{R}^{d}}\exp\left(-g(\mathbf{z})\right)\mathrm{d}\mathbf{z}\quad\text{ and}\quad Z_{\Omega}=\int_{\Omega}\exp\left(-g(\mathbf{z})\right)\mathrm{d}\mathbf{z},\]

then the first term of RHS of Eq. 60 satisfies

\[\int_{\Omega}\left|\frac{\exp\left(-g(\mathbf{z})\right)}{\int_{ \mathbb{R}^{d}}\exp\left(-g(\mathbf{z}^{\prime})\right)\mathrm{d}\mathbf{z}^{\prime}}- \frac{\exp\left(-g(\mathbf{z})\right)}{\int_{\Omega}\exp\left(-g(\mathbf{z}^{\prime}) \right)\mathrm{d}\mathbf{z}^{\prime}}\right|\mathrm{d}\mathbf{z}\] \[=\left(\frac{1}{\int_{\Omega}\exp\left(-g(\mathbf{z}^{\prime}) \right)\mathrm{d}\mathbf{z}^{\prime}}-\frac{1}{\int_{\mathbb{R}^{d}}\exp\left(-g( \mathbf{z}^{\prime})\right)\mathrm{d}\mathbf{z}^{\prime}}\right)\cdot\int_{\Omega}\exp \left(-g(\mathbf{z}^{\prime})\right)\mathrm{d}\mathbf{z}^{\prime}=1-\frac{Z_{\Omega}}{Z}\]

and the second term satisfies

\[\int_{\mathbb{R}^{d}-\Omega}\frac{\exp\left(-g(\mathbf{z})\right)}{\int_{\mathbb{R }^{d}}\exp\left(-g(\mathbf{z}^{\prime})\right)\mathrm{d}\mathbf{z}^{\prime}}\mathrm{ d}\mathbf{z}=\frac{\int_{\mathbb{R}^{d}}\exp\left(-g(\mathbf{z}^{\prime})\right)\mathrm{d}\mathbf{z}^{\prime}- \int_{\Omega}\exp\left(-g(\mathbf{z}^{\prime})\right)\mathrm{d}\mathbf{z}^{\prime}}{ \int_{\mathbb{R}^{d}}\exp\left(-g(\mathbf{z}^{\prime})\right)\mathrm{d}\mathbf{z}^{ \prime}}=1-\frac{Z_{\Omega}}{Z}.\]

Combining all these things, we have

\[2\cdot\left(1-\frac{Z_{\Omega}}{Z}\right)\leq\frac{\epsilon}{4}\quad\Rightarrow \quad\frac{1}{2}\leq\frac{Z_{\Omega}}{Z}\leq 1\]

where we suppose \(\epsilon\leq 1\) without loss of generality. Hence, the proof is completed.

**Lemma C.7**.: _Suppose \(\mu_{*}\), \(\tilde{\mu}_{*}\) and \(\epsilon\) are under the same settings as those in Lemma C.6, the variance of \(\tilde{\mu}_{*}\) can be upper bounded by \(2d/L\)._

Proof.: According to the fact that \(\mu_{*}\) is a \(L\)-strongly log-concave distribution defined on \(\mathbb{R}^{d}\) with the mean \(\mathbf{v}_{m}\), which satisfies

\[\int_{\mathbb{R}^{d}}\mu(\mathbf{z})\left\|\mathbf{z}-\mathbf{v}_{m}\right\|^{2}\mathrm{d} \mathbf{z}\leq\frac{d}{L}\]

following from Lemma E.8. Suppose

\[\Omega=\mathcal{B}(\mathbf{0},R),\quad Z=\int_{\mathbb{R}^{d}}\exp(-g(\mathbf{z})) \mathrm{d}\mathbf{z},\quad Z_{\Omega}=\int_{\Omega}\exp(-g(\mathbf{z}))\mathrm{d}\mathbf{z}\]

where \(R\) shown in Lemma C.2, then the variance bound can be reformulated as

\[\int_{\Omega}\frac{\exp(-g(\mathbf{z}))}{Z}\left\|\mathbf{z}-\mathbf{v}_{m}\right\|^{2} \mathrm{d}\mathbf{z}+\int_{\mathbb{R}^{d}-\Omega}\frac{\exp(-g(\mathbf{z}))}{Z}\left\| \mathbf{z}-\mathbf{v}_{m}\right\|^{2}\mathrm{d}\mathbf{z}\leq\frac{d}{L},\]

which implies

\[\int_{\Omega}\frac{\exp(-g(\mathbf{z}))}{Z_{\Omega}}\left\|\mathbf{z}-\mathbf{v}_{m} \right\|^{2}\mathrm{d}\mathbf{z}\leq\frac{Z}{Z_{\Omega}}\cdot\frac{d}{L}\leq\frac{2 d}{L}. \tag{61}\]

Note that the last inequality follows from Lemma C.6. Besides, suppose the mean of \(\tilde{\mu}_{*}\) is \(\mathbf{v}_{\hat{m}}\), then we have

\[\begin{split}&\int_{\Omega}\frac{\exp(-g(\mathbf{z}))}{Z_{\Omega}} \cdot\left\|\mathbf{z}-\mathbf{v}_{m}\right\|^{2}\mathrm{d}\mathbf{z}=\int_{\Omega}\frac{ \exp(-g(\mathbf{z}))}{Z_{\Omega}}\cdot\left\|\mathbf{z}-\mathbf{v}_{\tilde{m}}+\mathbf{v}_{ \tilde{m}}-\mathbf{v}_{m}\right\|^{2}\mathrm{d}\mathbf{z}\\ &=\int_{\Omega}\frac{\exp(-g(\mathbf{z}))}{Z_{\Omega}}\cdot\left\|\mathbf{ z}-\mathbf{v}_{\tilde{m}}\right\|^{2}\mathrm{d}\mathbf{z}+2\cdot\int_{\Omega}\frac{ \exp(-g(\mathbf{z}))}{Z_{\Omega}}\cdot\left\langle\mathbf{z}-\mathbf{v}_{\tilde{m}},\mathbf{v}_ {\tilde{m}}-\mathbf{v}_{m}\right\rangle\mathrm{d}\mathbf{z}\\ &\quad+\int_{\Omega}\frac{\exp(-g(\mathbf{z}))}{Z_{\Omega}}\cdot\left\| \mathbf{v}_{m}-\mathbf{v}_{\tilde{m}}\right\|^{2}\mathrm{d}\mathbf{z}\\ &=\int_{\Omega}\frac{\exp(-g(\mathbf{z}))}{Z_{\Omega}}\cdot\left\|\mathbf{ z}-\mathbf{v}_{\tilde{m}}\right\|^{2}\mathrm{d}\mathbf{z}+\int_{\Omega}\frac{\exp(-g(\mathbf{z} ))}{Z_{\Omega}}\cdot\left\|\mathbf{v}_{m}-\mathbf{v}_{\tilde{m}}\right\|^{2}\mathrm{d }\mathbf{z}\end{split} \tag{62}\]

Combining Eq. 61 and Eq. 62, the variance of \(\tilde{\mu}_{*}\) satisfies

\[\int_{\Omega}\frac{\exp(-g(\mathbf{z}))}{Z_{\Omega}}\cdot\left\|\mathbf{z}-\mathbf{v}_{ \tilde{m}}\right\|^{2}\mathrm{d}\mathbf{z}\leq\frac{2d}{L}.\]

Hence, the proof is completed. 

**Corollary C.8**.: _For each truncated target distribution defined as Eq. 58, their Cheeger constant can be lower bounded by \(\rho=\Omega(\sqrt{L/d})\)._

Proof.: It can be easily found that \(\tilde{\mu}_{*}\) is log-concave distribution, which means their Cheeger constant can be upper bounded by \(\rho=\Omega(1/(\operatorname{Tr}(\Sigma))^{1/2})\), where \(\Sigma\) is the covariance matrix of the distribution \(\tilde{\mu}_{*}\). Under these conditions, we have

\[\operatorname{Tr}\left(\Sigma\right)=\int_{\Omega}\frac{\exp(-g(\mathbf{z}))}{Z_{ \Omega}}\cdot\left\|\mathbf{z}-\mathbf{v}_{\tilde{m}}\right\|^{2}\mathrm{d}\mathbf{z} \leq\frac{2d}{L},\]

where the last inequality follows from Lemma C.7. Hence, \(\rho=\Omega(\sqrt{L/d})\) and the proof is completed. 

#### c.3.2 The conductance properties of \(\tilde{\mathcal{T}}_{*}\)

We prove the conductance properties of \(\tilde{\mathcal{T}}_{*,\mathbf{z}}\) with the following lemma.

**Lemma C.9** (Lemma 13 in [22]).: _Let \(\tilde{\mathcal{T}}_{*,\mathbf{z}}\) be a be a time-reversible Markov chain on \(\Omega\) with stationary distribution \(\tilde{\mu}_{*}\). Fix any \(\Delta>0\), suppose for any \(\mathbf{z},\mathbf{z}^{\prime}\in\Omega\) with \(\left\|\mathbf{z}-\mathbf{z}^{\prime}\right\|\leq\Delta\) we have \(\operatorname{TV}\left(\tilde{\mathcal{T}}_{*,\mathbf{z}},\tilde{\mathcal{T}}_{*, \mathbf{z}^{\prime}}\right)\leq 0.99\), then the conductance of \(\tilde{\mathcal{T}}_{*,\mathbf{z}}\) satisfies \(\phi\geq C\rho\Delta\) for some absolute constant \(C\), where \(\rho\) is the Cheeger constant of \(\tilde{\mu}_{*}\)._In order to apply Lemma C.9, we have known the Cheeger constant of \(\tilde{\mu}_{*}\) is \(\rho\). We only need to verify the corresponding condition, i.e., proving that as long as \(\|\mathbf{z}-\mathbf{z}^{\prime}\|\leq\Delta\), we have \(\mathrm{TV}\left(\tilde{\mathcal{T}}_{*,\mathbf{z}},\tilde{\mathcal{T}}_{*,\mathbf{z}^{ \prime}}\right)\leq 0.99\) for some \(\Delta\). Recalling Eq. 33, we have

\[\tilde{\mathcal{T}}_{*,\mathbf{z}}(\mathrm{d}\hat{\mathbf{z}})= \tilde{\mathcal{T}}^{\prime}_{*,\mathbf{z}}(\mathrm{d}\hat{\mathbf{z}}) \cdot\tilde{a}_{*,\mathbf{z}}(\hat{\mathbf{z}})+\left(1-\int_{\Omega}\tilde{a}_{*,\bm {z}}(\tilde{\mathbf{z}})\tilde{\mathcal{T}}^{\prime}_{*,\mathbf{z}}(\mathrm{d}\tilde{ \mathbf{z}})\right)\cdot\delta_{\mathbf{z}}(\mathrm{d}\hat{\mathbf{z}}) \tag{63}\] \[= \left(\frac{1}{2}\delta_{\mathbf{z}}(\mathrm{d}\hat{\mathbf{z}})+\frac{1} {2}\cdot\tilde{Q}^{\prime}_{*,\mathbf{z}}(\mathrm{d}\hat{\mathbf{z}})\right)\cdot \tilde{a}_{*,\mathbf{z}}(\hat{\mathbf{z}})+\left[1-\int\tilde{a}_{*,\mathbf{z}}(\tilde{ \mathbf{z}})\cdot\left(\frac{1}{2}\delta_{\mathbf{z}}(\mathrm{d}\hat{\mathbf{z}})+\frac{1} {2}\tilde{Q}^{\prime}_{*,\mathbf{z}}(\mathrm{d}\tilde{\mathbf{z}})\right)\right]\cdot \delta_{\mathbf{z}}(\mathrm{d}\hat{\mathbf{z}})\] \[= \left(\frac{1}{2}\delta_{\mathbf{z}}(\mathrm{d}\hat{\mathbf{z}})+\frac{1} {2}\cdot\tilde{Q}^{\prime}_{*,\mathbf{z}}(\mathrm{d}\hat{\mathbf{z}})\right)\cdot \tilde{a}_{*,\mathbf{z}}(\hat{\mathbf{z}})+\left(1-\frac{1}{2}\tilde{a}_{*,\mathbf{z}}(\bm {z})-\frac{1}{2}\int\tilde{a}_{*,\mathbf{z}}(\tilde{\mathbf{z}})\cdot\tilde{Q}^{\prime }_{*,\mathbf{z}}(\mathrm{d}\tilde{\mathbf{z}})\right)\cdot\delta_{\mathbf{z}}(\mathrm{d} \hat{\mathbf{z}})\] \[= \left(1-\frac{1}{2}\int\tilde{a}_{*,\mathbf{z}}(\tilde{\mathbf{z}})\cdot \tilde{Q}^{\prime}_{*,\mathbf{z}}(\mathrm{d}\tilde{\mathbf{z}})\right)\cdot\delta_{\bm {z}}(\mathrm{d}\hat{\mathbf{z}})+\frac{1}{2}\cdot\tilde{Q}^{\prime}_{*,\mathbf{z}}( \mathrm{d}\hat{\mathbf{z}})\cdot\tilde{a}_{*,\mathbf{z}}(\hat{\mathbf{z}})\] \[= \left(1-\frac{1}{2}\int_{\Omega_{\mathbf{z}}}\tilde{a}_{*,\mathbf{z}}( \tilde{\mathbf{z}})\tilde{Q}_{*,\mathbf{z}}(\mathrm{d}\tilde{\mathbf{z}})\right)+\frac{1} {2}\cdot\tilde{Q}_{*,\mathbf{z}}(\mathrm{d}\hat{\mathbf{z}})\cdot\tilde{a}_{*,\mathbf{z}}( \hat{\mathbf{z}})\cdot\mathbf{1}\left[\hat{\mathbf{z}}\in\Omega_{\mathbf{z}}\right],\]

where the second inequality follows from Eq. 31 and the last inequality follows from Eq. 30. Then the rest will be proving the upper bound of \(\mathrm{TV}\left(\tilde{\mathcal{T}}_{*,\mathbf{z}},\tilde{\mathcal{T}}_{*,\mathbf{z}^ {\prime}}\right)\), and we state another two useful lemmas as follows. 

**Lemma C.10** (Lemma B.6 in [41]).: _For any two points \(\mathbf{z},\mathbf{z}^{\prime}\in\mathbb{R}^{d}\), it holds that_

\[\mathrm{TV}\left(\tilde{Q}_{*,\mathbf{z}}(\cdot),\tilde{Q}_{*,\mathbf{z}^{\prime}}( \cdot)\right)\leq\frac{(1+3L\tau)\left\|\mathbf{z}-\mathbf{z}^{\prime}\right\|}{\sqrt{ 2\tau}}\]

Proof.: This lemma can be easily obtained by plugging the smoothness of \(g\), i.e., \(3L\), into Lemma B.6 in [41]. 

**Corollary C.11** (Variant of Lemma 6.5 in [41]).: _Under Assumption [41]-[42], we set_

\[\eta=\frac{1}{2}\log\frac{2L+1}{2L}\quad\mathrm{and}\quad G\coloneqq\left\| \nabla g(\mathbf{0})\right\|.\]

_If we set_

\[\tau\leq\frac{1}{16\cdot(3LR+G+\epsilon_{\mathrm{score}})^{2}}\quad\mathrm{and} \quad r=3\cdot\sqrt{\tau d\log\frac{8S}{\epsilon}}\]

_there exist absolute constants \(c_{0}\), such that \(\phi\geq c_{0}\rho\sqrt{\tau}\) where \(\rho\) is the Cheeger constant of the distribution \(\tilde{\mu}_{*}\)._

Proof.: By the definition of total variation distance, there exists a set \(\mathcal{A}\subseteq\Omega\) satisfying

\[\mathrm{TV}\left(\tilde{\mathcal{T}}_{*,\mathbf{z}}(\cdot),\tilde{\mathcal{T}}_{*, \mathbf{z}^{\prime}}(\cdot)\right)=\left|\tilde{\mathcal{T}}_{*,\mathbf{z}}(\mathcal{ A})-\tilde{\mathcal{T}}_{*,\mathbf{z}^{\prime}}(\mathcal{A})\right|.\]

Due to the closed form of \(\tilde{\mathcal{T}}_{*,\mathbf{z}}\) shown in Eq. 63, we have

\[\tilde{\mathcal{T}}_{*,\mathbf{z}}(\mathcal{A})=\left(1-\frac{1}{2}\int_{\tilde{ \mathbf{z}}\in\Omega_{\mathbf{z}}}\tilde{a}_{*,\mathbf{z}}(\tilde{\mathbf{z}})\tilde{Q}_{*,\bm {z}}(\mathrm{d}\tilde{\mathbf{z}})\right)+\frac{1}{2}\int_{\tilde{\mathbf{z}}\in \mathcal{A}}\tilde{a}_{*,\mathbf{z}}(\hat{\mathbf{z}})\cdot\mathbf{1}\left[\hat{\mathbf{z}} \in\Omega_{\mathbf{z}}\right]\tilde{Q}_{*,\mathbf{z}}(\mathrm{d}\hat{\mathbf{z}})\]

Under this condition, we have

\[\left|\tilde{\mathcal{T}}_{*,\mathbf{z}}(\mathcal{A})-\tilde{\mathcal{T }}_{*,\mathbf{z}^{\prime}}(\mathcal{A})\right|\leq \underbrace{\max_{\tilde{\mathbf{z}}}\left(1-\frac{1}{2}\int_{\Omega_ {\tilde{\mathbf{z}}}}\tilde{a}_{*,\mathbf{z}}(\tilde{\mathbf{z}})\tilde{Q}_{*,\mathbf{z}}( \mathrm{d}\tilde{\mathbf{z}})\right)}_{\text{Term 1}1}\] \[+\underbrace{\frac{1}{2}\left|\int_{\hat{\mathbf{zUpper bound \(\operatorname{Term}1\).We first consider to lower bound \(\tilde{a}_{*,\hat{\mathbf{z}}}(\tilde{\mathbf{z}})\) in the following. According to Eq. 32, we have

\[\tilde{a}_{*,\hat{\mathbf{z}}}(\tilde{\mathbf{z}})\geq\exp\left(-g(\tilde{\mathbf{z}})-\frac {\left\|\hat{\mathbf{z}}-\tilde{\mathbf{z}}+\tau\nabla g(\tilde{\mathbf{z}})\right\|^{2}}{ 4\tau}+g(\hat{\mathbf{z}})+\frac{\left\|\tilde{\mathbf{z}}-\hat{\mathbf{z}}+\tau\nabla g( \hat{\mathbf{z}})\right\|^{2}}{4\tau}\right),\]

which means

\[4\ln\tilde{a}_{*,\hat{\mathbf{z}}}(\tilde{\mathbf{z}})\geq \underbrace{\tau\cdot\left(\left\|\nabla g(\hat{\mathbf{z}})\right\|^ {2}-\left\|\nabla g(\tilde{\mathbf{z}})\right\|^{2}\right)}_{\operatorname{Term}1.1}\] \[\underbrace{-2\cdot\left(g(\tilde{\mathbf{z}})-g(\hat{\mathbf{z}})-\langle \nabla g(\hat{\mathbf{z}}),\tilde{\mathbf{z}}-\hat{\mathbf{z}}\rangle\right)}_{\operatorname {Term}1.2}\] \[\underbrace{+2\cdot\left(g(\hat{\mathbf{z}})-g(\tilde{\mathbf{z}})-\langle \nabla g(\hat{\mathbf{z}}),\hat{\mathbf{z}}-\tilde{\mathbf{z}}\rangle\right)}_{\operatorname {Term}1.3}.\]

Since \(\operatorname{Term}1.2\) and \(\operatorname{Term}1.3\) are grouped to more easily apply the strong convexity and smoothness of \(g\) (Lemma C.1), it has

\[\operatorname{Term}1.2\geq-3L\left\|\hat{\mathbf{z}}-\tilde{\mathbf{z}}\right\|^{2} \quad\text{and}\quad\operatorname{Term}1.3\geq L\left\|\hat{\mathbf{z}}-\tilde{ \mathbf{z}}\right\|^{2}\geq 0.\]

Besides, by requiring \(\tau\leq 1/3L\), we have

\[\operatorname{Term}1.1= \tau\cdot\left\langle\nabla g(\hat{\mathbf{z}})-\nabla g(\tilde{\mathbf{ z}}),\nabla g(\hat{\mathbf{z}})+\nabla g(\tilde{\mathbf{z}})\right\rangle\] \[\geq -\tau\cdot\left\|\nabla g(\hat{\mathbf{z}})-\nabla g(\tilde{\mathbf{z}}) \right\|\cdot\left\|\nabla g(\hat{\mathbf{z}})+\nabla g(\tilde{\mathbf{z}})\right\|\] \[\geq -3L\tau\left\|\hat{\mathbf{z}}-\tilde{\mathbf{z}}\right\|\cdot\left(2 \left\|\nabla g(\hat{\mathbf{z}})\right\|+3L\left\|\hat{\mathbf{z}}-\tilde{\mathbf{z}} \right\|\right)\geq-3L\tau^{2}\left\|\nabla g(\hat{\mathbf{z}})\right\|^{2}-6L \left\|\hat{\mathbf{z}}-\tilde{\mathbf{z}}\right\|^{2}.\]

Therefore,

\[4\ln\tilde{a}_{*,\hat{\mathbf{z}}}(\tilde{\mathbf{z}})\geq -3L\tau^{2}\left\|\nabla g(\hat{\mathbf{z}})\right\|^{2}-9L\left\| \hat{\mathbf{z}}-\tilde{\mathbf{z}}\right\|^{2}=-3L\tau^{2}\left\|\nabla g(\hat{\mathbf{z }})\right\|^{2}-9L\left\|\tau\cdot\nabla g(\hat{\mathbf{z}})+\sqrt{2\tau}\cdot \xi\right\|^{2}\] \[\geq -21L\tau^{2}\left\|\nabla g(\hat{\mathbf{z}})\right\|^{2}-36L\tau \left\|\xi\right\|^{2},\]

and

\[\ln\tilde{a}_{*,\hat{\mathbf{z}}}(\tilde{\mathbf{z}})\geq-6L\tau^{2}\left\|\nabla g( \hat{\mathbf{z}})\right\|^{2}-9L\tau\left\|\xi\right\|^{2}\geq-6L\tau^{2}\cdot \left(3LR+\left\|\nabla g(\mathbf{0})\right\|\right)^{2}-9L\tau\|\xi\|^{2}\]

where the last inequality follows from

\[\left\|\nabla g(\hat{\mathbf{z}})\right\|\leq\left\|\nabla g(\hat{\mathbf{z}})-\nabla g (\mathbf{0})\right\|+\left\|\nabla g(\mathbf{0})\right\|\leq 3LR+\left\|\nabla g(\mathbf{0}) \right\|.\]

Under these conditions, we have

\[\operatorname{Term}1\leq 1-\frac{1}{2}\cdot\exp\left(-6L\tau^{2}\left(3LR+\left\|\nabla g (\mathbf{0})\right\|\right)^{2}\right)\cdot\min\int_{\Omega_{\mathbf{z}}}\exp\left(-9 L\tau\|\xi\|^{2}\right)\cdot\tilde{q}_{*,\hat{\mathbf{z}}}(\tilde{\mathbf{z}})\mathrm{d} \tilde{\mathbf{z}} \tag{64}\] \[= 1-\frac{1}{2}\cdot\exp\left(-6L\tau^{2}\left(3LR+\left\|\nabla g (\mathbf{0})\right\|\right)^{2}\right)\cdot\mathbb{E}_{\xi\sim\mathcal{N}(\mathbf{0}, \mathbf{I})}\left[\exp\left(-9L\tau\|\xi\|^{2}\right)\right]\] \[\leq 1-0.4\cdot\exp\left(-6L\tau^{2}\left(3LR+\left\|\nabla g(\mathbf{0} )\right\|\right)^{2}\right)\cdot\exp\left(-18L\tau d\right),\]

where the last inequality follows from the Markov inequality shown in the following

\[\mathbb{E}_{\xi\sim\mathcal{N}(\mathbf{0},\mathbf{I})}\left[\exp\left(-9L \tau\|\xi\|^{2}\right)\right]\geq \exp\left(-18L\tau d\right)\cdot\mathbb{P}_{\xi\sim\mathcal{N}( \mathbf{0},\mathbf{I})}\left[\exp\left(-9L\tau\|\xi\|^{2}\right)\geq\exp\left(-18L \tau d\right)\right]\] \[= \exp\left(-18L\tau d\right)\cdot\mathbb{P}_{\xi\sim\mathcal{N}(\bm {0},\mathbf{I})}\left[\left\|\xi\|^{2}\leq 2d\right]\geq\exp\left(-18L\tau d\right) \cdot\left(1-\exp(-d/2)\right).\]

Then, by choosing

\[\tau\leq\frac{1}{16\sqrt{L}\cdot\left(3LR+\left\|\nabla g(\mathbf{0})\right\| \right)}, \tag{65}\]

it has \(6L\tau^{2}\left(3LR+\left\|\nabla g(\mathbf{0})\right\|\right)^{2}\leq 1/40\). Besides by choosing

\[\tau\leq\frac{1}{L^{2}R^{2}}\leq\frac{1}{40\cdot 18L\cdot\left(\sqrt{d}+\sqrt{\ln \frac{16S}{\epsilon}}\right)^{2}}\leq\frac{1}{40\cdot 18L\cdot d}, \tag{66}\]where the last inequality follows from the range of \(R\) shown in Lemma C.2, it has \(18Ld\tau\leq 1/40\). Under these conditions, considering Eq. 64, we have

\[\mathrm{Term}\;1\leq 1-0.5\cdot\min_{\mathbf{z}\in\Omega,\mathbf{z}\in\Omega_{\mathbf{z}}} \int_{\Omega_{\mathbf{z}}}\tilde{a}_{*,\mathbf{z}}(\tilde{\mathbf{z}})\tilde{Q}_{*,\mathbf{z}} (\mathrm{d}\tilde{\mathbf{z}})\leq 1-0.4\cdot e^{-1/20}. \tag{67}\]

Then, combining the step size choices of Eq. 65, Eq. 66, and Lemma C.2, since the requirement

\[\tau\leq\frac{1}{16\sqrt{L}\cdot(3LR+\left\|\nabla g(\mathbf{0})\right\|)},\quad \tau\leq\frac{1}{L^{2}R^{2}}\quad\mathrm{and}\quad\tau\leq\frac{d}{(3LR+\left\| \nabla g(\mathbf{0})\right\|+\epsilon_{\mathrm{score}})^{2}}\]

can be achieved by

\[\tau\leq 16^{-1}\cdot(3LR+\left\|\nabla g(\mathbf{0})\right\|+\epsilon_{\mathrm{ score}})^{-2}, \tag{68}\]

the range of \(\tau\) can be determined.

Upper bound \(\mathrm{Term}\) 2.In This part, we use similar techniques as those shown in Lemma 6.5 of [41]. According to the triangle inequality, we have

\[2\cdot\mathrm{Term}\;2\leq \int_{\mathbf{z}\in\mathcal{A}}\left(1-\tilde{a}_{*,\mathbf{z}}(\hat{\bm {z}})\right)\tilde{q}(\hat{\mathbf{z}}|\mathbf{z})\mathbf{1}\left[\hat{\mathbf{z}}\in\Omega_{ \mathbf{z}}\right]\mathrm{d}\hat{\mathbf{z}}+\int_{\mathbf{\hat{z}}\in\mathcal{A}}\left(1- \tilde{a}_{*,\mathbf{z}^{\prime}}(\hat{\mathbf{z}})\right)\tilde{q}(\hat{\mathbf{z}}|\mathbf{z }^{\prime})\mathbf{1}\left[\hat{\mathbf{z}}\in\Omega_{\mathbf{z}^{\prime}}\right]\mathrm{ d}\hat{\mathbf{z}}\] \[+\left|\int_{\mathbf{\hat{z}}\in\mathcal{A}}\left(\tilde{q}(\hat{\mathbf{ z}}|\mathbf{z})\mathbf{1}\left[\hat{\mathbf{z}}\in\Omega_{\mathbf{z}}\right]-\tilde{q}(\hat{\mathbf{z}}| \mathbf{z}^{\prime})\mathbf{1}\left[\hat{\mathbf{z}}\in\Omega_{\mathbf{z}^{\prime}}\right] \right)\mathrm{d}\hat{\mathbf{z}}\right|\] \[\leq 2\cdot\left(1-\min_{\mathbf{\hat{z}}\in\Omega,\mathbf{\hat{z}}\in\Omega_ {\mathbf{z}}}\int_{\Omega_{\mathbf{z}}}\tilde{a}_{*,\mathbf{z}}(\tilde{\mathbf{z}})\tilde{Q}_ {*,\mathbf{z}}(\mathrm{d}\tilde{\mathbf{z}})\right)\] \[+\underbrace{\left|\int_{\mathbf{\hat{z}}\in\mathcal{A}}\left(\tilde{ q}(\hat{\mathbf{z}}|\mathbf{z})\mathbf{1}\left[\hat{\mathbf{z}}\in\Omega_{\mathbf{z}}\right]-\tilde{q}( \hat{\mathbf{z}}|\mathbf{z}^{\prime})\mathbf{1}\left[\hat{\mathbf{z}}\in\Omega_{\mathbf{z}^{\prime }}\right]\right)\mathrm{d}\hat{\mathbf{z}}\right|}_{\mathrm{Term}\;2.1}. \tag{69}\]

Then, we upper bound \(\mathrm{Term}\) 2.1 as follows

\[\mathrm{Term}\;2.1\leq \left|\int_{\mathbf{\hat{z}}\in\mathcal{A}}\mathbf{1}\left[\hat{\mathbf{z}} \in\Omega_{\mathbf{z}^{\prime}}\right]\cdot\left(\tilde{q}_{i}\hat{\mathbf{z}}|\mathbf{z} )-\tilde{q}_{i}\hat{\mathbf{z}}|\mathbf{z}^{\prime})\right\rangle\right|+\left|\int_{ \mathbf{\hat{z}}\in\mathcal{A}}\left(\mathbf{1}\left[\hat{\mathbf{z}}\in\Omega_{\mathbf{z}} \right]-\mathbf{1}\left[\hat{\mathbf{z}}\in\Omega_{\mathbf{z}^{\prime}}\right]\right)\cdot \tilde{q}(\hat{\mathbf{z}}|\mathbf{z})\right|\] \[\leq \mathrm{TV}\left(\tilde{Q}_{*,\mathbf{z}}(\cdot),\tilde{Q}_{*,\mathbf{z} ^{\prime}}(\cdot)\right)+\max\left\{\int_{\mathbf{\hat{z}}\in\Omega_{\mathbf{z}^{ \prime}}-\Omega_{\mathbf{z}}}\tilde{q}(\hat{\mathbf{z}}|\mathbf{z})\mathrm{d}\hat{\mathbf{z}},\int_{\mathbf{\hat{z}}\in\Omega_{\mathbf{z}}-\Omega_{\mathbf{z}^{\prime}}}\tilde{q}(\hat {\mathbf{z}}|\mathbf{z}^{\prime})\mathrm{d}\hat{\mathbf{z}}\right\}\] \[\leq \mathrm{TV}\left(\tilde{Q}_{*,\mathbf{z}}(\cdot),\tilde{Q}_{*,\mathbf{z} ^{\prime}}(\cdot)\right)+\max\left\{\int_{\mathbf{\hat{z}}\in\mathbb{R}^{d}- \Omega_{\mathbf{z}}}\tilde{q}(\hat{\mathbf{z}}|\mathbf{z})\mathrm{d}\hat{\mathbf{z}},\int_{ \mathbf{\hat{z}}\in\mathbb{R}^{d}-\Omega_{\mathbf{z}^{\prime}}}\tilde{q}(\hat{\mathbf{z}} |\mathbf{z}^{\prime})\mathrm{d}\hat{\mathbf{z}}\right\}\]

According to the definition, \(\tilde{q}_{*,\mathbf{z}}(\cdot)\) is Gaussian distribution with mean \(\mathbf{z}-\tau\nabla g(\mathbf{z})\) and covariance matrix \(2\tau\mathbf{I}\), thus we have

\[\int_{\mathbf{\hat{z}}\in\mathbb{R}^{d}-\Omega_{\mathbf{z}}}\tilde{q}(\hat {\mathbf{z}}|\mathbf{z})\mathrm{d}\hat{\mathbf{z}}\leq\mathbb{P}_{\mathbf{\hat{z}}\sim\chi_{ d}^{2}}\left[\hat{\mathbf{z}}\geq\frac{1}{2}\left(r-\tau\left\|\nabla g(\mathbf{z}) \right\|\right)^{2}/\tau\right]\] \[\int_{\mathbf{\hat{z}}\in\mathbb{R}^{d}-\Omega_{\mathbf{z}^{\prime}}} \tilde{q}(\hat{\mathbf{z}}|\mathbf{z}^{\prime})\mathrm{d}\hat{\mathbf{z}}\leq\mathbb{P}_{ \mathbf{\hat{z}}\sim\chi_{d}^{2}}\left[\hat{\mathbf{z}}\geq\frac{1}{2}\left(r-\tau\left\| \nabla g(\mathbf{z})\right\|-\left\|\mathbf{z}-\mathbf{z}^{\prime}\right\|\right)^{2}/\tau \right].\]

Then, we start to lower bound

\[r-\tau\left\|\nabla g(\mathbf{z})\right\|-\left\|\mathbf{z}-\mathbf{z}^{\prime}\right\|.\]

Then, we require

\[\left\|\mathbf{z}-\mathbf{z}^{\prime}\right\|\leq 0.1r\quad\mathrm{and}\quad\tau\leq\frac{d}{35 \cdot\left(3LR+G\right)^{2}} \tag{70}\]

where the latter condition can be easily covered by the choice in Eq. 68 when \(d\geq 3\) without loss of generality. Under this condition, we have

\[\tau\leq(0.17)^{2}\cdot\frac{d}{(3LR+G)^{2}}\quad\Leftrightarrow\quad\sqrt{ \tau}\leq\frac{0.17\sqrt{d}}{3LR+G}. \tag{71}\]Since we have

\[\|\nabla g(\mathbf{z})\|=\|\nabla g(\mathbf{z})-\nabla g(\mathbf{0})+\nabla g(\mathbf{0})\|\leq 3L \cdot\|\mathbf{z}\|+G\leq 3LR+G,\]

by the smoothness, it has

\[\sqrt{\tau}\leq\frac{0.17\sqrt{d}}{\|\nabla g(\mathbf{z})\|}\quad\Leftrightarrow \quad\tau\,\|\nabla g(\mathbf{z})\|\leq 0.17\sqrt{\tau d} \tag{72}\]

Plugging Eq. 72 and Eq. 71 into Eq. 70, we have

\[r-\tau\,\|\nabla g(\mathbf{z})\|-\|\mathbf{z}-\mathbf{z}^{\prime}\|\geq 0.9r-0.17\sqrt{ \tau d}\geq\sqrt{6.4\tau d}\]

where the last inequality follows from the choice of \(r\) shown in Lemma C.3, i.e.,

\[r=3\cdot\sqrt{\tau d\log\frac{8S}{\epsilon}}\geq 3\cdot\sqrt{\tau d}.\]

Under these conditions, we have

\[\max\left\{\int_{\mathbf{\hat{z}}\in\mathbb{R}^{d}-\Omega_{\mathbf{z}}}\tilde{q}(\hat{ \mathbf{z}}|\mathbf{z})\mathrm{d}\hat{\mathbf{z}},\int_{\mathbf{\hat{z}}\in\mathbb{R}^{d}- \Omega_{\mathbf{z}^{\prime}}}\tilde{q}(\hat{\mathbf{z}}|\mathbf{z}^{\prime})\mathrm{d}\hat {\mathbf{z}}\right\}\leq\mathbb{P}_{\hat{\mathbf{z}}\sim\chi_{d}^{2}}\left(\|\mathbf{z}\| \geq 3.2d\right)\leq 0.1.\]

Then combine the above results and apply Lemma C.10, assume \(\tau\leq 1/(3L)\), we have

\[\mathrm{Term}\ 2.1\leq 0.1+\mathrm{TV}\left(\tilde{Q}_{*,\mathbf{z}}(\cdot),\tilde{Q }_{*,\mathbf{z}^{\prime}}(\cdot)\right)\leq 0.1+\sqrt{2/\tau}\cdot\|\mathbf{z}-\mathbf{z}^{ \prime}\|\]

Plugging the above into Eq. 69, we have

\[\mathrm{Term}\ 2\leq \left(1-\min_{\mathbf{\hat{z}}\in\Omega,\mathbf{\hat{z}}\in\Omega_{\mathbf{z} }}\int_{\Omega_{\mathbf{z}}}\tilde{a}_{*,\mathbf{\hat{z}}}(\tilde{\mathbf{z}})\tilde{Q}_{ *,\mathbf{\hat{z}}}(\mathrm{d}\tilde{\mathbf{z}})\right)+\frac{1}{2}\cdot\left(0.1+ \sqrt{\frac{2}{\tau}}\cdot\|\mathbf{z}-\mathbf{z}^{\prime}\|\right)\] \[\leq \left(1-0.8\cdot e^{-1/20}\right)+0.05+(2\tau)^{-1/2}\cdot\|\mathbf{z }-\mathbf{z}^{\prime}\|,\]

where the second inequality follows from Eq. 67.

After upper bounding \(\mathrm{Term}\ 1\) and \(\mathrm{Term}\ 2\), we have

\[\mathrm{TV}\left(\tilde{\mathcal{T}}_{*,\mathbf{z}}(\cdot),\tilde{ \mathcal{T}}_{*,\mathbf{z}^{\prime}}(\cdot)\right)\leq 1-0.4\cdot e^{-1/20}+\left(1-0.8\cdot e^{-1/20}\right)+0.05+(2 \tau)^{-1/2}\cdot\|\mathbf{z}-\mathbf{z}^{\prime}\|\] \[\leq 0.91+(2\tau)^{-1/2}\cdot\|\mathbf{z}-\mathbf{z}^{\prime}\|\leq 0.99\]

where the last inequality can be established by requiring \(\|\mathbf{z}-\mathbf{z}^{\prime}\|\leq\sqrt{2\tau}\). Combining Lemma C.9, the conductance of \(\tilde{\mu}_{*}\) satisfies

\[\phi\geq c_{0}\cdot\rho\sqrt{2\tau}.\]

Hence, the proof is completed. 

The connection between \(\tilde{\mu}_{S}\) and \(\tilde{\mu}_{*}\).With the conductance of truncated target distribution, we are able to find the convergence of the projected implementation of Alg. 2. Besides, the gap between the truncated target \(\tilde{\mu}_{*}\) and the true target \(\mu_{*}\) can be upper bounded by controlling \(R\) while such an \(R\) will be dominated by the range of \(R\) shown in Lemma C.2. In this section, we will omit several details since many of them have been proven in [41].

**Lemma C.12** (Lemma 6.4 in [41]).: _Let \(\tilde{\mu}_{S}\) be distributions of the outputs of the projected implementation of Alg. 2. Under Assumption [41]-[42], if the transition kernel \(\tilde{\mathcal{T}}_{\mathbf{z}}(\cdot)\) is \(\delta\)-close to \(\tilde{\mathcal{T}}_{*,\mathbf{z}}\) with \(\delta\leq\min\left\{1-\sqrt{2}/2,\phi/16\right\}\) (\(\phi\) denotes the conductance of \(\tilde{\mu}_{*}\)), then for any \(\lambda\)-warm start initial distribution with respect to \(\tilde{\mu}_{*}\), it holds that_

\[\mathrm{TV}\left(\tilde{\mu}_{S},\tilde{\mu}_{*}\right)\leq\lambda\cdot\left(1 -\phi^{2}/8\right)^{S}+16\delta/\phi.\]

**Lemma C.13** (Lemma 6.6 in [41]).: _For any \(\epsilon\in(0,1)\), set \(R\) to make it satisfy_

\[\mu\left(\mathcal{B}(\mathbf{0},R)\right)\geq 1-\frac{\epsilon}{12},\]

_and \(\tilde{\mu}_{*}\) be the truncated target distribution of \(\mu_{*}\). Then the total variation distance between \(\mu_{*}\) and \(\tilde{\mu}_{*}\) can be upper bounded by \(\mathrm{TV}\left(\tilde{\mu}_{*},\mu_{*}\right)\leq\epsilon/4\)._

### Main Theorems of InnerMALA implementation

**Lemma C.14**.: _Under Assumption **[A1]-[A2]**, we can upper bound \(G=\left\|\nabla g(\mathbf{0})\right\|\) as_

\[\left\|\nabla g(\mathbf{0})\right\|\leq L\cdot\sqrt{2(d+m_{2}^{2})}+3L\cdot \left\|\mathbf{x}_{0}\right\|.\]

_Furthermore, we can reformulate \(R\) as_

\[R=63\cdot\sqrt{\left(d+m_{2}^{2}+\|\mathbf{x}_{0}\|^{2}\right)\cdot\log\frac{16S}{ \epsilon}}\]

_to make it satisfy the requirement shown in Lemma C.3. Then, the range of inner step sizes, i.e., \(\tau\), will satisfy_

\[\tau\leq C_{\tau}\cdot\left(L^{2}\left(d+m_{2}^{2}+\|\mathbf{x}_{0}\|^{2}\right) \cdot\log\frac{16S}{\epsilon}\right)^{-1},\]

_where the absolute constant \(C_{\tau}=2^{-4}\cdot 3^{-8}\cdot 7^{-2}\)._

Proof.: To make the bound more explicit, we control \(R\) and \(G\) in our previous analysis. For \(G=\left\|\nabla g(\mathbf{0})\right\|\), according to Eq. 22, we have

\[\nabla g(\mathbf{z})=\nabla f_{(K-k-1)\eta}(\mathbf{z})+\frac{e^{-2\eta}\mathbf{z}-e^{- \eta}\mathbf{x}_{0}}{(1-e^{-2\eta})},\]

which means

\[\left\|\nabla g(\mathbf{0})\right\|\leq \left\|\nabla f_{(K-k-1)\eta}(\mathbf{0})\right\|+\left\|\frac{e^ {-\eta}\mathbf{x}_{0}}{1-e^{-2\eta}}\right\|\] \[\leq \left\|\nabla f_{(K-k-1)\eta}(\mathbf{0})\right\|+\sqrt{\frac{2L }{2L+1}}\cdot(2L+1)\cdot\|\mathbf{x}_{0}\|\leq\left\|\nabla f_{(K-k-1)\eta}( \mathbf{0})\right\|+(2L+1)\cdot\left\|\mathbf{x}_{0}\right\|.\]

Besides, we should note \(f_{(K-k-1)\eta}\) is the smooth (Assumption **[A1]**) energy function of \(p_{(K-k-1)\eta}\) denoting the underlying distribution of time \((K-k-1)\eta\) in the forward OU process. Then, we have

\[\left\|\nabla f_{(K-k-1)\eta}(\mathbf{0})\right\|^{2}= \mathbb{E}_{p_{(K-k-1)\eta}}\left[\left\|\nabla f_{(K-k-1)\eta}( \mathbf{0})\right\|^{2}\right]\] \[\leq 2\mathbb{E}_{p_{(K-k-1)\eta}}\left[\left\|\nabla f_{(K-k-1)\eta }(\mathbf{x})\right\|^{2}\right]+2\mathbb{E}_{p(K-k-1)\eta}\left[\left\|\nabla f _{(K-k-1)\eta}(\mathbf{x})-\nabla f_{(K-k-1)\eta}(\mathbf{0})\right\|^{2}\right]\] \[\leq 2Ld+2L^{2}\mathbb{E}_{p_{(K-k-1)\eta}}\left[\left\|\mathbf{x} \right\|^{2}\right]\leq 2Ld+2L^{2}\max\left\{d,m_{2}^{2}\right\}\leq 2L^{2}(d+m_{2} ^{2}) \tag{73}\]

where the first inequality follows from Lemma E.6, and the third inequality follows from Lemma E.7. Under these conditions, we have

\[\left\|\nabla g(\mathbf{0})\right\|\leq L\cdot\sqrt{2(d+m_{2}^{2})}+3L\cdot \left\|\mathbf{x}_{0}\right\|. \tag{74}\]

Then, for \(R\) defined as

\[R\geq\max\left\{8\cdot\sqrt{\frac{\|\nabla g(\mathbf{0})\|^{2}}{L^{2}}+\frac{d }{L}},63\cdot\sqrt{\frac{d}{L}\log\frac{16S}{\epsilon}}\right\},\]

we can choose \(R\) to be the upper bound of RHS. Considering

\[8\cdot\sqrt{\frac{\|\nabla g(\mathbf{0})\|^{2}}{L^{2}}+\frac{d}{L}}\leq 8 \cdot\sqrt{\frac{4L^{2}(d+m_{2}^{2})+18L^{2}\|\mathbf{x}_{0}\|^{2}}{L^{2}}+d}\leq 6 3\cdot\sqrt{(d+m_{2}^{2}+\|\mathbf{x}_{0}\|^{2})},\]

then we choose

\[R=63\cdot\sqrt{(d+m_{2}^{2}+\|\mathbf{x}_{0}\|^{2})\cdot\log\frac{16S}{\epsilon}}.\]

After determining \(R\), the choice of \(\tau\) can be relaxed to

\[\tau\leq C_{\tau}\cdot\left(L^{2}\left(d+m_{2}^{2}+\|\mathbf{x}_{0}\|^{2}\right) \cdot\log\frac{16S}{\epsilon}\right)^{-1},\]where the absolute constant \(C_{\tau}=2^{-4}\cdot 3^{-8}\cdot 7^{-2}\), since we have

\[(3LR+G+\epsilon_{\rm score})^{2}\leq 9L^{2}R^{2}+4G^{2}\] \[\leq 9L^{2}\cdot 63^{2}\cdot\left(d+m_{2}^{2}+\|\mathbf{x}_{0}\|^{2} \right)\cdot\log\frac{16S}{\epsilon}+4\left(4L^{2}\cdot\left(d+m_{2}^{2} \right)+18L^{2}\|\mathbf{x}_{0}\|^{2}\right)\] \[\leq 9\cdot 63^{2}\cdot L^{2}\left(d+m_{2}^{2}+\|\mathbf{x}_{0}\|^{2} \right)\cdot\log\frac{16S}{\epsilon}.\]

Hence, the proof is completed. 

**Theorem C.15**.: _Under Assumption [A1]-[A2], for any \(\epsilon\in(0,1)\), let \(\tilde{\mu}_{*}(\mathbf{z})\propto\exp(-g(\mathbf{z}))\mathbf{1}[\mathbf{z}\in\mathcal{B}( \mathbf{0},R)]\) be the truncated target distribution in \(\tilde{\mathcal{B}}(\mathbf{0},R)\) with_

\[R=63\cdot\sqrt{(d+m_{2}^{2}+\|\mathbf{x}_{0}\|^{2})\cdot\log\frac{16S}{\epsilon}}= \tilde{\mathcal{O}}\left((d+m_{2}^{2}+\|\mathbf{x}_{0}\|^{2})^{1/2}\right),\]

\(r\) _in Alg. 2 satisfies_

\[r=3\cdot\sqrt{\tau d\log\frac{8S}{\epsilon}}=\tilde{\mathcal{O}}(\tau^{1/2}d^{ 1/2})\]

_and \(\rho\) be the Cheeger constant of \(\tilde{\mu}_{*}\). Suppose \(\tilde{\mu}_{0}(\{\|\mathbf{x}\|\geq R/2\})\leq\epsilon/16\), the step size satisfy_

\[\tau\leq C_{\tau}\cdot\left(L^{2}\left(d+m_{2}^{2}+\|\mathbf{x}_{0}\|^{2}\right) \cdot\log\frac{16S}{\epsilon}\right)^{-1}=\tilde{\mathcal{O}}(L^{-2}\cdot(d+m _{2}^{2}+\|\mathbf{x}_{0}\|^{2})^{-1}),\]

_the score and energy estimation errors satisfy_

\[\epsilon_{\rm score}\leq\frac{c_{0}\rho}{32\cdot 36\cdot\sqrt{d\log\frac{8S}{ \epsilon}}}=\mathcal{O}(\rho d^{-1/2})\quad\text{and}\quad\epsilon_{\rm energy }\leq\frac{c_{0}\rho\sqrt{2\tau}}{32\cdot 5}=\mathcal{O}(\rho\tau^{1/2}),\]

_then for any \(\lambda\)-warm start with respect to \(\mu_{*}\) the output of both standard and projected implementation of Alg. 2 satisfies_

\[\operatorname{TV}\left(\mu_{S},\mu_{*}\right)=\frac{\epsilon}{2}+\lambda\left( 1-\frac{c_{0}^{2}\rho^{2}}{4}\cdot\tau\right)^{S}+\tilde{\mathcal{O}}(d^{1/2} \rho^{-1}\epsilon_{\rm score})+\mathcal{O}(\rho^{-1}\tau^{-1/2}\epsilon_{\rm energy })\]

Proof.: We characterize the condition on the step size \(\tau\). Combining Lemma C.2 and Corollary C.11, it requires the range of \(\tau\) to satisfy

\[\tau\leq 16^{-1}\cdot(3LR+\|\nabla g(\mathbf{0})\|+\epsilon_{\rm score})^{-2}.\]

Under this condition, we have

\[\tau\leq\frac{d\log\frac{8S}{\epsilon}}{(3LR+G)^{2}},\quad\tau\leq\frac{d\log \frac{8S}{\epsilon}}{\epsilon_{\rm score}^{2}},\quad\text{and}\quad\tau\leq \left(72^{2}\cdot\epsilon_{\rm score}^{2}\cdot d\log\frac{8S}{\epsilon}\right)^ {-1}\]

which implies

\[(3LR+G)\epsilon_{\rm score}\cdot\tau\leq\epsilon_{\rm score}\sqrt{\tau}\cdot \sqrt{d\log\frac{8S}{\epsilon}}\quad\text{and}\quad\epsilon_{\rm score}^{2} \cdot\tau\leq\epsilon_{\rm score}\sqrt{\tau}\cdot\sqrt{d\log\frac{8S}{ \epsilon}}.\]

Then, we have

\[\delta= 16\cdot\left[\frac{3\epsilon_{\rm score}}{2}\cdot\sqrt{\tau d\log \frac{8S}{\epsilon}}+\frac{(3LR+G)\epsilon_{\rm score}\cdot\tau}{2}+\frac{ \epsilon_{\rm score}^{2}\cdot\tau}{4}\right]\] \[\leq 16\cdot\left[\frac{3\epsilon_{\rm score}}{2}\cdot\sqrt{\tau d\log \frac{8S}{\epsilon}}+\frac{\epsilon_{\rm score}}{2}\cdot\sqrt{\tau d\log\frac{8S }{\epsilon}}+\frac{\epsilon_{\rm score}}{4}\cdot\sqrt{\tau d\log\frac{8S}{ \epsilon}}\right]\] \[= 36\epsilon_{\rm score}\cdot\sqrt{\tau d\log\frac{8S}{\epsilon}} \leq\frac{1}{2}\]which matches the requirement of Lemma C.3. Under this condition, if we require

\[\epsilon_{\mathrm{score}}\leq\frac{c_{0}\rho}{32\cdot 36\cdot\sqrt{d\log\frac{8S}{ \epsilon}}}=\mathcal{O}(\rho d^{-1/2})\quad\mathrm{and}\quad\epsilon_{\mathrm{ energy}}\leq\frac{c_{0}\rho\sqrt{2\tau}}{32\cdot 5}=\mathcal{O}(\rho\tau^{1/2}),\]

it makes

\[\delta+5\epsilon_{\mathrm{energy}}\leq 36\epsilon_{\mathrm{score}}\cdot\sqrt{ \tau d\log\frac{8S}{\epsilon}}+5\epsilon_{\mathrm{energy}}\leq\frac{c_{0}\rho \sqrt{2\tau}}{16}\leq\frac{\phi}{16}\]

and satisfies the requirements shown in Lemma C.12.

Then, we are able to put the results of these lemmas together to establish the convergence of Alg. 2. Note that if \(\mu_{0}\) is a \(\lambda\)-warm start to \(\mu_{*}\), it must be a \(\lambda\)-warm start to \(\tilde{\mu}_{*}\) since \(\tilde{\mu}_{*}(\mathcal{A})\geq\mu_{*}(\mathcal{A})\) for all \(\mathcal{A}\in\Omega\). Combining Lemma C.2, Lemma C.12 and Lemma C.13, we have

\[\mathrm{TV}\left(\mu_{S},\mu_{*}\right)\leq \mathrm{TV}\left(\mu_{S},\tilde{\mu}_{S}\right)+\mathrm{TV}\left( \tilde{\mu}_{S},\tilde{\mu}_{*}\right)+\mathrm{TV}\left(\tilde{\mu}_{*},\mu_{ *}\right)\] \[\leq \frac{\epsilon}{4}+\left(\lambda\cdot\left(1-\frac{\phi^{2}}{8} \right)^{S}+\frac{16(\delta+5\epsilon_{\mathrm{energy}})}{\phi}\right)+\frac{ \epsilon}{4}\] \[\leq \frac{\epsilon}{2}+\lambda\left(1-\frac{c_{0}^{2}\rho^{2}}{4} \cdot\tau\right)^{S}+408\epsilon_{\mathrm{score}}\cdot\frac{\sqrt{d\log\frac{8 S}{\epsilon}}}{c_{0}\rho}+\frac{57\epsilon_{\mathrm{energy}}}{c_{0}\rho\sqrt{ \tau}}\] \[= \frac{\epsilon}{2}+\lambda\left(1-\frac{c_{0}^{2}\rho^{2}}{4} \cdot\tau\right)^{S}+\tilde{\mathcal{O}}(d^{1/2}\rho^{-1}\epsilon_{\mathrm{ score}})+\mathcal{O}(\rho^{-1}\tau^{-1/2}\epsilon_{\mathrm{energy}}).\]

After combining this result with the choice of parameters shown in Lemma C.14, the proof is completed. 

**Lemma C.16**.: _Under the same assumptions and hyperparameter settings made in Theorem C.15, we use Gaussian-type initialization_

\[\frac{\mu_{0}(\mathrm{d}\mathbf{z})}{\mathrm{d}\mathbf{z}}\propto\exp\left(-L\|\mathbf{z} \|^{2}-\frac{\|\mathbf{x}_{0}-e^{-\eta}\mathbf{z}\|^{2}}{2(1-e^{-2\eta})}\right).\]

_If we set the iteration number as_

\[S=\tilde{\mathcal{O}}\left(L\rho^{-2}\cdot\left(d+m_{2}^{2}\right)\tau^{-1} \right),\]

_the standard and projected implementation of Alg. 2 can achieve_

\[\mathrm{TV}\left(\mu_{S},\mu_{*}\right)\leq\frac{3\epsilon}{4}+\tilde{ \mathcal{O}}(d^{1/2}\rho^{-1}\epsilon_{\mathrm{score}})+\mathcal{O}(\rho^{-1} \tau^{-1/2}\epsilon_{\mathrm{energy}}).\]

Proof.: We reformulate the target distribution \(\mu_{*}\) and the initial distribution \(\mu_{0}\) as follows

\[\frac{\mu_{*}(\mathrm{d}\mathbf{z})}{\mathrm{d}\mathbf{z}} \propto\exp\left[-\left(f_{(K-k-1)\eta}(\mathbf{z})+\frac{3L\|\mathbf{z} \|^{2}}{2}\right)-\left(\frac{\|\mathbf{x}_{0}-e^{-\eta}\mathbf{z}\|^{2}}{2(1-e^{-2 \eta})}-\frac{3L\|\mathbf{z}\|^{2}}{2}\right)\right]\coloneqq\exp\left(-\phi(\mathbf{z })-\psi(\mathbf{z})\right),\] \[\frac{\mu_{0}(\mathrm{d}\mathbf{z})}{\mathbf{z}} \propto\exp\left[-L\|\mathbf{z}\|^{2}-\frac{3L\|\mathbf{z}\|^{2}}{2}- \left(\frac{\|\mathbf{x}_{0}-e^{-\eta}\mathbf{z}\|^{2}}{2(1-e^{-2\eta})}-\frac{3L\|\bm {z}\|^{2}}{2}\right)\right]=\exp\left[-\frac{5L\|\mathbf{z}\|^{2}}{2}-\psi(\mathbf{z} )\right].\]

Under this condition, we have

\[\frac{\mu_{0}(\mathrm{d}\mathbf{z})}{\mu_{*}(\mathrm{d}\mathbf{z})}\leq\frac{\int_{ \mathbb{R}^{d}}\exp\left(-\phi(\mathbf{z}^{\prime})-\psi(\mathbf{z}^{\prime})\right) \mathrm{d}\mathbf{z}^{\prime}}{\int_{\mathbb{R}^{d}}\exp\left(-5L/2\cdot\|\mathbf{z}^ {\prime}\|^{2}-\psi(\mathbf{z}^{\prime})\right)\mathrm{d}\mathbf{z}^{\prime}}\cdot\exp \left(\phi(\mathbf{z})-\frac{5L\|\mathbf{z}\|^{2}}{2}\right) \tag{75}\]

Due to Assumption **[A1]**, we have

\[\frac{L\mathbf{I}}{2}\preceq\nabla^{2}f_{(K-k-1)\eta}(\mathbf{z}^{\prime})+\frac{3L}{2 }=\nabla^{2}\phi(\mathbf{z}^{\prime})\preceq\frac{5L\mathbf{I}}{2},\]

which means

\[\phi(\mathbf{z})\leq\phi(\mathbf{z}_{*})+\frac{5L}{4}\cdot\|\mathbf{z}-\mathbf{z}_{*}\|^{2} \leq\phi(\mathbf{z}_{*})+\frac{5L\|\mathbf{z}\|^{2}}{2}+\frac{5L\|\mathbf{z}_{*}\|^{2}}{2}\]\[\exp\left(\phi(\mathbf{z})-\frac{5L\|\mathbf{z}\|^{2}}{2}\right)\leq\exp\left(\phi(\mathbf{z}_{ *})+\frac{5L\|\mathbf{z}_{*}\|^{2}}{2}\right). \tag{76}\]

Since the function \(\phi(\mathbf{z})\) is strongly log-concave, it satisfies

\[\nabla\phi(\mathbf{z})\cdot\mathbf{z}\geq\frac{L\|\mathbf{z}\|^{2}}{4}-\frac{\|\nabla\phi( \mathbf{0})\|}{L}\quad\text{and}\quad\phi(\mathbf{z})\geq\frac{L\|\mathbf{z}\|^{2}}{16}+ \phi(\mathbf{z}_{*})-\frac{\|\nabla\phi(\mathbf{0})\|^{2}}{2L}\]

due to Lemma E.3 and Lemma E.4. Under these conditions, we have

\[\begin{split}&\int\exp\left[-\phi(\mathbf{z}^{\prime})-\psi(\mathbf{z}^{ \prime})\right]\mathrm{d}\mathbf{z}^{\prime}\leq\exp\left(-\phi(\mathbf{z}_{*})+\frac{ \|\nabla\phi(\mathbf{0})\|^{2}}{2L}\right)\cdot\int\exp\left[-\frac{L\|\mathbf{z}^{ \prime}\|^{2}}{16}-\psi(\mathbf{z}^{\prime})\right]\mathrm{d}\mathbf{z}^{\prime}\\ &=\exp\left(-\phi(\mathbf{z}_{*})+\frac{\|\nabla\phi(\mathbf{0})\|^{2}}{2 L}\right)\cdot\int\exp\left[-\frac{23L\|\mathbf{z}^{\prime}\|^{2}}{16}-\frac{\|\mathbf{x}_{ 0}-e^{-\eta}\mathbf{z}^{\prime}\|^{2}}{2(1-e^{-2\eta})}\right]\mathrm{d}\mathbf{z}^{ \prime}\end{split} \tag{77}\]

Besides, we have

\[\int\exp\left[-\frac{5L\|\mathbf{z}^{\prime}\|^{2}}{2}-\psi(\mathbf{z}^{\prime})\right] \mathrm{d}\mathbf{z}^{\prime}=\int\exp\left[-L\|\mathbf{z}^{\prime}\|^{2}-\frac{\|\mathbf{x }_{0}-e^{-\eta}\mathbf{z}^{\prime}\|^{2}}{2(1-e^{-2\eta})}\right]\mathrm{d}\mathbf{z}^ {\prime},\]

which implies

\[\begin{split}&\int\exp\left[-\frac{5L\|\mathbf{z}^{\prime}\|^{2}}{2}- \psi(\mathbf{z}^{\prime})\right]\mathrm{d}\mathbf{z}^{\prime}\cdot\int\exp\left[-\frac {7L\|\mathbf{z}^{\prime}\|^{2}}{16}\right]\mathrm{d}\mathbf{z}^{\prime}\\ &\geq\int\exp\left[-\frac{23L\|\mathbf{z}^{\prime}\|^{2}}{16}-\frac{ \|\mathbf{x}_{0}-e^{-\eta}\mathbf{z}^{\prime}\|^{2}}{2(1-e^{-2\eta})}\right]\mathrm{d} \mathbf{z}^{\prime}\end{split} \tag{78}\]

Plugging Eq. 76, Eq. 77 and Eq. 78 into Eq. 75, we have

\[\frac{\mu_{0}(\mathrm{d}\mathbf{z})}{\tilde{\mu}_{*}(\mathrm{d}\mathbf{z})}\leq\exp \left(\frac{5L\|\mathbf{z}_{*}\|^{2}}{2}+\frac{\|\nabla\phi(\mathbf{0})\|^{2}}{2L} \right)\cdot\int\exp\left[-\frac{7L\|\mathbf{z}^{\prime}\|^{2}}{16}\right]\mathrm{ d}\mathbf{z}^{\prime}. \tag{79}\]

Due to the strong convexity of \(\phi\), it has

\[\|\mathbf{z}_{*}\|^{2}\leq\frac{4\|\nabla\phi(\mathbf{0})-\nabla\phi(\mathbf{z}_{*})\|^{2} }{L^{2}}=\frac{4\|\nabla\phi(\mathbf{0})\|^{2}}{L^{2}}\]

and

\[\|\nabla\phi(\mathbf{0})\|^{2}=\left\|\nabla f_{(K-k-1)\eta}(\mathbf{0})\right\|^{2} \leq 2L^{2}(d+m_{2}^{2})\]

where the inequality follows from Eq. 73. Combining with the fact

\[\int\exp\left[-\frac{7L\|\mathbf{z}^{\prime}\|^{2}}{16}\right]\mathrm{d}\mathbf{z}^{ \prime}=\left(\frac{16\pi}{7L}\right)^{d/2},\]

Eq. 79 can be relaxed to

\[\lambda\leq\max_{\mathbf{z}}\frac{\mu_{0}(\mathrm{d}\mathbf{z})}{\tilde{\mu}_{*}( \mathrm{d}\mathbf{z})}\leq\exp\left(22L\cdot(d+m_{2}^{2})\right)\cdot\left(\frac{1 6\pi}{L}\right)^{d/2}=\exp\left(\mathcal{O}(L(d+m_{2}^{2}))\right)\]

which is independent on \(\|\mathbf{x}_{0}\|\). Then, In order to ensure the convergence of the total variation distance is smaller than \(\epsilon\), it suffices to choose \(\tau\) and \(S\) such that

\[\lambda\left(1-\frac{c_{0}^{2}\rho^{2}}{4}\cdot\tau\right)^{S}\leq\frac{ \epsilon}{4}\quad\Leftrightarrow\quad S=\mathcal{O}\left(\frac{\log(\lambda/ \epsilon)}{\rho^{2}\tau}\right)=\tilde{\mathcal{O}}\left(L\rho^{-2}\cdot \left(d+m_{2}^{2}\right)\tau^{-1}\right),\]

where the last two inequalities follow from Theorem C.15. Hence, the proof is completed. 

**Theorem C.17**.: _Under Assumption [1]-[1], for Alg. 1, we choose_

\[\eta=\frac{1}{2}\log\frac{2L+1}{2L}\quad\text{and}\quad K=4L\cdot\log\frac{(1+ L^{2})d+\|\nabla f_{*}(\mathbf{0})\|^{2}}{\epsilon^{2}}\]_and implement Step 3 of Alg. 1 with projected Alg. 2. For the \(k\)-th run of Alg. 2, we use Gaussian-type initialization_

\[\frac{\mu_{0}(\mathrm{d}\boldsymbol{z})}{\mathrm{d}\boldsymbol{z}}\propto\exp \left(-L\|\boldsymbol{z}\|^{2}-\frac{\|\hat{\boldsymbol{x}}_{k}-e^{-\eta} \boldsymbol{z}\|^{2}}{2(1-e^{-2\eta})}\right).\]

_If we set the hyperparameters as shown in Lemma C.16, it can achieve_

\[\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq\epsilon+\tilde{\mathcal{O}}( Ld^{1/2}\rho^{-1}\epsilon_{\mathrm{score}})+\mathcal{O}(\hat{\tau}^{-1/2}\cdot L ^{2}(d^{1/2}+m_{2}+Z)\rho^{-1}\epsilon_{\mathrm{energy}})\]

_with a gradient complexity as follows_

\[\tilde{\mathcal{O}}\left(L^{4}\rho^{-2}\hat{\tau}^{-1}\cdot\left(d+m_{2}^{2} \right)^{2}Z^{2}\right)\]

_for any \(\hat{\tau}\in(0,1)\) where \(Z\) denotes the maximal \(l_{2}\) norm of particles appearing in outer loops (Alg. 1)._

Proof.: According to Lemma B.3, we know that under the choice

\[\eta=\frac{1}{2}\ln\frac{2L+1}{2L},\]

it requires to run Alg. 2 for \(K\) times where

\[K=4L\cdot\log\frac{(1+L^{2})d+\|\nabla f_{*}(\boldsymbol{0})\|^{2}}{\epsilon^ {2}}.\]

For each run of Alg. 2, we require the total variation error to achieve

\[\mathrm{TV}\left(\hat{p}_{(k+1)\eta|k\eta}(\cdot|\hat{\boldsymbol {x}}),p_{(k+1)\eta|k\eta}^{\leftarrow}(\cdot|\hat{\boldsymbol{x}})\right)\leq \frac{\epsilon}{4L}\cdot\left[\log\frac{(1+L^{2})d+\|\nabla f_{* }(\boldsymbol{0})\|^{2}}{\epsilon^{2}}\right]^{-1}\] \[+\tilde{\mathcal{O}}(d^{1/2}\rho^{-1}\epsilon_{\mathrm{score}})+ \mathcal{O}(\rho^{-1}\tau_{k}^{-1/2}\epsilon_{\mathrm{energy}}).\]

Combining with Lemma C.16, we consider a step size

\[\tau_{k}= C_{\tau}\cdot\left(L^{2}\left(d+m_{2}^{2}+\|\hat{\boldsymbol{x}}_{k }\|^{2}\right)\cdot\log\frac{48LS\log\frac{(1+L^{2})d+\|\nabla f_{*}( \boldsymbol{0})\|^{2}}{\epsilon^{2}}}{\epsilon^{2}}\right)^{-1}\cdot\hat{\tau}\] \[= \tilde{\mathcal{O}}(L^{-2}\cdot(d+m_{2}^{2}+\|\hat{\boldsymbol{x} }_{k}\|^{2})^{-1}\cdot\hat{\tau})\]

where \(\tau^{\prime}\in(0,1)\), to solve the \(k\)-th inner sampling subproblem. Then, the maximum iteration number will be

\[S=\tilde{\mathcal{O}}\left(L^{3}\rho^{-2}\hat{\tau}^{-1}\cdot\left(d+m_{2}^{2} \right)^{2}\cdot\|\hat{\boldsymbol{x}}_{k}\|^{2}\right).\]

This means that with the total gradient complexity

\[K\cdot S=\tilde{\mathcal{O}}\left(L^{4}\rho^{-2}\hat{\tau}^{-1}\cdot\left(d+m_ {2}^{2}\right)^{2}Z^{2}\right)\]

where \(Z\) denotes the maximal \(l_{2}\) norm of particles appearing in outer loops (Alg. 1), we can obtain

\[\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq \epsilon+\tilde{\mathcal{O}}(Kd^{1/2}\rho^{-1}\epsilon_{\mathrm{ score}})+\mathcal{O}(KL(d+m_{2}^{2}+Z^{2})^{1/2}\hat{\tau}^{-1/2}\rho^{-1} \epsilon_{\mathrm{energy}})\] \[= \epsilon+\tilde{\mathcal{O}}(Ld^{1/2}\rho^{-1}\epsilon_{\mathrm{ score}})+\mathcal{O}(\hat{\tau}^{-1/2}\cdot L^{2}(d^{1/2}+m_{2}+Z)\rho^{-1}\epsilon_{ \mathrm{energy}}).\]

Hence, the proof is completed. 

**Lemma C.18**.: _Suppose we implement Alg. 2 with its projected version, we have_

\[Z^{2}\leq\tilde{\mathcal{O}}\left(L^{3}(d+m_{2}^{2})^{2}\rho^{-2}\right).\]

_where \(Z\) denotes the maximal \(l_{2}\) norm of particles appearing in outer loops (Alg. 1)_Proof.: Suppose we implement Alg. 2 with its projected version, where each update will be projected to a ball with a ratio \(r\) shown in Lemma C.2. Under these conditions, we have

\[\left\|\hat{\mathbf{x}}_{K}\right\|^{2}=\left\|\hat{\mathbf{x}}_{0}+\sum_{i=1}^{K}(\hat{ \mathbf{x}}_{i}-\hat{\mathbf{x}}_{i-1})\right\|^{2}\leq(K+1)\left\|\hat{\mathbf{x}}_{0} \right\|^{2}+(K+1)\cdot\sum_{i=1}^{K}\left\|\hat{\mathbf{x}}_{i}-\hat{\mathbf{x}}_{i-1 }\right\|^{2}\]

For each \(i\in\{1,2,\ldots K\}\), we have

\[\left\|\hat{\mathbf{x}}_{i}-\hat{\mathbf{x}}_{i-1}\right\|^{2}=\left\|\mathbf{z}_{S}-\mathbf{z} _{0}\right\|^{2}\leq(S+1)\cdot\sum_{j=1}^{S}\left\|\mathbf{z}_{j}-\mathbf{z}_{j-1} \right\|^{2}\leq 2S\cdot r^{2}.\]

Follows from Lemma C.16, it has

\[S\cdot r^{2}=\mathcal{O}\left(\frac{\log(\lambda/\epsilon)}{\rho^{2}\tau} \right)\cdot\tilde{\mathcal{O}}\left(\tau d\right)=\tilde{\mathcal{O}}\left( \frac{d\log(\lambda/\epsilon)}{\rho^{2}}\right)=\tilde{\mathcal{O}}\left(L(d+ m_{2}^{2})^{2}\rho^{-2}\right).\]

Then, we have

\[Z^{2}\leq\mathcal{O}(K^{2})\cdot\tilde{\mathcal{O}}\left(L(d+m_{2}^{2})^{2} \rho^{-2}\right)=\tilde{\mathcal{O}}\left(L^{3}(d+m_{2}^{2})^{2}\rho^{-2}\right),\]

Hence, the proof is completed. 

### Control the error from Energy Estimation

**Corollary C.19**.: _Suppose the diffusion model \(\mathbf{s_{\theta}}\) satisfies_

\[\left\|\hat{\mathbf{s}}_{\mathbf{\theta}}(\mathbf{x},t)+\nabla\log p_{t}(\mathbf{x})\right\|_{ \infty}\leq\frac{\rho\epsilon}{Ld^{1/2}},\]

_and another parameterized model \(\hat{l}_{\mathbf{\theta}}(\mathbf{x},t)\) is used to estimate the log-likelihood of \(p_{t}(\mathbf{x})\) satisfying_

\[\left\|\hat{l}_{\mathbf{\theta}^{\prime}}(\mathbf{x},t)+\log p_{t}(\mathbf{x})\right\|_{ \infty}\leq\frac{\rho\epsilon}{L^{2}\cdot(d^{1/2}+m_{2}+Z)}.\]

_If we implement Alg. 1 with the projected version of Alg. 2, it has_

\[\operatorname{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq\tilde{\mathcal{O}}(\epsilon)\]

_with the following gradient complexity_

\[\tilde{\mathcal{O}}\left(L^{4}\rho^{-2}\cdot\left(d+m_{2}^{2}\right)^{2}Z^{2} \right).\]

Proof.: Since we have highly accurate scores and energy estimation, we can construct \(\mathbf{s_{\theta}}\) and \(r_{\theta^{\prime}}\) (shown in Eq. 23) for the \(k\)-th inner loop as follows

\[\mathbf{s_{\theta}}(\mathbf{z})=\hat{\mathbf{s}}_{\mathbf{\theta}}(\mathbf{z},(K-k-1) \eta)+\frac{e^{-2\eta}\mathbf{z}-e^{-\eta}\hat{\mathbf{x}}_{k}}{1-e^{-2\eta}}\] \[r_{\mathbf{\theta}^{\prime}}(\mathbf{z},\mathbf{z}^{\prime})=\hat{l}_{\mathbf{ \theta}^{\prime}}(\mathbf{z},(K-k-1)\eta)+\frac{\left\|\hat{\mathbf{x}}_{k}-e^{-\eta} \cdot\mathbf{z}\right\|^{2}}{2(1-e^{-2\eta})}\] \[\qquad\qquad\qquad\qquad-\left(\hat{l}_{\mathbf{\theta}^{\prime}}( \mathbf{z}^{\prime},(K-k-1)\eta)+\frac{\left\|\hat{\mathbf{x}}_{k}-e^{-\eta}\cdot\mathbf{z }^{\prime}\right\|^{2}}{2(1-e^{-2\eta})}\right).\]

Under these conditions, we have

\[\epsilon_{\mathrm{energy}}\leq\frac{\rho\epsilon}{L^{2}\cdot(d^{1/2}+m_{2}+Z) }\quad\text{and}\quad\epsilon_{\mathrm{score}}\leq\frac{\rho\epsilon}{Ld^{1/2 }}.\]

Plugging these results into Theorem C.17 and setting \(\hat{\tau}=1/2\), we have

\[\operatorname{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq\tilde{\mathcal{O}}(\epsilon)\]

with the following gradient complexity

\[\tilde{\mathcal{O}}\left(L^{4}\rho^{-2}\cdot\left(d+m_{2}^{2}\right)^{2}Z^{2} \right).\]

**Corollary C.20**.: _Suppose the score estimation is extremely small, i.e.,_

\[\left\|\hat{\mathbf{s}}_{\mathbf{\theta}}(\mathbf{x},t)+\nabla\log p_{t}(\mathbf{x})\right\|_{ \infty}\ll\frac{\rho\epsilon}{Ld^{1/2}},\]

_and the log-likelihood function of \(p_{t}\) has a bounded \(3\)-order derivative, e.g.,_

\[\left\|\nabla^{(3)}f(\mathbf{z})\right\|\leq L,\]

_we have a non-parametric estimation for log-likelihood to make we have \(\operatorname{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq\tilde{\mathcal{O}}(\epsilon)\) with_

\[\tilde{\mathcal{O}}\left(L^{4}\rho^{-3}\cdot\left(d+m_{2}^{2}\right)^{2}Z^{3} \cdot\epsilon\right).\]

_gradient calls._

Proof.: Combining the Alg. 2 and the definition of \(\epsilon_{\mathrm{energy}}\) shown in Lemma C.4, we actually require to control

\[\epsilon_{\mathrm{energy}}\coloneqq\left(g(\tilde{\mathbf{z}}_{s})-g(\mathbf{z}_{s}) \right)-r_{\theta}(\tilde{\mathbf{z}}_{s},\mathbf{z}_{s})\]

for any \(s\in[0,S-1]\). Then, we start to construct \(r_{\theta}(\tilde{\mathbf{z}}_{s},\mathbf{z}_{s})\). Since we have

\[g(\tilde{\mathbf{z}}_{s})-g(\mathbf{z}_{s})=f_{(K-k-1)\eta}(\tilde{\mathbf{z}}_{s})+\frac{ \left\|\mathbf{x}_{0}-\tilde{\mathbf{z}}_{s}\cdot e^{-\eta}\right\|^{2}}{2(1-e^{-2\eta })}-f_{(K-k-1)\eta}(\mathbf{z}_{s})-\frac{\left\|\mathbf{x}_{0}-\mathbf{z}_{s}\cdot e^{- \eta}\right\|^{2}}{2(1-e^{-2\eta})},\]

we should only estimate the difference of the energy function \(f_{(K-k-1)\eta}\) which will be presented as \(f\) for abbreviation. Besides, we define the following function

\[h(t)=f\left((\tilde{\mathbf{z}}_{s}-\mathbf{z}_{s})\cdot t+\mathbf{z}_{s}\right),\]

which means

\[h^{(1)}(t) \coloneqq\frac{\mathrm{d}h(t)}{\mathrm{d}t}=\nabla f\left((\tilde {\mathbf{z}}_{s}-\mathbf{z}_{s})\cdot t+\mathbf{z}_{s}\right)\cdot(\tilde{\mathbf{z}}_{s}-\bm {z}_{s})\] \[h^{(2)}(t) \coloneqq\frac{\mathrm{d}^{2}h(t)}{(\mathrm{d}t)^{2}}=(\tilde{ \mathbf{z}}_{s}-\mathbf{z}_{s})^{\top}\nabla^{2}f\left((\tilde{\mathbf{z}}_{s}-\mathbf{z}_{s}) \cdot t+\mathbf{z}_{s}\right)(\tilde{\mathbf{z}}_{s}-\mathbf{z}_{s})\]

Under the high-order smoothness condition, i.e.,

\[\left\|\nabla^{3}f(\mathbf{z})\right\|\leq L\]

where \(\|\cdot\|\) denotes the nuclear norm, then we have

\[\left|h(1)-h(0)\right|\leq\sum_{i=1}^{2}\frac{h^{(i)}(0)}{i!}+\frac{L\cdot \left\|\tilde{\mathbf{z}}_{s}-\mathbf{z}_{s}\right\|^{3}}{3!}\leq\sum_{i=1}^{2}\frac{h ^{(i)}(0)}{i!}+\frac{Lr^{3}}{3!}.\]

It means we need to approximate \(h^{(i)}\) with high accuracy.

For \(i=1\), the ground truth \(h^{(1)}(0)\) is

\[h^{(1)}(0)=\frac{\mathrm{d}h(t)}{\mathrm{d}t}=\nabla f\left(\mathbf{z}_{s}\right) \cdot(\tilde{\mathbf{z}}_{s}-\mathbf{z}_{s})\]

we can approximate it numerically as

\[\tilde{h}^{(1)}(0)\coloneqq s_{\theta}(\mathbf{z}_{s})\cdot(\tilde{\mathbf{z}}_{s}-\bm {z}_{s})\]

since we have score approximation. Then it has

\[\delta^{(1)}(0)=h^{(1)}(0)-\tilde{h}^{(1)}(0)\leq\left\|\nabla f(\mathbf{z}_{s})-s _{\theta}(\mathbf{z}_{s})\right\|\cdot\left\|\tilde{\mathbf{z}}_{s}-\mathbf{z}_{s}\right\| \leq\epsilon_{\mathrm{score}}\cdot r. \tag{80}\]

Then, for \(i=2\), we obtain the ground truth \(h^{(2)}(0)\) by

\[h^{(1)}(t)-h^{(1)}(0)=\int_{0}^{t}h^{(2)}(\tau)\mathrm{d}\tau=th^{(2)}(0)+\int _{0}^{t}h^{(2)}(\tau)-h^{(2)}(0)\mathrm{d}\tau,\]

which means

\[h^{(2)}(0)=\frac{h^{(1)}(t)-h^{(1)}(0)}{t}+\frac{1}{t}\cdot\int_{0}^{t}h^{(2)} (\tau)-h^{(2)}(0)\mathrm{d}\tau.\]

[MISSING_PAGE_FAIL:49]

**Remark 3**.: _If we consider more high-order smooth, i.e.,_

\[\left\|\nabla^{(u)}f(\mathbf{z})\right\|\leq L,\]

_with similar techniques shown in Corollary C.20, we can have the following bound, i.e.,_

\[\epsilon_{\mathrm{energy}}=\mathcal{O}(Lr^{u})\]

_when \(\epsilon_{\mathrm{score}}\) is extremely small. Under this condition, since it has_

\[r=3\cdot\sqrt{\tau d\log\frac{8S}{\epsilon}}=\tilde{O}(\tau^{1/2}d^{1/2}),\]

_we have_

\[\epsilon_{\mathrm{energy}}=\mathcal{O}(Lr^{u})=\mathcal{O}(Ld^{u/2}\tau^{u/2} )=\tilde{\mathcal{O}}\left(L^{-u+1}d^{u/2}\left(d+m_{2}^{2}+\|\hat{\mathbf{x}}_{k} \|^{2}\right)^{-u/2}\hat{\tau}^{u/2}\right)=\tilde{\mathcal{O}}(L^{-u+1}\hat{ \tau}^{u/2}).\]

_Then, plugging this result into Theorem C.17 and considering \(\epsilon_{\mathrm{score}}\ll\epsilon\), we have_

\[\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq \epsilon+\tilde{\mathcal{O}}(Ld^{1/2}\rho^{-1}\epsilon_{\mathrm{ score}})+\mathcal{O}(\hat{\tau}^{-1/2}\cdot L^{2}(d^{1/2}+m_{2}+Z)\rho^{-1} \epsilon_{\mathrm{energy}})\] \[= \tilde{\mathcal{O}}(\epsilon)+\tilde{\mathcal{O}}\left(\hat{\tau }^{(u-1)/2}L^{-u+3}(d^{1/2}+m_{2}+Z)\rho^{-1}\right)\] \[= \tilde{\mathcal{O}}(\epsilon)+\tilde{\mathcal{O}}\left(\hat{\tau }^{(u-1)/2}(d^{1/2}+m_{2}+Z)\rho^{-1}\right)\]

_where we suppose \(L\geq 1\) in the last equation without loss of generality. Then, by supposing_

\[\hat{\tau}=\frac{\epsilon^{2/(u-1)}\rho}{d^{1/2}+m_{2}+Z}\]

_we have \(\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\leq\tilde{\mathcal{O}}(\epsilon)\) with_

\[\tilde{\mathcal{O}}\left(L^{4}\rho^{-3}\cdot\left(d+m_{2}^{2}\right)^{2}Z^{3} \cdot\epsilon^{-2/(u-1)}\cdot 2^{u}\right)\]

_where the last \(2^{u}\) appears since the estimation of high-order derivatives requires an exponentially increasing call of score estimations._

## Appendix D Implement RTK inference with ULD

In this section, we consider introducing a ULD to sample from \(p_{k+1|k}^{\leftarrow}(\mathbf{z}|\mathbf{x}_{0})\). To simplify the notation, we set

\[g(\mathbf{z})\coloneqq f_{(K-k-1)\eta}(\mathbf{z})+\frac{\left\|\mathbf{x}_{0}-\mathbf{z} \cdot e^{-\eta}\right\|^{2}}{2(1-e^{-2\eta})} \tag{84}\]

and consider \(k\) and \(\mathbf{x}_{0}\) to be fixed. Besides, we set

\[p^{\leftarrow}(\mathbf{z}|\mathbf{x}_{0})\coloneqq p_{k+1|k}^{\leftarrow}(\mathbf{z}|\mathbf{ x}_{0})\propto\exp(-g(\mathbf{z}))\]

According to Corollary B.5 and Corollary B.3, when we choose

\[\eta=\frac{1}{2}\log\frac{2L+1}{2L},\]

the log density \(g\) will be \(L\)-strongly log-concave and \(3L\)-smooth.

For the underdamped Langevin dynamics, we utilize a form similar to that shown in [40], i.e.,

\[\mathrm{d}\hat{\mathbf{z}}_{t} =\hat{\mathbf{v}}_{t}\mathrm{d}t \tag{85}\] \[\mathrm{d}\hat{\mathbf{v}}_{t} =-\gamma\hat{\mathbf{v}}_{t}\mathrm{d}t-\mathbf{s}_{\mathbf{\theta}}( \hat{\mathbf{z}}_{sr})\mathrm{d}t+\sqrt{2\gamma}\mathrm{d}\mathbf{B}_{t}\]

with a little abuse of notation for \(t\in[s\tau,(s+1)\tau)\). We denote the underlying distribution of \((\hat{\mathbf{z}}_{t},\hat{\mathbf{v}}_{t})\) as \(\hat{\pi}_{t}\), and the exact continuous SDE

\[\mathrm{d}\mathbf{z}_{t} =\mathbf{v}_{t}\mathrm{d}t\] \[\mathrm{d}\mathbf{v}_{t} =-\gamma\mathbf{v}_{t}\mathrm{d}t-\nabla g(\mathbf{z}_{t})\mathrm{ d}t+\sqrt{2\gamma}\mathrm{d}\mathbf{B}_{t}\]has the underlying distribution \((\mathbf{z}_{t},\mathbf{v}_{t})\sim\pi_{t}\). The stationary distribution of the continuous version is defined as

\[\pi^{\leftarrow}(\mathbf{z},\mathbf{v}|\mathbf{x}_{0})\propto\exp\left(-g(\mathbf{z})-\frac{\left\| \mathbf{v}\right\|^{2}}{2}\right)\]

where the \(\mathbf{z}\)-marginal of \(\pi^{\leftarrow}(\cdot|\mathbf{x}_{0})\) is \(p^{\leftarrow}(\cdot|\mathbf{x}_{0})\) which is the desired target distribution of inner loops. Therefore, by taking a small step size for the discretization and a large number of iterations, ULD will yield an approximate sample from \(p^{\leftarrow}(\cdot|\mathbf{x}_{0})\). Besides, in the analysis of ULD, we usually consider an alternate system of coordinates

\[(\phi,\psi)\coloneqq\mathcal{M}(\mathbf{z},\mathbf{v})\coloneqq(\mathbf{z},\mathbf{z}+\frac{2 }{\gamma}\mathbf{v}),\]

their distributions of the continuous time iterates \(\pi_{t}^{\mathcal{M}}\) and the target in these alternate coordinates \(\pi^{\mathcal{M}}\), respectively. Besides, we need to define log-Sobolev inequality as follows

**Definition 2** (Log-Sobolev Inequality).: _The target distribution \(p_{*}\) satisfies the following inequality_

\[\mathbb{E}_{p_{*}}\left[g^{2}\log g^{2}\right]-\mathbb{E}_{p_{*}}[g^{2}]\log \mathbb{E}_{p_{*}}[g^{2}]\leq 2C_{\mathrm{LSI}}\mathbb{E}_{p_{*}}\left\|\nabla g \right\|^{2}\]

_with a constant \(C_{\mathrm{LSI}}\) for all smooth function \(g\colon\mathbb{R}^{d}\to\mathbb{R}\) satisfying \(\mathbb{E}_{p_{*}}[g^{2}]<\infty\)._

**Remark 4**.: _Log-Sobolev inequality is a milder condition than strong log-concavity. Suppose \(p\) satisfies \(m\)-strongly log-concavity, it satisfies \(1/m\) LSI, which is proved in Lemma E.9._

**Definition 3** (Poincare Inequality).: _The target distribution \(p\) satisfies the following inequality_

\[\mathbb{E}_{\mathbf{x}\sim p}\left[\left\|g(\mathbf{x})-\mathbb{E}_{\mathbf{x}\sim p}[g(\bm {x})]\right\|^{2}\right]\leq C_{\mathrm{PI}}\mathbb{E}_{p}\left\|\nabla g \right\|^{2}\]

_with a constant \(C_{\mathrm{PI}}\) for all smooth function \(g\colon\mathbb{R}^{d}\to\mathbb{R}\) satisfying \(\mathbb{E}_{p_{*}}[g^{2}]<\infty\)._

In the following, we mainly follow the idea of proof shown in [40], which provides the convergence of KL divergence for ULD, to control the error from the sampling subproblems.

**Lemma D.1** (Proposition 14 in [40]).: _Let \(\pi_{t}^{\mathcal{M}}\) denote the law of the continuous-time underdamped Langevin diffusion with \(\gamma=c\sqrt{3L}\) for \(c\geq\sqrt{2}\) in the \((\phi,\psi)\) coordinates. Suppose the initial distribution \(\pi_{0}\) has a log-Sobolev (LSI) constant (in the altered coordinates) \(C_{\mathrm{LSI}}(\pi_{0}^{\mathcal{M}})\), then \(\{\pi_{t}^{\mathcal{M}}\}\) satisfies LSI with a constant that can be uniformly upper bounded by_

\[C_{\mathrm{LSI}}(\pi_{t}^{\mathcal{M}})\leq\exp\left(-\sqrt{\frac{2L}{3}}\cdot t \right)\cdot C_{\mathrm{LSI}}(\pi_{0}^{\mathcal{M}})+\frac{2}{L}.\]

**Lemma D.2** (Adapted from Proposition 1 of [26]).: _Consider the following Lyapunov functional_

\[\mathcal{F}(\pi^{\prime},\pi^{\leftarrow})\coloneqq\mathrm{KL}\left(\pi^{ \prime}\middle\|\pi^{\leftarrow}\right)+\mathbb{E}_{\pi^{\prime}}\left[\left\| \mathfrak{M}^{1/2}\nabla\log\frac{\pi^{\prime}}{\pi^{\leftarrow}}\right\|^{2} \right],\quad\mathrm{where}\quad\mathfrak{M}=\begin{bmatrix}\frac{1}{12L}& \frac{1}{\sqrt{6L}}\\ \frac{1}{\sqrt{6L}}&4\end{bmatrix}\otimes\mathbf{I}_{d}.\]

_For targets \(\pi^{\leftarrow}\propto\exp(-g)\) which are \(3L\)-smooth and satisfy LSI with constant \(1/L\), let \(\gamma=2\sqrt{6L}\). Then the law \(\pi_{t}\) of ULD satisfies_

\[\partial_{t}\mathcal{F}(\pi_{t},\pi^{\leftarrow})\leq-\frac{\sqrt{L}}{10\sqrt{ 6}}\cdot\mathcal{F}(\pi_{t},\pi^{\leftarrow}).\]

**Lemma D.3** (Variant of Lemma 4.8 in [1]).: _Let \(\hat{\pi}_{t}\) denote the law of SDE. 85 and \(\pi_{t}\) denote the law of the continuous time underdamped Langevin diffusion with the same initialization, i.e., \(\hat{\pi}_{0}=\pi_{0}\). If \(\gamma\asymp\sqrt{L}\) and the step size \(\tau\) satisfies_

\[\tau=\tilde{\mathcal{O}}\left(L^{-3/2}d^{-1/2}T^{-1/2}\right)\]

_then we have_

\[\chi^{2}(\hat{\pi}_{T}\|\pi_{T})\lesssim L^{3/2}d\tau^{2}T+\epsilon_{\mathrm{ score}}^{2}L^{-1/2}T\]Proof.: The main difference of this discretization analysis is whether the score \(\nabla\log p_{t}\) can be exactly obtained or only be approximated by \(\mathbf{s_{\theta}}\). Therefore, in this proof, we will omit various steps the same as those shown in [1].

We consider the following difference

\[G_{T}\coloneqq \frac{1}{\sqrt{2\gamma}}\sum_{s=0}^{S-1}\int_{s\tau}^{(s+1)\tau} \left\langle\nabla g(\mathbf{z}_{t})-\mathbf{s_{\theta}}(\mathbf{z}_{s\tau}),\mathrm{d}\bm {B}_{t}\right\rangle\] \[-\frac{1}{4\gamma}\sum_{s=0}^{S-1}\int_{s\tau}^{(s+1)\tau}\left\| \nabla g(\mathbf{z}_{t})-\mathbf{s_{\theta}}(\mathbf{z}_{s\tau})\right\|^{2}\mathrm{d}t.\]

From Girsanov's theorem, we obtain immediately using Ito's formula

\[\mathbb{E}_{\pi_{T}}\left[\left(\frac{\mathrm{d}\hat{\pi}_{T}}{ \mathrm{d}\pi_{T}}\right)^{2}\right]-1= \mathbb{E}\left[\exp\left(2G_{T}\right)\right]-1=\] \[\leq \frac{1}{\gamma}\cdot\sum_{s=0}^{S-1}\int_{s\tau}^{(s+1)\tau} \sqrt{\mathbb{E}\left[\exp(4G_{t})\right]\cdot\mathbb{E}\left[\left\|\nabla g (\mathbf{z}_{t})-\mathbf{s_{\theta}}(\mathbf{z}_{s\tau})\right\|^{4}\right]}\mathrm{d}t\] \[\leq \frac{4}{\gamma}\sum_{s=0}^{S-1}\cdot\int_{s\tau}^{(s+1)\tau} \sqrt{\mathbb{E}\left[\exp(4G_{t})\right]\cdot\mathbb{E}\left[\left\|\nabla g( \mathbf{z}_{t})-\nabla g(\mathbf{z}_{s\tau})\right\|^{4}\right]}\mathrm{d}t\] \[+\frac{4\epsilon_{\mathrm{score}}^{2}}{\gamma}\sum_{s=0}^{S-1} \int_{s\tau}^{(s+1)\tau}\sqrt{\mathbb{E}\left[\exp(4G_{t})\right]}\mathrm{d}t\]

According to Corollary 20 of [40], we have

\[\mathbb{E}\left[\exp(4G_{t})\right]\leq \sqrt{\mathbb{E}\left[\exp\left(\frac{16}{\gamma}\sum_{s=0}^{S-1} \int_{s\tau}^{(s+1)\tau\wedge t}\left\|\nabla g(\mathbf{z}_{r})-\mathbf{s_{\theta}}( \mathbf{z}_{s\tau})\right\|^{2}\mathrm{d}r\right)\right]}\] \[\leq \sqrt{\mathbb{E}\exp\left[\frac{32}{\gamma}\cdot\sum_{s=0}^{S-1} \left(\int_{s\tau}^{(s+1)\tau\wedge t}\left\|\nabla g(\mathbf{z}_{r})-\nabla g( \mathbf{z}_{s\tau})\right\|^{2}\mathrm{d}r+\int_{s\tau}^{(s+1)\tau\wedge t} \epsilon_{\mathrm{score}}^{2}\mathrm{d}r\right)\right]}\] \[= \exp\left(\frac{16t\epsilon_{\mathrm{score}}^{2}}{\gamma}\right) \cdot\sqrt{\mathbb{E}\exp\left[\frac{32}{\gamma}\cdot\sum_{s=0}^{S-1}\int_{s \tau}^{(s+1)\tau\wedge t}\left\|\nabla g(\mathbf{z}_{r})-\nabla g(\mathbf{z}_{s\tau}) \right\|^{2}\mathrm{d}r\right]}\] \[\leq 3\cdot\sqrt{\mathbb{E}\exp\left[\frac{32}{\gamma}\cdot\sum_{s=0} ^{S-1}\int_{s\tau}^{(s+1)\tau\wedge t}\left\|\nabla g(\mathbf{z}_{r})-\nabla g( \mathbf{z}_{s\tau})\right\|^{2}\mathrm{d}r\right]},\]

where the last inequality can be established by requiring

\[\epsilon_{\mathrm{score}}=\mathcal{O}\left(\gamma^{1/2}T^{-1/2}\right)\quad \Rightarrow\quad\frac{16t\epsilon_{\mathrm{score}}^{2}}{\gamma}\leq 1\]

since \(\exp(u)\leq 1+2u\) for any \(u\in[0,1]\).

With similar techniques utilized in Lemma 4.8 of [1], we know that if

\[\gamma\asymp\sqrt{3L},\quad\tau\lesssim\frac{\gamma^{1/2}}{6L\cdot d^{1/3}T^{1 /2}(\log S)^{1/2}},\quad\text{and}\quad T\gtrsim\frac{\sqrt{3L}}{L}=\sqrt{ \frac{3}{L}},\]

it holds that

\[\mathbb{E}\exp\left[\frac{32}{\gamma}\cdot\sum_{s=0}^{S-1}\int_{s\tau}^{(s+1) \tau\wedge t}\left\|\nabla g(\mathbf{z}_{r})-\nabla g(\mathbf{z}_{s\tau})\right\|^{2 }\mathrm{d}r\right]\leq\exp\left(\mathcal{O}\left(L^{3/2}d\tau^{2}T\log S \right)\right).\]

Furthermore, for

\[\tau\lesssim L^{-3/2}d^{-1/2}T^{-1/2}(\log S)^{-1/2},\]

[MISSING_PAGE_FAIL:53]

and similarly for \(\nabla\log\pi_{T}^{\mathcal{M}}\). This yields the expression

\[\mathbb{E}_{\pi_{T}^{\mathcal{M}}}\left[\left\|\nabla\log\frac{\pi_{T}^{\mathcal{ M}}}{\pi^{\mathcal{M}}}\right\|^{2}\right]=\mathbb{E}_{\pi_{T}}\left[\left\|( \mathcal{M}^{-1})^{\top}\nabla\log\frac{\pi_{T}}{\pi^{\leftarrow}}\right\|^{2} \right]. \tag{88}\]

According to the definition of \(\mathcal{M}\), we have

\[\mathcal{M}^{-1}(\mathcal{M}^{-1})^{\top}=\begin{bmatrix}1&-\gamma/2\\ -\gamma/2&\gamma^{2}/2\end{bmatrix}.\]

For any \(c_{0}>0\) and

\[\mathfrak{M}\coloneqq\begin{bmatrix}\frac{1}{12L}&\frac{1}{\sqrt{6}L}\\ \frac{1}{\sqrt{6}L}&4\end{bmatrix}\otimes\mathbf{I}_{d},\]

we have

\[L\mathfrak{M}-c_{0}\mathcal{M}^{-1}(\mathcal{M}^{-1})^{\top}=\begin{bmatrix}1/ 4-c_{0}&\sqrt{3L}(1/\sqrt{2}+c_{0}\sqrt{2})\\ \sqrt{3L}(1/\sqrt{2}+c_{0}\sqrt{2})&3L(4-c_{0})\end{bmatrix}.\]

The determinant is

\[3L\cdot\left[\left(\frac{1}{4}-c_{0}\right)\cdot(4-c_{0})-\left(\frac{1}{\sqrt {2}}+c_{0}\sqrt{2}\right)^{2}\right]>0\]

for \(c_{0}>0\) sufficiently small, which means that

\[\mathcal{M}^{-1}(\mathcal{M}^{-1})^{\top}\preceq c_{0}^{-1}L\mathfrak{M}.\]

Therefore, Eq. 88 becomes

\[\mathbb{E}_{\pi_{T}^{\mathcal{M}}}\left[\left\|\nabla\log\frac{\pi_{T}^{ \mathcal{M}}}{\pi^{\mathcal{M}}}\right\|^{2}\right]\lesssim 3L\cdot\mathbb{E}_{ \pi_{T}}\left[\left\|\mathfrak{M}^{1/2}\nabla\log\frac{\pi_{T}}{\pi^{ \leftarrow}}\right\|^{2}\right].\]

According to Lemma D.2, the decay of the Fisher information requires us to set

\[T\gtrsim L^{-1/2}\cdot\log\left[\epsilon^{-2}\cdot\left(\mathrm{KL}\left(\pi_ {0}\right\|\pi^{\leftarrow}\right)+\mathbb{E}_{\pi_{0}}\left(\left\|\mathfrak{ M}^{1/2}\nabla\log\frac{\pi_{0}}{\pi^{\leftarrow}}\right\|^{2}\right)\right) \right], \tag{89}\]

which yields \(\mathrm{KL}\left(\pi_{T}^{\mathcal{M}}\big{\|}\pi^{\mathcal{M}}\right)\leq \epsilon^{2}\). Besides, we can easily have

\[\mathbb{E}_{\pi_{0}}\left(\left\|\mathfrak{M}^{1/2}\nabla\log\frac{\pi_{0}}{ \pi^{\leftarrow}}\right\|^{2}\right)\lesssim\frac{1}{3L}\cdot\mathrm{FI}\left( \pi_{0}\right\|\pi^{\leftarrow}\right)=\frac{1}{3L}\cdot\mathbb{E}_{\pi_{0}} \left(\left\|\nabla\log\frac{\pi_{0}}{\pi^{\leftarrow}}\right\|^{2}\right).\]

According to the definition of LSI, we also have

\[\mathrm{KL}\left(\pi_{0}\big{\|}\pi^{\leftarrow}\right)\leq\frac{C_{\mathrm{ LSI}}}{2}\cdot\mathrm{FI}\left(\pi_{0}\big{\|}\pi^{\leftarrow}\right)=\frac{1}{2 L}\cdot\mathbb{E}_{\pi_{0}}\left(\left\|\nabla\log\frac{\pi_{0}}{\pi^{ \leftarrow}}\right\|^{2}\right).\]

Recall as well that this requires \(\gamma\asymp\sqrt{3L}\) in SDE. 85. For the remaining \(\mathrm{KL}\left(\hat{\pi}_{T}^{\mathcal{M}}\big{\|}\pi_{T}^{\mathcal{M}}\right)\) and \(\chi^{2}\left(\hat{\pi}_{T}^{\mathcal{M}}\big{\|}\pi_{T}^{\mathcal{M}}\right)\) in Eq. 87, we invoke Lemma D.3 with the value \(T=S\tau\) specified and desired accuracy \(\epsilon\),, which consequently yields

\[\tau=\tilde{\Theta}\left(\frac{\epsilon}{L^{3/4}d^{1/2}T^{1/2}}\right)\quad \mathrm{and}\quad S=\tilde{\Theta}\left(\frac{T^{3/2}L^{3/4}d^{1/2}}{\epsilon }\right). \tag{90}\]

Under this condition, we start to consider the initialization error. Suppose we have \(\pi_{0}=\mathcal{N}(\mathbf{0},e^{2\eta}-1)\otimes\mathcal{N}(\mathbf{0},\mathbf{ I})\), which implies

\[\mathrm{FI}\left(\pi_{0}\big{\|}\pi^{\leftarrow}\right)\lesssim \mathbb{E}_{\pi_{0}}\left[\left\|\nabla f_{(K-k-1)\eta}(\mathbf{ z})-\nabla f_{(K-k-1)\eta}(\mathbf{0})+\nabla f_{(K-k-1)\eta}(\mathbf{0})- \frac{e^{-\eta}\mathbf{x}_{0}}{1-e^{-2\eta}}\right\|^{2}\right]\] \[\leq 3L^{2}\mathbb{E}_{\pi_{0}}[\left\|\mathbf{z}\right\|^{2}]+3 \left\|\nabla f_{(K-k-1)\eta}(\mathbf{0})\right\|^{2}+\frac{3e^{-2\eta}}{(1-e^{ -2\eta})^{2}}\cdot\left\|\mathbf{x}_{0}\right\|^{2}\] \[= 3L^{2}\cdot\left(e^{2\eta}-1\right)+3\left\|\nabla f_{(K-k-1) \eta}(\mathbf{0})\right\|^{2}+\frac{3e^{-2\eta}}{(1-e^{-2\eta})^{2}}\cdot\left\| \mathbf{x}_{0}\right\|^{2}\]Following the \(\eta\) setting, i.e.,

\[\eta=\frac{1}{2}\log\frac{2L+1}{2L}\quad\Leftrightarrow\quad e^{2\eta}=\frac{2L+1} {2L},\]

which yields

\[\mathrm{FI}\left(\pi_{0}\big{\|}\pi^{\leftarrow}\right)\lesssim L+\big{\|}\nabla f_{(K-k-1)\eta}(\mathbf{0})\big{\|}^{2}+L^{2}\|\mathbf{x}_{0}\|^{2}\] \[\lesssim L+L^{2}(d+m_{2}^{2})+L^{2}\|\mathbf{x}_{0}\|^{2} \tag{91}\]

where the inequality follows from Eq. 73. Therefore, combining Eq. 91, Eq. 90 and Eq. 89, we have

\[T^{1/2}\gtrsim L^{-1/4}\cdot\left(\log\left[\frac{L(d+m_{2}^{2}+\|\mathbf{x}_{0}\| ^{2})}{\epsilon^{2}}\right]\right)^{1/2}\gtrsim L^{1/4}\cdot\left(\log\left[ \frac{\mathbb{E}_{\pi_{0}}\left(\big{\|}\nabla\log\frac{\pi_{0}}{\pi^{ \leftarrow}}\big{\|}^{2}\right)}{L\epsilon^{2}}\right]\right)^{1/2},\]

which implies

\[\tau =\tilde{\Theta}\left(\epsilon d^{-1/2}L^{-1/2}\cdot\left(\log \left[\frac{L(d+m_{2}^{2}+\|\mathbf{x}_{0}\|^{2})}{\epsilon^{2}}\right]\right)^{- 1/2}\right)\] \[S =\tilde{\Theta}\left(\epsilon^{-1}d^{1/2}\cdot\left(\log\left[ \frac{L(d+m_{2}^{2}+\|\mathbf{x}_{0}\|^{2})}{\epsilon^{2}}\right]\right)^{1/2} \right).\]

In this condition, the score estimation error is required to be

\[\epsilon_{\mathrm{score}}=\mathcal{O}\left(\gamma^{1/2}T^{-1/2}\cdot\epsilon \right)=\tilde{\mathcal{O}}\left(\epsilon/\sqrt{L}\right).\]

Hence, the proof is completed. 

**Theorem D.6**.: _Under Assumption **[A1]-[A2]**, for Alg. 1, we choose_

\[\eta=\frac{1}{2}\log\frac{2L+1}{2L}\quad\mathrm{and}\quad K=4L\cdot\log\frac{ \left(1+L^{2}\right)d+\left\|\nabla f_{*}(\mathbf{0})\right\|^{2}}{\epsilon^ {2}}\]

_and implement Step 3 of Alg. 1 with projected Alg. 3. For the \(k\)-th run of Alg. 3, we require Gaussian-type initialization and high-accurate score estimation, i.e.,_

\[\hat{\pi}_{0}=\mathcal{N}(\mathbf{0},e^{2\eta}-1)\otimes\mathcal{N}(\mathbf{0 },\mathbf{I})\quad\mathrm{and}\quad\epsilon_{\mathrm{score}}=\tilde{\mathcal{O}}( \epsilon).\]

_If we set the hyperparameters as shown in Lemma D.5, it can achieve \(\mathrm{TV}\left(\hat{p}_{K\eta},p_{*}\right)\lesssim\epsilon\) with an \(\tilde{\mathcal{O}}\left(L^{2}d^{1/2}\epsilon^{-1}\right)\) gradient complexity._

Proof.: According to Corollary B.5, we know that under the choice

\[\eta=\frac{1}{2}\ln\frac{2L+1}{2L},\]

it requires to run Alg. 3 for \(K\) times where

\[K=4L\cdot\log\frac{\left(1+L^{2}\right)d+\left\|\nabla f_{*}(\mathbf{0}) \right\|^{2}}{\epsilon^{2}}.\]

For each run of Alg. 3, we require the KL divergence error to achieve

\[\mathrm{KL}\left(\hat{p}_{(k+1)\eta|k\eta}(\cdot|\hat{\mathbf{x}})\big{\|}p_{(k+ 1)\eta|k\eta}^{\leftarrow}(\cdot|\hat{\mathbf{x}})\right)\leq \frac{\epsilon^{2}}{4L}\cdot\left[\log\frac{\left(1+L^{2}\right)d+ \left\|\nabla f_{*}(\mathbf{0})\right\|^{2}}{\epsilon^{2}}\right]^{-1}.\]

Combining with Theorem D.5, we consider a step size

\[\tau_{k}= \tilde{\mathcal{O}}\left(L^{-1}d^{-1/2}\epsilon\cdot\left(\log \left[L^{2}\cdot(d+m_{2}^{2}+\|\hat{\mathbf{x}}_{k}\|^{2})\right]\right)^{-1/2}\right)\]

then the iteration number will be

\[S_{k}=\tilde{\mathcal{O}}\left(L^{1/2}d^{1/2}\epsilon^{-1}\cdot\left(\log \left[L^{2}\cdot(d+m_{2}^{2}+\|\hat{\mathbf{x}}_{k}\|^{2})\right]\right)^{1/2} \right).\]

For an expectation perspective, we have

\[\mathbb{E}_{\hat{p}_{k\eta}}\left[\log(L^{2}\|\hat{\mathbf{x}}_{k}\|^{2})\right] \leq\log\left[\mathbb{E}_{\hat{p}_{k\eta}}(\|\hat{\mathbf{x}}_{k}\|^{2})\right]= \tilde{\mathcal{O}}(L)\]

where the last inequality follows from Lemma B.6. This means that with the total gradient complexity

\[K\cdot S=\tilde{\mathcal{O}}\left(L^{2}d^{1/2}\epsilon^{-1}\right)\]

Hence, the proof is completed.

Auxiliary Lemmas

**Lemma E.1** (Theorem 4 in [35]).: _Suppose \(p\propto\exp(-f)\) defined on \(\mathbb{R}^{d}\) satisfies LSI with constant \(\mu>0\). Along the Langevin dynamics, i.e.,_

\[\mathrm{d}\mathbf{x}_{t}=-\nabla f(\mathbf{x})\mathrm{d}t+\sqrt{2}\mathrm{d} \boldsymbol{B}_{t},\]

_where \(\mathbf{x}_{t}\sim p_{t}\), then it has_

\[\mathrm{KL}\left(p_{t}\big{\|}p\right)\leq\exp\left(-2\mu t\right)\cdot \mathrm{KL}\left(p_{0}\big{\|}p\right).\]

**Lemma E.2**.: _Suppose \(p\propto\exp(-f)\) defined on \(\mathbb{R}^{d}\) satisfies LSI with constant \(\mu>0\) where \(f\) is L-smooth, i.e.,_

\[\left\|\nabla f(\boldsymbol{x}^{\prime})-\nabla f(\boldsymbol{x})\right\|\leq L \left\|\boldsymbol{x}^{\prime}-\boldsymbol{x}\right\|.\]

_If \(p_{0}\) is the standard Gaussian distribution defined on \(\mathbb{R}^{d}\), then we have_

\[\mathrm{KL}\left(p_{0}\big{\|}p\right)\leq\frac{(1+2L^{2})d+2\left\|\nabla f( \boldsymbol{0})\right\|^{2}}{\mu}.\]

Proof.: According to the definition of LSI, we have

\[\mathrm{KL}\left(p_{0}\big{\|}p\right)\leq \frac{1}{2\mu}\int p_{0}(\boldsymbol{x})\left\|\nabla\log\frac{p_ {0}(\boldsymbol{x})}{p(\boldsymbol{x})}\right\|^{2}\mathrm{d}\boldsymbol{x}= \frac{1}{2\mu}\int p_{0}(\boldsymbol{x})\left\|-\boldsymbol{x}+\nabla f( \boldsymbol{x})\right\|^{2}\mathrm{d}\boldsymbol{x}\] \[\leq \mu^{-1}\cdot\left[\int p_{0}(\boldsymbol{x})\|\boldsymbol{x}\|^{ 2}\mathrm{d}\boldsymbol{x}+\int p_{0}(\boldsymbol{x})\|\nabla f(\boldsymbol{x}) -\nabla f(\boldsymbol{0})+\nabla f(\boldsymbol{0})\|^{2}\mathrm{d}\boldsymbol{ x}\right]\] \[\leq \mu^{-1}\cdot\left[(1+2L^{2})\int p_{0}(\boldsymbol{x})\| \boldsymbol{x}\|^{2}\mathrm{d}\boldsymbol{x}+2\left\|\nabla f(\boldsymbol{0}) \right\|^{2}\right]\] \[= \frac{(1+2L^{2})d+2\left\|\nabla f(\boldsymbol{0})\right\|^{2}}{ \mu}\]

where the third inequality follows from the \(L\)-smoothness of \(f_{*}\) and the last equation establishes since \(\mathbb{E}_{p_{0}}[\|\boldsymbol{x}\|^{2}]=d\) is for the standard Gaussian distribution \(p_{0}\) in \(\mathbb{R}^{d}\). 

**Lemma E.3** (Variant of Lemma B.1 in [41]).: _Suppose \(f\colon\mathbb{R}^{d}\to\mathbb{R}\) is a \(m\)-strongly convex function and satisfies \(L\)-smooth. Then, we have_

\[\nabla f(\boldsymbol{x})\cdot\boldsymbol{x}\geq\frac{m\left\|\boldsymbol{x} \right\|^{2}}{2}-\frac{\left\|\nabla f(\boldsymbol{0})\right\|^{2}}{2m}\]

_where \(\boldsymbol{x}_{*}\) is the global optimum of the function \(f\)._

Proof.: According to the definition of strongly convex, the function \(f\) satisfies

\[f(\boldsymbol{0})-f(\boldsymbol{x})\geq\nabla f(\boldsymbol{x})\cdot( \boldsymbol{0}-\boldsymbol{x})+\frac{m}{2}\cdot\left\|\boldsymbol{x}\right\|^{ 2}\;\Leftrightarrow\;\nabla f(\boldsymbol{x})\cdot\boldsymbol{x}\geq f( \boldsymbol{x})-f(\boldsymbol{0})+\frac{m}{2}\cdot\left\|\boldsymbol{x}\right\| ^{2}.\]

Besides, we have

\[f(\boldsymbol{x})-f(\boldsymbol{0})\geq\nabla f(\boldsymbol{0})\cdot \boldsymbol{x}+\frac{m}{2}\cdot\|\boldsymbol{x}\|^{2}\geq\frac{m}{2}\cdot\| \boldsymbol{x}\|^{2}-\frac{m}{2}\cdot\|\boldsymbol{x}\|^{2}-\frac{\|\nabla f( \boldsymbol{0})\|^{2}}{2m}=-\frac{\|\nabla f(\boldsymbol{0})\|^{2}}{2m}.\]

Combining the above two inequalities, the proof is completed. 

**Lemma E.4** (Lemma A.1 in [41]).: _Suppose a function \(f\) satisfy_

\[\nabla f(\boldsymbol{x})\cdot\boldsymbol{x}\geq\frac{m\|\boldsymbol{x}\|^{2}} {2}-\frac{\left\|\nabla f(\boldsymbol{0})\right\|}{2m},\]

_then we have_

\[f(\boldsymbol{x})\geq\frac{m}{8}\|\boldsymbol{x}\|^{2}+f(\boldsymbol{x}_{*})- \frac{\|\nabla f(\boldsymbol{0})\|^{2}}{4m}.\]

**Lemma E.5** (Lemma 1 in [18]).: _Consider the Ornstein-Uhlenbeck forward process_

\[\mathrm{d}\mathbf{x}_{t}=-\mathbf{x}_{t}\mathrm{d}t+\sqrt{2}\mathrm{d}\mathbf{B}_{t},\]

_and denote the underlying distribution of the particle \(\mathbf{x}_{t}\) as \(p_{t}\). Then, the score function can be rewritten as_

\[\nabla_{\mathbf{x}}\ln p_{t}(\mathbf{x})=\mathbb{E}_{\mathbf{x}_{0}\sim q_{t}\cdot[ \cdot|\mathbf{x})}\frac{e^{-t}\mathbf{x}_{0}-\mathbf{x}}{(1-e^{-2t})}, \tag{92}\]

**Lemma E.6** (Lemma 11 in [35]).: _Assume \(p\propto\exp(-f)\) and the energy function \(f\) is \(L\)-smooth. Then_

\[\mathbb{E}_{\mathbf{x}\sim p}\left[\left\|\nabla f(\mathbf{x})\right\|^{2} \right]\leq Ld\]

**Lemma E.7** (Lemma 10 in [8]).: _Suppose that Assumption **[A1]-[A2]** hold. Let \(\{\mathbf{x}_{t}\}_{t\in[0,T]}\) denote the forward process, i.e., Eq. 1, for all \(t\geq 0\),_

\[\mathbb{E}\left[\left\|\mathbf{x}\right\|^{2}\right]\leq\max\left\{d,m_{2}^{2 }\right\}.\]

**Lemma E.8**.: _Suppose \(q\) is a distribution which satisfies LSI with constant \(\mu\), then its variance satisfies_

\[\int q(\mathbf{x})\left\|\mathbf{x}-\mathbb{E}_{\tilde{q}}\left[\mathbf{x}\right]\right\| ^{2}\mathrm{d}\mathbf{x}\leq\frac{d}{\mu}.\]

Proof.: It is known that LSI implies Poincare inequality with the same constant, i.e., \(\mu\), which means if for all smooth function \(g\colon\mathbb{R}^{d}\to\mathbb{R}\),

\[\mathrm{var}_{q}\left(g(\mathbf{x})\right)\leq\frac{1}{\mu}\mathbb{E}_{q}\left[ \left\|\nabla g(\mathbf{x})\right\|^{2}\right].\]

In this condition, we suppose \(\mathbf{b}=\mathbb{E}_{q}[\mathbf{x}]\), and have the following equation

\[\int q(\mathbf{x})\left\|\mathbf{x}-\mathbb{E}_{q}\left[\mathbf{x}\right] \right\|^{2}\mathrm{d}\mathbf{x}=\int q(\mathbf{x})\left\|\mathbf{x}-\mathbf{b}\right\|^{2} \mathrm{d}\mathbf{x}\] \[= \int\sum_{i=1}^{d}q(\mathbf{x})\left(\mathbf{x}_{i}-\mathbf{b}_{i}\right)^{2} \mathrm{d}\mathbf{x}=\sum_{i=1}^{d}\int q(\mathbf{x})\left(\left\langle\mathbf{x},\mathbf{e}_{ i}\right\rangle-\left\langle\mathbf{b},\mathbf{e}_{i}\right\rangle\right)^{2}\mathrm{d}\mathbf{x}\] \[= \sum_{i=1}^{d}\int q(\mathbf{x})\left(\left\langle\mathbf{x},\mathbf{e}_{i} \right\rangle-\mathbb{E}_{q}\left[\left\langle\mathbf{x},\mathbf{e}_{i}\right\rangle \right]\right)^{2}\mathrm{d}\mathbf{x}=\sum_{i=1}^{d}\mathrm{var}_{q}\left(g_{i}( \mathbf{x})\right)\]

where \(g_{i}(\mathbf{x})\) is defined as \(g_{i}(\mathbf{x})\coloneqq\left\langle\mathbf{x},\mathbf{e}_{i}\right\rangle\) and \(\mathbf{e}_{i}\) is a one-hot vector ( the \(i\)-th element of \(\mathbf{e}_{i}\) is \(1\) others are \(0\)). Combining this equation and Poincare inequality, for each \(i\), we have

\[\mathrm{var}_{q}\left(g_{i}(\mathbf{x})\right)\leq\frac{1}{\mu}\mathbb{E}_{q} \left[\left\|\mathbf{e}_{i}\right\|^{2}\right]=\frac{1}{\mu}.\]

Hence, the proof is completed. 

**Lemma E.9** (Variant of Lemma 10 in [11]).: _Suppose \(-\log p_{*}\) is \(m\)-strongly convex function, for any distribution with density function \(p\), we have_

\[\mathrm{KL}\left(p\big{\|}p_{*}\right)\leq\frac{1}{2m}\int p(\mathbf{x})\left\| \nabla\log\frac{p(\mathbf{x})}{p_{*}(\mathbf{x})}\right\|^{2}\mathrm{d}\mathbf{x}.\]

_By choosing \(p(\mathbf{x})=g^{2}(\mathbf{x})p_{*}(\mathbf{x})/\mathbb{E}_{p_{*}}\left[g^{2}(\mathbf{x})\right]\) for the test function \(g\colon\mathbb{R}^{d}\to\mathbb{R}\) and \(\mathbb{E}_{p_{*}}\left[g^{2}(\mathbf{x})\right]<\infty\), we have_

\[\mathbb{E}_{p_{*}}\left[g^{2}\log g^{2}\right]-\mathbb{E}_{p_{*}}\left[g^{2} \right]\log\mathbb{E}_{p_{*}}\left[g^{2}\right]\leq\frac{2}{m}\mathbb{E}_{p_{*} }\left[\left\|\nabla g\right\|^{2}\right],\]

_which implies \(p_{*}\) satisfies \(1/m\)-log-Sobolev inequality._

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have clear claim that we improve the diffusion inference by RTK framework. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: We discussed the limitation in section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: See Section 4 A1, A2, E1, E2, E3 for Assumptions. Proof are provided in the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See our Appendix. Guidelines: ** The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have detailed information in the Appendix to reproduce the emprical results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ** At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See the Appendix for more information. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: We just have illustrative empirical result about the trend and generated samples. Not claims for accuracy or error from the emprisal side. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See appendix. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Conducted Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: It is a theoretical paper. No societal impact is visible in a short term. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No real data in this paper.

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: No real data are used. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Only synthetic data are used with detailed instructions. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA]

Justification: NA.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: NA. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.