# On Improved Conditioning Mechanisms and Pre-training Strategies for Diffusion Models

Tariq Berrada\({}^{1,2}\)  Pietro Astolfi\({}^{1}\)  Melissa Hall\({}^{1}\)  Reyhane Askari-Hemmat\({}^{1}\)

Yohann Benchetrit\({}^{1}\)  Marton Havasi\({}^{1}\)  Matthew Muckley\({}^{1}\)  Karteek Alahari\({}^{2}\)

Adriana Romero-Soriano\({}^{1,3,4,5}\)  Jakob Verbeek\({}^{1}\)  Michal Drozdzal\({}^{1}\)

\({}^{1}\)FAIR at Meta \({}^{2}\)Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, France

\({}^{3}\)McGill University \({}^{4}\)Mila, Quebec AI institute \({}^{5}\)Canada CIFAR AI chair

###### Abstract

Large-scale training of latent diffusion models (LDMs) has enabled unprecedented quality in image generation. However, the key components of the best performing LDM training recipes are oftentimes not available to the research community, preventing apple-to-apple comparisons and hindering the validation of progress in the field. In this work, we perform an in-depth study of LDM training recipes focusing on the performance of models and their training efficiency. To ensure apple-to-apple comparisons, we re-implement five previously published models with their corresponding recipes. Through our study, we explore the effects of (i) the mechanisms used to condition the generative model on semantic information (_e.g._, text prompt) and control metadata (_e.g._, crop size, random flip flag, _etc._) on the model performance, and (ii) the transfer of the representations learned on smaller and lower-resolution datasets to larger ones on the training efficiency and model performance. We then propose a novel conditioning mechanism that disentangles semantic and control metadata conditionings and sets a new state-of-the-art in class-conditional generation on the ImageNet-1k dataset - with FID improvements of 7% on 256 and 8% on 512 resolutions - as well as text-to-image generation on the CC12M dataset - with FID improvements of 8% on 256 and 23% on 512 resolution.

## 1 Introduction

Diffusion models have emerged as a powerful class of generative models and demonstrated unprecedented ability at generating high-quality and realistic images. Their superior performance is evident across a spectrum of applications, encompassing image [7, 14, 39, 41] and video synthesis [35], denoising [52], super-resolution [49] and layout-to-image synthesis [51]. The fundamental principle underpinning diffusion models is the iterative denoising of an initial sample from a trivial prior distribution, that progressively transforms it to a sample from the target distribution. The popularity of diffusion models can be attributed to several factors. First, they offer a simple yet effective approach for generative modeling, often outperforming traditional approaches such as Generative Adversarial Networks (GANs) [3, 16, 24, 25] and Variational Autoencoders (VAEs) [29, 48] in terms of visual fidelity and sample diversity. Second, diffusion models are generally more stable and less prone to mode collapse compared to GANs, which are notoriously difficult to stabilize without careful tuning of hyperparameters and training procedures [23, 30].

Despite the success of diffusion models, training such models at scale remains computationally challenging, leading to a lack of insights on the most effective training strategies. Training recipes of large-scale models are often closed (_e.g._, DALL-E, Imagen, Midjourney), and only a few studies have analyzed training dynamics in detail [7, 14, 26, 27]. Moreover, evaluation often involves human studies which are easily biased and hard to replicate [17, 56]. Due to the high computational costs, the research community mostly focused on the finetuning of large text-to-image models for different downstream tasks [1, 4, 54] and efficient sampling techniques [34, 36, 45]. However, there has been less focus on ablating different mechanisms to condition on user inputs such as text prompts, and strategies to pre-train using datasets of smaller resolution and/or data size. The benefits of conditioning mechanisms are two-fold: allowing users to have better control over the content that is being generated, and unlocking training on augmented or lower quality data by for example conditioning on the original image size [39] and other metadata of the data augmentation. Improving pre-training strategies, on the other hand, can allow for big cuts in the training cost of diffusion models by significantly reducing the number of iterations necessary for convergence.

Our work aims to disambiguate some of these design choices, and provide a set of guidelines that enable the scaling of the training of diffusion models in an efficient and effective manner. Beyond the main architectural choices (_e.g._, Unet _vs._ ViT), we focus on two other important aspects for generative performance and efficiency of training. First, we enhance conditioning by decoupling different conditionings based on their type: control metadata conditioning (_e.g._, crop size, random flip, _etc._), semantic-level conditioning based on class names or text-prompts. In this manner, we disentangle the contribution of each conditioning and avoid undesired interference among them. Second, we optimize the scaling strategy to larger dataset sizes and higher resolution by studying the influence of the initialization of the model with weights from models pre-trained on smaller datasets and resolutions. Here, we propose three improvements needed to seamlessly transition across resolutions: interpolation of the positional embeddings, scaling of the noise schedule, and using a more aggressive data augmentation strategy.

In our experiments we evaluate models at 256 and 512 resolution on ImageNet-1k and Conceptual Captions (CC12M), and also present results for ImageNet-22k at 256 resolution. We study the following five architectures: _Unet/LDM-G4_[39], _DiT-XL2 w/ LN_[38], _mDT-v2-XL2 w/ LN_[15], _PixArt-\(\alpha\)-XL2/_, and _mmDiT-XL2 (SD3)_[14]. We find that among the studied base architectures, _mmDiT-XL2 (SD3)_ performs the best. Our improved conditioning approach further boosts the performance of the best model consistently across metrics, resolutions, and datasets. In particular, we improve the previous state-of-the-art DiT result of 3.04 FID on ImageNet-1k at 512 resolution to 2.76. For CC12M at 512 resolution, we improve FID of 11.24 to 8.64 when using our improved conditioning, while also obtaining a (small) improvement in CLIPscore from 26.01 to 26.17. See Fig. 1 for qualitative examples of our model trained on CC12M.

Figure 1: **Qualitative examples. Images generated using our model trained on CC12M at 512 resolution.**

In summary, our contributions are the following:

* We present a systematic study of five different diffusion architectures, which we train from scratch using face-blurred ImageNet and CC12M datasets at 256 and 512 resolutions.
* We introduce a conditioning mechanism that disentangles different control conditionings and semantic-level conditioning, improving generation and avoiding interference between conditions.
* To transfer weights from pre-trained models we propose to interpolate positional embeddings, scale the noise schedule, and use stronger data augmentation, leading to improved performance.
* We obtain state-of-the-art results at 256 and 512 resolution for class-conditional generation on ImageNet-1k and text-to-image generation and CC12M.

## 2 Conditioning and pre-training strategies for diffusion models

In this section, we review and analyze the conditioning mechanisms and pre-training strategies used in prior work (see more detailed discussion of related work in App. A), and propose improved approaches based on the analysis.

### Conditioning mechanisms

**Background.** To control the generated content, diffusion models are usually conditioned on class labels or text prompts. Adaptive layer norm is a lightweight solution to condition on class labels, used for both UNets [21; 39; 41] and DiT models [38]. Cross-attention is used to allow more fine-grained conditioning on textual prompts, where particular regions of the sampled image are affected only by part of the prompt, see _e.g._[41]. More recently, another attention based conditioning was proposed in SD3 [14] within a transformer-based architecture that evolves both the visual and textual tokens across layers. It concatenates the image and text tokens across the sequence dimension, and then performs a self-attention operation on the combined sequence. Because of the difference between the two modalities, the keys and queries are normalized using RMSNorm [53], which stabilizes training. This enables complex interactions between the two modalities in one attention block instead of using both self-attention and cross-attention blocks.

Moreover, since generative models aim to learn the distribution of the training data, data quality is important when training generative models. Having low quality training samples, such as the ones that are poorly cropped or have unnatural aspect ratios, can result in low quality generations. Previous work tackles this problem by careful data curation and fine-tuning on high quality data, see _e.g._[7; 9]. However, strictly filtering the training data may deprive the model from large portions of the available data [39], and collecting high-quality data is not trivial. Rather than treating them as nuisance factors, SDXL [39] proposes an alternative solution where a UNet-based model is conditioned on parameters corresponding to image size and crop parameters during training. In this manner, the model is aware of these parameters and can account for them during training, while also offering users control over these parameters during inference. These _control conditions_ are transformed and additively combined with the timestep embedding before feeding them to the diffusion model.

**Disentangled control conditions.** Straightforward implementation of control conditions in DiT may cause interference between the time-step, class-level and control conditions if their corresponding embeddings are additively combined in the adaptive layer norm conditioning, _e.g._ causing changes in high-level content of the generated image when modifying its resolution, see Fig. 2. To disentangle the different conditions, we propose two modifications. First, we move the class embedding to be fed through the attention layers present in the DiT blocks. Second, to ensure that the control embedding

Figure 2: **Influence of control conditions. Images generated using the same latent sample. Top: Model trained with constant weighting of the size conditioning as used in SDXL [39], introducing undesirable correlations between image content and size condition. Bottom: Model trained using our cosine weighting of low-level conditioning, disentangling the size condition from the image content.**

does not overpower the timestep embedding when additively combined in the adaptive layer norm, we zero out the control embedding in early denoising steps, and gradually increase its strength.

Control conditions can be used for different types of data augmentations: (i) _high-level augmentations (\(\phi_{h}\))_ that affect the image composition - _e.g._ flipping, cropping and aspect ratio -, and (ii) _low-level augmentations (\(\phi_{l}\))_ that affect low-level details - _e.g._ image resolution and color. Intuitively, high-level augmentations should impact the image formation process early on, while low-level augmentations should enter the process only once sufficient image details are present. We achieve this by scaling the contribution of the low-level augmentations, \(\phi_{l}\), to the control embedding using a cosine schedule that downweights earlier contributions:

\[c_{\text{emb}}(\phi_{h},\phi_{l},t)=E_{h}(\phi_{h})+\gamma_{c}(t)\cdot E_{l}( \phi_{l}), \tag{1}\]

where the embedding functions \(E_{h},E_{l}\) are made of sinusoidal embeddings followed by a 2-layer MLP with SiLU activation, and where \(\gamma_{c}\) is the cosine schedule illustrated in Fig. 3.

**Improved text conditioning.** Most commonly used text encoders, like CLIP [40], output a constant number of tokens \(T\) that are fed to the denoising model (usually \(T=77\)). Consequently, when the prompt has less than \(T\) tokens, the remaining positions are filled by zero-padding, but remain accessible via cross-attention to the denoising network. To make better use of the conditioning vector, we propose a _noisy replicate_ padding mechanism where the padding tokens are replaced with copies of the text tokens, thereby pushing the subsequent cross-attention layers to attend to all the tokens in its inputs. As this might lead to redundant token embeddings, we improve the diversity of the feature representation across the sequence dimension, by perturbing the embeddings with additive Gaussian noise with a small variance \(\beta_{\text{txt}}\). To ensure enough diversity in the token embeddings, we scale the additive noise by \(\sigma(\phi_{\text{txt}})\sqrt{m-1}\), where \(m\) is the number of token replications needed for padding, and \(\sigma(\phi_{\text{txt}})\) is the per-channel standard deviation in the token embeddings.

**Integrating classifier-free guidance.** Classifier-free guidance (CFG) [20] allows for training conditional models by combining the output of the unconditional generation with the output of the conditional generation. Formally, given a latent diffusion model trained to predict the noise \(\epsilon\), CFG reads as: \(\epsilon^{\lambda}=\lambda\cdot\epsilon_{s}+(1-\lambda)\cdot\epsilon_{\emptyset}\), where \(\epsilon_{\emptyset}\) is the unconditional noise prediction, \(\epsilon_{s}\) is the noise prediction conditioned on the semantic conditioning \(s\) (_e.g._, text prompt), and \(\lambda\) is the hyper-parameter, known as _guidance scale_, which regulates the strength of the conditioning. Importantly, during training \(\lambda\) is set alternatively to 0 or 1, while at inference time it is arbitrarily changed in order to steer the generation to be more or less consistent with the conditioning. In our case, we propose the control conditioning to be an auxiliary guidance term, in order to separately regulate the strength of the conditioning on the control variables \(c\) and semantic conditioning \(s\) at inference time. In particular, we define the guided noise estimate as:

\[\epsilon^{\lambda,\beta}=\lambda\left[\beta\cdot\epsilon_{c,s}+(1-\beta)\cdot \epsilon_{\emptyset,s}\right]+(1-\lambda)\cdot\epsilon_{\emptyset,\emptyset}, \tag{2}\]

where \(\beta\) sets the strength of the control guidance, and \(\lambda\) sets the strength of the semantic guidance.

### On transferring models pre-trained on different datasets and resolutions

Background.Transfer learning has been a pillar of the deep learning community, enabling generalization to different domains and the emergence of foundational models such as DINO [5, 10] and CLIP [40]. Large pre-trained text-to-image diffusion models have also been re-purposed for different tasks, including image compression [4] and spatially-guided image generation [1]. Here, we are interested in understanding to which extent pre-training on other datasets and resolutions can be leveraged to achieve a more efficient training of large text-to-image models. Indeed, training diffusion models directly to generate high resolution images is computationally demanding, therefore, it is common to either couple them with super-resolution models, see _e.g._[44], or fine-tune them with high resolution data, see _e.g._[7, 14, 39]. Although most models can directly operate at a higher resolution than the one used for training, fine-tuning is important to adjust the model to the different statistics of high-resolution images. In particular, we find that the different statistics influence the positional embedding of patches, the noise schedule, and the optimal guidance scale. Therefore, we focus on improving the transferability of these components.

Figure 3: Weighting of low-level control conditions. The weight is zeroed out early on when image semantics are defined, and increased later when adding details.

**Positional Embedding.** Adapting to a higher resolution can be done in different ways. _Interpolation_ scales the - most often learnable - embeddings according to the new resolution [2, 47]. _Extrapolation_ simply replicates the embeddings of the original resolution to higher resolutions as illustrated in Fig. 4, resulting in a mismatch between the positional embeddings and the image features when switching to different resolutions. Most methods that use interpolation of learnable positional embeddings, _e.g_. [2, 47], adopt either bicubic or bilinear interpolation to avoid the norm reduction associated with the interpolation. In our case, we take advantage of the fact that our embeddings are sinusoidal and simply adjust the sampling grid to have constants limit under every resolution, see App. C.

**Scaling the noise schedule.** At higher resolution, the amount of noise necessary to mask objects at the same rate changes [14, 22]. If we observe a spatial patch at low resolution under a given uncertainty, upscaling the image by a factor \(s\) creates \(s^{2}\) observations of this patch of the form \(y_{t}^{(i)}=x_{t}+\sigma_{t}\epsilon^{(i)}-\) assuming the value of the patch is constant across the patch. This increase in the number of observations reduces the uncertainty around the value of that token, resulting in a higher signal-to-noise (SNR) ratio than expected. This issue gets further accentuated when the scheduler does not reach a terminal state with pure noise during training, _i.e_., a zero SNR [32], as the mismatch between the non-zero SNR seen during training and the purely Gaussian initial state of the sampling phase becomes significant. To resolve this, we scale the noise scheduler in order to recover the same uncertainty for the same timestep.

**Proposition 1**.: _When going from a scale of \(s\) to a scale \(s^{\prime}\), we update the \(\beta\) scheduler according to the following rule_

\[\bar{\alpha}_{t^{\prime}}=\frac{s^{2}\cdot\bar{\alpha}_{t}}{s^{\prime 2}+\bar{ \alpha}_{t}\cdot(s^{2}-s^{\prime 2})} \tag{3}\]

This increases the noise amplitude during intermediate denoising steps as illustrated in Fig. A1. The final equation obtained is similar to the one obtained in [14] with the accompanying change of variable \(t=\frac{\sigma^{2}}{1+\sigma^{2}}\).

**Pre-training cropping strategies.** When pre-training and finetuning at different resolutions, we can either first crop and then resize the crops according to the training resolution, or directly take differently sized crops from the training images. Using a different resizing during pre-training and finetuning may introduce some distribution shift, while using crops of different sizes may be detrimental to low-resolution training as the model will learn the distribution of smaller crops rather than full images, see Fig. 5. We experimentally investigate which strategy is more effective for low-resolution pre-training of high-resolution models.

**Guidance scale.** We discover that the optimal guidance scale for both FID and CLIPScore varies with the resolution of images. In App. D, we present a proof revealing that under certain conditions, the optimal guidance scale adheres to a scaling law with respect to the resolution, as

\[\lambda^{\prime}(s)=1+s\cdot(\lambda-1). \tag{4}\]

## 3 Experimental evaluation

### Experimental setup

**Datasets.** In our study, we train models on three datasets. To train class-conditional models, we use _ImageNet-1k_[11], which has 1.3M images spanning 1,000 classes, as well as _ImageNet-22k_[43], which contains 14.2M images spanning 21,841 classes. Additionally, we train text-to-image models using _Conceptual 12M_ (CC12M) [6], which contains 12M images with accompanying manually generated textual descriptions. We pre-process both datasets by blurring all faces. Differently from [7], we use the original captions for the CC12M dataset.

**Evaluation.** For image quality, we evaluate our models using the common FID [19] metric. We follow the standard evaluation protocol on ImageNet to have a fair comparison with the relevant

Figure 4: Interpolation and extrapolation of positional embeddings.

Figure 5: Low-resolution pre-training. Crop size used for pre-training impacts finetuning.

literature [3, 15, 38, 41]. Specifically, we compute the FID between the full training set and 50k synthetic samples generated using 250 DDIM sampling steps. For image-text alignment, we compute the CLIP [40] score similarly to [7, 14]. We measure conditional diversity, either using class-level or text prompt conditioning, using LPIPS [55]. LPIPS is measured pairwise and averaged among ten generations obtained with the same random seed, prompt, and initial noise, but different size conditioning (we exclude sizes smaller than the target resolution); then we report the average over 10k prompts. In addition to ImageNet and CC12M evaluations, we provide FID and CLIPScore on the COCO [33] validation set, which contains approximately 40k images with associated captions. For COCO evaluation [33], we follow the same setting as [14] for computing the CLIP score, using \(25\) sampling steps and a guidance scale of \(5.0\).

**Training.** To train our models we use the Adam [28] optimizer, with a learning rate of \(10^{-4}\) and \(\beta_{1},\beta_{2}=0.9,0.999\). When training at \(256\times 256\) resolution, we use a batch size of \(2,048\) images, a constant learning rate of \(10\times 10^{-4}\), train our models on two machines with eight A100 GPUs each. In preliminary experiments with the DiT architecture we found that the FID metric on ImageNet-1k at 256 resolution consistently improved with larger batches and learning rate, but that increasing the learning rate by another factor of two led to diverging runs. We report these results in supplementary. When training models at \(512\times 512\) resolution, we use the same approach but with a batch size of \(384\) distributed over 16 A100 GPUs. We train our ImageNet-1k models for 500k to 1M iterations and for 300k to 500k iterations for CC12M.

**Model architectures.** We train different diffusion architectures under the same setting to provide a fair comparison between model architectures. Specifically, we re-implement a UNet-based architecture following Stable Diffusion XL (SDXL) [39]1 and several transformer-based architectures: vanilla DiT [38], masked DiT (mDiT-v2) [15], PixArt DiT (PixArt-\(\alpha\)) [7], and multimodal DiT (mmDiT) as in Stable Diffusion 3 [14]. For vanilla DiT, which only supports class-conditional generation, we explore two variants one incorporating the class conditioning within LayerNorm and another one within the attention layer. Also, for text-conditioned models, we use the text encoder and tokenizer of CLIP (ViT-L/14) [40] having a maximum sequence length of \(T=77\). The final models share similar number parameters, _e.g._ for DiTs we inspect the XL/2 variant [38], for UNet (SDXL) we adopt similar size to the original LDM [41]. Similar to [14], we found the training of DiT with

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{3}{c}{ImageNet-1k} & \multicolumn{3}{c}{CC12M} \\ \cline{2-9}  & 256 & 512 & \multicolumn{3}{c}{256} & \multicolumn{3}{c}{512} \\ \cline{2-9}  & FID\({}_{\text{train}}\)\(\downarrow\) & FID\({}_{\text{train}}\)\(\downarrow\) & FID\({}_{\text{val}}\)\(\downarrow\) & CLIP\({}_{\text{COCO}}\)\(\uparrow\) & FID\({}_{\text{val}}\)\(\downarrow\) & FID\({}_{\text{COCO}}\)\(\downarrow\) & CLIP\({}_{\text{COCO}}\)\(\uparrow\) \\ \hline \hline _Results taken from references_ & _UNet (SDLM-G4)_[39] & \(3.60\) & — & \(17.01\) & \(24\) & — & \(9.62\) & — \\ _DiT-XL2 w/LN_[38] & \(2.27\) & \(3.04\) & — & — & — & — & — \\ _mDT-v2-XL2 w/ LN_[15] & \(1.79\) & — & — & — & — & — & — \\ _PixArt-\(\alpha\)-XL2_[7] & — & — & — & — & — & \(10.65\) & — \\ _mml-XL2 (SD3)_[14] & — & — & — & — & \(22.4\) & — & \({}^{*}\) & — \\ \hline \hline _Our re-implementation of existing architectures_ & _UNet_ (SDXL) & \(2.05\) & \(4.81\) & \(8.53\) & \(\mathbf{25.36}\) & \(12.56\) & \(7.26\) & \(24.79\) \\ _DiT-XL2 w/ LN_ & \(1.95\) & \(\mathbf{2.85}\) & — & — & — & — & — \\ _DiT-XL2 w/ Nat_ & \(\mathbf{1.71}\) & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ _mDT-v2-XL2 w/ LN_ & \(2.51\) & \(3.75\) & — & — & — & — & — \\ _PixArt-\(\alpha\)-XL2_[2] & \(2.06\) & \(3.05\) & ✗ & ✗ & ✗ & ✗ & ✗ \\ _mml-XL2 (SD3)_[14] & \(3.02\) & \(\mathbf{7.54}\) & \(24.78\) & \(\mathbf{11.24}\) & \(\mathbf{6.78}\) & \(\mathbf{26.01}\) \\ \hline \hline _Our improved architecture and training_ & _UNet-XL2 (ours)_ & \(\mathbf{1.59}\) & \(\mathbf{2.76}\) & \(\mathbf{6.79}\) & \(\mathbf{26.60}\) & \(\mathbf{6.27}\) & \(\mathbf{6.69}\) & \(\mathbf{26.17}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Comparison between different model architectures.** We compare results reported in the literature (top, reporting available numbers) with our reimplementations of existing architectures (middle), and to our best results obtained using architectural refinements and improved training. For 512 resolution, we trained models by fine-tuning models pre-trained at 256 resolution. In each column, we bold the best results among those in the first two blocks, and also those in the last row when they are equivalent or superior. ‘—’ denotes that numbers are unavailable in the original papers, or architectures are incompatible with text-to-image generation in our experiments. ‘✗’ indicates diverged runs. ‘*’ is used for Esser et al. [14] pre-trained on CC12M to denote that FID is computed differently and some details about their evaluation are unclear.

cross-attention to be unstable and had to resort to using RMSNorm [53] to normalize the key and query in the attention layers. We detail the models sizes and computational footprints in Tab. A1.

### Evaluation of model architectures and comparison with the state of the art

In Tab. 1, we report results for models with different architectures trained at both 256 and 512 resolutions for ImageNet and CC12M, and compare our results (2nd block of rows) with those reported in the literature, where available (1st block of rows). Where direct comparison is possible, we notice that our re-implementation outperforms the one of existing references. Overall, we found the mmDiT [14] architecture to perform best or second best in all settings compared to other alternatives. For this reason, we apply our conditioning improvements on top of this architecture (last row of the table), boosting the results as measured with FID and CLIPScore in all settings. Below, we analyse the improvements due to our conditioning mechanisms and pre-training strategies.

### Control conditioning

**Scheduling rate of control conditioning.** In Tab. 1(a) we consider the effect of controlling the conditioning on low-level augmentations via a cosine schedule for different decay rates \(\alpha\). We compare to baselines (first two rows) with constant weighting (as in SDXL [39]) and without control conditioning. We find that our cosine weighting schedule significantly reduces the dependence between size control and image semantics as it drastically improves the instance specific LPIPS (0.33 _vs._ 0.04) in comparison to uniform weighting. In terms of FID, we observe a small gap with the baseline (3.04 _vs._ 3.08), which increases (3.80 _vs._ 5.04) when computing FID by randomly sampling the size conditioning in the range [512,1024], see Tab. 1(b). Finally, the improved disentangling between semantics and low-level conditioning is clearly visible in the qualitative samples in Fig. 2.

**Crop and random-flip control conditioning.** A potential issue of horizontal flip data augmentations is that it can create misalignment between the text prompt and corresponding image. For example the prompt _"A teddy bear holding a baseball bat in their **right arm"_ will no longer be accurate when an image is flipped - showing a teddy bear holding the bat in their **left** arm. Similarly, cropping images can remove details mentioned in the corresponding caption. In Tab. 1(c) we evaluate models trained on CC12M@256 with and without horizontal flip conditioning, and find that adding this conditioning leads to slight improvements in both FID and CLIP as compared to using only crop conditioing. We depict qualitative comparison in Fig. 6, where we observe that flip conditioning improves prompt-layout consistency.

**Inference-time control conditioning of image size.** High-level augmentations (\(\phi_{h}\)) may affect the image semantics. As a result they influence the learned distribution and modify the generation diversity. For example, aspect ratio conditioning can harm the quality of generated images, when images of a particular class or text prompt are unlikely to appear with a given aspect ratio. In Tab. 1(b) we compare of different image size conditionings for inference. We find that conditioning on the same size distribution as encountered during the training of the model yields a significant boost in FID

\begin{table}
\begin{tabular}{c c c c c} \hline \hline _Init._ & \(t\) _weighting_ & _FID_ (\(\downarrow\)) & _LPIPS_ (\(\downarrow\)) & _LPIPS_/HR (\(\downarrow\)) \\ \hline _sem_ & _sem_ & \(3.29\) & \(-\) & \(-\) \\ _zero._ & _unif._ & \(\mathbf{3.08}\) & \(0.33\) & \(0.210\) \\ \hline _zero._ & _cos_(\(\alpha=1.0\)) & \(3.08\) & \(0.23\) & \(0.076\) \\ _zero._ & _cos_(\(\alpha=2.0\)) & \(3.09\) & \(0.18\) & \(0.045\) \\ _zero._ & _cos_(\(\alpha=4.0\)) & \(3.05\) & \(0.13\) & \(0.025\) \\ _zero._ & _cos_(\(\alpha=8.0\)) & \(\mathbf{3.04}\) & \(\mathbf{0.04}\) & \(\mathbf{0.009}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Control conditioning. We study different facets of control conditioning and their impact on the model performance. (a-b) We report FID\({}_{\text{train}}\) on ImageNet-1k@256 using 250 sampling steps. 120k training iterations.

as compared to generating all images with constant size conditioning or using uniformly randomly sampled sizes. Note that in all cases images are generated at 256 resolution.

**Control conditioning and guidance.** To understand how control condition impacts the generation process, we investigate the influence of control guidance \(\beta\) (introduced in Sec. 2.1 ) on FID and report the results in Fig. 7. We find that a higher control guidance scale results in improved FID scores. However, note that this improvement comes at the cost of compute due to the additional control term \(\epsilon_{c,s}\).

**Replication text padding.** We compare our noisy replication padding to the baseline zero-padding in Tab. 3. We observe that using a replication padding improves both FID and CLIP score, and that adding scaled perturbations further improves the results - \(0.35\) point improvement in CLIP score and \(0.4\) point improvement in FID.

### Transferring weights between datasets and resolutions

**Dataset shift.** We evaluate the effect of pre-training on ImageNet-1k (at 256 resolution) when training the models on CC12M or ImageNet-22k (at 512 resolution) by the time needed to achieve the same performance as a model trained from scratch. In Tab. 4a, when comparing models trained from scratch to ImageNet-1k pre-trained models (600k iterations) we observe two benefits: improved training convergence and performance boosts. For CC12M, we find that after only 100k iterations, both FID and CLIP scores improve over the baseline model trained with more than six times the

\begin{table}
\begin{tabular}{c c c c} \hline _Pre-train_ & _Finetune_ & _FID\({}_{\text{CLIP}}\)_ & _CLIP_ \\ \hline _IN22k (375k)_ & — & \(5.80\) & — \\ _IN1k_ & _IN22k (80k)_ & \(5.29\) (\(+8.67\%\)) & — \\ _IN1k_ & _IN22k (10k)_ & \(4.67\) (\(+17.82\%\)) & — \\ \hline _CC12M (600k)_ & — & \(7.54\) & \(24.78\) \\ _IN1k_ & _CC12M (600k)_ & \(7.59\) (\(-0.66\%\)) & \(25.09\) (\(+1.24\%\)) \\ _IN1k_ & _CC12M (100k)_ & \(7.27\) (\(+3.71\%\)) & \(25.62\) (\(+3.43\%\)) \\ _IN1k_ & _CC12M (120k)_ & \(7.25\) (\(+3.85\%\)) & \(25.69\) (\(+3.71\%\)) \\ \hline \end{tabular}
\end{table}
Table 4: Effect of pre-training across datasets and resolutions. Number of (pre-)training iterations given in thousands (k) per row. Relative improvements in FID and CLIP score given as percentage in parenthesis.

\begin{table}
\begin{tabular}{c c c c} \hline _Padding_ & \(\beta_{\text{int}}\) & _FID_ & _CLIP_ \\ \hline _zero_ & — & \(7.19\) & \(26.25\) \\ _replicate_ & \(0\) & \(6.93\) & \(26.47\) \\ _replicate_ & \(0.02\) & \(\mathbf{6.79}\) & \(\mathbf{26.60}\) \\ _replicate_ & \(0.05\) & \(6.82\) & \(26.58\) \\ _replicate_ & \(0.1\) & \(7.01\) & \(26.47\) \\ _replicate_ & \(0.2\) & \(7.02\) & \(26.41\) \\ \hline \end{tabular}
\end{table}
Table 3: Text padding. Our noisy replication embedding _vs_. baseline zero-padding. Models trained on CC12M@256.

Figure 6: Illustration of the impact of flip conditioning. Without the flip conditioning, the model may confuse left-right specifications. Including flipping as a control condition enables the model to properly follow left-right instructions.

Figure 7: Guidance scales. Left + center: The optimal guidance scale varies with the image resolution. Right: Decoupling the control guidance improves FID, the best reported performance is obtained with \(\beta=1.375\).

amount of training iterations. For ImageNet-22k, which is closer in distribution to ImageNet-1k than CC12M, the gains are even more significant, the finetuned model achieves an FID lower by \(0.5\) point after only 80k training iterations. In Tab. 3(b), we study the relative importance of pre-training vs finetuning when the datasets have dissimilar distributions but similar sample sizes. We fix a training budget in terms of number of training iterations \(N\), we first train our model on ImageNet-22k for \(K\) iterations before continuing the training on CC12M for the remaining \(N-K\) iterations. We see that the model pretrained for 200k iterations and finetuned for 150k performs better than the one spending the bulk of the training during pretraining phase. This validates the importance of domain specific training and demonstrates that the bulk of the gains from the pretrained checkpoint come from the representation learned during earlier stages.

**Resolution change.** We compare the performance boost obtained from training from scratch at \(512\) resolution _vs._ resuming from a \(256\)-resolution trained model. According to our results in Tab. 3(c), pretraining on low resolution significantly boosts the performance at higher resolutions, both for UNet and mmDiT, we find that higher resolution finetuning for short periods of time outperforms high resolution training from scratch by a large margin (\(\approx 25\%\)). These performance gains might in part be due to the increased batch size when pre-training the 256 resolution model, which allows the model to "see" more images as compared to training from-scratch at 512 resolution.

**Positional Embedding.** In Fig. 7(a), we compare the influence of the adjustment mechanism for the positional embedding. We find that our grid resampling approach outperforms the default extrapolation approach, resulting in 0.2 point difference in FID after 130k training iterations.

**Scaling the noise schedule.** We conducted an evaluation to ascertain the influence of the noise schedule by refining our mmDiT model post its low resolution training and report the results in Fig. 7(b). Remarkably, the application of the rectified schedule, for 40k iterations, resulted in an improvement of 0.7 FID points demonstrating its efficacy at higher resolutions.

**Pre-training cropping strategies.** During pretraining, the model sees objects that are smaller than what it sees during fine tuning, see Fig. 5. We aim to reduce this discrepancy by adopting more aggressive cropping during the pretraining phase. We experiment with three cropping ratios for training: \(0.9-1\) (global), \(0.4-0.6\) (local), \(0.4-1\) (mix). We report the results in Fig. 7(c). On ImageNet1K@256, the pretraining FID scores are \(2.36\), \(245.55\) and \(2.21\) for the local, global and mixed strategies respectively. During training at \(512\) resolution, we observe that the global and mix cropping strategies both outperform the local strategy. However, as reported in Fig. 7(c), the local strategy provides benefits at higher resolutions. Overall, training with the global strategy performs the best at \(256\) resolution but lags behind for higher resolution adaptation. While local cropping underperforms at lower resolutions, because it does not see any images in their totality, it outperforms the other methods at higher resolutions - an improvement of almost \(0.2\) FID points is consistent after the first \(50k\) training steps at higher resolution.

## 4 Conclusion

In this paper, we explored various approaches to enhance the conditional training of diffusion models. Our empirical findings revealed significant improvements in the quality and control over generated images when incorporating different coditioning mechanisms. Moreover, we conducted a

Figure 8: **Resolution shift. Experiments are conducted on ImageNet-1k at 512 resolution, FID is reported using 50 DDIM steps with respect to the ImageNet-1k validation set.**

comprehensive study on the transferability of these models across diverse datasets and resolutions. Our results demonstrated that leveraging pretrained representations is a powerful tool to improve the model performance while also cutting down the training costs. Furthermore, we provided valuable insights into efficiently scaling up the training process for these models without compromising performance. By adapting the schedulers and positional embeddings when scaling up the resolution, we achieved substantial reductions in training time while boosting the quality of the generated images. Additional experiments unveil the expected gains from different transfer strategies, making it easier for researchers to explore new ideas and applications in this domain. In Appendix B we discuss societal impact and limitations of our work.

## References

* [1] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In _CVPR_, 2023.
* [2] Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, and Filip Pavetic. FlexiViT: One model for all patch sizes. In _CVPR_, 2023.
* [3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In _ICLR_, 2019.
* [4] Marlene Careil, Matthew J. Muckley, Jakob Verbeek, and Stephane Lathuiliere. Towards image compression with perfect realism at ultra-low bitrates. In _ICLR_, 2024.
* [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _ICCV_, 2021.
* [6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In _CVPR_, 2021.
* [7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. PixArt-\(\alpha\): Fast training of diffusion transformer for photorealistic text-to-image synthesis. In _ICLR_, 2024.
* [8] Jaemin Cho, Abhay Zala, and Mohit Bansal. DALL-Eval: Probing the reasoning skills and social biases of text-to-image generation models. In _ICCV_, 2023.
* [9] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, and Devi Parikh. Emu: Enhancing image generation models using photogenic needles in a haystack. _arXiv preprint_, 2309.15807, 2023.
* [10] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In _ICLR_, 2024.
* [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In _ICCV_, 2009.
* [12] Prafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis. In _NeurIPS_, 2021.
* [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16\(\times\)16 words: Transformers for image recognition at scale. In _ICLR_, 2021.
* [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In _ICML_, 2024.
* [15] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. In _ICCV_, 2023.
* [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In _NeurIPS_, 2014.
* [17] Melissa Hall, Samuel J. Bell, Candace Ross, Adina Williams, Michal Drozdzal, and Adriana Romero Soriano. Towards geographic inclusion in the evaluation of text-to-image models. In _FAccT_, 2024.
* [18] Melissa Hall, Candace Ross, Adina Williams, Nicolas Carion, Michal Drozdzal, and Adriana Romero-Soriano. DIG in: Evaluating disparities in image generations with indicators for geographic diversity. _TMLR_, 2024.
* [19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In _NeurIPS_, 2017.

* Ho and Salimans [2021] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In _NeurIPS Workshop on Deep Generative Models and Downstream Applications_, 2021.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _NeurIPS_, 2020.
* Hoogeboom et al. [2023] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for high resolution images. In _ICML_, 2023.
* Kang et al. [2023] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up GANs for text-to-image synthesis. In _CVPR_, 2023.
* Karras et al. [2020] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In _CVPR_, 2020.
* Karras et al. [2021] Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In _NeurIPS_, 2021.
* Karras et al. [2022] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In _NeurIPS_, 2022.
* Karras et al. [2024] Tero Karras, Janne Hellsten, Miika Aittala, Timo Aila, Jaakko Lehtinen, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In _CVPR_, 2024.
* Kingma and Ba [2015] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.
* Kingma and Welling [2014] Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In _ICLR_, 2014.
* Lee et al. [2022] Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, and Ce Liu. ViTGAN: Training GANs with vision transformers. In _ICLR_, 2022.
* Levy et al. [2023] Mark Levy, Bruno Di Giorgi, Floris Weers, Angelos Katharopoulos, and Tom Nickson. Controllable music production with diffusion models and guidance gradients. _arXiv preprint_, 2311.00613, 2023.
* Lin et al. [2024] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In _WACV_, 2024.
* Lin et al. [2014] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. Zitnick. Microsoft COCO: common objects in context. In _ECCV_, 2014.
* Lipman et al. [2023] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In _ICLR_, 2023.
* Liu et al. [2024] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao Sun. Sora: A review on background, technology, limitations, and opportunities of large vision models. _arXiv preprint_, 2402.17177, 2024.
* Lu et al. [2022] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In _NeurIPS_, 2022.
* Luccioni et al. [2023] Alexandra Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. Stable bias: Analyzing societal representations in diffusion models. In _NeurIPS_, 2023.
* Peebles and Xie [2023] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _ICCV_, 2023.
* Podell et al. [2024] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In _ICLR_, 2024.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _ICML_, 2021.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.
* Ronneberger et al. [2015] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention_, 2015.

* [43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. _International Journal of Computer Vision (IJCV)_, 115(3):211-252, 2015.
* [44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In _NeurIPS_, 2022.
* [45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _ICLR_, 2021.
* [46] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _ICLR_, 2021.
* [47] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _ICML_, 2021.
* [48] A. Vahdat and J. Kautz. NVAE: A deep hierarchical variational autoencoder. In _NeurIPS_, 2020.
* [49] Chanyue Wu, Dong Wang, Yunpeng Bai, Hanyu Mao, Ying Li, and Qiang Shen. HSR-Diff: Hyperspectral image super-resolution via conditional diffusion models. In _ICCV_, 2023.
* [50] Tong Wu, Zhihao Fan, Xiao Liu, Yeyun Gong, Yelong Shen, Jian Jiao, Hai-Tao Zheng, Juntao Li, Zhongyu Wei, Jian Guo, Nan Duan, and Weizhu Chen. AR-Diffusion: Auto-regressive diffusion model for text generation. In _NeurIPS_, 2023.
* [51] Han Xue, Zhiwu Huang, Qianru Sun, Li Song, and Wenjun Zhang. Freestyle layout-to-image synthesis. In _CVPR_, 2023.
* [52] Cheng Yang, Lijing Liang, and Zhixun Su. Real-world denoising via diffusion model. _arXiv preprint_, 2305.04457, 2023.
* [53] Biao Zhang and Rico Sennrich. Root Mean Square Layer Normalization. In _NeurIPS_, 2019.
* [54] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _ICCV_, 2023.
* [55] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _CVPR_, 2018.
* [56] Sharon Zhou, Mitchell L. Gordon, Ranjay Krishna, Austin Narcomey, Durim Morina, and Michael S. Bernstein. HYPE: a benchmark for Human eYe Perceptual Evaluation of generative models. In _NeurIPS_, 2019.

## Appendix A Related work

**Diffusion Models.** Diffusion models have gained significant attention in recent years due to their ability to model complex stochastic processes and generate high-quality samples. These models have been successfully applied to a wide range of applications, including image generation [7, 21, 41], video generation [35], music generation [31], and text generation [50]. One of the earliest diffusion models was proposed in [21], which introduced denoising diffusion probabilistic models (DDPMs) for image generation. This work demonstrated that DDPMs can generate high-quality images that competitive with state-of-the-art generative models such as GANs [16]. Following this work, several variants of DDPMs were proposed, including score-based diffusion models [46], conditional diffusion models [12], and implicit diffusion models [45]. Overall, diffusion models have shown promising results in various applications due to their ability to model complex stochastic processes and generate high-quality samples [7, 14, 39, 41]. Despite their effectiveness, diffusion models also have some limitations, including the need for a large amount of training data and the required computational resources. Some works [26, 27] have studied and analysed the training dynamics of diffusion models, but most of this work considers the pixel-based models and small-scale settings with limited image resolution and dataset size. In our work we focus on the more scalable class of latent diffusion models [41], and consider image resolutions up to 512 pixels, and 14M training images.

**Model architectures.** Early work on diffusion models adopted the widely popular UNet architecture [39, 41]. The UNet is an encoder-decoder architecture where the encoder is made of residual blocks that produce progressively smaller feature maps, and the decoder progressively upsamples the feature maps and refines them using skip connections with the encoder [42]. For diffusion, UNets are also equipped with cross attention blocks for cross-modality conditioning and adaptive layer normalization that conditions the outputs of the model on the timestep [41]. More recently, vision transformer architectures [13] were shown to scale more favourably than UNets for diffusion models with the DiT architecture [38]. Numerous improvements have been proposed to the DiT in order to have more efficient and stable training, see _e.g_. [7, 14, 15]. In order to reduce the computational complexity of the model and train at larger scales, windowed attention has been proposed [7]. Latent masking during training has been proposed to encourage better semantic understanding of inputs in [15]. Others improved the conditioning mechanism by evolving the text tokens through the layers of the transformer and replacing the usual cross-attention used for text conditioning with a variant that concatenates the tokens from both the image and text modalities [14].

**Large scale diffusion training.** Latent diffusion models [41] unlocked training diffusion models at higher resolutions and from more data by learning the diffusion model in the reduced latent space of a (pre-trained) image autoencoder rather than directly in the image pixel space. Follow-up work has proposed improved scaling of the architecture and data [39]. More recently, attention-based architectures [7, 14, 15] have been adapted for large scale training, showing even more improvementsby scaling the model size further and achieving state-of-the-art performance on datasets such as ImageNet-1k. Efficiency gains were also obtained by [7] by transferring ImageNet pre-trained models to larger datasets.

## Appendix B Societal impact and limitations

Our research investigates the training of generative image models, which are widely employed to generate content for creative or education and accessibility purposes. However, together with these beneficial applications, these models are usually associated with privacy concerns (_e.g._, deepfake generation) and misinformation spread. In our paper, we deepen the understanding of the training dynamics of these modes, providing the community with additional knowledge that can be leveraged for safety mitigation. Moreover, we promote a safe and transparent/reproducible research by employing only publicly available data, which we further mitigate by blurring human faces.

Our work is mostly focused on training dynamics, and to facilitate reproducibility, we used publicly available datasets and benchmarks, without applying any data filtering. We chose datasets relying on the filtering/mitigation done by their respective original authors. In general, before releasing models like the ones described in our paper to the public, we recommend conducting proper evaluation of models trained using our method for bias, fairness and discrimination risks. For example, geographical disparities due to stereotypical generations could be revealed with methods described by Hall et al. [18], and social biases regarding gender and ethnicity could be captured with methods from Luccioni et al. [37] and Cho et al. [8].

While our study provides valuable insights into control conditioning and the effectiveness of representation transfer in diffusion models, there are several limitations that should be acknowledged. (i) There are cases where these improvements can be less pronounced. For example, noisy replicates for the text embeddings can become less pertinent if the model is trained exclusively with long prompts. (ii) While low resolution pretraining with local crops on ImageNet resulted in better FID at 512 resolution (see Table (c)c), it might not be necessary if pretraining on much larger datasets (_e.g._\(>\)100M samples, which we did not experiment in this work). Similarly, flip conditioning is only pertinent if the training dataset contains position sensitive information (left vs. right in captions, or rendered text in images), otherwise this condition will not provide any useful signal. (iii) We did not investigate the impact of data quality on training dynamics, which could have implications for the generalizability of our findings to datasets with varying levels of quality and diversity. (iv) As our analysis primarily focused on control conditioning, other forms of conditioning such as timestep and prompt conditioning were not explored in as much depth. Further research is needed to determine the extent to which these conditionings interact with control conditioning and how that impacts the quality of the models. (v) Our work did not include studies on other parts that are involved in the training and sampling of diffusion models, such as the different sampling mechanisms and training paradigms. This could potentially yield additional gains in performance and uncover new insights about the state-space of diffusion models.

## Appendix C Implementation details

Noise schedule scaling.In Fig. A1 we depict the rescaling of the noise for higher resolutions, following Eq. (3).

Computational costs.In Tab. A1 we compare the model size and computational costs of the different architectures studied in the main paper. All model architectures are based on the original implementations, but transposed to our codebase. Mainly, we use _bfloat16_ mixed precision and memory-efficient attention 2 from PyTorch.

Footnote 2: [https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)

Experimental details.To ensure efficient training of our models, we benchmark the most widely used hyperparameters and report the results of these experiments, which consist of the choice of the optimizer and its hyperparameters, the learning rate and the batch size. We then transpose the optimal settings to our other experiments. For FID evaluation, we use a guidance scale of \(1.5\) for \(256\) resolution and \(2.0\) for resolution \(512\). For evaluation on ImageNet-22k, we compute the FID score between 50k generated images and a subset of 200k images from the training set.

Training paradigm.We use the EDM [26] abstraction to train our models for epsilon prediction following DDPM paradigm. Differently from [15, 38], we do not use learned sigmas but follow a standard schedule. Specifically, we use a quadratic beta schedule with \(\beta_{\text{start}}=0.00085\) and \(\beta_{\text{end}}=0.012\). In DDPM [21], a noising step is formulated as follows, with \(x_{0}\) being the data sample and \(t\in[0,T]\) a timestep:

\[x_{t}=\sqrt{\bar{\alpha}_{t}}x_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon,\qquad \epsilon\sim\mathcal{N}(0,I), \tag{5}\]

while in EDM, the cumulative alpha products are converted to corresponding signal-to-noise ratio (SNR) proportions, the noising process is then reformulated as:

\[x_{t}=\frac{x_{0}+\sigma_{t}\epsilon}{\sqrt{1+\sigma_{t}^{2}}}, \tag{6}\]

and the loss is weighted by the inverse SNR \(\frac{1}{\sigma_{t}^{2}}\).

Scaling the training.A recurrent question when training deep networks is the coupling between the learning rate and the batch size. When multiplying the batch size by a factor \(\gamma\), some works recommend scaling the learning rate by the square root of \(\gamma\), while others scale the learning rate by the factor \(\gamma\) itself. In the following we experiment with training a class-conditional DiT model with different batch sizes and learning rates and report results after the same number of iterations.

From Tab. A2 we observe an improved performance by increasing the batch size and the learning rate in every learning rate setting. If the learning rate is too high the training diverges.

Influence of momentum.We conduct a grid search over the momentum parameters of Adam optimizer, similar to previous sections, we train a UNet model to 70k steps and compute FID with respect to the validation set of ImageNet1k using 50 DDIM steps. From Fig. A2, we can see that the default pytorch values \((\beta_{1}=0.9,\beta_{2}=0.999)\) are sub-optimal, resulting in an FID of 26.43 while the best performance is obtained when setting \((\beta_{1}=0.9,\beta_{2}=0.95)\) improves FID by 4.78 points in our experimental setting.

Setting the optimal guidance scale.**Proposition 2**.: _The optimal guidance scale \(\lambda\) scales with the upsampling factor \(s\) according to the law \(\lambda^{\prime}(s)=1+s\cdot(\lambda-1)\)._

This is verified in Fig. 7 where the \(\lambda^{\prime}(s=2)=1+2\cdot(1.5-1.0)=2.0\) which is the optimal guidance scale at \(512\) resolution according to the figure.

Text padding mechanism.In order to train at a large scale, most commonly used text encoders output a constant number of tokens \(T\) that are fed to the denoising model (usually \(T=77\)). Consequently, when the prompt has less than \(T\) words, the remaining tokens are padding tokens that do not contain useful information, but can still contribute in the cross-attention, see Figure A3. This raises the question of _whether better use can be made of padding text tokens to improve training performance and efficiency_. One common mitigation involves using recaptioning methods that provide longer captions. However, this creates an inconsistency between training and sampling as users are more likely to provide shorter prompts. Thus, to make better use of the conditioning vector, we explore alternative padding mechanisms for the text encoder. We explore a _'replicate'_ padding mechanism where the padding tokens are replaced with copies of the text tokens, thereby pushing the subsequent cross-attention layers to attend to all the tokens in its inputs. To improve the diversity of the feature representation across the sequence dimension, we perturb the embeddings with additive Gaussian noise with a small variance \(\beta_{\text{kt}}\). For shorter prompts with a high number of repeats \(m\), we scale the additive noise by \(\sqrt{m-1}\) to account for the reduction in posterior uncertainty induced by these repetitions:

\[\phi_{\text{kt}}=\phi_{\text{kt}}+\sqrt{m-1}\cdot\sigma_{\text{ch}}(\phi_{\text {kt}})\cdot\epsilon_{\text{kt}},\qquad\text{ with }\epsilon_{\text{kt}}\sim\mathcal{N}(0,I), \tag{7}\]

where \(\phi_{\text{ch}}\) is the standard deviation of the feature text embeddings over the feature dimension. See Figure A4 for an illustration comparing zero-padding, replication padding, and our noisy replication padding.

## Appendix D Derivations

### Derivation for Eq. (2)

Proof.: The formula can be obtained by iteratively applying the guidance across the conditions.

\[\epsilon^{\lambda,\beta} =\lambda\epsilon_{c}+(1-\lambda)\epsilon_{\emptyset} \tag{8}\] \[\epsilon^{\lambda,\beta} =\lambda\big{(}\beta\epsilon_{c,s}+(1-\beta)\epsilon_{c, \emptyset}\big{)}+(1-\lambda)\epsilon_{\emptyset,\emptyset} \tag{9}\]

### Derivation for Eq. (4)

Proof.: Assuming that the unconditional prediction \(\epsilon_{\emptyset}\) is distributed around the conditional distribution according to a normal law \(\epsilon_{\emptyset}|\epsilon_{c}\sim\mathcal{N}(\epsilon_{c},\delta^{2}I)\).

\[\epsilon_{\lambda} =\lambda c+(1-\lambda)(c+\delta\epsilon),\quad\epsilon\sim \mathcal{N}(0,I) \tag{10}\] \[\epsilon_{\lambda} =c+(1-\lambda)\delta\epsilon\] (11) \[\text{Var}(\epsilon_{\lambda}) =(1-\lambda)^{2}\delta^{2} \tag{12}\]

After upsampling by a scale factor of \(s\), the same low resolution patch has \(s^{2}\) observations, hence the variance is decreased by \(s^{2}\).

\[\text{Var}(\epsilon_{\lambda})_{hr}=(1-\lambda)^{2}\delta^{2}/s^{2} \tag{13}\]

Hence by equalizing the discrepancy between the conditional and unconditional predictions at low and high resolutions we obtain:

\[\text{Var}(\epsilon_{\lambda})_{s=1}=\text{Var}(\epsilon^{\prime}_{\lambda})_ {s}\implies(1-\lambda^{\prime})^{2}\delta^{2}/s^{2} =(1-\lambda)^{2}\delta^{2} \tag{14}\] \[\lambda^{\prime} =1+s\cdot(\lambda-1) \tag{15}\]

**Derivation for Eq. (3)**

Proof.: At timestep \(t\), the noisy observation is obtained as :

\[x_{t}=\frac{1}{\sqrt{1+\sigma_{t}^{2}}}\big{(}x_{0}+\sigma_{t}\epsilon\big{)}, \qquad\epsilon\sim\mathcal{N}(0,I) \tag{16}\]

Hence \(x_{0}\) can be estimated using the following formula:

\[x_{0}=\sqrt{1+\sigma_{t}^{2}}x_{t}-\sigma_{t}\epsilon \tag{17}\]

The statistics of the estimate of \(x_{0}\) are as follows.

\[\begin{cases}\mathbb{E}(x_{0})=\sqrt{1+\sigma_{t}^{2}}\mathbb{E}(x_{t})\\ \text{Var}(x_{0})=(1+\sigma_{t}^{2})\text{Var}(x_{t})+\sigma_{t}^{2}\end{cases} \tag{18}\]

Consequently, if we already have \(x_{t}\), we have an estimate of \(x_{0}\) with an error bound given by:

\[\text{Var}\big{(}x_{0}-\sqrt{1+\sigma^{2}}x_{t}\big{)}=\sigma_{t}^{2} \tag{19}\]

At higher resolutions, we have \(s^{2}\) corresponding observations for the same patch such that the error with respect to the estimate becomes:

\[\text{Var}\Big{(}x_{0}-\frac{1}{s^{2}}\sum_{i=1}^{s^{2}}\sqrt{1+\sigma_{t}^{2} }x_{t}^{(i)}\Big{)}=\sigma_{t}^{2}/s^{2} \tag{20}\]

Hence, if we want to keep the same uncertainty with respect to the low resolution patches, the following equality needs to be verified:

\[\sigma(t,s)=\sigma(t^{\prime},s^{\prime}) \implies\sigma_{t}/s=\sigma_{t}^{\prime}/s^{\prime} \tag{21}\] \[\implies\frac{1-\bar{\alpha}_{t}}{\bar{\alpha}_{t}}=(\frac{s}{s^{ \prime}})^{2}\cdot\frac{1-\bar{\alpha}_{t^{\prime}}}{\bar{\alpha_{t^{\prime}}}}\] (22) \[\implies\bar{\alpha}_{t^{\prime}}=\frac{s^{2}\cdot\bar{\alpha}_{t }}{s^{\prime 2}+\bar{\alpha}_{t}\cdot(s^{2}-s^{\prime 2})} \tag{23}\]

## Appendix E Additional results

Qualitative results.We provide additional qualitative examples on ImageNet-1k in Fig. A5.

[MISSING_PAGE_FAIL:20]

power-cosine profile outperforms these simpler schedules in both FID and LPIPS, improving image quality while better removing the unwanted distribution shift induced from choosing different samples during training.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline _Init._ & \(t\) _weighting_ & _FID_ (\(\downarrow\)) & _LPIPS_ (\(\downarrow\)) & _LPIPS_/_HR_ (\(\downarrow\)) \\ \hline — & _zero_ & \(3.29\) & — & — \\ _zero_ & _unif_ & \(\mathbf{3.08}\) & \(0.33\) & \(0.210\) \\ \hline _zero_ & _cos_(\(\alpha=1.0\)) & \(3.08\) & \(0.23\) & \(0.076\) \\ _zero_ & _cos_(\(\alpha=2.0\)) & \(3.09\) & \(0.18\) & \(0.045\) \\ _zero_ & _cos_(\(\alpha=4.0\)) & \(3.05\) & \(0.13\) & \(0.025\) \\ _zero_ & _cos_(\(\alpha=8.0\)) & \(\mathbf{3.04}\) & \(\mathbf{0.04}\) & \(\mathbf{0.009}\) \\ \hline _zero_ & _linear_ & \(3.13\) & \(0.07\) & \(0.041\) \\ \hline _zero_ & \(\delta(\sigma_{t}\leq 1.0)\) & \(3.15\) & \(0.09\) & \(0.046\) \\ _zero_ & \(\delta(\sigma_{t}\leq 2.0)\) & \(3.12\) & \(0.14\) & \(0.062\) \\ _zero_ & \(\delta(\sigma_{t}\leq 6.0)\) & \(3.09\) & \(0.26\) & \(0.170\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison of the power cosine schedule with other schedules. We report results for a linear and step function schedules.

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction clearly set the claims made in the paper and match theoretical and experimental results, which are addressed in Section 3. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We include discussion of limitations in Appendix B. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Proofs are provided Appendix D and contain cross-references to the relevant parts in the main paper. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We include details required for result reproducibility in Section 3.1. These include details about datasets used for training, evaluation metrics, training parameters, and model architectures. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: While we are not be able to include code, we do provide specific and thorough detailing of our training and evaluation methods in Section 3.1 to enable experimental reproduction. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Details corresponding to training and testing are included in Section 3.1. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Due to the computational cost of training and performing inference with generative models, we do not include error bars. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Information about compute resources needed for experiment reproduction is included in Section 3.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: This research conforms to the Code of Ethics, including mitigation of potential harms caused by the research process, considerations of social impact, and taking steps for impact mitigation. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss societal impacts and steps taken to mitigate potential harms in Appendix B.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not release models nor data in this work. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Use of existing assets is discussed, including which version, and cited throughout the paper. No assets are released. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not include crowdsourcing or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.