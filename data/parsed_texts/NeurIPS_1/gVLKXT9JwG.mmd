# Global Convergence Analysis of Local SGD for

Two-layer Neural Network without

Overparameterization

Yajie Bao\({}^{1}\)   Amarda Shehu\({}^{2}\)   Mingrui Liu\({}^{2}\)

\({}^{1}\)School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai, 200240

\({}^{2}\)Department of Computer Science, George Mason University, Fairfax, VA 22030

baoyajie2019stat@sjtu.edu.cn, {ashehu,mingruil}@gmu.edu

Corresponding Author.

###### Abstract

Local SGD, a cornerstone algorithm in federated learning, is widely used in training deep neural networks and shown to have strong empirical performance. A theoretical understanding of such performance on nonconvex loss landscapes is currently lacking. Analysis of the global convergence of SGD is challenging, as the noise depends on the model parameters. Indeed, many works narrow their focus to GD and rely on injecting noise to enable convergence to the local or global optimum. When expanding the focus to local SGD, existing analyses in the nonconvex case can only guarantee finding stationary points or assume the neural network is overparameterized so as to guarantee convergence to the global minimum through neural tangent kernel analysis. In this work, we provide the first global convergence analysis of the vanilla local SGD for two-layer neural networks _without overparameterization_ and _without injecting noise_, when the input data is Gaussian. The main technical ingredients of our proof are _a self-correction mechanism_ and _a new exact recursive characterization of the direction of global model parameters_. The self-correction mechanism guarantees the algorithm reaches a good region even if the initialization is in a bad region. A good (bad) region means updating the model by gradient descent will move closer to (away from) the optimal solution. The main difficulty in establishing a self-correction mechanism is to cope with the gradient dependency between two layers. To address this challenge, we divide the landscape of the objective into several regions to carefully control the interference of two layers during the correction process. As a result, we show that local SGD can correct the two layers and enter the good region in polynomial time. After that, we establish a new exact recursive characterization of the direction of global parameters, which is the key to showing convergence to the global minimum with linear speedup in the number of machines and reduced communication rounds. Experiments on synthetic data confirm theoretical results.

## 1 Introduction

Federated learning is a prevalent framework in distributed learning to significantly reduce the communication cost and effectively preserve the privacy of local clients [43, 27]. As the most popular algorithm in federated learning, local SGD has shown great empirical success in training deep neural networks (DNNs) [43, 39]. However, existing literature has not been able to fully explain or characterize the convergence of local SGD in training DNNs. Recently, extensive works are devoted to analyzing the convergence of local SGD and its variants in nonconvex optimization [39, 56, 20, 28,32, 40]. However, traditional nonconvex analysis only guarantees convergence to a stationary point, and convergence to the global minimum is in general NP-hard [21].

Despite the NP-hardness results for nonconvex optimization, an increasing body of research tries to address structured nonconvex optimization by first-order methods with noise injection. For instance, Ge et al. [17] considered strict-saddle functions and showed that SGD with isotropic noise can find local minima in polynomial time. This motivated several ensuing works [25, 33, 26, 55, 1] on designing different first-order algorithms to improve convergence to local minima by injecting noise. Noise-injecting schemes and their variants (such as, for instance, broadening from isotropic to anisotropic noise [64]) were shown to help convergence to global minima for many problems that satisfy one of two conditions: (i) local minima are global minima, as in matrix completion [18], dictionary learning [48], and certain deep linear networks [29]; or (ii) neural networks with distributional assumption, such as two-layer neural networks with the Gaussian input [63]. It is worth noting that there is also a rich history on noisy GD based on Langevin dynamics (LD) [30, 44, 52, 8, 41, 9]. For instance, recent work [6] proposes Exponential Family Langevin Dynamics (EFLD) to relax the Gaussian noise assumption and include noisy sign-SGD and variants of drop-out as special cases. When assuming the neural network is overparameterized, neural tangent kernel (NTK) analysis [23] guarantees convergence to the global minimum for local SGD [23, 22, 58, 10]. However, the NTK theory is far from sufficient, since neural networks outperform their NTK counterpart in practice [4] and in theory [38].

Despite existing global convergence analyses of first-order methods for solving nonconvex optimization problems such as neural networks, they either require explicitly injecting noise or assume overparameterization such that NTK analysis can apply. Practical federated learning algorithms such as local SGD do not inject any noise and do not belong to the NTK regime, but they can still converge to global minima. For example, McMahan et al. [43] shows that the local SGD algorithm can achieve around \(99\%\) accuracy when training neural networks for an image classification task. This motivates us to study the following question in this paper:

**Is it possible to formally prove that the local SGD algorithm can find global minima for two-layer neural networks without injecting noise and without overparameterization?**

In this paper, we give a positive answer to this question under Gaussian input. We are inspired by a line of work on neural network learning theory with Gaussian input [7, 49, 15, 37, 61, 13] and, in particular address the distributed version of the setting in [13]. Suppose \(N\) local machines share the following network:

\[f(\mathbf{Z},\mathbf{w},\mathbf{a})=\sum_{j=1}^{k}a_{j}\sigma(\mathbf{Z}_{i}^{\top}\bm {w}),\]

where \(\mathbf{Z}=(\mathbf{Z}_{1},\cdots,\mathbf{Z}_{k})\in\mathbb{R}^{d\times k}\) is the input matrix, \(\mathbf{w}\in\mathbb{R}^{d}\) is the weight, \(\mathbf{a}\in\mathbb{R}^{k}\) is the output weight, and \(\sigma(x)=\max\{x,0\}\) denotes the ReLU activation function. A good property of this network is positive homogeneity, i.e., \(f(\mathbf{Z},c\mathbf{w},\mathbf{a}/c)=f(\mathbf{Z},\mathbf{w},\mathbf{a})\) holds for any \(c>0\). We assume each entry of the input \(\mathbf{Z}\) is independently sampled from a standard Gaussian distribution. Then the response is generated by a noiseless teacher network: \(y=f(\mathbf{Z},\mathbf{w}^{*},\mathbf{a}^{*})\). Without loss of generality, we further assume \(\|\mathbf{w}^{*}\|=1\). We hope to learn a student network by collaboratively minimizing the following mean square loss among \(N\) local machines:

\[L(\mathbf{w},\mathbf{a})=\frac{1}{2}\mathbb{E}\left[\left(y-f(\mathbf{Z},\mathbf{w},\mathbf{a} )\right)^{2}\right]=\frac{1}{2}\mathbb{E}\left[\left(f(\mathbf{Z},\mathbf{w}^{*}, \mathbf{a}^{*})-f(\mathbf{Z},\mathbf{w},\mathbf{a})\right)^{2}\right]. \tag{1}\]

Obviously, \((\mathbf{w}^{*},\mathbf{a}^{*})\) is the global minimum of the objective (1) with zero loss. In particular, the loss function also has a spurious local minimum; hence, minimizing loss is a nonconvex optimization problem. Please refer to work in [13] for further details on the landscape of \(L(\mathbf{w},\mathbf{a})\).

Despite the special input distribution, to the best of our knowledge, there is currently no work demonstrating the global convergence of vanilla SGD or vanilla local SGD without overparameterization. Work in [13] proved that randomly initialized GD can converge to the global minimum or the local minimum with a constant probability. The initial region where GD can converge to the global minimum is also called the attraction basin, where the gradients of two layers both point in the correct directions to the ground truth. To obtain the global convergence with arbitrary initialization in the same initial region, Zhou et al. [63] proposed a new perturbed GD algorithm by carefully injecting noise to the weights in two layers. Although the convergence of vanilla SGD has not been exploredtheoretically, the simulation results in [13; 63] show that vanilla SGD with random initialization can converge to the global minimum with probability \(1\) when the ratio \(|\mathbf{1}^{\top}\mathbf{a}^{*}|/\|\mathbf{a}^{*}\|\) is large. This result motivates us to investigate the global convergence of local SGD without injecting additional noise to escape the local minimum.

In this paper, we analyze the global convergence of the vanilla local SGD for training a two-layer neural network with Gaussian input, whose initialization starts from the same initial region as in [13; 63]. Formally, our main contributions are summarized as follows:

1. We introduce a new self-correction mechanism of local SGD under a condition on \(\mathbf{a}^{*}\) (see Assumption 1): the signals from two layers can be corrected in _polynomial_ time even though the initial point comes from a bad region where the gradients point to the wrong direction of \((\mathbf{w}^{*},\mathbf{a}^{*})\). The condition also explains the simulation results in [13]. The self-correction process is very difficult to analyze due to the mutual influence effect of two layers. To address this challenge, we utilize a novel technique by carefully dividing the landscape of the objective (1) into several regions. In each region, the negative effect from one layer to another layer can be controlled to a negligible scale. We notice that Li and Yuan [37] also showed the self-correction phase of SGD for the two-layer network under Gaussian input. However, the network's structure in [37] is different from the network studied in this paper. In addition, Li and Yuan [37] also required bounding the noise of stochastic gradient and so cannot handle the vanilla SGD with Gaussian noise as in our case.
2. We show the global convergence of local SGD with linear speedup, which indicates that the iteration complexity is divided by the number of machines \(N\). In addition, we also show that the communication complexity of local SGD is reduced compared with the naive parallel version of SGD which needs to communicate at every iteration. The analysis in the convergence stage is very different from the GD in Du et al. [13]. We establish a new recursive dynamic to characterize the direction of the _global weight_ in the first layer. Moreover, the objective is not smooth, since the gradients incorporate the angle between the first layer's weight and the ground truth. Therefore, conventional analysis of local SGD for a general smooth objective [47; 54] cannot be applied in the convergence stage. Due to the inner structure of gradients under Gaussian input, we find that the discrepancy caused by the local updates can shrink as the angle decreases, which enables us to refine the bound of discrepancy to be dominated by the statistical bound of noise.
3. We conduct several simulations on the two-layer neural network to verify the theoretical results. The experiments demonstrate that local SGD indeed corrects the wrong signals from the initial point and exhibits speedup in the convergence stage, corroborating our theoretical results. The simulation results also verify that the condition imposed on \(|\mathbf{1}^{\top}\mathbf{a}^{*}|/\|\mathbf{a}^{*}\|\) is almost necessary to show the convergence with almost arbitrary initialization.

## 2 Related Work

Federated OptimizationThere is a wave of studies on federated optimization in different settings. In the convex optimization setting with homogeneous data, one-shot averaging was studied [65; 42; 60], where each machine solves a local optimization problem and the average happens only at the last iterate. Local SGD skips communication rounds, and the convergence analysis is shown for convex [47; 11; 34; 28; 54; 53; 32; 31] and nonconvex optimization problems [62; 24; 51; 39; 20; 56; 34; 28; 45; 59; 32]. There is a line of work which tried to compare minibatch SGD and local SGD in federated learning [54; 53]. However, these optimization algorithms only work for black-box functions and do not utilize the property of neural networks. As neural network loss landscapes are typically nonconvex, these federated optimization algorithm can only guarantee to find a stationary point instead of a global minimum.

Optimization Theory for Neural NetworksThere is a line of work studying two layer neural networks with Gaussian input [49; 12; 37; 61; 7; 19; 5]. Li and Liang [36] studied two layer neural networks with cross-entropy loss and showed that SGD can find the global minimum when the neural network is overparameterized. Du et al. [16] proved that GD can find the global minimum for two layer overparameterized neural networks under \(\ell_{2}\) loss. These results are later extended to deep neural networks by [14; 3; 66; 2] but are not directly applicable to analyzing local SGD in the distributed setting.

Federated Learning on Neural NetworksThere is a line of work which studied federated learning algorithms on overparameterized neural networks under the NTK regime [35; 22; 10; 58; 57]. In contrast, our analysis does not fall in the NTK regime: we directly study the dynamics of local SGD over neural networks without overparametrization.

## 3 Notations and Problem Setup

Denote \(\|\cdot\|\) the Euclidean norm and \(\langle\cdot,\cdot\rangle\) by the inner product. \(\mathbb{S}^{d-1}\) denotes the \(d\)-dimensional unit sphere and \(\mathbb{B}^{k}(\rho)\) denotes the \(k\)-dimensional ball with center zero and radius \(\rho\). For two vectors \(\mathbf{v},\mathbf{w}\in\mathbb{R}^{d}\), denote \(\angle(\mathbf{v},\mathbf{w})\in[0,\pi]\) the angle between \(\mathbf{v}\) and \(\mathbf{w}\). The uniform distribution is denoted by \(\mathrm{Unif}(\cdot)\). Moreover, we use \(\tilde{O}\) to hide logarithmic factors.

We adopt the weight-normalization technique [46] to the first layer by re-parametrizing \(\mathbf{w}=\mathbf{v}/\|\mathbf{v}\|\), which leads to the following prediction model:

\[f(\mathbf{Z},\mathbf{v},\mathbf{a})=\sum_{j=1}^{k}a_{j}\frac{\sigma(\mathbf{Z}_{i}^{ \top}\mathbf{v})}{\|\mathbf{v}\|}. \tag{2}\]

Given any sample \((\mathbf{Z},y)\), we denote the empirical loss by

\[\ell(\mathbf{v},\mathbf{a};\mathbf{Z},y)=\frac{1}{2}(y-f(\mathbf{Z},\mathbf{v},\mathbf{a}))^{2 }=\frac{1}{2}\left(f(\mathbf{Z},\mathbf{w}^{*},\mathbf{a}^{*})-f(\mathbf{Z},\mathbf{v},\bm {a})\right)^{2}. \tag{3}\]

In the distributed environment, suppose there are \(N\) local machines sharing the same teacher model \(f(\mathbf{Z},\mathbf{w}^{*},\mathbf{a}^{*})\). It means that given any local input \(\mathbf{Z}^{i}\), the response is \(y^{i}=f(\mathbf{Z}^{i},\mathbf{w}^{*},\mathbf{a}^{*})\). Let \(\mathcal{I}=\{t_{0},...,t_{R}\}\) be the set of synchronization time, where \(t_{0}=0\), \(t_{R}=T\) and \(t_{r+1}-t_{r}=I\) for any \(r\). The detailed procedure of local SGD is presented in Algorithm 1, where the initial point is from the same region in Du et al. [13] and Zhou et al. [63]: \(\mathbf{v}_{0}\in\mathbb{S}^{d-1}\) and \(\mathbf{a}_{0}\in\mathbb{B}^{k}(|\mathbf{1}^{\top}\mathbf{a}^{*}|/\sqrt{k})\). In each round, the \(i\)-th machine runs \(I\) steps of SGD using the stochastic gradients \(\nabla_{\mathbf{v}}\ell(\mathbf{v}_{t}^{i},\mathbf{a}_{t}^{i};\mathbf{Z}_{t}^{i},y_{t}^{i})\) and \(\nabla_{\mathbf{a}}\ell(\mathbf{v}_{t}^{i},\mathbf{a}_{t}^{i};\mathbf{Z}_{t}^{i},y_{t}^{i})\) computed by the local input \((\mathbf{Z}_{t}^{i},y_{t}^{i})\). At the end of a round, the server aggregates local weights to obtain the global weight and then synchronizes the global weight to each machine.

```  Initialize \(\mathbf{v}_{0}\in\mathbb{S}^{d-1}\) and \(\mathbf{a}_{0}\in\mathbb{B}^{k}\left(\frac{|\mathbf{1}^{\top}\mathbf{a}^{*}|}{\sqrt{k}}\right)\). for\(r=0,\dots,R-1\)do for\(i=1,\dots,N\)do  Synchronization: \(\mathbf{a}_{t_{r}}^{i}\leftarrow\mathbf{a}_{t_{r}}\) and \(\mathbf{v}_{t_{r}}^{i}\leftarrow\mathbf{v}_{t_{r}}\). for\(t=t_{r},\dots,t_{r+1}-1\)do  Sample \(\mathbf{Z}_{t}^{i}\) from the standard Gaussian distribution and generate the response \(y_{t}^{i}\).  Update \(\mathbf{a}_{t+1}^{i}=\mathbf{a}_{t_{t}}^{i}-\eta\nabla_{\mathbf{a}}\ell(\mathbf{v}_{t}^{i},\bm {a}_{t}^{i};\mathbf{Z}_{t}^{i},y_{t}^{i})\).  Update \(\mathbf{v}_{t+1}^{i}=\mathbf{v}_{t}^{i}-\eta\nabla_{\mathbf{v}}\ell(\mathbf{v}_{t}^{i},\mathbf{a} _{t}^{i};\mathbf{Z}_{t}^{i},y_{t}^{i})\). endfor endfor  Update \(\mathbf{a}_{t_{r+1}}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{a}_{t_{r+1}}^{i}\) and \(\mathbf{v}_{t_{r+1}}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{vAccording to Algorithm 1, the updates of auxiliary sequences can be written as

\[\mathbf{v}_{t+1}=\mathbf{v}_{t}-\eta\frac{1}{N}\sum_{i=1}^{N}\mathbf{P}_{t}^{i}\nabla_{\bm {v}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i};\mathbf{Z}_{t}^{i}),\quad\mathbf{a}_{t+1}=\mathbf{a }_{t}-\eta\frac{1}{N}\sum_{i=1}^{N}\nabla_{\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{ i};\mathbf{Z}_{t}^{i}).\]

For ease of technical presentation, we denote the averaged noise terms by \(\mathbf{\xi}_{t}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{P}_{t}^{i}\mathbf{\xi}_{t}^{i}\) and \(\mathbf{\epsilon}_{t}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{\epsilon}_{t}^{i}\), where \(\mathbf{\xi}_{t}^{i}=\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i};\mathbf{Z}_{t} ^{i})-\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\) and \(\mathbf{\epsilon}_{t}^{i}=\nabla_{\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i};\mathbf{Z} _{t}^{i})-\nabla_{\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\) are the local noises in stochastic gradients. In addition, for the iterates \(\mathbf{v}_{t}^{i}\) and \(\mathbf{v}_{t}\), we write \(\phi_{t}^{i}=\angle(\mathbf{v}_{t}^{i},\mathbf{w}^{*})\) and \(\phi_{t}=\angle(\mathbf{v}_{t},\mathbf{w}^{*})\), respectively.

### Exact Dynamic of Each Layer

In this subsection, we will give the exact dynamic of each layer in the training process of local SGD, which is the starting point of our analysis. Denote \(\mathbf{P}_{t}=\frac{1}{\|\mathbf{v}_{t}\|}(\mathbf{I}-\mathbf{w}_{t}\mathbf{w}_{t}^{\top})\) by the global projection matrix, where \(\mathbf{w}_{t}=\mathbf{v}_{t}/\|\mathbf{v}_{t}\|\). The proofs of this subsection are deferred to Appendix A.1.

**Lemma 1**.: _Let \(\tilde{\mathbf{v}}_{t+1}=\mathbf{v}_{t}-\eta\left(\mathbf{P}_{t}\frac{1}{N}\sum_{i=1}^ {N}\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})+\mathbf{\xi}_{t}\right)\) and \(\hat{\phi}_{t}=\angle(\tilde{\mathbf{v}}_{t},\mathbf{w}^{*})\). The first layer in local SGD satisfies that_

\[\|\mathbf{v}_{t+1}\|^{2}\sin^{2}\phi_{t+1}=(1-\eta\lambda_{t}\cos\phi_{t})^{2}\left\| \mathbf{v}_{t}\right\|^{2}\sin^{2}\phi_{t}-2\eta M_{1,t}+\eta^{2}M_{2,t}+H_{t}, \tag{4}\]

_where \(\lambda_{t}=\frac{1}{N}\sum_{i=1}^{N}\frac{\pi-\phi_{t}^{i}\left(\mathbf{a}_{t}^{i }\right)^{\top}\mathbf{a}^{*}}{2\pi}\frac{\mathbf{a}_{t}^{i}\right)^{\top}\mathbf{a}^{*}}{ \|\mathbf{v}_{t}\|^{2}}\), \(H_{t}=\|\mathbf{v}_{t+1}\|^{2}\sin^{2}\phi_{t+1}-\|\tilde{\mathbf{v}}_{t+1}\|^{2}\sin^{2 }\tilde{\phi}_{t+1}\) and_

\[M_{1,t}=\left(\mathbf{v}_{t}-\eta\mathbf{P}_{t}\frac{1}{N}\sum_{i=1}^{N}\nabla_{\bm {w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\right)^{\top}\left(\mathbf{I}-\mathbf{w}^{*}( \mathbf{w}^{*})^{\top}\right)\mathbf{\xi}_{t},\;M_{2,t}=\mathbf{\xi}_{t}^{\top}\left(\mathbf{ I}-\mathbf{w}^{*}(\mathbf{w}^{*})^{\top}\right)\mathbf{\xi}_{t}.\]

Lemma 1 is crucial to control the angle and show the linear speedup in further analysis. For each local machine, Lemma 5.5 in [13] provides the dynamic of \(\sin^{2}\phi_{t}^{i}\), which cannot characterize the dynamic of global quantity \(\sin\phi_{t}\) due to the nonlinearity. Here we introduce a new _intermediate variable_\(\tilde{\mathbf{v}}_{t+1}\) and find an equality (4) to show the recursive relation between \(\|\mathbf{v}_{t+1}\|^{2}\sin^{2}\phi_{t+1}\) and \(\|\mathbf{v}_{t}\|^{2}\sin^{2}\phi_{t}\) through the global quantities, such as \(M_{1,t}\) and \(M_{2,t}\). It is worthwhile noticing that \(M_{1,t}\) is the averaged noise in local SGD, whose variance is divided by the number of clients \(N\), namely _linear speedup term_. In fact, we can control the dynamic of \(\sin\phi_{t}\) by upper bounding the _discrepancy term_\(H_{t}\). Let us assume the last three terms in (4) are negligible and \((\mathbf{a}_{t}^{i})^{\top}\mathbf{a}^{*}>0\) for any \(i\in[N]\):

1. When \(\phi_{t}>\pi/2\), \(\|\mathbf{v}_{t}\|^{2}\sin^{2}\phi_{t}\) will continuously increase since \(\lambda_{t}\cos\phi_{t}<0\). It indicates that \(\phi_{t}\) can decrease to \(\pi/2\) if \(\|\mathbf{v}_{t}\|\) is upper bounded by a constant, which also means the first layer can be corrected and avoided converging to the spurious local minima.
2. When \(\phi_{t}<\pi/2\), \(\|\mathbf{v}_{t}\|^{2}\sin^{2}\phi_{t}\) will continuously decrease to zero. It indicates that \(\phi_{t}\) can converge to zero if \(\|\mathbf{v}_{t}\|\) is lower bounded by a constant. The initial region with \(\phi_{0}<\pi/2\) and \(\mathbf{a}_{0}^{\top}\mathbf{a}^{*}>0\) is also called the attraction basin in [13].

Through the remarks above, we can see that the _positive_ signal of the second layer is crucial to both the self-correction of the first layer and global convergence. Next lemma presents an exact dynamic of the averaged weight in the second layer.

**Lemma 2**.: _Let \(A_{0}=|\mathbf{1}^{\top}\mathbf{a}^{*}|^{2}-(\mathbf{1}^{\top}\mathbf{a}^{*})(\mathbf{1} ^{\top}\mathbf{a}_{0})\) and \(g(\phi)=(\pi-\phi)\cos\phi+\sin\phi\). For local SGD algorithm started with the initial point \((\mathbf{v}_{0},\mathbf{a}_{0})\), the second layer satisfies that_

\[\mathbf{a}_{t}^{\top}\mathbf{a}^{*}= \left(1-\eta\frac{\pi-1}{2\pi}\right)^{t}\mathbf{a}_{0}^{\top}\mathbf{a} ^{*}+\frac{1-\left(\frac{2\pi-\eta(k+\pi-1)}{2\pi-\eta(\pi-1)}\right)^{t}}{k} \left(1-\eta\frac{\pi-1}{2\pi}\right)^{t}A_{0}\] \[+\frac{\eta\|\mathbf{a}^{*}\|^{2}}{2\pi}\sum_{s=0}^{t-1}\left(1-\eta \frac{\pi-1}{2\pi}\right)^{t-1-s}\frac{1}{N}\sum_{i=1}^{N}B_{s}^{i}+S(\mathbf{ \epsilon}_{0:t-1}), \tag{5}\]

_where \(S(\mathbf{\epsilon}_{0:t-1})\) (defined in Appendix A) is the noise term involving \(\mathbf{\epsilon}_{0},\cdots,\mathbf{\epsilon}_{t-1}\) and_

\[B_{s}^{i}=g(\phi_{s}^{i})-1+\frac{\eta}{2\pi}\frac{|\mathbf{1}^{\top}\mathbf{a}^{*}|^ {2}}{\|\mathbf{a}^{*}\|^{2}}\sum_{l=0}^{s-1}\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s -1-l}(\pi-g(\phi_{l}^{i})).\]Notice that the first term \(g(\phi_{s}^{i})-1\) in \(B_{s}^{i}\) is negative whenever \(\phi_{s}^{i}<\pi/2\), but the second term in \(B_{s}^{i}\) is always positive for \(\phi_{s}^{i}\in(0,\pi]\). In particular, \(\pi-g(\phi_{t}^{i})\) tends to be larger when \(\phi_{t}^{i}\) gets closer to \(\pi\) (i.e., \(\mathbf{w}_{t}^{i}\) drifts away from \(\mathbf{w}^{*}\)). This insight provides a possibility that local SGD can correct the signal of the second layer by itself, instead of injecting additional noise like [63].

Through carefully inspecting the ingredients of dynamics, we have the following roadmap to show the global convergence of local SGD: (1) For arbitrary initialization \((\mathbf{v}_{0},\mathbf{a}_{0})\) except for a measure zero set, where the angle of the first layer between initialization and the global minimum is \(\pi\), show that \(\mathbf{a}_{t}^{\top}\mathbf{a}^{*}\) can turn to the positive signal in polynomial time; (2) Show that \(\mathbf{a}_{t}^{\top}\mathbf{a}^{*}\) can be lower bounded by a positive constant value and \(\phi_{t}\) will decrease below \(\pi/2\). (3) After entering the attraction basin, show that \((\mathbf{w}_{t},\mathbf{a}_{t})\) will converge to the ground truth with a linear speedup guarantee.

### Self-correction of Signals in Two Layers

In this subsection, we will show the iterates of local SGD can enter the attraction basin such that \(\mathbf{a}_{t}^{\top}\mathbf{a}^{*}>0\) and \(\phi_{t}<\pi/2\) after \(\widetilde{O}(\eta^{-1})\) steps. Before that, we introduce an assumption on \(\mathbf{a}^{*}\).

**Assumption 1**.: _Define \(\alpha=|\mathbf{1}^{\top}\mathbf{a}^{*}|^{2}/(k\|\mathbf{a}^{*}\|^{2})\). We assume \(k\geq 320(\pi-1)^{2}\) and the ground truth satisfies \(|\mathbf{1}^{\top}\mathbf{a}^{*}|^{2}\leq\frac{k(\pi+k-1)}{720\pi\log(4+k^{2})}\) and_

\[\alpha>\frac{\pi+k-1}{(\pi-1)k}\left(1-\frac{32(\pi-1)}{k}\right)^{-1}. \tag{6}\]

The conditions on \(k\) and \(|\mathbf{1}^{\top}\mathbf{a}^{*}|^{2}\) are imposed for technical reasons. We believe that a constant lower bound (the right-hand side of (6)) for \(\alpha\) is necessary to show the global convergence with almost arbitrary initialization, which is also verified by our simulation results in Table 1. We compute convergence probabilities of local SGD and minibatch SGD under different values of \(\alpha\). The initial points are randomly selected by \(\mathbf{v}_{0}\sim\mathrm{Unif}(\mathbb{S}^{d-1})\) and \(\mathbf{a}_{0}\sim\mathrm{Unif}(\mathbb{B}^{k}(|\mathbf{1}^{\top}\mathbf{a}^{*}|/\sqrt {k}))\) in each trial. If the convergence probability reaches 1, it means that local SGD or minibatch SGD can converge to the global minima with arbitrary initialization except for a measure zero set. When \(\alpha\leq 1/8\), both minibatch SGD and local SGD _cannot_ converge to the global minima with probability 1.

**Theorem 1** (Self-correction of the second layer).: _For any initial point \((\mathbf{v}_{0},\mathbf{a}_{0})\) with \(\phi_{0}\in[0,\pi)\), we denote \(\tau_{a}=\inf\{t\geq 0:\mathbf{a}_{t}^{\top}\mathbf{a}^{*}\geq\gamma_{a}\}\) the first time, where \(\gamma_{a}=\frac{16(\pi-1)|\mathbf{1}^{\top}\mathbf{a}^{*}|^{2}}{k(\pi+k-1)}\). Under Assumption 1, if_

\[ck^{2}\sqrt{\log(Ndk/\delta)}\max\left\{\eta(\sqrt{Ik}+I),\eta(\sqrt{Id}+I), \sqrt{\eta k\over N}\right\}\frac{\|\mathbf{a}^{*}\|^{2}}{|\mathbf{1}^{\top}\mathbf{a }^{*}|^{2}}<1, \tag{7}\]

_for a sufficiently large constant \(c\), then \(\tau_{a}\leq O(\eta^{-1}\log k)\) with probability at least \(1-\delta\)._

This theorem completes the first step of the roadmap, the self-correction of the signal in the second layer, whose proof is given in Appendix D.1. If \(|\mathbf{1}^{\top}\mathbf{a}^{*}|^{2}/\|\mathbf{a}^{*}\|^{2}\leq 1/\mathrm{poly}(\rho)\), Du et al. [13] proved GD can converge to the spurious local minima with the initial condition \(\mathbf{a}_{0}^{\top}\mathbf{a}^{*}<0\) and \(\phi_{0}>\pi/2\). Therefore, our results do not contradict theirs because we assume \(|\mathbf{1}^{\top}\mathbf{a}^{*}|^{2}/\|\mathbf{a}^{*}\|^{2}\) is large. The simulations of [13; 63] show that the success probability of converging to the global optimum of SGD increases as the ratio \(|\mathbf{1}^{\top}\mathbf{a}^{*}|/\|\mathbf{a}^{*}\|\) increasing. Theorem 1 can potentially explain this phenomenon since the condition (6) will be satisfied eventually when \(|\mathbf{1}^{\top}\mathbf{a}^{*}|/\|\mathbf{a}^{*}\|\) keeps increasing.

**Proof sketch of Theorem 1.** The proof is very technical since the angle \(\phi_{t}^{i}\) will affect the sign of \(B_{s}^{i}\) in the dynamic (5) of \(\mathbf{a}_{t}^{\top}\mathbf{a}^{*}\), while controlling \(\phi_{t}^{i}\) also requires bounding the scale and controlling

[MISSING_PAGE_FAIL:7]

### Convergence with Linear Speedup

With the correction guarantees in the previous subsection, we are ready to proceed with the convergence analysis of local SGD.

**Theorem 3**.: _Suppose the initial point \((\mathbf{v}_{0},\mathbf{a}_{0})\) satisfies \(\mathbf{a}_{0}^{\top}\mathbf{a}^{*}\geq\gamma_{a}/32\) and \(\phi_{0}\leq\tilde{\phi}^{l}\). For any \(\epsilon>0\), we choose \(\eta=\frac{1}{ck^{2}\|\mathbf{a}^{*}\|^{2}}\frac{N_{t}}{d\log(dN/\epsilon\delta)}\) for some absolute constant \(c>0\). If \(I\lesssim\frac{d}{\epsilon N}\min\left\{1,\frac{k^{2}d^{1/2}}{N^{1/2}},\frac{k^ {4}d}{N^{4}\epsilon}\right\}\) and \(\epsilon<\min\{d^{-1},dk^{-2}\}\), then \(\ell(\mathbf{v}_{T},\mathbf{a}_{T})\lesssim\epsilon\) holds with probability at least \(1-\delta\) where \(T=\widetilde{O}\left(\frac{dk^{4}}{N\epsilon}\right)\)._

We have the following implications about the result in Theorem 3:

1. To the best of our knowledge, this is the first convergence result with linear speedup on the number of machines \(N\) for two-layer neural networks. Besides, our convergence analysis does not rely on the overparameterization of the width of the second layer (i.e., \(k\)).
2. The dependency on \(\epsilon\) matches the best-known results of local SGD for strongly convex objective in Woodworth et al. [53]. In fact, the size of the first layer \(d\) resembles the variance of stochastic gradients \(\sigma^{2}\) in the traditional optimization literature. We can show \(\langle\nabla_{\mathbf{a}}L(\mathbf{w}_{t},\mathbf{a}_{t}),\mathbf{a}_{t}-\mathbf{a}^{*}\rangle\geq \frac{\pi-1}{2\pi}\|\mathbf{a}_{t}-\mathbf{a}^{*}\|^{2}-O(\phi_{t})\|\mathbf{a}^{*}\|^{2}\). Therefore, when the first layer converges has converged (\(\phi_{t}\approx 0\)), \(\mathbf{a}_{t}\) can converge to \(\mathbf{a}^{*}\) like the strongly convex regime.
3. According to the condition on \(I\), we can obtain the communication complexity in the convergence stage as \[R_{\text{conv}}=\frac{T}{I}=\widetilde{O}\left(\max\left\{k^{4},k^{2}\sqrt{ \frac{N}{d}},\frac{N^{2}\epsilon}{d}\right\}\right).\] (11) When \(N\lesssim\min\{dk^{4},\epsilon^{-1}\}\), the communication complexity in this stage can be \(R_{\text{conv}}=\widetilde{O}(k^{4})\), which is significantly reduced compared with the iteration complexity.

We show local SGD's convergence layer by layer. Next lemma ensures that the weight of the first layer can converge to the ground truth in polynomial time.

**Lemma 4** (Convergence of the first layer).: _Under the settings in Theorem 3. Suppose the initial point satisfies \(\phi_{0}\leq\tilde{\phi}^{l}\) and \(\mathbf{a}_{0}^{\top}\mathbf{a}^{*}\geq\gamma_{a}/32\). With probability at least \(1-\delta\), we can guarantee that \(\sin^{2}\phi_{t}\lesssim\epsilon\) holds for any \(\widetilde{O}\left(k^{2}\eta^{-1}\right)\leq t\leq\widetilde{O}(\eta^{-2})\)._

It is worthwhile noticing that \((\mathbf{v}_{\tau_{v}},\mathbf{a}_{\tau_{v}})\) in Theorem 2 satisfies the conditions for initial point in Lemma 4. Therefore, local SGD enters the attraction basin after finishing the self-correction process. In fact, showing the complexity \(\widetilde{O}(\epsilon^{-1})\) is not trivial based on the traditional analysis of local SGD [47, 54]. The issue comes from the inner-product noise term \(M_{1,t}\) and the discrepancy term \(H_{t}\) in Lemma 1, whose enrolled summations can only be bounded by \(O(\sqrt{\epsilon/d})\) and \(O\{(\sqrt{Id}+I)\epsilon\}\) respectively at the beginning of convergence stage. Thanks to the special structure of gradient, we find that the scales of \(M_{1,t}\) and \(H_{t}\) can shrink as \(\sin\phi_{t}\) decreases. In light of this, we can continuously refine the bound by the following contraction: for \((K+1)T_{v}\leq t\leq\widetilde{O}(\eta^{-2})\) it holds that

\[\sin^{2}\phi_{t}\lesssim\max\left\{\left(\sqrt{\frac{\epsilon}{d}}+(\sqrt{Id} +I)\epsilon\right)^{1+\frac{K}{2}},\epsilon\right\}.\]

By taking \(K=O(\log(1/\epsilon))\), we can obtain the target convergence \(\sin^{2}\phi_{t}\lesssim\epsilon\) with high probability. More details can be found in Appendix E.1.

**Lemma 5** (Convergence of the second layer).: _Under the choice for \(\eta\) and conditions for \(\epsilon\) in Theorem 3. Suppose \(\sin^{2}\phi_{t}\leq\epsilon\) holds for any \(0\leq t\leq\widetilde{O}(\eta^{-2})\). With probability at least \(1-\delta\), we can guarantee that \(\|\mathbf{a}_{t}-\mathbf{a}^{*}\|^{2}\lesssim\epsilon\) holds for any \(\widetilde{O}\left(\eta^{-1}\right)\leq t\leq\widetilde{O}(\eta^{-2})\)._

The lemma stated above guarantees the convergence of the second layer after that of the first layer, whose proof is deferred to Appendix E.2. Equipped with Lemmas 4 and 5, we can prove Theorem 3 by leveraging the closed form of the objective \(L(\mathbf{w},\mathbf{a})\) (see Lemma 6).

The perturbed GD method in Zhou et al. [63] requires manually adjusting the scale of injected noises and the learning rate between the transition of two phases. In local SGD, we can use a _universal_ learning rate in the self-correction stage and convergence stage. Considering an initial point that is closer to spurious local minima (\(\phi_{0}>\tilde{\phi}^{u}\)) defined in Lemma 3, conditions (7), (8) and (9) on the learning rate can be satisfied if we choose \(I=O\left\{d/(\epsilon N)\right\}\). Together with (10) and (11), if \(N\lesssim\min\{dk^{4},\epsilon^{-1}\}\), we can get the total communication complexity

\[R_{\text{total}}=\widetilde{O}\left(\frac{k^{11/2}}{\sin^{3}\phi_{0}}\right).\]

Therefore, our theory shows that local SGD can correct the signals and converge to the global minima with almost arbitrary initialization except for the case of initializing at the local minima (\(\phi_{0}=\pi\)).

## 5 Experiments

We now report some simulation results on synthetic data. We also compare the performance of two algorithms: local SGD and minibatch SGD. At each round, minibatch SGD updates the model weights by using the stochastic gradients with batch size \(NI\) in total to update the model, where each local machine computes \(I\) gradients and communicates with other machines. Minibatch SGD and local SGD have the same computation and communication structure [54].

In our first simulation, we set \(\|\mathbf{w}^{*}\|=\|\mathbf{a}^{*}\|=1\) and \(|\mathbf{1}^{\top}\mathbf{a}^{*}|=\sqrt{k}\). The results starting from three bad initial regions (i.e., \(\phi_{0}>\pi/2\) or \(\mathbf{a}_{0}^{\top}\mathbf{a}^{*}\leq 0\)) are reported in Figure 1. As we can see, the signals of two layers can be both corrected to a good region (i.e., both \(\cos(\phi_{t})\) and \(\mathbf{a}_{t}^{\top}\mathbf{a}^{*}\) go to \(1\) and the loss goes to \(0\) when \(t\) increases) even in the worst case when \(\mathbf{a}_{0}=-\mathbf{a}^{*}\) and \(\phi_{0}>\pi/2\). An interesting phenomenon is that local SGD can correct the signals faster than minibatch SGD. The reason is that the statistical error of stochastic gradients is not dominating in the self-correction process, so the effect of large batch size in minibatch SGD (i.e., with batch size \(NI\) for \(1\) iteration) is not as useful as smaller batch size in local SGD with more iterations (i.e., batch size \(1\) for \(I\) iterations on each machine with \(N\) machines in total). These results corroborate our theoretical analysis.

Figure 1: Converged trajectories of local SGD and minibatch SGD with different bad initial points. The dimensions of the two layers are \(k=10\) and \(d=25\). The number of skipped communication is \(I=8\). The batch size is 4. The two algorithms’ learning rates are \(\eta=0.1\).

In the second simulation, we plot the trajectories of local SGD and minibatch SGD under different values of \(\alpha\) in Figure 2. Here we set \(\mathbf{w}^{*}=\mathbf{1}_{d}/\sqrt{d}\) and \(\mathbf{a}^{*}=(\mathbf{1}_{\alpha k},0,\ldots,0)/\sqrt{\alpha k}\). The bad initial point is fixed as \(\mathbf{v}_{0}=(-1,0,\ldots,0)\) and \(\mathbf{a}_{0}=(-\sqrt{\alpha},0,\ldots,0)\), where \(\phi_{0}>\pi/2\) and \(\mathbf{a}_{0}^{\top}\mathbf{a}^{*}<0\). When \(\alpha=1/32,1/16\), we can see that minibatch SGD and local SGD converge to the local minima. When \(\alpha=1/4\), they can converge to the global minima with the same initial point. In this case, as we can see from Figure 2(c), the signals of two layers can be both corrected to a good region.

In the third simulation, we calculate the probabilities that local SGD and minibatch SGD converge to the global minimum under different values of \(\alpha\). The averaged results are taken over 100 independent repeated simulations. In each trial, we generate initial points by \(\mathbf{v}_{0}\sim\mathrm{Unif}(\mathbb{S}^{d-1})\) and \(\mathbf{a}_{0}\sim\mathrm{Unif}(\mathbb{B}^{k}(|\mathbf{1}^{\top}\mathbf{a}^{*}|/\sqrt {k}))\). The results are given in Table 1.

## 6 Conclusion

We theoretically investigate the convergence of local SGD, a cornerstone algorithm in federated learning with strong empirical performance. We demonstrate convergence to the global minimum for two-layer neural networks _without overparameterization_, _without injecting noise_, and when the input data is Gaussian. A new self-correction mechanism guarantees the algorithm reaches a good region even if the initialization is in a bad region. The landscape of the objective is divided into several regions to carefully control the interference of the two layers during the correction process. A new exact recursive characterization of the direction of global parameter provides the key to show convergence to the global minimum with linear speedup in the number of machines and reduced communication rounds. Experiments on simulated data corroborate the theoretical results. To the best of our knowledge, this work is the first to theoretically demonstrate the global convergence of the vanilla local SGD for neural networks without overparameterization.

Figure 2: Trajectories of local SGD and minibatch SGD under different values of \(\alpha\). The dimensions of the two layers are \(k=64\) and \(d=25\). The number of skipped communication is \(I=8\). The batch size is 16. The two algorithms’ learning rates are \(\eta=0.1\).

## Acknowledgments and Disclosure of Funding

We would like to thank the anonymous reviewers for their helpful comments. Mingrui Liu is supported by a grant from George Mason University. The work of Yajie Bao was done when he was virtually visiting Mingrui Liu's research group in the Department of Computer Science at George Mason University.

## References

* Allen-Zhu and Li [2018] Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via first-order oracles. _Advances in Neural Information Processing Systems_, 31, 2018.
* Allen-Zhu and Li [2019] Zeyuan Allen-Zhu and Yuanzhi Li. Can sgd learn recurrent neural networks with provable generalization? _Advances in Neural Information Processing Systems_, 32, 2019.
* Allen-Zhu et al. [2019] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _International Conference on Machine Learning_, pages 242-252. PMLR, 2019.
* Arora et al. [2019] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. _Advances in neural information processing systems_, 32, 2019.
* Bakshi et al. [2019] Ainesh Bakshi, Rajesh Jayaram, and David P Woodruff. Learning two layer rectified neural networks in polynomial time. In _Conference on Learning Theory_, pages 195-268. PMLR, 2019.
* Banerjee et al. [2022] Arindam Banerjee, Tiancong Chen, Xinyan Li, and Yingxue Zhou. Stability based generalization bounds for exponential family langevin dynamics. _International Conference on Machine Learning (ICML)_, 2022.
* Brutzkus and Globerson [2017] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. In _International conference on machine learning_, pages 605-614. PMLR, 2017.
* Chaudhari et al. [2017] Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing gradient descent into wide valleys. In _International Conference on Learning Representations_, 2017. URL [https://openreview.net/forum?id=B1YfAfcgl](https://openreview.net/forum?id=B1YfAfcgl).
* Chourasia et al. [2021] Rishav Chourasia, Jiayuan Ye, and Reza Shokri. Differential privacy dynamics of langevin diffusion and noisy gradient descent. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL [https://openreview.net/forum?id=Nfbelusrgx4](https://openreview.net/forum?id=Nfbelusrgx4).
* Deng et al. [2022] Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Local sgd optimizes overparameterized neural networks in polynomial time. In _International Conference on Artificial Intelligence and Statistics_, pages 6840-6861. PMLR, 2022.
* Dieuleveut and Patel [2019] Aymeric Dieuleveut and Kumar Kshitij Patel. Communication trade-offs for local-sgd with large step size. _Advances in Neural Information Processing Systems_, 32:13601-13612, 2019.
* Du and Lee [2018] Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic activation. In _International conference on machine learning_, pages 1329-1338. PMLR, 2018.
* Du et al. [2018] Simon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima. In _International Conference on Machine Learning_, pages 1339-1348. PMLR, 2018.
* Du et al. [2019] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In _International conference on machine learning_, pages 1675-1685. PMLR, 2019.

* Du et al. [2017] Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? _arXiv preprint arXiv:1709.06129_, 2017.
* Du et al. [2018] Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. _arXiv preprint arXiv:1811.03804_, 2018.
* Ge et al. [2015] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points--online stochastic gradient for tensor decomposition. In _Conference on learning theory_, pages 797-842. PMLR, 2015.
* Ge et al. [2016] Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. _Advances in neural information processing systems_, 29, 2016.
* Ge et al. [2017] Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. _arXiv preprint arXiv:1711.00501_, 2017.
* Haddadpour et al. [2019] Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Local sgd with periodic averaging: Tighter analysis and adaptive synchronization. In _Advances in Neural Information Processing Systems_, pages 11080-11092, 2019.
* Hillar and Lim [2013] Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. _Journal of the ACM (JACM)_, 60(6):1-39, 2013.
* Huang et al. [2021] Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. Fl-ntk: A neural tangent kernel-based framework for federated learning analysis. In _International Conference on Machine Learning_, pages 4423-4434. PMLR, 2021.
* Jacot et al. [2018] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* Jiang and Agrawal [2018] Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learning with sparse and quantized communication. In _Advances in Neural Information Processing Systems_, pages 2525-2536, 2018.
* Jin et al. [2017] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In _International conference on machine learning_, pages 1724-1732. PMLR, 2017.
* Jin et al. [2018] Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle points faster than gradient descent. In _Conference On Learning Theory_, pages 1042-1085. PMLR, 2018.
* Kairouz et al. [2021] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* Karimireddy et al. [2020] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_, pages 5132-5143. PMLR, 2020.
* Kawaguchi [2016] Kenji Kawaguchi. Deep learning without poor local minima. _Advances in neural information processing systems_, 29, 2016.
* Kennedy [1990] A. D. Kennedy. _The Theory of Hybrid Stochastic Algorithms_, pages 209-223. Springer US, Boston, MA, 1990. ISBN 978-1-4615-3784-7. doi: 10.1007/978-1-4615-3784-7_14. URL [https://doi.org/10.1007/978-1-4615-3784-7_14](https://doi.org/10.1007/978-1-4615-3784-7_14).
* Khaled et al. [2020] Ahmed Khaled, Konstantin Mishchenko, and Peter Richtarik. Tighter theory for local sgd on identical and heterogeneous data. In _International Conference on Artificial Intelligence and Statistics_, pages 4519-4529. PMLR, 2020.

* Koloskova et al. [2020] Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified theory of decentralized sgd with changing topology and local updates. In _International Conference on Machine Learning_, pages 5381-5393. PMLR, 2020.
* Levy [2016] Kfir Y Levy. The power of normalization: Faster evasion of saddle points. _arXiv preprint arXiv:1611.04831_, 2016.
* Li et al. [2020] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. _IEEE Signal Processing Magazine_, 37(3):50-60, 2020.
* Li et al. [2021] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning on non-iid features via local batch normalization. _arXiv preprint arXiv:2102.07623_, 2021.
* Li and Liang [2018] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. _Advances in neural information processing systems_, 31, 2018.
* Li and Yuan [2017] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In _Advances in Neural Information Processing Systems_, pages 597-607, 2017.
* Li et al. [2020] Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural networks beyond ntk. In _Conference on learning theory_, pages 2613-2682. PMLR, 2020.
* Lin et al. [2018] Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don't use large mini-batches, use local sgd. _arXiv preprint arXiv:1808.07217_, 2018.
* Liu et al. [2022] Mingrui Liu, Zhenxun Zhuang, Yunwen Lei, and Chunyang Liao. A communication-efficient distributed gradient clipping algorithm for training deep neural networks. _arXiv preprint arXiv:2205.05040_, 2022.
* Ma et al. [2018] Yi-An Ma, Yuansi Chen, Chi Jin, Nicolas Flammarion, and Michael I. Jordan. Sampling can be faster than optimization. _CoRR_, abs/1811.08413, 2018. URL [http://arxiv.org/abs/1811.08413](http://arxiv.org/abs/1811.08413).
* McDonald et al. [2010] Ryan McDonald, Keith Hall, and Gideon Mann. Distributed training strategies for the structured perceptron. In _Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics_, pages 456-464. Association for Computational Linguistics, 2010.
* McMahan et al. [2017] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.
* Neill [2011] R. M. Neill. _MCMC Using Hamiltonian Dynamics_, pages 113-162. Chapman and Hall/CRC, Boston, MA, 2011. ISBN 9780429138508. URL [https://www.taylorfrancis.com/chapters/edit/10.1201/b10905-10/mcmc-using-hamiltonian-dynamics-radford-neal](https://www.taylorfrancis.com/chapters/edit/10.1201/b10905-10/mcmc-using-hamiltonian-dynamics-radford-neal).
* Reddi et al. [2021] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. _ICLR_, 2021.
* Salimans and Kingma [2016] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. _Advances in neural information processing systems_, 29, 2016.
* Stich [2018] Sebastian U Stich. Local sgd converges fast and communicates little. _arXiv preprint arXiv:1805.09767_, 2018.
* Sun et al. [2016] Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere i: Overview and the geometric picture. _IEEE Transactions on Information Theory_, 63(2):853-884, 2016.
* Tian [2017] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. In _International conference on machine learning_, pages 3404-3413. PMLR, 2017.

* [50] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* [51] Jianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of communication-efficient sgd algorithms. _arXiv preprint arXiv:1808.07576_, 2018.
* [52] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics. In _28th International Conference on International Conference on Machine Learning_, ICML'11, page 681-688, Madison, WI, USA, 2011. Omnipress. ISBN 9781450306195.
* [53] Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs local sgd for heterogeneous distributed learning. _arXiv preprint arXiv:2006.04735_, 2020.
* [54] Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan, Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? In _International Conference on Machine Learning_, pages 10334-10343. PMLR, 2020.
* [55] Yi Xu, Rong Jin, and Tianbao Yang. First-order stochastic algorithms for escaping from saddle points in almost linear time. _Advances in neural information processing systems_, 31, 2018.
* [56] Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient momentum sgd for distributed non-convex optimization. _arXiv preprint arXiv:1905.03817_, 2019.
* [57] Yaodong Yu, Alexander Wei, Sai Praneeth Karimireddy, Yi Ma, and Michael Jordan. Tct: Convexifying federated learning using bootstrapped neural tangent kernels. _Advances in Neural Information Processing Systems_, 35:30882-30897, 2022.
* [58] Kai Yue, Richeng Jin, Ryan Pilgrim, Chau-Wai Wong, Dror Baron, and Huaiyu Dai. Neural tangent kernel empowered federated learning. In _International Conference on Machine Learning_, pages 25783-25803. PMLR, 2022.
* [59] Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. Fedpd: A federated learning framework with optimal rates and adaptivity to non-iid data. _arXiv preprint arXiv:2005.11418_, 2020.
* [60] Yuchen Zhang, John C Duchi, and Martin J Wainwright. Communication-efficient algorithms for statistical optimization. _The Journal of Machine Learning Research_, 14(1):3321-3363, 2013.
* [61] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. In _International conference on machine learning_, pages 4140-4149. PMLR, 2017.
* [62] Fan Zhou and Guojing Cong. On the convergence properties of a \(k\)-step averaging stochastic gradient descent algorithm for nonconvex optimization. _arXiv preprint arXiv:1708.01012_, 2017.
* [63] Mo Zhou, Tianyi Liu, Yan Li, Dachao Lin, Enlu Zhou, and Tuo Zhao. Toward understanding the importance of noise in training neural networks. In _International Conference on Machine Learning_, pages 7594-7602. PMLR, 2019.
* [64] Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects, 2019.
* [65] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient descent. In _Advances in neural information processing systems_, pages 2595-2603, 2010.
* [66] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes over-parameterized deep relu networks. _arXiv preprint arXiv:1811.08888_, 2018.

**Appendix**

###### Contents

* 1 Introduction
* 2 Related Work
* 3 Notations and Problem Setup
* 4 Theoretical Analysis
	* 4.1 Exact Dynamic of Each Layer
	* 4.2 Self-correction of Signals in Two Layers
	* 4.3 Convergence with Linear Speedup
* 5 Experiments
* 6 Conclusion
* A Preliminaries for Analysis
* A.1 Dynamic of the First Layer
* A.2 Dynamic of the Second Layer
* A.3 Concentration Inequalities for the Noises
* A.3.1 Sub-exponential Martingale Difference
* A.3.2 Scales of Sub-exponential Norm for Noises in Stochastic Gradients
* A.3.3 Corollaries of Lemma 11.
* B Bound the Weight's Norm during Training Process
* B.1 The Scale of the Second Layer's Weight
* B.2 The Scale of the First Layer's weight
* C Bound the Discrepancy during Training Process
* C.1 The Discrepancy in the First Layer
* C.2 The Discrepancy in the Second Layer
* D Self-correction of Signals
* D.1 Self-correction of the Second Layer
* D.2 Proof of Lemma 3
* D.3 Self-correction of the First Layer
* E Convergence with Linear Speedup
* E.1 Convergence of the First Layer
* E.2 Convergence of the Second Layer
* E.3 Conclusion

Preliminaries for Analysis

We write \(\mathbf{0}=(0,\dots,0)^{\top}\), \(\mathbf{1}=(1,\dots,1)^{\top}\) and denote \(\mathbf{I}\) by the identity matrix. The following lemma gives the closed forms of the objective and gradients.

**Lemma 6** (Du et al. [13]).: _If every entry of \(\mathbf{Z}\) is i.i.d. sampled from a Gaussian distribution with mean 0 and variance 1, and \(\|\mathbf{w}^{*}\|=1\), then the population loss is_

\[L(\mathbf{w},\mathbf{a}) =\frac{1}{2}\Bigg{[}\frac{\pi-1}{2\pi}\|\mathbf{a}\|^{2}+\frac{\pi-1 }{2\pi}\|\mathbf{a}^{*}\|^{2}-\frac{g(\phi)-1}{\pi}\mathbf{a}^{\top}\mathbf{a}^{*}\] \[\qquad\qquad\qquad+\frac{1}{2\pi}(\mathbf{1}^{\top}\mathbf{a}^{*})^{ 2}+\frac{1}{2\pi}(\mathbf{1}^{\top}\mathbf{a})^{2}-\frac{1}{\pi}(\mathbf{1}^{\top} \mathbf{a})(\mathbf{1}^{\top}\mathbf{a})\Bigg{]},\] (A.1)

_where \(g(\phi)=(\pi-\phi)\cos\phi+\sin\phi\). And the the expected gradient of \(\mathbf{w}\) and \(\mathbf{a}\) are_

\[\nabla_{\mathbf{w}}L(\mathbf{w},\mathbf{a}) =-\frac{\pi-\phi}{2\pi}(\mathbf{a}^{\top}\mathbf{a}^{*})\cdot\mathbf{w}^{*},\] (A.2) \[\nabla_{\mathbf{a}}L(\mathbf{w},\mathbf{a}) =\frac{1}{2\pi}\left[(\mathbf{1}\mathbf{1}^{\top}+(\pi-1)\mathbf{ I})\mathbf{a}-(\mathbf{1}\mathbf{1}^{\top}+(g(\phi)-1)\mathbf{I})\mathbf{a}^{*}\right].\] (A.3)

Denote the noises of stochastic gradients in each local machine \(i\in[N]\) by

\[\mathbf{\xi}_{t}^{i}=\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i}; \mathbf{Z}_{t}^{i})-\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i}),\quad\mathbf{ \epsilon}_{t}^{i}=\nabla_{\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i};\mathbf{Z}_{t }^{i})-\nabla_{\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i}).\]

Then we write the averaged noises as

\[\mathbf{\xi}_{t}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{P}_{t}^{i}\mathbf{\xi} _{t}^{i},\quad\mathbf{\epsilon}_{t}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{\epsilon}_{t}^{i}.\]

Let \(\mathbf{P}_{t}=\frac{1}{\|\mathbf{v}_{t}\|}(\mathbf{I}-\mathbf{w}_{t}\mathbf{w}_{t}^{\top})\). We introduce an auxiliary sequence \(\tilde{\mathbf{v}}_{t}\), which updates by

\[\tilde{\mathbf{v}}_{t+1} =\mathbf{v}_{t}-\eta\left(\mathbf{P}_{t}\frac{1}{N}\sum_{i=1}^{N} \nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i};\mathbf{a}_{t}^{i})+\mathbf{\xi}_{t}\right).\] (A.4)

Then it holds that

\[\tilde{\mathbf{v}}_{t+1} =\mathbf{v}_{t+1}-\eta\frac{1}{N}\sum_{i=1}^{N}(\mathbf{P}_{t}^{i}- \mathbf{P}_{t})\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\] \[=:\mathbf{v}_{t+1}-\eta\mathbf{h}_{t}.\] (A.5)

We define the \(\sigma\)-filtration in the training process: \(\mathcal{F}_{0}\) is the trivial filtration and

\[\mathcal{F}_{t}=\sigma\left(\{\mathbf{\xi}_{s}^{i},\mathbf{\epsilon}_{s}^{i}:i\in[N] \}_{s=0}^{t-1}\right),\quad\text{for }t\geq 1.\]

We also write \(\mathbb{E}_{t}[\cdot]=\mathbb{E}[\cdot\mid\mathcal{F}_{t}]\).

### Dynamic of the First Layer

**Lemma 1 restated.** The first layer in local SGD satisfies that

\[\|\mathbf{v}_{t+1}\|^{2}\sin^{2}\phi_{t+1}=\left(1-\eta\lambda_{t} \cos\phi_{t}\right)^{2}\|\mathbf{v}_{t}\|^{2}\sin^{2}\phi_{t}-2\eta M_{1,t}+\eta^ {2}M_{2,t}+H_{t},\] (A.6)

where \(\lambda_{t}=\frac{1}{N}\sum_{i=1}^{N}\frac{\pi-\phi_{t}^{i}}{2\pi}\frac{(\mathbf{a }_{t}^{i})^{\top}\mathbf{a}^{*}}{\|\mathbf{v}_{t}\|^{2}}\), \(H_{t}=\|\mathbf{v}_{t+1}\|^{2}\sin^{2}\phi_{t+1}-\|\tilde{\mathbf{v}}_{t+1}\|^{2}\sin^{ 2}\tilde{\phi}_{t+1}\) and

\[M_{1,t}=(\mathbf{v}_{t}-\eta\mathbf{P}_{t}\frac{1}{N}\sum_{i=1}^{N} \nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i}))^{\top}\left(\mathbf{I}-\mathbf{w }^{*}(\mathbf{w}^{*})^{\top}\right)\mathbf{\xi}_{t},\;M_{2,t}=\mathbf{\xi}_{t}^{\top} \left(\mathbf{I}-\mathbf{w}^{*}(\mathbf{w}^{*})^{\top}\right)\mathbf{\xi}_{t}.\]Proof.: Let \(\tilde{\mathbf{v}}_{t}=\mathbf{v}_{t}-\eta\mathbf{P}_{t}\frac{1}{N}\sum_{i=1}^{N}\nabla_{ \mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\), then we have \(\tilde{\mathbf{v}}_{t+1}=\tilde{\mathbf{v}}_{t}-\eta\mathbf{\xi}_{t}\). If follows that

\[\|\tilde{\mathbf{v}}_{t+1}\|^{2} \sin^{2}\tilde{\phi}_{t+1}=\|\tilde{\mathbf{v}}_{t+1}\|^{2}-\left( \tilde{\mathbf{v}}_{t+1}^{\top}\mathbf{w}^{*}\right)^{2}\] \[=\|\tilde{\mathbf{v}}_{t}-\eta\mathbf{\xi}_{t}\|^{2}-\left[(\tilde{\mathbf{v} }_{t}^{\top}\mathbf{w}^{*})^{2}-2\eta(\mathbf{\xi}_{t}^{\top}\mathbf{w}^{*})(\tilde{\mathbf{v }}_{t}^{\top}\mathbf{w}^{*})+\eta^{2}(\mathbf{\xi}_{t}^{\top}\mathbf{w}^{*})^{2}\right]\] \[=\|\tilde{\mathbf{v}}_{t}\|^{2}-(\tilde{\mathbf{v}}_{t}^{\top}\mathbf{w}^{*}) ^{2}-2\eta\tilde{\mathbf{v}}_{t}^{\top}(\mathbf{I}-\mathbf{w}^{*}(\mathbf{w}^{*})^{\top}) \mathbf{\xi}_{t}+\eta^{2}\mathbf{\xi}_{t}^{\top}(\mathbf{I}-\mathbf{w}^{*}(\mathbf{w}^{*})^{ \top})\mathbf{\xi}_{t}\] \[=\|\tilde{\mathbf{v}}_{t}\|^{2}-(\tilde{\mathbf{v}}_{t}^{\top}\mathbf{w}^{*})^ {2}-2\eta M_{1,t}+\eta^{2}M_{2,t}.\] (A.7)

From the definition \(\lambda_{t}=\frac{1}{N}\sum_{i=1}^{N}\frac{\pi-\phi_{t}^{i}}{2\pi}\frac{(\mathbf{a} _{t}^{i})^{\top}\mathbf{a}^{*}}{\|\mathbf{v}_{t}\|^{2}}\) and (A.2), we can write

\[\tilde{\mathbf{v}}_{t}^{\top}\mathbf{w}^{*} =\mathbf{v}_{t}^{\top}\mathbf{w}^{*}-\eta(\mathbf{w}^{*})^{\top}\mathbf{P}_{t }\frac{1}{N}\sum_{i=1}^{N}\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\] \[=\mathbf{v}_{t}^{\top}\mathbf{w}^{*}+\eta(\mathbf{w}^{*})^{\top}\left( \mathbf{I}-\mathbf{w}_{t}\mathbf{w}_{t}^{\top}\right)\mathbf{w}^{*}\frac{1}{N}\sum_{i=1}^{ N}\frac{\pi-\phi_{t}^{i}}{2\pi}\frac{(\mathbf{a}_{t}^{i})^{\top}\mathbf{a}^{*}}{\|\mathbf{v}_{t }\|}\] \[=\|\mathbf{v}_{t}\|\cos\phi_{t}+\eta\|\mathbf{v}_{t}\|\left(1-\cos^{2} \phi_{t}\right)\frac{1}{N}\sum_{i=1}^{N}\frac{\pi-\phi_{t}^{i}}{2\pi}\frac{(\bm {a}_{t}^{i})^{\top}\mathbf{a}^{*}}{\|\mathbf{v}_{t}\|^{2}}\] \[=\|\mathbf{v}_{t}\|\left(\cos\phi_{t}+\eta\lambda_{t}\sin^{2}\phi_{t} \right),\] (A.8)

and

\[\|\tilde{\mathbf{v}}_{t}\|^{2} =\left\|\mathbf{v}_{t}-\eta\mathbf{P}_{t}\frac{1}{N}\sum_{i=1}^{N} \nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\right\|^{2}\] \[\overset{(i)}{=}\|\mathbf{v}_{t}\|^{2}+\left(\frac{1}{N}\sum_{i=1}^{N }\frac{\eta}{2\pi}\frac{\pi-\phi_{t}^{i}}{\|\mathbf{v}_{t}\|}(\mathbf{a}_{t}^{i})^{ \top}\mathbf{a}^{*}\right)^{2}(\mathbf{w}^{*})^{\top}\left(\mathbf{I}-\mathbf{w}_{t}\mathbf{w} _{t}^{\top}\right)^{2}\mathbf{w}^{*}\] \[\overset{(ii)}{=}\|\mathbf{v}_{t}\|^{2}+\left(\frac{1}{N}\sum_{i=1}^{ N}\frac{\eta}{2\pi}\frac{\pi-\phi_{t}^{i}}{\|\mathbf{v}_{t}\|}(\mathbf{a}_{t}^{i})^{\top}\mathbf{a}^{* }\right)^{2}(1-\cos^{2}\phi_{t})\] \[\overset{(iii)}{=}\|\mathbf{v}_{t}\|^{2}\left(1+\eta^{2}\lambda_{t}^ {2}\sin^{2}\phi_{t}\right),\] (A.9)

where \((i)\) holds since \(\mathbf{P}_{t}\mathbf{v}_{t}=\mathbf{0}\), \((ii)\) is true due to \(\mathbf{I}-\mathbf{w}_{t}\mathbf{w}_{t}^{\top}\) is an idempotent matrix, and \((iii)\) comes from \(\|\mathbf{w}^{*}\|=1\). Combining (A.8) and (A.9), we can get

\[\|\tilde{\mathbf{v}}_{t}\|^{2}-\left(\tilde{\mathbf{v}}_{t}^{\top}\mathbf{w}^ {*}\right)^{2} =\left[1+\eta^{2}\lambda_{t}^{2}\sin^{2}\phi_{t}-\left(\cos\phi_{ t}+\eta\lambda_{t}\sin^{2}\phi_{t}\right)^{2}\right]\|\mathbf{v}_{t}\|^{2}\] \[=\left[\sin^{2}\phi_{t}-2\eta\lambda_{t}\cos\phi_{t}\sin^{2}\phi _{t}+\eta^{2}\lambda_{t}^{2}(\sin^{2}\phi_{t}-\sin^{4}\phi_{t})\right]\|\mathbf{v}_{t }\|^{2}\] \[=\left(\sin^{2}\phi_{t}-2\eta\lambda_{t}\cos\phi_{t}\sin^{2}\phi_{ t}+\eta^{2}\lambda_{t}^{2}\sin^{2}\phi_{t}\cos^{2}\phi_{t}\right)\|\mathbf{v}_{t}\|^{2}\] \[=\left(1-2\eta\lambda_{t}\cos\phi_{t}+\eta^{2}\lambda_{t}^{2}\cos ^{2}\phi_{t}\right)\|\mathbf{v}_{t}\|^{2}\sin^{2}\phi_{t}\] \[=\left(1-\eta\lambda_{t}\cos\phi_{t}\right)^{2}\|\mathbf{v}_{t}\|^{2} \sin^{2}\phi_{t}.\] (A.10)

Plugging (A.10) into (A.7) gives

\[\|\tilde{\mathbf{v}}_{t+1}\|^{2}\sin^{2}\tilde{\phi}_{t+1}=\left(1-\eta\lambda_{t} \cos\phi_{t}\right)^{2}\|\mathbf{v}_{t}\|^{2}\sin^{2}\phi_{t}-2\eta M_{1,t}+\eta^{ 2}M_{2,t},\]

where \(\mathbb{E}_{t}[M_{1,t}]=0\) since \(\mathbf{P}_{t}\), \(\tilde{\mathbf{v}}_{t}\in\mathcal{F}_{t}\) and \(\mathbb{E}_{t}[\mathbf{\xi}_{t}]=\mathbf{0}\), and \(M_{2,t}\geq 0\) because \(\mathbf{P}_{t}\) and \(\mathbf{I}-\mathbf{w}^{*}(\mathbf{w}^{*})^{\top}\) are both semi positive definite. The result follows from the definition of \(H_{t}\). 

### Dynamic of the Second Layer

**Lemma 2 restated.** Let \(A_{0}=(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}-(\mathbf{1}^{\top}\mathbf{a}^{*})(\mathbf{1}^{ \top}\mathbf{a}_{0})\). For local SGD algorithm started with the initial point \((\mathbf{v}_{0},\mathbf{a}_{0})\), the second layer satisfies that

\[\mathbf{a}_{t}^{\top}\mathbf{a}^{*}= \left(1-\eta\frac{\pi-1}{2\pi}\right)^{t}\mathbf{a}_{0}^{\top}\mathbf{a}^{* }+\frac{1-\left(\frac{2\pi-\eta(k+\pi-1)}{2\pi-\eta(\pi-1)}\right)^{t}}{k} \left(1-\eta\frac{\pi-1}{2\pi}\right)^{t}A_{0}\]\[\quad+\eta\sum_{s=0}^{t}\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{t-s}( \mathbf{1}^{\top}\mathbf{a}^{*})(\mathbf{1}^{\top}\mathbf{e}_{s}).\] (A.13)

Plugging (A.13) into (A.12), then enrolling (A.12) from timestamp \(0\) to \(t\) gives that

\[\mathbf{a}_{t+1}^{\top}\mathbf{a}^{*} =\left(1-\eta\frac{\pi-1}{2\pi}\right)^{t+1}\mathbf{a}_{0}^{\top}\mathbf{ a}^{*}+\frac{\eta\|\mathbf{a}^{*}\|^{2}}{2\pi}\sum_{s=0}^{t}\left(1-\eta\frac{\pi-1}{2 \pi}\right)^{t-s}\left(\frac{1}{N}\sum_{i=1}^{N}g(\phi_{s}^{i})-1\right)\] \[\quad+\left(\frac{\eta\|\mathbf{1}^{\top}\mathbf{a}^{*}\|}{2\pi} \right)^{2}\sum_{s=0}^{t}\left(1-\eta\frac{\pi-1}{2\pi}\right)^{t-s}\sum_{l=0} ^{s-1}\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s-1-l}\left(\pi-\frac{1}{N}\sum _{i=1}^{N}g(\phi_{l}^{i})\right)\] \[\quad+\frac{\eta^{2}}{2\pi}\sum_{s=0}^{t}\left(1-\eta\frac{\pi-1} {2\pi}\right)^{t-s}\sum_{l=0}^{s-1}\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s- 1-l}(\mathbf{1}^{\top}\mathbf{a}^{*})(\mathbf{1}^{\top}\mathbf{e}_{l})\]\[\left\|\mathbf{x}\right\|_{\psi_{1}}\leq\|\mathbf{x}\|_{\psi_{2}}\|\mathbf{y}\|_{ \psi_{2}}.\]

**Lemma 8** (Lemma 2.6.8 in [50]).: _If \(\mathbf{x}\) is a sub-gaussian random variable, then \(\mathbf{x}-\mathbb{E}[\mathbf{x}]\) is sub-gaussian too and_

\[\|\mathbf{x}-\mathbb{E}[\mathbf{x}]\|_{\psi_{2}}\lesssim\|\mathbf{x}\|_{\psi_{2 }}.\]

**Lemma 9** (Lemma 2.7.7 in [50]).: _Let \(\mathbf{x}\) and \(\mathbf{y}\) be sub-gaussian random variables. Then \(\mathbf{x}\mathbf{y}\) is sub-exponential and_

\[\|\mathbf{x}\mathbf{y}\|_{\psi_{1}}\leq\|\mathbf{x}\|_{\psi_{2}}\|\mathbf{y}\|_ {\psi_{2}}.\]

**Lemma 10**.: _Suppose \(\{\mathbf{x}_{t}\}_{t\geq 1}\subseteq\mathbb{R}\) are sub-exponential martingale difference sequence with norm \(\{\|\mathbf{x}_{t}\|_{\psi_{1}}\}_{t\geq 1}\) adapted to the filtration \(\{\mathcal{F}_{t}\}_{t\geq 1}\), that is \(\mathbb{E}[\mathbf{x}_{t}|\mathcal{F}_{t}]=0\) and \(\|\mathbf{x}_{t}\|_{\psi_{1}}=\inf\{s\geq 0:\mathbb{E}[e^{|\mathbf{x}_{t}|/s}| \mathcal{F}_{t}]\leq 2\}\). Then we have_

\[\mathbb{P}\left(\left|\sum_{s=1}^{t}\mathbf{x}_{s}\right|\geq\frac{1}{c}\sqrt{ \log(1/\delta)\sum_{s=1}^{t}\|\mathbf{x}_{s}\|_{\psi_{1}}^{2}}+c\log(1/\delta) \max_{1\leq s\leq t}\|\mathbf{x}_{s}\|_{\psi_{1}}\right)\leq 2\delta,\]

_where \(c>0\) is an absolute constant._Proof.: Invoking Proposition 2.7.1 in [50], we know

\[\mathbb{E}\left[\exp(\lambda\mathbf{x}_{t})\mid\mathcal{F}_{t}\right] \leq\exp\left(c^{-2}\lambda^{2}\|\mathbf{x}_{t}\|_{\psi_{1}}^{2}\right),\quad \text{if }|\lambda|\leq\frac{c}{\|\mathbf{x}_{t}\|_{\psi_{1}}}.\] (A.14)

Define

\[M_{t}=\exp\left\{\lambda\sum_{s=1}^{t}\mathbf{x}_{s}-c^{-2}\lambda^{2}\sum_{s= 1}^{t}\|\mathbf{x}_{s}\|_{\psi_{1}}^{2}\right\}.\]

It follows from (A.14) that for any \(t\geq 1\),

\[\mathbb{E}\left[M_{t}\mid\mathcal{F}_{t}\right]=M_{t-1}\mathbb{E} \left[\exp\left\{\lambda\mathbf{x}_{t}-c^{-2}\lambda^{2}\|\mathbf{x}_{t}\|_{ \psi_{1}}^{2}\right\}\mid\mathcal{F}_{t}\right]\leq M_{t-1}.\]

It further implies that

\[\mathbb{E}\left[M_{t}\right]=\mathbb{E}\left[\mathbb{E}\left[M_{t} \mid\mathcal{F}_{t}\right]\right]\leq\mathbb{E}\left[M_{t-1}\right]\leq\cdots \leq\mathbb{E}\left[M_{1}\right]\leq 1.\]

Using Markov's inequality, for \(0<\lambda\leq\frac{c}{\max_{1\leq s\leq t}\|\mathbf{x}_{t}\|_{\psi_{1}}}\), we have

\[\mathbb{P}\left(\sum_{s=1}^{t}\mathbf{x}_{s}\geq c^{-2}\lambda \sum_{s=1}^{t}\|\mathbf{x}_{s}\|_{\psi_{1}}^{2}+\frac{1}{\lambda}\log(1/ \delta)\right)=\mathbb{P}\left(M_{t}\geq\frac{1}{\delta}\right)\leq\delta.\] (A.15)

Now taking

\[\lambda=\min\left\{\sqrt{\frac{c^{2}\log(1/\delta)}{\sum_{s=1}^{t} \|\mathbf{x}_{s}\|_{\psi_{1}}^{2}}},\frac{c}{\max_{1\leq s\leq t}\|\mathbf{x}_{ s}\|_{\psi_{1}}}\right\},\]

and plugging it into (A.15) gives

\[\mathbb{P}\left(\sum_{s=1}^{t}\mathbf{x}_{s}\geq\frac{1}{c}\sqrt{ \log(1/\delta)\sum_{s=1}^{t}\|\mathbf{x}_{s}\|_{\psi_{1}}^{2}}+c\log(1/\delta) \max_{1\leq s\leq t}\|\mathbf{x}_{s}\|_{\psi_{1}}\right)\leq\delta.\]

Similarly, we can also prove

\[\mathbb{P}\left(\sum_{s=1}^{t}\mathbf{x}_{s}\leq-\frac{1}{c}\sqrt{ \log(1/\delta)\sum_{s=1}^{t}\|\mathbf{x}_{s}\|_{\psi_{1}}^{2}}-c\log(1/\delta) \max_{1\leq s\leq t}\|\mathbf{x}_{s}\|_{\psi_{1}}\right)\leq\delta.\]

Then the proof is finished. 

#### a.3.2 Scales of Sub-exponential Norm for Noises in Stochastic Gradients

For any \((\boldsymbol{v},\boldsymbol{a})\) with \(\boldsymbol{w}=\boldsymbol{v}/\|\boldsymbol{v}\|\), we denote

\[\boldsymbol{\xi}_{(1)} =\left(\sum_{i=1}^{k}a_{i}^{*}\sigma(\mathbf{Z}_{i}^{\top} \boldsymbol{w})\right)\left(\sum_{i=1}^{k}a_{i}\mathbf{Z}_{i}\mathbf{1}\left\{ \mathbf{Z}_{i}^{\top}\boldsymbol{w}\geq 0\right\}\right)\] \[\qquad\qquad-\mathbb{E}\left[\left(\sum_

[MISSING_PAGE_FAIL:21]

#### a.3.3 Corollaries of Lemma 11.

Let \(\{\alpha_{l}\}_{l\geq 0}\) be a sequence of real numbers. Recall that \(\mathbf{\xi}_{t}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{P}_{t}^{i}\mathbf{\xi}_{t}^{i}\) and \(\mathbf{\epsilon}_{t}=\sum_{i=1}^{N}\mathbf{\epsilon}_{t}^{i}/N\), where \(\{\mathbf{\xi}_{t}^{i}\}_{i\in[N]}\) and \(\{\mathbf{\epsilon}_{t}^{i}\}_{i\in[N]}\) are independent random variables given \(\mathcal{F}_{t}\). Applying Lemma 10 and Lemma 11(1), we can directly get the following results

\[\left|\sum_{l=0}^{s-1}\alpha_{l}\mathbf{a}_{l}^{\top}\mathbf{\epsilon}_{t}\right| \leq\frac{c}{N}\sqrt{\log(2/\delta)\sum_{l=0}^{s-1}\alpha_{l}^{2} \sum_{i=1}^{N}\|\mathbf{a}_{l}\|^{2}(\|\mathbf{a}^{*}\|\vee\|\mathbf{a}_{l}^{i}\|)^{2}}\] \[+\frac{c\log(2/\delta)}{N}\max_{l\leq s-1,i\in[N]}\left\{\alpha_{l }\|\mathbf{a}_{l}\|\left(\|\mathbf{a}^{*}\|\vee\|\mathbf{a}_{l}^{i}\|\right)\right\},\] (A.17)

and

\[\left|\sum_{l=0}^{s-1}\alpha_{l}\mathbf{1}^{\top}\mathbf{\epsilon}_{l}\right| \leq\frac{c}{N}\sqrt{k\log(2/\delta)\sum_{l=0}^{s-1}\alpha_{l}^{2} (\|\mathbf{a}^{*}\|\vee\|\mathbf{a}_{l}\|)^{2}}\] \[+\frac{c\sqrt{k}\log(2/\delta)}{N}\max_{l\leq s-1,i\in[N]}\left\{ \alpha_{l}\left(\|\mathbf{a}^{*}\|\vee\|\mathbf{a}_{l}^{i}\|\right)\right\}.\] (A.18)

Specially, choosing \(\mathbf{u}=\mathbf{e}_{j}\) for \(j\in[k]\), we also have

\[\|\mathbf{\epsilon}_{s}\|^{2}\leq k\max_{j\in[k]}|\mathbf{e}_{j}^{\top}\mathbf{\epsilon}_{ s}|^{2}\leq\frac{ck\log(k/\delta)}{N}\max_{i\in[N]}\left\{\left(\|\mathbf{a}^{*}\| \vee\|\mathbf{a}_{s}^{i}\|\right)^{2}\right\},\] (A.19)

hold with probability at least \(1-\delta\).

Similarly, using Lemma 11(2), we also have

\[\left|\sum_{l=0}^{s-1}\mathbf{\xi}_{l}^{\top}\mathbf{w}^{*}\right|\leq\frac{c\|\mathbf{a}^ {*}\|}{N\|\mathbf{v}\|}\sqrt{\log(2/\delta)\sum_{l=0}^{s-1}\sum_{i=1}^{N}\|\mathbf{a}_{ s}^{i}\|^{2}}+\frac{c\log(2/\delta)}{N\|\mathbf{v}\|}\|\mathbf{a}^{*}\|\max_{i\in[N]}\|,\] (A.20)

and

\[\|\mathbf{\xi}_{s}\|^{2}\leq\frac{cd\log(d/\delta)}{N\|\mathbf{v}\|}\|\mathbf{a}^{*}\|^{2} \max_{i\in[N]}\|\mathbf{a}_{s}^{i}\|^{2},\] (A.21)

hold with probability at least \(1-\delta\).

## Appendix B Bound the Weight's Norm during Training Process

### The Scale of the Second Layer's Weight

**Lemma 12** (The scale of the second layer's weight).: _Suppose the learning rate satisfies that_

\[c\max\left\{\eta(k\lor I)\log(k/\eta\delta),\sqrt{\frac{\eta k\log(k/\eta \delta)}{N}}\right\}<1,\] (B.1)

_for a large absolute constant \(c\). Then for any \(0\leq t\leq(\eta\|\mathbf{a}^{*}\|^{2})^{-2}\), we are guaranteed that_

\[\max_{i\in[N]}\|\mathbf{a}_{t}^{i}\|\leq K_{a}:=5\|\mathbf{a}^{*}\|,\quad A_{t}\geq-cK_ {a}\sqrt{\frac{\eta\log(1/\eta\delta)}{N}},\]

_with probability at least \(1-\delta\)._

Proof.: Recall the definition \(\mathbf{a}_{t}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{a}_{t}^{i}\), \(\mathbf{\epsilon}_{t}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{\epsilon}_{t}^{i}\) and

\[\nabla_{\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})=\frac{1}{2\pi}\left(\mathbf{1} \mathbf{1}^{\top}+(\pi-1)\mathbf{I}\right)\mathbf{a}_{t}^{i}-\frac{1}{2\pi}( \mathbf{1}\mathbf{1}^{\top}+(g(\phi_{t}^{i})-1)\mathbf{I})\mathbf{a}^{*}.\]Then notice that

\[\left\|\frac{1}{N}\sum_{i=1}^{N}\nabla_{\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a} _{t}^{i};\mathbf{\xi}_{t}^{i})\right\|^{2}\leq 2\left\|\frac{1}{N}\sum_{i=1}^{N}\nabla_{ \mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\right\|^{2}+2\left\|\frac{1}{N}\sum_{i =1}^{N}\mathbf{\epsilon}_{t}^{i}\right\|^{2}\] \[\qquad=2\left\|\frac{1}{2\pi}\left\{(\mathbf{1}\mathbf{1}^{\top}+(\pi-1) \mathbf{I})\mathbf{a}_{t}-\frac{1}{N}\sum_{i=1}^{N}(\mathbf{1}\mathbf{1}^{\top}+\left(g( \phi_{t}^{i})-1\right)\mathbf{I})\mathbf{a}^{*}\right\}\right\|^{2}+2\left\|\mathbf{ \epsilon}_{t}\right\|^{2}\] \[\qquad\leq\frac{(k+\pi-1)^{2}}{\pi^{2}}\left(\left\|\mathbf{a}_{t} \right\|^{2}+\left\|\mathbf{a}^{*}\right\|^{2}\right)+2\left\|\mathbf{\epsilon}_{t} \right\|^{2},\]

where the last inequality holds since \(\left\|\mathbf{1}\mathbf{1}^{\top}+(\pi-1)\mathbf{I}\right\|\leq\pi+k-1\) and \(g(\phi_{t}^{i})\leq\pi\). Denote \(A_{t}=(\mathbf{1}^{\top}\mathbf{a}_{t})^{2}-(\mathbf{1}^{\top}\mathbf{a}_{t})(\mathbf{1}^{\top}\bm {a}^{*})\). Together with the definition of \(\mathbf{a}_{t+1}\), we have

\[\left\|\mathbf{a}_{t+1}\right\|^{2} =\left\|\mathbf{a}_{t}-\eta\frac{1}{N}\sum_{i=1}^{N}\nabla_{\mathbf{a}}L( \mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i};\mathbf{\xi}_{t}^{i})\right\|^{2}\] \[=\left\|\mathbf{a}_{t}\right\|^{2}-2\eta\frac{1}{N}\sum_{i\in[N]} \left\langle\nabla_{\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i}),\mathbf{a}_{t}\right\rangle -2\eta\mathbf{a}_{t}^{\top}\mathbf{\epsilon}_{t}+\eta^{2}\left\|\frac{1}{N}\sum_{i=1}^{ N}\nabla_{\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i};\mathbf{\xi}_{t}^{i})\right\|^{2}\] \[=\left\|\mathbf{a}_{t}\right\|^{2}-\frac{\eta}{\pi}\mathbf{a}_{t}^{\top} \left(\mathbf{1}\mathbf{1}^{\top}+(\pi-1)\mathbf{I}\right)\mathbf{a}_{t}+\frac{\eta}{\pi} \frac{1}{N}\sum_{i=1}^{N}\mathbf{a}_{t}^{\top}\left(\mathbf{1}\mathbf{1}^{\top}+\left(g( \phi_{t}^{i})-1\right)\mathbf{I}\right)\mathbf{a}^{*}\] \[\qquad-2\eta\mathbf{a}_{t}^{\top}\mathbf{\epsilon}_{t}+\eta^{2}\left\| \frac{1}{N}\sum_{i=1}^{N}\nabla_{\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i};\mathbf{ \xi}_{t}^{i})\right\|^{2}\] \[\leq\left(1-\frac{\eta(\pi-1)}{2\pi}\right)\left\|\mathbf{a}_{t} \right\|^{2}-\frac{\eta}{\pi}A_{t}+\frac{\eta(\pi-1)}{2\pi}\|\mathbf{a}_{t}\|\|\mathbf{ a}^{*}\|-2\eta\mathbf{a}_{t}^{\top}\mathbf{\epsilon}_{t}+\eta^{2}\left\|\frac{1}{N}\sum_{i=1}^{ N}\nabla_{\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i};\mathbf{\xi}_{t}^{i})\right\|^{2}\] \[\leq\left(1-\frac{\eta(\pi-1)}{2\pi}\right)^{t+1}\|\mathbf{a}_{0}\|^ {2}+\frac{\eta(\pi-1)}{2\pi}\sum_{s=0}^{t}\left(1-\frac{\eta(\pi-1)}{2\pi} \right)^{t-s}\|\mathbf{a}_{s}\|\|\mathbf{a}^{*}\|\] \[\qquad-\frac{\eta}{\pi}\sum_{s=0}^{t}\left(1-\frac{\eta(\pi-1)}{2 \pi}\right)^{t-s}A_{s}+2\eta\left|\sum_{s=0}^{t}\left(1-\frac{\eta(\pi-1)}{2 \pi}\right)^{t-s}\mathbf{a}_{s}^{\top}\mathbf{\epsilon}_{s}\right|\] \[\qquad+2\eta^{2}\sum_{s=0}^{t}\left(1-\frac{\eta(\pi-1)}{2\pi} \right)^{t-s}\left(\frac{(k+\pi-1)^{2}}{\pi^{2}}\left(\|\mathbf{a}_{s}\|^{2}+\|\mathbf{ a}^{*}\|^{2}\right)+\|\mathbf{\epsilon}_{s}\|^{2}\right).\] (B.2)

Denote the event

\[\mathcal{E}_{t}=\left\{\forall s\leq t:\,\max_{i\in[N]}\|\mathbf{a}_{s}^{i}\|\leq 5 \|\mathbf{a}^{*}\|\right\}.\]

Let \(\alpha_{l}=\left(1-\eta\frac{\pi-1}{2\pi}\right)^{l}\), then it holds that \(\sum_{l=0}^{s-1}\alpha_{l}^{2}\leq\sum_{l=0}^{s-1}\alpha_{l}\leq\frac{2\pi}{ \eta(\pi-1)}\). Using concentration (A.17), we have

\[\mathds{1}_{\mathcal{E}_{t}}\eta\left|\sum_{s=0}^{t}\alpha_{s}\bm {a}_{s}^{\top}\mathbf{\epsilon}_{s}\right| \leq\mathds{1}_{\mathcal{E}_{t}}\eta c\sqrt{\frac{\log(1/\delta)} {N}\sum_{s=0}^{t}\alpha_{s}\|\mathbf{a}_{s}\|^{2}(\|\mathbf{a}^{*}\|\vee\max_{i}\|\mathbf{ a}_{s}^{i}\|)^{2}}\] \[\qquad+\mathds{1}_{\mathcal{E}_{t}}c\frac{\log(1/\delta)}{N}\max_{s \leq t}\alpha_{s}\|\mathbf{a}_{s}\|(\|\mathbf{a}^{*}\|\vee\max_{i}\|\mathbf{a}_{s}^{i}\|)\] \[\leq\mathds{1}_{\mathcal{E}_{t}}\cdot 10c\|\mathbf{a}^{*}\|^{2}\sqrt{ \frac{\eta\log(1/\delta)}{N}},\] (B.3)

holds with probability at least \(1-\delta\). Applying the concentration inequality (A.19) gives

\[\mathds{1}_{\mathcal{E}_{t}}\eta^{2}\sum_{s=0}^{t}\alpha_{s}\|\mathbf{\epsilon}_{t} \|^{2}\leq\mathds{1}_{\mathcal{E}_{t}}\eta^{2}\sum_{s=0}^{t}\alpha_{s}\frac{ck \log(tk/\delta)}{N}(\|\mathbf{a}^{*}\|\vee\max_{i}\|\mathbf{a}_{s}^{i}\|)^{2}\]\[\leq\mathds{1}_{\mathcal{E}_{t}}\cdot\frac{50ck\eta\log(tk/\delta)}{N}\| \boldsymbol{a}^{*}\|^{2},\] (B.4)

holds with probability at least \(1-\delta\). In addition, from (A.13), we know

\[\mathds{1}_{\mathcal{E}_{t}}A_{s} =\mathds{1}_{\mathcal{E}_{t}}\Bigg{\{}\left(1-\eta\frac{\pi+k-1}{ 2\pi}\right)^{s}A_{0}+\frac{\eta(\mathbf{1}^{\top}\boldsymbol{a}^{*})}{2\pi} \sum_{l=0}^{s-1}\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s-1-l}\left(\pi-\frac {1}{N}\sum_{i=1}^{N}g(\phi_{l}^{i})\right)\] \[\qquad+\eta\sum_{l=0}^{s-1}\left(1-\eta\frac{\pi+k-1}{2\pi}\right) ^{s-1-l}(\mathbf{1}^{\top}\boldsymbol{a}^{*})(\mathbf{1}^{\top}\boldsymbol{ \epsilon}_{l})\Bigg{\}}\] \[\geq\mathds{1}_{\mathcal{E}_{t}}\left\{-\eta\left|\sum_{l=0}^{s-1 }\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s-1-l}(\mathbf{1}^{\top}\boldsymbol{ a}^{*})(\mathbf{1}^{\top}\boldsymbol{\epsilon}_{l})\right|\right\}\] \[\geq\mathds{1}_{\mathcal{E}_{t}}\left\{-c\sqrt{\frac{\eta k\log(1 /\delta)}{N}\sum_{l=0}^{s-1}\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s-1-l}( \|\boldsymbol{a}^{*}\|\vee\max_{i}\|\boldsymbol{a}_{l}^{i}\|)^{2}}\right\}\] \[\geq\mathds{1}_{\mathcal{E}_{t}}\cdot-5c\|\boldsymbol{a}^{*}\|^{2 }\sqrt{\frac{\eta\log(1/\delta)}{N}},\] (B.5)

holds with probability at least \(1-\delta\). Plugging (B.3), (B.4) and (B.5) into (B.2), together with \(t\leq\eta^{-2}\), we can get

\[\|\boldsymbol{a}_{t+1}\|^{2}\mathds{1}_{\mathcal{E}_{t}} \leq\|\boldsymbol{a}_{0}\|^{2}+5\|\boldsymbol{a}^{*}\|^{2}+5c\| \boldsymbol{a}^{*}\|^{2}\sqrt{\frac{\eta k\log(1/\eta\delta)}{N}}+10c\| \boldsymbol{a}^{*}\|^{2}\sqrt{\frac{\eta\log(1/\eta\delta)}{N}}\] \[\qquad+4\eta\frac{\pi+k-1}{\pi}\left(25\|\boldsymbol{a}^{*}\|^{2 }+\|\boldsymbol{a}^{*}\|^{2}\right)+\frac{50c\eta k\log(t/\delta)}{N}\| \boldsymbol{a}^{*}\|^{2}\] \[\leq 9\|\boldsymbol{a}^{*}\|^{2},\] (B.6)

where the last inequality holds due to our assumption (B.1) and \(\|\boldsymbol{a}_{0}\|\leq\|\boldsymbol{a}^{*}\|\). By setting \(T=t+1\) and \(K_{a}=5\|\boldsymbol{a}^{*}\|\) in conclusion (1) of Lemma 15, we have

\[\mathds{1}_{\mathcal{E}_{t}}\|\boldsymbol{a}_{t+1}^{i}- \boldsymbol{a}_{t+1}\| \leq\mathds{1}_{\mathcal{E}_{t}}\frac{1}{N}\sum_{j=1}^{N}\| \boldsymbol{a}_{t+1}^{i}-\boldsymbol{a}_{t+1}^{j}\|\] \[\leq\mathds{1}_{\mathcal{E}_{t}}\cdot 10\|\boldsymbol{a}^{*}\|\left(2 \eta I+c\eta\sqrt{kI\log(k/\delta)}\right)\] \[\leq\mathds{1}_{\mathcal{E}_{t}}\cdot 4\|\boldsymbol{a}^{*}\|,\] (B.7)

where we used the assumption (B.1). In conjunction with (B.6), we can verify that

\[\mathbb{P}\left(\mathcal{E}_{t}\cap\{\|\boldsymbol{a}_{t+1}\|>5\| \boldsymbol{a}^{*}\|\}\right) \leq\mathbb{P}\left(\text{One of \eqref{eq:K_a}, \eqref{eq:K_a}, \eqref{eq:K_a}, and \eqref{eq:K_a} does not hold}\right)\] \[\leq 4\delta.\]

Therefore, we have

\[\mathbb{P}\left(\mathcal{E}_{t+1}\right) =\mathbb{P}\left(\mathcal{E}_{t}\cap\{\|\boldsymbol{a}_{t+1}\| \leq 5\|\boldsymbol{a}^{*}\|\}\right)\] \[\geq\mathbb{P}\left(\mathcal{E}_{t}\right)-\mathbb{P}\left( \mathcal{E}_{t}\cap\{\|\boldsymbol{a}_{t+1}\|>5\|\boldsymbol{a}^{*}\|\}\right)\] \[\geq\mathbb{P}\left(\mathcal{E}_{t}\right)-4\delta.\]

Hence by induction, we can verify that \(\mathbb{P}\left(\mathcal{E}_{t}\right)\geq 1-4t\delta\) for any \(t\leq(\eta K_{a}^{2})^{-2}\). By adjusting the level of \(\delta\) and \(\mathbb{P}(\mathcal{E}_{0})=1\) due to \(\|\boldsymbol{a}_{0}\|\leq\|\boldsymbol{a}^{*}\|\), we can finish the proof. 

### The Scale of the First Layer's weight

**Lemma 13** (The scale of the first layer's weight).: _Under the conditions of Lemma 12. Let \(K_{a}=5\|\boldsymbol{a}^{*}\|\), we assume the learning rate satisfies_

\[c\eta\left(1+\sqrt{\frac{I}{d}}\right)\sqrt{dI\log(Nd/\delta)}K_{a}^{2}<1,\]

_for a large absolute constant \(c\). Then for any \(0\leq t\leq(800c\eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4})^{-1}\), we are guaranteed that \(1/2\leq\|\boldsymbol{v}_{t}\|\leq 3\) with probability at least \(1-\delta\)._Proof.: In this proof, we suppose the event Lemma 12 holds and hide the indicator function everywhere. To facilitate the technical presentation, denote \(\tilde{\mathbf{v}}_{t}^{i}=\mathbf{v}_{t}^{i}-\eta\mathbf{P}_{t}^{i}\nabla_{\mathbf{w}}L( \mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\). Define the following event

\[\mathcal{U}_{t}=\left\{\forall s\leq t\leq(800c\eta^{2}(\sqrt{Id}+I)\sqrt{\log( Nd/\delta)}K_{a}^{4})^{-1}:1/2\leq\|\mathbf{v}_{s}\|\leq 3\right\}.\]

_Step 1: show \(\|\mathbf{v}_{t}\|\geq 1/2\) under the event \(\mathcal{U}_{t-1}\)._ From \(\mathbf{v}_{t}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{v}_{t}^{i}\) and \(\mathbf{P}_{t-1}\mathbf{v}_{t-1}=\mathbf{0}\), we have

\[\|\mathbf{v}_{t}\|^{2} =\left\|\mathbf{v}_{t-1}-\eta\frac{1}{N}\sum_{i=1}^{N}\mathbf{P}_{t-1 }^{i}\nabla_{\mathbf{w}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i})\right\|^{2}-\frac{2 \eta}{N}\sum_{i=1}^{N}\left\langle\tilde{\mathbf{v}}_{t-1}^{i},\mathbf{\xi}_{t-1} \right\rangle+\eta^{2}\|\mathbf{\xi}_{t-1}\|^{2}\] \[=\left\|\mathbf{v}_{t-1}-\eta\mathbf{P}_{t-1}\frac{1}{N}\sum_{i=1}^{N }\nabla_{\mathbf{w}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i})\right\|^{2}-\frac{2\eta}{ N}\sum_{i=1}^{N}\left\langle\tilde{\mathbf{v}}_{t-1}^{i},\mathbf{\xi}_{t-1} \right\rangle+\eta^{2}\|\mathbf{\xi}_{t-1}\|^{2}\] \[\quad+2\eta\left\langle\mathbf{v}_{t-1}-\eta\mathbf{P}_{t-1}\frac{1}{ N}\sum_{i=1}^{N}\nabla_{\mathbf{w}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i}),\frac{1}{N} \sum_{i=1}^{N}(\mathbf{P}_{t-1}-\mathbf{P}_{t-1}^{i})\nabla_{\mathbf{w}}L(\mathbf{w}_{ t-1}^{i},\mathbf{a}_{t-1}^{i})\right\rangle\] \[\quad+\eta^{2}\left\|\frac{1}{N}\sum_{i=1}^{N}(\mathbf{P}_{t-1}- \mathbf{P}_{t-1}^{i})\nabla_{\mathbf{w}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i})\right\| ^{2}\] \[\geq\|\mathbf{v}_{t-1}\|^{2}-\frac{2\eta}{N}\sum_{i=1}^{N}\left\langle \tilde{\mathbf{v}}_{t-1}^{i},\mathbf{\xi}_{t-1}\right\rangle-4\eta\|\mathbf{v}_{t-1}\|^{2} \frac{1}{N}\sum_{i=1}^{N}\|\mathbf{P}_{t-1}-\mathbf{P}_{t-1}^{i}\|\|\nabla_{ \mathbf{w}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i})\|\] \[\quad-2\eta^{2}\frac{1}{N}\sum_{i=1}^{N}\|\mathbf{P}_{t-1}\|\left\| \nabla_{\mathbf{w}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i})\right\|\frac{1}{N}\sum_{i= 1}^{N}\left\|\mathbf{P}_{t-1}-\mathbf{P}_{t-1}^{i}\right\|\left\|\nabla_{\mathbf{w }}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i})\right\|,\] (B.8)

where the inequality follows from dropping positive terms and the inductive assumption \(\|\mathbf{v}_{t-1}\|\geq 1/2\). With probability at least \(1-\delta\), it holds that

\[\mathds{1}_{\mathcal{U}_{t-1}} \|\mathbf{P}_{t-1}^{i}-\mathbf{P}_{t-1}\|=\mathds{1}_{\mathcal{U}_ {t-1}}\left(\left\|\frac{1}{\|\mathbf{v}_{t-1}^{i}\|}\left(\mathbf{I}-\mathbf{w}_{t-1 }^{i}(\mathbf{w}_{t-1}^{i})^{\top}\right)-\frac{1}{\|\mathbf{v}_{t-1}\|}\left(\mathbf{I }-\mathbf{w}_{t-1}\mathbf{w}_{t-1}^{\top}\right)\right\|\right)\] \[\leq\mathds{1}_{\mathcal{U}_{t-1}}\left\{\left|\frac{1}{\|\mathbf{v}_ {t-1}^{i}\|}-\frac{1}{\|\mathbf{v}_{t-1}\|}\right|+\frac{1}{\|\mathbf{v}_{t-1}\|}\|\mathbf{w }_{t-1}^{i}(\mathbf{w}_{t-1}^{i})^{\top}-\mathbf{w}_{t-1}\mathbf{w}_{t-1}^{\top}\|\right\}\] \[\overset{(i)}{\leq}\mathds{1}_{\mathcal{U}_{t-1}}\left\{\left\| \frac{1}{\|\mathbf{v}_{t-1}^{i}\|}-\frac{1}{\|\mathbf{v}_{t-1}\|}\right|+\frac{2\|\mathbf{w }_{t-1}^{i}-\mathbf{w}_{t-1}\|}{\|\mathbf{v}_{t-1}\|}\right\}\] \[\overset{(ii)}{\leq}\mathds{1}_{\mathcal{U}_{t-1}}\left\{\left\| \frac{1}{\|\mathbf{v}_{t-1}^{i}\|}-\frac{1}{\|\mathbf{v}_{t-1}\|}\right|+4\left\|\frac{ \mathbf{v}_{t-1}^{i}}{\|\mathbf{v}_{t-1}^{i}\|}-\frac{\mathbf{v}_{t-1}}{\|\mathbf{v}_{t-1}\|} \right\|\right\}\] \[\leq\mathds{1}_{\mathcal{U}_{t-1}}\left\{\left\|\frac{1}{\|\mathbf{v} _{t-1}^{i}\|}-\frac{1}{\|\mathbf{v}_{t-1}\|}\right|+\frac{4}{\|\mathbf{v}_{t-1}^{i}\|} \|\mathbf{v}_{t-1}^{i}-\mathbf{v}_{t-1}\|+4\|\mathbf{v}_{t-1}\|\left\|\frac{1}{\|\mathbf{v}_{t-1 }^{i}\|}-\frac{1}{\|\mathbf{v}_{t-1}\|}\right\|\right\}\] \[\overset{(iii)}{\leq}\mathds{1}_{\mathcal{U}_{t-1}}\left\{\frac{2 \|\mathbf{v}_{t-1}^{i}-\mathbf{v}_{t-1}\|}{\|\mathbf{v}_{t-1}^{i}\|}+\frac{8\|\mathbf{v}_{t-1}^{ i}-\mathbf{v}_{t-1}\|}{\|\mathbf{v}_{t-1}^{i}\|}\right\}\] \[\overset{(iv)}{\leq}\mathds{1}_{\mathcal{U}_{t-1}}\left\{\frac{10c \eta(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{2}}{1/2-6c\eta(\sqrt{Id}+I)\sqrt{ \log(Nd/\delta)}K_{a}^{2}}\right\}\] \[\overset{(v)}{\leq}\mathds{1}_{\mathcal{U}_{t-1}}\cdot 40c\eta( \sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{2},\] (B.9)

where \((i)\) holds due to \(\|\mathbf{w}_{t-1}^{i}\|=\|\mathbf{w}_{t-1}\|=1\); \((ii)\) and \((iii)\) hold due to the inductive assumption \(\|\mathbf{v}_{t-1}\|\geq 1/2\); \((iv)\) follows from the conclusion (1) in Lemma 14; and \((v)\) is true because \(6c\eta(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}\leq 1/4\). Substituting (B) into (B), with probability at least \(1-2\delta\), we have

\[\mathds{1}_{\mathcal{U}_{t-1}}\|\mathbf{v}_{t}\|^{2}\geq\mathds{1}_{\mathcal{U}_{t-1 }}\Biggl{\{}\|\mathbf{v}_{t-1}\|^{2}-\frac{2\eta}{N}\sum_{i=1}^{N}\left\langle\tilde{ \mathbf{v}}_{t-1}^{i},\mathbf{\xi}_{t-1}\right\rangle\]\[-160c\eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{2}\cdot\| \boldsymbol{v}_{t-1}\|^{2}\cdot\frac{1}{N}\sum_{i=1}^{N}\|\nabla_{\boldsymbol{w} }L(\boldsymbol{w}_{t-1}^{i},\boldsymbol{a}_{t-1}^{i})\|\] \[-80\eta^{3}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{2}\cdot\| \boldsymbol{\mathrm{P}}_{t-1}\|\cdot\left(\frac{1}{N}\sum_{i=1}^{N}\|\nabla_{ \boldsymbol{w}}L(\boldsymbol{w}_{t-1}^{i},\boldsymbol{a}_{t-1}^{i})\|\right)^{2}\Bigg{\}}\] \[\stackrel{{(i)}}{{\geq}}\mathds{1}_{\mathcal{U}_{t-1 }}\Bigg{\{}\left(1-800c\eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4} \right)\|\boldsymbol{v}_{t-1}\|^{2}-\frac{2\eta}{N}\sum_{i=1}^{N}\big{<}\tilde{ \boldsymbol{v}}_{t-1}^{i},\boldsymbol{\xi}_{t-1}\big{>}\Bigg{\}}\] \[\quad-1000\eta^{3}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{6}\] \[\stackrel{{(ii)}}{{\geq}}\mathds{1}_{\mathcal{U}_{t- 1}}\Bigg{\{}\left(1-800c\eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4} \right)^{t}\|\boldsymbol{v}_{0}\|\] \[\quad-2\eta\left|\sum_{s=0}^{t-1}\left(1-800c\eta^{2}(\sqrt{Id}+I) \sqrt{\log(Nd/\delta)}K_{a}^{4}\right)^{t-s}\Bigg{\langle}\frac{1}{N}\sum_{i=1 }^{N}\tilde{\boldsymbol{v}}_{s}^{i},\boldsymbol{\xi}_{s}\Bigg{\rangle}\right|\] \[\quad-1000\eta^{3}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{6}\sum _{s=0}^{t-1}\left(1-800c\eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4} \right)^{t-s}\Bigg{\}}\] \[\stackrel{{(iii)}}{{\geq}}\mathds{1}_{\mathcal{U}_{t -1}}\Bigg{\{}0.36-\frac{5}{4}\eta K_{a}^{2}-2\eta\left|\sum_{s=0}^{t-1}\left(1 -800c\eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4}\right)^{t-s}\Bigg{ }\Bigg{\langle}\frac{1}{N}\sum_{i=1}^{N}\tilde{\boldsymbol{v}}_{s}^{i}, \boldsymbol{\xi}_{s}\Bigg{\rangle}\right|\Bigg{\}},\] (B.10)

where \((i)\) holds because \(\|\nabla_{\boldsymbol{w}}L(\boldsymbol{w}_{t}^{i},\boldsymbol{v}_{t}^{i})\|\leq |(\boldsymbol{a}_{t}^{i})^{\top}\boldsymbol{a}^{*}|\leq 5K_{a}^{2}\) by using Lemma 12 and \(\|\boldsymbol{\mathrm{P}}_{t-1}\|\leq\frac{1}{\|\boldsymbol{v}_{t-1}\|}\leq 2\) holds under the event \(\mathcal{U}_{t-1}\); \((ii)\) holds due to \(\|\boldsymbol{v}_{0}\|=1\); and \((iii)\) follows from \(t\leq(800c\eta^{2}(d\lor I)K_{a}^{4}\sqrt{\log(Nd/\delta)})^{-1}\) such that

\[\left(1-800c\eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4} \right)^{t}\] \[\quad\geq\left(1-800c\eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)} K_{a}^{4}\right)^{\frac{1}{800c\eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4}}}\] \[\quad\geq\left(1-0.01\right)^{100}\geq 0.36.\]

In fact, the bound above holds because \(800c\eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4}\leq 0.01\) and \((1-x)^{\frac{1}{x}}\) is decreasing in \((0,1)\). According to the definition of \(\tilde{\boldsymbol{v}}_{s}^{i}\) and Lemma 12, we know

\[\mathds{1}_{\mathcal{U}_{t-1}}\left\|\frac{1}{N}\sum_{i=1}^{N}\tilde{ \boldsymbol{v}}_{s}^{i}\right\|=\mathds{1}_{\mathcal{U}_{t-1}}\left\| \boldsymbol{v}_{s}-\eta\frac{1}{N}\sum_{i=1}^{N}\boldsymbol{\mathrm{P}}_{s}^ {i}\nabla_{\boldsymbol{w}}L(\boldsymbol{w}_{s}^{i},\boldsymbol{a}_{s}^{i}) \right\|\leq 3+5\eta K_{a}^{2}.\] (B.11)

Recall that \(\boldsymbol{\xi}_{s}=\frac{1}{N}\sum_{i=1}^{N}\boldsymbol{\mathrm{P}}_{s}^{i} \boldsymbol{\xi}_{s}^{i}\). Invoking Lemmas 11 and 10, we have

\[\mathds{1}_{\mathcal{U}_{t-1}}\eta\left|\sum_{s=0}^{t-1}\left(1-800 \eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4}\right)^{t-1-s}\Bigg{\langle} \frac{1}{N}\sum_{i=1}^{N}\tilde{\boldsymbol{v}}_{s}^{i},\boldsymbol{\xi}_{s} \Bigg{\rangle}\right|\] \[\leq\mathds{1}_{\mathcal{U}_{t-1}}\Bigg{\{}\frac{c\eta K_{a}^{2} \left(3+5\eta K_{a}^{2}\right)}{\sqrt{N}}\sqrt{\log(2/\delta)}\sum_{s=0}^{t-1} \left(1-800\eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4}\right)^{2(t-1-s )}\] \[\quad\quad+\frac{c\eta K_{a}^{2}\log(2/\delta)}{N}\Bigg{\}}\] \[\leq\mathds{1}_{\mathcal{U}_{t-1}}\left\{\frac{c\left(3+5\eta K_{a }^{2}\right)}{20\sqrt{I\log(Nd/\delta)}}\sqrt{\frac{\log(2/\delta)}{N}}+\frac{c \eta K_{a}^{2}\log(2/\delta)}{N}\right\}\] \[\leq\mathds{1}_{\mathcal{U}_{t-1}}\left\{\frac{c\left(3+5\eta K_{a }^{2}\right)}{20\sqrt{NI}}+\frac{c\eta K_{a}^{2}\log(2/\delta)}{N}\right\},\] (B.12)holds with probability at least \(1-\delta\). Plugging (B.12) into (B.10), we get

\[\|\mathbf{v}_{t}\|^{2}\mathds{1}_{\mathcal{U}_{t-1}} \geq\mathds{1}_{\mathcal{U}_{t-1}}\left\{0.36-\frac{5}{4}\eta K_{a} ^{2}-2\left(\frac{c\left(3+5\eta K_{a}^{2}\right)}{20\sqrt{I\log(Nd/\delta)}} \sqrt{\frac{\log(2/\delta)}{N}}+\frac{c\eta K_{a}^{2}\log(2/\delta)}{N}\right)\right\}\] \[\geq\mathds{1}_{\mathcal{U}_{t-1}}\left\{0.36-\frac{1}{16}-\frac {1}{16}\right\}\] \[\geq\mathds{1}_{\mathcal{U}_{t-1}}\cdot\frac{1}{4},\] (B.13)

where the first inequality holds since \(\sqrt{Id\log(Nd/\delta)}\geq c\).

_Step 2: show \(\|\mathbf{v}_{t}\|\leq 3\) under the event \(\mathcal{U}_{t-1}\)._ From the update rule of \(\mathbf{v}_{t}^{i}\) and \(\mathbf{v}_{t}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{v}_{t}^{i}\), with probability at least \(1-\delta\), we have

\[\mathds{1}_{\mathcal{U}_{t-1}}\|\mathbf{v}_{t}\|^{2}=\mathds{1}_{ \mathcal{U}_{t-1}}\Bigg{\{}\left\|\mathbf{v}_{t-1}-\eta\mathbf{P}_{t-1}\frac{1}{N} \sum_{i=1}^{N}\nabla_{\mathbf{w}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i})\right\|^{2}- \frac{2\eta}{N}\sum_{i=1}^{N}\left\langle\tilde{\mathbf{v}}_{t-1}^{i},\mathbf{\xi}_{t- 1}\right\rangle+\eta^{2}\|\mathbf{\xi}_{t-1}\|^{2}\] \[\quad+2\eta\left\langle\mathbf{v}_{t-1}-\eta\mathbf{P}_{t-1}\frac{1}{ N}\sum_{i=1}^{N}\nabla_{\mathbf{w}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i}),\frac{1}{N} \sum_{i=1}^{N}(\mathbf{P}_{t-1}-\mathbf{P}_{t-1}^{i})\nabla_{\mathbf{w}}L(\mathbf{w}_{t -1}^{i},\mathbf{a}_{t-1}^{i})\right\rangle\] \[\quad+\eta^{2}\left\|\frac{1}{N}\sum_{i=1}^{N}(\mathbf{P}_{t-1}- \mathbf{P}_{t-1}^{i})\nabla_{\mathbf{w}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i})\right\| ^{2}\Bigg{\}}\] \[\quad\overset{(i)}{\leq}\mathds{1}_{\mathcal{U}_{t-1}}\Bigg{\{} \|\mathbf{v}_{t-1}\|^{2}+\eta^{2}\left\|\frac{1}{N}\sum_{i=1}^{N}\mathbf{P}_{t-1} \nabla_{\mathbf{w}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i})\right\|^{2}\] \[\quad+\frac{2\eta}{3}\|\mathbf{v}_{t-1}\|^{2}\frac{1}{N}\sum_{i=1}^{N }\|\mathbf{P}_{t-1}-\mathbf{P}_{t-1}^{i}\|\|\nabla_{\mathbf{w}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i})\|\] \[\quad+2\eta^{2}\left\|\frac{1}{N}\sum_{i=1}^{N}\mathbf{P}_{t-1} \nabla_{\mathbf{w}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i})\right\|\left\|\frac{1}{N} \sum_{i=1}^{N}(\mathbf{P}_{t-1}-\mathbf{P}_{t-1}^{i})\nabla_{\mathbf{w}}L(\mathbf{w}_{ t-1}^{i},\mathbf{a}_{t-1}^{i})\right\|\] \[\quad+\eta^{2}\left\|\frac{1}{N}\sum_{i=1}^{N}(\mathbf{P}_{t-1}- \mathbf{P}_{t-1}^{i})\nabla_{\mathbf{w}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i})\right\| ^{2}-\frac{2\eta}{N}\sum_{i=1}^{N}\left\langle\tilde{\mathbf{v}}_{t-1}^{i},\mathbf{\xi} _{t-1}\right\rangle+\eta^{2}\|\mathbf{\xi}_{t-1}\|^{2}\Bigg{\}}\] \[\overset{(ii)}{\leq}\mathds{1}_{\mathcal{U}_{t-1}}\Bigg{\{}\left( 1+5\eta K_{a}^{2}\max_{i}\|\mathbf{P}_{t-1}^{i}-\mathbf{P}_{t-1}\|\right)\|\mathbf{v }_{t-1}\|^{2}+25\eta^{2}K_{a}^{4}+25\eta^{2}K_{a}^{4}\max_{i}\|\mathbf{P}_{t- 1}^{i}-\mathbf{P}_{t-1}\|\] \[\quad+\frac{25\eta^{2}K_{a}^{4}}{4}\max_{i}\|\mathbf{P}_{t-1}^{i}- \mathbf{P}_{t-1}\|^{2}-\frac{2\eta}{N}\sum_{i=1}^{N}\left\langle\tilde{\mathbf{v}} _{t-1}^{i},\mathbf{\xi}_{t-1}\right\rangle+\eta^{2}\|\mathbf{\xi}_{t-1}\|^{2}\Bigg{\}}\] \[\overset{(iii)}{\leq}\mathds{1}_{\mathcal{U}_{t-1}}\Bigg{\{} \left(1+200c\eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4}\right)\|\mathbf{v }_{t-1}\|^{2}+25\eta^{2}K_{a}^{4}\] \[\quad+1000(1+10\eta K_{a}^{2})\eta^{3}K_{a}^{6}(\sqrt{Id}+I)\sqrt {\log(Nd/\delta)}-\frac{2\eta}{N}\sum_{i=1}^{N}\left\langle\tilde{\mathbf{v}}_{t-1 }^{i},\mathbf{\xi}_{t-1}\right\rangle+\eta^{2}\|\mathbf{\xi}_{t-1}\|^{2}\Bigg{\}}\] \[\leq\mathds{1}_{\mathcal{U}_{t-1}}\Bigg{\{}\left(1+200c\eta^{2}( \sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4}\right)^{t}\|\mathbf{v}_{0}\|^{2}\] \[\quad+2000\eta^{3}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{6}\sum _{s=0}^{t-1}\left(1+200c\eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4} \right)^{t-s}\] \[\quad+2\eta\left|\sum_{s=0}^{t-1}\left(1+200c\eta^{2}(\sqrt{Id}+I )\sqrt{\log(Nd/\delta)}K_{a}^{4}\right)^{t-1-s}\left\langle\frac{1}{N}\sum_{i=1 }^{N}\tilde{\mathbf{v}}_{s}^{i},\mathbf{\xi}_{s}\right\rangle\right|\]\[\leq\mathds{1}_{\mathcal{U}_{t-1}}\left\{\frac{c\eta K_{a}^{2}\left(3+5 \eta K_{a}^{2}\right)}{\sqrt{N}}\sqrt{\log(2/\delta)\sum_{s=0}^{t-1}\left(1+200c \eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4}\right)^{2(t-1-s)}}\right.\] \[\qquad+\left.\frac{c\eta K_{a}^{2}\log(2/\delta)}{N}\right\}\] \[\leq\mathds{1}_{\mathcal{U}_{t-1}}\left\{\frac{ce\left(3+5\eta K_{ a}^{2}\right)}{\sqrt{(I\lor d)N}}+\frac{c\eta K_{a}^{2}\log(2/\delta)}{N}\right\},\] (B.16)

Plugging (B.15) and (B.16) into (B.14) gives that

\[\mathds{1}_{\mathcal{U}_{t-1}}\|\mathbf{v}_{t}\|^{2} \leq\mathds{1}_{\mathcal{U}_{t-1}}\left\{e+10e\eta K_{a}^{2}+ \frac{2ce\log(1/\eta\delta)}{N}+2\left(\frac{ce\left(3+5\eta K_{a}^{2}\right) }{\sqrt{(I\lor d)N}}+\frac{c\eta K_{a}^{2}\log(2/\delta)}{N}\right)\right\}\] \[\leq\mathds{1}_{\mathcal{U}_{t-1}}\cdot 9,\]

holds with probability at least \(1-3\delta\). Together with (B.13), we have showed that

\[\mathbb{P}\left(\mathcal{U}_{t-1}\cap\{1/2\leq\|\mathbf{v}_{t}\|\leq 3\}\right) \geq\mathbb{P}\left(\mathcal{U}_{t-1}\right)-\mathbb{P}\left( \mathcal{U}_{t-1}\cap\{\|\mathbf{v}_{t}\|\leq 1/2\}\right)-\mathbb{P}\left( \mathcal{U}_{t-1}\cap\{\|\mathbf{v}_{t}\|>3\}\right)\] \[\geq\mathbb{P}\left(\mathcal{U}_{t-1}\right)-\mathbb{P}\left(\text {One of (B.9), (B.10), (B.13), and Lemma 12 does not hold}\right)\] \[\qquad-\mathbb{P}\left(\text{One of (B.14), (B.15), (B.16), and Lemma 12 does not hold}\right)\] \[\geq\mathbb{P}\left(\mathcal{U}_{t-1}\right)-8\delta.\]

Using induction and \(\mathbb{P}\left(\mathcal{U}_{0}\right)=1\) due to \(\|\mathbf{v}_{0}\|=1\), we can prove the desired conclusion by adjusting the level of \(\delta\). 

**Remark.** In the following proofs, we use \(c\) to denote the absolute constant, which does not depend on any variables of the model or training process. For conciseness, we do not distinguish the specific value of \(c\) in some contexts.

## Appendix C Bound the Discrepancy during Training Process

### The Discrepancy in the First Layer

**Lemma 14** (The discrepancy in the first layer).: _Denote \(O_{r,T}^{i,j}=\max_{t_{r}\leq t\leq T}\|\mathbf{v}_{t}^{i}-\mathbf{v}_{t}^{j}\|\) for \(t_{r}\leq T\leq t_{r+1}-1\). Suppose the learning rate satisfies the conditions in Lemmas 12 and 13.

_If \(\max_{i\in[N],t_{r}\leq t\leq T-1}\|\mathbf{a}_{t}^{i}\|\leq K_{a}\) holds, with probability at least \(1-\delta\), we have_

\[\max_{i,j\in[N]}O_{r,T}^{i,j}\leq c\eta(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^ {2}.\]

_In addition, we also have_

\[\max_{t_{r}\leq t\leq T}|(\mathbf{a}_{t}^{i}-\mathbf{a}_{t}^{j})^{\top}\mathbf{a}^{*}|\leq c \eta(\sqrt{kT}+I)\sqrt{\log(1/\eta\delta)}K_{a}^{2}.\]

Proof.: Denote the discrepancy term by

\[\mathbf{h}_{t}^{i,j} =\mathbf{P}_{t}^{j}\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{j},\mathbf{a}_{t}^{j} )-\mathbf{P}_{t}^{i}\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\] \[=\left[\frac{\pi-\phi_{t}^{j}}{2\pi}\frac{(\mathbf{a}_{t}^{j})^{\top} \mathbf{a}^{*}}{\|\mathbf{v}_{t}^{j}\|}(\mathbf{I}-\mathbf{w}_{t}^{j}(\mathbf{w}_{t}^{j})^{ \top})-\frac{\pi-\phi_{t}^{i}}{2\pi}\frac{(\mathbf{a}_{t}^{i})^{\top}\mathbf{a}^{*}}{\| \mathbf{v}_{t}^{i}\|}(\mathbf{I}-\mathbf{w}_{t}^{i}(\mathbf{w}_{t}^{i})^{\top})\right]\mathbf{ w}^{*}.\] (C.1)

Since \(\|\mathbf{w}^{*}\|=1\) and \(\|\mathbf{a}_{t}^{i}\|\leq 5K_{a}\) in Lemma 12, we can bound \(\mathbf{h}_{t}^{i,j}\) as

\[\|\mathbf{h}_{t}^{i,j}\|\leq \frac{|\phi_{t}^{j}-\phi_{t}^{i}|}{2\pi}\frac{(\mathbf{a}_{t}^{i})^{ \top}\mathbf{a}^{*}}{\|\mathbf{v}_{t}^{i}\|}+\frac{\pi-\phi_{t}^{j}}{2\pi}\frac{|(\mathbf{a }_{t}^{i}-\mathbf{a}_{t}^{j})^{\top}\mathbf{a}^{*}|}{\|\mathbf{v}_{t}^{i}\|}+\frac{\pi-\phi _{t}^{j}}{2\pi}(\mathbf{a}_{t}^{j})^{\top}\mathbf{a}^{*}\left|\frac{1}{\|\mathbf{v}_{t}^{i} \|}-\frac{1}{\|\mathbf{v}_{t}^{j}\|}\right|\] \[+\frac{\pi-\phi_{t}^{i}}{\pi}\frac{(\mathbf{a}_{t}^{j})^{\top}\mathbf{a} ^{*}}{\|\mathbf{v}_{t}^{j}\|}\left|\frac{1}{\|\mathbf{v}_{t}^{i}\|}-\frac{1}{\|\mathbf{v}_ {t}^{j}\|}\right|\] \[\leq 3K_{a}^{2}+|(\mathbf{a}_{t}^{i}-\mathbf{a}_{t}^{j})^{\top}\mathbf{a}^{*}| +6K_{a}^{2}\|\mathbf{v}_{t}^{i}-\mathbf{v}_{t}^{j}\|+5K_{a}^{2}\left(\frac{\|\mathbf{v}_{t} ^{i}-\mathbf{v}_{t}^{j}\|}{\|\mathbf{v}_{t}^{i}\|}+\frac{\|\mathbf{v}_{t}^{i}-\mathbf{v}_{t}^{j }\|}{2\|\mathbf{v}_{t}^{i}\|\|\mathbf{v}_{t}^{j}\|}\right)\] \[\leq 3K_{a}^{2}+|(\mathbf{a}_{t}^{i}-\mathbf{a}_{t}^{j})^{\top}\mathbf{a}^{*}| +10K_{a}^{2}\|\mathbf{v}_{t}^{i}-\mathbf{v}_{t}^{j}\|,\] (C.2)

where we also used the fact \(\max_{i\in[N]}\|\mathbf{I}-\mathbf{w}_{t}^{i}(\mathbf{w}_{t}^{i})^{\top}\|\leq 1\) and

\[\|\mathbf{w}_{t}^{i}(\mathbf{w}_{t}^{i})^{\top}\mathbf{w}^{*}-\mathbf{w}_{t}^{j}( \mathbf{w}_{t}^{j})^{\top}\mathbf{w}^{*}\| \leq\|\mathbf{w}_{t}^{i}(\mathbf{w}_{t}^{i}-\mathbf{w}_{t}^{j})^{\top}\mathbf{w}^{ *}\|+\|(\mathbf{w}_{t}^{i}-\mathbf{w}_{t}^{j})(\mathbf{w}_{t}^{j})^{\top}\mathbf{w}^{*}\|\] \[\leq 2\left\|\mathbf{w}_{t}^{i}-\mathbf{w}_{t}^{j}\right\|\] \[=2\left|\frac{1}{\|\mathbf{v}_{t}^{i}\|}-\frac{1}{\|\mathbf{v}_{t}^{j}\|} \right|.\]

From the definition of \(B_{s}^{i}\) and \(B_{s}^{j}\) in Lemma 2, we know for any \(i,j\in[N]\),

\[\left|B_{s}^{i}-B_{s}^{j}\right| \leq\left|g(\phi_{s}^{j})-g(\phi_{s}^{i})\right|+\frac{\eta}{2\pi} \frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{\|\mathbf{a}^{*}\|^{2}}\sum_{l=t_{r}}^{s- 1}\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s-1-l}\left|g(\phi_{l}^{j})-g(\phi _{l}^{i})\right|\] \[\leq\pi+\frac{\eta I}{2}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}} {\|\mathbf{a}^{*}\|^{2}},\]

where the last inequality holds due to \(g(\phi)\in[0,\pi]\) for \(\phi\in[0,\pi]\). Using concentration (A.18) and \(t\leq k\eta^{-1}\), with probability at least \(1-\delta\), we can guarantee

\[|S(\mathbf{\epsilon}_{t_{r}:s-1})| =\frac{\eta^{2}|\mathbf{1}^{\top}\mathbf{a}^{*}|}{2\pi}\left|\sum_{l=t_ {r}}^{s-1}\left(1-\frac{2\eta(\pi-1)}{\pi}\right)^{s-l-1}\sum_{m=t_{r}}^{l-1} \left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{l-1-m}(\mathbf{1}^{\top}\mathbf{\epsilon}_ {m})\right|\] \[\qquad+\eta\left|\sum_{j=t_{r}}^{s-1}\left(1-\frac{2\eta(\pi-1)}{ \pi}\right)^{s-1-j}\mathbf{\epsilon}_{j}^{\top}\mathbf{a}^{*}\right|\] \[\leq c\eta I\sqrt{\frac{\log(2t/\delta)}{N}}K_{a}^{2}\cdot\left( \eta\frac{|\mathbf{1}^{\top}\mathbf{a}^{*}|}{\|\mathbf{a}^{*}\|}\sum_{l=t_{r}}^{s-1} \left(1-\frac{2\eta(\pi-1)}{\pi}\right)^{s-l-1}+1\right)\] \[\leq 2c\eta I\sqrt{\frac{k\log(2t/\delta)}{N}}K_{a}^{2}.\] (C.3)Using the dynamic of \(\mathbf{a}_{t}^{j}\mathbf{a}^{*}\) in Lemma 2, with probability at least \(1-\delta\), we also have

\[\left|(\mathbf{a}_{t-1}^{j}-\mathbf{a}_{t-1}^{i})^{\top}\mathbf{a}^{*}\right| =\left|\frac{\eta\|\mathbf{a}^{*}\|^{2}}{2\pi}\sum_{s=t_{r}}^{t-2} \left(1-\eta\frac{\pi-1}{2\pi}\right)^{t-2-s}\left(B_{s}^{j}-B_{s}^{i}\right)+S (\mathbf{\epsilon}_{t_{r}:t-2}^{j})-S(\mathbf{\epsilon}_{t_{r}:t-2}^{i})\right|\] \[\leq\frac{\eta I\|\mathbf{a}^{*}\|^{2}}{2\pi}\left(\pi+\frac{\eta I}{2 }\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{\|\mathbf{a}^{*}\|^{2}}\right)+\left|S(\mathbf{ \epsilon}_{t_{r}:t-2}^{j})-S(\mathbf{\epsilon}_{t_{r}:t-2}^{i})\right|\] \[\leq\frac{\eta I}{2\pi}\left(\pi\|\mathbf{a}^{*}\|^{2}+\frac{\eta I}{ 2}(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}\right)+2c\eta\sqrt{kI\log(t/\delta)}K_{a}^{2}\] \[\leq 2\eta(\sqrt{kI}+I)\sqrt{\log(1/\eta\delta)}K_{a}^{2},\] (C.4)

where the second last inequality holds due to the concentration (C.3). Plugging (C.4) into (C.2) gives

\[\|\mathbf{v}_{t}^{i}-\mathbf{v}_{t}^{j}\| \leq\eta\left\|\sum_{s=t_{r}}^{t-1}\mathbf{h}_{s}^{i,j}\right\|+\eta \left\|\sum_{s=t_{r}}^{t-1}\mathbf{\mathrm{P}}_{s}^{i}\mathbf{\mathrm{\xi}}_{s}^{i}-\bm {\mathrm{P}}_{s}^{j}\mathbf{\mathrm{\xi}}_{s}^{j}\right\|\] \[\leq\eta\sum_{s=t_{r}}^{t-1}\left(3K_{a}^{2}+|(\mathbf{a}_{s}^{i}-\mathbf{ a}_{s}^{j})^{\top}\mathbf{a}^{*}|+10K_{a}^{2}\|\mathbf{v}_{s}^{i}-\mathbf{v}_{s}^{j}\| \right)+\eta\left\|\sum_{s=t_{r}}^{t-1}\mathbf{\mathrm{P}}_{s}^{i}\mathbf{\mathrm{\xi }}_{s}^{i}\right\|+\eta\left\|\sum_{s=t_{r}}^{t-1}\mathbf{\mathrm{P}}_{s}^{j}\mathbf{ \mathrm{\xi}}_{s}^{j}\right\|\] \[\leq\eta I\left(3K_{a}^{2}+2\eta(I+\sqrt{kI})K_{a}^{2}+10K_{a}^{2 }O_{r,T}^{i,j}\right)+4c\eta\sqrt{dI\log(N/\delta)}K_{a}^{2},\] (C.5)

where we used the following concentration

\[\max_{i}\left\|\sum_{s=t_{r}}^{t-1}\mathbf{\mathrm{P}}_{s}^{i}\mathbf{ \mathrm{\xi}}_{s}^{i}\right\|\leq\sqrt{d}\max_{i\in[N],\ell\in[d]}\left|(\mathbf{ \mathrm{P}}_{s}^{i}\mathbf{\mathrm{\xi}}_{s}^{i})_{\ell}\right|\leq c\sqrt{dI\log( dN/\delta)}K_{a}^{2}.\]

Taking maximum on the left hand side of (C.5) over \(t_{r}\leq t\leq T\), we can get

\[O_{r,T}^{i,j} \leq\left(1-10\eta IK_{a}^{2}\right)\left[\eta I\left(3K_{a}^{2}+ 2\eta(I+\sqrt{kI})K_{a}^{2}\right)+4c\eta\sqrt{dI\log(N/\delta)}K_{a}^{2}\right]\] \[\leq 2\left(4\sqrt{\frac{I}{d\log(Nd/\delta)}}+4c\right)\eta\sqrt{ dI\log(Nd/\delta)}K_{a}^{2}\] \[\leq 2\left(5c+4\sqrt{\frac{I}{d}}\right)\eta\sqrt{dI\log(Nd/ \delta)}K_{a}^{2},\]

where we also used the assumption \(\eta(I+\sqrt{kI})<1\). 

### The Discrepancy in the Second Layer

**Lemma 15** (The discrepancy in the second layer).: _Let \(\Xi_{r,T}^{i,j}=\max_{t_{r}\leq t\leq T}\|\mathbf{a}_{t}^{i}-\mathbf{a}_{t}^{j}\|\) for \(t_{r}\leq T\leq t_{r+1}-1\). Suppose the learning rate satisfies the conditions in Lemmas 12 and 13. If \(\max_{i\in[N],t_{r}\leq t\leq T-1}\|\mathbf{a}_{t}^{i}\|\leq K_{a}\) holds for \(K_{a}>\|\mathbf{a}^{*}\|\) and \(c\eta kI<1\), with probability at least \(1-\delta\), we have_

\[\max_{i,j}\Xi_{r,T}^{i,j}\leq 2K_{a}\left(2\eta I+c\eta\sqrt{kI\log(k/ \delta)}\right).\]

Proof.: Recall the closed form of \(\nabla_{\mathbf{a}}L(\mathbf{w},\mathbf{a})\), then for \(i,j\in[N]\), we have

\[\nabla_{\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})-\nabla_{\mathbf{a}}L( \mathbf{w}_{t}^{j},\mathbf{a}_{t}^{j})=\frac{\eta}{2\pi}\left(\mathbf{1}\mathbf{1}^{\top}+( \pi-1)\mathbf{\mathrm{I}}\right)(\mathbf{a}_{t}^{i}-\mathbf{a}_{t}^{j})-\frac{\eta}{2\pi} \left(g(\phi_{t}^{i})-g(\phi_{t}^{j})\right)\mathbf{a}^{*}.\]

Let \(t_{r}\) be the previous synchronization time before \(t\). Using the fact \(\mathbf{a}_{t_{r}}^{i}=\mathbf{a}_{t_{r}}^{j}=\mathbf{a}_{t_{r}}\), we have

\[\|\mathbf{a}_{t}^{i}-\mathbf{a}_{t}^{j}\|\leq\eta\left\|\sum_{s=t_{r}}^{t-1}\nabla_{\bm {a}}L(\mathbf{w}_{s}^{i},\mathbf{a}_{s}^{i})-\nabla_{\mathbf{a}}L(\mathbf{w}_{s}^{j},\mathbf{a}_{s}^ {j})\right\|+\eta\left\|\sum_{s=t_{r}}^{t-1}\mathbf{\mathrm{\epsilon}}_{s}^{i}-\bm {\mathrm{\epsilon}}_{s}^{j}\right\|\]\[\leq\frac{\eta(\pi+k-1)}{2\pi}\sum_{s=t_{r}}^{t-1}\|\mathbf{a}_{s}^{i}-\mathbf{ a}_{s}^{j}\|+\frac{\eta\|\mathbf{a}^{*}\|}{2\pi}\sum_{s=t_{r}}^{t-1}\left|g(\phi_{t-1}^{i })-g(\phi_{t-1}^{j})\right|+\eta\left\|\sum_{s=t_{r}}^{t-1}\mathbf{\epsilon}_{s}^{i }-\mathbf{\epsilon}_{s}^{j}\right\|\] \[\leq\frac{\eta(\pi+k-1)}{2\pi}\sum_{s=t_{r}}^{t-1}\|\mathbf{a}_{s}^{i }-\mathbf{a}_{s}^{j}\|+\frac{\eta\|\mathbf{a}^{*}\|}{2\pi}\pi I+c\sqrt{kI\log(k/\delta) }K_{a},\] (C.6)

where the last inequality holds due to Lemma 11(1). Taking maximum on the both sides over \(t_{r}\leq t\leq T\), we get

\[\Xi_{r,T}^{i,j} \leq\left(1-\frac{\eta(\pi+k-1)I}{2\pi}\right)^{-1}\left[\frac{ \pi\eta I}{2}\|\mathbf{a}^{*}\|+c\eta\sqrt{kI\log(k/\delta)}K_{a}\right]\] \[\leq 2K_{a}\left(2\eta I+c\eta\sqrt{kI\log(k/\delta)}\right).\]

## Appendix D Self-correction of Signals

Recalling the dynamic in Lemma 2,

\[\mathbf{a}_{t}^{\top}\mathbf{a}^{*}= \left(1-\eta\frac{\pi-1}{2\pi}\right)^{t}\mathbf{a}_{0}^{\top}\mathbf{a}^ {*}+\frac{1-\left(\frac{2\pi-\eta(k+\pi-1)}{2\pi-\eta(\pi-1)}\right)^{t}}{k} \left(1-\eta\frac{\pi-1}{2\pi}\right)^{t}A_{0}\] \[+\frac{\eta\|\mathbf{a}^{*}\|^{2}}{2\pi}\sum_{s=0}^{t-1}\left(1-\eta \frac{\pi-1}{2\pi}\right)^{t-1-s}\frac{1}{N}\sum_{i=1}^{N}B_{s}^{i}+S(\mathbf{ \epsilon}_{0:t-1}),\] (D.1)

where

\[B_{s}^{i}=g(\phi_{s}^{i})-1+\frac{\eta}{2\pi}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{ 2}}{\|\mathbf{a}^{*}\|^{2}}\sum_{l=0}^{s-1}\left(1-\eta\frac{\pi+k-1}{2\pi}\right) ^{s-1-l}(\pi-g(\phi_{l}^{i})).\] (D.2)

Using concentration (A.18), with probability at least \(1-\delta\), we can guarantee

\[\max_{t}|S(\mathbf{\epsilon}_{0:t-1})| =\frac{\eta^{2}|\mathbf{1}^{\top}\mathbf{a}^{*}|}{2\pi}\max_{t}\left\{ \left|\sum_{l=0}^{t-1}\left(1-\frac{2\eta(\pi-1)}{\pi}\right)^{t-l-1}\sum_{m= 0}^{l-1}\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{l-1-m}(\mathbf{1}^{\top}\mathbf{ \epsilon}_{m})\right|\right.\] \[\qquad+\left.\eta\left|\sum_{j=0}^{t}\left(1-\frac{2\eta(\pi-1)} {\pi}\right)^{t-1-j}\mathbf{\epsilon}_{j}^{\top}\mathbf{a}^{*}\right|\right\}\] \[\leq c\sqrt{\frac{\eta\log(2/\delta)}{N}}K_{a}^{2}\cdot\max_{t} \left[\eta\frac{|\mathbf{1}^{\top}\mathbf{a}^{*}|}{\|\mathbf{a}^{*}\|}\sum_{l=0}^{t-1} \left(1-\frac{2\eta(\pi-1)}{\pi}\right)^{t-l-1}+1\right]\] \[\leq 2c\sqrt{\frac{\eta k\log(2/\delta)}{N}}K_{a}^{2}.\] (D.3)

For any \(0\leq s\leq\widetilde{O}(\eta^{-2})\) and any \(i\in[N]\), with probability at least \(1-\delta\), it holds that

\[|\cos\phi_{s}^{i}-\cos\phi_{s}| =\left|\frac{(\mathbf{v}_{s}^{i})^{\top}\mathbf{w}^{*}}{\|\mathbf{v}_{s}^{i} \|}-\frac{\mathbf{v}_{s}^{\top}\mathbf{w}^{*}}{\|\mathbf{v}_{s}\|}\right|\] \[\leq\frac{|(\mathbf{v}_{s}^{i}-\mathbf{v}_{s})^{\top}\mathbf{w}^{*}|}{\|\mathbf{v }_{s}^{i}\|}+|\mathbf{v}_{s}^{\top}\mathbf{w}^{*}|\left|\frac{1}{\|\mathbf{v}_{s}^{i}\|}- \frac{1}{\|\mathbf{v}_{s}\|}\right|\] \[\stackrel{{(i)}}{{\leq}}\frac{\|\mathbf{v}_{s}^{i}-\mathbf{ v}_{s}\|}{\|\mathbf{v}_{s}\|-\|\mathbf{v}_{s}^{i}-\mathbf{v}_{s}\|}+\frac{\|\mathbf{v}_{s}^{i}-\mathbf{v}_{s} \|}{\|\mathbf{v}_{s}\|-\|\mathbf{v}_{s}^{i}-\mathbf{v}_{s}\|}\] \[\stackrel{{(ii)}}{{\leq}}\frac{2c\eta K_{a}^{2}( \sqrt{Id}+I)\sqrt{\log(Nd/\delta)}}{1/2-c\eta K_{a}^{2}(\sqrt{Id}+I)\sqrt{ \log(Nd/\delta)}}\]\[\leq 12c\eta K_{a}^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)},\] (D.4)

where \((i)\) follows from \(\|\mathbf{w}^{*}\|=1\) and the triangle inequality; and \((ii)\) holds due to Lemma 14. Define a good event

\[\mathcal{E}_{\mathrm{good}}=\cap_{\ell=0}^{4}\mathcal{E}_{\ell},\] (D.5)

where

\[\mathcal{E}_{0} =\left\{\max_{t\geq 0}|S(\mathbf{\epsilon}_{0:t-1})|\leq 2c\sqrt{ \frac{\eta k\log(2/\delta)}{N}}K_{a}^{2}\right\},\] \[\mathcal{E}_{1} =\left\{\max_{i\in[N],s\leq\widetilde{O}(\eta^{-2})}\left|\mathbf{a} _{m}^{\top}\mathbf{a}^{*}-(\mathbf{a}_{m}^{i})^{\top}\mathbf{a}^{*}\right|\leq c\eta^{2}(I +\sqrt{kI})\sqrt{\log(N/\eta\delta)}K_{a}^{2}\right\},\] \[\mathcal{E}_{2} =\left\{\max_{s\leq\widetilde{O}(\eta^{-2}),0\leq\tau\leq \widetilde{O}(\eta^{-2})}\left|\sum_{m=\tau}^{s-1}(\mathbf{\xi}_{m}^{\top}\mathbf{w}^{ *}+\mathbf{h}_{m}^{\top}\mathbf{w}^{*})\right|\leq cK_{a}\sqrt{\frac{\eta^{-1}\log(1/ \eta\delta)}{N}}\right\},\] \[\mathcal{E}_{3} =\left\{\max_{i\in[N],s\leq\widetilde{O}(\eta^{-2})}|\cos\phi_{s} ^{i}-\cos\phi_{s}|\leq 12c\eta K_{a}^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)} \right\},\] \[\mathcal{E}_{4} =\left\{\max_{0\leq\tau\leq\widetilde{O}(\eta^{-2})}\|\mathbf{a}_{ \tau}\|\leq K_{a},\quad\min_{0\leq\tau\leq\tau_{1,1}}A_{\tau}\geq-c\eta\sqrt{ \frac{\eta\log(1/\eta\delta)}{N}}\right\}.\]

According to Lemmas 14, 12 and relations (D.3), (D.4), (A.20), we know \(\mathbb{P}\left(\mathcal{E}_{\ell}^{c}\right)\leq\delta\) for \(0\leq\ell\leq 4\).

### Self-correction of the Second Layer

**Theorem 1 restated.** For any initial point \((\mathbf{v}_{0},\mathbf{a}_{0})\) with \(\phi_{0}\in[0,\pi)\), we denote \(\tau_{a}=\inf\{t\geq 0:\mathbf{a}_{t}^{\top}\mathbf{a}^{*}\geq\gamma_{a}\}\) the first time, where \(\gamma_{a}=\frac{16(\pi-1)|\mathbf{1}^{\top}\mathbf{a}^{*}|^{2}}{k(\pi+k-1)}\). Under Assumption 1, if

\[ck^{2}\sqrt{\log(Ndk/\delta)}\max\left\{\eta(\sqrt{Ik}+I),\eta(\sqrt{Id}+I), \sqrt{\frac{\eta k}{N}}\right\}\frac{\|\mathbf{a}^{*}\|^{2}}{|\mathbf{1}^{\top}\mathbf{a} ^{*}|^{2}}<1,\] (D.6)

for a sufficiently large constant \(c\), then \(\tau_{a}\leq\widetilde{O}(\eta^{-2})\) with probability at least \(1-\delta\).

Proof.: We first define a sequence of events as

\[\mathcal{A}_{t}:=\left\{\forall s\leq t:\ \mathbf{a}_{s}^{\top}\mathbf{a}^{*}\leq \gamma_{a}\right\},\quad\text{where }\gamma_{a}=\frac{16(\pi-1)(\mathbf{1}^{\top}\mathbf{a}^ {*})^{2}}{k(\pi+k-1)}.\]

Notice that once we show for some \(\tau>0\),

\[\mathbb{P}\left(\mathcal{A}_{\tau-1}\cap\left\{\mathbf{a}_{\tau}^{\top}\mathbf{a}^{*} <\gamma_{a}\right\}\right)\leq\delta,\]

then we can conclude that \(\mathbb{P}\left(\mathcal{A}_{\tau}\right)\leq\delta\). According to the definition of \(\tau_{a}\), we can guarantee \(\tau_{a}\leq\tau\) with probability at least \(1-\delta\).

For simplicity, we introduce the following stopping time w.r.t. the angle \(\phi\) during the training process,

\[\tau_{-}(\phi)=\inf_{t\geq 0}\left\{t:\phi_{t}\geq\phi\right\}\quad\text{for} \quad\phi\in[0,\pi].\]

Define \(\tilde{\phi}^{l}\) and \(\tilde{\phi}^{u}\) such that

\[\cos\tilde{\phi}^{l}=\frac{144\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2} }{k}\log(4+k^{2}),\quad\cos\tilde{\phi}^{u}=-\cos\tilde{\phi}^{l}.\]

Choose \(\tilde{\phi}^{o}=\arccos(1/5)\) such that \(\cos\tilde{\phi}^{l}\leq 1/5=\cos\tilde{\phi}^{o}\) due to Assumption 1 such that

\[(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}\leq\frac{k(\pi+k-1)}{720\pi\log(4+k^{2})}.\] (D.7)The correction condition (6) is equivalent to

\[\left(1-\frac{32(\pi-1)}{k}\right)\frac{\pi-1}{\pi+k-1}\frac{(\mathbf{ 1}^{\top}\mathbf{a}^{*})^{2}}{\|\mathbf{a}^{*}\|^{2}}>1.\] (D.8)

We denote

\[\mathcal{Z}_{1} =\left\{\tau_{-}(\tilde{\phi}_{0})>\tau_{1,1}-1\right\},\] \[\mathcal{Z}_{2} =\left\{\tau_{-}(\pi/2)>\tau_{-}(\tilde{\phi}^{o})+\tau_{1,2}-1 \right\},\] \[\mathcal{Z}_{3} =\left\{\tau_{-}(\tilde{\phi}^{u})>\tau_{-}(\pi/2)+\tau_{1,3}-1 \right\},\]

where \(\tau_{1,1}\), \(\tau_{1,2}\) and \(\tau_{1,3}\) are deterministic and will be defined later.

Initial region: \((0,\tilde{\phi}^{o})\).For any \(s\leq\min\left\{\tau_{-}(\tilde{\phi}^{o}),\widetilde{O}(\eta^{-2})\right\}\), under the event \(\mathcal{E}_{\mathrm{good}}\), it holds that

\[\cos\phi_{s}^{i} \geq\cos\phi_{s}-|\cos\phi_{s}^{i}-\cos\phi_{s}|\] \[\geq\cos\tilde{\phi}^{o}-12\sigma K_{a}^{2}(\sqrt{Id}+I)\sqrt{ \log(Nd/\delta)}\] \[\geq\frac{\cos\tilde{\phi}^{o}}{2}=\frac{1}{10}>\arccos(0.46\pi).\]

Since \(g(\cdot)\in[0,\pi]\) is decreasing in \([0,\pi]\), plugging it into (D.2), we can get the following lower bound

\[\frac{1}{N}\sum_{i=1}^{N}B_{s}^{i}\geq\frac{1}{N}\sum_{i=1}^{N}[g (\phi_{s}^{i})-1]\geq g(0.46\pi)-1,\]

Under the event \(\mathcal{E}_{\mathrm{good}}\), using the lower bound of \(A_{0}\) in (B.5), for \(t\leq\min\left\{\tau_{-}(\tilde{\phi}^{o}),\widetilde{O}(\eta^{-2})\right\}\), we can guarantee

\[\mathbf{a}_{t}^{\top}\mathbf{a}^{*} \geq\eta\|\mathbf{a}^{*}\|^{2}\frac{g(0.46\pi)-1}{2\pi}\sum_{s=0}^{t- 1}\left(1-\frac{\eta(\pi-1)}{2\pi}\right)^{t-1-s}\left[1-\left(1-\eta\frac{\pi +k-1}{2\pi}\right)^{s}\right]\] \[\qquad-\|\mathbf{a}^{*}\|^{2}\left(1-\frac{\eta(\pi-1)}{2\pi}\right)^ {t}-2c\sqrt{\frac{\eta\log(2/\delta)}{N}}K_{a}^{2}\] \[=\frac{g(0.46\pi)-1}{\pi-1}\|\mathbf{a}^{*}\|^{2}\left[1-\left(1- \frac{\eta(\pi-1)}{2\pi}\right)^{t}\right]\] \[\qquad-\frac{1-\left(\frac{2\pi-\eta(k+\pi-1)}{2\pi-\eta(\pi-1)} \right)^{t}}{k}\left(1-\eta\frac{\pi-1}{2\pi}\right)^{t}[g(0.46\pi)-1]\,\|\mathbf{ a}^{*}\|^{2}\] \[\qquad-\|\mathbf{a}^{*}\|^{2}\left(1-\frac{\eta(\pi-1)}{2\pi}\right) ^{t}-2c\sqrt{\frac{\eta k\log(2/\delta)}{N}}K_{a}^{2}.\]

Here we take

\[\tau_{1,1} =\frac{1}{\eta}\frac{\pi-1}{2\pi}\log\left(4+\frac{4(\pi-1)}{g(0.4 6\pi)-1}\right)\leq\frac{1}{\eta}\frac{\pi-1}{2\pi}\log\left(4+20(\pi-1)\right),\] (D.9)

where we used the fact \(g(0.46\pi)-1\geq 0.2\). Since \(\tau_{1,1}-1\leq\widetilde{O}(\eta^{-2})\), under the event \(\left\{\tau_{1,1}-1<\tau_{-}(\tilde{\phi}^{o})\right\}\cap\mathcal{E}_{\mathrm{ good}}\), it holds that

\[\mathbf{a}_{\tau_{1,1}}^{\top}\mathbf{a}^{*} \geq\frac{(g(0.46\pi)-1)\|\mathbf{a}^{*}\|^{2}}{2(\pi-1)}-2c\sqrt{ \frac{\eta k\log(2t/\delta)}{N}}K_{a}^{2}\] \[\overset{\text{(i)}}{\geq}\frac{0.2(\mathbf{1}^{\top}\mathbf{a}^{*}) ^{2}}{2k(\pi-1)}-2c\sqrt{\frac{\eta k\log(2t/\delta)}{N}}K_{a}^{2}\]\[\frac{(ii)}{\geq}\ \frac{32(\pi-1)(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k(\pi+k- 1)}-2c\sqrt{\frac{\eta k\log(2t/\delta)}{N}}K_{a}^{2}\] \[\frac{(iii)}{\geq}\ \frac{16(\pi-1)(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k( \pi+k-1)}=\gamma_{a},\]

where \((i)\) holds due to \(\|\mathbf{a}^{*}\|\geq|\mathbf{1}^{\top}\mathbf{a}^{*}|/\sqrt{k}\); \((ii)\) follows from \(k\geq 320(\pi-1)^{2}\); and \((iii)\) holds due to the assumption (D.6). We can conclude that

\[\mathbb{P}\left(\left\{\tau_{1,1}-1<\tau_{-}(\tilde{\phi}^{o}) \right\}\cap\{\tau_{a}>\tau_{1,1}\}\right) \leq\mathbb{P}\left(\left\{\tau_{1,1}-1<\tau_{-}(\tilde{\phi}^{o} )\right\}\cap\left\{\mathbf{a}_{\tau_{1,1}}^{\top}\mathbf{a}^{*}\leq\gamma_{a}\right\}\right)\] \[\leq\mathbb{P}\left(\left\{\tau_{1,1}-1<\tau_{-}(\tilde{\phi}^{o} )\right\}\cap(\mathcal{E}_{0}^{c}\cup\mathcal{E}_{3}^{c})\right)\] \[\leq\mathbb{P}\left(\mathcal{E}_{0}^{c}\cup\mathcal{E}_{3}^{c} \right)\leq 2\delta,\] (D.10)

where the second last inequality holds since

\[\left\{\tau_{1,1}-1<\tau_{-}(\tilde{\phi}^{o})\leq\widetilde{O}(\eta^{-2}) \right\}\cap\mathcal{E}_{\mathrm{good}}\subseteq\{\mathbf{a}_{\tau_{1,1}}^{\top} \mathbf{a}^{*}\geq\gamma_{a}\},\]

such that

\[\left\{\tau_{1,1}-1<\tau_{-}(\tilde{\phi}^{o})\right\}\cap\left\{\mathbf{a}_{\tau_{ 1,1}}^{\top}\mathbf{a}^{*}\leq\gamma_{a}\right\}\subseteq\left\{\tau_{1,1}-1<\tau_ {-}(\tilde{\phi}^{o})\right\}\cap\mathcal{E}_{\mathrm{good}}^{c}.\]

If \(\tau_{1,1}\geq\tau_{-}(\tilde{\phi}^{o})\), we regard \((\mathbf{v}_{\tau_{-}(\tilde{\phi}^{o})},\mathbf{a}_{\tau_{-}(\tilde{\phi}^{o})})\) as the new initial point and step into the analysis of the next region.

Initial region: \([\tilde{\phi}^{o},\pi/2)\).Denote \(t_{0}=\tau_{-}(\tilde{\phi}^{o})\). From now on, we assume the event \(\mathcal{Z}_{1}^{c}\) happens and hide the indicator. Recalling the definition of \(\mathbf{h}_{t}\), we have

\[\|\mathbf{h}_{t}\| \leq\frac{1}{N}\sum_{i=1}^{N}\|\mathbb{P}_{t}-\mathbb{P}_{t}^{i} \|\|\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\|\] \[\overset{(i)}{\leq}24c\eta(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a} ^{2}\cdot\frac{1}{N}\sum_{i=1}^{N}\|\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t} ^{i})\|\] \[\overset{(ii)}{\leq}24c\eta(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a} ^{4},\] (D.11)

where \((i)\) holds due to (B.9) and \((ii)\) is true because \(\|\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\|\leq|(\mathbf{a}_{t}^{i})^{\top} \mathbf{a}^{*}|\leq K_{a}^{2}\). In addition, we also have

\[\mathbf{v}_{t+1}^{\top}\mathbf{w}^{*} =\mathbf{v}_{t}^{\top}\mathbf{w}^{*}-\eta\frac{1}{N}\sum_{i=1}^{N}\mathbb{ P}_{t}^{i}\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i};\mathbb{Z}_{t}^{i})\] \[=\tilde{\mathbf{v}}_{t+1}^{\top}\mathbf{w}^{*}-\eta\mathbf{h}_{t}^{\top}\mathbf{w} ^{*}\] \[=\mathbf{v}_{t}^{\top}\mathbf{w}^{*}-\eta\left(\mathbb{P}_{t}\frac{1}{N} \sum_{i=1}^{N}\nabla_{\mathbf{w}}L(\mathbf{w}_{t},\mathbf{a}_{t})+\mathbf{\xi}_{t}\right)^{\top }\mathbf{w}^{*}-\eta\mathbf{h}_{t}^{\top}\mathbf{w}^{*}\] \[=\mathbf{v}_{t}^{\top}\mathbf{w}^{*}+\frac{\eta}{2\pi}\frac{1}{N}\sum_{i =1}^{N}\frac{\pi-\phi_{t}^{i}}{\|\mathbf{v}_{t}\|}(\mathbf{a}_{t}^{i})^{\top}\mathbf{a}^{* }\sin^{2}\phi_{t}-\eta\mathbf{h}_{t}^{\top}\mathbf{w}^{*}-\eta\mathbf{\xi}_{t}^{\top}\mathbf{w} ^{*}.\] (D.12)

Here we take

\[\tau_{1,2}=\frac{1}{\eta}\frac{2\pi}{\pi-1}\log\left(4+\frac{20(\pi+k-1)\|\mathbf{ a}^{*}\|^{2}}{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}\left(\pi-g(\pi/5)\right)}\right)\leq \frac{1}{\eta}\frac{2\pi}{\pi-1}\log(4+k^{2}).\] (D.13)

For any \(t_{0}\leq s\leq t-1\leq t_{0}+\tau_{1,2}\), under the event \(\mathcal{A}_{t-1}\cap\mathcal{E}_{\mathrm{good}}\), invoking (D.12) guarantees that,

\[\cos\phi_{s}=\frac{1}{\|\mathbf{v}_{s}\|}\left\{\mathbf{v}_{t_{0}}^{\top}\mathbf{w}^{*}+ \frac{\eta}{2\pi}\frac{1}{N}\sum_{i=1}^{N}\sum_{m=t_{0}}^{s-1}\frac{\pi-\phi_{ m}^{i}}{\|\mathbf{v}_{m}\|}(\mathbf{a}_{m}^{i})^{\top}\mathbf{a}^{*}\sin^{2}\phi_{m}-\eta\sum_{m=t _{0}}^{s-1}(\mathbf{h}_{m}^{\top}\mathbf{w}^{*}+\mathbf{\xi}_{m}^{\top}\mathbf{w}^{*})\right\}\]\[\leq 2\cos\tilde{\phi}^{o}+\frac{128(\pi-1)}{\pi+k-1}\frac{(\mathbf{1}^{ \top}\mathbf{a}^{*})^{2}}{k}\log(4+k^{2})\leq 3\cos\tilde{\phi}^{o},\] (D.14)

where \((i)\) holds due to \(\|\mathbf{v}_{s}\|\geq 1/2\), \(\|\mathbf{v}_{t_{0}}\|\geq 1/2\) and the definition of \(\mathcal{A}_{t-1}\); \((ii)\) comes from \(t_{0}\leq\tau_{1,1}\) and \(\mathcal{E}_{1}\); \((iii)\) follows from \(\phi_{t_{0}}\geq\tilde{\phi}^{l}\), and \(\mathcal{E}_{2}\); and \((iv)\) holds due to the assumption (D.6). Further, under the event \(\mathcal{A}_{t-1}\cap\mathcal{E}_{\mathrm{good}}\), we have for any \(i\in[N]\) and \(t_{0}\leq s\leq t-1\leq t_{0}+\tau_{1,2}-1\)

\[\cos\phi_{s}^{i} \leq 3\cos\tilde{\phi}^{o}+12\alpha fK_{a}^{2}(\sqrt{Id}+I)\sqrt{ \log(Nd/\delta)}\] \[\leq 4\cos\tilde{\phi}^{o}=\frac{4}{5}\leq\cos(\pi/5).\]

Under the event \(\mathcal{A}_{t-1}\cap\mathcal{E}_{\mathrm{good}}\cap\{\tau_{-}(\pi/2)-t_{0} \geq\tau_{1,2}\}\), it implies that for any \(t_{0}\leq s\leq t-1\leq t_{0}+\tau_{1,2}-1\),

\[\frac{1}{N}\sum_{i=1}^{N}B_{s}^{i} \geq\frac{1}{N}\sum_{i=1}^{N}\frac{\eta}{2\pi}\frac{(\mathbf{1}^{ \top}\mathbf{a}^{*})^{2}}{\|\mathbf{a}^{*}\|^{2}}\sum_{l=0}^{s-1}\left(1-\eta\frac{\pi +k-1}{2\pi}\right)^{s-1-l}(\pi-g(\phi_{l}^{i}))\] \[\geq\frac{\pi-g(\pi/5)}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*} )^{2}}{\|\mathbf{a}^{*}\|^{2}}\left[1-\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s} \right].\]

Together with (D.1) and (D.3), we can guarantee that

\[\mathbf{a}_{t}^{\top}\mathbf{a}^{*} =\left(1-\eta\frac{\pi-1}{2\pi}\right)^{t-t_{0}}\mathbf{a}_{t_{0}}^{ \top}\mathbf{a}^{*}+\frac{1-\left(\frac{2\pi-\eta(k+\pi-1)}{2\pi-\eta(\pi-1)}\right) ^{t-t_{0}}}{k}\left(1-\eta\frac{\pi-1}{2\pi}\right)^{t}A_{t_{0}}\] \[\qquad+\frac{\eta\|\mathbf{a}^{*}\|^{2}}{2\pi}\sum_{s=t_{0}}^{t-1} \left(1-\eta\frac{\pi-1}{2\pi}\right)^{t-1-s}\frac{1}{N}\sum_{i=1}^{N}B_{s}^{ i}+S(\mathbf{\epsilon}_{t_{0}:t-1})\] \[\geq\frac{\pi-g(\pi/5)}{\pi-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*} )^{2}}{\pi+k-1}\left[1-\left(1-\eta\frac{\pi-1}{2\pi}\right)^{t-t_{0}}\right]\] \[\qquad-\frac{1-\left(\frac{2\pi-\eta(k+\pi-1)}{2\pi-\eta(\pi-1)} \right)^{t-t_{0}}}{k}\left(1-\eta\frac{\pi-1}{2\pi}\right)^{t-t_{0}}[\pi-g(\pi /5)]\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{\pi+k-1}\] \[\qquad-5\|\mathbf{a}^{*}\|^{2}\left(1-\frac{\eta(\pi-1)}{2\pi}\right) ^{t-t_{0}}-2c\sqrt{\frac{\eta k\log(1/\delta)}{N}}K_{a}^{2}.\]

Choosing \(t=t_{0}+\tau_{1,2}\), it holds that

\[\mathbf{a}_{t_{0}+\tau_{1,2}}^{\top}\mathbf{a}^{*} \geq\frac{3}{4}\frac{\pi-g(\pi/5)}{\pi-1}\frac{(\mathbf{1}^{\top} \mathbf{a}^{*})^{2}}{\pi+k-1}-3c\sqrt{\frac{\eta k\log(1/\eta\delta)}{N}}K_{a}^{2}\] \[\geq\frac{\pi-g(\pi/5)}{4(\pi-1)}\frac{(\mathbf{1}^{\top}\mathbf{a}^ {*})^{2}}{\pi+k-1}\] \[\geq\frac{0.5}{4(\pi-1)}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{ \pi+k-1}\]\[\geq\frac{16(\pi-1)(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k(\pi+k-1)}= \gamma_{a},\]

where we used the assumptions (D.6) and \(k\geq 320(\pi-1)^{2}\).

Recalling that \(\mathcal{Z}_{2}=\left\{\tau_{-}(\pi/2)\geq\tau_{-}(\tilde{\phi}^{o})+\tau_{1,2}\right\}\), hence we have verified that

\[\mathbb{P}\left(\mathcal{Z}_{1}^{c}\cap\mathcal{Z}_{2}\cap\{\tau_ {a}>\tau_{1,1}+\tau_{1,2}\}\right) \leq\mathbb{P}\left(\mathcal{Z}_{1}^{c}\cap\mathcal{Z}_{2}\cap \left\{\tau_{a}>\tau_{-}(\tilde{\phi}^{o})+\tau_{1,2}\right\}\right)\] \[=\mathbb{P}\left(\mathcal{Z}_{1}^{c}\cap\mathcal{Z}_{2}\cap \mathcal{A}_{\tau_{-}(\tilde{\phi}^{o})+\tau_{1,2}-1}\cap\left\{\mathbf{a}_{t_{0}+ \tau_{1,2}}^{\top}\mathbf{a}^{*}\leq\gamma_{a}\right\}\right)\] \[\leq\mathbb{P}\left(\mathcal{A}_{\tau_{-}(\tilde{\phi}^{o})+\tau_ {1,2}-1}\cap\mathcal{E}_{\mathrm{good}}^{c}\right)\leq 5\delta.\] (D.15)

If \(\tau_{1,2}\geq\tau_{-}(\pi/2)\), we regard \((\mathbf{v}_{\tau_{-}(\pi/2)},\mathbf{a}_{\tau_{-}(\pi/2)})\) as the new initial point and step into the analysis of the next region.

Initial region: \([\pi/2,\tilde{\phi}^{u}\mathbf{)}\).From now on, we assign \(t_{0}=\tau_{-}(\pi/2)\) and consider the case \(\left\{\tau_{-}(\tilde{\phi}^{o})\leq\tau_{1,1}\right\}\cap\left\{t_{0}<\tau_{ -}(\tilde{\phi}^{o})+\tau_{1,2}\right\}\). Take

\[\tau_{1,3}=\frac{1}{\eta}\frac{2\pi}{\pi-1}\log\left(4+\frac{24( \pi-1)}{\zeta_{a}}\right)\leq\frac{1}{\eta}\frac{2\pi}{\pi-1}\log(4+k^{2}),\] (D.16)

where the inequality holds due to our assumptions \(k\geq 320(\pi-1)^{2}\) and (D.8). Similar to (D.14), for any \(t_{0}\leq s\leq t-1\leq\widetilde{O}(\eta^{-2})\), under the event \(\mathcal{A}_{t-1}\cap\mathcal{E}_{\mathrm{good}}\), we have

\[\cos\phi_{s} =\frac{1}{\|\mathbf{v}_{s}\|}\Bigg{\{}\mathbf{v}_{t_{0}}^{\top}\mathbf{w}^{*} +\frac{\eta}{2\pi}\frac{1}{N}\sum_{i=1}^{N}\sum_{m=t_{0}}^{s-1}\frac{\pi-\phi_ {m}^{i}}{\|\mathbf{v}_{m}\|}(\mathbf{a}_{m}^{i})^{\top}\mathbf{a}^{*}\sin^{2}\phi_{m}-\eta \sum_{m=t_{0}}^{s-1}(\mathbf{h}_{m}^{\top}\mathbf{w}^{*}+\mathbf{\xi}_{m}^{\top}\mathbf{w}^{*} )\Bigg{\}}\] \[\overset{(i)}{\leq}2\tau_{1,3}\eta\gamma_{a}+2c\tau_{1,3}\eta^{2 }(I+\sqrt{kI})\sqrt{\log(1/\eta\delta)}K_{a}^{2}+2\eta\max_{\tau\leq\tau_{1,1 }+\tau_{1,2}}\left|\sum_{m=\tau}^{s-1}(\mathbf{\xi}_{m}^{\top}\mathbf{w}^{*}+\mathbf{h}_{m }^{\top}\mathbf{w}^{*})\right|\] \[\overset{(ii)}{\leq}\frac{64\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top} \mathbf{a}^{*})^{2}}{k}\log(4+k^{2})+\frac{4\pi}{\pi-1}\log(4+k^{2})\cdot c\eta(I+ \sqrt{kI})\sqrt{\log(1/\eta\delta)}K_{a}^{2}\] \[\qquad+\frac{6\pi}{\pi-1}\log(4+k^{2})\cdot\left(24c\eta K_{a}^{ 2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}+cK_{a}\sqrt{\frac{\eta\log(Nd/\eta\delta )}{N}}\right)\] \[\leq\frac{128\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2} }{k}\log(4+k^{2}),\] (D.17)

where \((i)\) holds due to \(\phi_{t_{0}}\geq\pi/2\) and the definition of \(\mathcal{A}_{t-1}\); and \((ii)\) holds due to \(s-1-\tau\leq\widetilde{O}(\eta^{-2})\). Under the event \(\mathcal{A}_{t-1}\cap\mathcal{E}_{\mathrm{good}}\), we can guarantee that

\[\max_{i}\cos\phi_{s}^{i} \leq\cos\phi_{s}+\max_{i}\left|\cos\phi_{s}-\cos\phi_{s}^{i}\right|\] \[\leq\frac{128\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2 }}{k}\log(4+k^{2})+12c\eta K_{a}^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}\] \[\leq\frac{144\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2 }}{k}\log(4+k^{2})\] \[=\cos\tilde{\phi}^{l},\]

where we used the assumption (D.6). It implies that \(\min_{i}\phi_{s}^{i}\geq\tilde{\phi}^{l}\) under the event \(\mathcal{A}_{t-1}\cap\mathcal{E}_{\mathrm{good}}\). In addition, we notice that \(\cos\tilde{\phi}^{l}=-\cos\tilde{\phi}^{u}\), which implies \(\tilde{\phi}^{l}+\tilde{\phi}^{u}=\pi\) and \(\sin\tilde{\phi}^{l}=\sin\tilde{\phi}^{u}\). Under the event \(\mathcal{A}_{t-1}\cap\mathcal{E}_{\mathrm{good}}\cap\left\{t-1\leq\min\{\tau_{ -}(\tilde{\phi}^{u}),\widetilde{O}(\eta^{-2})\}\right\}\), for any \(s\leq t-1\), we have

\[\frac{1}{N}\sum_{i=1}^{N}\Bigg{\{}g(\phi_{s}^{i})+\frac{\eta}{2 \pi}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{\|\mathbf{a}^{*}\|^{2}}\sum_{l=t_{0}} ^{s-1}\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s-1-l}(1-g(\phi_{l}^{i}))\Bigg{\}}\]\[\geq g(\tilde{\phi}^{u})+\frac{\eta k}{2\pi}\sum_{l=t_{0}}^{s-1}\left( 1-\eta\frac{\pi+k-1}{2\pi}\right)^{s-1-l}\left(1-g(\tilde{\phi}^{l})\right)\] \[\geq g(\tilde{\phi}^{u})+\frac{\eta k}{2\pi}\sum_{l=t_{0}}^{s-1} \left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s-1-l}\left(1-g(\tilde{\phi}^{l})\right)\] \[=g(\tilde{\phi}^{u})+\frac{k(1-g(\tilde{\phi}^{l}))}{\pi+k-1} \left[1-\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s}\right]\] \[\geq g(\tilde{\phi}^{u})+1-g(\tilde{\phi}^{l})\] \[=(\pi-\tilde{\phi}^{u})\cos\tilde{\phi}^{u}+\sin\tilde{\phi}^{u}- (\pi-\tilde{\phi}^{l})\cos\tilde{\phi}^{l}-\sin\tilde{\phi}^{l}+1\] \[=(2\pi-\tilde{\phi}^{u}-\tilde{\phi}^{l})\cos\tilde{\phi}^{u}+1\] \[=-\frac{144\pi^{2}}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{ 2}}{k}\log(4+k^{2})+1\geq 0,\] (D.18)

where the last inequality holds due to the condition (D.7). Together with the definition of \(B_{s}^{i}\), we have

\[\frac{\mathbf{1}_{\mathcal{A}_{t-1}}}{N}\sum_{i=1}^{N}B_{s}^{i} =\frac{\mathbf{1}_{\mathcal{A}_{t-1}}}{N}\sum_{i=1}^{N}\left\{(g( \phi_{s}^{i})-1)+\frac{\eta}{2\pi}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{\| \mathbf{a}^{*}\|^{2}}\sum_{l=0}^{s-1}\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s-1- l}(\pi-g(\phi_{l}^{i}))\right\}\] \[\geq\frac{\eta}{2\pi}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{\| \mathbf{a}^{*}\|^{2}}\sum_{l=0}^{s-1}\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s-1- l}(\pi-1)\] \[=\underbrace{\frac{\pi-1}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a} ^{*})^{2}}{\zeta_{a}}}_{\zeta_{a}}-1\,-\frac{\pi-1}{\pi+k-1}\frac{(\mathbf{1}^ {\top}\mathbf{a}^{*})^{2}}{\|\mathbf{a}^{*}\|^{2}}\left(1-\eta\frac{\pi+k-1}{2\pi} \right)^{s}.\]

We know \(\zeta_{a}>0\) due to the condition (D.8). Under \(\mathcal{A}_{t-1}\cap\mathcal{E}_{\mathrm{good}}\cap\{t-1\leq\min\{\tau_{-}( \tilde{\phi}^{u}),\widetilde{O}(\eta^{-2})\}\}\), we can guarantee

\[\mathbf{a}_{t}^{\top}\mathbf{a}^{*} \geq\frac{\zeta_{a}\|\mathbf{a}^{*}\|^{2}}{\pi-1}\left[1-\left(1-\frac {\eta(\pi-1)}{2\pi}\right)^{t}\right]-5K_{a}^{2}\left(1-\frac{\eta(\pi-1)}{2 \pi}\right)^{t}\] \[\geq\frac{\zeta_{a}\|\mathbf{a}^{*}\|^{2}}{\pi-1}-\left(1-\frac{\eta (\pi-1)}{2\pi}\right)^{t}\left(6\|\mathbf{a}^{*}\|^{2}+\frac{\zeta_{a}\|\mathbf{a}^{*} \|^{2}}{\pi-1}\right)-2cK_{a}^{2}\sqrt{\frac{\eta k\log(1/\eta\delta)}{N}},\]

where we used \(|\mathbf{a}_{t_{0}}^{\top}\mathbf{a}^{*}|\leq\|\mathbf{a}^{*}\|\cdot K_{a}=5\|\mathbf{a}^{*}\|^ {2}\). Now let \(t=t_{0}+\tau_{1,3}\), under \(\mathcal{A}_{t-1}\cap\mathcal{E}_{\mathrm{good}}\cap\{t_{0}+\tau_{1,3}-1\leq \tau_{-}(\tilde{\phi}^{u})\}\), we have

\[\mathbf{a}_{t_{0}+\tau_{1,3}}^{\top}\mathbf{a}^{*} \geq\frac{3}{4}\frac{\zeta_{a}\|\mathbf{a}^{*}\|^{2}}{\pi-1}-2cK_{a}^ {2}\sqrt{\frac{\eta k\log(1/\eta\delta)}{N}}\] \[\geq\frac{24(\pi-1)(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k(\pi+k-1)} -2cK_{a}^{2}\sqrt{\frac{\eta k\log(1/\eta\delta)}{N}}\] \[\geq\frac{16(\pi-1)(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k(\pi+k-1)} =\gamma_{a},\] (D.19)

where we used the assumption (D.8) and (D.6). Since \(\mathcal{Z}_{3}=t_{0}+\tau_{1,3}-1\leq\tau_{-}(\tilde{\phi}^{u})\), we have

\[\mathbb{P}\left(\mathcal{Z}_{1}^{c}\cap\mathcal{Z}_{2}^{c}\cap\mathcal{Z}_{3} \cap\{\tau_{a}>\tau_{1,1}+\tau_{1,2}+\tau_{1,3}\}\right)\leq 5\delta.\] (D.20)

If \(\tau_{-}(\tilde{\phi}^{u})<\tau_{-}(\pi/2)+\tau_{1,3}\) (that is \(\mathcal{Z}_{3}^{c}\)), we regard \((\mathbf{v}_{\tau_{-}(\tilde{\phi}^{u})},\mathbf{a}_{\tau_{-}(\tilde{\phi}^{u})})\) as the new initial point and step into the analysis of the next region.

Initial region: \([\tilde{\phi}^{u},\pi)\).Now we assign \(t_{0}=\tau_{-}(\tilde{\phi}^{u})\) and choose \(\tau_{1,4}=\tau_{1,3}\). Recall that \(\cos\tilde{\phi}^{u}=-\frac{144\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*}) ^{2}}{k}\log(4+k^{2})\). Similar to what we showed in (D.17), for any \(t_{0}\leq s\leq t-1\leq t_{0}+\tau_{1,4}-1\), under the event \(\mathcal{A}_{t-1}\cap\mathcal{E}_{\mathrm{good}}\), we have,

\[\cos\phi_{s} =\frac{1}{\|\mathbf{v}_{s}\|}\Bigg{\{}\mathbf{v}_{t_{0}}^{\top}\mathbf{w}^{*} +\frac{\eta}{2\pi}\frac{1}{N}\sum_{i=1}^{N}\sum_{m=t_{0}}^{s-1}\frac{\pi-\phi _{m}^{i}}{\|\mathbf{v}_{m}\|}(\mathbf{a}_{m}^{i})^{\top}\mathbf{a}^{*}\sin^{2}\phi_{m}-\eta \sum_{m=t_{0}}^{s-1}(\mathbf{h}_{m}^{\top}\mathbf{w}^{*}+\mathbf{\xi}_{m}^{\top}\mathbf{w}^{*}) \Bigg{\}}\] \[\stackrel{{(i)}}{{\leq}}\frac{1}{\|\mathbf{v}_{s}\|} \left\{\frac{\cos\phi_{t_{0}}}{3}+\tau_{1,4}\eta\gamma_{a}+c\tau_{1,4}\eta^{2} (I+\sqrt{kI})\sqrt{\log(k/\eta\delta)}K_{a}^{2}+\eta\left|\sum_{m=t_{0}}^{s-1}( \mathbf{\xi}_{m}^{\top}\mathbf{w}^{*}+\mathbf{h}_{m}^{\top}\mathbf{w}^{*})\right|\right\}\] \[\leq\frac{1}{\|\mathbf{v}_{s}\|}\left\{\frac{\cos\tilde{\phi}^{u}}{3}+ \frac{36\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k}\log(4+k^{2})\right\}\] \[\stackrel{{(ii)}}{{\leq}}-\frac{4\pi}{\pi+k-1}\frac{ (\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k}\log(4+k^{2}).\] (D.21)

where \((i)\) holds due to \(\|\mathbf{v}_{m}\|\leq 3\), \(\phi_{0}>\pi/2\) and the definition of \(\mathcal{A}_{t-1}\); and \((ii)\) holds due to \(\|\mathbf{v}_{s}\|\leq 3\). Applying the bound \(|\cos\phi_{s}-\cos\phi_{s}^{i}|\) in (D.4) and the assumption D.6, we have

\[\max_{i}\cos\phi_{s}^{i} \leq\cos\phi_{s}+\max_{i}\left|\cos\phi_{s}-\cos\phi_{s}^{i}\right|\] \[\leq-\frac{4\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}} {k}\log(4+k^{2})+12c\eta K_{a}^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}\leq 0,\]

which means \(\min_{i}\phi_{s}^{i}\geq\frac{\pi}{2}\). It follows that for any \(s\leq t-1\),

\[\frac{1}{N}\sum_{i=1}^{N}B_{s}^{i} =\frac{1}{N}\sum_{i=1}^{N}\left\{(g(\phi_{s}^{i})-1)+\frac{\eta}{2 \pi}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{\|\mathbf{a}^{*}\|^{2}}\sum_{l=t_{0}} ^{s-1}\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s-1-l}(\pi-g(\phi_{l}^{i}))\right\}\] \[\geq\frac{1}{N}\sum_{i=1}^{N}\left\{-1+\frac{\eta}{2\pi}\frac{( \mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{\|\mathbf{a}^{*}\|^{2}}\sum_{l=t_{0}}^{s-1}\left( 1-\eta\frac{\pi+k-1}{2\pi}\right)^{s-1-l}(\pi-1)\right\}\] \[=\zeta_{a}-\frac{\pi-1}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{* })^{2}}{\|\mathbf{a}^{*}\|^{2}}\left(1-\eta\frac{\pi+k-1}{2\pi}\right)^{s}.\] (D.22)

Plugging (D.22) and (D.3) into (D.1), under the event \(\mathcal{A}_{t-1}\cap\mathcal{E}_{\mathrm{good}}\), we can get

\[\mathbf{a}_{t}^{\top}\mathbf{a}^{*} \geq\frac{\zeta_{a}\|\mathbf{a}^{*}\|^{2}}{\pi-1}\left[1-\left(1-\frac {\eta(\pi-1)}{2\pi}\right)^{t}\right]-|\mathbf{a}_{0}^{\top}\mathbf{a}^{*}|\left(1-\frac {\eta(\pi-1)}{2\pi}\right)^{t}\] \[\quad-\frac{1-\left(\frac{2\pi-\eta(k+\pi-1)}{2\pi-\eta(\pi-1)} \right)^{t}}{k}\left(1-\eta\frac{\pi-1}{2\pi}\right)^{t}\frac{(\pi-1)(\mathbf{ 1}^{\top}\mathbf{a}^{*})^{2}}{\pi+k-1}\] \[\quad-2cK_{a}^{2}\sqrt{\frac{M\eta\log(1/\eta\delta)}{N}}.\] (D.23)

Plugging \(t=t_{0}+\tau_{1,4}\), according to (D.23) and (D.19), we can guarantee that \(\phi_{\tau_{a}}\geq\pi/2\) and

\[\mathbb{P}\left(\mathcal{Z}_{1}^{c}\cap\mathcal{Z}_{2}^{c}\cap\mathcal{Z}_{3}^{c }\cap\left\{\tau_{a}>\sum_{q=1}^{4}\tau_{1,q}\right\}\right)\leq 5\delta.\] (D.24)

Conclusion.Combining (D.10), (D.15), (D.20) and (D.24), we have

\[\mathbb{P}\left(\tau_{a}>\sum_{q=1}^{4}\tau_{1,q}\right)\leq\mathbb{ P}\left(\{\tau_{a}>\tau_{1,1}\}\cap\mathcal{Z}_{1}\right)+\mathbb{P}\left(\left\{ \tau_{a}>\sum_{q=1}^{4}\tau_{1,q}\right\}\cap\mathcal{Z}_{1}^{c}\right)\] \[\leq 4\delta+\mathbb{P}\left(\left\{\tau_{a}>\sum_{q=1}^{4}\\[\leq 9\delta+\mathbb{P}\left(\left\{\tau_{a}>\sum_{q=1}^{3}\tau_{1,q} \right\}\cap\mathcal{Z}_{1}^{c}\cap\mathcal{Z}_{2}^{c}\cap\mathcal{Z}_{3}\right) +\mathbb{P}\left(\left\{\tau_{a}>\sum_{q=1}^{4}\tau_{1,q}\right\}\cap\mathcal{Z }_{1}^{c}\cap\mathcal{Z}_{2}^{c}\cap\mathcal{Z}_{3}^{c}\right)\] \[\leq 14\delta+\mathbb{P}\left(\left\{\tau_{a}>\sum_{q=1}^{4}\tau_{ 1,q}\right\}\cap\mathcal{Z}_{1}^{c}\cap\mathcal{Z}_{2}^{c}\cap\mathcal{Z}_{3}^ {c}\right)\] \[\leq 19\delta.\]

Therefore, we have proved \(\tau_{a}\leq\sum_{q=1}^{4}\tau_{1,q}\) with high probability for \(\phi_{0}\in[0,\tilde{\phi}^{o})\). The conclusion for other initial regions can be obtained in similar arguments. From the definitions in (D.9), (D.13) and (D.16), we have

\[\sum_{q=1}^{4}\tau_{1,q}\lesssim\eta^{-1}\log k.\]

### Proof of Lemma 3

**Lemma 3** restated.: Denote \(\tilde{\phi}^{u}=\arccos\left(-\frac{144\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top} \mathbf{a}^{*})^{2}}{k}\log(4+k^{2})\right)\) and

\[\varpi=\min\left\{\sin\tilde{\phi}^{u}\mathds{1}_{\phi_{0}\leq \tilde{\phi}^{u}}+\sin\phi_{0}\mathds{1}_{\phi_{0}>\tilde{\phi}^{u}},\,\sin \left(\pi-\left((20\|\mathbf{a}^{*}\|^{2})^{-1}\wedge 1\right)\right)\right\},\]

Under the same conditions of Theorem 1. If the learning rate satisfies

\[c\|\mathbf{a}^{*}\|^{2}\sqrt{\log(Mdk/\delta)}\max\left\{\eta( \sqrt{Id}+I),\sqrt{\frac{\eta}{N}}\right\}\leq\frac{\varpi^{2}}{16k},\] (D.25)

for a large absolute constant \(c>0\), we have \(\sin\phi_{\tau_{a}}\geq\frac{\varpi}{15\sqrt{k}}\) with probability at least \(1-\delta\).

Proof.: If \(\phi_{\tau_{a}}\leq\tilde{\phi}^{u}\), the lower bound trivially holds. Next we prove the lower bound for \(\sin\phi_{\tau_{a}}\) starting from the last initial region, that is \(\phi_{0}\in[\tilde{\phi}^{u},\pi)\). From (D.16) and \(|\mathbf{1}^{\top}\mathbf{a}^{*}|\geq\|\mathbf{a}^{*}\|\), we know that

\[\tau_{1,4}=\frac{1}{\eta}\log\left(4+\frac{16(\pi-1)}{\zeta_{a}} \right)\leq\frac{1}{\eta}\log\left(4+\frac{16(\pi-1)}{\frac{32(\pi-1)^{2}( \mathbf{1}^{*}\mathbf{a}^{*})^{2}}{k(\pi+k-1)\|\mathbf{a}^{*}\|^{2}}}\right) \leq\frac{1}{\eta}\log(4+k^{2}).\]

Let \(C=\frac{\log(1+2k^{2})}{4}\), then \(e^{2C}\leq(1+2k^{2})^{\frac{1}{2}}\leq 2k\). Denote \(\varpi_{0}=\sin\tilde{\phi}^{u}\mathds{1}_{\phi_{0}<\tilde{\phi}^{u}}+\sin\phi _{0}\mathds{1}_{\phi_{0}\geq\tilde{\phi}^{u}}\). From the definition of \(\boldsymbol{h}_{t}\) and (B.9), we know

\[\|\boldsymbol{h}_{t}\|\leq\frac{1}{N}\sum_{i=1}^{N}\|\mathbf{P}_{ t}^{i}-\mathbf{P}_{t}\|\|\nabla_{\boldsymbol{w}}L(\boldsymbol{w}_{t}^{i}, \boldsymbol{a}_{t}^{i})\|\leq 50c\eta(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{4},\] (D.26)

where we also used \(\|\nabla_{\boldsymbol{w}}L(\boldsymbol{w}_{t}^{i},\boldsymbol{a}_{t}^{i})\| \leq 5K_{a}^{2}\). Recall that \(\tilde{\boldsymbol{v}}_{t}-\boldsymbol{v}_{t}=\eta\boldsymbol{h}_{t-1}\), then it follows that

\[\big{|}\big{|}\|\tilde{\boldsymbol{v}}_{t}\|^{2}\sin^{2}\tilde{ \phi}_{t}-\|\boldsymbol{v}_{t}\|^{2}\sin^{2}\phi_{t}\big{|}\] \[\leq 7\|\tilde{\boldsymbol{v}}_{t}-\boldsymbol{v}_{t}\|+7\big{|}( \tilde{\boldsymbol{v}}_{t}-\boldsymbol{v}_{t})^{\top}\boldsymbol{w}^{*}\big{|} ^{2}\] \[\leq 14\eta\|\boldsymbol{h}_{t-1}\|,\] (D.27)

where we used \(\|\boldsymbol{v}_{t}\|\leq 3\) in Lemma 13 and \(\|\tilde{\boldsymbol{v}}_{t}\|\leq\|\boldsymbol{v}_{t}\|+\eta\|\boldsymbol{h}_{ t}\|<4\). For any \(t\), we define a variable

\[\tau_{t}(K_{a})=\sup_{0\leq s\leq t-1}\left\{\min_{i\in[N]}\phi_{ s}^{i}\leq\pi-\left(\frac{1}{20K_{a}^{2}}\wedge 1\right)\right\}.\] (D.28)If \(\tau_{t}(K_{a})\) does not exist, that is \(\min_{i\in[N]}\phi_{s}^{i}>\pi-\left((20K_{a}^{2})^{-1}\wedge 1\right)\) holds for any \(0\leq s\leq t-1\), we let \(\tau_{t}(K_{a})=0\). Then for any \(\tau_{t}(K_{a})\leq s\leq t-1\), we always have \(\min_{i\in[N]}\phi_{s}^{i}>\pi-\left((20K_{a}^{2})^{-1}\wedge 1\right)\) such that

\[1-\eta\lambda_{s}\cos\phi_{s} =1-\eta\cos\phi_{s}\cdot\frac{1}{N}\sum_{i=1}^{N}\frac{\pi-\phi_{s }^{i}\left(\mathbf{a}_{s}^{i}\right)^{\top}\mathbf{a}^{*}}{\|\mathbf{v}_{s}\|}\] \[\geq 1-\eta\frac{1}{N}\sum_{i=1}^{N}\frac{1}{2\pi\cdot(20K_{a}^{2 }\lor 1)}\frac{|(\mathbf{a}_{s}^{i})^{\top}\mathbf{a}^{*}|}{\|\mathbf{v}_{s}\|}\] \[\stackrel{{(ii)}}{{\geq}}1-\eta\frac{5K_{a}^{2}}{2 \pi\cdot(20K_{a}^{2}\lor 1)}\geq 1-\frac{\eta}{8}.\] (D.29)

Without loss of generality, we assume \(\arg\min_{i\in[N]}\phi_{\tau_{t}(K_{a})}^{i}=1\). It follows that

\[\sin^{2}\phi_{\tau_{t}(K_{a})} \geq\sin^{2}\phi_{\tau_{t}(K_{a})}^{1}-\left|\sin^{2}\phi_{\tau_{t }(K_{a})}-\sin^{2}\phi_{\tau_{t}(K_{a})}^{1}\right|\] \[\geq\sin^{2}\phi_{\tau_{t}(K_{a})}^{1}-2\left|\cos\phi_{\tau_{t}( K_{a})}-\cos\phi_{\tau_{t}(K_{a})}^{1}\right|\] \[\geq\sin^{2}\phi_{\tau_{t}(K_{a})}^{1}-\frac{2\|\mathbf{v}_{\tau_{t}(K _{a})}^{1}-\mathbf{v}_{\tau_{t}(K_{a})}\|}{\|\mathbf{v}_{\tau_{t}(K_{a})}\|}-2\|\mathbf{v}_ {\tau_{t}(K_{a})}^{1}\|\left|\frac{1}{\|\mathbf{v}_{\tau_{t}(K_{a})}\|}-\frac{1}{ \|\mathbf{v}_{\tau_{t}(K_{a})}^{1}\|}\right|\] \[\geq\sin^{2}(\pi-((20K_{a}^{2})^{-1}\wedge 1))-6c\eta(\sqrt{Id}+I) \sqrt{\log(Nd/\delta)}K_{a}^{2}\] \[\geq\frac{\sin^{2}(\pi-((20K_{a}^{2})^{-1}\wedge 1))}{2}.\] (D.30)

Notice that if \(t\leq 4C/\eta\), we have

\[\left(1-\frac{\eta}{8}\right)^{2t}\geq\left(1-\frac{\eta}{8}\right)^{\frac{8}{ \eta}\cdot C}\geq 2^{-2C}\geq e^{-2C}\geq\frac{1}{2k}.\] (D.31)

It holds because \(\eta/8\leq\frac{1}{2}\) and \(f(x)=(1-x)^{1/x}\) is decreasing in \([0,1]\). Invoking Lemma 1, we know

\[\|\mathbf{v}_{t}\|^{2} \sin^{2}\phi_{t}=\left(1-\eta\lambda_{t-1}\cos\phi_{t-1}\right)^ {2}\|\mathbf{v}_{t-1}\|^{2}\sin^{2}\phi_{t-1}-2\eta M_{1,t-1}+\eta^{2}M_{2,t-1}+H_ {t}\] \[\stackrel{{(i)}}{{\geq}}\left(1-\frac{\eta}{8} \right)^{2(t-\tau_{t}(K_{a}))}\|\mathbf{v}_{\tau_{t}(K_{a})}\|^{2}\sin^{2}\phi_{ \tau_{t}(K_{a})}-2\eta\sum_{s=\tau_{t}(K_{a})}^{t-1}\left(1-\frac{\eta}{8} \right)^{2(t-1-s)}(M_{1,s}+7\|\mathbf{h}_{s}\|)\] \[\geq\left(1-\frac{\eta}{8}\right)^{2t}\frac{1}{4}\min\left\{\sin^ {2}\phi_{\tau_{t}(K_{a})},\varpi_{0}^{2}\right\}-2\eta\sum_{s=\tau_{t}(K_{a}) }^{t-1}\left(1-\frac{\eta}{8}\right)^{2(t-1-s)}(M_{1,s}+7\|\mathbf{h}_{s}\|)\] \[\stackrel{{(ii)}}{{\geq}}\left(1-\frac{\eta}{8} \right)^{2t}\frac{1}{4}\min\left\{\frac{\sin^{2}\left[\pi-\left((20K_{a}^{2})^ {-1}\wedge 1\right)\right]}{2},\varpi_{0}^{2}\right\}\] \[\qquad-140c\eta(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}K_{a}^{2}-2 \eta\max_{0\leq\tau\leq t-1}\left|\sum_{s=\tau}^{t-1}\left(1-\frac{\eta}{8} \right)^{2(t-1-s)}M_{1,s}\right|\] \[\stackrel{{(iii)}}{{\geq}}\frac{1}{2k}\cdot\frac{1 }{4}\min\left\{\frac{\sin^{2}\left[\pi-\left((20K_{a}^{2})^{-1}\wedge 1\right) \right]}{2},\varpi_{0}^{2}\right\}-140c\eta(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)} K_{a}^{2}\] \[\qquad-\frac{12cK_{a}^{2}\eta}{N}(3+5\eta K_{a}^{2})\sqrt{\log( 1/\eta\delta)}\sum_{s=0}^{t-1}\sum_{i=1}^{N}\left(1-\frac{\eta}{8}\right)^{2(t -1-s)}\] \[\stackrel{{(iv)}}{{\geq}}\frac{\min\left\{\sin^{2} \left[\pi-\left((20K_{a}^{2})^{-1}\wedge 1\right)\right],\varpi_{0}^{2}\right\}}{25k},\]

where \((i)\) holds due to (D.27) and (D.29); \((ii)\) follows from (D.30) and (D.26); \((iii)\) comes from (D.31) and Lemma 11; and \((iv)\) holds due to the assumption D.25. Plugging the choice of \(\log(1+2k^{2})/4\), together with \(\|\mathbf{v}_{t}\|\leq 3\), we can prove the conclusion

\[\sin^{2}\phi_{\tau_{a}}\geq\frac{\min\left\{\sin^{2}\left[\pi-\left((20K_{a}^{2}) ^{-1}\wedge 1\right)\right],\varpi_{0}^{2}\right\}}{225k}\geq\frac{\varpi^{2}}{22 5k}.\]

### Self-correction of the First Layer

**Theorem 2 restated.** For the initial point \((\mathbf{v}_{0},\mathbf{a}_{0})\) with \(\mathbf{a}_{0}^{\top}\mathbf{a}^{*}\geq\gamma_{a}\) and \(\sin\phi_{0}>0\), we denote \(\tau_{v}=\inf\{t\geq 0:\phi_{v}\leq\tilde{\phi}^{\prime}\}\), where \(\cos\tilde{\phi}^{\prime}=\frac{144\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{* })^{2}}{k}\log(4+k^{2})\). If the learning rate satisfies

\[c\|\mathbf{a}^{*}\|^{2}\sqrt{\log(Ndk/\delta)}\max\left\{\eta(\sqrt{Id}+I),\sqrt{ \frac{\eta}{N}}\right\}\leq\sin^{2}\phi_{0},\] (D.32)

for a large absolute constant \(c>0\), it holds that \(\tau_{v}\leq O\left(\frac{1}{\eta}\frac{k^{2}}{\sin^{3}\phi_{0}}\right)\) and \(\mathbf{a}_{\tau_{v}}^{\top}\mathbf{a}^{*}\geq\gamma_{a}/32\) with probability at least \(1-\delta\).

Proof.: Find \(\tilde{\phi}^{\prime}\in(\pi/2,\pi)\) such that \(\cos\tilde{\phi}^{\prime}=-\frac{\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^ {2}}{k}\log(4+k^{2})\). We introduce the following stopping time w.r.t. the angle \(\phi\) during the training process,

\[\tau_{+}(\phi)=\inf_{t\geq 0}\left\{t:\phi_{t}\leq\phi\right\}\quad\text{ for}\quad\phi\in[0,\pi].\]

Initial angle:\(\phi_{0}\in(\tilde{\phi}^{\prime},\pi)\).For any \(s\leq\tau_{+}(\tilde{\phi}^{\prime})\), under the event \(\mathcal{E}_{\mathrm{good}}\), we have

\[\cos\phi_{s}^{i} \leq\cos\phi_{s}+\left|\cos\phi_{s}^{i}-\cos\phi_{s}\right|\] \[\leq\cos\tilde{\phi}^{\prime}+12c\eta K_{a}^{2}(\sqrt{Id}+I)\sqrt {\log(Nd/\delta)}\] \[\leq-\frac{\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k} \log(4+k^{2})+\frac{\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k}\log(4 +k^{2})=0.\]

It means that \(\phi_{s}^{i}\geq\pi/2\) holds for any \(0\leq s\leq\min\{\tau_{+}(\tilde{\phi}^{\prime}),\widetilde{O}(\eta^{-2})\}\). As a consequence, we have

\[\frac{1}{N}\sum_{i=1}^{N}B_{s}^{i}\geq\zeta_{a}-\frac{\pi-1}{\pi+k-1}\frac{( \mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{\|\mathbf{a}^{*}\|^{2}}\left(1-\eta\frac{\pi+k-1}{2 \pi}\right)^{s}.\]

Notice that if \(t\leq\frac{1}{\eta}\frac{2\pi}{\pi-1}\), we have

\[\left(1-\eta\frac{\pi-1}{2\pi}\right)^{t}\geq\left(1-\eta\frac{\pi-1}{2\pi} \right)^{\frac{1}{\eta}\frac{2\pi}{\pi-1}}\geq\frac{1}{4},\]

which holds because \(\eta\frac{\pi-1}{2\pi}\leq\frac{1}{2}\) and \((1-x)^{1/x}\) is decreasing in \([0,1]\). If \(t\geq\frac{1}{\eta}\frac{2\pi}{\pi-1}\), we also have

\[1-\left(1-\eta\frac{\pi-1}{2\pi}\right)^{t}\geq 1-\left(1-\eta\frac{\pi-1}{2\pi} \right)^{\frac{1}{\eta}\frac{2\pi}{\pi-1}}\geq 1-e^{-1}\geq\frac{1}{4},\]

because \((1-x)^{1/x}\leq e^{-1}\) for \(x\in[0,1]\). Together with the dynamic (D.1) and \(\mathbf{a}_{0}^{\top}\mathbf{a}^{*}\geq\gamma_{a}\), under the event \(\mathcal{E}_{\mathrm{good}}\), we have

\[\mathbf{a}_{s}^{\top}\mathbf{a}^{*}\geq\frac{\zeta_{a}\|\mathbf{a}^{*}\|^{2 }}{2(\pi-1)}\left[1-\left(1-\frac{\eta(\pi-1)}{2\pi}\right)^{s}\right]-2c\sqrt {\frac{\eta\log(2t/\delta)}{N}}K_{a}^{2}\] \[\qquad\qquad\qquad\qquad\qquad\qquad+\left(\gamma_{a}-\frac{(\pi -1)(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k(\pi+k-1)}\right)\left(1-\frac{\eta(\pi-1 )}{2\pi}\right)^{s}\] \[\overset{(i)}{\geq}\frac{1}{4}\frac{32(\pi-1)}{k(\pi+k-1)}+ \frac{1}{4}\left(\gamma_{a}-\frac{(\pi-1)(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k(\pi+k -1)}\right)-3c\sqrt{\frac{\eta k\log(1/\eta\delta)}{N}}K_{a}^{2}\]\[\stackrel{{(ii)}}{{=}}\frac{\gamma_{a}}{2}+\frac{1}{4} \left(\gamma_{a}-\frac{\gamma_{a}}{16}\right)-3c\sqrt{\frac{\eta k\log(1/\eta \delta)}{N}}K_{a}^{2}\] \[\stackrel{{(iii)}}{{\geq}}\frac{\gamma_{a}}{4},\] (D.33)

where \((i)\) holds due to (D.8); \((ii)\) holds since \(\gamma_{a}=\frac{16(\pi-1)}{k(\pi+k-1)}\) and \((iii)\) follows from the condition (D.6). Under the event \(\mathcal{E}_{\rm good}\), we know for any \(s\leq\min\{\tau_{+}(\tilde{\phi}^{\prime}),\widetilde{O}(\eta^{-2})\}\)

\[(\mathbf{a}_{s}^{i})^{\top}\mathbf{a}^{*} \geq\mathbf{a}_{s}^{\top}\mathbf{a}^{*}-\left|\mathbf{a}_{s}^{\top}\mathbf{a}^{*}- (\mathbf{a}_{s}^{i})^{\top}\mathbf{a}^{*}\right|\] \[\geq\frac{\gamma_{a}}{4}-cK_{a}\eta(\sqrt{Ik}+I)\sqrt{\log(k/ \delta)}\] \[=\frac{4(\pi-1)}{k(\pi+k-1)}-cK_{a}\eta(\sqrt{Ik}+I)\sqrt{\log(k/ \delta)}\] \[\geq\frac{\gamma_{a}}{8},\] (D.34)

holds with probability at least \(1-\delta\). Recall that \(\check{\mathbf{v}}_{t}-\mathbf{v}_{t}=\eta\mathbf{h}_{t-1}\), then it follows that

\[\left|\|\check{\mathbf{v}}_{t}\right\| ^{2}\sin^{2}\check{\phi}_{t}-\|\mathbf{v}_{t}\|^{2}\sin^{2}\phi_{t} \right|=\left|\|\check{\mathbf{v}}_{t}\|^{2}-\|\mathbf{v}_{t}\|^{2}-\left((\check{\mathbf{v }}_{t}^{\top}\mathbf{w}^{*})^{2}-(\mathbf{v}_{t}^{\top}\mathbf{w}^{*})^{2})\right|\] \[=\left|2\eta\mathbf{v}_{t}^{\top}\mathbf{h}_{t-1}+\eta^{2}\|\mathbf{h}_{t-1} \|^{2}-2\eta(\mathbf{v}_{t}^{\top}\mathbf{w}^{*})(\mathbf{h}_{t-1}^{\top}\mathbf{w}^{*})-\eta^ {2}(\mathbf{h}_{t-1}^{\top}\mathbf{w}^{*})^{2}\right|\] \[\leq 2\eta\left|\mathbf{v}_{t}^{\top}\left(\mathbf{I}-\mathbf{w}^{*}(\mathbf{ w}^{*})^{\top}\right)\mathbf{h}_{t-1}\right|+\eta^{2}\mathbf{h}_{t-1}^{\top}\left( \mathbf{I}-\mathbf{w}^{*}(\mathbf{w}^{*})^{\top}\right)\mathbf{h}_{t-1}\] \[\leq 2\eta\left\|\left(\mathbf{I}-\mathbf{w}^{*}(\mathbf{w}^{*})^{\top} \right)\mathbf{v}_{t}\right\|\left\|\mathbf{h}_{t-1}\right\|+\eta^{2}\mathbf{h}_{t-1}^{ \top}\left(\mathbf{I}-\mathbf{w}^{*}(\mathbf{w}^{*})^{\top}\right)\mathbf{h}_{t-1}\] \[\stackrel{{(i)}}{{=}}2\eta\left(\mathbf{v}_{t}^{\top} \left(\mathbf{I}-\mathbf{w}^{*}(\mathbf{w}^{*})^{\top}\right)\mathbf{v}_{t}\right)^{1/2} \|\mathbf{h}_{t-1}\|+\eta^{2}\mathbf{h}_{t-1}^{\top}\left(\mathbf{I}-\mathbf{w}^{*}(\mathbf{w }^{*})^{\top}\right)\mathbf{h}_{t-1}\] \[=2\eta\left(\|\mathbf{v}_{t}\|^{2}-(\mathbf{v}_{t}^{\top}\mathbf{w}^{*})^{2} \right)^{1/2}\|\mathbf{h}_{t-1}\|+\eta^{2}\mathbf{h}_{t-1}^{\top}\left(\mathbf{I}-\mathbf{ w}^{*}(\mathbf{w}^{*})^{\top}\right)\mathbf{h}_{t-1}\] \[\stackrel{{(ii)}}{{=}}2\eta\sin\phi_{t}\|\mathbf{v}_{t }\|\|\mathbf{h}_{t-1}\|+\eta^{2}\|\mathbf{h}_{t-1}\|^{2}.\] (D.35)

where \((i)\) holds due to \(\mathbf{I}-\mathbf{w}^{*}(\mathbf{w}^{*})^{\top}\) is idempotent; and \((ii)\) holds due to \(\|\mathbf{w}^{*}\|=1\). Invoking Lemma 1 and (D.26), for any \(s\leq t-1\leq\min\{\tau_{+}(\tilde{\phi}^{\prime}),\widetilde{O}(\eta^{-2})\}\), under the event \(\mathcal{E}_{\rm good}\), we can get

\[\|\mathbf{v}_{s}\|^{2}\sin^{2}\phi_{s} =\left(1-\eta\lambda_{s-1}\cos\phi_{s-1}\right)^{2}\|\mathbf{v}_{s-1} \|^{2}\sin^{2}\phi_{s-1}-2\eta M_{1,s-1}\] \[\qquad+\eta^{2}M_{2,s-1}+\|\mathbf{v}_{s}\|^{2}\sin^{2}\phi_{s}-\| \check{\mathbf{v}}_{s}\|^{2}\sin^{2}\tilde{\phi}_{s}\] \[\stackrel{{(i)}}{{\geq}}\|\mathbf{v}_{s-1}\|^{2}\sin^{2} \phi_{s-1}-2\eta M_{1,s-1}-6\eta\|\mathbf{h}_{s-1}\|-\eta^{2}\|\mathbf{h}_{s-1}\|^{2}\] \[\geq\|\mathbf{v}_{0}\|^{2}\sin^{2}\phi_{0}-2\eta\sum_{l=0}^{s-1}M_{1, l}-\eta\sum_{l=0}^{s-1}(6\|\mathbf{h}_{l}\|+\eta\|\mathbf{h}_{l}\|^{2})\] \[\geq\frac{\sin^{2}\phi_{0}}{3}-450c\eta(\sqrt{Id}+I)\sqrt{\log(Nd/ \delta)}K_{a}^{2}-24cK_{a}\left(3+5\eta K_{a}^{2}\right)\sqrt{\frac{\eta\log (1/\delta)}{N}}\] \[\geq\frac{\sin^{2}\phi_{0}}{6},\]

where \((i)\) holds due to (D.34) and \(\cos\phi_{s}<0\) for \(s\leq\tau_{+}(\tilde{\phi}^{\prime})\); \((ii)\) follows from assumption (D.32). It implies that \(\sin^{2}\phi_{s}\geq\sin^{2}\phi_{0}/54\) for any \(s\leq t-1\leq\min\{\tau_{+}(\tilde{\phi}^{\prime}),\widetilde{O}(\eta^{-2})\}\). Then we can lower bound \(\sin^{2}\phi_{s}^{i}\) by

\[\sin^{2}\phi_{s}^{i} \geq\sin^{2}\phi_{s}-\left|\sin^{2}\phi_{s}^{i}-\sin^{2}\phi_{s} \right|\] \[\geq\frac{\sin^{2}\phi_{0}}{54}-2\left|\cos\phi_{s}^{i}-\cos\phi_ {s}\right|\] \[\geq\frac{\sin^{2}\phi_{0}}{54}-4c\eta K_{a}^{2}(\sqrt{Id}+I)\sqrt {\log(Nd/\delta)}\] \[\geq\frac{\sin^{2}\phi_{0}}{64}.\]Together with the dynamic (D.12) and lower bound (D.34), we can guarantee

\[\cos\phi_{t} =\frac{1}{\|\mathbf{v}_{t}\|}\left\{\mathbf{v}_{0}^{\top}\mathbf{w}^{*}+\eta \sum_{m=t_{0}}^{t-1}\frac{1}{N}\sum_{i=1}^{N}\frac{\pi-\phi_{m}^{i}}{2\pi}\frac{ (\mathbf{a}_{m}^{i})^{\top}\mathbf{a}^{*}}{\|\mathbf{v}_{m}\|}\sin^{2}\phi_{m}-\eta\sum_{m=t _{0}}^{t-1}(\mathbf{\xi}_{m}^{\top}\mathbf{w}^{*}+\mathbf{h}_{m}^{\top}\mathbf{w}^{*})\right\}\] \[\geq\frac{1}{\|\mathbf{v}_{t}\|}\Bigg{\{}-3+\eta t\cdot\frac{\gamma_{ a}\sin^{2}\phi_{0}}{288}\cdot\frac{\arcsin(\sin\phi_{0}/6)}{2\pi}\] \[\qquad\qquad\qquad\qquad-c\eta K_{a}^{2}(\sqrt{Id}+I)\sqrt{\log( Nd/\delta)}-cK_{a}\sqrt{\frac{\eta\log(Nd/\eta\delta)}{N}}\Bigg{\}}.\] (D.36)

Now we take

\[\tau_{2,1}=\frac{1}{\eta}\cdot\frac{5429}{\gamma_{a}\sin^{2}\phi_{0}}\frac{1}{ \arcsin(\sin\phi_{0}/6)}\leq\frac{40000}{\eta\gamma_{a}}\frac{1}{\sin^{3}\phi_{ 0}},\]

where the inequality holds since \(\arcsin(\sin\phi_{0}/6)-\arcsin(0)\geq\sin\phi_{0}/6\) due to \(\arcsin^{\prime}(x)=\frac{1}{\sqrt{1-x^{2}}}\). Plugging it into (D.36), we have

\[\cos\phi_{\tau_{2,1}}\geq\frac{1}{\|\mathbf{v}_{t}\|}(-3+4-1)>\cos\tilde{\phi}^{f},\]

which implies

\[\mathbb{P}\left(\tau_{2,1}\leq\tau_{+}(\tilde{\phi}^{f})\right) =\mathbb{P}\left(\left\{\tau_{2,1}-1\leq\tau_{+}(\tilde{\phi}^{f} )\right\}\cap\left\{\cos\phi_{\tau_{2,1}}\leq\cos\tilde{\phi}^{f}\right\}\right)\] \[\leq\mathbb{P}\left(\left\{\tau_{2,1}-1\leq\tau_{+}(\tilde{\phi}^ {f})\right\}\cap\mathcal{E}_{\mathrm{good}}^{c}\right)\leq 5\delta.\]

Initial angle: \(\phi_{0}\in(\tilde{\phi}^{f},\tilde{\phi}^{f})\).Now assign \(t_{0}=\tau_{+}(\tilde{\phi}^{f})\). Denote

\[\tilde{\mathcal{A}}_{s}=\left\{\forall t_{0}\leq m\leq s:\mathbf{a}_{m}^{\top}\mathbf{ a}^{*}\geq\frac{\gamma_{a}}{16}\right\},\quad\text{for}\;s\geq t_{0}.\]

Clearly, \(\mathbb{P}(\tilde{\mathcal{A}}_{t_{0}})\geq 1-7\delta\) due to (D.33). Similar to (D.34), we can verify that \(\min_{i\in[N]}(\mathbf{a}_{m}^{i})^{\top}\mathbf{a}^{*}\geq\gamma_{a}/32\) for any \(m\leq s\) under \(\tilde{\mathcal{A}}_{s}\cap\mathcal{E}_{\mathrm{good}}\). In addition, under the event \(\mathcal{E}_{\mathrm{good}}\), for any \(\ell\geq 0\) we have

\[\cos\phi_{\ell} =\frac{1}{\|\mathbf{v}_{\ell}\|}\left\{\mathbf{v}_{0}^{\top}\mathbf{w}^{*}+ \eta\sum_{m=t_{0}}^{\ell-1}\frac{1}{N}\sum_{i=1}^{N}\frac{\pi-\phi_{m}^{i}}{2 \pi}\frac{(\mathbf{a}_{m}^{i})^{\top}\mathbf{a}^{*}}{\|\mathbf{v}_{m}\|}\sin^{2}\phi_{m}- \eta\sum_{m=t_{0}}^{\ell-1}(\mathbf{\xi}_{m}^{\top}\mathbf{w}^{*}+\mathbf{h}_{m}^{\top}\bm {w}^{*})\right\}\] \[\geq 6\cos\tilde{\phi}^{f}-2c\eta K_{a}^{2}(\sqrt{Id}+I)\sqrt{\log( Nd/\delta)}-2cK_{a}\sqrt{\frac{\eta\log(Nd/\eta\delta)}{N}}\] \[\geq-\frac{6\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k} \log(4+k^{2})-\frac{2\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k}\log( 4+k^{2})\] \[\geq-\frac{144\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k }\log(4+k^{2})=\cos\tilde{\phi}^{u}.\] (D.37)

It means that \(\tilde{\phi}^{l}\leq\phi_{\ell}\leq\tilde{\phi}^{f}\) for any \(\ell\leq s\leq\min\{\tau_{+}(\tilde{\phi}^{l}),\widetilde{O}(\eta^{-2})\}\) under the event \(\tilde{\mathcal{A}}_{s}\cap\mathcal{E}_{\mathrm{good}}\). Hence we know (D.18) holds. Consequently, the lower bound (D.22) also holds for \(\frac{1}{N}\sum_{i=1}^{N}B_{s}^{i}\). Therefore, similar to (D.33), we can further verify that

\[\mathbf{a}_{s+1}^{\top}\mathbf{a}^{*} \geq\frac{\zeta_{a}\|\mathbf{a}^{*}\|^{2}}{2(\pi-1)}\left[1-\left(1- \frac{\eta(\pi-1)}{2\pi}\right)^{s}\right]-3c\sqrt{\frac{\eta k\log(1/\eta \delta)}{N}}K_{a}^{2}\] \[\qquad+\left(\frac{\gamma_{a}}{4}-\frac{(\pi-1)(\mathbf{1}^{\top}\bm {a}^{*})^{2}}{k(\pi+k-1)}\right)\left(1-\frac{\eta(\pi-1)}{2\pi}\right)^{s}\] \[\geq\frac{\gamma_{a}}{16},\]

where we used \(\mathbf{a}_{t_{0}}^{\top}\mathbf{a}^{*}\geq\gamma_{a}/4\). This yields that for any \(t_{0}\leq s\leq t-1\),

\[\tilde{\mathcal{A}}_{s}\cap\mathcal{E}_{\mathrm{good}}\cap\left\{t-1\leq\tau_{+ }(\tilde{\phi}^{l})\right\}\subseteq\left\{\mathbf{a}_{s+1}^{\top}\mathbf{a}^{*}\geq \frac{\gamma_{a}}{16}\right\},\]which implies that

\[\tilde{\mathcal{A}}_{s}\cap\mathcal{E}_{\mathrm{good}}\cap\left\{t-1 \leq\tau_{+}(\tilde{\phi}^{l})\right\}\subseteq\tilde{\mathcal{A}}_{s+1}.\]

It follows that

\[\mathbb{P}\left(\tilde{\mathcal{A}}_{t}^{c}\cap\left\{t-1\leq\tau_ {+}(\tilde{\phi}^{l})\right\}\right) =\mathbb{P}\left(\tilde{\mathcal{A}}_{t-1}^{c}\cap\left\{t-1\leq \tau_{+}(\tilde{\phi}^{l})\right\}\right)\] \[\qquad+\mathbb{P}\left(\tilde{\mathcal{A}}_{t-1}\cap\left\{t-1\leq \tau_{+}(\tilde{\phi}^{l})\right\}\cap\left\{\boldsymbol{a}_{s+1}^{\top} \boldsymbol{a}^{*}<\frac{\gamma_{a}}{16}\right\}\right)\] \[\leq\mathbb{P}\left(\tilde{\mathcal{A}}_{t-1}^{c}\cap\left\{t-1 \leq\tau_{+}(\tilde{\phi}^{l})\right\}\right)+5\delta\] \[\leq\mathbb{P}\left(\tilde{\mathcal{A}}_{t_{0}}^{c}\cap\left\{t-1 \leq\tau_{+}(\tilde{\phi}^{l})\right\}\right)+5\delta(t-t_{0})\] \[\leq\mathbb{P}\left(\tilde{\mathcal{A}}_{t_{0}}^{c}\right)-5 \delta(t-t_{0})\] \[\leq 7\delta+5\delta(t-t_{0}).\] (D.38)

Similar to (D), we can guarantee that \(\min_{i\in[N]}(\boldsymbol{a}_{s}^{i})^{\top}\boldsymbol{a}^{*}\geq\gamma_{a}/ 64\) holds for any \(s\leq t-1\leq\min\{\tau_{+}(\tilde{\phi}^{l}),\tilde{O}(\eta^{-2})\}\) under the event \(\tilde{\mathcal{A}}_{t}\cap\mathcal{E}_{\mathrm{good}}\). It follows that

\[\cos\phi_{t} =\frac{1}{\|\boldsymbol{v}_{t}\|}\left\{\boldsymbol{v}_{0}^{\top} \boldsymbol{w}^{*}+\eta\sum_{m=t_{0}}^{\ell-1}\frac{1}{N}\sum_{i=1}^{N}\frac{ \pi-\phi_{m}^{i}}{2\pi}\frac{(\boldsymbol{a}_{m}^{i})^{\top}\boldsymbol{a}^{* }}{\|\boldsymbol{v}_{m}\|}\sin^{2}\phi_{m}-\eta\sum_{m=t_{0}}^{t-1}(\boldsymbol {\xi}_{m}^{\top}\boldsymbol{w}^{*}+\boldsymbol{h}_{m}^{\top}\boldsymbol{w}^{*})\right\}\] \[\geq\frac{1}{\|\boldsymbol{v}_{t}\|}\left\{\|\boldsymbol{v}_{0}\| \cos\phi_{0}+\frac{\eta t}{3}\cdot\frac{\gamma_{a}\sin^{2}\tilde{\phi}^{u}}{6 4}\cdot\frac{\pi-\tilde{\phi}^{u}}{2\pi}\right.\] \[\qquad\qquad\qquad-2c\eta K_{a}^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/ \delta)}-2cK_{a}\sqrt{\frac{\eta\log(Nd/\eta\delta)}{N}}\Bigg{\}}.\] (D.39)

Taking

\[\tau_{2,2}=\frac{9\cos\tilde{\phi}^{l}}{\eta}\frac{384\pi}{\gamma_ {a}\sin^{2}\tilde{\phi}^{u}}\frac{1}{\pi-\tilde{\phi}^{u}}.\]

Plugging \(t=t_{0}+\tau_{2,2}\) into (D), under \(\left\{t_{0}+\tau_{2,2}-1\leq\tau_{+}(\tilde{\phi}^{l})\right\}\cap\tilde{ \mathcal{A}}_{t_{0}+\tau_{2,2}}\cap\mathcal{E}_{\mathrm{good}}\), we can get

\[\cos\phi_{t_{0}+\tau_{2,2}} \geq 6\cos\tilde{\phi}^{f}+3\cos\tilde{\phi}^{l}-2c\eta K_{a}^{2}( \sqrt{Id}+I)\sqrt{\log(Nd/\delta)}-2cK_{a}\sqrt{\frac{\eta\log(Nd/\eta\delta) }{N}}\] \[\geq 2\cos\tilde{\phi}^{l}-2c\eta K_{a}^{2}(\sqrt{Id}+I)\sqrt{ \log(Nd/\delta)}-2cK_{a}\sqrt{\frac{\eta\log(Nd/\eta\delta)}{N}}\geq\cos\tilde {\phi}^{l},\]

where we used the condition (D.6). Invoking (D), we can have

\[\mathbb{P}\left(\tau_{+}(\tilde{\phi}^{l})\geq t_{0}+\tau_{2,2}\right) =\mathbb{P}\left(\left\{\tau_{+}(\tilde{\phi}^{l})\geq t_{0}+\tau _{2,2}-1\right\}\cap\left\{\cos\phi_{t_{0}+\tau_{2,2}}\leq\cos\tilde{\phi}^{l} \right\}\right)\] \[=\mathbb{P}\left(\left\{\tau_{+}(\tilde{\phi}^{l})\geq t_{0}+ \tau_{2,2}-1\right\}\cap\tilde{\mathcal{A}}_{t_{0}+\tau_{2,2}}\cap\left\{\cos \phi_{t_{0}+\tau_{2,2}}\leq\cos\tilde{\phi}^{l}\right\}\right)\] \[+\mathbb{P}\left(\left\{\tau_{+}(\tilde{\phi}^{l})\geq t_{0}+ \tau_{2,2}-1\right\}\cap\tilde{\mathcal{A}}_{t_{0}+\tau_{2,2}}^{c}\cap\left\{ \cos\phi_{t_{0}+\tau_{2,2}}\leq\cos\tilde{\phi}^{l}\right\}\right)\] \[\leq 5\delta+\mathbb{P}\left(\left\{\tau_{+}(\tilde{\phi}^{l})\geq t _{0}+\tau_{2,2}-1\right\}\cap\tilde{\mathcal{A}}_{t_{0}+\tau_{2,2}}^{c}\right)\] \[\leq 12\delta+5\delta\cdot\tau_{2,2}.\]

Recalling \(t_{0}=\tau_{+}(\tilde{\phi}^{f})\) and (D), we have

\[\mathbb{P}\left(\tau_{+}(\tilde{\phi}^{l})\geq\tau_{2,1}+\tau_{2, 2}\right) \leq\mathbb{P}\left(\left\{\tau_{+}(\tilde{\phi}^{l})\geq\tau_{+}( \tilde{\phi}^{f})+\tau_{2,2}\right\}\cap\left\{\tau_{+}(\tilde{\phi}^{f})\leq \tau_{2,1}\right\}\right)\] \[\qquad+\mathbb{P}\left(\left\{\tau_{+}(\tilde{\phi}^{l})\geq\tau _{+}(\tilde{\phi}^{f})+\tau_{2,2}\right\}\cap\left\{\tau_{+}(\tilde{\phi}^{f}) \geq\tau_{2,1}\right\}\right)\]\[\leq\mathbb{P}\left(\tau_{+}(\tilde{\phi}^{l})\geq\tau_{+}(\tilde{ \phi}^{f})+\tau_{2,2}\right)+\mathbb{P}\left(\tau_{+}(\tilde{\phi}^{f})\geq\tau _{2,1}\right)\] \[\leq 17\delta+5\delta\cdot\tau_{2,2}.\]

By adjusting the level of \(\delta\), we can verify that \(\tau_{v}\leq\tau_{2,1}+\tau_{2,2}\) such that \(\phi_{\tau_{v}}\leq\tilde{\phi}^{l}\) and \(\mathbf{a}_{\tau_{v}}^{\top}\mathbf{a}^{*}\geq\gamma_{a}/32\) with high probability. Now recalling that

\[\gamma_{a}=\frac{16(\pi-1)(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k(\pi+k-1)},\quad\sin ^{2}\tilde{\phi}^{u}=\sqrt{1-\left(\frac{144\pi}{\pi+k-1}\frac{(\mathbf{1}^{\top} \mathbf{a}^{*})^{2}}{k}\log(4+k^{2})\right)^{2}}\geq\frac{1}{2},\]

we have

\[\tau_{2,1}+\tau_{2,2} =\frac{1}{\eta\gamma_{a}}\left(\frac{40000}{\sin^{3}\phi_{0}}+ \frac{1}{\sin^{2}\tilde{\phi}^{u}}\frac{3465\pi\cos\tilde{\phi}^{l}}{\pi- \tilde{\phi}^{u}}\right)\] \[\lesssim\frac{1}{\eta}\left(\frac{k^{2}}{\sin^{3}\phi_{0}}+\log(k)\right)\] \[\lesssim\frac{1}{\eta}\frac{k^{2}}{\sin^{3}\phi_{0}},\]

where the last inequality holds due to (D.8). 

## Appendix E Convergence with Linear Speedup

### Convergence of the First Layer

**Lemma 4** restated.: Under the settings in Theorem 3. Suppose the initial point satisfies \(\phi_{0}\leq\tilde{\phi}^{l}\) and \(\mathbf{a}_{0}^{\top}\mathbf{a}^{*}\geq\gamma_{a}/32\). With probability at least \(1-\delta\), we can guarantee that \(\sin^{2}\phi_{t}\lesssim\epsilon\) holds for any \(\tilde{O}\left(k^{2}\eta^{-1}\right)\leq t\leq\tilde{O}(\eta^{-2})\).

Proof.: Denote the event \(\mathcal{C}_{t}=\{\mathbf{a}_{s}^{\top}\mathbf{a}^{*}\geq\gamma_{a}/128\cdot(\|\mathbf{a}^ {*}\|^{2}\wedge 1):\forall s\leq t\}\). For any \(s\leq t-1\), with probability at least \(1-3\delta\), we have

\[\mathds{1}_{\mathcal{C}_{t-1}}\cos\phi_{s} =\frac{\mathds{1}_{\mathcal{C}_{t-1}}}{\|\mathbf{v}_{s}\|}\left\{\mathbf{ v}_{0}^{\top}\mathbf{w}^{*}+\eta\sum_{l=0}^{s-1}\frac{\pi-\phi_{l}}{2\pi}\frac{ \mathbf{a}_{l}^{\top}\mathbf{a}^{*}}{\|\mathbf{v}_{l}\|}\sin^{2}\phi_{l}-\eta\sum_{l=0}^{s -1}(\mathbf{\xi}_{l}^{\top}\mathbf{w}^{*}+\mathbf{h}_{l}^{\top}\mathbf{w}^{*})\right\}\] \[\geq\frac{\mathds{1}_{\mathcal{C}_{t-1}}}{\|\mathbf{v}_{s}\|}\left\{ \frac{\cos\tilde{\phi}^{l}}{3}-2c\eta K_{a}^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/ \delta)}-2cK_{a}\sqrt{\frac{\eta\log(Nd/\eta\delta)}{N}}\right\}\] \[\geq\mathds{1}_{\mathcal{C}_{t-1}}\cdot\frac{2(\mathbf{1}^{\top}\mathbf{ a}^{*})^{2}}{k(\pi+k-1)},\] (E.1)

where we used the assumption \(I\lesssim\frac{d}{\epsilon N}\min\left\{1,\frac{k^{2}d^{1/2}}{N^{1/2}},\frac{k^{4} d}{N^{2}\epsilon}\right\}\) and \(\epsilon<dk^{-2}\). Together with (D.4), we can guarantee \(\cos\phi_{t}^{i}\geq\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k(\pi+k-1)}\) holds for any \(i\in[N]\) with probability at least \(1-\delta\). It follows that for any \(s\leq t-1\),

\[\mathds{1}_{\mathcal{C}_{t-1}}\frac{1}{N}\sum_{i=1}^{N}B_{s}^{i} \geq\mathds{1}_{\mathcal{C}_{t-1}}\left\{\frac{1}{N}\sum_{i=1}^{N }g(\phi_{s}^{i})-1\right\}\] \[\geq\mathds{1}_{\mathcal{C}_{t-1}}\left\{g\left(\arccos\left( \frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k(\pi+k-1)}\right)\right)-g(\pi/2)\right\}\] \[\overset{(i)}{\geq}\mathds{1}_{\mathcal{C}_{t-1}}\left\{g^{\prime }(\pi/6)\left|\arccos\left(\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k(\pi+k-1)} \right)-\arccos(0)\right|\right\}\] \[\overset{(ii)}{\geq}\mathds{1}_{\mathcal{C}_{t-1}}\cdot\frac{5 \pi}{12}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k(\pi+k-1)},\] (E.2)where \((i)\) holds since \(g^{\prime}(x)=-(\pi-x)\sin x\) is decreasing in \((0,\pi/2]\) and \(\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k(\pi+k-1)}\leq\frac{\sqrt{3}}{2}\) due to Assumption 1; and \((ii)\) holds since \(\arccos^{\prime}(x)=-\frac{1}{\sqrt{1-x^{2}}}\). Using the dynamic in Lemma 2, under the event \(\mathcal{C}_{t-1}\), we can get

\[\mathbf{a}_{t}^{\top}\mathbf{a}^{*} \geq\left(1-\eta\frac{\pi-1}{2\pi}\right)^{t}\mathbf{a}_{0}^{\top}\bm {a}^{*}+\frac{\eta\|\mathbf{a}^{*}\|^{2}}{2\pi}\sum_{s=0}^{t-1}\left(1-\eta\frac{ \pi-1}{2\pi}\right)^{t-1-s}\frac{1}{N}\sum_{i=1}^{N}B_{s}^{i}-|S(\mathbf{\epsilon }_{0:t-1})|\] \[\geq\left(1-\eta\frac{\pi-1}{2\pi}\right)^{t}\frac{\gamma_{a}}{32 }+\left[1-\left(1-\eta\frac{\pi-1}{2\pi}\right)^{t}\right]\frac{5\|\mathbf{a}^{*} \|^{2}}{12}\frac{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k(\pi+k-1)}-2c\sqrt{\frac{ \eta k\log(1/\eta\delta)}{N}}K_{a}^{2}\] \[\geq\frac{\|\mathbf{a}^{*}\|^{2}\wedge 1}{4}\min\left\{\frac{\gamma_{a}}{ 32},\frac{5(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{12k(\pi+k-1)}\right\}-2c\sqrt{ \frac{\eta k\log(1/\eta\delta)}{N}}K_{a}^{2}\] \[\geq\frac{\gamma_{a}}{128}\cdot(\|\mathbf{a}^{*}\|^{2}\wedge 1).\]

By adjusting the level of \(\delta\), we can guarantee

\[\mathbb{P}\left(\mathcal{C}_{\widetilde{O}(\eta^{-2})}\right)=\mathbb{P} \left(\mathbf{a}_{s}^{\top}\mathbf{a}^{*}\geq\frac{\gamma_{a}}{128}\cdot(\|\mathbf{a}^{*}\| ^{2}\wedge 1):\forall s\leq t\right)\geq 1-\delta.\]

Next we assume \(\mathcal{C}_{\widetilde{O}(\eta^{-2})}\) happens and hide the indicator.

Invoking (E.1), we have for any \(s\leq\widetilde{O}(\eta^{-2})\)

\[\cos\phi_{s}\lambda_{s}=\cos\phi_{s}\frac{1}{N}\sum_{i=1}^{N} \frac{\pi-\phi_{t}^{i}}{2\pi}\frac{(\mathbf{a}_{t}^{i})^{\top}\mathbf{a}^{*}}{\|\mathbf{v} _{t}\|^{2}} \geq\frac{2(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}{k(\pi+k-1)}\cdot \frac{\pi-\pi/2}{2\pi}\frac{\gamma_{a}}{128}\cdot(\|\mathbf{a}^{*}\|^{2}\wedge 1)\] \[\geq\frac{\gamma_{a}}{512}\cdot(\|\mathbf{a}^{*}\|^{2}\wedge 1)=: \frac{\tilde{\gamma}_{a}}{12},\] (E.3)

where we used \(\phi_{t}^{i}<\pi/2\). Invoking Lemma 1, we have

\[\|\mathbf{v}_{t}\|^{2}\sin^{2}\phi_{t} \leq\left(1-\frac{\eta\tilde{\gamma}_{a}}{12}\right)^{2}\|\mathbf{v} _{t-1}\|^{2}\sin^{2}\phi_{t-1}-2\eta M_{1,t-1}+\eta^{2}M_{2,t-1}+H_{t}\] \[\leq\left(1-\frac{\eta\tilde{\gamma}_{a}}{12}\right)^{2t}\|\mathbf{v }_{0}\|^{2}\sin^{2}\phi_{0}+2\eta\left|\sum_{s=0}^{t-1}\left(1-\frac{\eta \tilde{\gamma}_{a}}{12}\right)^{2(t-1-s)}M_{1,s}\right|\] \[\quad+\sum_{s=0}^{t-1}\left(1-\frac{\eta\tilde{\gamma}_{a}}{12} \right)^{2(t-1-s)}\left(\eta^{2}M_{2,t-1}+\|\mathbf{v}_{t}\|^{2}\sin^{2}\phi_{t}- \|\tilde{\mathbf{v}}_{t}\|^{2}\sin^{2}\tilde{\phi}_{t}\right).\] (E.4)

Recall the choices

\[\eta=\frac{1}{ck^{2}\|\mathbf{a}^{*}\|^{2}}\frac{N\epsilon}{d\log(dN/\epsilon \delta)},\quad T_{v}=\frac{1}{\eta}\log\left(\frac{1}{\epsilon}\right)\cdot \frac{ck^{2}}{(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}}\frac{1}{\|\mathbf{a}^{*}\|^{2} \wedge 1}.\] (E.5)

According to (D.35) and (D.26), with probability at least \(1-\delta\), we have

\[\sum_{s=0}^{t-1} \left(1-\frac{\eta\tilde{\gamma}_{a}}{12}\right)^{2(t-1-s)}\big{\|} \|\mathbf{v}_{t}\|^{2}\sin^{2}\phi_{s}-\|\tilde{\mathbf{v}}_{s}\|^{2}\sin^{2}\tilde{ \phi}_{s}\big{\|}\] \[\leq\sum_{s=0}^{t-1}\left(1-\frac{\eta\tilde{\gamma}_{a}}{12} \right)^{2(t-1-s)}\left(6\eta\sin\phi_{t}\|\mathbf{h}_{s}\|+\eta^{2}\|\mathbf{h}_{s}\| ^{2}\right)\] \[\leq\frac{c\eta^{2}(\sqrt{Id}+I)\sqrt{\log(Nd/\eta\delta)}K_{a}^{ 4}+c\eta^{4}(Id+I^{2})\log(Nd/\eta\delta)K_{a}^{8}}{1-\left(1-\frac{\eta\tilde {\gamma}_{a}}{12}\right)^{2}}\] \[\leq\frac{24}{\tilde{\gamma}_{a}}\left\{c\eta(\sqrt{Id}+I)\log^{ 1/2}(Nd/\eta\delta)K_{a}^{4}+c\eta^{3}(Id+I^{2})\log(Nd/\eta\delta)K_{a}^{8}\right\}\] \[\leq N\epsilon\left(\sqrt{\frac{I}{d}}+\frac{I}{d}\right)+\frac{N ^{3}\epsilon^{3}(I^{2}+Id)}{d^{3}k^{4}},\] (E.6)where we used \(\eta\tilde{\gamma}_{a}/12<1/2\). Using Lemma 11 and recalling the definition of \(M_{1,t}\), we have the following concentration

\[\eta\left|\sum_{s=0}^{t-1}\left(1-\frac{\eta\tilde{\gamma}_{a}}{12} \right)^{2(t-1-s)}M_{1,s}\right|\] \[\qquad=\eta\left|\sum_{s=0}^{t-1}\left(1-\frac{\eta\tilde{\gamma}_ {a}}{12}\right)^{2(t-1-s)}\left(\mathbf{v}_{s}-\eta\mathbf{P}_{s}\frac{1}{N}\sum_{ i=1}^{N}\nabla_{\mathbf{w}}L(\mathbf{w}_{s}^{i},\mathbf{a}_{s}^{i})\right)^{\top}\left( \mathbf{I}-\mathbf{w}^{*}(\mathbf{w}^{*})^{\top}\right)\mathbf{\xi}_{s}\right|\] \[\qquad\leq\frac{12cK_{a}^{2}\eta}{\sqrt{N}}\sqrt{\log(1/\delta) \sum_{s=0}^{t-1}\left(1-\frac{\eta\tilde{\gamma}_{a}}{12}\right)^{2(t-1-s)} \left\|\frac{1}{N}\sum_{i=1}^{N}\mathbf{v}_{s}^{i}-\eta\mathbf{P}_{s}\nabla_{\mathbf{ w}}L(\mathbf{w}_{s}^{i},\mathbf{a}_{s}^{i})\right\|^{2}}\] \[\qquad\leq\frac{cK_{a}^{2}\eta}{\sqrt{N}}\sqrt{\log(1/\delta) \sum_{s=0}^{t-1}\left(1-\frac{\eta\tilde{\gamma}_{a}}{12}\right)^{2(t-1-s)}(9 +\eta^{2}K_{a}^{2})}\] \[\qquad\leq\frac{2cK_{a}^{2}\eta}{\sqrt{N}}\sqrt{\frac{10\log(1/ \delta)}{1-\left(1-\frac{\eta\tilde{\gamma}_{a}}{12}\right)^{2}}}\leq 12cK_{a}^{2} \sqrt{\frac{\eta\log(1/\delta)}{N\tilde{\gamma}_{a}}}\leq\sqrt{\frac{\epsilon} {d}}\] (E.7)

holds with probability at least \(1-\delta\). Moreover, it holds that

\[M_{2,t}=\mathbf{\xi}_{t}^{\top}\left(\mathbf{I}-\mathbf{w}^{*}(\mathbf{w}^{*})^{\top} \right)\mathbf{\xi}_{t}\leq\|\mathbf{\xi}_{t}\|^{2}\leq\frac{cK_{a}^{4}d\log(d/\eta \delta)}{N},\]

which yields with probability at least \(1-\delta\) such that

\[\eta^{2}\sum_{s=0}^{t-1}\left(1-\frac{\eta\tilde{\gamma}_{a}}{12}\right)^{2(t -1-s)}M_{2,s}\leq\frac{24\eta}{\tilde{\gamma}_{a}}\cdot\frac{cK_{a}^{4}d\log(d /\eta\delta)}{N}\leq\epsilon.\] (E.8)

Plugging (E.6), (E.7) and (E.8) into (E.4), we can guarantee that with probability at least \(1-3\delta\), for any \(T_{v}\leq t\leq\widetilde{O}(\eta^{-2})\)

\[\sin^{2}\phi_{t} \leq\frac{1}{\|\mathbf{v}_{t}\|^{2}}\left\{3\epsilon^{2}+\sqrt{\frac {\epsilon}{d}}+\left[N\left(\sqrt{\frac{I}{d}}+\frac{I}{d}\right)+\frac{N^{3} \epsilon^{2}(I^{2}+Id)}{d^{3}k^{4}}+1\right]\epsilon\right\}\] \[\leq 4\max\left\{\sqrt{\frac{\epsilon}{d}}+\frac{(\sqrt{Id}+I)N \epsilon}{d},\epsilon\right\}\] \[=4\left(\sqrt{\frac{\epsilon}{d}}+\frac{(\sqrt{Id}+I)N\epsilon}{ d}\right),\] (E.9)

where we used \(I\lesssim\frac{d}{\epsilon N}\min\left\{1,\frac{k^{2}d^{1/2}}{N^{1/2}},\frac{k^{4}d}{N^{2}\epsilon}\right\}\) and \(\epsilon<d^{-1}\). Then we further have

\[\left\|\left(\mathbf{I}-\mathbf{w}^{*}(\mathbf{w}^{*})^{\top}\right)\left( \mathbf{v}_{t}-\eta\mathbf{P}_{t}\frac{1}{N}\sum_{i=1}^{N}\nabla_{\mathbf{w}}L(\mathbf{w} _{t}^{i},\mathbf{a}_{t}^{i})\right)\right\|^{2}\] \[\qquad\leq\mathbf{v}_{t}^{\top}\left(\mathbf{I}-\mathbf{w}^{*}(\mathbf{w}^{* })^{\top}\right)\mathbf{v}_{t}+\eta^{2}\frac{1}{N}\sum_{i=1}^{N}\left\|\mathbf{P}_ {t}\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\right\|^{2}\] \[\qquad\leq\|\mathbf{v}_{t}\|^{2}\sin^{2}\phi_{t}+4\eta^{2}\frac{1}{N }\sum_{i=1}^{N}\|\nabla_{\mathbf{w}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\|^{2}\] \[\qquad\leq 36\left(\sqrt{\frac{\epsilon}{d}}+\frac{(\sqrt{Id}+I)N \epsilon}{d}\right)+4\eta^{2}K_{a}^{4}\] \[\qquad\leq 49\left(\sqrt{\frac{\epsilon}{d}}+\frac{(\sqrt{Id}+I)N \epsilon}{d}\right).\]With this upper bound, we can improve the concentration (E.7) through

\[\eta\left|\sum_{s=0}^{t-1}\left(1-\frac{\eta\tilde{\gamma}_{a}}{12} \right)^{2(t-1-s)}M_{1,s}\right|\] \[\qquad\leq\frac{cK_{a}^{2}\eta}{\sqrt{N}}\sqrt{\log(1/\delta)\sum_ {s=0}^{t-1}\left(1-\frac{\eta\tilde{\gamma}_{a}}{12}\right)^{2(t-1-s)}\cdot 49 \left(\sqrt{\frac{\epsilon}{d}}+\frac{(\sqrt{Id}+I)N\epsilon}{d}\right)}\] \[\qquad\leq 4\sqrt{\frac{\epsilon}{d}}\left(\sqrt{\frac{\epsilon}{d}} +\frac{(\sqrt{Id}+I)N\epsilon}{d}\right)^{\frac{1}{2}}.\] (E.10)

Denote \(H=c\eta K_{a}^{4}(\sqrt{Id}+I)\sqrt{\log(Nd/\delta)}\). Using (E.9), we can further refine the bound (E.6) by

\[\left\|\mathbf{v}_{t}\|^{2}\sin^{2}\phi_{t}-\|\check{\mathbf{v}}_{t}\|^{2 }\sin^{2}\check{\phi}_{t}\right| \leq 6\eta\sin\phi_{t}\|\mathbf{h}_{t-1}\|+\eta^{2}\|\mathbf{h}_{t-1}\| ^{2}\] \[\leq 4\left(\sqrt{\frac{\epsilon}{d}}+\frac{(\sqrt{Id}+I)N \epsilon}{d}\right)^{\frac{1}{2}}\cdot 6\eta H+\eta^{2}H^{2}.\]

It follows that

\[\sum_{s=0}^{t-1}\left(1-\frac{\eta\tilde{\gamma}_{a}}{12}\right)^ {2(t-1-s)}\left|\|\mathbf{v}_{s}\|^{2}\sin^{2}\phi_{s}-\|\check{\mathbf{v}}_{s}\|^{2} \sin^{2}\check{\phi}_{s}\right|\] \[\qquad\leq 4\frac{(\sqrt{Id}+I)N\epsilon}{d}\left(\sqrt{\frac{ \epsilon}{d}}+\frac{(\sqrt{Id}+I)N\epsilon}{d}\right)^{\frac{1}{2}}+\epsilon.\] (E.11)

Combining (E.10), (E.11) and (E.8), we can show that for any \(2T_{v}\leq t\leq(\eta K_{a}^{2})^{-2}\)

\[\sin^{2}\phi_{t}\leq 4\max\left\{\left(\sqrt{\frac{\epsilon}{d}}+ \frac{(\sqrt{Id}+I)N\epsilon}{d}\right)^{1+\frac{1}{2}},\epsilon\right\}.\] (E.12)

Repeating (E.10)-(E.12) for \(K=\log(1/\epsilon)\) times, we can guarantee that for any \((K+1)T_{v}\leq t\leq\widetilde{O}(\eta^{-2})\)

\[\sin^{2}\phi_{t} \leq 4\max\left\{\left(\sqrt{\frac{\epsilon}{d}}+\frac{(\sqrt{Id} +I)N\epsilon}{d}\right)^{1+\frac{K}{2}},\epsilon\right\}\] \[=4\epsilon.\]

### Convergence of the Second Layer

**Lemma 5** **restated.** Under the choice for \(\eta\) and conditions for \(\epsilon\) in Theorem 3. Suppose \(\sin^{2}\phi_{t}\leq\epsilon\) holds for any \(0\leq t\leq\widetilde{O}(\eta^{-2})\). With probability at least \(1-\delta\), we can guarantee that \(\|\mathbf{a}_{t}-\mathbf{a}^{*}\|^{2}\lesssim\epsilon\) holds for any \(\widetilde{O}\left(\eta^{-1}\right)\leq t\leq\widetilde{O}(\eta^{-2})\).

Proof.: Let \(\mathbf{h}_{t}^{i}=\mathbf{v}_{t}^{i}-\mathbf{v}_{t}\). Similar to (D.35), we can verify that for any \(i\in[N]\)

\[\left\|\mathbf{v}_{t}^{i}\|^{2}\sin^{2}\phi_{t}^{i}-\|\mathbf{v}_{t}\|^{ 2}\sin^{2}\phi_{t}\right| \leq 2\eta\sin\phi_{t}\|\mathbf{v}_{t}\|\|\mathbf{h}_{t-1}^{i}\|+\eta^{2} \|\mathbf{h}_{t-1}^{i}\|^{2}\] \[\leq 2\eta\epsilon\cdot\sigma\eta K_{a}^{2}(\sqrt{Id}+I)\sqrt{ \log(Nd/\delta)}+c\eta^{4}(\sqrt{Id}+I)^{2}\log(Nd/\delta)\] \[\leq 4\eta\epsilon,\]where we used Lemma 14 and \(\sin^{2}\phi_{t}\leq\epsilon\). It further implies that

\[\sin^{2}\phi_{t}^{i} \leq\frac{\|\mathbf{v}_{t}\|^{2}\sin^{2}\phi_{t}+\big{\|}\|\mathbf{v}_{t}^{i }\|^{2}\sin^{2}\phi_{t}^{i}-\|\mathbf{v}_{t}\|^{2}\sin^{2}\phi_{t}\big{\|}}{\|\mathbf{v }_{t}^{i}\|^{2}}\] \[\leq\frac{9\epsilon+4\eta\epsilon}{\big{(}\|\mathbf{v}_{t}\|-\|\mathbf{v} _{t}-\mathbf{v}_{t}^{i}\|\big{)}^{2}}\] \[\leq 100\epsilon.\] (E.13)

Further, for any \(0\leq t\leq\widetilde{O}(\eta^{-2})\) and \(i\in[N]\) we have

\[\pi-g(\phi_{t}^{i})=g(0)-g(\phi_{t}^{i})\overset{(i)}{\leq}(\pi- \phi_{t}^{i})\sin\phi_{t}^{i}\cdot\phi_{t}^{i}\leq 5\pi\epsilon^{1/2}\cdot\phi_{t }^{i}\overset{(ii)}{\leq}5\pi\epsilon,\] (E.14)

where \((i)\) holds since \(g^{\prime}(x)=-(\pi-x)\sin x\); and \((ii)\) holds due to \(\arcsin^{\prime}(x)=\frac{1}{\sqrt{1-x^{2}}}\). Applying this bound, we can get

\[\eta\left\langle\frac{1}{N}\sum_{i=1}^{N}\nabla_{\mathbf{a}}L(\mathbf{w}_ {t}^{i},\mathbf{a}_{t}^{i}),\mathbf{a}_{t}-\mathbf{a}^{*}\right\rangle =\frac{\eta}{2\pi}(\mathbf{a}_{t}-\mathbf{a}^{*})^{\top}\left(\mathbf{1}\mathbf{1 }^{\top}+(\pi-1)\mathbf{I}\right)(\mathbf{a}_{t}-\mathbf{a}^{*})\] \[\qquad+\frac{\eta}{2\pi}\left(\pi-\frac{1}{N}\sum_{i=1}^{N}g(\phi _{t}^{i})\right)(\mathbf{a}_{t}-\mathbf{a}^{*})^{\top}\mathbf{a}^{*}\] \[\geq\frac{\eta(\pi-1)}{2\pi}\|\mathbf{a}_{t}-\mathbf{a}^{*}\|^{2}-3\eta \epsilon\|\mathbf{a}_{t}-\mathbf{a}^{*}\|\|\mathbf{a}^{*}\|\] \[\geq\frac{\eta(\pi-1)}{2\pi}\|\mathbf{a}_{t}-\mathbf{a}^{*}\|^{2}-6\eta \epsilon K_{a}^{2},\] (E.15)

where the last inequality holds due to Lemma 12 such that \(\|\mathbf{a}_{t}-\mathbf{a}^{*}\|\leq 6\|\mathbf{a}^{*}\|\). In addition, it holds that

\[\left\|\frac{1}{N}\sum_{i=1}^{N}\nabla_{\mathbf{a}}L(\mathbf{w}_{t}^{i}, \mathbf{a}_{t}^{i})\right\|^{2} =\frac{1}{4\pi^{2}}\left\|\left(\mathbf{1}\mathbf{1}^{\top}+(\pi-1) \mathbf{I}\right)(\mathbf{a}_{t}-\mathbf{a}^{*})+\left(\frac{1}{N}\sum_{i=1}^{N}g(\phi _{t}^{i})-\pi\right)\mathbf{a}^{*}\right\|^{2}\] \[\leq\frac{(\pi+k-1)^{2}}{2\pi^{2}}\|\mathbf{a}_{t}-\mathbf{a}^{*}\|^{2}+1 5\epsilon^{2}\|\mathbf{a}^{*}\|^{2},\] (E.16)

where we used the relation (E.14). Denote \(\tilde{\mathbf{a}}_{t}=\mathbf{a}_{t}-\mathbf{a}^{*}-\eta\frac{1}{N}\sum_{i=1}^{N}\nabla_ {\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\), then we have

\[\|\mathbf{a}_{t}-\mathbf{a}^{*}\|^{2} =\left\|\mathbf{a}_{t-1}-\mathbf{a}^{*}-\eta\frac{1}{N}\sum_{i=1}^{N} \nabla_{\mathbf{a}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i})-\eta\mathbf{\epsilon}_{t}\right\| ^{2}\] \[=\left\|\mathbf{a}_{t-1}-\mathbf{a}^{*}-\eta\frac{1}{N}\sum_{i=1}^{N} \nabla_{\mathbf{a}}L(\mathbf{w}_{t-1}^{i},\mathbf{a}_{t-1}^{i})\right\|^{2}-2\eta\left\langle \tilde{\mathbf{a}}_{t-1},\mathbf{\epsilon}_{t-1}\right\rangle+\eta^{2}\|\mathbf{\epsilon}_{ t-1}\|^{2}\] \[\leq\left(1-\frac{\eta(\pi-1)}{2\pi}+\frac{\eta^{2}(\pi+k-1)^{2} }{2\pi^{2}}\right)\|\mathbf{a}_{t-1}-\mathbf{a}^{*}\|^{2}\] \[\qquad-2\eta\left\langle\tilde{\mathbf{a}}_{t-1},\mathbf{\epsilon}_{t-1} \right\rangle+\eta^{2}\|\mathbf{\epsilon}_{t-1}\|^{2}+6\eta\epsilon^{2}K_{a}^{2}+ \frac{\eta^{2}\epsilon^{4}}{\pi}\|\mathbf{a}^{*}\|^{2}\] \[\leq\left(1-\frac{\eta(\pi-1)}{4\pi}\right)\|\mathbf{a}_{t-1}-\mathbf{a}^ {*}\|^{2}-2\eta\left\langle\tilde{\mathbf{a}}_{t-1},\mathbf{\epsilon}_{t-1}\right\rangle +\eta^{2}\|\mathbf{\epsilon}_{t-1}\|^{2}+6\eta\epsilon K_{a}^{2}+\frac{\eta^{2} \epsilon^{2}}{\pi}\|\mathbf{a}^{*}\|\] \[\leq\left(1-\frac{\eta(\pi-1)}{4\pi}\right)^{t}\|\mathbf{a}_{0}-\mathbf{a} ^{*}\|^{2}+2\eta\left|\sum_{s=0}^{t-1}\left(1-\frac{\eta(\pi-1)}{4\pi}\right)^{ s}\left\langle\tilde{\mathbf{a}}_{s},\mathbf{\epsilon}_{s}\right\rangle\right|\] \[\qquad+\eta^{2}\sum_{s=0}^{t-1}\left(1-\frac{\eta(\pi-1)}{4\pi} \right)^{t-s}\|\mathbf{\epsilon}_{s}\|^{2}\] \[\qquad+\sum_{s=0}^{t-1}\left(1-\frac{\eta(\pi-1)}{4\pi}\right)^{t-1 -s}\left(6\eta\epsilon K_{a}^{2}+15\eta^{2}\epsilon^{2}\|\mathbf{a}^{*}\|^{2} \right).\] (E.17)Using Lemma 12 again, we can verify that

\[\|\tilde{\mathbf{a}}_{t}\| \leq\|\mathbf{a}_{t}-\mathbf{a}^{*}\|+\eta\frac{1}{N}\sum_{i=1}^{N}\|\nabla_ {\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\|\] \[\leq 6K_{a}+\frac{\eta}{2\pi}\left(\|\left(\mathbf{1}\mathbf{1}^{\top}+( \pi-1)\mathbf{\mathrm{I}}\right)(\mathbf{a}_{t}-\mathbf{a}^{*})\|+(\pi-g(\phi_{t}^{i}))\| \mathbf{a}^{*}\|\right)\] \[\leq 6K_{a}+\frac{\eta(\pi+k-1)}{2\pi}\cdot 6\|\mathbf{a}^{*}\|+ \frac{\eta}{2}\|\mathbf{a}^{*}\|\] \[\leq 8K_{a}.\] (E.18)

Similar to (A.17) with \(\alpha_{s}=\left(1-\frac{\eta(\pi-1)}{4\pi}\right)^{t-s}\), we can guarantee

\[\eta\left|\sum_{s=0}^{t-1}\left(1-\frac{\eta(\pi-1)}{4\pi}\right) ^{t-s}\tilde{\mathbf{a}}_{s}^{\top}\mathbf{\epsilon}_{s}\right| \leq c\eta K_{a}^{2}\sqrt{\frac{\log(2/\delta)}{N}\sum_{s=0}^{t} \left(1-\frac{\eta(\pi-1)}{4\pi}\right)^{t-s}}+\frac{c\eta K_{a}^{2}\log(2/ \delta)}{N}\] \[\leq 4cK_{a}^{2}\sqrt{\frac{\eta\log(2/\delta)}{N}}\] (E.19)

with probability at least \(1-\delta\). Now take \(T_{a}=\frac{1}{\eta}\frac{8\pi}{\pi-1}\log\left(\frac{1}{\epsilon}\right)\). Plugging (E.19) into (E.17), together with the concentration inequality (A.19), we have for any \(T_{a}\leq t\leq\widetilde{O}(\eta^{-2})\)

\[\|\mathbf{a}_{t}-\mathbf{a}^{*}\|^{2} \leq\left(1-\frac{\eta(\pi-1)}{4\pi}\right)^{t}\|\mathbf{a}_{0}-\mathbf{a }^{*}\|^{2}+4cK_{a}^{2}\sqrt{\frac{\eta\log(2/\delta)}{N}}\] \[\qquad+\frac{2\pi c\eta k\log(t/\delta)}{N}K_{a}^{2}+12\pi\epsilon K _{a}^{2}+2\eta\epsilon^{2}K_{a}^{2}\] \[\leq\epsilon\|\mathbf{a}_{0}-\mathbf{a}^{*}\|^{2}+\sqrt{\epsilon}K_{a}^{2 }+\epsilon^{2}K_{a}^{2}+12\pi\epsilon K_{a}^{2}+2\eta\epsilon^{2}K_{a}^{2}\] \[\leq 2\sqrt{\epsilon}K_{a}^{2}.\] (E.20)

With this upper bound, we can refine (E.18) by

\[\|\tilde{\mathbf{a}}_{t}\|^{2}\leq 2\|\mathbf{a}_{t}-\mathbf{a}^{*}\|^{2}+2\eta^{2} \frac{1}{N}\sum_{i=1}^{N}\|\nabla_{\mathbf{a}}L(\mathbf{w}_{t}^{i},\mathbf{a}_{t}^{i})\|^{ 2}\leq 8\sqrt{\epsilon}K_{a}^{2}.\]

Consequently, the bound (E.19) can be improved to

\[\eta\left|\sum_{s=0}^{t-1}\left(1-\frac{\eta(\pi-1)}{4\pi}\right) ^{t-s}\tilde{\mathbf{a}}_{s}^{\top}\mathbf{\epsilon}_{s}\right|\leq 4cK_{a}^{2}\sqrt{ \frac{\eta\epsilon^{1/2}\log(2/\delta)}{N}}.\]

Then we can guarantee that for any \(2T_{a}\leq t\leq\widetilde{O}(\eta^{-2})\),

\[\|\mathbf{a}_{t}-\mathbf{a}^{*}\|^{2} \leq\left(1-\frac{\eta(\pi-1)}{4\pi}\right)^{t}\|\mathbf{a}_{0}-\mathbf{a} ^{*}\|^{2}+4cK_{a}^{2}\sqrt{\frac{\eta\epsilon\log(2/\delta)}{N}}\] \[\qquad+\frac{2\pi c\eta k\log(t/\delta)}{N}K_{a}^{2}+12\pi \epsilon K_{a}^{2}+2\eta\epsilon^{2}K_{a}^{2}\] \[\leq\epsilon\|\mathbf{a}_{0}-\mathbf{a}^{*}\|^{2}+\epsilon^{3/4}K_{a}^{2} +\epsilon^{2}K_{a}^{2}+12\pi\epsilon K_{a}^{2}+2\eta\epsilon^{2}K_{a}^{2}\] \[\lesssim\epsilon^{4/3}K_{a}^{2}.\] (E.21)

Repeating the refinement from (E.20) to (E.21) for at most \(\log(1/\epsilon)\) times, we can show finish the proof. 

### Conclusion

**Theorem 3 restated.** Suppose the initial point \((\mathbf{v}_{0},\mathbf{a}_{0})\) satisfies \(\mathbf{a}_{0}^{\top}\mathbf{a}^{*}\geq\gamma_{a}/32\) and \(\phi_{0}\leq\tilde{\phi}^{l}\). For any \(\epsilon>0\), we choose \(\eta=\frac{1}{ck^{2}\|\mathbf{a}^{*}\|^{2}}\frac{N\epsilon}{d\log(dN/c\delta)}\) for some absolute constant \(c>0\). If \(I\lesssim\frac{d}{cN}\min\left\{1,\frac{k^{2}d^{1/2}}{N^{1/2}},\frac{k^{4}d}{N ^{2}\epsilon}\right\}\) and \(\epsilon<\min\{N^{-1},d^{-1},dk^{-2}\}\), then \(\ell(\mathbf{v}_{T},\mathbf{a}_{T})=O(\epsilon K_{a}^{2})\) holds with probability at least \(1-\delta\) where \(T=\widetilde{O}\left(\frac{dk^{4}}{N\epsilon}\right)\).

Proof.: According to the dynamic of \(\mathbf{a}_{t}\), we know

\[\mathbf{1}^{\top}(\mathbf{a}_{t}-\mathbf{a}^{*}) =\left(1-\frac{\eta(\pi+k-1)}{2\pi}\right)\mathbf{1}^{\top}(\mathbf{a}_{t-1 }-\mathbf{a}^{*})+\frac{\eta}{2\pi}\left(\frac{1}{N}\sum_{i=1}^{N}g(\phi_{t-1}^{i}) -\pi\right)\mathbf{1}^{\top}\mathbf{a}^{*}+\eta\mathbf{1}^{\top}\mathbf{\epsilon}_{t-1}\] \[=\left(1-\frac{\eta(\pi+k-1)}{2\pi}\right)^{t}\mathbf{1}^{\top}(\mathbf{a }_{0}-\mathbf{a}^{*})+\eta\sum_{s=0}^{t-1}\left(1-\frac{\eta(\pi+k-1)}{2\pi}\right) ^{t-1-s}\mathbf{1}^{\top}\mathbf{\epsilon}_{s}\] \[\qquad+\frac{\eta}{2\pi}\sum_{s=0}^{t-1}\left(1-\frac{\eta(\pi+k- 1)}{2\pi}\right)^{t-1-s}\left(\frac{1}{N}\sum_{i=1}^{N}g(\phi_{s}^{i})-\pi \right)\mathbf{1}^{\top}\mathbf{a}^{*}.\] (E.22)

It follows from (E.14) that

\[\left|\frac{\eta}{2\pi}\sum_{s=0}^{t-1}\left(1-\frac{\eta(\pi+k-1)}{2\pi} \right)^{t-1-s}\left(\frac{1}{N}\sum_{i=1}^{N}g(\phi_{s}^{i})-\pi\right)\right| \leq\frac{5\pi\epsilon}{\pi+k-1}.\] (E.23)

Using Lemma 11(1), it holds that

\[\eta\left|\sum_{s=0}^{t-1}\left(1-\frac{\eta(\pi+k-1)}{2\pi}\right)^{t-1-s}\bm {1}^{\top}\mathbf{\epsilon}_{s}\right|\leq cK_{a}\sqrt{\frac{\eta\log(1/\delta)}{N}}.\] (E.24)

Plugging (E.23) and (E.24) into (E.22), and taking \(T_{1}=\frac{1}{\eta}\frac{2\pi}{\pi+k-1}\log\left(\frac{1}{\epsilon}\right)\), we can have

\[\left|\mathbf{1}^{\top}(\mathbf{a}_{t}-\mathbf{a}^{*})\right| \leq\epsilon\left|\mathbf{1}^{\top}(\mathbf{a}_{0}-\mathbf{a}^{*})\right|+ \sqrt{\epsilon}K_{a}+\frac{5\pi\epsilon}{\pi+k-1}|\mathbf{1}^{\top}\mathbf{a}^{*}|\] \[\leq\epsilon\sqrt{k}K_{a}+\sqrt{\epsilon}K_{a}+\frac{5\pi\epsilon }{\pi+k-1}|\mathbf{1}^{\top}\mathbf{a}^{*}|\] \[\leq 2\sqrt{\epsilon}K_{a},\] (E.25)

where we used \(|\mathbf{1}^{\top}\mathbf{a}_{0}|\leq|\mathbf{1}^{\top}\mathbf{a}^{*}|\) and \(\epsilon<1/k\). Invoking Lemma 6 and the normalized network in (2), we know

\[\ell(\mathbf{v},\mathbf{a})=L(\mathbf{w},\mathbf{a}) =\frac{1}{2}\Bigg{[}\frac{\pi-1}{2\pi}\|\mathbf{a}^{*}\|^{2}+\frac{ \pi-1}{2\pi}\|\mathbf{a}\|^{2}-\frac{g(\phi)-1}{\pi}\mathbf{a}^{\top}\mathbf{a}^{*}\] \[\qquad\qquad+\frac{1}{2\pi}(\mathbf{1}^{\top}\mathbf{a}^{*})^{2}+\frac{1} {2\pi}(\mathbf{1}^{\top}\mathbf{a})^{2}-\frac{1}{\pi}(\mathbf{1}^{\top}\mathbf{a})(\mathbf{1}^{ \top}\mathbf{a})\Bigg{]}\] \[=\frac{1}{2}\left[\frac{\pi-1}{2\pi}\|\mathbf{a}-\mathbf{a}^{*}\|^{2}+ \frac{\pi-g(\phi)}{\pi}\mathbf{a}^{\top}\mathbf{a}^{*}+\frac{1}{2\pi}|\mathbf{1}^{\top}( \mathbf{a}-\mathbf{a}^{*})|^{2}\right].\] (E.26)

Similar to (E.14), for any \(t\geq T_{v}\), we also have

\[\pi-g(\phi_{t})=g(0)-g(\phi_{t})\leq(\pi-\phi_{t})\sin\phi_{t}\cdot\phi_{t}\leq \pi\epsilon\cdot\phi_{t}\leq\pi\epsilon.\] (E.27)

Invoking Lemma 4 and 5 to (E.26), together with (E.27), we conclude that for \(T=\widetilde{O}\left(\frac{dk^{4}}{N\epsilon}\right)\),

\[\ell(\mathbf{v}_{T},\mathbf{a}_{T})\leq\frac{1}{2}\left(\frac{15}{2}\epsilon K_{a}+ \epsilon\|\mathbf{a}_{T}\|\|\mathbf{a}^{*}\|+4\epsilon K_{a}^{2}\right)\lesssim\epsilon \|\mathbf{a}^{*}\|^{2},\]

with probability at least \(1-\delta\).