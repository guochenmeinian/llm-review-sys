# _Perception Test_: A Diagnostic Benchmark for Multimodal Video Models

Viorica Patraucean\({}^{1}\)

DeepMind

Lucas Smaira

DeepMind

Ankush Gupta

DeepMind

Adria Recasens Continente

DeepMind

Larisa Markeeva

DeepMind

Dylan Banarse

DeepMind

Skanda Koppula

DeepMind

Joseph Heyward

DeepMind

Mateusz Malinowski

Yi Yang

DeepMind

Carl Doersch

DeepMind

Tatiana Matejovicova

DeepMind

Yury Sulsky

DeepMind

Antoine Miech

Alex Frechette

DeepMind

Hanna Klimczak

DeepMind

Raphael Koster

DeepMind

Junlin Zhang

DeepMind

Stephanie Winkler

DeepMind

Yusuf Aytar

DeepMind

Simon Osindero

DeepMind

Dima Damen

University of Bristol

Andrew Zisserman

University of Oxford, DeepMind

Joao Carreira\({}^{1}\)

DeepMind

###### Abstract

We propose a novel multimodal video benchmark - the _Perception Test_ - to evaluate the perception and reasoning skills of pre-trained multimodal models (e.g. Flamingo, SeViLA, or GPT-4). Compared to existing benchmarks that focus on _computational tasks_ (e.g. classification, detection or tracking), the _Perception Test_ focuses on _skills_ (Memory, Abstraction, Physics, Semantics) and _types of reasoning_ (descriptive, explanatory, predictive, counterfactual) across video, audio, and text modalities, to provide a comprehensive and efficient evaluation tool. The benchmark probes pre-trained models for their _transfer_ capabilities, in a zero-shot / few-shot or limited finetuning regime. For these purposes, the _Perception Test_ introduces 11.6k real-world videos, 23s average length, designed to show perceptually interesting situations, filmed by around 100 participants worldwide. The videos are densely annotated with six types of labels (multiple-choice and grounded video question-answers, object and point tracks, temporal action and sound segments), enabling both language and non-language evaluations. The fine-tuning and validation splits of the benchmark are publicly available (CC-BY license), in addition to a challenge server with a held-out test split. Human baseline results compared to state-of-the-art video QA models show a substantial gap in performance (91.4\(\%\) vs 46.2\(\%\)), suggesting that there is significant room for improvement in multimodal video understanding. Dataset, baselines code, and challenge server are available at [https://github.com/deepmind/perception_test](https://github.com/deepmind/perception_test)Introduction

Significant progress in multimodal models has been made recently due to large-scale training on multimodal data. Models like Flamingo [4], SeViLA [53], BeiT-3 [49], GPT-4 [43] show remarkable versatility, dealing with diverse data sources and tackling new tasks by observing only a handful of examples. This is a major departure from specialised models that are typical in computer vision, _e.g._ image or action classifiers [53, 20], object detectors [13], or object trackers [47], opening up the path towards general perception and reasoning models.

Benchmarking these models in a robust and efficient way is key to expanding their capabilities, by allowing researchers to rank model design and training choices, and identify areas for improvement. Many perception-related benchmarks exist, for example Imagenet for image classification [17], Kinetics for video action recognition [36], Audioset for audio event classification [24], TAO for object tracking [18], or VQA for image question-answering [23], to name only a few. While these benchmarks have led to amazing progress, they all target restricted aspects of perception, focusing on specific computational tasks: _e.g._ image benchmarks discard the temporal dimension, visual question-answering tends to focus on only high-level semantic scene understanding, and object tracking focuses on lower-level, texture-based cues. Gluing several datasets together [39, 43] to benchmark more general models (as is done in Flamingo, SeViLA, BeiT-3, or GPT-4) improves coverage, but results in an expensive evaluation procedure that still misses important general perception abilities, _e.g._ physics understanding or memory. Few existing benchmarks even define tasks over both audio and visual modalities [29], much less more complex combinations of modalities and tasks. Furthermore, most prior work provides large training sets and thus benchmark models for in-dataset capabilities.

In this work, we propose the _Perception Test_ - a benchmark formed of purposefully designed, filmed, and annotated real-world videos that aims to comprehensively assess the capabilities of multimodal perception models across different skill areas (Memory, Abstraction, Physics, Semantics), types of reasoning [54] (_descriptive_, _explanatory_, _predictive_, and _counterfactual_), and modalities (video, audio, text). Our benchmark draws inspiration from diagnostic synthetic datasets like CATER [23] or CLEVRER [54], behavioral tests like the Visual Turing Test [41, 23], experiments in developmental psychology [18, 6, 31], and motor-free perception screening tests used for children or adults [42, 22].

To avoid benchmark overfitting, we propose a generalisation-focused evaluation regime. We aim to benchmark any representation or model, pre-trained with any _external_ dataset or task, of any scale available. The _Perception Test_ itself contains a small training set that can optionally be used for fine-tuning task decoders or prompting the model, and the rest is used for evaluation (public validation and held out test sets). In this regime, we can more robustly assess the _transfer_ abilities of these models, such that improvement on the benchmark can more reliably predict generalisation to real-world operation.

The dataset contains 11.6K real-world videos, densely annotated with 190K object and 8.6K point tracks, 73.5K temporal action segments, 137K temporal sound segments, 38K multiple-choice video question-answer (mc-vQA) pairs and 6K grounded video question-answer (g-vQA) pairs, enabling both language and non-language evaluations, to ensure a thorough assessment; see Figure 1 and Table 3. Having multiple types of annotations per video is useful also for analysis and explainability purposes, as the correlations between successes and failures across tasks may uncover model biases.

We open-source the videos and annotations in the training and validation splits. An evaluation server is made available together with the videos from the held-out test split. Since currently there is no model that can tackle all the evaluation tasks in our benchmark, we provide baseline results for per-task models: object tracking, point tracking, temporal action localisation, temporal sound localisation, multiple-choice video question-answering, and grounded video question-answering. For the mc-vQA task, the performance is mapped across skill areas (memory, abstraction, physics, semantics), and types of reasoning (descriptive, explanatory, predictive, counterfactual) to obtain a comprehensive diagnostics report.

In the next section (section 2), we discuss related work in more detail, highlighting what sets the _Perception Test_ apart in the rich landscape of multimodal benchmarks. In sections 2 and 4, we describe the videos and annotations in the _Perception Test_, with details about the diversity of participants involved in filming the videos. In section 5 we introduce the computational tasks enabled by these annotations, together with evaluation metrics and baselines, including a human baseline. We conclude with a summary and directions of future work in section 6.

## 2 Related work

A large number of perception-related benchmarks exist in the literature, covering various computational tasks or modalities. We focus the discussion here on video benchmarks and highlight the differences between the _Perception Test_ and prior work, in terms of data collection process, covered modalities, and available annotations and tasks.

Figure 1: The _Perception Test_ contains 6 types of annotations (object & point tracks, action & sound segments, multiple-choice videoQA and grounded videoQA) and tasks spanning 4 skill areas (Memory, Abstraction, Physics, Semantics, and 4 types of reasoning (Descriptive, Explanatory, Predictive, Counterfactual). See the presentation video at [https://github.com/deepmind/perception_test](https://github.com/deepmind/perception_test) for more examples.

Existing real-world benchmarks rely on one of the following data sources: **(i)** Videos collected from the web or repositories like Youtube, _e.g._ Kinetics [36], ActivityNet [3], VGGSound [11], HVU [13], ActivityNet-QA [50], tGIFQA [33]; **(ii)** Videos collected on demand, filmed by volunteers doing arbitrary activities in indoor or outdoor scenes, _e.g._ EPIC-KITCHENS [15], Ego4D [39]; **(iii)** Videos collected on demand, filmed by crowd-sourced participants doing actions described in pre-defined scripts, mostly in indoor scenes, _e.g._ Charades [36], Something-Something v2 (SSv2) [36]. Invariably, all real-world benchmarks use crowd-sourced annotations to enable various computational tasks like action classification, object detection, or video captioning, to name only a few.

Annotating publicly available videos is useful for training. However, using this approach for general perception evaluation has multiple drawbacks. Large quantities of data would need to be amassed and carefully filtered and annotated to accumulate (statistically) sufficiently diverse samples showing perceptually interesting situations that require skills like memory, abstraction, physics, and semantics understanding. In addition, some types of data are simply not available, _e.g._ situations showing incorrect execution of simple tasks like tying shoe laces. As we aim to assess more diverse skills, we chose to design video scripts that show perceptually interesting and diverse situations and film these with crowd-sourced participants from different places in the world to ensure diversity of video content and appearance. Different from Charades where the scripts were designed by crowd-sourced workers, our scripts are designed by our research team, similar to Something-something (v2). However, we did not aim to obtain an exhaustive coverage of simple actions like in SSV2. Instead, we designed more complex scripts to probe for more advanced reasoning skills beyond action classification.

A few research works have highlighted the need for robust diagnostics benchmarks, _e.g._ CATER [23], CLEVRER [34], IntPhys [41], Physion [42]. Their authors developed synthetic datasets to evaluate in a more systematic way, across different levels of difficulty, models' abilities to reason about intuitive physics (object collisions, motion, object permanence). We share the same motivation of creating a diagnostic test, and we aim to cover aspects related to memory, abstraction, intuitive physics, and semantics, using real-world videos. To achieve this, in addition to designing the video scripts, our team also designed the questions for each script type for the high-level tasks (mc-vQA and g-vQA); the answers per video were provided by crowd-sourced annotators. Given that our videos are filmed in real world scenes using common household items, the distributions of objects, actions, and sounds in our benchmark have a significant overlap with standard computer vision datasets (_e.g._ 99.01% of the words in our benchmark also appear in VQAv2 [27]), hence the domain gap between the _Perception Test_ and existing large-scale training datasets should be minimal.

Table [1] summarises the characteristics of the _Perception Test_ compared to previous efforts. It can be observed that the _Perception Test_ has a better coverage of skill areas and higher density of annotations. Size-wise, it is comparable to Charades, but smaller than Ego4D or SSv2. We emphasise that the _Perception Test_ is not designed to be a large-scale training dataset. Instead, it is an evaluation benchmark, with limited fine-tuning or prompting data, meant to assess the transfer capabilities of models.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Dataset** & **Source** & **Skills** & **\# videos** & **Dens** & **L(s)** \\ \hline Charades & C,R & S & 10,000 & 14 & 30 \\ SSv2 & C,R & AS & 108,499 & 1 & 4 \\ Ego4D-v2 & R & MS & 205,534\({}^{\ddagger}\) & 9\({}^{\ast}\) & 492\({}^{\ddagger}\) \\ CLEVRER\({}^{\flat}\) & C,Y & P & 60,000 & N/A & 5 \\ _Perception Test_ & C,R & MAPS & 11,620 & 761 & 23 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Characteristics of different datasets compared to the _Perception Test_. Dataset sources: Scripted (C), Real (R) and Synthetic (Y). Skill areas: Memory (M), Abstraction (A), Physics (P), Semantics (S). Dens: Average number of annotations per video. L: Average video length in seconds. \({}^{\ddagger}\)number of annotated clips, \({}^{\ast}\)reported for hand-objects subset with the highest density of annotations, \({}^{\dagger}\)reported for ELM NLQ subset with highest average clip length. \({}^{\flat}\): Annotations are extracted directly from the simulator.

Videos in the _Perception Test_

Inspired by how human perception screening tests are carefully designed by experts in developmental psychology or medicine (_e.g._[12]), we designed video scripts and tasks to diagnose the perception skills of our models.

**Scripts design:** Our goal was not to obtain an exhaustive coverage of activities or types of scenes. Instead, we selected four areas - Memory, Abstraction, Physics, Semantics - within which several skills should be tested (see Table2 first column) through tasks that require different types of reasoning: descriptive, explanatory, predictive, or counterfactual [54]. The skills selection took into account blind spots of existing benchmarks, weaknesses of current models, and aspects that are important for real-world scene understanding.

We then created scripts describing simple situations or games that can be easily performed by any one person (non-professional actor) using the items available in a regular household, or items that can be easily crafted if not available (_e.g._ letters or geometric shapes crafted from paper or cardboard). Each script consists of a brief description of the scene, followed by a description of the actions to be performed, together with specification of the camera placement (static camera one viewpoint; static camera 2 viewpoints; static camera and moving camera). To enhance content diversity, each script had considerable room for variability in the number of objects to be included in the scene or types of actions to be performed, or order of actions.

We prioritised situations where we can test high-level concepts like memory through low-level tasks like object tracking and the other way around: low-level physics understanding probed through high-level tasks like question-answering. In addition, we included in each script elements that could make the situations more interesting and challenging. For example, in cooking scripts (_e.g._ making tea, making salad), we added _distractor actions_[40], i.e. actions not relevant for making tea and that have no impact on the outcome of the making tea sequence, like clapping hands, or hitting a kettle with a spoon; this allows probing for understanding of causal relations between actions. We also included _distractor objects_ in the scene description, i.e. objects that are not relevant for the current script, but which are relevant for other scripts, like tomatoes present on the table during the make tea activity [43]. For all the scripts, we also asked participants to include in the scene some _adversarial configurations of objects e.g._ a shoe on the table. This allows us to probe models for understanding of spatial relations of objects when the language biases are not valid. Finally, some of the script variations include _adversarial actions_[26], i.e. incorrectly executed actions. For example, when making the tea, all the steps are done normally, but one is incorrectly executed, like pouring water from an empty kettle. In this way, we can probe for understanding of task completion, in a more complex setup than the adversarial action classification used in SSv2 dataset [26].

Table2 and Figure2 show examples of situations included in the scripts to probe for different skills in the different areas and types of reasoning. Note that the videos associated with a script allow defining tasks and questions across multiple skill areas. All-in-all, we designed 37 scripts, each with 2-5 variations, to obtain a diverse dataset. Having multiple variations per script allows us to ask the exact same question with the same set of options, and the correct answer depends on the specific script variation - in this way, we can avoid language biases in questions that give away the answer [37]. Examples of videos included in the dataset can be found in the presentation video at [https://github.com/deepmind/perception_test](https://github.com/deepmind/perception_test)

**Video filming:** Ensuring diversity of participants and scenes depicted in the videos was a critical consideration when developing the benchmark. Using a crowdsourcing pool, we selected around 100 participants from different countries of different ethnicity and gender and aimed to have a diverse representation within each video script. We include in the appendix details about the self-reported demographics of participants. Each script variation was filmed by at least a dozen of different participants, using most often a mobile-phone camera, resulting in high-resolution audio-visual assets. For scripts to be filmed from two different viewpoints, the recording was most often done sequentially by repeating the script; a few participants recorded simultaneously using two filming devices. About \(15\%\) of the videos were filmed with a moving camera. Most of the videos were filmed indoors in the living room or kitchen, with a small number being filmed in the bathroom or outdoors (about \(1\%\)). Most of the activities are performed on a tabletop, but some are also performed on the floor or on a chair. To avoid privacy concerns, we instructed the participants to not record their faces or vices. This is not a limitation of the dataset since the focus in our scripts is on object interactions. The participants gave their consent for the data to be used, published, and stored for perpetuity.

**Splits:** The _Perception Test_ contains 11609 videos (with audio), 23s average length. It is divided into a small training split (2184 videos, \(\sim 20\%\) of the data) that can be used for fine-tuning or prompting, a validation split (5900 videos, \(\sim 50\%\) of the data), and a held-out test split (3525 videos, \(\sim 30\%\) of the data) available through the evaluation server. We optimised to obtain a good balance across all annotation types and camera motions across the 3 splits; see section 3 in the appendix.

## 4 Annotations in the _Perception Test_

We annotate these videos with six types of annotations to cover low-level and high-level aspects, both spatial and temporal, and enable language and non-language evaluations: object and point tracks, temporal action and sound segments, multiple-choice and grounded video question-answers. We include a summary of the number of annotations in Table 3 and visualisations in Figure 3.

\begin{table}
\begin{tabular}{l l} \hline \hline
**(Skill Area) Skill** & **Example of situations and questions or tasks** \\ \hline
**(M)**Visual discrimination & Objects are shown in front of the camera, with some shown more than once. **Task**: \\  & Detect which objects were shown multiple times. \\ \hline
**(M)** Change detection & The camera is filming a table, then looks away for a few seconds, then looks back at the table. Some changes may have occurred. **Task**: Explain what changed. \\ \hline
**(M)** Sequencing & Objects are put in a backpack. **Task**: List their order. \\ \hline
**(M)** Event recall & A person indicates a region on the table with the hand, then puts objects inside and outside the region. **Task**: List the objects put inside the region. \\ \hline \hline
**(A)** Object, action \& event counting & A person turns a lamp on and off. **Task**: Count the number of times the illumination changed in the scene. \\ \hline
**(A)** Feature matching & A person puts wooden letters on the table. **Task**: Which letters have the same colour? \\ \hline
**(A)** Pattern discovery & Geometric shapes are shown in a pattern. **Task**: What shape will be shown next? \\ \hline
**(A)** Pattern breaking & A person puts multiple cups all facing upwards and one facing downwards. **Task**: \\  & Indicate the object that breaks the pattern. \\ \hline \hline
**(P)** Object permanence & A person plays a cups-game with 3-4 cups by hiding a small object under one of the cups, then shuffles the cups. **Task**: Predict where is the hidden object after shuffling. \\ \hline
**(P)** Spatial relations \& containment & A person puts a bookmark in a book, then puts the same or another book in a backpack. **Task**: Where is the bookmark at the end? \\ \hline
**(P)** Object attributes & A person writes on a piece of paper. **Task**: Is the paper lined or plain? \\ \hline
**(P)** Motion \& occluded interactions & A person moves an occluder object in front of a small object, sometimes moving also the small (occluded) object. **Task**: Was the small object moved? \\ \hline
**(P)** Solidity \& collisions & A person launches objects against a blocker object, sometimes removing the blocker. **Task**: Does the object fall off the table? \\ \hline
**(P)** Conservation & A person pours an equal amount of water in 2 identical glasses, then pours all or part of the water from one glass in a taller or wider glass. **Task**: How much water is in the last glass? \\ \hline
**(P)** Stability & A person puts objects on top of each other in a stable or unstable configuration. **Task**: Predict if the configuration will be stable after placing the last object. \\ \hline \hline
**(S)** Distractor actions \& objects & A person makes tea, and does also some distractor actions unrelated to making tea, _e.g._ rotating a knife. **Task**: Identify the distractor action(s). \\ \hline
**(S)** Task completion \& adversarial actions & A person ties shoe laces, but sometimes pretends to tie, or ties the lace of one shoe to the lace of the other shoe. **Task**: Detect if the action is done correctly. \\ \hline
**(S)** Object \& part recognition & A person conceals a small object in one of their hands, then shuffles the hands. **Task**: Identify in which hand is the object held. \\ \hline
**(S)** Action \& sound recognition & All scripts. **Task**: Detect the actions and sounds in the video from a pre-defined list. \\ \hline
**(S)** Place recognition & All scripts. **Task**: Detect where is the action taking place. \\ \hline
**(S)** State recognition & A person uses an electric device. **Task**: Indicate if the device is on. \\ \hline
**(S)** General knowledge \& Language & Some objects are shown to the camera, some multiple times. **Task**: Given a list of arbitrary statements or word puzzles, some requiring general knowledge to solve, select the statement that contains a reference to the second distinct object shown. \\ \hline \hline \end{tabular}
\end{table}
Table 2: Examples of scripts probing for different skills in the four areas in the _Perception Test_: **(M)**:Memory, **(A)**:Abstraction, **(P)**:Physics, **(S)**:Semantics.

**Object tracks:** Object tracks represent the _root annotation_ of our benchmark. All the other annotations, except for multiple-choice vQA, are linked or grounded into object tracks. In the annotation process, we instructed annotators to focus on the objects that the person interacts with and the objects that are in the immediate vicinity of the area where the person is performing actions, which act as distractor objects. We annotated boxes at 1fps throughout the video, which gives a good trade-off between density of annotations and annotation cost. When the objects are occluded, the annotators marked an approximate position of the boxes. Some ambiguous classes still remain, like liquids being poured or objects being torn. The object names were defined from an open vocabulary. The annotators typically included object attributes as well (colour, material), resulting in a large number of unique names. A list of the most frequent words (object or attributes) is included in the appendix, Fig. A1(left), together with the distribution of object tracks into various categories, _e.g._ objects involved in actions or sounds correlated with camera motion (Table A1).

_Cups-game subset:_ We isolate the videos corresponding to the cups-game scripts, as they can be an interesting subset for probing object trackers' abilities to reason about motion, object permanence, or occluded interactions when different factors may influence the difficulty of the task, _e.g._ identical vs non-identical objects used in the game, transparent vs non-transparent objects, or number of objects used. This subset contains 598 videos, with 483 videos where the cups are identical, and 113 videos where the cups are transparent. Most of the videos have 3 cups (451 videos), 132 videos have 2 cups, and 34 videos have 4 cups. We also provide a visibility mask for each video showing when the hidden object is occluded.

**Point tracks:** Although object tracks based on bounding boxes allow probing some physical properties of objects, such as object permanence, solidity, and coarse motion, they do not fully describe articulated or non-rigid objects, thin objects that are not axis-aligned, or out-of-plane rotation. A better understanding of physical interactions arises if we can track how object _surfaces_ move and deform over time. To this end, we annotate point tracks on object surfaces following the protocol of TAP-Vid [12]. Annotators were instructed to select points spanning all the different parts of the objects labelled in the object tracking task. Points that are occluded are simply marked as occluded and not tracked. For translucent objects (_e.g._ glass cups), we only consider points to be 'visible' if they belong to the surface closest to the camera. The annotated points are dense in time (30fps). Table A2 in the appendix gives the distribution of points that are moving or static, as well as those on videos with moving cameras.

**Action segments with action-relevant objects:** To capture temporal understanding and enable grounding over time, we annotate the videos with temporal segments belonging to a fixed set of templated labels, _e.g. putting something into something_, similar to [26]. These are associated with action-relevant object tracks, i.e. objects involved in the action. The action boundaries are defined based on contact with action-relevant objects. For instance, when a person puts sugar in a tea, the _putting something into something_ action starts when the person picks up the spoon and ends when the person puts down the spoon. If, after putting the sugar, the person starts stirring with the same spoon,

\begin{table}
\begin{tabular}{l r r r r} \hline \hline
**Annotation type** & **\# classes** & **\# annot** & **\# videos** & **Rate (fps)** \\ \hline Objects tracks & 5101 & 189940 & 11609 & 1 \\ Point tracks & NA & 8647 & 145 & 30 \\ Action segments & 63 & 73503 & 11353 & 30 \\ Sound segments & 16 & 137128 & 11433 & 30 \\ mc-vQA & 132 & 38060 & 10361 & NA \\ g-vQA & 34 & 6086 & 3063 & 1 \\ \hline \hline
**Area** & \# videoQA & \multicolumn{1}{l}{**Reasoning**} & \multicolumn{1}{l}{\# videoQA} \\ \hline Memory & 7256 (36) & Descriptive & 31536 (106) \\ Abstraction & 12737 (58) & Explanatory & 4513 (14) \\ Physics & 23741 (80) & Predictive & 1278 (7) \\ Semantics & 24965 (82) & Counterfactual & 733 (5) \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Top**: Annotations in the _Perception Test_. Each object or point track contains frame-level annotations at a certain _frame rate_, _e.g._ each point is annotated on every frame, at 30 fps. Action and sound segments are annotated at the original video frame rate. # classes refers to the number of unique object names for object tracks and the number of unique questions for multiple-choice videoQA (mc-vQA) and grounded videoQA (g-vQA). **Bottom**: Number of videoQA pairs and (unique questions) per area and type of reasoning. Note that one question may be counted in multiple areas if it tests more than one skill. Each question is assigned a unique type of reasoning.

this defines a new segment as the type of action changed. The frequency of actions across the entire dataset is included in the appendix, Fig. A1 (right).

**Sound segments with sound-relevant objects:** Similarly to the action segment annotations but applied to the audio modality, we collect sound segment annotations grounded in object tracks. By watching the video and listening to the audio, the annotators define temporal sound segments and label them from a list of 16 audio segment labels. For each sound, the annotators also identify the object (or objects) involved in making the sound, or specify that these are out of the camera's field of view. For example, if an object is placed on the table making an audible sound, then both the object track and the table track are associated with the sound segment. The frequency of sounds across the entire dataset is included in the appendix, Fig. A2 (right).

**Question-answers for video-level reasoning:** Different from the existing VQA datasets, which rely on crowd-sourced questions and answers, our team designed the questions per script to cover different types of reasoning 34: descriptive, explanatory, predictive, counterfactual, and to cover aspects that are important for operating in the real world, _e.g._ understanding task completion, detecting changes, and so on. The answers for all the questions per video were provided by crowd-sourced participants. As we are interested in non-ambiguous evaluation, we favour the multiple-choice setup over the open-language answer setup. To define challenging negative options, we partly relied on human annotators, partly sampling from the correct answers of other videos in the same type of script. Table 3 bottom and Figure A3 in the appendix show the distribution of question-video pairs into perception skills, skill areas, and type of reasoning.

**Question-answers with answer-relevant objects:** As another way to connect high-level and low-level scene understanding capabilities, we define questions or tasks in language form, with answers given as object tracks. Similar to the multiple-choice question-answers above, our team defined the questions, and human raters selected the answers from the existing object tracks. The grounded questions are associated with skill areas and types of reasoning.

## 5 Computational tasks and baseline results in the _Perception Test_

**Computational tasks:** We defined six computational tasks based on the annotations available in the _Perception Test_. We summarise in Table 3 the task definitions (outputs, metrics) and the performance of top-performing baselines. It can be observed that the _Perception Test_ combines lower-level dense prediction tasks like object and point tracking, whose outputs are box and point trajectories, with higher-level tasks like video question answering. For all the tasks, the video and audio are available as inputs, together with a task specification where applicable, _e.g._ the coordinates of a box to track for object tracking, or a language question and options for multiple-choice videoQA. More details about the task definitions are included in the appendix. Note that many other computational tasks can be defined based on the available annotations, _e.g._ grounded temporal action/sound localisation.

**Baselines:** Ideally, a single model should be able to perform all the tasks in the _Perception Test_. Since such a model is not available in the literature, we include results obtained with per-task baselines on the validation split for all the six tasks in the _Perception Test_; see Table 3 for a summary of top-performing baselines and their average performance, and the appendix for more details. When selecting these baselines, we favoured strong-performing models that can be evaluated in a zero-shot or few-shot setting, as our focus is on generalisation. However, for action and sound localisation, such models do not exist in the literature, so fine-tuning on our set of classes was necessary. For the mc-vQA task, we also provide a human baseline and fine-tuned evaluation to further assess the difficulty of the dataset for humans and for SOTA video-language models, respectively.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Task** & **Output** & **Metric** & **Baseline** & **Score** \\ \hline Object tracking & box track & Avg. IoU & SiamFC [34] & 0.67 \\ Point tracking & point track & Avg. Jaccard & TAP-Net [33] & 0.40 \\ Temporal action localisation & list of action segments & mAP & ActionFormer [34] & 0.16 \\ Temporal sound localisation & list of sound segments & mAP & ActionFormer [34] & 0.15 \\ multiple-choice videoQA & answer (1 out of 3) & top-1 accuracy & SeViLA [35] & 0.46 \\ grounded videoQA & list of box tracks & HOTA [33] & MDETR [34]+Stark [35] & 0.10 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Computational tasks and top-performing baselines in the _Perception Test_: the model receives a video with audio, plus a task-specific input (_e.g._ the coordinates of a bounding box for the object tracking task), and produces a task-specific prediction, evaluated using dedicated metrics.

**Object tracking:** The overall performance of SiamFC [31] (UniTrack [30] implementation) on our benchmark confirms the findings from [21] that simple Siamese trackers are better when probed zero-shot than more complex recent trackers, _e.g._ Stark tracker [22] obtains 0.56 mean IoU on the _Perception Test_ vs 0.67 for SiamFC. Even for SiamFC, the tracking performance drops when the camera and/or the objects are moving. The results for the different categories of objects (involved in actions or in sounds, etc.) are included in Table 3.3 aggregated based on camera motion.

**Point tracking:** The performance of our baseline TAP-Net [19] is a bit lower on the _Perception Test_ compared to the performance reported by the authors on the Kinetics dataset [30] (0.466 vs 0.401); see detailed results in Table 4.4 We attribute this drop in performance mainly to the increased video length in our benchmark (23s in _Perception Test_ compared to 10s in Kinetics).

**Action localisation:** The confusion matrix for our fine-tuned baseline ActionFormer [57] (Fig. 4.4) shows that the model struggles mostly with rare action classes that are confused with more frequent ones, and it also confuses pretend actions with their non-pretend versions, _e.g. ironing something_ vs _pretending to iron something_. Using multimodal inputs does not increase the performance significantly, as summarised in Table 4.5 top. Overall, ActionFormer's performance on our benchmark is lower compared to other benchmarks (15.56 mAP on _Perception Test_ vs 22.7 mAP on EPIC-Kitchens [14]), most likely due to the presence of adversarial actions and our limited training set. We hope to see in the near future models that can handle open-vocabulary action classes (similarly to open-vocabulary object detection [33]), so that fine-tuning is no longer necessary.

**Sound localisation:** We adapted the same ActionFormer model [57] to perform the localisation task in the audio modality. The best performance is obtained when features from both video and audio modalities are used as input; see Table 4.5 bottom.

**Multiple-choice vQA:** We report results for two strong recent video language models: Flamingo [3] in zero-shot and few-shot setups, and SeViLA [33] in zero-shot and fine-tuned regimes. We also include a dummy frequency-based baseline and a human baseline. For the frequency baseline, given that each question-options pair is defined over multiple videos, we keep as answer the option that is most frequently the correct answer in the training set. One can also compute this baseline on a random subset of training examples for each question, see Table 4.6 to obtain a fairer dummy baseline for models using few-shot evaluation.

_Human baseline_. We ran a small study for the mc-vQA task with human participants. We used 126 questions from the dataset, with one video per question selected at random. We recruited 30 crowd-sourced participants (half male, half female, with advanced English skills), different from the raters annotating the videos. Each participant answered a subset of 42 questions, resulting in 10 answers per question. The performance per area and type of reasoning is detailed in Figure 4. The overall average accuracy was \(91.4\%\). The mistakes occurred in situations difficult to judge from the given viewpoint, _e.g._ if a configuration of objects would be stable (without seeing the end of the video), or in edge cases where humans overlooked details happening very early on in the video. It is worth noting that the participants did not require any training, which is similar to a zero-shot setup. The median time spent to answer 42 questions was 30 minutes.

It can be observed that both Flamingo and SeViLA are far from human performance when evaluated 0-shot or few-shot and cannot outperform the 8-shot dummy frequency baseline; see Figures 4.8, 4.5 and Table 4.6 On many skills in the Memory, Physics, and Abstraction areas, their performance is below the 8-shot frequency dummy baseline, and in a few cases, _e.g._ (Pigget) conservation task, collision, or counterfactual reasoning, they are even below the pure random baseline. For counterfactuals, our qualitative investigation shows that Flamingo tends to latch on the visible elements in the video, failing to imagine the alternate reality that counterfactual questions require; _e.g._ for videos where a person writes some letters on the paper, the (counterfactual) question posed is: _What would be the order of the written letters if the person had written them in reverse order?_. Flamingo often selects the written order as correct. Interestingly, the larger versions of the model (due to larger language branches) seem to fare worse overall, which might point to overfitting issues. However, we leave an in-depth analysis for future work. Fine-tuning SeViLA leads to better results compared to all-shot frequency baseline, but mainly in the Semantics area (Fig. 4.8). SeViLA's 0-shot/fine-tuned scores on _Perception Test_ (Fig. 4.8) are significantly lower than on NExT-QA benchmark [21]: 46.2 vs 63.6 for zero-shot, and 62.0 vs 73.8 on fine-tuned. We attribute this to the diversity of skills and the hard negative options included in the _Perception Test_.

**Grounded-vQA:** As no existing model can perform this task, we created a baseline by running MDETR [24] on the middle frame and then tracking the predictions using the Stark [52] object tracker. The performance is poor; see Table A7 and Figure A6. The failures are caused mainly by poor detection results - since the tasks are temporal in nature, extracting _seed_ boxes from the middle frame is not sufficient to solve the tasks, calling for models capable of dealing with both spatial and temporal dimensions.

## 6 Conclusion

We propose a diagnostic benchmark for multimodal models, to probe for memory, abstraction, physics, and semantic capabilities, across visual, audio, and text modalities, using real-world videos purposefully designed and filmed to show interesting perceptual situations. Solving the tasks requires different types of reasoning: descriptive, explanatory, predictive, and counterfactual. The videos are densely labeled with six types of annotations (objects and point tracks, action and sound segments, multiple-choice and grounded video question-answers). We are open-sourcing the videos and the annotations in the train and validation splits, together with per-task baseline results and evaluation code. A challenge server is available to evaluate models on the held-out test split. In principle, any model can be evaluated on our benchmark, either in a zero/few-shot setting or by fine-tuning on our limited train split. An ideal perception model would be able to perform all the tasks in our benchmark. Our results suggest that state-of-the-art zero-shot or few-shot video-language models are not able to outperform a dummy frequency-based baseline, whereas humans in the same setting are nearly perfect. We discuss limitations, and ethical and societal aspects in the appendix. We hope that our work will contribute to understanding models' limitations (through direct evaluation and interpretability analysis supported by the different types of annotations) and narrowing down areas of improvement to guide research towards general perception models.

Figure 3: Performance on the validation set across skills for the 0-shot and fine-tuned SeViLA compared to frequency dummy baseline. The black dashed line indicates the random baseline.

Figure 2: 0-shot human baseline compared to 8-shot Flamingo-3B, 0-shot and fine-tuned SeViLA, and random baseline on the validation set. In 0-shot and 8-shot regimes, both Flamingo and SeViLA are far from the 0-shot human baseline. SeViLA fine-tuning improves results to some extent, but the gap to human performance is still substantial.

## Acknowledgments

We are grateful to Luis Piloto, Kenneth Marino, Luyu Wang, Felix Hill, Martin Chadwick, Lucy Campbell-Gillingham, Bori Wu, Drew Jaegle, Pauline Luc, Marianne Monteiro, Anna Bulanova, Radu Isac, Muqthar Mohammad, Vijay Vibha Tumala, Mahesh Maddinala, Yiwen Luo, Alina Kuznetsova, Aida Nematzadeh, Lisa Anne Hendricks, Aishwarya Agrawal, Nando de Freitas, Matt Botvinick, Shane Legg, and Relja Arandjelovic for providing insightful input on this project.

## References

* Aguiar and Baillargeon [2002] A. Aguiar and R. Baillargeon. Developments in young infants' reasoning about occluded objects. _Cognitive Psychology_, 45:267-336, 2002.
* Alayrac et al. [2020] Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelovic, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. Self-supervised multimodal versatile networks. _Advances in Neural Information Processing Systems_, 33:25-37, 2020.
* Alayrac et al. [2022] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL [https://openreview.net/forum?id=EbMuimAbPbbs](https://openreview.net/forum?id=EbMuimAbPbbs)
* Alayrac et al. [2022] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022. URL [https://arxiv.org/abs/2204.14198](https://arxiv.org/abs/2204.14198)
* Alwassel et al. [2021] Humam Alwassel, Silvio Giancola, and Bernard Ghanem. TSP: Temporally-sensitive pretraining of video encoders for localization tasks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops_, pages 3173-3183, 2021.
* Baillargeon [1994] Renee Baillargeon. Physical reasoning in young infants: Seeking explanations for impossible events. _British Journal of Development Psychology_, 12:9-33, 1994.
* Bear et al. [2021] Daniel Bear, Elias Wang, Damian Mrowca, Felix J. Binder, Hsiao-Yu Tung, R. T. Pramod, Cameron Holdaway, Sirui Tao, Kevin A. Smith, Fan-Yun Sun, Fei-Fei Li, Nancy Kanwisher, Josh Tenenbaum, Dan Yamins, and Judith E. Fan. Physion: Evaluating physical prediction from vision in humans and machines. In Ioqaquin Vanschoren and Sai-Kit Yeung, editors, _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual_, 2021. URL [https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/d09bf41544aa336546c9077ebb5a3c3-Abstract-round1.html](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/d09bf41544aa336546c9077ebb5a3c3-Abstract-round1.html)
* Bertinetto et al. [2016] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS Torr. Fully-convolutional siamese networks for object tracking. _arXiv preprint arXiv:1606.09549_, 2016.
* Heilbron et al. [2015] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2015.
* Cehovin et al. [2016] Luka Cehovin, Ales Leonardis, and Matej Kristan. Visual object tracking performance measures revisited. _IEEE Transactions on Image Processing_, 25(3):1261-1274, 2016.
* Chen et al. [2020] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: A large-scale audio-visual dataset. In _International Conference on Acoustics, Speech, and Signal Processing (ICASSP)_, 2020.
* Cooke et al. [2005] Deirdre M Cooke, Kryss McKenna, Jennifer Fleming, and Ross Darnell. The reliability of the occupational therapy adult perceptual screening test (ot-apst). _British Journal of Occupational Therapy_, 68(11):509-517, 2005.
* Dai et al. [2021] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head: Unifying object detection heads with attentions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 7373-7382, June 2021.

* Damen et al. [2018] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The epic-kitchens dataset. In _European Conference on Computer Vision (ECCV)_, 2018.
* Damen et al. [2022] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision: Collection, pipeline and challenges for EPIC-KITCHENS-100. _International Journal of Computer Vision (IJCV)_, 130:33-55, 2022.
* Dave et al. [2020] Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia Schmid, and Deva Ramanan. Tao: A large-scale benchmark for tracking any object. _Lecture Notes in Computer Science_, pages 436-454, 2020. ISSN 1611-3349, doi: 10.1007/978-3-030-58558-7_26. URL [http://dx.doi.org/10.1007/978-3-030-58558-7_26](http://dx.doi.org/10.1007/978-3-030-58558-7_26)
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Diba et al. [2020] Ali Diba, Mohsen Fayyaz, Vivek Sharma, Manohar Paluri, Jurgen Gall, Rainer Stiefelhagen, and Luc Van Gool. Large scale holistic video understanding. In _European Conference on Computer Vision_, pages 593-610. Springer, 2020.
* Doersch et al. [2022] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens Continente, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. TAP-vid: A benchmark for tracking any point in a video. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022. URL [https://openreview.net/forum?id=Zmosb2KfzYd](https://openreview.net/forum?id=Zmosb2KfzYd)
* Dosovitskiy et al. [2021] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. _ICLR_, 2021.
* Fan et al. [2021] Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Mingzhen Huang, Juehuan Liu, Yong Xu, et al. Lasot: A high-quality large-scale single object tracking benchmark. _International Journal of Computer Vision_, 129(2):439-461, 2021.
* Frostig and Horne [1965] Marianne Frostig and David Horne. _The Frostig program for the development of visual perception: Teacher's guide_. Follett Publishing Company in collaboration with Curriculum Materials..., 1965.
* Geman et al. [2015] Donald Geman, Stuart Geman, Neil Hallonquist, and Laurent Younes. Visual Turing test for computer vision systems. _Proceedings of the National Academy of Science_, 112(12):3618-3623, March 2015. doi: 10.1073/pnas.1422953112.
* Gemmeke et al. [2017] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In _2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 776-780, 2017. doi: 10.1109/ICASSP.2017.7952261.
* Girdhar and Ramanan [2020] Rohit Girdhar and Deva Ramanan. CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning. In _ICLR_, 2020.
* Goyal et al. [2018] Raghav Goyal, Farzaneh Mahdisoltani, Guillaume Berger, Waseem Gharbieh, Ingo Bax, and Roland Memisevic. Evaluating visual "common sense" using fine-grained classification and captioning tasks, 2018. URL [https://openreview.net/forum?id=rkY9Z_kwf](https://openreview.net/forum?id=rkY9Z_kwf)
* Goyal et al. [2017] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6325-6334, Los Alamitos, CA, USA, jul 2017. IEEE Computer Society. doi: 10.1109/CVPR.2017.670. URL [https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.670](https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.670)
* Goyal et al. [2017] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2017.
* Grauman et al. [2018] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Q. Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh K. Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Z. Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, AkshayErapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James M. Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Yu Heng Khoo, Jachym Kollar, Satwik Kottur, Anurag Kumar, Federico Landini, Chiao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyama, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran K. Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David J. Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithabu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard A. Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3, 000 hours of egocentric video. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [30] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: A scalable dataset generator. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3749-3761, 2022.
* [31] Eileen Mavis Hetherington, Ross D. Parke, and Virginia Otis Locke. _Child psychology: A contemporary viewpoint, 5th ed._ McGraw-Hill, 1999.
* [32] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity benchmark for generic object tracking in the wild. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 43(5):1562-1577, 2019.
* [33] Yunseok Jang, Yale Song, Chris Dongjoo Kim, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Video Question Answering with Spatio-Temporal Reasoning. _IJCV_, 2019.
* [34] Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel Synnaeve, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. _arXiv preprint arXiv:2104.12763_, 2021.
* [35] Prannay Kaul, Weidi Xie, and Andrew Zisserman. Multi-modal classifiers for open-vocabulary object detection. In _ICML_, 2023.
* [36] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset, 2017.
* [37] Corentin Kervadec, Grigory Antipov, Moez Baccouche, and Christian Wolf. Roses are red, violets are blue... but should vqa expect them to? In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2776-2785, June 2021.
* [38] Zhenyang Li, Ran Tao, Efstratios Gavves, Cees G. M. Snoek, and Arnold W. M. Smeulders. Tracking by natural language specification. In _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 7350-7358, 2017. doi: 10.1109/CVPR.2017.777.
* [39] Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Yufan Chen, Peter Wu, Michelle A Lee, Yuke Zhu, et al. Multibench: Multiscale benchmarks for multimodal representation learning. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)_, 2021.
* [40] Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taixe, and Bastian Leibe. Hota: A higher order metric for evaluating multi-object tracking. _International Journal of Computer Vision_, pages 1-31, 2020.
* [41] Mateusz Malinowski and Mario Fritz. Towards a visual turing challenge. _arXiv preprint arXiv:1410.8027_, 2014.
* [42] Nancy A Martin and Morrison F Gardner. _Test of visual perceptual skills_. Academic Therapy Publications Novato, CA, 2006.
* [43] OpenAI. Gpt-4 technical report, 2023.
* [44] Ronan Riochet, Mario Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, Veronique Izard, and Emmanuel Dupoux. Intphys: A framework and benchmark for visual intuitive physics reasoning. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, PP, 03 2018. doi: 10.1109/TPAMI.2021.3083839.

* Schiappa et al. [2022] Madeline Chantry Schiappa, Shruti Vyas, Hamid Palangi, Yogesh S Rawat, and Vibhav Vineet. Robustness analysis of video-language models against visual and language perturbations. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022. URL [https://openreview.net/forum?id=A79jA54WeW9](https://openreview.net/forum?id=A79jA54WeW9)
* Sigurdsson et al. [2016] Gunnar A. Sigurdsson, Gul Varol, X. Wang, Ali Farhadi, Ivan Laptev, and Abhinav Kumar Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. _ArXiv_, abs/1604.01753, 2016.
* Sun et al. [2020] Peize Sun, Jinkun Cao, Yi Jiang, Rufeng Zhang, Enze Xie, Zehuan Yuan, Changhu Wang, and Ping Luo. Transtrack: Multiple-object tracking with transformer. _arXiv preprint arXiv: 2012.15460_, 2020.
* Tacchetti et al. [2019] Andrea Tacchetti, Leyla Isik, and Tomaso Poggio. Invariant Action Recognition Dataset, 2019. URL [https://doi.org/10.7910/DVN/DMTOPGJ](https://doi.org/10.7910/DVN/DMTOPGJ)
* Wang et al. [2022] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign language: Beit pretraining for all vision and vision-language tasks, 2022. URL [https://arxiv.org/abs/2208.10442](https://arxiv.org/abs/2208.10442)
* Wang et al. [2021] Zhongdao Wang, Hengshuang Zhao, Ya-Li Li, Shengjin Wang, Philip Torr, and Luca Bertinetto. Do different tracking tasks require different appearance models? _Advances in Neural Information Processing Systems_, 34:726-738, 2021.
* Xiao et al. [2021] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9777-9786, June 2021.
* Yan et al. [2021] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and Huchuan Lu. Learning spatio-temporal transformer for visual tracking. _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 10428-10437, 2021.
* Yan et al. [2022] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi Zhang, Chen Sun, and Cordelia Schmid. Multiview Transformers for Video Recognition. _arXiv e-prints_, art. arXiv:2201.04288, January 2022.
* Yi et al. [2020] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B. Tenenbaum. Clevrer: Collision events for video representation and reasoning. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/forum?id=HkxYzANYDB](https://openreview.net/forum?id=HkxYzANYDB)
* Yu et al. [2023] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. _arXiv preprint arXiv:2305.06988_, 2023.
* Yu et al. [2019] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for understanding complex web videos via question answering. In _AAAI_, pages 9127-9134, 2019.
* Zhang et al. [2022] Chenlin Zhang, Jianxin Wu, and Yin Li. Actionformer: Localizing moments of actions with transformers. In _European Conference on Computer Vision_, 2022.