# RS-Del: Edit Distance Robustness Certificates for Sequence Classifiers via Randomized Deletion

 Zhuoqun Huang

University of Melbourne

zhuoqun@unimelb.edu.au

&Neil G. Marchant

University of Melbourne

nmarchant@unimelb.edu.au

&Keane Lucas

Carnegie Mellon University

keanelucas@cmu.edu

Lujo Bauer

Carnegie Mellon University

lbauer@cmu.edu

&Olga Ohrimenko

University of Melbourne

oohrimenko@unimelb.edu.au

&Benjamin I. P. Rubinstein

University of Melbourne

brubinstein@unimelb.edu.au

###### Abstract

Randomized smoothing is a leading approach for constructing classifiers that are certifiably robust against adversarial examples. Existing work on randomized smoothing has focused on classifiers with continuous inputs, such as images, where \(\ell_{p}\)-norm bounded adversaries are commonly studied. However, there has been limited work for classifiers with discrete or variable-size inputs, such as for source code, which require different threat models and smoothing mechanisms. In this work, we adapt randomized smoothing for discrete sequence classifiers to provide certified robustness against edit distance-bounded adversaries. Our proposed smoothing mechanism _randomized deletion_ (RS-Del) applies random deletion edits, which are (perhaps surprisingly) sufficient to confer robustness against adversarial deletion, insertion and substitution edits. Our proof of certification deviates from the established Neyman-Pearson approach, which is intractable in our setting, and is instead organized around longest common subsequences. We present a case study on malware detection--a binary classification problem on byte sequences where classifier evasion is a well-established threat model. When applied to the popular MalConv malware detection model, our smoothing mechanism RS-Del achieves a certified accuracy of 91% at an edit distance radius of 128 bytes.

## 1 Introduction

Neural networks have achieved exceptional performance for classification tasks in unstructured domains, such as computer vision and natural language. However, they are susceptible to adversarial examples--inconspicuous perturbations that cause inputs to be misclassified [1, 2]. While a multitude of defenses have been proposed against adversarial examples, they have historically been broken by stronger attacks. For instance, six of nine defense papers accepted for presentation at ICLR2018 were defeated months before the conference took place [3]; another tranche of thirteen defenses were circumvented shortly after [4]. This arms race between attackers and defenders has led the community to investigate certified robustness, which aims to guarantee that a classifier's prediction cannot be changed under a specified set of adversarial perturbations [5, 6].

Most prior work on certified robustness has focused on classifiers with _continuous fixed-dimensional_ inputs, under the assumption of \(\ell_{p}\)-norm bounded perturbations. A variety of certification methods have been proposed for this setting, including deterministic methods which bound a network's output by convex relaxation [5; 7; 6; 8] or composing layerwise bounds [9; 10; 11; 12], and randomized smoothing which obtains high probability bounds for a base classifier smoothed under random noise [13; 14; 15]. Randomized smoothing has achieved state-of-the-art certified robustness against \(\ell_{2}\)-norm bounded perturbations on ImageNet, correctly classifying 71% of a test set under perturbations with \(\ell_{2}\)-norm up to \(127/255\) (half maximum pixel intensity) [16]. However, real-world attacks go beyond continuous modalities: a range of attacks have been demonstrated against models with discrete inputs, such as binary executables [17; 18; 19; 20], source code [21], and PDF files [22].

Unfortunately, there has been limited investigation of certified robustness for discrete modalities, where prior work has focused on perturbations with bounded Hamming (\(\ell_{0}\)) distance [23; 24; 25], in some cases under additional constraints [26; 27; 28; 29; 30]. While this work covers attackers that overwrite content, it does not cover attackers that insert/delete content, which is important for instance in the malware domain [17; 31]. One exception is Zhang et al.'s method for certifying robustness under string transformation rules that may include insertion and deletion, however their approach is only feasible for small rule sets and is limited to LSTMs [32]. Liu et al. [33] also study certification for a threat model that includes insertion/deletion in the context of point cloud classification.

In this paper we develop a comprehensive treatment of _edit distance certifications_ for sequence classifiers. We consider input sequences of varying length on finite domains. We cover threat models where an adversary can arbitrarily perturb sequences by substitutions, insertions, and deletions or any subset of these operations. Moreover, our framework encompasses adversaries that apply edits to blocks of tokens at a time. Such threat models are motivated in malware analysis, where attackers are likely to edit semantic objects in an executable (e.g., whole instructions), rather than at the byte-level. We introduce tunable decision thresholds, to adjust decision boundaries while still forming sound certifications. This permits trading-off misclassification rates and certification radii between classes, and is useful in settings where adversaries have a strong incentive to misclassify malicious instances as benign [34; 35].

We accomplish our certifications using randomized smoothing [13; 14], which we instantiate with a general-purpose deletion smoothing mechanism called RS-Del. Perhaps surprisingly, RS-Del does not need to sample all edit operations covered by our certifications. By smoothing using deletions only, we simplify our certification analysis and achieve the added benefit of improved efficiency by querying the base classifier with shorter sequences. RS-Del is compatible with arbitrary base classifiers, requiring only oracle query access such as via an inference API. To prove our robustness certificates, we have to deviate from the standard Neyman-Pearson approach to randomized smoothing, as it is intractable in our setting. Instead, we organize our proof around a representative longest common subsequence (LCS): the LCS serves as a reference point in the smoothed space between an input instance and a neighboring instance, allowing us to bound the confidence of the smoothed model between the two.

Finally, we present a comprehensive evaluation of RS-Del in the malware analysis setting1. We investigate tradeoffs between accuracy and the size of our certificates for varying levels of deletion, and observe that RS-Del can achieve a certified accuracy of 91% at an edit distance radius of 128 bytes using on the order of \(10^{3}\) model queries. By comparison, a brute force certification would require in excess of \(10^{308}\) model queries to certify the same radius. We also demonstrate asymmetric certificates (favouring the malicious class) and certificates covering edits at the machine instruction level by leveraging chunking and information from a disassembler. Finally, we assess the empirical robustness of RS-Del to several attacks from the literature, where we observe a reduction in attack success rates.

Footnote 1: Our implementation is available at [https://github.com/Dovermore/randomized-deletion](https://github.com/Dovermore/randomized-deletion).

## 2 Preliminaries

Sequence classificationLet \(\mathcal{X}=\Omega^{\star}\) represent the space of finite-length sequences (including the empty sequence) whose elements are drawn from a finite set \(\Omega\). For a sequence \(\mathbf{x}\in\mathcal{X}\), we denote its length by \(|\mathbf{x}|\) and the element at position \(i\) by \(x_{i}\) where \(i\) runs from \(1\) to \(|\mathbf{x}|\). We consider classifiers that map sequences in \(\mathcal{X}\) to \(K\) classes in \(\mathcal{Y}=\{0,\ldots,K-1\}\). For example, in our case study on malware detection, we take \(\mathcal{X}\) to be the space of byte sequences (binaries), and \(\Omega\) to be the set of bytes and \(\mathcal{Y}\) to be \(\{0,1\}\) where 0 and 1 denote benign and malicious binaries respectively.

Robustness certificationGiven a classifier \(f\colon\mathcal{X}\to\mathcal{Y}\), an input \(\mathbf{x}\in\mathcal{X}\) and a neighborhood \(\mathcal{N}(\mathbf{x})\subset\mathcal{X}\) around \(\mathbf{x}\), a _robustness certificate_ is a guarantee that the classifier's prediction is constant in the neighborhood, i.e.,

\[f(\mathbf{x})=f(\mathbf{x}^{\prime})\quad\forall\mathbf{x}^{\prime}\in\mathcal{N}(\mathbf{x}). \tag{1}\]

When the neighborhood corresponds to a ball of radius \(r\) centered on \(\mathbf{x}\), we make the dependence on \(r\) explicit by writing \(\mathcal{N}_{r}(\mathbf{x})\). We adopt the paradigm of _conservative_, _probabilistic_ certification, which is a natural fit for randomized smoothing [14]. Under this paradigm, a certifier may either assert that (1) holds with high probability, or decline to assert whether (1) holds or not.

Edit distance robustnessMost existing work on robustness certification focuses on classifiers that operate on _fixed-dimensional_ inputs in \(\mathbb{R}^{d}\), where the neighborhood of certification is an \(\ell_{p}\)-ball [13, 14, 5, 6]. However, \(\ell_{p}\) robustness is not well motivated for sequence classifiers: \(\ell_{p}\) neighborhoods are limited to constant-length sequences, and the norm is ill-defined for sequences with non-numeric elements. For example, an \(\ell_{p}\) neighborhood around a byte sequence like \(\mathbf{x}=(78,7\mathrm{A},2\mathrm{D},00)\) must include sequences additively perturbed by real-valued sequences like \(\mathbf{\delta}=(-0.09,0.07,0.01,0.1)\), which clearly results in a type mismatch. Even if one focuses on robustness for length-preserving sequence perturbations, the \(\ell_{p}\) neighborhood is a poor choice because it excludes sequences that are even slightly misaligned. Motivated by these shortcomings, we consider _edit distance_ robustness.

Edit distance is a natural measure for comparing sequences. Given a set of elementary edit operations (ops) \(O\), the edit distance \(\mathrm{dist}_{O}(\mathbf{x},\mathbf{x}^{\prime})\) is the minimum number of ops required to transform sequence \(\mathbf{x}\) into \(\mathbf{x}^{\prime}\). We consider three ops: delete a single element (del), insert a single element (ins), and substitute one element with another (sub). For generality, we allow \(O\) to be any combination of these ops. When \(O=\{\mathsf{del},\mathsf{ins},\mathsf{sub}\}\) the edit distance is known as Levenshtein distance [36]. A primary goal of this paper is to produce edit distance robustness certificates, where the neighborhood of certification is an edit distance ball \(\mathcal{N}_{r}(\mathbf{x})=\{\mathbf{x}^{\prime}\in\mathcal{X}:\mathrm{dist}_{O}(\bm {x}^{\prime},\mathbf{x})\leq r\}\).

Threat modelWe consider an adversary that has full knowledge of our base and smoothed models, source of randomness, certification scheme, and possesses unbounded computation. The attacker makes edits from \(O\) up to some budget, in order to misclassify a target \(\mathbf{x}\). In the context of our experimental case study on malware detection, edit distance is a reasonable proxy for the cost of running evasion attacks that iteratively apply localized functionality-preserving edits (e.g., [37, 38, 17, 19, 18]). Since the edit distance scales roughly linearly with the number of attack iterations, the adversary has an incentive to minimize edit distance for these attacks.

## 3 RS-Del: Randomized deletion smoothing

In this section, we propose a method for constructing sequence classifiers that are certifiably robust under bounded edit distance perturbations. Our method RS-Del extends randomized smoothing with a novel deletion mechanism and tunable decision thresholds. We review randomized smoothing in Section 3.1 and describe our deletion mechanism in Section 3.2, along with its practical aspects in Section 3.3. We summarize the certified robustness guarantees of our method in Table 1 and defer their derivation to Section 4.

### Randomized smoothing

Let \(f_{\mathrm{b}}:\mathcal{X}\to\mathcal{Y}\) be a _base_ classifier and \(\phi\colon\mathcal{X}\to\mathcal{D}(\mathcal{X})\) be a _smoothing mechanism_ that maps inputs to a distributions over (perturbed) inputs. Randomized smoothing composes \(f_{\mathrm{b}}\) and \(\phi\) to construct a new _smoothed_ classifier \(f\colon\mathcal{X}\to\mathcal{Y}\). For any input \(\mathbf{x}\in\mathcal{X}\), the smoothed classifier's prediction is

\[f(\mathbf{x}):=\operatorname*{arg\,max}_{y\in\mathcal{Y}}\left\{p_{y}(\mathbf{x};f_{ \mathrm{b}},\phi)-\eta_{y}\right\}, \tag{2}\]

where \(\mathbf{\eta}=\{\eta_{y}\}_{y\in\mathcal{Y}}\) is a set of real-valued tunable decision thresholds and

\[p_{y}(\mathbf{x};f_{\mathrm{b}},\phi)=\Pr_{\mathbf{z}\sim\phi(\mathbf{x})}\left[f_{ \mathrm{b}}(\mathbf{z})=y\right] \tag{3}\]is the probability that the base classifier \(f_{\mathrm{b}}\) predicts class \(y\) for a perturbed input drawn from \(\phi(\mathbf{x})\). We omit the dependence of \(p_{y}\) on \(f_{\mathrm{b}}\) and \(\phi\) where it is clear from context.

The viability of randomized smoothing as a method for achieving certified robustness is strongly dependent on the smoothing mechanism. Ideally, the mechanism should be chosen to yield a smoothed classifier with improved robustness under the chosen threat model, while minimizing any drop in accuracy compared to the base classifier. The mechanism should also be amenable to analysis, so that a tractable robustness certificate can be derived.

_Remark 1_.: Previous definitions of randomized smoothing (e.g., [13; 14]) do not incorporate decision thresholds, and effectively assume \(\mathbf{\eta}=0\). We introduce decision thresholds as a way to trade off error rates and robustness between classes. This is useful when there is asymmetry in misclassification costs across classes. For instance, in our case study on malware detection, robustness of benign examples is less important because adversaries have limited incentive to trigger misclassification of benign examples [34; 35; 40]. We note that the base classifier may also be equipped with decision thresholds, which provide another degree of freedom to trade off error rates between classes.

### Randomized deletion mechanism

We propose a smoothing mechanism that achieves certified edit distance robustness. Our smoothing mechanism perturbs a sequence by deleting elements at random, and is called _randomized deletion_, or RS-Del for short.

Consider a sequence \(\mathbf{x}\in\mathcal{X}\) whose elements are indexed by the set \([\mathbf{x}]=\{1,\ldots,|\mathbf{x}|\}\). We specify the distribution of \(\phi(\mathbf{x})\) for RS-Del in two stages. In the first stage, a random edit \(\epsilon\) is drawn from a distribution \(G(\mathbf{x})\) over the space of possible edits to \(\mathbf{x}\), denoted \(\mathcal{E}(\mathbf{x})\). Since we only consider deletions for smoothing, any edit can be represented by the set of element indices in \([\mathbf{x}]\) that _remain_ after deletion. Hence \(\mathcal{E}(\mathbf{x})\) is taken as the powerset of \([\mathbf{x}]\). We specify edit distribution \(G(\mathbf{x})\) so that each element is deleted i.i.d. with probability \(p_{\mathsf{del}}\in(0,1)\):

\[\Pr[G(\mathbf{x})=\epsilon]=\prod_{i=1}^{|\mathbf{x}|}p_{\mathsf{del}}^{\mathbf{1}_{i \neq\epsilon}}(1-p_{\mathsf{del}})^{\mathbf{1}_{i\in\epsilon}}, \tag{4}\]

where \(\mathbf{1}_{A}\) denotes the indicator function, which returns 1 if \(A\) evaluates to true and 0 otherwise. In the second stage, the edit \(\epsilon\) is applied to \(\mathbf{x}\) to yield the perturbed sequence:

\[\mathbf{z}=\mathrm{apply}(\mathbf{x},\epsilon)\coloneqq\left(x_{\epsilon_{(i)}}\right) _{i=1\ldots|\epsilon|}, \tag{5}\]

where \(\epsilon_{(i)}\) denotes the \(i\)-th smallest index in \(\epsilon\). The perturbed sequence \(\mathbf{z}\) is guaranteed to be a subsequence of \(\mathbf{x}\). Putting both stages together, the distribution of \(\phi(\mathbf{x})\) is

\[\Pr[\phi(\mathbf{x})=\mathbf{z}]=\sum_{\epsilon\in\mathcal{E}(\mathbf{x})}\Pr[G(\mathbf{x})= \epsilon]\mathbf{1}_{\mathrm{apply}(\mathbf{x},\epsilon)=\mathbf{z}}. \tag{6}\]

_Remark 2_.: It may be surprising that we are proposing a smoothing mechanism for certified edit distance robustness that does not use the full set of edit ops \(O\) covered by the threat model. It is a misconception that randomized smoothing requires perfect alignment between the mechanism and the threat model. All that is needed from a robustness perspective, is for the mechanism to return distributions that are statistically close for any pair of inputs that are close in \(O\) edit distance; this can be achieved solely with deletion. In fact, perfect alignment is known to be suboptimal for some \(\ell_{p}\) threat models [15]. Our deletion mechanism leads to a tractable robustness certificate covering the full set of edit ops (see Section 4). Moreover while benefiting robustness, our empirical results show that our deletion mechanism has only a minor impact on accuracy (see Section 5). Finally, our deletion mechanism reduces the length of the input, which is beneficial for computational efficiency (see Appendix G). This is not true in general for mechanisms employing insertions/substitutions.

### Practical considerations

We now discuss considerations for implementing and certifying RS-Del in practice. In doing so, we reference theoretical results for certification, which are covered later in Section 4.

Probabilistic certificationRandomized smoothing does not generally support exact evaluation of the classifier's confidence \(\mu_{y}:=p_{y}(\mathbf{x})\), which is required for exact prediction and exact evaluation of the certificates we develop in Section 4. While \(\mu_{y}\) can be evaluated exactly for RS-Del by enumerating over the possible edits \(\mathcal{E}(\mathbf{x})\), the computation scales exponentially in \(|\mathbf{x}|\) (see Appendix A). Since this is infeasible for even moderately-sized \(\mathbf{x}\), we follow standard practice in randomized smoothing and estimate \(\mu_{y}\) with a lower confidence bound using Monte Carlo sampling [14]. This procedure is described in pseudocode in Figure 1: lines 1-3 estimate the predicted class \(y=f(\mathbf{x})\), lines 4-5 compute a \(1-\alpha\) lower confidence bound on \(\mu_{y}\), and line 6 uses this information and the results in Table 1 to compute a probabilistic certificate that holds with probability \(1-\alpha\). If the lower confidence bound on \(\mu_{y}\) exceeds the corresponding detection threshold \(\eta_{y}\), the prediction and certificate are returned (line 8), otherwise we _abstain_ due to lack of statistical significance (line 7).

TrainingWhile randomized smoothing is compatible with any base classifier, it generally performs poorly for conventionally-trained classifiers [13]. We therefore train base classifiers specifically to be used with RS-Del, by replacing original sequences with perturbed sequences (drawn from \(\phi\)) at training time. This has been shown to achieve good empirical performance in prior work [14].

Sequence chunkingSo far, we have described edit distance robustness and RS-Del assuming edits are applied at the level of sequence elements. However in some applications it may be reasonable to assume edits are applied at a coarser level, to contiguous chunks of sequence elements. For example, in malware analysis, one can leverage information from a disassembler to group low-level sequence elements (bytes) into more semantically meaningful chunks, such as machine instructions, addresses and header fields (see Appendix C.3). Our methods are compatible with chunking--we simply reinterpret the sequence as a sequence of chunks, rather than a sequence of lower-level elements. This can yield a tighter robustness certificate, since edits within chunks are excluded.

## 4 Edit distance robustness certificate

We now derive an edit distance robustness certificate for RS-Del. We present the derivation in three parts: Section 4.1 provides an outline, Section 4.2 derives a lower bound on RS-Del's confidence score and Section 4.3 uses the bound to complete the derivation. All proofs are presented in Appendix B.

### Derivation outline

Following prior work [14; 13; 41], we derive an edit distance robustness certificate that relies on limited information about RS-Del. We allow the certificate to depend on the input \(\mathbf{x}\), the smoothed prediction \(y=f(\mathbf{x})\), the confidence score \(\mu_{y}=p_{y}(\mathbf{x};f_{\mathrm{b}})\), the decision threshold \(\eta_{y}\), and the architecture of \(f\), including the deletion smoothing mechanism \(\phi\), but excluding the architecture of

\begin{table}
\begin{tabular}{l l l l} \hline \hline \multicolumn{4}{c}{Edit ops \(O\)} \\ \hline del & ins & sub & Certified radius \(r^{\star}\) \\ \hline  & ✓ & & \(\lfloor\log\frac{1-\mu_{y}}{1-\mu_{y}}/\log_{\mathrm{Pad}}\rfloor\) \\ ✓ & & & \(\lfloor\log\frac{\mu_{y}}{\mu_{y}}/\log_{\mathrm{Pad}}\rfloor\) \\ ✓ & ✓ & & \(\lfloor\log\frac{\mu_{y}}{\mu_{y}}/\log_{\mathrm{Pad}}\rfloor\) \\ ✓ & ✓ & ✓ & \(\lfloor\log(1+\mu_{y}-\mu_{y})/\log_{\mathrm{Pad}}\rfloor\) \\  & & ✓ & \(\lfloor\log(1+\mu_{y}-\mu_{y})/\log_{\mathrm{Pad}}\rfloor\) \\  & ✓ & ✓ & \(\lfloor\log(1+\mu_{y}-\mu_{y})/\log_{\mathrm{Pad}}\rfloor\) \\ ✓ & & ✓ & \(\lfloor\log(1+\mu_{y}-\mu_{y})/\log_{\mathrm{Pad}}\rfloor\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Edit distance robustness certificates for RS-Del as a function of the edit ops \(O\) used to define the edit distance. Here \(\mu_{y}\) is the confidence for predicted class \(y\) and \(\nu_{y}\) is a threshold derived from \(\mathbf{\eta}\) defined in (10).

Figure 1: Probabilistic certification of RS-Del. Here \(\mathbf{x}\) is the input sequence, \(f_{\mathrm{b}}\) is the base classifier, \(p_{\mathrm{del}}\) is the deletion probability, \(\mathbf{\eta}\) is the set of decision thresholds, \(\alpha\) is the significance level, and \(n_{\mathrm{pred}},n_{\mathrm{bnd}}\) are sample sizes. BinLCB\((k,n,\alpha)\) returns a lower confidence bound for \(p\) at level \(\alpha\) given \(k\sim\mathrm{Bin}(n,p)\).

the base classifier \(f_{\mathrm{b}}\). Limiting the dependence in this way improves tractability and ensures that the certificate is applicable for any choice of base classifier \(f_{\mathrm{b}}\). Formally, the only information we assume about \(f_{\mathrm{b}}\) is that it is some classifier in the feasible base classifier set:

\[\mathcal{F}(\mathbf{x},\mu_{y})=\{\,h\in\mathcal{X}\rightarrow\mathcal{Y}:\mu_{y}=p _{y}(\mathbf{x};h)\,\}\,. \tag{7}\]

Recall that an edit distance robustness certificate at radius \(r\) for a classifier \(f\) at input \(\mathbf{x}\) is a guarantee that \(f(\mathbf{x})=f(\bar{\mathbf{x}})\) for any perturbed input \(\bar{\mathbf{x}}\) in the neighborhood \(\mathcal{N}_{r}(\mathbf{x})=\{\bar{\mathbf{x}}\in\mathcal{X}:\mathrm{dist}_{O}(\bar{ \mathbf{x}},\mathbf{x})\leq r\}\). We observe that this guarantee holds for RS-Del in the limited information setting iff the \(\mathbf{\eta}\)-adjusted confidence for predicted class \(y\) exceeds the \(\mathbf{\eta}\)-adjusted confidence for any other class for all perturbed inputs \(\bar{\mathbf{x}}\) and feasible base classifiers \(h\):

\[p_{y}(\bar{\mathbf{x}};h)-\eta_{y}\geq\max_{y^{\prime}\neq y}\{p_{y^{\prime}}(\bar {\mathbf{x}};h)-\eta_{y^{\prime}}\}\quad\forall\bar{\mathbf{x}}\in\mathcal{N}_{r}(\mathbf{ x}),h\in\mathcal{F}(\mathbf{x},\mu_{y}). \tag{8}\]

To avoid dependence on the confidence of the runner-up class, which is inefficient for probabilistic certification, we work with the following more convenient condition.

**Proposition 3**.: _A sufficient condition for (8) is \(\rho(\mathbf{x},\mu_{y})\geq\nu_{y}(\mathbf{\eta})\) where_

\[\rho(\mathbf{x},\mu_{y})\coloneqq\min_{\bar{\mathbf{x}}\in\mathcal{N}_{r}(\mathbf{x})}\min _{h\in\mathcal{F}(\mathbf{x},\mu_{y})}p_{y}(\bar{\mathbf{x}};h) \tag{9}\]

_is a tight lower bound on the confidence for class \(y\), and we define the threshold_

\[\nu_{y}(\mathbf{\eta})=\begin{cases}\frac{1}{2}+\eta_{y}-\min_{y^{\prime}\neq y}\eta _{y^{\prime}},&\eta_{y}\geq\min_{y^{\prime}\neq y}\eta_{y^{\prime}}\text{ and }|\mathcal{Y}|>2,\\ 1+\eta_{y}-\min_{y^{\prime}\neq y}\eta_{y^{\prime}},&\eta_{y}<\min_{y^{\prime} \neq y}\eta_{y^{\prime}}\text{ and }|\mathcal{Y}|>2,\\ \frac{1+\eta_{y}-\min_{y^{\prime}\neq y}\eta_{y^{\prime}}}{2},&|\mathcal{Y}|=2.\end{cases} \tag{10}\]

The standard approach for evaluating \(\rho(\mathbf{x},\mu_{y})\) is via the Neyman-Pearson lemma [42; 14], however this seems insurmountable in our setting due to the challenging geometry of the edit distance neighborhood. We therefore proceed by deriving a loose lower bound on the confidence \(\tilde{\rho}(\mathbf{x},\mu_{y})\leq\rho(\mathbf{x},\mu_{y})\), noting that the robustness guarantee still holds so long as \(\tilde{\rho}(\mathbf{x},\mu_{y})>\nu_{y}(\mathbf{\eta})\). The derivation proceeds in two steps. In the first step, covered in Section 4.2, we derive a lower bound for the inner minimization in (9), which we denote by \(\tilde{\rho}(\bar{\mathbf{x}},\mathbf{x},\mu_{y})\). Then in the second step, covered in Section 4.3, we complete the derivation by minimizing \(\tilde{\rho}(\bar{\mathbf{x}},\mathbf{x},\mu_{y})\) over the edit distance neighborhood. Our results are summarized in Table 1, where we provide certificates under various constraints on the edit ops.

### Minimizing over feasible base classifiers

In this section, we derive a loose lower bound on the classifier confidence with respect to feasible base classifiers

\[\tilde{\rho}(\bar{\mathbf{x}},\mathbf{x},\mu_{y})\leq\rho(\bar{\mathbf{x}},\mathbf{x},\mu_{y}) =\min_{h\in\mathcal{F}(\mathbf{x},\mu_{y})}p_{y}(\bar{\mathbf{x}};h). \tag{11}\]

To begin, we write \(p_{y}(\bar{\mathbf{x}};h)\) as a sum over the edit space by combining (3) and (6):

\[p_{y}(\bar{\mathbf{x}};h)=\sum_{\bar{\epsilon}\in\mathcal{E}(\bar{\mathbf{x}})}s(\bar{ \epsilon},\bar{\mathbf{x}};h)\quad\text{where}\quad s(\bar{\epsilon},\bar{\mathbf{x}} ;h)=\Pr\left[G(\bar{\mathbf{x}})=\bar{\epsilon}\right]\mathbf{1}_{h(\mathrm{apply}(\bar {\mathbf{x}},\bar{\epsilon}))=y}. \tag{12}\]

We would like to rewrite this sum in terms of the known confidence score at \(\mathbf{x}\), \(\mu_{y}=p_{y}(\mathbf{x};h)=\sum_{\epsilon\in\mathcal{E}(\mathbf{x})}s(\epsilon,\mathbf{x};h)\). To do so, we identify pairs of edits \(\bar{\epsilon}\) to \(\bar{\mathbf{x}}\) and \(\epsilon\) to \(\mathbf{x}\) for which the corresponding terms \(s(\bar{\epsilon},\bar{\mathbf{x}};h)\) and \(s(\epsilon,\mathbf{x};h)\) are proportional.

**Lemma 4** (Equivalent edits).: _Let \(\mathbf{z}^{*}\) be a longest common subsequence (LCS) [43] of \(\bar{\mathbf{x}}\) and \(\mathbf{x}\), and let \(\bar{\epsilon}^{*}\in\mathcal{E}(\bar{\mathbf{x}})\) and \(\epsilon^{*}\in\mathcal{E}(\bar{\mathbf{x}})\) be any edits such that \(\mathrm{apply}(\bar{\mathbf{x}},\bar{\epsilon}^{*})=\mathrm{apply}(\mathbf{x},\epsilon ^{*})=\mathbf{z}^{*}\). Then there exists a bijection \(m:2^{\bar{\epsilon}^{*}}\to 2^{\epsilon^{*}}\) such that \(\mathrm{apply}(\bar{\mathbf{x}},\bar{\epsilon})=\mathrm{apply}(\mathbf{x},\epsilon)\) for any \(\bar{\epsilon}\subseteq\bar{\epsilon}^{*}\) and \(\epsilon=m(\bar{\epsilon})\). Furthermore, we have \(s(\bar{\epsilon},\bar{\mathbf{x}};h)=p^{|\bar{\mathbf{x}}||-|\mathbf{x}|}_{\text{del}}s( \epsilon,\mathbf{x};h)\)._

Applying this proportionality result to all pairs of edits \(\bar{\epsilon},\epsilon\) related under the bijection \(m\) yields:

\[\sum_{\bar{\epsilon}\in 2^{\bar{\epsilon}^{*}}}s(\bar{\epsilon},\bar{\mathbf{x}};h)=p^{| \bar{\mathbf{x}}||-|\mathbf{x}|}_{\text{del}}\sum_{\epsilon\in 2^{\bar{\epsilon}^{*}}}s( \epsilon,\mathbf{x};h).\]Thus we can achieve our goal of writing \(p_{y}(\bar{\mathbf{x}};h)\) in terms of \(\mu_{y}\). A rearrangement of terms gives:

\[p_{y}(\bar{\mathbf{x}};h)=p_{\mathsf{del}}^{|\bar{\mathbf{x}}|-|\mathbf{x}|}\left(\mu_{y}- \sum_{\epsilon\notin 2^{\epsilon*}}s(\epsilon,\mathbf{x};h)\right)+\sum_{\bar{ \epsilon}\notin 2^{\epsilon*}}s(\bar{\epsilon},\bar{\mathbf{x}};h). \tag{13}\]

This representation is convenient for deriving a lower bound. Specifically, we can drop the sum over \(\bar{\epsilon}\notin 2^{\epsilon*}\) and upper-bound the sum over \(\epsilon\notin 2^{\epsilon*}\) to obtain a lower bound that is independent of \(h\).

**Theorem 5**.: _For any pair of inputs \(\bar{\mathbf{x}}\), \(\mathbf{x}\in\mathcal{X}\) we have_

\[\rho(\bar{\mathbf{x}},\mathbf{x},\mu_{y})\geq\tilde{\rho}(\bar{\mathbf{x}},\mathbf{x},\mu_{y}) =p_{\mathsf{del}}^{|\bar{\mathbf{x}}|-|\mathbf{x}|}\left(\mu_{y}-1+p_{\mathsf{del}}^{ \frac{1}{2}(\operatorname{dist}_{\mathrm{LCS}}(\bar{\mathbf{x}},\mathbf{x})+|\mathbf{x}| -|\bar{\mathbf{x}}|)}\right). \tag{14}\]

_where \(\operatorname{dist}_{\mathrm{LCS}}(\bar{\mathbf{x}},\mathbf{x})\) is the longest common subsequence (LCS) distance2 between \(\bar{\mathbf{x}}\) and \(\mathbf{x}\)._

Footnote 2: The LCS distance is equivalent to the generalized edit distance with \(O=\{\mathsf{del},\mathsf{ins}\}\).

### Minimizing over the edit distance neighborhood

In this section, we complete the derivation of our robustness certificate by minimizing the lower bound in (14) over the edit distance neighborhood:

\[\tilde{\rho}(\mathbf{x};\mu_{y})=\min_{\bar{\mathbf{x}}\in\mathcal{N}_{r}(\mathbf{x})} \tilde{\rho}(\bar{\mathbf{x}},\mathbf{x},\mu_{y}). \tag{15}\]

We are interested in general edit distance neighborhoods, where the edit ops \(O\) used to define the edit distance may be constrained. For example, if the attacker is capable of performing elementary substitutions and insertions, but not deletions, then \(O=\{\mathsf{sub},\mathsf{ins}\}\). As a step towards solving (15), it is therefore useful to express \(\tilde{\rho}(\bar{\mathbf{x}},\mathbf{x},\mu_{y})\) in terms of edit op counts, as shown below.

**Corollary 6**.: _Suppose there exists a sequence of edits from \(\bar{\mathbf{x}}\) to \(\mathbf{x}\) that consists of \(n_{\mathsf{sub}}\) substitutions, \(n_{\mathsf{ins}}\) insertions and \(n_{\mathsf{del}}\) deletions s.t. \(n_{\mathsf{sub}}+n_{\mathsf{ins}}+n_{\mathsf{del}}=\operatorname{dist}_{O}( \bar{\mathbf{x}},\mathbf{x})\) and \(n_{\mathsf{sub}},n_{\mathsf{ins}},n_{\mathsf{del}}\geq 0\). Then_

\[\tilde{\rho}(\bar{\mathbf{x}},\mathbf{x},\mu_{y})=p_{\mathsf{del}}^{n_{\mathsf{add}}-n _{\mathsf{ins}}}\left(\mu_{y}-1+p_{\mathsf{del}}^{n_{\mathsf{sub}}+n_{\mathsf{ min}}}\right).\]

This parameterization of the lower bound enables us to re-express (15) as an optimization problem over edit ops counts:

\[\tilde{\rho}(\mathbf{x};\mu_{y})=\min_{n_{\mathsf{sub}},n_{\mathsf{in}},n_{ \mathsf{del}}\in\mathcal{C}_{r}}p_{\mathsf{del}}^{n_{\mathsf{add}}-n_{\mathsf{ ins}}}\left(\mu_{y}-1+p_{\mathsf{del}}^{n_{\mathsf{sub}}+n_{\mathsf{ins}}} \right), \tag{16}\]

where \(\mathcal{C}_{r}\) encodes constraints on the set of counts. If any number of insertions, deletions or substitutions are allowed, then the edit distance is known as the _Levenshtein distance_ and \(\mathcal{C}_{r}\) consists of sets of counts that sum to \(r\). We solve the minimization problem for this case below.

**Theorem 7** (Levenshtein distance certificate).: _A lower bound on the classifier's confidence within the Levenshtein distance neighborhood \(\mathcal{N}_{r}(\mathbf{x})\) (with \(O=\{\mathsf{del},\mathsf{ins},\mathsf{sub}\}\)) is \(\tilde{\rho}(\mathbf{x};\mu_{y})=\mu_{y}-1+p_{\mathsf{del}}^{r}\). It follows that the classifier is certifiably robust for any Levenshtein distance ball with radius \(r\) less than or equal to the certified radius \(r^{\star}=\lfloor\log(1+\nu_{y}(\mathbf{\eta})-\mu_{y})/\log p_{\mathsf{del}}\rfloor\)._

It is straightforward to adapt this result to account for constraints on the edit ops \(O\). Results for all combinations of edit ops are provided in Table 1.

So far in this section we have obtained results that depend on the classifier's confidence \(\mu_{y}\), assuming it can be evaluated exactly. However since exact evaluation of \(\mu_{y}\) is not feasible in general (see Section 3.3), we extend our results to the probabilistic setting, assuming a \(1-\alpha\) lower confidence bound on \(\mu_{y}\) is available. This covers the probabilistic certification procedure described in Figure 1.

**Corollary 8**.: _Suppose the procedure in Figure 1 returns predicted class \(\hat{y}\) with certified radius \(r^{\star}\). Then an edit distance robustness certificate of radius \(r\leq r^{\star}\) holds at \(\mathbf{x}\) with probability \(1-\alpha\)._

## 5 Case study: robust malware detection

We now present a case study on the application of RS-Del to malware detection. We report on certified accuracy for Levenshtein and Hamming distance threat models. We show that by tuning decision thresholds we can increase certified radii for the malicious class while maintaining accuracy. We also evaluate RS-Del on a range of published malware classifier attacks. Due to space constraints, we present the complete study in Appendices C-F, where we report on training curves, the computational cost of training and certification, certified radii normalized by file size, and results where byte edits are chunked by instructions.

BackgroundMalware (malicious software) detection is a long standing problem in security where machine learning is playing an increasing role [44, 45, 46, 47]. Inspired by the success of neural networks in other domains, recent work has sought to design neural network models for static malware detection which operate on raw binary executables, represented as byte sequences [48, 49, 50]. While these models have achieved competitive performance, they are vulnerable to adversarial perturbations that allow malware to evade detection [18, 19, 20, 37, 51, 31, 52, 17, 39]. Our edit distance threat model reflects these attacks in the malware domain where, even though a variety of perturbations with different semantic effects are possible, any perturbation can be represented in terms of elementary byte deletions, insertions and substitutions. For perspective on the threat model, consider YARA [53], a rule-based tool that is widely used for static malware analysis in industry. Running Nextron System's YARA rule set3 on a sample of binaries from the VTFeed dataset (introduced below), we find 83% of rule matches are triggered by fewer than 128 bytes. This implies most rules can be evaded by editing fewer than 128 bytes--a regime that is covered by our certificates in some instances (see Table 2). Further background and motivation for the threat model is provided in Appendix C, along with a reduction of static malware detection to sequence classification.

Footnote 3: [https://valhalla.nextron-systems.com/](https://valhalla.nextron-systems.com/)

Experimental setupWe use two Windows malware datasets: Sleipnir2 which is compiled from public sources following Al-Dujaili et al. [54] and VTFeed which is collected from VirusTotal [17]. We consider three malware detection models: a model smoothed with our randomized deletion mechanism (RS-Del), a model smoothed with the randomized ablation mechanism proposed by Levine and Feizi [24] (RS-Abn), and a non-smoothed baseline (NS). All of the models are based on a popular CNN architecture called MalConv [48], and are evaluated on a held-out test set. We emphasize that RS-Del is the only model that provides edit distance certificates (general \(O\)), while RS-Abn provides a Hamming distance certificate (\(O=\{\texttt{sub}\}\)). We review RS-Abn in Appendix I, where we describe modifications required for discrete sequence classification, and provide an analysis comparing the Hamming distance certificates of RS-Abn and RS-Del. Details about the datasets, models, training procedure, calibration, parameter settings and hardware are provided in Appendix D.

Accuracy/robustness tradeoffsOur first set of experiments investigate tradeoffs between malware detection accuracy and robustness guarantees as parameters associated with the smoothed models are varied. Table 2 reports clean accuracy, median certified radius (CR) and median certified radius normalized by file size (NCR) on the test set for RS-Del as a function of \(p_{\text{del}}\). A reasonable tradeoff is observed at \(p_{\text{del}}=99.5\%\) for Sleipnir2, where a median certified radius of 137 bytes is attained with only a 2-3% drop in clean accuracy from the NS baseline. The corresponding median NCR is 0.06% and varies in the range 0-9% across the test set. We also vary the decision threshold \(\mathbf{\eta}\) at \(p_{\text{del}}=99.5\%\) for Sleipnir2, and obtain asymmetrical robustness guarantees with a median certified radius up to 582 bytes for the malicious class, while maintaining the same accuracies (see Table 8 in Appendix E.1).

Since there are no baseline methods that support edit distance certificates, we compare with RS-Abn, which produces a limited Hamming distance certificate. Figure 2 plots the certified accuracy curves for RS-Del and RS-Abn for different values of the associated smoothing parameters \(p_{\text{del}}\) and \(p_{\text{ab}}\). The certified accuracy is the fraction of instances in the test set for which the model's prediction is correct _and_ certifiably robust at radius \(r\), and is therefore sensitive to both robustness and accuracy. We find that RS-Del outperforms RS-Abn in terms of certified accuracy at all radii \(r\) when \(p_{\text{del}}=p_{\text{ab}}\), while covering a larger set of inputs (since RS-Del's edit distance ball includes RS-Abn's Hamming ball). Further results and interpretation for these experiments are provided in Appendix E.

Empirical robustness to attacksOur edit distance certificates are conservative and may underestimate robustness to adversaries with additional constraints (e.g., maintaining executability, preserving a malicious payload, etc.). To provide a more complete picture of robustness, we subject RS-Del and NS to six recently published attacks [37, 38, 17, 31, 20] covering white-box and black-box settings. We adapt gradient-based white-box attacks [17, 20] for randomized smoothing in Appendix H. We do not constrain the number of edits each attack can make, which yields adversarial examples _well outside_ the edit distance we can certify for four of six attacks (see Table 9 of Appendix F). We measure robustness in terms of the _attack success rate_, defined as the fraction of instances in the evaluation set for which the attack flips the prediction from malicious to benign on at least one repeat (we repeat each attack five times). For both Sleipnir2 and VTFeed, we observe that RS-Del achievesthe lowest attack success rate (best robustness) for four of six attacks. In particular, we observe a 20 percentage point decrease in the success rate of _Disp_--an attack with no known defense [17]. We refer the reader to Appendix F for details of this experiment, including setup, results and discussion.

## 6 Related work

There is a rich body of research on certifications under \(\ell_{p}\)-norm-bounded threat models [5; 7; 6; 8; 9; 11; 13; 14; 12; 55; 25; 56]. While a useful abstraction for computer vision, such certifications are inadequate for many problems including perturbations to executable files considered in this work. Even in computer vision, \(\ell_{p}\)-norm bounded defenses can be circumvented by image translation, rotation, blur, and other human-imperceptible transformations that induce extremely large \(\ell_{p}\) distances. One solution is to re-parametrize the norm-bounded distance in terms of image transformation parameters [57; 58; 59]. NLP faces a different issue: while the \(\ell_{0}\) threat model covers adversarial word substitution [28; 60], it is too broad and covers many natural (non-adversarial) examples as well. For example, "He loves cat" and "He hates cat" are 1 word in \(\ell_{0}\) distance from "He likes cat", but are semantically different. A radius 1 certificate will force a wrong prediction for at least one neighbor. To address this, Jia et al. [26] and Ye et al. [27] constrain the threat model to synonyms only.

In this paper we go beyond the \(\ell_{0}\) word substitution threat model of previous work [23; 24; 25; 56], as consideration of insertions and deletions is necessary in domains such as malware analysis. Such edits are not captured by the \(\ell_{0}\) threat model: there is no fixed input size, and even when edits are size-preserving, a few edits may lead to large \(\ell_{0}\) distances. Arguably, our edit distance threat model for sequences and RS-Del mechanism are of independent interest to natural language also.

Certification has been studied for variations of edit distance defined for sets and graphs. Liu et al. [33] apply randomized smoothing using a subsampling mechanism to certify point cloud classifiers against edits that include insertion, deletion and substitution of points. Since a point cloud is an _unordered_ set, the edit distance takes a simpler form than for sequences--it can be expressed in terms of set cardinalities rather than as a cost minimization problem over edit paths. This simplifies the analysis, allowing Liu et al. to obtain a tight certificate via the Neyman-Pearson lemma, which is not feasible

\begin{table}
\begin{tabular}{l r r r r} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{\(p_{\text{del}}\)} & Clean & Median & Median \\  & & Accuracy & CR & NCR \% \\ \hline \multicolumn{5}{c}{Sleipnir2 dataset} \\ \hline NS & — & 98.9\% & — & — \\ \hline \multirow{5}{*}{RS-Del} & 90\% & 97.1\% & 6 & 0.0023 \\  & 95\% & 97.8\% & 13 & 0.0052 \\  & 97\% & 97.4\% & 22 & 0.0093 \\  & 99\% & 98.1\% & 68 & 0.0262 \\  & **99.5\%** & **96.5\%** & **137** & **0.0555** \\  & 99.9\% & 83.7\% & 688 & 0.2269 \\ \hline \multicolumn{5}{c}{VTFeed dataset} \\ \hline NS & — & 98.9\% & — & — \\ \hline RS-Del & 97\% & 92.1\% & 22 & 0.0045 \\  & 99\% & 86.9\% & 68 & 0.0122 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Clean accuracy and robustness metrics for RS-Del as a function of dataset and deletion probability \(p_{\text{del}}\). All metrics are computed on the test set. “Median CR” is the median certified Levenshtein distance radius in bytes and “median NCR %” is the median certified Levenshtein distance radius normalized as a percentage of the file size. A good tradeoff is achieved when \(p_{\text{del}}=99.5\%\) (in bold).

Figure 2: Certified accuracy for RS-Del and RS-Abn [24] as a function of certificate radius (horizontal axis) and strength of the smoothing parameter (line style). Results are plotted for the Sleipnir2 test set. While RS-Del provides a Levenshtein distance certificate (with \(O=\{\text{del},\text{ins},\text{sub}\}\)), RS-Abn provides a more limited Hamming distance certificate (\(O=\{\text{sub}\}\)). The non-smoothed, non-certified model (NS) achieves a clean accuracy of 98% in this setting.

for sequences. In parallel work, Schuchardt et al. [61] consider edit distance certification for graph classification, as an application of a broader certification framework for group equivariant tasks. They apply sparsity-aware smoothing [62] to an isomorphism equivariant base classifier, to yield a smoothed classifier that is certifiably robust under insertions/deletions of edges and node attributes.

Numerous empirical defense methods have been proposed to improve robustness of machine learning classifiers in security systems [63; 64; 29; 65; 22]. Incer Romeo et al. [34] and Chen et al. [65] develop classifiers that are verifiably robust if their manually crafted features conform to particular properties (e.g., monotonicity, stability). These approaches permit a combination of (potentially vulnerable) learned behavior with domain knowledge, and thereby aim to mitigate adversarial examples. Chen et al. [22] seek guarantees against subtree insertion and deletion for PDF malware classification. Using convex over-approximation [5; 66] previously applied to computer vision, they certify fixed-input dimension classifiers popular in PDF malware analysis. Concurrent to our work, Saha et al. [29] propose to certify classifiers against patch-based attacks by aggregating predictions of fixed-sized chunks of input binaries. The patch attack threat model, however, is not widely assumed in the evasion literature for malware detectors and can be readily broken by many published attacks [17; 20]. Moreover, their de-randomized smoothing design assumes a fixed-width input (via padding and/or trimming) and reduces patch-based attacks to gradient-based \(\ell_{p}\) attacks. While tight analysis exists for arbitrary randomized smoothing mechanisms [23], they are computationally infeasible with the edit distance threat model. Overall, we are the first to explore certified adversarial defenses that apply to sequence classifiers under the edit distance threat model.

## 7 Conclusion

In this paper, we study certified robustness of discrete sequence classifiers. We identify critical limitations of the \(\ell_{p}\)-norm bounded threat model in sequence classification and propose edit distance-based robustness, covering substitution, deletion and insertion perturbations. We then propose a novel deletion smoothing mechanism called RS-Del that is equipped with certified guarantees under several constraints on the edit operations. We present a case study of RS-Del and its certifications applied to malware analysis. We consider two malware datasets using a recent static deep malware detector, MalConv [48]. We find that RS-Del can certify radii as large as \(128\) bytes (in Levenshtein distance) without significant loss in detection accuracy. A certificate of this size covers in excess of \(10^{606}\) files in the proximity of a 10KB input file (see Appendix A). Results also demonstrate RS-Del improving robustness against published attacks well beyond the certified radius.

Broader impact and limitationsRobustness certification seeks to quantify the risk of adversarial examples while randomized smoothing both enables certification and acts to mitigate the impact of attacks. Randomized smoothing can degrade (benign) accuracy of undefended models as demonstrated in our results at higher smoothing levels. While we have strived to select high quality datasets for our case study, we note that accuracy-robustness tradeoffs may vary for different datasets and/or model architectures. Our approach is scalable relative to alternative certification strategies, however it does incur computational overheads. Finally, it is known that randomized smoothing can have disparate impacts on class-wise accuracy [67].

## Acknowledgments

This work was supported by the Department of Industry, Science, and Resources, Australia under AUSMURI CATCH, and the U.S. Army Research Office under MURI Grant W911NF2110317.

## References

* [1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In _International Conference on Learning Representations_, ICLR, 2014.
* [2] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial Examples. In _International Conference on Learning Representations_, ICLR, 2015.

* [3] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In _Proceedings of the 35th International Conference on Machine Learning_, ICML, 2018.
* [4] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial example defenses. In _Advances in Neural Information Processing Systems_, NeurIPS, 2020.
* [5] Eric Wong and Zico Kolter. Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope. In _International Conference on Machine Learning_, ICML, 2018.
* [6] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified Defenses against Adversarial Examples. In _International Conference on Learning Representations_, ICLR, 2018.
* [7] Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan O'Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training verified learners with learned verifiers. _arXiv preprint:1805.10265_, 2018.
* [8] Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, and Inderjit Dhillon. Towards fast computation of certified robustness for relu networks. In _International Conference on Machine Learning_, ICML, 2018.
* [9] Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably robust neural networks. In _International Conference on Machine Learning_, ICML, 2018.
* [10] Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks. In _Advances in Neural Information Processing Systems_, NeurIPS, 2018.
* [11] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for training verifiably robust models. _arXiv preprint:1810.12715_, 2018.
* [12] Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning, and Cho-Jui Hsieh. Towards Stable and Efficient Training of Verifiably Robust Neural Networks. In _International Conference on Learning Representations_, ICLR, 2020.
* [13] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified Robustness to Adversarial Examples with Differential Privacy. In _IEEE Symposium on Security and Privacy_, S&P, 2019.
* [14] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified Adversarial Robustness via Randomized Smoothing. In _International Conference on Machine Learning_, ICML, 2019.
* [15] Greg Yang, Tony Duan, J. Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized Smoothing of All Shapes and Sizes. In _International Conference on Machine Learning_, ICML, 2020.
* [16] Nicholas Carlini, Florian Tramer, Krishnamurthy Dj Dvijotham, Leslie Rice, Mingjie Sun, and J Zico Kolter. (certified!!) adversarial robustness for free! In _International Conference on Learning Representations_, ICLR, 2023.
* [17] Keane Lucas, Mahmood Sharif, Lujo Bauer, Michael K. Reiter, and Saurabh Shintre. Malware makeover: Breaking ml-based static analysis by modifying executable bytes. In _Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security_, AsiaCCS, 2021.
* [18] Bojan Kolosnjaji, Ambra Demontis, Battista Biggio, Davide Maiorca, Giorgio Giacinto, Claudia Eckert, and Fabio Roli. Adversarial malware binaries: Evading deep learning for malware detection in executables. In _26th European Signal Processing Conference_, EUSIPCO, 2018.
* [19] Daniel Park, Haidar Khan, and Bulent Yener. Generation & Evaluation of Adversarial Examples for Malware Obfuscation. In _International Conference On Machine Learning And Applications_, ICMLA, 2019.

* [20] Felix Kreuk, Assi Barak, Shir Aviv-Reuven, Moran Baruch, Benny Pinkas, and Joseph Keshet. Deceiving End-to-End Deep Learning Malware Detectors using Adversarial Examples. _arXiv preprint:1802.04528_, 2018.
* [21] Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu, and Zhi Jin. Generating adversarial examples for holding robustness of source code processing models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, AAAI, 2020.
* [22] Yizheng Chen, Shiqi Wang, Dongdong She, and Suman Jana. On training robust PDF malware classifiers. In _29th USENIX Security Symposium_, 2020.
* [23] Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi Jaakkola. Tight certificates of adversarial robustness for randomly smoothed classifiers. In _Advances in Neural Information Processing Systems_, NeurIPS, 2019.
* [24] Alexander Levine and Soheil Feizi. Robustness certificates for sparse adversarial attacks by randomized ablation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, AAAI, 2020.
* [25] Jinyuan Jia, Binghui Wang, Xiaoyu Cao, Hongbin Liu, and Neil Zhenqiang Gong. Almost tight l0-norm certified robustness of top-k predictions against adversarial perturbations. In _International Conference on Learning Representations_, ICLR, 2022.
* [26] Robin Jia, Aditi Raghunathan, Kerem Goksel, and Percy Liang. Certified Robustness to Adversarial Word Substitutions. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing_, EMNLP-IJCNLP, 2019.
* [27] Mao Ye, Chengyue Gong, and Qiang Liu. SAFER: A structure-free approach for certified robustness to adversarial word substitutions. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, ACL, 2020.
* [28] Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. Generating natural language adversarial examples through probability weighted word saliency. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, ACL, 2019.
* [29] Shoumik Saha, Wenxiao Wang, Yigitcan Kaya, and Soheil Feizi. Adversarial robustness of learning-based static malware classifiers. _arXiv preprint:2303.13372_, 2023.
* [30] Han Cheol Moon, Shafiq Joty, Ruochen Zhao, Megh Thakkar, and Chi Xu. Randomized smoothing with masked inference for adversarially robust text classifications. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics_, ACL, 2023.
* [31] Luca Demetrio, Battista Biggio, Giovanni Lagorio, Fabio Roli, and Alessandro Armando. Functionality-preserving black-box optimization of adversarial windows malware. _IEEE Transactions on Information Forensics and Security_, 16:3469-3478, 2021.
* [32] Yuhao Zhang, Aws Albarghouthi, and Loris D'Antoni. Certified robustness to programmable transformations in lstms. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, EMNLP, 2021.
* [33] Hongbin Liu, Jinyuan Jia, and Neil Zhenqiang Gong. PointGuard: Provably robust 3D point cloud classification. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, CVPR, pages 6186-6195, 2021.
* [34] Inigo Incer Romeo, Michael Theodorides, Sadia Afroz, and David Wagner. Adversarially Robust Malware Detection Using Monotonic Classification. In _Proceedings of the Fourth ACM International Workshop on Security and Privacy Analytics_, IWSPA, 2018.
* [35] Samuel Pfrommer, Brendon G. Anderson, Julien Piet, and Somayeh Sojoudi. Asymmetric certified robustness via feature-convex neural networks, 2023.
* [36] V. I. Levenshtein. Binary Codes Capable of Correcting Deletions, Insertions and Reversals. _Soviet Physics Doklady_, 10:707, February 1966.

* Demetrio et al. [2019] Luca Demetrio, Battista Biggio, Giovanni Lagorio, Fabio Roli, and Alessandro Armando. Explaining vulnerabilities of deep learning to adversarial malware binaries. In _Proceedings of the Third Italian Conference on Cyber Security_, ITASEC, 2019.
* Nisi et al. [2021] Dario Nisi, Mariano Graziano, Yanick Fratantonio, and Davide Balzarotti. Lost in the Loader: The Many Faces of the Windows PE File Format. In _Proceedings of the 24th International Symposium on Research in Attacks, Intrusions and Defenses_, RAID, 2021.
* Song et al. [2022] Wei Song, Xuezixiang Li, Sadia Afroz, Deepali Garg, Dmitry Kuznetsov, and Heng Yin. MAB-Malware: A Reinforcement Learning Framework for Blackbox Generation of Adversarial Malware. In _Proceedings of the 2022 ACM Asia Conference on Computer and Communications Security_, AsiaCCS, 2022.
* Fleshman et al. [2019] William Fleshman, Edward Raff, Jared Sylvester, Steven Forsyth, and Mark McLean. Non-negative networks against adversarial attacks, 2019.
* [41] Krishnamurthy (Dj) Dvijotham, Jamie Hayes, Borja Balle, Zico Kolter, Chongli Qin, Andras Gyorgy, Kai Xiao, Sven Gowal, and Pushmeet Kohli. A framework for robustness certification of smoothed classifiers using f-divergences. In _International Conference on Learning Representations_, ICLR, 2020.
* Neyman and Pearson [1933] Jerzy Neyman and Egon Sharpe Pearson. IX. on the problem of the most efficient tests of statistical hypotheses. _Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character_, 231(694-706):289-337, February 1933.
* Wagner and Fischer [1974] Robert A. Wagner and Michael J. Fischer. The string-to-string correction problem. _Journal of the ACM_, 21(1):168-173, January 1974. ISSN 0004-5411.
* Defender Security Research Team [2019] Microsoft Defender Security Research Team. New machine learning model sifts through the good to unearth the bad in evasive malware, July 2019. URL [https://www.microsoft.com/en-us/security/blog/2019/07/25/new-machine-learning-model-sifts-through-the-good-to-unearth-the-bad-in-evasive-malware/](https://www.microsoft.com/en-us/security/blog/2019/07/25/new-machine-learning-model-sifts-through-the-good-to-unearth-the-bad-in-evasive-malware/).
* Liu et al. [2020] Kaijun Liu, Shengwei Xu, Guoai Xu, Miao Zhang, Dawei Sun, and Haifeng Liu. A Review of Android Malware Detection Approaches Based on Machine Learning. _IEEE Access_, 8, 2020.
* Lab [2021] Kaspersky Lab. Machine Learning for Malware Detection, 2021. URL [https://media.kaspersky.com/en/enterprise-security/Kaspersky-Lab-Whitepaper-Machine-Learning.pdf](https://media.kaspersky.com/en/enterprise-security/Kaspersky-Lab-Whitepaper-Machine-Learning.pdf).
* Limited [2022] Blackberry Limited. Cylance AI from Blackberry, 2022. URL [https://www.blackberry.com/us/en/products/cylance-endpoint-security/cylance-ai](https://www.blackberry.com/us/en/products/cylance-endpoint-security/cylance-ai).
* Raff et al. [2018] Edward Raff, Jon Barker, Jared Sylvester, Robert Brandon, Bryan Catanzaro, and Charles K. Nicholas. Malware detection by eating a whole EXE. In _The Workshops of the Thirty-Second AAAI Conference on Artificial Intelligence_, AAAI Workshops, 2018.
* Krcal et al. [2018] Marek Krcal, Ondrej Svec, Martin Balek, and Otakar Jasek. Deep Convolutional Malware Classifiers Can Learn from Raw Executables and Labels Only. In _The Workshops of 6th International Conference on Learning Representations_, 2018.
* Raff et al. [2021] Edward Raff, William Fleshman, Richard Zak, Hyrum S. Anderson, Bobby Filar, and Mark McLean. Classifying Sequences of Extreme Length with Constant Memory Applied to Malware Detection. In _Proceedings of the AAAI Conference on Artificial Intelligence_, AAAI, 2021.
* Suciu et al. [2019] Octavian Suciu, Scott E. Coull, and Jeffrey Johns. Exploring Adversarial Examples in Malware Detection. In _IEEE Security and Privacy Workshops_, S&PW, 2019.
* Demetrio et al. [2021] Luca Demetrio, Scott E. Coull, Battista Biggio, Giovanni Lagorio, Alessandro Armando, and Fabio Roli. Adversarial exemples: A survey and experimental evaluation of practical attacks on machine learning for windows malware detection. _ACM Trans. Priv. Secur._, 24(4), September 2021. ISSN 2471-2566.

* [53] VirusTotal. Yara: The pattern matching swiss knife for malware researchers, 2022. URL [https://virustotal.github.io/yara/](https://virustotal.github.io/yara/). Accessed: 2023-10-12.
* [54] Abdullah Al-Dujaili, Alex Huang, Erik Hemberg, and Una-May O'Reilly. Adversarial Deep Learning for Robust Detection of Binary Encoded Malware. In _IEEE Security and Privacy Workshops_, S&PW, 2018.
* [55] Klas Leino, Zifan Wang, and Matt Fredrikson. Globally-robust neural networks. In _International Conference on Machine Learning_, ICML, 2021.
* [56] Zayd Hammoudeh and Daniel Lowd. Feature partition aggregation: A fast certified defense against a union of sparse adversarial attacks, 2023.
* [57] Marc Fischer, Maximilian Baader, and Martin Vechev. Certified Defense to Image Transformations via Randomized Smoothing. In _Advances in Neural Information Processing Systems_, NeurIPS, 2020.
* [58] Linyi Li, Maurice Weber, Xiaojun Xu, Luka Rimanic, Bhavya Kailkhura, Tao Xie, Ce Zhang, and Bo Li. Tss: Transformation-specific smoothing for robustness certification. In _ACM SIGSAC Conference on Computer and Communications Security_, CCS, 2021.
* [59] Zhongkai Hao, Chengyang Ying, Yinpeng Dong, Hang Su, Jian Song, and Jun Zhu. GSmooth: Certified robustness against semantic transformations via generalized randomized smoothing. In _International Conference on Machine Learning_, ICML, 2022.
* [60] Jiehang Zeng, Jianhan Xu, Xiaoqing Zheng, and Xuanjing Huang. Certified robustness to text adversarial attacks by randomized [mask]. _Computational Linguistics_, 49:395-427, 2023.
* [61] Jan Schuchardt, Yan Scholten, and Stephan Gunnemann. (Provable) adversarial robustness for group equivariant tasks: Graphs, point clouds, molecules, and more. In _Advances in Neural Information Processing Systems_, NeurIPS, 2023.
* [62] Aleksandar Bojchevski, Johannes Gasteiger, and Stephan Gunnemann. Efficient robustness certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more. In _International Conference on Machine Learning_, ICML, 2020.
* [63] Ambra Demontis, Marco Melis, Battista Biggio, Davide Maiorca, Daniel Arp, Konrad Rieck, Igino Corona, Giorgio Giacinto, and Fabio Roli. Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection. _IEEE Transactions on Dependable and Secure Computing_, 16(4):711-724, 2019.
* [64] Erwin Quiring, Lukas Pirch, Michael Reimsbach, Daniel Arp, and Konrad Rieck. Against All Odds: Winning the Defense Challenge in an Evasion Competition with Diversification. _arXiv preprint:2010.09569_, 2020.
* [65] Yizheng Chen, Shiqi Wang, Yue Qin, Xiaojing Liao, Suman Jana, and David Wagner. Learning security classifiers with verified global robustness properties. In _ACM SIGSAC Conference on Computer and Communications Security_, CCS, 2021.
* [66] Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial defenses. In _Advances in Neural Information Processing Systems_, NeurIPS, 2018.
* [67] Jeet Mohapatra, Ching-Yun Ko, Lily Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Hidden cost of randomized smoothing. In _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, AISTATS, 2021.
* [68] Panagiotis Charalampopoulos, Solon P. Pissis, Jakub Radoszewski, Tomasz Walen, and Wiktor Zuba. Unary Words Have the Smallest Levenshtein k-Neighbourhoods. In _31st Annual Symposium on Combinatorial Pattern Matching_, CPM, 2020.
* [69] Spark Tsao. Faster and More Accurate Malware Detection Through Predictive Machine Learning, November 2019. URL [https://www.trendmicro.com/vinfo/pl/security/news/security-technology/faster-and-more-accurate-malware-detection-through-predictive-machine-learning-correlating-static-and-behavioral-features](https://www.trendmicro.com/vinfo/pl/security/news/security-technology/faster-and-more-accurate-malware-detection-through-predictive-machine-learning-correlating-static-and-behavioral-features).

* [70] R. Vinayakumar, Mamoun Alazab, K. P. Soman, Prabaharan Poornachandran, and Sitalakshmi Venkatraman. Robust Intelligent Malware Detection Using Deep Learning. _IEEE Access_, 7, 2019.
* [71] Hojjat Aghakhani, Fabio Giriti, Francesco Mecca, Martina Lindorfer, Stefano Ortolani, Davide Balzarotti, Giovanni Vigna, and Christopher Kruegel. When Malware is Packin' Heat; Limits of Machine Learning Classifiers Based on Static Analysis Features. In _Proceedings of Symposium on Network and Distributed System Security_, NDSS, 2020.
* [72] Frederic Perriot. Defeating Polymorphism Through Code Optimization. In _Proceedings of the 2003 Virus Bulletin Conference_, VB, 2003.
* [73] Mihai Christodorescu, Johannes Kinder, Somesh Jha, Stefan Katzenbeisser, and Helmut Veith. Malware normalization. Technical Report TR1539, Department of Computer Sciences, University of Wisconsin-Madison, 2005.
* [74] Andrew Walenstein, Rachit Mathur, Mohamed R. Chouchane, and Arun Lakhotia. Normalizing Metamorphic Malware Using Term Rewriting. In _Sixth IEEE International Workshop on Source Code Analysis and Manipulation_, SCAM, 2006.
* [75] Danilo Bruschi, Lorenzo Martignoni, and Mattia Monga. Code Normalization for Self-Mutating Malware. _IEEE Security & Privacy_, 5(2):46-54, 2007.
* [76] Konrad Rieck, Thorsten Holz, Carsten Willems, Patrick Dussel, and Pavel Laskov. Learning and classification of malware behavior. In _Detection of Intrusions and Malware, and Vulnerability Assessment_, DIMVA, 2008.
* [77] Yanfang Ye, Lifei Chen, Dingding Wang, Tao Li, Qingshan Jiang, and Min Zhao. Sbmds: an interpretable string based malware detection system using svm ensemble with bagging. _Journal in Computer Virology_, 5(4):283, November 2008. ISSN 1772-9904.
* [78] Yong Qiao, Yuexiang Yang, Lin Ji, and Jie He. Analyzing malware by abstracting the frequent itemsets in API call sequences. In _2013 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications_, TrustCom, 2013.
* [79] Haodi Jiang, Turki Turki, and Jason T. L. Wang. DLGraph: Malware detection using deep learning and graph embedding. In _International Conference On Machine Learning And Applications_, 2018.
* [80] Zhaoqi Zhang, Panpan Qi, and Wei Wang. Dynamic malware analysis with feature engineering and feature learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, AAAI, 2020.
* [81] Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval Elovici. Generic black-box end-to-end attack against state of the art API call based malware classifiers. In _Proceedings of the 21st International Symposium on Research in Attacks, Intrusions, and Defenses_, RAID, 2018.
* [82] Weiwei Hu and Ying Tan. Black-box attacks against RNN based malware detection algorithms. In _The Workshops of the Thirty-Second AAAI Conference on Artificial Intelligence_, AAAI Workshops, 2018.
* [83] Ishai Rosenberg, Asaf Shabtai, Yuval Elovici, and Lior Rokach. Query-efficient black-box attack against sequence-based malware classifiers. In _Annual Computer Security Applications Conference_, ACSAC, 2020.
* [84] Fenil Fadadu, Anand Handa, Nitesh Kumar, and Sandeep Kumar Shukla. Evading API call sequence based malware classifiers. In _Proceedings of 21st International Conference on Information and Communications Security_, ICICS, 2020.
* [85] National Security Agency. Ghidra (version 10.1.5). URL [https://www.nsa.gov/ghidra](https://www.nsa.gov/ghidra).
* [86] Marco Barreno, Blaine Nelson, Russell Sears, Anthony D Joseph, and J Doug Tygar. Can machine learning be secure? In _Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security_, AsiaCCS, 2006.

* Corporation [2022] Microsoft Corporation. PE format, June 2022. URL [https://docs.microsoft.com/en-us/windows/win32/debug/pe-format](https://docs.microsoft.com/en-us/windows/win32/debug/pe-format).
* VirusShare [2001] VirusShare.com. VirusShare.com. URL [https://virusshare.com/](https://virusshare.com/).
* Schultz et al. [2001] Matthew G. Schultz, Eleazar Eskin, Erez Zadok, and Salvatore J. Stolfo. Data mining methods for detection of new malicious executables. In _IEEE Symposium on Security and Privacy_, S&P, 2001.
* Kolter and Maloof [2006] J. Zico Kolter and Marcus A. Maloof. Learning to Detect and Classify Malicious Executables in the Wild. _Journal of Machine Learning Research_, 7(99):2721-2744, 2006.
* Software [2006] Chocolatey Software. Chocolatey software. URL [https://chocolatey.org/](https://chocolatey.org/).
* Software [2018] Chocolatey Software. Chocolatey software docs | security. URL [https://docs.chocolatey.org/en-us/information/security](https://docs.chocolatey.org/en-us/information/security).
* Demetrio and Biggio [2021] Luca Demetrio and Battista Biggio. secml-malware: Pentesting windows malware classifiers with adversarial EXEmples in python. _arXiv preprint:2104.12848_, 2021.
* Salman et al. [2019] Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and Greg Yang. Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers. In _Advances in Neural Information Processing Systems_, NeurIPS, 2019.
* Lowd and Meek [2005] Daniel Lowd and Christopher Meek. Good word attacks on statistical spam filters. In _Second Conference on Email and Anti-Spam_, CEAS, 2005.
* Scholten et al. [2022] Yan Scholten, Jan Schuchardt, Simon Geisler, Aleksandar Bojchevski, and Stephan Gunnemann. Randomized message-interception smoothing: Gray-box certificates for graph neural networks. In _Advances in Neural Information Processing Systems_, NeurIPS, 2022.
* Jia et al. [2020] Jinyuan Jia, Binghui Wang, Xiaoyu Cao, and Neil Zhenqiang Gong. Certified robustness of community detection against adversarial structural perturbation via randomized smoothing. In _Proceedings of The Web Conference 2020_, WWW, pages 2718-2724, 2020. doi: 10.1145/3366423.3380029.

Brute-force edit distance certification

In this appendix, we show that an edit distance certification mechanism based on brute-force search is computationally infeasible. Suppose we are interested in issuing an edit distance certificate at radius \(r\) for a sequence classifier \(f\) at input \(\mathbf{x}\). Recall from (1) that in order to issue a certificate, we must show there exists no input \(\bar{\mathbf{x}}\) within the edit distance neighborhood \(\mathcal{N}_{r}(\mathbf{x})\) that would change \(f\)'s prediction. This problem can theoretically be tackled in a brute-force manner, by querying \(f\) for all inputs in \(\mathcal{N}_{r}(\mathbf{x})\). In the best case, this would take time linear in \(|\mathcal{N}_{r}(\mathbf{x})|\), assuming \(f\) responds to queries in constant time. However the following lower bound [68], shows that the size of the edit distance neighborhood is too large even in the best case:

\[|\mathcal{N}_{r}(\mathbf{x})|\geq\sum_{i=0}^{r}255^{i}\sum_{j=i-r}^{r}\binom{|\mathbf{ x}|+j}{i}\geq 255^{r}.\]

For example, applying the loosest bound that is independent of \(\mathbf{x}\), we see that brute-force certification at radius \(r=128\) would require in excess of \(255^{r}\approx 10^{308}\) queries to \(f\). In contrast, our probabilistic certification mechanism (Figure 1) makes \(n_{\mathrm{pred}}+n_{\mathrm{bnd}}\) queries to \(f\), and we can provide high probability guarantees when the number of queries is of order \(10^{3}\) or \(10^{4}\).

## Appendix B Proofs for Section 4

In this appendix, we provide proofs of the theoretical results stated in Section 4.

### Proof of Proposition 3

A sufficient condition for (8) is

\[\min_{\bar{\mathbf{x}}\in\mathcal{N}_{r}(\mathbf{x})}\min_{h\in\mathcal{F}(\mathbf{x})}p_ {y}(\bar{\mathbf{x}};h)\geq\max_{\bar{\mathbf{x}}\in\mathcal{N}_{r}(\mathbf{x})}\max_{h\in \mathcal{F}(\mathbf{x})}\left(\eta_{y}+\max_{y^{\prime}\neq y}p_{y^{\prime}}(\bar{ \mathbf{x}};h)-\min_{y^{\prime}\neq y}\eta_{y^{\prime}}\right). \tag{17}\]

We first consider the multi-class case where \(|\mathcal{Y}|>2\). If \(\eta_{y}\geq\min_{y^{\prime}\neq y}\eta_{y^{\prime}}\), then \(p_{y}(\bar{\mathbf{x}};h)\geq\max_{y^{\prime}\neq y}p_{y^{\prime}}(\bar{\mathbf{x}};h)\) by (17) and we can upper-bound \(\max_{y^{\prime}\neq y}p_{y^{\prime}}(\bar{\mathbf{x}};h)\) by \(\frac{1}{2}\). On the other hand, if \(\eta_{y}\geq\min_{y^{\prime}\neq y}\eta_{y^{\prime}}\), we can only upper-bound \(\max_{y^{\prime}\neq y}p_{y^{\prime}}(\bar{\mathbf{x}};h)\) by \(1\). Thus when \(|\mathcal{Y}|>2\) (17) implies

\[\min_{\bar{\mathbf{x}}\in\mathcal{N}_{r}(\mathbf{x})}\min_{h\in\mathcal{F}(\mathbf{x})}p_ {y}(\bar{\mathbf{x}};h)\geq\begin{cases}\frac{1}{2}+\eta_{y}-\min_{y^{\prime}\neq y }\eta_{y^{\prime}},&\eta_{y}\geq\min_{y^{\prime}\neq y}\eta_{y^{\prime}},\\ 1+\eta_{y}-\min_{y^{\prime}\neq y}\eta_{y^{\prime}},&\eta_{y}<\min_{y^{\prime} \neq y}\eta_{y^{\prime}}.\end{cases}\]

Next, we consider the binary case where \(|\mathcal{Y}|=2\). Since the confidences sum to 1, we have \(\max_{y^{\prime}\neq y}p_{y^{\prime}}(\bar{\mathbf{x}};h)=1-p_{y}(\bar{\mathbf{x}};h)\). Putting this in (17) implies

\[\min_{\bar{\mathbf{x}}\in\mathcal{N}_{r}(\mathbf{x})}\min_{h\in\mathcal{F}(\mathbf{x})}p_ {y}(\bar{\mathbf{x}};h)\geq\frac{1+\eta_{y}-\min_{y^{\prime}\neq y}\eta_{y^{ \prime}}}{2}.\]

### Proof of Lemma 4

Let \(r_{S}\colon S\to\{1,\ldots,|S|\}\) be a bijection that returns the _rank_ of an element in an ordered set \(S\). Let \(\dot{r}_{S}\colon 2^{S}\to 2^{\{1,\ldots,|S|\}}\) be an elementwise extension of \(r_{S}\) that returns a _set of ranks_ for an ordered set of elements--i.e., \(\dot{r}_{S}(U)=\{\,r_{S}(i):i\in U\,\}\) for \(U\subseteq S\). We claim \(m(\bar{\epsilon})=\dot{r}_{\epsilon^{*}}^{-1}(\dot{r}_{\dot{\epsilon}^{*}}( \bar{\epsilon}))\) is a bijection that satisfies the required property.

To prove the claim, we note that \(m\) is a bijection from \(2^{\bar{\epsilon}^{*}}\) to \(2^{\epsilon^{*}}\) since it is a composition of bijections \(\dot{r}_{\bar{\epsilon}^{*}}\colon 2^{\bar{\epsilon}^{*}}\to 2^{\{1,\ldots,l\}}\) and \(\dot{r}_{\epsilon^{*}}^{-1}\colon 2^{\{1,\ldots,l\}}\to 2^{\epsilon^{*}}\) where \(l=|\bar{\epsilon}^{*}|=|\epsilon^{*}|\). Next, we observe that \(\dot{r}_{\bar{\epsilon}^{*}}(\bar{\epsilon})\) relabels indices in \(\bar{\epsilon}\) so they have the same effect when applied to \(\mathbf{z}^{*}\) as \(\bar{\epsilon}\) on \(\bar{\mathbf{x}}\) (this also holds for \(\dot{r}_{\epsilon^{*}}\) and \(\epsilon\)). Thus

\[\operatorname{apply}(\bar{\mathbf{x}},\bar{\epsilon}) =\operatorname{apply}(\mathbf{z}^{*},\dot{r}_{\bar{\epsilon}^{*}}( \bar{\epsilon}))\] \[=\operatorname{apply}(\mathbf{z}^{*},\dot{r}_{\epsilon^{*}}(\dot{r}_{ \epsilon}^{-1}(\dot{r}_{\bar{\epsilon}^{*}}(\bar{\epsilon}))))\] \[=\operatorname{apply}(\mathbf{x},m(\bar{\epsilon}))\]as required. To prove the final statement, we use (4), (5) and (12) to write

\[\frac{s(\bar{\epsilon},\bar{\mathbf{x}};h)}{s(\epsilon,\mathbf{x};h)} =\frac{\mathbf{1}_{h(\operatorname{apply}(\bar{\mathbf{x}},\bar{\epsilon}))= y}p_{\mathsf{del}}^{|\bar{\mathbf{x}}|-|\bar{\epsilon}|}(1-p_{\mathsf{del}})^{|\bar{ \epsilon}|}}{\mathbf{1}_{h(\operatorname{apply}(\mathbf{x},\bar{\epsilon}))=y}p_{ \mathsf{del}}^{|\bar{\mathbf{x}}|-|\epsilon|}(1-p_{\mathsf{del}})^{|\epsilon|}}\] \[=\frac{p_{\mathsf{del}}^{|\bar{\mathbf{x}}|-|\mathbf{z}|}(1-p_{\mathsf{ del}})^{|\mathbf{z}|}\mathbf{1}_{h(\mathbf{z})=y}}{p_{\mathsf{del}}^{|\mathbf{x}|-|\mathbf{z}|}(1-p_{ \mathsf{del}})^{|\mathbf{z}|}\mathbf{1}_{h(\mathbf{z})=y}}\] \[=p_{\mathsf{del}}^{|\bar{\mathbf{x}}|-|\mathbf{x}|},\]

where the second last line follows from the fact that \(\operatorname{apply}(\bar{\mathbf{x}},\bar{\epsilon})=\operatorname{apply}(\mathbf{x}, \epsilon)=\mathbf{z}\).

### Proof of Theorem 5

Let \(\bar{\epsilon}^{\star}\) and \(\epsilon^{\star}\) be defined as in Lemma 4. We derive an upper bound on the sum over \(\epsilon\in 2^{\epsilon^{\star}}\) that appears in (13). Observe that

\[\sum_{\epsilon\not=2^{\epsilon^{\star}}}s(\epsilon,\mathbf{x};h) \leq\sum_{\epsilon\not=2^{\epsilon^{\star}}}\Pr\left[G(\mathbf{x})= \epsilon\right]\] \[=1-p_{\mathsf{del}}^{|\mathbf{x}|-|\epsilon^{\star}|}\sum_{|\epsilon| =0}^{|\epsilon^{\star}|}\binom{|\epsilon^{\star}|}{|\epsilon|}p_{\mathsf{del} }^{|\epsilon^{\star}|-|\epsilon|}(1-p_{\mathsf{del}})^{|\epsilon|}\] \[=1-p_{\mathsf{del}}^{|\mathbf{x}|-|\epsilon^{\star}|}, \tag{18}\]

where the first line follows from the inequality \(\mathbf{1}_{h(\operatorname{apply}(\mathbf{x},\epsilon)=y)}\leq 1\); the second line follows from the law of total probability; the third line follows by constraining the indices \(\{1,\ldots,|\mathbf{x}|\}\setminus\bar{\epsilon}^{\star}\) to be deleted; and the last line follows from the normalization of the binomial distribution. Putting (18) and \(\sum_{\bar{\epsilon}\in 2^{\epsilon^{\star}}}s(\bar{\epsilon},\bar{\mathbf{x}};h)\geq 0\) in (13) gives

\[p_{y}(\bar{\mathbf{x}};h) \geq p_{\mathsf{del}}^{|\bar{\mathbf{x}}|-|\mathbf{x}|}\left(\mu_{y}-1-p _{\mathsf{del}}^{|\mathbf{x}|-|\epsilon^{\star}|}\right)\] \[=p_{\mathsf{del}}^{|\bar{\mathbf{x}}|-|\mathbf{x}|}\left(\mu_{y}-1-p_{ \mathsf{del}}^{\frac{1}{2}(\operatorname{dist}_{\operatorname{LCS}}(\bar{\mathbf{ x}},\mathbf{x})+|\mathbf{x}|-|\bar{\mathbf{x}}|)}\right). \tag{19}\]

In the second line above we use the following relationship between the LCS distance and the length of the LCS \(|\mathbf{z}^{\star}|=|\epsilon^{\star}|\):

\[\operatorname{dist}_{\operatorname{LCS}}(\bar{\mathbf{x}},\mathbf{x})=|\bar{\mathbf{x}}|+ |\mathbf{x}|-2|\mathbf{z}^{\star}|.\]

Since (19) is independent of the base classifier \(h\), the lower bound on \(\rho(\bar{\mathbf{x}},\mathbf{x},\mu_{y})\) follows immediately.

### Proof of Corollary 6

Since the length of \(\mathbf{x}\) can only be changed by inserting or deleting elements in \(\bar{\mathbf{x}}\), we have

\[|\mathbf{x}|-|\bar{\mathbf{x}}|=n_{\mathsf{ins}}-n_{\mathsf{del}}. \tag{20}\]

We also observe that the LCS distance can be uniquely decomposed in terms of the counts of insertion ops \(m_{\mathsf{ins}}\) and deletion ops \(m_{\mathsf{del}}\): \(\operatorname{dist}_{\operatorname{LCS}}(\bar{\mathbf{x}},\mathbf{x})=m_{\mathsf{del} }+m_{\mathsf{ins}}\). These counts can in turn be related to the given decomposition of edit ops counts for generalized edit distance. In particular, any substitution must be expressed as an insertion and deletion under LCS distance, which implies \(m_{\mathsf{ins}}=n_{\mathsf{ins}}+n_{\mathsf{sub}}\) and \(m_{\mathsf{del}}=n_{\mathsf{del}}+n_{\mathsf{sub}}\). Thus we have

\[\operatorname{dist}_{\operatorname{LCS}}(\bar{\mathbf{x}},\mathbf{x})=n_{\mathsf{del} }+n_{\mathsf{ins}}+2n_{\mathsf{sub}}. \tag{21}\]

Substituting (20) and (21) in (14) gives the required result.

### Proof of Theorem 7

Eliminating \(n_{\text{sub}}\) from (16) using the constraint \(n_{\text{sub}}=r-n_{\text{del}}-n_{\text{ins}}\), we obtain a minimization problem in two variables:

\[\min_{n_{\text{ins}},n_{\text{del}}\in\mathbb{N}_{0}} \psi(n_{\text{ins}},n_{\text{del}})\] s.t. \[0\leq n_{\text{ins}}+n_{\text{del}}\leq r\]

where \(\psi(n_{\text{ins}},n_{\text{del}})=p_{\text{del}}^{n_{\text{del}}-n_{\text{ min}}}\left(\mu_{y}-1+p_{\text{del}}^{r-n_{\text{del}}}\right)\). Observe that \(\psi\) is monotonically increasing in \(n_{\text{ins}}\) and \(n_{\text{del}}\):

\[\frac{\psi(n_{\text{ins}}+1,n_{\text{del}})}{\psi(n_{\text{ins}},n_{\text{del}})} =\frac{1}{p_{\text{del}}}\geq 1\] \[\frac{\psi(n_{\text{ins}},n_{\text{del}}+1)}{\psi(n_{\text{ins}},n_{\text{del}})} =\frac{(\mu_{y}-1)p_{\text{del}}^{n_{\text{del}}+1}+p_{\text{del }}^{r}}{(\mu_{y}-1)p_{\text{del}}^{n_{\text{del}}}+p_{\text{del}}^{r}}\geq 1,\]

where the second inequality follows since we only consider \(r\) and \(\mu_{y}\) such that the numerator and denominator are positive. Thus the minimizer is \((n_{\text{ins}}^{*},n_{\text{del}}^{*},n_{\text{sub}}^{*})=(0,0,r)\) and we find \(\rho(\mathbf{x};\mu_{y})=\mu_{y}-1+p_{\text{del}}^{r}\). The expression for the certified radius follows by solving \(\rho(\mathbf{x};\mu_{y})\geq\nu_{y}(\mathbf{\eta})\) for non-negative integer \(r\).

### Proof of Corollary 8

Recall that Corollary 6 gives the following lower bound on the classifier's confidence at \(\mathbf{x}\):

\[\tilde{\rho}(\bar{\mathbf{x}},\mathbf{x},\mu_{y})=p_{\text{del}}^{n_{\text{del}}-n_{ \text{ins}}}\left(\mu_{y}-1+p_{\text{del}}^{n_{\text{sub}}+n_{\text{min}}} \right).\]

Observe that we can replace \(\mu_{y}\) by a lower bound \(\underline{\mu}_{y}\) that holds with probability \(1-\alpha\) (as is done in lines 4-6 of Figure 1) and obtain a looser lower bound \(\tilde{\rho}(\bar{\mathbf{x}},\mathbf{x},\underline{\mu}_{y})\leq\tilde{\rho}(\bar{ \mathbf{x}},\mathbf{x},\mu_{y})\) that holds with probability \(1-\alpha\). Crucially, this looser lower bound has the same functional form, so all results depending on Corollary 6, namely Theorem 7 and Table 1, continue to hold albeit with probability \(1-\alpha\).

## Appendix C Background for malware detection case study

In this appendix, we provide background for our case study on malware detection, including motivation for studying certified robustness of malware detectors, a formulation of malware detection as a sequence classification problem, and a threat model for adversarial examples.

### Motivation

Malware (malicious software) detection is a vital capability for proactively defending against cyberattacks. Despite decades of progress, building and maintaining effective malware detection systems remains a challenge, as malware authors continually evolve their tactics to bypass detection and exploit new vulnerabilities. One technology that has lead to advancements in malware detection, is the application of machine learning (ML), which is now used in many commercial systems [69, 44, 46, 47] and continues to be an area of interest in the malware research community [70, 71, 45, 50]. While traditional detection techniques rely on manually-curated signatures or detection rules, ML allows a detection model to be learned from a training corpus, that can potentially generalize to unseen programs.

Although ML has an apparent advantage in detecting previously unseen malware, recent research has shown that ML-based static malware detectors can be evaded by applying adversarial perturbations [18, 19, 20, 37, 51, 31, 52, 17, 39]. A variety of perturbations have been considered with different effects at the semantic level, however all of them can be modeled as inserting, deleting and/or substituting bytes. This prompts us to advance certified robustness for sequence classifiers within this general threat model--where an attacker can perform byte-level edits.

### Related work

Several empirical defense methods have been proposed to improve robustness of ML classifiers [63, 64]. Incer Romeo et al. [34] compose manually crafted Boolean features with a classifier that is constrained to be monotonically increasing with respect to selected inputs. This approach permits a combination of (potentially vulnerable) learned behavior with domain knowledge, and thereby aims to mitigate adversarial examples. Demontis et al. [63] show that the sensitivity of linear support vector machines to adversarial perturbations can be reduced by training with \(\ell_{\infty}\) regularization of weights. In another work, Quiring et al. [64] take advantage of heuristic-based semantic gap detectors and an ensemble of feature classifiers to improve empirical robustness. Compared to our work on certified adversarial defenses, these approaches do not provide formal guarantees.

_Binary normalization_[72, 73, 74, 75] was originally proposed to defend against polymorphic/metamorphic malware, and can also be seen as a mitigation to certain adversarial examples. It attempts to sanitize binary obfuscation techniques by mapping malware to a canonical form before running a detection algorithm. However, binary normalization cannot fully mitigate attacks like _Disp_ (see Table 9), as deducing opaque and evasive predicates are NP-hard problems [17].

Dynamic analysis can provide additional insights for malware detection. In particular, it can record a program's behavior while executing it in a sandbox (e.g., collecting a call graph or network traffic) [76, 77, 78, 79, 80]. Though detectors built on top of dynamic analysis can be more difficult to evade, as the attacker needs to obfuscate the program's behavior, they are still susceptible to adversarial perturbations. For example, an attacker may insert API calls to obfuscate a malware's behavior [81, 82, 83, 84]. Applying RS-Del to certify detectors that operate on call sequences [80] or more general dynamic features would be an interesting future direction.

### Static ML-based malware detection

We formulate malware detection as a sequence classification problem, where the objective is to classify a file in its raw byte sequence representation as malicious or benign. In the notation of Section 2, we assume the space of input sequences (files) is \(\mathcal{X}=\Omega^{\star}\) where \(\Omega=\{0,1,\ldots,255\}\) denotes the set of bytes, and we assume the set of classes is \(\mathcal{Y}=\{0,1\}\) where 1 denotes the'malicious' class and 0 denotes the 'benign' class. Within this context, a _malware detector_ is simply a classifier \(f:\mathcal{X}\rightarrow\mathcal{Y}\).

Detector assumptionsMalware detectors are often categorized according to whether they perform static or dynamic analysis. Static analysis extracts information without executing code, whereas dynamic analysis extracts information by executing code and monitoring its behavior. In this work, we focus on machine learning-based static malware detectors, where the ability to extract and synthesize information is learned from data. Such detectors are suitable as base classifiers for RS-Del, as they can learn to make (weak) predictions for incomplete files where chunks of bytes are arbitrarily removed. We note that dynamic malware detectors are not compatible with RS-Del, since it is not generally possible to execute an incomplete file.

Incorporating semanticsIn Section 3.3, we noted that our methods are compatible with _sequence chunking_ where the original input sequence is partitioned into chunks, and reinterpreted as a sequence of chunks rather than a sequence of lower-level elements. In the context of malware detection, we can partition a byte sequence into semantically meaningful chunks using information from a disassembler, such as Ghidra [85]. For example, a disassembler can be applied to a Windows executable to identify chunks of raw bytes that correspond to components of the header, machine instructions, raw data, padding etc. Applying our deletion smoothing mechanism at the level of semantic chunks, rather than raw bytes, may improve robustness as it excludes edits within chunks that may be semantically invalid. It also yields a different chunk-level edit distance certificate, that may cover a larger set of adversarial examples than a byte-level certificate of the same radius. Figure 3 illustrates the difference between byte-level and chunk-level deletion for a Windows executable, where chunks correspond to machine instructions (such as push ebp) or non-instructions (NI).

### Threat model

We next specify the modeled attacker's goals, capabilities and knowledge for our malware detection case study [86].

Attacker's objectiveWe consider evasion attacks against a malware detector \(f:\mathcal{X}\rightarrow\mathcal{Y}\), where the attacker's objective is to transform an executable file \(\mathbf{x}\) so that it is misclassified by \(f\). To ensure the attacked file \(\bar{\mathbf{x}}\) is useful after evading detection, we require that it is _functionally equivalent_ to the original file \(\mathbf{x}\). We focus on evasion attacks that aim to misclassify a _malicious_ file as _benign_ in our experiments, as these attacks dominate prior work [52]. However, the robustness certificates derived in Section 4 also cover attacks in the opposite direction--where a _benign_ file is misclassified as _malicious_.

Attacker's capabilityWe measure the attacker's capability in terms of the number of elementary edits they make to the original file \(\mathbf{x}\). If the attacker is capable of making up to \(c\) elementary edits, then they can transform \(\mathbf{x}\) into any file in the edit distance ball of radius \(c\) centred on \(\mathbf{x}\):

\[\mathcal{A}_{c}(\mathbf{x})=\{\bar{\mathbf{x}}\in\mathcal{X}:\mathrm{dist}_{O}(\mathbf{x},\bar{\mathbf{x}})\leq c\}.\]

Here \(\mathrm{dist}_{O}(\mathbf{x},\bar{\mathbf{x}})\) denotes the edit distance from the original file \(\mathbf{x}\) to the attacked file \(\bar{\mathbf{x}}\) under the set of edit operations (ops) \(O\). We assume \(O\) consists of elementary byte-level or chunk-level deletions (del), insertions (ins) and substitutions (sub), or a subset of these operations.

We note that edit distance is a reasonable proxy for the cost of running evasion attacks that iteratively apply localized functionality-preserving edits (e.g., [19, 37, 38, 17, 39]). For these attacks, the edit distance scales roughly linearly with the number of attack iterations, and therefore the attacker has an incentive to minimize edit distance. While attacks do exist that make millions of elementary edits in the malware domain (e.g., [31]), we believe that an edit distance-constrained threat model is an important step towards realistic threat models for certified malware detection. (To examine the effect of large edits on robustness we include the _GAMMA_ attack [31] in experiments covered in Appendix F.)

_Remark 9_.: The set \(\mathcal{A}_{c}(\mathbf{x})\)_overestimates_ the capability of an edit distance-constrained attacker, because it may include files that are not functionally equivalent to \(\mathbf{x}\). For example, \(\mathcal{A}_{c}(\mathbf{x})\) may include files that are not malicious (assuming \(\mathbf{x}\) is malicious) or files that are invalid executables. This poses no problem for certification, since overestimating an attacker's capability merely leads to a stronger certificate than required. Indeed, overestimating the attacker's capability seem necessary, as functionally equivalent files are difficult to specify, let alone analyze.

Figure 3: Illustration of the deletion smoothing mechanism applied to an executable file at the byte-level versus chunk-level. _Left:_ An executable file where the elementary byte sequence representation is shown in the 2nd column and chunks that correspond to machine instructions are shown in the 3rd column (sourced from the Ghidra [85] disassembler). Bytes that do not correspond to machine instructions are marked NI. Shading represents bytes (light gray) or instruction chunks (dark gray) that are deleted in the corresponding perturbed file to the right. _Middle:_ A perturbed file produced by the deletion mechanism operating at the byte level (Byte). Notice that individual instructions may be partially deleted. _Right:_ A perturbed file produced by the deletion mechanism operating at the chunk-level (Insn).

Attacker's knowledgeIn our certification experiments in Appendix E, we assume the attacker has full knowledge of the malware detector and certification scheme. When testing published attacks in Appendix F, we consider both white-box and black-box access to the malware detector. In the black-box setting, the attacker may make an unlimited number of queries to the malware detector without observing its internal operation. We permit access to detection confidence scores, which are returned alongside predictions even in the black-box setting. In the white-box setting, the attacker can additionally inspect the malware detector's source code. Such a strong assumption is needed for white-box attacks against neural network-based detectors that compute loss gradients with respect to the network's internal representation of the input file [20, 17].

## Appendix D Experimental setup for malware detection case study

In this appendix, we detail the experimental setup for our malware detection case study.

### Datasets

Though our methods are compatible with executable files of any format, in our experiments we focus on the _Portable Executable (PE) format_[87], since datasets, malware detection models and adversarial attacks are more extensively available for this format. Moreover, PE format is the standard for executables, object files and shared libraries in the Microsoft Windows operating system, making it an attractive target for malware authors. We use two PE datasets which are summarized in Table 3 and described below.

Sleipnir2This dataset attempts to replicate data used in past work [54], which was not published with raw samples. We were able to obtain the raw malicious samples from a public malware repository called VirusShare [88] using the provided hashes. However, since there is no similar public repository for benign samples, we followed established protocols [89, 90, 20] to collect a new set of benign samples. Specifically, we set up a Windows 7 virtual machine with over 300 packages installed using Chocolatey package manager [91]. We then extracted PE files from the virtual machine, which were assumed benign4, and subsampled them to match the number of malicious samples. The dataset is randomly split into training, validation and test sets with a ratio of 60%, 20% and 20% respectively.

Footnote 4: Chocolatey packages are validated against VirusTotal [92].

VTFeedThis dataset was first used in recent attacks on end-to-end ML-based malware detectors [17]. It was collected from VirusTotal--a commercial threat intelligence service--by sampling PE files from the live feed over a period of two weeks in 2020. Labels for the files were derived from the 68 antivirus (AV) products aggregated on VirusTotal at the time of collection. Files were labeled _malicious_ if they were flagged malicious by 40 or more of the AV products, they were labeled _benign_ if they were not flagged malicious by any of the AV products, and any remaining files were excluded. Following Lucas et al. [17], the dataset is randomly split into training, validation and test sets with a ratio of 80%, 10%, and 10% respectively.

We note that VTFeed comes with strict terms of use, which prohibit us from loading it on our high performance computing (HPC) cluster. As a result, we use Sleipnir2 for comprehensive experiments (e.g., varying \(p_{\text{del}}\), \(\mathbf{\eta}\)) on the HPC cluster, and VTFeed for a smaller selection of experiments run on a local server.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & & \multicolumn{3}{c}{Number of samples} \\ \cline{3-5} Dataset & Label & Train & Validation & Test \\ \hline \multirow{2}{*}{Sleipnir2} & Benign & 20 948 & 7 012 & 6 999 \\  & Malicious & 20 768 & 6 892 & 6 905 \\ \hline \multirow{2}{*}{VTFeed} & Benign & 111 258 & 13 961 & 13 926 \\  & Malicious & 111 395 & 13 870 & 13 906 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Summary of datasets.

### Malware detection models

We experiment with malware detection models based on MalConv [48]. MalConv was one of the first _end-to-end_ neural network models proposed for malware detection--i.e., it learns to classify directly from raw byte sequences, rather than relying on manually engineered features. Architecturally, it composes a learnable embedding layer with a shallow convolutional network. A large window size and stride of 500 bytes are employed to facilitate scaling to long byte sequences. Though MalConv is compatible with arbitrarily long byte sequences in principle, we truncate all inputs to 2MB to support training efficiency. We use the original parameter settings and training procedure [48], except where specified in Appendix D.5.

Using MalConv as a basis, we consider three models as described below.

NsA vanilla non-smoothed (NS) MalConv model. This model serves as a non-certified, non-robust baseline--i.e., no specific techniques are employed to improve robustness to evasion attacks and certification is not supported.

Rs-AbnA smoothed MalConv model using the _randomized ablation_ smoothing mechanism proposed by Levine and Feizi [24] and reviewed in Appendix I. This model serves as a certified robust baseline, albeit covering a more restricted threat model than the edit distance threat model we propose in Section 2. Specifically, it supports robustness certification for the Hamming distance threat model, where the adversary is limited to substitution edits (\(O=\{\text{sub}\}\)). Since Levine and Feizi's formulation is for images, several modifications are required to support malware detection as described in Appendix G. To improve convergence, we also apply gradient clipping when learning parameters in the embedding layer (see Appendix G). We consider variants of this model for different values of the ablation probability \(p_{\text{ab}}\).

RS-DelA smoothed MalConv model using our proposed randomized deletion smoothing mechanism. This model supports robustness certification for the generalized edit distance threat model where \(O\subseteq\{\text{del},\text{ins},\text{sub}\}\). We consider variants of this model for different values of the deletion probability \(p_{\text{del}}\), decision thresholds \(\mathbf{\eta}\), and whether deletion/certification is performed at the byte-level (Byte) or chunk-level (Insn). We perform chunking as illustrated in Figure 3--i.e., we chunk bytes that correspond to distinct machine instructions using the Ghidra disassembler.

### Controlling false positive rates

Malware detectors are typically tuned to achieve a low false positive rate (FPR) (e.g., less than \(0.1\)-\(1\%\)) since producing too many false alarms is a nuisance to users.5 To make all malware detection models comparable, we calibrate the FPR to 0.5% on the test set for the experiments reported in Appendix E and 0.5% on the validation set for the experiments reported in Appendix F unless otherwise noted. This calibration is done by adjusting the decision threshold of the base MalConv model.

Footnote 5: [https://www.av-comparatives.org/testmethod/false-alarm-tests/](https://www.av-comparatives.org/testmethod/false-alarm-tests/)

### Compute resources

Experiments for the Sleipnir2 dataset were run a high performance computing (HPC) cluster, where the requested resources varied depending on the experiment. We generally requested a single NVIDIA P100 GPU when training and certifying models. Experiments for the VTFeed dataset were run on a local server due to restrictive terms of use. Compute resources and approximate wall clock running times are reported in Tables 4 and 5 for training and certification for selected parameter settings. Running times for other parameters settings are lower than the ones reported in these tables.

### Parameter settings

We specify the parameter settings and training procedure for MalConv, which is used standalone in NS, and as a base model for the smoothed models RS-Del and RS-Abn. Table 6 summarizes our setup, which is consistent across all three models except where specified. We follow the authors of MalConv [48] when setting parameters for the model and the optimizer, however we set a largermaximum input size of 2MiB to accommodate larger inputs without clipping. Due to differences in available GPU memory for the Sleipnir2 and VTFeed experiments, we use a larger batch size for VTFeed than for Sleipnir2. We also set a higher limit on the maximum number of epochs for VTFeed, as it is a larger dataset, although the NS and RS-Del models converge within 50 epochs for both datasets. To stabilize training for the smoothed models (RS-Del and RS-Abn), we modify the smoothing mechanisms during _training only_ to ensure at least 500 raw bytes are preserved. This may limit the number of deletions for RS-Del and the number of ablated (masked) bytes for RS-Abn. For RS-Abn, we clip the gradients for the embedding layer to improve convergence (see Appendix G).

## Appendix E Evaluation of robustness certificates for malware detection

In this appendix, we evaluate the robustness guarantees and accuracy of RS-Del for malware detection. We consider two instantiations of the edit distance threat model. First, in Appendix E.1, we consider the Levenshtein distance threat model, where the attacker's elementary edits are unconstrained and may include deletions, insertions and substitutions. Then, in Appendix E.2, we consider the more restricted Hamming distance threat model, where an attacker is only able to perform substitutions. We summarize our findings in Appendix E.3. Overall, we find that RS-Del generates robust predictions with minimal impact on model accuracy for the Levenshtein distance threat model, and outperforms RS-Abn[24] for the Hamming distance threat model.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Dataset & Requested resources & Model & Parameters & Time \\ \hline \multirow{8}{*}{Sleipnir2} & \multirow{8}{*}{\begin{tabular}{l} 1 NVIDIA P100 GPU, 12 cores on Intel Xeon Gold 6326 CPU \\ \end{tabular} } & NS & – & 5 min \\ \cline{3-6}  & & \begin{tabular}{l} RS-Del \\ \end{tabular} & \begin{tabular}{l} Byte, \\ \(p_{\text{del}}=95\%\) \\ \end{tabular} & \begin{tabular}{l} 65 hr \\ \end{tabular} \\ \cline{3-6}  & & RS-Del & \begin{tabular}{l} INsn, \\ \(p_{\text{del}}=95\%\) \\ \end{tabular} & \begin{tabular}{l} 140 hr \\ \end{tabular} \\ \cline{3-6}  & & RS-Abn & \begin{tabular}{l} \(p_{\text{ab}}=95\%\) \\ \end{tabular} & \begin{tabular}{l} 210 hr \\ \end{tabular} \\ \hline \multirow{8}{*}{VTFeed} & \begin{tabular}{l} 1 NVIDIA RTX3090 GPU, 6 cores on AMD Ryzen \\ Threadripper PRO 3975WX CPU \\ \end{tabular} & NS & – & 4 min \\ \cline{3-6}  & & \begin{tabular}{l} RS-Del \\ \end{tabular} & \begin{tabular}{l} Byte, \\ \(p_{\text{del}}=97\%\) \\ \end{tabular} & 
\begin{tabular}{l} 500 hr \\ \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
Table 5: Compute resources used for certification on the test set. The evaluation dataset is partitioned and processed on multiple compute nodes with the same specifications. The reported time is the sum of wall times on each compute node. Note that the times reported are for an unoptimized implementation where the smoothing mechanism is executed on the CPU.

We report the following quantities in our evaluation:

* _Certified radius (CR)._ The radius of the largest robustness certificate that can be issued for a given input, model and certification method. Note that this is a conservative measure of robustness since it is _tied to the certification method_. The _median CR_ is reported on the test set.
* _Certified accuracy_[13, 14], also known as _verified-robust accuracy_[66, 55], evaluates robustness certificates and accuracy of a model simultaneously with respect to a test set. It is defined as the fraction of instances in the test set \(\mathbb{D}\) for which the model \(f\)'s prediction is correct _and_ certified robust at radius \(r\) or greater: \[\textsc{CertAcc}_{r}(\mathbb{D})=\sum_{(\mathbf{x},y)\in\mathbb{D}}\frac{\mathbf{1}_{ f(\mathbf{x})=y}\mathbf{1}_{\mathrm{CR}(\mathbf{x})\geq r}}{|\mathbb{D}|}\] (22) where \(\mathrm{CR}(\mathbf{x})\) denotes the certified radius for input \(\mathbf{x}\) returned by the certification method.
* _Clean accuracy._ The fraction of instances in the test set for which the model's prediction is correct.

We briefly mention default parameter settings for the experiments presented in this appendix. When approximating the smoothed models (RS-Del and RS-Abn) we sample \(n_{\mathrm{pred}}=1000\) perturbed inputs for prediction and \(n_{\mathrm{bnd}}=4000\) perturbed inputs for certification, while setting the significance level \(\alpha\) to \(0.05\). Unless otherwise specified, we set the decision thresholds for the smoothed models so that \(\mathbf{\eta}=0\). After fixing \(\mathbf{\eta}\), the decision thresholds for the base models are tuned to yield a false positive rate of 0.5%. We note that the entire test set is used when reporting metrics and summary statistics in this appendix.

### Levenshtein distance threat model

We first present results for the Levenshtein distance threat model, where the attacker's elementary edits are unconstrained (\(O=\{\mathsf{del},\mathsf{ins},\mathsf{sub}\}\)). We vary three parameters associated with RS-Del: the deletion probability \(p_{\mathsf{del}}\), the decision thresholds of the smoothed model \(\mathbf{\eta}\), and the level of sequence chunking (i.e., whether sequences are chunked at the byte-level or instruction-level). We use NS as a baseline as there are no prior certified defenses for the Levenshtein distance threat model to our knowledge.

\begin{table}
\begin{tabular}{l l l} \hline \hline  & Parameter & Values \\ \hline \multirow{4}{*}{MalConv} & Max input size & 2097152 \\ \cline{2-3}  & Embedding size & 8 \\ \cline{2-3}  & Window size & 500 \\ \cline{2-3}  & Channels & 128 \\ \hline \multirow{4}{*}{Optimizer} & Python class & torch.optim.SGD \\ \cline{2-3}  & Learning rate & 0.01 \\ \cline{2-3}  & Momentum & 0.9 \\ \cline{2-3}  & Weight decay & 0.001 \\ \hline \multirow{4}{*}{Training} & Batch size & 24 (Sleipnir2), 32 (VTFeed) \\ \cline{2-3}  & Max. epoch & 50 (Sleipnir2), 100 (VTFeed) \\ \cline{1-1} \cline{2-3}  & Min. preserved bytes & 500 (RS-Del, RS-Abn), NA (NS) \\ \cline{1-1} \cline{2-3}  & Embedding gradient clipping & 0.5 (RS-Abn), \(\infty\) (RS-Del, NS) \\ \cline{1-1} \cline{2-3}  & Early stopping & If validation loss does not improve after 10 epochs \\ \hline \hline \end{tabular}
\end{table}
Table 6: Parameter settings for MalConv, the optimizer and training procedure. Parameter settings are consistent across all malware detection models (NS, RS-Del, RS-Abn) except where specified.

Certified accuracyFigure 4 plots the certified accuracy of RS-Del using byte-level deletion as a function of the radius (left horizontal axis), radius normalized by file size (right horizontal axis) on the Sleipnir2 dataset for several values of \(p_{\mathsf{del}}\). We observe that the curves for larger values of \(p_{\mathsf{del}}\) approximately dominate the curves for smaller values of \(p_{\mathsf{del}}\), for \(p_{\mathsf{del}}\leq 99.5\%\) (i.e., the accuracy is higher or close for all radii). This suggests that the robustness of RS-Del can be improved without sacrificing accuracy by increasing \(p_{\mathsf{del}}\) up to 99.5%. However, for the larger value \(p_{\mathsf{del}}=99.9\%\), we observe a drop in certified accuracy of around 10% for smaller radii and an increase for larger radii. By normalizing with respect to the file size, we can see that our certificate is able to certify up to \(1\%\) of the file size. We also include an analogous plot for chunk-level deletion in Figure 5 which demonstrates similar behavior. We note that chunk-level deletion arguably provides stronger guarantees, since the effective radius for chunk-level Levenshtein distance is larger than for byte-level Levenshtein distance.

It is interesting to relate these certification results to published evasion attacks. Figure 4 shows that we can achieve a certified accuracy in excess of 90% at a Levenshtein distance radius of 128 bytes when \(p_{\mathsf{del}}=99.5\%\). This radius is larger than the median Levenshtein distance of two attacks that manipulate headers of PE files [37, 38] (see Table 9 in Appendix F). We can therefore provide reasonable robustness guarantees against these two attacks. However, a radius of 128 bytes is orders of magnitude smaller than the median Levenshtein distances of other published attacks which range from tens of KB [17, 20] to several MB [31] (also reported in Table 9). While some of these attacks

Figure 4: Certified accuracy for RS-Del as a function of the radius in bytes (left horizontal axis), radius normalized by file size (right horizontal axis) and byte deletion probability \(p_{\mathsf{del}}\) (line styles). The results are plotted for the Sleipnir2 test set under the byte-level Levenshtein distance threat model (with \(O=\{\mathsf{del},\mathsf{ins},\mathsf{sub}\}\)). The grey vertical lines in the left plot represent the best achievable certified radius for RS-Del (setting \(\mu_{y}=1\) in the expressions in Table 1).

Figure 5: Certified accuracy for RS-Del with chunk-level deletion (Insn) as a function of the radius in chunks (left horizontal axis), radius normalized by sequence length in chunks (right horizontal axis) and chunk deletion probability \(p_{\mathsf{del}}\) (line styles). The results are plotted for the Sleipnir2 test set under the chunk-level Levenshtein distance threat model (with \(O=\{\mathsf{del},\mathsf{ins},\mathsf{sub}\}\)). The grey vertical lines in the left plot represent the best achievable certified radius for RS-Del (setting \(\mu_{y}=1\) in the expressions in Table 1).

arguably fall outside an edit distance constrained threat model, we consider them in our empirical evaluation of robustness in Appendix F.

Clean accuracy and abstention ratesTable 7 reports clean accuracy for RS-Del and the non-certified NS baseline. It also reports abstention rates for RS-Del, the median certified radius (CR), and the median certified radius normalized by file size (NCR). We find that clean accuracy for Sleipnir2 follows similar trends as certified accuracy: it is relatively stable for \(p_{\mathsf{del}}\) in the range 90-99.5%, but drops by more than 10% at \(p_{\mathsf{del}}=99.9\%\). We note that the clean accuracy of RS-Del (excluding \(p_{\mathsf{del}}=99.9\%\)) is at most 3% lower than the NS baseline for Sleipnir2 and at most 7% lower than the NS baseline for VTFeed. We observe minimal differences in the results for chunk-level (Insn) and byte-level (Byte) deletion smoothing, but note that the effective CR is larger for chunk-level smoothing, since each chunk may contain several bytes.

Accuracy under high deletionIt may be surprising that RS-Del can maintain high accuracy even when deletion is aggressive. We offer some possible explanations. First, we note that even with a high deletion probability of \(p_{\mathsf{del}}=99.9\%\), the smoothed model accesses almost all of the file in expectation, as it aggregates \(n_{\mathrm{pred}}=1000\) predictions from the base model each of which accesses a random \(0.1\%\) of the file in expectation. Second, we posit that malware detection may be "easy" for RS-Del on these datasets. This could be due to the presence of signals that are robust to deletion (e.g., file size or byte frequencies) or redundancy of signals (i.e., if a signal is deleted in one place it may be seen elsewhere).

Decision thresholdWe demonstrate how the decision thresholds \(\boldsymbol{\eta}\) introduced in Section 3.1 can be used to trade off certification guarantees between classes. We consider normalized decision thresholds where \(\sum_{y}\eta_{y}=1\) and \(\eta_{y}\in[0,1]\). We only specify the value of \(\eta_{1}\) when discussing our results, noting that \(\eta_{0}=1-\eta_{1}\) in our two-class setting.

Table 8 provides error rates and robustness metrics for several values of \(\eta_{1}\), using byte-level Levenshtein distance with \(p_{\mathsf{del}}=99.5\%\). When varying \(\eta_{1}\), we also vary the decision threshold of the base model to achieve a target false positive rate (FPR) of 0.5%. Looking at the table, we see that \(\eta_{1}\) has minimal impact on the false negative rate (FNR), which is stable around 7%. However, there

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & & & Clean accuracy & Median CR & Median \\ Dataset & Model & Parameters & (Abstain rate) \% & (UB) & NCR \% \\ \hline \multirow{8}{*}{Sleipnir2} & NS & \(-\) & \(98.9\) & \(-\) & \(-\) & \(-\) & \(-\) \\ \cline{2-7}  & & Byte, \(p_{\mathsf{del}}=90\%\) & \(97.1\) & \((0.2)\) & \(6\) & \((6)\) & \(0.0023\) \\  & & Byte, \(p_{\mathsf{del}}=95\%\) & \(97.8\) & \((0.0)\) & \(13\) & \((13)\) & \(0.0052\) \\  & & Byte, \(p_{\mathsf{del}}=97\%\) & \(97.4\) & \((0.1)\) & \(22\) & \((22)\) & \(0.0093\) \\  & & Byte, \(p_{\mathsf{del}}=99\%\) & \(98.1\) & \((0.1)\) & \(68\) & \((68)\) & \(0.0262\) \\  & & Byte, \(p_{\mathsf{del}}=99.5\%\) & \(\mathbf{96.5\,(0.2)}\) & \(\mathbf{137\,(138)}\) & \(\mathbf{0.0555}\) \\  & & Byte, \(p_{\mathsf{del}}=99.9\%\) & \(83.7\) & \((3.4)\) & \(688\) & \((692)\) & \(0.2269\) \\ \cline{2-7}  & \multirow{4}{*}{RS-Del} & Insn, \(p_{\mathsf{del}}=90\%\) & \(97.9\) & \((0.1)\) & \(6\) & \((6)\) & \(0.0026\) \\  & & Insn, \(p_{\mathsf{del}}=95\%\) & \(97.8\) & \((0.1)\) & \(13\) & \((13)\) & \(0.0056\) \\  & & Insn, \(p_{\mathsf{del}}=97\%\) & \(98.3\) & \((0.0)\) & \(22\) & \((22)\) & \(0.0095\) \\  & & Insn, \(p_{\mathsf{del}}=99\%\) & \(97.6\) & \((0.1)\) & \(68\) & \((68)\) & \(0.0292\) \\  & & Insn, \(p_{\mathsf{del}}=99.5\%\) & \(\mathbf{96.8\,(0.2)}\) & \(\mathbf{137\,(138)}\) & \(\mathbf{0.0589}\) \\  & & Insn, \(p_{\mathsf{del}}=99.9\%\) & \(86.1\) & \((0.2)\) & \(689\) & \((692)\) & \(0.2982\) \\ \hline \multirow{3}{*}{VTFeed} & NS & \(-\) & \(98.9\) & \(-\) & \(-\) & \(-\) & \(-\) \\ \cline{2-7}  & RS-Del & Byte, \(p_{\mathsf{del}}=97\%\) & \(92.1\) & \((0.9)\) & \(22\) & \((22)\) & \(0.0045\) \\ \cline{1-1}  & & Byte, \(p_{\mathsf{del}}=99\%\) & \(86.9\) & \((0.8)\) & \(68\) & \((68)\) & \(0.0122\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Clean accuracy and robustness metrics for RS-Del as a function of the dataset (Sleipnir2 and VTFeed), deletion probability \(p_{\mathsf{del}}\) and deletion level (Byte or Insn). All metrics are computed on the test set. Here “abstain rate” refers to the fraction of test instances for which RS-Del abstains (line 7 in Figure 1), and “UB” refers to an upper bound on the median CR for a best case smoothed model (based on Table 1 with \(\mu_{y}=1\)). A good tradeoff is achieved when \(p_{\mathsf{del}}=99.5\%\) for both the byte-level (Byte) and chunk-level (Insn) certificates (highlighted in bold face below).

is a significant impact on the median CR (and theoretical upper bound), as reported separately for each class. The median CR is balanced for both the malicious and benign class when \(\eta_{1}=50\%\), but favours the malicious class as \(\eta_{1}\) is decreased. For instance when \(\eta_{1}=5\%\) a significantly larger median CR is possible for malicious files (137 to 578) at the expense of the median CR for benign files (137 to 10). This asymmetry in the class-specific CR is a feature of the theory--that is, in addition to controlling a tradeoff between error rates of each class, \(\eta_{1}\) also controls a tradeoff between the CR for each class (see Table 1).

Figure 6 plots the certified true positive rate (TPR) and true negative rate (TNR) of RS-Del on the Sleipnir2 dataset for several values of \(\eta_{1}\). The certified TPR and TNR can be interpreted as class-specific analogues of the certified accuracy. Concretely, the certified TPR (TNR) at radius \(r\) is the fraction of malicious (benign) instances in the test set for which the model's prediction is correct _and_ certified robust at radius \(r\). The certified TPR and TNR jointly measure accuracy and robustness and complement the metrics reported in Table 8. Looking at Figure 6, we see that the certified TNR curves drop more rapidly to zero than the certified TPR curves as \(\eta_{1}\) decreases. Again, this suggests decreasing \(\eta_{1}\) sacrifices the certified radii of benign instances to increase the certified radii of malicious instances. We note that the curves for \(\eta_{1}=50\%\) correspond to the same setting as the certified accuracy curve in Figure 4 (with \(p_{\mathsf{del}}=99.5\%\)).

### Hamming distance threat model

We now turn to the more restricted Hamming distance threat model, where the attacker is limited to performing substitutions only (\(O=\{\mathsf{sub}\}\)). We choose to evaluate this threat model as it is covered in previous work on randomized smoothing, called _randomized ablation_[24] (abbreviated RS-Abn), and can serve as a baseline for comparison with our method. Recall that we adapt RS-Abn for malware detection by introducing a parameter called \(p_{\mathsf{ab}}\), which is the fraction of bytes that are "ablated" (replaced by a special masked value) (see Appendix D.2). This parameter is analogous to \(p_{\mathsf{del}}\) in RS-Del, except that the number of ablated bytes is deterministic in RS-Abn, whereas the number of deleted bytes is random in RS-Del. We compare RS-Del and RS-Abn for varying values of \(p_{\mathsf{del}}\) and \(p_{\mathsf{ab}}\) using the Sleipnir2 dataset and byte-level Hamming distance.

Certified accuracyFigure 2 plots the certified accuracy of RS-Del and RS-Abn for three values of \(p_{\mathsf{del}}\) and \(p_{\mathsf{ab}}\). We observe that the certified accuracy is uniformly larger for our proposed method RS-Del than for RS-Abn when \(p_{\mathsf{del}}=p_{\mathsf{ab}}\). The superior certification performance of RS-Del is somewhat surprising given it is not optimized for the Hamming distance threat model. One possible explanation relates to the learning difficulty of RS-Abn compared with RS-Del. Specifically, we find that stochastic gradient descent is slower to converge for RS-Abn despite our attempts to improve convergence (see Appendix G). Recall, that RS-Del provides certificates for any of the threat models in Table 1--in addition to the Hamming distance certificate--without needing to modify the smoothing mechanism.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & & & \multicolumn{3}{c}{Median CR (UB)} \\ \cline{4-6} \(\eta_{1}\) (\%) & FNR (\%) & FPR (\%) & \multicolumn{2}{c}{Malicious} & \multicolumn{2}{c}{Benign} \\ \hline \(50\) & \(6.8\) & \(0.5\) & \(137\) & \((138)\) & \(137\) & \((138)\) \\ \(25\) & \(6.9\) & \(0.5\) & \(275\) & \((276)\) & \(57\) & \((57)\) \\ \(10\) & \(6.8\) & \(0.5\) & \(455\) & \((459)\) & \(20\) & \((21)\) \\ \(5\) & \(6.6\) & \(0.5\) & \(578\) & \((597)\) & \(10\) & \((10)\) \\ \(1\) & \(7.1\) & \(0.5\) & \(582\) & \((918)\) & \(1\) & \((2)\) \\ \(0.5\) & \(6.9\) & \(0.5\) & \(506\) & \((1057)\) & \(0\) & \((0)\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Impact of the smoothed decision threshold \(\eta_{1}\) on false negative error rate (FNR) and median certified radius (CR) for malicious and benign files. The false positive rate (FPR) is set to a target value of 0.5% by varying the decision threshold of the base model. The results are reported for Sleipnir2 with \(p_{\mathsf{del}}=99.5\%\) using byte-level Levenshtein distance. “UB” refers to an upper bound on the median CR for a best case smoothed model (based on Table 1 with \(\mu_{y}=1\)).

TightnessRS-Abn is provably tight, in the sense that it is not possible to issue a larger Hamming distance certificate unless more information is made available to the certification mechanism or the ablation smoothing mechanism is changed. This tightness result for RS-Abn, together with the empirical results in Figure 2, suggests that RS-Del produces certificates which are tight or close to tight in practice, at least for the Hamming distance threat model. This is an interesting observation, since it is unclear how to derive a tight, computationally tractable certificate for RS-Del.

### Summary

Our evaluation shows that RS-Del provides non-trivial robustness guarantees with a low impact on accuracy. The certified radii we observe are close to the best radii theoretically achievable using our mechanism. For the Levenshtein byte-level edit distance threat model, we obtain radii of a few hundred bytes in size, which can certifiably defend against attacks that edit headers of PE files [37, 38, 52]. However, certifying robustness against more powerful attacks that modify thousands or millions of bytes remains an open challenge. By varying the detection threshold, we show that certification can be performed asymmetrically for benign and malicious instances. This can boost the certified radii of malicious instances by a factor of 4 in some cases. While there are no prior methods to use as baselines for the Levenshtein distance threat model, our comparisons with RS-Abn[24] for

Figure 6: Certified true positive rate (TPR) and true negative rate (TNR) of RS-Del as a function of the certificate radius \(r\) (horizontal axis) and the decision threshold \(\eta_{1}\) (line style). The results are plotted for the Sleipnir2 test set for byte-level deletion (Byte) with \(p_{\mathsf{del}}=99.5\%\) under the Levenshtein distance threat model (with \(O=\{\mathsf{del},\mathsf{ins},\mathsf{sub}\}\)). It is apparent that \(\eta_{1}\) controls a tradeoff in the certified radius between the malicious (measured by TPR) and benign (measured by TNR) classes. Note that in this setting, a non-smoothed, non-certified model (NS) achieves a clean TPR and TNR of 98.2% and 99.5% respectively.

the Hamming distance threat model show that RS-Del outperforms RS-Abn in terms of both accuracy and robustness.

## Appendix F Evaluation of robustness to published attacks

In this appendix, we empirically evaluate the robustness of RS-Del to several published evasion attacks. By doing so, we aim to provide a more complete picture of robustness, as our certificates are conservative and may _underestimate_ robustness to real attacks, which are subject to additional constraints (e.g., maintaining executability, preserving a malicious payload, etc.). We introduce the attacks in Appendix F.1, provide details of the experimental setup in Appendix F.2 and discuss the results in Appendix F.3.

### Attacks covered

We consider five recently published attacks designed for evading static PE malware detectors as summarized in Table 9. The attacks cover a variety of edit distance magnitudes from tens of bytes to millions of bytes. While attacks that edit millions of bytes arguably fall outside our edit distance-constrained threat model, we include one such attack (_GAMMA_) to test the limits of our methodology. We note that four of the five attacks are able to operate in a black-box setting and can therefore be applied directly to RS-Del. However, the white-box attacks are designed for neural network malware detectors with a specific architecture. In particular, they assume the network receives a raw byte sequence as input, that the initial layer is an embedding layer, and that gradients can be computed with respect to the output of the embedding layer. Although these architectural assumptions are satisfied by the base MalConv model, they are not satisfied by RS-Del, because additional operations are applied before the embedding layer and the aggregation of base model predictions is not differentiable. In Appendix H, we adapt the white-box attacks for RS-Del by applying two tricks: (1) we apply the smoothing mechanism _after_ the embedding layer, and (2) we replace majority voting with soft aggregation following Salman et al. [94].

\begin{table}
\begin{tabular}{p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt}} \hline \hline Attack & Supported settings & Attack distance & Optimizer & Description \\ \hline \multirow{2}{*}{_Disp_[17]} & White-box, black-box & \multirow{2}{*}{\(17.2\) KB} & \multirow{2}{*}{Gradient-guided} & Disassembles the PE file and displaces chunks of code to a new section, replacing the original code with semantic nops. \\ \cline{1-1} \cline{5-5}  & White-box & & & Fast Gradient Sign Method [2] & Replaces non-functional bytes in slack regions or the overlay of the PE file with adversarially-crafted noise. \\ \hline \multirow{2}{*}{_HDOS_[37]} & White-box, black-box & \multirow{2}{*}{\(58.0\) B} & \multirow{2}{*}{Genetic algorithm} & Manipulates bytes in the DOS header of the PE file which are not used in modern Windows. \\ \cline{1-1} \cline{5-5}  & White-box, black-box & & & Manipulates fields in the header of the PE file (debug information, section names, checksum, etc.) which do not impact functionality. \\ \hline \multirow{2}{*}{_GAMMA_[31]} & White-box & \multirow{2}{*}{\(17.0\) B} & \multirow{2}{*}{Genetic algorithm} & Manipulates fields in the header of the PE file (debug information, section names, checksum, etc.) which do not impact functionality. \\ \cline{1-1} \cline{5-5}  & & & & Appends sections extracted from benign files to the end of a malicious PE file and modifies the header accordingly. \\ \hline \hline \end{tabular}
\end{table}
Table 9: Evasion attacks used in our evaluation. The _attack distance_ refers to the median Levenshtein distance computed on a set of 500 attacked files from the Sleipnir2 test set. We use a closed source implementation of _Disp_ and open source implementations of the remaining attacks based on secml-malware[93].

### Experimental setup

Since some of the attacks take hours to run for a single file, we use smaller evaluation sets containing malware subsampled from the test sets in Table 3. The evaluation set we use for Sleipnir2 consists of 500 files, and the one for VTFeed consists of 100 files (matching [17]). We note that our evaluation sets are comparable in size to prior work [18; 20; 51]. For each evaluation set, we report attack success rates against malware detectors trained on the same dataset.

Since all attacks employ greedy optimization with randomization, they may fail on some runs, but succeed on others. We therefore repeat each attack 5 times per file and use the best performing attacked file in our evaluation. We define the attack success rate as the proportion of files initially detected as malicious for which at least one of the 5 attack repeats is successful at evading detection. Lower attack success rates correlate with improved robustness against attacks. We permit all attacks to run for up to 200 attack iterations of the internal optimizer. Early stopping is enabled for those attacks that support it (_Disp_, _Slack_, _GAMMA_), which means the attack terminates as soon as the model's prediction flips from malicious to benign.

Where possible, we run _direct attacks_ against RS-Del and compare success rates against NS as a baseline. We also consider _transfer attacks_ from NS to RS-Del as an important variation to the threat model, where an attacker has limited access to the target RS-Del during attack optimization. When running direct attacks against RS-Del, we use a reduced number of Monte Carlo samples (\(n_{\mathrm{pred}}=100\)) to make the computational cost of the attacks more manageable. For both direct and transfer attacks against RS-Del, we set \(p_{\mathsf{del}}=97\%\) and perform deletion and certification at the byte-level (Byte).

### Results

The results for direct attacks against RS-Del are presented in Table 10. For both the Sleipnir2 and VTFeed datasets, we observe that the robustness of RS-Del is superior (or equal) to NS against four of the six attacks. The two cases where RS-Del's robustness drops compared to NS are for the strongest attacks: _Slack_ and _GAMMA_. The results for transfer attacks from NS to RS-Del are presented in Table 11. Almost all of the attacks transfer poorly to RS-Del. In most cases the attack success rates drop to zero or single digit percentages. We hypothesize that _Slack_ and _GAMMA_ make such drastic changes to the original binary that they can overwhelm the malicious signal--enough to cross the decision boundary--akin to a good word attack [95]. We find that _HDOS_ and _HField_ are ineffective for both RS-Del and the baseline NS. Both attacks change up to 58 bytes in the header, and tend to fall within our certifications.

\begin{table}
\begin{tabular}{l l l c c} \hline \hline  & & & \multicolumn{2}{c}{Attack success rate (\%)} \\ \cline{3-5} Setting & Attack & Dataset & NS & RS-Del \\ \hline \multirow{4}{*}{White-box} & _Disp_[17] & Sleipnir2 & \(73.8\) & \(\mathbf{56.7}\) \\  & VTFeed & \(94.1\) & \(\mathbf{74.5}\) \\ \cline{2-5}  & _Slack_[20] & Sleipnir2 & \(\mathbf{57.9}\) & \(85.3\) \\  & VTFeed & \(96.0\) & \(\mathbf{43.9}\) \\ \hline \multirow{4}{*}{Black-box} & _HDOS_[37] & Sleipnir2 & \(0.0\) & \(0.0\) \\  & VTFeed & \(0.0\) & \(0.0\) \\ \cline{1-1}  & _HField_[38] & Sleipnir2 & \(0.607\) & \(\mathbf{0.0}\) \\ \cline{1-1}  & VTFeed & \(0.990\) & \(\mathbf{0.0}\) \\ \cline{1-1} \cline{2-5}  & _Disp_[17] & Sleipnir2 & \(0.809\) & \(\mathbf{0.0}\) \\ \cline{1-1}  & VTFeed & \(10.9\) & \(\mathbf{0.0}\) \\ \cline{1-1} \cline{2-5}  & _GAMMA_[31] & Sleipnir2 & \(99.2\) & \(\mathbf{54.1}\) \\ \cline{1-1}  & VTFeed & \(\mathbf{76.2}\) & \(100.0\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Success rates of direct attacks against RS-Del and the NS baseline. A lower success rate is better from our perspective as a defender, as it means the model is more robust to the attack. The model that achieves the lowest success rate for each attack/dataset is highlighted in boldface.

## Appendix G Efficiency of RS-Del

In this appendix, we discuss the training and computational efficiency of RS-Del. We provide comparisons with RS-Abn [24], which serves as a baseline in Appendix E.2 for a more restricted Hamming distance threat model.

Computational efficiencyTable 12 reports wall clock times for training and prediction. For training, we measure the time taken to complete 1 epoch of stochastic gradient descent on the Sleipnir2 training set, where inputs are perturbed by the smoothing mechanism. For prediction, we measure the time taken for a single 1MB input file using \(n_{\mathrm{pred}}=1000\) Monte Carlo samples. We split the prediction time into two components: (1) the time taken to generate perturbed inputs from the smoothing mechanism and (2) the time taken to aggregate predictions for the perturbed inputs using the base MalConv model. All times are recorded on a desktop PC fitted with an AMD Ryzen 7 5800X CPU and an NVIDIA RTX3090 GPU, using our PyTorch implementation of RS-Del and RS-Abn. We execute training and prediction for the base model on the GPU, and the smoothing mechanism on the CPU. We use a single PyTorch process, noting that times may be improved by running the smoothing mechanism in parallel or on the GPU.

We now make some observations about the results. First, we note that training is an order of magnitude faster for RS-Del compared with RS-Abn. We attribute this speed-up to the deletion smoothing mechanism of RS-Del, which drastically reduces the dimensionality of inputs, thereby reducing the time taken to perform forward and backward passes for the base model. On the contrary, the ablation smoothing mechanism of RS-Abn does not alter the dimensionality of inputs, so it does not have a performance advantage in this respect. Second, we observe that the total prediction time for RS-Del is approximately 150% faster than for RS-Abn. We expect this difference is also due to the effect of dimensionality reduction for the deletion smoothing mechanism.

Training efficiencyTraining curves for the base MalConv models used in RS-Del and RS-Abn are provided in Figure 7 for the Sleipnir2 dataset. Due to convergence issues for RS-Abn, we adapted training to incorporate gradient clipping when updating the embedding layer. This addresses imbalance in the gradients arising from the dominance of masked (ablated) values in the perturbed inputs. However, even with this fix, we observe slower convergence to a higher loss value for RS-Abn than for RS-Del. Combining the results of Table 12 and Figure 7, we conclude that RS-Abn beats RS-Abn in terms of training efficiency as it requires both fewer epochs to converge and takes less time per epoch.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline \multirow{3}{*}{Setting} & \multirow{3}{*}{Attack} & \multicolumn{3}{c}{Attack success rate (\%)} \\ \cline{3-5}  & & & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \cline{3-5}  & & & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \cline{3-5}  & & & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline \multirow{3}{*}{White-box} & \multirow{3}{*}{_Disp_[17]} & Sleipnir2 & \(73.8\) & \(0.414\) \\  & & VTFeed & \(94.1\) & \(0.0\) \\ \cline{2-5}  & & Sleipnir2 & \(57.9\) & \(2.90\) \\  & & VTFeed & \(96.0\) & \(1.01\) \\ \hline \multirow{3}{*}{Black-box} & \multirow{3}{*}{_HDOS_[37]} & Sleipnir2 & \(0.0\) & \(0.0\) \\  & & VTFeed & \(0.0\) & \(0.0\) \\ \cline{2-5}  & & Sleipnir2 & \(0.607\) & \(0.0\) \\  & & VTFeed & \(0.990\) & \(0.0\) \\ \cline{2-5}  & & Sleipnir2 & \(0.607\) & \(0.0\) \\  & & VTFeed & \(10.9\) & \(0.0\) \\ \cline{2-5}  & & Sleipnir2 & \(99.2\) & \(99.6\) \\  & & VTFeed & \(76.2\) & \(100.0\) \\ \hline \hline \end{tabular}
\end{table}
Table 11: Success rates of attacks transferred from NS to RS-Del.

## Appendix H Adapting attacks for smoothed classifiers

In this appendix, we show how to adapt gradient-guided white-box attacks to account for randomized smoothing.

We consider a generic family of white-box attacks that operate on neural network-based classifiers, where the first layer of the network is an embedding layer. Mathematically, we assume the classifier under attack \(f\) can be decomposed as

\[f=f_{\text{embed}}\circ f_{\text{soft}}\circ f_{\text{pred}} \tag{23}\]

where

* \(f_{\text{embed}}\) is the embedding layer, which maps an input sequence \(\mathbf{x}\in\mathcal{X}\) of length \(n=|\mathbf{x}|\) to an \(n\times d\) array of \(d\)-dimensional embedding vectors;
* \(f_{\text{soft}}\) represents the subsequent layers in the network, which map an \(n\times d\) embedding array to a probability distribution over classes \(\mathcal{Y}\); and
* \(f_{\text{pred}}\) is an optional final layer, which maps a probability distribution over classes to a prediction (e.g., by taking the \(\arg\max\) or applying a threshold).

\begin{table}
\begin{tabular}{l l c c c} \hline \hline  & & \multicolumn{3}{c}{Wall time (s)} \\ \cline{3-5}  & & & \multicolumn{2}{c}{Predict} \\ \cline{3-5} Model & Parameters & Train 1 epoch & Smoothing & Base model \\ \hline RS-Del & \(p_{\text{del}}=90\%\) & \(354\) & \(10.42\) & \(0.070\) \\ RS-Abn [24] & \(p_{\text{ab}}=90\%\) & \(1692\) & \(15.29\) & \(0.352\) \\ \hline RS-Del & \(p_{\text{del}}=99\%\) & \(329\) & \(8.79\) & \(0.043\) \\ RS-Abn [24] & \(p_{\text{ab}}=99\%\) & \(1788\) & \(15.60\) & \(0.352\) \\ \hline \hline \end{tabular}
\end{table}
Table 12: Comparison of runtime efficiency for two models: RS-Del (our method with byte-level deletion) and RS-Abn [24]. The first column of wall times measures the time taken to train each model for one epoch on Sleipnir2. The second and third columns of wall times measure the time taken to make a prediction for a 1MB input file. This is split into two components: the time taken to generate \(n_{\text{pred}}=1000\) perturbed inputs from the smoothing mechanism (second column) and the time taken to pass the perturbed inputs through the base model (third column).

Figure 7: Training curves for RS-Del (our method with byte-level deletion) and RS-Abn [24] for the Sleipnir2 dataset.

The attack may query any of the above components in isolation and compute gradients of \(f_{\mathrm{soft}}\). For instance, the attacks we consider [20, 37, 17] optimize the input in the embedding space \(\mathbf{e}\in\mathbb{R}^{n\times d}\), by computing the gradient \(\frac{\partial f_{\mathrm{soft}}(\mathbf{e})}{\partial\mathbf{e}}\).

While the above setup covers attacks against MalConv, or classifiers with similar architectures, it is not directly compatible with smoothed classifiers. There are two incompatibilities. First, a direct implementation of a smoothed classifier (as described in Section 3.1 and Figure 1) does not decompose like (23). And second, the aggregation of hard predictions from the base classifiers is non-differentiable. We explain how to address these incompatibilities below.

Leveraging commutativityConsider a smoothed classifier composed from a base classifier \(h\) and smoothing mechanism \(\phi\), and suppose the base classifier decomposes as in (23). Following Section 3.1 and Figure 1, the smoothed classifier's confidence score for class \(y\) can be expressed as

\[p_{y}(\mathbf{x})=\mathrm{smooth}_{N}(\mathbf{x};\phi,h_{\mathrm{embed}}\circ h_{ \mathrm{soft}}\circ h_{\mathrm{pred}}\circ\mathbf{1}_{\square=y}) \tag{24}\]

where

\[\mathrm{smooth}_{N}(\mathbf{x};\phi,f)=\frac{1}{N}\sum_{i=1}^{N}f(\mathbf{z}_{i}), \quad\text{with }\mathbf{z}_{i}\sim\phi(\mathbf{x})\]

is the (empirical) smoothing operation. This expression does not immediately decompose like (23), because the embedding layer is applied to the perturbed input \(\phi(\mathbf{x})\), not \(\mathbf{x}\). Fortunately, we can manipulate the expression into the desired form by swapping the order of \(\phi\) and \(h_{\mathrm{embed}}\). To do so, we extend the definition of \(\phi\) to operate on an embedding array so that \(h_{\mathrm{embed}}(\phi(\mathbf{x}))=\phi(h_{\mathrm{embed}}(\mathbf{x}))\), i.e., \(\phi\) and \(h_{\mathrm{embed}}\) commute. In particular, this can be done for randomized deletion (RS-Del) by applying the deletion edits to embedding vectors along the first dimension. Then (24) can equivalently be expressed as

\[p_{y}(\mathbf{x})=\underbrace{h_{\mathrm{embed}}}_{f_{\mathrm{embed}}}\circ \underbrace{\mathrm{smooth}_{N}(\phi,h_{\mathrm{soft}}\circ h_{\mathrm{pred}} \circ\mathbf{1}_{\square=y})}_{f_{\mathrm{soft}}}(\mathbf{x}). \tag{25}\]

Soft aggregationWhile (25) decomposes as required, the \(f_{\mathrm{soft}}\) component is not differentiable. This is due to the presence of \(h_{\mathrm{pred}}\), which is an \(\arg\max\) layer. To proceed, we replace the aggregation of predictions by the aggregation of softmax scores as proposed by Salman et al. [94]. This yields a differentiable approximation of the smoothed classifier:

\[p_{y}(\mathbf{x})\approx\underbrace{h_{\mathrm{embed}}}_{f_{\mathrm{embed}}} \circ\underbrace{\mathrm{smooth}_{N}(\phi,h_{\mathrm{soft}}\circ\square_{y}) }_{f_{\mathrm{soft}}}(\mathbf{x}).\]

Salman et al. note that this approximation performs well, and is empirically more effective than an alternative approach proposed by Cohen et al. [14, Appendix G.3].

## Appendix I Review of randomized ablation

In this appendix, we review _randomized ablation_[24], which serves as a baseline for the Hamming distance threat model in our experiments. It is based on _randomized smoothing_ (see Section 3.1) like our method, however the smoothing mechanism and robustness certificate differ. In this review, we formulate randomized ablation for sequence classifiers; we refer readers to Levine and Feizi [24] for a formulation for image classifiers.

### Ablation smoothing mechanism

Randomized ablation employs a smoothing mechanism that replaces a random subset of the input elements with a special null value NA. Levine and Feizi use an encoding for the null value tailored for images, that involves doubling the number of channels. This encoding is not suitable for discrete sequences, so we instead augment the sequence domain \(\Omega\) with a special null value: \(\Omega\rightarrow\Omega\cup\{\texttt{NA}\}\). The hyperparameter controlling the strength of ablation must also be adapted for our setting. Levine and Feizi use a hyperparameter \(k\) that corresponds to the number of elements _retained_ in the output. This is ineffective for inputs that vary in length, so we scale \(k\) in proportion with the input length. Specifically, we introduce an alternative hyperparameter \(p_{\texttt{ab}}\in(0,1)\) that represents the fraction of ablated elements and set \(k(|\mathbf{x}|)=\lceil(1-p_{\texttt{ab}})|\mathbf{x}|\rceil\).

Mathematically, the ablation mechanism has the following distribution when applied to an input sequence \(\mathbf{x}\):

\[\Pr[\phi(\mathbf{x})=\mathbf{z}]=\sum_{\epsilon\in\mathcal{E}_{k(|\mathbf{x}|)}(\mathbf{x})}\frac{ 1}{\binom{|\mathbf{x}|}{k(|\mathbf{x}|)}}\mathbf{1}_{\mathrm{ablate}(\mathbf{x},\epsilon)= \mathbf{z}},\]

where \(\mathcal{E}_{k}=\{\epsilon\in\mathcal{E}(\mathbf{x}):|\epsilon|=k\}\) consists of all sets of element indices of size \(k\) and

\[z_{i}=\mathrm{ablate}(\mathbf{x},\epsilon)_{i}=\begin{cases}x_{i},&\text{if $i\in \epsilon$ ``retained''},\\ \mathtt{NA},&\text{if $i\notin\epsilon$ ``ablated''}.\end{cases}\]

_Remark 10_.: Hyperparameter \(p_{\mathtt{ab}}\) has a similar interpretation as \(p_{\mathtt{del}}\) for randomized deletion, in that both hyperparameters control the proportion of sequence elements hidden (by ablation or deletion) from the base classifier.

### Hamming distance robustness certificate

Randomized ablation provides a Hamming distance (\(\ell_{0}\)) certificate that guarantees robustness under a bounded number of arbitrary substitutions. Levine and Feizi provide two certificates: one that makes use of the confidence score for the predicted class, and another that makes use of the top two confidence scores. We present the first certificate here, since it matches the certificate we consider in Section 4 and it is the simplest choice for binary classifiers (the focus of our experiments).

To facilitate comparison with randomized deletion, we reuse notation and definitions from Section 4. Recall from (15) that \(\tilde{\rho}(\mathbf{x};\mu_{y})\) is a lower bound on the smoothed classifier's confidence score \(p_{y}(\overline{\mathbf{x}};h)\) that holds for any input \(\widetilde{\mathbf{x}}\) in the edit distance ball of radius \(r\) centered on \(\mathbf{x}\) and any base classifier \(h\) such that \(p_{y}(\mathbf{x};h)=\mu_{y}\). For randomized ablation, we can replace the edit distance ball with a Hamming distance ball and obtain the following result (see [24] for the derivation):

\[\tilde{\rho}(\mathbf{x};\mu_{y})=\mu_{y}-1+\frac{\binom{|\mathbf{x}|-r}{k(|\mathbf{x}|)}}{ \binom{|\mathbf{x}|}{k(|\mathbf{x}|)}}.\]

Recall from Proposition 3, that the smoothed classifier is certifiably robust if \(\tilde{\rho}(\mathbf{x};\mu_{y})\geq\nu_{y}(\mathbf{\eta})\), where \(\mathbf{\eta}\) denotes the smoothed classifier's tunable decision thresholds. Hence the certified radius \(r^{\star}\) is the maximum value of \(r\in\{0,1,2,\ldots\}\) that satisfies

\[\mu_{y}-1+\frac{\binom{|\mathbf{x}|-r}{k(|\mathbf{x}|)}}{\binom{|\mathbf{x}|}{k(|\mathbf{x}|) }}-\nu_{y}(\mathbf{\eta})\geq 0.\]

This maximization problem does not have an analytic solution, however it can be solved efficiently using binary search since the LHS of the above inequality is a non-increasing function of \(r\). In practice, the exact confidence score \(\mu_{y}\) can be replaced with a \(1-\alpha\) lower confidence bound \(\underline{\mu}_{y}\) to yield a probabilistic certificate that holds with probability \(1-\alpha\) (see procedure in Figure 1 and Corollary 8).

Since randomized ablation and randomized deletion both admit Hamming distance certificates, it is interesting to compare them. The following result, first proved by Scholten et al. [96, Proposition 5] for a related mechanism, shows that randomized deletion admits a tighter certificate.

**Proposition 11**.: _Consider a randomized deletion classifier (\(\mathsf{RS}\)-\(\mathsf{Del}\)) with deletion probability \(p_{\mathtt{del}}=p\) and a randomized ablation classifier (\(\mathsf{RS}\)-\(\mathsf{Abn}\)) with ablation fraction \(p_{\mathtt{ab}}=p\). Suppose both classifiers make the same prediction \(y\) with the same confidence score \(\mu_{y}\) for input \(\mathbf{x}\) and that we are interested in issuing a Hamming distance certificate of radius \(r\). Then the lower bound on the confidence score over the certified region is tighter for \(\mathsf{RS}\)-\(\mathsf{Del}\) than for \(\mathsf{RS}\)-\(\mathsf{Abn}\)._Proof.: From Theorem 7 we have

\[\tilde{\rho}_{\mathsf{RS\text{-}Del}}(\mathbf{x};\mu_{y}) =\mu_{y}-1+p^{r}\] \[=\mu_{y}-1+\left(\frac{|\mathbf{x}|-(1-p)|\mathbf{x}|}{|\mathbf{x}|}\right)^{r}\] \[\geq\mu_{y}-1+\left(\frac{|\mathbf{x}|-\lceil(1-p)|\mathbf{x}|\rceil}{| \mathbf{x}|}\right)^{r}\] \[=\mu_{y}-1+\prod_{j=1}^{r}\frac{|\mathbf{x}|-k(|\mathbf{x}|)}{|\mathbf{x}|}\] \[\geq\mu_{y}-1+\prod_{j=1}^{r}\frac{(|\mathbf{x}|-k(|\mathbf{x}|)-j+1)}{(| \mathbf{x}|-j+1)}\] \[=\mu_{y}-1+\frac{\binom{|\mathbf{x}|-p}{k(|\mathbf{x}|)}}{\binom{|\mathbf{x}| }{k(|\mathbf{x}|)}}\] \[=\tilde{\rho}_{\mathsf{RS\text{-}Abn}}(\mathbf{x};\mu_{y})\]

_Remark 12_.: Jia et al. [25] extend randomized ablation to top-\(k\) prediction, while at the same time proposing enhancements to the Hamming distance certificate. These enhancements also apply to regular classification and involve: (1) discretizing lower/upper bounds on the confidence scores; and (2) using a better statistical method to estimate lower/upper bounds on the confidence scores. However, both of these enhancements would have a negligible impact in our experiments, as we shall now explain. The first enhancement tightens lower/upper bounds on the confidence scores by rounding up/down to the nearest integer multiple of \(q:-1/\binom{|\mathbf{x}|}{k(|\mathbf{x}|)}\). This improves the lower/upper bound by at most

\[q\leq\min\left\{\left(\frac{|\mathbf{x}|-k}{|\mathbf{x}|}\right)^{|\mathbf{x}|-k},\left( \frac{k}{|\mathbf{x}|}\right)^{k}\right\}\leq\frac{1}{|\mathbf{x}|}\]

if the number of retained elements satisfies \(1\leq k\leq|\mathbf{x}|\), as is the case in our experiments. However, since we consider inputs of length \(|\mathbf{x}|\geq 10^{3}\), this means the improvement in the bound due to discretization is at most \(q\leq 10^{-5}\). This improvement is comparable to the resolution of our estimator (\(1/n_{\mathrm{bnd}}\sim 10^{-4}\)), and consequently, discretization will not have a discernible impact. The second enhancement involves simultaneously estimating lower/upper bounds using a statistical method called SimuEM [97]. SimuEM has been demonstrated to improve tightness when there are multiple classes [25], however it has no impact when there are two classes (as is the case in our experiments).