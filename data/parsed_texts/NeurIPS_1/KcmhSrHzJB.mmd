[MISSING_PAGE_FAIL:1]

* Clear statistical semantics for a trained DNN model, beyond merely providing the conditional distribution over output variables given input variables. Instead, PGMs provide a joint distribution over all variables including the latent variables.
* Ability to import any PGM algorithm into DNNs, such as belief propagation or MCMC, for example to _reverse any_ DNN to use output variables as evidence and variables anywhere earlier as query variables.
* Improved calibration, i.e., improved predictions of probabilities at the output nodes by incorporation of PGM algorithms.

In this paper, we establish such a correspondence between DNNs of any structure and PGMs. Given an arbitrary DNN architecture, we first construct an infinite-width tree-structured PGM. We then demonstrate that during training, the DNN executes approximations of precise inference in the PGM during the forward propagation step. We prove our result exactly in the case of sigmoid activations. We indicate how it can be extended to ReLU activations by building on a prior result of Nair and Hinton (2010). Because the PGM in our result is a Markov network, the construction can extend even further, to all nonnegative activation functions provided that proper normalization is employed. We argue that modified variants of layer normalization and batch normalization could be viewed as approximations to proper MN normalization in this context, although formal analyses of such approximations are left for future work. Finally, in the context of sigmoid activations we empirically evaluate how the second and third benefits listed above follow from the result, as motivated and summarized now in the next paragraph.

A neural network of any architecture with only sigmoid activations satisfies the definition of a Bayesian network over binary variables--it is a directed acyclic graph with a conditional probability distribution at each node, conditional on the values of the node's parents--and thus is sometimes called a Bayesian belief network (BBN). Nevertheless, it can be shown that the standard gradient used in neural network training (whether using cross-entropy or other common error functions) is inconsistent with the BBN semantics, that is, with the probability distribution defined by this BBN. On the other hand, Gibbs sampling is a training method consistent with the BBN semantics, and hence effective for calibration and for reversing any neural network, but it is worth inefficient for training. Hamiltonian Monte Carlo (HMC) is more efficient than Gibbs and better fits BBN semantics than SGD. We demonstrate empirically that after training a network quickly using SGD, calibration can be improved by fine-tuning using HMC. The specific HMC algorithm employed here follows directly from the theoretical result, being designed to approximate Gibbs-sampling in the theoretical, infinite-width tree structured Markov network. The degree of approximation is controlled by the value of a single hyperparameter that is also defined based on the theoretical result.

The present paper stands apart from many other theoretical analyses of DNNs that view DNNs purely as _function approximators_ and prove theorems about the quality of function approximation. Here we instead show that DNNs may be viewed as statistical models, specifically PGMs. This work is also different from the field of _Bayesian neural networks_, where the goal is to seek and model a probability distribution over neural network parameters. In our work, the neural network itself defines a joint probability distribution over its variables (nodes). Our work therefore is synergistic with Bayesian neural networks but more closely related to older work on learning stochastic neural networks via expectation maximization (EM) (Amari, 1995) or approximate EM (Song et al., 2016).

Although the approach is different, our motivation is similar to that of Dutordoir et al. (2021) and Sun et al. (2020) in their work to link DNNs to deep Gaussian processes (GPs) (Damianou and Lawrence, 2013). By identifying the forward pass of a DNN with the mean of a deep GP layer, they aim to augment DNNs with advantages of GPs, notably the ability to quantify uncertainty over both output and latent nodes. What distinguishes our work from theirs is that we make the DNN-PGM approximation explicit and include _all_ sigmoid DNNs, not just unsupervised belief networks or other specific cases.

All code needed to reproduce our experimental results may be found at [https://github.com/engelhard-lab/DNN_TreePGM](https://github.com/engelhard-lab/DNN_TreePGM).

Background: Comparison to Bayesian Networks and Markov Networks

Syntactically a Bayesian network (BN) is a directed acyclic graph, like a neural network, whose nodes are random variables. Here we use capital letters to stand for random variables, and following Russell and Norvig (Russell and Norvig, 2020) and others, we take a statement written using such variables to be a claim for all specific settings of those variables. Semantically, a BN represents a full joint probability distribution over its variables as \(P(\vec{V})=\prod_{i}P(V_{i}|pa(V_{i}))\), where \(pa(V_{i})\) denotes the parents of variable \(V_{i}\). If the conditional probability distributions (CPDs) \(P(V_{i}|pa(V_{i}))\) are all logistic regression models, we refer to the network as a sigmoid BN.

It is well known that given sigmoid activation and a cross-entropy error, training a single neuron by gradient descent is identical to training a logistic regression model. Hence, a neural network under such conditions can be viewed as a "stacked logistic regression model", and also as a Bayesian network with logistic regression CPDs at the nodes. Technically, the sigmoid BN has a distribution over the input variables (variables without parents), whereas the neural network does not, and all nodes are treated as random variables. These distributions are easily added, and distributions of the input variables can be viewed as represented by the joint sample over them in our training set.

A Markov network (MN) syntactically is an undirected graph with potentials \(\phi_{i}\) on its cliques, where each potential gives the relative probabilities of the various settings for its variables (the variables in the clique). Semantically, it defines the full joint distribution on the variables as \(P(\vec{V})=\frac{1}{Z}\prod_{i}\phi_{i}(\vec{V})\) where the partition function \(Z\) is defined as \(\sum_{\vec{V}}\prod_{i}\phi_{i}(\vec{V})\). It is common to use a loglinear form of the same MN, which can be obtained by treating a setting of the variables in a clique as a binary feature \(f_{i}\), and the natural log of the corresponding entity for that setting in the potential for that clique as a weight \(w_{i}\) on that feature; the equivalent definition of the full joint is then \(P(\vec{V})=\frac{1}{Z}e^{\sum_{i}w_{i}f_{i}(\vec{V})}\). For training and prediction at this point the original graph itself is superfluous.

The potentials of an MN may be on subsets of cliques; in that case we simply multiply all potentials on subsets of a clique to derive the potential on the clique itself. If the MN can be expressed entirely as potentials on edges or individual nodes, we call it a "pairwise" MN. An MN whose variables are all binary is a binary MN.

A DNN of any architecture is, like a Bayesian network, a directed acyclic graph. A sigmoid activation can be understood as a logistic model, thus giving a conditional probability distribution for a binary variable given its parents. Thus, there is a natural interpretation of a DNN with sigmoid activations as a Bayesian network (e.g., Bayesian belief network). Note, however, that when the DNN has multiple, stacked hidden nodes, the values calculated for those nodes in the DNN by its forward pass do not match the values of the corresponding hidden nodes in a Bayesian network. Instead, for the remainder of this paper, we adopt the view that the DNN's forward pass might serve as an approximation to an underlying PGM and explore how said approximation can be precisely characterized. As reviewed in Appendix A, this Bayes net in turn is equivalent to (represents the same probability distribution) as a Markov network where every edge of weight \(w\) from variable \(A\) to variable \(B\) has a potential of the following form:

\begin{tabular}{|c|c|c|} \hline  & \(B\) & \(\neg B\) \\ \hline \(A\) & \(e^{w}\) & 1 \\ \hline \(\neg A\) & 1 & 1 \\ \hline \end{tabular}

For space reasons, we assume the reader is already familiar with the Variable Elimination (VE) algorithm for computing the probability distribution over any query variable(s) given evidence (known values) at other variables in the network. This algorithm is identical for Bayes nets and Markov nets. It repeatedly multiplies together all the potentials (in a Bayes net, conditional probability distributions) involving the variable to be eliminated, and then sums that variable out of the resulting table, until only the query variable(s) remain. Normalization of the resulting table yields the final answer. VE is an exact inference algorithm, meaning its answers are exactly correct.

The Construction of Tree-structured PGMs

Although both a binary pairwise Markov network (MN) and a Bayesian network (BN) share the same sigmoid functional structure as a DNN with sigmoid activations, it can be shown that the DNN does not in general define the same probability for the output variables given the input variables: forward propagation in the DNN is very fast but yields a different result than VE in the MN or BN, which can be much slower because the inference task is NP-complete. Therefore, if we take the distribution \(\mathcal{D}\) defined by the BN or MN to be the correct meaning of the DNN, the DNN must be using an approximation \(\mathcal{D}^{\prime}\) to \(\mathcal{D}\). Procedurally, the approximation can be shown to be exactly the following: the DNN repeatedly treats the _expectation_ of a variable \(V\), given the values of \(V\)'s parents, as if it were the actual _value_ of \(V\). Thus previously binary variables in the Bayesian network view and binary features in the Markov network view become continuous. This procedural characterization of the approximation of \(\mathcal{D}^{\prime}\) to \(\mathcal{D}\) yields exactly the forward pass in the neural network within the space of a similarly structured PGM, yet, on its own, does not yield a precise joint distribution for said PGM. We instead prefer in the PGM literature to characterize approximate distributions such as \(\mathcal{D}^{\prime}\) with an alternative PGM that precisely corresponds to \(\mathcal{D}^{\prime}\); for example, in some variational methods we may remove edges from a PGM to obtain a simpler PGM in which inference is more efficient. Treewidth-1 (tree-structured or forest-structured) PGMs are among the most desirable because in those models, exact inference by VE or other algorithms becomes efficient. We seek to so characterize the DNN approximation here.

This approach aligns somewhat with the idea of the computation tree that has been used to explore the properties of belief propagation by expressing the relevant message passing operations in the form of a tree (Tatikonda and Jordan, 2002; Ihler et al., 2005; Weitz, 2006). Naturally the design of the tree structured PGM proposed here differs from the computation trees for belief propagation as we instead aim to capture the behavior of the forward pass of the neural network. Nonetheless, both methods share similar general approaches, the construction of a simpler approximate PGM, and aims, to better understand the theoretical behavior of an approximation to a separate original PGM.

To begin, we consider the Bayesian network view of the DNN. Our first step in this construction is to copy the shared parents in the network into separate nodes whose values are not tied. The algorithm for this step is as follows:

1. Consider the observed nodes in the Bayesian network that correspond to the input of the neural network and their outgoing edges.
2. At each node, for each outgoing edge, create a copy of the current node that is only connected to one of the original node's children with that edge. Since these nodes are observed at this step, these copies do all share the same values. The weights on these edges remain the same.
3. Consider then the children of these nodes. Again, for each outgoing edge, make a copy of this node that is only connected to one child with that edge. In this step, for each copied node, we then also copy the entire subgraph formed by all ancestor nodes of the current node. Note that while weights across copies are tied, the values of the copies of any node are not tied. However, since we also copy the subtree of all input and intermediary hidden nodes relevant to a forward pass up to each copy, the probability of any of these copied nodes being true remains the same across copies (ignoring the influence of any information passed back from their children).
4. We repeat this process across each layer until we have separate trees for each output node in the original deep neural network graph.

This process ultimately creates a graph whose undirected structure is a tree or forest. In the directed structure, trees converge at the output nodes. The probability of any copy of a latent node given the observed input (and ignoring any information passed back through a node's descendant) is the same across all the copies, but when sampling, their values may not be.

The preceding step alone is still not sufficient to accurately express the deep neural network as a PGM. Recall that in the probabilistic graphical model view of the approximation made by the DNN's forward pass, the neural network effectively takes a local average, in place of its actual value, from the immediately previous nodes and passes that information only forward. The following additional step in the construction yields this same behavior. This next step of the construction creates \(L\) copies of every non-output node in the network (starting at the output and moving backward) while also copying the entire ancestor subtrees of each of these nodes, as was done in step 1. The weight of a copied edges is then set to its original value divided by \(L\). Note that this step results in a number of total copies that grows exponentially in the number of layers (i.e. \(L\) copies in the 2nd to last layer, \(L^{2}\) copies in the layer before, etc). Detailed algorithms for the two steps in the construction of the infinite tree-structured PGM are presented in Appendix B. As \(L\) approaches infinity, we show that both inference and the gradient in this PGM construction matches the forward pass and gradient in the neural network exactly.

This second step in the construction can be thought of intuitively by considering the behavior of sampling in the Bayesian network view. Since we make \(L\) copies of each node while also copying the subgraph of its ancestors, these copied nodes all share the same probabilities. As \(L\) grows large, even if we sampled every copied node only once, we would expect the average value across these \(L\) copies to match the probability of an individual copied node being true. Given that we set the new weights between these copies and their parents as the original weights divided by \(L\), the sum of products (new weights times parent values) yields the average parent value multiplied by the original weight. As \(L\) goes to infinity, we remove sampling bias and the result exactly matches the value of the sigmoid activation function of the neural network, where this expectation in the PGM view is passed repeatedly to the subsequent neurons. The formal proof of this result, based on variable elimination, is found in Appendix C. There, we show the following:

**Theorem 3.1** (Matching Probabilities).: _In the PGM construction, as \(L\rightarrow\infty\), \(P(H=1|\vec{x})\rightarrow\sigma(\sum_{j=1}^{M}w_{j}g_{j}+\sum_{i}^{N}\theta_{ i}\sigma(p_{i}))\), for an arbitrary latent node \(H\) in the DNN that has observed parents \(g_{1},...,g_{M}\) and latent parents \(h_{1},...,h_{N}\) that are true with probabilities \(\sigma(p_{1}),...,\sigma(p_{N})\). Here, \(\sigma(\cdot)\) is the logistic sigmoid function and \(w_{1},...,w_{M}\) and \(\theta_{1},...,\theta_{N}\) are the weights on edges between these nodes and \(H\)._

The PGM of our construction is a Markov network that always has evidence at the nodes \(\vec{X}\) corresponding to the input nodes of the neural network. As such, it is more specifically a conditional random field (CRF). Theorem 1 states the probability that a given node anywhere in the CRF is true given \(\vec{X}\) equals the output of that same node in the neural network given input \(\vec{X}\). The CRF may also have evidence at the nodes \(\vec{Y}\) that correspond to the output nodes of the neural network. Given the form of its potentials, as illustrated at the end of Section 2, the features in the CRF's loglinear form correspond exactly to the edges and are true if and only if the nodes on each end of the edge are true. It follows that the gradient of this CRF can be written as a vector with one entry for each feature \(f\) corresponding to each weight \(w\) of the neural network, of the form \(P(f|\vec{X})-P(f|\vec{X},\vec{Y})\). Building on Theorem 1, this gradient of the CRF can be shown to be identical to the gradient of the cross-entropy loss in the neural network: the partial derivative of the cross-entropy loss with respect

Figure 1: The first step of the PGM construction where shared latent parents are separated into copies along with the subtree of their ancestors. Copies of nodes H1 and H2 are made in this example.

to the weight \(w\) on an edge, or feature \(f\) of the CRF, is \(P(f|\vec{X})-P(f|\vec{X},\vec{Y})\). This result is more precisely stated below in Theorem 2 below, which is proven in Appendix D.

**Theorem 3.2** (Matching Gradients).: _In the PGM construction, as \(L\rightarrow\infty\), the derivative of the marginal log-likelihood, where all hidden nodes have been summed out, with respect to a given weight exactly matches the derivative of the cross entropy loss in the neural network with respect to the equivalent weight in its structure._

## 4 Implications and Extensions

We are not claiming that one should actually carry out the PGM construction used in the preceding section, since that PGM is infinite. Rather, its contribution is to give precise semantics to an entire neural network, as a joint probability distribution over all its variables, not merely as a machine computing the probability of its output variables given the input variables. Any Markov network, including any CRF, precisely defines a joint probability distribution over all its variables, hidden or observed, in a standard, well-known fashion. The particular CRF we constructed is the _right_ one in the very specific sense that it agrees with the neural network exactly in the gradients both use for training (Theorem 2). While the CRF is infinite, it is built using the original neural network as a template in a straightforward fashion and is tree-structured, and hence it is easy to understand. Beyond these contributions to pedagogy and comprehensibility, are there other applications of the theoretical results?

One application is an ability to use standard PGM algorithms such as Markov chain Monte Carlo (MCMC) to sample latent variables given observed values of input and output variables, such as for producing confidence intervals or understanding relationships among variables. One could already do so using Gibbs sampling in the BN or MN directly represented by the DNN itself (which we will call the "direct PGM"), but then one wouldn't be using the BN or MN with respect to which SGD training in the DNN is correct. For that, our result has shown that one instead needs to use Gibbs sampling in the infinite tree-structured PGM, which is impractical. Nevertheless, for any variable \(V\) in the original DNN, on each iteration a Gibbs sampler takes infinitely many samples of \(V\) given infinitely many samples of each of the members of \(V\)'s Markov blanket in the original DNN. By treating the variables of the original DNN as continuous, with their values approximating their sampled probabilities in the Gibbs sampler, we can instead apply Hamiltonian Monte Carlo or other MCMC methods for continuous variables in the much smaller DNN structure. We explore this approach empirically rather than theoretically in the next section. Another, related application of our result is that one could further fine-tune the trained DNN using other PGM algorithms, such as contrastive divergence. We also explore this use in the next section.

One might object that most results in this paper use sigmoid activation functions. Nair and Hinton showed that rectified linear units (ReLU) might be thought of as a combination of infinitely many sigmoid units with varying biases (Nair and Hinton, 2010). Hence our result in the previous section can be extended to ReLU activations by the same argument. More generally, with any non-negative activation function that can yield values greater than one, while our BN argument no longer holds, the MN version of the argument can be extended. An MN already requires normalization to represent a probability distribution. While Batch Normalization and Layer Normalization typically are motivated procedurally, to keep nodes from "saturating," and consequently to keep gradients from "exploding" or "vanishing," as the names suggest, they might also be used to bring variables into the range \([0,1]\) and hence to being considered as probabilities. Consider an idealized variant of these that begins by normalizing all the values coming from a node \(h\) of a neural network, over a given minibatch, to sum to \(1.0\); the argument can be extended to a set of \(h\) and all its siblings in a layer (or other portion of the network structure) assumed to share their properties. It is easily shown that if the parents of any node \(h\) in the neural network provide to \(h\) approximate probabilities that those parent variables are true in the distribution defined by the Markov network given the inputs, then \(h\) in turn provides to its children an approximate probability that \(h\) is true in the distribution defined by the Markov network given the inputs. Use of a modified Batch or Layer Normalization still would be only approximate and hence adds an additional source of approximation to the result of the preceding section. Detailed consideration of other activation functions is left for further work; in the next section we return to the sigmoid case.

Application of the Theory: A New Hamiltonian Monte Carlo Algorithm

To illustrate the potential utility of the infinite tree-structured PGM view of a DNN, in this section we pursue one of its implications in greater depth; other implications for further study are summarized in the Conclusion. We have already noted we can view forward propagation in an all-sigmoid DNN as exact inference in a tree-structured PGM, such that the CPD of each hidden variable is a logistic regression. In other words, each hidden node is a Bernoulli random variable, with parameter \(\lambda\) being a sigmoid activation (i.e. logistic function) applied to a linear function of the parent nodes. This view suggests alternative learning or fine-tuning algorithms such as contrastive divergence (CD) (Carreira-Perpinan and Hinton, 2005; Bengio and Delalleau, 2009; Sutskever and Tieleman, 2010). CD in a CRF uses MCMC inference with many MCMC chains to estimate the joint probability over the hidden variables given the evidence (the input and output variables in a standard DNN), and then takes a gradient step based on the results of this inference. But to increase speed, CD-\(n\) advances each MCMC chain only \(n\) steps before the next gradient step, with CD-1 often being employed. CD has a natural advantage over SGD, which samples the hidden variable values using only evidence in input values; instead, MCMC in CD uses _all_ the available evidence, both at input and output variables. Unfortunately, if the MCMC algorithm employed is Gibbs sampling on the many hidden variables found in a typical neural network, then it suffers from high cost in computational resources. MCMC has now advanced far beyond Gibbs sampling with methods such as Hamiltonian Monte Carlo (HMC), but HMC samples values in \([0,1]\) rather than \(\{0,1\}\). Neal (2012) first applied HMC to neural nets to sample the weights in a Bayesian approach, but still used Gibbs sampling on the hidden variables. Our theoretical results for the first time justify the use of HMC over the hidden variables rather than Gibbs sampling in a DNN, as follows.

Recall that the DNN is itself a BN with sigmoid CPDs, but if we take the values of the hidden variables to be binary then DNN training is not correct with respect to this BN. Instead, based on the correctness of our infinite tree-structured PGM, the probabilistic behavior of one hidden node in the BN is the result of sampling values across its \(L\) copies in the PGM. Within any copy, the value of the hidden node follows the Bernoulli distribution with the same probability distribution as the other copies, determined by the parent nodes. Since all the copies share the same parent nodes by the construction and are sampled independently, the sample average follows a normal distribution as the asymptotic distribution when \(L\rightarrow\infty\) by the central limit theorem. In practice, \(L\) is finite and this normal distribution is a reasonable approximation to the distribution of the hidden node. Thus in the BN, whose variables correspond exactly to those of the DNN, the variables have domain [0,1] rather than \(\{0,1\}\), as desired. We next precisely define this BN and the resulting HMC algorithm.

### Learning via Contrastive Divergence with Hamiltonian Monte Carlo Sampling

Consider a Bayesian network composed of input variables \(\mathbf{x}=\mathbf{h}_{0}\), a sequence of layers of hidden variables \(\mathbf{h}_{1},...,\mathbf{h}_{K}\), and output variables \(\mathbf{y}\). Each pair of consecutive layers forms a bipartite subgraph of the network as a whole, and the variables \(\mathbf{h}_{i}=(h_{i1},...,h_{iM_{i}})\) follow a multivariate normal distribution with parameters \(\mathbf{p}_{i}=(p_{i1},...,p_{iM_{i}})\) that depend on variables in the previous layer \(\mathbf{h}_{i-1}\) as follows:

\[h_{ij}\sim\mathcal{N}(p_{ij},p_{ij}(1-p_{ij})/L)\text{, where }\mathbf{p_{i}}= \sigma(\mathbf{W}_{i-1}\mathbf{h}_{i-1}+\mathbf{b}_{i-1}), \tag{1}\]

where \(\sigma:\mathbb{R}\rightarrow(0,1)\) is a non-linearity - here the logistic function - that is applied element-wise, and \(\mathbf{\theta}_{i}=(\mathbf{W}_{i},\mathbf{b}_{i})\) are parameters to be learned. The distribution in equation (1) is motivated by supposing that \(h_{ij}\) is the average of \(L\) copies of the corresponding node in the PGM, each of which is 1 with probability \(p_{ij}\) and zero otherwise, then applying the normal approximation to the binomial distribution. Importantly, this approximation is valid only for large \(L\).

For a complete setting of the variables \(\{\mathbf{x},\mathbf{h},\mathbf{y}\}\), where \(\mathbf{h}=\{\mathbf{h}_{1},...,\mathbf{h}_{K}\}\), and parameters \(\mathbf{\theta}=\{\mathbf{\theta}_{i}\}_{i=0}^{K}\), the likelihood \(p(\mathbf{y},\mathbf{h}|\mathbf{x};\mathbf{\theta})\) may be decomposed as:

\[p(\mathbf{y},\mathbf{h}|\mathbf{x};\mathbf{\theta})=p(\mathbf{y}|\mathbf{h}_{K};\mathbf{\theta}_{K})\cdot \prod_{i=1}^{K}\prod_{j=1}^{M_{i}}p_{\mathcal{N}}(h_{ij}|p_{ij}(\mathbf{h}_{i-1}; \mathbf{\theta}_{i-1})), \tag{2}\]

where \(p_{\mathcal{N}}(\cdot|\cdot)\) denotes the normal density, and a specific form for \(p(\mathbf{y}|\mathbf{h}_{K};\mathbf{\theta}_{K})\) has been omitted to allow variability in the output variables. In our experiments, \(\mathbf{y}\) is a Bernoulli(binary) or categorical random variable parameterized via the logistic(sigmoid) or softmax function, respectively.

[MISSING_PAGE_FAIL:8]

probabilistic distribution \(P(y|\mathbf{X})\) of the corresponding BN/MN is calculated by sampling or applying the VE algorithm on it.

To explore how the proposed algorithm performs in model calibration, a DNN is first trained with SGD for 100 or 1000 epochs, and then fine-tuned by Gibbs or HMC with different \(L\)'s for 20 epochs based on the trained DNN model. Here \(L\) defines the normal distribution for hidden nodes in Eqn. 1 and is explored across the set of values: \(\{10,100,1000\}\). The calibration is assessed by mean absolute error (MAE) in all the synthetic experiments and compared between non-extra fine-tuning (shown in the "DNN" column in Table 1) and fine-tuning with Gibbs or HMC. Since the ground truth of \(P(y|\mathbf{X})\) in the synthetic dataset can be achieved from the BN/MN, the MAE is calculated by comparing the predicted \(P(y|\mathbf{X})\) from the finetuned network and the true probability.

Table 1 shows that in general, DNN results tend to get worse with additional training, particularly with smaller weights, and the HMC-based fine-tuning approaches can mitigate this negative impact of additional training on the model calibration. Across all the HMC with different \(L\)'s, HMC (\(L\)=10) performs better than the others and DNN training itself for BNs and MNs with smaller weights. Additionally, the MAE of HMC (\(L\)=10) tends to be similar to Gibbs but runs much faster, especially in the BN simulations, whereas HMC (\(L\)=1000) is more similar to the NN. This is consistent with what we have argued in the theory that when \(L\) goes smaller, the number of the sampled copies for each hidden node decreases in our tree-PGM construction and HMC sampling performs more similar to Gibbs sampling; and as \(L\) increases, the probability of each hidden node given the input approaches the result of the DNN forward propagation and thus HMC performs more similar to DNN training.

#### 5.2.2 Covertype Experiments

Similar experiments are also run on the Covertype dataset to compare the calibration of SGD in DNNs, Gibbs and the HMC-based algorithm. Since the ground truth for the distribution of \(P(y|\mathbf{X})\) cannot be found, the metric for the calibration used in this experiment is the expected calibration error (ECE), which is a common metric for model calibration. To simplify the classification task, we choose the data with label 1 and 2 and build two binary subsets, each of which contains 1000 data points. Similarly, the number of training epochs is also 100 or 1000, while the fine-tuning epochs shown in Table 2 is 20.

Table 2 shows that HMC with \(L=10\) fine-tuning generally performs better than DNN results, and HMC with \(L=1000\) has the similar ECE as that in DNN. It meets the conclusion made in the synthetic experiments. Gibbs sampling, however, could perform worse than just using DNN. It could be because Gibbs may be too far removed from the DNN, whereas our proposed HMC is more in the middle. This suggests perhaps future work testing the gradual shift from DNN to HMC to Gibbs.

\begin{table}
\begin{tabular}{c|c|c|c c c c} \multirow{2}{*}{Data (Weight)} & \multirow{2}{*}{\# Train Epochs} & \multicolumn{4}{c}{Average Mean Absolute Error (\(\times 10^{-3}\)) (p-value)} \\ \cline{4-7}  & & & DNN & Gibbs & HMC-10 & HMC-100 & HMC-1000 \\ \hline \multirow{2}{*}{BN (0.3)} & 100 & 6.593 & 16.09 (1.0000) & **5.300 (c0.0001)** & 6.864 (0.9982) & 6.658 (1.0000) \\  & 1000 & 34.44 & 36.09 (0.9916) & 23.53 (c0.0001) & 34.96 (1.0000) & 34.55 (1.0000) \\ \hline \multirow{2}{*}{BN (1)} & 100 & 22.90 & **20.84 (0.0001)** & **22.48 (0.0001)** & 24.71 (1.0000) & 22.95 (0.9928) \\  & 1000 & 42.59 & **33.64 (0.0001)** & **33.07 (0.0001)** & 43.03 (1.0000) & 42.63 (0.9995) \\ \hline \multirow{2}{*}{BN (3)} & 100 & 72.76 & 76.12 (1.0000) & 76.54 (1.0000) & **72.62 (c0.0001)** & 72.63 (c0.0001) \\  & 1000 & 28.28 & 32.59 (1.0000) & 32.98 (1.0000) & 28.84 (1.0000) & 28.40 (1.0000) \\ \hline \multirow{2}{*}{BN (10)} & 100 & 186.10 & 192.8 (1.0000) & 196.1 (1.0000) & **184.6 (c0.0001)** & **184.8 (c0.0001)** \\  & 1000 & 54.89 & 79.69 (1.0000) & 72.64 (1.0000) & 54.81 (1.266) & **54.63 (c0.0001)** \\ \hline \multirow{2}{*}{MN (0.3)} & 100 & 6.031 & 14.03 (1.0000) & 4.515 (c0.001) & 6.382 (1.0000) & 6.070 (1.0000) \\  & 1000 & 38.11 & **34.83 (c0.0001)** & 26.54 (c0.0001) & 38.71 (1.0000) & 38.22 (1.0000) \\ \hline \multirow{2}{*}{MN (1)} & 100 & 9.671 & 17.81 (1.0000) & 8.887 (c0.0001) & **9.018 (c0.0001)** & 9.284 (c0.0001) \\  & 1000 & 27.80 & 23.44 (1.0000) & 19.92 (c0.0001) & 27.98 (0.9994) & 27.23 (c0.0001) \\ \hline \multirow{2}{*}{MN (3)} & 100 & 8.677 & 23.60 (1.0000) & **5.685 (c0.0001)** & 5.912 (c0.0001) & 5.964 (c0.0001) \\  & 1000 & 5.413 & 28.03 (1.0000) & 5.792 (0.9957) & 5.671 (1.0000) & 5.443 (1.0000) \\ \hline \end{tabular}
\end{table}
Table 1: Calibration performance on synthetic datasets. Experiments are run on each dataset 100 times to avoid randomness. T-tests are used to test whether Gibbs and HMC have smaller MAE than SGD, and highlighted cells mean that it is statistically significant to support the hypothesis.

## 6 Conclusion, Limitations, and Future Work

In this work, we have established a new connection between DNNs and PGMs by constructing an infinite-width tree-structured PGM corresponding to any given DNN architecture, then showing that inference in this PGM corresponds exactly to forward propagation in the DNN given sigmoid activation functions. This theoretical result is valuable in its own right, as it provides new perspective that may help us understand and explain relationships between PGMs and DNNs. Moreover, we anticipate it will inspire new algorithms that merge strengths of PGMs and DNNs. We have explored one such algorithm, a novel HMC-based algorithm for DNN training or fine-tuning motivated by our PGM construction, and we illustrated how it can be used to improve to improve DNN calibration.

Limitations of the present work and directions for future work include establishing formal results about how closely batch- and layer-normalization can be modified to approximate Markov network normalization when using non-sigmoid activations, establishing theoretical results relating HMC in the neural network to Gibbs sampling in the large treewidth-1 Markov network, and obtaining empirical results for HMC with non-sigmoid activations. Also of great interest is comparing HMC and other PGM algorithms to Shapley values, Integrated Gradients, and other approaches for assessing the relationship of some latent variables to each other or to inputs and/or outputs in a neural network. We note that the large treewidth-1 PGM is a substantial approximation to the _direct_ PGM of a DNN - in other words, the PGM whose structure exactly matches that of the DNN. In future work, we will explore other DNN fine-tuning methods, perhaps based on loopy belief propagation or other approximate algorithms often used in PGMs, that may allow us to more closely approximate inference in this direct PGM.

Another direction for further work is in the original motivation for this work. Both DNNs and PGMs are often used to model different components of very large systems, such as the entire gene regulatory network in humans. For example, in the National Human Genome Research Institute (NHGRI) program Impact of Genetic Variation on Function (IGVF), different groups are building models of different parts of gene regulation, from genotypic variants or CRISPRi perturbations of the genome, to resulting changes in transcription factor binding or chromatin remodeling, to post-translational modifications, all the way to phenotypes characterized by changes in the expression of genes in other parts of the genome IGVF Consortium (2024). Some of these component models are DNNs and others are PGMs. As a community we know from years of experience with PGMs that passing the outputs of one model to the inputs of another model is typically less effective than concatenating them into a larger model and fine-tuning and using this resulting model. But this concatenation and fine-tuning and usage could not be done with a mixture of PGM and DNN components until now. Having an understanding of DNN components as PGMs enables their combination with PGM components, and then performing fine-tuning and inference in the larger models using algorithms such as the new HMC algorithm theoretically justified, developed, and then evaluated in this paper. Furthermore, the same HMC approach can be employed to reason just as easily from desired gene expression changes at the output nodes back to variants or perturbations at the input nodes that are predictive of the desired changes. Ordinarily, to reason in reverse in this way in a DNN would require special invertible architectures or training of DNNs that operate only in the other direction such as diffusion. Experiments evaluating all these uses of HMC (or other approximate algorithms in PGMs such as loopy belief propagation or other message passing methods) are left for future work.

\begin{table}
\begin{tabular}{c|c|c c c c c} \hline \multirow{2}{*}{Data} & \multirow{2}{*}{\# Train Epochs} & \multicolumn{5}{c}{Test Expected Calibration Error (\(\times 10^{-2}\))} \\ \cline{3-7}  & & DNN & Gibbs & HMC-10 & HMC-100 & HMC-1000 \\ \hline Covertype (label 1) & 100 & 4.207 & 4.229 & 2.352 & 3.893 & 3.987 \\  & 1000 & 10.85 & 8.513 & 6.875 & 7.730 & 11.60 \\ \hline Covertype (label 2) & 100 & 4.268 & 7.796 & 7.719 & 4.913 & 4.354 \\  & 1000 & 6.634 & 14.67 & 5.233 & 5.394 & 7.713 \\ \hline \end{tabular}
\end{table}
Table 2: Calibration performance on Covertype datasets. Highlighted cells show the best calibrations among each row.

## Acknowledgements

The authors would like to thank Sayan Mukherjee, Samuel I. Berchuck, Youngsoo Baek, David B. Dunson, Andrew S. Allen, William H. Majoros, Jude W. Shavlik, Sriraam Natarajan, David E. Carlson, Kouros Owzar, and Juan Restrepo for their helpful discussion about the theoretical work. We are also grateful to Mengyue Han and Jinyi Zhou for their technical support.

This project is in part supported by Impact of Genomic Variation on Function (IGVF) Consortium of the National Institutes of Health via grant U01HG011967.

## References

* Buhrmester et al. (2021) Vanessa Buhrmester, David Munch, and Michael Arens. Analysis of explainers of black box deep neural networks for computer vision: A survey. _Machine Learning and Knowledge Extraction_, 3(4):966-989, 2021.
* Kingma and Welling (2014) Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In _2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings_, 2014.
* Richardson (2022) Oliver E Richardson. Loss as the inconsistency of a probabilistic dependency graph: Choose your model, not your loss function, 2022. URL [https://arxiv.org/abs/2202.11862](https://arxiv.org/abs/2202.11862).
* Choe et al. (2017) Yo Joong Choe, Jaehyeok Shin, and Neil Spencer. Probabilistic interpretations of recurrent neural networks. _Probabilistic Graphical Models_, 2017.
* Garriga-Alonso et al. (2018) Adria Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional networks as shallow gaussian processes. _arXiv preprint arXiv:1808.05587_, 2018.
* Nair and Hinton (2010) Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In _Proceedings of the 27th international conference on machine learning (ICML-10)_, pages 807-814, 2010.
* Amari (1995) Shun-ichi Amari. Information geometry of the em and em algorithms for neural networks. _Neural networks_, 8(9):1379-1408, 1995.
* Song et al. (2016) Zhao Song, Ricardo Henao, David Carlson, and Lawrence Carin. Learning sigmoid belief networks via monte carlo expectation maximization. In _Artificial Intelligence and Statistics_, pages 1347-1355. PMLR, 2016.
* Dutordoir et al. (2021) Vincent Dutordoir, James Hensman, Mark van der Wilk, Carl Henrik Ek, Zoubin Ghahramani, and Nicolas Durrande. Deep neural networks as point estimates for deep gaussian processes. _Advances in Neural Information Processing Systems_, 34, 2021.
* Sun et al. (2020) Shengyang Sun, Jiaxin Shi, and Roger Baker Grosse. Neural networks as inter-domain inducing points. In _Third Symposium on Advances in Approximate Bayesian Inference_, 2020.
* Damianou and Lawrence (2013) Andreas Damianou and Neil D Lawrence. Deep gaussian processes. In _Artificial intelligence and statistics_, pages 207-215. PMLR, 2013.
* Russell and Norvig (2020) Stuart Russell and Peter Norvig. _Artificial Intelligence: A Modern Approach (4th Edition)_. Pearson, 2020. ISBN 9780134610993. URL [http://aima.cs.berkeley.edu/](http://aima.cs.berkeley.edu/).
* Tatikonda and Jordan (2002) Sekhar C. Tatikonda and Michael I. Jordan. Loopy belief propagation and gibbs measures. In _Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence_, UAI'02, page 493-500, San Francisco, CA, USA, 2002. Morgan Kaufmann Publishers Inc. ISBN 1558608974.
* Ihler et al. (2005) Alexander T. Ihler, John W. Fisher III, and Alan S. Willsky. Loopy belief propagation: Convergence and effects of message errors. _Journal of Machine Learning Research_, 6(31):905-936, 2005. URL [http://jmlr.org/papers/v6/ihler05a.html](http://jmlr.org/papers/v6/ihler05a.html).
* Tatikonda and Jordan (2002)Dror Weitz. Counting independent sets up to the tree threshold. In _Proceedings of the Thirty-Eighth Annual ACM Symposium on Theory of Computing_, STOC '06, page 140-149, New York, NY, USA, 2006. Association for Computing Machinery. ISBN 1595931341. doi: 10.1145/1132516.1132538. URL [https://doi.org/10.1145/1132516.1132538](https://doi.org/10.1145/1132516.1132538).
* Carreira-Perpinan and Hinton (2005) Miguel A Carreira-Perpinan and Geoffrey E Hinton. On contrastive divergence learning. In _Aistats_, volume 10, pages 33-40. Citeseer, 2005.
* Bengio and Delalleau (2009) Yoshua Bengio and Olivier Delalleau. Justifying and generalizing contrastive divergence. _Neural computation_, 21(6):1601-1621, 2009.
* Sutskever and Tieleman (2010) Ilya Sutskever and Tijmen Tieleman. On the convergence properties of contrastive divergence. In Yee Whye Teh and Mike Titterington, editors, _Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics_, volume 9, pages 789-795. JMLR Workshop and Conference Proceedings, 2010. URL [http://proceedings.mlr.press/v9/sutskever10a.html](http://proceedings.mlr.press/v9/sutskever10a.html).
* Neal (2012) Radford M Neal. _Bayesian learning for neural networks_, volume 118. Springer Science & Business Media, 2012.
* Blackard (1998) Jock Blackard. Covertype. UCI Machine Learning Repository, 1998. DOI: [https://doi.org/10.24432/C50K5N](https://doi.org/10.24432/C50K5N).
* Consortium (2024) IGVF Consortium. Deciphering the impact of genomic variation on function. _Nature_, 633(8028):47-57, 2024.
* Sutton and McCallum (2012) Charles Sutton and Andrew McCallum. An introduction to conditional random fields. _Found. Trends Mach. Learn._, 4(4):267-373, April 2012. ISSN 1935-8237. doi: 10.1561/2200000013. URL [https://doi.org/10.1561/2200000013](https://doi.org/10.1561/2200000013).

Bayesian Belief Net and Markov Net Equivalence

We don't claim the following theorem is new, but we provide a proof because it captures several components of common knowledge to which we couldn't find a single reference.

**Theorem A.1**.: _Let \(N\) be a Bayesian belief network whose underlying undirected graph has treewidth 1, and let \(w_{AB}\) denote the coefficient of variable \(A\) in the logistic CPD for its child \(B\). Let \(M\) be a binary pairwise Markov random field with the same nodes and edges (now undirected) as \(N\). Let \(M\)'s potentials all have the value \(e^{w_{AB}}\) if the nodes \(A\) and \(B\) on either side of edge \(AB\) are true, and the value \(1\) otherwise. \(M\) and \(N\) represent the same joint probability distribution over their nodes._

Proof.: According to \(M\) the probability of a setting \(\vec{V}\) of its variables is

\[\frac{1}{Z}\Pi_{i}\phi_{i}(\vec{V})\]

where \(\phi_{i}\) are the potentials in \(M\), and \(Z\) is the partition function, defined as

\[Z=\Sigma_{\vec{V}}\Pi_{i}\phi_{i}(\vec{V})\]

We use \(DOM(\phi)\) to designate the variables in a potential \(\phi\). Because the nodes and structures of \(M\) and \(N\) agree, we will refer to the parents, children, ancestors, and descendants of any node in \(M\) to designate the corresponding nodes in \(N\). Likewise we will refer to the input and output variables of \(M\) as those nodes of \(N\) that have no parents and no children, respectively. Because \(M\) has treewidth 1, each node of \(M\) d-separates its set of ancestors from its set of descendants and indeed from all other nodes in \(M\). As a result, it is known that the partition function can be computed efficiently in treewidth-1 Markov networks, for example by the following recursive procedure \(f\) defined below. Let \(V_{0}\) be the empty set of variables, and let \(V_{1}\) be the input variables of \(M\). Let \(Ch(V)\) denote the children of any set \(V\) of variables in \(M\), and similarly let \(Pa(V)\) denote the parents of \(V\). For convenience, when \(V\) is a singleton we drop the set notation and let \(V\) denote the variable itself. For all natural numbers \(i\geq 0\):

\[f(V_{i})=\Pi_{N\in Ch(V_{i})}\Sigma_{N=0,1}\Pi_{\phi_{j}:DOM(\phi_{j})\subseteq V _{i},DOM(\phi_{j})\not\subseteq V_{i-1}}\phi_{j}(V_{i})\]

\[f(V_{m+1})=1\]

where \(V_{m}\) is not the full set of variable in \(M\) but \(V_{m+1}\) is the full set. Then \(Z=f(V_{1})\).

For each variable \(v\in\vec{V}\), we can multiply the potentials on the edges between \(v\) and its parents, to get a single potential \(\phi_{\{v,Pa(v)\}}\) over \(\{v,Pa(v)\}\). For a given setting of the parents of \(v\) in \(\vec{V}\), let \(\phi_{v|Pa(v)}\) denote the result of conditioning on this setting of the parents, and let \(\phi_{v,\neg v|Pa(v)}\) denote the result of summing out variable \(v\). Using these product potentials of \(M\), and given the method above for computing \(Z\) for a tree-structured Markov network, we can define the probability of a particular setting \(\vec{V}\) as

\[P(\vec{V})=\Pi_{v\in\vec{V}}\frac{\phi_{v|Pa(v)}}{\phi_{v,\neg v|Pa(v)}}\]

These terms are exactly the terms of the logistic conditional probabilities of the Bayesian belief network \(N\):

\[P(\vec{V})=\Pi_{v\in\vec{V}}P(v|Pa(v))\]

Note that in general when converting a Bayes net structure to a Markov net structure, to empower the Markov net to represent any probability distribution representable by the Bayes net we have to moralize. A corollary of the above theorem is that in the special case where the Bayes net uses only sigmoid activations, and its underlying undirected graph is tree-structured, moralization is not required.

Step 1 and Step 2 Construction Algorithms

In the following, calls to add to sets \(V\) or \(E\) (or to check if either set contains a vertex or edge) immediately edits/checks the respective set object that they reference.

```
0: A list \(H\) of the output vertices of the DNN's DAG \(G\)
0: A set of vertices \(V\) and edges \(E\) of the DNN's original DAG transformed to a tree structured graph, as shown in Figure 1
1: let \(V\) be an empty set of vertices
2: let \(E\) be an empty set of edges
3:\(V,E\leftarrow\) DNN_TREE(\(H\), \(G\), \(V\), \(E\),'')
4:procedure DNN_TREE(\(H\), \(G\), \(V\), \(E\), \(child\))\(\triangleright\) unroll the DNN graph into a tree(s) layerwise
5:for each \(vertex\)in \(H\)do
6:\(new\gets vertex\)
7:while\(new\)in \(V\)do\(\triangleright\) create a new copy of this vertex
8:\(new\gets new\) + '\(r\)' character
9:endwhile
10:\(V\).add(\(new\))
11:if\(child\) is labelled then\(\triangleright\) ie child is not''
12:\(weight\gets G\).getEdge(\(vertex\), \(child\).removeAll('\(r\)')) \(\triangleright\) get child's original vertex
13: let \(e\) be an edge between \(new\) and \(child\)\(\triangleright\) an edge between the parent copy and the child
14: E.add(\(e\)), E.addWeight(\(e\), \(weight\))
15:endif
16: DNN_TREE(\(G\).parents(\(vertex\)), \(G\), \(V\), \(E\), \(new\))\(\triangleright\) create unrolled subtrees for each parent
17:endfor
18:return V, E
19:endprocedure
```

**Algorithm 1** Step 1 of the PGM Construction

```
0: A graph \(G\) of the vertices and edges created in Step 1, a list \(H\) of the DNN's output vertices, an integer \(l\)
0: Vertices \(V\) and edges \(E\) of the final tree structure graph with \(l\) copies of each parent node from Step 1
1: let \(V\) be an empty set of vertices
2: let \(E\) be an empty set of edges
3:for each\(vertex\)in \(H\)do
4:\(V^{\prime},E^{\prime}\leftarrow\) DNN_COPY(\(vertex\), \(G\), \(vertex\), \(V\), \(E\), \(l\))
5:\(V\gets V\).union(\(V^{\prime}\)), \(E\gets E\).union(\(E^{\prime}\))
6:endfor
7:procedure DNN_COPY(\(current\), \(G\), \(copy\), \(V\), \(E\), \(l\))
8:\(V\).add(\(copy\))
9:for each\(parent\)in \(G\).parents(\(current\))do
10:\(weight\gets G\).getWeight(\(parent\), \(current\))
11:for\(i\gets 1\) to \(l\)do
12:\(n\gets parent\) + '\(\cdot\)' + i \(\triangleright\) create the ith copy in the current subtree
13:while\(n\)in \(V\)do\(\triangleright\) create a unique label so the graph retains its tree-structure
14:\(n\gets n\) + '\(r\)
15:endwhile
16: let \(e\) be an edge between \(copy\) and \(n\)\(\triangleright\) connect this new copied node to the previous
17:\(E\).add(\(e\)), \(E\).addWeight(\(e\), \(weight\)/\(l\))
18: DNN_COPY(\(parent\),\(G\),\(n\),\(V\),\(E\),\(l\))\(\triangleright\) create copies in the remaining subtrees
19:endfor
20:endfor
21:return\(V\), \(E\)
22:endprocedure
```

**Algorithm 2** Step 2 of the PGM Construction

## Appendix C A Proof Using Variable Elimination

In order to prove that as \(L\) goes to infinity, this PGM construction does indeed match the neural network's forward propagation, we consider an arbitrary latent node \(H\) with \(N\) unobserved parents

[MISSING_PAGE_FAIL:15]

\[\sum_{h^{\beta}_{\alpha},\neg h^{\beta}_{\alpha}}\phi(H,h^{1}_{1},...,h^{L}_{1},...,h^{1}_{N},...,h^{L}_{N}|\vec{x})\] \[=e^{\sum_{j=1}^{M}w_{j}g_{j}\times H}\times\sum_{h^{\beta}_{\alpha },\neg h^{\beta}_{\alpha}}\prod_{i=1}^{N}\prod_{k=1}^{L}e^{(\theta_{i}/L)H\times h ^{k}_{i}}\phi(h^{k}_{i})\] \[=e^{\sum_{j=1}^{M}w_{j}g_{j}\times H}\times\] \[\begin{bmatrix}\\ e^{p_{\alpha}}e^{(\theta_{\alpha}/L)H}\times\prod_{\begin{subarray}{c}i=1,..,N \\ (i,k)\neq(\alpha,\beta)\end{subarray}}\prod_{k=1,...L}e^{(\theta_{i}/L)H\times h ^{k}_{i}}\phi(h^{k}_{i})\\ \end{bmatrix}\] \[\begin{aligned} &\qquad\qquad+\prod_{\begin{subarray}{c}i=1,..,N \\ (i,k)\neq(\alpha,\beta)\end{subarray}}\prod_{k=1,...L}e^{(\theta_{i}/L)H\times h ^{k}_{i}}\phi(h^{k}_{i})\\ &=e^{\sum_{j=1}^{M}w_{j}g_{j}\times H}\times(e^{p_{\alpha}}e^{(\theta_{ \alpha}/L)H}+1)\\ &\qquad\qquad\qquad\times\begin{bmatrix}\prod_{i=1,..,N}\\ (i,k)\neq(\alpha,\beta)\end{subarray}}\prod_{k=1,...L}e^{(\theta_{i}/L)H\times h ^{k}_{i}}\phi(h^{k}_{i})\\ \end{bmatrix}_{.}\]

Summing out all \(L\) copies of \(h_{\alpha}\):

\[e^{\sum_{j=1}^{M}w_{j}g_{j}\times H}\times(e^{p_{\alpha}}e^{(\theta_{\alpha}/L )H}+1)^{L}\times\begin{bmatrix}\prod_{i=1,..,N}\prod_{k=1,...L}e^{(\theta_{i}/ L)H\times h^{k}_{i}}\phi(h^{k}_{i})\\ \end{bmatrix}_{.}\]

Then summing out the \(L\) copies of each latent parent:

\[e^{\sum_{j=1}^{M}w_{j}g_{j}\times H}\times\prod_{i}^{N}(e^{p_{i}}e^{(\theta_{ i}/L)H}+1)^{L}\,\]

Normalizing this message locally, \(\phi(H=1|\vec{x})\) becomes \(\phi(H=1|\vec{x})/\phi(H=0|\vec{x})\) and \(\phi(H=0|\vec{x})\) becomes \(1\). This then gives us:

\[\phi(H=1|\vec{x})\] \[=\left[e^{\sum_{j=1}^{M}w_{j}g_{j}\times 1}\times\prod_{i}^{N}(e^{p_ {i}}e^{(\theta_{i}/L)\times 1}+1)^{L}\right]\Big{/}\left[e^{\sum_{j=1}^{M}w_{j}g_{j} \times 0}\times\prod_{i}^{N}(e^{p_{i}}e^{(\theta_{i}/L)\times 0}+1)^{L}\right]\] \[=\frac{e^{\sum_{j=1}^{M}w_{j}g_{j}}\times\prod_{i}^{N}(e^{p_{i}}e ^{(\theta_{i}/L)}+1)^{L}}{\prod_{i}^{N}(e^{p_{i}}+1)^{L}}.\]

We then consider:\[\lim_{L\rightarrow\infty}\frac{e^{\sum_{j=1}^{M}w_{j}g_{j}}\times\prod_ {i}^{N}(e^{p_{i}}e^{(\theta_{i}/L)}+1)^{L}}{\prod_{i}^{N}(e^{p_{i}}+1)^{L}}=\] \[\lim_{L\rightarrow\infty}\exp\left(\sum_{j=1}^{M}w_{j}g_{j}-\sum _{i=1}^{N}L\times\log(e^{p_{i}}+1)\right.\] \[\left.\qquad\qquad+\sum_{i=1}^{N}L\times\log(e^{p_{i}}e^{(\theta_ {i}/L)}+1)\right)\]

and the logarithm of this limit is,

\[\lim_{L\rightarrow\infty}\left[\sum_{j=1}^{M}w_{j}g_{j}-\sum_{i=1 }^{N}L\times\log(e^{p_{i}}+1)\right.\] \[\left.\qquad\qquad+\sum_{i=1}^{N}L\times\log(e^{p_{i}}e^{(\theta_ {i}/L)}+1)\right]\] \[=\sum_{j=1}^{M}w_{j}g_{j}\ \ +\lim_{L\rightarrow\infty}\frac{\sum_{i=1 }^{N}\left(\log(e^{p_{i}}e^{(\theta_{i}/L)}+1)-\log(e^{p_{i}}+1)\right)}{1/L}.\]

The limit in the previous expression clearly has the indeterminate form of \(\frac{0}{0}\). Let \(G=\sum_{j=1}^{M}w_{j}g_{j}\) and consider the following change of variables, \(S=1/L\), and subsequent use of l'Hospital's rule.

\[G+\lim_{S\to 0^{+}}\frac{\sum_{i=1}^{N}\left(\log(e^{p_{i}}e ^{(\theta_{i}S)}+1)-\log(e^{p_{i}}+1)\right)}{S}\] \[=G+\lim_{S\to 0^{+}}\frac{\frac{\frac{\partial}{\partial S} \sum_{i=1}^{N}\left(\log(e^{p_{i}}e^{(\theta_{i}S)}+1)-\log(e^{p_{i}}+1)\right) }{\frac{\partial}{\partial S}S}}{\frac{\partial}{\partial S}S}\] \[=G+\lim_{S\to 0^{+}}\frac{\sum_{i=1}^{N}\frac{1}{e^{p_{i}}e ^{(\theta_{i}S)}+1}\times e^{p_{i}}e^{\theta_{i}S}\times\theta_{i}}{1}\] \[=G+\lim_{S\to 0^{+}}\sum_{i=1}^{N}\frac{e^{p_{i}}e^{\theta_{i }S}\times\theta_{i}}{e^{p_{i}}e^{\theta_{i}S}+1}\] \[=G+\sum_{i=1}^{N}\frac{e^{p_{i}}}{e^{p_{i}}+1}\times\theta_{i}\] \[=\sum_{j=1}^{M}w_{j}g_{j}+\sum_{i=1}^{N}\sigma(p_{i})\theta_{i}.\]

Therefore,

\[\lim_{L\rightarrow\infty}\frac{e^{\sum_{j=1}^{M}w_{j}g_{j}}\times \prod_{i}^{N}(e^{p_{i}}e^{(\theta_{i}/L)}+1)^{L}}{\prod_{i}^{N}(e^{p_{i}}+1)^{L }}\] \[=\exp(\sum_{j=1}^{M}w_{j}g_{j}+\sum_{i}^{N}\sigma(p_{i})\theta_{i }),\]

The collected potential at node \(H\) from summing out its ancestors (from the directed view) then has the form:

\begin{tabular}{|c|c|} \hline \(h_{i}\) & \(\neg h_{i}\) \\ \hline \(\exp(\sum_{j=1}^{M}w_{j}g_{j}+\sum_{i}^{N}\sigma(p_{i})\theta_{i})\) & 1 \\ \hline \end{tabular} This is exactly the form of messages that we assumed were originally passed to node \(H\). Suppose then that \(z\) is a hidden node whose parents in the original deep neural network's DAG are all observed. By our PGM construction, we have that node \(z\) collects potential \(e^{\sum_{x\in\vec{z}}w_{ix}x}\) for \(z\) true and \(1\) for \(z\) false from our initial forward step. Here \(w_{zx}\) is the weight between nodes \(z\) and \(x\). Consider, then, the nodes whose parents in the DNN's DAG are either one of these first layer hidden nodes, or an observed node. By our PGM construction, we have shown that so long as the nodes in the previous layer are either observed or have this exponential message product, as is the case here, the message product of the nodes that immediately follow will have the same form.

Note that in order to calculate probability, \(P(H|\vec{x})\), we must also consider the influence that the child of node \(H\), call this \(C\), has on node \(H\) itself (this would be information passed through and collected at later nodes in the tree network that are then passed back through this node \(C\) to node \(H\)) or may not exist at all in the case of output nodes. Suppose the weight between this child \(C\) and node \(H\) is \(\gamma\). Suppose also that the message coming from node \(C\) to \(H\) has the following form, where \(c\) is a non-negative real value that can be arbitrarily large.

\begin{tabular}{|c|c|c|} \hline \(C\) & \(\neg C\) \\ \hline \(c\) & 1 \\ \hline \end{tabular} Finally, note that with the \(L\) copies made in this graph, the potential between node \(H\) and \(C\) has the form:

\begin{tabular}{|c|c|c|} \hline  & \(H\) & \(\neg H\) \\ \hline \(C\) & \(e^{\gamma/L}\) & 1 \\ \hline \(\neg C\) & 1 & 1 \\ \hline \end{tabular} Using the potential collected at \(H\) from the forward pass and this addition information from \(C\), we can then calculate the probability of node \(H\) given the input evidence \(\vec{x}\):

\[P(H|\vec{x})=\frac{1}{Z}\times e^{(\sum_{j=1}^{M}w_{j}g_{j}+\sum_ {i}^{N}\sigma(p_{i})\theta_{i})\times H}\times\sum_{C}e^{\gamma/L\times C \times H}\phi(C)\] \[=\frac{1}{Z}\times e^{(\sum_{j=1}^{M}w_{j}g_{j}+\sum_{i}^{N} \sigma(p_{i})\theta_{i})\times H}\times(ce^{\gamma/L\times H}+1)\]

From this we have that:

\[P(H=1|\vec{x})\] \[=\frac{e^{(\sum_{j=1}^{M}w_{j}g_{j}+\sum_{i}^{N}\sigma(p_{i}) \theta_{i})\times 1}\times(ce^{\gamma/L\times C\times 1}+1)}{e^{(\sum_{j=1}^{M}w_{j} g_{j}+\sum_{i}^{N}\sigma(p_{i})\theta_{i})\times 1}\times(ce^{\gamma/L\times C \times 1}+1)+e^{0}\times(ce^{0}+1)}\] \[=\frac{e^{\sum_{j=1}^{M}w_{j}g_{j}+\sum_{i}^{N}\sigma(p_{i}) \theta_{i}}\times(ce^{\gamma/L\times C\times 1}+1)}{e^{\sum_{j=1}^{M}w_{j}g_{j}+\sum_{i} ^{N}\sigma(p_{i})\theta_{i}}\times(ce^{\gamma/L\times C\times 1}+1)+(c+1)}\] \[=\frac{e^{\sum_{j=1}^{M}w_{j}g_{j}+\sum_{i}^{N}\sigma(p_{i}) \theta_{i}}}{e^{\sum_{j=1}^{M}w_{j}g_{j}+\sum_{i}^{N}\sigma(p_{i})\theta_{i}} +(c+1)/(ce^{\gamma/L\times C\times 1}+1)}.\]

Note that \(\lim_{L\rightarrow\infty}(c+1)/(ce^{\gamma/L\times C\times 1}+1)=1\), i.e. as \(L\) grows increasingly large the information passed through this child node becomes negligible. We therefore have that \(P(H=1|\vec{x})=\sigma(\sum_{j=1}^{M}w_{j}g_{j}+\sum_{i}^{N}\sigma(p_{i})\theta _{i})\), which is exactly the sigmoid activation value found in the forward pass of the neural network. Since every node in this network have the same form of incoming messages from their parents and child, we have that the conditional probability in this PGM construction and the activation values of the DNN match for any node in any layer of the DNN/PGM.

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_EMPTY:20]

[MISSING_PAGE_EMPTY:21]

\[=[\prod_{i=1}^{k}(P(h_{i}=1|x,h_{i-1}=1)-P(h_{i}=1|x,h_{i-1}=0))]\] \[\quad\times[P(h_{n}=1,h_{n+1}=1|x,h_{k+1}=1)(P(h_{k+1}=1|x,h_{k}=1) -P(h_{k+1}=1|x,h_{k}=0))\] \[\quad\quad-P(h_{n}=1,h_{n+1}=1|x,h_{k+1}=0)(P(h_{k+1}=1|x,h_{k}=1) -P(h_{k+1}=1|x,h_{k}=0))]\] \[=[\prod_{i=1}^{k+1}(P(h_{i}=1|x,h_{i-1}=1)-P(h_{i}=1|x,h_{i-1}=0))]\] \[\quad\times[P(h_{n}=1,h_{n+1}=1|x,h_{k+1}=1)-P(h_{n}=1,h_{n+1}=1|x,h_{k+1}=0)].\]

We therefore have that \(P(h_{n}=1,h_{n+1}=1|x,y=1)-P(h_{n}=1,h_{n+1}=1|x,y=0)\) can be written as a product of the differences \(P(h_{i}=1|x,h_{i-1}=1)-P(h_{i}=1|x,h_{i-1}=0)\) multiplied by the difference \(P(h_{n}=1,h_{n+1}=1|x,h_{k}=1)-P(h_{n}=1,h_{n+1}=1|x,h_{k}=0)\), for any \(k<n\).

Note that from D.1 we have that \(P(h_{i}=1|x,h_{i-1}=1)=\sigma(S_{h_{i}}+w_{i-1}/L)\) where \(w_{i-1}\) is the weight between the two nodes and \(S_{h_{i}}\) is the internal summation at \(h_{i}\) from the forward pass of the neural network prior to applying the sigmoid activation function \(\sigma(\cdot)\). It similarly follows that \(P(h_{i}=1|x,h_{i-1}=0)=\sigma(S_{h_{i}})\). We then divide and multiply this difference by \(w_{i-1}/L\).

\[P(h_{i}=1|x,h_{i-1}=1)-P(h_{i}=1|x,h_{i-1}=0))\] \[=(w_{i-1}/L)\frac{P(h_{i}=1|x,h_{i-1}=1)-P(h_{i}=1|x,h_{i-1}=0))}{ w_{i-1}/L}\] \[=(w_{i-1}/L)\frac{\sigma(S_{h_{i}}+w_{i-1}/L)-\The entire weight update then has the form:

\[(y-\hat{y})\times\left[\prod_{i=1}^{n}w_{i-1}\frac{\partial}{\partial S_{h_{i}}} \sigma(S_{h_{i}})\right]\times\sigma(S_{n+1})\]

This is exactly the gradient update in the neural network when considering a specific path/weight combination in the network. The infinite width PGM naturally has branches for each possible path in the neural network and as such the complete weight updates of both views will align.

## Appendix E HMC Sampling Trajectories

Suppose the current chain state is \(\mathbf{h}^{(n)}=\mathbf{\rho}_{n}(0)\). We then draw a momentum \(\mathbf{\mu}_{n}(0)\sim\mathcal{N}(0,M)\). The HMC trajectories imply that after \(\Delta t\), we have:

\[\mathbf{\mu}_{n}(t+\frac{\Delta t}{2}) =\mathbf{\mu}_{n}(t)-\frac{\Delta t}{2}\nabla U(\mathbf{\rho})\bigg{|}_{ \mathbf{\rho}=\mathbf{\rho}_{n}(t)}\] (E.1) \[\mathbf{\rho}_{n}(t+\Delta t) =\mathbf{\rho}_{n}(t)+\Delta tM^{-1}\mathbf{\mu}_{n}(t+\frac{\Delta t}{2})\] \[\mathbf{\mu}_{n}(t+\Delta t) =\mathbf{\mu}_{n}(t+\frac{\Delta t}{2})-\frac{\Delta t}{2}\nabla U( \mathbf{\rho})\bigg{|}_{\mathbf{\rho}=\mathbf{\rho}_{n}(t+\Delta t)}.\]

We may then apply these equations to \(\mathbf{\rho}_{n}(0)\) and \(\mathbf{\mu}_{n}(0)\)\(L\) times to get \(\mathbf{\rho}_{n}(L\Delta t)\) and \(\mathbf{\mu}_{n}(L\Delta t)\). Thus, the transition from \(\mathbf{h}_{(n)}=\mathbf{\rho}_{n}\) to the next state \(\mathbf{h}^{(n+1)}\) is given by:

\[\mathbf{h}^{(n+1)}\Big{|}(\mathbf{h}^{(n)}=\mathbf{\rho}_{n}(0))=\begin{cases}\mathbf{\rho}_{n }(L\Delta t)&\text{with probability }\alpha(\mathbf{\rho}_{n}(0),\mathbf{\rho}_{n}(L \Delta t))\\ \mathbf{\rho}_{n}(0)&\text{otherwise}\end{cases}\] (E.2)

where

\[\alpha(\mathbf{\rho}_{n}(0),\mathbf{\rho}_{n}(L\Delta t))=\min\bigg{(}1,\exp\big{(}H( \mathbf{\rho}_{n}(0),\mathbf{\mu}_{n}(0)\big{)}-H(\mathbf{\rho}_{n}(L\Delta t),\mathbf{\mu}_{ n}(L\Delta t))\big{)}\bigg{)}.\] (E.3)

## Appendix F CD-k Algorithm

```
1:Initialized \(\mathbf{h}^{(0)}\), \(\mathbf{W}^{(0)}=\{\mathbf{W}^{(0)}_{i}|i=1,2,...,K\}\), \(\mathbf{b}^{(0)}=\{\mathbf{b}^{(0)}_{i}|i=1,2,...,K\}\)
2:\(\mathbf{W}^{(n)},\mathbf{b}^{(n)}\) when loss converges.
3:procedureBurn-in(\(N\))
4:for\(i\gets 0\) to \(N-1\)do
5:\(\mathbf{h}^{(i+1)}\leftarrow\text{Sampling}(\mathbf{h}^{(i)},\mathbf{W}^{(0)}\),\(\mathbf{b}^{(0)}\))
6:endfor
7:endprocedure
8:procedureTraining(\(M\))
9:for\(i\gets 0\) to \(M-1\)do
10:\(\mathbf{W}^{(i+1)},\mathbf{b}^{(i+1)}\leftarrow\text{Weight-updating}(\mathbf{h}^{(N+ik)}, \mathbf{W}^{(i)},\mathbf{b}^{(i)})\)
11:for\(j\gets 0\) to \(k-1\)do
12:\(\mathbf{h}^{(N+ik+j+1)}\leftarrow\text{Sampling}(\mathbf{h}^{(N+ik+j)},\mathbf{W}^{(i+1)}, \mathbf{b}^{(i+1)})\)
13:endfor
14:endprocedure
```

**Algorithm 3** CD-\(k\) Learning for the Deep Belief Network

In our experiments, \(k=1\), i.e. we do one sampling step before weight updating. We mainly use HMC sampling method for the sampling step, where the detailed trajectories are defined in Appendix E. The weight-updating step is defined in equations (2), (6) and (7).

We also run Gibbs method for comparison, where results are shown in Table 1. Both Gibbs and HMC are sampling methods to generate new states for all the hidden nodes and they both apply CD-1 or CD-k to learn the weight. While HMC samples real values in the range of \([0,1]\) under the normal distribution, Gibbs samples values in \(\{0,1\}\) under Bernoulli distribution. For node \(h_{ij}\), i.e., the \(j\)th node in \(\mathbf{h}_{i}\) layer, the Bernoulli distribution used to sample its new state is determined by its Markov blanket, which includes all the nodes in \(\mathbf{h}_{i-1},\mathbf{h}_{i},\text{and}\mathbf{h}_{i+1}\) in this case. It could be calculated by normalizing the joint probability distribution of the blanket when \(h_{ij}=1\) versus \(h_{ij}=0\), which is written as following:

\[p(h_{ij}=1)=\frac{p(h_{ij}=1|\mathbf{h}_{i-1})\cdot p(\mathbf{h}_{i+1}|\mathbf{h}_{i} \backslash\{h_{ij}\},h_{ij}=1)}{p(h_{ij}=0|\mathbf{h}_{i-1})\cdot p(\mathbf{h}_{i+1}| \mathbf{h}_{i}\backslash\{h_{ij}\},h_{ij}=0)+p(h_{ij}=1|\mathbf{h}_{i-1})\cdot p(\mathbf{h }_{i+1}|\mathbf{h}_{i}\backslash\{h_{ij}\},h_{ij}=1)}\]

We then sample a new state for \(h_{ij}\) from \(\text{Bern}(p(h_{ij}=1))\). In Gibbs sampling, the node is sampled one by one since a newly sampled node will affect the sampling of subsequent nodes in its Markov blanket. After sampling new values for all the hidden nodes, the weight is similarly updated using equations (2), (6) and (7). The Gibbs sampling does not depend on \(L\).

## Appendix G Experimental Setup

For all the experiments, the train-test split ratio is 80:20. For the training and finetuning, Adam optimizer is used with learning rate being \(1\times 10^{-4}\). To get the predicted probabilities for fine-tuned network, 1000 output probabilities are sampled and averaged. In synthetic experiments, both BNs and MNs have the structure with the input dimension being 4, two latent layers with 4 nodes in each one, and one binary output.

## Appendix H Running time

Running time of one BN experiment in the main text are shown in Table 3.

\begin{table}
\begin{tabular}{c|c|c|c c c c} \multirow{2}{*}{Data (Weight)} & \multirow{2}{*}{\# Train Epochs} & \multicolumn{5}{c}{Average Running Time (s)} \\ \cline{3-7}  & & & \multicolumn{2}{c}{DNN} & Gibbs & HMC-10 & HMC-100 & HMC-1000 \\ \hline \multirow{2}{*}{BN (0.3)} & 100 & 7.07 & 2769.81 & 666.33 & 650.10 & 758.37 \\  & 1000 & 60.31 & 2741.30 & 634.20 & 626.79 & 728.57 \\ \hline \multirow{2}{*}{BN (1)} & 100 & 7.22 & 2811.77 & 640.68 & 661.12 & 757.21 \\  & 1000 & 65.25 & 2784.53 & 617.21 & 643.96 & 730.81 \\ \hline \multirow{2}{*}{BN (3)} & 100 & 5.09 & 2616.04 & 615.96 & 615.05 & 636.93 \\  & 1000 & 44.83 & 2602.65 & 596.23 & 594.62 & 608.02 \\ \hline \multirow{2}{*}{BN (10)} & 100 & 6.87 & 2868.98 & 638.80 & 665.14 & 661.82 \\  & 1000 & 48.98 & 2666.23 & 617.72 & 646.08 & 646.06 \\ \hline \multirow{2}{*}{MN (0.3)} & 100 & 7.26 & 2703.71 & 645.57 & 635.42 & 729.60 \\  & 1000 & 62.38 & 2669.61 & 625.38 & 618.07 & 701.31 \\ \hline \multirow{2}{*}{MN (1)} & 100 & 4.94 & 2508.05 & 578.88 & 582.59 & 610.07 \\  & 1000 & 44.39 & 2474.70 & 559.11 & 560.54 & 584.33 \\ \hline \multirow{2}{*}{MN (3)} & 100 & 10.84 & 2610.14 & 591.43 & 579.26 & 605.13 \\  & 1000 & 52.82 & 2578.81 & 570.23 & 558.98 & 585.22 \\ \hline \end{tabular}
\end{table}
Table 3: Average running time of different methods on synthetic datasets. For DNN, it shows the time of the training for 100 and 1000 epochs. For Gibbs and HMC, it shows the time of the finetuning for 20 epochs.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] Justification: We provide the theorem, proof (in supplement), and empirical results promised in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? [Yes] Justification: The introduction, theorem, and last section ("Conclusion, Limitations...") note that the theorem and experiments are limited to sigmoid activations and that formal results for other activations are left for future work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Note that for space reasons, the detailed proof had to be moved to the supplement. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Note that a reader will have to download our provided software and data to reproduce experiments, although as much detail was included in the paper as space permits. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Yes, the data and code are all available. We provide the link for them at the end of the introduction. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All details are in the paper, appendix, or code link. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We conducted each individual synthetic experiment 100 times and performed paired t-tests to assess statistical significance. The p-values are reported in the results section. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We say the work was on an internal GPU cluster, and that all experiments are reported. The precise individual run-times are in Appendix H; in the text we simply summarize that SGD is the fastest, Gibbs is the slowest, and HMC is in between. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We read the Code, and the research here conforms fully. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The most important motivation and selling point for this work is the ability to better understand a trained neural network as a proper statistical model. This ability is key to trustworthy AI.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The data and models do not have high risk of misuse. The data are either synthetic, themselves from synthetic target models, or are publicly available. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All software and dataq sets are described and cited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Only assets are algorithms, theoretical results, and code. The first two are described in detail, and code is available (currently anonymized for double-blind review). Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No human subjects or crowdsourcing. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: see above justification Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.