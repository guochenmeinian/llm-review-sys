# Provable Editing of Deep Neural Networks using Parametric Linear Relaxation

 Zhe Tao

University of California, Davis

Davis, CA 95616, USA

zhetao@ucdavis.edu

&Aditya V. Thakur

University of California, Davis

Davis, CA 95616, USA

avthakur@ucdavis.edu

###### Abstract

Ensuring that a DNN satisfies a desired property is critical when deploying DNNs in safety-critical applications. There are efficient methods that can verify whether a DNN satisfies a property, as seen in the annual DNN verification competition (VNN-COMP). However, the problem of provably editing a DNN to satisfy a property remains challenging. We present PREPARED,1 the first efficient technique for provable editing of DNNs. Given a DNN \(\mathcal{N}\) with parameters \(\theta\), input polytope \(\mathrm{P}\), and output polytope \(\mathrm{Q}\), PREPARED finds new parameters \(\hat{\mathbf{\theta}}\) such that \(\forall\mathbf{x}\in\mathrm{P}\). \(\mathcal{N}(\mathbf{x};\hat{\mathbf{\theta}})\in\mathrm{Q}\) while minimizing the changes \(\|\hat{\mathbf{\theta}}-\theta\|\). Given a DNN and a property it violates from the VNN-COMP benchmarks, PREPARED is able to provably edit the DNN to satisfy this property within 45 seconds. PREPARED is efficient because it relaxes the NP-hard provable editing problem to solving a linear program. The key contribution is the novel notion of Parametric Linear Relaxation, which enables PREPARED to construct tight output bounds of the DNN that are parameterized by the new parameters \(\hat{\mathbf{\theta}}\). We demonstrate that PREPARED is more efficient and effective compared to prior DNN editing approaches **i)** using the VNN-COMP benchmarks, **ii)** by editing CIFAR10 and TinyImageNet image-recognition DNNs, and BERT sentiment-classification DNNs for local robustness, and **iii)** by training a DNN to model a geodynamics process and satisfy physics constraints.

Footnote 1: PREPARED stands for Provable Editing using Parametric linear Relaxation of Deep neural networks.

## 1 Introduction

Ensuring that a DNN is correct and avoids harmful behaviors is critical when deploying them, especially in safety-critical contexts [8]. In such scenarios, it is vital to guarantee that a DNN satisfies a given property, e.g., a safety specification [35]. Towards this end, DNN verifiers aim to determine whether a DNN satisfies the given property [1]. There are efficient methods for DNN verification that can handle a wide-range of DNN architectures and properties, as seen in the annual DNN verification competition (VNN-COMP) [5]. However, the success of DNN verification reveals the next challenge: provably editing a DNN to satisfy a property. Formally, we define the provable editing problem as:

**Definition 1.1**.: Given a DNN \(\mathcal{N}\) with parameters \(\theta\), an input polytope \(\mathrm{P}\stackrel{{\mathrm{def}}}{{=}}\left\{\mathbf{x}\mid \mathbf{D}\mathbf{x}\leq\mathbf{e}\right\}\) and an output polytope \(\mathrm{Q}\stackrel{{\mathrm{def}}}{{=}}\left\{\mathbf{y}\mid \mathbf{A}\mathbf{y}\leq\mathbf{b}\right\}\), the **provable editing problem** is to find new parameters \(\hat{\mathbf{\theta}}\) that

\[\min\|\hat{\mathbf{\theta}}-\theta\|\quad\text{s.t.}\quad\forall\mathbf{x}\in \mathrm{P}\). \(\mathcal{N}(\mathbf{x};\hat{\mathbf{\theta}})\in\mathrm{Q} \tag{1}\]

Provable editing can be used, for instance, to ensure that a classifier DNN is locally robust for an input **x**. In this case, the input polytope \(\mathrm{P}\stackrel{{\mathrm{def}}}{{=}}\left\{\mathbf{x}^{\prime }\mid\|\mathbf{x}-\mathbf{x}^{\prime}\|_{\infty}\leq\varepsilon\right\}\) is the \(L^{\infty}\) ball around **x** withradius \(\varepsilon\), and the output polytope \(\mathrm{Q}\stackrel{{\mathrm{def}}}{{=}}\{\mathbf{y}\mid\bigwedge_{j \neq l}\mathbf{y}_{j}<\mathbf{y}_{l}\}\) requires that the output label is \(l\); viz., \(\arg\max\mathbf{y}=l\). Moreover, provable editing can be used either during training to guarantee that the DNN satisfies a property, or post-deployment to repair a given DNN.

The provable editing problem is challenging due to the presence of the universal quantifier; viz., finding new parameters such that _for all_ inputs in the polytope \(\mathrm{P}\), the output of the edited network lies in the polytope \(\mathrm{Q}\). There are no existing approaches for efficiently and effectively solving the provable editing problem. Regularization-based approaches [26; 10; 28; 24] encode the property into the loss during training, but are unable to guarantee the property-satisfaction. SMT-based approaches [14; 16] provide guarantees, but are not efficient because they directly solve an NP-hard problem. Prior LP-based approaches [41; 42] are efficient because they can only handle a restricted class of provable editing problems; e.g., they assume that the vertices of the input polytope or the linear regions of the DNN can be efficiently enumerated, or the DNN architecture can be modified.

This paper presents PREPARED, the _first efficient_ technique for **provable editing** of DNNs that runs in **polynomial time** (Theorem 3.4). Given a DNN and a property it violates from the VNN-COMP'22 benchmarks, PREPARED is able to provably edit the DNN to satisfy this property within 45 seconds.

PREPARED approaches the problem of provable editing by constructing tight parametric bounds of the output \(\mathcal{N}(\mathbf{x};\hat{\boldsymbol{\theta}})\) for all inputs \(\mathbf{x}\) in the input polytope \(\mathrm{P}\)_in terms of the parameters_\(\hat{\boldsymbol{\theta}}\), and then constraining these parametric bounds to lie within the output polytope \(\mathrm{Q}\). These parametric bounds are constructed compositionally per layer of the DNN, and are expressed as linear constraints, so that efficient Linear Programming (LP) solvers can be used to find an optimal solution.

Our _key insight_ is to represent the parametric bounds indirectly via _underapproximations of the epigraph and hypograph_ of the DNN layer. Consider the ReLU layer \(\boldsymbol{y}\stackrel{{\mathrm{def}}}{{=}}\mathtt{ReLU}( \boldsymbol{x})\). Prior approaches [49; 39] overapproximate its upper bound with a linear function (where in Figure 1(a)) using constant input bounds \(\llbracket\boldsymbol{x},\boldsymbol{x}\rrbracket\). Such a linear relaxation is loose, and is only sound within the given constant input bounds \(\llbracket\boldsymbol{x},\boldsymbol{x}\rrbracket\). Our approach is able to _exactly_ represent the upper bound of ReLU output _for any_ input upper bound variable \(\boldsymbol{x}\) (where in Figure 1(b)). This is done by capturing the epigraph (where in Figure 1(c)) of \(\mathtt{ReLU}(\boldsymbol{x})\) with a linear constraint.

Our _key contribution_ is a novel _Parametric Linear Relaxation_ for DNN layers, which defines tight parametric bounds using linear constraints in terms of the layer parameters. Consider \(\boldsymbol{v}\stackrel{{\mathrm{def}}}{{=}}\boldsymbol{x}\hat{ \boldsymbol{w}}\), which is a simplified representation of an Affine layer. We would like to construct bounds for all \(\boldsymbol{x}\in\llbracket\boldsymbol{x},\boldsymbol{x}\rrbracket\) in terms of the variable parameter \(\hat{\boldsymbol{w}}\). Prior approaches could achieve a sound, but loose linear relaxation that is not in terms of the layer parameters (Figure 2(a)). In contrast, our approach _exactly_ represents the output bounds without any relaxation (Figure 2(b)) building upon our key insight of capturing the epigraph and hypograph of the DNN layer _in terms of the parameters_ (Figure 2(c)).

We evaluate PREPARED **i)** using the VNN-COMP benchmarks, **ii)** by editing CIFAR10 and TinyImageNet image-recognition DNNs, and BERT sentiment-classification DNNs for local robustness, and **iii)** by training a DNN to model a geodynamics process and satisfy physics constraints. Because there were no prior efficient approaches for provable editing, we implemented new _provable fine-tuning_ baselines by integrating prior DNN editing approaches with a verifier in the loop. PREPARED outperformed such baselines demonstrating its effectiveness and efficiency.

## 2 Related work

**Regularized training**[20; 45; 30; 10; 19; 22; 47] incorporates constraints as regularization into the training loss, but does not guarantee constraint-satisfaction in the trained DNN. For the DNN editing problem we are addressing, which involves universally-quantified logical formulas, DL2 [10] is the state-of-the-art regularized training method.

**Certified training**[26; 15; 36; 2; 31; 28; 24] is a type of regularized training geared towards adversarial robustness. SABR[28] and STAPS[24] are state-of-the-art certified training approaches. However, they also do not guarantee constraint-satisfaction.

**SMT-based editing approaches**[14; 23; 16] directly solve the NP-hard provable editing problem using an SMT solver. Thus, they are inefficient and do not scale beyond small DNNs. For instance, the recent DeepSaDe approach [16] incorporates an SMT solver during training, but only uses the SMT solver to edit the last layer. However, it still can take \(500\times\) longer than regularized training.

**LP-based editing approaches**[41; 42] relax the provable editing problem to solving an LP problem, which makes them efficient. However, prior approaches can only handle a restricted class of provable editing problems. The state-of-the-art provable editing approach APRNN[42] is efficient for editing input points. However, it does not scale to the general provable editing problem of Definition 1.1 due to the need for enumerating vertices of the input polytope. PRDNN[41] was the precursor to APRNN and suffers from similar limitations; moreover, PRDNN modifies the architecture of DNN and requires enumeration of linear regions to edit an input polytope. REASSURE[12] repairs a linear region of a DNN by adding a patch DNN for each such linear region. Thus, it also requires enumeration of linear regions to edit an input polytope. Further, it has been shown to be unsound and incomplete (Section 5 of [42]), and suffers from significant runtime and memory overheads (Section 7.8 of [42]).

**DNN verification**[38; 39; 49; 44; 37; 48; 46; 9; 4] aims to determine whether a DNN satisfies a given property. As shown in Section 4, provable editing can be used when a verifier determines that a DNN violates a property. Most verification techniques use linear relaxations to bound the DNN output. However, these linear relaxations cannot be used for provable editing because they are not parameterized in terms of the DNN parameters. Instead, verification techniques assume that the parameters are constant and focus on designing linear relaxations for activation layers like ReLU [34; 43]. However, it is not clear how even these activation layer relaxations could be used for provable editing. For instance, these relaxations require constant input bounds; loose constant input bounds result in loose linear relaxations. In the context of provable editing, if the ReLU layer follows an Affine layer with editable parameters, then the constant input bounds to the ReLU layer would be very loose (e.g., \([-\infty,\infty]\)) in order to be sound for all possible edits. Hence, the resulting linear relaxation would also be loose. In contrast, our technique does not require constant input bounds for any layer, and produces an exact upper bound for the ReLU layer (Theorem 3.9). Applying our Parametric Linear Relaxation to DNN verification remains future work.

## 3 Provable editing of DNNs using Parametric Linear Relaxation

We use \(\mathbf{x}\in\mathbb{R}\) to denote a scalar, \(\mathbf{\mathrm{x}}\in\mathbb{R}^{m}\) to denote a column vector, and \(\mathbf{\mathrm{W}}\in\mathbb{R}^{n\times m}\) to denote a matrix. Blue-colored variables with a dot denote LP decision variables, e.g., \(\underline{\mathbf{\mathrm{x}}},\underline{\mathbf{\mathrm{x}}},\dot{\mathbf{\mathrm{W}}},\dot{\mathbf{\mathrm{b}}}\), etc. The proofs for all theorems can be found in Appendix A.

**Definition 3.1** (Dnn).: Let \(\mathbf{\mathrm{y}}\stackrel{{\mathrm{def}}}{{=}}\mathcal{N}(\mathbf{ \mathrm{x}};\theta)\) denote an \(L\)-layer deep neural network (DNN) \(\mathcal{N}\) with parameters \(\theta\), input \(\mathbf{\mathrm{x}}\) and output \(\mathbf{\mathrm{y}}\). The DNN is defined iteratively as \(\mathbf{\mathrm{x}}^{(\ell+1)}\stackrel{{\mathrm{def}}}{{=}}\mathcal{N }^{(\ell)}(\mathbf{\mathrm{x}}^{(\ell)};\theta^{(\ell)})\) for each layer \(\mathcal{N}^{(\ell)}\) with parameters \(\theta^{(\ell)}\), where \(\mathbf{\mathrm{x}}^{(0)}\stackrel{{\mathrm{def}}}{{=}}\mathbf{\mathrm{x}}\), \(\mathbf{\mathrm{y}}\stackrel{{\mathrm{def}}}{{=}}\mathbf{\mathrm{x}}^{(L)}\)and \(0\leq\ell<L\). \(\blacksquare\)

For ease of exposition, we defer our approach for the general provable editing problem of Definition 1.1 to Appendix B. In this section, we consider the following provable interval editing problem, where the input and output constraints are constant interval bounds:

**Definition 3.2**.: Given a DNN \(\mathcal{N}\) with parameters \(\theta\), constant input bounds \([\underline{\mathbf{\mathrm{x}}},\overline{\mathbf{\mathrm{x}}}]\) and constant output bounds \([\underline{\mathbf{\mathrm{y}}},\overline{\mathbf{\mathrm{y}}}]\), the _provable interval editing problem_ is to find new parameters \(\hat{\mathbf{\theta}}\) that

\[\min\|\hat{\mathbf{\theta}}-\theta\|\quad\text{s.t.}\quad\forall\mathbf{\mathrm{x}}\in [\underline{\mathbf{\mathrm{x}}},\overline{\mathbf{\mathrm{x}}}].\;\mathcal{N}(\mathbf{ \mathrm{x}};\hat{\mathbf{\theta}})\in[\underline{\mathbf{\mathrm{y}}},\overline{\mathbf{ \mathrm{y}}}] \tag{2}\]

### Parametric Linear Relaxation

Consider a DNN \(\mathcal{N}\) with variable parameters \(\hat{\mathbf{\theta}}\) and input bounds \([\underline{\mathbf{\mathrm{x}}},\overline{\mathbf{\mathrm{x}}}]\), our goal is to construct sound parametric output bounds \([\underline{\mathbf{\mathrm{y}}},\overline{\mathbf{\mathrm{y}}}]\) of \(\mathcal{N}\) in terms of \(\hat{\mathbf{\theta}}\) and \([\underline{\mathbf{\mathrm{x}}},\overline{\mathbf{\mathrm{x}}}]\). The exact definition is

\[\underline{\mathbf{\mathrm{y}}}\leq\min\left\{\mathcal{N}(\mathbf{\mathrm{x}};\hat{ \mathbf{\theta}})\;\middle|\;\underline{\mathbf{\mathrm{x}}}\leq\mathbf{\mathrm{x}}\leq \overline{\mathbf{\mathrm{x}}}\right\}\wedge\overline{\mathbf{\mathrm{y}}}\geq\max \left\{\mathcal{N}(\mathbf{\mathrm{x}};\hat{\mathbf{\theta}})\;\middle|\;\underline{ \mathbf{\mathrm{x}}}\leq\mathbf{\mathrm{x}}\leq\overline{\mathbf{\mathrm{x}}}\right\} \tag{3}\]

However, this universally-quantified non-linear formula is expensive to solve [6; 7]. Our key contribution is a novel _Parametric Linear Relaxation_, a _poly-size linear formula_ that implies Equation 3.

**Definition 3.3**.: The _Parametric Linear Relaxation_\(\overline{\mathcal{Z}}_{\mathcal{N}}(\underline{\mathbf{\mathrm{y}}},\overline{ \mathbf{\mathrm{y}}},\underline{\mathbf{\mathrm{x}}},\overline{\mathbf{\mathrm{x}}};\hat{ \mathbf{\theta}})\) for \(\mathcal{N}\) is a poly-size linear formula that implies \(\forall\mathbf{\mathrm{x}}\in[\underline{\mathbf{\mathrm{x}}},\overline{\mathbf{\mathrm{x}}}]\). \(\mathcal{N}(\mathbf{\mathrm{x}};\hat{\mathbf{\theta}})\in[\underline{\mathbf{\mathrm{y}}}, \overline{\mathbf{\mathrm{y}}}]\). \(\blacksquare\)

In other words, any satisfying assignment \((\underline{\mathbf{\mathrm{y}}},\overline{\mathbf{\mathrm{y}}},\underline{\mathbf{\mathrm{x}}},\overline{\mathbf{\mathrm{x}}},\theta)\) of the formula \(\overline{\mathcal{Z}}_{\mathcal{N}}(\underline{\mathbf{\mathrm{y}}},\overline{ \mathbf{\mathrm{y}}},\underline{\mathbf{\mathrm{x}}},\underline{\mathbf{\mathrm{x}}};\hat{ \mathbf{\theta}})\) yields sound output bounds \([\underline{\mathbf{\mathrm{y}}},\overline{\mathbf{\mathrm{y}}}]\) for \(\mathcal{N}\), viz., \(\forall\mathbf{\mathrm{x}}\in[\underline{\mathbf{\mathrm{x}}},\overline{\mathbf{\mathrm{x}}}]\). \(\mathcal{N}(\mathbf{\mathrm{x}};\theta)\in[\underline{\mathbf{\mathrm{y}}},\overline{ \mathbf{\mathrm{y}}}]\). The size of the formula \(\overline{\mathcal{Z}}_{\mathcal{N}}(\underline{\mathbf{\mathrm{y}}},\overline{ \mathbf{\mathrm{y}}},\underline{\mathbf{\mathrm{x}}},\overline{\mathbf{\mathrm{x}}};\hat{ \mathbf{\theta}})\) is polynomial in the size of the DNN, i.e., the number of parameters, layers, and the input and output dimensions of each layer.

### Provable editing via Parametric Linear Relaxation

The following theorem shows how Parametric Linear Relaxation can be used to solve the provable interval editing problem.

**Theorem 3.4**.: _Given a provable interval editing problem (Definition 3.2) for DNN \(\mathcal{N}\) and parameters \(\theta\) with input bounds \([\underline{\mathbf{x}},\overline{\mathbf{x}}]\) and output bounds \([\underline{\mathbf{y}},\overline{\mathbf{y}}]\), let \(\overline{\varphi}_{\mathcal{N}}(\underline{\mathbf{y}},\underline{\mathbf{y} },\underline{\mathbf{x}},\overline{\mathbf{x}};\dot{\boldsymbol{\theta}})\) be a Parametric Linear Relaxation for \(\mathcal{N}\). Then the following linear program can be solved in polynomial time in the size of the DNN \(\mathcal{N}\), and whose solution is a solution to the provable interval editing problem:_

\[\min\|\dot{\boldsymbol{\theta}}-\theta\|\quad\text{s.t.}\quad\overline{\varphi }_{\mathcal{N}}\big{(}\underline{\mathbf{y}},\underline{\mathbf{y}},\underline {\mathbf{x}},\overline{\mathbf{x}};\dot{\boldsymbol{\theta}}\big{)}\wedge \big{(}\underline{\mathbf{y}}\leq\underline{\mathbf{y}}\wedge\overline{\mathbf{ y}}\leq\overline{\mathbf{y}}\big{)} \tag{4}\]

### Compositional construction of Parametric Linear Relaxation

**Definition 3.5**.: Given an \(L\)-layer DNN \(\mathcal{N}\) with parameters \(\dot{\boldsymbol{\theta}}\), constant input bounds \([\underline{\mathbf{x}},\overline{\mathbf{x}}]\) and Parametric Linear Relaxation \(\overline{\mathcal{Z}}_{\mathcal{N}^{(\ell)}}\) for each layer \(\mathcal{N}^{(\ell)}\), we define the _compositional Parametric Linear Relaxation_\(\overline{\mathcal{Z}}_{\mathcal{N}}\big{(}\underline{\mathbf{y}}, \overline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}};\dot{ \boldsymbol{\theta}}\big{)}\) as follows, where \([\underline{\mathbf{y}},\overline{\mathbf{y}}]\stackrel{{\text{ def}}}{{=}}[\underline{\mathbf{x}}^{(L)},\overline{\mathbf{x}}^{(L)}]\) and \([\underline{\mathbf{x}}^{(0)},\overline{\mathbf{x}}^{(0)}]\stackrel{{ \text{ def}}}{{=}}[\underline{\mathbf{x}},\overline{\mathbf{x}}]\):

\[\overline{\mathcal{Z}}_{\mathcal{N}}\big{(}\underline{\mathbf{y}},\overline{ \mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}};\dot{\boldsymbol{ \theta}}\big{)}\stackrel{{\text{ def}}}{{=}}\bigwedge_{0\leq\ell<L} \overline{\mathcal{Z}}_{\mathcal{N}^{(\ell)}}(\underline{\mathbf{x}}^{(\ell+ 1)},\overline{\mathbf{x}}^{(\ell+1)},\underline{\mathbf{x}}^{(\ell)};\dot{ \boldsymbol{\theta}}^{(\ell)}\big{)} \tag{5}\]

**Theorem 3.6**.: _Definition 3.5 is a Parametric Linear Relaxation for the DNN \(\mathcal{N}\)._

By Theorems 3.4 and 3.6, we see that to solve the provable editing problem, we just need to construct Parametric Linear Relaxations \(\overline{\mathcal{Z}}_{\mathcal{N}^{(\ell)}}(\underline{\mathbf{y}}, \underline{\mathbf{y}},\underline{\mathbf{x}},\underline{\mathbf{x}};\dot{ \boldsymbol{\theta}})\)_for each DNN layer \(\mathcal{N}^{(\ell)}\). In practice, \(\overline{\mathcal{Z}}_{\mathcal{N}^{(\ell)}}(\underline{\mathbf{y}}, \underline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}};\dot{ \boldsymbol{\theta}})\) is defined by separate formulas \(\overline{\mathcal{Z}}_{\mathcal{N}^{(\ell)}}(\underline{\mathbf{y}}, \underline{\mathbf{x}},\overline{\mathbf{x}};\dot{\boldsymbol{\theta}})\) and \(\underline{\varphi}_{\mathcal{N}^{(\ell)}}(\underline{\mathbf{y}}, \underline{\mathbf{x}},\overline{\mathbf{x}};\dot{\boldsymbol{\theta}})\) for the upper and lower bounds, respectively: \(\overline{\mathcal{Z}}_{\mathcal{N}^{(\ell)}}(\underline{\mathbf{y}}, \overline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}};\dot{ \boldsymbol{\theta}})\stackrel{{\text{ def}}}{{=}}\overline{ \mathcal{Z}}_{\mathcal{N}^{(\ell)}}(\underline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}};\dot{\boldsymbol{\theta}})\wedge\underline{\varphi}_{ \mathcal{N}^{(\ell)}}(\underline{\mathbf{y}},\underline{\mathbf{x}},\overline{ \mathbf{x}};\dot{\boldsymbol{\theta}})\). For brevity, we describe how to construct Parametric Linear Relaxation for ReLU and Affine layers. Appendix C presents Parametric Linear Relaxations for Tanh, Sigmoid and ELU layers.

### Parametric Linear Relaxation for ReLU layer

**Definition 3.7**.: \(\underline{\mathbf{y}}\stackrel{{\text{ def}}}{{=}}\text{ReLU}( \mathbf{x})\) with input \(\textbf{x}\in\mathbb{R}^{m}\) and output \(\textbf{y}\in\mathbb{R}^{m}\) is defined as \(\textbf{y}\stackrel{{\text{ def}}}{{=}}\max\{\textbf{x},0\}\).

**Definition 3.8**.: For a ReLU layer with variable input bounds \([\underline{\mathbf{x}},\overline{\mathbf{x}}]\), its Parametric Linear Relaxation is defined as \(\overline{\mathcal{Z}}_{\text{ReLU}}(\underline{\mathbf{y}},\underline{\mathbf{y }},\underline{\mathbf{x}},\overline{\mathbf{x}})\stackrel{{\text{ def}}}{{=}}\overline{\varphi}_{\text{ReLU}}(\underline{\mathbf{y}},\overline{\mathbf{x}}) \wedge\underline{\varphi}_{\text{ReLU}}(\underline{\mathbf{y}},\underline{\mathbf{x}})\) where:

\[\overline{\mathcal{P}}_{\text{ReLU}}(\overline{\mathbf{y}},\overline{ \mathbf{x}})\stackrel{{\text{ def}}}{{=}}\underline{\mathbf{y}} \geq\overline{\mathbf{x}}\wedge\underline{\mathbf{y}}\geq 0\qquad\underline{\varphi}_{\text{ReLU}}(\underline{\mathbf{y}},\underline{ \mathbf{x}})\stackrel{{\text{ def}}}{{=}}\underline{\mathbf{y}}= \mathbf{c}\underline{\mathbf{x}} \tag{6}\]

and \(\mathbf{c}\in\mathbb{R}^{m}\) are constants such that \(0\leq\mathbf{c}_{i}\leq 1\). \(\blacksquare\)

Because ReLU is a convex function, as seen in Figure 1(c), the upper bound constraint \(\underline{\mathbf{y}}_{i}\geq\overline{\mathbf{x}}_{i}\wedge\overline{\mathbf{y}}_{i}\geq 0\) for each \(\overline{\mathbf{y}}_{i}\) exactly captures the epigraph \(\widehat{\text{\small{\small{\small{\small{\small{\small{\small{\small{\small{\small{\small{ \left({\left({\left({\left({\left({\left({\cdot\cdot\cdot\cdot\cdot\cdot\cdot \cdot} )}}}}}}}}}\))) of \(\text{ReLU}(\overline{\mathbf{x}}_{i})\). For each lower bound \(\underline{\mathbf{y}}_{i}\), we use a linear relaxation \(\underline{\mathbf{y}}_{i}=\mathbf{c}_{i}\underline{\mathbf{x}}_{i}\) (\(\underline{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\right\) \left({\left({\left\left({\left({\left({\cdot{\left({\cdot{\cdot{\cdot}\cdot{\cdot{\cdot}\cdot{\cdot{\cdot{\cdot \cdot}\cdot{\cdot{\cdot \cdot

#### 3.5.1 Affine layer with constant input bounds

**Definition 3.11**.: For an Affine layer with constant input bounds \([\underline{\mathbf{x}},\overline{\mathbf{x}}]\) and variable parameters \(\hat{\mathbf{W}}\), \(\hat{\mathbf{b}}\), its Parametric Linear Relaxation is defined as \(\overline{\varphi}_{\mathit{ktf}}(\underline{\mathbf{y}},\overline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\hat{\mathbf{W}},\hat{\mathbf{b}}) \stackrel{{\text{\tiny{def}}}}{{=}}\overline{\varphi}_{\mathit{Aff} }(\underline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\hat{ \mathbf{W}},\hat{\mathbf{b}})\wedge\varphi_{\mathit{Aff}}(\underline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\hat{\mathbf{W}},\hat{\mathbf{b }})\) where:

\[\overline{\varphi}_{\mathit{Aff}}(\overline{\mathbf{y}},\underline{\mathbf{x}}, \overline{\mathbf{x}},\overline{\mathbf{x}},\hat{\mathbf{W}},\hat{\mathbf{b}}) \stackrel{{\text{\tiny{def}}}}{{=}}\bigwedge_{j}\overline{ \mathbf{y}}_{j}=\sum_{i}\hat{\mathbf{V}}_{ji}+\hat{\mathbf{b}}_{j}\ \wedge\ \hat{\mathbf{V}}\geq\hat{\mathbf{W}}\underline{\mathbf{x}}\ \wedge\ \hat{\mathbf{V}}\geq\hat{\mathbf{W}}\overline{\mathbf{x}} \tag{7}\] \[\underline{\varphi}_{\mathit{Aff}}(\underline{\mathbf{y}}, \underline{\mathbf{x}},\overline{\mathbf{x}},\hat{\mathbf{W}},\hat{\mathbf{b }}) \stackrel{{\text{\tiny{def}}}}{{=}}\bigwedge_{j}\underline{\mathbf{y}}_{j}=\sum_{i}\underline{\mathbf{y}}_{ji}+\hat{\mathbf{b}}_{j}\ \wedge\ \hat{\mathbf{V}}\leq\hat{\mathbf{W}}\underline{\mathbf{x}}\ \wedge\ \hat{\mathbf{V}}\leq\hat{\mathbf{W}}\overline{\mathbf{x}}\]

The core of this definition is constructing parametric bounds \([\underline{\mathbf{x}},\overline{\mathbf{v}}]\) for the multiplication \(\boldsymbol{x}\hat{\mathbf{w}}\) for all \(\boldsymbol{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}]\) in terms of the variable weight \(\hat{\mathbf{w}}\). As seen in Figure 2(c), for all \(\boldsymbol{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}]\), the upper bound () of \(\boldsymbol{x}\hat{\mathbf{w}}\) is defined by a piecewise-linear convex function \(\overline{f}(\hat{\mathbf{w}})\stackrel{{\text{\tiny{def}}}}{{=}}\max\{\underline{\boldsymbol{x}}\hat{\mathbf{w}},\overline{\mathbf{x}}\hat{\mathbf{w}}\}\). Hence, the upper bound constraint \(\overline{\mathbf{v}}\geq\underline{\boldsymbol{x}}\hat{\mathbf{w}}\wedge \overline{\mathbf{v}}\geq\overline{\mathbf{x}}\hat{\mathbf{w}}\)_exactly_ captures the epigraph () of \(\overline{f}(\hat{\mathbf{w}})\). Similarly, the lower bound () of \(\boldsymbol{x}\hat{\mathbf{w}}\) is defined by a piecewise-linear concave function \(\underline{f}(\hat{\mathbf{w}})\stackrel{{\text{\tiny{def}}}}{{=}}\min\{\underline{\boldsymbol{x}}\hat{\mathbf{w}},\overline{\mathbf{x}}\hat{\mathbf{w}}\}\). Hence, the lower bound constraint \(\underline{\mathbf{v}}\leq\underline{\boldsymbol{x}}\hat{\mathbf{w}}\wedge \underline{\mathbf{v}}\leq\overline{\mathbf{x}}\hat{\mathbf{w}}\)_exactly_ captures the hypograph () of \(\underline{f}(\hat{\mathbf{w}})\).

**Theorem 3.12**.: _Definition 3.11 is a Parametric Linear Relaxation for the Affine layer with constant input bounds \([\underline{\mathbf{x}},\overline{\mathbf{x}}]\) that captures the exact lower and upper bounds._

#### 3.5.2 Affine layer with variable input bounds

We freeze the parameter weight \(\hat{\mathbf{W}}\) to be constant \(\mathbf{W}\) (e.g., the original weight) when the input bounds are variable to avoid non-linearity.

**Definition 3.13**.: For an Affine layer with variable input bounds \([\underline{\mathbf{x}},\overline{\mathbf{x}}]\), constant weight \(\mathbf{W}\) and variable bias \(\hat{\mathbf{b}}\), its Parametric Linear Relaxation is defined as \(\underline{\underline{\gamma}}_{\mathit{ktf}}(\underline{\mathbf{y}},\underline{ \mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\mathbf{W},\hat{ \mathbf{b}})\stackrel{{\text{\tiny{def}}}}{{=}}\overline{\varphi}_{\mathit{Aff}}(\underline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\mathbf{W},\hat{\mathbf{b}})\wedge \underline{\varphi}_{\mathit{Aff}}(\underline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\mathbf{W},\hat{\mathbf{b}})\) where:

\[\overline{\varphi}_{\mathit{ktf}}(\underline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{w}},\hat{\mathbf{b}}) \stackrel{{\text{\tiny{def}}}}{{=}}\bigwedge_{j}\underline{\mathbf{y}}_{j}=\sum_{i}\overline{\mathbf{V}}_{ji}+\hat{\mathbf{b}}_{j} \tag{8}\] \[\underline{\varphi}_{\mathit{ktf}}(\underline{\mathbf{y}},\underline{ \mathbf{x}},\overline{\mathbf{x}},\mathbf{W},\hat{\mathbf{b}}) \stackrel{{\text{\tiny{def}}}}{{=}}\bigwedge_{j}\underline{\mathbf{y}}_{j}=\sum_{i}\underline{\mathbf{V}}_{ji}+\hat{\mathbf{b}}_{j}\]

Using constant weight \(\mathbf{W}\) avoids non-linearity due to the multiplication \(\boldsymbol{x}\boldsymbol{w}\) where \(\boldsymbol{x}\) is universally-quantified in variable bounds \([\underline{\mathbf{x}},\overline{\mathbf{x}}]\). Thus, the bounds \([\underline{\mathbf{v}},\overline{\mathbf{v}}]\) of \(\boldsymbol{x}\boldsymbol{w}\) for all \(\boldsymbol{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}]\) are determined by the sign of the constant \(\boldsymbol{w}\); viz., \([\underline{\mathbf{v}},\overline{\mathbf{v}}]\stackrel{{\text{\tiny{def}}}}{{=}}[\underline{\mathbf{x}}\boldsymbol{w},\overline{\mathbf{x}}\boldsymbol{w}]\) if \(\boldsymbol{w}\geq 0\), otherwise \([\underline{\mathbf{v}},\overline{\mathbf{v}}]\stackrel{{\text{\tiny{def}}}}{{=}}[\underline{\mathbf{x}}\boldsymbol{w},\underline{\mathbf{x}}\boldsymbol{w}]\).

**Theorem 3.14**.: _Definition 3.13 is a Parametric Linear Relaxation for the Affine layer with variable input bounds \([\underline{\mathbf{\hat{x}}},\underline{\mathbf{\mathbb{X}}}]\) that captures the exact upper and lower bounds._

### Parametric Linear Relaxation for Tanh, Sigmoid and ELU layers

Our approach can also handle other activation layers like Tanh, Sigmoid and ELU. Figure 3 illustrates their Parametric Linear Relaxations with detailed description deferred to Appendix C.

### On scalability

As described in Appendix B, our approach can restrict the edits to only the last \(k\) layers of the DNN \(\mathcal{N}\), freezing the parameters of the first \(L-k\) layers. Consequently, the resulting Parametric Linear Relaxation is a linear formula whose size is polynomial in the size of _only the last \(k\) editable layers \(\mathcal{N}^{(k:L)}\)_instead of the entire DNN \(\mathcal{N}\). This flexibility enables our approach to scale to large DNNs and BERT transformers, as demonstrated in Section 4.

## 4 Experimental evaluation

We have implemented PREPARED in PyTorch[32] and use Gurobi[17] as the LP solver. We demonstrate the effectiveness and efficiency of PREPARED on different tasks that use a wide-range of DNN architectures and properties. All experiments were run on a machine with Dual Intel Xeon Silver 4216 Processor 16-Core 2.1GHz with 384 GB of memory, SSD and RTX-A6000 with 48 GB of GPU memory running Ubuntu 20.04. Additional details about these experiments are in Appendix D.

**Baselines.** Because there were no efficient prior approaches for provable editing, we implemented _provable fine-tuning_PFT(\(\boldsymbol{\cdot}\)) baselines that combine prior (non-provable) DNN editing approaches with

Figure 4: Results of **(a)** single-property and **(b)** all-properties editing problems from VNN-COMP\({}^{\prime}\)22.

a verifier in the loop: the editing stops when the verifier confirms that the DNN satisfies the property (Appendix D.1). We consider the state-of-the-art DNN editing approaches DL2[10], APRNIN[42], SABR[28] and STAPS[24] as introduced in Section 2. We use PFT(DL2) to denote the provable fine-tuning baseline instantiated with DL2. We use verifiers \(\alpha\),\(\beta\)-CROWN[46, 44], MN-BaB[9], or DeepT[4], depending on the task. To the best of our knowledge, ours is the _first_ to provide a comprehensive evaluation of such verifier-in-the-loop baselines for provable editing.

### Provable editing on VNN competition benchmarks

Setup.We compare PREPARED against PFT(DL2) and PFT(APRNN) on VNN-COMP22 benchmarks [29]. The VNN-COMP22 benchmarks consist of DNNs along with one or more of their associated safety properties. For verification, the task would be to determine whether a DNN satisfies each of its properties. In the context of provable editing, we use these benchmarks in two scenarios: **i) Single-property editing**: Given a DNN and a property it violates, edit it to satisfy this single property with a time limit of 600 seconds. There are 423 such DNN-property instances. **ii) All-properties editing**: Given a DNN that violates at least one of its associated properties, edit it to satisfy the conjunction of all (satisfied or violated) properties associated with it with a time limit of 3600 seconds. There are 66 such DNN-property instances.

PFT(SABR) and PFT(STAPS) are not used because they do not handle all properties and DNN architectures in this experiment. In particular, SABR and STAPS have not discussed how to handle general logical formula with disjunctions, and their current implementations are designed for local robustness training. PFT(SABR) and PFT(STAPS) are used for local robustness editing in Section 4.2.

Metrics.The **effectiveness** is measured using the number of provably edited instances, and the **efficiency** is measured using the runtime. We were unable to determine the impact on the predictive performance (accuracy), because VNN-COMP does not come with such evaluation metrics and data.

Results.As shown in Figure 4, PREPARED_is the best provable editing approach_, significantly outperforming PFT(DL2) and PFT(APRNN) in terms of both _effectiveness_ and _efficiency_. As shown in the cactus plots (**Left** in Figures 4(a) and 4(b)), PREPARED can provably edit more instances in less time. As shown in the speed-up plots (**Middle** and **Right** in Figures 4(a) and 4(b)), PREPARED completes most problems in 10 seconds, taking a maximum of 45 seconds for single-property editing; it achieves from 10x to more than 100x speed-up over PFT(DL2) and PFT(APRNN). Specifically, PREPARED succeeds on all 423 single-property editing problems, while PFT(DL2) and PFT(APRNN) succeed on 185 and 117 problems, respectively. PREPARED succeeds on 62 out of 66 multi-property editing problems, while PFT(DL2) and PFT(APRNN) only succeed on 45 and 48 problems, respectively.

### Local robustness editing for image-recognition DNNs

**Definition 4.1**.: Consider a classifier DNN \(\mathcal{N}:\mathbb{R}^{m}\rightarrow\mathbb{R}^{n}\) and a dataset \(\mathcal{D}\) of input-label pairs \((\mathbf{x},l)\in\mathbb{R}^{m}\times\mathbb{Z}\). The DNN \(\mathcal{N}\)_correctly classifies_ an input-label pair \((\mathbf{x},l)\) iff \(\operatorname*{arg\,max}\mathcal{N}(\mathbf{x})=l\), and the _standard accuracy_ of a DNN on a dataset \(\mathcal{D}\) is the percentage of correctly classified input-label pairs in \(\mathcal{D}\). Given a perturbation \(\mathbf{\varepsilon}\in\mathbb{R}\), the DNN \(\mathcal{N}\) is _\(L^{\infty}\)-locally-robust_ on an input-label pair \((\mathbf{x},l)\) iff \(\mathcal{N}\) correctly classifies all perturbed inputs \(\mathbf{x}^{\prime}\) in the \(L^{\infty}\) box centered at \(\mathbf{x}\) with perturbation \(\mathbf{\varepsilon}\), viz., \(\forall\mathbf{x}^{\prime}\in\mathbb{R}^{m}\). \(\|\mathbf{x}^{\prime}-\mathbf{x}\|_{\infty}\leq\mathbf{\varepsilon}\implies\operatorname *{arg\,max}\mathcal{N}(\mathbf{x}^{\prime})=l\). The _certified accuracy_ of a DNN on a dataset \(\mathcal{D}\) is the percentage of \(L^{\infty}\)-locally-robust input-label pairs in \(\mathcal{D}\).

Setup.We compare PREPARED against PFT(DL2), PFT(SABR), and APRNIN on \(L^{\infty}\)-local-robustness editing for image-recognition DNNs. We edit CNN7 DNNs for CIFAR10[21] and TinyImageNet[33], trained in prior work [28], to be locally-robust for images in the edit set. The CIFAR10 DNN has 17.2M parameters, 79.24% standard accuracy and 75.77% certified accuracy (\(\varepsilon=0.5\)/255). The TinyImageNet DNN has 51.9M parameters, 28.85% standard accuracy and 20.46% certified accuracy (\(\varepsilon=1\)/255). For CIFAR10 and TinyImageNet, the edit sets consist of 50 misclassified fog-corrupted images from CIFAR10-C[18] and TinyImageNet-C[18], respectively. The generalization sets consist of 200 variant images with different corruption-levels, four variants per image in the edit set. We use MN-BaB[9] to compute the certified accuracy.

Metrics.**Efficacy** is measured using the certified accuracy (Definition 4.1) on the edit set. The original certified accuracy on the edit set is 0%. Efficacy is the most important metric: _a provable edit must guarantee 100% efficacy._ The **standard accuracy** and **certified accuracy** on the full test set are also important metrics: a good provable edit should have high accuracy. However, those accuracy metrics are relevant only if efficacy is 100%. The **generalization** accuracy on the generalization set measures how well the edit generalizes to inputs that are similar to the edit set. However, the generalization accuracy is relevant only if the efficacy is 100% and the standard and certified accuracies are good; that is, the predictive power of the edited DNN should not be sacrificed for better generalization. The **runtime** metric measures the time taken to edit the DNN.

Results.As shown in Table 1, PREPARED_is overall the best approach_. PREPARED achieves the best standard and certified accuracy, good generalization and short runtime. In particular, PFT(SABR) and PFT(STAPS) have significantly lower standard and certified accuracy; PFT(DL2) takes significantly longer time, and was unable to achieve 100% efficacy in four hours in the TinyImageNet experiment; APRIN was unable to achieve 100% efficacy.

### Local robustness editing for sentiment classification BERT transformers

Setup.We compare PREPARED against PFT(DL2) and APRNN on provable \(L^{\infty}\) local-robustness editing for BERT sentiment-classification transformers. We conduct this experiment on the Stanford Sentiment Treebank (SST) [40] DNNs. We use DeepT [4] as the verifier in this experiment. We take the "wider" 12-layer BERT transformer used in the DeepT paper [4]. The standard accuracy of this network is \(84.07\%\). Because DeepT cannot handle all sentences in the SST dataset with arbitrary \(\varepsilon\), we construct two editing sets from the SST validation set: all 66 verifiable sentences with \(\varepsilon=1\)e-4, where 60 of them are certified to be locally-robust; all 26 verifiable sentences with \(\varepsilon=5\)e-4, where 24 of them are certified to be locally-robust. PFT(SABR) and PFT(STAPS) are not compared in this experiment because SABR and STAPS do not handle the BERT transformer architecture.

Metrics.**Efficacy** is measured using the certified accuracy (Definition 4.1) on the edit set. Because DeepT is incomplete, we can only compute a lower bound of the efficacy. Efficacy is the most important metric: _a provable edit must guarantee 100% efficacy._ The original efficacy (Og.ffic.) is also presented in Table 2. The **standard accuracy** on the full test set is also an important metric: a good provable edit should have high accuracy. However, the standard accuracy is relevant only if efficacy is 100%. The **runtime** metric measures the time taken to edit the DNN.

Results.As seen in Table 2, PREPARED is the only approach that achieves 100% efficacy in this task. PREPARED also achieves good accuracy and short runtime. Both PFT(DL2) and APRNN were unable to achieve 100% efficacy, and decreased the efficacy in most experiments.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{CIFAR10 with \(\varepsilon\)=0.5\_fb255_} & \multicolumn{4}{c}{TinyImageNet with \(\varepsilon\)=1/255} \\ \cline{2-10}  & Effic. & Acc. & Cert. & Gen. & Time & Effic. & Acc. & Cert. & Gen. & Time \\ \hline PFT(DL2) & 50/60 & 73.59\% & 70.42\% & 197/200 & 4780s & 47/60 & 22.83\% & 15.06\% & 199/200 & 14400s\({}^{*}\) \\ PFT(SABR) & 50/60 & 57.10\% & 52.72\% & **200/200** & 38s & 50/60 & 20.24\% & 13.38\% & **197/200** & 230s \\ PFT(STAPS) & 50/60 & 57.38\% & 54.26\% & 197/200 & **15s** & 50/60 & 20.84\% & 13.14\% & 191/200 & **112s** \\ APRIN & 50/60 & 78.96\% & 75.14\% & 113/200 & 20s & 12/60 & 28.75\% & 18.61\% & 115/200 & 417s \\ PREPARED & 50/60 & **75.25\%** & **71.45\%** & 189/200 & 88s & 50/60 & **28.16\%** & **16.80\%** & 149/200 & 663s \\ \hline \hline \multicolumn{10}{l}{\({}^{*}\) Timeout in four hours.} \\ \end{tabular}
\end{table}
Table 1: Local robustness editing for image-recognition DNNs. Comparison of efficacy (Effic.), standard (Acc.), certified (Cert.) and generalization (Gen.) accuracy. Rows with <100% efficacy are shaded. Best results are **highlighted**.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{\(\varepsilon=1\)e-4} & \multicolumn{4}{c}{\(\varepsilon=5\)e-4} \\ \cline{2-10}  & Og. & Effic. & Effic. & Acc. & Time & Og. & Effic. & Effic. & Acc. & Time \\ \hline PFT(DL2) & & 52/66 & 82.97\% & 12108s & & 23/26 & 79.67\% & 5610s \\ APRNN & **60/66** & 61/66 & 83.47\% & 6s & 29/26 & 23/26 & 81.32\% & 5s \\ PREPARED & **60/66** & **83.52\%** & **981s** & & **26/26** & **82.42\%** & **326s** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Local robustness editing for sentiment-classification transformers. Comparison of the lower bound of efficacy before (Og.ffic.) and after edit (Effic.), as well as the stand accuracy (Acc.). Rows with <100% efficacy are shaded. Best results are **highlighted**.

### Provable training for physics-plausible DNNs

**Numerical model.** We consider a classic model of a buoyancy-driven mantle flow with a central circular plume [13], which is an incompressible flow with variable viscosity. The state of the system is a vector field \(\mathbf{F}\stackrel{{\text{def}}}{{=}}(u,v,\eta,\rho)\) of velocity field \(\mathbf{U}\stackrel{{\text{def}}}{{=}}(u,v)\), viscosity \(\eta\) and density \(\rho\), over space \((x,y)\in\Omega\) and time \(t\in\mathcal{T}\). We consider the following two physics constraints: **i) Conservation of mass \(\nabla\cdot\mathbf{U}=0\)** from the **continuity** equation; **ii) Dirichlet boundary condition \(\mathbf{U}\cdot\mathbf{n}=0\)** on the boundary \(\partial\Omega\), where \(\mathbf{n}\) is the outward normal of \(\partial\Omega\).

**Setup.** We compare GD(PREPARED), a combination of gradient-descent (GD) and PREPARED, against DL2, GD and GD(APRNH) on the task of provably training a DNN to model the geodynamics process and satisfy physics constraints. We discretize the space into a 31x31 grid and implement the numerical model to generate data for 150 time steps. We evenly split the data into training, validation and test sets. We train a ReLU ResNet with one single residual block of four conv2d layers with 32 channels and 3x3 kernel size. For DL2, we add the physics constraints as an unsupervised-learning regularization to the supervised training. For GD(APRNH) and GD(PREPARED), we first use vanilla gradient-descent (GD) to train a base model, then edit it with the constraints. Specifically, GD(APRNH) edits all training points, and GD(PREPARED) edits the convex-hull of all training points to satisfy the constraints.

**Metrics.** The **relative error** on the test set, the constraint-satisfaction errors, viz., **continuity error**, for the conservation of mass, and **boundary-condition error**, and the **runtime** are the metrics.

**Results.** As shown in Table 3, GD(PREPARED) _is overall the best approach._ GD(PREPARED) has good relative error, and its constraint-satisfaction error is significantly better than other approaches. Notably, its continuity error is at the same magnitude as the reference data, close to the theoretical value, zero, and is \(10^{5}\) to \(10^{8}\) times better than other approaches. GD achieves the best relative error and the shortest runtime. DL2 has the worst errors and takes significantly longer amount of time.

## 5 Conclusion

We have presented PREPARED, the first efficient approach for provable editing of DNNs that runs in polynomial time. We presented novel methods for constructing tight parametric bounds for the DNN output using Parametric Linear Relaxation, enabling PREPARED to use an LP solver to find an edit. To demonstrate its effectiveness and efficiency, PREPARED was used to provably edit DNNs and properties from the VNN-COMP\({}^{\prime}\)22 benchmark, provably edit CIFAR10 and TinyImageNet image-recognition DNNs, and BERT sentiment-classification DNNs for local robustness, and provably train a geodynamics DNN to satisfy physics constraints.

**Societal impact.** PREPARED represents a step towards the goal of ensuring safety, trustworthiness, and reliability of DNNs, which is crucial given their use in autonomous safety-critical systems. However, being a general technique for editing DNNs, it could be used to remove or add malicious behavior such as bias, backdoor attacks, etc.

**Limitations.** PREPARED requires as input a property that we wish the DNN to satisfy. These properties would need to formally encode the notions of safety, reliability, or trustworthiness, which may not always be possible or easy to do. e.g., defining safety properties for LLM-based chatbots. However, the machine learning and formal methods communities are making progress towards this goal; e.g., developing proxy specifications for complex properties that are easier to define and learning safety specifications from data [8].

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Method & Rel. Err. & Cont. Err. & BC Err. & Time \\ \hline Ref. & — & \(1.42\times 10^{-12}\) & \(1.74\times 10^{-12}\) & — \\ DL2 & \(1.59\)\% & \(1.58\times 10^{-4}\) & \(5.08\) & \(14\,674\)s \\ GD & \(\mathbf{0.26\%}\) & \(1.59\times 10^{-5}\) & \(5.86\times 10^{-1}\) & **395s** \\ GD(APRNH) & \(0.50\)\% & \(5.57\times 10^{-7}\) & \(4.96\times 10^{-5}\) & \(570\)s \\ GD(PREPARED) & \(0.50\)\% & \(\mathbf{1.42\times 10^{-12}}\) & \(\mathbf{2.29\times 10^{-7}}\) & \(684\)s \\ \hline \hline \end{tabular}
\end{table}
Table 3: Global physics property repair for geodynamics DNN. Comparison of relative error (Rel. Err.), continuity (Cont. Err.) and boundary-condition (BC Err.) errors. Errors on the test set (Ref.) are shaded. Best results are **highlighted**.

## Acknowledgments and Disclosure of Funding

We would like to thank the anonymous reviewers for their feedback and suggestions, which have greatly improved the quality of the paper. This work is supported in part by NSF grant CCF-2048123 and DOE Award DE-SC0022285.

## References

* Albarghouthi [2021] Aws Albarghouthi. Introduction to neural network verification. _Found. Trends Program. Lang._, 7(1-2):1-157, 2021. doi: 10.1561/2500000051. URL [https://doi.org/10.1561/2500000051](https://doi.org/10.1561/2500000051).
* Balunovic and Vechev [2020] Mislav Balunovic and Martin Vechev. Adversarial training and provable defenses: Bridging the gap. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/forum?id=SJxSDxrKDr](https://openreview.net/forum?id=SJxSDxrKDr).
* Bonaert et al. [2021] Gregory Bonaert, Dimitar I. Dimitrov, Maximilian Baader, and Martin T. Vechev. Fast and precise certification of transformers. [https://github.com/eth-sri/DeepT/tree/16ffe4075f1fa87c87fa2a187d8c46cfd51e07bf](https://github.com/eth-sri/DeepT/tree/16ffe4075f1fa87c87fa2a187d8c46cfd51e07bf), 2021.
* Bonaert et al. [2021] Gregory Bonaert, Dimitar I. Dimitrov, Maximilian Baader, and Martin T. Vechev. Fast and precise certification of transformers. In Stephen N. Freund and Eran Yahav, editors, _PLDI '21: 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, Virtual Event, Canada, June 20-25, 2021_, pages 466-481. ACM, 2021. doi: 10.1145/3453483.3454056. URL [https://doi.org/10.1145/3453483.3454056](https://doi.org/10.1145/3453483.3454056).
* Brix et al. [2023] Christopher Brix, Mark Niklas Muller, Stanley Bak, Taylor T. Johnson, and Changliu Liu. First three years of the international verification of neural networks competition (vnn-comp), 2023.
* Collins [1975] George E. Collins. Quantifier elimination for real closed fields by cylindrical algebraic decompostion. In H. Brakhage, editor, _Automata Theory and Formal Languages_, pages 134-183, Berlin, Heidelberg, 1975. Springer Berlin Heidelberg. ISBN 978-3-540-37923-2.
* Collins and Hong [1991] George E. Collins and Hoon Hong. Partial cylindrical algebraic decomposition for quantifier elimination. _Journal of Symbolic Computation_, 12(3):299-328, 1991. ISSN 0747-7171. doi: [https://doi.org/10.1016/S0747-7171](https://doi.org/10.1016/S0747-7171)(08)80152-6. URL [https://www.sciencedirect.com/science/article/pii/S0747717108801526](https://www.sciencedirect.com/science/article/pii/S0747717108801526).
* Dalrymple et al. [2024] David Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia, Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, Alessandro Abate, Joe Halpern, Clark Barret, Ding Zhao, Tan Zhi-Xuan, Jeannette Wing, and Joshua Tenenbaum. Towards guaranteed safe ai: A framework for ensuring robust and reliable AI systems. _arXiv preprint arXiv:2405.06624_, 2024.
* Ferrari et al. [2022] Claudio Ferrari, Mark Niklas Muller, Nikola Jovanovic, and Martin Vechev. Complete verification via multi-neuron relaxation guided branch-and-bound. In _International Conference on Learning Representations_, 2022.
* Fischer et al. [2019] Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce Zhang, and Martin T. Vechev. DL2: training and querying neural networks with logic. In _Proceedings of the 36th International Conference on Machine Learning (ICML)_, volume 97 of _Proceedings of Machine Learning Research_. PMLR, 2019.
* Fischer et al. [2023] Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce Zhang, and Martin T. Vechev. DL2: Training and querying neural networks with logic. [https://github.com/eth-sri/d12/tree/05e2d3fb4e2b00ef5968bf28d26219750a580908](https://github.com/eth-sri/d12/tree/05e2d3fb4e2b00ef5968bf28d26219750a580908), 2023.
* Fu and Li [2022] Feisi Fu and Wenchao Li. Sound and complete neural network repair with minimality and locality guarantees. In _10th International Conference on Learning Representations, ICLR 2022, Lisbon, Portugal, Oct 27-28, 2022_. OpenReview.net, 2022. URL [https://arxiv.org/abs/2110.07682](https://arxiv.org/abs/2110.07682).
* Gerya [2019] Taras Gerya. _Introduction to numerical geodynamic modelling_. Cambridge University Press, 2019.
* Goldberger et al. [2020] Ben Goldberger, Guy Katz, Yossi Adi, and Joseph Keshet. Minimal modifications of deep neural networks using verification. In Elvira Albert and Laura Kovacs, editors, _LPAR 2020: 23rd International Conference on Logic for Programming, Artificial Intelligence and Reasoning, Alicante, Spain, May 22-27, 2020_, volume 73 of _EPiC Series in Computing_, pages 260-278. EasyChair, 2020. doi: 10.29007/699q. URL [https://doi.org/10.29007/699q](https://doi.org/10.29007/699q).

* Gowal et al. [2019] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for training verifiably robust models, 2019.
* Goyal et al. [2024] Kshitij Goyal, Sebastijan Dumancic, and Hendrik Blockeel. Deepsade: Learning neural networks that guarantee domain constraint satisfaction. _Proceedings of the AAAI Conference on Artificial Intelligence_, 38(11):12199-12207, Mar. 2024. doi: 10.1609/aaai.v38i11.29109. URL [https://ojs.aaai.org/index.php/AAAI/article/view/29109](https://ojs.aaai.org/index.php/AAAI/article/view/29109).
* Optimization [2022] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2022. URL [https://www.gurobi.com](https://www.gurobi.com).
* Hendrycks and Dietterich [2019] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL [https://openreview.net/forum?id=HJz6tiCqYm](https://openreview.net/forum?id=HJz6tiCqYm).
* Hoernle et al. [2022] Nick Hoernle, Rafael Michael Karampatsis, Vaishak Belle, and Kobi Gal. Multiplexnet: Towards fully satisfied logical constraints in neural networks. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(5):5700-5709, Jun. 2022. doi: 10.1609/aaai.v36i5.20512. URL [https://ojs.aaai.org/index.php/AAAI/article/view/20512](https://ojs.aaai.org/index.php/AAAI/article/view/20512).
* Hu et al. [2016] Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural networks with logic rules. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2410-2420, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1228. URL [https://aclanthology.org/P16-1228](https://aclanthology.org/P16-1228).
* Krizhevsky et al. [2012] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). URL [http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html).
* Li et al. [2023] Zenan Li, Zehua Liu, Yuan Yao, Jingwei Xu, Taolue Chen, Xiaoxing Ma, and Jian L\({}^{*}\)[u]. Learning with logical constraints but without shortcut satisfaction. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=M2unceRvqhh](https://openreview.net/forum?id=M2unceRvqhh).
* Malioutov and Meel [2018] Dmitry Malioutov and Kuldeep S Meel. Mlic: A maxsat-based framework for learning interpretable classification rules. In _International Conference on Principles and Practice of Constraint Programming_, pages 312-327. Springer, 2018.
* Mao et al. [2023] Yuhao Mao, Mark Niklas Mueller, Marc Fischer, and Martin Vechev. Connecting certified and adversarial training. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL [https://openreview.net/forum?id=T21MohRvb](https://openreview.net/forum?id=T21MohRvb).
* Mao et al. [2023] Yuhao Mao, Mark Niklas Mueller, Marc Fischer, and Martin Vechev. TAPS: Connecting certified and adversarial training. [https://github.com/eth-sri/TAPS/tree/6b5cD9d3435c482f56f00e6f57b99b07b11ca4f](https://github.com/eth-sri/TAPS/tree/6b5cD9d3435c482f56f00e6f57b99b07b11ca4f), 2023.
* Mirman et al. [2018] Matthew Mirman, Timon Gehr, and Martin T. Vechev. Differentiable abstract interpretation for provably robust neural networks. In Jennifer G. Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 3575-3583. PMLR, 2018. URL [http://proceedings.mlr.press/v80/mirman18b.html](http://proceedings.mlr.press/v80/mirman18b.html).
* Mueller et al. [2023] Mark Niklas Mueller, Franziska Eckert, Marc Fischer, and Martin Vechev. SABR: Small adversarial bounding boxes. [https://github.com/eth-sri/sabr](https://github.com/eth-sri/sabr), 2023.
* Mueller et al. [2023] Mark Niklas Mueller, Franziska Eckert, Marc Fischer, and Martin Vechev. Certified training: Small boxes are all you need. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=ToFuxItJtUMH](https://openreview.net/forum?id=ToFuxItJtUMH).
* Muller et al. [2022] Mark Niklas Muller, Christopher Brix, Stanley Bak, Changliu Liu, and Taylor T. Johnson. The third international verification of neural networks competition (vnn-comp 2022): Summary and results, 2022. URL [https://arxiv.org/abs/2212.10376](https://arxiv.org/abs/2212.10376).
* Nandwani et al. [2019] Yatin Nandwani, Abhishek Pathak, Mausam, and Parag Singla. A primal dual formulation for deep learning with constraints. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper_files/paper/2019/file/cf708fcldecf0337aded484f8f4519ae-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/cf708fcldecf0337aded484f8f4519ae-Paper.pdf).

* De Palma et al. [2023] Alessandro De Palma, Rudy Bunel, Krishnamurthy Dvijotham, M. Pawan Kumar, and Robert Stanforth. Ibp regularization for verified adversarial robustness via branch-and-bound, 2023.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Z. Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. _CoRR_, abs/1912.01703, 2019. URL [http://arxiv.org/abs/1912.01703](http://arxiv.org/abs/1912.01703).
* Pouransari and Ghili [2014] Hadi Pouransari and Saman Ghili. Tiny imagenet visual recognition challenge. _CS 231N_, 7, 2014.
* Salman et al. [2019] Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation barrier to tight robustness verification of neural networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 9832-9842, 2019. URL [https://proceedings.neurips.cc/paper/2019/hash/246a3c5544feb05473ea718f61adfa16-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/246a3c5544feb05473ea718f61adfa16-Abstract.html).
* Seshia et al. [2022] Sanjit A. Seshia, Dorsa Sadigh, and S. Shankar Sastry. Toward verified artificial intelligence. _Commun. ACM_, 65(7):46-55, 2022. doi: 10.1145/3503914. URL [https://doi.org/10.1145/3503914](https://doi.org/10.1145/3503914).
* Shi et al. [2021] Zhouxing Shi, Yihan Wang, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Fast certified robust training with short warmup. In _ICML 2021 Workshop on Adversarial Machine Learning_, 2021. URL [https://openreview.net/forum?id=_jUobmvki51](https://openreview.net/forum?id=_jUobmvki51).
* Shi et al. [2023] Zhouxing Shi, Qirui Jin, J Zico Kolter, Suman Jana, Cho-Jui Hsieh, and Huan Zhang. Formal verification for neural networks with general nonlinearities via branch-and-bound. 2023.
* Singh et al. [2018] Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Puschel, and Martin T. Vechev. Fast and effective robustness certification. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicole Cessa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 10825-10836, 2018. URL [https://proceedings.neurips.cc/paper/2018/hash/f2f446980d8e971ef3da97af089481c3-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/f2f446980d8e971ef3da97af089481c3-Abstract.html).
* Singh et al. [2019] Gagandeep Singh, Timon Gehr, Markus Puschel, and Martin T. Vechev. An abstract domain for certifying neural networks. _Proc. ACM Program. Lang._, 3(POPL):41:1-41:30, 2019. doi: 10.1145/3290354. URL [https://doi.org/10.1145/3290354](https://doi.org/10.1145/3290354).
* Socher et al. [2013] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 conference on empirical methods in natural language processing_, pages 1631-1642, 2013.
* Sotoudeh and Thakur [2021] Matthew Sotoudeh and Aditya Thakur. Prdnn. [https://github.com/95616ARG/PRDNN](https://github.com/95616ARG/PRDNN), 2021.
* Tao et al. [2023] Zhe Tao, Stephanie Nawas, Jacqueline Mitchell, and Aditya V. Thakur. Architecture-preserving provable repair of deep neural networks. _Proc. ACM Program. Lang._, 7(PLDI), jun 2023. doi: 10.1145/3591238. URL [https://doi.org/10.1145/3591238](https://doi.org/10.1145/3591238).
* Tjandraatmadja et al. [2020] Christian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Patel, and Juan Pablo Vielma. The convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network verification. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/f6c2a0c4b566bc99d596e58638e342b0-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/f6c2a0c4b566bc99d596e58638e342b0-Abstract.html).
* Wang et al. [2021] Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J. Zico Kolter. Beta-crown: Efficient bound propagation with per-neuron split constraints for neural network robustness verification. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 29909-29921, 2021. URL [https://proceedings.neurips.cc/paper/2021/hash/fac7fead96dafceaf80c1ddfaee82a4-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/fac7fead96dafceaf80c1ddfaee82a4-Abstract.html).

* Xu et al. [2018] Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Van den Broeck. A semantic loss function for deep learning with symbolic knowledge. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 5502-5511. PMLR, 10-15 Jul 2018. URL [https://proceedings.mlr.press/v80/xu18h.html](https://proceedings.mlr.press/v80/xu18h.html).
* Xu et al. [2021] Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. Fast and complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL [https://openreview.net/forum?id=nVZtKB16LMn](https://openreview.net/forum?id=nVZtKB16LMn).
* Xu et al. [2022] Ziwei Xu, Yogesh Rawat, Yongkang Wong, Mohan S Kankanhalli, and Mubarak Shah. Don't pour cereal into coffee: Differentiable temporal logic for temporal action segmentation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 14890-14903. Curran Associates, Inc., 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/5f96a21345c138da929e9871fda138e-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/5f96a21345c138da929e9871fda138e-Paper-Conference.pdf).
* Zhang et al. [2018] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 4944-4953, 2018. URL [https://proceedings.neurips.cc/paper/2018/hash/d04863f100d59b3eb688aa11f95b0ae60-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/d04863f100d59b3eb688aa11f95b0ae60-Abstract.html).
* Zhang et al. [2018] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. _Advances in neural information processing systems_, 31, 2018.
* Zhang et al. [2023] Huan Zhang, Zhuoxing Shi, Kaidi Xu, Yihan Wang, Shiqi Wang, Linyi Li, Jinqi (Kathryn) Chen, and Zhuolin Yang. auto_LiRPA: An automatic linear relaxation based perturbation analysis library for neural networks and general computational graphs. [https://github.com/Verified-Intelligence/auto_LiRPA](https://github.com/Verified-Intelligence/auto_LiRPA), 2023.

## Appendix A Proofs

See 3.4

Proof.: Equation 4 is a linear program that can be solved in polynomial in the size of the DNN \(\mathcal{N}\) because by Definition 3.3, \(\overline{\underline{\mathcal{Z}}}_{\mathcal{N}}\big{(}\underline{\underline{ \mathcal{Y}}},\underline{\underline{\mathcal{Y}}},\underline{\underline{\mathbf{ x}}},\underline{\underline{\mathbf{x}}};\underline{\underline{\theta}}\big{)}\) is a poly-size linear formula whose size is polynomial in the size of the DNN \(\mathcal{N}\), i.e., the number of parameters, layers, and the input and output dimensions of each layer.

Equation 4 is a solution to the provable interval editing problem (Definition 3.2) because by Definition 3.3, \(\overline{\underline{\mathcal{Z}}}_{\mathcal{N}}\big{(}\underline{\underline{ \mathcal{Y}}},\underline{\underline{\mathcal{Y}}},\underline{\underline{ \mathbf{x}}},\underline{\underline{\mathbf{x}}};\underline{\underline{\theta}} \big{)}\) implies \(\forall\mathbf{x}\in[\underline{\underline{\mathbf{x}}},\underline{\underline{ \mathbf{x}}}]\). 

See 3.6

Proof.: Recall Definition 3.5:

\[\overline{\underline{\mathcal{Z}}}_{\mathcal{N}}\big{(}\underline{\underline{ \mathcal{Y}}},\underline{\underline{\mathcal{Y}}},\underline{\underline{ \mathbf{x}}},\underline{\underline{\mathbf{x}}};\underline{\dot{\theta}} \big{)}\stackrel{{\mathrm{def}}}{{=}}\bigwedge_{0\leq\ell<L} \overline{\underline{\mathcal{Z}}}_{\mathcal{N}^{(\ell)}}\big{(}\underline{ \underline{\mathbf{x}}}^{(\ell+1)};\underline{\underline{\mathbf{x}}}^{(\ell +1)};\underline{\underline{\mathbf{x}}}^{(\ell)};\underline{\underline{ \mathbf{x}}}^{(\ell)};\underline{\dot{\theta}}^{(\ell)}\big{)}\]

where \([\underline{\underline{\mathcal{Y}}},\underline{\underline{\mathcal{Y}}}] \stackrel{{\mathrm{def}}}{{=}}[\underline{\underline{\mathbf{x}}}^ {(L)},\underline{\mathbf{x}}^{(L)}]\) and \([\underline{\underline{\mathcal{X}}}^{(0)},\underline{\underline{\mathbf{x}}}^ {(0)}]\stackrel{{\mathrm{def}}}{{=}}[\underline{\underline{\mathbf{x }}},\underline{\underline{\mathbf{x}}}]\). Our goal is to prove that \(\overline{\underline{\mathcal{Z}}}_{\mathcal{N}}\big{(}\underline{\underline{ \mathcal{Y}}},\underline{\underline{\mathcal{Y}}},\underline{\underline{ \mathbf{x}}},\underline{\underline{\mathbf{x}}};\underline{\dot{\theta}}\big{)}\) is a poly-size linear formula that implies

\[\forall\mathbf{x}\in[\underline{\underline{\mathbf{x}}},\underline{\underline{ \mathbf{x}}}]\). \(\mathcal{N}\big{(}\mathbf{x};\underline{\dot{\theta}}\big{)}\in[\underline{ \underline{\mathbf{y}}},\underline{\underline{\mathbf{y}}}]\)

We first show that \(\overline{\underline{\mathcal{Z}}}_{\mathcal{N}}\big{(}\underline{\underline{ \mathcal{Y}}},\underline{\underline{\mathcal{Y}}},\underline{\underline{ \mathbf{x}}},\underline{\underline{\mathbf{x}}};\underline{\dot{\theta}}\big{)}\) is a poly-size linear formula. For Parametric Linear Relaxation \(\overline{\underline{\mathcal{N}}}_{\mathcal{N}^{(\ell)}}\big{(}\underline{ \underline{\mathbf{x}}}^{(\ell+1)},\underline{\mathbf{x}}^{(\ell+1)};\underline {\underline{\mathbf{x}}}^{(\ell)},\underline{\mathbf{x}}^{(\ell)};\underline{ \dot{\theta}}^{(\ell)}\big{)}\) for any layer \(\mathcal{N}^{(\ell)}\), by Definition 3.3, we have \(\overline{\underline{\mathcal{Z}}}_{\mathcal{N}^{(\ell)}}\big{(}\underline{ \underline{\mathbf{x}}}^{(\ell+1)},\underline{\mathbf{x}}^{(\ell+1)};\underline {\underline{\mathbf{x}}}^{(\ell)};\underline{\dot{\theta}}^{(\ell)}\big{)}\) is a poly-size linear formula whose size is polynomial in the number of parameters, input and output dimensions of layer \(\mathcal{N}^{(\ell)}\). Therefore, we have \(\overline{\underline{\mathcal{Z}}}_{\mathcal{N}}\big{(}\underline{\underline{ \mathcal{Y}}},\underline{\underline{\mathcal{Y}}},\underline{\underline{ \mathbf{x}}},\underline{\underline{\mathbf{x}}};\underline{\dot{\theta}}\big{)}\) is a poly-size linear formula whose size is polynomial in the number of parameters, layers, and the input and output dimensions of each layer.

Then we show that \(\overline{\underline{\mathcal{Z}}}_{\mathcal{N}}\big{(}\underline{\underline{ \mathcal{Y}}},\underline{\underline{\mathcal{Y}}},\underline{\underline{\mathbf{ x}}},\underline{\underline{\mathbf{x}}};\underline{\dot{\theta}}\big{)}\) implies \(\forall\mathbf{x}\in[\underline{\underline{\mathbf{x}}},\underline{\underline{ \mathbf{x}}}]\). \(\mathcal{N}\big{(}\mathbf{x};\underline{\dot{\theta}}\big{)}\in[\underline{ \underline{\mathbf{y}}},\underline{\underline{\mathbf{y}}}]\). By Definition 3.3, we have that the Parametric Linear Relaxation \(\overline{\underline{\mathcal{Z}}}_{\mathcal{N}^{(\ell)}}\big{(}\underline{ \underline{\mathbf{x}}}^{(\ell+1)},\underline{\mathbf{x}}^{(\ell+1)};\underline {\underline{\mathbf{x}}}^{(\ell)},\underline{\mathbf{x}}^{(\ell)};\underline{ \dot{\theta}}^{(\ell)}\big{)}\) for any layer \(\mathcal{N}^{(\ell)}\) implies the following formula:

\[\forall\mathbf{x}^{(\ell)}\in[\underline{\underline{\mathbf{x}}}^{(\ell)}, \underline{\mathbf{x}}^{(\ell)}]\). \(\mathcal{N}^{(\ell)}\big{(}\mathbf{x}^{(\ell)};\underline{\dot{\theta}}^{( \ell)}\big{)}\in[\underline{\underline{\mathbf{x}}}^{(\ell+1)},\underline{ \mathbf{x}}^{(\ell+1)}]\)

Therefore, we have \(\overline{\underline{\mathcal{Z}}}_{\mathcal{N}}\big{(}\underline{\underline{ \mathcal{Y}}},\underline{\underline{\mathbf{y}}},\underline{\underline{\mathbf{ x}}},\underline{\underline{\mathbf{x}}};\underline{\dot{\theta}}\big{)}\stackrel{{ \mathrm{def}}}{{=}}\bigwedge_{0\leq\ell<L}\overline{\underline{\mathcal{Z}}}_{ \mathcal{N}^{(\ell)}}\big{(}\underline{\underline{\mathbf{x}}}^{(\ell+1)}, \underline{\mathbf{x}}^{(\ell+1)};\underline{\underline{\mathbf{x}}}^{(\ell)}; \underline{\dot{\theta}}^{(\ell)}\big{)}\) implies the following formula:

\[\bigwedge_{0\leq\ell<L}\forall\mathbf{x}^{(\ell)}\in[\underline{\underline{ \mathbf{x}}}^{(\ell)},\underline{\mathbf{x}}^{(\ell)}]\). \(\mathcal{N}^{(\ell)}\big{(}\mathbf{x}^{(\ell)};\underline{\dot{\theta}}^{(\ell)} \big{)}\in[\underline{\underline{\mathbf{x}}}^{(\ell+1)},\underline{\mathbf{x}}^{( \ell+1)}]\)

where \([\underline{\underline{\mathcal{Y}}},\underline{\underline{\mathbf{y}}}] \stackrel{{\mathrm{def}}}{{=}}[\underline{\underline{\mathbf{x}}}^ {(L)},\underline{\mathbf{x}}^{(L)}]\) and \([\underline{\mathbf{x}}^{(0)},\underline{\mathbf{x}}^{(0)}]\stackrel{{ \mathrm{def}}}{{=}}[\underline{\underline{\mathbf{x}}},\underline{\underline{ \mathbf{x}}}]\). By induction, the formula above implies \(\forall\mathbf{x}\in[\underline{\underline{\mathbf{x}}},\underline{\underline{ \mathbf{x}}}]\). \(\mathcal{N}\big{(}\mathbf{x};\underline{\dot{\theta}}\big{)}\in[\underline{ \underline{\mathbf{y}}},\underline{\underline{\mathbf{y}}}]\). Therefore, we proved that \(\overline{\underline{\mathcal{Z}}}_{\mathcal{N}}\big{(}\underline{\underline{ \mathcal{Y}}},\underline{\underline{\mathbf{y}}},\underline{\underline{\mathbf{x}}}, \underline{\underline{\mathbf{x}}};\underline{\dot{\theta}}\big{)}\) is a poly-size linear formula that implies \(\forall\mathbf{x}\in[\underline{\underline{\mathbf{x}}},\underline{\underline{\mathbf{x}}}]\). \(\mathcal{N}\big{(}\mathbf{x};\underline{\dot{\theta}}\big{)}\in[\underline{ \underline{\mathbf{y}}},\underline{\underline{\mathbf{y}}}]\). 

### Proofs for Parametric Linear Relaxation of ReLU layers

**Lemma A.1**.: \(\overline{\mathcal{Z}}_{\mathtt{ReLU}}(\underline{\mathbf{y}},\overline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}})\) _defined in Definition 3.8 is a poly-size linear formula._

Proof.: As seen in Definition 3.8, \(\overline{\mathcal{Z}}_{\mathtt{ReLU}}(\underline{\mathbf{y}},\overline{\mathbf{y }},\underline{\mathbf{x}},\overline{\mathbf{x}})\) is a linear formula. Let \(m\) be the number of input dimensions of ReLU. \(\overline{\mathcal{Z}}_{\mathtt{ReLU}}(\underline{\mathbf{y}},\underline{ \mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}})\) has \(3m\) linear constraints over \(4m\) variables (\(\underline{\mathbf{x}}\in\mathbb{R}^{m}\), \(\overline{\mathbf{x}}\in\mathbb{R}^{m}\), \(\underline{\mathbf{y}}\in\mathbb{R}^{m}\) and \(\overline{\mathbf{y}}\in\mathbb{R}^{m}\)). Hence, \(\overline{\mathcal{Z}}_{\mathtt{ReLU}}(\underline{\mathbf{y}},\underline{ \mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}})\) is a poly-size linear formula whose size is polynomial in the number of input dimensions of the ReLU layer. 

**Lemma A.2**.: \(\overline{\mathcal{Z}}_{\mathtt{ReLU}}(\underline{\mathbf{y}},\overline{ \mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}})\overset{\text{\tiny def }}{=}\overline{\mathcal{Z}}_{\mathtt{ReLU}}(\overline{\mathbf{y}},\overline{ \mathbf{x}})\ \wedge\ \underline{\mathcal{Z}}_{\mathtt{ReLU}}(\underline{\mathbf{y}},\underline{ \mathbf{x}})\) _defined in Definition 3.8 implies \(\forall\mathbf{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}]\). \(\mathtt{ReLU}(\overline{\mathbf{x}})\in[\underline{\mathbf{y}},\overline{ \mathbf{y}}]\)._

Proof.: **For the upper bound**, recall that by Definition 3.8, we have

\[\overline{\mathcal{Z}}_{\mathtt{ReLU}}(\overline{\mathbf{y}},\overline{ \mathbf{x}})\overset{\text{\tiny def }}{=}\overline{\mathbf{y}}\geq\overline{\mathbf{x}}\wedge\overline{\mathbf{y}} \geq 0\]

Our goal is to prove that \(\overline{\mathcal{Z}}_{\mathtt{ReLU}}(\overline{\mathbf{y}},\overline{\mathbf{x }})\) implies

\[\forall\mathbf{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}].\ \overline{\mathbf{y}}\geq\mathtt{ReLU}(\mathbf{x})\]

which is equivalent to

\[\overline{\mathbf{y}}\geq\max\big{\{}\mathtt{ReLU}(\mathbf{x})\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\big{\}}\]

By the definition of the ReLU layer (Definition 3.7), we expand the formula above to

\[\overline{\mathbf{y}}\geq\max\big{\{}\max\{\mathbf{x},0\}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\big{\}}\]

Because \(\max\{\mathbf{x},0\}\) increases monotonically with respect to \(\mathbf{x}\), the above formula is equivalent to

\[\overline{\mathbf{y}}\geq\max\{\overline{\mathbf{x}},0\}\]

Because \(\max\{\overline{\mathbf{x}},0\}\) is a convex function, the above formula is equivalent to

\[\overline{\mathbf{y}}\geq\overline{\mathbf{x}}\ \wedge\ \overline{\mathbf{y}}\geq 0\]

which is the definition of \(\overline{\mathcal{Z}}_{\mathtt{ReLU}}(\overline{\mathbf{y}},\overline{ \mathbf{x}})\) in Definition 3.8. Therefore, we proved that \(\overline{\mathcal{Z}}_{\mathtt{ReLU}}(\overline{\mathbf{y}},\overline{ \mathbf{x}})\) implies \(\forall\mathbf{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}].\ \overline{\mathbf{y}}\geq\mathtt{ReLU}(\mathbf{x})\).

**For the lower bound**, recall that by Definition 3.8, we have

\[\underline{\mathcal{Z}}_{\mathtt{ReLU}}(\underline{\mathbf{y}},\underline{ \mathbf{x}})\overset{\text{\tiny def }}{=}\underline{\mathbf{y}}=\mathbf{c}\underline{\mathbf{x}}\]

where \(\mathbf{c}\in\mathbb{R}^{m}\) are constants such that \(0\leq\mathbf{c}_{i}\leq 1\). Our goal is to prove that \(\underline{\mathcal{Z}}_{\mathtt{ReLU}}(\underline{\mathbf{y}},\underline{ \mathbf{x}})\) implies

\[\forall\mathbf{x}\in[\underline{\mathbf{x}},\underline{\mathbf{x}}].\ \underline{\mathbf{y}}\leq\mathtt{ReLU}(\mathbf{x})\]

which is equivalent to

\[\underline{\mathbf{y}}\leq\min\big{\{}\mathtt{ReLU}(\mathbf{x})\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\big{\}}\]

By the definition of the ReLU layer (Definition 3.7), we expand the formula above to

\[\underline{\mathbf{y}}\leq\min\big{\{}\max\{\mathbf{x},0\}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\big{\}}\]

Because \(\max\{\mathbf{x},0\}\) increases monotonically with respect to \(\mathbf{x}\), the above formula is equivalent to

\[\underline{\mathbf{y}}\leq\max\{\underline{\mathbf{x}},0\}\]

which is equivalent to

\[\underline{\mathbf{y}}\leq\underline{\mathbf{x}}\ \vee\ \underline{\mathbf{y}}\leq 0\]

Given constant vector \(\mathbf{c}\) where \(0\leq\mathbf{c}\leq 1\), \(\underline{\mathcal{Z}}_{\mathtt{ReLU}}(\underline{\mathbf{y}},\underline{ \mathbf{x}})\overset{\text{\tiny def }}{=}\underline{\mathbf{y}}=\mathbf{c}\underline{\mathbf{x}}\) implies the above formula. Therefore, we proved that \(\underline{\mathcal{Z}}_{\mathtt{ReLU}}(\underline{\mathbf{y}},\underline{ \mathbf{x}})\) implies \(\forall\mathbf{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}].\ \underline{\mathbf{y}}\leq\mathtt{ReLU}(\mathbf{x})\).

**Lemma A.3**.: \(\overline{\varphi}_{\mathtt{ReLU}}(\underline{\mathbf{\dot{y}}},\overline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}})\stackrel{{\text{def}}}{{ \rightarrow}}\overline{\varphi}_{\mathtt{ReLU}}(\overline{\mathbf{y}},\overline{ \mathbf{x}})\wedge\underline{\varphi}_{\mathtt{ReLU}}(\underline{\mathbf{\dot{y}}},\underline{\mathbf{\dot{x}}})\) _defined in Definition 3.8 is implied by \(\overline{\mathbf{y}}=\max\big{\{}\mathtt{ReLU}(\mathbf{x})\ \big{|}\ \underline{\mathbf{\dot{x}}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\big{\}}\), hence captures the exact upper bound._

Proof.: Recall that by Definition 3.8, we have

\[\overline{\varphi}_{\mathtt{ReLU}}(\overline{\mathbf{y}},\overline{\mathbf{x}}) \stackrel{{\text{def}}}{{=}}\overline{\mathbf{y}}\geq\overline{ \mathbf{x}}\wedge\overline{\mathbf{y}}\geq 0\]

For any solution \((\overline{\mathbf{y}}^{*},\underline{\mathbf{x}},\overline{\mathbf{x}})\) to \(\overline{\mathbf{y}}=\max\big{\{}\mathtt{ReLU}(\mathbf{x})\ \big{|}\ \underline{\mathbf{\dot{x}}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\big{\}}\), we have

\[\overline{\mathbf{y}}^{*}\stackrel{{\text{def}}}{{=}}\max\big{\{} \mathtt{ReLU}(\mathbf{x})\ \big{|}\ \mathbf{x}\leq\overline{\mathbf{x}}\big{\}}\]

By the definition of ReLU layer (Definition 3.7), because ReLU is a monotonically increasing function, the definition above is equivalent to

\[\overline{\mathbf{y}}^{*}\stackrel{{\text{def}}}{{=}}\max\big{\{} \overline{\mathbf{x}},0\big{\}} \tag{9}\]

To prove that \(\overline{\varphi}_{\mathtt{ReLU}}(\overline{\mathbf{y}},\overline{\mathbf{x}})\) captures the exact upper bound, we will show that \((\overline{\mathbf{y}}^{*},\overline{\mathbf{x}})\) is a solution to \(\overline{\varphi}_{\mathtt{ReLU}}(\overline{\mathbf{y}},\overline{\mathbf{x}})\). In other words, the following instantiated formula \(\overline{\varphi}_{\mathtt{ReLU}}(\overline{\mathbf{y}}^{*},\overline{\mathbf{x}})\) is true:

\[\overline{\varphi}_{\mathtt{ReLU}}(\overline{\mathbf{y}}^{*},\overline{ \mathbf{x}})\stackrel{{\text{def}}}{{=}}\overline{\mathbf{y}}^{*} \geq\overline{\mathbf{x}}\wedge\overline{\mathbf{y}}^{*}\geq 0\]

By substituting \(\overline{\mathbf{y}}^{*}\) defined in Equation 9 above, our goal becomes

\[\max\big{\{}\overline{\mathbf{x}},0\big{\}}\geq\overline{\mathbf{x}}\wedge \max\big{\{}\overline{\mathbf{x}},0\big{\}}\geq 0\]

which is true. Hence, we have proved that \(\overline{\varphi}_{\mathtt{ReLU}}(\underline{\mathbf{y}},\overline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}})\) captures the exact upper bound. 

**Theorem 3.9**.: _Definition 3.8 is a Parametric Linear Relaxation for the ReLU layer that captures the exact upper bound._

Proof.: By Lemma A.1 and Lemma A.2, we proved that \(\overline{\varphi}_{\mathtt{ReLU}}(\underline{\mathbf{y}},\overline{\mathbf{y }},\underline{\mathbf{x}},\overline{\mathbf{x}})\) is a poly-size linear formula that implies \(\forall\mathbf{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}]\). \(\mathtt{ReLU}(\mathbf{x})\in[\underline{\mathbf{y}},\overline{\mathbf{y}}]\), hence is a Parametric Linear Relaxation (Definition 3.3) for ReLU. By Lemma A.3, we proved that \(\overline{\varphi}_{\mathtt{ReLU}}(\underline{\mathbf{y}},\overline{\mathbf{y }},\underline{\mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{W}}, \underline{\mathbf{b}})\) is implied by \(\overline{\mathbf{y}}=\max\big{\{}\mathtt{ReLU}(\mathbf{x})\ \big{|}\ \underline{\mathbf{\dot{x}}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\big{\}}\), hence captures the exact upper bound. 

### Proofs for Parametric Linear Relaxation of Affine layers with constant input bounds

**Lemma A.4**.: \(\overline{\varphi}_{\mathtt{Aff}}(\underline{\mathbf{y}},\overline{\mathbf{y }},\underline{\mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{W}}, \underline{\mathbf{b}})\) _defined in Definition 3.11 is a poly-size linear formula._

Proof.: As seen in Definition 3.11, \(\overline{\varphi}_{\mathtt{Aff}}(\underline{\mathbf{y}},\overline{\mathbf{y }},\underline{\mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{W}}, \underline{\mathbf{b}})\) is a linear formula. Let \(m\) and \(n\) be the number of input and output dimensions of the Affine layer. \(\overline{\varphi}_{\mathtt{Aff}}(\underline{\mathbf{y}},\overline{\mathbf{y }},\underline{\mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{W}}, \underline{\mathbf{b}})\) has \(2n+4mn\) linear constraints over \(2m+3n+3mn\) variables \((\underline{\mathbf{\dot{x}}}\in\mathbb{R}^{m},\ \overline{\mathbf{x}}\in\mathbb{R}^{m},\ \dot{\mathbf{b}}\in\mathbb{R}^{n},\ \underline{\mathbf{y}}\in\mathbb{R}^{n},\ \overline{\mathbf{y}}\in\mathbb{R}^{n},\)\(\dot{\mathbf{W}}\in\mathbb{R}^{n\times m},\ \dot{\underline{\mathbf{y}}}\in\mathbb{R}^{n\times m}\) and \(\overline{\mathbf{V}}\in\mathbb{R}^{n\times m}\)). Hence, \(\overline{\varphi}_{\mathtt{Aff}}(\underline{\mathbf{y}},\overline{\mathbf{y }},\underline{\mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{W}},\dot{ \mathbf{b}})\) is a poly-size linear formula whose size is polynomial in the number of input and output dimensions of the Affine layer. 

**Lemma A.5**.: \(\overline{\varphi}_{\mathtt{Aff}}(\underline{\mathbf{y}},\overline{\mathbf{y}}, \underline{\mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{W}},\underline{ \mathbf{b}})\stackrel{{\text{def}}}{{=}}\overline{\varphi}_{ \mathtt{Aff}}(\overline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\dot{ \mathbf{W}},\dot{\mathbf{b}})\wedge\underline{\varphi}_{\mathtt{Aff}}( \underline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\dot{ \mathbf{W}},\dot{\mathbf{b}})\) _defined in Definition 3.11 implies \(\forall\mathbf{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}]\). \(\mathtt{Affine}(\mathbf{x};\dot{\mathbf{W}},\dot{\mathbf{b}})\in[\underline{ \mathbf{y}},\overline{\mathbf{y}}]\)._

Proof.: **For the upper bound**, recall that by Definition 3.11, we have

\[\overline{\varphi}_{\mathtt{Aff}}(\overline{\mathbf{y}},\underline{\mathbf{x}}, \overline{\mathbf{x}},\dot{\mathbf{W}},\dot{\mathbf{b}})\stackrel{{ \text{def}}}{{=}}\bigwedge_{j}\overline{\mathbf{y}}_{j}=\sum_{i}\overline{\mathbf{V}}_{ ji}+\dot{\mathbf{b}}_{j}\ \wedge\ \overline{\mathbf{V}}\geq\dot{\mathbf{W}}\underline{\mathbf{x}}\ \wedge\ \overline{\mathbf{V}}\geq\dot{\mathbf{W}}\overline{\mathbf{x}}\]

our goal is to prove that \(\overline{\varphi}_{\mathtt{Aff}}(\overline{\mathbf{y}},\underline{\mathbf{x}}, \overline{\mathbf{x}},\overline{\mathbf{x}},\dot{\mathbf{W}},\dot{\mathbf{b}})\) implies

\[\forall\mathbf{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}].\ \overline{\mathbf{y}}\geq\mathtt{Affine}(\mathbf{x};\dot{\mathbf{W}},\dot{\mathbf{b}})\]

We prove the above formula by proving for each \(\overline{\mathbf{y}}_{j}\):

\[\overline{\mathbf{y}}_{j}\geq\max\Big{\{}\mathtt{Affine}(\mathbf{x};\dot{ \mathbf{W}},\dot{\mathbf{b}})_{j}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\Big{\}}\]By the definition of the Affine layer (Definition 3.10), we expand the formula above to

\[\overline{\mathbf{y}}_{j}\geq\max\left\{\sum_{i=0}^{m}\mathbf{x}_{i}\hat{\mathbf{W }}_{ji}+\hat{\mathbf{b}}_{j}\ \middle|\ \mathbf{x}\leq\mathbf{x}\leq\overline{ \mathbf{x}}\right\}\]

which is implied by the following formula

\[\overline{\mathbf{y}}_{j}\geq\sum_{i=0}^{m}\max\left\{\mathbf{x}_{i}\hat{ \mathbf{W}}_{ji}\ \middle|\ \mathbf{x}_{i}\leq\mathbf{x}_{i}\leq\overline{ \mathbf{x}}_{i}\right\}+\hat{\mathbf{b}}_{j}\]

Because \(\overline{\varphi}_{\texttt{Aff}}(\overline{\mathbf{y}},\underline{\mathbf{x} },\overline{\mathbf{x}},\dot{\mathbf{W}},\dot{\mathbf{b}})\) implies \(\overline{\mathbf{y}}_{j}=\sum_{i=0}^{m}\overline{\mathbf{y}}_{ji}+\dot{ \mathbf{b}}_{j}\), by substituting the left-hand-side \(\overline{\mathbf{y}}_{j}\) of the formula above with \(\sum_{i=0}^{m}\overline{\mathbf{y}}_{ji}+\dot{\mathbf{b}}_{j}\), our goal becomes:

\[\sum_{i=0}^{m}\overline{\mathbf{y}}_{ji}+\dot{\mathbf{b}}_{j}\geq\sum_{i=0}^{ m}\max\left\{\mathbf{x}_{i}\hat{\mathbf{W}}_{ji}\ \middle|\ \underline{\mathbf{x}}_{i}\leq\mathbf{x}_{i}\leq\overline{\mathbf{x}}_{i} \right\}+\dot{\mathbf{b}}_{j}\]

By subtracting \(\dot{\mathbf{b}}_{j}\) from both sides, our goal becomes:

\[\sum_{i=0}^{m}\overline{\mathbf{y}}_{ji}\geq\sum_{i=0}^{m}\max\left\{\mathbf{x }_{i}\hat{\mathbf{W}}_{ji}\ \middle|\ \mathbf{x}_{i}\leq\mathbf{x}_{i}\leq \overline{\mathbf{x}}_{i}\right\}\]

Then we prove the above formula by proving for each \(i\):

\[\overline{\mathbf{y}}_{ji}\geq\max\left\{\mathbf{x}_{i}\hat{\mathbf{W}}_{ji} \middle|\ \underline{\mathbf{x}}_{i}\leq\mathbf{x}_{i}\leq\overline{\mathbf{x}}_{i}\right\}\]

Because \(\mathbf{x}_{i}\hat{\mathbf{W}}_{ji}\) changes monotonically with respect to \(\mathbf{x}_{i}\), \(\max\left\{\mathbf{x}_{i}\hat{\mathbf{W}}_{ji}\ \middle|\ \underline{\mathbf{x}}_{i}\leq\mathbf{x}_{i}\leq\overline{\mathbf{x}}_{i}\right\}\) is taken when \(\mathbf{x}_{i}=\underline{\mathbf{x}}_{i}\) or \(\mathbf{x}_{i}=\overline{\mathbf{x}}_{i}\). In other words, \(\max\left\{\mathbf{x}_{i}\hat{\mathbf{W}}_{ji}\ \middle|\ \mathbf{x}_{i}\leq\mathbf{x}_{i}\leq\overline{\mathbf{x}}_{i}\right\}= \max\left\{\underline{\mathbf{x}}_{i}\hat{\mathbf{W}}_{ji},\ \overline{\mathbf{x}}_{i}\hat{\mathbf{W}}_{ji}\right\}\). By substituting the right-hand-side of the above formula, our goal becomes:

\[\overline{\mathbf{y}}_{ji}\geq\max\left\{\underline{\mathbf{x}}_{i}\hat{ \mathbf{W}}_{ji},\ \overline{\mathbf{x}}_{i}\hat{\mathbf{W}}_{ji}\right\}\]

The formula above is equivalent to \(\overline{\mathbf{y}}_{ji}\geq\underline{\mathbf{x}}_{i}\hat{\mathbf{W}}_{ji} \wedge\overline{\mathbf{y}}_{ji}\geq\overline{\mathbf{x}}_{i}\hat{\mathbf{W}} _{ji}\), which is implied by \(\overline{\varphi}_{\texttt{Aff}}(\overline{\mathbf{y}},\underline{\mathbf{x} },\overline{\mathbf{x}},\dot{\mathbf{W}},\dot{\mathbf{b}})\). Therefore, we have proved that \(\overline{\varphi}_{\texttt{Aff}}(\overline{\mathbf{y}},\underline{\mathbf{x} },\overline{\mathbf{x}},\dot{\mathbf{W}},\dot{\mathbf{b}})\) implies \(\forall\mathbf{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}].\ \overline{\mathbf{y}}\geq\texttt{Affine}(\mathbf{x};\dot{\mathbf{W}},\dot{ \mathbf{b}})\).

**For the lower bound**, recall that by Definition 3.11, we have

\[\underline{\varphi}_{\texttt{Aff}}(\underline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\dot{\mathbf{W}},\dot{\mathbf{b}})\ \stackrel{{\text{def}}}{{=}}\ \bigwedge_{j}\underline{\mathbf{y}}_{j}=\sum_{i}\underline{\mathbf{y}}_{ji}+\dot{ \mathbf{b}}_{j}\ \wedge\ \underline{\mathbf{y}}\leq\dot{\mathbf{W}}\underline{\mathbf{x}}\ \wedge\ \underline{\mathbf{y}}\leq\dot{\mathbf{W}}\underline{\mathbf{x}}\]

our goal is to prove that \(\underline{\varphi}_{\texttt{Aff}}(\underline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\dot{\mathbf{W}},\dot{\mathbf{b}})\) implies

\[\forall\mathbf{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}].\ \underline{\mathbf{y}}\leq\texttt{Affine}(\mathbf{x};\dot{\mathbf{W}},\dot{\mathbf{b}})\]

by proving for each \(j\) where \(\underline{\mathbf{y}}_{j}\):

\[\underline{\mathbf{y}}_{j}\leq\min\left\{\texttt{Affine}(\mathbf{x};\dot{ \mathbf{W}},\dot{\mathbf{b}})_{j}\ \middle|\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}\]

By the definition of the Affine layer (Definition 3.10), we expand the formula above to

\[\underline{\mathbf{y}}_{j}\leq\min\left\{\sum_{i=0}^{m}\mathbf{x}_{i}\hat{ \mathbf{W}}_{ji}+\dot{\mathbf{b}}_{j}\ \middle|\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}\]

which is implied by the following formula

\[\underline{\mathbf{y}}_{j}\leq\sum_{i=0}^{m}\min\left\{\mathbf{x}_{i}\hat{ \mathbf{W}}_{ji}\ \middle|\ \underline{\mathbf{x}}_{i}\leq\mathbf{x}_{i}\leq\overline{\mathbf{x}}_{i}\right\}+\dot{\mathbf{b}}_{j}\]Because \(\underline{\varphi}_{\_{\_}{Aff}}(\underline{\dot{y}},\underline{\mathbf{x}}, \overline{\mathbf{x}},\dot{\mathbf{W}},\dot{\mathbf{b}})\) implies \(\underline{\dot{y}}_{\_}j=\sum_{\_}{i=0}^{m}\underline{\dot{\mathbf{y}}}_{\_}ji +\dot{\mathbf{b}}_{\_}j\), by substituting the left-hand-side \(\underline{\dot{y}}_{\_}j\) of the formula above with \(\sum_{\_}{i=0}^{m}\underline{\dot{\mathbf{y}}}_{\_}ji+\dot{\mathbf{b}}_{\_}j\), our goal becomes:

\[\sum_{\_}{i=0}^{m}\underline{\dot{\mathbf{y}}}_{\_}ji+\dot{\mathbf{b}}_{\_}j \leq\sum_{\_}{i=0}^{m}\min\left\{\mathbf{x}_{\_}i\dot{\mathbf{W}}_{\_}ji\ \big{|}\ \underline{\mathbf{x}}_{\_}i\leq\mathbf{x}_{\_}i\leq\overline{\mathbf{x}}_{\_}i\right\}+\dot{\mathbf{b}}_{\_}j\]

By subtracting \(\dot{\mathbf{b}}_{\_}j\) from both sides, our goal becomes:

\[\sum_{\_}{i=0}^{m}\underline{\dot{\mathbf{y}}}_{\_}ji\leq\sum_{\_}{i=0}^{m} \min\left\{\mathbf{x}_{\_}i\dot{\mathbf{W}}_{\_}ji\ \big{|}\ \underline{\mathbf{x}}_{\_}i\leq\mathbf{x}_{\_}i\leq\overline{\mathbf{x}}_{\_}i\right\}\]

Then we will prove for each \(i\):

\[\underline{\dot{\mathbf{y}}}_{\_}ji\leq\min\left\{\mathbf{x}_{\_}i\dot{\mathbf{W}}_{\_} ji\ \big{|}\ \underline{\mathbf{x}}_{\_}i\leq\mathbf{x}_{\_}i\leq\overline{\mathbf{x}}_{\_}i\right\}\]

Because \(\mathbf{x}_{\_}i\dot{\mathbf{W}}_{\_}ji\) changes monotonically with respect to \(\mathbf{x}_{\_}i\), \(\min\left\{\mathbf{x}_{\_}i\dot{\mathbf{W}}_{\_}ji\ \big{|}\ \underline{\mathbf{x}}_{\_}i\leq\mathbf{x}_{\_}i\leq\overline{\mathbf{x}}_{\_}i\right\}\) is taken when \(\mathbf{x}_{\_}i=\underline{\mathbf{x}}_{\_}i\) or \(\mathbf{x}_{\_}i=\overline{\mathbf{x}}_{\_}i\). In other words, \(\min\left\{\mathbf{x}_{\_}i\dot{\mathbf{W}}_{\_}ji\ \big{|}\ \underline{\mathbf{x}}_{\_}i\leq\mathbf{x}_{\_}i\leq\overline{\mathbf{x}}_{\_}i \right\}=\min\left\{\underline{\mathbf{x}}_{\_}i\dot{\mathbf{W}}_{\_}ji,\ \overline{\mathbf{x}}_{\_}i\dot{\mathbf{W}}_{\_}ji\ \right\}\). By substituting the right-hand-side of the above formula, our goal becomes:

\[\underline{\dot{\mathbf{y}}}_{\_}ji\leq\min\left\{\underline{\mathbf{x}}_{\_}i \dot{\mathbf{W}}_{\_}ji,\ \overline{\mathbf{x}}_{\_}i\dot{\mathbf{W}}_{\_}ji\ \right\}\]

The formula above is equivalent to \(\underline{\dot{\mathbf{y}}}_{\_}ji\leq\underline{\mathbf{x}}_{\_}i\dot{ \mathbf{W}}_{\_}ji\ \wedge\ \underline{\dot{\mathbf{y}}}_{\_}ji\leq\overline{\mathbf{x}}_{\_}i\dot{ \mathbf{W}}_{\_}ji\), which is implied by \(\underline{\varphi}_{\_}{\_}{\mathtt{Aff}}(\underline{\dot{y}},\underline{ \mathbf{x}},\overline{\mathbf{x}},\dot{\mathbf{W}},\dot{\mathbf{b}})\). Therefore, we have proved that \(\underline{\varphi}_{\_}{\mathtt{Aff}}(\underline{\dot{y}},\underline{ \mathbf{x}},\overline{\mathbf{x}},\dot{\mathbf{W}},\dot{\mathbf{b}})\) implies \(\forall\mathbf{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}]\). 

**Lemma A.6**.: \(\underline{\overline{\varphi}}_{\_}{\mathtt{Aff}}(\underline{\dot{y}}, \overline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\dot{ \mathbf{W}},\dot{\mathbf{b}})\ \stackrel{{\text{\tiny{def}}}}{{=}}\overline{\varphi}_{\_}{\mathtt{Aff}}(\overline{\dot{y}},\underline{ \mathbf{x}},\overline{\mathbf{x}},\dot{\mathbf{W}},\dot{\mathbf{b}})\ \ \wedge\ \underline{\varphi}_{\_}{\mathtt{Aff}}(\underline{\dot{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\dot{\mathbf{W}},\dot{\mathbf{b}})\) _defined in Definition 3.11 is implied by \(\ \overline{\mathbf{y}}=\max\left\{\mathtt{Affine}\big{(}\mathbf{x};\dot{ \mathbf{W}},\dot{\mathbf{b}}\big{)}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}\) \(\wedge\ \underline{\dot{\mathbf{y}}}=\min\left\{\mathtt{Affine}\big{(}\mathbf{x};\dot{ \mathbf{W}},\dot{\mathbf{b}}\big{)}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}\), hence captures the exact upper and lower bounds._

Proof.: **For the upper bound**, recall that by Definition 3.11, we have

\[\overline{\varphi}_{\_}{\mathtt{Aff}}(\overline{\dot{y}},\underline{\mathbf{x}}, \overline{\mathbf{x}},\dot{\mathbf{W}},\dot{\mathbf{b}})\ \stackrel{{\text{\tiny{def}}}}{{=}}\bigwedge_{\_}j\overline{\dot{y}}_{\_}j=\sum_{\_}i\dot{\mathbf{W}}_{\_}ji+\dot{\mathbf{b}}_{\_}j\ \ \wedge\ \overline{\dot{\mathbf{V}}}\geq\dot{\mathbf{W}}\underline{\mathbf{x}}\ \wedge\ \overline{\dot{\mathbf{V}}}\geq\dot{\mathbf{W}}\overline{\mathbf{x}}\]

Given constant input bounds \([\underline{\mathbf{x}},\overline{\mathbf{x}}]\), for any solution \((\overline{\mathbf{y}}^{*},\mathbf{W},\mathbf{b})\) to \(\overline{\mathbf{y}}=\max\left\{\mathtt{Affine}\big{(}\mathbf{x};\dot{ \mathbf{W}},\dot{\mathbf{b}}\big{)}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}\), we have the exact constant output upper bound defined as

\[\overline{\mathbf{y}}^{*}\stackrel{{\text{\tiny{def}}}}{{=}}\max \left\{\mathtt{Affine}\big{(}\mathbf{x};\dot{\mathbf{W}},\mathbf{b}\big{)}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}\]

By the definition of Affine layer (Definition 3.10), it is equivalent to

\[\overline{\mathbf{y}}_{\_}j\stackrel{{\text{\tiny{def}}}}{{=}}\sum_{\_}i\max \left\{\mathbf{x}_{\_}i\mathbf{W}_{\_}ji\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}+\mathbf{b}_{\_}j \tag{10}\]

To prove that \(\overline{\varphi}_{\_}{\mathtt{Aff}}\) captures the exact upper bound, we will show that by substituting \((\overline{\mathbf{y}}^{*},\underline{\mathbf{x}},\overline{\mathbf{x}}, \overline{\mathbf{W}},\dot{\mathbf{b}})\) in \(\overline{\varphi}_{\_}{\mathtt{Aff}}(\underline{\dot{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\dot{\mathbf{W}},\dot{\mathbf{b}})\), the instantiated formula \(\overline{\varphi}_{\_}{\mathtt{Aff}}(\overline{\mathbf{y}}^{*},\underline{ \mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{x}},\mathbf{W},\mathbf{b})\), as shown below, is satisfiable:

\[\overline{\varphi}_{\_}{\mathtt{Aff}}(\overline{\mathbf{y}}^{*},\underline{ \mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{x}},\mathbf{W},\mathbf{b}) \stackrel{{\text{\tiny{def}}}}{{=}}\bigwedge_{\_}j\overline{\dot{y}}_{\_}j=\sum_{\_}i\overline{\dot{\mathbf{V}}}_{\_}ji+\mathbf{b}_{\_}j\ \ \wedge\ \overline{\dot{\mathbf{V}}}\geq\dot{\mathbf{W}}\underline{\mathbf{x}}\ \wedge\ \overline{\dot{\mathbf{V}}}\geq\dot{\mathbf{W}}\overline{\mathbf{x}}\]

which is equivalent to prove the following formula for any index \(j\):

\[\overline{\mathbf{y}}_{\_}^{*}=\sum_{\_}i\overline{\dot{V}}_{\_}ji+\mathbf{b}_{\_}j\ \ \wedge\ \overline{\dot{\mathbf{V}}}\geq\dot{\mathbf{W}}\underline{\mathbf{x}}\ \wedge\ \overline{\dot{\mathbf{V}}}\geq\dot{\mathbf{W}}\overline{\mathbf{x}}\]

By substituting \(\overline{\mathbf{y}}_{\_}j^{*}\) defined in Equation 10 above, our goal becomes

\[\sum_{\_}i\max\left\{\mathbf{x}_{\_}i\mathbf{W}_{\_}ji\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}+\mathbf{b}_{\_}j=\sum_{\_}i\overline{\dot{V}}_{\_}ji+\mathbf{b}_{\_}j\ \ \wedge\ \ \overline{\dot{\mathbf{V}}}\geq\dot{\mathbf{W}}\underline{\mathbf{x}}\ \wedge\ \overline{\dot{\mathbf{V}}}\geq\dot{\mathbf{W}}\overline{\mathbf{x}}\]which is implied by

\[\max\big{\{}\textbf{x}_{i}\textbf{W}_{ji}\ \big{|}\ \textbf{x}\leq\textbf{x} \leq\overline{\textbf{x}}\big{\}}=\overline{\textbf{V}}_{ji}\ \wedge\ \overline{\textbf{V}}_{ji}\geq\textbf{W}_{ji}\underline{\textbf{x}}_{i}\ \wedge\ \overline{\textbf{V}}_{ji}\geq\textbf{W}_{ji}\overline{\textbf{x}}_{i}\]

If \(\textbf{W}_{ji}\geq 0\), \(\overline{\textbf{V}}_{ji}^{*}\stackrel{{\text{\tiny def}}}{{=}} \textbf{W}_{ji}\overline{\textbf{x}}_{i}\) is a solution to this formula; otherwise \(\overline{\textbf{V}}_{ji}^{*}\stackrel{{\text{\tiny def}}}{{=}} \textbf{W}_{ji}\underline{\textbf{x}}_{i}\) is a solution to this formula. Hence, we have proved that \(\overline{\varphi}_{k\neq t}(\overline{\textbf{y}}^{*},\underline{\textbf{x}},\overline{\textbf{x}},\overline{\textbf{W}},\textbf{b})\) is satisfiable and \(\overline{\varphi}_{k\neq t}(\hat{\textbf{y}},\underline{\textbf{x}}, \overline{\textbf{x}},\overline{\textbf{W}},\hat{\textbf{b}})\) captures the exact upper bound.

**For the lower bound**, recall that by Definition 3.11, we have

\[\underline{\varphi}_{k\neq t}(\hat{\textbf{y}},\underline{\textbf{x}}, \overline{\textbf{x}},\hat{\textbf{W}},\hat{\textbf{b}})\ \stackrel{{\text{\tiny def}}}{{=}}\bigwedge_{j}\hat{\textbf{y}}_{j}=\sum_{i}\hat{\textbf{Y}}_{ji}+\hat{\textbf{b}}_{j}\ \wedge\ \hat{\textbf{Y}}\leq\hat{\textbf{W}}\underline{\textbf{x}}\ \wedge\ \hat{\textbf{Y}}\leq\hat{\textbf{W}}\overline{\textbf{x}}\]

Given constant input bounds \([\underline{\textbf{x}},\overline{\textbf{x}}]\), for any solution \((\underline{\textbf{y}}^{*},\textbf{W},\textbf{b})\) to \(\hat{\textbf{y}}=\min\big{\{}\texttt{Affine}\big{(}\textbf{x};\hat{\textbf{W}},\hat{\textbf{b}}\big{)}\ \big{|}\ \underline{\textbf{x}}\leq\textbf{x}\leq\overline{\textbf{x}}\big{\}}\), we have the exact constant output lower bound defined as

\[\underline{\textbf{y}}^{*}\stackrel{{\text{\tiny def}}}{{=}} \min\big{\{}\texttt{Affine}\big{(}\textbf{x};\textbf{W},\textbf{b}\big{)}\ \big{|}\ \underline{\textbf{x}}\leq\textbf{x}\leq\overline{\textbf{x}}\big{\}}\]

By the definition of Affine layer (Definition 3.10), it is equivalent to

\[\underline{\textbf{y}}_{j}^{*}\stackrel{{\text{\tiny def}}}{{=}} \sum_{i}\min\big{\{}\textbf{x}_{i}\textbf{W}_{ji}\ \big{|}\ \underline{\textbf{x}}\leq\textbf{x}\leq\overline{\textbf{x}}\big{\}}+\textbf{b}_{j} \tag{11}\]

To prove that \(\underline{\varphi}_{k\neq t}\) captures the exact lower bound, we will show that by substituting \((\underline{\textbf{y}}^{*},\underline{\textbf{x}},\overline{\textbf{x}}, \overline{\textbf{W}},\textbf{b})\) in \(\underline{\varphi}_{k\neq t}(\underline{\textbf{y}},\underline{\textbf{x}}, \overline{\textbf{x}},\overline{\textbf{W}},\hat{\textbf{b}})\), the instantiated formula \(\underline{\varphi}_{k\neq t}(\underline{\textbf{y}}^{*},\underline{\textbf{x}},\overline{\textbf{x}},\overline{\textbf{W}},\textbf{b})\), as shown below, is satisfiable:

\[\underline{\varphi}_{k\neq t}(\underline{\textbf{y}}^{*},\underline{\textbf{x}}, \overline{\textbf{x}},\overline{\textbf{W}},\textbf{b})\stackrel{{ \text{\tiny def}}}{{=}}\bigwedge_{j}\underline{\textbf{y}}_{j}^{*}=\sum_{i} \hat{\textbf{Y}}_{ji}+\textbf{b}_{j}\ \wedge\ \hat{\textbf{Y}}\leq\textbf{W}\underline{\textbf{x}}\ \wedge\ \hat{\textbf{Y}}\leq\textbf{W}\overline{\textbf{x}}\]

which is equivalent to prove the following formula for any index \(j\):

\[\underline{\textbf{y}}_{j}^{*}=\sum_{i}\hat{\textbf{Y}}_{ji}+\textbf{b}_{j}\ \wedge\ \hat{\textbf{Y}}\leq\textbf{W}\underline{\textbf{x}}\ \wedge\ \hat{\textbf{Y}}\leq\textbf{W}\overline{\textbf{x}}\]

By substituting \(\underline{\textbf{y}}_{j}^{*}\) defined in Equation 11 above, our goal becomes

\[\sum_{i}\min\big{\{}\textbf{x}_{i}\textbf{W}_{ji}\ \big{|}\ \underline{\textbf{x}}\leq\textbf{x}\leq\overline{\textbf{x}}\big{\}}+ \textbf{b}_{j}=\sum_{i}\hat{\textbf{Y}}_{ji}+\textbf{b}_{j}\ \wedge\ \hat{\textbf{Y}}\leq\textbf{W}\underline{\textbf{x}}\ \wedge\ \hat{\textbf{Y}}\leq\textbf{W}\overline{\textbf{x}}\]

which is implied by

\[\min\big{\{}\textbf{x}_{i}\textbf{W}_{ji}\ \big{|}\ \underline{\textbf{x}}\leq\textbf{x}\leq\overline{\textbf{x}}\big{\}}= \hat{\textbf{Y}}_{ji}\ \wedge\ \hat{\textbf{Y}}_{ji}\leq\textbf{W}_{ji}\underline{\textbf{x}}_{i}\ \wedge\ \hat{\textbf{Y}}_{ji}\leq\textbf{W}_{ji}\overline{\textbf{x}}_{i}\]

If \(\textbf{W}_{ji}\geq 0\), \(\textbf{Y}_{ji}^{*}\stackrel{{\text{\tiny def}}}{{=}}\textbf{W}_{ji}\underline{\textbf{x}}_{i}\) is a solution to this formula; otherwise \(\underline{\textbf{V}}_{ji}^{*}\stackrel{{\text{\tiny def}}}{{=}} \textbf{W}_{ji}\overline{\textbf{x}}_{i}\) is a solution to this formula. Hence, we have proved that \(\underline{\varphi}_{k\neq t}(\underline{\textbf{y}}^{*},\underline{\textbf{x}},\overline{\textbf{x}},\overline{\textbf{W}},\textbf{b})\) is satisfiable and \(\underline{\varphi}_{k\neq t}(\hat{\textbf{y}},\underline{\textbf{x}}, \overline{\textbf{x}},\overline{\textbf{W}},\hat{\textbf{b}})\) captures the exact lower bound. 

**Theorem 3.12**.: _Definition 3.11 is a Parametric Linear Relaxation for the Affine layer with constant input bounds \([\underline{\textbf{x}},\overline{\textbf{x}}]\) that captures the exact lower and upper bounds._

Proof.: By Lemma A.4 and Lemma A.5, we proved that \(\overline{\varphi}_{k\neq t}(\hat{\textbf{y}},\hat{\textbf{y}},\underline{ \textbf{x}},\overline{\textbf{x}},\hat{\textbf{W}},\hat{\textbf{b}})\) is a poly-size linear formula that implies \(\forall\textbf{x}\in[\underline{\textbf{x}},\overline{\textbf{x}}]\). \(\texttt{Affine}\big{(}\textbf{x};\hat{\textbf{W}},\hat{\textbf{b}}\big{)}\in[\hat{\textbf{y}},\hat{\textbf{y}}]\), hence is a Parametric Linear Relaxation (Definition 3.3) for the Affine layer. By Lemma A.6, we proved that \(\overline{\varphi}_{k\neq t}(\hat{\textbf{y}},\hat{\textbf{y}},\underline{ \textbf{x}},\overline{\textbf{x}},\hat{\textbf{W}},\hat{\textbf{b}})\) is implied by \(\hat{\textbf{y}}=\max\big{\{}\texttt{Affine}\big{(}\textbf{x};\hat{\textbf{W}},\hat{\textbf{b}}\big{)}\ \big{|}\ \underline{\textbf{x}}\leq\textbf{x}\leq\overline{\textbf{x}}\big{\}}\wedge\hat{ \textbf{y}}=\min\big{\{}\texttt{Affine}\big{(}\textbf{x};\hat{\textbf{W}},\hat{ \textbf{b}}\big{)}\ \big{|}\ \underline{\textbf{x}}\leq\textbf{x}\leq\overline{\textbf{x}}\big{\}}\), hence captures the exact upper and lower bounds.

### Proofs for Parametric Linear Relaxation of Affine layers with variable input bounds

**Lemma A.7**.: \(\overline{\Sigma}_{\texttt{Aff}}(\underline{\mathbf{y}},\overline{\mathbf{y}}, \underline{\mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{W}},\dot{\mathbf{ b}})\) _defined in Definition 3.13 is a poly-size linear formula._

Proof.: As seen in Definition 3.13, \(\overline{\varphi}_{\texttt{Aff}}(\underline{\mathbf{y}},\overline{\mathbf{y}}, \underline{\mathbf{x}},\underline{\mathbf{x}},\overline{\mathbf{W}},\dot{ \mathbf{b}})\) is a linear formula. Let \(m\) and \(n\) be the number of input and output dimensions of the Affine layer. \(\overline{\Sigma}_{\texttt{Aff}}(\underline{\mathbf{y}},\underline{\mathbf{y}},\underline{\mathbf{x}},\underline{\mathbf{x}},\overline{\mathbf{x}},\overline {\mathbf{W}},\dot{\mathbf{b}})\) has \(2n\) linear constraints over \(2m+3n\) variables (\(\underline{\mathbf{x}}\in\mathbb{R}^{m}\), \(\overline{\mathbf{x}}\in\mathbb{R}^{m}\), \(\dot{\mathbf{b}}\in\mathbb{R}^{n}\), \(\underline{\mathbf{y}}\in\mathbb{R}^{n}\) and \(\overline{\mathbf{y}}\in\mathbb{R}^{n}\)). Hence, \(\overline{\Sigma}_{\texttt{Aff}}(\underline{\mathbf{y}},\underline{\mathbf{y} },\underline{\mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{W}},\dot{ \mathbf{b}})\) is a poly-size linear formula whose size is polynomial in the number of input and output dimensions of the Affine layer. 

**Lemma A.8**.: \(\overline{\Sigma}_{\texttt{Aff}}(\underline{\mathbf{y}},\underline{\mathbf{y} },\underline{\mathbf{x}},\underline{\mathbf{x}},\overline{\mathbf{W}},\dot{ \mathbf{b}})\stackrel{{\text{def}}}{{=}}\overline{\varphi}_{ \texttt{Aff}}(\underline{\mathbf{y}},\underline{\mathbf{x}},\underline{\mathbf{ x}},\overline{\mathbf{W}},\dot{\mathbf{b}})\wedge\underline{\varphi}_{\texttt{Aff}}( \underline{\mathbf{y}},\underline{\mathbf{x}},\underline{\mathbf{x}}, \overline{\mathbf{W}},\dot{\mathbf{b}})\) _defined in Definition 3.13 implies that \(\forall\mathbf{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}]\)._ Affine\((\mathbf{x};\overline{\mathbf{W}},\dot{\mathbf{b}})\in[\underline{\mathbf{y}},\underline{\mathbf{y}}]\)_._

Proof.: **For the upper bound**, recall that by Definition 3.13, we have

\[\overline{\varphi}_{\texttt{Aff}}(\overline{\mathbf{y}},\underline{\mathbf{x }},\underline{\mathbf{x}},\overline{\mathbf{W}},\dot{\mathbf{b}})\stackrel{{ \text{def}}}{{=}}\bigwedge_{j}\overline{\mathbf{y}}_{j}=\sum_{i} \overline{\mathbf{V}}_{ji}+\dot{\mathbf{b}}_{j}\text{ where }\begin{cases} \overline{\mathbf{V}}_{ji}\stackrel{{\text{def}}}{{=}}\overline{ \mathbf{x}}_{i}\mathbf{W}_{ji}\text{ if }\mathbf{W}_{ji}\geq 0\\ \overline{\mathbf{V}}_{ji}\stackrel{{\text{def}}}{{=}}\underline{ \mathbf{x}}_{i}\mathbf{W}_{ji}\text{ otherwise }\end{cases}\]

Our goal is to prove that \(\overline{\varphi}_{\texttt{Aff}}(\overline{\mathbf{y}},\underline{\mathbf{x}}, \underline{\mathbf{x}},\overline{\mathbf{W}},\dot{\mathbf{b}})\) implies

\[\forall\mathbf{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}].\; \overline{\mathbf{y}}\geq\texttt{Affine}(\mathbf{x};\overline{\mathbf{W}}, \dot{\mathbf{b}})\]

We prove the above formula by proving for each \(\overline{\mathbf{y}}_{j}\):

\[\overline{\mathbf{y}}_{j}\geq\max\left\{\texttt{Affine}(\mathbf{x};\overline{ \mathbf{W}},\dot{\mathbf{b}})_{j}\;\middle|\;\underline{\mathbf{x}}\leq\mathbf{ x}\leq\overline{\mathbf{x}}\right\}\]

By the definition of the Affine layer (Definition 3.10), we expand the formula above to

\[\overline{\mathbf{y}}_{j}\geq\max\left\{\sum_{i=0}^{m}\mathbf{x}_{i}\mathbf{ W}_{ji}+\dot{\mathbf{b}}_{j}\;\middle|\;\underline{\mathbf{x}}\leq\mathbf{ x}\leq\overline{\mathbf{x}}\right\}\]

which is implied by the following formula

\[\overline{\mathbf{y}}_{j}\geq\sum_{i=0}^{m}\max\left\{\mathbf{x}_{i}\mathbf{ W}_{ji}\;\middle|\;\underline{\mathbf{x}}_{i}\leq\mathbf{x}_{i}\leq\overline{ \mathbf{x}}_{i}\right\}+\dot{\mathbf{b}}_{j}\]

By Definition 3.13, \(\overline{\varphi}_{\texttt{Aff}}(\overline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{W}},\dot{\mathbf{b}})\) implies \(\overline{\mathbf{y}}_{j}=\sum_{i=0}^{m}\overline{\mathbf{V}}_{ji}+\dot{ \mathbf{b}}_{j}\). By substituting the left-hand-side \(\overline{\mathbf{y}}_{j}\) of the formula above with \(\sum_{i=0}^{m}\overline{\mathbf{V}}_{ji}+\dot{\mathbf{b}}_{j}\), our goal becomes:

\[\sum_{i=0}^{m}\overline{\mathbf{V}}_{ji}+\dot{\mathbf{b}}_{j}\geq\sum_{i=0}^{ m}\max\left\{\mathbf{x}_{i}\mathbf{W}_{ji}\;\middle|\;\underline{\mathbf{x}}_{i}\leq \mathbf{x}_{i}\leq\overline{\mathbf{x}}_{i}\right\}+\dot{\mathbf{b}}_{j}\]

By subtracting \(\dot{\mathbf{b}}_{j}\) from both sides, our goal becomes:

\[\sum_{i=0}^{m}\overline{\mathbf{V}}_{ji}\geq\sum_{i=0}^{m}\max\left\{\mathbf{x }_{i}\mathbf{W}_{ji}\;\middle|\;\underline{\mathbf{x}}_{i}\leq\mathbf{x}_{i} \leq\overline{\mathbf{x}}_{i}\right\}\]

Then we prove the above formula by proving for each \(i\):

\[\overline{\mathbf{V}}_{ji}\geq\max\left\{\mathbf{x}_{i}\mathbf{W}_{ji}\; \middle|\;\underline{\mathbf{x}}_{i}\leq\mathbf{x}_{i}\leq\overline{\mathbf{x}}_{ i}\right\}\]

For the left-hand-side, we have \(\overline{\mathbf{V}}_{ji}\stackrel{{\text{def}}}{{=}}\begin{cases} \overline{\mathbf{x}}_{i}\mathbf{W}_{ji}\;\

[MISSING_PAGE_FAIL:22]

Proof.: **For the upper bound**, recall that by Definition 3.13, we have

\[\overline{\varphi}_{\texttt{Aff}}(\bar{\mathbf{y}},\underline{\mathbf{x}},\bar{ \mathbf{x}},\mathbf{W},\hat{\mathbf{b}})\stackrel{{\text{def}}}{{=} }\bigwedge_{j}\bar{\mathbf{y}}_{j}=\sum_{i}\bar{\mathbf{V}}_{ji}+\hat{\mathbf{ b}}_{j}\text{ where }\begin{cases}\bar{\mathbf{V}}_{ji}\stackrel{{\text{def}}}{{=}}\bar{\mathbf{x}}_{i} \mathbf{W}_{ji}\text{ if }\mathbf{W}_{ji}\geq 0\\ \bar{\mathbf{V}}_{ji}\stackrel{{\text{def}}}{{=}}\underline{ \mathbf{x}}_{i}\mathbf{W}_{ji}\text{ otherwise }\end{cases}\]

For any solution \((\overline{\mathbf{y}}^{*},\underline{\mathbf{x}},\overline{\mathbf{x}}, \mathbf{b})\) to \(\bar{\mathbf{y}}=\max\left\{\texttt{Affine}\big{(}\mathbf{x};\mathbf{W},\hat{ \mathbf{b}}\big{)}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}\), we have the exact constant output upper bound defined as

\[\overline{\mathbf{y}}^{*}\stackrel{{\text{def}}}{{=}}\max\left\{ \texttt{Affine}\big{(}\mathbf{x};\mathbf{W},\mathbf{b}\big{)}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}\]

By the definition of Affine layer (Definition 3.10), it is equivalent to

\[\overline{\mathbf{y}}_{j}^{*}\stackrel{{\text{def}}}{{=}}\sum_{i }\max\left\{\mathbf{x}_{i}\mathbf{W}_{ji}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}+ \mathbf{b}_{j} \tag{12}\]

To prove that \(\overline{\varphi}_{\texttt{Aff}}\) captures the exact upper bound, we will show that \((\overline{\mathbf{y}}^{*},\underline{\mathbf{x}},\overline{\mathbf{x}}, \mathbf{b})\) is a solution to \(\overline{\varphi}_{\texttt{Aff}}(\bar{\mathbf{y}},\underline{\mathbf{x}}, \overline{\mathbf{x}},\mathbf{W},\hat{\mathbf{b}})\). In other words, the following instantiated formula \(\overline{\varphi}_{\texttt{Aff}}(\overline{\mathbf{y}}^{*},\underline{ \mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{x}},\mathbf{W},\mathbf{b})\) is true:

\[\overline{\varphi}_{\texttt{Aff}}(\overline{\mathbf{y}}^{*},\underline{ \mathbf{x}},\overline{\mathbf{x}},\mathbf{W},\mathbf{b})\stackrel{{ \text{def}}}{{=}}\bigwedge_{j}\overline{\mathbf{y}}_{j}^{*}=\sum_{i}\overline {\mathbf{V}}_{ji}+\mathbf{b}_{j}\]

where \(\overline{\mathbf{V}}_{ji}\stackrel{{\text{def}}}{{=}}\overline{ \mathbf{x}}_{j}\mathbf{W}_{ji}\) if \(\mathbf{W}_{ji}\geq 0\), otherwise \(\overline{\mathbf{V}}_{ji}\stackrel{{\text{def}}}{{=}}\underline{ \mathbf{x}}_{j}\mathbf{W}_{ji}\).

This formula is equivalent to prove the following formula for any index \(j\):

\[\overline{\mathbf{y}}_{j}^{*}=\sum_{i}\overline{\mathbf{V}}_{ji}+\mathbf{b}_{j}\]

By substituting \(\overline{\mathbf{y}}_{j}^{*}\) defined in Equation 12 above, our goal becomes

\[\sum_{i}\max\left\{\mathbf{x}_{i}\mathbf{W}_{ji}\ \big{|}\ \underline{ \mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}+\mathbf{b}_{j}= \sum_{i}\overline{\mathbf{V}}_{ji}+\mathbf{b}_{j}\]

which is implied by

\[\max\left\{\mathbf{x}_{i}\mathbf{W}_{ji}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}= \overline{\mathbf{V}}_{ji}\]

If \(\mathbf{W}_{ji}\geq 0\), \(\max\left\{\mathbf{x}_{i}\mathbf{W}_{ji}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}= \overline{\mathbf{x}}_{j}\mathbf{W}_{ji}=\overline{\mathbf{V}}_{ji}\), the above formula is true; Otherwise, \(\max\left\{\mathbf{x}_{i}\mathbf{W}_{ji}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}= \underline{\mathbf{x}}_{j}\mathbf{W}_{ji}=\overline{\mathbf{V}}_{ji}\), the above formula is true. Hence, we have proved that \(\overline{\varphi}_{\texttt{Aff}}(\overline{\mathbf{y}}^{*},\underline{ \mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{W}},\mathbf{b})\) is true and \(\overline{\varphi}_{\texttt{Aff}}(\bar{\mathbf{y}},\underline{\mathbf{x}}, \overline{\mathbf{x}},\mathbf{W},\hat{\mathbf{b}})\) captures the exact upper bound.

**For the lower bound**, recall that by Definition 3.13, we have

\[\underline{\varphi}_{\texttt{Aff}}(\bar{\mathbf{y}},\underline{\mathbf{x}}, \overline{\mathbf{x}},\mathbf{W},\hat{\mathbf{b}})\stackrel{{\text{def}}}{{=}} \bigwedge_{j}\underline{\mathbf{y}}_{j}=\sum_{i}\underline{\mathbf{V}}_{ji}+ \hat{\mathbf{b}}_{j}\text{ where }\begin{cases}\underline{\mathbf{V}}_{ji}\stackrel{{\text{def}}}{{=}}\underline{\mathbf{x}}_{j}\mathbf{W}_{ji}\text{ if }\mathbf{W}_{ji}\geq 0\\ \underline{\mathbf{V}}_{ji}\stackrel{{\text{def}}}{{=}}\underline{ \mathbf{x}}_{j}\mathbf{W}_{ji}\text{ otherwise }\end{cases}\]

For any solution \((\mathbf{y}^{*},\underline{\mathbf{x}},\overline{\mathbf{x}},\mathbf{b})\) to \(\underline{\mathbf{y}}=\min\left\{\texttt{Affine}\big{(}\mathbf{x};\mathbf{W},\hat{\mathbf{b}}\big{)}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}\), we have the exact constant output lower bound defined as

\[\underline{\mathbf{y}}^{*}\stackrel{{\text{def}}}{{=}}\min\left\{ \texttt{Affine}\big{(}\mathbf{x};\mathbf{W},\mathbf{b}\big{)}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}\]

By the definition of Affine layer (Definition 3.10), it is equivalent to

\[\underline{\mathbf{y}}_{j}^{*}\stackrel{{\text{def}}}{{=}}\sum_{i} \min\left\{\mathbf{x}_{i}\mathbf{W}_{ji}\ \big{|}\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}+\mathbf{b}_{j} \tag{13}\]

To prove that \(\underline{\varphi}_{\texttt{Aff}}\) captures the exact lower bound, we will show that \((\underline{\mathbf{y}}^{*},\underline{\mathbf{x}},\overline{\mathbf{x}},\mathbf{b})\) is a solution to \(\underline{\varphi}_{\texttt{Aff}}(\underline{\mathbf{y}},\underline{\mathbf{x}},\underline{\mathbf{x}},\mathbf{W},\hat{\mathbf{b}})\). In other words, the following instantiated formula \(\underline{\varphi}_{\texttt{Aff}}(\underline{\mathbf{y}}^{*},\underline{ \mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{W}},\mathbf{b})\) is true:

\[\underline{\varphi}_{\texttt{Aff}}(\underline{\mathbf{y}}^{*},\underline{ \mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{x}},\mathbf{W},\mathbf{b})\stackrel{{\text{def}}}{{=}}\bigwedge_{j}\underline{\mathbf{y}}_{j}^{*}=\sum_{i}\underline{\mathbf{V}}_{ji}+\mathbf{b}_{j}\]where \(\underline{\mathbf{V}}_{ji}\stackrel{{\text{\tiny{def}}}}{{=}} \underline{\mathbf{x}}_{j}\mathbf{W}_{ji}\) if \(\mathbf{W}_{ji}\geq 0\), otherwise \(\underline{\mathbf{V}}_{ji}\stackrel{{\text{\tiny{def}}}}{{=}} \overline{\mathbf{x}}_{j}\mathbf{W}_{ji}\).

This formula is equivalent to prove the following formula for any index \(j\):

\[\underline{\mathbf{y}}_{j}^{*}=\sum_{i}\underline{\mathbf{V}}_{ji}+\mathbf{b}_ {j}\]

By substituting \(\underline{\mathbf{y}}_{j}^{*}\) defined in Equation 13 above, our goal becomes

\[\sum_{i}\min\left\{\mathbf{x}_{i}\mathbf{W}_{ji}\ \middle|\ \underline{ \mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}+\mathbf{b}_{j}= \sum_{i}\underline{\mathbf{V}}_{ji}+\mathbf{b}_{j}\]

which is implied by

\[\min\left\{\mathbf{x}_{i}\mathbf{W}_{ji}\ \middle|\ \underline{ \mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}=\underline{ \mathbf{V}}_{ji}\]

If \(\mathbf{W}_{ji}\geq 0\), \(\min\left\{\mathbf{x}_{i}\mathbf{W}_{ji}\ \middle|\ \underline{\mathbf{x}}\leq \mathbf{x}\leq\overline{\mathbf{x}}\right\}=\underline{\mathbf{x}}_{j} \mathbf{W}_{ji}=\underline{\mathbf{V}}_{ji}\), the above formula is true; Otherwise, \(\min\left\{\mathbf{x}_{i}\mathbf{W}_{ji}\ \middle|\ \underline{\mathbf{x}}\leq \mathbf{x}\leq\overline{\mathbf{x}}\right\}=\overline{\mathbf{x}}_{j} \mathbf{W}_{ji}=\underline{\mathbf{V}}_{ji}\), the above formula is true. Hence, we have proved that \(\underline{\mathbf{z}}_{\text{\tiny{aff}}}(\underline{\mathbf{y}}^{*}, \underline{\mathbf{x}},\overline{\mathbf{x}},\overline{\mathbf{W}},\mathbf{b})\) is true and \(\underline{\mathbf{z}}_{\text{\tiny{aff}}}(\underline{\mathbf{y}},\underline{ \mathbf{z}},\overline{\mathbf{x}},\overline{\mathbf{X}},\mathbf{W},\dot{ \mathbf{b}})\) captures the exact lower bound. 

**Theorem 3.14**.: _Definition 3.13 is a Parametric Linear Relaxation for the Affine layer with variable input bounds \([\underline{\mathbf{x}},\overline{\mathbf{x}}]\) that captures the exact upper and lower bounds._

Proof.: By Lemma A.7 and Lemma A.8, we proved that \(\overline{\underline{\mathbf{z}}}_{\text{\tiny{aff}}}(\underline{\mathbf{y}}, \overline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\overline{ \mathbf{W}},\dot{\mathbf{b}})\) is a poly-size linear formula that implies \(\forall\mathbf{x}\in[\underline{\mathbf{x}},\overline{\mathbf{x}}]\). \(\mathtt{Affine}\left(\mathbf{x};\mathbf{W},\dot{\mathbf{b}}\right)\in[ \underline{\mathbf{y}},\overline{\mathbf{y}}]\), hence is a Parametric Linear Relaxation (Definition 3.3) for the Affine layer. By Lemma A.9, we proved that \(\overline{\underline{\mathbf{z}}}_{\text{\tiny{aff}}}(\underline{\mathbf{y}}, \underline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}},\overline {\mathbf{W}},\dot{\mathbf{b}})\) is implied by \(\overline{\mathbf{y}}=\max\left\{\mathtt{Affine}\left(\mathbf{x};\mathbf{W}, \dot{\mathbf{b}}\right)\ \middle|\ \underline{\mathbf{x}}\leq\mathbf{x}\leq \overline{\mathbf{x}}\right\}\wedge\underline{\mathbf{y}}=\min\left\{\mathtt{ Affine}\left(\mathbf{x};\mathbf{W},\dot{\mathbf{b}}\right)\ \middle|\ \underline{\mathbf{x}}\leq\mathbf{x}\leq\overline{\mathbf{x}}\right\}\), hence captures the exact upper and lower bounds. 

## Appendix B General provable editing via Parametric Linear Relaxation

In this section we present our approach for solving the general provable editing problem (Definition 1.1):

**Definition B.1**.: Given Parametric Linear Relaxation \(\overline{\underline{\mathbf{z}}}_{\mathcal{N}}(\underline{\mathbf{y}}, \overline{\mathbf{y}},\underline{\mathbf{x}},\overline{\mathbf{x}};\dot{ \mathbf{\theta}})\) for DNN \(\mathcal{N}\) with parameters \(\theta\), an input polytope \(\mathrm{P}\stackrel{{\text{\tiny{def}}}}{{=}}\left\{\mathbf{x} \middle|\ \mathbf{D}\mathbf{x}\leq\mathbf{e}\right\}\), an output polytope \(\mathrm{Q}\stackrel{{\text{\tiny{def}}}}{{=}}\left\{\mathbf{y} \middle|\ \mathbf{A}\mathbf{y}\leq\mathbf{b}\right\}\). Given a hyperparameter \(k\) so that we only edit the DNN layers \(\mathcal{N}^{(k:L)}\) starting from the \(k\)th layer. Then a solution to the following linear program is a solution to the provable editing problem of Definition 1.1:

\[\min\|\dot{\mathbf{\theta}}-\theta\|\quad\text{s.t.}\quad\overline{\underline{ \mathbf{z}}}_{\mathcal{N}^{(k:L)}}(\underline{\mathbf{y}},\overline{\mathbf{y}}, \underline{\mathbf{x}}^{(k)},\overline{\mathbf{x}}^{(k)},\dot{\mathbf{\theta}}^{(k:L)})\wedge\psi(\underline{\mathbf{y}},\overline{\mathbf{y}},\mathbf{A},\mathbf{b}) \tag{14}\]

where a **sound constant interval bound**\(\left[\underline{\mathbf{x}}^{(k)},\overline{\mathbf{x}}^{(k)}\right]\) for the input \(\mathbf{x}^{(k)}\) to the slice \(\mathcal{N}^{(k:L)}\) to edit is computed by a sound bound propagation tool of DNNs like DeepPoly[39], auto_LiRPA[50] and DeepT[4]:

\[\left[\underline{\mathbf{x}}^{(k)},\overline{\mathbf{x}}^{(k)}\right] \stackrel{{\text{\tiny{def}}}}{{=}}\mathtt{Compute\_Bound}_{ \mathcal{N}^{(0:k)}}\big{(}\mathrm{P};\theta^{(0:k)}\big{)} \tag{15}\]

The **output constraint**\(\psi(\underline{\mathbf{y}},\overline{\mathbf{y}},\mathbf{A},\mathbf{b})\) encodes \(\mathbf{A}\mathbf{y}\leq\mathbf{b}\) for all \(\mathbf{y}\in\left[\underline{\mathbf{y}},\overline{\mathbf{y}}\right]\) using the parametric output bounds \(\left[\underline{\mathbf{y}},\overline{\mathbf{y}}\right]\) is defined as follows:

\[\psi(\underline{\mathbf{y}},\overline{\mathbf{y}},\mathbf{A},\mathbf{b}) \stackrel{{\text{\tiny{def}}}}{{=}}\bigwedge_{i=0}^{m}\sum_{j=0}^{n} \mathbf{E}_{ij}\leq\mathbf{b}_{i}\quad\text{where}\quad\mathbf{E}_{ij} \stackrel{{\text{\tiny{def}}}}{{=}}\begin{cases}\mathbf{A}_{ij} \overline{\mathbf{y}}_{j}&\mathbf{A}_{ij}\geq 0\\ \mathbf{A}_{ij}\underline{\mathbf{y}}_{j}&\text{otherwise}\end{cases} \tag{16}\]

### Provable editing for multiple properties via Parametric Linear Relaxation

Given \(n\) properties defined by input polytopes \(\mathrm{P}_{i}\stackrel{{\text{\tiny{def}}}}{{=}}\big{\{}\mathbf{x} \mid\!\mathbf{D}\mathbf{x}\leq{}_{i}\mathbf{e}\big{\}}\), and output polytopes \(\mathrm{Q}_{i}\stackrel{{\text{\tiny{def}}}}{{=}}\big{\{}\mathbf{y} \mid{}_{i}\mathbf{A}\mathbf{y}\leq{}_{i}\mathbf{b}\big{\}}\). This work can directly generalize to provably edit multiple properties:

\[\min\|\hat{\boldsymbol{\theta}}-\theta\|\quad\text{s.t.}\quad\bigwedge_{0\leq i <n}\forall\mathbf{x}\in\mathrm{P}_{i}\boldsymbol{\cdot}\mathcal{N}(\mathbf{x}; \hat{\boldsymbol{\theta}})\in\mathrm{Q}_{i} \tag{17}\]

In particular, most of our experiments in Section 4 involve editing DNNs for multiple properties altogether. Here we present our formulation for provably editing multiple properties, which encodes the conjunction of the constraints for each property.

\[\min\|\hat{\boldsymbol{\theta}}-\theta\|\quad\text{s.t.}\quad\bigwedge_{0\leq i <n}\overline{\mathbb{Z}}_{\mathcal{N}^{(k:L)}}\big{(}i\underline{\mathbf{y}},i\underline{\mathbf{y}},i\underline{\mathbf{x}}^{(k)},i\underline{\mathbf{x} }^{(k)},i\underline{\boldsymbol{\theta}}^{(k:L)}\big{)}\wedge\psi({}_{i} \underline{\mathbf{y}},i\underline{\mathbf{y}},i\underline{\mathbf{A}},{}_{i }\mathbf{b}) \tag{18}\]

where \(\big{[}i\underline{\mathbf{x}}^{(k)},i\underline{\mathbf{x}}^{(k)}\big{]} \stackrel{{\text{\tiny{def}}}}{{=}}\text{Compute\_Bound}_{ \mathcal{N}^{(0:k)}}\big{(}\mathrm{P}_{i};\theta^{(0:k)}\big{)}\).

### Provable editing for disjunctive properties via Parametric Linear Relaxation

Given a property defined by an input polytope \(\mathrm{P}\stackrel{{\text{\tiny{def}}}}{{=}}\big{\{}\mathbf{x} \mid\mathbf{D}\mathbf{x}\leq\mathbf{e}\big{\}}\), and \(n\) possible output polytopes \(\mathrm{Q}_{j}\stackrel{{\text{\tiny{def}}}}{{=}}\big{\{}\mathbf{y} \mid{}_{j}\mathbf{A}\mathbf{y}\leq{}_{j}\mathbf{b}\big{\}}\), this work can directly generalize to provably edit disjunctive property:

\[\min\|\hat{\boldsymbol{\theta}}-\theta\|\quad\text{s.t.}\quad\forall\mathbf{x} \in\mathrm{P}_{j}\boldsymbol{\cdot}\bigvee_{0\leq j<n}\mathcal{N}(\mathbf{x}; \hat{\boldsymbol{\theta}})\in\mathrm{Q}_{j} \tag{19}\]

by using a mixed-integer linear programming (MILP) solver. In particular, some properties in VNN-COMP (Section 4.1) involve editing DNNs for such disjunctive properties. Here we present our formulation:

\[\min\|\hat{\boldsymbol{\theta}}-\theta\|\quad\text{s.t.}\quad\overline{ \mathbb{Z}}_{\mathcal{N}^{(k:L)}}\big{(}\underline{\mathbf{y}},\underline{ \mathbf{y}},i\underline{\mathbf{x}}^{(k)},i\underline{\mathbf{x}}^{(k)};i \underline{\boldsymbol{\theta}}^{(k:L)}\big{)}\wedge\bigvee_{0\leq j<n}\psi( \underline{\mathbf{y}},i\underline{\mathbf{y}},i\underline{\mathbf{A}},{}_{j }\mathbf{b}) \tag{20}\]

where \(\big{[}\underline{\mathbf{x}}^{(k)},i\underline{\mathbf{x}}^{(k)}\big{]} \stackrel{{\text{\tiny{def}}}}{{=}}\text{Compute\_Bound}_{ \mathcal{N}^{(0:k)}}\big{(}\mathrm{P};\theta^{(0:k)}\big{)}\).

## Appendix C Parametric Linear Relaxation for Tanh, Sigmoid and ELU layers

### Parametric Linear Relaxation for Tanh layers

**Definition C.1**.: \(\mathbf{y}\stackrel{{\text{\tiny{def}}}}{{=}}\mathsf{Tanh}( \mathbf{x})\) with input \(\mathbf{x}\in\mathbb{R}^{m}\) and output \(\mathbf{y}\in\mathbb{R}^{m}\) is defined as \(\mathbf{y}_{i}\stackrel{{\text{\tiny{def}}}}{{=}}\tanh(\mathbf{x }_{i})\).

**Definition C.2**.: For Tanh layer with variable input bounds \([\underline{\mathbf{x}},i\underline{\mathbf{x}}]\), its Parametric Linear Relaxation is defined as \(\overline{\mathbb{Z}}_{\mathsf{Tanh}}(\underline{\mathbf{y}},i\underline{ \mathbf{y}},i\underline{\mathbf{x}},i\underline{\mathbf{x}})\stackrel{{ \text{\tiny{def}}}}{{=}}\overline{\varphi}_{\mathsf{Tanh}}(\underline{ \mathbf{y}},i\underline{\mathbf{x}})\wedge\underline{\varphi}_{\mathsf{Tanh }}(\underline{\mathbf{y}},i\underline{\mathbf{x}})\) where:

\[\overline{\varphi}_{\mathsf{Tanh}}(\underline{\mathbf{y}},i\underline{ \mathbf{x}})\stackrel{{\text{\tiny{def}}}}{{=}}\underline{ \mathbf{y}}\geq\mathsf{Tanh}(\mathbf{x}^{u})\ \wedge\ \underline{\mathbf{y}}\geq\mathbf{a}^{u}i \underline{\mathbf{x}}+\mathbf{b}^{u} \tag{21}\] \[\underline{\varphi}_{\mathsf{Tanh}}(\underline{\mathbf{y}},i \underline{\mathbf{x}})\stackrel{{\text{\tiny{def}}}}{{=}} \underline{\mathbf{y}}\leq\mathsf{Tanh}(\mathbf{x}^{l})\ \wedge\ \underline{\mathbf{y}}\leq\mathbf{a}^{l}i \underline{\mathbf{x}}+\mathbf{b}^{l}\]

where \(\mathbf{x}^{u}\in\mathbb{R}^{m}\), \(\mathbf{a}^{u}\in\mathbb{R}^{m}\), \(\mathbf{b}^{u}\in\mathbb{R}^{m}\) are constants that satisfy the following two conditions: **i)** let \(f(\boldsymbol{x})=\mathbf{a}^{u}_{i}\boldsymbol{x}+\mathbf{b}^{u}_{i}\), the line defined by function \(f(\boldsymbol{x})\) is tangent to the concave piece of \(\tanh\) at some \(\boldsymbol{x}\geq 0\); **ii)**\(\mathbf{a}^{u}\mathbf{x}^{u}+\mathbf{b}^{u}=\mathsf{Tanh}(\mathbf{x}^{u})\); viz., the line defined by function \(f(\boldsymbol{x})\) intersects with convex piece of \(\tanh\) at \(\mathbf{x}^{u}_{i}\) where \(\mathbf{x}^{l}_{i}\leq 0\).

Similarly, \(\mathbf{x}^{l}\in\mathbb{R}^{m}\), \(\mathbf{a}^{l}\in\mathbb{R}^{m}\), \(\mathbf{b}^{l}\in\mathbb{R}^{m}\) are constants that satisfy the following two conditions: **i)** let \(f(\boldsymbol{x})=\mathbf{a}^{l}_{i}\boldsymbol{x}+\mathbf{b}^{l}_{i}\), the line defined by function \(f(\boldsymbol{x})\) is tangent to the convex piece of \(\tanh\) at some \(\boldsymbol{x}\leq 0\); **ii)**\(\mathbf{a}^{l}\mathbf{x}^{l}+\mathbf{b}^{l}=\mathsf{Tanh}(\mathbf{x}^{l})\); viz., the line defined by function \(f(\boldsymbol{x})\) intersects with concave piece of \(\tanh\) at \(\mathbf{x}^{l}_{i}\) where \(\mathbf{x}^{l}_{i}\geq 0\). \(\blacksquare\)

**Theorem C.3**.: _Definition C.2 is a Parametric Linear Relaxation for the Tanh layer._Proof.: As seen in Definition C.2, \(\overline{\mathbb{Z}}_{\mathtt{Tanh}}(\underline{\hat{\mathbf{y}}},\overline{\hat{ \mathbf{y}}},\underline{\hat{\mathbf{x}}},\overline{\hat{\mathbf{x}}})\) is a linear formula. Let \(m\) be the number of input dimensions of \(\mathtt{Tanh}\). \(\overline{\mathbb{Z}}_{\mathtt{Tanh}}(\underline{\hat{\mathbf{y}}},\overline{ \hat{\mathbf{y}}},\underline{\hat{\mathbf{x}}},\overline{\hat{\mathbf{x}}})\) has \(4m\) linear constraints over \(4m\) variables (\(\underline{\hat{\mathbf{x}}}\in\mathbb{R}^{m}\), \(\overline{\hat{\mathbf{x}}}\in\mathbb{R}^{m}\), \(\overline{\hat{\mathbf{y}}}\in\mathbb{R}^{m}\) and \(\overline{\hat{\mathbf{y}}}\in\mathbb{R}^{m}\)). Hence, \(\overline{\mathbb{Z}}_{\mathtt{Tanh}}(\underline{\hat{\mathbf{y}}},\overline{ \hat{\mathbf{y}}},\underline{\hat{\mathbf{x}}},\overline{\hat{\mathbf{x}}})\) is a poly-size linear formula, whose size is polynomial in the number of input dimensions of the \(\mathtt{Tanh}\) layer.

Now our goal is to prove that \(\overline{\mathbb{Z}}_{\mathtt{Tanh}}(\underline{\hat{\mathbf{y}}},\overline{ \hat{\mathbf{y}}},\underline{\hat{\mathbf{x}}},\overline{\hat{\mathbf{x}}})\) defined in Definition C.2 implies

\[\forall\mathbf{x}\in[\underline{\hat{\mathbf{x}}},\overline{\hat{\mathbf{x}}}],\,\underline{\hat{\mathbf{y}}}\leq\mathtt{Tanh}(\mathbf{x})\leq\overline{\hat {\mathbf{y}}}\]

We will discuss the upper and lower bounds separately.

For the upper bound constraint \(\overline{\varphi}_{\mathtt{Tanh}}(\underline{\hat{\mathbf{y}}},\underline{\hat {\mathbf{x}}})\stackrel{{\text{\tiny def}}}{{=}}\overline{\hat{ \mathbf{y}}}\geq\mathtt{Tanh}(\mathbf{x}^{u})\,\wedge\,\overline{\hat{\mathbf{y }}}\geq\mathbf{a}^{u}\overline{\hat{\mathbf{x}}}+\mathbf{b}^{u}\), for the case when \(\underline{\hat{\mathbf{x}}}_{i}\leq\mathbf{x}^{u}_{i}\), we have \(\overline{\hat{\mathbf{y}}}_{i}\geq\tanh(\mathbf{x}^{u}_{i})\geq\tanh( \underline{\hat{\mathbf{x}}}_{i})\) because \(\tanh\) is monotonically increasing. For the case when \(\underline{\hat{\mathbf{x}}}_{i}>\mathbf{x}^{u}_{i}\), we have \(\overline{\hat{\mathbf{y}}}_{i}\geq\mathbf{a}^{u}_{i}\overline{\hat{\mathbf{ x}}}_{i}+\mathbf{b}^{u}_{i}\geq\tanh(\underline{\hat{\mathbf{x}}}_{i})\) because \(f(\mathbf{x})=\mathbf{a}^{u}_{i}\mathbf{x}+\mathbf{b}^{u}_{i}\) is tangent to the concave piece of \(\tanh\) in \([0,\infty]\), and \(\tanh\) is monotonically increasing in \([\mathbf{x}^{u}_{i},0]\). Therefore, \(\overline{\varphi}_{\mathtt{Tanh}}(\overline{\hat{\mathbf{y}}},\overline{\hat{ \mathbf{x}}})\) defined in Definition C.2 implies that \(\forall\mathbf{x}\in[\underline{\hat{\mathbf{x}}},\overline{\hat{\mathbf{x}}}]\). \(\mathtt{Tanh}(\mathbf{x})\leq\overline{\hat{\mathbf{y}}}\).

For the lower bound constraint \(\underline{\varphi}_{\mathtt{Tanh}}(\underline{\hat{\mathbf{y}}},\underline{ \hat{\mathbf{x}}})\stackrel{{\text{\tiny def}}}{{=}}\overline{ \hat{\mathbf{y}}}\leq\mathtt{Tanh}(\mathbf{x}^{l})\,\wedge\,\underline{\hat{ \mathbf{y}}}\leq\mathbf{a}^{l}\underline{\hat{\mathbf{x}}}+\mathbf{b}^{l}\), for the case when \(\underline{\hat{\mathbf{x}}}_{i}\geq\mathbf{x}^{l}_{i}\), we have \(\underline{\hat{\mathbf{y}}}_{i}\leq\tanh(\underline{\mathbf{x}}^{l}_{i})\leq \tanh(\underline{\hat{\mathbf{x}}}_{i})\) because \(\tanh\) is monotonically increasing. For the case when \(\underline{\hat{\mathbf{x}}}_{i}<\mathbf{x}^{l}_{i}\), we have \(\underline{\hat{\mathbf{y}}}\leq\mathbf{a}^{l}_{i}\underline{\hat{\mathbf{ x}}}_{i}+\mathbf{b}^{l}_{i}\leq\tanh(\underline{\hat{\mathbf{x}}}_{i})\) because \(f(\mathbf{x})=\mathbf{a}^{l}_{i}\mathbf{x}+\mathbf{b}^{l}_{i}\) is tangent to the convex piece of \(\tanh\) in \([-\infty,0]\), and \(\tanh\) is monotonically increasing in \([0,\mathbf{x}^{l}_{i}]\). Therefore, \(\underline{\varphi}_{\mathtt{Tanh}}(\overline{\hat{\mathbf{y}}},\overline{\hat{ \mathbf{x}}})\) defined in Definition C.2 implies that \(\forall\mathbf{x}\in[\underline{\hat{\mathbf{x}}},\overline{\hat{\mathbf{x}}}]\). \(\underline{\hat{\mathbf{y}}}\leq\mathtt{Tanh}(\mathbf{x})\).

### Parametric Linear Relaxation for Sigmoid layers

**Definition C.4**.: \(\mathbf{y}\stackrel{{\text{\tiny def}}}{{=}}\mathtt{Sigmoid}( \mathbf{x})\) with input \(\mathbf{x}\in\mathbb{R}^{m}\) and output \(\mathbf{y}\in\mathbb{R}^{m}\) is defined as \(\mathbf{y}_{i}\stackrel{{\text{\tiny def}}}{{=}}\frac{1}{1+e^{- \mathbf{x}_{i}}}\).

**Definition C.5**.: For Sigmoid layer with variable input bounds \([\underline{\hat{\mathbf{x}}},\overline{\hat{\mathbf{x}}}]\), its Parametric Linear Relaxation is defined as \(\overline{\mathbb{Z}}_{\mathtt{Sigmoid}}(\underline{\hat{\mathbf{y}}},\overline{ \hat{\mathbf{y}}},\underline{\hat{\mathbf{x}}},\overline{\hat{\mathbf{x}}}) \stackrel{{\text{\tiny def}}}{{=}}\overline{\varphi}_{\mathtt{Sigmoid }}(\overline{\hat{\mathbf{y}}},\overline{\hat{\mathbf{x}}})\wedge\underline{ \varphi}_{\mathtt{Sigmoid}}(\underline{\hat{\mathbf{y}}},\underline{\hat{\mathbf{x }}})\) where:

\[\overline{\varphi}_{\mathtt{Sigmoid}}(\underline{\hat{\mathbf{y}}},\overline{ \hat{\mathbf{x}}})\stackrel{{\text{\tiny def}}}{{=}}\overline{\hat{ \mathbf{y}}}\geq\mathtt{Sigmoid}(\mathbf{x}^{u})\,\wedge\,\overline{\hat{ \mathbf{y}}}\geq\mathbf{a}^{u}\overline{\hat{\mathbf{x}}}+\mathbf{b}^{u} \tag{22}\] \[\underline{\varphi}_{\mathtt{Sigmoid}}(\underline{\hat{\mathbf{y}}}, \underline{\hat{\mathbf{x}}})\stackrel{{\text{\tiny def}}}{{=}} \underline{\hat{\mathbf{y}}}\leq\mathtt{Sigmoid}(\mathbf{x}^{l})\,\wedge\, \underline{\hat{\mathbf{y}}}\leq\mathbf{a}^{l}\underline{\hat{\mathbf{x}}}+ \mathbf{b}^{l}\]

where \(\mathbf{x}^{u}\in\mathbb{R}^{m}\), \(\mathbf{a}^{u}\in\mathbb{R}^{m}\), \(\mathbf{b}^{u}\in\mathbb{R}^{m}\) are constants that satisfy the following two conditions: **i)** let \(f(\mathbf{x})=\mathbf{a}^{u}_{i}\mathbf{x}+\mathbf{b}^{u}_{i}\), the line defined by function \(f(\mathbf{x})\) is tangent to the concave piece of \(\mathtt{sigmoid}\) at some \(\mathbf{x}\geq 0\); **ii)**\(\mathbf{a}^{u}\mathbf{x}^{u}+\mathbf{b}^{u}=\mathtt{Sigmoid}(\mathbf{x}^{u})\); viz., the line defined by function \(f(\mathbf{x})\) intersects with convex piece of \(\mathtt{sigmoid}\) at \(\mathbf{x}^{u}_{i}\) where \(\mathbf{x}^{u}_{i}\leq 0\).

Similarly, \(\mathbf{x}^{l}\in\mathbb{R}^{m}\), \(\mathbf{a}^{l}\in\mathbb{R}^{m}\), \(\mathbf{b}^{l}\in\mathbb{R}^{m}\) are constants that satisfy the following two conditions: **i)** let \(f(\mathbf{x})=\mathbf{a}^{l}_{i}\mathbf{x}+\mathbf{b}^{l}_{i}\), the line defined by function \(f(\mathbf{x})\) is tangent to the convex piece of \(\mathtt{sigmoid}\) at \(\mathbf{x}^{l}_{i}\) where \(\mathbf{x}^{l}_{i}\geq 0\). \(\blacksquare\)

**Theorem C.6**.: _Definition C.5 is a Parametric Linear Relaxation for the Sigmoid layer._

Proof.: As seen in Definition C.5, \(\overline{\mathbb{Z}}_{\mathtt{Sigmoid}}(\underline{\hat{\mathbf{y}}},\overline{ \hat{\mathbf{y}}},\underline{\hat{\mathbf{x}}},\overline{\hat{\mathbf{x}}})\) is a linear formula. Let \(m\) be the number of input dimensions of Sigmoid. \(\overline{\mathbb{Z}}_{\mathtt{Sigmoid}}(\underline{\hat{\mathbf{y}}},\overline{ \hat{\mathbf{y}}},\underline{\hat{\mathbf{x}}},\overline{\hat{\mathbf{x}}})\) has \(4m\) linear constraints over \(4m\) variables (\(\underline{\hat{\mathbf{x}}}\in\mathbb{R}^{m}\), \(\overline{\hat{\mathbf{x}}}\in\mathbb{R}^{m}\), \(\underline{\hat{\mathbf{y}}}\in\mathbb{R}^{m}\) and \(\overline{\hat{\mathbf{y}}}\in\mathbb{R}^{m}\)). Hence, \(\overline{\mathbb{Z}}_{\mathtt{Sigmoid}}(\underline{\hat{\mathbf{y}}},\overline{ \hat{\mathbf{y}}},\underline{\hat{\mathbf{x}}},\overline{\hat{\mathbf{x}}})\) is a poly-size linear formula, whose size is polynomial in the number of input dimensions of the Sigmoid layer.

Now our goal is to prove that \(\overline{\mathbb{Z}}_{\mathtt{Sigmoid}}(\underline{\hat{\mathbf{y}}},\overline{ \hat{\mathbf{y}}},\underline{\hat{\mathbf{x}}},\overline{\hat{\mathbf{x}}})\) defined in Definition C.5 implies

\[\forall\mathbf{x}\in[\underline{\hat{\mathbf{x}}},\overline{\hat{\mathbf{x}}}],\,\underline{\hat{\mathbf{y}}}\leq\mathtt{Sigmoid}(\mathbf{x})\leq\overline{ \hat{\mathbf{y}}}\]

We will discuss the upper and lower bounds separately.

For the upper bound constraint \(\overline{\varphi}_{\mathtt{Sigmoid}}(\underline{\hat{\mathbf{y}}},\overline{ \hat{\mathbf{x}}})\stackrel{{\text{\tiny def}}}{{=}}\overline{ \hat{\mathbf{y}}}\geq\mathtt{Sigmoid}(\mathbf{x}^{u})\,\wedge\,\overline{\hat{\mathbf{y}}}

\(f(\mathbf{x})=\mathbf{a}_{i}^{l}\mathbf{x}+\mathbf{b}_{i}^{u}\) is tangent to the concave piece of sigmoid in \([0,\infty]\), and sigmoid is monotonically increasing in \([\mathbf{\mathbf{x}}_{i}^{u},0]\). Therefore, \(\overline{\varphi}_{\texttt{Sigmoid}}(\mathbb{Y},\mathbb{X})\) defined in Definition C.5 implies that \(\forall\mathbf{x}\in[\underline{\mathbf{x}},\mathbb{X}]\). \(\texttt{Sigmoid}(\mathbf{x})\leq\overline{\mathbf{y}}\).

**For the lower bound constraint \(\underline{\varphi}_{\texttt{Sigmoid}}(\underline{\mathbf{y}},\mathbb{X}) \stackrel{{\text{\tiny def}}}{{=}}\underline{\mathbf{y}}\leq \texttt{Sigmoid}(\mathbf{x}^{l})\ \wedge\ \underline{\mathbf{y}}\leq\mathbf{a}^{l} \underline{\mathbf{x}}+\mathbf{b}^{l}\)**, for the case when \(\underline{\mathbf{x}}_{i}\geq\mathbf{x}_{i}^{l}\), we have \(\underline{\mathbf{y}}_{i}\leq\texttt{sigmoid}(\mathbf{x}_{i}^{l})\leq\texttt {sigmoid}(\underline{\mathbf{x}}_{i})\) because sigmoid is monotonically increasing. For the case when \(\underline{\mathbf{x}}_{i}<\mathbf{x}_{i}^{l}\), we have \(\underline{\mathbf{y}}_{i}\leq\mathbf{a}_{i}^{l}\underline{\mathbf{x}}_{i}+ \mathbf{b}_{i}^{l}\leq\texttt{sigmoid}(\underline{\mathbf{x}}_{i})\) because \(f(\mathbf{x})=\mathbf{a}_{i}^{l}x+\mathbf{b}_{i}^{l}\) is tangent to the convex piece of sigmoid in \([-\infty,0]\), and sigmoid is monotonically increasing in \([0,\mathbf{x}_{i}^{l}]\). Therefore, \(\underline{\varphi}_{\texttt{Sigmoid}}(\mathbb{Y},\mathbb{X})\) defined in Definition C.5 implies that \(\forall\mathbf{x}\in[\underline{\mathbf{x}},\mathbb{X}]\). \(\underline{\mathbf{y}}\leq\texttt{Sigmoid}(\mathbf{x})\). 

### Parametric Linear Relaxation for ELU layers

**Definition C.7**.: \(\mathbf{y}\stackrel{{\text{\tiny def}}}{{=}}\texttt{ELU}( \mathbf{x})\) with input \(\mathbf{x}\in\mathbb{R}^{m}\) and output \(\mathbf{y}\in\mathbb{R}^{m}\) is defined as \(\mathbf{y}_{i}\stackrel{{\text{\tiny def}}}{{=}}\texttt{elu}( \mathbf{x}_{i})\) where \(\texttt{elu}(\mathbf{x})\stackrel{{\text{\tiny def}}}{{=}}\begin{cases} \mathbf{x}&\mathbf{x}\geq 0\\ \alpha(e^{\mathbf{x}}-1)&\mathbf{x}<0\end{cases}\) and \(\alpha>0\).

**Definition C.8**.: For ELU layer with variable input bounds \([\underline{\mathbf{x}},\mathbb{X}]\), its Parametric Linear Relaxation is defined as \(\overline{\underline{\varphi}}_{\texttt{ELU}}(\underline{\mathbf{y}},\mathbb{Y },\underline{\mathbf{x}},\mathbb{X})\stackrel{{\text{\tiny def}}}{ {=}}\overline{\varphi}_{\texttt{ELU}}(\mathbb{Y},\mathbb{X})\wedge\underline{ \varphi}_{\texttt{ELU}}(\underline{\mathbf{y}},\underline{\mathbf{x}})\) where:

\[\overline{\underline{\varphi}}_{\texttt{ELU}}(\mathbb{Y},\mathbb{X}) \stackrel{{\text{\tiny def}}}{{=}}\overline{\mathbf{y}}\geq \mathbb{X}\ \wedge\ \overline{\mathbf{y}}\geq 0 \tag{23}\] \[\underline{\underline{\varphi}}_{\texttt{ELU}}(\underline{ \mathbf{y}},\underline{\mathbf{x}})\stackrel{{\text{\tiny def}}}{ {=}}\underline{\mathbf{y}}\leq\mathbf{a}\underline{\mathbf{x}}+\mathbf{b}\]

where \(\mathbf{a}\in\mathbb{R}^{m}\), \(\mathbf{b}\in\mathbb{R}^{m}\) are constants such that the line defined by \(f(\mathbf{x})=\mathbf{a}_{i}\mathbf{x}+\mathbf{b}_{i}\) for any \(i\) is tangent to elu. 

**Theorem C.9**.: _Definition C.8 is a Parametric Linear Relaxation for the ELU layer._

Proof.: As seen in Definition C.8, \(\overline{\underline{\varphi}}_{\texttt{ELU}}(\underline{\mathbf{y}},\mathbb{Y },\underline{\mathbf{x}},\mathbb{X})\) is a linear formula. Let \(m\) be the number of input dimensions of ELU. \(\overline{\underline{\varphi}}_{\texttt{ELU}}(\underline{\mathbf{y}},\mathbb{Y },\underline{\mathbf{x}},\mathbb{X})\) has \(3m\) linear constraints over \(4m\) variables (\(\underline{\mathbf{x}}\in\mathbb{R}^{m},\mathbb{X}\in\mathbb{R}^{m}\), \(\underline{\mathbf{y}}\in\mathbb{R}^{m}\) and \(\overline{\mathbf{y}}\in\mathbb{R}^{m}\)). Hence, \(\overline{\underline{\varphi}}_{\texttt{ELU}}(\underline{\mathbf{y}}, \mathbb{Y},\underline{\mathbf{x}},\mathbb{X})\) is a poly-size linear formula, whose size is polynomial in the number of input dimensions of the ELU layer.

Now our goal is to prove that \(\overline{\underline{\varphi}}_{\texttt{ELU}}(\underline{\mathbf{y}},\mathbb{Y },\underline{\mathbf{x}},\mathbb{X})\) defined in Definition C.8 implies

\[\forall\mathbf{x}\in[\underline{\mathbf{x}},\mathbb{X}],\underline{\mathbf{y}} \leq\texttt{ELU}(\mathbf{x})\leq\overline{\mathbf{y}}\]

We will discuss the upper and lower bounds separately.

**For the upper bound constraint \(\overline{\varphi}_{\texttt{ELU}}(\mathbb{Y},\mathbb{X})\stackrel{{ \text{\tiny def}}}{{=}}\underline{\mathbf{y}}\geq\mathbb{X}\ \wedge\ \overline{\mathbf{y}}\geq 0\)**, for the case when \(\hat{\mathbf{x}}_{i}\geq 0\), by Definition C.7 we have \(\overline{\mathbf{y}}_{i}\geq\mathbb{X}_{i}=\texttt{elu}(\mathbb{X}_{i})\). For the case \(\hat{\mathbf{x}}_{i}<0\), because elu is monotonically increasing and \(\texttt{elu}(0)=0\), we have \(\underline{\mathbf{y}}_{i}\geq 0\geq\texttt{elu}(\mathbb{X}_{i})\). Therefore, \(\overline{\varphi}_{\texttt{ELU}}(\mathbb{Y},\mathbb{X})\) defined in Definition C.8 implies that \(\forall\mathbf{x}\in[\underline{\mathbf{x}},\mathbb{X}]\). \(\texttt{ELU}(\mathbf{x})\leq\overline{\mathbf{y}}\).

**For the lower bound constraint \(\underline{\underline{\varphi}}_{\texttt{ELU}}(\underline{\mathbf{y}}, \underline{\mathbf{x}})\stackrel{{\text{\tiny def}}}{{=}}\underline{ \mathbf{y}}\leq\mathbf{a}\underline{\mathbf{x}}+\mathbf{b}\)**, because the line defined by \(f(\mathbf{x})=\mathbf{a}_{i}\mathbf{x}+\mathbf{b}_{i}\) is tangent to elu and is a convex function, we have \(\underline{\mathbf{y}}_{i}\leq\mathbf{a}_{i}\underline{\mathbf{x}}_{i}+\mathbf{b}_{i}\leq \texttt{elu}(\underline{\mathbf{x}}_{i})\). Therefore, \(\underline{\underline{\varphi}}_{\texttt{ELU}}(\mathbb{Y},\mathbb{X})\) defined in Definition C.8 implies that \(\forall\mathbf{x}\in[\underline{\mathbf{x}},\mathbb{X}]\). 

## Appendix D Experiment Details

Provable fine-tuning \(\texttt{PFT}(\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt \texttt{\texttt{\texttt{\texttt{\texttttexttttexttttexttttexttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt{\texttt\texttt \,are satisfied, the loop terminates; otherwise the loop continues. According to the DNN architecture and editing property, we use the best verifier from (1) \(\alpha\),\(\beta\)-CROWN[46; 44]: winner of the VNN Competition VNN-COMP 2021, 2022 and 2023; (2) MN-BaB[9]: runner-up of VNN-COMP 2022 and 2023; and (3) DeepT[4]: state-of-the-art (incomplete) transformer verifier.

In order to improve the performance of provable fine-tuning with a non-provable DNN editing approach, viz., preserve the DNN's accuracy, we a) freeze all batch normalization layers, b) only edit the last few layers, c) incorporate changes of parameters as regularization, and d) sample points and incorporate the changes in their outputs as regularization.

### Provable Editing on VNN Competition Benchmarks

In this section we present the further details for Section 4.1.

Benchmarks.We take 12 out of 15 benchmarks from VNN-COMP'22[29], excluding **i)**"carvana_unet_2022" and "nn4sys": the support for the sigmoid activation function is not implemented in PREPARED, and is not supported by APRNN. **ii)**"vggnet6_2022": the bound computation for intermediate layers runs out of CPU and GPU memory using auto_LiRPA.

Setup details.For both PPT(DL2) and PPT(APRNN), we use DL2 to find counterexamples to edit, and we use \(\alpha\),\(\beta\)-CROWN, the winner of VNN-COMP, as the verifier. We only call the verifier when DL2 cannot find counterexamples. For all approaches, we edit the last few layers, which differs for different DNNs. For PREPARED, we use auto_LiRPA or DeepPoly to compute the constant bounds for the input to the first layer to edit.

Hyperparameters for PPT(DL2).SGD optimizer with a learning rate of 0.001, DL2 weight of 0.2, batch size of 32 for the **single-property** setting, and 256 for the **all-properties** setting.

Hyperparameters for PPT(APRNN).For different DNNs, APRNN edits from the last six layers to the last layer. we set a bound of [-10, 10] for the change of each parameter. We use DL2 to find counterexamples for APRNN to edit in each epoch, and use the same hyperparameters as the DL2 experiments for the DL2 oracle.

Hyperparameters for PREPARED.For different DNNs, PREPARED edits from the last eight layers to the last layer. We use auto_LiRPA[50] or DeepPoly[39] to compute the input bounds to the first layer to edit. we set a bound of [-10, 10] for the change of each parameter.

Results details.In Table 4 we present the number of succeed instances for provably editing single-properties and **all-properties** instances in each benchmark.

### Local robustness editing for image-recognition DNNs

In this section we present the experiment details for Section 4.2.

Property.Given an \(L^{\infty}\) local robustness perturbation \(\varepsilon\) for an DNN \(\mathcal{N}\) and a pair of input point \(\mathbf{x}\) and output label \(l\), the repair specification is \(\forall\mathbf{x}^{\prime}\). \(\|\mathbf{x}-\mathbf{x}^{\prime}\|_{\infty}\implies\arg\max\mathcal{N}(\mathbf{ x}^{\prime})=l\). In other words, for all inputs \(\mathbf{x}^{\prime}\) in the \(L^{\infty}\) ball around \(\mathbf{x}\) with radius \(\varepsilon\), the DNN \(\mathcal{N}\) should classify them as label \(l\).

Setup details.The validation set is the same 10% of the training set, randomly selected with seed 0. For PPT(DL2), PPT(SABB) and PPT(STAPS), we use MN-BaB, the complete verifier used by SABR and STAPS, as the verifier in the loop. MN-BaB is the runner-up of VNN-COMP. We only call the verifier when DL2, SABR or STAPS cannot find counterexamples. For PREPARED, we use DeepPoly to compute the constant bounds for the input to the first layer to edit.

Hyperparameters for PFT(SABB).We take the base hyperparameters from the SABR training script [27]. We freeze all batch normalization layers, and search the hyperparameters: **i)** learning rate in {5e-4, 1e-4, 5e-5, 1e-5}; **ii)** batch size in {\(5,10,25\)}; **iii)** lambda in {\(0.1,0.2,\ldots,0.9\)}; **iv)** end_epoch_eps in {\(80,60,40,20\)}; **v)** fine-tune the entire DNN, the last three layers, or the last layer to find the edit with the highest efficacy, then the best validation set accuracy. Other hyperparameters are the same as used for training the CIFAR10 (\(\varepsilon=\nicefrac{{2}}{{255}}\)) and TinyImageNet (\(\varepsilon=\nicefrac{{1}}{{255}}\)) CNN7 in the SABR paper. We use the following early-stopping criteria: **i)** CIFAR10: validation accuracy drops below 60%; **ii)** TinyImageNet: validation accuracy drops below 20%. The final hyperparameters for CIFAR10 are: Adam optimizer with a learning rate of 5e-4, batch size of 5,lambda of 0.1, end_epoch_eps of 80, and fine-tune the last three layers. The final hyperparameters for TinyImageNet are: Adam optimizer with a learning rate of 5e-4, batch size of 5, lambda of 0.1, end_epoch_eps of 80, and fine-tune the last three layers.

**Hyperparameters for** PFT(STAPS)**.** We take the base hyperparameters from the STAPS training script in the TAPS code [25]. We freeze all batch normalization layers, and search the following hyperparameters: **i)** learning rate in {5e-4, 1e-4, 5e-5, 1e-5}; **ii)** batch size in {5,10,25}; **iii)** reg_lambda in {0.1,0.2,..., 0.9}; **iv)** end_epoch_eps in {80,60,40,20}. **v)** fine-tune the entire DNN, the last three layers, or the last layer. to find the edit with the highest efficacy, then the best validation set accuracy. Other hyperparameters are the same as used for training CIFAR10 (\(\varepsilon=\nicefrac{{2}}{{255}}\)) and TinyImageNet (\(\varepsilon=\nicefrac{{1}}{{255}}\)) CNN7 using STAPS in the TAPS paper We use the following early-stopping criteria: **i)** CIFAR10: validation accuracy drops below 60%; **ii)** TinyImageNet: validation accuracy drops below 60%; **iii)** ResNet-100, 60% of the last three layers.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Benchmark & PFT(DL2) & PFT(APRMP) & PREPARED & Total \\ \hline acasxu & 0 & 5 & **47** & 47 \\ rl\_benchmarks & **103** & 84 & **103** & 103 \\ tllverifybench & 20 & 5 & **21** & 21 \\ cifar100\_tinyimagenet\_resnet & 0 & 0 & **29** & 29 \\ sri\_resnet\_a & 8 & 2 & **52** & 52 \\ sri\_resnet\_b & 8 & 0 & **44** & 44 \\ reach\_prob\_density & 0 & 0 & **14** & 14 \\ mnist\_fc & 1 & 1 & **22** & 22 \\ cifar\_biasfield & 1 & 0 & **4** & 4 \\ cifar2020 & 35 & 0 & **55** & 55 \\ collins\_rul\_cnn & 8 & 20 & **27** & 27 \\ oval21 & 1 & 0 & **5** & 5 \\ \hline all & 185 & 117 & **423** & 423 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison of the number of succeed instances for provably editing **single-instances** and **all-properties** instances in each benchmark.

below 20%. The final hyperparameters for CIFAR10 are: Adam optimizer with a learning rate of 1e-5, batch size of 10, reg_lambda of 0.9, end_epoch_ eps of 80, and fine-tune the last three layers. The final hyperparameters for TinyImageNet are: Adam optimizer with a Learning rate of 1e-5, batch size of 10, reg_lambda of 0.8, end_epoch_ eps of 80, and fine-tune the last three layers.

Hyperparameters for PFT(bl2).We take the base hyperparameters from the RobustnessG training script in the DL2 code [11]. We freeze all the batch normalization layers, and search for the following hyperparameters: **i)** learning rate in {1e-3, 5e-4, 1e-4, 5e-5}; **ii)** batch size in {\(5,10,25\)}; **iii)** fine-tune the entire DNN, the last layer or the last three layers. to find the edit with the highest efficacy, then the best validation set accuracy. We use the following early-stopping criteria: **i)** CIFAR10: validation accuracy drops below 70%; **ii)** TinyImageNet: validation accuracy drops below 20%. The final hyperparameters for CIFAR10 are: SGD optimizer with learning rate of 1e-4, batch size of 10, DL2 weight of 0.2, fine-tune the last three layers. The final hyperparameters for TinyImageNet are: SGD optimizer with learning rate of 1e-4, batch size of 10, DL2 weight of 0.2, fine-tune the last three layers.

Hyperparameters for APRNN.We edit up to the fifth last layer, and set a bound [-10, 10] for the change of each DNN parameter. For both CIFAR10 and TinyImageNet, the final result comes from editing the last layer.

Hyperparameters for PREPARED.We edit up to the fifth last layer, and set a bound [-10, 10] for the change of each DNN parameter. For both CIFAR10 and TinyImageNet, the final result comes from editing the last layer.

### Local robustness editing for sentiment classification BERT transformers

In this section we present the experiment details for Section 4.3.

Setup details.We use the default validation set used in the DeepT training scripts [3]. For PFT(bl2), we use DeepT as the verifier in the loop. We only call the verifier when DL2 cannot find counterexamples. For PREPARED, we use DeepT to compute the constant bounds for the input to the first layer to edit.

Hyperparameters for PFT(bl2).We incorporate the default training script and hyperparameters from DeepT [4]. We search the hyperparameters **i)** fine-tune the last three layers or only the last layer; **ii)** batch size in {\(4,8,16,32\)}; **iii)** learning rate in {1e-3, 5e-3, 1e-4, 5e-4}; to find the edit with the best efficacy, then the best validation accuracy. For \(\varepsilon\)=1e-4, the final hyperparameters are: fine-tune the last layer, batch size of 32, Adam optimizer with learning rate of 1e-4, DL2 weight of 0.1 For \(\varepsilon\)=5e-4, the final hyperparameters are: fine-tune the last layer, batch size of 16, Adam optimizer with learning rate of 1e-4, DL2 weight of 0.1.

Hyperparameters for PFT(aprun).We edit up to the third last layer, and set a bound [-10, 10] for the change of each DNN parameter. The final result comes from editing the last layer.

Hyperparameters for PREPARED.We edit up to the third last layer, and set a bound [-10, 10] for the change of each DNN parameter. The final result comes from editing the last layer.

### Provable training for physics-plausible DNNs

In this section we present the experiment details for Section 4.4.

Hyperparameters for DL2.We search the hyperparameters **i)** batch size in {\(4,8,16,32\)}; **ii)** learning rate in {1e-3, 5e-3, 1e-4, 5e-4}; We train 2000 epochs and take the epoch with the highest validation accuracy. The final hyperparameters are: batch size of 8, SGD optimizer with lr of 1e-3, DL2 weight of 0.2, 1241 epochs.

Hyperparameters for GD.We search the hyperparameters **i)** batch size in {\(4,8,16,32\)}; **ii)** learning rate in {1e-3, 5e-3, 1e-4, 5e-4}; We train 2000 epochs and take the epoch with the highest validation accuracy. The final hyperparameters are: batch size of 32, SGD optimizer with lr of 1e-3, 635 epochs.

Hyperparameters for GD(aprun).We edit up to the first layer, and set a bound [-10, 10] for the change of each DNN parameter. The final result comes from editing the last layer.

Hyperparameters for GD(PREPARED).We edit up to the first layer, and set a bound [-10, 10] for the change of each DNN parameter. We use DeepPoly to compute the input bounds to the first layer to edit. The final result comes from editing the last three layers.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The theoretical claims in the abstract and introduction are proved in Section 3 as well as Appendix A, B. The empirical claims in the abstract and introduction are justified in Section 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations are discussed in Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The full set of assumptions and informal proof sketch are provided in Section 3 complemented by formal proofs in Appendix A. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Section 3 complemented by Appendix B fully disclose all information needed to reproduce the technique, and Section 4 complemented by Appendix D fully disclose all information needed to reproduce the experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: The experiments Section 4.1-4.3 use open-access benchmarks. We plan to release on acceptance the data we generated for Section 4.4. For the implementation of our tool, we could not make the code open access due to ongoing IP restrictions. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental settings are presented in Section 4 complemented by further details in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Because the proposed algorithm is deterministic, we instead evaluate our approach on a wide-variety of benchmarks to demonstrate its efficacy and efficiency. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The evaluation platform and runtime are provided in Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: This is discussed in Sections 1 and 5. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper proposes an efficient and provable DNN editing approach, which does not involve releasing of data and models that have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Section 4 cities the original papers that produced the baselines, tools, models and benchmarks used in the paper. Appendix D cites commits for open-access code and scripts used in the paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not introduce new assets in the paper. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.