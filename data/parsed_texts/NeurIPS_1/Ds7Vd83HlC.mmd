# Faster Query Times for Fully Dynamic k-Center Clustering with Outliers

Leyla Biabani

Eindhoven University of Technology

Eindhoven, The Netherlands

l.biabani@tue.nl

&Annika Hennes

Heinrich Heine University Dusseldorf

Dusseldorf, Germany

annika.hennes@hhu.de

Morteza Monemizadeh

Eindhoven University of Technology

Eindhoven, The Netherlands

m.monemizadeh@tue.nl

&Melanie Schmidt

Heinrich Heine University Dusseldorf

Dusseldorf, Germany

mschmidt@hhu.de

###### Abstract

Given a point set \(P\subseteq M\) from a metric space \((M,d)\) and numbers \(k,z\in\mathbb{N}\), the _metric \(k\)-center problem with \(z\) outliers_ is to find a set \(C^{*}\subseteq P\) of \(k\) points such that the maximum distance of all but at most \(z\) outlier points of \(P\) to their nearest center in \(C^{*}\) is minimized. We consider this problem in the fully dynamic model, i.e., under insertions and deletions of points, for the case that the metric space has a bounded doubling dimension \(\dim\). We utilize a hierarchical data structure to maintain the points and their neighborhoods, which enables us to efficiently find the clusters. In particular, our data structure can be queried at any time to generate a \((3+\varepsilon)\)-approximate solution for input values of \(k\) and \(z\) in worst-case query time \(\varepsilon^{-O(\dim)}k\log n\log\log\Delta\), where \(\Delta\) is the ratio between the maximum and minimum distance between two points in \(P\). Moreover, it allows insertion/deletion of a point in worst-case update time \(\varepsilon^{-O(\dim)}\log n\log\Delta\). Our result achieves a significantly faster query time with respect to \(k\) and \(z\) than the current state-of-the-art by Pellizzoni, Pietracaprina, and Pucci [18], which uses \(\varepsilon^{-O(\dim)}(k+z)^{2}\log\Delta\) query time to obtain a \((3+\varepsilon)\)-approximate solution.

## 1 Introduction

Clustering is a fundamental problem in machine learning and it has applications in many areas ranging from natural to social sciences. As a basic unsupervised learning method, it allows us to find structure in data. Classical center-based clustering methods for objectives like \(k\)-means, \(k\)-median and \(k\)-center have been around for many decades [9, 13, 16]. Since the beginning of clustering, the data to be analyzed has changed dramatically, raising new challenges for clustering methods. Data nowadays comes in large to huge batches and is often prone to inherent change: Content on social media platforms is constantly added, re-organized or deleted, streaming services handle an always ongoing flow of people starting and stopping to watch, tweets are the fastest method to distribute news - and delete them later if desired. Data analysis methods thus face the challenge of analyzing huge amounts of data, ideally permanently updating their solution.

In this paper, we consider the problem of _dynamic_\(k\)-center clustering _with outliers_. The (metric) \(k\)-center problem is to find a set \(C^{*}\subseteq P\) of \(k\) points such that the maximum distance of all points of \(P\) to their nearest center in \(C^{*}\) is minimized. This objective is well understood, \(2\)-approximations for the problem are known [9, 13] and better approximations are not possible unless \(P\) equals \(NP\)[14].

Considering outliers for \(k\)-center is very natural. Since \(k\)-center minimizes the maximum radius, a single measuring error can destroy the structure completely. Thus, the \((k,z)\)-center problem allows us to ignore \(z\) points. These belong to no clusters but are deemed to be _outliers_, and so they are ignored when determining the maximum radius of a cluster. The concept was introduced in 2001 by Charikar, Khuller, Mount, and Narasimhan [6] who as one result provide a \(3\)-approximation for \(k\)-center with outliers. This variant has since been the subject of many papers, and the best-known approximation algorithm for it, which gives a \(2\)-approximation, was derived as recently as 2016 [2].

In parallel, fully dynamic \(k\)-center algorithms (without outliers) have been developed [1, 3, 8, 10] which work with a changing point set given as a stream of insertions and deletions. All these maintain a data structure that allows for continual updates to keep adapting to the changing data. An overview is given in Table 1. All results in this table support some form of _membership queries_: Given a point, return the cluster / center of the cluster of that point in the approximate solution. This type of query can typically be answered in time \(O(1)\). Reporting the centers takes more time or is not necessarily explained how to do.

Now the challenge is to combine the two parallel developments into a fully dynamic algorithm for \((k,z)\)-center clustering. We discuss the setting more precisely and then our results and related work.

Our settingWe say that a metric space has _doubling dimension_\(\dim\) if \(\dim\) is the smallest positive integer such that any ball of radius \(r\) can be covered by at most \(2^{\dim}\) balls of radius \(r/2\). For an overview of this concept, we also refer the reader to [12, 11]. Let \((M,d)\) denote such a metric space and let \(P\subseteq M\) be a set of points. By \(\text{d}_{\text{min}}\coloneqq\inf\{d(x,y)\mid x,y\in P,\ x\neq y\}\) and \(\text{d}_{\text{max}}\coloneqq\sup\{d(x,y)\mid x,y\in P\}\) we denote the minimum and maximum inter-point distance within \(P\), respectively. Without loss of generality, we assume that \(\text{d}_{\text{min}}=2\). Let \(\Delta\coloneqq\text{d}_{\text{max}}/\text{d}_{\text{min}}\) be the aspect ratio of \(P\).

We are observing a dynamic stream, i.e., we start with a point set \(P=\emptyset\) and then process a sequence of operations whose length is unknown a priori. Each operation either adds a point from \(M\) to \(P\) or it deletes a point which is currently in \(P\) from \(P\). We assume only deletions of points that are currently present are allowed, and we also assume that we can always compute \(d\) in time \(O(1)\) (otherwise, multiply all times mentioned in this paper by the time necessary to make one distance computation). We refer to the point set after time \(t\) (i.e., after \(t\) operations happened) as \(P_{t}\). If it is clear from the context, we will sometimes drop the subscript. We assume that \(d_{\text{max}}\) and \(d_{\text{min}}\) are fixed throughout the whole sequence of updates, i.e. \(d_{\text{max}}=\sup\{d(x,y)\mid x,y\in P_{t},t\geq 0\}\), and analogously for \(d_{\text{min}}\). Therefore, \(\Delta\) is an upper bound on the aspect ratio over all updates and is known in advance. This is in accordance with a sequence of other works [1, 4, 3, 7, 10, 19].

The problem that we address is the dynamic _\(k\)-center problem with \(z\) outliers_ or in short \((k,z)\)-_center problem_: Given \(P_{t}\) and numbers \(k\) and \(z\), produce a set \(C\subseteq P_{t}\) of size at most \(k\) such that the maximum distance of all but at most \(z\) points to their nearest neighbor is minimized. More formally, \(\min_{Z\subseteq P_{t},|Z|\leq z}\max_{x\in P_{t}\setminus Z}\min_{c\in C}d(x,c)\) is minimized among all possible choices of \(C\) with \(|C|\leq k\). 1

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Year & Ref. & Metric & Update time & Approx \\ \hline
2018 & [3] & general & amort. \(O(k^{2}\frac{\log\delta}{\varepsilon})\) & \(2+\varepsilon\) \\
2019 & [19] & Eucl. \(\mathbb{R}^{d}\) & amort. \(O(\varepsilon^{d}\log\Delta\log n)\) & \(16\) \\
2021 & [10] & doubl. & \(\varepsilon^{-O(\dim)}\log\Delta\log\log\Delta\) & \(2+\varepsilon\) \\
2023 & [1] & general & amort. \(O(k\operatorname{polylog}(n,\Delta))\) & \(2+\varepsilon\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Fully dynamic \(k\)-center algorithms _without outliers_.

Our focus is on building a dynamic algorithm that has a low update time for insertion and deletions. We are allowed to store \(P\) (and additional information). Our aim is to achieve an update time for our data structure that is independent of \(k\) and a query time that is at most linear in \(k\). For this, it is necessary to consider a restricted model, as Bateni _et al._[1] showed that in arbitrary metric spaces, an update time of \(\Omega(k)\) is necessary. We consider metric spaces _of bounded doubling dimension_. Obviously, time \(\Theta(k)\) is necessary to return an actual solution, so we distinguish between _update time_ for insertion and deletions of points and _query time_ for obtaining a solution from the data structure.

Our result compared to related workWe present a deterministic \((3+\varepsilon)\)-approximation algorithm for the \(k\)-center problem with \(z\) outliers in bounded doubling dimension that is fully dynamic, i.e., points can be inserted and deleted. Our algorithm is based on a data structure that requires space linear in \(n\) and does not need to know \(k\) and \(z\) in advance. As a result, we can answer queries for all \(k\) and \(z\), or in other words, \(k\) and \(z\) can be part of the query. Moreover, our algorithm does not require knowledge of the doubling dimension \(\dim\) of the underlying metric space, and it only appears in the analysis.

**Theorem 1.1** (Main theorem).: _Let \((M,d)\) be a metric space of bounded doubling dimension \(dim\), and let \(\varepsilon>0\) be an error parameter. There exists a deterministic dynamic algorithm that allows the insertion or deletion of points from \(M\) using worst-case \(\varepsilon^{-O(\dim)}\log n\log\Delta\) update time. Moreover, at any time \(t\), it can be queried by parameters \(k\) and \(z\) to compute a \((3+\varepsilon)\)-approximate solution for the \(k\)-center problem with \(z\) outliers of \(P_{t}\) using worst-case \(\varepsilon^{-O(\dim)}k\log n\log\log\Delta\) query time, where \(P_{t}\) is the set of points that are inserted but not deleted up to time \(t\), and \(n\) is the size of \(P_{t}\)._

The first approximation algorithm for this problem has been obtained by Chan, Lattanzi, Sozio, and Wang in 2022 [4], which has an approximation ratio of \(14+\varepsilon\). Very recently and in independent work, Pellizzoni, Pietracaprina, and Pucci [18] derived an algorithm with approximation ratio \(3+\varepsilon\). For the problem of distributed \(k\)-center with outliers, De Berg, Biabani, and Monemizadeh [7] give a randomized dynamic algorithm that also achieves a guarantee of \(3+\varepsilon\) with failure probability \(\delta\). Both of these and also we develop algorithms that answer _solution queries_ rather than membership queries, i.e., they can produce a center set at any time and this set is an approximately good solution for the current \(k\)-center with outliers instance. We state the update times (for insertions and deletions of points) and the query times (for reporting the current approximate solution) of these and our approaches in Table 2.

All three known results have a dependency on \((k+z)^{2}\) in their query time which we improve to a linear dependence on \(k\) and no dependence on \(z\). This gives our approach an advantage even if \(k\) or \(z\) are only mildly dependent on \(n\): Already for \(k\in O(\log n)\) and \(z\in O(\sqrt{n})\), the query complexity of [18] would be linear in \(n\) compared to the poly-logarithmic dependency in terms of \(n\) for our query time. More interestingly, for a realistic regime where \(z\) is an epsilon-fraction of \(n\), the query time of the dynamic algorithm presented in [18] has quadratic dependency in terms of \(n\) while our query time still has a logarithmic dependency in terms of \(n\) and linear dependency in terms of \(k\). [4] also states a bicriteria approximation where the number of outliers can be violated with improved running time, but the dependency on \(k\) is still quadratic. When comparing to [7], it should be noted that their algorithm is only applicable to the Euclidean metric space, is randomized and only works against oblivious adversaries. Further, the coreset size (and the space complexity) of the algorithm in [7] is \(x=O(k\epsilon^{-\dim}\log\Delta+z)\). In the streaming model, in order to extract the solution, one needs to run an offline algorithm on the coreset. In this way, the query time of this algorithm is \(O(kx^{2}\log x)=O(k(z^{2}+k^{2})\epsilon^{-2dim}\log^{2}\Delta)\) which is significantly worse than the query time of our algorithm.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Year & Ref. & Metric & Update time & Query time & Approx \\ \hline
2022 & [4] & general & \(O(\frac{1}{\varepsilon}(k+z)^{2}\log\Delta)\) & \(O(\frac{1}{\varepsilon}(k+z)^{2}\log\Delta)\) & \(14+\varepsilon\) \\
2023 & [18] & doubl. & \(\frac{1}{\varepsilon^{O(\dim)}}\log\Delta\) & \(\frac{1}{\varepsilon^{O(\dim)}}(k+z)^{2}\log\Delta\) & \(3+\varepsilon\) \\
2023 & [7] & doubl. & \(O(\frac{k}{\varepsilon\dim}+z)\log^{4}(\frac{k\Delta}{\varepsilon\delta})\) & \(O((\frac{k}{\varepsilon\dim}+z)^{2}k\log\left(\frac{k}{\varepsilon\dim}+z \right))\) & \(3+\varepsilon\) \\ now & Thm 1.1 & doubl. & \(\frac{1}{\varepsilon^{O(\dim)}}\log n\log\Delta\) & \(\frac{1}{\varepsilon^{O(\dim)}}k\log n\log\log\Delta\) & \(3+\varepsilon\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Fully dynamic \(k\)-center algorithms _with outliers_ and their update/query times.

The main difference of our approach is that it makes the greedy algorithm for \(k\)-center with outliers dynamic while the previous dynamic algorithms extract a coreset and run the greedy algorithm on this coreset. Known dynamic algorithms [4, 7, 18] for the \(k\)-center problem with outliers maintain a coreset after every update. In particular, in [18], they extract the coreset by simply reading the solution from the cover tree. To extract an approximate solution for this problem, one needs to run a known (offline) greedy algorithm on this coreset. In this way, the query complexities of those algorithms are dominated by the running time of the greedy algorithm. The novelty of our dynamic algorithm is that we make the greedy algorithm itself dynamic. To this end, we use heap data structures to compute a ball that covers the maximum number of points, and dynamic neighborhood sets to obtain points in the expanded maximum ball and update their corresponding keys in the heap to recursively find the next maximum balls.

Navigating nets.The state-of-the-art algorithm for the fully dynamic \(k\)-center problem (without outliers) in doubling metrics, developed by Goranci et al. [10] uses _navigating nets_ originally introduced by Krauthgamer and Lee [15]. The basic idea of a navigating net is to start with the point set \(P_{t}\) as the base net \(N_{1}\) and then compute coarser and coarser variants of it, resulting in a hierarchy of nets \(N_{1},N_{2},\ldots,N_{2^{i}},\ldots\). For every level in this hierarchy, the points are good representatives of the points in the level below but the higher we get in the net, we have fewer points that are better separated. More precisely, following Krauthgamer and Lee [15], every level is a so-called \(r\)_-net_ of the level below it. An \(r\)-net on a set \(X\) is a subset \(Y\subseteq X\) that satisfies two properties:

1. \(\forall p\in X\exists y\in Y\colon d(p,y)<r\) (Covering)
2. \(\forall x,y\in Y\colon d(x,y)\geq r\) (Packing)

In a navigating net, one always assures that \(N_{r}\) is an \(r\)-net of \(N_{r/2}\). Assume that the minimum inter-point distance \(\operatorname{d_{min}}\) satisfies \(\operatorname{d_{min}}=2\) and that the ratio between maximum and minimum inter-point distance is \(\Delta\). Then a navigating net with \(O(\log\Delta)\) levels can be computed in a greedy fashion. Intriguingly, one can show that in the resulting navigating net, the lowest level with \(|N_{r}|\leq k\) provides an \(8\)-approximation for the \(k\)-center problem (without outliers, i.e., \(z=0\)). This was first observed (in the context of streaming algorithms and there known as the _doubling algorithm_) by Charikar, Chekuri, Feder, and Motwani [5]. An additional trick to improve the approximation ratio was discovered by McCutchen and Khuller [17] (again, in the setting of streaming). They observed that the quality of the solutions depends heavily on what the radius of the lowest level is, and that this radius can be shifted by small amounts, and one of the shifted versions will give a \(2+\varepsilon\) approximation. By maintaining the navigating net structure and using a similar shifting trick, dynamic \(k\)-center algorithms maintain enough information to answer queries by checking one or a few levels of the resulting navigating net and reporting the points on that level as the solution, as for example in [1, 10].

Challenges with outliers.For the \((k,z)\)-center problem, the challenge is that there is no level in the navigating net which provides a good solution in itself. We know that an optimal \((k,z)\)-center solution has a radius that lies between the radius of a \(k\)-center solution (because any \(k\)-center solution is feasible for the \((k,z)\)-center problem) and the radius of a \((k+z)\)-center solution (because any \((k,z)\)-center solution is feasible for the \((k+z)\)-center problem). But as Figure 1 shows, neither of the two choices may provide a good estimate for the optimum \((k,z)\)-center solution. Say level \(r_{1}\) is the first to satisfy \(|N_{r_{1}}|\leq k+z\) and level \(r_{2}\) is the first to satisfy \(|N_{r_{2}}|\leq k\). Then the points in \(N_{r_{2}}\) are a feasible solution for the \((k,z)\)-center problem, but their radius may be too large, while the radius of the solution in \(N_{r_{1}}\) is guaranteed to be small enough, but these points may not constitute a feasible solution for \((k,z)\)-center. This is because every point in \(N_{r}\) represents several points in the levels below it. So we cannot simply divide the \(k+z\)

Figure 1: For \(k=1\) and \(z=3\), \(k\)-center and \((k+z)\)-center provide bad estimations for the best \((k,z)\)-center solution in this example. We see: (a) Dashed Ball: Optimum solution for \((k,z)\)-center. (b) Small Balls (too small): Optimum solution for \((k+z)\)-center. (c) Not drawn: the optimum solution for \(k\)-center is a ball around all points (too large).

points into \(k\) centers and \(z\) outliers (see Figure 1). Therefore, there is no individual level which we can query to obtain a solution. Instead, we need to compute a solution from the navigating net levels while making sure not to lose too much in terms of quality and query time.

Pellizzoni _et al._[18] base their approach on the so-called "cover tree" data structure, which is similar to that of a navigating net, to compute a coreset from which they construct the final solution. In their approach, they find the lowest level with \(|N_{r}|\leq 2^{O(\dim)}\varepsilon^{-\dim}(k+z)\) and demonstrate that such \(N_{r}\) serves as an \(\varepsilon\)-coreset for \((k,z)\)-center. However, since it is not a feasible solution, they utilize a \(3\)-approximation greedy algorithm to obtain a \((3+\varepsilon)\)-approximate clustering, which requires a time complexity of \(\Omega(\varepsilon^{-2\dim}(k+z)^{2})\). In this paper, we strategically integrate additional information at each level to directly emulate the offline \(3\)-approximation greedy algorithm by Charikar _et al._[6] in the dynamic setting. This approach enables us to achieve a time complexity that is linearly dependent on \(k\) and remarkably independent of \(z\).

## 2 Our data structure

In this section, we describe our data structure, which has \(O(\log\Delta)\) levels. In each level \(r\), we have net \(N_{r}\), which is an \(r\)-net of \(N_{r/2}\). For each point \(p\in N_{r}\), we keep its weight \(w_{r}(p)\) and local neighborhoods \(B^{1}_{p,r}\) and \(B^{3}_{p,r}\), which are balls of radius \((1+O(\varepsilon))r\) and \((3+O(\varepsilon))r\) respectively. Then we maintain max-heap \(H_{r}\) which is designed to store the total weights of neighborhoods \(B^{1}_{p,r}\) for all \(p\in N_{r}\). We also keep \(\text{rep}_{r}(p)\) for each point \(p\in P\), which is the representative of \(p\) in \(N_{r}\). Below, we provide a detailed explanation for elements of our data structure.

Input and set \(R\).To start building our data structure, we require three input parameters: \(\Delta\), \(\varepsilon^{\prime}\), and \(\beta\), where \(0<\varepsilon^{\prime}<1\), and \(\frac{1}{2}<\beta\leq 1\). \(\Delta\) denotes the spread ratio of the underlying point set \(P\) and \(\varepsilon^{\prime}\) is the given error parameter. We utilize \(\beta\) to define \(R:=\{0\}\cup\{\beta\cdot 2^{\ell}\mid 0\leq\ell\leq\lceil\log_{2}\Delta\rceil+1, \ell\in\mathbb{Z}\}\) as a set of levels. Additionally, we define \(\varepsilon:=2^{\lfloor\log\varepsilon^{\prime}\rfloor}\). This implies that \(\varepsilon^{\prime}/2<\varepsilon\leq\varepsilon^{\prime}\). Furthermore, if \(r\in R\), then either \(\varepsilon r\in R\) or \(\varepsilon r<\beta\).

Nets \(N_{r}\).We maintain net \(N_{r}\) for each \(r\in R\), satisfying the following conditions: \(N_{0}=N_{\beta}=P_{t}\), where \(P_{t}\) is the set of points at time \(t\), and for \(r>\beta\), \(N_{r}\) is an \(r\)-net of \(N_{r/2}\). For \(p\in N_{r}\) and \(q\in N_{r/2}\), we say \(p\) is a _parent of \(q\)_ (and \(q\) is a _child of \(p\)_) if \(d(p,q)\leq r\). In our data structure, we store all the parents and children for each point for all the nets. It is important to note that each point can have at most \(2^{O(\dim)}\) parents or children. In the full version, we explain how to handle each insert/delete in time \(2^{O(\dim)}\log\Delta\).

During the paper, we may refer to \(N_{\varepsilon r}\) for \(0<\varepsilon r<\beta\), which means \(\varepsilon r\notin R\). Besides, note that since \(\text{d}_{\text{min}}=2\) and \(\beta<2\), \(P_{t}\) is an \(r\)-net for any \(0\leq r\leq\beta\). To solve this issue, we define \(N_{r}:=P_{t}\) for any \(0<r<\beta\), however, we do not explicitly keep any \(r\)-nets for \(0<r<\beta\) in our data structure to prevent redundancy.

Representatives rep. and weights \(w_{r}\).For all \(r\in R\), we inductively define functions \(\text{rep}_{r}\colon P_{t}\to N_{r}\) such that each point \(p\in P_{t}\) has a unique _representative_\(\text{rep}_{r}(p)\) in net \(N_{r}\). To define \(\text{rep}_{r}(p)\), we consider three cases. First, if \(p\in N_{r}\), then we define \(\text{rep}_{r}(p):=p\). Second, if \(p\notin N_{r}\) but \(p\in N_{r/2}\), we uniquely choose one of the parents of \(p\) in \(N_{r}\) as \(\text{rep}_{r}(p)\). Third, if \(p\notin N_{r/2}\) and therefore \(p\notin N_{r}\), we define \(\text{rep}_{r}(p)=\text{rep}_{r}(\text{rep}_{r/2}(p))\). Again to avoid inconsistency, we define \(\text{rep}_{r}(p):=p\) for any \(0<r<\beta\), but we do not store this explicitly. We show in Lemma 2.1 that the distance between each point \(p\) and its representative in \(N_{r}\) is at most \(2r\).

Next, we define a weight function \(w_{r}\) for every level. For every point \(p\in N_{r}\), its _weight_\(w_{r}(p)\) denotes the number of points that \(p\) is representing in \(N_{r}\). More formally, \(w_{r}(p)=\big{|}\{q\in P_{t}\mid\text{rep}_{r}(q)=p\}\big{|}\). For a set \(X\subseteq N_{r}\), we define \(w_{r}(X)\coloneqq\sum_{p\in X}w_{r}(p)\). Equivalently, \(w_{r}(X)=\big{|}\{y\in P_{t}\mid\text{rep}_{r}(y)\in X\}\big{|}\). Note that the sum of weights of all representatives at any level is equal to the total number of points, so \(w_{r}(N_{r})=\lvert P_{t}\rvert\) for all \(r\in R\).

Since we store all the parents and children, we can easily update the representatives and therefore the weights after each insert/delete. The details are presented in the full version.

**Balls \(B^{1}\) and \(B^{3}\).** Let \(r\in R\). For every point \(p\in N_{cr}\), we maintain the _close neighborhood_\(B^{1}_{p,r}\coloneqq\operatorname{Ball}(p,(1+4\varepsilon)r)\cap N_{er}\) and the _extended neighborhood_\(B^{3}_{p,r}\coloneqq\operatorname{Ball}(p,(3+8\varepsilon)r)\cap N_{er}\), where \(\operatorname{Ball}(p,\lambda)=\{q\in M\mid d(p,q)\leq\lambda\}\) denotes the ball of radius \(\lambda\) around \(p\) in the metric space \(M\). Then \(B^{1}\coloneqq\{B^{1}_{p,r}\mid r\in R,\ p\in N_{er}\}\) and \(B^{3}\coloneqq\{B^{3}_{p,r}\mid r\in R,\ p\in N_{er}\}\) are the collections of local neighborhoods at different scales. As mentioned in Lemma 2.2, the size of each of these neighborhoods is at most \(2^{O(\dim)}/\varepsilon^{\dim}\). We also show that we can update these neighborhoods in time \(2^{O(\dim)}\varepsilon^{-2\dim}\log\Delta\) after each insert/delete in the proof of Lemma 2.3.

**Heaps \(H_{r}\).** For every \(r\in R\), we maintain a max-heap \(H_{r}\) of the set \(\{(p,w_{cr}(B^{1}_{p,r}))\mid p\in N_{er}\}\) representing the weight of the close neighborhood of any point in level \(r\). With \(H_{r}[p]\) we denote the value of the max heap element with key \(p\). Since \(B^{1}_{p,r}\) might change, we need to find the position of the respective heap element by its key \(p\). Hence, for any level \(r\), we keep a pointer from every element in \(N_{r}\) to its corresponding element in \(H_{r}\).

Whenever an update happens to a set \(B^{1}_{p,r}\) for some point \(p\in N_{er}\), we need to update the value of \(w_{er}(B^{1}_{p,r})\) as well as the value of \(H_{r}[p]\). Each update in the max-heap \(H_{r}\) can be done in \(O(\log n)\) time using the standard max-heap operations. In the full version, we show that \(2^{O(\dim)\varepsilon^{-\dim}}\log\Delta\) updates happen to set \(B^{1}\). Therefore, we need \(2^{O(\dim)\varepsilon^{-\dim}}\log\Delta\log n\) time to update the heaps.

A few lemmas about our data structure.Now we mention a few useful lemmas about our data structure. The formal proofs can be found in the full version. The first lemma is about the distance of each point to its representative, which is bounded because of a geometric series argument.

**Lemma 2.1**.: _Let \(p\in P_{t}\) and \(r\in R\). Then \(d(p,\text{rep}_{r}(p))\leq 2r\). In particular, \(d(p,\text{rep}_{er}(p))\leq 2\varepsilon r\), where \(\varepsilon=2^{\lfloor\log\varepsilon^{\prime}\rfloor}\) for the error parameter \(\varepsilon^{\prime}\)._

We next discuss the size of close neighborhoods and extended neighborhoods.

**Lemma 2.2**.: _Let \(r\in R\) and \(p\in N_{er}\). The sets \(B^{1}_{p,r}\) and \(B^{3}_{p,r}\) defined above are of size at most \(2^{O(\dim)}\varepsilon^{-\dim}\). In particular, the size is \(O(\varepsilon^{-\dim})\) for a constant doubling dimension \(\dim\)._

In the next lemma, we show that the update time is logarithmic in the spread ratio and the size of \(P_{t}\).

**Lemma 2.3**.: _Let \(n\) be the size of point set \(P_{t}\). Then after insertion/deletion of any point to \(P_{t}\), we can update our data structure in time \(2^{O(\dim)}\varepsilon^{-2\dim}\log\Delta\log n\)._

Besides insertion and deletion updates, our data structure supports solution queries: in every time step, it can produce a solution in the form of a center set and a radius. We will discuss this in more detail and present an algorithm that outputs a \((3+O(\varepsilon))\)-approximation in time \(\varepsilon^{-O(\dim)}k\log n\log\log\Delta\) in the following section.

## 3 Our algorithm

OverviewFor a given level \(r\) in \(R\) and a number of centers \(k\), the main procedure MaxCoverage produces a set of \(k\) centers and a value \(\mathit{outliersWeight}\). One run of this procedure is deemed successful if \(\mathit{outliersWeight}\leq z\) at the end. Indeed, we will later show that \(\mathit{outliersWeight}\) is an upper bound on the actual number of outliers in the clustering with centers \(C\) and radius \((3+10\varepsilon)r\), which implies that this is a feasible solution. The algorithm FindCenters finds one such level for which MaxCoverage is successful. We also ensure that this level is chosen such that MaxCoverage is not successful on the next lower level \(r/2\), which guarantees that the radius of our solution does not deviate from the optimal solution by too much.

In our algorithm, we repeatedly find areas that subsume the most points. In order to do this efficiently, we maintain the collections of close neighborhoods and their weights in max-heaps. Fix a level \(r\in R\). For \(k\) iterations, the root node of the current heap corresponds to the next center of our solution. After picking it, we mark points in the extended neighborhood because our final approximate solution will cover these points and they should not contribute to the weight of other neighborhoods anymore.

As these points are now covered, we decrease the weights of heap elements representing points therein accordingly. These steps are performed by MaxCoverage. Note that for maintenance reasons, one run of findCenters should leave the original heaps unchanged. We could make a copy of the heap before processing it in MaxCoverage, but it is faster to work on the original heaps and just roll back all the updates done during MaxCoverage at the end of the algorithm, as described in Lemma 3.8.

It remains to find a good value for \(r\). By applying a binary search, we find a radius \(r\) such that the value of _outliersWeight_ outputted by MaxCoverage(\(k\), \(r\)) is at most \(z\), while the respective output on the lower level \(r/2\) is more than \(z\). Note that it is possible to find such a pair of subsequent radii via binary search although the search space might not be monotone. In the special case that there are at most \(z\) points in total, we can just set the radius to 0. This is done in Algorithm 1 (findCenters\((k,z)\)).

AnalysisNext, we prove the approximation guarantee and time complexity of our algorithm.

Let \(r\) and \(k\) be fixed, and let \(i\in[1,k]\) be an integer. For any point \(y\in N_{\varepsilon r}\), we say \(y\) is _marked_ after the \(i\)-th iteration of the loop in Line 3 of MaxCoverage, if \(y\in\bigcup_{j=1}^{i}B_{c_{j},r}^{3}\), and we say it is _unmarked_ if it is not the case. For simplicity, we may refer to a point as marked or unmarked without specifying the iteration if it is clear from the context. We define set \(U_{i}:=\{y\in N_{\varepsilon r}\mid y\notin\bigcup_{j=1}^{i}B_{c_{j},r}^{3}\}\) as the _set of unmarked points_ after the \(i\)-th iteration. We next define \(O_{i}:=\{p\in P_{t}\mid\text{rep}_{\varepsilon r}(p)\notin\bigcup_{j=1}^{i}B_ {c_{j},r}^{3}\}\) as the set of points that are _considered outliers_ after the \(i\)-th iteration of the loop. Equivalently, \(O_{i}:=\{p\in P_{t}\mid\text{rep}_{\varepsilon r}(p)\in U_{i}\}\). After the \(i\)-th iteration of the loop in Line 3, for each \(y\in N_{\varepsilon r}\), \(H_{r}[y]\) is the total weight of unmarked points in \(B_{y,r}^{1}\).

**Lemma 3.1** (Invariant for heaps).: _Let \(k\) be an integer, \(0\leq i\leq k\), and \(r\in R\). Then for any \(y\in N_{\varepsilon r}\) it holds \(H_{r}[y]=w_{\varepsilon r}(B_{y,r}^{1}\cap U_{i})\) after the \(i\)-th iteration._

findCenters\((k,z)\) returns a solution when _outliersWeight_\(\leq z\). To show that this solution is feasible, it remains to prove that _outliersWeight_ is an upper bound on the actual number of outliers of this solution.

**Lemma 3.2** (Feasible solution).: _Consider a radius \(r\in R\), and an integer \(k\). Then MaxCoverage\((k,r)\) returns \(k\) centers and a value outliersWeight, such that the total number of points in \(P_{t}\) that are not within distance \((3+10\varepsilon)r\) of these \(k\) centers is at most outliersWeight._

Proof.: Let _outliersWeight_ denote MaxCoverage\((k,r)\). _outliersWeight_. We want to prove that MaxCoverage\((k,r)\) returns \(\{c_{1},\ldots,c_{k}\}\) such that \(|P_{t}\setminus\bigcup_{j\leq k}\text{Ball}(c_{j},(3+10\varepsilon)r)|\leq\)_outliersWeight_. In every iteration \(i\), _outliersWeight_ is decreased by \(w_{\varepsilon r}(y)\) for every \(y\in B_{c_{i},r}^{3}\setminus\) Marked. After processing \(y\), we add it to Marked. This way, we ensure that no \(y\) is charged twice. Therefore, by the end of the algorithm, _outliersWeight_ is decreased by \(w_{\varepsilon r}(\bigcup_{j\leq k}B_{c_{j},r}^{3})\)Hence, in the end, the value of _outliersWeight_ is

\[\begin{split}\mathit{outliersWeight}&=n-w_{\varepsilon r}( \bigcup_{j\leq k}B^{3}_{c_{j},r})\\ &=\left|P_{t}\right|-\left|\{p\in P_{t}\mid\text{rep}_{\varepsilon r }(p)\in\bigcup_{j\leq k}B^{3}_{c_{j},r}\}\right|\.\end{split}\]

For all \(p\in P_{t}\), if \(\text{rep}_{\varepsilon r}(p)\in\bigcup_{j\leq k}B^{3}_{c_{j},r}\), then \(d(c_{j},\text{rep}_{\varepsilon r}(p))\leq(3+8\varepsilon)r\) holds for at least one \(j\leq k\) by definition of \(B^{3}_{c_{j},r}\). Using Lemma 2.1, this implies \(d(c_{j},p)\leq d(c_{j},\text{rep}_{\varepsilon r}(p))+d(\text{rep}_{ \varepsilon r}(p),p)\leq(3+8\varepsilon)r+2\varepsilon r=(3+10\varepsilon)r\). Hence, \(\{p\in P_{t}\mid\text{rep}_{\varepsilon r}(p)\in\bigcup_{j\leq k}B^{3}_{c_{j},r}\}\subseteq\{p\in P_{t}\mid\exists j\leq k\colon d(c_{j},p)\leq(3+10 \varepsilon)r\}\) and therefore

\[\begin{split}\mathit{outliersWeight}&\geq\left|P_{t} \right|-\left|\{p\in P_{t}\mid\exists j\leq k\colon d(c_{j},p)\leq(3+10 \varepsilon)r\}\right|\\ &=\left|P_{t}\right|-\left|\bigcup_{j\leq k}\operatorname{Ball}(c_ {j},(3+10\varepsilon)r)\cap P_{t}\right|\.\end{split}\]

Consider a fixed optimal solution. The following insight is crucial for our proof: In every iteration of MaxCoverage\((k,r)\) for \(r\geq\operatorname{OPT}\), we select a ball that covers a sufficient number of unmarked points. To be precise, it exceeds the number of currently considered outliers in any optimal cluster.

**Lemma 3.3** (Invariant for picking \(c_{i}\)).: _Let \(r\geq\operatorname{OPT}\), where \(r\in R\), and let \(c^{*}\in P_{t}\) be a center of an optimal solution for \(k\)-center with \(z\) outliers. Assume that we execute MaxCoverage\((k,r)\). Then for any \(i\in[1,k]\), the \(c_{i}\) picked in Line 4 is such that \(|\operatorname{Ball}(c^{*},OPT)\cap O_{i-1}|\leq w_{\varepsilon r}(B^{1}_{c_{i},r}\cap U_{i-1})\)._

Proof.: We define \(\hat{c}:=\text{rep}_{\varepsilon r}(c^{*})\), the representative of \(c^{*}\) in \(N_{\varepsilon r}\). Recall that \(B^{1}_{\hat{c},r}=\{y\in N_{\varepsilon r}\mid d(\hat{c},y)\leq(1+4\varepsilon )r\}\). Lemma 3.1 states that \(H[y]=w_{\varepsilon r}(B^{1}_{y,r}\cap U_{i-1})\) holds for all \(y\in N_{\varepsilon r}\) after the \((i-1)\)-th iteration. Since \(c_{i}\) is on top of the max-heap \(H\), we have \(w_{\varepsilon r}(B^{1}_{\hat{c},r}\cap U_{i-1})\leq w_{\varepsilon r}(B^{1}_{ c_{i},r}\cap U_{i-1})\). Therefore, it remains to show that \(|\operatorname{Ball}(c^{*},\operatorname{OPT})\cap O_{i-1}|\leq w_{\varepsilon r }(B^{1}_{\hat{c},r}\cap U_{i-1})\).

According to the definition of representative and weight function, we first have

\[|\operatorname{Ball}(c^{*},\operatorname{OPT})\cap O_{i-1}|\leq\sum_{y\in \text{rep}_{\varepsilon r}(\operatorname{Ball}(c^{*},\operatorname{OPT})\cap O _{i-1})}w_{\varepsilon r}(y)\enspace.\]

We next show that for any point \(p\in\operatorname{Ball}(c^{*},\operatorname{OPT})\cap P_{t}\), its representative on level \(\varepsilon r\) lies in \(B^{1}_{\hat{c},r}\), i.e. \(\text{rep}_{\varepsilon r}(p)\in B^{1}_{\hat{c},r}\). Let \(p\in\operatorname{Ball}(c^{*},\operatorname{OPT})\cap P_{t}\) be an arbitrary point, then \(d(c^{*},p)\leq\operatorname{OPT}\leq r\). By the triangle inequality, \(d(\hat{c},\text{rep}_{\varepsilon r}(p))\leq d(\hat{c},c^{*})+d(c^{*},p)+d(p, \text{rep}_{\varepsilon r}(p))\). Besides, Lemma 2.1 implies that \(d(\hat{c},c^{*})\leq 2\varepsilon r\) and \(d(p,\text{rep}_{\varepsilon r}(p))\leq 2\varepsilon r\). Putting everything together we have

\[d(\hat{c},\text{rep}_{\varepsilon r}(p))\leq d(\hat{c},c^{*})+d(c^{*},p)+d(p, \text{rep}_{\varepsilon r}(p))\leq 2\varepsilon r+r+2\varepsilon r=(1+4 \varepsilon)r\enspace.\]

Besides, \(p\in O_{i-1}\) is equivalent to \(\text{rep}_{\varepsilon r}(p)\in U_{i-1}\). Therefore, \(p\in\operatorname{Ball}(c^{*},\operatorname{OPT})\cap O_{i-1}\) implies \(\text{rep}_{\varepsilon r}(p)\in B^{1}_{\hat{c},r}\cap U_{i-1}\), and we have \(\text{rep}_{\varepsilon r}(\operatorname{Ball}(c^{*},\operatorname{OPT})\cap O _{i-1})\subseteq B^{1}_{\hat{c},r}\cap U_{i-1}\). Thus,

\[\sum_{y\in\text{rep}_{\varepsilon r}(\operatorname{Ball}(c^{*},\operatorname{ OPT})\cap O_{i-1})}w_{\varepsilon r}(y)\leq\sum_{y\in B^{1}_{\hat{c},r}\cap U_{i-1}}w_{ \varepsilon r}(y)=w_{\varepsilon r}(B^{1}_{\hat{c},r}\cap U_{i-1})\enspace,\]

which finishes the proof. 

Utilizing Lemma 3.3, we can now show that the weight of the final output of MaxCoverage\((k,r)\) with \(r\geq\operatorname{OPT}\) is at least the number of points covered by any optimal solution.

**Lemma 3.4** (Invariant for coverage weight).: _Let \(C^{*}\) be the set of \(k\) centers of an optimal solution for \(k\)-center with \(z\) outliers on \(P_{t}\), and let \(\operatorname{OPT}\) be the radius of this solution. Let \(r\geq\operatorname{OPT}\), where \(r\in R\). Then,_

\[\left|\bigcup_{c^{*}\in C^{*}}\operatorname{Ball}(c^{*},OPT)\cap P_{t}\right| \leq w_{\varepsilon r}\left(\bigcup_{j=1}^{k}B^{3}_{c_{j},r}\right)\]

_(recall that \(B^{3}_{c_{j},r}=\operatorname{Ball}(c_{j},(3+8\varepsilon)r)\cap N_{ \varepsilon r}\) and \(w_{\varepsilon r}(B^{3}_{c_{j},r})=|\{y\in P_{t}\mid\text{rep}_{\varepsilon r }(y)\in B^{3}_{c_{j},r}\}|\)._Proof.: To prove the lemma, we show that for any \(i\in[0,k]\), there is a charging that satisfies the following conditions: 1) There is a set \(\{c_{1}^{*},\ldots,c_{i}^{*}\}\subseteq C^{*}\) and a function that charges each point in \(\bigcup_{j=1}^{i}\operatorname{Ball}(c_{j}^{*},OPT)\cap P_{t}\) to a point \(y\in\bigcup_{j=1}^{i}B^{3}_{c_{j},r}\), and 2) for any point \(y\in\bigcup_{j=1}^{i}B^{3}_{c_{j},r}\), at most \(w_{er}(y)\) points from \(\bigcup_{j=1}^{i}\operatorname{Ball}(c_{j}^{*},OPT)\cap P_{t}\) are mapped to \(y\).

We prove this claim by induction on \(i\). The base case \(i=0\) trivially holds. Assume that \(i\geq 1\) and the induction hypothesis holds for \(i-1\). That is, there is a charging satisfying conditions 1 and 2 for \(i-1\). Then, we prove that these conditions hold for \(i\).

To define center \(c_{i}^{*}\), we consider two cases:

**Case 1:**: there exists a center \(c^{*}\in C^{*}\setminus\{c_{1}^{*},\ldots,c_{i-1}^{*}\}\) for which \(B^{1}_{c_{i},r}\) hits the set of representatives of \(\operatorname{Ball}(c^{*},\operatorname{OPT})\). That is, we have \(\text{rep}_{er}(\operatorname{Ball}(c^{*},\operatorname{OPT}))\cap B^{1}_{c_{ i},r}\neq\emptyset\). We then let \(c_{i}^{*}=c^{*}\). We map every point of \(\operatorname{Ball}(c^{*},\operatorname{OPT})\) to its unique representative.
**Case 2:**: the first case is not correct. Then, we let \(c_{i}^{*}\) be any arbitrary point in \(C^{*}\setminus\{c_{1}^{*},\ldots,c_{i-1}^{*}\}\). In this case, we charge the uncharged points of \(\operatorname{Ball}(c_{i}^{*},\operatorname{OPT})\) to \(B^{1}_{c_{i},r}\).

We first consider Case 1. We charge each point \(p\in\operatorname{Ball}(c_{i}^{*},OPT)\) to its representative \(\text{rep}_{er}(p)\). We next prove the following claim:

_Claim._ Let \(p\in\operatorname{Ball}(c_{i}^{*},OPT)\). If Case 1 happens, then for every point \(q\in P_{t}\) that is charged to \(\text{rep}_{er}(p)\), it holds that \(\text{rep}_{er}(q)=\text{rep}_{er}(p)\).

Proof.: For the sake of contradiction, assume that there is a point \(q\in P_{t}\) such that \(q\) is charged to \(\text{rep}_{er}(p)\) and \(\text{rep}_{er}(q)\neq\text{rep}_{er}(p)\). It means that at an iteration \(j<i\), Case 2 happened and \(q\) is charged to \(\text{rep}_{er}(p)\). Since Case 2 happened at iteration \(j\), we have \(\text{rep}_{er}(\operatorname{Ball}(c_{i}^{*},OPT))\cap B^{1}_{c_{j},r}=\emptyset\) and also \(q\) is charged to a point in \(B^{1}_{c_{j},r}\). Adding it to the assumption that \(q\) is charged to \(\text{rep}_{er}(p)\) we have \(\text{rep}_{er}(p)\in B^{1}_{c_{j},r}\). Besides, \(p\in\operatorname{Ball}(c_{i}^{*},OPT)\) and therefore we have \(\text{rep}_{er}(p)\in\text{rep}_{er}(\operatorname{Ball}(c_{i}^{*},OPT))\). This implies that \(\text{rep}_{er}(p)\in\text{rep}_{er}(\operatorname{Ball}(c_{i}^{*},OPT))\cap B ^{1}_{c_{j},r}\), which is a contradiction to \(\text{rep}_{er}(\operatorname{Ball}(c_{i}^{*},OPT))\cap B^{1}_{c_{j},r}=\emptyset\). 

The claim implies that for each point \(p\in\operatorname{Ball}(c^{*},OPT)\), at most \(w_{er}(\text{rep}_{er}(p))\) points are charged to \(\text{rep}_{er}(p)\). See Figure 2, where the left blue ball represents \(B^{1}_{c_{i},r}\), the right blue one indicates the area that points from \(\text{rep}_{er}(\operatorname{Ball}(c^{*},\operatorname{OPT}))\) can lie in, and the orange circle represents \(\operatorname{Ball}(c^{*},\operatorname{OPT})\). For each point \(p\in\operatorname{Ball}(c^{*},OPT)\), we have \(\text{rep}_{er}(p)\in B^{3}_{c_{i},r}\): Let \(q\in\operatorname{Ball}(c^{*},\operatorname{OPT})\) be such that \(\text{rep}_{er}(p)\in\text{rep}_{er}(\operatorname{Ball}(c^{*},\operatorname {OPT}))\cap B^{1}_{c_{i},r}\). The distance of \(c_{i}\) to any representative of a point \(p\in\operatorname{Ball}(c^{*},\operatorname{OPT})\) is

\[d(c_{i},\text{rep}_{er}(p)) \leq d(c_{i},\text{rep}_{er}(q))+d(\text{rep}_{er}(q),q)+d(q,c^{* })+d(c^{*},p)+d(p,\text{rep}_{er}(p))\] \[\leq(1+4\varepsilon)r+2\varepsilon r+2\operatorname{OPT}+2 \varepsilon r\leq(3+8\varepsilon)r,\]

implying that \(\text{rep}_{er}(p)\in B^{3}_{c_{i},r}\).

For Case 2: note that if \(p\in P_{t}\setminus O_{i-1}\), then by definition of \(O_{i-1}\), we have \(\text{rep}_{er}(p)\notin U_{i-1}\), and hence \(\text{rep}_{er}(p)\in\cup_{j=1}^{i-1}B^{1}_{c_{j},r}\) by definition of \(U_{i-1}\). So, by induction hypothesis, \(p\) is already charged to \(\text{rep}_{er}(p)\). Therefore, we only need to charge each point \(p\in\operatorname{Ball}(c^{*},OPT)\cap O_{i-1}\). By Lemma 3.3 we have \(|\operatorname{Ball}(c^{*},OPT)\cap O_{i-1}|\leq w_{er}(B^{1}_{c_{i},r}\cap U _{i-1})\). Also, we only charged to

Figure 2: Illustration of the claim’s consequences.

the points in \(\cup_{j=1}^{i-1}B^{3}\), which means that no point is charged a point in \(U_{i-1}\). Therefore, points in \(\mathrm{Ball}(c^{*},OPT)\cap O_{i-1}\) can be charged to points in \(B^{1}_{c_{i},r}\cap U_{i-1}\). 

**Lemma 3.5**.: _Let \(r\in R\) and \(OPT\) be the optimal radius for \(k\)-center clustering of \(P_{t}\) with \(z\) outliers. Then if \(r\geq OPT\), the value \(\mathit{outliersWeight}\) returned by \(\textsc{MaxCoverage}(k,r)\) is at most \(z\)._

Proof.: Let \(C=\{c_{1},\ldots,c_{k}\}\) be the set of centers returned by \(\textsc{MaxCoverage}(k,r)\) and \(C^{*}\) be the set of centers of an optimal \(k\)-center clustering of \(P_{t}\) with \(z\) outliers. By the same reasoning as in the proof of Lemma 3.2, it is

\[\mathit{outliersWeight}=n-w_{\varepsilon r}(\cup_{i=1}^{k}B^{3}_{c_{i},r})\enspace.\]

Besides, Lemma 3.4 states that \(w_{er}(\cup_{i=1}^{k}B^{3}_{c_{i},r})\geq|\cup_{c^{*}\in C^{*}}\mathrm{Ball}(c ^{*},OPT)\cap P_{t}|\). Moreover, as \(C^{*}\) are the centers of an optimal solution, \(n-|\cup_{c^{*}\in C^{*}}\mathrm{Ball}(c^{*},OPT)\cap P_{t}|\leq z\). Putting everything together, we have

\[\mathit{outliersWeight}\leq n-w_{\varepsilon r}(\cup_{i=1}^{k}B^{3}_{c_{i},r} )\leq n-|\cup_{c^{*}\in C^{*}}\mathrm{Ball}(c^{*},OPT)\cap P_{t}|\leq z\enspace,\]

which finishes the proof. 

Lemma 3.2 and Lemma 3.5 together imply that the solution returned by Algorithm 1 is feasible. Further, it can be shown that the level \(\hat{r}\) at which \(\textsc{findCenters}(k,z)\) becomes successful, fulfills \(\hat{r}\leq r^{*}\). Together, this implies the following statement.

**Lemma 3.6** (Approximation guarantee).: _Let \(OPT>0\) be the optimal radius for the \(k\)-center clustering of \(P_{t}\) with \(z\) outliers, and let \(r^{*}\) be the minimum number in \(R\) such that \(r^{*}\geq OPT\). Then \(\textsc{findCenters}(k,z)\) returns a \((3+10\varepsilon)\rho\)-approximate solution for \(k\)-center clustering problem with \(z\) outliers, where \(\rho=\frac{r^{*}}{\mathrm{OPT}}\)._

For \(\rho\) defined in Lemma 3.6, \(1\leq\rho<2\) holds according to the definition of \(R\). It immediately leads to a \((6+O(\varepsilon))\)-approximation algorithm. Next in Lemma 3.7, we show how to get \(\rho\leq(1+\varepsilon)\), which improves the approximation ratio of our algorithm to \(3+O(\varepsilon)\). The idea is to run parallel instances of our data structure with different values for parameter \(\beta\), so that always \(OPT\leq r<(1+\varepsilon)OPT\) holds for a \(r\in R\) in one of the instances.

**Lemma 3.7** (Optimizing the approximation ratio).: _Let \(OPT>0\), and let \(\varepsilon>0\) be fixed. We define \(m:=\lceil 1/\log_{2}{(1+\varepsilon)}\rceil\). Suppose we have \(m\) parallel instances of our data structure with parameter \(\beta=2^{j/m-1}\) for the \(i\)-th instance. Then in at least one of the instances, we find a \((3+O(\varepsilon))\)-approximation solution for \(k\)-center clustering with \(z\) outliers by calling \(\textsc{findCenters}(k,z)\)._

The procedure \(\textsc{MaxCoverage}(k,r)\) needs to temporarily edit the heap \(H_{r}\). This can be done by working on a copy of the heap, as indicated in line 2. The following result describes how this can be done faster, without actually copying the heap. The idea is to work in place and roll back all changes in the end. The formal proof is given in the full version.

**Lemma 3.8** (Imitating of copying \(H_{r}\)).: _Let \(u\) be the number of times that \(\textsc{MaxCoverage}(k,r)\) updates heap \(H_{r}\). Then copying heap \(H_{r}\) in Line 2, as well as all these \(u\) updates, can be handled in total time \(O(u\log n)\)._

By using Lemma 3.8 and Lemma 2.2, we get the query time in Lemma 3.9.

**Lemma 3.9** (Query time).: _Let \(k\) and \(z\) be two given parameters. Then \(\textsc{findCenters}(k,z)\) described in Algorithm 1 has a time complexity of \(2^{O(\dim)}\varepsilon^{-2\dim}k\log n\log\log\Delta\)._

## 4 Conclusion

We developed a data structure for the fully dynamic \(k\)-center with \(z\) outliers problem in metrics of bounded doubling dimension. As compared to other works, the algorithm exhibits an improved query time while achieving the currently best-known approximation ratio of \(3+\varepsilon\). The query time is optimal with respect to the dependency on \(k\). Although the exponential dependency on the doubling dimension in the running times seems necessary, it is actually not clear if this could be improved to tackle even wider classes of metric spaces. This would be an interesting aspect for future work. However, Chan et al. [4] show that an amortized running time of \(\Omega(z)\) would be needed in general metric spaces.

Acknowledgements

The authors would like to thank the anonymous reviewers for their insightful comments. Annika Hennes' and Melanie Schmidt's research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - project number 456558332.

## References

* [1] MohammadHossein Bateni, Hossein Esfandiari, Hendrik Fichtenberger, Monika Henzinger, Rajesh Jayaram, Vahab Mirrokni, and Andreas Wiese. Optimal fully dynamic \(k\)-center clustering for adaptive and oblivious adversaries. In _Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 2677-2727. Society for Industrial and Applied Mathematics, 2023.
* [2] Deeparnab Chakrabarty, Prachi Goyal, and Ravishankar Krishnaswamy. The non-uniform \(k\)-center problem. In _Proceedings of the 43rd International Colloquium on Automata, Languages, and Programming (ICALP)_, volume 55 of _LIPIcs. Leibniz Int. Proc. Inform._, pages Art. No. 67, 15. Schloss Dagstuhl. Leibniz-Zent. Inform., Wadern, 2016.
* [3] T-H. Hubert Chan, Arnaud Guerqin, and Mauro Sozio. Fully dynamic \(k\)-center clustering. In _Proceedings of the 2018 World Wide Web Conference_, pages 579-587, 2018.
* [4] T-H. Hubert Chan, Silvio Lattanzi, Mauro Sozio, and Bo Wang. Fully dynamic \(k\)-center clustering with outliers. In _Computing and Combinatorics: 28th International Conference, COCOON 2022, Shenzhen, China, October 22-24, 2022, Proceedings_, pages 150-161. Springer, 2023.
* [5] Moses Charikar, Chandra Chekuri, Tomas Feder, and Rajeev Motwani. Incremental clustering and dynamic information retrieval. In _Proceedings of the twenty-ninth annual ACM symposium on Theory of computing_, pages 626-635, 1997.
* [6] Moses Charikar, Samir Khuller, David M. Mount, and Giri Narasimhan. Algorithms for facility location problems with outliers. In S. Rao Kosaraju, editor, _Proceedings of the Twelfth Annual Symposium on Discrete Algorithms, January 7-9, 2001, Washington, DC, USA_, pages 642-651. ACM/SIAM, 2001.
* [7] Mark de Berg, Leyla Biabani, and Morteza Monemizadeh. \(k\)-center clustering with outliers in the MPC and streaming model. _CoRR_, abs/2302.12811, 2023.
* [8] Hendrik Fichtenberger, Monika Henzinger, and Andreas Wiese. On fully dynamic constant-factor approximation algorithms for clustering problems. _CoRR_, abs/2112.07217, 2021.
* [9] Teofilo F Gonzalez. Clustering to minimize the maximum intercluster distance. _Theoretical computer science_, 38:293-306, 1985.
* [10] Gramoz Goranci, Monika Henzinger, Dariusz Leniowski, Christian Schulz, and Alexander Svozil. Fully dynamic \(k\)-center clustering in low dimensional metrics. In _2021 Proceedings of the Workshop on Algorithm Engineering and Experiments (ALENEX)_, pages 143-153. SIAM, 2021.
* [11] Anupam Gupta, Robert Krauthgamer, and James R Lee. Bounded geometries, fractals, and low-distortion embeddings. In _Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science, 2003_, pages 534-543. IEEE, 2003.
* [12] Juha Heinonen. _Lectures on analysis on metric spaces_. Springer Science & Business Media, 2001.
* [13] Dorit S Hochbaum and David B Shmoys. A unified approach to approximation algorithms for bottleneck problems. _Journal of the ACM (JACM)_, 33(3):533-550, 1986.
* [14] Wen-Lian Hsu and George L. Nemhauser. Easy and hard bottleneck location problems. _Discrete Applied Mathematics_, 1(3):209-215, 1979.

* [15] Robert Krauthgamer and James R Lee. Navigating nets: Simple algorithms for proximity search. In _Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms_, pages 798-807. Citeseer, 2004.
* [16] Stuart P. Lloyd. Least squares quantization in PCM. _IEEE Transactions on Information Theory_, 28(2):129-137, 1982. Originally published in 1957 as Bell Laboratories Technical Memorandum.
* [17] Richard Matthew McCutchen and Samir Khuller. Streaming algorithms for \(k\)-center clustering with outliers and with anonymity. In _Approximation, Randomization and Combinatorial Optimization. Algorithms and Techniques_, pages 165-178. Springer, 2008.
* [18] Paolo Pellizzoni, Andrea Pietracaprina, and Geppino Pucci. Fully dynamic clustering and diversity maximization in doubling metrics. _CoRR_, abs/2302.07771, 2023.
* [19] Melanie Schmidt and Christian Sohler. Fully dynamic hierarchical diameter \(k\)-clustering and \(k\)-center. _arXiv preprint arXiv:1908.02645_, 2019.

[MISSING_PAGE_FAIL:13]

follows that \(\mathrm{Ball}(p,\gamma\cdot r)\) can be covered by \(\gamma^{\dim}\cdot 2^{O(\dim)}\) balls of radius \(r/2\). Adding it to \(S_{r}\subseteq N_{r}\) implies that each of these balls of radius \(r/2\) contains at most one point from \(S_{r}\). Therefore, \(S_{r}\) is of size \(\gamma^{\dim}\cdot 2^{O(\dim)}\). 

**Lemma A.5**.: _Let \(p\in P\) and \(\gamma\geq 2\) be given. Then, the procedure \(\textsc{FindNeighbours}(p,\gamma)\) runs in time \(2^{O(\dim)}\gamma^{\dim}\log\Delta\)._

Proof.: The while loop in Line 4 iterates backward through all the \(O(\log\Delta)\) levels. On every level, all children of all \(s\in S_{2r}\) are examined. By Lemma A.4, \(|S_{2r}|\in\gamma^{\dim}2^{O(\dim)}\) and by Lemma A.1, every \(s\in|S_{2r}|\) has \(2^{2\dim}\) children. Multiplying these terms yields the claim. 

### Insertion

In this section, we show how to maintain our data structure after the insertion of point \(p\). As described in Algorithm 2, we start by calling FindNeighbours, which goes through the levels from top to bottom (high to low) and recursively computes set \(S_{r}\) for each level \(r\). Lemma A.3 implies that for any point \(q\in N_{r}\), if \(d(p,q)\leq r\), then \(S_{r}\) contains \(q\). This guarantees that later we can find all parents and children of \(p\) using these \(S_{r}\) sets. Next, we go through the levels from bottom to top to find the lowest level \(\hat{r}\), in which we can find a representative for \(p\), or in other words, there is a \(q\in N_{\hat{r}}\) within distance \(\hat{r}\) of \(p\). Then, we add \(p\) to all levels lower than \(\hat{r}\). For levels \(r>\hat{r}\), we do not need to add \(p\) to \(N_{r}\) because there is already a point in level \(r\) that can represent \(p\) by the definition of \(\hat{r}\). We determine this representative recursively according to \(\mathrm{rep}_{\hat{r}}(p)\). Finally, we increase the weight for the respective representative of \(p\) in each level by one.

```
1\(r_{\max}:=\beta\cdot 2^{[\log_{2}\Delta]}\)
2\(S_{\beta}\), \(S_{2\beta}\), \(S_{4\beta}\), \(S_{8\beta}\), \(\ldots\), \(S_{r_{\max}}\leftarrow\textsc{FindNeighbours}(p,2)\)
3// finding the lowest level \(\hat{r}\) that can have a representative for\(p\)
4\(r\leftarrow\beta\), \(\hat{r}\gets 2r_{\max}\)
5while\(r\leq r_{\max}\)do
6ifthere is a point \(s\in S_{r}\) such that \(d(p,s)\leq r\)then
7\(\hat{r}\gets r\)
8\(\mathrm{rep}_{\hat{r}}(p)\gets s\)
9forall\(s\in S_{r}\) such that \(d(p,s)\leq r\)do
10 Save \(s\) as a parent of \(p\) and save \(p\) as a child of \(s\)
11break
12\(r\gets 2r\)
13// adding \(p\) to all levels lower than \(\hat{r}\)
14forall\(r\in R\) such that \(r<\hat{r}\)do
15\(N_{r}\gets N_{r}\cup\{p\}\)
16 Save \(p\in N_{r}\) as a parent of \(p\in N_{r/2}\) and save \(p\in N_{r/2}\) as a child of \(p\in N_{r}\)
17\(\mathrm{rep}_{r}(p)\gets p\)
18UpdateNeighbourhoodAdd\((p,\hat{r})\)
19// finding the representative of \(p\) in all levels upper than \(\hat{r}\)
20\(r\gets 2\hat{r}\)
21while\(r\leq r_{\max}\)do
22\(\mathrm{rep}_{r}(p)\leftarrow\mathrm{rep}_{r}(\mathrm{rep}_{r/2}(p))\)
23\(r\gets 2r\)
24// updating the weight function
25forall\(r\in R\)do
26\(w_{r}(\mathrm{rep}_{r}(p))\gets w_{r}(\mathrm{rep}_{r}(p))+1\)
```

**Algorithm 2**Insert\((p)\)

After inserting the point \(p\) to \(N_{er}\), we need to compute \(B^{1}_{p,r}\) and \(B^{3}_{p,r}\). Besides, for any point \(q\in B^{1}_{p,r}\) (\(q\in B^{3}_{p,r}\) similarly), we need to add \(p\) to \(B^{1}_{q,r}\) (\(B^{3}_{q,r}\) similarly). We perform this operation in Procedure UpdateNeighbourhoodAdd. In Lemma A.6, we analyze the correctness and update time of this procedure.

**Lemma A.6**.: _Let \(p\) be a point to be inserted and \(\hat{r}\) as in the algorithm after execution of the while loop in Line 5. Then, UpdateNeighbourhoodAdd\((p,\hat{r})\) that we invoke in Line 18 of Algorithm 2, finds \(B^{1}_{p,r}\) and \(B^{3}_{p,r}\) for any \(r\in R\) such that \(\varepsilon r<\hat{r}\). Besides, the total update time of this procedure is \(2^{O(\dim)}\varepsilon^{-\dim}\log\Delta\)._

Proof.: By Lemma A.3, \(T_{r^{\prime}}\) contains all points of \(N_{r^{\prime}}\) within distance \((3+8\varepsilon)r^{\prime}/\varepsilon\) of \(p\). Then if we define \(r:=r^{\prime}/\varepsilon\), it holds that \(T_{r^{\prime}}\) contains all points of \(N_{er}\) within distance \((3+8\varepsilon)r^{\prime}/\varepsilon=(3+8\varepsilon)r\) of \(p\). This means that we can find \(B^{3}_{p,r}\) by exploring \(T_{r^{\prime}}\), which we do in Line 8 of the procedure. By restricting to points \(q\) in \(T_{r^{\prime}}\) such that \(d(p,q)\leq(1+4\varepsilon)r\), we find all the points that belong to \(B^{1}_{p,r}\). We next discuss the running time. Lemma A.5 states that the running time of FindNeighbours\((p,(3+8\varepsilon)/\varepsilon)\) is \(2^{O(\dim)}\varepsilon^{-\dim}\log\Delta\). Then, we iterate over \(T_{r^{\prime}}\) for at most \(O(\log\Delta)\) values of \(r^{\prime}\), which takes \(O(|T_{r^{\prime}}|)\) for each iteration. Lemma A.4 shows that \(T_{r^{\prime}}\) is of size \(((3+8\varepsilon)/\varepsilon)^{\dim}2^{O(\dim)}=2^{O(\dim)}\varepsilon^{-\dim}\). Therefore, the total running time of these \(O(\log\Delta)\) iterations is also \(2^{O(\dim)}\varepsilon^{-\dim}\log\Delta\), which finishes the proof. 

Updating the heaps. To avoid redundancy, we do not include updating Heaps \(H_{r}\) in the pseudocodes. However, it can be easily handled. Note that neighborhoods \(B^{1}_{x,r}\) and \(B^{1}_{y,r}\) are balls of the same size. Therefore, \(y\in B^{1}_{x,r}\) is equivalent to \(x\in B^{1}_{y,r}\). Hence, whenever the weight of a point \(x\) in \(N_{r}\) is changed, it is enough to update the weight of all points in \(B^{1}_{x,r}\) and their corresponding element in heap \(H_{r}\). The size of \(B^{1}_{x,r}\) is \(2^{O(\dim)}\varepsilon^{-\dim}\) by Lemma 2.2, and we can do each heap update in \(O(\log n)\) time. Therefore, an extra \(2^{O(\dim)}\varepsilon^{-\dim}\log n\) factor will be multiplied by the running time of Algorithm 2 due to maintaining the heaps.

**Lemma A.7**.: _Let \(n\) be the size of point set \(P_{t}\). Then after insertion of any point to \(P_{t}\), we can update our data structure in time \(2^{O(\dim)}\varepsilon^{-2\dim}\log\Delta\log n\)._

Proof.: As we discussed above, a \(2^{O(\dim)}\varepsilon^{-\dim}\log n\) factor will be multiplied by the running time of Algorithm 2 due to maintaining the heaps. Therefore, it is enough to show that the running time of Algorithm 2 is \(2^{O(\dim)}\varepsilon^{-\dim}\log\Delta\).

In Algorithm 2, we first call FindNeighbours\((p,2)\), which takes \(2^{O(\dim)}\log\Delta\) according to Lemma A.5. To find \(\hat{r}\), we next iterate over \(S_{r}\) for at most \(O(\log\Delta)\) values of \(r\). Then, we add \(p\) to all levels lower than \(\hat{r}\), which takes time \(\log\Delta\). We next invoke UpdateNeighbourhoodAdd\((p,\hat{r})\), which is done in time \(2^{O(\dim)}\varepsilon^{-\dim}\log\Delta\) according to Lemma A.6. This takes \(2^{O(\dim)}\log\Delta\), since \(S_{r}\) is of size \(2^{O}(\dim)\) by Lemma A.4. Finally, we update the representatives of \(p\) and their weights, which is done in time \(O(\log\Delta)\). Putting everything together proves that the running time of Algorithm 2 is \(2^{O(\dim)}\varepsilon^{-\dim}\log\Delta\), which finishes the proof.

### Deletion

In this section, we explain how to maintain our data structure after the deletion of point \(p\). As described in Algorithm 3, we first produce sets \(S_{r}\) and \(T_{r}\) for all \(r\in R\) that are used for updating parents and children as well as neighborhoods. We then iterate over the levels \(r\in R\) from bottom to top. During the algorithm, we may elevate some points from \(N_{r/2}\setminus N_{r}\) to \(N_{r}\) to maintain our data structure. We refer to the set of these points that are elevated to \(N_{r}\) as \(E_{r}\). After changing the representative of a point, we need to change the weights of its previous representative in the next level as well as the indirect previous representatives in the upper levels. To make this process of updating weights efficient, we define the set \(D_{r}\) and a function \(\delta_{r}\colon D_{r}\to\mathbb{Z}\) for each \(r\in R\). The set \(D_{r}\) refers to the set of points \(q\in N_{r}\) whose weight should be updated by the value \(\delta_{r}[q]\).

After deleting \(p\), we need to find a new representative for any point \(q\in N_{r/2}\) and also reduce \(w_{r/2}(q)\) from its previous indirect representatives in the upper levels. To do this, we decrease \(\delta_{2r}[\text{rep}_{2r}(p)]\) by \(w_{r/2}(q)\). After deleting \(p\) from \(N_{r}\), any point \(q\in N_{r/2}\) that was represented by \(p\) in \(N_{r}\) would not have any representative. Besides, we need to find a representative in \(N_{r}\) for the points in \(E_{r/2}\). Let \(q\in N_{r/2}\) be a point such that \(\text{rep}_{r}(q)=p\) or \(q\in E_{r/2}\). To find a new representative for \(q\) in \(N_{r}\), we consider two cases: either \(q\) has a parent in \(N_{r}\setminus\{p\}\) or \(p\) is the only parent of \(q\) in \(N_{r}\).

For the first case, we choose a parent \(x\in N_{r}\setminus\{p\}\) of \(q\) as its representative in \(N_{r}\). Then, we increase \(w_{r}(x)\) and \(\delta_{2r}[\text{rep}_{2r}(x)]\) by \(w_{r/2}(q)\) to update the weight of all points that represent \(q\) in higher levels directly or indirectly. For the second case, we have to elevate \(q\) to \(N_{r}\). After elevating \(q\) to \(N_{r}\), we need to find its parents and children. To find the children, it is enough to explore \(S_{r/2}\). It is because if \(s\in N_{r/2}\) is a child of \(q\in N_{r}\), then \(d(q,s)\leq r\). Adding it to \(d(p,q)\leq r\), we have \(d(p,s)\leq 2r=4(r/2)\) by triangle inequality. Therefore, \(s\in S_{r/2}\) and it is enough to explore \(S_{r/2}\). With a similar argument, it is enough to explore \(S_{2r}\) to find the parents of \(q\) in \(N_{2r}\).

We next call UpdateNeighbourhoodElevate\((q,r,T_{r})\). In Lemma A.9, we show that this procedure can compute \(B^{1}_{q,r/\varepsilon}\) and \(B^{3}_{q,r/\varepsilon}\) by exploring \(T_{r}\).

At the end of each iteration, we call UpdateNeighbourhoodRemove\((p,r)\). This procedure deletes \(p\) from all the neighborhoods of level \(r\) that contain \(p\). Note that for \(i\in\{1,3\}\), \(p\in B^{i}_{x,r}\) and \(x\in B^{i}_{p,r}\) are equivalent. Hence, it is enough to iterate over all \(x\in B^{i}_{p,r}\) and delete \(p\) from \(B^{i}_{x,r}\) for \(i=\{1,3\}\).

To show that UpdateNeighbourhoodElevate computes the neighborhoods in Lemma A.9, we first prove Lemma A.8, which is about the distance of the points in \(E_{r}\) to \(p\).

**Lemma A.8**.: _Let \(r\in R\), and let \(E_{r}\) be the set of elevated points in Algorithm 3. Then, for any point \(q\in E_{r}\) we have \(d(p,q)\leq r\)._

Proof.: Let \(\hat{r}\) be the smallest level in \(R\) such that \(q\) is elevated to it. It means that \(q\) is a child of \(p\in N_{\hat{r}}\), and therefore, \(d(p,q)\leq\hat{r}\). Besides, we have \(\hat{r}\leq r\), which implies \(d(p,q)\leq r\). 

We next call UpdateNeighbourhoodElevate to create \(B^{1}_{q,r}\) and \(B^{3}_{q,r}\), as well as to add \(q\) to the neighborhoods in level \(r\) that must contain \(q\).

**Lemma A.9**.: _Let \(r^{\prime}\in R\), \(q\in E_{r^{\prime}}\) and \(T_{r^{\prime}}\) as constructed in Line 3 of Algorithm 3. We define \(r:=r^{\prime}/\varepsilon\). The procedure UpdateNeighbourhoodElevate\((q,r^{\prime},T_{r^{\prime}})\) computes \(B^{1}_{q,r}\) and \(B^{3}_{q,r}\) and updates the neighborhoods \(B^{1}_{q^{\prime},r}\) (and \(B^{3}_{q^{\prime},r^{\prime}}\)) for all \(q^{\prime}\in B^{1}_{q,r}\) (\(q^{\prime}\in B^{3}_{q,r}\)) by adding \(q\) to them. Besides, this procedure runs in time \(O(\varepsilon^{-\dim 2O(\dim)})\)._

Proof.: Let \(p\) be the point to be deleted when calling Algorithm 3. \(q\in E_{r^{\prime}}\) implies that either \(p\) was the representative of \(q\) in \(N_{r^{\prime}}\), or \(q\in E_{r^{\prime}/2}\). If \(p\) was the representative of \(q\) in \(N_{r^{\prime}}\), we have \(d(p,q)\leq r^{\prime}\). Otherwise, if \(q\in E_{r^{\prime}/2}\), Lemma A.8 states that \(d(p,q)\leq r^{\prime}/2\leq r^{\prime}\). Therefore, \(d(p,q)\leq r^{\prime}\) always holds. By Lemma A.3, \(T_{r^{\prime}}\) contains all points \(x\in N_{r^{\prime}}\) such that \(d(p,x)\leq(1+(3+8\varepsilon)/\varepsilon)r^{\prime}=r^{\prime}+(3+8\varepsilon)r\), for \(r=r^{\prime}/\varepsilon\). Let \(q^{\prime}\in N_{r^{\prime}}\) be a point such that \(d(q,q^{\prime})\leq(3+8\varepsilon)r\). We claim that we add \(q^{\prime}\) to \(B^{3}_{q,r}\). To do this, it is enough to show that \(q^{\prime}\in T_{r^{\prime}}\). As we mentioned, \(d(p,q)\leq r^{\prime}\) and \(d(q,q^{\prime})\leq(3+8\varepsilon)r\). Then by the triangle inequality we have \(d(p,q^{\prime})\leq d(p,q)+d(q,q^{\prime})\leq r^{\prime}+(3+8\varepsilon)r\), which means that \(q^{\prime}\in T_{r^{\prime}}\) and it is enough to explore \(T_{r^{\prime}}\) for computing \(B^{3}_{q,r}\). Note that the same argument also works for \(B^{1}_{q,r}\).

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_EMPTY:18]

\(d(q,v)=d(q,\text{rep}_{2r}(q))\leq 2r\). Hence, \(d(p,v)\leq 4r=2(2r)\) holds by the triangle inequality. If \(v\) is added to \(D_{2r}\) in Line 17, we have \(v=\text{rep}_{2r}(p)\), and we know that \(d(p,\text{rep}_{2r}(p))\leq 2r\). Therefore, \(d(p,v)=d(p,\text{rep}_{2r}(p))\leq 2r\leq 2(2r)\). Finally, we consider the case that \(v\) is added to \(D_{2r}\) in Line 22. In this case, \(v=\text{rep}_{2r}(x)\) for a point \(x\in N_{r}\), and \(x=\text{rep}_{r}(q)\) for a point \(q\in N_{r/2}\) such that either \(q\) was a child of \(p\) or \(q\in E_{r/2}\). If \(q\) was a child of \(p\), then \(d(p,q)\leq r\), and if \(q\in E_{r/2}\), then \(d(p,q)\leq r/2\leq r\) by Lemma A.8. Therefore, \(d(p,q)\leq r\) always holds. Besides, \(d(q,x)\leq r\), as \(x\) is a parent of \(q\). Moreover, we have \(d(x,\text{rep}_{2r}(x))\leq 2r\). Putting everything together we have

\[d(p,v)=d(p,\text{rep}_{2r}(x))\leq d(p,q)+d(q,x)+d(x,\text{rep}_{2r}(x))\leq r +r+2r=2(2r)\enspace.\]

Thus, our claim holds.

In addition, \(D_{r}\subseteq N_{r}\), which means that the distance between any two points in \(D_{r}\) is more than \(r\). Adding this to the claim, the definition of doubling dimension implies that \(D_{r}\) is of size \(2^{O(\dim)}\). 

Now, we can discuss the update time for maintaining our data structure after each deletion.

**Lemma A.12**.: _Let \(n\) be the size of the point set \(P_{t}\). Then after deletion of any point from \(P_{t}\), we can update our data structure in time \(2^{O(\dim)}\varepsilon^{-2\dim}\log\Delta\log n\)._

Proof.: As we discussed in Section A.2, to maintain the heaps, a \(2^{O(\dim)}\varepsilon^{-\dim}\log n\) factor will be multiplied by the running time of Algorithm 3. Hence, it is enough to show that the running time of Algorithm 3 is \(2^{O(\dim)}\varepsilon^{-\dim}\log\Delta\).

In Algorithm 3, we first call FindNeighbours for \(\gamma=4\) and \(\gamma=1+(3+8\varepsilon)/\varepsilon\) to find sets \(S_{r}\) and \(T_{r}\), which take time \(2^{O(\dim)}\log\Delta\) and \(2^{O(\dim)}\varepsilon^{-\dim}\log\Delta\), respectively, according to Lemma A.5.

We next iterate over \(O(\log\Delta)\) levels and in each level, we iterate over some children of \(p\) and the elevated points \(E_{r/2}\), and then we call UpdateNeighbourhoodRemove, which takes \(O(|B^{3}_{p,r}|+|B^{1}_{p,r}|)=O(2^{O(\dim)}\varepsilon^{-\dim})\). The number of \(p\)'s children and the elevated points are \(2^{O(\dim)}\) by Lemma A.2 and Lemma A.10, respectively. Thus, we iterate over \(2^{O(\dim)}\) points in each level, and in each iteration, all the operations other than UpdateNeighbourhoodElevate can be done in time \(2^{O(\dim)}\). Besides, the running time of UpdateNeighbourhoodElevate is \(O(|T_{r^{\prime}}|)=O(2^{O(\dim)}\varepsilon^{-\dim})\). Hence, the total running time of the iteration over the \(O(\log\Delta)\) levels is \(O(2^{O(\dim)}\varepsilon^{-\dim}\log\Delta)\). 

Proof of Lemma 2.3Now, adding Lemma A.7 and Lemma A.12 together, finishes the proof of Lemma 2.3, which is about the update time of our data structure.

**Lemma 2.3**.: _Let \(n\) be the size of point set \(P_{t}\). Then after insertion/deletion of any point to \(P_{t}\), we can update our data structure in time \(2^{O(\dim)}\varepsilon^{-2\dim}\log\Delta\log n\)._

## Appendix B Omitted Proofs

**Lemma 2.1**.: _Let \(p\in P_{t}\) and \(r\in R\). Then \(d(p,\text{rep}_{r}(p))\leq 2r\). In particular, \(d(p,\text{rep}_{er}(p))\leq 2\varepsilon r\), where \(\varepsilon=2^{\lfloor\log\varepsilon^{\prime}\rfloor}\) for the error parameter \(\varepsilon^{\prime}\)._

Proof.: Let \(r^{\prime}\in R\). As \(N_{2^{\prime}}\) is a \(2r^{\prime}\)-net of \(N_{r^{\prime}}\), we know that \(d(\text{rep}_{r^{\prime}}(p),\text{rep}_{2r^{\prime}}(\text{rep}_{r^{\prime}} (p)))\leq 2r^{\prime}\). Further, by definition, \(\text{rep}_{2r^{\prime}}(p)=\text{rep}_{2r^{\prime}}(\text{rep}_{r^{\prime}} (p))\) and hence, \(d(\text{rep}_{r^{\prime}}(p),\text{rep}_{2r^{\prime}}(p))\leq 2r^{\prime}\). Thus,

\[d(p,\text{rep}_{r}(p))\leq\sum_{\begin{subarray}{c}r^{\prime}<r,\\ r^{\prime}\in R\end{subarray}}d(\text{rep}_{r^{\prime}}(p),\text{rep}_{2r^{ \prime}}(p))\leq 2\sum_{\begin{subarray}{c}r^{\prime}<r,\\ r^{\prime}\in R\end{subarray}}r^{\prime}=2\beta\sum_{\ell<\log\frac{r}{2}}2^{ \ell}\leq 2\beta 2^{\log\frac{r}{2}}=2r\enspace. \tag{1}\]

For \(r^{\prime}=\varepsilon r\), we make a case distinction. If \(\varepsilon r\geq\beta\), then \(\varepsilon r\in R\) and hence \(d(p,\text{rep}_{er}(p))\leq 2\varepsilon r\) by Equation (1). If \(\varepsilon r<\beta\), then \(d(p,\text{rep}_{er}(p))=0\leq 2\varepsilon r\) because \(\text{rep}_{er}(p)=p\) by definition. 

**Lemma 2.2**.: _Let \(r\in R\) and \(p\in N_{er}\). The sets \(B^{1}_{p,r}\) and \(B^{3}_{p,r}\) defined above are of size at most \(\frac{2^{O(\dim)}}{\varepsilon^{\dim}}\). In particular, the size is \(O(\varepsilon^{-\dim})\) for a constant doubling dimension \(\dim\)._Proof.: By recursively applying the definition of doubling dimension \(i\) times, we can cover a ball of radius \((3+8\varepsilon)r\) with at most \(2^{i\dim}\) balls of radius \(\frac{(3+8\varepsilon)r}{2^{i}}\). For \(i=\lceil\log\frac{3+8\varepsilon}{\varepsilon/2}\rceil\) it follows that \(B^{3}_{p,r}\) can be covered by \((2\cdot\frac{3+8\varepsilon}{\varepsilon/2})^{\dim}\in\frac{2^{O(\dim)}}{ \varepsilon^{\dim}}\) balls of radius \(\frac{\varepsilon r}{2}\). As \(B^{3}_{p,r}\subseteq N_{\varepsilon r}\), the distance between any pair of points in \(B^{3}_{p,r}\) is at least \(\varepsilon r\). Therefore, each of the \(\frac{\varepsilon r}{2}\)-balls contains at most one point, which finishes the proof. 

**Lemma 3.1**.: _Let \(k\) be an integer, \(0\leq i\leq k\), and \(r\in R\). After the \(i\)-th iteration of the loop in Line 3, \(H_{r}[y]\) is the total weight of unmarked points in \(B^{1}_{y,r}\) for each \(y\in N_{\varepsilon r}\). More formally, \(H_{r}[y]=w_{\varepsilon r}(B^{1}_{y,r}\cap U_{i})\) for any \(y\in N_{\varepsilon r}\) after the \(i\)-th iteration._

Proof.: Let \(H^{i}_{r}\) denote the state of the heap and \(M^{i}\) the set Marked at the end of iteration \(i\), and \(H^{0}_{r}\) and \(M^{0}\) denote the respective sets before entering the for-loop. It can easily be seen that \(M^{i}=\cup_{j\leq i}B^{3}_{c_{j},r}\) for every \(i\leq k\) by a simple induction. Initially, \(M^{0}=\emptyset=\cup_{j\leq 0}B^{3}_{c_{j},r}\). Now assume \(M^{i-1}=\cup_{j\leq i-1}B^{3}_{c_{j},r}\) for some \(i<k\). Noting that \(M^{i-1}\) is only updated in line 7 of MaxCoverage\((k,r)\), Lines 6 and 7 imply that all points from \(B^{3}_{c_{i},r}\) that are not in \(M^{i-1}\) yet are added to \(M^{i-1}\). Therefore, \(M^{i}=M^{i-1}\cup B^{3}_{c_{i},r}=\cup_{j\leq i}B^{3}_{c_{j},r}\). Recall that \(U_{i}=N_{\varepsilon r}\setminus\cup_{j\leq i}B^{3}_{c_{j},r}\). This directly implies that \(U_{i}\) corresponds to the set of points that are still unmarked at the end of iteration \(i\), that is \(U_{i}=N_{\varepsilon r}\setminus M^{i}\). From this, it also immediately follows that \(U_{i}=U_{i-1}\setminus B^{3}_{c_{i},r}\).

Now we can show \(H^{i}_{r}[x]=w_{\varepsilon r}(B^{1}_{x,r}\cap U_{i})\) for all \(i\leq k\). For \(i=0\), the heap is not modified and no points are covered yet, i.e. \(H^{0}_{r}[x]=w_{\varepsilon r}(B^{1}_{x,r})\) and \(U_{0}=N_{\varepsilon r}\). Now assume \(H^{i-1}_{r}[x]=w_{\varepsilon r}(B^{1}_{x,r}\cap U_{i-1})\) for some \(i<k\). Lines 6 to 9 in MaxCoverage\((k,r)\) are equivalent to

\[\forall y\in B^{3}_{c_{i},r}\setminus M^{i-1}\forall x\in B^{1}_{y,r}\colon \text{ decrease }H_{r}[x]\text{ by }w_{\varepsilon r}(y)\enspace. \tag{2}\]

Neighborhood membership is a symmetric relation in the following sense. If \(x\in N_{\varepsilon r}\) is contained within the local neighborhood of \(y\in N_{\varepsilon r}\), then the converse is also true: For every \(x,y\in N_{\varepsilon r}\) such that \(x\in B_{y,r}\) it also holds \(y\in B_{x,r}\). Therefore, Equation (2) is equivalent to

\[\forall x\in N_{\varepsilon r}\forall y\in B^{3}_{c_{i},r}\cap B^{1}_{x,r} \setminus M^{i-1}\colon\text{ decrease }H_{r}[x]\text{ by }w_{\varepsilon r}(y)\enspace,\]

which means that during iteration \(i\), the heap value of \(x\in N_{\varepsilon r}\) is reduced by \(w_{\varepsilon r}(B^{3}_{c_{i},r}\cap B^{1}_{x,r}\setminus M^{i-1})\). Because of the equality \(B^{3}_{c_{i},r}\cap B^{1}_{x,r}\setminus M^{i-1}=B^{3}_{c_{i},r}\cap B^{1}_{x,r}\cap U_{i-1}\), we can deduce

\[H^{i}_{r}[x] =H^{i-1}_{r}[x]-w_{\varepsilon r}(B^{3}_{c_{i},r}\cap B^{1}_{x,r }\cap U_{i-1})=w_{\varepsilon r}(B^{1}_{x,r}\cap U_{i-1})-w_{\varepsilon r}(B^ {3}_{c_{i},r}\cap B^{1}_{x,r}\cap U_{i-1})\] \[\stackrel{{(3)}}{{=}}w_{\varepsilon r}(B^{1}_{x,r} \cap U_{i-1}\setminus(B^{3}_{c_{i},r}\cap B^{1}_{x,r}\cap U_{i-1}))=w_{ \varepsilon r}(B^{1}_{x,r}\cap U_{i-1}\setminus B^{3}_{c_{i},r})\] \[=w_{\varepsilon r}(B^{1}_{x,r}\cap U_{i})\enspace,\]

where (3) follows from the fact that \(w_{\varepsilon r}(X\setminus Y)=w_{\varepsilon r}(X)-w_{\varepsilon r}(Y)\) for \(Y\subseteq X\subseteq N_{\varepsilon r}\). 

**Lemma 3.6**.: _Let \(OPT>0\) be the optimal radius for the \(k\)-center clustering of \(P_{t}\) with \(z\) outliers, and let \(r^{*}\) be the minimum number in \(R\) such that \(r^{*}\geq OPT\). Then findCenters\((k,z)\) returns a \((3+10\varepsilon)\rho\)-approximate solution for \(k\)-center clustering problem with \(z\) outliers, where \(\rho=\frac{r^{*}}{OPT}\)._

Proof.: We first show that \(\hat{r}\leq r^{*}\). To do this, we consider two cases: \(\hat{r}>2\) or \(\hat{r}\leq 2\). For the first case, it holds that the \(\mathit{outliersWeight}\) returned by MaxCoverage\((k,\hat{r}/2)\) is more than \(z\), and then we can conduct that \(OPT>\hat{r}/2\). It is because otherwise, MaxCoverage\((k,\hat{r}/2)\). \(\mathit{outliersWeight}\) would be less than \(z\) by Lemma 3.5, which is a contradiction. It means that \(r^{*}>\hat{r}/2\), and then according to the definition of \(R\) and \(r^{*}\) we have \(r^{*}\geq\hat{r}\). Now we consider the second case that \(\hat{r}\leq 2\). Since \(OPT>0\), we have \(OPT\geq\text{d}_{\text{min}}=2\). Adding it to \(OPT\leq r^{*}\), which is an assumption of this lemma, we have \(\hat{r}\leq 2\leq OPT\leq r^{*}\). Thus \(\hat{r}\leq r^{*}\) holds in both cases.

Due to Lemma 3.2 and Lemma 3.5, MaxCoverage\((k,\hat{r})\) returned by findCenters\((k,z)\) is a set of \(k\) centers such that all but at most \(z\) points in \(P_{t}\) are within distance \((3+10\varepsilon)\hat{r}\) of these centers. Hence, findCenters\((k,z)\) returns a \(\frac{(3+10\varepsilon)\hat{r}}{OPT}\)-approximation solution. We also showed that \(\hat{r}\leq r^{*}\), therefore, the approximation ratio is at most \((3+10\varepsilon)\rho\), where \(\rho=\frac{r^{*}}{OPT}\).

**Lemma 3.7**.: _Let \(OPT>0\), and let \(\varepsilon>0\) be fixed. We define \(m:=\lceil 1/\log_{2}{(1+\varepsilon)}\rceil\). Suppose we have \(m\) parallel instances of our data structure with parameter \(\beta=2^{i/m-1}\) for the \(i\)-th instance. Then in at least one of the instances, we find a \((3+O(\varepsilon))\)-approximation solution for \(k\)-center clustering with \(z\) outliers by calling findCenters\((k,z)\)._

Proof.: Since we assume \(OPT>0\), then we have \(OPT\geq\mathrm{d_{min}}=2\). Let \(j^{*}\in[1,\lceil\log\Delta\rceil]\) be such that \(2^{j^{*}-1}<OPT\leq 2^{j^{*}}\), and let \(i^{*}\in[1,m]\) be such that \(2^{j^{*}-1+(i^{*}-1)/m}<OPT\leq 2^{j^{*}-1+i^{*}/m}\). For the instance with parameter \(\beta=2^{i^{*}/m-1}\), we have \(\beta\cdot 2^{j^{*}}=2^{j^{*}-1+i^{*}/m}\in R\). Therefore by Lemma 3.6, findCenters\((k,z)\) returns a \((3+10\varepsilon)\rho^{*}\)-approximation solution, where \(\rho^{*}=2^{j^{*}-1+i^{*}/m}/OPT\). To finish the proof of Lemma, we show that \(\rho^{*}\leq(1+\varepsilon)\), which immediately implies a \((3+O(\varepsilon))\)-approximation solution. According to the definition of \(i^{*}\) we have \(\rho^{*}=2^{j^{*}-1+i^{*}/m}/OPT\leq 2^{j^{*}-1+i^{*}/m}/2^{j^{*}-1+(i^{*}-1)/ m}=2^{1/m}\). Besides, \(m\geq 1/\log_{2}{(1+\varepsilon)}\) holds by the definition for of \(m\), which means \(1/m\leq\log_{2}{(1+\varepsilon)}\). Therefore, \(2^{1/m}\leq(1+\varepsilon)\), and then we have \(\rho^{*}\leq 2^{1/m}\leq(1+\varepsilon)\), which finishes the proof. 

**Lemma 3.8**.: _Let \(u\) be the number of times that MaxCoverage\((k,r)\) updates heap \(H\). Then copying heap \(H_{r}\) in Line 2, as well as all these \(u\) updates, can be handled in total time \(O(u\log n)\)._

Proof.: Procedure MaxCoverage\((k,r)\) sets \(H\) as a copy of \(H_{r}\), and then it decreases the value of some keys in \(H\) during the algorithm. Note that it never inserts or deletes a new key to \(H\), and the only possible update is decreasing the value of a key. We claim that it is not necessary to really copy the heap \(H_{r}\), which takes \(O(|H_{r}|)=O(n)\) time. Instead, we apply all the updates on \(H_{r}\) and then undo them at the end of MaxCoverage\((k,r)\). Although the procedure can change \(H_{r}\) as we defined, we guarantee that \(H_{r}\) is the same at the start and end of the procedure. To do this, we keep a record of all \(u\) updates, and at end of MaxCoverage\((k,r)\), we undo all these \(u\) updates, which means increasing the value of each updated key with the decreased value. Since the number of updates is \(u\), then we can do all the reverse updates in \(O(u\log|H_{r}|)=O(u\log n)\). 

**Lemma 3.9**.: _Let \(k\) and \(z\) be two given parameters. Then findCenters\((k,z)\) described in Algorithm 1 has a time complexity of \(2^{O(\dim)}\varepsilon^{-2\dim k}\log n\log\Delta\)._

Proof.: findCenters\((k,z)\) perform a binary-search on \(R\) and in each search, it invokes MaxCoverage two times. Therefore, MaxCoverage is called \(O(\log|R|)\) times in total, which is \(O(\log\log\Delta)\) as \(|R|=O(\log\Delta)\). Hence, it remains to show that the running time of MaxCoverage\((k,r)\) is \(2^{O(\dim)}\varepsilon^{-2\dim k}\log n\).

Now, we discuss the running time of MaxCoverage\((k,r)\). This procedure consists of \(k\) iterations, in which we iterate on all elements \(y\in B^{3}_{c_{i},r}\). Recall that Lemma 2.2 states \(B^{3}_{c_{i},r}\) is of size at most \(\frac{2^{O(\dim)}}{e^{\dim}}\). Then for any \(y\), we iterate on all elements \(x\in B^{1}_{y,r}\) and update \(H[x]\) in Line 9. Again by Lemma 2.2, the size of \(B^{1}_{y,r}\) is at most \(\frac{2^{O(\dim)}}{e^{\dim}}\). Therefore, we update heap \(H\) at most \(\left(\frac{2^{O(\dim)}}{e^{\dim}}\right)^{2}\cdot k\) times. Then by Lemma 3.8, these heap updates and also copying heap \(H_{r}\) in Line 2 are done in total time of \(2^{O(\dim)}\varepsilon^{-2\dim k}\log n\). 

Proof of Theorem 1.1.: _Let \((M,d)\) be a metric space of bounded doubling dimension \(dim\), and let \(\varepsilon>0\) be an error parameter. There exists a deterministic dynamic algorithm that allows the insertion or deletion of points from \(M\) using worst-case \(\varepsilon^{-O(\dim)}\log n\log\Delta\) update time. Moreover, at any time \(t\), it can be queried by parameters \(k\) and \(z\) to compute a \((3+\varepsilon)\)-approximate solution for the \(k\)-center problem with \(z\) outliers of \(P_{t}\) using worst-case \(\varepsilon^{-O(\dim)}k\log n\log\log\Delta\) query time, where \(P_{t}\) is the set of points that are inserted but not deleted up to time \(t\), and \(n\) is the size of \(P_{t}\)._

Proof.: We maintain \(m=\lceil 1/\log_{2}(1+\varepsilon)\rceil\) instances of our data structure with parameter \(\beta=2^{i/m-1}\) for the \(i\)-th instance. We first discuss the approximation ratio. If \(OPT>0\), then we find a \((3+O(\varepsilon))\)-approximation solution by Lemma 3.7. Otherwise if \(OPT=0\), then by Lemma 3.5, the _outliersWeight_ returned by MaxCoverage\((k,0)\) is at most \(z\). In this case, we return a solution with radius \(0\) in Line 2, which is an exact solution.

Now we discuss the update time. If we maintain \(m=O(1/\log(1+\varepsilon))\) instances of our data structure in parallel, it incurs an \(O(1/\log(1+\varepsilon))=O(\varepsilon^{-1})\) factor in the time complexity. In each instance, the update time per insert/delete is \(2^{O(\dim)}\varepsilon^{-2\dim}\log\Delta\log n\) by Lemma 2.3, and the \(k\) centers can be computed in time \(O(2^{O(\dim)}\varepsilon^{-2\dim}k\log n\log\log\Delta)\) by Lemma 3.9 Therefore, we handle each insert/delete in \(2^{O(\dim)}\varepsilon^{-(2\dim+1)}\log\Delta\log n=\varepsilon^{-O(\dim)} \log n\log\Delta\) total time and we can find the centers in \(O(2^{O(\dim)}\varepsilon^{-(2\dim)+1}k\log n\log\log\Delta)=\varepsilon^{-O( \dim)}k\log n\log\log\Delta\) total time.