# Learning in the Presence of Low-dimensional Structure:

A Spiked Random Matrix Perspective

 Jimmy Ba\({}^{1,2,3}\), Murat A. Erdogdu\({}^{1,2}\), Taiji Suzuki\({}^{4,5}\), Zhichao Wang\({}^{6}\), Denny Wu\({}^{7,8}\)

\({}^{1}\)University of Toronto, \({}^{2}\)Vector Institute, \({}^{3}\)xAI, \({}^{4}\)University of Tokyo, \({}^{5}\)RIKEN AIP,

\({}^{6}\)University of California San Diego, \({}^{7}\)New York University, \({}^{8}\)Flatiron Institute

{jba,erdogdu}@cs.toronto.edu, taiji@mist.i.u-tokyo.ac.jp,

zhw036@ucsd.edu, dennywu@nyu.edu

###### Abstract

We consider the problem of learning a single-index target function \(f_{*}:\mathbb{R}^{d}\rightarrow\mathbb{R}\) under the spiked covariance data:

\[f_{*}(\mathbf{x})=\sigma_{*}\Big{(}\tfrac{1}{\sqrt{1+\theta}}\langle\mathbf{x},\mathbf{ \mu}\rangle\Big{)},\ \ \mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_{d}+\theta\mathbf{\mu}\mathbf{\mu}^{ \top}),\ \ \theta\asymp d^{\beta}\text{ for }\beta\in[0,1),\]

where the link function \(\sigma_{*}:\mathbb{R}\rightarrow\mathbb{R}\) is a degree-\(p\) polynomial with information exponent \(k\) (defined as the lowest degree in the Hermite expansion of \(\sigma_{*}\)), and it depends on the projection of input \(\mathbf{x}\) onto the spike (signal) direction \(\mathbf{\mu}\in\mathbb{R}^{d}\). In the proportional asymptotic limit where the number of training examples \(n\) and the dimensionality \(d\) jointly diverge: \(n,d\rightarrow\infty,n/d\rightarrow\psi\in(0,\infty)\), we ask the following question: how large should the spike magnitude \(\theta\) be, in order for \((i)\) kernel methods, \((ii)\) neural networks optimized by gradient descent, to learn \(f_{*}\)? We show that for kernel ridge regression, \(\beta\geq 1-\frac{1}{p}\) is both sufficient and necessary. Whereas for two-layer neural networks trained with gradient descent, \(\beta>1-\frac{1}{k}\) suffices. Our results demonstrate that both kernel methods and neural networks benefit from low-dimensional structures in the data. Further, since \(k\leq p\) by definition, neural networks can adapt to such structures more effectively.

## 1 Introduction

Learning under spiked covariance.Real-world data is often high-dimensional but it also exhibits certain low-dimensional structures. Indeed, the number of input features is exceedingly large in machine learning, yet most relevant information is concentrated in a low-dimensional subspace [17, 18]. Such structures have important consequences on learning performance. In random matrix theory, high dimensionality is reflected by the _proportional asymptotic limit_ where the numbers of training examples and input features diverge to infinity at the same rate, whereas the low-dimensional structure is often described by a _spiked random matrix model_[2, 1, 20, 1] in which a low-rank signal ("spike") is hidden in a large-dimensional noise ("bulk"). In this work, we restrict ourselves to the simple setting where the signal is rank-1, and consider the following data-generating process:

\[\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_{d}+\theta\mathbf{\mu}\mathbf{\mu}^{\top}),\quad f_{*}( \mathbf{x})=\sigma_{*}\Big{(}\tfrac{1}{\sqrt{1+\theta}}\langle\mathbf{x},\mathbf{\mu} \rangle\Big{)}, \tag{1.1}\]

where the goal is to estimate the target function (teacher) \(f_{*}\), which is a _single-index model_ depending on the signal direction \(\mathbf{\mu}\in\mathbb{R}^{d}\), and \(\sigma_{*}:\mathbb{R}\rightarrow\mathbb{R}\) is an unknown _link function_. The signal \(\mathbf{\mu}\) is also embedded in the spiked input data, and \(\theta>0\) controls the spike magnitude; we scale \(\theta\asymp d^{\beta}\) for \(\beta\in[0,1)\), where larger \(\beta\) indicates a more prominent low-dimensional structure, and hence the learning problem becomes easier. We aim to characterize the efficiency of learning \(f_{*}\) using kernel methods and neural networks (NNs) in relation to the strength of the low-dimensional signal \(\theta\).

Prior results: isotropic data.When \(\theta=0\), the input features are isotropic and do not reveal any information about the target \(f_{*}\). In this setting, the performance of kernel methods and two-layer NNs have been extensively studied in the proportional asymptotic limit.
* **Kernel methods.** For isotropic Gaussian or spherical input data, the performance of random features (RF) regression and kernel ridge regression (KRR) has been precisely characterized in the proportional limit [14, 1, 1, 15]. However, in this regime, RF models and KRR suffers from the "curse of dimensionality" [1, 21] and cannot achieve vanishing generalization error unless \(f_{*}\) is _linear_[1, 1, 15]. More generally, for KRR to learn a degree-\(p\) polynomial target, a sample complexity of \(n=\Omega(d^{p})\) is required [1, 1, 2, 1].
* **Neural networks + Gradient descent.** While NNs can efficiently _approximate_ a single-index model (with polynomial link \(\sigma_{*}\)), the _optimization_ complexity of gradient-based learning on isotropic data is known to be governed by the _information exponent_, defined as the index of the first non-zero Hermite coefficient of \(\sigma_{*}\)[1]. For NNs trained with gradient descent (GD) to learn a target with information exponent \(k\), recent works showed that a sample complexity of \(n=\tilde{\Theta}(d^{k})\) suffices1[1, 1, 15, 1, 16]. Hence, in the proportional regime (\(n\asymp d\)), learnability has only been established for \(k=1\)[1, 1, 15]. Hence, in the high-dimensional regime, KRR can only learn linear functions on the input, and NNs learn targets with information exponent \(k=1\), which cannot cover many important problems such as phase retrieval [17, 18, 19]. Following the intuition that low-dimensional structure in the input data can make the learning problem easier, we ask the following question.

Footnote 1: The appearance of \(k\) in the exponent of the dimensionality is also consistent with the correlational statistical query (CSQ) lower bound [1, 1], which relates to the hardness of learning via gradient descent.

_If the input data contains low-dimensional structure given by the spiked covariance model (1.1), can kernel methods and neural networks learn a larger class of \(f_{*}\) in the proportional regime?_

### Our Contributions

We answer this question in the affirmative by showing that in the proportional asymptotic limit when the spike magnitude \(\theta\) reaches a certain threshold, both KRR and GD-trained two-layer NN can learn the target function \(f_{*}\), where the link function \(\sigma_{*}\) is a degree-\(p\) polynomial with information exponent \(k\) (see Definition 1). Our findings are summarized below and illustrated in Figure 1.

* For KRR with the inner-product kernel, we give a sharp analysis of the prediction risk and show that to learn a link function \(\sigma_{*}\) with degree \(p\) in the proportional regime, a spike magnitude of \(\theta=\Omega\big{(}d^{1-\frac{1}{p}}\big{)}\) is _both necessary and sufficient_.
* For a two-layer NN with ReLU activation, we upper bound the prediction risk when representation learning is performed via one gradient descent step on the first-layer parameters (analogous to [1, 1]) and show that a spike magnitude of \(\theta=\omega\big{(}d^{1-\frac{1}{p}}\big{)}\) is _sufficient_ to learn \(\sigma_{*}\) with information exponent \(k\).

Our analysis reveals that both KRR and NNs can benefit from the low-dimensional structure (spike) in the input data, and they can learn a wider class of target functions in the proportional regime compared to the isotropic setting (i.e., \(p>1\) for KRR, and \(k>1\) for NNs). Furthermore, since for a given link function \(k\leq p\) by definition, we know that NNs require a smaller spike magnitude \(\theta\) to learn the same target compared to KRR (see Figure 1). In other words, a GD-trained NN can utilize such a low-dimensional structure more effectively than KRR.

Figure 1: Spike magnitude \(\theta\asymp d^{\beta}\) for KRR and GD-trained NN to learn the target \(f_{*}\) with degree \(p\) and information exponent \(k\) in the proportional regime. When \(k\neq p\), KRR requires a stronger low-dimensional structure (i.e., larger spike) to learn \(f_{*}\).

Related works.Related to our problem setting, [14] studied the sample complexity of KRR and NNs in a spiked covariance model, and showed that both models indeed benefit from a stronger low-dimensional structure (i.e., larger spike). However, their analysis assumed the number of spikes to be diverging and cannot cover the proportional regime we consider. More importantly, the NN analysis is limited to approximation, and gradient-based optimization guarantee is not given.

In [13], the authors presented a non-rigorous analysis on classifying XOR mixtures using an RF model and a two-layer NN trained with online SGD and showed that the NN can learn the task under smaller spike magnitude. Such a task is parallel to the information exponent \(k=2\) setting (see [1]; our results rigorously demonstrate that similar separation exists for a different class of target functions with potentially higher degree and information exponent.

## 2 Preliminaries: Problem Setting and Assumptions

Notations.\(\|\cdot\|\) denotes the \(\ell_{2}\) norm for vectors and the \(\ell_{2}\to\ell_{2}\) operator norm for matrices, and \(\|\cdot\|_{F}\) is the Frobenius norm. \(\mathcal{O}_{d}(\cdot)\) and \(o_{d}(\cdot)\) stand for the standard big-O and little-o notations, where the subscript highlights the asymptotic variable; we write \(\tilde{\mathcal{O}}(\cdot)\) when (poly-)logarithmic factors are ignored. \(\mathcal{O}_{d,\mathbb{P}}(\cdot)\) (resp. \(o_{d,\mathbb{P}}(\cdot)\)) represents big-O (resp. little-o) in probability as \(d\to\infty\). \(\Omega(\cdot),\Theta(\cdot),\omega(\cdot)\) are defined analogously. Denote \(\gamma\) as the standard Gaussian distribution in \(\mathbb{R}\). Given \(f:\mathbb{R}^{d}\to\mathbb{R}\), we denote its \(L^{p}\)-norm with respect to the data distribution as \(\left\|f\right\|_{L^{p}}\).

### Basic Assumptions

We consider Gaussian input data with a spiked covariance:

\[\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_{d}+\theta\mathbf{\mu}\mathbf{\mu}^{\top}),\quad\theta \asymp d^{\beta}\text{ for some }\beta\in[0,1), \tag{2.1}\]

where the signal direction \(\mathbf{\mu}\in\mathbb{R}^{d}\) satisfies \(\left\|\mathbf{\mu}\right\|=1\), and the spike magnitude \(\theta\) is allowed to grow with the input dimensionality \(d\) as specified by the exponent \(\beta\): as \(\beta\) gets larger, the spike magnitude increases and hence the data exhibits stronger low-dimensional structure. Following the terminology in [14], we may define the _effective dimensionality_ of the input data as \(d_{\text{eff}}\asymp d^{1-\beta}\), which captures the intuition that a larger spike renders the features more low-dimensional. As we will see, the sample complexity of KRR and two-layer NN is decided by this effective dimensionality.

Teacher model.We consider a _student-teaching setting_, where the labels \(y\) are generated from a teacher model (target function) \(f_{*}:\mathbb{R}^{d}\to\mathbb{R}\). In the spiked covariance model (2.1), it is known that input directions with large variations are often good predictors of the labels -- indeed, this is the main reason that principal component analysis is used in practice [10]. We therefore consider the following single-index target function where the index features align with the spike direction \(\mathbf{\mu}\):

\[y=f_{*}(\mathbf{x})=\sigma_{*}\Big{(}\tfrac{1}{\sqrt{1+\theta}}\langle\mathbf{x},\mathbf{ \mu}\rangle\Big{)}, \tag{2.2}\]

where the link function \(\sigma_{*}\in L^{2}(\gamma)\) is centered such that \(\mathbb{E}[\sigma_{*}(z)]=0\) for \(z\sim\mathcal{N}(0,1)\) (this can be achieved by subtracting the mean from the training labels as in [12]), and the normalization factor \((1+\theta)^{-1/2}\) ensures that the \(L^{2}\) norm of \(f_{*}\) remains constant. We remark that given the prior knowledge of low-dimensional structure, we may efficiently learn \(f_{*}\) by first performing PCA on the input features; our goal, however, is not to construct an optimal learning algorithm, but to understand the behavior of KRR and two-layer NN without such data preprocessing, similar to [14].

We also assume \(\sigma_{*}\) is a degree-\(p\) polynomial with information exponent (IE) \(k\) defined below.

**Definition 1** (Information exponent).: _Let \(\{h_{j}\}_{j=0}^{\infty}\) denote the normalized Hermite polynomials. The information exponent of \(f\in L^{2}(\gamma)\), which we denote by \(k\), is the index of the first non-zero Hermite coefficient of \(f\), that is, given the Hermite expansion \(f=\sum_{j=0}^{\infty}\alpha_{j}h_{j}\), \(k:=\min\{j>0:\alpha_{j}\neq 0\}\)._

It is clear that by definition, we always have \(k\leq p\), and equality is only achieved when \(\sigma_{*}\) is a "pure" degree-\(k\) Hermite polynomial. Intuitively, IE measures the magnitude of information contained in the gradient, and larger \(k\) implies increased gradient descent complexity [1, 1]. Such definition is also related to the recently introduced _leap complexity_[1].

**Remark**.: _In (2.2) we do not corrupt the training labels \(y\) with i.i.d. label noise, meaning that, in light of the bias-variance decomposition, we analyze the bias term describing the extent \(f_{*}\) is learned by the student model. If label noise is introduced, the additional variance term (describing the "overfitting" due to noise) can be handled by standard concentration argument._

### Learning Objective and Training Procedure

Given training examples \(\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{n}\), where \(\mathbf{x}_{i}\overset{\mathrm{i.i.d.}}{\sim}\mathcal{N}(0,\mathbf{I}_{d}+\theta\mathbf{\mu} \mathbf{\mu}^{\top})\) and \(y_{i}=f_{*}(\mathbf{x}_{i})\), we consider two student models obtained via the following training procedure in the proportional regime:

_Proportional asymptotic limit:_\(n,d\rightarrow\infty,\quad n/d\rightarrow\psi\in(0,\infty)\). (2.3)

Student model I: Kernel ridge regression.We focus on the inner-product kernel: \(k(\mathbf{x},\mathbf{y})=g(\frac{(\mathbf{x},\mathbf{y})}{d})\) for some \(g\in L^{2}(\gamma)\) (similar argument can also apply to rotationally invariant kernels as in [10, 11]). Denoting the associated reproducing kernel Hilbert space (RKHS) with \(\mathcal{H}\), the kernel ridge regression estimator is given by

\[\hat{f}_{\text{ker}}=\operatorname*{argmin}_{f\in\mathcal{H}}\Big{\{}\frac{1} {n}\sum_{i=1}^{n}(y_{i}-f(\mathbf{x}_{i}))^{2}+\lambda\|f\|_{\mathcal{H}}^{2}\Big{\}} \;\Rightarrow\;\hat{f}_{\text{ker}}(\mathbf{x})=k(\mathbf{x},\mathbf{X})^{\top}(\mathbf{K}+ \lambda\mathbf{I})^{-1}\mathbf{y}, \tag{2.4}\]

where \(\lambda>0\) is the ridge parameter, and \(\mathbf{K}\in\mathbb{R}^{n\times n}\) is the kernel Gram matrix with entries \(\mathbf{K}_{i,j}=k(\mathbf{x}_{i},\mathbf{x}_{j})\). Spectral properties of the kernel matrix and the prediction risk of KRR have been extensively studied in various high-dimensional settings [10, 1, 2, 1]. Since the kernel function is fixed before seeing the data, we intuitively expect that KRR cannot effectively adapt to a low-dimensional structure in the learning problem.

Student model II: Two-layer neural network.We aim to learn the following width-\(N\) network:

\[f_{\text{NN}}(\mathbf{x})=\frac{1}{\sqrt{N}}\sum_{i=1}^{N}a_{i}\sigma(\langle\mathbf{x },\mathbf{w}_{i}\rangle+b_{i})=\Big{\langle}\mathbf{a},N^{-1/2}\sigma(\mathbf{W}^{\top} \mathbf{x}+\mathbf{b})\Big{\rangle}, \tag{2.5}\]

where \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\) is a nonlinear activation, and the trainable parameters \(\mathbf{W}\in\mathbb{R}^{d\times N},\mathbf{a},\mathbf{b}\in\mathbb{R}^{N}\) are initialized from standard Gaussian distribution. We require the network width to be \(N=\Omega(d^{\varepsilon})\) for some small \(\varepsilon>0\). Such a choice entails that it is sufficient to have \(\Theta(d^{1+\varepsilon})\) total training parameters, which is almost proportional to the sample size \(n\asymp d\); in contrast, for KRR we need to store the kernel Gram matrix with \(n^{2}\) entries, which is less computationally efficient at test time.

It is known that NNs can adapt to the learning problem via _representation learning_, in which the trained features encode relevant information of the target function. To realize this advantage, we update the parameters in (2.5) via the two-stage procedure introduced in [1, 2, 1], where we first learn the representation by taking one gradient descent step on the first-layer parameters \(\mathbf{W}\), and then estimate the second-layer parameters \(\mathbf{a}\) separately (which is a convex problem). This procedure is summarized as follows (see Algorithm 1 in Appendix for more details).

1. _Feature learning for the 1st layer._ We optimize the _representation_ of the NN via gradient descent, where the training objective is the empirical squared loss \(\mathcal{L}(f)=\frac{1}{n}\sum_{i=1}^{n}(f(\mathbf{x}_{i})-y_{i})^{2}\). Specifically, denote the \(i\)-th column of the initialized weight matrix \(\mathbf{W}_{0}\) as \(\mathbf{w}_{i}^{(0)}\in\mathbb{R}^{d}\), and the initialized NN as \(f_{\text{NN}}^{0}\), we take one GD step with learning rate \(\eta\) as follows, \[\mathbf{w}_{i}^{(1)}\leftarrow\mathbf{w}_{i}^{(0)}-\eta\sqrt{N}\cdot\nabla_{\mathbf{w}_{i} ^{(0)}}\mathcal{L}\big{(}f_{\text{NN}}^{(0)}\big{)}.\] Due to the anisotropic input data, the pre-activation \(\langle\mathbf{x},\mathbf{w}_{i}^{(1)}\rangle\) may blow up if the trained parameter \(\mathbf{w}_{i}^{(1)}\) aligns with the signal direction \(\mathbf{\mu}\). To circumvent this issue, we normalize each neuron to have (roughly) unit pre-activation, i.e., for \(i\in[N]\), we perform the weight normalization: At high level, this resembles the often-used normalization layers in deep learning [1, 1].
2. _Ridge regression for the 2nd layer._ After obtaining the updated first-layer parameters \(\mathbf{W}_{1}\in\mathbb{R}^{d\times N}\), we optimize the second-layer \(\mathbf{a}\) by solving the ridge regression objective - this can be done by using GD to solve an \(\ell_{2}\)-penalized least squares. To circumvent the dependence between the training data \(\mathbf{X}\) and the learned \(\mathbf{W}_{1}\), we follow [1] and estimate the regression coefficients \(\hat{\mathbf{a}}\) using _a new set of training data_\(\{\tilde{\mathbf{x}}_{i},\tilde{y}_{i}\}_{i=1}^{n}\) with the same sample size \(n\). Denoting the feature matrix on the fresh training set \(\{\tilde{\mathbf{X}},\tilde{\mathbf{y}}\}\) as \(\mathbf{\Phi}:=\frac{1}{\sqrt{N}}\sigma(\tilde{\mathbf{X}}\mathbf{W}_{1}+\mathbf{b})\in\mathbb{ R}^{n\times N}\), the ridge regression estimator can be obtained by: \(\hat{\mathbf{a}}=\operatorname*{argmin}_{\mathbf{a}}\Big{\{}\frac{1}{n}\|\tilde{\mathbf{y}} -\mathbf{\Phi}\mathbf{a}\|^{2}+\lambda\|\mathbf{a}\|^{2}\Big{\}}\).

Learning in the proportional limit.Given a target function \(f_{*}\) and a learned (student) model \(\hat{f}\), we evaluate the model performance using the prediction risk:

\[\mathcal{R}(\hat{f}):=\mathbb{E}_{\mathbf{x}}[(\hat{f}(\mathbf{x})-f_{*}(\mathbf{x}))^{2}]= \|\hat{f}-f_{*}\|_{L^{2}}^{2},\]

where the expectation is taken over the (anisotropic) Gaussian data distribution. To benchmark the model performance in the proportional limit (2.3), we introduce the following notion of learnability.

**Definition 2** (Learnability in the proportional regime).: _We say an algorithm learns the target function \(f_{*}\) in the proportional regime if, for any (small) constant \(\epsilon>0\), there exists some constant \(\psi_{*}>0\) such that when \(n/d\to\psi\geq\psi_{*}\), the algorithm constructs \(\hat{f}\) using \(n\) i.i.d. training examples, and achieves \(\mathcal{R}(\hat{f})=\|\hat{f}-f_{*}\|_{L^{2}}^{2}\leq\epsilon\) with probability \(1\) when \(n,d\to\infty\) proportionally._

Based on this definition, in order for a student model \(\hat{f}\) (e.g., KRR or two-layer NN) to learn the teacher \(f_{*}\) in the proportional regime, it needs to achieve arbitrarily small prediction risk with high probability, and with _linear sample complexity_ (i.e., using \(n\asymp d\) training samples).

## 3 Kernel Ridge Regression

For an inner-product kernel, the performance of KRR depends on the behavior of the pairwise inner-product of the training examples \(\langle\mathbf{x}_{i},\mathbf{x}_{j}\rangle\). Intuitively speaking, since the inner-products concentrate in high dimensions, the kernel matrix can be approximated in operator norm via low-degree Taylor expansion, and thus KRR can only learn \(f_{*}\) that is low-degree (e.g., see [1, 23]). Introducing a low-dimensional structure to the data is one way to counteract such near-orthogonality in high dimensions. Hence we expect that KRR can achieve better performance when the spike magnitude \(\theta\) becomes larger - this intuition will be made rigorous in the following subsections.

### Sharp Analysis of the Prediction Risk

In this section, we study the prediction risk of KRR defined in (2.4) with a positive semidefinite kernel given by \(k(\mathbf{x}_{i},\mathbf{x}_{j})=g(\langle\mathbf{x}_{i},\mathbf{x}_{j}\rangle/d)\), i.e., \(g^{(k)}(0)\geq 0\) for all \(k\) (see [24, Section 13.1]). Recall that the strength of the low-dimensional component is determined by the spike magnitude \(\theta\asymp d^{\beta}\); in the following, we characterize how the exponent \(\beta\in[0,1)\) affects the performance of KRR.

We denote by \(P_{\leq\ell}:L^{2}\to L^{2}\) the orthogonal projector onto the subspace of polynomials of degree at most \(\ell\) for any \(\ell\in\mathbb{N}\). Similarly, we denote by \(P_{>\ell}=\text{Id}-P_{\leq\ell}\) the projector to the orthogonal complement where Id is the identity operator. To illustrate this notion, consider \(\ell=1\), then we have

\[f_{*}(\mathbf{x})=P_{\leq 1}f_{*}(\mathbf{x})+P_{>1}f_{*}(\mathbf{x})=a^{*}+\langle\mathbf{b}^ {*},\mathbf{x}\rangle+P_{>1}f_{*}(\mathbf{x}),\]

where \(a^{*},\mathbf{b}^{*}=\operatorname*{argmin}_{a,b}\mathbb{E}_{\mathbf{x}}[(f_{*}(\mathbf{x })-a-\langle\mathbf{b},\mathbf{x}\rangle)^{2}]\). Note that any polynomial \(f_{*}\) with degree at most \(\ell\) satisfies \(P_{>\ell}f_{*}=0\). Denoting the KRR model as \(\hat{f}_{\ker}\), we have the following sharp analysis of the asymptotic prediction risk.

**Theorem 3**.: _Given any fixed \(\ell\in\mathbb{N}\). Suppose that \(g^{(k)}(0)>0\) for all \(k\leq\ell\), and the spike magnitude scales as \(\theta\asymp d^{\beta}\) for \(\beta\in\left(1-\frac{1}{\ell},1-\frac{1}{\ell+1}\right)\). Then as \(n,d\to\infty\), \(n/d\to\psi\in(0,\infty)\), the prediction risk of the KRR estimator \(\hat{f}_{\ker}\) with \(\lambda=\mathcal{O}_{d}(1)\) satisfies the following with probability 1,_

\[\mathcal{R}(\hat{f}_{\ker})-\|P_{>\ell}f_{*}\|_{L^{2}}^{2}=o_{d}(1).\]

As a corollary of Theorem 3, based on Definition 2, we can obtain a sharp threshold of \(\beta\) for learning a degree-\(p\) polynomial link function in the proportional regime (Definition 2).

**Corollary 4**.: _Assume \(\sigma_{*}\) is a degree-\(p\) polynomial with \(p\geq 1\) defined in (2.2). Under the same assumptions of Theorem 3, if \(\beta>1-\frac{1}{p}\), then the KRR estimator with \(\lambda=\mathcal{O}_{d}(1)\) learns \(f_{*}\) in the proportional regime, i.e., \(\mathcal{R}(\hat{f}_{\ker})=o_{d}(1)\) with probability 1 when \(n,d\to\infty\) proportionally._

We make the following remarks on Theorem 3 and Corollary 4.

* At a high level, the above theorem aligns with the conclusion of [1] which assumes spherical data with a diverging number of spikes. Recall that the _effective dimensionality_ of our anisotropic Gaussian data is \(d_{\text{eff}}\asymp d^{1-\beta}\), and Theorem 3 implies that to extract the signal direction \(\mathbf{\mu}\) and learn a degree-\(p\) teacher model, a sample size of \(n=\omega(d_{\text{eff}}^{p})\) is required; note that for \(\beta>0\), this implies an improvement over the isotropic setting where the sample complexity is \(n=\omega(d^{p})\)* While we only state the result for inner-product KRR (similar to [10, 11]), we expect similar findings for general rotationally invariant kernels: \(k(\mathbf{x}_{i},\mathbf{x}_{j})=g(\langle\mathbf{x}_{i},\mathbf{x}_{j}\rangle,\|\mathbf{x}_{i}\|,\| \mathbf{x}_{j}\|)\), since the norm term has negligible contribution to the learning of a single-index target (2.2). In fact, [12] established a lower bound for rotationally invariant KRR, which, when applied to our setting, gives a necessary sample complexity of \(n\asymp d_{\text{eff}}^{\Omega(p)}\). Although this lower bound may not be sharp, it illustrates that the required spike magnitude needs to scale with target degree \(p\).

### Intuition behind the Analysis

We briefly summarize the two main ingredients for the proof of Theorem 3. As a preliminary step, we can rotate the signal \(\mathbf{\mu}\) to the \(\mathbf{e}_{1}\) direction due to the rotational invariance of Gaussian distribution. After such transformation, we have \(([\mathbf{x}_{i}]_{1},[\mathbf{x}_{i}]_{2:d})\stackrel{{\text{i.i.d.}}}{ {\sim}}\mathcal{N}(0,1+\theta)\oplus\mathcal{N}(0,\mathbf{I}_{d-1})\) with \(\theta=\Theta(d^{\beta})\).

Polynomial approximation of the kernel matrix.Firstly, we approximate the inner-product KRR with some degree-\(L\) polynomial kernel in the form of

\[k(\mathbf{x}_{i},\mathbf{x}_{j})=\sum_{k=1}^{L}c_{k}d^{-k}\langle\mathbf{x}_{i},\mathbf{x}_{j} \rangle^{k}.\]

Such approximation can be used to establish a learning lower bound for KRR. For instance in the proportional limit, [1] showed that for well-conditioned data, the inner-product kernel can be approximated by the linear kernel, which implies that KRR only learns linear \(\sigma_{*}\)[1]. For general deterministic datasets, similar approximation has been studied in [13] where the "orthogonality" of input data determines the polynomial approximation error. In our setting, the inner product \(\frac{1}{d}\mathbf{x}_{i}^{\top}\mathbf{x}_{j}=\mathcal{O}_{d}(d^{\beta-1})\) depends on the spike magnitude \(\theta\asymp d^{\beta}\). Consequently, we approximate our inner-product kernel by some polynomial kernel whose degree depends on \(\beta\).

Orthogonal polynomial expansion.While the approximation in the preceding step simplifies the kernel, such analysis is generally not sharp. To refine the result, we expend the polynomial kernel in the Hermite bases (analogous to the analysis of spherical data in [1, 1]). After rotation, the teacher model (2.2) can be written as \(f_{*}(\mathbf{x})=\sigma_{*}(z)\), for \(z\sim\mathcal{N}(0,1)\) and \([\mathbf{x}]_{1}=\sqrt{(1+\theta)}z\) independent with other entries \([\mathbf{x}]_{i}\) for \(2\leq i\leq d\). In light of the kernel trick [14, Section 3.7], we write \(k(\mathbf{x}_{i},\mathbf{x}_{j})=\langle\Phi(\mathbf{x}_{i}),\Phi(\mathbf{x}_{j})\rangle\) where the feature vector \(\Phi(\mathbf{x})\) is composed of Hermite polynomials of \(\mathbf{x}\) up to some degree. Importantly, we only need to extract the Hermite components with respect to the first entry \([\mathbf{x}]_{1}\) to learn a function \(\sigma_{*}(z)\).

## 4 Two-layer Neural Network

During the feature learning phase of NN training, we intuitively expect the parameters to align with the (rank-1) signal direction which allows NN to overcome the "curse of dimensionality". Indeed, in this section, we show that the first gradient step on the first-layer weights \(\mathbf{W}\) enables the model to "zoom in" to the low-dimensional structure, given that the spike magnitude \(\theta\) is sufficiently large.

### Upper Bound on the Prediction Risk

We consider the following standard Gaussian initialization for the two-layer NN (2.5):

\[[\mathbf{W}_{0}]_{ij}\stackrel{{\text{i.i.d.}}}{{\sim}}\mathcal{N}(0, 1/d),\quad[\mathbf{a}_{0}]_{i}\stackrel{{\text{i.i.d.}}}{{\sim}} \mathcal{N}(0,1/N),\quad[\mathbf{b}_{0}]_{i}\stackrel{{\text{i.i.d.}}}{ {\sim}}\mathcal{N}(0,1). \tag{4.1}\]

Denote the NN optimized by the two-stage procedure outlined in Section 2.2 as \(\hat{f}_{\rm NN}\). In the sequel, we restrict ourselves to \(\sigma(z)=\operatorname{ReLU}(z)=\max\{z,0\}\). We expect similar characterization to hold when the student activation \(\sigma\) has non-zero Hermite coefficients up to degree-\(p\) (see [1]).

The following theorem gives a sufficient condition on the spike magnitude \(\theta\asymp d^{\beta}\) in order for \(\hat{f}_{\rm NN}\) to learn the target function \(f_{*}\) with linear sample complexity (in terms of dimension dependence).

**Theorem 5**.: _Consider the NN training procedure in Section 2.2 with Gaussian initialization defined in (4.1), \(N=\Omega(d^{\varepsilon})\), \(\eta=\Omega(N^{1/2+\varepsilon})\), and appropriately chosen \(\ell_{2}\) regularization \(d^{\varepsilon-1}\lesssim\lambda\lesssim d^{-\varepsilon}\) for small \(\varepsilon>0\). Then, for \(\sigma=\text{ReLU}\), we have_

\[\mathcal{R}(\hat{f}_{\rm NN})=o_{d}(1)\text{ with probability 1, when }\beta>1-1/k,\]

_as \(n,d\to\infty,n/d\to\psi\). That is, the two-layer ReLU NN can learn the target function with the information exponent \(k\) in the proportional regime._The above theorem predicts that a spike magnitude of \(\theta=\omega(d^{1-\frac{1}{k}})\) is sufficient for a GD-trained two-layer NN to learn \(f_{*}\) in the \(n\asymp d\) regime. Similar to prior works [1, 1], the information exponent \(k\) also determines the complexity of the learning problem in our setting: larger \(k\) implies a more "difficult" task for GD, hence we need a larger \(\theta\) (i.e., stronger low-dimensional signal) to achieve linear sample complexity. Moreover, in light of the _effective dimensionality_ defined in [16], Theorem 5 implies a sample size of \(n=\omega(d^{k}_{\text{eff}})\) suffices to learn a single-index target with information exponent \(k\); this contrasts the \(n=\omega(d^{\text{fp}}_{\text{eff}})\) complexity of KRR given in Theorem 3. Such discrepancy will be highlighted and empirically validated in Section 5.

### Intuition behind the Analysis

Theorem 5 is established in two steps: first, we prove that under certain conditions, gradient descent can align the first-layer parameters \(\mathbf{W}\) with the signal direction \(\mathbf{\mu}\); then we show that after achieving such an alignment, \(f_{*}\) can be efficiently learned by optimizing the second-layer \(\mathbf{a}\).

Spiked covariance amplifies gradient signal.Due to our "mean-field" initialization (i.e., small second-layer coefficients), the initial NN has small output \(f^{(0)}_{\text{NN}}(\mathbf{x})\approx 0\). Therefore the gradient of the squared loss is dominated by the correlation between the student and teacher model. Concretely, consider the population gradient for one parameter vector \(\mathbf{w}\in\mathbb{R}^{d}\) at initialization (for simplicity we omit the bias unit here):

\[\mathbb{E}_{\mathbf{x}}\Big{[}\nabla_{\mathbf{w}}f^{(0)}_{\text{NN}}(\mathbf{ x})\Big{]}\approx-\mathbb{E}_{\mathbf{x}}[\mathbf{x}\sigma^{\prime}(\langle\mathbf{x}, \mathbf{w}\rangle)f_{*}(\mathbf{x})]\] \[\stackrel{{(i)}}{{=}}-\mathbf{\Sigma}\mathbb{E}_{\mathbf{x}} \Big{[}f^{\prime}_{*}(\mathbf{x})\sigma^{\prime}(\langle\mathbf{x},\mathbf{w}\rangle) \cdot(1+\theta)^{-1/2}\mathbf{\mu}+f_{*}(\mathbf{x})\sigma^{\prime\prime}(\langle\mathbf{x },\mathbf{w}\rangle)\cdot\mathbf{w}\Big{]}\] \[\stackrel{{(ii)}}{{=}}-\sqrt{1+\theta}\mathbf{\mu}\cdot \mathbb{E}_{\mathbf{x}}[\sigma^{\prime}_{*}(\langle\mathbf{x},\mathbf{\mu}\rangle)\sigma^{ \prime}(\langle\mathbf{x},\mathbf{w}\rangle)]\,+\,\text{residual terms},\]

where \((i)\) is due to multivariate Stein's lemma, and \((ii)\) follows from the definition of \(\mathbf{\Sigma}\). Utilizing the Hermite expansions of the student and teacher nonlinearities,

\[\sigma(z)=\sum_{i=0}^{\infty}\alpha_{i}h_{i}(z),\quad\sigma_{*}(z)=\sum_{i=0} ^{\infty}\alpha_{i}^{*}h_{i}(z),\]

we have the following decomposition on the strength of the correlation term,

\[\mathbb{E}_{\mathbf{x}}[\sigma^{\prime}_{*}(\langle\mathbf{x},\mathbf{\mu}\rangle)\sigma^ {\prime}(\langle\mathbf{x},\mathbf{w}\rangle)]\approx\sum_{i=0}^{\infty}(i+1)^{2} \alpha_{i+1}\alpha_{i+1}^{*}\cdot\Big{\langle}\sqrt{1+\theta}\mathbf{\mu},\mathbf{w} \Big{\rangle}^{i}.\]

This calculation illustrates that the (expected) first gradient step indeed contains the direction of the signal \(\mathbf{\mu}\). More importantly, the magnitude of the signal term is affected by two factors: \((i)\) it is amplified by a larger spike \(\theta\), and \((ii)\) it vanishes when the information exponent \(k\) is large, since \(\langle\mathbf{\mu},\mathbf{w}\rangle=\widehat{\Theta}_{\mathbb{P}}(d^{-1/2})\) at initialization. The gradient magnitude relates to the sample complexity because in order to establish learnability in the proportional regime, we need to achieve nontrivial gradient concentration using \(n\asymp d\) samples. Consequently, the sufficient condition in Theorem 5 requires a larger \(\theta\) when the information exponent \(k\) is large, and vice versa.

**Remark**.: _One caveat in the above derivation is that the Hermite coefficients of the student nonlinearity \(\{\alpha_{i}\}_{i=1}^{\infty}\) need to be non-zero up to degree-\(k\), which is not satisfied by ReLU or most commonly-used activation functions. We solve this issue by taking into account the bias units \(b_{i}\) at initialization which "diversity" the nonlinearity, similar to [1] (see Lemma 15 for details)._

Nonparametric learning with random biases.After the signal direction is identified via gradient descent, we need to learn the unknown link function \(\sigma_{*}\). We utilize random biases as a resource for univariate approximation - similar argument has appeared in many prior works [1, 1, 2, 1, 2, 3]. Recall that in the representation learning phase, the bias units \(b_{i}\) are not optimized. Therefore, if the weight vectors \(\mathbf{w}\) align with the signal direction \(\mathbf{w}\approx\mathbf{\mu}\), then learning the second-layer coefficients \(\mathbf{a}\) via ridge regression can be reduced to a univariate kernel regression problem; specifically, given \(\mathbf{x}_{i},\mathbf{x}_{j}\in\mathbb{R}^{d}\), the univariate kernel is given by \(k(z_{i},z_{j})=\mathbb{E}_{a,b}[\sigma(a\cdot z_{i}+b)\sigma(a\cdot z_{j}+b)]\), where \(z_{i}=\langle\mathbf{x}_{i},\mathbf{\mu}\rangle\), \(z_{j}=\langle\mathbf{x}_{j},\mathbf{\mu}\rangle\). For \(\sigma=\mathrm{ReLU}\), it is known that such kernel can efficiently learn a polynomial link function \(\sigma_{*}\) (e.g., see [1, 1]).

Experiments: Comparing KRR and NN

In this section, we empirically validate the theoretical results presented in previous sections. We construct target functions \(f_{*}\) with a link of varying degree \(p\) and information exponent \(k\) and experimentally compute the prediction risk of KRR and that of a two-layer NN.

\(k=p\): NN & KRR are comparable.We first consider the setting where \(k=p\), which indicates that the link function is a pure degree-\(p\) Hermite polynomial: \(\sigma_{*}(z)=h_{p}(z)\). Recall that \(\theta\asymp d^{\beta}\); Theorem 3 implies that KRR learns the target function in the proportional regime when the spike magnitude satisfies \(\beta>1-\frac{1}{p}\), which matches the sufficient condition for NN given in Theorem 5.

In the experiments of Figure 2, we compute the prediction risk of KRR and that of two-layer NN optimized via the two-stage procedure outlined in Section 2, where the link function \(\sigma_{*}\) is a pure degree-2 and degree-3 Hermite polynomial. We observe that both KRR and NN can learn \(f_{*}\) with linear sample complexity when the spike magnitude \(\theta\) exceeds the same threshold predicted by our theory; intuitively speaking, this means that for the \(k=p\) setting, NN and KRR utilize the low-dimensional structure with the same efficiency. Also, note that while Theorem 5 only provides an upper bound on the risk, the experiments demonstrate that the predicted scaling of \(\theta\) is sharp.

\(k<p\): NN outperforms KRR.Next, we consider a setting where the link function \(\sigma_{*}\) is a mixture of low- and high-degree components: \(\sigma_{*}(z)=\sum_{j\in\mathcal{J}}\alpha_{j}h_{j}(z)\), with \(\min\mathcal{J}=k,\max\mathcal{J}=p\). In this case, Theorem 3 predicts that KRR requires a spike magnitude of \(\beta>1-\frac{1}{p}\) to learn \(f_{*}\) with linear sample complexity, whereas Theorem 5 entails that a smaller spike magnitude \(\beta>1-\frac{1}{k}\) is sufficient for the two-layer NN.

Figure 2: Prediction risk of KRR (a)&(c) and GD-trained two-layer NN (b)&(d) to learn \(f_{*}\), where the link function \(\sigma_{*}(z)=h_{2}(z)\) in (a)&(b), and \(\sigma_{*}(z)=h_{3}(z)\) in (c)&(d). We set \(\psi=5\). For KRR we use the Gaussian RBF kernel and \(\lambda=10^{-2}\). For two-layer NN, we set \(\eta=\sqrt{N}\). The experiments are averaged over 50 runs. Darker color corresponds to a smaller prediction risk, and the red lines are predicted by the sufficient conditions of learnability given by Theorem 3 and 5.

Figure 3 shows that the predictions from Theorem 3 and 5 once again accurately align with the experimental results. Observe that when \(k<p\), GD-trained NN indeed outperforms KRR, in the sense that it requires a less prominent low-dimensional structure (i.e., smaller spike magnitude \(\theta\)) to attain linear sample complexity. This illustrates the benefit of NN under structured data due to the presence of representation (feature) learning. Roughly speaking, for target function with \(k<p\), the sample complexity of representation learning (i.e., finding the signal direction \(\mathbf{\mu}\) via gradient-based learning) is low compared to directly estimating \(f_{*}\) on the original input features; therefore GD-trained NNs achieve better performance than KRR.

## 6 Conclusion and Future Directions

We investigated the performances of KRR and GD-trained NNs in the proportional asymptotic limit. We analyzed the prediction risk of these models in learning a nonlinear single-index model (2.2), where the index feature in the teacher model only relies on the spike direction \(\mathbf{\mu}\) hidden in the anisotropic Gaussian input data (2.1). The strength \(\theta\) (signal-to-noise ratio) of this spike determines the level of low dimensionality, which affects the learnability of both NNs and KRR. Our results clearly demonstrate that both NNs and KRR benefit from this additional structure in the covariance; yet, NNs adapt to the low-dimensional patterns more efficiently.

It is worth noting that our theoretical analysis focuses on the "well-specified" scenario where the input spike perfectly aligns with the index features of \(f_{*}\). In a companion work [16], we investigate a more general setting where the spiked covariance data only provides partial information of \(f_{*}\) (i.e. the "misaligned" scenario), and show that feature learning (via gradient flow) also benefits from such structured data. Finally, extending our results to multi-index teacher models [14, 15] and considering more general training dynamics (e.g., beyond the first gradient step) applied to multiple-layer NNs are interesting directions left for future studies.

Figure 3: Prediction risk of KRR (a)&(c) and GD-trained two-layer NN (b)&(d) to learn \(f_{*}\), where the link function \(\sigma_{*}(z)=h_{1}(z)+h_{3}(z)\) in (a)&(b), and \(\sigma_{*}(z)=h_{2}(z)+h_{3}(z)\) in (c)&(d). We use the same hyperparameters as in Figure 2. The red lines are predicted by the sufficient conditions in Theorem 3 and 5.

## Acknowledgement

The authors thank Enric Boix-Adsera, Joan Bruna, Bruno Loureiro, and Alireza Mousavi-Hosseini for the discussions and feedback on the manuscript. MAE was partially supported by NSERC Grant [2019-06167], CIFAR AI Chairs program, CIFAR AI Catalyst grant. TS was partially supported by JSPS KAKENHI (20H00576) and JST CREST (JPMJCR2015). ZW was partially supported by NSF DMS-2055340 and NSF DMS-2154099.

## References

* [AAM22] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In _Conference on Learning Theory_, pages 4782-4887. PMLR, 2022.
* [ABA22] Emmanuel Abbe and Enric Boix-Adsera. On the non-universality of deep learning: quantifying the cost of symmetry. _Advances in Neural Information Processing Systems_, 35:17188-17201, 2022.
* [ABAM23] Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. _arXiv preprint arXiv:2302.11055_, 2023.
* [AGJ21] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient descent on non-convex losses from high-dimensional inference. _Journal of Machine Learning Research_, 22(106):1-51, 2021.
* [Bac17] Francis Bach. Breaking the curse of dimensionality with convex neural networks. _The Journal of Machine Learning Research_, 18(1):629-681, 2017.
* [Bac23] Francis Bach. _Learning Theory from First Principles_. MIT Press, 2023.
* [BAP05] Jinho Baik, Gerard Ben Arous, and Sandrine Peche. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. _The Annals of Probability_, 33(5):1643-1697, 2005.
* [BBSS22] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow neural networks. In _Advances in Neural Information Processing Systems_, 2022.
* [BEG\({}^{+}\)22] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. _Advances in Neural Information Processing Systems_, 35:21750-21764, 2022.
* [BES\({}^{+}\)22] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-dimensional asymptotics of feature learning: How one gradient step improves the representation. In _Advances in Neural Information Processing Systems 35_, 2022.
* [BGN11] Florent Benaych-Georges and Raj Rao Nadakuditi. The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices. _Advances in Mathematics_, 227(1):494-521, 2011.
* [BKH16] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* [BMR21] Peter L Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint. _Acta numerica_, 30:87-201, 2021.
* [BMZ23] Raphael Berthier, Andrea Montanari, and Kangjie Zhou. Learning time-scales in two-layers neural networks. _arXiv preprint arXiv:2303.00055_, 2023.
* [BS06] Jinho Baik and Jack W Silverstein. Eigenvalues of large sample covariance matrices of spiked population models. _Journal of multivariate analysis_, 97(6):1382-1408, 2006.

* [CC15] Yuxin Chen and Emmanuel Candes. Solving random quadratic systems of equations is nearly as easy as solving linear systems. _Advances in Neural Information Processing Systems_, 28, 2015.
* [Chi11] Theodore S Chihara. _An introduction to orthogonal polynomials_. Courier Corporation, 2011.
* [CM20] Sitan Chen and Raghu Meka. Learning polynomials in few relevant dimensions. In _Conference on Learning Theory_, pages 1161-1227. PMLR, 2020.
* [CS13] Xiuyuan Cheng and Amit Singer. The spectrum of random inner-product kernel matrices. _Random Matrices: Theory and Applications_, 2(04):1350010, 2013.
* [DL20] Oussama Dhifallah and Yue M Lu. A precise performance analysis of learning with random features. _arXiv preprint arXiv:2008.11904_, 2020.
* [DLS22] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In _Conference on Learning Theory_, pages 5413-5452. PMLR, 2022.
* [DWY21] Konstantin Donhauser, Mingqi Wu, and Fanny Yang. How rotational invariance of common kernels prevents generalization in high dimensions. In _International Conference on Machine Learning_, pages 2804-2814. PMLR, 2021.
* [EK10] Noureddine El Karoui. The spectrum of kernel random matrices. _The Annals of Statistics_, 38(1):1-50, 2010.
* [Fie82] James R Fienup. Phase retrieval algorithms: a comparison. _Applied optics_, 21(15):2758-2769, 1982.
* [GLK\({}^{+}\)20] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Generalisation error in learning with random features and the hidden manifold model. In _International Conference on Machine Learning_, pages 3452-3462. PMLR, 2020.
* [GMMM20] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural networks outperform kernel methods? _Advances in Neural Information Processing Systems_, 33:14820-14830, 2020.
* [GMMM21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers neural networks in high dimension. _The Annals of Statistics_, 49(2):1029-1054, 2021.
* 22, 2021.
* [HL22] Hong Hu and Yue M Lu. Universality laws for high-dimensional learning with random features. _IEEE Transactions on Information Theory_, 2022.
* [HITFF09] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. _The elements of statistical learning: data mining, inference, and prediction_, volume 2. Springer, 2009.
* [IS15] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International conference on machine learning_, pages 448-456. pmlr, 2015.
* [Joh01] Iain M Johnstone. On the distribution of the largest eigenvalue in principal components analysis. _The Annals of statistics_, 29(2):295-327, 2001.
* [LRZ20] Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm interpolants and restricted lower isometry of kernels. In _Conference on Learning Theory_, pages 2683-2711. PMLR, 2020.
* [LV07] John A Lee and Michel Verleysen. _Nonlinear dimensionality reduction_, volume 1. Springer, 2007.
* [LY22] Yue M Lu and Horng-Tzer Yau. An equivalence principle for the spectrum of random inner-product kernel matrices. _arXiv preprint arXiv:2205.06308_, 2022.

* [MHPG\({}^{+}\)23] Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Murat A Erdogdu. Neural networks efficiently learn low-dimensional representations with sgd. _International Conference on Learning Representations_, 2023.
* [MHWSE23] Alireza Mousavi-Hosseini, Denny Wu, Taiji Suzuki, and Murat A. Erdogdu. Gradient-based feature learning under structured data. In _Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)_, 2023.
* [Min17] Stanislav Minsker. On some extensions of Bernstein's inequality for self-adjoint operators. _Statistics & Probability Letters_, 127:111-119, 2017.
* [MM22] Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. _Communications on Pure and Applied Mathematics_, 75(4):667-766, 2022.
* [MMM21] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Generalization error of random feature and kernel methods: hypercontractivity and kernel matrix concentration. _Applied and Computational Harmonic Analysis_, 2021.
* [Rah17] Sharif Rahman. Wiener-hermite polynomial expansion for multivariate gaussian probability measures. _Journal of Mathematical Analysis and Applications_, 454(1):303-334, 2017.
* [RGKZ21] Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborova. Classifying high-dimensional gaussian mixtures: Where kernel methods fail and neural networks succeed. In _International Conference on Machine Learning_, pages 8936-8947. PMLR, 2021.
* [Sam23] Holger Sambale. Some notes on concentration for \(\alpha\)-subexponential random variables. In _High Dimensional Probability IX: The Ethereal Volume_, pages 167-192. Springer, 2023.
* [SS02] Bernhard Scholkopf and Alexander J Smola. _Learning with kernels: support vector machines, regularization, optimization, and beyond_. MIT press, 2002.
* [Sze39] Gabor Szeg. _Orthogonal polynomials_, volume 23. American Mathematical Soc., 1939.
* [TV23] Yan Shuo Tan and Roman Vershynin. Online stochastic gradient descent with arbitrary initialization solves non-smooth, non-convex phase retrieval. _Journal of Machine Learning Research_, 24(58):1-47, 2023.
* [Ver10] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. _arXiv preprint arXiv:1011.3027_, 2010.
* [Ver18] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* [WZ21] Zhichao Wang and Yizhe Zhu. Deformed semicircle law and concentration of nonlinear random matrices for ultra-wide neural networks. _arXiv preprint arXiv:2109.09304_, 2021.
* [WZ23] Zhichao Wang and Yizhe Zhu. Overparameterized random feature regression with nearly orthogonal data. In _International Conference on Artificial Intelligence and Statistics_, pages 8463-8493. PMLR, 2023.
* [XHM\({}^{+}\)22] Lechao Xiao, Hong Hu, Theodor Misiakiewicz, Yue Lu, and Jeffrey Pennington. Precise learning curves and higher-order scalings for dot-product kernel regression. _Advances in Neural Information Processing Systems_, 35:4558-4570, 2022.
* [YS19] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* [Zhi21] Nikita Zhivotovskiy. Dimension-free bounds for sums of independent matrices and simple tensors via the variational principle. _arXiv preprint arXiv:2108.08198_, 2021.

## Table of Contents

* 1 Introduction
	* 1.1 Our Contributions
* 2 Preliminaries: Problem Setting and Assumptions
	* 2.1 Basic Assumptions
	* 2.2 Learning Objective and Training Procedure
* 3 Kernel Ridge Regression
	* 3.1 Sharp Analysis of the Prediction Risk
	* 3.2 Intuition behind the Analysis
* 4 Two-layer Neural Network
	* 4.1 Upper Bound on the Prediction Risk
	* 4.2 Intuition behind the Analysis
* 5 Experiments: Comparing KRR and NN
* 6 Conclusion and Future Directions
* A Analysis of Kernel Ridge Regression
* A.1 Polynomial Approximation of Kernel Matrix
* A.2 Hermite Expansion of Polynomial Kernel
* A.3 Analysis of Prediction Risk
* B Analysis of Neural Network
* B.1 Hermite Expansion of Population Gradient
* B.2 Finite-sample Concentration
* B.3 Learning \(f_{*}\) with Neural Network Features
Analysis of Kernel Ridge Regression

In this section, we analyze the performance of KRR defined in Section 2. Recall that

\[\mathbf{x}_{i}\sim\mathcal{N}(0,\mathbf{I}_{d}+\theta\mathbf{\mu}\mathbf{\mu}^{\top}),\quad\left\| \mathbf{\mu}\right\|_{2}=1,\] (A.1)

where \(\theta\asymp d^{\beta},\) and \(\beta\in[0,1)\). Let \(\mathbf{\Sigma}=\mathbf{I}+\theta\mathbf{\mu}\mathbf{\mu}^{\top}\) and \(\mathbf{x}_{i}=\mathbf{\Sigma}^{1/2}\mathbf{z}_{i},\) where \(\mathbf{z}_{i}\sim\mathcal{N}(0,\mathbf{I})\), and \(\mathbf{\Sigma}^{1/2}=\mathbf{I}+(\sqrt{1+\theta}-1)\mathbf{\mu}\mathbf{\mu}^{\top}\). Then, the training label \(y_{i}\) can be equivalently expressed as

\[y_{i}=f_{*}(\mathbf{x}_{i})=\sigma_{*}(\langle\mathbf{z}_{i},\mathbf{\mu}\rangle),\]

for \(i\in[n]\). Recall \(\mathbf{X}^{\top}=[\mathbf{x},\ldots\mathbf{x}_{n}]\in\mathbb{R}^{d\times n}\). We define the nonlinear kernel matrix by \(\mathbf{K}=g(\frac{1}{d}\mathbf{X}\mathbf{X}^{\top})\in\mathbb{R}^{n\times n}\), where smooth function \(g:\mathbb{R}\rightarrow\mathbb{R}\) is applied to each entry of the matrix. Additionally, we write

\[\mathbf{K}(\mathbf{x},\mathbf{X})=\big{[}g\big{(}\mathbf{x}^{\top}\mathbf{x}_{1}/d\big{)},\ldots,g \big{(}\mathbf{x}^{\top}\mathbf{x}_{n}/d\big{)}\big{]}.\]

We analyze the kernel ridge regression estimator:

\[\hat{f}_{\text{ker}}(\mathbf{x})=\mathbf{K}(\mathbf{x},\mathbf{X})(\mathbf{K}+\lambda\mathbf{I})^{-1} \mathbf{y}.\]

Specifically, we aim to compute the prediction risk in the proportional regime:

\[\mathcal{R}(\hat{f}_{\text{ker}})=\mathbb{E}_{\mathbf{x}}(\hat{f}_{\text{ker}}(\bm {x})-f_{*}(\mathbf{x}))^{2},\]

where \(\mathbf{x}\in\mathbb{R}^{d}\) is an independent copy of \(\mathbf{x}_{i}\) sampled from (A.1). Since the kernel \(\mathbf{K}\) is rotationally invariant, i.e.,

\[k(\mathbf{S}\mathbf{x}_{i},\mathbf{S}\mathbf{x}_{j})=k(\mathbf{x}_{i},\mathbf{x}_{j}),\]

for any orthogonal matrix \(\mathbf{S}\in\mathbb{R}^{d\times d}\) and \(i,j\in[n]\), we may apply an orthogonal matrix \(\mathbf{O}\in\mathbb{R}^{d\times d}\) for each data sample such that

\[\mathbf{O}\mathbf{x}_{i}\sim\mathcal{N}(0,\mathbf{I}_{d}+\theta\mathbf{e}_{1}\mathbf{e}_{1}^{\top }),\qquad\mathbf{O}\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_{d}+\theta\mathbf{e}_{1}\mathbf{e}_{1}^ {\top}),\]

where \(\mathbf{e}_{1}\) is the standard coordinate vector. It is clear that in distribution, \(\hat{f}_{\text{ker}}(\mathbf{x})\) and \(f_{*}(\mathbf{x})\) remain unchanged before and after the transformation. Therefore, without loss of generality, we will consider the case that \(\mathbf{\mu}=\mathbf{e}_{1}\) for the proofs in this section, that is, we write

\[\mathbf{\Sigma}=\mathbf{I}_{d}+\theta\mathbf{e}_{1}\mathbf{e}_{1}^{\top}\] (A.2)

and training label \(y_{i}=f_{*}(\mathbf{x}_{i})\) only depends on the first entry of the data \(\mathbf{x}_{i}\) for \(i\in[n]\).

### Polynomial Approximation of Kernel Matrix

Denote the concatenation of \(\mathbf{x}\) and \(\mathbf{X}\) as \(\tilde{\mathbf{X}}^{\top}:=[\mathbf{x},\mathbf{X}^{\top}]\in\mathbb{R}^{d\times(n+1)}\), and the enlarged nonlinear kernel matrix by \(\tilde{\mathbf{K}}=g(\frac{1}{d}\tilde{\mathbf{X}}\tilde{\mathbf{X}}^{\top})\in\mathbb{R}^ {(n+1)\times(n+1)}\). First we present a preliminary approximation of \(\tilde{\mathbf{K}}\) by a proper polynomial kernel.

**Lemma 6**.: _Suppose that \(\beta\leq 1-\frac{1}{p}\) for some \(p\in\mathbb{N}\). Under the assumptions of Theorem 3, as \(n,d\rightarrow\infty\) proportionally, we have_

\[\left\|\tilde{\mathbf{H}}_{p}+\sigma_{p}\mathbf{I}_{n+1}-\tilde{\mathbf{K}}\right\|=o_{d, \mathbb{P}}(1),\]

_where_

\[\tilde{\mathbf{H}}_{p}:=P_{p}\big{(}\tilde{\mathbf{X}}\tilde{\mathbf{X}}^{\top}/d\big{)},\]

_and \(P_{p}(x)=\sum_{k=1}^{2p}g^{(k)}(0)x^{k}\), \(\sigma_{p}:=g(1)-P_{p}(1)\)._

**Proof.** For simplicity, we denote \(\mathbf{x}=\mathbf{x}_{0}\) and \(\mathbf{x}_{i}^{\top}=[\mathbf{x}_{i}^{1},\mathbf{\bar{x}}_{i}^{\top}]\), where \(\mathbf{x}_{i}^{1}\) is the first entry of \(\mathbf{x}_{i}\), and \(\bar{\mathbf{x}}_{i}\) the remaining entries for \(0\leq i\leq n\). Notice that \(\frac{1}{d}\mathbf{x}_{i}^{\top}\mathbf{x}_{j}=\frac{1}{d}\mathbf{z}_{i}\mathbf{\Sigma}\mathbf{z}_ {j}\), for any \(0\leq i,j\leq n\). By Bernstein inequality [25, Corollary 2.8.3], we have

\[\mathbb{P}\bigg{(}\bigg{|}\frac{1}{d}\mathbf{x}_{i}^{\top}\mathbf{x}_{j} \bigg{|}>t\bigg{)} \leq\mathbb{P}\bigg{(}\bigg{|}\frac{1}{d}\bar{\mathbf{x}}_{i}^{\top} \bar{\mathbf{x}}_{j}\bigg{|}>t/2\bigg{)}+\mathbb{P}\bigg{(}\bigg{|}\frac{1}{d}\mathbf{x }_{i}^{1}\mathbf{x}_{j}^{1}\bigg{|}>t/2\bigg{)}\] \[\leq\exp\left(-cd\min\{t^{2},t\}\right)+\exp\left(-ctd^{1-\beta }\right),\]for any \(i\neq j\) and \(t>0\). Therefore, \(\big{|}\frac{1}{d}\mathbf{x}_{i}^{\top}\mathbf{x}_{j}\big{|}=o_{d,\mathbb{P}}(d^{\beta-1} \log d)\) for \(\beta\in[1/2,1)\) and \(\big{|}\frac{1}{d}\mathbf{x}_{i}^{\top}\mathbf{x}_{j}\big{|}=o_{d,\mathbb{P}}(d^{-1/2} \log d)\) for \(\beta\in[0,1/2]\). Then we have

\[\|\text{offdig}(\tilde{\mathbf{H}}_{p}-\tilde{\mathbf{K}})\|^{2} \leq \|\text{offdig}(\tilde{\mathbf{H}}_{p}-\tilde{\mathbf{K}})\|_{F}^{2}\] \[\leq \sum_{i\neq j}\sum_{k=2p+1}^{\infty}\biggl{|}\frac{1}{d}\mathbf{x}_{i }^{\top}\mathbf{x}_{j}\Big{|}^{k}=\tilde{o}_{d,\mathbb{P}}(d^{-\frac{1}{p}}).\]

Here, \(\text{offdig}(\cdot)\) denotes setting the diagonal entries of the matrix to be zero. In addition, following the same proof as Theorem 1 of [1], we can obtain a similar approximation for the diagonal part \(\text{dig}(\tilde{\mathbf{H}}_{p}-\tilde{\mathbf{K}})\), the details of which we omit. 

### Hermite Expansion of Polynomial Kernel

Equipped with the above polynomial expansion of the kernel matrix, we now introduce a further approximation using orthogonal polynomials. Recall that

\[\mathcal{R}(\hat{f}_{\text{ker}})=\|f_{*}\|_{L^{2}}^{2}-2\mathbf{y}^{ \top}(\mathbf{K}+\lambda\mathbf{I})^{-1}\mathbf{E}+\mathbf{y}^{\top}(\mathbf{K}+\lambda\mathbf{I})^{-1 }\mathbf{M}(\mathbf{K}+\lambda\mathbf{I})^{-1}\mathbf{y},\]

where

\[\mathbf{E} := \mathbb{E}_{\mathbf{x}}[f_{*}(\mathbf{x})k(\mathbf{x},\mathbf{X})]^{\top}\in \mathbb{R}^{n}\] \[\mathbf{M} := \mathbb{E}_{\mathbf{x}}[k(\mathbf{X},\mathbf{x})k(\mathbf{x},\mathbf{X})]\in\mathbb{ R}^{n\times n}.\]

From Lemma 6 we know that for some \(p\in\mathbb{N}\),

\[\mathbf{K} = \mathbf{H}_{p}+\mathbf{\Delta}_{K},\] \[\mathbf{k}(\mathbf{x},\mathbf{X}) = \mathbf{H}_{p}(\mathbf{x},\mathbf{X})+\sigma_{p}\mathbf{I}_{n+1}+\mathbf{\Delta}_{K^{ \prime}},\]

for \(\|\mathbf{\Delta}_{K}\|=o_{d,\mathbb{P}}(1)\) and \(\big{\|}\mathbf{\Delta}_{K}^{\prime}\big{\|}=o_{d,\mathbb{P}}(1)\), where \(\sigma_{p}\) is defined in Lemma 6, \(\mathbf{H}_{p}:=P_{p}(\frac{1}{d}\mathbf{X}\mathbf{X}^{\top})\), and \(\mathbf{H}_{p}(\mathbf{x},\mathbf{X}):=P_{p}(\frac{1}{d}\mathbf{X}\mathbf{x})^{\top}\) is a row vector. Then following from Lemmas A.3, A.8 and A.13 in [1], we can conclude that \(\mathcal{R}(\hat{f}_{\text{ker}})=\mathcal{R}_{p}+o_{d,\mathbb{P}}(1)\), where

\[\mathcal{R}_{p}:=\|f_{*}\|_{L^{2}}^{2}-2\mathbf{y}^{\top}(\mathbf{H}_{p}+ \lambda\mathbf{I})^{-1}\mathbf{E}_{p}+\mathbf{y}^{\top}(\mathbf{K}_{p}+\lambda\mathbf{I})^{-1}\bm {M}_{p}(\mathbf{K}_{p}+\lambda\mathbf{I})^{-1}\mathbf{y},\] (A.3)

and

\[\mathbf{E}_{p} := \mathbb{E}_{\mathbf{x}}[f_{*}(\mathbf{x})\mathbf{H}_{p}(\mathbf{x},\mathbf{X})]^{\top }\in\mathbb{R}^{n}\] \[\mathbf{M}_{p} := \mathbb{E}_{\mathbf{x}}[\mathbf{H}_{p}(\mathbf{X},\mathbf{x})\mathbf{H}_{p}(\mathbf{x}, \mathbf{X})]\in\mathbb{R}^{n\times n}.\]

Now we analyze the asymptotic behavior of \(\mathcal{R}_{p}\) by utilizing the Hermitian polynomials to reorganize kernel \(\mathbf{H}_{p}\). Since \(\mathbf{\Sigma}\) now is a diagonal matrix in (A.2), from [1], we know the multivariate Hermite polynomials with respect to \(\mathcal{N}(0,\frac{1}{d}\mathbf{\Sigma})\) in \(\mathbb{R}^{d}\) are strongly orthogonal, each of which is the product of univariate Hermite polynomials on each coordinate. More precisely, for any \(k\in\mathbb{N}\) and multi-index \(\mathbf{I}:=(i_{1},i_{2},\ldots,i_{d})\in[k]^{d}\) with \(i_{1}+i_{2}+\cdots+i_{d}=k\), we define the \(k\)-th multivariate Hermite polynomial with respect to \(\mathbf{I}\) by \(h_{k,\mathbf{I}}(\mathbf{x}):=\prod_{j=1}^{d}h_{i_{j}}(x_{j})\) where \(\mathbf{x}=(x_{1},\ldots,x_{d})\) and \(h_{i_{j}}(\cdot)\) is the \(i_{j}\)-the univariate Hermite polynomials (note that the variance for the first coordinate and the remaining coordinates are different). Hence, any \(L\)-degree polynomial of \(\mathbf{x}\in\mathbb{R}^{d}\) has the form

\[f(\mathbf{x})=\sum_{k=0}^{L}\sum_{\begin{subarray}{c}|\mathbf{I}|=k\\ \mathbf{I}=\{i_{1},i_{2},\ldots,i_{d}\}\end{subarray}}c_{k,\mathbf{I}}h_{k,\mathbf{I}}( \mathbf{x}),\] (A.4)

for some coefficients \(c_{k,\mathbf{I}}\in\mathbb{R}\). Similar expansions using Gegenbauer polynomials for uniform unit sphere distribution have been used in [1, 2, 3]. The decomposition we consider is specifically related to the Hermite expansions of \(\sigma_{*}\):

\[f_{*}(\mathbf{x})=\sum_{i=k}^{p}\alpha_{j}h_{j}(\mathbf{\mu}^{\top}\mathbf{x})=\sum_{i=k}^ {p}\alpha_{j}h_{j}(z^{1}),\] (A.5)where \(h_{j}(\cdot)\) is the \(j\)-th Hermite polynomial with \(\mathbf{z}\sim\mathcal{N}(0,\mathbf{I})\). Notice that \(\mathbb{E}[h_{k,\mathbf{I}}(\mathbf{x})h_{l,\mathbf{J}}(\mathbf{x})]=\delta_{k,l}\delta_{\mathbf{I},\mathbf{J}}\) for \(\mathbf{x}\) sampled from the spiked model (2.1).

By the kernel trick (e.g., see Section 3.7 of [21]), a \(k\)-th monomial kernel satisfies

\[\langle\mathbf{x}_{i},\mathbf{x}_{j}\rangle^{k}=\langle\mathbf{x}_{i}^{\otimes k},\mathbf{x}_{ j}^{\otimes k}\rangle,\]

where each entry of \(\mathbf{x}_{i}^{\otimes k}\in\mathbb{R}^{d^{k}}\) is a \(k\)-th degree monomial of \(\mathbf{x}_{i}\). Hence we can rewrite the above inner product based on Hermite expansions of each entry of \(\mathbf{x}_{i}^{\otimes k}\) and \(\mathbf{x}_{j}^{\otimes k}\). Therefore, for kernel \(\mathbf{H}_{p}(\mathbf{x}_{i},\mathbf{x}_{j})\), there exists a Hermite polynomial feature map \(\mathbf{\Phi}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{L}\) where

\[L:=\sum_{k=0}^{2p}\binom{d+k-1}{k}\qquad B(d,k):=\binom{d+k-1}{k}\]

such that

\[\mathbf{H}_{p}(\mathbf{x}_{i}^{\top}\mathbf{x}_{j}/d)=\mathbf{\Phi}(\mathbf{x}_{i}/\sqrt{d})\mathbf{D} ^{2}\mathbf{\Phi}(\mathbf{x}_{i}/\sqrt{d})^{\top},\]

where each row vector \(\mathbf{\Phi}(\cdot)\) is the concatenation of orthogonal polynomials \(h_{k,\mathbf{I}}(\cdot)\) for \(0\leq k\leq L\) and \(\mathbf{D}\) is the diagonal matrix where the diagonal entries are the Hermite coefficients corresponding to \(\mathbf{H}_{p}\) which will be specified in the sequel. We define

\[\mathbf{\Phi}:=\begin{pmatrix}\mathbf{\Phi}(\mathbf{x}_{1}/\sqrt{d})\\ \mathbf{\Phi}(\mathbf{x}_{2}/\sqrt{d})\\ \cdots\\ \mathbf{\Phi}(\mathbf{x}_{n}/\sqrt{d})\end{pmatrix}\in\mathbb{R}^{n\times L},\] (A.6)

which gives \(\mathbf{H}_{p}=\mathbf{\Phi}\mathbf{\Phi}^{\top}\). Furthermore, since the variance of the first entry of \(\mathbf{x}_{i}\) is different from the other entries and the label \(y_{i}\) only depends on the first entry, we will separate the Hermite polynomials depending on \(\mathbf{x}_{i}^{1}\) and the terms depending on the remaining entries \(\bar{\mathbf{x}}_{i}\in\mathbb{R}^{d-1}\). That is, we further decompose each \(\mathbf{\Phi}(\mathbf{x}_{i}/\sqrt{d})=[\mathbf{\Phi}_{0}(\mathbf{x}_{i}/\sqrt{d}),\mathbf{\Phi}( \bar{\mathbf{x}}_{i}/\sqrt{d})]\), where \(\mathbf{\Phi}_{0}(\mathbf{x}_{i}/\sqrt{d})\in\mathbb{R}^{L_{0}}\) and

\[L_{0}:=\sum_{k=0}^{2p}\binom{d+k-2}{k-1}\qquad B_{0}(d,k):=\binom{d+k-2}{k-1},\]

where \(B_{0}(d,k)\) is the total number of multivariate Hermite polynomials of \(\mathbf{x}_{i}\) with degree \(k\), whose components with respect to \(\mathbf{x}_{i}^{1}\) is _not_ vanishing. Based on this decomposition, we also write

\[\bar{\mathbf{\Phi}}:=\begin{pmatrix}\mathbf{\Phi}(\bar{\mathbf{x}}_{1}/\sqrt{d})\\ \mathbf{\Phi}(\bar{\mathbf{x}}_{2}/\sqrt{d})\\ \cdots\\ \mathbf{\Phi}(\bar{\mathbf{x}}_{n}/\sqrt{d})\end{pmatrix}.\] (A.7)

**Lemma 7**.: _Let \(d_{\rm eff}:=d^{1-\beta}\). Under the assumptions of Theorem 3, we know that_

\[\mathbf{H}_{p}(\mathbf{x}_{i}^{\top}\mathbf{x}_{j}/d)=\mathbf{\Phi}(\mathbf{x}_{i}/\sqrt{d})\mathbf{D} ^{2}\mathbf{\Phi}(\mathbf{x}_{i}/\sqrt{d})^{\top},\]

_where \(\mathbf{D}^{2}\) is a diagonal matrix with entries_

\[\zeta(d,k,m)=\frac{g^{(k)}(0)}{d^{k-m}d_{\rm eff}^{m}}+o_{d}(d^{-k+m-1}d_{\rm eff }^{-m}),\] (A.8)

_for \(m\in[k]\) and \(k\in[2p]\). Furthermore, based on the definition of \(\mathbf{\Phi}_{0}\) as above, we have_

\[\mathbf{H}_{p}=\mathbf{\Phi}_{0}\mathbf{D}_{0}^{2}\mathbf{\Phi}_{0}^{\top}+\bar{\mathbf{H}}_{p},\]

_where each entry of \(\bar{\mathbf{H}}_{p}(\bar{\mathbf{x}}_{i},\bar{\mathbf{x}}_{j}):=P_{p}(\frac{1}{d}\bar{\bm {x}}_{i}^{\top}\bar{\mathbf{x}}_{j})\), and \(\mathbf{D}_{0}^{2}\) is a diagonal matrix defined analogously as \(\mathbf{D}^{2}\) but with \(m>0\) in (A.8). Note that \(\bar{\mathbf{H}}_{p}\) is an inner-product polynomial kernel matrix of \(\bar{\mathbf{X}}:=[\bar{\mathbf{x}}_{1},\ldots,\bar{\mathbf{x}}_{n}]\in\mathbb{R}^{(d-1) \times n}\)._

Proof.: This lemma follows from properties of multivariate Hermite polynomials [16]. Recall

\[\text{Var}(\mathbf{x}_{i}^{1})=1+\theta=\Theta(d^{\beta}).\]

By re-normalizing the variables, we obtain

\[P_{p}(\mathbf{x}_{i}^{\top}\mathbf{x}_{j}/d)=\sum_{k=0}^{2p}\sum_{m=0}^{k}\sum_{|\mathbf{I }|=k,i=m}\zeta(d,k,m)h_{k,\mathbf{I}}(\mathbf{x}_{i}),h_{k,\mathbf{I}}(\mathbf{x}_{j}),\]

where \(\mathbf{I}=(i_{1},\ldots,i_{d})\). As for \(\mathbf{\Phi}_{0}\), the diagonals of \(\mathbf{D}_{0}\) correspond to \(m\neq 0\) above.

### Analysis of Prediction Risk

The next lemma shows that \(\bar{\mathbf{H}}_{p}\) is approximated by a linear kernel which does not aid the learning of \(f_{*}\); this entails that we may focus on the term involving \(\mathbf{\Phi}_{0}\). Notice that \(\bar{\mathbf{H}}_{p}\) is a rotationally invariant kernel on isotropic Gaussian data \(\bar{\mathbf{X}}\) whose limiting global law has been analyzed in [13].

**Lemma 8**.: _Under the assumptions of Theorem 3, as \(n,d\to\infty\) proportionally, we have_

\[\left\|\bar{\mathbf{H}}_{p}+\sigma_{p}\mathbf{I}_{n}-\mathbf{K}_{\text{lin}}\right\|=o_{d, \mathbb{P}}(1),\]

_where_

\[\mathbf{K}_{\text{lin}}:=\frac{c_{1}}{d}\bar{\mathbf{X}}^{\top}\bar{\mathbf{X}}+c_{2}\mathbf{I} _{n}+\frac{c_{3}}{d}\mathbf{1}\mathbf{1}^{\top}\]

_and \(\left\|\mathbf{K}_{\text{lin}}\right\|=\mathcal{O}_{d,\mathbb{P}}(1)\). Here, \(c_{1},c_{2}\) and \(c_{3}\) are some positive constants that only depend on \(g^{(k)}(0)\) for \(k=0,1\) and \(2\)._

The above lemma is a consequence of Theorem 1 of [1], and hence we omit the detailed proof; for the formulae of constants \(c_{1},c_{2}\) and \(c_{3}\), we refer to Equation (57) of [1]. We remark that in the following concentration analysis, the exact limiting spectral distribution of \(\mathbf{K}_{\text{lin}}\) does not appear, since we do not consider the exact setting of \(\beta=1-\frac{1}{\ell}\) for \(\ell\in\mathbb{N}\).

**Lemma 9**.: _Under the assumptions of \(f_{*}\) in Theorem 3, we have that_

\[\mathbf{y}=\mathbf{\Phi}_{0}\mathbf{P}\mathbf{\alpha},\]

_for some vector \(\mathbf{\alpha}=[0,\ldots,0,\alpha_{k},\ldots,\alpha_{p},0,\ldots,0]^{\top}\in \mathbb{R}^{2p}\), where \(\mathbf{P}\in\mathbb{R}^{L_{0}\times 2p}\) is a projector onto the Hermite components with \(\mathbf{I}=(i_{1},0,\ldots,0)\)._

Here \(\mathbf{\alpha}\) represents the Hermite coefficients of the function \(f_{*}\). Lemma 9 directly follows from (A.5) since \(\mathbf{y}\) only depends on \(\mathbf{x}_{i}^{1}\) for \(i\in[d]\). Notice that here \(\bar{\mathbf{P}}\) selects \(h_{k}(\mathbf{x}_{i}^{1})\) for \(i\in[d]\) and \(k\in[2p]\) from all multivariate Hermite polynomials.

Next, we prove the following concentration statements, parallel to existing results in [1, 20, 21] for spherical data. For any \(\ell\in[2p]\), we decompose \(\mathbf{\Phi}_{0}=[\mathbf{\Phi}_{0,\leq\ell},\mathbf{\Phi}_{0,>\ell}]\) where \(\mathbf{\Phi}_{0,\leq\ell}\) are composed of all Hermite polynomials \(h_{k,\mathbf{I}}\) with \(k\leq\ell\), and \(\mathbf{\Phi}_{0,>\ell}\) collects all Hermite polynomials \(h_{k,\mathbf{I}}\) with \(k>\ell\). Similarly, we define \(\mathbf{D}_{0,\leq\ell}\) and \(\mathbf{D}_{0,>\ell}\) corresponding to \(\mathbf{\Phi}_{0,\leq\ell}\in\mathbb{R}^{n\times m}\) and \(\mathbf{\Phi}_{0,>\ell}\in\mathbb{R}^{n\times(L_{0}-m)}\), respectively. Here we denote by \(m\) the number of all Hermite polynomials \(h_{k,\mathbf{I}}\) with \(k\leq\ell\) where \(m\gg n\).

**Lemma 10**.: _Under the assumptions of Theorem 3, we have_

\[\mathbf{\Phi}_{0,\leq\ell}^{\top}\mathbf{\Phi}_{0,\leq\ell}=\mathbf{I}_{m}+o_{d,\mathbb{ P}}(1).\]

This lemma shows the concentration of the Gram matrix since \(\mathbb{E}[\mathbf{\Phi}_{0,\leq\ell}^{\top}\mathbf{\Phi}_{0,\leq\ell}]=\mathbf{I}_{m}\). The proof of this lemma can be derived from the concentration result [14]; see also Lemma 11 in [1]. The following two lemmas are the crucial ingredients of the proof of Theorem 3, which have been established in Lemmas 13 and 14 in [1] for the spherical setting.

**Lemma 11**.: _Under the assumptions of Theorem 3 with \(1-\frac{1}{\ell}<\beta<1-\frac{1}{\ell+1}\) for some \(\ell\in\mathbb{N}\), as \(n,d\to\infty\) proportionally, we have_

\[\left\|n(\mathbf{H}_{p}+\lambda\mathbf{I}_{n})^{-1}\mathbf{M}_{p}(\mathbf{H}_{p}+\lambda\mathbf{I} _{n})^{-1}-\frac{1}{n}\mathbf{\Phi}_{0,\leq\ell}\mathbf{\Phi}_{0,\leq\ell}^{\top} \right\|=o_{d,\mathbb{P}}(1).\]

**Proof.** Recall the definition of \(\mathbf{\Phi}\) in (A.6). From Lemma 7, we can obtain that \(\mathbf{M}_{p}=\mathbf{\Phi}\mathbf{D}^{4}\mathbf{\Phi}^{\top}\) due to the orthogonal polynomial decomposition in (A.4). Hence,

\[\mathbf{M}=\mathbf{\Phi}_{0,\leq\ell}\mathbf{D}^{4}_{0,\leq\ell}\mathbf{\Phi}_{0,\leq\ell}^{ \top}+\mathbf{\Phi}_{>\ell}\mathbf{D}^{4}_{>\ell}\mathbf{\Phi}_{>\ell}^{\top},\]

where \(\mathbf{\Phi}_{>\ell}\) is defined similarly as \(\mathbf{\Phi}_{0,>\ell}\) which is formed by all orthogonal components of \(\mathbf{\Phi}_{0,\leq\ell}\). In general, \(\mathbf{\Phi}_{>\ell}\) is composed by \(\mathbf{\Phi}_{0,>\ell}\) and \(\bar{\mathbf{\Phi}}\) defined in (A.7). Therefore,

\[n(\mathbf{H}_{p}+\lambda\mathbf{I}_{n})^{-1}\mathbf{M}(\mathbf{H}_{p}+\lambda\mathbf{ I}_{n})^{-1}\] \[= n(\mathbf{H}_{p}+\lambda\mathbf{I}_{n})^{-1}\mathbf{\Phi}_{0,\leq\ell}\mathbf{D} ^{4}_{0,\leq\ell}\mathbf{\Phi}_{0,\leq\ell}^{\top}(\mathbf{H}_{p}+\lambda\mathbf{I}_{n})^ {-1}\] \[\quad+n(\mathbf{H}_{p}+\lambda\mathbf{I}_{n})^{-1}\mathbf{\Phi}_{>\ell}\mathbf{D} ^{4}_{>\ell}\mathbf{\Phi}_{>\ell}^{\top}(\mathbf{H}_{p}+\lambda\mathbf{I}_{n})^{-1}.\]Recall that \(\mathbf{H}_{p}=\mathbf{\Phi}D^{2}\mathbf{\Phi}^{\top}\). By Lemma 7 we know that with high probability

\[\left\|n(\mathbf{H}_{p}+\lambda\mathbf{I}_{n})^{-1}\mathbf{\Phi}_{>\ell}\mathbf{D}_{>\ell}^{4} \mathbf{\Phi}_{>\ell}^{\top}(\mathbf{H}_{p}+\lambda\mathbf{I})^{-1}\right\|\leq Cnd_{\text {eff}}^{-\ell-1}=o_{d}(1).\]

For the second part, we let \(\mathbf{A}:=\mathbf{\Phi}_{>\ell}\mathbf{D}_{>\ell}^{4}\mathbf{\Phi}_{>\ell}^{\top}+\lambda\bm {I}_{n}\) and apply the Sherman-Morrison-Woodbury formula to obtain

\[n(\mathbf{H}_{p}+\lambda\mathbf{I}_{n})^{-1}\mathbf{\Phi}_{0,\leq\ell}\mathbf{D} _{0,\leq\ell}^{4}\mathbf{\Phi}_{0,\leq\ell}^{\top}(\mathbf{H}_{p}+\lambda\mathbf{I}_{n})^ {-1}\] \[= n\mathbf{A}^{-1}\mathbf{\Phi}_{0,\leq\ell}\Big{(}\mathbf{D}_{>\ell}^{-2}+\bm {\Phi}_{0,\leq\ell}^{\top}\mathbf{A}^{-1}\mathbf{\Phi}_{0,\leq\ell}\Big{)}^{-2}\mathbf{ \Phi}_{0,\leq\ell}^{\top}\mathbf{A}^{-1}\] \[= \mathbf{A}^{-1}\mathbf{\Phi}_{0,\leq\ell}\mathbf{B}^{2}\mathbf{\Phi}_{0,\leq\ell }^{\top}\mathbf{A}^{-1},\]

where

\[\mathbf{B}:=\left((n\mathbf{D}_{>\ell})^{-2}+\frac{1}{n}\mathbf{\Phi}_{0,\leq\ell}^{\top} \mathbf{A}^{-1}\mathbf{\Phi}_{0,\leq\ell}\right)^{-1}.\]

Following the proof of Lemma 13 in [16], we can show that \(\left\|\mathbf{A}^{-1}-\mathbf{I}_{n}\right\|=o_{d,\mathbb{P}}(1)\) and \(\left\|\mathbf{B}-\mathbf{I}_{n}\right\|=o_{d,\mathbb{P}}(1)\) due to assumption that \(1-\frac{1}{\ell}<\ell\) and Lemma 10. 

**Lemma 12**.: _Recall the definition of \(\mathbf{\Phi}_{0,\leq\ell}\) above. Under the assumptions of Theorem 3 with \(1-\frac{1}{\ell}<\beta<1-\frac{1}{\ell+1}\) for some \(\ell\in\mathbb{N}\), when \(n,d\to\infty\) proportionally, we have_

\[\left\|\mathbf{\Phi}_{0,\leq\ell}^{\top}(\mathbf{H}_{p}+\lambda\mathbf{I}_{n})^{-1}\mathbf{ \Phi}_{0,\leq\ell}\mathbf{D}_{0,\leq\ell}-\mathbf{I}_{m}\right\|=o_{d,\mathbb{P}}(1).\]

The proof of Lemma 12 is analogous to Lemma 11, the details of which we omit.

From (A.3), let us denote by

\[T_{1}:= \mathbf{y}^{\top}(\mathbf{H}_{p}+\lambda\mathbf{I})^{-1}\mathbf{E}_{p}\] \[T_{2}:= \mathbf{y}^{\top}(\mathbf{H}_{p}+\lambda\mathbf{I})^{-1}\mathbf{M}_{p}(\mathbf{H}_{p} +\lambda\mathbf{I})^{-1}\mathbf{y}.\]

Now we aim to control \(T_{1}\) and \(T_{2}\) based on Lemmas 11 and 12. We define the projection of \(\mathbf{y}\) onto the Hermite components with degree less than or equals to \(\ell\) and components with degree larger than \(\ell\) as: \(\mathbf{y}=\mathbf{y}_{1}+\mathbf{y}_{2}\), where \(\mathbf{y}_{1}=\mathbf{\Phi}_{0,\leq\ell}\mathbf{P}_{\leq\ell}\mathbf{\alpha}\) and \(\mathbf{y}_{2}=\mathbf{\Phi}_{0,>\ell}\mathbf{P}_{>\ell}\mathbf{\alpha}\). Here \(\mathbf{P}_{\leq\ell}\) projects onto the Hermite components with \(\mathbf{I}=(i_{1},0,\ldots,0)\) and degree no greater \(\ell\), and \(\mathbf{P}_{>\ell}=\mathbf{P}-\mathbf{P}_{\leq\ell}\). Similarly, we decompose \(\mathbf{E}_{p}\) as \(\mathbf{E}_{p}=\mathbf{E}_{p,1}+\mathbf{E}_{p,2}\), where we denote

\[\mathbf{E}_{p,1}:= \mathbf{\Phi}_{0,\leq\ell}\mathbf{D}_{0,\leq\ell}\mathbf{P}_{\leq\ell}\mathbf{ \alpha},\] \[\mathbf{E}_{p,2}:= \mathbf{\Phi}_{0,>\ell}\mathbf{D}_{0,>\ell}\mathbf{P}_{>\ell}\mathbf{\alpha}.\]

**Lemma 13**.: _Under the assumptions of Theorem 3, we have_

\[T_{1}=\left\|P_{\leq\ell}f_{*}\right\|_{L^{2}}^{2}+o_{d,\mathbb{P}}(1),\qquad T _{2}=\left\|P_{\leq\ell}f_{*}\right\|_{L^{2}}^{2}+o_{d,\mathbb{P}}(1).\]

**Proof.** For simplicity, we first control \(T_{2}\) and then apply some of these results to bound \(T_{1}\). Let \(T_{2}=T_{21}+2T_{22}+T_{23}\), where

\[T_{21}:= \mathbf{y}_{1}^{\top}(\mathbf{H}_{p}+\lambda\mathbf{I})^{-1}\mathbf{M}_{p}(\mathbf{H} _{p}+\lambda\mathbf{I})^{-1}\mathbf{y}_{1},\] \[T_{22}:= \mathbf{y}_{1}^{\top}(\mathbf{H}_{p}+\lambda\mathbf{I})^{-1}\mathbf{M}_{p}(\mathbf{H }_{p}+\lambda\mathbf{I})^{-1}\mathbf{y}_{2},\] \[T_{23}:= \mathbf{y}_{2}^{\top}(\mathbf{H}_{p}+\lambda\mathbf{I})^{-1}\mathbf{M}_{p}(\mathbf{H }_{p}+\lambda\mathbf{I})^{-1}\mathbf{y}_{2}.\]

Based on Lemmas 9, 11 and (A.8), we know that

\[T_{21}= \frac{1}{n^{2}}\mathbf{\alpha}^{\top}\mathbf{P}_{\leq\ell}^{\top}\mathbf{\Phi }_{0,\leq\ell}\mathbf{\Phi}_{0,\leq\ell}\mathbf{\Phi}_{0,\leq\ell}^{\top}\mathbf{\Phi}_{0, \leq\ell}\mathbf{P}_{\leq\ell}\mathbf{\alpha}\] \[= \left\|P_{\leq\ell}f_{*}\right\|_{L^{2}}^{2}+o_{d,\mathbb{P}}(1).\]

For \(T_{23}\), following from (23) in [16], when \(d_{\text{eff}}^{\ell+1}<1\), by Markov's inequality, we have

\[T_{23}= \frac{1}{n^{2}}\mathbf{\alpha}^{\top}\mathbf{P}_{>\ell}^{\top}\mathbf{\Phi} _{0,>\ell}^{\top}\mathbf{\Phi}_{0,\leq\ell}\mathbf{\Phi}_{0,\leq\ell}^{\top}\mathbf{\Phi}_{ 0,>\ell}\mathbf{P}_{>\ell}\mathbf{\alpha}+o_{d,\mathbb{P}}(1),\] \[= \frac{d_{\text{eff}}^{\ell+1}}{n}\left\|P_{>\ell}f_{*}\right\|_{L^{ 2}}^{2}+o_{d,\mathbb{P}}(1)=o_{d,\mathbb{P}}(1),\] (A.9)since polynomials in \(\mathbf{\Phi}_{0,\leq\ell}\) are orthogonal to polynomials in \(\mathbf{\Phi}_{0,>\ell}\). Notice that

\[|T_{22}|\leq 2T_{21}^{1/2}T_{23}^{1/2}=o_{d,\mathbb{P}}(1).\]

This finishes the proof for \(T_{2}\).

Now let us consider \(T_{1}=T_{11}+T_{12}+T_{13}\), where

\[T_{11}: =\;\mathbf{y}_{1}^{\top}(\mathbf{H}_{p}+\lambda\mathbf{I})^{-1}\mathbf{E}_{1,p},\] \[T_{12}: =\;\mathbf{y}_{2}^{\top}(\mathbf{H}_{p}+\lambda\mathbf{I})^{-1}\mathbf{E}_{1,p},\] \[T_{13}: =\;\mathbf{y}^{\top}(\mathbf{H}_{p}+\lambda\mathbf{I})^{-1}\mathbf{E}_{2,p}.\]

From Lemmas 9 and 12, we directly have that

\[T_{11} =\;\mathbf{\alpha}^{\top}\mathbf{P}_{\leq\ell}\mathbf{\alpha}+o_{d,\mathbb{P} }(1)\] \[=\;\|P_{\leq\ell}f_{*}\|_{L^{2}}^{2}+o_{d,\mathbb{P}}(1).\]

Applying (A.9) from above, we have \(T_{12}\leq T_{23}\|P_{\leq\ell}f_{*}\|_{L^{2}}^{2}=o_{d,\mathbb{P}}(1)\). Lastly, we know that

\[|T_{13}| \leq\;\|\mathbf{y}\|\big{\|}(\mathbf{K}+\lambda\mathbf{I})^{-1}\big{\|}\|\| \mathbf{E}_{2}\|\] \[\leq\;Cn\|f_{*}\|_{L^{2}}^{2}\frac{1}{d_{\text{eff}}^{d+1}}=o_{d, \mathbb{P}}(1),\]

based on the effective dimension \(d_{\text{eff}}=d^{1-\beta}\). This completes the proof of this lemma.

Combining all the above lemmas together, Theorem 3 can be established by considering (A.3) and the following approximation

\[\mathcal{R}(f_{\text{ker}}) =\;\|f_{*}\|_{L^{2}}-2T_{1}+T_{2}=\|P_{>\ell}f_{*}\|_{L^{2}}^{2}+ o_{d,\mathbb{P}}(1)\] \[=\;\|P_{>\ell}f_{*}\|_{L^{2}}^{2}+o_{d,\mathbb{P}}(1),\]

where \(P_{>\ell}\) is defined by in Section 3.

## Appendix B Analysis of Neural Network

For the two-layer neural network, we perform the gradient-based optimization procedure outlined in Algorithm 1. This algorithm is almost identical to the two-stage representation learning procedure analyzed in recent works [2, 3, 1], with the only difference being the additional normalization step (B.2) to handle the anisotropy of the input data.

### Hermite Expansion of Population Gradient

We first consider one parameter vector at random initialization \(\mathbf{w}_{i}\overset{\text{i.i.d.}}{\sim}\mathcal{N}(0,d^{-1}\mathbf{I}_{d})\). In the following, we restrict ourselves to parameters in a subset defined as

\[\mathcal{E}_{\tau}=\bigg{\{}i\in[N]\;\Big{|}\;\frac{1}{\tau\sqrt{d}}\leq| \langle\mathbf{\mu},\mathbf{w}_{i}\rangle|\leq\frac{\tau}{\sqrt{d}}\bigg{\}},\]

where the positive scalar \(\tau=\mathcal{O}(\operatorname{polylog}(d))\) will be appropriately selected in the sequel. Note that due to the Gaussian initialization, setting \(\tau=\Theta(1)\) ensures that a _constant_ fraction of neurons \(\mathbf{w}_{i}\) falls into the set \(\mathcal{E}_{\tau}\). The gradient of \(\mathbf{w}_{i}\) with respect to the population squared loss can be written as

\[\nabla_{\mathbf{w}_{i}}\Big{[}\mathbb{E}_{\mathbf{x}}\big{(}f_{*}(\mathbf{x})-f_{\text{NN} }^{(0)}(\mathbf{x})\big{)}^{2}\Big{]}=\mathbb{E}_{\mathbf{x}}\bigg{[}\frac{1}{\sqrt{N} }a_{i}\big{(}f_{\text{NN}}^{(0)}(\mathbf{x})-f_{*}(\mathbf{x})\big{)}\sigma^{\prime}( \langle\mathbf{x},\mathbf{w}_{i}\rangle+b_{i})\mathbf{x}\bigg{]}.\]

Due to our specific choice of parameter initialization (4.1), we know that the output of the student model at initialization \(f_{\text{NN}}^{(0)}\) can be ignored (see Lemma 18 for justification). We, therefore, focus on the following term capturing the correlation between the \(i\)-th student neuron and the teacher model:

\[g(\mathbf{w}_{i}) :=\mathbb{E}_{\mathbf{x}}[f_{*}(\mathbf{x})\sigma^{\prime}(\langle\mathbf{x}, \mathbf{w}_{i}\rangle+b_{i})\mathbf{x}]\] (B.3) \[\overset{(i)}{=}\mathbf{\Sigma}\mathbb{E}_{\mathbf{x}}\bigg{[}f^{\prime}_ {*}(\mathbf{x})\sigma^{\prime}(\langle\mathbf{x},\mathbf{w}_{i}\rangle+b_{i})\cdot\frac{1} {\sqrt{1+\theta}}\mathbf{\mu}+f_{*}(\mathbf{x})\sigma^{\prime\prime}(\langle\mathbf{x},\bm {w}_{i}\rangle+b_{i})\cdot\mathbf{w}_{i}\bigg{]},\] \[=\underbrace{\sqrt{1+\theta}\mathbf{\mu}\cdot\mathbb{E}[f^{\prime}_{ *}(\mathbf{x})\sigma^{\prime}(\langle\mathbf{x},\mathbf{w}_{i}\rangle+b_{i})]}_{(I)}+ \underbrace{(\mathbf{I}+\theta\mathbf{\mu}\mathbf{\mu}^{\top})\mathbf{w}_{i}\cdot\mathbb{E}[f _{*}(\mathbf{x})\sigma^{\prime\prime}(\langle\mathbf{x},\mathbf{w}_{i}\rangle+b_{i})]}_{( II)}.\]

where \((i)\) is due to Stein's lemma. Note that \(\left\|\frac{1}{\sqrt{1+\theta}}\mathbf{\Sigma}^{1/2}\mathbf{\mu}\right\|=1\). Therefore, if \(\|\mathbf{\Sigma}^{1/2}\mathbf{w}_{i}\|\approx 1\) (which is entailed by the defined event \(\mathcal{E}_{\tau}\)), we may decompose the first term in the last equation via the Hermite expansion as follows (e.g., see [2]):

\[(I) =\sqrt{1+\theta}\mathbf{\mu}\cdot\mathbb{E}[f^{\prime}_{*}(\mathbf{x}) \sigma^{\prime}(\langle\mathbf{x},\mathbf{w}_{i}\rangle+b_{i})]\] \[\overset{(i)}{\approx}\sqrt{1+\theta}\mathbf{\mu}\cdot\sum_{j=0}^{ \infty}(j+1)^{2}\alpha_{j+1}^{b_{i}}\alpha_{j+1}^{*}\cdot\Big{\langle}\sqrt{1+ \theta}\mathbf{\mu},\mathbf{w}_{i}\Big{\rangle}^{j},\] (B.4)

where \(\alpha^{b_{i}},\alpha^{*}\) are the Hermite coefficients of the (shifted) student and teacher nonlinearities:

\[\sigma(z+b_{i})=\sum_{j=0}^{\infty}\alpha_{j}^{b_{i}}h_{j}(z),\quad\sigma_{*}( z)=\sum_{j=0}^{\infty}\alpha_{j}^{*}h_{j}(z).\]

The following lemma controls the error in \((i)\) due to the norm fluctuation of \(\mathbf{w}_{i}\).

**Lemma 14**.: _Define \(\tilde{\mathbf{w}}_{i}=\mathbf{w}_{i}/\|\mathbf{\Sigma}^{1/2}\mathbf{w}_{i}\|\) for \(i\in\mathcal{E}(\tau)\), then we have_

\[\Delta:=\left\|\mathbb{E}_{\mathbf{x}}[f^{\prime}_{*}(\mathbf{x})\sigma^{\prime}( \langle\mathbf{x},\mathbf{w}_{i}\rangle+b_{i})]-\mathbb{E}_{\mathbf{x}}[f^{\prime}_{*}(\bm {x})\sigma^{\prime}(\langle\mathbf{x},\tilde{\mathbf{w}}_{i}\rangle+b_{i})]\right\| \lesssim\frac{1}{\sqrt{d}}+\frac{\tau}{d^{1-\beta}}.\]

Proof.: By Cauchy-Schwarz inequality, we have

\[\left\|\mathbb{E}[f^{\prime}_{*}(\mathbf{x})\sigma^{\prime}(\langle \mathbf{x},\mathbf{w}_{i}\rangle+b_{i})]-\mathbb{E}[f^{\prime}_{*}(\mathbf{x})\sigma^{ \prime}(\langle\mathbf{x},\tilde{\mathbf{w}}_{i}\rangle+b_{i})]\right\|\] \[\leq \sqrt{\left\|f^{\prime}_{*}(\mathbf{x})\right\|_{L^{2}}^{2}\cdot\left\| \sigma^{\prime}(\langle\mathbf{x},\mathbf{w}_{i}\rangle+b_{i})-\sigma^{\prime}(\langle \mathbf{x},\tilde{\mathbf{w}}_{i}\rangle+b_{i})\right\|_{L^{2}}^{2}}.\]

Note that \(\left\|f^{\prime}_{*}(\mathbf{x})\right\|_{L^{2}}=\mathbb{E}[\sigma^{\prime}_{*}( \xi)^{2}]^{1/2}=\mathcal{O}(1)\), where \(\xi\sim\mathcal{N}(0,1)\).

\[\|\sigma^{\prime}(\langle\mathbf{x},\mathbf{w}_{i}\rangle+b_{i})-\sigma^{\prime}( \langle\mathbf{x},\tilde{\mathbf{w}}_{i}\rangle+b_{i})\|_{L^{2}}\leq\left\|\mathbf{\Sigma} ^{1/2}\mathbf{w}_{i}\right\|-1\leq\frac{1}{\sqrt{d}}+\frac{\tau}{d^{1-\beta}},\]which concludes the proof. 

Similarly, for the second term, we have

\[(II) =(\mathbf{I}_{d}+\theta\mathbf{\mu}\mathbf{\mu}^{\top})\mathbf{w}_{i}\cdot\mathbb{E}[f _{*}(\mathbf{x})\sigma^{\prime\prime}(\left\langle\mathbf{x},\mathbf{w}_{i}\right\rangle)]\] \[\stackrel{{(i)}}{{\approx}}\theta\langle\mathbf{\mu},\mathbf{ w}_{i}\rangle\mathbf{\mu}\cdot\sum_{j=0}^{\infty}(j+1)(j+2)\alpha_{j+2}\alpha_{i}^{*} \cdot\left\langle\sqrt{1+\theta}\mathbf{\mu},\mathbf{w}_{i}\right\rangle^{j}\] \[\quad+\mathbf{w}_{i}\cdot\sum_{j=0}^{\infty}(j+1)(j+2)\alpha_{j+2} \alpha_{i}^{*}\cdot\left\langle\sqrt{1+\theta}\mathbf{\mu},\mathbf{w}_{i}\right\rangle ^{j},\] (B.5)

where \((i)\) can be justified using the same argument from Lemma 14, the details of which we omit.

Random bias units "diversify" the gradient update.Importantly, the shift introduced by the bias term \(b_{i}\) ensures almost all neurons will have a non-vanishing Hermite coefficient at the given degree, as shown by the following lemma.

**Lemma 15**.: _Given degree \(p\in\mathbb{N}\), for all \(b\in\mathbb{R}\) such that \(h_{p-2}(b)\neq 0\) when \(p\geq 2\), we have that the \(p\)-th Hermite coefficient of \(f(z)=\sigma(z+b)\) is non-zero, i.e., \(\left|\mathbb{E}_{z\sim\mathcal{N}(0,1)}[\sigma(z+b)h_{p}(z)]\right|>0\)._

Proof.: First note that when \(p=1\), we have

\[\int_{\mathbb{R}}\mathrm{ReLU}(x+b)xe^{-\frac{z^{2}}{2}}dx=\frac{1}{2}\Big{(} \text{erf}\Big{(}\tfrac{b}{\sqrt{2}}\Big{)}+1\Big{)}>0.\]

For \(b\geq 2\), we divide the computation based on the parity of \(p\). When \(p\) is even, i.e., \(p=2s\), for any \(b\in\mathbb{R}\), the Hermite coefficients are given by

\[\int_{\mathbb{R}}\mathrm{ReLU}(x+b)h_{2s}(x)e^{-\frac{z^{2}}{2}}dx=e^{-\frac{b^ {2}}{2}}h_{2s-2}(b),\]

for all \(s\in\mathbb{N}\). As for odd \(p=2s+1\), the odd-order Hermite coefficients are given by

\[\int_{\mathbb{R}}\mathrm{ReLU}(x+b)h_{2s+1}(x)e^{-\frac{z^{2}}{2}}dx=-e^{-\frac {b^{2}}{2}}h_{2s-1}(b).\]

Therefore, as long as \(b\) is not the zero of the Hermite polynomial of degree \(p-2\), the Hermite coefficients of \(\mathrm{ReLU}(x+b)\) would be nonzero. 

Approximation of population gradient.Note that for fixed \(p\in\mathbb{N}\), the condition \(h_{p-2}(b)\neq 0\) excludes less than \(p\) possible values of bias \(b_{i}\). Combining (B.4) and (B.5), we have the following characterization of the correlation term (B.3). Now given \(k\in\mathbb{N}\) as the information exponent of \(\sigma_{*}\), we restrict ourselves to \(b_{i}\) such that the corresponding degree-\(k\) Hermite coefficient of the shifted ReLU \(\alpha_{k}^{b_{i}}=\mathbb{E}_{z\sim\mathcal{N}(0,1)}[\sigma(z+b_{i})h_{k}(z)]\) is at least of order \(\frac{1}{\log^{C}d}\) for some constant \(C>0\). In Section B.3.1 we show that such a condition is satisfied by a sufficiently large set of \(b_{i}\).

**Proposition 16**.: _Given fixed \(k\in\mathbb{N}\) and \(\theta\asymp d^{\beta}\) where \(\beta\in[0,1)\), consider neurons \(\mathbf{w}_{i}\) where the index \(i\in\mathcal{E}_{\tau}\) for \(\tau=\mathcal{O}\Big{(}{\log^{C}d}\Big{)}\) ensures that \(\left|\alpha_{k}^{b_{i}}\right|=\Omega\Big{(}\frac{1}{\log^{C}d}\Big{)}\). Then we have_

\[\left\|(I)\right\|^{2} =\Theta\Big{(}\alpha_{k}^{b_{i}}\langle\mathbf{\mu},\mathbf{w}_{i}\rangle ^{2(k-1)}\cdot(1+\theta)^{k}\Big{)}+\mathcal{O}(\Delta).\] \[\left\|(II)\right\|^{2} =\mathcal{O}\Big{(}\mathrm{poly}(\tau)\cdot\big{(}d^{-k-1}(1+ \theta)^{k+2}+d^{-k}(1+\theta)^{k}\big{)}\big{)}+\mathcal{O}(\Delta).\]

_Where \(\Delta\lesssim\frac{1}{d^{1/2}}+\frac{\tau}{d^{1-\beta}}\) is defined in Lemma 14._

Proof.: Since the link function \(\sigma_{*}\) has information exponent \(k\), by definition, we know that \(\alpha_{i}^{*}=0\) for all \(i<k\). Also, since \(\alpha_{k}^{b_{i}}=\Omega\Big{(}\frac{1}{\log^{C}d}\Big{)}\), under event \(\mathcal{E}_{i}(\tau)\) with \(\tau=\mathcal{O}\Big{(}{\log^{C}d}\Big{)}\), the expectedgradient is dominated by the term involving \(\alpha_{k}^{*}\) in the summation. For \((I)\) we have

\[\left\|\sqrt{1+\theta}\mathbf{\mu}\cdot\sum_{j=0}^{\infty}(j+1)^{2} \alpha_{j+1}^{b_{i}}\alpha_{j+1}^{*}\cdot\left\langle\sqrt{1+\theta}\mathbf{\mu}, \mathbf{w}_{i}\right\rangle^{j}\right\|\] \[=(1+o_{d}(1))\left\|\sqrt{1+\theta}\mathbf{\mu}\cdot\alpha_{k}^{b_{i} }\alpha_{k}^{*}\cdot\left\langle\sqrt{1+\theta}\mathbf{\mu},\mathbf{w}_{i}\right\rangle ^{k-1}\right\|\] \[=\Theta\Big{(}\alpha_{k}^{b_{i}}\langle\mathbf{\mu},\mathbf{w}_{i}\rangle^ {k-1}(1+\theta)^{\frac{k}{2}}\Big{)}.\]

The computation of \(\|(II)\|\) follows from the exact same procedure. Finally, the proof is complete by including the norm fluctuation error \(\Delta\) in Lemma 14. 

Proposition 16 establishes that \((I)\) is the dominating term in the correlation \(g(\mathbf{w}_{i})\) for any \(\beta>1-\frac{1}{k}\) as shown in the following corollary.

**Corollary 17**.: _Under the same assumptions as Proposition 16, if \(\beta>1-\frac{1}{k}\), we have_

\[\|g(\mathbf{w}_{i})-(I)\|\lesssim d^{-\varepsilon}\|g(\mathbf{w}_{i})\|,\]

_for some small constant \(\varepsilon>0\)._

### Finite-sample Concentration

Given \(n\) training examples \(\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{n}\), denote the matrix of training data \(\mathbf{X}\in\mathbb{R}^{n\times d}\), and the vector of labels \(\mathbf{y}\in\mathbb{R}^{n}\). Similarly, we denote the initial first-layer parameters as \(\mathbf{W}_{0}\in\mathbb{R}^{d\times N},\mathbf{b}_{0}\in\mathbb{R}^{N},\mathbf{a}_{0}\in \mathbb{R}^{n}\). We first show that the initial gradient (B.1) is dominated by the correlation term (B.3).

**Lemma 18**.: _When \(n,d,N\to\infty\) proportionally, there exist some constants \(c,C>0\) such that_

\[\mathbb{P}\Big{(}\Big{\|}\tfrac{1}{n}\sum_{j=0}^{n}f_{\mathrm{NN}}^{(0)}(\mathbf{x} _{j})\sigma^{\prime}(\langle\mathbf{x}_{j},\mathbf{w}_{i}\rangle+b_{i})\mathbf{x}_{j} \Big{\|}\geq C\tfrac{\sqrt{1+\theta}}{d}\Big{)}\leq\exp\bigl{(}-c\log^{2}d \bigr{)}.\]

Proof.: Using the matrix & vector notation,

\[\frac{1}{n}\sum_{j=0}^{n}f_{\mathrm{NN}}^{(0)}(\mathbf{x}_{j})\sigma^{\prime}( \langle\mathbf{x}_{j},\mathbf{w}_{i}\rangle+b_{i})\mathbf{x}_{j}=\frac{1}{n}\mathbf{X}^{\top}( f_{\mathrm{NN}}^{(0)}(\mathbf{X})\odot\sigma^{\prime}(\mathbf{X}\mathbf{w}_{i}+b_{i}\mathbf{1}_{n})).\]

Since \(\sigma^{\prime}\) is bounded, we know that

\[\left\|\sum_{j=0}^{n}f_{\mathrm{NN}}^{(0)}(\mathbf{x}_{j})\sigma^{\prime}(\langle \mathbf{x}_{j},\mathbf{w}_{i}\rangle+b_{i})\mathbf{x}_{j}\right\|\lesssim\|\mathbf{X}\|\Big{\|} f_{\mathrm{NN}}^{(0)}(\mathbf{X})\Big{\|}=\frac{1}{\sqrt{N}}\|\mathbf{X}\|\|\sigma(\mathbf{X} \mathbf{W}_{0}+\mathbf{b}_{0})\mathbf{a}_{0}\|.\]

To control the RHS, we first recall that the operator norm of \(\mathbf{X}\) is bounded as follows,

\[\mathbb{P}\Big{(}\|\mathbf{X}\|\geq C\sqrt{(1+\theta)d}\Big{)}\leq\exp(-cd),\] (B.6)

for some constants \(c,C>0\). Next, we consider the event that the norm of each training example \(\mathbf{x}_{j}\) can be controlled as follows: \(\mathcal{B}:=\Big{\{}\big{\|}\|\mathbf{x}_{j}\|/\sqrt{d}-1\Big{|}\leq\nicefrac{{1} }{{2}},\;j\in[n]\Big{\}}\). Recall that \(\mathbf{x}_{j}=\mathbf{\Sigma}^{1/2}\mathbf{z}_{j}\); hence \(\|\mathbf{x}_{j}\|^{2}\leq\|\mathbf{z}_{j}\|^{2}+(1+\theta)|\langle\mathbf{z}_{j},\mathbf{\mu} \rangle\big{|}^{2}\), which is clearly dominated by \(\|\mathbf{z}_{j}\|^{2}\) when \(\beta<1\). Consequently, [23, Theorem 3.1.1] implies that

\[\mathbb{P}(\mathcal{B})\geq 1-n\exp(-cd).\] (B.7)

Under event \(\mathcal{B}\), we know that \(f_{j}:=\langle\mathbf{a}_{0},\sigma(\mathbf{W}_{0}^{\top}\mathbf{x}_{j}+\mathbf{b}_{0}) \rangle=\sum_{k=0}^{N}a_{j}\sigma(\langle\mathbf{x}_{j},\mathbf{w}_{k}\rangle+b_{k})\) is a sum of independent sub-exponential random variables, where the sub-exponential norm can be bounded as \(\|f_{j}\|_{\psi_{1}}\leq\|a_{j}\|_{\psi_{2}}\|\sigma(\langle\mathbf{x}_{j},\mathbf{w}_ {k}\rangle+b_{k})\|_{\psi_{2}}\lesssim N^{-1/2}\) conditioning on event \(\mathcal{B}\). Therefore, for each \(j\in[n]\), we have

\[\mathbb{P}(|f_{j}|\geq C\log d)\leq\exp\bigl{(}-c\log^{2}d\bigr{)}.\] (B.8)

Combining (B.6), (B.7) and taking a union bound over all \(f_{j}\) in (B.8) establishes the lemma. 

The following proposition establishes the finite-sample concentration of the correlation term \(g(\mathbf{w}_{i})\).

**Proposition 19**.: _Define the empirical correlation term_

\[g_{n}(\mathbf{w}_{i}):=\frac{1}{n}\sum_{j=1}^{n}\mathbf{x}_{j}f_{*}(\mathbf{x}_{j})\sigma^{ \prime}(\langle\mathbf{x}_{j},\mathbf{w}_{i}\rangle+b_{i}),\]

_and recall the population counterpart_

\[g(\mathbf{w}_{i})=\mathbb{E}_{\mathbf{x}}[f_{*}(\mathbf{x})\sigma^{\prime}(\langle\mathbf{x}, \mathbf{w}_{i}\rangle+b_{i})\mathbf{x}].\]

_Then for any fixed \(\mathbf{w}_{i}\) and \(b_{i}\), and any \(p\)-degree polynomial \(\sigma_{*}\) with information exponent \(k\), if \(\beta\in[0,1)\), we have_

\[\mathbb{P}\bigg{(}\|g_{n}(\mathbf{w}_{i})-g(\mathbf{w}_{i})\|\geq\log^{4p}(n)\sqrt{ \frac{d}{n}}\bigg{)}\leq n\exp\bigl{(}-c\log^{2}n\bigr{)},\] (B.9)

_for some constant \(c>0\). Moreover, when \(i\in\mathcal{E}_{\tau}\) and \(\alpha_{k}^{b_{i}}=\Omega\Bigl{(}\frac{1}{\log^{c}d}\Bigr{)}\), we have_

\[\mathbb{P}\Bigg{(}\|g_{n}(\mathbf{w}_{i})-g(\mathbf{w}_{i})\|\geq C\|g(\mathbf{w}_{i})\| \cdot\mathrm{polylog}(n,d)\sqrt{\frac{d_{\mathrm{eff}}^{k}}{n}}\Bigg{)}\leq n \exp\bigl{(}-c\log^{2}n\bigr{)},\] (B.10)

_for any \(1>\beta>1-\frac{1}{k}\), where \(d_{\mathrm{eff}}:=d^{1-\beta}\)._

Proof.: Define the projection \(\mathbf{P}:=\mathbf{I}_{d}-\mathbf{\mu\mu}^{\top}\), we have the following decomposition

\[\mathbf{\Sigma}^{1/2}\mathbf{z}_{i}=\mathbf{P}\mathbf{z}_{i}+\sqrt{1+\theta}\mathbf{\mu\mu}^{\top} \mathbf{z}_{i},\]

where Gaussian random vectors \(\mathbf{P}\mathbf{z}_{i}\) and \((\mathbf{\mu}^{\top}\mathbf{z}_{i})\mathbf{\mu}\) are centered and independent with each other. With this decomposition, we further decompose \(g_{n}(\mathbf{w}_{i})\) by

\[g_{n}(\mathbf{w}_{i}) =\ \frac{1}{n}\sum_{i=1}^{n}\mathbf{\Sigma}^{1/2}\mathbf{z}_{i}\sigma_{*}( \mathbf{\mu}^{\top}\mathbf{z}_{i})\sigma^{\prime}(\mathbf{w}^{\top}\mathbf{\Sigma}^{1/2}\mathbf{z} _{i}+b)\] \[=\ \underbrace{\left(\frac{1}{n}\sum_{i=1}^{n}\mathbf{P}\mathbf{z}_{i} \sigma_{*}(\mathbf{\mu}^{\top}\mathbf{z}_{i})\sigma^{\prime}(\mathbf{w}^{\top}\mathbf{\Sigma} ^{1/2}\mathbf{z}_{i}+b)\right)}_{\mathbf{I}_{1}}\] \[\qquad+\underbrace{\mathbf{\mu}\Bigg{(}\frac{\sqrt{1+\theta}}{n}\sum _{i=1}^{n}\mathbf{\mu}^{\top}\mathbf{z}_{i}\sigma_{*}(\mathbf{\mu}^{\top}\mathbf{z}_{i})\sigma ^{\prime}(\mathbf{w}^{\top}\mathbf{\Sigma}^{1/2}\mathbf{z}_{i}+b)\Bigg{)}}_{\mathbf{I}_{2}}.\]

Therefore, for any \(t>0\) and \(q\in(0,1)\), we have

\[\mathbb{P}(\|g_{n}(\mathbf{w}_{i})-g(\mathbf{w}_{i})\|>t)\] \[\leq\inf_{q\in(0,1)}\{\mathbb{P}(\|\mathbf{I}_{1}-\mathbb{E}\mathbf{I}_{ 1}\|\geq qt)+\mathbb{P}(\|\mathbf{I}_{2}-\mathbb{E}\mathbf{I}_{2}\|\geq(1-q)t)\}.\] (B.11)

In the following, we present the concentration inequalities of \(\mathbf{I}_{1}\) and \(\mathbf{I}_{2}\) separately.

Now we first consider the concentration of \(\mathbf{I}_{2}\). Notice that for any \(s>0\),

\[\mathbb{P}(\|\mathbf{I}_{2}-\mathbb{E}\mathbf{I}_{2}\|\geq s)=\mathbb{P}\Big{(}\sqrt{1+ \theta}\big{\|}\tfrac{1}{n}\sum_{i=1}^{n}f(\mathbf{z}_{i})-\mathbb{E}[f(\mathbf{z}_{i} )]\big{|}\geq s\Big{)},\]

where function \(f:\mathbb{R}^{d}\to\mathbb{R}\) is defined by \(f(\mathbf{z}):=\mathbf{\mu}^{\top}\mathbf{z}\sigma_{*}(\mathbf{\mu}^{\top}\mathbf{z})\sigma^{\prime }(\mathbf{w}^{\top}\mathbf{\Sigma}^{1/2}\mathbf{z}+b)\). Based on [23], we next estimate the Orlicz norm of \(f(\mathbf{z}_{i})\). Recall that the \(\alpha\)-exponential Orlicz norm of a random variable is defined by

\[\|X\|_{\psi_{\alpha}}=\inf_{t>0}\{t:\mathbb{E}[\exp\left(|X|^{\alpha}/t^{ \alpha}\right)]\leq 2\}.\]

For any \(m\in\mathbb{N}\) and \(\mathbf{z}\sim\mathcal{N}(0,\mathbf{I}_{d})\), since \(\sigma_{*}\) is any polynomial with degree at most \(p\), we have

\[\|f(\mathbf{z})\|_{L^{m}}=\mathbb{E}[|f(\mathbf{z})|^{m}]^{1/m} \leq\ \mathbb{E}_{\xi\sim\mathcal{N}(0,1)}[|\xi\sigma_{*}(\xi)|^{m} ]^{1/m}\] \[\leq\ m^{\frac{p+1}{2}}\|\xi\sigma_{*}(\xi)\|_{L^{2}},\]where the last inequality is due to Gaussian hypercontractivity [14, Lemma 20]. Thus, random variable \(f(\mathbf{z})\) has a finite Orlicz norm of order \(\alpha:=\frac{2}{p+1}\): there exists some universal constant \(C>0\) such that \(\left\|f(\mathbf{z})\right\|_{\psi_{\alpha}}\leq C\). Then by Theorem 1.5 of [13], we can obtain that

\[\mathbb{P}(\left\|\mathbf{I}_{2}-\mathbb{E}\mathbf{I}_{2}\right\|\geq s) \leq 2\exp\left(-c\bigg{(}s\sqrt{\frac{n}{1+\theta}}\bigg{)}^{\frac{2}{p+1}} \right),\] (B.12)

for any \(s>0\) and some universal constant \(c>0\).

Next, we prove the concentration of \(\mathbf{I}_{1}\). Let \(\mathbf{u}_{i}:=\mathbf{P}\mathbf{z}_{i}\sigma_{*}(\mathbf{\mu}^{\top}\mathbf{z}_{i})\sigma^{ \prime}(\mathbf{w}^{\top}\mathbf{\Sigma}^{1/2}\mathbf{z}_{i}+b)\) for \(i\in[n]\). Consider a matrix \(\mathbf{A}\in\mathbb{R}^{d\times n}\) where the \(i\)-th column of \(\mathbf{A}\) is \(\bar{\mathbf{u}}_{i}:=\mathbf{u}_{i}-\mathbb{E}\mathbf{u}_{i}\) for \(i\in[n]\). Then, for any \(s>0\) we have,

\[\mathbb{P}(\left\|\mathbf{I}_{1}-\mathbb{E}\mathbf{I}_{1}\right\|\geq s) \leq\mathbb{P}\bigg{(}\frac{1}{\sqrt{n}}\|\mathbf{A}\|\geq s\bigg{)}\]

which implies that it is sufficient to analyze the concentration of \(\|\mathbf{A}\|\). Notice that \(\mathbf{A}\) is composed of independent centered columns \(\bar{\mathbf{u}}_{i}\) with a population covariance \(\mathbf{S}:=\mathbb{E}[\bar{\mathbf{u}}_{i}\bar{\mathbf{u}}_{i}^{\top}]\). We can check that

\[\|\mathbf{S}\| = \sup_{\|\mathbf{v}\|=1}\mathrm{Var}(\mathbf{v}^{\top}\mathbf{u}_{i})=\sup_{\| \mathbf{v}\|=1,\;\mathbf{v}\perp\mathbf{\mu}}\mathrm{Var}(\mathbf{v}^{\top}\mathbf{u}_{i})\] (B.13) \[\leq \sup_{\|\mathbf{v}\|=1,\;\mathbf{v}\perp\mathbf{\mu}}\mathbb{E}[(\mathbf{v}^{\top }\mathbf{u}_{i})^{2}]=\sup_{\|\mathbf{v}\|=1,\;\mathbf{v}\perp\mathbf{\mu}}\mathbb{E}[(\mathbf{v}^ {\top}\mathbf{P}\mathbf{z}_{i})^{2}]\cdot\mathbb{E}[\sigma_{*}(\xi)^{2}]\leq C,\]

for some universal constant \(C>0\). Meanwhile,

\[\|\bar{\mathbf{u}}_{i}\| \leq \|\mathbf{P}\mathbf{z}_{i}\|\cdot|\sigma_{*}(\mathbf{\mu}^{\top}\mathbf{z}_{i})|+ \mathbb{E}[\|\mathbf{P}\mathbf{z}_{i}\|]\mathbb{E}[|\sigma_{*}(\mathbf{\mu}^{\top}\mathbf{z}_{ i})|]\] (B.14) \[\leq \|\mathbf{z}_{i}\|\cdot|\sigma_{*}(\mathbf{\mu}^{\top}\mathbf{z}_{i})|+\mathbb{ E}[\|\mathbf{z}_{i}\|]\cdot\mathbb{E}[|\sigma_{*}(\xi)|]\] (B.15) \[\leq \|\mathbf{z}_{i}\|\cdot|\sigma_{*}(\mathbf{\mu}^{\top}\mathbf{z}_{i})|+C^{ \prime}\sqrt{d},\] (B.16)

where the inequality is due to the sub-Gaussian tail of the norm of the Gaussian random vector. Applying Theorem 3.1.1 of [10], we know that \(\|\mathbf{z}_{i}\|\leq c^{\prime}\sqrt{d}\) almost surely, for some universal constant \(c^{\prime}>0\). Since \(\sigma_{*}\) is a polynomial with degree \(p\), we know that \(\|\sigma_{*}(\xi)\|_{\psi_{\alpha}}\) is uniformly bounded by some constant for \(\alpha=1/p\) and \(\xi\sim\mathcal{N}(0,1)\). Hence, \(|\sigma_{*}(\xi)|\leq\log^{2p}(n)\) almost surely. Together with (B.15), we can conclude that \(\|\bar{\mathbf{u}}_{i}\|\leq C\log^{2p}(n)\sqrt{d}\) almost surely as \(n\to\infty\). Therefore, by Theorem 5.44 in [10], we have that with probability at least \(1-n\exp\left(-cs^{2}\right)\),

\[\frac{1}{\sqrt{n}}\|\mathbf{A}\|\leq(C+s\log^{2p}(n))\sqrt{\frac{d}{n}}.\]

Combining (B.12) and choosing \(q=\frac{1}{2}\) and \(t=\log^{4p}(n)\sqrt{\frac{d}{n}}\) in (B.11), one can easily obtain (B.9) for any \(\beta\in[0,1)\). As a corollary of (B.9), applying Proposition 16, we can directly obtain (B.10). This completes the proof of the proposition.

### Learning \(f_{*}\) with Neural Network Features

Univariate approximation.We first adapt an argument from [10] showing that a ReLU nonlinearity can approximate a polynomial function in one dimension with the aid of random bias units.

**Lemma 20** (Corollary 3 in [10], adapted).: _Let \(\sigma(z)=\mathrm{ReLU}(z)\), \(s\sim\mathrm{Unif}(\{-1,1\})\) and \(b\sim\mathcal{N}(0,1)\). Then given twice-differentiable \(f:\mathbb{R}\to\mathbb{R}\), there exists some \(\upsilon(s,b)\) such that for any \(|z|\leq C\) with some constant \(C>0\),_

\[\mathbb{E}[\upsilon(s,b)\sigma(sz+b)]=f(z),\quad and\sup_{s,b}|\upsilon(s,b)| \lesssim\exp(C).\]

Proof.: Let \(\upsilon(s,b)=\frac{e^{2}/2\sqrt{2\pi}}{1-e^{-3c^{2}/2}}\mathbf{1}_{b\in[c,2c]}\), for any \(c>0\). Then we have

\[\mathbb{E}[\upsilon(s,b)\sigma(sz+b)]=1,\]for all \(|z|\leq c\). Now let \(v(s,b)=\frac{s}{f(c)}\mathbf{1}_{b\in[c,2c]}\), for any \(c>0\), where \(f(c):=\int_{c}^{2c}\frac{1}{\sqrt{2\pi}}e^{-b^{2}/2}db\). For similar reasons, we may verify that

\[\mathbb{E}[v(s,b)\sigma(sz+b)]=z,\]

for all \(|z|\leq c\). Let \(v(s,b)=2\sqrt{2\pi}f^{\prime\prime}(-sb)e^{b^{2}/2}\mathbf{1}_{b\in[0,c]}\), for any \(c>0\), where \(f\in C^{2}(\mathbb{R})\). Then we can easily check that

\[\mathbb{E}[v(s,b)\sigma(sz+b)]=f(z)+F(c)+G(c)z,\]

for all \(|z|\leq c\), where

\[F(c) =\ c(f^{\prime}(-c)+f^{\prime}(c))+f(0)-2f(c),\] \[G(c) =\ f^{\prime}(0)-2f^{\prime}(c).\]

Notice that this construction of \(v(s,b)\) approximates the function \(f(z)\) using some constant and linear function perturbations. Now recalling the first two cases, we know that we can approximate any constant and linear functions on this interval, and hence we obtain the desired claim by combining all three cases together. 

#### b.3.1 Analysis of the Learned Feature Map

Recall the training procedure in Algorithm 1. The following lemma shows that after the first-layer parameters are optimized for one (normalized) gradient step, there exist some second-layer coefficients that achieve small prediction risk with high probability.

**Proposition 21**.: _Given fixed \(k\in\mathbb{N}\) and \(\beta>1-\frac{1}{k}\), let \(\boldsymbol{W}_{1}\) be the weight matrix optimized via one normalized gradient descent step defined in Algorithm 1, there exists some second-layer coefficients \(\tilde{\boldsymbol{a}}\in\mathbb{R}^{N}\) such that \(\tilde{f}(\boldsymbol{x})=\frac{1}{\sqrt{N}}\sigma(\boldsymbol{W}_{1}^{\top} \boldsymbol{x}+\boldsymbol{b})\tilde{\boldsymbol{a}}=\frac{1}{\sqrt{N}}\sum_{ i=1}^{N}\tilde{a}_{i}\sigma(a_{i}\langle\boldsymbol{x},\boldsymbol{w}_{i}^{(1)} \rangle+b_{i})\) achieves_

\[\|f_{*}-\tilde{f}\|_{L^{2}}^{2}\lesssim 1/\log^{C}d,\ \text{ and }\ \|\tilde{ \boldsymbol{a}}\|\lesssim\log^{C}d,\]

_for some large constant \(C>0\), with probability \(1-\exp\bigl{(}-c\log^{2}N\bigr{)}\) when \(n,d\to\infty\) proportionally and \(N=\Omega(d^{c})\)._

**Proof.** We follow an argument similar to [BES\({}^{+}\)22, Appendix D] by selecting a suitable set of neurons that can approximate \(f_{*}\). Throughout this proof, we take \(\varepsilon>0\) to be some small non-vanishing constant and \(C>0\) to be some large constant. At a high level, we select the neurons following the univariate approximation result in Lemma 20 to fit a polynomial target function. To this end, we need to show that after one gradient descent step, there exists a sufficiently large set of neurons \(\boldsymbol{w}_{i}^{(1)}\) that align with the signal direction, i.e., \(\left|\sqrt{1+\theta}\langle\boldsymbol{w}_{i}^{(1)},\boldsymbol{\mu}\rangle \right|\approx 1\).

We first restrict ourselves to the "average-case" neurons around the equator \(\boldsymbol{w}_{i}\) where the indices \(i\in\mathcal{E}_{\tau}\). Due to the Gaussian initialization, setting \(\tau=\log^{2}d\) entails \(\mathbb{P}(i\notin\mathcal{E}_{\tau})\lesssim\exp\bigl{(}-\log^{2}d\bigr{)}\).

Now we consider the subset of bias units that gives an "informative" gradient update, i.e., the \(k\)-th Hermite coefficient is not vanishing: \(\alpha_{k}^{b_{i}}=\Omega\Bigl{(}\frac{1}{\log^{C}d}\Bigr{)}\) for some large constant \(C>0\). Recall that Lemma 15 implies that for bias unit satisfying \(h_{p-2}(b_{i})\neq 0\), the degree-\(k\) Hermite coefficient of \(f(z)=\sigma(z+b_{i})\) is non-zero. Therefore, we define the index set

\[\mathcal{B}_{\varepsilon}=\bigl{\{}i\in[N]\ \big{|}\,|h_{p-2}(b_{i})|\geq \varepsilon,\text{ if }p\geq 2\bigr{\}},\] (B.14)

Notice that all the zeros of Hermite polynomials are real and distinct, see Theorem 3.3.1 in [Sze39]. Additionally, from Theorem 3.3.2 in [Sze39], the zeros of \(h_{p+1}(x)\) are interlacing between the zeros of \(h_{p}(x)\). From the recurrence relation of Hermite polynomials, namely,

\[h_{p+1}(x)=xh_{p}(x)-h_{p}^{\prime}(x),\]

we know that \(h_{p}^{\prime}(x)\neq 0\) if \(x\) is a root of \(h_{p}\). Thus, applying the implicit function theorem, we can deduce that by choosing \(\varepsilon\asymp\frac{1}{\log^{C}d}\) for some large \(C>0\), we attain \(\alpha_{k}^{b_{i}}=\Omega\Bigl{(}\frac{1}{\log^{C}d}\Bigr{)}\), for any \(i\in\mathcal{B}_{\varepsilon}\). Due to the Gaussian initialization of \(b_{i}\), we know what the expected size of the set \(\mathbb{E}|\mathcal{B}_{\varepsilon}|=\Omega(N^{1-\varepsilon^{\prime}})\) for some small \(\varepsilon^{\prime}>0\). Therefore, Hoeffding's inequality implies that for large enough \(N=\Omega(d^{\varepsilon})\), we have \(\left|\frac{|\mathcal{B}_{\varepsilon}|}{\mathbb{E}|\mathcal{B}_{\varepsilon }|}-1\right|\leq N^{-1/2+\varepsilon^{\prime}}\) with probability \(1-\exp\bigl{(}-c\log^{2}N\bigr{)}\).

Gradient approximation.For the subset of neurons with indices \(i\in\mathcal{E}_{\varepsilon}\), we introduce a sequence of approximations to the gradient update given as

\[g_{n}(\mathbf{w}_{i}^{(0)})=\frac{1}{n}\mathbf{X}^{\top}\big{(}f_{\mathrm{NN}}^{(0)}(\mathbf{ X})-f_{*}(\mathbf{X})\big{)}\odot\sigma^{\prime}(\mathbf{X}\mathbf{w}_{i}^{(0)}+b_{i}\mathbf{1}_{n}),\]

where we dropped the factor \(\frac{a_{i}}{\sqrt{N}}\) for convenience. First recall that by Lemma 18, we can ignore the initial NN output as follows:

\[\Big{\|}g_{n}(\mathbf{w}_{i}^{(0)})-\tfrac{1}{n}\mathbf{X}^{\top}(f_{*}(\mathbf{X})\odot \sigma^{\prime}(\mathbf{X}\mathbf{w}_{i}^{(0)}+b_{i}\mathbf{1}_{n}))\Big{\|}\lesssim\tfrac{ \sqrt{1+\theta}}{d},\] (B.15)

with probability \(1-\exp\bigl{(}-c\log^{2}d\bigr{)}\) for some constant \(c>0\). Note that since \(\beta<1\), the RHS can always be upper-bounded by \(d^{-\varepsilon}\) for some small \(\varepsilon>0\). Next, we take into account the concentration and truncation error in Propositions 19 and 16, respectively. Define the population gradient (again omitting the scalar \(\frac{a_{i}}{\sqrt{N}}\)),

\[g(\mathbf{w}_{i}^{(0)})=\mathbb{E}_{\mathbf{x}}\Big{[}\mathbf{x}f_{*}(\mathbf{x})\sigma^{\prime }(\langle\mathbf{x},\mathbf{w}_{i}^{(0)}\rangle+b_{i})\Big{]}.\]

(B.10) in Proposition 19 implies that for any fixed \(p\in\mathbb{N}\), \(\beta>1-\frac{1}{k}\), and large enough \(n,d\), with probability \(1-\exp\bigl{(}-c\log^{2}d\bigr{)}\), we have

\[\Big{\|}\frac{1}{n}\mathbf{X}^{\top}(f_{*}(\mathbf{X})\odot\sigma^{\prime}(\mathbf{X}\mathbf{w} _{i}^{(0)}+b_{i}\mathbf{1}_{n}))-g(\mathbf{w}_{i}^{(0)})\Big{\|}\lesssim d^{- \varepsilon}\Big{\|}g(\mathbf{w}_{i}^{(0)})\Big{\|},\] (B.16)

for some \(\varepsilon>0\). As for the population counterpart, Corollary 17 implies that for neurons with indices \(i\in\mathcal{E}_{\tau}\cap\mathcal{B}_{\varepsilon}\), if \(\beta>1-\frac{1}{k}\), then we have

\[\Big{\|}g(\mathbf{w}_{i}^{(0)})-\alpha_{k}^{b_{i}}\alpha_{k}^{*}\sqrt{1+\theta} \Big{\langle}\sqrt{1+\theta}\mathbf{\mu},\mathbf{w}_{i}^{(0)}\Big{\rangle}^{k-1}\mathbf{ \mu}\Big{\|}\lesssim d^{-\varepsilon}\Bigl{(}1+\Big{\|}g\bigl{(}\mathbf{w}_{i}^{(0) }\bigr{)}\Big{\|}\Bigr{)},\] (B.17)

where \(\Big{\|}g\bigl{(}\mathbf{w}_{i}^{(0)}\bigr{)}\Big{\|}\asymp(1+\theta)^{\frac{k}{2} }\Big{|}\Big{\langle}\mathbf{w}_{i}^{(0)},\mathbf{\mu}\Big{\rangle}^{k-1}\Bigg{|}= \tilde{\Theta}\Bigl{(}(1+\theta)^{\frac{k}{2}}d^{\frac{1-k}{2}}\Bigr{)}\) due to the condition \(i\in\mathcal{E}_{\tau}\) for \(\tau=\log^{C}d\). Combining (B.15), (B.16), and (B.17), we know that the gradient vector \(g_{n}(\mathbf{w}_{i}^{(0)})\) achieves significant alignment with the target direction \(\mathbf{\mu}\) when \(\beta>1-\frac{1}{k}\).

Normalization step.Now we consider the normalization procedure (B.2). When \(\beta>1-\frac{1}{k}\), (B.15), (B.16), and (B.17) imply that for neurons \(\mathbf{w}_{i}\) in the subset \(i\in\mathcal{E}_{\tau}\cap\mathcal{B}_{\varepsilon}\), we have

\[\mathbf{w}_{i}^{(1)} =\mathbf{w}_{i}^{(0)}-\eta a_{i}\cdot\frac{1}{n}\mathbf{X}^{\top}\big{(}f_ {\mathrm{NN}}^{(0)}(\mathbf{X})-f_{*}(\mathbf{X})\big{)}\odot\sigma^{\prime}(\mathbf{X}\mathbf{ w}_{i}^{(0)}+b_{i}\mathbf{1}_{n})\] \[\stackrel{{(i)}}{{=}}\mathbf{w}_{i}^{(0)}+\eta a_{i} \cdot\bigg{(}\alpha_{k}^{b_{i}}\alpha_{k}^{*}\sqrt{1+\theta}\Big{\langle}\sqrt {1+\theta}\mathbf{\mu},\mathbf{w}_{i}^{(0)}\Big{\rangle}^{k-1}(\mathbf{\mu}+\mathbf{\Delta}) \bigg{)},\]

where \(\|\mathbf{\Delta}\|\lesssim d^{-\varepsilon}\) for some \(\varepsilon>0\), with probability \(1-\exp\bigl{(}-c\log^{2}d\bigr{)}\). Now recall that we select a sufficiently large learning rate \(\eta=\Omega(N^{1/2+\varepsilon})\) for some small \(\varepsilon>0\). Since \(a_{i}\sim\mathcal{N}(0,1/N)\),

\[\Big{\|}\mathbf{w}_{i}^{(1)}-\kappa_{i}\mathbf{\mu}\Big{\|}\lesssim d^{-\varepsilon}| \kappa_{i}|,\] (B.18)

where \(\kappa_{i}=\eta a_{i}\alpha_{k}^{b_{i}}\alpha_{k}^{*}\sqrt{1+\theta}\Big{\langle} \sqrt{1+\theta}\mathbf{\mu},\mathbf{w}_{i}^{(0)}\Big{\rangle}^{k-1}=\tilde{\Theta} \Bigl{(}N^{\varepsilon}(1+\theta)^{\frac{k}{2}}d^{\frac{1-k}{2}}\Bigr{)}\) by the choice of \(\tau\asymp\log^{C}d\) and \(\alpha_{k}^{b_{i}}\asymp\frac{1}{\log^{C}d}\). Define the normalization factor

\[\chi_{i}=\Bigl{(}\tfrac{1}{n}\sum_{j=1}^{n}\langle\mathbf{x}_{j},\mathbf{w}_{i}^{(1)} \rangle^{2}\Bigr{)}^{1/2}=\frac{1}{\sqrt{n}}\Big{\|}\mathbf{X}\mathbf{w}_{i}^{(1)} \Big{\|},\]

by (B.18) we know that

\[|\chi_{i}-\kappa_{i}\|\mathbf{X}\mathbf{\mu}\|=\frac{1}{\sqrt{n}}\Big{\|} \Big{\|}\mathbf{X}\mathbf{w}_{i}^{(1)}\Big{\|}-\kappa_{i}\|\mathbf{X}\mathbf{\mu}\|\Big{\|}\] \[\leq\frac{1}{\sqrt{n}}\Big{\|}\mathbf{X}\Bigl{(}\mathbf{w}_{i}^{(1)}-\kappa _{i}\mathbf{\mu}\Big{)}\Big{\|}\lesssim d^{-\varepsilon}(1+\theta)^{1/2}|\kappa_{i}|,\]with high probability, where we used the proportional scaling of \(n,d\) in the last inequality. Now note that \(\frac{1}{\sqrt{n}}\|\mathbf{X}\mathbf{\mu}\|\to\sqrt{1+\theta}\) almost surely when \(n,d\to\infty\) proportionally, we have

\[\left|\chi_{i}^{-1}-\left(\kappa_{i}\sqrt{1+\theta}\right)^{-1}\right|=\frac{ \left|\chi_{i}-\kappa_{i}(1+\theta)^{1/2}\right|}{\left|\chi_{i}\kappa_{i}(1+ \theta)^{1/2}\right|}\lesssim d^{-\varepsilon}(1+\theta)^{-1/2}\big{|}\kappa_ {i}^{-1}\big{|}.\]

Combining the above calculations and recalling \(N=\Omega(d^{\kappa})\), we arrive at the following characterization of the normalized weight vector:

\[\left\|\left(\tfrac{1}{n}\sum_{j=1}^{n}\langle\mathbf{x}_{j},\mathbf{w}_{ i}^{(1)}\rangle^{2}\right)^{-1/2}\cdot\mathbf{w}_{i}^{(1)}-\mathrm{sign}(\kappa_{i} )(1+\theta)^{-1/2}\mathbf{\mu}\right\|\] \[\leq\chi_{i}^{-1}\Big{\|}\mathbf{w}_{i}^{(1)}-\kappa_{i}\mathbf{\mu} \Big{\|}+\left|\mathrm{sign}(\kappa_{i})(1+\theta)^{-1/2}-\kappa_{i}\chi_{i}^{ -1}\right|\lesssim d^{-\varepsilon}(1+\theta)^{-1/2},\] (B.19)

with probability \(1-\exp\bigl{(}-c\log^{2}d\bigr{)}\). Observe that (B.19) entails that after the normalization step:

\[\mathbf{w}_{i}^{(1)}\leftarrow\left(\tfrac{1}{n}\sum_{j=1}^{n}\langle\mathbf{x}_{j}, \mathbf{w}_{i}^{(1)}\rangle^{2}\right)^{-1/2}\cdot\mathbf{w}_{i}^{(1)},\]

the updated weight vector with index \(i\in\mathcal{E}_{\tau}\cap\mathcal{B}_{\varepsilon}\) approximates the target direction in the sense that \(\left\|\sqrt{1+\theta}\cdot\mathbf{w}_{i}^{(1)}-\mathrm{sign}(\kappa_{i})\mathbf{\mu} \right\|\lesssim d^{-\varepsilon}\) with high probability, for some small \(\varepsilon>0\).

Approximation of link function.Now we construct certain second-layer coefficients \(\tilde{a}_{i}\) on a subset of neurons with indices \(i\in\mathcal{I}\) such that the (sub)network approximates the link function \(\sigma_{*}\). Roughly speaking, we wish to find some collection of neurons that utilizes the univariate approximation result in Lemma 20:

\[\tilde{f}(\mathbf{x}):=\sum_{i\in\mathcal{I}}\tfrac{\tilde{a}_{i}}{\sqrt{N}}\sigma (\langle\mathbf{x},\mathbf{w}_{i}^{(1)}\rangle+b_{i})\approx\mathbb{E}[\upsilon(s,b) \cdot\sigma(s\langle\mathbf{x},(1+\theta)^{-1/2}\mathbf{\mu}\rangle+b)].\]

First, we consider the ideal setting where \(\mathbf{w}_{i}\) is exactly in the direction of \(\mathbf{\mu}\) up to sign flip, that is, we may write \(\mathbf{w}_{i}=s_{i}(1+\theta)^{-1/2}\mathbf{\mu}\) for \(s_{i}\sim\mathrm{Unif}(\{+1,-1\})\). This allows us to reparameterize \(\langle\mathbf{x},(1+\theta)^{-1/2}\mathbf{\mu}\rangle=:z\in\mathbb{R}\), and choose the second-layer coefficients \(\tilde{a}\) according to Lemma 20 on domain \(|z|\leq\log\log d\). This implies that \(|\upsilon(s,b)|=\mathcal{O}(\exp(\log\log d))=\mathcal{O}(\log d)\). The corresponding "infinite-width" neural network is written as

\[f_{\infty}(z)=\mathbb{E}_{s\sim\mathrm{Unif}(\{+1,-1\}),b\sim\mathcal{N}(0,1 )}[\upsilon(s,b)\cdot\sigma(sz+b)].\]

From Lemma 20 we know that for \(|z|\leq\log\log d\), we have \(f_{\infty}(z)=\sigma_{*}(z)\).

Now we control the difference between the ideal model and the finite-width network of interest. First we restrict ourselves to \(b\in\mathcal{B}_{\varepsilon}^{\prime}\) defined as a "continuous" analogue of (B.14): \(\mathcal{B}_{\varepsilon}^{\prime}=\bigl{\{}b\,\big{|}\,|h_{p-2}(b)|\geq \varepsilon,\text{ if }p\geq 2\bigr{\}}\), with \(\varepsilon\asymp\tfrac{1}{\log^{c}d}\) for large \(C>0\), and write the corresponding infinite-width model as

\[f_{\infty}^{\varepsilon}(z)=\mathbb{E}_{s,b}[\upsilon(s,b)\sigma(sz+b)\cdot \mathbf{1}_{b\in\mathcal{B}_{\varepsilon}^{\prime}}].\]

By the Lipschitz property of \(\sigma\), we have \(|f_{\infty}(z)-f_{\infty}^{\varepsilon}(z)|\lesssim\sup|\upsilon(s,b)|\cdot \tfrac{1}{\log^{c}d}=o_{d}(1)\).

Next, we bound the error due to truncation at the interval \(|z|\leq\log\log d\). Recall that \(\langle\mathbf{x},(1+\theta)^{-1/2}\mathbf{\mu}\rangle=:z\sim\mathcal{N}(0,1)\); hence we have the following evaluation

\[\mathbb{E}_{z\sim\mathcal{N}(0,1)}\Bigl{[}\bigl{(}\sigma_{*}(z)- f_{\infty}^{\varepsilon}(z)\bigr{)}^{2}\Bigr{]}\leq\mathbb{E}_{z}\Bigl{[} \bigl{(}\sigma_{*}(z)-f_{\infty}(z)\bigr{)}^{2}\Bigr{]}+\mathbb{E}_{z}\Bigl{[} \bigl{(}f_{\infty}(z)-f_{\infty}^{\varepsilon}(z)\bigr{)}^{2}\Bigr{]}\] \[\lesssim\frac{1}{\log^{C}d}+\mathbb{E}_{z}\bigl{[}(\sigma_{*}(z )-f_{\infty}(z))^{2}\mathbf{1}_{|z|\geq\log\log d}\bigr{]}.\] (B.20)

To upper-bound the above expectation, observe that

\[\mathbb{E}_{z}[\sigma_{*}(z)^{2}\mathbf{1}_{|z|\geq\log\log d}]\lesssim\int_{|z |>\log\log d}\sigma_{*}(z)^{2}\exp\bigl{(}-z^{2}/2\bigr{)}\,\mathrm{d}z \lesssim\frac{1}{\log^{C}d},\] (B.21)

for large \(C>0\), where we used the fact that \(\sigma_{*}\) is a degree-\(p\) polynomial with bounded \(L^{2}\) norm in the last inequality. On the other hand, by Cauchy-Schwarz we have

\[|f_{\infty}(z)|\leq\sqrt{\mathbb{E}[\upsilon(s,b)^{2}]\cdot\mathbb{E}[\sigma( sz+b)^{2}]}\lesssim\sup|\upsilon(s,b)|\cdot\sqrt{\mathbb{E}[\sigma(sz+b)^{2}]}.\]

[MISSING_PAGE_FAIL:28]

#### b.3.2 Ridge Regression Estimator

Proposition 21 suggests there exists some second-layer coefficients with small \(\ell_{2}\) norm can achieve small prediction risk. Based on a standard concentration argument, we can show that the ridge regression estimator with appropriately chosen \(\lambda\) also achieves low risk (e.g., see [1]). Here we briefly sketch an argument following [1, Appendix D.3].

Given feature map \(\mathbf{x}\rightarrow\frac{1}{\sqrt{N}}\sigma(\mathbf{W}_{1}^{\top}\mathbf{x}+\mathbf{b})\) conditioned on first-layer weights \(\mathbf{W}_{1},\mathbf{b}\), we denote the associated Hilbert space as \(\mathcal{H}\). We define the optimal predictor in the RKHS as \(\tilde{f}:=\operatorname*{argmin}_{f\in\mathcal{H}}\mathbb{E}_{\mathbf{x}}(f(\mathbf{x })-f_{*}(\mathbf{x}))^{2}\), which takes the form of \(\tilde{f}(\mathbf{x})=\left\langle\frac{1}{\sqrt{N}}\sigma(\mathbf{W}_{1}^{\top}\mathbf{x} +\mathbf{b}),\tilde{\mathbf{a}}\right\rangle\) for some \(\tilde{\mathbf{a}}\in\mathbb{R}^{N}\). We have the orthogonal decomposition of \(f_{*}\) in \(L^{2}\):

\[f_{*}(\mathbf{x})=\tilde{f}(\mathbf{x})+f_{\perp}(\mathbf{x}).\]

Given fresh training data \(\{(\tilde{\mathbf{x}}_{i},\tilde{y}_{i})\}_{i=1}^{n}\), it is known that the prediction risk of ridge regression admits the following decomposition:

\[\mathcal{R} =\mathbb{E}_{\mathbf{x}}\bigg{(}\big{(}f_{*}(\mathbf{x})-\tilde{f}(\mathbf{x}) \big{)}+\bigg{(}\tilde{f}(\mathbf{x})-\frac{1}{n}\mathbf{\phi}_{x}\Big{(}\widehat{\mathbf{ \Sigma}}_{\Phi}+\lambda\mathbf{I}\Big{)}^{-1}\mathbf{\Phi}^{\top}\tilde{\mathbf{y}}\Big{)} \bigg{)}^{2}\] \[\lesssim\underbrace{\mathbb{E}_{\mathbf{x}}\big{(}f_{*}(\mathbf{x})-\tilde {f}(\mathbf{x})\big{)}^{2}}_{R_{1}}+\underbrace{\mathbb{E}_{\mathbf{x}}\bigg{(}\tilde{ f}(\mathbf{x})-\frac{1}{n}\mathbf{\phi}_{x}\Big{(}\widehat{\mathbf{\Sigma}}_{\Phi}+\lambda \mathbf{I}\Big{)}^{-1}\mathbf{\Phi}^{\top}\tilde{\mathbf{f}}\bigg{)}^{2}}_{R_{2}}\] \[\quad+\underbrace{\frac{1}{n^{2}}\mathbf{f}_{\perp}^{\top}\mathbf{\Phi} \Big{(}\widehat{\mathbf{\Sigma}}_{\Phi}+\lambda\mathbf{I}\Big{)}^{-1}\mathbf{\Sigma}_{\Phi }\Big{(}\widehat{\mathbf{\Sigma}}_{\Phi}+\lambda\mathbf{I}\Big{)}^{-1}\mathbf{\Phi}^{\top} \mathbf{f}_{\perp}}_{R_{3}},\]

where \(\mathbf{\phi}_{\mathbf{x}}:=\frac{1}{\sqrt{N}}\sigma(\mathbf{W}_{1}^{\top}\mathbf{x}+\mathbf{b})\) for \(\mathbf{x}\in\mathbb{R}^{d}\), which gives \(\mathbf{\Phi}^{\top}=[\mathbf{\phi}_{\tilde{\mathbf{x}}_{1}}^{\top},\ldots,\mathbf{\phi}_{ \tilde{\mathbf{x}}_{i}}^{\top},\ldots,\mathbf{\phi}_{\tilde{\mathbf{x}}_{n}}^{\top}]\), where \(\tilde{\mathbf{x}}_{i}\) is the \(i\)-th row of \(\tilde{\mathbf{X}}\); we denoted \(\widehat{\mathbf{\Sigma}}_{\Phi}:=\frac{1}{n}\mathbf{\Phi}^{\top}\mathbf{\Phi}\), \(\mathbf{\Sigma}_{\Phi}:=\frac{1}{N}\mathbb{E}_{\mathbf{x}}\Big{[}\sigma(\mathbf{W}_{1}^{ \top}\mathbf{x}+\mathbf{b})\sigma(\mathbf{W}_{1}^{\top}\mathbf{x}+\mathbf{b})^{\top}\Big{]}\). Also, the \(i\)-th entry of vector \(\tilde{\mathbf{f}}\) and \(\mathbf{f}_{\perp}\) are given by \([\tilde{\mathbf{f}}]_{i}=\tilde{f}(\tilde{\mathbf{x}}_{i})\), \([\mathbf{f}_{\perp}]_{i}=f_{\perp}(\tilde{\mathbf{x}}_{i})\), respectively.

Concentration of feature covariance.We first establish the concentration of the kernel matrix in the proportional regime. The following lemma provides an estimate of the norm of the features.

**Lemma 22**.: _Given the weight matrix after one gradient step \(\mathbf{W}_{1}\) as specified in Algorithm 1, then we have \(\left\|\mathbf{\Sigma}^{1/2}\mathbf{W}_{1}\right\|_{F}\lesssim\sqrt{N}\), with probability \(1-\exp\bigl{(}-c\log^{2}d\bigr{)}\), as \(n,d,N\rightarrow\infty,n/d\rightarrow\psi>1\)._

Proof.: Write \(\mathbf{v}_{i}=\mathbf{\Sigma}^{1/2}(\mathbf{w}_{i}^{(0)}-\eta a_{i}\cdot g_{n}(\mathbf{w}_{i} ^{(0)}))\), by definition \(\mathbf{x}=\mathbf{\Sigma}^{1/2}\mathbf{z}\) for \(\mathbf{z}\sim\mathcal{N}(0,\mathbf{I}_{d})\), and

\[\left\|\mathbf{\Sigma}^{1/2}\mathbf{w}_{i}^{(1)}\right\|^{-2}=\frac{\frac{1}{n}\mathbf{v}^ {\top}\mathbf{Z}^{\top}\mathbf{Z}\mathbf{v}}{\left\|\mathbf{v}\right\|^{2}}\geq\lambda_{\min} \biggl{(}\frac{1}{n}\mathbf{Z}^{\top}\mathbf{Z}\biggr{)},\]

which implies that \(\left\|\mathbf{\Sigma}^{1/2}\mathbf{w}_{i}^{(1)}\right\|^{2}\leq\lambda_{\min}^{-1} \Bigl{(}\frac{1}{n}\mathbf{Z}^{\top}\mathbf{Z}\Bigr{)}\lesssim 1\) almost surely as \(n,d\rightarrow\infty\), \(n/d>1\). Summing over \(\mathbf{w}_{i}^{(1)}\) yields the desired claim. 

Observe that the feature vector \(\mathbf{\phi}_{\mathbf{x}}=\frac{1}{\sqrt{N}}\sigma(\mathbf{W}_{1}^{\top}\mathbf{x}+\mathbf{b})= \frac{1}{\sqrt{N}}\sigma(\mathbf{W}_{1}^{\top}\mathbf{\Sigma}^{1/2}\mathbf{z}+\mathbf{b})\) is a sub-Gaussian vector with parameter \(\frac{1}{\sqrt{N}}\left\|\mathbf{\Sigma}^{1/2}\mathbf{W}_{1}\right\|_{F}\lesssim 1\) due to the Lipschitzity of \(\sigma\); hence we may apply the following sub-Gaussian concentration result.

**Lemma 23**.: _Under the same assumption as Lemma 22, as \(d\rightarrow\infty\), we have \(\left\|\mathbf{\phi}_{\mathbf{x}}\right\|\leq\log d\) almost surely._

Proof.: Notice that \(\left\|\cdot\right\|\) and \(\sigma(\cdot)\) are 1-Lipschitz convex functions. Following [13, Section 3], we can utilize the convex concentration inequality (see [13, Chapter 5]) to obtain

\[\mathbb{P}(\left|\left\|\mathbf{\phi}_{\mathbf{x}}\right\|-\mathbb{E}_{\mathbf{x}}[\left\| \mathbf{\phi}_{\mathbf{x}}\right\|\right\|]>t)\leq 2\exp{(-ct^{2})},\] (B.26)for any \(t>0\), conditioning on the event that \(\left\|\mathbf{\Sigma}^{1/2}\mathbf{W}_{1}\right\|\leq\left\|\mathbf{\Sigma}^{1/2}\mathbf{W}_{1} \right\|_{F}\lesssim\sqrt{N}\). From Lemma 22, we know this event that \(\left\|\mathbf{\Sigma}^{1/2}\mathbf{W}_{1}\right\|_{F}\lesssim\sqrt{N}\) occurs with probability at least \(1-\exp\left(-c\log^{2}d\right)\). Furthermore, under this event, we have

\[\mathbb{E}_{\mathbf{x}}[\left\|\mathbf{\phi}_{\mathbf{x}}\right\|]^{2}\leq\mathbb{E}[ \left\|\mathbf{\phi}_{\mathbf{x}}\right\|^{2}]=\frac{1}{N}\Big{\|}\mathbf{\Sigma}^{1/2}\mathbf{W }_{1}\Big{\|}_{F}^{2}\mathbb{E}[\sigma(\xi)^{2}]\lesssim 1.\]

Thus, we can take \(t=\log d\) and combine Lemma 22 and (B.26) to attain the assertion.

**Lemma 24** ([31], adapted).: _Under the same assumption as Lemma 22, given \(n\lambda\geq c\log^{2}d\) for some constant \(c>0\), we have_

\[\mathbb{P}\Big{(}\Big{\|}(\mathbf{\Sigma}_{\Phi}+\lambda\mathbf{I})^{-1/2}(\mathbf{\Sigma} _{\Phi}-\mathbf{\widehat{\Sigma}}_{\Phi})(\mathbf{\Sigma}_{\Phi}+\lambda\mathbf{I})^{-1/2 }\Big{\|}\geq t\Big{)}\leq Cn\exp\biggl{(}-\frac{n\lambda t^{2}}{2(1+t/3)\log ^{2}d}\biggr{)},\]

_if \(t^{2}\geq\log^{2}d\cdot(1+t/3)/\lambda n\)._

Proof.: Let us denote

\[\mathbf{G}_{i}=\frac{1}{n}(\mathbf{\Sigma}_{\Phi}+\lambda\mathbf{I})^{-1/2}(\mathbf{\phi}_{\bm {x}_{i}}\mathbf{\phi}_{\mathbf{x}_{i}}^{\top}-\mathbf{\Sigma}_{\Phi})(\mathbf{\Sigma}_{\Phi}+ \lambda\mathbf{I})^{-1/2}.\]

Notice that \(\mathbb{E}[\mathbf{G}_{i}]=0\). From Lemma 23, we know that \(\|[\mathbf{G}_{i}]\leq\log^{2}d/\lambda n\) almost surely. Moreover, we may verify that \(\mathbb{E}[\mathbf{G}_{i}^{2}]\preccurlyeq\frac{1}{n}\frac{\log^{2}d}{\lambda n} \mathbf{I}_{N}\) and

\[\operatorname{Tr}\mathbb{E}[\mathbf{G}_{i}^{2}]\leq\frac{1}{n}\frac{\log^{2}d}{ \lambda n}\operatorname{Tr}(\mathbf{\Sigma}_{\Phi}+\lambda\mathbf{I})^{-1}\mathbf{\Sigma} _{\Phi}\leq\frac{\log^{2}d}{\lambda n}.\]

Here we use the fact that all the eigenvalues of \((\mathbf{\Sigma}_{\Phi}+\lambda\mathbf{I})^{-1}\mathbf{\Sigma}_{\Phi}\) are between 0 and 1. Thus, we can apply Matrix Bernstein bound (see (3.9) in [31]) to conclude this concentration result. 

With Lemma 24 we have the following useful corollary.

**Corollary 25**.: _Given \(n\lambda\gg\log^{4}d\), for any \(t\in(0,1)\), for sufficiently large \(d\),_

\[\left\|(\mathbf{\Sigma}_{\Phi}+\lambda\mathbf{I})^{-1/2}(\mathbf{\Sigma}_{\Phi}-\mathbf{ \widehat{\Sigma}}_{\Phi})(\mathbf{\Sigma}_{\Phi}+\lambda\mathbf{I})^{-1/2}\right\|=o_{ d}(1),\]

_with probability at least \(1-n\exp\bigl{(}-c\log^{2}d\bigr{)}\)._

Prediction risk of ridge regression.We upper-bound the prediction risk of the ridge regression estimator by controlling the decomposed error terms separately.

**Proposition 26**.: _Under the training procedure described in Algorithm 1 with \(N=\Omega(d^{\varepsilon})\) and \(d^{\varepsilon-1}\lesssim\lambda\lesssim d^{\varepsilon}\) for small \(\varepsilon>0\), if \(\beta>1-\frac{1}{k}\), then we have \(\mathcal{R}(\hat{f}_{\mathrm{NN}})=o_{d}(1)\) with probability 1 when \(n,d\to\infty,n/d\to\psi\)._

Proof.: First note that \(R_{1}=\mathbb{E}_{\mathbf{x}}\bigl{(}f_{*}(\mathbf{x})-\tilde{f}(\mathbf{x})\bigr{)}^{2}= \left\|f_{\perp}\right\|_{L^{2}}^{2}\) by definition. For \(R_{2}\), [1, 1, 2, Appendix D.3] showed that \(R_{2}\lesssim\left\|f_{*}-\tilde{f}\right\|_{L^{2}}^{2}+\lambda\|\tilde{f}\|_ {\mathcal{H}}^{2}\). Finally, for \(R_{3}\) due to the concentration in Corollary 25, we have the following evaluation,

\[R_{3} =\frac{1}{n^{2}}\mathbf{f}_{\perp}^{\top}\mathbf{\Phi}\Bigl{(}\mathbf{\widehat {\Sigma}}_{\Phi}+\lambda\mathbf{I}\Bigr{)}^{-1}\mathbf{\Sigma}_{\Phi}\Bigl{(}\mathbf{ \widehat{\Sigma}}_{\Phi}+\lambda\mathbf{I}\Bigr{)}^{-1}\mathbf{\Phi}^{\top}\mathbf{f}_{\perp}\] \[\leq\frac{1}{n}\bigg{\|}\frac{1}{\sqrt{n}}\mathbf{f}_{\perp}^{\top} \mathbf{\Phi}\Bigl{(}\mathbf{\widehat{\Sigma}}_{\Phi}+\lambda\mathbf{I}\Bigr{)}^{-1/2} \bigg{\|}^{2}\cdot\left\|\Bigl{(}\mathbf{\widehat{\Sigma}}_{\Phi}+\lambda\mathbf{I} \Bigr{)}^{-1/2}(\mathbf{\Sigma}_{\Phi}-\mathbf{\widehat{\Sigma}}_{\Phi})\Bigl{(}\mathbf{ \widehat{\Sigma}}_{\Phi}+\lambda\mathbf{I}\Bigr{)}^{-1/2}\right\|\] \[\quad+\frac{1}{n}\bigg{\|}\frac{1}{n}\mathbf{f}_{\perp}^{\top}\mathbf{ \Phi}\Bigl{(}\mathbf{\widehat{\Sigma}}_{\Phi}+\lambda\mathbf{I}\Bigr{)}^{-1}\mathbf{\Phi} ^{\top}\bigg{\|}^{2}\leq\frac{1}{n}\|\mathbf{f}_{\perp}\|^{2},\]

Recall that in Proposition 21 we constructed some \(\tilde{f}\) that achieves \(\|f_{*}-\tilde{f}\|_{L^{2}}^{2}\lesssim 1/\log^{C}d\) and \(\|\tilde{f}\|_{\mathcal{H}}^{2}\lesssim\log^{C}d\). This implies that \(\left\|f_{\perp}\right\|_{L^{2}}^{2}\lesssim 1/\log^{C}d\) and hence \(R_{1}=o_{d,\mathbb{P}}(1)\). Similarly, for \(\lambda=\mathcal{O}(d^{-\varepsilon})\), we have \(R_{2}=o_{d,\mathbb{P}}(1)\). The proof is completed by Markov's inequality on \(R_{3}\).