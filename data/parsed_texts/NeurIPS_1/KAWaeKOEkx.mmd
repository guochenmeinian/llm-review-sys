# DropCompute: simple and more robust distributed synchronous training via compute variance reduction

Niv Giladi\({}^{1,2,*}\) Shahar Gottlieb\({}^{1,2}\) Moran Shkolnik\({}^{1,2}\) Asaf Karnieli\({}^{2}\)

Ron Banner\({}^{2}\) Elad Hoffer\({}^{2}\) Kfir Yehuda Levy\({}^{1}\) Daniel Soudry\({}^{1}\)

\({}^{1}\)Technion - Israel Institute of Technology

\({}^{2}\)Habana-Labs

{giladiniv, moranshkolnik, elad.hoffer, kfiryehud, daniel.soudry}@gmail.com

{sgottlieb, akarnieli, rbanner}@habana.ai

Equal contribution2

###### Abstract

**Background.** Distributed training is essential for large scale training of deep neural networks (DNNs). The dominant methods for large scale DNN training are synchronous (e.g. _All-Reduce_), but these require waiting for all workers in each step. Thus, these methods are limited by the delays caused by straggling workers.

**Results.** We study a typical scenario in which workers are straggling due to variability in compute time. We find an analytical relation between compute time properties and scalability limitations, caused by such straggling workers. With these findings, we propose a simple yet effective decentralized method to reduce the variation among workers and thus improve the robustness of synchronous training. This method can be integrated with the widely used _All-Reduce_. Our findings are validated on large-scale training tasks using 200 Gaudi Accelerators. A reference implementation2 is provided.

Footnote 2: [https://github.com/paper-submissions/dropcompute](https://github.com/paper-submissions/dropcompute)

## 1 Introduction

Deep Neural Networks (DNNs) training continues to scale over size and computational footprint, as a result of a higher number of trainable parameters, wider and deeper models, and growing amounts of training data. As improvements in model quality (measured by test loss, for example) (Kaplan et al., 2020) lead over hardware capabilities (Hooker, 2020), this scale-up translates into a need for a growing number of training devices working in tandem (Chowdhey et al., 2022), turning distributed training to the standard approach for training DNNs on a large scale.

Distributed training typically refers to three parallelism paradigms -- data parallel, model parallel and layer pipelining (Ben-Nun and Hoefler, 2019). Several variants and hybrid solutions exist in modern implementations such as tensor parallel (Narayanan et al., 2021) and parameter sharding (Rajbhandari et al., 2020; Rasley et al., 2020). These can be used separately or combined as they are orthogonal to each other. Mainly, data parallelism is straightforward, where the data is sharded among workers, and all workers share the same global model state. At each step, workers compute gradients locally and then aggregate them before taking an optimization step. When training synchronously, workers update their parameters in lockstep. This ensures that all workers hold a consensus on the same model and that gradients are averaged over all workers before being applied to the model. This approach is easy to implement and allows for good convergence properties, and correspondingly is the prevalent optimization method.

Although state-of-the-art models use synchronous optimization for training, synchronous methods scale poorly, as stragglers and communication overhead might severely deteriorate system utilization. Many resources are invested in alleviating these issues in large-scale training systems since even minor improvements can be worth hundreds of thousands of dollars. These issues are exacerbated as the required scale grows, even in homogeneous high-performance computing clusters (Petrini et al., 2003; Hoefler et al., 2010). In this paper, we are interested in cases where significant computing variance between the workers exists. This includes (but is not limited to) straggling workers.

For instance, certain learning tasks entail heterogeneity in the required computation of data, such as varying sentence lengths in language processing, or different image sizes and frame numbers in computer vision. In addition, recent state-of-the-art models use all three parallelism paradigms (data (DP), tensor (TP), and pipeline (PP) parallelism), thus each data parallel node is a set of processing units (accelerators) communicating between them (via TP and PP) to calculate the model gradients collectively. This could potentially intensify compute variance between data parallel workers. Moreover, sub-optimal hardware systems can also lead to straggling workers. Although some sources of heterogeneity can be mitigated by non-trivial engineering work, they still persist as challenges that can introduce compute variance and substantial performance drops. This is further discussed in appendix A.

In this paper, we suggest a simple, yet effective method called _DropCompute_ to improve the robustness and scalability of synchronous optimization in the face of compute variance. We model the compute time as a random variable and show that under reasonable assumptions, a tail of straggling workers slows down the system at a rate that is not proportional to the contributed compute by these straggling workers. We harness the gradient accumulation method widely used in Large Language Models (LLMs) (Ott et al., 2018; Liu et al., 2019) to implement the method in a few lines of code on a relevant large-scale learning task.

The contributions of our work include:

* _DropCompute_: a novel, decentralized method to better handle heterogeneity or stragglers without additional hyper-parameters. _DropCompute_ is hardware and framework agnostic, runs on top of existing optimizers, and can also be combined with other methods that improve other aspects of robustness such as communication overhead.
* A theoretical convergence proof of SGD with stochastic batch size, as in _DropCompute_.
* A theoretical runtime analysis on standard synchronous training and the proposed method. We find an approximation of the expected speedup using _DropCompute_, and show this speedup goes to infinity as the number of workers grows.
* An empirical evaluation of the proposed method on a relevant large scale task, using up to 200 accelerators connected with high bandwidth communication. For example, when using _DropCompute_ before and after system optimizations (for both software and hardware) we show accelerations of 18% and 5%, respectively.

Figure 1: _DropCompute improves robustness and scalability of synchronous training._ A scale graph, showcasing the proposed method runtime performance of synchronous training of a 1.5 billion parameter model with additive noise, simulating compute variance. The baseline represents existing synchronous training and the dashed black line is linear scaling. (left) Real measurements of up to 200 workers. (right) An extrapolation to 2048 workers using a theoretical estimation, also proposed in the paper. More details are provided in section 5.2.

Related Work

The challenge of training deep neural networks on a large scale has been extensively explored. With rapidly growing models and data sizes, numerous works tackled the weaknesses in synchronous DNN training on a large scale and suggested methods to alleviate these weaknesses.

**Redundancy methods.** This line of work addresses the straggling worker problem using a redundancy mechanism. Redundant workers or redundant data are used such that straggling workers will not slow down the entire system (Chen et al., 2016; Bitar et al., 2020). These methods provide better robustness to synchronous training, even in the event of a complete failure of a subset of the workers or considerable communication slowdown. However, the robustness is limited by the redundancy factor, and more generally, more compute resources are required and full utilization cannot be achieved. In addition, some coordination method is required to keep the training synchronous, i.e., keeping consensus between the model replicas of the workers. In particular, Chen et al. (2016); Bitar et al. (2020) use a centralized approach of a parameter server to determine which workers are left out at each iteration. Modern large-scale systems use decentralized variants of _All-Reduce_(von Luxburg et al., 2009; Patarasuk & Yuan, 2009), so it is not trivial to determine which workers should be considered at each step, given that each worker can see a different subset of straggling workers. Moreover, combining redundancy with communication primitive collectives (e.g., _All-Reduce_) requires adaptation to existing underlying frameworks (Sanders et al., 2019).

**Asynchronous optimization.** Another approach is introducing asynchrony to the optimization. Asynchronous training is inherently more scalable than synchronous training by being robust to all kinds of workers and communication faults. This includes periodic synchronization by exchanging parameters every \(\tau\) optimization steps (Stich, 2019; Lin et al., 2020; Wang & Joshi, 2021; Zhang et al., 2015; Wang et al., 2020; Li et al., 2020, 2020, 2020), approximate distributed averaging where each worker communicates with a subset of workers each step (Jiang et al., 2017; Lian et al., 2017; Assran et al., 2019; Yang et al., 2020), and many more. These works provide better scale-up properties and improve time performance. The main drawback is asynchronous optimization itself. In practice, the convergence is less stable, and more optimization steps are needed to generalize as well as synchronous training. In addition, hyperparameters should be chosen more precisely to guarantee convergence and generalization properties (Giladi et al., 2019; Mitliagkas et al., 2016). Due to these issues, asynchronous methods are less commonly used on a large scale.

**Sparse and compressed communication.** Alternatively, several works addressed only the communication overhead. A common technique is to reduce the amount of data exchanged by the workers at each step. This can be done by gradient pruning (Xu et al., 2021), gradient compression (Seide et al., 2014; Chen et al., 2020; Tang et al., 2021) or low-rank approximation (Vogels et al., 2019). These works reduce the communication overhead in a deterministic form while ignoring any compute variance across processing workers. This makes these works orthogonal to ours and potentially can be combined with our method.

## 3 Reducing Compute Variance

This paper proposes a method called _DropCompute_ that improves the robustness of synchronous training by reducing compute variance. First, we describe the vanilla synchronous training framework. Then, we introduce the proposed approach.

### Problem setup

We start with a model formulation for data-parallel synchronous SGD training with \(N\) workers, where each worker holds a replica of the model parameters \(\theta\). In parallelism paradigms that combine TP or PP with DP, \(N\) represents the count of data-parallel workers, not the total number of workers involved in training. Given a dataset \(\mathcal{D}\), we are interested in minimizing the empirical loss

\[\mathcal{L}(\mathcal{D},\theta)=\frac{1}{|\mathcal{D}|}\sum_{z\in\mathcal{D}} \ell(z,\theta),\]

where \(\ell(z,\theta)\) is the loss with respect to data-point \(z\) and the model parameters \(\theta\). At each step, the workers calculate gradients based on a local batch and then aggregate the gradients before taking an 

[MISSING_PAGE_FAIL:4]

### Convergence analysis of _DropCompute_

Using _DropCompute_, the batch size is no longer fixed, but a random variable, which can also potentially depends on the data samples. To the best of our knowledge, this setting is somewhat different from existing convergence guarantees. Therefore, in this section, we provide convergence guarantees for _DropCompute_ by analyzing the convergence of SGD with stochastic batch size.

**Assumption 4.1**.: _Following the notations in section 3.1, consider a possibly non-convex smooth loss function \(\mathcal{L}(\mathcal{D},\theta)\), with a global minimum \(\theta^{*}\in\mathbb{R}^{d}\), and the following (commonly used) assumptions_

1. \(\mathbf{L}\)**-smooth**_: All functions_ \(\mathcal{L}(\cdot,\theta)\) _are with L-Lipschitzian gradients._
2. _Unbiased estimation_: _Each_ \(\nabla\ell(z,\theta_{i}),z\in\mathcal{D}\) _is an unbiased estimate of_ \(\nabla\mathcal{L}(\mathcal{D},\theta_{i})\) _which is the true (full batch) gradient at_ \(\theta_{i}\)2_. Namely,_ \(\forall i:\mathbb{E}[\nabla\ell(z,\theta_{i})|\theta_{i}]=\nabla\mathcal{L}( \mathcal{D},\theta_{i})\,.\)__ Footnote 2: This is easily satisfied when all workers can access all data.
3. _Bounded variance_: _The variance of the stochastic gradient is bounded by a constant_ \(\sigma\)_,_ \[\forall i:\mathbb{E}[\|\nabla\ell(z,\theta_{i})-\nabla\mathcal{L}(\mathcal{D},\theta_{i})\|^{2}|\theta_{i}]\leq\sigma^{2}\,.\]

**Theorem 4.1**.: _Under the above assumption, applying SGD with DropCompute (Algorithm 1), ensures_

\[\mathbb{E}\|\nabla\mathcal{L}(\mathcal{D},\bar{\theta})\|^{2}\leq\frac{2Lb_{ \max}(\mathcal{L}(\mathcal{D},\theta_{1})-\mathcal{L}(\mathcal{D},\theta^{*}) )}{K}+\frac{2\sigma\sqrt{L(\mathcal{L}(\mathcal{D},\theta_{1})-\mathcal{L}( \mathcal{D},\theta^{*}))}}{\sqrt{K}}\,, \tag{2}\]

_where \(b_{\max}\) is the maximal total batch size, \(K\) is the total number of samples that are used throughout the training, \(\theta_{1}\) is the initial model, and \(\bar{\theta}\) is a random sample of \(\theta_{i}\) from the trajectory obtained by Algorithm 1, where \(i\) is selected with probability proportional to the total batch size at iteration \(i\). The expectation is with respect to the randomization introduced due to sampling from \(\mathcal{D}\) throughout the optimization process and with respect to choosing \(\bar{\theta}\)._

The bound in Theorem 4.1 is similar to existing fixed batch-size guarantees (Dekel et al., 2012). The second term in the bound does not degrade with the batch sizes, and behaves like \(O(1/\sqrt{K})\), while the first term behaves like \(O(b_{\max}/K)\). This implies that as long as \(b_{\max}\leq O(\sqrt{K})\), the second term is dominant and we are in the regime of linear speedup. This shows we can attain a linear speedup despite using changing batch sizes, as long as the maximal batch size is bounded.

Similarly, in Theorem D.1 in appendix D.1 we show that the loss itself converges, in the convex case along with a proof. The proof of Theorem 4.1 is provided in appendix D.2. Lastly, we discuss the impact of the stochastic batch size on generalization in appendix D.4.

### Iteration time in standard synchronous distributed training

We start with finding a closed-form expression for the cumulative distribution function (CDF) of the iteration time, denoted as \(T\), defined as

\[T=\max(T_{1},T_{2},...,T_{N})\,,\]where \(T_{n}\) represents the time taken by worker \(n\) to compute its local batch, which follows some cumulative probability function \(F_{T_{n}}(x)=\mathbb{P}(T_{n}<x)\), which we assume is independent for each worker. Let \(F_{T}(x)\) represent the cumulative distribution function and \(f_{T}(x)\) represent the probability density function of the maximum iteration time \(T\). The relation between \(F_{T}(x)\) and \(F_{T_{n}}(x)\) is

\[F_{T}(x)=\mathbb{P}\left(\max\left(T_{1},\ldots,T_{N}\right)\leq x\right)=\prod _{n=1}^{N}F_{T_{n}}(x)\,.\]

Differentiating with respect to \(x\) and applying the chain rule gives:

\[f_{T}(x)=\frac{dF_{T}}{dx}(x)=\sum_{n=1}^{N}f_{T_{n}}(x)\prod_{n^{\prime}\neq n }^{N}F_{T_{n^{\prime}}}(x)\,.\]

In the special case where all of the workers' iteration time distributions are identically and independently distributed (i.i.d.), this reduces to the well-known formula:

\[f_{T}(x)=N\cdot f_{T_{n}}(x)\cdot F_{T_{n}}(x)^{N-1} \tag{3}\]

If the iteration time of each worker is distributed normally (\(\sim\mathcal{N}(\mu,\sigma^{2})\)), the expected value of \(T\) can be approximated as shown by Bailey et al. (2014):

\[\mathbb{E}(T)\approx\sigma\cdot\left((1-\gamma)\cdot\Phi^{-1}\left(1-\frac{1} {N}\right)+\gamma\cdot\Phi^{-1}\left(1-\frac{1}{e\cdot N}\right)\right)+\mu \tag{4}\]

where \(\Phi\) is the CDF of the standard normal distribution, and \(\gamma\) is the euler-mascheroni constant. Asymptotically the total iteration time is \(\mathbb{E}[T]=\Theta(\sqrt{\log N})\). When the number of micro-batches \(M\gg 1\), we can make a similar approximation under Central Limit Theorem (CLT) conditions. More details are in appendix C.2.

### Iteration time and number of micro-batches with _DropCompute_

When using _DropCompute_ with a constant threshold \(\tau\), each worker is preempted at \(\tilde{T}_{n}=\min\left\{\tau,T_{n}\right\}\) and joins the _AllReduce_. Therefore, the total iteration time with _DropCompute_ is

\[\tilde{T}+T^{c}=\min\left\{\tau,T\right\}+T^{c}\,,\]

where \(T^{c}\) is a serial latency present in each iteration, which includes the _AllReduce_ step. This upper limit serves to clip extreme values of \(T_{n}\), effectively constraining the range of potential outcomes for \(\tilde{T}\). As a result, the compute time variability decreases, leading to a narrower distribution and enhanced compute efficiency. These effects are illustrated in Figure 2.

As a consequence of preempting each worker at \(\tilde{T}_{n}\), the number of micro-batches computed in each step varies. Denote as \(t_{n}^{(m)}\) the compute latency of a single micro-batch \(m\) for worker \(n\), and \(T_{n}^{(m)}=\sum_{j=1}^{m}t_{n}^{(j)}\). We can define the average number of micro-batches computed by each worker before reaching threshold \(\tau\) as

\[\tilde{M}=\frac{1}{N}\sum_{n=1}^{N}\sum_{m=1}^{M}\left\{\begin{array}{ll}1, &\text{if }T_{n}^{(m)}<\tau\\ 0,&\text{otherwise}\end{array}\right\}\,.\]

Under CLT conditions, the expected value for \(\tilde{M}\) can be approximated in a closed form:

\[\mathbb{E}[\tilde{M}]\approx\sum_{m=1}^{M}\Phi\left(\frac{\tau-m\cdot\mu}{ \sqrt{m\cdot\sigma^{2}}}\right) \tag{5}\]

where \(\mu,\sigma^{2}\) are the mean and variance for a single micro-batch \(t_{n}^{(m)}\) compute latency, and \(\Phi\) is the CDF of the standard normal distribution. This approximation closely fits the real value of \(\tilde{M}\) and can be used to analyze the expected gain from _DropCompute_. More details in appendix C.2.

### Choosing the threshold

The throughput of the system can be seen as the number of micro-batches computed per second. For \(N\) workers, this can be written as \(NM/(T+T^{c})\). To evaluate the effectiveness of _DropCompute_, we consider the difference in throughput between the baseline and when using _DropCompute_. Doing so, we can define the effective speedup for \(\tau\) as:

\[S_{\text{eff}}(\tau)=\frac{\text{DropCompute Throughput}}{\text{Baseline Throughput}}=\frac{N\tilde{M}/(\min\left\{\tau,T\right\}+T^{c})}{NM/(T+T^{c})}= \frac{\tilde{M}(T+T^{c})}{M(\min\left\{\tau,T\right\}+T^{c})} \tag{6}\]

Given the statistical characteristics of the training setting, it is possible to estimate analytically the expected value of the effective speedup \(\mathbb{E}[S_{\text{eff}}(\tau)]\) by using Equations 5 and 4. Moreover, when plugging in the asymptotic form of \(\mathbb{E}[T]\), we find the expected speedup increases to infinity with \(N\)

\[\mathbb{E}[T]=\Theta(\sqrt{\log N})\ \ \Rightarrow\ \ \mathbb{E}[S_{\text{eff}}( \tau)](N)\underset{N\rightarrow\infty}{\longrightarrow}\infty\]

As shown in figure 3b, Equation 4 is less accurate when samples deviate from a normal distribution.

To find the optimal compute threshold, we synchronize the empirical distribution of micro-batch compute latency between all workers after a few iterations. Given this distribution, we find \(T\) and \(\tilde{M}\), and search in a decentralized way for the threshold \(\tau^{*}\) that maximizes the effective speedup \(S_{\text{eff}}(\tau)\)

Figure 3: **Statistical characteristics of the micro-batch computation latency \(t_{n}^{(m)}\) can provide a reliable estimate of the effective speedup \(S_{\text{eff}}\). The graphs depict \(S_{\text{eff}}(\tau)\) based on Equation 6. Calculations rely on samples of \(t_{n}^{(m)}\) to calculate \(\tilde{M}\) and \(T\), before plugging them into Equation 6. In the ‘simulation’ curves, we directly use the samples to calculate \(\tilde{M}\) and \(T\). For the ‘analytical’ curves, only the mean and variance of \(t_{n}^{(m)}\) are used to approximate \(\tilde{M}\) and \(T\) using Equations 5 and 4, respectively. For ‘analytical given \(\mathbb{E}[T]\)’, \(\tilde{M}\) is approximated using Equations 5, but \(\mathbb{E}[T]\) is calculated directly from the samples. More details in appendix C.2. Panels: (a) The \(t_{n}^{(m)}\) samples follow a normal distribution. (b) samples are taken from BERT1.5B pre-training with simulated delay as described in Section 5.2. (c) **The optimal compute threshold can be found automatically.** The effective speedup, micro-batch completion rate, and step speedup as a function of \(\tau\) in a simulated delay environment.

Figure 2: **Reduction in variance and mean iteration time using _DropCompute_.** Iteration time distribution of 200 workers using _DropCompute_ on BERT 1.5B. (left) Step time \(T_{n}\) distribution of all workers, without _DropCompute_. (right) Maximum iteration time \(T\), across all workers, using _DropCompute_, with different drop rates. The dashed ‘simulation’ distribution is generated by drawing \(T_{n}\) randomly from an independent normal distribution separately for each worker, using the empiric mean and variance of that worker.

defined in Equation 6. Overall, the cost of synchronizing the empirical distribution and finding \(\tau^{*}\) is negligible, compared to a full training session, because it happens only once in a training session. Lowering the threshold leads to reduced compute time but higher compute drop rates. Figure 2(c) highlights this trade-off and the optimal \(\tau^{*}\) is marked.

### Compensating for dropped samples

The effective speedup metric \(S_{\text{eff}}\), accounts for dropped samples by treating them as a source of slowdown in direct proportion to the drop rate. This consideration enables us to execute additional computations to achieve the theoretical speedup. The extent of extra time spent on redundant calculations can be as much as \(R=(M/\bar{M}-1)\) times the computational effort required when not applying _DropCompute_.

For instance, when 10% of the samples are dropped, we can expect to perform approximately 11% more calculations. Achieving this can be approached in several ways. One straightforward compensation method for LLM training involves adding an extra \(R\cdot I_{\text{base}}\) steps to the training process, where \(I_{\text{base}}\) represents the number of training steps conducted without using _DropCompute_. In practice, achieving the original accuracy often requires even fewer additional steps, as illustrated in Figure 5, resulting in an even higher effective speedup.

Another method of compensating for the dropped samples is to increase the maximal batch size. When increasing the batch by \(R\) and dropping in average \(1-\bar{M}/M\), we keep the average batch size the same as without using _DropCompute_, hence compensating for the lost samples. A third method, orthogonal to the first two, is resampling dropped samples before starting a new epoch to diversify the overall samples seen by the model. These approaches are rigorously tested and compared in Table 0(b)

## 5 Experiments

To be useful, _DropCompute_ must possess two properties. First, it should not compromise the accuracy of the trained model. This property is put to test in section 5.1 where we fully train BERT-Large and ResNet-50 (Devlin et al., 2018; He et al., 2015), each on a different task, with different drop rates to compare accuracy. Second, _DropCompute_ should maintain a high level of runtime performance, especially when compute variance or straggling workers exist and vanilla synchronous training time deteriorates. Section 5.2 tests runtime performance of _DropCompute_ by training a 1.5 billion parameter language model, BERT1.5B (Devlin et al., 2018) with additive noise to the compute time of each worker.

**Experimental setup.** The analysis of all BERT models is performed on the same dataset as Devlin et al. (2018), which is a concatenation of Wikipedia and BooksCorpus with 2.5B and 800M words respectively. The finetuning of the pretrained models is performed on SQuAD-v1 (Rajpurkar et al., 2016). We verify the generality of _DropCompute_ by additional evaluation of a ResNet-50 model for image classification on ImageNet (Deng et al., 2009). The experiments depicted in section 5.2 and section 5.1 are executed on Habana Gaudi-1 and Gaudi-2 accelerators, respectively, with high performance network (Habana, 2020).

### Generalization performance

The sole difference in the optimization when _DropCompute_ is applied is that the batch size is not deterministic, but stochastic, as explained in section 3.2. To complement theorem 4.1, we examine the generalization performance achieved with a stochastic batch size on two popular tasks.

**Image classification.** To evaluate the generality of stochastic batch size and _DropCompute_ in particular, we evaluate the Top-1 accuracy of a ResNet-50 model on the Imagenet dataset using our method. Since it is not common to use gradient accumulation in large scale training of this task, we simulate the drops such that each worker randomly drops its local batch, so the total batch size is stochastic. This simulated environment enables us to examine the extent of drop rate we can use without compromising accuracy. Figure 10 in appendix B.2.2 shows that up to \(10\%\) drop rate, which is more than what _DropCompute_ operates on, there is a negligible deterioration in accuracy.

**Large language model.** Training LLMs is resource intensive, typically using large batch sizes, which makes _DropCompute_ appealing. We evaluate _DropCompute_ method on this task by fully pretraining BERT-Large model several times, each with a different drop rate. We follow the optimization regime described in You et al. (2019) with a batch size of 64K for phase-1 and 32K for phase-2 (more details are provided in appendix B.2). Each of the pretrained models is fine-tuned on the SQuAD task 3 times with different initializations. Fine-tuning is performed without drops, as it is not a large scale resource consuming task. Table 1(a) shows the average accuracy (\(\pm\) standard deviation) obtained for each drop rate. As shown, _DropCompute_ at drop rates of up to \(10\%\) have negligible accuracy difference. Higher values measured up to \(20\%\) of dropped gradients provide acceleration with a small yet discernible hit on accuracy. We note that these results are for a fixed budget of steps. In the presence of compute variance, the effective speedup indicates that additional steps can be executed while still maintaining competitive runtime performance. This notion is demonstrated in section 5.2.

### Runtime performance

The main purpose of our proposed method is to maintain runtime performance when compute variance is present. We examine this by measuring the speedup of _DropCompute_ over standard synchronous training in several settings. First, we measure the potential speedup for different drop rates and training settings by post analysis of synchronous training without drops. In addition, we introduce compute variance by training with additive noise, and measure actual speedups using _DropCompute_. The experiments in this section are performed on BERT1.5B. Details are provided in appendix B.1.

**Training with different number of workers and micro-batches.** We evaluate the potential speedup of _DropCompute_ on several training settings with natural heterogeneity and no drops. For each setting, we post analyze what would have been the speedup for different drop rates. As can be seen in Figure 4, _DropCompute_ exhibits increasing benefits with a growing number of workers and compute requirements. However, there are diminishing returns in terms of speedup with more accumulations. This could possibly be explained by the amortization time of a large number of micro-batches.

**Simulated delay environment.** Although _DropCompute_ may have value when the workers' compute latency variance is low, its significance becomes crucial when the workers' compute latency exhibits high variability. To evaluate our method, we introduce a delay environment where random latency is added to each micro-batch computation. This additive noise follows a bounded log-normal distribution. Detailed information and motivation regarding the additive noise are in appendix B.1. The experiments are executed with 12 gradient accumulations and a local batch size of 192. In Figure 1, the negative impact of compute variance on scalability is demonstrated and mitigated

\begin{table}

\end{table}
Table 1: **Maintaining the accuracy of BERT-Large pretraining. Fine-tuning results on SQuAD v1.1, where the F1 score is obtained by the pretrained model. (a) The effect of different drop rates during pretraining on the final accuracy, without compensating for the dropped samples. (b) When 10% drop rate is used during pretraining, with different methods of compensating for the dropped samples.**

Figure 4: _DropCompute exhibits increasing benefit on a large scale. Effective speedup versus drop rate with (left) 32 accumulations and varying workers, and (right) 112 workers and varying number of accumulations._

using _DropCompute_. The results in Figure 1 also correspond to section 4 and Equation 11, where a theoretical extrapolation follows the same trend line. When utilizing _DropCompute_ in this setup, achieving the same training loss as the baseline might requires additional training steps, however, it leads to a notable reduction in overall training time. Figure 5 demonstrates it in a training session with 64 workers, where approximately 3% more steps is needed to reach the same loss, in 13% less time.

## 6 Discussion

**Summary.** Efficient scalable systems are a key component to enable the continued development of deep learning models. To this day, state-of-the-art models rely on synchronous distributed optimization. The challenge to maintain synchronous training as an efficient solution grows larger with the quickly growing model sizes and data. Therefore, improving the robustness and scalability of distributed synchronous training is an important endeavor. This paper tackles the challenge of maintaining synchronous training scalable in the face of compute variance. We propose _DropCompute_ to improve the robustness of synchronous training. Workers drop their remaining compute when they reach a compute threshold, determined by exchanging and analyzing the compute latency distribution. We find that for a small percentage of dropped data, a much larger percentage of time can be saved, depending on the compute latency distribution of the workers. In addition, we provide theoretical convergence guarantees and runtime predictions. We further discuss the motivation behind _DropCompute_ and how it effectively solves the problem in appendix A.

**Limitations.** While _DropCompute_ is simple and straightforward, it deals with system efficiency, and as such, the user-level implementation provided is not optimal. Mainly, the provided implementation is limited by using many gradient accumulations and integrating compute timeout in between them. However, we believe that this is not a major concern since having multiple gradient accumulations is a common practice in training LLM on a large scale and is used in state-of-the-art training configurations (Smith et al., 2022; Nvidia, 2023). In addition, _DropCompute_ addresses variance that originates from the compute stage of the training iteration and does not solve the potential issue of network variance during the all-reduce stage.

**Future directions.**_DropCompute_ is described and analyzed in this paper as a method built on top of synchronous training. However, this method can be integrated with other possibly asynchronous methods such as periodic synchronization. In appendix B.3, we implement _DropCompute_ on top of Local-SGD (Lin et al., 2020) and show that _DropCompute_ can also improve the robustness of Local-SGD to stragglers. A different extension for _DropCompute_ is to apply it during the model backward calculation and save the partial gradients that were already calculated. This would generalize _DropCompute_ for workloads that do not utilize gradient accumulations. However, it will require further study as it differs from the stochastic batch-size setting where the entire data sample is either saved or dropped.

## Acknowledgments

We thank Itay Hubara for technical advising and valuable comments on the manuscript. The research of DS was Funded by the European Union (ERC, A-B-C-Deep, 101039436). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be held responsible for them. DS also acknowledges the support of Schmidt Career Advancement Chair in AI.

Figure 5: _DropCompute_ improves training time for workers with compute variance. Train loss curve of BERT1.5B pretraining, in a simulated delay environment. (left) Horizontal axis in training steps, and (right) horizontal axis in training time.

## References

* Assran et al. (2019) Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat. Stochastic gradient push for distributed deep learning. In _International Conference on Machine Learning_, pp. 344-353. PMLR, 2019.
* Bailey et al. (2014) David H Bailey, Jonathan M Borwein, Marcos Lopez de Prado, and Qiji Jim Zhu. Pseudomathematics and financial charlatanism: The effects of backtest over fitting on out-of-sample performance. _Notices of the AMS_, 61(5):458-471, 2014.
* Ben-Nun and Hoefler (2019) Tal Ben-Nun and Torsten Hoefler. Demystifying parallel and distributed deep learning: An in-depth concurrency analysis. _ACM Comput. Surv._, 52(4), aug 2019. ISSN 0360-0300. doi: 10.1145/3320060. URL [https://doi.org/10.1145/3320060](https://doi.org/10.1145/3320060).
* Bitar et al. (2020) Rawad Bitar, Mary Wootters, and Salim El Rouayheb. Stochastic gradient coding for straggler mitigation in distributed learning. _IEEE Journal on Selected Areas in Information Theory_, 1(1):277-291, 2020.
* Chen et al. (2020) Chia-Yu Chen, Jiamin Ni, Songtao Lu, Xiaodong Cui, Pin-Yu Chen, Xiao Sun, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Viji Srinivasan, Wei Zhang, et al. Scalecom: Scalable sparsified gradient compression for communication-efficient distributed training. _Advances in Neural Information Processing Systems_, 33:13551-13563, 2020.
* Chen et al. (2016) Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. Revisiting distributed synchronous sgd. _arXiv preprint arXiv:1604.00981_, 2016.
* Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* Dehghani et al. (2023) Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch n'pack: Navit, a vision transformer for any aspect ratio and resolution. _arXiv preprint arXiv:2307.06304_, 2023.
* Dekel et al. (2012) Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction using mini-batches. _Journal of Machine Learning Research_, 13(1), 2012.
* Deng et al. (2009) J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In _CVPR09_, 2009.
* Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Geiping et al. (2022) Jonas Geiping, Micah Goldblum, Phil Pope, Michael Moeller, and Tom Goldstein. Stochastic training is not necessary for generalization. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=ZBESeIUBBk](https://openreview.net/forum?id=ZBESeIUBBk).
* Giladi et al. (2019) Niv Giladi, Mor Shpigel Nacson, Elad Hoffer, and Daniel Soudry. At stability's edge: How to adjust hyperparameters to preserve minima selection in asynchronous training of neural networks? _arXiv preprint arXiv:1909.12340_, 2019.
* Goyal et al. (2017) Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. _arXiv preprint arXiv:1706.02677_, 2017.
* Habana et al. (2020) Habana. Habana gaudi training whitepaper, June 2020. URL [https://habana.ai/wp-content/uploads/pdf/2020/Habana%20GAUDI%20Training%20Whitepaper%20v1.2.pdf](https://habana.ai/wp-content/uploads/pdf/2020/Habana%20GAUDI%20Training%20Whitepaper%20v1.2.pdf).
* Habana (2023) Habana. Pre-training the bert 1.5b model with deepspeed, January 2023. URL [https://developer.habana.ai/blog/pre-training-the-bert-1-5b-model-with-deepspeed/](https://developer.habana.ai/blog/pre-training-the-bert-1-5b-model-with-deepspeed/).
* Hazan et al. (2016) Elad Hazan et al. Introduction to online convex optimization. _Foundations and Trends(r) in Optimization_, 2(3-4):157-325, 2016.
* Hazan et al. (2017)Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015.
* Hoefler et al. (2010) Torsten Hoefler, Timo Schneider, and Andrew Lumsdaine. Characterizing the influence of system noise on large-scale applications by simulation. In _SC'10: Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis_, pp. 1-11. IEEE, 2010.
* Hoffer et al. (2017) Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. _Advances in neural information processing systems_, 30, 2017.
* Hooker (2020) Sara Hooker. The hardware lottery. _CoRR_, abs/2009.06489, 2020. URL [https://arxiv.org/abs/2009.06489](https://arxiv.org/abs/2009.06489).
* Jiang et al. (2017) Zhanhong Jiang, Aditya Balu, Chinmay Hegde, and Soumik Sarkar. Collaborative deep learning in fixed topology networks. _Advances in Neural Information Processing Systems_, 30, 2017.
* Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361).
* Kosec et al. (2021) Matej Kosec, Sheng Fu, and Mario Michael Krell. Packing: Towards 2x NLP BERT acceleration. _CoRR_, abs/2107.02027, 2021. URL [https://arxiv.org/abs/2107.02027](https://arxiv.org/abs/2107.02027).
* Levin and Peres (2017) David A Levin and Yuval Peres. _Markov chains and mixing times_, volume 107. American Mathematical Soc., 2017.
* Levy (2017) Kfir Levy. Online to offline conversions, universality and adaptive minibatch sizes. _Advances in Neural Information Processing Systems_, 30, 2017.
* Li et al. (2020a) Shigang Li, Tal Ben-Nun, Giorgi Nadiradze, Salvatore Di Girolamo, Nikoli Dryden, Dan Alistarh, and Torsten Hoefler. Breaking (global) barriers in parallel stochastic optimization with wait-avoiding group averaging. _IEEE Transactions on Parallel and Distributed Systems_, 32(7):1725-1739, 2020a.
* Li et al. (2020b) Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. _Proceedings of Machine learning and systems_, 2:429-450, 2020b.
* Lian et al. (2017) Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. _Advances in Neural Information Processing Systems_, 30, 2017.
* Lin et al. (2022) Huangxing Lin, Weihong Zeng, Yihong Zhuang, Xinghao Ding, Yue Huang, and John Paisley. Learning rate dropout. _IEEE Transactions on Neural Networks and Learning Systems_, pp. 1-11, 2022. doi: 10.1109/TNNLS.2022.3155181.
* Lin et al. (2020) Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, and Martin Jaggi. Don't use large mini-batches, use local sgd. In _International Conference on Learning Representations_, 2020.
* Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019. URL [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692).
* Mattson et al. (2019) Peter Mattson, Christine Cheng, Cody Coleman, Greg Diamos, Paulius Micikevicius, David A. Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, David Brooks, Dehao Chen, Debojyoti Dutta, Udit Gupta, Kim M. Hazelwood, Andrew Hock, Xinyuan Huang, Bill Jia, Daniel Kang, David Kanter, Naveen Kumar, Jeffery Liao, Guokai Ma, Deepak Narayanan, Tayo Oguntebi, Gennady Pekhimenko, Lillian Pentecost, Vijay Janapa Reddi, Taylor Robie, Tom St. John, Carole-Jean Wu, Lingjie Xu, Cliff Young, and Matei Zaharia. Mlperf training benchmark. _CoRR_, abs/1910.01500, 2019. URL [http://arxiv.org/abs/1910.01500](http://arxiv.org/abs/1910.01500).
* Mitliagkas et al. (2016) Ioannis Mitliagkas, Ce Zhang, Stefan Hadjis, and Christopher Re. Asynchrony begets momentum, with an application to deep learning. In _2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pp. 997-1004. IEEE, 2016.
* Mitliagkas et al. (2017)Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. Efficient large-scale language model training on gpu clusters using megatron-lm. In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, SC '21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450384421. doi: 10.1145/3458817.3476209. URL [https://doi.org/10.1145/3458817.3476209](https://doi.org/10.1145/3458817.3476209).
* Neelakantan et al. (2015) Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks, 2015. URL [https://arxiv.org/abs/1511.06807](https://arxiv.org/abs/1511.06807).
* Nvidia (2023) Nvidia. mlcommons training results v3.0 gpt3. [https://github.com/mlcommons/training_results_v3.0/tree/main/NVIDIA/benchmarks/gpt3/implementations/pytorch](https://github.com/mlcommons/training_results_v3.0/tree/main/NVIDIA/benchmarks/gpt3/implementations/pytorch), 2023.
* Ott et al. (2018) Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In _Proceedings of the Third Conference on Machine Translation: Research Papers_, pp. 1-9, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6301. URL [https://aclanthology.org/W18-6301](https://aclanthology.org/W18-6301).
* Patarasuk and Yuan (2009) Pitch Patarasuk and Xin Yuan. Bandwidth optimal all-reduce algorithms for clusters of workstations. _Journal of Parallel and Distributed Computing_, 69(2):117-124, 2009.
* Petrini et al. (2003) Fabrizio Petrini, Darren J Kerbyson, and Scott Pakin. The case of the missing supercomputer performance: Achieving optimal performance on the 8,192 processors of asci q. In _Proceedings of the 2003 ACM/IEEE conference on Supercomputing_, pp. 55, 2003.
* Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020a. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).
* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020b.
* Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_, pp. 1-16. IEEE, 2020.
* Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pp. 2383-2392, Austin, Texas, November 2016. Association for Computational Linguistics.
* Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pp. 3505-3506, 2020.
* Sanders et al. (2019) Peter Sanders, Kurt Mehlhorn, Martin Dietzfelbinger, and Roman Dementiev. _Sequential and Parallel Algorithms and Data Structures_. Springer, 2019.
* Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.
* Seide et al. (2014) Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and application to data-parallel distributed training of speech dnns. In _Interspeech 2014_, September 2014.
* Sun et al. (2017)Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. _arXiv preprint arXiv:2201.11990_, 2022.
* Sobkowicz et al. (2013) Pawel Sobkowicz, Mike Thelwall, Kevan Buckley, Georgios Paltoglou, and Antoni Sobkowicz. Lognormal distributions of user post lengths in internet discussions-a consequence of the weberfechner law? _EPJ Data Science_, 2(1):1-20, 2013.
* Stich (2019) Sebastian U. Stich. Local SGD converges fast and communicates little. In _International Conference on Learning Representations_, 2019.
* Tan & Le (2021) Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In _International conference on machine learning_, pp. 10096-10106. PMLR, 2021.
* Tang et al. (2021) Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He. 1-bit adam: Communication efficient large-scale training with adam's convergence speed. In Marina Meila and Tong Zhang (eds.), _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pp. 10118-10129. PMLR, 18-24 Jul 2021.
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Vogels et al. (2019) Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. Powersgd: Practical low-rank gradient compression for distributed optimization. _Advances in Neural Information Processing Systems_, 32, 2019.
* van Luxburg et al. (2018) U von Luxburg, S Bengio, HM Wallach, R Fergus, SVN Vishwanathan, and R Garnett. [https://github.com/baidu-research/baidu-allreduce](https://github.com/baidu-research/baidu-allreduce).
* Wang & Joshi (2021) Jianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of local-update sgd algorithms. _Journal of Machine Learning Research_, 22, 2021.
* Wang et al. (2020) Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat. Slowmo: Improving communication-efficient distributed sgd with slow momentum. In _International Conference on Learning Representations_, 2020.
* Xu et al. (2021) Hang Xu, Kelly Kostopoulou, Aritra Dutta, Xin Li, Alexandros Ntoulas, and Panos Kalnis. Deepreduce: A sparse-tensor communication framework for federated deep learning. _Advances in Neural Information Processing Systems_, 34:21150-21163, 2021.
* Yang et al. (2020) Donglin Yang, Wei Rang, and Dazhao Cheng. Mitigating stragglers in the decentralized training on heterogeneous clusters. In _Proceedings of the 21st International Middleware Conference_, pp. 386-399, 2020.
* You et al. (2017) Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. _arXiv preprint arXiv:1708.03888_, 2017.
* You et al. (2019) Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. _arXiv preprint arXiv:1904.00962_, 2019.
* Zhang et al. (2015) Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with elastic averaging sgd. _Advances in neural information processing systems_, 28, 2015.
* Zheng et al. (2020) Shuai Zheng, Haibin Lin, Sheng Zha, and Mu Li. Accelerated large batch optimization of bert pretraining in 54 minutes, 2020.