# WeiPer: OOD Detection using Weight Perturbations of Class Projections

Maximilian Granz

Institute for Computer Science

Free University of Berlin

Arnimallee 7 14195 Berlin

maximilian.granz@fu-berlin.de

&Manuel Heurich

Institute for Computer Science

Free University of Berlin

Arnimallee 7 14195 Berlin

manuel.heurich@fu-berlin.de

Equal contribution.

Tim Landgraf

Institute for Computer Science

Free University of Berlin

Arnimallee 7 14195 Berlin

tim.landgraf@fu-berlin.de

Equal contribution.

###### Abstract

Recent advances in out-of-distribution (OOD) detection on image data show that pre-trained neural network classifiers can separate in-distribution (ID) from OOD data well, leveraging the class-discriminative ability of the model itself. Methods have been proposed that either use logit information directly or that process the model's penultimate layer activations. With "WeiPer", we introduce perturbations of the class projections in the final fully connected layer which creates a richer representation of the input. We show that this simple trick can improve the OOD detection performance of a variety of methods and additionally propose a distance-based method that leverages the properties of the augmented WeiPer space. We achieve state-of-the-art OOD detection results across multiple benchmarks of the OpenOOD framework, especially pronounced in difficult settings in which OOD samples are positioned close to the training set distribution. We support our findings with theoretical motivations and empirical observations, and run extensive ablations to provide insights into why WeiPer works. Our code is available at: [https://github.com/mgranz/weiper](https://github.com/mgranz/weiper).

## 1 Introduction

Out-of-Distribution (OOD) detection has emerged as a pivotal area of machine learning research. It addresses the challenge of recognizing input data that deviates significantly from the distribution seen during training. This capability is critical because machine learning models, particularly deep neural networks, are known to make overconfident and incorrect predictions on such unseen data Hendrycks and Gimpel (2016). The need for OOD detection is driven by practical considerations. In real-world applications, a model frequently encounters data that is not represented in its training set. For instance, in autonomous driving, a system trained in one geographic location might face drastically different road conditions in another. Without robust OOD detection, these models risk making unsafe decisions Amodei et al. (2016).

Over the last few years, the field has made significant steps towards setting up benchmarks and open baseline implementations. Thanks to the efforts of the OpenOOD team Zhang et al. (2023b); Yanget al. (2022), we can evaluate new methods across CIFAR10, CIFAR100 and ImageNet, and compare them against a variety of methods, on the same network checkpoints. To this date, however, there is no single method outperforming the competition on all datasets Tajwar et al. (2021), which indicates a variety of ways in which OOD data differs from the training set. Here, we introduce _WeiPer_, a method that can be applied to any pretrained model, any training loss used, with no limitation on the data modality to separate ID and OOD datapoints. _WeiPer_ creates a representation of the data by projecting the latent representation of the penultimate layer onto a cone of vectors around the class-projections of the final layer's weight matrix. This allows extracting additional structural information on the training distribution compared to using the class projections alone and specifically exploits the fact that the OOD data often extends into the cluster of positive samples of the respective class in a conical shape (see Figure 1). In addition to _WeiPer_, our KL-divergence-based method _WeiPer_+KLD represents a novel OOD detection score that is based on the following observation:

When ignoring the individual dimensions and examining the activation distribution across all dimensions, we observe that ID samples exhibit a similar "fingerprint" distribution. The more feature dimensions there are, the better our estimate of this source distribution becomes. We demonstrate that measuring the discrepancy between the per-sample distribution and the training set's mean distribution in the augmented _WeiPer_ space leads to improved OOD detection accuracy. We evaluate _WeiPer_ on OpenOOD using our proposed KL-divergence-based scoring function (**KLD**), MSP Hendrycks and Gimpel (2016), and ReAct Sun et al. (2021). Additionally, we conduct an ablation study to understand the influence of each component of _WeiPer_ and analyze _WeiPer_'s performance. Our results confirm that the weight perturbations allow _WeiPer_ to outperform the competition on two out of eight benchmarks, demonstrating consistently better performance on near OOD tasks. _WeiPer_ represents a versatile, off-the-shelf method for state-of-the-art post-hoc OOD detection. However, the performance of _WeiPer_ comes at a cost: The larger the _WeiPer_ space, the more memory is required.

Figure 1: Why random perturbations? **Left**: We visualize densities of CIFAR10 (ID, blue) and CIFAR100 (OOD, red) as contour plots along the two logit dimensions spanned by \(\mathbf{w}_{0}\) and \(\mathbf{w}_{1}\), zoomed in on the positive cluster of class zero. The blue axis denotes the vector associated with that class, and one of its perturbations is depicted by the turquoise line. **Right**: When projecting the data onto both vectors, we obtain the densities shown in the _top_ and _bottom_ panel, respectively. The vertical blue lines mark the 5-percentile (highest 5%) of the true ID data (CIFAR10, blue). At this decision boundary, the classifier would produce false positives in the marked dashed red tail area. A single perturbation of the class-associated vector yields already a reduction of the false positive rate (FPR) from \(1.34\)% to \(0.79\)%. Visually, we confirm that OOD data mostly resides close to 0, extending into the positive cluster in a particular conical shape, which is exploited by the cone of _WeiPer_ vectors.

In summary, we present the following **contributions**:

* We discover that OOD detection can be improved by considering linear projections of the penultimate layer that correlate with the final output, i.e., the class representations. We construct these projections by perturbing the weights of the final layer.
* We uncover a fingerprint-like nature of the ID samples in both the penultimate space and our newly found perturbed space, proposing a novel post-hoc detection method that leverages this structure. The activation distributions of the penultimate space and our _WeiPer_ space over the dimensions of each sample are similar for each ID input, yielding distributions in both spaces that we compare to the mean ID distribution using KL divergence.
* We evaluate our findings by testing the proposed methods and two other MSP-based methods on the perturbed class projections using the OpenOOD benchmark, achieving state-of-the-art performance on near OOD tasks.

## 2 Related work

OOD detection.Generally, we can distinguish two types of OOD detection methods - one that requires retraining of the model, including novel loss variants, data augmentations, or even outlier exposure settings. Here, we focus on post-hoc methods that can be added with little effort to any existing pipeline. They can be applied to any pretrained model, irrespective of its architecture, loss objective or data modality. Post-hoc methods can be distinguished further in:

1) **Confidence-based** methods Guo et al. (2017); Hendrycks et al. (2022, 2022); Liu et al. (2023, 2020); Wang et al. (2022) process samples in the model's logit space, i.e. using the network directly to detect ID/OOD data points. A prominent example is the Maximum Softmax Probability (MSP) (Hendrycks and Gimpel, 2016) which simply uses the maximum logit as the main OOD decision metric. Some methods Ahn et al. (2023); Djurisic et al. (2022); Sun et al. (2021) additionally introduce transformations such as cutoffs of the features in the penultimate layer or masks on the weight matrix Sun and Li (2021) to allocate where ID data resides and combine these with confidence metrics. Several recent methods have employed f-divergences to improve OOD detection, focusing on enhancing the boundary definition between ID and OOD samples Darrin et al. (2022); Picot et al. (2022).

2) **Distance-based** methods Bendale and Boult (2015); Lee et al. (2018); Liu et al. (2023, 2021); Ren et al. (2021); Sastry and Oore (2020); Sun et al. (2022); Zhang et al. (2023) define distance measures between the training distribution and an input sample in latent space, i.e. primarily the penultimate layer of the network. Deep Nearest Neighbors Sun et al. (2022) uses the distance to the \(k\)-th closest neighbor in latent space, while MDS Lee et al. (2018) models the data as Gaussian and uses the Mahalonibis-Distance. Models of the data distribution can improve the OOD detection performance, e.g. using histograms to approximate the training density and then define a distance measure on them. A recent work Liu et al. (2023) proposed creating a histogram-based distribution on the product of the penultimate activations and the gradient of a separate KL-loss and then defined a metric on these modified discrete densities.

Both approaches of 1) and 2) are not exclusive. NNGuide Park et al. (2023) combines both confidence and distance measures into a joint score, improving performance in case one of the scores fails.

Random weight perturbations and projections.Weight perturbations, i.e. adding noise values to the weights of a network, have been used for a variety of applications: in sensitivity analyses Cheney et al. (2017); Xiang et al. (2019), for studying robustness against adversarial attacks Rakin et al. (2018); Wu et al. (2020), and as training regularization Khan et al. (2018); Wen et al. (2018). Random projections from the latent space of the neural network have been described in the context of generative modeling Bonneel et al. (2014); Jerome H. Friedman and Schroeder (1984); Kolouri et al. (2016); Liutkus et al. (2019); Nguyen et al. (2021); Paty and Cuturi (2019), e.g. to improve the Wasserstein distance calculation or for robustness. A previous work described random projections from the penultimate layer to detect out-of-distribution samples with a normalizing flow Kuan and Mueller (2022).

## 3 Method

### Preliminaries

We consider a pretrained neural network classifier \(f:\mathcal{X}\rightarrow\mathbb{R}^{C}\) that maps samples \(x\) from an input space \(\mathcal{X}\in\mathbb{R}^{D}\) to a logit vector \(f(\mathbf{x})\in\mathbb{R}^{C}\), by applying a linear projection \(\mathbf{W}_{\text{fc}}\) to the feature representation in the penultimate layer

\[(z_{1},...,z_{K})^{T}=\mathbf{z}=h(\mathbf{x})=\left(h_{1}(\mathbf{x}),...,h_{K}(\mathbf{x}) \right)^{T} \tag{1}\]

such that \(f(\mathbf{x})=\mathbf{W}_{\text{fc}}^{T}\mathbf{z}\), with \(D\), \(K\) and \(C\) representing the dimensionality of the input, the penultimate layer and the output layer, respectively. We define the rows of the final weight layer to be \(\mathbf{w}_{1},...,\mathbf{w}_{C}\).

In the following it is useful to introduce \(Z\) as random vector from which we draw our latent samples. We denote the densities of the latent activations of the training data with \(p_{Z_{\text{train}}}\), of the test data with \(p_{Z_{\text{test}}}\) and those of OOD samples with \(p_{Z_{\text{test}}}\) admitted by the random vectors \(Z_{\text{train}}\), \(Z_{\text{test}}\) and \(Z_{\text{ood}}\), respectively. To ease notation, we will treat \(Z\) and all its subsets, both as sets, e.g. \(Z_{\text{train}}\) is the set of all training activations in the penultimate layer.

An OOD detector is a binary classifier \(O\) that decides if samples are drawn from an ID or OOD distribution by usually only considering samples drawn from \(p_{Z_{\text{test}}}\) or \(p_{Z_{\text{ood}}}\). Commonly, this is achieved by thresholding a scalar score function \(S\).

\[O(\mathbf{x})=\begin{cases}\text{ID}&\text{if }S(\mathbf{x})>\lambda\\ \text{OOD}&\text{otherwise}\end{cases} \tag{2}\]

For MSP, the score function is simply the maximum softmax probability

\[S(\mathbf{x})=\operatorname{MSP}(\mathbf{x})=\max_{i=1,...,C}\frac{e^{f(\mathbf{x})_{i}}}{ \sum_{j=1}^{C}e^{f(\mathbf{x})_{j}}}=:\operatorname{MSP}(f(\mathbf{x})). \tag{3}\]

Note, that for clarity, we define \(\operatorname{MSP}\) also as a function of the logits. Other methods propose metrics on the penultimate layer, e.g. by incorporating distance measures between a given latent activation \(\mathbf{z}\) of a new sample and the distribution of activations \(p_{Z_{\text{train}}}\) of the training set.

### WeiPer: Weight perturbations

A neural network classifier maps the data distribution to the distribution of the logits \(\mathbf{W}_{\text{fc}}Z\). The training objective of the network ensures an optimal separation of classes and lets the model learn to exploit features in \(Z\) specific to the training distribution. OOD samples, hence, often yield lower logit scores. Confidence methods leverage this property, but could potentially be improved by capturing more of the underlying distribution of the penultimate layer. A confidence score measures properties of the logit distribution \(\mathbf{W}_{\text{fc}}Z\). Is there additional information in the penultimate layer of the network, and if so, how can we utilize it?

Applying the weight matrix \(\mathbf{W}_{\text{fc}}\) to the penultimate space can be understood as \(C\) projections of \(Z\) onto the row vectors \(\mathbf{w}\). According the Cramer-Wold theorem (Cramer & Wold (1936)), we can reconstruct the source density \(p_{Z}\) from all one-dimensional linear projections, and Cuesta-Albertos et al. (2007) has shown that a K-dimensional subset of projections suffices (for more details see Appendix A.1.1). The question remains which projections extract the most relevant information?

Drawing vectors \(\mathbf{w}\in W=\mathcal{N}(0,I)\) from a standard normal and projecting onto them often results in similar densities for ID and OOD data, i.e. \(\mathbf{w}^{T}Z_{\text{train}}\approx\mathbf{w}^{T}Z_{\text{ood}}\), deteriorating detection performance (see Table 1, RP). This aligns with Papyan et al. (2020), suggesting limited information in the penultimate layer compared to the logits. We hypothesize that the latent distribution shows relevant structure only along certain dimensions. We applied PCA to the latent activations \(Z\) and inspected the resulting projections. This analysis supports the notion that the informative dimensions lie in the directions of the class projections \(\mathbf{w}_{1},...,\mathbf{w}_{C}\) (see Appendix A.1.3). Hence, we construct projections that correlate with these vectors but at the same time deviate enough to obtain new information.

Definition of WeiPerWe define perturbations \(\mathbf{\eta}\), drawn from a standard normal and add them to \(\mathbf{W}_{\text{fc}}\). To ensure that all perturbed vectors have the same angular deviation from the original weight vector, we normalize the perturbations to be the same length as their corresponding row vector and multiply them by a factor \(\delta\):

\[\tilde{\mathbf{w}}_{i,j}=\mathbf{w}_{j}+\delta\frac{\mathbf{\eta}_{i}\|\mathbf{w}_{j}\|}{\|\mathbf{ \eta}_{i}\|}=:\mathbf{w}_{j}+\tilde{\mathbf{\eta}}_{i},\quad\mathbf{\eta}_{i}\sim\mathcal{ N}(\mathbf{0},\mathbf{I}_{K}) \tag{4}\]

for \(i=1,...,r\), where \(\delta\) represents the length ratio between \(\mathbf{w}_{j}\) and the perturbation \(\mathbf{\eta}_{i}\). For large \(K=\text{dim}(Z)\), \(\mathbf{w}_{j}\) and \(\tilde{\mathbf{\eta}}_{i}\) are almost orthogonal and thus \(\delta\) actually adjusts the angle \(\alpha\approx\arctan(\delta)\) of the perturbed vector bundle. We set \(\delta\) to be constant across all \(j=1,...,K\) and treat both \(\delta\) and \(r\) as hyperparameters. This proceedure is related to the Distributional Sliced Wasserstein distance Nguyen et al. (2021) as they sample projections from a distribution such that the mean angle between the projections is greater than \(\arccos(C)\) for a constant \(C\).The whole set of vectors we define is

\[W=\{\tilde{\mathbf{w}}_{1,1},...,\tilde{\mathbf{w}}_{1,C},...,\tilde{\mathbf{w}}_{r,C}\} \tag{5}\]

We can think of the resulting weight matrix \(\tilde{\mathbf{W}}\) as \(r\) repetitions of the weight matrix \(\mathbf{W}_{\text{fc}}\) on which we add perturbation matrices \(\tilde{\mathbf{H}}_{i}\). The \(j\)-th row \(\tilde{\mathbf{H}}_{i,j}\) corresponds to a perturbation vector \(\tilde{\mathbf{\eta}}_{j}\), normalized to match the respective row \(\mathbf{w}_{j}\).

\[\tilde{\mathbf{W}}:=\begin{bmatrix}\tilde{\mathbf{W}}_{1}\\ \vdots\\ \tilde{\mathbf{W}}_{r}\end{bmatrix}=\begin{bmatrix}\mathbf{W}_{\text{fc}}+\tilde{\mathbf{H }}_{1}\\ \vdots\\ \mathbf{W}_{\text{fc}}+\tilde{\mathbf{H}}_{r}\end{bmatrix}, \tag{6}\]

Since \(\tilde{\mathbf{W}}_{i}Z=\mathbf{W}_{\text{fc}}Z+\mathbf{H}_{i}Z\), we call \(\tilde{\mathbf{W}}Z\) the perturbed logit space. Our weight perturbations method, we call WeiPer, essentially increases the output dimension of a model. Hence, it can be combined with many scoring functions. We demonstrate this with the two following postprocessors.

### Baseline MSP scoring function

If the perturbations do not deviate too much from the class projections \(\mathbf{w}_{j}\), i.e. the row vectors of the final layer, the class cluster will still be separated from the other classes in the new projections and we can apply \(\operatorname{MSP}\) on the perturbed logit space. In fact, we find that class clusters on the perturbed projections can be better distinguished from the OOD cluster than on the original class projection

Figure 2: _WeiPer_ perturbs the weight vectors of \(\mathbf{W}_{\text{fc}}\) by an angle controlled by \(\delta\). For each weight, we construct \(r\) perturbations resulting in \(r\) weight matrices \(\tilde{\mathbf{W}}_{1},...,\tilde{\mathbf{W}}_{r}\). **KLD**: For _WeiPer_+KLD, we treat \(z_{1},...,z_{k}\sim p_{\mathbf{z}}\) and \(w_{1,1}^{T}\mathbf{z},...,w_{r,C}^{T}\mathbf{z}\sim p_{\tilde{W}\mathbf{z}}\) as samples of the same distribution induced by \(z\) and \(\tilde{W}z\), respectively. We approximate the densities with histograms and smooth the result with uniform kernel \(T_{k_{*}}\). Afterwards, we compare the densities \(T_{k_{*}}(q_{\mathbf{z}})\) with the mean distribution over the training samples \(\mathbb{E}_{\mathbf{z}\in Z_{\text{size}}}(q_{\mathbf{z}})\) for \(q_{\mathbf{z}}=p_{\mathbf{z}}\) and \(q_{\mathbf{z}}=p_{\mathbf{W}\mathbf{z}}\), respectively. **MSP:** For a score function \(S\) on the logit space \(\mathbb{R}^{C}\), we define the perturbed score \(S_{\text{WeiPer}}\) as the mean over all the perturbed logit spaces \(\tilde{\mathbf{W}}\mathbf{z}\). Choose \(S=\operatorname{MSP}\) and call the resulting detector \(\operatorname{MSP}_{\text{WeiPer}}\).

defined by \(\mathbf{W}_{\rm fc}\) (see improvements of _WeiPer_+MSP(\(\mathbf{x}\)) over MSP in Table 2). Figure 1 illustrates a visual example. We calculate the MSP on the perturbed logit space as

\[\textit{WeiPer+MSP}(\mathbf{x}):=\mathrm{MSP}_{\textit{WeiPer}}(\mathbf{x}):=\frac{1}{ r}\sum_{i=1}^{r}\mathrm{MSP}(\tilde{\mathbf{W}}_{i}\mathbf{z}) \tag{7}\]

the mean over all the maximum softmax predictions of the perturbed logits. We analyze why \(\mathrm{MSP}_{\textit{WeiPer}}\) could be capable of capturing more of the penultimate layer distribution than MSP in Appendix A.1.2.

### Our KL divergence score function

Following our line of argument motivated by Theorem A.1, it seems natural to choose a density-based score function. When pooling all activations of the penultimate layer, an ID sample's activation distribution exhibits remarkable differences to that of an OOD sample. We observe the following properties:

* The majority of samples exhibit a bimodal distribution of their penultimate activations. An activation either belongs to the mode close to zero, or to the second mode (and rarely takes values in between).
* see the upper left panel in Figure 3.
* The activation distribution is specific to the ID samples, i.e. the activation distribution of OOD samples differs from its distribution of ID samples and thus from the ID prototype.

Concluding on all three points, we make the _assumption_ that all features \(\mathbf{z}=h(\mathbf{x})\) of an ID input \(x\) can be thought of as samples

\[z_{1},...,z_{K}\sim p_{\mathbf{z}}\text{, where }(z_{1},...,z_{K})^{T}=\mathbf{z}, \tag{8}\]

of the same underlying activation distribution \(p_{\mathbf{z}}\). Furthermore, the density of \(p_{\mathbf{z}}\) matches the mean distribution over all ID samples

\[p_{\mathbf{z}}\approx\mathbb{E}_{\mathbf{z}^{\prime}\in Z_{\textit{min}}}[p_{\mathbf{z}^{ \prime}}]. \tag{9}\]

We assume, the same is true for the logits. They naturally separate into a non-class cluster and a class cluster with the ratio \(1:C-1\). Here, we could apply the same procedure, but especially for datasets with a small number of classes we would only get \(C\) samples. This is where the cone of _WeiPer_ vectors creates an advantage: They sit at a fixed angle to a class projection and thus preserve the class structure similarly across each projection onto one vector of the cone (e.g., like Figure 1 right - bottom panel). Analogous to Equation (9), we treat each projection

\[\mathbf{w}_{1,1}^{T}\mathbf{z},...,\mathbf{w}_{r,C}^{T}\mathbf{z}\sim p_{\mathbf{Wz}} \tag{10}\]

as a sample of the same underlying distribution and observe that

\[p_{\mathbf{Wz}}\approx\mathbb{E}_{\mathbf{z}^{\prime}\in Z_{\textit{min}}}[p_{\mathbf{Wz}^ {\prime}}]. \tag{11}\]

Figure 3: Histogram of all 512 activations in the penultimate layer (left pair) and the activations in _WeiPer_ space (right pair) of a ResNet18 trained on CIFAR10. We perturb the weight matrix \(100\) times to produce a \(10\cdot 100=1000\)-dimensional perturbed logit space. For each pair, the left panel shows the mean distribution over all samples (ID = CIFAR10, OOD = CIFAR100). The right panels show the distribution \(p_{\mathbf{z}}\) and \(p_{\mathbf{Wz}}\), respectively, for two randomly chosen samples with smoothing applied (\(s_{1}=s_{2}=2\))

We demonstrate both behaviors in Figure 3.

In practice, we discretize \(p_{\mathbf{z}}\) and \(p_{\mathbf{W}_{\mathbf{z}}}\) as histogram-based densities by splitting the value range into \(n_{\text{bins}}\) bins (see Equation (21) in the Appendix). Compared to the mean distribution, \(p_{\mathbf{z}}\) and \(p_{\tilde{\mathbf{W}}_{\mathbf{z}}}\) still have a sparse signal. We smoothen the densities with a function

\[T_{k_{s}}(p(t)):=\operatorname{normalize}((p*k_{s})(t)+\varepsilon) \tag{12}\]

by convolving \(p\) with a uniform kernel \(k_{s}\) of size \(s\) and prevent densities from being zero by adding \(\varepsilon>0\) which we set to the fixed values \(\varepsilon:=0.01\) for the penultimate layer and \(\varepsilon:=0.025\) for the _WeiPer_ space. Note, that tuning both epsilons might increase performance as we observed in early stages of our experiments, but will add two additional hyperparameters. We normalize the density to sum up to one again, here defined by normalize. Afterwards, we compare each of the densities with the KL divergence, respectively:

\[D_{\text{KL}}(\mathbf{x}\mid q_{\mathbf{z}},k_{s},\varepsilon):=\operatorname{KL} \bigl{(}T_{k_{s}}(q_{\mathbf{z}})\parallel\operatorname{\mathbb{E}}_{\mathbf{z}\in \mathcal{Z}_{\text{min}}}[q_{\mathbf{z}}]\bigr{)}+\operatorname{KL}\bigl{(} \operatorname{\mathbb{E}}_{\mathbf{z}\in\mathcal{Z}_{\text{min}}}[q_{\mathbf{z}}] \parallel T_{k_{s}}(q_{\mathbf{z}})\bigr{)}, \tag{13}\]

where \(q_{z}\) is either \(p_{\mathbf{z}}\) or \(p_{\tilde{\mathbf{W}}_{\mathbf{z}}^{\ast}}\). We discuss why our method does not suffer from the curse-of-dimensionality in contrast to other methods as investigated by Ghosal et al. (2023) in Appendix A.1.5

_WeiPer_+KLD combines the KL divergence on the penultimate space, the KL divergence and MSP on the perturbed logit space into one final score:

\[\text{\emph{WeiPer+KLD}}(\mathbf{x}):=D_{\text{KL}}(x\mid p_{\mathbf{z}},s_{1})+ \lambda_{1}D_{\text{KL}}(x\mid p_{\tilde{\mathbf{W}}^{\ast}},s_{2})-\lambda_{2} \operatorname{MSP}_{\text{WeiPer}}(\mathbf{x}) \tag{14}\]

The full list of hyperparameters is \(r\) and \(\delta\) for the _WeiPer_ application and \(n_{\text{bins}},\lambda_{1},\lambda_{2},s_{1},s_{2}\) for the KL divergence score function. Figure 2 provides a visual explanation and a quick overview of _WeiPer_ and both its postprocessors.

## 4 Experiments

**Setup.** We evaluate _WeiPer_ using the OpenOOD Zhang et al. (2023b) framework that includes three vision benchmarks: _CIFAR10_Krizhevsky (2009), _CIFAR100_Krizhevsky (2009), and _ImageNet_Deng et al. (2009). Each of them contains a respective ID dataset \(\mathcal{D}_{\text{in}}\) and several OOD datasets, subdivided into _near_ datasets \(\mathcal{D}_{\text{near}}\) and _far_ datasets \(\mathcal{D}_{\text{far}}\) (see Table 1). The terms near and far indicate their similarity to \(\mathcal{D}_{\text{in}}\) and, therefore, the difficulty of separating their samples.

OpenOOD also provides three model checkpoints trained on each CIFAR dataset whereas for ImageNet the methods are evaluated on a single official _torchvision_Marcel & Rodriguez (2010) checkpoint of ResNet50 He et al. (2016) and ViT-B/16 Dosovitskiy et al. (2020) respectively. We report our scores together with the results of Zhang et al. (2023b) in Table 2.

Due to resource constraints, we only evaluate our methods on the models trained with the standard preprocessor, that includes random cropping, horizontal flipping and normalizing, on the cross entropy objective. Additionally to the KL divergence score function and MSP, we evaluate _WeiPer_ on ReAct. But instead of combining ReAct with the energy-based score function Liu et al. (2020) as in OpenOOD, we apply \(\operatorname{MSP}_{\text{WeiPer}}\) and call it _WeiPer_+ReAct. The hyperparameters of our methods were tuned by finding the best combination over a predefined and discrete range of values on the OpenOOD validation sets to assure a fair comparison to the competition (see Table 8). For ImageNet, results are based on a subset of the training data, comprising 300,000 randomly selected, balanced samples (300 per class). For an analysis across different training set sizes, refer to Table 6.

Metrics.We evaluate the methods with the Area Under the Receiver Operating characteristic Curve, AUROC, Bradley (1997) metric as a threshold-independent score and the FPR95 as a quality metric. The FPR95 score reports the False Positive Rate at the True Positive Rate threshold 95%.

\begin{table}
\begin{tabular}{c c c c} \hline \hline \(\mathcal{D}_{\text{in}}\) & **CIFAR10** & **CIFAR100** & **ImageNet-1k** \\ \hline \(\mathcal{D}_{\text{out}}^{\text{near}}\) & CIFAR100, & CIFAR10, & ssb-hard, \\ TinyImageNet & TinyImageNet & ninco \\ \hline \multirow{4}{*}{\(\mathcal{D}_{\text{out}}^{\text{far}}\)} & MNIST, & MNIST, & iNaturalist, \\  & SVHN, & SVHN, & Texture, \\ \cline{1-1}  & Texture, & Texture, & OpenImage-O \\ \cline{1-1}  & Places365 & Places365 & \\ \hline \hline \end{tabular}
\end{table}
Table 1: The individual benchmark datasets.

Results.Table 2 reports the performance of _WeiPer_ in comparison to the state-of-the-art OOD detectors on each benchmark. We compare our approach based on the \(\mathcal{D}_{\textbf{near}}\) and \(\mathcal{D}_{\textbf{far}}\) detection performances and report the mean over all datasets in each category. Table 3 portrays the mean relative performance on \(\mathcal{D}_{\textbf{near}}\) and \(\mathcal{D}_{\textbf{far}}\) of every postprocessor. The score is calculated as follows:

\[S_{\text{rel}}(P):=\frac{1}{3}\big{(}A_{\text{CIFAR10}}(P)+A_{\text{CIFAR100}}(P )+\frac{1}{2}\big{(}A_{\text{ImageNet(ResNet50)}}(P)+A_{\text{ImageNet(ViT)}}(P) \big{)}\big{)} \tag{15}\]

where

\[A_{\mathcal{D}}(P):=\frac{\text{AUROC}_{\mathcal{D}_{\textbf{near}/\text{far}}} (P)}{\max_{P\in\mathcal{P}}\text{AUROC}_{\mathcal{D}_{\textbf{near}/\text{far}} }(P)} \tag{16}\]

It is designed such that each result on each dataset \(\mathcal{D}\) is equally weighted and scoring \(1.0\) means that the postprocessor \(P\) is top performing across all datasets.

_WeiPer_+KLD achieves three out of eight top AUROC scores and the best performance on all near benchmarks, establishing a new state of the art performance by a significant margin (see Table 3).

Especially for the most challenging benchmark, separating \(\mathcal{D}_{\textbf{near}}\) on ImageNet with a ResNet50, we outperform our strongest competitor, ASH Djurisic et al. (2022), by 1.88% AUROC (we even achieve an AUROC score of 80.29 when using a 1M training samples instead of 300k, see Table 6). Additionally, _WeiPer_+KLD performs well on many far benchmarks, being the best method for ResNet50 on ImageNet, reaching into the top three positions on CIFAR10 far and into the top three on the CIFAR100 far benchmark. With its relative performance in Table 3, _WeiPer_+KLD reaches 3rd place overall in the far benchmark.

Only on Vit-B/16 trained on ImageNet, _WeiPer_+KLD shows a significant performance dent, especially on the far benchmark. ViT-B/16 uses a comparably narrow penultimate layer having fewer features

than classes and therefore compresses the class clusters. Some dimensions may thus compress two classes while others represent a feature specific to only one class. This introduces more noise into \(p_{\mathbf{z}}\) which could impair the detection performance. Future experiments will reveal whether _WeiPer_ benefits from higher dimensionalities of the latent space.

WeiPer on existing methods.Additionally, _WeiPer_ enhances the MSP performance by 1-4.1% AUROC across all benchmarks and _WeiPer_+ReAct consistently outperforms ReAct with an energy-based score, although in their evaluation, this variant was better than ReAct+MSP (see Table 3).

Ablation study.We determine the effect of each hyperparameter in Figure 4 by freezing single hyperparameters and optimizing only the one in question. As expected, increasing the number of random perturbations \(r\) leads to a better median performance, while the standard deviation decreases for larger \(r\). Note, that it is possible to have better performance for lower \(r\) by recrolling the weights a few times and choosing the best performing ones. All methods show a significant performance boost compared to using no perturbations \(\delta=0\) and seem to be best at \(\delta=2\) for _WeiPer_+KLD, which corresponds to an angle of \(\alpha\approx 63^{\circ}\) and \(\delta=4\) (\(\alpha\approx 76^{\circ}\)) for MSP and ReAct.

On CIFAR10, _WeiPer_+KLD only improves marginally by applying \(\mathrm{MSP}_{\text{WeiPer}}\), which is not the case for the other benchmarks (see Table 7), where \(\lambda_{2}>0\). Furthermore, we study the performance of random projections that are independent from the weights \(W_{\text{fc}}\). We show that using only random projections (RP, see Table 2) without adding \(\mathrm{MSP}_{\text{WeiPer}}\), we are hardly able to detect any OOD samples. This supports the claim that utilizing the class directions is necessary. The supplementary material presents all the other KLD-specific hyperparameters and we also investigate their influences to the performance in Figure 6. We outline the selected parameters for each benchmark in Table 7.

Figure 4: We investigate the effect of _WeiPer_ hyperparameters \(r\) and \(\delta\) on the performance of the three postprocessors. The left pair shows results on CIFAR10, the right pair corresponds to ImageNet (using ResNet18 for both). Models were tested using their respective near OOD datasets. The panels corresponding to \(\delta\) depict AUROC performance minus the initial AUROC performance at \(\delta=0\). The graphs show the mean over 25 runs and the shaded area around them represents the value range (min to max) over those runs. All other parameters of the methods were fixed to the optimal setting.

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline  & \multicolumn{3}{c}{\(\mathcal{D}_{\text{near}}\)} & \multicolumn{3}{c}{\(\mathcal{D}_{\text{far}}\)} \\ \hline
**Postprocessor** & \(S_{\text{rel}}\) & **Postprocessor** & \(S_{\text{rel}}\) & **Postprocessor** & \(S_{\text{rel}}\) & **Postprocessor** & \(S_{\text{rel}}\) \\ \hline
**WeiPer+KLD** & 0.988 & OpenMax & 0.943 & NAC & 0.999 & MLS & 0.932 \\ RMDS & 0.984 & VIM & 0.943 & VIM & 0.970 & TempScale & 0.929 \\
**WeiPer+MSP** & 0.977 & EBO & 0.940 & KNN & 0.963 & EBO & 0.924 \\ GEN & 0.975 & SHE & 0.934 & **WeiPer+KLD** & 0.959 & MSP & 0.920 \\
**WeiPer+ReAct** & 0.974 & KLM & 0.918 & RMDS & 0.959 & SHE & 0.919 \\ TempScale & 0.967 & DICE & 0.901 & **WeiPer+ReAct** & 0.951 & DICE & 0.909 \\ KNN & 0.963 & ASH & 0.870 & GEN & 0.947 & KLM & 0.893 \\ MSP & 0.963 & MDS & 0.829 & **WeiPer+MSP** & 0.944 & MDS & 0.877 \\ ReAct & 0.955 & GradNorm & 0.722 & ReAct & 0.943 & ASH & 0.844 \\ MLS & 0.954 & NAC & - & OpenMax & 0.935 & GradNorm & 0.700 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Mean relative scores of all the postprocessors (post-hoc methods), see Equation (15).

## 5 Limitations

_WeiPer+_KLD has more hyperparameters than other competitors: 6 in total. As discussed in the previous section, \(r\) can be seen as a memory / performance trade-off (see Figure 4). In the supplementary material (see Figure 6) we investigate the other parameters and find that they all have only one local maximum in the range we were searching and should thus be easy to optimize. We tried to choose the same smoothing size \(s_{1}=s_{2}\) for both densities, but the ablations show that both are optimal at different sizes. While \(\mathrm{MSP}_{\mathrm{WeiPer}}\) is not really used for CIFAR10 (\(\lambda_{2}\approx 0\)) it is beneficial for CIFAR100 and ImageNet. As WeiPer blows up the dimension we also conduct a memory and time comparison to other methods in Table 4 and Table 5. We demonstrate that with a combination of a confidence and a distance based metric it is possible to achieve competitive near results across the board where all other methods seem to deteriorate in at least one benchmark.

## 6 Conclusion

We show that multiple random perturbations of the class projections in the final layer of the network can provide additional information that we can exploit for detecting out-of-distribution samples. _WeiPer_ creates a representation of the data by projecting the latent activation of a sample onto vector bundles around the class-specific weight vectors of the final layer. We then employ a new approach to construct a score allowing the subsequent separation of ID and OOD data. It relies on the fingerprint-like nature of features of the penultimate and the _WeiPer_-representations by assuming they were sampled by the same underlying distribution. In a thorough evaluation, we first show that _WeiPer_ enhances MSP and ReAct+MSP performance significantly and show that _WeiPer_+KLD achieves top scores in most benchmarks, representing the new state-of-the-art solution in post-hoc OOD methods on near benchmarks.

## 7 Acknowledgements

We appreciate the reviewers' comments, which greatly helped enhance this manuscript and inspired us to conduct additional key experiments. Maximilian Granz was supported by the Elsa-Neumann-Scholarship from the state of Berlin, which provided essential funding for the initial stages of this research. We also thank Leon Sixt and Manolis Panagiotou for their feedback throughout the project.

## References

* Ahn et al. (2023) Ahn, Y. H., Park, G.-M., and Kim, S. T. Line: Out-of-distribution detection by leveraging important neurons. _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 19852-19862, 2023. URL [https://api.semanticscholar.org/CorpusID:257757417](https://api.semanticscholar.org/CorpusID:257757417).
* Amodei et al. (2016) Amodei, D., Olah, C., Steinhardt, J., Christiano, P. F., Schulman, J., and Mane, D. Concrete problems in ai safety. _ArXiv_, abs/1606.06565, 2016. URL [https://api.semanticscholar.org/CorpusID:10242377](https://api.semanticscholar.org/CorpusID:10242377).
* Bendale & Boult (2015) Bendale, A. and Boult, T. E. Towards open set deep networks. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 1563-1572, 2015. URL [https://api.semanticscholar.org/CorpusID:14240373](https://api.semanticscholar.org/CorpusID:14240373).
* 45, 2014. URL [https://api.semanticscholar.org/CorpusID:1907942](https://api.semanticscholar.org/CorpusID:1907942).
* Bradley (1997) Bradley, A. P. The use of the area under the roc curve in the evaluation of machine learning algorithms. _Pattern Recognit._, 30:1145-1159, 1997. URL [https://api.semanticscholar.org/CorpusID:13806304](https://api.semanticscholar.org/CorpusID:13806304).
* Cheney et al. (2017) Cheney, N., Schrimpf, M., and Kreiman, G. On the robustness of convolutional neural networks to internal architecture and weight perturbations. _ArXiv_, abs/1703.08245, 2017. URL [https://api.semanticscholar.org/CorpusID:13217484](https://api.semanticscholar.org/CorpusID:13217484).
* Cheney et al. (2018)Cramer, H. and Wold, H. Some theorems on distribution functions. _Journal of the London Mathematical Society_, s1-11(4):290-294, 1936. doi: [https://doi.org/10.1112/jlms/s1-11.4.290](https://doi.org/10.1112/jlms/s1-11.4.290). URL [https://londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/jlms/s1-11.4.290](https://londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/jlms/s1-11.4.290).
* Cuesta-Albertos et al. (2007) Cuesta-Albertos, J., Fraiman, R., and Ransford, T. A sharp form of the cramer-wold theorem. _Journal of Theoretical Probability_, 20:201-209, 06 2007. doi: 10.1007/s10959-007-0060-7.
* Darrin et al. (2022) Darrin, M., Piantanida, P., and Colombo, P. Rainproof: An umbrella to shield text generators from out-of-distribution data. _arXiv preprint arXiv:2212.09171_, 2022.
* Deng et al. (2009) Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pp. 248-255, 2009. doi: 10.1109/CVPR.2009.5206848.
* Djurisic et al. (2022) Djurisic, A., Bozanic, N., Ashok, A., and Liu, R. Extremely simple activation shaping for out-of-distribution detection. _ArXiv_, abs/2209.09858, 2022. URL [https://api.semanticscholar.org/CorpusID:252383259](https://api.semanticscholar.org/CorpusID:252383259).
* Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. _arXiv e-prints_, art. arXiv:2010.11929, October 2020. doi: 10.48550/arXiv.2010.11929.
* Ghosal et al. (2023) Ghosal, S. S., Sun, Y., and Li, Y. How to overcome curse-of-dimensionality for out-of-distribution detection?, 2023.
* Guo et al. (2017) Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. In Precup, D. and Teh, Y. W. (eds.), _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pp. 1321-1330. PMLR, 06-11 Aug 2017. URL [https://proceedings.mlr.press/v70/guo17a.html](https://proceedings.mlr.press/v70/guo17a.html).
* He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 770-778, 2016. doi: 10.1109/CVPR.2016.90.
* Hendrycks & Gimpel (2016) Hendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. _ArXiv_, abs/1610.02136, 2016. URL [https://api.semanticscholar.org/CorpusID:13046179](https://api.semanticscholar.org/CorpusID:13046179).
* Hendrycks et al. (2022a) Hendrycks, D., Basart, S., Mazeika, M., Mostajabi, M., Steinhardt, J., and Song, D. X. Scaling out-of-distribution detection for real-world settings. In _International Conference on Machine Learning_, 2022a. URL [https://api.semanticscholar.org/CorpusID:227407829](https://api.semanticscholar.org/CorpusID:227407829).
* Hendrycks et al. (2020) Hendrycks, D., Basart, S., Mazeika, M., Zou, A., Kwon, J., Mostajabi, M., Steinhardt, J., and Song, D. Scaling out-of-distribution detection for real-world settings. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pp. 8759-8773. PMLR, 17-23 Jul 2022b. URL [https://proceedings.mlr.press/v162/hendrycks22a.html](https://proceedings.mlr.press/v162/hendrycks22a.html).
* Jerome Friedman & Schroeder (1984) Jerome H. Friedman, W. S. and Schroeder, A. Projection pursuit density estimation. _Journal of the American Statistical Association_, 79(387):599-608, 1984. doi: 10.1080/01621459.1984.10478086. URL [https://www.tandfonline.com/doi/abs/10.1080/01621459.1984.10478086](https://www.tandfonline.com/doi/abs/10.1080/01621459.1984.10478086).
* Khan et al. (2018) Khan, M. E., Nielsen, D., Tangkaratt, V., Lin, W., Gal, Y., and Srivastava, A. Fast and scalable bayesian deep learning by weight-perturbation in adam. In _International Conference on Machine Learning_, 2018. URL [https://api.semanticscholar.org/CorpusID:49187225](https://api.semanticscholar.org/CorpusID:49187225).
* Kolouri et al. (2016) Kolouri, S., Zou, Y., and Rohde, G. K. Sliced wasserstein kernels for probability distributions. In _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 5258-5267, 2016. doi: 10.1109/CVPR.2016.568.
* Krizhevsky (2009) Krizhevsky, A. Learning multiple layers of features from tiny images. pp. 32-33, 2009. URL [https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf).
* Krizhevsky (2012)Kuan, J.-L. and Mueller, J. W. Back to the basics: Revisiting out-of-distribution detection baselines. _ArXiv_, abs/2207.03061, 2022. URL [https://api.semanticscholar.org/CorpusID:250334470](https://api.semanticscholar.org/CorpusID:250334470).
* Lee et al. (2018) Lee, K., Lee, K., Lee, H., and Shin, J. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. _ArXiv_, abs/1807.03888, 2018. URL [https://api.semanticscholar.org/CorpusID:49667948](https://api.semanticscholar.org/CorpusID:49667948).
* Liu et al. (2020) Liu, W., Wang, X., Owens, J. D., and Li, Y. Energy-based out-of-distribution detection. _ArXiv_, abs/2010.03759, 2020. URL [https://api.semanticscholar.org/CorpusID:222208700](https://api.semanticscholar.org/CorpusID:222208700).
* Liu et al. (2023a) Liu, X., Lochman, Y., and Christopher, Z. Gen: Pushing the limits of softmax-based out-of-distribution detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023a.
* Liu et al. (2023b) Liu, Y., Tian, C. X., Li, H., Ma, L., and Wang, S. Neuron activation coverage: Rethinking out-of-distribution detection and generalization. _ArXiv_, abs/2306.02879, 2023b. URL [https://api.semanticscholar.org/CorpusID:259075869](https://api.semanticscholar.org/CorpusID:259075869).
* Liutkus et al. (2019) Liutkus, A., Simsekli, U., Majewski, S., Durmus, A., and Stoter, F.-R. Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions. In Chaudhuri, K. and Salakhutdinov, R. (eds.), _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pp. 4104-4113. PMLR, 09-15 Jun 2019. URL [https://proceedings.mlr.press/v97/liutkus19a.html](https://proceedings.mlr.press/v97/liutkus19a.html).
* Marcel & Rodriguez (2010) Marcel, S. and Rodriguez, Y. Torchvision the machine-vision package of torch. In _Proceedings of the 18th ACM International Conference on Multimedia_, MM '10, pp. 1485-1488, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781605589336. doi: 10.1145/1873951.1874254. URL [https://doi.org/10.1145/1873951.1874254](https://doi.org/10.1145/1873951.1874254).
* Nguyen et al. (2021) Nguyen, K., Ho, N., Pham, T., and Bui, H. Distributional sliced-wasserstein and applications to generative modeling. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=QYj070ACDK](https://openreview.net/forum?id=QYj070ACDK).
* 24663, 2020. URL [https://api.semanticscholar.org/CorpusID:221172897](https://api.semanticscholar.org/CorpusID:221172897).
* Park et al. (2023) Park, J., Jung, Y. G., and Teoh, A. B. J. Nearest neighbor guidance for out-of-distribution detection. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 1686-1695, 2023.
* Paty & Cuturi (2019) Paty, F.-P. and Cuturi, M. Subspace robust Wasserstein distances. In Chaudhuri, K. and Salakhutdinov, R. (eds.), _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pp. 5072-5081. PMLR, 09-15 Jun 2019. URL [https://proceedings.mlr.press/v97/patyl9a.html](https://proceedings.mlr.press/v97/patyl9a.html).
* Picot et al. (2022) Picot, M., Noiry, N., Piantanida, P., and Colombo, P. Adversarial attack detection under realistic constraints. 2022.
* Rakin et al. (2018) Rakin, A. S., He, Z., and Fan, D. Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack. _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 588-597, 2018. URL [https://api.semanticscholar.org/CorpusID:53716271](https://api.semanticscholar.org/CorpusID:53716271).
* Ren et al. (2021) Ren, J. J., Fort, S., Liu, J. Z., Roy, A. G., Padhy, S., and Lakshminarayanan, B. A simple fix to mahalanobis distance for improving near-ood detection. _ArXiv_, abs/2106.09022, 2021. URL [https://api.semanticscholar.org/CorpusID:235458597](https://api.semanticscholar.org/CorpusID:235458597).
* Sastry & Oore (2020) Sastry, C. S. and Oore, S. Detecting out-of-distribution examples with Gram matrices. In III, H. D. and Singh, A. (eds.), _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pp. 8491-8501. PMLR, 13-18 Jul 2020. URL [https://proceedings.mlr.press/v119/sastry20a.html](https://proceedings.mlr.press/v119/sastry20a.html).
* Sastry & Oore (2021)Sun, Y. and Li, Y. Dice: Leveraging sparsification for out-of-distribution detection. 2021. URL [https://api.semanticscholar.org/CorpusID:250626952](https://api.semanticscholar.org/CorpusID:250626952).
* Sun et al. (2021) Sun, Y., Guo, C., and Li, Y. React: Out-of-distribution detection with rectified activations. In _Neural Information Processing Systems_, 2021. URL [https://api.semanticscholar.org/CorpusID:244709089](https://api.semanticscholar.org/CorpusID:244709089).
* Sun et al. (2022) Sun, Y., Ming, Y., Zhu, X., and Li, Y. Out-of-distribution detection with deep nearest neighbors. In _International Conference on Machine Learning_, 2022. URL [https://api.semanticscholar.org/CorpusID:248157551](https://api.semanticscholar.org/CorpusID:248157551).
* Tajwar et al. (2021) Tajwar, F., Kumar, A., Xie, S. M., and Liang, P. No true state-of-the-art? good detection methods are inconsistent across datasets. _ArXiv_, abs/2109.05554, 2021. URL [https://api.semanticscholar.org/CorpusID:237264010](https://api.semanticscholar.org/CorpusID:237264010).
* Wang et al. (2022) Wang, H., Li, Z., Feng, L., and Zhang, W. Vim: Out-of-distribution with virtual-logit matching. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 4911-4920, 2022. URL [https://api.semanticscholar.org/CorpusID:247595202](https://api.semanticscholar.org/CorpusID:247595202).
* Wen et al. (2018) Wen, Y., Vicol, P., Ba, J., Tran, D., and Grosse, R. B. Flipout: Efficient pseudo-independent weight perturbations on mini-batches. _ArXiv_, abs/1803.04386, 2018. URL [https://api.semanticscholar.org/CorpusID:3861760](https://api.semanticscholar.org/CorpusID:3861760).
* Wu et al. (2020) Wu, D., Wang, Y., and Xia, S. Revisiting loss landscape for adversarial robustness. _ArXiv_, abs/2004.05884, 2020. URL [https://api.semanticscholar.org/CorpusID:215744953](https://api.semanticscholar.org/CorpusID:215744953).
* Xiang et al. (2019) Xiang, L., Zeng, X., Niu, Y., and Liu, Y. Study of sensitivity to weight perturbation for convolution neural network. _IEEE Access_, PP:1-1, 07 2019. doi: 10.1109/ACCESS.2019.2926768.
* Yang et al. (2022) Yang, J., Wang, P., Zou, D., Zhou, Z., Ding, K., Peng, W., Wang, H., Chen, G., Li, B., Sun, Y., Du, X., Zhou, K., Zhang, W., Hendrycks, D., Li, Y., and Liu, Z. OpenOOD: Benchmarking generalized out-of-distribution detection. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022. URL [https://openreview.net/forum?id=gT6j4_tskUt](https://openreview.net/forum?id=gT6j4_tskUt).
* Zhang et al. (2023a) Zhang, J., Fu, Q., Chen, X., Du, L., Li, Z., Wang, G., xiaoguang Liu, Han, S., and Zhang, D. Out-of-distribution detection based on in-distribution data patterns memorization with modern hopfield energy. 2023a. URL [https://openreview.net/forum?id=KkazG4lgKL](https://openreview.net/forum?id=KkazG4lgKL).
* Zhang et al. (2023b) Zhang, J., Yang, J., Wang, P., Wang, H., Lin, Y., Zhang, H., Sun, Y., Du, X., Zhou, K., Zhang, W., Li, Y., Liu, Z., Chen, Y., and Hai, L. Openood v1.5: Enhanced benchmark for out-of-distribution detection. _arXiv preprint arXiv:2306.09301_, 2023b.

Appendix

### WeiPer: Theoretical details

#### a.1.1 Weight perturbations

**Theorem A.1** (Cuesta-Albertos et al. (2007)).: _Let X and Y be two \(\mathbb{R}^{K}\)-valued random vectors. Suppose the absolute moments \(m_{k}:=\mathbb{E}(\|X\|^{k})\) are finite and \(\sum_{k=1}^{\infty}(m_{k})^{-1/k}=\infty\). If the set \(W_{XY}=\{\mathbf{w}\in\mathbb{R}^{K}:\mathbf{w}^{T}X\overset{\text{\tiny{$\pm$}}}{=} \mathbf{w}^{T}Y\}\) has positive Lebesgue measure, then \(X\overset{\text{\tiny{$\pm$}}}{=}Y\)._

We provide a simple proof for the case that \(W_{XY}=\mathbb{R}^{K}\). For the complete proof, we refer to Cuesta-Albertos et al. (2007).

Proof.: The characteristic function of a random vector \(X\) is defined as

\[\phi_{X}(\mathbf{w}):=\int e^{i\mathbf{w}^{T}\mathbf{x}}dX \tag{17}\]

By the uniqueness theorem, every random vector \(X\) has a unique characteristic funtion \(\phi_{X}\). If the assumption in the theorem holds, then for all realizations \(\mathbf{x}=X(\omega)\) and \(\mathbf{y}=Y(\omega)\), we have

\[\mathbf{w}^{T}\mathbf{x}=\mathbf{w}^{T}\mathbf{y} \tag{18}\]

and thus \(\phi_{X}=\phi_{Y}\). Therefore we have \(X=Y\) by the uniqueness. 

Note, that is enough cover all the directions in \(\mathbb{R}^{K}\) instead of covering the whole space with the set of projections \(W\). Since if \(t\mathbf{w}\not\in W\) for \(t\in\mathbb{R}\), but \(w\in W\cap W_{XY}\) then \(t\mathbf{w}\in W_{XY}\). For \(\delta>0\) our set of perturbed class projections indeed covers the all directions if \(r\to\infty\).

To generally apply the theorem, \(Z\) must be defined on a bounded set with finite measure (Hausdorff moment problem), which is true for virtually all practical problems. More importantly, \(W\) needs to be a \(K\)-dimensional subset of \(\mathbb{R}^{K}\). Note that the theorem also applies for a set of weight matrices

\[\{\mathbf{W}\in\mathbb{R}^{K\times K}:\mathbf{W}X\overset{\text{\tiny{$\pm$}}}{=}\mathbf{ W}Y\} \tag{19}\]

when their row vectors form a \(K\) dimensional set as their marginal distributions \(\mathbf{w}_{i}^{T}X\overset{\text{\tiny{$\pm$}}}{=}\mathbf{w}_{i}^{T}Y\) would be equal for \(i=1,...,K\). We are using this theorem solely as motivation since it is not possible to draw direct implications. However, with a score function \(S\) we are measuring properties of the logit distribution of the training data \(\mathbf{W}_{\text{fc}}Z_{\text{train}}=(\mathbf{w}_{1}^{T}Z_{\text{train}},...,\mathbf{w}_ {C}^{T}Z_{\text{train}})\) and check if they match the properties of some unknown logit distribution \(\mathbf{W}_{\text{fc}}Z\) that might be test data or OOD data. In the ideal case the logits match in distribution

\[\mathbf{W}_{\text{fc}}Z_{\text{train}}\overset{\text{\tiny{$\pm$}}}{=}\mathbf{W}_{ \text{fc}}Z\text{ if and only if }S\text{ is maximized,} \tag{20}\]

e.g. if \(D\) is a distance and \(S=-D\), \(S(\mathbf{W}_{\text{fc}}Z_{\text{test}})=0\) and \(S(\mathbf{W}_{\text{fc}}Z_{\text{ood}})<0\). Now the theorem says that if we chose a \(K\)-dimensional set \(W\) of projections and we had a score function \(S_{\text{WeiPer}}\) that fulfills the property of Equation (20) on the infinite dimensional space spanned by the projections of \(W\), not just the distributions on the projection would be equal when \(S_{\text{WeiPer}}\) is minimized but also the penultimate distributions \(Z_{\text{train}}\overset{\text{\tiny{$\pm$}}}{=}Z_{\text{test}}\).

#### a.1.2 MSP on the perturbed logit space

Continuing from the previous section, \(S_{\text{WeiPer}}=\operatorname{MSP}_{\text{WeiPer}}\) is a score function on the infinite dimensional space spanned by \(W\) for \(r\to\infty\), so ideally \(\operatorname{MSP}_{\text{WeiPer}}(Z_{\text{test}})=0\) then not only the logit distributions match \(\mathbf{W}_{\text{fc}}Z_{\text{train}}\overset{\text{\tiny{$\pm$}}}{=}\mathbf{W}_{ \text{fc}}Z_{\text{test}}\), but also their penultimate distributions \(Z_{\text{train}}\overset{\text{\tiny{$\pm$}}}{=}Z_{\text{test}}\) which would make \(\operatorname{MSP}_{\text{WeiPer}}\) a stronger metric than \(\operatorname{MSP}\).

#### a.1.3 PCA on the penultimate space

We draw the following conclusions from Figure 5: The OOD and ID distributions differ much stronger along the first \(C\) principal components, and they are more similar for the other components. This indicates, most of the signal may lie in the \(C\)-dimensional subspace.

We argue that instead of taking random projections, we can utilize the class projections. Since we have a trained classifier at hand, it is likely that the informative dimensions are: \(\mathrm{span}(\mathbf{w}_{1},...,\mathbf{w}_{C})\), the span of the row vectors of \(\mathbf{W}_{\mathrm{fc}}\). Hence, a better choice than Gaussian vectors for the set of projections \(W\) are vectors \(\mathbf{w}\) that correlate with these basis vectors in \(\mathbf{W}_{\mathrm{fc}}\) but at the same time deviate enough such that we obtain new information from projections onto them.

#### a.1.4 KL divergence: Density definition

We gave a brief description of our construction of the densities in the main paper. The formal definition is:

\[p_{\mathbf{z}}(t):=\frac{1}{Kl_{b}}\sum_{i=1}^{K}[z_{i}\in b(t)]. \tag{21}\]

Here \(l_{b}\) is the bin length, \(b(t)\) is the bin range in which \(t\) falls, and \([.]\) is the Iverson bracket which evaluates to one if true or zero if the statement is false. Note that we are dividing by \(l_{b}\) such that the density integrates to one. This is the usual definition for descretizing a density into equal sized bins.

#### a.1.5 Curse of dimensionality

In contrast to other distance-based methods, our KL divergence score does not suffer from the curse-of-dimensionality, which deteriorates the performance of methods like KNN Sun et al. (2022) as investigated by Ghosal et al. (2023). We disregard the dimension and only consider the activations in the penultimate space or in the perturbed logit space. In our case, more dimensions means more samples to approximate the activation distribution \(p_{\mathbf{z}}\). We believe that our method thrives when applied to networks with higher dimensional penultimate space, but this still needs to be evaluated in future experiments. However, in the perturbed logit space, we can control the size of the space with \(r\) (see Figure 4). Our ablation results show that increasing \(r\) and thus blowing up the dimensionality only increases performance.

Figure 5: We applied PCA to \(Z_{\text{Train}}\) of CIFAR10 and projected \(Z_{\text{train}}\) (blue), \(Z_{\text{test}}\) (purple) and \(Z_{\text{ood}}\) (red, CIFAR100) to the first 20 principal components. We observe density spikes in the first 10 dimensions, likely corresponding to the class clusters. The dimensions 10-19 exhibit less structure as their densities appear to be Gaussian. Along these directions the ID and OOD data are more similar compared to the first ten principal components.

[MISSING_PAGE_EMPTY:16]

[MISSING_PAGE_EMPTY:17]

### Penultimate layer distribution

Figure 7: Density plots for CIFAR10 and CIFAR100 of a ResNet18 trained for 300 epochs. We present the densities as in Figure 3, but this time we show it for more datasets and for two different samples \(\mathbf{z}_{1},\mathbf{z}_{2}\) in the penultimate and the perturbed logit space, respectively. The OOD set for the ID set CIFAR10 is CIFAR100 and vice versa for CIFAR100. The range of the \(y\)-axis is adjusted such that the differences between ID and OOD become visible. We therefore report the mean over the maximum density \(\max_{p}\) of the penultimate dimensions to show, up to which value the maximum would go. We apply smoothing over \(k_{s}\) neighbors in each plot and construct the histograms with \(n_{\text{bins}}\) bins. We report the parameters in Table 9. The class clusters and the activation clusters are clearly visible for CIFAR10 and merge into the bigger cluster for CIFAR100, probably because of the lower class to non-class ratio. It is harder to see for CIFAR100, but for both datasets, it seems harder for the OOD data to sample in the class cluster or the activated feature cluster.

\begin{table}
\begin{tabular}{c l} \hline \hline
**Hyperparameter** & **Values** \\ \hline \(r\) & 100 \\ \(\lambda\) & [1.8, 2.0, 2.2, 2.4] \\ \(n_{\text{bins}}\) & [60, 80, 100] \\ \(\lambda_{1}\) & [0.1, 1, 2.5, 4] \\ \(\lambda_{2}\) & [0.1, 0.25, 1, 2.5, 5] \\ \(s_{1}\) & [4, 8, 12, 20, 40] \\ \(s_{2}\) & [15, 25, 40] \\ \hline \hline \end{tabular}
\end{table}
Table 8: Set of values for the hyperparameter search.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \(n_{\rm bins}\) & \(s\) & \(\max_{p}\) \\ \hline CIFAR10 & 1000 & 2 & 364.22 \\ CIFAR100 & 1000 & 2 & 32.85 \\ ImageNet (ResNet50) & 100 & 4 & 1.78 \\ ImageNet (ViT-B/16) & 100 & 4 & 0.89 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Plotting parameters: \(s\) is the kernel size for the uniform kernel that was used for smoothing. and \(\max_{p}=\max_{t}\mathbb{E}_{\mathbf{z}\in Z_{\rm min}}[p_{\mathbf{z}}](t)\) denotes the maximum of the mean density of the penultimate densities \(p_{\mathbf{z}}\). The perturbed densities \(p_{\tilde{\mathbf{W}}\mathbf{z}}\) are scaled similarly.

Figure 8: Density plots for ImageNet (ResNet50 and ViT-B/16). We chose SSB-hard as OOD set and apply the same plotting procedure as defined for CIFAR10/CIFAR100. For the respective plotting parameters, see Table 9. For ViT-B/16, the class clusters are distinguishable from the non-class clusters but not for ResNet50. Still, the difference between ID and OOD is captured in the higher activations which could explain why activation shaping Djurisic et al. (2022); Sun et al. (2021) works well for ImageNet.

### Full CIFAR results

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{2}{c}{**CIFAR100**} & \multicolumn{2}{c}{**TIN**} & \multicolumn{2}{c}{\(\mathcal{D}_{\textbf{near}}\)} \\ \cline{2-9}  & AUROC \(\uparrow\) & FPR95\(\downarrow\) & AUROC \(\uparrow\) & FPR95\(\downarrow\) & AUROC \(\uparrow\) & FPR95\(\downarrow\) \\ \hline \hline \multicolumn{9}{c}{_Benchmark: CIFAR101_ _/_Rochbon: ResNet18_} \\ \hline OpenMax & 86.91\(\pm\)0.31 & 48.06\(\pm\)3.25 & 88.32\(\pm\)0.28 & 39.18\(\pm\)1.44 & 87.62\(\pm\)0.29 & 43.62\(\pm\)2.27 \\ MSP & 87.19\(\pm\)0.33 & 53.08\(\pm\)4.86 & 88.87\(\pm\)0.19 & 43.27\(\pm\)3.00 & 88.09\(\pm\)0.25 & 43.71\(\pm\)3.92 \\ TransScale & 87.17\(\pm\)0.40 & 55.18\(\pm\)5.07 & 89.00\(\pm\)0.23 & 14.16\(\pm\)3.63 & 88.09\(\pm\)0.31 & 50.96\(\pm\)4.32 \\ ODN & 82.18\(\pm\)1.87 & 79.07\(\pm\)0.54 & 83.15\(\pm\)1.84 & 73.38\(\pm\)4.22 & 78.11\(\pm\)1.85 & 76.96\(\pm\)0.68 \\ MDS & 83.93\(\pm\)2.27 & 52.18\(\pm\)3.62 & 84.81\(\pm\)2.53 & 46.99\(\pm\)4.36 & 84.20\(\pm\)2

[MISSING_PAGE_FAIL:21]

### Full ImageNet results

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{2}{c}{**SSB-hard**} & \multicolumn{2}{c}{**NINCO**} & \multicolumn{2}{c}{\(\mathcal{D}_{\textbf{near}}\)} \\ \cline{2-7}  & AUROC \(\uparrow\) & FPR95\(\downarrow\) & AUROC \(\uparrow\) & FPR95\(\downarrow\) & AUROC \(\uparrow\) & FPR95\(\downarrow\) \\ \hline \multicolumn{7}{c}{_Benchmark: ImageNet-1K / Backbone: ResNet50_} \\ OpenMax & 71.37 & 77.33 & 78.17 & 60.81 & 74.77 & 69.07 \\ MSP & 72.09 & 74.49 & 79.95 & 56.88 & 76.02 & 65.68 \\ TempScale & 72.87 & 73.90 & 81.41 & 55.10 & 77.14 & 64.50 \\ ODN & 71.74 & 76.83 & 77.77 & 68.16 & 74.75 & 72.50 \\ MDS

[MISSING_PAGE_FAIL:23]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract introduces the key contribution WeiPer, the KL-Divergence-based scoring function and the setting in which it outperforms its competition. Furthermore, our introduction briefly presents our methods mechanism and lists our contributions and mentions its limitations. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In the Limitations section, we address two points of critique that we feel are most relevant: the number of hyperparameters and the memory consumption (also analyzed in Table 5). In our methods section, we clearly state our assumptions which are be justified by observations (e.g. in the figures) and by the empirical results. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Our paper only includes one theoretical result (Theorem A.1) which is proofed in Cuesta-Albertos et al. (2007). However, we provide a proof for our case. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: With the use of the standardized OpenOOD benchmark, the results of our paper are well reproducible. We provide details on the hyperparameter choices in Table 7 and Table 8 and the exact code to reproduce the results with the supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). *4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide our implementation in the supplementary material. It is integrated in the OpenOOD framework, which makes it easy to setup. All required packages will be installed when setting up OpenOOD. The benchmark datasets are publicly available. When published, we will release a public GitHub repository with our implementation. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We describe the training details of the used baseline models and their checkpoints in the Experiments section. With our method functioning in a post-hoc fashion, these training checkpoints remain unchanged. The optimization details for our postprocessor are also located in section 4. For the specific hyperparameter ranges and found configurations see Table 7 and Table 8. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?Answer: [Yes] Justification: Due to WeiPers random nature our results exhibit noise dependent on the hyperparameter \(r\) and we display the minimum to maximum range in Figure 4 and Figure 6. We chose minimum to maximum results instead of standard deviation as the maximal performing perturbations could be strategically sampled. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We report the memory usage / performance trade-off in Figure 4. In Appendix A.7, we report on specifications of our machine, GPU usage and execution times. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Our research does not involve human subjects or participants. The datasets conform with the NeurIPS Code of Ethics, specifically, the listed concerns. Our work does not introduce new data. Our work aims to contribute to safety in AI. It proposed a method that is applicable to various applications that utilize Machine Learning, but is neither obstructing nor contributing to areas with a larger societal impact. Finally, our research conforms with the NeurIPS Code of Conduct.

Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: We classify our contribution as foundational research, as it is a tool to improve Machine Learning models in various use cases. Hence, a specific assessment of the societal impact is difficult. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: Both, data and models, are not prone to a high risk of misuse, see questions 9. and 10. Hence, we propose no safeguards in our work. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Our work builds on data, the evaluation framework OpenOOD and previous OOD detectors that are introduced and cited in the Experiments section and Related Work section, respectively. Our contribution is licensed by CC BY 4.0. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The implementation of our contribution is well annotated and we provide optimization details as well as a thorough description of our method, as it is the key contribution of our work. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.

* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.