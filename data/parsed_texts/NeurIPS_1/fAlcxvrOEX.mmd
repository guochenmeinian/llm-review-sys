# AdjointDEIS: Efficient Gradients for Diffusion Models

Zander W. Blasingame

Clarkson University

blasinzw@clarkson.edu &Chen Liu

Clarkson University

cliu@clarkson.edu

###### Abstract

The optimization of the latents and parameters of diffusion models with respect to some differentiable metric defined on the output of the model is a challenging and complex problem. The sampling for diffusion models is done by solving either the _probability flow_ ODE or diffusion SDE wherein a neural network approximates the score function allowing a numerical ODE/SDE solver to be used. However, naive backpropagation techniques are memory intensive, requiring the storage of all intermediate states, and face additional complexity in handling the injected noise from the diffusion term of the diffusion SDE. We propose a novel family of bespoke ODE solvers to the continuous adjoint equations for diffusion models, which we call _AdjointDEIS_. We exploit the unique construction of diffusion SDEs to further simplify the formulation of the continuous adjoint equations using _exponential integrators_. Moreover, we provide convergence order guarantees for our bespoke solvers. Significantly, we show that the continuous adjoint equations for diffusion SDEs actually simplify to a simple ODE. Lastly, we demonstrate the effectiveness of AdjointDEIS for guided generation with an adversarial attack in the form of the face morphing problem. Our code will be released at [https://github.com/zblasingame/AdjointDEIS](https://github.com/zblasingame/AdjointDEIS).

## 1 Introduction

Diffusion models are a large family of state-of-the-art generative models which learn to map samples drawn from Gaussian white noise into the data distribution [1, 2]. These diffusion models have achieved state-of-the-art performance on prominent tasks such as image generation [3, 4, 5], audio generation [6, 7], or video generation [8]. Often, state-of-the-art models are quite large and training them is prohibitively expensive [9]. As such, it is fairly common to adapt a pre-trained model to a specific task for post-training. In this way, the generative model can learn new concepts, identities, or tasks without having to train the entire model [10, 11, 12]. Additional work has also proposed algorithms for guiding the generative process of diffusion models [13, 14].

One method of guiding or directing the generative process is to solve an optimization problem w.r.t. some guidance function \(\mathcal{L}\) defined in the image space \(\mathbb{R}^{d}\). This guidance function works on the output of the diffusion model and assesses how "good" the output is. However, the diffusion model works by iteratively removing noise until a clean sample is reached. As such, we need to be able to efficiently backpropagate gradients through the entire generative process. As Song et al. [15] showed, the diffusion SDE can be simplified to an associated ODE, and as such, many efficient ODE/SDE solvers have been developed for diffusion models [16, 17, 18]. However, naively applying backpropagation to the diffusion model is inflexible and memory intensive; moreover, such an approach is not trivial to apply to the diffusion models that used an SDE solver instead of an ODE solver.

### Contributions

Inspired by the work of Chen et al. [19] we study the application of continuous adjoint equations to diffusion models, with a focus on training-free guided generation with diffusion models. We introduce several theoretical contributions and technical insights to both improve the ability to perform certain guided generation tasks and to gain insight into guided generation with diffusion models.

First, we introduce _AdjointDEIS_ a bespoke family of ODE solvers which can efficiently solve the continuous adjoint equations for both diffusion ODEs and SDEs. Moreover, we show that the continuous adjoint equations for diffusion SDEs simplify to a mere ODE. Next, we show how to calculate the continuous adjoint equation for conditional information which evolves with _time_ (rather than being constant). To the best of our knowledge, we are the first to consider conditional information which evolves with time for neural ODEs. Overall, multiple theoretical contributions and technical insights are provided to bring a new family of techniques for the guided generation of diffusion models, which we evaluate experimentally on the task of face morphing.

### Diffusion Models

In this subsection, we provide a brief overview of diffusion models. Diffusion models learn a generative process by first perturbing the data distribution into an isotropic Gaussian by progressively adding Gaussian noise to the data distribution. Then a neural network is trained to perform denoising steps, allowing for sampling of the data distribution via sampling of a Gaussian distribution [2, 9]. Assume that we have an \(d\)-dimensional random variable \(\mathbf{x}\in\mathbb{R}^{d}\) with some distribution \(p_{\text{data}}(\mathbf{x})\). Then diffusion models begin by diffusing \(p_{\text{data}}(\mathbf{x})\) according to the diffusion SDE [2], an Ito SDE given as

\[\mathrm{d}\mathbf{x}_{t}=f(t)\mathbf{x}_{t}\;\mathrm{d}t+g(t)\;\mathrm{d} \mathbf{w}_{t} \tag{1.1}\]

where \(t\in[0,T]\) denotes time with fixed constant \(T>0\), \(f(\cdot)\) and \(g(\cdot)\) denote the drift and diffusion coefficients, and \(\mathbf{w}_{t}\) denotes the standard Wiener process. The trajectories of \(\mathbf{x}_{t}\) follow the distributions \(p_{t}(\mathbf{x}_{t})\) with \(p_{0}(\mathbf{x}_{0})\equiv p_{\text{data}}(\mathbf{x})\) and \(p_{T}(\mathbf{x}_{T})\approx\mathcal{N}(\mathbf{0},\mathbf{I})\). Under some regularity conditions Song et al. [15] showed that Equation (1.1) has a reverse process as time runs backwards from \(T\) to \(0\) with initial marginal distribution \(p_{T}(\mathbf{x}_{T})\) governed by

\[\mathrm{d}\mathbf{x}_{t}=\left[f(t)\mathbf{x}_{t}-g^{2}(t)\nabla_{\mathbf{x}} \log p_{t}(\mathbf{x}_{t})\right]\mathrm{d}t+g(t)\;\mathrm{d}\hat{\mathbf{w}}_ {t} \tag{1.2}\]

Figure 1: A high-level overview of the AdjointDEIS solver to the continuous adjoint equations for diffusion models. The sampling schedule consists of \(\{t_{n}\}_{n=0}^{N}\) timesteps for the diffusion model and \(\{\tilde{t}_{n}\}_{n=0}^{M}\) timesteps for AdjointDEIS. The gradients \(\mathbf{a}_{\mathbf{x}}(T)\) can be used to optimize \(\mathbf{x}_{T}\) to find some optimal \(\mathbf{x}_{T}^{*}\).

where \(\bar{\mathbf{w}}_{t}\) is the standard Wiener process as time runs backwards. Solving Equation (1.2) is what allows diffusion models to draw samples from \(p_{\text{data}}(\mathbf{x})\) by sampling \(p_{T}(\mathbf{x}_{T})\). The unknown term in Equation (1.2) is the _score function_\(\nabla_{\mathbf{x}}\log p_{t}(\mathbf{x}_{t})\), which in practice is modeled by a neural network that estimates the scaled score function, \(\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t)\approx-\sigma_{t}\nabla_{\mathbf{x}} \log p_{t}(\mathbf{x}_{t})\), or some closely related quantity like \(\mathbf{x}_{0}\)-prediction [1, 2, 20].

**Probability Flow ODE.** The practical choice of a step size when discretizing SDEs is limited by the randomness of the Wiener process as a large step size, _i.e._, a small number of steps, can cause non-convergence, particularly in high-dimensional spaces [16]. Sampling an equivalent Ordinary Differential Equation (ODE) over an SDE would enable faster sampling. Song et al. [15] showed there exists an Ordinary Differential Equation (ODE) whose marginal distribution at time \(t\) is identical to that of Equation (1.2) given as

\[\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}=f(t)\mathbf{x}_{t}-\frac{1}{2}g^{ 2}(t)\nabla_{\mathbf{x}}\log p_{t}(\mathbf{x}_{t}). \tag{1.3}\]

The ODE in Equation (1.3) is known as the _probability flow ODE_[15]. As the noise prediction network, \(\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t)\), is trained to model the scaled score function, Equation (1.3) can be parameterized as

\[\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}=f(t)\mathbf{x}_{t}+\frac{g^{2}(t )}{2\sigma_{t}}\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t) \tag{1.4}\]

w.r.t. the noise prediction network. For brevity, we refer to this as a diffusion ODE.

Although there exist several popular choices for the drift and diffusion coefficients, we opt to use the _de facto_ choice which is known as the Variance Preserving (VP) type diffusion SDE [1, 15, 21]. The coefficients for VP-type SDEs are given as

\[f(t)=\frac{\mathrm{d}\log\alpha_{t}}{\mathrm{d}t},\quad g^{2}(t)=\frac{ \mathrm{d}\sigma_{t}^{2}}{\mathrm{d}t}-2\frac{\mathrm{d}\log\alpha_{t}}{ \mathrm{d}t}\sigma_{t}^{2}, \tag{1.5}\]

which corresponds to sampling \(\mathbf{x}_{t}\) from the distribution \(q(\mathbf{x}_{t}\mid\mathbf{x}_{0})=\mathcal{N}(\alpha_{t}\mathbf{x}_{0}, \sigma_{t}^{2}\mathbf{I})\).

## 2 Adjoint Diffusion ODEs

**Problem statement.** Given the diffusion ODE in Equation (1.4), we wish to solve the following optimization problem:

\[\underset{\mathbf{x}_{T},\mathbf{z},\theta}{\arg\min}\ \mathcal{L}\bigg{(} \mathbf{x}_{T}+\int_{T}^{0}f(t)\mathbf{x}_{t}+\frac{g^{2}(t)}{2\sigma_{t}}\mathbf{ \epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)\ \mathrm{d}t\bigg{)}. \tag{2.1}\]

_I.e._, we seek to find the optimal \(\mathbf{x}_{T}\), \(\mathbf{z}\), and \(\theta\) that satisfies our guidance function \(\mathcal{L}\). _N.B._, the noise-prediction model is conditioned on additional information \(\mathbf{z}\).

Unlike GANs which can update the latent representation through GAN inversion [22, 23], as seen in Equation (2.1) diffusion models require more care as they model an ODE or SDE and require numerical solvers. Therefore, to update the latent representation, model parameters, and conditional information, we must backpropagate the gradient of loss defined on the output, \(\partial\mathcal{L}(\mathbf{x}_{0})/\partial\mathbf{x}_{0}\) through the entire ODE or SDE.

A key insight of this work is the connection between the adjoint ODE used in neural ODEs by Chen et al. [19] and specialized ODE/SDE solvers by [16, 17, 18] for diffusion models. It has been well observed that diffusion models are a type of neural ODE [24, 15]. Since a diffusion model can be thought of as a neural ODE, we can solve the continuous adjoint equations [25] to find useful gradients for guided generation. We can then exploit the unique structure of diffusion models to develop efficient bespoke ODE solvers for the continuous adjoint equations.

Let \(\mathbf{f}_{\theta}\) describe a parameterized neural field of the probability flow ODE, _i.e._, the R.H.S of Equation (1.4), defined as

\[\mathbf{f}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)=f(t)\mathbf{x}_{t}+\frac{g^{2}(t) }{2\sigma_{t}}\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t). \tag{2.2}\]

Then \(\mathbf{f}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)\) describes a neural ODE which admits an adjoint state, \(\mathbf{a}_{\mathbf{x}}\coloneqq\partial\mathcal{L}/\partial\mathbf{x}_{t}\) (and likewise for \(\mathbf{a}_{\mathbf{z}}(t)\) and \(\mathbf{a}_{\theta}(t)\)), which solve the continuous adjoint equations [25, Theorem 5.2] in the form of the following Initial Value Problem (IVP):

\[\mathbf{a}_{\mathbf{x}}(0) =\frac{\partial\mathcal{L}}{\partial\mathbf{x}_{0}}, \frac{\mathrm{d}\mathbf{a}_{\mathbf{x}}}{\mathrm{d}t}(t) =-\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{f}_{\theta}( \mathbf{x}_{t},\mathbf{z},t)}{\partial\mathbf{x}_{t}},\] \[\mathbf{a}_{\mathbf{z}}(0) =\mathbf{0}, \frac{\mathrm{d}\mathbf{a}_{\mathbf{z}}}{\mathrm{d}t}(t) =-\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{f}_{\theta}( \mathbf{x}_{t},\mathbf{z},t)}{\partial\mathbf{z}},\] \[\mathbf{a}_{\theta}(0) =\mathbf{0}, \frac{\mathrm{d}\mathbf{a}_{\theta}}{\mathrm{d}t}(t) =-\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{f}_{\theta}( \mathbf{x}_{t},\mathbf{z},t)}{\partial\theta}. \tag{2.3}\]

We refer to this system of equations in Equation (2.3) as the _adjoint diffusion ODE1_ as it describes the continuous adjoint equations for the empirical probability flow ODE.

Footnote 1: A more precise and technical name would be the _empirical adjoint probability flow ODE_; however, for brevity’s sake we prefer the adjoint diffusion ODE and leave the reader to infer from context that this describes an empirical model with learned score function.

_N.B._, in the literature of diffusion models, the sampling process is often done in reverse time _i.e._, the initial noise is \(\mathbf{x}_{T}\) and the final sample is \(\mathbf{x}_{0}\). Due to this convention, solving the adjoint diffusion ODE _backwards_ actually means integrating _forwards_ in time. Thus, while diffusion models learn to compute \(\mathbf{x}_{t}\) from \(\mathbf{x}_{s}\) with \(s>t\), the adjoint diffusion ODE seeks to compute \(\mathbf{a}_{\mathbf{x}}(s)\) from \(\mathbf{a}_{\mathbf{x}}(t)\).

### Simplified Formulation of the Empirical Adjoint Probability Flow ODE

We show that rather than treating \(\mathbf{f}_{\theta}\) as a black box, the specific structure of the probability flow ODE is carried over to the adjoint probability flow ODE, allowing the adjoint probability flow ODE to be simplified into a special exact formulation.

By evaluating the gradient of \(\mathbf{f}_{\theta}\) w.r.t. \(\mathbf{x}_{t}\) for each term in Equation (2.2) we can rewrite the adjoint diffusion ODE for \(\mathbf{a}_{\mathbf{x}}(t)\) in Equation (2.3) as

\[\frac{\mathrm{d}\mathbf{a}_{\mathbf{x}}}{\mathrm{d}t}(t)=-f(t)\mathbf{a}_{ \mathbf{x}}(t)-\frac{g^{2}(t)}{2\sigma_{t}}\mathbf{a}_{\mathbf{x}}(t)^{\top} \frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)}{\partial \mathbf{x}_{t}}. \tag{2.4}\]

Due to the gradient of the drift term in Equation (2.4), further manipulations are required to put the empirical adjoint probability flow ODE into a sufficiently "nice" form. We follow the approach used by [16; 18] to simplify the empirical probability flow ODE with the use of exponential integrators and a change of variables. By applying the integrating factor \(\exp\left(\int_{0}^{t}f(\tau)\ \mathrm{d}\tau\right)\) to Equation (2.4), we find:

\[\frac{\mathrm{d}}{\mathrm{d}t}\bigg{[}e^{\int_{0}^{t}f(\tau)\ \mathrm{d}\tau} \mathbf{a}_{\mathbf{x}}(t)\bigg{]}=-e^{\int_{0}^{t}f(\tau)\ \mathrm{d}\tau}\frac{g^{2}(t)}{2\sigma_{t}}\mathbf{a}_{\mathbf{x}}(t)^{ \top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)}{ \partial\mathbf{x}_{t}}. \tag{2.5}\]

Then, the exact solution at time \(s\) given time \(t<s\) is found to be

\[\mathbf{a}_{\mathbf{x}}(s)=\underbrace{e^{\int_{s}^{t}f(\tau)\ \mathrm{d}\tau}\mathbf{a}_{\mathbf{x}}(t)}_{\text{linear}}-\underbrace{\int_{t}^{s}e^{\int_{s}^{u}f(\tau)\ \mathrm{d}\tau}\frac{g^{2}(u)}{2\sigma_{u}}\mathbf{a}_{\mathbf{x}}(u)^{\top} \frac{\mathbf{\epsilon}_{\theta}(\mathbf{x}_{u},\mathbf{z},u)}{\partial\mathbf{x}_{u}}\ \mathrm{d}u}_{\text{non-linear}}. \tag{2.6}\]

Like with solvers for diffusion models which leverage exponential integrators, we are able to transform the adjoint diffusion ODE into a non-stiff form by separating the linear and non-linear component. Moreover, we can compute the linear in closed form, thereby _eliminating_ the discretization error in the linear term. However, we still need to approximate the non-linear term which consists of a difficult integral about the complex noise-prediction model. This is where the insight of Lu et al. [16] to integrate in the log-SNR domain becomes invaluable. Let \(\lambda_{t}:=\log(\alpha_{t}/\sigma_{t})\) be one half of the log-SNR. Then, with using this new variable and computing the drift and diffusion coefficients in closed form, we can rewrite Equation (2.6) as

\[\mathbf{a}_{\mathbf{x}}(s)=\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a}_{\mathbf{x}}(t)+\frac{1}{\alpha_{s}}\int_{t}^{s}\alpha_{u}\sigma_{u}\frac{\mathrm{d}\lambda_{u}}{\mathrm{d}u}\mathbf{a}_{\mathbf{x}}(u)^{\top}\frac{\mathbf{\epsilon}_{\theta}(\mathbf{x}_{u},\mathbf{z},u)}{\partial\mathbf{x}_{u}}\ \mathrm{d}u. \tag{2.7}\]

As \(\lambda_{t}\) is a strictly decreasing function w.r.t. \(t\) it therefore has an inverse function \(t_{\lambda}\) that satisfies \(t_{\lambda}(\lambda_{t})=t\), and, with abuse of notation, we let \(\mathbf{x}_{\lambda}\coloneqq\mathbf{x}_{t_{\lambda}(\lambda)}\), \(\mathbf{a}_{\mathbf{x}}(\lambda)\coloneqq\mathbf{a}_{\mathbf{x}}(t_{\lambda}(\lambda))\), \(\&c.\) and let the reader infer from context if the function is mapping the log-SNR back into the time domain or already in the time domain. Then by rewriting Equation (2.7) as an exponentially weighted integral and performing an analogous derivation for \(\mathbf{a}_{\mathbf{z}}(t)\) and \(\mathbf{a}_{\theta}(t)\), we arrive at the following.

**Proposition 2.1** (Exact solution of adjoint diffusion ODEs).: _Given initial values \([\mathbf{a}_{\mathbf{x}}(t),\mathbf{a}_{\mathbf{z}}(t),\mathbf{a}_{\theta}(t)]\) at time \(t\in(0,T)\), the solution \([\mathbf{a}_{\mathbf{x}}(s),\mathbf{a}_{\mathbf{z}}(s),\mathbf{a}_{\theta}(s)]\) at time \(s\in(t,T]\) of adjoint diffusion ODEs in Equation (2.3) is_

\[\mathbf{a}_{\mathbf{x}}(s) =\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a}_{\mathbf{x}}(t)+\frac{1 }{\alpha_{s}}\int_{\lambda_{t}}^{\lambda_{s}}\alpha_{\lambda}^{2}e^{-\lambda} \mathbf{a}_{\mathbf{x}}(\lambda)^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}( \mathbf{x}_{\lambda},\mathbf{z},\lambda)}{\partial\mathbf{x}_{\lambda}}\; \mathrm{d}\lambda, \tag{2.8}\] \[\mathbf{a}_{\mathbf{z}}(s) =\mathbf{a}_{\mathbf{z}}(t)+\int_{\lambda_{t}}^{\lambda_{s}} \alpha_{\lambda}e^{-\lambda}\mathbf{a}_{\mathbf{x}}(\lambda)^{\top}\frac{ \partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{\lambda},\mathbf{z},\lambda)}{ \partial\mathbf{z}}\;\mathrm{d}\lambda,\] (2.9) \[\mathbf{a}_{\theta}(s) =\mathbf{a}_{\theta}(t)+\int_{\lambda_{t}}^{\lambda_{s}}\alpha_{ \lambda}e^{-\lambda}\mathbf{a}_{\mathbf{x}}(\lambda)^{\top}\frac{\partial\mathbf{ \epsilon}_{\theta}(\mathbf{x}_{\lambda},\mathbf{z},\lambda)}{\partial\theta}\; \mathrm{d}\lambda. \tag{2.10}\]

The complete derivations of Proposition 2.1 can be found in Appendix B.1.

There is a nice symmetry between Equations (2.8) to (2.10), while the adjoint of the solution trajectories evolves with a weighting of \(\alpha_{t}/\alpha_{s}\) in the linear term and the integral term is weighted by \(\alpha_{t}^{2}/\alpha_{s}^{2}\) reflecting the double partial \(\partial\mathbf{x}_{t}\) in the adjoint and Jacobian terms. Conversely, the adjoint state for the conditional information and model parameters evolves with no weighting on the linear term and the integral is only weighted by \(\alpha_{t}/\alpha_{s}\). This follows from the vector fields being independent of \(\mathbf{a}_{\mathbf{z}}\) and \(\mathbf{a}_{\theta}\). These equations, while reflecting the special nature of this formulation of diffusion models, also have an appealing parallel with the exact solution for diffusion ODEs Lu et al. [16, Proposition 3.1].

### Numerical Solvers for AdjointDEIS

The numerical solver for the adjoint empirical probability flow ODE, now in light of Equation (2.8), only needs to focus on approximating the exponentially weighted integral of \(\mathbf{\epsilon}_{\theta}\) from \(\lambda_{t}\) to \(\lambda_{s}\), a well-studied problem in the literature on exponential integrators [26, 27]. To approximate this integral, we evaluate the Taylor expansion of the Jacobian vector product to further simplify the ODE. For notational convenience let \(\mathbf{V}(\mathbf{x};t)\) denote the scaled vector-Jacobian product of the adjoint state \(\mathbf{a}_{\mathbf{x}}(t)\) and the gradient of the model w.r.t. \(\mathbf{x}_{t}\), _i.e._,

\[\mathbf{V}(\mathbf{x};t)=\alpha_{t}^{2}\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac {\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)}{\partial\mathbf{ x}_{t}}, \tag{2.11}\]

and likewise we let \(\mathbf{V}^{(n)}(\mathbf{x};\lambda)\) denote the \(n\)-th derivative w.r.t. to \(\lambda\). For \(k\geq 1\), the \((k-1)\)-th Taylor expansion at \(\lambda_{t}\) is

\[\mathbf{V}(\mathbf{x};\lambda)=\sum_{n=0}^{k-1}\frac{(\lambda-\lambda_{t})^{n }}{n!}\mathbf{V}^{(n)}(\mathbf{x};\lambda_{t})+\mathcal{O}((\lambda-\lambda_{t })^{k}). \tag{2.12}\]

Plugging this expansion into Equation (2.8) and letting \(h=\lambda_{s}-\lambda_{t}\) yields

\[\mathbf{a}_{\mathbf{x}}(s)=\underbrace{\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a }_{\mathbf{x}}(t)}_{\begin{subarray}{c}\text{Linear term}\\ \text{Exactly computed}\end{subarray}}+\underbrace{\frac{1}{\alpha_{s}}\sum_{n=0}^{k-1 }\mathbf{V}^{(n)}(\mathbf{x};\lambda_{t})}_{\begin{subarray}{c}\text{ Derivatives}\\ \text{Approximated}\end{subarray}}\underbrace{\int_{\lambda_{t}}^{\lambda_{s}}\frac{( \lambda-\lambda_{t})^{n}}{n!}e^{-\lambda}\;\mathrm{d}\lambda}_{\begin{subarray}{c}\text{ Coefficients}\\ \text{Analytically computed}\end{subarray}}+\underbrace{\mathcal{O}(h^{k+1})}_{\begin{subarray}{c}\text{Higher order errors}\\ \text{Omitted}\end{subarray}}. \tag{2.13}\]

With this expansion, the number of terms which need to be estimated is further reduced as the exponentially weighted integral \(\int_{\lambda_{t}}^{\lambda_{s}}\frac{(\lambda-\lambda_{t})^{n}}{n!}e^{-\lambda}\; \mathrm{d}\lambda\) can be solved **analytically** by applying \(n\) times integration-by-parts [16, 28]. Therefore, the only errors in solving this ODE occur in the approximation of the \(n\)-th order total derivatives of the vector-Jacobian product and the higher-order error terms \(\mathcal{O}(h^{k+1})\). By dropping the \(\mathcal{O}(h^{k+1})\) error term and approximating the first \((k-1)\)-th derivatives of the vector-Jacobian product, we can derive \(k\)-th order solvers for adjoint diffusion ODEs. We decide to name such solvers as _Adjoint Diffusion Exponential Integrator Sampler (AdjointDEIS)_ reflecting our use of the _exponential integrator_ to simplify the ODEs and pay homage to DEIS from [18] that explored the use of exponential integrators for diffusion ODEs. Consider the case of \(k=1\), by dropping the error term \(\mathcal{O}(h^{2})\) we construct the AdjointDEIS-1 solver with the following algorithm.

**AdjointDEIS-1.** Given an initial augmented adjoint state \([\mathbf{a}_{\mathbf{x}}(t),\mathbf{a}_{\mathbf{z}}(t),\mathbf{a}_{\theta}(t)]\) at time \(t\in(0,T)\), the solution \([\mathbf{a}_{\mathbf{x}}(s),\mathbf{a}_{\mathbf{z}}(s),\mathbf{a}_{\theta}(s)]\) at time \(s\in(t,T]\) is approximated by

\[\mathbf{a}_{\mathbf{x}}(s) =\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a}_{\mathbf{x}}(t)+\sigma_{ s}(e^{h}-1)\frac{\alpha_{t}^{2}}{\alpha_{s}^{2}}\mathbf{a}_{\mathbf{x}}(t)^{ \top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)}{ \partial\mathbf{x}_{t}},\] \[\mathbf{a}_{\mathbf{z}}(s) =\mathbf{a}_{\mathbf{z}}(t)+\sigma_{s}(e^{h}-1)\frac{\alpha_{t}}{ \alpha_{s}}\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{\epsilon}_{ \theta}(\mathbf{x}_{t},\mathbf{z},t)}{\partial\mathbf{z}},\] \[\mathbf{a}_{\theta}(s) =\mathbf{a}_{\theta}(t)+\sigma_{s}(e^{h}-1)\frac{\alpha_{t}}{ \alpha_{s}}\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{\epsilon}_{ \theta}(\mathbf{x}_{t},\mathbf{z},t)}{\partial\theta}. \tag{2.14}\]

Higher-order expansions of Equation (2.12) require estimations of the \(n\)-th order derivatives of the vector Jacobian product which can be approximated via _multi-step_ methods, such as Adams-Bashforth methods [29]. This has the added benefit of reduced computational overhead, as the multi-step method just reuses previous values to approximate the higher-order derivatives. Moreover, multi-step methods are empirically more efficient than single-step methods [29]. Combining the Taylor expansions in Equation (2.12) with techniques for designing multi-step solvers, we propose a novel multi-step second-order solver for the adjoint empirical probability flow ODE which we call _AdjointDEIS-2M_. This algorithm combines the previous values of the vector Jacobian product at time \(t\) and time \(r\) to predict \(\mathbf{a}_{s}\)_without_ any additional intermediate values.

**AdjointDEIS-2M.** We assume having a previous solution \(\mathbf{a}_{\mathbf{x}}(r)\) and model output \(\mathbf{\epsilon}_{\theta}(\mathbf{x}_{r},\mathbf{z},r)\) at time \(r<t<s\), let \(\rho\) denote \(\rho=\frac{\lambda_{t}-\lambda_{r}}{h}\). Then the solution \(\mathbf{a}_{s}\) at time \(s\) to Equation (2.4) is estimated to be

\[\mathbf{a}_{\mathbf{x}}(s)=\frac{\alpha_{t}}{\alpha_{s}}\mathbf{ a}_{\mathbf{x}}(t) +\sigma_{s}(e^{h}-1)\frac{\alpha_{t}^{2}}{\alpha_{s}^{2}}\mathbf{ a}_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t}, \mathbf{z},t)}{\partial\mathbf{x}_{t}}\] \[+\sigma_{s}\frac{e^{h}-1}{2\rho}\bigg{(}\frac{\alpha_{t}^{2}}{ \alpha_{s}^{2}}\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{\epsilon}_{ \theta}(\mathbf{x}_{t},\mathbf{z},t)}{\partial\mathbf{x}_{t}}-\frac{\alpha_{r} ^{2}}{\alpha_{s}^{2}}\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{ \epsilon}_{\theta}(\mathbf{x}_{r},\mathbf{z},r)}{\partial\mathbf{x}_{r}} \bigg{)}. \tag{2.15}\]

For brevity, we omit the details of the AdjointDEIS-2M solver for \(\mathbf{a}_{\mathbf{z}}(t)\) and \(\mathbf{a}_{\theta}(t)\); rather, we provide the complete derivation and details in Appendix B. Likewise, the full algorithm can be found in Appendix G.1. The advantage of a higher-order solver is that it is generally more efficient, requiring fewer steps due to its higher convergence order. We show that AdjointDEIS-\(k\) is a \(k\)-th order solver, as stated in the following theorem. The proof is in Appendix C.

**Theorem 2.1** (AdjointDEIS-\(k\) as a \(k\)-th order solver).: _Assume the function \(\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)\) and its associated vector-Jacobian products follow the regularity conditions detailed in Appendix C, then for \(k=1,2\), AdjointDEIS-\(k\) is a \(k\)-th order solver for adjoint diffusion ODEs, i.e., for the sequence \(\{\tilde{\mathbf{a}}_{\mathbf{x}}(t_{i})\}_{i=1}^{M}\) computed by AdjointDEIS-\(k\), the global truncation error at time \(T\) satisfies \(\tilde{\mathbf{a}}_{\mathbf{x}}(t_{M})-\mathbf{a}_{\mathbf{x}}(T)=\mathcal{O}(h _{max}^{2})\), where \(h_{max}=\max_{1\leq j\leq M}(\lambda_{t_{i}}-\lambda_{t_{i-1}})\). Likewise, AdjointDEIS-\(k\) is a \(k\)-th order solver for the estimated gradients w.r.t. \(\mathbf{z}\) and \(\theta\)._

As previous work has shown that higher-order solvers may be unsuitable for large guidance scales [16, 17, 18] we do explicitly construct or analyze any solvers for \(k>2\) and leave such explorations for future study.

### Scheduled Conditional Information

Thus far, we have held the conditional information constant across time, _i.e._, at each time \(t\in[0,T]\) the conditional information supplied to the neural network is \(\mathbf{z}\). What if, however, we had some scheduled conditional information \(\mathbf{z}_{t}\)? We show that with some mild assumptions, using scheduled conditional information \(\mathbf{z}_{t}\) does not actually change the continuous adjoint equation for \(\mathbf{z}_{t}\) from the equations derived from \(\mathbf{z}\) sans a substitution of \(\mathbf{z}_{t}\) with \(\mathbf{z}\).

While motivated by the case of scheduled conditional information in guided generation with diffusion models, this result applies to neural ODEs more generally, which could open future research directions. We state this result more formally in Theorem 2.2 with the proof in Appendix D. Note, as this applies more generally than to just AdjointDEIS, so we express this result for some arbitrary neural ODE with vector field \(\mathbf{f}_{\theta}(\mathbf{x}_{t},\mathbf{z}_{t},t)\) and use the forward-time flow convention rather than the reverse-time convention of diffusion models.

**Theorem 2.2**.: _Suppose there exists a function \(\mathbf{z}:[0,T]\to\mathbb{R}^{z}\) which can be defined as a cadlag2 piecewise function where \(\mathbf{z}\) is continuous on each partition of \([0,T]\) given by \(\Pi=\{0=t_{0}<t_{1}<\cdots<t_{n}=T\}\) and whose right derivatives exist for all \(t\in[0,T]\). Let \(\boldsymbol{f}_{\theta}:\mathbb{R}^{d}\times\mathbb{R}^{z}\times\mathbb{R}\to \mathbb{R}^{d}\) be continuous in \(t\), uniformly Lipschitz in \(\mathbf{x}\), and continuously differentiable in \(\mathbf{x}\). Let \(\mathbf{x}:\mathbb{R}\to\mathbb{R}^{d}\) be the unique solution for the ODE_

Footnote 2: French: _continue à gauche, limite à detroite_.

\[\frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}(t)=\boldsymbol{f}_{\theta}(\mathbf{x }_{t},\mathbf{z}_{t},t),\]

_with initial condition \(\mathbf{x}(0)=\mathbf{x}_{0}\). Let \(\mathcal{L}:\mathbb{R}^{d}\to\mathbb{R}\) be a scalar-valued loss function defined on the output of the neural ODE. Then \(\partial\mathcal{L}/\partial\mathbf{z}(t)\coloneqq\mathbf{a}_{\mathbf{z}}(t)\) and there exists a unique solution \(\mathbf{a}_{\mathbf{z}}:\mathbb{R}\to\mathbb{R}^{z}\) to the following IVP:_

\[\mathbf{a}_{\mathbf{z}}(T)=\mathbf{0},\qquad\frac{\mathrm{d}\mathbf{a}_{ \mathbf{z}}}{\mathrm{d}t}(t)=-\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{\partial \boldsymbol{f}_{\theta}(\mathbf{x}_{t},\mathbf{z}_{t},t)}{\partial\mathbf{z}_ {t}}.\]

## 3 Adjoint Diffusion SDEs

As recent work [30; 31] has shown, diffusion SDEs have useful properties over probability flow ODEs for image manipulation and editing. In particular, it has been shown that probability flow ODEs are invariant in Nie et al. [31, Theorem 3.2] and that diffusion SDEs are contractive in Nie et al. [31, Theorem 3.1], _i.e._, any gap in the mismatched prior distributions \(p_{t}(\mathbf{x}_{t})\) and \(\tilde{p}_{t}(\mathbf{x}_{t})\) for the true distribution \(p_{t}\) and edited distribution \(\tilde{p}_{t}\) will remain between \(p_{0}(\mathbf{x}_{0})\) and \(\tilde{p}_{0}(\mathbf{x}_{0})\), whereas for diffusion SDEs the gap can be reduced between \(\tilde{p}_{t}(\mathbf{x}_{t})\) and \(p_{t}(\mathbf{x}_{t})\) as \(t\) tends towards \(0\). Motivated by this reasoning, we present a framework for solving the adjoint diffusion SDE using exponential integrators.

The diffusion SDE with noise prediction model is given by

\[\mathrm{d}\mathbf{x}_{t}=\left[f(t)\mathbf{x}_{t}+\frac{g^{2}(t)}{\sigma_{t}} \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)\right]\,\mathrm{d} t+g(t)\;\mathrm{d}\tilde{\mathbf{w}}_{t}, \tag{3.1}\]

where '\(\mathrm{d}t\)' is an infinitesimal _negative_ timestep. Note how the drift term of the SDE looks remarkably similar to the probability flow ODE sans a missing factor of \(1/2\) in front of the noise prediction model. This is due to differing manipulations of the forward Kolomogorov equations--which describe the evolution of \(p_{t}(\mathbf{x}_{t})\)--used by Anderson [32] to derive the reverse-time SDE and later by Song et al. [15] to derive the probability-flow ODE. This connection is _very_ important as it enables one to simplify the AdjointDEIS solvers for the adjoint diffusion SDE.

We show that for the special case of Stratonovich SDEs 3 with a diffusion coefficient \(\boldsymbol{g}(t)\) which does not depend on the process state \(\mathbf{x}_{t}\), then the adjoint process has a unique strong solution that evolves with what is essentially an ODE. Intuitively, this tracks as the stochastic term \(\boldsymbol{g}(t)\circ\mathrm{d}\mathbf{w}_{t}\) has nothing to do with \(\mathbf{x}_{t}\). We state this observation somewhat informally in the following theorem. The proof can be found in Appendix E.

Footnote 3: The notation ‘\(\circ\mathrm{d}\mathbf{w}_{t}\)’ denotes Stratonovich integration which differs from integration in the Ito sense. More details on this are found in Appendix E.

**Theorem 3.1**.: _Let \(\boldsymbol{f}:\mathbb{R}^{d}\times\mathbb{R}\to\mathbb{R}^{d}\) be in \(\mathcal{C}^{\infty,1}_{b}\) and \(\boldsymbol{g}:\mathbb{R}\to\mathbb{R}^{d\times w}\) be in \(\mathcal{C}^{1}_{b}\). Let \(\mathcal{L}:\mathbb{R}^{d}\to\mathbb{R}\) be a scalar-valued differentiable function. Let \(\mathbf{w}_{t}:[0,T]\to\mathbb{R}^{w}\) be a \(w\)-dimensional Wiener process. Let \(\mathbf{x}:[0,T]\to\mathbb{R}^{d}\) solve the Stratonovich SDE_

\[\mathrm{d}\mathbf{x}_{t}=\boldsymbol{f}(\mathbf{x}_{t},t)\;\mathrm{d}t+ \boldsymbol{g}(t)\circ\mathrm{d}\mathbf{w}_{t},\]

_with initial condition \(\mathbf{x}_{0}\). Then the adjoint process \(\mathbf{a}_{\mathbf{x}}(t)\coloneqq\partial\mathcal{L}(\mathbf{x}_{T})/ \partial\mathbf{x}_{t}\) is a strong solution to the backwards-in-time ODE_

\[\mathrm{d}\mathbf{a}_{\mathbf{x}}(t)=-\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{ \partial\boldsymbol{f}}{\partial\mathbf{x}_{t}}(\mathbf{x}_{t},t)\;\mathrm{d}t. \tag{3.2}\]

This is a boon for us, as diffusion models use only a mere scalar diffusion coefficient, \(g(t)\). Therefore, the continuous adjoint equations for the diffusion SDE just simplify to an ODE. Not only that, but as mentioned before, the drift term of the diffusion SDE and probability flow ODE differ only by a factor of \(2\) in the term with the noise prediction network. As only the drift term of the diffusionSDE is used when constructing the continuous adjoint equations, it follows that the only difference between the continuous adjoint equations for the probability flow ODE and diffusion SDE is a factor of 2. Therefore, the exact solutions are given by:

**Proposition 3.1** (Exact solution of adjoint diffusion SDEs).: _Given initial values \([\mathbf{a}_{\mathbf{x}}(t),\mathbf{a}_{\mathbf{z}}(t),\mathbf{a}_{\theta}(t)]\) at time \(t\in(0,T)\), the solution \([\mathbf{a}_{\mathbf{x}}(s),\mathbf{a}_{\mathbf{z}}(s),\mathbf{a}_{\theta}(s)]\) at time \(s\in(t,T]\) of adjoint diffusion SDEs is_

\[\mathbf{a}_{\mathbf{x}}(s) =\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a}_{\mathbf{x}}(t)+\frac{2 }{\alpha_{s}}\int_{\lambda_{t}}^{\lambda_{s}}\alpha_{\lambda}^{2}e^{-\lambda} \mathbf{a}_{\mathbf{x}}(\lambda)^{\top}\frac{\mathbf{\epsilon}_{\theta}(\mathbf{x} _{\lambda},\mathbf{z},\lambda)}{\partial\mathbf{x}_{\lambda}}\;\mathrm{d}\lambda, \tag{3.3}\] \[\mathbf{a}_{\mathbf{z}}(s) =\mathbf{a}_{\mathbf{z}}(t)+2\int_{\lambda_{t}}^{\lambda_{s}} \alpha_{\lambda}e^{-\lambda}\mathbf{a}_{\mathbf{x}}(\lambda)^{\top}\frac{ \partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{\lambda},\mathbf{z},\lambda)}{ \partial\mathbf{z}}\;\mathrm{d}\lambda,\] (3.4) \[\mathbf{a}_{\theta}(s) =\mathbf{a}_{\theta}(t)+2\int_{\lambda_{t}}^{\lambda_{s}}\alpha_ {\lambda}e^{-\lambda}\mathbf{a}_{\mathbf{x}}(\lambda)^{\top}\frac{\partial\mathbf{ \epsilon}_{\theta}(\mathbf{x}_{\lambda},\mathbf{z},\lambda)}{\partial\theta}\; \mathrm{d}\lambda. \tag{3.5}\]

**Remark 3.1**.: _While the adjoint diffusion SDEs evolve with an ODE, the same cannot be said for the underlying state, \(\mathbf{x}_{t}\). Rather this evolves with a backwards SDE (more details in Appendix E) which requires the **same** realization of the Wiener process used to sample the image as the one used in the backwards SDE._

### Solving Backwards Diffusion SDEs

Lu et al. [17] propose the following first-order solver for diffusion SDEs

\[\mathbf{x}_{t}=\frac{\alpha_{t}}{\alpha_{s}}\mathbf{x}_{s}-2\sigma_{t}(e^{h}- 1)\mathbf{\epsilon}_{\theta}(\mathbf{x}_{s},s)+\sigma_{t}\sqrt{e^{2h}-1}\mathbf{ \epsilon}_{s}, \tag{3.6}\]

where \(\mathbf{\epsilon}_{s}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\). To solve the SDE backwards in time, we follow the approach initially proposed by Wu and la Torre [33] and used by later works [31]. Given a particular realization of the Wiener process that admits \(\mathbf{x}_{t}\sim\mathcal{N}(\alpha_{t}\mathbf{x}_{0}\mid\sigma_{t}^{2} \mathbf{I})\), then for two samples \(\mathbf{x}_{t}\) and \(\mathbf{x}_{s}\) the noise \(\mathbf{\epsilon}_{s}\) can be calculated by rearranging Equation (3.6) to find

\[\mathbf{\epsilon}_{s}=\frac{\mathbf{x}_{t}-\frac{\alpha_{t}}{\alpha_{s}}\mathbf{x} _{s}+2\sigma_{t}(e^{h}-1)\mathbf{\epsilon}_{\theta}(\mathbf{x}_{s},\mathbf{z},s)} {\sigma_{t}\sqrt{e^{2h}-1}} \tag{3.7}\]

With this the sequence \(\{\mathbf{\epsilon}_{t_{i}}\}_{i=1}^{N}\) of added noises can be calculated which will **exactly** reconstruct the original input from the initial realization of the Wiener process. This technique is referred to as _Cycle-SDE_ after the CycleDiffusion paper [33].

## 4 Related Work

Our proposed solutions can be viewed as a _training-free_ method for guided generation. As an active area of research, there have been several proposed approaches to the problem of training-free guided generation, which either dynamically optimize the solution trajectory during sampling [34; 35; 36], or optimize the whole solution trajectory [37; 38; 39; 40]. Our solutions fall into the latter category of optimizing the whole solution trajectory along with additional conditional information.

While Nie et al. [37] explored the use of the continuous adjoint equations to optimize the solution trajectories of diffusion SDEs they don't consider the ODE case and make use of the _special_ structure of diffusion SDEs to simplify the continuous adjoint equations as we did. Recent work by Pan et al. [39] explore the using continuous adjoint equations for guided generation but does not consider the SDE case and uses a different scheme to simplify the continuous adjoint equations. We provide a more detailed comparison against these approaches and further discussion on related methods in Appendix A.

## 5 Experiments

To illustrate the efficacy of our technique, we examine an application of guided generation in the form of the face morphing attack. The face morphing attack is a new emerging attack on Face Recognition (FR) systems. This attack works by creating a singular morphed face image \(\mathbf{x}_{0}^{(ab)}\) that shares biometric information with the two contributing faces \(\mathbf{x}_{0}^{(a)}\) and \(\mathbf{x}_{0}^{(b)}\)[41, 42, 43]. A successfully created morphed face image can trigger a false accept with either of the two contributing identities in the targeted Face Recognition (FR) system, see Figure 2 for an illustration. Recent work in this space has explored the use of diffusion models to generate these powerful attacks [44, 41, 45]. All prior work on diffusion-based face morphing used a pre-trained diffusion autoencoder [46] trained on the FFHQ [47] dataset at a \(256\times 256\) resolution. We illustrate the use of AdjointDEIS solvers by modifying the Diffusion Morph (DiM) architecture proposed by Blasingame and Liu [41] to use the AdjointDEIS solvers to find the optimal initial noise \(\mathbf{x}_{T}^{(ab)}\) and conditional \(\mathbf{z}_{ab}\). The AdjointDEIS solvers are used to calculate the gradients with respect to the identity loss [45] defined as

\[\mathcal{L}_{ID}=d(v_{ab},v_{a})+d(v_{ab},v_{b}),\quad\mathcal{L }_{diff}=\big{|}d(v_{ab},v_{a})-d(v_{ab},v_{b})\big{)}\big{|}, \tag{5.1}\] \[\mathcal{L}_{ID}^{\star}=\mathcal{L}_{ID}+\mathcal{L}_{diff},\]

where \(v_{a}=F(\mathbf{x}_{0}^{(a)}),v_{b}=F(\mathbf{x}_{0}^{(b)}),v_{ab}=F(\mathbf{ x}_{0}^{(ab)})\), and \(F:\mathcal{X}\to V\) is an FR system which embeds images into a vector space \(V\) which is equipped with a measure of distance, \(d\). We used the ArcFace [48] FR system for identity loss.

We compare against three preexisting DiM methods, the original DiM algorithm [41], Fast-DiM [44], and Morph-PIPE [45] as well as a GAN-inversion-based face morphing attack, MIPGAN-I and MIPGAN-II [49] based on the StyleGAN [47] and StyleGAN2 [50] architectures respectively. Fast-DiM improves DiM by using higher-order ODE solvers to decrease the number of sampling steps required to create a morph. Morph-PIPE performs a very simple version of guided generation by generating a large batch of morphed images derived from a discrete set of interpolations between \(\mathbf{x}_{T}^{(a)}\) and \(\mathbf{x}_{T}^{(b)}\), and \(\mathbf{z}_{a}\) and \(\mathbf{z}_{b}\). For reference purposes, we compare against a reference GAN-based method [49] which uses GAN-inversion w.r.t.to the identity loss to find the optimal morphed face, and we include prior state-of-the-art Webmorph, a commercial off-the-shelf system [51].

We run our experiments on SYN-MAD 2022 [51] morphed pairs that are constructed from the Face Research Lab London dataset [52], more details in Appendix H.4. The morphed images are evaluated against three FR systems, the ArcFace [48], ElasticFace [53], and AdaFace [54] models;

Figure 3: Comparison of DiM morphs on the FRLL dataset. From left to right, identity \(a\), DiM-A, Fast-DiM, Morph-PIPE, AdjointDEIS (ODE), AdjointDEIS (SDE), and identity \(b\).

Figure 2: Example of guided morphed face generation with AdjointDEIS on the FRLL dataset.

further details are found in Appendix H.5. To measure the efficacy of a morphing attack, the Mated Morph Presentation Match Rate (MMPMR) metric [55] is used. The MMPMR metric as proposed by Scherhag et al. [55] is defined as

\[M(\delta)=\frac{1}{M}\sum_{m=1}^{M}\left\{\left[\min_{\,n\in\{1, \ldots,N_{m}\}}S_{m}^{n}\right]>\delta\right\} \tag{5.2}\]

where \(\delta\) is the verification threshold, \(S_{m}^{n}\) is the similarity score of the \(n\)-th subject of morph \(m\), \(N_{m}\) is the total number of contributing subjects to morph \(m\), and \(M\) is the total number of morphed images.

In our experiments, we used a learning rate of \(0.01\), \(N=20\) sampling steps, \(M=20\) steps for AdjointDEIS, and 50 optimization steps for gradient descent. For the sampling process we used the DDIM solver [2], a widely used first-order solver. Following [56] we observed that using recorded values of \(\{\mathbf{x}_{t_{i}}\}_{i=1}^{N}\) for the backward pass improved performance. Note, this does not mean we stored the vector-Jacobians or any other internal states of the neural network. Moreover, due to our use of Cycle-SDE this choice was mandated for the SDE case. We discussion this decision further in Appendix F.1.

In Table 1 we present the effectiveness of the morphing attacks against the three FR systems. Guided generation with AdjointDEIS massively increases the performance of DiM, supplanting the old state-of-the-art for face morphing. Interestingly, the SDE variant did not fare as well as the ODE variant. This is likely due to the difficulty in discretizing SDEs with large step sizes [15, 16, 17]. We present further results in Appendix F that explore the impact of the choice of learning rate and the number of discretization steps for AdjointDEIS.

## 6 Conclusion

We present a unified view on guided generation by updating latent, conditional, and model information of diffusion models with a guidance function using the continuous adjoint equations. We propose AdjointDEIS, a family of solvers for the continuous adjoint equations of diffusion models. We exploit the unique construction of diffusion models to create efficient numerical solvers by using exponential integrators. We prove the convergence order of solvers and show that the continuous adjoint equations for diffusion SDEs evolve with an ODE. Furthermore, we show how to handle conditional information that is scheduled in time, further expanding the generalizability of the proposed technique. Our results in face morphing show that the gradients produced by AdjointDEIS can be used for guided generation tasks.

**Limitations.** There are several limitations. Empirically, we only explored a small subset of the true potential AdjointDEIS by evaluating on a single scenario, _i.e._, face morphing. Likewise, we only explored a few different hyperparameter options. In particular, we did not explore much the impact of the number of optimization steps and the number of sampling steps for diffusion SDEs on the visual quality of the generated face morphs.

**Broader Impact.** Guided generation techniques can be misused for a variety of harmful purposes. In particular, our approach provides a powerful tool for adversarial attacks. However, better knowledge of such techniques should hopefully help direct research in hardening systems against such kinds of attacks.

\begin{table}
\begin{tabular}{l r r r r} \hline \hline  & & \multicolumn{3}{c}{**MMPMR**(\(\uparrow\))} \\ \cline{3-5}
**Morphing Attack** & **NFE**(\(\downarrow\)) & **AdaFace** & **ArcFace** & **ElasticFace** \\ \hline Webmorph [51] & - & 97.96 & 96.93 & 98.36 \\ MIPGAN-I [49] & - & 72.19 & 77.51 & 66.46 \\ MIPGAN-II [49] & - & 70.55 & 72.19 & 65.24 \\ DiM-A [41] & 350 & 92.23 & 90.18 & 93.05 \\ Fast-DiM [44] & 300 & 92.02 & 90.18 & 93.05 \\ Morph-PIPE [45] & 2350 & 95.91 & 92.84 & 95.5 \\
**DiM + AdjointDEIS-1 (ODE)** & 2250 & **99.8** & **98.77** & **99.39** \\
**DiM + AdjointDEIS-1 (SDE)** & 2250 & 98.57 & 97.96 & 97.75 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Vulnerability of different FR systems across different morphing attacks on the SYN-MAD 2022 dataset. FMR = 0.1%.

## Acknowledgments

The authors would like to thank Fangyikang Wang for his helpful feedback on the Taylor expansion of the vector Jacobians. The authors would also like to acknowledge the fruitful discussions with Pierre Marion and Quentin Berthet on using the adjoint methods from a bilevel optimization perspective.

## References

* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 6840-6851. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf).
* Song et al. [2021] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=St1giarCHLP](https://openreview.net/forum?id=St1giarCHLP).
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, June 2022.
* Ramesh et al. [2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. _arXiv e-prints_, art. arXiv:2204.06125, April 2022. doi: 10.48550/arXiv.2204.06125.
* Saharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 36479-36494. Curran Associates, Inc., 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf).
* Liu et al. [2023] Haobe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D. Plumbley. AudioLDM: Text-to-Audio Generation with Latent Diffusion Models. _arXiv e-prints_, art. arXiv:2301.12503, January 2023. doi: 10.48550/arXiv.2301.12503.
* Hawley [2024] Scott H. Hawley. Pictures of midi: Controlled music generation via graphical prompts for image-based diffusion inpainting, 2024. URL [https://arxiv.org/abs/2407.01499](https://arxiv.org/abs/2407.01499).
* Blattmann et al. [2023] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. _arXiv e-prints_, art. arXiv:2304.08818, April 2023. doi: 10.48550/arXiv.2304.08818.
* Karras et al. [2022] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In _Proc. NeurIPS_, 2022.
* Ruiz et al. [2022] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. _arXiv preprint arXiv:2208.12242_, 2022.
* Gal et al. [2022] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. _arXiv e-prints_, art. arXiv:2208.01618, August 2022. doi: 10.48550/arXiv.2208.01618.
* Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.

* Ho and Salimans [2021] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In _NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications_, 2021. URL [https://openreview.net/forum?id=q8A&xfYbI](https://openreview.net/forum?id=q8A&xfYbI).
* Bansal et al. [2023] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal Guidance for Diffusion Models. _arXiv e-prints_, art. arXiv:2302.07121, February 2023. doi: 10.48550/arXiv.2302.07121.
* Song et al. [2021] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=PxTIG12RRHS](https://openreview.net/forum?id=PxTIG12RRHS).
* Lu et al. [2022] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan LI, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 5775-5787. Curran Associates, Inc., 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/260a14acce2a89dad36adc8eefef7c59e-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/260a14acce2a89dad36adc8eefef7c59e-Paper-Conference.pdf).
* Lu et al. [2023] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023.
* Zhang and Chen [2023] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In _International Conference on Learning Representations_, 2023.
* Chen et al. [2018] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018. URL [https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf).
* Salimans and Ho [2022] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=TIdIXIpzh0I](https://openreview.net/forum?id=TIdIXIpzh0I).
* Song and Ermon [2019] Yang Song and Stefano Ermon. _Generative modeling by estimating gradients of the data distribution_. Curran Associates Inc., Red Hook, NY, USA, 2019.
* Abdal et al. [2019] R. Abdal, Y. Qin, and P. Wonka. Image2stylegan: How to embed images into the stylegan latent space? In _IEEE/CVF Int'l Conf. on Comp. Vision (ICCV)_, pages 4431-4440, 2019. doi: 10.1109/ICCV.2019.00453.
* Abdal et al. [2020] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images? In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8296-8305, 2020.
* Lipman et al. [2023] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=PqvMRDCJT9t](https://openreview.net/forum?id=PqvMRDCJT9t).
* Kidger [2022] Patrick Kidger. _On Neural Differential Equations_. PhD thesis, Oxford University, 2022.
* Hochbruck and Ostermann [2010] Marlis Hochbruck and Alexander Ostermann. Exponential integrators. _Acta Numerica_, 19:209-286, 2010. doi: 10.1017/S0962492910000048.
* Adamu [2011] Iyabo Ann Adamu. _Numerical approximation of SDEs & the stochastic Swift-Hohenberg equation_. PhD thesis, Heriot-Watt University, 2011.
* Gonzalez et al. [2023] Martin Gonzalez, Nelson Fernandez Pinto, Thuy Tran, elies Gherbi, Hatem Hajri, and Nader Masmoudi. Seeds: Exponential sde solvers for fast high-quality sampling from diffusion models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 68061-68120. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/d6f764aae383d9ff28a0f89f71defbd9-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/d6f764aae383d9ff28a0f89f71defbd9-Paper-Conference.pdf).
** Atkinson et al. [2011] K. Atkinson, W. Han, and D.E. Stewart. _Numerical Solution of Ordinary Differential Equations_. Pure and Applied Mathematics: A Wiley Series of Texts, Monographs and Tracts. Wiley, 2011. ISBN 9781118164525. URL [https://books.google.com/books?id=Q2jGgLlKCYQC](https://books.google.com/books?id=Q2jGgLlKCYQC).
* Meng et al. [2022] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_, 2022.
* Nie et al. [2024] Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, and Chongxuan Li. The blessing of randomness: SDE beats ODE in general diffusion-based image editing. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=DesTwumUG00](https://openreview.net/forum?id=DesTwumUG00).
* Anderson [1982] Brian D.O. Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326, 1982. ISSN 0304-4149. doi: [https://doi.org/10.1016/0304-4149](https://doi.org/10.1016/0304-4149)(82)90051-5. URL [https://www.sciencedirect.com/science/article/pii/0304414982900515](https://www.sciencedirect.com/science/article/pii/0304414982900515).
* Wu and De la Torre [2023] Chen Henry Wu and Fernando De la Torre. A latent space of stochastic diffusion models for zero-shot image editing and guidance. In _ICCV_, 2023.
* Yu et al. [2023] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Training-free energy-guided conditional diffusion model. _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.
* Blasingame and Liu [2024] Zander W. Blasingame and Chen Liu. Greedy-dim: Greedy algorithms for unreasonably effective face morphs. In _2024 IEEE International Joint Conference on Biometrics (IJCB)_, pages 1-10, September 2024.
* Liu et al. [2023] Xingchao Liu, Lemeng Wu, Shujian Zhang, Chengyue Gong, Wei Ping, and Qiang Liu. Flowgrad: Controlling the output of generative odes with gradients. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 24335-24344, 2023.
* Nie et al. [2022] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Anima Anandkumar. Diffusion models for adversarial purification. In _International Conference on Machine Learning (ICML)_, 2022.
* Wallace et al. [2023] Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik. End-to-end diffusion latent optimization improves classifier guidance, 2023.
* Pan et al. [2024] Jiachun Pan, Jun Hao Liew, Vincent Tan, Jiashi Feng, and Hanshu Yan. AdjointDPM: Adjoint sensitivity method for gradient backpropagation of diffusion probabilistic models. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=y331DRBgWI](https://openreview.net/forum?id=y331DRBgWI).
* Marion et al. [2024] Pierre Marion, Anna Korba, Peter Bartlett, Mathieu Blondel, Valentin De Bortoli, Arnaud Doucet, Felipe Llinares-Lopez, Courtney Paquette, and Quentin Berthet. Implicit diffusion: Efficient optimization through stochastic sampling. _arXiv preprint arXiv:2402.05468_, 2024.
* Blasingame and Liu [2024] Zander W. Blasingame and Chen Liu. Leveraging diffusion for strong and high quality face morphing attacks. _IEEE Transactions on Biometrics, Behavior, and Identity Science_, 6(1):118-131, 2024. doi: 10.1109/TBIOM.2024.3349857.
* Raghavendra et al. [2016] R. Raghavendra, K. B. Raja, and C. Busch. Detecting morphed face images. In _IEEE 8th Int'l Conf. on Biometrics Theory, Applications and Systems (BTAS)_, pages 1-7, 2016. doi: 10.1109/BTAS.2016.7791169.
* 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 2959-2963, 2022. doi: 10.1109/ICASSP43922.2022.9746477.
* Raghavendra et al. [2020]* [44] Zander W. Blasingame and Chen Liu. Fast-dim: Towards fast diffusion morphs. _IEEE Security & Privacy_, 22(4):103-114, June 2024. doi: 10.1109/MSEC.2024.3410112.
* [45] Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, and Busch Christoph. Morph-pipe: Plugging in identity prior to enhance face morphing attack based on diffusion model. In _Norwegian Information Security Conference (NISK)_, 2023.
* [46] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward a meaningful and decodable representation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10619-10629, June 2022.
* [47] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. In _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4396-4405, 2019. doi: 10.1109/CVPR.2019.00453.
* [48] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 4690-4699, 2019.
* [49] Haoyu Zhang, Sushma Venkatesh, Raghavendra Ramachandra, Kiran Raja, Naser Damer, and Christoph Busch. Mipgan--generating strong and high quality morphing attacks using identity prior driven gan. _IEEE Transactions on Biometrics, Behavior, and Identity Science_, 3(3):365-383, 2021. doi: 10.1109/TBIOM.2021.3072349.
* [50] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. Analyzing and improving the image quality of stylegan. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8107-8116, 2020. doi: 10.1109/CVPR42600.2020.00813.
* [51] Marco Huber, Fadi Boutros, Anh Thi Luu, Kiran Raja, Raghavendra Ramachandra, Naser Damer, Pedro C. Neto, Tiago Goncalves, Ana F. Sequeira, Jaime S. Cardoso, Joao Tremoco, Miguel Lourenco, Sergio Serra, Eduardo Cermeno, Marija Ivanovska, Borut Batagelj, Andrej Kronovsek, Peter Peer, and Vitomir Struc. Syn-mad 2022: Competition on face morphing attack detection based on privacy-aware synthetic training data. In _2022 IEEE International Joint Conference on Biometrics (IJCB)_, pages 1-10, 2022. doi: 10.1109/IJCB54206.2022.10007950.
* [52] Lisa DeBruine and Benedict Jones. Face Research Lab London Set. 5 2017. doi: 10.6084/m9.figshare.5047666.v5. URL [https://figshare.com/articles/dataset/Face_Research_Lab_London_Set/5047666](https://figshare.com/articles/dataset/Face_Research_Lab_London_Set/5047666).
* [53] Fadi Boutros, Naser Damer, Florian Kirchbuchner, and Arjan Kuijper. Elasticface: Elastic margin loss for deep face recognition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops_, pages 1578-1587, June 2022.
* [54] Minchul Kim, Anil K Jain, and Xiaoming Liu. Adaface: Quality adaptive margin for face recognition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.
* [55] Ulrich Scherhag, Andreas Nautsch, Christian Rathgeb, Marta Gomez-Barrero, Raymond N. J. Veldhuis, Luuk Spreeuwers, Maikel Schils, Davide Maltoni, Patrick Grother, Sebastien Marcel, Ralph Breithaupt, Raghavendra Ramachandra, and Christoph Busch. Biometric systems under morphing attacks: Assessment of morphing techniques and vulnerability reporting. In _2017 International Conference of the Biometrics Special Interest Group (BIOSIG)_, pages 1-7, 2017. doi: 10.23919/BIOSIG.2017.8053499.
* [56] Suyong Kim, Weiqi Ji, Sili Deng, Yingbo Ma, and Christopher Rackauckas. Stiff neural ordinary differential equations. _Chaos: An Interdisciplinary Journal of Nonlinear Science_, 31(9), 2021.
* [57] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22532-22541, 2023.

* Kidger et al. [2020] Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations for irregular time series. _Advances in Neural Information Processing Systems_, 33:6696-6707, 2020.
* Li et al. [2020] Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, and David Duvenaud. Scalable gradients for stochastic differential equations. In Silvia Chiappa and Roberto Calandra, editors, _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, volume 108 of _Proceedings of Machine Learning Research_, pages 3870-3882. PMLR, 26-28 Aug 2020. URL [https://proceedings.mlr.press/v108/li20i.html](https://proceedings.mlr.press/v108/li20i.html).
* Kunita [2019] Hiroshi Kunita. Stochastic differential equations and stochastic flows. _Stochastic Flows and Jump-Diffusions_, pages 77-124, 2019.
* Kidger et al. [2021] Patrick Kidger, James Foster, Xuechen (Chen) Li, and Terry Lyons. Efficient and accurate gradients for neural sdes. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 18747-18761. Curran Associates, Inc., 2021. URL [https://proceedings.neurips.cc/paper_files/paper/2021/file/9ba196c7a6e89eafd0954de80fc1b224-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/9ba196c7a6e89eafd0954de80fc1b224-Paper.pdf).
* Duta et al. [2021] Ionut Cosmin Duta, Li Liu, Fan Zhu, and Ling Shao. Improved residual networks for image and video recognition. In _2020 25th International Conference on Pattern Recognition (ICPR)_, pages 9415-9422, 2021. doi: 10.1109/ICPR48806.2021.9412193.
* An et al. [2021] Xiang An, Xuhan Zhu, Yuan Gao, Yang Xiao, Yongle Zhao, Ziyong Feng, Lan Wu, Bin Qin, Ming Zhang, Debing Zhang, and Ying Fu. Partial fc: Training 10 million identities on a single machine. In _2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)_, pages 1445-1449, 2021. doi: 10.1109/ICCVW54120.2021.00166.

**Organization of the appendix.** In Appendix A we provide a detailed comparison between our work and related work. Appendix B provides the full derivations for the construction of the AdjointDEIS-\(k\) solvers. Likewise, in Appendix C we present the proof for Theorem 2.1. In a similar manner Appendix D presents the proof for Theorem 2.2. A more detailed discussion of the case of adjoint diffusion SDEs is held in Appendix E along with the proof of Theorem 3.1. We include additional experiments on the impact of AdjointDEIS on the face morphing problem in Appendix F. The details of the implementation of our approach are included in Appendix G. Additional details specific to our experiments are likewise included in Appendix H. Finally, for completeness, we include Appendix I to show the closed-form solutions of the drift and diffusion coefficients.

## Appendix A Additional Related Work

In this section, we compare several recent methods for training-free guided generation which we broadly classify into two categories:

1. Techniques which directly optimize the solution trajectory during sampling [34, 35, 36]
2. Techniques which search for the optimal latents \(\mathbf{x}_{T}\) and or \(\mathbf{z}\) (this can include optimizing the solution trajectory as well) [38, 39].

In Table 2 we compare several different techniques for training-free guided diffusion in whether they explicitly optimize the _whole_ solution trajectory, _i.e._, optimizing \(\mathbf{x}_{T}\), if they optimize additional information like \(\mathbf{z}\) or \(\theta\), and whether the formulation is for diffusion ODEs, SDEs, or both.

Using Table 2 as a high-level overview, we spend the rest of this section providing a more detailed comparison and discussion of the related works which we separate by the two categories.

### Optimizing the Solution Trajectory During Sampling

**FlowGrad.** The FlowGrad [36] technique controls the generative process by solving the following optimal control problem

\[\min_{\mathbf{u}} \mathcal{L}(\mathbf{x}_{0})+\lambda\int_{T}^{0}\|\mathbf{u}(t)\|^{2} \;\mathrm{d}t,\] (A.1) s.t. \[\mathbf{x}_{0}=\mathbf{x}_{T}+\int_{T}^{0}\mathbf{f}_{\theta}(\mathbf{ x}_{t},\mathbf{z},t)+\mathbf{u}(t)\;\mathrm{d}t\] (A.2)

where \(\mathbf{u}\) is the control function. This optimization objective learns to alter the flow, \(\mathbf{f}_{\theta}\), by \(\mathbf{u}\). In practice, this amounts to injecting a control step governed by \(\mathbf{u}(t)\) for a discretized schedule. This technique does not allow for learning an optimal \(\mathbf{x}_{T}\), \(\mathbf{z}\), or \(\theta\).

**FreeDoM.** Another recent work, FreeDoM [34] looks at gradient guided generation of images by calculating the gradient w.r.t. \(\mathbf{x}_{t}\) by using the approximated clean image

\[\mathbf{x}_{0}\approx\frac{\mathbf{x}_{t}-\sigma_{t}\mathbf{\epsilon}_{\theta}( \mathbf{x}_{t},\mathbf{z},t)}{\alpha_{t}}\] (A.3)

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Method** & **ODE** & **SDE** & **Optimize**\(\mathbf{x}_{T}\) & **Optimize**\((\mathbf{z},\theta)\) \\ \hline FlowGrad [36] & ✓ & ✗ & ✗ & ✗ \\ FreeDoM [34] & ✗ & ✓ & ✗ & ✗ \\ Greedy [35] & ✓ & ✓ & ✗ & ✗ \\ DOODL [38] & ✓ & ✗ & ✓ & ✗ \\ DiffPure [37] & ✗ & ✓ & ✓ & ✗ \\ AdjointDPM [39] & ✓ & ✗ & ✓ & ✓ \\ Implicit Diffusion [40] & ✓ & ✓ & ✓ & ✓ \\
**AdjointDEIS** & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 2: Overview of different training-free guidance methods for diffusion models.

at each timestep. Let \(h_{i}=\lambda_{t_{i}}-\lambda_{t_{i-1}}\). The strategy can be described as

\[\mathbf{x}_{t_{i-1}} =\frac{\alpha_{t_{i-1}}}{\alpha_{t_{i}}}\mathbf{x}_{t_{i}}-2\sigma _{t_{i-1}}(e^{h_{i}}-1)\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t_{i}},\mathbf{z},t_{ i})+\sigma_{t_{i-1}}\sqrt{e^{2h_{i}}-1}\mathbf{\epsilon}_{t_{i}},\] (A.4) \[\hat{\mathbf{x}}_{0} =\frac{\mathbf{x}_{t_{i}}-\sigma_{t_{i}}\mathbf{\epsilon}_{\theta}( \mathbf{x}_{t_{i}},\mathbf{z},t_{i})}{\alpha_{t_{i}}},\] (A.5) \[\mathbf{g}_{t_{i}} =\frac{\partial\mathcal{L}(\hat{\mathbf{x}}_{0})}{\partial\mathbf{ x}_{t}},\] (A.6) \[\mathbf{x}_{t_{i-1}} =\mathbf{x}_{t_{i-1}}-\eta_{t_{i}}\mathbf{g}_{t_{i}},\] (A.7)

where \(\eta_{t_{i}}\) is a learning rate defined per timestep. Importantly, FreeDoM operates on diffusion SDEs. They have an addition algorithm in which the add noise back to the image, in essence going back one timestep and applying the guidance step again.

**Greedy.** Similar to FreeDoM, greedy guided generation [35] looks to alter the generative trajectory by injecting the gradient defined on the approximated clean image; however, this technique does so w.r.t. the prediction noise, _i.e._,

\[\mathbf{\epsilon}^{\prime}_{t_{i}} =\mathrm{stopgrad}(\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t_{i}}, \mathbf{z},t_{i}))\] (A.8) \[\hat{\mathbf{x}}_{0} =\frac{\mathbf{x}_{t_{i}}-\sigma_{t_{i}}\mathbf{\epsilon}^{\prime}_{t _{i}}}{\alpha_{t_{i}}},\] (A.9) \[\mathbf{g}_{t_{i}} =\frac{\partial\mathcal{L}(\hat{\mathbf{x}}_{0})}{\partial\mathbf{ \epsilon}_{t_{i}}},\] (A.10) \[\mathbf{\epsilon}^{\prime}_{t_{i}} =\mathbf{\epsilon}^{\prime}_{t_{i}}-\eta_{t_{i}}\mathbf{g}_{t_{i}}.\] (A.11)

The technique would work for either diffusion ODEs or SDEs.

### Optimizing the Entire Solution Trajectory

**DOODL.** The algorithm DOODL [38] looks at the gradient calculation based on the invertibility of EDICT [57]. This method can find the gradient w.r.t. \(\mathbf{x}_{T}\); however, it cannot for the other quantities. DOODL additionally has further overhead due to the dual diffusion process of EDICT. Further analysis of DOODL compared to continuous adjoint equations for diffusion models can be found in [39].

The remaining methods are much closer to our work as they use continuous adjoint equations for guidance. We provide a high-level summary that compares these methods to ours in Table 3.

**DiffPure.** Work by Nie et al. [37] examined the use of continuous adjoint equations for cleaning adversarial images, an application of guided generation. There exist a few key differences between their work and ours. First, while they consider the SDE case they use an Euler-Maruyama numerical scheme, meaning there is a discretization error incurred in the linear term of the drift coefficient. Moreover, the solver for the continuous adjoint equation results in an Euler scheme which suffers from a low convergence order and poor stability, particularly for stiff equations. Our approach used exponential integrators to greatly simplify the continuous adjoint equations whilst improving numerical stability by transforming the continuous adjoint equations into non-stiff form.

**AdjointDPM.** More closely related to our work is the recent AdjointDPM [39] which also explores the use of adjoint sensitivity methods for backpropagation through the _probability flow_ ODE. While

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & DiffPure [37] & AdjointDPM [39] & **AdjointDEIS** \\ \hline Discretization domain & \(\mathbf{\epsilon}_{\theta}\) over \(t\) & \(\mathbf{\epsilon}_{\theta}\) over \(\rho\) & \(\mathbf{\epsilon}_{\theta}\) over \(\lambda\) \\ Solver type & Black box SDE solver & Black box ODE solver & Custom solver \\ Exponential Integrators & ✗ & ✓ & ✓ \\ Closed form SDE coefficients & ✗ & ✗ & ✓ \\ Interoperability with existing samplers & ✗ & ✗ & ✓ \\ Decoupled ODE schedule & ✗ & ✗ & ✓ \\ Supports SDEs & ✓ & ✗ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of solvers for the continuous adjoint equations for diffusion models.

they also propose to use the continuous adjoint equations to find gradients for diffusion models, our work differs in several ways which we enumerate in Table 3. For clarity, we use orange to denote their notation. They reparameterize Equation (1.4) as

\[\frac{\mathrm{d}\mathsf{y}}{\mathrm{d}\rho}=\hat{\epsilon}_{\theta}(e^{\int_{0}^{ -1}(\rho)\,f(\tau)\,\mathrm{d}\tau}\mathsf{y},\gamma^{-1}(\rho),c)\] (A.12)

where \(c\) denotes the conditional information, \(\rho=\gamma(t)\), and \(\frac{\mathrm{d}\gamma}{\mathrm{d}t}=e^{-\int_{0}^{t}f(\tau)\,\mathrm{d}\tau\, \frac{\rho^{2}(t)}{2\sigma_{t}}}\). which gives them the following ODE for calculating the adjoint.

\[\frac{\mathrm{d}}{\mathrm{d}\rho}\bigg{[}\frac{\partial\mathcal{L}}{\partial \gamma_{\rho}}\bigg{]}=-\frac{\partial\mathcal{L}}{\partial\gamma_{\rho}}^{\top }\frac{\partial\epsilon_{\theta}(e^{\int_{0}^{-1}(\rho)\,f(\tau)\,\mathrm{d} \tau}\mathsf{y}_{\rho},\mathbf{z},\gamma^{-1}(\rho))}{\partial\gamma_{\rho}}\] (A.13)

In our approach we integrate over \(\lambda_{t}\) whereas they integrate over \(\rho\). Moreover, we provide custom solvers designed specifically for diffusion ODEs instead of using a black-box ODE solver. Our approach is also interoperable with other forward ODE solvers, meaning our AdjointDEIS solver is agnostic to the ODE solver used to generate the output; however, the AdjointDPM model is tightly coupled to its forward solver. Lastly and most importantly, our method is more general and supports diffusion SDEs, not just ODEs.

Although the original paper omitted a closed form expression for \(\gamma^{-1}(\rho)\), we provide to give a comparison between both methods, and to ensure that AdjointDPM can be fully implemented. In the VP SDE scheme with a linear noise schedule \(\log\alpha_{t}\) is found to be

\[\log\alpha_{t}=-\frac{\beta_{1}-\beta_{0}}{4}t^{2}-\frac{\beta_{0}}{2}t\] (A.14)

on \(t\in[0,1]\) with \(\beta_{0}=0.1,\beta_{1}=20\), following Song et al. [15]. Then \(\gamma^{-1}(\rho)\) is found to be

\[\gamma^{-1}(\rho)=\frac{\beta_{0}-\sqrt{\beta_{0}^{2}+4\log\frac{1}{\sqrt{ \frac{1}{\alpha_{0}^{2}}(\rho+\sigma_{0})^{2}+1}}}(\beta_{0}-\beta_{1})}{\beta _{0}-\beta_{1}}\] (A.15)

Implicit Diffusion.Concurrent work to ours by Marion et al. [40] has also explored the use of continuous adjoint equations for the guidance of diffusion models. They, however, focus on an efficient scheme to parallelize the solution to the adjoint ODE from the perspective of bi-level optimization rather than the adjoint technique itself. So, while we focused on the numerical solvers for the continuous adjoint equations, they focused on an efficient implementation of the optimization problem from the perspective of bi-level optimization.

## Appendix B Derivation of AdjointDEIS

In this section, we provide the full derivations for the family of AdjointDEIS solvers. First recall the full definition of the continuous adjoint equations for the empirical probability flow ODE:

\[\mathbf{a}_{\mathbf{x}}(0)=\frac{\partial\mathcal{L}}{\partial \mathbf{x}_{0}}, \frac{\mathrm{d}\mathbf{a}_{\mathbf{x}}}{\mathrm{d}t}(t)=-\mathbf{a}_{ \mathbf{x}}(t)^{\top}\frac{\partial\mathbf{f}_{\theta}(\mathbf{x}_{t},\mathbf{z},t )}{\partial\mathbf{x}_{t}},\] \[\mathbf{a}_{\mathbf{z}}(0)=\mathbf{0}, \frac{\mathrm{d}\mathbf{a}_{\mathbf{z}}}{\mathrm{d}t}(t)=-\mathbf{ a}_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{f}_{\theta}(\mathbf{x}_{t},\mathbf{z},t )}{\partial\mathbf{z}},\] \[\mathbf{a}_{\theta}(0)=\mathbf{0}, \frac{\mathrm{d}\mathbf{a}_{\theta}}{\mathrm{d}t}(t)=-\mathbf{a}_{ \mathbf{x}}(t)^{\top}\frac{\partial\mathbf{f}_{\theta}(\mathbf{x}_{t},\mathbf{z},t )}{\partial\theta}.\] (B.1)

We can simplify the equations by explicitly solving gradients of the neural vector field \(\mathbf{f}_{\theta}\) for the drift term to obtain

\[\frac{\mathrm{d}\mathbf{a}_{\mathbf{x}}}{\mathrm{d}t}(t)= -f(t)\mathbf{a}_{\mathbf{x}}(t)-\frac{g^{2}(t)}{2\sigma_{t}} \mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{\partial\epsilon_{\theta}(\mathbf{x}_{t },\mathbf{z},t)}{\partial\mathbf{x}_{t}},\] \[\frac{\mathrm{d}\mathbf{a}_{\mathbf{z}}}{\mathrm{d}t}(t)= \mathbf{0}-\frac{g^{2}(t)}{2\sigma_{t}}\mathbf{a}_{\mathbf{x}}(t)^{ \top}\frac{\partial\epsilon_{\theta}(\mathbf{x}_{t},\mathbf{z},t)}{\partial \mathbf{z}},\] \[\frac{\mathrm{d}\mathbf{a}_{\theta}}{\mathrm{d}t}(t)= \mathbf{0}-\frac{g^{2}(t)}{2\sigma_{t}}\mathbf{a}_{\mathbf{x}}(t)^{ \top}\frac{\partial\epsilon_{\theta}(\mathbf{x}_{t},\mathbf{z},t)}{\partial \theta}.\] (B.2)

**Remark B.1**.: _The last two equations in Equations (B.2) the vector fields are independent of \((\mathbf{a}_{\mathbf{z}},\mathbf{a}_{\theta})\), reducing these equations to mere integrals; however, it is often useful to compute the whole system \(\mathbf{a}_{\text{aug}}=(\mathbf{a}_{\mathbf{x}},\mathbf{a}_{\mathbf{z}},\mathbf{ a}_{\theta})\) as an augmented ODE._

**Remark B.2**.: _Likewise, the last two equations in Equations (B.2) are functionally identical with a simple swap of \(\mathbf{z}\) for \(\theta\) or vice versa._

As such, for the sake of brevity, the derivations for the AdjointDEIS solvers for \((\mathbf{a}_{\mathbf{z}},\mathbf{a}_{\theta})\) will only explicitly include the derivations for \(\mathbf{a}_{\mathbf{z}}\).

### Simplified Formulation of the Continuous Adjoint Equations

Focusing first on the continuous adjoint equation for \(\mathbf{a}_{\mathbf{x}}\) we apply the integrating factor \(\exp\big{(}\int_{0}^{t}f(\tau)\ \mathrm{d}\tau\big{)}\) to Equation (B.2) to find

\[\frac{\mathrm{d}}{\mathrm{d}t}\bigg{[}e^{\int_{0}^{t}f(\tau)\ \mathrm{d}\tau}\mathbf{a}_{ \mathbf{x}}(t)\bigg{]}=-e^{\int_{0}^{t}f(\tau)\ \mathrm{d}\tau}\frac{g^{2}(t)}{2\sigma_{t}}\mathbf{a}_{\mathbf{x}}(t)^{ \top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)}{ \partial\mathbf{x}_{t}}.\] (B.3)

Then, the exact solution at time \(s\) given time \(t<s\) is found to be

\[e^{\int_{0}^{s}f(\tau)\ \mathrm{d}\tau}\mathbf{a}_{\mathbf{x}}(s) =e^{\int_{0}^{t}f(\tau)\ \mathrm{d}\tau}\mathbf{a}_{\mathbf{x}}(t)-\int_{t}^{s}e^{\int_{0}^{u}f( \tau)\ \mathrm{d}\tau}\frac{g^{2}(u)}{2\sigma_{u}}\mathbf{a}_{\mathbf{x}}(u)^{ \top}\frac{\mathbf{\epsilon}_{\theta}(\mathbf{x}_{u},\mathbf{z},u)}{\partial \mathbf{x}_{u}}\ \mathrm{d}u\] \[\mathbf{a}_{\mathbf{x}}(s) =e^{\int_{s}^{t}f(\tau)\ \mathrm{d}\tau}\mathbf{a}_{\mathbf{x}}(t)- \int_{t}^{s}e^{\int_{s}^{u}f(\tau)\ \mathrm{d}\tau}\frac{g^{2}(u)}{2\sigma_{u}} \mathbf{a}_{\mathbf{x}}(u)^{\top}\frac{\mathbf{\epsilon}_{\theta}(\mathbf{x}_{u}, \mathbf{z},u)}{\partial\mathbf{x}_{u}}\ \mathrm{d}u\] (B.4)

To simplify Equation (B.4), recall that \(f(t)\) is defined as

\[f(t)=\frac{\mathrm{d}\log\alpha_{t}}{\mathrm{d}t},\] (B.5)

for VP type SDEs. Furthermore, let \(\lambda_{t}\coloneqq\log(\alpha_{t}/\sigma_{t})\) be one half of the log-SNR. Then the diffusion coefficient can be simplified using the log-derivative trick such that

\[g^{2}(t)=\frac{\mathrm{d}\sigma_{t}^{2}}{\mathrm{d}t}-2\frac{\mathrm{d}\log \alpha_{t}}{\mathrm{d}t}\sigma_{t}^{2}=2\sigma_{t}^{2}\bigg{(}\frac{\mathrm{d} \log\sigma_{t}}{\mathrm{d}t}-\frac{\mathrm{d}\log\alpha_{t}}{\mathrm{d}t}\bigg{)} =-2\sigma_{t}^{2}\frac{\mathrm{d}\lambda_{t}}{\mathrm{d}t}.\] (B.6)

Using this updated expression of \(g^{2}(t)\) along with computing the integrating factor in closed form enables us to express Equation (B.4) as

\[\mathbf{a}_{\mathbf{x}}(s)=\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a}_{\mathbf{x} }(t)+\frac{1}{\alpha_{s}}\int_{t}^{s}\alpha_{u}\sigma_{u}\frac{\mathrm{d} \lambda_{u}}{\mathrm{d}u}\mathbf{a}_{\mathbf{x}}(u)^{\top}\frac{\mathbf{\epsilon }_{\theta}(\mathbf{x}_{u},\mathbf{z},u)}{\partial\mathbf{x}_{u}}\ \mathrm{d}u.\] (B.7)

Lastly, by rewriting the integral in terms of an exponentially weighted integral \(\alpha_{u}\sigma_{u}=\alpha_{u}^{2}\sigma_{u}/\alpha_{u}=\alpha_{u}^{2}e^{- \lambda_{u}}\) we find

\[\mathbf{a}_{\mathbf{x}}(s)=\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a}_{\mathbf{x }}(t)+\frac{1}{\alpha_{s}}\int_{\lambda_{t}}^{\lambda_{s}}\alpha_{\lambda}^{2 }e^{-\lambda}\mathbf{a}_{\mathbf{x}}(\lambda)^{\top}\frac{\mathbf{\epsilon}_{ \theta}(\mathbf{x}_{\lambda},\mathbf{z},\lambda)}{\partial\mathbf{x}_{\lambda} }\ \mathrm{d}\lambda.\] (B.8)

This change of variables is possible as \(\lambda_{t}\) is a strictly decreasing function w.r.t. \(t\) and therefore it has an inverse function \(t_{\lambda}\) which satisfies \(t_{\lambda}(\lambda_{t})=t\), and, with abuse of notation, we let \(\mathbf{x}_{\lambda}\coloneqq\mathbf{x}_{t_{\lambda}(\lambda)}\), \(\mathbf{a}_{\mathbf{x}}(\lambda)\coloneqq\mathbf{a}_{\mathbf{x}}(t_{\lambda}( \lambda))\), \(\mathbf{\epsilon}\).and let the reader infer from context if the function is mapping the log-SNR back into the time domain or already in the time domain.

Now we will show the derivations to find a simplified form of the continuous adjoint equation for the conditional information. Using the continuous adjoint equation from Equations (B.2) for \(\mathbf{a}_{\mathbf{z}}(t)\) along with the log-SNR, we can express the evolution of \(\mathbf{a}_{\mathbf{z}}(t)\) as

\[\frac{\mathrm{d}\mathbf{a}_{\mathbf{z}}}{\mathrm{d}t}(t)=\sigma_{t}\frac{ \mathrm{d}\lambda_{t}}{\mathrm{d}t}\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{ \partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)}{\partial\mathbf{z }}.\] (B.9)

As we would like to express this as an exponential integrator, we simply multiply \(\sigma_{t}\) by \(\alpha_{t}/\alpha_{t}\) to obtain \(\alpha_{t}\cdot\sigma_{t}/\alpha_{t}=\alpha_{t}e^{-\lambda_{t}}\), as such we can rewrite Equation (B.9) as

\[\frac{\mathrm{d}\mathbf{a}_{\mathbf{z}}}{\mathrm{d}t}(t)=\alpha_{t}e^{-\lambda _{t}}\frac{\mathrm{d}\lambda_{t}}{\mathrm{d}t}\mathbf{a}_{\mathbf{x}}(t)^{ \top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)}{ \partial\mathbf{z}}.\] (B.10)

Using Equations (B.8) and (B.10), we arrive at Proposition 2.1 from the main paper.

**Proposition B.1**.: _Given initial values \([\mathbf{a}_{\mathbf{x}}(t),\mathbf{a}_{\mathbf{z}}(t),\mathbf{a}_{\theta}(t)]\) at time \(t\in(0,T)\), the solution \([\mathbf{a}_{\mathbf{x}}(s),\mathbf{a}_{\mathbf{z}}(s),\mathbf{a}_{\theta}(s)]\) at time \(s\in(t,T]\) of the adjoint empirical probability flow ODE in Equation (B.4) is_

\[\mathbf{a}_{\mathbf{x}}(s) =\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a}_{\mathbf{x}}(t)+\frac{1} {\alpha_{s}}\int_{\lambda_{t}}^{\lambda_{s}}\alpha_{\lambda}^{2}e^{-\lambda} \mathbf{a}_{\mathbf{x}}(\lambda)^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}( \mathbf{x}_{\lambda},\mathbf{z},\lambda)}{\partial\mathbf{x}_{\lambda}}\; \mathrm{d}\lambda,\] (B.11) \[\mathbf{a}_{\mathbf{z}}(s) =\mathbf{a}_{\mathbf{z}}(t)+\int_{\lambda_{t}}^{\lambda_{s}} \alpha_{\lambda}e^{-\lambda}\mathbf{a}_{\mathbf{x}}(\lambda)^{\top}\frac{ \partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{\lambda},\mathbf{z},\lambda)}{ \partial\mathbf{z}}\;\mathrm{d}\lambda,\] (B.12) \[\mathbf{a}_{\theta}(s) =\mathbf{a}_{\theta}(t)+\int_{\lambda_{t}}^{\lambda_{s}}\alpha_{ \lambda}e^{-\lambda}\mathbf{a}_{\mathbf{x}}(\lambda)^{\top}\frac{\partial\mathbf{ \epsilon}_{\theta}(\mathbf{x}_{\lambda},\mathbf{z},\lambda)}{\partial\theta} \;\mathrm{d}\lambda.\] (B.13)

Then to find the AdjointDEIS solvers we take a \(k\)-th order Taylor expansion about \(\lambda_{t}\) and integrate in the log-SNR domain.

### Taylor Expansion

For \(k\geq 1\), the \((k-1)\)-th Taylor expansion at \(\lambda_{t}\) of the inner term of the exponentially weighted integral in Equation (B.11) is

\[\mathbf{a}_{\mathbf{x}}(\lambda)^{\top}\frac{\partial\mathbf{\epsilon }_{\theta}(\mathbf{x}_{\lambda},\mathbf{z},\lambda)}{\partial\mathbf{x}_{ \lambda}} =\sum_{n=0}^{k-1}\frac{(\lambda-\lambda_{t})^{n}}{n!}\frac{\mathrm{ d}^{n}}{\mathrm{d}\lambda^{n}}\left[\alpha_{\lambda}^{2}\mathbf{a}_{\mathbf{x}}( \lambda)^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{\lambda}, \mathbf{z},\lambda)}{\partial\mathbf{x}_{\lambda}}\right]_{\lambda=\lambda_{t} }+\mathcal{O}((\lambda-\lambda_{t})^{k}).\] (B.14)

Then plugging this into Equation (B.11) yields

\[\mathbf{a}_{\mathbf{x}}(s) =\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a}_{\mathbf{x}}(t) +\frac{1}{\alpha_{s}}\int_{\lambda_{t}}^{\lambda_{s}}e^{-\lambda} \sum_{n=0}^{k-1}\frac{(\lambda-\lambda_{t})^{n}}{n!}\frac{\mathrm{d}^{n}}{ \mathrm{d}\lambda^{n}}\bigg{[}\alpha_{\lambda}^{2}\mathbf{a}_{\mathbf{x}}( \lambda)^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{\lambda}, \mathbf{z},\lambda)}{\partial\mathbf{x}_{\lambda}}\bigg{]}_{\lambda=\lambda_{t }}\;\mathrm{d}\lambda\] \[+\mathcal{O}(h^{k+1})\] \[=\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a}_{\mathbf{x}}(t) +\frac{1}{\alpha_{s}}\sum_{n=0}^{k-1}\underbrace{\frac{\mathrm{d} ^{n}}{\mathrm{d}\lambda^{n}}\bigg{[}\alpha_{\lambda}^{2}\mathbf{a}_{\mathbf{ x}}(\lambda)^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{\lambda}, \mathbf{z},\lambda)}{\partial\mathbf{x}_{\lambda}}\bigg{]}_{\lambda=\lambda_{s }}}_{\text{estimated}}\underbrace{\int_{\lambda_{t}}^{\lambda_{s}}\frac{( \lambda-\lambda_{t})^{n}}{n!}e^{-\lambda}\;\mathrm{d}\lambda}_{\text{analytically computed}}\] \[+\underbrace{\mathcal{O}(h^{k+1})}_{\text{omitted}},\] (B.15)

where \(h=\lambda_{s}-\lambda_{t}\).

The exponentially weighted integral \(\int_{\lambda_{t}}^{\lambda_{s}}\frac{(\lambda-\lambda_{t})^{n}}{n!}e^{- \lambda}\;\mathrm{d}\lambda\) can be solved _analytically_ by applying \(n\) times integration by parts [16; 26] such that

\[\int_{\lambda_{t}}^{\lambda_{s}}e^{-\lambda}\frac{(\lambda-\lambda _{t})^{n}}{n!}\;\mathrm{d}\lambda=\frac{\sigma_{s}}{\alpha_{s}}h^{n+1}\varphi _{n+1}(h),\] (B.16)

with the special \(\varphi\)-functions [26]. These functions are defined as

\[\varphi_{n+1}(h)\coloneqq\int_{0}^{1}e^{(1-u)h}\frac{u^{n}}{n!}\; \mathrm{d}u,\qquad\varphi_{0}(h)=e^{h},\] (B.17)

which satisfy the recurrence relation \(\varphi_{k+1}(h)=(\varphi_{k}(h)-\varphi_{k}(0))/h\) and have closed forms for \(k=1,2\):

\[\varphi_{1}(h) =\frac{e^{h}-1}{h},\] (B.18) \[\varphi_{2}(h) =\frac{e^{h}-h-1}{h^{2}}.\] (B.19)Likewise, the Taylor expansion of the exponentially weighted integral in Equation (B.12) yields

\[\mathbf{a}_{\mathbf{z}}(s) =\mathbf{a}_{\mathbf{z}}(t)+\int_{\lambda_{t}}^{\lambda_{s}}e^{- \lambda}\sum_{n=0}^{k-1}\frac{(\lambda-\lambda_{t})^{n}}{n!}\frac{\mathrm{d}^{n }}{\mathrm{d}\lambda^{n}}\bigg{[}\alpha_{\lambda}\mathbf{a}_{\mathbf{x}}( \lambda)^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{\lambda}, \mathbf{z},\lambda)}{\partial\mathbf{z}}\bigg{]}_{\lambda=\lambda_{t}}\,\mathrm{ d}\lambda+\mathcal{O}(h^{k+1})\] \[=\mathbf{a}_{\mathbf{z}}(t)+\sum_{n=0}^{k-1}\underbrace{\frac{ \mathrm{d}^{n}}{\mathrm{d}\lambda^{n}}\bigg{[}\alpha_{\lambda}\mathbf{a}_{ \mathbf{x}}(\lambda)^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{ \lambda},\mathbf{z},\lambda)}{\partial\mathbf{z}}\bigg{]}_{\lambda=\lambda_{t}} \underbrace{\int_{\lambda_{t}}^{\lambda_{s}}\frac{(\lambda-\lambda_{t})^{n}}{ n!}e^{-\lambda}\ \mathrm{d}\lambda}_{\text{analytically computed}}+\underbrace{\mathcal{O}(h^{k+1})}_{ \text{omitted}}.\] (B.20)

### AdjointDEIS-1

For \(k=1\) and omitting the higher-order error term, Equation (B.15) becomes:

\[\mathbf{a}_{\mathbf{x}}(s) =\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a}_{\mathbf{x}}(t)+\frac{1}{ \alpha_{s}}\alpha_{t}^{2}\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{ \epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)}{\partial\mathbf{x}_{t}}\int_ {\lambda_{t}}^{\lambda_{s}}\frac{(\lambda-\lambda_{t})^{0}}{0!}e^{-\lambda}\ \mathrm{d}\lambda\] \[=\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a}_{\mathbf{x}}(t)+\sigma_{ s}(e^{h}-1)\frac{\alpha_{t}^{2}}{\alpha_{s}^{2}}\mathbf{a}_{\mathbf{x}}(t)^{\top} \frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)}{\partial \mathbf{x}_{t}}\qquad\text{By Equation \eqref{eq:E1}.}\] (B.21)

Likewise, the continuous adjoint equation for \(\mathbf{z}\), Equation (B.20), becomes when \(k=1\) by omitting the higher-order error term:

\[\mathbf{a}_{\mathbf{z}}(s) =\mathbf{a}_{\mathbf{z}}(t)+\alpha_{t}\mathbf{a}_{\mathbf{x}}(t)^{ \top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)}{ \partial\mathbf{z}}\int_{\lambda_{t}}^{\lambda_{s}}\frac{(\lambda-\lambda_{t}) ^{0}}{0!}e^{-\lambda}\ \mathrm{d}\lambda\] \[=\mathbf{a}_{\mathbf{z}}(t)+\sigma_{s}(e^{h}-1)\frac{\alpha_{t}}{ \alpha_{s}}\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{\epsilon}_{ \theta}(\mathbf{x}_{t},\mathbf{z},t)}{\partial\mathbf{z}}\qquad\text{By Equation \eqref{eq:E1}.}\] (B.22)

And the first-order solver for \(\mathbf{a}_{\theta}(t)\) can be found in a similar fashion, thus we have derived the AdjointDEIS-1 solvers.

### AdjointDEIS-2M

Consider the following definition of the limit in the log-SNR domain

\[\frac{\mathrm{d}}{\mathrm{d}\lambda}\bigg{[}\alpha_{\lambda}^{2}\mathbf{a}_{ \mathbf{x}}(\lambda)^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{ \lambda},\mathbf{z},\lambda)}{\partial\mathbf{x}_{\lambda}}\bigg{]}=\lim_{ \lambda_{r}\to\lambda_{t}}\frac{\mathbf{V}(\mathbf{x};\lambda_{t})-\mathbf{V}( \mathbf{x};\lambda_{r})}{\rho h},\] (B.23)

where \(\rho=\frac{\lambda_{t}-\lambda_{r}}{h}\) with \(h=\lambda_{s}-\lambda_{t}\) and where \(r\) is some previous step \(r<t<s\). Again, \(\mathbf{V}(\mathbf{x};\lambda_{t})\) is overloaded to mean \(\mathbf{V}(\mathbf{x};t_{\lambda}(\lambda_{t}))\). Then by omitting higher-order error \(\mathcal{O}(h^{k+1})\), Equation (B.15) becomes:

\[\mathbf{a}_{\mathbf{x}}(s) =\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a}_{\mathbf{x}}(t)+\frac{1} {\alpha_{s}}\bigg{[}\mathbf{V}(\mathbf{x};\lambda_{t})\int_{\lambda_{t}}^{ \lambda_{s}}\frac{(\lambda-\lambda_{t})^{0}}{0!}\ \mathrm{d}\lambda+\mathbf{V}^{(1)}( \mathbf{x};\lambda_{t})\int_{\lambda_{t}}^{\lambda_{s}}\frac{(\lambda-\lambda_ {t})^{1}}{1!}\ \mathrm{d}\lambda\bigg{]}\] \[=\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a}_{\mathbf{x}}(t)+\frac{1} {\alpha_{s}}\bigg{[}\frac{\sigma_{s}}{\alpha_{s}}(e^{h}-1)\mathbf{V}(\mathbf{ x};\lambda_{t})+\frac{\sigma_{s}}{\alpha_{s}}(e^{h}-h-1)\mathbf{V}^{(1)}(\mathbf{x}; \lambda_{t})\bigg{]}.\] (B.24)

By applying the same approximation used in Lu et al. [16] of

\[\frac{e^{h}-h-1}{h}\approx\frac{e^{h}-1}{2},\] (B.25)

then we can rewrite the second term of the Taylor expansion as

\[\frac{\sigma_{s}}{\alpha_{s}}(e^{h}-h-1)\mathbf{V}^{(1)}(\mathbf{x}; \lambda_{t}) \approx\frac{\sigma_{s}}{\alpha_{s}}(e^{h}-h-1)\frac{\mathbf{V}( \mathbf{x};\lambda_{t})-\mathbf{V}(\mathbf{x};\lambda_{r})}{\rho h}\qquad \text{By Equation \eqref{eq:E1}}\] \[\approx\frac{\sigma_{s}}{\alpha_{s}}\frac{e^{h}-1}{2\rho}\big{(} \mathbf{V}(\mathbf{x};\lambda_{t})-\mathbf{V}(\mathbf{x};\lambda_{r})\big{)} \qquad\text{By Equation \eqref{eq:E1}}\] \[=\frac{\sigma_{s}}{\alpha_{s}}\frac{e^{h}-1}{2\rho}\bigg{(} \alpha_{t}^{2}\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{\epsilon}_{ \theta}(\mathbf{x}_{t},\mathbf{z},t)}{\partial\mathbf{x}_{t}}-\alpha_{r}^{2} \mathbf{a}_{\mathbf{x}}(r)^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x} _{r},\mathbf{z},r)}{\partial\mathbf{x}_{r}}\bigg{)}.\] (B.26)Then Equation (B.24) becomes

\[\mathbf{a}_{\mathbf{x}}(s)=\frac{\alpha_{t}}{\alpha_{s}}\mathbf{a}_{ \mathbf{x}}(t)+\sigma_{s}(e^{h}-1)\frac{\alpha_{t}^{2}}{\alpha_{s}^{2}}\mathbf{a }_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t}, \mathbf{z},t)}{\partial\mathbf{x}_{t}}\\ +\sigma_{s}\frac{e^{h}-1}{2\rho}\bigg{(}\frac{\alpha_{t}^{2}}{ \alpha_{s}^{2}}\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{\epsilon}_{ \theta}(\mathbf{x}_{t},\mathbf{z},t)}{\partial\mathbf{x}_{t}}-\frac{\alpha_{r} ^{2}}{\alpha_{s}^{2}}\mathbf{a}_{\mathbf{x}}(r)^{\top}\frac{\partial\mathbf{ \epsilon}_{\theta}(\mathbf{x}_{r},\mathbf{z},r)}{\partial\mathbf{x}_{r}}\bigg{)}.\] (B.27)

Likewise, consider the scaled vector-Jacobian product of the adjoint state \(\mathbf{a}_{\mathbf{x}}(t)\) and the gradient of the model w.r.t. \(\mathbf{z}\), _i.e._,

\[\mathbf{V}(\mathbf{z};t)=\alpha_{t}\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{ \partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)}{\partial\mathbf{ z}},\] (B.28)

along with a corresponding definition of first-derivative w.r.t. \(\lambda\) as defined in Equation (B.23). As such Equation (B.20), when \(k=2\), becomes the following when omitting the higher-order error term:

\[\mathbf{a}_{\mathbf{z}}(s) =\mathbf{a}_{\mathbf{z}}(t)+\mathbf{V}(\mathbf{z};\lambda_{t}) \int_{\lambda_{t}}^{\lambda_{s}}\frac{(\lambda-\lambda_{t})^{0}}{0!}\;\mathrm{ d}\lambda+\mathbf{V}^{(1)}(\mathbf{z};\lambda_{t})\int_{\lambda_{t}}^{\lambda_{s}} \frac{(\lambda-\lambda_{t})^{1}}{1!}\;\mathrm{d}\lambda\] \[=\mathbf{a}_{\mathbf{z}}(t)+\frac{\sigma_{s}}{\alpha_{s}}(e^{h}-1 )\mathbf{V}(\mathbf{z};\lambda_{t})+\frac{\sigma_{s}}{\alpha_{s}}(e^{h}-h-1) \mathbf{V}^{(1)}(\mathbf{z};\lambda_{t}).\] (B.29)

The second term of the Taylor expansion can be rewritten as

\[\frac{\sigma_{s}}{\alpha_{s}}(e^{h}-h-1)\mathbf{V}^{(1)}(\mathbf{ z};\lambda_{t}) \approx\frac{\sigma_{s}}{\alpha_{s}}(e^{h}-h-1)\frac{\mathbf{V}( \mathbf{z};\lambda_{t})-\mathbf{V}(\mathbf{z};\lambda_{r})}{\rho h}\\ \approx\frac{\sigma_{s}}{\alpha_{s}}\frac{e^{h}-1}{2\rho}\big{(} \mathbf{V}(\mathbf{z};\lambda_{t})-\mathbf{V}(\mathbf{z};\lambda_{r})\big{)} \qquad\text{By Equation~{}\eqref{eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq: eq:eq:eq:eq: eq:eq:eq: eq:eq:eq: eq:eq:eq: eq:eq: eq:eq: eq:eq: eq:eq: eq:eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq:

**Assumption C.4**.: \(\rho_{i}>c>0\) _for all \(i=1,\ldots,M\) and some constant \(c\)._

The first assumption is required by Taylor's theorem. The second assumption is a mild assumption to ensure that Theorem C.1 holds, which is used to replace \(\tilde{\mathbf{V}}(\{\mathbf{x}_{t},\mathbf{z},\theta\},t)\) with \(\mathbf{V}(\{\mathbf{x}_{t},\mathbf{z},\theta\},t)+\mathcal{O}(\tilde{\mathbf{ a}}_{\mathbf{x}}(t)-\mathbf{a}_{\mathbf{x}}(t))\) so the Taylor expansion w.r.t. \(\lambda_{s}\) is applicable. The third assumption is a technical assumption to exclude a significantly large step size. The last assumption is necessary for the case when \(k=2\). For our proofs, we follow a similar outline to that taken by Lu et al. (2017, Appendix A).

### The Vector-Jacobian Product is Lipschitz

**Lemma C.1** (Vector-Jacobian Product is Lipschitz.).: _Let \(\mathbf{f}_{\theta}:\mathbb{R}^{d}\times\mathbb{R}^{z}\times[0,T]\to\mathbb{R}^{d}\) be continuous in \(t\) and uniformly Lipschitz and continuously differentiable in \(\mathbf{x}\). Let \(\mathbf{x}:[0,T]\to\mathbb{R}^{d}\) be the unique solution to_

\[\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}=\mathbf{f}_{\theta}(\mathbf{x}_{t}, \mathbf{z},t)\]

_with initial condition \(\mathbf{x}_{0}\). Then the following map_

\[(\mathbf{a},t)\mapsto-\mathbf{a}^{\top}\frac{\partial\mathbf{f}_{\theta}(\mathbf{ x}_{t},\mathbf{z},t)}{\partial[\mathbf{x}_{t},\mathbf{z},\theta]}\]

_is Lipschitz in \(\mathbf{a}\). Moreover, the Lipschitz constant \(L>0\) is given by_

\[L=\sup_{t\in[0,T]}\bigg{|}\frac{\partial\mathbf{f}_{\theta}(\mathbf{x}_{t}, \mathbf{z},t)}{\partial\mathbf{x}_{t}}\bigg{|}.\] (C.3)

Proof.: Now, as \(\mathbf{x}_{t}\) is continuous and \(\mathbf{f}_{\theta}\) is continuously differentiable in \(\mathbf{x}\), so \(t\mapsto\frac{\partial\mathbf{f}_{\theta}}{\partial[\mathbf{x}_{t},\mathbf{z}, \theta]}(\mathbf{x}_{t},\mathbf{z},t)\) is a continuous function on the compact set \([0,T]\), so it is bounded by some \(L>0\). Likewise, for \(\mathbf{a}\in\mathbb{R}^{d}\) the map \((\mathbf{a},t)\mapsto-\mathbf{a}^{\top}\frac{\partial\mathbf{f}_{\theta}(\mathbf{ x}_{t},\mathbf{z},t)}{\partial[\mathbf{x}_{t},\mathbf{z},\theta]}\) is Lipschitz in \(\mathbf{a}\) with Lipschitz constant \(L\) and this constant is independent of \(t\). 

### Proof of Theorem 2.1 when \(k=1\)

Proof.: First, we consider the case of the adjoint state \(\mathbf{a}_{\mathbf{x}}(t)\). Recall that the AdjointDEIS-1 solver for \(\mathbf{a}_{\mathbf{x}}\) with higher-order error terms is given by

\[\mathbf{a}_{\mathbf{x}}(t_{i+1})=\frac{\alpha_{t_{i}}}{\alpha_{t_{i+1}}} \mathbf{a}_{\mathbf{x}}(t_{i})+\sigma_{t_{i+1}}(e^{h_{i}}-1)\frac{\alpha_{t}^ {2}}{\alpha_{t_{i+1}}^{2}}\mathbf{a}_{\mathbf{x}}(t_{i})^{\top}\frac{\partial \mathbf{\epsilon}_{\theta}(\mathbf{x}_{t_{i}},\mathbf{z},t)}{\partial\mathbf{x}_{ t_{i}}}+\mathcal{O}(h_{i}^{2}),\] (C.4)

where we let \(t_{i}=t\), \(t_{i+1}=s\), \(h_{i}=\lambda_{t_{i+1}}-\lambda_{t_{i}}\) from Equation (B.21). By Theorem C.1 and Equation (B.21) it holds that

\[\tilde{\mathbf{a}}_{\mathbf{x}}(t_{i+1}) =\frac{\alpha_{t_{i}}}{\alpha_{t_{i+1}}}\tilde{\mathbf{a}}_{ \mathbf{x}}(t_{i})+\sigma_{t_{i+1}}(e^{h_{i}}-1)\frac{\alpha_{t_{i}}^{2}}{ \alpha_{t_{i+1}}^{2}}\tilde{\mathbf{a}}_{\mathbf{x}}(t_{i})^{\top}\frac{ \partial\mathbf{\epsilon}_{\theta}(\tilde{\mathbf{x}}_{t_{i}},\mathbf{z},t)}{ \partial\tilde{\mathbf{x}}_{t_{i}}}\] \[=\frac{\alpha_{t_{i}}}{\alpha_{t_{i+1}}}\tilde{\mathbf{a}}_{ \mathbf{x}}(t_{i})+\sigma_{t_{i+1}}(e^{h_{i}}-1)\frac{\alpha_{t_{i}}^{2}}{ \alpha_{t_{i+1}}^{2}}\bigg{(}\mathbf{a}_{\mathbf{x}}(t_{i})^{\top}\frac{ \partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t_{i}},\mathbf{z},t)}{\partial \mathbf{x}_{t_{i}}}+\mathcal{O}(\tilde{\mathbf{a}}_{\mathbf{x}}(t_{i})- \mathbf{a}_{\mathbf{x}}(t_{i}))\bigg{)}\] \[=\frac{\alpha_{t_{i}}}{\alpha_{t_{i+1}}}\mathbf{a}_{\mathbf{x}}(t _{i})+\sigma_{t_{i+1}}(e^{h_{i}}-1)\frac{\alpha_{t_{i}}^{2}}{\alpha_{t_{i+1}}^{ 2}}\mathbf{a}_{\mathbf{x}}(t_{i})^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}( \mathbf{x}_{t_{i}},\mathbf{z},t)}{\partial\mathbf{x}_{t_{i}}}+\mathcal{O}( \tilde{\mathbf{a}}_{\mathbf{x}}(t_{i})-\mathbf{a}_{\mathbf{x}}(t_{i}))\] \[=\mathbf{a}_{\mathbf{x}}(t_{i+1})+\mathcal{O}(h_{max}^{2})+ \mathcal{O}(\tilde{\mathbf{a}}_{\mathbf{x}}(t_{i})-\mathbf{a}_{\mathbf{x}}(t_ {i})).\] (C.5)

Repeat, this argument, from \(\tilde{\mathbf{a}}_{\mathbf{x}}(t_{0})=\mathbf{a}_{\mathbf{x}}(0)\) then we find

\[\tilde{\mathbf{a}}_{\mathbf{x}}(t_{M})=\mathbf{a}_{\mathbf{x}}(T)+\mathcal{O} (Mh_{max}^{2})=\mathbf{a}_{\mathbf{x}}(T)+\mathcal{O}(h_{max}).\] (C.6)

Although the argument for the adjoint state \(\mathbf{a}_{\mathbf{z}}(t)\) follows an analogous form to the one above, we explicitly state it for completeness. Recall that the AdjointDEIS-1 solver for \(\mathbf{a}_{\mathbf{z}}\) with higher-order error terms is given by

\[\mathbf{a}_{\mathbf{z}}(t_{i+1})=\mathbf{a}_{\mathbf{z}}(t_{i})+\sigma_{t_{i+1}} (e^{h_{i}}-1)\frac{\alpha_{t_{i}}}{\alpha_{t_{i+1}}}\mathbf{a}_{\mathbf{x}}(t_ {i})^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t_{i}},\mathbf{z},t )}{\partial\mathbf{z}}+\mathcal{O}(h_{i}^{2}).\] (C.7)

[MISSING_PAGE_FAIL:24]

Proof of Theorem 2.2

For additional clarity, we let \(\mathbf{x}(t)\equiv\mathbf{x}_{t}\) and likewise, \(\mathbf{z}(t)\equiv\mathbf{z}_{t}\).

Proof.: Recall that \(\mathbf{z}(t)\) is a piecewise function of time with partitions of the time domain given by \(\Pi=\{0=t_{0}<t_{1}<\dots<t_{n}=T\}\). Without loss of generality we consider some time interval \([t_{m-1},t_{m}]\) for some \(1\leq m\leq n\). Consider the augmented state defined on the interval \(\pi\):

\[\frac{\mathrm{d}}{\mathrm{d}t}\begin{bmatrix}\mathbf{x}\\ \mathbf{z}\end{bmatrix}(t)=\mathbf{f}_{\text{aug}}=\begin{bmatrix}\mathbf{f}_{\theta}( \mathbf{x}(t),\mathbf{z}(t),t)\\ \overrightarrow{\partial}\mathbf{z}(t)\end{bmatrix},\] (D.1)

where \(\overrightarrow{\partial}\mathbf{z}:[0,T]\to\mathbb{R}^{z}\) denotes the right derivative of \(\mathbf{z}\) at time \(t\). Let \(\mathbf{a}_{\text{aug}}\) denote the associated augmented adjoint state

\[\mathbf{a}_{\text{aug}}(t)\coloneqq\begin{bmatrix}\mathbf{a}_{\mathbf{x}}\\ \mathbf{a}_{\mathbf{z}}\end{bmatrix}(t).\] (D.2)

The Jacobian of \(\mathbf{f}_{\text{aug}}\) has the form

\[\frac{\partial\mathbf{f}_{\text{aug}}}{\partial[\mathbf{x},\mathbf{z}]}=\begin{bmatrix} \frac{\partial\mathbf{f}_{\theta}(\mathbf{x},\mathbf{z},t)}{\partial\mathbf{x}}& \frac{\partial\mathbf{f}_{\theta}(\mathbf{x},\mathbf{z},t)}{\partial\mathbf{z}}\\ \overrightarrow{\partial}&\end{bmatrix}.\] (D.3)

As the conditional information \(\mathbf{z}(t)\) evolves with \(\overrightarrow{\partial}\mathbf{z}(t)\) on \([t_{m-1},t_{m}]\) in the forward flow of time. The derivative of the \(\overrightarrow{\partial}\mathbf{z}\) w.r.t. \(\mathbf{z}\) is clearly \(\mathbf{0}\) as \(\overrightarrow{\partial}\mathbf{z}\) is a function only of time \(t\). Remark, that as the bottom row of the Jacobian \(\mathbf{f}_{\text{aug}}\) is all \(\mathbf{0}\) and \(\mathbf{f}_{\theta}\) is continuous in \(t\) we can consider the evolution of \(\mathbf{a}_{\text{aug}}\) over the whole interval \([0,T]\) rather than just the partition \([t_{m-1},t_{m}]\). Using Equations (D.2) and (D.3) we can define the evolution of the adjoint augmented state on \([0,T]\) as

\[\frac{\mathrm{d}\mathbf{a}_{\text{aug}}}{\mathrm{d}t}(t)=-\left[\mathbf{a}_{ \mathbf{x}}\quad\mathbf{a}_{\mathbf{z}}\right](t)\frac{\partial\mathbf{f}_{\text{ aug}}}{\partial[\mathbf{x},\mathbf{z}]}(t).\] (D.4)

Therefore, \(\mathbf{a}_{\mathbf{z}}(t)\) evolves with the ODE

\[\mathbf{a}_{\mathbf{z}}(T)=0,\qquad\frac{\mathrm{d}\mathbf{a}_{\mathbf{z}}}{ \mathrm{d}t}(t)=-\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{\partial\mathbf{f}_{\theta }(\mathbf{x}(t),\mathbf{z}(t),t)}{\partial\mathbf{z}(t)}.\] (D.5)

We have thus shown the evolution of \(\mathbf{a}_{\mathbf{z}}(t)\) for some continuously differentiable function \(\mathbf{z}(t)\).

Now we prove the solution is unique and exists. As \(\mathbf{x}(t)\) is continuous and \(\mathbf{f}_{\theta}\) is continuously differentiable in \(\mathbf{x}\), it follows that the map \(t\mapsto\frac{\partial\mathbf{f}_{\theta}}{\partial\mathbf{x}}(\mathbf{x}(t), \mathbf{z}(t),t)\) is a continuous function on the compact set \([0,T]\), and therefore it is bounded by some \(L>0\). Correspondingly, for \(\mathbf{a}_{\mathbf{x}}\in\mathbb{R}^{d}\) it follows that the map \((\mathbf{a}_{\mathbf{x}},t)\mapsto-\mathbf{a}_{\mathbf{x}}^{\top}\frac{ \partial\mathbf{f}_{\theta}}{\partial[\mathbf{x},\mathbf{z}]}(\mathbf{x}(t), \mathbf{z}(t),t)\) is Lipschitz in \(\mathbf{a}_{\mathbf{x}}\) with Lipschitz constant \(L\) and this constant is independent of \(t\). Therefore, by the Picard-Lindelolf theorem the solution \(\mathbf{a}_{\mathbf{z}}(t)\) exists and is unique. 

### Continuous-time Extension of Discrete Conditional Information

The proof above makes some fairly lax assumptions about \(\mathbf{z}(t)\). While we assumed the process was cadlag and had right derivatives this restraints are too loose for adaptive step-size solvers. While this is not an issue for our AdjointDEIS, Kidger et al. [58] pointed out that an adaptive solver would take a long time to compute the backward pass as it would have to slow down at the discontinuities. One could follow the approach taken in [58] and construct natural cubic splines from a fully observed, but irregularly sampled time series \(\{\mathbf{z}_{t_{i}}\}_{i=1}^{N}\) with \(0=t_{0}<\dots<t_{n}=T\). Then define \(\mathbf{Z}:[0,T]\to\mathbb{R}^{z}\) as the natural cubic spline with knots at \(t_{0},\dots,t_{n}\) such that \(\mathbf{Z}(t_{i})=\mathbf{z}_{t_{i}}\).

### Connection to Neural CDEs

Kidger et al. [58, Theorem C.1] showed that any equation of the form

\[\mathbf{x}_{t}=\mathbf{x}_{0}+\int_{0}^{t}\mathbf{h}_{\theta}(\mathbf{x}_{s}, \mathbf{z}_{s},s)\ \mathrm{d}s,\] (D.6)can be rewritten as a neural _controlled differential equation_ (CDE) of the form

\[\mathbf{x}_{t}=\mathbf{x}_{0}+\int_{0}^{t}\mathbf{f}_{\theta}(\mathbf{x}_{s},s)\; \mathrm{d}\mathbf{z}_{s},\] (D.7)

where \(\int\;\mathrm{d}\mathbf{z}_{s}\) is the Riemann-Stieltjes integral. _N.B._, the converse is not true.

While there a certainly benefits to modeling with a neural CDE over a neural ODE, diffusion models in particular pre-train the model in the neural ODE sense. Moreover, we are also interested in updating \(\mathbf{z}_{t_{i}}\) at each \(t_{i}\) via gradient descent to solve our optimization problem.

## Appendix E Details on Adjoints for SDEs

In this section, we provide further details on the continuous adjoint equations for diffusion SDEs that we omitted from the main paper due to their technical nature and for the purpose of brevity.

Consider the Ito integral given by

\[\mathbf{x}_{T}=\int_{0}^{T}\mathbf{x}_{t}\;\mathrm{d}\mathbf{w}_{t},\] (E.1)

where \(\mathbf{x}_{t}\) is a continuous semi-martingale adapted to the filtration generated by the Wiener process \(\{\mathbf{w}_{t}\}_{t\in[0,T]}\), \(\{\mathcal{F}_{t}\}_{t\in[0,T]}\). The following quantity, however, is not defined

\[\int_{T}^{0}\mathbf{x}_{t}\;\mathrm{d}\mathbf{w}_{t}.\] (E.2)

This is because \(\mathbf{x}_{t}\) and \(\mathbf{w}_{t}\) are adapted to \(\{\mathcal{F}_{t}\}_{t\in[0,T]}\) which is defined in forwards time. This means \(\mathbf{x}_{t}\) does not anticipate future events only depends on _past_ events. While this is generally sufficient when we wish to integrate backwards in time, we want _future_ events to inform _past_ events.

### Stratonovich Symmetric Integrals and Two-sided Filtration

Clearly, we need a different tool to model this backwards SDE. As such, taking inspiration from the work on neural SDEs [59], we follow the treatment of Kunita [60] for the forward and backward Fisk-Stratonovich integrals using _two-sided filtration_. Let \(\{\mathcal{F}_{s,t}\}_{s\leq t,s,t\in[0,T]}\) be a two-sided filtration, where \(\mathcal{F}_{s,t}\) is the \(\sigma\)-algebra generated by \(\{\mathbf{w}_{v}-\mathbf{w}_{u}:s\leq u\leq v\leq t\}\) for \(s,t\in[0,T]\) such that \(s\leq t\).

**Forward time.** For a continuous semi-martingale \(\{\mathbf{x}_{t}\}_{t\in[0,T]}\) adapted to the forward filtration \(\{\mathcal{F}_{0,t}\}_{t\in[0,t]}\), the Stratonovich stochastic integral is given as

\[\int_{0}^{T}\mathbf{x}_{t}\circ\mathrm{d}\mathbf{w}_{t}=\lim_{|\Pi|\to 0} \sum_{k=1}^{N}\frac{\mathbf{x}_{t_{k}}+\mathbf{x}_{t_{k-1}}}{2}(\mathbf{w}_{t_{ k}}-\mathbf{w}_{t_{k-1}})\] (E.3)

where \(\Pi=\{0=t_{0}<\cdots<t_{N}=T\}\) is a partition of the interval \([0,T]\) and \(|\Pi|=\max_{k}t_{k}-t_{k-1}\). The forward filtration \(\{\mathcal{F}_{0,t}\}_{t\in[0,t]}\) is analogous to the filtration defined in the prior section; therefore, any continuous semi-martingale adapted to it only considers _past_ events and does not anticipate _future_ events.

**Reverse time.** Consider the backwards Wiener process \(\widetilde{\mathbf{w}}_{t}=\mathbf{w}_{t}-\mathbf{w}_{T}\) that is adapted to the backward filtration \(\{\mathcal{F}_{s,T}\}_{s\in[0,T]}\), then for a continuous semi-martingale \(\{\widetilde{\mathbf{x}}_{t}\}_{t\in[0,T]}\) adapted to the backward filtration, the backward Stratonovich integral is

\[\int_{0}^{T}\widetilde{\mathbf{x}}_{t}\circ\mathrm{d}\widetilde{\mathbf{w}}_{t }=\lim_{|\Pi|\to 0}\sum_{k=1}^{N}\frac{\widetilde{\mathbf{x}}_{t_{k}}+ \widetilde{\mathbf{x}}_{t_{k-1}}}{2}(\widetilde{\mathbf{w}}_{t_{k-1}}- \widetilde{\mathbf{w}}_{t_{k}})\] (E.4)

The backward filtration \(\{\mathcal{F}_{s,T}\}_{s\in[0,T]}\) is the opposite of the forward filtration in the sense that continuous semi-martingales adapted to it only depend on _future_ events and do not anticipate _past_ events. As such, time is effectively reversed.

**Remark E.1**.: _While the Stratonovich symmetric integrals give us a powerful tool for integrating forwards and backwards in time with stochastic integrals, it is important that we use the **same** realization of the Wiener process._

### Stochastic Flow of Diffeomorphisms

Consider the Stratonovich SDE defined as

\[\mathbf{x}_{T}=\mathbf{x}_{0}+\int_{0}^{T}\mathbf{f}(\mathbf{x}_{t},t)\;\mathrm{d}t+ \int_{0}^{T}\mathbf{g}(\mathbf{x}_{t},t)\circ\mathrm{d}\mathbf{w}_{t},\] (E.5)

where \(\mathbf{f},\mathbf{g}\in\mathcal{C}_{b}^{\infty,1}\), _i.e._, they belong to the class of functions with infinitely many bounded derivatives w.r.t. the state and bounded first derivatives w.r.t. time. Thus, the SDE has a unique strong solution. Given a realization of the Wiener process, there exists a smooth mapping \(\Phi\) called the _stochastic flow_ such that \(\Phi_{s,t}(\mathbf{x}_{s})\) is the solution at time \(t\) of the process described in Equation (E.5) started at \(\mathbf{x}_{s}\) at time \(s\leq t\). This then defines a collection of continuous maps \(\mathcal{S}=\{\Phi_{s,t}\}_{s\leq t,s\in[0,T]}\) from \(\mathbb{R}^{d}\) to itself.

Kunita [60, Theorem 3.7.1] shows that with probability 1 this collection \(\mathcal{S}\) satisfies the flow property

\[\Phi_{s,t}(\mathbf{x}_{s})=\Phi_{u,t}(\Phi_{s,u}(\mathbf{x}_{s}))\quad s\leq u \leq t,\mathbf{x}_{s}\in\mathbb{R}^{d},\] (E.6)

and that each \(\Phi_{s,t}\) is a smooth diffeomorphism from \(\mathbb{R}^{d}\) to itself. Hence, \(\mathcal{S}\) is the stochastic flow of diffeomorphisms generated by Equation (E.5). Moreover, the backward flow \(\widetilde{\Psi}_{s,t}\coloneqq\Phi_{s,t}^{-1}\) satisfies the backwards SDE:

\[\widetilde{\Psi}_{s,t}(\mathbf{x}_{t})=\mathbf{x}_{t}-\int_{s}^{t}\mathbf{f}( \widetilde{\Psi}_{u,t}(\mathbf{x}_{t}),u)\;\mathrm{d}u-\int_{s}^{t}\mathbf{g}( \widetilde{\Psi}_{u,t}(\mathbf{x}_{t}),u)\circ\mathrm{d}\widetilde{\mathbf{w} }_{u},\] (E.7)

for all \(s,t\in[0,T]\) such that \(s\leq t\). This formulation makes intuitive sense as the _backwards_ SDE differs only from the _forwards_ SDE by a negative sign.

### Continuous Adjoint Equations

Now consider the adjoint flow \(\mathbf{A}_{s,t}(\mathbf{x}_{s})=\partial\mathcal{L}(\Phi_{s,t}(\mathbf{x}_{s }))/\partial\mathbf{x}_{s}\), then \(\widetilde{\mathbf{A}}_{s,t}(\mathbf{x}_{t})=\mathbf{A}_{s,t}(\widetilde{ \Psi}_{s,t}(\mathbf{x}_{t}))\). Li et al. [59] show that \(\widetilde{\mathbf{A}}_{s,t}(\mathbf{x}_{t})\) satisfies the backward SDE:

\[\widetilde{\mathbf{A}}_{s,t}(\mathbf{x}_{t})=\frac{\partial\mathcal{L}}{ \partial\mathbf{x}_{t}}+\int_{s}^{t}\widetilde{\mathbf{A}}_{u,t}(\mathbf{x}_{ t})\frac{\partial\mathbf{f}}{\partial\mathbf{x}_{u}}(\widetilde{\Psi}_{u,t}(\mathbf{x}_{ t}),u)\;\mathrm{d}u+\int_{s}^{t}\widetilde{\mathbf{A}}_{u,t}(\mathbf{x}_{t}) \frac{\partial\mathbf{g}}{\partial\mathbf{x}_{u}}(\widetilde{\Psi}_{u,t}(\mathbf{x }_{t}),u)\circ\mathrm{d}\widetilde{\mathbf{w}}_{u}.\] (E.8)

As the drift and diffusion coefficient of this SDE are in \(\mathcal{C}_{b}^{\infty,1}\), the system has a unique strong solution.

### Proof of Theorem 3.1

We are now ready to put all of this together to prove the result from the main paper.

Proof.: \[\mathrm{d}\mathbf{x}_{t}=\mathbf{f}(\mathbf{x}_{t},t)\;\mathrm{d}t+\mathbf{g}(t) \circ\mathrm{d}\mathbf{w}_{t}.\] (E.9)

By Equation (E.8) the adjoint state admitted by the flow of diffeomorphisms generated by Equation (E.9) evolves with the SDE

\[\widetilde{\mathbf{A}}_{s,t}(\mathbf{x}_{t}) =\frac{\partial\mathcal{L}}{\partial\mathbf{x}_{t}}+\int_{s}^{t }\widetilde{\mathbf{A}}_{u,t}(\mathbf{x}_{t})\frac{\partial\mathbf{f}}{\partial \mathbf{x}_{u}}(\widetilde{\Psi}_{u,t}(\mathbf{x}_{t}),u)\;\mathrm{d}u+ \underbrace{\int_{s}^{t}\widetilde{\mathbf{A}}_{u,t}(\mathbf{x}_{t})\frac{ \partial\mathbf{g}}{\partial\mathbf{x}_{u}}(u)\circ\mathrm{d}\widetilde{\mathbf{w }}_{u}}_{=\mathbf{0}}\] \[=\frac{\partial\mathcal{L}}{\partial\mathbf{x}_{t}}+\int_{s}^{t} \widetilde{\mathbf{A}}_{u,t}(\mathbf{x}_{t})\frac{\partial\mathbf{f}}{\partial \mathbf{x}_{u}}(\widetilde{\Psi}_{u,t}(\mathbf{x}_{t}),u)\;\mathrm{d}u.\] (E.10)

Clearly, the adjoint state evolves with an ODE revolving around only the drift coefficient, _i.e._, \(\mathbf{f}\). Therefore, we can rewrite the evolution of the adjoint state as

\[\mathrm{d}\mathbf{a}_{\mathbf{x}}(t)=-\mathbf{a}_{\mathbf{x}}(t)^{\top}\frac{ \partial\mathbf{f}}{\partial\mathbf{x}_{t}}(\mathbf{x}_{t},t)\;\mathrm{d}t.\] (E.11)

### Converting the Ito SDE to Stratonovich

The diffusion SDE in Equation (3.1) is defined as an Ito SDE. However, Theorem 3.1 is defined as Stratonovich SDEs. However, an Ito SDE can be easily converted into the Stratonovich form, _i.e._, for some Ito SDE of the form

\[\mathrm{d}\mathbf{x}_{t}=\mathbf{f}(\mathbf{x}_{t},t)\;\mathrm{d}t+\mathbf{g}(\mathbf{x }_{t},t)\;\mathrm{d}\mathbf{w}_{t}\] (E.12)

with a differentiable function \(\sigma\), there exists a corresponding Stratonovich SDE of the form

\[\mathrm{d}\mathbf{x}_{t}=[\mathbf{f}(\mathbf{x}_{t},t)+\frac{1}{2}\frac{\partial \mathbf{g}}{\partial\mathbf{x}}(\mathbf{x}_{t},t)\cdot\mathbf{g}(\mathbf{x}_{t},t)]\; \mathrm{d}t+\mathbf{g}(\mathbf{x}_{t},t)\circ\mathrm{d}\mathbf{w}_{t}.\] (E.13)

As Equation (3.1) is defined such that \(\mathbf{g}(\mathbf{x}_{t},t)=g(t)\) and is independent of the state \(\mathbf{x}_{t}\), then the SDE may be written in Stratonovich form as

\[\mathrm{d}\mathbf{x}_{t}=\left[f(t)\mathbf{x}_{t}+\frac{g^{2}(t)}{\sigma_{t}} \mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)\right]\;\mathrm{d}t+g(t) \circ\mathrm{d}\bar{\mathbf{w}}_{t}.\] (E.14)

## Appendix F Additional Experiments

In this section, we include some additional experiments which did not fit within the main paper.

### Impact of Discretization Steps

One of the advantages of AdjointDEIS is that the solver for the diffusion ODE and continuous adjoint equations are distinct. This means that we do not have to force \(N=M\) enabling greater flexibility when using AdjointDEIS. As such, we explore the impact of using fewer steps to estimate the gradient while keeping the number of sampling steps \(N=20\) fixed. In Figure 4 we illustrate the impact of the change in the number of discretization steps when estimating the gradients. Unsurprisingly, the fewer steps we take, the less accurate the gradients are. This matches the empirical data presented in Table 4 which measures the impact of the performance of face morphing measured in MMPMR.

To explore the degradation in performance further. As Kidger [25] points out, due to recalculating the solution trajectory of the underlying state \(\mathbf{x}_{t}\) backwards can differ, in not significant ways from the \(\mathbf{x}_{t}\) calculated in the forward pass due to truncation errors. This is especially true for non-algebraically reversible solvers [61], although these can suffer from poor regions of stability. Consider the toy

\begin{table}
\begin{tabular}{l r r r} \hline \hline  & \multicolumn{2}{c}{**MMPMR(\(\uparrow\))**} \\ \cline{2-4} \(M\) (\(\downarrow\)) & **AdaFace** & **ArcFace** & **ElasticFace** \\ \hline
15 & **94.89** & **90.59** & **94.07** \\
10 & 94.27 & 91.21 & 92.84 \\
05 & 69.94 & 60.74 & 64.21 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Impact of number of discretization steps, \(M\), on face morphing with AdjointDEIS. FMR = 0.1%, \(\eta=0.1\).

Figure 4: Morphed faces created by guided generation with AdjointDEIS with differing number of discretization steps.

example of the ODE \(\dot{x}(t)=\lambda x(t)\), where \(\lambda<0\) and some numerical ODE solver. During the forward solve, most numerical ODE solvers with a non-trivial region of stability will find a reasonably useful solution as the errors decay exponentially; however, during the backward solve any small error is magnified exponentially instead.

One possible solution to this is to use interpolated adjoints wherein the solution states, \(\mathbf{x}_{t}\), but not the internal states of \(\boldsymbol{f}_{\theta}\) are stored. The backward solve can then interpolate between them as needed. Kim et al. [56] report that interpolated adjoints performed well on a stiff differential equation. In the case when \(N=M\) it is quite convenient to simply store \(\{\mathbf{x}_{t_{i}}\}_{i=1}^{m}\) during the forward solve. Moreover, in our use case where \(N\) is quite small this adds little overhead. In Table 5 we compare recording the solution states vs finding them via solving DDIM forwards in time. We observe that AdjointDEIS performed better when using the recorded states.

### Impact of Learning Rate

We measure the impact of the learning rate on guided generation with AdjointDEIS in Table 4. Unsurprisingly, high learning rates lower performance, especially for less accurate gradients. _I.e._, when \(M\) is small. We illustrate an example of the impact in Figure 5. Clearly, the learning rate of \(\eta=1\) starts to distort the images even if it still fools the FR system.

### Number of Steps

As alluded to in the main paper, one of the drawbacks of diffusion SDEs is that they require small step sizes to work properly. We observe that the missing high frequency content is added back in when the step size is increased, see Figure 6.

## Appendix G Implementation Details

### AdjointDEIS-2M Algorithm

For completeness we have the full AdjointDEIS-2M solver implemented in Algorithm 1 for solving the continuous adjoint equations for diffusion ODEs. We assume that there is another solver that

Figure 5: Morphed faces created by guided generation with AdjointDEIS with different learning rates. All used \(M=20\) the ODE variant.

\begin{table}
\begin{tabular}{c r r r} \hline \hline  & \multicolumn{3}{c}{**MMPMR(\(\uparrow\))**} \\ \cline{2-4} Record & **AdaFace** & **ArcFace** & **ElasticFace** \\ \hline ✗ & 94.27 & 89.78 & 93.87 \\ ✓ & 95.5 & 92.64 & 95.91 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Recording the solution states vs backwards solve with DDIM. \(M=20,\eta=0.001\). FMR = 0.1%.

solves the backward ODE to yield \(\{\tilde{\mathbf{x}}_{t_{i}}\}_{i=0}^{M}\). Remark that \(\mathbf{a}_{\text{aug}}\coloneqq[\mathbf{a}_{\mathbf{x}},\mathbf{a}_{\mathbf{z}},\mathbf{a}_{\theta}]\). Also Algorithm 1 can be used to solve the continuous adjoint equations for diffusion SDEs by simply adding the factor of 2 into the update equations.

### Code

Our code for AdjointDEIS will be available here at [https://github.com/zblasingame/AdjointDEIS](https://github.com/zblasingame/AdjointDEIS).

### Repositories Used

For reproducibility purposes, we provide a list of links to the official repositories of other works used in this paper.

1. The SYN-MAD 2022 dataset used in this paper can be found at [https://github.com/marcohuber/SYN-MAD-2022](https://github.com/marcohuber/SYN-MAD-2022).
2. The ArcFace models, MS1M-RetinaFace dataset, and MS1M-ArcFace dataset can be found at [https://github.com/deepinsight/insightface](https://github.com/deepinsight/insightface).
3. The ElasticFace model can be found at [https://github.com/fdbtrs/ElasticFace](https://github.com/fdbtrs/ElasticFace).
4. The AdaFace model can be found at [https://github.com/mk-minchul/AdaFace](https://github.com/mk-minchul/AdaFace).
5. The official Diffusion Autoencoders repository can be found at [https://github.com/phizaz/diffae](https://github.com/phizaz/diffae).
6. The official MIPGAN repository can be found at [https://github.com/ZHYYYYYYYYY/MIPGAN-face-morphing-algorithm](https://github.com/ZHYYYYYYYYY/MIPGAN-face-morphing-algorithm).

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline  & & & \multicolumn{4}{c}{**MMPMR(\(\uparrow\))**} \\ \cline{3-6}
**SDE** & \(\eta\) & \(M\) (\(\downarrow\)) & **AdaFace** & **ArcFace** & **ElasticFace** \\ \hline ✗ & 1 & 20 & 98.77 & 98.98 & 98.77 \\ ✗ & 0.1 & 20 & **99.8** & 98.77 & **99.39** \\ ✗ & 0.01 & 20 & 95.5 & 92.64 & 95.91 \\ ✗ & 1 & 10 & 50.92 & 49.69 & 50.92 \\ ✗ & 0.1 & 10 & 94.27 & 91.21 & 92.84 \\ ✗ & 1 & 05 & 2.66 & 2.04 & 1.84 \\ ✗ & 0.1 & 05 & 69.94 & 60.74 & 64.21 \\ ✓ & 1 & 20 & 98.57 & **99.59** & 98.98 \\ ✓ & 0.1 & 20 & 98.57 & 97.96 & 97.75 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Impact of learning rate, \(\eta\), on face morphing with AdjointDEIS. FMR = 0.1%.

Figure 6: Morphed faces created by guided generation with AdjointDEIS with different number of sampling steps. SDE solver, \(M=N\).

```
0: Initial values \(\mathbf{a}_{\mathbf{x}}(0)\), monotonically increasing time steps \(\{t_{i}\}_{i=0}^{M}\), and noise prediction model \(\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},\mathbf{z},t)\).
1: Denote \(h_{i}=\lambda_{t_{i+1}}-\lambda_{t_{i}}\), for \(i=0,\ldots,M-1\).
2:\(\tilde{\mathbf{a}}_{\mathbf{x}}(t_{0})\leftarrow\mathbf{a}_{\mathbf{x}}(0)\)\(\triangleright\) Initialize an empty buffer \(Q\).
3:\(\tilde{\mathbf{a}}_{\mathbf{z}}(t_{0})\leftarrow\mathbf{0},\tilde{\mathbf{a}}_{ \theta}(t_{0})\leftarrow\mathbf{0}\).
4:\(Q\stackrel{{\text{buffer}}}{{\longleftarrow}}[\tilde{\mathbf{V}}( \mathbf{x},t_{0}),\tilde{\mathbf{V}}(\mathbf{z},t_{0}),\tilde{\mathbf{V}}( \theta,t_{0})]\)
5:\(\tilde{\mathbf{a}}_{\mathbf{x}}(t_{1})\leftarrow\frac{\alpha_{t_{1}}}{\alpha_{ t_{1}}}\tilde{\mathbf{a}}_{\mathbf{x}}(t_{0})+\sigma_{t_{1}}(e^{h_{0}}-1)\frac{ \alpha_{t_{0}}^{2}}{\alpha_{t_{1}}^{2}}\tilde{\mathbf{a}}_{\mathbf{x}}(t_{0}) ^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}(\tilde{\mathbf{x}}_{t_{0}},\mathbf{ z},t_{0})}{\partial\tilde{\mathbf{x}}_{t_{0}}}\)
6:\(\tilde{\mathbf{a}}_{\mathbf{z}}(t_{1})\leftarrow\tilde{\mathbf{a}}_{\theta}(t_ {0})+\sigma_{t_{1}}(e^{h_{0}}-1)\frac{\alpha_{t_{0}}}{\alpha_{t_{1}}}\tilde{ \mathbf{a}}_{\mathbf{x}}(t_{0})^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}( \tilde{\mathbf{x}}_{t_{0}},\mathbf{z},t_{0})}{\partial\theta}\)
7:\(\tilde{\mathbf{a}}_{\theta}(t_{1})\leftarrow\tilde{\mathbf{a}}_{\theta}(t_{0})+ \sigma_{t_{1}}(e^{h_{0}}-1)\frac{\alpha_{t_{0}}}{\alpha_{t_{1}}}\tilde{\mathbf{ a}}_{\mathbf{x}}(t_{0})^{\top}\frac{\partial\mathbf{\epsilon}_{\theta}(\tilde{ \mathbf{x}}_{t_{0}},\mathbf{z},t_{0})}{\partial\theta}\)
8:\(Q\stackrel{{\text{buffer}}}{{\longleftarrow}}[\tilde{\mathbf{V}}( \mathbf{x},t_{1}),\tilde{\mathbf{V}}(\mathbf{z},t_{1}),\tilde{\mathbf{V}}( \theta,t_{1})]\)
9:for\(i\gets 1,2,\ldots,M-1\)do
10:\(\rho_{i}\leftarrow\frac{h_{i-1}}{h_{i}}\)
11:\(\mathbf{D}_{i}\leftarrow\Big{(}1+\frac{1}{2\rho_{i}}\Big{)}\tilde{\mathbf{V}}( \mathbf{x};t_{i})-\frac{1}{2\rho_{i}}\tilde{\mathbf{V}}(\mathbf{x};t_{i-1})\)
12:\(\mathbf{E}_{i}\leftarrow\Big{(}1+\frac{1}{2\rho_{i}}\Big{)}\tilde{\mathbf{V}}( \mathbf{z};t_{i})-\frac{1}{2\rho_{i}}\tilde{\mathbf{V}}(\mathbf{z};t_{i-1})\)
13:\(\mathbf{F}_{i}\leftarrow\Big{(}1+\frac{1}{2\rho_{i}}\Big{)}\tilde{\mathbf{V}}( \theta;t_{i})-\frac{1}{2\rho_{i}}\tilde{\mathbf{V}}(\theta;t_{i-1})\)
14:\(\tilde{\mathbf{a}}_{\mathbf{x}}(t_{i+1})\leftarrow\frac{\alpha_{t_{i}}}{\alpha_ {t_{i+1}}}\tilde{\mathbf{a}}_{\mathbf{x}}(t_{i})+\frac{\sigma_{t_{i+1}}}{ \alpha_{t_{i+1}}}(e^{h_{i}}-1)\mathbf{D}_{i}\)
15:\(\tilde{\mathbf{a}}_{\mathbf{z}}(t_{i+1})\leftarrow\tilde{\mathbf{a}}_{\mathbf{z }}(t_{i})+\frac{\sigma_{t_{i+1}}}{\alpha_{t_{i+1}}}(e^{h_{i}}-1)\mathbf{E}_{i}\)
16:\(\tilde{\mathbf{a}}_{\theta}(t_{i+1})\leftarrow\tilde{\mathbf{a}}_{\theta}(t_{i})+ \frac{\sigma_{t_{i+1}}}{\alpha_{t_{i+1}}}(e^{h_{i}}-1)\mathbf{F}_{i}\)
17:if\(i<M-1\)then
18:\(Q\stackrel{{\text{buffer}}}{{\longleftarrow}}[\tilde{\mathbf{V}}( \mathbf{x},t_{i+1}),\tilde{\mathbf{V}}(\mathbf{z},t_{i+1}),\tilde{\mathbf{V}}( \theta,t_{i+1})]\)
19:endif
20:endfor
21:return\(\tilde{\mathbf{a}}_{\mathbf{x}}(t_{M}),\tilde{\mathbf{a}}_{\mathbf{z}}(t_{M}), \tilde{\mathbf{a}}_{\theta}(t_{M})\).
```

**Algorithm 1** AdjointDEIS-2M.

## Appendix H Experimental Details

In this section, we outline the details for the experiments run in Section 5.

### DiM Algorithm

For completeness, we provide the DiM algorithm from [41] following the notation used in [35]. The original bona fide images are denoted \(\mathbf{x}_{0}^{(a)}\) and \(\mathbf{x}_{0}^{(b)}\). The conditional encoder is \(\mathcal{E}:\mathcal{X}\rightarrow\mathcal{Z}\), \(\Phi\) is the numerical diffusion ODE solver, \(\Phi^{+}\) is the numerical diffusion ODE solver as time runs forwards from \(0\) to \(T\). The algorithm is presented in Algorithm 2.

### Nfe

In our reporting of the NFE we record the number of times the diffusion noise prediction U-Net is evaluated both during the encoding phase, \(N_{E}\), and solving of the PF-ODE or diffusion SDE, \(N\). We chose to report \(N+N_{E}\) over \(N+2N_{E}\) as even though two bona fide images are encoded resulting in \(2N_{E}\) NFE during encoding, this process can simply be batched together, reducing the NFE down to \(N_{E}\). When reporting the NFE for the Morph-PIPE model, we report \(N_{E}+BN\) where \(B\) is the number of blends. While a similar argument can be made that the morphed candidates could be generated in a large batch of size \(B\), reducing the NFE of the sampling process down to \(N\), we chose to report \(BN\) as the number of blends, \(B=21\), used in the Morph-PIPE is quite large, potentially resulting in Out Of Memory (OOM) errors, especially if trying to process a mini-batch of morphs. Using \(N_{E}+N\) reporting over \(N_{E}+BN\), the NFE of Morph-PIPE is 350, which is comparable to DiM. The reporting of NFE for AdjointDEIS was calculated as \(N_{E}+n_{opt}(N+M)\) where \(n_{opt}\)is the number of optimization steps and \(M\) is the number of discretization steps for the continuous adjoint equations.

### Hardware

All of the main experiments were done on a single NVIDIA Tesla V100 32GB GPU. On average, the guided generation experiments for our approach took between 6 - 8 hours for the whole dataset of face morphs with a batch size of 8. Some additional follow-up work for the camera-ready version used an NVIDIA H100 Tensor Core 80GB GPU with a batch size of 16.

### Datasets

The SYN-MAD 2022 dataset is derived from the Face Research Lab London (FRLL) dataset [52]. FRLL is a dataset of high-quality captures of 102 different individuals with frontal images and neutral lighting. There are two images per subject, an image of a "neutral" expression and one of a "smiling" expression. The ElasticFace [53] FR system was used to select the top 250 most similar pairs, in terms of cosine similarity, of bona fide images for both genders, resulting in a total of 489 bona fide image pairs for face morphing [51], as some pairs did not generate good morphs on the reference set; we follow this minimal subset.

### FR Systems

All three FR systems use the Improved ResNet (IResNet-100) architecture [62] as the neural net backbone for the FR system. The ArcFace model is a widely used FR system [41, 43, 45, 49]. It employs an additive angular margin loss to enforce intra-class compactness and inter-class distance, which can enhance the discriminative ability of the feature embeddings [48]. ElasticFace builds upon the ArcFace model by using an elastic penalty margin over the fixed penalty margin used by ArcFace. This change results in an FR system with state-of-the-art performance [53]. Lastly, the AdaFace model employs an adaptive margin loss by weighting the loss relative to an approximation of the image quality [54]. The image quality is approximated via feature norms and is used to give less weight to misclassified images, reducing the impact of "low" quality images on training. This improvement allows the AdaFace model to achieve state-of-the-art performance in FR tasks.

The AdaFace and ElasticFace models are trained on the MS1M-ArcFace dataset, whereas the ArcFace model is trained on the MS1M-RetinaFace dataset. _N.B._, the ArcFace model used in the identity loss is not the same ArcFace model used during evaluation. The model used in the identity loss is an IResNet-100 trained on the Glint360k dataset [63] with the ArcFace loss. We use the cosine distance to measure the distance between embeddings from the FR models. All three FR systems require images of \(112\times 112\) pixels. We resize every image, post alignment from dlib which ensuresthe images are square, to \(112\times 112\) using bilinear down-sampling. The image tensors are then normalized such that they take values in \([-1,1]\). Lastly, the AdaFace FR system was trained on BGR images so the image tensor is shuffled from the RGB format to the BGR format.

## Appendix I Analytic Formulations of Drift and Diffusion Coefficients

For completeness, we show how to analytically compute the drift and diffusion coefficients for a linear noise schedule Ho et al. [1] in the VP scenario Song et al. [15]. With a linear noise schedule \(\log\alpha_{t}\) is found to be

\[\log\alpha_{t}=-\frac{\beta_{1}-\beta_{0}}{4}t^{2}-\frac{\beta_{0}}{2}t\] (I.1)

on \(t\in[0,1]\) with \(\beta_{0}=0.1,\beta_{1}=20\), following Song et al. [15]. The drift coefficient becomes

\[f(t)=-\frac{\beta_{1}-\beta_{0}}{2}t-\frac{\beta_{0}}{2}\] (I.2)

and as \(\sigma_{t}=\sqrt{1-\alpha_{t}^{2}}\) we find

\[\frac{\mathrm{d}\sigma_{t}^{2}}{\mathrm{d}t} =\frac{\mathrm{d}}{\mathrm{d}t}\bigg{[}1-\exp\bigg{(}-\frac{\beta _{1}-\beta_{0}}{4}t^{2}-\frac{\beta_{0}}{2}t\bigg{)}^{2}\bigg{]}\] \[=((\beta_{1}-\beta_{0})t+\beta_{0})\exp\bigg{(}-\frac{\beta_{1}- \beta_{0}}{2}t^{2}-2\beta_{0}t\bigg{)}\] (I.3)

Therefore, the diffusion coefficient \(g^{2}(t)\) is

\[g^{2}(t) =\underbrace{((\beta_{1}-\beta_{0})t+\beta_{0})\exp\bigg{(}-\frac {\beta_{1}-\beta_{0}}{2}t^{2}-2\beta_{0}t\bigg{)}}_{\frac{\mathrm{d}\sigma_{t} ^{2}}{\mathrm{d}t}}\] \[\underbrace{+\big{(}(\beta_{1}-\beta_{0})t+\beta_{0}\bigg{)}\bigg{[} 1-\exp\bigg{(}-\frac{\beta_{1}-\beta_{0}}{4}t^{2}-\frac{\beta_{0}}{2}t\bigg{)}^ {2}\bigg{]}}_{-2\frac{\mathrm{d}\log\alpha_{t}\sigma_{t}^{2}}{\mathrm{d}t}}\] (I.4)

Importantly, \(\frac{\mathrm{d}\sigma_{t}}{\mathrm{d}t}\) does not exist at time \(t=0\), as \(\sigma_{t}\) is discontinuous at that point, and so an approximation is needed when starting from this initial step. In practice, adding a small \(\epsilon\ll 1\) to \(t=0\) should suffice.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed the limitations of this work in Section 6.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The full set of assumptions, derivations, and proofs are found in Appendices B to E.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We presented the implementation details in Appendix G, including the algorithm as well as the repositories used.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The dataset we use are all public dataset, we have provided links to all the repositories used in the experiments in Appendix G. We provide detailed derivations of the AdjointDEIS solvers Appendix B. Interested readers can implement the algorithms themselves. We intend to release our code at [https://github.com/zblasingame/AdjointDEIS](https://github.com/zblasingame/AdjointDEIS).
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental details are presented in Appendix H.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Due the computationally demanding nature of the guided generation we do not report error bars.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [Yes] Justification: The hardware used in this paper is explained in Appendix H.3.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We conducted the research conforming in every aspect with the NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We address the broader impacts in Section 6.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The dataset used in the experiments are public datasets.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The details of the datasets and models used from other researchers are decsribed in Appendices G and H.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [No] Justification: No new assets were created at the time of submission.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We did not perform crowdsourcing. Human faces are used in the experiments, but the dataset we used are all public dataset.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Used public datasets, as such no IRB approvals were needed.