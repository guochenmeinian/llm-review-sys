# GuardT2I: Defending Text-to-Image Models

from Adversarial Prompts

 Yijun Yang\({}^{1,2}\), Ruiyuan Gao\({}^{1}\), Xiao Yang\({}^{2}\), Jianyuan Zhong\({}^{1}\), Qiang Xu\({}^{1}\)

\({}^{1}\)The Chinese University of Hong Kong, \({}^{2}\)Tsinghua University

{yjyang,rygao,jyzhong,qxu}@cse.cuhk.edu.hk,{yangyj16,yangxiao19}@tsinghua.org.cn

This work was carried out as part of Yijun Yang's internship at Tsinghua University.Corresponding authors.

###### Abstract

Recent advancements in Text-to-Image models have raised significant safety concerns about their potential misuse for generating inappropriate or Not-Safe-For-Work contents, despite existing countermeasures such as NSFW classifiers or model fine-tuning for inappropriate concept removal. Addressing this challenge, our study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance Text-to-Image models' robustness against adversarial prompts. Instead of making a binary classification, GuardT2I utilizes a large language model to conditionally transform text guidance embeddings within the Text-to-Image models into natural language for effective adversarial prompt detection, without compromising the models' inherent performance. Our extensive experiments reveal that GuardT2I outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios. Our framework is available at [https://github.com/cure-lab/GuardT2I](https://github.com/cure-lab/GuardT2I).

## 1 Introduction

The recent advancements in Text-to-Image (T2I) models, such as Midjourney[6], Leonardo.Ai[3], DALL-E 3[10], and others[26, 33, 37, 28, 20, 35], have significantly facilitated the generation of high-quality images from textual prompts, as demonstrated in Fig. 1 (a). As the widespread application of T2I models continues, concerns about their misuse have become increasingly prominent [38, 29, 45, 27, 47, 46, 41, 8]. In response, T2I service providers have implemented defensive strategies. However, sophisticated adversarial prompts that appear innocuous to humans can manipulate these models to produce explicit Not-Safe-for-Work (NSFW) content, such as pornography, violence, and political sensitivity [29, 45, 46, 38], raising significant safety challenges, as illustrated in Fig. 1 (b).

Existing defensive methods for T2I models can be broadly classified into two categories: _training interference_ and post-hoc content moderation. _Training interference_ focuses on removing inappropriate concepts during the training process through techniques like dataset filtering [10, 28] or fine-tuning to forget NSFW concepts [12, 15]. While effective in suppressing NSFW generation, these methods often compromise image quality in normal use cases and remain vulnerable to adversarial attacks [42]. On the other hand, _post-hoc content moderation methods_, such as OpenAI-Moderation and Safety-Checker, maintain the synthesis quality therefore being widely used in T2I services [6, 3, 10]. These methods rely on text or image classifiers to identify and block malicious prompts or generated content. However, they struggle to effectively defend against adversarial prompts, as reported in [45, 46].

In this paper, we introduce a new defensive framework called GuardT2I, specifically designed to protect T2I models from adversarial prompts. Our key observation is that although adversarial prompts (as shown in Fig. 1 (b)) may have noticeable visual differences compared to explicitprompts, they still contain the same underlying semantic information within the T2I model's latent space. Therefore, we approach the defense against adversarial prompts as a generative task and harness the power of the large language model (LLM) to effectively handle the semantic meaning embedded in implicit adversarial prompts. Specifically, we modify LLM to a conditional LLM, c-LLM, and fine-tune the c-LLM to "translate" the latent representation of prompts back to plain texts, which can reveal the real intention of the user. For legitimate prompts, as shown in Fig. 1 (c), GuardT2I tries to reconstruct the input prompt, as shown in Fig. 1 (c)'s _Prompt Interpretation_. For adversarial prompts, instead of reconstructing the input prompt, GuardT2I would generate the prompt interpretation conform to the underlying semantic meaning of the adversarial prompt whenever possible, as demonstrated in Fig. 1 (d). Consequently, by estimating the similarity between the input and the synthetic prompt interpretation, we can identify adversarial prompts. GuardT2I accomplishes defense without altering the original T2I models. This ensures that the performance and generation qualities of the T2I models remain intact. Additionally, GuardT2I operates in parallel with the T2I models, thereby imposing no additional inference latency during normal usage. Moreover, GuardT2I has the capability to halt the diffusion steps of malicious prompts at an early stage, which helps to reduce computational costs.

Overall, the **contributions** of this work include:

* To the best of our knowledge, GuardT2I is the first generative paradigm defensive framework specifically designed for T2I models. Through the transformation of latent variables from T2I models into natural language, our defensive framework not only demonstrates exceptional generalizability across various adversarial prompts, but also provide decision-making interpretation.
* We propose a conditional LLM (_c-LLM_) to "translate" the latent back to plain text, coupled with bi-level parsing methods for prompt moderation.
* We perform extensive evaluations for GuardT2I against various malicious attacks, including rigorous adaptive attacks, where attackers have full knowledge of GuardT2I and try to deceive it for NSFW syntheses.

Experimental results demonstrate that GuardT2I outperforms baselines, such as Microsoft Azure [2], Amazon AWS Comprehend [2], and OpenAI-Moderation [23, 19], by a large margin, particularly when facing adaptive attacks. Furthermore, our in-depth analysis reveals that the adaptive adversarial prompts that can bypass GuardT2I tend to have much-weakened synthesis quality.

## 2 Related Work

### Adversarial Prompts

Diffusion-based T2I models, trained on extensive internet-sourced datasets, are adept at producing vibrant and creative imagery [36, 26, 6]. However, the lack of curation in these datasets leads to

Figure 1: **Overview of GuardT2I. GuardT2I can effectively halt the generation process of adversarial prompts to avoid NSFW generations, without compromising normal prompts or increasing inference time.**

generations of NSFW content by the models [38; 27]. Such content may encompass depictions of _violence_, _pornography_, _bullying_, _gore_, _political sensitivity_, _racism_[27]. Currently, such risk mainly comes from two types of adversarial prompts, _i.e_., manually and automatically generated ones.

**Manually Crafted Attacking Prompts.** Schramowski _et al._[38] amass a collection of handwritten adversarial prompts, referred to as _I2P_, from various online communities. These prompts not only lead to the generation of NSFW content but also eschew explicit NSFW keywords. Furthermore, Rando _et al._[29] reverse-engineer the safety filters of a popular T2I model, Stable Diffusion [33]. By adding extraneous text, which effectively deceived the model's safety mechanisms.

**Automatically Generated Adversarial Prompts**. Researchers propose adversarial attack algorithms to automatically construct adversarial prompts for T2I models to induce NSFW contents [38; 45; 46; 41] or functionally disable the T2I models [18]. For instance, by considering the existence of safety prompt filters, SneakyPrompt [46] "jailbreak" T2I models for NSFW images with reinforcement learning strategies. MMA-Diffusion [45] presents a gradient-based attacking method, and showcases current defensive measures in commercial T2I services, such as Midjourney [6] and Leonardo.Ai [3], can be bypassed in the black-box attack way.

### Defensive Methods

**Model Fine-tuning** techniques target at developing harmless T2I models. Typically, they involve concept-erasing solutions [12; 15; 38], which change the weights of existing T2I models [12; 15] or the inference guidance [12; 38] to eliminate the generation capability of inappropriate content. Although their concepts are meaningful, currently, their methods are not practical. For one thing, the deleterious effects they are capable of mitigating are not comprehensive, because they can only eliminate harmful content that has clear definitions or is exemplified by enough images, and their methods lack scalability. For another, their methods inadvertently affect the quality of benign image generation [48; 16; 38]. Due to these drawbacks, current T2I online services [6; 3] and open-sourced models [33; 26] seldom consider this kind of method.

**Post-hoc Content Moderators** refer to content moderators applied on top of T2I systems. The moderation can be applied to _images_ or _prompts_. _Image-based moderators_, like safety checkers in SD [1; 30], operate on the syntheses to detect and censor NSFW elements. They suffer from significant inference costs because they take the output from T2I models as input. _Prompt-based moderators_ refer to prompt filters to prevent the generation of harmful content. Due to its lower cost and higher accuracy compared to image-based ones, currently, these technologies are extensively employed by online services, such as Midjourney [6] and Leonardo.Ai [3]. More examples in this category include OpenAI's Moderation API [23], Detoxify [13] and NSFW-Text-Classifier [21].

Note that most existing content moderators treat content moderation as a classification task, which necessitates extensive amounts of meticulously labeled data and operate in a black-box manner [19]. Therefore, they fail to adapt to unseen/customized NSFW concepts, as summarized in Tab. 1 and lack interpretability of the decision-making process, not to mention advanced adversarial prompt threats [45; 46; 38]. By contrast, in this paper, we take a generative perspective to build GuardT2I, which is more generalizable to various NSFW content and provides interpretation.

## 3 Method

Overview.As illustrated in Fig. 2 (a), T2I models rely on a text encoder, \(\tau(\cdot)\), to convert a user's prompt \(\mathbf{p}\) into a guidance embedding \(\mathbf{e}\), defined by \(\mathbf{e}=\tau(\mathbf{p})\in\mathbb{R}^{d}\). This embedding effectively dictates the semantic content of the image produced by the diffusion model [22]. We have observed that an adversarial prompt, denoted as \(\mathbf{p}_{\text{adv}}\), which may appear benign or nonsensical to humans, can contain the same underlying semantic information within the T2I model's latent space as an explicit prompt does, leading the diffusion model to generate NSFW content.

This observation has motivated us to introduce the concept of _Prompt Interpretation_ (see Fig. 2 (b)) in order to convert the implicit guidance embedding \(\mathbf{e}\) into plain text. By moderating the _Prompt

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multirow{2}{*}{
\begin{tabular}{c} **Open** \\ **Source** \\ \end{tabular} } & \multicolumn{3}{c}{**Property**} \\ \cline{3-5}  & & **Paatqin** & **Label** & **Inter-** & **Custom-** \\ \hline OpenAI & ✗ & Classifier & ✗ & ✗ \\ Microsoft & ✗ & Classifier & ✗ & ✗ \\ AWS & ✗ & Classifier & ✗ & ✗ \\ SafetyChecker & ✗ & Classifier & ✗ & ✗ \\ NSFW cls. & ✔ & Classifier & ✗ & ✗ \\ Detoxify & ✔ & Classifier & ✗ & ✗ \\ Perplexities & Classifier & ✔ & ✗ & ✗ \\
**GARDT2I** & ✔ & **Generator** & ✔ & ✔ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of our generative defensive approach with existing classification-based ones.

_Interpretation_, we can easily identify adversarial prompts (see Fig. 2 (c)). To be specific, when given a guidance embedding for a normal prompt, as depicted in Fig. 1 (c), the GuardT2I model accurately reconstructs the input prompt with slight variations. However, when encountering an adversarial prompt's guidance embedding, like the one shown in Fig. 2 (b), the generated prompt interpretation will differ significantly from the original input and may contain explicit NSFW words, _e.g._ "sex", and "fuck", which can be easily distinguished. Furthermore, the generated prompt interpretation enhances decision-making transparency, as illustrated in Fig. 2 (d).

Text Generation with cLLM.Translating the latent representation \(\mathbf{e}\) back to plain text presents a significant challenge due to the implicitness of latents. To resolve this issue, we approach it as a conditional generation problem and incorporate cross-attention modules to pre-trained LLMs, resulting in a conditional LLM (_c-LLM_) to fulfill this conditional generation task. To be specific, we employ a decoder-only architecture, comprising of \(L\) stacked transformer layers, as outlined in Fig. 3, and insert cross-attention layers in each transformer block. These cross-attention layers receive the guidance embedding \(\mathbf{e}\) as the query and utilize the scaled dot product attention mechanism to calculate the _attention score_[43], as follows:

\[\text{Attention}(\mathbf{Q}=\mathbf{e},\mathbf{K},\mathbf{V})=\text{softmax} \left(\frac{\mathbf{e}\mathbf{K}^{T}}{\sqrt{d}}\right)\cdot\mathbf{V} \tag{1}\]

Finally, the output from the final layer of the c-LLM is projected through a linear projection layer into the token space and translated to text.

To fine-tune c-LLM, we curate a sub-dataset sourced from the LAION-COCO dataset [40], as the training set, denoted as \(\mathcal{D}\). It is important to note that the source dataset \(\mathcal{D}\) should be unfiltered, meaning it naturally contains both Safe-For-Work (SFW) and NSFW prompts. This deliberate inclusion enables the resulting c-LLM, trained on this dataset, to acquire potentially generate NSFW prompts in natural language.3 We input the prompt \(\mathbf{p}\) from \(\mathcal{D}\) into the text encoder of T2I models, yielding the corresponding guidance embedding, expressed as \(\mathbf{e}=\tau(\mathbf{p})\in\mathbb{R}^{d}\)(see Fig. 3). The resulting dataset, comprising pairs of guidance embeddings and

Figure 3: Architecture of c-LLM. T2I’s text guidance embedding \(\mathbf{e}\) is fed to c-LLM through the multi-head cross attention layer’s query entry. \(\mathbf{L}\) indicates the total number of transformer blocks.

Figure 2: **The Workflow of GuardT2I against Adversarial Prompts.****(a)** GuardT2I halts the generation process of adversarial prompts. **(b)** Within GuardT2I, the c-LLM translates the latent guidance embedding \(\mathbf{e}\) into natural language, accurately reflecting the user’s intent. **(c)** A double-folded generation parse detects adversarial prompts. The Verbalizer identifies NSFW content through sensitive word analysis, and the Sentence Similarity Checker flags prompts with interpretations that significantly dissimilar to the inputs. **(d)** Documentation of prompt interpretations ensures transparency in decision-making transparency, as illustrated in Fig. 2 (d).

their corresponding prompts \((\mathbf{e},\mathbf{p})\), is named the _Mapped Guidance Embedding Dataset_, \(\mathcal{D}_{e}\), and serves in the training of cLLM.

For a given training sample \((\mathbf{e}_{i},\mathbf{p}_{i})\) from \(\mathcal{D}_{e}\), c-LLM is tasked with generating a sequence of interpreted prompt tokens \(\hat{\mathbf{y}}=(\hat{y}_{1},\hat{y}_{2},...,\hat{y}_{n})\) conditioned on the T2I's guidance embedding \(\mathbf{e}\). The challenges arise from potential information loss during the compression of \(\mathbf{e}\), and the discrepancy between the LLM's pre-training tasks and the current conditional generation task. These challenges may hinder the decoder's ability to accurately reconstruct the target prompt \(\mathbf{p}\) using only \(\mathbf{e}\), as illustrated in Fig. 3. To address this issue, we employ _teacher forcing_[44] training technique, wherein the c-LLM is fine-tuned with both \(\mathbf{e}\) and the ground truth prompt \(\mathbf{p}\). We parameterize the c-LLM by \(\theta\), and our optimization goal focuses on minimizing the cross-entropy (CE) loss at each prompt token position \(t\), conditioned upon the guidance embedding \(\mathbf{e}\). By denoting the token sequence of prompt \(\mathbf{p}\) as \(\mathbf{y}=(y_{1},y_{2},...,y_{n})\) the loss function can be depicted as:

\[\mathcal{L}_{CE}(\theta)=-\sum_{t=1}^{n}log(p_{\theta}(\hat{y_{t}}|y_{0},y_{1},...,y_{t-1};\mathbf{e})), \tag{2}\]

where \(y_{0}\) indicates the special \(<BOS>\) begin of sentence token. The underlying concept of the aforementioned objective Eq. (2) aims to tune c-LLM to minimize the discrepancy between the predicted token sequence \(\hat{\mathbf{y}}\) and the target token sequence \(\mathbf{y}\). Teacher forcing ensures that the model is exposed to the ground truth prompt \(\mathbf{p}\) at each step of the generation, thereby conditioning the model to predict the next token in the sequence more accurately [44; 9; 43]. The approach is grounded in the concept that a well-optimized model, through minimizing \(\mathcal{L}_{CE}(\theta)\), will produce an output probability distribution \(p_{\theta}(\cdot|y_{0},y_{1},...,y_{t-1};\mathbf{e})\in\mathbb{R}^{|V|}\), where \(|V|\) represents the size of the vocabulary codebook, which closely matches the one-hot encoded target token \(y_{t}\), thereby enhancing the fidelity and coherence of the generated prompt interpretations [44; 9; 43; 17].

A Double-folded Generation Parse Detects Adversarial Prompts.After revealing the true intent of input prompts with plain text, in this step, we introduce a bi-level parsing mechanism including _Verbalizer_ and _Sentence Similarity Checker_ to detect malicious prompts.

Firstly, _**Verbalizer**_, \(V(\cdot,\mathcal{S})\), as a simple and direct moderation method, is used to check either the _Prompt Interpretation_ contains any explicit words, _e.g._ "fuck", as illustrated in Fig. 2 (c). Here, \(\mathcal{S}\) denotes a developer-defined NSFW word list. Notably, \(\mathcal{S}\) is adaptable, allowing real-time updates to include emerging NSFW words, while maintaining the system's effectiveness against evolving threats.

In addition, we utilize the _**Sentence Similarity Checker**_ to examine the similarity in text space. For a benign prompt, its _Prompt Interpretation_ is expected to be identical to the itself, indicating high similarity during inference. In contrast, adversarial prompts reveal the obscured intent of the attacker, resulting in significant discrepancy with the original prompt. We measure this discrepancy using an established sentence similarity model [32], flagging low similarity ones as potentially malicious.

**Resistance to Adaptive Attacks.** GuardT2I demonstrates considerable robustness even under adaptive attacks. To deceive both T2I and GuardT2I simultaneously, the adversarial prompts must appear nonsensical yet retain similar semantic content in T2I's latent space, while also resembling their prompt interpretation to bypass GuardT2I. This requirement creates conflicting optimization directions: while adaptive attacks aim for prompts that differ visually from explicit ones, GuardT2I requires similarity in prompt interpretation and absence of explicit NSFW words. Consequently, increasing GuardT2I's bypass rate leads to a reduced NSFW generation rate by the T2I model, making it challenging for adaptive attackers to circumvent GuardT2I effectively.

Figure 4: Workflow of _Sentence Similarity Checker_. **(a)** Normal Prompt: In the case of a normal prompt, its prompt interpretation closely aligns with the original prompt, resulting in a SFW decision. **(b)** Adversarial Prompt: Conversely, for an adversarial prompt, its prompt interpretation significantly differs from the original prompt both, therefore be identified.

## 4 Experiments

### Experimental Settings

Training Dataset.LAION-COCO [40] represents a substantial dataset comprising 600M high-quality captions that are paired with publicly sourced web images. This dataset encompasses a diverse range of prompts, including both standard and NSFW content, mirroring real-world scenarios. We use a subset of LAION-COCO consisting of 10M randomly sampled prompts to fine-tune our c-LLM.

**Test Adversarial Prompt Datasets.** I2P [38] comprises 4.7k hand-crafted adversarial prompts. These prompts can guide T2Is towards NSFW syntheses, including self-harm, violence, shocking content, hate, harassment, sexual content, and illegal activities. We further extract 200 sexual-themed prompts from I2P to form the I2P-sexual adversarial prompt dataset. SneakyPrompt [46], Ring-A-Bell [42], P4D [11], and MMA-Diffusion [45] generate adversarial prompts automatically, we directly employ their released benchmark for evaluation.

**Target Model.** We employ Stable Diffusion v1.5 [7], a popular open-source T2I model, as the target model of our evaluation. This model has been selected due to its extensive adoption within the community and its foundational influence on subsequent commercial T2I models [3, 26, 25, 6, 4].

**Implementation.** Our GuardT2I comprises three components: Verbalizer, Sentence Similarity Checker, and c-LLM. Verbalizer operates based on predefined 25 NSFW words. We utilize the off-the-shelf _Sentence-transformer_[32], to function as the Sentence Similarity Checker. We implement c-LLM with 24 transformer blocks. Its initial weights are sourced from [34]. Please refer to Appendix for more detailed implementation. Note that GuardT2I as an LLM-based solution, also follows the scaling law [14], one can implement GuardT2I with other types of pre-trained LLMs and text similarity models, based on real scenarios.

**Baselines.** We employ both commercial moderation API models and popular open-source moderators as baselines. OpenAI Moderation [23, 19] classifies five type NSFW themes, including sexual content, hateful content, violence, self-harm, and harassment. If any of these categories are flagged, the prompt is rejected [19]. Microsoft Azure Content Moderator [5], as a classifier-based API moderator, focuses on sexually explicit and offensive NSFW themes. AWS Comprehend [2] treats NSFW prompt detection as a binary classification task. If the model classifies the prompt as toxic, it is rejected. NSFW-text-classifier [21] is an open-source binary NSFW classifier. Detoxity [13] is capable of detecting four types of inappropriate prompts, including pornography content, threats, insults, and identity-based hate.

\begin{table}
\begin{tabular}{l|l|c c c c c c c} \hline \hline \multirow{3}{*}{} & \multirow{3}{*}{**Method**} & \multicolumn{4}{c|}{**Adversarial Prompts**} \\ \cline{3-10}  & & \multicolumn{2}{c|}{**Snacky**} & \multicolumn{2}{c|}{**MiMA-Prompt [46]**} & \multicolumn{2}{c|}{**I2P-Sexual\({}^{\ast}\)[38]**} & \multicolumn{2}{c|}{**I2P-Sexual\({}^{\ast}\)[38]**} & \multicolumn{2}{c|}{**I2P-Sexual\({}^{\ast}\)[38]**} & \multicolumn{2}{c|}{**Ring-A-Bell [42]**} & \multicolumn{2}{c}{**PdD**[11]} & \multicolumn{2}{c}{**Avc. Strp. (\(\downarrow\))**} \\ \hline \multirow{7}{*}{\begin{tabular}{} \end{tabular} } & OpenAI-Moderation [23] & **98.50** & 73.02 & 97.93 & 84.60 & 99.35 & 95.68 & _91.51_ & _+11.59_ \\  & Microsoft Azure [5] & 81.89 & 90.66 & 55.04 & 54.25 & _99.42_ & 81.90 & 77.19 & +18.64 \\  & AWS Comprehend [2] & 97.09 & 97.33 & 69.67 & 70.50 & 98.76 & 91.51 & 87.48 & \(\pm\)13.70 \\  & NSFW-text-classifier [21] & 85.80 & 97.78 & 66.98 & 65.39 & 64.34 & 57.97 & 73.04 & \(\pm\)15.32 \\  & Cloudity [13] & 75.10 & 79.27 & 54.63 & 51.83 & 96.27 & 82.22 & 73.22 & \(\pm\)17.06 \\  & **GardT2I (Ours)** & 97.86 & **98.86** & **93.05** & **92.56** & **99.91** & **98.36** & **96.77** & **\(\pm\)3.15** \\ \hline \multirow{7}{*}{\begin{tabular}{} \end{tabular} } & OpenAI-Moderation [23] & **98.48** & 58.99 & **92.14** & _83.39_ & _98.21_ & _94.87_ & 87.68 & \(\pm\)15.10 \\  & Microsoft Azure [5] & 82.83 & 91.58 & 54.97 & 60.12 & 99.56 & 90.38 & 79.91 & \(\pm\)18.19 \\  & AWS Comprehend [2] & 97.24 & 97.30 & 77.47 & 73.25 & 98.80 & 91.73 & \(\pm\)93.90 & \(\pm\)11.14 \\  & NSFW-text-classifier [21] & 66.46 & 67.33 & 53.62 & 51.54 & 53.86 & 51.06 & 57.31 & \(\pm\)_7.51_ \\  & Dediotry [13] & 85.97 & _97.51_ & 67.02 & 64.44 & 95.52 & 80.98 & 81.91 & \(\pm\)13.95 \\  & **GardT2I (Ours)** & 98.28 & **98.95** & _89.64_ & **91.66** & _99.92_ & **88.51** & **96.16** & **\(\pm\)_4.35_ \\ \hline \multirow{7}{*}{\begin{tabular}{} \end{tabular} } & OpenAI-Moderation [23] & **4.40** & 40.20 & 35.50 & 99.09 & _0.70_ & **25.42** & _27.35_ & _22.27_ \\  & Microsoft Azure [5] & 61.53 & 57.60 & 77.50 & 98.32 & 1.05 & 80.00 & 62.67 & \(\pm\)33.51 \\  & AWS Comprehend [2] & 19.78 & **4.95** & 90.50 & 95.56 & 6.32 & 80.42 & 49.59 & \(\pm\)43.57 \\  & NSFW-text-classifier [21] & 84.61 & 48.10 & 92.50 & 94.45 & 68.42 & 87.92 & 79.33 & \(\pm\)_17.88_ \\  & D detoxify [13] & 51.64 & 13.70 & 76.00 & 79.20 & 15.09 & 90.83 & 54.41 & \(\pm\)33.52 \\  & **GardT2I (Ours)** & 6.50 & 6.59 & **25.50** & **34.96** & **0.35** & _41.67_ & **19.26** & \(\pm\)_17.14_ \\ \hline \multirow{7}{*}{
\begin{tabular}{} \end{tabular} } & ESD [12] & 28.57 & 66.7 & 36.25 & - & 98.60 & 79.16 & _6.86_ & \(\pm\)_9.31_ \\  & SLD-medium [38] & 58.24 & 85.00 & 39.10 & - & 98.95 & 80.51 & 72.36 & \(\pm\)_23.66_ \\ \cline{1-1}  & SLD-strong [38] & 41.76 & 80.82 & _30.12_ & - & _97.19_ & _73.75_ & 64.73 & \(\pm\)_27.93_ \\ \cline{1-1}  & **GardT2I (Ours)** & **9.89** & **10.20** & **26.4** & - & **3.16** & **8.75** & **11.68** & \(\pm\)_8.71_ \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison with baselines. **Bolded** values are the highest performance. The _underlined italicized_ values are the second highest performance. * indicates human-written adversarial prompts.

SLD [38] and ESD [11] are concept-erasing methods, which are designed to reduce the probability of NSFW generation. Therefore, we use the Attack Success Rate (ASR) as our evaluation metric. For GuardT2I, we set the threshold at FPR@5%, a common adaptation. As a concept-erasing method, ESD [11] only removes a single NSFW concept, "nudity", by fine-tuning the T2I model. This limitation means it fails to mitigate other NSFW themes such as violence, self-harm, and illegal content. Consequently, our evaluation focuses solely on "adult content". All implementations of the baseline models and the tested adversarial prompts are released by their original papers.

**Evaluation Metrics.** Rejecting adversarial prompts is a detection task, for which we employ standard metrics including AUROC, AUPRC, and FPR@TPR95. These metrics are used to evaluate GuardT2I and baseline models, in line with established practices in [27, 19]. Higher values of AUROC and AUPRC signify superior performance, whereas a lower FPR@TPR95 value is preferable. Due to space limitation, detailed explanations of these metrics are provided in Appendix.

### Main Results

Tab 2 presents a comprehensive evaluation of the proposed GuardT2I moderator in comparison with several baseline methods across multiple adversarial prompt datasets. The results demonstrate that GuardT2I consistently outperforms existing approaches in key performance metrics. Specifically, GuardT2I achieves the highest average AUROC of **98.36%** and the highest average AUPRC of **98.51%**, surpassing all baseline methods, including OpenAI-Moderation, Microsoft Azure, AWS Comprehend, NSFW-text-classifier, and Detoxify. Furthermore, GuardT2I exhibits superior effectiveness in minimizing false positives and attack success rates, attaining an average FPR@TPR95 of **19.26%** and an average ASR of **8.75%**, both of which are significantly lower than those of the compared baselines. The reduced standard deviations across these metrics (**\(\pm\)3.15** for AUROC, **\(\pm\)4.35** for AUPRC, and **\(\pm\)17.14** for FPR@TPR95) further indicate the robustness and consistency of GuardT2I's performance. These findings collectively highlight the superior capability of GuardT2I in effectively moderating adversarial prompts, ensuring both high detection accuracy and resilience against various attack strategies.

GuardT2I causes little impact on normal use cases.Tab. 2's FPR@TPR95 results corroborate GuardT2I is harmless to normal prompts, demonstrating a significantly lower FPR of 18.39%, which is 89.23% lower than the top-performing baseline average. This metric is critical in practical scenarios where high FPR can frustrate user experience. Moreover, we evaluate the performance of GuardT2I using the FID [24] and CLIP-Score [24] metrics to assess image quality and text alignment in Tab. 3. We compared our approach to the concept-erasing defense methods ESD [12] and SLD [38], which aim to reduce the probability of generating NSFW images. Additionally, we reported the average Attack Success Rate (ASR) to indicate the effectiveness of the defense methods.

Figure 5: ROC curves of our GuardT2I and baselines against various adversarial prompts. The black line represents the GuardT2I model’s consistent and high AUROC scores across different thresholds.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline \multirow{2}{*}{**Method**} & **Image Fidelity** & **Text Alignment** & **Defense Effectiveness** \\  & FID [24] (\(\downarrow\)) & CLIP-Score [24] (\(\uparrow\)) & ASR (Avg.)(\(\downarrow\)) \\ \hline ESDu1 [12] & **49.24** & _0.1501_ & _61.86_ \\ SLD-Medium [38] & 54.15 & 0.1476 & 72.36 \\ SLD-Strong [38] & 56.44 & 0.1455 & 64.73 \\
**GuardT2I(Ours)** & 52.10 & **0.1502** & **11.68** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Normal Use Case Results. **Bolded** values are the highest performance. The _underlined italicized_ values are the second highest performance.

[MISSING_PAGE_FAIL:8]

sarial prompts, the significance of prompt interpretations becomes even more pronounced. As illustrated in Tab. 4's lower section, GuardT2I interprets adversarial prompts' corresponding text guidance embedding into readable sentences. These sentences, which serve as prompt interpretations, can reveal the actual intention of the attacker. As analyzed in Fig. 7, the original adversarial prompts' prominent words seem safe for work, while after being parsed by our GuardT2I we can get their actual intentions. The ability to provide interpretability is a distinctive feature of GuardT2I, distinguishing it from classifier-based methods that typically lack such transparency. This capability not only differentiates GuardT2I but also adds significant value by shedding light on the decision-making process, offering developers of T2I a deeper understanding.

### Evaluation on Adaptive Attacks

Considering attackers have complete knowledge of both T2I and GuardT2I, we modify the most recent MMA-Diffusion adversarial attack [45], which provides a flexible gradient-based optimization flow to attack T2I models, by adding an additional term to attack GuardT2I, as depicted in Eq. (3), to perform adaptive attacks.

\[L_{adaptive}=(1-\alpha)\cdot L_{T2I}+\alpha\cdot L_{GuardT2I}, \tag{3}\]

where \(L_{T2I}\) is the original attack loss proposed by MMA-Diffusion, which steers T2I model towards generating NSFW content. Besides, \(L_{GuardT2I}\) is the loss function from GuardT2I's _Sentence Similarity Checker_, which can attack GuardT2I by optimizing with gradients, and \(\alpha\) is a hyperparameter to trade off two items.

The experiments are performed on a NVIDIA-A800-(80G) GPU with the default attack settings of MMA-Diffusion. We sample 100 NSFW prompts from MMA-Diffusion's dataset, and report the results with various \(\alpha\) in Tab. 5, where"GuardT2I bypass Rate" indicates the percentage of adaptive prompts that bypass GuardT2I. "T2I NSFW Content Rate" represents the percentage of bypassed prompts that result in the T2I generating NSFW content. Therefore, the "Adaptive Attack Success Rate" is calculated as "GuardT2I Bypass Rate" \(\times\) "T2I NSFW Content Rate". Following [45], a synthesis is considered NSFW, once it can trigger the NSFW detector nested in Stable Diffusion [7].

The results show that adaptive attacks on the entire system are challenging due to conflicting optimization directions. Specifically, \(L_{T2I}\) aims to find prompts that appear different and malicious semantic according to the embeddings of T2I. On the other hand, GuardT2I requires any bypassed prompts to stay close to their semantics according to the embeddings of T2I models. As a result, an increase in the "GuardT2I Bypass Rate" leads to a decrease in the "T2I NSFW Generation Rate", and vice versa. Therefore, even for adaptive attackers, evading GuardT2I becomes difficult, with an overall "Attack Success Rate" no higher than 16%. In a sanity check with doubled attack iterations (1000, \(\sim\)30 minutes per adv. prompt), the highest "Adaptive Attack Success Rate" observed is 24%. By contrast, that of Safety Checker is higher than 85.48% as reported by [45]. Moreover, qualitative results show that the successful adversarial prompts trend to degrade the synthesis quality, as illustrated in Fig. 8, weakening the threat posed by adaptive attacks. To strengthen GuardT2I's robustness, developers can set a more strict threshold. If some users are still concerned about moving to GuardT2I from the alternative moderators then they can use both in parallel.

### Ablation Study

Tab. 6 explores the roles of two key components in GuardT2I: _Verbalizer_ and _Sentence Similarity Checker_. _Verbalizer_ shows variable effectiveness across different adversarial prompts, indicating its limited capacity to handle complex cases independently. As a complementary, _Sentence Similarity Checker_ consistently achieves high AUROC scores above 91%, demonstrating its ability to discern subtle differences between prompts effectively. Combining both components results in the highest performance, highlighting a synergistic effect. The _Verbalizer_ analyzes the linguistic structure, while the _Sentence Similarity Checker_ assesses semantic coherence, together providing a comprehensive defense against adversarial prompts.

Figure 8: Syntheses generated by successful adaptive attack prompts. Adaptive adversarial prompts that can bypass GuardT2I tend to have much-weakened synthesis quality.

## 5 Discussion

**Failure Case Analysis.** We analyze two types of failure cases involving both false negatives and false positives. As shown in Fig. 9 (a), a false negative occurred when an adversarial prompt [38] led to the generation of unauthorized T2I content about Trump, mistakenly classified as normal. To prevent such errors, we can enrich Verbalizer by including specific keywords like "Donald Trump." In addition, we have observed that GuardT2I occasionally suffers from false alarms due to the rare appearance of certain terminologies. However, the rare terminology is either difficult for T2I model to depict, as demonstrated in Fig. 9 (b), making the false alarm less harmful.

**Computational Cost.** Tab. 7 compares the computational costs of GuardT2I and the image classifier-based post-hoc SafetyChecker [1]. GuardT2I operates in parallel with T2I, allowing for an immediate cessation of the generation process upon detection of harmful messages. As long as GuardT2I's inference speed is faster than the image generation speed of the T2I model, it does not introduce additional latency from the user's perspective. In contrast, SafetyChecker requires a full diffusion process of 50 iterations to classify NSFW content, making it significantly less efficient. Particularly in the presence of an adversarial prompt, GuardT2I responds approximately 300 times faster than SafetyChecker.

## 6 Conclusion

By adopting a generative approach, GuardT2I enhances the robustness of T2I models against adversarial prompts, mitigating the potential misuse for generating NSFW content. Our proposed GuardT2I offers the capability to track and measure the prompts of T2I models, ensuring compliance with safety standards. Furthermore, it provides fine-grained control that accommodates diverse adversarial prompt threats. Unlike traditional classification methods, GuardT2I leverages the c-LLM to transform text guidance embeddings within T2I models into natural language, enabling effective detection of adversarial prompts without compromising T2I models' inherent performance. Through extensive experiments, we have demonstrated that GuardT2I outperforms leading commercial solutions such as OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios. And show decent robustness against adaptive attacks. We firmly believe that our interpretable GuardT2I model can contribute to the development of safer T2I models, promoting responsible behavior in real-world scenarios.

## Acknowledgements

This work was supported in part by General Research Fund of Hong Kong Research Grants Council (RGC) under Grant No. 1420352, the Research Matching Grant Scheme under Grant (No. 7106937, 8601130, and 8601440), and the NSFC Projects (No. 92370124, and 62076147).

\begin{table}
\begin{tabular}{c|c c c c c} \hline \multirow{2}{*}{**Adv. Prompt**} & \multicolumn{4}{c}{**Generation Parsing (\(\uparrow\))**} & \multicolumn{1}{c}{\multirow{2}{*}{**Surface Sentence-Sim. Ours**}} \\ \cline{2-2} \cline{4-6}  & \multicolumn{2}{c}{**Verbalizer Sentence-Sim. Ours**} \\ \hline SneakyPrompt [46] & 53.30 & 97.39 & **97.86** & \\ MMA-Diffusion [45] & 80.20 & 97.17 & **98.86** & \\ I2P-Sexual [38] & 53.25 & 91.42 & **93.05** & \\ I2P [38] & 51.85 & 92.41 & **92.56** & \\
**Avg.** & 59.65 & 94.60 & **95.58** & \\ \hline \end{tabular}
\end{table}
Table 6: Ablation Study on Verbalizer and Sentence Similarity Checker.

\begin{table}
\begin{tabular}{c c c c c c} \hline \multirow{2}{*}{**Adv. Prompt**} & \multicolumn{4}{c}{**Generation Parsing (\(\uparrow\))**} & \multicolumn{1}{c}{\multirow{2}{*}{**Surface Sentence-Sim. Ours**}} \\ \cline{2-2} \cline{4-6}  & \multicolumn{2}{c}{**Verbalizer Sentence-Sim. Ours**} \\ \hline SneakyPrompt [46] & 53.30 & 97.39 & **97.86** & \\ MMA-Diffusion [45] & 80.20 & 97.17 & **98.86** & \\ I2P-Sexual [38] & 53.25 & 91.42 & **93.05** & \\ I2P [38] & 51.85 & 92.41 & **92.56** & \\
**Avg.** & 59.65 & 94.60 & **95.58** & \\ \hline \end{tabular}
\end{table}
Table 7: Comparison of Model Parameters and Inference Times on NVIDIA-A800

Figure 9: Failure cases of GuardT2I. (a) Fake news of the famous individual. (b) GuardT2I alarms rarely used terminology.

## References

* [1] Safety Checker nested in Stable Diffusion. [https://huggingface.co/CompVis/stable-diffusion-safety-checker](https://huggingface.co/CompVis/stable-diffusion-safety-checker), 2023.
* [2] AWS Comprehend. [https://docs.aws.amazon.com/comprehend/latest/dg/what-is.html](https://docs.aws.amazon.com/comprehend/latest/dg/what-is.html), 2024.
* [3] Leonardo.Ai. [https://leonardo.ai/](https://leonardo.ai/), 2024.
* [4] Lexica. [https://lexica.art/](https://lexica.art/), 2024.
* [5] Microsoft Azure Content Moderator. [https://learn.microsoft.com/zh-cn/azure/ai-services/content-moderator/api-reference](https://learn.microsoft.com/zh-cn/azure/ai-services/content-moderator/api-reference), 2024.
* [6] Midjourney. [https://midjourney.com/](https://midjourney.com/), 2024.
* [7] Stable Diffusion V1.5 checkpoint. [https://huggingface.co/runwayml/stable-diffusion-v1-57text=chi+venezuela+drogenius](https://huggingface.co/runwayml/stable-diffusion-v1-57text=chi+venezuela+drogenius), 2024.
* [8] Zhongjie Ba, Jieming Zhong, Jiachen Lei, Peng Cheng, Qinglong Wang, Zhan Qin, Zhibo Wang, and Kui Ren. SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via Substitution. _arXiv preprint arXiv:2309.14122_, 2023.
* [9] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. In _Proceedings of the International Conference on Learning Representations_, 2015.
* [10] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions. [https://cdn.openai.com/papers/dall-e-3.pdf](https://cdn.openai.com/papers/dall-e-3.pdf), 2023.
* [11] Zhi-Yi Chin, Chieh-Ming Jiang, Ching-Chun Huang, Pin-Yu Chen, and Wei-Chen Chiu. Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts. _arXiv preprint arXiv:2309.06135_, 2023.
* [12] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing Concepts from Diffusion Models. _arXiv preprint arXiv:2303.07345_, 2023.
* [13] Laura Hanu and Unitary team. Detoxify. [https://github.com/unitaryai/detoxify](https://github.com/unitaryai/detoxify), 2020.
* [14] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.
* [15] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating Concepts in Text-to-Image Diffusion Models. _arXiv preprint arXiv:2303.13516_, 2023.
* [16] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Benita Teufel, Marco Bellagente, Minguk Kang, Taesung Park, Jure Leskovec, Jun-Yan Zhu, Li Fei-Fei, Jiajun Wu, Stefano Ermon, and Percy Liang. Holistic Evaluation of Text-To-Image Models. _arXiv preprint arXiv:2311.04287_, 2023.
* [17] Haoran Li and Wei Lu. Mixed Cross Entropy Loss for Neural Machine Translation. In _Proceedings of the International Conference on Machine Learning_, pages 6425-6436, 2021.
* [18] Han Liu, Yuhao Wu, Shixuan Zhai, Bo Yuan, and Ning Zhang. Riatig: Reliable and imperceptible adversarial text-to-image generation with natural prompts. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 20585-20594, June 2023.
* [19] Todor Markov, Chong Zhang, Sandhini Agarwal, Florentine Eleundou Nekoul, Theodore Lee, Steven Adler, Angela Jiang, and Lilian Weng. A holistic approach to undesired content detection in the real world. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 15009-15018, 2023.
* [20] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_, 2021.

* [21] Michellejieli. NSFW text classifier. [https://huggingface.co/michellejieli/NSFW_text_classifier](https://huggingface.co/michellejieli/NSFW_text_classifier), 2023.
* [22] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. In _Proceedings of the International Conference on Machine Learning_, pages 16784-16804, 2022.
* [23] OpenAI. Moderation overview. [https://platform.openai.com/docs/guides/moderation/overview](https://platform.openai.com/docs/guides/moderation/overview), 2023.
* [24] I. Pavlov, A. Ivanov, and S. Stafievskiy. Text-to-Image Benchmark: A benchmark for generative models. [https://github.com/boomb0om/text2image-benchmark](https://github.com/boomb0om/text2image-benchmark), September 2023. Version 0.1.0.
* [25] PlaygroundAI. Playground. [https://playgroundai.com/](https://playgroundai.com/), 2023.
* [26] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. _arXiv preprint arXiv:2307.01952_, 2023.
* [27] Yiting Qu, Xinyue Shen, Xinlei He, Michael Backes, Savvas Zannettou, and Yang Zhang. Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models. _arXiv preprint arXiv:2305.13873_, 2023.
* [28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [29] Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tramer. Red-Teaming the Stable Diffusion Safety Filter. _arXiv preprint arXiv:2210.04610_, 2022.
* [30] Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tramer. Red-Teaming the Stable Diffusion Safety Filter. _arXiv preprint arXiv:2210.04610_, 2022.
* [31] Sashank Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In _International Conference on Learning Representations_, 2018.
* [32] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In _Proceedings of the Conference on Empirical Methods in Natural Language Processing_, 2019.
* [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10674-10685, 2022.
* [34] Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. Leveraging pre-trained checkpoints for sequence generation tasks. _Transactions of the Association for Computational Linguistics_, 8:264-280, 2020.
* [35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.
* [36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.
* [37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. In _Proceedings of the Advances in Neural Information Processing Systems_, 2022.
* [38] Patrick Schramowski, Manuel Brack, Bjorn Deiseroth, and Kristian Kersting. Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22522-22531, 2023.
* [39] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An Open Large-scale Dataset for Training Next Generation Image-text Models. In _Proceedings of the Advances in Neural Information Processing Systems_, 2022.

* [40] Christoph Schuhmann, Andreas Kopf, Theo Coombes, Richard Vencu, Benjamin Trom, and Romain Beaumont. LAION-COCO. [https://laion.ai/blog/laion-coco/](https://laion.ai/blog/laion-coco/), 2022.
* [41] Yu-Lin Tsai, Chia-Yi Hsu, Chulin Xie, Chih-Hsun Lin, Jia-You Chen, Bo Li, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang. Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models? _arXiv preprint arXiv:2310.10012_, 2023.
* [42] Yu-Lin Tsai, Chia-Yi Hsu, Chulin Xie, Chih-Hsun Lin, Jia You Chen, Bo Li, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang. Ring-a-bell! how reliable are concept removal methods for diffusion models? In _The Twelfth International Conference on Learning Representations_, 2023.
* [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In _Proceedings of the Advances in Neural Information Processing Systems_, pages 5998-6008, 2017.
* [44] Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. _Neural computation_, 1989.
* [45] Yijun Yang, Ruiyuan Gao, Xiaosen Wang, Tsung-Yi Ho, Nan Xu, and Qiang Xu. MMA-Diffusion: MultiModal Attack on Diffusion Models. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2024.
* [46] Yuchen Yang, Bo Hui, Haolin Yuan, Neil Gong, and Yinzhi Cao. Sneakyprompt: Jailbreaking text-to-image generative models. In _Proceedings of the IEEE Symposium on Security and Privacy_, 2024.
* [47] Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, and Sijia Liu. To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now. _arXiv preprint arXiv:2310.11868_, 2023.
* [48] Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, and Sijia Liu. To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now. _arXiv preprint arXiv:2310.11868_, 2023.

## Appendix

This supplementary material provides additional details and results that are not included in the main paper due to page limitations. The following items are included in this supplementary material.

## Appendix A Preliminaries of Diffusion-based Text-to-Image Model

Text-guided Stable Diffusion Models.Stable Diffusion (SD) models [33], a subclass of diffusion models, streamline text-guided diffusion and denoising processes in the latent space, thereby boosting efficiency.

During training, the initial image \(x_{0}\) and prompt \(\mathbf{p}\) are encoded into latent spaces using \(\mathcal{E}(\cdot)\) and \(\tau(\cdot)\) respectively, resulting in \(z_{0}=\mathcal{E}(x_{0})\) and guidance embedding, \(\mathbf{e}=\tau(\mathbf{p})\). Noise is incrementally introduced across \(T\) diffusion steps, generating a series of samples \(z_{1},...,z_{T}\) through \(z_{t+1}=a_{t}z_{t}+b_{t}\epsilon_{t}\), where \(\epsilon_{t}\) follows a Gaussian distribution. Ideally, with a large \(T\), the final \(z_{T}\) approximates \(\mathcal{N}(0,1)\).

This property allows us to generate latent vectors for images by starting with Gaussian noise \(z_{T}\sim\mathcal{N}(0,1)\) and gradually reducing noise. To achieve this, we train a neural network, \(\epsilon_{\theta}\), implemented as an Unet in SD, which predicts \(z_{t+1}\) based on the input \(z_{t}\). For prompt guidance, the prompt embedding \(\mathbf{e}\) is injected as an condition to run conditional diffusion steps, \(\epsilon_{\theta}(z_{t}|\tau(\mathbf{p}))\). Additionally, by replacing the prompt with a null prompt \(\varnothing\) with a fixed probability, the model can generate images unconditionally. The denoising diffusion model is trained by minimizing the following loss function:

\[L(\theta)=\mathbb{E}_{t,z_{0}=\mathcal{E}(x_{0}),\epsilon\sim\mathcal{N}(0,1)} [|\epsilon-\epsilon_{\theta}(z_{t+1},t|\tau(\mathbf{p})|_{2}^{2}], \tag{4}\]

During the inference phase, the latent noise is extrapolated in two directions: towards \(\epsilon(z_{t}|\tau(p))\) and away from \(\epsilon(z_{t}|\varnothing)\). This process is carried out as follows:

\[\hat{\epsilon}_{\theta}(z_{t}|\tau(\mathbf{p}))=\epsilon_{\theta}(z_{t}|\tau( \varnothing))+g\cdot(\epsilon_{\theta}(z_{t}|\tau(\mathbf{p}))-\epsilon_{ \theta}(z_{t}|\tau(\varnothing))), \tag{5}\]

where \(g\) indicates guidance scale, typically \(g>1\). Subsequently, the image decoder, \(\mathcal{D}(\cdot)\), will decode the latent image embedding to an image.

## Appendix B Inference Workflow of GuardT2I

```
0: T2I's prompt embedding \(\mathbf{e}\) from original prompt \(\mathbf{p}\), c-LLM \((\cdot)\); Verbalizer \(V(\cdot,\mathcal{S})\) with NSFW word list \(\mathcal{S}\); Text similarity checker \(Sim(\cdot,\cdot)\) and threshold \(s\)
0: Early stop diffusion process / Accept the input prompt
1:\(\mathbf{p}_{t}=\text{c-LLM}\)\((\mathbf{e})\)
2:if\(V(\mathbf{p}_{I},\mathcal{S})\)then
3: Early Stop: NSFW Prompt Detected
4:elseif\(Sim(\mathbf{p},\mathbf{p}_{I})<s\)then
5: Early Stop: Adv. Prompt Detected
6:else
7: Accept: Normal Prompt
8:endif
```

**Algorithm 1** Inference Workflow of GuardT2I

## Appendix C Evaluation Metric

**AUROC**: The AUROC metric measures the ability of our model to discriminate between adversarial and normal prompts. It quantifies the trade-off between the TPR and the FPR, providing an overall assessment of the model's performance across different thresholds.

**AUPRC**: The AUPRC metric focuses on the precision-recall trade-off, providing a more detailed evaluation.

**FPR@TPR95%**: FPR@TPR95% quantifies the proportion of false positives (incorrectly identified as adversarial examples) when the model correctly identifies 95% of the true positives (actual adversarial prompts). A lower FPR@TPR95 value is desirable, as it indicates that the model can maintain high accuracy in detecting adversarial examples with fewer mistakes. This metric is particularly important in commercial scenarios where frequent false alarms are unacceptable. Note that FPR@TPR95 provides a specific slice of the ROC curve at a high-recall threshold. Developers have the flexibility to adjust the threshold to achieve desired performance based on specific application scenarios.

## Appendix D Implementation Details

### Settings of the target Stable Diffusion model.

For the target SDv1.5 model, we set the guidance scale to 7.5, the number of inference steps to 50, and the image size to \(512\times 512\), 4 syntheses per prompt, throughout evaluations.

### Hardware platform.

We conduct our training and main experiments on the NVIDIA RTX4090 GPU with 24GB of memory. For adaptive attack and computational cost evaluation, we conduct experiments on the NVIDIA A800 GPU with 80 GB of memory.

### Implementation details of GuardT2I.

Our GuardT2I comprises three primary components: Verbalizer, Sentence Similarity Checker and c-LLM. The Verbalizer operates based on predefined 25 NSFW words; details of which can be found in the Tab. A-1. We utilize an off-the-shelf sentence-transformer checkpoint [32], to function as the Sentence Similarity Checker. The architecture diagram of c-LLM is introduced in Fig. 3, which is a transformer-based model consisting of 24 standard transformer blocks [43], each with 1024 hidden dimensions. This model is initialized from a publicly available checkpoint [34], pre-trained on an extensive text corpus [39]. We fine-tune c-LLM using the Adam optimizer [31] with a learning rate of \(1\times 10^{-5}\), and a batch size of 1024 for 50 epochs, using around 768 GPU hours on NVIDIA4090. Note that, GuardT2I as a LLM-based solution, also follows the scaling law [14], one can implement GuardT2I with other type of pretrained LLMs and more powerful text similarity models, according to the real-world hardware resource budgets.

### NSFW Word List of Verbalizer.

Table A-1 provides a comprehensive compilation of NSFW sensitive words utilized in the Verbalizer component of our GuardT2I framework. For our specific use case, we primarily focus on pornographic-related words. It is crucial to emphasize that this list is not fixed and developers can dynamically adjust it to suit specific application scenarios in real-time. This customizable feature ensures the Verbalizer remains adaptable and responsive to the distinct requirements and sensitivities of diverse contexts. By leveraging this flexibility, developers can effectively tailor the Verbalizer to guard T2I models.

## Appendix E Additional Failure Case Analysis

We have expanded on the failure case visualizations discussed in Section 5 and have provided an enlarged version in Figure A-1. This figure offers a more detailed illustration of the specific instances where our system encountered challenges and produced undesired outputs. By thoroughly examining these failure cases, we gain valuable insights into the areas that require improvement and refinement in our approach.

To address these challenges, we propose two promising solutions. Firstly, enriching the Verbalizer with specific keywords, such as the example of incorporating the term "Trump and Thanos" can enhance the system's ability to handle these failures. Secondly, employing an active learning technique can further improve GuardT2I's performance by iteratively learning from and adapting to the feedback received from these failure cases.

\begin{table}
\begin{tabular}{c} \hline \hline
**NSFW Word List** \\ \hline sexual \\ nude \\ nudity \\ sex \\ 18+ \\ naked \\ nsfw \\ porn \\ dick \\ vagina \\ naked child \\ explicit content \\ unsensored \\ fuck \\ nipples \\ visible nipples \\ breast \\ areola \\ pussy penis \\ vagina \\ suck \\ cock \\ blowjock \\ blow \\ \hline \hline \end{tabular}
\end{table}
Table A-1: NSFW word listFigure A-1: **Additional failure case analysis.** Upper section: The adversarial prompt [38] generates _shocking_ content (fake news about Trump/Thanos) but is mistakenly flagged as a normal prompt. Lower section: GuardT2I occasionally produces false alarms due to the reconstruction of rarely used terminology (see **bolded** words), resulting in false positives.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction highly align with the proposed method and the contributions claimed in the introduction section is supported by experimental results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please refer to Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Statements related to theoretical materials are well-cited and introduced.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Model architecture is introduced in Section 3, and the training and implementation details can be found in Section 4.1, Appendix D and Appendix D.4. Codes and model checkpoints will be publicly released upon acceptance. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: Codes and model checkpoints will be publicly released upon acceptance. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Training and implementation details can be found in Section 4.1, Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Our GuardT2I significantly outperforms the baselines, surpassing the level of influence caused by randomness. Due to limited computational resources, we only fixed the random seed during evaluation. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We use NVIDIA 4090 for training and main evaluations (Appendix D). For adaptive attacks, since it requires more VRAM, around 65 GB, we conduct the experiments on NVIDIA A800 (80G) see Section 4.3. For the time of execution, we report it in Tab. 7. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We keep anonymization in submission. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: As a defensive framework, our GuardT2I contributes to the safe use of T2I models, therefore having positive societal impacts as introduced in 1 instead of negative ones. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We perform adaptive attacks on our GuardT2I to show the worst case, as described in Sec. 4.3. The results indicate that GuardT2I exhibits decent robustness even under rigorous adaptive attacks. Furthermore, adaptive adversarial prompts that can bypass GuardT2I tend to have much weakened synthesis quality, as illustrated in Fig. 8. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The employed datasets and models are all available for research purposes and are well cited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The paper alone is self-contained. Moreover, codes and models will be released upon acceptance. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: All showcase images are model-generated. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: All showcase images are model-generated. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.