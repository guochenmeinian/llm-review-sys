# Introducing Spectral Attention for

Long-Range Dependency in Time Series Forecasting

Bong Gyun Kang\({}^{1}\)

Dongjun Lee\({}^{1}\)

HyunGi Kim\({}^{2}\)

DoHyun Chung\({}^{3}\)

Sungroh Yoon\({}^{1,2}\)

\({}^{1}\) Interdisciplinary Program in Artificial Intelligence, Seoul National University

\({}^{2}\) Department of Electrical and Computer Engineering, Seoul National University

\({}^{3}\) Department of Future Automotive Mobility, Seoul National University

Denotes equal contribution. {luckypanda, elite1717}@snu.ac.krCorresponding author. sryoon@snu.ac.kr

###### Abstract

Sequence modeling faces challenges in capturing long-range dependencies across diverse tasks. Recent linear and transformer-based forecasters have shown superior performance in time series forecasting. However, they are constrained by their inherent inability to effectively address long-range dependencies in time series data, primarily due to using fixed-size inputs for prediction. Furthermore, they typically sacrifice essential temporal correlation among consecutive training samples by shuffling them into mini-batches. To overcome these limitations, we introduce a fast and effective Spectral Attention mechanism, which preserves temporal correlations among samples and facilitates the handling of long-range information while maintaining the base model structure. Spectral Attention preserves long-period trends through a low-pass filter and facilitates gradient to flow between samples. Spectral Attention can be seamlessly integrated into most sequence models, allowing models with fixed-sized look-back windows to capture long-range dependencies over thousands of steps. Through extensive experiments on 11 real-world time series datasets using 7 recent forecasting models, we consistently demonstrate the efficacy of our Spectral Attention mechanism, achieving state-of-the-art results.

## 1 Introduction

Time series forecasting (TSF) stands as a core task in machine learning, ubiquitous in our lives through applications such as weather forecasting, traffic flow estimation, and financial investment. Over time, TSF techniques have evolved from statistical [5, 10, 18] and machine learning approaches [2, 9, 20] to deep learning models like Recurrent Neural Networks [15, 25, 41] and Convolution based Networks [12, 45]. Following the success of Transformers [44] in various domains, Transformer-based models have also become mainstream in the time series domain [23, 27, 36, 49, 54, 55, 56]. Recently, methodologies based on Multi-layer Perceptron have received renewed attention [7, 8, 28, 52]. However, despite the advancements, long-term dependency modeling in TSF remains challenging [53].

Unlike image models, where data are randomly sampled from the image distribution and are thus independent of each other [16], TSF models sample data from the continuous signal, dependenton the time variable \(t\) as shown in Figure 1a. This leads to a high level of correlation between each training sample, which consists of a fixed-sized look-back window before \(t\) (as input) and the subsequent prediction sequence after \(t\) (as label). Therefore, the conventional approach of shuffling the consecutive samples into mini-batches deprives the model of utilizing the crucial inherent temporal correlation between the samples. This restricts the model's consideration to only a fixed-size look-back window for sequence modeling, limiting the ability to address long-range dependencies (Figure 1b).

Recent studies pointed out that simply increasing the look-back window leads to substantially detrimental effects such as increased model size and longer training and inference times [53]. This is particularly challenging for transformer forecasters, which exhibit quadratic time/memory complexity [36, 44, 54], and even for efficient models using techniques like Sparse Attention, which have O(nlogn) complexity [23, 27, 55]. Furthermore, if the commonly used look-back window of 96 is extended fivefold, the model can only utilize time steps of less than 500, making it difficult to consider long-range dependencies spanning thousands or the entire dataset. Also, increasing the look-back window may not be beneficial, often leading to decreased performance [53], highlighting the fact that current models are not sufficient in capturing long-range dependencies.

To address this limitation, we propose Spectral Attention, which can be applied to most TSF models and enables the model to utilize long-range temporal correlations in sequentially obtained training data. With the stream of consecutive training samples (Figure 1c), Spectral Attention stores an exponential moving average (EMA) of the activations with various smoothing factors at each time step. This serves as a low-pass filter, inherently embedding long-range information over a thousand steps. Attention is then applied to the stored EMA activations of various smoothing factors (low-pass filters with different frequencies), enabling the model to learn which periodic trends to consider when predicting the future, thereby enhancing its performance. Spectral Attention is even applicable to models such as iTransformer [31], which do not preserve the temporal order of time series data internally.

We further extend Spectral Attention, where computations depend on the activation of the previous timesteps, to Batched Spectral Attention, enabling parallel training across multiple timesteps. This extension makes Spectral Attention faster and more practical and allows for the direct utilization of temporal relationships among consecutive data samples within a mini-batch in the training base model. In Batched Spectral Attention, the EMA is unfolded over time to perform Spectral Attention simultaneously across multiple time steps. This unfolding allows gradients at time \(t\) to propagate through the Spectral Attention module to the previous time step within the mini-batch, achieving effects similar to Backpropagation Through Time (BPTT) [47] and extends the model's effective input window.

**Our approach** preserves the base TSF model architecture and learning objective while enabling the model to leverage long-term trends spanning thousands of steps. By effectively utilizing the temporal correlation of training samples, our method allows gradients to flow back in time beyond the look-back window, extending to the entire mini-batch. Also, our method requires little additional training time and memory. We conducted experiments on 7 recent TSF models and 11 real-world datasets and demonstrated consistent performance enhancement in all architecture, achieving state-of-the-art results. We summarize the main contributions as follows:

Figure 1: (a) Training data are sampled for each time step from continuous sequences, exhibiting high temporal correlations. (b) Conventional approaches simply ignore this temporal information with a shuffled batch. (c) We address the temporal correlation between the samples for the first time, enabling the model to consider long-range dependencies that surpass the look-back window.

* We propose Spectral Attention, which addresses long-range dependencies spanning thousands of steps through frequency filtering and attention mechanisms, leveraging the temporal correlation among the consecutive samples.
* We propose Batched Spectral Attention, which enables parallel training across multiple timesteps and expends the effective input window, allowing the gradient to flow through time within the mini-batch.
* Batched Spectral Attention is applicable to most existing TSF models and practical in real-world scenarios with minimal additional memory and comparable training time. Also, it allows finetuning with a trained TSF model.
* Batched Spectral Attention demonstrates consistent model-agnostic performance improvements, particularly showcasing superior performance on datasets with significant long-term trend variations.

## 2 Related Works

**Classic TSF models.** Statistical TSF methods, such as ARIMA [35], Holt-Winters [18], and Gaussian Process [10], assume that temporal variations adhere to predefined patterns. However, their practical applicability is largely limited by the complex nature of real-world data. Machine learning approaches, such as Support Vector Machines [4] and Random Forests [14] have proven to be effective even compared to early artificial neural networks [13; 17; 40]. Convolutional network-based methods leverage convolution kernels to capture temporal variations sliding along the temporal dimension [12; 45]. Recurrent neural network (RNN) grasp changes over time via state transitions across different time steps. [25; 41]. However, RNN-based models exhibit limitations in modeling long-range dependencies due to challenges such as vanishing gradients and error accumulation [30; 43]. Recently, transformer and linear-based models have emerged as alternatives, demonstrating superior performance compared to recurrent models [46; 53].

**Transformer and Linear based models.** Transformer-based models [44] address temporal relationships between time points using the attention. LogSparseTransformer [27], Reformer [23], and Informer [55] have been proposed to make the Transformer architecture efficient, addressing the quadratic time complexity. The Autoformer [49] incorporates series decomposition as an inner block of Transformer and aggregates similar sub-series by utilizing the Auto-Correlation mechanism. PatchTST [36] introduces patching, a channel-independent approach that processes each variable separately and focuses on cross-time attention. Crossformer [54] utilizes a channel-dependent approach to learn cross-variate dependencies. This is achieved through the use of cross-time and cross-dimension attention. iTransformer [31] applies attention and FFN in an inverted way, where attention handles correlations between channels and FFN handles the temporal information. Recently, to address Transformers' potential difficulties in capturing long-range dependencies [53], methodologies based on the linear model and Multi-Layer Perceptron (MLP) structures have emerged. DLinear [53] utilizes the decomposition method introduced in Autoformer and predicts by adding the output of two linear layers for each seasonal and trend element. TiDE [7] proposes an architecture based on MLP residual blocks that combines information from dynamic and static covariates with a look-back window for encoding, followed by decoding. TSMixer [8] performs forecasting by repeatedly mixing time and features using an MLP. RLinear [28] comprises of a single linear layer with RevIN [21] for normalization and de-normalization.

**Frequency-utilizing models.** Using the frequency domain for TSF is a well-established approach [3; 42; 19]. Conventional approaches leverage frequency information during the preprocessing stage [37] or decompose time series based on frequency filtering [39]. In deep TSF models, research has also explored architectural advancements that are aware of the frequency information. SAAM [34], which is applicable to RNNs, performs FFT and autocorrelation on the input signal. WaveFroM [51] uses discrete wavelet transform (DWT) to project time series into wavelet domains of various scales and performs forecasting through graph convolution and dilated convolution. FEDformer [56] adopts a mixture-of-experts strategy to refine the decomposition of seasonal and trend components and introduce sparse attention mechanisms in the frequency domain. TimesNet [48] transforms 1D time series into 2D tensors utilizing multi-periodicity by identifying dominant frequencies through Fourier Transform, modeling temporal variations effectively. FreTS [52] leverages frequency-domain MLPs to achieve global signal analysis and compact energy representation, addressing the limitations of point-wise mappings and information bottlenecks in conventional MLP-based methods. FITS [50] employs interpolation within the complex frequency domain to construct a concise and robust model. While these models leverage frequency information, they are limited in modeling long-range dependencies, as the frequency conversion is confined to the look-back window. On the other hand, our Spectral Attention is the first to achieve long-range dependency modeling beyond the look-back window by incorporating consecutive data streams during model training.

## 3 Methods

Problem Statement.In multivariate time series forecasting, time series data is given \(\mathbb{D}_{T}:\{x_{1},...,x_{T}\}\in\mathbb{R}^{T\times N}\) at time \(T\) with \(N\) variates. Our goal is, at arbitrary future time \(t>T\), to predict future \(S\) time steps \(Y_{t}=\{x_{t+1},...,x_{t+S}\}\in\mathbb{R}^{S\times N}\). To achieve this goal, TSF model \(f\) utilizes length \(L\) look-back window as input \(X_{t}=\{x_{t-L+1},...,x_{t}\}\in\mathbb{R}^{L\times N}\) making prediction \(P_{t}=f(X_{t})\), \(P\in\mathbb{R}^{S\times N}\).

Model is trained with the training dataset \(D_{T}=\{(X_{t},Y_{t})|L\leq t\leq T-S\}\). While conventional methods typically randomly sample each \(X,Y\) from \(D_{T}\) to constitute the mini-batch, we utilize sequential sampling to incorporate temporal correlations between samples into the learning process.

### Spectral Attention

Spectral Attention (\(SA\)) can be applied to every TSF model that satisfies the aforementioned problem statement. This base TSF model is represented by \(P=f(X)\), and \(SA\) can be applied to arbitrary activation \(F\) within the model. The base model can be reformulated as \(P=f_{2}(F,E)\) and \(F,E=f_{1}(X)\). \(F,E\) are intermediate state and SA module takes an arbitrary subset \(F\) as input and transforms it into \(F^{\prime}\) of the same size; \(F^{\prime}=SA(F)\), \(P^{\prime}=f_{2}(F^{\prime},E)\). The resulting SA plugged model \(f_{SA}\) is depicted in Figure 2a.

With \(X_{t}\) as the base model input, \(SA\) takes \(D\)-dimensional feature vector \(F_{t}\in\mathbb{R}^{D}\) as input. \(SA\) updates the exponential moving average (EMA) \(M_{t}\in\mathbb{R}^{K\times D}\) of \(F_{t}\) in its internal memory with the \(K\)_smoothing factors_\(\{\alpha_{1},...,\alpha_{K}\}\in\mathbb{R}^{K}\) (\(\alpha_{1}<\cdots<\alpha_{K}\)) as shown in Figure 2b.

\[M_{t+1}^{k,i}=\alpha_{k}\times M_{t}^{k,i}+(1-\alpha_{k})\times F_{t}^{i} \tag{1}\]

EMA retains the trend of features over long-range time periods based on the smoothing factor. It operates as a low-pass filter, with the -3db (half) cut-off frequency of \(freq_{cut}=\frac{1}{2\pi}cos^{-1}\left[1-\frac{(1-\alpha)^{2}}{2\alpha}\right]\), effectively preserving the trend over 6,000 period with \(\alpha=0.999\).

To represent high-frequency patterns contrasting with the low-pass filtered long-range pattern \(M_{t}\), we generated \(H_{t}\in\mathbb{R}^{K\times D}\) by subtracting \(M_{t}\) from \(F_{t}\).

\[H_{t}^{k,i}=F_{t}^{i}-M_{t}^{K-k-1,i} \tag{2}\]

Figure 2: (a) Plug-in Spectral Attention (SA) module takes a subset of intermediate feature \(F\) and returns \(F^{\prime}\) with long-range information beyond the look-back window. The model is trained end-to-end, and gradients flow through the SA module. (b) To capture the long-range dependency, SA stores momentums of feature \(F\) generated from the sequential inputs. Multiple momentum parameters \(\alpha_{i}\) capture dependencies across various ranges. (c) SA module computes \(F^{\prime}\) by attending multiple low-frequency (\(M^{\alpha_{i}}\)) and high-frequency (\(F-M^{\alpha_{i}}\)) components and feature (\(F\)) using learnable Spectral Attention Matrix (SA-Matrix)

\(SA\) contains learnable parameters: _sa-matrix_\(\in\mathbb{R}^{(2K+1)\times D}\), which learns what frequency the model should attend to for each feature. \(2\times H_{t}\), \(F_{t}\), \(2\times M_{t}\) are concatenated on dimension 0, resulting in \(\mathbb{R}^{(2K+1)\times D}\), which is then weighted summed with _sa-matrix_ on dimension 0, generating output \(F^{\prime}_{t}\) (Figure 2c).

\[F^{\prime}_{t}=sum(softmax(\textit{sa-matrix},dim\ 0)\cdot concat((2\times H_{t},F_{t},2 \times M_{t}),dim\ 0),dim\ 0) \tag{3}\]

The _sa-matrix_ is initialized so that \(softmax(\textit{sa-matrix})\) resembles a Gaussian distribution on axis 0. This results in symmetric value on axis 0 (_sa-matrix\({}^{K+1-i}\) = sa-matrix\({}^{K+1+i}\)_) and makes \(SA\) an identity function on initialization (\(\because H^{k}+M^{K-k-1}=F\)).

\[F=SA_{init}(F) \tag{4}\]

\(SA\) allows the model to attend to multiple frequencies of its feature signal, enabling it to focus on either long-range dependencies or high-frequency patterns as needed and shift the feature \(F\) distribution on the frequency domain. By initializing \(SA\) as an identity function, the model can be fine-tuned with the already trained base model, allowing efficient implementation.

### Batched Spectral Attention

Batched Spectral Attention (\(BSA\)) enables batch training over multiple time steps. The main concept involves unfolding EMA, which facilitates gradients to flow across consecutive samples in a mini-batch, akin to BPTT. This enables efficient parallel training and promotes the model to extract long-range information beneficial for future prediction, extending the effective look-back window. The overall flow of \(BSA\) is depicted in Figure 3.

With mini-batch of size \(B\), consecutive samples \(X_{[t,t+B-1]}=\{X_{t},...X_{t+B-1}\}\in\mathbb{R}^{B\times S\times N}\) are given as input. Following aforementioned \(SA\) setting, \(BSA\) takes \(F_{[t,t+B-1]}=\{F_{t},...F_{t+B-1}\}\in\mathbb{R}^{B\times D}\) as input. In the next step, \(BSA\) utilizes \(F_{[t,t+B-1]}\) and the stored \(M_{t}\in\mathbb{R}^{K\times D}\) to generate \(M_{t+b}(0\leq b\leq B)\) by unfolding the Equation 1.

\[M^{k,i}_{t+b}=\alpha^{b}_{k}\times M^{k,i}_{t}+(1-\alpha_{k})\alpha^{b-1}_{k} \times F^{i}_{t}+\cdots+(1-\alpha_{k})\times F^{i}_{t+b-1} \tag{5}\]

This equation can be transformed to calculate \(M_{[t,t+B]}\in\mathbb{R}^{(B+1)\times K\times D}\) in parallel as follows.

\[M^{:,k,i}_{[t,t+B]}=lower\text{-}triangle(A^{k})\times\textit{ concat}((M^{k,i}_{t},F^{:,i}_{[t,t+B-1]}),dim\ 0) \tag{6}\]

\[A\in\mathbb{R}^{K\times(B+1)\times(B+1)},\ A^{k,p,q}=(1-\alpha_{k})^{I\{q>0\} }\alpha^{p-q}_{k} \tag{7}\]

\(A\) refers to unfolding matrix and \(I\) refers to indicator function. \(M_{t+B}\) is stored in \(BSA\) for the next mini-batch input. \(F^{\prime}_{[t,t+B-1]}\) is computed in parallel, similar to Equation 2 and 3, using \(F_{[t,t+B-1]}\), \(B_{[t,t+B-1]}\), and _sa-matrix\(\in\mathbb{R}^{(2K+1)\times D}\)_. The \(lower\text{-}triangle\) function prevents gradients from the past timestep from flowing into future models, aligning with the characteristics of time-series data.

Figure 3: BSA module takes a sequentially-sampled mini batch \(\{X_{t},...X_{t+B-1}\}\) and computes the corresponding EMA momentums \(\{M_{t},...M_{t+B-1}\}\) over time. This is done via single matrix multiplication enabling parallelization. We made the momentum parameter \(\alpha_{i}\) learnable, allowing the model to directly learn the periodicity of the information essential for the future prediction.

At the beginning of each training epoch, \(M_{0}\) is initialized to \(F_{0}\) for all \(\alpha\), enhancing stability for subsequent EMA accumulation. Since \(SA\) is proposed to address long-range dependencies in training, it lacks sufficient information in the early stages when not enough steps have been seen. Therefore, for the stability of training, we linearly warm up the learning rate for the first \(1/(1-max(\textit{smoothing factors}))\) timesteps at the beginning of each training epoch. The overall training of both the base model and \(BSA\) is conducted according to Algorithm 1.

In SA, the _smoothing factors_\(\{\alpha_{1},...,\alpha_{K}\}\)\(\in\mathbb{R}^{K}\) were given as scalar values, whereas in \(BSA\), they are expressed by learnable parameters. This is because \(BSA\) can utilize additional past information in training beyond the look-back window by incorporating a batch-sized time window, allowing it to determine the extent of long-range dependency required for training. To keep the smoothing factors between 0 and 1, we initialized learnable parameters by applying an inverse sigmoid to the initial smoothing factors and then applied a sigmoid function in training.

So far, we assume the feature \(F\) from the base model as a vector. However, the output of the intermediate layers of the model is often represented as a tensor with two or more dimensions. In real practice, we use additional channel dimensions in \(BSA\) to process the activation tensor, which acts as applying multiple \(BSA\) modules simultaneously.

### Consecutive dataset split

In the TSF scenario, the entire time series data \(\{x_{1},...,x_{T_{end}}\}\) is divided into train, validation, and test sets in chronological order. Let \(T_{tr}\) and \(T_{val}\) denote the last time in the train and validation data respectively. Model training occurs using data for t in [1, \(T_{tr}\)], while model selection for the best model can utilize data for t in [\(T_{tr+1}\), \(T_{val}\)]. The test set comprises predicting time steps [\(T_{val+1}\), \(T_{end}\)], which are not accessible during training or validation. However, since each training sample consists of the look-back window of size L and a prediction window of size S, the training input samples are restricted to \(X_{[L,T_{tr}-S]}\). Validation samples and test input samples range from \(X_{[T_{tr},T_{val}-S]}\), and from \(X_{[T_{val},T_{end}-S]}\), respectively. While this approach is plausible for independent data like images, it is unnatural for sequential data, as it leaves unreachable gaps (\(X_{[T_{tr}-S+1,T_{tr}-1]}\), \(X_{[T_{val}-S+1,T_{val}-1]}\)), undermining the consecutive characteristics. We filled the missing gaps, making train, validation, and test sets consecutive so that our \(BSA\) model could update momentum continuously. For fair evaluation, added samples were not used for either model training, validation, or performance assessment. Detailed explanations of model validation and evaluation are provided in Appendix A.3. The full code is available at [https://github.com/DJLee1208/BSA_2024](https://github.com/DJLee1208/BSA_2024).

```
Input: Trained up to (\(E\)-1)th epoch  TSF model \(f_{\theta}\), \(BSA\) with sa-matrix, smoothing factors  Train data: \(\mathfrak{D}_{tr}=\{(X_{0},Y_{0}),...,(X_{tr},Y_{tr})\}\)  Valid data: \(\mathfrak{D}_{val}=\{(X_{tr+1},Y_{tr+1}),...,(X_{val},Y_{val})\}\)  mini-batch size: B Train: \(st=0,ed=B-1\)  initialize \(M_{0}\) in \(BSA\) with \(X_{0}\) and \(f_{\theta}\) (\(:=f_{2\theta}\cdot f_{1\theta}\)) for\(X_{[st,ed]},Y_{[st,ed]}\) in \(\mathfrak{D}_{tr}\)do {: train phase \(\}\)\(\mathcal{D}_{]st,ed}\gets f_{2\theta}\cdot BSA\cdot f_{1\theta}(X_{[st,ed]})\) \(\mathcal{L}=\mathcal{L}_{\text{mes}}(P_{[st,ed]},Y_{[st,ed]})\)  Compute \(\nabla\mathcal{L}\) and adjust learning rate update \(f_{\theta}\), sa-matrix, smoothing factors \(st+=B\), \(ed=min(ed+B,tr)\) endfor \(st=tr+1,ed=tr+B\) for\(X_{[st,ed]},Y_{[st,ed]}\) in \(\mathfrak{D}_{val}\)do {: Validation phase \(\}\)\(P_{[st,ed]}\leftarrow f_{2\theta}\cdot BSA\cdot f_{1\theta}(X_{[st,ed]})\) \(\mathcal{L}=\mathcal{L}_{\text{mes}}(P_{[st,ed]},Y_{[st,ed]})\)  accumulate \(\mathcal{L}\) and calculate validation loss \(\mathcal{L}_{val}\) \(st+=B\), \(ed=min(ed+B,val)\) endfor Output: Trained up to \(E\) th epoch \(f_{\theta}\), sa-matrix, \(\{\alpha_{1},...,\alpha_{K}\}\), \(\mathcal{L}_{val}\) for \(E\) th epoch
```

**Algorithm 1** Batched Spectral Attention (1 epoch)

## 4 Experiments

We first evaluate BSA using state-of-the-art TSF models and various real-world time series forecasting scenarios in section 4.1. Next, to demonstrate that BSA effectively addresses long-range dependencies and is robust to distribution shift, we perform experiments on synthetic signals of various frequencies in section 4.2. Finally, we analyze the BSA's performance variations depending on the insertion sites within the base model, examine computation and memory costs, and conduct an ablation study in section 4.3.

**Datasets.** We use eleven real-world public datasets: Weather, Traffic, ECL, ETT (4 sub-datasets; h1, h2, m1, m2), Exchange, PEMS03, EnergyData, and Illness [6, 26, 29, 49]. In the Illness dataset,

[MISSING_PAGE_FAIL:7]

7). BSA effectively improves MSE and MAE across all architectures. The average performance gain, in terms of MSE, ranged from as low as 0.96% to as high as 7.2%, with linear-based models demonstrating relatively high performance. Paired t-tests demonstrate statistically significant (p-value < 0.05) improvements in most MSE and all MAE across various models. This result emphasizes the model-agnostic versatility of BSA. Furthermore, BSA exhibited overall performance gain across all datasets, with the enhancement being statistically significant in 82% of cases. BSA's higher gain on ETTm compared to ETTh, both derived from the same signal but with different sampling rates, further indicates BSA's effectiveness in handling long-range dependencies.

To understand how BSA addresses long-range dependencies, we conduct an internal inspection. In Figure 3(a), we present the heat map of the trained SA-Matrix of the DLinear (Temperature(\({}^{\circ}\)C) channel, Weather data). The positive x-axis corresponds to the \(log_{10}\) values of the periods preserved by the low-pass filter, derived with the smoothing factor. Negative values correspond to high-frequency components. The blue graph in Figure 3(b) represents the SA-Matrix averaged over the feature dimension, illustrating the frequencies to which BSA attends overall. The red graph represents the result of applying the Fast Fourier Transform (FFT) and denoising to the raw signal. The blue graph skewed towards the low-frequency side indicates that the BSA effectively captures the long-range trend of the data. Figure 3(c) depicts the graph for the SWDR (Short Wave Downward Radiation per unit area, W/m) channel in the same SA-Matrix. While not identical to Figure 3(b), it also exhibits strong attention towards the low-frequency pattern. In contrast, Figure 3(d), the FFT graph for the HULL (High UseLess Load channel, ETTh1 data), shows that the signal itself lacks long-range trends, resulting in the symmetric SA-Matrix. This result demonstrates that BSA operates as intended, learning the low-frequency components of the signal for future prediction. We provided other graphs and detailed information on the graph plotting method in Appendix B.2.

### Synthetic datasets

To further demonstrate that BSA learns long-range dependencies beyond the look-back window, we add sine waves with periods of 100, 300, and 1000 to the natural data while maintaining the mean and standard deviation (Refer to Appendix C for details on synthetic data generation). Figure 5 illustrates the performance improvement of BSA over the iTransformer model on the ETTh1 and

Figure 4: This figure illustrates the analysis of the SA-matrix of the DLinear model trained on the 720-step prediction task for the Weather and ETTh1 datasets. Panel (a) shows the heatmap of the SA-matrix, and (b)-(d) show the attention and FFT graphs.

Figure 5: Results of the iTransformer model on synthetic (a) ETTh1 and (b) ETTh2 datasets. The x-axis is the prediction length (96, 192, 336, 720), and the y-axis is the performance improvement (%) compared to the base model. Each color represents the different periods of the sine wave added to the natural data. 0 indicates original data and serves as the baseline.

ETTh2 datasets. The x-axis is the prediction length, and each line represents the period of the added sine wave. Performance improvement is calculated as 100\(\times\)(base MSE - BSA MSE) / base MSE. While the base model with a 96-length look-back window is expected to learn the 100-period trend, BSA outperformed it, especially for 96 and 192-step predictions (green line). The yellow line (period 300) shows nearly a 30% performance improvement across all prediction lengths. While the base model fails to learn the long-range interactions within a period of 300, BSA captures and utilizes the underlying trend. BSA also learns the 1000-period signal (red line) and demonstrates substantial improvements, especially in long prediction-length (336, 720) tasks. These results show that BSA effectively learns long-range patterns beyond the look-back window, essential for future prediction.

Figure 6 is generated from LUFL (Low UseFul Load) channel of ETTh1 data and with added sine waves of periods 100, 300, and 1000. The red arrow shows the added synthetic trend in the FFT graph (red line). As long-range trends are introduced, the attention graph (blue line) shifts from resembling symmetric Gaussian to a low-frequency bias. Longer sine wave periods cause greater shifts, prioritizing long-range information for predictions.

### Analysis and ablation studies

The BSA module offers high flexibility as it can be applied to arbitrary activations of the base model. In Table 2, we analyzed the performance changes by applying BSA at various locations within the model. Each location corresponds to a number in the Transformer architecture depicted in Figure 7. While we uniformly applied BSA to position 1 for the main result Table 1, the results in Table 2 suggest the potential for further performance enhancement by applying BSA at appropriate locations. Additionally, while there is variability depending on the placement, the performance consistently remains higher compared to the baseline, demonstrating the stability of our method. We provided a full result in Appendix D.1.

BSA shows consistent performance improvement across varying look-back window (input) lengths. Table 3 demonstrates BSA's superiority for look-back window lengths of 48, 96, and 192. Notably, while the baseline model's performance drops significantly with shorter inputs, BSA maintains high performance.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline \multirow{2}{*}{Location} & \multicolumn{2}{c}{iTransformer} & \multicolumn{2}{c}{DLinear} \\  & MSE & MAE & MSE & MAE \\ \hline
1 & 0.2357 & 0.2682 & **0.2196** & **0.2815** \\
2 & 0.2352 & 0.2681 & - & - \\
3 & 0.2329 & **0.2654** & - & - \\
4 & 0.2508 & 0.2752 & 0.2327 & 0.2994 \\ Query-5 & **0.2326** & 0.2671 & - & - \\ Key-6 & 0.2538 & 0.2762 & - & - \\ Value-7 & 0.2415 & 0.2702 & - & - \\ \hline baseline & 0.2556 & 0.2766 & 0.2444 & 0.3084 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance analysis on BSA insertion site. Each number corresponds with the insertion site in Figure 7.

We also demonstrate the impact of using BSA on the training time, peak memory, and the number of model parameters. The extra computation required for BSA is constant with data length and linear with a look-back window length (c.f. quadratic for the base model with transformer architecture). Table 4 demonstrates the computational cost of the BSA is quite small, showing less than a 5% increase even with the large PEMS03 dataset.

We conduct an ablation study on the three key components that constitute BSA (Table 5). "BPTT" refers to whether gradients can flow between samples within the mini-batch. Without BPTT, BSA learns similarly to SA. "SFs" denotes whether to use multiple smoothing factors. Lastly, "Learn SF" indicates whether the smoothing factor is treated as learnable. The results indicate that each component significantly contributes to the performance improvement of BSA.

## 5 Conclusion

Our study addresses the challenges in handling long-range dependencies inherent in time series data by introducing a fast and effective Spectral Attention mechanism. By preserving temporal correlations and enabling the flow of gradients between samples, this mechanism facilitates the model in capturing crucial long-range interactions essential for accurate future predictions. Therefore, our research paves the way for fixed-sized input models to effectively handle long-range dependencies extending far beyond the input window. Through extensive experimentation, we demonstrated that our Spectral Attention mechanism enhances performance across various base architectures, with its ability to grasp long-term dependencies being the key factor behind this improvement. BSA effectively tackles long-term fluctuations, complementing the base model's capacity to manage intricate yet short-term patterns. This integrated model holds promise for improving real-world application performance. For instance, it could boost weather forecast accuracy by simultaneously capturing minute-by-minute weather changes and seasonal variations. Moreover, when predicting deterioration from a patient's real-time data, it can consider medications with lengthy onset times. Our study has limitations: we did not analyze the impact of BSA placement within the base model in detail. Also, BSA's performance gains may be limited when applied to datasets with only high-frequency information within the look-back window. These issues should be addressed in future research.

## 6 Acknowledgement

This research was supported by a grant of the MD-Phd/Medical Scientist Training Program through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health & Welfare, Republic of Korea, National Research Foundation of Korea (NRF) grants funded by the Korea government (Ministry of Science and ICT, MSIT) (2022R1A3B1077720 and 2022R1A5A708390811), Institute of Information & Communications Technology Planning & Evaluation (IITP) grants funded by the Korea government (MSIT) (2021-0-01343: Artificial Intelligence Graduate School Program (Seoul National University), 2022-0-00959 and IITP-2024-RS-2024-00397085: Leading Generative AI Human Resources Development), and the BK21 FOUR program of the Education and Research Program for Future ICT Pioneers, Seoul National University in 2024, AI-Bio Research Grant through Seoul National University, Hyundai Motor Company, HUINNO AIM Company through HA-Rnd-2325-PredictClinicalDeterioration.

\begin{table}
\begin{tabular}{l|c c c c} \multicolumn{1}{c|}{_Input-length_} & \multicolumn{3}{c}{Weather} & \multicolumn{3}{c}{PESMS03} \\ _Length_ & MSE(\%) & MAE*(\%) & MSE(\%) & MAE*(\%) \\ \hline
48 & 12.08 & 5.21 & 29.45 & 17.35 \\ \hline
96 & 7.78 & 3.03 & 24.55 & 13.66 \\
192 & 6.87 & 3.34 & 9.63 & 5.18 \\ \end{tabular}
\end{table}
Table 3: BSA’s performance improvement (%, denoted with *) compared to base model across three input lengths \(\{48,96,192\}\) (iTransformer, average over 4 prediction lengths and 3 seeds). The full result, including other models, is in Appendix D.2.

\begin{table}
\begin{tabular}{l|c c c c} \multicolumn{1}{c|}{_Input-length_} & \multicolumn{2}{c}{Weather} & \multicolumn{2}{c}{PESMS03} \\ _Length_ & MSE(\%) & MAE*(\%) & MSE(\%) & MAE*(\%) \\ \hline
48 & 12.08 & 5.21 & 29.45 & 17.35 \\ \hline
96 & 7.78 & 3.03 & 24.55 & 13.66 \\
192 & 6.87 & 3.34 & 9.63 & 5.18 \\ \end{tabular}
\end{table}
Table 4: Computational cost increase with the BSA (%), averaged over 3 seeds and 4 prediction lengths on the PEMS03 dataset. TN: TimesNet, iTF: iTransformer, CF: Crossformer, PTST: PatchTST. The full result, including other datasets, is in Appendix D.3.

## References

* [1] Abien Fred Agarap. Deep learning using rectified linear units (relu). _arXiv preprint arXiv:1803.08375_, 2018.
* [2] Nesreen K Ahmed, Amir F Atiya, Neamat El Gayar, and Hisham El-Shishiny. An empirical comparison of machine learning models for time series forecasting. _Econometric reviews_, 29(5-6):594-621, 2010.
* [3] Peter Bloomfield. _Fourier analysis of time series: an introduction_. John Wiley & Sons, 2004.
* [4] Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm for optimal margin classifiers. In _Proceedings of the fifth annual workshop on Computational learning theory_, pages 144-152, 1992.
* [5] George EP Box and Gwilym M Jenkins. Some recent advances in forecasting and control. _Journal of the Royal Statistical Society. Series C (Applied Statistics)_, 17(2):91-109, 1968.
* [6] Luis M Candanedo, Veronique Feldheim, and Dominique Deramaix. Data driven prediction models of energy use of appliances in a low-energy house. _Energy and buildings_, 140:81-97, 2017.
* [7] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, and Rose Yu. Long-term forecasting with tide: Time-series dense encoder. _arXiv preprint arXiv:2304.08424_, 2023.
* [8] Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 459-469, 2023.
* [9] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. _Annals of statistics_, pages 1189-1232, 2001.
* [10] Agathe Girard, Carl Rasmussen, Joaquin Q Candela, and Roderick Murray-Smith. Gaussian process priors with uncertain inputs application to multiple-step ahead time series forecasting. _Advances in neural information processing systems_, 15, 2002.
* [11] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_, 2016.
* [12] Pradeep Hewage, Ardhendu Behera, Marcello Trovati, Ella Pereira, Morteza Ghahremani, Francesco Palmieri, and Yonghuai Liu. Temporal convolutional neural (tcn) network for an effective weather forecasting using time-series data from the local weather station. _Soft Computing_, 24:16453-16482, 2020.
* [13] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. _Neural computation_, 18(7):1527-1554, 2006.
* [14] Tin Kam Ho. Random decision forests. In _Proceedings of 3rd international conference on document analysis and recognition_, volume 1, pages 278-282. IEEE, 1995.
* [15] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.
* [16] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. _arXiv preprint arXiv:1909.06335_, 2019.
* [17] MJC Hu and Halbert E Root. An adaptive data processing system for weather forecasting. _Journal of Applied Meteorology and Climatology_, 3(5):513-523, 1964.
* [18] Rob J Hyndman and George Athanasopoulos. _Forecasting: principles and practice_. OTexts, 2018.

* [19] A. Kahraman, G. Yang, and P. Hou. Wind power forecasting using lstm incorporating fourier transformation based denoising technique. In _20th International Workshop on Large-Scale Integration of Wind Power into Power Systems as well as on Transmission Networks for Offshore Wind Power Plants (WIW 2021)_, volume 2021, pages 94-98, 2021.
* [20] Kyoung-jae Kim. Financial time series forecasting using support vector machines. _Neurocomputing_, 55(1-2):307-319, 2003.
* [21] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In _International Conference on Learning Representations_, 2021.
* [22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [23] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. _arXiv preprint arXiv:2001.04451_, 2020.
* [24] Diego Krapf, Enzo Marinari, Ralf Metzler, Gleb Oshanin, Xinran Xu, and Alessio Squarcini. Power spectral density of a single brownian trajectory: what one can and cannot learn from it. _New Journal of Physics_, 20(2):023029, 2018.
* [25] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In _The 41st international ACM SIGIR conference on research & development in information retrieval_, pages 95-104, 2018.
* [26] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In _The 41st international ACM SIGIR conference on research & development in information retrieval_, pages 95-104, 2018.
* [27] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. _Advances in neural information processing systems_, 32, 2019.
* [28] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. Revisiting long-term time series forecasting: An investigation on linear mapping. _arXiv preprint arXiv:2305.10721_, 2023.
* [29] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. Scinet: Time series modeling and forecasting with sample convolution and interaction. _Advances in Neural Information Processing Systems_, 35:5816-5828, 2022.
* [30] Yeqi Liu, Chuanyang Gong, Ling Yang, and Yingyi Chen. Dstp-rnn: A dual-stage two-phase attention-based recurrent neural network for long-term and multivariate time series prediction. _Expert Systems with Applications_, 143:113082, 2020.
* [31] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. _arXiv preprint arXiv:2310.06625_, 2023.
* [32] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Rethinking the stationarity in time series forecasting. _NeurIPS_, 2022.
* [33] MickOhrberg. Brownian noise -- Wikipedia, the free encyclopedia, 2024. [Online; accessed 9-April-2024].
* [34] Fernando Moreno-Pino, Pablo M Olmos, and Antonio Artes-Rodriguez. Deep autoregressive models with spectral attention. _Pattern Recognition_, 133:109014, 2023.
* [35] Brian K Nelson. Time series analysis using autoregressive integrated moving average (arima) models. _Academic emergency medicine_, 5(7):739-744, 1998.
* [36] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. _arXiv preprint arXiv:2211.14730_, 2022.

* [37] Vigneashwara Pandiyan, Rita Drissi-Daoudi, Sergey Shevchik, Giulio Masinelli, Roland Loge, and Kilian Wasmer. Analysis of time, frequency and time-frequency domain features from acoustic emissions during laser powder-bed fusion process. _Procedia CIRP_, 94:392-397, 2020.
* [38] Adam Paszke, S. Gross, Francisco Massa, A. Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Z. Lin, N. Gimelshein, L. Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. _NeurIPS_, 2019.
* [39] Hadi Rezaei, Hamidreza FaaIjou, and Gholamreza Mansourfar. Stock price prediction using deep learning and frequency decomposition. _Expert Systems with Applications_, 169:114332, 2021.
* [40] Frank Rosenblatt. _The perceptron, a perceiving and recognizing automaton Project Para_. Cornell Aeronautical Laboratory, 1957.
* [41] Alaa Sagheer and Mostafa Kotb. Time series forecasting of petroleum production using deep lstm recurrent networks. _Neurocomputing_, 323:203-213, 2019.
* [42] Donghwan Song, Adrian Matias Chung Baek, and Namhun Kim. Forecasting stock market indices using padding-based fourier transform denoising and time series deep learning models. _IEEE Access_, 9:83786-83796, 2021.
* [43] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. _Advances in neural information processing systems_, 27, 2014.
* [44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [45] Renzhuo Wan, Shuping Mei, Jun Wang, Min Liu, and Fan Yang. Multivariate temporal convolutional network: A deep neural networks approach for multivariate time series forecasting. _Electronics_, 8(8):876, 2019.
* [46] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Transformers in time series: A survey. _arXiv preprint arXiv:2202.07125_, 2022.
* [47] Paul J Werbos. Backpropagation through time: what it does and how to do it. _Proceedings of the IEEE_, 78(10):1550-1560, 1990.
* [48] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. _ICLR_, 2023.
* [49] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. _Advances in neural information processing systems_, 34:22419-22430, 2021.
* [50] Zhijian Xu, Ailing Zeng, and Qiang Xu. FITS: Modeling time series with $10k$ parameters. In _The Twelfth International Conference on Learning Representations_, 2024.
* [51] Fuhao Yang, Xin Li, Min Wang, Hongyu Zang, Wei Pang, and Mingzhong Wang. Waveform: Graph enhanced wavelet learning for long sequence forecasting of multivariate time series. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 10754-10761, 2023.
* [52] Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An, Defu Lian, Longbing Cao, and Zhendong Niu. Frequency-domain mlps are more effective learners in time series forecasting. _Advances in Neural Information Processing Systems_, 36, 2024.
* [53] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In _Proceedings of the AAAI conference on artificial intelligence_, volume 37, pages 11121-11128, 2023.

* [54] Yunhao Zhang and Junchi Yan. Crosformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In _The eleventh international conference on learning representations_, 2022.
* [55] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 11106-11115, 2021.
* [56] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In _International conference on machine learning_, pages 27268-27286. PMLR, 2022.

Details on Datasets, Models, and Training

### Details on datasets

**Dataset Information.** We conducted experiments on 11 real-world datasets to assess the performance of baseline models and the proposed BSA method. The Weather dataset [49] includes 21 meteorological factors acquired every 10 minutes in 2020 from the Weather Station of the Max Planck Institute for Biogeochemistry. The Traffic dataset [49] records hourly road occupancy rates from 862 sensors on San Francisco Bay Area freeways, covering the period from January 2015 to December 2016. The ECL dataset [49] captures the hourly electricity consumption of 321 clients. The ETT dataset [55] contains seven factors related to electricity transformers, spanning from July 2016 to July 2018. It is divided into four sub-datasets: ETTh1 and ETTh2 are recorded hourly, while ETTm1 and ETTm2 are collected every 15 minutes. The Exchange dataset [49] comprises panel data of daily exchange rates from eight countries, ranging from 1990 to 2016. Illness dataset [49] contains weekly data on influenza-like illness (ILI) cases recorded by the U.S. Centers for Disease Control and Prevention (CDC) from 2002 to 2021. The dataset tracks the proportion of ILI patients relative to the overall number of patients seen during that period. PEMS03 dataset [29] is a sub-dataset of the PEMS dataset, which includes public traffic network data from California, recorded at 5-minute intervals. The EnergyData dataset [6] comprises hourly end-use measurements gathered from 454 residential properties and 140 commercial establishments located in the Pacific Northwest. All these public datasets were downloaded from the referenced sources in March 2024.

**Dataset split.** We adhere to the data processing protocol and train-validation-test split used in TimesNet [48], where the training, validation, and test datasets are sequentially separated in chronological order. Our paper's data split ratios for train, validation, and test set are as follows: (0.7, 0.1, 0.2) for Weather, Traffic, ECL, Exchange, Ilnness, PEMS03, and EnergyData datasets and (0.6, 0.2, 0.2) for ETT. The details of datasets are provided in Table 6.

**Forecasting setting.** Following the approach in iTransformer [31], the look-back window length is set to [96], while the forecast lengths are {96, 192, 336, 720} for the Weather, Traffic, ECL, ETT, PEMS03, and EnergyData datasets. Forecasting lengths is only {96} for the Exchange dataset since the dataset is too short, which causes biased best model selection with the validation set. For Illness dataset, the look-back window length is {36} and the forecasting lengths are {24, 36, 48, 60}.

### Details on Model Implementations

**Model configurations.** In this section, we discuss the configurations of the baseline models and specify where the BSA module was integrated into each baseline model. For each dataset, the base model structure configuration was directly **replicated from the Time-Series-Library**[48] scripts

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline
**Dataset** & **Channel** & **Pre-Train** & **Finetune** & **Data Split** & **Sampling Rate** & **Type** \\ \hline Weather* & 21 & 40 & 20 & (36792, 5271, 10540) & 10min & Weather \\ Traffic* & 862 & 20 & 20 & (12185, 1757, 3509) & Hourly & Transportation \\ ECL* & 321 & 30 & 20 & (18317, 2633, 5261) & Hourly & Electricity \\ ETTh1, ETTh2 & 7 & 30 & 30 & (10357, 3485, 3485) & Hourly & Electricity \\ ETTm1, ETTm2 & 7 & 30 & 20 & (41713, 13937, 13937) & 15min & Electricity \\ Exchange & 8 & 30 & 30 & (5216, 761, 1518) & Daily & Economy \\ Illness & 7 & 30 & 20 & (692, 120, 226) & Weekly & Health \\ PEMS03 & 358 & 30 & 20 & (18250, 2623, 5242) & 5min & Transportation \\ EnergyData & 28 & 30 & 20 & (13719, 1975, 3948) & Hourly & Energy \\ \hline \hline \end{tabular}
\end{table}
Table 6: Summary of Datasets. _Channel_ denotes the number of time series variables (channels) for each dataset. _Pre-Train_ is the number of training epochs for baseline model saturation. _Finetune_ is the number of fine-tuning epochs for our method. _Data Split_ means the number of time steps in (Train, Validation, Test) data split respectively. _Sampling Rate_ denotes how often the data samples are collected. _Type_ is to show the domain in which the data is acquired. We have indicated cases where the data split count matches exactly with that used in TimesNet [48] by marking them with an asterisk (*). For the ETT and the Exchange dataset, the length of the downloaded data differed from the data length reported in TimesNet.

when available. Where configurations were not provided, we adjusted them to align closely with the available examples.

1. **DLinear**[53]: The only hyperparameter for this baseline model is the (moving_average = 25) for the series decomposition module from Autoormer [49]. We set the Individual to True so that there are separate linear models for each number of input variables. The BSA module is implemented at the very beginning of the forward pass right before series decomposition. The BSA module is implemented for each channel, which is the number of input variables for this case.

2. **iTransformer**[31]: The hyperparameters for this baseline model are as follows:

(d_model = 512, d_ff = 512, dropout = 0.1, num_heads = 8, encoder_layers = 3, activation = GELU [11]) for the Weather, ECL, PEMS03, and EnergyData datasets,

(d_model = 512, d_ff = 512, dropout = 0.1, num_heads = 8, encoder_layers = 4, activation = GELU) for the Traffic dataset,

(d_model = 128, d_ff = 128, dropout = 0.1, num_heads = 8, encoder_layers = 2, activation = GELU) for the EIT and Exchange datasets.

The BSA module is implemented at the beginning part of the forward pass right after normalization from the Non-stationary Transformer [32] and right before the input embedding. The BSA module is implemented for each channel, which is the number of input variables for this case.

3. **Crossformer**[54]: The hyperparameters for this baseline model are as follows:

(seg_len = 12, win_size = 2, factor = 3, d_model = 32, d_ff = 32, dropout = 0.1, num_heads = 8, encoder_layers = 2, activation = RELU [1]) for the Weather and EnergyData dataset,

(seg_len = 12, win_size = 2, factor = 3, d_model = 128, d_ff = 128, dropout = 0.1, num_heads = 2, encoder_layers = 2, activation = RELU) for the Traffic dataset,

(seg_len = 12, win_size = 2, factor = 3, d_model = 256, d_ff = 512, dropout = 0.1, num_heads = 8, encoder_layers = 2, activation = RELU) for the ECL and PEMS03 dataset,

(seg_len = 12, win_size = 2, factor = 3, d_model = 512, d_ff = 2048, dropout = 0.1, num_heads = 8, encoder_layers = 2, activation = RELU) for the ETTh1 and ETTh2 datsets,

(seg_len = 12, win_size = 2, factor = 1, d_model = 512, d_ff = 2048, dropout = 0.1, num_heads = 8, encoder_layers = 2, activation = RELU) for the ETTm1 and ETTm2 datasets,

(seg_len = 12, win_size = 2, factor = 3, d_model = 64, d_ff = 64, dropout = 0.1, num_heads = 8, encoder_layers = 2, activation = RELU) for the Exchange dataset.

The BSA module is implemented at the very beginning of the forward pass, right before the input embedding. The BSA module is implemented for each channel, which is the number of input variables for this case.

4. **PatchTST**[36]: The hyperparameters for this baseline model are as follows:

(patch_len = 16, stride = 8, d_model = 512, d_ff = 2048, dropout = 0.1, num_heads = 4, encoder_layers = 2, activation = GELU [11]) for the Weather and EnergyData dataset,

(patch_len = 16, stride = 8, d_model = 512, d_ff = 512, dropout = 0.1, num_heads = 8, encoder_layers = 2, activation = GELU) for the Traffic dataset,

(patch_len = 16, stride = 8, d_model = 512, d_ff = 2048, dropout = 0.1, num_heads = 8, encoder_layers = 2, activation = GELU) for the ECL and PEMS03 dataset,

(patch_len = 16, stride = 8, d_model = 512, d_ff = 2048, dropout = 0.1, num_heads = 8, encoder_layers = 1, activation = GELU) for the ETTh1 datset,

(patch_len = 16, stride = 8, d_model = 512, d_ff = 2048, dropout = 0.1, num_heads = 4, encoder_layers = 3, activation = GELU) for the ETTh2, ETTm1, and ETTm2 datsets.

The BSA module is implemented at the beginning part of the forward pass right after normalization from the Non-stationary Transformer [32] and right before the patch embedding. The BSA module is implemented for each channel, which is the number of input variables for this case.

5. **TimesNet**[48]: The hyperparameters for this baseline model are as follows:

(top_k = 5, num_kernels = 6, embed = 'timeF', freq = 'h', d_model = 32, d_ff = 32, dropout = 0.1, encoder_layers = 2) for the Weather, ETTh2, ETTm2, and EnergyData datasets,

(top_k = 5, num_kernels = 6, embed = 'timeF', freq = 'h', d_model = 512, d_ff = 512, dropout = 0.1, encoder_layers = 2) for the Traffic dataset,

(top_k = 5, num_kernels = 6, embed = 'timeF', freq = 'h', d_model = 256, d_ff = 512, dropout = 0.1, encoder_layers = 2) for the ECL and PEMS03 dataset,

(top_k = 5, num_kernels = 6, embed = 'timeF', freq = 'h', d_model = 16, d_ff = 32, dropout = 0.1, encoder_layers = 2) for the ETTh1 and ETTm1 datasets,(top_k = 5, num_kernels = 6, embed = "timeF", freq = 'h', d_model = 64, d_ff = 64, dropout = 0.1, encoder_layers = 2) for the Exchange dataset.

The BSA module is implemented at the beginning part of the forward pass right after normalization from the Non-stationary Transformer [32] and right before the input embedding. The BSA module is implemented for each channel, which is the number of input variables for this case.

6. **FreTS**[52]: The hyperparameters for this baseline model are as follows:

(embed_size = 128, hidden_size = 256, sparsity_threshold = 0.01, scale = 0.02) for all datasets. Based on the paper, the channel-independent strategy is selected.

The BSA module is implemented at the very beginning of the forward pass, right before the token embedding. The BSA module is implemented for each channel, which is the number of input variables for this case.

7. **RLinear**[28]: The only hyperparameter for this baseline model is the (dropout = 0.1). We set the Individual to True so that there are separate linear models for each number of input variables. The BSA module is implemented at the beginning part of the forward pass right after normalization from the RevIN [21] and right before the linear layer. The BSA module is implemented for each channel, which is the number of input variables for this case.

### Details on Training

**Pre-training and Finetuning configurations.** To show how our BSA module performs when added to the baseline models, we saturated the models by greedy hyperparameter search.

The hyperparameter search space for the base model is as follows: the possible learning rate is (0.03, 0.01, 0.003, 0.001, 0.0003), and the weight decay is (0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003).

The hyperparameter search space for BSA finetuning is as follows: the possible learning rate for the SA-Matrix in the BSA module is (0.08, 0.05, 0.03, 0.01, 0.003, 0.001), learning rate for the rest of the model, i.e. original modules, is (0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001), learning rate for smoothing factor \(\alpha_{k}\) is (none, 0.03, 0.01, 0.003, 0.001, 0.0001, 0.00001), initialization for smoothing factor \(\alpha_{k}\) is ([0.9, 0.99, 0.999], [0.9, 0.99, 0.999], [0.9, 0.95, 0.992, 0.999], [0.8, 0.96, 0.992, 0.9984, 0.99968]).

**Model selection** Hyperparameter search is conducted based on the validation set. While models trained using conventional sample shuffling evenly represent the entire time series distribution from which the dataset is sampled, BSA learns data in chronological order. Consequently, the final model tends to favor the distribution of the later part of the data. This can be seen as a mild version of catastrophic forgetting, commonly occurring in continual learning. To mitigate this effect, we assigned higher weights to the later samples during the validation process. The weights are represented by \(0.5+0.5\times\sin{(\frac{\pi}{2}\times\frac{val\cdot idx}{val\_len})}\), continuously increasing from 0.5 for the first sample to 1.0 for the last sample.

**Optimization** The whole code is implemented in PyTorch [38]. Each experiment was conducted on a single NVIDIA GeForce RTX 3090Ti or NVIDIA A40 or NVIDIA L40 GPU. The default batch size for baseline model saturation is 64, while for our method--which involves fine-tuning after integrating the BSA module--it is 256. If a baseline model is too heavy and results in GPU memory overflow, the batch size is adjusted to fit within the available memory. We used the ADAM [22] optimizer and L2 loss (MSE loss) for the model optimization. The baseline saturation training epoch is set to 40 epochs for the Weather dataset, 20 epochs for the Traffic dataset, and 30 epochs for the rest of the datasets. The finetuning epoch is set to 30 epochs for the ETTh1, ETTh2, and Exchange datasets and 20 epochs for the rest of the datasets.

## Appendix B Real world dataset experiments

### Full experiment results

The experiments were conducted on a total of 11 public real-world datasets and 7 forecasting models. The average results (MSE, MAE) of the experiments conducted with three random seeds 0, 1, and 2 are shown in Table 7, and the standard deviations are shown in Table 8. The values reported in Table 1 are labeled as Avg in Table 7. For the Exchange dataset, we only report experiments with a prediction length of [96]. Using prediction lengths of {192, 336, 720} causes improper best model selection due to an insufficient number of validation samples. The empty results indicate that the training is too heavy, and the results are not yet available. We expect to have the results ready by the rebuttal.

### SA-Matrix and FFT visualization

To investigate how the SA-Matrix of BSA predominantly attends to specific frequency bands, we employed the heatmaps, the Gaussian kernel density estimate graphs, and the FFT graphs as shown in Figure 9, 10, 11, 12, 15 and 13.

The 2D heatmap depicts the learnable parameters of the SA-Matrix, defined as _sa-matrix\(\in\mathbb{R}^{(2K+1)\times D}\)_, where \(K\) represents the number of smoothing factors and \(D\) represents the dimension of the input features. Consequently, the y-axis of the 2D heatmap matches the length of \(D\), and the x-axis corresponds to \(2K+1\), symmetrically arranged around zero. On the x-axis, positive values indicate the low-frequency regions, while negative values represent the high-frequency regions.

The Gaussian kernel density estimate graphs intuitively reveal which frequency bands the SA-Matrix predominantly attends to. The \(K\) smoothing factors were modified according to Equation 8 and symmetrically arranged around zero, serving as the data points for kernel density estimation (KDE). The weight values from each row of the SA-Matrix were converted to probabilities using the softmax function, and the resulting outputs established a mapping of these weight values to data points necessary for Gaussian KDE. This mapping facilitated the construction of the Gaussian KDE graph. Subsequently, the overall probability density of the SA-Matrix was estimated by calculating the mean across columns. Consequently, the y-axis of the graphs denotes the attention level of the SA-Matrix, whereas the x-axis indicates the scalar indices, encompassing the range of the smoothing factors. To better represent the variation in weight values across frequency bands, we adjusted the bandwidth of the KDE function to 0.4, based on the product of the Gaussian kernel's covariance factor and the standard deviation of the sampled weights in the matrix.

\[\alpha_{k}^{{}^{\prime}}=\log_{10}(\frac{1}{1-\sigma(\alpha_{k})}) \tag{8}\]

The FFT graph is depicted with a red line. FFT analysis was conducted for each variable to determine if the fine-tuned SA-Matrix aligns with the frequencies exhibited by the dataset. To compare the low-frequency parts, the negative log-scale was used on the x-axis, showing that progression to the right indicates decreasing frequencies. The Gaussian filter with a fixed standard deviation of 5 was utilized to smooth the signal's amplitude.

The denoising phase was executed on the FFT output to analyze the correlation between the frequency tendencies of the dataset and the manipulated weights in the SA-Matrix. Figure 8(c) depicts Brownian noise, with the x-axis denoting frequency and the y-axis in decibels (dB). Both axes are configured

Figure 8: The illustration compares the pre-denoising and post-denoising results alongside Brownian noise. For (a) and (b), the y-axis represents amplitude, and the x-axis is on a negative log scale, indicating that moving toward the right corresponds to lower frequencies. Conversely, in (c), the y-axis represents decibels, and the x-axis is on a log scale, where moving toward the left corresponds to lower frequencies.

on a log scale. Converting the y-axis from decibels to amplitude is akin to implementing linear scaling, which reveals a steeper rise in noise at lower frequencies. This noise generally arises in the low-frequency bands of most real-world systems, such as electronic systems and environmental sciences [24]. Figure 8 (a) shows that noise escalates within the low-frequency regions across variables, leading to the assumption that unique Brownian noise originates in each system. Consequently, a linear approximation was used for the denoising function, which was performed manually. The outcomes of this process are presented in Figure 8 (b).

## Appendix C Synthetic dataset experiments

We constructed synthetic datasets by adding sine waves with periods of 100, 300, and 1000 to each channel (variable) of the ETTh1 and ETTh2 datasets. The scale of the sine wave is set to the standard deviation value of each channel. The phases of the sine waves for each channel are randomly sampled from a uniform distribution from 0 to \(2\pi\) to make decorrelated sine waves. The full results are reported in Table 9.

### SA-Matrix and FFT visualization

As illustrated in Figure 14, the LUFL, MULL, OT, and LULL channels are organized into columns, with each graph positioned within rows that correspond to the periods added to the dataset. Figure 14 (a) shows the SA-Matrix from the iTransformer model fine-tuned with a 720 prediction length on the ETTh1 data that does not include added sine waves. From (b) to (d), the models were trained with synthetic datasets increasingly augmented with sine waves at periods of 100, 300, and 1000. As the number of added periods increases, there is a shift in the SA-Matrix weight distribution toward low-frequency patterns in each channel.

## Appendix D Analysis and Ablation Studies

### BSA insertion site analysis

Table 10 shows the full results for different BSA module insertion sites in the iTransformer [31] and DLinear [53] models on the Weather Dataset. Table 2 is from the average value of Table 10. The BSA module insert position can be found in Figure 7.

### BSA variable input lengths analysis

Table 11 shows the full results of the performance of the base model and BSA according to changes in Input length. Experiments were conducted on Weather and PEMS03 data using DLinear [53], RLinear [28], and iTransformformer [31] models. Table 3 in the main text shows the summarized results.

### BSA computational cost analysis

Table 12 presents a comprehensive analysis of BSA's computational cost. We measured the model training time (sec/1step), peak memory usage (GB), and the number of parameters (M) for both the base model and the model with BSA applied. The experiments were conducted using the lightweight Weather dataset with 21 channels and the heavy PEMS03 dataset with 358 channels. Tests were

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c c c c c c} \hline \hline  & & \multicolumn{4}{c|}{base} & \multicolumn{4}{c|}{1} & \multicolumn{4}{c|}{2} & \multicolumn{4}{c}{3} & \multicolumn{4}{c}{4} & \multicolumn{4}{c}{5} & \multicolumn{4}{c}{6} & \multicolumn{4}{c}{7} \\  & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{5}{*}{weather} & 96 & 0.1706 & 0.2906 & 0.1598 & 0.2027 & 0.1589 & 0.2027 & 0.1538 & 0.1990 & 0.1667 & 0.2077 & 0.1545 & 0.1909 & 0.1670 & 0.2080 & 0.1520 & 0.2011 \\  & 193 & 0.2706 & 0.2944 & 0.2952 & 0.2466 & 0.2343 & 0.1906 & 0.1484 & 0.2149 & 0.2269 & 0.2543 & 0.2451 & 0.2076 & 0.2498 & 0.2150 & 0.2094 \\  & 336 & 0.2762 & 0.2952 & 0.2536 & 0.2560 & 0.2510 & 0.2846 & 0.2528 & 0.2347 & 0.2719 & 0.2940 & 0.2519 & 0.2855 & 0.2756 & 0.2948 & 0.2534 & 0.2854 \\  & 720 & 0.3551 & 0.3735 & 0.3737 & 0.3277 & 0.3806 & 0.3255 & 0.3349 & 0.3497 & 0.3456 & 0.3238 & 0.3350 & 0.3513 & 0.3472 & 0.3434 & 0.3440 \\  & Avg & 0.2556 & 0.2766 & 0.2357 & 0.3282 & 0.2352 & 0.2881 & 0.2329 & 0.2549 & 0.2508 & 0.2752 & 0.2326 & 0.2671 & 0.2538 & 0.2742 & 0.2415 & 0.2702 \\ \hline \hline \multicolumn{11}{c}{} & \multicolumn{4}{c|}{Dlinear} & \multicolumn{4}{c|}{4} & \multicolumn{4}{c}{5} & \multicolumn{4}{c}{6} & \multicolumn{4}{c}{7} \\  & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{5}{*}{weather} & 96 & 0.1630 & 0.2463 & 0.1505 & 0.2406 & - & - & - & - & 0.1519 & 0.2030 & - & - & - & - & - & - & - \\  & 192 & 0.2091 & 0.2802 & 0.1985 & 0.2724 & - & - & - & - & 0.1912 & 0.2601 & - & - & - & - & - & - \\  & 336 & 0.2634 & 0.3721 & 0.2590 & 0.3251 & - & - & - & - & 0.2375 & 0.2493 & - & - & - & - & - & - \\  & 720 & 0.3421 & 0.8036 & 0.3167 & 0.3697 & - & - & - & - & 0.2799 & 0.3463 & - & - & - & - & - & - \\  & Avg & 0.2444 & 0.3084 & 0.2372 & 0.2994 & - & - & - & - & 0.2196 & 0.2815 & - & - & - & - & - & - \\ \hline \end{tabular}
\end{table}
Table 10: Full results for different BSA module locations for iTransformer [31] and DLinear [53] on the Weather dataset. Positions 1 and 4 are the only available options for the DLinear Model.

\begin{table}
\begin{tabular}{c|c c c c c|c c c c|c c c c} \hline \hline  & & \multicolumn{4}{c|}{Dlinear} & \multicolumn{4}{c|}{RLinear} & \multicolumn{4}{c}{iTransformer} \\  & & base & \multicolumn{4}{c|}{BSA} & \multicolumn{4}{c}{base} & \multicolumn{4}{c}{BSA} & \multicolumn{4}{c}{base} & \multicolumn{4}{c}{BSA} & \multicolumn{4}{c}{base} & \multicolumn{4}{c}{BSA} \\  & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \hline \multirow{5}{*}{Weather} & 96 & 0.12187 & 0.2950 & 0.1616 & 0.2332 & 0.1898 & 0.2322 & 0.1783 & 0.2233 & 0.2054 & 0.20271 & 0.1713 & 0.2124 \\  & 192 & 0.2477 & 0.3203 & 0.1994 & 0.2715 & 0.2315 & 0.2654 & 0.2706 & 0.2583 & 0.2490 & 0.2695 & 0.2178 & 0.2541 \\  & 336 & 0.3086 & 0.3696 & 0.2460 & 0.3094 & 0.2897 & 0.3064 & 0.2768 & 0.2980 & 0.3075 & 0.3099 & 0.2716 & 0.2947 \\  & 720 & 0.3817 & 0.4204 & 0.3085 & 0.3585 & 0.3699 & 0.3566 & 0.3579 & 0.3499 & 0.3831 & 0.3595 & 0.3459 & 0.3442 \\  & Avg & 0.2892 & 0.3513 & 0.2289 & 0.3923 & 0.2702 & 0.2902 & 0.2984 & 0.2824 & 0.2862 & 0.2915 & 0.2517 & 0.2763 \\ \hline \multirow{5}{*}{Weather} & 96 & 0.1630 & 0.2363 & 0.1519 & 0.2203 & 0.1656 & 0.2108 & 0.1594 & 0.2065 & 0.1706 & 0.2096 & 0.1598 & 0.2037 \\  & 192 & 0.2091 & 0.2832 & 0.1912 & 0.2601 & 0.2119 & 0.2512 & 0.2013 & 0.2444 & 0.2203 & 0.2544 & 0.2052 & 0.2466 \\  & 336 & 0.2634 & 0.3271 & 0.2375 & 0.2979 & 0.2679 & 0.2911 & 0.2541 & 0.2823 & 0.2762 & 0.2952 & 0.2526 & 0.2856 \\  & 720 & 0.3421 & 0.3868 & 0.2979 & 0.3463 & 0.3465 & 0.3412 & 0.315 & 0.3338 & 0.3551 & 0.3473 & 0.3252 & 0.3371 \\  & Avg & 0.2444 & 0.3084 & 0.2196 & 0.2815 & 0.2480 & 0.2736 & 0.2366 & 0.2667 & 0.2556 & 0.2766 & 0.2357 & 0.2682 \\ \hline \multirow{5}{*}{Weather} & 96 & 0.1514 & 0.2208 & 0.1466 & 0.2138 & 0.1522 & 0.1982 & 0.1471 & 0.1960 & 0.1650 & 0.2100 & 0.1521 & 0.2008 \\  & 192 & 0.1952 & 0.2660 & 0.1856 & 0.2530 & 0.1963 & 0.2383 & 0.1898 & 0.2360 & 0.2098 & 0.2510 & 0.2023 & 0.2450 \\  & 336 & 0.2492 & 0.3100 & 0.2315 & 0.2914 & 0.2502 & 0.2785 & 0.2933 & 0.2733 & 0.2625 & 0.2903 & 0.2428 & 0.2810 \\  & 720 & 0.3324 & 0.3771 & 0.2958 & 0.3436 & 0.3285 & 0.3313 & 0.3111 & 0.3242 & 0.3356 & 0.3410 & 0.3089 & 0.3291 \\  & Avg & 0.2321 & 0.2934 & 0.2149 & 0.2754 & 0.2318 & 0.2616 & 0.2218 & 0.2574 & 0.2432 & 0.2731 & 0.2265 & 0.2640 \\ \hline \hline \multirow{5}{*}{PEMS03} & 96 & 0.5159 & 0.5565 & 0.4340 & 0.5001 & 1.2342 & 0.8375 & 1.0040 & 0.7

[MISSING_PAGE_FAIL:23]

Figure 9: The 2D heatmaps of the SA-Matrix from the DLinear model finetuned with a 720 prediction length on the Weather dataset.

Figure 10: The kernel density estimate graphs of the SA-Matrix from the DLinear model finetuned with a 720 prediction length on the Weather dataset and FFT graphs of the data.

Figure 11: The kernel density estimate graphs of the SA-Matrix from the iTransformer model finetuned with a 720 prediction length on the Weather Data and FFT graphs of the data.

Figure 12: The kernel density estimate graphs of the SA-Matrix from the DLinear model finetuned with a 720 prediction length on the ETTh1 dataset and FFT graphs of the data.

Figure 13: The kernel density estimate graphs of the SA-Matrix from the DLinear model finetuned with a 720 prediction length on the ETTm1 dataset and FFT graphs of the data.

Figure 14: The kernel density estimate graphs of LUFL, MULL, OT, and LULL channels of the SA-Matrix from the iTransformer model with a 720 prediction length on the original and synthetic ETTh1 data. Row (a) represents the original data, while rows (b) - (d) display synthetic datasets. These datasets were generated by adding sine waves with periods of 100, 300, and 1000, respectively.

Figure 15: The kernel density estimate graphs of the SA-Matrix from the iTransformer model finetuned with a 720 prediction length on the ETTm1 dataset and FFT graphs of the data.

Figure 16: Illustration of the SWDR (W/m) channel of the Pre-Activation (left) and Post-Activation (right) through SA-Matrix from the DLinear model trained with a 720 prediction length on the weather dataset.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count toward the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes]" is generally preferable to "[No]", it is perfectly acceptable to answer "[No]" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No]" or "[NA]" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material provided in the appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer:[Yes] Justification: The paper's contributions and scope are well-explained throughout the abstract and introduction, and the entire paper is written to provide a consistent and comprehensive conclusion. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: Our paper discusses the limitations and future research directions to address them in the conclusion section 5. Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: Our paper proposes a new model architecture and demonstrates its superiority through a practical approach with extensive experiments rather than theoretical proofs. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our paper provides detailed information to reproduce all experiments. Additionally, the complete experiment code will be provided.

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: [https://github.com/DJLee1208/BSA_2024](https://github.com/DJLee1208/BSA_2024) Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ** At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Our paper provides detailed information about the experiments, including data splits, hyperparameters, how they were chosen, and the type of optimizer, in the main text and the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In the main results, we used a paired t-test to demonstrate the superiority of our methodology and reported the p-value. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We reported the types and number of GPUs used in the experiments, as well as the packages used for training.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Yes, the research conducted in the paper fully conforms to the NeurIPS Code of Ethics in every respect. We have ensured that all ethical guidelines and standards have been meticulously followed throughout the study. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Our paper discusses the potential broader impacts of our research in the conclusion. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards**Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: Our model does not pose such risks, and therefore, safeguards for responsible data or model release were not applicable. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

1. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Yes, we have properly credited the creators or original owners of assets used in the paper, including publicly available models and packages, with accurate citations. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.

1. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not include new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used.

* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer:[NA] Justification: This paper does not involve research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.