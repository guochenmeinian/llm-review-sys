# A graphon-signal analysis of graph neural networks

 Ron Levie

Faculty of Mathematics

Technion - Israel Institute of Technology

levieron@technion.ac.il

###### Abstract

We present an approach for analyzing message passing graph neural networks (MPNNs) based on an extension of graphon analysis to a so called graphon-signal analysis. A MPNN is a function that takes a graph and a signal on the graph (a graph-signal) and returns some value. Since the input space of MPNNs is non-Euclidean, i.e., graphs can be of any size and topology, properties such as generalization are less well understood for MPNNs than for Euclidean neural networks. We claim that one important missing ingredient in past work is a meaningful notion of graph-signal similarity measure, that endows the space of inputs to MPNNs with a regular structure. We present such a similarity measure, called the graphon-signal cut distance, which makes the space of all graph-signals a dense subset of a compact metric space - the graphon-signal space. Informally, two deterministic graph-signals are close in cut distance if they "look like" they were sampled from the same random graph-signal model. Hence, our cut distance is a natural notion of graph-signal similarity, which allows comparing any pair of graph-signals of any size and topology. We prove that MPNNs are Lipschitz continuous functions over the graphon-signal metric space. We then give two applications of this result: 1) a generalization bound for MPNNs, and, 2) the stability of MPNNs to subsampling of graph-signals. Our results apply to any regular enough MPNN on any distribution of graph-signals, making the analysis rather universal.

## 1 Introduction

In recent years, the need to accommodate non-regular structures in data science has brought a boom in machine learning methods on graphs. Graph deep learning (GDL) has already made a significant impact on the applied sciences and industry, with ground-breaking achievements in computational biology [2, 10, 17, 28], and a wide adoption as a general-purpose tool in social media, e-commerce, and online marketing platforms, among others. These achievements pose exciting theoretical challenges: can the success of GDL models be grounded in solid mathematical frameworks? Since the input space of a GDL model is non-Euclidean, i.e., graphs can be of any size and any topology, less is known about GDL than standard neural networks. We claim that contemporary theories of GDL are missing an important ingredient: meaningful notions of metric on the input space, namely, graph similarity measures that are defined for _all graphs of any size_, which respect and describe in some sense the behavior of GDL models. In this paper, we aim at providing an analysis of GDL by introducing such appropriate metrics, using _graphon theory_.

A graphon is an extension of the notion of a graph, where the node set is parameterized by a probability space instead of a finite set. Graphons can be seen as limit objects of graphs, as the number of nodes increases to infinity, under an appropriate metric. One result from graphon theory (that reformulates Szemeredi's regularity lemma from discrete mathematics) states that any sufficiently large graph behaves as if it was randomly sampled from a stochastic block model with a fixed number of classes.

This result poses an "upper bound" on the complexity of graphs: while deterministic large graphs may appear to be complex and intricate, they are actually approximately regular and behave random-like.

In this paper we extend this regularity result to an appropriate setting for message passing neural networks (MPNNs), a popular GDL model. Since MPNNs take as input a graph with a signal defined over the nodes (a graph-signal), we extend graphon theory from a theory of graphs to a theory of graph-signals. We define a metric, called the _graph-signal cut distance_ (see Figure 1 for illustration), and formalize regularity statements for MPNNs of the following sort.

(1) Any deterministic graph-signal behaves as if it was randomly sampled from a stochastic block model, where the number of blocks only depends on how much we want the graph-signal to look random-like, and not on the graph-signal itself.

(2) If two graph-signals behave as if they were sampled from the same stochastic block model, then any (regular enough) MPNN attains approximately the same value on both.

Formally, (1) is proven by extending Szemeredi's weak regularity lemma to graphon-signals. As a result of this new version of the regularity lemma, we show that the space of graph-signals is a dense subset of the space of graphon-signals, which is shown to be compact. Point (2) is formalized by proving that MPNNs with Lipschitz continuous message functions are Lipschitz continuous mappings from the space of graph-signals to an output space, in the graphon-signal cut distance.

We argue that the above regularity result is a powerful property of MPNNs. To illustrate this, we use the new regularity result to prove two corollaries. First, a generalization bound of MPNNs, showing that if the learned MPNN performs well on the training graph-signals, it is guaranteed to also perform well on test graph-signals. This is shown by first bounding the covering number of the graphon-signal space, and then using the Lipschitzness of MPNNs. Second, we prove that MPNNs are stable to graph-signal subsampling. This is done by first showing that randomly subsampling a graphon-signal produces a graph-signal which is close in cut distance to the graphon-signal, and then using the Lipschitzness of MPNNs.

As opposed to past works that analyze MPNNs using graphon analysis, we do not assume any generative model on the data. Our results apply to any regular enough MPNN on any distribution of graph-signals, making the analysis rather universal. We note that past works about generalization in GNNs [14; 23; 26; 30] consider special assumptions on the data distribution, and often on the MPNN model. Our work provides upper bounds under no assumptions on the data distribution, and only mild Lipschitz continuity assumptions on the message passing functions. Hence, our theory bounds the generalization error when all special assumptions (that are often simplistic) from other papers are not met. We show that when all assumptions fail, MPNNs still have generalization and sampling guarantees, albeit much slower ones. See Table 1. This is also true for past sampling theorems, e.g., [18; 22; 27; 31; 32].

The problem with graph-signal domains.Since the input space of MPNNs is non-Euclidean, results like universal approximation theorems and generalization bounds are less well developed for MPNNs than Euclidean deep learning models. For example, analysis like in [6] is limited to graphs of fixed sizes, seen as adjacency matrices. The graph metric induced by the Euclidean metric on

Figure 1: Illustration of the graph-signal cut distance. Left: a stochastic block model (SBM) with a signal. The color of the block represents the value of the signal at this block. The thickness of the edges between the blocks (including self-loops) represents the probability/density of edges between the blocks. Middle: a small graph-signal which looks like was sampled from the SMB. The color of the nodes represents the signal values. Right: a large graph-signal which looks like was sampled from the SMB. In graphon-signal cut distance, these two graph-signals are close to each other.

adjacency matrices is called _edit-distance_. This reduction of the graph problem to the Euclidean case does not describe the full complexity of the problem. Indeed, the edit-distance is defined for weighted graphs, and non-isomorphic simple graphs are always far apart in this metric. This is an unnatural description of the reality of machine learning on graphs, where different large non-isomorphic simple graphs can describe the same large-scale phenomenon and have similar outputs for the same MPNN.

Other papers that consider graphs of arbitrary but bounded size are based on taking the union of the Euclidean edit-distance spaces up to a certain graph size [3]. If one omits the assumption that all graphs are limited by a predefined size, the edit-metric becomes non-compact - a topology too fine to explain the behavior of real MPNNs. For example, two graphs with different number of nodes are always far apart in edit-distance, while most MPNN architectures in practice are not sensitive to the addition of one node to a large graph. In [19], the expressivity of GNNs is analyzed on spaces of graphons. It is assumed that graphons are Lipschitz continuous kernels. The metric on the graphon space is taken as the \(L_{\infty}\) distance between graphons as functions. We claim that the Lipschitz continuity of the graphons in [19], the choice of the \(L_{\infty}\) metric, and the choice of an arbitrary compact subset therein, are not justified as natural models for graphs, and are not grounded in theory. Note that graphon analysis is measure theoretic, and results like the regularity lemma are no longer true when requiring Lipschitz continuity for the graphons. Lastly, in papers like [18; 26; 27; 31], the data is assumed to be generated by one, or a few graphons, which limits the data distribution significantly. We claim that this discrepancy between theory and practice is an artifact of the inappropriate choices of the metric on the space of graphs, and the choice of a limiting generative model for graphs.

## 2 Background

For \(n\in\mathbb{N}\), we denote \([n]=\{1,\ldots,n\}\). We denote the Lebesgue \(p\) space over the measure space \(\mathcal{X}\) by \(\mathcal{L}^{p}(\mathcal{X})\), or, in short, \(\mathcal{L}^{p}\). We denote by \(\mu\) the standard Lebesgue measure on \([0,1]\). A _partition_ is a sequence \(\mathcal{P}_{k}=\{P_{1},\ldots,P_{k}\}\) of disjoint measurable subsets of \([0,1]\) such that \(\bigcup_{j=1}^{k}P_{j}=[0,1]\). The partition is called _equipartition_ if \(\mu(P_{i})=\mu(P_{j})\) for every \(i,j\in[k]\). We denote the indicator function of a set \(S\) by \(\mathds{1}_{S}\). See A for more details. We summarize our notations in A.

### Message passing neural networks

Most graph neural networks used in practice are special cases of MPNN (see [15] and [11] of a list of methods). MPNNs process graphs with node features, by repeatedly updating the feature at each node using the information from its neighbors. The information is sent between the different nodes along the edges of the graph, and hence, this process is called _message passing_. Each node merges all messages sent from its neighbors using an _aggregation scheme_, where typical choices is to sum, average or to take the coordinate-wise maximum of the messages. In this paper we focus on normalized sum aggregation (see A.1). For more details on MPNNs we refer the reader to A.

### Szemeredi weak regularity lemma

The following is taken from [13; 25]. Let \(G=\{V,E\}\) be a simple graph with nodes \(V\) and edges \(E\). For any two subsets \(U,S\subset V\), denote the number of edges with one end point at \(U\) and the other at \(S\) by \(e_{G}(U,S)\). Let \(\mathcal{P}=\{V_{1},\ldots,V_{k}\}\) be a partition of \(V\). The partition is called _equipartition_ if \(||V_{i}|-|V_{j}||\leq 1\) for every \(i,j\in[k]\). Given two node set \(U,S\subset V\), if the edges between each pair of classes \(V_{i}\) and \(V_{j}\) were random, we would expect the number of edges of \(G\) connecting \(U\) and \(S\) to be close to the expected value \(e_{\mathcal{P}(U,S)}:=\sum_{i=1}^{k}\sum_{j=1}^{k}\frac{e_{G}(V_{i},V_{j})}{|V _{i}||V_{j}|}\left|V_{i}\cap U\right||V_{j}\cap S|\). Hence, the _irregularity_, that measures how non-random like the edges between \(\{V_{j}\}_{j=1}^{k}\) are, is defined to be

\[\operatorname{irreg}_{G}(\mathcal{P})=\max_{U,S\subset V}\left|e_{G}(U,S)-e_{ \mathcal{P}}(U,S)\right|/\left|V\right|^{2}. \tag{1}\]

**Theorem 2.1** (Weak Regularity Lemma [13]).: _For every \(\epsilon>0\) and every graph \(G=(V,E)\), there is an equipartition \(\mathcal{P}=\{V_{1},\ldots,V_{k}\}\) of \(V\) into \(k\leq 2^{c/\epsilon^{2}}\) classes such that \(\operatorname{irreg}_{G}(\mathcal{P})\leq\epsilon\). Here, \(c\) is a universal constant that does not depend on \(G\) and \(\epsilon\)._Theorem 2.1 asserts that we can represent any large graph \(G\) by a smaller, coarse-grained version of it: the weighted graph \(G^{e}\) with node set \(V^{\epsilon}=\{V_{1},\ldots,V_{k}\}\), where the edge weight between the nodes \(V_{i}\) and \(V_{j}\) is \(\frac{e_{G}(V_{i},V_{j})}{|V_{1}|\cdots|V_{j}|}\). The "large-scale" structure of \(G\) is given by \(G^{\epsilon}\), and the number of edges between any two subsets of nodes \(U_{i}\subset V_{i}\) and \(U_{j}\subset V_{j}\) is close to the "expected value" \(e_{\mathcal{P}(U_{i},U_{j})}\). Hence, the deterministic graph \(G\) "behaves" as if it was randomly sampled from \(G^{\epsilon}\).

### Graphon analysis

A graphon [4, 24] can be seen as a weighted graph with a "continuous" node set, or more accurately, the nodes are parameterized by an atomless standard probability space called the _graphon domain_. Since all such graphon domains are equivalent to \([0,1]\) with the standard Lebesgue measure (up to a measure preserving bijection), we take \([0,1]\) as the node set. The space of graphons \(\mathcal{W}_{0}\) is defined to be the set of all measurable symmetric function \(W:[0,1]^{2}\rightarrow[0,1]\), \(W(x,y)=W(y,x)\). The edge weight \(W(x,y)\) of a graphon \(W\in\mathcal{W}_{0}\) can be seen as the probability of having an edge between the nodes \(x\) and \(y\).

Graphs can be seen as special graphons. Let \(\mathcal{I}_{m}=\{I_{1},\ldots,I_{m}\}\) be an _interval equipartition_: a partition of \([0,1]\) into intervals of equal length. The graph \(G=\{V,E\}\) with adjacency matrix \(A=\{a_{i,j}\}_{i,j=1}^{m}\)_induces_ the graph \(W_{G}\), defined by \(W_{G}(x,y)=a_{\lceil xm\rceil,\lceil ym\rceil}\)1. Note that \(W_{G}\) is piecewise constant on the partition \(\mathcal{I}_{m}\). We hence identify graphs with their induced graphons. A graphon can also be seen as a generative model of graphs. Given a graphon \(W\), a corresponding random graph is generated by sampling i.i.d. nodes \(\{X_{n}\}\) from he graphon domain, and connecting each pair \(X_{n},X_{m}\) in probability \(W(X_{n},X_{m})\) to obtain the edges of the graph.

Footnote 1: In the definition of \(W_{G}\), the convention is that \(\lceil 0\rceil=1\).

### Regularity lemma for graphons

A simple way to formulate the regularity lemma in the graphon language is via stochastic block models (SBM). A SBM is a piecewise constant graphon, defined on a partition of the graphon domain \([0,1]\). The _number of classes_ of the SBM is defined to be the number of sets in the partition. A SBM is seen as a generative model for graphs, where graphs are randomly sampled from the graphon underlying the SBM, as explained above. Szemeredi weak regularity lemma asserts that for any error tolerance \(\epsilon\), there is a number of classes \(k\), such that any deterministic graph (of any size and topology) behaves as if it was randomly sampled from a SBM with \(k\) classes, up to error \(\epsilon\). Hence, in some sense, every graph is approximately _quasi-random_.

To write the weak regularity lemma in the graphon language, the notion of irregularity (1) is extended to graphons. For any measurable \(W:[0,1]^{2}\rightarrow\mathbb{R}\) the _cut norm_ is defined to be

\[\|W\|_{\square}=\sup_{U,S\subset[0,1]}\left|\int_{U\times S}W(x,y)dxdy\right|,\]

where \(U,S\subset[0,1]\) are measurable. It can be verified that the irregularity (1) is equal to the cut norm of a difference between graphons induced by adequate graphs. The _cut metric_ between two graphons \(W,V\in\mathcal{W}_{0}\) is defined to be \(d_{\square}(W,V)=\|W-V\|_{\square}\). The _cut distance_ is defined to be

\[\delta_{\square}(W,V)=\inf_{\phi\in S_{[0,1]}}\|W-V^{\phi}\|_{\square},\]

where \(S_{[0,1]}\) is the space of measure preserving bijections \([0,1]\rightarrow[0,1]\), and \(V^{\phi}(x,y)=V(\phi(x),\phi(y))\) (see Section 3.1 and Appendix A.3 for more details). The cut distance is a pseudo metric on the space of graphons. By considering equivalence classes of graphons with zero cut distance, we can construct a metric space \(\widetilde{\mathcal{W}_{0}}\) for which \(\delta_{\square}\) is a metric. The following version of the weak regularity lemma is from [25, Lemma 7].

**Theorem 2.2**.: _For every graphon \(W\in\mathcal{W}_{0}\) and \(\epsilon>0\) there exists a step graphon \(W^{\prime}\in\mathcal{W}_{0}\) with respect to a partition of at most \(\lceil 2^{c/\epsilon^{2}}\rceil\) sets such that \(\delta_{\square}(W,W^{\prime})\leq\epsilon\), for some universal constant \(c\)._

The exact definition of a step graphon is given in Definition 3.3. It is possible to show, using Theorem 2.2, that \(\widetilde{\mathcal{W}_{0}}\) is a compact metric space [25, Lemma 8]. Instead of recalling this construction here, we refer to Section 3.4 for the extension of this construction to graphon-signals.

## 3 Graphon-signal analysis

A graph-signal \((G,\mathbf{f})\) is a graph \(G\), that may be weighted or simple, with node set \([n]\), and a signal \(\mathbf{f}\in\mathbb{R}^{n\times k}\) that assigns the value \(f_{j}\in\mathbb{R}^{k}\) for every node \(j\in[n]\). A graphon-signal will be defined in Section 3.1 similarly to a graph-signal, but over the node set \([0,1]\). In this section, we show how to extend classical results in graphon theory to a so called graphon-signal theory. All proofs are given in the appendix.

### The graphon signal space

For any \(r>0\), define the _signal space_

\[\mathcal{L}_{r}^{\infty}[0,1]:=\big{\{}f\in\mathcal{L}^{\infty}[0,1]\;\big{|}\; \forall x\in[0,1],\;\;|f(x)|\leq r\big{\}}. \tag{2}\]

We define the following "norm" on \(\mathcal{L}_{r}^{\infty}[0,1]\) (which is not a vector space).

**Definition 3.1** (Cut norm of a signal).: _For a signal \(f:[0,1]\to\mathbb{R}\), the cut norm\(\|f\|_{\square}\) is defined as_

\[\|f\|_{\square}:=\sup_{S\subseteq[0,1]}\bigg{|}\int_{S}f(x)d\mu(x)\bigg{|}, \tag{3}\]

_where the supremum is taken over the measurable subsets \(S\subset[0,1]\)._

In Appendix A.2 we prove basic properties of signal cut norm. One important property is the equivalence of the signal cut norm to the \(L_{1}\) norm

\[\forall f\in\mathcal{L}_{r}^{\infty}[0,1],\;\;\;\|f\|_{\square}\leq\|f\|_{1} \leq 2\|f\|_{\square}.\]

Given a bound \(r\) on the signals, we define the space of _graphon-signals_ to be the set of pairs \(\mathcal{WL}_{r}:=\mathcal{W}_{0}\times\mathcal{L}_{r}^{\infty}[0,1]\). We define the _graphon-signal cut norm_, for measurable \(W,V:[0,1]^{2}\to\mathbb{R}\) and \(f,g:[0,1]\to\mathbb{R}\), by

\[\|(W,f)\|_{\square}=\|W\|_{\square}+\|f\|_{\square}.\]

We define the _graphon-signal cut metric_ by \(d_{\square}\big{(}(W,f),(V,g)\big{)}=\|(W,f)-(V,g)\|_{\square}\).

We next define a pseudo metric that makes the space of graphon-signals a compact space. Let \(S^{\prime}_{[0,1]}\) be the set of measurable measure preserving bijections between co-null sets of \([0,1]\), namely,

\[S^{\prime}_{[0,1]}=\big{\{}\phi:A\to B\;\big{|}\;A,B\;\text{co-null in}\;[0,1], \;\;\text{and}\;\;\forall S\in A,\;\mu(S)=\mu(\phi(S))\big{\}},\]

where \(\phi\) is a measurable bijection and \(A,B,S\) are measurable. For \(\phi\in S^{\prime}_{[0,1]}\), we define \(W^{\phi}(x,y):=W(\phi(x),\phi(y))\), and \(f^{\phi}(z)=f(\phi(z))\). Note that \(W^{\phi}\) and \(f^{\phi}\) are only defined up to a null-set, and we arbitrarily set \(W,W^{\phi},f\) and \(f^{\phi}\) to \(0\) in their respective null-sets, which does not affect our analysis. Define the _cut distance_ between two graphon-signals \((W,f),(V,g)\in\mathcal{WL}_{r}\) by

\[\delta_{\square}\big{(}(W,f),(V,g)\big{)}=\inf_{\phi\in S^{\prime}_{[0,1]}}d_ {\square}\big{(}(W,f),(V,g)^{\phi}\big{)}. \tag{4}\]

Here, \((V,g)^{\phi}:=(V^{\phi},g^{\phi})\). More details on this construction are given in Appendix A.3.

The graphon-signal cut distance \(\delta_{\square}\) is a pseudo-metric, and can be made into a metric by introducing the equivalence relation: \((W,f)\sim(V,g)\) if \(\delta_{\square}((W,f),(V,g))=0\). The quotient space \(\widehat{\mathcal{WL}_{r}}:=\mathcal{WL}_{r}/\sim\) of equivalence classes \([(W,f)]\) of graphon-signals \((W,f)\) is a metric space with the metric \(\delta_{\square}([(W,f)],[(V,g)])=\delta_{\square}((W,f),(V,g))\). By abuse of terminology, we call elements of \(\widehat{\mathcal{WL}_{r}}\) also graphon-signals. A graphon-signal in \(\widehat{\mathcal{WL}_{r}}\) is defined irrespective of a specific "indexing" of the nodes in \([0,1]\).

### Induced graphon-signals

Any graph-signal can be identified with a corresponding graphon-signal as follows.

**Definition 3.2**.: _Let \((G,\mathbf{f})\) be a graph-signal with node set \([n]\) and adjacency matrix \(A=\{a_{i,j}\}_{i,j\in[n]}\). Let \(\{I_{k}\}_{k=1}^{n}\) with \(I_{k}=[(k-1)/n,k/n)\) be the equipartition of \([0,1]\) into \(n\) intervals. The graphon-signal \((W,f)_{(G,\mathbf{f})}=(W_{G},f_{\mathbf{f}})\) induced by \((G,\mathbf{f})\) is defined by_

\[W_{G}(x,y)=\sum_{i,j=1}^{n}a_{ij}\mathds{1}_{I_{i}}(x)\mathds{1}_{I_{j}}(y), \quad\text{ and}\quad f_{\mathbf{f}}(z)=\sum_{i}^{n}f_{i}\mathds{1}_{I_{i}}(z).\]We denote \((W,f)_{(G,\mathbf{f})}=(W_{G},f_{\mathbf{f}})\). We identify any graph-signal with its induced graphon-signal. This way, we define the cut distance between a graph-signal and a graphon-signal. As before, the cut distance between a graph-signal \((G,\mathbf{f})\) and a graphon-signal \((W,g)\) can be interpreted as how much the deterministic graph-signal \((G,\mathbf{f})\) "looks like" it was randomly sampled from \((W,g)\).

### Graphon-signal regularity lemma

To formulate our regularity lemma, we first define spaces of step functions.

**Definition 3.3**.: _Given a partition \(\mathcal{P}_{k}\), and \(d\in\mathbb{N}\), we define the space \(\mathcal{S}^{d}_{\mathcal{P}_{k}}\) of step functions of dimension \(d\) over the partition \(\mathcal{P}_{k}\) to be the space of functions \(F:[0,1]^{d}\to\mathbb{R}\) of the form_

\[F(x_{1},\ldots,x_{d})=\sum_{j=(j_{1},\ldots,j_{d})\in[k]^{d}}c_{j}\prod_{l=1}^{ d}\mathds{1}_{P_{j_{l}}}(x_{l}), \tag{5}\]

_for any choice of \(\{c_{j}\in\mathbb{R}\}_{j\in[k]^{d}}\)._

We call any element of \(\mathcal{W}_{0}\cap\mathcal{S}^{2}_{\mathcal{P}_{k}}\) a _step graphon_ with respect to \(\mathcal{P}_{k}\). A step graphon is also called a _stochastic block model (SBM)_. We call any element of \(\mathcal{L}^{\infty}_{r}[0,1]\cap\mathcal{S}^{1}_{\mathcal{P}_{k}}\) a _step signal_. We also call \([\mathcal{W}\mathcal{L}_{r}]_{\mathcal{P}_{k}}:=(\mathcal{W}_{0}\cap\mathcal{S }^{2}_{\mathcal{P}_{k}})\times(\mathcal{L}^{\infty}_{r}[0,1]\cap\mathcal{S}^{1 }_{\mathcal{P}_{k}})\) the space of SBMs with respect to \(\mathcal{P}_{k}\).

In Appendix B.2 we give a number of versions of the graphon-signal regularity lemma. Here, we show one version in which the partition is fixed regardless of the graphon-signal.

**Theorem 3.4** (Regularity lemma for graphon-signals - equipartition).: _For any \(c>1\), and any sufficiently small \(\epsilon>0\), for every \(n\geq 2^{\lceil\frac{\alpha}{4\epsilon^{2}}\rceil}\) and every \((W,f)\in\mathcal{WL}_{r}\), there exists a step graphon-signal \((W_{n},f_{n})\in[\mathcal{WL}_{r}]_{\mathcal{I}_{n}}\) such that_

\[\delta_{\square}\big{(}(W,f),(W_{n},f_{n})\big{)}\leq\epsilon, \tag{6}\]

_where \(\mathcal{I}_{n}\) is the equipartition of \([0,1]\) into \(n\) intervals._

Figure 2 illustrates the graphon-signal regularity lemma. By identifying graph-signals with their induced graphon-signals, (6) shows that the space of graph-signals is dense in the space of graphon-signals with cut distance.

Similarly to the classical case, Theorem 3.4 is interpreted as follows. While deterministic graph-signals may seem intricate and complex, they are actually regular, and "look like" random graph-signals that were sampled from SBMs, where the number of blocks of the SBM only depends on the desired approximation error between the SBM and the graph-signal, and not on the graph-signal itself.

**Remark 3.5**.: _The lower bound \(n\geq 2^{\lceil\frac{\alpha}{4\epsilon^{2}}\rceil}\) on the number of steps in the graphon-signal regularity lemma is essentially tight in the following sense. There is a universal constant \(C\) such that for every \(\epsilon>0\) there exists a graphon-signal \((W,f)\) such that no step graphon-signal \((W^{\prime},f^{\prime})\) with less than \(2^{\lceil\frac{C}{4\epsilon^{2}}\rceil}\) steps satisfies \(\delta_{\square}\big{(}(W,f),(W^{\prime},f^{\prime})\big{)}\leq\epsilon\). To see this, [8, Theorem 1.4, Theorem 7.1] shows that the bound in the standard weak regularity lemma (for graphs/graphons) is essentially tight in the above sense. For the graphon-signal case, we can take the graphon \(W^{\prime}\) from [8, Theorem 7.1] which does not allow a regularity partition with less than \(2^{\lceil\frac{C}{4\epsilon^{2}}\rceil}\) steps, and consider the graphon-signal \((W^{\prime},1)\), which then also does not allow such a regularity partition._

Figure 2: Illustration of the graphon-signal regularity lemma. The values of the graphon are in gray scale over \([0,1]^{2}\), and the signal is plotted in color on the diagonal of \([0,1]^{2}\). (a) A graphon-signal. (b) Representation of the same graphon-signal under the “good” permutation/measure preserving bijection guaranteed by the regularity lemma. (c) The approximating step graphon-signal guaranteed by the regularity lemma.

### Compactness of the graphon-signal space and its covering number

We prove that \(\widetilde{\mathcal{WL}_{r}}\) is compact using Theorem 3.4, similarly to [25, Lemma 8]. Moreover, we can bound the number of balls of radius \(\epsilon\) required to cover \(\widetilde{\mathcal{WL}_{r}}\).

**Theorem 3.6**.: _The metric space \((\widetilde{\mathcal{WL}_{r}},\delta_{\square})\) is compact. Moreover, given \(r>0\) and \(c>1\), for every sufficiently small \(\epsilon>0\), the space \(\widetilde{\mathcal{WL}_{r}}\) can be covered by_

\[\kappa(\epsilon)=2^{k^{2}} \tag{7}\]

_balls of radius \(\epsilon\), where \(k=\lceil 2^{\frac{\alpha_{\epsilon}}{k^{\alpha_{\epsilon}}}}\rceil\)._

The Proof of Theorem 3.6 is given in Appendix C. This is a powerful result - the space of arbitrarily large graph-signals is dense in the "small" space \(\widetilde{\mathcal{WL}_{r}}\). We will use this property in Section 4.3 to prove a generalization bound for MPNNs.

### Graphon-signal sampling lemmas

In this section we prove that randomly sampling a graphon signal produces a graph-signal that is close in cut distance to the graphon signal. Let us first describe the sampling setting. More details on the construction are given in Appendix D.1. Let \(\Lambda=(\lambda_{1},\ldots\lambda_{k})\in[0,1]^{k}\) be \(k\) independent uniform random samples from \([0,1]\), and \((W,f)\in\mathcal{WL}_{r}\). We define the _random weighted graph_\(W(\Lambda)\) as the weighted graph with \(k\) nodes and edge weight \(w_{i,j}=W(\lambda_{i},\lambda_{j})\) between node \(i\) and node \(j\). We similarly define the _random sampled signal_\(f(\Lambda)\) with value \(f_{i}=f(\lambda_{i})\) at each node \(i\). Note that \(W(\Lambda)\) and \(f(\Lambda)\) share the sample points \(\Lambda\). We then define a random simple graph as follows. We treat each \(w_{i,j}=W(\lambda_{i},\lambda_{j})\) as the parameter of a Bernoulli variable \(e_{i,j}\), where \(\mathbb{P}(e_{i,j}=1)=w_{i,j}\) and \(\mathbb{P}(e_{i,j}=0)=1-w_{i,j}\). We define the _random simple graph_\(\mathbb{G}(W,\Lambda)\) as the simple graph with an edge between each node \(i\) and node \(j\) if and only if \(e_{i,j}=1\).

We note that, given a graph signal \((G,\mathbf{f})\), sampling a graph-signal from \((W,f)_{(G,\mathbf{f})}\) is equivalent to subsampling the nodes of \(G\) independently and uniformly (with repetitions), and considering the resulting subgraph and subsignal. Hence, we can study the more general case of sampling a graphon-signal, where graph-signal sub-sampling is a special case. We now extend [24, Lemma 10.16], which bounds the cut distance between a graphon and its sampled graph, to the case of a sampled graph-signal.

**Theorem 3.7** (Sampling lemma for graphon-signals).: _Let \(r>1\). There exists a constant \(K_{0}>0\) that depends on \(r\), such that for every \(k\geq K_{0}\), every \((W,f)\in\mathcal{WL}_{r}\), and for \(\Lambda=(\lambda_{1},\ldots\lambda_{k})\in[0,1]^{k}\) independent uniform random samples from \([0,1]\), we have_

\[\mathbb{E}\bigg{(}\delta_{\square}\Big{(}\big{(}W,f\big{)},\big{(}W(\Lambda),f(\Lambda)\big{)}\Big{)}\bigg{)}<\frac{15}{\sqrt{\log(k)}},\]

_and_

\[\mathbb{E}\bigg{(}\delta_{\square}\Big{(}\big{(}W,f\big{)},\big{(}\mathbb{G} (W,\Lambda),f(\Lambda)\big{)}\Big{)}\bigg{)}<\frac{15}{\sqrt{\log(k)}}.\]

The proof of Theorem 3.7 is given in Appendix D.2

## 4 Graphon-signal analysis of MPNNs

In this section, we propose utilizing the compactness of the graphon-signal space under cut distance, and the sampling lemma, to prove regularity results for MPNNs, uniform generalization bounds, and stability to subsampling theorems.

### MPNNs on graphon signals

Next, we define MPNNs on graphon-signals, in such a way that the application of a MPNN on an induced graphon-signal is equivalent to applying the MPNN on the graph-signal and then inducing it. A similar construction was presented in [26], for average aggregation, but we use normalized sum aggregation.

At each layer, we define the message function \(\Phi(x,y)\) as a linear combination of simple tensors as follows. Let \(K\in\mathbb{N}\). For every \(k\in[K]\), let \(\xi^{k}_{t},\xi^{k}_{t}:\mathbb{R}^{d}\to\mathbb{R}^{p}\) be Lipschitz continuous functions that we call the _receiver_ and _transmitter message functions_ respectively. Define the _message function_\(\Phi:\mathbb{R}^{2d}\to\mathbb{R}^{p}\) by

\[\Phi(a,b)=\sum_{k=1}^{K}\xi^{k}_{r}(a)\xi^{k}_{t}(b),\]

where the multiplication is elementwise along the feature dimension. Given a signal \(f\), define the _message kernel_\(\Phi_{f}:[0,1]^{2}\to\mathbb{R}^{p}\) by

\[\Phi_{f}(x,y)=\Phi(f(x),f(y))=\sum_{k=1}^{K}\xi^{k}_{r}(f(x))\xi^{k}_{t}(f(y)).\]

We see the \(x\) variable of \(\Phi_{f}(x,y)\) as the receiver of the message, and \(y\) as the transmitter. Define the aggregation of a message kernel \(Q:[0,1]^{2}\to\mathbb{R}^{p}\), with respect to the graphon \(W\in\mathcal{W}_{0}\), to be the signal \(\mathrm{Agg}(W,Q)\in\mathcal{L}_{r}^{\infty}[0,1]\), defined by

\[\mathrm{Agg}(W,Q)(x)=\int_{0}^{1}W(x,y)Q(x,y)dy,\]

for an appropriate \(r>0\). A _message passing layer (MPL)_ takes the form \(f^{(t)}\mapsto\mathrm{Agg}(W,\Phi_{f^{(t)}}^{(t+1)})\), where \(f^{(t)}\) is the signal at layer \(t\). Each MPL is optionally followed by an _update layer_, which updates the signal pointwise via \(f^{(t+1)}=\mu^{(t+1)}\big{(}f^{(t)}(x),\mathrm{Agg}(W,\Phi_{f^{(t)}}^{(t+1)})( x)\big{)}\), where \(\mu^{(t+1)}\) is a learnable mapping called the _update function_. A MPNN is defined by choosing the number of layers \(T\), and defining message and update functions \(\{\mu^{t},(\xi^{k}_{r}),(\xi^{t}_{t}\xi^{k}_{t})\}_{k\in[K],t\in[T]}\). A MPNN only modifies the signal, and keeps the graph/graphon intact. We denote by \(\Theta_{t}(W,f)\) the output of the MPNN applied on \((W,f)\in\mathcal{WL}_{r}\) at layer \(t\in[T]\). More details on the construction are given in SectionE.1.

The above construction is rather general. Indeed, it is well known that many classes of functions \(F:\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}^{C}\) (e.g., \(L^{2}\) functions) can be approximated by (finite) linear combinations of simple tensors \(F(a,b)\approx\sum_{k=1}^{K}\xi^{k}_{1}(a)\xi^{k}_{2}(b)\). Hence, message passing based on general message functions \(\Phi:\mathbb{R}^{2d}\to\mathbb{R}^{p}\) can be approximated by our construction. Moreover, many well-known MPNNs can be written using our formulation with a small \(K\), e.g., [29; 36] and spectral convolutional networks [9; 20; 21], if we replace the aggregation in these method with normalized sum aggregation.

In SectionE.1 we show that for any graph-signal \((G,\mathbf{f})\), we have \(\Theta_{t}(W,f)_{(G,\mathbf{f})}=(W,f)_{\Theta_{t}(G,\mathbf{f})}\), where the MPNN on a graph-signal is defined with normalized sum aggregation

\[\big{(}\mathrm{Agg}(G,\Phi_{\mathbf{f}})\big{)}_{i}=\frac{1}{n}\sum_{j\in[n]} a_{i,j}(\Phi_{\mathbf{f}})_{i,j}.\]

Here, \(n\) is the number of nodes, and \(\{a_{i,j}\}_{i,j\in[n]}\) is the adjacency matrix of \(G\). Hence, we may identify graph-signals with their induced graphon-signals when analyzing MPNNs.

### Lipschitz continuity of MPNNs

We now show that, under the above construction, MPNNs are Lipschitz continuous with respect to cut distance.

**Theorem 4.1**.: _Let \(\Theta\) be a MPNN with \(T\) layers. Suppose that there exist constants \(L,B>0\) such that for every layer \(t\in[T]\), every \(y\in\{\mathrm{t},\mathrm{r}\}\) and every \(k\in[K]\),_

\[\big{|}\mu^{t}(0)\big{|}\,,\;\big{|}^{t}\xi^{k}_{y}(0)\big{|}\leq B,\quad\text {and}\quad L_{\mu^{t}},\;L_{\epsilon\xi^{k}_{y}}<L,\]

_where \(L_{\mu^{t}}\) and \(L_{\epsilon\xi^{k}_{y}}\) are the Lipschitz constants of \(\mu^{t}\) and \(\epsilon^{t\xi}_{y}\). Then, there exists a constant \(L_{\Theta}\) (that depends on \(T,K,B\) and \(L\)) such that for every \((W,f),(V,g)\in\mathcal{WL}_{r}\),_

\[\|\Theta(W,f)-\Theta(V,g)\|_{\square}\leq L_{\Theta}\Big{(}\|f-g\|_{\square}+ \|W-V\|_{\square}\Big{)}.\]

The constant \(L_{\Theta}\) depends exponentially on \(T\), and polynomially on \(K,B\) and \(L\). For formulas of \(L_{\Theta}\), under different assumptions on the hypothesis class of the MPNN, we refer to AppendixF.

### A generalization theorem for MPNN

In this section we prove a uniform generalization bound for MPNNs. For background on generalization analysis, we refer the reader to Appendix G.1. While uniform generalization bounds are considered a classical approach in standard neural networks, the approach is less developed in the case of MPNNs. For some works on generalization theorems of MPNNs, see [14; 23; 26; 30; 33].

When a MPNN is used for classification or regression, \(\Theta_{T}\) is followed by global pooling. Namely, for the output signal \(g:[0,1]\rightarrow\mathbb{R}^{p}\), we return \(\int g(x)dx\in\mathbb{R}^{p}\). This is then typically followed by a learnable mapping \(\mathbb{R}^{p}\rightarrow\mathbb{R}^{C}\). In our analysis, we see this mapping as part of the loss, which can hence be learnable. The combined loss is assumed to be Lipschitz continuous2.

Footnote 2: We note that loss functions like cross-entropy are not Lipschitz continuous. However, the composition of cross-entropy on softmax is Lipschitz, which is the standard way of using cross-entropy.

We model the ground truth classifier into \(C\) classes as a piecewise constant function \(\mathcal{C}:\widetilde{\mathcal{W}\mathcal{L}_{r}}\rightarrow\{0,1\}^{C}\), where the sets of different steps in \(\widetilde{\mathcal{W}\mathcal{L}_{r}}\) are Borel measurable sets, correspond to different classes. We consider an arbitrary probability Borel measure \(\nu\) on \(\widetilde{\mathcal{W}\mathcal{L}_{r}}\) as the data distribution. More details on the construction are given in Appendix G.2.

Let \(\operatorname{Lip}(\widetilde{\mathcal{W}\mathcal{L}_{r}},L_{1})\) be the space of Lipschitz continuous mappings \(\Upsilon:\widetilde{\mathcal{W}\mathcal{L}_{r}}\rightarrow\mathbb{R}^{C}\) with Lipschitz constant \(L_{1}\). By Theorem 4.1, we may assume that our hypothesis class of MPNNs is a subset of \(\operatorname{Lip}(\widetilde{\mathcal{W}\mathcal{L}_{r}},L_{1})\) for some given \(L_{1}\). Let \(\mathbf{X}=(X_{1},\ldots,X_{N})\) be independent random samples from the data distribution \((\widetilde{\mathcal{W}\mathcal{L}_{r}},\nu)\). Let \(\Upsilon_{\mathbf{X}}\) be a model that may depend on the sampled data, e.g., via training. Let \(\mathcal{E}\) be a Lipschitz continuous loss function3 with Lipschitz constant \(L_{2}\). For every function \(\Upsilon\) in the hypothesis class \(\operatorname{Lip}(\widetilde{\mathcal{W}\mathcal{L}_{r}},L_{1})\) (i.e. \(\Upsilon_{\mathbf{X}}\)), define the _statistical risk_

Footnote 3: The loss \(\mathcal{E}\) may have a learnable component (that depends on the dataset \(\mathbf{X}\)), as long as the total Lipschitz bound of \(\mathcal{E}\) is \(L_{2}\).

\[\mathcal{R}(\Upsilon)=\mathbb{E}\big{(}\mathcal{E}(\Upsilon,\mathcal{C})\big{)} =\int\mathcal{E}(\Upsilon(x),\mathcal{C}(x))d\nu(x).\]

We define the empirical risk \(\hat{\mathcal{R}}(\Upsilon_{\mathbf{X}},\mathbf{X})=\frac{1}{N}\sum_{i=1}^{N} \mathbb{E}\big{(}\Upsilon_{\mathbf{X}}(X_{i}),\mathcal{C}(X_{i})\big{)}\).

**Theorem 4.2** (MPNN generalization theorem).: _Consider the above classification setting, and let \(L=L_{1}L_{2}\). Let \(X_{1},\ldots,X_{N}\) be independent random samples from the data distribution \((\widetilde{\mathcal{W}\mathcal{L}_{r}},\nu)\). Then, for every \(p>0\), there exists an event \(\mathcal{U}^{p}\subset\widetilde{\mathcal{W}\mathcal{L}_{r}}^{N}\), with probability_

\[\nu^{N}(\mathcal{U}^{p})\geq 1-Cp-2\frac{C^{2}}{N},\]

_in which_

\[\Big{|}\mathcal{R}(\Upsilon_{\mathbf{X}})-\hat{\mathcal{R}}(\Upsilon_{ \mathbf{X}},\mathbf{X})\Big{|}\leq\xi^{-1}(N/2C)\Big{(}2L+\frac{1}{\sqrt{2}} \big{(}L+\mathcal{E}(0,0)\big{)}\big{(}1+\sqrt{\log(2/p)}\big{)}\Big{)}, \tag{8}\]

_where \(\xi(\epsilon)=\frac{\kappa(\epsilon)^{2}\log(\kappa(\epsilon))}{\epsilon^{2}}\), \(\kappa\) is the covering number of \(\widetilde{\mathcal{W}\mathcal{L}_{r}}\) given in (7), and \(\xi^{-1}\) is the inverse function of \(\xi\)._

The theorem is proved in Appendix G.4. Note that the term \(\xi^{-1}(N/2C)\) in (8) decreases to zero as the size of the training set \(N\) goes to infinity.

In Table 1 we compare the assumptions and dependency on the number of data points of different generalization theorems. All past works consider special assumptions. Our work provides upper bounds under no assumptions on the data distribution, and only mild assumptions on the MPNN (Lipschitz continuity of the message passing and update functions). In Table 2 in Appendix G.5 we present experiments that illustrate the generalization capabilities of MPNNs with normalized sum aggregation.

### Stability of MPNNs to graph-signal subsampling

When working with very large graphs, it is often the practice to subsample the large graph, and apply a MPNN to the smaller subsampled graph [5; 7; 16]. Here, we show that such an approach is justifiedtheoretically. Namely, any (Lipschitz continuous) MPNN has approximately the same outcome on the large graph and its subsampled version.

Transferability and stability analysis [18; 22; 27; 31; 32] often studies a related setting. Namely, it is shown that a MPNN applied on a randomly sampled graph \(G\) approximates the MPNN on the graph \(W\) from which the graph is sampled. However, previous analyses assumed that the generating graph \(W\) has metric properties. Namely, it is assumed that there is some probability metric space \(\mathcal{M}\) which is the graphon domain, and the graphon \(W:\mathcal{M}\times\mathcal{M}\rightarrow[0,1]\) is Lipschitz continuous with respect to \(\mathcal{M}\), where the dimension of \(\mathcal{M}\) affects the asymptotics. This is an unnatural setting, as general graphons are only assumed to be measurable, not continuous. Constraining the construction to Lipschitz continuous graphons with a uniformly bounded Lipschitz constant only accounts for a small subset of \(\mathcal{WL}_{r}\), and, hence, limits the analysis significantly. In comparison, our analysis applies to any graphon-signal in \(\mathcal{WL}_{r}\). When we only assume that the graphon is measurable, \([0,1]\) is only treated as a standard (atomless) probability space, which is very general, and equivalent for example to \([0,1]^{d}\) for any \(d\in\mathbb{N}\), and to any Polish space. Note that graphon theory allows restricting the graphon domain to \([0,1]\) since \([0,1]\), as a measure space, is very generic.

**Theorem 4.3**.: _Consider the setting of Theorem 4.2, and let \(\Theta\) be a MPNN with Lipschitz constant \(L\). Denote_

\[\Sigma=\big{(}W,\Theta(W,f)\big{)},\quad\text{and}\quad\Sigma(\Lambda)=\Big{(} \mathbb{G}(W,\Lambda),\Theta\big{(}\mathbb{G}(W,\Lambda),f(\Lambda)\big{)} \Big{)}.\]

_Then_

\[\mathbb{E}\Big{(}\delta_{\square}\big{(}\Sigma,\Sigma(\Lambda)\big{)}\Big{)}< \frac{15}{\sqrt{\log(k)}}L.\]

## 5 Discussion

We presented an extension of graphon theory to a graphon-signal theory. Especially, we extended well-known regularity, compactness, and sampling lemmas from graphons to graphon-signals. We then showed that the normalized sum aggregation of MPNNs is in some sense compatible with the graphon-signal cut distance, which leads to the Lipschitz continuity of MPNNs with respect to cut distance. This then allowed us to derive generalization and sampling theorems for MPNNs. The strength of our analysis is in its generality and simplicity- it is based on a natural notion of graph similarity, that allows studying the space of _all_ graph-signals, it applies to any graph-signal data distribution, and does not impose any restriction on the number of parameters of the MPNNs, only to their regularity through the Lipschitzness of the message functions.

The main limitation of the theory is the very slow asymptotics of the generalization and subsampling theorems. This follows the fact that the upper bound on the covering number of the compact space \(\widetilde{\mathcal{WL}_{r}}\) grows faster than the covering number of any finite-dimensional compact space. Yet, we believe that our work can serve as a point of departure for future works, that 1) will model subspaces of \(\widetilde{\mathcal{WL}_{r}}\) of lower complexity, which approximate the support of the data-distribution in real-life settings of graph machine learning, and, 2) will lead to improved asymptotics. Another open problem is to find an essentially tight estimate of the covering number of \(\widetilde{\mathcal{WL}_{r}}\), which may be lower than the estimate presented in this paper.

\begin{table}
\begin{tabular}{|l|c|c|c|} \hline
**Generalization analysis paper** & **Assumption on the graphs** & **No weight sharing** & **General MPL**. & **Dependencies on \(N\)** \\ \hline Generalization Limits of DNNs [14] & bounded degree & ✗ & ✗ & \(N^{-1/2}\) \\ \hline PAC-bayesian MPNN [23] & bounded degree & ✗ & ✗ & \(N^{-1/2}\) \\ \hline PAC-bayesian GCN [23] & bounded degree & ✓ & ✗ & \(N^{-1/2}\) \\ \hline VC meets IVL [30] & bounded color complexity & ✓ & ✗ & \(N^{-1/2}\) \\ \hline Generalization Analysis of MPNNs [26] & sampled from a small set of graphons & ✓ & ✓ & \(N^{-1/2}\) \\ \hline
**Our graphon-signal theory** & **non** & ✓ & ✓ & \(\xi^{-1}(N)\) \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of the assumptions made by different GNN generalization analysis papers.

## Acknowledgments

We thank Ningyuan (Teresa) Huang for providing the experiments of Table 2.

Ron Levie is partially funded by ISF (Israel Science Foundation) grant #1937/23: Analysis of graph deep learning using graphon theory.

## References

* Alon et al. [2003] N. Alon, W. de la Vega, R. Kannan, and M. Karpinski. Random sampling and approximation of max-csps. _Journal of Computer and System Sciences_, 67(2):212-243, 2003. ISSN 0022-0000. doi: [https://doi.org/10.1016/S0022-0000](https://doi.org/10.1016/S0022-0000)(03)00008-4. Special Issue on STOC 2002.
* Atz et al. [2021] K. Atz, F. Grisoni, and G. Schneider. Geometric deep learning on molecular representations. _Nature Machine Intelligence_, 3:1023-1032, 2021.
* Azizian and Lelarge [2021] W. Azizian and M. Lelarge. Expressive power of invariant and equivariant graph neural networks. In _ICLR_, 2021.
* Borgs et al. [2008] C. Borgs, J. T. Chayes, L. Lovasz, V. T. Sos, and K. Vesztergombi. Convergent sequences of dense graphs i: Subgraph frequencies, metric properties and testing. _Advances in Mathematics_, 219(6):1801-1851, 2008.
* Chen et al. [2018] J. Chen, T. Ma, and C. Xiao. FastGCN: Fast learning with graph convolutional networks via importance sampling. In _International Conference on Learning Representations_, 2018.
* Chen et al. [2019] Z. Chen, S. Villar, L. Chen, and J. Bruna. On the equivalence between graph isomorphism testing and function approximation with gnns. In _NeurIPS_. Curran Associates, Inc., 2019.
* Chiang et al. [2019] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, KDD '19, page 257-266, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450362016. doi: 10.1145/3292500.3330925.
* Conlon and Fox [2012] D. Conlon and J. Fox. Bounds for graph regularity and removal lemmas. _Geometric and Functional Analysis_, 22:1191-1256, 2012.
* Defferrard et al. [2016] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In _NeurIPS_. Curran Associates Inc., 2016. ISBN 9781510838819.
* 702.e13, 2020. ISSN 0092-8674.
* Fey and Lenssen [2019] M. Fey and J. E. Lenssen. Fast graph representation learning with PyTorch Geometric. In _ICLR 2019 Workshop on Representation Learning on Graphs and Manifolds_, 2019.
* Folland [1999] G. B. Folland. _Real analysis: modern techniques and their applications_, volume 40. John Wiley & Sons, 1999.
* Frieze and Kannan [1999] A. M. Frieze and R. Kannan. Quick approximation to matrices and applications. _Combinatorica_, 19:175-220, 1999.
* Garg et al. [2020] V. Garg, S. Jegelka, and T. Jaakkola. Generalization and representational limits of graph neural networks. In H. D. III and A. Singh, editors, _ICML_, volume 119 of _Proceedings of Machine Learning Research_, pages 3419-3430. PMLR, 13-18 Jul 2020.
* Gilmer et al. [2017] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In _International Conference on Machine Learning_, pages 1263-1272, 2017.
* Hamilton et al. [2017] W. L. Hamilton, R. Ying, and J. Leskovec. Inductive representation learning on large graphs. In _Advances in Neural Information Processing Systems_, page 1025-1035. Curran Associates Inc., 2017. ISBN 9781510860964.
* Jumper et al. [2019] J. M. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Zidek, A. Potapenko, A. Bridgland, C. Meyer, S. A. A. Kohl, A. Ballard, A. Cowie, B. Romera-Paredes, S. Nikolov, R. Jain, J. Adler, T. Back, S. Petersen, D. A. Reiman, E. Clancy, M. Zielinski, M. Steinegger, M. Pacholska, T. Berghammer, S. Bodenstein, D. Silver, O. Vinyals,A. W. Senior, K. Kavukcuoglu, P. Kohli, and D. Hassabis. Highly accurate protein structure prediction with alphafold. _Nature_, 596:583 - 589, 2021.
* Keriven et al. [2020] N. Keriven, A. Bietti, and S. Vaiter. Convergence and stability of graph convolutional networks on large random graphs. In _Advances in Neural Information Processing Systems_. Curran Associates, Inc., 2020.
* Keriven et al. [2021] N. Keriven, A. Bietti, and S. Vaiter. On the universality of graph neural networks on large random graphs. In _NeurIPS_. Curran Associates, Inc., 2021.
* Kipf and Welling [2017] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In _ICLR_, 2017.
* Levie et al. [2019] R. Levie, F. Monti, X. Bresson, and M. M. Bronstein. Cayleynets: Graph convolutional neural networks with complex rational spectral filters. _IEEE Transactions on Signal Processing_, 67(1):97-109, 2019. doi: 10.1109/TSP.2018.2879624.
* Levie et al. [2021] R. Levie, W. Huang, L. Bucci, M. Bronstein, and G. Kutyniok. Transferability of spectral graph convolutional neural networks. _Journal of Machine Learning Research_, 22(272):1-59, 2021.
* Liao et al. [2021] R. Liao, R. Urtasun, and R. Zemel. A PAC-bayesian approach to generalization bounds for graph neural networks. In _ICLR_, 2021.
* Lovasz [2012] L. M. Lovasz. Large networks and graph limits. In _volume 60 of Colloquium Publications_, 2012. doi: 10.1090/coll/060.
* Lovasz and Szegedy [2007] L. M. Lovasz and B. Szegedy. Szemeredi's lemma for the analyst. _GAFA Geometric And Functional Analysis_, 17:252-270, 2007.
* Maskey et al. [2022] S. Maskey, R. Levie, Y. Lee, and G. Kutyniok. Generalization analysis of message passing neural networks on large random graphs. In _NeurIPS_. Curran Associates, Inc., 2022.
* Maskey et al. [2023] S. Maskey, R. Levie, and G. Kutyniok. Transferability of graph neural networks: An extended graphon approach. _Applied and Computational Harmonic Analysis_, 63:48-83, 2023. ISSN 1063-5203. doi: [https://doi.org/10.1016/j.acha.2022.11.008](https://doi.org/10.1016/j.acha.2022.11.008).
* Mendez-Lucio et al. [2021] O. Mendez-Lucio, M. Ahmad, E. A. del Rio-Chanona, and J. K. Wegner. A geometric deep learning approach to predict binding conformations of bioactive molecules. _Nature Machine Intelligence_, 3:1033-1039, 2021.
* Morris et al. [2019] C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen, G. Rattan, and M. Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. _Proceedings of the AAAI Conference on Artificial Intelligence_, 33(01):4602-4609, Jul. 2019. doi: 10.1609/aaai.v33i01.33014602.
* Morris et al. [2021] C. Morris, F. Geerts, J. Tonshoff, and M. Grohe. Wl meet vc. In _ICML_. PMLR, 2023.
* Ruiz et al. [2021] L. Ruiz, L. F. O. Chamon, and A. Ribeiro. Graphon signal processing. _IEEE Transactions on Signal Processing_, 69:4961-4976, 2021.
* 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 5255-5259, 2021.
* Scarselli et al. [2018] F. Scarselli, A. C. Tsoi, and M. Hagenbuchner. The vapnik-chervonenkis dimension of graph and recursive neural networks. _Neural Networks_, 108:248-259, 2018.
* Shalev-Shwartz and Ben-David [2014] S. Shalev-Shwartz and S. Ben-David. _Understanding Machine Learning: From Theory to Algorithms_. Cambridge University Press, 2014. doi: 10.1017/CBO9781107298019.
* Williams [1991] D. Williams. _Probability with Martingales_. Cambridge University Press, 1991. doi: 10.1017/CBO9780511813658.
* Xu et al. [2019] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_, 2019.

Basic definitions and properties of graphon-signals

In this appendix, we give basic properties of graphon-signals, cut norm, and cut distance.

### Lebesgue spaces and signal spaces

For \(1\leq p<\infty\), the space \(\mathcal{L}^{p}[0,1]\) is the space of (equivalence classes up to null-set) of measurable functions \(f:[0,1]\to\mathbb{R}\), with finite \(L_{1}\) norm

\[\|f\|_{p}=\left(\int_{0}^{1}|f(x)|^{p}dx\right)^{1/p}<\infty.\]

The space \(\mathcal{L}^{\infty}[0,1]\) is the space of (equivalence classes) of measurable functions with finite \(L_{\infty}\) norm

\[\|f\|_{\infty}=\operatorname*{ess\ sup}_{x\in[0,1]}|f(x)|=\inf\{a\geq 0\mid|f(x) |\leq a\text{ for almost every }x\in[0,1]\}.\]

### Properties of cut norm

Every \(f\in\mathcal{L}^{\infty}_{r}[0,1]\) can be written as \(f=f_{+}-f_{-}\), where

\[f_{+}(x)=\left\{\begin{array}{cc}f(x)&f(x)>0\\ 0&f(x)\leq 0.\end{array}\right.\]

and \(f_{-}\) is defined similarly. It is easy to see that the supremum in (3) is attained for \(S\) which is either the support of \(f_{+}\) or \(f_{-}\), and

\[\|f\|_{\square}=\max\{\|f_{+}\|_{1},\|f_{-}\|_{1}\}.\]

As a result, the signal cut norm is equivalent to the \(L_{1}\) norm

\[\frac{1}{2}\|f\|_{1}\leq\|f\|_{\square}\leq\|f\|_{1}. \tag{9}\]

Moreover, for every \(r>0\) and measurable function \(W:[0,1]^{2}\to[-r,r]\),

\[0\leq\|W\|_{\square}\leq\|W\|_{1}\leq\|W\|_{2}\leq\|W\|_{\infty}\leq r.\]

The following lemma is from [24, Lemma 8.10].

**Lemma A.1**.: _For every measurable \(W:[0,1]^{2}\to\mathbb{R}\), the supremum_

\[\sup_{S,T\subset[0,1]}\left|\int_{S}\int_{T}W(x,y)dxdy\right|\]

_is attained for some \(S,T\)._

### Properties of cut distance and measure preserving bijections

Recall that we denote the standard Lebesgue measure of \([0,1]\) by \(\mu\). Let \(S_{[0,1]}\) be the space of measurable bijections \([0,1]\to[0,1]\) with measurable inverse, that are measure preserving, namely, for every measurable \(A\subset[0,1]\), \(\mu(A)=\mu(\phi(A))\). Recall that \(S^{\prime}_{[0,1]}\) is the space of measurable bijections between co-null sets of \([0,1]\).

For \(\phi\in S_{[0,1]}\) or \(\phi\in S^{\prime}_{[0,1]}\), we define \(W^{\phi}(x,y):=W(\phi(x),\phi(y))\). In case \(\phi\in S^{\prime}_{[0,1]}\), \(W^{\phi}\) is only define up to a null-set, and we arbitrarily set \(W\) to \(0\) in this null-set. This does not affect our analysis, as the cut norm is not affected by changes to the values of functions on a null sets. The _cut-metric_ between graphons is then defined to be

\[\delta_{\square}(W,W^{\phi}) =\inf_{\phi\in S_{[0,1]}}\ \|W-W^{\phi}\|_{\square}\] \[=\inf_{\phi\in S_{[0,1]}}\ \sup_{S,T\subseteq[0,1]}\bigg{|}\int_{S \times T}\big{(}W(x,y)-W(\phi(x),\phi(y))\big{)}dxdy\bigg{|}.\]

**Remark A.2**.: _Note that \(\delta_{\square}\) can be defined equivalently with respect to \(\phi\in S^{\prime}_{[0,1]}\). Indeed, By [24, Equation (8.17) and Theorem 8.13], \(\delta_{\square}\) can be defined equivalently with respect to the measure preserving maps that are not necessarily invertible. These include the extensions of mappings from \(S^{\prime}_{[0,1]}\) by defining \(\phi(x)=0\) for every \(x\) in the co-null set underlying \(\phi\)._

Similarly to the graphon case, the graphon-signal distance \(\delta_{\square}\) is a pseudo-metric. By introducing an equivalence relation \((W,f)\sim(V,g)\) if \(\delta_{\square}((W,f),(V,g))=0\), and the quotient space \(\widehat{\mathcal{WL}_{r}}:=\mathcal{WL}_{r}/\sim\), \(\widehat{\mathcal{WL}_{r}}\) is a metric space with a metric \(\delta_{\square}\) defined by \(\delta_{\square}([(W,f)],[V,g)])=d_{\square}(W,V)\) where \([(W,f)],[(V,g)]\), are the equivalence classes of \((W,f)\) and \((V,g)\) respectively. By abuse of terminology, we call elements of \(\widehat{\mathcal{WL}_{r}}\) also graphon-signals.

**Remark A.3**.: _We note that \(\widehat{\mathcal{WL}_{r}}\neq\widetilde{\mathcal{W}_{0}}\times\widehat{ \mathcal{L}_{r}^{\infty}[0,1]}\) (for the natural definition of \(\widehat{\mathcal{L}_{r}^{\infty}[0,1]}\)), since in \(\widehat{\mathcal{WL}_{r}}\) we require that the measure preserving bijection is shared between the graphon \(W\) and the signal \(f\). Sharing the measure preserving bijection between \(W\) and \(f\) is an important modelling requirement, as \(\phi\) is seen as a "re-indexing" of the node set \([0,1]\). When re-indexing a node \(x\), both the neighborhood \(W(x,\cdot)\) of \(x\) and the signal value \(f(x)\) at \(x\) should change together, otherwise, the graphon and the signal would fall out of alignment._

We identify graphs with their induced graphons and signal with their induced signals

## Appendix B Graphon-signal regularity lemmas

In this appendix, we prove a number of versions of the graphon-signal regularity lemma, where Theorem 3.4 is one version.

### Properties of partitions and step functions

Given a partition \(\mathcal{P}_{k}\) and \(d\in\mathbb{N}\), the next lemma shows that there is an equipartition \(\mathcal{E}_{n}\) such that the space \(\mathcal{S}^{d}_{\mathcal{E}_{n}}\) uniformly approximates the space \(\mathcal{S}^{d}_{\mathcal{P}_{k}}\) in \(\mathcal{L}^{1}[0,1]^{d}\) norm (see Definition 3.3).

**Lemma B.1** (Equitizing partitions).: _Let \(\mathcal{P}_{k}\) be a partition of \([0,1]\) into \(k\) sets (generally not of the same measure). Then, for any \(n>k\) there exists an equipartition \(\mathcal{E}_{n}\) of \([0,1]\) into \(n\) sets such that any function \(F\in\mathcal{S}^{d}_{\mathcal{P}_{k}}\) can be approximated in \(L_{1}[0,1]^{d}\) by a function from \(F\in\mathcal{S}^{d}_{\mathcal{E}_{n}}\) up to small error. Namely, for every \(F\in\mathcal{S}^{d}_{\mathcal{P}_{k}}\) there exists \(F^{\prime}\in\mathcal{S}^{d}_{\mathcal{E}_{n}}\) such that_

\[\|F-F^{\prime}\|_{1}\leq d\|F\|_{\infty}\frac{k}{n}.\]

Proof.: Let \(\mathcal{P}_{k}=\{P_{1},\ldots,P_{k}\}\) be a partition of \([0,1]\). For each \(i\), we divide \(P_{i}\) into subsets \(\mathbf{P}_{i}=\{P_{i,1},\ldots,P_{i,m_{i}}\}\) of measure \(1/n\) (up to the last set) with a residual, as follows. If \(\mu(P_{i})<1/n\), we choose \(\mathbf{P}_{i}=\{P_{i,1}=P_{i}\}\). Otherwise, we take \(P_{i,1},\ldots,P_{i,m_{i}-1}\) of measure \(1/n\), and \(\mu(P_{i,m_{i}})\leq 1/n\). We call \(P_{i,m_{i}}\) the remainder.

We now define the sequence of sets of measure \(1/n\)

\[\mathcal{Q}:=\{P_{1,1},\ldots,P_{1,m_{1}-1},P_{2,1},\ldots,P_{2,m_{2}-1}, \ldots,P_{k,1},\ldots,P_{k,m_{k}-1}\}, \tag{10}\]

where, by abuse of notation, for any \(i\) such that \(m_{i}=1\), we set \(\{P_{i,1},\ldots,P_{i,m_{i}-1}\}=\emptyset\) in the above formula. Note that in general \(\cup\mathcal{Q}\neq[0,1]\). We moreover define the union of residuals \(\Pi:=P_{1,m_{1}}\cup P_{2,m_{2}}\cup\cdots\cup P_{k,m_{k}}\). Note that \(\mu(\Pi)=1-\mu(\cup\mathcal{Q})=1-k\frac{1}{n}=h/n\), where \(k\) is the number of elements in \(\mathcal{Q}\), and \(h=n-k\). Hence, we can partition \(\Pi\) into \(h\) parts \(\{\Pi_{1},\ldots\Pi_{h}\}\) of measure \(1/n\) with no residual. Thus we have obtain the equipartition of \([0,1]\) to \(n\) sets of measure \(1/n\)

\[\mathcal{E}_{n}:=\{P_{1,1},\ldots,P_{1,m_{1}-1},P_{2,1},\ldots,P_{2,m_{2}-1}, \ldots,S_{k,1},\ldots,S_{k,m_{k}-1},\Pi_{1},\Pi_{2},\ldots,\Pi_{h}\}. \tag{11}\]

For convenience, we also denote \(\mathcal{E}_{n}=\{Z_{1},\ldots,Z_{n}\}\).

Let

\[F(x)=\sum_{j=(j_{1},\ldots,j_{d})\in[k]^{d}}c_{j}\prod_{l=1}^{d}\mathds{1}_{P_{ j_{l}}}(x_{l})\in\mathcal{S}^{d}_{\mathcal{P}_{k}}.\]We can write \(F\) with respect to the equipartition \(\mathcal{E}_{n}\) as

\[F(x)=\sum_{j=(j_{1},\ldots,j_{d})\in[n]^{d};\ \forall l=1,\ldots,d,\ Z_{j_{l}} \not\subset\Pi}\tilde{c}_{j}\prod_{l=1}^{d}\mathds{1}_{Z_{j_{l}}}(x_{l})\ +\ E(x),\]

for some \(\{\tilde{c}_{j}\}\) with the same values as the values of \(\{c_{j}\}\). Here, \(E\) is supported in the set \(\Pi^{(d)}\subset[0,1]^{d}\), defied by

\[\Pi^{(d)}=\big{(}\Pi\times[0,1]^{d-1}\big{)}\cup\big{(}[0,1]\times\Pi\times[0, 1]^{d-2}\big{)}\cup\ldots\cup\big{(}[0,1]^{d-1}\times\Pi\big{)}.\]

Consider the step function

\[F^{\prime}(x)=\sum_{j=(j_{1},\ldots,j_{d})\in[n]^{d};\ \forall l=1,\ldots,d,\ Z_{j_{l}}\not\subset\Pi}\tilde{c}_{j}\prod_{l=1}^{d} \mathds{1}_{Z_{j_{l}}}(x_{l})\in\mathcal{S}^{d}_{\mathcal{E}_{n}}.\]

Since \(\mu(\Pi)=k/n\), we have \(\mu(\Pi^{(d)})=dk/n\), and so

\[\|F-F^{\prime}\|_{1}\leq d\|F\|_{\infty}\frac{k}{n}.\]

**Lemma B.2**.: _Let \(\{Q_{1},Q_{2},\ldots,Q_{m}\}\) partition of \([0,1]\). Let \(\{I_{1},I_{2},\ldots,I_{m}\}\) be a partition of \([0,1]\) into intervals, such that for every \(j\in[m]\), \(\mu(Q_{j})=\mu(I_{j})\). Then, there exists a measure preserving bijection \(\phi:[0,1]\to[0,1]\in S^{1}_{[0,1]}\) such that4_

Footnote 4: Namely, there is a measure preserving bijection \(\phi\) between two co-null sets \(C_{1}\) and \(C_{2}\) of \([0,1]\), such that \(\phi(Q_{j}\cap C_{1})=I_{j}\cap C_{2}\).

\[\phi(Q_{j})=I_{j}\]

Proof.: By the definition of a standard probability space, the measure space induced by \([0,1]\) on a non-null subset \(Q_{j}\subseteq[0,1]\) is a standard probability space. Moreover, each \(Q_{j}\) is atomless, since \([0,1]\) is atomless. Since there is a measure-preserving bijection (up to null-set) between any two atomless standard probability spaces, we obtain the result. 

**Lemma B.3**.: _Let \(\mathcal{S}=\{S_{j}\subset[0,1]\}_{j=0}^{m-1}\) be a collection of measurable sets (that are not disjoint in general), and \(d\in\mathbb{N}\). Let \(\mathcal{C}^{d}_{\mathcal{S}}\) be the space of functions \(F:[0,1]^{d}\to\mathbb{R}\) of the form_

\[F(x)=\sum_{j=(j_{1},\ldots,j_{d})\in[m]^{d}}c_{j}\prod_{l=1}^{d}\mathds{1}_{S_ {j_{l}}}(x_{l}),\]

_for some choice of \(\{c_{j}\in\mathbb{R}\}_{j\in[m]^{d}}\). Then, there exists a partition \(\mathcal{P}_{k}=\{P_{1},\ldots,P_{k}\}\) into \(k=2^{m}\) sets, that depends only on \(\mathcal{S}\), such that_

\[\mathcal{C}^{d}_{\mathcal{S}}\subset\mathcal{S}^{d}_{\mathcal{P}_{k}}.\]

Proof.: The partition \(\mathcal{P}_{k}=\{P_{1},\ldots,P_{k}\}\) is defined as follows. Let

\[\tilde{\mathcal{P}}=\big{\{}P\subset[0,1]\mid\exists\,x\in[0,1],\ P=\cap\{S_{ j}\in\mathcal{S}|x\in S_{j}\}\big{\}}.\]

We must have \(|\tilde{\mathcal{P}}|\leq 2^{m}\). Indeed, there are at most \(2^{m}\) different subsets of \(\mathcal{S}\) for the intersections. We endow an arbitrarily order to \(\tilde{\mathcal{P}}\) and turn it into a sequence. If the size of \(\tilde{\mathcal{P}}\) is strictly smaller than \(2^{m}\), we add enough copies of \(\{\emptyset\}\) to \(\tilde{\mathcal{P}}\) to make the size of the sequence \(2^{m}\), that we denote by \(\mathcal{P}_{k}\), where \(k=2^{m}\). 

The following simple lemma is proved similarly to Lemma B.3. We give it without proof.

**Lemma B.4**.: _Let \(\mathcal{P}_{k}=\{P_{1},\ldots,P_{k}\},\mathcal{Q}_{m}=\{Q_{1},\ldots,Q_{k}\}\) be two partitions. Then, there exists a partition \(\mathcal{Z}_{km}\) into \(km\) sets such that for every \(d\),_

\[\mathcal{S}^{d}_{\mathcal{P}_{k}}\subset\mathcal{S}^{d}_{\mathcal{Z}_{mk}}, \quad\text{and}\quad\mathcal{S}^{d}_{\mathcal{Q}_{m}}\subset\mathcal{S}^{d}_{ \mathcal{Z}_{mk}}.\]

### List of graphon-signal regularity lemmas

The following lemma from [25, Lemma 4.1] is a tool in the proof of the weak regularity lemma.

**Lemma B.5**.: _Let \(\mathcal{K}_{1},\mathcal{K}_{2},\ldots\) be arbitrary nonempty subsets (not necessarily subspaces) of a Hilbert space \(\mathcal{H}\). Then, for every \(\epsilon>0\) and \(v\in\mathcal{H}\) there is \(m\leq\lceil 1/\epsilon^{2}\rceil\) and \(v_{i}\in\mathcal{K}_{i}\) and \(\gamma_{i}\in\mathbb{R}\), \(i\in[m]\), such that for every \(w\in\mathcal{K}_{m+1}\)_

\[\bigg{|}\left\langle w,v-(\sum_{i=1}^{m}\gamma_{i}v_{i})\right\rangle\bigg{|} \leq\epsilon\;\|w\|\|v\|. \tag{12}\]

The following theorem is an extension of the graphon regularity lemma from [25] to the case of graphon-signals. Much of the proof follows the steps of [25].

**Theorem B.6** (Weak regularity lemma for graphon-signals).: _Let \(\epsilon,\rho>0\). For every \((W,f)\in\mathcal{WL}_{r}\) there exists a partition \(\mathcal{P}_{k}\) of \([0,1]\) into \(k=\lceil r/\rho\rceil\big{(}2^{2\lceil 1/\epsilon^{2}\rceil}\big{)}\) sets, a step function graphon \(W_{k}\in\mathcal{S}_{\mathcal{P}_{k}}^{2}\cap\mathcal{W}_{0}\) and a step function signal \(f_{k}\in\mathcal{S}_{\mathcal{P}_{k}}^{1}\cap\mathcal{L}_{r}^{\infty}[0,1]\), such that_

\[\|W-W_{k}\|_{\square}\leq\epsilon\quad\text{and}\;\;\|f-f_{k}\|_{ \square}\leq\rho. \tag{13}\]

Proof.: We first analyze the graphon part. In Lemma B.5, set \(\mathcal{H}=\mathcal{L}^{2}([0,1]^{2})\) and for all \(i\in\mathbb{N}\), set

\[\mathcal{K}_{i}=\mathcal{K}=\big{\{}\mathds{1}_{S\times T}\;\big{|}\;S,T\subset [0,1]\;\text{measurable}\big{\}}.\]

Then, by Lemma B.5, there exists \(m\leq\lceil 1/\epsilon^{2}\rceil\) two sequences of sets \(\mathcal{S}_{m}=\{S_{i}\}_{i=1}^{m}\), \(\mathcal{T}_{m}=\{T_{i}\}_{i=1}^{m}\), a sequence of coefficients \(\{\gamma_{i}\in\mathbb{R}\}_{i=1}^{m}\), and

\[W_{\epsilon}^{\prime}=\sum_{i=1}^{m}\gamma_{i}\mathds{1}_{S_{i} \times T_{i}},\]

such that for any \(V\in\mathcal{K}\), given by \(V(x,y)=\mathds{1}_{S}(x)\mathds{1}_{T}(y)\), we have

\[\bigg{|}\int V(x,y)\big{(}W(x,y)-W_{\epsilon}^{\prime}(x,y)\big{)} dxdy\bigg{|} =\bigg{|}\int_{S}\int_{T}\big{(}W(x,y)-W_{\epsilon}^{\prime}(x,y) \big{)}dxdy\bigg{|} \tag{14}\] \[\leq\epsilon\|\mathds{1}_{S\times T}\|\|W\|\leq\epsilon. \tag{15}\]

We may choose exactly \(m=\lceil 1/\epsilon^{2}\rceil\) by adding copies of the empty set to \(\mathcal{S}_{m}\) and \(\mathcal{T}_{m}\), if the constant \(m\) guaranteed by Lemma B.5 is strictly less than \(\lceil 1/\epsilon^{2}\rceil\). Let \(W_{\epsilon}(x,y)=(W_{\epsilon}^{\prime}(x,y)+W_{\epsilon}^{\prime}(y,x))/2\). By the symmetry of \(W\), it is easy to see that (15) is also true when replacing \(W_{\epsilon}^{\prime}\) by \(W_{\epsilon}\). Indeed,

\[\bigg{|}\int V(x,y)\big{(}W(x,y)-W_{\epsilon}(x,y)\big{)}dxdy \bigg{|}\] \[\leq 1/2\bigg{|}\int V(x,y)\big{(}W(x,y)-W_{\epsilon}^{\prime}(x,y )\big{)}dxdy\bigg{|}+1/2\bigg{|}\int V(y,x)\big{(}W(x,y)-W_{\epsilon}^{\prime }(x,y)\big{)}dxdy\bigg{|}\] \[\leq\epsilon.\]

Consider the concatenation of the two sequences \(\mathcal{T}_{m},\mathcal{S}_{m}\) given by \(\mathcal{Y}_{2m}=\mathcal{T}_{m}\cup\mathcal{S}_{m}\). Note that in the notation of Lemma B.3, \(W_{\epsilon}\in\mathcal{C}_{\mathcal{Y}_{2m}}^{2}\). Hence, by Lemma B.3, there exists a partition \(\mathcal{Q}_{n}\) into \(n=2^{2m}=2^{2\lceil\frac{1}{\epsilon^{2}}\rceil}\) sets, such that \(W_{\epsilon}\) is a step graphon with respect to \(\mathcal{Q}_{n}\).

To analyze the signal part, we partition the range of the signal \([-r,r]\) into \(j=\lceil r/\rho\rceil\) intervals \(\{J_{i}\}_{i=1}^{j}\) of length less or equal to \(2\rho\), where the left edge point of each \(J_{i}\) is \(-r+(i-1)\frac{\rho}{r}\). Consider the partition of \([0,1]\) based on the preimages \(\mathcal{Y}_{j}=\{Y_{i}=f^{-1}(J_{i})\}_{i=1}^{j}\). It is easy to see that for the step signal

\[f_{\rho}(x)=\sum_{i=1}^{j}a_{i}\mathds{1}_{Y_{i}}(x),\]

where \(a_{i}\) the midpoint of the interval \(Y_{i}\), we have

\[\|f-f_{\rho}\|_{\square}\leq\|f-f_{\rho}\|_{1}\leq\rho.\]Lastly, by Lemma B.4, there is a partition \(\mathcal{P}_{k}\) of \([0,1]\) into \(k=\lceil r/\rho\rceil\Big{(}2^{2\lceil 1/\epsilon^{2}\rceil}\Big{)}\) sets such that \(W_{\epsilon}\in\mathcal{S}^{2}_{\mathcal{P}_{k}}\) and \(f_{\rho}\in\mathcal{S}^{1}_{\mathcal{P}_{k}}\).

**Corollary B.7** (Weak regularity lemma for graphon-signals - version 2).: _Let \(r>0\) and \(c>1\). For every sufficiently small \(\epsilon>0\) (namely, \(\epsilon\) that satisfies (17)), and for every \((W,f)\in\mathcal{WL}_{r}\) there exists a partition \(\mathcal{P}_{k}\) of \([0,1]\) into \(k=\Big{(}2^{\lceil 2c/\epsilon^{2}\rceil}\Big{)}\) sets, a step graphon \(W_{k}\in\mathcal{S}^{2}_{\mathcal{P}_{k}}\cap\mathcal{W}_{0}\) and a step signal \(f_{k}\in\mathcal{S}^{1}_{\mathcal{P}_{k}}\cap\mathcal{L}^{\infty}_{r}[0,1]\), such that_

\[d_{\square}\big{(}(W,f),(W_{k},f_{k})\big{)}\leq\epsilon.\]

Proof.: First, evoke Theorem B.6, with errors \(\|W-W_{k}\|_{\square}\leq\nu\) and \(\|f-f_{k}\|_{\square}\leq\rho=\epsilon-\nu\). We now show that there is some \(\epsilon_{0}>0\) such that for every \(\epsilon<\epsilon_{0}\), there is a choice of \(\nu\) such that the number of sets in the partition, guaranteed by Theorem B.6, satisfies

\[k(\nu):=\lceil r/(\epsilon-\nu)\rceil\Big{(}2^{2\lceil 1/\nu^{2}\rceil}\Big{)} \leq 2^{\lceil 2c/\epsilon^{2}\rceil}.\]

Denote \(c=1+t\). In case

\[\nu\geq\sqrt{\frac{2}{2(1+0.5t)/\epsilon^{2}-1}}, \tag{16}\]

we have

\[2^{2\lceil 1/\nu^{2}\rceil}\leq 2^{2(1+0.5t)/\epsilon^{2}}.\]

On the other hand, for

\[\nu\leq\epsilon-\frac{r}{2^{t/\epsilon^{2}}-1},\]

we have

\[\lceil r/(\epsilon-\nu)\rceil\leq 2^{2(0.5t)/\epsilon^{2}}.\]

The reconcile these two conditions, we restrict to \(\epsilon\) such that

\[\epsilon-\frac{r}{2^{t/\epsilon^{2}}-1}\geq\sqrt{\frac{2}{2(1+0.5t)/\epsilon^ {2}-1}}. \tag{17}\]

There exists \(\epsilon_{0}\) that depends on \(c\) and \(r\) (and hence also on \(t\)) such that for every \(\epsilon<\epsilon_{0}\) (17) is satisfied. Indeed, for small enough \(\epsilon\),

\[\frac{1}{2^{t/\epsilon^{2}}-1}=\frac{2^{-t/\epsilon^{2}}}{1-2^{-t/\epsilon^{ 2}}}<2^{-t/\epsilon^{2}}<\frac{\epsilon}{r}\Big{(}1-\frac{1}{1+0.1t}\Big{)},\]

so

\[\epsilon-\frac{r}{2^{t/\epsilon^{2}}-1}>\epsilon(1+0.1t).\]

Moreover, for small enough \(\epsilon\),

\[\sqrt{\frac{2}{2(1+0.5t)/\epsilon^{2}-1}}=\epsilon\sqrt{\frac{1}{(1+0.5t)- \epsilon^{2}}}<\epsilon/(1+0.4t).\]

Hence, for every \(\epsilon<\epsilon_{0}\), there is a choice of \(\nu\) such that

\[k(\nu)=\lceil r/(\epsilon-\nu)\rceil\Big{(}2^{\lceil 1/\nu^{2}\rceil}\Big{)} \leq 2^{2(0.5t)/\epsilon^{2}}2^{2(1+0.5t)/\epsilon^{2}}\leq 2^{\lceil 2c/ \epsilon^{2}\rceil}.\]

Lastly, we add as many copies of \(\emptyset\) to \(\mathcal{P}_{k(\nu)}\) as needed so that we get a sequence of \(k=2^{\lceil 2c/\epsilon^{2}\rceil}\) sets.

**Theorem B.8** (Regularity lemma for graphon-signals - equipartition version).: _Let \(c>1\) and \(r>0\). For any sufficiently small \(\epsilon>0\), and every \((W,f)\in\mathcal{WL}_{r}\) there exists \(\phi\in S^{\prime}_{[0,1]}\), a step function graphon \([W^{\phi}]_{n}\in\mathcal{S}^{2}_{\mathcal{L}_{n}}\cap\mathcal{W}_{0}\) and a step signal \([f^{\phi}]_{n}\in\mathcal{S}^{1}_{\mathcal{L}_{n}}\cap\mathcal{L}^{\infty}_{r} [0,1]\), such that_

\[d_{\square}\Big{(}\;(W^{\phi},f^{\phi})\;,\;\big{(}[W^{\phi}]_{n},[f^{\phi}]_{n }\big{)}\;\Big{)}\leq\epsilon, \tag{18}\]

_where \(\mathcal{I}_{n}\) is the equipartition of \([0,1]\) into \(n=2^{[2c/\epsilon^{2}]}\) intervals._

Proof.: Let \(c=1+t>1\), \(\epsilon>0\) and \(0<\alpha,\beta<1\). In Corollary B.7, consider the approximation error

\[d_{\square}\big{(}(W,f),(W_{k},f_{k})\big{)}\leq\alpha\epsilon.\]

with a partition \(\mathcal{P}_{k}\) into \(k=2^{\lceil\frac{2(1+t/2)}{(\alpha\alpha)^{2}}\rceil}\) sets. We next equatize the partition \(\mathcal{P}_{k}\) up to error \(\epsilon\beta\). More accurately, in Lemma B.1, we choose

\[n=\;\lceil 2^{\frac{2(1+0.5\epsilon)}{(\alpha\alpha)^{2}}+1}/(\epsilon\beta )\rceil,\]

and note that

\[n\geq 2^{\lceil\frac{2(1+0.5\epsilon)}{(\alpha\alpha)^{2}}\rceil}\lceil 1/ \epsilon\beta\rceil=k\lceil 1/\epsilon\beta\rceil.\]

By Lemma B.1 and by the fact that the cut norm is bounded by \(L_{1}\) norm, there exists an equipartition \(\mathcal{E}_{n}\) into \(n\) sets, and step functions \(W_{n}\) and \(f_{n}\) with respect to \(\mathcal{E}_{n}\) such that

\[\|W_{k}-W_{n}\|_{\square}\leq 2\epsilon\beta\quad\text{and}\quad\|f_{k}-f_{n} \|_{1}\leq r\epsilon\beta.\]

Hence, by the triangle inequality,

\[d_{\square}\big{(}(W,f),(W_{n},f_{n})\big{)}\leq d_{\square}\big{(}(W,f),(W_{ k},f_{k})\big{)}+d_{\square}\big{(}(W_{k},f_{k}),(W_{n},f_{n})\big{)}\leq \epsilon(\alpha+(2+r)\beta).\]

In the following, we restrict to choices of \(\alpha\) and \(\beta\) which satisfy \(\alpha+(2+r)\beta=1\). Consider the function \(n:(0,1)\rightarrow\mathbb{N}\) defined by

\[n(\alpha):=\lceil 2^{\frac{4(1+0.5\epsilon)}{(\alpha\alpha)^{2}}+1}/(\epsilon \beta)\rceil=\lceil(2+r)\cdot 2^{\frac{9(1+0.5\epsilon)}{4(\alpha\alpha)^{2}}+1}/( \epsilon(1-\alpha))\rceil.\]

Using a similar technique as in the proof of Corollary B.7, there is \(\epsilon_{0}>0\) that depends on \(c\) and \(r\) (and hence also on \(t\)) such that for every \(\epsilon<\epsilon_{0}\), we may choose \(\alpha_{0}\) (that depends on \(\epsilon\)) which satisfies

\[n(\alpha_{0})=\lceil(2+r)\cdot 2^{\frac{2(1+0.5\epsilon)}{(\alpha\alpha)^{2}}+1} /(\epsilon(1-\alpha_{0}))\rceil<2^{\lceil\frac{2\epsilon}{\epsilon^{2}}\rceil}. \tag{19}\]

Moreover, there is a choice \(\alpha_{1}\) which satisfies

\[n(\alpha_{1})=\lceil(2+r)\cdot 2^{\frac{2(1+0.5\epsilon)}{(\alpha\alpha)^{2}}+1} /(\epsilon(1-\alpha_{1}))\rceil>2^{\lceil\frac{2\epsilon}{\epsilon^{2}}\rceil}. \tag{20}\]

We note that the function \(n:(0,1)\rightarrow\mathbb{N}\) satisfies the following intermediate value property. For every \(0<\alpha_{1}<\alpha_{2}<1\) and every \(m\in\mathbb{N}\) between \(n(\alpha_{1})\) and \(n(\alpha_{2})\), there is a point \(\alpha\in[\alpha_{1},\alpha_{2}]\) such that \(n(\alpha)=m\). This follows the fact that \(\alpha\mapsto(2+r)\cdot 2^{\frac{2(1+0.5\epsilon)}{(\alpha\alpha)^{2}}+1}/( \epsilon(1-\alpha))\) is a continuous function. Hence, by (19) and (20), there is a point \(\alpha\) (and \(\beta\) such that \(\alpha+(2+r)\beta=1\)) such that

\[n(\alpha)=n=\lceil 2^{\frac{2(1+0.5\epsilon)}{(\alpha\alpha)^{2}}+1}/(\epsilon \beta)\rceil=2^{\lceil\frac{2c/\epsilon^{2}}\rceil}.\]

By a slight modification of the above proof, we can replace \(n\) with the constant \(n=\lceil 2^{\frac{2\epsilon}{\epsilon^{2}}}\rceil\). As a result, we can easily prove that for any \(n^{\prime}\geq 2^{\lceil\frac{2\epsilon}{\epsilon^{2}}\rceil}\) we have the approximation property (18) with \(n^{\prime}\) instead of \(n\). This is done by choosing an appropriate \(c^{\prime}>c\) and using Theorem B.8 on \(c^{\prime}\), giving a constant \(n^{\prime}=\lceil 2^{\frac{2\epsilon^{2}}{\epsilon^{2}}}\rceil\geq 2^{\lceil \frac{2\epsilon}{\epsilon^{2}}\rceil}=n\). This leads to the following corollary.

**Corollary B.9** (Regularity lemma for graphon-signals - equipartition version 2).: _Let \(c>1\) and \(r>0\). For any sufficiently small \(\epsilon>0\), for every \(n\geq 2^{\lceil\frac{2\epsilon}{\epsilon^{2}}\rceil}\) and every \((W,f)\in\mathcal{WL}_{r}\), there exists \(\phi\in S^{\prime}_{[0,1]}\), a step function graphon \([W^{\phi}]_{n}\in\mathcal{S}^{2}_{\mathcal{L}_{n}}\cap\mathcal{W}_{0}\) and a step function signal \([f^{\phi}]_{n}\in\mathcal{S}^{1}_{\Next, we prove that we can use the average of the graphon and the signal in each part for the approximating graphon-signal. For that we define the projection of a graphon signal upon a partition.

**Definition B.10**.: _Let \(\mathcal{P}_{n}=\{P_{1},\ldots,P_{n}\}\) be a partition of \([0,1]\), and \((W,f)\in\mathcal{WL}_{r}\). We define the projection of \((W,f)\) upon \((\mathcal{S}_{P}^{2}\times\mathcal{S}_{P}^{1})\cap\mathcal{WL}_{r}\) to be the step graphon-signal \((W,f)_{\mathcal{P}_{n}}=(W_{\mathcal{P}_{n}},f_{\mathcal{P}_{n}})\) that attains the value_

\[W_{\mathcal{P}_{n}}(x,y)=\int_{P_{i}\times P_{j}}W(x,y)dxdy\;,\quad f_{ \mathcal{P}_{n}}(x)=\int_{P_{i}}f(x)dx\]

_for every \((x,y)\in P_{i}\times P_{j}\)._

At the cost of replacing the error \(\epsilon\) by \(2\epsilon\), we can replace \(W^{\prime}\) with its projection. This was shown in [1]. Since this paper does not use the exact same setting as us, for completeness, we write a proof of the claim below.

**Corollary B.11** (Regularity lemma for graphon-signals - projection version).: _For any \(c>1\), and any sufficiently small \(\epsilon>0\), for every \(n\geq 2^{\lceil\frac{8\epsilon}{2}\rceil}\) and every \((W,f)\in\mathcal{WL}_{r}\), there exists \(\phi\in S^{\prime}_{[0,1]}\) such that such that_

\[d_{\square}\Big{(}\;\big{(}W^{\phi},f^{\phi}\big{)}\;,\;\big{(}[W^{\phi}]_{ \mathcal{I}_{n}},[f^{\phi}]_{\mathcal{I}_{n}}\big{)}\;\Big{)}\leq\epsilon.\]

_where \(\mathcal{I}_{n}\) is the equipartition of \([0,1]\) into \(n\) intervals._

We first prove a simple lemma.

**Lemma B.12**.: _Let \(\mathcal{P}_{n}=\{P_{1},\ldots,P_{n}\}\) be a partition of \([0,1]\), and Let \(V,R\in\mathcal{S}_{\mathcal{P}_{n}}^{2}\cap\mathcal{W}_{0}\). Then, the supremum of_

\[\sup_{S,T\subset[0,1]}\left|\int_{S}\int_{T}\big{(}V(x,y)-R(x,y)\big{)}dxdy\right| \tag{21}\]

_is attained for \(S,T\) of the form_

\[S=\bigcup_{i\in s}P_{i}\;,\quad T=\bigcup_{j\in t}P_{j},\]

_where \(t,s\subset[n]\). Similarly for any two signals \(f,g\in\mathcal{S}_{\mathcal{P}_{n}}^{1}\cap\mathcal{L}_{r}^{\infty}[0,1]\), the supremum of_

\[\sup_{S\subset[0,1]}\left|\int_{S}\big{(}f(x)-g(x)\big{)}dx\right| \tag{22}\]

_is attained for \(S\) of the form_

\[S=\bigcup_{i\in s}P_{i},\]

_where \(s\subset[n]\)._

Proof.: First, by Lemma A.1, the supremum of (21) is attained for some \(S,T\subset[0,1]\). Given the maximizers \(S,T\), without loss of generality, suppose that

\[\int_{S}\int_{T}\big{(}V(x,y)-R(x,y)\big{)}dxdy>0.\]

we can improve \(T\) as follows. Consider the set \(t\subset[n]\) such that for every \(j\in t\)

\[\int_{S}\int_{T\cap P_{j}}\big{(}V(x,y)-R(x,y)\big{)}dxdy>0.\]

By increasing the set \(T\cap P_{j}\) to \(P_{j}\), we can only increase the size of the above integral. Indeed,

\[\int_{S}\int_{P_{j}}\big{(}V(x,y)-R(x,y)\big{)}dxdy =\frac{\mu(P_{j})}{\mu(T\cap P_{j})}\int_{S}\int_{T\cap P_{j}} \big{(}V(x,y)-R(x,y)\big{)}dxdy\] \[\geq\int_{S}\int_{T\cap P_{j}}\big{(}V(x,y)-R(x,y)\big{)}dxdy.\]Hence, by increasing \(T\) to

\[T^{\prime}=\bigcup_{\{j|T\cap P_{j}\neq\emptyset\}}P_{j},\]

we get

\[\int_{S}\int_{T^{\prime}}\big{(}V(x,y)-R(x,y)\big{)}dxdy\geq\int_{S}\int_{T} \big{(}V(x,y)-R(x,y)\big{)}dxdy.\]

We similarly replace each \(T\cap P_{j}\) such that

\[\int_{S}\int_{T\cap P_{j}}\big{(}V(x,y)-R(x,y)\big{)}dxdy\leq 0\]

by the empty set. We now repeat this process for \(S\), which concludes the proof for the graphon part.

For the signal case, let \(f=f_{+}-f_{-}\), and suppose without loss of generality that \(\|f\|_{\square}=\|f\|_{1}\). It is easy to see that the supremum of (22) is attained for the support of \(f_{+}\), which has the required form. \(\blacksquare\)

Proof.: Proof of CorollaryB.11 Let \(W_{n}\in\mathcal{S}_{\mathcal{P}_{n}}(\cap)\mathcal{W}_{0}\) be the step graphon guaranteed by CorollaryB.9, with error \(\epsilon/2\) and measure preserving bijection \(\phi\in S^{\prime}_{[0,1]}\). Without loss of generality, we suppose that \(W^{\phi}=W\). Otherwise, we just denote \(W^{\prime}=W^{\phi}\) and replace the notation \(W\) with \(W^{\prime}\) in the following. By LemmaB.12, the infimum underlying \(\|W_{\mathcal{P}_{n}}-W_{n}\|_{\square}\) is attained for for some

\[S=\bigcup_{i\in s}P_{i}\;,\quad T=\bigcup_{j\in t}P_{j}.\]

We now have, by definition of the projected graphon,

\[\|W_{n}-W_{\mathcal{P}_{n}}\|_{\square} =\left|\sum_{i\in s,j\in t}\int_{P_{i}}\int_{P_{j}}(W_{ \mathcal{P}_{n}}(x,y)-W_{n}(x,y))dxdy\right|\] \[=\left|\sum_{i\in s,j\in t}\int_{P_{i}}\int_{P_{j}}(W(x,y)-W_{n}( x,y))dxdy\right|\] \[=\left|\int_{S}\int_{T}(W(x,y)-W_{n}(x,y))dxdy\right|=\|W_{n}-W\| _{\square}.\]

Hence, by the triangle inequality,

\[\|W-W_{\mathcal{P}_{n}}\|_{\square}\leq\|W-W_{n}\|_{\square}+\|W_{n}-W_{ \mathcal{P}_{n}}\|_{\square}<2\|W_{n}-W\|_{\square}.\]

A similar argument shows

\[\|f-f_{\mathcal{P}_{n}}\|_{\square}<2\|f_{n}-f\|_{\square}.\]

Hence,

\[d_{\square}\Big{(}\;\big{(}W^{\phi},f^{\phi}\big{)}\;,\;\big{(}[W^{\phi}]_{ \mathcal{I}_{n}},[f^{\phi}]_{\mathcal{I}_{n}}\big{)}\;\Big{)}\leq 2d_{\square} \Big{(}\;\big{(}W^{\phi},f^{\phi}\big{)}\;,\;\big{(}[W^{\phi}]_{n},[f^{\phi}]_ {n}\big{)}\;\Big{)}\leq\epsilon.\]

\(\blacksquare\)

## Appendix C Compactness and covering number of the graphon-signal space

In this appendix we prove Theorem3.6.

Given a partition \(\mathcal{P}_{k}\), recall that

\[[\mathcal{W}\mathcal{L}_{r}]_{\mathcal{P}_{k}}:=(\mathcal{W}_{0}\cap \mathcal{S}^{2}_{\mathcal{P}_{k}})\times(\mathcal{L}^{\infty}_{r}[0,1]\cap \mathcal{S}^{1}_{\mathcal{P}_{k}})\]

is called the space of SBMs or step graphon-signals with respect to \(\mathcal{P}_{k}\). Recall that \(\widehat{\mathcal{W}\mathcal{L}_{r}}\) is the space of equivalence classes of graphon-signals with zero \(\delta_{\square}\) distance, with the \(\delta_{\square}\) metric (defined on arbitrary representatives). By abuse of terminology, we call elements of \(\widehat{\mathcal{W}\mathcal{L}_{r}}\) also graphon-signals.

**Theorem C.1**.: _The metric space \((\widehat{\mathcal{W}\mathcal{L}_{r}},\delta_{\square})\) is compact._The proof is a simple extension of [25, Lemma 8] from the case of graphon to the case of graphon-signal. The proof relies on the notion of martingale. A martingale is a sequence of random variables for which, for each element in the sequence, the conditional expectation of the next value in the sequence is equal to the present value, regardless of all prior values. The Martingale convergence theorem states that for any bounded martingale \(\{M_{n}\}_{n}\) over the probability pace \(X\), the sequence \(\{M_{n}(x)\}_{n}\) converges for almost every \(x\in X\), and the limit function is bounded (see [12, 35]).

Proof.: [Proof of Theorem C.1] Consider a sequence \(\{[(W_{n},f_{n})]\}_{n\in\mathbb{N}}\subset\widetilde{\mathcal{WL}_{r}}\), with \((W_{n},f_{n})\in\mathcal{WL}_{r}\). For each \(k\), consider the equipartition into \(m_{k}\) intervals \(\mathcal{I}_{m_{k}}\), where \(m_{k}=2^{30\lceil(r^{2}+1)\rceil k^{2}}\). By Corollary B.11, there is a measure preserving bijection \(\phi_{n,k}\) (up to nullset) such that

\[\|(W_{n},f_{n})^{\phi_{n,k}}-(W_{n},f_{n})^{\phi_{n,k}}_{\mathcal{I}_{m_{k}}} \|_{\mathbb{C};r}<1/k,\]

where \((W_{n},f_{n})^{\phi_{n,k}}_{\mathcal{I}_{m_{k}}}\) is the projection of \((W_{n},f_{n})^{\phi_{n,k}}\) upon \(\mathcal{I}_{m_{k}}\) (Definition B.10). For every fixed \(k\), each pair of functions \((W_{n},f_{n})^{\phi_{n,k}}_{\mathcal{I}_{m_{k}}}\) is defined via \(m_{k}^{2}+m_{k}\) values in \([0,1]\). Hence, since \([0,1]^{m_{k}^{2}+m_{k}}\) is compact, there is a subsequence \(\{n_{j}^{k}\}_{j\in\mathbb{N}}\), such that all of these values converge. Namely, for each \(k\), the sequence

\[\{(W_{n_{j}^{k}},f_{n_{j}^{k}})^{\phi_{n_{j}^{k},k}}_{\mathcal{I}_{m_{k}}}\}_{ j=1}^{\infty}\]

converges pointwise to some step graphon-signal \((U_{k},g_{k})\) in \([\mathcal{WL}_{r}]_{r_{k}}\) as \(j\to\infty\). Note that \(\mathcal{I}_{m_{l}}\) is a refinement of \(\mathcal{I}_{m_{k}}\) for every \(l>k\). As as a result, by the definition of projection of graphon-signals to partitions, for every \(l>k\), the value of \((W_{n}^{\phi_{n,k}})_{\mathcal{I}_{m_{k}}}\) at each partition set \(I_{m_{k}}^{i}\times I_{m_{k}}^{j}\) can be obtained by averaging the values of \((W_{n}^{\phi_{n,l}})_{\mathcal{I}_{m_{l}}}\) at all partition sets \(I_{m_{l}}^{i^{\prime}}\times I_{m_{l}}^{j^{\prime}}\) that are subsets of \(I_{m_{k}}^{i}\times I_{m_{k}}^{j}\). A similar property applies also to the signal. Moreover, by taking limits, it can be shown that the same property holds also for \((U_{k},g_{k})\) and \((U_{l},g_{l})\). We now see \(\{(U_{k},g_{k})\}_{k=1}^{\infty}\) as a sequence of random variables over the standard probability space \([0,1]^{2}\). The above discussion shows that \(\{(U_{k},g_{k})\}_{k=1}^{\infty}\) is a bounded martingale. By the martingale convergence theorem, the sequence \(\{(U_{k},g_{k})\}_{k=1}^{\infty}\) converges almost everywhere pointwise to a limit \((U,g)\), which must be in \(\mathcal{WL}_{r}\).

Lastly, we show that there exist increasing sequences \(\{k_{z}\in\mathbb{N}\}_{z=1}^{\infty}\) and \(\{t_{z}=\eta_{j_{z}}^{k_{z}}\}_{z\in\mathbb{N}}\) such that \((W_{t_{z}},f_{t_{z}})^{\phi_{t_{z},k_{z}}}\) converges to \((U,g)\) in cut distance. By the dominant convergence theorem, for each \(z\in\mathbb{N}\) there exists a \(k_{z}\) such that

\[\|(U,g)-(U_{k_{z}},g_{k_{z}})\|_{1}<\frac{1}{3z}.\]

We choose such an increasing sequence \(\{k_{z}\}_{z\in\mathbb{N}}\) with \(k_{z}>3z\). Similarly, for ever \(z\in\mathbb{N}\), there is a \(j_{z}\) such that, with the notation \(t_{z}=n_{j_{z}}^{k_{z}}\),

\[\|(U_{k_{z}},g_{k_{z}})-(W_{t_{z}},f_{t_{z}})^{\phi_{t_{z},k_{z}}}_{\mathcal{I} _{m_{k_{z}}}}\|_{1}<\frac{1}{3z},\]

and we may choose the sequence \(\{t_{z}\}_{z\in\mathbb{N}}\) increasing. Therefore, by the triangle inequality and by the fact that the \(L_{1}\) norm bounds the cut norm,

\[\delta_{\square}\big{(}(U,g),(W_{t_{z}},f_{t_{z}})\big{)} \leq\|(U,g)-(W_{t_{z}},f_{t_{z}})^{\phi_{t_{z},k_{z}}}\|_{\square}\] \[\leq\|(U,g)-(U_{k_{z}},g_{k_{z}})\|_{1}+\|(U_{k_{z}},g_{k_{z}})-(W _{t_{z}},f_{t_{z}})^{\phi_{t_{z},k_{z}}}\|_{1}\] \[\quad+\|(W_{t_{z}},f_{t_{z}})^{\phi_{t_{z},k_{z}}}_{\mathcal{I}_{ m_{k_{z}}}}-(W_{t_{z}},f_{t_{z}})^{\phi_{t_{z},k_{z}}}\|_{\square}\] \[\leq\frac{1}{3z}+\frac{1}{3z}+\frac{1}{3z}\leq\frac{1}{z}.\]

The next theorem bounds the covering number of \(\widetilde{\mathcal{WL}_{r}}\).

**Theorem C.2**.: _Let \(r>0\) and \(c>1\). For every sufficiently small \(\epsilon>0\), the space \(\widetilde{\mathcal{WL}_{r}}\) can be covered by_

\[\kappa(\epsilon)=2^{k^{2}} \tag{23}\]

_balls of radius \(\epsilon\) in cut distance, where \(k=\lceil 2^{2c/\epsilon^{2}}\rceil\)._Proof.: Let \(1<c<c^{\prime}\) and \(0<\alpha<1\). Given an error tolerance \(\alpha\epsilon>0\), using Theorem B.8, we take the equipartition \(\mathcal{I}_{n}\) into \(n=2^{\lceil\frac{2\epsilon}{\alpha}\rceil}\) intervals, for which any graphon-signal \((W,f)\in\widetilde{\mathcal{W}\mathcal{L}_{r}}\) can be approximated by some \((W,f)_{n}\) in \([\widetilde{\mathcal{W}\mathcal{L}_{r}}]_{\mathcal{I}_{n}}\), up to error \(\alpha\epsilon\). Consider the rectangle \(\mathcal{R}_{n,r}=[0,1]^{n^{2}}\times[-r,r]^{n}\). We identify each element of \([\widetilde{\mathcal{W}\mathcal{L}_{r}}]_{\mathcal{I}_{n}}\) with an element of \(\mathcal{R}_{n,r}\) using the coefficients of (5). More accurately, the coefficients \(c_{i,j}\) of the step graphon are identifies with the first \(n^{2}\) entries of a point in \(\mathcal{R}_{n,r}\), and the the coefficients \(b_{i}\) of the step signals are identifies with the last \(n\) entries of a point in \(\mathcal{R}_{n,r}\). Now, consider the quantized rectangle \(\tilde{\mathcal{R}}_{n,r}\), defined as

\[\tilde{\mathcal{R}}_{n,r}=\big{(}(1-\alpha)\epsilon\mathbb{Z}\big{)}^{n^{2}+2 rn}\cap\mathcal{R}_{n,r}.\]

Note that \(\tilde{\mathcal{R}}_{n}\) consists of

\[M\leq\lceil\frac{1}{(1-\alpha)\epsilon}\rceil^{n^{2}+2rn}\leq 2^{\big{(}-\log \big{(}(1-\alpha)\epsilon\big{)}+1\big{)}(n^{2}+2rn)}\]

points. Now, every point \(x\in\mathcal{R}_{n,r}\) can be approximated by a quantized version \(x_{Q}\in\tilde{\mathcal{R}}_{n,r}\) up to error in normalized \(\ell_{1}\) norm

\[\|x-x_{Q}\|_{1}:=\frac{1}{M}\sum_{j=1}^{M}\Big{|}x^{j}-x_{Q}^{j}\Big{|}\leq(1- \alpha)\epsilon,\]

where we re-index the entries of \(x\) and \(x_{Q}\) in a 1D sequence. Let us denote by \((W,f)_{Q}\) the quantized version of \((W_{n},f_{n})\), given by the above equivalence mapping between \((W,f)_{n}\) and \(\mathcal{R}_{n,r}\). We hence have

\[\|(W,f)-(W,f)_{Q}\|_{\square}\leq\|(W,f)-(W_{n},f_{n})\|_{\square}+\|(W_{n},f_ {n})-(W,f)_{Q}\|_{\square}\leq\epsilon.\]

We now choose the parameter \(\alpha\). Note that for any \(c^{\prime}>c\), there exists \(\epsilon_{0}>0\) that depends on \(c^{\prime}-c\), such that for any \(\epsilon<\epsilon_{0}\) there is a choice of \(\alpha\) (close to 1) such that

\[M\leq\lceil\frac{1}{(1-\alpha)\epsilon}\rceil^{n^{2}+2rn}\leq 2^{\big{(}-\log \big{(}(1-\alpha)\epsilon\big{)}+1\big{)}(n^{2}+2rn)}\leq 2^{k^{2}}\]

where \(k=\lceil 2^{2c^{\prime}/\epsilon^{2}}\rceil\). This is shown similarly to the proof of Corollary B.7 and Theorem B.8. We now replace the notation \(c^{\prime}\to c\), which concludes the proof.

## Appendix D Graphon-signal sampling lemmas

In this appendix, we prove Theorem 3.7. We denote by \(\mathcal{W}_{1}\) the space of measurable functions \(U:[0,1]\to[-1,1]\), and call each \(U\in\mathcal{W}_{1}\) a kernel.

### Formal construction of sampled graph-signals

Let \(W\in\mathcal{W}_{0}\) be a graphon, and \(\Lambda^{\prime}=(\lambda^{\prime}_{1},\ldots\lambda^{\prime}_{k})\in[0,1]^{k}\). We denote by \(W(\Lambda^{\prime})\) the adjacency matrix

\[W(\Lambda^{\prime})=\{W(\lambda^{\prime}_{i},\lambda^{\prime}_{j})\}_{i,j\in[ k]}.\]

By abuse of notation, we also treat \(W(\Lambda^{\prime})\) as a weighted graph with \(k\) nodes and the adjacency matrix \(W(\Lambda^{\prime})\). We denote by \(\Lambda=(\lambda_{1},\ldots,\lambda_{k}):(\lambda^{\prime}_{1},\ldots\lambda^ {\prime}_{k})\mapsto(\lambda^{\prime}_{1},\ldots\lambda^{\prime}_{k})\) the identity random variable in \([0,1]^{k}\). We hence call \((\lambda_{1},\ldots,\lambda_{k})\) random independent samples from \([0,1]\). We call the random variable \(W(\Lambda)\) a _random sampled weighted graph_.

Given \(f\in\mathcal{L}_{r}^{\infty}[0,1]\) and \(\Lambda^{\prime}=(\Lambda^{\prime}_{1},\ldots,\Lambda^{\prime}_{k})\in[0,1]^{k}\), we denote by \(f(\Lambda^{\prime})\) the discrete signal with \(k\) nodes, and value \(f(\lambda^{\prime}_{i})\) for each node \(i=1,\ldots,k\). We define the _sampled signal_ as the random variable \(f(\Lambda)\).

We then define the random sampled simple graph as follows. First, for a deterministic \(\Lambda^{\prime}\in[0,1]^{k}\), we define a 2D array of Bernoulli random variables \(\{e_{i,j}(\Lambda^{\prime})\}_{i,j\in[k]}\) where \(e_{i,j}(\Lambda^{\prime})=1\) in probability \(W(\lambda^{\prime}_{i},\lambda^{\prime}_{j})\), and zero otherwise, for \(i,j\in[k]\). We define the probability space \(\{0,1\}^{k\times k}\) with normalized counting measure, defined for any \(S\subset\{0,1\}^{k\times k}\) by

\[P_{\Lambda^{\prime}}(S)=\sum_{\mathbf{z}\in S}\prod_{i,j\in[k]}P_{\Lambda^{ \prime};i,j}(z_{i,j}),\]

where

\[P_{\Lambda^{\prime};i,j}(z_{i,j})=\left\{\begin{array}{cc}W(\lambda^{\prime} _{i},\lambda^{\prime}_{j})&\text{if }z_{i,j}=1\\ 1-W(\lambda^{\prime}_{i},\lambda^{\prime}_{j})&\text{if }z_{i,j}=0.\end{array}\right.\]

We denote the identity random variable by \(\mathbb{G}(W,\Lambda^{\prime}):\mathbf{z}\mapsto\mathbf{z}\), and call it a _random simple graph sampled from \(W(\Lambda^{\prime})\)_.

Next we also allow to "plug" the random variable \(\Lambda\) into \(\Lambda^{\prime}\). For that, we define the joint probability space \(\Omega=[0,1]^{k}\times\{0,1\}^{k\times k}\) with the product \(\sigma\)-algebra of the Lebesgue sets in \([0,1]^{k}\) with the power set \(\sigma\)-algebra of \(\{0,1\}^{k\times k}\), with measure, for any measurable \(S\subset\Omega\),

\[\mu(S)=\int_{[0,1]^{k}}P_{\Lambda^{\prime}}\big{(}S(\Lambda^{\prime})\big{)}d \Lambda^{\prime},\]

where

\[S(\Lambda^{\prime})\subset\{0,1\}^{k\times k}:=\{\mathbf{z}=\{z_{i,j}\}_{i,j \in[k]}\in\{0,1\}^{k\times k}\mid(\Lambda^{\prime},\mathbf{z})\in S\},\]

We call the random variable \(\mathbb{G}(W,\Lambda):\Lambda^{\prime}\times\mathbf{z}\mapsto\mathbf{z}\) the _random simple graph generated by \(W\)_. We extend the domains of the random variables \(W(\Lambda)\), \(f(\Lambda)\) and \(\mathbb{G}(W,\Lambda^{\prime})\) to \(\Omega\) trivially (e.g., \(f(\Lambda)(\Lambda^{\prime},\mathbf{z})=f(\Lambda)(\Lambda^{\prime})\) and \(\mathbb{G}(W,\Lambda^{\prime})(\Lambda^{\prime},\mathbf{z})=\mathbb{G}(W, \Lambda^{\prime})(\mathbf{z})\)), so that all random variables are defined over the same space \(\Omega\). Note that the random sampled graphs and the random signal share the same sample points.

Given a kernel \(U\in\mathcal{W}_{1}\), we define the random sampled kernel \(U(\Lambda)\) similarly.

Similarly to the above construction, given a weighted graph \(H\) with \(k\) nodes and edge weights \(h_{i,j}\), we define the _simple graph sampled from \(H\)_ as the random variable simple graph \(\mathbb{G}(H)\) with \(k\) nodes and independent Bernoulli variables \(e_{i,j}\in\{0,1\}\), with \(\mathbb{P}(e_{i,j}=1)=h_{i,j}\), as the edge weights. The following lemma is taken from [24, Equation (10.9)].

**Lemma D.1**.: _Let \(H\) be a weighted graph of \(k\) nodes. Then_

\[\mathbb{E}\big{(}d_{\square}(\mathbb{G}(H),H)\big{)}\leq\frac{11}{\sqrt{k}}.\]

The following is a simple corollary of Lemma D.1, using the law of total probability.

**Corollary D.2**.: _Let \(W\in\mathcal{W}_{0}\) and \(k\in\mathbb{N}\). Then_

\[\mathbb{E}\big{(}d_{\square}(\mathbb{G}(W,\Lambda),W(\Lambda))\big{)}\leq\frac {11}{\sqrt{k}}.\]

### Sampling lemmas of graphon-signals

The following lemma, from [24, Lemma 10.6], shows that the cut norm of a kernel is approximated by the cut norm of its sample.

**Lemma D.3** (First Sampling Lemma for kernels).: _Let \(U\in\mathcal{W}_{1}\), and \(\Lambda\in[0,1]^{k}\) be uniform independent samples from \([0,1]\). Then, with probability at least \(1-4e^{-\sqrt{k}/10}\),_

\[-\frac{3}{k}\leq\|U[\Lambda]\|_{\square}-\|U\|_{\square}\leq\frac{8}{k^{1/4}}.\]

We derive a version of Lemma D.3 with expected value using the following lemma.

**Lemma D.4**.: _Let \(z:\Omega\to[0,1]\) be a random variable over the probability space \(\Omega\). Suppose that in an event \(\mathcal{E}\subset\Omega\) of probability \(1-\epsilon\) we have \(z<\alpha\). Then_

\[\mathbb{E}(z)\leq(1-\epsilon)\alpha+\epsilon.\]Proof.: \[\mathbb{E}(z)=\int_{\Omega}z(x)dx=\int_{\mathcal{E}}z(x)dx+\int_{\Omega\setminus \mathcal{E}}z(x)dx\leq(1-\epsilon)\alpha+\epsilon.\]

As a result of this lemma, we have a simple corollary of Lemma D.3.

**Corollary D.5** (First sampling lemma - expected value version).: _Let \(U\in\mathcal{W}_{1}\) and \(\Lambda\in[0,1]^{k}\) be chosen uniformly at random, where \(k\geq 1\). Then_

\[\mathbb{E}\,|\|U[\Lambda]\|_{\Box}-\|U\|_{\Box}|\leq\frac{14}{k^{1/4}}.\]

Proof.: By Lemma D.4, and since \(6/k^{1/4}>4e^{-\sqrt{k}/10}\),

\[\mathbb{E}\big{|}\|U[\Lambda]\|_{\Box}-\|U\|_{\Box}\big{|}\leq\big{(}1-4e^{- \sqrt{k}/10}\big{)}\frac{8}{k^{1/4}}+4e^{-\sqrt{k}/10}<\frac{14}{k^{1/4}}.\]

We note that a version of the first sampling lemma, Lemma D.3, for signals instead of kernels, is just a classical Monte Carlo approximation, when working with the \(L_{1}[0,1]\) norm, which is equivalent to the signal cut norm.

**Lemma D.6** (First sampling lemma for signals).: _Let \(f\in\mathcal{L}_{r}^{\infty}[0,1]\). Then_

\[\mathbb{E}\,|\|f(\Lambda)\|_{1}-\|f\|_{1}|\leq\frac{r}{k^{1/2}}.\]

Proof.: By standard Monte Carlo theory, since \(r^{2}\) bounds the variance of \(f(\lambda)\), where \(\lambda\) is a random uniform sample from \([0,1]\), we have

\[\mathbb{V}(\|f(\Lambda)\|_{1})=\mathbb{E}\big{(}\,|\|f(\Lambda)\|_{1}-\|f\|_{1 }|^{2}\,\big{)}\leq\frac{r^{2}}{k}.\]

Here, \(\mathbb{V}\) denotes variance, and we note that \(\mathbb{E}\|f(\Lambda)\|_{1}=\frac{1}{k}\sum_{j=1}^{k}|f(\lambda_{j})|=\|f\|_{1}\). Hence, by Cauchy Schwarz inequality,

\[\mathbb{E}\,|\|f(\Lambda)\|_{1}-\|f\|_{1}|\leq\sqrt{\mathbb{E}\big{(}\,\|f( \Lambda)\|_{1}-\|f\|_{1}|^{2}\,\big{)}}\leq\frac{r}{k^{1/2}}.\]

We now extend [24, Lemma 10.16], which bounds the cut distance between a graphon and its sampled graph, to the case of a sampled graphon-signal.

**Theorem D.7** (Second sampling lemma for graphon signals).: _Let \(r>1\). Let \(k\geq K_{0}\), where \(K_{0}\) is a constant that depends on \(r\), and let \((W,f)\in\mathcal{WL}_{r}\). Then,_

\[\mathbb{E}\Big{(}\delta_{\Box}\big{(}(W,f),(W(\Lambda),f(\Lambda))\big{)} \Big{)}<\frac{15}{\sqrt{\log(k)}},\]

_and_

\[\mathbb{E}\Big{(}\delta_{\Box}\big{(}(W,f),(\mathbb{G}(W,\Lambda),f(\Lambda)) \big{)}\Big{)}<\frac{15}{\sqrt{\log(k)}}.\]

The proof follows the steps of [24, Lemma 10.16] and [4]. We note that the main difference in our proof is that we explicitly write the measure preserving bijection that optimizes the cut distance. While this is not necessary in the classical case, where only a graphon is sampled, in our case we need to show that there is a measure preserving bijection that is shared by the graphon and the signal. We hence write the proof for completion.

Proof.: Denote a generic error bound, given by the regularity lemma Theorem B.8 by \(\epsilon\). If we take \(n\) intervals in the Theorem B.8, then the error in the regularity lemma will be, for \(c\) such that \(2c=3\),

\[\lceil 3/\epsilon^{2}\rceil=\log(n)\]\[3/\epsilon^{2}+1\geq\log(n).\]

For small enough \(\epsilon\), we increase the error bound in the regularity lemma to satisfy

\[4/\epsilon^{2}>3/\epsilon^{2}+1\geq\log(n).\]

More accurately, for the equipartition to intervals \(\mathcal{I}_{n}\), there is \(\phi^{\prime}\in S^{\prime}_{[0,1]}\) and a piecewise constant graphon signal \(([W^{\phi}]_{n},[f^{\phi}]_{n})\) such that

\[\|W^{\phi^{\prime}}-[W^{\phi^{\prime}}]_{n}\|_{\Box}\leq\alpha\frac{2}{\sqrt{ \log(n)}}\]

and

\[\|f^{\phi^{\prime}}-[f^{\phi^{\prime}}]_{n}\|_{\Box}\leq(1-\alpha)\frac{2}{ \sqrt{\log(n)}},\]

for some \(0\leq\alpha\leq 1\). If we choose \(n\) such that

\[n=\lceil\frac{\sqrt{k}}{r\log(k)}\rceil,\]

then an error bound in the regularity lemma is

\[\|W^{\phi^{\prime}}-[W^{\phi^{\prime}}]_{n}\|_{\Box}\leq\alpha\frac{2}{\sqrt{ \frac{1}{2}\log(k)-\log\big{(}\log(k)\big{)}-\log(r)}}\]

and

\[\|f^{\phi^{\prime}}-[f^{\phi^{\prime}}]_{n}\|_{\Box}\leq(1-\alpha)\frac{2}{ \sqrt{\frac{1}{2}\log(k)-\log\big{(}\log(k)\big{)}-\log(r)}},\]

for some \(0\leq\alpha\leq 1\). Without loss of generality, we suppose that \(\phi^{\prime}\) is the identity. This only means that we work with a different representative of \([(W,f)]\in\widehat{\mathcal{WL}_{r}}\) throughout the proof. We hence have

\[d_{\Box}(W,W_{n})\leq\alpha\frac{2\sqrt{2}}{\sqrt{\log(k)-2\log\big{(}\log(k) \big{)}-2\log(r)}}\]

and

\[\|f-f_{n}\|_{1}\leq(1-\alpha)\frac{4\sqrt{2}}{\sqrt{\log(k)-2\log\big{(}\log(k )\big{)}-2\log(r)}},\]

for some step graphon-signal \((W_{n},f_{n})\in[\mathcal{WL}_{r}]_{\mathcal{I}_{n}}\).

Now, by the first sampling lemma (Corollary D.5),

\[\mathbb{E}\big{|}d_{\Box}\big{(}W(\Lambda),W_{n}(\Lambda)\big{)}-d_{\Box}(W,W _{n})\big{|}\leq\frac{14}{k^{1/4}}.\]

Moreover, by the fact that \(f-f_{n}\in\mathcal{L}_{2r}^{\infty}[0,1]\), Lemma D.6 implies that

\[\mathbb{E}\big{|}\|f(\Lambda)-f_{n}(\Lambda)\|_{1}-\|f-f_{n}\|_{1}\big{|}\leq \frac{2r}{k^{1/2}}.\]

Therefore,

\[\mathbb{E}\Big{(}d_{\Box}\big{(}W(\Lambda),W_{n}(\Lambda)\big{)} \Big{)} \leq\mathbb{E}\big{|}d_{\Box}\big{(}W(\Lambda),W_{n}(\Lambda) \big{)}-d_{\Box}(W,W_{n})\big{|}+d_{\Box}(W,W_{n})\] \[\leq\frac{14}{k^{1/4}}+\alpha\frac{2\sqrt{2}}{\sqrt{\log(k)-2\log \big{(}\log(k)\big{)}-2\log(r)}}.\]

Similarly, we have

\[\mathbb{E}\|f(\Lambda)-f_{n}(\Lambda)\|_{1} \leq\mathbb{E}\big{|}\|f(\Lambda)-f_{n}(\Lambda)\|_{1}-\|f-f_{n} \|_{1}\big{|}+\|f-f_{n}\|_{1}\] \[\leq\frac{2r}{k^{1/2}}+(1-\alpha)\frac{4\sqrt{2}}{\sqrt{\log(k)-2 \log\big{(}\log(k)\big{)}-2\log(r)}}.\]Now, let \(\pi_{\Lambda}\) be a sorting permutation in \([k]\), such that

\[\pi_{\Lambda}(\Lambda):=\{\Lambda_{\pi_{\Lambda^{-1}(i)}^{-1}}\}_{i=1}^{k}=( \lambda^{\prime}_{1},\ldots,\lambda^{\prime}_{k})\]

is a sequence in a non-decreasing order. Let \(\{I^{i}_{k}=[i-1,i)/k\}_{i=1}^{k}\) be the intervals of the equipartition \(\mathcal{I}_{k}\). The sorting permutation \(\pi_{\Lambda}\) induces a measure preserving bijection \(\phi\) that sorts the intervals \(I^{i}_{k}\). Namely, we define, for every \(x\in[0,1]\),

\[\text{if }x\in I^{i}_{k},\quad\phi(x)=J_{i,\pi_{\Lambda}(i)}(x), \tag{24}\]

where \(J_{i,j}:I^{i}_{k}\to I^{j}_{k}\) are defined as \(x\mapsto x-i/k+j/k\), for all \(x\in I^{i}_{k}\).

By abuse of notation, we denote by \(W_{n}(\Lambda)\) and \(f_{n}(\Lambda)\) the induced graphon and signal from \(W_{n}(\Lambda)\) and \(f_{n}(\Lambda)\) respectively. Hence, \(W_{n}(\Lambda)^{\phi}\) and \(f_{n}(\Lambda)^{\phi}\) are well defined. Note that the graphons \(W_{n}\) and \(W_{n}(\Lambda)^{\phi}\) are stepfunctions, where the set of values of \(W_{n}(\Lambda)^{\phi}\) is a subset of the set of values of \(W_{n}\). Intuitively, since \(k\gg m\), we expect the partition \(\{[\lambda^{\prime}_{i},\lambda^{\prime}_{i+1})\}_{i=1}^{k}\) to be "close to a refinement" of \(\mathcal{I}_{n}\) in high probability. Also, we expect the two sets of values of \(W_{n}(\Lambda)^{\phi}\) and \(W_{n}\) to be identical in high probability. Moreover, since \(\Lambda^{\prime}\) is sorted, when inducing a graphon from the graph \(W_{n}(\Lambda)\) and "sorting" it to \(W_{n}(\Lambda)^{\phi}\), we get a graphon that is roughly "aligned" with \(W_{n}\). The same philosophy also applied to \(f_{n}\) and \(f_{n}(\Lambda)^{\phi}\). We next formalize these observations.

For each \(i\in[n]\), let \(\lambda^{\prime}_{j_{i}}\) be the smaller point of \(\Lambda^{\prime}\) that is in \(I^{i}_{n}\), set \(j_{i}=j_{i+1}\) if \(\Lambda^{\prime}\cap I^{i}_{n}=\emptyset\), and set \(j_{n+1}=k+1\). For every \(i=1,\ldots,n\), we call

\[J_{i}:=\left[j_{i}-1,j_{i+1}-1\right)/k\]

the \(i\)-th step of \(W_{n}(\Lambda)^{\phi}\) (which can be the empty set). Let \(a_{i}=\frac{j_{i}-1}{k}\) be the left edge point of \(J_{i}\). Note that \(a_{i}=\left|\Lambda\cap\left[0,i/n\right)\right|/k\) is distributed binomially (up to the normalization \(k\)) with \(k\) trials and success in probability \(i/n\).

\[\|W_{n}-W_{n}(\Lambda)^{\phi}\|_{\Box} \leq\|W_{n}-W_{n}(\Lambda)^{\phi}\|_{1}\] \[=\,\sum_{i}\sum_{k}\int_{I^{i}_{n}\cap J_{i}}\int_{I^{k}_{n}\cap J _{k}}\left|W_{n}(x,y)-W_{n}(\Lambda)^{\phi}(x,y)\right|dxdy\] \[\quad+\sum_{i}\sum_{j\neq i}\sum_{k}\sum_{l\neq k}\int_{I^{i}_{n }\cap J_{j}}\int_{I^{k}_{n}\cap J_{l}}\left|W_{n}(x,y)-W_{n}(\Lambda)^{\phi}(x,y)\right|dxdy\] \[=\,\sum_{i}\sum_{j\neq i}\sum_{k}\sum_{l\neq k}\int_{I^{i}_{n} \cap J_{j}}\int_{I^{k}_{n}\cap J_{l}}\left|W_{n}(x,y)-W_{n}(\Lambda)^{\phi}(x,y)\right|dxdy\] \[=\,\sum_{i}\sum_{k}\int_{I^{i}_{n}\setminus J_{i}}\int_{I^{k}_{n} \setminus J_{k}}\left|W_{n}(x,y)-W_{n}(\Lambda)^{\phi}(x,y)\right|dxdy\] \[\leq\,\sum_{i}\sum_{k}\int_{I^{i}_{n}\setminus J_{i}}\int_{I^{k}_ {n}\setminus J_{k}}1dxdy\leq 2\sum_{i}\int_{I^{i}_{n}\setminus J_{i}}1dxdy\] \[\leq 2\sum_{i}(|i/n-a_{i}|+|(i+1)/n-a_{i+1}|).\]

Hence,

\[\mathbb{E}\|W_{n}-W_{n}(\Lambda)^{\phi}\|_{\Box} \leq 2\sum_{i}(\mathbb{E}\left|i/n-a_{i}\right|+\mathbb{E}\left|(i +1)/n-a_{i+1}\right|)\] \[\leq 2\sum_{i}\left(\sqrt{\mathbb{E}(i/n-a_{i})^{2}}+\sqrt{ \mathbb{E}\big{(}(i+1)/n-a_{i+1}\big{)}^{2}}\right)\]

By properties of the binomial distribution, we have \(\mathbb{E}(ka_{i})=ik/n\), so

\[\mathbb{E}(ik/n-ka_{i})^{2}=\mathbb{V}(ka_{i})=k(i/n)(1-i/n).\]

As a result

\[\mathbb{E}\|W_{n}-W_{n}(\Lambda)^{\phi}\|_{\Box} \leq 5\sum_{i=1}^{n}\sqrt{\frac{(i/n)(1-i/n)}{k}}\] \[\leq 2\int_{1}^{n}\sqrt{\frac{(i/n)(1-i/n)}{k}}di,\]

[MISSING_PAGE_EMPTY:27]

As a result, for large enough \(k\),

\[\mathbb{E}\Big{(}\delta_{\square}\big{(}(W,f),(W(\Lambda),f(\Lambda))\big{)}\Big{)} <\frac{15}{\sqrt{\log(k)}},\]

and

\[\mathbb{E}\Big{(}\delta_{\square}\big{(}(W,f),(\mathbb{G}(W,\Lambda),f(\Lambda ))\big{)}\Big{)}<\frac{15}{\sqrt{\log(k)}}.\]

## Appendix E Graphon-signal MPNNs

In this appendix we give properties and examples of MPNNs.

### Properties of graphon-signal MPNNs

Consider the construction of MPNN from Section 4.1. We first explain how a MPNN on a grpah is equivalent to a MPNN on the induced graphon.

Let \(G\) be a graph of \(n\) nodes, with adjacency matrix \(A=\{a_{i,j}\}_{i,j\in[n]}\) and signal \(\mathbf{f}\in\mathbb{R}^{n\times d}\). Consider a MPL \(\theta\), with receiver and transmitter message functions \(\xi_{r}^{k},\xi_{t}^{k}:\mathbb{R}^{d}\to\mathbb{R}^{p}\), for \(k\in[K]\), where \(K\in\mathbb{N}\), and update function \(\mu:\mathbb{R}^{d+p}\to\mathbb{R}^{s}\). The application of the MPL on \((G,\mathbf{f})\) is defined as follows. We first define the message kernel \(\Phi_{\mathbf{f}}:[n]^{2}\to\mathbb{R}^{p}\), with entries

\[\Phi_{\mathbf{f}}(i,j)=\Phi(\mathbf{f}_{i},\mathbf{f}_{j})=\sum_{k=1}^{K}\xi_ {r}^{k}(\mathbf{f}_{i})\xi_{t}^{k}(\mathbf{f}_{j}).\]

We then aggregate the message kernel with normalized sum aggregation

\[\big{(}\mathrm{Agg}(G,\Phi_{\mathbf{f}})\big{)}_{i}=\frac{1}{n}\sum_{j\in[n]}a _{i,j}\Phi_{\mathbf{f}}(i,j).\]

Lastly, we apply the update function, to obtain the output \(\theta(G,\mathbf{f})\) of the MPL with value at each node \(i\)

\[\theta(G,\mathbf{f})_{i}=\eta\Big{(}\mathbf{f}_{i},\big{(}\mathrm{Agg}(G,\Phi_ {\mathbf{f}})\big{)}_{i}\Big{)}\in\mathbb{R}^{s}.\]

**Lemma E.1**.: _Consider a MPL \(\theta\) as in the above setting. Then, for every graph signal \((G,A,\mathbf{f})\),_

\[\theta\Big{(}(W,f)_{(G,\mathbf{f})}\Big{)}=(W,f)_{\theta(G,\mathbf{f})}.\]

Proof.: Let \(\{I_{i},\ldots,I_{n}\}\) be the equipartition to intervals. For each \(j\in[n]\), let \(y_{j}\in I_{j}\) be an arbitrary point. Let \(i\in[n]\) and \(x\in I_{i}\). We have

\[\mathrm{Agg}(G,\Phi_{\mathbf{f}})_{i} =\frac{1}{n}\sum_{j\in[n]}a_{i,j}\Phi_{\mathbf{f}}(i,j)=\frac{1}{ n}\sum_{j\in[n]}W_{G}(x,y_{j})\Phi_{f_{\mathbf{f}}}(x,y_{j})\] \[=\int_{0}^{1}W_{G}(x,y)\Phi_{f_{\mathbf{f}}}(x,y)dy=\mathrm{Agg}(W _{G},\Phi_{f_{\mathbf{f}}})(x).\]

Therefore, for every \(i\in[n]\) and every \(x\in I_{i}\),

\[f_{\theta(G,\mathbf{f})}(x) =f_{\eta}\big{(}\mathbf{f},\mathrm{Agg}(G,\Phi_{\mathbf{f}}) \big{)}(x)=\eta\big{(}\mathbf{f}_{i},\mathrm{Agg}(G,\Phi_{\mathbf{f}})_{i} \big{)}\] \[=\eta\big{(}f_{\mathbf{f}}(x),\mathrm{Agg}(W_{G},\Phi_{f_{ \mathbf{f}}})(x)\big{)}=\theta(W_{G},f_{\mathbf{f}})(x).\]

### Examples of MPNNs

The GIN convolutional layer [36] is defined as follows. First, the message function is

\[\Phi(a,b)=b\]

and the update function is

\[\eta(x,y)=M\big{(}(1+\epsilon)x+y\big{)}.\]

where \(M\) is a multi-layer perceptron (MLP) and \(\epsilon\) a constant. Each layer may have a different MLP and different constant \(\epsilon\). The standard GIN is defined with sum aggregation, but we use normalized sum aggregation.

Given a graph-signal \((G,\mathbf{f})\), with \(\mathbf{f}\in\mathbb{R}^{n\times d}\) with adjacency matrix \(A\in\mathbb{R}^{n\times n}\), a spectral convolutional layer based on a polynomial filter \(p(\lambda)=\sum_{j=0}^{J}\lambda^{j}C_{j}\), where \(C_{j}\in\mathbb{R}^{d\times p}\), is defined to be

\[p(A)\mathbf{f}=\sum_{j=0}^{J}\frac{1}{n^{j}}A^{j}\mathbf{f}C_{j},\]

followed by a pointwise non-linearity like ReLU. Such a convolutional layer can be seen as \(J+1\) MPLs. We first apply \(J\) MPLs, where each MPL is of the form

\[\theta(\mathbf{f})=\big{(}\mathbf{f},\frac{1}{n}A\mathbf{f}\big{)}.\]

We then apply an update layer

\[U(\mathbf{f})=\mathbf{f}C\]

for some \(C\in\mathbb{R}^{(J+1)d\times p}\), followed by the pointwise non-linearity. The message part of \(\theta\) can be written in our formulation with \(\Phi(a,b)=b\), and the update part of \(\theta\) with \(\eta(c,d)=(c,d)\). The last update layer \(U\) is linear followed by the pointwise non-linearity.

## Appendix F Lipschitz continuity of MPNNs

In this appendix we prove Theorem4.1. For \(v\in\mathbb{R}^{d}\), we often denote by \(|v|=\|v\|_{\infty}\). We define the \(L_{1}\) norm of a measurable function \(h:[0,1]\to\mathbb{R}^{d}\) by

\[\|h\|_{1}:=\int_{0}^{1}|h(x)|\,dx=\int_{0}^{1}\|h(x)\|_{\infty}dx.\]

Similarly,

\[\|h\|_{\infty}:=\sup_{x\in\mathbb{R}^{d}}|h(x)|=\sup_{x\in\mathbb{R}^{d}}\|h(x )\|_{\infty}.\]

We define Lipschitz continuity with respect to the infinity norm. Namely, \(Z:\mathbb{R}^{d}\to\mathbb{R}^{c}\) is called Lipschitz continuous with Lipschitz constant \(L\) if

\[|Z(x)-Z(y)|=\|Z(x)-Z(y)\|_{\infty}\leq L\|x-z\|_{\infty}=L\left|x-z\right|.\]

We denote the minimal Lipschitz bound of the function \(Z\) by \(L_{Z}\).

We extend \(\mathcal{L}_{r}^{\infty}[0,1]\) to the space of functions \(f:[0,1]\to\mathbb{R}^{d}\) with the above \(L_{1}\) norm.

Define the space \(\mathcal{K}_{q}\) of _kernels_ bounded by \(q>0\) to be the space of measurable functions

\[K:[0,1]^{2}\to[-q,q].\]

The cut norm, cut metric, and cut distance are defined as usual for kernels in \(\mathcal{K}_{q}\).

### Lipschitz continuity of message passing and update layers

In this subsection we prove that message passing layers and update layers are Lipschitz continuous with respect to he graphon-signal cut metric.

**Lemma F.1** (Product rule for message kernels).: _Let \(\Phi_{f},\Phi_{g}\) be the message kernels corresponding to the signals \(f,g\). Then_

\[\|\Phi_{f}-\Phi_{g}\|_{L^{1}[0,1]^{2}}\leq\sum_{k=1}^{K}\Big{(}L_{\xi_{r}^{k}}\| \xi_{t}^{k}\|_{\infty}+\|\xi_{r}^{k}\|_{\infty}L_{\xi_{t}^{k}}\Big{)}\|f-g\|_{ 1}.\]

Proof.: Suppose \(p=1\) For every \(x,y\in[0,1]^{2}\)

\[|\Phi_{f}(x,y)-\Phi_{g}(x,y)|=\left|\sum_{k=1}^{K}\xi_{r}^{k}(f(x) )\xi_{t}^{k}(f(y))-\sum_{k=1}^{K}\xi_{r}^{k}(g(x))\xi_{t}^{k}(g(y))\right|\] \[\leq\sum_{k=1}^{K}\big{|}\xi_{r}^{k}(f(x))\xi_{t}^{k}(f(y))-\xi_{ r}^{k}(g(x))\xi_{t}^{k}(g(y))\big{|}\] \[\leq\sum_{k=1}^{K}\Big{(}\big{|}\xi_{r}^{k}(f(x))\xi_{t}^{k}(f(y) )-\xi_{r}^{k}(g(x))\xi_{t}^{k}(f(y))\big{|}+\big{|}\xi_{r}^{k}(g(x))\xi_{t}^{k} (f(y))-\xi_{r}^{k}(g(x))\xi_{t}^{k}(g(y))\big{|}\,\Big{)}\] \[\leq\sum_{k=1}^{K}\Big{(}L_{\xi_{r}^{k}}\left|f(x)-g(x)\right| \big{|}\xi_{t}^{k}(f(y))\big{|}+\big{|}\xi_{r}^{k}(g(x))\big{|}\,L_{\xi_{t}^{k }}\left|f(y)-g(y)\right|\Big{)}.\]

Hence,

\[\|\Phi_{f}-\Phi_{g}\|_{L^{1}[0,1]^{2}}\] \[\leq\sum_{k=1}^{K}\int_{0}^{1}\int_{0}^{1}\Big{(}L_{\xi_{r}^{k}} \left|f(x)-g(x)\right|\big{|}\xi_{t}^{k}(f(y))\big{|}+\big{|}\xi_{r}^{k}(g(x) )\big{|}\,L_{\xi_{t}^{k}}\left|f(y)-g(y)\right|\Big{)}dxdy\] \[\leq\sum_{k=1}^{K}\Big{(}L_{\xi_{r}^{k}}\|f-g\|_{1}\|\xi_{t}^{k} \|_{\infty}+\|\xi_{r}^{k}\|_{\infty}L_{\xi_{t}^{k}}\|f-g\|_{1}\Big{)}\] \[=\sum_{k=1}^{K}\Big{(}L_{\xi_{r}^{k}}\|\xi_{t}^{k}\|_{\infty}+\| \xi_{r}^{k}\|_{\infty}L_{\xi_{t}^{k}}\Big{)}\|f-g\|_{1}.\]

**Lemma F.2**.: _Let \(Q,V\) be two message kernels, and \(W\in\mathcal{W}_{0}\). Then_

\[\|\mathrm{Agg}(W,Q)-\mathrm{Agg}(W,V)\|_{1}\leq\|Q-V\|_{1}.\]

Proof.: \[\mathrm{Agg}(W,Q)(x)-\mathrm{Agg}(W,V)(x)=\int_{0}^{1}W(x,y)(Q(x,y)-V(x,y))dy\]

So

\[\|\mathrm{Agg}(W,Q)-\mathrm{Agg}(W,V)\|_{1} =\int_{0}^{1}\left|\int_{0}^{1}W(x,y)(Q(x,y)-V(x,y))dy\right|dx\] \[\leq\int_{0}^{1}\int_{0}^{1}\left|W(x,y)(Q(x,y)-V(x,y))\right|dydx\] \[\leq\int_{0}^{1}\int_{0}^{1}\left|(Q(x,y)-V(x,y))\right|dydx=\|Q- V\|_{1}.\]

As a result of Lemma F.2 and the product rule Lemma F.1, we have the following corollary, that computes the error in aggregating two message kernels with the same graphon.

**Corollary F.3**.: \[\|\mathrm{Agg}(W,\Phi_{f})-\mathrm{Agg}(W,\Phi_{g})\|_{1}\leq\sum_{k=1}^{K} \Big{(}L_{\xi_{r}^{k}}\|\xi_{t}^{k}\|_{\infty}+\|\xi_{r}^{k}\|_{\infty}L_{\xi_ {t}^{k}}\Big{)}\|f-g\|_{1}.\]Next we fix the message kernel, and bound the difference between the aggregation of the message kernel with respect to two different graphons. Let \(L^{+}[0,1]\) be the space of measurable function \(f:[0,1]\to[0,1]\). The following lemma is a trivial extension of [24, Lemma 8.10] from \(\mathcal{K}_{1}\) to \(\mathcal{K}_{r}\).

**Lemma F.4**.: _For any kernel \(Q\in\mathcal{K}_{r}\)_

\[\|Q\|_{\Box}=\sup_{f,g\in L^{+}[0,1]}\left|\int_{[0,1]^{2}}f(x)Q(x,y)g(y)dxdy \right|,\]

_where the supremum is attained for some \(f,g\in L^{+}[0,1]\)._

The following Lemma is proven as part of the proof of [24, Lemma 8.11].

**Lemma F.5**.: _For any kernel \(Q\in\mathcal{K}_{r}\)_

\[\sup_{f,g\in L^{\infty}_{1}[0,1]}\left|\int_{[0,1]^{2}}f(x)Q(x,y)g(y)dxdy\right| \leq 4\|Q\|_{\Box}.\]

For completeness, we give here a self-contained proof.

Proof.: Any function \(f\in L^{\infty}_{1}[0,1]\) can be written as \(f=f_{+}-f_{-}\), where \(f_{+},f_{-}\in L^{+}[0,1]\). Hence, by Lemma F.4,

\[\sup_{f,g\in L^{\infty}_{1}[0,1]}\left|\int_{[0,1]^{2}}f(x)Q(x,y)g( y)dxdy\right|\] \[=\sup_{f_{+},f_{-},g_{+},g_{-}\in L^{+}[0,1]}\left|\int_{[0,1]^{2} }(f_{+}(x)-f_{-}(x))Q(x,y)(g_{+}(y)-g_{-}(y))dxdy\right|\] \[\leq\sum_{s\in\{+,-\}}\sup_{f_{s},g_{s}\in L^{+}[0,1]}\left|\int_{ [0,1]^{2}}f_{s}(x)Q(x,y)g_{s}(y)dxdy\right|=4\|Q\|_{\Box}.\]

Next we state a simple lemma.

**Lemma F.6**.: _Let \(f=f_{+}-f_{-}\) be a signal, where \(f_{+},f_{-}:[0,1]\to(0,\infty)\) are measurable. Then the supremum in the cut norm \(\|f\|_{\Box}=\sup_{S\subset[0,1]}\left|\int_{S}f(x)dx\right|\) is attained as the support of either \(f_{+}\) or \(f_{-}\)._

**Lemma F.7**.: _Let \(f\in\mathcal{L}^{\infty}_{r}[0,1]\), \(W,V\in\mathcal{W}_{0}\), and suppose that \(\left|\xi^{k}_{\mathrm{r}}(f(x))\right|,\left|\xi^{k}_{\mathrm{t}}(f(x)) \right|\leq\rho\) for every \(x\in[0,1]\) and \(k=1,\ldots,K\). Then_

\[\|\mathrm{Agg}(W,\Phi_{f})-\mathrm{Agg}(V,\Phi_{f})\|_{\Box}\leq 4K\rho^{2}\|W -V\|_{\Box}.\]

_Moreover, if \(\xi^{k}_{\mathrm{r}}\) and \(\xi^{k}_{\mathrm{t}}\) are non-negatively valued for every \(k=1,\ldots,K\), then_

\[\|\mathrm{Agg}(W,\Phi_{f})-\mathrm{Agg}(V,\Phi_{f})\|_{\Box}\leq K\rho^{2}\|W -V\|_{\Box}.\]

Proof.: Let \(T=W-V\). Let \(S\) be the maximizer of the supremum underlying the cut norm of \(\mathrm{Agg}(T,\Phi_{f})\). Suppose without loss of generality that \(\int_{S}\mathrm{Agg}(T,\Phi_{f})(x)dx>0\). Denote \(q^{k}_{r}(x)=\xi^{k}_{\mathrm{r}}(f(x))\) and \(q^{k}_{\mathrm{t}}(x)=\xi^{k}_{\mathrm{t}}(f(x))\). We have

\[\int_{S}\big{(}\mathrm{Agg}(W,\Phi_{f})(x)-\mathrm{Agg}(V,\Phi_{f })(x)\big{)}dx =\int_{S}\mathrm{Agg}(T,\Phi_{f})(x)dx\] \[=\sum_{k=1}^{K}\int_{S}\int_{0}^{1}q^{k}_{\mathrm{r}}(x)T(x,y)q^{k }_{\mathrm{t}}(y)dydx.\]

Let

\[v^{k}_{\mathrm{r}}(x)=\left\{\begin{array}{cc}q^{k}_{\mathrm{r}}(x)/\rho&x \in S\\ 0&x\notin S.\end{array}\right. \tag{25}\]Moreover, define \(v_{\mathrm{t}}^{k}=q_{\mathrm{t}}^{k}/\rho\), and note that \(v_{\mathrm{r}}^{k},v_{\mathrm{t}}^{k}\in L_{1}^{\infty}[0,1]\). We hence have, by Lemma F.5,

\[\int_{S}\mathrm{Agg}(T,\Phi_{f})(x)dx =\sum_{k=1}^{K}\rho^{2}\int_{0}^{1}\int_{0}^{1}v_{\mathrm{r}}^{k}( x)T(x,y)v_{\mathrm{t}}^{k}(y)dydx\] \[\leq\sum_{k=1}^{K}\rho^{2}\left|\int_{0}^{1}\int_{0}^{1}v_{\mathrm{ r}}^{k}(x)T(x,y)v_{\mathrm{t}}^{k}(y)dydx\right|\] \[\leq 4K\rho^{2}\|T\|_{\square}.\]

Hence,

\[\|\mathrm{Agg}(W,\Phi_{f})-\mathrm{Agg}(V,\Phi_{f})\|_{\square}\leq 4K\rho^{2} \|T\|_{\square}\]

Lastly, in case \(\xi_{\mathrm{r}}^{k},\xi_{\mathrm{t}}^{k}\) are nonnegatively valued, so are \(q_{\mathrm{r}}^{k},q_{\mathrm{t}}^{k}\), and hence by Lemma F.4,

\[\int_{S}\mathrm{Agg}(T,\Phi_{f})(x)dx\leq K\rho^{2}\|T\|_{\square}.\]

**Theorem F.8**.: _Let \((W,f),(V,g)\in\mathcal{WL}_{r}\), and suppose that \(\left|\xi_{\mathrm{r}}^{k}(f(x))\right|,\left|\xi_{\mathrm{t}}^{k}(f(x))\right|\leq\rho\) and \(L_{\xi_{\mathrm{t}}^{k}},L_{\xi_{\mathrm{t}}^{k}}<L\) for every \(x\in[0,1]\) and \(k=1,\ldots,K\). Then,_

\[\|\mathrm{Agg}(W,\Phi_{f})-\mathrm{Agg}(V,\Phi_{g})\|_{\square}\leq 4KL\rho \|f-g\|_{\square}+4K\rho^{2}\|W-V\|_{\square}.\]

Proof.: By Lemma F.1, Lemma F.2 and Lemma F.7,

\[\|\mathrm{Agg}(W,\Phi_{f})-\mathrm{Agg}(V,\Phi_{g})\|_{\square}\] \[\leq\|\mathrm{Agg}(W,\Phi_{f})-\mathrm{Agg}(W,\Phi_{g})\|_{ \square}+\|\mathrm{Agg}(W,\Phi_{g})-\mathrm{Agg}(V,\Phi_{g})\|_{\square}\] \[\leq\sum_{k=1}^{K}\Big{(}L_{\xi_{\mathrm{r}}^{k}}\|\xi_{\mathrm{t }}^{k}\|_{\infty}+\|\xi_{\mathrm{r}}^{k}\|_{\infty}L_{\xi_{\mathrm{t}}^{k}} \Big{)}\|f-g\|_{1}+4K\rho^{2}\|W-V\|_{\square}\] \[\leq 4KL\rho\|f-g\|_{\square}+4K\rho^{2}\|W-V\|_{\square}.\]

Lastly, we show that update layers are Lipschitz continuous. Since the update function takes two functions \(f:[0,1]\to\mathbb{R}^{d_{i}}\) (for generally two different output dimensions \(d_{1},d_{2}\)), we "concatenate" these two inputs and treat it as one input \(f:[0,1]\to\mathbb{R}^{d_{1}+d_{2}}\).

**Lemma F.9**.: _Let \(\eta:\mathbb{R}^{d+p}\to\mathbb{R}^{s}\) be Lipschitz with Lipschitz constant \(L_{\eta}\), and let \(f,g\in\mathcal{L}_{r}^{\infty}[0,1]\) with values in \(\mathbb{R}^{d+p}\) for some \(d,p\in\mathbb{N}\)._

_Then_

\[\|\eta(f)-\eta(g)\|_{1}\leq L_{\eta}\|f-g\|_{1}.\]

Proof.: \[\|\eta(f)-\eta(g)\|_{1} =\int_{0}^{1}\left|\eta\big{(}f(x)\big{)}-\eta\big{(}g(x)\big{)} \right|dx\] \[\leq\int_{0}^{1}L_{\eta}\left|f(x)-g(x)\right|dx=L_{\eta}\|f-g\|_ {1}.\]

### Bounds of signals and MPLs with Lipschitz message and update functions

We will consider three settings for the MPNN Lipschitz bounds. In all settings, the transmitter, receiver, and update functions are Lipschitz. In the first setting all message and update functions are assumed to be bounded. In the second setting, there is no additional assumption over Lipschitzness of the transmitter, receiver, and update functions. In the third setting, we assume that the message function \(\Phi\) is also Lipschitz with Lipschitz bound \(L_{\Phi}\), and that all receiver and transmitter functionsare non-negatively bounded (e.g., via an application of ReLU or sigmoid in their implementation). Note that in case \(K=1\) and all functions are differentiable, by the product rule, \(\Phi\) can be Lipschitz only in two cases: if both \(\xi_{r}\) and \(\xi_{t}\) are bounded and Lipschitz, or if either \(\xi_{r}\) or \(\xi_{t}\) is constant, and the other function is Lipschitz. When \(K>1\), we can have combinations of these cases.

We next derive bounds for the different settings. A bound for setting 1 is given in Theorem F.8. Moreover, When the receiver and transmitter message functions and the update functions are bounded, so is the signal at each layer.

#### Bounds for setting 2.

Next we show boundedness when the receiver and transmitter message and update functions are only assumed to be Lipschitz.

Define the _formal bias_\(B_{\xi}\) of a function \(\xi:\mathbb{R}^{d_{1}}\to\mathbb{R}^{d_{2}}\) to be \(\xi(0)\)[26]. We note that the formal bias of an affine-linear operator is its classical bias.

**Lemma F.10**.: _Let \((W,f)\in\mathcal{WL}_{r}\), and suppose that for every \(\mathrm{y}\in\{\mathrm{r},\mathrm{t}\}\) and \(k=1,\ldots,K\)_

\[\left|\xi_{\mathrm{y}}^{k}(0)\right|\leq B,\quad L_{\xi_{\mathrm{y}}^{k}}<L.\]

_Then,_

\[\|\xi_{\mathrm{y}}^{k}\circ f\|_{\infty} \leq Lr+B\]

_and_

\[\|\mathrm{Agg}(W,\Phi_{f})\|_{\infty} \leq K(Lr+B)^{2}.\]

Proof.: Let \(\mathrm{y}\in\{\mathrm{r},\mathrm{t}\}\). We have

\[\left|\xi_{\mathrm{y}}^{k}(f(x))\right|\leq\left|\xi_{\mathrm{y}}^{k}(f(x))- \xi_{\mathrm{y}}^{k}(0)\right|+B\leq L_{\xi_{\mathrm{y}}^{k}}\left|f(x)\right| +B\leq Lr+B,\]

so,

\[\left|\mathrm{Agg}(W,\Phi_{f})(x)\right| =\left|\sum_{k=1}^{K}\int_{0}^{1}\xi_{\mathrm{r}}^{k}(f(x))W(x,y) \xi_{\mathrm{t}}^{k}(f(y))dy\right|\] \[\leq K(Lr+B)^{2}.\]

Next, we have a direct result of Theorem F.8.

**Corollary F.11**.: _Suppose that for every \(\mathrm{y}\in\{\mathrm{r},\mathrm{t}\}\) and \(k=1,\ldots,K\)_

\[\left|\xi_{\mathrm{y}}^{k}(0)\right|\leq B,\quad L_{\xi_{\mathrm{y}}^{k}}<L.\]

_Then, for every \((W,f),(V,g)\in\mathcal{WL}_{r}\),_

\[\|\mathrm{Agg}(W,\Phi_{f})-\mathrm{Agg}(V,\Phi_{g})\|_{\square}\leq 4K(L^{2}r+ LB)\|f-g\|_{\square}+4K(Lr+B)^{2}\|W-V\|_{\square}.\]

#### Bound for setting 3.

**Lemma F.12**.: _Let \((W,f)\in\mathcal{WL}_{r}\), and suppose that_

\[|\Phi(0,0)|<B,\quad L_{\Phi}<L.\]

_Then,_

\[\|\Phi_{f}\|_{\infty}\leq Lr+B\]

_and_

\[\|\mathrm{Agg}(W,\Phi_{f})\|_{\infty}\leq Lr+B.\]

Proof.: We have

\[|\Phi(f(x),f(y))|\leq|\Phi(f(x),f(y))-\Phi(0,0)|+B\leq L_{\Phi}\left|(f(x),f(y) )\right|+B\leq Lr+B,\]

so,

\[\left|\mathrm{Agg}(W,\Phi_{f})(x)\right| =\left|\int_{0}^{1}W(x,y)\Phi(f(x),f(y))dy\right|\] \[\leq Lr+B.\]

**Additional bounds.**

**Lemma F.13**.: _Let \(f\) be a signal, \(W,V\in\mathcal{W}_{0}\), and suppose that \(\|\Phi_{f}\|_{\infty}\leq\rho\) for every \(k=1,\ldots,K\), and that \(\xi_{\mathrm{r}}^{k}\) and \(\xi_{\mathrm{t}}^{k}\) are non-negatively valued. Then_

\[\|\mathrm{Agg}(W,\Phi_{f})-\mathrm{Agg}(V,\Phi_{f})\|_{\square}\leq K\rho\|W-V \|_{\square}.\]

Proof.: The proof follows the steps of Lemma F.7 until (25), from where we proceed differently. Since all of the functions \(q_{\mathrm{r}}^{k}\) and \(q_{\mathrm{t}}^{k}\), \(k\in[K]\), and since \(\|\Phi_{f}\|_{\infty}\leq\rho\), the product of each \(q_{\mathrm{r}}^{k}(x)q_{\mathrm{t}}^{k}(y)\) must be also bounded by \(\rho\) for every \(x\in[0,1]\) and \(k\in[K]\). Hence, we may replace the normalization in (25) with

\[v_{\mathrm{r}}^{k}(x)=\left\{\begin{array}{cc}q_{\mathrm{r}}^{k}(x)/\rho_{ \mathrm{r}}^{k}&x\in S\\ 0&x\notin S\end{array}\right.,\quad v_{\mathrm{t}}^{k}(y)=\left\{\begin{array}[ ]{cc}q_{\mathrm{t}}^{k}(y)/\rho_{\mathrm{t}}^{k}&y\in S\\ 0&y\notin S,\end{array}\right.\]

where for every \(k\in[K]\), \(\rho_{\mathrm{r}}^{k}\rho_{\mathrm{t}}^{k}=\rho\). This guarantees that \(v_{\mathrm{r}}^{k},v_{\mathrm{t}}^{k}\in L_{1}^{\infty}[0,1]\). Hence,

\[\int_{S}\mathrm{Agg}(T,\Phi_{f})(x)dx=\sum_{k=1}^{K}\int_{0}^{1} \int_{0}^{1}\rho_{\mathrm{r}}^{k}v_{\mathrm{r}}^{k}(x)T(x,y)\rho_{\mathrm{t}}^ {k}v_{\mathrm{t}}^{k}(y)dydx\] \[\qquad\leq\sum_{k=1}^{K}\rho\left|\int_{0}^{1}\int_{0}^{1}v_{ \mathrm{r}}^{k}(x)T(x,y)v_{\mathrm{t}}^{k}(y)dydx\right|\leq K\rho\|T\|_{ \square}.\]

**Theorem F.14**.: _Let \((W,f),(V,g)\in\mathcal{WL}_{r}\), and suppose that \(\|\Phi\|_{\infty,\|}\xi_{\mathrm{r}}^{k}\|_{\infty},\|\xi_{\mathrm{t}}^{k}\|_{ \infty}\leq\rho\), all message functions \(\xi\) are non-negative valued, and \(L_{\xi_{\mathrm{t}}^{k}},L_{\xi_{\mathrm{t}}^{k}}<L\), for every \(k=1,\ldots,K\). Then,_

\[\|\mathrm{Agg}(W,\Phi_{f})-\mathrm{Agg}(V,\Phi_{g})\|_{\square}\leq 4KL\rho\|f-g \|_{\square}+K\rho\|W-V\|_{\square}.\]

The proof follows the steps of Theorem F.8.

**Corollary F.15**.: _Suppose that for every \(\mathrm{y}\in\{\mathrm{r},\mathrm{t}\}\) and \(k=1,\ldots,K\)_

\[\left|\Phi(0,0)\right|,\left|\xi_{\mathrm{y}}^{k}(0)\right|\leq B,\quad L_{ \phi},L_{\xi_{\mathrm{y}}^{k}}<L,\]

_and \(\xi,\Phi\) are all non-negatively valued. Then, for every \((W,f),(V,g)\in\mathcal{WL}_{r}\),_

\[\|\mathrm{Agg}(W,\Phi_{f})-\mathrm{Agg}(V,\Phi_{g})\|_{\square}\leq 4K(L^{2}r+ LB)\|f-g\|_{\square}+K(Lr+B)\|W-V\|_{\square}.\]

The proof follows the steps of Corollary F.11.

### Lipschitz continuity theorems for MPNNs

The following recurrence sequence will govern the propagation of the Lipschitz constant of the MPNN and the bound of signal along the layers.

**Lemma F.16**.: _Let \(\mathbf{a}=(a_{1},a_{2},\ldots)\) and \(\mathbf{b}=(b_{1},b_{2},\ldots)\). The solution to \(e_{t+1}=a_{t}e_{t}+b_{t}\), with initialization \(e_{0}\), is_

\[e_{t}=Z_{t}(\mathbf{a},\mathbf{b},e_{0}):=\prod_{j=0}^{t-1}a_{j}e_{0}+\sum_{j= 1}^{t-1}\prod_{i=1}^{j-1}a_{t-i}b_{t-j}, \tag{26}\]

_where, by convention,_

\[\prod_{i=1}^{0}a_{t-i}:=1.\]

_In case there exist \(a,b\in\mathbb{R}\) such that \(a_{i}=a\) and \(b_{i}=b\) for every \(i\),_

\[e_{t}=a^{t}e_{0}+\sum_{j=0}^{t-1}a^{j}b.\]

**Setting 1**.: _Let \(\Theta\) be a MPNN with \(T\) layers. Suppose that for every layer and every \(\mathrm{y}\) and \(k\),_

\[\|^{t}\xi_{\mathrm{y}}^{k}\|_{\infty},\;\|h^{t}\|_{\infty}\leq\rho,\quad L_{\eta^ {t}},L_{\iota\xi_{\mathrm{y}}^{k}}<L.\]

_Let \((W,f),(V,g)\in\mathcal{WL}_{r}\). Then, for MPNN with no update function_

\[\|\Theta_{t}(W,f)-\Theta_{t}(V,g)\|_{\square}\leq(4KL\rho)^{t}\|f-g\|_{\square} +\sum_{j=0}^{t-1}(4KL\rho)^{j}4K\rho^{2}\|W-V\|_{\square},\]

_and for MPNN with update function_

\[\|\Theta_{t}(W,f)-\Theta_{t}(V,g)\|_{\square}\leq(4KL^{2}\rho)^{t}\|f-g\|_{ \square}+\sum_{j=0}^{t-1}(4KL^{2}\rho)^{j}4K\rho^{2}L\|W-V\|_{\square}.\]

Proof.: We prove for MPNNs with update function, where the proof without update function is similar. We can write a recurrence sequence for a bound \(\|\Theta_{t}(W,f)-\Theta_{t}(V,g)\|_{\square}\leq e_{t}\), by Theorem F.8 and Lemma F.9, as

\[e_{t+1}=4KL^{2}\rho e_{t}+4K\rho^{2}L\|W-V\|_{\square}.\]

The proof now follows by applying Lemma F.16 with \(a=4KL^{2}\rho\) and \(b=4K\rho^{2}L\). 

**Setting 2**.: _Let \(\Theta\) be a MPNN with \(T\) layers. Suppose that for every layer \(t\) and every \(\mathrm{y}\in\{\mathrm{r},\mathrm{t}\}\) and \(k\in[K]\),_

\[\left|\eta^{t}(0)\right|,\;\left|{}^{t}\xi_{\mathrm{y}}^{k}(0)\right|\leq B, \quad L_{\eta^{t}},\;L_{\iota\xi_{\mathrm{y}}^{k}}<L\]

_with \(L,B>1\). Let \((W,f)\in\mathcal{WL}_{r}\). Then, for MPNN without update function, for every layer \(t\),_

\[\|\Theta_{t}(W,f)\|_{\infty}\leq(2KL^{2}B^{2})^{2^{t}}\|f\|_{\infty}^{2^{t}},\]

_and for MPNN with update function, for every layer \(t\),_

\[\|\Theta_{t}(W,f)\|_{\infty}\leq(2KL^{3}B^{2})^{2^{t}}\|f\|_{\infty}^{2^{t}},\]

Proof.: We first prove for MPNNs without update functions. Denote by \(C_{t}\) a bound on \(\|^{t}f\|_{\infty}\), and let \(C_{0}\) be a bound on \(\|f\|_{\infty}\). By Lemma F.10, we may choose bounds such that

\[C_{t+1}\leq K(LC_{t}+B)^{2}=KL^{2}C_{t}^{2}+2KLBC_{t}+KB^{2}.\]

We can always choose \(C_{t},K,L>1\), and therefore,

\[C_{t+1}\leq KL^{2}C_{t}^{2}+2KLBC_{t}+KB^{2}\leq 2KL^{2}B^{2}C_{t}^{2}.\]

Denote \(a=2KL^{2}B^{2}\). We have

\[C_{t+1} =a(C_{t})^{2}=a(aC_{t-1}^{2})^{2}=a^{1+2}C_{t-1}^{4}=a^{1+2}(a(C_ {t-2})^{2})^{4}\] \[=a^{1+2+4}(C_{t-2})^{8}=a^{1+2+4+8}(C_{t-3})^{16}\leq a^{2^{t}}C_ {0}^{2^{t}}.\]

Now, for MPNNs with update function, we have

\[C_{t+1} \leq LK(LC_{t}+B)^{2}+B\] \[=KL^{3}C_{t}^{2}+2KL^{2}BC_{t}+KB^{2}L+B\] \[\leq 2KL^{3}B^{2}C_{t}^{2},\]

and we proceed similarly.

**Theorem F.19**.: _Let \(\Theta\) be a MPNN with \(T\) layers. Suppose that for every layer \(t\) and every \(\mathrm{y}\in\{\mathrm{r},\mathrm{t}\}\) and \(k\in[K]\),_

\[\left|\eta^{t}(0)\right|,\ \left|{}^{t}\xi_{\mathrm{y}}^{k}(0)\right|\leq B, \quad L_{\eta^{t}},\ L_{t\xi_{\mathrm{y}}^{k}}<L,\]

_with \(L,B>1\). Let \((W,g),(V,g)\in\mathcal{WL}_{r}\). Then, for MPNNs without update functions_

\[\|\Theta_{t}(W,f)-\Theta_{t}(V,g)\|_{\square} \leq\prod_{j=0}^{t-1}4K(L^{2}r_{j}+LB)\|f-g\|_{\square}\] \[\quad+\sum_{j=1}^{t-1}\prod_{i=1}^{j-1}4K(L^{2}r_{t-i}+LB)4K(Lr_{ t-j}+B)^{2}\|W-V\|_{\square},\]

_where_

\[r_{i}=(2KL^{2}B^{2})^{2^{i}}\|f\|_{\infty}^{2^{i}},\]

_and for MPNNs with update functions_

\[\|\Theta_{t}(W,f)-\Theta_{t}(V,g)\|_{\square} \leq\prod_{j=0}^{t-1}4K(L^{3}r_{j}+L^{2}B)\|f-g\|_{\square}\] \[\quad+\sum_{j=1}^{t-1}\prod_{i=1}^{j-1}4K(L^{3}r_{t-i}+L^{2}B)4KL (Lr_{t-j}+B)^{2}\|W-V\|_{\square},\]

_where_

\[r_{i}=(2KL^{3}B^{2})^{2^{i}}\|f\|_{\infty}^{2^{i}}.\]

Proof.: We prove for MPNNs without update functions. The proof for the other case is similar. By Corollary F.11, since the signals at layer \(t\) are bounded by

\[r_{t}=(2KL^{2}B^{2})^{2^{t}}\|f\|_{\infty}^{2^{t}},\]

we have

\[\|\Theta_{t+1}(W,f)-\Theta_{t+1}(V,g)\|_{\square}\] \[\leq 4K(L^{2}r_{t}+LB)\|\Theta_{t}(W,f)-\Theta_{t}(V,g)\|_{ \square}+4K(Lr_{t}+B)^{2}\|W-V\|_{\square}.\]

We hence derive a recurrence sequence for a bound \(\|\Theta_{t}(W,f)-\Theta_{t}(V,g)\|_{\square}\leq e_{t}\), as

\[e_{t+1}=4K(L^{2}r_{t}+LB)e_{t}+4K(Lr_{t}+B)^{2}\|W-V\|_{\square}.\]

We now apply Lemma F.16. 

**Setting 3**.: _Suppose that for every layer \(t\) and every \(\mathrm{y}\in\{\mathrm{r},\mathrm{t}\}\) and \(k=1,\ldots,K\),_

\[\left|\eta^{t}(0)\right|,\ \left|\Phi^{t}(0,0)\right|,\ \left|{}^{t}\xi_{ \mathrm{y}}^{k}(0)\right|\leq B,\quad L_{\eta^{t}},\ L_{\Phi^{t}},\ L_{t\xi_{ \mathrm{y}}^{k}}<L,\]

_and \(\xi,\Phi\) are all non-negatively valued. Then, for MPNNs without update function_

\[\|\Theta^{t}(W,f)\|_{\infty}\leq L^{t}\|f\|_{\infty}+\sum_{j=1}^{t-1}L^{j}B,\]

_and for MPNNs with update function_

\[\|\Theta^{t}(W,f)\|_{\infty}\leq L^{2t}\|f\|_{\infty}+\sum_{j=1}^{t-1}L^{2j}( LB+B),\]

Proof.: We first prove for MPNNs without update functions. By Lemma F.10, there is a bound \(e_{t}\) of \(\|\Theta^{t}(W,f)\|_{\infty}\) that satisfies

\[e_{t}=Le_{t-1}+B.\]

Solving this recurrence sequence via Lemma F.16 concludes the proof.

Lastly, for MPNN with update functions, we have a bound that satisfies

\[e_{t}=L^{2}e_{t-1}+LB+B,\]

and we proceed as before.

**Lemma F.21**.: _Suppose that for every \(\mathrm{y}\in\{\mathrm{r},\mathrm{t}\}\) and \(k=1,\ldots,K\)_

\[\big{|}\eta^{t}(0)\big{|}\,,\ \left|\Phi(0,0)\right|,\big{|}\xi_{\mathrm{y}}^{k}(0) \big{|}\leq B,\quad L_{\Phi},L_{\xi_{\mathrm{y}}^{k}}<L,\]

_and \(\xi,\Phi\) are all non-negatively valued. Let \((W,g),(V,g)\in\mathcal{WL}_{r}\). Then, for MPNNs without update functions_

\[\|\Theta^{t}(W,\Phi_{f})-\Theta^{t}(V,\Phi_{g})\|_{\square}=O(K^{t}L^{2t+t^{2} }r^{t}B^{t})\Big{(}\|W-V\|_{\square}+\|f-g\|_{\square}\Big{)},\]

_and for MPNNs with update functions_

\[\|\Theta^{t}(W,\Phi_{f})-\Theta^{t}(V,\Phi_{g})\|_{\square}=O(K^{t}L^{3t+2t^{2 }}r^{t}B^{t})\Big{(}\|W-V\|_{\square}+\|f-g\|_{\square}\Big{)}\]

Proof.: We start with MPNNs without update functions. By Corollary F.15 and Lemma F.20, there is a bound \(e_{t}\) on the error \(\|\Theta^{t}(W,\Phi_{f})-\Theta^{t}(V,\Phi_{g})\|_{\square}\) at step \(t\) that satisfies

\[e_{t}=4K(L^{2}r_{t-1}+LB)e_{t-1}+K(Lr+B)\|W-V\|_{\square}\] \[=4K\Big{(}L^{2}\big{(}L^{t}\|f\|_{\infty}+\sum_{j=1}^{t-1}L^{j}B \big{)}+LB\Big{)}e_{t-1}+K\Big{(}L\big{(}L^{t}\|f\|_{\infty}+\sum_{j=1}^{t-1}L ^{j}B\big{)}+B\Big{)}\|W-V\|_{\square}.\]

Hence, by Lemma F.16, and \(Z\) defined by (26),

\[e_{t}=Z_{t}(\mathbf{a},\mathbf{b},\|f-g\|_{\square})=O(K^{t}L^{2t+t^{2}}r^{t}B^ {t})\big{(}\|f-g\|_{\square}+\|W-V\|_{\square}\big{)},\]

where in the notations of Lemma F.16,

\[a_{t}=4K\Big{(}L^{2}(L^{t}\|f\|_{\infty}+\sum_{j=1}^{t-1}L^{j}B)+LB\Big{)}\]

and

\[b_{t}=K\Big{(}L(L^{t}\|f\|_{\infty}+\sum_{j=1}^{t-1}L^{j}B)+B\Big{)}\|W-V\|_{ \square}.\]

Next, for MPNNs with update functions, there is a bound that satisfies

\[e_{t} =4K(L^{3}r_{t-1}+L^{2}B)e_{t-1}+K(L^{2}r+LB)\|W-V\|_{\square}\] \[=4K\Big{(}L^{3}\big{(}L^{2t}\|f\|_{\infty}+\sum_{j=1}^{t-1}L^{2j }(LB+B)\big{)}+L^{2}B\Big{)}e_{t-1}\] \[\quad+K\Big{(}L^{2}\big{(}L^{2t}\|f\|_{\infty}+\sum_{j=1}^{t-1}L^ {2j}(LB+B)\big{)}+LB\Big{)}\|W-V\|_{\square}.\]

Hence, by Lemma F.16, and \(Z\) defined by (26),

\[e_{t}=O(K^{t}L^{3t+2t^{2}}r^{t}B^{t})\big{(}\|f-g\|_{\square}+\|W-V\|_{\square }\big{)}.\]

## Appendix G Generalization bound for MPNNs

In this appendix we prove Theorem 4.2.

### Statistical learning and generalization analysis

In the statistical setting of learning, we suppose that the dataset comprises independent random samples from a probability space that describes all possible data \(\mathcal{P}\). We suppose that for each \(x\in\mathcal{P}\) there is a ground truth value \(y_{x}\in\mathcal{Y}\), e.g., the ground truth class or value of \(x\), where \(\mathcal{Y}\) is, in general, some measure space. The _loss_ is a measurable function \(\mathcal{L}:\mathcal{Y}^{2}\to\mathbb{R}_{+}\) that definessimilarity in \(\mathcal{Y}\). Given a measurable function \(\Theta:\mathcal{P}\to\mathcal{Y}\), that we call the _model_ or _network_, its accuracy on all potential inputs is defined as the _statistical risk_\(R_{\mathrm{stat}}(\Theta)=\mathbb{E}_{x\sim\mathcal{P}}\Big{(}\mathcal{L}( \Theta(x),y_{x})\Big{)}\). The goal in learning is to find a network \(\Theta\), from some _hypothesis space_\(\mathcal{T}\), that has a low statistical risk. In practice, the statistical risk cannot be computed analytically. Instead, we suppose that a dataset \(\mathcal{X}=\{x_{m}\}_{m=1}^{M}\subset\mathcal{P}\) of \(M\in\mathbb{N}\) random independent samples with corresponding values \(\{y_{m}\}_{m=1}^{M}\subset\mathcal{Y}\) is given. We estimate the statistical risk via a "Monte Carlo approximation," called the _empirical risk_\(R_{\mathrm{emp}}(\Theta)=\frac{1}{M}\sum_{m=1}^{M}\mathcal{L}(\Theta(x_{m}),y_ {m})\). The network \(\Theta\) is chosen in practice by optimizing the empirical risk. The goal in generalization analysis is to show that if a learned \(\Theta\) attains a low empirical risk, then it is also guaranteed to have a low statistical risk.

One technique for bounding the statistical risk in terms of the empirical risk is to use the bound \(R_{\mathrm{stat}}(\Theta)\leq R_{\mathrm{emp}}(\Theta)+E\), where \(E\) is the _generalization error_\(E=\sup_{\Theta\in\mathcal{T}}|R_{\mathrm{stat}}(\Theta)-R_{\mathrm{emp}}(\Theta)|\), and to find a bound for \(E\). Since the trained network \(\Theta=\Theta_{\mathcal{X}}\) depends on the data \(\mathcal{X}\), the network is not a constant when varying the dataset, and hence the empirical risk is not really a Monte Carlo approximation of the statistical risk in the learning setting. If the network \(\Theta\) was fixed, then Monte Carlo theory would have given us a bound of \(E^{2}\) of order \(O\big{(}\kappa(p)/M\big{)}\) in an event of probability \(1-p\), where, for example, in Hoeffding's inequality Theorem G.2, \(\kappa(p)=\log(2/p)\). Let us call such an event a _good sampling event_. Since the good sampling event depends on \(\Theta\), computing a naive bound to the generalization error would require intersecting all good sampling events for all \(\Theta\in\mathcal{T}\). Uniform convergence bounds are approaches for intersecting adequate sampling events that allow bounding the generalization error more efficiently. This intersection of events leads to a term in the generalization bound, called the _complexity/capacity_, that describes the richness of the hypothesis space \(\mathcal{T}\). This is the philosophy behind approaches such as VC-dimension, Rademacher dimension, fat-shattering dimension, pseudo-dimension, and uniform covering number (see, e.g., [34]).

### Classification setting

We define a ground truth classifier into \(C\) classes as follows. Let \(\mathcal{C}:\widehat{\mathcal{WL}_{r}}\to\mathbb{R}^{C}\) be a measurable piecewise constant function of the following form. There is a partition of \(\mathcal{WL}_{r}\) into disjoint measurable sets \(B_{1},\ldots,B_{C}\subset\widehat{\mathcal{WL}_{r}}\) such that \(\bigcup_{i=1}^{C}B_{i}=\widehat{\mathcal{WL}_{r}}\), and for every \(i\in[C]\) and every \(x\in B_{i}\),

\[\mathcal{C}(x)=e_{i},\]

where \(e_{i}\in\mathbb{R}^{C}\) is the standard basis element with entries \((e_{i})_{j}=\delta_{i,j}\), where \(\delta_{i,j}\) is the Kronecker delta.

We define an arbitrary data distribution as follows. Let \(\mathcal{B}\) be the Borel \(\sigma\)-algebra of \(\widehat{\mathcal{WL}_{r}}\), and \(\nu\) be any probability measure on the measurable space \((\widehat{\mathcal{WL}_{r}},\mathcal{B})\). We may assume that we complete \(\mathcal{B}\) with respect to \(\nu\), obtaining the \(\sigma\)-algebra \(\Sigma\). If we do not complete the measure, we just denote \(\Sigma=\mathcal{B}\). Defining \((\widehat{\mathcal{WL}_{r}},\Sigma,\nu)\) as a complete measure space or not will not affect our construction.

Let \(\mathcal{S}\) be a metric space. Let \(\mathrm{Lip}(\mathcal{S},L)\) be the space of Lipschitz continuous mappings \(\Upsilon:\mathcal{S}\to\mathbb{R}^{C}\) with Lipschitz constant \(L\). Note that by Theorem 4.1, for every \(i\in[C]\), the space of MPNN with Lipschitz continuous input and output message functions and Lipschitz update functions, restricted to \(B_{i}\), is a subset of \(\mathrm{Lip}(B_{i},L_{1})\) which is the restriction of \(\mathrm{Lip}(\widehat{\mathcal{WL}_{r}},L_{1})\) to \(B_{i}\subset\widehat{\mathcal{WL}_{r}}\), for some \(L_{1}>0\). Moreover, \(B_{i}\) has finite covering \(\kappa(\epsilon)\) given in (23). Let \(\mathcal{E}\) be a Lipschitz continuous loss function with Lipschitz constant \(L_{2}\). Therefore, since \(\mathcal{C}|_{B_{i}}\) is in \(\mathrm{Lip}(B_{i},0)\), for any \(\Upsilon\in\mathrm{Lip}(\widehat{\mathcal{WL}_{r}},L_{1})\), the function \(\mathcal{E}(\Upsilon|_{B_{i}},\mathcal{C}|_{B_{i}})\) is in \(\mathrm{Lip}(B_{i},L)\) with \(L=L_{1}L_{2}\).

### Uniform Monte Carlo approximation of Lipschitz continuous functions

The proof of Theorem 4.2 is based on the following Theorem G.3, which studies uniform Monte Carlo approximations of Lipschitz continuous functions over metric spaces with finite covering.

**Definition G.1**.: _A metric space \(\mathcal{M}\) is said to have covering number \(\kappa:(0,\infty)\to\mathbb{N}\), if for every \(\epsilon>0\), the space \(\mathcal{M}\) can be covered by \(\kappa(\epsilon)\) ball of radius \(\epsilon\)._

**Theorem G.2** (Hoeffding's Inequality).: _Let \(Y_{1},\ldots,Y_{N}\) be independent random variables such that \(a\leq Y_{i}\leq b\) almost surely. Then, for every \(k>0\),_

\[\mathbb{P}\Big{(}\Big{|}\frac{1}{N}\sum_{i=1}^{N}(Y_{i}-\mathbb{E}[Y_{i}])\Big{|} \geq k\Big{)}\leq 2\exp\Big{(}-\frac{2k^{2}N}{(b-a)^{2}}\Big{)}.\]

The following theorem is an extended version of [26, Lemma B.3], where the difference is that we use a general covering number \(\kappa(\epsilon)\), where in [26, Lemma B.3] the covering number is exponential in \(\epsilon\). For completion, we repeat here the proof, with the required modification.

**Theorem G.3** (Uniform Monte Carlo approximation for Lipschitz continuous functions).: _Let \(\mathcal{X}\) be a probability metric space5, with probability measure \(\mu\), and covering number \(\kappa(\epsilon)\). Let \(X_{1},\ldots,X_{N}\) be drawn i.i.d. from \(\mathcal{X}\). Then, for every \(p>0\), there exists an event \(\mathcal{E}^{p}_{\mathrm{Lip}}\subset\mathcal{X}^{N}\) (regarding the choice of \((X_{1},\ldots,X_{N})\)), with probability_

Footnote 5: A metric space with a probability Borel measure, where we either take the completion of the measure space with respect to \(\mu\) (adding all subsets of null-sets to the \(\sigma\)-algebra) or not.

\[\mu^{N}(\mathcal{E}^{p}_{\mathrm{Lip}})\geq 1-p,\]

_such that for every \((X_{1},\ldots,X_{N})\in\mathcal{E}^{p}_{\mathrm{Lip}}\), for every bounded Lipschitz continuous function \(F:\mathcal{X}\to\mathbb{R}^{d}\) with Lipschitz constant \(L_{F}\), we have_

\[\left\|\int F(x)d\mu(x)-\frac{1}{N}\sum_{i=1}^{N}F(X_{i})\right\|_{\infty}\leq 2 \xi^{-1}(N)L_{f}+\frac{1}{\sqrt{2}}\xi^{-1}(N)\|F\|_{\infty}(1+\sqrt{\log(2/p)}), \tag{27}\]

_where \(\xi(r)=\frac{\kappa(r)^{2}\log(\kappa(r))}{r^{2}}\) and \(\xi^{-1}\) is the inverse function of \(\xi\)._

Proof.: Let \(r>0\). There exists a covering of \(\mathcal{X}\) by a set of balls \(\{B_{j}\}_{j\in[J]}\) of radius \(r\), where \(J=\kappa(r)\). For \(j=2,\ldots,J\), we define \(I_{j}:=B_{j}\setminus\cup_{i<j}B_{i}\), and define \(I_{1}=B_{1}\). Hence, \(\{I_{j}\}_{j\in[J]}\) is a family of measurable sets such that \(I_{j}\cap I_{i}=\emptyset\) for all \(i\neq j\in[J]\), \(\bigcup_{j\in[J]}I_{j}=\chi\), and \(\mathrm{diam}(I_{j})\leq 2r\) for all \(j\in[J]\), where by convention \(\mathrm{diam}(\emptyset)=0\). For each \(j\in[J]\), let \(z_{j}\) be the center of the ball \(B_{j}\).

Next, we compute a concentration of error bound on the difference between the measure of \(I_{j}\) and its Monte Carlo approximation, which is uniform in \(j\in[J]\). Let \(j\in[J]\) and \(q\in(0,1)\). By Hoeffding's inequality Theorem G.2, there is an event \(\mathcal{E}^{q}_{j}\) with probability \(\mu(\mathcal{E}^{q}_{j})\geq 1-q\), in which

\[\left\|\frac{1}{N}\sum_{i=1}^{N}\mathds{1}_{I_{j}}(X_{i})-\mu(I_{k})\right\|_ {\infty}\leq\frac{1}{\sqrt{2}}\frac{\sqrt{\log(2/q)}}{\sqrt{N}}. \tag{28}\]

Consider the event

\[\mathcal{E}^{Jq}_{\mathrm{Lip}}=\bigcap_{j=1}^{J}\mathcal{E}^{q}_{j},\]

with probability \(\mu^{N}(\mathcal{E}^{Jq}_{\mathrm{Lip}})\geq 1-Jq\). In this event, (28) holds for all \(j\in\mathcal{J}\). We change the failure probability variable \(p=Jq\), and denote \(\mathcal{E}^{p}_{\mathrm{Lip}}=\mathcal{E}^{Jq}_{\mathrm{Lip}}\).

Next we bound uniformly the Monte Carlo approximation error of the integral of bounded Lipschitz continuous functions \(F:\chi\to\mathbb{R}^{F}\). Let \(F:\chi\to\mathbb{R}^{F}\) be a bounded Lipschitz continuous function with Lipschitz constant \(L_{F}\). We define the step function

\[F^{r}(y)=\sum_{j\in[J]}F(z_{j})\mathds{1}_{I_{j}}(y).\]Then,

\[\begin{split}\left\|\frac{1}{N}\sum_{i=1}^{N}F(X_{i})-\int_{\chi}F(y)d \mu(y)\right\|_{\infty}&\leq\left\|\frac{1}{N}\sum_{i=1}^{N}F(X_{i} )-\frac{1}{N}\sum_{i=1}^{N}F^{r}(X_{i})\right\|_{\infty}\\ &+\left\|\frac{1}{N}\sum_{i=1}^{N}F^{r}(X_{i})-\int_{\chi}F^{r}(y) d\mu(y)\right\|_{\infty}\\ &+\left\|\int_{\chi}F^{r}(y)d\mu(y)-\int_{\chi}F(y)d\mu(y)\right\|_{ \infty}\\ &=:(1)+(2)+(3).\end{split} \tag{29}\]

To bound (1), we define for each \(X_{i}\) the unique index \(j_{i}\in[J]\) s.t. \(X_{i}\in I_{j_{i}}\). We calculate,

\[\begin{split}\left\|\frac{1}{N}\sum_{i=1}^{N}F(X_{i})-\frac{1}{N }\sum_{i=1}^{N}F^{r}(X_{i})\right\|_{\infty}&\leq\!\frac{1}{N} \sum_{i=1}^{N}\left\|F(X_{i})-\sum_{j\in\mathcal{J}}F(z_{j})\mathds{1}_{I_{j}} (X_{i})\right\|_{\infty}\\ &=\!\frac{1}{N}\sum_{i=1}^{N}\left\|F(X_{i})-F(z_{j_{i}})\right\| _{\infty}\\ &\leq rL_{F}.\end{split}\]

We proceed by bounding (2). In the event of \(\mathcal{E}_{\mathrm{Lip}}^{p}\), which holds with probability at least \(1-p\), equation (28) holds for all \(j\in\mathcal{J}\). In this event, we get

\[\begin{split}\left\|\frac{1}{N}\sum_{i=1}^{N}F^{r}(X_{i})-\int_{ \chi}F^{r}(y)d\mu(y)\right\|_{\infty}&=\left\|\sum_{j\in[J]} \left(\frac{1}{N}\sum_{i=1}^{N}F(z_{j})\mathds{1}_{I_{j}}(X_{i})-\int_{I_{j}}F (z_{j})dy\right)\right\|_{\infty}\\ &\leq\sum_{j\in[J]}\|F\|_{\infty}\left|\frac{1}{N}\sum_{i=1}^{N} \mathds{1}_{I_{j}}(X_{i})-\mu(I_{j})\right|\\ &\leq J\|F\|_{\infty}\frac{1}{\sqrt{2}}\frac{\sqrt{\log(2J/p)}}{ \sqrt{N}}.\end{split}\]

Recall that \(J=\kappa(r)\). Then, with probability at least \(1-p\)

\[\begin{split}&\left\|\frac{1}{N}\sum_{i=1}^{N}F^{r}(X_{i})-\int_{ \chi}F^{r}(y)d\mu(y)\right\|_{\infty}\\ &\leq\kappa(r)\|F\|_{\infty}\frac{1}{\sqrt{2}}\frac{\sqrt{\log( \kappa(r))+\log(2/p)}}{\sqrt{N}}.\end{split}\]

To bound (3), we calculate

\[\begin{split}\left\|\int_{\mathcal{X}}F^{r}(y)d\mu(y)-\int_{ \mathcal{X}}F(y)d\mu(y)\right\|_{\infty}&=\left\|\int_{\chi}\sum _{j\in[J]}F(z_{j})\mathds{1}_{I_{j}}d\mu(y)-\int_{\chi}F(y)d\mu(y)\right\|_{ \infty}\\ &\leq\sum_{j\in[J]}\int_{I_{j}}\left\|F(z_{j})-F(y)\right\|_{ \infty}d\mu(y)\\ &\leq rL_{F}.\end{split}\]By plugging the bounds of \((1),(2)\) and \((3)\) into (29), we get

\[\left\|\frac{1}{N}\sum_{i=1}^{N}F(X_{i})-\int_{\chi}F(y)d\mu(y) \right\|_{\infty} \leq 2rL_{F}+\kappa(r)\|F\|_{\infty}\frac{1}{\sqrt{2}}\frac{\sqrt{ \log(\kappa(r))+\log(2/p)}}{\sqrt{N}}\] \[\leq 2rL_{F}+\frac{1}{\sqrt{2}}\kappa(r)\|F\|_{\infty}\frac{\sqrt{ \log(\kappa(r))}+\sqrt{\log(2/p)}}{\sqrt{N}}\] \[\leq 2rL_{F}+\frac{1}{\sqrt{2}}\kappa(r)\|F\|_{\infty}\frac{\sqrt{ \log(\kappa(r))}}{\sqrt{N}}(1+\sqrt{\log(2/p)}).\]

Lastly, choosing \(r=\xi^{-1}(N)\) for \(\xi(r)=\frac{\kappa(r)^{2}\log(\kappa(r))}{r^{2}}\), gives \(\frac{\kappa(r)\sqrt{\log(\kappa(r))}}{\sqrt{N}}=r\), so

\[\left\|\frac{1}{N}\sum_{i=1}^{N}F(X_{i})-\int_{\chi}F(y)d\mu(y) \right\|_{\infty}\] \[\leq 2\xi^{-1}(N)L_{f}+\frac{1}{\sqrt{2}}\xi^{-1}(N)\|F\|_{ \infty}(1+\sqrt{\log(2/p)}).\]

Since the event \(\mathcal{E}_{\mathrm{Lip}}^{p}\) is independent of the choice of \(F:\chi\to\mathbb{R}^{F}\), the proof is finished. 

### A generalization theorem for MPNNs

The following generalization theorem of MPNN is now a direct result of Theorem G.3.

Let \(\mathrm{Lip}(\widetilde{\mathcal{WL}_{r}},L_{1})\) denote the space of Lipschitz continuous functions \(\Theta:\mathcal{WL}_{r}\to\mathbb{R}^{C}\) with Lipschitz bound bounded by \(L_{1}\) and \(\|\Theta\|_{\infty}\leq L_{1}\). We note that the theorems of Appendix F.2 prove that MPNN with Lipschitz continuous message and update functions, and bounded formal biases, are in \(\mathrm{Lip}(\widetilde{\mathcal{WL}_{r}},L_{1})\).

**Theorem G.4** (MPNN generalization theorem).: _Consider the classification setting of Appendix G.2. Let \(X_{1},\ldots,X_{N}\) be independent random samples from the data distribution \((\widetilde{\mathcal{WL}_{r}},\Sigma,\nu)\). Then, for every \(p>0\), there exists an event \(\mathcal{E}^{p}\subset\widetilde{\mathcal{WL}_{r}}^{N}\) regarding the choice of \((X_{1},\ldots,X_{N})\), with probability_

\[\nu^{N}(\mathcal{E}^{p})\geq 1-Cp-2\frac{C^{2}}{N},\]

_in which for every function \(\Upsilon\) in the hypothesis class \(\mathrm{Lip}(\widetilde{\mathcal{WL}_{r}},L_{1})\), with we have_

\[\Big{|}\mathcal{R}(\Upsilon_{\mathbf{X}})-\hat{\mathcal{R}}(\Upsilon_{\mathbf{ X}},\mathbf{X})\Big{|}\leq\xi^{-1}(N/2C)\Big{(}2L+\frac{1}{\sqrt{2}}\big{(}L+ \mathcal{E}(0,0)\big{)}\big{(}1+\sqrt{\log(2/p)}\big{)}\Big{)}, \tag{30}\]

_where \(\xi(r)=\frac{\kappa(r)^{2}\log(\kappa(r))}{r^{2}}\), \(\kappa\) is the covering number of \(\widetilde{\mathcal{WL}_{r}}\) given in (23), and \(\xi^{-1}\) is the inverse function of \(\xi\)._

Proof.: For each \(i\in[C]\), let \(S_{i}\) be the number of samples of \(\mathbf{X}\) that falls within \(B_{i}\). The random variable \((S_{1},\ldots,S_{C})\) is multinomial, with expected value \((N/C,\ldots,N/C)\) and variance \((\frac{N(C-1)}{C^{2}},\ldots,\frac{N(C-1)}{C^{2}})\leq(\frac{N}{C},\ldots, \frac{N}{C})\). We now use Chebyshev's inequality, which states that for any \(a>0\),

\[P\Big{(}\left|S_{i}-N/C\right|>a\sqrt{\frac{N}{C}}\Big{)}<a^{-2}.\]

We choose \(a\sqrt{\frac{N}{C}}=\frac{N}{2C}\), so \(a=\frac{N^{1/2}}{2C^{1/2}}\), and

\[P(\left|S_{i}-N/C\right|>\frac{N}{2C})<\frac{2C}{N}.\]

Therefore,

\[P(S_{i}>\frac{N}{2C})>1-\frac{2C}{N}.\]We intersect these events of \(i\in[C]\), and get an event \(\mathcal{E}_{\text{mult}}\) of probability more than \(1-2\frac{C^{2}}{N}\) in which \(S_{i}>\frac{N}{2C}\) for every \(i\in[C]\). In the following, given a set \(B_{i}\) we consider a realization \(M=S_{i}\), and then use the law of total probability.

From Theorem G.3 we get the following. For every \(p>0\), there exists an event \(\mathcal{E}_{i}^{p}\subset B_{i}^{M}\) regarding the choice of \((X_{1},\ldots,X_{M})\subset B_{i}\), with probability

\[\nu^{M}(\mathcal{E}_{\text{Lip}}^{p})\geq 1-p,\]

such that for every function \(\Upsilon^{\prime}\) in the hypothesis class \(\text{Lip}(\widetilde{\mathcal{W}\mathcal{L}_{r}},L_{1})\), we have

\[\left|\int\mathcal{E}\big{(}\Upsilon^{\prime}(x),\mathcal{C}(x) \big{)}d\nu(x)-\frac{1}{M}\sum_{i=1}^{M}\mathbb{E}\big{(}\Upsilon^{\prime}(X_ {i}),\mathcal{C}(X_{i})\big{)}\right| \tag{31}\] \[\leq 2\xi^{-1}(M)L+\frac{1}{\sqrt{2}}\xi^{-1}(M)\|\mathcal{E} \big{(}\Upsilon^{\prime}(\cdot),\mathcal{C}(\cdot)\big{)}\|_{\infty}(1+\sqrt{ \log(2/p)})\] (32) \[\leq 2\xi^{-1}(N/2C)L+\frac{1}{\sqrt{2}}\xi^{-1}(N/2C)(L+ \mathcal{E}(0,0))(1+\sqrt{\log(2/p)}), \tag{33}\]

where \(\xi(r)=\frac{\kappa(r)^{2}\log(\kappa(r))}{r^{2}}\), \(\kappa\) is the covering number of \(\widetilde{\mathcal{W}\mathcal{L}_{r}}\) given in (23), and \(\xi^{-1}\) is the inverse function of \(\xi\). In the last inequality, we use the bound, for every \(x\in\mathcal{W}\mathcal{L}_{r}\),

\[\big{|}\mathcal{E}\big{(}\Upsilon^{\prime}(x),\mathcal{C}(x)\big{)}\big{|} \leq\big{|}\mathcal{E}\big{(}\Upsilon^{\prime}(x),\mathcal{C}(x)\big{)}- \mathcal{E}(0,0)\big{|}+\left|\mathcal{E}(0,0)\right|\leq L_{2}\left|L_{1}-0 \right|+\left|\mathcal{E}(0,0)\right|.\]

Since (31) is true for any \(\Upsilon^{\prime}\in\text{Lip}(\widetilde{\mathcal{W}\mathcal{L}_{r}},L_{1})\), it is also true for \(\Upsilon_{\mathbf{X}}\) for any realization of \(\mathbf{X}\), so we also have

\[\Big{|}\mathcal{R}(\Upsilon_{\mathbf{X}})-\mathcal{\hat{R}}(\Upsilon_{\mathbf{ X}},\mathbf{X})\Big{|}\leq 2\xi^{-1}(N/2C)L+\frac{1}{\sqrt{2}}\xi^{-1}(N/2C)(L+ \mathcal{E}(0,0))(1+\sqrt{\log(2/p)}).\]

Lastly, we denote

\[\mathcal{E}^{p}=\mathcal{E}_{\text{mult}}\cap\Big{(}\bigcup_{i=1}^{C} \mathcal{E}_{i}^{p}\Big{)}.\]

### Experiments

The nontrivial part in our construction of the MPNN architecture is the choice of normalized sum aggregation as the aggregation method of the MPNNs. We hence show the accuracy and generalization gap of this aggregation scheme in practice in Table 1.

Most MPNNs typically use sum, mean or max aggregation. Intuitively, normalized sum aggregation is close to average aggregation, due its "normalized nature." For example, normalized sum and mean aggregations are well behaved for dense graphs with number of nodes going to infinity, while sum aggregation diverges for such graphs. Moreover, sum aggregation cannot be extended to graphons, while normalized sum and mean aggregations can. In Table 2, we first show that MPNNs with normalized sum aggregation perform well and generalize well. We then compare the normalized sum aggregation MPNNs (in rows 1 and 3 of Table 2) to baseline MPNNs with mean aggregation (rows 2 and 4 in Table 2), and show that normalized sum aggregation is not worse than mean aggregation.

The source code, courtesy of Ningyuan (Teresa) Huang, is available as part of [https://github.com/nhuang37/finegrain_expressivity_GNN](https://github.com/nhuang37/finegrain_expressivity_GNN).

## Appendix H Stability of MPNNs to graph subsampling

Lastly, we prove Theorem 4.3.

**Theorem H.1**.: _Consider the setting of Theorem 4.2, and let \(\Theta\) be a MPNN with Lipschitz constant \(L\). Denote_

\[\Sigma=\big{(}W,\Theta(W,f)\big{)},\quad\text{and}\quad\Sigma(\Lambda)=\Big{(} \mathbb{G}(W,\Lambda),\Theta\big{(}\mathbb{G}(W,\Lambda),f(\Lambda)\big{)} \Big{)}.\]

_Then_

\[\mathbb{E}\Big{(}\delta_{\square}\big{(}\Sigma,\Sigma(\Lambda)\big{)}\Big{)}< \frac{15}{\sqrt{\log(k)}}L.\]

[MISSING_PAGE_FAIL:43]

\(\|f\|_{\square}\): cut norm of a signal Definition 3.1.

\(\mathcal{W}\mathcal{L}_{r}\): graphon-signal space (page 5).

\(\|(W,f)\|_{\square}\): graphon-signal cut distance (page 5).

\(\delta_{\square}\big{(}(W,f),(V,g)\big{)}\): graphon-signal cut distance (4).

\(\widehat{\mathcal{W}\mathcal{L}_{r}}\): graphon-signal space modulo zero cut distance (page 5).

\((W,f)_{(G,f)}=(W_{G},f_{\mathbf{f}})\): induced graphon-signal Definition 3.2.

\(\mathcal{S}^{d}_{\mathcal{P}_{k}}\): the space of step functions of dimension \(d\) over the partition \(\mathcal{P}_{k}\)Definition 3.3.

\(\mathcal{W}_{0}\cap\mathcal{S}^{2}_{\mathcal{P}_{k}}\): the space of step graphons/ stochastic block models (page 6).

\(\mathcal{L}^{\infty}_{r}[0,1]\cap\mathcal{S}^{1}_{\mathcal{P}_{k}}\): the space of step signals (page 6).

\([\mathcal{W}\mathcal{L}_{r}]_{\mathcal{P}_{k}}\): the space of graphon-signal stochastic block models with respect to the partition \(\mathcal{P}_{k}\) (page 6).

\(W(\Lambda)\): random weighted graph (page 7).

\(f(\Lambda)\): random sampled signal (page 7).

\(\mathbb{G}(W,\Lambda)\): random simple graph (page 7).

\(\Phi(x,y)\): message function (page 8).

\(\xi^{k}_{r},\xi^{k}_{r}:\mathbb{R}^{d}\to\mathbb{R}^{p}\): receiver and transmitter message functions (page 8).

\(\Phi_{f}:[0,1]^{2}\to\mathbb{R}^{p}\): message kernel (page 8).

\(\mathrm{Agg}(W,Q)\): aggregation of message \(Q\) with respect to graphon \(W\) (page 8).

\(f^{(t)}\): signal at layer \(t\) (page 8).

\(\mu^{(t+1)}\): update function (page 8).

\(\Theta_{t}(W,f)\): the output of the MPNN applied on \((W,f)\in\mathcal{W}\mathcal{L}_{r}\) at layer \(t\in[T]\) (page 8).

\(\mathrm{Lip}(\widehat{\mathcal{W}\mathcal{L}_{r}},L_{1})\): the space of Lipschitz continuous mappings \(\Upsilon:\widehat{\mathcal{W}\mathcal{L}_{r}}\to\mathbb{R}^{C}\) with Lipschitz constant \(L_{1}\) (page 9).