# Provably Bounding Neural Network Preimages

 Suhas Kotha\({}^{*}\)

Carnegie Mellon

suhask@andrew.cmu.edu &Christopher Brix\({}^{*}\)

RWTH Aachen

brix@cs.rwth-aachen.de &Zico Kolter

Carnegie Mellon

Bosch Center for AI

zkolter@cs.cmu.edu &Krishnamurthy (Dj) Dvijotham\({}^{\dagger}\)

Google DeepMind

dvijothamcs@gmail.com &Huan Zhang\({}^{\dagger}\)

UIUC

huan@huan-zhang.com

###### Abstract

Most work on the formal verification of neural networks has focused on bounding the set of outputs that correspond to a given set of inputs (for example, bounded perturbations of a nominal input). However, many use cases of neural network verification require solving the inverse problem, or over-approximating the set of inputs that lead to certain outputs. We present the INVPROP algorithm for verifying properties over the preimage of a linearly constrained output set, which can be combined with branch-and-bound to increase precision. Contrary to other approaches, our efficient algorithm is GPU-accelerated and does not require a linear programming solver. We demonstrate our algorithm for identifying safe control regions for a dynamical system via backward reachability analysis, verifying adversarial robustness, and detecting out-of-distribution inputs to a neural network. Our results show that in certain settings, we find over-approximations over \(2500\times\) tighter than prior work while being \(2.5\times\) faster. By strengthening robustness verification with output constraints, we consistently verify more properties than the previous state-of-the-art on multiple benchmarks, including a large model with 167k neurons in VNN-COMP 2023. Our algorithm has been incorporated into the \(\alpha\),\(\beta\)-CROWN verifier, available at [https://abcrown.org](https://abcrown.org).

## 1 Introduction

Applying neural networks to safety-critical settings often requires reasoning about constraints on the inputs and outputs of a neural network. For example, for a physical system controlled by a neural network policy, it is of interest to understand which initial states will lead to an unsafe state such as colliding with an obstacle. Formal verification of neural networks seeks to provide provable guarantees demonstrating that networks satisfy formal specifications on their inputs and outputs. Most work to date has focused on developing algorithms that can bound the outputs of a neural network given constraints on the inputs, which can be used for applications such as analyzing the robustness of a neural network to perturbations of a given input (Wong and Kolter, 2018; Dvijotham et al., 2018; Zhang et al., 2018; Raghunathan et al., 2018; Gehr et al., 2018).

In this work, we address the inverse problem of over-approximating a neural network's preimage: given a set of outputs \(\mathcal{S}_{\text{out}}\) (described by linear constraints on the network output), we seek to find a set that provably contains all inputs that lead to such outputs. For example, for our neural network policy, this would correspond to the states that collide with an obstacle one step in the future. Though the verification problem is already challenging due to non-convexity and high dimensionality, this new problem is even more difficult since neural networks are generally not invertible.

Specifically, representative efficient verifiers (such as state-of-the-art bound-propagation-based methods (Zhang et al., 2022)) can only compute bounds utilizing constraints on the input and critically depend on having tight bounds on intermediate activations. In the setting of this paper, however, the bounds derived from the input constraints are almost vacuous, since the only constraints on the input are that it should be from the valid input domain. We efficiently solve this problem by significantly generalizing the existing bound propagation-based verification framework, allowing one to leverage output constraints when tightening the intermediate activations. Our contributions are as follows:

* We develop an effective bound propagation framework, Inverse Propagation for Neural Network Verification (INVPROP), for the _inverse verification problem_ for neural networks, i.e., the problem of over-approximating the set of inputs that leads to a given set of outputs. Importantly, INVPROP requires no linear programming solver and can compute bounds on any intermediate layer.
* We unify INVPROP and traditional bound propagation into a more general verification framework, allowing us to connect our method to standard tools, such as the state-of-the-art bound propagation tool \(\alpha\),\(\beta\)-CROWN (Zhang et al., 2018; Xu et al., 2021; Wang et al., 2021; Zhang et al., 2022; Brix et al., 2023; Muller et al., 2023). Our contribution allows \(\alpha\),\(\beta\)-CROWN to tighten intermediate bounds with respect to output constraints, which could not be done by the original tool.
* We demonstrate that tight inverse verification requires multiple iterative refinements of intermediate bounds. While layer bounds in standard bound propagation only depend on the bounds of their predecessors, INVPROP incorporates bounds of _all_ layers in the network.
* We improve the state of the art on a control benchmark (Rober et al., 2022, 2022) by providing \(2500\times\) tighter bounds, \(2.5\times\) faster, for a Double Integrator and \(257\times\) tighter bounds, \(3.29\times\) faster, for a 6D Quadrotor. Furthermore, we demonstrate that INVPROP can strengthen robustness verification with output constraints and verify more robustness properties in less time compared to existing tools. Finally, we demonstrate its applicability in OOD detection.

## 2 Background and Problem Setup

### Notation

We use \([L]\) for \(L\in\mathbb{N}\) to refer to the set \(\{1,2,\ldots,L\}\), \(\mathbf{W}_{:,j}^{(i)}\) to refer to column \(j\) of the matrix \(\mathbf{W}^{(i)}\), \([\cdot]_{+}\) to refer to \(\max(0,\cdot)\), and \([\cdot]_{-}\) to refer to \(-\min(0,\cdot)\). We use boldface symbols for vectors and matrices (such as \(\mathbf{x}^{(i)}\) and \(\mathbf{W}^{(i)}\)) and regular symbols for scalars (such as \(x_{j}^{(i)}\)). We use \(\mathbf{x}\odot\mathbf{y}\) to denote element-wise multiplication of vectors \(\mathbf{x},\mathbf{y}\).

We define an \(L\) layer ReLU neural network by its weight matrices \(\mathbf{W}^{(i)}\) and bias vectors \(\mathbf{b}^{(i)}\) for \(i\in[L]\). The output of the neural network for the input \(\hat{\mathbf{x}}^{(0)}\) from a bounded input domain \(\mathcal{X}\) is computed by alternately applying linear layers \(\mathbf{x}^{(i)}=\mathbf{W}^{(i)}\hat{\mathbf{x}}^{(i-1)}+\mathbf{b}^{(i)}\) and ReLU layers \(\hat{\mathbf{x}}^{(i)}=\max(\mathbf{0},\mathbf{x}^{(i)})\) until we receive the output \(\mathbf{x}^{(L)}\) (which we refer to as the logits). Note that we treat softmax as a component of the loss function, not the neural network.

### Problem Statement

Given a neural network \(f:\mathcal{X}\subseteq\mathbb{R}^{\text{in}}\rightarrow\mathbb{R}^{\text{out}}\) and an output constraint \(\mathcal{S}_{\text{out}}\subseteq\mathbb{R}^{\text{out}}\), we want to compute \(f^{-1}(\mathcal{S}_{\text{out}})\subseteq\mathcal{X}\). Since precisely computing or expressing \(f^{-1}(\mathcal{S}_{\text{out}})\) is an intractable problem in general, we strive to compute a tight over-approximation \(\mathcal{S}_{\text{over}}\) such that \(f^{-1}(\mathcal{S}_{\text{out}})\subseteq\mathcal{S}_{\text{over}}\). In particular, we target the convex hull of the preimage via a cutting-plane representation. \(\mathcal{S}_{\text{out}}\) will be defined by a set of linear constraints parameterized by \(\mathbf{H}f\left(\mathbf{x}\right)+\mathbf{d}\leq\mathbf{0}\) in this work.

### Applications

Backward Reachability Analysis for Neural Feedback Loops.Establishing safety guarantees for neural network policies is a challenging task. One problem of interest is to find a set of initial states that does not reach a particular set of future states under the neural network policy. This can be helpful in collision avoidance or control with safety constraints. For example, consider a discrete-time double integrator controller (Hu et al., 2020) where the state at time \(t+1\) can be directly computed based on state at time \(t\) following the equation

\[\mathbf{x}_{t+1}=f(\mathbf{x}_{t})=\begin{bmatrix}1&1\\ 0&1\end{bmatrix}\mathbf{x}_{t}+\begin{bmatrix}0.5\\ 1\end{bmatrix}\pi(\mathbf{x}_{t})\]with policy \(\pi:\mathbb{R}^{2}\rightarrow\mathbb{R}\). If there is an obstacle in the room covering the region \([4.5,5.0]\times[-0.25,0.25]\), it is of interest to understand which states will enter the unsafe region in the next time-step. We can represent this obstacle set with linear constraints that define \(\mathcal{S}_{\text{out}}\).

\(f^{-1}(\mathcal{S}_{\text{out}})\) denotes the set of states \(\mathbf{x_{t}}\) such that \(\mathbf{x_{t+1}}\) lies in the unsafe region given the control policy \(\pi\). Overapproximating this set allows us to define the set of states to avoid one timestep in advance. We can compose \(f^{-1}\) with itself \(t\) times to obtain the set of initial states where \(\pi\) would drive the system to the unsafe set after \(t\) steps.

Robustness VerificationAdversarial robustness is one classic problem for neural network verification where verifiers assess whether all perturbations of an input yield the same classification. INVPROP can be used to speed up this verification by exploiting an implicit output constraint. We follow Wang et al. (2021) and use the canonical form of verifying a property by proving that \(\min_{x\in\mathcal{X}}f(x)>0\). \(f(x)\) will be negative if and only if \(x\) is an adversarial example. This minimization problem can be rewritten as \(\inf_{x\in X\wedge f(x)\leq 0}f(x)\)(Yang et al., 2021). If no adversarial example exists, the infimum is computed over an empty set and returns positive infinity, indicating local robustness. If at least one adversarial example exists, the result is guaranteed to be negative. With the output constraint \(f(x)\geq 0\), we only analyze inputs that yield an incorrect classification, reducing the search space and tightening verification bounds.

Determining what inputs lead to confident predictions.It is challenging to train a classifier that knows when it doesn't know. A simple and effective approach is to train with an additional logit representing OOD abstention. Labeled data for this additional class is generated via outlier exposure, or adding synthetic training data known to be OOD (Hendrycks et al., 2018; Chen et al., 2021).

Consider the logits \(y\) produced by a binary classifier trained in this manner. The classifier can classify in-distribution data by comparing \(y_{0}\) and \(y_{1}\), the logits corresponding to the two in-distribution labels. The quantity \(\max(y_{0},y_{1})-y_{2}\), known as the logit gap, can be used to identify out-of-distribution data (Fig. 7 in Appendix A). Can the logit gap correctly identify data far from the training data as being OOD? To answer this question, we can compute the preimage of the set

\[\mathcal{S}_{\text{out}}=\{\mathbf{y}:\max(y_{0},y_{1})\geq y_{2}\}\]

We detail how to model this set with linear constraints in Appendix F.1. INVPROP enables us to over-approximate \(f^{-1}(\mathcal{S}_{\text{out}})\), answering whether the classifier learned to correctly identify OOD data.

## 3 Approach

### Convex over-approximation

Suppose we want to find a convex over-approximation of \(f^{-1}(\mathcal{S}_{\text{out}})\). The tightest such set is its convex hull, which is the intersection of all half-spaces that contain \(f^{-1}(\mathcal{S}_{\text{out}})\)(Boyd et al., 2004). This intersection is equivalent to \(\bigcap_{\mathbf{c}\in\mathbb{R}^{n}}\{\mathbf{x}:\mathbf{c}^{\top}\mathbf{x}\geq\min_{f(\mathbf{ x}^{\prime})\in\mathcal{S}_{\text{in}}}\mathbf{c}^{\top}\mathbf{x}^{\prime}\}\), which means that we can build an over-approximation by taking this intersection for finitely many \(\mathbf{c}\). Furthermore, replacing the minimization problem with a lower bound to its value still yields a valid half-space, where a tighter bound yields a tighter approximation.

We focus on convex over-approximations for two reasons: First, checking that the preimage satisfies a linear constraint \(\mathbf{c}^{\top}\mathbf{x}+d\geq 0\) is equivalent to minimizing the linear function \(\mathbf{x}\mapsto\mathbf{c}^{\top}\mathbf{x}\) over the preimage, which is in turn equivalent to minimizing the linear function over \(\operatorname{CONV}\left(f^{-1}(\mathcal{S}_{\text{out}})\right)\)(Boyd et al., 2004). Second, the convex hull and its tractable over-approximations are conveniently represented as intersections of linear constraints on the preimage, each of which can be quickly computed after a single precomputation phase to tighten bounds on intermediate neurons using the INVPROP algorithm outlined in the next section.

For the above reasons, we will focus on solving the following constrained optimization problem.

\[\min_{\mathbf{x}}\quad\mathbf{c}^{\top}\mathbf{x};\qquad\text{ s.t. }\mathbf{x}\in\mathcal{X};\quad f\left(\mathbf{x}\right)\in\mathcal{S}_{\text{out}} \tag{1}\]

Note that this differs from the widely studied forward verification problem, which can be phrased as

\[\min_{\mathbf{x}\in\mathcal{S}_{\text{in}}}\quad\mathbf{c}^{\top}f\left(\mathbf{x}\right)\]

where \(\mathcal{S}_{\text{in}}\) is a set representing constraints on the input, and the goal of the verification is to establish that the output \(f\left(\mathbf{x}\right)\) satisfies a linear constraint of the form \(\mathbf{c}^{\top}f\left(\mathbf{x}\right)\geq d\) for all \(x\in\mathcal{S}_{\text{in}}\).

### The INVPROP Algorithm

As a brief overview of our method, we first construct the Mixed Integer Linear Program (MILP) for solving this optimization problem, which generates the over-approximation \(\mathcal{S}_{\text{MILP}}\) when solved for finitely many \(\mathbf{c}\). Then, we relax the MILP to a Linear Program (LP), which can construct the over-approximation \(\mathcal{S}_{\text{LP}}\). Finally, we relax the LP via its Lagrangian dual, which will be used to construct our over-approximation \(\mathcal{S}_{\text{over}}\). This chain of relaxations is visualized in Figure 1.

The Mixed Integer Linear Programming (MILP) FormulationFor feed-forward ReLU neural networks, the non-linearities from the max operator can be encoded via integer variables, and this problem admits a MILP encoding similar to prior work in adversarial robustness (Tjeng et al., 2017). Problem (1) is equivalent to:

\[\min_{\mathbf{x},\hat{\mathbf{x}}} \mathbf{c}^{\top}\mathbf{x}\] (2a) s.t. \[\mathbf{x}\in\mathcal{X};\quad\hat{\mathbf{x}}^{(0)}=\mathbf{x};\quad\mathbf{ H}\mathbf{x}^{(L)}+\mathbf{d}\leq\mathbf{0}; \tag{2b}\] \[\mathbf{x}^{(i)}=\mathbf{W}^{(i)}\hat{\mathbf{x}}^{(i-1)}+\mathbf{b}^{(i )}\quad i\in[L];\] (2c) \[\hat{\mathbf{x}}^{(i)}=\max(0,\mathbf{x}^{(i)});\quad i\in[L-1] \tag{2d}\]

The Linear Programming (LP) FormulationUnfortunately, finding an exact solution to this MILP is NP-complete (Katz et al., 2017). To sidestep the intractability of exact verification, we can compute lower bounds for this program via its convex relaxation. We consider bounds on the outputs of intermediate layers:

\[l_{j}^{(i)}\leq x_{j}^{(i)}\leq u_{j}^{(i)}\]

Based on these bounds, we can take the ReLU triangle relaxation (Ehlers, 2017) to get the LP

\[\min_{\mathbf{x},\mathbf{x}} \mathbf{c}^{\top}\mathbf{x}\] (3a) s.t. \[\mathbf{x}\in\mathcal{X};\quad\mathbf{l}^{(0)}\leq\mathbf{x}\leq\mathbf{u}^{(0)}; \quad\hat{\mathbf{x}}^{(0)}=\mathbf{x};\quad\mathbf{H}\mathbf{x}^{(L)}+\mathbf{d}\leq \mathbf{0} \tag{3b}\] \[\mathbf{x}^{(i)}=\mathbf{W}^{(i)}\hat{\mathbf{x}}^{(i-1)}+\mathbf{b}^{(i)}\] (3c) \[\mathbf{0}\leq\hat{\mathbf{x}}^{(i)};\quad\mathbf{x}^{(i)}\leq\hat{\mathbf{x} }^{(i)};\quad\hat{\mathbf{x}}^{(i)}\leq\frac{\mathbf{u}^{(i)}}{\mathbf{u}^{(i)}-\mathbf{l}^{( i)}}\odot\left(\mathbf{x}^{(i)}-\mathbf{l}^{(i)}\right);\quad i\in[L] \tag{3d}\]

where the bounds \(\mathbf{l}^{(0)}\leq\mathbf{x}\leq\mathbf{u}^{(0)}\) are either the bounds implicit in \(\mathcal{X}\), or a refinement of these obtained via previous rounds of bound propagation or input branching (see Algorithm 1 for details). Most efficient neural network verifiers do not solve an LP formulation of verification directly because LP solvers are often slow for problem instances involving neural networks. Instead, the bound propagation framework (Zhang et al., 2018; Wang et al., 2021; Zhang et al., 2022) is a practical and efficient way to lower bound the LP relaxation of the forward verification problem. However, there are _two major roadblocks_ to applying existing methods here: Typical bound-propagation cannot directly handle the output constraints (Eq. 3b) and the objective involving input layer variables (Eq. 3a). This is true for optimizing bounds on the input, intermediate layers, and the output layer, all of which need to be iteratively tightened as demonstrated later.

Figure 1: **Visualization of relaxations. The inner green region depicts the true \(f^{-1}(\mathcal{S}_{\text{out}})\), the blue relaxation depicts the intersection of finite half-spaces solved via MILP, the red relaxation displays the same via LP, and the purple relaxation displays the same via INVPROP. Though this diagram displays looseness, we provide a comprehensive methodology to reduce the error in all three relaxations up to arbitrary precision (Section 3.3).**The Inverse Propagation (INVPROP) FormulationBy changing the LP above to optimize the quantity \(\hat{x}_{j}^{(0)}\) (input layer) or \(x_{j}^{(i)}\) (intermediate layers) for \(i\in[L-1]\), the bounds for the \(j\)-th neuron of layer \(i\) can be tightened in separate LP calls. However, this program is too expensive to be run multiple times for each neuron.

Inspired by the success of CROWN-family neural network verifiers [22, 23, 24], we efficiently lower bound the solution of the LP by optimizing its Lagrangian dual. This dual is highly structured [20], allowing us to bound input half-spaces \(\mathbf{c}^{\top}\mathbf{x}\) and intermediate bounds \(l_{j}^{(i)},u_{j}^{(i)}\) by closed-form expressions of the dual variables. Our main generalization is the ability to optimize input or intermediate layer bounds with the output constraints **after** them in the neural network, as shown in the following theorem.

**Theorem 1** (Bounding input half-spaces).: _Given an output set \(\mathcal{S}_{\text{out}}=\{\mathbf{y}:\mathbf{H}\mathbf{y}+\mathbf{d}\leq\mathbf{0}\}\) and vector \(\mathbf{c}\), \(g_{\text{c}}(\mathbf{\alpha},\mathbf{\gamma})\) is a lower bound to the linear program in (3) for \(\mathbf{0}\leq\mathbf{\alpha}\leq\mathbf{1}\), \(\mathbf{\gamma}\geq\mathbf{0}\), and \(g_{\text{c}}\) defined via_

\[g_{\text{c}}(\mathbf{\alpha},\mathbf{\gamma})= \left[\mathbf{c}^{\top}-\mathbf{\nu}^{(1)\top}\mathbf{W}^{(1)}\right]_{+} \mathbf{l}^{(0)}-\left[\mathbf{c}^{\top}-\mathbf{\nu}^{(1)\top}\mathbf{W}^{(1)}\right]_{- }\mathbf{u}^{(0)}\] \[-\sum_{i=1}^{L}\mathbf{\nu}^{(i)\top}\mathbf{b}^{(i)}+\sum_{i=1}^{L-1} \sum_{j\in\mathcal{I}^{\pm(i)}}\left[\frac{u_{j}^{(i)}l_{j}^{(i)}[\hat{\nu}_{j} ^{(i)}]_{+}}{u_{j}^{(i)}-l_{j}^{(i)}}\right]\]

_where every term can be directly recursively computed via_

\[\mathcal{I}^{-(i)} =\{j:u_{j}^{(i)}\leq 0\};\quad\mathcal{I}^{+(i)}=\{j:l_{j}^{(i)} \geq 0\};\quad\mathcal{I}^{\pm(i)}=\{j:l_{j}^{(i)}<0<u_{j}^{(i)}\}\] \[\mathbf{\nu}^{(L)} =-\mathbf{\gamma};\quad\hat{\nu}_{j}^{(i)}=\mathbf{\nu}^{(i+1)\top} \mathbf{W}_{:,j}^{(i+1)}\quad\forall i\in[L-1]\] \[\nu_{j}^{(i)} =\left\{\begin{array}{ll}\hat{\nu}_{j}^{(i)},&j\in\mathcal{I}^{ +(i)}\\ 0,&j\in\mathcal{I}^{-(i)}\\ \frac{u_{j}^{(i)}}{u_{j}^{(i)}-l_{j}^{(i)}}[\hat{\nu}_{j}^{(i)}]_{+}-\alpha_{j }^{(i)}[\hat{\nu}_{j}^{(i)}]_{-},&j\in\mathcal{I}^{\pm(i)}\end{array}\right.\]

Proof.: Full proof is presented in Appendix D. 

In Appendix C, we show how to bound intermediate neurons in the network using a similar approach. Since the intermediate bounds might depend on bounds on neurons in subsequent layers (due to the output constraint), we cannot simply optimize bounds in a single forward pass layer by layer, unlike prior work such as \(\alpha\)-CROWN [23]. Instead, we must iteratively tighten intermediate layer bounds with respect to the tightest bounds on all neurons computed thus far. This iterative approach can tighten the initially loose bounds by several orders of magnitude, as shown in Figure 2. After performing this procedure once, the intermediate bounds can be used to tightly lower bound \(\mathbf{c}^{\top}\mathbf{x}\) for any \(\mathbf{c}\) via Theorem 1. Therefore, this computation can be shared across all the constraints \(\mathbf{c}\) we use to describe \(\mathcal{S}_{\text{over}}\). Our algorithm can be expressed in terms of forward/backward passes through layers of the neural network and implemented via standard deep learning modules in libraries like PyTorch [24]. Since all of the operations are auto-differentiable, we can tighten our lower bound using standard gradient ascent (projected by the dual variable constraints).

Figure 2: **Necessity of Iterative Tightening.** Our approach enables us to iteratively tighten the bounds of all layers, with each iteration allowing for a smaller approximation ratio with respect to the true preimage. Green: Sum of bound intervals for all neurons in the third layer (second hidden layer); Blue: ratio between volumes of over-approximation and preimage. Measured for step \(t=10\) of the control benchmark defined in Section 4.1. Note that the improvement from iterative tightening is two orders of magnitude for intermediate bounds, and four orders of magnitude for the volume of the over-approximation.

**Algorithm 1** INVPROP. Can be applied to branches for non-convex overapproximations. Lower and upper bound of all neurons and all constraints are optimized using distinct \(\alpha\) and \(\gamma\) values in \(g\).

Initialize \(\mathbf{l}^{(i)},\mathbf{u}^{(i)}\) via cheap bound propagation methods for the forward verification problem while lower bounds for \(\mathbf{c}^{\top}\hat{\mathbf{x}}^{(0)}\) are improving do for\(i\in\{L-1,L-2,\ldots,1,0\}\)do for\(j\in\) layer \(i\) neurons, \(b\in\) {lower, upper} do  Optimize \(g^{b}_{ij}\) from Theorem 2 via gradient ascent to improve bound on neuron \(j\) in layer \(i\) with sense \(b\) (lower/upper) if\(b=\)upper then update \(u^{(i)}_{j}\) else update \(l^{(i)}_{j}\) Optimize \(g_{\mathbf{c}}\) from Theorem 2 via gradient ascent to improve lower bound on \(\mathbf{c}^{\top}\mathbf{x}\) for all \(\mathbf{c}\)

**Connection to forward verification** Our bound in Theorem 1 introduces the dual variable \(\mathbf{\gamma}\), which enforces the output constraint during optimization. In fact, we can use this variable to get a better conceptual interpretation of our result. For the optimization problem in (1), taking the dual with respect to the constraint \(\mathbf{H}f\left(\mathbf{x}\right)+\mathbf{d}\leq\mathbf{0}\) yields the lower bound

\[\max_{\mathbf{\gamma}}\min_{\mathbf{x}} \mathbf{c}^{\top}\mathbf{x}+\mathbf{\gamma}^{\top}\left(\mathbf{H}f\left(\mathbf{ x}\right)+\mathbf{d}\right)\] s.t. \[\mathbf{x}\in\mathcal{X};\quad\mathbf{\gamma}\geq 0\]

The objective can be represented as minimizing a linear function of the output of a residual neural network with a skip connection from the input to the output layer, subject to constraints on the input. Now, \(f(\mathbf{x})\) appears in the objective, similar to the standard forward verification problem with an augmented network architecture. Similarly, optimizing an intermediate bound is equivalent to a skip connection from layer \(i\) to the output (Appendix E). These connections allow us to implement our method using standard verification tools (Brix et al., 2023; Xu et al., 2021, 2020).

When \(f\) is a feedforward ReLU network, the lower bound described in this section is precisely the same as Theorem 1 (since both are solving the dual of the same linear program). This shows that the introduction of \(\mathbf{\gamma}\) constitutes our generalization to the bound propagation framework.

Selection of \(\mathbf{c}\)The dependence of the optimization on \(\mathbf{c}\) is not a major limitation. The bounds of all intermediate neurons are optimized only with respect to the input and output constraints, not the hyperplanes used to describe the input half-space. Thus, their optimized bounds can be shared across the optimization of all input hyperplanes. For applications such as adversarial robustness, the box constraint of two hyperplanes per dimension could be chosen to scalably bound the input. The selection of appropriate \(\mathbf{c}\) depends on the application and is not our main focus.

### Branch and Bound

Though more rounds of iterative tightening will lower the gap in \(\mathcal{S}_{\text{over}}\supseteq\mathcal{S}_{\text{LP}}\), our current formulation still faces two sources of looseness: the gap in \(\mathcal{S}_{\text{LP}}\supseteq\mathcal{S}_{\text{MILP}}\) and the gap in \(\mathcal{S}_{\text{MILP}}\supseteq f^{-1}(\mathcal{S}_{\text{out}})\). To overcome both of these issues, we can make use of branching (Bunel et al., 2020). While there are several possibilities here, we focus on input branching, which gave the biggest empirical gains in our experiments described in Section 4. More concretely, we divide the input domain \(\mathcal{X}=[\mathbf{l}^{(0)},\mathbf{u}^{(0)}]\) into several regions by choosing a coordinate \(i\) and computing

\[\mathcal{X}_{a}=\mathcal{X}\cap\{\mathbf{x}:\mathbf{x}_{i}\geq s_{i}\},\mathcal{X}_{b} =\mathcal{X}\cap\{\mathbf{x}:\mathbf{x}_{i}\leq s_{i}\}\]

so that \(\mathcal{X}=\mathcal{X}_{a}\cup\mathcal{X}_{b}\). Doing this recursively, we obtain several regions and can compute an overapproximation of \(f^{-1}(\mathcal{S}_{\text{out}})\) when the input is in each of those regions, and take a union of the resulting sets. The finer the input domain of our over-approximation, the tighter the approximation.

## 4 Results

We evaluate INVPROP on three different benchmarks. Across benchmarks, we find orders of magnitude improvement over prior work, and our methods work even for relatively large networks (167k neurons) and high dimensionality inputs (8112 dimensions). All implementation details are described in Appendix F and the utilized hardware is described in Appendix G.

[MISSING_PAGE_FAIL:7]

ReBreach-LP with 15625 output partitions. We run our algorithm with no branching. In Figure 4, we compare these three strategies, and we find that our over-approximation of the pre-image (in blue) is smaller than the state-of-the-art over-approximations by \(257\) times while being \(3.29\) times faster. Therefore, our algorithm scales well to higher dimensional control examples.

### Robustness Verification

Similar to us, Yang et al. (2021) encode the implicit output constraint of the local robustness verification query to tighten the bounds on neurons. However, they need an LP solver, which does not scale to large problems. The state-of-the-art verification toolkit \(\alpha\),\(\beta\)-CROWN (Zhang et al., 2018; Xu et al., 2021; Wang et al., 2021; Zhang et al., 2022) is able to scale to large networks, but does not currently utilize the implicit output constraint. We demonstrate the benefit of encoding the output constraint by extending \(\alpha\),\(\beta\)-CROWN and comparing the performance on the benchmark used by Yang et al. (2021) as well as benchmarks from the VNN-COMP 2023 (Brix et al., 2023).

MnistYang et al. (2021) provides four networks with ReLU activation functions and dense linear layers of 2/6/6/9 layers of 100/100/200/200 neurons, each trained on MNIST. For each network, 50 inputs were tested for local robustness. For the complete benchmark definition, we refer to Yang et al. (2021). The DeepSRGR results are taken from Yang et al. (2021), they do not report a timeout for their experiments. Both \(\alpha\),\(\beta\)-CROWN and \(\alpha\),\(\beta\)-CROWN+INVPROP were run with a per-input timeout of 5 minutes. Except for the input bounds, all bounds of all layers of each network are tightened using output constraints. We report the results in Table 3. Notably, we can verify more instances than the SOTA tool \(\alpha\),\(\beta\)-CROWN, in sometimes less than one fifth the average runtime. We include a more detailed comparison between the results for \(\alpha\),\(\beta\)-CROWN and \(\alpha\),\(\beta\)-CROWN+INVPROP in Appendix H.

Vnn-Comp '23: YoloIn 2023, the VNN-COMP contained YOLO as an unsocored benchmark for object detection. It is a modified version of YOLOv2 (Redmon and Farhadi, 2016) with a network of 167k neurons and 5 residual layers, and is available at (Zhong, 2023; Brix, 2023). The network processes \(3\times 52\times 52\) images and uses convolutions, average pooling and ReLU activations. This benchmark is well suited for INVPROP, as the definition of the adversarial examples is a conjunction of constraints over the output. Therefore, a strong output constraint can be used to tighten the bounds of intermediate layers. The benchmark consists of 464 instances, of which 72 were randomly selected for the VNN-COMP. For our comparison, we remove those 348 instances that \(\alpha\),\(\beta\)-CROWN verifies as robust without tightening the bounds of any intermediate layer. Those instances are verified within

\begin{table}
\begin{tabular}{l c c c} \hline \hline Method & ReBreach-LP & ReBreach-LP & INVPROP \\ \hline Partitions & N/A & 15625 & N/A \\ Approx Vol \(\downarrow\) & 64 & 0.064 & 0.0000249 \\ Time (sec) & 11.8 \(\pm\) 2.2 & 662.2 \(\pm\) 31.4 & 213.8 \(\pm\) 11.1 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Quadrotor Results.** We compare the tightness of the over-approximations and run time (mean and std taken over 5 runs).

Figure 4: **6D Quadrotor.** We visualize the three dimensions of the 6D quadrotor that specify physical position. We consider the initial state range of the black box and simulate a few one-step trajectories, indicated by the orange lines. Our obstacle is the red box. For the three images, the blue box represents the over-approximation computed by (a) ReBreach-LP with no partitioning, (b) ReBreach-LP with 15625 partitions, and (c) INVPROP, respectively. A tighter blue box is better.

less than four seconds each by both methods, with no room for improvement. We compare the performance of \(\alpha,\beta\)-CROWN and \(\alpha,\beta\)-CROWN + INVPROP on the remaining 116 instances in Figure 5. \(\alpha,\beta\)-CROWN can verify 48 instances, all other instances reach the timeout of five minutes. After extending \(\alpha,\beta\)-CROWN with INVPROP on the last two intermediate layers, almost all instances can be solved faster, and 6 previously timed out instances become verifiable.

### OOD Detection

Consider the calibrated OOD detector presented as discussed in Section 2.3, encoded by a four layer MLP with \(200\), \(200\), \(3\), and \(2\) neurons. We over-approximate the set of inputs which induce a sufficiently high in-distribution (ID) confidence (measured by \(\max\{y_{0},y_{1}\}>y_{2}\)) using 40 hyperplanes of equal slope, pictured in green in Figure 6. This set is non-convex, making the convex hull a poor over-approximation. With 4 input space branches, we get a much tighter over-approximation, as shown in the right plot. We compare the performance of our approach with and without branching over the input space with the MILP baseline (see Table 4). This demonstrates a simple proof-of-concept for how INVPROP can be used for verifying some calibration properties.

## 5 Related Work

Formally Verified Neural Networks.There has been a large body of work on the formal verification of neural networks, tackling the problem from the angles of Interval Bound Propagation [11, 12], Convex Relaxations [13, 14, 15, 16], Abstract Interpretation [20], LP [1], SDP [13], SMT [15], and MILP [21]. However, most of this work is for forward verification (i.e., bounding the NN output given a

\begin{table}
\begin{tabular}{l c c c} \hline \hline Method & Input & Approx & \\ Branch. & Ratio & Time (sec) \\ \hline MILP & no & 1.47 & 1562.26 \\ INVPROP & no & 4.39 & \(3.77\pm 0.02\) \\ INVPROP & yes & 1.14 & \(12.02\pm 0.06\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: **OOD Results.** Comparison of methods for over-approximating the preimage of the OOD region (Figure 6). A lower approximation ratio (Approx Rat.) is better.

Figure 5: **YOLO Results.** Runtime comparison of \(\alpha,\beta\)-CROWN and \(\alpha,\beta\)-CROWN+INVPROP on the YOLO benchmark (167k neurons and 5 residual layers). For the comparison, only those test instances were used that could not immediately be verified by \(\alpha,\beta\)-CROWN without any iterative tightening of intermediate layer bounds. \(\alpha,\beta\)-CROWN+INVPROP can verify more properties and is faster for almost all instances.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Network & \multicolumn{2}{c}{DeepSRGR} & \multicolumn{2}{c}{\(\alpha,\beta\)-CROWN} & \multicolumn{2}{c}{\(\alpha,\beta\)-CROWN + INVPROP} \\  & verif. & avg. time [s] & verif. & avg. time [s] & verif. & avg. time [s] \\ \hline FFN4 (3x100) & 35 & 781 & 45 & 7.4 & 45 & 6.4 \\ FFN5 (6x100) & 31 & 1689 & 38 & 6.7 & 39 & 8.7 \\ FFN6 (6x200) & 31 & 6178 & 29 & 5.4 & 30 & 1.0 \\ FFN7 (9x200) & 36 & 8960 & 37 & 3.2 & 40 & 1.9 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **MNIST Results.** Robustness verification with DeepSRGR (output constraints via LP solver), \(\alpha,\beta\)-CROWN (GPU support) and \(\alpha,\beta\)-CROWN+INVPROP (output constraints with GPU support)

Figure 6: **OOD Detection.** Green: Empirical ID inputs (1 million samples). Blue: border of the verified preimage. Branching is union of preimages for each of 4 quadrants.

set of inputs). Our work strictly generalizes bound propagation as presented in Xu et al. 2021 since setting \(\mathbf{\gamma}=\mathbf{0}\) in Theorem 1 recovers its results.

Formal Analysis of the Input DomainPrior work has studied trying to determine the inverse of a neural network [Kindermann and Linden, 1990, Saad and Wunsch, 2007], though these methods are prohibitively slow. Zhang et al. [2018b] used an LP solver to compute changes to a given input that would lead to a change in the classification result while adhering to additional criteria. Yang et al. [2021b] encode the overapproximated network in an LP problem to tighten bounds by incorporating the output constraint. They require an LP solver and we compare our performance against theirs in Section 4.2. Zhong et al. [2023], Wu et al. [2022] similarly tighten intermediate bounds by using an LP solver. Dimitrov et al. [2022] compute input regions that only consist of adversarial examples (i.e., adhere to a given output constraint) that are maximized using an LP solver. Finally, a concurrent work Zhang et al. [2023] targets under- and over-approximating the preimage via LiRPA-based forward verification [Xu et al., 2020] with input-space and ReLU-space partitioning; since our work can be viewed as a generalization of LiPRA, their work may benefit from our results of tightening the intermediate layer bounds using output constraints during bound propagation.

Formal Verification for Neural Feedback Loops.Our control application was motivated by the growing body of work on backward reachability analysis and over-approximating states that result in a target set [Everett et al., 2021]. The original method solely utilized input constraints for deriving intermediate bounds. The later development of Everett et al. 2022 improved upon this by optimizing \(\mathbf{l}^{(0)}\) and \(\mathbf{u}^{(\mathbf{0})}\) with respect to output bounds but their implementation faces numerical instability when applied for many iterations. We also support the partitioning over the input space introduced by Rober et al. 2022a. The work of Vincent and Schwager 2021 explores utilizing an LP with complete neuron branching to verify control safety, which can be viewed as a domain-specific implementation of our MILP formulation of the inverse verification problem.

Certified OOD detection.There is a wide variety of OOD detection systems [Yang et al., 2021a, Salehi et al., 2022]. Commonly, they are evaluated empirically based on known OOD data [Wang et al., 2022, Liang et al., 2018, Chen et al., 2021]. Therefore, they fail to provide verified guarantees for their effectiveness. In fact, many OOD detection systems are susceptible to adversarial attacks [Sehwag et al., 2019, Chen et al., 2022]. Meinke et al. [2021] show how to verify robustness to adversarial examples around given input samples. Berrada et al. [2021] develop a general framework for certifying properties of output distributions of neural networks given constraints on the input distribution. However, this work is still constrained to verifying properties of the outputs given constraints (albeit probabilistic) on the inputs, and INVPROP is able to certify arbitrary regions of the input space that lead to confident predictions.

## 6 Discussion

We present the challenge of over-approximating neural network preimages and provide an efficient algorithm to solve it. By doing so, we demonstrate strong performance on multiple application areas. We believe there is a large scope for future investigation and new applications of INVPROP.

LimitationsFor higher-dimensional instances, our method would best work for box over-approximations (\(2d\) hyperplanes in \(d\) dimensions) and would struggle at more complicated shapes such as spheres. We expect more complex problems will benefit from a domain-specific strategy. Moreover, iteratively refining all of the intermediate bounds utilizing INVPROP faces a quadratic dependence on network depth, as opposed to a linear dependence in traditional forward verification. To mitigate this, output constraints could only be applied to layers close to the output, as we used for robustness. Finally, our branching strategy might not scale to higher dimensions, though this trade-off is well-studied [Bunel et al., 2020, Wang et al., 2021, Palma et al., 2023].

Potential Negative Social ImpactOur work improves reliable ML through facilitating systems that provably align with practitioner expectations and we expect INVPROP to have positive societal impact. We acknowledge that our improvements may be repurposed as model attacks, though we believe the positive use cases of our technique greatly outweigh current speculation of misusage.

AcknowledgementsWe thank Michael Everett and Nicholas Rober for helpful discussion and feedback on the paper. Huan Zhang acknowledges the support of the Schmidt Futures AI2050 Early Career Fellowship.

## References

* Berrada et al. (2021) Leonard Berrada, Sumanth Dathathri, Krishnamurthy Dvijotham, Robert Stanforth, Rudy R Bunel, Jonathan Uesato, Sven Gowal, and M Pawan Kumar. Make sure you're unsure: A framework for verifying probabilistic specifications. _Advances in Neural Information Processing Systems_, 34:11136-11147, 2021.
* Boyd et al. (2004) Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. _Convex optimization_. Cambridge University Press, 2004.
* Brix (2023) Christopher Brix. Vnn-comp 2023 benchmarks. [https://github.com/ChristopherBrix/vnncomp2023_benchmarks](https://github.com/ChristopherBrix/vnncomp2023_benchmarks), 2023.
* Brix et al. (2023) Christopher Brix, Mark Niklas Muller, Stanley Bak, Taylor T. Johnson, and Changliu Liu. First three years of the international verification of neural networks competition (vnn-comp), 2023. URL [https://arxiv.org/abs/2301.05815](https://arxiv.org/abs/2301.05815).
* Bunel et al. (2020) Rudy Bunel, P Mudigonda, Ilker Turkaslan, P Torr, Jingyue Lu, and Pushmeet Kohli. Branch and bound for piecewise linear neural network verification. _Journal of Machine Learning Research_, 21(2020), 2020.
* Chen et al. (2021) Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Atom: Robustifying out-of-distribution detection using outlier mining. _In Proceedings of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)_, 2021.
* Chen et al. (2022) Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Robust out-of-distribution detection for neural networks. In _The AAAI-22 Workshop on Adversarial Machine Learning and Beyond_, 2022. URL [https://openreview.net/forum?id=WMIoz7O_DPz](https://openreview.net/forum?id=WMIoz7O_DPz).
* Dimitrov et al. (2022) Dimitar Iliev Dimitrov, Gagandeep Singh, Timon Gehr, and Martin Vechev. Provably robust adversarial examples. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=UMfhoMtlaP5](https://openreview.net/forum?id=UMfhoMtlaP5).
* Dvijotham et al. (2018) Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A Mann, and Pushmeet Kohli. A dual approach to scalable verification of deep networks. In _UAI_, volume 1, page 3, 2018.
* Ehlers (2017) Ruediger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In _International Symposium on Automated Technology for Verification and Analysis_, pages 269-286. Springer, 2017.
* Everett et al. (2021) Michael Everett, Golnaz Habibi, Chuangchuang Sun, and Jonathan P How. Reachability analysis of neural feedback loops. _IEEE Access_, 9:163938-163953, 2021.
* Everett et al. (2022) Michael Everett, Rudy Bunel, and Shayegan Omidshafiei. Drip: Domain refinement iteration with polytopes for backward reachability analysis of neural feedback loops. _arXiv preprint arXiv:2212.04646_, 2022.
* Gehr et al. (2018) Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In _2018 IEEE Symposium on Security and Privacy (SP)_, pages 3-18, 2018. doi: 10.1109/SP.2018.00058.
* Gowal et al. (2018) Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for training verifiably robust models. _arXiv preprint arXiv:1810.12715_, 2018.
* Hendrycks et al. (2018) Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. _arXiv preprint arXiv:1812.04606_, 2018.
* Hu et al. (2020) Haimin Hu, Mahyar Fazlyab, Manfred Morari, and George J. Pappas. Reach-sdp: Reachability analysis of closed-loop systems with neural network controllers via semidefinite programming. In _2020 59th IEEE Conference on Decision and Control (CDC)_, pages 5929-5934, 2020. doi: 10.1109/CDC42340.2020.9304296.
* Hu et al. (2020)Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In _International conference on computer aided verification_, pages 97-117. Springer, 2017.
* Kindermann and Linden [1990] J Kindermann and A Linden. Inversion of neural networks by gradient descent. _Parallel Computing_, 14(3):277-286, 1990. ISSN 0167-8191. doi: [https://doi.org/10.1016/0167-8191](https://doi.org/10.1016/0167-8191)(90)90081-J. URL [https://www.sciencedirect.com/science/article/pii/016781919090081J](https://www.sciencedirect.com/science/article/pii/016781919090081J).
* Liang et al. [2018] Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In _International Conference on Learning Representations_, 2018. URL [https://openreview.net/forum?id=H1VGklxRZ](https://openreview.net/forum?id=H1VGklxRZ).
* Meinke et al. [2021] Alexander Meinke, Julian Bitterwolf, and Matthias Hein. Provably robust detection of out-of-distribution data (almost) for free. _arXiv preprint arXiv:2106.04260_, 2021.
* Muller et al. [2023] Mark Niklas Muller, Christopher Brix, Stanley Bak, Changliu Liu, and Taylor T. Johnson. The third international verification of neural networks competition (vnn-comp 2022): Summary and results, 2023.
* De Palma et al. [2023] Alessandro De Palma, Rudy Bunel, Krishnamurthy Dvijotham, M. Pawan Kumar, and Robert Stanforth. Ibp regularization for verified adversarial robustness via branch-and-bound, 2023.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems 32_, pages 8024-8035. Curran Associates, Inc., 2019. URL [http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf](http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf).
* Raghunathan et al. [2018] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. _arXiv preprint arXiv:1801.09344_, 2018.
* Redmon and Farhadi [2016] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster, stronger. _CoRR_, abs/1612.08242, 2016. URL [http://arxiv.org/abs/1612.08242](http://arxiv.org/abs/1612.08242).
* Robert et al. [2022a] Nicholas Robert, Michael Everett, Songan Zhang, and Jonathan P How. A hybrid partitioning strategy for backward reachability of neural feedback loops. _arXiv preprint arXiv:2210.07918_, 2022a.
* Roder et al. [2022b] Nicholas Rober, Sydney M Katz, Chelsea Sidrane, Esen Yel, Michael Everett, Mykel J Kochenderfer, and Jonathan P How. Backward reachability analysis of neural feedback loops: Techniques for linear and nonlinear systems. _arXiv preprint arXiv:2209.14076_, 2022b.
* Saad and Wunsch [2007] Emad W. Saad and Donald C. Wunsch. Neural network explanation using inversion. _Neural Networks_, 20(1):78-93, 2007. ISSN 0893-6080. doi: [https://doi.org/10.1016/j.neunet.2006.07.005](https://doi.org/10.1016/j.neunet.2006.07.005). URL [https://www.sciencedirect.com/science/article/pii/S0893608006001730](https://www.sciencedirect.com/science/article/pii/S0893608006001730).
* Salehi et al. [2022] Mohammadreza Salehi, Hossein Mirzaei, Dan Hendrycks, Yixuan Li, Mohammad Hossein Rohban, and Mohammad Sabokrou. A unified survey on anomaly, novelty, open-set, and out of-distribution detection: Solutions and future challenges. _Transactions on Machine Learning Research_, 2022. URL [https://openreview.net/forum?id=aRtjVZvbPK](https://openreview.net/forum?id=aRtjVZvbPK).
* Salman et al. [2019] Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation barrier to tight robustness verification of neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* Sehwag et al. [2019] Vikash Sehwag, Arjun Nitin Bhagoji, Liwei Song, Chawin Sitawarin, Daniel Cullina, Mung Chiang, and Prateek Mittal. Analyzing the robustness of open-world machine learning. In _Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security_, AISec'19, page 105-116, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450368339. doi: 10.1145/3338501.3357372. URL [https://doi.org/10.1145/3338501.3357372](https://doi.org/10.1145/3338501.3357372).
* Szegedy et al. [2015]Wang Shiqi, Kexin Pei, Whitehouse Justin, Junfeng Yang, and Suman Jana. Efficient formal safety analysis of neural networks. In _32nd Conference on Neural Information Processing Systems (NIPS)_, Montreal, Canada, 2018.
* Singh et al. (2018) Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Puschel, and Martin Vechev. Fast and effective robustness certification. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018. URL [https://proceedings.neurips.cc/paper/2018/file/f2f446980d8e971ef3da97af089481c3-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/f2f446980d8e971ef3da97af089481c3-Paper.pdf).
* Singh et al. (2019) Gagandeep Singh, Timon Gehr, Markus Puschel, and Martin Vechev. An abstract domain for certifying neural networks. _Proc. ACM Program. Lang._, 3(POPL), jan 2019. doi: 10.1145/3290354. URL [https://doi.org/10.1145/3290354](https://doi.org/10.1145/3290354).
* Tjeng et al. (2017) Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed integer programming. _arXiv preprint arXiv:1711.07356_, 2017.
* Vincent and Schwager (2021) Joseph A. Vincent and Mac Schwager. Reachable polyhedral marching (rpm): A safety verification algorithm for robotic systems with deep neural network components. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 9029-9035, 2021. doi: 10.1109/ICRA48506.2021.9561956.
* Wang et al. (2022) Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit matching. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.
* Wang et al. (2021) Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. Beta-CROWN: Efficient bound propagation with per-neuron split constraints for complete and incomplete neural network verification. _Advances in Neural Information Processing Systems_, 34, 2021.
* Wong and Kolter (2018) Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In _International Conference on Machine Learning_, pages 5286-5295. PMLR, 2018.
* Wu et al. (2022) Haoze Wu, Clark Barrett, Mahmood Sharif, Nina Narodytska, and Gagandeep Singh. Scalable verification of gnn-based job schedulers. _Proc. ACM Program. Lang._, 6(OOPSLA2), oct 2022. doi: 10.1145/3563325. URL [https://doi.org/10.1145/3563325](https://doi.org/10.1145/3563325).
* Xu et al. (2020) Kaidi Xu, Zhuoxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certified robustness and beyond. _Advances in Neural Information Processing Systems_, 33, 2020.
* Xu et al. (2021) Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. Fast and Complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=nYZUXB16LNn](https://openreview.net/forum?id=nYZUXB16LNn).
* Yang et al. (2021a) Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. _arXiv preprint arXiv:2110.11334_, 2021a.
* Yang et al. (2021b) Pengfei Yang, Renjue Li, Jianlin Li, Cheng-Chao Huang, Jingyi Wang, Jun Sun, Bai Xue, and Lijun Zhang. Improving neural network verification through spurious region guided refinement. In Jan Friso Groote and Kim Guldstrand Larsen, editors, _Tools and Algorithms for the Construction and Analysis of Systems_, pages 389-408, Cham, 2021b. Springer International Publishing. ISBN 978-3-030-72016-2.
* Zhang et al. (2018a) Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. _Advances in Neural Information Processing Systems_, 31:4939-4948, 2018a. URL [https://arxiv.org/pdf/1811.00866.pdf](https://arxiv.org/pdf/1811.00866.pdf).
* Zhang et al. (2022) Huan Zhang, Shiqi Wang, Kaidi Xu, Linyi Li, Bo Li, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. General cutting planes for bound-propagation-based neural network verification. _Advances in Neural Information Processing Systems_, 2022.
* Zhang et al. (2021)Xin Zhang, Armando Solar-Lezama, and Rishabh Singh. Interpreting neural network judgments via minimal, stable, and symbolic corrections. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018b. URL [https://proceedings.neurips.cc/paper_files/paper/2018/file/300891a62162b960cf02ce3827bb363c-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2018/file/300891a62162b960cf02ce3827bb363c-Paper.pdf).
* Zhang et al. (2023) Xiyue Zhang, Benjie Wang, and Marta Kwiatkowska. Provable preimage under-approximation for neural networks. _arXiv preprint arXiv:2305.03686_, 2023.
* Zhong (2023) Xiangru Zhong. Yolo-benchmark. [https://github.com/xiangruzh/Yolo-Benchmark](https://github.com/xiangruzh/Yolo-Benchmark), 2023.
* Zhong et al. (2023) Yuyi Zhong, Quang-Trung Ta, and Siau-Cheng Khoo. Arena: Enhancing abstract refinement for neural network verification. In Cezara Dragoi, Michael Emmi, and Jingbo Wang, editors, _Verification, Model Checking, and Abstract Interpretation_, pages 366-388, Cham, 2023. Springer Nature Switzerland. ISBN 978-3-031-24950-1.

## Appendix A OOD Detection

For the OOD benchmark, we train a neural network on two clusters of data. Standard training would lead to an uncalibrated predictor, we add in OOD data through sampling the input space and rejecting inputs that are too close to any training data point. Training a classifier on this dataset induces the contour on the right, which is a calibrated classifier we can certify properties over. For our experiments, we consider a high-confidence output as \(90\%\) confidence of the data being in-distribution.

## Appendix B Example

**Example 1**.: Consider the toy neural network in Figure 8 under \(\mathcal{X}=[-2,2]\) and \(\mathcal{S}_{\text{out}}=\{y:1\leq y\leq 1.02\}\). Suppose we wanted to find the tightest \([l_{0}^{(1)},u_{0}^{(1)}]\) (bounds on \(x_{0}^{(1)}\)). If we only enforce the constraints preceding the ReLU (standard bound propagation), we get \([-2,2]\). If we only enforce the constraints following the ReLU, we get \((-\infty,1.02]\). However, when we utilize all of the constraints, we find that the true intermediate bounds are \([0,0.01]\), which is over \(100\) times tighter than the intersection of the two previous methods (all derivations provided below). Therefore, new techniques are necessary for optimizing intermediate bounds in this setting.

### Deriving Intermediate Bounds for Example B

Preceding Constraints.The linear layer gives us \(\mathbf{x}_{0}^{(1)}=\hat{\mathbf{x}}_{0}^{(0)}\). Since the only constraint we have is \(\hat{\mathbf{x}}_{0}^{(0)}\in[-2,2]\), the tightest intermediate bounds we can derive are \([-2,2]\).

Following Constraints.Since the output of a ReLU is non-negative, we know that \(\hat{\mathbf{x}}_{0}^{(1)},\hat{\mathbf{x}}_{1}^{(1)}\geq 0\). Since \(\mathbf{x}_{0}^{(2)}=\hat{\mathbf{x}}_{0}^{(1)}+\hat{\mathbf{x}}_{1}^{(1)}\) and the output constraint enforces \(\mathbf{x}_{0}^{(2)}\leq 1.02\), we derive that \(\hat{\mathbf{x}}_{0}^{(1)}\leq 1.02\). If we set \(\hat{\mathbf{x}}_{0}^{(1)}=1.02-\hat{\mathbf{x}}_{1}^{(1)}\), we see that \(\hat{\mathbf{x}}_{0}^{(1)}\) can take the entire interval \([0,1.02]\). Since the only constraint we have on \(\mathbf{x}_{0}^{(1)}\) is that \(\text{ReLU}(\hat{\mathbf{x}}_{0}^{(1)})=\mathbf{x}^{(1)}\), our desired interval is the preimage of \([0,1.02]\). This means that \(\mathbf{x}_{0}^{(1)}\) can take any value in the interval \((-\infty,1.02]\).

All Constraints.We first note that by the first linear layer, we have \(\mathbf{x}_{1}^{(1)}=\mathbf{x}_{0}^{(1)}+1\). Therefore, if \(\mathbf{x}_{0}^{(1)}\) is less than \(0\), then \(\mathbf{x}_{1}^{(1)}\) is less than \(1\), which means \(\text{ReLU}(\mathbf{x}_{0}^{(1)})+\text{ReLU}(\mathbf{x}_{1}^{(1)})\) is less than \(1\), which contradicts the output constraint. If \(\mathbf{x}_{0}^{(1)}\) is always non-negative, then we have that the output is equivalent to \(\text{ReLU}(\mathbf{x}_{0}^{(1)})+\text{ReLU}(\mathbf{x}_{0}^{(1)}+1)=2\mathbf{x}_{0}^{(1) }+1\). Therefore, the output constraint implies \(\mathbf{x}_{0}^{(1)}\leq 0.01\). Any \(\mathbf{x}_{0}^{(1)}\in[0,0.01]\) is achievable by setting the input to the desired value.

Figure 7: To improve our models ability to detect data outside the training distribution (of green and red points), we randomly sample points that are far from every training point. In the right contour, we see that the models confidence of in-distribution increases as we approach the centers of the distribution, demonstrating our model is more calibrated.

Optimizing Intermediate Bounds

We present a generalized theorem which provides a lower bound for any linear combination of \(\hat{\mathbf{x}}^{(0)}\) and \(\mathbf{x}^{(i)}\). We prove this theorem in Appendix D. We note that by selecting the coefficients of these variables (named \(\mathbf{c}^{(i)}\) for \(i\in\{0,1,\ldots,L-1\}\)), we recover the two following functionalities which we name \(g_{\mathbf{c}}(\mathbf{\alpha},\mathbf{\gamma})\) and \(g^{b}_{i,j}(\mathbf{\alpha},\mathbf{\gamma})\) (respectively).

* When the only nonzero coefficients are \(\mathbf{c}^{(0)}=\mathbf{c}\), we recover Theorem 1. We refer to this bound as \(g_{\mathbf{c}}(\mathbf{\alpha},\mathbf{\gamma})\)
* When the only nonzero coefficient is \(c^{(i)}_{j}=1\), we can lower bound \(l^{(i)}_{j}\) by \(g^{\text{lower}}_{i,j}(\mathbf{\alpha},\mathbf{\gamma})\). If we set \(c^{(i)}_{j}=-1\), we can upper bound \(u^{(i)}_{j}\) by \(-g^{\text{upper}}_{i,j}(\mathbf{\alpha},\mathbf{\gamma})\).

With this, we present our generalized theorem.

**Theorem 2** (Lower-bounding combination of neurons).: _Given an output set \(\mathcal{S}_{\text{out}}=\{\mathbf{y}:\mathbf{H}\mathbf{y}+\mathbf{d}\leq\mathbf{0}\}\) and vector \(\mathbf{c}\), \(g(\mathbf{\alpha},\mathbf{\gamma})\) is a lower bound to the linear program in (3) with objective \(\mathbf{c}^{(0)\top}\hat{\mathbf{x}}^{(0)}+\sum_{i=1}^{L-1}\mathbf{c}^{(i)\top}\mathbf{x}^{(i)}\) for \(\mathbf{0}\leq\mathbf{\alpha}\leq\mathbf{1}\), \(\mathbf{\gamma}\geq\mathbf{0}\), and \(g\) defined via_

\[g(\mathbf{\alpha},\mathbf{\gamma}) =\left[\mathbf{c}^{(0)\top}-\mathbf{\nu}^{(1)\top}\mathbf{W}^{(1)}\right] _{+}\mathbf{l}^{(0)}-\left[\mathbf{c}^{(0)\top}-\mathbf{\nu}^{(1)\top}\mathbf{W}^{(1)} \right]_{-}\mathbf{u}^{(0)}\] \[-\sum_{i=1}^{L}\mathbf{\nu}^{(i)\top}\mathbf{b}^{(i)}+\sum_{i=1}^{L-1} \sum_{j\in\mathcal{I}^{\pm(i)}}\left[\frac{u^{(i)}_{j}l^{(i)}_{j}[\hat{p}^{(i) }_{j}]_{+}}{u^{(j)}_{i}-l^{(j)}_{i}}\right]\]

_where every term can be directly recursively computed via_

\[\mathcal{I}^{-(i)} =\{j:u^{(i)}_{j}\leq 0\}\] \[\mathcal{I}^{+(i)} =\{j:l^{(i)}_{j}\geq 0\}\] \[\mathcal{I}^{\pm(i)} =\{j:l^{(i)}_{j}<0<u^{(i)}_{j}\}\] \[\mathbf{\nu}^{(L)} =-\mathbf{\gamma}\] \[\hat{\nu}^{(i)}_{j} =\mathbf{\nu}^{(i+1)\top}\mathbf{W}^{(i+1)}_{-,j}\] \[\nu^{(i)}_{j} =\left\{\begin{array}{ll}\hat{\nu}^{(i)}_{j}-c^{(i)}_{j},&j\in \mathcal{I}^{+(i)}\\ -c^{(i)}_{j},&j\in\mathcal{I}^{-(i)}\\ \frac{u^{(i)}_{j}}{u^{(j)}_{i}-l^{(i)}_{i}}[\hat{\nu}^{(i)}_{j}]_{+}-\alpha^{(i )}_{j}[\hat{\nu}^{(i)}_{j}]_{-}-c^{(i)}_{j},&j\in\mathcal{I}^{\pm(i)}\end{array}\right.\]

Figure 8: A simple neural network from \(\mathbb{R}\) to \(\mathbb{R}\). Every node is the sum of its incoming nodes unless the edge is labeled ReLU.

Dual Derivation

We first incorporate the output constraint \(\mathbf{H}\mathbf{x}^{(L)}+\mathbf{d}\leq\mathbf{0}\) by folding in this linear transformation of the output into the final linear layer of the network, as customary in prior work (Wang et al., 2021; Zhang et al., 2022).

We now prove the generalized theorem as described in C. As a conceptual overview of our proof, we take the Lagrange dual of the linear program to derive an unconstrained optimization problem. From here, we derive constraints that must be satisfied in the \(\max\min\) formulation. These constraints yield the bound propagation procedure we display in Theorem 1.

We start with the convex relaxation.

\[\min_{\mathbf{x},\mathbf{\hat{x}}} \mathbf{e}^{(0)\top}\hat{\mathbf{x}}^{(0)}+\sum_{i=1}^{L-1}\mathbf{c}^{(i) \top}\mathbf{x}^{(i)}\] s.t. \[\mathbf{l}^{(0)}\leq\hat{\mathbf{x}}^{(0)}\leq\mathbf{u}^{(0)}\] \[\mathbf{x}^{(L)}\leq\mathbf{0};\] \[\mathbf{x}^{(i)}=\mathbf{W}^{(i)}\hat{\mathbf{x}}^{(i-1)}+\mathbf{b}^{(i) };\quad i\in[L],\] \[\hat{x}^{(i)}_{j}\geq 0;j\in\mathcal{I}^{\pm(i)}\] \[\hat{x}^{(i)}_{j}\geq x^{(i)}_{j};j\in\mathcal{I}^{\pm(i)}\] \[(u^{(i)}_{j}-l^{(i)}_{j})\hat{x}^{(i)}_{j}\leq u^{(i)}_{j}x^{(i)} _{j}-u^{(i)}_{j}l^{(i)}_{j};j\in\mathcal{I}^{\pm(i)}\] \[\hat{x}^{(i)}_{j}=x^{(i)}_{j};j\in\mathcal{I}^{+(i)}\] \[\hat{x}^{(i)}_{j}=0;j\in\mathcal{I}^{-(i)}\]

From this, we take the Lagrange dual of most of the constraints. In doing so, we introduce a new dual variable for each constraint. Note that we do not dualize every constraint since they can easily be dealt with later.

\[\min_{\mathbf{x},\mathbf{\hat{x}}}\max_{\mathbf{\nu},\mathbf{\mu},\mathbf{\tau},\mathbf{ \gamma},\mathbf{\lambda}} \mathbf{c}^{(0)\top}\hat{\mathbf{x}}^{(0)}+\sum_{i=1}^{L-1}\mathbf{c}^{(i) \top}\mathbf{x}^{(i)}+\mathbf{\gamma}^{\top}\mathbf{x}^{(L)}+\sum_{i=1}^{L}\mathbf{\nu}^{(i) \top}\left(\mathbf{x}^{(i)}-\mathbf{W}^{(i)}\hat{\mathbf{x}}^{(i-1)}-\mathbf{b}^{(i)}\right)\] \[+\sum_{i=1}^{L-1}\sum_{j\in\mathcal{I}^{\pm(i)}}\left[\mu^{(i)}_{ j}\left(-\hat{x}^{(i)}_{j}\right)+\tau^{(i)}_{j}\left(x^{(i)}_{j}-\hat{x}^{(i)}_{j }\right)+\lambda^{(i)}_{j}\left((u^{(i)}_{j}-l^{(i)}_{j})\hat{x}^{(i)}_{j}-u^ {(i)}_{j}x^{(i)}_{j}+u^{(i)}_{j}l^{(i)}_{j}\right)\right]\] s.t. \[\mathbf{l}^{(0)}\leq\hat{\mathbf{x}}^{(0)}\leq\mathbf{u}^{(0)};\quad\hat{x}^{ (i)}_{j}=0,j\in\mathcal{I}^{-(i)};\quad\hat{x}^{(i)}_{j}=x^{(i)}_{j},j\in \mathcal{I}^{+(i)}\] \[\mathbf{\mu}\geq 0;\quad\mathbf{\tau}\geq 0;\quad\mathbf{\gamma}\geq 0;\quad \mathbf{\lambda}\geq 0\]

Since we took the dual of a linear program, the solution to the min-max optimization is equivalent to the solution of the max-min optimization by strong duality. Therefore, we can solve the following program (equivalent up to rearrangement).

\[\max_{\mathbf{\nu},\mathbf{\mu},\mathbf{\tau},\mathbf{\gamma},\mathbf{\lambda}}\min_{\mathbf{x},\hat{\mathbf{x}}} \left(\mathbf{\nu}^{(L)}+\mathbf{\gamma}\right)^{\top}\mathbf{x}^{(L)}+\left( \mathbf{c}^{(0)\top}-\mathbf{\nu}^{(1)\top}\mathbf{W}^{(1)}\right)\hat{\mathbf{x}}^{(0)}\] \[+\sum_{i=1}^{L-1}\sum_{j\in\mathcal{I}^{+(i)}}\left(\nu_{j}^{(i)}- \mathbf{\nu}^{(i+1)\top}\mathbf{W}^{(i+1)}_{:,j}+c_{j}^{(i)}\right)x_{j}^{(i)}+ \sum_{i=1}^{L-1}\sum_{j\in\mathcal{I}^{-(i)}}\left(\nu_{j}^{(i)}+c_{j}^{(i)} \right)x_{j}^{(i)}\] \[+\sum_{i=1}^{L-1}\sum_{j\in\mathcal{I}^{\pm(i)}}\left[\left(\nu_{ j}^{(i)}+\tau_{j}^{(i)}-\lambda_{j}^{(i)}u_{j}^{(i)}+c_{j}^{(i)}\right)x_{j}^{(i)}\right.\] \[\left.+\left(-\mathbf{\nu}^{(i+1)\top}\mathbf{W}^{(i+1)}_{:,j}-\mu_{j }^{(i)}-\tau_{j}^{(i)}+(u_{j}^{(i)}-l_{j}^{(i)})\lambda_{j}^{(i)}\right)\hat{x }_{j}^{(i)}\right]\] \[-\sum_{i=1}^{L}\mathbf{\nu}^{(i)\top}\mathbf{b}^{(i)}+\sum_{i=1}^{L-1 }\sum_{j\in\mathcal{I}^{\pm(i)}}\lambda_{j}^{(i)}u_{j}^{(i)}l_{j}^{(i)}\] s.t. \[\mathbf{l}^{(0)}\leq\hat{\mathbf{x}}^{(0)}\leq\mathbf{u}^{(0)}\] \[\mathbf{\mu}\geq 0;\quad\mathbf{\tau}\geq 0;\quad\mathbf{\gamma}\geq 0;\quad\mathbf{ \lambda}\geq 0\]

To minimize \(\left(\mathbf{c}^{(0)\top}-\mathbf{\nu}^{(1)\top}\mathbf{W}^{(1)}\right)\hat{\mathbf{x}}^{ (0)}\) subject to \(\mathbf{l}^{(0)}\leq\hat{\mathbf{x}}^{(0)}\leq\mathbf{u}^{(0)}\), we can consider the choice we must make in each dimension. If the \(j\)th entry of the coefficient is positive, we should set \(\hat{x}_{j}^{(0)}=l_{j}^{(0)}\). Otherwise, we should set \(\hat{x}_{j}^{(0)}=u_{j}^{(0)}\).

\[\max_{\mathbf{\nu},\mathbf{\mu},\mathbf{\tau},\mathbf{\gamma},\mathbf{\lambda}}\min_{ \mathbf{x},\hat{\mathbf{x}}} \left(\mathbf{\nu}^{(L)}+\mathbf{\gamma}\right)^{\top}\mathbf{x}^{(L)}+\left[ \mathbf{c}^{(0)\top}-\mathbf{\nu}^{(1)\top}\mathbf{W}^{(1)}\right]_{+}\mathbf{l}^{(0)}- \left[\mathbf{c}^{(0)\top}-\mathbf{\nu}^{(1)\top}\mathbf{W}^{(1)}\right]_{-}\mathbf{u}^{(0)}\] \[+\sum_{i=1}^{L-1}\sum_{j\in\mathcal{I}^{+(i)}}\left(\nu_{j}^{(i)} -\mathbf{\nu}^{(i+1)\top}\mathbf{W}^{(i+1)}_{:,j}+c_{j}^{(i)}\right)x_{j}^{(i)}+ \sum_{i=1}^{L-1}\sum_{j\in\mathcal{I}^{-(i)}}\left(\nu_{j}^{(i)}+c_{j}^{(i)} \right)x_{j}^{(i)}\] \[+\sum_{i=1}^{L-1}\sum_{j\in\mathcal{I}^{\pm(i)}}\left[\left(\nu_{ j}^{(i)}+\tau_{j}^{(i)}-\lambda_{j}^{(i)}u_{j}^{(i)}+c_{j}^{(i)}\right)x_{j}^{(i)}\right.\] \[\left.+\left(-\mathbf{\nu}^{(i+1)\top}\mathbf{W}^{(i+1)}_{:,j}-\mu_{j }^{(i)}-\tau_{j}^{(i)}+(u_{j}^{(i)}-l_{j}^{(i)})\lambda_{j}^{(i)}\right)\hat{x }_{j}^{(i)}\right]\] \[-\sum_{i=1}^{L}\mathbf{\nu}^{(i)\top}\mathbf{b}^{(i)}+\sum_{i=1}^{L-1 }\sum_{j\in\mathcal{I}^{\pm(i)}}\lambda_{j}^{(i)}u_{j}^{(i)}l_{j}^{(i)}\] s.t. \[\mathbf{\mu}\geq 0;\quad\mathbf{\tau}\geq 0;\quad\mathbf{\gamma}\geq 0;\quad\mathbf{ \lambda}\geq 0\]

From here, we note that the variables \(\mathbf{x}\) or \(\hat{\mathbf{x}}\) are unconstrained variables. Therefore, if any of their coefficients are nonzero, the inner minimization can immediately drive its value to \(-\infty\). As such, the outer maximization must set all of these coefficients to zero. Therefore, we can derive constraints from this restructured optimization and remove the free variables \(\mathbf{x},\hat{\mathbf{x}}\).

\[\max_{\mathbf{\nu},\mathbf{\mu},\mathbf{\tau},\mathbf{\gamma},\mathbf{\lambda}} \Big{[}\mathbf{c}^{(0)\top}-\mathbf{\nu}^{(1)\top}\mathbf{W}^{(1)}\Big{]}_{ +}\mathbf{l}^{(0)}-\Big{[}\mathbf{c}^{(0)\top}-\mathbf{\nu}^{(1)\top}\mathbf{W}^{(1)}\Big{]} _{-}\mathbf{u}^{(0)}-\sum_{i=1}^{L}\mathbf{\nu}^{(i)\top}\mathbf{b}^{(i)}\] \[+\sum_{i=1}^{L-1}\sum_{j\in\mathcal{I}^{\pm(i)}}\lambda_{j}^{(i)} u_{j}^{(i)}l_{j}^{(i)}\] s.t. \[\mathbf{\nu}^{(L)}=-\mathbf{\gamma}\] \[\nu_{j}^{(i)}=\mathbf{\nu}^{(i+1)\top}\mathbf{W}^{(i+1)}_{:,j}-c_{j}^ {(i)},j\in\mathcal{I}^{+(i)}\] \[\nu_{j}^{(i)}=-c_{j}^{(i)},j\in\mathcal{I}^{-(i)}\] \[\nu_{j}^{(i)}=\lambda_{j}^{(i)}u_{j}^{(i)}-\tau_{j}^{(i)}-c_{j}^{( i)},j\in\mathcal{I}^{\pm(i)}\] \[\mathbf{\nu}^{(i+1)\top}\mathbf{W}^{(i+1)}_{:,j}=(u_{j}^{(i)}-l_{j}^{( i)})\lambda_{j}^{(i)}-\left(\mu_{j}^{(i)}+\tau_{j}^{(i)}\right),j\in\mathcal{I}^ {\pm(i)}\] \[\mathbf{\mu}\geq 0;\quad\mathbf{\tau}\geq 0;\quad\mathbf{\gamma}\geq 0;\quad \mathbf{\lambda}\geq 0\]

For the following, we define \(\hat{\nu}_{j}^{(i)}=\mathbf{\nu}^{(i+1)\top}\mathbf{W}^{(i+1)}_{:,j}\). We note that since the upper and lower bounds of the neuron relaxation can not be tight simultaneously, at least one of \((u_{j}^{(i)}-l_{j}^{(i)})\lambda_{j}^{(i)}\) and \(\mu_{j}^{(i)}+\tau_{j}^{(i)}\) must be non-zero. Therefore, we can write them as \((u_{j}^{(i)}-l_{j}^{(i)})\lambda_{j}^{(i)}=[\hat{\nu}_{j}^{(i)}]_{+}\) and \(\mu_{j}^{(i)}+\tau_{j}^{(i)}=[\hat{\nu}_{j}^{(i)}]_{-}\). We can then use the fact that \(\tau_{j}^{(i)}\) lies in the interval \(0\) and \(\hat{\nu}_{j}^{(i)}\) to get the following bound propagation procedure.

\[\max_{\mathbf{\nu},\mathbf{\alpha},\mathbf{\gamma}} \Big{[}\mathbf{c}^{(0)\top}-\mathbf{\nu}^{(1)\top}\mathbf{W}^{(1)}\Big{]} _{+}\mathbf{l}^{(0)}-\Big{[}\mathbf{c}^{(0)\top}-\mathbf{\nu}^{(1)\top}\mathbf{W}^{(1)} \Big{]}_{-}\mathbf{u}^{(0)}-\sum_{i=1}^{L}\mathbf{\nu}^{(i)\top}\mathbf{b}^{(i)}\] s.t. \[\mathbf{\nu}^{(L)}=-\mathbf{\gamma}\] \[\nu_{j}^{(i)}=-c_{j}^{(i)},j\in\mathcal{I}^{-(i)}\] \[\hat{\nu}_{j}^{(i)}=\mathbf{\nu}^{(i+1)\top}\mathbf{W}^{(i+1)}_{:,j}\] \[\nu_{j}^{(i)}=\frac{u_{j}^{(i)}}{u_{j}^{(i)}-l_{j}^{(i)}}[\hat{\nu }_{j}^{(i)}]_{+}-\alpha_{j}^{(i)}[\hat{\nu}_{j}^{(i)}]_{-}-c_{j}^{(i)},j\in \mathcal{I}^{\pm(i)}\] \[\alpha_{j}^{(i)}\in[0,1];\quad\mathbf{\gamma}\geq 0\]

In this program, \(\alpha_{j}^{(i)}\) are optimizable parameters controlling the relaxation of neuron \(j\) in layer \(i\), similar to the ones appearing in Xu et al. 2021. \(\mathbf{\gamma}\), as discussed in the body of the paper, is the parameter which enforces the output constraint throughout this entire bound propagation procedure.

## Appendix E Connection to Forward Verification for Intermediate Bounds

We consider the general objective presented in C and aim to minimize \(\mathbf{c}^{(0)\top}\hat{\mathbf{x}}^{(0)}+\sum_{i=1}^{L-1}\mathbf{c}^{(i)\top}\mathbf{x}^{(i)}\)

\[\min_{\mathbf{x}} \mathbf{c}^{(0)\top}\hat{\mathbf{x}}^{(0)}+\sum_{i=1}^{L-1}\mathbf{c}^{(i) \top}\mathbf{x}^{(i)}\] s.t. \[\mathbf{x}\in\mathcal{X};\quad\mathbf{H}f(\mathbf{x})+\mathbf{d}\leq\[\min_{\mathbf{x}} \mathbf{c}^{(0)\top}\mathbf{\hat{x}}^{(0)}+\sum_{i=1}^{L-1}\mathbf{c}^{(i)\top} \mathbf{x}^{(i)}+\mathbf{\gamma}\left(\mathbf{H}f(\mathbf{x})+\mathbf{d}\right)\] s.t. \[\mathbf{x}\in\mathcal{X};\quad\mathbf{\gamma}\geq 0\]

We note that the objective here can be expressed as a neural network with residual stream \(f(\mathbf{x})\) and a skip connection with linear weight \(\mathbf{c}^{(i)}\) from the pre-activations of layer \(i\) to the final output for every layer. In theory, we could construct this neural network and directly pass it to a forward verification tool which could iteratively tighten all bounds for a solution.

## Appendix F Implementation

As stated in Algorithm 1, INVPROP first initializes bounds for all layers using some computationally cheap technique. Based on the input bounds given by \(\mathcal{X}\), we first compute intermediate bounds using interval propagation and then tighten them based on the reverse symbolic interval propagation (RSIP) technique Singh et al. (2019). While INVPROP will iteratively tighten those bounds over time, we found RSIP to reduce the total necessary runtime significantly compared to an initialization based solely on interval propagation. We initialize all \(\mathbf{\alpha}\) with \(0.5\) and all \(\mathbf{\gamma}\) with \(0.025\).

The optimization is performed for all lower and upper bounds of all neurons in each layer in parallel, starting with the last layer and moving forward to the input layer. The bounds of the cutting hyperplanes are optimized last, together with the bounds on the input neurons. The improvements on \(\mathbf{c}^{\top}\mathbf{x}\) are measured every 10th iteration. Before doing so, the bounds of the hyperplanes are tightened for 10 extra steps to improve their precision. We detect convergence by monitoring \(\mathbf{c}^{\top}\mathbf{x}\). If successive iterations see minimal improvement, we stop or branch on the input space. All cutting hyperplanes are evenly distributed to maximize their information gain. For 40 hyperplanes in a 2D input space, we rotate each plane by \(9^{\circ}\). All reported runtimes under \(10\) minutes are computed as the average of five runs.

### Encoding Non-Linear Constraints

To support non-linear output sets, such as the maximum operation used in the OOD example in Section 4.3, the non-linearity needs to be encoded, such that it can be expressed as linear constraints over the modified network. We rewrite \(\max(y_{1},y_{2})=\max(y_{1}-y_{2},0)+y_{2}\) and add an additional ReLU layer for this operation. Note that to pass \(y_{2}\) through this layer without modifying it, one can either write \(y_{2}=\max(y_{2},0)-\max(-y_{2},0)\), or \(y_{2}=\max(y_{2}-M,0)+M\) where \(M\) is the lower bound of all possible \(y_{2}\). We find that avoiding the additional ReLU relaxations that would occur for the first approach is beneficial for the optimization and compute a lower bound of \(M\) using interval propagation. As \(y_{3}\) (for the OOD class) should not be changed by this operation either, we apply the same trick of subtracting and adding its lower bound.

### Control Benchmark Encoding

We encode the entire control formula \(\mathbf{x}=\mathbf{A}\mathbf{x}+\mathbf{B}\mathbf{u}\) as one feedforward network by encoding the residual connection as regular fully connected layers. To this end, we use the same technique described in Section F.1 to shift the bounds into the positive regime, feed them forward and then shift them back.

To compute \(\mathcal{S}_{\text{over}}\) for a timestep \(t>1\), we first stack the network \(t\) times, then simplify it by merging consecutive linear layers. All bounds of layers not affected by this merging that also appear in the network for \(t-1\) timesteps are reused. Their bounds are already tight enough and are not optimized further. Note that this is different than using the previous \(\mathcal{S}_{\text{over}}\) as the new target region and using an unstacked network: All bounds of the new layers are still optimized w.r.t. the precise target \(\mathcal{S}_{\text{out}}\). Therefore, we do not suffer from accumulating inaccuracies.

## Appendix G Hardware

For the control benchmark, all experiments were performed on a Dual Xeon Gold 6138.

For the OOD benchmark, all experiments were performed on an Intel Xeon Platinum 8160 processor using 8 cores and 40GB RAM, as well as a V100-SXM2 GPU.

For the MNIST and YOLO robostness verification benchmarks, an AWS instance of type g5.2xlarge was used, with is equipped with a AMD EPYC 7R32 processor with 8 cores, 32GB RAM, as well as an A10G GPU.

## Appendix H Robustness Verification

For the comparison in Section 4.2, we have implemented the concept of INVPROP in \(\alpha\),\(\beta\)-CROWN. Figure 9 compares the performance of \(\alpha\),\(\beta\)-CROWN and \(\alpha\),\(\beta\)-CROWN + INVPROP on the MNIST benchmark defined by Yang et al. (2021).

Figure 9: Detailed runtime comparison of \(\alpha\),\(\beta\)-CROWN and \(\alpha\),\(\beta\)-CROWN + INVPROP. Except for a few instances, \(\alpha\),\(\beta\)-CROWN extended with INVPROP can verify the properties faster, and can prove robustness for some instances that cause a timeout for pure \(\alpha\),\(\beta\)-CROWN. The timeout per instance is 5 minutes.