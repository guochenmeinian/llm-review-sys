# Robust covariance estimation with missing values and cell-wise contamination

Karim Lounici

CMAP

Ecole Polytechnique

Palaiseau, France

karim.lounici@polytechnique.edu

&Gregoire Pacreau

CMAP

Ecole Polytechnique

Palaiseau, France

gregoire.pacreau@polytechnique.edu

###### Abstract

Large datasets are often affected by cell-wise outliers in the form of missing or erroneous data. However, discarding any samples containing outliers may result in a dataset that is too small to accurately estimate the covariance matrix. Moreover, the robust procedures designed to address this problem require the invertibility of the covariance operator and thus are not effective on high-dimensional data. In this paper, we propose an unbiased estimator for the covariance in the presence of missing values that does not require any imputation step and still achieves near minimax statistical accuracy with the operator norm. We also advocate for its use in combination with cell-wise outlier detection methods to tackle cell-wise contamination in a high-dimensional and low-rank setting, where state-of-the-art methods may suffer from numerical instability and long computation times. To complement our theoretical findings, we conducted an experimental study which demonstrates the superiority of our approach over the state of the art both in low and high dimension settings.

## 1 Introduction

Outliers are a common occurrence in datasets, and they can significantly affect the accuracy of data analysis. While research on outlier detection and treatment has been ongoing since the 1960s, much of it has focused on cases where entire samples are outliers (Huber's contamination model) [11][13]. While sample-wise contamination is a common issue in many datasets, modern data analysis often involves combining data from multiple sources. For example, data may be collected from an array of sensors, each with an independent probability of failure, or financial data may come from multiple companies, where reporting errors from one source do not necessarily impact the validity of the information from the other sources. Discarding an entire sample as an outlier when only a few features are contaminated can result in the loss of valuable information, especially in high-dimensional datasets where samples are already scarce. It is important to identify and address the specific contaminated features, rather than simply treating the entire sample as an outlier. In fact, if each dimension of a sample has a contamination probability of \(\varepsilon\), then the probability of that sample containing at least one outlier is given by \(1-(1-\varepsilon)^{p}\), where \(p\) is the dimensionality of the sample. In high dimension, this probability can quickly exceed \(50\%\), surpassing the breakdown point of many robust estimators designed for the Huber sample-wise contamination setting. Hence, it is crucial to develop robust methods that can handle cell-wise contaminations and still provide accurate results.

The issue of cell-wise contamination, where individual cells in a dataset may be contaminated, was first introduced in [3]. However, the issue of missing data due to outliers was studied much earlier, dating back to the work of [3]. Although missing values in a dataset are much easier to detect than outliers, they can lead to errors in estimating the location and scale of the underlying distribution [23]and can negatively affect the performance of supervised learning algorithms [17]. This motivated the development of the field of data imputation. Several robust estimation methods have been proposed to handle missing data, including Expectation Maximization (EM)-based algorithms [7], maximum likelihood estimation [14] and Multiple Imputation [25], among which we can find k-nearest neighbor imputation [39] and iterative imputation [42]. Recently, sophisticated solutions based on deep learning, GANs [47][38], VAE [27] or Diffusion schemes [43] have been proposed to perform complex tasks like artificial data generation or image inpainting. The aforementioned references focus solely on minimising the entrywise error for imputed entries. Noticeably, our practical findings reveal that applying state-of-the-art imputation methods to complete the dataset, followed by covariance estimation on the completed dataset, does not yield satisfactory results when evaluating the covariance estimation error using the operator norm.

In comparison to data missingness or its sample-wise counterpart, the cell-wise contamination problem is less studied. The Detection Imputation (DI) algorithm of [32] is an EM type procedure combining a robust covariance estimation method with an outlier detection method to iteratively update the covariance estimation. Other methods include adapting methodology created for Huber contamination for the cell-wise problem, such as in [6] or [2]. In high dimensional statistics, however, most of these methods fail due to high computation time and numerical instability. Or they are simply not designed to work in this regime since they are based on the Mahalanobis distance, which requires an inversion of the estimated covariance matrix. This is a major issue since classical covariance matrix estimators have many eigenvalues close to zero or even exactly equal to zero in high-dimension. To the best of our knowledge, no theoretical result exists concerning the statistical accuracy of these methods in the cell-wise contamination setting contrarily to the extensive literature on Huber's contamination [1].

**Contributions.** In this paper we address the problem of high-dimensional covariance estimation in the presence of missing observations and cell-wise contamination. To formalize this problem, we adopt and generalize the setting introduced in [10]. We propose and investigate two different strategies, the first based on filtering outliers and debiasing and the second based on filtering outliers followed by imputation and standard covariance estimation. We propose novel computationally efficient and numerically stable procedures that avoid matrix inversion, making them well-suited for high-dimensional data. We derive non-asymptotic estimation bounds of the covariance with the operator norm and minimax lower bounds, which clarify the impact of the missing value rate and outlier contamination rate. Our theoretical results also improve over [26] in the MCAR and no contamination. Next, we conduct an experimental study on synthetic data, comparing our proposed methods to the state-of-the-art (SOTA) methods. Our results demonstrate that SOTA methods fail in the high-dimensional regime due to matrix inversions, while our proposed methods perform well in this regime, highlighting their effectiveness. Then we demonstrate the practical utility of our approach by applying it to real-life datasets, which highlights that the use of existing estimation methods significantly alters the spectral properties of the estimated covariance matrices. This implies

Figure 1: Left: Estimation error of the covariance matrix for \(n=100\), \(p=50\), \(\mathbf{r}(\Sigma)=2\) under a Dirac contamination (tailMV and DDCMV are our methods). Here \(\varepsilon=1\) and \(\delta\) varies in \((0,1)\). Right: For each method, mean computation time (in seconds) over 20 repetitions and whether it uses matrix inversion. For \(p=100\), we had to raise \(r\left(\Sigma\right)\) to \(10\) otherwise both DI and TSGS would fail due to numerical instability.

that cell-wise contamination can significantly impact the results of dimension reduction techniques like PCA by completely altering the computed principal directions. Our experiments demonstrate that our methods are more robust to cell-wise contamination than SOTA methods and produce reliable estimates of the covariance.

## 2 Missing values and cell-wise contamination setting

Let \(X_{1},\ldots,X_{n}\) be \(n\) i.i.d. copies of a zero mean random vector \(X\) admitting unknown covariance operator \(\Sigma=\mathbb{E}\left[X\otimes X\right]\), where \(\otimes\) is the outer product. Denote by \(X_{i}^{(j)}\) the \(j\)th component of vector \(X_{i}\) for any \(j\in[p]\). All our results are non-asymptotic and cover a wide range of configurations for \(n\) and \(p\) including the high-dimensional setting \(p\gg n\). In this paper, we consider the following two realistic scenarios where the measurements are potentially corrupted.

Missing values.We assume that each component \(X_{i}^{(j)}\) is observed independently from the others with probability \(\delta\in(0,1]\). Formally, we observe the random vector \(Y\in\mathbb{R}^{p}\) defined as follows:

\[Y_{i}^{(j)}=d_{i,j}X_{i}^{(j)},1\leq i\leq n,1\leq j\leq p \tag{1}\]

where \(d_{ij}\) are independent realisations of a bernoulli random variable of parameter \(\delta\). This corresponds the Missing Completely at Random (MCAR) setting of [35]. Our theory also covers the more general Missing at Random (MAR) setting in Theorem [2]

Cell-wise contamination.Here we assume that some missing components \(X_{i}^{(j)}\) can be replaced with probability \(\varepsilon\) by some independent noise variables, representing either a poisoning of the data or random mistakes in measurements. The observation vector \(Y\) then satisfies:

\[Y_{i}^{(j)}=d_{i,j}X_{i}^{(j)}+(1-d_{i,j})e_{i,j}\xi_{i}^{(j)},1\leq i\leq n,1 \leq j\leq p \tag{2}\]

where \(\xi_{i}^{(j)}\) are independent erroneous measurements and \(e_{i,j}\) are i.i.d. bernoulli random variables with parameter \(\varepsilon\). We also assume that all the variables \(X_{i}\), \(\xi_{i}^{(j)}\), \(d_{i,j}\), \(e_{i,j}\) are mutually independent. In this scenario, a component \(X_{i}^{(j)}\) is either perfectly observed with probability \(\delta\), replaced by a random noise with probability \(\varepsilon^{\prime}=\varepsilon(1-\delta)\) or missing with probability \((1-\delta)(1-\varepsilon)\). Cell-wise contamination as introduced in [3] corresponds to the case where \(\varepsilon=1\), and thus \(\varepsilon^{\prime}=1-\delta\).

In both of these settings, the task of estimating the mean of the random vectors \(X_{i}\) is well-understood, as it reduces to the classical Huber setting for component-wise mean estimation. One could for instance apply the Tuker median on each component separately [3]. However, the problem becomes more complex when we consider non-linear functions of the data, such as the covariance operator. Robust covariance estimators originally designed for the Huber setting may not be suitable when applied in the presence of missing values or cell-wise contaminations.

We study a simple estimator based on a correction of the classical covariance estimator on \(Y_{1},\ldots,Y_{n}\) as introduced in [2] for the missing values scenario. The procedure is based on the following observation, linking \(\Sigma^{Y}\) the covariance of the data with missing values and \(\Sigma\) the true covariance:

\[\Sigma=\left(\delta^{-1}-\delta^{-2}\right)\text{diag}(\Sigma^{Y})+\delta^{-2 }\Sigma^{Y} \tag{3}\]

Note that this formula assumes the knowledge of \(\delta\). In the missing values scenario, \(\delta\) can be efficiently estimated by a simple count of the values exactly set to \(0\) or equal to NaN (not a number). In the contamination setting [2], the operator \(\Sigma^{Y}=\mathbb{E}\left(Y\otimes Y\right)\) satisfies, for \(\Lambda=\mathbb{E}\left[\xi\otimes\xi\right]\):

\[\Sigma^{Y}=\delta^{2}\Sigma+(\delta-\delta^{2})\text{diag}(\Sigma)+ \varepsilon(1-\delta)\Lambda.\]

In this setting, as one does not know the exact location and number of outliers we propose to estimate \(\delta\) by the proportion of data remaining after the application of a filtering procedure.

Notations.We denote by \(\odot\) the Hadamard (or term by term) product of two matrices and by \(\otimes\) the outer product of vectors, i.e. \(\forall x,y\in\mathbb{R}^{d},x\otimes y=xy^{\top}\). We denote by \(\left\|.\right\|\) and \(\left\|.\right\|_{F}\) the operator and Frobenius norms of a matrix respectively. We denote by \(\left\|.\right\|_{2}\) the vector \(l_{2}\)-norm.

## 3 Estimation of covariance matrices with missing values

We consider the scenario outlined in [1] where the matrix \(\Sigma\) is of approximately low rank. To quantify this, we use the concept of effective rank, which provides a useful measure of the inherent complexity of a matrix. Specifically, the effective rank of \(\Sigma\) is defined as follows

\[\mathbf{r}(\Sigma):=\frac{\mathbb{E}\left\|X\right\|_{2}^{2}}{\left\|\Sigma\right\| }=\frac{\text{tr}\left(\Sigma\right)}{\left\|\Sigma\right\|} \tag{4}\]

We note that \(0\leq\mathbf{r}(\Sigma)\leq\text{rank}(\Sigma)\). Furthermore, for approximately low rank matrices with rapidly decaying eigenvalues, we have \(\mathbf{r}(\Sigma)\ll\text{rank}(\Sigma)\). This section presents a novel analysis of the estimator defined in equation [3], which yields a non-asymptotic minimax optimal estimation bound in the operator norm. Our findings represent a substantial enhancement over the suboptimal guarantees reported in [26][18]. Similar results could be established for the Frobenius norm using more straightforward arguments, as those in [4] or [31]. We give priority to the operator norm since it aligns naturally with learning tasks such as PCA. See [19][21][22] and the references cited therein.

We need the notion of Orlicz norms. For any \(\alpha\geq 1\), the \(\psi_{\alpha}\)-norms of a real-valued random variable \(V\) are defined as: \(\left\|V\right\|_{\psi_{\alpha}}=\inf\{u>0,\mathbb{E}\exp\left(|V|^{\alpha}/u ^{\alpha}\right)\leq 2\}\). A random vector \(X\in\mathbb{R}^{p}\) is sub-Gaussian if and only if \(\forall x\in\mathbb{R}^{p}\), \(\left\|\left\langle X,x\right\rangle\right\|_{\psi_{2}}\lesssim\left\|\left\langle X,x\right\rangle\right\|_{L^{2}}\).

Minimax lower-bound.We now provide a minimax lower bound for the covariance estimation with missing values problem. Let \(\mathcal{S}_{p}\) the set of \(p\times p\) symmetric semi-positive matrices. Then, define \(\mathcal{C}_{\overline{r}}=\{S\in\mathcal{S}_{p}:\mathbf{r}(S)\leq\overline{r}\}\) the set of matrices of \(\mathcal{S}_{p}\) with effective rank at most \(\overline{r}\).

**Theorem 1**.: _Let \(p,n,\overline{r}\) be strictly positive integers such that \(p\geq\max\{n,2\overline{r}\}\). Let \(X_{1},\ldots,X_{n}\) be i.i.d. random vectors in \(\mathbb{R}^{p}\) with covariance matrix \(\Sigma\in\mathcal{C}_{\overline{r}}\). Let \((d_{i,j})_{1\leq i\leq n,1\leq j\leq p}\) be an i.i.d. sequence of Bernoulli random variables with probability of success \(\delta\in(0,1]\), independent from the \(X_{1},\ldots,X_{n}\). We observe \(n\) i.i.d. vectors \(Y_{1},\ldots,Y_{n}\in\mathbb{R}^{p}\) such that \(Y_{i}^{(j)}=d_{i,j}X_{i}^{(j)}\), \(i\in[n]\), \(j\in[p]\). Then there exists two absolute constants \(C>0\) and \(\beta\in(0,1)\) such that:_

\[\inf_{\widehat{\Sigma}}\max_{\widehat{\Sigma}\in\mathcal{C}_{\overline{r}}} \mathbb{P}_{\Sigma}\left(\left\|\widehat{\Sigma}-\Sigma\right\|\geq C\frac{ \left\|\Sigma\right\|}{\delta}\sqrt{\frac{\mathbf{r}(\Sigma)}{n}}\right)\geq\beta \tag{5}\]

_where \(\inf_{\widehat{\Sigma}}\) represents the infimum over all estimators \(\widehat{\Sigma}\) of matrix \(\Sigma\) based on \(Y_{1},\ldots,Y_{n}\)._

Sketch of proof.: We first build a sufficiently large test set of hard-to-learn covariance operators exploiting entropy properties of the Grassmann manifold such that the distance between any two distinct covariance operator is at least of the order \(\frac{\left\|\Sigma\right\|}{\delta}\sqrt{\frac{\mathbf{r}(\Sigma)}{n}}\). Next, in order to control the Kullback-Leibler divergence of the observations with missing values, we exploit in particular interlacing properties of the eigenvalues of the perturbed covariance operators [37]. 

This lower bound result improves upon [26] Theorem 2] as it relaxes the hypotheses on \(n\) and \(\overline{r}\). More specifically, the lower bound in [26] requires \(n\geq 2\overline{r}^{2}/\delta^{2}\) while we only need the mild assumption \(p\geq\max\{n,2\overline{r}\}\). Our proof leverages the properties of the Grassmann manifold, which has been previously utilized in different settings such as sparse PCA without missing values or contamination [45] and low-rank covariance estimation without missing values or contamination [23]. However, tackling missing values in the Grassmann approach adds a technical challenge to these proofs as they modify the distribution of observations. Our proof requires several additional nontrivial arguments to control the distribution divergences, which is a crucial step in deriving the minimax lower bound.

Non-asymptotic upper-bound in the operator norm.We provide an upper bound of the estimation error in operator norm. We write \(Y_{i}=d_{i}\odot X_{i}\). Let \(\widehat{\Sigma}^{Y}=n^{-1}\sum_{i=1}^{n}Y_{i}\otimes Y_{i}\) be the classical covariance estimator of the covariance of \(Y\). When the dataset contains missing values and corruptions, \(\widehat{\Sigma}^{Y}\) is a biased estimator of \(\Sigma\). Exploiting Equation [3], [26] proposed the following unbiased estimator of the covariance matrix \(\Sigma\):

\[\widehat{\Sigma}=\delta^{-2}\widehat{\Sigma}^{Y}+(\delta^{-1}-\delta^{-2}) \text{diag}\left(\widehat{\Sigma}^{Y}\right). \tag{6}\]

The following result is from [18] Theorem 4.2].

**Lemma 1**.: _Let \(X_{1},\ldots,X_{n}\) be i.i.d. sub-Gaussian random variables in \(\mathbb{R}^{p}\), with covariance matrix \(\Sigma\), and let \(d_{ij},i\in[1,n],j\in[1,p]\) be i.i.d bernoulli random variables with probability of success \(\delta>0\). Then there exists an absolute constant \(C\) such that, for \(t>0\), with probability at least \(1-e^{-t}\):_

\[\left\|\widehat{\Sigma}-\Sigma\right\|\leq C\left\|\Sigma\right\|\left(\sqrt{ \frac{\mathbf{r}(\Sigma)\log\mathbf{r}(\Sigma)}{\delta^{2}n}}\vee\sqrt{\frac{t}{\delta ^{2}n}}\vee\frac{\mathbf{r}(\Sigma)(t+\log\mathbf{r}(\Sigma))}{\delta^{2}n}\log(n)\right) \tag{7}\]

This result uses a recent unbounded version of the non-commutative Bernstein inequality, thus yielding some improvement upon the previous best known bound of [26]. Theorem[1] and Lemma[1] provide some important insights on the minimax rate of estimation in the missing values setting. In the high-dimensional regime \(p\geq\max\{n,2\overline{r}\}\) and \(n\geq\delta^{-2}\mathbf{r}(\Sigma)(\log\mathbf{r}(\Sigma))\log^{2}n\), we observe that the two bounds coincide up to a logarithmic factor in \(\mathbf{r}(\Sigma)\), hence clarifying the impact of missing data on the estimation rate via the parameter \(\delta\).

Heterogeneous missingness.We can extend the correction to the more general case where each feature has a different missing value rate known as the Missing at Random (MAR) setting in [35]. We denote by \(\delta_{j}\in(0,1]\) the probability to observe feature \(X^{(j)}\), \(1\leq j\leq p\) and we set \(\delta:=(\delta_{j})_{j\in[p]}\). As in the MCAR setting, the probabilities \((\delta_{j})_{j\in[p]}\) can be readily estimated by tallying the number of missing entries for each feature. Hence they will be assumed to be known for the sake of brevity. Let \(\delta_{\text{inv}}=(\delta_{j}^{-1})_{j\in[p]}\) be the vector containing the inverse of the observing probabilities and \(\Delta_{\text{inv}}=\delta_{\text{inv}}\otimes\delta_{\text{inv}}\). In this case, the corrected estimator becomes :

\[\widehat{\Sigma}=\Delta_{\text{inv}}\odot\widehat{\Sigma}^{Y}+\left(\text{diag }\left(\delta_{\text{inv}}\right)-\Delta_{\text{inv}}\right)\odot\text{diag} \left(\widehat{\Sigma}^{Y}\right) \tag{8}\]

Let \(\bar{\delta}=\max_{j}\{\delta_{j}\}\) and \(\delta=\min_{j}\{\delta_{j}\}\) be the largest and smallest probabilities to observe a feature.

**Theorem 2**.: _(i) Let \(X_{1},\ldots,X_{n}\) be i.i.d. sub-Gaussian random variables in \(\mathbb{R}^{p}\), with covariance matrix \(\Sigma\). We consider the MAR setting described above. Then the estimator [8] satisfies, for any \(t>0\), with probability at least \(1-e^{-t}\)_

\[\left\|\widehat{\Sigma}-\Sigma\right\|\leq C\left\|\Sigma\right\|\frac{\bar{ \delta}}{\bar{\delta}^{2}}\left(\sqrt{\frac{\mathbf{r}(\Sigma)\log\mathbf{r}(\Sigma)}{ n}}\vee\sqrt{\frac{t}{n}}\vee\frac{\mathbf{r}(\Sigma)(t+\log\mathbf{r}(\Sigma))}{\bar{ \delta}n}\log n\right) \tag{9}\]

_(ii) Let \(p,n,\overline{r}\) be strictly positive integers such that \(p\geq\max\{n,2\overline{r}\}\). Let \(X_{1},\ldots,X_{n}\) be i.i.d. random vectors in \(\mathbb{R}^{p}\) with covariance matrix \(\Sigma\in\mathcal{C}_{\overline{r}}\). Then,_

\[\inf_{\widehat{\Sigma}}\max_{\Sigma\in\mathcal{C}_{\overline{r}}}\mathbb{P}_{ \Sigma}\left(\left\|\widehat{\Sigma}-\Sigma\right\|\geq C\frac{\left\|\Sigma \right\|}{\bar{\delta}}\sqrt{\frac{\mathbf{r}(\Sigma)}{n}}\right)\geq\beta. \tag{10}\]

If \(\bar{\delta}\asymp\bar{\delta}\) then the rates for the MCAR and MAR settings match. The proof is a straightforward adaptation of the proof in the MCAR setting.

## 4 Optimal estimation of covariance matrices with cell-wise contamination

In this section, we consider the cell-wise contamination setting [2].We derive both an upper bound on the operator norm error of the estimator [6] and a minimax lower bound for this specific setting. Let us assume that the \(\xi_{1},\ldots\xi_{n}\) are sub-Gaussian r.v. Note also that \(\Lambda:=\mathbb{E}[\xi_{1}\otimes\xi_{1}]\) is diagonal in the cell-wise contamination setting [2].

Minimax lower-bound.The lower bound for missing values still applies to the contaminated case as missing values are a particular case of cell-wise contamination. But we want a more general lower bound that also covers the case of adversarial contaminations.

**Theorem 3**.: _Let \(p,n,\overline{r}\) be strictly positive integers such that \(p\geq\max\{n,2\overline{r}\}\). Let \(X_{1},\ldots,X_{n}\) be i.i.d. random vectors in \(\mathbb{R}^{p}\) with covariance matrix \(\Sigma\in\mathcal{C}_{\overline{r}}\). Let \((\tilde{d}_{i,j})_{1\leq i\leq n,1\leq j\leq p}\) be i.i.d. sequence of bernoulli random variables of probability of success \(\delta\in(0,1]\), independent to the \(X_{1},\ldots,X_{n}\). We observe \(n\) i.i.d. vectors \(Y_{1},\ldots,Y_{n}\in\mathbb{R}^{p}\) satisfying [2] where \(\xi_{i}\) are i.i.d. of arbitrary distribution \(Q\). Then there exists two absolute constants \(C>0\) and \(\beta\in(0,1)\) such that:_

\[\inf_{\widehat{\Sigma}}\max_{\Sigma\in\mathcal{C}_{\overline{r}}}\max_{Q} \mathbb{P}_{\Sigma,Q}\left(\left\|\widehat{\Sigma}-\Sigma\right\|\geq C\frac{ \left\|\Sigma\right\|}{\delta}\sqrt{\frac{\mathbf{r}(\Sigma)}{n}}\bigvee\frac{ \varepsilon(1-\delta)}{\delta}\right)\geq\beta \tag{11}\]_where \(\inf_{\widehat{\Sigma}}\) represents the infimum over all estimators of matrix \(\Sigma\) and \(\max_{Q}\) is the maximum over all contamination \(Q\)._

The proof of this theorem adapts an argument developed to derive minimax lower bounds in the Huber contamination setting. See App. C.3 for the full proof.

Non-asymptotic upper-bound in the operator norm.Note that the term \(\varepsilon(1-\delta)\Lambda\) in the cell-wise contamination setting is negligible when \(\delta\approx 1\) or \(\varepsilon\approx 0\). Using the DDC detection procedure of [23], we can detect the contaminations and make \(\varepsilon\) smaller without decreasing \(\delta\) too much. For simplicity, we assume from now on that the \(\xi_{i}^{(j)}\) are i.i.d. with common variance \(\sigma_{\xi}^{2}\). Hence \(\Lambda=\sigma_{\xi}^{2}I_{p}\). We further assume that the \(\xi_{i}^{(j)}\) are sub-Gaussian since we observed in our experiments that filtering removed all the large-valued contaminated cells and only a few inconspicuous contaminated cells remained. Our procedure [6] satisfies the following result.

**Theorem 4**.: _Let the assumptions of Theorem [] be satisfied. We assume in addition that the observations \(Y_{1},\ldots,Y_{n}\) satisfy \(\boldsymbol{\widehat{2}}\) with \(\varepsilon\in[0,1)\) and \(\delta\in(0,1]\) and i.i.d. sub-Gaussian \(\xi_{i}^{(j)}\)'s. Then, for any \(t>0\), with probability at least \(1-e^{-t}\):_

\[\left\|\widehat{\Sigma}-\Sigma\right\| \lesssim\|\Sigma\|\left(\sqrt{\frac{\boldsymbol{r}(\Sigma)\log \boldsymbol{r}(\Sigma)}{\delta^{2}n}}\vee\sqrt{\frac{t}{\delta^{2}n}}\vee\frac {\boldsymbol{r}(\Sigma)(t+\log\boldsymbol{r}(\Sigma))}{\delta^{2}n}\log(n) \right)+\frac{\varepsilon(1-\delta)\sigma_{\xi}^{2}}{\delta}\] \[\qquad+\frac{(1-\delta)\varepsilon}{\delta^{2}\sqrt{|\log((1- \delta)\varepsilon)|}}\sigma_{\xi}^{2}\left(\sqrt{\frac{p}{n}}\vee\frac{p}{n} \vee\sqrt{\frac{t}{n}}\vee\frac{t}{n}\right)\] \[\qquad+D(\delta,p)\sqrt{\frac{t+\log(p)}{n}}+\sqrt{\delta(1- \delta)\varepsilon\,\sigma_{\xi}^{2}\,p}\sqrt{\mathrm{tr}(\Sigma)}\log(n)\frac {t+\log(p)}{n},\]

_where \(D(\delta,p)=\sqrt{\frac{(1-\delta)}{\delta^{2}}\varepsilon(p-2)\sigma_{\xi}^{ 2}\left[2\left\|\Sigma\right\|+\sigma_{\xi}^{2}\right]+\frac{(1-\delta)}{ \delta^{3}}\varepsilon\sigma_{\xi}^{4}\left(|\mathrm{tr}(\Sigma)-\delta(p-2)| +\left\|\Sigma\right\|\right)}\)._

See App F.3 for the proof. As emphasized in [20], the effective rank \(\boldsymbol{r}(\Sigma)\) provides a measure of the statistical complexity of the covariance learning problem in the absence of any contamination. However, when cell-wise contamination is present, the statistical complexity of the problem may increase from \(\boldsymbol{r}(\Sigma)\) to \(\boldsymbol{r}(\Lambda)=p\). Fortunately, if the filtering process reduces the proportion of cell-wise contamination \(\varepsilon\) such that \((1-\delta)\varepsilon\,\mathrm{tr}(\Lambda)\leq\delta\mathrm{tr}(\Sigma)\) and \(\varepsilon\parallel\Lambda\parallel\leq\delta\left\|\Sigma\right\|\). Then we can effectively mitigate the impact of cell-wise contamination. Indeed, we deduce from Theorem [] that

\[\left\|\widehat{\Sigma}-\Sigma\right\| \lesssim\|\Sigma\|\left(\sqrt{\frac{\boldsymbol{r}(\Sigma)\log \boldsymbol{r}(\Sigma)}{\delta^{2}n}}\vee\sqrt{\frac{t}{\delta^{2}n}}\vee\frac {\boldsymbol{r}(\Sigma)(t+\log\boldsymbol{r}(\Sigma))}{\delta^{2}n}\log(n) \right)+\frac{\varepsilon(1-\delta)\sigma_{\xi}^{2}}{\delta}\] \[\qquad+\frac{1}{\delta}\sqrt{(1-\delta)\mathrm{tr}(\Sigma)}\sqrt {\frac{t+\log(p)}{n}}\bigg{(}\sqrt{\delta\,\sigma_{\xi}^{2}}+\sqrt{\mathrm{tr} (\Sigma)}\log(n)\sqrt{\frac{t+\log(p)}{n}}\bigg{)}, \tag{12}\]

where we considered for convenience the reasonable scenario where \(\delta\left(p-2\right)\geq\mathrm{tr}(\Sigma)\) and \(\sigma_{\xi}^{2}\geq\left\|\Sigma\right\|\). The combination of the upper bound ([12] with the lower bound in Theorem 3 provides the first insights into the impact of cell-wise contamination on covariance estimation.

## 5 Experiments

In our experiments, MV refers either to the debiased MCAR covariance estimator [6] or to its MAR extension [8]. The synthetic data generation is described in App. A. We also performed experiments on real life datasets described in App. B. All experiments were conducted on a 2020 MacBook Air with a M1 processor (8 cores, 3.4 GHz). [1]

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

### The effect of cell-wise contamination on real-life datasets

We tested the methods on \(8\) datasets from sklearn and Woolridge's book on econometrics [40]. These are low dimensional datasets (less than \(20\) features) representing various medical, social and economic phenomena. We also included \(2\) high-dimensional datasets. See App. [3] for the list of the datasets.

One interesting observation is that the instability of Mahalanobis distance-based algorithms is not limited to high-dimensional datasets. Even datasets with a relatively small number of features can exhibit instability. This can be seen in the performance of DI on the Attend dataset, as depicted in Figure 8 where it fails to provide accurate results. Similarly, both TSGS and DI fail to perform well on the CEOSAL2 dataset, as shown in Figure [4] despite both datasets having fewer than \(15\) features.

On the Abalone dataset, once we have removed 4 obvious outliers (which are detected by both DDC and the tail procedure), all estimators reached a consensus with the non-robust classical estimator, meaning that this dataset provides a ground truth against which we can evaluate and compare the performance of robust procedures in our study. To this end, we artificially contaminate \(5\%\) of the cells at random in the dataset with a Dirac contamination and compare the spectral error of the different robust estimators. As expected, TSGS and all our new procedures succeed at correcting the error, however DI becomes unstable (see Table 3). DDC MIWAE is close to SOTA TSGS for cellwise contamination and DDC II performs better. We also performed experiments on two high-dimensional datasets, where our methods return stable estimates of the covariance (DDCMV99 and DDCMV95 are within \(\approx 3\%\) of each other) and farther away from the classical estimator (See Figures [4] and [3]). Note also that DDCII's computation time explodes and even returns out-of-memory errors due to the high computation cost of II that we already highlighted in Table [1]

Figure 4: Estimation error as a function of the contamination rate for \(n=500\), \(p=400\), \(\mathbf{r}(\Sigma)=5\) and Dirac contamination.

Figure 5: Estimation error as a function of the contamination rate for \(n=500\), \(p=400\), \(\mathbf{r}(\Sigma)=5\) and Gaussian contamination.

Figure 6: DI fails on ATTEND since the covariance matrix is approximately low rank. The dataset has only \(8\) features and the effective rank of its covariance matrix is below \(2\).

Figure 7: Woolridge’s CEOSAL dataset fails both TSGS and DI with its dimension of \(13\) and effective rank of around \(2.5\).

## 6 Conclusion and future work

In this paper, we have extended theoretical guarantees on the spectral error of our covariance estimators robust to missing data to the missing at random setting. We have also derived the first theoretical guarantees in the cell-wise contamination setting. We highlighted in our numerical experimental study that in the missing value setting, our debiased estimator designed to tackle missing values without imputation offers statistical accuracy similar to the SOTA IterativeImputer for a dramatic computational gain. We also found that SOTA algorithms in the cell-wise contamination setting often fail in the standard setting \(p<n\) for dataset with fast decreasing eigenvalues (resulting in approximately low rank covariance), a setting which is commonly encountered in many real life applications. This is due to the fact that these methods use matrix inversion which is unstable to small eigenvalues in the covariance structure and can even fail to return any estimate. In contrast, we showed that our strategy combining filtering with estimation procedures designed to tackle missing values produce far more stable and reliable results. In future work, we plan to improve our theoretical upper and lower bounds in the cell-wise contamination setting to fully clarify the impact of this type of contamination in covariance estimation.

Acknowledgements.This paper is based upon work partially supported by the Chaire _Business Analytic for Future Banking_ and EU Project ELIAS under grant agreement No. 101120237.

## References

* [1] Pedro Abdalla and Nikita Zhivotovskiy. Covariance Estimation: Optimal Dimension-free Guarantees for Adversarial Corruption and Heavy Tails, July 2023.
* [2] Claudio Agostinelli, Andy Leung, Victor J. Yohai, and Ruben H. Zamar. Robust estimation of multivariate location and scatter in the presence of cellwise and casewise contamination, June 2014.
* [3] Fatemah Alqallaf, Stefan Van Aelst, Victor J. Yohai, and Ruben H. Zamar. Propagation of outliers in multivariate data. _The Annals of Statistics_, 37(1):311-331, February 2009.
* [4] Florentina Bunea and Luo Xiao. On the sample covariance matrix estimator of reduced effective rank population matrices, with applications to fPCA. _Bernoulli_, 21(2), May 2015.
* [5] Mengjie Chen, Chao Gao, and Zhao Ren. Robust Covariance and Scatter Matrix Estimation under Huber's Contamination Model. _arXiv:1506.00691 [math, stat]_, June 2017.
* [6] Mike Danilov, Victor Yohai, and Ruben Zamar. Robust Estimation of Multivariate Location and Scatter in the Presence of Missing Data. _JASA. Journal of the American Statistical Association_, 107, September 2012.
* [7] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data via the EM Algorithm. _Journal of the Royal Statistical Society. Series B (Methodological)_, 39(1):1-38, 1977.
* [8] Sheela Devadas, Peter J Haine, and Keaton Stubis. The Schur-Horn Theorem. 2015.
* [9] Weinan Dong, Daniel Yee Tak Fong, Jin-sun Yoon, Eric Yuk Fai Wan, Laura Elizabeth Bedford, Eric Ho Man Tang, and Cindy Lo Kuen Lam. Generative adversarial networks for imputing missing data for big data clinical research. _BMC Medical Research Methodology_, 21(1):78, April 2021.
* [10] Alessio Farcomeni. Robust Constrained Clustering in Presence of Entry-Wise Outliers. _Technometrics_, 56, February 2014.
* [11] Peter J. Huber. Robust Estimation of a Location Parameter. _The Annals of Mathematical Statistics_, 35(1):73-101, 1964.
* [12] Peter J. Huber and Elvezio M. Ronchetti. _Robust Statistics, 2nd Edition | Wiley_. Wiley Series in Probability and Statistics. John Wiley and Sons, Inc., 2009.
* [13] Mia Hubert, Michiel Debruyne, and Peter J. Rousseeuw. Minimum Covariance Determinant and Extensions. _WIREs Computational Statistics_, 10(3), May 2018.
* [14] Mortaza Jamshidian and Peter M. Bentler. ML Estimation of Mean and Covariance Structures with Missing Data Using Complete Data Routines. _Journal of Educational and Behavioral Statistics_, 24(1):21-41, 1999.
* [15] Daniel Jarrett, Bogdan Cebere, Tennison Liu, Alicia Curth, and Mihaela van der Schaar. Hyperimpute: Generalized iterative imputation with automatic model selection. 2022.
* [16] Charles R. Johnson. Matrix theory and applications (charles r. johnson, ed.). _Proceedings of symposia in applied mathematics_, 40, 1989.
* [17] Julie Josse, Nicolas Prost, Erwan Scornet, and Gael Varoquaux. On the consistency of supervised learning with missing values. _arXiv:1902.06931 [cs, math, stat]_, July 2020.
* [18] Yegor Klochkov and Nikita Zhivotovskiy. Uniform Hanson-Wright type concentration inequalities for unbounded entries via the entropy method, August 2019.
* 2013, 2016.
* [20] Vladimir Koltchinskii and Karim Lounici. Concentration inequalities and moment bounds for sample covariance operators. _Bernoulli_, 23(1):110-133, February 2017.
* [21] Vladimir Koltchinskii and Karim Lounici. New asymptotic results in principal component analysis. _Sankhya A_, 79(2):254-297, Aug 2017.
* 157, 2017.
* [23] Vladimir Koltchinskii, Karim Lounici, and Alexander B. Tsybakov. Estimation of Low-Rank Covariance Function, April 2015.
* [24] Andy Leung, Hongyang Zhang, and Ruben H. Zamar. Robust regression estimation and inference in the presence of cellwise and casewise contamination. _Computational Statistics & Data Analysis_, 99:1-11, July 2016.
* [25] Roderick Little and Donald Rubin. Statistical Analysis with Missing Data, Second Edition. Wiley Series in Probability and Mathematical Statistics. Probability and Mathematical Statistics. Wiley edition, 2002.
* [26] Karim Lounici. High-dimensional covariance matrix estimation with missing observations. _Bernoulli_, 20(3):1029-1058, August 2014.
* [27] Chao Ma, Sebastian Tschiatschek, Jose Miguel Hernandez-Lobato, Richard Turner, and Cheng Zhang. VAEM: A Deep Generative Model for Heterogeneous Mixed Type Data, June 2020.
* [28] Pierre-Alexandre Mattei and Jes Frellsen. MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 4413-4423. PMLR, 2019.
* [29] Alain Pajor. Metric Entropy of the Grassmann Manifold. _Complex Geometry Analysis_, 34:181-188, 1998.
* [30] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* [31] Nikita Puchkin, Fedor Noskov, and Vladimir Spokoiny. Sharper dimension-free bounds on the Frobenius distance between sample covariance and its expectation, August 2023.
* [32] Jakob Raymaekers and Peter J. Rousseeuw. Handling cellwise outliers by sparse regression and robust covariance. _arXiv:1912.12446 [stat]_, December 2020.
* [33] Jakob Raymaekers and Peter J. Rousseeuw. Fast robust correlation for high-dimensional data. _Technometrics_, 63(2):184-198, April 2021.
* [34] Peter J. Rousseeuw and Wannes Van den Bossche. Detecting deviating data cells. _Technometrics_, 60(2):135-145, April 2018.
* [35] Donald B. Rubin. Inference and Missing Data. _Biometrika_, 63(3):581-592, 1976.
* [36] Eckhard Schlemm. The kearns-sault inequality for bernoulli and poisson-binomial distributions. _Journal of theoretical probability_, 29:48-62, 2016.
* [37] R. C. Thompson. Principal submatrices of normal and Hermitian matrices. _Illinois Journal of Mathematics_, 10(2):296-308, June 1966.
* [38] Joel A. Tropp. An introduction to matrix concentration inequalities. _Foundations and Trends(r) in Machine Learning_, 8(1-2):1-230, 2015.

* [39] Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie, Robert Tibshirani, David Botstein, and Russ B. Altman. Missing value estimation methods for DNA microarrays. _Bioinformatics_, 17(6):520-525, June 2001.
* [40] Alexandre B. Tsybakov. Nonparametric estimators. In Alexandre B. Tsybakov, editor, _Introduction to Nonparametric Estimation_, Springer Series in Statistics, pages 1-76. Springer, New York, NY, 2009.
* [41] John W. Tukey. The Ninther, a Technique for Low-Effort Robust (Resistant) Location in Large Samples. In H. A. David, editor, _Contributions to Survey Sampling and Applied Statistics_, pages 251-257. Academic Press, January 1978.
* [42] Stef van Buuren and Karin Groothuis-Oudshoorn. Mice: Multivariate Imputation by Chained Equations in R. _Journal of Statistical Software_, 45:1-67, December 2011.
* [43] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices, November 2011.
* [44] Roman Vershynin. _High-Dimensional Probability: An Introduction with Applications in Data Science_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, Cambridge, 2018.
* [45] Vincent Q. Vu and Jing Lei. Minimax sparse principal subspace estimation in high dimensions. _The Annals of Statistics_, 41(6):2905-2947, December 2013.
* [46] Jeffrey M. Wooldridge. _Introductory econometrics : a modern approach / Jeffrey M. Wooldridge,..._ Cengage learning, 2016.
* [47] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. GAIN: Missing Data Imputation using Generative Adversarial Nets, June 2018.
* [48] Shuhan Zheng and Nontawat Charoenphakdee. Diffusion models for missing value imputation in tabular data, March 2023.