# LuminAIRe: Illumination-Aware Conditional Image Repainting for Lighting-Realistic Generation

 Jiajun Tang\({}^{1,2}\)   Haofeng Zhong\({}^{1,2}\)   Shuchen Weng\({}^{1,2}\)   Boxin Shi\({}^{1,2}\)

\({}^{1}\)National Key Laboratory for Multimedia Information Processing

\({}^{2}\)National Engineering Research Center of Visual Technology

School of Computer Science, Peking University

{jiajun.tang, hfzhong, shuchenweng, shiboxin}@pku.edu.cn

Corresponding author.

###### Abstract

We present the iIL**Lumin**ation-**A**ware conditional **I**mage **R**epainting (LuminAIRe) task to address the unrealistic lighting effects in recent conditional image repainting (CIR) methods. The environment lighting and 3D geometry conditions are explicitly estimated from given background images and parsing masks using a parametric lighting representation and learning-based priors. These 3D conditions are then converted into illumination images through the proposed physically-based illumination rendering and illumination attention module. With the injection of illumination images, physically-correct lighting information is fed into the lighting-realistic generation process and repainted images with harmonized lighting effects in both foreground and background regions can be acquired, whose superiority over the results of state-of-the-art methods is confirmed through extensive experiments. For facilitating and validating the LuminAIRe task, a new dataset Car-LuminAIRe with lighting annotations and rich appearance variants is collected.

## 1 Introduction

Advanced image editing is in high demand across a multitude of applications, _e.g._, old photo colorization [78, 32, 68], damaged image restoration [48, 73, 72], and artistic style transfer [22, 35, 70]. Recently, conditional image repainting (CIR) [67, 66, 58] has emerged as an innovative research topic, proven effective in controllable image editing while "freeing" users from the necessity of expert proficiency and retaining the "freedom" to actualize their creative visions for image modification. By utilizing provided attributes or textual descriptions, fine-grained strokes, and Gaussian noise to separately represent colors, contours, and texture conditions, users could insert generative objects with desired appearances in specified image positions, as shown in the blue line of Fig. 1.

Although CIR methods have made great progress in synthesizing photo-realistic and visually-pleasing conditional images by avoiding gradient vanishing pitfall [67], adopting flexible condition representation [66], and designing condition fusion modules [58], there is still a crucial element missing from the CIR task: making the synthesized results harmonized with the illumination of the scene, _e.g._, spatially-varying dark and bright regions in accordance to the lighting condition in the background, physically-accurate highlight effects for highly-specular surfaces (shining objects), and perceptually-realistic shadow avoiding "floating objects" artifacts, as shown in the lower right example of Fig. 1.

Specifically, existing CIR methods handle image harmonization purely in 2D image space by estimating a pixel-wise color tone transformation of the repainted regions from the background regions. Current approaches use semantic parsing maps as "geometry" representations and do not exploit the lighting information contained in given background images, which prevents them from havingawareness of physically-based lighting in 3D space. To introduce physically-correct 3D lighting instead of hallucinated lighting effects into CIR results, there remain some major challenges: _(i)_ The lighting condition in 3D space should be extracted from the limited field of view (limited-FoV) 2D LDR images; _(ii)_ the lighting condition should be physically-correctly transformed back into 2D image space; _(iii)_ a dataset suitable for learning-based solutions to the proposed task is needed.

To achieve _lighting-realistic generation_ within the CIR pipeline in an illumination-aware manner, we hereby propose the task of iIL**Lumin**ation-**A**ware conditional **I**mage **R**epainting, denoted as **Lumin**A**IRe**. We first lift geometry conditions from 2D parsing maps to 3D1 normal maps using learning-based shape priors and estimate lighting conditions from limited-FoV LDR background images by designing a proper parametric representation. Then, we use physically-based reflection models to render _illumination candidate images_ to capture possible lighting effects in 2D image space. With the help of _illumination attention module_, surface regions with different reflective properties are learned to adopt correct lighting effects in the resulting appearance. A dataset containing rich geometry and lighting annotations with abundant object variants is collected to facilitate the learning-based solution of the LuminAIRe task. As far as we know, we are the first to emphasize illumination-awareness in the image editing task of conditional image repainting.

Footnote 1: Strictly speaking, the normal maps are in 2.5D. Here we use 3D to simply distinguish it from 2D.

Our contributions can be summarized as follows:

* introducing a new task of i**Lumin**ation-**A**ware conditional **I**mage **R**epainting (**LuminA**IRe**) by exploiting the lighting information from background images;
* designing a full LuminAIRe pipeline that represents, extracts, converts, and injects lighting information to acquire more realistically repainted results; and
* collecting a new dataset Car-LuminAIRe with rich material and lighting condition variants for facilitating and validating the LuminAIRe task.

## 2 Related Work

Our method aims at introducing physical lighting constraints into generative image synthesis pipelines. In this section, we briefly review relevant works first and then discuss the relationships to our task.

**Controllable image synthesis.** Researchers have presented numerous works to synthesize images under the guidance of diverse user-provided conditions, _e.g._, synthesizing specific object with category label [10; 43; 45; 75], transferring the texture from paintings to daily photos [22; 16; 35; 70], restoring

Figure 1: Illustration of proposed LuminAIRe task and result. Compared with the previous CIR task [58] (blue line) which takes all condition inputs2 at once conducting a conditional image-to-image generation purely in 2D image space, LuminAIRe (orange line) exploits 3D lighting and geometry information and repaints both foreground (fg.) and background (bg.) regions via a lighting-realistic generation process. The 3D information is transformed back to 2D image space in the form of an illumination image, with the desired reflective properties obtained from an illumination attention module. LuminAIRe handles _(i)_ surface shading, _(ii)_ highlight effects, and _(iii)_ realistic shadow in the repainted image (top right).

the colors of old photos [11; 12; 69; 68], and directly generating images from text descriptions [51; 50; 55; 56]. Recently, with the development of the condition injection mechanism [31; 47; 80; 34], researchers explore to control synthesized images with multiple cross-modality conditions, _e.g._, condition guided image inpainting [44; 55], controllable person synthesis [54; 65], and inversion-based style transfer [79]. However, few works focus on synthesizing images strictly following lighting conditions. Following DIH-GAN [6] that considers introducing illumination estimation in harmonization task that adjusts the highlight of the inserted given object, we further explore the lighting condition in synthesizing illumination-consistent objects under the guidance of multiple cross-modality conditions.

**Conditional image repainting and image harmonization.** Conditional image repainting (CIR) aims at synthesizing reasonable visual content on an existing image, where the generated visual content should both meet the requirement of the user-provided conditions (_e.g._, color, geometry, and texture) and in harmonization with the existing background image. The first CIR task is proposed in MISC [67] for person image synthesis, where the foreground person image is synthesized first and then composited with the background. Weng _et al_. [66] design the semantic-bridge attention mechanism which allows more freely expressed color conditions by the users in text. UniCoRN [58] breaks the two-stage dependency and proposes a unified architecture that achieves more visually pleasing results. Despite recent achievements made by previous works in condition consistency, existing CIR models suffer from the issue of illumination inconsistency: although techniques such as color tone transform are applied, the lighting from the given background and on the generated visual contents often differ a lot, making lighting effects in the image rather unrealistic, such as incorrect shading, highlights, and shadows. In this paper, we address this issue by exploiting lighting and shape constraints in 3D space, which allows a more physically-correct rendering processing for generating lighting effects. Image harmonization methods [23; 24; 25; 14; 42; 62; 59], with a similar goal of CIR to realistically composite image foreground and background regions, have focused on illumination harmonization recently [6; 8]. However, this thread of works has poor control of visual content in foreground regions and may fail to preserve the color tone in background regions as they were.

**Lighting representation and estimation.** Achieving illumination-aware synthesis/repainting requires appropriate lighting representation and estimation from images. Lalonde _et al_. [37] is the first to use shadows, shading, and sky appearance variations observed in the image to infer outdoor lighting. A physics-based Hosek-Wilkie (HW) sky model [29; 30] is proposed to recover HDR parameters for deep outdoor lighting estimation [28]. A more weather-robust Lalonde-Matthews (LM) model [38; 77] is then proposed to cover more comprehensive lighting conditions in the outdoor environment. More recently, a learning-based lighting representation [27] is used on a large sky panorama dataset [36] with an autoencoder network. The encoder-decoder framework is further proposed [39] to estimates lighting as a spherical HDR lighting map. HDSky [74] and SOLD-Net [60] disentangle several physically meaningful attributes into separate learned latent spaces by hierarchical autoencoders and make the estimation editable. Parametric models such as spherical harmonic (SH) coefficients [7; 21] and spherical Gaussian (SG) [19; 40] are also widely used, especially in indoor scenes. Gardner _et al_. [20], Neurlllum [57], and SOLID-Net [81] design sophisticated networks to hallucinate the missing parts in the panoramic view and predict lighting as environment maps. 3D volumetric lighting representations are also widely used in recent works, which facilitate the lighting-realistic scene editing for indoor [41] and outdoor [64] scenes, however heavily require computation resources. Considering the demand for lighting-realistic generation, we propose a parametric lighting representation for outdoor scenes that is both easy to predict and simple to use.

## 3 Problem Formulation

For self-containedness, we briefly review the CIR formulation before introducing ours.

### Preliminaries about CIR

The previous CIR tasks [66; 67; 58] aim at generating the repainted image \(y^{\tau}\) by repainting certain regions in an image \(x\in\mathbb{R}^{3\times H\times W}\) according to user-specified conditions in different modalities: \(x^{\text{g}}\), \(x^{\text{p}}\), \(x^{\text{c}}\), and \(x^{\text{b}}\) for the "geometry", "texture", "color", and background conditions respectively.

In their works, the "geometry" condition \(x^{\text{g}}\in\mathbb{L}^{N_{\text{g}}\times H\times W}\) is a binary semantic parsing mask, where \(N_{\text{g}}\) is the number of possible parts of the visual content to be repainted and \(\mathbb{L}=\{0,1\}\); the "texture"condition \(x^{\rm P}\sim\mathcal{N}(0,1)\) is a Gaussian noise; the "color" condition can be represented as attributes \(x^{\rm c}\in\mathbb{L}^{N_{\rm v}\times N_{\rm v}}\) or text descriptions \(x^{\rm c}=\{x^{\rm t}_{t}\}_{t=1}^{N_{\rm v}}\), where \(N_{\rm c}\), \(N_{\rm v}\), and \(N_{\rm L}\) represent the numbers of attributes and available choices, and the length of the user-inputted sentences, respectively; the background condition \(x^{\rm b}\in\mathbb{R}^{3\times H\times W}\) is the image of background region with respect to the repainted region as foreground region, _i.e._, \(x^{\rm b}=(1-m)\odot x\), where the binary mask \(m\) indicating foreground region can be directly acquired from the parsing mask \(x^{\rm g}\), as shown in lower left of Fig. 1.

The repainted image \(y^{\rm r}\) can be further decomposed as a blending of repainted foreground image \(y^{\rm f}\) and repainted background image \(y^{\rm b}\):

\[y^{\rm r}=m\odot y^{\rm f}+(1-m)\odot y^{\rm b}. \tag{1}\]

Previous works assume unchanged background region, _i.e._, \(y^{\rm b}=x^{\rm b}\), leaving the key question of CIR tasks as generating realistic foreground region \(y^{\rm f}\) constrained by given conditions:

\[y^{\rm f}={\rm F}^{\rm G}(x^{\rm g},x^{\rm p},x^{\rm c},x^{\rm b}), \tag{2}\]

where previous works ignore clues in 3D space and implement the generation pipeline \({\rm F}^{\rm G}\) as a _conditional image-to-image generation_ purely in 2D image space. To make the repainted image harmonized as a whole, previous works [58; 67] design additional harmonization modules to adjust the color tone of intermediate repainting result based on clues in \(x^{\rm b}\).

### Formulation of LuminAIRe

However, the image-based harmonization modules have limited representation ability for complex lighting effects (_e.g._, varying shading and shiny surfaces) due to a lack of 3D representation. Besides, directly using \(x^{\rm b}\) as \(y^{\rm b}\) in Eq. (1) may neglect possible light transport effects (_e.g._, shadows) introduced by the repainted region as its corresponding behaviors in the 3D real world might be.

As illustrated by the rendering equation [33], a physically-correct and -realistic appearance of an object is derived from its _geometry_, _reflective property_, and omnidirectional _environment lighting_ in 3D space. Therefore, to make the repainted image \(y^{\rm r}\) more _lighting-realistic_, the repainted foreground \(y^{\rm f}\) should also be conditioned by the lighting condition \(L\) and geometry condition \(G\) in 3D space:

\[y^{\rm f}={\rm F}^{\rm F}(x^{\rm g},x^{\rm p},x^{\rm c},x^{\rm b},L,G). \tag{3}\]

Given \(L\) and \(G\) in 3D space, a proper 2D representation \(x^{\rm i}\) containing both the information from \(L\) and \(G\) should be derived for compatibility with current image generation architectures:

\[x^{\rm i}={\rm R}^{\rm i}(L,G), \tag{4}\]

and then the _lighting-realistic generation_ for foreground \(y^{\rm f}\) can be rewritten as:

\[y^{\rm f}={\rm F}^{\rm F}(x^{\rm g},x^{\rm p},x^{\rm c},x^{\rm b},x^{\rm i}). \tag{5}\]

Similarly, the repainted background \(y^{\rm b}\) should also be conditioned on \(x^{\rm i}\) to recover lighting effects:

\[y^{\rm b}={\rm F}^{\rm B}(x^{\rm b},x^{\rm i}). \tag{6}\]

The limited-FoV background image \(x^{\rm b}\) itself is a partial observation of environment lighting and thus can provide clues about \(L\). Therefore, the lighting condition \(L\) can be inferred in the form of:

\[L={\rm F}^{\rm L}(x^{\rm b}). \tag{7}\]

Similarly, by finding the shape priors of certain types of objects, the 3D geometry condition \(G\) can be lifted from its "2D flattened version", _i.e._, parsing mask \(x^{\rm g}\):

\[G={\rm F}^{\rm Geo}(x^{\rm g}). \tag{8}\]

Moreover, in our LuminAIRe formulation, we extend the attributes \(x^{\rm c}\) beyond colors, which allows the users to describe the _reflective property_ and have control over the lighting effects of repainting results. A sample of attributes is shown as the **bold text** in the lower left of Fig. 1.

As aforementioned, both the repainted foreground \(y^{\rm f}\) and background \(y^{\rm b}\) are given by the _lighting-realistic generation_ in our LuminAIRe formulation, which leads to more realistic and harmonized results than traditional CIR pipelines [58], as shown in Fig. 1.

## 4 Data Preparation

To tackle the data shortage issue, we create the first dataset suitable for the LuminAIRe task, named Car-LuminAIRE, with its data preparation process and data sample shown in Fig. 2.

**3D car models with hierarchical semantic labeling.** Collecting large-scale real data for learning-based LuminAIRe methods is infeasible since the geometry and lighting capture in 3D space requires specialized equipment and extensive human labor. Therefore, here we resort to computer graphics techniques to create photo-realistic synthetic data. The cars are chosen as the foreground objects for the obviousness of lighting effects and the availability of high-quality synthetic models. We collect 198 detailed 3D car models in 17 different categories from online model stores [2; 4] and then label the parts of the models in 3D space, which allows us to get the accurate parsing mask in 2D image space from any viewpoint. Following the common structure of vehicles, we divide the car models into 35 semantic part labels. The part labels are organized in a hierarchical way (_e.g._, the _door window_ is a sub-part of the _door_) to accommodate car models in different granularity. Besides 3D labeling, we manually adjust the scales of each model to fit the real-world dimensions.

**Background images with lighting annotations.** Then we prepare background images with known lighting annotations. Here we use the SUN360-HDR dataset [27; 76], which contains HDR panoramic environment maps (envmaps) corresponding to the LDR panoramas of outdoor scenes in the SUN360 dataset [71]. Limited field-of-view (limited-FoV) background images are cropped from the LDR panoramas with virtual cameras of randomized FoVs and camera poses. For each cropped background image, the corresponding HDR envmap in the SUN360-HDR dataset [27; 76] is warped to align with the viewing direction of the virtual camera. Background images unsuitable for realistic object insertion are manually filtered out, leaving 1,321 images of diverse scenes and lighting conditions.

**Enhanced data rendering with realistic placement.** For each background image, we randomly select insertion points within the central region of the "placeable flat ground" marked by an off-the-shelf segmentation toolbox [15]. Then, for each 2D insertion point in the image, we calculate the relative transformation from the camera coordination \(O_{\text{c}}\) to the local coordination of the object \(O_{\text{o}}\) from the depth \(d\) and the normal \(Z_{\text{o}}\) estimated by depth [52; 53] and normal [5] estimation methods. With the aligned envmaps and the ray-tracing based Blender [3] Cycles rendering engine, physically-correct lighting effects can be rendered into the composited images. In the rendering process, besides the original materials of the models, several physics-based rendering (PBR) car paint materials are randomly applied for more appearance variants, especially in lighting effects; besides, the inserted models are randomly rotated around \(Z_{\text{o}}\) axis for more geometry variants. The rendered images are filtered to ensure reasonable pixel portions of both foreground and background regions. At last, 52,581 composited images at the resolution of \(256\times 256\) are collected, accompanied by parsing mask and normal map annotations, as shown in the data sample of Figure 2.

## 5 Method

To realize the LuminAIRe formulation, we first estimate 3D lighting and geometry from background images and parsing masks (Sec. 5.1). Then the lighting information is injected into the lighting-realistic generation process as illumination images (Sec. 5.2). By further introducing hierarchical labeling enhancement (Sec. 5.3), our method can generate reasonable results even with coarse-level parsing masks. Our pipeline is shown in Fig. 3, with detailed network architectures and loss functions for network modules in supplementary materials.

Figure 2: Data preparation process of the Car-LuminAIRE dataset.

### Estimating 3D Information with Learning-based Priors

Our Car-LuminAIRe dataset consists of outdoor scene images, where the lighting can be approximately decomposed into the high-frequency sunlight and the low-frequency ambient light [60]. Accordingly, we model the lighting condition \(L\) as the addition of a directional light and a 2-nd order spherical harmonics (SH) lighting, which can be specifically described as lighting parameters:

\[L=\{z_{\text{vis}},z_{\text{int}},z_{\text{ang}},c_{\text{sun}},l_{\text{sun}}, \sigma_{\text{SH}}\}, \tag{9}\]

where \(z_{\text{vis}}\in\{0,1\}\) is the sun visibility, \(z_{\text{int}}\) is the intensity of sunlight, \(z_{\text{ang}}\) describes the "size" of the sun (in solid angle formally), \(c_{\text{sun}}\in\mathbb{R}^{3}\) is the normalized sun color in RGB channels, \(l_{\text{sun}}\in\mathbb{R}^{2}\) indicates the sun position, and \(\sigma_{\text{SH}}\in\mathbb{R}^{3\times 9}\) is the 2-nd order SH coefficients for RGB channels.

As shown in Fig. 4, the parametric representation3 in Eq. (9) can well fit real-world lighting in sunny, cloudy, and low light conditions. On the other hand, the proposed parametric lighting representation is convenient for network prediction. Here we design a _NetL_ to serve as \(\mathrm{F}^{\mathrm{L}}\) in Eq. (7), where \(l_{\text{sun}}\) is estimated by a classification task and other parameters are estimated by regression tasks. To apply our method to other types of background scenes, specifically tailored lighting representations can be directly adopted, without modification to our underlying formulation of LuminAIRe.

Footnote 3: Lighting parameters are converted back to tone-mapped HDR environment maps for visualization.

For 3D geometry, we use the normal map \(G\in\mathbb{R}^{3\times H\times W}\) as the representation where each pixel indicates the surface normal direction \(\mathbf{n}\) at that surface point in 3D space. For certain types of objects, there exist strong shape priors (such as sedans and hatchbacks), which can be learned in a supervised way. Similarly, a _NetS_ of encoder-decoder structure is further proposed to serve as \(\mathrm{F}^{\mathrm{Gco}}\) in Eq. (8).

### Injecting Lighting Information using Illumination Images

To bridge the 3D lighting and geometry with 2D images, the rendering equation [33] is a handy tool to serve as \(\mathrm{R}^{\mathrm{i}}\) in Eq. (4), which physically models the image formation process as the light reflection:

\[L_{\mathrm{o}}(\mathbf{\omega}_{\mathrm{r}})=\int_{\Omega_{\mathbf{n}}}L_{\mathrm{i}}( \mathbf{\omega}_{\mathrm{i}})f_{\mathrm{r}}(\mathbf{\omega}_{\mathrm{i}},\mathbf{\omega}_{ \mathrm{r}})(\mathbf{n}\cdot\mathbf{\omega}_{\mathrm{i}})\mathrm{d}\mathbf{\omega}_{ \mathrm{i}}, \tag{10}\]

where \(L_{\mathrm{i}}(\mathbf{\omega}_{\mathrm{i}})\) is the environment lighting from direction \(\mathbf{\omega}_{\mathrm{i}}\), \(L_{\mathrm{o}}(\mathbf{\omega}_{\mathrm{r}})\) is the reflected lighting toward direction \(\mathbf{\omega}_{\mathrm{r}}\), \(\Omega_{\mathbf{n}}\) is the visible hemisphere determined by surface normal \(\mathbf{n}\), and \(f_{\mathrm{r}}(\mathbf{\omega}_{\mathrm{i}},\mathbf{\omega}_{\mathrm{r}})\) describes the reflective properties of all possible combination of incoming and outgoing directions.

Figure 4: Our lighting representation can capture most of the lighting effects in all weather conditions.

Figure 3: Overview of LuminAIRe pipeline.

For a certain image pixel with the known camera viewing direction \(\mathbf{v}\), ideally, the pixel intensity can be calculated as \(L_{\mathrm{o}}(-\mathbf{v})\), and accurate lighting effects can be calculated as _illumination images_.

However, with \(L\) and \(G\) estimated from input conditions, \(f_{\mathrm{r}}(\mathbf{\omega}_{\mathrm{i}},\mathbf{\omega}_{\mathrm{r}})\) still remains unknown. Therefore, in a similar spirit to Gao _et al_. [18] and Pandey _et al_. [46], instead of directly calculating the actual illumination image, we use a set of uncoolored "standard materials" as \(f_{\mathrm{r}}\) in Eq. (10) and render corresponding _illumination candidate images_\(\{I_{\mathrm{c}}\}\). For the physics-based rendering of \(\{I_{\mathrm{c}}\}\), we use the Lambertian reflectance model \(f_{\mathrm{diff}}(\mathbf{\omega}_{\mathrm{i}},\mathbf{\omega}_{\mathrm{r}})=\nicefrac{{ 1}}{{\pi}}\) and normalized Blinn-Phong model [9]\(f_{\mathrm{spec}}(\mathbf{\omega}_{\mathrm{i}},\mathbf{\omega}_{\mathrm{r}})=(\nicefrac{ {\left(\mathbf{v}+\mathbf{v}\right)(\mathbf{n}\cdot\mathbf{h})^{\mathrm{e}}}}{{\mathrm{s}} \pi}\) with \(M\) different values of roughness \(\rho\), where \(\mathbf{h}=\nicefrac{{\mathbf{\omega}_{\mathrm{i}}+\mathbf{\omega}_{\mathrm{r}}}}{{\left| \left|\mathbf{\omega}_{\mathrm{i}}+\mathbf{\omega}_{\mathrm{r}}\right|\right|}}\) is the half vector. At last, we have \(\{I_{\mathrm{c}}\}=\{I_{\mathrm{diff}}\}\cup\{I_{\mathrm{spec}}^{\rho}\}_{i= 1}^{M}\).

As shown in Fig. 5, most lighting effects in different appearance variants can be covered by the linear combinations of the pre-computed \(\{I_{\mathrm{c}}\}\). However, it's worth noting that the correspondence of the appearance image and \(\{I_{\mathrm{c}}\}\) may vary pixel-wisely (_e.g._, the tires, hood, and windshield have different reflective properties thus different lighting effects). Accordingly, we design an _illumination attention_ module \(\mathrm{A}^{\mathrm{I}}\) to estimate the combination coefficient maps \(C_{\mathrm{I}}=\mathrm{A}^{\mathrm{I}}(E)\) for each image pixel, where \(E\) is the feature embedding map containing information of both part labels and part-associated attributes in a pixel-aligned way. After the illumination image \(I\) derived as \(I=\sum_{i=1}^{M+1}C_{\mathrm{I}}^{i}\odot I_{\mathrm{c}}^{i}\), which covers lighting effects of parts with different materials, we use \(I\) as \(x^{i}\) in Eq. (5) and conduct lighting-realistic generations of foreground and background regions using our proposed _NetF_ and _NetB_ respectively. For _NetF_, we adopt the network backbone of \(\mathrm{F}^{\mathrm{G}}\) in UniCoRN [58], and the illumination image \(I\) is injected in a similar way as other conditions in 2D image space at different resolutions. The _NetB_ is also an encoder-decoder architecture, serving as \(\mathrm{F}^{\mathrm{B}}\) in Eq. (6). We adopt the same loss functions for _NetF_ as used in UniCoRN [58].

### Generating Realistic Results from Coarse Parsing Masks

As mentioned in Sec. 4, the parsing masks in our Car-LuminAIRE dataset can be very coarse, which also reflects the possible application scenarios when the user only specifies interested parts. Previous CIR formulations may fail to generate realistic results in regions without fine-grained labels since their generation follows a strictly pixel-wise semantic mapping between labels and images. We hereby introduce a _hierarchical labeling enhancement_: randomly coarsening the input parsing mask at training time (_e.g._, _door glass_ label becomes _door_ label) and encouraging the fine-grained parts (_door glass_) to be generated. Besides, the part-associated attributes of lower-level parts (_door glass_) should be also associated with their upper-level parts (_door_) to avoid loss of condition in attributes \(x^{c}\), which can be done by modifying the association matrix [58]\(A\in\mathbb{L}^{N_{\mathrm{c}}\times N_{\mathrm{f}}}\) accordingly.

## 6 Experiments

In this section, we conduct comparisons with state-of-the-art methods and validate our design with an ablation study and a robustness test. Please see supplementary materials for implementation details.

### Comparison with State-of-the-art Methods

**Baseline methods.** We conduct quantitative and qualitative comparisons with three state-of-the-art CIR methods (**UniCoRN**[58], **Weng _et al_. [66], and **MISC**[67]) and a most-relevant conditional image generation method (**Pavllo _et al_. [49]). Among them, modifications are made for **Pavllo _et al_. [49] and **Weng _et al_. [66] to accept conditions represented as attributes.

**Quantitative metrics.** Following previous work [58], we adopt Frechet inception distance (FID) [26] for assessment of perception quality, R-precision [66] for assessment of alignment between generated

Figure 5: Our illumination candidate images can cover realistic lighting effects in appearance variants.

images \(y^{\tau}\) and given attributes \(x^{c}\), and M-score [61] for assessment of authenticity. We use the latest manipulation detection model [17, 13] for calculating the M-score [61]. We also report the structural similarity index (SSIM) [63] for comparing the major image structure with the reference image.

The scores in Tab. 1 and the second and the third columns of Fig. 6 show that results of **MISC**[67] and **Weng _et al._**[66] are far from lighting-realistic with "crayon-drawing-like" appearances, since the color tone transform is not applied [66], or conducted in a two-phase manner [67]. As shown in the fourth column of Fig. 6, **Pavllo _et al._**[49] tend to generate foreground regions in flat shadings with fewer texture patterns, which makes its results generally look reasonable when only focusing on foreground regions or in low light or cloudy scenes (as indicated by the FID and user study results), but computer vision models can easily find the disharmony due the sharp boundaries between foreground and background regions [58], as also indicated by the worst M-score. **UniCoRN**[58] fails to generate correct lighting effects from its unified color tone transform (the first row), therefore tends to hallucinate highlights at the top of cars regardless of lighting in background regions (the second row). The hallucinated lighting effects along with the undesired texture pattern on car bodies drastically damage the perceptual preferences, as confirmed by the FID score and user study results in Tab. 1. **LuminAIRe** generates realistic lighting effects close to the reference images in both sunny (the first and the third rows) and cloudy (the second row) scenes of specified materials and even when a coarse-level parsing mask is given (the third row), with a large margin in all quantitative metrics compared with baseline methods. **LuminAIRe** also learns to avoid the undesired texture pattern with the hints of the smoothly varied shading in the illumination images (Fig. 7).

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{Quantitative Evaluation} & \multicolumn{2}{c}{User Study} \\ \cline{2-7}  & FID \(\downarrow\) & R-precn \(\uparrow\) & M-score \(\downarrow\) & SSIM \(\uparrow\) & Real. \(\uparrow\) & Har. \(\uparrow\) \\ \hline MSC [67] & 53.84 & 34.94\% & 31.23 & 0.6660 & 0.25\% & 0.28\% \\ Weng _et al._[66] & 38.12 & 46.66\% & 30.84 & 0.6697 & 0.85\% & 0.85\% \\ Pavllo _et al._[49] & 9.29 & 56.98\% & 36.77 & 0.7050 & 43.00\% & 36.72\% \\ UnicORN [58] & 11.55 & 62.13\% & 29.72 & 0.6940 & 7.78\% & 9.90\% \\ LuninaAIRe (Ours) & **4.62** & **74.13\%** & **13.08** & **0.7211** & **48.12\%** & **52.25\%** \\ \hline Ours-H & 5.83 & 63.27\% & 13.97 & 0.7163 & — & — \\ Ours-HA & 6.31 & 63.94\% & 13.95 & **0.7214** & — & — \\ Ours-HAI & 8.00 & 62.13\% & 15.83 & 0.7054 & — & — \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with the state-of-the-art methods and variants of our proposed method. Quantitative evaluation scores and user study results are shown. \(\uparrow\) (\(\downarrow\)) means higher (lower) is better. “Real.” and “Har.” are abbreviations of “Realistic” and “Harmonized”.

Figure 6: Qualitative comparison with the state-of-the-art methods, with given conditions (conds.).

### Evaluations

**Ablation study.** We conduct an ablation study with three variants of **Ours**: _(i)_**Ours-H**, _(ii)_**Ours-HA**, and _(iii)_**Ours-HAI**, where "-H", "-A", and "-I" mean disabling the hierarchical labeling enhancement, the illumination attention, and the illumination injection for the foreground, respectively.

The hierarchical labeling enhancement is confirmed helpful in generating realistic results with coarse-level parsing masks, as shown in Fig. 7 and the third row of Fig. 6, where **Ours** generates more consistent and better repaintings at regions with no specified part labels (marked in blue purple), which is also demonstrated by the FID and R-prcn score in Tab. 1. The second row of Fig. 7 shows an example where the lack of illumination attention module wrongly renders a diffuse appearance, with further evidence from the drop of FID from **Ours-H** to **Ours-HA** in Tab. 1. It's quite obvious from Tab. 1 and Fig. 7 that the illumination injection helps foreground generation by comparaing **Ours-HA** and **Ours-HAI**. From **UniCoRN** to **Ours-HAI**, the improvements in FID score and M-score validate the contribution of the lighting-realistically generated background.

Besides, **Ours-HA** gets an unexpectedly good SSIM score. It's possibly because a slight misalignment of lighting effects (especially highlights) due to errors in lighting or geometry estimation would lead to a considerable drop in the SSIM score (which honestly measures the pixel-wise difference) but with very little harm to the lighting-realistic perception (as indicated by the FID and M-score).

**User study.** We also conduct a user study with 20 volunteers on the Amazon Mechanical Turk [1] platform, where 200 sets of results randomly drawn from the test set are shown and volunteers are asked to choose one in each set with _(i)_ the most _realistic_ foreground and _(ii)_ the most _harmonized_ lighting. The results of the user study in Tab. 1 are basically aligned with the trending of FID and SSIM scores in quantitative evaluation, showing that repainting results of our **LuminAIRe** are most favored subjectively, with a greater lead in realistic and harmonized lighting perception.

**Robustness Test.** Fig. 8 shows the robustness of our method to varying materials and geometry conditions, where different materials and geometry conditions are correctly handled with realistic lighting effects accordingly and consistently generated. To test the robustness of our method to varying parsing masks (_e.g._, casually-drawn parsing masks), we compare in Fig. 9 the repainting

Figure 8: Our method can generate realistic lighting effects with given materials (left), which are consistent across different geometry conditions (right). Green and blue boxes mark individual cases.

Figure 7: Ablation study for three variants of our proposed method, with given conditions (conds.).

results of from the input parsing masks before and after the disturbing, where the boarders are randomly extended and the inner structures are coarsened. To test the robustness of our method to varying lighting conditions, we conduct an experiment where the estimated lighting conditions are rotated clockwise while all other conditions are left unchanged. The results in Fig. 10 show that our method correctly handles most of the lighting rotations in the sense of the lighting effects on the foreground objects and the shadow effects in the background regions. The repainting results in the second column with no lighting conditions given ("No light") further validate the effectiveness of our illumination injection module. To test the robustness of our method to varying background conditions, we also show the results of in-the-wild examples in the supplemental material.

## 7 Conclusion

In this paper, we introduce the task of LuminAIRe for the realistic generation of lighting effects. The synthetic Car-LuminAIRe dataset is collected for the newly proposed task. Extensive experiments and the user study confirm that our method achieves perceptually more lighting-realistic and harmonized repainting results compared with the state-of-the-art methods. The effectiveness and consistency of our illumination-aware design are shown in the robustness test.

**Limitations and future works.** In this paper, only the results of cars as foreground objects are shown, resulting from the inadequate feasibility of data collection. Besides, our model can not handle complex thin structures and some translucent glass materials very well, which are not well covered by our synthetic data for now. As a single-image-based method for generic outdoor scenes, our method currently ignores the non-local inter-reflections with other objects and focuses on the shadows cast directly on the ground. Therefore, datasets of richer object categories and finer details will be helpful to boost the training of learning-based methods. Combining the lighting constraints with the newly emerged latent diffusion models [55] would also be an interesting direction for our future work.

**Acknowledgement.** This work is supported by the National Natural Science Foundation of China under Grant No. 62136001, 62088102.

Figure 10: Qualitative results of repaintings and illumination images as the estimated lighting rotates.

Figure 9: Qualitative results of normal maps, illumination images and repaintings using original (first row) and disturbed (second row) parsing masks as input conditions. Backgrounds are omitted here.

## References

* [1] Amazon Mechanical Turk. [https://www.mturk.com](https://www.mturk.com).
* Hum3D store. [https://hum3d.com](https://hum3d.com).
* [3] Blender. [https://www.blender.org](https://www.blender.org).
* [4] Squir Home. [https://squir.com](https://squir.com).
* [5] G. Bae, I. Budvytis, and R. Cipolla. Estimating and exploiting the aleatoric uncertainty in surface normal estimation. In _Proc. of International Conference on Computer Vision_, 2021.
* [6] Z. Bao, C. Long, G. Fu, D. Liu, Y. Li, J. Wu, and C. Xiao. Deep image-based illumination harmonization. In _Proc. of Computer Vision and Pattern Recognition_, 2022.
* [7] J. T. Barron and J. Malik. Intrinsic scene properties from a single RGB-D image. In _Proc. of Computer Vision and Pattern Recognition_, 2013.
* [8] A. Bhattad and D. Forsyth. Cut-and-paste object insertion by enabling deep image prior for reshading. In _Proc. of International Conference on 3D Vision_, 2022.
* [9] J. F. Blinn. Models of light reflection for computer synthesized pictures. _SIGGRAPH Comput. Graph._, 11(2):192-198, 1977.
* [10] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN training for high fidelity natural image synthesis. _arXiv preprint arXiv:1809.11096_, 2018.
* [11] Z. Chang, S. Weng, Y. Li, S. Li, and B. Shi. L-CoDer: Language-based colorization with color-object decoupling transformer. In _Proc. of European Conference on Computer Vision_, 2022.
* [12] Z. Chang, S. Weng, P. Zhang, Y. Li, S. Li, and B. Shi. L-CoIns: Language-based colorization with instance awareness. In _Proc. of Computer Vision and Pattern Recognition_, 2023.
* [13] X. Chen, C. Dong, J. Ji, j. Cao, and X. Li. Image manipulation detection by multi-view multi-scale supervision. In _Proc. of International Conference on Computer Vision_, 2021.
* [14] W. Cong, J. Zhang, L. Niu, L. Liu, Z. Ling, W. Li, and L. Zhang. DoveNet: Deep image harmonization via domain verification. In _Proc. of Computer Vision and Pattern Recognition_, 2020.
* [15] M. Contributors. MMSegmentation: Opennmlab semantic segmentation toolbox and benchmark. [https://github.com/open-mnlab/mmsegmentation](https://github.com/open-mnlab/mmsegmentation), 2020.
* [16] Y. Deng, F. Tang, W. Dong, C. Ma, X. Pan, L. Wang, and C. Xu. StyTr2: Image style transfer with transformers. In _Proc. of Computer Vision and Pattern Recognition_, 2022.
* [17] C. Dong, X. Chen, R. Hu, J. Cao, and X. Li. MVSS-Net: Multi-view multi-scale supervised networks for image manipulation detection. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(3):3539-3553, 2023.
* [18] D. Gao, G. Chen, Y. Dong, P. Peers, K. Xu, and X. Tong. Deferred neural lighting: Free-viewpoint relighting from unstructured photographs. _ACM Transactions on Graphics (Proc. of ACM SIGGRAPH Asia)_, 39(6):1-15, 2020.
* [19] M.-A. Gardner, Y. Hold-Geoffroy, K. Sunkavalli, C. Gagne, and J.-F. Lalonde. Deep parametric indoor lighting estimation. In _Proc. of International Conference on Computer Vision_, 2019.
* [20] M.-A. Gardner, K. Sunkavalli, E. Yumer, X. Shen, E. Gambaretto, C. Gagne, and J.-F. Lalonde. Learning to predict indoor illumination from a single image. In _Proc. of ACM SIGGRAPH Asia_, 2017.
* [21] M. Garon, K. Sunkavalli, S. Hadap, N. Carr, and J.-F. Lalonde. Fast spatially-varying indoor lighting estimation. In _Proc. of Computer Vision and Pattern Recognition_, 2019.
* [22] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In _Proc. of Computer Vision and Pattern Recognition_, 2016.
* [23] J. J. A. Guerreiro, M. Nakazawa, and B. Stenger. PCT-Net: Full resolution image harmonization using pixel-wise color transformations. In _Proc. of Computer Vision and Pattern Recognition_, 2023.
* [24] Z. Guo, Z. Gu, B. Zheng, J. Dong, and H. Zheng. Transformer for image harmonization and beyond. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(11):12960-12977, 2023.

* [25] Z. Guo, D. Guo, H. Zheng, Z. Gu, B. Zheng, and J. Dong. Image harmonization with transformer. In _Proc. of International Conference on Computer Vision_, 2021.
* [26] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, G. Klambauer, and S. Hochreiter. GANs trained by a two time-scale update rule converge to a nash equilibrium. In _Proc. of Advances in Neural Information Processing Systems_, 2017.
* [27] Y. Hold-Geoffroy, A. Athawale, and J.-F. Lalonde. Deep sky modeling for single image outdoor lighting estimation. In _Proc. of Computer Vision and Pattern Recognition_, 2019.
* [28] Y. Hold-Geoffroy, K. Sunkavalli, S. Hadap, E. Gambaretto, and J.-F. Lalonde. Deep outdoor illumination estimation. In _Proc. of Computer Vision and Pattern Recognition_, 2017.
* [29] L. Hosek and A. Wilkie. An analytic model for full spectral sky-dome radiance. _ACM Trans. on Graphics (TOG)_, 31(4), 2012.
* [30] L. Hosek and A. Wilkie. Adding a solar-radiance function to the Hosek-Wilkie skylight model. _IEEE Computer Graphics and Applications_, 33(3):44-52, 2013.
* [31] X. Huang and S. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In _Proc. of International Conference on Computer Vision_, 2017.
* [32] Z. Huang, N. Zhao, and J. Liao. UniColor: A unified framework for multi-modal colorization with transformer. In _Proc. of ACM SIGGRAPH Asia_, 2022.
* [33] J. T. Kajiya. The rendering equation. In _Proc. of ACM SIGGRAPH_, 1986.
* [34] W. Kim, B. Son, and I. Kim. ViLT: Vision-and-language transformer without convolution or region supervision. In _Proc. of International Conference on Machine Learning_, 2021.
* [35] G. Kwon and J. C. Ye. CLIPstyler: Image style transfer with a single text condition. In _Proc. of Computer Vision and Pattern Recognition_, 2022.
* [36] J.-F. Lalonde, L.-P. Asselin, J. Becirovski, Y. Hold-Geoffroy, M. Garon, M.-A. Gardner, and J. Zhang. The laval HDR sky database., 2016. [http://sky.hdrdb.com](http://sky.hdrdb.com).
* [37] J.-F. Lalonde, A. A. Efros, and S. G. Narasimhan. Estimating the natural illumination conditions from a single outdoor image. _International Journal of Computer Vision_, 2012.
* [38] J.-F. Lalonde and I. Matthews. Lighting estimation in outdoor image collections. In _Proc. of International Conference on 3D Vision_, 2014.
* [39] C. LeGendre, W.-C. Ma, G. Fyffe, J. Flynn, L. Charbonnel, J. Busch, and P. Debevec. DeepLight: Learning illumination for unconstrained mobile mixed reality. In _Proc. of Computer Vision and Pattern Recognition_, 2019.
* [40] Z. Li, M. Shafiei, R. Ramamoorthi, K. Sunkavalli, and M. Chandraker. Inverse rendering for complex indoor scenes: Shape, spatially-varying lighting and SVBRDF from a single image. In _Proc. of Computer Vision and Pattern Recognition_, 2020.
* [41] Z. Li, L. Yu, M. Okunev, M. Chandraker, and Z. Dong. Spatiotemporally consistent hdr indoor lighting estimation. _ACM Trans. on Graphics (TOG)_, 42(3):1-15, 2023.
* [42] J. Ling, H. Xue, L. Song, R. Xie, and X. Gu. Region-aware adaptive instance normalization for image harmonization. In _Proc. of Computer Vision and Pattern Recognition_, 2021.
* [43] T. Miyato and M. Koyama. cGANs with projection discriminator. _arXiv preprint arXiv:1802.05637_, 2018.
* [44] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever, and M. Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. In _Proc. of International Conference on Machine Learning_, 2022.
* [45] A. Odena, C. Olah, and J. Shlens. Conditional image synthesis with auxiliary classifier GANs. In _Proc. of International Conference on Machine Learning_, 2017.
* [46] R. Pandey, S. O. Escolano, C. Legendre, C. Haene, S. Bouaziz, C. Rhemann, P. Debevec, and S. Fanello. Total relighting: learning to relight portraits for background replacement. _ACM Transactions on Graphics (Proc. of ACM SIGGRAPH)_, 40(4):1-21, 2021.

* [47] T. Park, M.-Y. Liu, T.-C. Wang, and J.-Y. Zhu. Semantic image synthesis with spatially-adaptive normalization. In _Proc. of Computer Vision and Pattern Recognition_, 2019.
* [48] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature learning by inpainting. In _Proc. of Computer Vision and Pattern Recognition_, 2016.
* [49] D. Pavllo, A. Lucchi, and T. Hofmann. Controlling style and semantics in weakly-supervised image generation. In _Proc. of European Conference on Computer Vision_, 2020.
* [50] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with CLIP latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [51] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image generation. In _Proc. of International Conference on Machine Learning_, 2021.
* [52] R. Ranftl, A. Bochkovskiy, and V. Koltun. Vision transformers for dense prediction. In _Proc. of International Conference on Computer Vision_, 2021.
* [53] R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V. Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(3):1623-1637, 2020.
* [54] Y. Ren, X. Fan, G. Li, S. Liu, and T. H. Li. Neural texture extraction and distribution for controllable person image synthesis. In _Proc. of Computer Vision and Pattern Recognition_, 2022.
* [55] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In _Proc. of Computer Vision and Pattern Recognition_, 2022.
* [56] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Proc. of Advances in Neural Information Processing Systems_, 2022.
* [57] S. Song and T. Funkhouser. Neural Illumination: Lighting prediction for indoor environments. In _Proc. of Computer Vision and Pattern Recognition_, 2019.
* [58] J. Sun, S. Weng, Z. Chang, S. Li, and B. Shi. UniCoRN: A unified conditional image repainting network. In _Proc. of Computer Vision and Pattern Recognition_, 2022.
* [59] K. Sunkavalli, M. K. Johnson, W. Matusik, and H. Pfister. Multi-scale image harmonization. _ACM Transactions on Graphics (Proc. of ACM SIGGRAPH)_, 29(4):1-10, 2010.
* [60] J. Tang, Y. Zhu, H. Wang, J.-H. Chan, S. Li, and B. Shi. Estimating spatially-varying lighting in urban scenes with disentangled representation. In _Proc. of European Conference on Computer Vision_, 2022.
* [61] S. Tripathi, S. Chandra, A. Agrawal, A. Tyagi, J. M. Rehg, and V. Chari. Learning to generate synthetic data via compositing. In _Proc. of Computer Vision and Pattern Recognition_, 2019.
* [62] Y.-H. Tsai, X. Shen, Z. Lin, K. Sunkavalli, X. Lu, and M.-H. Yang. Deep image harmonization. In _Proc. of Computer Vision and Pattern Recognition_, 2017.
* [63] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE Transactions on Image Processing_, 13(4):600-612, 2004.
* [64] Z. Wang, W. Chen, D. Acuna, J. Kautz, and S. Fidler. Neural light field estimation for street scenes with differentiable virtual object insertion. In _Proc. of European Conference on Computer Vision_, 2022.
* [65] Z. Wang, X. Qi, K. Yuan, and M. Sun. Self-supervised correlation mining network for person image generation. In _Proc. of Computer Vision and Pattern Recognition_, 2022.
* [66] S. Weng, W. Li, D. Li, H. Jin, and B. Shi. Conditional image repainting via semantic bridge and piecewise value function. In _Proc. of European Conference on Computer Vision_, 2020.
* [67] S. Weng, W. Li, D. Li, H. Jin, and B. Shi. MISC: Multi-condition injection and spatially-adaptive compositing for conditional person image synthesis. In _Proc. of Computer Vision and Pattern Recognition_, 2020.
* [68] S. Weng, J. Sun, Y. Li, S. Li, and B. Shi. CT\({}^{2}\): Colorization transformer via color tokens. In _Proc. of European Conference on Computer Vision_, 2022.

* [69] S. Weng, H. Wu, Z. Chang, J. Tang, S. Li, and B. Shi. L-code: Language-based colorization using color-object decoupled conditions. In _Proc. of the AAAI Conference on Artificial Intelligence_, 2022.
* [70] S. Weng, P. Zhang, Z. Chang, X. Wang, S. Li, and B. Shi. Affective image filter: Reflecting emotions from text to images. In _Proc. of International Conference on Computer Vision_, 2023.
* [71] J. Xiao, K. A. Ehinger, A. Oliva, and A. Torralba. Recognizing scene viewpoint using panoramic place representation. In _Proc. of Computer Vision and Pattern Recognition_, 2012.
* [72] W. Xiong, J. Yu, Z. Lin, J. Yang, X. Lu, C. Barnes, and J. Luo. Foreground-aware image inpainting. In _Proc. of Computer Vision and Pattern Recognition_, 2019.
* [73] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang. Free-form image inpainting with gated convolution. In _Proc. of International Conference on Computer Vision_, 2019.
* [74] P. Yu, J. Guo, F. Huang, C. Zhou, H. Che, X. Ling, and Y. Guo. Hierarchical disentangled representation learning for outdoor illumination estimation and editing. In _Proc. of International Conference on Computer Vision_, 2021.
* [75] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-attention generative adversarial networks. In _Proc. of International Conference on Machine Learning_, 2019.
* [76] J. Zhang and J.-F. Lalonde. Learning high dynamic range from outdoor panoramas. In _Proc. of International Conference on Computer Vision_, 2017.
* [77] J. Zhang, K. Sunkavalli, Y. Hold-Geoffroy, S. Hadap, J. Eisenmann, and J.-F. Lalonde. All-weather deep outdoor lighting estimation. In _Proc. of Computer Vision and Pattern Recognition_, 2019.
* [78] R. Zhang, P. Isola, and A. A. Efros. Colorful image colorization. In _Proc. of European Conference on Computer Vision_, 2016.
* [79] Y. Zhang, N. Huang, F. Tang, H. Huang, C. Ma, W. Dong, and C. Xu. Inversion-based creativity transfer with diffusion models. In _Proc. of Computer Vision and Pattern Recognition_, 2023.
* [80] P. Zhu, R. Abdal, Y. Qin, and P. Wonka. SEAN: Image synthesis with semantic region-adaptive normalization. In _Proc. of Computer Vision and Pattern Recognition_, 2020.
* [81] Y. Zhu, Y. Zhang, S. Li, and B. Shi. Spatially-varying outdoor lighting estimation from intrinsics. In _Proc. of Computer Vision and Pattern Recognition_, 2021.