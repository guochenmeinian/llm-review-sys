# Adaptive Selective Sampling for Online Prediction

with Experts

Rui M. Castro

Eindhoven University of Technology,

Eindhoven Artificial Intelligence Systems Institute (EAISI)

rmcastro@tue.nl &Fredrik Hellstrom

University College London

f.hellstrom@ucl.ac.uk &Tim van Erven

University of Amsterdam

tim@timvanerven.nl

###### Abstract

We consider online prediction of a binary sequence with expert advice. For this setting, we devise label-efficient forecasting algorithms, which use a selective sampling scheme that enables collecting much fewer labels than standard procedures. For the general case without a perfect expert, we prove best-of-both-worlds guarantees, demonstrating that the proposed forecasting algorithm always queries sufficiently many labels in the worst case to obtain optimal regret guarantees, while simultaneously querying much fewer labels in more benign settings. Specifically, for a scenario where one expert is strictly better than the others in expectation, we show that the label complexity of the label-efficient forecaster is roughly upper-bounded by the square root of the number of rounds. Finally, we present numerical experiments empirically showing that the normalized regret of the label-efficient forecaster can asymptotically match known minimax rates for pool-based active learning, suggesting it can optimally adapt to benign settings.

## 1 Introduction

This paper considers online prediction with expert advice in settings where collecting feedback might be costly or undesirable. In the classical framework of sequence prediction with expert advice, a forecasting algorithm aims to sequentially predict a stream of labels on the basis of predictions issued by a number of experts (see, for instance, [1, 2, 3] and references therein). Typically, the forecaster receives the correct label after making a prediction, and uses that feedback to update its prediction strategy. There are, however, situations where collecting labels is costly and potentially unnecessary. In the context of online prediction, this naturally leads to the notion of _selective sampling_ strategies, also called _label-efficient prediction_[4, 5, 6, 7, 8, 9]. In this line of work, there is a natural tension between performance (in terms of regret bounds) and label complexity, i.e., the number of labels collected. For a worst-case scenario, the optimal label-efficient strategy amounts to "flipping a coin" to decide whether or not to collect feedback, irrespective of past actions and performance [5]. Indeed, in the worst case, the number of labels that one has to collect is linear in the number of rounds for any algorithm [10]. This is a rather pessimistic perspective, and can miss the opportunity to reduce label complexity when prediction is easy. With this in mind, the adaptive selective sampling algorithms we develop follow naturally from a simple design principle: optimize the label collection probability at any time while preserving worst-case regret guarantees. This principled perspective leads to a general way to devise simple but rather powerful algorithms. These are endowed with optimal worst-caseperformance guarantees, while allowing the forecaster to naturally adapt to benign scenarios and collect much fewer labels than standard (non-selective sampling) algorithms.

From a statistical perspective, the scenario above is closely related to the paradigm of _active learning_[11; 12; 13; 14; 15; 16]. For instance, in pool-based active learning, the learner has access to a large pool of unlabeled examples, and can sequentially request labels from selected examples. This extra flexibility, when used wisely, can enable learning a good prediction rule with much fewer labeled examples than what is needed in a _passive learning_ setting, where labeled examples are uniformly sampled from the pool in an unguided way [17; 14; 15; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27]. Our work is partly motivated by such active learning frameworks, with the aim of devising a simple and adaptive methodology that does not rely on intricate modeling assumptions.

The main contributions of this paper are novel label-efficient exponentially weighted forecasting algorithms, which optimally decide whether or not to collect feedback. The proposed approach confirms, in a sound way, the intuition that collecting labels is more beneficial whenever there is a lack of consensus among the (weighted) experts. The proposed algorithms are designed to ensure that, in adversarial settings, they retain the known worst-case regret guarantees for full-information forecasters (i.e., forecasters that collect all labels) while providing enough flexibility to attain low label complexity in benign scenarios. To characterize the label complexity of the label-efficient forecaster, we focus on a scenario where the expected loss difference between the best expert and all other experts for all \(n\) rounds is lower-bounded by \(\Delta\), and show that the label complexity is roughly \(\sqrt{n}/\Delta^{2}\), ignoring logarithmic factors. This shows that the label-efficient forecaster achieves the "best of both worlds": it smoothly interpolates between the worst case, where no method can have optimal regret with less than \(O(n)\) queries, and the benign, stochastic case, where it is sufficient to make \(O(\sqrt{n})\) queries. Finally, to further examine the performance of the label-efficient forecaster, we conduct a simulation study. We find that the performance of the label-efficient forecaster is comparable to its full-information counterpart, while collecting significantly fewer labels. Intriguingly, for a threshold prediction setting studied in [14], the numerical results indicate that the label-efficient forecaster optimally adapts to the underlying prediction problem, so that its normalized regret displays the same asymptotic behavior as known minimax rates for active learning.

Before formally introducing our setting, we discuss additional related work. Selective sampling for online learning was studied by [4; 5; 28], with a focus on probabilistic threshold functions and margin-based sampling strategies. Similarly, [8] consider kernel-based linear classifiers, and base their sampling procedure on the estimated margin of the classifier. For the same setting as we consider, [29; 30] propose a selective sampling approach based on the maximum (unweighted) prediction disagreement among the experts, and numerically demonstrate its merits. Finally, results in a similar spirit to ours have recently been established in different settings. Namely, for a strongly convex loss, [9] devised an algorithm for selective sampling with expert advice, which provably retains worst-case regret guarantees, where the sampling strategy is based on the variance of the forecaster's prediction. [31] study a setting with shifting hidden domains, and establish a tradeoff between regret and label complexity in terms of properties of these domains. For a setting where the hypothesis class has bounded VC dimension and the data satisfies a Tsybakov noise condition, [32] devise a sampling strategy, with bounds on the regret and label complexity, based on a notion of disagreement where hypotheses are discarded based on their discrepancy relative to the empirical risk minimizer.

## 2 Setting

Throughout, we focus on a binary prediction task with the zero-one loss as a performance metric. We refer to \(y_{t}\) as the outcome at time \(t\in[n]\coloneqq\{1,\ldots,n\}\). No assumptions are made on this sequence, which can potentially be created in an adversarial way. To aid in the prediction task, the forecaster has access to the predictions of \(N\) experts. The prediction of the forecaster at time \(t\) can only be a function of the expert predictions (up to time \(t\)) and the observed outcomes up to time \(t-1\). Furthermore, the algorithm can make use of internal randomization.

Formally, let \(f_{i,t}\in\{0,1\}\), with \(i\in[N]:=\{1,\ldots,N\}\) and \(t\in[n]\), denote the advice of the experts. At every time \(t\in[n]\), the forecasting algorithm must: (i) output a prediction \(\hat{y}_{t}\) of \(y_{t}\); (ii) decide whether or not to observe \(y_{t}\). Specifically, for each round \(t\in[n]\):

* The environment chooses the outcome \(y_{t}\) and the expert advice \(\left\{f_{i,t}\right\}_{i=1}^{N}\). Only the expert advice is revealed to the forecaster.

* The forecaster outputs a (possibly randomized) prediction \(\hat{y}_{t}\), based on all of the information that it has observed so far.
* The forecaster decides whether or not to have \(y_{t}\) revealed. We let \(Z_{t}\) be the indicator of that decision, where \(Z_{t}=1\) if \(y_{t}\) is revealed and \(Z_{t}=0\) otherwise.
* A loss \(\ell(\hat{y}_{t},y_{t}):=\mathbb{1}\left\{\hat{y}_{t}\neq y_{t}\right\}\) is incurred by the forecaster and a loss \(\ell_{i,t}:=\ell(f_{i,t},y_{t})\) is incurred by expert \(i\), regardless of the value of \(Z_{t}\).

Our goal is to devise a forecaster that observes as few labels as possible, while achieving low regret with respect to any specific expert. Regret with respect to the best expert at time \(n\) is defined as

\[R_{n}:=L_{n}-\min_{i\in[N]}L_{i,n}\,\]

where \(L_{n}:=\sum_{t=1}^{n}\ell(\hat{y}_{t},y_{t})\) and \(L_{i,n}:=\sum_{t=1}^{n}\ell(f_{i,t},y_{t})\). Note that the regret \(R_{n}\) is, in general, a random quantity. In this work, we focus mainly on the expected regret \(\mathbb{E}[R_{n}]\).

Clearly, when no restrictions are imposed on the number of labels collected, the optimal approach would be to always observe the outcomes (i.e., take \(Z_{t}=1\) for all \(t\in[n]\)). This is optimal in a worst-case sense, but there are situations where one can predict as efficiently while collecting much fewer labels. The main goal of this paper is the development and analysis of methods that are able to capitalize on such situations, while still being endowed with optimal worst-case guarantees.

### Exponentially weighted forecasters

All proposed algorithms in this paper are variations of _exponentially weighted forecasters_[2]. For each time \(t\in[n]\), such algorithms assign a weight \(w_{i,t}\geq 0\) to the \(i\)th expert. The forecast prediction at time \(t\) and decision whether to observe the outcome or not are randomized, and based exclusively on the expert weights and the expert predictions at that time. Therefore, \(\hat{y}_{t}\sim\text{Ber}(p_{t})\) and \(Z_{t}\sim\text{Ber}(q_{t})\) are conditionally independent Bernoulli random variables given \(p_{t}\) and \(q_{t}\). Here, \(p_{t}\) and \(q_{t}\) depend on the past only via the weights \(\{w_{i,j-1}\}_{i\in[N],j\in[t]}\) and the current expert predictions \(\{f_{i,t}\}_{i\in[N]}\). The exact specifications of \(p_{t}\) and \(q_{t}\) depend on the setting and assumptions under consideration.

After a prediction has been made, the weights for all experts are updated using the exponential weights update based on the importance-weighted losses \(\ell_{i,t}Z_{t}/q_{t}\). Specifically,

\[w_{i,t}=w_{i,t-1}\ e^{-\eta\frac{\ell_{i,t}Z_{t}}{q_{t}}}, \tag{1}\]

where \(\eta>0\) is the learning rate. To ensure that the definition in (1) is sound for any \(q_{t}\geq 0\), we set \(w_{i,t}=w_{i,t-1}\) if \(q_{t}=0\). Finally, we define the weighted average of experts predicting label \(1\) at time \(t\) as

\[A_{1,t}:=\frac{\sum_{i=1}^{N}w_{i,t-1}f_{i,t}}{\sum_{i=1}^{N}w_{i,t-1}}. \tag{2}\]

This quantity plays a crucial role in our sampling strategy. We will use the name exponentially weighted forecaster liberally to refer to any forecaster for which \(p_{t}\) is a function of \(A_{1,t}\). Throughout, we assume that the weights for all forecasters are uniformly initialized as \(w_{i,0}=1/N\) for \(i\in[N]\).

## 3 Regret bounds with a perfect expert

In this section, we consider a very optimistic scenario where one expert is perfect, in the sense that it does not make any mistakes. The results and derivation for this setting are didactic, and pave the way for more general scenarios where this assumption is dropped. We say that the \(i\)th expert is perfect if \(\ell_{i,t}=0\) for all \(t\in[n]\). The existence of such an expert implies that \(\min_{i\in[N]}L_{i,n}=0\). Therefore, the regret of a forecaster is simply the number of errors it makes, that is, \(R_{n}=L_{n}\). In such a scenario, any reasonable algorithm should immediately discard experts as soon as they make even a single mistake. For an exponentially weighted forecaster, this is equivalent to setting \(\eta=\infty\). Due to the uniform weight initialization, the scaled weight vector \(N\cdot(w_{1,t},\ldots,w_{N,t})\) is thus binary, and indicates which experts agree with all the observed outcomes up to time \(t\).

First, consider a scenario where the forecaster always collects feedback, that is, \(q_{t}=1\) for all \(t\in[n]\). A natural forecasting strategy at time \(t\) is to _follow the majority_, that is, to predict according to the majority of the experts that have not made a mistake so far. When the forecaster predicts the wrong label, this implies that at least half of the experts still under consideration are not perfect. Since the number of experts under consideration is at least halved for each mistake the forecaster incurs, this strategy is guaranteed to make at most \(\log_{2}N\) mistakes. Therefore, we have

\[R_{n}=L_{n}\leq\log_{2}N. \tag{3}\]

Clearly, this implies the following bound on the expected cumulative loss, and thus the regret:

\[\bar{L}_{n}^{(N)}:=\mathbb{E}[L_{n}]\leq\log_{2}N. \tag{4}\]

Here, the superscript \((N)\) explicitly denotes the dependence on the number of experts. This bound is tight when the minority is always right and nearly equal in size to the majority.

A natural question to ask is if there exists an algorithm that achieves the expected cumulative loss bound (4) while not necessarily collecting all labels. This is, in fact, possible. The most naive approach is to not collect a label if all experts still under consideration agree on their prediction, as in that case, they must all be correct due to the existence of a perfect expert. However, a more refined strategy that can collect fewer labels is possible, leading to the following theorem.

**Theorem 1**.: _Consider the exponentially weighted follow-the-majority forecaster with \(\eta=\infty\). Specifically, let \(p_{t}=\mathbb{1}\left\{A_{1,t}\geq 1/2\right\}\), so that \(\hat{y}_{t}=\mathbb{1}\left\{A_{1,t}\geq 1/2\right\}\). Furthermore, let_

\[q_{t}=\begin{cases}0&\text{if }A_{1,t}\in\{0,1\},\\ -\frac{1}{\log_{2}\min(A_{1,t},1-A_{1,t})}&\text{otherwise.}\end{cases}\]

_For this forecaster, we have_

\[\bar{L}_{n}^{(N)}\leq\log_{2}N\.\]

Recall that \(A_{1,t}\) is simply the proportion of experts still under consideration that predict \(y_{t}=1\). It is insightful to look at the expression for \(q_{t}\), as it is somewhat intuitive. The bigger the disagreement between the experts' predictions, the higher the probability that we collect a label. Conversely, when \(A_{1,t}\) approaches either \(0\) or \(1\), \(q_{t}\) quickly approaches zero, meaning we rarely collect a label. Theorem 1 tells us that, remarkably, we can achieve the same worst-case bound as the full-information forecaster while sometimes collecting much less feedback. The proof of this result, in Appendix A, uses a clean induction argument that constructively gives rise to the expression for \(q_{t}\). This principled way of reasoning identifies, in a sense, the best way to assess disagreement between experts: the specified \(q_{t}\) is the lowest possible sampling probability that preserves worst-case regret guarantees.

A slightly better regret bound is possible by using a variation of follow the majority, called the boosted majority of leaders. For this algorithm, the upper bound is endowed with a matching lower bound (including constant factors). In Appendix B, we devise a label-efficient version of the boosted majority of leaders, retaining the same worst-case regret bound as its full-information counterpart.

## 4 General regret bounds without a perfect expert

In this section, we drop the assumption of the existence of a perfect expert. It is therefore no longer sensible to use an infinite learning rate \(\eta\), since this would discard very good experts based on their first observed error. We consider the general exponentially weighted forecaster described in Section 2.1, now simply with \(p_{t}=A_{1,t}\).

For the scenario where \(q_{t}=1\) for all \(t\), a classical regret bound is well-known (see, for instance, (Bishop, 2006, Thm 2.2)). Specifically, for the general exponentially weighted forecaster, with \(p_{t}=A_{1,t}\), \(q_{t}=1\), and uniform weight initialization, we have

\[\bar{R}_{n}:=\mathbb{E}[R_{n}]=\mathbb{E}\bigg{[}L_{n}-\min_{i\in[N]}L_{i,n} \bigg{]}\leq\frac{\ln N}{\eta}+\frac{n\eta}{8}. \tag{5}\]

In Theorem 2 below, we prove a stronger version of (5) that allows for an adaptive label-collection procedure. As before, we focus on the expected regret, \(\bar{R}_{n}=\mathbb{E}[R_{n}]\). As done in Section 3 for the case of a perfect expert, we identify an expression for \(q_{t}\), which is not necessarily identically \(1\), but still ensures the bound in (5) is valid. To state our main result, we need the following definition, which is guaranteed to be sound by Lemma 1.

**Definition 1**.: _For \(x\in[0,1]\) and \(\eta>0\), define_

\[q^{*}(x,\eta)=\inf\left\{q\in(0,1]:x+\frac{q}{\eta}\ln\left(1-x+xe ^{-\eta/q}\right)\leq\frac{\eta}{8},\right. \tag{6}\] \[\left.1-x+\frac{q}{\eta}\ln\left(x+(1-x)e^{-\eta/q}\right)\leq \frac{\eta}{8}\right\}\.\]

In the following theorem, we present the label-efficient version of (5).

**Theorem 2**.: _Consider an exponentially weighted forecaster with \(p_{t}=A_{1,t}\) and_

\[q_{t}\geq q^{*}(A_{1,t},\eta):=q_{t}^{*}\.\]

_For this forecaster, we have_

\[\bar{R}_{n}=\mathbb{E}\!\left[L_{n}-\min_{i\in[N]}L_{i,n}\right]\leq\frac{\ln N }{\eta}+\frac{n\eta}{8}. \tag{7}\]

The proof, which is deferred to Appendix C, is similar to that used for Theorem 1, but with key modifications to account for the lack of a perfect expert. In particular, we need to account for the finite, importance-weighted weight updates, and carefully select \(q_{t}^{*}\) accordingly. While the proof allows for non-uniform weight initializations, we focus on the uniform case, as this enables us to optimally tune the learning rate. The result for general weight initializations is given in Appendix C.

Theorem 2 shows that the proposed label-efficient forecaster satisfies the same expected regret bound as the exponentially weighted forecaster with \(q_{t}:=1\). While the expression for \(q^{*}(x,\eta)\) in (6) is somewhat opaque, the underlying motivation is constructive, and it arises naturally in the proof of the theorem. In fact, \(q_{t}^{*}\) is the smallest possible label-collection probability ensuring the regret bound (7). One may wonder if \(q_{t}^{*}\) is well defined, as it is the infimum of a set that may be empty. However, as shown in the following lemma, this set always contains the point \(1\), ensuring that \(q_{t}^{*}\leq 1\).

**Lemma 1**.: _For all \(\eta>0\) and \(x\in[0,1]\), we have_

\[1\in\left\{q\in(0,1]:x+\frac{q}{\eta}\ln\left(1-x+xe^{-\eta/q}\right)\leq\frac {\eta}{8}\right\}\.\]

The proof is presented in Appendix D, and is essentially a consequence of Hoeffding's inequality. While \(q^{*}(x,\eta)\) does not admit an analytic solution, its behavior as a function of \(x\), depicted in Figure 1, is rather intuitive. Since \(\eta=\sqrt{8(\ln N)/n}\) minimizes the regret bound (7), we are primarily interested in small values of \(\eta\). When the learning rate \(\eta\) is not too large, the behavior of \(q_{t}^{*}\) can be interpreted as follows: the larger the (weighted) disagreement of the experts is, the closer the value of \(A_{1,t}\) is to the point \(1/2\). In this case, \(q_{t}^{*}\) will be close to \(1\), and we collect a label with high probability. When \(A_{1,t}\) is close to 0 or 1, the (weighted) experts essentially agree, so the probability of collecting a label will be small. For large learning rates, the behavior of \(q_{t}^{*}\) appears a bit strange, but note that for \(\eta\geq 8\), the regret bound is vacuous. Thus, for this case, \(q^{*}(x,\eta)=0\) for all \(x\in[0,1]\).

The regret guarantee in Theorem 2 is valid provided one uses any choice \(q_{t}\geq q_{t}^{*}\). The following lemma provides both an asymptotic characterization of \(q_{t}^{*}\) as \(\eta\to 0\), as well as a simple upper bound that can be used for both analytical purposes and practical implementations.

Figure 1: The function \(q^{*}(x,\eta)\) for various values of \(\eta\). Panel (b) is a zoomed version of panel (a).

**Lemma 2**.: _For any \(x\in[0,1]\), we have_

\[\lim_{\eta\to 0}q^{*}(x,\eta)=4x(1-x)\;.\]

_Furthermore, for any \(\eta>0\) and \(x\in[0,1]\),_

\[q^{*}(x,\eta)\leq\min(4x(1-x)+\eta/3,1)\enspace. \tag{8}\]

The proof of this result is somewhat technical and tedious, and deferred to Appendix E. In the remainder of this paper, we will use this upper bound extensively.

## 5 Label complexity

We now examine the label complexity, defined as \(S_{n}:=\sum_{t=1}^{n}Z_{t}\). In [10, Thm. 13], it is shown that there exists a setting for which the expected regret of a forecaster that collects \(m\) labels is lower-bounded by \(cn\sqrt{\ln(N-1)/m}\) for some constant \(c\). Hence, in the worst case, the number of collected labels needs to be linear in \(n\) in order to achieve an expected regret that scales at most as \(\sqrt{n}\). However, since \(q_{t}^{*}\) can be less than \(1\), it is clear that the label-efficient exponentially weighted forecaster from Theorem 2 can collect fewer than \(n\) labels in more benign settings. To this end, we consider a scenario with a unique best expert, which at each round is separated from the rest in terms of its expected loss. To state the condition precisely, we need to define \(\mathbb{E}_{t}=\mathbb{E}[\,\cdot\mid\mathcal{F}_{t-1}]\) as the expectation at time \(t\) conditional on all possible randomness up to time \(t-1\), that is, for \(\mathcal{F}_{t}=\sigma(\{Z_{j},y_{j},f_{1,j},\ldots,f_{N,j}\}_{j=1,\ldots,t})\). With this, we assume that there is a unique expert \(i^{*}\in[N]\) such that, for all \(i\neq i^{*}\) and \(t\in[n]\),

\[\mathbb{E}_{t}[\ell_{i,t}-\ell_{i^{*},t}]\geq\Delta>0\qquad\text{ almost surely.}\]

The parameter \(\Delta\) characterizes the difficulty of the given learning problem. If \(\Delta\) is large, the best expert significantly outperforms the others, and is thus easily discernible, whereas if \(\Delta\) is small, the best expert is harder to identify. In particular, if the vectors \((y_{t},f_{1,t},\ldots,f_{N,t})\) are independent and identically distributed over rounds \(t\in[n]\), \(\Delta\) is just the difference in expected loss between the best and the second-best expert in a single round, which is a common measure of difficulty for stochastic bandits [33, Thm. 2.1]. This difficulty measure has also been used in the context of prediction with expert advice [34]. Similar stochastic assumptions are standard in (batch) active learning, and highly relevant in practical settings (see [14, 15, 18, 21, 27] and references therein). Strictly speaking, our result holds under a more general assumption, where the best expert _emerges after a time \(\tau^{*}\)_ instead of being apparent from the first round. This means that the best expert is even allowed to perform the worst for some rounds, as long as it performs well in sufficiently many other rounds. While we state and prove the result under this more general condition in Appendix F, we present the simpler assumption here for clarity.

We now state our main result for the label complexity.

**Theorem 3**.: _Consider the label-efficient exponentially weighted forecaster from Theorem 2 with \(q_{t}=\min(4A_{1,t}(1-A_{1,t})+\eta/3,1)\) and any \(\eta>0\). Suppose that there exists a single best expert \(i^{*}\) such that, for all \(i\neq i^{*}\) and all \(t\in[n]\),_

\[\mathbb{E}_{t}[\ell_{i,t}-\ell_{i^{*},t}]\geq\Delta>0\qquad\text{almost surely.}\]

_Then, for any \(n\geq 4\), the expected label complexity is at most_

\[\mathbb{E}[S_{n}]\leq\frac{50}{\eta\Delta^{2}}\ln\Big{(}\frac{N\ln n}{\eta} \Big{)}+3\eta n+1\;. \tag{9}\]

Proof sketch.: Initially, the sampling probability \(q_{t}\) is large, but as we collect more labels, it will become detectable that one of the experts is better than the others. As this happens, \(q_{t}\) will tend to decrease until it (nearly) reaches its minimum value \(\eta/3\). We therefore divide the forecasting process into time \(t\leq\tau\) and \(t>\tau\). With a suitable choice of \(\tau\approx 1/(\eta\Delta^{2})\) (up to logarithmic factors), we can guarantee that the sampling probability is at most \(q_{t}\leq 4\eta/3\) for all \(t>\tau\) with sufficiently high probability. This is shown by controlling the deviations of the cumulative importance-weighted loss differences \(\tilde{\Lambda}_{t}^{i}=\sum_{j=1}^{t}(l_{i,j}-l_{i^{*},j})/q_{j}\) for \(i\neq i^{*}\) from their expected values by using an anytime version of Freedman's inequality. With this, we simply upper bound the label complexity for the first \(\tau\) rounds by \(\tau\), and over the remaining rounds, the expected number of collected labels is roughly \(\eta(n-\tau)\leq\eta n\). This leads to a total expected label complexity of \(1/(\eta\Delta^{2})+\eta n\), up to logarithmic factors. The full proof is deferred to Appendix F.

As mentioned earlier, the learning rate optimizing the regret bound (7) is \(\eta=\sqrt{8\ln(N)/n}\). For this particular choice, the label complexity in (9) is roughly \(\sqrt{n}/\Delta^{2}\), up to constants and logarithmic factors. We have thus established that the label-efficient forecaster achieves the best of both worlds: it queries sufficiently many labels in the worst case to obtain optimal regret guarantees, while simultaneously querying much fewer labels in more benign settings. It is interesting to note that the label complexity dependence of \(1/\Delta^{2}\) on \(\Delta\) is less benign than the dependence of the regret bound from, e.g., (34, Thm. 11), which is \(1/\Delta\). The underlying reason for this is that, while the two are similar, the label complexity is not directly comparable to the regret. In particular, the label complexity has much higher variance.

The bound of Theorem 3 relies on setting the sampling probability \(q_{t}\) to be the upper bound on \(q_{t}^{*}\) from Lemma 2. This bound is clearly loose when \(q_{t}^{*}\) is approximately zero, and one may wonder if the label complexity of the algorithm would be radically smaller when using a forecaster for which \(q_{t}=q_{t}^{*}\) instead. With the choice \(\eta=\sqrt{8\ln(N)/n}\), which optimizes the bound in (7), it seems unlikely that the label complexity will substantially change, as numerical experiments suggest that the label complexity attained with \(q_{t}\) set as \(q_{t}^{*}\) or the corresponding upper bound from (8) appear to be within a constant factor. That being said, for larger values of \(\eta\), the impact of using the upper bound in (8) is likely much more dramatic.

## 6 Numerical experiments

To further assess the behavior of the label-efficient forecaster from Theorem 2, we consider a classical active learning scenario in a batch setting, for which there are known minimax rates for the risk under both active and passive learning paradigms. We will set the sampling probability to be

\[q_{t}=\min(4A_{1,t}(1-A_{1,t})+\eta/3,1)\.\]

Let \(D_{n}=\left((X_{t},Y_{t})\right)_{t=1}^{n}\) be an ordered sequence of independent and identically distributed pairs of random variables with joint distribution \(D\). The first entry of \((X_{i},Y_{i})\) represents a feature, and the second entry is the corresponding label. The goal is to predict the label \(Y_{i}\in\{0,1\}\) based on the feature \(X_{i}\). Specifically, we want to identify a map \((x,D_{n})\mapsto\hat{g}_{n}(x,D_{n})\in\{0,1\}\) such that, for a pair \((X,Y)\sim D\) that is drawn independently from \(D_{n}\), we have small (zero-one loss) expected risk

\[\text{Risk}(\hat{g}_{n}):=\mathbb{P}(\hat{g}_{n}(X,D_{n})\neq Y)\.\]

Concretely, we consider the following scenario, inspired by the results in (14). Let the features \(X_{i}\) be uniformly distributed in \([0,1]\), and \(Y_{i}\in\{0,1\}\) be such that \(\mathbb{P}(Y_{i}=1|X_{i}=x)=\zeta(x)\). Specifically, let \(\tau_{0}\in[0,1]\) such that \(\zeta(x)\geq 1/2\) when \(x\geq\tau_{0}\) and \(\zeta(x)\leq 1/2\) otherwise. Furthermore, assume that for all \(x\in[0,1]\), \(\zeta(x)\) satisfies

\[c|x-\tau_{0}|^{\kappa-1}\leq|\zeta(x)-1/2|\leq C|x-\tau_{0}|^{\kappa-1}\,\]

for some \(c,C>0\) and \(\kappa>1\). If \(\tau_{0}\) is known, the optimal classifier as \(n\to\infty\). This shows that there are potentially massive gains for active learning, particularly when \(\kappa\) is close to 1. A natural question is whether similar conclusions hold for streaming active learning. In this setting, instead of selecting which example \(X_{i}^{\prime}\) to query, the learner observes the features \((X_{1},\dots,X_{n})\) sequentially, and decides at each time \(t\) whether or not it should query the corresponding label. This is analogous to the online prediction setting discussed in this paper.

We now study this setting numerically. For the simulations, we use the specific choice

\[\zeta(x)=\frac{1}{2}+\frac{1}{2}\text{sign}(x-\tau_{0})|x-\tau_{0}|^{\kappa-1}\,,\]

to generate sequences \((Y_{1},\dots,Y_{n})\), based on a sequence of features \((X_{1},\dots,X_{n})\) sampled from the uniform distribution on \([0,1]\). Furthermore, we consider the class of \(N\) experts such that

\[f_{i,t}=1\left\{X_{t}\geq\frac{i-1}{N-1}\right\}\,,\]

with \(i\in[N]\) and \(t\in[n]\).

### Expected regret and label complexity

In the simulations, we set \(\tau_{0}=1/2\) and \(N=\lceil\sqrt{n}\rceil+1\left\{\lceil\sqrt{n}\rceil\text{ is even}\right\}\). This choice enforces that \(N\) is odd, ensuring the optimal classifier is one of the experts. Throughout, we set \(\eta=\sqrt{8\ln(N)/n}\), which minimizes the regret bound (7). First, we investigate the expected regret relative to the optimal prediction rule for the label-efficient exponentially weighted forecaster with \(q_{t}\) given by (8), and compare it with the corresponding regret for the full-information forecaster that collects all labels. Specifically, the regret at time \(t\) of a forecaster that predicts \(\{\hat{Y}_{j}\}_{j=1}^{n}\) is given by

\[\mathbb{E}[R_{t}]=\sum_{j=1}^{t}\mathbb{E}[\ell(\hat{Y}_{t},Y_{t})-\ell(g^{*}(X _{t}),Y_{t})]\;.\]

Figure 2: Numerical results for expected regret and label complexity when \(n=50000\) and \(N=225\). Panels (a) and (b) depict the expected regret \(\mathbb{E}[R_{t}]\) as a function of \(t\), for \(\kappa=2\) and \(\kappa=1.5\) respectively. Panels (c) and (d) depict the expected label complexity of the label-efficient forecaster, \(\mathbb{E}[S_{t}]\), as a function of \(t\) for \(\kappa=2\) and \(\kappa=1.5\) respectively. The expectations were estimated from \(500\) independent realizations of the process and the shaded areas indicate the corresponding pointwise \(95\%\) confidence intervals.

Furthermore, to study the potential reduction in the number of collected labels, we also evaluate the expected label complexity \(\mathbb{E}[S_{t}]\) of the label-efficient forecaster. To approximate the expectations above, we use Monte-Carlo averaging with \(500\) independent realizations. Further experimental details are given in Appendix G. The results are shown in Figure 2. We see that the regret is comparable for the full-information and label-efficient forecasters, albeit slightly higher for the latter. Since \(P(g^{*}(X)\neq Y)=\frac{1}{2}-\frac{1}{\kappa 2^{\kappa}}\), the expected cumulative loss of the optimal classifier grows linearly with \(t\). For instance, when \(\kappa=2\), we have \(\sum_{j=1}^{t}\mathbb{E}[\ell(g^{*}(X_{t}),Y_{t})]=3t/8\). Hence, the regret relative to the best expert is much smaller than the pessimistic (i.e., worst-case for adversarial environments) bound in (7). We also observe that the expected label complexity grows sub-linearly with \(t\), as expected, and that \(\mathbb{E}[S_{n}]\ll n\), demonstrating that a good prediction rule can be learned with relatively few labels. When \(\kappa=1.5\), the number of collected labels is significantly smaller than when \(\kappa=2\). This is in line with the known minimax rates for active learning from [14]. To further examine this connection, we now turn to normalized regret.

### Normalized regret relative to the number of samples

To relate the results of the label-efficient forecaster with known minimax rates for active learning, we investigate the expected regret normalized by the number of samples. Specifically, let

\[r(t)=\frac{1}{t}\mathbb{E}[R_{t}]=\frac{1}{t}\sum_{j=1}^{t}\mathbb{E}[\ell( \widehat{Y}_{t},Y_{t})-\ell(g^{*}(X_{t}),Y_{t})]\;.\]

For the full-information forecaster, we expect that \(r(t)\asymp t^{-\kappa/(2\kappa-1)}\) as \(t\to\infty\). The same holds for the label-efficient forecaster, but in this case, the relation between \(r(t)\) and the expected number of collected labels \(\mathbb{E}[S_{t}]\) is more interesting. If the label-efficient forecaster performs optimally, we expect \(r(t)\asymp\mathbb{E}[S_{t}]^{-\kappa/(2\kappa-2)}\) as \(t\to\infty\). To examine this, we plot \(r(t)\) against \(\mathbb{E}[S_{t}]\) (which equals \(t\) for the full-information forecaster) in logarithmic scales, so the expected asymptotic behavior corresponds to a linear decay with slopes given by \(-\kappa/(2\kappa-1)\) for the full-information forecaster and \(-\kappa/(2\kappa-2)\) for the label-efficient forecaster. This is shown in Figure 3 for \(\kappa=1.5\) and \(\kappa=2\).

We see that the observed behavior is compatible with the known asymptotics for active learning, and similar results arise when considering different values of \(\kappa\). More importantly, it appears that the label-efficient forecaster optimally adapts to the underlying setting. This is remarkable, as the label-efficient forecaster does not rely on any domain knowledge. Indeed, it has no knowledge of the statistical setting, and in particular, it has no knowledge of the parameter \(\kappa\), which encapsulates the difficulty of the learning task. Note that our regret bounds are too loose to provide a theoretical justification of these observations via an online-to-batch conversion, and that such theoretical analyses will only be fruitful when considering non-parametric classes of experts, for which the asymptotics of the excess risk are \(\omega(1/\sqrt{n})\).

Figure 3: Numerical results for the normalized regret as a function of the expected label complexity when \(n=50000\) and \(N=225\). The straight dotted lines are displayed for comparison, and have slopes given by \(-\kappa/(2\kappa-1)\) (full information) and \(-\kappa/(2\kappa-2)\) (label efficient). The expectations were estimated from \(500\) independent realizations of the process and the shaded areas indicate the corresponding pointwise \(95\%\) confidence intervals.

Discussion and outlook

In this paper, we presented a set of adaptive label-efficient algorithms. These follow from a very straightforward design principle, namely, identifying the smallest possible label collection probability \(q_{t}\) that ensures that a known worst-case expected regret bound is satisfied. This leads to simple, yet powerful, algorithms, endowed with best-of-both-worlds guarantees. We conjecture that a similar approach can be used for a broader class of prediction tasks and losses than what is considered in this paper. For instance, the results we present can be straightforwardly extended to a setting where the expert outputs take values in \([0,1]\), as long as the label sequence and forecaster prediction remain binary and take values in \(\{0,1\}\). In fact, the same inductive approach can be used when \(y_{t}\in[0,1]\) and one considers a general loss function. However, the resulting label collection probability will be significantly more complicated than that of Definition 1. An interesting side effect of our analysis is that it leads to an inductive proof of the regret bound for standard, full-information algorithms. Extending the label complexity result, and in particular connecting it with known minimax theory of active learning in statistical settings, remains an interesting avenue for future research. Finally, another intriguing direction is to extend our approach to the bandit setting. In the setting we consider in this paper, we observe the losses of all experts when observing a label. In contrast, in the bandit setting, only the loss of the selected arm would be observed for each round. This would necessitate the forecaster to incorporate more exploration in its strategy, and the analysis of a label-efficient version seems like it would be quite different from what is used in this paper, although some of the ideas may transfer.

## Acknowledgements

The authors would like to thank Wojciech Kotlowski, Gabor Lugosi, and Menno van Eersel for fruitful discussions that contributed to this work. The algorithmic ideas underlying this work were developed when R. Castro was a research fellow at the University of Wisconsin - Madison. This work was partly done while F. Hellstrom was visiting the Eindhoven University of Technology and the University of Amsterdam supported by EURANDOM and a STAR visitor grant, the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation, and the Chalmers AI Research Center (CHAIR). T. van Erven was supported by the Netherlands Organization for Scientific Research (NWO), grant number VI.Vidi.192.095.

## References

* [1] Volodimir G. Vovk. Aggregating strategies. In _Proc. Workshop on Computational Learning Theory (COLT)_, Rochester, NY, USA, 1990.
* [2] N. Littlestone and M.K. Warmuth. The weighted majority algorithm. _Information and Computation_, 108(2):212-261, 1994.
* [3] N. Cesa-Bianchi and G. Lugosi. _Prediction, Learning, and Games_. Cambridge University Press, 2006.
* [4] David Helmbold and Sandra Panizza. Some label efficient learning results. In _Proc. Conference on Computational Learning Theory (COLT)_, Nashville, TN, USA, July 1997.
* [5] N. Cesa-Bianchi, A. Conconi, and C. Gentile. Learning probabilistic linear-threshold classifiers via selective sampling. In _Proc. Conference on Learning Theory (COLT)_, Washington, DC, USA, Aug. 2003.
* [6] Bhuvesh Kumar, Jacob D Abernethy, and Venkatesh Saligrama. Activeheedge: Hedge meets active learning. In _Proc. International Conference on Machine Learning (ICML)_, Edinburgh, Scotland, June 2012.
* [7] Ofer Dekel, Claudio Gentile, and Karthik Sridharan. Selective sampling and active learning from single and multiple teachers. _The Journal of Machine Learning Research_, 13(1):2655-2697, 2012.
* [8] Francesco Orabona and Nicolo Cesa-Bianchi. Better algorithms for selective sampling. In _Proc. International Conference on Machine Learning (ICML)_, Bellevue, WA, USA, June 2011.

* [9] Dirk van der Hoeven, Nikita Zhivotovskiy, and Nicolo Cesa-Bianchi. A regret-variance trade-off in online learning. In _Proc. Conference on Neural Information Processing Systems (NeurIPS)_, New Orleans, LA, USA, Nov. 2022.
* [10] N. Cesa-Bianchi, G. Lugosi, and G. Stoltz. Minimizing regret with label efficient prediction. _IEEE Transactions on Information Theory_, 51(6):2152-2162, 2005.
* [11] D. J. C. Mackay. Information-based objective functions for active data selection. _Neural Computation_, 4:698-714, 1991.
* [12] D. Cohn, Z. Ghahramani, and M. Jordan. Active learning with statistical models. _Journal of Artificial Intelligence Research_, pages 129-145, 1996.
* [13] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by committee algorithm. _Machine Learning_, 28(2-3):133-168, Aug. 1997.
* [14] Rui M. Castro and Robert D. Nowak. Minimax bounds for active learning. _Transactions on Information Theory_, 54(5):2339-2353, Apr. 2008.
* [15] N. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In _Proc. International Conference on Machine Learning (ICML)_, Pittsburgh, PA, USA, June 2006.
* [16] Alina Beygelzimer, Sanjoy Dasgupta, and John Langford. Importance weighted active learning. In _Proc. International Conference on Machine Learning (ICML)_, Montreal, June 2009.
* [17] R. Castro, R. Willett, and R. Nowak. Faster rates in regression via active learning. In _Proc. Conference on Neural Information Processing Systems (NeurIPS)_, 2005. extended version available at [http://homepages.cae.wisc.edu/](http://homepages.cae.wisc.edu/)\(\sim\)rcastro/ECE-05-3.pdf.
* [18] S. Dasgupta, A. Kalai, and C. Monteleoni. Analysis of perceptron-based active learning. In _Proc. Conference on Learning Theory (COLT)_, Bertinoro, Italy, June 2005.
* [19] S. Dasgupta. Coarse sample complexity bounds for active learning. In _Proc. Conference on Neural Information Processing (NeurIPS)_, Vancouver, Canada, Dec. 2005.
* [20] M. F. Balcan, C. Berlind, A. Blum, E. Cohen, K. Patnaik, and L. Song. Active learning and best-response dynamics. In _Proc. Conference on Neural Information Processing Systems (NeurIPS)_, Montreal, Canada, Dec. 2014.
* [21] P. Awasthi, M. Balcan, and P. M. Long. The power of localization for efficiently learning linear separators with noise. In _Proc. Annual ACM Symposium on Theory of Computing (STOC)_, New York, NY, USA, June 2014.
* [22] A. Krause and C. Guestrin. Nonmyopic active learning of Gaussian processes: An exploration-exploitation approach. In _Proc. International Conference on Machine Learning (ICML)_, 2007.
* [23] X. Zhu, J. Lafferty, and Z. Ghahramani. Combining active learning and semi-supervised learning using Gaussian fields and harmonic functions. In _Workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining, ICML_, Washington, D.C., USA, Aug. 2003.
* [24] J. L. Williams, J. W. Fisher, and A. S. Willsky. Performance guarantees for information theoretic active inference. In _Proc. International Conference on Artificial Intelligence and Statistics (AISTATS)_, San Juan, Puerto Rico, Mar. 2007.
* [25] B. Chen, R. Castro, and A. Krause. Joint optimization and variable selection of high-dimensional gaussian processes. In _Proc. International Conference on Machine Learning (ICML)_, Edinburgh, Scotland, July 2012.
* [26] A. Epshteyn, A. Vogel, and G. DeJong. Active reinforcement learning. In _Proc. International Conference on Machine Learning (ICML)_, Helsinki, Finland, July 2008.
* [27] Steve Hanneke. Theory of disagreement-based active learning. _Foundations and Trends in Machine Learning_, 7(2-3):131-309, 2014.

* [28] Nicolo Cesa-Bianchi, Claudio Gentile, and Luca Zaniboni. Worst-case analysis of selective sampling for linear classification. _Journal of Machine Learning Research_, 7(44):1205-1230, 2006.
* [29] Peilin Zhao, Steven Hoi, and Jinfeng Zhuang. Active learning with expert advice. In _Proc. Uncertainty in Artificial Intelligence (UAI)_, Bellevue, WA, USA, July 2013.
* [30] Shuji Hao, Peiying Hu, Peilin Zhao, Steven C. H. Hoi, and Chunyan Miao. Online active learning with expert advice. _ACM Trans. Knowl. Discov. Data_, 12(5), June 2018.
* [31] Yining Chen, Haipeng Luo, Tengyu Ma, and Chicheng Zhang. Active online learning with hidden shifting domains. In _Proc. International Conference on Artificial Intelligence and Statistics (AISTATS)_, San Diego, CA, USA, Aug. 2021.
* [32] Boshuang Huang, Sudeep Salgia, and Qing Zhao. Disagreement-based active learning in online settings. _IEEE Transactions on Signal Processing_, 70:1947-1958, Mar. 2022.
* [33] Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends in Machine Learning_, 5(1), 2012.
* [34] Pierre Gaillard, Gilles Stoltz, and Tim van Erven. A second-order bound with excess losses. In _Proc. Conference on Learning Theory (COLT)_, Barcelona, Spain, June 2014.
* [35] A. B. Tsybakov. On nonparametric estimation of density level sets. _Annals of Statistics_, 25:948-969, 1997.
* [36] A.R. Karlin and Y. Peres. _Game Theory, Alive_. American Mathematical Society, 2017.
* [37] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization, 2011.
* [38] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In _Proc. International Conference on Machine Learning (ICML)_, Edinburgh, Scotland, UK, June 2012.

## Appendix A Proof of Theorem 1

**Theorem 1**.: _Consider the exponentially weighted follow the majority forecaster with \(\eta=\infty\). Specifically, let \(p_{t}=\mathbb{1}\)\(\{A_{1,t}\geq 1/2\}\), so that \(\hat{y}_{t}=\mathbb{1}\)\(\{A_{1,t}\geq 1/2\}\). Furthermore, let_

\[q_{t}=\begin{cases}0&\text{if }A_{1,t}\in\{0,1\},\\ -\frac{1}{\log_{2}\min(A_{1,t},1-A_{1,t})}&\text{otherwise.}\end{cases}\]

_For this forecaster, we have_

\[\bar{L}_{n}^{(N)}\leq\log_{2}N\.\]

Proof.: The main idea is to proceed by induction on \(n\). For \(n=1\), the result holds trivially, regardless of the choice for \(q_{t}\). Now, suppose that \(\bar{L}_{t}^{(N)}\leq\log_{2}N\) for all values \(t\in[n-1]\), any sequence of observations and expert predictions, and any number of experts \(N\). Based on this assumption, we will derive a bound for \(\bar{L}_{n}^{(N)}\). Let \(k:=\sum_{i=1}^{N}\ell_{i,1}\) be the number of experts that make a mistake when \(t=1\). Note that \(0\leq k\leq N-1\), as there is one perfect expert. When \(k<N/2\), the majority vote \(\hat{y}_{1}\) is necessarily equal to \(y_{1}\). Therefore,

\[\bar{L}_{n}^{(N)} =\mathbb{E}[\ell(\hat{y}_{1},y_{1})]+\sum_{t=2}^{n}\mathbb{E}[ \ell(\hat{y}_{t},y_{t})]=\sum_{t=2}^{n}\mathbb{E}[\ell(\hat{y}_{t},y_{t})]\] \[=\sum_{t=2}^{n}q_{1}\mathbb{E}[\ell(\hat{y}_{t},y_{t})|Z_{1}=1]+( 1-q_{1})\mathbb{E}[\ell(\hat{y}_{t},y_{t})|Z_{1}=0]\] \[=q_{1}\bar{L}_{2:n}^{(N-k)}+(1-q_{1})\bar{L}_{2:n}^{(N)}\] \[\leq\log_{2}N\, \tag{10}\]

where \(\bar{L}_{2:n}^{(N)}\) denotes the expected cumulative loss in rounds \(2,\ldots,n\) with \(N\) experts. Because the internal state of the algorithm only consists of a list of experts that have made no errors so far, the task in rounds \(2,\ldots,n\) is equivalent to a task over \(n-1\) rounds starting with the experts that remain after round \(1\). Therefore, we can apply the induction hypothesis to obtain \(\bar{L}_{2:n}^{(N-k)}\leq\log_{2}(N-k)\leq\log_{2}N\) and \(\bar{L}_{2:n}^{(N)}\leq\log_{2}N\), which justifies the last inequality. We conclude that, when \(k<N/2\), the bound holds regardless of the choice of \(q_{1}\).

On the other hand, if \(N/2\leq k\leq N-1\), the forecaster incurs an error at time \(t=1\). Using the induction hypothesis and an analogous reasoning as above, we find that

\[\bar{L}_{n}^{(N)}=1+q_{1}\bar{L}_{2:n}^{(N-k)}+(1-q_{1})\bar{L}_{2:n}^{(N)} \leq 1+q_{1}\log_{2}(N-k)+(1-q_{1})\log_{2}(N). \tag{11}\]

To ensure that the right-hand-side of (11) satisfies the desired bound, we need to select \(q_{1}\) such that

\[1+q_{1}\log_{2}(N-k)+(1-q_{1})\log_{2}(N)\leq\log_{2}N\,\]

which can be written equivalently as

\[\frac{1}{q_{1}}\leq\log_{2}N-\log_{2}(N-k)\.\]

If we do not observe a label, we do not know the value \(k\). All we know is that, since \(y_{1}\in\{0,1\}\), we have \(k\in\left\{\sum_{i=1}^{N}f_{i,1},N-\sum_{i=1}^{N}f_{i,1}\right\}\). Therefore, to ensure the induction proof works, we need

\[q_{1}\geq-\frac{1}{\log_{2}\min\left(A_{1,1},1-A_{1,1}\right)}\,\]

where \(A_{1,1}=\frac{1}{N}\sum_{i=1}^{N}f_{i,1}\). Note that the case \(k=N\) cannot occur as there is always a perfect expert, but to ensure that the above definition is sound, we define \(q_{1}=0\) when \(A_{1,1}\in\{0,1\}\).

Label-efficient boosted majority of leaders

Through a variation of follow the majority, a slightly better regret bound than the one in Theorem 1 can be obtained, as well as a matching lower bound. As shown by [36, Proposition 18.1.3], there exists an adversarial strategy for the environment such that any forecaster will incur at least \(\lfloor\log_{2}N\rfloor/2\geq\lfloor\log_{4}N\rfloor\) errors in expectation. A matching upper bound can be obtained, when \(N\) is a power of two, by considering a forecaster that incorporates randomness in its predictions. This is referred to as the _boosted majority of leaders_. As shown in the following theorem, this procedure can be made label-efficient while ensuring the same expected regret bound.

**Theorem 4**.: _Consider an exponentially weighted forecaster for which \(\eta=\infty\) and_

\[p_{t}=\left\{\begin{array}{ll}0&\mbox{if }A_{1,t}\leq 1/4\\ 1+\log_{4}A_{1,t}&\mbox{if }1/4<A_{1,t}\leq 1/2\\ -\log_{4}(1-A_{1,t})&\mbox{if }1/2<A_{1,t}\leq 3/4\\ 1&\mbox{if }A_{1,t}>3/4\end{array}\right.\]

_and_

\[q_{t}=\left\{\begin{array}{ll}0&\mbox{if }A_{1,t}=0\\ -1/\log_{4}A_{1,1}&\mbox{if }0<A_{1,t}<1/4\\ 1&\mbox{if }1/4<A_{1,t}\leq 3/4\\ -1/\log_{4}(1-A_{1,1})&\mbox{if }3/4<A_{1,t}<1\\ 0&\mbox{if }A_{1,t}=1\end{array}\right..\]

_For this forecaster, we have_

\[\bar{L}_{n}^{(N)}\leq\log_{4}N\;.\]

Proof.: The proof technique is analogous to that of Theorem 1: we use an induction argument to find a choice of \(q_{t}\) that guarantees the desired regret bound. We will proceed by analyzing different cases, depending on the value of \(A_{1,t}\). First, assume that \(A_{1,1}\leq 1/4\). If \(y_{1}=0\), then

\[\bar{L}_{n}^{(N)}=0+q_{1}\bar{L}_{2:n}^{(N(1-A_{1,1}))}+(1-q_{1})\bar{L}_{2:n} ^{(N)}\;.\]

Thus, taking \(q_{1}\geq 0\) suffices. If \(y_{1}=1\), then

\[\bar{L}_{n}^{(N)}=1+q_{1}\bar{L}_{2:n}^{(NA_{1,1})}+(1-q_{1})\bar{L}_{2:n}^{(N )}\;,\]

and, using the same reasoning as before, it suffices to take

\[q_{1}\geq-\frac{1}{\log_{4}A_{1,1}}\;.\]

An analogous reasoning applies when \(A_{1,1}>3/4\), so that for this case, it suffices to take

\[q_{1}\geq-\frac{1}{\log_{4}(1-A_{1,1})}\;.\]

Now, consider the case \(1/4<A_{1,1}\leq 1/2\). If \(y_{1}=0\), then

\[\bar{L}_{n}^{(N)} =1+\log_{4}A_{1,1}+q_{1}\bar{L}_{2:n}^{(N(1-A_{1,1}))}+(1-q_{1}) \bar{L}_{2:n}^{(N)}\] \[\leq 1+\log_{4}A_{1,1}+q_{1}\log_{4}(N(1-A_{1,1}))+(1-q_{1})\log_{ 4}N\;,\]

which implies that

\[q_{1}\geq-\frac{1+\log_{4}A_{1,1}}{\log_{4}(1-A_{1,1})}\;.\]

Similarly, if \(y_{1}=1\), we must have

\[q_{1}\geq-\frac{-\log_{4}A_{1,1}}{\log_{4}A_{1,1}}=1\;.\]

Since we do not know \(y_{1}\) before the decision, the only possibility is to take \(q_{1}=1\).

A similar reasoning applies to the case \(1/2<A_{1,1}\leq 3/4\). Therefore, the above relations determine the expression for \(q_{t}\) in the theorem, while enforcing the desired regret bound.

Proof of Theorem 2

As mentioned after Theorem 2, an analogous result holds for non-uniform weight initializations. We will state and prove this more general result below, from which Theorem 2 as stated in the main text follows as a special case.

**Theorem 2** (with non-uniform weight initialization).: _Consider an exponentially weighted forecaster with initial weight vector \(\mathbf{w}_{\cdot,0}=(w_{1,0},\ldots,w_{N,0})\) such that \(\sum_{i\in[N]}w_{i,0}=1\), \(p_{t}=A_{1,t}\) and_

\[q_{t}\geq q^{*}(A_{1,t},\eta):=q_{t}^{*}\.\]

_For this forecaster, we have_

\[\mathbb{E}\Big{[}L_{n}^{(\mathbf{w}_{\cdot,0})}\Big{]}\leq\mathbb{E}\bigg{[}\min_{ i\in[N]}\biggl{(}L_{i,n}-\frac{\ln w_{i,0}}{\eta}\biggr{)}\bigg{]}+\frac{n\eta}{8}. \tag{12}\]

_where the superscript in \(L_{n}^{(\mathbf{w}_{\cdot,0})}:=\sum_{t=1}^{n}\ell(\hat{y}_{t},y_{t})\) makes the dependence on the initial weights explicit. In particular, for the choice of initial weights \(w_{i,0}:=1/N\) for all \(i\in[N]\), we have_

\[\bar{R}_{n}=\mathbb{E}[L_{n}]-\mathbb{E}\bigg{[}\min_{i\in[N]}L_{i,n}\bigg{]} \leq\frac{\ln N}{\eta}+\frac{n\eta}{8}\.\]

Proof.: The proof strategy is similar to that used in Theorem 1. We begin by noting that at time \(t\), the internal state of the label-efficient exponentially weighted forecaster is determined by the weight vector \(\mathbf{w}_{\cdot,t-1}=(w_{1,t-1},\ldots,w_{N,t-1})\). Therefore, it suffices to focus on the requirements for \(q_{1}\) for an arbitrary weight vector. As in Theorem 1, we proceed by induction on \(n\).

For \(n=1\), the theorem statement is trivially true, as this algorithm coincides with the ordinary exponentially weighted forecaster (also, the right-hand-side of (12) is bounded from below by \(1/2\)). Proceeding by induction in \(n\), suppose (12) holds for \(1,\ldots,n-1\) outcomes. Let \(\bar{L}_{n|1}^{(\mathbf{w}_{\cdot,0})}\) denote the expected cumulative loss of the forecaster given \((y_{1},\{f_{i,1}\}_{i=1}^{N})\), i.e., the true label and the expert predictions for time \(t=1\):

\[\bar{L}_{n|1}^{(\mathbf{w}_{\cdot,0})}=\mathbb{E}\Big{[}L_{n}^{(\mathbf{w}_{\cdot,0})} \ \big{|}\ y_{1},\{f_{i,1}\}_{i=1}^{N}\Big{]}\.\]

Then, we have

\[\bar{L}_{n|1}^{(\mathbf{w}_{\cdot,0})} =\mathbb{E}\big{[}\ell(\hat{y}_{1},y_{1})\ |\ y_{1},\{f_{i,1}\}_{i=1}^ {N}\big{]}+\mathbb{E}\Bigg{[}\sum_{t=2}^{n}\ell(\hat{y}_{t},y_{t})\ \big{|}\ y_{1},\{f_{i,1}\}_{i=1}^{N}\Bigg{]}\] \[=A_{1,1}\!+\!(1\!-\!2A_{1,1})y_{1}+q_{1}\mathbb{E}\Bigg{[}\!\sum_ {t=2}^{n}\ell(\hat{y}_{t},y_{t})\,\big{|}\,Z_{1}\!=\!1,y_{1},\{f_{i,1}\}_{i=1} ^{N}\Bigg{]}\] \[\qquad+(1\!-\!q_{1})\mathbb{E}\Bigg{[}\!\sum_{t=2}^{n}\ell(\hat{y }_{t},y_{t})\,\big{|}\,Z_{1}\!=\!0,y_{1},\{f_{i,1}\}_{i=1}^{N}\Bigg{]}\] \[\leq A_{1,1}+(1-2A_{1,1})y_{1}+q_{1}\bar{L}_{n-1}^{(\mathbf{w}_{\cdot, 1})}+(1-q_{1})\bar{L}_{n-1}^{(\mathbf{w}_{\cdot,0})}\.\]

In order to prove the desired result, it is enough to show that

\[\bar{L}_{n|1}^{(\mathbf{w}_{\cdot,0})}\leq\mathbb{E}\bigg{[}\min_{i\in[N]}\biggl{(} \ell_{i,1}+L_{i,2:n}-\frac{\ln w_{i,0}}{\eta}\biggr{)}\bigg{]}+\frac{n\eta}{8}= \mathbb{E}\bigg{[}\biggl{(}\ell_{i^{\prime},1}+L_{i^{\prime},2:n}-\frac{\ln w _{i^{\prime},0}}{\eta}\biggr{)}\bigg{]}+\frac{n\eta}{8}\.\]

Here, we let \(L_{i,2:n}:=\sum_{t=2}^{n}\ell_{i,t}\) and let \(i^{\prime}\) denote the \(\arg\min\) of the right-hand side. Now, using the induction hypothesis, we obtain

\[\bar{L}_{n|1}^{(\mathbf{w}_{\cdot,0})} \leq A_{1,1}+(1-2A_{1,1})y_{1}+\frac{(n-1)\eta}{8}\] \[\qquad+\mathbb{E}\bigg{[}\min_{i\in[N]}\biggl{(}L_{i,2:n}+q_{1} \frac{-\ln w_{i,1}}{\eta}+(1-q_{1})\frac{-\ln w_{i,0}}{\eta}\biggr{)}\bigg{]}\] \[\leq A_{1,1}+(1-2A_{1,1})y_{1}+\frac{(n-1)\eta}{8}\] \[\qquad+\mathbb{E}\bigg{[}\biggl{(}L_{i^{\prime},2:n}+q_{1}\frac{- \ln w_{i^{\prime},1}}{\eta}+(1-q_{1})\frac{-\ln w_{i^{\prime},0}}{\eta}\biggr{)} \bigg{]}\.\]In the last step, we used the fact that since the upper bound holds for the minimum \(i\), it holds for \(i^{\prime}\) in particular. To ensure that the bound in the theorem holds, it is thus sufficient to select \(A_{1,1}\) such that it satisfies

\[A_{1,1}+(1-2A_{1,1})y_{1}+\left(\frac{q_{1}}{\eta}\left(\ln w_{i^{\prime},0}- \ln w_{i^{\prime},1}\right)-\ell_{i^{\prime},1}\right)\leq\frac{\eta}{8}\;.\]

Notice that, after the first observation, we are back in a situation similar to that at time \(t=1\), but possibly with a different weight vector. Specifically, for \(i\in[N]\),

\[w_{i,1}=\frac{w_{i,0}e^{-\eta\ell_{i,1}/q_{1}}}{\sum_{i=1}^{N}w_{i,0}e^{-\eta \ell_{i,1}/q_{1}}}\;.\]

It is important at this point that \(w_{i,1}\) depends on the choice \(q_{1}\), so we cannot simply solve the above equation for \(q_{1}\). To proceed, it is easier to consider the two possible values of \(y_{1}\) separately.

Case \(y_{1}=0\):Note that

\[w_{i,1}=\frac{w_{i,0}e^{-\eta\ell(f_{i,1},y_{1})/q_{1}}}{1-A_{1,1}+A_{1,1}e^{ -\eta/q_{1}}}\;.\]

Therefore, it suffices to have

\[A_{1,1}+\frac{q_{1}}{\eta}\ln\left(1-A_{1,1}+A_{1,1}e^{-\eta/q_{1}}\right)\leq \frac{\eta}{8}\;. \tag{13}\]

Case \(y_{1}=1\):Similarly as above,

\[w_{i,1}=\frac{w_{i,0}e^{-\eta\ell(f_{i,1},y_{1})/q_{1}}}{A_{1,1}+(1-A_{1,1})e^{ -\eta/q_{1}}}\;.\]

Therefore, it suffices to have

\[1-A_{1,1}+\frac{q_{1}}{\eta}\ln\left(A_{1,1}+(1-A_{1,1})e^{-\eta/q_{1}}\right) \leq\frac{\eta}{8}\;. \tag{14}\]

As we do not know the values of \(y_{1}\) when computing \(q_{1}\), we must simultaneously satisfy (13) and (14). Nevertheless, these two conditions involve only \(\eta\) and \(A_{1,1}\). Thus, we can identify the range of values that \(q_{1}\) can take, as a function of \(\eta\) and \(A_{1,1}\), while still ensuring the desired regret bound. Specifically, we require that \(q_{1}\geq q_{1}^{*}\), where

\[q_{1}^{*}:=q_{1}^{*}(A_{1,1},\eta)=\inf\left\{q\in(0,1]:A_{1,1}+ \frac{q}{\eta}\ln\left(1-A_{1,1}+A_{1,1}e^{-\eta/q}\right)\leq\frac{\eta}{8},\right.\] \[\left.1-A_{1,1}+\frac{q}{\eta}\ln\left(A_{1,1}+(1-A_{1,1})e^{-\eta /q}\right)\leq\frac{\eta}{8}\right\}\;.\]

At this point, it might be unclear if \(q_{1}^{*}\) is well defined, namely, if there always exists \(q\in[0,1]\) satisfying both (13) and (14). This is indeed the case, as shown in Lemma 1. By noting that \(\mathbb{E}[\bar{L}_{i,n|1}^{(\mathbf{w}_{i,0})}]=\bar{L}_{i,n}^{(\mathbf{w}_{i,0})}\), we have completed the induction step.

As stated at the beginning of the proof, looking at \(q_{1}\) suffices to determine the general requirements for \(q_{t}\), concluding the proof. The statement given in (7) follows from instantiating the general result with uniform initial weights. 

## Appendix D Proof of Lemma 1

**Lemma 1**.: _For all \(\eta>0\) and \(x\in[0,1]\), we have_

\[1\in\left\{q\in(0,1]:x+\frac{q}{\eta}\ln\left(1-x+xe^{-\eta/q}\right)\leq\frac{ \eta}{8}\right\}\;.\]

Proof.: Let \(B\sim\text{Ber}(x)\), with \(x\in[0,1]\). Note that \(\mathbb{E}[e^{-\eta B}]=(1-x)+xe^{-\eta}\). Note also that, by [3, Lem. A.1] we have \(\ln\mathbb{E}[e^{-\eta B}]\leq-\eta x+\eta^{2}/8\). Putting these two facts together we conclude that

\[x+\frac{1}{\eta}\ln\left(1-x+xe^{-\eta}\right)\leq x-x+\eta/8=\eta/8\;.\]

Therefore, the point \(1\) is always contained in Definition 1, concluding the proof.

## Appendix E Proof of Lemma 2

**Lemma 2**.: _For any \(x\in[0,1]\), we have_

\[\lim_{\eta\to 0}q^{*}(x,\eta)=4x(1-x)\;.\]

_Furthermore, for any \(\eta>0\) and \(x\in[0,1]\),_

\[q^{*}(x,\eta)\leq\min(4x(1-x)+\eta/3,1)\;\;.\]

Proof.: As already shown in Lemma 1, we know that \(q^{*}(x,\eta)\leq 1\). Let \(\eta>0\) be arbitrary. Note that \(q^{*}(x,\eta)\) needs to be a solution in \(q\) of the following equation:

\[\underbrace{\eta x+q\ln(1-x+xe^{-\eta/q})}_{:=g(\eta)}\leq\frac{\eta^{2}}{8}\;. \tag{15}\]

Note that \(\lim_{\eta\to 0}g(\eta)=0\), so we can extend the definition of \(g\) to \(0\) by continuity. Specifically,

\[g(\eta):=\left\{\begin{array}{ll}\eta x+q\ln(1-x+xe^{-\eta/q})&\mbox{ if } \eta>0\\ 0&\mbox{ if }\eta=0\end{array}\right.\;.\]

We proceed by using a Taylor expansion of \(g(\eta)\) around \(0\). Tedious, but straightforward computations yields

\[g^{\prime}(\eta)=\frac{\partial}{\partial\eta}g(\eta)=x\left(1-\frac{1}{x+(1 -x)e^{\eta/q}}\right)\;;\quad g^{\prime\prime}(\eta)=\frac{1}{q}x(1-x)\frac{e^ {\eta/q}}{(x+(1-x)e^{\eta/q})^{2}}\;,\]

and

\[g^{\prime\prime\prime}(\xi)=\frac{1}{q^{2}}(-\tau+3\tau^{2}-2\tau^{3})\;,\; \mbox{with}\;\tau=\frac{xe^{-\xi/q}}{1-x+xe^{-\xi/q}}\;,\]

where \(\xi>0\). In conclusion,

\[g(\eta) =g(0)+g^{\prime}(0)\eta+g^{\prime\prime}(0)\frac{\eta^{2}}{2}+g^ {\prime\prime\prime}(\xi)\frac{\eta^{3}}{6}\] \[=\frac{1}{q}x(1-x)\frac{\eta^{2}}{2}+g^{\prime\prime\prime}(\xi) \frac{\eta^{3}}{6}\;.\]

where \(\xi\in[0,\eta]\). At this point we can examine the structure of the solution of (15) when \(\eta\to 0\). Note that \(q^{*}(x,\eta)\) necessarily satisfies

\[g(\eta)=\frac{1}{q^{*}(x,\eta)}x(1-x)\frac{\eta^{2}}{2}+o(\eta^{2})=\frac{\eta ^{2}}{8}\;,\]

implying that \(q^{*}(x,\eta)\to 4x(1-x)\) as \(\eta\to 0\), proving the first statement in the lemma.

For the second statement in the lemma, one needs to more carefully control the error term \(g^{\prime\prime\prime}(\xi)\). We begin by noting that \(\tau\in[0,x]\). This implies that \(-\tau+3\tau^{2}-2\tau^{3}\leq x(1-x)\) (this can be checked by algebraic manipulation1). Therefore, \(q^{*}(x,\eta)\leq q\), where \(q\) is the solution of

Footnote 1: It suffices to check that the solutions in \(\tau\) of \(-1+3\tau-2\tau^{2}\leq 1-x\), if they exist, satisfy \(\tau\in[0,x]\).

\[\frac{1}{q}x(1-x)\frac{\eta^{2}}{2}+\frac{1}{q^{2}}x(1-x)\frac{\eta^{3}}{6}= \frac{\eta^{2}}{8}\;.\]

This is a simple quadratic equation in \(q\), yielding the solution

\[q=2x(1-x)+\sqrt{(2x(1-x))^{2}+\frac{4}{3}x(1-x)\eta}\;.\]

Although the above expression is a valid upper bound on \(q^{*}(x,\eta)\), it is not a very convenient one. A more convenient upper bound can be obtained by noting that

\[2x(1-x)+\sqrt{(2x(1-x))^{2}+\frac{4}{3}x(1-x)\eta}\leq 4x(1-x)+ \eta/3\] \[\iff(2x(1-x))^{2}+\frac{4}{3}x(1-x)\eta\leq(2x(1-x)+\eta/3)^{2}\] \[\iff\frac{4}{3}x(1-x)\eta\leq\frac{4}{3}x(1-x)+\frac{\eta^{2}}{9}\;.\]

Thus, we have \(q\leq 4x(1-x)+\eta/3\), concluding the proof. 

[MISSING_PAGE_FAIL:18]

denote the weighted proportion of experts that agrees with the best expert at time \(t\). Then

\[A^{*}_{t+1}\geq\frac{w_{i^{*},t}}{\sum_{i=1}^{N}w_{i,t}}=\frac{1}{1+\sum_{i\neq i^ {*}}e^{-\eta\tilde{\Lambda}^{i}_{t}}}\geq\frac{1}{1+Ne^{-\eta\tilde{\Lambda}^{ \min}_{t}}}\;.\]

Note that \(A_{1,t}\in\{A^{*}_{t+1},1-A^{*}_{t+1}\}\). Consequently,

\[q_{t+1}\leq 4A^{*}_{t+1}(1-A^{*}_{t+1})+\frac{\eta}{3}\leq 4(1-A^{*}_{t+1})+ \frac{\eta}{3}\leq\frac{4}{1+e^{\eta\tilde{\Lambda}^{\min}_{t}}/N}+\frac{\eta} {3}\;.\]

The desired condition from (18) is therefore satisfied if, for all \(i\neq i^{*}\),

\[\tilde{\Lambda}^{i}_{t}\geq\frac{1}{\eta}\ln\!\left(\frac{N}{\eta}\right)\qquad \text{for all }t=\tau,\ldots,n-1. \tag{20}\]

To study the evolution of \(\tilde{\Lambda}^{i}_{t}\), we use a martingale argument. Consider any fixed \(i\neq i^{*}\), and define the martingale difference sequence

\[X_{t}=-(\tilde{\ell}_{i,t}-\tilde{\ell}_{i^{*},t})+\mathbb{E}_{t}[\tilde{\ell }_{i,t}-\tilde{\ell}_{i^{*},t}]=-\frac{(\ell_{i,t}-\ell_{i^{*},t})Z_{t}}{q_{t} }+\mathbb{E}_{t}[\ell_{i,t}-\ell_{i^{*},t}]\;.\]

Without loss of generality, we may assume that \(\eta\leq 3\), because otherwise (19) holds trivially for any pair of \(\tau\) and \(\delta_{1}\). Then \(q_{t}\geq\eta/3\), \(|X_{t}|\leq 2/q_{t}\leq 6/\eta\), and

\[\mathbb{E}[X^{2}_{t}|\mathcal{F}_{t-1}]\leq\mathbb{E}\Big{[}(\tilde{\ell}_{i,t }-\tilde{\ell}_{i^{*},t})^{2}|\mathcal{F}_{t-1}\Big{]}=\mathbb{E}\Big{[}\frac{ (\ell_{i,t}-\ell_{i^{*},t})^{2}}{q_{t}}|\mathcal{F}_{t-1}\Big{]}\leq\frac{3}{ \eta}\;.\]

Hence, by Lemma 3, we have

\[\tilde{\Lambda}^{i}_{t}\geq\sum_{j=1}^{t}\mathbb{E}_{t}[\ell_{i,t}-\ell_{i^{*},t}]-\max\big{\{}4\sqrt{\frac{3t}{\eta}\ln(1/\delta_{2})},\,\frac{12}{\eta}\ln (1/\delta_{2})\big{\}}\qquad\text{for all }t\in[n]\;. \tag{21}\]

Using Assumption (17), it follows that

\[\tilde{\Lambda}^{i}_{t}\geq t\Delta-\max\big{\{}4\sqrt{\frac{3t}{\eta}\ln(1/ \delta_{2})},\frac{12}{\eta}\ln(1/\delta_{2})\big{\}}\qquad\text{for all }t \geq\tau^{*} \tag{22}\]

with probability at least \(1-(\ln(n)\delta_{2})\) for any \(\delta_{2}\in(0,1/e]\). By taking \(\delta_{2}=\min\{\delta_{1}/(N\ln n),1/e\}\) and applying the union bound, we can make (22) hold for all \(i\neq i^{*}\) simultaneously with probability at least \(1-\delta_{1}\). A sufficient condition for (22) to imply (20) is then to take

\[\tau=\left\lceil\frac{48\ln(1/\delta_{2})}{\eta\Delta^{2}}+\frac{2}{\eta \Delta}\ln\Big{(}\frac{N}{\eta}\Big{)}\right\rceil\;,\]

matching the definition in (16). We prove this claim in Lemma 4 below. Note that if our choice of \(\tau>n\), then (19) still holds trivially, because \(S_{n}\leq n\). Evaluating (19) with the given choice of \(\tau\) and taking \(\delta_{1}=\eta\) (assuming \(\eta<1\), since (19) holds trivially if \(\delta_{1}\geq 1\)), we arrive at the following bound:

\[\mathbb{E}[S_{n}] \leq\left\lceil\frac{48\ln(1/\delta_{2})}{\eta\Delta^{2}}+\frac{2 }{\eta\Delta}\ln\Big{(}\frac{N}{\eta}\Big{)}\right\rceil+\tfrac{4}{3}\eta n+ n\delta_{1}\] \[\leq\frac{48\ln(N\ln(n)/\delta_{1})}{\eta\Delta^{2}}+\frac{2}{ \eta\Delta}\ln\Big{(}\frac{N}{\eta}\Big{)}+\tfrac{4}{3}\eta n+n\delta_{1}+1\] \[=\frac{48\ln(N\ln(n)/\eta)}{\eta\Delta^{2}}+\frac{2}{\eta\Delta} \ln\Big{(}\frac{N}{\eta}\Big{)}+\tfrac{4}{3}\eta n+\eta n+1\] \[\leq\frac{50\ln(N\ln(n)/\eta)}{\eta\Delta^{2}}+3\eta n+1\;,\]

thus completing the proof.

**Lemma 4**.: _Assume that_

\[\tilde{\Lambda}_{t}^{i}\geq t\Delta-\max\big{\{}4\sqrt{\frac{3t}{\eta}\ln(1/\delta_ {2})},\frac{12}{\eta}\ln(1/\delta_{2})\big{\}}\quad\text{for all}\quad t\leq n\;.\]

_Then, with_

\[\tau=\left\lceil\frac{48\ln(1/\delta_{2})}{\eta\Delta^{2}}+\frac{2}{\eta\Delta }\ln\Big{(}\frac{N}{\eta}\Big{)}\right\rceil\;,\]

_we have_

\[\tilde{\Lambda}_{t}^{i}\geq\frac{1}{\eta}\ln\!\left(\frac{N}{\eta}\right)\quad \text{for all}\quad t=\tau,\ldots,n-1\;.\]

Proof.: We will consider the two possible outcomes of the maximum separately. First, we consider the case where the first term is the maximum. Then, we need to find \(\tau\) such that for \(t\geq\tau\),

\[\tilde{\Lambda}_{t}^{i}\geq t\Delta-4\sqrt{\frac{3t}{\eta}\ln(1/\delta_{2})}- \frac{1}{\eta}\ln\!\left(\frac{N}{\eta}\right)\geq 0\;.\]

A straightforward calculation shows that this is satisfied for

\[t\geq\frac{\left(\sqrt{48\ln(1/\delta_{2})+4\Delta\ln(N/\eta)}+4\sqrt{3\Delta \ln(1/\delta_{2})}\;\right)^{2}}{4\eta\Delta^{2}}\;.\]

Since \((a+b)^{2}\leq 2a^{2}+2b^{2}\) for \(a,b>0\), this is satisfied given the simpler condition

\[t\geq\left\lceil\frac{48}{\eta\Delta^{2}}\ln(1/\delta_{2})+\frac{2}{\eta \Delta}\ln\!\left(\frac{N}{\eta}\right)\right\rceil=\tau\;.\]

Next, we assume that the second term is the maximum. Then, we have

\[\tilde{\Lambda}_{t}^{i} \geq t\Delta-\frac{12}{\eta}\ln(1/\delta_{2})\] \[\geq\tau\Delta-\frac{12}{\eta}\ln(1/\delta_{2})\] \[=\left\lceil\frac{48\ln(1/\delta_{2})}{\eta\Delta^{2}}+\frac{2}{ \eta\Delta}\ln\Big{(}\frac{N}{\eta}\Big{)}\right\rceil\Delta-\frac{12}{\eta} \ln(1/\delta_{2})\] \[\geq\frac{48\ln(1/\delta_{2})}{\eta\Delta}+\frac{2}{\eta}\ln\Big{(} \frac{N}{\eta}\Big{)}-\frac{12\ln(1/\delta_{2})}{\eta\Delta}\] \[\geq\frac{2}{\eta}\ln\Big{(}\frac{N}{\eta}\Big{)}\;,\]

where we used the assumption that \(t\geq\tau\). Thus, for this case, the desired condition holds with the specified \(\tau\). Therefore, with the specified \(\tau\), the desired statement holds for \(t=\tau,\ldots,n-1\). 

## Appendix G Experimental details

In this section, we describe the simulation study in Section 6 in detail. The full code, which can be executed in less than one hour on an M1 processor, is provided in the supplementary material.

We consider a sequential prediction problem with \(n=50000\) total rounds and \(N=225\) experts. The number of experts is chosen to be higher than \(\sqrt{n}\) and odd. Then, we repeat the following simulation for \(500\) independent runs. First, we generate \(n\) independent features \(\{X_{i}\}_{i\in[n]}\) from the uniform distribution on \([0,1]\). Then, for each feature \(X_{i}\), we randomly generate a label \(Y_{i}\), where the probability \(\mathbb{P}(Y_{i}=1|X_{i}=x)=\zeta(x)\), where

\[\zeta(x)=\frac{1}{2}+\frac{1}{2}\text{sign}(x-1/2)|x-1/2|^{\kappa-1}\;.\]

Note that the optimal prediction of the label is simply \(1\;\{x\geq 1/2\}\). We run simulations both for \(\kappa=1.5\) and \(\kappa=2\).

We set the experts to be threshold classifiers, with thresholds uniformly spaced across \([0,1]\). Specifically, for all \(i\in[N]\) and \(t\in[n]\),

\[f_{i,t}=\mathbb{1}\left\{X_{t}\geq\frac{i-1}{N-1}\right\}\;.\]

In order to optimize the regret bound in (7), we set \(\eta=\sqrt{8\ln(N)/n}\). For each time \(t\in[n]\), we consider two different weight vectors: \(\{w^{P}_{i,t}\}_{i\in[N]}\), corresponding to the passive, full-information forecaster, and \(\{w^{A}_{i,t}\}_{i\in[N]}\), corresponding to the active, label-efficient forecaster. The weight for each expert for both forecasters is uniformly initialized as \(w^{P}_{i,0}=w^{A}_{i,0}=1/N\).

Then, for each timestep \(t\in[n]\), we proceed as follows. First, we compute the probability of prediction the label \(1\) for each forecaster, given by \(p^{P}_{t}\) for the full-information forecaster and \(p^{A}_{t}\) for the label-efficient forecaster, as

\[p^{P}_{t}=A^{P}_{1,t}=\frac{\sum_{i=1}^{N}w^{P}_{i,t-1}f_{i,t}}{\sum_{i=1}^{N} w^{P}_{i,t-1}}\;,\qquad p^{A}_{t}=A^{A}_{1,t}=\frac{\sum_{i=1}^{N}w^{A}_{i,t-1}f _{i,t}}{\sum_{i=1}^{N}w^{A}_{i,t-1}}\;,\]

where the expert predictions are computed based on \(X_{t}\). Here, the weighted proportion of experts that predict the label \(1\) are given by \(A^{P}_{1,t}\) for the full-information forecaster and \(A^{A}_{1,t}\) for the label-efficient forecaster. On the basis of this, each forecaster issues a prediction for the label, given by \(\hat{y}^{P}_{t}\sim\text{Ber}(p^{P}_{t})\) for the full-information forecaster and \(\hat{y}^{A}_{t}\sim\text{Ber}(p^{A}_{t})\) for the label-efficient one.

Finally, the weights for each forecaster are updated as follows. For the full-information forecaster, the weight assigned to each expert is updated as

\[w^{P}_{i,t}=w^{P}_{i,t-1}\;e^{-\eta\ell_{i,t}}\;, \tag{23}\]

where \(\ell_{i,t}=\mathbb{1}\left\{f_{i,t}\neq Y_{t}\right\}\). In order to determine whether the label-efficient forecaster observes a label or not, we compute \(q_{t}=\min(4A^{A}_{1,t}(1-A^{A}_{1,t})-\eta/3,1)\) and generate \(Z_{t}\sim\text{Ber}(q_{t})\). Then, the weights of the label-efficient forecaster are updated as

\[w^{A}_{i,t}=w^{A}_{i,t-1}\;e^{-\eta\frac{\ell_{i,t}Z_{t}}{q_{t}}}\;. \tag{24}\]

Hence, if \(Z_{t}=0\), the weights of the label-efficient forecaster are not updated, since the label is not observed.

After all \(n\) rounds have been completed, we compute the regret of the full-information forecaster \(R^{P}_{n}\) and the label-efficient forecaster \(R^{A}_{n}\) as

\[R^{P}_{n}=\sum_{t=1}^{n}\mathbb{1}\left\{\hat{y}^{P}_{t}\neq Y_{t}\right\}- \mathbb{1}\left\{y^{*}_{t}\neq Y_{t}\right\}\;,\qquad R^{A}_{n}\quad=\sum_{t= 1}^{n}\mathbb{1}\left\{\hat{y}^{A}_{t}\neq Y_{t}\right\}-\mathbb{1}\left\{y^{ *}_{t}\neq Y_{t}\right\}\;.\]

Here, for each \(t\in[n]\), the optimal prediction is given by \(y^{*}_{t}=\mathbb{1}\left\{X_{t}\geq 1/2\right\}\). Furthermore, we compute the label complexity \(S_{n}\) as

\[S_{n}=\sum_{t=1}^{n}Z_{t}\;.\]

The results of these simulations are presented in Figure 2 and 3.