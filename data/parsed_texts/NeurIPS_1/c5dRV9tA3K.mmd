# Emma-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual Representation Learning

 Ping Guo\({}^{\lx@sectionsign,\lx@sectionsign}\) Xiangpeng Wei\({}^{\dagger,*}\) Yue Hu\({}^{\lx@sectionsign,\lx@sectionsign,*}\) Baosong Yang\({}^{\dagger}\) Dayiheng Liu\({}^{\dagger}\) Fei Huang\({}^{\dagger}\) Jun Xie\({}^{\dagger}\)

\({}^{\ddagger}\)Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China

\({}^{\lx@sectionsign}\)School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China

\({}^{\dagger}\)Machine Intelligence Technology Lab, Alibaba DAMO Academy, Hangzhou, China

{guoping, huyue}@iie.ac.cn, pemywei@gmail.com

Corresponding Author.

###### Abstract

Expressing universal semantics common to all languages is helpful in understanding the meanings of complex and culture-specific sentences. The research theme underlying this scenario focuses on learning universal representations across languages with the usage of massive parallel corpora. However, due to the sparsity and scarcity of parallel data, there is still a big challenge in learning authentic "universals" for any two languages. In this paper, we propose Emma-X: an **EM**-like **M**ultilingual pre-training **A**lgorithm, to learn **(X)**Cross-lingual universals with the aid of excessive multilingual non-parallel data. Emma-X unifies the cross-lingual representation learning task and an extra semantic relation prediction task within an EM framework. Both the extra semantic classifier and the cross-lingual sentence encoder approximate the semantic relation of two sentences, and supervise each other until convergence. To evaluate Emma-X, we conduct experiments on **xrete**, a newly introduced benchmark containing 12 widely studied cross-lingual tasks that fully depend on sentence-level representations. Results reveal that Emma-X achieves state-of-the-art performance. Further geometric analysis of the built representation space with three requirements demonstrates the superiority of Emma-X over advanced models 2.

Footnote 2: Codes and datasets of the xrete benchmark: [https://github.com/guopingiie/EMMA-X](https://github.com/guopingiie/EMMA-X)

## 1 Introduction

Research on how to express universal semantics for natural languages (metaphorically as "alphabet of human thoughts" by Leibniz and von Leibniz [1996]) has lasted a long time. Usually, these universal meanings underlying all human natural languages are referred to as irreducible semantic cores [20]. These common cores across languages can serve as a bridge, to help better understand the exact meanings of complex sentences in different languages.

In the context of computational linguistics, various works [14, 15, 16, 17, 18, 19, 21, 22, 23] have led to great improvements on learning cross-lingual universal representations with the usage of parallel corpora, and verify that multilingual universality contributes a major performance on cross-lingual understanding. However, due to the sparsity and scarcity of parallel data, these advanced techniques face a big challenge in learning real universality among all languages. For instance, among the widely-available top 100 languages that theoretically can build 4950 language pairs, only about 200 language pairs have considerable parallel data [1, 17, 16].

2022). Recently, Large Language Models (Llms) (e.g., DaLM (Chowdhery et al., 2022), OPT (Zhang et al., 2022), BLOOMZ (Workshop et al., 2023), ChatGPT, etc.) have reached a milestone in the field of Natural Language Processing, for their promising capability at understanding and following complex natural language instructions in different languages. By modeling a wide variety of sentence samples in discrete sentence space, Llms can capture some universal linguistic phenomena to gain cross-lingual transferability. This is consistent with our goal of building a universal basement that supports all languages. The difference lies in that we achieve it through learning universal continuous representations across different languages.

Concretely, we propose Emma-X to tackle the above challenge from a continuous perspective. Emma-X can learn cross-lingual universal sentence representations with excessive non-parallel multilingual data by unifying two highly dependent tasks in an EM (Moon, 1996) framework: semantic relation classification and cross-lingual universal representation learning. For the former, we introduce a Gaussian Mixture Model (Everitt and Hand, 1981) classifier (**GMM classifier**) to deal with the key challenge of forming positive sentence pairs for non-parallel multilingual corpora, by annotating the semantic relationship of sentence pairs in any two arbitrary languages on the fly. For the latter, we employ **a cross-lingual encoder** to learn universal sentence representations via contrastive learning, where positive pairs are chosen by GMM classifier. Further, we construct training signals according to the output of the cross-lingual encoder, to inversely supervise GMM classifier. From the perspective of EM algorithm, in E-step, both modules try to approximate the semantic relationship given a sentence pair sampled from two arbitrary languages. One module is supervised by the approximation of the other to build its own expectation. In M-step, two modules update their parameters by maximizing expectations, respectively. We give a theoretical justification about how these two tasks can be interpreted from an EM perspective (Section 4).

To incentivize the research of universal sentence representation learning, we form a Cross-lingual REpresentation Transfer Evaluation (**x****rete**) benchmark, which includes 12 cross-lingual tasks covering more than 50 languages. xrete fully depends on sentence-level representations. Experimental results demonstrate that Emma-X significantly outperforms pre-trained language models (Conneau et al., 2020; Chi et al., 2021) by 32% at most on xrete. We also perform an evaluation of ChatGPT on xrete to explore its multilingual performance. Detailed analysis also shows that Emma-X can mitigate the representation discrepancy between head and massive long-tail languages. We further conduct geometric analysis directly on representation space from three perspectives: _Invariance_(Abend and Rappoport, 2017), _Canonical Form_(Teller, 2000) and _Isotropy_(Mu and Viswanath, 2018), which provides a further understanding of the cross-lingual transferability of these models.

## 2 Preliminaries

Cross-lingual representation learning aims at mapping sentences from different languages into a unified continuous space, where synonyms across different languages are pulled closer. Given a sentence \(\mathbf{x}\), the representation is formulated as

\[\boldsymbol{\gamma}^{(\mathbf{x})}=f\big{[}g\big{(}\mathcal{M}(\mathbf{x}; \Theta_{\mathcal{M}})\big{)}\big{]}, \tag{1}\]

where \(\mathcal{M}(\cdot;\Theta_{\mathcal{M}})\) denotes the encoder network with a set of trainable parameters \(\Theta_{\mathcal{M}}\), which is typically implemented as a transformer encoder architecture (Vaswani et al., 2017; Devlin et al., 2019; Lee et al., 2022; Feng et al., 2022). \(f(\cdot)\) is L-2 normalization and \(g(\cdot)\) is the aggregate function. We take the final hidden states of "[CLS]" token as the aggregate sentence representation.

To learn reasonable representations that can express universal semantics across different languages, various well-designed techniques have been applied to \(\gamma^{(\mathbf{x})}\). A predominant one is to build contrastive learning (CTL) (Saunshi et al., 2019) objective with parallel corpora. The basic idea is to maximize the similarity between representations (i.e., \(\boldsymbol{\gamma}^{(\mathbf{x})}\) and \(\boldsymbol{\gamma}^{(\mathbf{y})}\)) of two semantically-equivalent sentences \((\mathbf{x},\mathbf{y})\), while keep randomly sampled irrelevant ones \(\boldsymbol{\gamma}^{(\mathbf{y}^{\prime})}\) away. Formally, assume \(\mathcal{B}\) to be a batch of multilingual parallel bitexts, the contrastive loss under InfoNCE (Oord et al., 2018) formulation is

\[\mathcal{L}_{\mathbf{CTL}}=-\log\frac{e^{s(\gamma^{(\mathbf{x})},\gamma^{( \mathbf{y}^{\prime})})}}{e^{s(\gamma^{(\mathbf{x})},\gamma^{(\mathbf{y})})}+ \sum_{\mathbf{y}^{\prime}\in\mathcal{B},\mathbf{y}^{\prime}\neq\mathbf{y}}e^{ s(\gamma^{(\mathbf{x})},\gamma^{(\mathbf{y}^{\prime})})}}, \tag{2}\]

where \(s(\cdot)\) is implemented as the cosine similarity \(s(\gamma^{(\mathbf{x})},\gamma^{(\mathbf{y}^{\prime})})=\frac{\gamma^{(\mathbf{x })\top}\gamma^{(\mathbf{y})}}{\|\gamma^{(\mathbf{x})}\|\cdot\|\gamma^{(\mathbf{ y})}\|}\), \(\mathbf{y}\) and \(\mathbf{y}^{\prime}\) are typically called positive and negative samples.

## 3 Methodology

We propose Emma-X that fully exploits massive monolingual data to learn cross-lingual universal representations. As illustrated in Figure 1, Emma-X consists of two modules: 1) A GMM classifier \(\mathcal{G}(\cdot;\Theta_{\mathcal{G}})\) to approximate the semantic relation of non-parallel sentences. 2) A cross-lingual encoder \(\mathcal{M}(\cdot;\Theta_{\mathcal{M}})\) to convert multilingual sentences into universal representations. For optimization, Emma-X unifies these two modules in an EM framework with dual supervision. In this section, we begin with a definition of the semantic relation rank (SS3.1). Then, we introduce model initialization (SS3.2) and the proposed training paradigm (SS3.3), followed by a dual supervision strategy (SS3.4). For a clearer presentation, an Algorithm of Emma-X is shown in Algorithm 1.

### Rank of Semantic Relations

Mainstream methods model semantic relations with a strict binary separation: positives and negatives. However, the boundary between positives and negatives is blurry, and many samples cannot be clearly classified as either positives or negatives. So it cannot maximize the potential of models to perceive more subtle semantic changes. Also, a binary separation will lead to far more negative samples than positive ones (imbalanced data). To more accurately capture the semantic relation between two sentences and alleviate imbalanced problem, we subdivide the relation into \(N\) semantic ranks, where the semantic similarity of each rank decreases as \(N\) increases, e.g., \(c=1\) denotes two sentences are paraphrases of each other, while \(c=N\) implies sentences are irrelevant. In practice, we set \(N\) to 4.

### Model Initialization

In Emma-X, the GMM classifier \(\mathcal{G}(\cdot;\Theta_{\mathcal{G}})\) and cross-lingual encoder \(\mathcal{M}(\cdot;\Theta_{\mathcal{M}})\) are initialized by training with massive parallel corpora, respectively.

**Initialization of Cross-lingual Encoder.** It is initialized with Xlm-r (Conneau et al., 2020) and then continuously trained with InfoNCE (Oord et al., 2018) loss by Eq.2. Following Hictl(Wei et al., 2021) and InfoXLM (Chi et al., 2021), we treat the parallel sentence pairs as the query sentence \(\mathbf{x}\) and its positive counterpart \(\mathbf{y}\), while treating the randomly sampled sentence as a negative one \(\mathbf{y}^{\prime}\).

**Initialization of GMM Classifier.** A reasonable solution to warm up the GMM classifier is to use the available cross-lingual parallel corpora as training signals. Suppose \(\mathbf{x}\) and \(\mathbf{y}\) are parallel sentences,

Figure 1: Illustration of Emma-X, involving two modules (GMM classifier and Cross-lingual Encoder) that are reciprocated from each other and are updated alternatively. \(\mathbf{x}\) means the current instance, \(\{\mathbf{y}_{1},\mathbf{y}_{2},\mathbf{y}_{3},...\}\) are samples in various languages for comparison. \(\boldsymbol{\gamma}^{(\mathbf{x})}\) is the continuous representation given a discrete sentence \(\mathbf{x}\). \(c^{*}_{\mathcal{M}}\) and \(c^{*}_{\mathcal{G}}\) formulate the semantic ranks approximated according to Eq. 10 and Eq. 9, to supervise the GMM classifier and cross-lingual encoder, respectively.

and \(\mathbf{y}^{\prime}\) is an outlier. We set the semantic ranks for \((\gamma^{(\mathbf{x})},\gamma^{(\mathbf{y})})\) and \((\gamma^{(\mathbf{x})},\gamma^{(\mathbf{y}^{\prime})})\) as \(c=1\) and \(c=N\), respectively, according to the definition described in SS 3.1. To obtain the fine-grained semantic ranks, we design a linear interpolation strategy similar to Wei et al. (2022) and mixup (Zhang et al., 2018), which provides virtual training examples for each semantic rank. Formally,

\[\boldsymbol{\gamma}^{(\mathcal{G})}=(1-\lambda)\cdot\boldsymbol{\gamma}^{( \mathbf{y})}+\lambda\cdot\boldsymbol{\gamma}^{(\mathbf{y}^{\prime})}, \tag{3}\]

where \(\lambda\in[0,1]\) is sampled from a uniform distribution. We compute \(r=\lceil(1-\lambda)\cdot(c=1)+\lambda\cdot(c=N)\rceil\) as the soft semantic rank for \((\gamma^{(\mathbf{x})},\gamma^{(\mathcal{G})})\), where \(\lceil\cdot\rceil\) means the least integer greater than or equal to the input. The virtual training examples are grouped together with the real parallel corpora to optimize the GMM classifier:

\[\mathcal{L}_{\mathbf{MLE}}=-\log P_{\mathcal{G}}(c=1|\mathbf{x},\mathbf{y}; \Theta_{\mathcal{G}})-\log P_{\mathcal{G}}(c=N|\mathbf{x},\mathbf{y}^{\prime}; \Theta_{\mathcal{G}})-\sum_{\bar{\mathbf{y}}}\log P_{\mathcal{G}}(c=r| \mathbf{x},\bar{\mathbf{y}};\Theta_{\mathcal{G}}). \tag{4}\]

The posterior probability \(P_{\mathcal{G}}(\cdot)\) is formulated as

\[P_{\mathcal{G}}(c=r|\mathbf{x},\mathbf{y};\Theta_{\mathcal{G}})=\frac{\pi_{r} \cdot\mathcal{N}_{r}\big{(}\gamma^{(\mathbf{x})}-\gamma^{(\mathcal{Y})}|\mu_{ r},\sigma_{r}\big{)}}{\sum_{j=1}^{N}\pi_{j}\cdot\mathcal{N}_{j}\big{(}\gamma^{( \mathbf{x})}-\gamma^{(\mathcal{Y})}|\mu_{j},\sigma_{j}\big{)}}, \tag{5}\]

where the Gaussian form \(\mathcal{N}_{r}\) is assigned with a prior probability \(\pi_{r}\), mean \(\mu_{r}\) and standard deviation \(\sigma_{r}\) that are all parameterized by trainable variables, thus \(\Theta_{\mathcal{G}}=\{(\pi_{r},\mu_{r},\sigma_{r})\mid r\in[1,N]\}\). See **Appendix G** for the specific calculation of Gaussian form \(\mathcal{N}_{r}\).

### The EM Iteration Framework

After initialization, Emma-X further trains the GMM classifier \(\mathcal{G}(\cdot;\Theta_{\mathcal{G}})\) and cross-lingual encoder \(\mathcal{M}(\cdot;\Theta_{\mathcal{M}})\) with only multilingual non-parallel data with an EM framework.

**E-Step.** For optimization, we represent a training batch of multilingual non-parallel sentences as \(\mathcal{X}=\{\mathbf{x}_{1},\mathbf{x}_{2},...,\mathbf{x}_{I}\}\) accompanied by a queue of random sentences as \(\mathcal{Y}=\{\mathbf{y}_{1},\mathbf{y}_{2},...,\mathbf{y}_{K}\}\) for instance comparison. Formally, the expectation for GMM classifier is:

\[\mathcal{L}_{\mathbf{MLE}}(\mathcal{X},\mathcal{Y};\Theta_{\mathcal{G}})=- \mathbb{E}_{\mathbf{x}_{i}\sim\mathcal{X}}\,\mathbb{E}_{\mathbf{y}_{k}\sim \mathcal{Y}}\big{[}\log P_{\mathcal{G}}(c=c^{*}_{\mathcal{M}}|\mathbf{x}_{i}, \mathbf{y}_{k};\Theta_{\mathcal{G}})\big{]}, \tag{6}\]

where \(c^{*}_{\mathcal{M}}\in[1,N]\) represents an approximated semantic rank for the combination of anchor \(\mathbf{x}_{i}\) and another random sentence \(\mathbf{y}_{k}\), based on the cosine similarity among representations (i.e., \(\boldsymbol{\gamma}^{(\mathbf{x}_{i})}\) and \(\boldsymbol{\gamma}^{(\mathbf{y}_{k})}\)) produced by the cross-lingual encoder (i.e., \(\mathcal{M}(\cdot;\Theta_{\mathcal{M}})\)). Please refer to SS3.4 for details.

Correspondingly, the expectation for the cross-lingual encoder can be calculated with contrastive learning, where the positive samples are established by the maximum a posteriori approximation (argmax prediction) \(c^{*}_{\mathcal{G}}\) given by the GMM classifier. Specifically, we apply ranking InfoNCE (Hoffmann et al., 2022) as the training objective, which recursively takes parallel sentence pairs in each rank (e.g, \(c^{*}_{\mathcal{G}}\)) as positives and ranks that are larger than \(c^{*}_{\mathcal{G}}\) as negatives. Formally,

\[\mathcal{L}_{\mathbf{CTL}}(\mathcal{X},\mathcal{Y};\Theta_{ \mathcal{M}})=-\mathbb{E}_{\mathbf{x}_{i}\sim\mathcal{X}}\Bigg{[}\log\frac{ \sum_{\mathbf{y}_{k}\sim\mathcal{Y}_{c^{*}_{\mathcal{G}}=1}}e^{s[\gamma^{( \mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})}]}}{\sum_{\mathbf{y}_{k}\sim \mathcal{Y}_{c^{*}_{\mathcal{G}}\in[1,N]}}e^{s[\gamma^{(\mathbf{x}_{i})}, \gamma^{(\mathbf{y}_{k})}]}}+\log\frac{\sum_{\mathbf{y}_{k}\sim\mathcal{Y}_{c^{* }_{\mathcal{G}}=2}}e^{s[\gamma^{(\mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})}]}}{ \sum_{\mathbf{y}_{k}\sim\mathcal{Y}_{c^{*}_{\mathcal{G}}\in[2,N]}}e^{s[\gamma^ {(\mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})}]}}\] \[+...+\log\frac{\sum_{\mathbf{y}_{k}\sim\mathcal{Y}_{c^{*}_{ \mathcal{G}}=N-1}}e^{s[\gamma^{(\mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})}]}}{ \sum_{\mathbf{y}_{k}\sim\mathcal{Y}_{c^{*}_{\mathcal{G}}\in[1-N,N]}}e^{s[\gamma ^{(\mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})}]}}\Bigg{]}, \tag{7}\]

where \(c^{*}_{\mathcal{G}}\in[1,N]\) represents a semantic rank approximated by the posteriori of GMM classifier (SS3.4). For simplicity, we omit the temperature term in Eq. 7, and please see **Appendix G** for details.

**M-Step.** We use gradient descent algorithm to update the parameters of each module by optimizing its expectation. At each time step \(t\), where \(\eta\) and \(\eta^{\prime}\) are learning rates for each expectation, formally,

\[\Theta^{t+1}_{\mathcal{G}} \leftarrow\Theta^{t}_{\mathcal{G}}-\eta\times\nabla_{\Theta_{ \mathcal{G}}}\mathcal{L}_{\mathbf{MLE}}(\mathcal{X},\mathcal{Y};\Theta_{ \mathcal{G}}), \tag{8}\] \[\Theta^{t+1}_{\mathcal{M}} \leftarrow\Theta^{t}_{\mathcal{M}}-\eta^{\prime}\times\nabla_{ \Theta_{\mathcal{M}}}\mathcal{L}_{\mathbf{CTL}}(\mathcal{X},\mathcal{Y};\Theta_{ \mathcal{M}}).\]```
Input: multilingual parallel and non-parallel corpora Output:\(\mathcal{M}(\cdot;\Theta_{\mathcal{M}})\) and \(\mathcal{G}(\cdot;\Theta_{\mathcal{G}})\)  Phase 1 ; /* Warm-up two modules with multilingual parallel corpora */ while not convergencedo  Sample a batch of multilingual bitexts \(\mathcal{B}\); for\((\mathbf{x},\mathbf{y})\in\mathcal{B}\)do  Sample \(\mathbf{y}^{\prime}\in\mathcal{B},\mathbf{y}^{\prime}\neq\mathbf{y}\), Build virtual training examples \(\tilde{\mathbf{y}}\) with Eq. 3;  Compute \(\mathcal{L}_{\mathbf{CTL}}\) with Eq. 2 to update \(\Theta_{\mathcal{M}}\) and \(\mathcal{L}_{\mathbf{MLE}}\) with Eq. 4 to update \(\Theta_{\mathcal{G}}\);  end while  Phase 2 ; /* Emma-X training with EM framework using only non-parallel corpora */ while not convergencedo E-Step  Sample a batch of multilingual anchors \(\mathcal{X}\), Queue a batch of random sentences \(\mathcal{Y}\); for\((\mathbf{x}_{i},\mathbf{y}_{k})\in\mathcal{X}\times\mathcal{Y}\)do  Approximate semantic rank \(c^{*}_{\mathcal{G}}\) with Eq. 9 and \(c^{*}_{\mathcal{M}}\) with Eq. 10 ;  end for  Compute \(\mathcal{L}_{\mathbf{MLE}}(\mathcal{X},\mathcal{Y};\Theta_{\mathcal{G}})\) according to Eq. 6 and \(\mathcal{L}_{\mathbf{CTL}}(\mathcal{X},\mathcal{Y};\Theta_{\mathcal{M}})\) according to Eq. 7; M-Step  Update \(\Theta_{\mathcal{M}}\) and \(\Theta_{\mathcal{G}}\) according to Eq. 8;  end while
```

**Algorithm 1**Emma-X Training Algorithm

### Dual Supervision

The approximated semantic ranks \(c^{*}_{\mathcal{G}}\) and \(c^{*}_{\mathcal{M}}\) are critical in Emma-X training algorithm. To preserve their quality, we propose dual supervision: predictions from one module are fed to the other to calculate the expectation. In this section, we explain in detail how we approximate the semantic ranks for GMM classifier and cross-lingual encoder, respectively.

**Approximate Semantic Rank with GMM classifier.** The way to obtain semantic rank with semantic classifier is straightforward. The semantic rank corresponding to the highest probability among multiple Gaussian distributions is chosen as the prediction, which is further used to supervise the cross-lingual encoder \(\mathcal{M}(\cdot;\Theta_{\mathcal{M}})\), as illustrated in Eq. 7. Formally,

\[c^{*}_{\mathcal{G}}=\operatorname*{argmax}_{r}P_{\mathcal{G}}(c=r|\mathbf{x}_{ i},\mathbf{y}_{k};\Theta_{\mathcal{G}}). \tag{9}\]

**Approximate Semantic Rank with Cross-lingual Encoder.** One common way to calculate sentence relation is to measure the similarity between two real-valued representations. Assuming \(s_{r}\) (a scalar initialized as \(\frac{r}{N}\)) can reflect the general similarity score in semantic rank \(c=r\). Given a random sentence pair \((\mathbf{x}_{i},\mathbf{y}_{k})\), it's similarity score is close to \(s_{r}\), the sentence pair is likely to belong to rank \(c=r\). Cross-lingual encoder \(\mathcal{M}(\cdot;\Theta_{\mathcal{M}})\) determines the semantic relation for each pair according to

\[c^{*}_{\mathcal{M}}=\operatorname*{argmin}_{r}|s(\gamma^{(\mathbf{x}_{i})}, \gamma^{(\mathbf{y}_{k})})-s_{r}|, \tag{10}\]

where \(|\cdot|\) refers to absolute value. Symmetrically, \(c^{*}_{\mathcal{M}}\) is used to supervise GMM classifier (Eq. 6).

During the training process, the general similarity score for each semantic rank may vary. Thus, we propose a moving-average strategy to adaptively adjust the value of \(s_{r}\) to simulate this change. Specifically, at time step \(t\), \(s_{r}\) is updated by cosine similarity of all the sentence pairs, which are currently categorized into the rank \(c=r\) according to the cross-lingual encoder in Eq. 9.

\[s^{t}_{r}\leftarrow\epsilon\cdot s^{t-1}_{r}+(1-\epsilon)\cdot s(\gamma^{( \mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})}),\qquad\text{if}\ \mathbf{y}_{k} \in\mathcal{Y}_{c^{*}_{\mathcal{G}}=r}. \tag{11}\]

Here \(\epsilon\in[0,1]\) is a momentum coefficient to make \(s_{n}\) evolve smoothly during training.

## 4 Theoretical Analysis

In this section, we provide theoretical justification for Emma-X and demonstrate the mutual influence between two modules with rigorous interpretation from an EM algorithm perspective. We show that under dual supervision, minimizing the positive terms in Eq. 7 intrinsically maximizes the objective of a classical clustering algorithm. For simplicity, we assume that each semantic rank has the same number of sentence pairs \(n\) and represents model parameters with \(\Theta\). In Emma-X, we model the semantic relation of sentence pair \(\left(\mathbf{x}_{i},\mathbf{y}_{k}\right)\) through a joint distribution \(P(\mathbf{x}_{i},\mathbf{y}_{k})\) with the semantic rank \(c\) as a latent variable. Let \(Q(c)\) be a prior distribution over the possible values of semantic ranks. That is \(\sum_{r}Q(c=r)=1,Q(c)\geq 0\). The training goal is to maximize the following likelihood:

\[\operatorname*{argmax}_{\Theta}\sum_{\mathbf{x}_{i}\in\mathcal{X} }\sum_{\mathbf{y}_{k}\in\mathcal{Y}}\log P(\mathbf{x}_{i},\mathbf{y}_{k}|\Theta) =\operatorname*{argmax}_{\Theta}\sum_{\mathbf{x}_{i}\in\mathcal{X }}\sum_{\mathbf{y}_{k}\in\mathcal{Y}}\log\sum_{r=1}^{N}P(\mathbf{x}_{i}, \mathbf{y}_{k},c=r|\Theta) \tag{12}\] \[\geq\operatorname*{argmax}_{\Theta}\sum_{\mathbf{x}_{i}\in \mathcal{X}}\sum_{\mathbf{y}_{k}\in\mathcal{Y}}\sum_{r=1}^{N}Q(c=r)\log\frac{ P(\mathbf{x}_{i},\mathbf{y}_{k},c=r|\Theta)}{Q(c=r)}.\]

**E-Step.** To make the inequality hold with equality, we have:

\[Q(c=r)=\frac{P(\mathbf{x}_{i},\mathbf{y}_{k},c=r|\Theta)}{\sum_{j=1}^{N}P( \mathbf{x}_{i},\mathbf{y}_{k},c=j|\Theta)}=P(c=r|\mathbf{x}_{i},\mathbf{y}_{k},\Theta), \tag{13}\]

which is the posterior probability and is approximated by the prediction from GMM classifier. Since each sentence pair \(\left(\mathbf{x}_{i},\mathbf{y}_{k}\right)\) belongs to only one semantic rank, we approximate \(Q(c=r)=\mathbb{I}(c_{\mathcal{G}}^{*}=r)\), which is a one-hot distribution.

**M-Step.** We try to maximize the likelihood in Eq. 12 under the semantic rank \(c_{\mathcal{G}}^{*}\) :

\[\operatorname*{argmax}_{\Theta}\sum_{\mathbf{x}_{i}\in\mathcal{X} }\sum_{\mathbf{y}_{k}\in\mathcal{Y}}\sum_{r=1}^{N}Q(r)\log\frac{P(\mathbf{x}_{i },\mathbf{y}_{k},r|\Theta)}{Q(r)} \approx\operatorname*{argmax}_{\Theta}\sum_{\mathbf{x}_{i}\in \mathcal{X}}\sum_{\mathbf{y}_{k}\in\mathcal{Y}}\sum_{r=1}^{N}\log P(\mathbf{x }_{i},\mathbf{y}_{k}|c_{\mathcal{G}}^{*}=r,\Theta) \tag{14}\] \[\geq\operatorname*{argmax}_{\Theta}n(n-1)\sum_{r=1}^{N}\tilde{ \mu}_{r}^{2},\]

The above derivation uses the assumption that \(P(\mathbf{x}_{i},\mathbf{y}_{k}|c_{\mathcal{G}}^{*}=r,\Theta)\sim\mathcal{N}_ {r}\big{(}\mathbf{x}_{i}-\mathbf{y}_{k}|\tilde{\mu}_{r},\tilde{\sigma}_{r} \big{)}\), with \(\tilde{\mu}_{r}\) and \(\tilde{\sigma}_{r}\) being the mean value and standard deviation of the Euclidean distance between sentence pairs in semantic rank \(r\). Detailed proof of Eq. 14 is in **Appendix G**.

Next, we prove that minimizing the positive terms in expectation \(\mathcal{L}_{\mathbf{CTL}}(\mathcal{X},\mathcal{Y};\Theta_{\mathcal{M}})\) actually equal to maximizing a lower bound of Eq. 14. As we apply dual supervision, data in the contrastive label space also follows the distribution \(\mathcal{N}_{r}\big{(}\mathbf{x}_{i}-\mathbf{y}_{k}|\tilde{\mu}_{r},\tilde{ \sigma}_{r}\big{)}\). Hence, under mild assumptions, we can get:

\[\mathcal{L}_{\mathbf{CTL}}^{+}(\mathcal{X},\mathcal{Y};\Theta_{\mathcal{M}})= n^{2}\sum_{r=1}^{N-1}\tilde{\mu}_{r}^{2}\,<\,n(n-1)\sum_{r=1}^{N}\tilde{\mu}_{r}^{2 }\,\leq\,\sum_{\mathbf{x}_{i}\in\mathcal{X}}\sum_{\mathbf{y}_{k}\in\mathcal{Y} }\log P(\mathbf{x}_{i},\mathbf{y}_{k}|\Theta), \tag{15}\]

where \(\mathcal{L}_{\mathbf{CTL}}^{+}(\cdot)\) means the positive terms. In the derivation, we use the intrinsic property of semantic ranks (\(\tilde{\mu}_{1}<\tilde{\mu}_{2}<...<\tilde{\mu}_{N}\)). Detailed proof is in **Appendix G**. Eq. 15 demonstrates that with dual supervision, minimizing the contrastive loss can partially maximize the likelihood in Eq. 12.

## 5 Experiments

To thoroughly evaluate the performance of Emma-X, we conduct experiments on **xrete** benchmark to verify the transfer ability of Emma-X on various cross-lingual downstream tasks with strong baselines (pre-trained models: Xlm-r [Conneau et al., 2020], InfoXLM [Chi et al., 2021], Hictl [Wei et al., 2021], sentence models: LaBSE [Feng et al., 2022], S-BERT [Reimers and Gurevych, 2020]) and ChatGPT in Section 5.2. See **Appendices C** and **D** for details. We further conduct geometric analysis in Section 5.3 to better interpret the cross-lingual transferability in Emma-X.

### Setup

**Corpus & Model.** We collect parallel corpora from CCAligned [El-Kishky et al., 2020], CCMatrix [Schwenk et al., 2021], WMT [Akhbardeh et al., 2021], and MultiUN [Ziemski et al., 2016], involving \(94\) languages with \(3.2\) billion sentence pairs. In addition, we add CC-100 [Conneau et al., 2020]as the large-scale monolingual corpus with about 800 billion sentences that covers 94 languages. The cross-lingual encoder starts from the well-trained Xlm-r large model [13]. The GMM classifier is implemented as a mixture of Gaussian forms, each of which consists of a prior \(\pi\in\mathbb{R}^{1}\), a mean \(\mu\in\mathbb{R}^{1024}\) and a standard deviation \(\sigma\in\mathbb{R}^{1024}\), all are trainable variables. We set the total semantic ranks as \(N=4\). The statistics of all data and hyper-parameters are shown in Appendix A.

### xrete Evaluation

**xrete** includes 12 cross-lingual tasks divided into 4 different categories. We report the "translate-train" performance in Table 1 on most tasks but zero-shot performance on BUCC and Tatoeba following [12]. Table 2 presents zero-shot comparisons with sentence models.

**Comparisons with Pre-trained Models.** In Table 1, Emma-X consistently outperforms all baseline models (Xlm-r [13], Hicli [14] and InfoXLM [15]) with 7.97% improvements on average. Specifically, Emma-X achieves 88.1% accuracy on XNLI [13] and 50.21% accuracy on ANLI [12] with up to 6.4% improvements than baselines. On the MultiSTS [12] task, Emma-X achieves an 87.3 correlation score, outperforming several strong baselines by 5.1\(\sim\)21.4, and even achieves comparable performance in the cross-lingual and the monolingual settings (see Appendix F for language-specific results). Furthermore, Emma-X obtains a 67.2 Pearson score on QE [12] task, which is comparable to the winner on the leaderboard3 without any specific finetuning techniques. As for sentence retrieval, Emma-X consistently outperforms previous strong baselines among all 4 tasks [12], and demonstrates 2.6%\(\sim\)39.6% improvements over these baselines. Similar results can be found in sentence classification tasks. Emma-X obtains an 81.3% top-1 accuracy averaged on XCOPA [13], MultiEURLEX [14] and PAWS-X [15] tasks, outperforming Xlm-r, InfoXLM and Hicli by 7.0%, 3.9% and 3.5% improvements, respectively. On MultiARC [12] task, Emma-X shows the lowest error rates among all models. The consistent improvements on all tasks reveal that Emma-X can obtain better universal representations for different natural languages with various

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{3}{c}{**Similarity**} & \multicolumn{3}{c}{**Retieval**} & \multicolumn{3}{c}{**Coulonativeness**} \\  & XNLI & MultiSTS & QE & LakeQA & Mask-X & BUCC & Tatoeba & XCOPA & Male(UKE) & Male(UKE) & Male(UKE) & Male(UKE) \\ \hline Motives & Acc. (T) & Acc. (T) & Spearman (T) & Pearson (T) & maxP20(T) & maxP20(T) & 1 F1 (T) & Acc. (T) & Acc. (T) & MAE (UKE) & Acc. (T) \\ \hline \hline \multirow{2}{*}{\begin{tabular}{l} MultiExtraction \\ \end{tabular} } & 55.1\({}^{*}\) & - & 55.8\({}^{*}\) & - & - & 21.6\({}^{*}\) & 38.6\({}^{*}\) & 56.7\({}^{*}\) & 39.2\({}^{*}\) & 86.1\({}^{*}\) & 67.4\({}^{*}\) & 48.2\({}^{*}\) & 81.9\({}^{*}\) \\  & Xlr.7\({}^{*}\) & 77.5\({}^{*}\) & - & - & 21.6\({}^{*}\) & 36.8\({}^{*}\) & 32.5\({}^{*}\) & - & - & - & 80.9\({}^{*}\) \\  & Xlr.8\({}^{*}\) & 49.12\({}^{*}\) & 61.5\({}^{*}\) & 58.7\({}^{*}\) & 40.7\({}^{*}\) & 45.7\({}^{*}\) & 66.0\({}^{*}\) & 57.7\({}^{*}\) & 69.2\({}^{*}\) & 66.6\({}^{*}\) & - & 88.9\({}^{*}\) \\  & Bien.\({}^{*}\) & 84.7\({}^{*}\) & - & - & - & - & 77.6\({}^{*}\) & 61.7\({}^{*}\) & - & - & 92.8\({}^{*}\) \\  & ChaGPaT & 60.9\({}^{*}\) & 41.7 & 68.6 & 60.9 & - & - & - & - & 74.2\({}^{*}\) & 68.7 & 40.2 & 64.2 \\ \hline \hline \multirow{2}{*}{
\begin{tabular}{l} _Ours re-implementation, translator-train and (uncorable) the revised on English training data and on the data translated to the target language_} \\ \end{tabular} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} \\  & Xlr.8\({}^{*}\) & 82.8 & 48.8\({}^{*}\) & 69.3\({}^{*}\) & 62.1 & 40.3 & 48.6 & 67.9 & 59.1 & 71.2 & 66.9 & 44.9 & 90.1 \\  & Bien.\({}^{*}\) & 84.2\({}^{*}\) & 40.10 & 82.2 & 64.1 & 44.9 & 57.1 & 77.4 & 66.2 & 74.6 & 67.7 & 36.2 & 93.0 \\  & Bien.\({}^{*}\) & 85.1 & 40.02 & 81.6 & 64.9 & 46.1 & 54.8 & 77.6 & 65.8 & 74.8 & 68.3 & 38.2 & 92.

topics and domains. We further conduct experiments with ChatGPT on xrete tasks without 4 Retrieval tasks. We list the prompts for each task in **Appendix E**. ChatGPT's zero-shot performance is worse than fine-tuned pre-trained models and the performance gap is very large on most tasks.

**Zero-shot comparisons with sentence models.** Compared with Xlm-r and InfoXLM, which adopt the same amount of training data as Emma-X, Emma-X consistently outperforms Xlm-r and InfoXLM by 73.2% and 25.1% on average, as shown in Table 2. The results further prove the effectiveness of our pre-training technique. Through the reciprocation between GMM classifier and cross-lingual encoder, Emma-X can generate reliable semantic rank for multilingual non-parallel corpora, which can provide more supervision signals than previous baselines. Emma-X even achieves comparable results with strong supervised methods: LaBSE [14] and S-BERT [15], which both trained on supervised data. LaBSE is trained on a fine-filtered bilingual corpus with 6B translation pairs (2 times larger than Emma-X), while S-BERT is distilled from a S-BERT model fine-tuned on English NLI, STS datasets, and 50M English paraphrase pairs. Compared with these two methods, Emma-X can achieve the best results on QE and Mewski-X by outperforming S-BERT and LaBSE by 71.7% and 117.8% averaged. Emma-X performs worse than these baselines on MultiSTS and BUCC, for these two tasks only contain rich-resource languages, which already have great deal of parallel data.

**Performance on Long-tail Languages.** One goal of Emma-X is to learn universal sentence representations accommodated for more languages. To better prove this, we report the retrieval accuracy on FLORES-200 [20] and Tatoeba [1]. We reformulate FLORES-200, which contains manual transitions in 204 languages (totaling 3001 sentences) to perform retrieval tasks in the same way as Tatoeba and report the performance in terms of language data scale in Table 3. Details about the separation of languages and FLORES-200 are shown in **Appendices A** and **B**. On head languages, Emma-X performs worse than LaBSE by about 4.6% but outperforms S-BERT by 3.5%. On the long-tail languages, Emma-X can surpass S-BERT by 4.3% averaged on two tasks. Emma-X can even exceed the strongest LaBSE by 2.1% on FLORES. One reason for the superior results on long-tail languages is that for those long-tail languages that have only bi-lingual parallel data with rich-resource languages (often English), Emma-X can provide multi-lingual semantic relation signals for them with arbitrary languages through dual supervision.

### Geometric Analysis

To interpret the advantages of Emma-X, we evaluate the geometric characteristics of it on FLORES-200 dataset [20] without any fine-tuning. The criteria of three requirements are Invariance, measured with KL-divergence (KL-D) [13], Canonical Form, measured with Calinski-Harabasz Index (CH-I) [16] and Isotropy, measured with principal ratio (PR) [21]. Details of them are shown in **Appendix B**. We report the numerical results in Table 4 and visualize each characteristic in Figure 2.

**Invariance & Canonical Form** aim to measure how languages are aligned in the representation space. If the sentence representations are universal, then sentences in different languages should follow a similar distribution, which is measured by invariance in KL-divergence. Similarly, canonical form measures how well semantic equivalent sentences are grouped into one cluster, with a clustering metric (CH-I). In Table 4, S-BERT outperforms other baselines in "Invariance" and "Canonical Form" on head languages. However, Emma-X shows better performance on long-tail languages in these two metrics, which is consistent with Table 3. Figure 2 presents similar results. Among the 20 languages we randomly sampled from FLORES, Emma-X can align 17 languages as shown in Figure 1(d), with "xh, eo, ur" as outliers. In Figure 1(e), 1(g), 1(f) and 1(h), different colors represent different languages. So

\begin{table}
\begin{tabular}{l|c c c|c c c|c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{3}{c|}{**Head Langs**} & \multicolumn{3}{c|}{**Long-tail Langs**} & \multirow{2}{*}{**Invariance**} & \multicolumn{3}{c}{**All Langs**} \\  & \multicolumn{1}{c}{**Invariance**} & \multicolumn{1}{c}{**Canonical Form**} & \multicolumn{1}{c}{**Isotropy**} & \multicolumn{1}{c}{**Invariance**} & \multicolumn{1}{c}{**Canonical Form**} & \multicolumn{1}{c}{**Isotropy**} & \multicolumn{1}{c}{**Invariance**} & \multicolumn{1}{c}{**Canonical Form**} & \multicolumn{1}{c}{**Isotropy**} \\ \hline \hline Metrics & KL-D (↓) & CH-I (↑) & PR (↑) & KL-D (↓) & CH-I (↑) & PR (↑) & KL-D (↓) & CH-I(↑) & PR (↑) \\ \hline XLM-R (cls) & 0.7356 & 30.19 & 0.3681 & 2.0042 & 7.96 & 0.3686 & 1.6501 & 20.80 & 0.3683 \\ InfoXLM (cls) & 0.4491 & 38.82 & 0.4478 & 1.8555 & 13.02 & 0.4406 & 1.4747 & 31.51 & 0.4665 \\ S-BERT (mean) & **0.4115** & **1082** & 0.4519 & 1.3112 & 44.32 & 0.4414 & **0.9782** & **102.36** & 0.4467 \\ \hline Extra-X (cls) & 0.3603 & 43.52 & **0.5318** & **0.3963** & **46.53** & **0.5732** & 1.1904 & 48.70 & **0.5918** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparisons with existing methods on FLORES dataset for geometric analysis. “cls” and “mean” represent different pooling strategies to obtain sentence representations.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]

## Acknowledgements

We thank the anonymous reviewers for their constructive comments. This work is supported by the National Natural Science Foundation of China (Grant No.U21B2009). This research is also supported by the Strategic Priority Research Program of the Chinese Academy of Science, Grant No.XDC02030400.

## References

* Abend and Rappoport (2017) Omri Abend and Ari Rappoport. The state of the art in semantic representation. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 77-89, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1008. URL [https://aclanthology.org/P17-1008](https://aclanthology.org/P17-1008).
* Aharoni et al. (2019) Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 3874-3884, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1388. URL [https://aclanthology.org/N19-1388](https://aclanthology.org/N19-1388).
* Akhbardeh et al. (2021) Farhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ondrej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta R. Costa-jussa, Cristina Espana-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie, Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. Findings of the 2021 conference on machine translation (WMT21). In _Proceedings of the Sixth Conference on Machine Translation_, pages 1-88, Online, November 2021. Association for Computational Linguistics. URL [https://aclanthology.org/2021.wmt-1.1](https://aclanthology.org/2021.wmt-1.1).
* Artetxe and Schwenk (2019) Mikel Artetxe and Holger Schwenk. Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. _Transactions of the Association for Computational Linguistics_, 7:597-610, 2019. doi: 10.1162/tacl_a_00288. URL [https://aclanthology.org/Q19-1038](https://aclanthology.org/Q19-1038).
* Artetxe et al. (2020) Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of monolingual representations. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 4623-4637, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.421. URL [https://aclanthology.org/2020.acl-main.421](https://aclanthology.org/2020.acl-main.421).
* Bapna et al. (2020) Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Nikhil Baljekar, Xavier Garcia, Wolfgang Macherey, Theresa Breiner, Vera Sadinger Axelrod, Jason Riesa, Yuan Cao, Mia Chen, Klaus Macherey, Maxim Krikun, Pidong Wang, Alexander Gutkin, Apu Shah, Yanping Huang, Zhifeng Chen, Yonghui Wu, and Macduff Richard Hughes. Building machine translation systems for the next thousand languages. Technical report, Google Research, 2022.
* Botha et al. (2020) Jan A. Botha, Zifei Shan, and Daniel Gillick. Entity Linking in 100 Languages. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 7833-7845, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.630. URL [https://aclanthology.org/2020.emnlp-main.630](https://aclanthology.org/2020.emnlp-main.630).
* Bowman et al. (2015) Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing_, pages 632-642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL [https://www.aclweb.org/anthology/D15-1075](https://www.aclweb.org/anthology/D15-1075).
* Calinski and Harabasz (1974) T. Calinski and J Harabasz. A dendrite method for cluster analysis. _Communications in Statistics_, 3(1):1-27, 1974. doi: 10.1080/03610927408827101. URL [https://www.tandfonline.com/doi/abs/10.1080/03610927408827101](https://www.tandfonline.com/doi/abs/10.1080/03610927408827101).
* Cesa-Bianchi et al. (2019)Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In _Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)_, pages 1-14, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL [https://aclanthology.org/S17-2001](https://aclanthology.org/S17-2001).
* a multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 6974-6996, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.559. URL [https://aclanthology.org/2021.emnlp-main.559](https://aclanthology.org/2021.emnlp-main.559).
* Chen et al. (2023) Nuo Chen, Linjun Shou, Ming Gong, Jian Pei, Bowen Cao, Jianhui Chang, Daxin Jiang, and Jia Li. Alleviating over-smoothing for unsupervised sentence representation, 2023.
* Chi et al. (2021) Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, and Ming Zhou. InfoXLM: An information-theoretic framework for cross-lingual language model pre-training. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 3576-3588, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.280. URL [https://aclanthology.org/2021.naacl-main.280](https://aclanthology.org/2021.naacl-main.280).
* Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.
* Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL [https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf).
* Chuang et al. (2022) Yung-Sung Chuang, Rumen Dangovski, Hongyin Luo, Yang Zhang, Shiyu Chang, Marin Soljacic, Shang-Wen Li, Scott Yih, Yoon Kim, and James Glass. DiffCSE: Difference-based contrastive learning for sentence embeddings. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 4207-4218, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.311. URL [https://aclanthology.org/2022.naacl-main.311](https://aclanthology.org/2022.naacl-main.311).
* Conneau and Lample (2019) Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf).
* Conneau et al. (2018) Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 2475-2485, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1269. URL [https://www.aclweb.org/anthology/D18-1269](https://www.aclweb.org/anthology/D18-1269).
* Conneau et al. (2019)Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 8440-8451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL [https://aclanthology.org/2020.acl-main.747](https://aclanthology.org/2020.acl-main.747).
* Costa-jussa et al. (2022) Marta R Costa-jussa, James Cross, Onur Celebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine translation. _arXiv preprint arXiv:2207.04672_, 2022.
* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423).
* Ebrahimi et al. (2022) Abteen Ebrahimi, Manuel Mager, Arturo Oncevay, Vishrav Chaudhary, Luis Chiruzzo, Angela Fan, John Ortega, Ricardo Ramos, Annette Rios, Ivan Vladimir Meza Ruiz, Gustavo Gimenez-Lugo, Elisabeth Mager, Graham Neubig, Alexis Palmer, Rolando Coto-Solano, Thang Vu, and Katharina Kann. AmericansNLI: Evaluating zero-shot natural language understanding of pretrained multilingual models in truly low-resource languages. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 6279-6299, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.435. URL [https://aclanthology.org/2022.acl-long.435](https://aclanthology.org/2022.acl-long.435).
* El-Kishky et al. (2020) Ahmed El-Kishky, Vishrav Chaudhary, Francisco Guzman, and Philipp Koehn. CCAligned: A massive collection of cross-lingual web-document pairs. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 5960-5969, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.480. URL [https://aclanthology.org/2020.emnlp-main.480](https://aclanthology.org/2020.emnlp-main.480).
* Everitt and Hand (1981) BS Everitt and DJ Hand. Finite mixture distributions. _Monographs on Applied Probability and Statistics_, 1981.
* Feng et al. (2022) Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Language-agnostic BERT sentence embedding. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 878-891, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.62. URL [https://aclanthology.org/2022.acl-long.62](https://aclanthology.org/2022.acl-long.62).
* Gao et al. (2021) Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 6894-6910, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552. URL [https://aclanthology.org/2021.emnlp-main.552](https://aclanthology.org/2021.emnlp-main.552).
* Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012)_, pages 394-398, Montreal, Canada, 7-8 June 2012. Association for Computational Linguistics. URL [https://aclanthology.org/S12-1052](https://aclanthology.org/S12-1052).
* Goswami et al. (2021) Koustava Goswami, Sourav Dutta, Haytham Assem, Theodorus Fransen, and John P. McCrae. Cross-lingual sentence embedding using multi-task learning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 9099-9113, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.716. URL [https://aclanthology.org/2021.emnlp-main.716](https://aclanthology.org/2021.emnlp-main.716).
* Ganin and Leva (2015)Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc'Aurelio Ranzato, Francisco Guzman, and Angela Fan. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. _Transactions of the Association for Computational Linguistics_, 10:522-538, 2022. doi: 10.1162/tacl_a_00474. URL [https://aclanthology.org/2022.tacl-1.30](https://aclanthology.org/2022.tacl-1.30).
* Gunel et al. (2021) Beliz Gunel, Jingfei Du, Alexis Conneau, and Veselin Stoyanov. Supervised contrastive learning for pre-trained language model fine-tuning. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=cu7IUiOhuJH](https://openreview.net/forum?id=cu7IUiOhuJH).
* Guo et al. (2018) Mandy Guo, Qinlan Shen, Yinfei Yang, Heming Ge, Daniel Cer, Gustavo Hernandez Abrego, Keith Stevens, Noah Constant, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Effective parallel corpus mining using bilingual sentence embeddings. In _Proceedings of the Third Conference on Machine Translation: Research Papers_, pages 165-176, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6317. URL [https://aclanthology.org/W18-6317](https://aclanthology.org/W18-6317).
* He et al. (2020) Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9726-9735. IEEE Computer Society, 2020.
* Hoffmann et al. (2022) David T Hoffmann, N Behrmann, J Gall, Thomas Brox, and M Noroozi. Ranking info noise contrastive estimation: Boosting contrastive learning via ranked positives. In _AAAI Conference on Artificial Intelligence_, 2022.
* Hu et al. (2020) Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 4411-4421. PMLR, 13-18 Jul 2020. URL [https://proceedings.mlr.press/v119/hu20b.html](https://proceedings.mlr.press/v119/hu20b.html).
* Hu et al. (2021) Junjie Hu, Melvin Johnson, Orhan Firat, Aditya Siddhant, and Graham Neubig. Explicit alignment objectives for multilingual bidirectional encoders. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 3633-3643, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.284. URL [https://aclanthology.org/2021.naacl-main.284](https://aclanthology.org/2021.naacl-main.284).
* Huang et al. (2019) Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. Unicoder: A universal language encoder by pre-training with multiple cross-lingual tasks. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2485-2494, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1252. URL [https://aclanthology.org/D19-1252](https://aclanthology.org/D19-1252).
* Irwin et al. (2009) Jeannie Y Irwin, Henk Harkema, Lee M Christensen, Titus Schleyer, Peter J Haug, and Wendy W Chapman. Methodology to develop and evaluate a semantic representation for nlp. In _AMIA Annual Symposium Proceedings_, volume 2009, page 271. American Medical Informatics Association, 2009.
* Kepler et al. (2019) Fabio Kepler, Jonay Trenous, Marcos Treviso, Miguel Vera, and Andre F. T. Martins. OpenKiwi: An open source framework for quality estimation. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations_, pages 117-122, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-3020. URL [https://aclanthology.org/P19-3020](https://aclanthology.org/P19-3020).
* Keung et al. (2020) Phillip Keung, Yichao Lu, Gyorgy Szarvas, and Noah A. Smith. The multilingual Amazon reviews corpus. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 4563-4568, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.369. URL [https://aclanthology.org/2020.emnlp-main.369](https://aclanthology.org/2020.emnlp-main.369).
* Liu et al. (2019)Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _3rd International Conference on Learning Representations, San Diego, CA_, 2015. URL [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980).
* Kudo and Richardson (2018) Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 66-71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL [https://aclanthology.org/D18-2012](https://aclanthology.org/D18-2012).
* 86, 1951. doi: 10.1214/aoms/1177729694. URL [https://doi.org/10.1214/aoms/1177729694](https://doi.org/10.1214/aoms/1177729694).
* Kvapilkova et al. (2020) Ivana Kvapilkova, Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Ondrej Bojar. Unsupervised multilingual sentence embeddings for parallel corpus mining. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop_, pages 255-262, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-srw.34. URL [https://aclanthology.org/2020.acl-srw.34](https://aclanthology.org/2020.acl-srw.34).
* Lee et al. (2022) Seonghyeon Lee, Dongha Lee, Seongbo Jang, and Hwanjo Yu. Toward interpretable semantic textual similarity via optimal transport-based contrastive sentence learning. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5969-5979, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.412. URL [https://aclanthology.org/2022.acl-long.412](https://aclanthology.org/2022.acl-long.412).
* Leibniz and Freiherr (1996) Gottfried Wilhelm Leibniz and Gottfried Wilhelm Freiherr von Leibniz. _Leibniz: New essays on human understanding_. Cambridge University Press, 1996.
* Li et al. (2023) Ziheng Li, Shaohan Huang, Zihan Zhang, Zhi-Hong Deng, Qiang Lou, Haizhen Huang, Jian Jiao, Furu Wei, Weiwei Deng, and Qi Zhang. Dual-alignment pre-training for cross-lingual sentence embedding, 2023.
* Luo et al. (2021) Fuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi, Songfang Huang, Fei Huang, and Luo Si. VECO: Variable and flexible cross-lingual pre-training for language understanding and generation. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 3980-3994, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.308. URL [https://aclanthology.org/2021.acl-long.308](https://aclanthology.org/2021.acl-long.308).
* Moon (1996) T.K. Moon. The expectation-maximization algorithm. _IEEE Signal Processing Magazine_, 13(6):47-60, 1996. doi: 10.1109/79.543975.
* Mu and Viswanath (2018) Jiaqi Mu and Pramod Viswanath. All-but-the-top: Simple and effective postprocessing for word representations. In _International Conference on Learning Representations_, 2018. URL [https://openreview.net/forum?id=HkuGJ3kCb](https://openreview.net/forum?id=HkuGJ3kCb).
* Ni et al. (2022) Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In _Findings of the Association for Computational Linguistics: ACL 2022_, pages 1864-1874, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.146. URL [https://aclanthology.org/2022.findings-acl.146](https://aclanthology.org/2022.findings-acl.146).
* van den Oord et al. (2018) Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _CoRR_, abs/1807.03748, 2018. URL [http://arxiv.org/abs/1807.03748](http://arxiv.org/abs/1807.03748).
* Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.
* Ouyang et al. (2018)Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE-M: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 27-38, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.3. URL [https://aclanthology.org/2021.emnlp-main.3](https://aclanthology.org/2021.emnlp-main.3).
* Zweigenbaum and Rapp (2018) Serge Sharoff Pierre Zweigenbaum and Reinhard Rapp. Overview of the third bucc shared task: Spotting parallel sentences in comparable corpora. In Reinhard Rapp, Pierre Zweigenbaum, and Serge Sharoff, editors, _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Paris, France, may 2018. European Language Resources Association (ELRA). ISBN 979-10-95546-07-8.
* Ponti et al. (2020) Edoardo Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen. XCOPA: A multilingual dataset for causal commonsense reasoning. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 2362-2376, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.185. URL [https://aclanthology.org/2020.emnlp-main.185](https://aclanthology.org/2020.emnlp-main.185).
* Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pages 2383-2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL [https://aclanthology.org/D16-1264](https://aclanthology.org/D16-1264).
* Reimers and Gurevych (2020) Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingual using knowledge distillation. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 4512-4525, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.365. URL [https://aclanthology.org/2020.emnlp-main.365](https://aclanthology.org/2020.emnlp-main.365).
* Roy et al. (2020) Uma Roy, Noah Constant, Rami Al-Rfou, Aditya Barua, Aaron Phillips, and Yinfei Yang. LAReQA: Language-agnostic answer retrieval from a multilingual pool. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 5919-5930, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.477. URL [https://aclanthology.org/2020.emnlp-main.477](https://aclanthology.org/2020.emnlp-main.477).
* Ruder et al. (2021) Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, and Melvin Johnson. XTREME-R: Towards more challenging and nuanced multilingual evaluation. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 10215-10245, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.802. URL [https://aclanthology.org/2021.emnlp-main.802](https://aclanthology.org/2021.emnlp-main.802).
* Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQA: Commonsense reasoning about social interactions. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 4463-4473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL [https://aclanthology.org/D19-1454](https://aclanthology.org/D19-1454).
* Saunshi et al. (2019) Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A theoretical analysis of contrastive unsupervised representation learning. In _International Conference on Machine Learning_, pages 5628-5637. PMLR, 2019.
* Schwenk et al. (2021) Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. CCMatrix: Mining billions of high-quality parallel sentences on the web. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 6490-6500, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.507. URL [https://aclanthology.org/2021.acl-long.507](https://aclanthology.org/2021.acl-long.507).
* Schlichting et al. (2019)Lucia Specia, Frederic Blain, Marina Fomicheva, Chrysoula Zerva, Zhenhao Li, Vishrav Chaudhary, and Andre F. T. Martins. Findings of the WMT 2021 shared task on quality estimation. In _Proceedings of the Sixth Conference on Machine Translation_, pages 684-725, Online, November 2021. Association for Computational Linguistics. URL [https://aclanthology.org/2021.wmt-1.71](https://aclanthology.org/2021.wmt-1.71).
* Teller (2000) Virginia Teller. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. _Computational Linguistics_, 26(4):638-641, 12 2000. ISSN 0891-2017. doi: 10.1162/089120100750105975. URL [https://doi.org/10.1162/089120100750105975](https://doi.org/10.1162/089120100750105975).
* Ttyajamorn et al. (2021) Nattapong Ttyajamorn, Tomoyuki Kajiwara, Yuki Arase, and Makoto Onizuka. Language-agnostic representation from multilingual sentence encoders for cross-lingual similarity estimation. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 7764-7774, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.612. URL [https://aclanthology.org/2021.emnlp-main.612](https://aclanthology.org/2021.emnlp-main.612).
* van der Maaten and Hinton (2008) Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of Machine Learning Research_, 9(86):2579-2605, 2008. URL [http://jmlr.org/papers/v9/vandermaaten08a.html](http://jmlr.org/papers/v9/vandermaaten08a.html).
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems 30, NIPS 2017 4-9 December 2017, Long Beach, CA, USA_, pages 5998-6008, 2017. URL [http://papers.nips.cc/paper/7181-attention-is-all-you-need](http://papers.nips.cc/paper/7181-attention-is-all-you-need).
* Wang and Liu (2021) Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2495-2504, June 2021.
* Wang et al. (2021) Liang Wang, Wei Zhao, and Jingming Liu. Aligning cross-lingual sentence representations with dual momentum contrast. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 3807-3815, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.309. URL [https://aclanthology.org/2021.emnlp-main.309](https://aclanthology.org/2021.emnlp-main.309).
* Wei et al. (2021) Xiangpeng Wei, Rongxiang Weng, Yue Hu, Luxi Xing, Heng Yu, and Weihua Luo. On learning universal representations across languages. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=Uu1Nw-eeTxJ](https://openreview.net/forum?id=Uu1Nw-eeTxJ).
* Wei et al. (2022) Xiangpeng Wei, Heng Yu, Yue Hu, Rongxiang Weng, Weihua Luo, and Rong Jin. Learning to generalize to more: Continuous semantic augmentation for neural machine translation. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 7930-7944, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.546. URL [https://aclanthology.org/2022.acl-long.546](https://aclanthology.org/2022.acl-long.546).
* Wenzek et al. (2020) Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In _Proceedings of the Twelfth Language Resources and Evaluation Conference_, pages 4003-4012, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://aclanthology.org/2020.lrec-1.494](https://aclanthology.org/2020.lrec-1.494).
* Wierzbicka (1999) Anna Wierzbicka. _Emotions across languages and cultures: Diversity and universals_. Cambridge university press, 1999.
* Williams et al. (2018) Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 1112-1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL [https://aclanthology.org/N18-1101](https://aclanthology.org/N18-1101).
* Williams et al. (2019)BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, and et al. Bloom: A 176b-parameter open-access multilingual language model, 2023.
* Wu et al. (2022) Xing Wu, Chaochen Gao, Zijia Lin, Jizhong Han, Zhongyuan Wang, and Songlin Hu. Infocse: Information-aggregated contrastive learning of sentence embeddings, 2022. URL [https://arxiv.org/abs/2210.06432](https://arxiv.org/abs/2210.06432).
* Yang et al. (2021) Nan Yang, Furu Wei, Binxing Jiao, Daxing Jiang, and Linjun Yang. xMoCo: Cross momentum contrastive learning for open-domain question answering. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 6120-6129, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.477. URL [https://aclanthology.org/2021.acl-long.477](https://aclanthology.org/2021.acl-long.477).
* Yang et al. (2019) Yinfei Yang, Gustavo Hernandez Abrego, Steve Yuan, Mandy Guo, Qinlan Shen, Daniel Cer, Yunhsuan Sung, Brian Strope, and Ray Kurzweil. Improving multilingual sentence embedding using bi-directional dual encoder with additive margin softmax. In _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19_, pages 5370-5378. International Joint Conferences on Artificial Intelligence Organization, 7 2019a. doi: 10.24963/ijcai.2019/746. URL [https://doi.org/10.24963/ijcai.2019/746](https://doi.org/10.24963/ijcai.2019/746).
* Yang et al. (2019) Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 3687-3692, Hong Kong, China, November 2019b. Association for Computational Linguistics. doi: 10.18653/v1/D19-1382. URL [https://aclanthology.org/D19-1382](https://aclanthology.org/D19-1382).
* Zhang et al. (2021) Dejiao Zhang, Shang-Wen Li, Wei Xiao, Henghui Zhu, Ramesh Nallapati, Andrew O. Arnold, and Bing Xiang. Pairwise supervised contrastive learning of sentence representations. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 5786-5798, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.467. URL [https://aclanthology.org/2021.emnlp-main.467](https://aclanthology.org/2021.emnlp-main.467).
* Zhang et al. (2022) Dejiao Zhang, Wei Xiao, Henghui Zhu, Xiaofei Ma, and Andrew Arnold. Virtual augmentation supported contrastive learning of sentence representations. In _Findings of the Association for Computational Linguistics: ACL 2022_, pages 864-876, Dublin, Ireland, May 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.70. URL [https://aclanthology.org/2022.findings-acl.70](https://aclanthology.org/2022.findings-acl.70).
* Zhang et al. (2018) Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In _International Conference on Learning Representations_, 2018. URL [https://openreview.net/forum?id=r1Ddp1-Rb](https://openreview.net/forum?id=r1Ddp1-Rb).
* Zhang et al. (2022b) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022b.
* Zhang et al. (2022c) Yanzhao Zhang, Richong Zhang, Samuel Mensah, Xudong Liu, and Yongyi Mao. Unsupervised sentence representation via contrastive learning with mixing negatives. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(10):11730-11738, Jun. 2022c. doi: 10.1609/aaai.v36i10.21428. URL [https://ojs.aaai.org/index.php/AAAI/article/view/21428](https://ojs.aaai.org/index.php/AAAI/article/view/21428).
* Zhang et al. (2019) Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 1298-1308, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1131. URL [https://aclanthology.org/N19-1131](https://aclanthology.org/N19-1131).

Michal Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. The United Nations parallel corpus v1.0. In _Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)_, pages 3530-3534, Portoroz, Slovenia, May 2016. European Language Resources Association (ELRA). URL [https://aclanthology.org/L16-1561](https://aclanthology.org/L16-1561).
* Zweigenbaum et al. (2017) Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. Overview of the second BUCC shared task: Spotting parallel sentences in comparable corpora. In _Proceedings of the 10th Workshop on Building and Using Comparable Corpora_, pages 60-67, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-2512. URL [https://www.aclweb.org/anthology/W17-2512](https://www.aclweb.org/anthology/W17-2512).

Training Corpora and Hyper-parameters

### Training Corpora

As for monolingual data, we follow Conneau et al. (2020) to build a Common-Crawl Corpus using the CCNet (Wenzek et al., 2020) tool4, which is widely used in the literature Huang et al. (2019); Luo et al. (2021); Chi et al. (2021); Wei et al. (2021). Further, we collect parallel corpora from CCAligned El-Kishky et al. (2020), CCMatrix Schwenk et al. (2021), WMT Akhbadeh et al. (2021), and MultiUN Ziemski et al. (2016), involving 94 languages with more than 4.8 billion sentence pairs. We use the OpusFilter5 tool to remove noisy bitexts, which results in 3.2 billion sentence pairs. Table 7 shows the statistics for both monolingual and parallel data. We apply subword tokenization directly on raw text data using Sentence Piece Model Kudo and Richardson (2018) without any additional preprocessing. To better support our motivation that Emma-X can cover more languages than previous cross-lingual sentence representations, we divide Tatoeba Artetxe and Schwenk (2019) into two subsets: "Head", containing languages usually covered in previous methods, and "Long-tail", with other languages. We treat the 36 languages contained in XTREME Ruder et al. (2021) as head languages, which are: "**ar, he, vi, id, jv, tl, eu, ml, ta, te, af, nl, en, de, el, bn, hi, mr, ur, fa, fr, it, pt, es, bg, ru, ja, ka, to, th, sw, zh, kk, tr, et, fi, hu, az, lt, pl, uk, ro"**. The remaining 76 languages in Tatoeba are treated as long-tail ones.

Footnote 4: [https://github.com/facebookresearch/cc](https://github.com/facebookresearch/cc) ̇net

Footnote 5: [https://github.com/Helsinki-NLP/OpusFilter](https://github.com/Helsinki-NLP/OpusFilter)

Footnote 6: [https://en.wikinews.org/wiki/MainPage](https://en.wikinews.org/wiki/MainPage)

Footnote 7: [https://en.wikibooks.org/wiki/Wikijunior](https://en.wikibooks.org/wiki/Wikijunior)

### Hyper-parameters

The parameters of Emma-X are first initialized with Xlm-r, with 24 layers of Transformer (Vaswani et al., 2017) encoder, 1024 hidden states, and 16 attention heads. We set the total semantic ranks as 4. The GMM classifier is implemented as a mixture of Gaussian forms, each of which consists of a prior \(\pi\in\mathbb{R}^{1}\), a mean \(\mu\in\mathbb{R}^{1024}\), and a variance \(\sigma\in\mathbb{R}^{1024}\), all are trainable variables. We optimize the GMM classifier with Adam (\(\beta_{1}\)=0.9, \(\beta_{2}\)=0.999) Kingma and Ba (2015) using a batch size of 1024 and a learning rate of 3e-5. For cross-lingual encoder, we apply the same training setting as MoCo He et al. (2020), with the momentum queue \(K\) to be 256 and temperature as 0.04. We set the momentum coefficient to 0.999 and use the Adam optimizer with a cosine decay learning rate whose peak is 5e-4.

## Appendix B FLORES-200 Dataset and Geometric Analysis

### Flores-200 dataset

FLORES-200 Goyal et al. (2022), Costa-jussa et al. (2022) is a many-to-many multilingual benchmark, which consists of 3001 sentences in 204 total languages. FLORES-200 sourced all sentences from English WikiMedia and translated these English sentences into 204 languages by human translators. In particular, sentences in FLORES-200 have a much larger breadth of topics, for they are collected from three different sources: WikiNews6, WikiJunior7 and WikiVoyage8. We summarize the basic statistics of all languages in FLORES-200 in Table 8. Similar to Tatoeba Artetxe and Schwenk (2019), we treat English data "eng_Latin" as retrieval labels and report the retrieval accuracy using the same scripts as Tatoeba in XTREME (Ruder et al., 2021). We set the 68 languages: "**bel_Cyrl, bos_Latin, hun_Latin, epo_Latin, khm_Khmr, urd_Arab, sp_Cyrl, jav_Latin, hye_Armn, gla_Latin, por_Latin, lit_Latin, bul_Cyrl, slk_Latin, mal_Mlym, ita_Latin, nno_Latin, mar_Deva, hrv_Latin, hin_Deva, kat_Geor, ben_Beng, fin_Latin, cym_Latin, oci_Latin, cat_Latin, fao_Latin, xho_Latin, spa_Latin, ron_Latin, amb_Etih, ces_Latin, swe_Latin, nld_Latin, tat_Cyrl, kor_Hang, glg_Latin, fra_Latin, eus_Latin, ind_Latin, dan_Latin, tha_Thai, deu_Latin, tel_Telu, afr_Latin, pol_Latin, est_Latin, uig_Arab, ukr_Cyrl, uzn_Latin, heb_Hebr, kaz_Cyrl, nob_Latin, rus_Cyrl, vie_Latin, arb_Arab, zho_Hans, tuk_Latin, khk_Cyrl, jpn_Jpan, ell_Grek, isl_Latin, tam_Taml, siv_Latin, tur_Latin, mkd_Cyrl, tgl_Latin, gle_Latin" as "Head" languages, and the remaining 135 languages (excluded English data) as "Long-tail" ones.

[MISSING_PAGE_FAIL:21]

The higher the CH-I is, the better the semantically equivalent sentences are clustered.

**Isotropy Measurement** A high-dimensional embedding space often demonstrates poor isotropy, and deteriorates into a low-dimensional manifold that greatly limits the expressive ability of the embedding space. We adopt principal ratio (PR) (Mu and Viswanath, 2018) to measure isotropy. Let \(\mathbf{\varepsilon}\) be the sentence representation matrix, \(\mathbf{\nu}\) be the set of the eigenvectors of \(\mathbf{\varepsilon}\), the Isotropy \(\mathbf{\mathit{\Sigma}}_{\mathbf{\mathit{so}}}\) is

\[\mathcal{I}_{\mathit{so}}=\min_{v\in\mathbf{\mathcal{V}}}\sum_{e\in \mathcal{E}}\exp(v^{\mathsf{T}}e)/\max_{v\in\mathbf{\mathcal{V}}}\sum_{e\in\mathcal{ E}}\exp(v^{\mathsf{T}}e). \tag{18}\]

The closer \(\mathbf{\mathit{\Sigma}}_{\mathbf{\mathit{so}}}\) is to \(1\), the more isotropic the representation space is.

## Appendix C Xrete: Cross-lingual Representation Transfer Evaluation

xrete consists of 12 tasks that fall into four different categories. In our "translate-train-all" setting, we individually fine-tune models with English training set and its translated training sets on each task. Then we report the performance of our fine-tuned model. We give an overview in Table 9 and describe the task details as follows.

XnliThe Cross-lingual Natural Language Inference corpus Conneau et al. (2018) tasks the systems with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral). A crowdsourcing-based procedure is used for collecting English examples, which are later translated into ten target languages for evaluation. Training data stays consistent with the English training data of MultiNLI Williams et al. (2018). For evaluation, we concatenate two sentences as input and apply a new classification head to distinguish sentence relationships. We perform "translate-train-all" evaluation, where the model is first fine-tuned on English training data and its translated data in other languages, then evaluated on test sets.

AmericasNLI (ANLI)The AmericasNLI Ebrahimi et al. (2022) is an extension of XNLI task to 10 Indigenous languages of the Americas. All of these languages are truly low-resource languages and serve as a good testbed for zero-shot cross-lingual transferability. As Spanish is more relative to the target languages, the Spanish version of XNLI subset is translated for evaluation. For training, both English and Spanish versions of MultiNLI training data are provided. We evaluate on ANLI following the same settings as in XNLI.

MultiSTSThe Multilingual Semantic Textual Similarity dataset Cer et al. (2017), Reimers and Gurevych (2020) aims to assign a semantic similarity score for a pair of sentences. The MultiSTS dataset contains 7 cross-lingual sentence pairs and 3 monolingual pairs. Stanford NLI Bowman et al. (2015) and English STS Cer et al. (2017) are provided as training sets. We report the results after first fine-tuning on English training set using a Siamese network structure (Reimers and Gurevych, 2020). Then we compute the cosine similarity between the sentence pairs and compute Spearman's rank correlation between the predicted score and gold score following Reimers and Gurevych (2020).

\begin{table}
\begin{tabular}{l l r r r r r r} \hline \hline
**Task category** & **Task** & **Train** & **Dev** & **Test** & **Lang.** & **Metric** & **Domain** \\ \hline \multirow{2}{*}{Inference} & AmericasNLI & 392,702 & 222.743 & 738-750 & 10 & Accuracy & Misc. \\  & XNLI & 392,702 & 2,490 & 5,010 & 15 & Accuracy & Misc. \\ \hline \multirow{2}{*}{Semantic Similarity} & Multi-STS & 550,152,542,749 & 10,000+1,500 & 250 & 7 & Spearman & Misc. \\  & WMT210ETask1 & 7,000 & 1,000 & 1,000 & 7 (11) & Pearson & News \\ \hline \multirow{3}{*}{Sentence Retrieval} & LARQA & 87,599 & 10,579 & 1,190 & 11 & mAP@20 & Wikipedia \\  & Mewsli-X & 116,093 & 10,252 & 428-1,482 & 11 (50) & mAP@20 & News \\  & BUCC & - & - & 1,896-14,330 & 5 & FI & Wiki/news \\  & Tatoea & - & - & 1,000 & 36 (122) & Accuracy & Misc. \\ \hline \multirow{4}{*}{Classification} & XCOPA & 33,410+400 & 100 & 500 & 11 & Accuracy & Misc. \\  & MultiEURLEX & 55,000 & 5,000 & 5,000 & 23 & Accuracy & Legal \\ \cline{1-1}  & MultiARC & 200,000 & 5,000 & 5,000 & 6 & MAE & Reviews \\ \cline{1-1}  & PAWS-X & 49,401 & 2,000 & 2,000 & 7 & Accuracy & Wiki/Quora \\ \hline \hline \end{tabular}
\end{table}
Table 9: Overview of xrete tasks. For tasks that have training and dev sets in other languages, we only report the number of sentences in English sets. We report the number of test examples per language.

WMT21QTETask1 (QE)The WMT21 Quality Estimation Task 1 Sentence-level Direct Assessment Specia et al. (2021) aims at testing the translation quality and this task has been applied to test the sensitivity of language models to semantic similarity Tiyajamorn et al. (2021). The training and evaluation sets are collected from Wikipedia by translating sentences using state-of-the-art translation models to 6 languages and annotated by professional translators. In WMT21, 4 new language pairs with no training data are given to test zero-shot cross-lingual transferability. Our evaluation setting on QE is similar to that on MultiSTS, but we report Pearson's rank correlation (Kepler et al., 2019).

LAReQAThe Language-Agnostic Retrieval Question Answering Roy et al. (2020) is a QA retrieval task where models are required to retrieve all relevant answers in different languages over a large multilingual pool. The dataset is constructed on XQuAD Artetxe et al. (2020) and a question is linked with answer sentences in different languages. The training set of SQuAD v1.1 Rajpurkar et al. (2016) is used to fine-tune the model to adapt to QA retrieval task. During the evaluation, sentence embeddings are also obtained by a siamese network, and we retrieve the sentences with the highest cosine similarity as predictions.

Mewsli-XMewsli (**M**ultilingual Entities in **News**, **I**nked) requires linking an entity mention to its entry in a language-agnostic knowledge base Botha et al. (2020). Mewsli-X Ruder et al. (2021) features 15k mentions in 11 languages. For each mention, Mewsli-X offers entity descriptions candidate pool containing 1M candidates across 50 languages. Fine-tuning is done on a predefined set of English-only mention-entity pairs from English Wikipedia hyperlinks. Our evaluation setting is identical to LAReQA.

BUCCThe second and third shared task of the workshop on Building and Using Parallel Corpora Zweigenbaum et al. (2017), Pierre Zweigenbaum and Rapp (2018) aims to examine the ability of models to detect parallel sentence pairs in a pair of monolingual corpora. The dataset provides train and test splits in 5 languages. Following XTREME Hu et al. (2020), we directly evaluate on BUCC without fine-tuning and retrieve sentences with the highest cosine similarity.

TatoebaThe goal of the Tatoeba dataset Artetxe and Schwenk (2019) is to find the nearest neighbor for each sentence in the other language according to cosine similarity and compute the error rate. The dataset consists of up to 1,000 English-aligned sentence pairs covering 122 languages. Following XTREME Hu et al. (2020), we directly evaluate on Tatoeba without fine-tuning and retrieve sentences with the highest cosine similarity.

XcopraIn the Cross-lingual Choice of Plausible Alternatives dataset Ponti et al. (2020), each XCOPA instance corresponds to a premise and two alternatives. The task is formulated as a binary classification to predict the more plausible choice. The English COPA Gordon et al. (2012) training set and Social IQA Sap et al. (2019) training data are used for fine-tuning, while the validation and test sets of English COPA are translated and re-annotated into 11 languages for evaluation.

MultiEURLEXThe MultiEURLEX dataset Chalkidis et al. (2021) is a legal topic classification task that comprises 65k European Union (EU) laws in 23 official EU languages. The dataset provides multi-granular labels per document. The dataset is split into training, development, and test subsets chronologically, resulting in 55k training documents for 7 languages, and 5k each for development and test subsets in all 23 languages.

MultiARCThe Multilingual Amazon Reviews Corpus Keung et al. (2020) is a large-scale collection of Amazon reviews for multilingual text classification in 6 languages. Different languages are directly gathered from the marketplaces in different countries. The goal is to predict the reviewer's rating on the 5-star scale using the test of the review as input. The data is clearly split into training (200,000 reviews), development (5,000 reviews), and test sets (5,000 reviews) for each language.

Paws-xThe Cross-lingual Paraphrase Adversaries from Word Scrambling Yang et al. (2019) dataset requires identifying whether two sentences are paraphrases. A subset of the evaluation pairs in English PAWS Zhang et al. (2019) are human-translated into 6 typologically distinct languages for evaluation, while the English PAWS training set is used for training.

[MISSING_PAGE_FAIL:24]

Equations and Theoretical Analysis

### Details of Equations

Details of Gaussian Form \(\mathcal{N}_{r}\)In Emma-X, GMM classifier is introduced to determine the semantic rank of sentence pairs. The posterior probability \(P_{\mathcal{G}}(\cdot)\) of GMM classifier is already discussed in Eq. 5. We show the explicit calculation of Gaussian form \(\mathcal{N}_{r}(\gamma^{(\mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})})\) as:

\[\mathcal{N}_{r}(\gamma^{(\mathbf{x}_{i})}-\gamma^{(\mathbf{y}_{k})}|\mu_{r}, \sigma_{r})=\frac{\pi_{r}}{(2\pi)^{(d/2)}|\text{diag}(\sigma_{r})|}\cdot e^{ \left(-\frac{1}{2}\left[\left(\gamma^{(\mathbf{x}_{i})}-\gamma^{(\mathbf{y}_{k} )}\right)-\mu_{r}\right]^{T}\text{diag}(\sigma_{r}^{-2})\left[\left(\gamma^{( \mathbf{x}_{i})}-\gamma^{(\mathbf{y}_{k})}\right)-\mu_{r}\right]\right)}, \tag{19}\]

where \(d\) is the dimension of hidden states of \(\gamma^{(\mathbf{x}_{i})}\) and \(\gamma^{(\mathbf{y}_{k})}\).

Details of contrastive learningThe training objective of cross-lingual encoder in Emma-X is the ranking InfoNCE loss. We show the explicit expansion of this loss (Eq. 7) as:

\[\mathcal{L}_{\mathbf{CTL}}(\mathcal{X},\mathcal{Y};\Theta_{ \mathcal{M}})=-\mathbb{E}_{\mathbf{x}_{i}\sim\mathcal{X}}\Bigg{[} \tag{20}\] \[\underbrace{\log\frac{\sum_{\mathbf{y}_{k}\sim\mathcal{Y}_{ \mathcal{C}^{\ast}_{0}=1}}e^{\frac{\frac{\mu_{1}(\mathbf{x}_{i})}{\gamma^{( \mathbf{y}_{k})}}\gamma^{(\mathbf{y}_{k})}}{\tau_{1}}}}{\sum_{\mathbf{y}_{i} \sim\mathcal{Y}_{\mathcal{C}^{\ast}_{0}=1}}e^{\frac{\frac{\mu_{1}(\mathbf{x}_{ i})}{\gamma^{(\mathbf{y}_{i})}}\gamma^{(\mathbf{y}_{i})}}{\tau_{1}}}+\sum_{ \mathbf{y}_{i}\sim\mathcal{Y}_{\mathcal{C}^{\ast}_{0}=2}}e^{\frac{\frac{\mu_{1 }(\mathbf{x}_{i})}{\gamma^{(\mathbf{y}_{i})}}\gamma^{(\mathbf{y}_{i})}}{\tau_{ 1}}+...+\sum_{\mathbf{y}_{i}\sim\mathcal{Y},\mathcal{Y}_{\mathcal{C}^{\ast}_{0 }=4}}e^{\frac{\mu_{1}(\mathbf{y}_{i})}{\gamma^{(\mathbf{y}_{i})}}{\tau_{1}}}}}}_{ \ell_{1}}\] \[+\underbrace{\log\frac{\sum_{\mathbf{y}_{k}\sim\mathcal{Y}_{ \mathcal{C}^{\ast}_{0}=2}}e^{\frac{\mu_{1}(\mathbf{x}_{i})}{\gamma^{(\mathbf{y }_{i})}}\gamma^{(\mathbf{y}_{i})}}{\tau_{2}}}{\sum_{\mathbf{y}_{i}\sim\mathcal{ Y}_{\mathcal{C}^{\ast}_{0}=2}}e^{\frac{\mu_{1}(\mathbf{x}_{i})}{\gamma^{(\mathbf{y}_{i})}} \gamma^{(\mathbf{y}_{i})}}{\tau_{2}}+\sum_{\mathbf{y}_{i}\sim\mathcal{Y}_{ \mathcal{C}^{\ast}_{0}=3}}e^{\frac{\mu_{1}(\mathbf{x}_{i})}{\gamma^{(\mathbf{y }_{i})}}{\tau_{2}}}+\sum_{\mathbf{y}_{i}\sim\mathcal{Y}_{\mathcal{C}^{\ast}_{0} =4}}e^{\frac{\mu_{1}(\mathbf{x}_{i})}{\gamma^{(\mathbf{y}_{i})}}{\tau_{2}}}}}_{ \ell_{2}}\] \[+\underbrace{\log\frac{\sum_{\mathbf{y}_{k}\sim\mathcal{Y}_{ \mathcal{C}^{\ast}_{0}=3}}e^{\frac{\mu_{1}(\mathbf{y}_{i})}{\gamma^{(\mathbf{y }_{i})}}{\tau_{3}}}}{\sum_{\mathbf{y}_{i}\sim\mathcal{Y}_{\mathcal{C}^{\ast}_{0 }=3}}e^{\frac{\mu_{1}(\mathbf{y}_{i})}{\gamma^{(\mathbf{y}_{i})}}{\tau_{3}}}+ \sum_{\mathbf{y}_{i}\sim\mathcal{Y}_{\mathcal{C}^{\ast}_{0}=4}}e^{\frac{\mu_{ 1}(\mathbf{y}_{i})}{\tau_{3}}\gamma^{(\mathbf{y}_{i})}}{\tau_{3}}}}_{\ell_{3}} \Bigg{]},\]

where \(\tau_{r}\) represents the temperature term. As small temperature \(\tau\) tends to be less tolerant to similar samples, and large \(\tau\) tends to cluster similar samples together (Wang and Liu, 2021), we empirically set \(\tau_{1}<\tau_{2}<\tau_{3}<\tau_{4}\), which remains the same as Hoffmann et al. (2022).

### Theoretical Analysis

In this section, we provide detailed proof for Eq. 14 and Eq. 15. Next, we prove the feasibility of our dual supervision. GMM classifier clusters sentence pairs in terms of Euclidean distance, while cross-lingual encoder minimizes the covariance of each semantic relation rank via cosine distance. Finally, we prove that these two metrics are actually equivalent to each other in the unit hypersphere of the embedding space.

Proof of Eq. 14.We provide the derivation of Eq. 14. With the assumption that \(P(\mathbf{x}_{i},\mathbf{y}_{k}|c^{*}_{\mathcal{G}}=r,\Theta)\sim\mathcal{N}_{r} \big{(}\mathbf{x}_{i}-\mathbf{y}_{k}|\tilde{\mu}_{r},\tilde{\sigma}_{r}\big{)}\), we have,

\[\sum_{\mathbf{x}_{i}\in\mathcal{X}}\sum_{\mathbf{y}_{k}\in\mathcal{Y }}\sum_{r=1}^{N}Q(r)\log\frac{P(\mathbf{x}_{i},\mathbf{y}_{k},r|\Theta)}{Q(r)} \approx\sum_{\mathbf{x}_{i}\in\mathcal{X}}\sum_{\mathbf{y}_{k}\in \mathcal{Y}}\sum_{r=1}^{N}\log P(\mathbf{x}_{i},\mathbf{y}_{k}|c^{*}_{\mathcal{ G}}=r,\Theta) \tag{21}\] \[=\sum_{\mathbf{x}_{i}\in\mathcal{X}}\sum_{\mathbf{y}_{k}\in \mathcal{Y}}\sum_{r=1}^{N}\Big{(}\log\big{(}\frac{1}{(2\pi)^{(d/2)}|\tilde{ \sigma}_{r}|^{1/2}}\big{)}\] \[+\frac{1}{2}\big{[}(\mathbf{x}_{i}-\mathbf{y}_{k})-\tilde{\mu}_{r }\big{]}^{T}\tilde{\sigma}_{r}^{-1}\big{[}(\mathbf{x}_{i}-\mathbf{y}_{k})- \tilde{\mu}_{r}\big{]}\Big{)}\] \[\geq\sum_{r=1}^{N}\big{[}\sum_{\mathbf{x}_{i}\in\mathcal{X}}\sum _{\mathbf{y}_{k}\in\mathcal{Y}}(\mathbf{x}_{i}-\mathbf{y}_{k})\big{]}^{2}-2 \tilde{\mu}_{r}\sum_{\mathbf{x}_{i}\in\mathcal{X}}\sum_{\mathbf{y}_{k}\in \mathcal{Y}}(\mathbf{x}_{i}-\mathbf{y}_{k})+n\tilde{\mu}_{r}^{2}\] \[=\sum_{r=1}^{N}n^{2}\tilde{\mu}_{r}^{2}-n\tilde{\mu}_{r}^{2}\] \[=n(n-1)\sum_{r=1}^{N}\tilde{\mu}_{r}^{2},\]

with \(n\) denoting the number of sentence pairs in semantic rank \(r\). Here, we ignore the impact of \(\tilde{\sigma}_{r}\).

Proof of Eq. 15.As we apply dual supervision, data in the contrastive label space also follows the distribution \(\mathcal{N}_{r}\big{(}\mathbf{x}_{i}-\mathbf{y}_{k}|\tilde{\mu}_{r},\tilde{ \sigma}_{r}\big{)}\). Hence, under mild assumptions, we can get:

\[\mathcal{L}^{+}_{\mathbf{CTL}}(\mathcal{X},\mathcal{Y};\Theta_{ \mathcal{M}}) =\mathbb{E}_{\mathbf{x}_{i}\sim\mathcal{X}}\sum_{r=1}^{N-1}\log \sum_{\mathbf{y}_{k}\sim\mathcal{Y}_{c^{*}_{\mathcal{G}}=r}^{\Theta(\gamma^{( \mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})})}}e^{s_{\mathcal{G}}^{1}(\gamma^{( \mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})})} \tag{22}\] \[=\sum_{\mathbf{x}_{i}\in\mathcal{X}}\sum_{\mathbf{y}_{k}\in \mathcal{Y}}\sum_{r=1}^{N-1}s(\mathbf{x}_{i},\mathbf{y}_{k})\] \[=\sum_{\mathbf{x}_{i}\in\mathcal{X}}\sum_{\mathbf{y}_{k}\in \mathcal{Y}}\sum_{r=1}^{N-1}\frac{(\mathbf{x}_{i}-\mathbf{y}_{k})^{2}-2}{2}\] \[=n^{2}\sum_{r=1}^{N-1}\tilde{\mu}_{r}^{2}.\]

Based on the definition of semantic ranks, we have \(\tilde{\mu}_{1}<\tilde{\mu}_{2}<...<\tilde{\mu}_{N}\). Empirically, the number of sentence pairs in each rank \(n\) is larger than the number of semantic ranks \(N\). Hence, it can be derived that:

\[\mathcal{L}^{+}_{\mathbf{CTL}}(\mathcal{X},\mathcal{Y};\Theta_{ \mathcal{M}}) =n^{2}\sum_{r=1}^{N-1}\tilde{\mu}_{r}^{2} \tag{23}\] \[<n^{2}\sum_{r=1}^{N-1}\tilde{\mu}_{r}^{2}+n^{2}\tilde{\mu}_{N}^{2 }-n\sum_{r=1}^{N}\tilde{\mu}_{r}^{2}\] \[=n(n-1)\sum_{r=1}^{N}\tilde{\mu}_{r}^{2}\] \[\leq\sum_{\mathbf{x}_{i}\in\mathcal{X}}\sum_{\mathbf{y}_{k}\in \mathcal{Y}}\sum_{r=1}^{N}Q(r)\log\frac{P(\mathbf{x}_{i},\mathbf{y}_{k},r| \Theta)}{Q(r)}.\]

Therefore, we prove that minimizing the positive terms \(\mathcal{L}^{+}_{\mathbf{CTL}}(\mathcal{X},\mathcal{Y};\Theta_{\mathcal{M}})\) in contrastive learning is equivalent to maximizing a lower bound of the likelihood in Eq. 12.

According to the definition of semantic ranks, the approximated semantic rank \(c^{*}_{\mathcal{G}}\) from GMM classifier should satisfy the following restriction,

\[\mathbb{E}_{\mathbf{y}_{k}\sim\mathcal{Y}_{c^{*}_{\mathcal{G}}=1}}||\gamma^{( \mathbf{x}_{i})}-\gamma^{(\mathbf{y}_{k})}||<\mathbb{E}_{\mathbf{y}_{k}\sim \mathcal{Y}_{c^{*}_{\mathcal{G}}=2}}||\gamma^{(\mathbf{x}_{i})}-\gamma^{( \mathbf{y}_{k})}||<...<\mathbb{E}_{\mathbf{y}_{k}\sim\mathcal{Y}_{c^{*}_{ \mathcal{G}}=N}}||\gamma^{(\mathbf{x}_{i})}-\gamma^{(\mathbf{y}_{k})}||. \tag{24}\]Similarly, the approximated semantic rank \(c^{*}_{\mathcal{M}}\) from cross-lingual encoder should satisfy the following restriction,

\[\mathbb{E}_{\mathcal{Y}_{k}\sim\mathcal{Y}_{c^{*}_{\mathcal{M}}=1}^{c}s[\gamma^{ (\mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})}]}>\mathbb{E}_{\mathcal{Y}_{k}\sim \mathcal{Y}_{c^{*}_{\mathcal{M}}=2}^{c}s[\gamma^{(\mathbf{x}_{i})},\gamma^{( \mathbf{y}_{k})}]}>...>\mathbb{E}_{\mathcal{Y}_{k}\sim\mathcal{Y}_{c^{*}_{ \mathcal{M}}=N}}s[\gamma^{(\mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})}]. \tag{25}\]

Next, we prove that these two restrictions are interchangeable with each other in a unit hypersphere. For simplicity, we consider only two ranks, but extending the explanation to more ranks is trivial. As the Euclidean distance is always larger than \(0\), we have:

\[\mathbb{E}_{\mathcal{Y}_{k}\sim\mathcal{Y}_{c^{*}_{\mathcal{Q}}=1} }||\gamma^{(\mathbf{x}_{i})}-\gamma^{(\mathbf{y}_{k})}|| <\mathbb{E}_{\mathcal{Y}\sim\mathcal{Y}_{c^{*}_{\mathcal{Q}}=2} }||\gamma^{(\mathbf{x}_{i})}-\gamma^{(\mathbf{y}_{k})}|| \tag{26}\] \[\Leftrightarrow\mathbb{E}_{\mathcal{Y}_{k}\sim\mathcal{Y}_{c^{*}_ {\mathcal{Q}}=1}^{c}}(\gamma^{(\mathbf{x}_{i})}-\gamma^{(\mathbf{y}_{k})})^{2 }<\mathbb{E}_{\mathcal{Y}_{k}\sim\mathcal{Y}_{c^{*}_{\mathcal{Q}}=2}}(\gamma^ {(\mathbf{x}_{i})}-\gamma^{(\mathbf{y}_{k})})^{2}\] \[\Leftrightarrow\mathbb{E}_{\mathcal{Y}_{k}\sim\mathcal{Y}_{c^{*}_ {\mathcal{Q}}=1}^{c}}(2-2\gamma^{(\mathbf{x}_{i})}\gamma^{(\mathbf{y}_{k})})< \mathbb{E}_{\mathcal{Y}_{k}\sim\mathcal{Y}_{c^{*}_{\mathcal{Q}}=2}}(2-2\gamma^ {(\mathbf{x}_{i})}\gamma^{(\mathbf{y}_{k})})\] \[\Leftrightarrow\mathbb{E}_{\mathcal{Y}_{k}\sim\mathcal{Y}_{c^{*}_ {\mathcal{Q}}=1}^{c}}s[\gamma^{(\mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})}]> \mathbb{E}_{\mathcal{Y}_{k}\sim\mathcal{Y}_{c^{*}_{\mathcal{Q}}=2}}s[\gamma^{ (\mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})}]\] \[\Leftrightarrow\mathbb{E}_{\mathcal{Y}_{k}\sim\mathcal{Y}_{c^{*}_ {\mathcal{M}}=1}}s[\gamma^{(\mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})}]> \mathbb{E}_{\mathcal{Y}_{k}\sim\mathcal{Y}_{c^{*}_{\mathcal{M}}=2}}s[\gamma^{ (\mathbf{x}_{i})},\gamma^{(\mathbf{y}_{k})}].\]

From the above analyses, we can tell that the approximated semantic rank from one module can provide a reasonable supervision signal to guide the training of the other module. Hence, all sentence pairs will be uniformly distributed according to a unified ranking semantic similarity in the embedding space.

**Basic Prompt for XNLI/ANLI**

**Task Description:** Read the following and determine the relationship between Hypothesis and Premise. Choose relation from "contradiction", "neutral", or "entailment".

**Hypothesis:** Yo... no puedo pensar por que deberias halplarme asi, dijo ella, con menos de lo que le habia asegurado antes.

**Premise:** Ella era una buena amiga de el, por esto le dolia que le halpara asi.

**Basic Prompt for MultiSTS**

**Task Description:** Read the following sentences and measure the real-valued meaning similarity between these two sentences. You can choose the meaning similarity score, ranging from 0 for no meaning overlap to 5 for meaning equivalence.

**Sentence1:** A person is on a baseball team.

**Sentence2:** Fine Person spielt in einem Team Basketball.

**Basic Prompt for QE**

**Task Description:** Read the Source sentence and its Translation, and estimate the quality of the Translation. You can rate the translation from 0-1 according to the perceived translation quality.

**Source:** In Franta a incept stagnarea demografica de lunga durata, refacerea durand o generatie.

**Translation:** In France, long-term demographic stagnation has started, restoring a generation.

**Basic Prompt for XCOPA**

**Task Description:** Read the Premise and determine which choice is the effect(or cause) of the Premise. Choose from "Choice1" or "Choice2".

**Premise:** Kuki kurukuna wasiman haykurqanku.

**Choice1:** Kuki kurukunaga wasimanta chinkarqanku.

**Choice2:** Kuki kuruqa wasip kurkunta mikhurqanku.

**Basic Prompt for MultiEURLEX**

**Task Description:** Read the following sentences and determine the legal topic of the given sentence. The legal topic should choose from 'international organisations','social questions', 'production', 'technology and research', 'environment', 'energy', 'transport', 'law', 'finance', 'education and communications', 'trade', 'agriculture', 'forestry and fisheries', 'economics', 'agri-foodstuffs', 'EUROPEAN UNION','science', 'politics', 'international relations', 'industry', 'geography', 'business and competition', 'employment and working conditions'.

**Sentence:** NEUVOSTON ASETUS (EU) No 1390/2013, annettu 16 paivanai joulukuuta 2013, Euroopan unionin ja Komorien liiton kesken naden valiessa kalastuskumppanuussopimukessa maaratyjen kalastusmahddollisuuksien ja taloudellissen korvauksen vahvistamisesta hyuksytyn poytakirjan mukaisten kalastusmahddollisuuksien jakamisesta...

**Basic Prompt for MultiARC**

**Task Description:** Read the following review and predict a 5-star scale rating (1 means the poorest experience and 5 represents excellent or outstanding performance) that can best match the review.

**Review:** no me lego el articulo me lo mando por correos normal sin seguimiento y nunca me llego tota un desastre

**Basic Prompt for PAWS-X**

**Task Description:** Read the following sentences and determine whether two sentences are paraphrases. Return yes or no.

**Sentence1:** La excepcion fue entre fines de 2005 y 2009 cuando jugo en Suecia con Carlstad United BK, Serbia con FK Borac Cacak y el FC Terek Grozny de Rusia.

**Sentence2:** La excepcion se dio entre fines del 2005 y 2009, cuando jugo con Suecia en el Carlstad United BK, Serbia con el FK Borac Cacak y el FC Terek Grozny de Rusia.

\begin{table}
\begin{tabular}{|l|} \hline \hline
**Basic Prompt for XNLI/ANLI** \\ \hline \hline
**Task Description:** Read the following and determine the relationship between Hypothesis and Premise. Choose relation from “contradiction”, “neutral”, or “entailment”. \\ \hline
**Hypothesis:** Yo... no puedo pensar por que deberias halplarme asi, dijo ella, con menos de lo que le habia asegurado antes. \\ \hline
**Premise:** Ella era una buena amiga de el, por esto le dolia que le halpara así. \\ \hline
**Basic Prompt for MultiSTS** \\ \hline \hline
**Task Description:** Read the following sentences and measure the real-valued meaning similarity between these two sentences. You can choose the meaning similarity score, ranging from 0 for no meaning overlap to 5 for meaning equivalence. \\ \hline
**Sentence1:** A person is on a baseball team. \\
**Sentence2:** Fine Person spielt in einem Team Basketball. \\ \hline
**Basic Prompt for QE** \\ \hline
**Task Description:** Read the Source sentence and its Translation, and estimate the quality of the Translation. You can rate the translation from 0-1 according to the perceived translation quality. \\ \hline
**Source:** In Franta a incept stagnarea demografica de lunga durata, refacerea durand o generatie. \\ \hline
**Translation:** In France, long-term demographic stagnation has started, restoring a generation. \\ \hline
**Basic Prompt for XCOPA** \\ \hline
**Task Description:** Read the Premise and determine which choice is the effect(or cause) of the Premise. Choose from “Choice1” or “Choice2”. \\ \hline
**Premise:** Kuki kurukuna wasiman haykurqanku. \\
**Choice1:** Kuki kurukunaga wasimanta chinkarqanku. \\
**Choice2:** Kuki kuruqa wasip kurkunta mikhurqanku. \\ \hline
**Basic Prompt for MultiEURLEX** \\ \hline \hline
**Task Description:** Read the following sentences and determine the legal topic of the given sentence. The legal topic should choose from “international organisations’, ‘social questions’, ‘production’, ‘technology and research’, ‘environment’, ‘energy’, ‘transport’, ‘law’, ‘finance’, ‘education and communications’, ‘trade’, ‘agriculture’, ‘forestry and fisheries’, ‘economics’, ‘agri-foodstuffs’, ‘EUROPEAN UNION’, ‘science’, ‘politics’, ‘international relations’, ‘industry’, ‘geography’, ‘business and competition’, ‘employment and working conditions’. \\ \hline
**Sentence:** NEUVOSTON ASETUS (EU) No 1390/2013, annettu 16 paivanai joulukuuta 2013, Euroopan unionin ja Komorien liiton kesken näiden valiessa kalastuskumppanuussopimukessa maaratyjen kalastusmahddollisuuksien ja taloudellissen korvauksen vahvistamisesta hyuksytyn poytakirjan mukaisten kalastusmahddollisuuksien jakamisesta... \\ \hline
**Basic Prompt for MultiARC** \\ \hline \hline
**Task Description:** Read the following review and predict a 5-star scale rating (1 means the poorest experience and 5 represents excellent or outstanding performance) that can best match the review. \\ \hline
**Review:** no me lego el articulo me lo mando por correos normal sin seguimiento y nunca me llego tota un desastre \\ \hline
**Basic Prompt for PAWS-X** \\ \hline
**Task Description:** Read the following sentences and determine whether two sentences are paraphrases. Return yes or no. \\ \hline
**Sentence1:** La excepcion fue entre fines de 2005 y 2009 cuando jugo en Suecia con Carlstad United BK, Serbia con FK Borac Cacak y el FC Terek Grozny de Rusia. \\ \hline
**Sentence2:** La excepcion se dio entre fines del 2005 y 2009, cuando jugo con Suecia en el Carlstad United BK, Serbia con el FK Borac Cacak y el FC Terek Grozny de Rusia. \\ \hline \hline \end{tabular}
\end{table}
Table 13: Prompts of ChatGPT on each task.

[MISSING_PAGE_FAIL:29]