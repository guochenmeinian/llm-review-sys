# Learning Temporal Higher-order Patterns to Detect Anomalous Brain Activity

Ali Behrouz

Cornell University

Ithaca, NY

ab2947@cornell.edu &Farmoosh Hashemi

Cornell University

Ithaca, NY

sh2574@cornell.edu

###### Abstract

Due to recent advances in machine learning on graphs, representing the connections of the human brain as a network has become one of the most pervasive analytical paradigms. However, most existing graph machine learning-based methods suffer from a subset of five critical limitations: They are (1) designed for simple pairwise interactions while recent studies on the human brain show the existence of higher-order dependencies of brain regions, (2) designed to perform on pre-constructed networks from time-series data, which limits their generalizability, (3) designed for classifying brain networks, limiting their ability to reveal underlying patterns that might cause the symptoms of a disease or disorder, (4) designed for learning of static patterns, missing the dynamics of human brain activity, and (5) designed in supervised setting, relying their performance on the existence of labeled data. To address these limitations, we present HADiB, an end-to-end anomaly detection model that automatically learns the structure of the hypergraph representation of the brain from neuroimage data. HADiB uses a tetra-stage message-passing mechanism along with an attention mechanism that learns the importance of higher-order dependencies of brain regions. We further present a new adaptive hypergraph pooling to obtain brain-level representation, enabling HADiB to detect the neuroimage of people living with a specific disease or disorder. Our experiments on Parkinson's Disease, Attention Deficit Hyperactivity Disorder, and Autism Spectrum Disorder show the efficiency and effectiveness of our approaches in detecting anomalous brain activity.

## 1 Introduction

Recent advancements in neuroscience and neuroimaging have led researchers to shift from examining isolated brain regions to exploring network models [1]. This shift is largely attributed to the rapid progress in technologies like functional Magnetic Resonance Imaging (fMRI) and structural Magnetic Resonance Imaging (sMRI) [2], which can provide large-scale neuroimaging data with better quality. In network models of the brain, brain regions of interest (ROIs) are represented as nodes, and the similarities between these regions form edges [3]. Brain network models have demonstrated their effectiveness in enhancing our understanding of brain diseases and disorders [4, 5]. As a result, empirical data on brain networks has substantially increased in size and complexity, leading to a strong demand for appropriate tools and methods to model and analyze this data [5]

Simultaneously, machine learning techniques for analyzing graph-structured data have gained attention across various fields, including drug discovery, neuroscience, and biology [6, 7, 8]. Although numerous studies have confirmed the effectiveness of machine learning for human brain network analysis, most have concentrated on graph or node classification tasks [9, 10]. These tasks usually aim at disease detection [11], biological feature prediction [9], or functional system identification [12].

However, the identification of abnormal brain activity, especially in those with neurological disorders, remains a critical focus for researchers. This is essential for understanding the mechanisms behind symptoms, enabling early detection, and facilitating the development of medical treatments. Most existing studies consider pairwise interaction among brain regions, neglecting the effect of non-pairwise interactions on the emerging dynamics. Recently, several studies have discussed the importance of higher-order correlation of brain regions. Rosenthal et al. [13] show that the network context might not be directly accessible at the level of individual regions, and Santoro et al. [14] show the signatures of higher-order patterns in brain functional activity. Temporal hypergraph representation of the brain from neuroimaging data can address this limitation by capturing both higher-order interactions of brain regions as well as higher-order patterns that are correlated to emerging dynamics of brain activity. Hypergraphs are powerful paradigms to model higher-order interactions, where each connection, also known as hyperedge, can connect a group of nodes at once.

**Limitation of Previous Methods.** The process of brain network analysis generally unfolds in two sequential steps. Initially, brain networks are derived from individual neuroimage data (e.g., fMRI). This typically involves choosing a brain atlas, which identifies specific ROIs to serve as nodes, and edges that show association between ROIs. For example in fMRI data, from each designated brain region, fMRI blood-oxygen-level-dependent (BOLD) signal sequences are then extracted, and the subsequent phase of edge generation involves calculating pairwise connectivity between these nodes, often based on Pearson correlation and/or mutual information. Finally, the connectivity measures established between node pairs are utilized for the downstream tasks. Although this process along with diverse machine learning methods is widely used in the existing literature [9, 15, 16], both the pipeline and existing methods suffer from a subset of the following limitations, which makes directly applying them in real-world scenarios challenging and/or impractical:

1. Capturing linear correlation and ignoring temporal order: Most existing studies assume that the true dependency structure between brain regions is known prior to model training. That is, existing methods often use Pearson correlation and/or mutual information between the signals of brain regions (e.g, BOLD signals), while there is no statistical measure of dependency for truly capturing functional connectivity [17]. Moreover, these statistical correlations focus on capturing linear correlation and ignore temporal order, which means shuffling the time in each time window does not change the results. In order to mitigate the above limitation and to ensure that the model can effectively learn meaningful network representations for use in downstream tasks it is crucial to establish an approach for constructing the dependency structure of the network that accurately reflects the underlying neuroimage data.

2. Missing higher-order dependencies: Existing studies assume that the dependencies of brain regions can be captured by pair-wise interactions, while the signatures of higher-order patterns in brain functional activity have been seen in recent studies [14]. Moreover, the network context might not be directly accessible at the level of individual regions and requires considering higher-order interactions between a group of regions [13, 18]. Accordingly, ignoring the higher-order dependencies of ROIs might result in suboptimal performance.

3. Missing hierarchical structure of the brain: The human brain is comprised of functional systems (FS) [19], which are groups of ROIs that perform similar functions as an integrated network [20]. These communities are essential in understanding the functional organization of the brain [21] as ROIs in the same functional modules tend to have similar behaviors and clustered representations [20]. Most existing studies neglect the hierarchical structure of the brain, leading to missing the functional dependencies of ROIs in the same FS.

4. Missing the dynamics of the network: Some existing studies neglect the fact that the functional connectivity of the human brain dynamically changes over time, even in resting-state neuroimaging data [22]. In task-dependent neuroimage data, subjects are asked to perform different tasks in different time windows, and the dynamics of the brain activity during these tasks play an important role in understanding neurological disease/disorder [23]. Moreover, Liegeois et al. [24] shows that brain dynamics at different timescales capture distinct aspects. Accordingly, mitigating the above limitation requires a model that not only captures the dynamics of the brain activity but it also can capture its dynamics at different timescales.

5. Designed for classification tasks: Most existing studies have focused on semi-supervised/supervised classification tasks, e.g., detecting diseases [11], predicting biological features [9], or identifying functional systems [12]. This setting not only relies their performance on the existence of labeled data, but it also limits their ability to reveal underlying patterns that might cause the symptoms of a disease or disorder. That is, a black-box classification method only predicts the label for a given brain network while understanding the cause of the brain disease/disorder (i.e., detecting abnormal brain activity) is a crucial step in facilitating early detection, and developing medical treatments.

**Contributions.** To address all the above limitations, we design HADiB (**H**ypergraph **A**nomaly **D**etection **i** in **B**rain), an end-to-end unsupervised anomaly detection method that can detect anomalous patterns at different level of granularity. HADiB first uses a novel hierarchical multivariate time-series encoder that can capture both cross-time and cross-ROI dependencies of ROIs' signals at different time scales. Next, to mitigate **I** and **2**, HADiB employs a temporal hypergraph constructor that learns the temporal higher-order dependencies between ROIs' signals. To address the **3** and **4**, HADiB uses a tetra-stage message passing at different levels of granularity to learn the ROI, functional system, and brain encodings, taking advantage of the hierarchical structure of the brain. Experimental evaluation supports the need for considering higher-order patterns and shows the superior performance of HADiB over baselines, as well as the importance of HADiB's critical components. Finally, using real-world case studies we show how HADiB can be used to detect abnormal brain regions or functional systems in a control group with brain disease or disorder.

## 2 Related Work

**Anomaly Detection in Hypergraphs and Time Series.** The problem of anomaly detection, which aims to detect abnormal nodes, edges, or subgraphs within a graph, has been extensively studied for both static and temporal graphs [25; 26]. Surprisingly, hypergraph anomaly detection is relatively unexplored. Park et al. [27] uses scan statistics to detect anomalous nodes. Leontjeva et al. [28] suggest taking advantage of the structural features of the nodes to detect anomalies. Recently, Lee et al. [29] designed a fast and effective algorithm based on a proximity matrix to detect abnormal nodes. Moreover, several studies use hypergraphs to learn higher-order patterns in time series forecasting [27; 30]. However, all these methods **I** are designed for general applications and cannot take advantage of special properties of the brain, **2** cannot detect anomalies at different levels of granularity (e.g., hyperedge, node, etc.).

**Machine Learning and Anomaly Detection in Brain Networks.** Several studies have recently analyzed brain networks to differentiate between healthy and diseased human brains [31; 32; 33]. With the success of graph neural networks in graph data analysis, deep learning models have been developed to predict brain diseases by studying brain network structures [34; 15; 35; 36; 11; 10]. However, these models focus on graph or node classification and aren't directly suited for anomaly detection. There are also methods aimed at detecting anomalies in brain regions or subgraphs, which could signal disease presence [4; 37; 38]. These studies adopt non-learning approaches and rely on pre-defined patterns or rules for detecting anomalies. Due to the complex nature of brain activity, it is unrealistic to assume that all abnormal brain activities follow the same pre-define pattern or rule. To address this limitation and to learn the abnormal patterns in the human brain from data, recently, Behrouz and Seltzer [39] proposed ADMire, an unsupervised anomaly detection method that uses multiplex random walks to extract networks motif and to detect anomalous patterns in the brains of people living with a disease or disorder, accordingly.

Figure 1: **Schematic of the HADiB. HADiB consists of three stages: (1) Feature Extraction, (2) Hypergraph Construction, and (3) Hypergraph Learning.**

All these methods target anomaly detection in brain networks with pairwise interactions, missing temporal higher-order patterns that play important roles in understanding the network context and its dynamics over time [13; 14]. Moreover, they do not take advantage of the hierarchical structure of the brain, missing the dependencies of ROIs' activity in the same functional systems [20; 21].

## 3 Methods

Each neuroimage data (e.g., fMRI or EEG) can be represented as a multivariate timeseries \(\mathbf{X}_{1:T}=(\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\in\mathbb{R}^{n\times T}\), where \(n\) is the number of ROIs. Next, we formally define temporal hypergraphs:

**Definition 1** (Temporal Hypergraphs).: _A temporal hypergraph \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{X})\), can be represented as a sequence of snapshots \(\mathcal{G}=\{\mathcal{G}^{(t)}\}_{t=1}^{T}\) that arrive over time, where \(\mathcal{G}^{(t)}=(\mathcal{V},\mathcal{E}^{(t)},\mathcal{X}^{(t)})\) is the \(t\)-th snapshot, \(\mathcal{V}\) is the set of nodes, \(e_{i}\in\mathcal{E}^{(t)}\subseteq 2^{\mathcal{V}}\) are hyperedges, and \(\mathcal{X}^{(t)}\in\mathbb{R}^{|\mathcal{V}|\times f}\) is a matrix that encodes node attribute information for nodes in \(\mathcal{V}\). Note that we treat each hyperedge \(e_{i}\) as the set of all vertices connected by \(e_{i}\)._

Given a hypergraph \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{X})\), we represent the set of hyperedges attached to a node \(u\) before time \(t\) as \(\mathcal{E}^{(t)}(u)=\{(e,t^{\prime})|t^{\prime}<t,u\in e\}\). We say two hyperedges \(e\) and \(e^{\prime}\) are adjacent if \(e\cap e^{\prime}\neq\emptyset\) and use \(\mathcal{E}^{t}(e)=\{(e^{\prime},t^{\prime})|t^{\prime}<t,e^{\prime}\cap e\neq\emptyset\}\) to represent the set of hyperedges adjacent to \(e\) before time \(t\). Each hypergraph snapshot \(\mathcal{G}^{(t)}\) can be represented by an adjacency matrix \(\mathcal{A}^{(t)}\in\{0,1\}^{|\mathcal{V}|\times|\mathcal{E}^{(t)}|}\) such that \(\mathcal{A}^{(t)}_{u,e}=1\) if \(u\in e\) and \(\mathcal{A}^{(t)}_{u,e}=0\) otherwise.

We focus on the problem of anomaly detection in the human brain at three levels of granularity: 1 Hyperedge anomaly detection, 2 ROI anomaly detection, and 3 FS/Brain anomaly detection.

### HADiB: An Unsupervised Anomaly Detection Method in Human Brain

In this section, we design HADiB, an unsupervised anomaly detection method for the human brain, and discuss each of its components in detail. Figure 1 illustrates the overview of HADiB architecture.

**Hierarchical Feature Extraction from Time Series.** The Time Mixer module aims to encode the multivariate time series data at different time scales. To this end, we first map the input sequence \(\mathbf{X}_{1:T}\) into a hierarchical series \(\mathbb{X}=\{\mathbf{X}^{(s)}\}_{s\in\mathcal{S}}\) such that \(\mathbf{X}_{1:T_{s}}^{(s)}=(\mathbf{x}_{1}^{(s)},\ldots,\mathbf{x}_{T_{s}}^{(s )})\in\mathbb{R}^{n\times T_{s}}\), \(\mathcal{S}\) is the set of all scales, and each \(s\in\mathcal{S}\) is a time window of \(\mathbf{X}^{(s)}\) (i.e., \(T_{s}=\lceil\frac{T}{s}\rceil\)). At each time scale \(s\in\mathcal{S}\), we need to capture both cross-ROI and cross-time dependencies. Accordingly, inspired by Mlp-Mixer[40], we use a Time-Mixer and ROI-Mixer modules to capture cross-time and cross-ROI dependencies, respectively. In the Time-Mixer module, we have:

\[\mathbf{H}_{\text{TIME}}^{(s)}=\mathbf{X}^{(s)}+\mathbf{W}_{\text{TIME}}^{(1)^ {(s)}}\sigma\left(\mathbf{W}_{\text{TIME}}^{2^{(s)}}\text{LayerNorm}\left( \mathbf{X}^{(s)}\right)\right),\]

where \(\mathbf{W}_{\text{TIME}}^{(1)^{(s)}}\) and \(\mathbf{W}_{\text{TIME}}^{(2)^{(s)}}\) are learnable metrics, \(\text{LayerNorm}(.)\) is layer-normalization [41], and \(\sigma(.)\) is a non-linearity (e.g., GeLU [42]). To capture cross-ROI dependencies, we use ROI-Mixer as follows:

\[\mathbf{H}_{\text{ROI}}^{(s)} =\mathbf{H}_{\text{TIME}}^{(s)}\] \[+\sigma\left(\text{LayerNorm}\left(\mathbf{H}_{\text{TIME}}^{(s)} \right)\mathbf{W}_{\text{ROI}}^{(1)^{(s)}}\right)\mathbf{W}_{\text{ROI}}^{(2)^{ (s)}}, \tag{1}\]

where \(\mathbf{W}_{\text{ROI}}^{(1)^{(s)}}\) and \(\mathbf{W}_{\text{ROI}}^{(2)^{(s)}}\) are learnable metrics. Note that, while the feature extractor consists of simple all-MLP Time- and ROI-Mixer modules, its architecture (i.e., capturing cross-time and cross-ROI dependencies) make it a powerful model (see SS 4).

Finally, we project \(\mathbf{H}_{\text{ROI}}^{(s)}\) to have the same size encoding for different time scales:

\[\mathbf{H}_{\text{PROI}}^{(s)}=\text{MLP}\left(\mathbf{H}_{\text{ROI}}^{(s)} \right). \tag{2}\]

The above procedure encodes the signals at different time scales separately. Accordingly, it does not take advantage the complementary information provided by different time scales [24]. To address this challenge, we concatenate the \(\mathbf{H}_{\text{Proj}}^{(s)}\) for different \(s\in\mathcal{S}\) in the order of the time scale, and use an Mlp-Mixer module [40] to combine the encodings:

\[\mathbf{H}_{\text{F}}=\text{Mlp-Mixer}\left(\bigoplus_{s\in\mathcal{S}}\mathbf{H }_{\text{Proj}}^{(s)}\right), \tag{3}\]

where \(\bigoplus\) denotes concatination along the ROI dimension (i.e., \(\mathbf{H}_{\text{F}}\in\mathbb{R}^{n\times d}\)).

**Temporal Hypergraph Construction.** To capture dependencies between brain regions, while considering the extracted temporal features, we use a self-attention mechanism to learn temporal hypergraph representation of the brain, where each node is a ROI and each hyperedge represents the association of a group of nodes. As discussed in SS1, the connectivity of the human brain dynamically changes over time [22]. For example in task-dependent fMRI, subjects are asked to perform a task in each time window. To this end, we use \(s_{\max}=\max\{s\in\mathcal{S}\}\), as the time window to construct different snapshots of the network. Accordingly, using zero-padding, we use the above feature extraction procedure for \(\mathbf{X}_{1:(t+1)s_{\max}}\) to obtain \(\mathbf{H}_{\text{F}}^{(t)}\) for all \(0\leq t\leq\lfloor\frac{T}{s_{\max}}\rfloor\). Next, to construct the \(t\)-th snapshot of the hypergraph, we use modified self-attention mechanism [43] as follows:

\[\mathcal{A}^{(t)}=\text{Sigmoid}\left(\frac{Q^{(t)}K^{(t)}}{\sqrt{K^{(T)^{ \top}}}}\right), \tag{4}\]

where \(Q^{(t)}=\mathbf{H}_{\text{F}}^{(t)}\mathbf{W}_{\text{ATTN}}^{(1)}\), \(K^{(t)}=\mathbf{H}^{(t)}_{\text{F}}^{\top}\mathbf{W}_{\text{ATTN}}^{(2)}\), and \(\mathbf{W}_{\text{ATTN}}^{(1)}\) and \(\mathbf{W}_{\text{ATTN}}^{(2)}\) are learnable matrices. We interpret \(\mathcal{A}^{(t)}\) as the adjacency matrix of the hypergraph representation of the brain in the \(t\)-th snapshot. Inspired by results reported by Said et al. [44], we use \(\mathcal{X}^{(t)}=\mathbf{H}_{\text{F}}^{(t)}\) as the node features.

**Hypergraph Message-Passing.** After constructing the temporal hypergraph, to learn the dependencies between temporal patterns of different scales, we propose a tetra-stage message passing mechanism, which contains 1 ROI-ROI, 2 ROI-Hyperedge, 3 Hyperedge-FS, and 4 FS-FS.

1 ROI-ROI phase: To learn the local dependencies of ROIs, we iteratively aggregate messages from the local neighborhood of ROIs. Given \(t\)-th snapshot, if \(u\) and \(v\) are in a hyperedge \(e\in\mathcal{E}^{(t)}\), then we update ROI encodings in \(\ell\)-th layer of neural network as follows:

\[m_{u\to v}^{(\ell)} =\mathbf{W}_{\text{ROI}}^{(\ell)}\text{Concat}\left(\hat{\mathbf{ h}}_{u}^{(t)^{(\ell-1)}},\hat{\mathbf{h}}_{v}^{(t)^{(\ell-1)}},\zeta_{e}^{(t)^{( \ell-1)}}\right),\] \[\hat{\mathbf{h}}_{u}^{(t)^{(\ell)}} =\text{Sum}\left(\left\{m_{v\to u}^{(\ell)}|v\in\mathcal{N}^{(t)}(u) \right\}\right)+\hat{\mathbf{h}}_{u}^{(t)^{(\ell-1)}}, \tag{5}\]

where \(\mathbf{W}_{\text{ROI}}^{(\ell)}\) is a learnable matrix, \(\zeta_{e}^{(t)^{(\ell-1)}}\) is the encoding (strength) of hyperedge \(e\) (we discuss and compute it in 2), and \(\mathcal{N}^{(t)}(u)\) is the set of nodes that are connected to \(u\) by at least a hyperedge in \(t\)-th snapshot. We initialize the \(\hat{\mathbf{h}}_{u}^{(t)^{(0)}}=\mathbf{h}_{F}^{(t)}(u)\), where \(\mathbf{h}_{F}^{(t)}(u)\) is the corresponding row of matrix \(\mathbf{H}_{F}^{(t)}\) for \(u\).

3 ROI-Hyperedge phase: To obtain the encoding of a hyperedge, one can simply consider the summation of all ROI's encoding connected by the hyperedge. However, the strength of each dependencies for each ROI is different. Accordingly, we use the attention mechanism in Equation 4 to learn the importance of each hyperedge for each node (ROI) in the hypergraph. Let \(\zeta_{e}^{(t)}\) be the encoding of hyperedge \(e\in\mathcal{E}^{(t)}\):

\[\zeta_{e}^{(t)^{(\ell)}}=\sum_{u\in e}\mathcal{A}_{u,e}^{(t)}\hat{\mathbf{h}}_ {u}^{(t)^{(\ell)}}, \tag{6}\]

where \(\mathcal{A}_{u,e}^{(t)}\) is computed by Equation 4.

4 Hyperedge-FS phase: We consider each FS as a set of ROIs. Accordingly, to encode an FS from its ROIs' encoding one might suggest aggregating the encoding of its ROIs. However, this aggregation misses the dependencies of ROIs in each FS. To this end, we propose a pooling function \(\text{Pool}(.)\) that aggregates the encodings of hyperedge within each FS. This approach not only incorporates the information from different ROIs, but also considers their dependencies and their roles in the dynamics of the system. Let \(\mathcal{F}\) be the set of all functional systems, for \(f\in\mathcal{F}\) we use \(\mathcal{E}_{f}\) to denote the set of hyperedges within the \(f\) and matrix \(\mathbf{Z}_{f}^{(t)^{(t)}}=\bigoplus_{e\in\mathcal{E}_{f}}\ \ \zeta_{e}^{(t)^{(t)}}\). We compute the FS encoding as:

\[\hat{\mathbf{Z}}_{f}^{(t)^{(t)}}=\textsc{Gru}\left(\mathbf{Z}_{f}^{(t)^{(t)}}, \mathbf{Z}_{f}^{(t-1)^{(t)}}\right)\]

\[\tilde{\mathbf{Z}}_{f}^{(t)^{(t)}}\!\!=\!\hat{\mathbf{Z}}_{f}^{(t)^{(t)}}\!\! +\sigma\left(\texttt{Softmax}\left(\texttt{LayerNorm}\left(\hat{\mathbf{Z}}_{f }^{(t)^{(t)}}\right)^{\!\top}\right)\right)^{\top}\]

\[\psi_{f}^{(t)^{(t)}}=\textsc{Mean}\left(\tilde{\mathbf{Z}}_{f}^{(t)^{(t)}}+ \sigma\left(\texttt{LayerNorm}\left(\tilde{\mathbf{Z}}_{f}^{(t)^{(t)}}\right) \mathbf{W}_{P}^{(1)}\right)\mathbf{W}_{P}^{(2)}\right)\]

where \(\textsc{Mean}(.)\) is mean function along the hyperedge dimension. The above pooling method uses a similar architecture as the feature extractor to capture both cross-hyperedge and cross-feature dependencies. However, there are two main differences: First, to reduce the number of parameters, we bind features in a non-parametric manner using a \(\texttt{Softmax}(.)\) function. Second, the first line, the Gru cell [45], is used to update the encodings over time, capturing the dynamics of the functional systems.

4) FS-FS phase: Finally, we perform structural learning at the level of functional systems to capture the dependencies of brain activity at a higher level and use it to obtain brain-level encoding. To this end, we assume that all functional systems are connected and use a self-attention mechanism [43] to learn the strength of the dependencies of functional systems. Given \(\psi_{f}^{(t)^{(t)}}\) for each \(f\in\mathcal{F}\), let \(\Psi^{(t)^{(t)}}\in\mathbb{R}^{|\mathcal{F}|\times d}\) be the matrix whose rows are \(\psi_{f}^{(t)^{(t)}}\). We use

\[\mathcal{A}_{\mathcal{F}}^{(t)}=\texttt{Sigmoid}\left(\frac{Q_{\mathcal{F}}^{( t)}K_{\mathcal{F}}^{(t)}}{\sqrt{K_{\mathcal{F}}^{(T)^{\top}}}}\right), \tag{7}\]

where \(Q_{\mathcal{F}}^{(t)}=\Psi^{(t)^{(t)}}\mathbf{W}_{\mathcal{F}}^{(1)}\), \(K_{\mathcal{F}}^{(t)}=\Psi^{(t)^{(t)}}\mathbf{W}_{\mathcal{F}}^{(2)}\), and \(\mathbf{W}_{\mathcal{F}}^{(1)}\) and \(\mathbf{W}_{\mathcal{F}}^{(2)}\) are learnable matrices. Accordingly, we can obtain the brain-level encoding as the weighted aggregation of functional system encodings \(\Upsilon^{(t)^{(t)}}=\sum_{f\in\mathcal{F}}\mathcal{A}_{f,f}^{(t)}\psi_{f}^{(t )^{(t)}}\).

**Joint Training.** In the training phase, we want the model to learn to detect abnormal brain activities at the level of higher-order dependencies (i.e., hyperedges), ROIs (i.e., nodes), and function system or Brain. Accordingly, we use contrastive learning to train the model in an unsupervised manner. We generate negative samples at the level of hyperedges and nodes in the hypergraph. We adopt the commonly used negative sample generation method [25; 26] to generate negative hyperedges. That is, for each hyperedge \(e=\{u_{1},u_{2},\ldots,u_{k}\}\) we choose \(\alpha\)% of nodes connected by \(e\) and change them to a randomly selected set of vertices. We use binary cross-entropy loss, \(\mathcal{L}_{\textsc{ENTROPY}}\), to learn hyperedge encodings. To learn the time series encoding, we follow existing studies [46; 47] and replace a brain signal in the time window with another signal that is randomly selected from the batch. We also follow these studies and use the contrastive loss, \(\mathcal{L}_{\textsc{CONTRAST}}\) proposed by Woo et al. [47].

**Loss Function.** As we discussed earlier, ROIs in the same functional systems have similar patterns, and accordingly, it is expected to have similar encodings. To this end, inspired by DGI [48], we maximize the mutual information between ROIs encodings and their corresponding functional system encoding. We refer to this loss function as \(\mathcal{L}_{\textsc{MI}}\). Accordingly, we aim to minimize \(\mathcal{L}=\theta_{1}\mathcal{L}_{\textsc{ENTROPY}}+\theta_{2}\mathcal{L}_{ \textsc{CONTRAST}}-(1-\theta_{1}-\theta_{2})\mathcal{L}_{\textsc{MI}}\).

## 4 Experiments

**Datasets.** We use five real-world datasets: 1 PD [49] consists of the functional MRI images of 25 participants with and 21 participants without PD, who do the ANT task [50]. 2 ADHD [51] contains data for 100 subjects in the ADHD group and 100 subjects in the typically developed (TD) control group. 3 The Seizure detection TUH-EEG dataset [52] consists of EEG data (31 channels) of 642 subjects. 4 ASD [53] contains data for 45 subjects in the ASD group and 45 subjects in the TD control group. 5 ABIDE [54] consists of resting-state functional MRI of 1009 subjects (516 with ASD) parcellated by Craddock 200 atlas. For the first part of the experiment, we follow the methodology used in existing studies [26; 55; 39] and inject 1% and 5% anomalous edges into the brain networks in the control group.

**Baselines.** We compare our HADiB with state-of-the-art methods. In all tasks, we use 

1 Graph-based methods: GOutlier [56], NetWalk[57], BrainGnn[58], BrainNetCnn[59], AD-Mire[39], and BNTransformer[9].

3 Hypergraph-based methods: HyperSAGCN[60], NHP[61], HashNWalk[29].

4 Time-series-based methods: Usad[62] and Mvts[63]. We may exclude some baselines in some tasks as they cannot be applied in that setting.

**Quantitative Results.** In the first experiment, we compare the performance of HADiB with baselines in hyperedge, ROI, and graph anomaly detection tasks. Table 1 reports the the AUC of HADiB and baselines. HADiB outperforms all the baselines in all three tasks. The reason is fourfold:

1 Graph-based methods can only capture the pair-wise dependencies of ROIs, missing higher-order dependencies.

2 Hypergraph-based methods also miss the temporal properties as well as the dynamics of the functional connectivity.

3 HADiB's training strategy and encoding of the brain at different level of granularity can provide complementary information.

4 HADiB is an end-to-end model that simultaneously learns the hypergraph structure and its element (e.g., node, hyperedge)

\begin{table}
\begin{tabular}{l l c c c c c c c c c} \hline \hline \multicolumn{2}{c}{Methods} & \multicolumn{2}{c}{PD} & \multicolumn{2}{c}{ABIDE} & \multicolumn{2}{c}{ADHD} & \multicolumn{2}{c}{TUH-EEG} & \multicolumn{2}{c}{ASD} \\ \cline{3-11} \multicolumn{2}{c}{} & Anomaly \% & 1\% & 5 \% & 1\% & 5 \% & 1\% & 5 \% & 1\% & 5 \% & 1\% & 5 \% \\ \hline \hline \multicolumn{1}{c}{} & \multicolumn{1}{c}{Graph-based Methods} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline \multirow{8}{*}{} & GOutlier & 61.24\({}_{1,41}\) & 59.98\({}_{8,23}\) & 64.08\({}_{1,33}\) & 63.23\({}_{1,41}\) & 63.23\({}_{1,41}\) & 63.27\({}_{1,23}\) & 64.10\({}_{2,42}\) & 65.63\({}_{1,41,33}\) & 64.12\({}_{2,44,33}\) & 60.80\({}

encodings. However, all the baselines use pre-computed functional connectivity, which limits their generalizability.

**Ablation Study.** We conduct ablation studies to validate the effectiveness of HADiB's critical components. Table 2 shows the AUC for the hyperedge anomaly detection task. The first row reports the performance of the complete HADiB implementation. Each subsequent row shows results for HADiB with one module modification: row 1 replaces feature extraction with Pearson's correlation, row 2 considers only one time scale, row 3 removes the self-attention mechanism, row 4 removes Gru cell in the pooling, rows 5-7 removes one of the loss functions at each time, rows 8 replaces the tetra stage messaging with a simple message passing, and finally, the last row converts the hypergraph to a graph. These results show that each component is critical for achieving HADiB's superior performance. The greatest contribution comes from the training process and loss functions, followed by Feature Extraction modules.

## 5 Case Studies

In the following case studies, we train our model on the healthy control group and then test it on the condition group. To this end, we report how anomalous brain activities found by HADiB are distributed in the brains of people living with PD/ADHD/ASD.

**Parkinson's Disease.** Figure 2 reports the distribution of anomalous ROIs within the PD group. A majority (95%) of the identified anomalies by HADiB involve ROIs located in one of the Left Thalamus, Supramarginal Gyrus, Superior Parietal, Medial Orbitofrontal, or Pars Opercularis. Interestingly, these ROIs are correlated with some PD symptoms (e.g., affected motor skills). Also, these results are consistent with previous studies using resting-state fMRI [64, 65].

**Attention Deficit Hyperactivity Disorder.** Figure 2 reports the distribution of anomalous ROIs within the brain networks of the ADHD group. Most found abnormal ROIs (80% of all found anomalies) by HADiB are located in the Left Temporal Pole, Frontal Pole, Left Lateral Occipital, and Lingual Gyrus. These findings are consistent with previous studies on ADHD by using diffusion tensor imaging [66] and Forman-Ricci curvature changes [4].

**Autism Spectrum Disorder.** The distribution of anomalous ROIs within the brain networks of the ASD group is visualized in Figure 2. More than 80% of all found anomalies by HADiB involve ROIs located in the Right Cerebellum Cortex, Left Cerebellum Cortex, Right Cerebellum Cortex, Frontal Pole, Left Lateral Occipital, and Right Superior Temporal Gyrus. Our results on findings of abnormal activity in the cerebellum cortex are consistent with previous studies [67].

## 6 Conclusion

We present HADiB, an end-to-end unsupervised learning method on brain time series data to detect abnormal brain activity at different levels of granularity that might suggest a brain disease or disorder. HADiB uses a hierarchical multivariate time-series encoder to capture both cross-time and cross-ROI dependencies of ROIs' signals at different time scales and employs a temporal hypergraph constructor module on top of that to learn the temporal higher-order dependencies between ROIs. Using a novel hierarchical tetra-stage message passing on the constructed hypergraph, HADiB first learns ROI-level, hyperedge-level, and brain-level encodings and then leverages them to detect abnormal brain activity at different levels. Our experimental results show the superior performance of HADiB against baselines and the potential of HADiB in detecting abnormal brain activity.

Figure 2: The distribution of found anomalies by HADiB in condition groups.

## References

* Bassett and Sporns [2017] Danielle S. Bassett and Olaf Sporns. Network neuroscience. _Nature Neuroscience_, 20(3):353-364, Mar 2017. ISSN 1546-1726. doi: 10.1038/nn.4502. URL [https://doi.org/10.1038/nn.4502](https://doi.org/10.1038/nn.4502).
* Misic and Sporns [2016] Bratislav Misic and Olaf Sporns. From regions to connections and networks: new bridges between brain and behavior. _Current Opinion in Neurobiology_, 40:1-7, 2016. ISSN 0959-4388. doi: [https://doi.org/10.1016/j.conb.2016.05.003](https://doi.org/10.1016/j.conb.2016.05.003). URL [https://www.sciencedirect.com/science/article/pii/S095943881630054X](https://www.sciencedirect.com/science/article/pii/S095943881630054X). Systems neuroscience.
* Finn et al. [2015] Emily S. Finn, Xilin Shen, Dustin Scheinost, Monica D. Rosenberg, Jessica Huang, Marvin M. Chun, Xenophon Papademetris, and R. Todd Constable. Functional connectome fingerprinting: identifying individuals using patterns of brain connectivity. _Nature Neuroscience_, 18(11):1664-1671, Nov 2015. ISSN 1546-1726. doi: 10.1038/nn.4135. URL [https://doi.org/10.1038/nn.4135](https://doi.org/10.1038/nn.4135).
* Chatterjee et al. [2021] Tanima Chatterjee, Reka Albert, Stuti Thapliyal, Nazanin Azarhooshang, and Bhaskar DasGupta. Detecting network anomalies using forman-ricci curvature and a case study for human brain networks. _Scientific Reports_, 11(1):8121, Apr 2021. ISSN 2045-2322. doi: 10.1038/s41598-021-87587-z. URL [https://doi.org/10.1038/s41598-021-87587-z](https://doi.org/10.1038/s41598-021-87587-z).
* Preti et al. [2017] Maria Giulia Preti, Thomas AW Bolton, and Dimitri Van De Ville. The dynamic functional connectome: State-of-the-art and perspectives. _Neuroimage_, 160:41-54, 2017.
* Xiong et al. [2019] Zhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong Li, Zhaojun Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, et al. Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism. _Journal of medicinal chemistry_, 63(16):8749-8760, 2019.
* Abrate and Bonchi [2021] Carlo Abrate and Francesco Bonchi. Counterfactual graphs for explainable classification of brain networks. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery; Data Mining_, KDD '21, page 2495-2504, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383325. doi: 10.1145/3447548.3467154. URL [https://doi.org/10.1145/3447548.3467154](https://doi.org/10.1145/3447548.3467154).
* Gao et al. [2023] Ziqi Gao, Chenran Jiang, Jiawen Zhang, Xiaosen Jiang, Lanqing Li, Peilin Zhao, Huanming Yang, Yong Huang, and Jia Li. Hierarchical graph learning for protein-protein interaction. _Nature Communications_, 14(1):1093, 2023.
* Kan et al. [2022] Xuan Kan, Wei Dai, Hejie Cui, Zilong Zhang, Ying Guo, and Carl Yang. Brain network transformer. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL [https://openreview.net/forum?id=1cJ1cbA6NLN](https://openreview.net/forum?id=1cJ1cbA6NLN).
* Cui et al. [2022] Hejie Cui, Wei Dai, Yanqiao Zhu, Xiaoxiao Li, Lifang He, and Carl Yang. Interpretable graph neural networks for connectome-based brain disorder analysis. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 375-385. Springer, 2022.
* Zhu et al. [2022] Yanqiao Zhu, Hejie Cui, Lifang He, Lichao Sun, and Carl Yang. Joint embedding of structural and functional brain networks with graph neural networks for mental illness diagnosis. In _2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)_, pages 272-276. IEEE, 2022.
* Behrouz and Hashemi [2022] Ali Behrouz and Farnoosh Hashemi. Cs-mlgcn: Multiplex graph convolutional networks for community search in multiplex networks. In _Proceedings of the 31st ACM International Conference on Information and Knowledge Management_, CIKM '22, page 3828-3832, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450392365. doi: 10.1145/3511808.3557572. URL [https://doi.org/10.1145/3511808.3557572](https://doi.org/10.1145/3511808.3557572).

* Rosenthal et al. [2018] Gideon Rosenthal, Frantisek Vasa, Alessandra Griffa, Patric Hagmann, Enrico Amico, Joaquin Goni, Galia Avidan, and Olaf Sporns. Mapping higher-order relations between brain structure and function with embedded vector representations of connectomes. _Nature Communications_, 9(1):2178, Jun 2018. ISSN 2041-1723. doi: 10.1038/s41467-018-04614-w. URL [https://doi.org/10.1038/s41467-018-04614-w](https://doi.org/10.1038/s41467-018-04614-w).
* Santoro et al. [2023] Andrea Santoro, Federico Battiston, Giovanni Petri, and Enrico Amico. Higher-order organization of multivariate time series. _Nature Physics_, 19(2):221-229, Feb 2023. ISSN 1745-2481. doi: 10.1038/s41567-022-01852-0. URL [https://doi.org/10.1038/s41567-022-01852-0](https://doi.org/10.1038/s41567-022-01852-0).
* Kan et al. [2021] Xuan Kan, Hejie Cui, Ying Guo, and Carl Yang. Effective and interpretable fmri analysis via functional brain network generation. _arXiv preprint arXiv:2107.11247_, 2021.
* Behrouz and Seltzer [2023] Ali Behrouz and Margo Seltzer. Anomaly detection in human brain via inductive learning on temporal multiplex networks. In _Machine Learning for Healthcare Conference_, volume 219. PMLR, 2023.
* Mohanty et al. [2020] Rosaleena Mohanty, William A. Sethares, Veena A. Nair, and Vivek Prabhakaran. Rethinking measures of functional connectivity via feature extraction. _Scientific Reports_, 10(1):1298, Jan 2020. ISSN 2045-2322. doi: 10.1038/s41598-020-57915-w. URL [https://doi.org/10.1038/s41598-020-57915-w](https://doi.org/10.1038/s41598-020-57915-w).
* Zu et al. [2016] Chen Zu, Yue Gao, Brent Munsell, Minjeong Kim, Ziwen Peng, Yingying Zhu, Wei Gao, Daoqiang Zhang, Dinggang Shen, and Guorong Wu. Identifying high order brain connectome biomarkers via learning on hypergraph. In _Machine Learning in Medical Imaging: 7th International Workshop, MLMI 2016, Held in Conjunction with MICCAI 2016, Athens, Greece, October 17, 2016, Proceedings 7_, pages 1-9. Springer, 2016.
* van den Heuvel and Hulleke [2010] Martijn P. van den Heuvel and Hilleke E. Hulshoff Pol. Exploring the brain network: A review on resting-state fmri functional connectivity. _European Neuropsychopharmacology_, 20(8):519-534, 2010. ISSN 0924-977X. doi: [https://doi.org/10.1016/j.euroneuro.2010.03.008](https://doi.org/10.1016/j.euroneuro.2010.03.008). URL [https://www.sciencedirect.com/science/article/pii/S0924977X10000684](https://www.sciencedirect.com/science/article/pii/S0924977X10000684).
* Smith et al. [2013] Stephen M Smith, Diego Vidaurre, Christian F Beckmann, Matthew F Glasser, Mark Jenkinson, Karla L Miller, Thomas E Nichols, Emma C Robinson, Gholamreza Salimi-Khorshidi, Mark W Woolrich, Deanna M Barch, Kamil Ugurbil, and David C Van Essen. Functional connectomics from resting-state fMRI. _Trends Cogn Sci_, 17(12):666-682, November 2013.
* van den Heuvel et al. [2009] Martijn P van den Heuvel, Rene C W Mandl, Rene S Kahn, and Hilleke E Hulshoff Pol. Functionally linked resting-state networks reflect the underlying structural connectivity architecture of the human brain. _Hum Brain Mapp_, 30(10):3127-3141, October 2009.
* Calhoun et al. [2014] Vince D. Calhoun, Robyn Miller, Godfrey Pearlson, and Tulay Adali. The chronnectome: Time-varying connectivity networks as the next frontier in fmri data discovery. _Neuron_, 84(2):262-274, 2014. ISSN 0896-6273. doi: [https://doi.org/10.1016/j.neuron.2014.10.015](https://doi.org/10.1016/j.neuron.2014.10.015). URL [https://www.sciencedirect.com/science/article/pii/S0896627314009131](https://www.sciencedirect.com/science/article/pii/S0896627314009131).
* Hernandez et al. [2015] Leanna M Hernandez, Jeffrey D Rudie, Shulamite A Green, Susan Bookheimer, and Mirella Dapretto. Neural signatures of autism spectrum disorders: insights into brain network dynamics. _Neuropsychopharmacology_, 40(1):171-189, 2015.
* Liegeois et al. [2019] Raphael Liegeois, Jingwei Li, Ru Kong, Csaba Orban, Dimitri Van De Ville, Tian Ge, Mert R. Sabuncu, and B. T. Thomas Yeo. Resting brain dynamics at different timescales capture distinct aspects of human behavior. _Nature Communications_, 10(1):2317, May 2019. ISSN 2041-1723. doi: 10.1038/s41467-019-10317-7. URL [https://doi.org/10.1038/s41467-019-10317-7](https://doi.org/10.1038/s41467-019-10317-7).
* Ma et al. [2021] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z. Sheng, Hui Xiong, and Leman Akoglu. A comprehensive survey on graph anomaly detection with deep learning. _IEEE Transactions on Knowledge and Data Engineering_, pages 1-1, 2021. doi: 10.1109/TKDE.2021.3118815.

* Akoglu et al. [2015] Leman Akoglu, Hanghang Tong, and Danai Koutra. Graph based anomaly detection and description: a survey. _Data Mining and Knowledge Discovery_, 29(3):626-688, May 2015. ISSN 1573-756X. doi: 10.1007/s10618-014-0365-y. URL [https://doi.org/10.1007/s10618-014-0365-y](https://doi.org/10.1007/s10618-014-0365-y).
* Park et al. [2009] Youngser Park, C Priebe, D Marchette, and Abdou Youssef. Anomaly detection using scan statistics on time series hypergraphs. In _Link Analysis, Counterterrorism and Security (LACTS) Conference_, page 9. SIAM Pennsylvania, 2009.
* Leontjeva et al. [2012] Anna Leontjeva, Konstantin Tretyakov, Jaak Vilo, and Taavi Tamkivi. Fraud detection: Methods of analysis for hypergraph data. In _2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining_, pages 1060-1064. IEEE, 2012.
* Lee et al. [2022] Geon Lee, Minyoung Choe, and Kijung Shin. Hashwalk: Hash and random walk based anomaly detection in hyperedge streams. In Lud De Raedt, editor, _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22_, pages 2129-2137. International Joint Conferences on Artificial Intelligence Organization, 7 2022. doi: 10.24963/ijcai.2022/296. URL [https://doi.org/10.24963/ijcai.2022/296](https://doi.org/10.24963/ijcai.2022/296). Main Track.
* Sawhney et al. [2021] Ramit Sawhney, Shivam Agarwal, Arnav Wadhwa, Tyler Derr, and Rajiv Ratn Shah. Stock selection via spatiotemporal hypergraph attention network: A learning to rank approach. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 497-504, 2021.
* Jie et al. [2016] Biao Jie, Mingxia Liu, Xi Jiang, and Daoqiang Zhang. Sub-network based kernels for brain network classification. In _Proceedings of the 7th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics_, pages 622-629, 2016.
* Chen et al. [2011] Gang Chen, B Douglas Ward, Chunming Xie, Wenjun Li, Zhilin Wu, Jennifer L Jones, Malgorzata Franczak, Piero Antuono, and Shi-Jiang Li. Classification of alzheimer disease, mild cognitive impairment, and normal cognitive status with large-scale network analysis based on resting-state functional mr imaging. _Radiology_, 259(1):213, 2011.
* Wee et al. [2011] Chong-Yaw Wee, Pew-Thian Yap, Wenbin Li, Kevin Denny, Jeffrey N Browndyke, Guy G Potter, Kathleen A Welsh-Bohmer, Lihong Wang, and Dinggang Shen. Enriched white matter connectivity networks for accurate identification of mci patients. _Neuroimage_, 54(3):1812-1822, 2011.
* Behrouz et al. [2023] Ali Behrouz, Parsa Delavari, and Farnoosh Hashemi. Unsupervised representation learning of brain activity via bridging voxel activity and functional connectivity. In _NeurIPS 2023 AI for Science Workshop_, 2023. URL [https://openreview.net/forum?id=HSvg?qFFd2](https://openreview.net/forum?id=HSvg?qFFd2).
* Cui et al. [2021] Hejie Cui, Wei Dai, Yanqiao Zhu, Xiaoxiao Li, Lifang He, and Carl Yang. Brainnnexplainer: An interpretable graph neural network framework for brain network based disease analysis. _arXiv preprint arXiv:2107.05097_, 2021.
* Kan et al. [2022] Xuan Kan, Hejie Cui, Joshua Lukemire, Ying Guo, and Carl Yang. Fbnetgen: Task-aware gnn-based fmri analysis via functional brain network generation. _arXiv preprint arXiv:2205.12465_, 2022.
* Zhang et al. [2016] Jingyuan Zhang, Bokai Cao, Sihong Xie, Chun-Ta Lu, Philip S Yu, and Ann B Ragin. Identifying connectivity patterns for brain diseases via multi-side-view guided deep architectures. In _Proceedings of the 2016 SIAM International Conference on Data Mining_, pages 36-44. SIAM, 2016.
* Liu et al. [2020] Jiaxin Liu, Wei Zhao, Ye Hong, Sheng Gao, Xi Huang, Yingjie Zhou, Alzheimer's Disease Neuroimaging Initiative, et al. Learning features of brain network for anomaly detection. In _IEEE INFOCOM 2020-IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)_, pages 900-905. IEEE, 2020.
* Behrouz and Seltzer [2023] Ali Behrouz and Margo Seltzer. ADMIRE++: Explainable anomaly detection in the human brain via inductive learning on temporal multiplex networks. In _ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH)_, 2023. URL [https://openreview.net/forum?id=t4H8acYudJ](https://openreview.net/forum?id=t4H8acYudJ).

* Tolstikhin et al. [2021] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Peter Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. MLP-mixer: An all-MLP architecture for vision. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL [https://openreview.net/forum?id=EI2K0XKdnP](https://openreview.net/forum?id=EI2K0XKdnP).
* Ba et al. [2020] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.
* Hendrycks and Gimpel [2020] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2020.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Said et al. [2023] Anwar Said, Roza G. Bayrak, Tyler Derr, Mudassir Shabbir, Daniel Moyer, Catie Chang, and Xenofon Koutsoukos. Neurograph: Benchmarks for graph machine learning in brain connectomics, 2023.
* Chung et al. [2014] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. _arXiv preprint arXiv:1412.3555_, 2014.
* Yue et al. [2022] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. Ts2vec: Towards universal representation of time series. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 8980-8987, 2022.
* Woo et al. [2022] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=PilZY3omXV2](https://openreview.net/forum?id=PilZY3omXV2).
* Velickovic et al. [2019] Petar Velickovic, William Fedus, William L. Hamilton, Pietro Lio, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. In _International Conference on Learning Representations_, 2019. URL [https://openreview.net/forum?id=rklz9iAcKQ](https://openreview.net/forum?id=rklz9iAcKQ).
* Day et al. [2019] Trevor K M Day, Tara M Madhyastha, Mary K Askren, Peter Boord, Thomas J Montine, and Thomas J Grabowski. Attention network test fMRI data for participants with parkinson's disease and healthy elderly. _F1000Res_, 8:780, June 2019.
* Fan et al. [2005] Jin Fan, Bruce D McCandliss, John Fossella, Jonathan I Flombaum, and Michael I Posner. The activation of attentional networks. _Neuroimage_, 26(2):471-479, 2005.
* Brown et al. [2012] Jesse A Brown, Jeffrey D Rudie, Anita Bandrowski, John D Van Horn, and Susan Y Bookheimer. The UCLA multimodal connectivity database: a web-based platform for brain connectivity matrix sharing and analysis. _Front Neuroinform_, 6:28, November 2012.
* Shah et al. [2018] Vinit Shah, Eva Von Weltin, Silvia Lopez, James Riley McHugh, Lillian Veloso, Meysam Golmohammadi, Iyad Obeid, and Joseph Picone. The temple university hospital seizure detection corpus. _Frontiers in neuroinformatics_, 12:83, 2018.
* Craddock et al. [2013] Cameron Craddock, Yassine Benhajali, Carlton Chu, Francois Chouinard, Alan Evans, Andras Jakab, Budhachandra Singh Khundrakpam, John David Lewis, Qingyang Li, Michael Milham, et al. The neuro bureau preprocessing initiative: open sharing of preprocessed neuroimaging data and derivatives. _Frontiers in Neuroinformatics_, 7:27, 2013.
* Craddock et al. [2013] Cameron Craddock, Yassine Benhajali, Carlton Chu, Francois Chouinard, Alan Evans, Andras Jakab, Budhachandra Singh Khundrakpam, John David Lewis, Qingyang Li, Michael Milham, et al. The neuro bureau preprocessing initiative: open sharing of preprocessed neuroimaging data and derivatives. _Frontiers in Neuroinformatics_, 7(27):5, 2013.
* Behrouz and Seltzer [2022] Ali Behrouz and Margo Seltzer. Anomaly detection in multiplex dynamic networks: from blockchain security to brain disease prediction. In _NeurIPS 2022 Temporal Graph Learning Workshop_, 2022. URL [https://openreview.net/forum?id=UDGZDfwmay](https://openreview.net/forum?id=UDGZDfwmay).

* Aggarwal et al. [2011] Charu C. Aggarwal, Yuchen Zhao, and Philip S. Yu. Outlier detection in graph streams. In _2011 IEEE 27th International Conference on Data Engineering_, pages 399-409, 2011. doi: 10.1109/ICDE.2011.5767885.
* Yu et al. [2018] Wenchao Yu, Wei Cheng, Charu C. Aggarwal, Kai Zhang, Haifeng Chen, and Wei Wang. Netwalk: A flexible deep embedding approach for anomaly detection in dynamic networks. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \(\&\) Data Mining_, KDD '18, page 2672-2681, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450355520. doi: 10.1145/3219819.3220024. URL [https://doi.org/10.1145/3219819.3220024](https://doi.org/10.1145/3219819.3220024).
* Li et al. [2021] Xiaoxiao Li, Yuan Zhou, Nicha Dvornek, Muhan Zhang, Siyuan Gao, Juntang Zhuang, Dustin Scheinost, Lawrence H Staib, Pamela Ventola, and James S Duncan. Braingnn: Interpretable brain graph neural network for fmri analysis. _Medical Image Analysis_, 74:102233, 2021.
* Kawahara et al. [2017] Jeremy Kawahara, Colin J Brown, Steven P Miller, Brian G Booth, Vann Chau, Ruth E Grunau, Jill G Zwicker, and Ghassan Hamarneh. Brainnetcnn: Convolutional neural networks for brain networks; towards predicting neurodevelopment. _NeuroImage_, 146:1038-1049, 2017.
* Zhang et al. [2020] Ruochi Zhang, Yuesong Zou, and Jian Ma. Hyper-sagnn: a self-attention based graph neural network for hypergraphs. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/forum?id=ryeHuJbPtH](https://openreview.net/forum?id=ryeHuJbPtH).
* Yadati et al. [2020] Naganand Yadati, Vikram Nitin, Madhav Nimishakavi, Prateek Yadav, Anand Louis, and Partha Talukdar. Nhp: Neural hypergraph link prediction. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, pages 1705-1714, 2020.
* Audibert et al. [2020] Julien Audibert, Pietro Michiardi, Frederic Guyard, Sebastien Marti, and Maria A Zuluaga. Usad: Unsupervised anomaly detection on multivariate time series. In _Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 3395-3404, 2020.
* Yildiz Potter et al. [2022] Ilkay Yildiz Potter, George Zerveas, Carsten Eickhoff, and Dominique Duncan. Unsupervised multivariate time-series transformers for seizure identification on eeg. In _2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)_, pages 1304-1311. IEEE, 2022.
* Boutet et al. [2021] Alexandre Boutet, Radhika Madhavan, Gavin J. B. Elias, Suresh E. Joel, Robert Gramer, Manish Ranjan, Vijayashankar Paramanandam, David Xu, Jurgen Germann, Aaron Loh, Suneil K. Kalia, Mojgan Hodaie, Bryan Li, Sreeram Prasad, Ailish Coblentz, Renato P. Munhoz, Jeffrey Ashe, Walter Kucharczyk, Alfonso Fasano, and Andres M. Lozano. Predicting optimal deep brain stimulation parameters for parkinson's disease using functional mri and machine learning. _Nature Communications_, 12(1):3043, May 2021. ISSN 2041-1723. doi: 10.1038/s41467-021-23311-9. URL [https://doi.org/10.1038/s41467-021-23311-9](https://doi.org/10.1038/s41467-021-23311-9).
* Tahmasian et al. [2017] Masoud Tahmasian, Simon B. Eickhoff, Kathrin Giehl, Frank Schwartz, Damian M. Herz, Alexander Drzezga, Thilo van Eimeren, Angela R. Laird, Peter T. Fox, Habibolah Khazaie, Mojtaba Zarei, Carsten Eggers, and Claudia R. Eickhoff. Resting-state functional reorganization in parkinson's disease: An activation likelihood estimation meta-analysis. _Cortex_, 92:119-138, 2017. ISSN 0010-9452. doi: [https://doi.org/10.1016/j.cortex.2017.03.016](https://doi.org/10.1016/j.cortex.2017.03.016). URL [https://www.sciencedirect.com/science/article/pii/S0010945217300965](https://www.sciencedirect.com/science/article/pii/S0010945217300965).
* Lei et al. [2014] Du Lei, Jun Ma, Xiaoxia Du, Guohua Shen, Xingming Jin, and Qiyong Gong. Microstructural abnormalities in the combined and inattentive subtypes of attention deficit hyperactivity disorder: a diffusion tensor imaging study. _Scientific reports_, 4(1):6875, 2014.
* Rogers et al. [2013] Tiffany D Rogers, Eric McKimm, Price E Dickson, Dan Goldowitz, Charles D Blaha, and Guy Mittleman. Is autism a disease of the cerebellum? an integration of clinical and pre-clinical research. _Front Syst Neurosci_, 7:15, May 2013.