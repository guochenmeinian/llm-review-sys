# The Sample-Communication Complexity Trade-off in Federated Q-Learning

Sudeep Salgia

Carnegie Mellon University

ssalgia@andrew.cmu.edu

&Yuejie Chi

Carnegie Mellon University

yuejiechi@cmu.edu

###### Abstract

We consider the problem of Federated Q-learning, where \(M\) agents aim to collaboratively learn the optimal Q-function of an unknown infinite horizon Markov Decision Process with finite state and action spaces. We investigate the trade-off between sample and communication complexity for the widely used class of intermittent communication algorithms. We first establish the converse result, where we show that any Federated Q-learning that offers a linear speedup with respect to number of agents in sample complexity needs to incur a communication cost of at least \(\Omega(\frac{1}{1-\gamma})\), where \(\gamma\) is the discount factor. We also propose a new Federated Q-learning algorithm, called Fed-DVR-Q, which is the first Federated Q-learning algorithm to simultaneously achieve order-optimal sample and communication complexities. Thus, together these results provide a complete characterization of the sample-communication complexity trade-off in Federated Q-learning.

## 1 Introduction

Reinforcement Learning (RL) [14] refers to an online sequential decision making paradigm where the learning agent aims to learn an optimal policy, i.e., a policy that maximizes the long-term reward, through repeated interactions with an unknown environment. RL finds applications across a diverse array of fields including, but not limited to, autonomous driving, games, recommendation systems, robotics and Internet of Things (IoT) [15, 16, 17].

The primary hurdle in RL applications is often the high-dimensional nature of the decision space that necessitates the learning agent to have to access to an enormous amount of data in order to have any hope of learning the optimal policy. Moreover, the sequential collection of such an enormous amount of data through a single agent is extremely time-consuming and often infeasible in practice. Consequently, practical implementations of RL involve deploying multiple agents to collect data in parallel. This decentralized approach to data collection has fueled the design and development of distributed or federated RL algorithms that can collaboratively learn the optimal policy without actually transferring the collected data to a centralized server. Such a federated approach to RL, which does not require the transfer of local data, is gaining interest due to lower bandwidth requirements and lower security and privacy risks. In this work, we focus on federated variants of Q-learning algorithms where the agents collaborate to directly learn the optimal Q-function without forming an estimate of the underlying unknown environment.

A particularly important aspect of designing Federated RL algorithms, including Federated Q-learning algorithms, is to address the natural tension between sample and communication complexity. At one end of the spectrum lies the naive approach of running a centralized algorithm with optimal sample complexity after transferring and combining all the collected data at a central facility/server. Such an approach trivially achieves the optimal sample complexity while suffering from a very high and infeasible communication complexity. On the other hand, several recently proposedalgorithms (Khodadadian et al., 2022; Woo et al., 2023) operate in more practical regimes, offering significantly lower communication complexities as compared to the naive approach at the cost of sub-optimal sample complexities. These results suggest the existence of underlying trade-off between sample and communication complexities of Federated RL algorithms. The primary goal of this work is to better understand this trade-off in context of Federated Q-learning by investigating these following fundamental questions:

* _Fundamental limit of communication: What is the minimum amount of communication required by a federated Q-learning algorithm to achieve any statistical benefit of collaboration?_
* _Optimal algorithm design: How does one design a federated Q-Learning algorithm that simultaneously offers optimal order sample and communication complexity guarantees i.e., operates on the optimal frontier of sample-communication complexity trade-off?_

### Main Results

We consider a setup where \(M\) distributed agents collaborate to learn the optimal Q-function of an infinite horizon Markov Decision Process which is defined over a finite state space \(\mathcal{S}\) and a finite action set \(\mathcal{A}\), and has a discount factor of \(\gamma\in(0,1)\). We consider a commonly considered setup in federated learning called the intermittent communication setting, where the clients intermittently share information among themselves with the help of a central server. In this work, we provide a complete characterization of the trade-off between sample and communication complexity under the aforementioned setting by providing answers to both the questions. The main result of this work is twofold and is summarized below.

* _Fundamental bounds on communication complexity of Federated Q-learning_: We establish lower bounds on the communication complexity of Federated Q-learning, both in terms of number of communication rounds and the overall number of bits that need to be transmitted in order to achieve any speed up in convergence with respect to the number of agents. Specifically, we show that in order for an intermittent communication algorithm to obtain _any_ benefit of collaboration, i.e., _any_ order of speed up w.r.t. the number of agents, the number of communication rounds must be least \(\Omega(\frac{1}{(1-\gamma)\log^{2}N})\) and the number of _bits_ sent by each agent to the server must be least \(\Omega(\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)\log^{2}N})\), where \(N\) denotes the number of samples taken by the algorithm for each state-action pair.
* _Achieving the optimal sample-communication complexity trade-off_: We propose a new Federated Q-Learning algorithm called Federated Doubly Variance Reduced Q Learning, Fed-DVR-Q for short, that simultaneously achieves optimal order of sample complexity and the minimal order of communication as dictated by the lower bound. We show that Fed-DVR-Q learns an \(\varepsilon\)-optimal Q-function in the \(\ell_{\infty}\) sense with \(\tilde{\mathcal{O}}\left(\frac{|\mathcal{S}||\mathcal{A}|}{M\varepsilon^{2}(1- \gamma)^{3}}\right)\) i.i.d. samples from the generative model at each agent while incurring a total communication cost of \(\tilde{\mathcal{O}}\left(\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)}\right)\)_bits_ per agent across \(\tilde{\mathcal{O}}\left(\frac{1}{(1-\gamma)}\right)\) rounds of communication. Thus, Fed-DVR-Q not only improves upon both the sample and communication complexities of existing algorithms, but also is the _first algorithm_ to achieve both order-optimal sample and communication complexities (See Table 1 for a comparison).

### Related Work

Single agent Q-Learning.Q-Learning has been extensively studied in the single-agent setting in terms of both its asymptotic convergence (Jaakkola et al., 1993; Tsitsiklis, 1994; Szepesvari, 1997; Borkar and Meyn, 2000) and its finite-time sample complexity in both synchronous (Even-Dar and Mansour, 2004; Beck and Srikant, 2012; Wainwright, 2019a; Chen et al., 2020; Li et al., 2023) and asynchronous settings (Chen et al., 2021b; Li et al., 2023, 2021; Qu and Wierman, 2020).

Distributed RL.There has also been a considerable effort towards developing distributed and federated RL algorithms. The distributed variants of the classical TD learning algorithm have been investigated in a series of studies (Chen et al., 2021c; Doan et al., 2019, 2021; Sun et al., 2020; Wai, 2020; Wang et al., 2020; Zeng et al., 2021b). The impact of environmental heterogeneity in federated TD learning was studied in Wang et al. (2023). A distributed version of actor-criticalgorithms was studied by Shen et al. (2023) where the authors established convergence of their algorithm and demonstrated a linear speed up in the number of agents in their sample complexity bound. Chen et al. (2022) proposed a new distributed actor-critic algorithm which improved the dependence of sample complexity on the error \(\varepsilon\) and incurs a communication cost of \(\tilde{\mathcal{O}}(\varepsilon^{-1})\). Chen et al. (2021) have proposed a communication efficient distributed policy gradient algorithm and have analyzed its convergence and established a communication complexity of \(\mathcal{O}(1/(M\varepsilon))\). Xie and Song (2023) adopts a distributed policy optimization perspective, which is different from the Q-learning paradigm considered in this work. Moreover, the algorithm in Xie and Song (2023) obtains a linear communication cost, which is worse than that obtained in our work. Similarly, Zhang et al. (2024) focuses on on-policy learning and incurs a communication cost that depends polynomially on the required error \(\varepsilon\). Several other studies (Yang et al., 2023; Zeng et al., 2021; Lan et al., 2024) have also developed and analyzed other distributed/federated variants of the classical natural policy gradient method (Kakade, 2001). Assran et al. (2019); Espeholt et al. (2018); Mnih et al. (2016) have developed distributed algorithms to train deep RL networks more efficiently.

Distributed Q-learning.Federated Q-learning has been explored relatively recently. Khodadadian et al. (2022) proposed and analyzed a federated Q-learning algorithm in the asynchronous setting with a sample complexity of \(\tilde{\mathcal{O}}\left(\frac{|\mathcal{S}|^{2}}{M\mu_{\min}^{2}(1-\gamma)^ {3}\varepsilon^{2}}\right)\), where \(\mu_{\min}\) is the minimum entry of the stationary state-action occupancy distribution of the sample trajectories over all agents. Jin et al. (2022) study the impact of environmental heterogeneity across clients in Federated Q-learning. They propose an algorithm where the local environments are different at each client but each client knows their local environment. Under this setting, they propose an algorithm that achieves a sample and communication complexity of \(\mathcal{O}(\frac{1}{(1-\gamma)^{3}\varepsilon})\) and \(\mathcal{O}(\frac{1}{(1-\gamma)^{3}\varepsilon})\) rounds respectively. Woo et al. (2023) proposed new algorithms with improved analysis for Federated Q-learning under both synchronous and asynchronous settings. Their proposed algorithm achieves a sample complexity and communication complexity of \(\tilde{\mathcal{O}}(\frac{|\mathcal{S}||\mathcal{A}|}{M(1-\gamma)^{3} \varepsilon^{2}})\) and \(\tilde{\mathcal{O}}(\frac{M|\mathcal{S}||\mathcal{A}|}{1-\gamma})\) real numbers respectively under the synchronous setting and that of \(\tilde{\mathcal{O}}(\frac{M|\mathcal{S}||\mathcal{A}|}{M\mu_{\text{arg}}(1- \gamma)^{3}\varepsilon^{2}})\) and \(\tilde{\mathcal{O}}\left(\frac{M|\mathcal{S}||\mathcal{A}|}{1-\gamma}\right)\) real numbers respectively under the asynchronous setting. Here, \(\mu_{\text{avg}}\) denotes the minimum entry of the average stationary state-action occupancy distribution of all agents. In a follow up work, Woo et al. (2024) propose a Federated Q-learning for offline RL in finite horizon setting and establish a sample and communication complexity of \(\tilde{\mathcal{O}}(\frac{H^{7}|\mathcal{S}||\mathcal{C}_{\text{avg}}}{M \varepsilon^{2}})\) and \(\tilde{\mathcal{O}}(H)\), where \(H\) denotes the length of the horizon and \(C_{\text{avg}}\) denotes the average single-policy concentrability coefficient of all agents.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline Algorithm/Reference & Number of Agents & Sample Complexity & Communication Complexity \\ \hline Q-learning (Li et al., 2023) & \(1\) & \(\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^{4}\varepsilon^{2}}\) & N/A \\ \hline Variance Reduced Q-learning (Wainwright, 2019) & \(1\) & \(\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^{3}\varepsilon^{2}}\) & N/A \\ \hline Fed-SynQ (Woo et al., 2023) & \(M\) & \(\frac{|\mathcal{S}||\mathcal{A}|}{M(1-\gamma)^{5}\varepsilon^{2}}\) & \(\frac{M}{1-\gamma}\) \\ \hline Fed-DVR-Q (This work) & \(M\) & \(\frac{|\mathcal{S}||\mathcal{A}|}{M(1-\gamma)^{3}\varepsilon^{2}}\) & \(\frac{1}{1-\gamma}\) \\ \hline Lower bound ((Azar et al., 2013), This work) & \(M\) & \(\frac{|\mathcal{S}||\mathcal{A}|}{M(1-\gamma)^{3}\varepsilon^{2}}\) & \(\frac{1}{1-\gamma}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of sample and communication complexity of various single-agent and Federated Q-learning algorithms for learning an \(\varepsilon\)-optimal Q-function under the synchronous setting. We hide logarithmic factors and burn-in costs for all results for simplicity of presentation. In the above table, \(\mathcal{S}\) and \(\mathcal{A}\) represent state and action spaces respectively and \(\gamma\) denotes the discount factor. We report the communication complexity only in terms of number of rounds as other algorithms assume transmission of real numbers and hence do not report bit level costs. For the lower bound, Azar et al. (2013) and this work establish the bound for sample and communication complexity respectively.

Accuracy-Communication Trade-off in Federated Learning.The trade-off between communication complexity and accuracy (equivalently, sample complexity) has been studied in various federated and distributed learning problems, including stochastic approximation algorithms for convex optimization. Duchi et al. (2014); Braverman et al. (2016) establish the celebrated inverse linear relationship between the error and the communication cost the problem of distributed mean estimation. Similar trade-off for distributed stochastic optimization, multi-armed bandits and linear bandits has been studied and established across numerous studies (Woodworth et al., 2018, 2021; Tsitsiklis and Luo, 1987; Shi and Shen, 2021; Salgia and Zhao, 2023).

## 2 Problem Formulation and Preliminaries

In this section, we provide a brief background of Markov Decision Processes, outline the performance measures for Federated Q-learning algorithms and describe the class of intermittent communication algorithms considered in this work.

### Markov Decision Processes

In this work, we focus on an infinite-horizon Markov Decision Process (MDP), denoted by \(\mathcal{M}\), over a state space \(\mathcal{S}\) and an action space \(\mathcal{A}\) and with a discount factor \(\gamma\in(0,1)\). Both the state and action spaces are assumed to be finite sets. In an MDP, the state \(s\) evolves dynamically under the influence of actions based on a probability transition kernel, \(P:(\mathcal{S}\times\mathcal{A})\times\mathcal{S}\rightarrow[0,1]\). The entry \(P(s^{\prime}|s,a)\) denotes the probability of moving to state \(s^{\prime}\) when an action \(a\) is taken in the state \(s\). An MDP is also associated with a deterministic reward function \(r:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\), where \(r(s,a)\) denotes the immediate reward obtained for taking the action \(a\) in the state \(s\). Thus, the transition kernel \(P\) along with the reward function \(r\) completely characterize an MDP. In this work, we consider the synchronous setting, where each agent has access to an independent generative model or simulator from which they can draw independent samples from the unknown underlying distribution \(P(\cdot|s,a)\) for each state-action pair \((s,a)\)(Kearns and Singh, 1998).

A policy \(\pi:\mathcal{S}\rightarrow\Delta(\mathcal{A})\) is a rule for selecting actions across different states, where \(\Delta(\mathcal{A})\) denotes the simplex over \(\mathcal{A}\) and \(\pi(a|s)\) denotes the probability of choosing action \(a\) in a state \(s\). Each policy \(\pi\) is associated with a state value function and a state-action value function, or the Q-function, denoted by \(V^{\pi}\) and \(Q^{\pi}\) respectively. \(V^{\pi}\) and \(Q^{\pi}\) measure the expected discounted cumulative reward attained by \(\pi\) starting from a particular state \(s\) and state-action pair \((s,a)\) respectively. Mathematically, \(V^{\pi}\) and \(Q^{\pi}\) are given as

\[V^{\pi}(s):=\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t},a_{t})\bigg{|} \ s_{0}=s\right];\quad Q^{\pi}(s,a):=\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma ^{t}r(s_{t},a_{t})\bigg{|}\ s_{0}=s,a_{0}=a\right], \tag{1}\]

where \(a_{t}\sim\pi(\cdot|s_{t})\) and \(s_{t+1}\sim P(\ \cdot\ |s_{t},a_{t})\) for all \(t\geq 0\). The expectation is taken w.r.t. the randomness in the trajectory \(\{s_{t},a_{t}\}_{t=1}^{\infty}\). Since the rewards lie in \([0,1]\), it follows immediately that both the value function and Q-function lie in the range \([0,\frac{1}{1-\gamma}]\).

An optimal policy \(\pi^{*}\) is a policy that maximizes the value function uniformly over all the states and it has been shown that such an optimal policy \(\pi^{*}\) always exists (Puterman, 2014). The optimal value and Q-functions are those corresponding to that of an optimal policy \(\pi^{*}\) are denoted as \(V^{\star}:=V^{\pi^{*}}\) and \(Q^{\star}:=Q^{\pi^{*}}\) respectively. The optimal Q-function, \(Q^{\star}\), is also the unique fixed point of the Bellman operator \(\mathcal{T}:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S}\times\mathcal{A}\), given by

\[(\mathcal{T}Q)(s,a)=r(s,a)+\gamma\cdot\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)} \left[\max_{a^{\prime}\in\mathcal{A}}Q(s^{\prime},a^{\prime})\right]. \tag{2}\]

Q-learning (Watkins and Dayan, 1992) aims to learn the optimal policy by first learning \(Q^{*}\) as the solution to the fixed point equation \(\mathcal{T}Q=Q\) and then obtain a deterministic optimal policy via the maximization \(\pi^{*}(s)=\arg\max_{a}Q^{*}(s,a)\).

Let \(Z\in\mathcal{S}^{|\mathcal{S}||\mathcal{A}|}\) be a random vector whose \((s,a)^{\text{th}}\) coordinate is drawn from the distribution \(P(\cdot|s,a)\), independently of all other coordinates. We define the random operator \(\mathcal{T}_{Z}:(\mathcal{S}\times\mathcal{A})\rightarrow(\mathcal{S}\times \mathcal{A})\) as

\[(\mathcal{T}_{Z}Q)(s,a)=r(s,a)+\gamma V(Z(s,a)), \tag{3}\]where \(V(s^{\prime})=\max_{a^{\prime}\in\mathcal{A}}Q(s^{\prime},a^{\prime})\). The operator \(\mathcal{T}_{Z}\) can be interpreted as the sample Bellman Operator, where we have the relation \(\mathcal{T}Q=\mathbb{E}_{Z}[\mathcal{T}_{Z}Q]\) for all Q-functions.

Lastly, the federated learning setup considered in this work consists of \(M\) agents, where all the agents face a common, unknown MDP, i.e., the transition kernel and the reward functions are the same across agents, which is popularly known as the homogeneous setting. For a given value of \(\varepsilon\in(0,\frac{1}{1-\gamma})\), the objective of agents is to collaboratively learn an \(\varepsilon\)-optimal estimate (in the \(\ell_{\infty}\) sense) of the optimal Q-function of the unknown MDP.

### Performance Measures

We measure the performance of a Federated Q-learning algorithm \(\mathscr{A}\) using two metrics -- sample complexity and communication complexity. For a given MDP \(\mathcal{M}\), let \(\widehat{Q}_{\mathcal{M}}(\mathscr{A},N,M)\) denote the estimate of \(Q^{\star}_{\mathcal{M}}\), the optimal Q-function of the MDP \(\mathcal{M}\), returned by an algorithm \(\mathscr{A}\), when given access to \(N\) i.i.d. samples from the generative model for each \((s,a)\) pair at all the \(M\) agents. The minimax error rate of the algorithm \(\mathscr{A}\), denoted by \(\mathsf{ER}(\mathscr{A};N,M)\), is defined as

\[\mathsf{ER}(\mathscr{A};N,M):=\sup_{\mathcal{M}=(P,r)}\mathbb{E} \left[\|\widehat{Q}_{\mathcal{M}}(\mathscr{A},N,M)-Q^{\star}_{\mathcal{M}}\|_{ \infty}\right], \tag{4}\]

where the expectation is taken over the samples and any randomness in the algorithm. Given a value of \(\varepsilon>0\), the sample complexity of \(\mathscr{A}\), denoted by \(\mathsf{SC}(\mathscr{A};\varepsilon,M)\) is given as

\[\mathsf{SC}(\mathscr{A};\varepsilon,M):=|\mathcal{S}||\mathcal{A} |\cdot\min\{N\in\mathbb{N}:\mathsf{ER}(\mathscr{A};N,M)\leq\varepsilon\}. \tag{5}\]

Similarly, we can also define a high-probability version for any \(\delta\in(0,1)\) as follows:

\[\mathsf{SC}(\mathscr{A};\varepsilon,M,\delta):=|\mathcal{S}|| \mathcal{A}|\cdot\min\{N\in\mathbb{N}:\Pr(\sup_{\mathcal{M}}\|\widehat{Q}_{ \mathcal{M}}(\mathscr{A},N,M)-Q^{\star}_{\mathcal{M}}\|_{\infty}\leq \varepsilon)\geq 1-\delta\}.\]

We measure the communication complexity of any federated learning algorithm both in terms of frequency of information exchange and total number of bits uploaded by the agents. For each agent \(m\), let \(C^{m}_{\mathsf{round}}(\mathscr{A};N)\) and \(C^{m}_{\mathsf{bit}}(\mathscr{A};N)\) respectively denote the number of times agent \(m\) sends a message to the server and the total number of bits uploaded by agent \(m\) to the server when an algorithm \(\mathscr{A}\) is run with \(N\) i.i.d. samples from the generative model for each \((s,a)\) pair at each agent. The communication complexity of \(\mathscr{A}\), when measured in terms of frequency of communication and total number of bits exchanged, is given by

\[\mathsf{CC}_{\mathsf{round}}(\mathscr{A};N):=\frac{1}{M}\sum_{m=1 }^{M}C^{m}_{\mathsf{round}}(\mathscr{A};N);\quad\mathsf{CC}_{\mathsf{bit}}( \mathscr{A};N):=\frac{1}{M}\sum_{m=1}^{M}C^{m}_{\mathsf{bit}}(\mathscr{A};N), \tag{6}\]

respectively. Similarly, for a given value of \(\varepsilon\in(0,\frac{1}{1-\gamma})\), we can also define \(\mathsf{CC}_{\mathsf{round}}(\mathscr{A};\varepsilon)\) and \(\mathsf{CC}_{\mathsf{bit}}(\mathscr{A};\varepsilon)\) based on when \(\mathscr{A}\) is run to guarantee a minimax error of at most \(\varepsilon\).

### Intermittent Communication Algorithms

In this work, we consider a popular class of federated learning algorithms referred to as algorithms with intermittent communication. The intermittent communication setting provides a natural framework to extend single agent Q-learning algorithms to the distributed setting. As the name suggests, under this setting, the agents intermittently communicate with each other, sharing their updated beliefs about \(Q^{\star}\). Between two communication rounds, each agent updates their belief about \(Q^{\star}\) using stochastic fixed point iteration based on the locally available data, similar to a single agent setup. Such intermittent communication algorithms have been extensively studied and used to establish lower bounds on communication complexity of distributed stochastic convex optimization (Woodworth et al., 2018, 2021).

A generic Federated Q-learning algorithm with intermittent communication is outlined in Algorithm 1. It is characterized by the following five parameters: (i) total number of updates \(T\); (ii) the number of communication rounds \(R\); (iii) a step size schedule \(\{\eta_{t}\}_{t=1}^{T}\); (iv) a communication schedule \(\{t_{r}\}_{r=1}^{R}\); (v) batch size \(B\). During the \(t^{\text{th}}\) iteration, each agent \(m\) computes \(\{\widehat{T}_{Z_{\text{g}}}(Q_{t-1}^{m})\}_{b=1}^{B}\), a minibatch of sample Bellman operators at the current estimate \(Q_{t-1}^{m}\), using \(B\) samples from the generative model for each \((s,a)\) pair, and obtains an intermediate local estimate using the Q-learning update as follows:

\[Q_{t-\frac{1}{2}}^{m}=(1-\eta_{t})Q_{t-1}^{m}+\frac{\eta_{t}}{B}\sum_{b=1}^{B} \mathcal{T}_{Z_{b}}(Q_{t-1}^{m}). \tag{7}\]

Here \(\eta_{t}\in(0,1]\) is the step-size chosen corresponding to the \(t^{\text{th}}\) time step. The intermediate estimates are averaged based on a communication schedule \(\mathcal{C}=\{t_{r}\}_{r=1}^{R}\) consisting of \(R\) rounds, i.e.,

\[Q_{t}^{m}=\begin{cases}\frac{1}{M}\sum_{j=1}^{M}Q_{t-\frac{1}{2}}^{j}&\text{ if }t\in\mathcal{C},\\ Q_{t-\frac{1}{2}}^{m}&\text{otherwise.}\end{cases} \tag{8}\]

In the above equation, the averaging step can also be replaced with any distributed mean estimation routine that includes compression to control the bit level costs. Without loss of generality, we assume that \(Q_{0}^{m}=0\) for all agents \(m\) and \(t_{R}=T\), i.e., the last iterates are always averaged. It is straightforward to note that the number of samples taken by an intermittent communication algorithm is \(BT\), i.e, \(N=BT\) and the communication complexity \(\textsf{CC}_{\textsf{round}}=R\).

## 3 Lower Bound

In this section, we investigate the first of the two questions regarding the lower bound on communication complexity. The following theorem establishes a lower bound on the communication complexity of a Federated Q-learning algorithm with intermittent communication.

**Theorem 1**.: _Assume that \(\gamma\in[5/6,1)\) and the state and action spaces satisfy \(|\mathcal{S}|\geq 4\) and \(|\mathcal{A}|\geq 2\). Let \(\mathscr{A}\) be a Federated Q-learning algorithm with intermittent communication that is run for \(T\geq\max\{16,\frac{1}{1-\gamma}\}\) steps with a step size schedule of either \(\eta_{t}:=\frac{1}{1+c_{u}(1-\gamma)t}\) or \(\eta_{t}:=\eta\) for all \(1\leq t\leq T\). If_

\[R=\textsf{CC}_{\textsf{round}}(\mathscr{A};N)\leq\frac{c_{0}}{(1-\gamma)\log^ {2}N};\text{ or }\textsf{CC}_{\textsf{pdf}}(\mathscr{A};N)\leq\frac{c_{1}| \mathcal{S}||\mathcal{A}|}{(1-\gamma)\log^{2}N}\]

_for some universal constants \(c_{0},c_{1}>0\) then, for all choices of communication schedule, batch size \(B\), \(c_{\eta}>0\) and \(\eta\in(0,1)\), the minimax error of \(\mathscr{A}\) satisfies_

\[\textsf{ER}(\mathscr{A};N,M)\geq\frac{C_{\gamma}}{\log^{3}N\sqrt{N}},\]

_for all \(M\geq 2\) and \(N=BT\). Here \(C_{\gamma}>0\) is a constant that depends only on \(\gamma\)._

The above theorem states that in order for an intermittent communication algorithm to obtain _any_ benefit of collaboration, i.e., for the error rate \(\textsf{ER}(\mathscr{A};N,M)\) to decrease w.r.t. number of agents, the number of communication rounds must be least \(\Omega(\frac{1}{(1-\gamma)\log^{2}N})\). This implies that any Federated Q-learning algorithm that offers order optimal sample complexity, and thereby also a linear speed up with respect to the number of agents, must have at least \(\Omega(\frac{1}{(1-\gamma)\log^{2}N})\) rounds of communication and transmit \(\Omega(\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)\log^{2}N})\) bits of information per agent. This characterizes the converse relation for the sample-communication tradeoff in Federated Q-learning. We would like to point out that our lower bound extends to the asynchronous setting as the assumption of i.i.d. noise corresponding to a generative model is a special case of Markovian noise observed in asynchronous setting.

The lower bound on the communication complexity of Federated Q-learning is a consequence of the bias-variance trade-off that governs the convergence of the algorithm. While a careful choice of step-sizes alone is sufficient to balance this trade-off in the centralized setting, the choice of communication schedule also plays an important role in balancing this trade-off in the federated setting. The local steps between two communication rounds induce a positive estimation bias that depends on the standard deviation of the iterates and is a well-documented issue of "over-estimation" in Q-learning (Hasselt, 2010). Since such a bias is driven by _local_ updates, it does not reflect any benefit of collaboration. During a communication round, the averaging of iterates across agents allows the algorithm an opportunity to counter this bias by reducing the effective variance of the updates through averaging. In our analysis, we show that if the communication is infrequent, the local bias becomes the dominant term and averaging of iterates is insufficient to counter the impact of the positive bias induced by the local steps. As a result, we do not observe any statistical gains when the communication is infrequent. The analysis is inspired the analysis of Q-learning by Li et al. (2023) and is based on analyzing the convergence of an intermittent communication algorithm on a specifically chosen "hard" instance of MDP. Please refer to Appendix B for a detailed proof.

_Remark_ 1 (Communication complexity of policy evaluation). Several recent studies (Liu and Olshevsky, 2023; Tian et al., 2024) established that a single round of communication is sufficient to achieve linear speedup of TD learning for _policy evaluation_, which do not contradict with our results focusing on Q-learning for _policy learning_. The latter is more involved due to the nonlinearity of the Bellman optimality operator. Specifically, if the operator whose fixed point is to be found is linear in the decision variable (e.g., the value function in TD learning) then the fixed point update only induces a variance term corresponding to the noise. However, if the operator is non-linear, then in addition to the variance term, we also obtain a _bias_ term in the fixed point update. While the variance term can be controlled with one-shot averaging, more frequent communication is necessary to ensure that the bias term is small enough.

_Remark_ 2 (Extension to asynchronous Q-learning). We would like to point out that our lower bound extends to the asynchronous setting (Li et al., 2023) as the assumption of i.i.d. noise corresponding to a generative model is a special case of Markovian noise observed in the asynchronous setting.

## 4 The Fed-DVR-Q algorithm

Having characterized the lower bound on the communication complexity of Federated Q-learning, we explore our second question of interest -- designing a federated Q-learning algorithm that achieves this lower bound while simultaneously offering an optimal order of sample complexity.

We propose a new Federated Q-learning algorithm, Fed-DVR-Q, that achieves not only a communication complexity of \(\textsf{CC}_{\textsf{round}}=\tilde{\mathcal{O}}(\frac{1}{1-\gamma})\) and \(\textsf{CC}_{\textsf{bit}}=\tilde{\mathcal{O}}(\frac{|\mathcal{S}||\mathcal{ A}|}{1-\gamma})\) but also the optimal order of sample complexity (upto logarithmic factors), thereby providing a tight characterization of the achievability frontier that matches with the converse result derived in the previous section.

### Algorithm Description

```
1:Input : Error bound \(\varepsilon>0\), failure probability \(\delta>0\)
2:\(k\gets 1,Q^{(0)}\leftarrow\mathbf{0}\)
3://Set parameters as described in Sec. 4.1.3
4:for\(k=1,2,\ldots,K\)do
5:\(Q^{(k)}\leftarrow\textsc{RefineEstimate}(Q^{(k-1)},B,I,L_{k},D_{k},J)\)
6:\(k\gets k+1\)
7:endfor
8:return\(Q^{(K)}\)
```

**Algorithm 2**Fed-DVR-Q

```
1:Input : Error bound \(\varepsilon>0\), failure probability \(\delta>0\)
2:\(k\gets 1,Q^{(0)}\leftarrow\mathbf{0}\)
3://Set parameters as described in Sec. 4.1.3
4:for\(k=1,2,\ldots,K\)do
5:\(Q^{(k)}\leftarrow\textsc{RefineEstimate}(Q^{(k-1)},B,I,L_{k},D_{k},J)\)
6:\(k\gets k+1\)
7:endfor
8:return\(Q^{(K)}\)
```

**Algorithm 3**Fed-DVR-Q

#### 4.1.1 The RefineEstimate sub-routine

RefineEstimate, starting from \(\overline{Q}\), an initial estimate of \(Q^{\star}\), uses variance reduced Q-learning updates to obtain an improved estimate of \(Q^{\star}\). It is characterized by four parameters -- the initial estimate \(\overline{Q}\), the number of local iterations \(I\), the recentering sample size \(L\) and the batch size \(B\)which can be appropriately tuned to control the quality of the returned estimate. Additionally, it also takes input two parameters \(D\) and \(J\) required by the compressor.

The first step in RefineEstimate is to collaboratively approximate \(\mathcal{T}\overline{Q}\) for the variance reduced updates. To this effect, each agent \(m\) builds an approximation of \(\mathcal{T}\overline{Q}\) as follows:

\[\widetilde{\mathcal{T}}_{L}^{(m)}(\overline{Q}):=\frac{1}{\lceil L/M\rceil} \sum_{l=1}^{\lceil L/M\rceil}\mathcal{T}_{Z_{i}^{(m)}}(\overline{Q}), \tag{9}\]

where \(\{Z_{1}^{(m)},Z_{2}^{(m)},\ldots,Z_{\lceil L/M\rceil}^{(m)}\}\) are \(\lceil L/M\rceil\) i.i.d. samples with \(Z_{1}^{(m)}\sim Z\). Each agent sends \(\mathscr{C}\left(\widetilde{\mathcal{T}}_{L}^{(m)}(\overline{Q})-\overline{Q} ;D,J\right)\), a compressed version of the difference \(\widetilde{\mathcal{T}}_{L}^{(m)}(\overline{Q})-\overline{Q}\), to the server, which collects all the estimates from the agents and constructs the estimate

\[\widetilde{\mathcal{T}}_{L}(\overline{Q})=\overline{Q}+\frac{1}{M}\sum_{m=1} ^{M}\mathscr{C}\left(\widetilde{\mathcal{T}}_{L}^{(m)}(\overline{Q})-\overline {Q};D,J\right) \tag{10}\]

and sends it back to the agents for the variance reduced updates. We defer the description of the compression routine to the end of this section. Equipped with the estimate \(\widetilde{\mathcal{T}}_{L}(\overline{Q})\), RefineEstimate constructs a sequence \(\{Q_{i}\}_{i=1}^{I}\) using the following iterative update scheme initialized with \(Q_{0}=\overline{Q}\). During the \(i^{\text{th}}\) iteration, each agent \(m\) carries out the following update:

\[Q_{i-\frac{1}{2}}^{m}=(1-\eta)Q_{i-1}+\eta\left[\widehat{\mathcal{T}}_{i}^{(m) }Q_{i-1}-\widehat{\mathcal{T}}_{i}^{(m)}\overline{Q}+\widetilde{\mathcal{T}}_{ L}(\overline{Q})\right]. \tag{11}\]

In the above equation, \(\eta\in(0,1)\) is the step size and \(\widehat{\mathcal{T}}_{i}^{(m)}Q:=\frac{1}{B}\sum_{z\in\mathcal{Z}_{i}^{(m)}} \mathcal{T}_{z}Q\), where \(\mathcal{Z}_{i}^{(m)}\) is the minibatch of \(B\) i.i.d. random variables drawn according to \(Z\), independently at each agent \(m\) for all iterations \(i\). Each agent then sends a compressed version of the update, i.e., \(\mathscr{C}\left(Q_{i-\frac{1}{2}}^{m}-Q_{i-1};D,J\right)\), to the server, which uses them to compute the next iterate

\[Q_{i}=Q_{i-1}+\frac{1}{M}\sum_{m=1}^{M}\mathscr{C}\left(Q_{i-\frac{1}{2}}^{m} -Q_{i-1};D,J\right), \tag{12}\]

and broadcast it to the clients. After \(I\) such updates, the obtained iterate \(Q_{I}\) is returned by the routine. A pseudocode of the RefineEstimate routine is given in Algorithm 3 in Appendix A.

#### 4.1.2 The Compression Operator

The compressor, \(\mathscr{C}(\cdot;D,J)\), used in the proposed algorithm Fed-DVR-Q is based on the popular stochastic quantization scheme. In addition to the input vector \(Q\) to be quantized, the quantizer \(\mathscr{C}\) takes two input parameters \(D\) and \(J\). \(D\) corresponds to an upper bound on \(\ell_{\infty}\) norm of \(Q\), i.e., \(\|Q\|_{\infty}\leq D\). \(J\) corresponds to the resolution of the compressor, i.e., number of bits used by the compressor to represent each coordinate of the output vector.

The compressor first splits the interval \([0,D]\) into \(2^{J}-1\) intervals of equal length where \(0=d_{1}<d_{2},\cdots<d_{2^{J}}=D\) correspond to end points of the intervals. Each coordinate of \(Q\) is then separately quantized as follows. The value of the \(n^{\text{th}}\) coordinate, \(\mathscr{C}(Q)[n]\), is set to be \(d_{j_{n}-1}\) with probability \(\frac{d_{j_{n}}-Q[n]}{d_{j_{n}}-d_{j_{n}-1}}\) and to \(d_{j_{n}}\) with the remaining probability, where \(j_{n}:=\min\{j:d_{j}<Q[i]\leq d_{j+1}\}\). It is straightforward to note that each coordinate of \(\mathscr{C}(Q)\) can be represented using \(J\) bits.

#### 4.1.3 Setting the parameters

The desired convergence of the iterates \(\{Q^{(k)}\}\) is obtained by carefully choosing the parameters of the sub-routine RefineEstimate and the compression operator \(\mathscr{C}\). For all epochs \(k\geq 1\), we set the number of iterations \(I\) and the batch size \(B\) of RefineEstimate and the number of bits \(J\) of the compressor \(\mathscr{C}\) to be \(\lceil\frac{2}{\eta(1-\gamma)}\rceil\), \(\lceil\frac{2}{M}(\frac{12\gamma}{(1-\gamma)})^{2}\log(\frac{8KI|\mathcal{S}|| \mathcal{A}|}{\delta})\rceil\) and \(\lceil\log_{2}(\frac{70}{\eta(1-\gamma)}\sqrt{2}\frac{2}{M}\log(\frac{8KI| \mathcal{S}||\mathcal{A}|}{\delta}))\rceil\) respectively. The total number of epochs is set to \(K=\lceil\frac{1}{2}\log_{2}(\frac{1}{1-\gamma})\rceil+\lceil\frac{1}{2}\log_{ 2}(\frac{1}{(1-\gamma)\epsilon^{2}})\rceil\). The recentering sample sizes \(L_{k}\) and bounds \(D_{k}\) are set to be the following functions of epoch index \(k\):

\[L_{k}:=\frac{19600}{(1-\gamma)^{2}}\log\left(\frac{8KI|\mathcal{S}||\mathcal{A }|}{\delta}\right)\cdot\begin{cases}4^{k}&\text{if }k\leq K_{0}\\ 4^{k-K_{0}}&\text{if }k>K_{0}\end{cases};\quad D_{k}:=16\cdot\frac{2^{-k}}{1-\gamma}, \tag{13}\]where \(K_{0}=\lceil\frac{1}{2}\log_{2}(\frac{1}{1-\gamma})\rceil\). The piecewise definition of \(L_{k}\) is crucial to obtain the optimal dependence with respect to \(\frac{1}{1-\gamma}\), similar to the two-step procedure outlined in Wainwright (2019).

### Performance Guarantees

The following theorem characterizes the sample and communication complexity of Fed-DVR-Q.

**Theorem 2**.: _Consider any \(\delta\in(0,1)\) and \(\varepsilon\in(0,1]\). Under the federated learning setup described in Section 2.1, the sample and communication complexities of the Fed-DVR-Q algorithm, when run with the choice of parameters described in Sec. 4.1.3 and a learning rate \(\eta\in(0,1)\), satisfy the following relations for some universal constant \(C_{1}>0\):_

\[\text{SC}(\text{Fed-DVR-Q};\varepsilon,M,\delta) \leq\frac{C_{1}}{\eta M(1-\gamma)^{3}\varepsilon^{2}}\log_{2}\left( \frac{1}{(1-\gamma)\varepsilon}\right)\log\left(\frac{8KI|\mathcal{S}|| \mathcal{A}|}{\delta}\right),\] \[\text{CC}_{\text{round}}(\text{Fed-DVR-Q};\varepsilon,\delta) \leq\frac{16}{\eta(1-\gamma)}\log_{2}\left(\frac{1}{(1-\gamma) \varepsilon}\right),\] \[\text{CC}_{\text{bit}}(\text{Fed-DVR-Q};\varepsilon,\delta) \leq\frac{32|\mathcal{S}|\mathcal{A}|}{\eta(1-\gamma)}\log_{2} \left(\frac{1}{(1-\gamma)\varepsilon}\right)\log_{2}\left(\frac{70}{\eta(1- \gamma)}\sqrt{\frac{2}{M}\log\left(\frac{8KI|\mathcal{S}||\mathcal{A}|}{\delta }\right)}\right).\]

A proof of Theorem 2 can be found in Appendix C. A few implications of the theorem are in order.

Optimal Sample-Communication complexity trade-off.As shown by the above theorem, Fed-DVR-Q offers a linear speed up in the sample complexity with respect to the number of agents while simultaneously achieving the same order of communication complexity as dictated by the lower bound derived in Theorem 1, both in terms of frequency and bit level complexity. Moreover, Fed-DVR-Q is the _first_ Federated Q-Learning algorithm that achieves a sample complexity with optimal dependence on all the salient parameters, i.e., \(|\mathcal{S}|,|\mathcal{A}|\) and \(\frac{1}{1-\gamma}\), in addition to linear speedup w.r.t. to number of agents and thereby bridges the existing gap between upper and lower bounds on sample complexity for Federated Q-learning. Thus, Theorem 1 and 2 together provide a characterization of optimal operating point of the sample-communication complexity trade-off in Federated Q-learning.

Role of Minibatching.The commonly adopted approach in intermittent communication algorithm is to use a local update scheme that takes multiple small (i.e., \(B=\mathcal{O}(1)\)), noisy updates between communication rounds, as evident from the algorithm design in Khodadadian et al. (2022), Woo et al. (2023) and even numerous FL algorithms for stochastic optimization McMahan et al. (2017), Haddadpour et al. (2019), Khaled et al. (2020). In Fed-DVR-Q, we replace the local update scheme of taking multiple small, noisy updates by a single, large update with smaller variance, obtained by averaging the noisy updates over a minibatch of samples. The use of updates with smaller variance in variance reduced Q-learning yields the algorithm its name. While both the approaches result in similar sample complexity guarantees, the local update scheme requires more frequent averaging across clients to ensure that the bias of the estimate, also commonly referred to as "client drift", is not too large. On the other hand, the minibatching approach does not encounter the problem of bias accumulation from local updates and hence can afford more infrequent averaging allowing Fed-DVR-Q to achieve optimal order of communication complexity.

Compression.Fed-DVR-Q is the first algorithm in Federated Q-Learning to analyze and establish communication complexity at the bit level. All existing studies on Federated RL focus only on the frequency of communication and assume transmission of real numbers with infinite bit precision. On the other hand, the our analysis provides a more holistic view point of communication complexity and provides bounds at the bit level, which is of great practical significance. While some recent other studies (Wang et al., 2023) also consider quantization in Federated RL, their objective is to understand the impact of message size on convergence with no constraint on the frequency of communication, unlike the holistic viewpoint adopted in this work.

Conclusion and Future Directions

We presented a complete characterization of the sample-communication trade-off for Federated Q-learning algorithms with intermittent communication. We showed that no Federated Q-learning algorithm with intermittent communication can achieve a linear speedup with respect to the number of agents if its number of communication rounds are sublinear in \(\frac{1}{1-\gamma}\). We also proposed a new Federated Q-learning algorithm called Fed-DVR-Q that uses variance reduction along with minibatching to achieve optimal-order sample and communication complexities. In particular, we showed that Fed-DVR-Q has a sample complexity of \(\tilde{\mathcal{O}}(\frac{|\mathcal{S}||\mathcal{A}|}{M(1-\gamma)^{3}\delta^{2}})\), which is order-optimal in all salient problem parameters, and a communication complexity of \(\tilde{\mathcal{O}}(\frac{1}{1-\gamma})\) rounds and \(\tilde{\mathcal{O}}(\frac{|\mathcal{S}||\mathcal{A}|}{1-\gamma})\) bits.

The results in this work raise several interesting questions that are worth exploring. While we focus on the tabular setting in this work, it is of great interest to investigate to the trade-off in other settings where we use function approximation to model the \(Q^{\star}\) and \(V^{\star}\) functions. Moreover, it is interesting to explore the trade-off in the finite horizon setting, where there is no discount factor. Furthermore, it is also worthwhile to explore if the communication complexity can be further reduced by going beyond the class of intermittent communication algorithms.

## Acknowledgement

We would like to thank the anonymous reviewers for their constructive feedback. This work is supported in part by the grants NSF CCF-2007911, CCF-2106778, CNS-2148212, ECCS-2318441, ONR N00014-19-1-2404 and AFRL FA8750-20-2-0504, and in part by funds from federal agency and industry partners as specified in the Resilient & Intelligent NextG Systems (RINGS) program.

## References

* Assran et al. (2019) M. Assran, J. Romoff, N. Ballas, J. Pineau, and M. Rabbat. Gossip-based actor-learner architectures for deep reinforcement learning. In _Proceedings of the 33rd Annual Conference on Neural Information Processing Systems_, volume 32, 2019.
* Azar et al. (2013) M. G. Azar, R. Munos, and H. J. Kappen. Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model. _Machine Learning_, 91(3):325-349, 2013.
* Beck and Srikant (2012) C. Beck and R. Srikant. Error bounds for constant step-size q-learning. _Systems & Control Letters_, 61(12):1203-1208, 2012. ISSN 0167-6911.
* Borkar and Meyn (2000) V. S. Borkar and S. P. Meyn. The o.d.e. method for convergence of stochastic approximation and reinforcement learning. _SIAM Journal on Control and Optimization_, 38(2):447-469, 2000. doi: 10.1137/S0363012997331639.
* Braverman et al. (2016) M. Braverman, A. Garg, T. Ma, H. L. Nguyen, and D. P. Woodruff. Communication lower bounds for statistical estimation problems via a distributed data processing inequality. In _Proceedings of the 48th Annual ACM Symposium on Theory of Computing_, pages 1011-1020, 2016.
* Chen et al. (2021a) T. Chen, K. Zhang, G. B. Giannakis, and T. Basar. Communication-efficient policy gradient methods for distributed reinforcement learning. _IEEE Transactions on Control of Network Systems_, 9(2):917-929, 2021a.
* Chen et al. (2020) Z. Chen, S. T. Maguluri, S. Shakkottai, and K. Shanmugam. Finite-sample analysis of contractive stochastic approximation using smooth convex envelopes. In _Proceedings of the 34th Annual Conference on Neural Information Processing Systems_, volume 33, pages 8223-8234, 2020.
* Chen et al. (2021b) Z. Chen, S. T. Maguluri, S. Shakkottai, and K. Shanmugam. A lyapunov theory for finite-sample guarantees of asynchronous q-learning and td-learning variants, 2021b.
* Chen et al. (2021c) Z. Chen, Y. Zhou, and R. Chen. Multi-agent off-policy tdc with near-optimal sample and communication complexity. In _Proceedings of the 55th Asilomar Conference on Signals, Systems, and Computers_, pages 504-508, 2021c.
* Chen et al. (2021d)Z. Chen, Y. Zhou, R.-R. Chen, and S. Zou. Sample and communication-efficient decentralized actor-critic algorithms with finite-time analysis. In _Proceedings of the 39th International Conference on Machine Learning_, pages 3794-3834. PMLR, 2022.
* Doan et al. (2019) T. Doan, S. Maguluri, and J. Romberg. Finite-time analysis of distributed td (0) with linear function approximation on multi-agent reinforcement learning. In _Proceedings of the 36th International Conference on Machine Learning_, pages 1626-1635. PMLR, 2019.
* Doan et al. (2021) T. T. Doan, S. T. Maguluri, and J. Romberg. Finite-time performance of distributed temporal-difference learning with linear function approximation. _SIAM Journal on Mathematics of Data Science_, 3(1):298-320, 2021.
* Duchi et al. (2014) J. C. Duchi, M. I. Jordan, M. J. Wainwright, and Y. Zhang. Optimality guarantees for distributed statistical estimation, 2014. URL [http://arxiv.org/abs/1405.0782](http://arxiv.org/abs/1405.0782).
* Espeholt et al. (2018) L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In _Proceedings of the 35th International conference on machine learning_, pages 1407-1416. PMLR, 2018.
* Even-Dar and Mansour (2004) E. Even-Dar and Y. Mansour. Learning rates for q-learning. _Journal of Machine Learning Research_, 5, 2004. ISSN 1532-4435.
* Freedman (1975) D. A. Freedman. On tail probabilities for martingales. _The Annals of Probability_, 3(1):100-118, 1975.
* Haddadpour et al. (2019) F. Haddadpour, M. M. Kamani, M. Mahdavi, and V. R. Cadambe. Local SGD with periodic averaging: Tighter analysis and adaptive synchronization. In _Proceedings of the 33rd Annual Conference on Neural Information Processing Systems_, volume 32, 2019.
* Hasselt (2010) H. v. Hasselt. Double q-learning. In _Proceedings of the 23rd International Conference on Neural Information Processing Systems_, page 2613-2621. Curran Associates Inc., 2010.
* Jaakkola et al. (1993) T. Jaakkola, M. Jordan, and S. Singh. Convergence of stochastic iterative dynamic programming algorithms. In _Proceedings of the 7th Annual Conference on Neural Information Processing Systems_, volume 6, 1993.
* Jin et al. (2022) H. Jin, Y. Peng, W. Yang, S. Wang, and Z. Zhang. Federated reinforcement learning with environment heterogeneity. In _Proceedings of the 25th International Conference on Artificial Intelligence and Statistics_, pages 18-37. PMLR, 2022.
* Kakade (2001) S. M. Kakade. A natural policy gradient. _Proceedings of the 15th Annual Conference on Neural Information Processing Systems_, 14, 2001.
* Kearns and Singh (1998) M. Kearns and S. Singh. Finite-sample convergence rates for q-learning and indirect algorithms. In _Proceedings of the 12th Annual Conference on Neural Information Processing Systems_, 1998.
* Khaled et al. (2020) A. Khaled, K. Mishchenko, and P. Richtarik. Tighter Theory for Local SGD on Identical and Heterogeneous Data. In _Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, AISTATS_, pages 4519-4529. PMLR, 2020. URL [http://arxiv.org/abs/1909.04746](http://arxiv.org/abs/1909.04746).
* Khodadadian et al. (2022) S. Khodadadian, P. Sharma, G. Joshi, and S. T. Maguluri. Federated reinforcement learning: Linear speedup under markovian sampling. In _Proceedings of the 39th International Conference on Machine Learning_, pages 10997-11057. PMLR, 2022.
* Kober et al. (2013) J. Kober, J. A. Bagnell, and J. Peters. Reinforcement learning in robotics: A survey. _The International Journal of Robotics Research_, 32(11):1238-1274, 2013.
* Lan et al. (2024) G. Lan, D.-J. Han, A. Hashemi, V. Aggarwal, and C. G. Brinton. Asynchronous federated reinforcement learning with policy gradient updates: Algorithm design and convergence analysis, 2024.
* Li et al. (2021) G. Li, Y. Wei, Y. Chi, Y. Gu, and Y. Chen. Sample complexity of asynchronous q-learning: Sharper analysis and variance reduction. _IEEE Transactions on Information Theory_, 68(1):448-473, 2021.
* Li et al. (2021)G. Li, C. Cai, Y. Chen, Y. Wei, and Y. Chi. Is q-learning minimax optimal? a tight sample complexity analysis. _Operations Research_, 2023.
* Lim et al. (2020) H.-K. Lim, J.-B. Kim, J.-S. Heo, and Y.-H. Han. Federated reinforcement learning for training control policies on multiple iot devices. _Sensors_, 20(5), 2020. ISSN 1424-8220. doi: 10.3390/s20051359.
* Liu and Olshevsky (2023) R. Liu and A. Olshevsky. Distributed TD(0) with almost no communication. _IEEE Control Systems Letters_, 7:2892-2897, 2023.
* McMahan et al. (2017) B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS_, pages 1273-1282. PMLR, 2017.
* Mnih et al. (2016) V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In _Proceedings of the 33rd International Conference on Machine Learning_, pages 1928-1937. PMLR, 2016.
* Puterman (2014) M. Puterman. _Markov decision processes: Discrete Stochastic Dynamic Programming_. John Wiley & Sons, 2014.
* Qu and Wierman (2020) G. Qu and A. Wierman. Finite-time analysis of asynchronous stochastic approximation and q-learning. In _Proceedings of the 33rd Conference on Learning Theory_, pages 3185-3205. PMLR, 2020.
* Salgia and Zhao (2023) S. Salgia and Q. Zhao. Distributed linear bandits under communication constraints. In _Proceedings of the 40th International Conference on Machine Learning, ICML_, pages 29845-29875. PMLR, 2023.
* Shen et al. (2023) H. Shen, K. Zhang, M. Hong, and T. Chen. Towards understanding asynchronous advantage actor-critic: Convergence and linear speedup. _IEEE Transactions on Signal Processing_, 2023.
* Shi and Shen (2021) C. Shi and C. Shen. Federated Multi-Armed Bandits. In _Proceedings of the 35th AAAI Conference on Artificial Intelligence_, pages 9603-9611, 2021. URL [http://arxiv.org/abs/2101.12204](http://arxiv.org/abs/2101.12204).
* Silver et al. (2016) D. Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershalvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of go with deep neural networks and tree search. _Nature_, 529:484-489, 2016.
* Sun et al. (2020) J. Sun, G. Wang, G. B. Giannakis, Q. Yang, and Z. Yang. Finite-time analysis of decentralized temporal-difference learning with linear function approximation. In _Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics_, pages 4485-4495. PMLR, 2020.
* Sutton and Barton (2018) R. Sutton and A. Barton. _Reinforcement learning: An introduction_. MIT Press, 2018.
* Szepesvari (1997) C. Szepesvari. The asymptotic convergence-rate of q-learning. _Proceedings of the 11th Annual Conference on Neural Information Processing Systems_, 10, 1997.
* Tian et al. (2024) H. Tian, I. C. Paschalidis, and A. Olshevsky. One-shot averaging for distributed TD (\(\lambda\)) under Markov sampling. _IEEE Control Systems Letters_, 2024.
* Tsitsiklis (1994) J. N. Tsitsiklis. Asynchronous stochastic approximation and q-learning. _Machine learning_, 16:185-202, 1994.
* Tsitsiklis and Luo (1987) J. N. Tsitsiklis and Z. Q. Luo. Communication complexity of convex optimization. _Journal of Complexity_, 3(3):231-243, 1987. ISSN 10902708. doi: 10.1016/0885-064X(87)90013-6.
* Vershynin (2018) R. Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* Wai (2020) H.-T. Wai. On the convergence of consensus algorithms with markovian noise and gradient bias. In _Proceedings of 59th IEEE Conference on Decision and Control_, pages 4897-4902. IEEE, 2020.
* Wainwright (2019a) M. Wainwright. Stochastic approximation with cone-contractive operators: Sharp l-infinity-bounds for q -learning, 2019a.

M. Wainwright. Variance-reduced q-learning is minimax optimal, 2019b.
* Wang et al. (2020) G. Wang, S. Lu, G. Giannakis, G. Tesauro, and J. Sun. Decentralized td tracking with linear function approximation and its finite-time analysis. _Proceedings of the 34th Annual Conference on Neural Information Processing Systems_, 33:13762-13772, 2020.
* Wang et al. (2023) H. Wang, A. Mitra, H. Hassani, G. J. Pappas, and J. Anderson. Federated temporal difference learning with linear function approximation under environmental heterogeneity, 2023.
* Watkins & Dayan (1992) C. J. Watkins and P. Dayan. Q-learning. _Machine learning_, 8:279-292, 1992.
* Woo et al. (2023) J. Woo, G. Joshi, and Y. Chi. The blessing of heterogeneity in federated q-learning: Linear speedup and beyond. In _Proceedings of the 40th International Conference on Machine Learning_, page 37157-37216, 2023.
* Woo et al. (2024) J. Woo, L. Shi, G. Joshi, and Y. Chi. Federated offline reinforcement learning: Collaborative single-policy coverage suffices, 2024.
* Woodworth et al. (2018) B. Woodworth, J. Wang, A. Smith, B. McMahan, and N. Srebro. Graph oracle models, lower bounds, and gaps for parallel stochastic optimization. In _Proceedings of the 32nd Annual Conference on Neural Information Processing Systems_, volume 31, 2018.
* Woodworth et al. (2021) B. Woodworth, B. Bullins, O. Shamir, and N. Srebro. The min-max complexity of distributed stochastic convex optimization with intermittent communication. In _Proceedings of the 34th Conference on Learning Theory, COLT_, pages 4386-4437. PMLR, 2021.
* Xie & Song (2023) Z. Xie and S. Song. Fedkl: Tackling data heterogeneity in federated reinforcement learning by penalizing kl divergence. _IEEE Journal on Selected Areas in Communications_, 41(4):1227-1242, 2023.
* Yang et al. (2023) T. Yang, S. Cen, Y. Wei, Y. Chen, and Y. Chi. Federated natural policy gradient methods for multi-task reinforcement learning, 2023.
* Yurtsever et al. (2020) E. Yurtsever, J. Lambert, A. Carballo, and K. Takeda. A survey of autonomous driving: Common practices and emerging technologies. _IEEE access_, 8:58443-58469, 2020.
* Zeng et al. (2021a) S. Zeng, M. A. Anwar, T. T. Doan, A. Raychowdhury, and J. Romberg. A decentralized policy gradient approach to multi-task reinforcement learning. In _Proceedings of the 37th Conference on Uncertainty in Artificial Intelligence, UAI_, pages 1002-1012. PMLR, 2021a.
* Zeng et al. (2021b) S. Zeng, T. T. Doan, and J. Romberg. Finite-time analysis of decentralized stochastic approximation with applications in multi-agent and multi-task learning. In _Proceedings of the 60th IEEE Conference on Decision and Control_, pages 2641-2646. IEEE, 2021b.
* Zhang et al. (2024) C. Zhang, H. Wang, A. Mitra, and J. Anderson. Finite-time analysis of on-policy heterogeneous federated reinforcement learning, 2024.

Additional details about RefineEstimate

We outline below the pseudocode of the RefineEstimate routine described in Sec. 4.1.1.

```
1:Input: Initial estimate \(\overline{Q}\), batch size \(B\), Number of iterations \(I\), recentering sample size \(L\), quantization bound \(D\), message size \(J\)
2:// Build an approximation for \(\mathcal{T}\overline{Q}\) which is to be used for variance reduced updates
3:for\(m=1,2,\ldots,M\)do
4: Draw \(\llbracket I/M\rrbracket\) i.i.d. samples from the generative model for each \((s,a)\) pair and evaluate \(\widetilde{\mathcal{T}}_{L}^{(m)}(\overline{Q})\) according to Eqn. (9)
5: Send \(\mathscr{C}(\widetilde{\mathcal{T}}_{L}^{(m)}(\overline{Q})-\overline{Q};D,J)\) to the server
6: Receive \(\frac{1}{M}\sum_{m=1}^{M}\mathscr{C}(\widetilde{\mathcal{T}}_{L}^{(m)}( \overline{Q})-\overline{Q};D,J)\) from the server and compute \(\widetilde{\mathcal{T}}_{L}(\overline{Q})\) according to Eqn. (10)
7:endfor
8:\(Q_{0}\leftarrow\overline{Q}\)
9:// Variance reduced updates with minibatching
10:for\(i=1,2,\ldots,I\)do
11:for\(m=1,2,\ldots,M\)do
12: Draw \(B\) i.i.d. samples from the from the generative model for each \((s,a)\) pair
13: Compute \(Q_{i-\frac{1}{2}}^{m}\) according to Eqn. (11)
14: Send \(\mathscr{C}(Q_{i-\frac{1}{2}}^{m}-Q_{i-1};D,J)\) to the server
15: Receive \(\frac{1}{M}\sum_{m=1}^{M}\mathscr{C}(Q_{i}^{m}-Q_{i-1};D,J)\) from the server and compute \(Q_{i}\) according to Eqn. (12)
16:endfor
17:endfor
18:return\(Q_{I}\)
```

**Algorithm 3**RefineEstimate\((\overline{Q},B,I,L,D,J)\)

## Appendix B Proof of Theorem 1

In this section, we prove the main result of the paper, the lower bound on the communication complexity of federated Q-learning algorithms. At a high level, the proof consists of the following three steps.

Introducing the "hard" MDP instance.The proof builds upon analyzing the behavior of a generic algorithm \(\mathscr{A}\) outlined in Algorithm 1 over a particular instance of MDP. The particular choice of MDP is inspired by, and borrowed from, other lower bound proofs in the single-agent setting (Li et al., 2023) and helps highlight core issues that lie at the heart of the sample-communication complexity trade-off. Following Li et al. (2023), the construction is first over a small state-action space that allows us to focus on a simpler problem before generalizing it to larger state-action spaces.

Establishing the performance of intermittent communication algorithms.In the second step, we analyze the error of the iterates generated by an intermittent communication algorithm \(\mathscr{A}\). The analysis is inspired by the single-agent analysis in Li et al. (2023), which highlights the underlying bias-variance trade-off. Through careful analysis of the algorithm dynamics in the federated setting, we uncover the impact of communication on the bias-variance trade-off and the resulting error of the iterates to obtain the lower bound on the communication complexity.

Generalization to larger MDPs.As the last step, we generalize our construction of the "hard" instance to more general state-action space and extend our insights to obtain the statement of the theorem.

### Introducing the "hard" instance

We first introduce an MDP instance \(\mathcal{M}_{h}\) that we will use throughout the proof to establish the result. Note that this MDP is identical to the one considered in Li et al. (2023) to establish the lower bounds on the performance of single-agent Q-learning algorithm. It consists of four states \(\mathcal{S}=\{0,1,2,3\}\). Let \(\mathcal{A}_{s}\) denote the action set associated with the state \(s\). The probability transition kernel and the reward function of \(\mathcal{M}_{h}\) is given as follows:

\[\mathcal{A}_{0}=\{1\} \qquad P(0|0,1)=1 r(0,1)=0, \tag{14a}\] \[\mathcal{A}_{1}=\{1,2\} \qquad P(1|1,1)=p P(0|1,1)=1-p r(1,1)=1,\] (14b) \[\qquad P(1|1,2)=p P(0|1,2)=1-p r(1,2)=1,\] (14c) \[\mathcal{A}_{2}=\{1\} \qquad P(2|2,1)=p P(0|2,1)=1-p r(2,1)=1,\] (14d) \[\mathcal{A}_{3}=\{1\} \qquad P(3|3,1)=1 r(3,1)=1, \tag{14e}\]

where the parameter \(p=\frac{4\gamma-1}{3\gamma}\). We have the following results about the optimal \(Q\) and \(V\) functions of this hard MDP instance.

**Lemma 1** ([Li et al., 2023, Lemma 3]).: _Consider the MDP \(\mathcal{M}_{h}\) constructed in Eqn. (14). We have,_

\[V^{\star}(0)=Q^{\star}(0,1)=0\] \[V^{\star}(1)=Q^{\star}(1,1)=Q^{\star}(1,2)=V^{\star}(2)=Q^{\star }(2,1)=\frac{1}{1-\gamma p}=\frac{3}{4(1-\gamma)}\] \[V^{\star}(3)=Q^{\star}(3,1)=\frac{1}{1-\gamma}.\]

Throughout the next section of the proof, we focus on this MDP with four states and two actions. In Appendix B.4, we generalize the proof to larger state-action spaces.

### Notation and preliminary results

For convenience, we first define some notation that will be used throughout the proof.

Useful relations of the learning rates.We consider two kinds of step size sequences that are commonly used in Q-learning -- the constant step size schedule, i.e., \(\eta_{t}=\eta\) for all \(t\in\{1,2,\ldots,T\}\) and the rescaled linear step size schedule, i.e., \(\eta_{t}=\frac{1}{1+e_{n}(1-\gamma)t}\), where \(c_{n}>0\) is a universal constant that is independent of the problem parameters.

We define the following quantities:

\[\eta_{k}^{(t)}=\eta_{k}\prod_{i=k+1}^{t}(1-\eta_{i}(1-\gamma p)) \qquad\text{ for all }0\leq k\leq t, \tag{15}\]

where we take \(\eta_{0}=1\) and use the convention throughout the proof that if a product operation does not have a valid index, we take the value of that product to be 1. For any integer \(0\leq\tau<t\), we have the following relation, which will be proved at the end of this subsection for completeness:

\[\prod_{k=\tau+1}^{t}(1-\eta_{k}(1-\gamma p))+(1-\gamma p)\sum_{k= \tau+1}^{t}\eta_{k}^{(t)}=1. \tag{16}\]

Similarly, we also define,

\[\widetilde{\eta}_{k}^{(t)}=\eta_{k}\prod_{i=k+1}^{t}(1-\eta_{i}) \qquad\text{ for all }0\leq k\leq t, \tag{17}\]

which satisfies the relation

\[\prod_{k=\tau+1}^{t}(1-\eta_{k})+\sum_{k=\tau+1}^{t}\widetilde{ \eta}_{k}^{(t)}=1. \tag{18}\]for any integer \(0\leq\tau<t\). The claim follows immediately by plugging \(p=0\) in (16). Note that for constant step size, the sequence \(\widetilde{\eta}_{k}^{(t)}\) is clearly increasing. For the rescaled linear step size, we have,

\[\frac{\widetilde{\eta}_{k-1}^{(t)}}{\widetilde{\eta}_{k}^{(t)}}= \frac{\eta_{k}}{\eta_{k-1}(1-\eta_{k})}=1-\frac{(1-c_{\eta}(1-\gamma))\eta_{k} }{1-c_{\eta}(1-\gamma)\eta_{k}}\leq 1 \tag{19}\]

whenever \(c_{\eta}\leq\frac{1}{1-\gamma}\). Thus, \(\widetilde{\eta}_{k}^{(t)}\) is an increasing sequence as long as \(c_{\eta}\leq\frac{1}{1-\gamma}\). Similarly, \(\eta_{k}^{(t)}\) is also clearly increasing for the constant step size schedule. For the rescaled linear step size schedule, we have,

\[\frac{\eta_{k-1}^{(t)}}{\eta_{k}^{(t)}}=\frac{\eta_{k}}{\eta_{k- 1}(1-\eta_{k}(1-\gamma p))}\leq\frac{\eta_{k}}{\eta_{k-1}(1-\eta_{k})}\leq 1,\]

whenever \(c_{\eta}\leq\frac{1}{1-\gamma}\). The last bound follows from Eqn. (19).

Proof of (16).We can show the claim using backward induction. For the base case, note that,

\[(1-\gamma p)\eta_{t}^{(t)}+(1-\gamma p)\eta_{t-1}^{(t)} =(1-\gamma p)\eta_{t}+(1-\gamma p)\eta_{t-1}(1-(1-\gamma p)\eta_{ t})\] \[=1-(1-\eta_{t}(1-\gamma p))(1-\eta_{t-1}(1-\gamma p))=1-\prod_{k= t-1}^{t}(1-\eta_{k}(1-\gamma p)),\]

as required. Assume (16) is true for some \(\tau\). We have,

\[(1-\gamma p)\sum_{k=\tau}^{t}\eta_{k}^{(t)} =(1-\gamma p)\eta_{\tau}^{t}+(1-\gamma p)\sum_{k=\tau+1}^{t}\eta_{ k}^{(t)}\] \[=(1-\gamma p)\eta_{\tau}\prod_{k=\tau+1}^{t}(1-\eta_{k}(1-\gamma p ))+1-\prod_{k=\tau+1}^{t}(1-\eta_{k}(1-\gamma p))\] \[=1-\prod_{k=\tau}^{t}(1-\eta_{k}(1-\gamma p)),\]

thus completing the induction step.

Sample transition matrix.Recall \(Z\in\mathcal{S}^{|\mathcal{S}||\mathcal{A}|}\) is a random vector whose \((s,a)\)-th coordinate is drawn from the distribution \(P(\cdot|s,a)\). We use \(\widehat{P}_{t}^{m}\) to denote the sample transition at time \(t\) and agent \(m\) obtained by averaging \(B\) i.i.d. samples from the generative model. Specifically let \(\{Z_{t,b}^{m}\}_{b=1}^{B}\) denote a collection of \(B\) i.i.d. copies of \(Z\) collected at time \(t\) at agent \(m\). Then, for all \(s,a,s^{\prime}\),

\[\widehat{P}_{t}^{m}(s^{\prime}|s,a)=\frac{1}{B}\sum_{b=1}^{B}P_{t,b}^{m}(s^{\prime}|s,a), \tag{20}\]

where \(P_{t,b}^{m}(s^{\prime}|s,a)=\mathbb{1}\left\{Z_{t,b}^{m}(s,a)=s^{\prime}\right\}\) for \(s^{\prime}\in\mathcal{S}\).

Preliminary relations of the iterates.We state some preliminary relations regarding the evolution of the Q-function and the value function across different agents that will be helpful for the analysis later.

We begin with the state \(0\), where we have \(Q_{t}^{m}(0,1)=V_{t}^{m}(0)=0\) for all agents \(m\in[M]\) and \(t\in[T]\). This follows almost immediately from the fact that state \(0\) is an absorbing state with zero reward. Note that \(Q_{0}^{m}(0,1)=V_{0}^{m}(0)=0\) holds for all clients \(m\in[M]\). Assuming that \(Q_{t-1}^{m}(0,1)=V_{t-1}^{m}(0)=0\) for all clients for some time instant \(t-1\), by induction, we have,

\[Q_{t-1/2}^{m}(0,1)=(1-\eta_{t})Q_{t-1}^{m}(0,1)+\eta_{t}(\gamma V _{t-1}^{m}(0))=0.\]

Consequently, \(Q_{t}^{m}(0,1)=0\) and \(V_{t}^{m}(0)=0\), for all agents \(m\), irrespective of whether there is averaging.

For state \(3\), the iterates satisfy the following relation:

\[Q^{m}_{t-1/2}(3,1) =(1-\eta_{t})Q^{m}_{t-1}(3,1)+\eta_{t}(1+\gamma V^{m}_{t-1}(3))\] \[=(1-\eta_{t})Q^{m}_{t-1}(3,1)+\eta_{t}(1+\gamma Q^{m}_{t-1}(3,1))\] \[=(1-\eta_{t}(1-\gamma))Q^{m}_{t-1}(3,1)+\eta_{t},\]

where the second step follows by noting \(V^{m}_{t}(3)=Q^{m}_{t}(3,1)\). Once again, one can note that averaging step does not affect the update rule implying that the following holds for all \(m\in[M]\) and \(t\in[T]\):

\[V^{m}_{t}(3)=Q^{m}_{t}(3,1)=\sum_{k=1}^{t}\eta_{k}\left(\prod_{i= k+1}^{t}(1-\eta_{i}(1-\gamma))\right)=\frac{1}{1-\gamma}\left[1-\prod_{i=1}^{ t}(1-\eta_{i}(1-\gamma))\right], \tag{21}\]

where the last step follows from Eqn. (16) with \(p=1\).

Similarly, for state \(1\) and \(2\), we have,

\[Q^{m}_{t-1/2}(1,1) =(1-\eta_{t})Q^{m}_{t-1}(1,1)+\eta_{t}(1+\gamma\widehat{P}^{m}_{t }(1|1,1)V^{m}_{t-1}(1)), \tag{22}\] \[Q^{m}_{t-1/2}(1,2) =(1-\eta_{t})Q^{m}_{t-1}(1,2)+\eta_{t}(1+\gamma\widehat{P}^{m}_{t }(1|1,2)V^{m}_{t-1}(1)),\] (23) \[Q^{m}_{t-1/2}(2,1) =(1-\eta_{t})Q^{m}_{t-1}(2,1)+\eta_{t}(1+\gamma\widehat{P}^{m}_{t }(2|2,1)V^{m}_{t-1}(2)). \tag{24}\]

Since the averaging makes a difference in the update rule, we further analyze the update as required in later proofs.

### Main analysis

We first focus on establishing a bound on the number of communication rounds, i.e., \(\mathsf{CC_{round}}(\mathscr{A})\) (where we drop the dependency with other parameters for notational simplicity), and then use this lower bound to establish the bound on the bit level communication complexity \(\mathsf{CC_{bit}}(\mathscr{A})\).

To establish the lower bound on \(\mathsf{CC_{round}}(\mathscr{A})\) for any intermittent communication algorithm \(\mathscr{A}\), we analyze the convergence behavior of \(\mathscr{A}\) on the MDP \(\mathcal{M}_{h}\). We assume that the averaging step in line 6 of Algorithm 1 is carried out exactly. Since the use of compression only makes the problem harder, it is sufficient for us to consider the case where there is no loss of information in the averaging step for establishing a lower bound. Lastly, throughout the proof, without loss of generality we assume that

\[\log N\leq\frac{1}{1-\gamma}, \tag{25}\]

otherwise, the lower bound in Theorem 1 reduces to the trivial lower bound.

We divide the proof into following three parts based on the choice of learning rates and batch sizes:

1. Small learning rates: For constant learning rates, \(0\leq\eta<\frac{1}{(1-\gamma)T}\) and for rescaled linear learning rates, the constant \(c_{\eta}\) satisfies \(c_{\eta}\geq\log T\).
2. Large learning rates with small \(\eta_{T}/(BM)\): For constant learning rates, \(\eta\geq\frac{1}{(1-\gamma)T}\) and for rescaled linear learning rates, the constant \(c_{\eta}\) satisfies \(0\leq c_{\eta}\leq\log T\leq\frac{1}{1-\gamma}\) (c.f. (25)). Additionally, the ratio \(\frac{\eta_{T}}{BM}\) satisfies \(\frac{\eta_{T}}{BM}\leq\frac{1-\gamma}{100}\).
3. Large learning rates with large \(\eta_{T}/(BM)\): We have the same condition on the learning rates as above. However, in this case the ratio \(\frac{\eta_{T}}{BM}\) satisfies \(\frac{\eta_{T}}{BM}>\frac{1-\gamma}{100}\).

We consider each of the cases separately in the following three subsections.

#### b.3.1 Small learning rates

In this subsection, we prove the lower bound for small learning rates, which follow from similar arguments in Li et al. (2023).

For this case, we focus on the dynamics of state \(2\). We claim that the same relation established in Li et al. (2023) continues to hold, which will be established momentarily:

\[\mathbb{E}[V_{T}^{m}(2)]=\left(\frac{1}{M}\sum_{j=1}^{M}\mathbb{E}[V_{T}^{j}(2)] \right)=\sum_{k=1}^{T}\eta_{k}^{(t)}=\frac{1-\eta_{0}^{(T)}}{1-\gamma p}. \tag{26}\]

Consequently, for all \(m\in[M]\), we have

\[V^{\star}(2)-\mathbb{E}[V_{T}^{m}(2)]=\frac{\eta_{0}^{(T)}}{1-\gamma p}. \tag{27}\]

To obtain lower bound on \(V^{\star}(2)-\mathbb{E}[V_{T}^{m}(2)]\), we need to obtain a lower bound on \(\eta_{0}^{(T)}\), which from (Li et al., 2023, Eqn. (120)) we have

\[\log(\eta_{0}^{(T)})\geq-1.5\sum_{t=1}^{T}\eta(1-\gamma p)\geq-2\sum_{t=1}^{T} \frac{1}{t\log T}\geq-2\qquad\Longrightarrow\qquad\eta_{0}^{(T)}\geq e^{-2}\]

when \(T\geq 16\) for both choices of learning rates. On plugging this bound in (27), we obtain,

\[\mathbb{E}[\|Q_{T}^{m}-Q^{\star}\|_{\infty}]\geq\mathbb{E}[|Q^{\star}(2)-Q_{T} ^{m}(2)|]\geq V^{\star}(2)-\mathbb{E}[V_{T}^{m}(2)]\geq\frac{3}{4e^{2}(1-\gamma )\sqrt{N}} \tag{28}\]

holds for all \(m\in[M]\), \(N\geq 1\) and \(M\geq 2\). Thus, it can be noted that the error rate \(\textsf{ER}(\mathscr{A};N,M)\) is bounded away from a constant value irrespective of the number of agents and the number of communication rounds. Thus, even with \(\textsf{CC}_{\textsf{round}}=\Omega(T)\), we will not observe any collaborative gain if the step size is too small.

Proof of (26).Recall that from (24), we have,

\[Q_{t-1/2}^{m}(2,1)=(1-\eta_{t})V_{t-1}^{m}(2)+\eta_{t}(1+\gamma\widehat{P}_{t} ^{m}(2|2,1)V_{t-1}^{m}(2)).\]

Here, \(Q_{t-1}^{m}(2,1)=V_{t-1}^{m}(2)\) as the second state has only a single action.

* When \(t\) is not an averaging instant, we have, \[V_{t}^{m}(2)=Q_{t}^{m}(2,1)=(1-\eta_{t})V_{t-1}^{m}(2)+\eta_{t}(1+\gamma \widehat{P}_{t}^{m}(2|2,1)V_{t-1}^{m}(2)).\] (29) On taking expectation on both sides of the equation, we obtain, \[\mathbb{E}[V_{t}^{m}(2)] =(1-\eta_{t})\mathbb{E}[V_{t-1}^{m}(2)]+\eta_{t}(1+\gamma \mathbb{E}[\widehat{P}_{t}^{m}(2|2,1)V_{t-1}^{m}(2)])\] \[=(1-\eta_{t})\mathbb{E}[V_{t-1}^{m}(2)]+\eta_{t}\left(1+\gamma \mathbb{E}[\widehat{P}_{t}^{m}(2|2,1)]\mathbb{E}[V_{t-1}^{m}(2)]\right)\] \[=(1-\eta_{t})\mathbb{E}[V_{t-1}^{m}(2)]+\eta_{t}\left(1+\gamma p \mathbb{E}[V_{t-1}^{m}(2)]\right)\] \[=(1-\eta_{t}(1-\gamma p))\mathbb{E}[V_{t-1}^{m}(2)]+\eta_{t}.\] (30) In the second step, we used the fact that \(\widehat{P}_{t}^{m}(2|2,1)\) is independent of \(V_{t-1}^{m}(2)\).
* Similarly, if \(t\) is an averaging instant, we have, \[V_{t}^{m}(2) =Q_{t}^{m}(2,1)=\frac{1}{M}\sum_{j=1}^{M}Q_{t-1/2}^{j}(2,1)\] \[=(1-\eta_{t})\frac{1}{M}\sum_{j=1}^{M}V_{t-1}^{j}(2)+\frac{1}{M} \sum_{j=1}^{M}\eta_{t}(1+\gamma\widehat{P}_{t}^{j}(2|2,1)V_{t-1}^{j}(2)).\] (31) Once again, upon taking expectation we obtain, \[\mathbb{E}[V_{t}^{m}(2)]=(1-\eta_{t})\frac{1}{M}\sum_{j=1}^{M}\mathbb{E}[V_{t-1 }^{j}(2)]+\frac{1}{M}\sum_{j=1}^{M}\eta_{t}(1+\gamma\mathbb{E}[\widehat{P}_{t} ^{j}(2|2,1)V_{t-1}^{j}(2)])\]\[=(1-\eta_{t})\frac{1}{M}\sum_{j=1}^{M}\mathbb{E}[V_{t-1}^{j}(2)]+ \frac{1}{M}\sum_{j=1}^{M}\eta_{t}(1+\gamma p\mathbb{E}[V_{t-1}^{j}(2)])\] \[=(1-\eta_{t}(1-\gamma p))\left(\frac{1}{M}\sum_{j=1}^{M}\mathbb{E} [V_{t-1}^{j}(2)]\right)+\eta_{t}. \tag{32}\]

Eqns. (30) and (32) together imply that for all \(t\in[T]\),

\[\left(\frac{1}{M}\sum_{m=1}^{M}\mathbb{E}[V_{t}^{m}(2)]\right)=(1-\eta_{t}(1- \gamma p))\left(\frac{1}{M}\sum_{m=1}^{M}\mathbb{E}[V_{t-1}^{m}(2)]\right)+ \eta_{t}. \tag{33}\]

On unrolling the above recursion with \(V_{0}^{m}=0\) for all \(m\in[M]\), we obtain the desired claim (26).

#### b.3.2 Large learning rates with small \(\frac{\eta_{T}}{BM}\)

In this subsection, we prove the lower bound for case of large learning rates when the ratio \(\frac{\eta_{T}}{BM}\) is small. For the analysis in this part, we focus on the dynamics of state \(1\). Unless otherwise specified, throughout the section we implicitly assume that the state is \(1\).

We further define a key parameter that will play a key role in the analysis:

\[\tau:=\min\{k\in\mathbb{N}:\forall\;t\geq k,\eta_{t}\leq\eta_{k}\leq 3\eta_{t}\}. \tag{34}\]

It can be noted that for constant step size sequence \(\tau=1\) and for rescaled linear stepsize \(\tau=T/3\).

Step 1: introducing an auxiliary sequence.We define an auxiliary sequence \(\widehat{Q}_{t}^{m}(a)\) for \(a\in\{1,2\}\) and all \(t=1,2,\ldots,T\) to aid our analysis, where we drop the dependency with state \(s=1\) for simplicity. The evolution of the sequence \(\widehat{Q}_{t}^{m}\) is defined in Algorithm 4, where \(\widehat{V}_{t}^{m}=\max_{a\in\{1,2\}}\widehat{Q}_{t}^{m}(a)\). In other words, the iterates \(\{\widehat{Q}_{t}^{m}\}\) evolve exactly as the iterates of Algorithm 1 except for the fact that sequence \(\{\widehat{Q}_{t}^{m}\}\) is initialized at the optimal \(Q\)-function of the MDP. We would like to point out that we assume that the underlying stochasticity is also identical in the sense that the evolution of both \(Q_{t}^{m}\) and \(\widehat{Q}_{t}^{m}\) is governed by the same \(\widehat{P}_{t}^{m}\) matrices. The following lemma controls the error between the iterates \(Q_{t}^{m}\) and \(\widehat{Q}_{t}^{m}\), allowing us to focus only on \(\widehat{Q}_{t}^{m}\).

```
1:Input : \(T,R,\{\eta_{t}\}_{t=1}^{T},\mathcal{C}=\{t_{r}\}_{r=1}^{R},B\)
2:Set \(\widehat{Q}_{0}^{m}(a)\gets Q^{*}(1,a)\) for \(a\in\{1,2\}\) and all agents \(m\)// Different initialization
3:for\(t=1,2,\ldots,T\)do
4:for\(m=1,2,\ldots,M\)do
5: Compute \(\widehat{Q}_{t-\frac{1}{2}}^{m}\) according to Eqn. (7)
6: Compute \(\widehat{Q}_{t}^{m}\) according to Eqn. (8)
7:endfor
8:endfor
```

**Algorithm 4** Evolution of \(\widehat{Q}\)

**Lemma 2**.: _The following relation holds for all agents \(m\in[M]\), all \(t\in[T]\) and \(a\in\{1,2\}\):_

\[Q_{t}^{m}(1,a)-\widehat{Q}_{t}^{m}(a)\geq-\frac{1}{1-\gamma}\prod_{i=1}^{t}(1- \eta_{i}(1-\gamma)).\]

By Lemma 2, bounding the error of the sequence \(\widehat{Q}_{t}^{m}\) allows us to obtain a bound on the error of \(Q_{t}^{m}\). To that effect, we define the following terms for any \(t\leq T\) and all \(m\in[M]\):

\[\Delta_{t}^{m}(a):=\widehat{Q}_{t}^{m}(a)-Q^{*}(1,a);\quad a=1,2;\]\[\Delta_{t,\max}^{m}=\max_{a\in\{1,2\}}\Delta_{t}^{m}(a).\]

In addition, we use \(\overline{\Delta}_{t}=\frac{1}{M}\sum_{m=1}^{M}\Delta_{t}^{m}\) to denote the error of the averaged iterate1, and similarly,

Footnote 1: We use this different notation in appendix as opposed to the half-time indices used in the main text to improve readability of the proof.

\[\overline{\Delta}_{t,\max}:=\max_{a\in\{1,2\}}\overline{\Delta}_{t}(a). \tag{35}\]

We first derive a basic recursion regarding \(\Delta_{t}^{m}(a)\). From the iterative update rule in Algorithm 4, we have,

\[\Delta_{t}^{m}(a) =(1-\eta_{t})\Delta_{t-1}^{m}(a)+\eta_{t}(1+\gamma\widehat{P}_{t} ^{m}(1|1,a)\widehat{V}_{t-1}^{m}-Q^{\star}(1,a))\] \[=(1-\eta_{t})\Delta_{t-1}^{m}(a)+\eta_{t}\gamma(\widehat{P}_{t}^{ m}(1|1,a)\widehat{V}_{t-1}^{m}-pV^{\star}(1))\] \[=(1-\eta_{t})\Delta_{t-1}^{m}(a)+\eta_{t}\gamma(p(\widehat{V}_{t- 1}^{m}-V^{\star}(1))+(\widehat{P}_{t}^{m}(1|1,a)-p)\widehat{V}_{t-1})\] \[=(1-\eta_{t})\Delta_{t-1}^{m}(a)+\eta_{t}\gamma(p\Delta_{t-1,\max }^{m}+(\widehat{P}_{t}^{m}(1|1,a)-p)\widehat{V}_{t-1}^{m}).\]

Here in the last line, we used the following relation:

\[\Delta_{t,\max}^{m}=\max_{a\in\{1,2\}}(\widehat{Q}_{t}^{m}(a)-Q^{\star}(1,a))= \max_{a\in\{1,2\}}\widehat{Q}_{t}^{m}(a)-V^{\star}(1)=\widehat{V}_{t-1}^{m}-V^ {\star}(1),\]

as \(Q^{\star}(1,1)=Q^{\star}(1,2)=V^{\star}(1)\).

Recursively unrolling the above expression, and using the expression (17), we obtain the following relation: for any \(t^{\prime}<t\) when there is no averaging during the interval \((t^{\prime},t)\)

\[\Delta_{t}^{m}(a)=\left(\prod_{k=t^{\prime}+1}^{t}(1-\eta_{k})\right)\Delta_{t ^{\prime}}^{m}(a)+\sum_{k=t^{\prime}+1}^{t}\widetilde{\eta}_{k}^{(t)}\gamma(p \Delta_{k-1,\max}^{m}+(\widehat{P}_{k}^{m}(1|1,a)-p)\widehat{V}_{k-1}^{m}). \tag{36}\]

For any \(t^{\prime},t\) with \(t^{\prime}<t\), we define the notation

\[\varphi_{t^{\prime},t} :=\prod_{k=t^{\prime}+1}^{t}(1-\eta_{k}), \tag{37}\] \[\xi_{t^{\prime},t}^{m}(a) :=\sum_{k=t^{\prime}+1}^{t}\widetilde{\eta}_{k}^{(t)}\gamma( \widehat{P}_{k}^{m}(1|1,a)-p)\widehat{V}_{k-1}^{m},\quad a=1,2;\] (38) \[\xi_{t^{\prime},t,\max}^{m} :=\max_{a\in\{1,2\}}\xi_{t^{\prime},t}^{m}(a). \tag{39}\]

Note that by definition, \(\mathbb{E}[\xi_{t^{\prime},t}^{m}(a)]=0\) for \(a\in\{1,2\}\) and all \(m\), \(t^{\prime}\) and \(t\). Plugging them into the previous expression leads to the simplified expression

\[\Delta_{t}^{m}(a)=\varphi_{t^{\prime},t}\Delta_{t^{\prime}}^{m}(a)+\left[\sum _{k=t^{\prime}+1}^{t}\widetilde{\eta}_{k}^{(t)}\gamma p\Delta_{k-1,\max}^{m} \right]+\xi_{t^{\prime},t}^{m}(a).\]

We specifically choose \(t^{\prime}\) and \(t\) to be the consecutive averaging instants to analyze the behaviour of \(\Delta_{t}^{m}\) across two averaging instants. Consequently, we can rewrite the above equation as

\[\Delta_{t}^{m}(a)=\varphi_{t^{\prime},t}\overline{\Delta}_{t^{\prime}}(a)+ \left[\sum_{k=t^{\prime}+1}^{t}\widetilde{\eta}_{k}^{(t)}\gamma p\Delta_{k-1, \max}^{m}\right]+\xi_{t^{\prime},t}^{m}(a). \tag{40}\]

Furthermore, after averaging, we obtain,

\[\overline{\Delta}_{t}(a)=\varphi_{t^{\prime},t}\overline{\Delta}_{t^{\prime}}(a )+\frac{1}{M}\sum_{m=1}^{M}\left[\sum_{k=t^{\prime}+1}^{t}\widetilde{\eta}_{k} ^{(t)}\gamma p\Delta_{k-1,\max}^{m}\right]+\frac{1}{M}\sum_{m=1}^{M}\xi_{t^{ \prime},t}^{m}(a). \tag{41}\]Step 2: deriving a recursive bound for \(\mathbb{E}[\overline{\Delta}_{t,\max}]\).Bounding (40), we obtain,

\[\Delta_{t,\max}^{m} \geq\varphi_{t^{\prime},t}\overline{\Delta}_{t^{\prime},\max}+ \left[\sum_{k=t^{\prime}+1}^{t}\widetilde{\eta}_{k}^{(t)}\gamma p\Delta_{k-1, \max}^{m}\right]+\xi_{t^{\prime},t,\max}^{m}-\varphi_{t^{\prime},t}|\overline{ \Delta}_{t^{\prime}}(1)-\overline{\Delta}_{t^{\prime}}(2)|, \tag{42a}\] \[\Delta_{t,\max}^{m} \leq\varphi_{t^{\prime},t}\overline{\Delta}_{t^{\prime},\max}+ \left[\sum_{k=t^{\prime}+1}^{t}\widetilde{\eta}_{k}^{(t)}\gamma p\Delta_{k-1, \max}^{m}\right]+\xi_{t^{\prime},t,\max}^{m}, \tag{42b}\]

where in the first step we used the fact that

\[\max\{a_{1}+b_{1},a_{2}+b_{2}\}\geq\min\{a_{1},a_{2}\}+\max\{b_{1},b_{2}\}= \max\{a_{1},a_{2}\}+\max\{b_{1},b_{2}\}-|a_{1}-a_{2}|. \tag{43}\]

On taking expectation, we obtain,

\[\mathbb{E}[\Delta_{t,\max}^{m}] \geq\varphi_{t^{\prime},t}\mathbb{E}[\overline{\Delta}_{t^{\prime },\max}]+\left[\sum_{k=t^{\prime}+1}^{t}\widetilde{\eta}_{k}^{(t)}\gamma p \mathbb{E}[\Delta_{k-1,\max}^{m}]\right]+\mathbb{E}[\xi_{t^{\prime},t,\max}^{m} ]-\varphi_{t^{\prime},t}\mathbb{E}[|\overline{\Delta}_{t^{\prime}}(1)-\overline {\Delta}_{t^{\prime}}(2)|], \tag{44a}\] \[\mathbb{E}[\Delta_{t,\max}^{m}] \leq\varphi_{t^{\prime},t}\mathbb{E}[\overline{\Delta}_{t^{\prime },\max}]+\left[\sum_{k=t^{\prime}+1}^{t}\widetilde{\eta}_{k}^{(t)}\gamma p \mathbb{E}[\Delta_{k-1,\max}^{m}]\right]+\mathbb{E}[\xi_{t^{\prime},t,\max}^{m}]. \tag{44b}\]

Similarly, using (41) and (43) we can write,

\[\overline{\Delta}_{t,\max}\geq\varphi_{t^{\prime},t}\overline{ \Delta}_{t^{\prime},\max}+\frac{1}{M}\sum_{m=1}^{M}\left[\sum_{k=t^{\prime}+1}^ {t}\widetilde{\eta}_{k}^{(t)}\gamma p\Delta_{k-1,\max}^{m}\right]-\varphi_{t^ {\prime},t}|\overline{\Delta}_{t^{\prime}}(1)-\overline{\Delta}_{t^{\prime}}(2)|\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+\max\left\{ \frac{1}{M}\sum_{m=1}^{M}\xi_{t^{\prime},t}^{m}(1),\frac{1}{M}\sum_{m=1}^{M} \xi_{t^{\prime},t}^{m}(2)\right\} \tag{45a}\] \[\implies\mathbb{E}[\overline{\Delta}_{t,\max}]\geq\varphi_{t^{ \prime},t}\mathbb{E}[\overline{\Delta}_{t^{\prime},\max}]+\frac{1}{M}\sum_{m=1 }^{M}\left[\sum_{k=t^{\prime}+1}^{t}\widetilde{\eta}_{k}^{(t)}\gamma p\mathbb{ E}[\Delta_{k-1,\max}^{m}]\right]-\varphi_{t^{\prime},t}\mathbb{E}[|\overline{ \Delta}_{t^{\prime}}(1)-\overline{\Delta}_{t^{\prime}}(2)|]\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+\mathbb{E}\left[ \max\left\{\frac{1}{M}\sum_{m=1}^{M}\xi_{t^{\prime},t}^{m}(1),\frac{1}{M}\sum_{ m=1}^{M}\xi_{t^{\prime},t}^{m}(2)\right\}\right]. \tag{45b}\]

On combining (44b) and (45b), we obtain,

\[\mathbb{E}[\overline{\Delta}_{t,\max}] \geq\frac{1}{M}\sum_{m=1}^{M}\left[\mathbb{E}[\Delta_{t,\max}^{m }]-\mathbb{E}[\xi_{t^{\prime},t,\max}^{m}]\right]-\varphi_{t^{\prime},t}\mathbb{ E}[|\overline{\Delta}_{t^{\prime}}(1)-\overline{\Delta}_{t^{\prime}}(2)|]\] \[\qquad\qquad\qquad\qquad\qquad+\mathbb{E}\left[\max\left\{ \frac{1}{M}\sum_{m=1}^{M}\xi_{t^{\prime},t}^{m}(1),\frac{1}{M}\sum_{m=1}^{M} \xi_{t^{\prime},t}^{m}(2)\right\}\right]. \tag{46}\]

In order to simplify (46), we make use of the following lemmas.

**Lemma 3**.: _Let \(t^{\prime}<t\) be two consecutive averaging instants. Then for all \(m\in[M]\),_

\[\mathbb{E}[\Delta_{t,\max}^{m}]-\mathbb{E}[\xi_{t^{\prime},t,\max }^{m}]\geq\left(\prod_{k=t^{\prime}+1}^{t}(1-\eta_{k}(1-\gamma p))\right) \mathbb{E}[\overline{\Delta}_{t^{\prime},\max}]+\mathbb{E}[\xi_{t^{\prime},t,\max}^{m}]\left[\sum_{k=t^{\prime}+1}^{t}\eta_{k}^{(t)}-1\right]_{+}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad-\varphi_{t^{ \prime},t}\mathbb{E}[|\overline{\Delta}_{t^{\prime}}(1)-\overline{\Delta}_{t^{ \prime}}(2)|],\]

_where \([x]_{+}=\max\{x,0\}\)._

**Lemma 4**.: _For all consecutive averaging instants \(t^{\prime},t\) satisfying \(t-\max\{t^{\prime},\tau\}\geq 1/\eta_{\tau}\) and all \(m\in[M]\), we have,_

\[\mathbb{E}[\xi_{t^{\prime},t,\max}^{m}]\geq\frac{1}{240\log\left(\frac{180B}{ \eta_{\tau}(1-\gamma)}\right)}\cdot\frac{\nu}{\nu+1},\]\[\mathbb{E}\left[\max\left\{\frac{1}{M}\sum_{m=1}^{M}\xi_{t^{\prime},t}^{m}(1), \frac{1}{M}\sum_{m=1}^{M}\xi_{t^{\prime},t}^{m}(2)\right\}\right]\geq\frac{1}{2 40\log\left(\frac{180BM}{\eta_{T}(1-\gamma)}\right)}\cdot\frac{\nu}{\nu+\sqrt{ M}},\]

_where \(\nu:=\sqrt{\frac{20\eta_{T}}{B(1-\gamma)}}\)._

**Lemma 5**.: _For all \(t\in\{t_{r}\}_{r=1}^{R}\), we have_

\[\mathbb{E}[|\overline{\Delta}_{t}(1)-\overline{\Delta}_{t}(2)|]\leq\sqrt{\frac {8\eta_{T}}{3BM(1-\gamma)}}.\]

Thus, on combining the results from Lemmas 3, 4, and 5 and plugging them into (46), we obtain the following relation for \(t,t^{\prime}\geq\tau\):

\[\mathbb{E}[\overline{\Delta}_{t,\max}] \geq\left(\prod_{k=t^{\prime}+1}^{t}(1-\eta_{k}(1-\gamma p)) \right)\mathbb{E}[\overline{\Delta}_{t^{\prime},\max}]+\mathbb{E}[\xi_{t^{ \prime},t,\max}^{m}]\left[\sum_{k=t^{\prime}+1}^{t}\eta_{k}^{(t)}-1\right]_{+}\] \[\geq(1-\eta_{\tau}(1-\gamma p))^{t-t^{\prime}}\mathbb{E}[\overline {\Delta}_{t^{\prime},\max}]+\left(\frac{1-(1-\eta_{\tau}(1-\gamma p))^{t-t^{ \prime}}}{5760\log\left(\frac{180B}{\eta_{T}(1-\gamma)}\right)(1-\gamma p)} \right)\cdot\frac{\nu}{\nu+1}\cdot\mathbb{1}\left\{t-t^{\prime}\geq\frac{8}{ \eta_{\tau}}\right\}\] \[\quad-2(1-\eta_{T})^{t-t^{\prime}}\sqrt{\frac{8\eta_{T}}{3BM(1- \gamma)}}+\frac{1}{240\log\left(\frac{180BM}{\eta_{T}(1-\gamma)}\right)}\cdot \frac{\nu}{\nu+\sqrt{M}}\cdot\mathbb{1}\left\{t-t^{\prime}\geq\frac{8}{\eta_{ \tau}}\right\}, \tag{47}\]

where we used the relation \(\varphi_{t^{\prime},t}\leq(1-\eta_{T})^{t-t^{\prime}}\), as well as the value of \(\nu\) as defined in Lemma 4 along with the fact

\[\sum_{k=t^{\prime}+1}^{t}\eta_{k}^{(t)}-1\geq\frac{1-(1-\eta_{\tau}(1-\gamma p ))^{t-t^{\prime}}}{24(1-\gamma p)} \tag{48}\]

for all \(t,t^{\prime}\geq\tau\) such that \(t-t^{\prime}\geq 8/\eta_{\tau}\).

Proof of (48).: We have,

\[\sum_{k=t^{\prime}+1}^{t}\eta_{k}^{(t)}-1 =\sum_{k=t^{\prime}+1}^{t}\left(\eta_{k}\prod_{i=k+1}^{t}(1-\eta_ {i}(1-\gamma p))\right)-1\] \[\geq\sum_{k=t^{\prime}+1}^{t}\left(\eta_{t}\prod_{i=k+1}^{t}(1- \eta_{\tau}(1-\gamma p))\right)-1\] \[\geq\eta_{t}\sum_{k=t^{\prime}+1}^{t}(1-\eta_{\tau}(1-\gamma p))^ {t-k}-1\] \[\geq\eta_{t}\cdot\left(\frac{1-(1-\eta_{\tau}(1-\gamma p))^{t-t^{ \prime}}}{\eta_{\tau}(1-\gamma p)}\right)-1\] \[\geq\frac{1-(1-\eta_{\tau}(1-\gamma p))^{t-t^{\prime}}}{3(1- \gamma p)}-1. \tag{49}\]

To show (48), it is sufficient to show that \(\frac{1-(1-\eta_{\tau}(1-\gamma p))^{t-t^{\prime}}}{3(1-\gamma p)}\geq\frac{8 }{7}\) for \(t-t^{\prime}\geq 8/\eta_{\tau}\). Thus, for \(t-t^{\prime}\geq 8/\eta_{\tau}\) we have,

\[\frac{1-(1-\eta_{\tau}(1-\gamma p))^{t-t^{\prime}}}{3(1-\gamma p)}\geq\frac{1- \exp(-\eta_{\tau}(1-\gamma p)\cdot(t-t^{\prime}))}{3(1-\gamma p)}\]\[\geq\frac{1-\exp(-8(1-\gamma p))}{3(1-\gamma p)}. \tag{50}\]

Since \(\gamma\geq 5/6\), \(1-\gamma p\leq 2/9\). For \(x\leq 2/9\), the function \(f(x)=\frac{1-e^{-8x}}{3x}\geq 8/7\), proving the claim.

Step 3: lower bounding \(\mathbb{E}[\overline{\Delta}_{T,\max}]\).We are now interested in evaluating \(\mathbb{E}[\overline{\Delta}_{T,\max}]\) based on the recursion (47). To this effect, we introduce some notation to simplify the presentation. Let

\[R_{\tau}:=\min\{r:t_{r}\geq\tau\}. \tag{51}\]

For \(r=R_{\tau},\ldots,R\), we define the following terms:

\[x_{r} :=\mathbb{E}[\overline{\Delta}_{t_{r},\max}],\] \[\alpha_{r} :=(1-\eta_{\tau}(1-\gamma p))^{t_{r}-t_{r-1}},\] \[\beta_{r} :=(1-\eta_{T})^{t_{r}-t_{r-1}},\] \[\mathcal{I}_{r} :=\{r\geq r^{\prime}>R_{\tau}:t_{r^{\prime}}-t_{r^{\prime}-1}\geq 8 /\eta_{\tau}\},\] \[C_{1} :=\frac{1}{5760\log\left(\frac{180B}{\eta_{T}(1-\gamma)}\right)(1 -\gamma p)}\cdot\frac{\nu}{\nu+1},\] \[C_{2} :=\sqrt{\frac{32\eta_{T}}{3BM(1-\gamma)}},\] \[C_{3} :=\frac{1}{240\log\left(\frac{180B}{\eta_{T}(1-\gamma)}\right)} \cdot\frac{\nu}{\nu+\sqrt{M}}.\]

With these notations in place, the recursion in (47) can be rewritten as

\[x_{r}\geq\alpha_{r}x_{r-1}-\beta_{r}C_{2}+C_{3}\mathbb{1}\left\{r\in\mathcal{I }_{r}\right\}+(1-\alpha_{r})C_{1}\mathbb{1}\left\{r\in\mathcal{I}_{r}\right\}, \tag{52}\]

for all \(r\geq R_{\tau}\). We claim that \(x_{r}\) satisfies the following relation for all \(r\geq R_{\tau}+1\) (whose proof is deferred to the end of this step):

\[x_{r}\geq\left(\prod_{i=R_{\tau}+1}^{r}\alpha_{i}\right)x_{R_{ \tau}} -\sum_{k=R_{\tau}+1}^{r}\beta_{k}\left(\prod_{i=k+1}^{r}\alpha_{i }\right)C_{2}+\sum_{k=R_{\tau}+1}^{r}\left(\prod_{i=k+1}^{r}\alpha_{i}\right) \mathbb{1}\left\{k\in\mathcal{I}_{k}\right\}C_{3}\] \[+C_{1}\left(\prod_{i\notin\mathcal{I}_{r}}\alpha_{i}\right)\left( 1-\prod_{i\in\mathcal{I}_{r}}\alpha_{i}\right), \tag{53}\]

where we recall that if there is no valid index for a product, its value is taken to be \(1\).

Invoking (53) for \(r=R\) and using the relation \(x_{R_{\tau}-1}\geq 0\), we obtain,

\[x_{R} \geq-\sum_{k=R_{\tau}}^{R}\beta_{k}\left(\prod_{i=k+1}^{R}\alpha _{i}\right)C_{2}+\sum_{k=R_{\tau}}^{R}\left(\prod_{i=k+1}^{R}\alpha_{i}\right) C_{3}\mathbb{1}\left\{k\in\mathcal{I}_{k}\right\}+C_{1}\left(\prod_{i\notin \mathcal{I}_{R}}\alpha_{i}\right)\left(1-\prod_{i\in\mathcal{I}_{R}}\alpha_{i }\right)\] \[\geq-RC_{2}+C_{1}\left(\prod_{i\notin\mathcal{I}_{R}}\alpha_{i} \right)\left(1-\prod_{i\in\mathcal{I}_{R}}\alpha_{i}\right)\] \[\geq-R\cdot\sqrt{\frac{32\eta_{T}}{3BM(1-\gamma)}}+\left(\prod_{i \notin\mathcal{I}_{R}}\alpha_{i}\right)\left(1-\prod_{i\in\mathcal{I}_{R}} \alpha_{i}\right)\cdot\frac{1}{5760\log\left(\frac{180B}{\eta_{T}(1-\gamma)} \right)(1-\gamma p)}\cdot\frac{\nu}{\nu+1}, \tag{54}\]

where we used the fact \(\beta_{k}\left(\prod_{i=k+1}^{R}\alpha_{i}\right)\leq 1\) and that \(C_{3}\geq 0\). Consider the expression

\[\prod_{i\notin\mathcal{I}_{R}}\alpha_{i}=\prod_{i\notin\mathcal{I}_{R}}(1-\eta _{\tau}(1-\gamma p))^{t_{i}-t_{i-1}}\geq 1-\eta_{\tau}(1-\gamma p)\cdot\underbrace{ \sum_{i\notin\mathcal{I}_{R}}(t_{i}-t_{i-1})}_{=:\mathcal{T}_{1}}. \tag{55}\]Consequently,

\[\left(1-\prod_{i\in\mathcal{I}_{R}}\alpha_{i}\right)=1-\left(1-\eta_{\tau}(1-\gamma p )\right)^{T-\tau-T_{1}}\geq 1-\exp\left(-\eta_{\tau}(1-\gamma p)\left(T-\tau-T_{1} \right)\right). \tag{56}\]

Note that \(T_{1}\) satisfies the following bound

\[T_{1}:=\sum_{i\notin\mathcal{I}_{R}}(t_{i}-t_{i-1})\leq(R-|\mathcal{I}_{R}|) \cdot\frac{8}{\eta_{\tau}}\leq\frac{8R}{\eta_{\tau}}. \tag{57}\]

We split the remainder of the analysis based on the step size schedule.

* For the constant step size schedule, i.e., \(\eta_{t}=\eta\geq\frac{1}{(1-\gamma)T}\), we have, \(R_{\tau}=0\), with \(\tau=0\) and \(t_{0}=0\) (as all agents start at the same point). If \(R\leq\frac{1}{96000(1-\gamma)\log\left(\frac{180B}{\eta(1-\gamma)}\right)}\), then, (55), (56) and (57) yield the following relations: \[T_{1} \leq\frac{8R}{\eta}\leq\frac{T}{12000\log(180N)},\] \[\prod_{i\notin\mathcal{I}_{R}}\alpha_{i} \geq 1-\eta(1-\gamma p)\cdot T_{1}\geq 1-\frac{32R(1-\gamma)}{3} \geq 1-\frac{1}{9000\log(180N)},\] \[\left(1-\prod_{i\in\mathcal{I}_{R}}\alpha_{i}\right) \geq 1-\exp\left(-\eta(1-\gamma p)\left(T-T_{1}\right)\right)\geq 1- \exp\left(-\frac{4}{3}\left(1-\frac{1}{9000\log(180N)}\right)\right).\] On plugging the above relations into (54), we obtain \[x_{R}\geq\frac{\sqrt{40}}{96000\log\left(\frac{180B}{\eta(1-\gamma)}\right)(1- \gamma)}\cdot\left(\frac{\nu}{\nu+1}-\frac{\nu}{5\sqrt{M}}\right)\] (58) where recall that \(\nu:=\sqrt{\frac{20\eta}{3B(1-\gamma)}}\). Consider the function \(f(x)=\frac{x}{x+1}-\frac{x}{5\sqrt{M}}\). We claim that for \(x\in[0,\sqrt{M}]\) and all \(M\geq 2\), \[f(x)\geq\frac{7}{20}\min\{x,1\}.\] (59) The proof of the above claim is deferred to the end of the section. In light of the above claim, we have, \[x_{R} \geq\frac{\sqrt{40}}{96000\log\left(\frac{180B}{\eta(1-\gamma)} \right)(1-\gamma)}\cdot\frac{7}{20}\cdot\min\left\{1,\sqrt{\frac{20\eta}{3B(1 -\gamma)}}\right\}\] \[\geq\frac{\sqrt{40}}{96000\log\left(180N\right)}\cdot\frac{7}{20} \cdot\min\left\{\frac{1}{1-\gamma},\sqrt{\frac{20}{3(1-\gamma)^{4}N}}\right\},\] (60) where we used the fact that \(M\geq 2\), \(\frac{\sqrt{x}}{\log(1/x)}\) is an increasing function and the relation \(\frac{\nu}{M}=\frac{20\eta}{3BM(1-\gamma)}\leq\frac{1}{15}\leq 1\).
* Next, we consider the rescaled linear step size schedule, where \(\tau=T/3\) (cf. (34)). To begin, we assume \(t_{R_{\tau}}\leq\max\{\frac{3T}{4},T-\frac{1}{6\eta_{\tau}(1-\gamma p)}\}\). It is straightforward to note that \[\max\left\{\frac{3T}{4},T-\frac{1}{6\eta_{\tau}(1-\gamma p)}\right\}=\begin{cases} \frac{3T}{4}&\text{if }c_{\eta}\geq 3\\ T-\frac{1}{6\eta_{\tau}(1-\gamma p)}&\text{if }c_{\eta}<3.\end{cases}\] If \(R\leq\frac{1}{384000(1-\gamma)\log\left(\frac{180B}{\eta_{\tau}(1-\gamma)} \right)\cdot(5+c_{\eta})}\) then, (55), (56) and (57) yield the following relations: \[T_{1}\leq\frac{8R}{\eta_{\tau}},\qquad\prod_{i\notin\mathcal{I}_{R}}\alpha_{i} \geq 1-\eta_{\tau}(1-\gamma p)\cdot T_{1}\geq 1-\frac{32R(1-\gamma)}{3}\geq 1- \frac{1}{36000}.\]For \(c_{\eta}\geq 3\), we have, \[\left(1-\prod_{i\in\mathcal{I}_{R}}\alpha_{i}\right) \geq 1-\exp\left(-\eta_{r}(1-\gamma p)\left(T-t_{R_{\tau}}-T_{1} \right)\right)\] \[\geq 1-\exp\left(-\frac{(1-\gamma)T}{(3+c_{\eta}(1-\gamma)T)}+ \frac{32R(1-\gamma)}{3}\right)\] \[\geq\frac{1}{2(3+c_{\eta})},\] where we used \(T\geq\frac{1}{1-\gamma}\) in the second step. Similarly, for \(c_{\eta}<3\), we have, \[\left(1-\prod_{i\in\mathcal{I}_{R}}\alpha_{i}\right) \geq 1-\exp\left(-\eta_{r}(1-\gamma p)\left(T-t_{R_{\tau}}-T_{1} \right)\right)\] \[\geq 1-\exp\left(-\frac{1}{6}+\frac{32R(1-\gamma)}{3}\right)\] \[\geq\frac{1}{10}.\] On plugging the above relations into (54), we obtain \[x_{R} \geq\frac{18\sqrt{1.6}}{384000\log\left(\frac{180B}{\eta_{T}(1- \gamma)}\right)(1-\gamma)(5+c_{\eta})}\cdot\left(\frac{\nu}{\nu+1}-\frac{\nu} {18\sqrt{M}}\right)\] \[\geq\frac{18\sqrt{1.6}}{384000\log\left(\frac{180B}{\eta_{T}(1- \gamma)}\right)(1-\gamma)(5+c_{\eta})}\cdot\frac{7}{20}\cdot\min\left\{1,\sqrt{ \frac{20\eta_{T}}{3B(1-\gamma)}}\right\}\] \[\geq\frac{18\sqrt{1.6}}{384000\log\left(\frac{180B}{\eta_{T}(1- \gamma)}\right)(5+c_{\eta})}\cdot\frac{7}{20}\cdot\min\left\{\frac{1}{1-\gamma },\sqrt{\frac{20}{3B(1-\gamma)^{3}}}\right\}\] \[\geq\frac{18\sqrt{1.6}}{384000\log\left(180N(1+\log N)\right)(5+ \log N)}\cdot\frac{7}{20}\cdot\min\left\{\frac{1}{1-\gamma},\sqrt{\frac{20}{3B (1+\log N)(1-\gamma)^{4}N}}\right\},\] (61) where we again used the facts that \(M\geq 2\), \(c_{\eta}\leq\log N\), \(\frac{\sqrt{x}}{\log(1/x)}\) is an increasing function and the relation \(\frac{\nu}{M}=\frac{20\eta_{T}}{3BM(1-\gamma)}\leq 1\).
* Last but not least, let us consider the rescaled linear step size schedule case when \(t_{R_{\tau}}>\max\{\frac{3T}{4},T-\frac{1}{6\eta_{\tau}(1-\gamma p)}\}\). The condition implies that the time between the communication rounds \(R_{\tau}-1\) and \(R_{\tau}\) is at least \(T_{0}:=\max\{\frac{5T}{12},\frac{2T}{3}-\frac{1}{6\eta_{\tau}(1-\gamma p)}\}\). Thus, (47) yields that \[\mathbb{E}[\overline{\Delta}_{t_{R_{\tau}}}]\geq\left(\frac{1-(1-\eta_{\tau}(1 -\gamma p))^{T_{0}}}{5760\log\left(\frac{180}{B\eta_{T}(1-\gamma)}\right)(1- \gamma p)}\right)\cdot\frac{\nu}{\nu+1}-2(1-\eta_{T})^{T_{0}}\sqrt{\frac{8\eta _{T}}{3BM(1-\gamma)}}.\] (62) Using the above relation along with (53), we can conclude that \[x_{R} \geq(1-\eta_{\tau}(1-\gamma p))^{T-t_{R_{\tau}}}\left(\frac{1-(1- \eta_{\tau}(1-\gamma p))^{T_{0}}}{5760\log\left(\frac{180}{B\eta_{T}(1-\gamma )}\right)(1-\gamma p)}\right)\cdot\frac{\nu}{\nu+1}\] \[\qquad-2(1-\eta_{T})^{T_{0}}\cdot(1-\eta_{\tau}(1-\gamma p))^{T-t_ {R_{\tau}}}\sqrt{\frac{8\eta_{T}}{3BM(1-\gamma)}}-RC_{2}.\] (63)

[MISSING_PAGE_FAIL:26]

Proof of (53).We now return to establish (53) using induction. For the base case, (52) yields

\[x_{R_{\tau}+1}\geq\alpha_{R_{\tau}+1}x_{R_{\tau}}-\beta_{R_{\tau}+1}C_{2}+C_{3} \mathbbm{1}\left\{R_{\tau}+1\in\mathcal{I}_{R_{\tau}+1}\right\}+(1-\alpha_{R_{ \tau}+1})C_{1}\mathbbm{1}\left\{R_{\tau}+1\in\mathcal{I}_{R_{\tau}+1}\right\}. \tag{68}\]

Note that this is identical to the expression in (53) for \(r=R_{\tau}+1\) as

\[\left(\prod_{i\notin\mathcal{I}_{R_{\tau}+1}}\alpha_{i}\right)\left(1-\prod_{i \in\mathcal{I}_{R_{\tau}+1}}\alpha_{i}\right)=(1-\alpha_{R_{\tau}+1})\mathbbm{1 }\left\{R_{\tau}+1\in\mathcal{I}_{R_{\tau}+1}\right\}\]

based on the adopted convention for products with no valid indices. For the induction step, assume (53) holds for some \(r\geq R_{\tau}+1\). On combining (52) and (53), we obtain,

\[x_{r+1} \geq\alpha_{r+1}x_{r}-\beta_{r+1}C_{2}+C_{3}\mathbbm{1}\left\{(r+ 1)\in\mathcal{I}_{r+1}\right\}+(1-\alpha_{r+1})C_{1}\mathbbm{1}\left\{r+1\in \mathcal{I}_{r+1}\right\}\] \[\geq\alpha_{r+1}\left(\prod_{i=R_{\tau}+1}^{r}\alpha_{i}\right)x_ {R_{\tau}}-\alpha_{r+1}\sum_{k=R_{\tau}+1}^{r}\beta_{k}\left(\prod_{i=k+1}^{r} \alpha_{i}\right)C_{2}+\alpha_{r+1}\sum_{k=R_{\tau}+1}^{r}\left(\prod_{i=k+1}^{ r}\alpha_{i}\right)C_{3}\mathbbm{1}\left\{k\in\mathcal{I}_{k}\right\}\] \[+\alpha_{r+1}C_{1}\left(\prod_{i\notin\mathcal{I}_{\tau}}\alpha_{i }\right)\left(1-\prod_{i\in\mathcal{I}_{\tau}}\alpha_{i}\right)-\beta_{r+1}C_{2 }+C_{3}\mathbbm{1}\left\{(r+1)\in\mathcal{I}_{r+1}\right\}+(1-\alpha_{r+1})C_ {1}\mathbbm{1}\left\{(r+1)\in\mathcal{I}_{r+1}\right\}\] \[\geq\left(\prod_{i=R_{\tau}+1}^{r+1}\alpha_{i}\right)x_{R_{\tau} }-\sum_{k=R_{\tau}+1}^{r+1}\beta_{k}\left(\prod_{i=k+1}^{r+1}\alpha_{i}\right) C_{2}+\sum_{k=R_{\tau}+1}^{r+1}\left(\prod_{i=k+1}^{r+1}\alpha_{i}\right)C_{3} \mathbbm{1}\left\{k\in\mathcal{I}_{k}\right\}\] \[\qquad+\alpha_{r+1}C_{1}\left(\prod_{i\notin\mathcal{I}_{\tau}} \alpha_{i}\right)\left(1-\prod_{i\in\mathcal{I}_{\tau}}\alpha_{i}\right)+(1- \alpha_{r+1})C_{1}\mathbbm{1}\left\{(r+1)\in\mathcal{I}_{r+1}\right\}. \tag{69}\]

If \((r+1)\notin\mathcal{I}_{r+1}\), then \(\left(1-\prod_{i\in\mathcal{I}_{\tau}}\alpha_{i}\right)=\left(1-\prod_{i\in \mathcal{I}_{r+1}}\alpha_{i}\right)\) and \(\alpha_{r+1}\left(\prod_{i\notin\mathcal{I}_{r}}\alpha_{i}\right)=\left(\prod_ {i\notin\mathcal{I}_{r+1}}\alpha_{i}\right)\). Consequently,

\[\alpha_{r+1}C_{1}\left(\prod_{i\notin\mathcal{I}_{\tau}}\alpha_{i}\right)\left( 1-\prod_{i\in\mathcal{I}_{\tau}}\alpha_{i}\right)+(1-\alpha_{r+1})C_{1}\mathbbm{1 }\left\{(r+1)\in\mathcal{I}_{r+1}\right\}=C_{1}\left(\prod_{i\notin\mathcal{I} _{r+1}}\alpha_{i}\right)\left(1-\prod_{i\in\mathcal{I}_{r+1}}\alpha_{i}\right). \tag{70}\]

On the other hand, if \((r+1)\in\mathcal{I}_{r+1}\), then \(\left(\prod_{i\notin\mathcal{I}_{\tau}}\alpha_{i}\right)=\left(\prod_{i\notin \mathcal{I}_{\tau+1}}\alpha_{i}\right)\). Consequently, we have,

\[\alpha_{r+1}C_{1}\left(\prod_{i\notin\mathcal{I}_{\tau}}\alpha_{i }\right)\left(1-\prod_{i\in\mathcal{I}_{\tau}}\alpha_{i}\right)+(1-\alpha_{r+1} )C_{1}\mathbbm{1}\left\{(r+1)\in\mathcal{I}_{r+1}\right\}\] \[=\alpha_{r+1}C_{1}\left(\prod_{i\notin\mathcal{I}_{\tau+1}}\alpha_{i }\right)\left(1-\prod_{i\in\mathcal{I}_{\tau}}\alpha_{i}\right)+(1-\alpha_{r+1} )C_{1}\] \[\geq C_{1}\left(\prod_{i\notin\mathcal{I}_{\tau+1}}\alpha_{i}\right) \left[\alpha_{r+1}\left(1-\prod_{i\in\mathcal{I}_{\tau}}\alpha_{i}\right)+(1- \alpha_{r+1})\right]\] \[\geq C_{1}\left(\prod_{i\notin\mathcal{I}_{\tau+1}}\alpha_{i} \right)\left(1-\prod_{i\in\mathcal{I}_{\tau+1}}\alpha_{i}\right). \tag{71}\]

Combining (69), (70) and (71) proves the claim.

Proof of (59).To establish this result, we separately consider the cases \(x\leq 1\) and \(x\geq 1\).

* When \(x\leq 1\), we have \[f(x)=\frac{x}{x+1}-\frac{1}{5\sqrt{M}}\geq x\cdot\left(\frac{1}{2}-\frac{x}{5 \sqrt{M}}\right)\geq\frac{7x}{20},\] (72) where in the last step, we used the relation \(M\geq 2\).

* Let us now consider the case \(x\geq 1\). The second derivative of \(f\) is given by \(f^{\prime\prime}(x)=-\frac{1}{2(x+1)^{3}}\). Clearly, for all \(x\geq 1\), \(f^{\prime\prime}<0\) implying that \(f\) is a concave function. It is well-known that a continuous, bounded, concave function achieves its minimum values over a compact interval at the end points of the interval (Bauer's minimum principle). For all \(M\geq 2\), we have, \[f(1)=\frac{1}{2}-\frac{1}{5\sqrt{M}}\geq\frac{7}{20};\quad f(\sqrt{M})=\frac{ \sqrt{M}}{\sqrt{M}+1}-\frac{1}{5}\geq\frac{7}{20}.\] Consequently, we can conclude that for all \(x\in[1,\sqrt{M}]\), \[f(x)\geq\frac{7}{20}.\] (73) Combining (72) and (73) proves the claim.

#### b.3.3 Large learning rates with large \(\frac{\eta_{T}}{BM}\)

In order to bound the error in this scenario, note that \(\frac{\eta_{T}}{BM}\) controls the variance of the stochastic updates in the fixed point iteration. Thus, when \(\frac{\eta_{T}}{BM}\) is large, the variance of the iterates is large, resulting in a large error. To demonstrate this effect, we focus on the dynamics of state \(2\). This part of the proof is similar to the large learning rate case of Li et al. (2023). For all \(t\in[T]\), define:

\[\overline{V}_{t}(2):=\frac{1}{M}\sum_{m=1}^{M}V_{t}^{m}(2). \tag{74}\]

Thus, from (33), we know that \(\mathbb{E}[\overline{V}_{t}(2)]\) obeys the following recursion:

\[\mathbb{E}[\overline{V}_{t}(2)]=(1-\eta_{t}(1-\gamma p))\mathbb{E}[\overline{V }_{t-1}(2)]+\eta_{t}.\]

Upon unrolling the recursion, we obtain,

\[\mathbb{E}[\overline{V}_{T}(2)]=\left(\prod_{k=t+1}^{T}(1-\eta_{k}(1-\gamma p)) \right)\mathbb{E}[\overline{V}_{t}(2)]+\sum_{k=t+1}^{T}\eta_{k}^{(T)}.\]

Thus, the above relation along with (16) and the value of \(V^{*}(2)\) yields us,

\[V^{*}(2)-\mathbb{E}[\overline{V}_{T}(2)]=\prod_{k=t+1}^{T}(1-\eta_{k}(1-\gamma p ))\left(\frac{1}{1-\gamma p}-\mathbb{E}[\overline{V}_{t}(2)]\right). \tag{75}\]

Similar to Li et al. (2023), we define

\[\tau^{\prime}:=\min\left\{0\leq t^{\prime}\leq T-2\;\middle|\;\mathbb{E}[( \overline{V}_{t})^{2}]\geq\frac{1}{4(1-\gamma)^{2}}\;\text{for all}\;t^{\prime} +1\leq t\leq T\right\}.\]

If such a \(\tau^{\prime}\) does not exist, it implies that either \(\mathbb{E}[(\overline{V}_{T})^{2}]<\frac{1}{4(1-\gamma)^{2}}\) or \(\mathbb{E}[(\overline{V}_{T-1})^{2}]<\frac{1}{4(1-\gamma)^{2}}\). If the former is true, then,

\[V^{*}(2)-\mathbb{E}[\overline{V}_{T}(2)]=\frac{3}{4(1-\gamma)}-\sqrt{\mathbb{ E}[(\overline{V}_{T})^{2}]}>\frac{1}{4(1-\gamma)}. \tag{76}\]

Similarly, if \(\mathbb{E}[(\overline{V}_{T-1})^{2}]<\frac{1}{4(1-\gamma)^{2}}\), it implies \(\mathbb{E}[\overline{V}_{T-1}]<\frac{1}{2(1-\gamma)}\). Using (33), we have,

\[\mathbb{E}[\overline{V}_{T}(2)]=(1-\eta_{T}(1-\gamma p))\mathbb{E}[\overline{ V}_{T-1}(2)]+\eta_{T}\leq\mathbb{E}[\overline{V}_{T-1}(2)]+1<\frac{1}{2(1- \gamma)}+\frac{1}{6(1-\gamma)}=\frac{2}{3(1-\gamma)}.\]

Consequently,

\[V^{*}(2)-\mathbb{E}[\overline{V}_{T}(2)]>\frac{3}{4(1-\gamma)}-\frac{2}{3(1- \gamma)}>\frac{1}{12(1-\gamma)}. \tag{77}\]

For the case when \(\tau^{\prime}\) exists, we divide the proof into two cases.

* We first consider the case when the learning rates satisfy: \[\prod_{k=\tau^{\prime}+1}^{T}(1-\eta_{k}(1-\gamma p))\geq\frac{1}{2}.\] (78) The analysis for this case is identical to that considered in Li et al. (2023). We explicitly write the steps for completeness. Specifically, \[V^{*}(2)-\mathbb{E}[\overline{V}_{T}(2)] =\left(\prod_{k=\tau^{\prime}+1}^{T}(1-\eta_{k}(1-\gamma p))\right) \left(\frac{1}{1-\gamma p}-\mathbb{E}[\overline{V}_{\tau^{\prime}}(2)]\right)\] \[\geq\frac{1}{2}\cdot\left(\frac{3}{4(1-\gamma)}-\sqrt{\mathbb{E}[ (\overline{V}_{\tau^{\prime}}(2))^{2}]}\right)\] \[\geq\frac{1}{2}\cdot\left(\frac{3}{4(1-\gamma)}-\frac{1}{2(1- \gamma)}\right)\geq\frac{1}{8(1-\gamma)},\] (79) where the first line follows from (75), the second line from the condition on step sizes and the third line from the definition of \(\tau^{\prime}\).
* We now consider the other case where, \[0\leq\prod_{k=\tau^{\prime}+1}^{T}(1-\eta_{k}(1-\gamma p))<\frac{1}{2}.\] (80) Using (Li et al., 2023, Eqn.(134)), for any \(t^{\prime}<t\) and all agents \(m\), we have the relation \[V_{t}^{m}(2)=\frac{1}{1-\gamma p}-\prod_{k=t^{\prime}+1}^{t}(1- \eta_{k}(1-\gamma p))\left(\frac{1}{1-\gamma p}-V_{t^{\prime}}^{m}(2)\right)\] \[\qquad\qquad\qquad\qquad\qquad+\sum_{k=t^{\prime}+1}\eta_{k}^{(t) }\gamma(\hat{P}_{k}^{m}(2|2)-p)V_{k-1}^{m}(2).\] The above equation is directly obtained by unrolling the recursion in (24) along with noting that \(Q_{t}(2,1)=V_{t}(2)\) for all \(t\). Consequently, we have, \[\overline{V}_{T}(2) =\frac{1}{1-\gamma p}-\prod_{k=t^{\prime}+1}^{T}(1-\eta_{k}(1- \gamma p))\left(\frac{1}{1-\gamma p}-\overline{V}_{t^{\prime}}(2)\right)\] \[\qquad\qquad\qquad+\frac{1}{M}\sum_{m=1}^{M}\sum_{k=t^{\prime}+1} ^{T}\eta_{k}^{(T)}\gamma(\hat{P}_{k}^{m}(2|2)-p)V_{k-1}^{m}(2).\] (81) Let \(\{\mathscr{F}_{t}\}_{t=0}^{T}\) be a filtration such that \(\mathscr{F}_{t}\) is the \(\sigma\)-algebra corresponding to \(\{(\hat{P}_{s}^{m}(2|2))_{m=1}^{M}\}_{s=1}^{t}\). It is straightforward to note that \(\left\{\frac{1}{M}\sum_{m=1}^{M}\eta_{k}^{(T)}\gamma(\hat{P}_{k}^{m}(2|2)-p)V_ {k-1}^{m}(2)\right\}_{k}\) is a martingale sequence adapted to the filtration \(\mathscr{F}_{k}\). Thus, using the result from (Li et al., 2023, Eqn.(139)), we can conclude that \[\text{Var}(\overline{V}_{T}(2))\geq\mathbb{E}\left[\sum_{k=\tau^{\prime}+2}^{ T}\text{Var}\left(\frac{1}{M}\sum_{m=1}^{M}\eta_{k}^{(T)}\gamma(\hat{P}_{k}^{m}(2|2 )-p)V_{k-1}^{m}(2)\left|\mathscr{F}_{k-1}\right.\right)\right].\] (82) We have, \[\text{Var}\left(\frac{1}{M}\sum_{m=1}^{M}\eta_{k}^{(T)}\gamma( \hat{P}_{k}^{m}(2|2)-p)V_{k-1}^{m}(2)\left|\mathscr{F}_{k-1}\right.\right)\] \[=\frac{1}{M^{2}}\sum_{m=1}^{M}\text{Var}\left(\eta_{k}^{(T)}\gamma (\hat{P}_{k}^{m}(2|2)-p)V_{k-1}^{m}(2)\left|\mathscr{F}_{k-1}\right.\right)\] \[=\frac{(\eta_{k}^{(T)})^{2}}{BM}\gamma^{2}p(1-p)\left(\frac{1}{M} \sum_{m=1}^{M}(V_{k-1}^{m}(2))^{2}\right)\]\[\geq\frac{(1-\gamma)(4\gamma-1)}{9BM}\cdot(\eta_{k}^{(T)})^{2}\cdot( \overline{V}_{k-1}(2))^{2}, \tag{83}\]

where the first line follows from that fact that variance of sum of i.i.d. random variables is the sum of their variances, the second line from variance of Binomial random variable and the third line from Jensen's inequality. Thus, (82) and (83) together yield,

\[\text{Var}(\overline{V}_{T}(2)) \geq\frac{(1-\gamma)(4\gamma-1)}{9BM}\cdot\sum_{k=\tau^{\prime}+2 }^{T}(\eta_{k}^{(T)})^{2}\cdot\mathbb{E}[(\overline{V}_{k-1}(2))^{2}]\] \[\geq\frac{(1-\gamma)(4\gamma-1)}{9BM}\cdot\frac{1}{4(1-\gamma)^{2 }}\cdot\sum_{k=\max\{\tau,\tau^{\prime}\}+2}^{T}(\eta_{k}^{(T)})^{2}, \tag{84}\]

where the second line follows from the definition of \(\tau^{\prime}\). We focus on bounding the third term in the above relation. We have,

\[\sum_{k=\max\{\tau^{\prime},\tau\}+2}^{T}\left(\eta_{k}^{(T)} \right)^{2} \geq\sum_{k=\max\{\tau^{\prime},\tau\}+2}^{T}\left(\eta_{k}\prod _{i=k+1}^{T}(1-\eta_{i}(1-\gamma p)\right)^{2}\] \[\geq\sum_{k=\max\{\tau^{\prime},\tau\}+2}^{T}\left(\eta_{T}\prod _{i=k+1}^{t}(1-\eta_{\tau}(1-\gamma p))\right)^{2}\] \[=\eta_{T}^{2}\sum_{k=\max\{\tau^{\prime},\tau\}+2}^{T}(1-\eta_{ \tau}(1-\gamma p))^{2(t-k)}\] \[\geq\eta_{T}^{2}\cdot\frac{1-(1-\eta_{\tau}(1-\gamma p))^{2(T- \max\{\tau^{\prime},\tau\}-1)}}{\eta_{\tau}(1-\gamma p)(2-\eta_{\tau}(1-\gamma p ))}\] \[\geq\eta_{T}\cdot\frac{1}{4(1-\gamma)}\cdot c^{\prime}, \tag{85}\]

where the second line follows from monotonicity of \(\eta_{t}\) and the numerical constant \(c^{\prime}\) in the fifth step is given by the following claim whose proof is deferred to the end of the section:

\[1-(1-\eta_{\tau}(1-\gamma p))^{2(T-\max\{\tau^{\prime},\tau\}-1)} \geq\begin{cases}1-e^{-8/9}&\text{for constant step sizes},\\ 1-\exp\left(-\frac{8}{3\max\{1,c_{\eta}\}}\right)&\text{for linearly rescaled step sizes}\end{cases}. \tag{86}\]

Thus, (84) and (85) together imply

\[\text{Var}(\overline{V}_{T}(2)) \geq\frac{(4\gamma-1)}{36BM(1-\gamma)}\cdot\sum_{k=\tau^{\prime}+ 2}^{T}(\eta_{k}^{(T)})^{2}\] \[\geq\frac{c^{\prime}(4\gamma-1)}{144(1-\gamma)}\cdot\frac{\eta_{T }}{BM(1-\gamma)}\geq\frac{c^{\prime}(4\gamma-1)}{144(1-\gamma)}\cdot\frac{1}{10 0}, \tag{87}\]

where the last inequality follows from the bound on \(\frac{\eta_{T}}{BM}\).

Thus, for all \(N\geq 1\), we have,

\[\mathbb{E}[(V^{\star}(2)-\overline{V}_{T}(2))^{2}]=\mathbb{E}[(V^{\star}(2)- \mathbb{E}[\overline{V}_{T}(2)])^{2}]+\text{Var}(\overline{V}_{T}(2))\geq\frac {c^{\prime\prime}}{(1-\gamma)N},\]

for some numerical constant \(c^{\prime\prime}\). Similar to the small learning rate case, the error rate is bounded away from a constant value irrespective of the number of agents and the number of communication rounds. Thus, even with \(\textsf{CC}_{\textsf{round}}=\Omega(T)\), we will not observe any collaborative gain in this scenario.

Proof of (86).To establish the claim, we consider two cases:* \(\tau^{\prime}\geq\tau\): Under this case, we have, \[(1-\eta_{\tau}(1-\gamma p))^{2(T-\max\{\tau^{\prime},\tau\}-1)} =(1-\eta_{\tau}(1-\gamma p))^{2(T-\tau^{\prime}-1)}\] \[\leq(1-\eta_{\tau}(1-\gamma p))^{T-\tau^{\prime}}\leq\prod_{k= \tau^{\prime}+1}^{T}(1-\eta_{k}(1-\gamma p))\leq\frac{1}{2},\] where the last inequality follows from (80).
* \(\tau\geq\tau^{\prime}\): For this case, we have \[(1-\eta_{\tau}(1-\gamma p))^{2(T-\max\{\tau^{\prime},\tau\}-1)} =(1-\eta_{\tau}(1-\gamma p))^{2(T-\tau-1)}\] \[\leq(1-\eta_{\tau}(1-\gamma p))^{T-\tau}\leq\exp\left(-\frac{2T \eta_{\tau}(1-\gamma p)}{3}\right).\] For the constant stepsize schedule, we have, \[\exp\left(-\frac{2T\eta_{\tau}(1-\gamma p)}{3}\right)\leq\exp\left(-\frac{2T }{3}\cdot\frac{1}{(1-\gamma)T}\cdot\frac{4(1-\gamma)}{3}\right)=\exp\left(- \frac{8}{9}\right)\] (90) For linearly rescaled stepsize schedule, we have, \[\exp\left(-\frac{2T\eta_{\tau}(1-\gamma p)}{3}\right)\leq\exp\left(-\frac{2T }{3}\cdot\frac{1}{1+c_{\eta}(1-\gamma)T/3}\cdot\frac{4(1-\gamma)}{3}\right)= \exp\left(-\frac{8}{3\max\{1,c_{\eta}\}}\right)\] (91) On combining (88), (89), (90) and (91), we arrive at the claim.

### Generalizing to larger state action spaces

We now elaborate on how we can extend the result to general state-action spaces along with the obtaining the lower bound on the bit level communication complexity. For the general case, we instead consider the following MDP. For the first four states \(\{0,1,2,3\}\), the probability transition kernel and reward function are given as follows.

\[\mathcal{A}_{0}=\{1\} \qquad P(0|0,1)=1 r(0,1)=0, \tag{92a}\] \[\mathcal{A}_{1}=\{1,2,\ldots,|\mathcal{A}|\} P(1|1,a)=p P(0|1,a)=1-p r(1,a)=1,\forall\;a\in\mathcal{A}\] (92b) \[\mathcal{A}_{2}=\{1\} \qquad P(2|2,1)=p P(0|2,1)=1-p r(2,1)=1,\] (92c) \[\mathcal{A}_{3}=\{1\} \qquad P(3|3,1)=1 r(3,1)=1, \tag{92d}\]

where the parameter \(p=\frac{4\gamma-1}{3\gamma}\). The overall MDP is obtained by creating \(|\mathcal{S}|/4\) copies of the above MDP for all sets of the form \(\{4r,4r+1,4r+2,4r+3\}\) for \(r\leq|\mathcal{S}|/4-1\). It is straightforward to note that the lower bound on the number of communication rounds immediately transfers to the general case as well. Moreover, note that the bound on \(\mathsf{CC}_{\mathsf{found}}\) implies the bound \(\mathsf{CC}_{\mathsf{bit}}=\Omega\left(\frac{1}{(1-\gamma)\log^{2}N}\right)\) as every communication entails sending \(\Omega(1)\) bits.

To obtain the general lower bound on bit level communication complexity, note that we can carry out the analysis in the previous section for all \(|\mathcal{A}|/2\) pairs of actions in state \(1\) corresponding to the set of states \(\{0,1,2,3\}\). Moreover, the algorithm \(\mathscr{A}\), needs to ensure that the error is low across all the \(|\mathcal{A}|/2\) pairs. Since the errors are independent across all these pairs, each of them require \(\Omega\left(\frac{1}{(1-\gamma)\log^{2}N}\right)\) bits of information to be transmitted during the learning horizon leading to a lower bound of \(\Omega\left(\frac{|\mathcal{A}|}{(1-\gamma)\log^{2}N}\right)\). Note that since we require a low \(\ell_{\infty}\) error, \(\mathscr{A}\) needs to ensure that the error is low across all the pairs, resulting in a communication cost linearly growing with \(|\mathcal{A}|\). Upon repeating the argument across all \(|\mathcal{S}|/4\) copies of the MDP, we arrive at the lower bound of \(\mathsf{CC}_{\mathsf{bit}}=\Omega\left(\frac{|\mathcal{S}||\mathcal{A}|}{(1 -\gamma)\log^{2}N}\right)\).

### Proofs of auxiliary lemmas

#### b.5.1 Proof of Lemma 2

Note that a similar relationship is also derived in Li et al. (2023), but needing to take care of the averaging over multiple agents, we present the entire arguments for completeness. We prove the claim using an induction over \(t\). It is straightforward to note that the claim is true for \(t=0\) and all agents \(m\in\{1,2,\ldots,M\}\). For the inductive step, we assume that the claim holds for \(t-1\) for all clients. Using the induction hypothesis, we have the following relation between \(V_{t-1}^{m}(1)\) and \(\widehat{V}_{t-1}^{m}\):

\[V_{t-1}^{m}(1)=\max_{a\in\{1,2\}}Q_{t-1}^{m}(1,a)\geq\max_{a\in\{1,2\}} \widehat{Q}_{t-1}^{m}(a)-\frac{1}{1-\gamma}\prod_{i=1}^{t-1}(1-\eta_{i}(1- \gamma))=\widehat{V}_{t-1}^{m}-\frac{1}{1-\gamma}\prod_{i=1}^{t-1}(1-\eta_{i} (1-\gamma)). \tag{93}\]

For \(t\notin\{t_{r}\}_{r=1}^{R}\) and \(a\in\{1,2\}\), we have,

\[Q_{t}^{m}(1,a)-\widehat{Q}_{t}^{m}(a) =Q_{t-1/2}^{m}(1,a)-\widehat{Q}_{t-1/2}^{m}(a)\] \[=(1-\eta_{t})Q_{t-1}^{m}(1,a)+\eta_{t}(1+\gamma\widehat{P}_{t}^{ m}(1|1,a)V_{t-1}^{m}(1))\] \[\qquad\qquad-\left[(1-\eta_{t})\widehat{Q}_{t-1}^{m}(a)+\eta_{t}( 1+\gamma\widehat{P}_{t}^{m}(1|1,a)\widehat{V}_{t-1}^{m})\right]\] \[=(1-\eta_{t})(Q_{t-1}^{m}(1|1,a)-\widehat{Q}_{t-1}^{m}(a))+\eta_{ t}\gamma\widehat{P}_{t}^{m}(1|1,a)(V_{t-1}^{m}(1)-\widehat{V}_{t-1}^{m})\] \[\geq-\frac{(1-\eta_{t})}{1-\gamma}\prod_{i=1}^{t-1}(1-\eta_{i}(1- \gamma))-\widehat{P}_{t}^{m}(1|1,a)\cdot\frac{\eta_{t}\gamma}{1-\gamma}\prod_ {i=1}^{t-1}(1-\eta_{i}(1-\gamma))\] \[\geq-\frac{(1-\eta_{t})}{1-\gamma}\prod_{i=1}^{t-1}(1-\eta_{i}(1- \gamma))-\frac{\eta_{t}\gamma}{1-\gamma}\prod_{i=1}^{t-1}(1-\eta_{i}(1-\gamma))\] \[\geq-\frac{1}{1-\gamma}\prod_{i=1}^{t}(1-\eta_{i}(1-\gamma)). \tag{94}\]

For \(t\in\{t_{r}\}_{r=1}^{R}\) and \(a\in\{1,2\}\), we have,

\[Q_{t}^{m}(1,a)-\widehat{Q}_{t}^{m}(a) =\frac{1}{M}\sum_{m=1}^{M}Q_{t-1/2}^{m}(1,a)-\frac{1}{M}\sum_{m=1} ^{M}\widehat{Q}_{t-1/2}^{m}(a)\] \[=\frac{1}{M}\sum_{m=1}^{M}\left[(1-\eta_{t})Q_{t-1}^{m}(1,a)+\eta _{t}(1+\gamma\widehat{P}_{t}^{m}(1|1,a)V_{t-1}^{m}(1))\right]\] \[\qquad\qquad-\frac{1}{M}\sum_{m=1}^{M}\left[(1-\eta_{t})\widehat {Q}_{t-1}^{m}(a)+\eta_{t}(1+\gamma\widehat{P}_{t}^{m}(1|1,a)\widehat{V}_{t-1} ^{m})\right]\] \[=\frac{1}{M}\sum_{m=1}^{M}\left[(1-\eta_{t})(Q_{t-1}^{m}(1,a)- \widehat{Q}_{t-1}^{m}(a))+\eta_{t}\gamma\widehat{P}_{t}^{m}(1|1,a)(V_{t-1}^{m }(1)-\widehat{V}_{t-1}^{m})\right]\] \[\geq-\frac{1}{1-\gamma}\prod_{i=1}^{t}(1-\eta_{i}(1-\gamma)), \tag{95}\]

where the last step follows using the same set of arguments as used in (94). The inductive step follows from (94) and (95).

#### b.5.2 Proof of Lemma 3

In order to bound the term \(\mathbb{E}[\Delta_{t,\max}^{m}]-\mathbb{E}[\xi_{t^{\prime},t,\max}^{m}]\), we make use of the relation in (44a), which we recall

\[\mathbb{E}[\Delta_{t,\max}^{m}]\geq\varphi_{t^{\prime},t}\mathbb{E}[\overline{ \Delta}_{t^{\prime},\max}]+\left[\sum_{k=t^{\prime}+1}^{t}\widetilde{\eta}_{k }^{(t)}\gamma p\mathbb{E}[\Delta_{k-1,\max}^{m}]\right]+\mathbb{E}[\xi_{t^{ \prime},t,\max}^{m}]-\varphi_{t^{\prime},t}\mathbb{E}[|\overline{\Delta}_{t^{ \prime}}(1)-\overline{\Delta}_{t^{\prime}}(2)|].\]* To aid the analysis, we consider the following recursive relation for any fixed agent \(m\): \[y_{t}=(1-\eta_{t})y_{t-1}+\eta_{t}(\gamma py_{t-1}+\mathbb{E}[\xi^{m}_{t^{\prime},t,\max}]).\] (96) Upon unrolling the recursion, we obtain, \[y_{t}=\left(\prod_{k=t^{\prime}+1}^{t}(1-\eta_{k})\right)y_{t^{ \prime}}+\sum_{k=t^{\prime}+1}^{t}\left(\eta_{k}\prod_{i=k+1}^{t}(1-\eta_{i}) \right)\gamma py_{k-1}\] \[+\sum_{k=t^{\prime}+1}^{t}\left(\eta_{k}\prod_{i=k+1}^{t}(1-\eta_{ i})\right)\mathbb{E}[\xi^{m}_{t^{\prime},t,\max}]\] \[=\varphi_{t^{\prime},t}y_{t^{\prime}}+\sum_{k=t^{\prime}+1}^{t} \widehat{\eta}_{k}^{(t)}\gamma py_{k-1}+\sum_{k=t^{\prime}+1}^{t}\widehat{\eta }_{k}^{(t)}\mathbb{E}[\xi^{m}_{t^{\prime},t,\max}].\] (97) Initializing \(y_{t^{\prime}}=\mathbb{E}[\overline{\Delta}_{t^{\prime},\max}]\) in (97) and plugging this into (44a), we have \[\mathbb{E}[\Delta^{m}_{t,\max}]\geq y_{t}-\varphi_{t^{\prime},t}\mathbb{E}[| \overline{\Delta}_{t^{\prime}}(1)-\overline{\Delta}_{t^{\prime}}(2)|],\] where we used \(\sum_{k=t^{\prime}+1}^{t}\widehat{\eta}_{k}^{(t)}\leq 1\) (cf. (18)). We now further simply the expression of \(y_{t}\). By rewriting (96) as \[y_{t}=(1-\eta_{t}(1-\gamma p))y_{t-1}+\eta_{t}\mathbb{E}[\xi^{m}_{t^{\prime},t,\max}],\] it is straight forward to note that \(y_{t}\) is given as \[y_{t}=\left(\prod_{k=t^{\prime}+1}^{t}(1-\eta_{k}(1-\gamma p))\right)y_{t^{ \prime}}+\mathbb{E}[\xi^{m}_{t^{\prime},t,\max}]\left[\sum_{k=t^{\prime}+1}^{t} \eta_{k}^{(t)}\right].\] (98) Consequently, we have, \[\mathbb{E}[\Delta^{m}_{t,\max}]-\mathbb{E}[\xi^{m}_{t^{\prime},t, \max}]\geq \left(\prod_{k=t^{\prime}+1}^{t}(1-\eta_{k}(1-\gamma p))\right) \mathbb{E}[\overline{\Delta}_{t^{\prime},\max}]\] \[+\mathbb{E}[\xi^{m}_{t^{\prime},t,\max}]\left[\sum_{k=t^{\prime}+1 }^{t}\eta_{k}^{(t)}-1\right]-\varphi_{t^{\prime},t}\mathbb{E}[|\overline{ \Delta}_{t^{\prime}}(1)-\overline{\Delta}_{t^{\prime}}(2)|].\] (99)
* We can consider a slightly different recursive sequence defined as \[w_{t}=(1-\eta_{t})w_{t-1}+\eta_{t}(\gamma pw_{t-1}).\] (100) Using a similar sequence of arguments as outlined in (96)-(98), we can conclude that if \(w_{t^{\prime}}=\mathbb{E}[\overline{\Delta}_{t^{\prime},\max}]\), then \(\mathbb{E}[\Delta^{m}_{t,\max}]\geq w_{t}+\mathbb{E}[\xi^{m}_{t^{\prime},t,\max }]-\varphi_{t^{\prime},t}\mathbb{E}[|\overline{\Delta}_{t^{\prime}}(1)- \overline{\Delta}_{t^{\prime}}(2)|]\) and consequently, \[\mathbb{E}[\Delta^{m}_{t,\max}]\geq\left(\prod_{k=t^{\prime}+1}^{t}(1-\eta_{ k}(1-\gamma p))\right)\mathbb{E}[\overline{\Delta}_{t^{\prime},\max}]+\mathbb{E}[ \xi^{m}_{t^{\prime},t,\max}]-\varphi_{t^{\prime},t}\mathbb{E}[|\overline{\Delta }_{t^{\prime}}(1)-\overline{\Delta}_{t^{\prime}}(2)|].\] (101) On combining (99) and (101), we arrive at the claim.

#### b.5.3 Proof of Lemma 4

We begin with bounding the first term \(\mathbb{E}[\xi^{m}_{t^{\prime},t,\max}]\); the second bound follows in an almost identical derivation.

Step 1: applying Freedman's inequality.Using the relation \(\max\{a,b\}=\frac{a+b+\lfloor a-b\rfloor}{2}\), we can rewrite \(\mathbb{E}[\xi^{cm}_{t^{\prime},t,\max}]\) as

\[\mathbb{E}[\xi^{m}_{t^{\prime},t,\max}] =\mathbb{E}\left[\frac{\xi^{m}_{t^{\prime},t}(1)+\xi^{m}_{t^{ \prime},t}(2)}{2}+\left|\frac{\xi^{m}_{t^{\prime},t}(1)-\xi^{m}_{t^{\prime},t} (2)}{2}\right|\right]\] \[=\frac{1}{2}\mathbb{E}\Bigg{[}\Bigg{|}\underbrace{\sum_{k=t^{ \prime}+1}^{t}\widetilde{\eta}^{(t)}_{k}\gamma(\widehat{P}^{m}_{k}(1|1,1)- \widehat{P}^{m}_{k}(1|1,2))\widehat{V}^{m}_{k-1}}_{=:\zeta^{m}_{t^{\prime},t}} \Bigg{|}\Bigg{]}, \tag{102}\]

where we used the definition in (38) and the fact that \(\mathbb{E}[\xi^{cm}_{t^{\prime},t}(1)]=\mathbb{E}[\xi^{m}_{t^{\prime},t}(2)]=0\). Decompose \(\zeta^{m}_{t^{\prime},t}\) as

\[\zeta^{m}_{t^{\prime},t}=\sum_{k=t^{\prime}+1}^{t}\sum_{b=1}^{B} \widetilde{\eta}^{(t)}_{k}\frac{\gamma}{B}(P^{m}_{k,b}(1|1,1)-P^{m}_{k,b}(1|1, 2))\widehat{V}^{m}_{k-1}=:\sum_{l=1}^{L}z_{l}, \tag{103}\]

where for all \(1\leq l\leq L\)

\[z_{l}:=\frac{\gamma}{B}(P^{m}_{k(l),b(l)}(1|1,1)-P^{m}_{k(l),b(l)} (1|1,2))\widehat{V}^{m}_{k(l)-1}\]

with

\[k(l):=\lfloor l/B\rfloor+t^{\prime}+1;\quad b(l)=((l-1)\mod B)+1; \quad L=(t-t^{\prime})B.\]

Let \(\{\mathscr{F}_{l}\}_{l=1}^{L}\) be a filtration such that \(\mathscr{F}_{l}\) is the \(\sigma\)-algebra corresponding to \(\{P^{m}_{k(j),b(j)}(1|1,1),P^{m}_{k(j),b(j)}(1|1,2)\}_{j=1}^{L}\). It is straightforward to note that \(\{z_{l}\}_{l=1}^{L}\) is a martingale sequence adapted to the filtration \(\{\mathscr{F}\}_{l=1}^{L}\). We will use the Freedman's inequality [Freedman, 1975, Li et al., 2023] to obtain a high probability bound on \(|\zeta^{m}_{t^{\prime},t}|\).

* To that effect, note that \[\sup_{l}|z_{l}| \leq\sup_{l}\left|\widetilde{\eta}^{(t)}_{k(l)}\cdot\frac{\gamma} {B}\cdot(P^{m}_{k(l),b(l)}(1|1,1)-P^{m}_{k(l),b(l)}(1|1,2))\cdot\widehat{V}^{ m}_{k(l)-1}\right|\] \[\leq\widetilde{\eta}^{(t)}_{k(l)}\cdot\frac{\gamma}{B(1-\gamma)}\] \[\leq\frac{\eta_{t}}{B(1-\gamma)},\] (104) where the second step follows from the bounds \(|(P^{m}_{k(l),b(l)}(1|1,1)-P^{m}_{k(l),b(l)}(1|1,2))|\leq 1\) and \(\widehat{V}^{m}_{k(l)-1}\leq\frac{1}{1-\gamma}\) and the third step uses \(c_{\eta}\leq\frac{1}{1-\gamma}\) and the fact that \(\widetilde{\eta}^{(T)}_{k}\) is increasing in \(k\) in this regime. (cf. (19)).
* Similarly, \[\text{Var}(z_{l}|\mathscr{F}_{l-1}) \leq\left(\widetilde{\eta}^{(t)}_{k(l)}\right)^{2}\frac{\gamma^{2} }{B^{2}}\cdot\left(\widehat{V}^{m}_{k(l)-1}\right)^{2}\cdot\text{Var}(P^{m}_ {k(l),b(l)}(1|1,1)-P^{m}_{k(l),b(l)}(1|1,2))\] \[\leq\left(\widetilde{\eta}^{(t)}_{k(l)}\right)^{2}\frac{\gamma^{ 2}}{B^{2}(1-\gamma)^{2}}\cdot 2p(1-p)\] \[\leq\frac{2\left(\widetilde{\eta}^{(t)}_{k(l)}\right)^{2}}{3B^{2} (1-\gamma)}.\] (105)

Using the above bounds (104) and (105) along with Freedman's inequality yield that

\[\Pr\left(|\zeta^{m}_{t^{\prime},t}|\geq\sqrt{\frac{8\log(2/\delta )}{3B^{2}(1-\gamma)}\sum_{l=1}^{L}\left(\widetilde{\eta}^{(t)}_{k(l)}\right)^{2 }}+\frac{4\eta_{t}\log(2/\delta)}{3B(1-\gamma)}\right)\leq\delta. \tag{106}\]Setting \(\delta_{0}=\frac{(1-\gamma)^{2}}{2}\cdot\mathbb{E}[|\zeta_{t^{\prime},t}^{m}|^{2}]\), with probability at least \(1-\delta_{0}\), it holds

\[|\zeta_{t^{\prime},t}^{m}|\geq\sqrt{\frac{8\log(2/\delta_{0})}{3B(1-\gamma)} \sum_{k=t^{\prime}+1}^{t}\left(\widetilde{\eta}_{k}^{(t)}\right)^{2}}+\frac{4 \eta_{t}\log(2/\delta_{0})}{3B(1-\gamma)}=:D. \tag{107}\]

Consequently, plugging this back to (102), we obtain

\[\mathbb{E}[\xi_{t^{\prime},t,\max}^{m}] =\frac{1}{2}\mathbb{E}[|\zeta_{t^{\prime},t}^{m}|]\] \[\geq\frac{1}{2}\mathbb{E}[|\zeta_{t^{\prime},t}^{m}|\mathbb{1}\{| \zeta_{t^{\prime},t}^{m}|\leq D\}]\] \[\geq\frac{1}{2D}\mathbb{E}[|\zeta_{t^{\prime},t}^{m}|^{2}\mathbb{ 1}\left\{|\zeta_{t^{\prime},t}^{m}|\leq D\right\}]\] \[\geq\frac{1}{2D}\left(\mathbb{E}[|\zeta_{t^{\prime},t}^{m}|^{2}]- \mathbb{E}[|\zeta_{t^{\prime},t}^{m}|^{2}\mathbb{1}\left\{|\zeta_{t^{\prime},t }^{m}|>D\right\}]\right)\] \[\geq\frac{1}{2D}\left(\mathbb{E}[|\zeta_{t^{\prime},t}^{m}|^{2}]- \frac{\Pr(|\zeta_{t^{\prime},t}^{m}|>D)}{(1-\gamma)^{2}}\right)\geq\frac{1}{4D} \cdot\mathbb{E}[|\zeta_{t^{\prime},t}^{m}|^{2}]. \tag{108}\]

Here, the penultimate step used the fact that \(|\zeta_{t^{\prime},t}^{m}|\leq\sum_{k=t^{\prime}+1}^{t}\frac{ \widetilde{\eta}_{k}^{(t)}}{(1-\gamma)}\leq\frac{1}{(1-\gamma)}\), and the last step used the definition of \(\delta_{0}\). Thus, it is sufficient to obtain a lower bound on \(\mathbb{E}[|\zeta_{t^{\prime},t}^{m}|^{2}]\) in order obtain a lower bound for \(\mathbb{E}[\zeta_{t^{\prime},t}^{m}|^{2}]\).

Step 2: lower bounding \(\mathbb{E}[|\zeta_{t^{\prime},t}^{m}|^{2}]\).To proceed, we introduce the following lemma pertaining to lower bounding \(\widehat{V}_{t}^{m}\) that will be useful later.

**Lemma 6**.: _For all time instants \(t\in[T]\) and all agent \(m\in[M]\):_

\[\mathbb{E}\left[\left(\widehat{V}_{t}^{m}\right)^{2}\right]\geq\frac{1}{2(1- \gamma)^{2}}.\]

We have,

\[\mathbb{E}[|\zeta_{t^{\prime},t}^{m}|^{2}] =\mathbb{E}\left[\sum_{l=1}^{L}\text{Var}\left(z_{l}|\mathscr{F}_ {l-1}\right)\right]=\mathbb{E}\left[\sum_{l=1}^{L}\mathbb{E}\left[z_{l}^{2}| \mathscr{F}_{l-1}\right]\right]\] \[\geq\sum_{l=1}^{L}\left(\widetilde{\eta}_{k(l)}^{(t)}\right)^{2} \frac{\gamma^{2}}{B^{2}}\cdot 2p(1-p)\cdot\mathbb{E}\left[\left(\widehat{V}_{k(l)-1}^{m} \right)^{2}\right]\] \[\geq\sum_{l=1}^{L}\left(\widetilde{\eta}_{k(l)}^{(t)}\right)^{2} \frac{\gamma^{2}}{B^{2}}\cdot 2p(1-p)\cdot\frac{1}{2(1-\gamma)^{2}}\] \[\geq\frac{2}{9B(1-\gamma)}\cdot\sum_{k=\max\{t^{\prime},\tau\}+1 }^{t}\left(\widetilde{\eta}_{k}^{(t)}\right)^{2}, \tag{109}\]

where the third line follows from Lemma 6 and the fourth line uses \(\gamma\geq 5/6\).

Step 3: finishing up.We finish up the proof by bounding \(\sum_{k=\max\{t^{\prime},\tau\}+1}^{t}\left(\widetilde{\eta}_{k}^{(t)}\right)^ {2}\) for \(t-\max\{t^{\prime},\tau\}\geq 1/\eta_{\tau}\). We have

\[\sum_{k=\max\{t^{\prime},\tau\}+1}^{t}\left(\widetilde{\eta}_{k}^{(t)}\right)^ {2}\geq\sum_{k=\max\{t^{\prime},\tau\}+1}^{t}\left(\eta_{k}\prod_{i=k+1}^{t}(1 -\eta_{i})\right)^{2}\]\[\stackrel{{\rm(i)}}{{\geq}} \sum_{k=\max\{t^{\prime},\tau\}+1}^{t}\left(\eta_{t}\prod_{i=k+1}^ {t}(1-\eta_{\tau})\right)^{2}\] \[=\eta_{t}^{2}\sum_{k=\max\{t^{\prime},\tau\}+1}^{t}(1-\eta_{\tau}) ^{2(t-k)}\] \[\geq\eta_{t}^{2}\cdot\frac{1-(1-\eta_{\tau})^{2(t-\max\{t^{\prime},\tau\})}}{\eta_{\tau}(2-\eta_{\tau})}\] \[\geq\eta_{t}\cdot\frac{1-\exp(-2)}{6}\geq\frac{\eta_{t}}{10}\geq \frac{\eta_{T}}{10}, \tag{110}\]

where (i) follows from the monotonicity of \(\eta_{k}\). Plugging (110) into the expressions of \(D\) (cf. (107)) we have

\[D= \sqrt{\frac{8\log(2/\delta_{0})}{3B(1-\gamma)}\sum_{k=t^{\prime}+1 }^{t}\left(\widetilde{\eta}_{k}^{(t)}\right)^{2}}+\frac{4\eta_{t}\log(2/\delta _{0})}{3B(1-\gamma)}\] \[\leq\frac{9}{2}\mathbb{E}[|\zeta_{t^{\prime},t}^{m}|^{2}]\cdot \sqrt{\frac{8\log(2/\delta_{0})}{3}}\left(\frac{1}{B(1-\gamma)}\sum_{k=t^{ \prime}+1}^{t}\left(\widetilde{\eta}_{k}^{(t)}\right)^{2}\right)^{-1/2}+60\cdot \mathbb{E}[|\zeta_{t^{\prime},t}^{m}|^{2}]\cdot\log(2/\delta_{0})\] \[\leq 3\mathbb{E}[|\zeta_{t^{\prime},t}^{m}|^{2}]\cdot\log(2/ \delta_{0})\left[\sqrt{\frac{60B(1-\gamma)}{\eta_{t}}}+20\right]\] \[\leq 60\mathbb{E}[|\zeta_{t^{\prime},t}^{m}|^{2}]\cdot\log(2/ \delta_{0})\left[\sqrt{\frac{3B(1-\gamma)}{20\eta_{T}}}+1\right],\]

where the second line follows from (109) and (110), and the third line follows from (110). On combining the above bound with (108), we obtain,

\[\mathbb{E}[\xi_{t^{\prime},t,\max}^{m}]\geq\frac{1}{240\log(2/ \delta_{0})}\cdot\frac{\nu}{\nu+1}, \tag{111}\]

where \(\nu:=\sqrt{\frac{20\eta_{T}}{3B(1-\gamma)}}\). Note that we have,

\[\delta_{0}=\frac{(1-\gamma)^{2}}{2}\cdot\mathbb{E}[|\zeta_{t^{\prime},t}^{m}|^ {2}]\geq\frac{(1-\gamma)}{9B}\cdot\sum_{k=t^{\prime}+1}^{t}\left(\widetilde{ \eta}_{k}^{(t)}\right)^{2}\geq\frac{\eta_{T}(1-\gamma)}{90B}.\]

Combining the above bound with (111) yields us the required bound.

Step 4: repeating the argument for the second claim.We note that second claim in the theorem, i.e., the lower bound on \(\mathbb{E}\left[\max\left\{\frac{1}{M}\sum_{m=1}^{M}\xi_{t^{\prime},t}^{m}(1),\frac{1}{M}\sum_{m=1}^{M}\xi_{t^{\prime},t}^{m}(2)\right\}\right]\) follows through an identical series of arguments where the bounds in Eqns. (104) and (105) contain an additional factor of \(M\) in the denominator (effectively replacing \(B\) with \(BM\)), which is carried through in all the following steps.

#### b.5.4 Proof of Lemma 5

Using Eqns. (41) and (38), we can write

\[\overline{\Delta}_{t}(1)-\overline{\Delta}_{t}(2)= \left(\prod_{k=t^{\prime}+1}^{t}(1-\eta_{k})\right)(\overline{ \Delta}_{t^{\prime}}(1)-\overline{\Delta}_{t^{\prime}}(2))\] \[\quad+\frac{1}{M}\sum_{m=1}^{M}\sum_{k=t^{\prime}+1}^{t}\left( \eta_{k}\prod_{i=k+1}^{t}(1-\eta_{i})\right)\gamma(\widehat{P}_{k}^{m}(1|1,1) -\widehat{P}_{k}^{m}(1|1,2))\widehat{V}_{k-1}^{m}.\]Upon unrolling the recursion, we obtain,

\[\overline{\Delta}_{t}(1)-\overline{\Delta}_{t}(2)=\sum_{k=1}^{t}\sum_{m=1}^{M} \left(\eta_{k}\prod_{i=k+1}^{t}(1-\eta_{i})\right)\frac{\gamma}{M}(\widehat{P}_ {k}^{m}(1|1,1)-\widehat{P}_{k}^{m}(1|1,2))\widehat{V}_{k-1}^{m}.\]

If we define a filtration \(\mathscr{F}_{k}\) as the \(\sigma\)-algebra corresponding to \(\{\widehat{P}_{1}^{1}(1|1,1),\widehat{P}_{1}^{1}(1|1,2),\ldots,\widehat{P}_{l} ^{M}(1|1,1),\widehat{P}_{l}^{M}(1|1,2)\}_{l=1}^{k}\), then it is straightforward to note that \(\{\overline{\Delta}_{t}(1)-\overline{\Delta}_{t}(2)\}_{t}\) is a martingale sequence adapted to the filtration \(\{\mathscr{F}_{t}\}_{t}\). Using Jensen's inequality, we know that if \(\{Z_{t}\}_{t}\) is a martingale adapted to a filtration \(\{\mathscr{G}_{t}\}_{t}\), then for a convex function \(f\) such that \(f(Z_{t})\) is integrable for all \(t\), \(\{f(Z_{t})\}_{t}\) is a sub-martingale adapted to \(\{\mathscr{G}_{t}\}_{t}\). Since \(f(x)=|x|\) is a convex function, \(\{|\overline{\Delta}_{t}(1)-\overline{\Delta}_{t}(2)|\}_{t}\) is a submartingale adapted to the filtration \(\{\mathscr{F}_{t}\}_{t}\). As a result,

\[\sup_{1\leq t\leq T}\mathbb{E}[|\overline{\Delta}_{t}(1)-\overline{\Delta}_{t} (2)|]\leq\mathbb{E}[|\overline{\Delta}_{T}(1)-\overline{\Delta}_{T}(2)|]\leq \left(\mathbb{E}[(\overline{\Delta}_{T}(1)-\overline{\Delta}_{T}(2))^{2}]\right) ^{1/2}. \tag{112}\]

We use the following observation about a martingale sequence \(\{X_{i}\}_{i=1}^{t}\) adapted to a filtration \(\{\mathscr{G}_{i}\}_{i=1}^{t}\) to evaluate the above expression. We have,

\[\mathbb{E}\left[\left(\sum_{i=1}^{t}X_{i}\right)^{2}\right] =\mathbb{E}\left[\mathbb{E}\left[\left(\sum_{i=1}^{t}X_{i}\right) ^{2}\Big{|}\mathscr{G}_{t-1}\right]\right]\] \[=\mathbb{E}\left[\mathbb{E}\left[X_{t}^{2}+2X_{t}\left(\sum_{i=1} ^{t-1}X_{i}\right)+\left(\sum_{i=1}^{t-1}X_{i}\right)^{2}\Big{|}\mathscr{G}_{t -1}\right]\right]\] \[=\mathbb{E}\left[X_{t}^{2}\right]+\mathbb{E}\left[\left(\sum_{i=1 }^{t-1}X_{i}\right)^{2}\right]\] \[=\sum_{i=1}^{t}\mathbb{E}\left[X_{i}^{2}\right], \tag{113}\]

where the third step uses the facts that \(\left(\sum_{i=1}^{t-1}X_{i}\right)\) is \(\mathscr{G}_{t-1}\) measure and \(\mathbb{E}[X_{t}|\mathscr{G}_{t-1}]=0\) and fourth step is obtained by recursively applying second and third steps. Using the relation in Eqn. (113) in Eqn. (112), we obtain,

\[\sup_{1\leq t\leq T}\mathbb{E}[|\overline{\Delta}_{t}(1)-\overline {\Delta}_{t}(2)|] \leq\left(\mathbb{E}[(\overline{\Delta}_{T}(1)-\overline{\Delta}_ {T}(2))^{2}]\right)^{1/2}\] \[\leq\left(\sum_{k=1}^{T}\mathbb{E}\left[\left(\sum_{m=1}^{M} \widetilde{\eta}_{k}^{(T)}\cdot\frac{\gamma}{M}\cdot(\widehat{P}_{k}^{m}(1|1,1)-\widehat{P}_{k}^{m}(1|1,2))\widehat{V}_{k-1}^{m}\right)^{2}\right]\right)^{1 /2}\] \[\leq\left(\sum_{k=1}^{T}\left(\widetilde{\eta}_{k}^{(T)}\right) ^{2}\cdot\frac{2\gamma^{2}p(1-p)}{BM^{2}}\cdot\sum_{m=1}^{M}\mathbb{E}\left[ \left(\left(\tilde{V}_{k-1}^{m}\right)^{2}\right]\right)^{1/2}\] \[\leq\left(\sum_{k=1}^{T}\left(\widetilde{\eta}_{k}^{(T)}\right) ^{2}\cdot\frac{2\gamma^{2}p(1-p)}{BM(1-\gamma)^{2}}\right)^{1/2}. \tag{114}\]

Let us focus on the term involving the step sizes. We separately consider the scenario for constant step sizes and linearly rescaled step sizes. For constant step sizes, we have,

\[\sum_{k=1}^{T}\left(\widetilde{\eta}_{k}^{(T)}\right)^{2}=\sum_{k=1}^{T}\left( \eta_{k}\prod_{i=k+1}^{T}(1-\eta_{i})\right)^{2}=\sum_{k=1}^{T}\eta^{2}(1-\eta )^{2(T-k)}\leq\frac{\eta^{2}}{1-(1-\eta)^{2}}\leq\eta. \tag{115}\]

Similarly, for linearly rescaled step sizes, we have,

\[\sum_{k=1}^{T}\left(\widetilde{\eta}_{k}^{(T)}\right)^{2}=\sum_{k=1}^{\tau} \left(\widetilde{\eta}_{k}^{(T)}\right)^{2}+\sum_{k=\tau+1}^{T}\left(\eta_{k} \prod_{i=k+1}^{T}(1-\eta_{i})\right)^{2}\]\[\leq\sum_{k=1}^{\tau}\left(\widetilde{\eta}_{\tau}^{(T)}\right)^{2}+ \sum_{k=\tau+1}^{T}\eta_{k}^{2}(1-\eta_{T})^{2(T-k)}\] \[\leq\eta_{\tau}^{2}(1-\eta_{T})^{2(T-\tau)}\cdot\tau+\eta_{\tau}^ {2}\cdot\frac{1}{\eta_{T}(2-\eta_{T})}\] \[\leq 3\eta_{T}\cdot\eta_{T}\cdot T\cdot\exp\left(-\frac{4T\eta_{T}} {3}\right)+3\eta_{T}\] \[\leq\frac{9}{4e}\eta_{T}+3\eta_{T}\] \[\leq 4\eta_{T}, \tag{116}\]

where the second step uses \(c_{\eta}\leq\log N\leq\frac{1}{1-\gamma}\) and the fact that \(\widetilde{\eta}_{k}^{(T)}\) is increasing in \(k\) in this regime. (See Eqn. (19)) and fifth step uses \(xe^{-4x/3}\leq 3/4e\). On plugging results from Eqns. (115) and (116) into Eqn. (114) along with the value of \(p\), we obtain,

\[\sup_{1\leq t\leq T}\mathbb{E}[|\overline{\Delta}_{t}(1)-\overline{\Delta}_{t }(2)|]\leq\sqrt{\frac{8\eta_{T}}{3BM(1-\gamma)}}, \tag{117}\]

as required.

#### b.5.5 Proof of Lemma 6

For the proof, we fix an agent \(m\). In order to obtain the required lower bound on \(\widehat{V}_{t}^{m}\), we define an auxiliary sequence \(\overline{Q}_{t}^{m}\) that evolves as described in Algorithm 5. Essentially, \(\overline{Q}_{t}^{m}\) evolves in a manner almost identical to \(\widehat{Q}_{t}^{m}\) except for the fact that there is only one action and hence there is no maximization step in the update rule.

```
1:\(r\gets 1\), \(\overline{Q}_{0}^{m}=Q^{*}(1,1)\) for all \(m\in\{1,2,\ldots,M\}\)
2:for\(t=1,2,\ldots,T\)do
3:for\(m=1,2,\ldots,M\)do
4:\(\overline{Q}_{t-1/2}^{m}\leftarrow(1-\eta_{t})\overline{Q}_{t-1}^{m}(a)+\eta_ {t}(1+\widehat{P}_{t}^{m}(1|1,1)\overline{Q}_{t-1}^{m})\)
5: Compute \(\overline{Q}_{t}^{m}\) according to Eqn. (8)
6:endfor
7:endfor
```

**Algorithm 5**Evolution of \(\overline{Q}\)

It is straightforward to note that \(\widehat{Q}_{t}^{m}(1)\geq\overline{Q}_{t}^{m}\), which can be shown using induction. From the initialization, it follows that \(\widehat{Q}_{0}^{m}(1)\geq\overline{Q}_{0}^{m}\). Assuming the relation holds for \(t-1\), we have,

\[\widehat{Q}_{t-1/2}^{m}(1) =(1-\eta_{t})\widehat{Q}_{t-1}^{m}(1)+\eta_{t}(1+\gamma\widehat{ P}_{t}^{m}(1|1,1)\widehat{V}_{t-1}^{m})\] \[\geq(1-\eta_{t})\widehat{Q}_{t-1}^{m}(1)+\eta_{t}(1+\gamma\widehat {P}_{t}^{m}(1|1,1)\widehat{Q}_{t-1}^{m}(1))\] \[\geq(1-\eta_{t})\overline{Q}_{t-1}^{m}+\eta_{t}(1+\gamma\widehat {P}_{t}^{m}(1|1,1)\overline{Q}_{t-1}^{m})\] \[=\overline{Q}_{t-1/2}^{m}.\]

Since \(\widehat{Q}_{t}^{m}\) and \(\overline{Q}_{t}^{m}\) follow the same averaging schedule, it immediately follows from the above relation that \(\widehat{Q}_{t}^{m}(1)\geq\overline{Q}_{t}^{m}\). Since \(\widehat{V}_{t}^{m}\geq\widehat{Q}_{t}^{m}(1)\geq\overline{Q}_{t}^{m}\), we will use the sequence \(\overline{Q}_{t}^{m}\) to establish the required lower bound on \(\widehat{V}_{t}^{m}\).

We claim that for all time instants \(t\) and all agents \(m\),

\[\mathbb{E}[\overline{Q}_{t}^{m}]=\frac{1}{1-\gamma p}. \tag{118}\]

Assuming (118) holds, we have

\[\mathbb{E}[(\widehat{V}_{t}^{m})^{2}]\geq\left(\mathbb{E}[\widehat{V}_{t}^{m}] \right)^{2}\geq\left(\mathbb{E}[\overline{Q}_{t}^{m}]\right)^{2}\geq\left( \frac{1}{1-\gamma p}\right)^{2}\geq\frac{1}{2(1-\gamma)^{2}},\]as required. In the above expression, the first inequality follows from Jensen's inequality, the second from the relation \(\widehat{V}_{t}^{m}\geq\overline{Q}_{t}^{m}\geq 0\) and the third from (118).

We now move now to prove the claim (118) using induction. For the base case, \(\mathbb{E}[\overline{Q}_{0}^{m}]=\frac{1}{1-\gamma p}\) holds by choice of initialization. Assume that \(\mathbb{E}[\overline{Q}_{t-1}^{m}]=\frac{1}{1-\gamma p}\) holds for some \(t-1\) for all \(m\).

* If \(t\) is not an averaging instant, then for any client \(m\), \[\overline{Q}_{t}^{m} =(1-\eta_{t})\overline{Q}_{t-1}^{m}+\eta_{t}(1+\gamma\widehat{P}_ {t}^{m}(1|1,1)\overline{Q}_{t-1}^{m})\] \[\implies\mathbb{E}[\overline{Q}_{t}^{m}] =(1-\eta_{t})\mathbb{E}[\overline{Q}_{t-1}^{m}]+\eta_{t}(1+\gamma \mathbb{E}[\widehat{P}_{t}^{m}(1|1,1)\overline{Q}_{t-1}^{m}])\] \[=(1-\eta_{t})\mathbb{E}[\overline{Q}_{t-1}^{m}]+\eta_{t}(1+ \gamma p\mathbb{E}[\overline{Q}_{t-1}^{m}])\] \[=\frac{(1-\eta_{t})}{1-\gamma p}+\eta_{t}\left(1+\frac{\gamma p}{ 1-\gamma p}\right)=\frac{1}{1-\gamma p}.\] (119) The third line follows from the independence of \(\widehat{P}_{t}^{m}(1|1,1)\) and \(\overline{Q}_{t-1}^{m}\) and the fourth line uses the inductive hypothesis.
* If \(t\) is an averaging instant, then for all clients \(m\), \[\overline{Q}_{t}^{m} =\frac{(1-\eta_{t})}{M}\sum_{j=1}^{M}\overline{Q}_{t-1}^{j}+\eta_ {t}\frac{1}{M}\sum_{j=1}^{M}(1+\gamma\widehat{P}_{t}^{j}(1|1,1)\overline{Q}_{t -1}^{j})\] \[\implies\mathbb{E}[\overline{Q}_{t}^{m}] =\frac{(1-\eta_{t})}{M}\sum_{j=1}^{M}\mathbb{E}[\overline{Q}_{t-1} ^{j}]+\eta_{t}\frac{1}{M}\sum_{j=1}^{M}(1+\gamma\mathbb{E}[\widehat{P}_{t}^{j }(1|1,1)\overline{Q}_{t-1}^{j}])\] \[=\frac{(1-\eta_{t})}{M}\sum_{j=1}^{M}\frac{1}{1-\gamma p}+\eta_ {t}\frac{1}{M}\sum_{j=1}^{M}\left(1+\frac{\gamma p}{1-\gamma p}\right)=\frac{ 1}{1-\gamma p},\] (120) where we again make use of independence and the inductive hypothesis.

Thus, (119) and (120) taken together complete the inductive step.

## Appendix C Analysis of Fed-DVR-Q

In this section, we prove Theorem 2 that outlines the performance guarantees of Fed-DVR-Q. There are two main parts of the proof. The first part deals with establishing that for the given choice of parameters described in Section 4.1.3, the output of the algorithm is an \(\varepsilon\)-optimal estimate of \(Q^{\star}\) with probability \(1-\delta\). The second part deals with deriving the bounds on the sample and communication complexity based on the choice of prescribed parameters. We begin with the second part, which is easier of the two.

### Establishing the sample and communication complexity bounds

Establishing the communication complexity.We begin with bounding \(\mathsf{CC}_{\mathsf{round}}\). From the description of Fed-DVR-Q, it is straightforward to note that each epoch, i.e., each call to the RefineEstimate routine, involves \(I+1\) rounds of communication, one for estimating \(\mathcal{T}\overline{Q}\) and the remaining ones during the iterative updates of the Q-function. Since there are a total of \(K\) epochs,

\[\mathsf{CC}_{\mathsf{round}}(\mathsf{Fed\mbox{-}DVR\mbox{-}Q};\varepsilon,M, \delta)\leq(I+1)K\leq\frac{16}{\eta(1-\gamma)}\log_{2}\left(\frac{1}{(1- \gamma)\varepsilon}\right),\]

where the second bound follows from the prescribed choice of parameters in Sec. 4.1.3. Similarly, since the quantization step is designed to compress each coordinate into \(J\) bits, each message transmitted by an agent has a size of no more than \(J\cdot|\mathcal{S}||\mathcal{A}|\) bits. Consequently,

\[\mathsf{CC}_{\mathsf{bit}}(\mathsf{Fed\mbox{-}DVR\mbox{-}Q}; \varepsilon,M,\delta) \leq J\cdot|\mathcal{S}||\mathcal{A}|\cdot\mathsf{CC}_{\mathsf{ round}}(\mathsf{Fed\mbox{-}DVR\mbox{-}Q};\varepsilon,M,\delta)\] \[\leq\frac{32|\mathcal{S}||\mathcal{A}|}{\eta(1-\gamma)}\log_{2} \left(\frac{1}{(1-\gamma)\varepsilon}\right)\log_{2}\left(\frac{70}{\eta(1- \gamma)}\sqrt{\frac{4}{M}\log\left(\frac{8KI|\mathcal{S}||\mathcal{A}|}{ \delta}\right)}\right),\]

where once again in the second step we plugged in the choice of \(J\) from Sec. 4.1.3.

Establishing the sample complexity.In order to establish the bound on the sample complexity, note that during epoch \(k\), each agent takes a total of \(\lceil L_{k}/M\rceil+I\cdot B\) samples, where the first term corresponds to approximating \(\widetilde{\mathcal{T}}_{L}(Q^{(k-1)})\) and the second term corresponds to the samples taken during the iterative update scheme. Thus, the total sample complexity is obtained by summing up over all the \(K\) epochs. We have,

\[\mathsf{SC}(\mathsf{Fed\text{-}DVR\text{-}Q};\varepsilon,M, \delta)\leq\sum_{k=1}^{K}\left(\left\lceil\frac{L_{k}}{M}\right\rceil+I\cdot B \right)\leq I\cdot B\cdot K+\frac{1}{M}\sum_{k=1}^{K}L_{k}+K.\]

To continue, notice that

\[\frac{1}{M}\sum_{k=1}^{K}L_{k} \leq\frac{39200}{M(1-\gamma)^{2}}\log\left(\frac{8KI|\mathcal{S} ||\mathcal{A}|}{\delta}\right)\left(\sum_{k=1}^{K_{0}}4^{k}+\sum_{k=K_{0}+1}^{ K}4^{k-K_{0}}\right)\] \[\leq\frac{39200}{3M(1-\gamma)^{2}}\log\left(\frac{8KI|\mathcal{S} ||\mathcal{A}|}{\delta}\right)\left(4^{K_{0}}+4^{K-K_{0}}\right)\] \[\leq\frac{156800}{3M(1-\gamma)^{2}}\log\left(\frac{8KI|\mathcal{S} ||\mathcal{A}|}{\delta}\right)\left(\frac{1}{1-\gamma}+\frac{1}{(1-\gamma) \varepsilon^{2}}\right),\]

where the first line follows from the choice of \(L_{k}\) in Sec. 4.1.3 and the last line follows from \(K_{0}=\lceil\frac{1}{2}\log_{2}(\frac{1}{1-\gamma})\rceil\). Plugging this relation and the choices of \(I\) and \(B\) (cf. Sec. 4.1.3) into the previous bound yields

\[\mathsf{SC}(\mathsf{Fed\text{-}DVR\text{-}Q};\varepsilon,M,\delta) \leq\frac{4608}{\eta M(1-\gamma)^{3}}\log_{2}\left(\frac{1}{(1- \gamma)\varepsilon}\right)\log\left(\frac{8KI|\mathcal{S}||\mathcal{A}|}{ \delta}\right)+K\] \[\qquad+\frac{156800}{3M(1-\gamma)^{2}}\log\left(\frac{8KI|\mathcal{ S}||\mathcal{A}|}{\delta}\right)\left(\frac{1}{1-\gamma}+\frac{1}{(1-\gamma) \varepsilon^{2}}\right)\] \[\leq\frac{313600}{\eta M(1-\gamma)^{3}\varepsilon^{2}}\log_{2} \left(\frac{1}{(1-\gamma)\varepsilon}\right)\log\left(\frac{8KI|\mathcal{S}|| \mathcal{A}|}{\delta}\right)+K.\]

Plugging in the choice of \(K\) finishes the proof.

### Establishing the error guarantees

In this section, we show that the Q-function estimate returned by the \(\mathsf{Fed\text{-}DVR\text{-}Q}\) algorithm is \(\varepsilon\)-optimal with probability at least \(1-\delta\). We claim that the estimates of the Q-function generated by the algorithm across different epochs satisfy the following relation for all \(k\leq K\) with probability \(1-\delta\):

\[\|Q^{(k)}-Q^{\star}\|_{\infty}\leq\frac{2^{-k}}{1-\gamma}. \tag{121}\]

The required bound on \(\|Q^{(K)}-Q^{\star}\|_{\infty}\) immediately follows by plugging in the value of \(K\). Thus, for the remainder of the section, we focus on establishing the above claim.

Step 1: fixed-point contraction of RefineEstimate.Firstly, note that the variance-reduced update scheme carried out during the RefineEstimate routine resembles that of the classic Q-learning scheme, i.e., fixed-point iteration, with a different operator defined as follows:

\[\mathcal{H}(Q):=\mathcal{T}(Q)-\mathcal{T}(\overline{Q})+ \widetilde{\mathcal{T}}_{L}(\overline{Q}),\quad\text{for some fixed } \overline{Q}. \tag{122}\]

Thus, the update scheme at step \(i\geq 1\) in (11) can then be written as

\[Q_{i-\frac{1}{2}}^{m}=(1-\eta)Q_{i-1}+\eta\widehat{\mathcal{H}} _{i}^{(m)}(Q_{i-1}), \tag{123}\]

where \(\widehat{\mathcal{H}}_{i}^{(m)}(Q):=\widehat{\mathcal{T}}_{i}^{(m)}(Q)- \widehat{\mathcal{T}}_{i}^{(m)}(\overline{Q})+\widetilde{\mathcal{T}}_{L}( \overline{Q})\) is a stochastic, unbiased estimate of the operator \(\mathcal{H}\), similar to \(\widehat{\mathcal{T}}_{i}^{(m)}(Q)\). Let \(Q_{\mathcal{H}}^{\star}\) denote the fixed point of \(\mathcal{H}\). Then the update scheme in (123) drives the sequence \(\{Q_{i}^{m}\}_{i\geq 0}\) to \(Q_{\mathcal{H}}^{\star}\); further, as long as \(\|Q^{\star}-Q_{\mathcal{H}}^{\star}\|_{\infty}\) is small, the required error \(\|Q_{i}-Q^{\star}\|_{\infty}\) can also be controlled. The following lemmas formalize these ideas and pave the path to establish the claim in (121). The proofs are deferred to Appendix C.3.

[MISSING_PAGE_FAIL:41]

Now we move to the second case, for \(k>K_{0}\). From (125) and choice of \(L_{k}\) (Sec. 4.1.3), we have

\[\|Q^{(k)}-Q^{\star}\|_{\infty} \leq\|Q^{(k-1)}-Q^{\star}\|_{\infty}\left(\frac{1}{6}+\frac{7}{6} \sqrt{\frac{16\kappa^{\prime}}{L_{k}(1-\gamma)^{2}}}\right)+\frac{7}{6}\sqrt{ \frac{100\kappa^{\prime}}{L_{k}(1-\gamma)^{3}}}+\frac{13D_{k}}{420}\] \[\leq\frac{2^{-(k-1)}}{1-\gamma}\left(\frac{1}{6}+2^{-(k-K_{0})} \cdot\frac{7}{6}\sqrt{\frac{8}{19600}}\right)+2^{-(k-K_{0})}\cdot\frac{7}{6} \sqrt{\frac{50}{19600(1-\gamma)}}+\frac{104}{420}\cdot\frac{2^{-(k-1)}}{1-\gamma}\] \[\leq\frac{2^{-(k-1)}}{1-\gamma}\left(\frac{1}{6}+\frac{7}{6} \sqrt{\frac{1}{196}}+\frac{1}{4}\right)\] \[\leq\frac{2^{-k}}{1-\gamma}. \tag{127}\]

By a union bound argument, we can conclude that the relation \(\|Q^{(k)}-Q^{\star}\|_{\infty}\leq\frac{2^{-k}}{1-\gamma}\) holds for all \(k\leq K\) with probability at least \(1-\delta\).

Step 3: confirm the compressor bound.The only thing left to verify is that the inputs to the compressor are always bounded by \(D_{k}\) during the \(k\)-th epoch, for all \(1\leq k\leq K\). The following lemma provides a bound on the input to the compressor during any run of the RefineEstimate routine.

**Lemma 9**.: _Consider the RefineEstimate routine described in Algorithm 3 with some for some fixed \(\overline{Q}\). For all \(i\leq I\) and all agents \(m\), the following bound holds with probability \(1-\frac{\delta}{2K}\):_

\[\|Q^{m}_{i-\frac{1}{2}}-Q_{i-1}\|_{\infty}\leq\eta\|\overline{Q}-Q^{\star}_{ \mathcal{H}}\|_{\infty}\left(\frac{7}{6}\cdot(1+\gamma)+2\gamma\right)+\frac{ \eta D(1+\gamma)}{70}.\]

For the \(k\)-th epoch, it follows that

\[\eta\|Q^{(k-1)}-Q^{\star}_{\mathcal{H}_{k}}\|_{\infty}\left(\frac {7}{6}\cdot(1+\gamma)+2\gamma\right)+\frac{\eta D_{k}(1+\gamma)}{70}\] \[\leq\frac{13}{3}\left(\|Q^{(k-1)}-Q^{\star}\|_{\infty}+\|Q^{ \star}-Q^{\star}_{\mathcal{H}_{k}}\|_{\infty}\right)+\frac{D_{k}(1+\gamma)}{70}\] \[\leq\frac{13}{3}\cdot\frac{15}{14}\cdot\|Q^{(k-1)}-Q^{\star}\|_{ \infty}+\frac{2D_{k}}{70}\] \[\leq\left(\frac{195}{42}+\frac{16}{70}\right)\cdot\frac{2^{-(k-1) }}{1-\gamma}\] \[\leq 8\cdot\frac{2^{-(k-1)}}{1-\gamma}:=D_{k}.\]

In the third step, we used the same sequence of arguments as used in (126) and (127) and, in the fourth step, we used the bound on \(\|Q^{(k-1)}-Q^{\star}\|_{\infty}\) from (121) and the prescribed value of \(D_{k}\).

### Proof of auxiliary lemmas

#### c.3.1 Proof of Lemma 7

Let us begin with analyzing the evolution of the sequence \(\{Q_{i}\}_{i=1}^{I}\) during a run of the RefineEstimate routine. The sequence \(\{Q_{i}\}_{i=1}^{I}\) satisfies the following recursion:

\[Q_{i} =Q_{i-1}+\frac{1}{M}\sum_{m=1}^{M}\mathscr{C}\left(Q^{m}_{i-\frac {1}{2}}-Q_{i-1};D,J\right)\] \[=Q_{i-1}+\frac{1}{M}\sum_{m=1}^{M}\left(Q^{m}_{i-\frac{1}{2}}-Q_{ i-1}+\zeta^{m}_{i}\right)\]\[=\frac{1}{M}\sum_{m=1}^{M}\left(Q_{i-\frac{1}{2}}^{m}+\zeta_{i}^{m} \right)=(1-\eta)Q_{i-1}+\frac{\eta}{M}\sum_{m=1}^{M}\widehat{\mathcal{H}}_{i}^{( m)}(Q_{i-1})+\underbrace{\frac{1}{M}\sum_{m=1}^{M}\zeta_{i}^{m}}_{=\zeta_{i}}. \tag{128}\]

In the above expression, \(\zeta_{i}^{m}\) denotes the quantization noise introduced at agent \(m\) in the \(i\)-th update. Subtracting \(Q_{\mathcal{H}}^{\star}\) from both sides of (128), we obtain

\[Q_{i}-Q_{\mathcal{H}}^{\star} =(1-\eta)(Q_{i-1}-Q_{\mathcal{H}}^{\star})+\frac{\eta}{M}\sum_{m=1 }^{M}\left(\widehat{\mathcal{H}}_{i}^{(m)}(Q_{i-1})-Q_{\mathcal{H}}^{\star} \right)+\zeta_{i}\] \[=(1-\eta)(Q_{i-1}-Q_{\mathcal{H}}^{\star})+\frac{\eta}{M}\sum_{m=1 }^{M}\left(\widehat{\mathcal{H}}_{i}^{(m)}(Q_{i-1})-\widehat{\mathcal{H}}_{i} ^{(m)}(Q_{\mathcal{H}}^{\star})\right)\] \[\qquad\qquad\qquad\qquad\qquad\qquad+\frac{\eta}{M}\sum_{m=1}^{M} \left(\widehat{\mathcal{H}}_{i}^{(m)}(Q_{\mathcal{H}}^{\star})-\mathcal{H}(Q_ {\mathcal{H}}^{\star})\right)+\zeta_{i}. \tag{129}\]

Consequently,

\[\|Q_{i}-Q_{\mathcal{H}}^{\star}\|_{\infty}\leq(1-\eta)\|Q_{i-1}-Q_ {\mathcal{H}}^{\star}\|_{\infty} +\frac{\eta}{M}\sum_{m=1}^{M}\left\|\widehat{\mathcal{H}}_{i}^{( m)}(Q_{i-1})-\widehat{\mathcal{H}}_{i}^{(m)}(Q_{\mathcal{H}}^{\star})\right\|_{\infty}\] \[+\left\|\frac{\eta}{M}\sum_{m=1}^{M}\left(\widehat{\mathcal{H}}_{ i}^{(m)}(Q_{\mathcal{H}}^{\star})-\mathcal{H}(Q_{\mathcal{H}}^{\star})\right) \right\|_{\infty}+\left\|\zeta_{i}\right\|_{\infty}, \tag{130}\]

which we shall proceed to bound each term separately.

* Regarding the second term, it follows that \[\left\|\widehat{\mathcal{H}}_{i}^{(m)}(Q)-\widehat{\mathcal{H}}_{i}^{(m)}(Q_ {\mathcal{H}}^{\star})\right\|_{\infty}=\left\|\widehat{\mathcal{H}}_{i}^{(m) }(Q)-\widehat{\mathcal{H}}_{i}^{(m)}(Q_{\mathcal{H}}^{\star})\right\|_{\infty} \leq\gamma\left\|Q-Q_{\mathcal{H}}^{\star}\right\|_{\infty},\] (131) which holds for all \(Q\) since \(\widehat{\mathcal{H}}_{i}^{(m)}\) is a \(\gamma\)-contractive operator.
* Regarding the third term, notice that \[\frac{1}{M}\sum_{m=1}^{M}\left(\widehat{\mathcal{H}}_{i}^{(m)}(Q_{\mathcal{H} }^{\star})-\mathcal{H}(Q_{\mathcal{H}}^{\star})\right)=\frac{1}{MB}\sum_{m=1} ^{M}\sum_{z\in\mathcal{Z}_{i}^{(m)}}\left(\mathcal{T}_{z}(Q_{\mathcal{H}}^{ \star})-\mathcal{T}_{z}(\overline{Q})-\mathcal{T}(Q_{\mathcal{H}}^{\star})+ \mathcal{T}(\overline{Q})\right).\] Note that \(\mathcal{T}_{z}(Q_{\mathcal{H}}^{\star})-\mathcal{T}_{z}(\overline{Q})- \mathcal{T}(Q_{\mathcal{H}}^{\star})+\mathcal{T}(\overline{Q})\) is a zero-mean random vector satisfying \[\|\mathcal{T}_{z}(Q_{\mathcal{H}}^{\star})-\mathcal{T}_{z}(\overline{Q})- \mathcal{T}(Q_{\mathcal{H}}^{\star})+\mathcal{T}(\overline{Q})\|_{\infty}\leq 2 \gamma\|\overline{Q}-Q_{\mathcal{H}}^{\star}\|_{\infty}.\] (132) Thus, each of its coordinate is a \((2\gamma\|\overline{Q}-Q_{\mathcal{H}}^{\star}\|_{\infty})^{2}\)-sub-Gaussian vector. Applying the tail bounds for a maximum of sub-Gaussian random variables [20], we obtain that \[\left\|\frac{1}{M}\sum_{m=1}^{M}\left(\widehat{\mathcal{H}}_{i}^{(m)}(Q_{ \mathcal{H}}^{\star})-\mathcal{H}(Q_{\mathcal{H}}^{\star})\right)\right\|_{ \infty}\leq 2\gamma\|\overline{Q}-Q_{\mathcal{H}}^{\star}\|_{\infty}\cdot \sqrt{\frac{2}{MB}\log\left(\frac{8KI|\mathcal{S}||\mathcal{A}|}{\delta} \right)}\] holds with probability at least \(1-\frac{\delta}{4KI}\).
* Turning to the last term, by the construction of the compression routine described in Section 4.1.2, it is straightforward to note that \(\zeta_{i}^{m}\) is a zero-mean random vector whose coordinates are independent, \(D^{2}\cdot 4^{-J}\)-sub-Gaussian random variables. Thus, \(\zeta_{i}\) is also a zero-mean random vector whose coordinates are independent, \(\frac{D^{2}}{M\cdot 4^{J}}\)-sub-Gaussian random variables. Hence, we can similarly conclude that \[\|\zeta_{i}\|_{\infty}\leq D\cdot 2^{-J}\cdot\sqrt{\frac{2}{M}\log\left(\frac{8KI| \mathcal{S}||\mathcal{A}|}{\delta}\right)}\] (134) holds with probability at least \(1-\frac{\delta}{4KI}\).

Combining the above bounds into (130), and introducing the short-hand notation \(\kappa:=\log\left(\frac{8KI|\mathcal{S}||\mathcal{A}|}{\delta}\right)\), we obtain with probability at least \(1-\frac{\delta}{2KI}\),

\[\|Q_{i}-Q_{\mathcal{H}}^{\star}\|_{\infty}\leq(1-\eta(1-\gamma))\|Q_{i-1}-Q_{ \mathcal{H}}^{\star}\|_{\infty}+2\eta\gamma\|\overline{Q}-Q_{\mathcal{H}}^{ \star}\|_{\infty}\cdot\sqrt{\frac{2\kappa}{MB}}+D\cdot 2^{-J}\cdot\sqrt{\frac{2 \kappa}{M}}.\]

Unrolling the above recursion over \(i=1,\ldots,I\) yields the following relation, which holds with probability at least \(1-\frac{\delta}{2K}\):

\[\|Q_{I}-Q_{\mathcal{H}}^{\star}\|_{\infty} \leq(1-\eta(1-\gamma))^{I}\left\|Q_{0}-Q_{\mathcal{H}}^{\star} \right\|_{\infty}+\sqrt{\frac{2\kappa}{M}}\left(\frac{2\eta\gamma}{\sqrt{B}} \|\overline{Q}-Q_{\mathcal{H}}^{\star}\|_{\infty}+D\cdot 2^{-J}\right)\cdot\sum_{i=1} ^{I}\left(1-\eta(1-\gamma)\right)^{I-i}\] \[\leq(1-\eta(1-\gamma))^{I}\left\|\overline{Q}-Q_{\mathcal{H}}^{ \star}\right\|_{\infty}+\frac{1}{\eta(1-\gamma)}\sqrt{\frac{2\kappa}{M}}\left( \frac{2\eta\gamma}{\sqrt{B}}\|\overline{Q}-Q_{\mathcal{H}}^{\star}\|_{\infty}+ D\cdot 2^{-J}\right)\] \[\leq\|\overline{Q}-Q_{\mathcal{H}}^{\star}\|_{\infty}\left((1- \eta(1-\gamma))^{I}+\frac{2\gamma}{(1-\gamma)}\sqrt{\frac{2\kappa}{MB}}\right) +\frac{D\cdot 2^{-J}}{\eta(1-\gamma)}\cdot\sqrt{\frac{2\kappa}{M}} \tag{135}\] \[\leq\frac{\|\overline{Q}-Q_{\mathcal{H}}^{\star}\|_{\infty}}{6}+ \frac{D}{70}\leq\frac{1}{6}\left(\|\overline{Q}-Q^{\star}\|_{\infty}+\|Q^{ \star}-Q_{\mathcal{H}}^{\star}\|_{\infty}\right)+\frac{D}{70}. \tag{136}\]

Here, the fourth step is obtained by plugging in the prescribed values of \(B,I\) and \(J\) in Sec. 4.1.3.

#### c.3.2 Proof of Lemma 8

Intuitively, the error \(\|Q_{\mathcal{H}}^{\star}-Q^{\star}\|_{\infty}\) depends on the error term \(\widetilde{\mathcal{T}}_{L}(\overline{Q})-\mathcal{T}(\overline{Q})\). If the latter is small, then \(\mathcal{H}(Q)\) is close to \(\mathcal{T}(Q)\) and consequently so are \(Q_{\mathcal{H}}^{\star}\) and \(Q^{\star}\). Thus, we begin with bounding the term \(\widetilde{\mathcal{T}}_{L}(\overline{Q})-\mathcal{T}(\overline{Q})\). We have,

\[\widetilde{\mathcal{T}}_{L}(\overline{Q})-\mathcal{T}(\overline{Q})\] \[=\overline{Q}+\frac{1}{M}\sum_{m=1}^{M}\mathscr{C}\left( \widetilde{\mathcal{T}}_{L}^{(m)}(\overline{Q})-\overline{Q}\right)-\mathcal{ T}(\overline{Q})\] \[=\frac{1}{M}\sum_{m=1}^{M}\left(\widetilde{\mathcal{T}}_{L}^{(m) }(\overline{Q})+\tilde{\zeta}_{L}^{(m)}\right)-\mathcal{T}(\overline{Q})\] \[=\frac{1}{M}\sum_{m=1}^{M}\left(\widetilde{\mathcal{T}}_{L}^{(m) }(\overline{Q})-\widetilde{\mathcal{T}}_{L}^{(m)}(Q^{\star})-\mathcal{T}( \overline{Q})+\mathcal{T}(Q^{\star})\right)+\frac{1}{M}\sum_{m=1}^{M}\tilde{ \zeta}_{L}^{(m)}+\frac{1}{M}\sum_{m=1}^{M}\left(\widetilde{\mathcal{T}}_{L}^{ (m)}(Q^{\star})-\mathcal{T}(Q^{\star})\right), \tag{137}\]

where once again \(\tilde{\zeta}_{L}^{(m)}:=\widetilde{\mathcal{T}}_{L}^{(m)}(\overline{Q})- \overline{Q}-\mathscr{C}\left(\widetilde{\mathcal{T}}_{L}^{(m)}(\overline{Q} )-\overline{Q}\right)\) denotes the quantization error at agent \(m\). Similar to the arguments of (133) and (134), we can conclude that each of the following relations hold with probability at least \(1-\frac{\delta}{6K}\):

\[\left\|\frac{1}{M}\sum_{m=1}^{M}\left(\widetilde{\mathcal{T}}_{L }^{(m)}(\overline{Q})-\widetilde{\mathcal{T}}_{L}^{(m)}(Q^{\star})-\mathcal{ T}(\overline{Q})+\mathcal{T}(Q^{\star})\right)\right\|_{\infty} \leq 2\gamma\|\overline{Q}-Q^{\star}\|_{\infty}\cdot\sqrt{\frac{2}{L} \log\left(\frac{12K|\mathcal{S}||\mathcal{A}|}{\delta}\right)}, \tag{138}\] \[\left\|\frac{1}{M}\sum_{m=1}^{M}\tilde{\zeta}_{L}^{(m)}\right\|_{ \infty} \leq D\cdot 2^{-J}\cdot\sqrt{\frac{2}{M}\log\left(\frac{12K|\mathcal{S}|| \mathcal{A}|}{\delta}\right)}. \tag{139}\]

For the third term, we can rewrite it as

\[\frac{1}{M}\sum_{m=1}^{M}\left(\widetilde{\mathcal{T}}_{L}^{(m)}(Q^{\star})- \mathcal{T}(Q^{\star})\right)=\frac{1}{M\lceil L/M\rceil}\sum_{m=1}^{M}\sum_{ l=1}^{\lceil L/M\rceil}\left(\mathcal{T}_{Z_{i}^{(m)}}(Q^{\star})-\mathcal{T}(Q^{ \star})\right).\]We will use Bernstein inequality element wise to bound the above term. Let \(\mathbf{\sigma}^{\star}\in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{A}|}\) be such that \([\mathbf{\sigma}^{\star}(s,a)]^{2}=\text{Var}(\mathcal{T}_{Z}(Q^{\star})(s,a))\), i.e., \((s,a)\)-th element of \(\mathbf{\sigma}\) denotes the standard deviation of the random variable \(\mathcal{T}_{Z}(Q^{\star})(s,a)\). Since \(\|\mathcal{T}_{Z}(Q^{\star})-\mathcal{T}(Q^{\star})\|_{\infty}\leq\frac{1}{1 -\gamma}\) a.s., Bernstein inequality gives us that

\[\left|\frac{1}{M}\sum_{m=1}^{M}\left(\widetilde{\mathcal{T}}_{L} ^{(m)}(Q^{\star})(s,a)-\mathcal{T}(Q^{\star})(s,a)\right)\right|\leq\mathbf{\sigma }^{\star}(s,a)\sqrt{\frac{2}{L}\log\left(\frac{6K|\mathcal{S}||\mathcal{A}|}{ \delta}\right)}+\frac{2}{3L(1-\gamma)}\log\left(\frac{6K|\mathcal{S}||\mathcal{ A}|}{\delta}\right). \tag{140}\]

holds simultaneously for all \((s,a)\in\mathcal{S}\times\mathcal{A}\) with probability at least \(1-\frac{\delta}{6K}\). On combining (137), (138), (139) and (140), we obtain that

\[\left|\widetilde{\mathcal{T}}_{L}(\overline{Q})(s,a)-\mathcal{T} (\overline{Q})(s,a)\right|=\|\overline{Q}-Q^{\star}\|_{\infty}\cdot\sqrt{\frac {8\kappa^{\prime}}{L}}+\mathbf{\sigma}^{\star}(s,a)\sqrt{\frac{2\kappa^{\prime}}{L }}+\frac{2\kappa^{\prime}}{3L(1-\gamma)}+D\cdot 2^{-J}\cdot\sqrt{\frac{2\kappa^{ \prime}}{M}}, \tag{141}\]

holds simultaneously for all \((s,a)\in\mathcal{S}\times\mathcal{A}\) with probability at least \(1-\frac{\delta}{2K}\), where \(\kappa^{\prime}=\log\left(\frac{12K|\mathcal{S}||\mathcal{A}|}{\delta}\right)\). We use this bound in (141) to obtain a bound on \(\|Q^{\star}_{\mathcal{H}}-Q^{\star}\|_{\infty}\) using the following lemma.

**Lemma 10** (Wainwright (2019b)).: _Let \(\pi^{\star}\) and \(\pi^{\star}_{\mathcal{H}}\) respectively denote the optimal policies w.r.t. \(Q^{\star}\) and \(Q^{\star}_{\mathcal{H}}\). Then,_

\[\|Q^{\star}_{\mathcal{H}}-Q^{\star}\|_{\infty}\leq\max\left\{(I- \gamma P^{\pi^{\star}})^{-1}\left|\widetilde{\mathcal{T}}_{L}(\overline{Q})- \mathcal{T}(\overline{Q})\right|,(I-\gamma P^{\pi^{\star}_{\mathcal{H}}})^{-1} \left|\widetilde{\mathcal{T}}_{L}(\overline{Q})-\mathcal{T}(\overline{Q}) \right|\right\}.\]

_Here, for any deterministic policy \(\pi\), \(P^{\pi}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}|\times|\mathcal{S}||\mathcal{ A}|}\) is given by \((P^{\pi}Q)(s,a)=\sum_{s^{\prime}\in\mathcal{S}}P(s^{\prime}|s,a)Q(s^{\prime},\pi(s^{ \prime}))\)._

Furthermore, it was shown in Wainwright (2019b, Proof of Lemma 4) that if the error \(|\widetilde{\mathcal{T}}_{L}(\overline{Q})(s,a)-\mathcal{T}(\overline{Q})(s,a)|\) satisfies

\[\left|\widetilde{\mathcal{T}}_{L}(\overline{Q})(s,a)-\mathcal{T} (\overline{Q})(s,a)\right|\leq z_{0}\|\overline{Q}-Q^{\star}\|_{\infty}+z_{1} \mathbf{\sigma}^{\star}(s,a)+z_{2} \tag{142}\]

for some \(z_{0},z_{1},z_{2}\geq 0\) with \(z_{1}<1\), then the bound in Lemma 10 can be simplified to

\[\|Q^{\star}_{\mathcal{H}}-Q^{\star}\|_{\infty}\leq\frac{1}{1-z_{1}}\left( \frac{z_{0}}{1-\gamma}\|\overline{Q}-Q^{\star}\|_{\infty}+\frac{z_{1}}{(1- \gamma)^{3/2}}+\frac{z_{2}}{1-\gamma}\right). \tag{143}\]

On comparing, (141) with (142), we obtain

\[z_{0}\equiv\sqrt{\frac{8\kappa^{\prime}}{L}};\quad z_{1}\equiv \sqrt{\frac{2\kappa^{\prime}}{L}};\quad z_{2}\equiv\frac{2\kappa^{\prime}}{3L(1 -\gamma)}+D\cdot 2^{-J}\cdot\sqrt{\frac{2\kappa^{\prime}}{M}}.\]

Moreover, the condition \(L\geq 32\kappa^{\prime}\) implies that \(z_{1}<1\) and \(\frac{1}{1-z_{1}}\leq\sqrt{2}\). Thus, on plugging in the above values in (143), we can conclude that

\[\|Q^{\star}_{\mathcal{H}}-Q^{\star}\|_{\infty} \leq\|\overline{Q}-Q^{\star}\|_{\infty}\cdot\sqrt{\frac{16\kappa ^{\prime}}{L(1-\gamma)^{2}}}+\sqrt{\frac{64\kappa^{\prime}}{L(1-\gamma)^{3}}} +\frac{2\kappa^{\prime}\sqrt{2}}{3L(1-\gamma)^{2}}+\frac{D\cdot 2^{-J}}{(1- \gamma)}\cdot\sqrt{\frac{4\kappa^{\prime}}{M}}\] \[\leq\|\overline{Q}-Q^{\star}\|_{\infty}\cdot\sqrt{\frac{8\kappa^ {\prime}}{L(1-\gamma)^{2}}}+\sqrt{\frac{32\kappa^{\prime}}{L(1-\gamma)^{3}}}+ \frac{2\sqrt{2}\kappa^{\prime}}{3L(1-\gamma)^{2}}+\frac{D}{40}, \tag{144}\]

where once again we use the value of \(J\) in the last step.

#### c.3.3 Proof of Lemma 9

From the iterative update rule in (123), for any agent \(m\) we have,

\[Q^{m}_{i-\frac{1}{2}}-Q_{i-1}=\eta(\widehat{\mathcal{H}}^{(m)}_{i-1}(Q_{i-1})-Q_ {i-1})\]\[=\eta(\widehat{\mathcal{H}}_{i-1}^{(m)}(Q_{i-1})-\widehat{\mathcal{H}}_{i-1}^{(m)} (Q_{\mathcal{H}}^{\star})+\widehat{\mathcal{H}}_{i-1}^{(m)}(Q_{\mathcal{H}}^{ \star})-\mathcal{H}(Q_{\mathcal{H}}^{\star})+Q_{\mathcal{H}}^{\star}-Q_{i-1}).\]

Thus,

\[\|Q_{i-\frac{1}{2}}^{m}-Q_{i-1}\|_{\infty} \leq\eta\left(\|\widehat{\mathcal{H}}_{i-1}^{(m)}(Q_{i-1})-\widehat {\mathcal{H}}_{i-1}^{(m)}(Q_{\mathcal{H}}^{\star})\|_{\infty}+\|\widehat{ \mathcal{H}}_{i-1}^{(m)}(Q_{\mathcal{H}}^{\star})-\mathcal{H}(Q_{\mathcal{H}}^ {\star})\|_{\infty}+\|Q_{\mathcal{H}}^{\star}-Q_{i-1}\|_{\infty}\right)\] \[\leq\eta\left(\gamma\|Q_{i-1}-Q_{\mathcal{H}}^{\star}\|_{\infty}+ 2\gamma\|\overline{Q}-Q_{\mathcal{H}}^{\star}\|_{\infty}+\|Q_{\mathcal{H}}^{ \star}-Q_{i-1}\|_{\infty}\right)\] \[=\eta\left((1+\gamma)\|Q_{i-1}-Q_{\mathcal{H}}^{\star}\|_{\infty} +2\gamma\|\overline{Q}-Q_{\mathcal{H}}^{\star}\|_{\infty}\right)\] \[\leq\eta\|\overline{Q}-Q_{\mathcal{H}}^{\star}\|_{\infty}\left( \frac{7}{6}\cdot(1+\gamma)+2\gamma\right)+\frac{\eta D(1+\gamma)}{70},\]

holds with probability \(1-\frac{\delta}{2K7}\). Here, the second inequality follows from (131) and (132), The last step in the above relation follows from (135) evaluated at a general value of \(i\) and the prescribed value of \(J\). By a union bound argument, the above relation holds for all \(i\) with probability at least \(1-\frac{\delta}{2K}\).

## Appendix D Numerical Experiments

In this section, we corroborate our theoretical results through simulations. For the simulations, we consider an MDP with \(3\) states and two actions, i.e., \(\mathcal{S}=\{0,1,2\}\) and \(\mathcal{A}=\{0,1\}\). The discount parameter is set to \(\gamma=0.9\). The reward and transition kernel of the MDP is based on the hard instance constructed in Appendix B. Specifically, the reward and transition kernel of state \(0\) is given by the expression in Eqn. (14a). Similarly, the reward and transition kernel corresponding to state \(1\) and \(2\) are identical and given by Eqns. (14b) and (14c) with \(p=0.8\).

We perform three empirical studies. In the first study, we compare the proposed algorithm Fed-DVR-Q to the Fed-SynQ algorithm proposed in Woo et al. (2023). We consider a Federated Q-learning setting with \(5\) agents. The parameters for both the algorithms were set to the suggested values in the respective papers. Both the algorithms were run with \(10^{7}\) samples at each agent. For the communication cost of Fed-SynQ we assume that each real number is expressed using \(32\) bits. In Fig 0(a), we plot the error rate of the algorithm as a function of the number of samples used. In Fig. 0(b) we plot the corresponding communication complexities. As evident from Fig 0(a), Fed-DVR-Q achieves a smaller error than Fed-SynQ under the same sample budget. Similarly, as suggested by Fig. 0(b), Fed-DVR-Q also requires much less communication (measured in terms of the number of bits transmitted) than Fed-SynQ, demonstrating the effectiveness of the proposed approach and corroborating our theoretical results.

In the second study, we examine the effect of the number of agents on the sample and communication complexity of Fed-DVR-Q. We vary the number of agents from \(5\) to \(25\) in multiples of \(5\) and record the sample and communication complexity to achieve an error rate of \(\varepsilon=0.03\). The sample

Figure 1: Comparison between sample and communication complexities of Fed-DVR-Q and the algorithm Fed-SynQ from Woo et al. (2023).

and communication complexities as a function of number of agents are plotted in Figs. 1(a) and 1(b) respectively. The sample complexity decreases as \(1/M\) while the communication complexity is independent of the number of agents. This corroborates the linear speedup phenomenon suggested by our theoretical results and the independence between communication complexity and the number of agents.

In the last study, we compare the communication complexity of Fed-DVR-Q as function of the discount parameter \(\gamma\). We consider the same setup as in the first study and vary the values of \(\gamma\) from \(0.7\) to \(0.9\) in steps of \(0.05\). We run the algorithm to achieve an accuracy of \(\varepsilon=0.1\) with parameter choices prescribed in Sec. 4.1.3. We plot the communication cost of Fed-DVR-Q against the effective horizon, i.e., \(\frac{1}{1-\gamma}\) in Fig. 3. As evident from the figure, the communication scales linearly with the effective horizon, which matches the theoretical claim in Theorem 2.

Figure 3: Communication complexity of Fed-DVR-Q as a function of effective horizon, i.e., \(\frac{1}{1-\gamma}\).

Figure 2: Dependence of sample and communication complexities of Fed-DVR-Q on the number of agents.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract and introduction, we describe that we study the sample-communication complexity trade-off in Federated Q-learning and derive both converse and achievability results. In Sec. 3 we derive the lower bound on communication complexity and in Sec. 4 we outline the algorithm that matches the lower bound derived earlier. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We consider an infinite horizon MDP in the tabular setting and derive the results for the class of intermittent communication algorithms. We acknowledge that these assumptions might be restrictive for a certain class of applications and extension to more general settings is discussed as a future direction in Sec. 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Both Theorem 1 and 2 clearly state all assumptions used in the statement of main result. The proofs for both the theorems can be found in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have a section with numerical experiments in Appendix D. The section contains all relevant details of our implementation to reproduce the results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.

In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper does not have associated code or data. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The relevant details can be found in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The error bars associated with the plots are small and hence we omit them. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The empirical studies require no specific compute resources can be easily completed on a regular laptop. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We have read the NeurIPS Code of Ethics and the paper conforms to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper is concerned with foundational research and is theoretical in nature with no direct societal impact. Guidelines: ** The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper is theoretical is nature and does not involve release of data or code and hence poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use any existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release any new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve any crowdsourcing. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.