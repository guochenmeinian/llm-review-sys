# Can Language Models Perform Robust Reasoning

in Chain-of-thought Prompting with Noisy Rationales?

Zhanke Zhou\({}^{1}\)  Rong Tao\({}^{1}\)  Jianing Zhu\({}^{1}\)  Yiwen Luo\({}^{2}\)  Zengmao Wang\({}^{2}\)  Bo Han\({}^{1}\)

\({}^{1}\)TMLR Group, Hong Kong Baptist University \({}^{2}\)Wuhan University \(\{\)cszzzhou, csrongtao, csjnzhu, bhanmll}@comp.hkbu.edu.hk

{luoyiwen, wangzengmao}@whu.edu.cn

Correspondence to Bo Han (bhanmll@comp.hkbu.edu.hk).

###### Abstract

This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with _noisy rationales_, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We construct NoRa dataset that is tailored to evaluate the robustness of reasoning in the presence of noisy rationales. Our findings on NoRa dataset reveal a prevalent vulnerability to such noise among current LLMs, with existing robust methods like self-correction and self-consistency showing limited efficacy. Notably, compared to prompting with clean rationales, GPT-3.5 drops by \(1.4\%\)-\(19.8\%\) in accuracy with irrelevant thoughts and more drastically by \(2.2\%\)-\(40.4\%\) with inaccurate thoughts.

Addressing this challenge necessitates external supervision that should be accessible in practice. Here, we propose the method of contrastive denoising with noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning capabilities by contrasting noisy rationales with _only one clean rationale_, which can be the minimal requirement for denoising-purpose prompting. This method follows a principle of exploration and exploitation: (1) rephrasing and selecting rationales in the input space to achieve explicit denoising and (2) exploring diverse reasoning paths and voting on answers in the output space. Empirically, CD-CoT demonstrates an average improvement of \(17.8\%\) in accuracy over the base model and shows significantly stronger denoising capabilities than baseline methods. The source code is publicly available at: [https://github.com/tmlr-group/NoisyRationales](https://github.com/tmlr-group/NoisyRationales).

**Question-1 (Q1):** In base-9, what is 86+57?

**Reunion-1 (R1):** In base-9, the digits are

"012345678". We have \(6+7=13\) in base-10. Since we're in base-9, that exceeds the maximum value of 8 for a single digit. 13 mod 9 = 4, so the digit is 4 and the carry is 1. We have \(8+5+1=14\) in base 10. 14 mod 9 = 5, so the digit is 5 and the carry is 1. A leading digit 1. So the answer is 154.

**Answer-1 (A1):** 154.

...Q2, R2, A2, Q3, R3, A3...

**Test Question:** In base-9, what is 62+58?

**Question-1 (Q1):** In base-9, what is 86+57?

**Question-1 (Q1):** In base-9, what is 86+57?

**Rationale-1 (R1):** In base-9, the digits are

"012345678". We have \(6+7=13\) in base-10. Since we're in base-9, that exceeds the maximum value of 8 for a

single digit.13 mod 9 = 4, so the digit is 4 and the carry is 1. We have \(8+5+1=14\) in base 10. 14 mod 9 = 5, so the digit is 5 and the carry is 1. 5 + 9 = 14. A leading digit is

1. So the answer is 154.

**Answer-1 (A1):** 154.

...Q2, R2, A2, Q3, R3, A3...

**Test Question:** In base-9, what is 62+58?

Figure 1: Exemplars of noisy questions [68] and _noisy rationales_ (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.

## 1 Introduction

In-context learning (ICL) is a common approach in large language models (LLMs), enabling models to extrapolate from a few examples and adapt without fine-tuning [4; 84; 16]. However, ICL's efficacy is closely tied to the quality and clarity of the prompting examples, particularly in the prevailing chain-of-thought (CoT) strategy that provides rationales, _i.e._, intermediate reasoning steps to solve a question [85]. Recent research has shown that LLMs struggle with noisy questions: they are easily distracted by irrelevant context and exhibit instability with slight input modifications [68; 78; 107].

Notably, this work shifts focus from the well-studied noisy questions (Noisy-Q) problem to the under-explored _noisy rationales_ (Noisy-R) problem, wherein _factually inaccurate or irrelevant reasoning steps_ are paired with valid question-answer examples, as illustrated in Fig. 1. Here, the emphasis on Noisy-R is due to its practical challenges, with examples drawn from diverse sources such as crowd-sourced platforms, dialogue systems, and machine-generated data 2[25; 45; 73; 2; 77; 48]. However, the robustness of LLMs against Noisy-R remains unknown. A new benchmarking dataset is needed to conduct a systematic evaluation of current LLMs and verify the corresponding countermeasures.

Footnote 2: The sources of noisy rationales are discussed in Appendix C with extensive real-world examples.

In this work, we first construct the NoRa (**No**isy **R**ationales) dataset, a comprehensive testbed to evaluate the robustness of LLM reasoning against noisy rationales across various reasoning domains (in Sec. 3). The NoRa contains a total of \(26391\) questions, covering three types of reasoning tasks: mathematical, symbolic, and commonsense. We uniformly formalize the generation of noisy rationales by inserting irrelevant or inaccurate thoughts, controlling reasoning difficulty through noise ratios, and guaranteeing the overall prompting correctness without modifying the question or answer.

With the NoRa dataset, we evaluate several LLMs and reveal that all of them are _intrinsically vulnerable_ to noisy rationales (in Sec. 4). For example, compared to prompting with clean rationales, GPT-3.5 exhibits an average \(3.0\%\) - \(33.3\%\) decrease in accuracy with noisy rationales, as in Fig. 2. Besides, only limited improvements are achieved with existing robust methods based on the model's intrinsic denoising ability, _e.g._, self-consistency [83] and self-denoise [102]. We show that Noisy-R is much more challenging than Noisy-Q, requiring context-specific knowledge to guide the denoising.

To solve this, we propose to rectify the rationales with _only one clean CoT demonstration_ that can be the most attainable supervision in practice (in Sec. 5). We assume that LLMs can rectify rationales by _contrasting_ a noisy rationale with a clean one, as in Fig. 3. Guided by this principle, we design the framework of **C**ontrastive **D**enoising with noisy **CoT** (CD-CoT) with four steps: rationale rephrasing, rationale selecting, rationale exploring, and answer voting. Technically, the first two steps aim to achieve explicit denoising, while the last two steps are for diverse reasoning paths. Empirically, CD-CoT achieves an average improvement of \(17.8\%\) in accuracy _w.r.t._ the base model (refer to Tab. 8). Notably, it presents much stronger denoising power than baselines in rectifying the rationales.

**Contributions.** To our best knowledge, we are the _first_ to investigate the problem of noisy rationales.

* We formalize the under-explored noisy rationale problem in the prevailing chain-of-thought prompting and construct the NoRa dataset to benchmark the robustness of LLMs against noisy rationales (Sec. 3).
* We systematically evaluate LLMs with NoRa dataset and extract several insightful observations, _e.g._, the unsatisfactory robustness and limited denoising power of LLMs under noisy rationales (Sec. 4).
* We propose to rectify the noisy rationales with only one clean CoT demonstration, design a simple yet effective method, CD-CoT, and verify its effectiveness through comprehensive experiments (Sec. 5).

Figure 3: Chain modeling of the noisy rationale problem: Recovering chain (3) from chain (1) with the guidance of chain (2). From question \(x_{i}\) to answer \(y_{i}\), the rationale of chain (3) includes clean thoughts \(T_{i}^{(j)}\) and noisy thoughts \(\hat{T}_{i}^{(j)}\).

Figure 2: Results of GPT-3.5 with 0-shot, 3-shot clean rationales, and 3-shot noisy rationales: Both inaccurate and irrelevant rationales degenerate performance significantly, while the proposed CD-CoT improves robustness against noisy rationales.

Related Work

**Limitations of in-context learning (ICL).** Though effective, ICL suffers from the susceptibility to manual generation and selection of demonstrations (examples), where the ultimate performance is closely tied to the demonstrations' quality and clarity. Recent investigations on _noisy questions_ have shown that (i) LLMs can be distracted by irrelevant or adversarial context, as they are designed to pay close attention to the context provided in the prompt [32; 58; 68; 78] and (ii) LLM reasoning is unstable, namely, small modifications to the prompt could potentially cause large variations in the model's output [102; 107]. Besides, another line of research regarding _noisy answers_[42; 18] justifies the feasibility of misleading an LLM to agree factual errors such as "1+1=3" in base-10 calculation.

**Countermeasures.** Two intrinsic traits of LLMs are desirable for addressing the above limitations:

* _Self-correction_, wherein LLMs attempt to correct their initial responses based solely on their inherent capabilities without external feedback, _e.g._, by refining prompts through iterative corrections of responses or question trajectories [91; 89]. Although LLMs can learn to ignore irrelevant information by examples or instructions [68], they are proved to be still struggling to correct their responses without external feedback, and at times, their performance might even degrade after self-correction [29; 81].
* _Self-consistency_ aims to obtain a consistent answer against input perturbations. This is achieved by generating multiple responses via randomized smoothing on input questions [98] or diverse paths for answering one question [83] followed by the answer aggregation. This strategy brings improvements with extra costs for repeated reasoning. Moreover, it cannot explicitly rectify questions or rationales.

**Noisy rationales**, as the research focus of this work, mainly originates from (1) the inherent imperfections, inconsistencies, and inaccuracies of humans' cognitive processes [53; 10] and (2) the diversity, unpredictability, and hallucination of the LLMs' generative mechanisms [103; 30; 101]. A detailed literature review and discussion of noise rationales are in Appendix B and C, respectively.

## 3 The NoRa Dataset

In this section, we introduce the NoRa (**N**oisy **R**ationales) dataset for benchmarking the robustness against noisy rationales. NoRa consists of \(26391\) questions and \(5\) subsets, covering mathematical, symbolic, and commonsense reasoning tasks, where ICL and CoT demonstrations play a crucial role.

### Definition of Noisy Rationales

We start by formalizing the ICL and CoT demonstrations. Given a test question \(x_{\text{test}}\) and an LLM \(f_{\theta}\), one expects to get the correct answer \(y_{\text{test}}\) as \(f_{\theta}(x_{\text{test}})\!\mapsto\!y_{\text{test}}\). This zero-shot manner cannot guarantee effectiveness, especially when encountering unfamiliar contexts or scenarios. To boost effectiveness, the ICL techniques prompt the LLM with a few examples \(S_{n}\!=\!\{(x_{i},y_{i})\}_{i=1}^{n}\) collected in the current context, each composed of a question \(x_{i}\) and answer \(y_{i}\), and then construct the new input \(x_{\text{ICL}}\) as

\[x_{\text{ICL}}=\left[S_{n},x_{\text{test}}\right]=\left[x_{1},y_{1},\ldots,x_{ n},y_{n},\ x_{\text{test}}\right]. \tag{1}\]

The guidance by \(S_{n}\) makes \(f_{\theta}(x_{\text{ICL}})\!\mapsto\!y_{\text{test}}\) much easier than \(f_{\theta}(x_{\text{test}})\!\mapsto\!y_{\text{test}}\). Then, the CoT further refines \(x_{\text{ICL}}\) by constructing the step-by-step rationale \(\mathcal{T}_{i}\), consisting of several thoughts \(T_{i}^{(j)}\), namely,

\[x_{\text{CoT}}=\left[x_{1},\mathcal{T}_{1},y_{1},\ldots,x_{n},\mathcal{T}_{n}, y_{n},\ x_{\text{test}}\right],\ \ \text{where}\ \ \mathcal{T}_{i}=\left[T_{i}^{(1)},T_{i}^{(2)},T_{i}^{(3)},\ldots,T_{i}^{(k)}\right]. \tag{2}\]

However, as aforementioned, the thoughts in CoT (Eqn. 2) can be noisy in practice. This noise can be attributed to (1) _irrelevant thoughts_, which are irrelevant but correct, or (2) _inaccurate thoughts_, which are relevant but factually wrong. Here, we _uniformly_ formalize these two kinds of noise as

\[\hat{\mathcal{T}_{i}}=\left[T_{i}^{(1)},\hat{T}_{i}^{(1)},T_{i}^{(2)},\hat{T}_{ i}^{(2)},\ldots,T_{i}^{(k)},\hat{T}_{i}^{(k)}\right], \tag{3}\]

where \(\hat{T}_{i}^{(j)}\) represents a noisy thought (irrelevant or inaccurate) that is coherent with the previous clean thought \(T_{i}^{(j)}\) (relevant and correct). The following introduces the definition of noisy thoughts.

**Irrelevant thoughts** refer to incorporating irrelevant information unhelpful for solving the question, _e.g._, discussing the genetic overlap of siblings when the task is to deduce family roles in relationship reasoning. Redundant information may be introduced by the LLM's diverse response generation or 

[MISSING_PAGE_FAIL:4]

## 4 Evaluating Language Models on NoRa dataset

In this section, we comprehensively evaluate representative LLMs and robust methods on the newly constructed NoRa dataset. We first introduce the basic evaluation setups and then present several observations on the _unsatisfactory robustness_ of current LLMs and methods under noisy rationales.

**Baseline methods.** We select five representative methods as baselines to ensure a comprehensive assessment that encompasses the two traits of self-correction and self-consistency. ISC [29] and SP [89] exemplify self-correction, focusing on response rectification and prompt rephrasing, respectively. SM [62], SD [102], and SC [83] fall under self-consistency: SM [62] injects perturbations into prompts for robustness, SD [102] masks prompts and asks LLMs to reconstruct them, while SC directly samples outputs without denoising. These methods are further introduced in Appendix E.1.

**LLM basis.** We employ GPT-3.5-turbo-0613 [17] as our base LLM (denoted as \(\overline{\text{Base}}\)) for the analyses presented in this study. In addition, we conduct evaluations on three supplementary models, including Gemini-Pro (Jan. 2024) [76], Llama2-70B [79], and Mixtral-8x7B [33]. For all baselines, we consistently set the temperature parameter \(\tau\) to the value of 1. In order to obtain consistent results, we evaluate 300 questions for each task and repeat the model reasoning five times for each question.

**Evaluation metric.** Given a set of test question \(\mathcal{Q}=\{(x_{\text{test}},y_{\text{test}})\}\) and a set of CoT-prompting examples \(\mathcal{P}=[x_{1},\mathcal{T}_{1},y_{1},\ldots,x_{n},\mathcal{T}_{n},y_{n}]\), we define the accuracy of the denoising method \(\mathcal{M}\) with a specific LLM \(f_{\theta}\), namely, \(\operatorname{Acc}(\mathcal{M},\mathcal{Q},\mathcal{P})=\Sigma_{(x_{\text{test,base}})\in\mathcal{Q}}\nicefrac{{1}}{{2}}\nicefrac{{\left[\mathcal{M}( \mathcal{P},x_{\text{test}})\right]}}{{\left[\mathcal{Q}\right]}}\). We report the results in percentage (%) with one decimal point. Therein, \(\operatorname{Acc}(\mathcal{M},\mathcal{Q},\mathcal{P}_{\text{clean}})\), \(\operatorname{Acc}(\mathcal{M},\mathcal{Q},\mathcal{P}_{\text{irrelevant}})\), and \(\operatorname{Acc}(\mathcal{M},\mathcal{Q},\mathcal{P}_{\text{inaccurate}})\) indicate accuracy with clean, irrelevant, and inaccurate rationales, respectively. When there is no prompting example, _i.e._, \(\mathcal{P}=\emptyset\), then \(\operatorname{Acc}(\mathcal{M},\mathcal{Q},\emptyset)\) represents the zero-shot result.

**Unreliability revealing with noisy rationales.** We conduct the reasoning tasks on LLM with Noisy-R and summarize the results in Tab. 3. _Overall, the base LLM with all the existing reasoning methods is severely affected by irrelevant or inaccurate noise, with overall showing a \(0.2\%\)-\(25.3\%\) decrease with irrelevant noise and a more drastic \(0.1\%\)-\(54.0\%\) decrease with inaccurate noise compared with clean rationales._ While robust methods like SP and SD exhibit resilience to noise on partial tasks, their performance remains inconsistent and often declines. To further reveal the unreliability, we start by analyzing the two categories of robust methods mentioned above in the following observations.

\begin{table}
\begin{tabular}{c|c|c|c c c c|c c c c} \hline \hline \multirow{2}{*}{Task} & \multirow{2}{*}{Method \(\mathcal{M}\)} & \multirow{2}{*}{Acc\((\mathcal{M},\mathcal{Q},\mathcal{P}_{\text{clean}})\)} & \multicolumn{3}{c|}{\(\operatorname{Acc}(\mathcal{M},\mathcal{Q},\mathcal{P}_{\text{clean}})\)} & \multicolumn{3}{c}{\(\operatorname{Acc}(\mathcal{M},\mathcal{Q},\mathcal{P}_{\text{inaccurate}})\)} \\  & & & Easy & Medium & Hard & Avg. & Easy & Medium & Hard & Avg. \\ \hline \multirow{8}{*}{Mask} & Base & 46.4 & 39.3 & 30.3 & 26.6 & 32.1 & 28.2 & 10.1 & 6.0 & 13.1 \\  & w/ ISC [29] & 24.3 & 17.7 & 14.7 & 12.7 & 15.0 & 18.4 & 13.7 & 12.3 & 14.8 \\  & w/ SP [89] & 26.2 & 25.5 & 25.5 & 21.9 & 24.3 & 20.0 & 18.4 & **14.3** & 17.6 \\ Base-9 & w/ SM [62] & 37.4 & 30.0 & 22.7 & 16.5 & 23.1 & 24.7 & **19.2** & 12.4 & **18.8** \\  & w/ SD [102] & 47.9 & 37.2 & 25.4 & 24.7 & 29.1 & 29.3 & 12.5 & 8.7 & 16.8 \\  & w/ SC [83] & **61.5** & **51.1** & **39.0** & **36.2** & **42.1** & **32.7** & 15.3 & 7.5 & 18.5 \\ \hline \multirow{8}{*}{Mask} & Base & 23.9 & 19.1 & 13.6 & 10.7 & 14.5 & 14.0 & 6.7 & 3.6 & 8.1 \\  & w/ ISC [29] & 11.2 & 8.3 & 7.8 & 6.0 & 7.4 & 6.5 & 5.2 & 4.7 & 5.5 \\  & w/ SP [89] & 20.7 & 17.5 & 16.7 & 14.0 & 16.0 & 14.1 & **10.7** & **10.8** & **11.9** \\ Base-11 & w/ SM [62] & 16.3 & 12.0 & 6.0 & 5.7 & 7.9 & 12.0 & 9.3 & 7.7 & 9.7 \\  & w/ SD [102] & 17.9 & 12.3 & 12.0 & 13.3 & 12.5 & 17.0 & 8.7 & 5.3 & 10.3 \\  & w/ SC [83] & **33.7** & **25.3** & 16.3 & **15.0** & **18.9** & **19.7** & 9.3 & 3.3 & 10.8 \\ \hline \multirow{8}{*}{Symbolic} & Base & 32.7 & 28.1 & 25.1 & 23.0 & 25.4 & 29.1 & 26.1 & 22.7 & 26.0 \\  & w/ ISC [29] & 23.9 & 20.0 & 16.3 & 15.5 & 17.3 & 19.2 & 18.3 & 18.1 & 18.5 \\  & w/ SP [89] & 23.2 & 23.0 & 22.6 & 22.7 & 22.8 & 23.7 & 22.5 & 23.2 \\  & w/ SM [62] & 25.0 & 20.7 & 19.7 & 16.7 & 19.0 & 21.0 & 20.3 & 20.0 & 20.4 \\  & w/ SD [102] & 9.9 & 10.1 & 10.9 & 10.3 & 10.4 & 10.1 & 10.9 & 10.4 & 10.5 \\  & w/ SC [83] & **35.3** & **31.0** & **28.3** & **27.0** & **28.8** & **33.3** & **30.7** & **26.0** & **30.0** \\ \hline \multirow{8}{*}{Symbolic} & Base & 9.2 & 6.3 & 7.2 & 6.0 & 6.5 & 7.0 & 6.8 & 6.0 & 6.6 \\  & w/ ISC [29] & 4.9 & 4.6 & 2.7 & 3.7 & 3.7 & 3.4 & 4.3 & 3.5 & 3.7 \\ \cline{1-1}  & w/ SP [89] & 5.1 & 4.3 & 4.1 & 3.9 & 4.1 & 4.9 & 4.0 & 4.5 & 4.5 \\ \cline{1-1}  & w/ SM [62] & 1.7 & 0.7 & 0.7 & 1.3 & 1.0 & 1.3 & 0.7 & 0.3 & 0.8 \\ \cline{1-1}  & w/ SD [102] & 0.1 & 0.1 & 0.1 & 0.2 & 0.1 & 0.1 & 0.3 & 0.0 & 0.1 \\ \cline{1-1}  & w/ SC [83] & **13.0** & **7.7** & **9.0** & **6.3** & **7.7** & **8.0** & **8.0** & **8.7** & **8.2** \\ \hline \multirow{8}{*}{Commonsense} & Base & 45.7 & 44.3 & 42.3 & 41.4 & 42.7 & 36.7 & 33.4 & 28.3 & 23.8 \\  & w/ ISC [29] & 21.8 & 24.3 & 22.5 & 21.4 & 22.7 & 20.3 & 26.5 & 24.0 & 24.6 \\ \cline{1-1}  & w/ SP [89] & 47.9 & 48.2 & 46.7 & 48.1 & 47.7 & 49.6 & 46.6 & 45.5 & 47.6 \\ \cline{1-1}  & w/ SM [62] & 53.3 & 50.3 & 50.0 & 36.7 & 49.0 & 47.7 & 49.0 & 49.3 & **48.7** \\ \cline{1-1}  & w/ SD [102] & **54.0** & **58.3** & **57.3** & **57.7** & **57.8** & **57.0** & **58.3** & **53.7** & **56.3** \\ \cline{1-1}  & w/ SC [83] & 52.0 & 46.3 & 45.0 & 44.7 & 45.3 & 44.7 & 44.7 & 38.0 & 42.5 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The **boldface** numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of \(\overline{\text{Base}}\) are highlighted in gray.

_Observation 4.1_.: **Self-correction methods perform poorly on most tasks with noisy rationales.** Therein, ISC [29] and SP [89] rely on the inherent capabilities of LLMs to enhance the quality of generated responses. However, in the absence of external feedback, the model's self-correction ability in reasoning tasks is limited, often resulting in the miscorrection of the given content (see Tab. 12). SP can only slightly improve the accuracy of commonsense tasks, while ISC performs unsatisfactorily across all tasks. As can be seen from Tab. 3, these methods perform even worse than the base model.

_Observation 4.2_.: **Self-consistency methods can improve robustness without true denoising.** Two self-consistency approaches, SM [62] and SD [102], are originally proposed to address Noisy-Q issues. When applied to our Noisy-R scenarios, they tend to easily disrupt the intrinsic logical coherence within the thought chain. Although these methods utilizing smooth strategies (_e.g._, random smoothing or masking) perform well on the commonsense dataset, they can hardly handle the more difficult reasoning tasks and even degenerate close to 0%, _e.g._, in the Symbolic Longer task. Another method, SC [83], performs better than the base model in all tasks, improving both clean and noisy reasoning performance. However, SC does not conduct explicit denoising on rationales during its reasoning procedure. In addition, SC also requires a high computation cost (refer to Appendix F.2).

Besides these methods, next, we analyze LLMs' _intrinsic_ properties under noisy rationales as follows.

_Observation 4.3_.: **Adjusting model temperature can help reasoning under noisy rationales.** In Tab. 4, we evaluate the base LLM using different temperatures on 3-shot demonstrations. Overall, reducing temperature can enhance the model's accuracy under both noisy and clean rationale reasoning, compared to the default temperature of 1. However, the relationship between temperature and accuracy is not linear for noisy reasoning; instead, there are multiple peaks in accuracy within the temperature range of \(0\) to \(1\). Additionally, it is found that excessively low temperatures (_e.g._, 0) tend to result in verbose and repeated responses, which cause the model to exceed token limits up to \(30\%\) in symbolic tasks where the length of expected answers is quite variable among different questions.

_Observation 4.4_.: **Prompting with more noisy examples boosts reasoning accuracy on most tasks.** In Tab. 5, we evaluate the model using different numbers of exemplars while keeping the temperature at 1. In general, the LLM's accuracy will still improve as the number of noisy examples increases in the clean and noisy settings. However, it should be noted that in tasks with high-level noise from NoRa-Math, increasing prompting examples can degenerate accuracy. For example, in the base-9 inaccurate-hard task, prompting with noisy rationales is even worse than the 0-shot accuracy of \(7.2\%\). Further, we provide a deeper analysis of increasing the number of noisy examples in Appendix F.6.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline Task & Setting & \multicolumn{3}{c|}{Temperature} & \multicolumn{1}{c}{Setting} \\  & & 0 & 0.3 & 0.5 & 0.7 & 1 \\ \hline \multirow{4}{*}{Base-9} & clean & **61.0** & 69.0 & 57.5 & 55.3 & 46.4 \\  & ina. easy & 78.0 & 78.0 & 69.7 & 55.3 & 46.4 \\  & ina. hard & 5.0 & 78.0 & 69.7 & 55.3 & 46.4 \\ \hline \multirow{4}{*}{Base-11} & clean & **34.9** & 33.8 & 31.6 & 29.8 & 23.9 \\  & inr. early & 21.7 & 23.1 & 23.3 & 23.3 & 24.9 \\  & inr. hard & 21.0 & **17.5** & 15.5 & 14.1 & 10.7 \\ \hline \multirow{4}{*}{Sym.(L)} & clean & 34.2 & **35.8** & 35.7 & 34.6 & 32.7 \\  & inr. easy & 26.8 & 34.5 & 29.2 & 29.1 & 28.1 \\  & inr. hard & 27.0 & 26.1 & 22.4 & 20.4 & 22.7 \\ \hline \multirow{4}{*}{Sym.(L)} & clean & 6.3 & 8.3 & 8.9 & **9.3** \\  & ina. easy & 5.0 & 7.3 & **8.6** & 8.7 & 7.0 \\  & inr. hard & 4.0 & 6.1 & **6.3** & 8.2 & 60.0 \\ \hline \multirow{4}{*}{Sym.(L)} & clean & **34.9** & 33.8 & 31.6 & 29.8 \\  & inr. early & 21.7 & 13.5 & 29.1 & 27.6 \\ \cline{1-1}  & inr. hard & 21.0 & **17.5** & 15.5 & 14.1 & 10.7 \\ \hline \multirow{4}{*}{Sym.(L)} & clean & 34.2 & **35.8** & 35.7 & 34.6 & 32.7 \\ \cline{1-1}  & inr. easy & 26.8 & 34.5 & 29.2 & 29.1 & 28.1 \\ \cline{1-1}  & inr. hard & 27.0 & 26.1 & 22.4 & 20.4 & 22.7 \\ \hline \multirow{4}{*}{Sym.(L)} & clean & 2.7 & 7.9 & 31.3 & **12.2** \\ \cline{1-1}  & inr. easy & 2.3 & 5.4 & 7.0 & 8.3 & 8.9 \\ \cline{1-1}  & inr. hard & 1.9 & 4.0 & **6.3** & **6.3** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparing performances of the base model with different temperatures. a varying number of examples Sym.(E)/(L) are symbolic tasks. (“—” denotes over token limit).

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline Task & Zero-shot & Few-shot (No Shuffle) & Shuffle Questions \(x_{i}\) & Shuffle Rationales \(\mathcal{T}_{i}\) & Shuffle Answers \(y_{i}\) \\ \hline Math Base-9 & 7.2 & **46.4** & 45.5 (0.9\%\(\downarrow\)) & 34.5 (11.9\%\(\downarrow\)) & 35.7 (10.7\%\(\downarrow\)) \\ \hline Math Base-11 & 5.5 & **—** & 23.9 & **1** & **24.8** (0.9\%\(\uparrow\)) & 21.6 (2.3\%\(\downarrow\)) & 21.1 (11.7\%\(\downarrow\)) \\ \hline Symbolic Equal & 8.8 & **—** & 32.7 & **32.7** (0.0\%\(\downarrow\)) & **32.8** (0.1\%\(\uparrow\)) & 32.3 (0.4\%\(\downarrow\)) \\ \hline Symbolic Longer & 0.0 & **—** & **9.2** & **1** & **7.0** (2.2\%\(\downarrow\)) & 6.2 (3.0\%\(\downarrow\)) & 6.3 (2.9\%\(\downarrow\)) \\ \hline Commonsense & 40.0 & **—** & **45.7** & **1** & **38.7** (0.7\%\(\downarrow\)) & 39.7 (6.0\%\(\downarrow\)) & 39.8 (5.9\%\(\downarrow\)) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Performance (in accuracy%) on NoRa dataset under different few-shot shuffle configurations.

**Observation 4.5**.: **Different LLMs are generally vulnerable to noisy rationales.** In Tab. 6, we evaluate different LLMs across three settings: 0-shot CoT, 3-shot clean rationales, and 3-shot medium-level noisy rationales. Notably, Gemini-Pro outperforms GPT-3.5 in overall performance. However, it demonstrates a similar degree of sensitivity to noise, with a \(2.4\%\)-\(15.7\%\) performance decline with irrelevant rationales and a \(7.8\%\)-\(66.8\%\) decline with inaccurate rationales compared to clean rationales. While Mixtral 8x7B shows a slight underperformance compared to GPT-3.5, it also manifests a vulnerability to noise, incurring a \(1.4\%\)-\(11.2\%\) loss with irrelevant rationales and a greater \(4.2\%\)-\(23.8\%\) loss with inaccurate rationales. By contrast, Llama2-70B performs suboptimally, with a \(0.4\%\)-\(2.0\%\) drop for irrelevant thoughts and a larger \(1.0\%\)-\(2.2\%\) drop for inaccurate thoughts.

**Further investigation.** Inspired by Min et al. [54], we further explore the mapping among questions, rationales, and answers through shuffling experiments. Specifically, given the 3-shot prompting examples \(\{(x_{1},\mathcal{T}_{1},y_{1}),(x_{2},\mathcal{T}_{2},y_{2}),(x_{3},\mathcal{ T}_{3},y_{3})\}\), we test three configurations, _i.e._, shuffle questions \(\{(x_{1},\mathcal{T}_{3},y_{3}),(x_{2},\mathcal{T}_{1},y_{1}),(x_{3},\mathcal{ T}_{2},y_{2})\}\), shuffle answers \(\{(x_{1},\mathcal{T}_{1},y_{3}),(x_{2},\mathcal{T}_{2},y_{1}),(x_{3},\mathcal{ T}_{3},y_{2})\}\), and shuffle rationales \(\{(x_{1},\mathcal{T}_{3},y_{1}),(x_{2},\mathcal{T}_{1},y_{2}),(x_{3},\mathcal{ T}_{2},y_{3})\}\). These break the original mappings. The results under these configurations are shown in Tab. 7, which induces the following observation.

**Observation 4.6**.: **Shuffling the mappings of prompting examples degenerates the reasoning but still performs better than without prompting.** This means that while LLMs may not heavily rely on the exact mapping (of question, rationale, and answer), they still benefit from demonstrating information even with shuffling. Notably, this finding is consistent with the conclusions of [54] that LLMs learn more abstract task information from the demonstrations rather than simply memorizing question-answer pairs. _More importantly, LLMs are less vulnerable to shuffled mappings than noisy rationales_. Unlike shuffling, the irrelevant or inaccurate information in noisy rationales introduces misleading elements that significantly interfere with the model's ability to learn _correct_ task patterns, thereby resulting in more severe performance degradation. This extends [54]'s finding and shows that the quality of reasoning steps can be more crucial than the exact mapping of prompting examples.

## 5 Method

This section aims to enable LLMs to discern and remove noisy thoughts. The observations in Sec. 4 and previous works show that current LLMs cannot achieve this with their intrinsic denoising ability, even enhanced with self-denoising methods. Therefore, we would claim that the _external supervision_ is necessary for enhancement, which should be sufficient for denoising and accessible in practice. Existing methods with external supervision [29, 81, 9] require (1) oracle feedback on the test question, (2) human feedback of errors on specific tokens or positions, or (3) expert knowledge to construct detailed descriptions of specific tasks. By contrast, we believe that _a clean CoT demonstration_ is more attainable and practical, which can be the _minimal requirement_ for denoising-purpose prompting.

Therein, we assume that LLMs _can_ identify noisy thoughts by _contrasting_ a pair of noisy and clean rationales and discerning their differences, similar to contrastive learning [26, 6, 35]. Here, the denoising power could come from the abilities of the instruction following and step-by-step reasoning [84, 66]. Hence, we propose the framework of CD-CoT, Contrastive **D**enoising with noisy **CoT**. The design principle is to explore and then exploit, _i.e._, (1) rephrasing and selecting rationales in input space to achieve explicit denoising, and then (2) exploring diverse rationales and voting answers in output space for deriving the final answer, as in Figs. 4 & 5. The details are as follows.

Figure 4: CD-CoT’s first two steps for data denoising. First, it rephrases the \(i\)-th noisy example by contrasting it with the clean example. Then, with the obtained \(N\) rephrased examples, it selects the \(M\) qualified candidates by checking the validity of the rephrased answers \(\hat{y}_{i1},\dots,\hat{y}_{iN}\)_w.r.t._\(y_{i}\).

Figure 5: CD-CoT constructs \(M\) inputs (\(K\)-shot) by allocating the \(K\cdot M\) rephrased rationales. These inputs are concatenated with the clean example and test question and then fed to an LLM for reasoning separately. The obtained \(D\) answers are equally voted to obtain the final answer \(y\).

```
1:an LLM \(f_{\theta}\), the prompt of contrastive denoising \(\mathcal{P}_{\text{denoise}}\), one test question \(x_{\text{test}}\), one clean example \((x_{\text{C}},\mathcal{T}_{\text{C}},y_{\text{C}})\), K prompting examples \(S_{n}=\{(x_{i},\mathcal{T}_{i},y_{i})\}_{i=1}^{K}\), hyper-parameters \(N,M\), and reasoning budget \(\{B_{i}\}_{i=1}^{M}\) (satisfies that \(\Sigma_{i=1}^{M}B_{i}=D\), where \(D\) is the total budget).
2:for\(i=1\dots K\)do
3: initialize the set of rephrased results of \(i\)-th example \(\mathcal{R}_{i}\leftarrow\emptyset\).
4:for\(j=1\dots N\)do
5:\(\#\) Step-1: Rationale Rephrasing via Supervised Contrasting
6: obtain a rephrased example as \((x_{i},\hat{\mathcal{T}}_{i},\hat{y}_{i})\gets f_{\theta}\Big{(}\mathcal{ P}_{\text{denoise}}(x_{\text{C}},\mathcal{T}_{\text{C}},y_{\text{C}},x_{i}, \mathcal{T}_{i},y_{i})\Big{)}\).
7:if match answer \(\hat{y}_{i}=y_{i}\), then store the rephrased example as \(\mathcal{R}_{i}\leftarrow\mathcal{R}_{i}\cup\{(x_{i},\hat{\mathcal{T}}_{i}, \hat{y}_{i})\}\).
8:endfor
9: randomly select \(M\) rephrased examples from \(\mathcal{R}_{i}\) and obtain \(\tilde{\mathcal{R}}_{i}=\{(x_{is},\hat{\mathcal{T}}_{is},\hat{y}_{is})\}_{s=1} ^{M}\).
10:endfor
11:\(\#\) Step-3: Rationale Exploration
12: initialize the set of answers \(\mathcal{Y}\leftarrow\emptyset\).
13:for\(i=1\dots M\)do
14: construct an input \(\mathcal{P}_{i}\leftarrow\{(x_{ji},\hat{\mathcal{T}}_{ji},\hat{y}_{ji})\}_{j=1} ^{K}\), where \((x_{ji},\hat{\mathcal{T}}_{ji},\hat{y}_{ji})\) is the \(i\)-th element of \(\hat{\mathcal{R}}_{j}\).
15: concatenate \(\mathcal{P}_{i}\) with the clean example and test question as \(\mathcal{P}_{i}\leftarrow\mathcal{P}_{i}\cup\{(x_{\text{C}},\mathcal{T}_{\text {C}},y_{\text{C}}),x_{\text{test}}\}\).
16:for\(j=1\dots B_{M}\)do
17: get one answer by LLM reasoning as \(y_{j}\gets f_{\theta}(\mathcal{P}_{i})\).
18: store the answer as \(\mathcal{Y}\leftarrow\mathcal{Y}\cup\{y_{j}\}\).
19:endfor
20:endfor
21:\(\#\) Step-4: Answer Voting
22: initialize the dictionary of answer count \(\mathcal{C}\) that \(\forall y_{j}\in\mathcal{Y},\mathcal{C}[y_{j}]=0\).
23:for\(j=1\dots D\)do
24: update \(\mathcal{C}[y_{j}]\leftarrow(\mathcal{C}[y_{j}]+1)\).
25:endfor
26: get the final answer \(y\) with maximum counts as \(y\leftarrow\arg\max_{y}\mathcal{C}[y]\).
27:return the answer \(y\).
```

**Algorithm 1** CD-CoT: Contrastive Denoising with Noisy Chain-of-Thought.

### Implementation

**Step-1: Rephrasing via Supervised Contrasting (\(1\) to \(N\)).** First, we establish a general prompt of contrastive rephrasing to construct a pair of contrastive examples, as shown in the template below. This steers the model towards learning from the clean example and then rephrasing and rectifying the noisy examples. To be specific, given one clean example and \(K\) noisy examples, we generate \(N\) rephrased rationales for each noisy example independently and obtain \(K\cdot N\) rephrased rationales.

```
1:\(\#\) Step-1: Rationale Rephrasing via Supervised Contrasting (\(N\) to \(N\)).
2:for\(j=1\dots D\)do
3: update \(\mathcal{C}[y_{j}]\leftarrow(\mathcal{C}[y_{j}]+1)\).
4:endfor
5:get the final answer \(y\) with maximum counts as \(y\leftarrow\arg\max_{y}\mathcal{C}[y]\).
6:return the answer \(y\).
```

**Algorithm 2** T-Convergence of Contrastive Rephrasing via Supervised Contrasting

**Step-2: Rationale Selection (\(N\) to \(M\), \(N\geq M\)).** Next, we employ answer matching to select those rephrased examples with unchanged answers, leaving behind a refined candidate pool. Subsequently, we randomly select \(M\) rephrased rationales from the pool and concatenate them to form the contexts.

**Step-3: Rationale Exploration (\(M\) to \(D\), \(M\leq D\)).** For the \(M\) different contexts, we explore rationales by repeated reasoning with the budget of \(D\) reasoning repetitions. Notably, a higher temperature parameter, _e.g._, 1, is set to introduce more randomness in generating diverse rationales.

**Step-4: Answer Voting (\(D\) to \(1\)).** Ultimately, all the \(D\) answers are equally voted into a final answer.

**Instantiation.** By tuning the hyper-parameters \(N\), \(M\), and \(D\), we balance exploration and exploitation in the input and output space. The overall procedure of our proposed CD-CoT is presented in Algorithm 1. Besides, we further explain the details of each step of this algorithm in Appendix E.2.

**Theoretical analysis.** To understand the underlying mechanism of CD-CoT, we also conduct the theoretical analysis based on the distinguishability [90] of in-context learning. The full analysis is in Appendix D, where we find that the noisy demonstration in ICL can decrease the distinguishability of in-context matching with the clean-prompt distribution, while our method can mitigate this issue. Besides, we build a self-supervised variant of CD-CoT and empirically evaluate it in Appendix F.7.

### Empirical Study

In this part, we empirically verify the effectiveness of CD-CoT and start by introducing the baselines.

**Baseline methods.** We employ three methods that require _additional information_: (1) Self-Correction with Oracle Feedback (SCO) [29] utilizes the _ground truth answers_ of test questions to determine when to terminate the self-correction loop; (2) Backtracking (BT) [81] guides self-correction by providing the model with the _position_ of the first noisy thought; (3) Contrastive Chain-of-Thought (CC) [9] conducts direct reasoning with all the noisy or _clean examples_ without implicit or explicit denoising.

**Main results.** As in Tab. 8, CD-CoT demonstrates a significant performance improvement across all datasets, with an average improvement of \(17.8\%\) compared with the base model under noisy settings. Notably, on Math-Base-9, Math-Base-11, and Symbolic-Equal, CD-CoT surpasses all baseline methods by a significant margin. On Symbolic-Longer and Commonsense, CD-CoT only slightly lags behind SCO. However, SCO requires the ground truth answer to the test question, which should be unknown in practice, as pointed out in [29]. In comparison, CD-CoT only necessitates an additional clean demonstration, making it much more practical to apply across realistic scenarios. _Notably, CD-CoT outperforms SCO in \(20\) out of \(30\) settings and surpasses BT, CC in all \(30\) settings_.

Besides, CD-CoT displays _remarkable resistance_ to the magnitude of noise. Therein, CD-CoT demonstrates enhanced resilience against inaccurate noise on mathematical tasks, which are quite challenging. For instance, on Math Base-9 with inaccurate rationales, the average accuracies of SCO and BT decline significantly by \(28.8\%\) and \(26.3\%\) compared to the accuracies with clean rationales. In contrast, CD-CoT exhibits a more modest decline of \(7.0\%\). An ablation study of components in Appendix F.3 demonstrates the denoising power and performance gain of CD-CoT, attributed to its contrastive denoising with rationale rephrasing as well as repeated reasoning with voting components.

**Ablation study of varying hyper-parameters.** By manipulating the values of \(N,\,M,\,D,\,\text{and}\,C\), we generate diverse algorithm instances. Here, \(D\) denotes the reasoning times allocated to the \(M\) inputs, while \(C\) signifies whether the clean example is used in step 3. As demonstrated in Tab. 9, the clean example utilized by CD-CoT during the reasoning process plays a pivotal role. The omission of this clean example results in an average decrease of \(3.3\%\) and \(4.5\%\) in accuracy under irrelevant noise and inaccurate noise, respectively. Besides, the accuracy exhibits subtle variations when employing different algorithm instances, with the highest average accuracy observed at \(51.3\%\) and the lowest average accuracy at \(49.3\%\). Further, Tab. 10 presents the average number of tokens used in reasoning. We set \(M\!=\!2\) to strike a balance. Please refer to Appendix E.3 for detailed hyper-parameter selection.

\begin{table}
\begin{tabular}{c|c|c|c|c c c c c c c c} \hline \hline \multirow{2}{*}{Task} & \multirow{2}{*}{Method \(\mathcal{M}\)} & \multirow{2}{*}{Additional information} & \multirow{2}{*}{\(Acc(\mathcal{M},\,\mathcal{Q},\mathcal{P}_{\text{cons}})\)} & \multicolumn{4}{c|}{\(Acc(\mathcal{M},\,\mathcal{Q},\mathcal{P}_{\text{cons}})\)} \\  & & & & Easy & Medium & Hard & Arg. & Easy & Medium & Hard & Arg. \\ \hline \multirow{4}{*}{Math} & Base & \multirow{4}{*}{w/SC [29]} & Base & - & 46.4 & 39.3 & 30.3 & 26.6 & 32.1 & 23.2 & 10.1 & 6.0 & 13.1 \\  & & w/SD [29] & Ground Truth & 53.6 & 46.3 & 39.6 & 36.4 & 40.8 & 34.7 & 22.0 & 1.7 & 24.8 \\  & & w/SD [81] & Noise Position & 27.2 & 37.2 & 34.2 & 29.9 & 34.4 & 30.1 & 18.4 & 14.1 & 20.9 \\  & & w/CC [9] & Clean Demo & 44.9 & 43.3 & 44.6 & 45.5 & 44.3 & 37.2 & 31.7 & 30.7 & 33.2 \\  & & w/CD-CoT (ours) & Clean Demo & **60.7** & **59.7** & **60.7** & **57.2** & **59.2** & **54.0** & **58.7** & **48.4** & **58.7** \\ \hline \multirow{4}{*}{Math} & Base & \multirow{4}{*}{w/SC [29]} & Base & 23.9 & 19.1 & 13.6 & 10.7 & 14.5 & 14.0 & 6.7 & 3.8 & 8.1 \\  & & w/SD [29] & Ground Truth & **33.0** & 29.2 & 24.0 & 20.0 & 24.4 & **29.2** & 20.0 & 17.2 & 22.1 \\  & & w/SD [29] & Noise Position & 24.3 & **17.5** & 17.2 & 15.7 & 13.5 & 12.8 & 9.2 & 6.8 & 9.6 \\  & & w/CD [9] & Clean Demo & 22.3 & 19.1 & 18.4 & 18.2 & 18.6 & 19.0 & 15.3 & 14.6 & 16.3 \\  & & w/CD-CoT (ours) & Clean Demo & 31.0 & **33.7** & **32.7** & **34.7** & **33.7** & **20.0** & **20.7** & **25.3** & **28.3** \\ \hline \multirow{4}{*}{Symbolic} & Base & \multirow{4}{*}{w/SC [29]} & Base & - & 32.7 & 28.1 & 25.1 & 23.0 & 25.4 & 29.1 & 26.1 & 22.7 & 26.0 \\  & & w/SD [29] & Ground Truth & 38.5 & 34.9 & 31.4 & 22.7 & 31.7 & 34.0 & 34.1 & 34.5 & 34.2 \\  & & w/SD [29] & Noise Position & 37.8 & 35.0 & 27.7 & 27.6 & 37.8 & 35.2 & **27.9** & **25.0** \\  & & w/CD [9] & Clean Demo & 37.8 & 33.8 & 32.7 & 32.0 & 32.8 & 31.3 & 33.0 & 29.9 & 31.4 \\  & & w/CD-CoT (ours) & Clean Demo & **42.7** & **44.7** & **44.0** & **43.8** & **42.6** & **41.3** & **42.7** & **42.2** \\ \hline \multirow{4}{*}{Symbolic} & Base & \multirow{4}{*}{w/SC [29]} & Base & \multirow{4}{*}{w/SC [29]} & - & 9.2 & 6.3 & 7.2 & 6.0 & 6.5 & 7.0 & 6.8 & 6.0 & 6.6 \\  & & w/SD [29] & Ground Truth & **18.7** & **12.1** & 10.5 & 11.3 & 11.3 & **15.2** & **15.9** & **9.8** & **13.6** \\  & & w/SD [29] & Noise Position & 7.2 & 3.4 & 3.5 & 7.5 & 7.5 & 3.8 & 3.6 & 3.6 & 3.7 \\  & & w/CD [9] & Clean Demo & 9.4 & 9.8 & 7.9 & 7.9 & 8.5 & 8.5 & 7.4 & 6.5 & 7.5 \\  & & w/CD-CoT (ours) & Clean Demo & 12.3 & 12.0 & **12.0** & **13.0** & **12.3** & 12.0 & 10.0 & 11.1 \\ \hline \multirow{4}{*}{Communense} & Base & \multirow{4}{*}{w/SC [29]} & - & 45.7 & 44.3 & 42.3 & 41.4 & 42.7 & 36.7 & 33.4 & 28.3 & 32.8 \\  & & w/SD [29] & Ground Truth & **63.5** & 40.4 & 56.4 & 60.8 & 56.4 & 58.6 & 55.9 & 57.8 \\  & & w/SD [29] & Noise Position & 47.7 & 25.8 & 25.3 & 25.8 & 11.6 & 11.0 & 15.8 & 12.8 \\  & & w/CD [9] & Clean Demo & 48.3 & 45.7 & 43.6 & 44.0 & 44.4 & 42.1 & 40.8 & 40.5 & 41.1 \\  & & w/CD-CoT (ours) & Clean Demo & 49.0 & 50.2 & 54.7 & 50.3 & 51.8 & 51.0 & 49.7 & 49.7 & 50.1 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Performance of denoising methods that require additional information for supervision.

**Ablation study with different LLMs.** We examine the generalization of CD-CoT across different LLMs. As shown in Tab. 11, with comparably more powerful LLMs, _e.g._, GPT-3.5-turbo and Gemini-Pro, CD-CoT demonstrates notable improvements in average accuracy. It respectively achieves increases of \(23.4\%\) and \(21.6\%\) in accuracy compared to base models and surpasses all the baselines.

**Case Study.** We illustrate the denoising effects of various robust methods using Math Base-9 as an example. The results in Tab. 12 indicate that the introduction of random characters by SM disrupts the logic of the rationale. SD fails to eliminate all the noise while recovering the input content, and SP alters the original rationale's reasoning process even when noise removal is successful. In contrast, CD-CoT significantly removes noisy thoughts and ensures format alignment with the original rationale. More denoising examples and an entire case study are in Appendix F.9 and G, respectively.

## 6 Conclusion

In this work, we investigate the under-explored problem of noisy rationales in LLMs. We introduce the NoRa dataset, which tests LLMs against irrelevant or inaccurate thoughts in question-answer scenarios. Our findings show LLMs' vulnerability to noisy rationales is inadequately mitigated by existing robust methods. We thereby design the CD-CoT method to enhance the robustness via contrastive denoising. The extension advocates for advancing LLMs by strategies, _e.g._, external knowledge bases with a retrieval-augmented framework, robust inductive reasoning to extract rules from noisy examples, and multi-modal data integration to enhance the robustness of LLM reasoning.

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c} \hline \hline \multicolumn{2}{c|}{Hyper-parameters} & \multicolumn{3}{c|}{\#Tokens in step-3 (irr.)} & \multicolumn{3}{c}{\#Tokens in step-3 (ina.)} \\ \(N\) & \(M\) & \(D\) & \(C\) & Base-9 & Sym.(E) & Com. & Base-9 & Sym.(E) & Com. \\ \hline
5 & 1 & 5 & Y & 1440 & 3162 & 788 & 1428 & 3170 & 798 \\
5 & 1 & 5 & N & 1301 & 2685 & 660 & 1295 & 2732 & 667 \\
5 & 2 & 2-3 & Y & 2175 & 4934 & 1269 & 2156 & 4989 & 1311 \\
5 & 2 & 2-3 & N & 1864 & 4044 & 1005 & 1842 & 4087 & 1039 \\
5 & 3 & 1+2+2 & Y & 2902 & 6704 & 1772 & 2878 & 6785 & 1821 \\
5 & 3 & 1+2+2 & Z & 2146 & 5360 & 1372 & 2399 & 5443 & 1420 \\
5 & 5 & 1 & Y & 3468 & 10340 & 2764 & 4339 & 10514 & 2105 \\
5 & 5 & 1 & N & 3535 & 8099 & 2088 & 3506 & 8303 & 2163 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Comparison of #tokens on medium-level tasks.

\begin{table}
\begin{tabular}{c|c|c c|c c|c c} \hline \hline \multicolumn{2}{c|}{Model} & \multicolumn{3}{c|}{\begin{tabular}{c} Acc(\(\mathcal{M}\), \(\mathcal{Q}\), \(\mathcal{P}_{\text{unbiased}}\)) \\ Base-9 & Sym.(E) \\ \end{tabular} } & \multicolumn{3}{c|}{
\begin{tabular}{c} Acc(\(\mathcal{M}\), \(\mathcal{Q}\), \(\mathcal{P}_{\text{unbiased}}\)) \\ Base-9 & Sym.(E) \\ \end{tabular} } \\ \hline \multirow{4}{*}{GPT-3-5-turbo} & Base & 30.3 & 25.1 & 42.3 & 10.1 & 26.1 & 33.4 \\  & SC & 36.6 & 28.3 & 45.0 & 17.3 & 30.7 & 44.7 \\  & BT & 34.2 & 22.7 & 28.3 & 18.4 & 22.7 & 11.0 \\  & CD-CoT & 44.3 & 32.7 & 43.6 & 31.7 & 33.0 & 40.8 \\  & CD-CoT & **60.7** & **42.7** & **54.7** & **58.7** & **41.3** & **49.7** \\ \hline \multirow{4}{*}{Gemini-Pro} & Base & 72.3 & 38.9 & 53.2 & 21.2 & 36.7 & 33.5 \\  & SC & 80.3 & 43.3 & 60.0 & 32.3 & 45.0 & 42.7 \\  & BT & 82.4 & 29.3 & 37.8 & 26.7 & 28.7 & 33.3 \\  & CC & 67.5 & 37.3 & 50.2 & 43.6 & 35.0 & 45.6 \\  & CD-CoT & **92.7** & **49.3** & **57.7** & **76.2** & **53.3** & **55.7** \\ \hline \multirow{4}{*}{LLMaA2-70B} & Base & 2.8 & 8.7 & 41.9 & 2.7 & 9.1 & 40.2 \\  & SC & **5.0** & 10.3 & **46.7** & **3.0** & 9.7 & **46.0** \\  & BT & 1.4 & 11.2 & 36.1 & 0.9 & 12.5 & 36.2 \\  & CC & 1.1 & **16.3** & 29.9 & 2.8 & **14.0** & 28.3 \\  & CD-CoT & 4.0 & 9.7 & 39.3 & 2.7 & 9.7 & 39.7 \\ \hline \multirow{4}{*}{Mixtral-8x7B} & Base & 16.3 & 17.9 & 34.9 & 3.7 & 15.1 & 31.1 \\  & SC & 20.0 & 21.7 & 37.0 & 2.7 & 18.0 & 37.7 \\  & BT & 4.1 & 9.7 & 6.2 & 2.4 & 10.1 & 10.5 \\  & CC & **24.4** & 18.5 & 36.0 & **12.5** & 18.3 & 35.7 \\  & CD-CoT & 8.7 & **22.7** & **40.3** & 4.7 & **21.3** & **40.3** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Comparing methods with different LLMs.

\begin{table}
\begin{tabular}{c c c|c c c|c c c c} \hline \hline \multicolumn{2}{c|}{Hyper-parameters} & \multicolumn{3}{c|}{\#Tokens in step-3 (irr.)} & \multicolumn{3}{c}{\#Tokens in step-3 (ina.)} \\ \(N\) & \(M\) & \(D\) & \(C\) & Base-9 & Sym.(E) & Com. & Base-9 & Sym.(E) & Com. \\ \hline
5 & 1 & 5 & Y & 1440 & 3162 & 788 & 1428 & 3170 & 798 \\
5 & 2 & 2-3 & Y & 1301 & 2685 & 660 & 1295 & 2732 & 667 \\
5 & 2 & 2-3 & Y & 2175 & 4934 & 1269 & 2156 & 4989 & 1311 \\
5 & 2 & 2-3 & N & 1864 & 4044 & 1005 & 1842 & 4087 & 1039 \\
5 & 3 & 1+2+2 & Y & 2902 & 6704 & 1772 & 2878 & 6785 & 1821 \\
5 & 3 & 1+2+2 & Z & 2146 & 5360 & 1372 & 2399 & 5443 & 1420 \\
5 & 5 & 1 & Y & 3468 & 10340 & 2764 & 4339 & 10514 & 2105 \\
5 & 5 & 1 & N & 3535 & 8099 & 2088 & 3506 & 8303 & 2163 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Comparison of accuracy on medium-level tasks.

## Acknowledgements

ZKZ, RT, JNZ, and BH were supported by Guangdong Basic and Applied Basic Research Foundation Nos. 2022A1515011652 and 2024A1515012399, NSFC General Program No. 62376235, HKBU Faculty Niche Research Areas No. RC-FNRA-IG/22-23/SCI/04, and HKBU CSD Departmental Incentive Scheme. YWL and ZMW were supported by National Natural Science Foundation of China under Grants 62271357, Natural Science Foundation of Hubei Province under Grants 2023BAB072, and Fundamental Research Funds for the Central Universities under Grants 2042023kf0134. The authors especially thank Jiangchao Yao, Xuan Li, and Xiao Feng for the constructive discussions and insightful suggestions that helped improve this work.

## References

* [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [2] Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. Do language models know when they're hallucinating references? _arXiv preprint arXiv:2305.18248_, 2023.
* [3] Afra Feyza Akyurek, Ekin Akyurek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, and Niket Tandon. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. _arXiv preprint arXiv:2305.08844_, 2023.
* [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In _NeurIPS_, 2020.
* [5] Paul Chandler and John Sweller. Cognitive load theory and the format of instruction. _Cognition and instruction_, 1991.
* [6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _ICML_, 2020.
* [7] Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, Kaili Ma, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-of-distribution generalization on graphs. In _NeurIPS_, 2022.
* [8] Yongqiang Chen, Wei Huang, Kaiwen Zhou, Yatao Bian, Bo Han, and James Cheng. Understanding and improving feature learning for out-of-distribution generalization. In _NeurIPS_, 2023.
* [9] Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, and Lidong Bing. Contrastive chain-of-thought prompting. _arXiv preprint arXiv:2311.09277_, 2023.
* [10] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. Quac: Question answering in context. _arXiv preprint arXiv:1808.07036_, 2018.
* [11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 2023.
* [12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.
* [13] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In _ICML_, 2019.
* [14] Michela Del Vicario, Antonio Scala, Guido Caldarelli, H Eugene Stanley, and Walter Quattrociocchi. Modeling confirmation bias and polarization. _Scientific reports_, 2017.

* [15] Ronald A DeVore and George G Lorentz. _Constructive approximation_. 1993.
* [16] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. _arXiv preprint arXiv:2301.00234_, 2022.
* [17] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. _Minds and Machines_, 2020.
* [18] C Daniel Freeman, Laura Culp, Aaron Parisi, Maxwell L Bileschi, Gamaleldin F Elsayed, Alex Rizkowsky, Isabelle Simpson, Alex Alemi, Azade Nova, Ben Adlam, et al. Frontier language models are not robust to adversarial arithmetic, or" what do i need to say so you agree _2+ 2= 5? arXiv preprint arXiv:2311.07587_, 2023.
* [19] Chengguang Gan and Tatsunori Mori. Sensitivity and robustness of large language models to prompt in japanese. _arXiv preprint arXiv:2305.08714_, 2023.
* [20] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daume Iii, and Kate Crawford. Datasheets for datasets. _Communications of the ACM_, 2021.
* [21] Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Naumann, Michel Galley, Jianfeng Gao, and Hoifung Poon. Self-verification improves few-shot clinical information extraction. _arXiv preprint arXiv:2306.00024_, 2023.
* [22] Chen Gong, Qizhou Wang, Tongliang Liu, Bo Han, Jane You, Jian Yang, and Dacheng Tao. Instance-dependent positive and unlabeled learning with labeling bias estimation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2021.
* [23] Thomas L Griffiths. Understanding human intelligence through human limitations. _Trends in Cognitive Sciences_, 2020.
* [24] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In _NeurIPS_, 2018.
* [25] Alex Havrilla and Maia Iyer. Understanding the effect of noise in llm training data with algorithmic chains of thought. _arXiv preprint arXiv:2402.04004_, 2024.
* [26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _CVPR_, 2020.
* [27] Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Jiaqing Liang, and Yanghua Xiao. Can large language models understand real-world complex instructions? In _AAAI_, 2024.
* [28] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. _arXiv preprint arXiv:2210.11610_, 2022.
* [29] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. In _ICLR_, 2024.
* [30] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. _arXiv preprint arXiv:2311.05232_, 2023.
* [31] Irving L Janis. Groupthink. _IEEE Engineering Management Review_, 2008.
* [32] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In _EMNLP_, 2017.

* [33] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mistral of experts. _arXiv preprint arXiv:2401.04088_, 2024.
* [34] Shuyang Jiang, Yuhao Wang, and Yu Wang. Selfevolve: A code evolution framework via large language models. _arXiv preprint arXiv:2306.02907_, 2023.
* [35] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In _NeurIPS_, 2020.
* [36] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. _arXiv preprint arXiv:2303.17491_, 2023.
* [37] BJK Kleijn and AW van der Vaart. The bernstein-von-mises theorem under misspecification. _Electronic Journal of Statistics_, 2012.
* [38] Sin Yee Koh. The inversion of majority/minority at the de/reterritorialised urban higher education enclave: Xiamen university malaysia. _Urban Studies_, 2022.
* [39] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In _NeurIPS_, 2022.
* [40] Seonmin Koo, Chanjun Park, Seolhwa Lee, Jaehyung Seo, Sugyeong Eo, Hyeonseok Moon, and Heuiseok Lim. Uncovering the risks and drawbacks associated with the use of synthetic data for grammatical error correction. _IEEE Access_, 2023.
* [41] Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In _ICML_, 2018.
* [42] Fangyu Lei, Xiang Li, Yifan Wei, Shizhu He, Yiming Huang, Jun Zhao, and Kang Liu. S 3 haq: A three-stage approach for multi-hop text-table hybrid question answering. _arXiv preprint arXiv:2305.11725_, 2023.
* [43] Miaoran Li, Baolin Peng, and Zhu Zhang. Self-checker: Plug-and-play modules for fact-checking with large language models. _arXiv preprint arXiv:2305.14623_, 2023.
* [44] Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize large language model to be jailbreaker. _arXiv preprint arXiv:2311.03191_, 2023.
* [45] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. _arXiv preprint arXiv:2310.06500_, 2023.
* [46] Ke Liang, Yue Liu, Sihang Zhou, Wenxuan Tu, Yi Wen, Xihong Yang, Xiangjun Dong, and Xinwang Liu. Knowledge graph contrastive learning based on relation-symmetrical structure. _IEEE Transactions on Knowledge and Data Engineering_, 2023.
* [47] Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu, Fuchun Sun, and Kunlun He. A survey of knowledge graph reasoning on graph types: Static, dynamic, and multi-modal. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2024.
* [48] Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah A Smith, and Yejin Choi. We're afraid language models aren't modeling ambiguity. _arXiv preprint arXiv:2304.14399_, 2023.
* [49] Feng Liu, Bo Han, Tongliang Liu, Chen Gong, Gang Niu, Mingyuan Zhou, Masashi Sugiyama, et al. Probabilistic margins for instance reweighting in adversarial training. In _NeurIPS_, 2021.
* [50] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? _arXiv preprint arXiv:2101.06804_, 2021.

* [51] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. _arXiv preprint arXiv:2104.08786_, 2021.
* [52] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. _arXiv preprint arXiv:2303.17651_, 2023.
* [53] Richard E Mayer. _Thinking and problem solving: An introduction to human cognition and learning_. Scott, Foresman, 1977.
* [54] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In _EMNLP_, 2022.
* [55] Carey K Morewedge and Daniel Kahneman. Associative processes in intuitive judgment. _Trends in cognitive sciences_, 2010.
* [56] Paul Norris. _Emotional reasoning_. University of Massachusetts Amherst, 2000.
* [57] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. _arXiv preprint arXiv:2308.03188_, 2023.
* [58] Lalchand Pandia and Allyson Ettinger. Sorting through the noise: Testing robustness of information processing in pre-trained language models. In _EMNLP_, 2021.
* [59] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. _arXiv preprint arXiv:2304.01904_, 2023.
* [60] Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models. In _NeurIPS_, 2021.
* [61] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 2019.
* [62] Alexander Robey, Eric Wong, Hamed Hassani, and George J Pappas. Smoothllm: Defending large language models against jailbreaking attacks. _arXiv preprint arXiv:2310.03684_, 2023.
* [63] Tomas Rokicki, Herbert Kociemba, Morley Davidson, and John Dethridge. The diameter of the rubik's cube group is twenty. _SIAM REVIEW_, 2014.
* [64] Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. "everyone wants to do the model work, not the data work": Data cascades in high-stakes ai. In _CHI_, 2021.
* [65] Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. _arXiv preprint arXiv:2210.01240_, 2022.
* [66] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a mirage? In _NeurIPS_, 2023.
* [67] Jeremy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback at scale. _arXiv preprint arXiv:2303.16755_, 2023.
* [68] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Scharli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In _ICML_, 2023.
* [69] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In _NeurIPS_, 2023.

* [70] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _Nature_, 2016.
* [71] Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L Hamilton. Cluttr: A diagnostic benchmark for inductive reasoning from text. _arXiv preprint arXiv:1908.06177_, 2019.
* [72] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_, 2022.
* [73] Joe Stacey, Jianpeng Cheng, John Torr, Tristan Guigue, Joris Driesen, Alexandru Coca, Mark Gaynor, and Anders Johannsen. Lucid: Llm-generated utterances for complex and interesting dialogues. _arXiv preprint arXiv:2403.00462_, 2024.
* [74] Zhenheng Tang, Yuxin Wang, Xin He, Longteng Zhang, Xinglin Pan, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Bingsheng He, et al. Fusionai: Decentralized training and deploying llms with massive consumer-level gpus. _arXiv preprint arXiv:2309.01172_, 2023.
* [75] Zhenheng Tang, Xuez Kang, Yiming Yin, Xinglin Pan, Yuxin Wang, Xin He, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Amelie Chi Zhou, Bo Li, Bingsheng He, and Xiaowen Chu. Fusionlm: A decentralized llm training system on geo-distributed gpus with adaptive compression. _arXiv preprint arXiv:2410.12707_, 2024.
* [76] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [77] David Thorstad. Cognitive bias in large language models: Cautious optimism meets anti-panglossian meliorism. _arXiv preprint arXiv:2311.10932_, 2023.
* [78] Qingyuan Tian, Hanlun Zhu, Lei Wang, Yang Li, and Yunshi Lan. R3 prompting: Review, rephrase and resolve for chain-of-thought reasoning in large language models under noisy context prompting: Review, rephrase and resolve for chain-of-thought reasoning in large language models under noisy context. In _EMNLP_, 2023.
* [79] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [80] Amos Tversky and Daniel Kahneman. Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. _Science_, 1974.
* [81] Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, and Victor Carbone. Llms cannot find reasoning errors, but can correct them! _arXiv preprint arXiv:2311.08516_, 2023.
* [82] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models-a critical investigation. In _NeurIPS_, 2023.
* [83] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In _ICLR_, 2023.
* [84] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022.
* [85] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In _NeurIPS_, 2022.

* [86] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. _arXiv preprint arXiv:2303.03846_, 2023.
* [87] G Elliott Wimmer and Daphna Shohamy. Preference by association: how memory mechanisms in the hippocampus bias decisions. _Science_, 2012.
* [88] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyurek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. _arXiv preprint arXiv:2307.02477_, 2023.
* [89] Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Tao Gui, Qi Zhang, and Xuanjing Huang. Self-polish: Enhance reasoning in large language models via problem refinement. _arXiv preprint arXiv:2305.14497_, 2023.
* [90] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. _arXiv preprint arXiv:2111.02080_, 2021.
* [91] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. _arXiv preprint arXiv:2309.03409_, 2023.
* [92] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In _NeurIPS_, 2023.
* [93] Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. Selfee: Iterative self-revising llm empowered by self-feedback generation. _Blog post, May_, 2023.
* [94] Xi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning. In _NeurIPS_, 2022.
* [95] Simon Chi Lok Yu, Jie He, Pasquale Minervini, and Jeff Z Pan. Evaluating the adversarial robustness of retrieval-based in-context learning for large language models. _arXiv preprint arXiv:2405.15984_, 2024.
* [96] Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. Improving language models via plug-and-play retrieval feedback. _arXiv preprint arXiv:2305.14002_, 2023.
* [97] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. In _NeurIPS_, 2022.
* [98] Jiehang Zeng, Jianhan Xu, Xiaoqing Zheng, and Xuanjing Huang. Certified robustness to text adversarial attacks by randomized [mask]. _Computational Linguistics_, 2023.
* [99] Yiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. _arXiv preprint arXiv:2211.04486_, 2022.
* [100] Yongqi Zhang, Zhanke Zhou, Quanming Yao, Xiaowen Chu, and Bo Han. Adaprop: Learning adaptive propagation for graph neural network based knowledge graph reasoning. In _SIGKDD_, 2023.
* [101] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren's song in the ai ocean: A survey on hallucination in large language models. _arXiv preprint arXiv:2309.01219_, 2023.
* [102] Zhen Zhang, Guanhua Zhang, Bairu Hou, Wenqi Fan, Qing Li, Sijia Liu, Yang Zhang, and Shiyu Chang. Certified robustness for large language models with self-denoising. _arXiv preprint arXiv:2307.07171_, 2023.
* [103] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In _ICLR_, 2023.

* [104] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large language models: A survey. _ACM Transactions on Intelligent Systems and Technology_, 2023.
* [105] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _arXiv preprint arXiv:2303.18223_, 2023.
* [106] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In _ICML_, 2021.
* [107] Hongyi Zheng and Abulhair Saparov. Noisy exemplars make large language models more robust: A domain-agnostic behavioral analysis. In _EMNLP_, 2023.
* [108] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In _NeurIPS_, 2023.
* [109] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. _arXiv preprint arXiv:2205.10625_, 2022.
* [110] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. _arXiv preprint arXiv:2211.01910_, 2022.
* [111] Zhanke Zhou, Jiangchao Yao, Jiaxu Liu, Xiawei Guo, Quanming Yao, Li He, Liang Wang, Bo Zheng, and Bo Han. Combating bilateral edge noise for robust link prediction. In _NeurIPS_, 2023.
* [112] Zhanke Zhou, Chenyu Zhou, Xuan Li, Jiangchao Yao, Quanming Yao, and Bo Han. On strengthening and defending graph reconstruction attack with markov chain approximation. In _ICML_, 2023.
* [113] Zhanke Zhou, Yongqi Zhang, Jiangchao Yao, Quanming Yao, and Bo Han. Less is more: One-shot subgraph reasoning on large-scale knowledge graphs. In _ICLR_, 2024.
* [114] Yilun Zhu, Joel Ruben Antony Moniz, Shruti Bhargava, Jiarui Lu, Dhivya Piraviperumal, Site Li, Yuan Zhang, Hong Yu, and Bo-Hsiang Tseng. Can large language models understand context? _arXiv preprint arXiv:2402.00858_, 2024.

## Appendix

* A Further Discussion
* B Related Work
* B.1 In-context Learning
* B.2 Self-correction
* B.3 Self-consistency
* B.4 External Supervision
* B.5 Relation with Literature
* C Benchmark
* C.1 Motivation
* C.2 A Further Discussion on Noisy Rationales in CoT Demonstrations
* C.3 Real-world Examples
* C.4 Noise Generation
* C.5 NoRa Datasheet
* C.6 Full Examples of the NoRa Dataset
* D Theoretical Analysis
* E Implementation Details
* E.1 Baseline Methods
* E.2 Contrastive Denoising with Noisy Chain-of-Thought
* E.3 Hyper-parameter Optimization
* F Full Experiments
* F.1 Detailed Setups of the Experiments
* F.2 Supplementary Results of the Main Experiments
* F.3 The Superior Performance and Denoising Effectiveness
* F.4 The Difficulty of Irrelevant Noise Semantics
* F.5 The Number of Noisy Thoughts
* F.6 The Number of Noisy Examples
* F.7 Variants of CD-CoT
* F.8 New Datasets and Large-scale Real Problems
* F.9 Rephrased Examples of Different Denoising Methods
* G Further Case Study
* H NeurIPS ChecklistFurther Discussion

**Broader impact.** This paper presents work that aims to advance the field of trustworthy machine learning and large language models. We do not find any negative societal consequences of our work. This paper does not raise any ethical concerns. This study does not involve human subjects, practices, data set releases, potentially harmful insights, methodologies, applications, potential conflicts of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security issues, legal compliance, or research integrity issues.

**Limitations.** The proposed CD-CoT method is currently dependent on human-annotated, clean rationales. Future enhancements could include developing a self-supervised variant that does not rely on such examples. Exploring strategies like using contrasting noisy examples or incorporating an external knowledge base, possibly through a retrieval-augmented denoising framework, may offer significant advances in automation and robustness of reasoning.

**Extensions.** CoT and its variants have predominantly focused on deductive reasoning, leaving inductive reasoning largely unexplored. Investigating the ability of LLMs to extract rules from noisy examples is a compelling area. Additionally, theoretical analysis of noisy ICL can offer deeper insights into the Noisy-R problem. Expanding the NoRa dataset to include multi-modal scenarios, particularly visual data, is also crucial for a more comprehensive understanding of the robustness of foundation models. Further extensions include knowledge-enhanced denoising [100, 46, 47, 113], generalization to out-of-distribution noisy scenarios [7, 8], and training to fundamentally improve the robustness of language models [75, 74].

## Appendix B Related Work

In this section, we provide a detailed literature review as an extension of the preliminaries (Sec. 2), including in-context learning (Appendix B.1), self-correction methods (Appendix B.2), self-consistency methods (Appendix B.3), and external supervision (Appendix B.4). We further discuss the relation between our work and literature in Appendix B.5. We also provide Fig. 6 to better understand different reasoning settings.

**Input: CoT with Noisy Rationales**

**Question-1:** In base-9, what is 86+57?

**Question-2:** In base-9, what is 63+54?

**Question-3:** In base-9, what is 31+58?

**Question-3:** In base-9, what is 62+58?

**Input:** CoT with Noisy Questions**

**Question-1:** In base-9, what is 86+57?

**Question-1:** In base-9, what is 66+57?

**Rationale-1:** In base-9, the digits are "012345678". We have 6 + 7 = 13 in base-10. Since we're in base-9, that exceeds the maximum value of 8 for a single digit. 13 \(\med{9}\) = 4, so the digit is 4 and the carry is 1. We have 8 + 5 + 1 = 14 in base 10. 14 mod \(9\) = 5, so the digit is 5 and the carry is 1. A leading digit 1. So the answer is 154.

**Answer-1:** 154.

**Question:** In base-9, what is 62+58?

**Question:** In base-9, what is 62+58?

**Input:** CoT with Noisy Rationales**

**Question-1:** In base-9, what is 86+57?

**Rationale-1:** In base-9, what is 66+57?

**Rationale-1:** In base-9, what is 66+57?

**Rationale-1:** In base-9, what is 66+58?

**Question:** In base-9, what is 62+58?

**Question:** In base-9, what is 62+58?

**Input:** CoT with Noisy Rationales**

**Question-1:** In base-9, what is 86+57?

**Rationale-1:** In base-9, what is 66+57?

**Rationale-1:** In base-9, the digits are "012345678". We have 6 + 7 = 13 in base-10. **13 + 8 = 21**. Since we're in base-9, that exceeds the maximum value of 8 for a single digit. 13 \(\med{9}\) = 4, so the digit is 4 and the carry is 1. We have 8 + 5 + 1 = 14 in base 10. 14 mod \(9\) = 5, so the digit is 5 and the carry is 1. **5 + 9 = 14**. A leading digit is 1. So the answer is 154.

**Answer-1:** 154.

**Question:** In base-9, what is 62+58?

**Question:** In base-9, what is 62+58?

Figure 6: Illustrations of different reasoning settings.

### In-context Learning

With the scaling of model size and corpus size [61, 4, 11, 105], large language models (LLMs) have demonstrated remarkable performance across a variety of tasks through in-context learning (ICL), that is, learning from a few input-output examples in the context. As a training-free framework, ICL can save on expensive training costs and be easily adapted to diverse new tasks [88, 1].

However, standard ICL faces challenges when addressing _intricate_ reasoning problems. Wei et al. [85] proposes chain-of-thought prompting (CoT), a methodology that explicitly guides LLMs in generating sequential reasoning steps to enhance their performance on intricate reasoning tasks. In particular, CoT replaces the input-output exemplars in ICL with triplets in the form of <_input, rationale, output>_, thereby enabling the model to learn explicit reasoning processes.

In the literature, various versions of chain-of-thought prompting have been proposed. To alleviate the human effort required for constructing triplet exemplars, Kojima et al. [39] proposes zero-shot CoT, a method that explicitly encourages models to generate a chain of reasoning first and then derive the final answer by employing natural language prompts like "Let's think step by step". Wang et al. [83] shows that self-consistency sampling can improve reasoning accuracy by sampling diverse reasoning paths and then taking the majority vote. Zelikman et al. [97] proposes STaR, which leverages a small amount of human-annotated rationale data, along with a large dataset of question-label pairs without rationales. This approach iteratively generates additional rationales and enhances the model's ability to generate reasoning steps by fine-tuning.

Least-to-most prompting (LtM) [109] enhances the reasoning capability of models by decomposing a given problem into sub-problems. In particular, LtM initially breaks down the task into a series of consecutive sub-problems and subsequently answers them one by one. During the process of responding, the answer to the preceding sub-problem is incorporated into the prompt for the succeeding one. Tree of Thoughts (ToT) [92] extends LtM by exploring multiple reasoning possibilities at each step. Specifically, ToT first decomposes a given problem into several reasoning steps and generates multiple answers for each step, ultimately constructing a tree structure. Subsequently, ToT employs BFS or DFS to traverse the tree, yielding the final rationale and answer.

**Vulnerabilities of ICL.** Despite being promising, some works point out the brittleness and over-sensitivity of ICL. Liu et al. [50], Perez et al. [60], Zhang et al. [99] demonstrate that ICL performance depends heavily on the choice of exemplars. Meanwhile, Zhao et al. [106], Lu et al. [51] observe that the arrangement order of in-context examples is also crucial to the ICL performance, potentially shifting results from near state-of-the-art to a random guessing. Moreover, Ye and Durrett [94], Gan and Mori [19], Zheng and Saparov [107], Zhang et al. [102] reveal LLMs' deficiencies when handling subtle perturbations within the prompts, even when such perturbations do not alter any semantic meaning. Yu et al. [95] explores the robustness of retrieval-augmented in-context learning (ICL) against demonstration attacks and test sample attacks. It focuses on perturbing the example questions (i.e., noisy questions) or labels, while our work focuses on the rationales of the examples (i.e., noisy rationales). In addition, previous work on safety [44, 112] and data noise [24, 49, 22, 111] might also inspire the robust problems in ICL. Overall, the enhancement of reasoning performance brought about by ICL is inherently unstable and susceptible to example selection, example ordering, and prompt perturbations. These observations underscore the importance the robustness of other aspects.

The aforementioned efforts primarily revolve around the idealized ICL, which utilizes high-quality prompts free from any noise or interference. Conversely, a parallel line of research has emerged, exploring the impact of noisy prompts on the performance of LLMs. Min et al. [54] examines the impact of in-context examples on ICL. This work observes that incorporating out-of-distribution input texts significantly diminishes the performance of standard question answering. Wei et al. [86] devises two different set-ups of ICL: ICL with flipped labels and ICL with semantically unrelated labels. Their investigation reveals that LLMs possess the capability to override semantic priors when confronted with in-context exemplars that contradict these priors. This phenomenon also suggests that larger models may be more susceptible to the influence of the noise present in examples. Shi et al. [68] examines the impact of irrelevant context on LLMs, and the results suggest that the inclusion of irrelevant information can significantly impair the performance of the models. These studies further illuminate the fragility and instability inherent in the reasoning capabilities of LLMs.

However, the previous works mainly consider the noisy questions/answers in standard ICL. In contrast, we move to the under-explored noisy rationales problem in the context of CoT, as illustrated in Fig. 6.

Numerous strategies have been proposed to address the vulnerabilities of LLM reasoning during in-context learning. These approaches can be categorized into _self-correction_ and _self-consistency_, which are introduced as follows.

### Self-correction

Self-correction emerges as a promising direction to enhance LLM reasoning, where LLMs attempt to correct their initial responses based on feedback. One popular line of research involves utilizing manual labor or external systems to evaluate and refine models. However, this can be costly due to the manual labor involved. Another line of research leverages the LLM's inherent capabilities to correct its initial responses without the crutch of external feedback. This methodology is a promising way to make LLM-based solutions practical and deployable [57].

**Self-correction with internal feedback.** In this line of research, the LLM is required to correct response trajectories based solely on its inherent capabilities. Huang et al. [28] first demonstrates the self-improvement potential of LLMs by utilizing a pre-trained LLM to generate rationale-augmented answers for unlabeled questions using CoT and majority voting and then fine-tuning the LLM using those self-generated labels, eventually improving the general reasoning ability.

When addressing problems, people typically engage in trial and error, coupled with reflective thinking, to discern the correct solutions. Inspired by this, Madaan et al. [52] proposes Self-refine, a simple and direct approach to improving LLM's output. In this approach, an LLM is used to create an initial output. Then, the model provides feedback on its own output in multiple dimensions. Based on this feedback, the model refines its initial output and repeats this process until it reaches a specified limit or the LLM determines that no further adjustments are necessary.

Encouraged by the augmented efficacy achieved through self-feedback mechanisms, Ye et al. [93] releases SelFee, a new instruction-following language model that generates self-feedback on its response and self-revises based on the feedback. The development of SelFee involves the fine-tuning of LLaMA by utilizing training instances generated by ChatGPT.

In addition, Gero et al. [21] introduces Self-verification, suggesting that by asking LLMs to provide provenance for their own outputs and conducting checks, it is possible to alleviate LLMs' issues regarding accuracy and interpretability in crucial domains such as healthcare. On the other hand, Xi et al. [89] focuses on the simplicity and comprehensibility of the given questions, proposing Self-polish (SP). This method instructs the LLM to iteratively refine the test question by removing irrelevant information and rearranging the logical structure, thereby improving the reasoning performance.

While the self-correction methodologies based on internal feedback appear promising, [29] categorizes such self-correction methods as _intrinsic self-correction_ (ISC) and demonstrates that the model's performance drops on all benchmarks after using ISC. This work points out that LLMs struggle to self-correct their responses without external feedback, and the corrected responses often exhibit inferior quality compared to their initial counterparts. Saparov and He [65] reveals that while models are able to produce valid reasoning steps with high probability when dealing with proof problems, they struggle with proof planning. In other words, when models occasionally generate incorrect proof steps, they are not able to return to the correct path.

Building upon this observation, Tyen et al. [81] further decomposes the self-correction process into two core components: mistake finding and output correction. This work demonstrates that current state-of-the-art LLMs cannot find mistakes reliably, even in the most simple and unambiguous cases, and suggests this is a main contributing factor to LLMs' inability to self-correct reasoning errors.

**Self-correction with external feedback.** External feedback offers a valuable external perspective, proving particularly advantageous in pinpointing errors that the large language model may not inherently recognize [57]. The sources of external feedback can be categorized as 1) human feedback [67, 69, 36, 81], 2) external tools [34, 96, 43], and 3) other models [59, 3].

Scheurer et al. [67] proposes Imitation Learning from Language Feedback (ILF), an approach leveraging informative human feedback that involves conditioning the model on input, initial output, and feedback; selecting the most feedback-incorporated refinement; and fine-tuning the model to maximize the chosen refinement's likelihood given the input.

Similarly, Shinn et al. [69] introduces Reflexion, which fortifies language agents by relying on linguistic feedback generated by themselves rather than weight updates, resulting in noteworthy enhancements compared to a baseline agent across a spectrum of tasks. Kim et al. [36] demonstratesthe effectiveness of self-critique on computer tasks by proposing Recursively Criticizes and Improves (RCI). This method utilizes a pre-trained LLM agent to execute computer tasks, guiding it using a simple prompting scheme where the agent recursively criticizes and improves its output.

Although Reflexion [69] and RCI [36] do not explicitly utilize human feedback, Huang et al. [29] points out that these approaches employ the labels of test questions to determine when to halt the self-correction loop. Consequently, they are implicitly guided by human feedback. Huang et al. [29] categorizes these methods as _self-correction with oracle feedback_ (SCO).

Due to the difficulty of LLMs relying solely on their inherent capabilities to correct responses for reasoning tasks [29], Tyen et al. [81] decomposes the self-correction process and observes challenges in accurately identifying error locations for LLMs. Consequently, they propose Backtracking (BT), which guides the correction process of models by providing the initial error location in the problem.

Furthermore, the incorporation of external tools can enhance the model's self-correction performance. For instance, to improve the code generation capabilities of the model, Jiang et al. [34] utilizes error messages generated by an external interpreter as feedback, requiring LLM to debug the initially generated code acting as a professional programmer. Introducing external sources of knowledge, such as Wikipedia, can also significantly enhance the accuracy of the model in reasoning tasks [96, 43].

In addition to leveraging external tools, certain efforts revolve around the utilization of external models to generate feedback. Paul et al. [59] introduces a critic model that generates structured feedback for the responses produced by the LLM during the self-correction process. The feedback is then incorporated into the input, allowing the LLM to refine its own output iteratively. Likewise, Akyurek et al. [3] presents a collaborative framework involving multiple agents, where the critique generator is trained with the aim of maximizing the performance of the reasoning LLM.

### Self-consistency

In our study, self-consistency represents a category of robustness methods, typically involving the generation of multiple diverse answers to a given question and subsequently converging them. This approach serves to mitigate the impact of noise inherent in the question. Among existing methods, two primary approaches are employed to generate distinct answers: (1) generating multiple samples via randomized smoothing on input, followed by separate and independent reasoning processes [62, 102, 98], and (2) leveraging the inherent randomness in the decoding process of the LLM to sample multiple distinct reasoning paths for the same input [83].

**Sampling diverse outputs.** Relying on the idea that an intricate reasoning problem usually allows for various distinct thinking ways, each leading to the unique correct solution, Wang et al. [83] first explores a varied array of reasoning paths and subsequently identifies the most consistent answer by majority vote. This approach boosts the model's performance across various reasoning tasks.

**Randomized smoothing over inputs.** Randomized smoothing serves as a straightforward yet effective mechanism commonly employed to defend against adversarial attacks by introducing multiple perturbations to the input to smooth potential noise [13, 98, 62, 102].

To address the vulnerability of LLMs to jailbreaking attacks, Robey et al. [62] proposes Smooth LLM (SM), which is based on randomized smoothing. Utilizing the discovery that prompts generated adversarially exhibit fragility to character-level modifications, this method initiates by randomly introducing perturbations to multiple copies of a given input prompt. Subsequently, it consolidates the associated predictions to identify adversarial inputs.

However, as pointed out in [102], randomized smoothing relies on introducing additional noise to input, and its certification success hinges on the model's performance with corrupted data. To address the issue, Zhang et al. [102] proposes Self-denoising (SD), which denoises the corrupted inputs with LLMs using the multitasking nature of LLMs.

### External Supervision

In addition to self-correction and self-consistency methods, there is a line of methods that require external supervision or information. In our study, we compare diverse denoising methods that require varying degrees of external supervision, including (1) ground truth for test questions, (2) noise positions, and (3) additional clean demonstrations.

To provide a thorough comparison, We adopt Self-correction with Oracle feedback (SCO) [29] as a strong baseline, which uses the labels of test questions to determine when to terminate the self-correction loop, as discussed in B.2. We also employ Backtracking (BT) [81], which necessitates the location of the initial noise in the prompt, to demonstrate the effectiveness of our method. In addition, we utilize Contrastive Chain-of-Thought (CC) [9] in experiments which needs similar external supervision as our proposed method: a clean demonstration. This approach enhances reasoning by presenting the LLM with all the clean and noisy examples concurrently.

### Relation with Literature

There are some implicit connections with existing papers. Here, we would like to further clarify our work's major differences and unique contributions, compared with [92], [54] and [9].

As discussed in B.1, Yao et al. [92] proposes Tree-of-Thought (ToT) for deliberate reasoning tasks. ToT decomposes a given problem into several reasoning steps and generates multiple answers for each step, ultimately constructing a tree structure. Subsequently, ToT employs BFS or DFS to traverse the tree, yielding the final rationale and answer. Namely, ToT focuses on the rationale (thoughts) generation in the output space of an LLM, wherein the generated thoughts can be irrelevant or inaccurate. By contrast, Our work focuses on the input space with noisy prompting examples. From this view of input space and output space, ToT is orthogonal to our work.

Moreover, ToT focuses on problem-solving by reasoning, _e.g._, Game of 24 and Mini Crosswords, while our work investigates the robustness problem of noisy rationales. ToT does not consider imperfect scenarios with noisy inputs. This means that when ToT is prompted by examples with noisy rationales, the reasoning might also be degenerated. Hence, a further investigation on reasoning robustness with these up-to-date prompting techniques, possibly with a focus on the output space, can be conducted based on our work.

As for [54], this work explores the question-answer mapping of prompting examples. They find that breaking the mapping has nearly no influence on LLM's in-context learning.

However, it's important to note that [54] primarily focuses on the relationship between input and label, investigating the question-answer mapping of prompting examples. Their work does not delve into the role of rationales in the examples, which is the central focus of our research. In contrast, our experiments keep the input-label mapping intact and instead explore the impact of introducing noise into the rationales. This allows us to isolate the effect of rationale quality on the model's performance without altering the fundamental relationship between questions and answers.

Hence, when the form of prompting example shifts from (question, answer) to (question, rationale, answer), more aspects can be investigated. We conduct additional experiments to further explore the mapping among question, rationale, and answer in Sec. 4.

Chia et al. [9] proposes Contrastive Chain-of-Thought (CC) [9] to enhance LLM reasoning. In our work, we focus on improving reasoning in noisy settings and define the noisy rationale problem as "factually inaccurate or irrelevant reasoning steps paired with valid question-answer prompts." Fig. 1 shows an example. Here, only one rationale is given in each demonstration, which can be potentially noisy but unknown to the model. However, in CC's setting, each ICL demonstration explicitly includes a clean rationale and a wrong rationale. Notably, this rationale is wrong instead of noisy as it induces the wrong answer. Therefore, CC's setting and ours are totally different. In addition, empirical results in Tab. 8 show that our method CD-CoT significantly outperforms CC when given the same information.

The problem of noisy rationale is challenging and non-trivial. We believe that the investigation of noisy rationale is still in the budding stage. Several potential solutions can be progressively devised to address this problem. For example, accessing external databases is a promising solution. However, this retrieve augmented generation (RAG) is usually used to address the lack of knowledge of LLMs but not for data denoising. Therein, verifying the factual relevance and accuracy of a rationale is quite challenging: (1) there is a semantic gap between a rationale and the corresponding query instructions of a database; (2) how can the models realize the need for data verification and then find a suitable database for that are also cutting-edge research directions?

Hence, we believe that RAG for denoising is not a trivial solution. More research attention and effort should be paid to achieve this solution. In this work, we verify that an LLM can achieve good denoising results when prompted with only one clean rationale and several noisy rationales. This solution, sharing a similar spirit to RAG that requires additional information, significantly mitigates the intrinsic vulnerabilities of LLMs.

Benchmark

In this section, we provide a comprehensive overview of the NoRa benchmark, including the motivation for constructing NoRa (Appendix C.1), a further discussion on the origins and impact of noisy rationales in CoT demonstrations (Appendix C.2), real-world examples of noisy rationales from both model and human perspectives (Appendix C.3), the detailed process of generating noise for the NoRa dataset (Appendix C.4), a comprehensive datasheet for the NoRa dataset (Appendix C.5), and full examples of the NoRa dataset across various reasoning tasks (Appendix C.6).

### Motivation

The robustness of LLMs against noisy inputs deserves more attention, particularly in the context of Chain-of-Thought (CoT) reasoning. While CoT methods have shown remarkable effectiveness, they face two fundamental challenges: the susceptibility to selected demonstrations and the complexity of generating high-quality demonstrations.

Recent studies have revealed that LLMs can be easily distracted by irrelevant or adversarial context, and their reasoning process is unstable under small prompt modifications. Furthermore, misleading information can cause LLMs to agree with factual errors. These vulnerabilities become particularly concerning when we consider that demonstrations in practice are not always clean or accurate.

The problem of noisy rationales in demonstrations arises from various practical scenarios:

1. **Human-generated noise**: Just as humans make mistakes in data annotation (which has motivated extensive research in label-noise learning), they can also provide imperfect demonstrations. Different users with varying expertise and background knowledge may contribute demonstrations of varying quality.
2. **System-generated noise**: Noise can emerge from automated systems, including dialogue systems and machine-generated data, especially in crowdsourced platforms.
3. **Demonstration selection challenges**: While incorporating human supervision in dealing with noisy rationales is possible, it is often expensive and impractical at scale. The sensitivity of LLMs to in-context learning examples makes this particularly challenging.

Our empirical studies have revealed widespread vulnerability among prevailing LLMs to noisy rationales, with existing reasoning methods showing limited efficacy. Compared to clean rationales, we observe performance decreases of 0.4-19.8% with irrelevant noise and more severe degradation of 2.0-66.8% with inaccurate noise.

The NoRa benchmark addresses this crucial gap in current research. While most existing work assumes clean demonstrations for in-context learning, we argue that noisy rationales represent a practical and challenging problem that requires immediate attention. This benchmark serves two primary purposes:

1. It provides a systematic way to evaluate LLM robustness against different types of noise in rationales.
2. It helps identify the limitations of current approaches and motivates the development of more robust reasoning strategies.

We believe the NoRa dataset and our findings will contribute significantly to the development of more trustworthy foundation models capable of handling real-world, imperfect demonstrations.

### A Further Discussion on Noisy Rationales in CoT Demonstrations

In this part, we further clarify the background and settings of the Noisy-R problem investigated in our work, _i.e._, noisy rationales in CoT demonstrations. Having established the presence of irrelevant and inaccurate thoughts as primary noise types, we now turn to their origins and impact within human-model interactions.

**Irrelevant and inaccurate thoughts can be generated by both the model and humans.** On the one hand, irrelevant or inaccurate thoughts are likely to appear within the rationales of LLMs' answers. These rationales, drawn from historical dialogues between users and the model, can create a noisy context for new conversations. On the other hand, human demonstrators actively provide in-context demonstrations to enhance the reasoning capabilities of models. However, these demonstrations, whether crafted by humans or sourced from datasets, can also contain noisy rationales.

**Irrelevant thoughts - model perspective:** LLMs tend to explain the concept of terms during reasoning. For instance, if you ask GPT-4 to debug an error related to the 'concurrent' package, it tends to start by explaining what the 'concurrent' package is rather than immediately addressing the debug request. Such explanations, while informative, may introduce irrelevant noise into the conversation. Here are some key reasons for their emergence.

1. **Cognitive bias of models.** When addressing complex queries, LLMs tend to include explanations for terms or concepts mentioned in the prompt that are unnecessary for solving the specific problem presented, as a kind of irrelevant thought. This behavior exhibits a form of cognitive bias where the model, unable to assess the inquirer's level of understanding, leads to explicating background information. This is comparable to a lecturer explaining the basics of a subject to a class without first assessing their students' existing knowledge, potentially leading to irrelevant elaborations.
2. **Lack of precise contextual understanding.** Despite LLMs' proficiency in processing language and recognizing patterns, they cannot always fully grasp the precise context or specific requirements of a problem. This shortfall can result in the production of thoughts that, although seemingly related, do not directly contribute to resolving the question at hand. Their responses might align more with the broader theme of the discussion rather than the specific, critical details needed for a precise solution.
3. **User query ambiguity of the dialogue mechanism.** Ambiguities in user queries can stem from the use of vague or multifaceted language, prompting the model to generate a wide array of responses. This situation is similar to a search engine returning a variety of results for a query that lacks specificity. The model, attempting to cover all potential meanings of the query, may produce responses that contain thoughts unrelated to the user's actual intent. For instance, if someone asks, "What is force calculation?", the model might provide information on both Newtonian mechanics and quantum mechanics. These responses, encompassing a broad range of topics, could influence the answers to subsequent physics questions, leading to a continuation of the ambiguity and further complicating the conversation.
4. **Progressive disclosure bias of the dialogue mechanism.** When engaging with LLMs, individuals often initiate the dialogue by describing simpler concepts and progressively work their way toward articulating the complex question at hand because of human limitations in language or comprehension abilities, which prevent a person from presenting the entire complexity of an issue in one go. This step-by-step approach, while natural for humans who struggle to directly convey intricate problems, can introduce extraneous content that contributes to noise within the model's contextual reasoning. As the conversation builds, the LLM will factor in these initial, possibly tangential, explanations into its understanding of the context, potentially leading to a dilution of the focus necessary for solving the specific issue. This phenomenon reflects a human cognitive strategy in communication that may not be optimally aligned with the operational mechanisms of LLMs for efficient problem-solving.

**Irrelevant thoughts - human perspective:** When a human is actively demonstrating CoT reasoning, the introduction of irrelevant thoughts could be due to a variety of reasons.

1. **Cognitive overload of humans.** Humans may introduce irrelevant information when they are trying to process too much information at once, which can lead to a loss of focus and the inclusion of tangential thoughts. For instance, a programmer is struggling with a bug in a complex piece of software and asks an LLM for help. To provide context, the programmer starts explaining the issue with a CoT rationale approach, intending to walk the LLM through their thought process. However, due to the complexity of the code and the stress of finding the bug, the programmer gets sidetracked. They include unnecessary details about the different error messages encountered in the past, unrelated functions in the code, and general thoughts on software development.
2. **Associative thinking of humans.** Humans naturally think in an associative manner, where one thought may lead to another that is only loosely related to the task at hand. This can result in straying from the main point during a CoT explanation. For example, while a programmer outlines the steps to diagnose a software issue for an LLM, they might recall a similar problem encountered in a different project. This memory could lead them to mention troubleshooting strategies, tools, or anecdotes from that experience, which, although related to the broader theme of problem-solving, do not directly contribute to the current issue.
3. **Irrelevant content in datasets.** In the future, it is likely that companies or professional organizations will increasingly utilize databases to assemble CoT prompts. However, these databases, whether privately maintained or publicly accessible, can contain irrelevant reasoning processes. This is especially true for databases sourced from crowdsourcing platforms or open forums, where the information is contributed by a diverse set of individuals with varying levels of expertise and focus. When these datasets are used to provide incorrect information for CoT reasoning, the noise can originate from the inclusion of off-topic discussions, personal opinions, or overly verbose explanations that do not directly address the problem at hand. Such noise can be inadvertently introduced into the CoT process when humans provide explanations that contain unnecessary or tangential information.

Similarly, we analyze the two sources of inaccurate thoughts as follows.

**Inaccurate thought - model perspective:** For models, LLMs may produce erroneous thoughts during the reasoning process, especially when dealing with complex problems. For example, when tackling a base-9 math problem in a zero-shot setting, GPT-3.5 may generate some inaccurate reasoning steps. The former dialogue will become inaccurate and noisy in the context of subsequent dialogues. Here are some key reasons for their emergence.

1. **Outdated or incomplete training data of the model.** Language models are built upon datasets that may not be current or fully comprehensive. When faced with problems that require up-to-date knowledge or complete understanding, which are absent in their training data, models may rely on outdated or incomplete information, resulting in inaccurate outputs. For example, in the field of medicine, if new research suggests a change in treatment protocol after the model's last update, it wouldn't be able to advise on the new information.
2. **Adaptation to novel reasoning contexts of the model.** New challenges may require models to reason within contexts that slightly or significantly differ from their training data. For instance, a model extensively trained on base-10 arithmetic might struggle with a base-9 math problem because it requires a shift in the underlying numerical framework. This kind of scenario demands on-the-fly adaptation to a novel reasoning context, which can lead to generating thoughts that do not accurately apply the learned principles from the base-10 system to the newly introduced base-9 system.
3. **Misinterpretation of complex subjects of dialogue mechanism.** Users often fail to clearly articulate their complete requirements at the outset of an inquiry, leading to LLMs generating misunderstandings and inaccurate thoughts that do not align with user expectations. The process of correcting these thoughts is inherently a reasoning process laden with noisy contexts. As users provide feedback to refine the model's output, the iterative nature of this interaction can introduce additional inaccuracies as the model attempts to reconcile the new information with the previously misunderstood context.

**Inaccurate thought - human perspective:** Inaccurate thoughts in CoT can stem from the information provided by humans, whether it is self-made on the spot or sourced from a database for in-context learning by LLMs. These CoT demos can include inaccurate noise due to various factors.

1. **Personal knowledge limitations of human.** Individuals may possess incomplete or outdated knowledge on a given subject, leading to the provision of incorrect information when creating a CoT. For instance, a person without expertise in mathematics might attempt to construct a CoT for a complex math problem and inadvertently introduce incorrect steps or conclusions. Their understanding may be based on heuristics or educational background that hasn't been updated to reflect more recent methodologies or discoveries in the field.
2. **Cognitive biases of human.** Human reasoning can be influenced by a range of cognitive biases, such as confirmation bias, where an individual tends to search for, interpret, and remember information in a way that confirms their preconceptions, neglecting contrary information. Or the oversimplification of complex issues might lead to inaccurate reasoning steps within a CoT. These biases can skew the logic flow and result in conclusions that do not hold up under scrutiny or are based on flawed premises.

3. **Data quality issues of database.** The databases that humans rely on for creating CoTs might contain errors or biases introduced during data collection and processing. If this flawed data is used for in-context learning by LLMs, it can impart incorrect patterns of thought or factual inaccuracies. For example, a dataset with biased sampling methods might lead to generalizations that do not accurately represent the broader population or situation.
4. **Contextual misplacement of databases.** Information from databases may be stripped of its original context, leading to misinterpretation when reused. When humans include such decontextualized information in a CoT, they might not properly align it with the new context, introducing misunderstandings or inaccuracies. This is particularly problematic in nuanced fields where context heavily influences the meaning and applicability of information, such as legal precedents or cultural studies.

Given the convenience and adaptability of CoT reasoning, broader adoption in LLM applications is expected in the future. This structured approach enables LLMs to break down complex problems and explain their reasoning in a way that resembles how humans think, proving essential for sophisticated problem-solving. Nonetheless, we are bound to face the noisy reasoning challenges, stemming from both model-generated and human-contributed contexts, as mentioned above.

To address these challenges, we must focus on continuously improving training methods, keeping models updated with the latest information, enhancing their ability to parse context and ambiguity, and refining algorithms to diminish biases and logical inaccuracies.

**Differences between inaccurate and irrelevant thoughts.** Given the context and question, a thought is either relevant or irrelevant. Within the relevant thoughts, those accurate ones are desirable, and those inaccurate, termed as "inaccurate thoughts" in this work, are proven to be harmful to LLMs. Hence, the extreme case of "inaccurate and irrelevant thought" is not covered in this work. For clarity and simplicity, we investigate these two kinds of noise separately. Specifically, in definitions,

* Irrelevant thoughts refer to incorporating extraneous details that are unhelpful for solving the question. Redundant information may be introduced by the LLM's diverse response generation or by humans when clarifying concepts in problem-solving examples;
* Inaccurate thoughts refer to factual errors in rationales that are common in mathematical calculation or transcription. The emergence of noise can be due to algorithmic limitations, errors in training data, misinterpretations of context or instructions, and logical fallacies.

Note that the "accuracy" and "relevance" of thoughts are related to the context of the given question. Basically, the question provides the context, and an LLM generates thought conditional on the context. Taking the examples of the Base-9 dataset in Tab. 1,

* Irrelevant thought is "There are five oceans on Earth: the Atlantic, Pacific, Indian, Arctic, and Southern.", which is accurate but not relevant and not useful;
* Inaccurate thought here is "5 + 9 = 14", which is relevant to the question and previous thought but is inaccurate for the base-9 calculation.

In the NoRa dataset, only the relevant thought with factual error will be classified as an "inaccurate thought". There is no irrelevant and inaccurate thought in NoRa, which should be rare in practice.

We focus on the two major types of noise in this work. And empirically, inaccurate thoughts bring severe degradations. Compared with clean rationales, a 1.4%-19.8% decrease with irrelevant noise and a more drastic 2.2%-40.4% decrease with inaccurate noise.

### Real-world Examples

The emphasis on noisy rationale is due to its practical challenges, with examples drawn from diverse sources such as crowdsourced platforms, dialogue systems, and AI-generated data. Here, the Noisy-R mainly originates from (1) the inherent imperfections, inconsistencies, and inaccuracy of humans' cognitive processes and (2) the diversity, unpredictability, and hallucination of the LLMs' generative mechanisms, as discussed in more detail in Appendix C.2.

Briefly, irrelevant and inaccurate thoughts can be generated by both the model and humans. From model perspective, the generated rationales by the model can be noisy. From human perspective, the annotated rationales by humans can be noisy.

In the following four parts, we elaborate on the evidence (including sources and examples) of irrelevant and inaccurate thoughts from both the model perspective and the human perspective.

#### c.3.1 Irrelevant Thoughts from Model Perspective

**Cognitive bias of models:**

* **Source.** The presence of cognitive biases in LLMs is demonstrated, and the implications of using biased reasoning under the guise of expertise are discussed, calling for stronger education, risk management, and continued research as the widespread adoption of this technology increases [38]. Thorstad [77] examines the inherent cognitive biases within large language models, suggesting that while there is reason for cautious optimism regarding the potential to mitigate these biases, a realistic, anti-Panglossian approach is necessary to drive meaningful improvements. It advocates for a balanced perspective that neither idealizes the perfectibility of AI nor dismisses the progress made in addressing its limitations.
* **Examples.** Tab. 13 demonstrates examples of cognitive bias of models.

**Lack of precise contextual understanding:**

* **Source.** Zhu et al. [114] introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models' ability to understand the context and finds that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. He et al. [27] designs eight features for complex instructions and construct a comprehensive evaluation dataset from real-world scenarios, and compares the performance of representative Chinese-oriented and English-oriented models in following complex instructions through extensive experiments.
* **Examples.** Tab. 14 demonstrates the lack of precise contextual understanding in LLMs.

#### c.3.2 Irrelevant Thoughts from Human Perspective

**Associative thinking of humans:**

* **Source.** Morewedge and Kahneman [55] investigates the underlying cognitive mechanisms of intuition, focusing on how unconscious associative processes can lead to rapid and efficient problem-solving and decision-making. It aims to elucidate the often-overlooked

\begin{table}
\begin{tabular}{p{142.3pt}} Example 1 \\ \hline \hline
**Question:** What rubber toy filled with helium will make a child happy? Answer Choices: (A) loved (B) learn \(C\) eat cake (D) balloon (E) become adult **Rationale:** Let’s think step by step. A rubber toy filled with helium will make a child happy if the child loves the toy. If the child loves the toy, or she will learn to play with it. If the child learns to play with it, he or she will eat cake. If the child acts cake, he or she will become an adult. **Answer:** The answer is **D.** **Explanation:** There is information related to the answer in reasoning, but there is also noise information that is irrelevant to the final answer, and it has nothing to do with whether the child is happy or not because he becomes an adult because he eats the cake. \\ \hline \hline \multicolumn{2}{p{142.3pt}}{Example 2} \\ \hline
**Question:** What might a couple have a lot of when they are deciding on stopping being married to each other? Answer Choices: (A) pleasure (B) detachment \(C\) exercise (D) bankruptcy (E) fights **Rationale:** Let’s think step by step. A couple might have a lot of pleasure when they are deciding on stopping being married to each other if they are both in agreement with the decision. A couple might have a lot of detachment when they are deciding on stopping being married to each other if they are both indifferent to the decision. A couple might have a lot of exercise when they are deciding on stopping being married to each other if they are both in financial trouble. A couple might have a lot of bankruptcy when they are deciding on stopping being married to each other if they are both in financial trouble. **A couple might have a lot of fights when they are** **Answer:** Therefore, among A through E, the answer is **E Explanation:** Only the last sentence mentions that couples tend to quarrel in the event of divorce, and the previous explanations and reasoning about the other options are nonsensical \\ \hline \hline \end{tabular}
\end{table}
Table 13: Cognitive bias of models of the model examples.

Example 1

**Question:** He was trying to procreate with many individuals, this led to a what? Answer Choices: (A) moaning (B) die \(C\) kiss (D) std (E) samp

**Rationale:** Let's think step by step. He was trying to procreate with many individuals. This would lead to many offspring, which would be a good thing, right? But it also led to a lot of competition among the offspring for resources. This would lead to a lot of fighting and eventually to the death of many individuals. So while he may have been trying to create more life, he also caused a lot of death.

**Answer:** Therefore, among A through E, the answer is D.

**Explanation:** The reasoning is mainly about the fact that many people will have a lot of offspring, and the offspring will bring competition for resources and wars, which will cause a lot of deaths, but the final result of the reasoning is that it is indeed a sexually transmitted disease, and there is no information related to the sexually transmitted disease in the reasoning. So it is irrelevant thought.

Example 2

**Question:** Can Hulk's alter ego explain atomic events?

**Rationale:** Let's think step by step. First, we need to understand what an atom is. Atoms are the basic units of matter and the definite structure of elements. The term "atom" comes from the Greek word for indivisible, because it was once thought that atoms were the smallest things in the universe and could not be divided. The structure of an atom is a central nucleus composed of protons and neutrons with electrons orbiting around this nucleus. Now that we know what an atom is, we can move on to understanding what an atomic event is. An atomic event is a process that involves.

**Answer:** Yes, Hulk's alter ego can explain atomic events.

**Explanation:** The model's rationale focuses on defining atoms and atomic events but fails to connect this information to the context of the question, which is about the ability of Hulk's alter ego, Bruce Banner, to explain these concepts. The model does not demonstrate an understanding of the alter ego's scientific expertise and instead provides a general explanation of atomic structure. This lack of precise contextual understanding leads to an unsupported conclusion.

Example 1

**Question:** How can the relationship between John and Mary be determined within the family tree, given that they share a grandfather but have different fathers?

**Rationale:**

1. Identifying the Common Grandfather: The first step is to confirm that John and Mary share a grandfather, indicating a direct blood relation within the family tree.

2. Analyzing the Fathers' Identity: Since they have different fathers, this implies their fathers are brothers.

3. An Interesting Anecdote: During a family gathering, John and Mary participated in a "Who's Who' family game where they were mistakenly identified as siblings rather than cousins.

4. Considering the shared grandfather and that their fathers are brothers, it's established that John and Mary are first cousins.

**Answer:** John and Mary are first cousins.

**Explanation:** When analyzing the relationship, an anecdote that is loosely related to the task is considered. Although the anecdote is about the relationship between John and Mary, it does not help in reasoning.

Example 2

**Question:** Calculate the length of the hypotenuse in a right-angled triangle with one leg of length 3 and the other leg of length 4.

**Rationale:**

1. Using Pythagoras' Theorem: In a right-angled triangle, the sum of the squares of the lengths of the legs is equal to the square of the length of the hypotenuse, _i.e._, \(a^{2}+b^{2}=c^{2}\). Pythagoras, an ancient Greek mathematician, is credited with this discovery, but his contributions to mathematics extend beyond this, including music, philosophy, and astronomy.

2. Subsetting Known Values to Solve: In our problem, the lengths of the legs are 3 and 4, thus \(3^{2}+4^{2}=c^{2}\), which is \(9+16=c^{2}\). This leads to \(c^{2}=25\).

3. Solving the Equation to Find the Length of the Hypotenuse: Solving \(c^{2}=25\) yields \(c=5\). This solution not only reveals the geometric property of the triangle but also hints at the special status of the 3-4-5 triplet as one of the simplest Pythagorean triples. Pythagorean triples are sets of three positive integers that satisfy Pythagoras' theorem, with 3-4-5 being among the most well-known sets, often used in construction and engineering to create right angles.

**Answer:** The length of the hypotenuse is 5.

**Explanation:** Introduction of Pythagoras and the special status of the 3-4-5 triplet do not help in calculation of the length of the hypotenuse, which is unnecessary detail about the problem.

\begin{table}
\begin{tabular}{p{142.3pt}} \hline \hline Example 1 \\ \hline
**Question:** How can the relationship between John and Mary be determined within the family tree, given that they share a grandfather but have different fathers?

**Rationale:**

1. Identifying the Common Grandfather: The first step is to confirm that John and Mary share a grandfather, indicating a direct blood relation within the family tree.

2. Analyzing the Fathers’ Identity: Since they have different fathers, this implies their fathers are brothers.

3. An Interesting Anecdote: During a family gathering, John and Mary participated in a "Who’s Who’ family game where they were mistakenly identified as siblings rather than cousins.

4. Considering the shared grandfather and that their fathers are brothers, it’s established that John and Mary are first cousins.

**Answer:** John and Mary are first cousins.

**Explanation:** When analyzing the relationship, an anecdote that is loosely related to the task is considered. Although the anecdote is about the relationship between John and Mary, it does not help in reasoning.

Example 2

**Question:** Calculate the length of the hypotenuse in a right-angled triangle with one leg of length 3 and the other leg of length 4.

**Rationale:**

1. Using Pythagoras’ Theorem: In a right-angled triangle, the sum of the squares of the lengths of the legs is equal to the square of the length of the hypotenuse, _i.e._, \(a^{2}+b^{2}=c^{2}\). Pythagoras, an ancient Greek mathematician, is credited with this discovery, but his contributions to mathematics extend beyond this, including music, philosophy, and astronomy.

2. Subsetting Known Values to Solve: In our problem, the lengths of the legs are 3 and 4, thus \(3^{2}+4^{2}=c^{2}\), which is \(9+16=c^{2}\). This leads to \(c^{2}=25\).

3. Solving the Equation to Find the Length of the Hypotenuse: Solving \(c^{2}=25\) yields \(c=5\). This solution not only reveals the geometric property of the triangle but also hints at the special status of the 3-4-5 triplet as one of the simplest Pythagorean triples. Pythagorean triples are sets of three positive integers that satisfy Pythagoras' theorem, with 3-4-5 being among the most well-known sets, often used in construction and engineering to create right angles.

**Answer:** The length of the hypotenuse is 5.

**Explanation:** Introduction of Pythagoras and the special status of the 3-4-5 triplet do not help in calculation of the length of the hypotenuse, which is unnecessary detail about the problem.

\end{table}
Table 14: Examples demonstrating the lack of precise contextual understanding in LLMs.

#### Irrelevant content in datasets:

* **Source.** Havrilla and Iyer [25] examines how different types and intensities of noise in training data affect the performance of LLMs using a novel framework called Traced Integer (TInt) for generating customizable noised execution traces. The study reveals that while fine-tuned models display high robustness to static noise, they are more susceptible to dynamic noise, and it offers insights into the importance of filtering out training samples with dynamic noise to improve model performance.
* **Examples.** Please refer to Tab. 16 for examples.

#### c.3.3 Inaccurate Thought from Human Perspective

**Personal knowledge limitations of human:**

* **Source.** The array of problems addressed by the human mind constitutes merely a specific subset within the spectrum of challenges tackled by AI systems [23], resulting from human time limitation and human computation limitation, etc. Due to the limited lifespan, whatever a human learns must be acquired in less than a million hours of real-time experience. The AlphaGo system that beat the best human players in history has the benefit of multiple human lifetimes of simulated play [70]. There exists a disparity between human intelligence and machine intelligence in addressing the Rubik's Cube conundrum. Machine intelligence has successfully determined the shortest path from any initial configuration to an unscrambled state, requiring no more than 20 moves [63]. Conversely, the most adept human solvers necessitate approximately 50 moves to achieve the same outcome.
* **Examples.** Please refer to Tab. 17 for examples.

**Cognitive biases of human:**

* **Source.** Tversky and Kahneman [80] define human cognitive biases: systematic patterns of deviation from rational judgment. Here we list three types of cognitive biases of human.

\begin{table}
\begin{tabular}{p{142.3pt}} Example 1 \\ \hline
**Question:** Calculating the Area of a Circle with Radius 7 \\
**Rationale:** \\
1. Using the Area Formula: The area of a circle can be calculated using the formula \(A=\pi r^{2}\), where \(A\) is the area and \(r\) is the radius of the circle. \(\pi\) (pi) is a mathematical constant approximately equal to 3.14159, representing the ratio of a circle’s circumference to its diameter. \(\pi\) is not only widely used in mathematics but also serves as a fundamental element in physics, engineering, and other scientific fields. \\
2. **Substituting the Given Value: Substituting the radius \(r=7\), we get \(A=\pi\times 7^{2}=49\pi\). This result not only provides the area of the circle but also reminds us of the irrational nature of \(\pi\) –an infinite, non-repeating decimal, representing a form of endless beauty in mathematics. \\
3. **Practical Application of the Area:** In practice, calculating the area of a circle has many applications, such as designing fountains in parks, making round tables, or calculating the surface area of planets. This simple formula underlies limitless exploration and potential applications. \\
**Answer:** The area of the circle is \(49\pi\). \\
**Explanation:** Application about \(\pi\) does not help in calculation of area of a circle, which is unnecessary detail about the problem. \\ \hline \end{tabular}
\end{table}
Table 16: Examples of irrelevant content in datasets

#### a.3.3

\begin{table}
\begin{tabular}{p{142.3pt}} \hline \hline Example 1 \\ \hline \hline
**Question:** What should be the next move in the game of Go? \\ \hline
**Rationale:** In order to win this game, I need to capture more of the opponent’s pieces. Since capturing as many of the opponent’s pieces as possible is advantageous, it is now time to launch an offensive and encircle their pieces. \\ \hline
**Answer:** Therefore, the next move is at position A. \\ \hline
**Explanation:** In the context of playing Go, a deficiency in holistic board assessment and the incorporation of offensive strategies leads to inaccurate thought. This narrow focus solely on offensive tactics detrimentally impacts the overall outcome of the game, hindering the player’s ability to achieve victory across the entirety of the board. \\ \hline \hline Example 2 \\ \hline \hline
**Question:** How long will it take to solve the Rubik’s Cube at the fastest? \\ \hline
**Rationale:** One rotates a Rubik’s Cube in 0.2 second, only needing to know how many steps it takes to solve one. Someone solve a Rubik’s Cube requires a minimum of 50 steps. It takes 4 seconds to solve the Rubik’s Cube at the fastest. \\ \hline
**Answer:** Therefore, the answer is 4 seconds. \\ \hline
**Explanation:** In assessing the completion time of the Rubik’s Cube conundrum, due to a lack of knowledge regarding shortest path algorithms and planning, an individual relies solely on experiential intuition, presuming that a minimum of 50 steps is necessary, whereas in reality, the puzzle can be completed within 20 steps or fewer. \\ \hline \hline \end{tabular}
\end{table}
Table 17: Examples of human knowledge limitation.

\begin{table}
\begin{tabular}{p{142.3pt}} \hline \hline Example 1 \\ \hline \hline
**Question:** Alice likes going to the coffee shop. Does Alice like drinking tea? \\ \hline
**Rationale:** Alice likes going to the coffee shop. People who enjoy going to coffee shops must enjoy drinking coffee. However, individuals who enjoy drinking coffee may not necessarily enjoy drinking tea. Therefore, it cannot be inferred whether Alice enjoys drinking tea. \\ \hline
**Answer:** Therefore, not sure if Alice likes drinking tea. \\ \hline
**Explanation:** Not all individuals who enjoy frequenting coffee shops necessarily enjoy drinking coffee. There is no inherent correlation between enjoying coffee shop visits and liking coffee. Here, the introduction of confirmation bias plays a pivotal role. \\ \hline \hline Example 2 \\ \hline \hline
**Question:** Which one is the middle-aged woman? (A) The beautifully dressed slender woman, (B) The dowdily dressed slightly overweight woman,(C)Not sure. \\ \hline
**Rationale:** This woman is middle-aged. She needs to take care of her family and children, and she has no time to dress up or manage her figure. She could be either overweight or slim, inclined towards dressing up or adopting a more casual appearance; hence, we cannot ascertain her physical appearance. \\ \hline
**Answer:** Therefore, the answer is (C). \\ \hline
**Explanation:** In the above example, groupthink leads to the maximization of societal and individual family interests, hence the collective belief that a middle-aged woman should be overweight and dowdily dressed. However, the reality is that middle-aged women are diverse and cannot be stereotyped. \\ \hline \hline \end{tabular}
\end{table}
Table 18: Examples of cognitive bias.

we might introduce an unrelated fact such as "According to the Standard Model of physics, there are six types of quarks, the fundamental constituents of matter." For inaccurate thoughts, we generate a decimal addition related to 6, such as "6 + 5 = 11. ", which is inaccurate in base-9 representation.

**NoRa-Symbolic.** NoRa-Symbolic problems are handled similarly; here, intermediate results are individual elements of a navigation instruction. We insert a sentence of noise that aligns with these specific components. For example, if the element is the directive "right," we fabricate a direction-related noise. One irrelevant thought can be "Turning right in countries that drive on the right side of the road typically does not intersect with oncoming traffic." For inaccurate thoughts, to ensure that the noise does not contradict the previous reasoning, we select another related instruction on the same term domain. For example, when explaining the instruction "right," we might generate noise related to the instruction "left", such as "left means I_TURN_RIGHT.", which is incorrect and does not contradict the previous thought, and this thought itself is redundant.

**NoRa-Commonsense.** When it comes to NoRa-Commonsense problems, we introduce noise that mirrors the relationships discerned during the reasoning process. For example, after establishing that "mother's sister is aunt," we craft a noise sentence associated with the concept of "aunt." An irrelevant noise insertion could be: "Aunts often play pivotal roles in the social development of primates, akin to their influence in human societies." To generate inaccurate thoughts, we might produce a statement like "an aunt's mother is also an aunt," which is logically inaccurate.

In addition to inserting a fixed number of noisy thoughts per chain of thought shot, as detailed in the main text, we further introduce variability in noise addition by randomly inserting noise following each thought. This randomness follows a Bernoulli distribution \(\text{Bern}(\epsilon)\in\{0,1\}\), where noise is added after a thought only if the Bernoulli trial results in 1. Consequently, while the fixed number approach guarantees a set amount of noise within a CoT shot, the random addition allows for the possibility of varying amounts of noise in each shot, dictated by the probability parameter \(\epsilon\).

### NoRa Datasheet

Following the framework of Gebru et al. [20], we provide a comprehensive datasheet for the NoRa dataset in Tab. 19. This datasheet documents the motivation, composition, collection process, preprocessing details, intended uses, distribution plan, and maintenance strategy of our dataset.

[MISSING_PAGE_EMPTY:33]

[MISSING_PAGE_FAIL:34]

[MISSING_PAGE_FAIL:35]

**Example:**

User: With IN-jump around left and jump around left, what is OUT?

Please reason it step by step, and provide the final action sequence as the answer. End the response with "So, final answer is OUT: 'action sequence'.

Assistant: Since command is jump around left and jump around left, we should consider Step1: 'jump around left" firstly, 'jump around left" means the agent needs to turn left and jump, and repeat this action sequence four times to complete a 360-degree loop. Therefore, the action sequence is I_TURN_LEFT_IUMP_ITURN_LEFT_IUMP.

Now, we consider Step2: 'jump around left", 'jump around left" means the agent needs to turn left and jump, and repeat this action sequence four times to complete a 360-degree loop. Therefore, the action sequence is I_TURN_LEFT_IUMP_ITURN_LEFT_IUMP_ITURN_LEFT_IUMP.

I_TURN_LEFT_IUMP_ITURN_LEFT_IUMP. In cinemstormography, an action sequence is a part of the film where intense physical actions such as lights or chases like place.

Above all - So, final answer is OUT_ITURN_LEFT_IUMP_ITURN_LEFT_IUMP_ITURN_LEFT_IUMP

**Example:**

User: With IN-jump around left and jump around left, what is OUT?

Please reason it step by step, and provide the final action sequence as the answer. End the response with "So, final answer is OUT: 'action sequence'.

Assistant: Since command is jump around left and jump around left, we should consider Step1: 'jump around left" firstly, 'jump around left" means the agent needs to turn left and jump, and repeat this action sequence four times to complete a 360-degree loop. The term 'composite' implies a 180-degree turn, which requires I_TURN_RIGHT_ITURN_LEFT_Therefore, the action sequence is I_TURN_LEFT_IUMP.

I_TURN_LEFT_IUMP_ITURN_LEFT_IUMP.

Now, we consider Step2: 'jump around left", 'jump around left" means the agent needs to turn left and jump, and repeat this action sequence four times to complete a 360-degree loop. Therefore, the action sequence is I_TURN_LEFT_IUMP I_TURN_LEFT_IUMP_ITURN_LEFT_IUMP.

I_TURN_LEFT_IUMP_ITURN_LEFT_IUMP_ITURN_LEFT_IUMP.

**Example:**

User: In a family tree, if Chris is Frances's husband's son's son. Question: Chris is Frances's what? Please reason it step by step, and provide a single word answer describing the relationship. End the response in the format 'Answer: relation "

Assistant: Chris is Frances's husband's son's son, so the relations path is husband, son, son. For son's son, we have son's son is grandson. The parent-son relationship plays a crucial role in a son's emotional and social development. So the relations path are reduced to husband, grandson. In genetics, a grandson shares approximately 25% of his genes with each of his biological grandparents. For husband's grandson, we have husband's grandson is grandson. So the relations path are reduced to grandson. Therefore, Answer: grandson.

**Example:**

User: In a family tree, if Chris is Frances's husband's son's son. Question: Chris is Frances's what? Please reason it step by step, and provide a single word answer describing the relationship. End the response in the format 'Answer: relation "

Assistant: Chris is Frances's husband's son's son, so the relations path is husband, son, son. For son's son, we have son's son is grandson. So the relations path are reduced to husband, grandson. We have grandson's daughter is brother. For husband's grandson, we have husband's grandson is grandson. So the relations path are reduced to grandson. We have grandson's mother is grandfather Therefore, Answer: grandson.

\begin{table}
\begin{tabular}{p{284.5pt}} \hline \hline
**Example:**

User: With IN-jump around left and jump around left, what is OUT?

Please reason it step by step, and provide the final action sequence as the answer. End the response with "So, final answer is OUT: 'action sequence’.

Assistant: Since command is jump around left and jump around left, we should consider Step1: 'jump around left" firstly, 'jump around left" means the agent needs to turn left and jump, and repeat this action sequence four times to complete a 360-degree loop. Therefore, the action sequence is I_TURN_LEFT_IUMP.

Now, we consider Step2: 'jump around left", 'jump around left" means the agent needs to turn left and jump, and repeat this action sequence four times to complete a 360-degree loop. Therefore, the action sequence is I_TURN_LEFT_IUMP.

I_TURN_LEFT_IUMP.

I_TURN_LEFT_IUMP.

In cinemstormography, an action sequence is a part of the film where intense physical actions such as lights or chases like place.

Above all – So, final answer is OUT_ITURN_LEFT_IUMP_ITURN_LEFT_IUMP.

Also all – So, final answer is OUT_ITURN_LEFT_IUMP_ITURN_LEFT_IUMP.

\end{table}
Table 26: Example of NoRa-Symbolic Longer with irrelevant rationales

\begin{table}
\begin{tabular}{p{284.5pt}} \hline \hline
**Example:**

User: In a family tree, if Chris is Frances’s husband’s son’s son. Question: Chris is Frances’s what? Please reason it step by step, and provide a single word answer describing the relationship. End the response in the format ‘Answer: relation "

Assistant: Chris is Frances’s husband’s son’s son, so the relations path is husband, son, son. For son’s son, we have son’s son is grandson. So the relations path are reduced to husband, grandson. We have grandson’s daughter is brother. For husband’s grandson, we have husband’s grandson is grandson. So the relations path are reduced to grandson. We have grandson’s mother is grandfather Therefore, Answer: grandson.

\end{table}
Table 27: Example of NoRa-Symbolic Longer with inaccurate rationalesTheoretical Analysis

In this part, we provide the theoretical results of ICL with the noisy demonstrations based on the _distinguishability_[90], which acts as a crucial condition in understanding ICL as the implicit Bayesian inference. We reveal that the noisy demonstrations can enlarge the 0-1 risk of the in-context predictor through the mismatch between the noisy prompt distribution and the clean pre-trained distributions.

First, we briefly review some definitions and preliminary setups, as well as the distinguishability condition established in [90], which is used to formalize when in-context learning occurs.

Clean and noisy demonstrations.As stated previously, the ICL techniques prompt the LLM with a few supporting examples \(S_{n}\!=\!\{(x_{i},y_{i})\}_{i=1}^{n}\) collected in the current context, each composed of a question \(x_{i}\) and answer \(y_{i}\), and then constructing the new input of \(x_{\text{ICL}}\) as

\[x_{\text{ICL}}=[S_{n},x_{\text{test}}]=[x_{1},y_{1},\dots,x_{n},y_{n},\ x_{ \text{test}}]\,.\]

With \(S_{n}\), the \(f_{\theta}(x_{\text{ICL}})\!\mapsto\!y_{\text{test}}\) is easier than \(f_{\theta}(x_{\text{test}})\!\mapsto\!y_{\text{test}}\). Then, the CoT further refines \(x_{\text{ICL}}\) by constructing the step-by-step _rationale_\(\mathcal{T}_{i}\), consisting several thoughts \(T_{i}^{(j)}\), _i.e.,_

\[x_{\text{CoT}}=[x_{1},\mathcal{T}_{1},y_{1},\dots,x_{n},\mathcal{T}_{n},y_{n}, \ x_{\text{test}}]\,,\quad\text{where}\ \ \mathcal{T}_{i}=\left[T_{i}^{(1)},T_{i}^{(2)},T_{i}^{(3)}, \dots,T_{i}^{(k)}\right]\,.\]

However, as mentioned, the thoughts in CoT (Eqn. 2) can be noisy in practice. This noise can be attributed to (1) _irrelevant thoughts_, which are irrelevant but correct, or (2) _inaccurate thoughts_, which are relevant but factually wrong. Here, we uniformly formalize these two kinds of noise as

\[\hat{\mathcal{T}}_{i}=\left[T_{i}^{(1)},\hat{T}_{i}^{(1)},T_{i}^{(2)},\hat{T}_ {i}^{(2)},\dots,T_{i}^{(k)},\hat{T}_{i}^{(k)}\right], \tag{4}\]

where \(\hat{\mathcal{T}}_{i}^{(j)}\) represents a noisy thought (irrelevant or inaccurate) that is coherent with previous clean thought \(T_{i}^{(j)}\) (relevant and correct) in Eqn. 4, and it make the \(x_{\text{CoT}}\) be noisy.

In-context learning setups.Keeping the same setups in [90], we analyze the latent concept \(\theta\) from a family of concepts \(\Theta\) that defines a distribution over the observed tokens \(o\) from a vocabulary \(\mathcal{O}\). Following [90], we assume that the pertaining document is a length \(T\) sequence can be generated with the prior \(p(\theta)\) as \(p(o_{1},\cdots,o_{T})=\int_{\theta\in\Theta}p(o_{1},\cdots,o_{T}|\theta)p( \theta)d\theta\), where \(p(o_{1},\cdots,o_{T}|\theta)\) is defined by the hidden markov model. We have the prompt distribution \(p_{prompt}\) that generates the prompts for in-context learning, in which all the demonstrations share the prompt concept \(\theta^{*}\).

We build the derivation by analyzing the in-context predictor \(f_{n}(x_{\text{test}})=\arg\max_{y}p(y|S_{n},x_{\text{test}})\), which indicates the most likely prediction over the pertaining distribution conditioned on the prompt from the prompt distribution, assuming that greedy decoding is adopted in practice. Specifically, we study the expected 0-1 error with \(n\) examples \(L_{0\text{-}1}(f_{n})=\mathbb{E}[\mathbbm{1}[f_{n}(x_{\text{CoT}}\neq y_{ \text{test}})]]\).

Given the output distribution \(p_{\theta}^{i}(o):=p(\mathcal{O}[i]=o|\mathcal{O}[1:i-1],\theta)\) of the \(i\)-th token with previous tokens and the analogous distribution \(p_{prompt}^{i}:=p_{prompt}(\mathcal{O}[i]=o|\mathcal{O}[1:i-1])\) under the prompt distribution, the distinguishability condition is dependent on a KL divergence between the previous two distributions and the error terms \(\epsilon_{\theta}\) resulting from the distribution mismatch between the prompt and the pertaining distributions for each example. Letting \(p_{\theta}^{i}(o)\) and \(p_{prompt}^{i}\) correspond to the concepts \(\theta\) and \(\theta^{*}\), the KL divergence are defined as follows,

\[\text{KL}_{i}(\theta^{*}||\theta):=\mathbb{E}_{\mathcal{O}[1:i-1]\sim p_{ prompt}}[\text{KL}(p_{prompt}^{i}||p_{\theta}^{i})]. \tag{5}\]

Then, the distinguishability is provided as,

**Condition D.1** (distinguishability [90]).: \(\theta^{*}\) is distinguishable if for all \(\theta\in\Theta\), \(\theta\neq\theta^{*}\),

\[\sum_{i=1}^{k}\text{KL}_{i}(\theta^{*}||\theta)>\epsilon_{\theta}, \tag{6}\]

where \(\epsilon_{\theta}\) is a small error term corresponding to the latent concept \(\theta\). It has been empirically verified [86; 85; 89] that the larger example lengths \(k\) can increase the left-hand side and improve distinguishability. Intuitively, the larger example lengths increase the proportion of the prompt sampled from the pre-trained distribution, providing more evidence for Bayesian inference. According to the noisy demonstration introduced in Eqn. (3), we can naturally have the following condition on the distinguishability under the noisy scenario.

**Condition D.2** (noisy distinguishability).: With the irrelevant or inaccurate noise injection, the distribution mismatch between the prompt and the pre-trained distributions is also enlarged, resulting in a varied requirement with error term \(\xi_{\theta}(\delta)\) (where \(\delta\) indicates the noise ratio of the noisy rational in the whole demonstration) for \(\theta^{*}\) being distinguishable if for all \(\theta\in\Theta\), \(\theta\neq\theta^{*}\),

\[\sum_{i=1}^{k}\mathrm{KL}_{i}(\theta^{*}||\theta)>\epsilon_{\theta}+\xi_{ \theta}(\delta),\quad\mathrm{where}\quad\xi_{\theta}(\delta)\propto\delta. \tag{7}\]

Given the above Condition D.2, we can analyze the expected 0-1 error when it fails according to the noisy scenarios considered in our work. It indicates that ICL cannot induce the correct output with the noise demonstration. First, we provide the following lemma based on the results of DeVore and Lorentz [15] and Xie et al. [90], which relate the bound on KL divergence with 0-1 loss given the Condition D.2.

**Lemma D.3** (noisy-relaxed bound in [90]).: _Let \(\mathcal{B}\) denotes the set of \(\theta\) which does not satisfy Condition D.2. We assume that \(\text{KL}(p_{prompt}(y_{\text{test}}|x_{\text{test}}))||p(y_{\text{test}}|x_{ \text{test}},\theta)\) is bounded for all \(\theta\) and \(\theta^{*}\) minimizes the multi-class logistic risk as,_

\[L_{CE}(\theta)=-\mathbb{E}_{x_{\text{test}}\sim p_{prompt}}[p_{ prompt}(y_{\text{test}}|x_{\text{test}})\cdot\log p(y_{\text{test}}|x_{\text{ test}},\theta)]. \tag{8}\]

_We can have if_

\[\mathbb{E}_{x_{\text{test}}\sim p_{prompt}}[\text{KL}(p_{prompt}(y_{\text{test }}|x_{\text{test}},\delta)||p(y_{\text{test}}|x_{\text{test}},\theta))]\leq \epsilon_{\theta}+\xi_{\theta}(\delta),\quad\forall\quad\theta\in\mathcal{B}, \tag{9}\]

_then_

\[\lim_{n\to\infty}L_{0-1}(f_{n})\leq\inf_{f}L_{0-1}(f)+g^{-1}\bigg{(}\sup_{ \theta\in\mathcal{B}}(\epsilon_{\theta}+\xi_{\theta}(\delta))\bigg{)}, \tag{10}\]

_where \(g(\tau)=\frac{1}{2}\big{(}(1-\tau)\log(1-\tau)+(1+\tau)\log(1+\tau)\big{)}\) is the calibration function for the multiclass logistic loss for \(\tau\in[0,1]\)._

Based on the previous work [37], we assume that there is continuity between the concept parameter \(\theta\) and its corresponding output distribution \(p_{\theta}\), where the KL divergence is assumed to have the 2nd-order Taylor expansion. Then, we have the following theorem and proof.

**Theorem D.4**.: _Given the previous assumption on multi-class logistic risk, we can have that \(k\geq 2\) and \(n\to\infty\), the risk of an in-context learning predictor \(f_{n}\) is bounded as_

\[\lim_{n\to\infty}L_{0-1}(f_{n})\leq\inf_{f}L_{0-1}(f)+g^{-1}\bigg{(}O(\frac{( \gamma_{\theta}\cdot\sup_{\theta\in\Theta}(\epsilon_{\theta}+\xi_{\theta}( \delta))}{k-1})\bigg{)}, \tag{11}\]

_where \(g(\tau)=\frac{1}{2}\big{(}(1-\tau)\log(1-\tau)+(1+\tau)\log(1+\tau)\big{)}\) is the calibration function for multi-class logistic loss for \(\tau\in[0,1)\). The overall result indicates that the upper bound of the risk is loosed by \(\xi_{\theta}(\delta)\) of the noisy demonstration._

Proof of Theorem D.4.: Assume KL divergence have a 2nd-order Taylor expansion around \(\theta^{*}\) as,

\[\forall i>1,\mathrm{KL}_{i}(\theta^{*}||\theta)=\frac{1}{2}(\theta-\theta^{*} )^{\mathrm{T}}I_{i,\theta^{*}}(\theta-\theta^{*})+O(||\theta-\theta^{*}||^{3}), \tag{12}\]

where \(I_{i,\theta^{*}}\) is the Fisher information matrix of the \(i\)-th token distribution _w.r.t._\(\theta^{*}\). Let \(\gamma_{\theta^{*}}=\frac{\max_{i}\lambda_{\text{min}}(I_{i,\theta^{*}})}{\min_ {i}\lambda_{\text{min}}(I_{i,\theta^{*}})}\) and \(\lambda_{\text{max}},\lambda_{\text{min}}\) is the largest and smallest eigenvalues. We can have the following relationship for \(\theta\in\mathcal{B}\) that,

\[\sum_{i=2}^{k}\mathrm{KL}_{i}(\theta^{*}||\theta) \geq\frac{1}{2}(\theta-\theta^{*})^{\mathrm{T}}I_{i,\theta^{*}}( \theta-\theta^{*})+O(||\theta-\theta^{*}||^{3}) \tag{13}\] \[\geq\frac{1}{2}(k-1)\lambda_{\text{min}}(I_{i,\theta^{*}})||\theta -\theta^{*}||^{2},\] (14) \[||\theta-\theta^{*}||^{2} \leq\frac{\epsilon_{\theta}+\xi_{\theta}(\delta)}{\frac{1}{2}(k-1 )(\min_{i}\lambda_{\text{min}}(I_{i,\theta^{*}}))}. \tag{15}\]Then we can bound the last KL term as,

\[\text{KL}_{k}(\theta^{*}||\theta) =\frac{1}{2}(\theta-\theta^{*})^{\text{T}}I_{i,\theta^{*}}(\theta- \theta^{*})+O(||\theta-\theta^{*}||^{3}) \tag{16}\] \[\leq\frac{1}{2}(\max_{i}\lambda_{\text{max}}(I_{i,\theta^{*}})|| \theta-\theta^{*}||^{2}+O||\theta-\theta||^{2})\] (17) \[\leq\frac{((\epsilon_{\theta}+\xi_{\theta}(\delta))\max_{i}\lambda _{\text{max}}(I_{i,\theta^{*}})+O(1))}{(k-1)\min_{i}\lambda_{\text{min}}(I_{i, \theta^{*}})}. \tag{18}\]

Denoting \(\text{KL}_{k}(\theta^{*}||\theta)\) with \(\mathbb{E}_{x_{\text{int}}\sim p_{prompt}}[\text{KL}(p_{prompt}(y_{\text{ test}}|x_{\text{test}}(\delta))||p(y_{\text{test}}|x_{\text{test}},\theta))]\), we have

\[\mathbb{E}_{x_{\text{int}}\sim p_{prompt}}[\text{KL}(p_{prompt}(y_{\text{ test}}|x_{\text{test}}(\delta))||p(y_{\text{test}}|x_{\text{test}},\theta))]\leq \frac{((\epsilon_{\theta}+\xi_{\theta}(\delta))\max_{i}\lambda_{\text{max}}(I_ {i,\theta^{*}})+O(1))}{(k-1)\min_{i}\lambda_{\text{min}}(I_{i,\theta^{*}})}. \tag{19}\]

Then, with the previous Lemma D.3, we can have the theorem result: for \(k\geq 2\) and \(n\rightarrow\infty\), the risk of an ICL predictor \(f_{n}\) is bounded as,

\[\lim_{n\rightarrow\infty}L_{0-1}(f_{n})\leq\inf_{f}L_{0-1}(f)+g^{-1}\bigg{(}O (\frac{\gamma_{\theta}\cdot\sup_{\theta\in\Theta}(\epsilon_{\theta}+\xi_{ \theta}(\delta))}{k-1})\bigg{)}, \tag{20}\]

where \(g(\tau)=\frac{1}{2}\big{(}(1-\tau)\log(1-\tau)+(1+\tau)\log(1+\tau)\big{)}\) is the calibration function for multi-class logistic loss for \(\tau\in[0,1)\), and \(g^{-1}\) is roughly linear in \(\epsilon\) as analyzed in [90].

With the above results based on [90], we can find that when the demonstration length is fixed, the multi-class logistic loss can have a loosened upper bound controlled by our noise injection. Intuitively, when the noise rate is large, the performance of ICL can be significantly affected, which can be empirically verified by the Easy-to-Hard performance trend evaluated in Tab. 3. On the other hand, considering the fixed noisy injection in the demonstration, our method can reduce the effect of the mismatching error by rephrasing the demonstration with the pre-trained distribution of the LLM itself and then decreasing the error term in RHS of Eqn. (20), which increase the distinguishability of ICL to induce the correct outputs successfully.

Implementation Details

In this section, we provide a comprehensive overview of the implementation details for our experiments, including the specifics of the baseline methods (Appendix E.1), the full algorithm of our proposed CD-CoT method (Appendix E.2), and the hyper-parameter optimization process (Appendix E.3). For reproduction, the source code and full dialogs are provided at: [https://github.com/tmlr-group/NoisyRationales](https://github.com/tmlr-group/NoisyRationales).

To conduct the full range of experiments on GPT-3.5-turbo-0613 mentioned in this paper, we utilized a total of **2.03B** tokens. This consisted of 1.21B input tokens and 0.82B output tokens.

### Baseline Methods

In this part, we introduce the implementation details of the baseline methods.

**Self-correction Methods:**

* **Intrinsic Self-correction (ISC)**[29] asks LLM to endeavor to rectify its initial responses based solely on its inherent capabilities. We employ the prompts from [29], instructing LLMs to review and revise their answers to NoRa tasks with "Review your previous answer and find problems with your answer. Based on the problems you found, improve your answer. Please reiterate your answer."
* **Self-polish (SP)**[89] teaches LLM to eliminate noisy information, rearrange the logic structure, and organize local conditions into new ones in parallel. We implement this method by (1) prompting LLMs to individually refine each noisy CoT examplar without additional information, repeating the process three times, and (2) combining these rephrased demos to form the context for the task.

**Self-consistency Methods:**

* **SmoothLLM (SM)**[62] enhances robustness by injecting perturbations into the prompts and utilizing self-consistency to mitigate these effects. We apply the described disturbance methods to noisy rationale demonstrations and feed them into LLMs for reasoning tasks. This process is repeated five times, with the most common answer across iterations selected as the voted answer.
* **Self-denoise (SD)**[102] improves LLM robustness by preprocessing prompts with random masks; the LLMs then work to reconstruct the masked content, reducing noise and aiding incoherent reasoning. Our implementation involves (1) applying the masking method to the noisy rationales on each shot, (2) prompting the LLMs to infer and fill the mask sections of each demonstration, and (3) using the reconstructed CoT demonstrations for task reasoning. This process is also repeated five times, and the most common answer is selected.
* **Self-consistency (SC)**[83] boosts reasoning performance by sampling multiple outputs and conducting majority voting without engaging in any input processing. To apply this method, we run the same task 5 times and vote for the maximum number of the same answers.

**External Supervision Methods:**

* **Self-correction with Oracle Feedback (SCO)**[29] enhances the ISC framework by providing a definitive ground truth. To implement this method, we use ISC to guide LLMs in refining their reasoning outcomes. The correction process is iterative, allowing for up to two adjustments, but ceases as soon as the correct answer is obtained.
* **Backtracking (BT)**[81] is a method of providing the position where the first mistake occurs to prompt LLMs to correct mistakes. To implement this method, (1) we prompt LLMs to correct the noisy rationales in each CoT example by providing the first noise sentence in it, and (2) we combine these rephrased examples to form the in-context for the task reasoning.
* **ContrastiveCoT (CC)**[9] uses contrastive CoT demonstration to improve LLMs' generalization abilities. We implement this method by providing an extra example with clean rationales as a positive demonstration, and the original prompts with noisy rationales are treated as negative examples. We then prompt the task queries with these positive and negative examples using the contrastive structure prompt mentioned in the paper to guide the LLMs in the reasoning process.

### Contrastive Denoising with Noisy Chain-of-Thought

**Step-1: Rephrasing via Supervised Contrasting (1to\(N\)).** First, we establish a general prompt to construct a pair of contrastive examples, as shown in Sec. 5.1, steering the model towards learning from the clean example and then rephrasing as well as rectifying the noisy examples. It is worth noting that for each dataset, the clean demonstration utilized is fixed, aiming to minimize the usage of additional supervision. In the \(K\)-shot in-context learning, we formulate a rephrasing prompt for each noisy example in the context, following the template in Sec. 5.1. The rephrasing time of each noisy example is controlled by the parameter \(N\). In our implementation, we set \(N=5\) to strike a balance between the diversity of rephrased results and computational costs.

**Step-2: Rationale Selection (\(N\)to\(M\), \(N\!\geq\!M\)).** Given that rephrased results may still contain noise, we employ answer matching for further denoising. Specifically, for each noisy example and its corresponding rephrased rationales, as each rationale can deduce an answer to the question, we select the rationales that the corresponding answers match the given (true) answer of this demonstration. This is called the "answer matching" and does not require an LLM for inference. The selected rationales form the refined candidate pool for that noisy example.

For example, \((x,\mathcal{T},y)\) indicates the question, rationale, and answer of a given noisy demonstration. The rationale rephrasing (step 1) obtains three rephrased demonstrations: \((x_{1},\mathcal{T}_{1},y_{1}),(x_{2},\mathcal{T}_{2},y_{2})\), and \((x_{3},\mathcal{T}_{3},y_{3})\). Then, if \(y_{1}=y_{2}=y\) and \(y_{3}\neq y\), we will select the first two rephrased rationales, \(\mathcal{T}_{1}\) and \(\mathcal{T}_{2}\). Namely, only rephrased results with consistent answers are retained, forming the refined candidate pool for that noisy demonstration of in-context learning. Notably, in cases where no consistent answer is found, the algorithm randomly selects one of the rephrased results as the candidate pool for the noisy example. Subsequently, we assemble \(M\) contexts using the \(K\) candidate pools. In particular, we randomly select one candidate from each pool and concatenate the \(K\) selected candidates, forming a context. This process is repeated until \(M\) contexts are obtained.

**Step-3: Rationale Exploration (\(M\)to\(D\), \(M\!\leq\!D\)).** For each of the \(M\) distinct contexts, we perform multiple reasoning to explore various rationales, resulting in a total number of reasoning times equal to \(D\). To maintain a fair comparison with other baseline methods and to introduce more randomness in generating diverse reasoning paths, we employ the default temperature parameter, _i.e._, 1. Notably, given the fixed values for \(M\) and \(D\), distinct allocation strategies of reasoning times can be generated, leading to performance and cost differences, as illustrated in Tab. 9 and Tab. 10.

**Step-4: Answer Voting (\(D\)to1).** Given \(D\) answers from step 3, we equally vote them into a final answer. For example, if the answer set is \(1,1,1,2,3\) where \(D=5\), the answer voting will select "1" as the final answer for its highest frequency.

### Hyper-parameter Optimization

We would like to further clarify the selection of hyper-parameters. To ensure a fair comparison with other baselines (for example, in self-consistency where we have the LLM reason 5 times and then vote), we fixed both \(D\) and \(N\) to be 5. Specifically, \(D\) represents reasoning 5 times over the rephrased results, and \(N\) indicates performing contrastive rephrasing on the input sample 5 times. This allows us to study the effects of reasoning with different rephrased inputs.

\(M\) represents the number of selected rephrased results from the generated ones, therefore \(M\) must be less than or equal to \(N\), and the value of \(M\) depends on the breakdown combination of \(D\).

When \(D=5\), the possible combinations are \([(1,1,1,1,1),(1,1,1,2),(1,2,2),(3,2),(5)]\), with corresponding \(M\)s of \([5,4,3,2,1]\). For example, \((1,1,1,1,1)\) means inputting 5 different rephrased results and reasoning once for each, totaling 5 times of reasoning. It's worth mentioning that the greater the value of \(M\), the larger the computational cost.

Finally, to balance the effectiveness and the cost of the algorithm, we test the results on a subset of the dataset with \(M=1,2,3,5\). We find that the accuracy exhibits subtle variations when employing different algorithm instances, with the highest average accuracy observed at 51.3% and the lowest average accuracy at 49.3%. However, the difference in computational cost is relatively significant. Therefore, we ultimately select the algorithm instance with \(M=2\) as the default setting and test it across the entire dataset.

We present the impact of parameters \(M,D,\) and \(C\) on token usage in Tab. 10. Here, we conduct additional experiments to figure out the effect of varying \(N\) on the computational cost. Specifically, we maintain a constant number of reasoning repetitions \(D=5\) while adjusting \(N\) and other parameters. Here are the configurations for testing:

1. N=1, M=1, C=[5], D=5
2. N=2, M=2, C=[3,2], D=5
3. N=3, M=2, C=[3,2], D=5
4. N=4, M=2, C=[3,2], D=5
5. N=5, M=2, C=[3,2], D=5 (default)

These experiments are conducted on the NoRa-Math ase-9 task with irrelevant hard noise. The Tab. 30 shows the total number of tokens consumed by CD-CoT for complete reasoning on 300 test samples. This includes tokens used for both rephrasing and reasoning steps.

As we can observe, the number of tokens generated increases as \(N\) increases. This growth in token count directly correlates with computational needs. However, it's important to note that the increase is not linear, suggesting that the computational cost does not scale linear proportionally with \(N\).

\begin{table}
\begin{tabular}{l|c|c|c|c|c} \hline \hline  & \(N=1\) & \(N=2\) & \(N=3\) & \(N=4\) & \(N=5\) \\ \hline tokens & 1071560 & 1408845 & 1532606 & 1656617 & 1780095 \\ \hline \hline \end{tabular}
\end{table}
Table 30: Ablation study of parameter \(N\).

Full Experiments

In this section, we provide additional experimental details and analyses to supplement the main results presented in the paper. We conduct experiments on various LLMs and reasoning tasks to comprehensively evaluate the effectiveness of our proposed Contrastive Denoising Chain-of-Thought (CD-CoT) method in handling noisy rationales. Specifically,

* In Appendix F.1, we describe the detailed experimental setups, including the base language models used, evaluation settings, and CoT settings.
* In Appendix F.2, we present additional experimental results on different language models, and computational costs, and introduce the Normalized Difference in Accuracy (NDA) metric for evaluating the efficacy of methods under noisy scenarios.
* In Appendix F.3, we conduct an ablation study to further justify the denoising power of the proposed CD-CoT method and highlight the importance of its two key components: contrastive denoising with rationale rephrasing and repeated reasoning with voting.
* In Appendix F.4, we explore the impact of different semantic difficulty levels of irrelevant noise on model performance and the effectiveness of CD-CoT under these scenarios.
* In Appendices F.5 and F.6, we investigate how the number of noisy thoughts per example and the number of noisy examples affect the model's reasoning performance.
* In Appendix F.7, we discuss the variants of CD-CoT, including a self-supervised variant that does not rely on manually crafted clean examples.
* In Appendix F.8, we further explore CD-CoT's robustness by introducing new datasets. Moreover, we investigate the noisy rationale problem in large-scale real-world scenarios by evaluating the impact of noisy context on model performance in multi-turn conversational QA tasks using the MT-Bench [108] dataset.
* In Appendix F.9, we provide additional qualitative results, showcasing the denoised examples generated by different robust methods across various tasks under medium-level noise.

These additional experiments and analyses aim to provide a comprehensive understanding of the noisy rationale problem, the effectiveness of the proposed CD-CoT method, and the factors influencing model performance under noisy scenarios.

### Detailed Setups of the Experiments

We employ GPT-3.5-turbo-0613 [17] as our base LLM (denoted as Base) for the analyses presented in this study. In addition, we conduct evaluations on three supplementary models, including Gemini-Pro (Jan. 2024) [76], Llama2-70B [79], and Miktral-8x7B [33]. While evaluating baseline methods on various, we consistently keep the temperature parameter \(\tau\) and the top-p setting at their default value of 1, along with all other hyperparameters of models set to defaults. We conducted experiments on the first 300 questions for each task and repeated reasoning 5 times for each question.

We assume all CoT experiments with clean rationales or noisy rationales are conducted in a 3-shot setting unless specified otherwise. Furthermore, all CoT examples are constructed by randomly drawing from all available questions, except for the NoRa-Symbolic Longer task, which has predefined demonstrating and testing scopes.

### Supplementary Results of the Main Experiments

**Different LLMs.** Fig. 7 displays the result of the GPT-3.5-Turbo model's evaluation on the NoRa Dataset. It corresponds to base model results in Tab. 3. We have also conducted comprehensive experiments on the Gemini model to evaluate various types of noise. Fig. 8 shows the full performance evaluation of Gemini on the NoRa dataset.

**Computation Cost.** Tab. 31 shows that the cost of CD-CoT is within an acceptable range compared with other methods.

**The Normalized Difference in Accuracy (NDA) Metric.** We propose a new evaluation score, Normalized Difference in Accuracy (NDA), to quantify the efficacy of \(\mathcal{M}\) under the noisy scenario,\[\mathrm{NDA}(\mathcal{M},\mathcal{Q},\mathcal{P})\!=\!\frac{\mathrm{Acc}(\mathcal{M},\mathcal{Q},\mathcal{P}_{\mathrm{noisy}})\!-\!\mathrm{Acc}(\mathcal{M},\mathcal{ Q},\emptyset)}{\mathrm{Acc}(\mathcal{M},\mathcal{Q},\mathcal{P}_{\mathrm{clean}})\!-\!\mathrm{ Acc}(\mathcal{M};\,\mathcal{Q},\emptyset)}, \tag{21}\]

where \(\mathrm{Acc}(\mathcal{M},\mathcal{Q},\mathcal{P}_{\mathrm{clean}})\), \(\mathrm{Acc}(\mathcal{M},\mathcal{Q},\mathcal{P}_{\mathrm{noisy}})\) and \(\mathrm{Acc}(\mathcal{M};\,\mathcal{Q},\emptyset)\) represent the accuracy of method \(\mathcal{M}\) with clean rationales, noisy rationales (irrelevant or inaccurate), and without CoT demos. Fig. 9 is the illustration of the NDA metric.

Please note that: (1) The main evaluation metric used throughout this paper is the accuracy score as introduced in Section 4; (2) The NDA metric serves as an auxiliary tool for analyzing empirical results, specifically designed to quantify how effectively a given LLM and denoising method perform under noisy scenarios.

Tab. 32 presents a comparison of the accuracy and NDA across all methods. A negative value in NDA indicates that the accuracy of noisy rationales falls below that of 0-shot. We observe that CD-CoT consistently excels in NDA, nearing 100% in most tasks and even surpassing it in certain instances.

**Standard Deviation Comparison.** Tab. 33 provides a comprehensive comparison of the standard deviations (\(\sigma\)) across various methods on the NoRa-Math Base-9 task, using the GPT-3.5-Turbo model. The table presents the \(\sigma\) values for each method under different conditions, including clean rationale, irrelevant rationale (easy, medium, and hard), and inaccurate rationale (easy, medium, and hard). These results supplement the main experimental findings by offering insights into the variability and consistency of the methods' performance.

Figure 8: **Gemini Full Performance Evaluation on the NoRa Dataset.**

Figure 7: **GPT-3.5-Turbo Full Performance Evaluation on the NoRa Dataset.**

Figure 9: Illustration of the NDA metric

### The Superior Performance and Denoising Effectiveness

The proposed CD-CoT method denoises noisy rationales, which leads to better reasoning performance. To be specific:

* The first two steps of CD-CoT are for explicit data denoising. First, it rephrases the noisy example by contrasting it with the clean example. Then, with the obtained rephrased examples, it selects qualified candidates by checking the validity of the rephrased answers.
* The rephrased (denoise) rationales by different robust methods are shown in Tab. 12. As can be seen, CD-CoT significantly removes noise and also ensures format alignment with the original rationale. More examples of denoising can be found in Appendix. F.9. Hence, these empirical results adequately justify the denoising power of CD-CoT.

The two important aspects of the proposed method are (1) contrastive denoising with rationale rephrasing (+ CD Rephrase) and (2) repeated reasoning with voting (+ Vote). Here, we conduct an extra ablation study on the NoRa-Math Base-9 dataset to justify the denoising power of CD-CoT.

In this ablation study, we compare the performance of CD-CoT with different combinations of clean and noisy rationales on the NoRa-Math Base-9 dataset. The "Base model" column represents the accuracy of the base model without any additional components. The "+ CD Rephrase" column shows the accuracy when the contrastive denoising with rationale rephrasing (CD Rephrase) is applied. The "+ Vote" column represents the accuracy when the final answer is determined by a majority vote of five runs. Finally, the "CD-CoT" column shows the accuracy of the complete CD-CoT method, which includes both CD rephrasing and voting.

Since CD-CoT requires 3-shot noisy rationales and 1-shot clean rationale for contrastive rephrasing, we compare its performance with the following setups. The results are summarized in the Tab. 34.

* 3-shot clean rationales (without CD-CoT)
* 3-shot noisy rationales (irrelevant or inaccurate) of varying difficulty levels + 1-shot clean rationale (with CD-CoT)
* 4-shot clean rationales (without CD-CoT)

\begin{table}
\begin{tabular}{c|c|c|c|c|c c|c c|c c} \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{Method \(\mathcal{M}\)} & \multirow{2}{*}{
\begin{tabular}{c} Acc(\(\mathcal{M}\), \(\mathcal{Q}\), \(\mathcal{P}_{\text{base}}\)) \\ \end{tabular} } & \multiFrom the experimental results, we can observe that:

_Observation_ F.1 (CD-CoT significantly improves reasoning performance across various noisy scenarios). CD-CoT brings a significant improvement compared to the base model when prompted with 3 noisy rationales + 1 clean rationale across various noise ratios of both irrelevant and inaccurate noise. The base accuracy of CD-CoT can be attributed to the improvement brought by the contrastive denoising with rationale rephrasing method. This accuracy is further enhanced in the voting method, which demonstrates the effectiveness of the self-consistency approach in improving reasoning performance.

_Observation_ F.2 (Noisy rationales with CD-CoT does not outperform full clean rationales). The accuracy before voting and the accuracy after voting in CD-CoT do not exceed the accuracy of the base model prompted with 4 clean rationales. This indicates that while CD-CoT is effective at denoising rationales and enhancing reasoning performance in noisy situations, it does not inherently improve reasoning beyond the capabilities of the model when clean rationales are provided.

_Observation_ F.3 (Contrastive denoising and voting in CD-CoT are important). The results highlight the importance and effectiveness of the two key aspects of CD-CoT: (1) contrastive denoising with rationale rephrasing, which helps to filter out the noise and align the format of the rationales, and (2) repeated reasoning with voting, which enhances the robustness and accuracy of the final predictions. Notably, CD-CoT proves to be a valuable approach for improving the reasoning performance in the presence of noisy rationales, though it does not surpass the performance with full clean rationales.

_Observation_ F.4 (CD-CoT is more effective at managing irrelevant noise compared to inaccurate noise). As shown in Tab. 33, CD-CoT is more effective at handling irrelevant noise than it is at managing inaccurate noise at the same level. This is because irrelevant noise is easier to differentiate from the target information, making it simpler to remove during the rephrasing step of the contrasting denoising process. Additionally, Appendix. F.9 presents the denoised results for high-noise scenarios.

**Variants of prompt for CD-CoT.** We also conduct additional experiments to test the sensitivity of CD-CoT to prompt designs. Specifically, we generate several variants of prompts for the proposed CD-CoT method, including a shorter prompt, the default prompt, and a longer one:

**The Shorter Prompt**

Here are two examples: the first one has proper explanation and answer, while the second one has distracted explanation and correct answer. Please follow the first example's explanation and provide the correct explanation and answer for the second one.

**The Default Prompt**

Here are two examples for the same type of task: the first example has correct explanation and correct answer, and the second example has distracted explanation and correct answer. Please follow the first example and give me the correct explanation and answer for the second example, which should be logically consistent with the first one.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline CoT shots & Base model & + CD Rephrase & + Vote & CD-CoT \\ \hline
3-shot clean shots & 46.4 & — & **62.3** & — \\ \hline
3-shot irrelevant-easy + 1-shot clean & 46.0 & 47.2 & 57.3 & **60.0** \\ \hline
3-shot irrelevant-medium + 1-shot clean & 41.3 & 47.2 & 54.7 & **60.7** \\ \hline
3-shot irrelevant-hard + 1-shot clean & 36.2 & 46.5 & 47.3 & **58.3** \\ \hline
3-shot inaccurate-easy + 1-shot clean & 36.5 & 42.6 & 44.7 & **53.0** \\ \hline
3-shot inaccurate-medium + 1-shot clean & 23.4 & 43.0 & 32.3 & **58.7** \\ \hline
3-shot inaccurate-hard + 1-shot clean & 16.1 & 37.0 & 24.3 & **47.0** \\ \hline
4-shot clean rationale & **50.8** & — & **67.3** & — \\ \hline \hline \end{tabular}
\end{table}
Table 34: Ablation study of CD rephrase and vote components on the NoRa-Math Base-9 dataset.

**The Longer Prompt**

The following presents two examples of the same type of task. The first example contains both a correct explanation and a correct answer. The second example, however, includes a distracted explanation but still provides the correct answer. Your task is to analyze these examples and then provide a revised version explanation of the second example along with its answer. Ensure that your revised explanation is logically consistent with the first example.

We conduct experiments on NoRa-Math Base-9 to compare these three prompts. The results in Tab. 35 show that the performance of CD-CoT is only marginally influenced by these prompts. Note that the prompt of CD-CoT remains simple. Cooperating with advanced methods for iterating prompts and rationales, such as APE [110] and Star [97], can further improve this method.

**Rephrasing performance of SD.** Besides, we observe that the SD method performs better on some tasks when using irrelevant and inaccurate rationales compared to clean ones. A thorough examination of the empirical results from the log files reveals some inconsistencies in the performance of the SD method, especially in the NoRa-Symbolic Equal and NoRa-Commonsense tasks.

Specifically, for NoRa-Symbolic Equal task:

* In this task, comparing SD and SC reveals that the domain knowledge of LLM is relatively weak. According to the experiment results, when SD rephrases symbolic tasks, it often directly returns the final answer of the original response in many cases.
* The quality of the rephrased results basically does not change with the increase in noise.
* As a result, the accuracies of SD under clean and noisy settings are very close, with a maximum difference of less than 1%, which we think is within the acceptable range of error.

For NoRa-Commonsense task:

* We observe different behaviors of SD in clean and noisy settings.
* In noisy settings, SD does not follow the prompt to make minor corrections. Instead, SD generates short rationales with only the answer, discarding the original rationale (as shown in Tables. 36-37). This simplification can benefit commonsense tasks where the model already has a strong inherent understanding.
* On the other hand, in the clean setting, SD unexpectedly generates entirely new rationales instead of making minor corrections. These new rationales may deviate from the original correct reasoning, potentially introducing errors or inconsistencies that weren't present in the initial, accurate solution.

In Tab. 36 and Tab. 37, we present two representative examples of SD in the commonsense task. Our findings show that: (i) when provided with a clear rationale, SD generates entirely new rationales rather than simply making minor corrections; (ii) when given a noisy rationale, SD produces brief rationales that include only the answer, completely discarding the original rationale.

Overall, SD's denoising effect relies on LLMs' intrinsic capability. SD performs five maskings and reconstructions per noisy example, concatenating results into five prompts for LLM inference. In contrast, SC performs five direct inferences on noisy prompts without explicit denoising. By comparing SD and SC's performance, we can gain insights into the model's denoising effects.

In Tab. 3, SC outperforms SD in NoRa-Math and NoRa-Symbolic tasks, while SD only marginally excels in the NoRa-Commonsense task. This pattern persists in the clean settings. This suggests that

\begin{table}
\begin{tabular}{c|c|c} \hline \hline method & Irrelevant-medium & Inaccurate-medium \\ \hline Base Model & 0.284 & 0.082 \\ \hline CD-CoT w/ original prompt & 0.49 & 0.48 \\ \hline CD-CoT w/ short prompt & 0.46 & 0.46 \\ \hline CD-CoT w/ long prompt & 0.47 & 0.48 \\ \hline \hline \end{tabular}
\end{table}
Table 35: Ablation study of different prompts in CD-CoT.

LLMs struggle to reconstruct masked prompts in complex, domain-specific tasks but perform better in simpler common tasks, highlighting the varying levels of internal knowledge within LLMs across different domains.

Besides, the counterintuitive results on the NoRa-Commonsense task may be attributed to task-specific characteristics. Observation of reconstructed masked prompts in this task indicates that LLMs tend to bypass reconstruction instructions in noisy settings, directly providing final answers. This accidental removal of all rationales, including noisy ones, effectively acts as a noise filter. Combined with the LLM's natural strength in the commonsens task, this unplanned filtering likely explains the small improvement in accuracy under noisy conditions.

### The Difficulty of Irrelevant Noise Semantics

We conduct extra experiments with different semantic difficulties. Specifically, we set up three levels of semantic difficulty:

* **Level-1: Topic-irrelevant.** Noisy thought has little relevance to the task. Here, noisy thought involves concepts that are clearly irrelevant to the knowledge required for the task. For example, (1) in physics, the number one is the charge of a single unit of positive or negative electricity; (2) in chemistry, two is the atomic number of helium, the second most abundant element in the universe. These two are irrelevant thoughts.
* **Level-2: Topic-relevant and task-irrelevant.** The noisy thought has some apparent relevance to the task but does not help solve the problem. Noisy thought may involve some terminology or concepts that are relevant to the task but do not provide useful information.

\begin{table}
\begin{tabular}{l} \hline \hline
**The original clean rationale** \\ Carrie is Hugh’s sister’s husband’s daughter, so the relations path is sister, husband, daughter. For husband’s daughter, we have husband’s daughter is daughter. So the relations path are reduced to sister, daughter. For sister’s daughter, we have sister’s daughter is nicee. So the relations path are reduced to niece. Therefore, Answer: niece. \\ \hline
**The rephrased rationale by SD** \\ To determine Carrie’s relationship to Hugh, we can break down the given information step by step: 1. Carrie is Hugh’s sister’s husband’s daughter. 2. Hugh’s sister is Carrie’s sibling. 3. Hugh’s sister’s husband is Carrie’s brother-in-law. 4. Carrie is the daughter of Hugh’s sister’s husband. Therefore, the answer is: Carrie is Hugh’s niece. \\ \hline
**The original noisy rationale** \\ Carrie is Hugh’s sister’s husband’s daughter, so the relations path is sister, husband, daughter. For husband’s daughter, we have husband’s daughter is daughter. We have daughter’s grandmother is niece. So the relations path are reduced to sister, daughter. For sister’s daughter, we have sister’s daughter is niece. We have daughter’s niece is niece. So the relations path are reduced to niece. Therefore, Answer: niece. \\ \hline
**The rephrased rationale by SD** \\ niece. \\ \hline \hline \end{tabular}
\end{table}
Table 36: Rephrasing performance of SD under different noisy settings.

\begin{table}
\begin{tabular}{l} \hline \hline
**The original clean rationale** \\ Francisco is Lynn’s son’s father’s brother, so the relations path is son, father, brother. For father’s brother, we have father’s brother is uncle. So the relations path are reduced to son, uncle. For son’s uncle, we have son’s uncle is brother. So the relations path are reduced to brother. Therefore, Answer: brother. \\ \hline
**The rephrased rationale by SD** \\ To determine Francisco’s relationship with Lynn, we start with the information provided: Francisco is Lynn’s son’s father’s brother. Step 1: Lynn’s son’s father’s Is Lynn’s husband. Step 2: Francisco is Lynn’s husband’s brother. Based on this information, we can conclude that Francisco is Lynn’s brother-in-law. Answer: brother-in-law \\ \hline
**The original noisy rationale** \\ Francisco is Lynn’s son’s father’s brother, so the relations path is son, father, brother. For father’s brother, we have father’s brother is uncle. So the relations path are reduced to son, uncle. For son’s uncle, we have son’s uncle is brother. We have uncle’s niece is nephew. So the relations path are reduced to brother. We have brother’s nephew is mother. Therefore, Answer: brother. \\ \hline
**The rephrased rationale by SD** \\ Answer: brother. \\ \hline \hline \end{tabular}
\end{table}
Table 37: Rephrasing performance of SD under different noisy settings.

For example, (1) the modulo operation (mod) finds the remainder after the division of one number by another. (2) In base-9, the leading digit 1 indicates that the number is greater than or equal to \(1*9^{2}=81\) but less than \(2*9^{2}=162\). (3) In base-9, the number 9 is represented as '10' since it is the base value itself. (4) The number 11 in base-9 is equivalent to the decimal value ninety-nine, which is one less than the square of the base value.
* **Level-3: Topic-relevant, task-relevant, but not helpful.** Here, the noisy thought has some relevance to the task, but is not necessary information to solve the problem. The noisy thought may involve concepts, principles, or methods that are relevant to the task, but not directly applicable to the problem at hand. For example, (1) in a base-9 system, the place values are units, nines, eighty-ones, and so on, analogous to units, tens, hundreds, etc., in base-10; (2) the concept of carrying is essential when performing addition in positional numeral systems like base-9 and base-10. The modulo operation is a crucial step in the process of adding numbers in base-9, as it determines the digit and carry values; (3) the modulo operation is a crucial step in the process of adding numbers in base-9, as it determines the digit and carry values.

Based on the above definition and prompt, we generate the data with the following four steps.

* **Step-1: Initial generation and human evaluation.** We used this prompt to generate an initial set of irrelevant statements for each level. These were then manually reviewed and filtered to ensure they accurately represented the intended level of irrelevance. We selected high-quality examples for each level.
* **Step-2: Scaled generation.** Using these high-quality examples as ICL demonstrations, we prompted GPT-4 to generate a larger set of irrelevant statements for each level.
* **Step-3: Validation through similarity analysis.** To confirm that our generated statements indeed represented different levels of irrelevance, we conducted a cosine similarity analysis. This analysis quantitatively demonstrated the semantic differences between levels, as shown in our previous response.
* **Step-4: Dataset construction.** we integrated these generated irrelevant statements into our dataset. We inserted them into relevant demonstrations at appropriate positions, following a probability distribution that corresponds to the intended difficulty level of the task.

The experiment results with the above irrelevant thoughts are elaborated as follows: Here, the setup is kept the same as the main experiments to be consistent. Therein, we test the GPT-3.5 model with 3-shot irrelevant rationales where the noise ratio is 0.8.

For a reasonable quantification of relevance, we choose the metric of cosine similarity with the embeddings API provided by OpenAI with model: text-embedding-ada-002. We calculate the cosine similarity between all noise-inserted sample sentences and the original clean inference sentences and then take their average value.

For comparison, the accuracy when prompting with clean rationales is 46.4% for NoRa-Math Base-9 and 32.7% for NoRa-Symbolic Equal. Specifically, we have the following observations:

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline Dataset & Semantic level & Method & Acc(\%) \\ \hline \multirow{4}{*}{Math Base-9} & \multirow{2}{*}{Level-1 (Cosine Similarity = 0.75)} & base model & 26.6 \\  & & CD-CoT & 58.3 \\ \cline{2-4}  & & base model & 30.6 \\ \cline{2-4}  & & CD-CoT & 60.0 \\ \cline{2-4}  & & base model & 33.0 \\ \cline{2-4}  & & CD-CoT & 59.7 \\ \hline \multirow{4}{*}{Symbolic Equal} & \multirow{2}{*}{Level-1 (Cosine Similarity = 0.73)} & base model & 23.0 \\  & & CD-CoT & 44.0 \\ \cline{1-1} \cline{2-4}  & & base model & 26.9 \\ \cline{1-1} \cline{2-4}  & & CD-CoT & 46.0 \\ \cline{1-1} \cline{2-4}  & & base model & 27.7 \\ \cline{1-1} \cline{2-4}  & & CD-CoT & 45.0 \\ \hline \hline \end{tabular}
\end{table}
Table 38: Performance of GPT-3.5-turbo under different noise semantics.

_Observation_ F.5 (CD-CoT method is still effective under different semantic levels of noise). Notably, we further verify that the proposed CD-CoT method is still effective under this imperfect scenario with different semantic levels of noise. Specifically, CD-CoT can achieve 58.3%, 60.0%, and 59.7% accuracy on the three semantic levels of noise for the NoRa-Math Base-9 tasks and 44.0%, 46.0%, and 45.0% accuracy on the NoRa-Symbolic Equal tasks. Compared to the base model, CD-CoT shows a significant improvement of 29.4% and 19.1% in accuracy on semantic level-2 reasoning for the NoRa-Math Base-9 and NoRa-Symbolic Equal tasks, respectively. Similarly, CD-CoT demonstrates a 26.7% and 17.3% increase in accuracy on semantic level-3 reasoning for the NoRa-Math Base-9 and NoRa-Symbolic Equal tasks.

_Observation_ F.6 (CD-CoT maintains a robust and stable performance across different semantic levels of noise.). As the semantic level of noise increases, the base model exhibits a certain level of robustness to the irrelevant noise but still suffers a considerable performance drop (15.8% and 5.8% decrease for semantic level-2, and 13.4% and 5.0% decrease for semantic level-3 on the NoRa-Math Base-9 and NoRa-Symbolic Equal tasks, respectively). In contrast, CD-CoT maintains a robust and stable performance across different semantic levels of noise.

### The Number of Noisy Thoughts

As introduced in the main text, we use noise ratios of 0.3, 0.5, and 0.8 to control the number of noisy thoughts. The statistics of the NoRa dataset are shown in Tab. 2, showing the exact number of clean/noisy thoughts in each task. NoRa-Math entails 8 thoughts, NoRa-Symbolic varies from 2 to 12, and NoRa-Commonsense requires 5 thoughts.

To figure out the exact number of noisy thoughts for the model to be robust, we conduct further experiments to cover a wider range of noisy thoughts. Specifically,

* We compare the model's performance on NoRa-Math Base-9 and NoRa-Symbolic Equal tasks using clean and noisy rationales containing varying numbers of irrelevant or inaccurate thoughts.
* We gradually increase the number of noisy thoughts from 1 to 6 to investigate the impact of the noise level on the model's accuracy.
* The total number of thoughts per shot, including both clean and noisy thoughts, is shown in parentheses for the NoRa-Math Base-9 task.
* For the NoRa-Symbolic Equal task, the number of thoughts in the clean demonstrations varies across different examples.
* To ensure a consistent comparison, we select a subset of examples with a similar number of thoughts (ranging from 8 to 12) for both clean and noisy rationale experiments.
* We report the average number of thoughts (10.2) for the clean rationales in this subset.
* Consequently, the total number of thoughts per shot for noisy rationales in the NoRa-Symbolic Equal task is also calculated based on this average.

According to the experimental results in the Tab. 39, we can observe that:

_Observation_ F.7 (The impact of noisy thoughts varies across different noise types). For the NoRa-Math Base-9 task, the model's accuracy remains relatively stable when only one irrelevant noisy thought is introduced. However, the accuracy drops significantly as the number of irrelevant noisy thoughts increases. On the other hand, for the NoRa-Symbolic Equal task, even a single irrelevant or inaccurate noisy thought can lead to a noticeable decrease in accuracy.

_Observation_ F.8 (Inaccurate noisy thoughts have a more severe impact on the model's performance compared to irrelevant noisy thoughts). In the NoRa-Math Base-9 task, the accuracy drops drastically from 46.4% to 34.8% with just one inaccurate, noisy thought, and it further deteriorates to 6.2% when the number of inaccurate, noisy thoughts reaches 6. In contrast, with 6 irrelevant noisy thoughts, the accuracy decreases to 25.3%, which is still higher than that of inaccurate thoughts.

_Observation_ F.9 (Model's robustness to noisy thoughts varies depending on the nature of the task). The NoRa-Symbolic Equal task exhibits a similar trend, where the accuracy decreases as the number of noisy thoughts increases, regardless of the noise type. However, the decline in accuracy is more gradual compared to the NoRa-Math Base-9 task, suggesting that the model's robustness to noisy thoughts varies depending on the nature of the task.

The experimental results demonstrate that the presence of noisy thoughts, even in small numbers, can significantly impact the model's reasoning performance. The extent of the impact depends on factors such as the task type, noise type, and number of noisy thoughts.

### The Number of Noisy Examples

In this part, we provide a deeper analysis and extract more insights into in-context learning with varying numbers of noisy examples.

First, we obtain the accuracy of the base model with different numbers of noisy prompting examples, including 0-shot, 1-shot, 3-shot, and 5-shot. According to the results shown in Tab. 40, we propose the following hypothesis: _The trend of model performance as the number of noisy examples increases depends on the change in performance between 1-shot and 0-shot learning._

Namely, if the performance in 1-shot learning is better than in 0-shot learning, this indicates that noisy examples still help the model in recognizing task patterns. Therefore, model performance can improve with an increasing number of noisy examples, albeit at a slower rate compared to clean examples. On the other hand, if the performance in one-shot learning is less than in zero-shot learning, it suggests that noisy examples interfere with the model's existing knowledge of the task, causing model performance to decline as the number of noisy examples increases.

\begin{table}
\begin{tabular}{c|c|c c c c} \hline \hline \multirow{2}{*}{Task} & \multirow{2}{*}{Setting} & \multicolumn{4}{c}{\#Prompting Examples} \\  & & 0-shot & 1-shot & 3-shot & 5-shot \\ \hline \multirow{2}{*}{Base-9} & ina-easy & 7.2 & 17.5 & 23.2 & **25.6** \\  & ina-hard & 7.2 & **11.3** & 6.0 & 5.7 \\ \hline \multirow{2}{*}{Base-11} & irr. easy & 5.5 & 8.9 & 19.1 & **26.3** \\  & irr. hard & 5.5 & 7.7 & 10.7 & **16.1** \\ \hline \multirow{2}{*}{Sym.(E)} & ina-easy & 8.8 & 17.3 & **29.1** & — \\  & ina-hard & 8.8 & 15.0 & **22.7** & — \\ \hline \multirow{2}{*}{Sym.(L)} & irr. easy & 0.0 & 2.3 & 7.0 & **8.9** \\  & irr. hard & 0.0 & 1.9 & **6.0** & — \\ \hline \hline \end{tabular}
\end{table}
Table 40: Performances of base model under varying number of examples (”—” denotes over token limit, bolded number denotes the highest accuracy).

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline Dataset & Noise Type & Noisy Thought Num & Accuracy \\ \hline \multirow{8}{*}{NoRa-Math Base-9} & clean & 0 (total thoughts per shot: 8) & 46.4 \\ \cline{3-4}  & & & 1 (9) & 46.4 \\ \cline{3-4}  & & & 2 (10) & 40.0 \\  & irrelevant & & 3 (11) & 34.2 \\  & irrelevant & & 4 (12) & 30.5 \\  & & & 5 (13) & 28.8 \\  & & & 6 (14) & 25.3 \\ \cline{3-4}  & & & 1 & 34.8 \\  & & & 2 & 24.8 \\  & inaccurate & & 3 & 15.1 \\  & & & 4 & 11.1 \\  & & & 5 & 6.8 \\  & & & 6 & 6.2 \\ \hline \multirow{8}{*}{NoRa-Symbolic Equal} & clean & 0 (total thoughts per shot: 10.2) & 39.8 \\ \cline{2-4}  & & & 1 (11.2) & 32.8 \\  & & & 2 (12.2) & 34.2 \\  & irrelevant & & 3 (13.2) & 33.3 \\  & & & 4 (14.2) & 30.6 \\  & & & 5 (15.2) & 28.4 \\  & & & 6 (16.2) & 29.2 \\ \hline \multirow{8}{*}{NoRa-Symbolic Equal} & & & 1 & 34.5 \\  & & & 2 & 31.3 \\ \cline{1-1}  & inaccurate & & 3 & 32.4 \\ \cline{1-1}  & & & 4 & 31.4 \\ \cline{1-1}  & & & 5 & 28.3 \\ \cline{1-1}  & & & 6 & 27.3 \\ \hline \hline \end{tabular}
\end{table}
Table 39: Impact of the number of noisy thoughts on GPT-3.5 performance.

Next, we conduct a further quantitative analysis of the impact of noisy shots on model performance. We evaluate (0-shot, 1-shot, 3-shot) results partition, there are 8 possible results in total: (CCC), (CCW), (CWC), (CWW), (WCC), (WCW), (WWC), (WWW). Specifically, 'C' stands for correct and 'W' stands for wrong. For example, CCC means the cases in which test questions are correctly answered in 0-shot (C), 1-shot (C), and 3-shot (C). WWC means the cases in which test questions are wrongly answered in 0-shot (W), 1-shot (W), and correctly answered in 3-shot (C).

To analyze the effects of noisy shots, we categorize these eight results into three types:

* The model answers correctly at zero-shot but starts to answer incorrectly as noisy shots increase. The corresponding results are (CCW), (CWC), and (CWW).
* The model answers incorrectly at zero-shot but starts to answer correctly as noisy shots increase. The corresponding results are (WCC), (WCW), and (WWC).
* The model's performance is consistent at zero-shot and with noisy shots. The corresponding results are (CCC) and (WWW).

By observing Tab. 41, we arrive at the following analysis.

_Observation_ F.10 (Compared to zero-shot, models can benefit from noisy shots in many cases). Across all datasets, (WWW) is the most prevalent outcome, while the second most common results are almost always (WWC) or (WCW). This implies that there is a relatively large proportion of samples for which the model answers incorrectly at zero-shot, yet correctly as noisy shots increase.

_Observation_ F.11 (The instances where noisy shots have a negative effect are very rare.). For the first category of results, namely (CCW), (CWC), and (CWW), the average proportion across all datasets is 0.039. This indicates that the proportion of samples where the model answers correctly at zero-shot but incorrectly as noisy shots increase is very small. In other words, the instances where noisy shots have a negative effect are rare.

_Observation_ F.12 (There are significantly more instances where the model benefits from increasing noisy shots). For the second category of results, which includes (WCC), (WCW), and (WWC), the average proportion across all datasets is 0.186, which is greater than that of the first category. This means that there is a larger proportion of samples where the model answers incorrectly at zero-shot but correctly as noisy shots increase. Namely, there are more instances where noisy shots have a positive effect.

_Observation_ F.13 (The type of instances where the model produces consistent answers is dominant across all datasets). For the third category of results, namely (CCC) and (WWW), the average proportion across all datasets is the highest, at 0.776. This category of results does not allow for an assessment of the effect of noisy shots.

We also provide a case study of in-context learning using a varying number of noisy examples. We provide cases corresponding to some possible outcomes (_i.e._, [(CCC), (CCW), (CWC)]) for (0-shot, 1-shot, 3-shot) on the NoRa-Math Base-9 dataset with inaccurate easy-level noise in Tables. 42-44.

### Variants of CD-CoT

Acquiring a human-annotated and clean rationale can be challenging in some practical scenarios. However, we would further clarify the relatively low cost and the necessity of this clean rationale to guarantee the effectiveness of robust reasoning.

Current LLMs cannot discern and remove noisy thoughts with their intrinsic denoising ability, even when enhanced with the self-denoising method. This is supported by the empirical observations

\begin{table}
\begin{tabular}{c|c|c c c c c c c c} \hline \hline \multirow{2}{*}{Task} & \multirow{2}{*}{Setting} & \multicolumn{8}{c}{\#Result Partition} \\  & & CCC & CCW & CWC & CWW & WCC & WCW & WWC & WWW \\ \hline \multirow{2}{*}{Base-9} & ina\_easy & 0.006 & 0.01 & 0.019 & 0.037 & 0.049 & 0.11 & 0.158 & **0.611** \\  & ina\_hard & 0.003 & 0.01 & 0.004 & 0.055 & 0.012 & 0.088 & 0.041 & **0.786** \\ \hline \multirow{2}{*}{Base-11} & irr. easy & 0.004 & 0.003 & 0.013 & 0.036 & 0.029 & 0.054 & 0.144 & **0.718** \\  & irr. hard & 0.001 & 0.001 & 0.008 & 0.043 & 0.005 & 0.069 & 0.075 & **0.797** \\ \hline \multirow{2}{*}{Sym.(E)} & ina\_easy & 0.053 & 0.008 & 0.012 & 0.015 & 0.067 & 0.045 & 0.16 & **0.641** \\  & ina\_hard & 0.049 & 0.007 & 0.015 & 0.016 & 0.042 & 0.052 & 0.121 & **0.698** \\ \hline \multirow{2}{*}{Sym.(L)} & irr. easy & 0.0 & 0.0 & 0.0 & 0.0 & 0.005 & 0.018 & 0.058 & **0.919** \\  & irr. hard & 0.0 & 0.0 & 0.0 & 0.0 & 0.004 & 0.015 & 0.056 & **0.925** \\ \hline \hline \end{tabular}
\end{table}
Table 41: Results partition of (0-shot, 1-shot, 3-shot).

in Sec. 4. Hence, external supervision is necessary, which should be sufficient for denoising and accessible in practice. Existing methods with external supervision require (1) oracle feedback on the test question, (2) human feedback of errors on specific tokens or positions, or (3) expert knowledge to construct detailed descriptions of specific tasks.

By contrast, we believe that a clean CoT demonstration is more attainable and practical, which can be the minimal requirement for denoising-purpose prompting. With the proposed CD-CoT method, we show that LLMs can identify noisy thoughts by contrasting a noisy rationale with a clean one.

Besides, we build a self-supervised variant of CD-CoT that does not rely on manually crafted clean examples, _i.e._, this variant can get a clean rationale automatically. Specifically, we prompt LLM with a fixed question using the zero-shot CoT approach [39], allowing it to answer N times. We then select the responses with the correct answer based on the ground-truth label.

Then, we conduct experiments on two datasets, NoRa-Math Base-9 and NoRa-Symbolic Equal, with the default model (GPT-3.5-0613). The hyper-parameters are temperature=0.1 and top-p=1. We set N=50 for NoRa-Math Base-9 and N=100 for NoRa-Symbolic Equal. Other settings are kept the same as default with the original CD-CoT.

\begin{table}
\begin{tabular}{p{28.5pt}} \hline \hline
**Question** \\ In base-9, what is 574-267 Please reason it step by step. End the response with the result in “Answer: \(\framebox{$result$}\) \\
**0-shot (Correct)** \\ To add in base-9, we start by adding the digits in the rightmost column. In the units column, 7 + 6 = 13. Since we are in base-9, we need to regroup. We can rewrite 13 as 4 in the units column and carry-over 1 to the next column. In the next column, we add the carried-over 1 to the digits in that column. We have 1 + 5 + 2 = 8. So, the result in the next column is 8. Finally, we put the digits 8 and 4 together to get the final answer. Answer: \(\framebox{$84$}\) \\
**1-shot (Correct)** \\ To add 57 and 26 in base-9, we’ll start with the units digit. In base-9, the digits are “012345678”. 7 + 6 = 13. Since 13 is greater than 8 (the maximum digit value in base-9), we need to regroup. We can write 13 as 13 = 1 * 9 + 4. Therefore, the units digit is 4 and there is a carry of 1 to the next column. Moving on to the next column, we have 5 + 2 + 1 (the carried digit) = 8. 8 is a valid digit in base-9, so we do not need to regroup. The tens digit is 8. Therefore, the sum of 57 and 26 in base-9 is 84. Answer: \(\framebox{$84$}\) \\
**3-shot (Wrong)** \\ In base-9, the digits are “012345678”. We have 7 + 6 = 13. Since we’re in base-9, we need to determine the digit and carry. 13 divided by 9 is 1 with a remainder of 4. So the digit is 4, and we carry 1. Next, we have 5 + 2 + 1 = 8. So the digit is 8 and there is no carry. A leading digit is not needed since there is no carry. Thus, the answer is 48. Answer: \(\framebox{$48$}\) \\ \hline \hline \end{tabular}
\end{table}
Table 43: Case for (CCW): 0-shot correct, 1-shot correct, 3-shot wrong.

\begin{table}
\begin{tabular}{p{28.5pt}} \hline \hline
**Question** \\ In base-9, what is 25+18? Please reason it step by step. End the response with the result in “Answer: \(\framebox{$result$}\) \\
**0-shot (Correct)** \\ To perform addition in base-9, we need to first understand the place values. In base-9, the place values range from right to left as follows: ones, nines, eighty-ones, and so on. Let’s start with the ones place. 5 plus 8 is equal to 13. However, in base-9, we can only have digits from 0 to 8. Therefore, we need to regroup. Since 13 is greater than 9, we can subtract 9 from it and carry over 1 to the next place value (nines place). The remainder is 4. Now let’s move on to the nines place. We have 1 (carry over) plus 2 plus 1 (from the nines place) which is equal to 4. Therefore, when we add 25 and 18 in base-9, we get 44. Answer: \(\framebox{$44$}\) \\
**1-shot (Correct)** \\ In base-9, the digits are “012345678”. We have 5 + 8 = 13 in base-10. Since we’re in base-9, that exceeds the maximum value of 8 for a single digit. 13 mod 9 = 4, so the digit is 4 and the carry is 1. Next, we have 2 + 1 + 1 (carry) = 4 in base-10. 4 in base-9 is written as 4. Since there’s no carry this time, we have no additional digits. So, the sum is 44 in base-9. Answer: \(\framebox{$44$}\) \\
**3-shot (Correct)** \\ In base-9, the digits are “012345678”. First, we add the digits in the rightmost place value, which is 5 + 8. We have 5 + 8 = 13 in base-10. Since we’re in base-9, the maximum value for a single digit is 8. 13 mod 9 = 4, so the digit is 4 and the carry is 1. Next, we add the digits in the next place value, which is 1 (from the carry) + 2 + 1. We have 1 + 2 + 1 = 4 in base 10. Again, 4 mod 9 = 4, so the digit is 4 and there is no carry. Therefore, the final result is 44. Answer: \(\framebox{$44$}\) \\ \hline \hline \end{tabular}
\end{table}
Table 42: Case for (CCC): 0-shot correct, 1-shot correct, 3-shot correct.

**Question**

In base-9, what is 67+25? Please reason it step by step. End the response with the result in "Answer: \(\framebox{$result$}\).

**0-shot (Correct)**

To add in base-9, we start by adding the digits in the rightmost place value and moving leftwards. In the units place, 7+5=12. However, in base-9, there are only digits from 0 to 8. Therefore, we need to do a "carry" to the next place value. The digit 12 is written as 13 in base-9, since the digit 12 in base-9 is represented as 10, followed by 11. Now, we move to the next place value, the 9s place. We have a carry of 1 from the units place, so the addition becomes 1+6+2 = 9. In base-9 notation, 9 is written as 10. Therefore, the sum of 67+25 in base-9 is 103. Answer: \(\framebox{$103$}\)

**1-shot (Wrong)**

In base-9, the digits are "012345678". We have 7 + 5 = 12 in base-10. Since we're in base-9, that doesn't exceed the maximum value of 8 for a single digit. 12 and 9 = 3, so the digit is 3 and the carry is 1. We have 1 + 6 + 2 = 9 in base-10. 9 mod 9 = 0, so the digit is 0 and the carry is 1. We have 1 + 1 + 0 = 2 in base-10. So the final digit is 2. Therefore, the answer is 203. Answer: \(\framebox{$203$}\)

**3-shot (Correct)**

In base-9, the digits are "012345678". We have 7 + 5 = 12 in base-10. Since we're in base-9, that exceeds the maximum value of 8 for a single digit. 12 mod 9 = 3, so the digit is 3 and the carry is 1. We have 6 + 2 + 1 = 9 in base 10. 9 mod 9 = 0, so the digit is 0 and the carry is 1. A leading digit is 1. So the answer is 103. Answer: \(\framebox{$103$}\)

On the NoRa-Math Base-9 dataset, the LLM answers incorrectly in the initial 20 attempts. When we increase N to 50, the LLM gets it right twice. We use one of the self-generated correct rationales to execute CD-CoT, testing the effect of LLM's self-generated rationale. On the NoRa-Symbolic Equal dataset, we need to increase N to 100 to get a correct self-generated response.

Regarding the empirical results in Tab. 45, we derive the following detailed analysis.

_Observation_ F.14 (The self-supervised variant of CD-CoT achieves enhancements across various tasks). On NoRa-Math Base-9, the average accuracy is increased by 26.8 percentage points. On NoRa-Symbolic Equal, it gains an average increase of 3.9 points in accuracy.

_Observation_ F.15 (LLM struggles to generate a clean and correct rationale on its own). Considering the fact that the model requires 50 attempts on the simple NoRa-Math Base-9 task to obtain the correct result, it indicates that the LLM struggles to generate a clean and correct rationale on its own. Additionally, experiments on the NoRa-Symbolic Equal indicate that as the task's difficulty increases, it becomes more challenging for the model to resolve it by relying solely on its inherent capabilities.

_Observation_ F.16 (Self-generated clean examples are not as effective as manually designed clean examples). Although the self-supervised variant of CD-CoT acquires improvement across tasks, its gains on the difficult NoRa-Symbolic Equal tasks are quite modest, with an average increase of only 3.9 points. Conversely, the original CD-CoT registers an average accuracy enhancement of 39.5 points on NoRa-Math Base-9 tasks and 15.1 points on NoRa-Symbolic Equal tasks, respectively. We can conclude that self-generated clean examples can provide some guidance to the LLM on simpler tasks, but they are not as effective as manually designed clean examples. On more difficult tasks, the guidance provided by self-generated clean examples is very limited, and these tasks require additional supervision to steer the model effectively.

In addition to the self-supervised CD-CoT, exploring strategies like using contrasting noisy examples or incorporating an external knowledge base, possibly through a retrieval-augmented denoising framework, may offer significant advances in automation and robustness of reasoning. We believe that these are promising directions that deserve further investigation.

\begin{table}
\begin{tabular}{c|c|c c c} \hline \hline \multirow{2}{*}{Task} & \multirow{2}{*}{Setting} & \multicolumn{3}{c}{\#Method} \\  & & Base Model & Variant of CD-CoT & CD-CoT \\ \hline \multirow{2}{*}{Base-9} & irr.-med & 30.3 & 47.0 & 60.7 \\  & ina.-med & 10.1 & 47.0 & 58.7 \\ \hline \multirow{2}{*}{Sym.(E)} & irr.-easy & 28.1 & 33.0 & 44.7 \\  & ina.-easy & 29.1 & 32.0 & 42.6 \\ \hline \hline \end{tabular}
\end{table}
Table 45: Performances of different variants of CD-CoT.

### New Datasets and Large-scale Real Problems

Recall that the constructed NoRa benchmark covers five prevailing datasets from three different domains. The current LLMs present significant vulnerability to noisy rationales in all five datasets of NoRa, while the proposed method CD-CoT has shown advanced and consistent robustness against noisy rationales. Specifically,

* Robustness with different datasets: CD-CoT consistently outperforms other methods in all five datasets in NoRa.
* Robustness with different noise levels: The results shown in Tab. 8 demonstrate the remarkable robustness of CD-CoT to varying noise levels. Across the NoRa-Math, NoRa-Symbolic, and NoRa-Commonsense tasks, the performance decline of CD-CoT remains modest as the noise level increases.
* Robust with different LLMs: The results in Tab. 11 further indicate that CD-CoT substantially improves over all three other baselines on the more powerful LLMs. Even on the relatively smaller Mistral-8x7B, CD-CoT significantly outperforms the other baselines on most tasks.

Note that Sec. 3 introduces a general framework for generating noisy rationales with existing datasets. This means, more datasets can be integrated into NoRa if necessary in future research. Here, we conduct additional experiments to generalize CD-CoT to three new datasets that are not covered in NoRa, including GSM-8K [12], Blocksworld [82], and BIG-Bench Hard Dyck Languages [72]. Specifically,

* GSM-8K: A math dataset of linguistically diverse grade school math word problems.
* Blocksworld: A planning dataset simulating block stacking tasks.
* BIG-Bench Hard Dyck Languages: A symbolic dataset designed for predicting the sequence of closing parentheses in a Dyck-4 word.

Then, we generate noisy rationales and compare the following setups:

* Zero-shot: Base model with no demonstration.
* CoT (clean rationales): Base model with 3 clean demonstrations.
* CoT (noisy rationales): Base model with 3 noisy demonstrations.
* CD-CoT (noisy rationales): Base model with 3 noisy demonstrations and our CD-CoT method.

The reasoning accuracy in Tab. 46 shows that CD-CoT consistently outperforms the zero-shot setting and standard CoT prompting under noisy rationales. This is consistent with the findings in Sec. 5.2, showing CD-CoT's strong capability of generalization to new datasets.

Along with the introduction of new datasets, we also examine the issue of noisy rationale in large-scale real-world scenarios, particularly in conversational question-answering (QA) tasks. While our primary focus is on reasoning tasks, we discuss the differences and relationships between these areas.

Conversational QA tasks are usually conducted in a zero-shot setting, where users directly input the task instructions and hardly provide examples to the model. NoRa, on the other hand, considers the in-context learning scenario, where the model is given a few examples before answering questions. In-context learning can significantly enhance the model's performance on complex or new tasks. The purpose of NoRa is to assess the model's robustness to noisy examples in in-context learning.

\begin{table}
\begin{tabular}{c|c|c c c c} \hline \hline Dataset & Setup & \multicolumn{4}{c}{\#Method} \\  & & Zero-shot & CoT (clean) & CoT (noisy) & CD-CoT (noisy) \\ \hline GSM-8K & 300 samples & 84.3 & 87.7 & 84.3 & 86.0 \\ \hline Blocksworld & 200 samples & 2.0 & 25.0 & 13.0 & 25.5 \\ \hline Hard Dyck Languages & 250 samples & 12.4 & 40.8 & 29.2 & 35.2 \\ \hline \hline \end{tabular}
\end{table}
Table 46: Performances of different methods on new datasets.

However, for multi-turn conversational QA tasks, the content of previous answers can be considered as context for the model when answering the next question. Therefore, by evaluating how noisy context affects a model's subsequent question responses, we can explore the issue of noisy rationales within the realm of conversational QA tasks.

We conduct experiments on MT-Bench [108] to investigate the noisy rationale problem in QA scenarios. MT-Bench is a multi-turn question collection and covers eight principal types of tasks: Writing, Roleplay, Extraction, Reasoning, Math, Coding, Knowledge I, and Knowledge II.

For clean context, we directly use the original question to prompt the model, and the first round of dialogue generated serves as the clean context. For noisy context, we prompt the model to insert some irrelevant information as noise when answering the first turn's question, and the first round of dialogue serves as the noisy context. We then continue to use the second turn's question along with the generated context to prompt the model to obtain the target response.

We conduct conversational QA tasks on GPT-3.5-turbo-0613 and score the generated responses with GPT-4. The GPT-4 is to score the responses from both the first and second turns separately.

As the results shown in Tab. 47, the noisy context decreases both the scores (quality) of the first-round and second-round QA. Specifically, the average scores across different types of tasks are shown in Tab. 48. Notably, a 0.48% decrease in average score appears in the second response, which is the final target of the conversation.

In addition, we provide a case study for illustration. As shown in Tab. 49, the noisy context in round 1 leads to a bad, noisy response in round 2.

In the clean context, the model provides an accurate analysis and answer to the initial question. When the correct round serves as the context for the second question, the model consistently delivers a precise analysis and response.

In the noisy context, the model's response and analysis of the initial question contain errors. Specifically, as shown in Tab. 49, it incorrectly counts multiple integers for both case 1 and case 2, culminating in a wrong answer for the first question. When the flawed round serves as the context for the second question, the model generates the same intermediate results as in the first round and ultimately provides an incorrect answer to the second question.

In summary, during multi-turn conversational QA tasks, the model's response to the current question is influenced by the preceding dialogue content (_i.e._, the context), particularly for tasks that require reasoning and mathematical computations.

\begin{table}
\begin{tabular}{c|c c c c c c c c} \hline \hline \multirow{2}{*}{Round} & \multirow{2}{*}{Setting} & \multicolumn{6}{c}{\#Avg.score} \\  & & \multicolumn{2}{c}{First Response} & \multicolumn{2}{c}{Second Response} \\ \hline Clean Context & & 8.73 & 8.23 \\ \hline Noisy Context & & 8.16 & 7.75 \\ \hline \hline \end{tabular}
\end{table}
Table 47: Average scores with different contexts.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline \multirow{2}{*}{Round} & \multirow{2}{*}{Setting} & \multicolumn{6}{c}{\#Avg.score} \\  & & \multicolumn{2}{c}{First Response} & \multicolumn{2}{c}{Second Response} \\ \hline Clean Context & & 8.73 & 8.23 \\ \hline Noisy Context & & 8.16 & 7.75 \\ \hline \hline \end{tabular}
\end{table}
Table 47: Average scores with different contexts.

[MISSING_PAGE_FAIL:58]

[MISSING_PAGE_FAIL:59]

[MISSING_PAGE_FAIL:60]

\begin{table}
\begin{tabular}{c|l} \hline \hline \multicolumn{2}{c}{Original Noisy Rationale} \\ \hline \multicolumn{2}{c}{Cindy is Pamela’s mother’s mother’s daughter, so the relations path is mother, mother, daughter. We have daughter’s grandmother is nicee. For mother’s daughter, we have mother’s sister. We have daughter’s sister is mother. So the relations path are reduced to mother, sister. For mother’s sister, we have mother’s sister is aunt. So the relations path are reduced to aunt. We have aunt’s grandson is grandmother. Therefore, Answer: aunt.} \\ \hline \multicolumn{2}{c}{Denoised Rationale} \\ \hline \multirow{4}{*}{Smooth LLM} & Cindy is 2ame1’t’s mother’s mother’s daughter, so the relations path is mother, mother/* daughter. We ha9e \&aajbfer/* s grandmother’s grandmother is n-ece. Fol mother’s daughter, we have mother’s daughter2is sister. We [av1 daughtchr’s sisterOr is noter. lo the relatioses path are rhduced to tötder, sisterI For mother’s sister,zwR bfve mother’s sister is aunt. So tze relatiosxpath are aldVced toUaunt. Wa have aunt’s grandskn is grandmothH. Therffore, Answer: aunz.} \\ \hline \multicolumn{2}{c}{Self-denoise} & Answer: aunt. \\ \hline \multirow{4}{*}{Self-polish} & Steps: 1: Cindy is the daughter of Pamela’s mother. 2. Therefore, Cindy is Pamela’s mother’s mother’s daughter, 3. Cindy is the sister of Pamela’s mother. 4. Since Cindy is Pamela’s mother’s sister, Cindy is Pamela’s aunt. Answer: Aunt \\ \cline{2-2}  & Cindy is Pamela’s mother’s mother’s mother’s father, \\ \cline{1-1}  & Cindy is Pamela’s mother’s mother’s father, \\ \cline{1-1} Self-denoise & Gnidy is Pamela’s mother’s mother’s daughter, so the relations path is mother, mother, daughter. For mother’s daughter, we have mother’s daughter is Cindy. In humans, females typically have two X chromosomes, one from each parent. So the relations path is reduced to mother, Cindy. In certain bird species, like the QueenPairs, sisters’ often cooperate in raising the offspring of one mother, demonstrating altrium. \\ \cline{1-1}  & Something can be reduced to mother’s sister, which is aunt. In certain bird species, like the Canadian Flycytactcher, “aunt’ birds often help raise their siblings’ offspring. Therefore, Answer: aunt. \\ \hline \multirow{4}{*}{Self-polish} & Cindy is Pamela’s mother’s mother’s daughter. Let’s break down the relationship step by step to find a single word answer describing the relationship. First, let’s identify Cindy’s relationship to Pamela’s mother’s mother. Cindy is the daughter of Pamela’s mother’s mother’s mother’s mother. To find the single word answer, we consider the direct relationship between Cindy and Pamela’s mother’s mother’s mother. The relationship between Cindy and Pamela’s mother’s mother is a parent-child relationship. Therefore, the single word answer describing the relationship between Cindy and Pamela is: Answer: daughter \\ \hline CD-CoT & Cindy is Pamela’s aunt. \\ \hline \hline \end{tabular}
\end{table}
Table 52: Rephrased rationales by different robust methods on NoRa-Commonsense irrelevant medium task.

\begin{table}
\begin{tabular}{c|l} \hline \hline \multicolumn{2}{c}{Cindy is Pamela’s mother’s daughter, so the relations path is mother, mother, daughter. For mother’s daughter, we have mother’s daughter is sister. In humans, daughters typically have two X chromosomes, one from each parent. So the relations path are reduced to mother, sister. In certain bird species, ‘sisters’ often cooperate in raising the offspring of one sibling, demonstrating altrium. For mother’s sister, we have mother’s sister is aunt. So the relations path are reduced to aunt. In certain bird species, like the Seychelles Warbler, ‘aunt’ birds often help raise their siblings’ offspring. Therefore, Answer: aunt.} \\ \hline \hline \multirow{4}{*}{Smooth LLM} & \multicolumn{2}{c}{Denoised Rationale} \\ \hline \multirow{4}{*}{Smooth LLM} & Sindyis Pamela’s mother’s moth

[MISSING_PAGE_EMPTY:62]

[MISSING_PAGE_EMPTY:63]

## Appendix H NeurIPS Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have summarized the position and key contributions of the paper in the abstract and introduction parts.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations are discussed in Appendix A.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The assumptions and proof are provided in Appendix D.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The technical details of implementation are introduced in Appendix E.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The source files are publicly available at: [https://github.com/tmlr-group/NoisyRationales](https://github.com/tmlr-group/NoisyRationales).
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (_e.g._, data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experiment setups are introduced in Appendix F, and technical details of implementation are introduced in Appendix E.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We conduct evaluations on 300 questions for each task and repeat the reasoning 5 times for each question to obtain consistent and reliable results. The empirical results with mean and std values are provided in Tab. 33.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]Justification: We employ GPT-3.5-turbo-0613 [17] as our base LLM for the analyses presented in this study. In addition, we conduct evaluations on three supplementary models, including Gemini-Pro [76], Llama2-70B [79], and Mixtral-8x7B [33].
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We have carefully checked the NeurIPS Code of Ethics and confirmed that our paper obeys it.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The broader impacts are introduced in Appendix A. We believe that this paper does not raise any negative societal impacts or ethical concerns.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (_e.g._, pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper does not release new models. The constructed NoRa dataset is based on existing datasets.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (_e.g._, code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: In the paper, we have introduced the resources of the models and datasets used.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We construct the NoRa dataset. Its details are in Appendix C.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper is not about crowdsourcing experiments or research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper is not about research with human subjects.