# Understanding and Improving Training-free

Loss-based Diffusion Guidance

Yifei Shen\({}^{1}\)1 Xinyang Jiang\({}^{1}\) Yifan Yang\({}^{1}\) Yezhen Wang\({}^{2}\) Dongqi Han\({}^{1}\) Dongsheng Li\({}^{1}\)

\({}^{1}\)Microsoft Research Asia \({}^{2}\)National University of Singapore

Footnote 1: Contact yifeishen@microsoft.com

###### Abstract

Adding additional guidance to pretrained diffusion models has become an increasingly popular research area, with extensive applications in computer vision, reinforcement learning, and AI for science. Recently, several studies have proposed training-free loss-based guidance by using off-the-shelf networks pretrained on clean images. This approach enables zero-shot conditional generation for universal control formats, which appears to offer a free lunch in diffusion guidance. In this paper, we aim to develop a deeper understanding of training-free guidance, as well as overcome its limitations. We offer a theoretical analysis that supports training-free guidance from the perspective of optimization, distinguishing it from classifier-based (or classifier-free) guidance. To elucidate their drawbacks, we theoretically demonstrate that training-free guidance is more susceptible to misaligned gradients and exhibits slower convergence rates compared to classifier guidance. We then introduce a collection of techniques designed to overcome the limitations, accompanied by theoretical rationale and empirical evidence. Our experiments in image and motion generation confirm the efficacy of these techniques.

## 1 Introduction

Diffusion models represent a class of powerful deep generative models that have recently broken the long-standing dominance of generative adversarial networks (GANs) [8]. These models have demonstrated remarkable success in a variety of domains, including the generation of images and videos in computer vision [33, 3], the synthesis of molecules and proteins in computational biology [20, 52], as well as the creation of trajectories and actions in the field of reinforcement learning (RL) [21].

One critical area of research in the field of diffusion models involves enhancing controllability, such as pose manipulation in image diffusion [50], modulation of quantum properties in molecule diffusion [20], and direction of goal-oriented actions in RL diffusion [23]. The predominant techniques for exerting control over diffusion models include classifier guidance and classifier-free guidance. Classifier guidance involves training a time-dependent classifier to map a noisy image, denoted as \(\mathbf{x}_{t}\), to a specific condition \(\mathbf{y}\), and then employing the classifier's gradient to influence each step of the diffusion process [8]. Conversely, classifier-free guidance bypasses the need for a classifier by training an additional diffusion model conditioned on \(\mathbf{y}\)[18]. However, both approaches necessitate extra training to integrate the conditions. Moreover, their efficacy is often constrained when the data-condition pairs are limited and typically lack the zero-shot generalization capability.

Recently, several studies [2, 49, 37] have introduced training-free guidance that builds upon the concept of classifier guidance. These models eschew the need for training a classifier on noisy images; instead, they estimate the clean image from its noisy counterpart using Tweedie's formula and then employ pretrained networks, designed for clean images, to guide the diffusion process. Given thatcheckpoints for these networks pretrained on clean images are widely accessible online, this form of guidance can be executed in a zero-shot manner. A unique advantage of training-free guidance is that it can be applied to universal control formats, such as style, layout, and FaceID [2, 49, 37] without any additional training efforts. Furthermore, these algorithms have been successfully applied to offline reinforcement learning, enabling agents to achieve novel goals not previously encountered during training. In contrast to classifier guidance and classifier-free guidance, it is proved in Appendix E of [27] that training-free guidance does not offer an approximation to the exact conditional energy. Therefore, from a theoretical perspective, it is intriguing to understand how and when these methods succeed or fail. From an empirical standpoint, it is crucial to develop algorithms that can address and overcome these limitations.

This paper seeks to deepen the understanding of training-free guidance by examining its mechanisms and inherent limitations, as well as overcoming these limitations. Specifically, our major contributions can be summarized as follows:

* **How does training-free guidance work?** Although exact conditional energy is difficult to approximate in a training-free manner, from the optimization standpoint, we show that training-free guidance can effectively decrease the guidance loss function. The optimization perspective clarifies the mystery of why the guidance weights should be meticulously designed in relation to the guidance function and diffusion time, as observed in [49].
* **When does training-free guidance not work?** We theoretically identify the susceptibility of training-free guidance to misaligned gradient issues and slower convergence rates. We attribute these challenges to a decrease in the smoothness of the guidance network in contrast to the classifier guidance.
* **Improving training-free guidance:** We introduce random augmentation to alleviate the misaligned gradient and Polyak step size scheduling to improve convergence. The efficacy of these methods is empirically confirmed across various diffusion models (i.e., image diffusion and motion diffusion) and under multiple conditions (i.e., segmentation, sketch, text, object avoidance, and targeting)2. Footnote 2: The code is available at [https://github.com/BIGKnight/Understanding-Training-free-Diffusion-Guidance](https://github.com/BIGKnight/Understanding-Training-free-Diffusion-Guidance)

## 2 Preliminaries

### Diffusion Models

Diffusion models are characterized by forward and reverse processes. The forward process, occurring over a time interval from \(0\) to \(T\), incrementally transforms an image into Gaussian noise. On the contrary, the reverse process, from \(T\) back to \(0\), reconstructs the image from the noise. Let \(\mathbf{x}_{t}\) represent the state of the data point at time \(t\); the forward process systematically introduces noise to the data by following a predefined noise schedule given by \(\mathbf{x}_{t}=\sqrt{\alpha_{t}}\mathbf{x}_{0}+\sigma_{t}\mathbf{\epsilon}_{t}\), where \(\alpha_{t}\in[0,1]\) is monotonically decreasing with \(t\), \(\sigma_{t}=\sqrt{1-\alpha_{t}}\), and \(\mathbf{\epsilon}_{t}\sim\mathcal{N}(0,\mathbf{I})\) is random noise. Diffusion models use a neural network to learn the noise at each step:

\[\min_{\theta}\mathbb{E}_{\mathbf{x}_{t},\mathbf{\epsilon},t}[\|\mathbf{\epsilon}_{\theta}( \mathbf{x}_{t},t)-\mathbf{\epsilon}_{t}\|^{2}_{2}]=\min_{\theta}\mathbb{E}_{\mathbf{x}_{t},\mathbf{\epsilon},t}[\|\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t)+\sigma_{t}\nabla_{\bm {x}_{t}}\log p_{t}(\mathbf{x}_{t})\|^{2}_{2}],\]

where \(p_{t}(\mathbf{x}_{t})\) is the distribution of \(\mathbf{x}_{t}\). The reverse process is obtained by the following ODE:

\[\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}=f(t)\mathbf{x}_{t}-\frac{g^{2}(t)}{2} \nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{x}_{t})=f(t)\mathbf{x}_{t}+\frac{g^{2}(t)}{2 \sigma_{t}}\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t), \tag{1}\]

where \(f(t)=\frac{\mathrm{d}\log\sqrt{\alpha_{t}}}{\mathrm{d}t}\), \(g^{2}(t)=\frac{\mathrm{d}\sigma_{t}^{2}}{\mathrm{d}t}-2\frac{\mathrm{d}\log \sqrt{\alpha_{t}}}{\mathrm{d}t}\sigma_{t}^{2}\). The reverse process enables generation as it converts a Gaussian noise into the image.

### Diffusion Guidance

In diffusion control, we aim to sample \(\mathbf{x}_{0}\) given a condition \(\mathbf{y}\). The conditional score function is expressed as follows:

\[\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{x}_{t}|\mathbf{y})=\nabla_{\mathbf{x}_{t}}\log p_{t }(\mathbf{x}_{t})+\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{y}|\mathbf{x}_{t}). \tag{2}\]The conditions are specified by the output of a neural network, and the energy is quantified by the corresponding loss function. If \(\ell(f_{\phi}(\cdot),\cdot)\) represents the loss function as computed by neural networks, then the distribution of the clean data is expected to follow the following formula [2; 49; 37]:

\[p_{0}(\mathbf{x}_{0}|\mathbf{y})\propto p_{0}(\mathbf{x}_{0})\exp(-\ell(f_{\phi}(\mathbf{x}_{0} ),\mathbf{y})). \tag{3}\]

For instance, consider a scenario where the condition is the object location. In this case, \(f_{\phi}\) represents a fastRCNN architecture, and \(\ell\) denotes the classification loss and bounding box loss. By following the computations outlined in [27], we can derive the exact formula for the second term in the RHS of (2) as:

\[\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{y}|\mathbf{x}_{t})=\nabla_{\mathbf{x}_{t}}\log\mathbb{ E}_{p(\mathbf{x}_{0}|\mathbf{x}_{t})}[\exp(-\ell(f_{\phi}(\mathbf{x}_{0}),\mathbf{y})]. \tag{4}\]

**Classifier guidance**[8] involves initially training a time-dependent classifier to predict the output of the clean image \(\mathbf{x}_{0}\) based on noisy intermediate representations \(\mathbf{x}_{t}\) during the diffusion process, i.e., to train a time-dependent classifier \(f_{\psi}(\mathbf{x}_{t},t)\) such that \(f_{\psi}(\mathbf{x}_{t},t)\approx f_{\phi}(\mathbf{x}_{0})\). Then the gradient of the time-dependent classifier is used for guidance, given by \(\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{y}|\mathbf{x}_{t}):=-\nabla_{\mathbf{x}_{t}}\ell(f_ {\psi}(\mathbf{x}_{t},t),\mathbf{y})\). This term equals (4) if the loss is cross-entropy.

**Training-free loss-based guidance**[2; 49; 6] puts the expectation in (4) inside the loss function:

\[\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{y}|\mathbf{x}_{t}):= \nabla_{\mathbf{x}_{t}}\log\left[\exp(-\ell(f_{\phi}(\mathbb{E}_{p(\mathbf{ x}_{0}|\mathbf{x}_{t})}(\mathbf{x}_{0})),\mathbf{y}))\right]\stackrel{{(a)}}{{=}}- \nabla_{\mathbf{x}_{t}}\ell\left[f_{\phi}\left(\frac{\mathbf{x}_{t}-\sigma_{t}\mathbf{e}_{ \theta}(\mathbf{x}_{t},t)}{\sqrt{\alpha_{t}}}\right),\mathbf{y}\right], \tag{5}\]

where (a) uses Tweedie's formula \(\mathbb{E}_{p(\mathbf{x}_{0}|\mathbf{x}_{t})}(\mathbf{x}_{0})=\frac{\mathbf{x}_{t}-\sigma_{t} \mathbf{e}_{\theta}(\mathbf{x}_{t},t)}{\sqrt{\alpha_{t}}}\). Leveraging this formula permits the use of a pretrained off-the-shelf network designed for processing clean data. The gradient of the last term in the energy function is obtained via backpropagation through both the guidance network and the diffusion backbone.

## 3 Analysis of Training-Free Guidance

### How does Training-free Guidance Work?

**On the difficulty of approximating \(\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{y}|\mathbf{x}_{t})\) in high-dimensional space.** Despite being intuitive, [27] has shown that training-free guidance in (5) does not offer an approximation to the true energy in (4). The authors of [37] consider to directly approximate (4) with a Gaussian distribution:

\[\nabla_{\mathbf{x}_{t}}\log\mathbb{E}_{p(\mathbf{x}_{0}|\mathbf{x}_{t})}[\exp (-\ell(f_{\phi}(\mathbf{x}_{0}),\mathbf{y})]\stackrel{{(a)}}{{\approx}} \nabla_{\mathbf{x}_{t}}\log\mathbb{E}_{q(\mathbf{x}_{0}|\mathbf{x}_{t})}[\exp(-\ell(f_{\phi }(\mathbf{x}_{0}),\mathbf{y})] \tag{6}\] \[\approx\nabla_{\mathbf{x}_{t}}\log\frac{1}{n}\sum_{i=1}^{n}\exp(-\ell (f_{\phi}(\mathbf{x}_{0}^{i}),\mathbf{y})),\quad\mathbf{x}_{0}^{i}\sim q(\mathbf{x}_{0}|\mathbf{x} _{t}),\]

where \(q(\mathbf{x}_{0}|\mathbf{x}_{t})\) is chosen as \(\mathcal{N}(\mathbb{E}_{p(\mathbf{x}_{0}|\mathbf{x}_{t})}(\mathbf{x}_{0}),r_{t}^{2}\mathbf{I})\) and \(r_{t}\) is a tunable parameter. As demonstrated in [37], the approximation is effective for one-dimensional distribution. However, we find that the approximation denoted by (a) does not extend to high-dimensional data (e.g., images) if the surrogate distribution \(q\) is sub-Gaussian. This is due to the well-known high-dimensional probability phenomenon [43] that if \(q\) has sub-Gaussian coordinates (e.g., iid and bounded), then \(q(\mathbf{x}_{0}|\mathbf{x}_{t})\) tends to concentrate on a spherical shell centered at \(\mathbb{E}_{p(\mathbf{x}_{0}|\mathbf{x}_{t})}(\mathbf{x}_{0})\) with radius \(r_{t}\) (details are in Appendix C.1). Since the spherical shell represents a low-dimensional manifold with zero measure in the high-dimensional space, there is a significant likelihood that the supports of \(p(\mathbf{x}_{0}|\mathbf{x}_{t})\) and \(q(\mathbf{x}_{0}|\mathbf{x}_{t})\) do not overlap, rendering the approximation (a) ineffective.

**Understanding training-free guidance from an optimization perspective.** We instead analyze the training-free guidance from the optimization perspective. Intuitively, in each step, the gradient is taken and the loss of the guidance network decreases. At the initial stage of the diffusion (\(t\) is large), the diffusion trajectory can exhibit substantial deviations between adjacent steps and may increase the objective value. So the objective value will oscillate at the beginning. When \(t\) is smaller, the change to the sample is more fine-grained, leading to a bounded change in the objective value. Therefore, the objective value is guaranteed to decrease when \(t\) is small, as showing in the next proposition.

**Proposition 3.1**.: _Assume that the guidance loss function \(\ell(f_{\phi}(\mathbf{x}_{0}),\mathbf{y})\) is \(\mu\)-PL (defined in Definition D.2 in appendix) and \(L_{f}\)-Lipschitz with respect to clean images \(\mathbf{x}_{0}\), and the score function \(\nabla\log p_{t}(\mathbf{x}_{t})\) is \(L_{p}\)-Lipschitz (defined in Definition D.1 in appendix) with respect to noisy image \(\mathbf{x}_{t}\). Denote \(\lambda_{\min}\) as the minimum eigenvalue of the (semi)-definite matrix \(\text{Cov}[\mathbf{x}_{0}|\mathbf{x}_{t}]\). Then the following conditions hold: (1) Consider the loss function \(\ell_{t}(\mathbf{x}_{t})=\ell\left(f_{\phi}\left(\frac{\mathbf{x}_{t}+\sigma_{t}^{2} \nabla\log p_{t}(\mathbf{x}_{t})}{\sqrt{\alpha_{t}}}\right),\mathbf{y}\right)\) and denote \(\kappa_{1}=\frac{\mu\lambda_{\min}^{2}}{L_{f}(1+L_{p})\sqrt{\alpha_{t}\sigma_{t }^{3}}}\). After one gradient step \(\hat{\mathbf{x}}_{t}=\mathbf{x}_{t}-\eta\nabla_{\mathbf{x}_{t}}\ell_{t}(\mathbf{x}_{t}),\eta= \frac{\sqrt{\alpha_{t}}}{L_{f}(1+L_{p})}\), we have \(\ell_{t}(\hat{\mathbf{x}}_{t})\leq(1-\kappa_{1})\ell_{t}(\mathbf{x}_{t})\); (2) Consider a diffusion process that adheres to a bounded change in the objective function such that for any diffusion step, i.e., \(\ell_{t-1}(\mathbf{x}_{t-1})\leq\frac{\ell_{t}(\hat{\mathbf{x}}_{t})}{(1-\kappa_{2})}\) for some \(\kappa_{2}<\kappa_{1}\), then the objective function converges at a linear rate, i.e., \(\ell_{t-1}(\mathbf{x}_{t-1})\leq\frac{1-\kappa_{1}}{1-\kappa_{2}}\ell_{t}(\mathbf{x}_{t})\).

The proof is given in Appendix D.2. The Lipschitz continuity and PL conditions are basic assumptions in optimization, and it has been shown that neural networks can locally satisfy these conditions [5]. These assumptions, while not essential, simplify the proof and results for clarity, similar to the assumptions taken in [26]. The optimization perspective clarifies the mystery of why the guidance weights (i.e., the step size in optimization) should be carefully selected with respect to the guidance function and time \(t\). For example, in [49], most guidance weights \(\eta\) are chosen to be proportional to \(\sqrt{\alpha_{t}}\) and dependent on guidance network, which differs from the weights used in classifier guidance and aligns with our step size in Proposition 3.1.

Then we empirically verify Proposition 3.1 via experiments in Figure 1. We use ResNet-50 trained on clean images to guide ImageNet pretrained diffusion models. The loss value at each diffusion step is plotted. As a reference, we choose \(100\) images from the class "indigo bird" in ImageNet training set and compute the loss value, which is referred to as "Correct Image Loss" in the figure. The objective value oscillates when \(t\) is large, followed by a swift decrease, which verifies our analysis. More convergence figures are given in Figure 7 in Appendix.

An intriguing aspect of the theory is that the loss remains low regardless of the success of the guidance, akin to the loss associated with correct images. Figure 0(b) demonstrates this phenomenon: despite the absence of an indigo bird in the image, the loss is still minimal. This phenomenon can be attributed to the effect of misaligned gradients, which is explored in detail in the following subsection.

### Limitations of Training-free Guidance

In this subsection, we examine the disadvantages of employing training-free guidance networks as opposed to training-based classifier guidance.

**Training-free guidance is more sensitive to the misaligned gradient.** Adversarial gradient is a significant challenge for neural networks, which refers to minimal perturbations deliberately applied to inputs that can induce disproportionate alterations in the model's output [38]. The resilience of a model to adversarial gradients is often analyzed through the lens of its Lipschitz constant [34]. If the model has a lower Lipschitz constant, then the output is less sensitive to the input perturbations and thus is more robust.

In the classifier or training-free guidance, the gradient of the guidance network is added to the image. In contrast to yielding a direction that meaningfully minimizes the loss, the adversarial gradient primarily serves to minimize the loss in a manner that is not necessarily aligned with the intended guidance direction. As a result, we refer the adversarial gradient of guidance network as the _misaligned gradient_ in diffusion guidance.

Figure 1: The classifier loss of a successful and a failure guidance example. The target class is “indigo bird”.

Compared with the off-the-shelve guidance network used in training-free guidance, time-dependent classifiers are trained on noise-augmented images. Our finding is that adding Gaussian noise improves the Lipschitzness of the guidance network. This transition mitigates the misaligned gradient challenge by inherently enhancing the model's robustness to such perturbations, as shown in the next proposition.

**Proposition 3.2**.: _(Time-dependent network is more robust and smooth) Given a bounded loss function \(\ell(\mathbf{x})\leq C\), the loss \(\hat{\ell}(\mathbf{x})=\mathbb{E}_{\mathbf{\epsilon}\sim\mathcal{N}(0,\mathbf{I})}[\ell( \mathbf{x}+\sigma_{t}\mathbf{\epsilon})]\) is \(C\sqrt{\frac{2\mathbf{\gamma}}{\pi\sigma_{t}^{2}}}\)-Lipschitz and \(\nabla\hat{\ell}\) is \(\frac{2C}{\sigma_{t}}\)-Lipschitz._

The proof is given in Appendix D.3. We then support Proposition 3.2 with both qualitative and quantitative experiments. For qualitative experiments, we present visualizations of the accumulated gradients for both the time-dependent and off-the-shelf time-independent classifiers corresponding to different classes in Figure 1(b) and Figure 1(c), respectively. These visualizations are generated by initializing an image with a random background and computing \(1000\) gradient steps for each classifier. For the time-dependent classifier, the input time for the \(t\)-th gradient step is \(1000-t\). The images are generated purely by the classifier gradients without diffusion involved. For comparative analysis, we include the accumulated gradient of an adversarially robust classifier [35], as shown in Figure 1(a), which has been specifically trained to resist misaligned (adversarial) gradients. The resulting plots reveal a stark contrast: the gradient of the time-dependent classifier visually resembles the target image, whereas the gradient of the time-independent classifier does not exhibit such recognizability. This observation suggests that off-the-shelf time-independent classifiers are prone to generating misaligned gradients for guidance compared to the time-dependent classifier used in classifier guidance. The quantitative experiments are given in Table 4 in Appendix.

Figure 2 provides a more intuitive visual explanation of diffusion guidance compared to the existing formula-based approaches as shown in (7). The gradient produced by the guidance network represents a valid image. When these gradients are incorporated into the images, the diffusion model is able to identify the object and enhance it into a clearer and more vivid representation.

**Training-free guidance slows down the convergence of reverse ODE.** The efficiency of an algorithm in solving reverse ordinary differential equations is often gauged by the number of non-linear function estimations (NFEs) required to achieve convergence. This metric is vital for algorithmic design, as it directly relates to computational cost and time efficiency [36]. In light of this, we explore the convergence rates associated with various guidance paradigms, beginning our analysis with a reverse ODE framework that incorporates a generic gradient guidance term. The formula is expressed as

\[\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}=f(t)\mathbf{x}_{t}+\frac{g^{2}(t)}{2}(\bm {\epsilon}_{\theta}(\mathbf{x}_{t},t)+\nabla_{\mathbf{x}_{t}}v(\mathbf{x}_{t},t)), \tag{7}\]

where \(h(\cdot,\cdot)\) can be either a time-dependent classifier or a time-independent classifier with Tweedie's formula. The subsequent proposition elucidates the relationship between the discretization error and the smoothness of the guidance function.

**Proposition 3.3**.: _Let \(u(\mathbf{x}_{t},t)=\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t)+\nabla_{\mathbf{x}_{t}}v(\mathbf{ x}_{t},t)\) in (7), \(h_{\max}=\max_{t}\frac{1}{2}\left[\log(\frac{\alpha_{t}}{1-\alpha_{t}})-\log( \frac{\alpha_{t-1}}{1-\alpha_{t-1}})\right]\). Assume we run DDIM solver for \(M\) steps and \(M=O(1/h_{\max})\). Then the error is bounded by \(O((1+L^{M})/M)\)._

Figure 2: Gradients of different classifiers on random backgrounds. The images in the first row correspond to the target class “cock”, and the second row to “goldfinch”.

The proof is given in Appendix D.4. Proposition 3.2 establishes that time-dependent classifiers exhibit superior gradient Lipschitz constants compared to their off-the-shelf time-independent counterparts. This disparity in smoothness slows down the convergence for training-free guidance methods, necessitating a greater number of NFEs to achieve the desired level of accuracy when compared to classifier guidance. To provide quantitative support, we compare the convergence speed of training-based PPAP [12] and training-free FreeDoM in Table 5 in Appendix.

## 4 Improving Training-free Guidance

In this section, we propose to adopt random augmentation to mitigate the misaligned gradient issue, and Polyak step size [15] to mitigate the convergence issue. In addition to these two techniques, our method and baselines will also incorporate a trick named time travel, often referred to as "restart sampling", and its theoretical framework is detailed in [46].

```
for\(t=T,\cdots,0\)do \(\mathbf{x}_{t-1}=\text{DDIM}(\mathbf{x}_{t})\) \(\hat{\mathbf{x}}_{0}=\frac{\mathbf{x}_{t}-\sigma_{t}\mathbf{\varepsilon}_{\theta}(\mathbf{x} _{t},t)}{\sqrt{\alpha_{t}}}\)\(\triangleright\) Tweedie's formula \(\mathbf{g}_{t}=\frac{1}{|\mathcal{T}|}\sum_{T\in\mathcal{T}}\nabla_{\mathbf{x}_{t}} \ell(f_{\phi}(T(\hat{\mathbf{x}}_{0})),\mathbf{y})\) \(\mathbf{x}_{t-1}=\mathbf{x}_{t-1}-\eta\cdot\mathbf{g}_{t}\) endfor
```

**Algorithm 1** Random Augmentation

### Random Augmentation

As established by Proposition 3.2, the introduction of Gaussian perturbations enhances the Lipschitz property of a neural network. A direct application of this principle involves creating multiple noisy instances of an estimated clean image and passing them into the guidance network, a method analogous to the one described in (6). However, given the high-dimensional nature of image data, achieving a satisfactory approximation of the expected value necessitates an impractically large number of noisy copies. To circumvent this issue, we propose an alternative strategy that employs a diverse set of data augmentations in place of solely adding Gaussian noise. This approach effectively introduces perturbations within a lower-dimensional latent space, thus requiring fewer samples. The suite of data augmentations utilized, denoted by \(\mathcal{T}\), is derived from the differentiable data augmentation techniques outlined in [51], which encompasses transformations such as translation, resizing, color adjustments, and cutout operations. The details are shown in Algorithm 1 and the rationale is shown in the following proposition.

**Proposition 4.1**.: _(Random augmentation improves smoothness) Given a bounded non-Lipschitz loss function \(\ell(\mathbf{x})\), the loss \(\hat{\ell}(\mathbf{x})=\mathbb{E}_{\mathbf{\epsilon}\sim p(\mathbf{\epsilon})}[\ell(\mathbf{x} +\mathbf{\epsilon})]\) is \(C\int_{\mathbb{R}^{n}}\|\nabla p(\mathbf{t})\|_{2}\mathrm{d}\mathbf{t}\)-Lipschitz and its gradient is \(C\int_{\mathbb{R}^{n}}\|\nabla^{2}p(\mathbf{t})\|_{\mathrm{op}}\mathrm{d}\mathbf{t}\)-Lipschitz._

The proof is shown in Appendix D.5. Echoing the experimental methodology delineated in Section 3.2, we present an analysis of the accumulated gradient effects when applying random augmentation to a ResNet-50 model. Specifically, we utilize a set of \(|\mathcal{T}|=10\) diverse transformations as our augmentation strategy. The results of this experiment are visualized in Figure 1(d), where the target object's color and shape emerge in the gradient profile. This observation suggests that the implementation of random augmentation can alleviate the misaligned gradient issue. The quantitative effect of random augmentation is given in Table 4 in Appendix. The computational efficiency of random augmentation is further discussed in Appendix C.4.

### Polyak Step Size

In Section 3.1, we analyzed training-free guidance from the optimization perspective. To accelerate the convergence, gradient step size should be adaptive to the gradient landscape. We adopt Polyak step size, which has near-optimal convergence rates under various conditions [15]. The algorithm is shown in Algorithm 2 and the term \(\|\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t)\|\) is used to both estimate the gap to optimal values and balance the magnitude of diffusion term and guidance term.

We implement Polyak step size within the context of a training-free guidance framework called FreeDoM [49] and benchmark the performance of this implementation using the DDIM sampler with \(50\) steps. As shown in Figure 3, FreeDoM is unable to effectively guide the generation process when faced with a significant discrepancy between the unconditional generation and the specified condition. An illustrative example is the difficulty in guiding the model to generate faces oriented to the left when the unconditionally generated faces predominantly orient to the right, as shown in Figure 2(b). This challenge, which arises due to the insufficiency of \(50\) steps for convergence under the condition, is ameliorated by substituting gradient descent with adaptive step size, thereby illustrating the benefits of employing a better step size in the guidance process. The quantitative experiments are given in Table 5 in Appendix.

## 5 Experiments

In this section, we evaluate the efficacy of our proposed techniques across various diffusion models and guidance conditions. We compare our methods with established baselines: Universal Guidance (UG) [2], Loss-Guided Diffusion with Monte Carlo (LGD-MC) [37], Training-Free Energy-Guided Diffusion Models (FreeDoM) [49], and Manifold Preserving Guided Diffusion (MPGD) [16]. LGD-MC utilizes (6) while UG and FreeDoM are built on (5). MPGD utilizes an auto-encoder to ensure the manifold constraints. Furthermore, time travel trick (Algorithm 3) is adopted in UG, FreeDoM, and MPGD to improve the performance. Please refer to Appendix B for details of baselines. For the sampling method, DDIM with \(100\) steps is adopted as in [49; 37]. The method "Ours" is built on FreeDoM, with Polyak step size and random augmentation.

### Guidance to CelebA-HQ Diffusion

In this subsection, we adopt the experimental setup from [49]. Specifically, we utilize the CelebA-HQ diffusion model [19] to generate high-quality facial images. We explore three guidance conditions: segmentation, sketch, and text. For segmentation guidance, BiSeNet [48] generates the facial segmentation maps, with an \(\ell_{2}\)-loss applied between the estimated map of the synthesized image and the provided map. Sketch guidance involves using the method from [45] to produce facial sketches, where the loss function is the \(\ell_{2}\)-loss between the estimated sketch of \(\hat{\mathbf{x}}_{0}\) and the given sketch. For text guidance, we employ CLIP [32] as both the image and text encoders, setting the loss to be the \(\ell_{2}\) distance between the image and text embeddings.

We randomly select \(1000\) samples each of segmentation maps, sketches, and text descriptions. The comparative results are presented in Table 1. Consistent with [49], the time-travel number for all methods is set to \(s=1\). Figure 4 displays a random selection of the generated images. More image samples are provided in the supplementary materials. We find that the baselines failed to guide if the condition differs from unconditionally generated images significantly, as discussed in Section 4.2.

### Guidance to ImageNet Diffusion

For the unconditional ImageNet diffusion, we employ text guidance in line with the approach used in FreeDoM and UG [2; 49]. We utilize CLIP-B/16 as the image and text encoder, with cosine similarity

Figure 3: The effects of step size.

serving as the loss function to measure the congruence between the image and text embeddings. To evaluate performance and mitigate the potential for high-scoring adversarial images, we use CLIP-L/14 for computing the CLIP score. In FreeDoM and MPGD-Z, resampling is conducted for time steps ranging from \(800\) to \(300\), with the time-travel number fixed at \(10\), as described in [49]. Given that UG resamples at every step, we adjust its time-travel number \(s=5\) to align the execution time with that of FreeDoM. The textual prompts for our experiments are sourced from [25]. The comparison of different methods is depicted in Table 2. The corresponding randomly selected images are illustrated in Figure 5. The table indicates that our method achieves the highest consistency with the provided prompts. As shown in Figure 5, LGD-MC and MPGD tend to overlook elements of the prompts. Both UG and FreeDoM occasionally produce poorly shaped objects, likely influenced by misaligned gradients. Our approach addresses this issue through the implementation of random augmentation. Additionally, none of the methods successfully generate images that accurately adhere to positional prompts such as "left to" or "below". This limitation is inherent to CLIP and extends to all text-to-image generative models [41]. More image samples are provided in the supplementary materials.

### Guidance to Human Motion Diffusion

In this subsection, we extend our evaluation to human motion generation using the Motion Diffusion Model (MDM) [40], which represents motion through a sequence of joint coordinates and is trained on a large corpus of text-motion pairs with classifier-free guidance. We apply the targeting guidance and object avoidance guidance as described in [37]. Let \(\mathbf{x}_{0}(t)\) denote the joint coordinates at time \(t\), \(\mathbf{y}_{t}\) the target location, \(\mathbf{y}_{\text{obs}}\) the obstacle location, \(r\) the radius of the objects, and \(T\) the total number of frames. The loss function is defined as follows:

\[\ell=\|\mathbf{y}_{t}-\mathbf{x}_{0}(T)\|_{2}^{2}+\sum_{i}\text{sigmoid}(-(\|\mathbf{x}_{ 0}(i)-\mathbf{y}_{\text{obs}}\|-r)\times 50)\times 100. \tag{8}\]

Our experimental configuration adheres to the guidelines set forth in [37]. We assess the methods using the targeting loss (the first term in (8)), the object avoidance loss (the second term in (8)), and the CLIP score calculated by MotionCLIP [39]. In this application, MPGD-Z cannot be applied as there are no auto-encoder. MPGD w/o proj suffers from the shortcut and cannot achieve good performance, as discussed in Appendix B.2. In our method, random augmentation is omitted because the guidance is not computed by neural networks so the adversarial gradient issues are not obvious. The quantitative results of our investigation are summarized in Table 3, while Figure 6 showcases randomly selected samples. Our methods exhibit enhanced control quality over the generated motion. The videos are provided in the supplementary materials.

\begin{table}
\begin{tabular}{c|c c|c c|c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{2}{c|}{Segmentation maps} & \multicolumn{2}{c|}{Sketches} & \multicolumn{2}{c}{Tests} \\ \cline{2-7}  & Distance\(\downarrow\) & FID\(\downarrow\) & Distance\(\downarrow\) & FID\(\downarrow\) & Distance\(\downarrow\) & FID\(\downarrow\) \\ \hline UG [2] & 2247.2 & 39.91 & 52.15 & 47.20 & 12.08 & 44.27 \\ LGD-MC [37] & 2088.5 & 38.99 & 49.46 & 54.47 & 11.84 & 41.74 \\ FreeDoM [49] & 1657.0 & 38.65 & 34.21 & 52.18 & 11.17 & 46.13 \\ MPGD-Z [16] & 1976.0 & 39.81 & 37.23 & 54.18 & 10.78 & 42.45 \\ Ours & **1575.7** & **33.31** & **30.41** & **41.26** & **10.72** & **41.25** \\ \hline \hline \end{tabular}
\end{table}
Table 1: The performance comparison of various methods on CelebA-HQ with different types of zero-shot guidance. The experimental settings adhere to Table 1 of [49].

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline Methods & LGD-MC [37] & UG [2] & FreeDoM [49] & MPGD-Z [16] & Ours \\ \hline CLIP Score\(\uparrow\) & 24.3 & 25.7 & 25.9 & 25.1 & **27.7** \\ \hline \hline \end{tabular}
\end{table}
Table 2: The performance comparison of various methods on unconditional ImageNet with zero-shot text guidance. We compare various methods using ImageNet pretrained diffusion models with CLIP-B/16 guidance. For evaluating performance, the CLIP score is computed using CLIP-L/14.

### Related Work on Training-Free Guidance

Due to space limitation, we only introduce the related work on training-free guidance while leaving more related work in Appendix A. The current training-free guidance strategies for diffusion models can be divided into two primary categories. The first category is the loss-based guidance in this paper, which is universally applicable to universal control formats and diffusion models. These methods predict a clean image, subsequently leveraging pretrained networks to guide the diffusion process. Central to this approach are the algorithms based on (5), which have been augmented through techniques like time-travel [2; 49] and the introduction of Gaussian noise [37]. The adjoint sensitivity method [31] and spherical Gaussian constraint [47] have been adopted to estimate a more accurate guidance gradient. Extensions of these algorithms have found utility in domains with constrained data-condition pairs, such as molecule generation [14], and in scenarios necessitating zero-shot guidance, like open-ended goals in offline reinforcement learning [44]. In molecular generation and offline reinforcement learning, they outperform training-based alternatives as additional training presents challenges. This paper delves deeper into the mechanics of this paradigm and introduces a suite of enhancements to bolster its performance. The efficacy of our proposed modifications is demonstrated across image and motion generation, with promising potential for generalization to molecular modeling and reinforcement learning tasks.

The second category of training-free guidance is tailored to text-to-image or text-to-video diffusion models, which is based on insights into their internal backbone architecture. For instance, object layout and shape have been linked to the cross-attention mechanisms [17], while network activations have been shown to preserve object appearance [42]. These understandings facilitate targeted editing of object layout and appearance (Diffusion Self-Guidance [11]) and enable the imposition of conditions in ControlNet through training-free means (FreeControl [30]). Analyzing these methodologies is challenging due to their reliance on emergent representations during training. Nonetheless, certain principles from this paper remain relevant; for example, as noted in Proposition 3.3, these methods often necessitate extensive diffusion steps, with instances such as [30; 11] employing \(1000\) steps. A thorough examination and refinement of these techniques remain an avenue for future research.

## 6 Conclusions

In this paper, we conducted a comprehensive investigation into training-free guidance, which employs pretrained diffusion models and guides them using the off-the-shelf trained on clean images. Our exploration delved into the underlying mechanisms and fundamental limits of these models. Moreover, we proposed a set of enhancement techniques and verified their effectiveness both theoretically and empirically.

**Limitations.** Despite our efforts to mitigate the shortcomings of training-free methods and enhance their performance, certain limitations remain. Notably, the refined training-free guidance still necessitates a higher number of NFEs when compared with extensive training methods such as classifier-free guidance. This is because misaligned gradient cannot be fully eliminated without training.

**Ethical Consideration.** Similar to other models designed for image creation, our model also has the unfortunate potential to be used for creating deceitful or damaging material. We pledge to restrict the usage of our model exclusively to the realm of research to prevent such misuse.

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c} \hline \multirow{2}{*}{Methods} & \multicolumn{2}{c|}{“Backwards”} & \multicolumn{2}{c|}{“Balanced Beam”} & \multicolumn{2}{c|}{“Walking”} & \multicolumn{2}{c}{“Jogging”} \\  & Loss\(\downarrow\) & CLIP\(\uparrow\) & Loss\(\downarrow\) & CLIP\(\uparrow\) & Loss\(\downarrow\) & CLIP\(\uparrow\) & Loss\(\downarrow\) & CLIP\(\uparrow\) \\ \hline Unconditional [40] & \(3.55+9.66\) & 65.6 & \(47.92+0\) & **70.8** & \(48.88+0\) & 37.6 & \(144.84+0\) & **61.72** \\ FreeDoM [49] & \(1.09+6.63\) & 67.23 & \(9.83+4.48\) & 62.65 & \(1.64+7.55\) & 40.12 & \(34.95+7.83\) & 58.74 \\ LGD-MC [37] & \(0.98+6.48\) & 67.31 & \(4.42+0.02\) & 63.13 & \(1.30+0.39\) & 38.82 & \(6.12+2.38\) & 57.89 \\ Ours & **0.68+1.32** & **67.50** & **1.13+0.30** & 63.02 & **0.43+0.31** & **40.40** & **2.93+1.15** & 60.03 \\ \hline \end{tabular}
\end{table}
Table 3: Comparison of various methods on MDM with zero-shot targeting and object avoidance guidance. Loss is reported as a two-component metric: the first part is the MSE between the target and the actual final position of the individual; the second part measures the object avoidance loss.

Figure 4: Qualitative results of CelebA-HQ with zero-shot segmentation, sketch, and text guidance. The images are randomly selected.

Figure 5: Qualitative results of ImageNet model with zero-shot text guidance. The images are randomly selected.

Figure 6: Qualitative results of human motion diffusion with zero-shot object avoidance and targeting guidance. Instances of intersection with obstacles are highlighted by marking the person in red. The trajectories are randomly selected.

## References

* [1] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. In _International conference on machine learning_, pp. 284-293. PMLR, 2018.
* [2] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 843-852, 2023.
* [3] Tim Brooks, Bill Peebles, Connor Homes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators).
* [4] Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep Mukhopadhyay. Adversarial attacks and defences: A survey. _arXiv preprint arXiv:1810.00069_, 2018.
* [5] Yixuan Chen, Yubin Shi, Mingzhi Dong, Xiaochen Yang, Dongsheng Li, Yujiang Wang, Robert Dick, Qin Lv, Yingying Zhao, Fan Yang, et al. Over-parameterized model optimization with polyak-lojasiewicz condition. 2023.
* [6] Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. _arXiv preprint arXiv:2209.14687_, 2022.
* [7] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse problems using manifold constraints. _Advances in Neural Information Processing Systems_, 35:25683-25696, 2022.
* [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.
* [9] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 9185-9193, 2018.
* [10] Bradley Efron. Tweedie's formula and selection bias. _Journal of the American Statistical Association_, 106(496):1602-1614, 2011.
* [11] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. _Advances in Neural Information Processing Systems_, 36, 2024.
* [12] Hyojun Go, Yunsung Lee, Jin-Young Kim, Seunghyun Lee, Myeongho Jeong, Hyun Seung Lee, and Seungtaek Choi. Towards practical plug-and-play diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 1962-1971, 2023.
* [13] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. _arXiv preprint arXiv:1412.6572_, 2014.
* [14] Xu Han, Caihua Shan, Yifei Shen, Can Xu, Han Yang, Xiang Li, and Dongsheng Li. Training-free multi-objective diffusion model for 3d molecule generation. In _The Twelfth International Conference on Learning Representations_, 2023.
* [15] Elad Hazan and Sham Kakade. Revisiting the polyak step size. _arXiv preprint arXiv:1905.00313_, 2019.
* [16] Yutong He, Naoki Murata, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Dongjun Kim, Wei-Hsiang Liao, Yuki Mitsufuji, J Zico Kolter, Ruslan Salakhutdinov, et al. Manifold preserving guided diffusion. _arXiv preprint arXiv:2311.16424_, 2023.
* [17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.
* [18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* [20] Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In _International conference on machine learning_, pp. 8867-8887. PMLR, 2022.

* [21] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In _International Conference on Machine Learning_, pp. 9902-9915. PMLR, 2022.
* [22] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-lojasiewicz condition. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16_, pp. 795-811. Springer, 2016.
* [23] Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo. Adaptdiffuser: Diffusion models as adaptive self-evolving planners. _arXiv preprint arXiv:2302.01877_, 2023.
* [24] Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, and Trevor Darrell. More control for free! image synthesis with semantic diffusion guidance. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pp. 289-299, 2023.
* [25] Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, and Qiang Liu. Fusedream: Training-free text-to-image generation with improved clip+ gan space optimization. _arXiv preprint arXiv:2112.01573_, 2021.
* [26] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _Advances in Neural Information Processing Systems_, 35:5775-5787, 2022.
* [27] Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, and Jun Zhu. Contrastive energy prediction for exact energy-guided diffusion sampling in offline reinforcement learning. _arXiv preprint arXiv:2304.12824_, 2023.
* [28] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 11461-11471, 2022.
* [29] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. _arXiv preprint arXiv:1706.06083_, 2017.
* [30] Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Bochen Guan, Yin Li, and Bolei Zhou. Freccontrol: Training-free spatial control of any text-to-image diffusion model with any condition. _arXiv preprint arXiv:2312.07536_, 2023.
* [31] Jiachun Pan, Jun Hao Liew, Vincent YF Tan, Jiashi Feng, and Hanshu Yan. Adjointdpm: Adjoint sensitivity method for gradient backpropagation of diffusion probabilistic models. _arXiv preprint arXiv:2307.10711_, 2023.
* [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.
* [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.
* [34] Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. _Advances in Neural Information Processing Systems_, 32, 2019.
* [35] Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adversarially robust imagenet models transfer better? _Advances in Neural Information Processing Systems_, 33:3533-3545, 2020.
* [36] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.
* [37] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. Loss-guided diffusion models for plug-and-play controllable generation. 2023.
* [38] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. _arXiv preprint arXiv:1312.6199_, 2013.

* [39] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In _European Conference on Computer Vision_, pp. 358-374. Springer, 2022.
* [40] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano. Human motion diffusion model. _arXiv preprint arXiv:2209.14916_, 2022.
* [41] Shengbang Tong, Erik Jones, and Jacob Steinhardt. Mass-producing failures of multimodal systems with language models. _Advances in Neural Information Processing Systems_, 36, 2024.
* [42] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 1921-1930, 2023.
* [43] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* [44] Wei Wang, Dongqi Han, Xufang Luo, Yifei Shen, Charles Ling, Boyu Wang, and Dongsheng Li. Toward open-ended embodied tasks solving. In _NeurIPS 2023 Agent Learning in Open-Endedness Workshop_, 2023.
* [45] Xiaoyu Xiang, Ding Liu, Xiao Yang, Yiheng Zhu, Xiaohui Shen, and Jan P Allebach. Adversarial open domain adaptation for sketch-to-photo synthesis. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, 2022.
* [46] Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong Tian, Ziming Liu, and Tommi Jaakkola. Restart sampling for improving generative processes. _Advances in Neural Information Processing Systems_, 36, 2024.
* [47] Lingxiao Yang, Shutong Ding, Yifan Cai, Jingyi Yu, Jingya Wang, and Ye Shi. Guidance with spherical gaussian constraint for conditional diffusion. _arXiv preprint arXiv:2402.03201_, 2024.
* [48] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In _Proceedings of the European conference on computer vision (ECCV)_, pp. 325-341, 2018.
* [49] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Training-free energy-guided conditional diffusion model. _arXiv preprint arXiv:2303.09833_, 2023.
* [50] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 3836-3847, 2023.
* [51] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for data-efficient gan training. _Advances in neural information processing systems_, 33:7559-7570, 2020.
* [52] Shuxin Zheng, Jiyan He, Chang Liu, Yu Shi, Ziheng Lu, Weitao Feng, Fusong Ju, Jiaxi Wang, Jianwei Zhu, Yaosen Min, et al. Towards predicting equilibrium distributions for molecular systems with deep learning. _arXiv preprint arXiv:2306.05445_, 2023.

Related Works

### Training-based Gradient Guidance

The training-based gradient guidance paradigm, such as classifier guidance, is a predominant approach for diffusion guidance. The core objective is to train a time-dependent network that approximates \(p_{t}(\mathbf{y}|\mathbf{x}_{t})\) in the RHS of (2), and to utilize the resulting gradient as guidance. The most well-known example is classifier guidance, which involves training a classifier on noisy images. However, classifier guidance is limited to class conditions and is not adaptable to other forms of control, such as image and text guidance. To address this limitation, there are two main paradigms. The first involves training a time-dependent network that aligns features extracted from both clean and noisy images, as described by [24]. The training process is outlined as follows:

\[\min_{\psi}\quad\mathbb{E}_{p(\mathbf{x}_{0},\mathbf{x}_{t})}d(f_{\psi}(\mathbf{x}_{t},t), f_{\phi}(\mathbf{x}_{0})),\]

where \(d(\cdot,\cdot)\) represents a loss function, such as cross-entropy or the \(\ell_{2}\) norm. If time-dependent networks for clean images are already available, training can proceed in a self-supervised fashion without the need for labeled data. The second paradigm, as outlined by [21], involves training an energy-based model to approximate \(p_{t}(\mathbf{y}|\mathbf{x}_{t})\). The training process is described as follows:

\[\min_{\psi}\quad\mathbb{E}_{p(\mathbf{x}_{0},\mathbf{x}_{t})}|\ell(f_{\psi}(\mathbf{x}_{t},t),\mathbf{y})-\ell(f_{\phi}(\mathbf{x}_{0}),\mathbf{y})|.\]

However, it is observed in [27] that none of these methods can accurately approximate the true energy in (4). The authors of [27] propose an algorithm to learn the true energy. The loss function is a contrastive loss

\[\min_{\psi}\quad\mathbb{E}_{p(\mathbf{x}_{0}^{i},\mathbf{x}_{t}^{i})}\exp(-\ell(f_{ \phi}(\mathbf{x}_{0}),\mathbf{y}))\left[-\sum_{i=1}^{K}\log\frac{\exp(\ell(f_{\psi}(\bm {x}_{t}^{i},t),\mathbf{y}^{i})}{\sum_{j=1}^{K}\exp(-\ell(f_{\psi}(\mathbf{x}_{t}^{i},t),\mathbf{y}^{i}))}\right],\]

where \((\mathbf{x}_{0}^{i},\mathbf{x}_{t}^{i})\) are \(K\) paired data samples from \(p(\mathbf{x}_{0}^{i},\mathbf{y}^{i})\). Theorem 3.2 in [27] proves that the optimal \(f_{\psi^{*}}\) satisfied that \(\nabla_{\mathbf{x}_{t}}\ell(f_{\psi^{*}}(\mathbf{x}_{t}^{i},t),\mathbf{y}^{i})=\nabla_{\bm {x}_{t}}p_{t}(\mathbf{y}|\mathbf{x}_{t})\).

Although this paper focuses on training-free guidance, the findings in this paper can be naturally extended to all training-based gradient guidance schemes. Firstly, the issue of adversarial gradients cannot be resolved without additional training; hence, all the aforementioned methods are subject to adversarial gradients. Empirical evidence for this is presented in Fig. 2, which illustrates that the gradients from an adversarially robust classifier are markedly more vivid than those from time-dependent classifiers. Consequently, it is anticipated that incorporating additional adversarial training into these methods would enhance the quality of the generated samples. Secondly, since these methods are dependent on gradients, employing a more sophisticated gradient solver could further improve their NFEs.

### Adversarial Attack and Robustness

Adversarial attacks and robustness constitute a fundamental topic in deep learning [4]. An adversarial attack introduces minimal, yet strategically calculated, changes to the original data that are often imperceptible to humans, leading models to make incorrect predictions. The most common attacks are gradient-based, for example, the Fast Gradient Sign Method (FGSM) [13], Projected Gradient Descent (PGD) [29], Smoothed Gradient Attacks [1], and Momentum-Based Attacks [9]. An attack is akin to classifier guidance or training-free guidance, which uses the gradient of a pre-trained network for guidance. Should the gradient be adversarial, the guidance will be compromised. This paper establishes the relationship between training-free loss-guided diffusion models and adversarial attacks in two ways. Firstly, we prove that training-free guidance is more sensitive to an adversarial gradient. Secondly, in Section 4.2, we demonstrate that borrowing an adaptive gradient scheduler can improve convergence. The optimizers from adversarial attack literature may also expedite the convergence of the diffusion ODE.

## Appendix B Baselines and Experimental Settings

### Details of the Baselines

We use the following training-free diffusion guidance methods as baselines for comparison:* **Universal Guidance (UG) [2]** employs guidance as delineated in (5) and uses time-travel strategies outlined in Algorithm 3 to enhance performance. The time-travel trick is used for all time steps \(t\). UG also utilizes backward guidance, which takes multiple gradient steps at each time step.
* **FreeDoM [49]** is also founded on (5) and time-travel trick. In addition, FreeDoM incorporates a time-dependent step size for each gradient guidance and judiciously selects the diffusion step for executing time-travel trick.
* **Loss-guided Diffusion with Monte Corlo (LGD-MC) [37]** utilizes guidance from (6) and we set \(n=10\) in the experiments.
* **Manifold Guided Preserving Diffusion (MPGD) [16]** takes the derivative with respect to estimated clean image \(\mathbb{E}[\mathbf{x}_{0}|\mathbf{x}_{t}]\) instead of \(\mathbf{x}_{t}\). Let \(\mathbf{x}_{0|t}=\mathbb{E}[\mathbf{x}_{0}|\mathbf{x}_{t}]\), MPGD steps are expressed as following: \[\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{y}|\mathbf{x}_{t}):=-\sqrt{\alpha_{t-1}}\nabla_{ \mathbf{x}_{0|t}}\log\left[\exp(-\ell(f_{\phi}(\mathbf{x}_{0|t}),\mathbf{y})\right].\] MPGD-Z adopts an additional auto-encoder to preserve manifold constraints. The details procedures of MPGD-Z are described in Algorithm 3 of [16].

### MPGD for Motion Diffusion

For the process of motion diffusion, the application of both MPGD-Z and MPGD-AE is precluded due to the absence of pretrained auto-encoders specific to motion diffusion. An implementation of MPGD without projection (MPGD w/o proj) was attempted for motion diffusion; however, it was unsuccessful in accurately navigating towards the target. This failure is attributed to the presence of spurious correlations within the targeting loss specific to MPGD, a phenomenon not observed in the other baseline methodologies. Sepcifically, the gradient formulation in MPGD is detailed as follows:

\[\text{grad\_MPGD}=\begin{cases}2(\mathbf{y}_{\text{target}}-\mathbf{x}_{0|t})&\text{ if }t==T\\ 0&\text{otherwise}\end{cases}\]

The gradient in FreeDoM and other methods is given by

\[\text{grad\_DPS}=2(\mathbf{y}_{\text{target}}-\mathbf{x}_{0|t})\cdot\frac{\mathbf{I}+ \sigma_{t}^{2}\nabla^{2}\log p_{t}(\mathbf{x}_{t})}{\sqrt{\alpha_{t}}}.\]

Analysis of the aforementioned equations reveals that, within the MPGD framework, only the final motion step is influenced by the gradient, a characteristic not shared by alternative methodologies. Consequently, this exclusive focus on the last step results in disproportionately strong guidance at this juncture, while earlier steps suffer from a lack of directional input. This imbalance may adversely affect the overall quality of the samples produced. Empirical observations substantiate that MPGD struggles to achieve targeted outcomes when a nuanced adjustment of step size is required. Given these limitations, MPGD has been excluded from the comparative analysis presented in Table 3.

### Prompts for Motion Diffusion

We follow the prompts and evaluation settings in [37]. The prompts are (i) "the person is walking backwards"; (ii) "a person walking around in a balance beam"; (iii) "the person is walking"; (iv) "the person is jogging". We consider three different directions for each prompt, and each direction has 10 random seeds, the metrics are then averaged together over the \(30\) synthesized motions.

## Appendix C More Discussions

### Concentration of Estimated Clean Samples

It has been demonstrated in [7] that, given a fixed \(\mathbf{x}_{0}\), the distribution of the noisy data \(\mathbf{x}_{t}\) is concentrated on a spherical shell. An extension of this theorem presented in the "High Dimensional Probability" textbook by Vershynin [43] elucidates that the conditional distribution \(q(\mathbf{x}_{0}|\mathbf{x}_{t})\) also exhibits concentration on a spherical shell, provided that its coordinates are sub-Gaussian.

**Theorem C.1**.: _(Theorem 3.1.1 in [43]) Denote \(\mathbf{x}_{0}=\mathbf{x}_{t}+\mathbf{g}\in\mathbb{R}^{n}\), where \(g=(g_{1},\cdots,g_{n})\). Assume that \(g_{i}\) are independent identically distributed, \(\mathbb{E}[g_{i}^{2}]=\frac{r_{2}^{2}}{n}\) and there exists constant \(c_{1}\) such that \(\mathbb{P}[g_{i}\geq t]\leq\exp(-c_{1}t^{2})\), then we have_

\[\mathbb{P}[\|\mathbf{x}_{t}-\mathbf{x}_{0}\|_{2}^{2}-r_{t}^{2}|\geq t]\leq\exp(-c_{2}nt ^{2}),\]

_where \(c_{2}\) is a constant._

Theorem C.1 establishes that the distribution \(q(\mathbf{x}_{0}|\mathbf{x}_{t})\) exhibits concentration on a spherical shell at an exponential rate. For high-dimensional data, such as images, it is reasonable to infer that \(\mathbf{x}_{0}\) is predominantly situated on this spherical shell.

### Time-travel Trick

```
1:for\(t=T,\cdots,1\)do
2:for\(i=1,\cdots,s\)do
3:\(\mathbf{x}_{t-1}^{i}=\text{DDIM with Guidance}(\mathbf{x}_{t}^{i-1})\)
4:if\(i<s\)then
5:\(\beta_{t}=\alpha_{t}/\alpha_{t-1},\mathbf{n}\sim\mathcal{N}(0,\mathbf{I})\)
6:\(\mathbf{x}_{t}^{i}=\sqrt{\beta_{t}}\mathbf{x}_{t-1}^{i}+\sqrt{1-\beta_{t}}\mathbf{n}\)
7:endif
8:\(\mathbf{x}_{t-1}^{0}=\mathbf{x}_{t-1}^{s}\)
9:endfor
10:endfor
```

**Algorithm 3** Time Travel

The technique of time-travel, also referred to as "resampling", has been proposed as a solution to complex generative problems [28], facilitating successful training-free guidance in tasks such as CLIP-guided ImageNet generation and layout guidance for stable diffusion, as illustrated in Figure 2 of [49] and Figure 8 of [2], respectively. The procedure of the time-travel trick is shown in Algorithm 3, which involves recursive execution of individual sampling steps.

### More Quantitative Experiments

**Two-stage convergence:** We plot the convergence curve of more images in Figure 5. The diffusion model is ImageNet diffusion and the guidance network is ResNet-50. The convergence is two-stage: the objective value oscillates when t is large, followed by a swift decrease, which verifies Proposition 3.1.

**Misaligned gradients:** For quantitative experiments, we use robust ResNet-50 to assess the presence of misaligned gradient. Since ResNet-50 serves as our guidance network, a large loss gap between

Figure 7: More experiments of the convergence plot.

the standard ResNet-50 and the robust variant indicates a severe adversarial gradient issue. In Table 4, the values represent the loss value. The columns denote the guidance network type, while the rows indicate the loss tested on either ResNet-50 or Robust ResNet-50. The values reported are the average over 1000 images, providing a quantitative evaluation. When using ResNet-50 for guidance, its loss on ResNet-50 is as low as that of real images from the same class (value 5.91). However, the loss for Robust ResNet-50 is significantly higher (value 6.17), suggesting susceptibility to misaligned gradients. By employing ResNet-50 with random augmentation for guidance, we observe a marked reduction in this gap (from 5.91 to 5.98), underscoring the effectiveness of our proposed method.

**Slower Convergence:** Table 5 aims to compare the convergence of training-free guidance with training-based guidance, specifically PPAP [12]. Given that classifier guidance presents a simple scenario and all methods converge relatively fast, it tends to obscure the observations. Consequently, we have directed our attention towards the more complex task of segmentation map guidance in Table 1. The columns in Table 5 list different methods, while the rows indicate the sampling steps. The values represent the "distance" mentioned in Table 1 and are averaged over 1000 images under the same conditions as those in Table 1. Our observations reveal a significant discrepancy in the objective values between the training-free FreeDoM and the training-based PPAP [12], particularly at a lower number of steps (20 or 50 steps), which indicates slower convergence. However, when we incorporate the Polyak step size into FreeDoM, this convergence issue is substantially mitigated.

### Efficiency of Random Augmentation

Given the multiple invocations of the guidance network necessitated by random augmentation (RA), concerns regarding the efficiency of this approach are understandable. However, it is important to note that, compared to the diffusion backbone, the guidance network exhibits a more lightweight architecture, thereby mitigating any significant increase in computational demand. To empirically illustrate this point, we present the computation times associated with varying degrees of augmentation in Table 6. In the conducted experiments, we set the cardinality of the set \(\mathcal{T}\) to \(10\), thereby having little impact on the inference time. These experiments were conducted on a single NVIDIA A100 GPU.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline Setting & w/o RA & \(|\mathcal{T}|=1\) & \(|\mathcal{T}|=10\) & \(|\mathcal{T}|=20\) & \(|\mathcal{T}|=30\) \\ \hline CLIP Score & 0.541 & 0.544 & 0.571 & 0.592 & 0.625 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Inference time (seconds per diffusion step) of different random augmentation configurations. The diffusion backbone is ImageNet diffusion and the guidance network is CLIP-B/16.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline  & RN-50 & RN-50+RA & Robust RN-50 & Real Data \\ \hline RN-50 & 5.91 & 5.91 & 5.93 & 5.91 \\ \hline Robust RN-50 & 6.17 & 5.98 & 5.93 & 5.93 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Quantitative experiments for the adversarial gradient. RN-50 stands for ResNet-50 and RA stands for random augmentation trick. Robust RN-50 is adversarial robust ResNet-50 from [35]. The columns represent different guidance networks.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline  & FreeDoM & FreeDoM + P & PPAP (Training-based) \\ \hline DDIM-20 & 2439 & 2190 & 2032 \\ \hline DDIM-50 & 1821 & 1635 & 1607 \\ \hline DDIM-100 & 1657 & 1504 & 1509 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Quantitative experiments for the slower convergence. P stands for Polyak step size. The experimental setting follows the segmentation map guidance of Table 1.

Proofs

### Definitions

This subsection introduces a few definitions that are useful in the following sections.

**Definition D.1**.: (\(L\)-Lipschitz) A function \(f:\mathbb{R}^{n}\to\mathbb{R}^{m}\) is said to be \(L\)-Lipschitz if there exists a constant \(L\geq 0\) such that \(\|f(\mathbf{x}_{2})-f(\mathbf{x}_{1})\|\leq L\|\mathbf{x}_{2}-\mathbf{x}_{1}\|\) for all \(\mathbf{x}_{1},\mathbf{x}_{2}\in\mathbb{R}^{n}\).

**Definition D.2**.: (PL condition) A function \(f:\mathbb{R}^{n}\to\mathbb{R}\) satisfies PL condition with parameter \(\mu\) if \(\|\nabla f(\mathbf{x})\|^{2}\geq\mu f(\mathbf{x})\).

### Proof for Proposition 3.1

Denote \(\hat{\mathbf{x}}_{0}=\mathbb{E}_{p(\mathbf{x}_{0}|\mathbf{x}_{t})}(\mathbf{x}_{0})\), the gradient guidance term in (5) can be written as the following:

\[\nabla_{\mathbf{x}_{t}}\ell\left[f_{\phi}(\hat{\mathbf{x}}_{0}),\mathbf{y} \right]=\frac{\partial\ell}{\partial\hat{\mathbf{x}}_{0}}\nabla_{\mathbf{x}_{t}}\left( \frac{\mathbf{x}_{t}+\sigma_{t}^{2}\nabla_{x_{t}}\log p_{t}(\mathbf{x}_{t})}{\sqrt{ \alpha_{t}}}\right)=\frac{\partial\ell}{\partial\hat{\mathbf{x}}_{0}}\frac{\text{ Cov}[\mathbf{x}_{0}|\mathbf{x}_{t}]}{\sigma_{t}^{2}\sqrt{\alpha_{t}}}, \tag{9}\]

where the last equality follows the variance of Tweedie's formula \(\text{Cov}[\mathbf{x}_{0}|\mathbf{x}_{t}]=\sigma_{t}^{2}(\mathbf{I}+\sigma_{t}^{2}\nabla^{ 2}\log p_{t}(\mathbf{x}_{t}))\)[10].

For the first condition, the Lipschitz constant satisfies

\[|\ell_{t}(\mathbf{x}_{1})-\ell_{t}(\mathbf{x}_{2})| \leq\frac{L_{f}}{\sqrt{\alpha_{t}}}\|\mathbf{x}_{1}-\nabla_{\mathbf{x}_{1 }}\log p_{t}(\mathbf{x}_{1})-\mathbf{x}_{2}+\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{x}_{2})\|\] \[\leq\frac{L_{f}(1+L_{p})}{\sqrt{\alpha_{t}}}\|\mathbf{x}_{1}-\mathbf{x}_{ 2}\|,\]

and the PL constant satisfies

\[\ell_{t}(\mathbf{x}_{t})\leq\frac{1}{\mu}\left\|\frac{\partial\ell}{ \partial\hat{\mathbf{x}}_{0}}\right\|^{2}=\frac{1}{\mu}\|\nabla_{\mathbf{x}_{t}}\ell \cdot\text{Cov}^{-1}(\mathbf{x}_{0}|\mathbf{x}_{t})\sigma_{t}^{2}\sqrt{\alpha_{t}}\|^ {2}\leq\frac{\sigma_{t}^{4}\alpha_{t}}{\mu\lambda_{min}^{2}}\|\nabla_{\mathbf{x}_{ t}}\ell\|^{2}.\]

The second and third conditions directly follow Lemma D.3.

**Lemma D.3**.: _(Linear Convergence Under PL condition; [22]) Denote \(\mathbf{x}^{0}\) as the initial point and \(\mathbf{x}^{t}\) as the point after \(t\) gradient steps. If the function is \(L\)-Lipschitz and \(\mu\)-PL, gradient descent with a step size \(\eta=\frac{1}{L}\) converges to a global solution with \(\ell(\mathbf{x}^{t})\leq(1-\mu/L)^{t}\ell(\mathbf{x}^{0})\)._

### Proof for Proposition 3.2

Proof.: The proof of this theorem is based on the proof of Lemma 1 in [34]. By the definition of expectation, we have

\[\hat{f}(\mathbf{x}) =\mathbb{E}_{\mathbf{\epsilon}\sim\mathcal{N}(0,\mathbf{I})}[f(\mathbf{x}+ \sigma_{t}\mathbf{\epsilon})]=(f\otimes\mathcal{N}(0,\sigma_{t}^{2}\mathbf{I}))(\mathbf{ x})\] \[=\frac{1}{(2\pi\sigma_{t}^{2})^{n/2}}\int_{\mathbb{R}^{n}}f(\mathbf{ z})\exp\left(-\frac{1}{2\sigma_{t}^{2}}\|\mathbf{x}-\mathbf{z}\|^{2}\right)\mathrm{d} \mathbf{z}.\]

We then show that for any unit direction \(\mathbf{u}\), \(\mathbf{u}^{T}\nabla\hat{f}(\mathbf{x})\leq\sqrt{\frac{2}{\pi\sigma_{t}^{2}}}\). The derivative of \(\hat{f}\) is given by

\[\nabla\hat{f}(\mathbf{x}) =\frac{1}{(2\pi\sigma_{t}^{2})^{n/2}}\int_{\mathbb{R}^{n}}f(\mathbf{z })\nabla\exp\left(-\frac{1}{2\sigma_{t}^{2}}\|\mathbf{x}-\mathbf{z}\|^{2}\right) \mathrm{d}\mathbf{z}\] \[=\frac{1}{(2\pi\sigma_{t}^{2})^{n/2}\sigma_{t}^{2}}\int_{\mathbb{R }^{n}}f(\mathbf{z})(\mathbf{x}-\mathbf{z})\exp\left(-\frac{1}{2\sigma_{t}^{2}}\|\mathbf{x}- \mathbf{z}\|^{2}\right)\mathrm{d}\mathbf{z}.\]

Thus, the Lipschitz constant is computed as

\[\mathbf{u}^{T}\nabla\hat{f}(\mathbf{x}) \leq\frac{C}{(2\pi\sigma_{t}^{2})^{n/2}}\int_{\mathbb{R}^{n}}|\mathbf{ u}^{T}(\mathbf{x}-\mathbf{z})/\sigma_{t}^{2}|\exp\left(-\frac{1}{2\sigma_{t}^{2}}\|\mathbf{x}- \mathbf{z}\|^{2}\right)\mathrm{d}\mathbf{z}\] \[=\frac{C}{(2\pi\sigma_{t}^{2})^{1/2}}\int_{-\infty}^{\infty}|s| \exp\left(-\frac{1}{2}s^{2}\right)\mathrm{d}s=\sqrt{\frac{2}{\pi\sigma_{t}^{2}}}.\]

[MISSING_PAGE_EMPTY:19]

### Human Motion

See "TeX Source/videos.pptx".

Figure 8: More qualitative results of CelebA-HQ with zero-shot segmentation guidance. The images are randomly selected.

Figure 9: More qualitative results of CelebA-HQ with zero-shot sketch guidance. The images are randomly selected.

Figure 10: More qualitative results of CelebA-HQ with zero-shot text guidance. The images are randomly selected.

Figure 11: More qualitative results of ImageNet with zero-shot text guidance. The images are randomly selected.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We confirm that the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations are discussed in the conclusions. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The assumptions and proofs are provided in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper details the evaluation benchmarks and baselines, while the code is made available in the supplementary materials. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: The source code is provided in the supplementary material. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Our experiments are designed in alignment with established studies, which guided the setup of these components. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: In the experiments, we adhere to methodologies outlined in existing studies, which do not include error bars. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All experiments were conducted on a single A100 GPU, although less powerful GPUs, such as the V100 with 16GB of memory, are sufficient for replicating the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We confirm that the research conducted in the paper conform. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The societal impacts are discussed in the conclusions. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: All checkpoints referenced are documented in existing literature, which did not specify any safeguards. Consequently, it is challenging to implement safeguards for these checkpoints. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Yes. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. ** For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.