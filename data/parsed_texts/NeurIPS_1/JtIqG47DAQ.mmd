# Penalising the biases in norm regularisation enforces sparsity

Etienne Boursier

INRIA CELESTE, LMO, Orsay, France

etienne.boursier@inria.fr &Nicolas Flammarion

TML Lab, EPFL, Switzerland

nicolas.flammarion@epfl.ch

###### Abstract

Controlling the parameters' norm often yields good generalisation when training neural networks. Beyond simple intuitions, the relation between regularising parameters' norm and obtained estimators remains theoretically misunderstood. For one hidden ReLU layer networks with unidimensional data, this work shows the parameters' norm required to represent a function is given by the total variation of its second derivative, weighted by a \(\sqrt{1+x^{2}}\) factor. Notably, this weighting factor disappears when the norm of bias terms is not regularised. The presence of this additional weighting factor is of utmost significance as it is shown to enforce the uniqueness and sparsity (in the number of kinks) of the minimal norm interpolator. Conversely, omitting the bias' norm allows for non-sparse solutions. Penalising the bias terms in the regularisation, either explicitly or implicitly, thus leads to sparse estimators.

## 1 Introduction

Although modern neural networks are not particularly limited in terms of their number of parameters, they still demonstrate remarkable generalisation capabilities when applied to real-world data (Belkin et al., 2019; Zhang et al., 2021). Intriguingly, both theoretical and empirical studies have indicated that the crucial factor determining the network's generalisation properties is not the sheer number of parameters, but rather the norm of these parameters (Bartlett, 1996; Neyshabur et al., 2014). This norm is typically controlled through a combination of explicit regularisation techniques, such as weight decay (Krogh and Hertz, 1991), and some form of implicit regularisation resulting from the training algorithm employed (Soudry et al., 2018; Lyu and Li, 2019; Ji and Telgarsky, 2019; Chizat and Bach, 2020).

Neural networks with a large number of parameters can approximate any continuous function on a compact set (Barron, 1993). Thus, without norm control, the space of estimated functions encompasses all continuous functions. In the parameter space, this implies considering neural networks with infinite width and unbounded weights (Neyshabur et al., 2014). Yet, when weight control is enforced, the exact correspondence between the parameter space (i.e., the parameters \(\theta\) of the network) and the function space (i.e., the estimated function \(f_{\theta}\) produced by the network's output) becomes unclear. Establishing this correspondence is pivotal for comprehending the generalisation properties of overparameterised neural networks. Two fundamental questions arise.

**Question 1**.: _What quantity in the function space, does the parameters' norm of a neural network correspond to?_

**Question 2**.: _What functions are learnt when fitting training data with minimal parameters' norm?_

We study these questions in the context of a one-hidden ReLU layer network with a skip connection. Previous research (Kurkova and Sanguineti, 2001; Bach, 2017) has examined generalisation guarantees for small representational cost functions, where the representational cost refers to thenorm required to parameterise the function. However, it remains challenging to interpret this representational cost using classical analysis tools and identify the corresponding function space. To address this issue, Question 1 seeks to determine whether this representational cost can be translated into a more interpretable functional (pseudo) norm. Note that Question 1 studies the parameters' norm required to fit a function on an entire domain. In contrast, when training a neural network for a regression task, we only fit a finite number of points given by the training data. Question 2 arises to investigate the properties of the learned functions when minimising some empirical loss with a regularisation of the parameters' norm regardless of whether it is done explicitly or implicitly.

In relation to our work, Savarese et al. (2019), Ongie et al. (2019) address Question 1 for one-hidden layer ReLU neural networks, focusing on univariate and multivariate functions, respectively. For a comprehensive review of this line of work, we recommend consulting the survey of Parhi and Nowak (2023). On the other hand, Parhi and Nowak (2021), Debarre et al. (2022), Stewart et al. (2022) investigate Question 2 specifically in the univariate case. Additionally, Sanford et al. (2022) examine a particular multidimensional case. However, all of these existing studies overlook the bias parameters of the neural network when considering the \(\ell_{2}\) regularisation term. By omitting the biases, the analysis and solutions to these questions become simpler.

In sharp contrast, our work addresses both Questions 1 and 2 for univariate functions _while also incorporating regularisation of the bias parameters_. It may appear as a minor detail--it is commonly believed that similar estimators are obtained whether or not the biases' norm1 is penalised (see e.g. Ng, 2011). Nonetheless, our research demonstrates that penalising the bias terms enforce sparsity and uniqueness of the estimated function, which is not achieved without including the bias regularisation. The practical similarity between these two explicit regularisations can be attributed to the presence of implicit regularisation, which considers the bias terms as well. The updates performed by first-order optimisation methods do not distinguish between bias and weight parameters, suggesting that they are subject to the same implicit regularisation. Consequently, while both regularisation approaches may yield similar estimators in practical settings, we contend that the theoretical estimators obtained with bias term regularisation capture the observed implicit regularisation effect. Hence, it is essential to investigate the implications of penalising the bias terms when addressing Questions 1 and 2, as the answers obtained in this scenario significantly differ from those without bias penalisation.

Footnote 1: Even though Goodfellow et al. (2016, Chapter 7) claim that penalising the biases might lead to underfitting, our work does not focus on the optimisation aspect and assumes interpolation occurs.

It is also worth mentioning that Shevchenko et al. (2022), Safran et al. (2022) prove that gradient flow learns sparse estimators for networks with ReLU activations. These sparsity guarantees are yet much weaker (larger number of activated directions) as they additionally deal with optimisation considerations (in opposition to directly considering the minimiser of the optimisation problem in both our work and the line of works mentioned above).

Contributions.After introducing the setting in Section 2, we address Question 1 in Section 3 using a similar analysis approach as Savarese et al. (2019). The key result, Theorem 1, establishes that the representational cost of a function, when allowed a _free_ skip connection, is given by the weighted total variation of its second derivative, incorporating a \(\sqrt{1+x^{2}}\) term. Notably, penalising the bias terms introduces a \(\sqrt{1+x^{2}}\) multiplicative weight in the total variation, contrasting with the absence of bias penalisation.

This weighting fundamentally impacts the answer to Question 2. In particular, it breaks the shift invariance property of the function's representational cost, rendering the analysis technique proposed by Debarre et al. (2022) inadequate2. To address this issue, we delve in Sections 4 and 5 into the computation and properties of solutions to the optimisation problem:

Footnote 2: Although shift invariance is useful for analytical purposes, it is not necessarily desirable in practice. Notably, Nacson et al. (2022) show that dealing with uncentered data might be beneficial for learning relevant features, relying on the lack of shift invariance.

\[\inf_{f}\left\|\sqrt{1+x^{2}}f^{\prime\prime}\right\|_{\mathrm{TV}}\quad\text { subject to }\forall i\in[n],\ f(x_{i})=y_{i}.\]

In Section 4, we reformulate this problem as a continuous dynamic program, enabling a simpler analysis of the minimisation problem. Leveraging this dynamic program reformulation, Section 5 establishes the uniqueness of the solution. Additionally, under certain data assumptions, we demonstrate that the minimiser is among the sparest interpolators in terms of the number of kinks. It is worth noting that similar results have been studied in the context of sparse spikes deconvolution (Candes and Fernandez-Granda, 2014; Fernandez-Granda, 2016; Poon et al., 2019), and our problem can be seen as a generalisation of basis pursuit (Chen et al., 2001) to infinite-dimensional parameter spaces. However, classical techniques for sparse spikes deconvolution are ill-suited for addressing Question 2, as the set of sparsest interpolators is infinite in our setting.

Finally, the significance of bias term regularisation in achieving sparser estimators during neural network training is illustrated on toy examples in Section 6. To ensure conciseness, only proof sketches are presented in the main paper, while the complete proofs can be found in the Appendix.

## 2 Infinite width networks

This section introduces the considered setting, representing unidimensional functions as infinite width networks. Some precise mathematical arguments are omitted here, since this construction follows directly the lines of Savarese et al. (2019); Ongie et al. (2019). This work considers unidimensional functions \(f_{\theta}:\mathbb{R}\to\mathbb{R}\) parameterised by a one hidden layer neural networks with ReLU activation as

\[f_{\theta}(x)=\sum_{j=1}^{m}a_{j}\sigma(w_{j}x+b_{j}),\]

where \(\sigma(z)=\max(0,z)\) is the ReLU activation and \(\theta=(a_{j},w_{j},b_{j})_{j\in[m]}\in\mathbb{R}^{3m}\) are the parameters defining the neural network. The vector \(\mathbf{a}=(a_{j})_{j\in[m]}\) stands for the weights of the last layer, while \(\mathbf{w}\) and \(\mathbf{b}\) respectively stand for the weights and biases of the hidden layer. For any width \(m\) and parameters \(\theta\), the quantity of importance is the squared Euclidean norm of the parameters: \(\|\theta\|_{2}^{2}=\sum_{j=1}^{m}a_{j}^{2}+w_{j}^{2}+b_{j}^{2}\).

We recall that contrary to Savarese et al. (2019); Ongie et al. (2019), the bias terms are included in the considered norm here. We now define the representational cost of a function \(f:\mathbb{R}\to\mathbb{R}\) as

\[R(f)=\inf_{\begin{subarray}{c}m\in\mathbb{N}\\ \theta\in\mathbb{R}^{3m}\end{subarray}}\frac{1}{2}\|\theta\|_{2}^{2}\quad\text{ such that}\quad f_{\theta}=f.\]

By homogeneity of the parameterisation, a typical rescaling trick (see e.g. Neyshabur et al., 2014; Theorem 1) allows to rewrite

\[R(f)=\inf_{m,\theta\in\mathbb{R}^{3m}}\|\mathbf{a}\|_{1}\quad\text{such that}\quad f_{\theta}=f\text{ and }w_{j}^{2}+b_{j}^{2}=1\text{ for any }j\in[m].\]

Note that \(R(f)\) is only finite when the function \(f\) is exactly described as a finite width neural network. We aim at extending this definition to a much larger functional space, i.e. to any function that can be arbitrarily well approximated by finite width networks, while keeping a (uniformly) bounded norm of the parameters. Despite approximating the function with finite width networks, the width necessarily grows to infinity when the approximation error goes to \(0\). Similarly to Ongie et al. (2019), define

\[\overline{R}(f)=\lim_{\varepsilon\to 0^{+}}\left(\inf_{m,\theta\in\mathbb{R}^{3m}} \frac{1}{2}\|\theta\|_{2}^{2}\quad\text{such that}\quad|f_{\theta}(x)-f(x)| \leq\varepsilon\text{ for any }x\in\left[-\nicefrac{{1}}{{\varepsilon}},\nicefrac{{1}}{{\varepsilon}}\right] \right).\]

Note that the approximation has to be restricted to the compact set \(\left[-\nicefrac{{1}}{{\varepsilon}},\nicefrac{{1}}{{\varepsilon}}\right]\) to avoid problematic degenerate situations. The functional space for which \(\overline{R}(f)\) is finite is much larger than for \(R\), and includes every compactly supported Lipschitz function, while coinciding with \(R\) when the latter is finite.

By rescaling argument again, we can assume the hidden layer parameters \((w_{j},b_{j})\) are in \(\mathbb{S}_{1}\) and instead consider the \(\ell_{1}\) norm of the output layer weights. The parameters of a network can then be seen as a discrete signed measure on the unit sphere \(\mathbb{S}_{1}\). When the width goes to infinity, a limit is then properly defined and corresponds to a possibly continuous signed measure. Mathematically, define \(\mathcal{M}(\mathbb{S}_{1})\) the space of signed measures \(\mu\) on \(\mathbb{S}_{1}\) with finite total variation \(\left\|\mu\right\|_{\mathrm{TV}}\). Following the typical construction of Bengio et al. (2005); Bach (2017), an infinite width network is parameterised by a measure \(\mu\in\mathcal{M}(\mathbb{S}_{1})\) as3

Footnote 3: By abuse of notation, we write both \(f_{\theta}\) and \(f_{\mu}\), as it is clear from context whether the subscript is a vector or a measure.

\[f_{\mu}:x\mapsto\int_{\mathbb{S}_{1}}\sigma(wx+b)\mathrm{d}\mu(w,b). \tag{1}\]

Similarly to Ongie et al. (2019), \(\overline{R}(f)\) verifies the equality

\[\overline{R}(f)=\inf_{\mu\in\mathcal{M}(\mathbb{S}_{1})}\|\mu\|_{\mathrm{TV}} \,\text{ such that }f=f_{\mu}.\]The right term defines the \(\mathcal{F}_{1}\) norm (Kurkova and Sanguineti, 2001), i.e. \(\overline{R}(f)=\|f\|_{\mathcal{F}_{1}}\). The \(\mathcal{F}_{1}\) norm is intuited to be of major significance for the empirical success of neural networks. In particular, generalisation properties of small \(\mathcal{F}_{1}\) norm estimators are derived by Kurkova and Sanguineti (2001); Bach (2017), while many theoretical results support that training one hidden layer neural networks with gradient descent often yields an implicit regularisation on the \(\mathcal{F}_{1}\) norm of the estimator (Lyu and Li, 2019; Ji and Telgarsky, 2019; Chizat and Bach, 2020; Boursier et al., 2022). However, this implicit regularisation of the \(\mathcal{F}_{1}\) norm is not systematic and some works support that a different quantity can be implicitly regularised on specific examples (Razin and Cohen, 2020; Vardi and Shamir, 2021; Chistikov et al., 2023). Still, \(\mathcal{F}_{1}\) norm seems to be closely connected to the implicit bias and its significance is the main motivation of this paper. While previous works also studied the representational costs of functions by neural networks (Savarese et al., 2019; Ongie et al., 2019), they did not penalise the bias term in the parameters' norm, studying a functional norm slightly differing from the \(\mathcal{F}_{1}\) norm. This subtlety is at the origin of different levels of sparsity between the obtained estimators with or without penalising the bias terms, as discussed in Sections 5 and 6; where sparsity of an estimator here refers to the minimal width required for a network to represent the function (or similarly to the cardinality of the support of \(\mu\) in Equation (1)). This notion of sparsity is more meaningful than the sparsity of the parameters \(\theta\) here, since different \(\theta\) (with different levels of sparsity) can represent the exact same estimated function.

### Unpenalised skip connection

Our objective is now to characterise the \(\mathcal{F}_{1}\) norm of unidimensional functions and minimal norm interpolators, which can be approximately obtained when training a neural network with norm regularisation. The analysis and result yet remain complex despite the unidimensional setting. Allowing for an unpenalised affine term in the neural network representation leads to a cleaner characterisation of the norm and description of minimal norm interpolators. As a consequence, we parameterise in the remaining of this work finite and infinite width networks as follows:

\[f_{\theta,a_{0},b_{0}}:x\mapsto a_{0}x+b_{0}+f_{\theta}(x),\quad\text{and}\quad f _{\mu,a_{0},b_{0}}:x\mapsto a_{0}x+b_{0}+f_{\mu}(x),\]

where \((a_{0},b_{0})\in\mathbb{R}^{2}\). The affine part \(a_{0}x+b_{0}\) actually corresponds to a _free_ skip connection in the neural network architecture (He et al., 2016) and allows to ignore the affine part in the representational cost of the function \(f\), which we now define as

\[\overline{R}_{1}(f)=\lim_{\varepsilon\to 0^{+}}\left(\inf_{\begin{subarray}{c}m, \theta\in\mathbb{R}^{3_{m}}\\ (a_{0},b_{0})\in\mathbb{R}^{2}\end{subarray}}\frac{1}{2}\|\theta\|_{2}^{2} \quad\text{such that}\quad|f_{\theta,a_{0},b_{0}}(x)-f(x)|\leq\varepsilon\text { for any }x\in[-\nicefrac{{1}}{{\varepsilon}},\nicefrac{{1}}{{\varepsilon}}] \right).\]

The representational cost \(\overline{R}_{1}(f)\) is similar to \(\overline{R}(f)\), but allows for a _free_ affine term in the network architecture. Similarly to \(\overline{R}(f)\), it can be proven, following the lines of Savarese et al. (2019); Ongie et al. (2019), that \(\overline{R}_{1}(f)\) verifies

\[\overline{R}_{1}(f)=\inf_{\begin{subarray}{c}\mu\in\mathcal{M}(\mathbb{S}_{1} )\\ a_{0},b_{0}\in\mathbb{R}\end{subarray}}\|\mu\|_{\mathrm{TV}}\text{ such that }f=f_{\mu,a_{0},b_{0}}.\]

The remaining of this work studies more closely the cost \(\overline{R}_{1}(f)\). Theorem 1 in Section 3 can be directly extended to the cost \(\overline{R}(f)\), i.e. without unpenalised skip connection. Its adapted version is given by Theorem 4 in Appendix C for completeness.

Multiple works also consider free skip connections as it allows for a simpler analysis (e.g. Savarese et al., 2019; Ongie et al., 2019; Debarre et al., 2022; Sanford et al., 2022). Since a skip connection can be represented by two ReLU neurons (\(z=\sigma(z)-\sigma(-z)\)), it is commonly believed that considering a free skip connection does not alter the nature of the obtained results. This belief is further supported by empirical evidence in Section 6 and Appendix B, where our findings hold true both with and without free skip connections.

## 3 Representational cost

Theorem 1 below characterises the representational cost \(\overline{R}_{1}(f)\) of any univariate function.

**Theorem 1**.: _For any Lipschitz function \(f:\mathbb{R}\to\mathbb{R}\),_

\[\overline{R}_{1}(f)=\left\|\sqrt{1+x^{2}}f^{\prime\prime}\right\|_{\mathrm{TV }}=\int_{\mathbb{R}}\sqrt{1+x^{2}}\;\mathrm{d}|f^{\prime\prime}|(x).\]

#### For any non-Lipschitz function, \(\overline{R}_{1}(f)=\infty\).

In Theorem 1, \(f^{\prime\prime}\) is the distributional second derivative of \(f\), which is well defined for Lipschitz functions. Without penalisation of the bias terms, the representational cost is given by the total variation of \(f^{\prime\prime}\)(Savarese et al., 2019). Theorem 1 states that penalising the biases adds a weight \(\sqrt{1+x^{2}}\) to \(f^{\prime\prime}\). This weighting favors sparser estimators when training neural networks, as shown in Section 5. Also, the space of functions that can be represented by infinite width neural networks with finite parameters' norm, when the bias terms are ignored, corresponds to functions with bounded total variation of their second derivative. When including these bias terms in the representational cost, second derivatives additionally require a _light tail_. Without a _free_ affine term, Theorem 4 in Appendix C characterises \(\overline{R}(f)\), which yields an additional term accounting for the affine part of \(f\).

We note that Remark 4.2 of E and Wojtowytsch (2021) and Theorem 1 by Li et al. (2020) are closely related to Theorems 1 and 4. However, these results only establish an equivalence between the norm \(\overline{R}(f)\) and another norm that quantifies the total variation of \(\sqrt{1+x^{2}}f^{\prime\prime}\), aside from the affine term. Our result, on the other hand, provides an exact equality between both norms, which proves to be particularly useful in the analysis of minimal norm interpolators.

**Example 1**.: _If the function \(f\) is given by a finite width network \(f(x)=\sum_{i=1}^{n}a_{i}\sigma(w_{i}x+b_{i})\) with \(a_{i},w_{i}\neq 0\) and pairwise different \(\frac{b_{i}}{w_{i}}\); Theorem 1 yields \(\overline{R}_{1}(f)=\sum_{i=1}^{n}|a_{i}|\sqrt{w_{i}^{2}+b_{i}^{2}}\). This exactly corresponds to half of the squared \(\ell_{2}\) norm of the vector \((c_{i}a_{i},\frac{w_{i}}{c_{i}},\frac{b_{i}}{c_{i}})_{i=1,\ldots,n}\) when \(c_{i}=\sqrt{\frac{\sqrt{w_{i}+b_{i}^{2}}}{|a_{i}|}}\). This vector is thus a minimal representation of the function \(f\)._

According to Theorem 1, the minimisation problem considered when training one hidden ReLU layer infinite width neural network with \(\ell_{2}\) regularisation is equivalent to the minimisation problem

\[\inf_{f}\sum_{i=1}^{n}(f(x_{i})-y_{i})^{2}+\lambda\left\|\sqrt{1+x^{2}}f^{ \prime\prime}\right\|_{\mathrm{TV}}. \tag{2}\]

_What types of functions do minimise this problem? Which solutions does the \(\left\|\sqrt{1+x^{2}}f^{\prime\prime}\right\|_{\mathrm{TV}}\) regularisation term favor?_ These fundamental questions are studied in the following sections. We show that this regularisation favors functions that can be represented by small (finite) width neural networks. On the contrary, when the weight decay term does not penalise the biases of the neural network, such a sparsity is not particularly preferred as highlighted by Section 6.

## 4 Computing minimal norm interpolator

To study the properties of solutions obtained by training data with either an implicit or explicit weight decay regularisation, we consider the minimal norm interpolator problem

\[\inf_{\theta,a_{0},b_{0}}\frac{1}{2}\|\theta\|_{2}^{2}\quad\text{such that } \forall i\in[n],\;f_{\theta,a_{0},b_{0}}(x_{i})=y_{i}, \tag{3}\]

where \((x_{i},y_{i})_{i\in[n]}\in\mathbb{R}^{2n}\) is a training set. Without loss of generality, we assume in the following that the observations \(x_{i}\) are ordered, i.e., \(x_{1}<x_{2}<\ldots<x_{n}\). Thanks to Theorem 1, this problem is equivalent, when allowing infinite width networks, to

\[\inf_{f}\left\|\sqrt{1+x^{2}}f^{\prime\prime}\right\|_{\mathrm{TV}}\quad\text {such that }\forall i\in[n],\;f(x_{i})=y_{i}. \tag{4}\]

Lemma 1 below actually makes these problems equivalent as soon as the width is larger than some threshold smaller than \(n-1\). Equation (4) then corresponds to Equation (2) when the regularisation parameter \(\lambda\) is infinitely small.

**Lemma 1**.: _The problem in Equation (4) admits a minimiser. Moreover, with \(i_{0}\coloneqq\min\{i\in[n]|x_{i}\geq 0\}\), any minimiser is of the form_

\[f(x)=ax+b+\sum_{i=1}^{n-1}a_{i}(x-\tau_{i})_{+}\]

_where \(\tau_{i}\in(x_{i},x_{i+1}]\) for any \(i\in\{1,\ldots,i_{0}-2\}\), \(\tau_{i_{0}-1}\in(x_{i_{0}-1},x_{i_{0}})\) and \(\tau_{i}\in[x_{i},x_{i+1})\) for any \(i\in\{i_{0},\ldots,n-1\}\)._

Lemma 1 already provides a first guarantee on the sparsity of any minimiser of Equation (4). It indeed includes at most \(n-1\) kinks. In contrast, minimal norm interpolators with an infinite numof kinks exist when the bias terms are not regularised (Debarre et al., 2022). An even stronger sparse recovery result is given in Section 5. Lemma 1 can be seen as a particular case of Theorem 1 of Wang et al. (2021). In the multivariate case and without a free skip connection, the latter states that the minimal norm interpolator has at most one kink (i.e. neuron) per _activation cone_ of the weights4 and has no more than \(n+1\) kinks in total. The idea of our proof is that several kinks among a single activation cone could be merged into a single kink in the same cone. The resulting function then still interpolates, but has a smaller representational cost.

Footnote 4: See Equation (21) in the Appendix for a mathematical definition.

Lemma 1 allows to only consider \(2\) parameters for each interval \((x_{i},x_{i+1})\) (potentially closed at one end). Actually, the degree of freedom is only \(1\) on such intervals: choosing \(a_{i}\) fixes \(\tau_{i}\) (or inversely) because of the interpolation constraint. Lemma 2 below uses this idea to recast the minimisation Problem (4) as a dynamic program with unidimensional state variables \(s_{i}\in\mathbb{R}\) for any \(i\in[n]\).

**Lemma 2**.: _If \(x_{1}<0\) and \(x_{n}\geq 0\), then we have for \(i_{0}=\min\{i\in[n]|x_{i}\geq 0\}\) the following equivalence of optimisation problems_

\[\min_{\forall i\in[n],f(x_{i})=y_{i}}\left\|\sqrt{1+x^{2}f^{\prime\prime}} \right\|_{\mathrm{TV}}=\min_{(s_{i_{0}-1},s_{i_{0}})\in\Lambda}g_{i_{0}}(s_{i_ {0}},s_{i_{0}-1})+c_{i_{0}-1}(s_{i_{0}-1})+c_{i_{0}}(s_{i_{0}}) \tag{5}\]

_where the set \(\Lambda\) and the functions \(g_{i}\) and \(c_{i}\) are defined in Equations (6) to (8) below._

Let us describe the dynamic program defining the functions \(c_{i}\), which characterises the minimal norm interpolator thanks to Lemma 2. First define for any \(i\in[n-1]\), the slope \(\delta_{i}\coloneqq\frac{y_{i+1}-y_{i}}{x_{i+1}-x_{i}}\); the function

\[g_{i+1}(s_{i+1},s_{i})\coloneqq\sqrt{(x_{i+1}(s_{i+1}-\delta_{i})-x_{i}(s_{i}- \delta_{i}))^{2}+\left(s_{i+1}-s_{i}\right)^{2}}\text{ for any }(s_{i+1},s_{i}) \in\mathbb{R}^{2}; \tag{6}\]

\[\text{ and the intervals }\quad S_{i}(s)\coloneqq\begin{cases}(-\infty,\delta_{i} ]\text{ if }s>\delta_{i}\\ \{\delta_{i}\}\text{ if }s=\delta_{i}\\ [\delta_{i},+\infty)\text{ if }s<\delta_{i}\end{cases}\qquad\qquad\text{ for any }s\in\mathbb{R}.\]

The set \(\Lambda\) is then the union of three product spaces given by

\[\Lambda\coloneqq(-\infty,\delta_{i_{0}-1})\times(\delta_{i_{0}-1},+\infty) \cup\{(\delta_{i_{0}-1},\delta_{i_{0}-1})\}\cup(\delta_{i_{0}-1},+\infty) \times(-\infty,\delta_{i_{0}-1}). \tag{7}\]

Finally, we define the functions \(c_{i}:\mathbb{R}\to\mathbb{R}_{+}\) recursively as \(c_{1}=c_{n}\equiv 0\) and

\[c_{i+1}:s_{i+1}\mapsto\min_{s_{i}\in S_{i}(s_{i+1})}g_{i+1}(s_{i+ 1},s_{i})+c_{i}(s_{i})\quad\text{ for any }i\in\{1,\ldots,i_{0}-2\} \tag{8}\] \[c_{i}:s_{i}\mapsto\min_{s_{i+1}\in S_{i}(s_{i})}g_{i+1}(s_{i+1},s _{i})+c_{i+1}(s_{i+1})\quad\text{ for any }i\in\{i_{0},\ldots,n-1\}.\]

Equation (8) defines a dynamic program with a continuous state space. Intuitively for \(i\geq i_{0}\), the variable \(s_{i}\) accounts for the left derivative at the point \(x_{i}\). The term \(g_{i+1}(s_{i+1},s_{i})\) is the minimal cost (in neuron norm) for reaching the point \((x_{i+1},y_{i+1})\) with a slope \(s_{i+1}\), knowing that the left slope is \(s_{i}\) at the point \((x_{i},y_{i})\). Similarly, the interval \(S_{i}(s_{i})\) gives the reachable slopes5 at \(x_{i+1}\), knowing the slope in \(x_{i}\) is \(s_{i}\). Finally, \(c_{i}(s_{i})\) holds for the minimal cost of fitting all the points \((x_{i+1},y_{i+1}),\ldots,(x_{n},y_{n})\) when the left derivative in \((x_{i},y_{i})\) is given by \(s_{i}\). It is defined recursively by minimising the sum of the cost for reaching the next point \((x_{i+1},y_{i+1})\) with a slope \(s_{i+1}\), given by \(g_{i+1}(s_{i+1},s_{i})\); and the cost of fitting all the points after \(x_{i+1}\), given by \(c_{i+1}\). This recursive definition is illustrated in Figure 1 below. A symmetric definition holds for \(i<i_{0}\).

Footnote 5: Here, a single kink is used in the interval \([x_{i},x_{i+1}]\), thanks to Lemma 3.

The idea to derive Equation (6) is to first use Lemma 1 to get a finite representation of a minimal function \(f^{*}\). From there, the minimal cost for connecting the point \((x_{i+1},y_{i+1})\) with \((x_{i},y_{i})\) is done by using a single kink in between. The restrictions given by \(s_{i}\) and \(s_{i+1}\) then yield a unique possible kink. Minimizing its neuron norm then yields Equation (6)

**Remark 1**.: _Equation (5) actually considers the junction of two dynamic programs: a first one corresponding to the points with negative \(x\) values and a second one for positive values. This separation around \(x=0\) is not needed for Lemma 2, but allows for a cleaner analysis in Section 5. Lemmas 1 and 2 also hold for any arbitrary choice of \(i_{0}\). In particular for \(i_{0}=1\), Equation (5) would not consider the junction of two dynamic programs anymore, but a single one._Lemma 2 formulates the minimisation of the representational cost among the interpolating functions as a simpler dynamic program on the sequence of slopes at each \(x_{i}\). This equivalence is the key technical result of this work, from which Section 5 defers many properties on the minimiser(s) of Equation (4).

## 5 Properties of minimal norm interpolator

Thanks to the dynamic program formulation given by Lemma 2, this section derives key properties on the interpolating functions of minimal representational cost. In particular, it shows that Equation (4) always admits a unique minimum. Moreover, under some condition on the training set, this minimising function has the smallest number of kinks among the set of interpolators.

**Theorem 2**.: _The following optimisation problem admits a unique minimiser:_

\[\inf_{f}\left\|\sqrt{1+x^{2}}f^{\prime\prime}\right\|_{\mathrm{TV}}\quad\text { such that }\forall i\in[n],\ f(x_{i})=y_{i}.\]

The proof of Theorem 2 uses the correspondence between interpolating functions and sequences of slopes \((s_{i})_{i\in[n]}\in\mathcal{S}\), where the set \(\mathcal{S}\) is defined by Equation (23) in Appendix D.2. In particular, we show that the following problem admits a unique minimiser:

\[\min_{\mathbf{s}\in\mathcal{S}}\sum_{i=1}^{n-1}g_{i+1}(s_{i+1},s_{i}). \tag{9}\]

We note in the following \(\mathbf{s}^{*}\in\mathcal{S}\) the unique minimiser of the problem in Equation (9). From this sequence of slopes \(\mathbf{s}^{*}\), the unique minimising function of Equation (4) can be recovered. Moreover, \(\mathbf{s}^{*}\) minimises the dynamic program given by the functions \(c_{i}\) as follows:

\[c_{i+1}(s^{*}_{i+1})=g_{i+1}(s^{*}_{i+1},s^{*}_{i})+c_{i}(s^{*}_ {i})\quad\text{for any }i\in[i_{0}-2]\] \[c_{i}(s^{*}_{i})=g_{i+1}(s^{*}_{i+1},s^{*}_{i})+c_{i+1}(s^{*}_{i+ 1})\quad\text{for any }i\in\{i_{0},\ldots,n-1\}.\]

Using simple properties of the functions \(c_{i}\) given by Lemma 7 in Appendix E, properties on \(\mathbf{s}^{*}\) can be derived besides the uniqueness of the minimal norm interpolator. Lemma 3 below gives a first intuitive property of this minimiser, which proves helpful in showing the main result of the section.

**Lemma 3**.: _For any \(i\in[n]\), \(s^{*}_{i}\in[\min(\delta_{i-1},\delta_{i}),\max(\delta_{i-1},\delta_{i})]\), where \(\delta_{0}\coloneqq\delta_{1}\) and \(\delta_{n}\coloneqq\delta_{n-1}\) by convention._

A geometric interpretation of Lemma 3 is that the optimal (left or right) slope in \(x_{i}\) is between the line joining \((x_{i-1},y_{i-1})\) with \((x_{i},y_{i})\) and the line joining \((x_{i},y_{i})\) with \((x_{i+1},y_{i+1})\).

### Recovering a sparsest interpolator

We now aim at characterising when the minimiser of Equation (4) is among the set of sparsest interpolators, in terms of number of kinks. Before describing the minimal number of kinks required

Figure 1: Recursive definition of the dynamic program for \(i\geq i_{0}\).

to fit the data in Lemma 4, we partition \([x_{1},x_{n})\) into intervals of the form \([x_{n_{k}},x_{n_{k+1}})\) where

\[n_{0}=1\text{ and for any }k\geq 0\text{ such that }n_{k}<n, \tag{10}\]

and \(\operatorname{sign}(0)\coloneqq 0\) by convention. If we note \(f_{\operatorname{lin}}\) the canonical piecewise linear interpolator, it is either convex, concave or affine on every interval \([x_{n_{k}-1},x_{n_{k+1}}]\). This partitioning thus splits the space into convex, concave and affine parts of \(f_{\operatorname{lin}}\), as illustrated by Figure 2 on a toy example. This partition is crucial in describing the sparsest interpolators, thanks to Lemma 4.

**Lemma 4**.: _If we denote by \(\|f^{\prime\prime}\|_{0}\) the cardinality of the support of the measure \(f^{\prime\prime}\),_

\[\min_{\begin{subarray}{c}f\\ \forall i,f(x_{i})=y_{i}\end{subarray}}\|f^{\prime\prime}\|_{0}=\sum_{k\geq 1 }\left\lceil\frac{n_{k+1}-n_{k}}{2}\right\rceil\mathds{1}_{\delta_{n_{k-1}} \neq\delta_{n_{k}}}.\]

Lemma 4's proof idea is that for any interval \([x_{k-1},x_{k+1})\) where \(f_{\operatorname{lin}}\) is convex (resp. concave) non affine, any function requires at least one positive (resp. negative) kink to fit the three data points in this interval. The result then comes from counting the number of such disjoint intervals and showing that a specific interpolator exactly reaches this number.

The minimal number of kinks required to interpolate the data is given by Lemma 4. Before giving the main result of this section, we introduce the following assumption on the data \((x_{k},y_{k})_{k\in[n]}\).

**Assumption 1**.: _For the sequence \((n_{k})_{k}\) defined in Equation (10):_

\[n_{k+1}-n_{k}\leq 3\text{ or }\delta_{n_{k}}=\delta_{n_{k}-1}\quad\text{for any }k\geq 0.\]

Assumption 1 exactly means there are no \(6\) (or more) consecutive points \(x_{k},\dots,x_{k+5}\) such that \(f_{\operatorname{lin}}\) is convex (without \(3\) aligned points) or concave on \([x_{k},x_{k+5}]\). This assumption depends a lot on the structure of the true model function (if there is any). For example, it holds if the truth is given by a piecewise linear function, while it may not if the truth is given by a quadratic function. Theorem 3 below shows that under Assumption 1, the minimal cost interpolator is amongst the sparsest interpolators, in number of its kinks.

**Theorem 3**.: _If Assumption 1 holds, then_

\[\operatorname*{argmin}_{\begin{subarray}{c}f\\ \forall i,f(x_{i})=y_{i}\end{subarray}}\|\sqrt{1+x^{2}}f^{\prime\prime}\|_{ \operatorname{TV}}\in\operatorname*{argmin}_{\begin{subarray}{c}f\\ \forall i,f(x_{i})=y_{i}\end{subarray}}\|f^{\prime\prime}\|_{0}. \tag{11}\]

Theorem 3 states conditions under which the interpolating function \(f\) with the smallest representational cost \(\overline{R}_{1}(f)\) also has the minimal number of kinks, i.e. ReLU hidden neurons, among the set of interpolators. It illustrates how norm regularisation, and in particular adding the biases' norm to the weight decay, favors estimators with a small number of neurons. While training neural networks with norm regularisation, the final estimator can actually have many non-zero neurons, but they all align towards a few key directions. As a consequence, the obtained estimator is actually equivalent to a small width network, meaning they have the same output for every input \(x\in\mathbb{R}\).

Recall that such a sparsity does not hold when the bias terms are not regularised. More precisely, some sparsest interpolators have a minimal representational cost in that case, but there are also minimal cost interpolators with an arbitrarily large (even infinite) number of kinks (Debarre et al., 2022). There is thus no particular reason that the obtained estimator is sparse when minimising

Figure 2: Partition given by \((n_{k})_{k}\) on a toy example.

the representational cost without penalising the bias terms. Section 6 empirically illustrates this difference of sparsity in the recovered estimators, depending on whether or not the bias parameters are penalised in the norm regularisation.

The generalisation benefit of this sparsity remains unclear. Indeed, generalisation bounds in the literature often rely on the parameters' norm rather than the network width (i.e., sparsity level). The relation between sparsest and min norm interpolators is important to understand in the particular context of implicit regularisation. In particular, while Boursier et al. (2022) conjectured that the implicit bias for regression problem was towards min norm interpolators, Chistikov et al. (2023) recently proved that the implicit bias could sometimes instead lead to sparsest interpolators. Our result suggests that both min norm and sparsest interpolators often coincide, which could explain the prior belief of convergence towards min norm interpolators. Yet, Theorem 3 and Chistikov et al. (2023) instead suggest that, at least in some situations, implicit bias favors sparsest interpolators, yielding different estimators6.

Footnote 6: Note that this nuance only holds for regression tasks. Instead, it is known that implicit regularisation favors min norm interpolators in classification tasks (Chizat and Bach, 2020), which always coincide with sparsest interpolators in that case (see Corollary 1 in Section 5.2) for univariate data.

**Remark 3**.: _Theorem 3 states that sparse recovery, given by Equation (11), occurs if Assumption 1 holds. When \(n_{k+1}-n_{k}\geq 4\), i.e. there are convex regions of \(f_{\mathrm{lin}}\) with at least \(6\) points, Appendix A gives a counterexample where Equation (11) does not hold. However, Equation (11) can still hold under weaker data assumptions than Assumption 1. In particular, Appendix A gives a necessary and sufficient condition for sparse recovery when there are convex regions with exactly \(6\) points. When we allow for convex regions with at least \(7\) points, it however becomes much harder to derive conditions where sparse recovery still occurs._

**Remark 4**.: _The counterexample presented in Appendix A reveals an unexpected outcome: minimal representational cost interpolators may not necessarily belong to the sparsest interpolators. This finding is closely related to the idea that it may not be generally feasible to characterize the implicit regularisation of gradient descent as minimising parameters norm (Vardi and Shamir, 2021; Chistikov et al., 2023). In particular, Vardi and Shamir (2021), Chistikov et al. (2023) rely on examples where minimal norm interpolators are not the sparsest ones; and the implicit regularisation instead favors the latter. We believe that this inherent limitation is one of the underlying reason for the different implicit regularization effects observed in other settings such as matrix factorization (Gunasekar et al., 2017; Razin and Cohen, 2020; Li et al., 2020a)._

### Application to classification

In the binary classification setting, max-margin classifiers, defined as the minimiser of the problem

\[\min_{f}\overline{R}(f)\quad\text{such that}\;\forall i\in[n],y_{i}f(x_{i})\geq 1, \tag{12}\]

are known to be the estimators of interest. Indeed, gradient descent on the cross entropy loss \(l(\hat{y},y)=\log(1+e^{-\hat{y}y})\) converges in direction to such estimators (Lyu and Li, 2019; Chizat and Bach, 2020). Theorem 3 can be used to characterise max- margin classifiers, leading to Corollary 1.

**Corollary 1**.: \[\operatorname*{argmin}_{\begin{subarray}{c}f\\ \forall i\in[n],y_{i}f(x_{i})\geq 1\end{subarray}}\overline{R}_{1}(f) \subset\operatorname*{argmin}_{\begin{subarray}{c}f\\ \forall i\in[n],y_{i}f(x_{i})\geq 1\end{subarray}}\|f^{\prime\prime}\|_{0}.\]

Theorem 3 yields that the max-margin classifier is among the sparsest margin classifiers, when a free skip connection is allowed. We believe that the left minimisation problem admits a unique solution. However, uniqueness cannot be directly derived from Theorem 3, but would instead require another thorough analysis, using an adapted dynamic programming reformulation. Since the uniqueness property is of minor interest, we here prefer to focus on a direct corollary of Theorem 3. We emphasise that no data assumptions are required for classification tasks, apart from being univariate.

## 6 Experiments

This section compares, through Figure 3, the estimators that are obtained with and without counting the bias terms in the regularisation, when training a one-hidden ReLU layer neural network. The code is made available at github.com/eboursier/penalising_biases.

For this experiment, we train neural networks by minimising the empirical loss, regularised with the \(\ell_{2}\) norm of the parameters (either with or without the bias terms) with a regularisation factor \(\lambda=10^{-3}\). Each neural network has \(m=200\) hidden neurons and all parameters are initialised i.i.d. as centered Gaussian variables of variance \(\nicefrac{{1}}{{\sqrt{m}}}\) (similar results are observed for larger initialisation scales).7 There is no free skip connection here, which illustrates its benignity: the results that are expected by the above theory also happen without free skip connection. Experiments with a free skip connection are given in Appendix B and yield similar observations.

Footnote 7: For small initialisations, both methods yield sparse estimators, since implicit regularisation of the bias terms is significant in that case. Our goal is only to illustrate the differences in the minimisers of the two problems (with and without bias penalisation), without any optimisation consideration.

As predicted by our theoretical study, penalising the bias terms in the \(\ell_{2}\) regularisation enforces the sparsity of the final estimator. The estimator of Figure 2(a) indeed counts \(2\) kinks (the smallest number required to fit the data), while in Figure 2(b), the directions of the neurons are scattered. More precisely, the estimator is almost _smooth_ near \(x=-0.5\), while the sparse estimator of Figure 2(a) is clearly not differentiable at this point. Also, the estimator of Figure 2(b) includes a clear additional kink at \(x=0\). Figure 3 thus illustrates that counting the bias terms in regularisation can lead to sparser estimators.

## 7 Conclusion

This work studies the importance of parameters' norm for one hidden ReLU layer neural networks in the univariate case. In particular, the parameters' norm required to represent a function is given by \(\left\lVert\sqrt{1+x^{2}}f^{\prime\prime}\right\rVert_{\mathrm{TV}}\) when allowing for a free skip connection. In comparison to weight decay, which omits the bias parameters in the norm, an additional \(\sqrt{1+x^{2}}\) weighting term appears in the representational cost. This weighting is of crucial importance since it implies uniqueness of the minimal norm interpolator. Moreover, it favors sparsity of this interpolator in number of kinks. Minimising the parameters' norm (with the biases), which can be either obtained by explicit or implicit regularisation when training neural networks, thus leads to sparse interpolators. We believe this sparsity is a reason for the good generalisation properties of neural networks observed in practice.

Although these results provide some understanding of minimal norm interpolators, extending them to more general and difficult settings remains open. Even if the representational cost might be described in the multivariate case [as done by Ongie et al., 2019, without bias penalisation], characterising minimal norm interpolators seems very challenging in that case. Characterising minimal norm interpolators, with no free skip connection, also presents a major challenge for future work.

Figure 3: Final estimator when training one-hidden layer network with \(\ell_{2}\) regularisation. The green dots correspond to the data and the green line is the estimated function. Each blue star represents a hidden neuron \((w_{j},b_{j})\) of the network: its \(x\)-axis value is given by \(-b_{j}/w_{j}\), which coincides with the position of the kink of its associated ReLU; its \(y\)-axis value is given by the output weight \(a_{j}\).

## Acknowledgments and Disclosure of Funding

The authors thank Gal Vardi, for suggesting the proof of Corollary 1, through a direct use of Theorem 3. The authors also thank Claire Boyer, Julien Fageot and Loucas Pillaud-Vivien for very helpful discussions and feedback.

## References

* Bach (2017) Francis Bach. Breaking the curse of dimensionality with convex neural networks. _The Journal of Machine Learning Research_, 18(1):629-681, 2017.
* Barron (1993) Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. _IEEE Transactions on Information theory_, 39(3):930-945, 1993.
* Bartlett (1996) Peter Bartlett. For valid generalization the size of the weights is more important than the size of the network. _Advances in neural information processing systems_, 9, 1996.
* Belkin et al. (2019) Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias-variance trade-off. _Proceedings of the National Academy of Sciences_, 116(32):15849-15854, 2019.
* Bengio et al. (2005) Yoshua Bengio, Nicolas Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex neural networks. _Advances in neural information processing systems_, 18, 2005.
* Boursier et al. (2022) Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion. Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs. _arXiv preprint arXiv:2206.00939_, 2022.
* Candes and Fernandez-Granda (2014) Emmanuel J Candes and Carlos Fernandez-Granda. Towards a mathematical theory of super-resolution. _Communications on pure and applied Mathematics_, 67(6):906-956, 2014.
* Chen et al. (2001) Scott Shaobing Chen, David L Donoho, and Michael A Saunders. Atomic decomposition by basis pursuit. _SIAM review_, 43(1):129-159, 2001.
* Chistikov et al. (2023) Dmitry Chistikov, Matthias Englert, and Ranko Lazic. Learning a neuron by a shallow relu network: Dynamics and implicit bias for correlated inputs. _arXiv preprint arXiv:2306.06479_, 2023.
* Chizat and Bach (2020) Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In _Conference on Learning Theory_, pages 1305-1338. PMLR, 2020.
* Debarre et al. (2022) Thomas Debarre, Quentin Denoyelle, Michael Unser, and Julien Fageot. Sparsest piecewise-linear regression of one-dimensional data. _Journal of Computational and Applied Mathematics_, 406:114044, 2022.
* E and Wojtowytsch (2021) Weinan E and Stephan Wojtowytsch. Representation formulas and pointwise properties for barron functions, 2021.
* Fernandez-Granda (2016) Carlos Fernandez-Granda. Super-resolution of point sources via convex programming. _Information and Inference: A Journal of the IMA_, 5(3):251-303, 2016.
* Goodfellow et al. (2016) Ian Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_. MIT Press, 2016. [http://www.deeplearningbook.org](http://www.deeplearningbook.org).
* Gunasekar et al. (2017) Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. _Advances in Neural Information Processing Systems_, 30, 2017.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Ji and Telgarsky (2019) Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In _Conference on Learning Theory_, pages 1772-1798. PMLR, 2019.
* Li et al. (2018)Anders Krogh and John Hertz. A simple weight decay can improve generalization. _Advances in neural information processing systems_, 4, 1991.
* Kurkova and Sanguineti (2001) Vera Kurkova and Marcello Sanguineti. Bounds on rates of variable-basis and neural-network approximation. _IEEE Transactions on Information Theory_, 47(6):2659-2665, 2001.
* Li et al. (2020) Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning. _arXiv preprint arXiv:2012.09839_, 2020a.
* Li et al. (2020) Zhong Li, Chao Ma, and Lei Wu. Complexity measures for neural networks with general activation functions using path-based norms. _arXiv preprint arXiv:2009.06132_, 2020b.
* Lyu and Li (2019) Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In _International Conference on Learning Representations_, 2019.
* Nacson et al. (2022) Mor Shpigel Nacson, Kavya Ravichandran, Nathan Srebro, and Daniel Soudry. Implicit bias of the step size in linear diagonal neural networks. In _International Conference on Machine Learning_, pages 16270-16295. PMLR, 2022.
* Neyshabur et al. (2014) Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. _arXiv preprint arXiv:1412.6614_, 2014.
* Ng (2011) Andrew Ng. Sparse autoencoder. _CS294A Lecture notes_, 72(2011):1-19, 2011.
* Ongie et al. (2019) Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view of bounded norm infinite width relu nets: The multivariate case. In _International Conference on Learning Representations (ICLR 2020)_, 2019.
* Parhi and Nowak (2021) Rahul Parhi and Robert D Nowak. Banach space representer theorems for neural networks and ridge splines. _Journal of Machine Learning Research_, 22(43):1-40, 2021.
* Parhi and Nowak (2023) Rahul Parhi and Robert D Nowak. Deep learning meets sparse regularization: A signal processing perspective. _arXiv preprint arXiv:2301.09554_, 2023.
* Poon et al. (2019) Clarice Poon, Nicolas Keriven, and Gabriel Peyre. Support localization and the fisher metric for off-the-grid sparse regularization. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1341-1350. PMLR, 2019.
* Razin and Cohen (2020) Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. _Advances in neural information processing systems_, 33:21174-21187, 2020.
* Safran et al. (2022) Itay Safran, Gal Vardi, and Jason D Lee. On the effective number of linear regions in shallow univariate relu networks: Convergence guarantees and implicit bias. _Advances in Neural Information Processing Systems_, 35:32667-32679, 2022.
* Sanford et al. (2022) Clayton Sanford, Navid Ardeshir, and Daniel Hsu. Intrinsic dimensionality and generalization properties of the \(\mathcal{R}\)-norm inductive bias. _arXiv preprint arXiv:2206.05317_, 2022.
* Savarese et al. (2019) Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded norm networks look in function space? In _Conference on Learning Theory_, pages 2667-2690. PMLR, 2019.
* Shevchenko et al. (2022) Alexander Shevchenko, Vyacheslav Kungurtsev, and Marco Mondelli. Mean-field analysis of piecewise linear solutions for wide relu networks. _The Journal of Machine Learning Research_, 23(1):5660-5714, 2022.
* Soudry et al. (2018) Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. _The Journal of Machine Learning Research_, 19(1):2822-2878, 2018.
* Stewart et al. (2022) Lawrence Stewart, Francis Bach, Quentin Berthet, and Jean-Philippe Vert. Regression as classification: Influence of task formulation on neural network features. _arXiv preprint arXiv:2211.05641_, 2022.
* Vardi and Shamir (2021) Gal Vardi and Ohad Shamir. Implicit regularization in relu networks with the square loss. In _Conference on Learning Theory_, pages 4224-4258. PMLR, 2021.
* Vardi and Srebro (2020)Yifei Wang, Jonathan Lacotte, and Mert Pilanci. The hidden convex optimization landscape of regularized two-layer relu networks: an exact characterization of optimal solutions. In _International Conference on Learning Representations_, 2021.
* Zhang et al. (2021) Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.

Discussing Assumption 1

Theorem 3 requires Assumption 1, which assumes that there are no convex (or concave) regions of \(f_{\mathrm{lin}}\) with at least \(6\) data points. Actually, when there is a convex (or concave) region with exactly \(6\) data points, i.e. \(n_{k+1}=n_{k}+4\), Theorem 3 holds (for this region) if and only if for \(i=n_{k}+1\):

\[\frac{\langle u_{i},w_{i-1}\rangle\langle u_{i+1},w_{i+1}\rangle}{ \|w_{i-1}\|\ \|w_{i+1}\|}-\langle u_{i},u_{i+1}\rangle\leq\sqrt{\|u_{i}\|^{2} -\frac{\langle u_{i},w_{i-1}\rangle^{2}}{\|w_{i-1}\|^{2}}}\sqrt{\|u_{i+1}\|^{2 }-\frac{\langle u_{i+1},w_{i+1}\rangle^{2}}{\|w_{i+1}\|^{2}}} \tag{13}\] \[\text{where }\quad u_{i}=(x_{i},1);\quad w_{i-1}=\frac{\delta_{i}- \delta_{i-1}}{\delta_{i}-\delta_{i-2}}(x_{i},1)+\frac{\delta_{i-1}-\delta_{i-2 }}{\delta_{i}-\delta_{i-2}}(x_{i-1},1);\] \[\text{and }\quad u_{i+1}=(x_{i+1},1);\quad w_{i+1}=\frac{\delta_{i+2 }-\delta_{i+1}}{\delta_{i+2}-\delta_{i}}(x_{i+2},1)+\frac{\delta_{i+1}-\delta_ {i}}{\delta_{i+2}-\delta_{i}}(x_{i+1},1).\]

The proof of this result (omitted here) shows that the problem

\[\min_{(s_{i},s_{i+1})\in[\delta_{i-1},\delta_{i}]\times[\delta_{i},\delta_{i+1 }]}g_{i}(s_{i},s_{i-1}^{*})+g_{i+1}(s_{i+1,s_{i}})+g_{i+2}(s_{i+2}^{*},s_{i+1})\]

is minimised for \((s_{i},s_{i+1})=(\delta_{i},\delta_{i})\) if and only if Equation (13) holds, which corresponds to the (unique) sparsest way to interpolate the data on this convex region. To show that the minimum is reached at that point, we can first notice that \(s_{i-1}^{*}=\delta_{i-2}\) and \(s_{i+2}^{*}=\delta_{i+2}\). Then, it requires a meticulous study of the directional derivatives of the (convex but non-differentiable) objective function at the point \((\delta_{i},\delta_{i})\).

Figure 4 below illustrates a case of \(6\) data points, where the condition of Equation (13) does not hold. Clearly, the minimal norm interpolator differs from the (unique) sparsest interpolator in that case.

When considering more than \(6\) points, studying the minimisation problem becomes cumbersome and no simple condition of sparse recovery can be derived. When generating random data with large convex regions, e.g. \(35\) points, the minimal norm interpolator is rarely among the sparsest interpolators. Moreover, it seems that its number of kinks could be arbitrarily close to \(34\), which is the trivial upper bound of the number of kinks given by Lemma 3; while the sparsest interpolators only have \(17\) kinks.

## Appendix B Additional experiments

Figure 5 shows the minimiser of Equation (4) on the toy example of Figure 2. The minimising function is computed thanks to the dynamic program given by Lemma 2. Although the variables of this dynamic program are continuous, we can efficiently solve it by approximating the constraint space of each slope \(s_{i}\) as a discrete grid of \([\delta_{i-1},\delta_{i}]\) thanks to Lemma 3. For the data used in Figure 5, Assumption 1 holds. It is clear that the minimiser is very sparse, counting only \(4\) kinks. The partition given by Figure 2 then shows that this is indeed the smaller possible number of kinks, thanks to Lemma 4. On the other hand, the canonical piecewise linear interpolator \(f_{\mathrm{lin}}\) is is not as sparse and counts \(7\) kinks here.

Figure 4: Case of difference between minimal norm interpolator and sparsest interpolator.

Similar observations can be made: the obtained estimator when counting the bias terms in regularisation only has \(2\) kinks, while the estimator obtained by omitting the biases in the regularisation is much smoother (and thus much less sparse in the number of kinks). The only difference is that the latter estimator here does not have a clear kink at \(x=0\), but is instead even smoother on the interval \([-0.5,0]\). This is explained by the presence of more scattered kinks in this interval. Despite this slight difference, the main observation remains unchanged: the estimator is a sparsest one when counting the bias terms, while it counts a lot of kinks (and is even smooth) when omitting the biases.

## Appendix C Proofs of Section 3

Theorem 4 below extends the characterisation of the representational cost \(\overline{R}_{1}(f)\) of Theorem 1.

**Theorem 4**.: _For any Lipschitz function \(f:\mathbb{R}\to\mathbb{R}\),_

\[\overline{R}_{1}(f)=\left\|\sqrt{1+x^{2}}f^{\prime\prime}\right\| _{\mathrm{TV}}=\int_{\mathbb{R}}\sqrt{1+x^{2}}\;\mathrm{d}|f^{\prime\prime}|(x)\] \[\text{and}\quad\overline{R}(f)=\left\|\sqrt{1+x^{2}}f^{\prime \prime}\right\|_{\mathrm{TV}}+D(x_{f},\mathcal{C}_{f}),\]

Figure 5: Minimiser of Equation (4) on a toy data example.

Figure 6: Final estimators when training one-hidden ReLU neural networks with \(\ell_{2}\) regularisation. The green dots correspond to the data, while the green line is the estimated function. Each blue star represents a hidden neuron \((w_{j},b_{j})\) of the network: its \(x\)-axis value is given by \(-b_{j}/w_{j}\), which coincides with the position of the kink of its associated ReLU; its \(y\)-axis value is given by the output layer weight \(a_{j}\).

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_EMPTY:17]

It now remains to prove that \(\min_{u\in\mathcal{C}}\|(a,b)-u\|^{2}=\min_{(\varphi,\mu_{2})\in\Gamma}\int_{-\frac{ \pi}{2}}^{\frac{\pi}{2}}\mathrm{d}|\mu_{2}|\). Fix in the following \(\varphi\) such that \(\|\varphi\|_{\infty}\leq 1\) and note

\[\begin{cases}x=a-\int_{-\pi/2}^{\pi/2}\cos\theta\,\varphi(\theta)\mathrm{d}\nu_ {+}(\theta),\\ y=b-\int_{-\pi/2}^{\pi/2}\sin\theta\,\varphi(\theta)\mathrm{d}\nu_{+}(\theta). \end{cases}\]

It now suffices to show that for any fixed \(\varphi\):

\[\min_{\mu_{2}\text{ s.t. }(\varphi,\mu_{2})\in\Gamma}\int_{-\frac{\pi}{2}}^{ \frac{\pi}{2}}\mathrm{d}|\mu_{2}|=\|(x,y)\|\,.\]

The constraint set is actually \(\left\{\mu_{2}\mid\int_{-\pi/2}^{\pi/2}(\cos\theta,\sin\theta)\,\mathrm{d}\mu_ {2}(\theta)=(x,y)\right\}\). Now define

\[\theta^{*}=\arcsin\left(\frac{\mathrm{sign}(x)y}{\sqrt{x^{2}+y^{2}}}\right) \quad\text{and}\quad\mu_{2}^{*}=\mathrm{sign}(x)\sqrt{x^{2}+y^{2}}\delta_{ \theta^{*}},\]

where \(\delta_{\theta^{*}}\) is the Dirac distribution located at \(\theta^{*}\). This definition is only valid if \(x\neq 0\), otherwise we choose \(\mu_{2}^{*}=-y\delta_{-\frac{\pi}{2}}\).

Note that \(\mu_{2}^{*}\) is in the constraint set and \(\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\mathrm{d}|\mu_{2}|=\|(x,y)\|\), i.e.

\[\min_{\mu_{2}\text{ s.t. }(\varphi,\mu_{2})\in\Gamma}\int_{-\frac{\pi}{2}}^{ \frac{\pi}{2}}\mathrm{d}|\mu_{2}|\leq\|(x,y)\|\,.\]

Now consider any \(\mu_{2}\) in the constraint set and decompose \(\mu_{2}=\mu_{2}^{+}-\mu_{2}^{-}\) with \((\mu_{2}^{+},\mu_{2}^{-})\in\mathcal{M}_{+}([-\frac{\pi}{2},\frac{\pi}{2}))^{2}\). Define

\[(x_{+},y_{+}) =\left(\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos\theta\,\mathrm{d }\mu_{2}^{+},\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\sin\theta\,\mathrm{d}\mu_{2 }^{+}\right)\] \[(x_{-},y_{-}) =\left(\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos\theta\,\mathrm{ d}\mu_{2}^{-},\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\sin\theta\,\mathrm{d}\mu_{2 }^{-}\right)\]

By Cauchy-Schwarz inequality,

\[\int\cos^{2}(\theta)\,\mathrm{d}\mu_{2}^{+}(\theta)\int\mathrm{d}\mu_{2}^{+}( \theta)\geq x_{+}^{2},\]

\[\int\sin^{2}(\theta)\,\mathrm{d}\mu_{2}^{+}(\theta)\int\mathrm{d}\mu_{2}^{+}( \theta)\geq y_{+}^{2}.\]

Summing these two inequalities yields

\[\int\mathrm{d}\mu_{2}^{+}\geq\sqrt{x_{+}^{2}+y_{+}^{2}}.\]

Similarly, we have

\[\int\mathrm{d}\mu_{2}^{-}\geq\sqrt{x_{-}^{2}+y_{-}^{2}}.\]

Recall that \(\int\mathrm{d}|\mu_{2}|=\int\mathrm{d}\mu_{2}^{+}+\int\mathrm{d}\mu_{2}^{-}\). By triangle inequality, this yields:

\[\int\mathrm{d}|\mu_{2}| \geq\|(x_{+},y_{+})\|+\|(x_{-},y_{-})\|\] \[\geq\|(x_{+},y_{+})-(x_{-},y_{-})\|=\|(x,y)\|.\]

As a consequence:

\[\min_{\mu_{2}\text{ s.t. }(\varphi,\mu_{2})\in\Gamma}\int_{-\frac{\pi}{2}}^{ \frac{\pi}{2}}\mathrm{d}|\mu_{2}|\geq\|(x,y)\|Proof of Section 4

### Proof of Lemma 1

We first need to show the existence of a minimum. Using the definition of \(\overline{R}_{1}(f)\) and Theorem 1, Equation (4) is equivalent to

\[\inf_{\mu,a,b}\left\|\mu\right\|_{\mathrm{TV}}\quad\text{such that for any }i\in[n],\ f_{\mu,a,b}(x_{i})=y_{i}. \tag{20}\]

Consider a sequence \((\mu_{j},a_{j},b_{j})_{j}\) such that \(f_{\mu_{j},a_{j},b_{j}}(x_{i})=y_{i}\) for any \(i\) and \(j\) and \(\left\|\mu_{j}\right\|_{\mathrm{TV}}\) converges to the infimum of Equation (20). The sequence \(\left\|\mu_{j}\right\|_{\mathrm{TV}}\) is necessarily bounded. This also implies that both \((a_{j})\) and \((b_{j})\) are bounded9. Since the space of finite signed measures on \(\mathbb{S}_{1}\) is a Banach space, there is a subsequence converging weakly towards some \((\mu,a,b)\). By weak convergence, \((\mu,a,b)\) is in the constraints set of Equation (20) and \(\left\|\mu\right\|_{\mathrm{TV}}=\lim_{j}\left\|\mu_{j}\right\|_{\mathrm{TV}} \cdot(\mu,a,b)\) is thus a minimiser of Equation (20). We thus proved the existence of a minimum for Equation (4), which is reached for \(f_{\mu,a,b}\).

Footnote 9: To see that, we can first consider the difference \(f_{\mu_{j},a_{j},b_{j}}(x_{1})-f_{\mu_{j},a_{j},b_{j}}(x_{2})\) to show that \((a_{j})_{j}\) is bounded. This then leads to the boundedness of \((b_{j})_{j}\) when considering \(f_{\mu_{j},a_{j},b_{j}}(x_{1})\).

Define for the sake of the proof the activation cones \(C_{i}\) as

\[C_{0} =\left\{\theta\in\mathbb{R}^{2}\mid\forall i=1,\ldots,n,\langle \theta,(x_{i},1)\rangle\geq 0\right\},\] \[C_{i} =\left\{\theta\in\mathbb{R}^{2}\mid\langle\theta,(x_{i+1},1) \rangle\geq 0>\langle\theta,(x_{i+1})\rangle\right\}\quad\text{for any }i=1,\ldots,i_{0}-2,\] \[C_{i_{0}-1} =\left\{\theta\in\mathbb{R}^{2}\mid\langle\theta,(x_{i_{0}},1) \rangle>0>\langle\theta,(x_{i_{0}-1},1)\rangle\right\}, \tag{21}\] \[C_{i} =\left\{\theta\in\mathbb{R}^{2}\mid\langle\theta,(x_{i+1},1) \rangle>0\geq\langle\theta,(x_{i},1)\rangle\right\}\quad\text{for any }i=i_{0}, \ldots,n-1,\] \[C_{n} =\left\{\theta\in\mathbb{R}^{2}\setminus\{0\}\mid\forall i=1, \ldots,n,\langle\theta,(x_{i},1)\rangle\leq 0\rangle\right\}.\]

As the \(x_{i}\) are ordered, note that \((C_{0},C_{1},-C_{1},\ldots,C_{n-1},-C_{n-1},C_{n})\) forms a partition of \(\mathbb{R}^{2}\). To prove Lemma 1, it remains to show that any minimiser \((\mu_{a},b)\) of Equation (20) has a function \(f_{\mu,a,b}\) of the form

\[f_{\mu,a,b}(x)=\tilde{a}x+\tilde{b}+\sum_{i=1}^{n-1}\tilde{a}_{i}\sigma( \langle\theta_{i},(x,1)\rangle)\quad\text{where }\theta_{i}\in C_{i}.\]

Let \(f\) be a minimiser of Equation (4). Let \(\mu,a,b\) be a minimiser of Equation (20) such that \(f_{\mu,a,b}=f\). Define \(\tilde{\mu},\tilde{a},\tilde{b}\) as

\[\mathrm{d}\tilde{\mu}(\theta)=\begin{cases}\mathrm{d}\mu(\theta)+\mathrm{d} \mu(-\theta)&\text{for }\theta\in C_{i}\text{ for any }i=1,\ldots,n-1\\ 0&\text{for }\theta\in-C_{i}\text{ for any }i=1,\ldots,n-1\\ \mathrm{d}\mu(\theta)&\text{otherwise},\end{cases}\]

\[\tilde{a} =a-\sum_{j=1}^{n-1}\int_{-C_{j}}\theta_{1}\mathrm{d}\mu(\theta),\] \[\tilde{b} =b-\sum_{j=1}^{n-1}\int_{-C_{j}}\theta_{2}\mathrm{d}\mu(\theta).\]

Thanks to the identity \(\sigma(u)-u=\sigma(-u)\), \(f_{\mu,a,b}=f_{\tilde{\mu},\tilde{b}}\). Moreover, \(\left\|\tilde{\mu}\right\|_{\mathrm{TV}}\leq\left\|\mu\right\|_{\mathrm{TV}}\), so we can assume w.l.o.g. that the support of \(\mu\) is included10 in \(\bigcup_{i=0}^{i}C_{i}\). In that case, for any \(i=,1\ldots,n\)

Footnote 10: We here transform the triple \((\mu,a,b)\), but the corresponding function \(f\) remains unchanged.

\[f(x_{i}) =ax_{i}+b+\sum_{j=0}^{i-1}\int_{C_{j}}\langle\theta,(x_{i},1) \rangle\mathrm{d}\mu(\theta)\] \[=ax_{i}+b+\sum_{j=0}^{i-1}\langle\int_{C_{j}}\theta\mathrm{d}\mu( \theta),(x_{i},1)\rangle. \tag{22}\]First, the reduction

\[\tilde{a} =a+\int_{C_{0}}\theta_{1}\mathrm{d}\mu(\theta)\] \[\tilde{b} =b+\int_{C_{0}}\theta_{2}\mathrm{d}\mu(\theta)\] \[\tilde{\mu} =\mu_{\left\|\bigcup_{i=1}^{n-1}C_{i}\right.},\]

does not increase the total variation of \(\mu\) and still interpolates the data. As a consequence, the support of \(\mu\) is included in \(\bigcup_{i=1}^{n-1}C_{i}\). Now let \(\mu=\mu_{+}-\mu_{-}\) be the Jordan decomposition of \(\mu\) and define for any \(i\in[n-1]\)

\[\alpha_{i}=\int_{C_{i}}\theta\mathrm{d}\mu_{+}(\theta)\quad\text{and}\quad \beta_{i}=\int_{C_{i}}\theta\mathrm{d}\mu_{-}(\theta).\]

Note that \(\alpha_{i}\) and \(\beta_{i}\) are both in the positive convex cone \(C_{i}\). For \(\theta_{i}\coloneqq\alpha_{i}-\beta_{i}\), Equation (22) rewrites

\[f(x_{i})=ax_{i}+b+\sum_{j=1}^{i-1}\langle\theta_{i},(x_{i},1)\rangle.\]

If \(\theta_{i}\in\overline{C}_{i}\cup\overline{-C}_{i}\), we can then define

\[\tilde{\mu}=\mu-\mu_{\left|C_{i}\right.}+\left\|\theta_{i}\right\|\delta_{ \frac{\theta_{i}}{\left|\overline{\theta}_{i}\right.}}.\]

Thanks to Equation (22), the function \(f_{\tilde{\mu},a,b}\) still interpolates the data and

\[\left\|\tilde{\mu}\right\|_{\mathrm{TV}}\leq\left\|\mu\right\|_{\mathrm{TV}}- \left\|\mu_{\left|C_{i}\right.}\right\|_{\mathrm{TV}}+\left\|\theta_{i}\right\|.\]

By minimisation of \(\left\|\mu\right\|_{\mathrm{TV}}\), this is an equality. Moreover as \(\mu\) is a measure on the sphere,

\[\left\|\mu_{\left|C_{i}\right.}\right\|_{\mathrm{TV}}=\int_{C_{i}}\|\theta \|\mathrm{d}\mu_{+}(\theta)+\int_{C_{i}}\|\theta\|\mathrm{d}\mu_{-}(\theta)\]

\[\geq\left\|\int_{C_{i}}\theta\mathrm{d}\mu_{+}(\theta)\right\|+\left\|\int_{C _{i}}\theta\mathrm{d}\mu_{-}(\theta)\right\|\]

\[=\left\|\alpha_{i}\right\|+\left\|\beta_{i}\right\|\geq\left\|\theta_{i}\right\|.\]

By minimisation, all inequalities are equalities. Jensen's case of equality implies for the first inequality that both \(\mu_{+\left|C_{i}\right.}\) and \(\mu_{-\left|C_{i}\right.}\) are Diracs, while the second inequality implies that either \(\alpha_{i}=0\) or \(\beta_{i}=0\). Overall, \(\mu_{\left|C_{i}\right.}\) is at most a single Dirac.

Now if \(\theta_{i}\not\in\overline{C}_{i}\cup\overline{-C}_{i}\), assume first that \(\langle\theta_{i},(x_{i+1},1)\rangle>0\). This implies \(\langle\theta_{i},(x_{i},1)\rangle>0\) since \(\theta_{i}\not\in\overline{C}_{i}\cup\overline{-C}_{i}\). This then implies that either \(\alpha_{i}\in\overset{\circ}{C}_{i}\) or \(\beta_{i}\in\overset{\circ}{C}_{i}\), depending on whether \(i\geq i_{0}\) or \(i<i_{0}\). Assume first that \(\beta_{i}\in\overset{\circ}{C}_{i}\) (\(i\geq i_{0}\)) and define

\[t=\sup\left\{t^{\prime}\in[0,1]\mid t^{\prime}\alpha_{i}-\beta_{i}\in \overline{-C}_{i}\right\}.\]

By continuity, \(t\alpha_{i}-\beta_{i}\in\overline{-C}_{i}\). Moreover \(0<t<1\), since \(\beta_{i}\in\overset{\circ}{C}_{i}\) and \(\theta_{i}\not\in\overline{-C}_{i}\). We now define

\[\tilde{\mu}=\mu-\mu_{\left|C_{i}\right.}+(1-t)\|\alpha_{i}\|\delta_{\frac{ \alpha_{i}}{\left|\alpha_{i}\right.}}+\|t\alpha_{i}-\beta_{i}\|\delta_{\frac{ \alpha_{i}}{\left|\alpha_{i}-\beta_{i}\right.}}.\]

The function \(f_{\tilde{\mu},a,b}\) still interpolates the data. Similarly to the case \(\theta_{i}\in\overline{C}_{i}\cup\overline{-C}_{i}\), the minimisation of \(\left\|\mu\right\|_{\mathrm{TV}}\) implies that \(\mu_{\left|C_{i}\right.}\) is at most a single Dirac.

If \(\alpha_{i}\in\overset{\circ}{C}_{i}\) instead, similar arguments follow defining

\[t=\sup\left\{t^{\prime}\in[0,1]\mid\alpha_{i}-t^{\prime}\beta_{i}\in \overline{C}_{i}\right\}.\]

Symmetric arguments also hold if \(\langle\theta_{i},(x_{i+1},1)\rangle<0\). In any case, \(\mu_{\left|C_{i}\right.}\) is at most a single Dirac. This holds for any \(i=1,\ldots,n-1\), which finally leads to Lemma 1.

### Proof of Lemma 2

Before proving Lemma 2, let us show a one to one mapping from the parameterisation given by Lemma 1 to a parameterisation given by the sequences of slopes in the points \(x_{i}\). Let us define the sets

\[\mathcal{S}=\bigg{\{}(s_{1},\ldots,s_{n})\in\mathbb{R}^{n}\ |\forall i=1,\ldots,i_{0}-2,s_{i} \in S_{i}(s_{i+1}), \tag{23}\]

\[(s_{i_{0}-1},s_{i_{0}})\in\Lambda\quad\text{and}\quad\forall i=i_{0},\ldots,n-1,s_{i+1}\in S_{i}(s_{i})\quad\bigg{\}}\]

and

\[\mathcal{I}=\bigg{\{}(a,b,(a_{i},\tau_{i})_{i=1,\ldots,n-1})\ |\forall j=1, \ldots,n,\ ax_{j}+b+\sum_{i=1}^{n-1}a_{i}(x_{j}-\tau_{i})_{+}=y_{j},\quad\tau_{ i_{0}-1}\in(x_{i_{0}-1},x_{i_{0}}),\] \[\tau_{i}=\frac{x_{i}+x_{i+1}}{2}\ \text{if}\ a_{i}=0,\quad\forall i \in\{1,\ldots,i_{0}-2\},\tau_{i}\in(x_{i},x_{i+1}]\] \[\text{and}\quad\forall i\in\{i_{0},\ldots,n-1\},\tau_{i}\in[x_{i}, x_{i+1})\quad\bigg{\}}.\]

The condition \(\tau_{i}=\frac{x_{i}+x_{i+1}}{2}\) if \(a_{i}=0\) in the definition of \(\mathcal{I}\) is just to avoid redundancy, as any arbitrary value of \(\tau_{i}\) would yield the same interpolating function. Lemma 6 below gives a one to one mapping between these two sets.

**Lemma 6**.: _The function_

\[\psi:\begin{array}{l}\mathcal{I}\rightarrow\mathcal{S}\\ (a_{0},b_{0},(a_{i},\tau_{i})_{i=1,\ldots,n-1})\mapsto(\sum_{j=0}^{i-1}a_{j})_ {i=1,\ldots,n-1}\end{array}\]

_is a one to one mapping. Its inverse is given by_

\[\psi^{-1}:\begin{array}{l}\mathcal{S}\rightarrow\mathcal{I}\\ (s_{i})_{i\in[n]}\mapsto(a_{0},b_{0},(a_{i},\tau_{i})_{i\in[n-1]})\end{array}\]

_where_

\[a_{0}=s_{1};\qquad b_{0}=y_{1}-s_{1}x_{1};\qquad a_{i}=s_{i+1}-s_{i}\quad\text {for any}\ i\in[n-1];\]

\[\tau_{i}=\begin{cases}\frac{s_{i+1}-\delta_{i}}{s_{i+1}-s_{i}}x_{i+1}+\frac{ \delta_{i}-s_{i}}{s_{i+1}-s_{i}}x_{i}&\text{if}\ s_{i+1}\neq s_{i}\\ \frac{x_{i}+x_{i+1}}{2}&\text{otherwise}\end{cases}.\]

Proof.: For \((a_{0},b_{0},(a_{i},\tau_{i})_{i=1,\ldots,n-1})\in\mathcal{I}\), let \(f\) be the associated interpolator:

\[f(x)=a_{0}x+b_{0}+\sum_{i=1}^{n-1}a_{i}(x-\tau_{i})_{+}\]

and let \((s_{i})_{i\in[n]}=\psi(a_{0},b_{0},(a_{i},\tau_{i})_{i})\). Given the definition of \(\psi\), it is straightforward to check that \(s_{i}\) corresponds to the left (resp. right) derivative of \(f\) at \(x_{i}\geq 0\) (resp. \(x_{i}<0\)). We actually have the two following inequalities linking the parameters \((a_{0},b_{0},(a_{i},\tau_{i})_{i})\) and \((s_{i})_{i}\) for any \(i\in[n-1]\):

\[s_{i}+a_{i}=s_{i+1},\]

\[y_{i}+s_{i}(x_{i+1}-x_{i})+a_{i}(x_{i+1}-\tau_{i})=y_{i+1}.\]

The first equality comes from the (left or right) derivatives of \(f\) in \(x_{i}\), while the second equality is due to the interpolation of the data by \(f\). These two equalities imply that an interpolator with ReLU parameters in \(\mathcal{I}\) (i.e., \(f\)) can be equivalently described by its (left or right) derivatives in each \(x_{i}\). A straightforward computation then allows to show that \(\psi\) and \(\psi^{-1}\) are well defined and indeed verify \(\psi\circ\psi^{-1}=I_{\mathcal{S}}\) and \(\psi^{-1}\circ\psi=I_{\mathcal{I}}\). 

Using this bijection from \(\mathcal{I}\) to \(\mathcal{S}\), we can now prove Lemma 2. Note for the remaining of the proof \(\alpha=\min\limits_{\forall i\in[n],f(x_{i})=y_{i}}\int_{\mathbb{R}}\sqrt{1+x ^{2}}\mathrm{d}|f^{\prime\prime}(x)|\). Thanks to Lemma 1, we have the first equivalence:

\[\alpha=\min\limits_{(a_{0},b_{0},(a_{i},\tau_{i})_{i=1,\ldots,n-1})\in\mathcal{ I}}\sum_{i=1}^{n-1}|a_{i}|\sqrt{1+\tau_{i}^{2}}.\]

For any \((a_{0},b_{0},(a_{i},\tau_{i})_{i=1,\ldots,n-1})\in\mathcal{I}\), we can define thanks to Lemma 6\((s_{i})_{i}=\psi(a_{0},b_{0},(a_{i},\tau_{i})_{i})\in\mathcal{S}\). We then have \((a_{0},b_{0},(a_{i},\tau_{i})_{i})=\psi^{-1}((s_{i})_{i})\). Moreover, by definition of \(\psi^{-1}\), we can easily check that

\[|a_{i}|\sqrt{1+\tau_{i}^{2}} =\sqrt{a_{i}^{2}+(a_{i}\tau_{i})^{2}}\] \[=\sqrt{(s_{i+1}-s_{i})^{2}+((s_{i+1}-\delta_{i})x_{i+1}+(s_{i}- \delta_{i})x_{i})^{2}}\] \[=g_{i+1}(s_{i+1},s_{i}). \tag{24}\]

As \(\psi\) is a one to one mapping, we have for any function \(h\) the equivalence \(\min_{u\in\psi^{-1}(\mathcal{S})}h(u)=\min_{s\in\mathcal{S}}h(\psi^{-1}(s))\). In particular, thanks to Equation (24):

\[\min_{(a_{0},b_{0},(a_{i},\tau_{i})_{i=1},\ldots,n-1)\in\mathcal{I}}\sum_{i=1}^ {n-1}|a_{i}|\sqrt{1+\tau_{i}^{2}}=\min_{(s_{i})_{i}\in\mathcal{S}}\sum_{i=1}^{n -1}g_{i+1}(s_{i+1},s_{i}). \tag{25}\]

From there, define for any \(i\geq i_{0}\),

\[d_{i}(s_{i})=\min_{\begin{subarray}{c}(\tilde{s})_{j}\in\mathcal{S}\\ \text{s.t.}\ \tilde{s}_{i}=s_{i}\end{subarray}}\sum_{j=i}^{n-1}g_{j+1}(\tilde{s} _{j+1},\tilde{s}_{j});\]

and for any \(i<i_{0}\)

\[d_{i}(s_{i})=\min_{\begin{subarray}{c}(\tilde{s})_{j}\in\mathcal{S}\\ \text{s.t.}\ \tilde{s}_{i}=s_{i}\end{subarray}}\sum_{j=1}^{i-1}g_{j+1}(\tilde{s} _{j+1},\tilde{s}_{j}).\]

Obviously, we have from Equation (25) and the definition of \(\mathcal{S}\) that

\[\alpha=\min_{(s_{i_{0}-1},s_{i_{0}})\in\mathcal{S}}g_{i_{0}}(s_{i_{0}},s_{i_{0 }-1})+d_{i_{0}-1}(s_{i_{0}-1})+d_{i_{0}}(s_{i_{0}}). \tag{26}\]

It now remains to show by induction that for any \(i\) that \(c_{i}=d_{i}\). This is obviously the case for \(i=n\). Let us now consider \(i\in\{i_{0},\ldots,n-1\}\). The definition of \(d_{i}\) leads to

\[d_{i}(s_{i}) =\min_{\begin{subarray}{c}(\tilde{s})_{j}\in\mathcal{S}\\ \text{s.t.}\ \tilde{s}_{i}=s_{i}\end{subarray}}\sum_{j=i}^{n-1}g_{j+1}(\tilde{s }_{j+1},\tilde{s}_{j})\] \[=\min_{s_{i+1}\in S_{i}(s_{i})}\min_{\begin{subarray}{c}(\tilde{ s})_{j}\in\mathcal{S}\\ \text{s.t.}\ \tilde{s}_{i}=s_{i+1}\end{subarray}}g_{i+1}(s_{i+1},s_{i})+\sum_{j=i+1 }^{n-1}g_{j+1}(\tilde{s}_{j+1},\tilde{s}_{j})\] \[=\min_{s_{i+1}\in S_{i}(s_{i})}g_{i+1}(s_{i+1},s_{i})+\min_{ \begin{subarray}{c}(\tilde{s})_{j}\in\mathcal{S}\\ \text{s.t.}\ \tilde{s}_{i}=s_{i+1}\end{subarray}}\sum_{j=i+1}^{n-1}g_{j+1}( \tilde{s}_{j+1},\tilde{s}_{j}). \tag{27}\]

Now note that for any \(s_{i+1}\in S_{i}(s_{i})\), we have the equality of the sets

\[\big{\{}(\tilde{s}_{j})_{j\geq i+1}\ |\ (\tilde{s})_{j\in[n-1]}\in\mathcal{S} \text{ s.t.}\ \tilde{s}_{i}=s_{i}\text{ and }\tilde{s}_{i+1}=s_{i+1}\big{\}}=\big{\{}( \tilde{s}_{j})_{j\geq i+1}\ |\ (\tilde{s})_{j\in[n-1]}\in\mathcal{S}\text{ s.t.}\ \tilde{s}_{i+1}=s_{i+1}\big{\}}\]

Since the last term in Equation (27) only depends on \((\tilde{s}_{j})_{j\geq i+1}\), this implies that

\[d_{i}(s_{i}) =\min_{s_{i+1}\in S_{i}(s_{i})}g_{i+1}(s_{i+1},s_{i})+\min_{ \begin{subarray}{c}(\tilde{s})_{j}\in\mathcal{S}\\ \text{s.t.}\ \tilde{s}_{i+1}=s_{i+1}\end{subarray}}\sum_{j=i+1}^{n-1}g_{j+1}( \tilde{s}_{j+1},\tilde{s}_{j})\] \[=\min_{s_{i+1}\in S_{i}(s_{i})}g_{i+1}(s_{i+1},s_{i})+d_{i+1}(s_{ i+1}).\]

By induction, it naturally comes from the definition of \(c_{i}\) that \(c_{i}=d_{i}\) for any \(i\geq i_{0}\). Symmetric arguments hold for any \(i<i_{0}\), which finally gives \(c_{i}=d_{i}\) for any \(i\in[n]\). Equation (26) then yields Lemma 2.

Proof of Section 5

The proofs of this section are shown in the case where \(x_{1}<0\) and \(x_{n}\geq 0\). When all the \(x\) are positive, i.e., \(x_{1}\geq 0\), the adapted version of Lemma 2 would yield for \(i_{0}=1\) the equivalence11

Footnote 11: Note that in that case \(c_{1}\not\equiv 0\). Instead, \(c_{1}\) is defined through the recursion given in Equation (8).

\[\min_{\begin{subarray}{c}f\\ \forall i\in[n],f(x_{i})=y_{i}\end{subarray}}\int_{\mathbb{R}}\sqrt{1+x^{2}} \mathrm{d}|f^{\prime\prime}(x)|=\min_{s_{i_{0}}\in\mathbb{R}}c_{i_{0}}(s_{i_{0}}).\]

The proofs of Appendix E can then be easily adapted to this case (and similarly if \(x_{n}<0\)). Appendix E.5 at the end of the section more precisely states how to adapt them to this case.

### Proof of Theorem 2

Before proving Theorem 2, Lemma 7 below provides important properties verified by the functions \(c_{i}\) defined in Equation (8).

**Lemma 7**.: _For each \(i\in\{i_{0},\ldots,n-1\}\), the function \(c_{i}\) is convex, \(\sqrt{1+x_{i}^{2}}\)-Lipschitz on \(\mathbb{R}\) and minimised for \(s_{i}=\delta_{i}\). Moreover, on both intervals \((-\infty,\delta_{i}]\) and \([\delta_{i},+\infty)\):_

1. _either_ \(c_{i}(s_{i})=\sqrt{1+x_{i}^{2}}|s_{i}-\delta_{i}|+c_{i+1}(\delta_{i})\) _for all_ \(s_{i}\) _in the considered interval, or_ \(c_{i}\) _is strictly convex on the considered interval;_
2. \(|c_{i}(s_{i})-c_{i}(s^{\prime}_{i})|\geq\frac{1+x_{i}x_{i+1}}{\sqrt{1+x_{i+1}^{ 2}}}|s_{i}-s^{\prime}_{i}|\) _for all_ \(s_{i},s^{\prime}_{i}\) _in the considered interval._

_Similarly, for each \(i\in\{1,\ldots,i_{0}-2\}\), the function \(c_{i+1}\) is convex, \(\sqrt{1+x_{i+1}^{2}}\)-Lipschitz on \(\mathbb{R}\) and minimised for \(s_{i+1}=\delta_{i}\). Moreover, on both intervals \((-\infty,\delta_{i}]\) and \([\delta_{i},+\infty)\):_

1. _either_ \(c_{i+1}(s_{i+1})=\sqrt{1+x_{i+1}^{2}}|s_{i+1}-\delta_{i}|+c_{i}(\delta_{i})\) _for all_ \(s_{i+1}\) _in the considered interval, or_ \(c_{i+1}\) _is strictly convex on the considered interval;_
2. \(|c_{i+1}(s_{i+1})-c_{i+1}(s^{\prime}_{i+1})|\geq\frac{1+x_{i}x_{i+1}}{\sqrt{1+ x_{i}^{2}}}|s_{i+1}-s^{\prime}_{i+1}|\) _for all_ \(s_{i+1},s^{\prime}_{i+1}\) _in the considered interval._

Proof.: For any \(i\in\{1,\ldots,i_{0}-2\}\), we prove the result by (backward) induction. Since \(c_{n}=0\), a straightforward calculation gives12

Footnote 12: This calculation uses the fact that both \(x_{n-1}\) and \(x_{n}\) are positive, which implies that the minimal \(s_{n}\) in the definition of \(c_{n-1}\) is \(\delta_{n-1}\)

\[c_{n-1}(s_{n-1})=\sqrt{1+x_{n-1}^{2}}|s_{n-1}-\delta_{n-1}|,\]

which gives the wanted properties for \(i=n-1\).

Now consider \(i\in\{i_{0},\ldots,n-2\}\) such that \(c_{i+1}\) verifies all the properties in the first part of Lemma 7. We first show the Lipschitz property of \(c_{i}\). Let \(s_{i},s^{\prime}_{i}<\delta_{i}\) first. By inductive assumption, the function \(s_{i+1}\mapsto g_{i+1}(s_{i+1},s_{i})+c_{i+1}(s_{i+1})\) reaches a minimum on \([\delta_{i},+\infty)\). Consider \(s_{i+1}\geq\delta_{i}\) such that

\[c_{i}(s_{i})=g_{i+1}(s_{i+1},s_{i})+c_{i+1}(s_{i+1}).\]

Also by minimisation, \(c_{i}(s^{\prime}_{i})\leq g_{i+1}(s_{i+1},s^{\prime}_{i})+c_{i+1}(s_{i+1})\). For the vectors \(u=(x_{i+1},1)\) and \(v=(x_{i},1)\), it then holds:

\[c_{i}(s^{\prime}_{i})-c_{i}(s_{i}) \leq g_{i+1}(s_{i+1},s^{\prime}_{i})-g_{i+1}(s_{i+1},s_{i})\] \[=\|(s_{i+1}-\delta_{i})u-(s^{\prime}_{i}-\delta_{i})v\|-\|(s_{i+ 1}-\delta_{i})u-(s_{i}-\delta_{i})v\|\] \[\leq\|(s_{i}-s^{\prime}_{i})v\|=\sqrt{1+x_{i}^{2}}|s_{i}-s^{ \prime}_{i}|.\]

The first equality comes from the definition of \(g_{i+1}\) as a norm and the second inequality comes from the triangle inequality. By symmetry, we showed \(|c_{i}(s^{\prime}_{i})-c_{i}(s_{i})|\leq\sqrt{1+x_{i}^{2}}|s_{i}-s^{\prime}_{i}|\) for \(s_{i},s^{\prime}_{i}<\delta_{i}\).

Note that if \(s_{i}=\delta_{i}\), then \(s_{i+1}=\delta_{i}\) and we show similarly that \(c_{i}(s^{\prime}_{i})-c_{i}(\delta_{i})\leq\sqrt{1+x_{i}^{2}}|\delta_{i}-s^{ \prime}_{i}|\). Moreover,

\[c_{i}(s^{\prime}_{i})-c_{i}(\delta_{i}) =\min_{s^{\prime}_{i+1}\geq\delta_{i}}\|(s^{\prime}_{i+1}-\delta_{ i})u-(s^{\prime}_{i}-\delta_{i})v\|+c_{i+1}(s^{\prime}_{i+1})-c_{i+1}(\delta_{i})\] \[\geq\min_{s^{\prime}_{i+1}\geq\delta_{i}}\|(s^{\prime}_{i+1}- \delta_{i})u-(s^{\prime}_{i}-\delta_{i})v\|-\|(s^{\prime}_{i+1}-\delta_{i})u\|\] \[\geq 0.\]

The first inequality comes from the Lipschitz property of \(c_{i+1}\). The second from the fact that \((s^{\prime}_{i+1}-\delta_{i})u\) and \((s^{\prime}_{i}-\delta_{i})v\) are negatively correlated, since \(x_{i}\) and \(x_{i+1}\) are both positive. As a consequence, \(c_{i}\) is \(\sqrt{1+x_{i}^{2}}\)-Lipschitz on \((-\infty,\delta_{i}]\). Symmetrically, it is also \(\sqrt{1+x_{i}^{2}}\)-Lipschitz on \([\delta_{i},+\infty)\), which finally implies it is \(\sqrt{1+x_{i}^{2}}\)-Lipschitz on \(\mathbb{R}\). Moreover, the last calculation also shows that \(c_{i}\) is minimised for \(s_{i}=\delta_{i}\).

Let us now show that \(c_{i}\) verifies the first point on \((-\infty,\delta_{i}]\). By continuity, we only have to show it on \((-\infty,\delta_{i})\). Let \(s_{i}\in(-\infty,\delta_{i})\), we then have by definition

\[c_{i}(s_{i})=\min_{s_{i+1}\geq\delta_{i}}g_{i+1}(s_{i+1},s_{i})+c_{i+1}(s_{i+1 }).\]

If \(\delta_{i+1}\leq\delta_{i}\), note that both functions \(g_{i+1}(\cdot,s_{i})\) and \(c_{i+1}\) are increasing on \([\delta_{i},+\infty)\)13. The minimum is thus reached for \(s_{i+1}=\delta_{i}\) and

Footnote 13: Here again, we use the fact that \(x_{i}\) and \(x_{i+1}\) are positive.

\[c_{i}(s_{i})=\sqrt{1+x_{i}^{2}}|s_{i}-\delta_{i}|+c_{i+1}(\delta_{i}).\]

If \(\delta_{i+1}>\delta_{i}\), both functions \(g_{i+1}(\cdot,s_{i})\) and \(c_{i+1}\) are increasing on \([\delta_{i+1},+\infty)\). As a consequence, we can then rewrite

\[c_{i}(s_{i})=\min_{s_{i+1}\in[\delta_{i},\delta_{i+1}]}g_{i+1}(s_{i+1},s_{i})+c _{i+1}(s_{i+1}). \tag{28}\]

Assume first that \(c_{i+1}(s_{i+1})=\sqrt{1+x_{i+1}^{2}}|s_{i+1}-\delta_{i+1}|+c_{i+2}(\delta_{i+ 1})\) on \([\delta_{i},\delta_{i+1}]\). By triangle inequality, we actually have

\[g_{i+1}(s_{i+1},s_{i})\geq g_{i+1}(\delta_{i+1},s_{i})-\sqrt{1+x_{i+1}^{2}}| \delta_{i+1}-s_{i+1}|.\]

This leads for \(s_{i+1}\in[\delta_{i},\delta_{i+1}]\) to

\[g_{i+1}(s_{i+1},s_{i})+c_{i+1}(s_{i+1})\geq g_{i+1}(\delta_{i+1},s_{i})+c_{i+2 }(\delta_{i+1}).\]

The minimum in Equation (28) is thus reached for \(s_{i+1}=\delta_{i+1}\), which finally gives for any \(s_{i}\leq\delta_{i}\)

\[c_{i}(s_{i})=g_{i+1}(\delta_{i+1},s_{i})+c_{i+2}(\delta_{i+1}).\]

Since \(\delta_{i+1}>\delta_{i}\), it is easy to check that \(g_{i+1}(\delta_{i+1},\cdot)\) is strictly convex on \((-\infty,\delta_{i})\) and so is \(c_{i}\).

Let us now assume the last case, where \(c_{i+1}\) is strictly convex on \([\delta_{i},\delta_{i+1}]\). By contradiction, assume that the first point on \((-\infty,\delta_{i}]\) does not hold. Note in the following \(h(s_{i+1},s_{i})=g_{i+1}(s_{i+1},s_{i})+c_{i+1}(s_{i+1})\). For \(s_{i},s^{\prime}_{i}<\delta_{i}\), by continuity of \(h\), let \(s_{i+1},s^{\prime}_{i+1}\in[\delta_{i},\delta_{i+1}]\) be such that

\[c_{i}(s_{i})=h(s_{i+1},s_{i})\quad\text{and}\quad c_{i}(s^{\prime}_{i})=h(s^{ \prime}_{i+1},s^{\prime}_{i}).\]

For any \(t\in(0,1)\), by convexity of \(h\):

\[c_{i}(ts_{i}+(1-t)s^{\prime}_{i}) \leq h(t(s_{i+1},s_{i})+(1-t)(s_{i+1},s_{i}))\] \[\leq th(s_{i+1},s_{i})+(1-t)h(s^{\prime}_{i+1},s^{\prime}_{i})\] \[=tc_{i}(s_{i})+(1-t)c_{i}(s^{\prime}_{i}).\]

\(c_{i}\) is thus convex on \((-\infty,\delta_{i}]\). Moreover, the case of equality corresponds to the case of equality for both \(g_{i+1}\) and \(c_{i+1}\):

\[g_{i+1}(t(s_{i+1},s_{i})+(1-t)(s_{i+1},s_{i}))=tg_{i+1}(s_{i+1},s_{i})+(1-t)g_{ i+1}(s^{\prime}_{i+1},s^{\prime}_{i})\]

\[c_{i+1}(ts_{i+1}+(1-t)s^{\prime}_{i+1})=tc_{i+1}(s_{i+1})+(1-t)c_{i+1}(s^{ \prime}_{i+1}).\]

The former leads to the colinearity of the vectors \((s_{i+1}-\delta_{i},s_{i}-\delta_{i})\) and \((s^{\prime}_{i+1}-\delta_{i},s^{\prime}_{i}-\delta_{i})\); the latter gives \(s_{i+1}=s^{\prime}_{i+1}\) by strict convexity of \(c_{i+1}\). Two cases are then possible

\[\begin{cases}\text{either }s_{i+1}=\delta_{i}=s^{\prime}_{i+1}\\ \text{or }s_{i}=s^{\prime}_{i}.\end{cases}\]The former case then implies that \(c_{i}(s_{i})=\sqrt{1+x_{i}^{2}}|s_{i}-\delta_{i}|+c_{i+1}(\delta_{i})\). Since \(c_{i}(\delta_{i})=c_{i+1}(\delta_{i})\), \(c_{i}\) is \(\sqrt{1+x_{i}^{2}}\)-Lipschitz and convex on \((-\infty,\delta_{i}]\), this leads to \(c_{i}(s)=\sqrt{1+x_{i}^{2}}|s-\delta_{i}|+c_{i+1}(\delta_{i})\) for any \(s\in(-\infty,\delta_{i}]\). This contradicts the assumption that the first point does not hold on \((-\infty,\delta_{i}]\). Necessarily, we have \(s_{i}=s_{i}^{\prime}\). So \(c_{i}\) is strictly convex on \((-\infty,\delta_{i}]\), which leads to another contradiction: the first point does hold on \((-\infty,\delta_{i}]\).

Finally, we just showed that in any case, \(c_{i}\) is either strictly convex or equal to \(s_{i}\mapsto\sqrt{1+x_{i}^{2}}|s_{i}-\delta_{i}|\) on \((-\infty,\delta_{i}]\). Symmetric arguments yield the same on \([\delta_{i},+\infty)\). \(c_{i}\) is thus minimised in \(\delta_{i}\), \(\sqrt{1+x_{i}^{2}}\)-Lipschitz and verifies the first point on both intervals \((-\infty,\delta_{i}]\) and \([\delta_{i},+\infty)\). This directly implies that \(c_{i}\) is convex on \(\mathbb{R}\).

It now remains to show the second point on the two intervals. Let us show it on \((-\infty,\delta_{i}]\): on \((-\infty,\delta_{i})\) is actually sufficient by continuity. Consider \(s_{i}<s_{i}^{\prime}<\delta_{i}\) and \(s_{i+1}\in[\delta_{i},+\infty)\) such that

\[c_{i}(s_{i})=g_{i+1}(s_{i+1},s_{i})+c_{i+1}(s_{i+1}).\]

By definition of \(c_{i}\),

\[c_{i}(s_{i})-c_{i}(s_{i}^{\prime})\geq g_{i+1}(s_{i+1},s_{i})-g_{i+1}(s_{i+1},s _{i}^{\prime}).\]

Straightforward computations yield that the function

\[h_{2}:\quad\begin{array}{c}(-\infty,\delta_{i}]\to\mathbb{R}_{+}\\ s\mapsto g_{i+1}(s_{i+1},s)\end{array}\]

is convex and \(h_{2}^{\prime}(\delta_{i})=-\frac{1+x_{i}x_{i+1}}{\sqrt{1+x_{i+1}^{2}}}\). Thus, \(h_{2}^{\prime}\leq-\frac{1+x_{i}x_{i+1}}{\sqrt{1+x_{i+1}^{2}}}\), which finally implies

\[c_{i}(s_{i})-c_{i}(s_{i}^{\prime}) \geq h(s_{i})-h(s_{i}^{\prime})\] \[\geq\frac{1+x_{i}x_{i+1}}{\sqrt{1+x_{i+1}^{2}}}(s_{i}^{\prime}-s_{ i}).\]

The second point is thus verified on \((-\infty,\delta_{i})\) and on \((-\infty,\delta_{i}]\) by continuity. Symmetric arguments lead to the same property on \([\delta_{i},+\infty)\).

By induction, this implies the first part of Lemma 7. Symmetric arguments lead to the second part of Lemma 7 for \(i\leq i_{0}-2\). 

We can now prove Theorem 2. Following the proof of Lemma 2, there is a unique minimiser of Equation (4) if and only if the following problem admits a unique minimiser:

\[\min_{\mathbf{s}\in\mathcal{S}}\sum_{i=1}^{n-1}g_{i+1}(s_{i+1},s_{i}). \tag{29}\]

We already know that the minimum is attained thanks to Lemma 1. By construction of the functions \(c_{i}\), any minimum \(\tilde{\mathbf{s}}\) of Equation (29) verifies

\[\tilde{s}_{i}\in\operatorname*{argmin}_{s_{i}\in S_{i}(\tilde{s}_{i+1})}g_{i+1 }(\tilde{s}_{i+1},s_{i})+c_{i}(s_{i})\quad\text{for any }i\in[i_{0}-2] \tag{30}\]

\[(\tilde{s}_{i_{0}-1},\tilde{s}_{i_{0}})\in\operatorname*{argmin}_{(s_{i_{0}-1},s_{i_{0}})\in\Lambda}g_{i_{0}}(s_{i_{0}},s_{i_{0}-1})+c_{i_{0}-1}(s_{i_{0}-1} )+c_{i_{0}}(s_{i_{0}}) \tag{31}\]

\[\tilde{s}_{i+1}\in\operatorname*{argmin}_{s_{i+1}\in S_{i}(\tilde{s}_{i})}g_{i +1}(s_{i+1},\tilde{s}_{i})+c_{i+1}(s_{i+1})\quad\text{for any }i\in\{i_{0}, \ldots,n-1\}\]

It now remains to show that all these problems admit unique minimisers. First assume Equation (31) admits different minimisers \((s_{i_{0}-1},s_{i_{0}})\) and \((s_{i_{0}-1}^{\prime},s_{i_{0}}^{\prime})\). Note in the following \(h_{i_{0}-1}:(s,s^{\prime})\mapsto g_{i_{0}}(s,s^{\prime})+c_{i_{0}-1}(s^{\prime })+c_{i_{0}}(s)\). By minimisation and convexity of the three functions \(g_{i_{0}},c_{i_{0}-1},c_{i_{0}}\), for any \(t\in(0,1)\):

\[h_{i_{0}-1}(t(s_{i_{0}-1},s_{i_{0}})+(1-t)(s_{i_{0}-1}^{\prime},s_{i_{0}}^{ \prime}))\leq th_{i_{0}-1}(s_{i_{0}-1},s_{i_{0}})+(1-t)h_{i_{0}-1}(s_{i_{0}-1}^{ \prime},s_{i_{0}}^{\prime}) \tag{32}\]

\[=h_{i_{0}-1}(s_{i_{0}-1},s_{i_{0}}). \tag{33}\]

The whole segment joining \((s_{i_{0}-1},s_{i_{0}})\) and \((s_{i_{0}-1}^{\prime},s_{i_{0}}^{\prime})\) is then a minimiser. Without loss of generality, we can thus assume that both \(s_{i_{0}}\) and \(s_{i_{0}-1}^{\prime}\) are on the same side of \(\delta_{i_{0}-1}\) (e.g. smaller than \(\delta_{i_{0}-1}\)) and both \(s_{i_{0}}\) and \(s_{i_{0}}^{\prime}\) are on the same side of \(\delta_{i_{0}}\).

Moreover, Equation (33) implies an equality on \(g_{i_{0}}\) that leads to the colinearity of the vectors \((s_{i_{0}-1}-\delta_{i_{0}-1},s_{i_{0}}-\delta_{i_{0}-1})\) and \((s_{i_{0}-1}^{\prime}-\delta_{i_{0}-1},s_{i_{0}}^{\prime}-\delta_{i_{0}-1})\). In particular, both \(s_{i_{0}}\neq s_{i_{0}}^{\prime}\) and 

[MISSING_PAGE_FAIL:26]

We showed \(s_{i+1}^{*}\in[\min(\delta_{i},\delta_{i+1}),\max(\delta_{i},\delta_{i+1})]\) for any \(i\in\{i_{0},\ldots,n\}\). Symmetric arguments hold for \(i\in[i_{0}-1]\). This concludes the proof of Lemma 3.

### Proof of Lemma 4

Let us first prove that any sparsest interpolator \(f\) has at least a number of kinks given by the right sum. For that, we actually show that for \(k\geq 1\), on any interval \((x_{n_{k}-1},x_{n_{k+1}})\) with \(\delta_{n_{k}-1}\neq\delta_{n_{k}}\), \(f\) has at least \(\left\lceil\frac{n_{k+1}-n_{k}}{2}\right\rceil\) kinks, whose signs are given by \(\operatorname{sign}(\delta_{n_{k}}-\delta_{n_{k}-1})\). Consider any \(k\geq 1\) such that \(\delta_{n_{k}-1}\neq\delta_{n_{k}}\). Assume w.l.o.g. that \(\delta_{n_{k}-1}<\delta_{n_{k}}\). By the definition of Equation (10):

\[\delta_{j}>\delta_{j-1}\quad\text{for any }j\in\{n_{k},\ldots,n_{k+1}-1\}.\]

Obviously, \(f\) must count at least one positive kink on each interval of the form14\((x_{j-1},x_{j+1})\) for any \(n_{k}\leq j\leq n_{k+1}-1\). Note that we can build \(\left\lceil\frac{n_{k+1}-n_{k}}{2}\right\rceil\) disjoint such intervals. Thus, \(f\) has at least \(\left\lceil\frac{n_{k+1}-n_{k}}{2}\right\rceil\) positive kinks on \((x_{n_{k}-1},x_{n_{k+1}})\).

Footnote 14: Otherwise, the derivative would be weakly decreasing on the interval, contradicting interpolation.

The intervals of the form \((x_{n_{k}-1},x_{n_{k+1}})\) with \(\delta_{n_{k}-1}<\delta_{n_{k}}\) are disjoint by definition. As a consequence, \(f\) has a total number of positive kinks at least

\[\sum_{k\geq 1}\left\lceil\frac{n_{k+1}-n_{k}}{2}\right\rceil\mathds{1}_{ \delta_{n_{k}-1}<\delta_{n_{k}}}.\]

Similarly, \(f\) has a total number of negative kinks at least

\[\sum_{k\geq 1}\left\lceil\frac{n_{k+1}-n_{k}}{2}\right\rceil\mathds{1}_{ \delta_{n_{k}-1}>\delta_{n_{k}}},\]

which leads to the first part of Lemma 4

\[\min_{\begin{subarray}{c}f\\ \forall i,f(x_{i})=y_{i}\end{subarray}}\|f^{\prime\prime}\|_{0}\geq\sum_{k\geq 1 }\left\lceil\frac{n_{k+1}-n_{k}}{2}\right\rceil\mathds{1}_{\delta_{n_{k}-1} \neq\delta_{n_{k}}}.\]

We now construct an interpolating function that has exactly the desired number of kinks. Note that the problem considered in Lemma 4 is shift invariant (which is not the case of Equation (4)). As a consequence, we can assume without loss of generality that \(x_{1}\geq 0\). This simplifies the definition of the following sequence of slopes \(\mathbf{s}\in S\):

\[s_{1}=\delta_{1}\]

\[\text{and for any }i\in\{2,\ldots,n\},\quad s_{i}=\begin{cases}\delta_{i-1} \text{ if }(s_{i-1}=\delta_{i-1}\text{ or }i=n_{k}\text{ for some }k\geq 1)\\ s_{i}=\delta_{i}\text{ otherwise.}\end{cases}\]

It is easy to check that \(\mathbf{s}\in S\). We now consider the function \(f\) associated to the sequence of slopes by the mapping of Lemma 6 and an interval \([x_{n_{k}-1},x_{n_{k+1}})\) with \(\delta_{n_{k}-1}\neq\delta_{n_{k}}\). By definition, \(s_{n_{k}+1+2p}=\delta_{n_{k}+1+2p}\) for any \(p\) such that \(n_{k}+1\leq n_{k}+1+2p<n_{k+1}\). This implies that \(f\) has no kink in the interval \([x_{n_{k}+1+2p},x_{n_{k}+2+2p})\). From there, a simple calculation shows that \(f\) has at most \(\left\lceil\frac{n_{k+1}-n_{k}}{2}\right\rceil\) kinks on \([x_{n_{k}},x_{n_{k+1}})\). Moreover, as \(s_{i}=\delta_{i-1}\) if \(i=n_{k}\), \(f\) has no kink on intervals \([x_{n_{k}},x_{n_{k+1}})\) when \(\delta_{n_{k}-1}=\delta_{n_{k}}\). \(f\) is thus an interpolating function with at most

\[\sum_{k\geq 1}\left\lceil\frac{n_{k+1}-n_{k}}{2}\right\rceil\mathds{1}_{ \delta_{n_{k}-1}\neq\delta_{n_{k}}},\]

kinks, which concludes the proof of Lemma 4.

### Proof of Theorem 3

Let \(f\) be the minimiser of Equation (4). The proof of Theorem 3 separately shows that \(f\) has exactly \(\left\lceil\frac{n_{k+1}-n_{k}}{2}\right\rceil\mathds{1}_{\delta_{n_{k}-1}\neq \delta_{n_{k}}}\) kinks on each \((x_{n_{k}-1},x_{n_{k+1}})\). Fix in the following \(k\geq 0\).

Assume first that \(\delta_{n_{k}-1}=\delta_{n_{k}}\). Then Lemma 3 along with the definitions of \(n_{k}\) and \(n_{k+1}\) directly imply that \(s^{*}_{i}=\delta_{n_{k}-1}\) for any \(i=n_{k},\ldots,n_{k+1}-1\). This then implies that the associated interpolator, i.e. \(f\) has no kink on \((x_{n_{k}-1},x_{n_{k+1}})\).

Now assume that \(\delta_{n_{k}-1}\neq\delta_{n_{k}}\). Without loss of generality, assume \(\delta_{n_{k}-1}<\delta_{n_{k}}\). By the definition of Equation (10):

\[\delta_{j}>\delta_{j-1}\quad\text{for any }j\in\{n_{k},\ldots,n_{k+1}-1\}.\]

Moreover, by definition of \(n_{k}\), we have

\[\begin{cases}\text{either }n_{k}=1\\ \text{or }\delta_{n_{k}-1}\leq\delta_{n_{k}-2}\end{cases}\qquad\text{and} \qquad\begin{cases}\text{either }n_{k+1}=n\\ \text{or }\delta_{n_{k+1}}\leq\delta_{n_{k+1}-1}\end{cases}\]

Since \(n_{k+1}\leq n_{k}+3\) by Assumption 1, Lemma 8 below states that for all the cases, \(f\) has exactly \(\left\lceil\frac{n_{k+1}-n_{k}}{2}\right\rceil\) kinks on \((x_{n_{k}-1},x_{n_{k+1}})\).

Symmetric arguments hold if \(\delta_{n_{k}-1}>\delta_{n_{k}}\). In conclusion, \(f\) has exactly \(\left\lceil\frac{n_{k+1}-n_{k}}{2}\right\rceil\mathds{1}_{\delta_{n_{k}-1}\neq \delta_{n_{k}}}\) kinks on each \((x_{n_{k}-1},x_{n_{k+1}})\). This implies that \(f\) has at most

\[\sum_{k\geq 1}\left\lceil\frac{n_{k+1}-n_{k}}{2}\right\rceil\mathds{1}_{ \delta_{n_{k}-1}\neq\delta_{n_{k}}}\]

kinks in total. This concludes the proof of Theorem 3, thanks to Lemma 4.

**Lemma 8**.: _For any \(k\geq 0\), if \(\delta_{n_{k}-1}<\delta_{n_{k}}\), then the minimiser of Equation (4)\(f\) has_

1. \(1\) _kink on_ \((x_{n_{k}-1},x_{n_{k+1}})\) _if_ \(n_{k+1}=n_{k}+1\)_;_
2. \(1\) _kink on_ \((x_{n_{k}-1},x_{n_{k+1}})\) _if_ \(n_{k+1}=n_{k}+2\)_;_
3. \(2\) _kinks on_ \((x_{n_{k}-1},x_{n_{k+1}})\) _if_ \(n_{k+1}=n_{k}+3\)_._

Lemma 8 is written in this non-compact way since its proof shows separately (with similar arguments) the three cases.

Proof.: 1) Consider \(n_{k+1}=n_{k}+1\). First assume that \(x_{n_{k}}\geq 0\). Lemma 3 implies that \(s^{*}_{n_{k}+1}\in[\delta_{n_{k}+1},\delta_{n_{k}}]\) and \(s^{*}_{n_{k}}\in[\delta_{n_{k}-1},\delta_{n_{k}}]\). In particular, \(s^{*}_{n_{k}}\leq\delta_{n_{k}}\), which implies that \(s^{*}_{n_{k}+1}=\delta_{n_{k}}\). Similarly, \(s^{*}_{n_{k}-1}\geq\delta_{n_{k}-1}\), which implies that \(s^{*}_{n_{k}}=\delta_{n_{k}-1}\). Using the mapping from Lemma 6, both values \(s^{*}_{n_{k}}\) and \(s^{*}_{n_{k}+1}\) yield that the associated function \(f\) has exactly one kink on \((x_{n_{k}-1},x_{n_{k}+1})\), which is located at \(x_{n_{k}}\). Similar arguments hold if \(x_{n_{k}}<0\).

2) Consider now \(n_{k+1}=n_{k}+2\).

First assume that \(x_{n_{k}+2}<0\). Thanks to Lemma 3, we can show similarly to the case 1) that \(s^{*}_{n_{k}+1}=\delta_{n_{k}+1}\).

Now assume that \(x_{n_{k}+2}\geq 0\). Similarly to the case 1), \(s^{*}_{n_{k}+2}=\delta_{n_{k}+1}\). The minimisation problem of the slopes becomes on \(s^{*}_{i+1}\) for \(i=n_{k}\):

\[s^{*}_{i+1}=\operatorname*{argmin}_{s\in\tilde{S}}g_{i+1}(s,s^{*}_{i})+g_{i+2} (\delta_{i+1},s),\]

where \(\tilde{S}=S_{i}(s^{*}_{i})\) if \(x_{i+1}\geq 0\), and \(\tilde{S}=\{\delta_{i+1}\}\) otherwise. Note that \(g_{i+1}(s,s^{*}_{i})\) is \(\sqrt{1+x_{i+1}^{2}}\)-Lipschitz in its first argument, while \(g_{i+2}(\delta_{i+1},s)=\sqrt{1+x_{i+1}^{2}}|s-\delta_{i+1}|\). Moreover, \(s^{*}_{n_{k}}\in[\delta_{n_{k}-1},\delta_{n_{k}}]\). As a consequence, either \(x_{n_{k}+1}\geq 0\) and \(s^{*}_{n_{k}}=\delta_{n_{k}}=s^{*}_{n_{k}+1}\); or \(s^{*}_{n_{k}+1}=\delta_{n_{k}+1}\).

Symmetrically, when reasoning on the points \(x_{n_{k}-1},x_{n_{k}}\):

* either \(s^{*}_{n_{k}}=\delta_{n_{k}-1}\);
* or (\(x_{n_{k}}<0\) and \(s^{*}_{n_{k}}=\delta_{n_{k}}=s^{*}_{n_{k}+1}\)).

There are thus two possible cases in the end:

[MISSING_PAGE_FAIL:29]

Similarly to the proof of Lemma 1, we can first show the existence of a minimum.15 Let us now consider \(f\) a minimiser of

Footnote 15: Proving that the sequence \((a_{j},b_{j})\) is bounded is here a bit more tricky. Either the data is linearly separable, in which case the minimum is \(0\), or the data is not linearly separable. When the data is not linearly separable, then \((a_{j},b_{j})\) is necessarily bounded, since \((\mu_{j},a_{j},b_{j})\) would behave as a linear classifier for arbitrarily large \((a_{j},b_{j})\).

\[\min_{\begin{subarray}{c}f\\ \forall i\in[n],y_{i}f(x_{i})\geq 1\end{subarray}}\left\|\sqrt{1+x^{2}}f^{ \prime\prime}\right\|_{\mathrm{TV}}. \tag{37}\]

Define the set

\[S=\big{\{}n_{k}\mid k\in\{2,\ldots,K-1\}\}\cup\{n_{k}-1\mid k\in\{2,\ldots,K-1 \}\big{\}}.\]

By continuity of \(f\), we can choose an alternative training set \((\tilde{x}_{i},\tilde{y}_{i})\) satisfying:

\[\tilde{x}_{i}\in[x_{n_{k}-1},x_{n_{k}}]\quad\text{for any }i\in\{n_{k}-1,n_{k}\},\] \[y_{i}=f(\tilde{x}_{i})\quad\text{for any }i\in S.\]

Then, a direct application of Theorem 2 yields that the minimisation problem

\[\min_{\begin{subarray}{c}f\\ \forall i\in S,y_{i}=f(\tilde{x}_{i})\end{subarray}}\left\|\sqrt{1+x^{2}}\tilde {f}^{\prime\prime}\right\|_{\mathrm{TV}}, \tag{38}\]

admits a unique minimiser, that we denote \(f_{\mathrm{reg}}\). But also note that this unique minimiser is also in the constraint set of Equation (37) thanks to Lemma 3, so that

\[\left\|\sqrt{1+x^{2}}f^{\prime\prime}_{\mathrm{reg}}\right\|_{\mathrm{TV}}\geq \left\|\sqrt{1+x^{2}}f^{\prime\prime}\right\|_{\mathrm{TV}}.\]

However, since \(f\) is in the constraint set of Equation (38), we actually have an equality, and by unicity of the minimiser of Equation (38),

\[f_{\mathrm{reg}}=f.\]

Moreover, it is easy to check that Assumption 1 holds for the data \((\tilde{x}_{i},y_{i})_{i\in S}\), with \(n_{k+1}=n_{k}+2\). As a consequence, Theorem 3 implies that the minimiser of Equation (38) is among the sparsest interpolators for the set \((\tilde{x}_{i},y_{i})_{i\in S}\), i.e. it exactly counts \(K-2\) kinks. This then implies that \(\|f^{\prime\prime}\|_{0}=K-2\), so that

\[\operatorname*{argmin}_{\begin{subarray}{c}f\\ \forall i\in[n],y_{i}f(x_{i})\geq 1\end{subarray}}\overline{R}_{1}(f)\subset \operatorname*{argmin}_{\begin{subarray}{c}f\\ \forall i\in[n],y_{i}f(x_{i})\geq 1\end{subarray}}\|f^{\prime\prime}\|_{0}. \tag{39}\]