Improving self-training under distribution shifts via anchored confidence with theoretical guarantees

Taejong Joo & Diego Klabjan

Department of Industrial Engineering & Management Sciences

Northwestern University

Evanston, IL, USA

{taejong.joo,d-klabjan}@northwestern.edu

###### Abstract

Self-training often falls short under distribution shifts due to an increased discrepancy between prediction confidence and actual accuracy. This typically necessitates computationally demanding methods such as neighborhood or ensemble-based label corrections. Drawing inspiration from insights on early learning regularization, we develop a principled method to improve self-training under distribution shifts based on temporal consistency. Specifically, we build an uncertainty-aware temporal ensemble with a simple relative thresholding. Then, this ensemble smooths noisy pseudo labels to promote selective temporal consistency. We show that our temporal ensemble is asymptotically correct and our label smoothing technique can reduce the optimality gap of self-training. Our extensive experiments validate that our approach consistently improves self-training performances by 8% to 16% across diverse distribution shift scenarios without a computational overhead. Besides, our method exhibits attractive properties, such as improved calibration performance and robustness to different hyperparameter choices.

## 1 Introduction

In this work, we address the challenge of adapting pre-trained neural networks at test time under distribution shifts, a problem known as test-time adaptation (TTA) or source-free domain adaptation (SFDA). Distribution shifts--where a model trained on one distribution is then tested on a different one--are ubiquitous in many practical scenarios due to demographic subpopulation shift [1] and changes in data collection environments [2, 3, 4]. Despite the robust performance of neural networks under independent and identically distributed (i.i.d.) settings, they often suffer from substantial performance degradation under such shifts [5, 6]. Recently, TTA and SFDA have proven their effectiveness in resolving these critical issues by effectively leveraging information about the distribution shifts contained in unlabeled samples given at the test time.

Self-training is the basis of many state-of-the-art methods in TTA and SFDA [7, 8, 9], utilizing pseudo labels generated from the model's own predictions to train a model on unlabeled samples [10]. Since pseudo labels are regarded as true labels in self-training, the success of self-training methods highly depends on how to filter incorrect pseudo labels to prevent self-confirmation bias [11]. This issue has been effectively handled via simple confidence-based thresholding in i.i.d. settings [12, 13, 14]. However, the distribution shifts make it hard to filter incorrect pseudo labels due to high noise rates even under high threshold [15]. Thus, sophisticated methods filter incorrect pseudo labels based on a neighborhood structure of the data [16, 17, 18] and consistency of multiple predictions under different models [19] or augmentations [20](cf. Section 5), which are computationally intensive by nature.

Recent insights in [8] suggest an alternative strategy can be also effective: promoting temporal consistency can enhance self-training performance in SFDA without the computational burden of previous methods. The temporal consistency regularizer, so called early learning regularization (ELR) [21], was originally developed to address neural networks' tendency to learn clean information first and then gradually memorize noisy labels [22; 23]; this setting is naturally connected to self-training scenarios when we regard the pseudo labels as random noisy labels. However, the impacts of ELR on self-training have not been fully understood. Also, since ELR does not consider unique characteristics of distribution shifts, we aim to answer the following question: _Is there any principled way to improve the way of memorizing all past predictions tailored for self-training under distribution shifts?_

In this work, we show that the answer is affirmative by proposing Anchored Confidence (AnCon) that uses _confident predictions_ to support a generalized notion of _temporal consistency_. Specifically, we construct a generalized temporal ensemble, which weighs predictions based on predictive uncertainty, and then use the ensemble as a smoothing vector in label smoothing [24]. Then, through rigorous theoretical analyses, we show that our simple heuristic for the generalized temporal ensemble is asymptotically correct and that the label smoothing formulation can reduce the optimality gap. As a result, AnCon can correct wrong pseudo labels without expensive computations and can be easily applied to self-training methods by replacing one-hot pseudo labels with smooth pseudo labels, unlike neighborhood-based or centroid methods. Through extensive experiments, we show that AnCon improves self-training under diverse distribution shift scenarios and posses many attractive properties.

Our contribution can be summarized as follows: **1)** We develop AnCon, which is the first algorithm that attempts to improve self-training under distribution shifts by generalizing a notion of temporal consistency with theoretical guarantees; **2)** Without any additional forward passes or neighborhood search, AnCon improves self-training performances 8% and 16% under domain shifts and image corruptions, respectively; **3)** Remarkably, we also show that AnCon significantly improves calibration performance and is robust with respect to model selection methods and hyperparameter choices.

## 2 Background

**Notation and setup** For an input space \(\mathcal{X}\subseteq\mathbb{R}^{d}\) and a label space \(\mathcal{Y}=[K]:=\{1,2,\cdots,K\}\), we define \(X\) and \(Y\) to be random variables of input and output with probability densities \(p_{X}\) and \(p_{Y}\), respectively. We also define a neural network \(f(\cdot;\theta):\mathcal{X}\rightarrow\triangle^{K-1}\) parameterized by a parameter \(\theta\in\mathbb{R}^{p}\) where \(\triangle^{K-1}\) is the probability simplex with \(K\) elements. Our goal is to minimize the cross-entropy loss \(\min_{\theta\in\mathbb{R}^{p}}l(\theta):=\mathbb{E}_{XY}[H(f(X;\theta),p_{Y|X })]\) where \(H(f(x;\theta),p_{Y|X=x}):=-\sum_{k\in[K]}p(Y=k|x)\log f_{k}(x;\theta)\). In the SFDA setting, we are given an initial parameter \(\theta_{0}\) that is trained on a different data generating distribution \((X^{\prime},Y^{\prime})\), e.g., \(\theta_{0}\in\arg\min_{\theta\in\mathbb{R}^{p}}\mathbb{E}_{X^{\prime}Y^{\prime} }[H(f(X^{\prime};\theta),p_{Y^{\prime}|X^{\prime}})]\). We can think about this setting as either (1) \((X^{\prime},Y^{\prime})\) being pre-training data and \(\theta_{0}\) the foundation model with the task of fine-tuning the model on unlabeled \(X\) or (2) a transfer learning problem with \((X^{\prime},Y^{\prime})\) being the source domain data and \(X\) the target domain. Here, we assume only covariate shift without concept shift; that is, \(p_{X}\neq^{D}p_{X^{\prime}}\) but \(p_{Y|X}=^{D}p_{Y^{\prime}|X^{\prime}}\). Even in this case, we note that suboptimality under the distribution shift, i.e., \(\min_{\theta\in\mathbb{R}^{p}}l(\theta)-l(\theta_{0})\), can be large.

**Self-training** In this work, we tackle the distribution shifts by using the self-training method that replaces the true label \(Y(x)\) by the pseudo label, i.e., \(\min_{\theta_{new}}l(\theta_{new};\theta)=\mathbb{E}_{X}[-\log f_{\hat{Y}(x; \theta)}(x;\theta_{new})]\) where \(\hat{Y}(x;\theta):=\arg\max_{k\in[K]}f_{k}(x;\theta)\) is a pseudo label under \(\theta\). Specifically, the algorithmic framework of self-training is as follows: given \(\theta_{0}\), we iteratively find model parameter \(\theta_{m+1}\) for \(m=0,1,\cdots\) with \(\theta_{m+1}\in\arg\min_{\theta_{new}}\hat{l}(\theta_{new};\theta_{m})\). For later use, we also define a prediction confidence \(c(x;\theta):=\max_{k\in[K]}f_{k}(x;\theta)\) and \(\theta_{0:m}:=(\theta_{0},\cdots,\theta_{m})\).

**Early learning regularization** In the learning from noisy labels (LFN) scenario, [21] identified an "early-learning phenomenon" where neural networks initially learn information contained in clean labels before gradually memorizing noisy labels, leading to a performance deterioration as training progresses. To mitigate this issue, ELR penalizes predictions that deviate from earlier predictions by defining a target network from the past predictions: \(\bar{f}_{ELR}(x;\theta_{0:m}):=\sum_{j=0}^{m}(1-\gamma)\cdot\gamma^{m-j}f(x; \theta_{j})\). Then, adding an auxiliary loss of \(L_{ELR}(\theta;\theta_{0:m})=\mathbb{E}_{X}[\log(1-f(X;\theta)^{T}\bar{f}_{ELR }(X;\theta_{0:m}))]\) to \(\hat{l}(\theta;\theta_{m})\) can prevent memorization of noisy labels while preserving correct patterns.

Notably, this insight has recently been confirmed to be applicable in the SFDA setting by [8]. This observation is appealing because ELR can be efficiently implemented by reusing past predictions without additional forward passes or neighborhood searching, unlike dominant methods in SFDA[16; 17; 18; 19; 20]. Nevertheless, given that ELR stems from a general property of the neural network training in the i.i.d. setting, herein we aim to step towards a more principled approach to encourage temporal consistency tailored for distribution shift scenarios.

## 3 Anchored confidence

In this section, we introduce AnCon, which promotes the temporal consistency on selectively chosen predictions via label smoothing. In Section 3.1, we first explain the idea of promoting selective temporal consistency based on confident predictions via label smoothing [24] with a temporal ensemble. Then, in Section 3.2, we explain how to effectively construct temporal ensemble for improving self-training under distribution shifts. Finally, we theoretically analyze the efficacy of AnCon by drawing connection between our method and knowledge distillation in Section 3.3.

### Selective temporal consistency via label smoothing

In this work, we utilize label smoothing [24] to promote the selective temporal consistency instead of using an auxiliary loss function like ELR. Specifically, given a generalized temporal ensemble \(\bar{f}(x;\theta_{0:m},\mathbf{w}_{0:m})\) with \(\mathbf{w}_{0:m}:=(w_{0},\cdots,w_{m})\) which will be specified in Section 3.2, we construct a regularized pseudo label \(\bar{Y}(X;\theta_{0:m},\mathbf{w}_{0:m})\) by using \(\bar{f}(x;\theta_{0:m},\mathbf{w}_{0:m})\) as a smoothing vector for the pseudo label \(\hat{Y}(x;\theta_{m})\). That is, we perform self-training by

\[\min_{\theta}\mathbb{E}_{X}[H(f(X;\theta),\tilde{Y}(X;\theta_{0:m},\mathbf{w}_ {0:m}))],\quad\tilde{Y}(X;\theta_{0:m},\mathbf{w}_{0:m})=(1-\lambda)E_{1}( \hat{Y}(x;\theta_{m}))+\lambda\bar{f}(x;\theta_{0:m},\mathbf{w}_{0:m}) \tag{1}\]

where \(\lambda\in[0,1]\) is a coefficient, \(E_{1}(\cdot)\) is the one-hot encoding, and \(\bar{f}(x):=(\bar{f}_{1}(x),\cdots,\bar{f}_{K}(x))\) is the \(K\)-dimensional output of the generalized temporal ensemble (cf. (2)).

Thus, AnCon can control the usage of potentially noisy information in \(\hat{Y}(x;\theta_{m})\) based on its consistency with \(\bar{f}(x;\theta_{0:m},\mathbf{w}_{0:m})\). Not only can this approach preserve the early learning phenomenon as in ELR (cf. Section 2), but the label smoothing formulation also significantly stabilizes the self-training performance under different hyperparameter choices due to the fact that the optimal values of hyperparameters are less problem dependent compared to its equivalent auxiliary regularization [25]. Beyond removing the burden of hyperparameter search, this robustness is a particularly intriguing property under distribution shift scenarios where the model selection becomes a challenging task.

Further, encouraging the temporal consistency through label smoothing enables us to connect our method with knowledge distillation (KD) [26], which can provide a wide range of principled techniques and theoretical results developed in KD. As a concrete example, we will show when and how AnCon can reduce the optimality gap of self-training, i.e., a case with \(\lambda=0\), in Section 3.3.

### Constructing an effective generalized temporal ensemble with prediction confidences

Next, we construct the generalized temporal ensemble that makes the selective temporal consistency in (1) work effectively in self-training under distribution shifts. Specifically, given \(\theta_{0:m}\) and weights \(\mathbf{w}_{0:m}(x)\in\triangle^{m}\) for each \(x\in\mathcal{X}\), the prediction by the generalized temporal ensemble is

\[\bar{f}_{k}(x;\theta_{0:m},\mathbf{w}_{0:m}):=\sum_{i=0}^{m}w_{i}(x)\cdot p(y =k|x,\theta_{i}),\quad k\in[K] \tag{2}\]

where \(p(y|x,\theta_{i})\) is the prediction made by \(f(x;\theta_{i})\), which can be either soft (\(p(y=j|x,\theta_{i})=f_{j}(x;\theta_{i})\)) or hard (\(p(y=j|x,\theta_{i})=1\) if \(j=\arg\max_{k\in[K]}f_{k}(x;\theta_{i})\) and \(p(y=j|x,\theta_{i})=0\) otherwise). In this work, we use hard prediction because soft prediction puts more weights on recent predictions since self-training tends to keep increasing the prediction confidence during training.

Surprisingly, we will show that the following simple relative thresholding for determining \(\mathbf{w}_{0:m}\) gives the asymptotic optimal weights achieving the minimum worst-case optimality gap of self-training:

\[w_{m}(x)\propto\mathbf{1}(c(x;\theta_{m})>\delta_{m}^{(\beta)}),\quad\delta_{ m}^{(\beta)}:=\sum_{i=0}^{m}(1-\beta)\beta^{m-i}\hat{\mathbb{E}}_{X}[c(X; \theta_{i})] \tag{3}\]

where \(\delta_{m}^{(\beta)}\) is an exponential moving average (EMA) of prediction confidence with hyperparameter \(\beta\) and \(\hat{\mathbb{E}}[c(X;\theta_{i})]\) is a Monte-Carlo approximation of \(\mathbb{E}[c(X;\theta_{i})]\) with mini-batch samples.

Intuitively, our weighting mechanism aggregates only relatively confident predictions with a uniform weight. Given the observation that relative ordering of confidence is highly correlated with accuracy even under distribution shifts [27, 28], our thresholding rule would tend to put non-zero weights on correct predictions. Besides, by employing the relative criterion, the thresholding does not suffer from the problems that neglect predictions obtained in the early stage of training.

We also remark that AnCon has almost the same computational cost as ELR. Specifically, (2) can be implemented by \(\tilde{f}_{k}(x;\theta_{0:m},\mathbf{w}_{0:m})=\tilde{f}_{k}(x;\theta_{0:m-1}, \mathbf{w}_{0:m-1})+w_{m}(x)p(y=k|x,\theta_{m})\) that requires to store the weighted sum of previous predictions without additional forward passes or storing previous parameters, which is the same as storing the previous logit vector in ELR. Similarly, (3) can be efficiently implemented by \(\delta_{m}^{(\beta)}=\beta\delta_{m-1}^{(\beta)}+(1-\beta)\hat{\mathbb{E}}_{X} [c(X;\theta_{m})]\) that requires constant additional computational costs compared to vanilla self-training with the constant being small. Therefore, AnCon shares the same computational benefits as ELR compared to other state-of-the-art methods in SFDA.

**On optimality of the relative thresholding in (3)** From the optimization perspective, the optimal weights \(\mathbf{w}_{0:m}^{\dagger}\) correspond to the weights under which self-training with \(\tilde{Y}(X;\theta_{0:m},\mathbf{w}_{0:m})\) can minimize the expected loss:

\[\mathbf{w}_{0:m}^{\dagger}\in\operatorname*{arg\,min}_{\mathbf{w}_{0:m}}l( \theta_{\mathbf{w}_{0:m}}^{\dagger}),\quad\theta_{\mathbf{w}_{0:m}}^{\dagger} \in\operatorname*{arg\,min}_{\theta}\hat{\mathbb{E}}_{X}[H(f(X;\theta), \tilde{Y}(X;\theta_{0:m},\mathbf{w}_{0:m}))]. \tag{4}\]

Unfortunately, \(l(\theta_{\mathbf{w}_{0:m}}^{\dagger})\), or its empirical counterpart, is not available in self-training due to the absence of labels. Further, even if labels are given, solving (4) is intractable due to non-smoothness of \(l(\theta_{\mathbf{w}_{0:m}}^{\dagger})\) with respect to \(\mathbf{w}_{0:m}\) and the cost of finding \(\theta_{\mathbf{w}_{0:m}}^{\dagger}\).

To circumvent this issue, we show in Section 3.3 that (4) can be relaxed to the problem of finding ensemble weights that give a maximum likelihood estimation (MLE) solution under certain conditions. As a result, instead of solving the intractable optimization in (4), we find the optimal weights by

\[\mathbf{w}_{0:m}^{\dagger}\in\operatorname*{arg\,max}_{\mathbf{w}_{0:m}} \mathbb{E}_{XY}[\log\bar{f}_{Y}(X;\theta_{0:m},\mathbf{w}_{0:m})]. \tag{5}\]

In the following theorem which is proven in Appendix B.2, we show that the simple relative thresholding in (3) can make \(\bar{f}(x;\theta_{0:m},\mathbf{w}_{0:m})\) asymptotically correct for samples where the neural network tends to be relatively confident during self-training and thus our simple weighting mechanism in (3) to be the solution of (5) in the asymptotic region.

**Theorem 3.1**.: _Let \(A_{i}(c):=\{x\in\mathcal{X}|c(x;\theta_{i})>c\}\), \(Q(x;\mathbf{c}_{0:m}):=\sum_{i=0}^{m}\mathbf{1}(x\in A_{i}(c_{i}))\), and \(\bar{p}(x;\mathbf{c}_{0:m})=\frac{1}{Q(x;\mathbf{c}_{0:m})}\sum_{i=0}^{m} \mathbb{E}_{Y|X=x}[\mathbf{1}(Y(x)=\hat{Y}(x;\theta_{i}))]\mathbf{1}(x\in A_{i} (c_{i}))\) for \(x\) such that \(Q(x;\mathbf{c}_{0:m})\)>0. Let us assume that random events \(\mathbf{1}(Y(x)=\hat{Y}(x;\theta_{i}))\) and \(\mathbf{1}(Y(x)=\hat{Y}(x;\theta_{j}))\) are conditionally independent given \(X\in A_{i}(c)\) for \(j\in\{0,\cdots,i-1\}\), \(x\in\mathcal{X}\), \(c\in[0,1)\). If \(x\in\mathcal{X}\) such that \(\bar{p}(x;\mathbf{c}_{0:m})>1/2\), then for the generalized temporal ensemble in (3), it holds that_

\[p(\operatorname*{arg\,max}_{k\in[K]}\bar{f}_{k}(x;\theta_{0:m},\mathbf{w}_{0:m })\neq Y(x))\leq\exp\left(-\frac{Q(x;\mathbf{c}_{0:m})}{2}\cdot\xi(\bar{p}(x; \mathbf{c}_{0:m}))\right) \tag{6}\]

_where \(\xi(z):=2z-1-\log(2z)\) is a positive increasing function in \(z\in[0.5,1]\)._

The result states that as long as the average accuracy for _relatively confident predictions_ over iterations exceeds 50%, the error rate of the generalized temporal ensemble monotonically decreases as \(Q(x;\mathbf{c}_{0:m})\) increases. Furthermore, \(\bar{f}(x;\theta_{0:m},\mathbf{w}_{0:m})\) is asymptotically correct on \(x\) such that \(Q(x;\mathbf{c}_{0:m})\rightarrow\infty\) as \(m\rightarrow\infty\). AnCon aims to achieve these desirable properties through the _uncertainty-aware_ temporal consistency that helps to satisfy the condition \(\bar{p}(x;\mathbf{c}_{0:m})>0.5\). Specifically, as shown in Figure 3(a) in Appendix, our generalized temporal ensemble's accuracy tends to significantly increase as the number of confident samples increases, being consistent with our theory. We note that this monotonic improvement would not be the case for the temporal ensemble without uncertainty-awareness and vanilla self-training (cf. Figure 3(a)).

Finally, we emphasize that the assumption \(\bar{p}(x;\mathbf{c}_{0:m})>0.5\) applies only to _relatively confident predictions_ which are _averaged over iterations_. This is significantly weaker than requiring a lower bound of an expected accuracy of _each sample_ for _every iteration_, which is the case when LFN methods are directly applied to the self-training scenario. Also, due to its dependency on the choice of the confidence thresholds \(\mathbf{c}_{0:m}\), the assumption can hold by controlling \(\mathbf{c}_{0:m}\) at the expense of loosening the upper bound in (6) (e.g., selecting 90th-quantile as in Figure 3(b) in Appendix). Specifically, increasing the thresholds can improve \(\xi(\tilde{p}(x;\mathbf{c}_{0:m}))\) and enhance the chance of satisfying \(\tilde{p}(x;\mathbf{c}_{0:m})>1/2\) but reducing \(Q(x;\mathbf{c}_{0:m})\). While this trade-off necessitates a proper choice of \(\mathbf{c}_{0:m}\), our extensive experiments show that setting the threshold \(c_{m}\) by the EMA of prediction confidence, i.e., \(\delta_{m}^{(\beta)}\) in (3), works effectively.

### Theoretical insights from knowledge distillation

In this section, we present a novel connection between AnCon and KD for addressing intractability of (4). KD is a framework for training a small student network \(f\), e.g., ResNet-50 [29], with an additional supervision from a large teacher network \(f^{(t)}\), e.g., ResNet-152. Specifically, a cross-entropy under KD is \(l_{KD}(\theta)=\mathbb{E}_{XY}\left[H(f(X;\theta),(1-\lambda_{KD})E_{1}(Y(X))+ \lambda_{KD}f^{(t)}(X))\right]\) with \(\lambda_{KD}\in[0,1]\), which bears a significant similarity with AnCon (cf. (1)). Indeed, AnCon can be understood as a special case of KD called self-distillation when \(f\) and \(f^{(t)}\) have the same architecture, where the generalized temporal ensemble \(\bar{f}\) corresponds to the teacher network \(f^{(t)}\) with a notable difference that the pseudo label \(\hat{Y}\) is used instead of the true label \(Y\). Based on this connection, we perform a convergence analysis of AnCon by modifying the partial variance reduction theory [30], as given below. We note that the usage of \(\bar{Y}\) and \(\bar{f}\) results in an inherently biased gradient estimator, which requires special treatments for the convergence analysis unlike the typical self-distillation setting.

**Setup** Following [30], we assume a linear model \(f_{k}(x;\theta)=\exp(\Theta_{k}^{T}x)/\sum_{i\in[K]}\exp(\Theta_{i}^{T}x)\) with \(\Theta_{i}\in\mathbb{R}^{d}\) for \(i\in[K]\), \(\theta:=\text{Concat}(\Theta_{1},\cdots,\Theta_{K})\in\mathbb{R}^{dK}\), and \(\text{Concat}(\cdot)\) is the concatenation operation. Also, we assume a bounded support for \(X\); that is, \(\parallel x\parallel\leq C\) for all \(x\) where \(p_{X}(x)>0\). Under this setting, we repeat the following steps starting from a given \(\theta_{0}\) (i.e., \(m=0\)):

1. Outer temporal ensemble update: Update \(\mathbf{w}_{0:m}\) by (3) to obtain \(\bar{f}(x;\theta_{0:m},\mathbf{w}_{0:m})\) (cf. (2)).
2. Inner parameter update: With \(\theta_{m,0}:=\theta_{m}\) and \((\theta_{0:m},\mathbf{w}_{0:m})\), solve (1) with stochastic gradient descent \(\theta_{m,t+1}=\theta_{m,t}-\gamma g_{\xi}^{(m,t)}\) for \(t\in\{0,1,\cdots,T-1\}\) and set \(\theta_{m+1}=\theta_{m,T}\).
3. Iteration number update: Set \(m=m+1\) and terminate if \(m=\hat{T}\).

Here, \(\gamma\) is the learning rate, \(\xi\) contains \(b\) random samples from \(p_{X}\), and the stochastic gradient under AnCon is defined as \(g_{\xi}^{(m,t)}:=\nabla_{\theta_{m,t}}\frac{1}{b}\sum_{X_{i}\in\xi}H(f(X_{i}; \theta_{m,t}),\hat{Y}(X_{i};\theta_{0:m},\mathbf{w}_{0:m}))\). For the linear model, we note that \(g_{\xi}^{(m,t)}=\nabla\tilde{l}_{\xi}(\theta_{m,t})-\lambda\hat{g}_{\xi}( \theta_{0:m},\mathbf{w}_{0:m})\) where \(\tilde{l}_{\xi}(\theta_{m,t})=\frac{1}{b}\sum_{X_{i}\in\xi}H(f(X_{i};\theta_{ m,t}),\hat{Y}(X_{i};\theta_{m}))\) and \(\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m}):=\text{Concat}(\hat{g}_{1,\xi}( \theta_{0:m},\mathbf{w}_{0:m}),\cdots,\hat{g}_{K,\xi}(\theta_{0:m},\mathbf{w} _{0:m}))\) with \(\hat{g}_{k,\xi}(\theta_{0:m},\mathbf{w}_{0:m}):=\frac{1}{b}\sum_{X_{i}\in\xi}[ (\tilde{f}_{k}(X_{i};\theta_{0:m},\mathbf{w}_{0:m})-\hat{Y}_{k}(X_{i};\theta_ {m}))X_{i}]\) for \(k\in[K]\).

In Theorem 3.2 which is proven in Appendix B.3, we analyze the convergence of the inner parameter update step under AnCon and vanilla self-training.

**Theorem 3.2**.: _Let us assume \(l(\theta)\) satisfies \(L\)-smoothness, \(\mathcal{L}\)-expected smoothness, and \(\mu\)-Polyak-Lojasiewicz (PL) condition (cf. Assumptions B.1, B.2, and B.3 in Appendix B.1). For \(\gamma\leq\frac{\mu}{4C\cdot L}\), a carefully chosen \(\lambda\) (cf. \(\lambda=\lambda_{m}^{\dagger}:=\frac{\mathbb{E}_{\xi}[(\nabla\tilde{l}_{\xi} (\theta^{*}),\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m}))]}{\mathbb{E}_{\xi}[ \hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})]\|^{2}+\frac{L\gamma}{L^{2}}\| \hat{g}(\theta_{0:m},\mathbf{w}_{0:m})\|^{2}}\) in Lemma B.6 in Appendix B.5), and any realization of \(\theta_{m}\), it holds that_

\[\underbrace{\mathbb{E}[l(\theta_{m,t})-l(\theta^{*})|\theta_{m}]}_{\text{ Optimality gap}}\leq\underbrace{(1-\gamma\mu)^{t}(l(\theta_{m})-l(\theta^{*}))}_{\text{ Improvement over the initial model}}+\underbrace{\frac{8C^{2}}{\mu}g^{\xi}(\theta_{m})}_{\text{ Bias of the pseudo label}}+\underbrace{\frac{2}{\mu}N(\lambda_{m}^{\dagger};\theta_{0:m}, \mathbf{w}_{0:m})}_{\text{Neighborhood size}} \tag{7}\]

_where \(l^{*}:=l(\theta^{*})\) with \(\theta^{*}\in\arg\min_{\theta\in\Theta}l(\theta)\), \(g^{\xi}(\theta):=\mathbb{E}[\mathds{1}(\hat{Y}(X;\theta)\neq Y(X))]\), \(N(\lambda;\theta_{0:m},\mathbf{w}_{0:m})=\lambda^{2}\parallel\hat{g}(\theta_{0 :m},\mathbf{w}_{0:m})\parallel^{2}+\frac{L\gamma}{2}\mathbb{E}_{\xi}\left[ \parallel\nabla\tilde{l}_{\xi}(\theta^{*})-\lambda\hat{g}_{\xi}(\theta_{0:m}, \mathbf{w}_{0:m})\parallel^{2}\right]\), and \(N(\lambda_{m}^{\dagger};\theta_{0:m},\mathbf{w}_{0:m})\leq N(0)\) where \(N(0):=N(0;\theta_{0:m},\mathbf{w}_{0:m})\) for any \((\theta_{0:m},\mathbf{w}_{0:m})\)._

_Further, when \(\mathbf{w}_{0:m}\) is such that \(\mathbb{E}_{\xi}[\langle\nabla\tilde{l}_{\xi}(\theta^{*}),\hat{g}_{\xi}(\theta_ {0:m},\mathbf{w}_{0:m})\rangle]\geq 0\), i.e., the teacher has a sufficiently good performance, it holds that_

\[\frac{N(\lambda_{m}^{\dagger};\theta_{0:m},\mathbf{w}_{0:m})}{N(0)}\leq\min \left(1,2\min\left(1,\frac{C^{2}g^{KL}(\theta_{0:m},\mathbf{w}_{0:m})}{\sigma_{ \ast}\sigma(\theta_{0:m},\mathbf{w}_{0:m})}\right)+\frac{2C^{2}g^{C}(\theta_ {0:m},\mathbf{w}_{0:m})}{L\gamma\sigma_{(\theta_{0:m},\mathbf{w}_{0:m})}^{2}}\right) \tag{8}\]_where \(\sigma_{*}^{2}:=\mathbb{E}_{\xi}\parallel\nabla\tilde{l}_{\xi}(\theta^{*})\parallel^ {2}\), \(\sigma_{(\theta_{0:m},\mathbf{w}_{0:m})}^{2}:=\mathbb{E}_{\xi}\parallel\hat{g}_{ \xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2}\), \(g^{KL}(\theta_{0:m},\mathbf{w}_{0:m})=\mathbb{E}_{X}[D_{KL}(f(X;\theta^{*}) \parallel\bar{f}(X;\theta_{0:m},\mathbf{w}_{0:m}))]\) with \(D_{KL}(p\parallel q)\) is the KL-divergence between \(p\) and \(q\), and \(g^{C}(\theta_{0:m},\mathbf{w}_{0:m})=\parallel\mathbb{E}_{X}[\bar{f}(X;\theta_ {0:m},\mathbf{w}_{0:m})]-\mathbb{E}_{X}[\bar{Y}(X;\theta_{m})]\parallel^{2}\)._

In Theorem 3.2, (7) characterizes the optimality gap in the inner loop optimization. Specifically, the first term is about reducing the initial optimality gap over iterations and motivates why we need _adaptation_, e.g., by self-training, if the performance deteriorates under severe distribution shifts. The second term is about the bias of the pseudo label and motivates the challenges of self-training under poorly performing pseudo labels in the case of severe distribution shifts. Crucially, these two terms can be fully characterized by the quality of initial model \(\theta_{m}\) and do not depend on \(\mathbf{w}_{0:m}\). Thus, we concentrate on the impacts of \(\mathbf{w}_{0:m}\) designed in (1)-(3) on \(N(\lambda_{m}^{\dagger};\theta_{0:m},\mathbf{w}_{0:m})\) to show that AnCon's effectiveness on improving self-training performance under distribution shifts.

First, Theorem 3.2 shows the effectiveness of our label smoothing formulation in (1) under properly chosen \(\lambda_{m}^{\dagger}\). Specifically, compared to vanilla self-training, AnCon results in the smaller neighborhood size of the stochastic gradient descent; \(N(\lambda_{m}^{\dagger};\theta_{0:m},\mathbf{w}_{0:m})\leq N(0)\). That is, the result suggests that AnCon is at least better than vanilla self-training under the mild regularity conditions.

Further, under the additional assumption of a sufficiently good performance temporal ensemble, (8) motivates AnCon's weighting mechanism as a relaxed solution of the intractable optimization problem in (4). Specifically, if the marginal distribution of the pseudo labels does not change quickly over outer iterations (which is the case especially for the later training stages as shown in Figure 5 in Appendix), changing \(\mathbf{w}_{0:m}\) would have only a marginal impact on \(g^{C}(\theta_{0:m},\mathbf{w}_{0:m})\). Thus, the weighting mechanism that minimizes \(g^{KL}(\theta_{0:m},\mathbf{w}_{0:m})\) would minimize the worst-case optimality gap, which justifies our approach of circumventing intractability of (4) with (5). In this regard, AnCon's weighting mechanism in (3) could be thought of as a relaxed solution of (4) as it minimizes \(g^{KL}(\theta_{0:m},\mathbf{w}_{0:m})\) in the asymptotic region (cf. Theorem 3.1).

We conclude this section by analyzing the three iterative steps where the pseudo labels and the temporal ensembles keep updating.

**Corollary 3.2.1**.: _Let us assume \(l(\theta)\) satisfies \(L\)-smoothness, \(\mathcal{L}\)-expected smoothness, and \(\mu\)-PL condition. For \(\gamma\leq\frac{\mu}{4\mathcal{L}\mathcal{L}}\), \(\lambda\) carefully adjusted at each outer temporal ensemble update (cf. \(\lambda=\lambda_{j}^{\dagger}\) in Lemma B.6 for each outer iteration \(j\in\{0,\cdots,\hat{T}-1\}\)), and any initial parameter \(\theta_{0}\), it holds that_

\[\mathbb{E}[l(\theta_{\hat{T}})-l^{*}]\leq(1-\mu\gamma)^{T\cdot(\hat{T}-1)}(l( \theta_{0})-l^{*})+\zeta_{\hat{T}}\mathbb{E}_{j\sim I^{\hat{T}}[\frac{1}{\mu} }g^{\mathcal{E}}(\theta_{j})+\tfrac{2}{\mu}N(\lambda_{j}^{\dagger};\theta_{ 0:j},\mathbf{w}_{0:j})] \tag{9}\]

_where \(p(I^{(\hat{T})}=j)\propto(1-\mu\gamma)^{T\cdot(\hat{T}-1-j)}\) for \(j\in\{0,\cdots,\hat{T}-1\}\) and \(\zeta_{\hat{T}}=\sum_{i=0}^{\hat{T}-1}(1-\mu\gamma)^{T\cdot i}\)._

Corollary 3.2.1 is proved in Appendix B.4 and gives a whole picture of the optimality gap under AnCon. We first remark the trade-off associated with \(\hat{T}\) on the suboptimality \(\mathbb{E}[l(\theta_{\hat{T}})-l^{*}]\), which characterize the early-learning phenomenon observed in the biased gradient settings (e.g., self-training [31] and LFN [32]). Specifically, in (9), increasing \(\hat{T}\) reduces the first term \((1-\mu\gamma)^{T\cdot(\hat{T}-1)}(l(\theta_{0})-l^{*})\) but increases the coefficient of the second term \(\zeta_{\hat{T}}\). Therefore, a longer training with a large \(\hat{T}\) may not enhance the self-training performance especially under a large second term due to inaccurate pseudo labels or the temporal ensemble.

Nevertheless, for each outer loop iteration, \(g^{\mathcal{E}}(\theta_{j})\) would be smaller under AnCon than its value under vanilla self-training as \(\mathbb{E}[l(\theta_{j})-l^{*}]\) has a tighter upper bound under AnCon due to Theorem 3.2. Therefore, with the guarantee \(N(\lambda_{j}^{\dagger};\theta_{0:j},\mathbf{w}_{0:j})\leq N(0)\), AnCon would achieve a tighter upper bound of (9) than the vanilla self-training method, enabling longer training with smaller value of the second term as observed in Figure 0(b). Finally, we remark that this theoretical superiority of AnCon can be extended to the self-training methods with other weighting mechanisms in the asymptotic region when \(\mathbb{E}_{\xi}[(\nabla\tilde{l}_{\xi}(\theta^{*}),\hat{g}_{\xi}(\theta_{0:m },\mathbf{w}_{0:m}))]\geq 0\) (cf. Theorem 3.2).

## 4 Experiments

**Goal and baselines** Part of the experiments shows that AnCon surpasses the vanilla self-training method that uses \(\hat{Y}(x;\theta)\) as a pseudo label, which serves as a strong baseline, under different types

[MISSING_PAGE_FAIL:7]

Notably, AnCon significantly improves performances of both GCE and NRC via fundamentally different mechanisms for handling noisy pseudo labels (e.g., reducing test accuracy of 3% and 8% on average in OfficeHome, respectively). Specifically, NRC filters incorrect predictions based on local consistency, while AnCon uses temporal consistency. Combining NRC and AnCon leverages pseudo labels that are both locally and temporally consistent, resulting in significant performance improvements over NRC or Self-Training + AnCon (cf. Table 1). In addition, GCE reduces the impact of wrong pseudo labels rather than finding them. Applying GCE to AnCon minimizes the effects of potentially wrong but temporally consistent pseudo labels, which can be implied by the performance of GCE + AnCon compared to GCE or Self-Training + AnCon (cf. Table 1). Thus, the impressive performance gains from AnCon, which would be orthogonal to the gains from state-of-the-art methods in SFDA, show its significant practical implications.

### Self-training under synthetic corruption operations

While we have considered the domain shift, e.g., adaptation of a model trained on synthetic images to real images, in Section 4.1, this section examines the self-training's ability to adapt to distribution shifts by synthetic image corruptions. This setting has been used to measure the robustness of neural networks with respect to a general out-of-distribution setting. To this end, we consider **ImageNet-C**[6], which consists of 50,000 images drawn from a validation set of ImageNet [43] where each image is corrupted by 15 types of synthetic corruptions related to noise, blur, weather and digital.

**Result**: Consistent with the findings under the domain shift, AnCon outperforms the average performances of self-training and ELR under varying levels of corruption intensities (cf. Figure 0(a) and Tables 7-11 in Appendix), improving the self-training method's accuracy by 16% on average. Further, the gains from AnCon is significant when the distribution shifts are intense (e.g., improving accuracies by 20% and 52% on average in intensities of 4 and 5) where the initial model trained on the source domain significantly deteriorates. Specifically, for Shot, Impulse, and Gaussian corruptions with the most extreme shift intensity of 5, where the initial model achieves accuracies of (3.04%, 1.76%, 2.12%), AnCon achieves (22.56%, 26.56%, 25.85%) (cf. Table 11). This striking improvement compared to vanilla self-training with performances (0.26%, 1.72%, 1.04%) and ELR with performances (8.00%, 14.12%, 16.00%), underscores the importance of the AnCon's uncertainty-aware temporal consistency scheme, as shown in Corollary 3.2.1. We note that this impressive result is also explained by AnCon's ability to prevent the gradual performance degradation during the course of training with the extremely noisy pseudo labels (cf. Figure 0(b)). Combined with previous results in domain shift scenarios, we expect that AnCon would work effectively in various out-of-distribution settings.

### Versatility of AnCon

In previous sections, we have shown the universality of AnCon by evaluating it on diverse distribution shift scenarios. In this section, we show versatility of AnCon by analyzing its attractive properties in robustness and uncertainty representation.

#### 4.3.1 Robustness to model selection

There is no universally agreed model selection criterion, such as cross-validation in the i.i.d. setting, in self-training under distribution shifts. This is partly due to the variety of distribution shift scenarios, where an effective criterion in one may be ineffective or inapplicable in another; for instance, a

Figure 1: **Section 4.2: (a) Test accuracy for each intensity level in ImageNet-C. (b) Performance degeneration in the defocus blur corruption with intensity 4. Section 4.3.1: (c) Maximum performance changes under different model selection methods. We present performances for individual corruptions in Appendix. For all boxplots used in the paper, the box represents interquantile range with whiskers as \(\pm\) 1.5 interquantile range and the horizontal line inside the box represents the median.**

principled criteria called importance-weighted cross validation [44] in UDA cannot be applied to SFDA. In this regard, it would be an important characteristic of a self-training method under distribution shift to be robust with respect to different choices of model selection criteria. Therefore, we evaluate robustness with respect to the following different model selection criteria: InfoMax [35], Corr-C [36], and Ent [45] (see Appendix D.3 for the description).

Figure 0(c) shows that AnCon's maximum performance change due to different model selection methods is much lower than that of other methods, especially under severe distribution shifts. This valuable advantage can be contributed to the property of AnCon that can prevent performance degeneration (cf. Figure 0(b)). Given that, in practice, we barely know when the model collapse happens and which model selection criteria are the best, the results highlight a significant practical value of AnCon.

#### 4.3.2 Robustness to the choice of hyperparameters

Throughout this paper, we have shown that our single configuration of parameters (\(\lambda=0.3,\beta=0.9\)) work well across a wide range of benchmark problems. In this section, we aim to show our findings can be preserved when the hyperparameter values deviate from the default setting by performing a sensitivity analysis for values \(\lambda\in\{0.1,0.3,0.5,0.7,0.9\}\) and \(\beta\in\{0.1,0.3,0.5,0.7,0.9\}\). We also test two frequently used annealing schedules that \(\lambda_{m}=m/I\) and \(\lambda_{m}=\min(1,2m/I)\), called full and half, respectively. Figure 2 shows that AnCon is stable even under extreme values of hyperparameters. Specifically, for both hyperparameters, the maximum average performance change is less than 1%, and \(\beta\) barely impacts the performance of AnCon. Indeed, our analysis suggests to increase \(\lambda\) from our default setting; that is, to put a higher weight on the general temporal ensemble's prediction. Here, we note that our suboptimal choices of hyperparameters are due to our rigorous and practical hyperparameter choice. Given the challenging nature of hyperparameter optimization under distribution shifts, the stable performances of AnCon under arbitrary choices of hyperparameters would enable AnCon to be seamlessly applied to diverse practical settings.

#### 4.3.3 Improved calibration performance

We have shown that all self-training methods significantly improve the performance of the baseline method after the adaptation period. However, it is widely known that these noticeable improvements come with the price of sacrificing an uncertainty representation ability which is critical in real-world decision-making scenarios [46, 47]. Specifically, the calibration performance, which is the gap between the prediction confidence and accuracy, usually monotonically increases as self-training keeps reducing the uncertainty for all predictions during the course of training. In this regard, we analyze the calibration performance with respect to the expected calibration error (ECE; see Appendix for definition). Here, a lower ECE means a lower gap between confidence and accuracy.

As shown in Figure 2(a) and Table 5 in Appendix, AnCon gives much lower ECE compared to other methods. Considering ELR and GCE both have regularization effects, we conjecture that this phenomenon is due to selective regularization in AnCon that increases prediction confidences of samples only if the past confident predictions are consistent with the current prediction. Especially, in

Figure 3: (a) ECEs under five levels of intensities in ImageNet-C; (b) Accuracy and ECE changes during the course of training in VisDa.

Figure 2: Sensitivity analysis with respect to \(\lambda\) and \(\beta\) on four domain pairs (Ar-Pr, Pr-Cl, Rw-Cl, Rw-Pr) in OfficeHome. Here, green triangles are means.

Figure 2(b) which confirms the accuracy-calibration dilemma in VisDa, AnCon is shown to limit the ECE increases during training compared to all other methods. That is, AnCon helps to significantly reduce the price of the calibration performance we need to pay for improving accuracy, which are both important measures in practice.

### Algorithmic design choices

Recall that we define \(\bar{f}(x;\theta_{0:m},\mathbf{w}_{0:m})=\sum_{i=0}^{m}w_{i}(x)\cdot p(y|x, \theta_{i})\) with our simple design choices: the relative thresholding for weighting scheme \(w_{i}(x)\propto\mathbf{1}(c(x;\theta_{i})>\delta_{i}^{(\beta)})\) and hard prediction for \(p(y|x,\theta_{i})\). In Appendix C.3, we found that our simple design choices are more appropriate for the distribution shift settings than several more sophisticated alternatives, which can be summarized as follows.

* More sophisticated weighting schemes (e.g., Entropy (\(w_{i}(x)\propto\exp{\{-H_{E}[f(x;\theta_{i})]\}}\))) reduce the self-training performance, despite being a more accurate measure of prediction uncertainty. We conjecture that the poor calibration performance of the neural network in self-training under distribution shifts prevents the sophisticated weighting schemes from accurately reflecting the goodness of the prediction.
* Various soft prediction schemes, which can give more information about the non-leading entry values, leads to performance reductions. We conjecture that the continuously increasing confidence in the later stage of self-training would make soft prediction ignore early-stage predictions which may be valuable to memorize.

## 5 Related work

Filtering incorrect pseudo labelsPopular confidence-based thresholding methods [13; 14; 9] fall short under distribution shifts since even high confident predictions can be highly incorrect. Therefore, recent advances in SFDA and TTA utilize higher order information to filter incorrect pseudo labels. For instance, based on the intuition that true labels of adjacent samples would be same, centroids for each predicted class can be maintained in the feature space and then the pseudo label for each input is corrected by the adjacent centroid [16]. The idea of using per-class centroids has been extended to incorporate more general clustering structures [18; 17]. However, the neighborhood structure-based methods are computationally demanding due to storage of memory banks in the feature space and nearest neighbors search. Such computational complexity persists in other approaches, which are based on the consistency of multiple predictions from different augmentations [20] and models trained with different loss functions [19]. Compared to these solutions, AnCon can efficiently estimate correct labels with only limited extra memory overhead of storing past predictions.

Learning from noisy labelsTreating pseudo labels as inherently noisy, techniques from the LFN literature have been integrated to self-training. For instance, the LFN literature has proposed robust loss functions that reduce impacts of random noisy labels [48; 33], and a recent large-scale experimental study shows the applicability of the generalized cross-entropy in the SFDA setting [31]. The effectiveness of ELR on SFDA [8] bears a similar idea because ELR was developed to regularize the neural networks' tendencies to memorize incorrect labels [21]. Despite their effectiveness, by nature, these approaches do not consider important characteristics of the unbounded and instance-dependent noise rates inherent in self-training under distribution shifts, which results in significant suboptimality in both theory and practice. However, by considering the unique characteristics of self-training under distribution shift, AnCon relaxes the conditions required to achieve optimality as well as boosts the self-training performance in diverse scenarios.

## 6 Conclusion

This paper introduces AnCon, which effectively improves self-training performances under diverse distribution shift scenarios by promoting selective temporal consistency based on confident predictions. As a result, AnCon effectively mitigates the detrimental effects of noisy pseudo labels without much computational overhead, unlike the previous methods. We show that AnCon not only advances our theoretical understanding of a generalized notion of temporal consistency in self-training but also can be a practical asset as a simple and effective self-training method with attractive properties. In Appendix C.4, we present limitations and future directions, such as adaptive determination of \(\lambda\), combining local and temporal consistency, and extending the selective temporal consistency in the sequential decision making problems.

## Acknowledgments and Disclosure of Funding

We would like to thank Jihyeon Hyeong, Yuchen Lou, Jiezhong Wu, and anonymous reviewers for their valuable discussions and constructive suggestions during the preparation of this manuscript. We declare that there was no funding received for this work.

## References

* (1) Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In _Conference on Fairness, Accountability and Transparency_, 2018.
* (2) Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do Imagenet classifiers generalize to Imagenet? In _International Conference on Machine Learning_, 2019.
* (3) Neal Jean, Marshall Burke, Michael Xie, W Matthew Davis, David B Lobell, and Stefano Ermon. Combining satellite imagery and machine learning to predict poverty. _Science_, 353(6301):790-794, 2016.
* (4) Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In _European Conference on Computer Vision_, 2018.
* (5) Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. In _Advances in Neural Information Processing Systems_, 2020.
* (6) Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _International Conference on Learning Representations_, 2019.
* (7) Hong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-training for domain adaptation. In _Advances in Neural Information Processing Systems_, 2021.
* (8) Li Yi, Gezheng Xu, Pengcheng Xu, Jiaqi Li, Ruizhi Pu, Charles Ling, Ian McLeod, and Boyu Wang. When source-free domain adaptation meets learning with noisy labels. In _International Conference on Learning Representations_, 2023.
* (9) David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain adaptation. _arXiv preprint arXiv:2106.04732_, 2021.
* (10) Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In _Workshop on Challenges in Representation Learning, ICML_, 2013.
* (11) Eric Arazo, Diego Ortego, Paul Albert, Noel E O'Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In _International Joint Conference on Neural Networks_, 2020.
* (12) Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In _Advances in Neural Information Processing Systems_, 2020.
* (13) Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. In _Advances in Neural Information Processing Systems_, 2021.
* (14) Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, et al. Freematch: Self-adaptive thresholding for semi-supervised learning. _arXiv preprint arXiv:2205.07246_, 2022.
* (15) Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. In _Advances in Neural Information Processing Systems_, 2019.

* [16] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In _International Conference on Machine Learning_, 2020.
* [17] Shiqi Yang, Joost van de Weijer, Luis Herranz, Shangling Jui, et al. Exploiting the intrinsic neighborhood structure for source-free domain adaptation. In _Advances in Neural Information Processing Systems_, 2021.
* [18] Shiqi Yang, Yaxing Wang, Joost Van De Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adaptation. In _IEEE/CVF International Conference on Computer Vision_, 2021.
* [19] Hao-Wei Yeh, Thomas Westfechtel, Jia-Bin Huang, and Tatsuya Harada. Boosting source-free domain adaptation via confidence-based subsets feature alignment. In _International Conference on Pattern Recognition_, 2022.
* [20] Nazmul Karim, Niluthpol Chowdhury Mithun, Abhinav Rajvanshi, Han-pang Chiu, Supun Samarasekera, and Nazanin Rahnavard. C-sfda: A curriculum learning aided self-training framework for efficient source free domain adaptation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023.
* [21] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. In _Advances in Neural Information Processing Systems_, 2020.
* [22] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3): 107-115, 2021.
* [23] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In _International Conference on Machine Learning_, 2017.
* [24] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2016.
* [25] Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing neural networks by penalizing confident output distributions. _arXiv preprint arXiv:1701.06548_, 2017.
* [26] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.
* [27] Yaodong Yu, Stephen Bates, Yi Ma, and Michael Jordan. Robust calibration with multi-domain temperature scaling. In _Advances in Neural Information Processing Systems_, 2022.
* [28] Taejong Joo and Diego Klabjan. IW-GAE: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation. In _International Conference on Machine Learning_, 2024.
* [29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2016.
* [30] Mher Safaryan, Alexandra Peste, and Dan Alistarh. Knowledge distillation performs partial variance reduction. In _Advances in Neural Information Processing Systems_, 2023.
* [31] Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Vincent Gehler, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. If your data distribution shifts, use self-learning. _Transactions on Machine Learning Research_, 2022.
* [32] Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. In _International Conference on Artificial Intelligence and Statistics_, 2020.

* Ghosh et al. [2017] Aritra Ghosh, Himanshu Kumar, and P Shanti Sastry. Robust loss functions under label noise for deep neural networks. In _AAAI Conference on Artificial Intelligence_, 2017.
* Press et al. [2023] Ori Press, Steffen Schneider, Matthias Kummerer, and Matthias Bethge. Rdumb: A simple approach that questions our progress in continual test-time adaptation. In _Advances in Neural Information Processing Systems_, 2023.
* Shi and Sha [2012] Yuan Shi and Fei Sha. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. In _International Coference on International Conference on Machine Learning_, 2012.
* Tu et al. [2023] Weijie Tu, Weijian Deng, Tom Gedeon, and Liang Zheng. Assessing model out-of-distribution generalization with softmax prediction probability baselines and a correlation method, 2023. URL [https://openreview.net/forum?id=1maKoEyeqx](https://openreview.net/forum?id=1maKoEyeqx).
* Hu et al. [2023] Dapeng Hu, Jian Liang, Jun Hao Liew, Chuhui Xue, Song Bai, and Xinchao Wang. Mixed samples as probes for unsupervised model selection in domain adaptation. In _Advances in Neural Information Processing Systems_, 2023.
* Ioffe and Szegedy [2015] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International Conference on Machine Learning_, 2015.
* Salimans and Kingma [2016] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In _Advances in Neural Information Processing Systems_, 2016.
* Saenko et al. [2010] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In _European Conference on Computer Vision_, 2010.
* Venkateswara et al. [2017] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2017.
* Peng et al. [2017] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge. _arXiv preprint arXiv:1710.06924_, 2017.
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2009.
* Sugiyama et al. [2007] Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert Muller. Covariate shift adaptation by importance weighted cross validation. _Journal of Machine Learning Research_, 8(5), 2007.
* Morerio et al. [2017] Pietro Morerio, Jacopo Cavazza, and Vittorio Murino. Minimal-entropy correlation alignment for unsupervised deep domain adaptation. _arXiv preprint arXiv:1711.10288_, 2017.
* Amodei et al. [2016] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Concrete problems in AI safety. _arXiv preprint arXiv:1606.06565_, 2016.
* Lee and See [2004] John D Lee and Katrina A See. Trust in automation: Designing for appropriate reliance. _Human Factors_, 46(1):50-80, 2004.
* Liu and Guo [2020] Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing noise rates. In _International Conference on Machine Learning_, 2020.
* Teodorovich Polyak [1963] Boris Teodorovich Polyak. Gradient methods for minimizing functionals. _Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki_, 3(4):643-653, 1963.
* Charles and Papailiopoulos [2018] Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms that converge to global optima. In _International Conference on Machine Learning_, 2018.
* Jacot et al. [2018] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In _Advances in Neural Information Processing Systems_, 2018.

* [52] Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. _The Annals of Mathematical Statistics_, pages 493-507, 1952.
* [53] Taejong Joo, Uijung Chung, and Min-Gwan Seo. Being Bayesian about categorical probability. In _International Conference on Machine Learning_, 2020.
* [54] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In _International Conference on Machine Learning_, 2016.
* [55] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael Jordan. Conditional adversarial domain adaptation. In _Advances in Neural Information Processing Systems_, 2018.
* [56] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In _International Conference on Machine Learning_, 2019.

List of acronyms

\begin{tabular}{l l} \hline \hline TTA & Test time adaptation \\ SFDA & Source-free domain adaptation \\ i.i.d. & independent and identically distributed \\ ELR & Early learning regularization \\ LFN & Learning from noisy labels \\ KD & Knowledge distillation \\ EMA & Exponential moving average \\ GCE & Generalized cross-entropy \\ NRC & Neighborhood reciprocity clustering \\ PL & Polyak-Lojasiewicz \\ \hline \hline \end{tabular}

## Appendix B Proof of claims

### Assumptions

Let us recall that \(l(\theta)=\mathbb{E}_{XY}[H(f(X;\theta),p_{Y|X})]\) and \(l_{\xi}(\theta)=\frac{1}{b}\sum_{X_{i}\in\xi}H(f(X_{i};\theta),p_{Y|X_{i}})\) where \(\xi\) contains \(b\) number of random samples from \(p_{X}\). We also denote \(\theta^{*}\in\arg\min_{\theta\in\mathbb{R}^{p}}l(\theta)\). In this work, we assume the following three regularity conditions on \(l(\theta)\) which is differentiable with respect to \(\theta\), which are mild but essential for most theoretical studies with convergence analyses.

**Assumption B.1** (\(L\)-smoothness).: \(l(\cdot)\) _is \(L\)-smooth for some constant \(L>0\); that is,_

\[l(\theta^{\prime})\leq l(\theta)+\langle\nabla l(\theta),\theta^{\prime}- \theta\rangle+\frac{L}{2}\parallel\theta^{\prime}-\theta\parallel^{2},\quad \forall\theta\in\mathbb{R}^{p}\text{ and }\theta^{\prime}\in\mathbb{R}^{p}. \tag{10}\]

**Assumption B.2** (\(\mathcal{L}\)-expected smoothness [30]).: \(l(\cdot)\) _is \(\mathcal{L}\)-smooth in expectation with respect to \(\xi\sim\mathcal{D}\); that is,_

\[\mathbb{E}_{\xi\sim\mathcal{D}}[\parallel\nabla l_{\xi}(\theta)-\nabla l_{\xi} (\theta^{*})\parallel^{2}]\leq 2\mathcal{L}(l(\theta)-l(\theta^{*}))),\quad \forall\theta\in\mathbb{R}^{p}. \tag{11}\]

**Assumption B.3** (\(\mu\)-Polyak-Lojasiewicz (PL) condition [49]).: \(l(\cdot)\) _satisfies the \(\mu\)-PL condition for some constant \(\mu>0\); that is,_

\[\parallel\nabla l(\theta)\parallel^{2}\geq 2\mu(l(\theta)-l(\theta^{*})),\quad \forall\theta\in\mathbb{R}^{p}. \tag{12}\]

We remark that Assumptions B.1 and B.2 can trivially hold under bounded parameter values that can be guaranteed by optimizing neural networks with finite iterations under a gradient or weight clipping. Assumption B.3 holds for infinite-width neural networks, i.e., the neural tangent kernel (NTK) regime [50]. Given that the gradient descent training dynamics of neural networks can be well approximated by NTK [51], the PL condition can be generally regarded as a mild assumption.

### Proof of Theorem 3.1

Proof.: For \(x\) such that \(Q(x;\mathbf{c}_{0:m})>0\), let us consider subsequence of index \(l\) such that \(x\in A_{l}(c_{l})\); that is, \((j_{1},\cdots,j_{Q(x;\mathbf{c}_{0:m})})\) where \(j_{l}=\min\{l\geq j_{l-1}|x\in A_{l}(c_{l})\}\) and \(j_{0}=0\). Let \(S_{Q(x;\mathbf{c}_{0:m})}=\sum_{i=1}^{Q(x;\mathbf{c}_{0:m})}\mathbf{1}(Y(x)= \hat{Y}(x;\theta_{j_{i}}))\). Then, we have the following inequality

\[p\left(\operatorname*{arg\,max}_{k}\bar{f}_{k}(x;\theta_{0:m}, \mathbf{w}_{0:m})\neq Y(x)\right)\leq p\left(S_{Q(x;\mathbf{c}_{0:m})}\leq \frac{Q(x;\mathbf{c}_{0:m})}{2}\right)\\ \leq\exp\left(-\frac{Q(x;\mathbf{c}_{0:m})}{2}\cdot(2\bar{p}(x; \mathbf{c}_{0:m})-1-\log(2\bar{p}(x;\mathbf{c}_{0:m})))\right) \tag{13}\]

where the first inequality holds because \(\operatorname*{arg\,max}_{k}\bar{f}_{k}(x;\theta_{0:m},\mathbf{w}_{0:m})=Y(x)\) if \(S_{Q(x;\mathbf{c}_{0:m})}>Q(x;\mathbf{c}_{0:m})/2\) and the second inequality holds due to Lemma B.4 given later in Appendix B.5 with \(p_{i}=p(Y(x)=\hat{Y}(x;\theta_{i}))\), \(o=Q(x;\mathbf{c}_{0:m})\), and \(q=1/2\).

### Proof of Theorem 3.2

Proof.: By Lemma B.5, any realization of \(\theta_{m}\) satisfies

\[\mathbb{E}[l(\theta_{m,t})-l(\theta^{*})|\theta_{m}]\leq(1-\gamma \mu)^{t}(l(\theta_{m})-l(\theta^{*}))+\frac{8C^{2}}{\mu}g^{\mathcal{E}}(\theta_{ m})\\ +\frac{2}{\mu}\left(\lambda^{2}\parallel\hat{g}(\theta_{0:m}, \mathbf{w}_{0:m})\parallel^{2}+\frac{L\gamma}{2}\mathbb{E}_{\xi}\left[\parallel \nabla\tilde{l}_{\xi}(\theta^{*})-\lambda\hat{g}_{\xi}(\theta_{0:m},\mathbf{w} _{0:m})\parallel^{2}\right]\right). \tag{14}\]

Then, from the definition \(N(\lambda;\theta_{0:m},\mathbf{w}_{0:m})=\lambda^{2}\parallel\hat{g}(\theta_{0 :m},\mathbf{w}_{0:m})\parallel^{2}+\frac{L\gamma}{2}\mathbb{E}_{\xi}\left[ \parallel\nabla\tilde{l}_{\xi}(\theta^{*})-\lambda\hat{g}_{\xi}(\theta_{0:m}, \mathbf{w}_{0:m})\parallel^{2}\right]\), we get (7). To show \(N(\lambda_{m}^{\dagger};\theta_{0:m},\mathbf{w}_{0:m})\leq N(0)\), we apply Lemma B.6 as

\[\frac{N(\lambda_{m}^{\dagger};\theta_{0:m},\mathbf{w}_{0:m})}{N(0)}=1-\frac{ \left(\mathbb{E}_{\xi}[\langle\nabla\tilde{l}_{\xi}(\theta^{*}),\hat{g}_{\xi}( \theta_{0:m},\mathbf{w}_{0:m})\rangle]\right)^{2}}{\left(\mathbb{E}_{\xi} \parallel\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2}+\frac{2}{L \gamma}\parallel\hat{g}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2}\right) \left(\mathbb{E}_{\xi}\parallel\nabla\tilde{l}_{\xi}(\theta^{*})\parallel^{2} \right)}. \tag{15}\]

To show (8), let us assume \(\mathbb{E}_{\xi}[\langle\nabla\tilde{l}_{\xi}(\theta^{*}),\hat{g}_{\xi}(\theta _{0:m},\mathbf{w}_{0:m})\rangle]\geq 0\). Then, we get the following inequality:

\[\frac{\mathbb{E}_{\xi}[\langle\nabla\tilde{l}_{\xi}(\theta^{*}), \hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\rangle]}{(\mathbb{E}_{\xi} \parallel\nabla\tilde{l}_{\xi}(\theta^{*})\parallel^{2})^{1/2}\cdot(\mathbb{E }_{\xi}\parallel\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2})^{1/2}} \tag{16}\] \[=\frac{\mathbb{E}_{\xi}\parallel\nabla\tilde{l}_{\xi}(\theta^{*}) \parallel^{2}+\mathbb{E}_{\xi}\parallel\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0 :m})\parallel^{2}-\mathbb{E}_{\xi}\parallel\nabla\tilde{l}_{\xi}(\theta^{*})- \hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2}}{2(\mathbb{E}_{\xi} \parallel\nabla\tilde{l}_{\xi}(\theta^{*})\parallel^{2})^{1/2}\cdot(\mathbb{E }_{\xi}\parallel\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2})^{1/2}}\] (17) \[\geq 1-\min\left(1,\frac{\mathbb{E}_{\xi}\parallel\nabla\tilde{l }_{\xi}(\theta^{*})-\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2}} {2(\mathbb{E}_{\xi}\parallel\nabla\tilde{l}_{\xi}(\theta^{*})\parallel^{2})^{1 /2}\cdot(\mathbb{E}_{\xi}\parallel\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m}) \parallel^{2})^{1/2}}\right)\] (18) \[\geq 1-\min\left(1,\frac{C^{2}\mathbb{E}_{X}D_{KL}(f(X;\theta^{*} )\parallel\bar{f}(X;\theta_{0:m},\mathbf{w}_{0:m}))}{(\mathbb{E}_{\xi} \parallel\nabla\tilde{l}_{\xi}(\theta^{*})\parallel^{2})^{1/2}\cdot(\mathbb{E }_{\xi}\parallel\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2})^{1/2 }}\right) \tag{19}\]

where (18) holds due to \(a^{2}+b^{2}\geq 2ab\); (19) holds due to the assumption \(\parallel x\parallel\leq C\), \(\parallel\cdot\parallel_{2}\leq\parallel\cdot\parallel_{1}\), and the Pinsker's inequality applied as

\[\mathbb{E}_{\xi}\parallel\nabla\tilde{l}_{\xi}(\theta^{*})-\hat{g }_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2}\leq C^{2}\mathbb{E}_{\xi} \frac{1}{b}\sum_{X_{i}\in\xi}\parallel f(X_{i};\theta^{*})-\bar{f}(X_{i}; \theta_{0:m},\mathbf{w}_{0:m})\parallel^{2}\\ \leq C^{2}\mathbb{E}_{\xi}\frac{1}{b}\sum_{X_{i}\in\xi}\parallel f (X_{i};\theta^{*})-\bar{f}(X_{i};\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2} _{1}\\ \leq 2C^{2}\mathbb{E}_{X}D_{KL}(f(X;\theta^{*}),\bar{f}(X;\theta_{0 :m},\mathbf{w}_{0:m})). \tag{20}\]

Thus, we get our desired result by combining the following inequality with (15):

\[\frac{N(\lambda_{m}^{\dagger};\theta_{0:m},\mathbf{w}_{0:m})}{N(0)}=1-\frac{ \left(\mathbb{E}_{\xi}[\langle\nabla\tilde{l}_{\xi}(\theta^{*}),\hat{g}_{\xi}( \theta_{0:m},\mathbf{w}_{0:m})\rangle]\right)^{2}}{\left(\mathbb{E}_{\xi} \parallel\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2}+\frac{2}{L \gamma}\parallel\hat{g}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2}\right)\left( \mathbb{E}_{\xi}\parallel\nabla\tilde{l}_{\xi}(\theta^{*})\parallel^{2} \right)} \tag{21}\]

\[=1-\left(\frac{\left(\mathbb{E}_{\xi}[\langle\nabla\tilde{l}_{\xi}(\theta^{*}), \hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\rangle]\right)^{2}}{(\mathbb{E}_{ \xi}\parallel\nabla\tilde{l}_{\xi}(\theta^{*})\parallel^{2})^{1/2}\cdot(\mathbb{E }_{\xi}\parallel\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2})^{1/2 }}\right)^{2}\frac{1}{1+\frac{2}{L\gamma}\frac{\parallel\hat{g}(\theta_{0:m}, \mathbf{w}_{0:m})\parallel^{2}}{\mathbb{E}_{\xi}\parallel\hat{g}_{\xi}(\theta_{0 :m},\mathbf{w}_{0:m})\parallel^{2}}} \tag{22}\]\[\leq 1-\left(1-\min\left(1,\frac{C^{2}\mathbb{E}_{X}D_{KL}(f(X;\theta^{*}) \parallel\bar{f}(X;\theta_{0:m},\mathbf{w}_{0:m}))}{(\mathbb{E}_{\xi}\parallel \nabla\bar{l}_{\xi}(\theta^{*})\parallel^{2})^{1/2}\cdot(\mathbb{E}_{\xi} \parallel\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2})^{1/2}} \right)\right)^{2}\frac{1}{1+\frac{2}{L\gamma}\frac{\parallel\hat{g}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2}}{\mathbb{E}_{\xi}\parallel\hat{g}_{\xi}(\theta_ {0:m},\mathbf{w}_{0:m})\parallel^{2}}} \tag{23}\]

\[\leq 2\min\left(1,\frac{C^{2}\mathbb{E}_{X}D_{KL}(f(X;\theta^{*})\parallel \bar{f}(X;\theta_{0:m},\mathbf{w}_{0:m}))}{(\mathbb{E}_{\xi}\parallel\nabla \bar{l}_{\xi}(\theta^{*})\parallel^{2})^{1/2}\cdot(\mathbb{E}_{\xi}\parallel \hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2})^{1/2}}\right)+\frac {2}{L\gamma}\frac{\parallel\hat{g}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2} }{\mathbb{E}_{\xi}\parallel\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m}) \parallel^{2}} \tag{24}\]

\[\leq 2\min\left(1,\frac{C^{2}\mathbb{E}_{X}D_{KL}(f(X;\theta^{*}) \parallel\bar{f}(X;\theta_{0:m},\mathbf{w}_{0:m}))}{(\mathbb{E}_{\xi}\parallel \nabla\bar{l}_{\xi}(\theta^{*})\parallel^{2})^{1/2}\cdot(\mathbb{E}_{\xi} \parallel\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2})^{1/2}}\right) \tag{25}\]

\[+\frac{2C^{2}\parallel\mathbb{E}_{X}[\bar{f}(X;\theta_{0:m},\mathbf{w}_{0:m})] -\mathbb{E}_{X}[\hat{Y}(X;\theta_{m})]\parallel^{2}}{\mathbb{E}_{\xi}\parallel \hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2}} \tag{26}\]

where (24) holds due to \(1-\frac{(1-u)^{2}}{1+v}\leq 2u+v\)[30]. 

### Proof of Corollary 3.2.1

Proof.: For any realization of \(\theta_{m}\) for \(m\in\{0,1,\cdots,\hat{T}-1\}\), applying Theorem 3.2 with \(\lambda=\lambda_{m}^{\dagger}\) (cf. Lemma B.6) gives

\[\mathbb{E}[l(\theta_{m+1})-l^{*}|\theta_{m}]=\mathbb{E}[l(\theta_{ m,T})-l^{*}|\theta_{m}]\\ \leq(1-\mu\gamma)^{T}(l(\theta_{m})-l^{*})+\frac{8C^{2}}{\mu}g^{ \mathcal{E}}(\theta_{m})+\frac{2}{\mu}N(\lambda_{m}^{\dagger};\theta_{0:m}, \mathbf{w}_{0:m}). \tag{27}\]

Thus, by recursively applying (27) to \(\theta_{0}\), we get our desired result as

\[\mathbb{E}[l(\theta_{\hat{T}})-l^{*}]\leq(1-\mu\gamma)^{T\cdot(\hat{T}-1)}(l( \theta_{0})-l^{*})+\sum_{i=0}^{\hat{T}-1}(1-\mu\gamma)^{T\cdot(\hat{T}-1-i)} \left(\frac{8C^{2}}{\mu}g^{\mathcal{E}}(\theta_{i})+\frac{2}{\mu}N(\lambda_{i} ^{\dagger};\theta_{0:i},\mathbf{w}_{0:i})\right). \tag{28}\]

### Supporting lemmas

**Lemma B.4**.: _Let \(A_{i}\sim Bern(p_{i})\) with \(p_{i}\in[0,1]\) and \(S_{o}=\sum_{i\in[o]}A_{i}\), where \(A_{i}\) and \(A_{j}\) are independent for \(i\neq j\). Then for any \(q<\bar{p}:=\frac{1}{o}\sum_{i\in[o]}p_{i}\), we have the tail probability bound_

\[p(S_{o}\leq q\cdot o)\leq\exp\left(o\left(q-\bar{p}-q\log\frac{q}{\bar{p}} \right)\right). \tag{29}\]

Proof.: For any \(t\), the moment generating function of \(S_{m}\) is given by

\[M(t)=\mathbb{E}_{S_{o}}\left[\exp(tS_{o})\right]=\prod_{i\in[o] }\mathbb{E}_{A_{i}}\left[\exp(tA_{i})\right]=\exp\left(\sum_{i\in[o]}\log( \mathbb{E}_{A_{i}}[\exp(tA_{i})])\right)\\ =\exp\left(\sum_{i\in[o]}\log(p_{i}(\exp(t)-1)+1)\right)\leq\exp \left(\sum_{i\in[o]}p_{i}(\exp(t)-1)\right)=\exp(o\bar{p}(\exp(t)-1)) \tag{30}\]

where the first equality comes from the independence and the inequality is from \(\log x\leq x-1\).

Then, for \(t<0\) and \(q<\bar{p}\), the Chernoff bound [52] gives

\[p(S_{o}\leq q\cdot o)\leq M(t)\exp(-t\cdot q\cdot o)=\exp(o\bar{p}(\exp(t)-1)-t \cdot q\cdot o). \tag{31}\]

Finally, setting \(t=\log\left(q/\bar{p}\right)<0\) gives our desired result.

**Lemma B.5** (Modification from [30]).: _Let us assume that \(l(\theta)\) satisfies the \(L\)-smoothness, the \(\mathcal{L}\)-expected smoothness, and the \(\mu\)-PL condition. Also, we assume a linear model \(f_{k}(x;\theta)=\{\exp(\theta^{T}x)\}_{k}/\sum_{i\in[K]}\{\exp(\theta^{T}x)\}_{i}\) with \(\theta\in\mathbb{R}^{d\times K}\). Then, for the stochastic gradient descent \(\theta_{m,t+1}=\theta_{m,t}-\gamma g_{\xi}^{(m,t)}\) with \(g_{\xi}^{(m,t)}=\nabla\hat{\mathbb{E}}_{X_{i}\in\xi}[H(f(X_{i};\theta_{m,t}), \check{Y}(X_{i};\theta_{0:m},\mathbf{w}_{0:m})]\), \(\theta_{m,0}:=\theta_{m}\), and a constant learning rate \(\gamma\leq\frac{\mu}{4L\cdot L}\), any realization of \(\theta_{m}\) satisfies_

\[\mathbb{E}[l(\theta_{m,t})-l(\theta^{*})|\theta_{m}]\leq(1-\gamma \mu)^{t}(l(\theta_{m})-l(\theta^{*}))+\frac{8C^{2}}{\mu}g^{\xi}(\theta_{m})\\ +\frac{1}{\mu}\left(2\lambda^{2}\parallel\hat{g}(\theta_{0:m}, \mathbf{w}_{0:m})\parallel^{2}+L\gamma\mathbb{E}_{\xi}\left[\parallel\nabla \tilde{l}_{\xi}(\theta^{*})-\lambda\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m })\parallel^{2}\right]\right). \tag{32}\]

Proof.: For any realization of \(\theta_{m,t}\), we have the following bound:

\[\mathbb{E}_{\xi}[l(\theta_{m,t+1})-l(\theta^{*})|\theta_{m,t}]\leq (l(\theta_{m,t})-l(\theta^{*}))-\gamma\langle\nabla l(\theta_{m,t}),g^{(m,t)} \rangle+\frac{L\gamma^{2}}{2}\mathbb{E}_{\xi}\left[\parallel g_{\xi}^{(m,t)} \parallel^{2}\right] \tag{33}\] \[=(l(\theta_{m,t})-l(\theta^{*}))-\gamma\langle\nabla l(\theta_{m,t }),\nabla l(\theta_{m,t})-b^{(m,t)}\rangle+\frac{L\gamma^{2}}{2}\mathbb{E}_{ \xi}\left[\parallel\nabla\tilde{l}_{\xi}(\theta_{m,t})-\lambda\hat{g}_{\xi}( \theta_{0:m},\mathbf{w}_{0:m})\parallel^{2}\right]\] (34) \[=(l(\theta_{m,t})-l(\theta^{*}))-\gamma\parallel\nabla l(\theta_{ m,t})\parallel^{2}+\gamma\langle\nabla l(\theta_{m,t}),b^{(m,t)}\rangle\] \[\qquad+\frac{L\gamma^{2}}{2}\mathbb{E}_{\xi}\left[\parallel\nabla \tilde{l}_{\xi}(\theta_{m,t})-\lambda\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0: m})\parallel^{2}\right]\] (35) \[\leq(1-\gamma\mu)(l(\theta_{m,t})-l(\theta^{*}))-\frac{\gamma\mu} {2}(l(\theta_{m,t})-l(\theta^{*}))+\gamma\parallel b^{(m,t)}\parallel^{2}\] \[\qquad+\frac{L\gamma^{2}}{2}\mathbb{E}_{\xi}\left[\parallel\nabla \tilde{l}_{\xi}(\theta_{m,t})-\lambda\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0 :m})\parallel^{2}\right]\] (36) \[\leq(1-\gamma\mu)(l(\theta_{m,t})-l(\theta^{*}))-\frac{\gamma\mu} {2}(l(\theta_{m,t})-l(\theta^{*}))+\gamma\parallel b^{(m,t)}\parallel^{2}\] \[\qquad+L\gamma^{2}\mathbb{E}_{\xi}\left[\parallel\nabla\tilde{l}_ {\xi}(\theta_{m,t})-\nabla\tilde{l}_{\xi}(\theta^{*})\parallel^{2}\right]+L \gamma^{2}\mathbb{E}_{\xi}\left[\parallel\nabla\tilde{l}_{\xi}(\theta^{*})- \lambda\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel^{2}\right]\] (37) \[\leq(1-\gamma\mu)(l(\theta_{m,t})-l(\theta^{*}))-\frac{\gamma\mu} {2}\left(1-\gamma\frac{4L\mathcal{L}}{\mu}\right)(l(\theta_{m,t})-l(\theta^{*} ))+\gamma\parallel b^{(m,t)}\parallel^{2}\] \[\qquad+L\gamma^{2}\mathbb{E}_{\xi}\left[\parallel\nabla\tilde{l}_ {\xi}(\theta^{*})-\lambda\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\parallel ^{2}\right]\] (38) \[\leq(1-\gamma\mu)(l(\theta_{m,t})-l(\theta^{*}))+\gamma\parallel b ^{(m,t)}\parallel^{2}+L\gamma^{2}\mathbb{E}_{\xi}\left[\parallel\nabla\tilde{l }_{\xi}(\theta^{*})-\lambda\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m}) \parallel^{2}\right]\] (39) \[\leq(1-\gamma\mu)(l(\theta_{m,t})-l(\theta^{*}))+2\gamma\parallel \nabla l(\theta_{m,t})-\nabla\tilde{l}(\theta_{m,t})\parallel^{2}\] \[\qquad+2\gamma\lambda^{2}\parallel\hat{g}(\theta_{0:m},\mathbf{w }_{0:m})\parallel^{2}+L\gamma^{2}\mathbb{E}_{\xi}\left[\parallel\nabla\tilde{l }_{\xi}(\theta^{*})-\lambda\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m}) \parallel^{2}\right]\] (40) \[\leq(1-\gamma\mu)(l(\theta_{m,t})-l(\theta^{*}))+8\gamma C^{2}\cdot g ^{\xi}(\theta_{m})\] \[\qquad+2\gamma\lambda^{2}\parallel\hat{g}(\theta_{0:m},\mathbf{w }_{0:m})\parallel^{2}+L\gamma^{2}\mathbb{E}_{\xi}\left[\parallel\nabla\tilde{l }_{\xi}(\theta^{*})-\lambda\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m}) \parallel^{2}\right] \tag{41}\]

where (33) holds due to \(L\)-smoothness; (34) holds due to \(g_{\xi}^{(m,t)}=\nabla l_{\xi}(\theta_{m,t})-b_{\xi}^{(m,t)}=\nabla\tilde{l}_ {\xi}(\theta_{m,t})-\lambda\hat{g}_{\xi}(\theta_{0:m},\mathbf{w}_{0:m})\); (36) holds due to \(\langle a,b\rangle\leq\frac{1}{4}\parallel a\parallel^{2}+\parallel b\parallel ^{2}\); (37) holds due to the inequality \(\parallel x+y\parallel^{2}\leq 2\parallel x\parallel^{2}+2\parallel y \parallel^{2}\); (38) holds due to the expected smoothness assumption; (39) holds due to the condition \(\gamma\leq\frac{1}{4L}\frac{1}{L}\); (41) holds due to the law of total expectation with the random event \(\mathbf{1}(Y(X)=\hat{Y}(X;\theta_{m}))\) and then applying the bounded support assumption.

[MISSING_PAGE_EMPTY:19]

the neural network in self-training under distribution shifts prevents the estimated uncertainty of a prediction from accurately reflecting the goodness of the prediction. Further, the ineffectiveness of these sophisticated weighting schemes persists even when we improve the calibration performance with last-layer Dirichlet [53] and Monte-Carlo Dropout [54] (see Appendix D.3 for detail). Therefore, our relative thresholding is simple yet effective in self-training under distribution shift.

_Soft prediction for \(p(y|x,\theta_{i})\)_ We compare hard prediction vs soft prediction with a softmax temperature parameter \(T\in\{0.5,0.75,1.0,1.25,1.5\}\) (cf. Appendix D.3 for the definition). By design, soft prediction is more informative, giving values in non-leading entries. However, in Figure 5(b), soft prediction turns out to underperform hard prediction for various values of \(T\). We conjecture that the continuously increasing confidence in the later stage of self-training would make soft prediction ignore early-stage predictions which may be valuable to memorize. Thus, we believe that hard prediction would be more appropriate for AnCon.

### Limitations and future directions

Albeit AnCon proving its effectiveness with a fixed value of \(\lambda\), Theorem 3.1 suggests that its optimal value depends on the temporal ensemble performance. Thus, future works could aim for adaptively determining \(\lambda\). In addition, we observe that all self-training based methods fall short without sophisticated model designs tailored for distribution shifts [16; 31] (cf. Appendix E). In this regard, it would be valuable to rigorously understand properties of function classes that enable successful self-training under distribution shifts. Also, while AnCon significantly relaxes the conditions required to achieve optimality compared to LFN techniques, the on-average correct prediction condition still can be violated in challenging self-training scenarios. Given the compatibility of AnCon and NRC

Figure 5: (a) Counting the number of pseudo labels for each class with 5,000 training samples in ImageNet-C over 100 training epochs, which shows that the marginal distribution of pseudo labels barely changes during training. (b) Changes in the total variation distance of the marginal distributions of the pseudo labels for each two consecutive epochs.

Figure 6: Ablation study of (a) weighting and (b) prediction schemes.

(cf. Table 1), efficiently combining local and temporal consistencies could be a step toward further relaxing the optimality condition. Finally, we remark that the idea of selective temporal consistency in sequential decision-making scenarios could be a notable future direction of research. Extending AnCon to sequential decision-making scenarios presents significant challenges as they involve the fundamentally different mechanisms. In sequential decision-making, leveraging observed rewards and balancing exploitation and exploration are core aspects (e.g., constructing the upper confidence bound of the reward in the bandit problem), unlike in self-training. Therefore, the main challenges for applying AnCon to this setting would be defining rewards and incorporating exploration strategies into pseudo label generation.

## Appendix D Additional details

### SFDA training configurations

Our training configuration is based on [16]. Specifically, we train ResNet-50 [29] as a source classifier, which is used as an initial point for SFDA, for 50 epochs, with minibatch stochastic gradient descent with Nesterov momentum by using the initial learning rate of 0.01, the momentum parameter of 0.9, and the minibatch size of 64 in the Office Home dataset. The learning rate is decayed by \((1+10\) \(\cdot\) current iteration number / maximum number of iterations\()^{-0.75}\), and for the pre-trained layers we use a 10 times smaller learning rate. For Office-31, we change the number of epochs by 100 and other configurations are the same as those used in the setting in Office Home. Similarly, we only change the number of epochs to 10, the learning rate to 0.001, and the architecture to ResNet-101 for VisDa. We split dataset into 90% of the training set and 10% of the validation set, and the best model is chosen based on the lowest error rate on the validation set. For adapting the pre-trained model in the target domain, we use the exactly same configurations, except reducing the number of epochs to 30 for Office-31 and OfficeHome and to 15 for VisDa.

### ImageNet-C training configuration

For applying self-training, we follow the training configuration used in [31]; specifically, given ResNet-50 pretrained on ImageNet, we perform self-training without threshold by running the stochastic gradient descent optimizer with the learning rate of 0.001 and the minibatch size of 100 for 20 epochs. We remark that other techniques such as momentum and learning rate scheduling are not used.

### Further experimental details in analyses

#### Definition of ECE

\[ECE(\theta;\mathcal{D})=\sum_{g=1}^{G}\frac{|\mathcal{G}_{g}^{\theta}|}{| \mathcal{D}|}|Acc(\mathcal{G}_{g}^{\theta})-Conf(\mathcal{G}_{g}^{\theta}))| \tag{44}\]

where \(\mathcal{G}_{g}^{\theta}:=\{x\in\mathcal{D}|\frac{g}{G}\leq c(x;\theta)<\frac{ g+1}{G}\}\), \(Acc(\mathcal{G}_{g}^{\theta})\) is the average accuracy in \(\mathcal{G}_{g}^{\theta}\), and \(Conf(\mathcal{G}_{g}^{\theta})\) is the average confidence in \(\mathcal{G}_{g}^{\theta}\).

Configuration of the sophisticated uncertainty representation methodsWe used the regularization coefficient of 0.001 for the belief matching loss [53] and 20 forward passes to compute the posterior predictive distribution for MC-dropout with dropout probabilities 0.2 and 0.5 [54].

Definitions of model selection criteria

* InfoMax (higher is better) \[IM(\theta)=H(\mathbb{E}_{X}[f(X;\theta)])-\mathbb{E}_{X}[H(f(X;\theta))]\] (45) where \(H(p)=-\sum_{k\in[K]}p_{i}\log p_{i}\) for \(p\in\triangle^{K-1}\). Note that we abuse the notation \(H(\cdot)\) with the definition of the cross-entropy; that is, \(H(q,p)=-\sum_{k\in[K]}p_{i}\log q_{i}\) and \(H(p)=-\sum_{k\in[K]}p_{i}\log p_{i}\).

[MISSING_PAGE_FAIL:22]

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_FAIL:24]

## NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In Section 1, we clearly state this work's scope and contributions, which are shown in Theorem 3.1, Theorem 3.2, and extensive experiments in Section 4.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We include "Limitations and future directions" in Section 6 and discuss implications of the assumptions for theoretical results.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have added all assumptions in the statements of Theorems 3.1 and 3.2 with proofs in Appendix.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We include training configurations of all experiments in Appendix D.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We release our code at [https://github.com/tjoo512/ancon](https://github.com/tjoo512/ancon).
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We include training configurations of all experiments in Appendix D.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We have included error bars with box plots for experiments that have high variance across different random seeds.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide computational resources used in the research in the Appendix.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We thoroughly read the NeurIPS Code of Ethics and have confirmed that our research does not violate any of them.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]Justification: This work considers general principle of promoting temporal consistency for improving self-training under distribution shifts. Therefore, it would not have direct societal impacts, which are contributed particularly by our work.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our experiments concern only standard models with publicly available benchmarks.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite papers related to code, data, and models used in our experiments.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not include the new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not include crowdsourcing or research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not include crowdsourcing or research with human subjects.