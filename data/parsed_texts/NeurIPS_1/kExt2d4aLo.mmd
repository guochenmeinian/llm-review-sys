# Can Generative AI Solve Your In-Context Learning Problem? A Martingale Perspective

Andrew Jesson

Correspondence to adj2147@columbia.edu.  Department of Statistics, Columbia University.  Department of Computer Science, Columbia University.

Nicolas Beltran-Velez

Department of Statistics, Columbia University.  Department of Computer Science, Columbia University.

David Blei

###### Abstract

This work is about estimating when a conditional generative model (CGM) can solve an in-context learning (ICL) problem. An in-context learning (ICL) problem comprises a CGM, a dataset, and a prediction task. The CGM could be a multi-modal foundation model; the dataset, a collection of patient histories, test results, and recorded diagnoses; and the prediction task to communicate a diagnosis to a new patient. A Bayesian interpretation of ICL assumes that the CGM computes a posterior predictive distribution over an unknown Bayesian model defining a joint distribution over latent explanations and observable data. From this perspective, Bayesian model criticism is a reasonable approach to assess the suitability of a given CGM for an ICL problem. However, such approaches--like posterior predictive checks (PPCs)--often assume that we can sample from the likelihood and posterior defined by the Bayesian model, which are not explicitly given for contemporary CGMs. To address this, we show when ancestral sampling from the predictive distribution of a CGM is equivalent to sampling datasets from the posterior predictive of the assumed Bayesian model. Then we develop the generative predictive \(p\)-value, which enables PPCs and their cousins for contemporary CGMs. The generative predictive \(p\)-value can then be used in a statistical decision procedure to determine when the model is appropriate for an ICL problem. Our method only requires generating queries and responses from a CGM and evaluating its response log probability. We empirically evaluate our method on synthetic tabular, imaging, and natural language ICL tasks using large language models.

## 1 Introduction

An in-context learning (ICL) problem involves a conditional generative model (CGM), a dataset, and a prediction task [1, 2]. The CGM could be a pre-trained foundation model. The dataset might consist of patient histories, test results, and diagnoses. The prediction task could be providing a diagnosis to a new patient based on their history and test results [3]. This is a complex problem that requires both accurate diagnosis and proper communication to the patient. This complexity makes it difficult to evaluate whether the model is suitable for the dataset and prediction task.

An interpretation of ICL sees a CGM prompted with in-context examples as producing data (either responses or examples of the prediction problem) from a posterior predictive under a Bayesian model. A natural question arises when we accept this premise, "Is the Bayesian model a good model for the prediction problem?" This question is what Bayesian model criticism tries to answer. This field has produced many methods but, they typically assume access to components defined by the Bayesian model, like the likelihood and posterior. In this work we show how to do model criticism in ICL using contemporary generative AI. Specifically, we demonstrate how to implement posterior predictive checks (PPCs) [4, 5] and their cousins [6, 7] when we only have access to the predictive distribution. The result is a practical and interpretable test on whether a model can solve an ICL problem.

Input: proof once again that if the filmmakers just follow the books Label: negative Input: in impressive Label: positive Input: the top japanese animations Label: positive Input: a spoof comedy Label: positive (a) SST2 ICL dataset \(\mathrm{x}^{n}\) Input: follows the formula, but throws in too many conflicts to keep the story compelling. (b) query z. Label: negative, Label: negative, Label: negative, Label: negative, Label: negative (c) CGM responses y (f) CGM responses y ```

Listing 1: An example illustrating two ICL problems. One that the model \(\boldsymbol{\theta}\) (Llama-2 7B [8]) can solve, and one that it cannot. Left: (a) examples from the SST2 task [9] comprising part of an ICL dataset \(\mathrm{x}^{n}\), (b) a new query \(\mathrm{z}\), and (c) some responses \(\mathrm{y}\) sampled from the CGM \(p_{\boldsymbol{\theta}}(\mathrm{y}\mid\mathrm{z},\mathrm{x}^{n})\) when prompted with the dataset and query. The true label is "negative" and the CGM responds correctly. Right: the same format but the dataset (d) and query (e) are taken from the MQP task [10]. Here, the true label is "similar," but the model responds incorrectly with "different" half of the time.

## 2 What is an in-context learning problem?

An ICL problem is a tuple \(\left(\mathrm{f}^{*},\mathrm{x}^{n},\boldsymbol{\theta}\right)\) comprising a prediction task \(\mathrm{f}^{*}\), a dataset \(\mathrm{x}^{n}\), and a conditional generative model (CGM) \(\boldsymbol{\theta}\). The prediction task is generalized as providing a response \(\mathrm{y}\) to a query \(\mathrm{z}\). The set of valid responses to a user query implies a distribution over responses \(p(\mathrm{y}\mid\mathrm{z},\mathrm{f}^{*})\). The dataset \(\mathrm{x}^{n}=\left\{\left(\mathrm{z}_{i},\mathrm{y}_{i}\right)\right\}_{i=1} ^{n}\) comprises \(n\) query and response examples of the prediction task; \(\mathrm{z}_{i},\mathrm{y}_{i}\sim p(\mathrm{z},\mathrm{y}\mid\mathrm{f}^{*})\). A practical abstraction decomposes queries and responses into elements called tokens. As such, queries and responses--by extension, examples and datasets--are represented as sequences of tokens. For example, \((\mathrm{z},\mathrm{y})\equiv(\mathrm{t}_{1}^{\mathrm{z}},\mathrm{t}_{2}^{ \mathrm{z}},\dots,\mathrm{t}_{1}^{\mathrm{y}},\mathrm{t}_{2}^{\mathrm{y}},\dots )\equiv(\mathrm{t}_{1}^{\mathrm{x}},\mathrm{t}_{2}^{\mathrm{z}},\dots)\). A CGM \(\boldsymbol{\theta}\) defines a predictive distribution over the next token in an example \(\mathrm{t}_{j}^{\mathrm{x}}\) given previous example tokens and \(\mathrm{t}_{<j}^{\mathrm{x}}\), and a tokenized dataset; \(p_{\boldsymbol{\theta}}(\mathrm{t}_{j}^{\mathrm{x}}\mid\mathrm{t}_{<j}^{ \mathrm{x}},\mathrm{x}^{n})\). By ancestral sampling, the CGM effectively defines additional predictive distributions over responses \(p_{\boldsymbol{\theta}}(\mathrm{y}\mid\mathrm{z},\mathrm{x}^{n})\), examples \(p_{\boldsymbol{\theta}}(\mathrm{z},\mathrm{y}\mid\mathrm{x}^{n})\), and datasets \(p_{\boldsymbol{\theta}}(\mathrm{x}\mid\mathrm{x}^{n})\). Figure 1 illustrates examples from two different ICL tasks. Figures 0(a) to 0(c) gives an example from the SST2 sentiment prediction task for which Llama-2 7B frequently yields accurate answers. Figures 0(d) to 0(f) gives an example from the medical questions pairs (MQP) prediction task for which Llama-2 7B yields random answers on average. Next, we give several reasons why model generated responses or examples may be inappropriate for the ICL task \(\mathrm{f}^{*}\).

## 3 What is a model?

Again let \(\boldsymbol{\theta}\) denote a model, but now the model could be Bayesian linear regression, a Gaussian process, or perhaps a large language model (LLM). A model defines a joint distribution \(p_{\boldsymbol{\theta}}(\mathrm{x},\mathrm{f})\) over observable data \(\mathrm{x}=\left\{\mathrm{x}_{1},\mathrm{x}_{2},\dots\right\}=\left\{\left( \mathrm{z}_{1},\mathrm{y}_{1}\right),\left(\mathrm{z}_{2},\mathrm{y}_{2}\right),\dots\right\}\) and latent explanations \(\mathrm{f}\). The notation \(\mathrm{f}\) denotes both tasks and explanations, but we will clearly distinguish between them. The model joint distribution factorizes as \(p_{\boldsymbol{\theta}}(\mathrm{x},\mathrm{f})=p_{\boldsymbol{\theta}}(\mathrm{ x}\mid\mathrm{f})p_{\boldsymbol{\theta}}(\mathrm{f})\), where \(p_{\boldsymbol{\theta}}(\mathrm{f})\) is the prior over explanations and \(p_{\boldsymbol{\theta}}(\mathrm{x}\mid\mathrm{f})\) is the likelihood of the dataset given an explanation. From a frequentist perspective, the prior distribution over \(\mathrm{f}\) would be ignored and a model would define a set of distributions over datasets indexed by \(\mathrm{f}\); \(\{p_{\boldsymbol{\theta}}(\mathrm{x}\mid\mathrm{f}):\mathrm{f}\in\mathcal{F}\}\). A model \(\boldsymbol{\theta}\) alongside data \(\mathrm{x}^{n}\) further defines the posterior \(p_{\boldsymbol{\theta}}(\mathrm{f}\mid\mathrm{x}^{n})\) and posterior predictive \(p_{\boldsymbol{\theta}}(\mathrm{x}\mid\mathrm{x}^{n})=\int p_{\boldsymbol{ \theta}}(\mathrm{x}\mid\mathrm{f})\ d\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf \mathbf{\mathbf \mathbf{\mathbf                                                 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \variables like \(\mathrm{f}\)? We justify this with two key assumptions: First, if the model \(p_{\mathbf{\theta}}\) is exchangeable (i.e., the distribution \(p_{\mathbf{\theta}}(\mathrm{x})\) is invariant to permutations of the data), de Finetti's theorem [11] guarantees the existence of such a latent variable \(\mathrm{f}\). Therefore, assuming we adopt a unique representation of \(\mathrm{f}\), there is no issue in writing \(p_{\mathbf{\theta}}(\mathrm{x},\mathrm{f})\). Alternatively, if \(p_{\mathbf{\theta}}(\mathrm{x})\) approximates an exchangeable distribution \(p(\mathrm{x})\), as is the case with ICL problems, then we can treat the statement \(p_{\mathbf{\theta}}(\mathrm{x},\mathrm{f})\) as a convenient abuse of notation, meant to represent \(p(\mathrm{x},\mathrm{f})\). Throughout, we assume that either of these conditions hold.

## 4 A model is a choice to be criticised

A model \(\mathbf{\theta}\) over an observation space \(\mathcal{X}\) is used to make inferences based on observations \(\mathrm{x}^{n}\). These inferences can include probabilities of the next word in a sequence, model uncertainty, and other quantities of interest. However, since the model is a choice made by the practitioner, there is no guarantee that these inferences reflect reality or adequately model the data. For example, a randomly initialized LLM can be used to make inferences about next word probabilities, but those inferences are meaningless for modeling natural language. As models become more complex and widely used, it is crucial to understand when they can be trusted.

Much of the discussion around the reliability of CGMs has focused on "hallucination" detection, prediction, and mitigation [12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35]. An interesting subset of these methods are based on uncertainty quantification where inferences about the variability of responses from the posterior predictive distribution [36; 37; 38; 39], or about the variability of explanations [33; 35; 40; 41; 42] are used to predict when a model may hallucinate. However, these methods do not address the fundamental question of when to trust those inferences so they are susceptible to failure if the model is not appropriate for a task.

A growing body of work is formalizing the connection between ICL with pre-trained CGMs and Bayesian inference [35; 43; 44; 45; 46; 47; 48]. Notably, the works of Jesson et al. [35], Fong et al. [45], Lee et al. [46], Falck et al. [47], Ye et al. [48] show how to transform Bayesian functionals of the model likelihood \(p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{f})\) and model posterior \(p_{\mathbf{\theta}}(\mathrm{f}\mid\mathrm{x}^{n})\) into functionals of the model predictive distribution \(p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{x}^{n})\), which can be computed by contemporary CGMs. These works pave the way for using Bayesian model criticism techniques such as posterior predictive checks as a response to our research question. In the following we formalize how this is done.

## 5 Posterior predictive checks are model critics for ICL problems.

Posterior predictive checks (PPCs) [5; 6; 7; 49] are Bayesian model criticism methods that use the posterior predictive to evaluate a model's capability to make inferences from observations. Model capability is measured by the posterior predictive \(p\)-value, based on the hypothesis that the data are generated according to the model \(\mathbf{\theta}\). Following Moran et al. [7], we assume access to main \(\mathrm{x}^{n}\) and holdout \(\mathrm{x}^{\mathrm{test}}\) sets, both distributed according to the reference likelihood \(p(\mathrm{x}\mid\mathrm{f}^{*})\). A class of PPCs assess how well a model \(\mathbf{\theta}\) fit to \(\mathrm{x}^{n}\) explains the holdout data \(\mathrm{x}^{\mathrm{test}}\). To measure fit, a PPC defines a discrepancy function, such as the negative log marginal model likelihood \(g_{\mathbf{\theta}}(\mathrm{x},\mathrm{x}^{n}):=-\sum_{z_{i},y_{i}\in\mathrm{x}} \log p_{\mathbf{\theta}}(\mathrm{z}_{i},y_{i}\mid\mathrm{x}^{n})\), or the negative log model likelihood \(g_{\mathbf{\theta}}(\mathrm{x},\mathrm{f}):=-\sum_{x_{i},y_{i}\in\mathrm{x}}\log p _{\mathbf{\theta}}(\mathrm{z}_{i},y_{i}\mid\mathrm{f})\). Higher values indicate poor model fit, while lower values suggest the model explains the data well.

Defining a goodness-of-fit measure is only half of the story. A PPC needs to define what are relatively high or relatively low values of the discrepancy function. To do this, a reference distribution of values is defined by measuring the discrepancy function over datasets sampled from the model posterior predictive distribution. The posterior predictive \(p\)-value is then evaluated as

\[p_{\mathsf{ppc}}:=\iint\mathbbm{1}\big{\{}g_{\mathbf{\theta}}(\mathrm{x},\cdot)\geq g _{\mathbf{\theta}}(\mathrm{x}^{\mathrm{test}},\cdot)\big{\}}d\mathsf{P}_{\mathbf{ \theta}}(\mathrm{x}\mid\mathrm{f})d\mathsf{P}_{\mathbf{\theta}}(\mathrm{f}\mid \mathrm{x}^{n}). \tag{1}\]

The PPC compares the discrepancy function value for holdout data \(g_{\mathbf{\theta}}(\mathrm{x}^{\mathrm{test}},\cdot)\) with its distribution under the model \(g_{\mathbf{\theta}}(\mathrm{X},\cdot)\). If the model-generated data often has a greater or equal discrepancy than the holdout data, we can be confident the model explains the holdout data well. Conversely, if the holdout data's discrepancy is frequently higher, we should be less confident in the model's ability to explain it, raising doubts about the model's ability to solve the ICL problem. Algorithm 2 in Appendix C describes a \(p_{\mathsf{ppc}}\) estimator.

The generative predictive p-value and how to estimate it.

Modern CGMs--such as LLMS--do not provide an explicit representaion of the joint distribution over observations and explanations. For discrepancy functions that depend on \(\mathrm{f}\) this is a problem.

Our solution to the inaccessibility of component distributions relies on the intuition that a large enough dataset \(\mathrm{x}^{N}\coloneqq\{\mathrm{z}_{i},\mathrm{y}_{i}\}_{i=1}^{N}\) generated by the likelihood \(\mathrm{x}^{N}\sim p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{f})\) contains roughly the same information as the explanation \(\mathrm{f}\) itself for an identifiable Bayesian model. Thus, it makes sense to express functions of \(\mathrm{f}\), like the model likelihood \(p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{f})\), that are not defined by a CGM, as functions of large datasets, such as the predictive distribution \(p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{x}^{N})\), which are defined.

Now, where do the additional \(N-n\) examples come from if we are only given the \(n\) examples comprising \(\mathrm{x}^{n}\)? The generation of sufficiently large datasets is done by generating hypothetical completions \(\mathrm{x}^{n+1:N}\) of the observed ICL dataset \(\mathrm{x}^{n}\) by ancestrally sampling from the model predictive distribution \(p_{\mathbf{\theta}}(\mathrm{z},\mathrm{y}\mid\mathrm{x}^{n})\) (also called predictive resampling by Fong et al. [45]):

\[\mathrm{z}_{n+1},\mathrm{y}_{n+1}\sim p_{\mathbf{\theta}}(\mathrm{z},\mathrm{y} \mid\mathrm{x}^{n}),\mathrm{z}_{n+2},\quad\mathrm{y}_{n+2}\sim p_{\mathbf{\theta}} (\mathrm{z},\mathrm{y}\mid\mathrm{x}^{n},\mathrm{z}_{n+1},\mathrm{y}_{n+1}), \quad\ldots\]

As the generated examples are added to the conditional of the predictive distribution after each step, this process can be thought of as reasoning toward one explanation by imagining a sequence of sets of observations that are consistent with a smaller and smaller set of explanations as the sequence length increases. As a stochastic process, it is encouraged to reason toward a different explanation each time it is run to complete \(\mathrm{x}^{n}\) with \(N-n\) imagined examples.

Building off this intuition, we define martingale and generative predictive \(p\)-values below. We prove that under general conditions the martingale predictive \(p\)-value is equal to the posterior predictive \(p\)-value. We then show how to estimate the generative predictive \(p\)-value for a given ICL problem.

### The martingale predictive p-value

Our method is built on Doob's theorem for estimators (Theorem 2), which helps us transform statements about the random variable \(h(\mathrm{F})\)--a function of explanations \(\mathrm{F}\)--to statements about the random variable \(\mathbb{E}[h(\mathrm{F})\mid\mathrm{X}_{1},\mathrm{X}_{2},\ldots,\mathrm{X}_{ n}]\), which is a function of observations \((\mathrm{X}_{1},\mathrm{X}_{2},\ldots,\mathrm{X}_{n})\). Thus, we can proceed without direct access to \(p_{\mathbf{\theta}}(\mathrm{z},\mathrm{y}\mid\mathrm{f})\)and \(p_{\mathbf{\theta}}(\mathrm{f}\mid\mathrm{x}_{n})\) and define a \(p\)-value that depends on infinite datasets \(\mathrm{x}^{\infty}\coloneqq(\mathrm{x}_{i},y_{i})_{i=1}^{\infty}\) rather than \(\mathrm{f}\)

\[p_{\text{mpc}}\coloneqq\iint\mathbb{1}\left\{g_{\mathbf{\theta}}(\mathrm{x}, \mathrm{x}^{\infty})\geq g_{\mathbf{\theta}}(\mathrm{x}^{\text{test}},\mathrm{x}^ {\infty})\right\}d\mathrm{P}_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{x}^{\infty}) d\mathrm{P}_{\mathbf{\theta}}(\mathrm{x}^{n:\infty}\mid\mathrm{x}^{n}). \tag{2}\]

Doob's Theorem is an application of martingales, so--in line with the current literature [45, 46, 47]--we call this formulation the martingale predictive \(p\)-value.

The main theoretical result of this paper establishes the equality of the posterior and martingale predictive \(p\)-values; Equations (1) and (2). We formalize this statement in the following theorem.

**Theorem 1**.: _Let \(\mathrm{F}\sim\mathrm{P}_{\mathbf{\theta}}\), and \(\mathrm{X}_{1},\mathrm{X}_{2},\ldots\) i.i.d \(\sim\mathrm{P}_{\mathbf{\theta}}^{\mathrm{f}}\). Assume Conditions 1 to 3 and let,_

\[\int|\log p_{\mathbf{\theta}}(\mathrm{x}^{m}\mid\mathrm{f})|d\mathrm{P}_{\mathbf{ \theta}}(\mathrm{f})<\infty:\quad\forall\mathrm{x}^{m}\in\mathcal{X}^{m}.\]

_Then, \(p_{\text{mpc}}=p_{\text{mpc}}\)._

Proof.: The proof makes use of Doob's Theorem and is presented in Appendix B. 

### The generative predictive p-value

The martingale predictive \(p\)-value cannot be exactly computed because it is impossible to generate infinite datasets. Thus, we define the generative predictive \(p\)-value that clips the limits to infinity by some feasibly large number \(N\) to estimate Equation (2) as

\[p_{\text{gpp}}\coloneqq\iint\mathbb{1}\left\{g_{\mathbf{\theta}}(\mathrm{x}, \mathrm{x}^{N})\geq g_{\mathbf{\theta}}(\mathrm{x}^{\text{test}},\mathrm{x}^{N}) \right\}d\mathrm{P}_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{x}^{N})d\mathrm{P}_{ \mathbf{\theta}}(\mathrm{x}^{n:N}\mid\mathrm{x}^{n}). \tag{3}\]

Note that the generative predictive \(p\)-value enables us to replace any distributions that depend on latent mechanisms \(\mathrm{f}\) or infinite datasets \(\mathrm{x}^{\infty}\) with ones that depend on finite sequences. The price we pay for using finite \(N\) is estimation error between \(p_{\text{gpp}}\) and \(p_{\text{ppc}}\). We leave the formal analysis of this error to future work.

### CGM estimators for the generative predictive p-value

```
0: data \(\{\mathrm{x}^{n},\mathrm{x}^{\mathrm{test}}\}\), discrepancy function \(g_{\mathbf{\theta}}(\mathrm{x},\mathrm{x}^{N})\), # replicates \(\mathrm{M}\), # approx. samples \(\mathrm{N}\)
1:for\(i\gets 1\) to \(\mathrm{M}\)do
2:\(\mathrm{x}^{N}_{i}\leftarrow\mathrm{x}^{n}\)\(\triangleright\) initialize f sample data
3:for\(j\gets n+1\) to \(N\)do
4:\(\mathrm{z}_{j},\mathrm{y}_{j}\sim p_{\mathbf{\theta}}(\mathrm{z},\mathrm{y}|\mathrm{x }^{N})\)\(\triangleright\) sample example from model
5:\(\mathrm{x}^{N}_{i}\leftarrow(\mathrm{x}^{N}_{i},\mathrm{z}_{j},\mathrm{y}_{j})\)\(\triangleright\) update approximation context
6:\(\mathrm{x}_{i}\leftarrow()\)\(\triangleright\) initialize replicant data
7:for\(j\gets 1\) to \(n\)do
8:\(\mathrm{z}_{j},\mathrm{y}_{j}\sim p_{\mathbf{\theta}}(\mathrm{z},\mathrm{y}|\mathrm{ x}^{N}_{i})\)\(\triangleright\) sample example from model
9:\(\mathrm{x}_{i}\leftarrow(\mathrm{x}_{i},\mathrm{z}_{j},\mathrm{y}_{j})\)\(\triangleright\) update replicant data
10:return\(\frac{1}{\mathrm{M}}\sum_{i=1}^{\mathrm{M}}\mathbbm{1}\left\{g_{\mathbf{\theta}}( \mathrm{x}_{i},\mathrm{x}^{N}_{i})\geq g_{\mathbf{\theta}}(\mathrm{x}^{\mathrm{ test}},\mathrm{x}^{N}_{i})\right\}\)\(\triangleright\) estimate \(p\)-value
```

**Algorithm 1**\(\widehat{p}_{\mathrm{gpc}}\)

We derive an estimator for the generative predictive \(p\)-value in Equation (3) that uses Monte Carlo estimates to approximate the integrals. Algorithm 1 describes the estimation procedure. The key stage that differentiates the generative predictive \(p\)-value algorithm from the standard posterior predictive \(p\)-value algorithm is described in Lines 2 to 5. Here datasets \(\mathrm{x}^{N}_{i}\) of length \(N\) are ancestrally sampled from the CGM predictive distribution to approximate sampling a mechanism \(\mathrm{f}_{i}\). This is in contrast to sampling an explanation directly from the model posterior as shown in Algorithm 2 Line 2 of Appendix C. When sampling replication data \(\mathrm{x}_{i}\) in Lines 6 to 9, the CGM predictive distribution is conditioned on \(\mathrm{x}^{N}_{i}\) and \(n\) new samples are independently generated. This procedure is repeated \(M\) times, and the \(p\)-value is empirically estimated as before.

## 7 Empirical evaluation

This section reports the following empirical findings: (1) The generative predictive \(p\)-value is an accurate predictor of model capability in tabular, natural language, and imaging ICL problems. (2) The \(p\)-value computed under the NLL discrepancy is also an indicator of whether there are enough in-context examples \(n\). (3) The number of generated examples \(N-n\) interpolates the \(p\)-value between the posterior predictive \(p\)-value under the NLML discrepancy and the NLL discrepancy using the model posterior \(p_{\mathbf{\theta}}(\mathrm{f}\mid\mathrm{x}^{n})\) and likelihood \(p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{f})\). These findings show that the \(p\)-value computed under either discrepancy yields an accurate predictor of whether generative AI can solve your in-context learning problem. If you also need to know whether there are enough in-context examples, we suggest using the NLL discrepancy function. If computational efficiency is a primary concern, we suggest using the NLML discrepancy as dataset completion generation is not required.

**Models.** We evaluate our methods using two model types. For tabular and imaging tasks, we use a Llama-2 regression model for sequences of continuous variables [35]. The model is optimized from scratch for next token (variable or pixel) prediction following the procedure of Touvron et al. [8]. For natural language tasks, we use pre-trained Llama-2 7B [8] and Gemma-2 9B [50] LLMs (Gemma-2 9B results are reported in Appendix F).

**Data.** For tabular tasks, queries z are sampled uniformly from the interval \([-2,2]\). Responses y are drawn from a normal distribution with a mean \(\mu(\mathrm{z})\), parameterized by eithe

Figure 2: Tabular data tasks.

polynomial (in-distribution), a random ReLU neural network (in-distribution or OOD), or a radial basis function (RBF) kernel Gaussian process with a length scale of 0.3 (OOD). The training data comprise 8000 unique in-distribution datasets with 2000 \(\mathrm{z}-\mathrm{y}\) examples each. An in-distribution ReLU-NN task is illustrated in Figure 1(a). The mean function \(\mu(\mathrm{z})\) is plotted by the blue line, and the blue shaded region outlines the 95% CI of \(p_{\mathbf{\theta}}(\mathrm{y}\mid\mathrm{z},\mathrm{f}^{*})\). An OOD GP task is illustrated in Figure 1(b). In-distribution test data comprise a set of 200 new random datasets with 500 \(\mathrm{z}-\mathrm{y}\) examples each. The OOD test data comprise 200 random datasets with 500 \(\mathrm{z}-\mathrm{y}\) examples each.

For pre-trained LLM experiments, the delineation between in- and out-of-distribution is opaque. Instead, we use in-capability or out-of-capability to differentiate between tasks a model can or cannot perform well. Figure 2(a) illustrates in-capability tasks where the error rate of Llama-2 7B is considerably better than random guessing. The in-capability data are the SST2 [9] sentiment analysis (positive vs. negative) and AG News Zhang et al. [51] topic classification (World, Sports, Business, Sci/Tech) datasets. Figure 2(b) illustrates out-of-capability tasks where the error rate is only marginally better than random. The out-of-capability data are the Medical Questions Pairs (MQP) [10] differentiation (similar vs. different) and RTE [52] natural language inference (entailment vs. not entailment) datasets.

For imaging ICL experiments, we use SVHN for in-distribution data [53], MNIST as "near" OOD data [54], and CIFAR-10 as "far" OOD data [55]. Our Llama-2 regression model takes a sequence of flattened, grayscale, 8x8 images as input. It is fit to random sequences of 16 images from the SVHN "extra" split, which has over 500k examples. A series of in-distribution generative fill tasks is shown in Figure 3(a). In each row, the model is prompted with three in-context examples and asked to complete the missing half of the 4th example. Each completion in the "fill" column is sensible, even when the completed number differs from the "real" number. Figure 3(b) illustrates completions for near-OOD MNIST tasks. We see in rows 1, 2, 3, and 6 that the fills are often sensible, but the model is prone to hallucinating odd completions (row 4) and artifacts (row 5). Figure 3(c) illustrates completions for far-OOD CIFAR-10 tasks. The completions are surprisingly consistent at this resolution, but as the result in row 4 demonstrates, the model hallucinates completions from its domain.

Figure 4: Generative fill tasks using the test sets of SVHN, MNIST, and CIFAR-10.

Figure 3: Natural language in-capability vs. out-of-capability tasks. Green solid line is the ICL error rate for Llama-2 7B. Gray dashed line is the random guessing error rate.

**Discrepancy functions.** We evaluate the \(p\)-value using discrepancy functions defined as

\[g_{\mathbf{\theta}}(\mathrm{x},\mathrm{x}^{(\cdot)}):=-\frac{1}{|\mathrm{x}|}\sum_{x_{ i},y_{i}\in\mathrm{x}}\frac{1}{|(z_{i},y_{i})|}\sum_{t_{j}\in(z_{i},y_{i})}\log p_{ \mathbf{\theta}}(\mathrm{t}_{j}\mid\mathrm{t}_{<j},\mathrm{x}^{(\cdot)}),\]

where \(|(z_{i},y_{i})|\) is the number of tokens in the evaluated example. Following this template, the per-token negative log marginal likelihood (NLML) is written \(g_{\mathbf{\theta}}(\mathrm{x},\mathrm{x}^{n})\) and an estimate of the per-token the negative log-likelihood (NLL) is written \(g_{\mathbf{\theta}}(\mathrm{x},\mathrm{x}^{N})\), where \(\mathrm{x}^{N}\) is generated as in Algorithm 1.

**Predicting model capability.** The \(p\)-values are calculated using either Algorithm 1 or Algorithm 3 and a significance level \(\alpha\) is selected to yield a binary predictor of model capability \(\mathbb{1}\{p_{\text{gpc}}<\alpha\}\); a model is predicted as incapable of solving the ICL problem if the estimated generative predictive p-value is less than the significance level. We report results for significance levels \(\alpha\in[0.01,0.05,0.1,0.2,0.5]\). For the NLL discrepancy function, replication data x is independently sampled from the likelihood under a hypothetical dataset completion \(p_{\mathbf{\theta}}(\mathrm{z},\mathrm{y}\mid\mathrm{x}^{N})\). For the NLML discrepancy function, replication data is independently sampled from the predictive distribution \(p_{\mathbf{\theta}}(\mathrm{z},\mathrm{y}\mid\mathrm{x}^{n})\).

**Evaluation metrics.** We evaluate the capability predictor using standard metrics: FPR measures in-capability tasks misclassified as out-of-capability, Precision reflects correctly identified out-of-capability tasks, and Recall measures correctly detected out-of-capability tasks. F1 Score and Accuracy assess overall performance (see Figure 5 for definitions).

We also provide the distribution of \(p\)-values across tasks to assess how confidently the model distinguishes between the different ICL problems. Lower \(p\)-values indicate stronger confidence that a model cannot solve a problem.

### The generative predictive \(p\)-value accurately predicts model capability

**Tabular data.** We first evaluate whether the generative predictive \(p\)-value effectively predicts OOD tabular data tasks. The parameters for Algorithm 1 are \(M=40\) replications and \(N-n=200\) generated examples. The ICL dataset \(\mathrm{x}^{n}\) size is varied from \(n=2\) to \(n=200\). Figure 6 plots precision, recall, F1, and accuracy curves and shows that the \(p\)-value estimates under either discrepancy function provide non-trivial OOD predictors for all \(\alpha\) settings.

**Natural language ICL.** Next, we evaluate whether the generative predictive \(p\)-value effectively predicts out-of-capacity natural language tasks. The parameters for Algorithm 1 are \(M=20\) replications and \(N-n=10\) generated examples. The ICL dataset \(\mathrm{x}^{n}\) size is varied from \(n=4\) to \(n=64\). Figure 7 plots precision, recall, F1, and accuracy curves and shows that the \(p\)-value estimates under the NLL discrepancy provide non-trivial (accuracy \(>0.5\)) out-of-capability predictors in the domain of natural language for all \(\alpha\) settings. The NLML discrepancy \(g_{\mathbf{\theta}}(\mathrm{x},\mathrm{x}^{n})\) is also generally robust outside of the small \(n\) and small \(\alpha\) setting.

Figure 5: Evaluation metrics for GPC performance.

Figure 6: Tabular OOD detection. Metric values vs. context length. In-distribution functions are from unseen random ReLU-NNs. OOD functions are from an RBF kernel GP.

**Generative fill.** Finally, we evaluate whether the generative predictive \(p\)-value effectively predicts OOD generative fill tasks. The parameters for Algorithm 1 are \(M=100\) replications and \(N-n=8\) generated examples The ICL dataset \(\text{x}^{n}\) size is varied from \(n=2\) to \(n=8\). Figure 8 plots the OOD prediction metric curves and shows that the \(p\)-value estimates under either discrepancy function provide non-trivial (accuracy \(>0.666\)) OOD predictors for all \(\alpha\) settings.

**Discussion.** Figures 6 to 8 reveal several trends. First, the NLML discrepancy (blue) yields better precision, indicating that it is less likely to misclassify an in-capability ICL problem as unsolvable. Second, the NLL discrepancy (purple) yields higher recall, indicating that it is less likely to misclassify an out-of-capability ICL problem as solvable. Third, the NLL discrepancy with significance level \(\alpha=0.05\) yields a generally more accurate predictor than the NLML discrepancy function for any significance level in the set evaluated. Finally, the recall of a predictor under the NLML discrepancy is sensitive to the number of in-context examples \(n\). Next, we look deeper into the relationship between dataset size and the discrepancy functions.

### The NLL discrepancy also indicates whether you have enough data

Both discrepancy functions yield accurate predictors of model capability, but the NLL discrepancy also provides information about whether there are enough in-context examples to reliably solve a task. We use prediction RMSE over task responses to measure reliability. Figures 8(a) and 8(b) plot the

Figure 8: Generative fill OOD detection. Metric values vs. context length. In-distribution tasks are from the SVHN test set. Near and far OOD tasks are from the MNIST and CIFAR-10 test sets.

Figure 7: Llama-2-7B out-of-capability detection. Metric values vs. context length. In-capability tasks are from SST2 and AG News datasets. Out-of-capability tasks are from RTE and MQP datasets.

Figure 9: (a) and (b) Scatter plots of response RMSE vs. \(p\)-values for the NLL and NLML discrepancies. Points are style-coded by ICL dataset size \(n\). (c) Risk vs. \(n\). (d) Accuracy vs. \(n\)RMSE against the \(p\)-values computed under the NLL and NLML discrepancies for in-distribution polynomial tabular tasks. We see that lower \(p\)-values correlate with higher RMSE for the NLL discrepancy, but not for the NLML discrepancy. This added information is useful for reducing risk in recommendation systems that autonomously respond if the \(p\)-value is greater than the significance level \(\alpha\). For example, at \(\alpha=0.1\), the NLL discrepancy reduces the generation of responses with higher error because it accounts for the number of examples provided. Taking the risk as the sum of task RMSEs for tasks predicted as in-capability, Figures 9c and 9d show that the NLL discrepancy results in substantially reduced risk, even when we closely match the accuracies of each predictor. Figure 13 in the appendix gives further insight into how the distributions of \(p\)-values evolve with dataset size for each discrepancy function.

The number of generated examples \(N-n\) interpolates the \(p-value\) estimate between the NLML and the ideal NLL discrepancies

Inspection of Equations (1) to (3) makes clear that the dataset completion size \(N-n\) should closely interpolate \(p\)-value estimates between \(p_{\text{ppc}}\) computed with the NLML discrepancy and with the NLL discrepancy using the likelihood and posterior of a Bayesian model. To verify this, we use a reference Bayesian polynomial regression model to compute the \(p_{\text{ppc}}\). We use our Llama-2 regression model fit to datasets generated from the reference model likelihood under different explanations to compute the \(p_{\text{ppc}}\). We let datasets generated by random ReLU-NNs serve as OOD tasks. Figure 10 demonstrates that our expectation is true. Specifically, the \(p\)-value estimates at \(N-n=2\) are distributionally close to those calculated under the NLML, and they more closely approximate those calculated under the reference NLL discrepancy as we increase \(N-n\) to \(100\). The latter observation is also illustrated in Figure 12.

Since the \(p\)-values computed under either discrepancy yield accurate predictors of model capability, the choice between discrepancy functions ultimately comes down to a decision on whether the added computational cost of generating dataset completions is justified. If you need to know whether there are enough in-context examples to generate an accurate response--a necessity in risk-sensitive applications--then we recommend using the NLL discrepancy function. If computational efficiency or the cost of response deferral are primary concerns--practical user experience concerns--we suggest using the NLML discrepancy.

## 8 Conclusion

This work introduces the _generative predictive \(p\)-value_, a metric for determining whether a Conditional Generative Model can solve an In-Context Learning problem. It extends Bayesian model criticism techniques like PPCs to generative models by sampling dataset completions from the model's predictive distribution to approximate sampling latent explanations from a Bayesian model posterior. Empirical evaluations on tabular, natural language, and imaging tasks show that the generative predictive \(p\)-value can effectively identify the limits of model capability, distinguishing between in-capability and out-of-capability tasks for models like Llama-2 7B and Gemma-2 9B. This approach is a practical method to assess model suitability that advances Bayesian model criticism for CGMs. While we have focused on model capability prediction, the \(p\)-value estimates could also be used for model selection or as a general measure of task uncertainty. We are eager to explore extensions beyond ICL tasks to improve the reliability of generative AI systems.

## References

* [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [2] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. _arXiv preprint arXiv:2301.00234_, 2022.
* [3] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. _CoRR_, 2023.
* [4] Irwin Guttman. The use of the concept of a future observation in goodness-of-fit problems. _Journal of the Royal Statistical Society: Series B (Methodological)_, 29(1):83-100, 1967.
* [5] Donald B Rubin. Bayesianly justifiable and relevant frequency calculations for the applied statistician. _The Annals of Statistics_, pages 1151-1172, 1984.
* [6] Andrew Gelman, Xiao-Li Meng, and Hal Stern. Posterior predictive assessment of model fitness via realized discrepancies. _Statistica sinica_, pages 733-760, 1996.
* [7] Gemma E Moran, David M Blei, and Rajesh Ranganath. Holdout predictive checks for bayesian model criticism. In _2023 IMS International Conference on Statistics and Data Science (ICSDS)_, page 94, 2023.
* [8] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv:2307.09288_, 2023.
* [9] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _EMNLP_, pages 1631-1642, 2013.
* [10] Clara H McCreery, Namit Katariya, Anitha Kannan, Manish Chablani, and Xavier Amatriain. Effective transfer learning for identifying similar questions: Matching user questions to covid-19 faqs. In _Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 3458-3465, 2020.
* [11] Edwin Shields Hewitt and Leonard J. Savage. Symmetric measures on cartesian products. _Transactions of the American Mathematical Society_, 80:470-501, 1955. URL [https://api.semanticscholar.org/CorpusID:53585081](https://api.semanticscholar.org/CorpusID:53585081).
* [12] Nouha Dziri, Andrea Madotto, Osmar R Zaiane, and Avishek Joey Bose. Neural path hunter: Reducing hallucination in dialogue systems via path grounding. In _EMNLP_, pages 2197-2214, 2021.
* [13] Dan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin Jiang, Qun Liu, and Pascale Fung. Read before generate! Faithful long form question answering with machine reading. In _ACL_, pages 744-756, 2022.
* [14] Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan Catanzaro. Factuality enhanced language models for open-ended text generation. _NeurIPS_, 35:34586-34599, 2022.
* [15] Sabrina J Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. Reducing conversational agents' overconfidence through linguistic calibration. _TACL_, 10:857-872, 2022.
* [16] Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. RARR: Researching and revising what language models say, using language models. In _ACL_, pages 16477-16508, 2023.
* [17] Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing. Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources. In _ICLR_, 2023.

* [18] Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A stitch in time saves nine: Detecting and mitigating hallucinations of LLMs by validating low-confidence generation. _arXiv:2307.03987_, 2023.
* [19] Philip Feldman, James R Foulds, and Shimei Pan. Trapping LLM hallucinations using tagged context prompts. _arXiv:2306.06085_, 2023.
* [20] Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang. Mitigating language model hallucination with interactive question-knowledge alignment. _arXiv:2305.13669_, 2023.
* [21] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. _arXiv:2302.12813_, 2023.
* [22] Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. _TMLR_, 2023.
* [23] Amos Azaria and Tom Mitchell. The internal state of an LLM knows when it's lying. In _EMNLP_, pages 967-976, 2023.
* [24] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. In _ICLR_, 2024.
* [25] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. In _ICLR_, 2023.
* [26] Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner. Steering Llama 2 via contrastive activation addition. _arXiv:2312.06681_, 2023.
* [27] Junyu Luo, Cao Xiao, and Fenglong Ma. Zero-resource hallucination prevention for large language models. _CoRR_, 2023.
* [28] Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Wen-tau Yih. Trusting your evidence: Hallucinate less with context-aware decoding. In _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)_, pages 783-791, 2024.
* [29] Neil Band, Xuechen Li, Tengyu Ma, and Tatsunori Hashimoto. Linguistic calibration of language models. _arXiv:2404.00474_, 2024.
* [30] Kenneth Li, Oam Patel, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. _NeurIPS_, 36, 2024.
* [31] Niels Mundler, Jingxuan He, Slobodan Jenko, and Martin Vechev. Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation. In _ICLR_, 2024.
* [32] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason E Weston. Chain-of-verification reduces hallucination in large language models. In _ICLR 2024 Workshop on Reliable and Responsible Foundation Models_, 2024.
* [33] Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. _Nature_, 630(8017):625-630, 2024.
* [34] Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, and Yarin Gal. Semantic entropy probes: Robust and cheap hallucination detection in llms. _arXiv preprint arXiv:2406.15927_, 2024.
* [35] Andrew Jesson, Nicolas Beltran-Velez, Quentin Chu, Sweta Karlekar, Jannik Kossen, Yarin Gal, John P Cunningham, and David Blei. Estimating the hallucination rate of generative ai. _arXiv e-prints_, pages arXiv-2406, 2024.
* [36] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. _arXiv:2207.05221_, 2022.

* Manakul et al. [2023] Potsawee Manakul, Adian Liusie, and Mark JF Gales. SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. In _EMNLP_, 2023.
* Cole et al. [2023] Jeremy R Cole, Michael JQ Zhang, Daniel Gillick, Julian Martin Eisenschlos, Bhuwan Dhingra, and Jacob Eisenstein. Selectively answering ambiguous questions. _EMNLP_, 2023.
* Chen and Mueller [2024] Jiuhai Chen and Jonas Mueller. Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. In _ACL_, pages 5186-5200, 2024.
* Kuhn et al. [2023] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. _ICLR_, 2023.
* Elaraby et al. [2023] Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, and Shizhu Liu. Halo: Estimation and reduction of hallucinations in open-source weak large language models. _arXiv:2308.11764_, 2023.
* Lin et al. [2024] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Generating with confidence: Uncertainty quantification for black-box large language models. _TMLR_, 2024.
* Xie et al. [2021] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In _International Conference on Learning Representations_, 2021.
* Muller et al. [2021] Samuel Muller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers can do bayesian inference. In _International Conference on Learning Representations_, 2021.
* Fong et al. [2023] Edwin Fong, Chris Holmes, and Stephen G Walker. Martingale posterior distributions. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 85(5):1357-1391, 2023.
* Lee et al. [2023] Hyungi Lee, Eunggu Yun, Giung Nam, E Fong, and Juho Lee. Martingale posterior neural processes. In _The Eleventh International Conference on Learning Representations_. International Conference on Learning Representations, 2023.
* Falck et al. [2024] Fabian Falck, Ziyu Wang, and Chris Holmes. Is in-context learning in large language models bayesian? a martingale perspective. _arXiv preprint arXiv:2406.00793_, 2024.
* Ye et al. [2024] Naimeng Ye, Hanming Yang, Andrew Siah, and Hongseok Namkoong. Pre-training and in-context learning is bayesian inference a la de finetti. _arXiv preprint arXiv:2408.03307_, 2024.
* Meng [1994] Xiao-Li Meng. Posterior predictive \(p\)-values. _The annals of statistics_, 22(3):1142-1160, 1994.
* Gemma [2024] Gemma Team. Gemma, 2024.
* Zhang et al. [2015] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. _NeurIPS_, 2015.
* Dagan et al. [2006] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Joaquin Quinonero-Candela, Ido Dagan, Bernardo Magnini, and Florence d'Alche Buc, editors, _Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment_, pages 177-190, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3-540-33428-6.
* Netzer et al. [2011] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. Reading digits in natural images with unsupervised feature learning. In _NIPS workshop on deep learning and unsupervised feature learning_. Granada, 2011.
* LeCun et al. [1998] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. _University of Toronto_, 2009.
* Doob [1949] Joseph L Doob. Application of the theory of martingales. _Le calcul des probabilites et ses applications_, pages 23-27, 1949.
* Miller [2018] Jeffrey W Miller. A detailed treatment of doob's theorem. _arXiv preprint arXiv:1801.03122_, 2018.

Model intuition

In this section we give an interpretation of the component parts of a Bayesian model, how they are used to make inferences about uncertainty, and how to relate inferences in classical domains to inferences in more complex domains like language.

**The model prior \(p_{\mathbf{\theta}}(\mathrm{f})\)** can be thought of as a library over the possible explanations a model could ascribe to observations. It is a special kind of library, where the probability of finding an explanation in the library at random is also defined. The model prior encodes everything "known" to a model; all the latent patterns available as explanations for--or an index of all the probability distributions ascribable to--any set of observations. The model prior may or may not assign non-zero probability to an explanation \(\mathrm{f}\) equivalent to a given ICL problem task \(\mathrm{f}^{*}\). If no such explanation has coverage under the prior, then the model may not be able to provide an accurate solution to the ICL problem.

For example, a Bayesian linear regression model with fixed noise, defines the part of the prior over explanations \(p_{\mathbf{\theta}}(\mathrm{f})\) related to the outcome \(y\) as a set of coefficient vectors; a set of hyperplanes. If the ICL dataset and prediction task are characterized by a linear relationship between continuous valued queries and responses--illustrated in Figure 10(a)--the prior would be suitable for the ICL problem. However, if they are characterized by polynomial relationships--illustrated in Figure 10(c)--then the relationship would not have coverage under the prior and the precision of responses under the model would be limited by the capacity of a hyperplane to fit a polynomial surface. By analogy, a LLM that is pre-trained or fine-tuned on a large set of integrals expressed in natural language may have the functional capacity to integrate; generalize to unseen functions and domains of the classes and spaces covered in the training set. So if the ICL dataset and task are related to the integration of polynomials, the learned library of mappings from text to token distributions may be appropriate for that ICL problem. However, if the LLM training corpus did not contain content related to calculus, then learned library of mappings may not include the functional capability to solve the ICL problem.

**The model likelihood \(p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{f})\)** encodes the variety of observations \(\mathrm{x}\) that could be generated according to a given explanation \(\mathrm{f}\). The variety encoded by this distribution is often called _aleatoric_ uncertainty--alestory is a pretentious word for random--which refers to the inherent _randomness_ over generated datasets when sampling according to the likelihood under a given explanation \(\mathrm{f}\). For example, given the explanation implied by a fair coin, we will still be _uncertain_ whether the outcome of a single coin flip will be heads or tails. More contemporarily, if you are already familiar with this concept and I were to say, "I would like to share the idea of aleatoric uncertainty with you," you would know _which_ idea I want to share, but, before reading this paragraph, you would be uncertain about _how_ I would share it with you. When the model likelihood \(p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{f})\) is indexed by an explanation that is equivalent to the task \(\mathrm{f}\equiv\mathrm{f}^{*}\), then \(p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{f})\) is equal to the reference likelihood \(p(\mathrm{x}\mid\mathrm{f}^{*})\). So

Figure 11: Examples of misaligned model and data combinations. Transformer models (pink) are fit to either linear or polynomial noisy data defined by reference Bayesian models (blue).

even though we may be uncertain about _which_ dataset would be generated according to the model likelihood, we could still be certain that the generated dataset would correspond to the task. However, if there is a discrepancy between the model and reference likelihood then the model may not be suitable for the in-context learning problem.

**The model posterior**

\[p_{\mathbf{\theta}}(\mathrm{f}\mid\mathrm{x}^{n})=\frac{p_{\mathbf{\theta}}(\mathrm{x}^{ n}\mid\mathrm{f})p_{\mathbf{\theta}}(\mathrm{f})}{\int p_{\mathbf{\theta}}(\mathrm{x}^{n} \mid\mathrm{f})d\mathrm{P}_{\mathbf{\theta}}(\mathrm{f})},\]

derived from the model joint \(p_{\mathbf{\theta}}(\mathrm{x},\mathrm{f})\) via Bayes's theorem, encodes variety over explanations that _could_ have generated a specific set of observations \(\mathrm{x}^{n}\). This variety is often called _epistemic_ uncertainty. Epistemic is a pretentious term referring to knowledge, conveying that we may not yet know _which_ explanation f among subset of reasonable explanations best explains possible datasets x under the ICL problem.

For example, given only four observations--say, two heads and two tails--the sample mean estimate for the probability of observing heads is 0.5. However, we may still be uncertain about whether the coin generating the outcomes was fair or biased because the variance of that estimate is still a non-negligible 0.0625 when we assume the coin is actually fair. Related to our contemporary example, if I only say, "I would like to share an idea with you," you can probably imagine an abundance of ideas that I could be referring to and thus still be uncertain about which one I have chosen to share.

A relevant feature of epistemic uncertainty is that it is reducible as we observe more context. In the coin flip example, as we observe more outcomes, our certainty about the probability of heads increases. In the second example, you may have a better idea about the class of ideas I may share based on what has been presented thus far.

As a function of both the model likelihood and prior, the model posterior inherits the limitations of both. But it also provides information about whether an in-context learning problem can be solved reliably. Namely, variety over explanations is indicative of being uncertain about which task the ICL dataset corresponds to. This variety can lead to the model generating responses corresponding to tasks other than \(\mathrm{f}^{*}\). But it may also be indicative of when more examples (larger \(n\)) can improve the quality of solutions to an in-context learning problem.

**The model posterior predictive**

\[p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{x}^{n})=\int p_{\mathbf{\theta}}(\mathrm{x} \mid\mathrm{f})d\mathrm{P}_{\mathbf{\theta}}(\mathrm{f}\mid\mathrm{x}^{n}),\]

is derived from the model to generate new observations \(\mathrm{x}\) given past observations \(\mathrm{x}^{n}\). Poetically, the model posterior predictive gives the model a voice to respond to observations with observations. The model posterior predictive convolves the model likelihood of the observations given an explanation with the model posterior over explanations. This process entangles variety over explanations after observing a dataset \(\mathrm{x}^{n}\) and variety over observations \(\mathrm{x}\) for each specific explanation \(\mathrm{f}\); the model posterior predictive entangles aleatoric and epistemic uncertainty.

**Model inferences.** A model \(\mathbf{\theta}\) defined over an observation space \(\mathcal{X}\) is used make inferences about observations from that space \(\mathrm{x}^{n}\). Inferences like the probability of the the next word given a sequence of words, model uncertainty, and countless other things. But a model is a choice--the practitioner makes a modeling decision--and so the inferences derived from observations under a model may or may not be grounded in reality.

Figure 11 illustrates inferences about the predictive distribution made by different models given different datasets. When the data and model are well aligned-- Figures 10(a) and 10(b)--model inferences (pink) overlap with those made by the reference model (blue) and appear purple. However, inferences made by a misaligned polynomial model--Figure 10(c)--are much wider that those made by the reference linear model. And inferences made by a misaligned linear model--Figure 10(d)--are more narrow than those ogf the reference polynomial model, which results in the model being confident and wrong. Similarly, a very general LLM may have compromised data efficiency for rare domains, while a highly specialized LLM may no longer generalize beyond its domain.

These examples illustrate that while models are used to quantify empirical facts--like the frequency of an event occurring--they also carry a subjective aspect that needs to be considered when using model inferences in practice. This consideration guides our question of when a model will provide reliable inferences for a given ICL problem.

Proofs for theoretical results

We restate our formalization and assumptions for convenience. Observable examples \((\mathrm{z},\mathrm{y})\in\mathcal{X}\) are modeled by the \((\mathcal{X},\mathcal{A})\)-random variable \(\mathrm{X}_{i}\) and explanations \(\mathrm{f}\in\mathcal{F}\) are modeled by the \((\mathcal{F},\mathcal{B})\)-random variable \(\mathrm{F}\), where \(\mathcal{A}\) and \(\mathcal{B}\) are the relevant sigma algebras. For each \(\mathrm{f}\in\mathcal{F}\), let the model \(\mathbf{\theta}\) define a probability measure \(\mathrm{P}_{\mathbf{\theta}}^{\mathrm{f}}\) on \((\mathcal{X},\mathcal{A})\). Let the model \(\mathbf{\theta}\) further define a probability measure \(\mathrm{P}_{\mathbf{\theta}}\) on \((\mathcal{F},\mathcal{B})\). And let \(\mathrm{P}_{\mathbf{\theta}}\) and \(\mathrm{P}_{\mathbf{\theta}}^{\mathrm{f}}\) define the joint measure \(\mathrm{M}_{\mathbf{\theta}}\) over \(((\mathrm{X}_{1},\mathrm{X}_{2},\ldots),\mathrm{F})\). Our method rests on Doob's theorem for estimators [56], which assumes the following three conditions.

**Condition 1**.: _The observation \(\mathcal{X}\) and explanation \(\mathcal{F}\) spaces are complete and separable metric spaces._

**Condition 2**.: _The set of probability measures \(\{\mathrm{P}_{\mathbf{\theta}}^{\mathrm{f}}:\mathrm{f}\in\mathcal{F}\}\) defined by the model \(\theta\) is a measurable family; the mapping \(\mathrm{f}\mapsto\mathrm{P}_{\mathbf{\theta}}^{\mathrm{f}}(A)\) is measurable for every \(A\in\mathcal{A}\)._

**Condition 3**.: _The model \(\mathbf{\theta}\) is identifiable;_

\[\mathrm{f}\neq\mathrm{f}^{\prime}\Rightarrow\mathrm{P}_{\mathbf{\theta}}^{\mathrm{ f}}\neq\mathrm{P}_{\mathbf{\theta}}^{\prime}. \tag{4}\]

We state Doob's theorem for convenience.

**Theorem 2**.: _Doob's Theorem for estimators. Let \(\mathrm{F}\sim\mathrm{P}_{\mathbf{\theta}}\) and \(\mathrm{X}_{1},\mathrm{X}_{2},\ldots\) i.i.d \(\sim\mathrm{P}_{\mathbf{\theta}}^{\mathrm{f}}\). Then under general conditions on identifiability, \(\mathcal{F}\) and \(\mathcal{X}\) (see Appendix B), and a measurable function \(h:\mathcal{F}\rightarrow\mathbb{R}\) such that \(\int|h(\mathrm{f})|d\mathrm{P}_{\mathbf{\theta}}(\mathrm{f})<\infty\), then_

\[\lim_{n\rightarrow\infty}\mathbb{E}[h(\mathrm{F})\mid\mathrm{X}_{1},\mathrm{X} _{2},\ldots,\mathrm{X}_{n}]=h(\mathrm{F})\text{ a.s. }[\mathrm{M}_{\mathbf{\theta}}]. \tag{5}\]

Proof.: Miller [57] provides a detailed proof of this theorem. 

**Lemma 3**.: _Let \(\mathrm{F}\sim\mathrm{P}_{\mathbf{\theta}}\), and \(\mathrm{X}_{1},\mathrm{X}_{2},\ldots\) i.i.d \(\sim\mathrm{P}_{\mathbf{\theta}}^{\mathrm{f}}\). Assume Conditions 1 to 3 and let \(\{\int|\log p_{\mathbf{\theta}}(\mathrm{x}^{m}\mid\mathrm{f})|d\mathrm{P}_{\mathbf{ \theta}}(\mathrm{f})<\infty:\forall\mathrm{x}^{m}\in\mathcal{X}^{m}\}\). Then,_

\[g_{\mathbf{\theta}}(\mathrm{x},\mathrm{F})=-\frac{1}{|\mathrm{x}|}\log p_{\mathbf{ \theta}}(\mathrm{x}\mid\mathrm{F})=-\frac{1}{|\mathrm{x}|}\log p_{\mathbf{\theta }}(\mathrm{x}\mid\mathrm{X}^{\infty})=g_{\mathbf{\theta}}(\mathrm{x},\mathrm{X}^{ \infty}).\]

Proof.: \[p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{F}) =\lim_{n\rightarrow\infty}\int p_{\mathbf{\theta}}(\mathrm{x}\mid \mathrm{f})d\mathrm{P}_{\mathbf{\theta}}(\mathrm{f}\mid\mathrm{X}_{1},\ldots, \mathrm{X}_{n})\] \[=\lim_{n\rightarrow\infty}\int p_{\mathbf{\theta}}(\mathrm{x}\mid \mathrm{f},\mathrm{X}_{1},\ldots,\mathrm{X}_{n})d\mathrm{P}_{\mathbf{\theta}}( \mathrm{f}\mid\mathrm{X}_{1},\ldots,\mathrm{X}_{n})\] \[=\lim_{n\rightarrow\infty}p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{X }_{1},\ldots,\mathrm{X}_{n})\]

\[g(\mathrm{x},\mathrm{F}) =-\frac{1}{|\mathrm{x}|}\log p_{\mathbf{\theta}}(\mathrm{x}\mid \mathrm{F})\] \[=-\frac{1}{|\mathrm{x}|}\log\lim_{n\rightarrow\infty}p_{\mathbf{ \theta}}(\mathrm{x}\mid\mathrm{X}_{1},\ldots,\mathrm{X}_{n})\] \[=\lim_{n\rightarrow\infty}-\frac{1}{|\mathrm{x}|}\log p_{\mathbf{ \theta}}(\mathrm{x}\mid\mathrm{X}_{1},\ldots,\mathrm{X}_{n})\] \[=-\frac{1}{|\mathrm{x}|}\log p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{ X}^{\infty})\] \[=g(\mathrm{x},\mathrm{X}^{\infty})\]

**Theorem 2**.: _Under the conditions of Lemma 3,_

\[p_{\text{ppc}}=p_{\text{mpc}}\]Proof.: Define an alternative probability model such that \(\mathrm{F}\sim\mathrm{P}_{\mathbf{\theta}}^{\mathrm{x}^{n}}\) and \(\mathrm{X}_{1},\mathrm{X}_{2},\dots\) i.i.d \(\sim\mathrm{P}_{\mathbf{\theta}}^{\mathrm{f}_{\mathbf{\theta}}\mathrm{x}^{n}}\). Let \(p_{a}\), \(P_{b}\), and \(g_{a}\) denote the relevant quantities respecting this model. For example, \(p_{a}(\mathrm{y}\mid\mathrm{x})=p_{\mathbf{\theta}}(\mathrm{y}\mid\mathrm{x}, \mathrm{x}^{n})\) and \(P_{a}(\mathrm{f})=\mathrm{P}_{\mathbf{\theta}}(\mathrm{f}\mid\mathrm{x}^{n})\). Note that \(p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{f})=p_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{ f},\mathrm{x}^{n})=p_{a}(\mathrm{x}\mid\mathrm{f})\) since X and X\({}^{n}\) are independent when f is known.

\[p_{\text{ppc}} =\iint\mathbb{1}\left\{g_{\mathbf{\theta}}(\mathrm{x},\mathrm{f}) \geq g_{\mathbf{\theta}}(\mathrm{x}^{\text{test}},\mathrm{f})\right\}dP_{\mathbf{ \theta}}(\mathrm{x}\mid\mathrm{f})dP_{\mathbf{\theta}}(\mathrm{f}\mid\mathrm{x}^{n})\] \[=\iint\mathbb{1}\left\{g_{a}(\mathrm{x},\mathrm{f})\geq g_{a}( \mathrm{x}^{\text{test}},\mathrm{f})\right\}dP_{a}(\mathrm{x},\mathrm{f})\] \[=\iint\mathbb{1}\left\{g_{a}(\mathrm{x},\mathrm{f})\geq g_{a}( \mathrm{x}^{\text{test}},\mathrm{f})\right\}dP_{a}(\mathrm{x},\mathrm{f}, \mathrm{x}^{n+1:\infty})\] \[=\iint\mathbb{1}\left\{g_{a}(\mathrm{x},\mathrm{f})\geq g_{a}( \mathrm{x}^{\text{test}},\mathrm{f})\right\}dP_{a}(\mathrm{x},\mid\mathrm{f}, \mathrm{x}^{n+1:\infty})dP_{a}(\mathrm{f},\mathrm{x}^{n+1:\infty})\] \[=\iint\mathbb{1}\left\{g_{\mathbf{\theta}}(\mathrm{x},\mathrm{x}^{n +1:\infty})\geq g_{\mathbf{\theta}}(\mathrm{x}^{\text{test}},\mathrm{x}^{\infty}) \right\}dP_{\mathbf{\theta}}(\mathrm{x}\mid\mathrm{x}^{\infty})dP_{\mathbf{\theta}}( \mathrm{x}^{n+1:\infty}\mid\mathrm{x}^{n})\] \[=p_{\text{mpc}}\]

## Appendix C Posterior predictive p-value algorithm

Algorithm 2 details the procedure for calculating the posterior predictive p-value in Equation (1) given train data \(\mathrm{x}^{n}\), test data \(\mathrm{x}^{\text{test}}\), a discrepancy function \(g_{\mathbf{\theta}}(\mathrm{x},\mathrm{f})\), and the nuber of replication datasets to generate \(M\).

```
1:data \(\{\mathrm{x}^{n},\mathrm{x}^{\text{test}}\}\), discrepancy function \(g_{\mathbf{\theta}}(\mathrm{x},\mathrm{f})\), # replicates \(\mathrm{M}\)
2:for\(i\gets 1\) to \(\mathrm{M}\)do\(\triangleright\) sample explanation f
3:\(\mathrm{f}_{i}\sim p_{\mathbf{\theta}}(\mathrm{f}\mid\mathrm{x}^{n})\)\(\triangleright\) initialize replicant data
4:for\(j\gets 1\) to \(n\)do\(\triangleright\) sample example from model likelihood
5:\(\mathrm{x}_{i}\leftarrow(\mathrm{x}_{i},\mathrm{z}_{j},\mathrm{y}_{j})\)\(\triangleright\) update replicant data
6:return\(\frac{1}{\mathrm{M}}\sum_{i=1}^{\mathrm{M}}\mathbb{1}\left\{g_{\mathbf{\theta}}( \mathrm{x}_{i},\mathrm{f}_{i})\geq g_{\mathbf{\theta}}(\mathrm{x}^{\text{test}}, \mathrm{f}_{i})\right\}\)\(\triangleright\) estimate \(p\)-value
```

**Algorithm 2**\(\widehat{p}_{\text{ppc}}\)

## Appendix D Lite generative predictive p-value algorithm

```
1:data \(\{\mathrm{x}^{n},\mathrm{x}^{\text{test}}\}\), a discrepancy function \(g_{\mathbf{\theta}}(\mathrm{x},\mathrm{x}^{n})\), # replicates \(\mathrm{M}\)
2:for\(i\gets 1\) to \(\mathrm{M}\)do\(\triangleright\) initialize replicant data
3:\(\mathrm{x}_{i}\leftarrow()\)\(\triangleright\) sample example from model
4:\(\mathrm{x}_{i}\leftarrow(\mathrm{x}_{i},\mathrm{z}_{j},\mathrm{y}_{j})\)\(\triangleright\) update replicant data
5:return\(\frac{1}{\mathrm{M}}\sum_{i=1}^{\mathrm{M}}\mathbb{1}\left\{g_{\mathbf{\theta}}( \mathrm{x}_{i},\mathrm{x}^{n})\geq g_{\mathbf{\theta}}(\mathrm{x}^{\text{test}}, \mathrm{x}^{n})\right\}\)\(\triangleright\) estimate \(p\)-value
```

**Algorithm 3**\(\widehat{p}_{\text{gpc}}^{\text{lie}}\)

Algorithm 3 summarizes a "lite" version of the estimator that forgoes approximate sampling from the model posterior and likelihood. Instead, it samples replication data directly from the model predictive 

[MISSING_PAGE_EMPTY:17]

hand side of Figure 16. We again see in the Recall curve that the NLL discrepancy leads to a more sensitive predictor than the NLML discrepancy. The F1 and Accuracy curves show that the NLML based \(p\)-value leads to a much more effective predictor for Gemma-2 9B.

Figure 16: Natural language model suitability detection ablation. Precision, recall, F1, and accuracy metrics vs. number of in-context examples. SST2 ICL datasets are taken to be in-capability for Gemma-2 9b. MQP ICL datasets are taken to be out-of-capability for Gemma-2 9b.

Figure 15: Natural language task with Gemma-2 9B. The generative predictive \(p\)-value against dataset size \(n\) and it’s relationship to the false positive rate. The first figure shows the generative predictive \(p\)-values, and the second and third figures show the false positive rate with the NLML and NLL discrepancy functions, respectively.