# Byzantine-Tolerant Methods for Distributed

Variational Inequalities

 Nazarii Tupitsa

MBZUAI, MIPT

Abdulla Jasem Almansoori

MBZUAI

Yanlin Wu

MBZUAI

Martin Takac

MBZUAI

Karthik Nandakumar

MBZUAI

Samuel Horvath

MBZUAI

Eduard Gorbunov

MBZUAI

Corresponding author: eduard.gorbunov@mbzuai.ac.ae

###### Abstract

Robustness to Byzantine attacks is a necessity for various distributed training scenarios. When the training reduces to the process of solving a minimization problem, Byzantine robustness is relatively well-understood. However, other problem formulations, such as min-max problems or, more generally, variational inequalities, arise in many modern machine learning and, in particular, distributed learning tasks. These problems significantly differ from the standard minimization ones and, therefore, require separate consideration. Nevertheless, only one work (Adibi et al., 2022) addresses this important question in the context of Byzantine robustness. Our work makes a further step in this direction by providing several (provably) Byzantine-robust methods for distributed variational inequality, thoroughly studying their theoretical convergence, removing the limitations of the previous work, and providing numerical comparisons supporting the theoretical findings.

## 1 Introduction

Modern machine learning tasks require to train large models with billions of parameters on huge datasets to achieve reasonable quality. Training of such models is usually done in a distributed manner since otherwise it can take a prohibitively long time (Li, 2020). Despite the attractiveness of distributed training, it is associated with multiple difficulties not existing in standard training.

In this work, we focus on one particular aspect of distributed learning - _Byzantine tolerance/robustness_ - the robustness of distributed methods to the presence of _Byzantine workers_2, i.e., such workers that can send incorrect information (maliciously or due to some computation errors/faults) and are assumed to be omniscient. For example, this situation can appear in collaborative training, when several participants (companies, universities, individuals) that do not necessarily know each other train some model together (Kijsipongse et al., 2018; Diskin et al., 2021) or when the devices used in training are faulty (Ryabinin et al., 2021). When the training reduces to the distributed _minimization_ problem, the question of Byzantine robustness is studied relatively well both in theory and practice (Karimireddy et al., 2022; Lyu et al., 2020).

Footnote 2: The term “Byzantine workers” is a standard term for the field (Lamport et al., 1982; Lyu et al., 2020). We do not aim to offend any group of people but rather use common terminology.

However, there are a lot of problems that cannot be reduced to minimization, e.g., adversarial training (Goodfellow et al., 2015; Madry et al., 2018), generative adversarial networks (GANs) (Goodfellow et al., 2014), hierarchical reinforcement learning (Wayne and Abbott, 2014; Vezhnevets et al., 2017), adversarial examples games (Bose et al., 2020), and other problems arising in game theory, control theory, and differential equations (Facchinei and Pang, 2003). Such problems lead to min-maxor, more generally, variational inequality (VI) problems (Gidel et al., 2018) that have significant differences from minimization ones and require special consideration (Harker and Pang, 1990; Ryu and Yin, 2022). Such problems can also be huge scale, meaning that, in some cases, one has to solve them distributedly. Therefore, similarly to the case of minimization, the necessity in Byzantine-robust methods for distributed VIs arises.

The only existing work addressing this problem is (Adibi et al., 2022), where the authors propose the first Byzantine-tolerant distributed method for min-max and VI problems called Robust Distributed Extragradient (RDEG). However, several interesting directions such as application of \((\delta,c)\)-robust aggregation rules, client momentum (Karimireddy et al., 2021), and checks of computations (Gorbunov et al., 2022b) studied for minimization problems are left unexplored in the case of VIs. Moreover, (Adibi et al., 2022) prove the convergence to the solution's neighborhood that can be reduced only via increasing the batchsize and rely on the assumption that the number of workers is sufficiently large and the fraction of Byzantine workers is smaller than \(\nicefrac{{1}}{{16}}\), which is much smaller than for SOTA results in minimization case. _Our work closes these gaps in the literature and resolves the limitations of the results from (Adibi et al., 2022)._

### Setting

To make the further presentation precise, we need to introduce the problem and assumptions we make. We consider the distributed unconstrained variational inequality (non-linear equation) problem3:

Footnote 3: We assume that the problem (1) has a unique solution \(\mathbf{x}^{*}\). This assumption can be relaxed, but for simplicity of exposition, we enforce it.

\[\text{find}\ \mathbf{x}^{*}\in\mathbb{R}^{d}\ \text{such that}\ F(\mathbf{x}^{*})=0,\ \text{where}\ F(\mathbf{x}):=\frac{1}{G}\sum_{i\in\mathbb{G}}F_{i}(\mathbf{x}), \tag{1}\]

where \(\mathbb{G}\) denotes the set of regular/good workers and operators \(F_{i}\) have an expectation form \(F_{i}(\mathbf{x}):=\mathbb{E}_{\mathbf{\xi}_{i}}[\mathbf{g}_{i}(\mathbf{x};\mathbf{\xi}_{i})]\). We assume that \(n\) workers connected with a server take part in the learning/optimization process and \([n]=\mathbb{G}\sqcup\mathcal{B}\), where \(\mathcal{B}\) is the set of _Byzantine workers_ - the subset \(\mathcal{B}\) of workers \([n]\) that can deviate from the prescribed protocol (send incorrect information, e.g., arbitrary vectors instead of stochastic estimators) either intentionally or not and are _omniscient4_, i.e., Byzantine workers can know the results of computations on regular workers and the aggregation rule used by the server. The number of Byzantine workers \(B=|\mathcal{B}|\) is assumed to satisfy \(B\leq\delta n\), where \(\delta<\nicefrac{{1}}{{2}}\) (otherwise Byzantines form a majority and the problem becomes impossible to solve). The number of regular workers is denoted as \(G=|G|\).

Footnote 4: This assumption gives Byzantine workers a lot of power and rarely holds in practice. Nevertheless, if the algorithm is robust to such workers, then it is provably robust to literally any type of workers deviating from the protocol.

Assumptions.Here, we formulate the assumptions related to the stochasticity and properties of operators \(\{F_{i}\}_{i\in\mathbb{G}}\).

**Assumption 1**.: _For all \(i\in\mathbb{G}\) the stochastic estimator \(\mathbf{g}_{i}(\mathbf{x},\mathbf{\xi}_{i})\) is an unbiased estimator of \(F_{i}(\mathbf{x})\) with bounded variance, i.e., \(\mathbb{E}_{\mathbf{\xi}_{i}}[\mathbf{g}_{i}(\mathbf{x},\mathbf{\xi}_{i})]=F_{i}(\mathbf{x})\) and for some \(\sigma\geq 0\)_

\[\mathbb{E}_{\mathbf{\xi}_{i}}\left[\left\|\mathbf{g}_{i}(\mathbf{x},\mathbf{\xi}_{i})-F_{i}( \mathbf{x})\right\|^{2}\right]\leq\sigma^{2}. \tag{2}\]

The above assumption is known as the bounded variance assumption. It is classical for the analysis of stochastic optimization methods (Nemirovski et al., 2009; Juditsky et al., 2011) and is used in the majority of existing works on Byzantine robustness with theoretical convergence guarantees.

Further, we assume that the data heterogeneity across the workers is bounded.

**Assumption 2**.: _There exists \(\zeta\geq 0\) such that for all \(\mathbf{x}\in\mathbb{R}^{d}\)_

\[\frac{1}{G}\sum_{i\in Q}\lVert F_{i}(\mathbf{x})-F(\mathbf{x})\rVert^{2}\leq\zeta^{2}. \tag{3}\]

Condition (3) is a standard notion of data heterogeneity in Byzantine-robust distributed optimization (Wu et al., 2020; Zhu and Ling, 2021; Karimireddy et al., 2022; Gorbunov et al., 2023a). It is worth mentioning that without any kind of bound on the heterogeneity of \(\{F_{i}\}_{i\in\mathbb{G}}\), it is impossible to tolerate Byzantine workers. In addition, homogeneous case (\(\zeta=0\)) is also very important and arises in collaborative learning, see (Kijispongse et al., 2018; Diskin et al., 2021).

Finally, we formulate here several assumptions on operator \(F\). Each particular result in this work relies only on a subset of listed assumptions.

**Assumption 3**.: _Operator \(F:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is \(L\)-Lipschitz, i.e.,_

\[\|F(\mathbf{x})-F(\mathbf{y})\|\leq L\|\mathbf{x}-\mathbf{y}\|,\quad\forall\ \mathbf{x},\mathbf{y}\in \mathbb{R}^{d}.\] (Lip)

**Assumption 4**.: _Operator \(F:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is \(\mu\)-quasi strongly monotone, i.e., for \(\mu\geq 0\)_

\[\langle F(\mathbf{x}),\mathbf{x}-\mathbf{x}^{*}\rangle\geq\mu\|\mathbf{x}-\mathbf{x}^{*}\|^{2}, \quad\forall\ \mathbf{x}\in\mathbb{R}^{d}.\] (QSM)

**Assumption 5**.: _Operator \(F:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is monotone, i.e.,_

\[\langle F(\mathbf{x})-F(\mathbf{y}),\mathbf{x}-\mathbf{y}\rangle\geq 0,\quad\forall\ \mathbf{x},\mathbf{y}\in\mathbb{R}^{d}.\] (Mon)

**Assumption 6**.: _Operator \(F:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is \(\ell\)-star-cocoercive, i.e., for \(\ell\geq 0\)_

\[\langle F(\mathbf{x}),\mathbf{x}-\mathbf{x}^{*}\rangle\geq\frac{1}{\ell}\|F(\mathbf{x})\|^{2},\quad\forall\ \mathbf{x}\in\mathbb{R}^{d}.\] (SC)

Assumptions 3 and 5 are quite standard for the literature on VIs. Assumptions 4 and 6 can be seen as structured non-monotonicity assumptions. Indeed, there exist examples of non-monotone (and even non-Lipschitz) operators such that Assumptions 4 and 6 holds (Loizou et al., 2021). However, Assumptions 3 and 5 imply neither (QSM) nor (SC). It is worth mentioning that Assumption 4 is also known under different names, i.e., strong stability (Mertikopoulos and Zhou, 2019) and strong coherent (Song et al., 2020) conditions.

Robust aggregation.We use the formalism proposed by Karimireddy et al. (2021, 2022).

**Definition 1.1** (\((\delta,c)\)-Ragg(Karimireddy et al., 2021, 2022)).: _Let there exist a subset \(\mathcal{G}\) of random vectors \(\{\mathbf{y}_{1},\ldots,\mathbf{y}_{n}\}\) such that \(G\geq(1-\delta)n\) for some \(\delta<\nicefrac{{1}}{{2}}\) and \(\mathbb{E}\|\mathbf{y}_{i}-\mathbf{y}_{j}\|^{2}\leq\rho^{2}\) for any fixed pair \(i,j\in\mathcal{G}\) and some \(\rho\geq 0\). Then, \(\tilde{\mathbf{y}}=\texttt{Ragg}(\mathbf{y}_{1},\ldots,\mathbf{y}_{n})\) is called \((\delta,c)\)-robust aggregator if for some constant \(c\geq 0\)_

\[\mathbb{E}\left[\|\tilde{\mathbf{y}}-\overline{\mathbf{y}}\|^{2}\right]\leq c\delta\rho ^{2}, \tag{4}\]

_where \(\overline{\mathbf{y}}=\frac{1}{G}\sum_{i\in\mathcal{G}}y_{i}\). Further, if the value of \(\rho\) is not used to compute \(\widehat{\mathbf{y}}\), then \(\widehat{\mathbf{y}}\) is called agnostic \((\delta,c)\)-robust aggregator and denoted as \(\widehat{\mathbf{y}}=\texttt{ARagg}(\mathbf{y}_{1},\ldots,\mathbf{y}_{n})\)._

The above definition is tight in the sense that for any estimate \(\widehat{\mathbf{y}}\) the best bound one can guarantee is \(\mathbb{E}\left[\|\widehat{\mathbf{y}}-\overline{\mathbf{y}}\|^{2}\right]=\Omega( \delta\rho^{2})\)(Karimireddy et al., 2021). Moreover, there are several examples of \((\delta,c)\)-robust aggregation rules that work well in practice; see Appendix B.

Another important concept for Byzantine-robust learning is the notion of permutation invariance.

**Definition 1.2** (Permutation invariant algorithm).: _Define the set of stochastic gradients computed by each of the \(n\) workers at some round \(t\) to be \([\tilde{\mathbf{g}}_{1,t},\ldots,\tilde{\mathbf{g}}_{n,t}]\). For a good worker \(i\in\mathcal{G}\), these represent the true stochastic gradients whereas for a bad worker \(j\in\mathcal{B}\), these represent arbitrary vectors. The output of any optimization algorithm \(\texttt{Alg}\) is a function of these gradients. A permutation-invariant algorithm is one which for any set of permutations over \(t\) rounds \(\{\pi_{1},\ldots,\pi_{t}\}\), its output remains unchanged if we permute the gradients._

\[\texttt{Alg}\begin{pmatrix}[\tilde{\mathbf{g}}_{1,1},...,\tilde{\mathbf{g}}_{n,1}],\\...\\ [\tilde{\mathbf{g}}_{1,t},...,\tilde{\mathbf{g}}_{n,t}]\end{pmatrix}=\texttt{Alg} \begin{pmatrix}[\tilde{\mathbf{g}}_{\pi_{1}(1),1},...,\tilde{\mathbf{g}}_{\pi_{1}(n),1}], \\...\\ [\tilde{\mathbf{g}}_{\pi_{1}(1),t},...,\tilde{\mathbf{g}}_{\pi_{t}(n),t}]\end{pmatrix}\]

As Karimireddy et al. (2021) prove, any permutation-invariant algorithm fails to converge to any predefined accuracy of the solution (under Assumption 1) even if all regular workers have the same operators/functions, i.e., even when \(\zeta=0\).

### Our Contributions

Now we are ready to describe the main contributions of this work.

\(\bullet\)**Methods with provably robust aggregation.** We propose new methods called Stochastic Gradient Descent-Ascent and Stochastic Extragradient with Robust Aggregation (SGDA-RA and SEG-RA) - variants of popular SGDA [17, 16] and SEG [15, 14]. We prove that SGDA-RA and SEG-RA work with any \((\delta,c)\)-robust aggregation rule and converge to the desired accuracy _if the batchsize is large enough_. In experiments, we observe that SGDA-RA and SEG-RA outperform RDEG in several cases.

\(\bullet\)**Client momentum.** As the next step, we add client momentum to SGDA-RA and propose Momentum SGDA-RA (M-SGDA-RA). As it is shown by [11, 16], client momentum helps to break the permutation invariance of the method and ensures convergence to any predefined accuracy with any batchsize for _non-convex minimization problems_. In the case of star-cocoercive quasi-strongly monotone VIs, we prove the convergence to the neighborhood of the solution; the size of the neighborhood can be reduced via increasing batchsize only - similarly to the results for RDEG, SGDA-RA, and SEG-RA. We discuss this limitation in detail and point out the non-triviality of this issue. Nevertheless, we show in the experiments that client momentum does help to achieve better accuracy of the solution.

\(\bullet\)**Methods with random checks of computations.** Finally, for homogeneous data case (\(\zeta=0\)), we propose a version of SGDA and SEG with random checks of computations (SGDA-CC, SEG-CC and their restarted versions - R-SGDA-CC and R-SEG-CC). We prove that the proposed methods converge _to any accuracy of the solution without any assumptions on the batchsize_. This is the first result of this type on Byzantine robustness for distributed VIs. Moreover, when the target accuracy of the solution is small enough, the obtained convergence rates for R-SGDA-CC and R-SEG-CC are not worse than the ones for distributed SGDA and SEG derived in the case of \(\delta=0\) (no Byzantine workers); see the comparison of the convergence rates in Table 1. In the numerical experiments, we consistently observe the superiority of the methods with checks of computations to the previously proposed methods.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Setup & Method & Citation & Metric & Complexity & BS \\ \hline \multirow{4}{*}{SC, QSM} & SGDA-RA & Cor. 1 & & \(\frac{\ell}{\mu}+\frac{1}{c\text{obs}}\) & \(\frac{\alpha\delta\sigma^{2}}{\mu\sigma^{2}}\) \\  & M-SGDA-RA & Cor. 4 & & \(\frac{\ell^{2}\mu\sigma^{2}+\frac{1}{\mu\sigma^{2}}}{\mu\sigma^{2}\sigma^{2}}\) & \(\frac{\ell}{\mu}+\frac{\sigma^{2}\mu\sigma^{2}}{\mu\sigma^{2}\sigma^{2}}+\frac{ \sigma^{2}\mu^{2}\sigma^{2}}{\mu\sigma^{2}\sigma^{2}}\) & 1 \\  & SEG-CC & Cor. 6 & & \(\frac{\ell}{\mu}+\frac{\sigma^{2}\mu\sigma^{2}}{\mu\sigma^{2}}+\frac{\sigma^{2 }\mu^{2}\sigma^{2}}{\mu\sigma^{2}\sigma^{2}\sigma^{2}}\) & 1 \\  & R-SGDA-CC & Cor. 8 & & \(\frac{\ell}{\mu}+\frac{\sigma^{2}\mu}{\mu\sigma^{2}}+\frac{\mu^{2}\sigma^{2}}{ \mu\sqrt{2\mu}}\) & 1 \\ \hline \multirow{4}{*}{Lip, QSM} & SEG-RA & Cor. 3 & & & \(\frac{\ell\sigma^{2}}{\mu\sigma^{2}}+\frac{1}{\beta}\frac{1}{\beta\sigma\sigma^{2 }}+\frac{\sigma^{2}\mu^{2}}{\mu\sigma^{2}\sigma^{2}}+\frac{\sigma^{2}\mu^{2}}{ \mu\sigma^{2}\sigma^{2}}+\frac{\sigma^{2}\mu^{2}\sigma^{2}}{\mu\sigma^{2}\sigma ^{2}\sigma^{2}\sigma^{2}\sigma^{2}\sigma^{2}\sigma^{2}\sigma^{2}\sigma^{2}}\) & \(\frac{\ell}{\mu}+\frac{\sigma^{2}\mu}{\mu\sigma^{2}}+\frac{\sigma^{2}\mu^{2}}{ \mu\sigma^{2}\sigma^{2}\sigma^{2}\sigma^{2}\sigma^{2}\sigma^{2}\sigma^{2}}\) & 1 \\  & R-SEG-CC & Cor. 11 & & & \(\frac{\ell}{\mu}+\frac{\sigma^{2}}{\mu\sigma^{2}}+\frac{\sigma^{2}\mu^{2}}{ \mu\sigma^{2}}\) & 1 \\ \hline \multirow{4}{*}{Lip, QSM} & RDEG & Adibi et al. [16]\(\|\mathbf{x}^{T}-\mathbf{x}^{*}\|^{2}\) & \(\frac{L}{\mu}\) & \(\frac{\sigma^{2}\mu^{2}R^{2}}{L^{2}\xi^{2}}\) \\ \cline{1-1}  & & & & & \\ \hline \hline \end{tabular} \({}^{(1)}\) consider only homogeneous case (\(\zeta=0\)).

\end{table}
Table 1: Summary of known and new complexity results for Byzantine-robust methods for distributed variational inequalities. Column “Setup” indicates the varying assumptions. By the complexity, we mean the number of stochastic oracle calls needed for a method to guarantee that Metric \(\leq\varepsilon\) (for RDEG \(\mathbf{P}\{\text{Metric}\leq\varepsilon\}\geq 1-\delta_{\text{RDEG}}\), \(\delta_{\text{RDEG}}\in(0,1]\)) and “Metric” is taken from the corresponding column. For simplicity, we omit numerical and logarithmic factors in the complexity bounds. Column “BS” indicates the minimal batch-size used for achieving the corresponding complexity. Notation: \(c,\delta\) are robust aggregator parameters; \(\alpha\) = momentum parameter; \(\beta\) = ratio of inner and outer stepsize in SEG-like methods; \(n\) = total numbers of peers; \(m\) = number of checking peers; \(G\) = number of peers following the protocol; \(R\) = any upper bound on \(\|\mathbf{x}^{0}-\mathbf{x}^{n}\|\); \(\mu\) = quasi-strong monotonicity parameter; \(\ell\) = star-cocoercivity parameter; \(L\) = Lipschitzness parameter; \(\sigma^{2}\) = bound on the variance. The definition \(x^{T}\) can vary; see corresponding theorems for the exact formulas.

### Related Work

Byzantine-robust methods for minimization problems.Classical distributed methods like Parallel SGD[22] cannot tolerate even one Byzantine worker. The most evident vulnerability of such methods is an aggregation rule (averaging). Therefore, many works focus on designing and application of different aggregation rules to Parallel SGD-like methods [23, 24, 25, 26, 27]. However, this is not sufficient for Byzantine robustness: there exist particular attacks [1, 25] that can bypass popular defenses. [17] formalize the definition of robust aggregation (see Definition 1.1), show that many standard aggregation rules are non-robust according to that definition, and prove that any permutation-invariant algorithm with a fixed batchsize can converge only to the ball around the solution with algorithm-independent radius. Therefore, more in-depth algorithmic changes are required that also explain why RDEG, SGDA-RA, and SEG-RA are not converging to any accuracy without increasing batchsize.

One possible way to resolve this issue is to use client momentum [17, 23] that breaks permutation-invariance and allows for convergence to any accuracy. It is also worth mentioning a recent approach by [13], who propose an alternative definition of robust aggregation to the one considered in this paper, though to achieve the convergence to any accuracy in the homogeneous case [13] apply client momentum like in [17, 23]. Another line of work achieves Byzantine robustness through the variance reduction mechanism [25, 26]. Finally, for the homogeneous data case, one can apply validation test [17, 23] or checks of computations [11]. For the summary of other advances, we refer to [18].

Methods for min-max and variational inequalities problems.As mentioned before, min-max/variational inequalities (VIs) problems have noticeable differences with standard minimization. In particular, it becomes evident from the differences in the algorithms' behavior. For example, a direct analog of Gradient Descent for min-max/VIs - Gradient Descent-Ascent (GDA) [18, 25, 26] - fails to converge for a simple bilinear game. Although GDA converges for a different class of problems (cocoercive/star-cocoercive ones) and its version with alternating steps works well in practice and even provably converges locally [23], many works focus on Extragradient (EG) type methods [19, 25] due to their provable convergence for monotone Lipschitz problems and beyond [19]. Stochastic versions of GDA and EG (SGDA and SEG) are studied relatively well, e.g., see [15, 26, 27, 28] for the recent advances.

On the results from [1].In the context of Byzantine robustness for distributed min-max/VIs, the only existing work is [1]. The authors propose a method called Robust Distributed Extragradient (RDEG) - a distributed version of EG that uses a univariate trimmed-mean estimator from [26] for aggregation. This estimator satisfies a similar property to (4) that is shown for \(\delta<\nicefrac{{1}}{{16}}\) and large enough \(n\) (see the discussion in Appendix B). In contrast, the known \((\delta,c)\)-robust aggregation rules allow larger \(\delta\), and do not require large \(n\). Despite these evident theoretical benefits, such aggregation rules were not considered in prior works on Byzantine robustness for distributed variational inequalities/min-max problems.

## 2 Main Results

In this section, we describe three approaches proposed in this work and formulate our main results.

### Methods with Robust Aggregation

We start with the Stochastic Gradient Descent-Accent with \((\delta,c)\)-robust aggregation (SGDA-RA):

\[\mathbf{x}^{t+1}=\mathbf{x}^{t}-\gamma\text{RAGG}(\mathbf{g}_{1}^{t},\ldots,\mathbf{g}_{n}^{t} ),\ \ \text{where}\ \mathbf{g}_{i}^{t}=\mathbf{g}_{i}(\mathbf{x}^{t},\mathbf{\xi}_{i}^{t})\ \ \forall\ i\in\mathcal{G}\ \ \text{and}\ \ \mathbf{g}_{i}^{t}=*\ \ \forall\ i\in\mathcal{B},\]

where \(\{\mathbf{g}_{i}^{t}\}_{i\in\mathcal{G}}\) are sampled independently. The main result for SGDA-RA is given below.

**Theorem 1**.: _Let Assumptions 1, 2, 4 and 6 hold. Then after \(T\) iterations_ SGDA-RA _(Algorithm 1) with \((\delta,c)\)_-Ragg and \(\gamma\leq\frac{1}{2\ell}\) outputs \(\mathbf{x}^{T}\) such that_

\[\mathbb{E}\big{\|}\mathbf{x}^{T}-\mathbf{x}^{*}\big{\|}^{2}\leq\Big{(}1- \frac{\gamma\mu}{2}\Big{)}^{T}\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2}+\frac{ 2\gamma\sigma^{2}}{\mu G}+\frac{2\gamma c\delta(24\sigma^{2}+12\zeta^{2})}{\mu }+\frac{c\delta(24\sigma^{2}+12\zeta^{2})}{\mu^{2}}.\]

The first two terms in the derived upper bound are standard for the results on SGDA under Assumptions 1, 4, and 6, e.g., see (Beznosikov et al., 2023). The third and the fourth terms come from the presence of Byzantine workers and robust aggregation since the existing \((\delta,c)\)-robust aggregation rules explicitly depend on \(\delta\). The fourth term cannot be reduced without increasing batchsize even when \(\zeta=0\) (homogeneous data case). This is expected since SGDA-RA is permutation invariant. When \(\sigma=0\) (regular workers compute full operators), then SGDA-RA converges linearly to the ball centered at the solution with radius \(\Theta(\sqrt{c\delta}/\mu)\) that matches the lower bound from (Karimireddy et al., 2022). In contrast, the known results for RDEG are derived for homogeneous data case (\(\zeta=0\)). The proof of Theorem 1 is deferred to Appendix D.1.

Using a similar approach we also propose a version of Stochastic Extragradient method with \((\delta,c)\)-robust aggregation called SEG-RA:

\[\mathbf{\widetilde{x}}^{t} =\mathbf{x}^{t}-\gamma_{1}\text{Ragg}(\mathbf{g}_{\mathbf{z}_{t}}^{t},\ldots, \mathbf{g}_{\mathbf{z}_{n}}^{t}),\ \ \text{where}\ \mathbf{g}_{\mathbf{z}_{t}}^{t}=\mathbf{g}_{i}(\mathbf{x}^{t}, \mathbf{\xi}_{i}^{t}),\ \ \forall\ i\in\mathbb{G}\ \text{ and }\ \mathbf{g}_{\mathbf{z}_{t}}^{t}=*\ \ \forall\ i\in\mathbb{B},\] \[\mathbf{x}^{t+1} =\mathbf{x}^{t}-\gamma_{2}\text{Ragg}(\mathbf{g}_{\mathbf{\eta}_{1}}^{t}, \ldots,\mathbf{g}_{\mathbf{\eta}_{n}}^{t}),\ \ \text{where}\ \mathbf{g}_{\mathbf{\eta}_{i}}^{t}=\mathbf{g}_{i}(\mathbf{\widetilde{x}}^{t},\mathbf{\eta}_{i}^{t}),\ \ \forall\ i\in\mathbb{G}\ \text{ and }\mathbf{g}_{\mathbf{\eta}_{i}}^{t}=*\ \ \forall\ i\in\mathbb{B},\]

where \(\{\mathbf{g}_{\mathbf{\eta}_{i}}^{t}\}_{i\in\mathbb{G}}\) and \(\{\mathbf{g}_{\mathbf{\eta}_{i}}^{t}\}_{i\in\mathbb{G}}\) are sampled independently. Our main convergence result for SEG-RA is presented in the following theorem; see Appendix D.2 for the proof.

**Theorem 2**.: _Let Assumptions51, 2, 3 and 4 hold. Then after \(T\) iterations_ SEG-RA _(Algorithm 2) with \((\delta,c)\)_-Ragg, \(\gamma_{1}\leq\frac{1}{2\mu+2L}\) and \(\beta=\nicefrac{{\gamma_{2}}}{{\gamma_{1}}}\leq\nicefrac{{1}}{{4}}\) outputs \(\mathbf{x}^{T}\) such that_

Footnote 5: SGDA-based and SEG-based methods are typically analyzed under different assumptions. Although (SC) follows from (Lip) and (QSM) with \(\ell=\nicefrac{{L^{2}}}{{\mu}}\), some operators may satisfy (SC) with significantly smaller \(\ell\). Next, when \(\mu=0\), SGDA is not guaranteed to converge (Gidel et al., 2018), while SEG does

\[\mathbb{E}\big{\|}\mathbf{x}^{T}-\mathbf{x}^{*}\big{\|}^{2}\leq\ \left(1-\frac{\mu\beta\gamma_{1}}{4}\right)^{T}\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*} \big{\|}^{2}+\frac{8\gamma_{1}\sigma^{2}}{\mu\beta G}+8c\delta(24\sigma^{2}+12 \zeta^{2})\bigg{(}\frac{\gamma_{1}}{\beta\mu}+\frac{2}{\mu^{2}}\bigg{)}.\]

Similar to the case of SGDA-RA, the bound for SEG-RA has the term that cannot be reduced without increasing batchsize even in the homogeneous data case. RDEG, which is also a modification of SEG, has the same linearly convergent term, but SEG-RA has a better dependence on the batchsize, needed to obtain the convergence to any predefined accuracy, that is \(\Theta(\varepsilon^{-1})\) versus \(\Theta(\varepsilon^{-2})\) for RDEG; see Cor. 3.

In heterogeneous case when \(\sigma=0\), SEG-RA also converges linearly to the ball centered at the solution with radius \(\Theta(\sqrt{c\delta}/\mu)\) that matches the lower bound.

### Client Momentum

Next, we focus on the version of SGDA-RA that utilizes worker momentum \(\mathbf{m}_{i}^{t}\), i.e.,

\[\mathbf{x}^{t+1}=\mathbf{x}^{t}-\gamma\text{Ragg}(\mathbf{m}_{1}^{t},\ldots,\mathbf{m}_{n}^{t} ),\ \ \text{with}\ \mathbf{m}_{i}^{t}=(1-\alpha)\mathbf{m}_{i}^{t-1}+\alpha\mathbf{g}_{i}^{t},\]

where \(\mathbf{g}_{i}^{t}=\mathbf{g}_{i}(\mathbf{x}^{t},\mathbf{\xi}_{i}^{t}),\ \forall\ i\in\mathbb{G}\) and \(\mathbf{g}_{i}^{t}=*\ \forall\ i\in\mathbb{B}\) and \(\{\mathbf{g}_{\mathbf{\xi}_{i}}^{t}\}_{i\in\mathbb{G}}\) are sampled independently. Our main convergence result for this version called M-SGDA-RA is summarized in the following theorem.

**Theorem 3**.: _Let Assumptions 1, 2, 4, and 6 hold. Then after \(T\) iterations_ M-SGDA-RA _(Algorithm 3) with \((\delta,c)\)_-Ragg outputs \(\mathbf{\widetilde{x}}^{T}\) such that_

\[\mathbb{E}\Big{[}\big{\|}\mathbf{\widetilde{x}}^{T}-\mathbf{x}^{*}\big{\|}^{2}\Big{]} \leq\frac{2\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2}}{\mu\gamma\alpha W_{T}}+ \frac{8\gamma c\delta(24\sigma^{2}+12\zeta^{2})}{\mu\alpha^{2}}+\frac{6\gamma \sigma^{2}}{\mu\alpha^{2}G}+\frac{4c\delta(24\sigma^{2}+12\zeta^{2})}{\mu^{2} \alpha^{2}}.\]

_where \(\mathbf{\widetilde{x}}^{T}=\frac{1}{W_{T}}\sum_{t=0}^{T}w_{t}\mathbf{\widehat{x}}^{t}\), \(\mathbf{\widehat{x}}^{t}=\frac{\alpha}{1-(1-\alpha)^{t+1}}\sum_{j=0}^{t}(1-\alpha)^ {t-j}\mathbf{x}^{j}\), \(w_{t}=\big{(}1-\frac{\mu\gamma\alpha}{2}\big{)}^{-t-1}\), and \(W_{T}=\sum_{t=0}^{T}w_{t}\)._Despite the fact that M-SGDA-RA is the first algorithm (for VIs) non-invariant to permutations, it also requires large batches to achieve convergence to any accuracy. Even in the context of minimization, which is much easier than VI, the known SOTA analysis of Momentum-SGD relies **in the convex case** on the unbiasedness of the estimator that is not available due to a robust aggregation. Nevertheless, we prove6 the convergence to the ball centered at the solution with radius \(\Theta\big{(}\nicefrac{{\sqrt{c\Theta}}}{{\Theta(\nicefrac{{\mathrm{\zeta}}}{{ \mathrm{\sigma}}})}}/\nicefrac{{\mathrm{\alpha}\mu}}{{\mathrm{\mu}}}\big{)}\); see Appendix D.3. Moreover, we show that M-SGDA-RA outperforms in the experiments other methods that require large batches.

Footnote 6: In contrast to Theorems 1-2, the result from Theorem 3 is given for the averaged iterate. We consider the averaged iterate to make the analysis simpler. We believe that one can extend the analysis to the last iterate as well, but we do not do it since we expect that the same problem (the need for large batches) will remain in the last-iterate analysis.

### Random Checks of Computations

We start with the Stochastic Gradient Descent-Accent with Checks of Computations (SGDA-CC). At each iteration of SGDA-CC, the server selects \(m\) workers (uniformly at random) and requests them to check the computations of other \(m\) workers from the previous iteration. Let \(V_{t}\) be the set of workers that verify/check computations, \(A_{t}\) are active workers at iteration \(t\), and \(V_{t}\cap A_{t}=\varnothing\). Then, the update of SGDA-CC can be written as

\[\mathbf{x}^{t+1}=\mathbf{x}^{t}-\gamma\overline{\mathbf{g}}^{t},\ \ \text{if}\ \ \overline{\mathbf{g}}^{t}=\frac{1}{|A_{t}|}\sum_{i\in A_{t}}\mathbf{g}_{i}(\mathbf{x}^{t},\mathbf{\xi}_{i}^{t})\ \ \text{ is accepted},\]

where \(\{\mathbf{g}_{i}(\mathbf{x}^{t},\mathbf{\xi}_{i}^{t})\}_{i\in\mathcal{G}}\) are sampled independently.

The acceptance (of the update) event occurs when the condition \(\big{\|}\overline{\mathbf{g}}^{t}-\mathbf{g}_{i}(\mathbf{x}^{t},\mathbf{\xi}_{i}^{t})\big{\|} \leq C\sigma\) holds for the majority of workers. If \(\overline{\mathbf{g}}^{t}\) is rejected, then all workers re-sample \(\mathbf{g}_{i}(\mathbf{x}^{t},\mathbf{\xi}_{i}^{t})\) until acceptance is achieved. The rejection probability is bounded, as per (Gorbunov et al., 2022b), and can be adjusted by choosing a constant \(C=\Theta(1)\). We assume that the server knows the seeds for generating randomness on workers, and thus, verification of computations is possible. Following each aggregation of \(\mathbf{g}_{i}(\mathbf{x}^{t},\mathbf{\xi}_{i}^{t})_{i\in\mathcal{G}}\), the server selects uniformly at random \(2m\) workers: \(m\) workers check the computations at the previous step of the other \(m\) workers. For instance, at the \((t+1)\)-th iteration, the server asks a checking peer \(i\) to compute \(\mathbf{g}_{j}(\mathbf{x}^{t},\mathbf{\xi}_{j}^{t})\), where \(j\) is a peer being checked. This is possible if all seeds are broadcasted at the start of the training. Workers assigned to checking do not participate in the training while they check and do not contribute to \(\overline{\mathbf{g}}^{t}\). Therefore, each Byzantine peer is checked at each iteration with a probability of \(\sim\nicefrac{{m}}{{n}}\) by some good worker (see the proof of Theorem 4). If the results are mismatched, then both the checking and checked peers are removed from training.

This design ensures that every such mismatch, whether it is caused by honest or Byzantine peers, eliminates at least one Byzantine peer and at most one honest peer (see details in Appendix E.1). It's worth noting that we assume any information is accessible to Byzantines except when each of them will be checked. As such, Byzantine peers can only reduce their relative numbers, which leads us to the main result for SGDA-CC, which is presented below.

**Theorem 4**.: _Let Assumptions 1, 4 and 6 hold. Then after \(T\) iterations SGDA-CC (Algorithm 5) with \(\gamma\leq\frac{1}{2t}\) outputs \(\mathbf{x}^{T}\) such that_

\[\mathbb{E}\big{\|}\mathbf{x}^{T+1}-\mathbf{x}^{*}\big{\|}^{2}\leq\Big{(}1-\frac{ \gamma\mu}{2}\Big{)}^{T+1}\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2}+\frac{4 \gamma\sigma^{2}}{\mu(n-2B-m)}+\frac{2q\sigma^{2}nB}{m}\bigg{(}\frac{\gamma}{ \mu}+\gamma^{2}\bigg{)},\]

_where \(q=2C^{2}+12+\frac{12}{n-2B-m}\); \(q=\Theta(1)\) since \(C=\Theta(1)\)._

The above theorem (see Appendix E.1 for the proof) provides the first result that does not require large batchsizes to converge to any predefined accuracy. The first and the second terms in the convergence bound correspond to the SOTA results for SGDA (Loziov et al., 2021). Similarly to the vanilla SGDA, the convergence can be obtained by decreasing stepsize, however, such an approach does not benefit from collaboration, since the dominating term \(\frac{\gamma\sigma^{2}nB}{\mu m}\) (coming from the presence of Byzantine workers) is not inversely dependent on \(n\). Moreover, the result is even worse than for single node SGDA in terms of dependence on \(n\).

To overcome this issue we consider the restart technique for SGDA-CC and propose the next algorithm called R-SGDA-CC. This method consists of \(r\) stages. On the \(t\)-th stage R-SGDA-CC runs SGDA-CC with \(\gamma_{t}\) for \(K_{t}\) iterations from the starting point \(\widehat{\mathbf{x}}^{t}\), which is the output from the previous stage, and defines the obtained point as \(\widehat{\mathbf{x}}^{t+1}\) (see details in Appendix E.2). The main result for R-SGDA-CC is given below.

**Theorem 5**.: _Let Assumptions 1, 4 and 6 hold. Then, after \(r=\left\lceil\log_{2}\frac{R^{2}}{\varepsilon}\right\rceil-1\) restarts_

\[\text{\sf R-SGDA-CC}\] _(Algorithm 6) with \[\gamma_{t}=\min\left\{\frac{1}{2t},\sqrt{\frac{(n-2B-m)R^{2}}{6\sigma^{2}2^{t }K_{t}}},\sqrt{\frac{m^{2}R^{2}}{72q\sigma^{2}2^{t}B^{2}n^{2}}}\right\}\] and \[K_{t}=\left\lceil\max\left\{\frac{8\ell}{\mu},\frac{96\sigma^{2}t}{(n-2B-m) \mu^{2}R^{2}},\frac{34n\sigma B\sqrt{q^{2}t^{2}}}{m\mu R}\right\}\right\rceil\], where \[R\geq\left\|\mathbf{x}^{0}-\mathbf{x}^{*}\right\|\], outputs \[\widehat{\mathbf{x}}^{r}\] such that \[\mathbb{E}\|\widehat{\mathbf{x}}^{r}-\mathbf{x}^{*}\|^{2}\leq\varepsilon\]. Moreover, the total number of executed iterations of SGDA-CC is_ \[\sum_{t=1}^{r}K_{t}=\Theta\left(\frac{\ell}{\mu}\log\frac{\mu R_{0}^{2}}{ \varepsilon}+\frac{\sigma^{2}}{(n-2B-m)\mu\varepsilon}+\frac{nB\sigma}{m\sqrt {\mu\varepsilon}}\right). \tag{5}\]

The above result implies that R-SGDA-CC also converges to any accuracy without large batch-sizes (see Appendix E.2 for details). However, as the accuracy tends to zero, the dominant term \(\frac{\sigma^{2}}{(n-2B-m)\mu\varepsilon}\) inversely depends on the number of workers. This makes R-SGDA-CC benefit from collaboration, as the algorithm becomes more efficient with an increasing number of workers. Moreover, when \(B\) and \(m\) are small the derived complexity result for R-SGDA-CC matches the one for parallel SGDA [Loziou et al., 2021], which is obtained for the case of no Byzantine workers.

Next, we present a modification of Stochastic Extragradient with Checks of Computations (SEG-CC):

\[\widetilde{\mathbf{x}}^{t} =\mathbf{x}^{t}-\gamma_{1}\overline{g}_{\mathbf{\xi}}^{t}, \text{if}\ \ \overline{g}_{\mathbf{\xi}}^{t}=\frac{1}{\left|A_{t}\right|}\sum_{ i\in A_{t}}\mathbf{g}_{i}(\mathbf{x}^{t},\mathbf{\xi}_{i}^{t})\ \ \text{is accepted},\] \[\mathbf{x}^{t+1} =\mathbf{x}^{t}-\gamma_{2}\overline{g}_{\mathbf{\eta}}^{t}, \text{if}\ \ \overline{g}_{\mathbf{\eta}}^{t}=\frac{1}{\left|A_{t}\right|} \sum_{i\in A_{t}}\mathbf{g}_{i}(\widetilde{\mathbf{x}}^{t},\mathbf{\eta}_{i}^{t})\ \ \text{is accepted},\]

where \(\{\mathbf{g}_{i}(\mathbf{x}^{t},\mathbf{\xi}_{i}^{t})\}_{i\in G}\) and \(\{\mathbf{g}_{i}(\widetilde{\mathbf{x}}^{t},\mathbf{\eta}_{i}^{t})\}_{i\in G}\) are sampled independently. The events of acceptance \(\overline{g}_{\mathbf{\eta}}^{t}\big{(}\text{or}\ \overline{g}_{\mathbf{\xi}}^{t}\big{)}\) happens if

\[\left\|\overline{g}^{t}-\mathbf{g}_{i}(\mathbf{x}^{t},\mathbf{\xi}_{i}^{t})\right\|\leq C \sigma\ \left(\text{or}\left\|\overline{g}_{\mathbf{\eta}}^{t}-\mathbf{g}_{i}( \widetilde{\mathbf{x}}^{t},\mathbf{\eta}_{i}^{t})\right\|\leq C\sigma\right)\]

holds for the majority of workers. An iteration of SEG-CC actually represents two subsequent iteration of SGDA-CC, so we refer to the beginning of the section for more details. Our main convergence results for SEG-CC are summarized in the following theorem; see Appendix E.3 for the proof.

**Theorem 6**.: _Let Assumptions 1, 3 and 4 hold. Then after \(T\) iterations SEG-CC (Algorithm 7) with \(\gamma_{1}\leq\frac{1}{2\mu+2L}\) and \(\beta=\nicefrac{{\gamma_{2}}}{{\gamma_{1}}}\leq\nicefrac{{1}}{{4}}\) outputs \(\mathbf{x}^{T}\) such that_

\[\mathbb{E}\big{\|}\mathbf{x}^{T}-\mathbf{x}^{*}\big{\|}^{2}\leq\left(1-\frac{\mu\beta \gamma_{1}}{4}\right)^{T}\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2}+2\sigma^{ 2}\bigg{(}\frac{4\gamma_{1}}{\beta\mu^{2}(n-2B-m)}+\frac{\gamma_{1}qnB}{m} \bigg{)},\]

_where \(q=2C^{2}+12+\frac{12}{n-2B-m}\); \(q=\Theta(1)\) since \(C=\Theta(1)\)._

Similarly to SGDA-CC, SEG-CC does not require large batchsizes to converge to any predefined accuracy and does not benefit of collaboration, though the first two terms correspond to the SOTA convergence results for SEG under bounded variance assumption [Juditsky et al., 2011]. The last term appears due to the presence of the Byzantine workers. The restart technique can also be applied; see Appendix E.4 for the proof.

**Theorem 7**.: _Let Assumptions 1, 3, 4 hold. Then, after \(r=\left\lceil\log_{2}\frac{R^{2}}{\varepsilon}\right\rceil-1\) restarts_

\[\text{\sf R-SEG-CC}\] _(Algorithm 8) with \[\gamma_{1_{t}}=\min\left\{\frac{1}{2L},\sqrt{\frac{(G-B-m)R^{2}}{16\sigma^{2} ^{t}K_{t}}},\sqrt{\frac{mR^{2}}{8q\sigma^{2}2^{t}Bn}}\right\}\], \[\gamma_{2_{t}}=\min\left\{\frac{1}{4L},\sqrt{\frac{m^{2}R^{2}}{64q \sigma^{2}R^{2}B^{2}n^{2}}},\sqrt{\frac{(G-B-m)R^{2}}{64\sigma^{2}K_{t}}}\right\}\] and \[K_{t}=\left\lceil\max\left\{\frac{8L}{\mu},\frac{16n\sigma B\sqrt{q^{2}}}{m \mu R},\frac{256\sigma^{2}\sigma^{t}}{(G-B-m)\mu^{2}R^{2}}\right\}\right\rceil\], where \[R\geq\left\|\mathbf{x}^{0}-\mathbf{x}^{*}\right\|\] outputs \[\widehat{\mathbf{x}}^{r}\] such that \[\mathbb{E}\|\widehat{\mathbf{x}}^{r}-\mathbf{x}^{*}\|^{2}\leq\varepsilon\]. Moreover, the total number of executed iterations of SEG-CC is_ \[\sum_{t=1}^{r}K_{t}=\Theta\left(\frac{\ell}{\mu}\log\frac{\mu R_{0}^{2}}{ \varepsilon}+\frac{\sigma^{2}}{(n-2B-m)\mu\varepsilon}+\frac{nB\sigma}{m\sqrt {\mu\varepsilon}}\right). \tag{6}\]The above result states that \(\mathsf{R\text{-}SEG\text{-}CC}\) also converges to any accuracy without large batchsizes; see Appendix E.4. But with accuracy tending to zero (\(\varepsilon\to 0\)) the dominating term \(\frac{\sigma^{2}}{(n-2B-m)\mu e}\) inversely depends on the number of workers, hence \(\mathsf{R\text{-}SEG\text{-}CC}\) benefits from collaboration. Moreover, when \(B\) and \(m\) are small the derived complexity result for \(\mathsf{R\text{-}SEG\text{-}CC}\) matches the one for parallel/mini-batched SEG[Juditsky et al., 2011], which is obtained for the case of no Byzantine workers.

## 3 Numerical Experiments

Quadratic game.To illustrate our theoretical results, we conduct numerical experiments on a quadratic game

\[\min_{y}\max_{z}\frac{1}{s}\sum_{i=1}^{s}\frac{1}{2}y^{\top}\mathbf{A}_{1,i}y+ y^{\top}\mathbf{A}_{2,i}z-\frac{1}{2}z^{\top}\mathbf{A}_{3,i}z+b_{1,i}^{\top}y -b_{2,i}^{\top}z.\]

The above problem can be re-formulated as a special case of (1) with \(F\) defined as follows:

\[F(\mathbf{x})=\frac{1}{s}\sum_{i=1}^{s}\mathbf{A}_{i}\mathbf{x}+b_{i},\ \ \text{where}\ \mathbf{x}=(y^{\top},z^{\top})^{\top},\ b_{i}=(b_{1,i}^{\top},b_{2,i}^{\top})^{\top}, \tag{7}\]

with symmetric matrices \(\mathbf{A}_{j,i}\) s.t. \(\mu\mathbf{I}\preccurlyeq\mathbf{A}_{j,i}\preccurlyeq\mathbf{\ell}\mathbf{1}\), \(\mathbf{A}_{i}\in\mathbb{R}^{d\times d}\) and \(b_{i}\in\mathbb{R}^{d}\); see Appendix F for the detailed description.

We set \(\ell=100\), \(\mu=0.1\), \(s=1000\) and \(d=50\). Only one peer checked the computations on each iteration (\(m=1\)). We used RFA (geometric median) with bucketing as an aggregator since it showed the best performance. For approximating the median we used Weiszfeld's method with \(10\) iterations and parameter \(\nu=0.1\)[Pillutla et al., 2022]. RDEG[Adibi et al., 2022] provably works only if \(n\geq 100\), so here we provide experiments with \(n=150\), \(B=20\), \(\gamma=2e-5\). We set the parameter \(\alpha=0.1\) for \(\mathsf{M\text{-}S\text{-}S\text{-}\text{-}\text{-}\text{-}\textpropose the first methods in this setting that provably converge to any predefined accuracy in the case of homogeneous data. We believe this is an important step towards building a strong theory of Byzantine robustness in the case of distributed VIs.

However, our work has several limitations. First of all, one can consider different/more general assumptions about operators (Beznosikov et al., 2023; Gorbunov et al., 2022; Gorbunov et al., 2022; Gorbunov et al., 2022) in the analysis of the proposed methods. Next, as we mention in the discussion after Theorem 3, our result for M-SGDA-RA requires large batchsizes, and it remains unclear to us whether this requirement can be removed. Finally, the only results that do not require large batchsizes are derived using the checks of computations that create (although small) computation overhead. Obtaining similar results without checks of computations remains an open problem. Addressing these limitations is a prominent direction for future research.

Figure 1: Error plots for quadratic games experiments under different Byzantine attacks. The first row shows the outperformance of M-SGDA-RA over methods without checks of computations. The second row illustrates advantages of SGDA-CC and SEG-CC.

Figure 2: Error plots for the robust neural network experiment on MNIST under different byzantine attacks (BF, LF, IPM, and ALIE). Each algorithm is shown with a consistent choice of color and style across plots, as indicated in the legends.

## Acknowledgments and Disclosure of Funding

This work of N. Tupitsa was supported by a grant for research centers in the field of artificial intelligence, provided by the Analytical Center for the Government of the Russian Federation in accordance with the subsidy agreement (agreement identifier 000000D730321P5Q0002) and the agreement with the Moscow Institute of Physics and Technology dated November 1, 2021 No. 70-2021-00138.

## References

* Adibi et al. (2022) A. Adibi, A. Mitra, G. J. Pappas, and H. Hassani. Distributed statistical min-max learning in the presence of byzantine agents. In _2022 IEEE 61st Conference on Decision and Control (CDC)_, pages 4179-4184. IEEE, 2022.
* Alistarh et al. (2018) D. Alistarh, Z. Allen-Zhu, and J. Li. Byzantine stochastic gradient descent. _Advances in Neural Information Processing Systems_, 31, 2018.
* Allen-Zhu et al. (2021) Z. Allen-Zhu, F. Ebrahimianghazani, J. Li, and D. Alistarh. Byzantine-resilient non-convex stochastic gradient descent. In _International Conference on Learning Representations_, 2021.
* Allouah et al. (2023) Y. Allouah, S. Farhadkhani, R. Guerraoui, N. Gupta, R. Pinot, and J. Stephan. Fixing by mixing: A recipe for optimal byzantine ml under heterogeneity. In _International Conference on Artificial Intelligence and Statistics_, pages 1232-1300. PMLR, 2023.
* Baruch et al. (2019) M. Baruch, G. Baruch, and Y. Goldberg. A little is enough: Circumventing defenses for distributed learning, 2019.
* Beznosikov et al. (2023) A. Beznosikov, E. Gorbunov, H. Berard, and N. Loizou. Stochastic gradient descent-ascent: Unified theory and new efficient methods. In _International Conference on Artificial Intelligence and Statistics_, pages 172-235. PMLR, 2023.
* Blanchard et al. (2017) P. Blanchard, E. M. El Mhamdi, R. Guerraoui, and J. Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. _Advances in Neural Information Processing Systems_, 30, 2017.
* Bose et al. (2020) J. Bose, G. Gidel, H. Berard, A. Cianflone, P. Vincent, S. Lacoste-Julien, and W. Hamilton. Adversarial example games. _Advances in neural information processing systems_, 33:8921-8934, 2020.
* Browder (1966) F. E. Browder. Existence and approximation of solutions of nonlinear variational inequalities. _Proceedings of the National Academy of Sciences_, 56(4):1080-1086, 1966.
* Chen et al. (2017) Y. Chen, L. Su, and J. Xu. Distributed statistical machine learning in adversarial settings: Byzantine gradient descent. _Proceedings of the ACM on Measurement and Analysis of Computing Systems_, 1(2):1-25, 2017.
* Damaskinos et al. (2019) G. Damaskinos, E.-M. El-Mhamdi, R. Guerraoui, A. Guirguis, and S. Rouault. Aggregathor: Byzantine machine learning via robust gradient aggregation. _Proceedings of Machine Learning and Systems_, 1:81-106, 2019.
* Dem'yanov and Pevnyi (1972) V. F. Dem'yanov and A. B. Pevnyi. Numerical methods for finding saddle points. _USSR Computational Mathematics and Mathematical Physics_, 12(5):11-52, 1972.
* Diskin et al. (2021) M. Diskin, A. Bukhtiyarov, M. Ryabinin, L. Saulnier, A. Sinitsin, D. Popov, D. V. Pyrkin, M. Kashirin, A. Borzunov, A. Villanova del Moral, et al. Distributed deep learning in open collaborations. _Advances in Neural Information Processing Systems_, 34:7879-7897, 2021.
* Facchinei and Pang (2003) F. Facchinei and J.-S. Pang. _Finite-dimensional variational inequalities and complementarity problems_. Springer, 2003.
* Gidel et al. (2018) G. Gidel, H. Berard, G. Vignoud, P. Vincent, and S. Lacoste-Julien. A variational inequality perspective on generative adversarial networks. In _International Conference on Learning Representations_, 2018. URL [https://openreview.net/pdf?id=r11aEnA5Ym](https://openreview.net/pdf?id=r11aEnA5Ym).
* Gidel et al. (2018)I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. _Advances in Neural Information Processing Systems_, 27, 2014.
* Goodfellow et al. (2015) I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. _International Conference on Learning Representations_, 2015.
* Gorbunov et al. (2020) E. Gorbunov, D. Kovalev, D. Makarenko, and P. Richtarik. Linearly converging error compensated sgd. _Advances in Neural Information Processing Systems_, 33:20889-20900, 2020.
* Gorbunov et al. (2022a) E. Gorbunov, H. Berard, G. Gidel, and N. Loizou. Stochastic extragradient: General analysis and improved rates. In _International Conference on Artificial Intelligence and Statistics_, pages 7865-7901. PMLR, 2022a.
* Gorbunov et al. (2022b) E. Gorbunov, A. Borzunov, M. Diskin, and M. Ryabinin. Secure distributed training at scale. In _International Conference on Machine Learning_, pages 7679-7739. PMLR, 2022b. URL [https://proceedings.mlr.press/v162/gorbunov22a/gorbunov22a.pdf](https://proceedings.mlr.press/v162/gorbunov22a/gorbunov22a.pdf).
* Gorbunov et al. (2023a) E. Gorbunov, S. Horvath, P. Richtarik, and G. Gidel. Variance reduction is an antidote to byzantines: Better rates, weaker assumptions and communication compression as a cherry on the top. _International Conference on Learning Representations_, 2023a.
* Gorbunov et al. (2023b) E. Gorbunov, A. Taylor, S. Horvath, and G. Gidel. Convergence of proximal point and extragradient-based methods beyond monotonicity: the case of negative comonotonicity. In _International Conference on Machine Learning_, pages 11614-11641. PMLR, 2023b.
* Guerraoui et al. (2018) R. Guerraoui, S. Rouault, et al. The hidden vulnerability of distributed learning in byzantium. In _International Conference on Machine Learning_, pages 3521-3530. PMLR, 2018.
* Gulrajani et al. (2017) I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of wasserstein gans. _Advances in neural information processing systems_, 30, 2017.
* Harker and Pang (1990) P. T. Harker and J.-S. Pang. Finite-dimensional variational inequality and nonlinear complementarity problems: a survey of theory, algorithms and applications. _Mathematical programming_, 48(1-3):161-220, 1990.
* Hsieh et al. (2020) Y.-G. Hsieh, F. Iutzeler, J. Malick, and P. Mertikopoulos. Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling. _Advances in Neural Information Processing Systems_, 33:16223-16234, 2020.
* Juditsky et al. (2011) A. Juditsky, A. Nemirovski, and C. Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. _Stochastic Systems_, 1(1):17-58, 2011.
* Karimireddy et al. (2021) S. P. Karimireddy, L. He, and M. Jaggi. Learning from history for byzantine robust optimization. In _International Conference on Machine Learning_, pages 5311-5319. PMLR, 2021.
* Karimireddy et al. (2022) S. P. Karimireddy, L. He, and M. Jaggi. Byzantine-robust learning on heterogeneous datasets via bucketing. In _International Conference on Learning Representations_, 2022. URL [https://arxiv.org/pdf/2006.09365.pdf](https://arxiv.org/pdf/2006.09365.pdf).
* Kijsipongse et al. (2018) E. Kijsipongse, A. Piyatumrong, et al. A hybrid gpu cluster and volunteer computing platform for scalable deep learning. _The Journal of Supercomputing_, 74(7):3236-3263, 2018.
* Kingma and Ba (2014) D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Korpelevich (1976) G. M. Korpelevich. The extragradient method for finding saddle points and other problems. _Matecon_, 12:747-756, 1976.
* Krasnosel'skii (1955) M. A. Krasnosel'skii. Two remarks on the method of successive approximations. _Uspekhi matematicheskikh nauk_, 10(1):123-127, 1955.
* Krizhevsky et al. (2009) A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Krizhevsky et al. (2012)L. Lamport, R. Shostak, and M. Pease. The byzantine generals problem. _ACM Transactions on Programming Languages and Systems_, 4(3):382-401, 1982.
* Li (2020) C. Li. Demystifying gpt-3 language model: A technical overview, 2020. "[https://lambdalabs.com/blog/demystifying-gpt-3](https://lambdalabs.com/blog/demystifying-gpt-3)".
* Loizou et al. (2021) N. Loizou, H. Berard, G. Gidel, I. Mitliagkas, and S. Lacoste-Julien. Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected co-coercivity. _Advances in Neural Information Processing Systems_, 34:19095-19108, 2021.
* Lugosi and Mendelson (2021) G. Lugosi and S. Mendelson. Robust multivariate mean estimation: the optimality of trimmed mean. 2021.
* Lyu et al. (2020) L. Lyu, H. Yu, X. Ma, L. Sun, J. Zhao, Q. Yang, and P. S. Yu. Privacy and robustness in federated learning: Attacks and defenses. _arXiv preprint arXiv:2012.06337_, 2020.
* Madry et al. (2018) A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018.
* Mann (1953) W. R. Mann. Mean value methods in iteration. _Proceedings of the American Mathematical Society_, 4(3):506-510, 1953.
* Mertikopoulos and Zhou (2019) P. Mertikopoulos and Z. Zhou. Learning in games with continuous action sets and unknown payoff functions. _Mathematical Programming_, 173(1):465-507, 2019.
* Mishchenko et al. (2020) K. Mishchenko, D. Kovalev, E. Shulgin, P. Richtarik, and Y. Malitsky. Revisiting stochastic extra-gradient. In _International Conference on Artificial Intelligence and Statistics_, pages 4573-4582. PMLR, 2020.
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL [https://openreview.net/forum?id=B1QRgziT-](https://openreview.net/forum?id=B1QRgziT-).
* Nemirovski et al. (2009) A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. _SIAM Journal on optimization_, 19(4):1574-1609, 2009.
* Pethick et al. (2023) T. Pethick, O. Fercoq, P. Latafat, P. Patrinos, and V. Cevher. Solving stochastic weak minty variational inequalities without increasing batch size. In _The Eleventh International Conference on Learning Representations_, 2023.
* Pillutla et al. (2022) K. Pillutla, S. M. Kakade, and Z. Harchaoui. Robust aggregation for federated learning. _IEEE Transactions on Signal Processing_, 70:1142-1154, 2022.
* Popov (1980) L. D. Popov. A modification of the arrow-hurwicz method for search of saddle points. _Mathematical notes of the Academy of Sciences of the USSR_, 28:845-848, 1980.
* Ryabinin et al. (2021) M. Ryabinin, E. Gorbunov, V. Plokhotnyuk, and G. Pekhimenko. Moshpit sgd: Communication-efficient decentralized training on heterogeneous unreliable devices. _Advances in Neural Information Processing Systems_, 34:18195-18211, 2021.
* Ryu and Yin (2022) E. K. Ryu and W. Yin. _Large-Scale Convex Optimization: Algorithms & Analyses via Monotone Operators_. Cambridge University Press, 2022.
* Song et al. (2020) C. Song, Z. Zhou, Y. Zhou, Y. Jiang, and Y. Ma. Optimistic dual extrapolation for coherent non-monotone variational inequalities. _Advances in Neural Information Processing Systems_, 33:14303-14314, 2020.
* Stich (2019) S. U. Stich. Unified optimal analysis of the (stochastic) gradient method. _arXiv preprint arXiv:1907.04232_, 2019.
* Tran-Dinh (2023) Q. Tran-Dinh. Sublinear convergence rates of extragradient-type methods: A survey on classical and recent developments. _arXiv preprint arXiv:2303.17192_, 2023.
* Tikhonov and Voskresens (2018)A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and K. Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In _International Conference on Machine Learning_, pages 3540-3549. PMLR, 2017.
* Wayne and Abbott (2014) G. Wayne and L. Abbott. Hierarchical control using networks trained with higher-level forward models. _Neural computation_, 26(10):2163-2193, 2014.
* Wu et al. (2020) Z. Wu, Q. Ling, T. Chen, and G. B. Giannakis. Federated variance-reduced stochastic gradient descent with robustness to byzantine attacks. _IEEE Transactions on Signal Processing_, 68:4583-4596, 2020.
* Xie et al. (2019) C. Xie, S. Koyejo, and I. Gupta. Fall of empires: Breaking byzantine-tolerant sgd by inner product manipulation, 2019.
* Yang and Li (2021) Y.-R. Yang and W.-J. Li. Basgd: Buffered asynchronous sgd for byzantine learning. In _International Conference on Machine Learning_, pages 11751-11761. PMLR, 2021.
* Yin et al. (2018) D. Yin, Y. Chen, R. Kannan, and P. Bartlett. Byzantine-robust distributed learning: Towards optimal statistical rates. In _International Conference on Machine Learning_, pages 5650-5659. PMLR, 2018.
* Zhang et al. (2022) G. Zhang, Y. Wang, L. Lessard, and R. B. Grosse. Near-optimal local convergence of alternating gradient descent-ascent for minimax optimization. In _International Conference on Artificial Intelligence and Statistics_, pages 7659-7679. PMLR, 2022.
* Zhu and Ling (2021) H. Zhu and Q. Ling. Broadcast: Reducing both stochastic and compression noise to robustify communication-efficient federated learning. _arXiv preprint arXiv:2104.06685_, 2021.
* Zinkevich et al. (2010) M. Zinkevich, M. Weimer, L. Li, and A. Smola. Parallelized stochastic gradient descent. _Advances in neural information processing systems_, 23, 2010.

###### Contents

* 1 Introduction
	* 1.1 Setting
	* 1.2 Our Contributions
	* 1.3 Related Work
* 2 Main Results
	* 2.1 Methods with Robust Aggregation
	* 2.2 Client Momentum
	* 2.3 Random Checks of Computations
* 3 Numerical Experiments
* 4 Conclusion
* A Examples of \((\delta,c)\)-Robust Aggregation Rules
* A.1 Aggregators
* A.2 Bucketing algorithm
* A.3 Robust Aggregation examples
* B Further Details on RDEG
* C Auxiliary results
* C.1 Basic Inequalities
* C.2 Usefull Lemmas
* D Methods that use robust aggregators
* D.1 Proofs for SGDA-RA
* D.1.1 Quasi-Strongly Monotone Case
* D.2 Proofs for SEG-RA
* D.2.1 Auxiliary results
* D.2.2 Quasi-Strongly Monotone Case
* D.3 Proofs for M-SGDA-RA
* D.3.1 Quasi-Strongly Monotone Case
* E Methods with random check of computations
* E.1 Proofs for SGDA-CC
* E.1.1 Star Co-coercieve Case
* E.1.2 Quasi-Strongly Monotone Case
* E.1.3 Monotone Case
* E.2 Proofs for R-SGDA-CC
* E.2.1 Quasi-Strongly Monotone Case

* E.3 Proofs for SEG-CC
* E.3.1 Auxiliary results
* E.3.2 Lipschitz Case
* E.3.3 Quasi-Strongly Monotone Case
* E.3.4 Lipschitz Monotone Case
* E.4 Proofs for R-SEG-CC
* E.4.1 Quasi Strongly Monotone Case
* F Extra Experiments and Experimental details
* F.1 Quadratic games
* F.2 Generative Adversarial Networks
Examples of \((\delta,c)\)-Robust Aggregation Rules

This section is about how to construct an aggregator satisfying 1.1.

### Aggregators

This subsection examines various aggregators that lack robustness. It means that new attacks can be easily designed to exploit the aggregation scheme, causing its failure. We analyze three commonly employed defenses that are representative.

**Krum.** For \(i\neq j\), let \(i\to j\) denote that \(\mathbf{x}_{j}\) belongs to the \(n-q-2\) closest vectors to \(\mathbf{x}_{i}\). Then,

\[\textsc{Krum}(\mathbf{x}_{1},\ldots,\mathbf{x}_{n}):=\operatorname*{argmin}_{i}\sum_{i \to j}\left\lVert\mathbf{x}_{i}-\mathbf{x}_{j}\right\rVert^{2}.\]

Krum is computationally expensive, requiring \(\mathcal{O}(n^{2})\) work by the server Blanchard et al. (2017). **CM.** Coordinate-wise median computes for the \(k\)-th coordinate:

\[[\textsc{CM}(\mathbf{x}_{1},\ldots,\mathbf{x}_{n})]_{k}:=\text{median}([\mathbf{x}_{1}]_{k},\ldots,[\mathbf{x}_{n}]_{k})=\operatorname*{argmin}_{i}\sum_{j=1}^{n}\left\lvert [\mathbf{x}_{i}]_{k}-[\mathbf{x}_{j}]_{k}\right\rvert.\]

Coordinate-wise median is fast to implement requiring only \(\Theta(n)\) time Chen et al. (2017).

**RFA.** Robust federated averaging (RFA) computes the geometric median

\[\text{RFA}(\mathbf{x}_{1},\ldots,\mathbf{x}_{n}):=\operatorname*{argmin}_{\mathbf{v}}\sum _{i=1}^{n}\left\lVert\mathbf{v}-\mathbf{x}_{i}\right\rVert_{2}.\]

Although there is no closed form solution for the geometric median, an approximation technique presented by Pillutla et al. (2022) involves performing several iterations of the smoothed Weiszfeld algorithm, with each iteration requiring a computation of complexity \(\Theta(n)\).

### Bucketing algorithm

We use the process of _\(s\)-bucketing_, propose by (Yang and Li, 2021; Karimireddy et al., 2022) to randomly divide \(n\) inputs, \(\mathbf{x}_{1}\) to \(\mathbf{x}_{n}\), into \(\lceil n/s\rceil\) buckets, each containing no more than \(s\) elements. After averaging the contents of each bucket to create \(\mathbf{y}_{1},\ldots,\mathbf{y}_{\lceil n/s\rceil}\), we input them into the aggregator Aggr. The Bucketing Algorithm outlines the procedure. Our approach's main feature is that the resulting set of averaged \(\mathbf{y}_{1},\ldots,\mathbf{y}_{\lceil n/s\rceil}\) are more homogeneous (with lower variance) than the original inputs.

```
1:input\(\{\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\}\), \(s\in\mathbb{N}\), aggregation rule Aggr
2: pick random permutation \(\pi\) of \([n]\)
3: compute \(\mathbf{y}_{i}\leftarrow\frac{1}{s}\sum_{k=(i-1)\cdot s+1}^{\min(n\,,i\,:s)}\mathbf{x}_ {\pi(k)}\) for \(i=\{1,\ldots,\lceil n/s\rceil\}\)
4:output\(\widehat{\mathbf{x}}\leftarrow\textsc{Aggr}(\mathbf{y}_{1},\ldots,\mathbf{y}_{\lceil n/s \rceil})\) // aggregate after bucketing
```

**Algorithm**Bucketing Algorithm

### Robust Aggregation examples

Next we recall the result from (Karimireddy et al., 2022), that shows that aggregators which we saw, can be made to satisfy 1.1 by combining with bucketing.

**Theorem 8**.: _Suppose we are given \(n\) inputs \(\{\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\}\) such that \(\mathbb{E}\lVert\mathbf{x}_{i}-\mathbf{x}_{j}\rVert^{2}\leq\rho^{2}\) for any fixed pair \(i,j\in\mathbb{G}\) and some \(\rho\geq 0\) for some \(\delta\leq\delta_{\max}\), with \(\delta_{\max}\) to be defined. Then, running Bucketing Algorithm with \(s=\lfloor\frac{\delta_{\max}}{\delta}\rfloor\) yields the following:_

* _Krum:_ \(\mathbb{E}\lVert\textsc{Krum}\circ\textsc{Bucketing}(\mathbf{x}_{1},\ldots,\mathbf{x }_{n})-\bar{\mathbf{x}}\rVert^{2}\leq\Theta(\delta\rho^{2})\) _with_ \(\delta_{\max}<\frac{1}{4}\)_._
* _Geometric median:_ \(\mathbb{E}\lVert\textsc{RFA}\circ\textsc{Bucketing}(\mathbf{x}_{1},\ldots,\mathbf{x }_{n})-\bar{\mathbf{x}}\rVert^{2}\leq\Theta(\delta\rho^{2})\) _with_ \(\delta_{\max}<\frac{1}{2}\)_._* _Coordinate-wise median:_ \(\mathbb{E}\|\mathsf{CM}\circ\mathsf{Bucketing}(\mathbf{x}_{1},\ldots,\mathbf{x}_{n})- \bar{\mathbf{x}}\|^{2}\ \leq\ \Theta(d\delta\rho^{2})\) _with_ \(\delta_{\max}<\frac{1}{2}\)_._

Note that all these methods satisfy the notion of an _agnostic_ Byzantine robust aggregator (Definition 1.1).

Further Details on RDEG

Originally RDEG was proposed for min-max problems and represents a variation of SEG with Univariate Trimmed-Mean Estimator aggregation rule. For convenience we give here RDEG pseudo-code we used in experiments.

In this section we use the notation \(\pi\in(0,1)\) for a confidence level.

```
1:TRIM\({}_{\epsilon,\alpha,\delta}\), \(\gamma\)
2:for\(t=1,...\)do
3:for\(\text{worker}\ i\in[n]\)in parallel
4:\(g^{t}_{\mathbf{\xi}_{t}}\gets g_{i}(\mathbf{x}^{t},\mathbf{\xi}_{i})\)
5:send\(\mathbf{g}^{t}_{\mathbf{\xi}_{t}}\) if \(i\in\mathcal{G}\), else send \(*\) if Byzantine
6:\(\widehat{\mathbf{g}}_{\mathbf{\xi}^{t}}(\mathbf{x}^{t})\) = TRIM\({}_{\epsilon,\alpha,\delta}\) (\(g^{t}_{\mathbf{\xi}_{1}},\ldots,g^{t}_{\mathbf{\xi}_{n}}\))
7:\(\widetilde{x}^{t}\leftarrow\mathbf{x}^{t}-\gamma_{1}\widehat{\mathbf{g}}^{t}_{\mathbf{\xi} ^{t}}(\mathbf{x}^{t})\).
8:for worker \(i\in[n]\)in parallel
9:\(\mathbf{g}^{t}_{\mathbf{\eta}_{i}}\gets g_{i}(\widetilde{x}^{t},\mathbf{\eta}_{i})\)
10:send\(\mathbf{g}^{t}_{\mathbf{\eta}_{i}}\) if \(i\in\mathcal{G}\), else send \(*\) if Byzantine
11:\(\widehat{\mathbf{g}}_{\mathbf{\eta}^{t}}(\widetilde{x}^{t})\) = TRIM\({}_{\epsilon,\alpha,\delta}\) (\(g^{t}_{\mathbf{\eta}_{1}},\ldots,g^{t}_{\mathbf{\eta}_{n}}\))
12:\(\mathbf{x}^{t+1}\leftarrow\mathbf{x}^{t}-\gamma_{2}\widehat{\mathbf{g}}_{\mathbf{\eta}^{t}}( \widetilde{x}^{t})\).
```

**Algorithm 1**RDRIG

Performance of Univariate Trimmed-Mean Estimator.The Trim operator takes as input \(n\) vectors, and applies coordinatewisely the univariate trimmed mean estimator from Lugosi and Mendelson (2021), described bellow here as Univariate Trimmed-Mean Estimator Algorithm.

```
1:Corrupted data set \(Z_{1},\ldots,Z_{n/2}\), \(\widetilde{Z}_{1},\ldots\widetilde{Z}_{n/2}\), corruption fraction \(\delta\), and confidence level \(\pi\).
2:Set \(\epsilon=8\delta+24\frac{\log(4/\pi)}{n}\)
3:Let \(Z^{*}_{1}\leq Z^{*}_{2}\leq\cdots\leq Z^{*}_{n/2}\) represent a non-decreasing arrangement of \(\{Z_{i}\}_{i\in[n/2]}\). Compute quantiles: \(\gamma=Z^{*}_{\epsilon n/2}\) and \(\beta=Z^{*}_{(1-\epsilon)n/2}\).
4:Compute robust mean estimate \(\widehat{\mu}_{Z}\) as follows:

\[\widehat{\mu}_{Z}=\frac{2}{n}\sum_{i=1}^{n/2}\phi_{\gamma,\beta}(\widetilde{Z} _{i});\phi_{\gamma,\beta}(x)=\begin{cases}\beta&x>\beta\\ x&x\in[\gamma,\beta]\\ \gamma&x<\gamma\end{cases}\]

The following result on the performance of Univariate Trimmed-Mean Estimator plays a key role in the analysis of RDEG.

**Theorem**.: _[_Adibi et al._,_,_2022_, Theorem 1]_ _Consider the trimmed mean estimator. Suppose \(\delta\in[0,1/16)\), and let \(\pi\in(0,1)\) be such that \(\pi\geq 4e^{-n/2}\). Then, there exists an universal constant \(c\), such that with probability at least \(1-\pi\),_

\[|\widehat{\mu}_{Z}-\mu_{Z}|\leq c\sigma_{Z}\left(\sqrt{\delta}+\sqrt{\frac{ \log(1/\pi)}{n}}\right).\]

Using the latter componentwise result the authors states that

\[\|\widehat{\mathbf{g}}_{\mathbf{\xi}^{t}}\big{(}\mathbf{x}^{t}\big{)}-F(\mathbf{x}^{t})\|\leq c \sigma\left(\sqrt{\delta}+\sqrt{\frac{\log(1/\pi)}{n}}\right).\]

In fact this result is very similar to the Definition 1.1. The main difference is that for Univariate Trimmed-Mean Estimator we have a bound with some probability. The other difference that usingthe following representation of the result with \(\rho^{2}=c^{2}\sigma^{2}\)

\[\|\widehat{\mathbf{g}}_{\mathbf{\xi}^{t}}\left(\mathbf{x}^{t}\right)-F(\mathbf{x}^{t})\|^{2} \leq\delta\rho^{2}+\frac{\rho^{2}\log(1/\pi)}{n},\quad\text{w.p. }1-\pi\]

Univariate Trimmed-Mean Estimator has the additional term inversely depending on the number of workers.

Moreover, the result requires \(\delta\in[0,1/16)\) in contrast to the aggregators we used, that work for wider range of corruption level \(\delta\in[0,1/5]\).

Performance guarantees for Rdeg.The authors of [1] consider only homogeneous case.

**Theorem**.: _[_1_, Theorem 3]_ _Suppose Assumptions 3 and 4 hold in conjunction with the assumptions on \(\delta\) and \(n\): the fraction \(\delta\) of corrupted devices satisfies \(\delta\in[0,1/16)\), and the number of agents \(n\) is sufficiently large: \(n\geq 48\log(16dT^{2})\). Then, with \(\pi=1/(4dT^{2})\) and step-size \(\eta\leq 1/(4L)\), Rdeg guarantees the following with probability at least \(1-1/T\):_

\[\|\mathbf{x}^{*}-\mathbf{x}^{T+1}\|^{2}\leq 2e^{-\frac{T}{4\kappa}}R^{2}+\frac{8c \sigma R\kappa}{L}\left(\sqrt{\delta}+\sqrt{\frac{\log(4dT^{2})}{n}}\right), \tag{10}\]

_where \(\kappa=\mu/L\)._

The result implies that Rdeg benefits of collaboration only when the corruption level is small. In fact, the term \(\frac{\log(4dT^{2})}{n}\leq\frac{\log(4dT^{2})}{48\log(16dT^{2})}\leq 1/48\), so the corruption level should be less than \(1/48\) to make Rdeg significantly benefit of collaboration in contrast to our SEG-CC that requires corruption level only less than \(1/5\). Moreover, in case of larger corruption level, Rdeg converges to a ball centered at the solution with radius \(\widetilde{\Theta}\bigg{(}\sqrt{\frac{\sqrt{\delta}\sigma R\kappa}{L}}\bigg{)}\) in contrast to our methods SGDA-RA, SEG-RA and M-SGDA-RA converge to a ball centered at the solution with radius \(\widetilde{\Theta}\bigg{(}\sqrt{\frac{c\delta\sigma^{2}}{\mu^{2}}}\bigg{)}\), that has a better dependence on \(\sigma\). It is crucial with increasing batchsize (\(b\) = batchsize), since \(\sigma^{2}\) depends on a batchsize as \(\frac{1}{b}\).

[MISSING_PAGE_EMPTY:21]

**Lemma C.2**.: _Let \(K>0\) be a positive integer and \(\eta_{1},\eta_{2},\ldots,\eta_{K}\) be random vectors such that \(\mathbb{E}_{k}[\eta_{k}]\stackrel{{\text{def}}}{{=}}\mathbb{E}[\eta_ {k}\mid\eta_{1},\ldots,\eta_{k-1}]=0\) for \(k=2,\ldots,K\). Then_

\[\mathbb{E}\left[\left\|\sum_{k=1}^{K}\eta_{k}\right\|^{2}\right]=\sum_{k=1}^{K }\mathbb{E}[\|\eta_{k}\|^{2}]. \tag{21}\]

Proof.: We start with the following derivation:

\[\mathbb{E}\left[\left\|\sum_{k=1}^{K}\eta_{k}\right\|^{2}\right] = \mathbb{E}[\|\eta_{K}\|^{2}]+2\mathbb{E}\left[\left\langle\eta_{ K},\sum_{k=1}^{K-1}\eta_{k}\right\rangle\right]+\mathbb{E}\left[\left\|\sum_{k=1} ^{K-1}\eta_{k}\right\|^{2}\right]\] \[= \mathbb{E}[\|\eta_{K}\|^{2}]+2\mathbb{E}\left[\left\langle\mathbb{ E}_{K}[\eta_{K}],\sum_{k=1}^{K-1}\eta_{k}\right\rangle\right]+\mathbb{E}\left[ \left\|\sum_{k=1}^{K-1}\eta_{k}\right\|^{2}\right]\] \[= \mathbb{E}[\|\eta_{K}\|^{2}]+2\mathbb{E}\left[\left\langle\mathbb{ E}_{K}[\eta_{K}],\sum_{k=1}^{K-1}\eta_{k}\right\rangle\right]+\mathbb{E}\left[ \left\|\sum_{k=1}^{K-1}\eta_{k}\right\|^{2}\right]\] \[= \mathbb{E}[\|\eta_{K}\|^{2}]+\mathbb{E}\left[\left\|\sum_{k=1}^{K -1}\eta_{k}\right\|^{2}\right].\]

Applying similar steps to \(\mathbb{E}\left[\left\|\sum_{k=1}^{K-1}\eta_{k}\right\|^{2}\right],\mathbb{E} \left[\left\|\sum_{k=1}^{K-2}\eta_{k}\right\|^{2}\right],\ldots,\mathbb{E} \left[\left\|\sum_{k=1}^{2}\eta_{k}\right\|^{2}\right]\), we get the result. 

**Lemma C.3**.: _Suppose_

\[r_{K}\leq r_{0}(1-a\gamma)^{K}+\frac{c_{1}\gamma}{b}+\frac{c_{0}}{b} \tag{22}\]

_holds for \(\gamma\leq\gamma_{0}\). Then the choise of_

\[b\geq\frac{3c_{0}}{\varepsilon}\]

_and_

\[\gamma\leq\min\biggl{(}\gamma_{0},\frac{c_{0}}{c_{1}}\biggr{)}\]

_implies that \(r_{K}\leq\varepsilon\) for_

\[K\geq\frac{1}{a}\max\biggl{(}\frac{c_{1}}{c_{0}},\frac{1}{\gamma_{0}}\biggr{)} \ln\frac{3r_{0}}{\varepsilon}\]

Proof.: Since \(b\geq\frac{3c_{0}}{\varepsilon}\) then \(\frac{c_{0}}{b}\leq\frac{\varepsilon}{3}\) and \(\frac{c_{1}\gamma}{b}\leq\frac{c_{1}\gamma\varepsilon}{3c_{0}}\). The choise of \(\gamma\leq\min\Bigl{(}\gamma_{0},\frac{c_{0}}{c_{1}}\Bigr{)}\) implies that \(\frac{c_{1}\gamma}{b}\leq\frac{\varepsilon}{3}\).

The choice of \(K\geq\frac{1}{a}\max\Bigl{(}\frac{c_{1}}{c_{0}},\frac{1}{\gamma_{0}}\Bigr{)}\ln \frac{3r_{0}}{\varepsilon}\) implies that \(r_{0}(1-a\gamma)^{K}\leq\frac{\varepsilon}{3}\) and finishes the proof. 

**Lemma C.4** (see also Lemma 2 from Stich (2019) and Lemma D.2 from Gorbunov et al. (2020)).: _Let \(\{r_{k}\}_{k\geq 0}\) satisfy_

\[r_{K}\leq r_{0}(1-a\gamma)^{K+1}+c_{1}\gamma+c_{2}\gamma^{2} \tag{23}\]

_for all \(K\geq 0\) with some constants \(a,c_{2}>0\), \(c_{1}\geq 0\), \(\gamma\leq\gamma_{0}\)._

_Then for_

\[\gamma=\min\left\{\gamma_{0},\frac{\ln\left(\max\{2,\min\{a\gamma_{0}K/c_{1},a^ {2}r_{0}K^{2}/c_{2}\}\}\right)\}{a(K+1)}\right\} \tag{24}\]

_we have that_

\[r_{K} = \widetilde{\Theta}\left(r_{0}\exp\left(-a\gamma_{0}(K+1)\right)+ \frac{c_{1}}{aK}+\frac{c_{2}}{a^{2}K^{2}}\right).\]

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_FAIL:24]

Next we use Lemmas C.1 and D.1 to derive

\[\mathbb{E}_{\boldsymbol{\xi}}\big{\|}\boldsymbol{x}^{t+1}-\boldsymbol {x}^{*}\big{\|}^{2} \leq \Big{(}1+\frac{\gamma\mu}{2}\Big{)}\big{\|}\boldsymbol{x}^{t}- \boldsymbol{x}^{*}\big{\|}^{2}+\big{(}2\gamma^{2}\ell-2\gamma\big{)}\langle F( \boldsymbol{x}^{t}),\boldsymbol{x}^{t}-\boldsymbol{x}^{*}\rangle\] \[+\frac{2\gamma^{2}\sigma^{2}}{G}+2c\delta\rho^{2}\bigg{(}\frac{ \gamma}{\mu}+\gamma^{2}\bigg{)},\]

that together with the choice of \(\gamma\leq\frac{1}{2\ell}\) and Assumption (QSM) allows to obtain

\[\mathbb{E}_{\boldsymbol{\xi}}\big{\|}\boldsymbol{x}^{t+1}- \boldsymbol{x}^{*}\big{\|}^{2} \leq \Big{(}1-\frac{\gamma\mu}{2}\Big{)}\big{\|}\boldsymbol{x}^{t}- \boldsymbol{x}^{*}\big{\|}^{2}+\frac{2\gamma^{2}\sigma^{2}}{G}+2c\delta\rho^{ 2}\bigg{(}\frac{\gamma}{\mu}+\gamma^{2}\bigg{)}.\]

Next we take full expectation of both sides and obtain

\[\mathbb{E}\big{\|}\boldsymbol{x}^{t+1}-\boldsymbol{x}^{*}\big{\|}^{2}\leq\Big{(} 1-\frac{\gamma\mu}{2}\Big{)}\mathbb{E}\big{\|}\boldsymbol{x}^{t}-\boldsymbol{x} ^{*}\big{\|}^{2}+\frac{2\gamma^{2}\sigma^{2}}{G}+2c\delta\rho^{2}\bigg{(}\frac{ \gamma}{\mu}+\gamma^{2}\bigg{)}.\]

The latter implies

\[\mathbb{E}\big{\|}\boldsymbol{x}^{T}-\boldsymbol{x}^{*}\big{\|}^{2}\leq\Big{(} 1-\frac{\gamma\mu}{2}\Big{)}^{T}\big{\|}\boldsymbol{x}^{0}-\boldsymbol{x}^{*} \big{\|}^{2}+\frac{4\gamma\sigma^{2}}{\mu G}+\frac{4\gamma c\delta\rho^{2}}{\mu }+\frac{4c\delta\rho^{2}}{\mu^{2}},\]

where \(\rho\) is bounded by Lemma D.1 with \(\alpha=1\). 

**Corollary 1**.: _Let assumptions of Theorem 1 hold. Then \(\mathbb{E}\big{\|}\boldsymbol{x}^{T}-\boldsymbol{x}^{*}\big{\|}^{2}\leq\varepsilon\) holds after_

\[T\geq\bigg{(}4+\frac{4\ell}{\mu}+\frac{1}{3c\delta G}\bigg{)}\ln\frac{3R^{2}}{\varepsilon}\]

_iterations of_ SGDA-RA _with \(\gamma=\min\Big{(}\frac{1}{2\ell},\frac{1}{2\mu+\frac{\mu}{c\delta G}}\Big{)}\) and \(b\geq\frac{72c\delta\sigma^{2}}{\mu^{2}\varepsilon}\)._

Proof.: If \(\zeta=0\), \(\rho^{2}=24\sigma^{2}\) the result of Theorem 1 can be simplified as

\[\mathbb{E}\big{\|}\boldsymbol{x}^{T}-\boldsymbol{x}^{*}\big{\|}^{2}\leq\Big{(} 1-\frac{\gamma\mu}{2}\Big{)}^{T}\big{\|}\boldsymbol{x}^{0}-\boldsymbol{x}^{*} \big{\|}^{2}+\frac{2\gamma\sigma^{2}}{\mu G}+\frac{48\gamma c\delta\sigma^{2}} {\mu}+\frac{24c\delta\sigma^{2}}{\mu^{2}}.\]

Applying Lemma C.3 to the last bound we get the result of the corollary. 

### Proofs for Seg-Ra

```
1:RAGg, \(\gamma\)
2:for\(t=1,...\)do
3:forworker \(i\in[n]\)in parallel
4:\(\boldsymbol{g}^{t}_{\boldsymbol{\xi}_{i}}\gets\boldsymbol{g}_{i}( \boldsymbol{x}^{t},\boldsymbol{\xi}_{i})\)
5:send\(\boldsymbol{g}^{t}_{\boldsymbol{\xi}_{i}}\) if \(i\in G\), else send\(*\) if Byzantine
6:\(\widehat{\boldsymbol{g}}_{\boldsymbol{\xi}^{t}}(\boldsymbol{x}^{t})\) = RAGg\((\boldsymbol{g}^{t}_{\boldsymbol{\xi}_{1}},\ldots,\boldsymbol{g}^{t}_{ \boldsymbol{\xi}_{n}})\)
7:\(\widetilde{\boldsymbol{x}}^{t}\leftarrow\boldsymbol{x}^{t}-\gamma_{1}\widehat{ \boldsymbol{g}}_{\boldsymbol{\xi}^{t}}(\boldsymbol{x}^{t})\). // update params using robust aggregate
8:for worker \(i\in[n]\)in parallel
9:\(\boldsymbol{g}^{t}_{\boldsymbol{\eta}_{i}}\gets\boldsymbol{g}_{i}( \widetilde{\boldsymbol{x}}^{t},\boldsymbol{\eta}_{i})\)
10:send\(\boldsymbol{g}^{t}_{\boldsymbol{\eta}_{i}}\) if \(i\in G\), else send\(*\) if Byzantine
11:\(\widehat{\boldsymbol{g}}_{\boldsymbol{\eta}^{t}}(\widetilde{\boldsymbol{x}}^{t})\) = RAGg\((\boldsymbol{g}^{t}_{\boldsymbol{\eta}_{1}},\ldots,\boldsymbol{g}^{t}_{ \boldsymbol{\eta}_{n}})\)
12:\(\boldsymbol{x}^{t+1}\leftarrow\boldsymbol{x}^{t}-\gamma_{2}\widehat{ \boldsymbol{g}}_{\boldsymbol{\eta}^{t}}(\widetilde{\boldsymbol{x}}^{t})\). // update params using robust aggregate
```

**Algorithm 2** Seg-Ra

To analyze the convergence of SEG introduce the following notation

\[\overline{\boldsymbol{g}}_{\boldsymbol{\xi}^{k}}(\boldsymbol{x}^{k})= \boldsymbol{g}_{\boldsymbol{\xi}^{k}}(\boldsymbol{x}^{k})=\frac{1}{G}\sum_{i \in G}\boldsymbol{g}_{i}(\boldsymbol{x}^{k},\boldsymbol{\xi}^{k}_{i})\]

[MISSING_PAGE_FAIL:26]

To upper bound the last term we use simple inequality (16), and apply \(L\)-Lipschitzness of \(F(x)\):

\[\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\left\|\widehat{\mathbf{x} }^{k+1}-\mathbf{x}^{*}\right\|^{2}\right] \stackrel{{\eqref{eq:L1}}}{{\leq}} \left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-\gamma_{1}^{2}\mathbb{E} _{\mathbf{\xi}^{k}}\left[\left\|\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\right\| ^{2}\right]\] \[+4\gamma_{1}^{2}\mathbb{E}_{\mathbf{\xi}^{k}}\left[\left\|\overline{ \mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})-F\left(\widetilde{\mathbf{x}}^{k}\right)\right\| ^{2}\right]\] \[+4\gamma_{1}^{2}\mathbb{E}_{\mathbf{\xi}^{k}}\left[\left\|\overline{ \mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})-F(\mathbf{x}^{k})\right\|^{2}\right]\] \[+4\gamma_{1}^{2}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\left\| \overline{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right)-F\left( \widetilde{\mathbf{x}}^{k}\right)\right\|^{2}\right]\] \[\stackrel{{\eqref{eq:L1},\eqref{eq:L2},\eqref{eq:L3}}} {{\leq}} \left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-\gamma_{1}^{2}\left(1-4L^ {2}\gamma_{1}^{2}\right)\mathbb{E}_{\mathbf{\xi}^{k}}\left[\left\|\widehat{\mathbf{g}} _{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\right\|^{2}\right]\] \[+4\gamma_{1}^{2}\mathbb{E}_{\mathbf{\xi}^{k}}\left[\left\|\overline{ \mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})-\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k}) \right\|^{2}\right]\] \[+\frac{4\gamma_{1}^{2}\sigma^{2}}{G}+\frac{4\gamma_{1}^{2}\sigma^ {2}}{G}\] \[\stackrel{{\eqref{eq:L1},Lemma\ D.1}}{{\leq}} \left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-\gamma_{1}^{2}\left(1-4 \gamma_{1}^{2}L^{2}\right)\mathbb{E}_{\mathbf{\xi}^{k}}\left[\left\|\widehat{\mathbf{ g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\right\|^{2}\right]\] \[+\frac{8\gamma_{1}^{2}\sigma^{2}}{G}+4\gamma_{1}^{2}c\delta\rho^{2}\] \[\stackrel{{\eqref{eq:L2}}}{{\leq}} \left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}+\frac{8\gamma_{1}^{2} \sigma^{2}}{G}+4\gamma_{1}^{2}c\delta\rho^{2}.\]

Finally, we use the above inequality together with (31):

\[\left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-2\widehat{P}_{k}+\gamma _{1}^{2}\mathbb{E}\left[\left\|\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{ \mathbf{x}}^{k})\right\|^{2}\mid\mathbf{x}^{k}\right] \leq \left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}+\frac{8\gamma_{1}^{2} \sigma^{2}}{G}+4\gamma_{1}^{2}c\delta\rho^{2},\]

where \(\widehat{P}_{k}=\gamma_{1}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\left( \overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\mathbf{x}^{k}-\mathbf{x}^{ *}\right)\right]\). Rearranging the terms, we obtain (30). 

**Lemma D.3**.: _Consider_ SEG-RA _(Algorithm 2). Let Assumptions 2, 3, 4 and Corollary 2 hold. If_

\[\gamma_{1}\leq\frac{1}{2\mu+2L}, \tag{34}\]

_then \(\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})=\overline{\mathbf{g}}_{ \mathbf{\eta}^{k}}\left(\mathbf{x}^{k}-\gamma_{1}\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{ x}^{k})\right)\) satisfies the following inequality_

\[\widehat{P}_{k} \geq \frac{\mu\gamma_{1}}{2}\left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}+ \frac{\gamma_{1}^{2}}{4}\mathbb{E}_{\mathbf{\xi}^{k}}\left[\left\|\overline{\mathbf{g} }_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\right\|^{2}\right]-\frac{8\gamma_{1}^{2}\sigma^{2} }{G}-\frac{9\gamma_{1}^{2}c\delta\rho^{2}}{2}, \tag{35}\]

_or simply_

\[-\widehat{P}_{k} \leq\]

_where \(\widehat{P}_{k}=\gamma_{1}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\left( \overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\mathbf{x}^{k}-\mathbf{x}^{ *}\right)\right]\) and \(\rho^{2}=24\sigma^{2}+12\zeta^{2}\) by Lemma D.1 with \(\alpha=1\)._Proof.: Since \(\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}[\}]=\mathbb{E}[\cdot\mid\mathbf{x}^{k}]\) and \(\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})=\overline{\mathbf{g}}_{\bm {\eta}^{k}}\left(\mathbf{x}^{k}-\gamma_{1}\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k })\right)\), we have

\[-\widehat{P}_{k}\] \[= -\gamma_{1}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\langle \overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\mathbf{x}^{k}-\mathbf{x}^{*} \rangle\right]\] \[= -\gamma_{1}\mathbb{E}_{\mathbf{\xi}^{k}}\left[\langle\mathbb{E}_{\mathbf{ \eta}^{k}}[\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})],\mathbf{x}^ {k}-\gamma_{1}\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})-\mathbf{x}^{*}\rangle\right]\] \[\overset{\eqref{eq:14}}{=} -\gamma_{1}\mathbb{E}_{\mathbf{\xi}^{k}}\left[\langle F(\mathbf{x}^{k}- \gamma_{1}\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})),\mathbf{x}^{k}-\gamma_{1} \widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})-\mathbf{x}^{*}\rangle\right]\] \[-\frac{\gamma_{1}^{2}}{2}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}} \left[\left\|\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})\right\| ^{2}\right]-\frac{\gamma_{1}^{2}}{2}\mathbb{E}_{\mathbf{\xi}^{k}}\left[\left\| \widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\right\|^{2}\right]\] \[+\frac{\gamma_{1}^{2}}{2}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}} \left[\left\|\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})-\widehat {\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\right\|^{2}\right]\] \[\overset{\eqref{eq:15}}{\leq} -\mu\gamma_{1}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\left\| \mathbf{x}^{k}-\mathbf{x}^{*}-\gamma_{1}\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k}) \right\|^{2}\right]-\frac{\gamma_{1}^{2}}{2}\mathbb{E}_{\mathbf{\xi}^{k}}\left[ \left\|\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\right\|^{2}\right]\] \[+\frac{4\gamma_{1}^{2}}{2}\mathbb{E}_{\mathbf{\xi}^{k}}\left[\left\| \overline{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})-F\left(\widetilde{\mathbf{x}}^{k} \right)\right\|^{2}\right]\] \[+\frac{4\gamma_{1}^{2}}{2}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}} \left[\left\|\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k} \right)-F\left(\widetilde{\mathbf{x}}^{k}\right)\right\|^{2}\right]\] \[\overset{\eqref{eq:16},\eqref{eq:17},\eqref{eq:18},\eqref{eq:19}, Lem.\,\,D.1,Cor.\,\,2}{\leq} -\frac{\mu\gamma_{1}}{2}\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2}-\frac{\gamma_{1}^ {2}}{2}(1-2\gamma_{1}\mu-4\gamma_{1}^{2}L^{2})\mathbb{E}_{\mathbf{\xi}^{k}}\left[ \left\|\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\right\|^{2}\right]\] \[+\frac{4\gamma_{1}^{2}\sigma^{2}}{2G}+\frac{4\gamma_{1}^{2}\sigma ^{2}}{2G}+4\gamma_{1}^{2}c\delta\rho^{2}\] \[\overset{\eqref{eq:19}}{\leq} -\frac{\mu\gamma_{1}}{2}\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2}- \frac{\gamma_{1}^{2}}{2}\mathbb{E}_{\mathbf{\xi}^{k}}\left[\left\|\widehat{\mathbf{g} }_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\right\|^{2}\right]+\frac{4\gamma_{1}^{2}\sigma^{2 }}{G}+4\gamma_{1}^{2}c\delta\rho^{2}\]

So one have

\[-\widehat{P}_{k} \leq\]

or simply

\[-\widehat{P}_{k} \leq -\frac{\mu\gamma_{1}}{2}\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2} +\frac{4\gamma_{1}^{2}\sigma^{2}}{G}+4\gamma_{1}^{2}c\delta\rho^{2}\]

that concludes the proof. 

#### d.2.2 Quasi-Strongly Monotone Case

Combining Lemmas D.2 and D.3, we get the following result.

**Theorem** (Theorem 2 duplicate).: _Let Assumptions 1, 2, 3 and 4 hold. Then after \(T\) iterations \(\mathsf{SEG-RA}\) (Algorithm 2) with \((\delta,c)\)-\(\mathsf{RA}_{\text{\emph{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text \text{\texttexttexttexttexttexttexttexttexttexttexttexttexttexttext{\texttexttexttexttexttext{  \texttexttexttexttext{\texttexttext{\textProof of Theorem 2.: Since \(\mathbf{x}^{k+1}=\mathbf{x}^{k}-\gamma_{2}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde {\mathbf{x}}^{k}\right)\), we have

\[\left\|\mathbf{x}^{k+1}-\mathbf{x}^{*}\right\|^{2} = \left\|\mathbf{x}^{k}-\gamma_{2}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}\left( \widetilde{\mathbf{x}}^{k}\right)-\mathbf{x}^{*}\right\|^{2}\] \[= \left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-2\gamma_{2}\langle \widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right),\mathbf{x}^{k} -\mathbf{x}^{*}\rangle+\gamma_{2}^{2}\left\|\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}\left( \widetilde{\mathbf{x}}^{k}\right)\right\|^{2}\] \[\leq \left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-2\gamma_{2}\langle \widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right),\mathbf{x}^{k} -\mathbf{x}^{*}\rangle+2\gamma_{2}^{2}\left\|\overline{\mathbf{g}}_{\mathbf{\eta}^{k}} \left(\widetilde{\mathbf{x}}^{k}\right)\right\|^{2}\] \[+ 2\gamma_{2}^{2}\left\|\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}\left( \widetilde{\mathbf{x}}^{k}\right)-\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}\left( \widetilde{\mathbf{x}}^{k}\right)\right\|^{2}+2\gamma_{2}\langle\overline{\mathbf{g}} _{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right)-\widehat{\mathbf{g}}_{\mathbf{ \eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right),\mathbf{x}^{k}-\mathbf{x}^{*}\rangle\] \[\leq (1+\lambda)\left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-2\gamma_{2} \langle\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right), \mathbf{x}^{k}-\mathbf{x}^{*}\rangle+2\gamma_{2}^{2}\left\|\overline{\mathbf{g}}_{\mathbf{ \eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right)\right\|^{2}\] \[+\gamma_{2}^{2}\bigg{(}2+\frac{1}{\lambda}\bigg{)}\left\| \overline{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right)-\widehat {\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right)\right\|^{2}\]

Taking the expectation, conditioned on \(\mathbf{x}^{k}\),

\[\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left\|\mathbf{x}^{k+1}-\mathbf{x} ^{*}\right\|^{2} \leq (1+\lambda)\left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-2\beta\gamma _{1}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\langle\overline{\mathbf{g}}_{\mathbf{\eta} ^{k}}\left(\widetilde{\mathbf{x}}^{k}\right),\mathbf{x}^{k}-\mathbf{x}^{*}\rangle\] \[+2\beta^{2}\gamma_{1}^{2}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}} \left\|\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right) \right\|^{2}+\gamma_{2}^{2}c\delta\rho^{2}\bigg{(}2+\frac{1}{\lambda}\bigg{)},\]

using the definition of \(\widehat{P}_{k}=\gamma_{1}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\langle \overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\mathbf{x}^{k}-\mathbf{x}^{* }\rangle\right]\), we continue our derivation:

\[\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\left\|\mathbf{x}^{k+1}- \mathbf{x}^{*}\right\|^{2}\right]\] \[= (1+\lambda)\left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-2\beta\widehat {P}_{k}+2\beta^{2}\gamma_{1}^{2}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left\| \overline{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right)\right\|^{2}\] \[+\gamma_{2}^{2}c\delta\rho^{2}\bigg{(}2+\frac{1}{\lambda}\bigg{)}\] \[\overset{\text{\eqref{eq:22}}}{\leq} (1+\lambda)\left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-2\beta\widehat {P}_{k}+2\beta^{2}\bigg{(}2\widehat{P}_{k}+\frac{8\gamma_{1}^{2}\sigma^{2}}{G} +4\gamma_{1}^{2}c\delta\rho^{2}\bigg{)}\] \[+\gamma_{2}^{2}c\delta\rho^{2}\bigg{(}2+\frac{1}{\lambda}\bigg{)}\] \[\overset{\text{\eqref{eq:22}}}{\leq} (1+\lambda)\left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}\] \[+2\beta(1-2\beta)\bigg{(}-\frac{\mu\gamma_{1}}{2}\left\|\mathbf{x}^{k }-\mathbf{x}^{*}\right\|^{2}+\frac{4\gamma_{1}^{2}\sigma^{2}}{G}+4\gamma_{1}^{2} c\delta\rho^{2}\bigg{)}\] \[+\frac{16\gamma_{2}^{2}\sigma^{2}}{G}+8\gamma_{2}^{2}c\delta\rho^ {2}+\gamma_{2}^{2}c\delta\rho^{2}\bigg{(}2+\frac{1}{\lambda}\bigg{)}\] \[\leq \Big{(}1+\lambda-2\beta(1-2\beta)\frac{\mu\gamma_{1}}{2}\Big{)} \left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}\] \[+\frac{\gamma_{1}^{2}\sigma^{2}}{G}+\gamma_{1}^{2}c\delta\rho^{2} +\frac{16\gamma_{2}^{2}\sigma^{2}}{G}+8\gamma_{2}^{2}c\delta\rho^{2}+\gamma_{2}^{ 2}c\delta\rho^{2}\bigg{(}2+\frac{1}{\lambda}\bigg{)}\] \[\overset{\text{\ref{eq:22}}}{\leq} \Big{(}1+\lambda-\frac{\mu\gamma_{2}}{2}\Big{)}\left\|\mathbf{x}^{k}- \mathbf{x}^{*}\right\|^{2}+\frac{\sigma^{2}}{G}\big{(}\gamma_{1}^{2}+16\gamma_{2}^{ 2}\big{)}\] \[+c\delta\rho^{2}\bigg{(}\gamma_{1}^{2}+10\gamma_{2}^{2}+\frac{ \gamma_{2}^{2}}{\lambda}\bigg{)}\] \[\overset{\text{\ref{eq:22}}}{\leq} \Big{(}1-\frac{\mu\gamma_{2}}{4}\Big{)}\left\|\mathbf{x}^{k}-\mathbf{x}^{ *}\right\|^{2}+\frac{\sigma^{2}}{G}\big{(}\gamma_{1}^{2}+16\gamma_{2}^{2}\big{)}\] \[+c\delta\rho^{2}\bigg{(}\gamma_{1}^{2}+10\gamma_{2}^{2}+\frac{4 \gamma_{2}}{\mu}\bigg{)}\]Next, we take the full expectation from the both sides

\[\mathbb{E}\left[\left\|\mathbf{x}^{k+1}-\mathbf{x}^{*}\right\|^{2}\right]\leq\Big{(}1- \frac{\mu\gamma_{2}}{4}\Big{)}\mathbb{E}\left[\left\|\mathbf{x}^{k}-\mathbf{x}^{*} \right\|^{2}\right]+\frac{\sigma^{2}}{G}\big{(}\gamma_{1}^{2}+16\gamma_{2}^{2} \big{)}+c\delta\rho^{2}\bigg{(}\gamma_{1}^{2}+10\gamma_{2}^{2}+\frac{4\gamma_{ 2}}{\mu}\bigg{)}. \tag{38}\]

Unrolling the recurrence, together with the bound on \(\rho\) given by Lemma D.1 we derive the result of the theorem:

\[\mathbb{E}\left[\left\|\mathbf{x}^{K}-\mathbf{x}^{*}\right\|^{2}\right]\leq\Big{(}1- \frac{\mu\gamma_{2}}{4}\Big{)}^{K}\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2}+ \frac{4\sigma^{2}(\gamma_{1}^{2}+16\gamma_{2}^{2})}{\mu\gamma_{2}G}+\frac{4c \delta\rho^{2}(\gamma_{1}^{2}+10\gamma_{2}^{2}+\frac{4\gamma_{2}}{\mu})}{\mu \gamma_{2}}.\]

**Corollary 3**.: _Let assumptions of Theorem 2 hold. Then \(\mathbb{E}\big{\|}\mathbf{x}^{T}-\mathbf{x}^{*}\big{\|}^{2}\leq\varepsilon\) holds after_

\[T\geq 4\bigg{(}\frac{2}{\beta}+\frac{2\ell}{\beta\mu}+\frac{1}{3\beta c\delta G }\bigg{)}\ln\frac{3R^{2}}{\varepsilon}\]

_iterations of_ SEG-RA _with \(\gamma_{1}=\min\Bigl{(}\frac{1}{2\mu+2L},\frac{1}{2\mu+\frac{\mu}{12c\delta G}} \Bigr{)}\) and \(b\geq\frac{288c\delta\rho^{2}}{\beta\mu^{2}\varepsilon}\)._

Proof.: Next, we plug \(\gamma_{2}=\beta\gamma_{1}\leq\nicefrac{{\gamma_{1}}}{{4}}\) into the result of Theorem 2 and obtain

\[\mathbb{E}\left[\left\|\mathbf{x}^{k+1}-\mathbf{x}^{*}\right\|^{2}\right]\leq\bigg{(}1 -\frac{\mu\beta\gamma_{1}}{4}\bigg{)}\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2} +\frac{8\gamma_{1}^{2}\sigma^{2}}{\mu\beta G}+\frac{8\gamma_{1}c\delta\rho^{2 }}{\beta\mu}+\frac{16c\delta\rho^{2}}{\beta\mu^{2}}. \tag{39}\]

If \(\zeta=0\), \(\rho^{2}=24\sigma^{2}\) the last reccurence can be unrolled as

\[\mathbb{E}\big{\|}\mathbf{x}^{T}-\mathbf{x}^{*}\big{\|}^{2}\leq\ \bigg{(}1- \frac{\mu\beta\gamma_{1}}{4}\bigg{)}^{T}\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^ {2}+\frac{8\gamma_{1}\sigma^{2}}{\mu\beta G}+\frac{8\cdot 24\gamma_{1}c \delta\sigma^{2}}{\beta\mu}+\frac{16\cdot 24c\delta\sigma^{2}}{\beta\mu^{2}}.\]

Applying Lemma C.3 to the last bound we get the result of the corollary. 

### Proofs for M-Sgda-Ra

```
1:Ragg, \(\gamma\), \(\alpha\in[0,1]\)
2:for\(t=0,...\)do
3:for worker \(i\in[n]\)in parallel
4:\(\mathbf{g}_{i}^{t}\leftarrow\mathbf{g}_{i}(\mathbf{x}^{t},\mathbf{\xi}_{i})\) and \(\mathbf{m}_{i}^{t}\leftarrow(1-\alpha)\mathbf{m}_{i}^{t-1}+\alpha\mathbf{g}_{i}^{t}\) // worker momentum
5:send\(\mathbf{m}_{i}^{t}\) if \(i\in\mathbb{G}\), else send \(*\) if Byzantine
6:\(\widehat{\mathbf{m}}^{t}\) = Ragg (\(\mathbf{m}_{1}^{t},\ldots,\mathbf{m}_{n}^{t}\)) and \(\mathbf{x}^{t+1}\leftarrow\mathbf{x}^{t}-\gamma\widehat{\mathbf{m}}^{t}\). // update params using robust aggregate
```

**Algorithm 3** M-Sgda-Ra

#### d.3.1 Quasi-Strongly Monotone Case

**Theorem** (Theorem 3 duplicate).: _Let Assumptions 1, 2, 4, and 6 hold. Then after \(T\) iterations M-Sgda-RA (Algorithm 3) with \((\delta,c)\)-Ragg outputs \(\overline{\mathbf{x}}^{T}\) such that_

\[\mathbb{E}\Big{[}\big{\|}\overline{\mathbf{x}}^{T}-\mathbf{x}^{*}\big{\|}^{2}\Big{]} \leq\frac{2\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2}}{\mu\gamma\alpha W_{T}}+ \frac{4c\delta\rho^{2}}{\mu^{2}\alpha^{2}}+\frac{8\gamma c\delta\rho^{2}}{\mu \alpha^{2}}+\frac{6\gamma\sigma^{2}}{\mu\alpha^{2}G},\]

_where \(\overline{\mathbf{x}}^{T}=\frac{1}{W_{T}}\sum_{t=0}^{T}w_{t}\widehat{\mathbf{x}}^{t}\), \(\widehat{\mathbf{x}}^{t}=\frac{\alpha}{1-(1-\alpha)^{t+1}}\sum_{j=0}^{t}(1-\alpha )^{t-j}\mathbf{x}^{j}\), \(w_{t}=\left(1-\frac{\mu\gamma\alpha}{2}\right)^{-t-1}\), and \(W_{T}=\sum_{t=0}^{T}w_{t}\) and \(\rho^{2}=24\sigma^{2}+12\zeta^{2}\) by Lemma D.1._Proof of Theorem 3.: Since \(\widetilde{\mathbf{m}}^{t}=\widetilde{\mathbf{m}}^{t}-\overline{\mathbf{m}}^{t}+\overline{\mathbf{m}} ^{t}\) and \(\overline{\mathbf{m}}^{t}=\alpha\overline{\mathbf{q}}^{t}+(1-\alpha)\overline{\mathbf{m}}^{t-1}\) one has

\[\left\|\mathbf{x}^{t+1}-\mathbf{x}^{*}\right\|^{2} = \left\|\mathbf{x}^{t}-\mathbf{x}^{*}-\gamma\widehat{\mathbf{m}}^{t}\right\|^{2 }=\left\|\mathbf{x}^{t}-\mathbf{x}^{*}\right\|^{2}-2\gamma\langle\widehat{\mathbf{m}}^{t}, \mathbf{x}^{t}-\mathbf{x}^{*}\rangle+\gamma^{2}\big{\|}\widehat{\mathbf{m}}^{t}\big{\|}^{2}=\] \[= \left\|\mathbf{x}^{t}-\mathbf{x}^{*}\right\|^{2}-2\gamma\langle\widehat{ \mathbf{m}}^{t}-\overline{\mathbf{m}}^{t},\mathbf{x}^{t}-\mathbf{x}^{*}\rangle+\gamma^{2} \big{\|}\widehat{\mathbf{m}}^{t}\big{\|}^{2}\] \[-2\gamma\langle\overline{\mathbf{m}}^{t},\mathbf{x}^{t}-\mathbf{x}^{*}\rangle.\]

Next, unrolling the following recursion

\[\langle\overline{\mathbf{m}}^{t},\mathbf{x}^{t}-\mathbf{x}^{*}\rangle = \alpha\langle\overline{\mathbf{q}}^{t},\mathbf{x}^{t}-\mathbf{x}^{*}\rangle+( 1-\alpha)\langle\overline{\mathbf{m}}^{t-1},\mathbf{x}^{t}-\mathbf{x}^{*}\rangle\] \[= \alpha\langle\overline{\mathbf{q}}^{t},\mathbf{x}^{t}-\mathbf{x}^{*}\rangle+( 1-\alpha)\langle\overline{\mathbf{m}}^{t-1},\mathbf{x}^{t-1}-\mathbf{x}^{*}\rangle+(1- \alpha)\langle\overline{\mathbf{m}}^{t-1},\mathbf{x}^{t}-\mathbf{x}^{t-1}\rangle\] \[= \alpha\langle\overline{\mathbf{q}}^{t},\mathbf{x}^{t}-\mathbf{x}^{*}\rangle+( 1-\alpha)\langle\overline{\mathbf{m}}^{t-1},\mathbf{x}^{t-1}-\mathbf{x}^{*}\rangle+(1- \alpha)\gamma\langle\overline{\mathbf{m}}^{t-1},\widehat{\mathbf{m}}^{t}\rangle\]

one obtains

\[\langle\overline{\mathbf{m}}^{t},\mathbf{x}^{t}-\mathbf{x}^{*}\rangle = \alpha\sum_{j=0}^{t}(1-\alpha)^{t-j}\langle\overline{\mathbf{g}}^{j}, \mathbf{x}^{j}-\mathbf{x}^{*}\rangle-(1-\alpha)\gamma\sum_{j=1}^{t}(1-\alpha)^{t-j} \langle\overline{\mathbf{m}}^{j-1},\widehat{\mathbf{m}}^{j}\rangle\]

Applying the latter and (11) for \(\langle\widehat{\mathbf{m}}^{t}-\overline{\mathbf{m}}^{t},\mathbf{x}^{t}-\mathbf{x}^{*}\rangle\) with \(\lambda=\frac{\mu\gamma\alpha}{2}\) we obtain

\[2\alpha\gamma\sum_{j=0}^{t}(1-\alpha)^{t-j}\langle\overline{\mathbf{ g}}^{j},\mathbf{x}^{j}-\mathbf{x}^{*}\rangle\] \[\leq \left(1+\frac{\mu\gamma\alpha}{2}\right)\big{\|}\mathbf{x}^{t}-\mathbf{x} ^{*}\big{\|}^{2}-\left\|\mathbf{x}^{t+1}-\mathbf{x}^{*}\right\|^{2}+\frac{2\gamma}{\mu \alpha}\big{\|}\widehat{\mathbf{m}}^{t}-\overline{\mathbf{m}}^{t}\big{\|}^{2}\] \[+\gamma^{2}\big{\|}\widehat{\mathbf{m}}^{t}\big{\|}^{2}+2\gamma^{2}( 1-\alpha)\sum_{j=1}^{t}(1-\alpha)^{t-j}\langle\overline{\mathbf{m}}^{j-1},\widehat {\mathbf{m}}^{j}\rangle\] \[\stackrel{{\eqref{eq:2}}}{{\leq}} \left(1+\frac{\mu\gamma\alpha}{2}\right)\big{\|}\mathbf{x}^{t}-\mathbf{x} ^{*}\big{\|}^{2}-\left\|\mathbf{x}^{t+1}-\mathbf{x}^{*}\right\|^{2}+\frac{2\gamma}{\mu \alpha}\big{\|}\widehat{\mathbf{m}}^{t}-\overline{\mathbf{m}}^{t}\big{\|}^{2}\] \[+\gamma^{2}\big{\|}\widehat{\mathbf{m}}^{t}\big{\|}^{2}+\gamma^{2} \sum_{j=1}^{t}(1-\alpha)^{t-j}\big{\|}\widehat{\mathbf{m}}^{j}\big{\|}^{2}\] \[+\gamma^{2}(1-\alpha)^{2}\sum_{j=1}^{t}(1-\alpha)^{t-j}\big{\|} \overline{\mathbf{m}}^{j-1}\big{\|}^{2}\] \[\stackrel{{\eqref{eq:2}}}{{\leq}} \left(1+\frac{\mu\gamma\alpha}{2}\right)\big{\|}\mathbf{x}^{t}-\mathbf{x} ^{*}\big{\|}^{2}-\left\|\mathbf{x}^{t+1}-\mathbf{x}^{*}\right\|^{2}+\frac{2\gamma}{\mu \alpha}\big{\|}\widehat{\mathbf{m}}^{t}-\overline{\mathbf{m}}^{t}\big{\|}^{2}\] \[+4\gamma^{2}\sum_{j=1}^{t}(1-\alpha)^{t-j}\big{\|}\widehat{\mathbf{m }}^{j}-\overline{\mathbf{m}}^{j}\big{\|}^{2}\] \[+3\gamma^{2}\sum_{j=1}^{t}(1-\alpha)^{t-j}\big{\|}\overline{\mathbf{ m}}^{j-1}\big{\|}^{2}\]

Since \(\overline{\mathbf{m}}^{t}=\alpha\sum_{j=0}^{t}(1-\alpha)^{t-j}\overline{\mathbf{g}}^{j}\) and hence \(\big{\|}\overline{\mathbf{m}}^{t}\big{\|}^{2}\leq\alpha\sum_{j=0}^{t}(1-\alpha)^{t- j}\big{\|}\overline{\mathbf{g}}^{j}\big{\|}^{2}\) one has

\[2\alpha\gamma\sum_{j=0}^{t}(1-\alpha)^{t-j}\langle\overline{\mathbf{g }}^{j},\mathbf{x}^{j}-\mathbf{x}^{*}\rangle\] \[\leq \left(1+\frac{\mu\gamma\alpha}{2}\right)\big{\|}\mathbf{x}^{t}-\mathbf{x} ^{*}\big{\|}^{2}-\left\|\mathbf{x}^{t+1}-\mathbf{x}^{*}\right\|^{2}+\frac{2\gamma}{\mu \alpha}\big{\|}\widehat{\mathbf{m}}^{t}-\overline{\mathbf{m}}^{t}\big{\|}^{2}\] \[+4\gamma^{2}\sum_{j=1}^{t}(1-\alpha)^{t-j}\big{\|}\widehat{\mathbf{m }}^{j}-\overline{\mathbf{m}}^{j}\big{\|}^{2}\] \[+3\gamma^{2}\alpha\sum_{j=1}^{t}(1-\alpha)^{t-j}\sum_{i=0}^{j}(1- \alpha)^{j-i}\big{\|}\overline{\mathbf{g}}^{i}\big{\|}^{2}.\]Next by taking an expectation \(\mathbb{E}_{\mathbf{\xi}}\) of both sides of the above inequality and rearranging terms obtain

\[2\alpha\gamma\sum_{j=0}^{t}(1-\alpha)^{t-j}\langle F^{j},\mathbf{x}^{j}- \mathbf{x}^{*}\rangle\] \[\leq \Big{(}1+\frac{\mu\gamma\alpha}{2}\Big{)}\big{\|}\mathbf{x}^{t}-\mathbf{x} ^{*}\big{\|}^{2}-\mathbb{E}_{\mathbf{\xi}}\big{\|}\mathbf{x}^{t+1}-\mathbf{x}^{*}\big{\|}^{2}\] \[+\frac{2\gamma}{\mu\alpha}\mathbb{E}_{\mathbf{\xi}}\big{\|}\mathbf{\widetilde {m}}^{t}-\mathbf{\overline{m}}^{t}\big{\|}^{2}+4\gamma^{2}\sum_{j=1}^{t}(1-\alpha) ^{t-j}\mathbb{E}_{\mathbf{\xi}}\big{\|}\mathbf{\widetilde{m}}^{j}-\mathbf{\overline{m}}^{ j}\big{\|}^{2}\] \[+3\gamma^{2}\alpha\sum_{j=1}^{t}(1-\alpha)^{t-j}\sum_{i=0}^{j}(1 -\alpha)^{j-i}\mathbb{E}_{\mathbf{\xi}}\big{\|}\mathbf{\overline{g}}^{i}\big{\|}^{2}.\]

Next we use Lemma C.1 to bound \(\mathbb{E}_{\mathbf{\xi}}\big{\|}\mathbf{\overline{g}}^{j}\big{\|}^{2}\) and Assumption (QSM) to obtain the following bound

\[-\alpha\sum_{j=0}^{t}(1-\alpha)^{t-j}\langle F^{j},\mathbf{x}^{j}- \mathbf{x}^{*}\rangle \leq -\alpha\sum_{j=0}^{t}(1-\alpha)^{t-j}\big{\|}\mathbf{x}^{j}-\mathbf{x}^{* }\big{\|}^{2}\leq-\alpha\mu\big{\|}\mathbf{x}^{t}-\mathbf{x}^{*}\big{\|}^{2}.\]

Gathering the above results we have

\[\alpha\gamma\sum_{j=0}^{t}(1-\alpha)^{t-j}\langle F^{j},\mathbf{x}^{j }-\mathbf{x}^{*}\rangle\] \[\leq \Big{(}1-\frac{\mu\gamma\alpha}{2}\Big{)}\big{\|}\mathbf{x}^{t}-\mathbf{x }^{*}\big{\|}^{2}-\mathbb{E}_{\mathbf{\xi}}\big{\|}\mathbf{x}^{t+1}-\mathbf{x}^{*}\big{\|}^ {2}\] \[+\frac{2\gamma}{\mu\alpha}\mathbb{E}_{\mathbf{\xi}}\big{\|}\mathbf{ \widetilde{m}}^{t}-\mathbf{\overline{m}}^{t}\big{\|}^{2}+4\gamma^{2}\sum_{j=1}^{t }(1-\alpha)^{t-j}\mathbb{E}_{\mathbf{\xi}}\big{\|}\mathbf{\widehat{m}}^{j}-\mathbf{ \overline{m}}^{j}\big{\|}^{2}\] \[+3\gamma^{2}\alpha\ell\sum_{j=1}^{t}(1-\alpha)^{t-j}\sum_{i=0}^{j }(1-\alpha)^{j-i}\langle F(\mathbf{x}^{i}),\mathbf{x}^{i}-\mathbf{x}^{*}\rangle\] \[+\frac{3\gamma^{2}\alpha\sigma^{2}}{G}\sum_{j=1}^{t}(1-\alpha)^{t -j}\sum_{i=0}^{j}(1-\alpha)^{j-i}.\]

Next we take full expectation of both sides and obtain

\[\alpha\gamma\sum_{j=0}^{t}(1-\alpha)^{t-j}\mathbb{E}\langle F^{j },\mathbf{x}^{j}-\mathbf{x}^{*}\rangle\] \[\leq \Big{(}1-\frac{\mu\gamma\alpha}{2}\Big{)}\mathbb{E}\big{\|}\mathbf{x }^{t}-\mathbf{x}^{*}\big{\|}^{2}-\mathbb{E}\big{\|}\mathbf{x}^{t+1}-\mathbf{x}^{*}\big{\|}^ {2}\] \[+\frac{2\gamma}{\mu\alpha}\mathbb{E}\big{\|}\mathbf{\widetilde{m}}^{t }-\mathbf{\overline{m}}^{t}\big{\|}^{2}+4\gamma^{2}\sum_{j=1}^{t}(1-\alpha)^{t-j} \mathbb{E}\big{\|}\mathbf{\widehat{m}}^{j}-\mathbf{\overline{m}}^{j}\big{\|}^{2}\] \[+3\gamma^{2}\alpha\ell\sum_{j=1}^{t}(1-\alpha)^{t-j}\sum_{i=0}^{j }(1-\alpha)^{j-i}\mathbb{E}\langle F(\mathbf{x}^{i}),\mathbf{x}^{i}-\mathbf{x}^{*}\rangle\] \[+\frac{3\gamma^{2}\sigma^{2}}{\alpha G}.\]

Introducing the following notation

\[\mathbf{Z}^{t}=\sum_{j=0}^{t}(1-\alpha)^{t-j}\mathbb{E}\langle F^{j},\mathbf{x}^{j}- \mathbf{x}^{*}\rangle\]

[MISSING_PAGE_EMPTY:33]

If \(\gamma\leq\frac{\alpha}{12d}\) then the following is true

\[\frac{\alpha\gamma}{2}\sum_{t=0}^{T}w_{t}\mathbf{Z}^{t}\leq\left\|\mathbf{x}^{0}-\mathbf{x}^{ *}\right\|^{2}+W_{T}\bigg{(}\frac{2\gamma c\delta\rho^{2}}{\mu\alpha}+\frac{4 \gamma^{2}c\delta\rho^{2}}{\alpha}+\frac{3\gamma^{2}\sigma^{2}}{\alpha G}\bigg{)}. \tag{40}\]

Using the notations for \(\mathbf{Z}^{t}\) and (QSM) we have

\[\mathbf{Z}^{t}=\sum_{j=0}^{t}(1-\alpha)^{t-j}\mathbb{E}\langle F^{j},\mathbf{x}^{j}-\bm {x}^{*}\rangle\geq\mu\sum_{j=0}^{t}(1-\alpha)^{t-j}\mathbb{E}\|\mathbf{x}^{j}-\mathbf{ x}^{*}\|.\]

and consequently by Jensen's inequality

\[\mathbf{Z}^{t}\geq\mu\sum_{j=0}^{t}(1-\alpha)^{t-j}\mathbb{E}\|\mathbf{x}^{j}-\mathbf{x}^{ *}\|\geq\mu\frac{1-(1-\alpha)^{t+1}}{\alpha}\mathbb{E}\Bigg{\|}\mathbf{x}^{*}-\sum _{j=0}^{t}\frac{\alpha(1-\alpha)^{t-j}}{1-(1-\alpha)^{t+1}}\mathbf{x}^{j}\Bigg{\|}.\]

With the definition \(\widehat{\mathbf{x}}^{t}=\frac{\alpha}{1-(1-\alpha)^{t+1}}\sum_{j=0}^{t}(1-\alpha )^{t-j}\mathbf{x}^{j}\) then the above implies that

\[\mathbf{Z}^{t}\geq\mu\mathbb{E}\big{\|}\widehat{\mathbf{x}}^{t}-\mathbf{x}^{*}\big{\|},\]

that together with (40) gives

\[\frac{\alpha\gamma\mu}{2}\sum_{t=0}^{T}w_{t}\mathbb{E}\big{\|}\widehat{\mathbf{x}} ^{t}-\mathbf{x}^{*}\big{\|}\leq\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2}+W_{T} \bigg{(}\frac{2\gamma c\delta\rho^{2}}{\mu\alpha}+\frac{4\gamma^{2}c\delta\rho ^{2}}{\alpha}+\frac{3\gamma^{2}\sigma^{2}}{\alpha G}\bigg{)}. \tag{41}\]

Applying the Jensen inequality again with \(\overline{\mathbf{x}}^{T}=\frac{1}{W_{T}}\sum_{t=0}^{T}w_{t}\widehat{\mathbf{x}}^{t}\) we derive the final result

\[\mathbb{E}\Big{[}\big{\|}\overline{\mathbf{x}}^{T}-\mathbf{x}^{*}\big{\|}^{2}\Big{]} \leq\frac{2\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2}}{\mu\gamma\alpha W_{T}}+ \frac{4c\delta\rho^{2}}{\mu^{2}\alpha^{2}}+\frac{8\gamma c\delta\rho^{2}}{\mu \alpha^{2}}+\frac{6\gamma\sigma^{2}}{\mu\alpha^{2}G}, \tag{42}\]

together with the bound on \(\rho\) given by Lemma D.1. 

**Corollary 4**.: _Let assumptions of Theorem 3 hold. Then \(\mathbb{E}\big{\|}\overline{\mathbf{x}}^{T}-\mathbf{x}^{*}\big{\|}^{2}\leq\varepsilon\) holds after_

\[T\geq\frac{1}{\alpha}\bigg{(}4+\frac{24\epsilon}{\mu\alpha}+\frac{1}{8c\delta G }\bigg{)}\ln\frac{3R^{2}}{\varepsilon}\]

_iterations of_ M-Sgda-ra _with \(\gamma=\min\Bigl{(}\frac{\alpha}{12\epsilon},\frac{1}{2\mu+\frac{1}{16c\delta G }}\Bigr{)}\)._

Proof.: If \(\zeta=0\), \(\rho^{2}=24\sigma^{2}\) the result of Theorem 3 can be simplified as

\[\mathbb{E}\Big{[}\big{\|}\overline{\mathbf{x}}^{T}-\mathbf{x}^{*}\big{\|}^{2}\Big{]} \leq\frac{2\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2}}{\mu\gamma\alpha W_{T}}+ \frac{4\cdot 24c\delta\sigma^{2}}{\mu^{2}\alpha^{2}}+\frac{8\cdot 24\gamma c \delta\sigma^{2}}{\mu\alpha^{2}}+\frac{6\gamma\sigma^{2}}{\mu\alpha^{2}G}.\]

Since \(\frac{2}{\mu\gamma\alpha W_{T}}\ \leq\bigl{(}1-\frac{\mu\gamma\alpha}{2}\bigr{)}^{T+1}\) we can apply Lemma C.3 and get the result of the corollary.

Methods with random check of computations

We replace \((\delta_{\max},c)\)-Ragg with the simple mean, but introduce additional verification that have to be passed to accept the mean. The advantage of such aggregation that it coincides with "good" mean if there are no peers violating the protocol But if there is at least one peer violating the protocol at iteration \(t\) we can bound the variance similar to Lemma D.1.

```
0:\(t\), \(G_{t}\cup\beta_{t}\), \(C_{t}\), Banned\({}_{t}=\varnothing\)
1:\(C_{t+1}=\{c_{1}^{t+1},\ldots,c_{m}^{t+1}\}\), \(C_{t+1}\subset(G_{t}\cup\beta_{t})\setminus C_{t}\) and \(\mathcal{U}_{t+1}=\{u_{1}^{t+1},\ldots,u_{m}^{t+1}\}\), \(\mathcal{U}_{t+1}\subset(G_{t}\cup\beta_{t})\setminus C_{t}\), where \(2m\) workers \(c_{1}^{t+1},\ldots,c_{m}^{t+1},u_{1}^{t+1},\ldots,u_{m}^{t+1}\) are choisen uniformly at random without replacement.
2:for\(i=1,...,m\)in parallel\(c_{i}^{t+1}\) checks computations of \(u_{i}^{t+1}\) during the next iteration
3:\(c_{i}^{t+1}\) receives a query to recalculate \(\boldsymbol{g}(\boldsymbol{x}^{t},\boldsymbol{\xi}_{u_{i}^{t+1}}^{t})\)
4:\(c_{i}^{t+1}\) sends the recalculated \(\boldsymbol{g}(\boldsymbol{x}^{t},\boldsymbol{\xi}_{u_{i}^{t+1}}^{t})\)
5:for\(i=1,...,m\)do
6:if\(\boldsymbol{g}(\boldsymbol{x}^{t},\boldsymbol{\xi}_{u_{i}^{t}}^{t})\neq \boldsymbol{g}_{u_{i}^{t}}^{t}\)then
7: Banned\({}_{t}=\text{Banned}_{t}\cup\{u_{i}^{t},c_{i}^{t}\}\).
8:endif
9:\(C_{t+1},G_{t}\cup\beta_{t}\setminus\text{Banned}_{t}\)
```

**Algorithm 4**CheckComputations

**Lemma E.1**.: _Let Assumption 2 is satisfied with \(\zeta=0\). Then the error between the ideal average \(\overline{\boldsymbol{g}}^{t}\) and the average with the recomputation rule \(\boldsymbol{g}^{t}\) can be bounded as_

\[\mathbb{E}_{\boldsymbol{\xi}}\big{\|}\widehat{\boldsymbol{g}}^{t}-\overline{ \boldsymbol{g}}^{t}\big{\|}^{2}\leq\rho^{2}\mathbb{1}_{t},\]

_where \(\rho^{2}=q\sigma^{2}\) with \(q=2C^{2}+12+\frac{12}{n-2B-m}\) and \(C=\Theta(1)\)._

Proof of Lemma e.1.: Denote the set \(\widetilde{G}\) in the following way \(\widetilde{G}=\{i\in G_{t}\setminus C_{t}:\|\widehat{\boldsymbol{g}}^{t}- \boldsymbol{g}_{i}^{t}\|\leq C\sigma\}\).

\[\widehat{\boldsymbol{g}}^{t}=\left\{\begin{matrix}\frac{1}{n}\sum_{i=1}^{n} \boldsymbol{g}_{i}^{t},&\text{if number of workers}>\frac{n}{2},\\ \textbf{recompute},&\text{otherwise}.\end{matrix}\right.\]

So that we have

\[\big{\|}\widehat{\boldsymbol{g}}^{t}-\overline{\boldsymbol{g}}^{ t}\big{\|}^{2} = \left\|\widehat{\boldsymbol{g}}^{t}-\frac{1}{|\widetilde{G}|} \sum_{i\in\widetilde{G}}\boldsymbol{g}_{i}^{t}+\frac{1}{|\widetilde{G}|} \sum_{i\in\widetilde{G}}\boldsymbol{g}_{i}^{t}-\overline{\boldsymbol{g}}^{t} \right\|^{2}\] \[\leq 2\Bigg{\|}\widehat{\boldsymbol{g}}^{t}-\frac{1}{|\widetilde{G}| }\sum_{i\in\widetilde{G}}\boldsymbol{g}_{i}^{t}\Bigg{\|}^{2}+2\Bigg{\|}\frac {1}{|\widetilde{G}|}\sum_{i\in\widetilde{G}}\boldsymbol{g}_{i}^{t}-\overline {\boldsymbol{g}}^{t}\Bigg{\|}^{2}\] \[\leq 2C^{2}\sigma^{2}+2\Bigg{\|}\frac{1}{|\widetilde{G}|}\sum_{i\in \widetilde{G}}\boldsymbol{g}_{i}^{t}-\overline{\boldsymbol{g}}^{t}\Bigg{\|}^{2}\]

If \(\delta\leq\nicefrac{{1}}{{4}}\) then an acceptance of \(\widehat{\boldsymbol{g}}^{t}\) implies that \(|\widetilde{G}|>\nicefrac{{n}}{{4}}\) and \(|\widetilde{G}|>\nicefrac{{|G_{t}\setminus C_{t}|}}{{3}}\).

\[\Bigg{\|}\frac{1}{|\widetilde{G}|}\sum_{i\in\widetilde{G}} \boldsymbol{g}_{i}^{t}-\overline{\boldsymbol{g}}^{t}\Bigg{\|}^{2} \leq \frac{1}{|\widetilde{G}|}\sum_{i\in\widetilde{G}}\lVert\boldsymbol {g}_{i}^{t}-\overline{\boldsymbol{g}}^{t}\rVert^{2}\leq\frac{1}{|\widetilde{G}| }\sum_{i\in G_{t}\setminus C_{t}}\lVert\boldsymbol{g}_{i}^{t}-\overline{ \boldsymbol{g}}^{t}\rVert^{2}\] \[\leq \frac{3}{|\widetilde{G}_{t}\setminus C_{t}|}\sum_{i\in G_{t} \setminus C_{t}}\lVert\boldsymbol{g}_{i}^{t}-\overline{\boldsymbol{g}}^{t} \rVert^{2}\]Bringing the above results together gives that

\[\mathbb{E}\big{\|}\widehat{\mathbf{g}}^{t}-\overline{\mathbf{g}}^{t}\big{\|}^{2}\leq 2C^{2 }\sigma^{2}+\frac{6}{\left|\widehat{G}_{t}\setminus C_{t}\right|}\sum_{i\in G} \mathbb{E}\big{\|}\mathbf{g}_{i}^{t}-\overline{\mathbf{g}}^{t}\big{\|}^{2}\]

Since checks of computations are only possible in homogeneous case (\(\zeta=0\)) then \(\mathbb{E}\|\mathbf{g}_{i}^{t}-F^{t}\|^{2}=\sigma^{2}\) and

\[\mathbb{E}_{\mathbf{\xi}}\big{\|}\mathbf{g}_{i}^{t}-\overline{\mathbf{g}}^{t}\big{\|}^{2} \leq 2\mathbb{E}_{\mathbf{\xi}}\big{\|}\mathbf{g}_{i}^{t}-F^{t}\big{\|}^{2}+2\mathbb{E }_{\mathbf{\xi}}\big{\|}F^{t}-\overline{\mathbf{g}}^{t}\big{\|}^{2}\leq 2\sigma^{2}+ \frac{2\sigma^{2}}{\left|\bar{G}\right|}. \tag{43}\]

Since \(\left|\widehat{G}_{t}\setminus C_{t}\right|>n-2B-m\)

\[\mathbb{E}_{\mathbf{\xi}}\big{\|}\widehat{\mathbf{g}}^{t}-\overline{\mathbf{g}}^{t}\big{\|} ^{2}\leq 2C^{2}\sigma^{2}+12\sigma^{2}+\frac{12\sigma^{2}}{\left|\widehat{G}_ {t}\setminus C_{t}\right|}\leq 2C^{2}\sigma^{2}+12\sigma^{2}+\frac{12\sigma^{2}}{n- 2B-m}.\]

### Proofs for Sgda-Cc

```
1:\(\gamma\)
2:\(C_{0}=\varnothing\)
3:for\(t=1,...\)do
4:for worker \(i\in(\widehat{G}_{t}\cup\mathcal{B}_{t})\setminus C_{t}\)in parallel
5:end\(\mathbf{g}_{i}^{t}=\begin{cases}\mathbf{g}_{i}(\mathbf{x}^{t},\mathbf{\xi}_{i}),&\text{if }i\in\widehat{G}_{t}\setminus C_{t},\\ \ast,&\text{if }i\in\mathcal{B}_{t}\setminus C_{t},\end{cases}\)
6:\(\widehat{\mathbf{g}}^{t}=\frac{1}{W_{t}}\sum_{i\in W_{t}}\mathbf{g}_{i}^{t},\ W_{t}=(\widehat{G}_{t}\cup\mathcal{B}_{t})\setminus C_{t}\)
7:if\(\left|\{i\in W_{t}\mid\|\widehat{\mathbf{g}}^{t}-\mathbf{g}_{i}^{t}\|\leq C\sigma\} \right|\geq\nicefrac{{\left|\left|\mathbf{w}_{i}\right|}}{{2}}\)then
8:\(\mathbf{x}^{t+1}\leftarrow\mathbf{x}^{t}-\gamma\widehat{\mathbf{g}}^{t}\).
9:else
10: recompute
11:endif
12:\(C_{t+1},\widehat{G}_{t+1}\cup\mathcal{B}_{t+1}=\mathsf{CheckComputations}(C_{t},\widehat{G}_{t}\cup\mathcal{B}_{t})\)
```

**Algorithm 5** Sgda-Cc

#### e.1.1 Star Co-coercieve Case

Next we provide convergence guarantees for Sgda-Cc (Algorithm 5) under Assumption 6.

**Theorem 9**.: _Let Assumptions 1 and 6 hold. Next, assume that_

\[\gamma=\min\left\{\frac{1}{2\ell},\sqrt{\frac{(n-2B-m)R^{2}}{6\sigma^{2}K}}, \sqrt{\frac{m^{2}R^{2}}{72\rho^{2}B^{2}n^{2}}}\right\} \tag{44}\]

_where \(\rho^{2}=q\sigma^{2}\) with \(q=2C^{2}+12+\frac{12}{n-2B-m}\) and \(C=\Theta(1)\) by Lemma E.1 and \(R\geq\|\mathbf{x}^{0}-\mathbf{x}^{*}\|\). Then after \(K\) iterations of Sgda-Cc (Algorithm 5) it outputs \(\mathbf{x}^{T}\) such that_

\[\sum_{k=0}^{K-1}\mathbb{E}\big{\|}F(\mathbf{x}^{k})\big{\|}\leq\ell\sum_{k=0}^{K-1 }\mathbb{E}[\langle F(\mathbf{x}^{k}),\mathbf{x}^{k}-\mathbf{x}^{*}\rangle]\leq\frac{2 \ell R^{2}}{\gamma}.\]

Proof.: Since \(\left|\widehat{G}_{t}\setminus C_{t}\right|\geq n-2B-m\) one can derive using the results of Lemmas C.1 and E.1 that

\[\mathbb{E}\left[\|\mathbf{x}^{k+1}-\mathbf{x}^{*}\|^{2}\mid\mathbf{x}^{k}\right] = \mathbb{E}\left[\|\mathbf{x}^{k}-\mathbf{x}^{*}-\gamma\widehat{\mathbf{g}}^{k}\|^{ 2}\mid\mathbf{x}^{k}\right]\] \[= \|\mathbf{x}^{k}-\mathbf{x}^{*}\|^{2}-2\gamma\mathbb{E}\left[\langle \mathbf{x}^{k}-\mathbf{x}^{*},\widehat{\mathbf{g}}^{k}\rangle\mid\mathbf{x}^{k}\right]+\gamma^{2 }\mathbb{E}\left[\|\widehat{\mathbf{g}}^{k}\|^{2}\mid\mathbf{x}^{k}\right]\] \[\leq \|\mathbf{x}^{k}-\mathbf{x}^{*}\|^{2}-2\gamma\langle\mathbf{x}^{k}-\mathbf{x}^{*},F(\mathbf{x}^{k})\rangle+2\ell\gamma^{2}\langle\mathbf{x}^{k}-\mathbf{x}^{*},F(\mathbf{x}^{k})\rangle\] \[-2\gamma\mathbb{E}\left[\langle\mathbf{x}^{k}-\mathbf{x}^{*},\widehat{ \mathbf{g}}^{k}-\overline{\mathbf{g}}^{k}\rangle\mid\mathbf{x}^{k}\right]+2\gamma^{2}\rho^{ 2}\mathbb{1}_{k}+\frac{2\gamma^{2}\sigma^{2}}{n-2B-m},\]where \(1_{k}\) is an indicator function of the event that at least \(1\) Byzantine peer violates the protocol at iteration \(k\).

To estimate the inner product in the right-hand side we apply Cauchy-Schwarz inequality and then Lemma E.1

\[-2\gamma\mathbb{E}\left[\langle\mathbf{x}^{k}-\mathbf{x}^{*},\widehat{\mathbf{g }}^{k}-\overline{\mathbf{g}}^{k}\rangle\mid\mathbf{x}^{k}\right] \leq 2\gamma\|\mathbf{x}^{k}-\mathbf{x}^{*}\|\mathbb{E}\left[\|\widehat{\mathbf{g }}^{k}-\overline{\mathbf{g}}^{k}\|\mid\mathbf{x}^{k}\right]\] \[\leq 2\gamma\|\mathbf{x}^{k}-\mathbf{x}^{*}\|\sqrt{\mathbb{E}\left[\|\widehat {\mathbf{g}}^{k}-\overline{\mathbf{g}}^{k}\|^{2}\mid\mathbf{x}^{k}\right]}\] \[\leq 2\gamma\rho\|\mathbf{x}^{k}-\mathbf{x}^{*}\|\,\mathbb{1}_{k}.\]

Since \(\gamma\leq\frac{1}{2\ell}\) the above results imlies

\[\gamma\langle\mathbf{x}^{k}-\mathbf{x}^{*},F(\mathbf{x}^{k})\rangle \leq \|\mathbf{x}^{k}-\mathbf{x}^{*}\|^{2}-\mathbb{E}\left[\|\mathbf{x}^{k+1}-\bm {x}^{*}\|^{2}\mid\mathbf{x}^{k}\right]\] \[-2\gamma\mathbb{E}\left[\langle\mathbf{x}^{k}-\mathbf{x}^{*},\widehat{ \mathbf{g}}^{k}-\overline{\mathbf{g}}^{k}\rangle\mid\mathbf{x}^{k}\right]+2\gamma^{2}\rho^ {2}\mathbb{1}_{k}+\frac{2\gamma^{2}\sigma^{2}}{n-2B-m}.\] \[\leq \|\mathbf{x}^{k}-\mathbf{x}^{*}\|^{2}-\mathbb{E}\left[\|\mathbf{x}^{k+1}-\bm {x}^{*}\|^{2}\mid\mathbf{x}^{k}\right]\] \[+2\gamma^{2}\rho^{2}\mathbb{1}_{k}+\frac{2\gamma^{2}\sigma^{2}}{n -2B-m}+2\gamma\rho\|\mathbf{x}^{k}-\mathbf{x}^{*}\|\mathbb{1}_{k}.\]

Taking the full expectation from the both sides of the above inequality and summing up the results for \(k=0,1,\ldots,K-1\) we derive

\[\frac{\gamma}{K}\sum\limits_{k=0}^{K-1}\mathbb{E}[\langle F(\mathbf{x }^{k}),\mathbf{x}^{k}-\mathbf{x}^{*}\rangle]\] \[\leq \frac{1}{K}\sum\limits_{k=0}^{K-1}\left(\mathbb{E}\left[\|\mathbf{x} ^{k}-\mathbf{x}^{*}\|^{2}\right]-\mathbb{E}\left[\|\mathbf{x}^{k+1}-\mathbf{x}^{*}\|^{2} \right]\right)+\frac{2\gamma^{2}\sigma^{2}}{n-2B-m}\] \[+\frac{2\gamma\rho}{K}\sum\limits_{k=0}^{K-1}\mathbb{E}\left[\| \mathbf{x}^{k}-\mathbf{x}^{*}\|\mathbb{1}_{k}\right]+\frac{2\gamma^{2}\rho^{2}}{K} \sum\limits_{k=0}^{K-1}\mathbb{E}[\mathbb{1}_{k}]\] \[\leq \frac{\|\mathbf{x}^{0}-\mathbf{x}^{*}\|^{2}-\mathbb{E}[\|\mathbf{x}^{K}-\mathbf{ x}^{*}\|^{2}]}{K}+\frac{2\gamma^{2}\sigma^{2}}{n-2B-m}\] \[+\frac{2\gamma\rho}{K}\sum\limits_{k=0}^{K-1}\sqrt{\mathbb{E} \left[\|\mathbf{x}^{k}-\mathbf{x}^{*}\|^{2}\right]\mathbb{E}[\mathbb{1}_{k}]}+\frac{2 \gamma^{2}\rho^{2}}{K}\sum\limits_{k=0}^{K-1}\mathbb{E}[\mathbb{1}_{k}].\]

Since \(F\) satisfies Assumption 6, \(\sum\limits_{k=0}^{K-1}\mathbb{E}[\langle F(\mathbf{x}^{k}),\mathbf{x}^{k}-\mathbf{x}^{*} \rangle]\geq 0\). Using this and new notation \(R_{k}=\|\mathbf{x}^{k}-\mathbf{x}^{*}\|\), \(k>0\), \(R_{0}\geq\|\mathbf{x}^{0}-\mathbf{x}^{*}\|\) we get

\[0 \leq \frac{R_{0}^{2}-\mathbb{E}[R_{K}^{2}]}{K}+\frac{2\gamma^{2}\sigma ^{2}}{n-2B-m} \tag{45}\] \[+\frac{2\gamma\rho}{K}\sum\limits_{k=0}^{K-1}\sqrt{\mathbb{E} \left[R_{k}^{2}\right]\mathbb{E}[\mathbb{1}_{k}]}+\frac{2\gamma^{2}\rho^{2}}{K }\sum\limits_{k=0}^{K-1}\mathbb{E}[\mathbb{1}_{k}]\]

implying (after changing the indices) that

\[\mathbb{E}[R_{k}^{2}]\leq R_{0}^{2}+\frac{2\gamma^{2}\sigma^{2}k}{n-2B-m}+2 \gamma\rho\sum\limits_{l=0}^{k-1}\sqrt{\mathbb{E}\left[R_{l}^{2}\right] \mathbb{E}[\mathbb{1}_{l}]}+2\gamma^{2}\rho^{2}\sum\limits_{l=0}^{k-1} \mathbb{E}[\mathbb{1}_{l}] \tag{46}\]

holds for all \(k\geq 0\). In the remaining part of the proof we derive by induction that

\[R_{0}^{2}+\frac{2\gamma^{2}\sigma^{2}k}{n-2B-m}+2\gamma\rho\sum\limits_{l=0}^{ k-1}\sqrt{\mathbb{E}\left[R_{l}^{2}\right]\mathbb{E}[\mathbb{1}_{l}]}+2\gamma^{2} \rho^{2}\sum\limits_{l=0}^{k-1}\mathbb{E}[\mathbb{1}_{k}]\leq 2R_{0}^{2} \tag{47}\]for all \(k=0,\ldots,K\). For \(k=0\) this inequality trivially holds. Next, assume that it holds for all \(k=0,1,\ldots,T-1\), \(T\leq K-1\). Let us show that it holds for \(k=T\) as well. From (46) and (47) we have that \(\mathbb{E}[R_{k}^{2}]\leq 2\overline{R}_{0}^{2}\) for all \(k=0,1,\ldots,T-1\). Therefore,

\[\mathbb{E}[R_{T}^{2}] \leq R_{0}^{2}+\frac{2\gamma^{2}\sigma^{2}T}{n-2B-m}+2\gamma\rho\sum \limits_{l=0}^{T-1}\sqrt{\mathbb{E}\left[R_{l}^{2}\right]\mathbb{E}\left[ \mathbb{1}_{l}\right]}+2\gamma^{2}\rho^{2}\sum\limits_{l=0}^{T-1}\mathbb{E}[ \mathbb{1}_{l}]\] \[\leq R_{0}^{2}+\frac{2\gamma^{2}\sigma^{2}T}{n-2B-m}+2\sqrt{2}\gamma \rho R_{0}\sum\limits_{l=0}^{T-1}\sqrt{\mathbb{E}\left[\mathbb{1}_{l}\right]}+2 \gamma^{2}\rho^{2}\sum\limits_{l=0}^{T-1}\mathbb{E}[\mathbb{1}_{l}].\]

If a Byzantine peer deviates from the protocol at iteration \(k\), it will be detected with some probability \(p_{k}\) during the next iteration. One can lower bound this probability as

\[p_{k}\geq m\cdot\frac{G_{k}}{n_{k}}\cdot\frac{1}{n_{k}}=\frac{m(1-\delta_{k})} {n_{k}}\geq\frac{m}{n}.\]

Therefore, each individual Byzantine worker can violate the protocol no more than \(\nicefrac{{1}}{{p}}\) times on average implying that

\[\mathbb{E}[R_{T}^{2}] \leq R_{0}^{2}+\frac{2\gamma^{2}\sigma^{2}T}{n-2B-m}+\frac{2\sqrt{2} \gamma\rho R_{0}nB}{m}+\frac{2\gamma^{2}\rho^{2}nB}{m}\]

Taking

\[\gamma=\min\left\{\frac{1}{2\ell},\sqrt{\frac{(n-2B-m)R_{0}^{2}}{6\sigma^{2}K}},\sqrt{\frac{m^{2}R_{0}^{2}}{72\rho^{2}B^{2}n^{2}}}\right\}\]

we ensure that

\[\frac{2\gamma^{2}\sigma^{2}T}{n-2B-m}+\frac{2\sqrt{2}\gamma\rho R_{0}nB}{m}+ \frac{2\gamma^{2}\rho^{2}nB}{m}\leq\frac{R_{0}^{2}}{3}+\frac{R_{0}^{2}}{3}+ \frac{R_{0}^{2}}{3}=R_{0}^{2},\]

and, as a result, we get

\[\mathbb{E}[R_{T}^{2}]\leq 2R_{0}^{2}\equiv 2R \tag{48}\]

. Therefore, (47) holds for all \(k=0,1,\ldots,K\). Together with (45) it implies

\[\sum\limits_{k=0}^{K-1}\mathbb{E}[\langle F(\boldsymbol{x}^{k}),\boldsymbol{x} ^{k}-\boldsymbol{x}^{*}\rangle]\leq\frac{2R_{0}^{2}}{\gamma}.\]

The last inequality together with Assumption 6 implies

\[\sum\limits_{k=0}^{K-1}\mathbb{E}\big{\|}F(\boldsymbol{x}^{k})\big{\|}\leq\frac {2\ell R_{0}^{2}}{\gamma}.\]

**Corollary 5**.: _Let assumptions of Theorem 9 hold. Then \(\frac{1}{K}\sum\limits_{k=0}^{K-1}\mathbb{E}\big{\|}F(\boldsymbol{x}^{k}) \big{\|}\leq\varepsilon\) holds after_

\[K=\Theta\bigg{(}\frac{\ell^{2}R^{2}}{\varepsilon}+\frac{\sigma^{2}\ell^{2}R^{ 2}}{n\varepsilon^{2}}+\frac{\sigma n^{2}\ell R}{m\varepsilon}\bigg{)}\]

_iterations of_ SGDA-CC_._

Proof.: \[\frac{1}{K}\sum\limits_{k=0}^{K-1}\mathbb{E}\big{\|}F(\boldsymbol {x}^{k})\big{\|}\leq\frac{2\ell R^{2}}{\gamma K} \leq \frac{2\ell R^{2}}{K}\Bigg{(}2\ell+\sqrt{\frac{6\sigma^{2}K}{(n-2B -m)R^{2}}}+\sqrt{\frac{72\rho^{2}B^{2}n^{2}}{m^{2}R^{2}}}\Bigg{)}\] \[\leq \frac{4\ell^{2}R^{2}}{K}+\sqrt{\frac{24\sigma^{2}\ell^{2}R^{2}}{( n-2B-m)K}}+\frac{17\rho Bn\ell R}{mK}\]Let us chose \(K\) such that each of the last three terms less or equal \(\varepsilon/3\), then

\[K=\max\biggl{(}\frac{6\ell^{2}R^{2}}{\varepsilon},\frac{216\sigma^{2}\ell^{2}R^{2 }}{(n-2B-m)\varepsilon^{2}},\frac{51\rho Bn\ell R}{m\varepsilon}\biggr{)}\]

where \(\rho^{2}=q\sigma^{2}\) with \(q=2C^{2}+12+\frac{12}{n-2B-m}\) and \(C=\Theta(1)\) by Lemma E.1. The latter implies that

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\bigl{\|}F(\mathbf{x}^{k})\bigr{\|}\leq\varepsilon.\]

Using the definition of \(\rho\) from Lemma E.1 and if \(B\leq\frac{n}{4}\), \(m<<n\) the bound for \(K\) can be easily derived. 

#### e.1.2 Quasi-Strongly Monotone Case

**Theorem** (Theorem 4 duplicate).: _Let Assumptions 1, 4 and 6 hold. Then after \(T\) iterations_ SGDA-CC _(Algorithm 5) with \(\gamma\leq\frac{1}{2t}\) outputs \(\mathbf{x}^{T}\) such that_

\[\mathbb{E}\bigl{\|}\mathbf{x}^{T+1}-\mathbf{x}^{*}\bigr{\|}^{2}\leq\Bigl{(}1-\frac{ \gamma\mu}{2}\Bigr{)}^{T+1}\bigl{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\bigr{\|}^{2}+\frac{ 4\gamma\sigma^{2}}{\mu(n-2B-m)}+\frac{2\rho^{2}nB}{m}\biggl{(}\frac{\gamma}{ \mu}+\gamma^{2}\biggr{)}.\]

_where \(\rho^{2}=q\sigma^{2}\) with \(q=2C^{2}+12+\frac{12}{n-2B-m}\) and \(C=\Theta(1)\) by Lemma E.1._

Proof of Theorem 4.: The proof is similar to the proof of Theorem 1

\[\mu\mathbb{E}\Bigl{[}\bigl{\|}\mathbf{\overline{x}}^{K}-\mathbf{x}^{*} \bigr{\|}^{2}\Bigr{]} =\] \[= \frac{\mu}{K}\sum_{k=0}^{K-1}\mathbb{E}\bigl{[}\bigl{\|}\mathbf{x}^{k }-\mathbf{x}^{*}\bigr{\|}^{2}\bigr{]}\stackrel{{\text{\tiny{(\ref{eq: 100})}}}}{{\leq}}\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\bigl{[}\langle F(\mathbf{x} ^{k}),\mathbf{x}^{k}-\mathbf{x}^{*}\rangle\bigr{]}\]

Since \(\widehat{\mathbf{g}}^{t}=\widehat{\mathbf{g}}^{t}-F^{t}+F^{t}\) one has

Applying (11) for \(\langle\overline{\mathbf{g}}^{t},\mathbf{x}^{t}-\mathbf{x}^{*}\rangle\) with \(\lambda=\frac{\gamma\mu}{2}\) and (12) for \(\bigl{\|}\widehat{\mathbf{g}}^{t}\bigr{\|}^{2}=\bigl{\|}\widehat{\mathbf{g}}^{t}- \overline{\mathbf{g}}^{t}+\overline{\mathbf{g}}^{t}\bigr{\|}^{2}\) we derive

\[\bigl{\|}\mathbf{x}^{t+1}-\mathbf{x}^{*}\bigr{\|}^{2} \leq\] \[+\frac{2\gamma}{\mu}\bigl{\|}\widehat{\mathbf{g}}^{t}-\overline{\mathbf{ g}}^{t}\bigr{\|}^{2}+2\gamma^{2}\bigl{\|}\widehat{\mathbf{g}}^{t}-\overline{\mathbf{g}}^{t} \bigr{\|}^{2}+2\gamma^{2}\bigl{\|}\overline{\mathbf{g}}^{t}\bigr{\|}^{2}.\]

Next by taking an expectation \(\mathbb{E}_{\mathbf{\xi}}\) of both sides of the above inequality and rearranging terms obtain

\[\mathbb{E}_{\mathbf{\xi}}\bigl{\|}\mathbf{x}^{t+1}-\mathbf{x}^{*}\bigr{\|}^{2} \leq\] \[+\frac{2\gamma}{\mu}\mathbb{E}_{\mathbf{\xi}}\bigl{\|}\widehat{\mathbf{g} }^{t}-\overline{\mathbf{g}}^{t}\bigr{\|}^{2}+2\gamma^{2}\mathbb{E}_{\mathbf{\xi}} \bigl{\|}\widehat{\mathbf{g}}^{t}-\overline{\mathbf{g}}^{t}\bigr{\|}^{2}+2\gamma^{2} \mathbb{E}_{\mathbf{\xi}}\bigl{\|}\overline{\mathbf{g}}^{t}\bigr{\|}^{2}.\]

The difference with the proof of Theorem 1 is that we suppose that the number of peer violating the protocol at an iteration \(t\) is known to any "good" peer. So the result of Lemma E.1 writes as follows

\[\mathbb{E}_{\mathbf{\xi}}\bigl{\|}\widehat{\mathbf{g}}^{t}-\overline{\mathbf{g}}^{t} \bigr{\|}^{2}\leq\rho^{2}\mathbb{I}_{t},\]

where \(\mathbb{I}_{t}\) is an indicator function of the event that at least \(1\) Byzantine peer violates the protocol at iteration \(t\).

Together with Lemma C.1 we can proceed as follows m

\[\mathbb{E}_{\mathbf{\xi}}\bigl{\|}\mathbf{x}^{t+1}-\mathbf{x}^{*}\bigr{\|}^{2} \leq \Bigl{(}1+\frac{\gamma\mu}{2}\Bigr{)}\bigl{\|}\mathbf{x}^{t}-\mathbf{x}^ {*}\bigr{\|}^{2}+\bigl{(}2\gamma^{2}\ell-2\gamma\bigr{)}\langle F(\mathbf{x}^{t}), \mathbf{x}^{t}-\mathbf{x}^{*}\rangle\] \[+\frac{2\gamma^{2}\sigma^{2}}{G}+2\mathbb{I}_{t}\rho^{2}\biggl{(} \frac{\gamma}{\mu}+\gamma^{2}\biggr{)},\]Since \(\gamma\leq\frac{1}{2\ell}\) and Assumption (QSM) holds we derive

\[\mathbb{E}_{\xi}\big{\|}\mathbf{x}^{t+1}-\mathbf{x}^{*}\big{\|}^{2} \leq \Big{(}1-\frac{\gamma\mu}{2}\Big{)}\big{\|}\mathbf{x}^{t}-\mathbf{x}^{*} \big{\|}^{2}+\frac{2\gamma^{2}\sigma^{2}}{G}+2\mathbb{1}_{t}\rho^{2}\bigg{(} \frac{\gamma}{\mu}+\gamma^{2}\bigg{)}.\]

Next we take full expectation of both sides and obtain

\[\mathbb{E}\big{\|}\mathbf{x}^{t+1}-\mathbf{x}^{*}\big{\|}^{2}\leq\Big{(}1-\frac{\gamma \mu}{2}\Big{)}\mathbb{E}\big{\|}\mathbf{x}^{t}-\mathbf{x}^{*}\big{\|}^{2}+\frac{2\gamma ^{2}\sigma^{2}}{n-2B-m}+2\rho^{2}\bigg{(}\frac{\gamma}{\mu}+\gamma^{2}\bigg{)} \mathbb{E}\mathbb{1}_{t}.\]

The latter implies

\[\mathbb{E}\big{\|}\mathbf{x}^{T+1}-\mathbf{x}^{*}\big{\|}^{2}\leq\Big{(}1- \frac{\gamma\mu}{2}\Big{)}^{T+1}\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2}+ \frac{4\gamma\sigma^{2}}{\mu(n-2B-m)}\\ +2\rho^{2}\bigg{(}\frac{\gamma}{\mu}+\gamma^{2}\bigg{)}\sum_{i}^ {T}\mathbb{E}\mathbb{1}_{i}\Big{(}1-\frac{\gamma\mu}{2}\Big{)}^{T-i}.\]

If a Byzantine peer deviates from the protocol at iteration \(t\), it will be detected with some probability \(p_{t}\) during the next iteration. One can lower bound this probability as

\[p_{t}\geq m\cdot\frac{G_{t}}{n_{t}}\cdot\frac{1}{n_{t}}=\frac{m(1-\delta_{t})} {n_{t}}\geq\frac{m}{n}.\]

Therefore, each individual Byzantine worker can violate the protocol no more than \(\nicefrac{{1}}{{p}}\) times on average implying that

\[\mathbb{E}\left[\sum_{t=0}^{\infty}\mathbb{1}_{t}\right]\leq\frac{nB}{m}\]

that implies

\[\mathbb{E}\bigg{[}\sum_{i}^{T}\mathbb{1}_{i}\Big{(}1-\frac{\gamma\mu}{2} \Big{)}^{T-i}\bigg{]}\leq\mathbb{E}\bigg{[}\sum_{i}^{T}\mathbb{1}_{i}\bigg{]} \leq\frac{nB}{m}. \tag{49}\]

The latter together with the above bound on \(\mathbb{E}\big{\|}\mathbf{x}^{T+1}-\mathbf{x}^{*}\big{\|}^{2}\) implies the result of the theorem.

\[\mathbb{E}\big{\|}\mathbf{x}^{T+1}-\mathbf{x}^{*}\big{\|}^{2}\leq\Big{(}1-\frac{ \gamma\mu}{2}\Big{)}^{T+1}\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2}+\frac{4 \gamma\sigma^{2}}{\mu(n-2B-m)}+\frac{2\rho^{2}nB}{m}\bigg{(}\frac{\gamma}{\mu}+ \gamma^{2}\bigg{)}.\]

**Corollary 6**.: _Let assumptions of Theorem 4 hold. Then \(\mathbb{E}\big{\|}\mathbf{x}^{T}-\mathbf{x}^{*}\big{\|}^{2}\leq\varepsilon\) holds after_

\[T=\widetilde{\Theta}\bigg{(}\frac{\ell}{\mu}+\frac{\sigma^{2}}{\mu^{2}(n-2B-m )\varepsilon}+\frac{q\sigma^{2}Bn}{\mu^{2}m\varepsilon}+\frac{q\sigma^{2}Bn}{ \mu^{2}m\sqrt{\varepsilon}}\bigg{)}\]

_iterations of_ SGDA-CC _(Alg. 5) with_

\[\gamma=\min\left\{\frac{1}{2\ell},\frac{2\ln\Big{(}\max\Bigl{\{}2,\min\Bigl{\{} \frac{m(n-2B-m)\mu^{2}R^{2}K}{8mq^{2}+4q\sigma^{2}nB(n-2B-m)},\frac{mu^{2}R^{2 }K^{2}}{8qnB\sigma^{2}}\Bigr{\}}\Bigr{\}}\Bigr{)}}{\mu(K+1)}\right\}.\]

Proof.: Using the definition of \(\rho\) (\(\rho^{2}=q\sigma^{2}=\Theta(\sigma^{2})\)) from Lemma E.1 and if \(B\leq\frac{n}{4}\), \(m<<n\) the result of Theorem 4 can be simplified as

\[\mathbb{E}\big{\|}\mathbf{x}^{T+1}-\mathbf{x}^{*}\big{\|}^{2}\leq\Big{(}1-\frac{ \gamma\mu}{2}\Big{)}^{T+1}\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2}+\frac{4 \gamma\sigma^{2}}{\mu(n-2B-m)}+\frac{2q\sigma^{2}nB}{m}\bigg{(}\frac{\gamma}{ \mu}+\gamma^{2}\bigg{)}.\]

Applying Lemma C.4 to the last bound we get the result of the corollary.

#### e.1.3 Monotone Case

**Theorem 10**.: _Suppose the assumptions of Theorem 9 and Assumption 5 hold. Next, assume that_

\[\gamma=\min\left\{\frac{1}{2\ell},\sqrt{\frac{(n-2B-m)R^{2}}{6\sigma^{2}K}},\sqrt{ \frac{m^{2}R^{2}}{72\rho^{2}B^{2}n^{2}}}\right\} \tag{50}\]

_where \(\rho^{2}=q\sigma^{2}\) with \(q=2C^{2}+12+\frac{12}{n-2B-m}\) and \(C=\Theta(1)\) by Lemma E.1 and \(R\geq\|\mathbf{x}^{0}-\mathbf{x}^{*}\|\). Then after \(K\) iterations of_ SGDA-CC _(Algorithm 5)_

\[\mathbb{E}\Big{[}\texttt{gap}_{B_{R}(x^{*})}\big{(}\overline{\mathbf{x}}^{K}\big{)} \Big{]}\leq\frac{3R^{2}}{\gamma K}, \tag{51}\]

_where \(\texttt{gap}_{B_{R}(x^{*})}\big{(}\overline{\mathbf{x}}^{K}\big{)}=\max\limits_{u \in B_{R}(x^{*})}\langle F(\mathbf{u}),\overline{\mathbf{x}}^{K}-\mathbf{u}\rangle\), \(\overline{\mathbf{x}}^{K}=\frac{1}{K}\sum\limits_{k=0}^{K-1}\mathbf{x}^{k}\) and \(R\geq\|\mathbf{x}^{0}-\mathbf{x}^{*}\|\)._

Proof.: Combining (16), (14) one can derive

\[2\gamma\big{\langle}F(\mathbf{x}^{k}),\mathbf{x}^{k}-\mathbf{u}\big{\rangle} \leq \big{\|}\mathbf{x}^{k}-\mathbf{u}\big{\|}^{2}-\big{\|}\mathbf{x}^{k+1}-\mathbf{u }\big{\|}^{2}\] \[+2\gamma^{2}\big{\|}\widehat{\mathbf{g}}^{k}-\overline{\mathbf{g}}^{k} \big{\|}^{2}+2\gamma^{2}\big{\|}\overline{\mathbf{g}}^{k}\big{\|}^{2}.\]

Assumption 5 implies that

\[\big{\langle}F(\mathbf{u}),\mathbf{x}^{k}-\mathbf{u}\big{\rangle}\leq\big{\langle}F(\mathbf{x} ^{k}),\mathbf{x}^{k}-\mathbf{u}\big{\rangle} \tag{52}\]

and consequently by Jensen inequality

\[2\gamma K\big{\langle}F(\mathbf{u}),\overline{\mathbf{x}}^{K}-\mathbf{u} \big{\rangle} \leq \big{\|}\mathbf{x}^{0}-\mathbf{u}\big{\|}^{2}-2\gamma\sum\limits_{k=0}^{ K-1}\langle\widehat{\mathbf{g}}^{k}-\overline{\mathbf{g}}^{k},\mathbf{x}^{k}-\mathbf{u}\rangle\] \[-2\gamma\sum\limits_{k=0}^{K-1}\langle\overline{\mathbf{g}}^{k}-F^{k },\mathbf{x}^{k}-\mathbf{u}\rangle+2\gamma^{2}\sum\limits_{k=0}^{K-1}\Big{(}\big{\|} \widehat{\mathbf{g}}^{k}-\overline{\mathbf{g}}^{k}\big{\|}^{2}+\big{\|}\overline{\mathbf{ g}}^{k}\big{\|}^{2}\Big{)}.\]

Then maximization in \(\mathbf{u}\) gives

\[2\gamma K\texttt{gap}_{B_{R}(x^{*})}\big{(}\overline{\mathbf{x}}^{K} \big{)} \leq \max\limits_{u\in B_{R}(x^{*})}\big{\|}\mathbf{x}^{0}-\mathbf{u}\big{\|} ^{2}+2\gamma^{2}\sum\limits_{k=0}^{K-1}\Big{(}\big{\|}\widehat{\mathbf{g}}^{k}- \overline{\mathbf{g}}^{k}\big{\|}^{2}+\big{\|}\overline{\mathbf{g}}^{k}\big{\|}^{2} \Big{)}\] \[+2\gamma\max\limits_{u\in B_{R}(x^{*})}\left(\sum\limits_{k=0}^{ K-1}\langle\overline{\mathbf{g}}^{k}-\widehat{\mathbf{g}}^{k},\mathbf{x}^{k}-\mathbf{u}\rangle\right)\] \[+2\gamma\max\limits_{u\in B_{R}(x^{*})}\left(\sum\limits_{k=0}^{ K-1}\langle F^{k}-\overline{\mathbf{g}}^{k},\mathbf{x}^{k}-\mathbf{u}\rangle\right).\]

Taking the full expectation from the both sides of the previous inequality gives

\[2\gamma K\mathbb{E}\Big{[}\texttt{gap}_{B_{R}(x^{*})}\big{(} \overline{\mathbf{x}}^{K}\big{)}\Big{]} \leq \max\limits_{u\in B_{R}(x^{*})}\big{\|}\mathbf{x}^{0}-\mathbf{u}\big{\|} ^{2}\] \[+2\gamma\mathbb{E}\Bigg{[}\max\limits_{u\in B_{R}(x^{*})}\left( \sum\limits_{k=0}^{K-1}\langle F^{k}-\overline{\mathbf{g}}^{k},\mathbf{x}^{k}-\mathbf{u} \rangle\right)\Bigg{]}.\] \[+2\gamma^{2}\mathbb{E}\Bigg{[}\sum\limits_{k=0}^{K-1}\Big{(}\big{ \|}\widehat{\mathbf{g}}^{k}-\overline{\mathbf{g}}^{k}\big{\|}^{2}+\big{\|}\overline{ \mathbf{g}}^{k}\big{\|}^{2}\Big{)}\Bigg{]}\]Firstly obtain the bound for the terms that do not depend on \(\mathbf{u}\) using Assumption 1, Lemma E.1 and Theorem 9

\[2\gamma^{2}\mathbb{E}\Bigg{[}\sum_{k=0}^{K-1}\left(\left\|\mathbf{\widehat {g}}^{k}-\mathbf{\overline{g}}^{k}\right\|^{2}+\left\|\mathbf{\overline{g}}^{k}\right\| ^{2}\right)\Bigg{]}\] \[\leq 2\gamma^{2}\mathbb{E}\Bigg{[}\sum_{k=0}^{K-1}\left\|\mathbf{\widehat {g}}^{k}-\mathbf{\overline{g}}^{k}\right\|^{2}\Bigg{]}+2\gamma^{2}\mathbb{E}\Bigg{[} \sum_{k=0}^{K-1}\left\|\mathbf{\overline{g}}^{k}\right\|^{2}\Bigg{]}\] \[\leq 2\gamma^{2}\rho^{2}\mathbb{E}\Bigg{[}\sum_{k=0}^{K-1}1_{k}\Bigg{]} +\frac{2\gamma^{2}K\sigma^{2}}{|G_{t}\setminus C_{t}|}+2\gamma^{2}\ell\sum_{k =0}^{K-1}\mathbb{E}\left\langle F^{k},\mathbf{x}^{k}-\mathbf{x}^{*}\right\rangle\] \[\leq \frac{2\gamma^{2}nBc\rho^{2}}{m}+\frac{2\gamma^{2}K\sigma^{2}}{| G_{t}\setminus C_{t}|}+4\ell\gamma R^{2}.\]

Since \(\mathbb{E}\big{[}\big{\|}\mathbf{x}^{k}-\mathbf{u}\big{\|}\big{]}\leq\mathbb{E}\big{[} \big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}\big{]}+\|\mathbf{x}^{*}-\mathbf{u}\|\leq\mathbb{E }\big{[}\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}\big{]}+\max\limits_{u\in B_{R}(x ^{*})}\|\mathbf{x}^{*}-\mathbf{u}\|\stackrel{{\eqref{eq:2}}}{{\leq}}3R\) one can derive that

\[2\gamma\mathbb{E}\Bigg{[}\max\limits_{u\in B_{R}(x^{*})}\Bigg{(} \sum_{k=0}^{K-1}\langle\mathbf{\overline{g}}^{k}-\mathbf{\widehat{g}}^{k},\mathbf{x}^{k}- \mathbf{u}\rangle\Bigg{)}\Bigg{]}\] \[\leq 2\gamma\mathbb{E}\Bigg{[}\max\limits_{u\in B_{R}(x^{*})}\Bigg{(} \sum_{k=0}^{K-1}\langle\mathbf{\overline{g}}^{k}-\mathbf{\widehat{g}}^{k},\mathbf{x}^{*}- \mathbf{u}\rangle\Bigg{)}+\sum_{k=0}^{K-1}\langle\mathbf{\overline{g}}^{k}-\mathbf{\widehat {g}}^{k},\mathbf{x}^{k}-\mathbf{x}^{*}\rangle\Bigg{]}\] \[\leq 2\gamma\sum_{k=0}^{K-1}\mathbb{E}\Big{[}\max\limits_{u\in B_{R}( x^{*})}\langle\mathbf{\overline{g}}^{k}-\mathbf{\widehat{g}}^{k}\big{\|}\big{\|}\mathbf{x}^{*}- \mathbf{u}\big{\|}\Big{]}+2\gamma\mathbb{E}\Bigg{[}\mathbb{E}\Bigg{[}\sum_{k=0}^{K -1}\big{\|}\mathbf{\overline{g}}^{k}-\mathbf{\widehat{g}}^{k}\big{\|}\big{\|}\mathbf{x}^{k} -\mathbf{x}^{*}\big{\|}\big{\|}\mathbf{x}^{k}\Bigg{]}\Bigg{]}\] \[\leq 2\gamma\sum_{k=0}^{K-1}\mathbb{E}\big{[}R\big{\|}\mathbf{\overline{ g}}^{k}-\mathbf{\widehat{g}}^{k}\big{\|}\big{]}+2\gamma\mathbb{E}\Bigg{[}\sum_{k=0}^{K-1} \rho\mathbb{1}_{k}\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}\Bigg{]}\] \[\leq 2\gamma\rho R\mathbb{E}\Bigg{[}\sum_{k=0}^{K-1}\mathbb{1}_{k} \Bigg{]}+4\gamma\rho R\mathbb{E}\Bigg{[}\sum_{k=0}^{K-1}\mathbb{1}_{k}\Bigg{]} \leq 6\gamma R\rho\mathbb{E}\Bigg{[}\sum_{k=0}^{K-1}\mathbb{1}_{k}\Bigg{]} \leq\frac{6nB\gamma R\rho}{m}\]

Following Beznosikov et al. [2023] one can derive he bound for the next term:

\[\mathbb{E}\left[\sum_{k=0}^{K-1}\langle F^{k}-\mathbf{\overline{g}}^{ k},\mathbf{x}^{k}\rangle\right] = \mathbb{E}\left[\sum_{k=0}^{K-1}\langle\mathbb{E}[F^{k}-\mathbf{ \overline{g}}^{k}\mid x^{k}],\mathbf{x}^{k}\rangle\right]=0,\] \[\mathbb{E}\left[\sum_{k=0}^{K-1}\langle F^{k}-\mathbf{\overline{g}}^{ k},\mathbf{x}^{0}\rangle\right] = \sum_{k=0}^{K-1}\langle\mathbb{E}[F^{k}-\mathbf{\overline{g}}^{k}], \mathbf{x}^{0}\rangle=0,\]we have

\[2\gamma\mathbb{E}\left[\max_{\mathbf{u}\in B_{R}(x^{*})}\sum_{k=0}^{K-1} \langle F^{k}-\overline{\mathbf{g}}^{k},\mathbf{x}^{k}-\mathbf{u}\rangle\right]\] \[= 2\gamma\mathbb{E}\left[\sum_{k=0}^{K-1}\langle F^{k}-\overline{ \mathbf{g}}^{k},\mathbf{x}^{k}\rangle\right]+2\gamma\mathbb{E}\left[\max_{\mathbf{u}\in B_{ R}(x^{*})}\sum_{k=0}^{K-1}\langle F^{k}-\overline{\mathbf{g}}^{k},-\mathbf{u}\rangle\right]\] \[= 2\gamma\mathbb{E}\left[\max_{\mathbf{u}\in B_{R}(x^{*})}\sum_{k=0}^{ K-1}\langle F^{k}-\overline{\mathbf{g}}^{k},-\mathbf{u}\rangle\right]\] \[= 2\gamma\mathbb{E}\left[\sum_{k=0}^{K-1}\langle F^{k}-\overline{ \mathbf{g}}^{k},\mathbf{x}^{0}\rangle\right]\] \[+2\gamma\mathbb{E}\left[\max_{\mathbf{u}\in B_{R}(x^{*})}\sum_{k=0}^{ K-1}\langle F^{k}-\overline{\mathbf{g}}^{k},-\mathbf{u}\rangle\right]\] \[= 2\gamma K\mathbb{E}\left[\max_{\mathbf{u}\in B_{R}(x^{*})}\left\langle \frac{1}{K}\sum_{k=0}^{K-1}(F^{k}-\overline{\mathbf{g}}^{k}),\mathbf{x}^{0}-\mathbf{u} \right\rangle\right]\] \[\stackrel{{\eqref{eq:2}}}{{\leq}} 2\gamma K\mathbb{E}\left[\max_{\mathbf{u}\in B_{R}(x^{*})}\left\{ \frac{\gamma}{2}\left\|\frac{1}{K}\sum_{k=0}^{K-1}(F^{k}-\overline{\mathbf{g}}^{k}) \right\|^{2}+\frac{1}{2\gamma}\|\mathbf{x}^{0}-\mathbf{u}\|^{2}\right\}\right]\] \[= \gamma^{2}\mathbb{E}\left[\left\|\sum_{k=0}^{K-1}(F^{k}-\overline {\mathbf{g}}^{k})\right\|^{2}\right]+\max_{\mathbf{u}\in B_{R}(x^{*})}\|\mathbf{x}^{0}-\mathbf{ u}\|^{2}.\]

We notice that \(\mathbb{E}[F^{k}-\overline{\mathbf{g}}^{k}\mid F^{0}-\overline{\mathbf{g}}^{0},\ldots, F^{k-1}-\overline{\mathbf{g}}^{k-1}]=0\) for all \(k\geq 1\), i.e., conditions of Lemma C.2 are satisfied. Therefore, applying Lemma C.2, we get

\[2\gamma\mathbb{E}\left[\max_{\mathbf{u}\in B_{R}(x^{*})}\sum_{k=0}^{ K-1}\langle F^{k}-\overline{\mathbf{g}}^{k},\mathbf{x}^{k}-\mathbf{u}\rangle\right] \leq \gamma^{2}\sum_{k=0}^{K-1}\mathbb{E}[\|F^{k}-\overline{\mathbf{g}}^{ k}\|^{2}] \tag{53}\] \[+\max_{\mathbf{u}\in B_{R}(x^{*})}\|\mathbf{x}^{0}-\mathbf{u}\|^{2}\] \[\leq \frac{\gamma^{2}K\sigma^{2}}{|\overline{G}_{t}\setminus C_{t}|}+ \max_{\mathbf{u}\in B_{R}(x^{*})}\|\mathbf{x}^{0}-\mathbf{u}\|^{2}. \tag{54}\]

Assembling the above results together gives

\[2\gamma K\mathbb{E}\Big{[}\mathsf{Gap}_{B_{R}(x^{*})}\big{(} \overline{\mathbf{x}}^{K}\big{)}\Big{]}\] \[\leq 2\max_{u\in B_{R}(x^{*})}\big{\|}\mathbf{x}^{0}-\mathbf{u}\big{\|}^{2}+ \frac{2\gamma^{2}nB\rho^{2}}{m}+\frac{3\gamma^{2}K\sigma^{2}}{|\overline{G}_{t }\setminus C_{t}|}+4\ell\gamma R^{2}+\frac{6nB\gamma R\rho}{m}\] \[\leq 2\max_{u\in B_{R}(x^{*})}\big{\|}\mathbf{x}^{0}-\mathbf{u}\big{\|}^{2}+ \frac{2\gamma^{2}nB\rho^{2}}{m}+\frac{3\gamma^{2}K\sigma^{2}}{n-2B-m}\] \[+4\ell\gamma R^{2}+\frac{6nB\gamma R\rho}{m}\stackrel{{ \eqref{eq:2}}}{{\leq}}6R^{2}.\]

**Corollary 7**.: _Let assumptions of Theorem 10 hold. Then \(\mathbb{E}\Big{[}\,\mathcal{Gap}_{B_{R}(x^{*})}\big{(}\overline{\mathbf{x}}^{K} \big{)}\Big{]}\leq\varepsilon\) holds after_

\[K=\Theta\bigg{(}\frac{\ell R^{2}}{\varepsilon}+\frac{\sigma^{2}R^{2}}{n \varepsilon^{2}}+\frac{\sigma n^{2}R}{m\varepsilon}\bigg{)}\]

_iterations of_ SGDA-CC_._Proof.: \[\mathbb{E}\Big{[}\mathsf{Gap}_{B_{R}(x^{*})}\big{(}\overline{ \boldsymbol{x}}^{K}\big{)}\Big{]}\leq\frac{3R^{2}}{\gamma K} \leq \frac{3R^{2}}{K}\Bigg{(}2\ell+\sqrt{\frac{6\sigma^{2}K}{(n-2B-m)R^ {2}}}+\sqrt{\frac{72\rho^{2}B^{2}n^{2}}{m^{2}R^{2}}}\Bigg{)}\] \[\leq \frac{6\ell R^{2}}{K}+\sqrt{\frac{54\sigma^{2}R^{2}}{(n-2B-m)K}}+ \frac{26\rho BnR}{mK}\]

Let us chose \(K\) such that each of the last three terms less or equal \(\nicefrac{{\varepsilon}}{{3}}\), then

\[K=\max\!\left(\frac{18\ell R^{2}}{\varepsilon},\frac{9\cdot 54\sigma^{2}R^{2}}{( n-2B-m)\varepsilon^{2}},\frac{78\rho BnR}{m\varepsilon}\right)\]

guarantees that

\[\mathbb{E}\Big{[}\mathsf{Gap}_{B_{R}(x^{*})}\big{(}\overline{\boldsymbol{x}}^{K }\big{)}\Big{]}\leq\varepsilon.\]

Using the definition of \(\rho\) from Lemma E.1 and if \(B\leq\frac{n}{4}\), \(m<<n\) the bound for \(K\) can be easily derived. 

### Proofs for \(\mathsf{R}\)-Sgda-Cc

#### e.2.1 Quasi-Strongly Monotone Case

```
0:\(\boldsymbol{x}^{0}\) - starting point, \(r\) - number of restarts, \(\{\gamma_{t}\}_{t=1}^{r}\) - stepsizes for Sgda-Cc (Alg. 5), \(\{K_{t}\}_{t=1}^{r}\) - number of iterations for Sgda-Cc (Alg. 5)
1:\(\widehat{\boldsymbol{x}}^{0}=\boldsymbol{x}^{0}\)
2:for\(t=1,2,\ldots,r\)do
3: Run Sgda-Cc (Alg. 5) for \(K_{t}\) iterations with stepsize \(\gamma_{t}\), starting point \(\widehat{\boldsymbol{x}}^{t-1}\).
4: Define \(\widehat{\boldsymbol{x}}^{t}\) as \(\widehat{\boldsymbol{x}}^{t}=\frac{1}{K_{t}}\sum\limits_{k=0}^{K_{t}} \boldsymbol{x}^{k,t}\), where \(\boldsymbol{x}^{0,t},\boldsymbol{x}^{1,t},\ldots,\boldsymbol{x}^{K_{t},t}\) are the iterates produced by Sgda-Cc (Alg. 5).
5:endfor
5:\(\widehat{\boldsymbol{x}}^{r}\)
```

**Algorithm 6**\(\mathsf{R}\)-Sgda-Cc

**Theorem** (Theorem 5 duplicate).: _Let Assumptions 1, 4 and 6 hold. Then, after \(r=\left\lceil\log_{2}\frac{R^{2}}{\varepsilon}\right\rceil-1\) restarts \(\mathsf{R}\)-Sgda-Cc (Algorithm 6) with \(\gamma_{t}=\min\left\{\frac{1}{2t},\sqrt{\frac{(n-2B-m)R^{2}}{6\sigma^{2}2^{K _{t}}}},\sqrt{\frac{m^{2}R^{2}}{72q\sigma^{2}2^{2}R^{2}n^{2}}}\right\}\) and \(K_{t}=\left\lceil\max\left\{\frac{8\ell}{\mu},\frac{96\sigma^{2}2^{t}}{(n-2B- m)\mu^{2}R^{2}},\frac{34\pi\rho B\sqrt{q^{2}t}}{m\mu R}\right\}\right\rceil\), where \(R\geq\|\boldsymbol{x}^{0}-\boldsymbol{x}^{*}\|\), outputs \(\widehat{\boldsymbol{x}}^{r}\) such that \(\mathbb{E}\|\widehat{\boldsymbol{x}}^{r}-\boldsymbol{x}^{*}\|^{2}\leq\varepsilon\). Moreover, the total number of executed iterations of Sgda-Cc is_

\[\sum\limits_{t=1}^{r}K_{t}=\Theta\left(\frac{\ell}{\mu}\log\frac{\mu R_{0}^{2} }{\varepsilon}+\frac{\sigma^{2}}{(n-2B-m)\mu\varepsilon}+\frac{nB\sigma}{m \sqrt{\mu\varepsilon}}\right). \tag{55}\]

_With \(q=2C^{2}+12+\frac{12}{n-2B-m}\) and \(C=\Theta(1)\) by Lemma E.1._

Proof of Theorem 5.:\(\overline{\boldsymbol{x}}^{K}=\frac{1}{K}\sum_{k=0}^{K-1}\boldsymbol{x}^{k}\)

\[\mu\mathbb{E}\Big{[}\big{\|}\overline{\boldsymbol{x}}^{K}- \boldsymbol{x}^{*}\big{\|}^{2}\Big{]} = \mu\mathbb{E}\Bigg{[}\Bigg{\|}\frac{1}{K}\sum_{k=0}^{K-1}\! \big{(}\boldsymbol{x}^{k}-\boldsymbol{x}^{*}\big{)}\Bigg{\|}^{2}\Bigg{]}\leq \mu\mathbb{E}\Bigg{[}\frac{1}{K}\sum_{k=0}^{K-1}\!\big{\|}\boldsymbol{x}^{k}- \boldsymbol{x}^{*}\big{\|}^{2}\Bigg{]}\] \[= \frac{\mu}{K}\sum_{k=0}^{K-1}\mathbb{E}\Big{[}\big{\|}\boldsymbol {x}^{k}-\boldsymbol{x}^{*}\big{\|}^{2}\Big{]}\stackrel{{\text{( \ref{eq:R})}}}{{\leq}}\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\big{[}\langle F( \boldsymbol{x}^{k}),\boldsymbol{x}^{k}-\boldsymbol{x}^{*}\rangle\big{]}\]Theorem 9 implies that SGDA-CC with

\[\gamma=\min\left\{\frac{1}{2\ell},\sqrt{\frac{(n-2B-m)R_{0}^{2}}{6\sigma^{2}K}}, \sqrt{\frac{m^{2}R_{0}^{2}}{72\rho^{2}B^{2}n^{2}}}\right\}\]

guarantees

\[\mu\mathbb{E}\Big{[}\big{\|}\mathbf{\overline{x}}^{K}-\mathbf{x}^{*}\big{\|}^{2}\Big{]} \leq\frac{2}{R_{0}^{2}}\gamma K\]

after \(K\) iterations.

After the first restart we have

\[\mathbb{E}\Big{[}\big{\|}\mathbf{\widehat{x}}^{1}-\mathbf{x}^{*}\big{\|}^{2}\Big{]}\leq \frac{2R_{0}^{2}}{\mu\gamma_{1}K_{1}}\leq\frac{R_{0}^{2}}{2}.\]

Next, assume that we have \(\mathbb{E}[\|\mathbf{\widehat{x}}^{t}-\mathbf{x}^{*}\|^{2}]\leq\frac{R_{0}^{2}}{2^{t}}\) for some \(t\leq r-1\). Then, Theorem 9 implies that

\[\mathbb{E}\Big{[}\big{\|}\mathbf{\widehat{x}}^{t+1}-\mathbf{x}^{*}\big{\|}^{2}\big{|} \;\mathbf{\widehat{x}}^{t}\Big{]}\leq\frac{2\|\mathbf{\widehat{x}}^{t}-\mathbf{x}^{*}\|^{2 }}{\mu\gamma_{t}K_{t}}.\]

Taking the full expectation from the both sides of previous inequality we get

\[\mathbb{E}\Big{[}\big{\|}\mathbf{\widehat{x}}^{t+1}-\mathbf{x}^{*}\big{\|}^{2}\Big{]} \leq\frac{2\mathbb{E}[\|\mathbf{\widehat{x}}^{t}-\mathbf{x}^{*}\|^{2}]}{\mu\gamma_{t}K _{t}}\leq\frac{2R_{0}^{2}}{2^{t}\mu\gamma_{t}K_{t}}\leq\frac{R_{0}^{2}}{2^{t+ 1}}.\]

Therefore, by mathematical induction we have that for all \(t=1,\ldots,r\)

\[\mathbb{E}\left[\|\mathbf{\widehat{x}}^{t}-\mathbf{x}^{*}\|^{2}\right]\leq\frac{R_{0}^ {2}}{2^{t}}.\]

Then, after \(r=\left\lceil\log_{2}\frac{R_{0}^{2}}{\varepsilon}\right\rceil-1\) restarts of SGDA-CC we have \(\mathbb{E}\left[\|\mathbf{\widehat{x}}^{r}-\mathbf{x}^{*}\|^{2}\right]\leq\varepsilon\). The total number of iterations executed by SGDA-CC is

\[\sum\limits_{t=1}^{r}K_{t} = \Theta\left(\sum\limits_{t=1}^{r}\max\left\{\frac{\ell}{\mu}, \frac{\sigma^{2}2^{t}}{(n-2B-m)\mu^{2}R_{0}^{2}},\frac{nB\rho 2^{\frac{t}{2}}}{m\mu R_{0}}\right\}\right)\] \[= \Theta\left(\frac{\ell}{\mu}r+\frac{\sigma^{2}2^{r}}{(n-2B-m)\mu ^{2}R_{0}^{2}}+\frac{nB\rho 2^{\frac{t}{2}}}{m\mu R_{0}}\right)\] \[= \Theta\left(\frac{\ell}{\mu}\log\frac{\mu R_{0}^{2}}{\varepsilon} +\frac{\sigma^{2}}{(n-2B-m)\mu^{2}R_{0}^{2}}\cdot\frac{\mu R_{0}^{2}}{ \varepsilon}+\frac{nB\rho}{m\mu R_{0}}\cdot\sqrt{\frac{\mu R_{0}^{2}}{ \varepsilon}}\right)\] \[= \Theta\left(\frac{\ell}{\mu}\log\frac{\mu R_{0}^{2}}{\varepsilon }+\frac{\sigma^{2}}{(n-2B-m)\mu\varepsilon}+\frac{nB\rho}{m\sqrt{\mu \varepsilon}}\right).\]

**Corollary 8**.: _Let assumptions of 5 hold. Then \(\mathbb{E}\left[\|\mathbf{\widehat{x}}^{r}-\mathbf{x}^{*}\|^{2}\right]\leq\varepsilon\) holds after_

\[\sum\limits_{t=1}^{r}K_{t}=\Theta\left(\frac{\ell}{\mu}\log\frac{\mu R^{2}}{ \varepsilon}+\frac{\sigma^{2}}{n\mu\varepsilon}+\frac{n^{2}\sigma}{m\sqrt{ \mu\varepsilon}}\right) \tag{56}\]

_iterations of SGDA-CC._

Proof.: Using the definition of \(\rho\) from Lemma E.1 and if \(B\leq\frac{n}{4}\), \(m<<n\) the bound for \(\sum\limits_{t=1}^{r}K_{t}\) can be easily derived.

### Proofs for SEG-CC

```
0:Ragg, \(\gamma\)
1:for\(t=1,\ldots\)do
2:for worker \(i\in[n]\)in parallel
3:\(\boldsymbol{g}_{\boldsymbol{\xi}_{t}}^{t}\gets\boldsymbol{g}_{\boldsymbol{ \xi}_{t}}(\boldsymbol{x}^{t},\boldsymbol{\xi}_{t})\)
4:send\(\boldsymbol{g}_{\boldsymbol{\xi}_{t}}^{t}\) if \(i\in\mathbb{G}_{t}\), else send \(*\) if Byzantine
5:\(\widehat{\boldsymbol{g}}_{\boldsymbol{\xi}^{t}}(\boldsymbol{x}^{t})=\frac{1}{ \overline{\nu}_{t-\frac{1}{2}}}\sum_{i\in W_{t-\frac{1}{2}}}\boldsymbol{g}_{ \boldsymbol{\xi}_{t}}^{t}\), \(W_{t-\frac{1}{2}}=(\mathbb{G}_{t}\cup\mathbb{B}_{t})\setminus C_{t}\)
6:if\(\left|\left\{i\in W_{t-\frac{1}{2}}\;\middle|\;\left\|\widehat{\boldsymbol{g}}_{ \boldsymbol{\xi}^{t}}(\boldsymbol{x}^{t})-\boldsymbol{g}_{\boldsymbol{\xi}_{t} ^{t}}^{t}\right\|\leq C\sigma\right\}\right|\geq\left|\nicefrac{{\text{$w_{t -\frac{1}{2}}$}}}{{2}}\right|\)then
7:\(\widetilde{\boldsymbol{x}}^{t}\leftarrow\boldsymbol{x}^{t}-\gamma_{1}\widehat {\boldsymbol{g}}_{\boldsymbol{\xi}^{t}}(\boldsymbol{x}^{t})\)
8:else
9:recompute
10:endif
11:\(C_{t+\frac{1}{2}},\mathbb{G}_{t+\frac{1}{2}}\cup\mathbb{B}_{t+\frac{1}{2}}= \texttt{CheckComputations}(C_{t},\mathbb{G}_{t}\cup\mathbb{B}_{t})\)
12:for worker \(i\in[n]\)in parallel
13:\(\boldsymbol{g}_{\boldsymbol{\eta}_{i}}^{t}\leftarrow\boldsymbol{g}_{i}( \widetilde{\boldsymbol{x}}^{t},\boldsymbol{\eta}_{i})\)
14:send\(\boldsymbol{g}_{\boldsymbol{\eta}_{i}}^{t}\) if \(i\in\mathbb{G}\), else send \(*\) if Byzantine
15:if\(\left|\left\{i\in W_{t}\;\middle|\;\left\|\widehat{\boldsymbol{g}}_{\boldsymbol{ \eta}^{t}}(\widetilde{\boldsymbol{x}}^{t})-\boldsymbol{g}_{\boldsymbol{\eta} _{i}}^{t}\right\|\leq C\sigma\right\}\right|\geq\nicefrac{{\text{$|w_{i}|$}}}{ {2}}\)then
16:\(\boldsymbol{x}^{t+1}\leftarrow\boldsymbol{x}^{t}-\gamma_{2}\widehat{\boldsymbol{g }}_{\boldsymbol{\eta}^{t}}(\widetilde{\boldsymbol{x}}^{t})\)
17:else
18:recompute
19:endif
20:\(C_{t+1},\mathbb{G}_{t+1}\cup\mathbb{B}_{t+1}=\texttt{CheckComputations}(C_{t+ \frac{1}{2}},\mathbb{G}_{t+\frac{1}{2}}\cup\mathbb{B}_{t+\frac{1}{2}})\)
```

**Algorithm 7** SEG-CC

#### e.3.1 Auxilary results

Similarly to Section E.1 we state the following. If a Byzantine peer deviates from the protocol at iteration \(k\), it will be detected with some probability \(p_{k}\) during the next iteration. One can lower bound this probability as

\[p_{k}\geq m\cdot\frac{G_{k}}{n_{k}}\cdot\frac{1}{n_{k}}=\frac{m(1-\delta_{k})} {n_{k}}\geq\frac{m}{n}.\]

Therefore, each individual Byzantine worker can violate the protocol no more than \(\nicefrac{{1}}{{p}}\) times on average implying that

\[\sum_{l=0}^{\infty}\mathbb{E}[\mathbb{1}_{l}]+\sum_{l=0}^{\infty}\mathbb{E} \Big{[}\mathbb{1}_{l-\frac{1}{2}}\Big{]}\leq\frac{nB}{m}. \tag{57}\]

**Lemma E.2**.: _Let Assumption 3 holds. Let Algorithm 7 is run with \(\gamma_{1}\leq\nicefrac{{1}}{{2L}}\) and \(\beta=\nicefrac{{\gamma_{2}}}{{\gamma_{1}}}\leq\nicefrac{{1}}{{2}}\). Then its iterations satisfy_

\[2\gamma_{2}\big{\langle}\overline{\boldsymbol{g}}_{\boldsymbol{ \eta}^{k}}(\widetilde{\boldsymbol{x}}^{k}),\widetilde{\boldsymbol{x}}^{k}- \boldsymbol{u}\big{\rangle} \leq \left\|\boldsymbol{x}^{k}-\boldsymbol{u}\right\|^{2}-\left\| \boldsymbol{x}^{k+1}-\boldsymbol{u}\right\|^{2}-2\gamma_{2}\big{\langle} \widehat{\boldsymbol{g}}_{\boldsymbol{\eta}^{k}}(\widetilde{\boldsymbol{x}}^{k} )-\overline{\boldsymbol{g}}_{\boldsymbol{\eta}^{k}}(\widetilde{\boldsymbol{x}}^{k }),\boldsymbol{x}^{k}-\boldsymbol{u}\big{\rangle}\] \[+2\gamma_{2}^{2}\big{\|}\widehat{\boldsymbol{g}}_{\boldsymbol{ \eta}^{k}}(\widetilde{\boldsymbol{x}}^{k})-\overline{\boldsymbol{g}}_{ \boldsymbol{\eta}^{k}}(\widetilde{\boldsymbol{x}}^{k})\big{\|}^{2}+4\gamma_{1} \gamma_{2}\big{\|}\overline{\boldsymbol{g}}_{\boldsymbol{\eta}^{k}}( \widetilde{\boldsymbol{x}}^{k})-F(\widetilde{\boldsymbol{x}}^{k})\big{\|}^{2}\] \[+4\gamma_{1}\gamma_{2}\big{\|}F(\boldsymbol{x}^{k})-\overline{ \boldsymbol{g}}_{\boldsymbol{\xi}^{k}}(\boldsymbol{x}^{k})\big{\|}^{2}+4\gamma_{1} \gamma_{2}\big{\|}\overline{\boldsymbol{g}}_{\boldsymbol{\xi}^{k}}( \boldsymbol{x}^{k})-\widehat{\boldsymbol{g}}_{\boldsymbol{\xi}^{k}}(\boldsymbol{ x}^{k})\big{\|}^{2}.\]

Proof.: Since \(\boldsymbol{x}^{k+1}=\boldsymbol{x}^{k}-\gamma_{2}\widehat{\boldsymbol{g}}_{ \boldsymbol{\eta}^{k}}(\widetilde{\boldsymbol{x}}^{k})\), we have

\[\big{\|}\boldsymbol{x}^{k+1}-\boldsymbol{u}\big{\|}^{2} = \big{\|}\boldsymbol{x}^{k}-\gamma_{2}\widehat{\boldsymbol{g}}_{ \boldsymbol{\eta}^{k}}(\widetilde{\boldsymbol{x}}^{k})-\boldsymbol{u}\big{\|}^{2}\] \[= \big{\|}\boldsymbol{x}^{k}-\boldsymbol{u}\big{\|}^{2}-2\gamma_{2} \big{\langle}\widehat{\boldsymbol{g}}_{\boldsymbol{\eta}^{k}}(\widetilde{ \boldsymbol{x}}^{k}),\boldsymbol{x}^{k}-\boldsymbol{u}\big{\rangle}+\gamma_{2}^{2} \big{\|}\widehat{\boldsymbol{g}}_{\boldsymbol{\eta}^{k}}(\widetilde{\boldsymbol{x}}^{k })\big{\|}^{2}.\]Rearranging the terms gives that

\[2\gamma_{2}\langle\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{ \mathbf{x}}^{k}),\mathbf{x}^{k}-\mathbf{u}\rangle = \left\|\mathbf{x}^{k}-\mathbf{u}\right\|^{2}-\left\|\mathbf{x}^{k+1}-\mathbf{u} \right\|^{2}\] \[-2\gamma_{2}\langle\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{ \mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\mathbf{x}^ {k}-\mathbf{u}\rangle+\gamma_{2}^{2}\big{\|}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})\big{\|}^{2}.\]

Next we use (14)

\[2\langle\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k} ),\mathbf{x}^{k}-\mathbf{u}\rangle = 2\langle\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k} ),\mathbf{x}^{k}-\widetilde{\mathbf{x}}^{k}\rangle+2\langle\overline{\mathbf{g}}_{\mathbf{ \eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{u}\rangle\] \[= 2\gamma_{1}\langle\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{ \mathbf{x}}^{k}),\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\rangle+2\langle \overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\widetilde{\mathbf{x}}^ {k}-\mathbf{u}\rangle\] \[\stackrel{{\eqref{eq:2}}}{{=}} -\gamma_{1}\Big{(}\big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})-\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{ 2}-\big{\|}\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}-\big{\|} \overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})\big{\|}^{2}\Big{)}\] \[+2\langle\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k} ),\widetilde{\mathbf{x}}^{k}-\mathbf{u}\rangle\]

and obtain the following

\[2\gamma_{2}\langle\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{ \mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{u}\rangle = \left\|\mathbf{x}^{k}-\mathbf{u}\right\|^{2}-\left\|\mathbf{x}^{k+1}-\mathbf{u} \right\|^{2}-2\gamma_{2}\langle\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{ \mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\mathbf{x}^ {k}-\mathbf{u}\rangle\] \[+\gamma_{2}^{2}\big{\|}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})\big{\|}^{2}\] \[+\gamma_{1}\gamma_{2}\Big{(}\big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^ {k}}(\widetilde{\mathbf{x}}^{k})-\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|} ^{2}-\big{\|}\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}-\big{\|} \overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})\big{\|}^{2}\Big{)}\] \[\stackrel{{\eqref{eq:2}}}{{\leq}} \left\|\mathbf{x}^{k}-\mathbf{u}\right\|^{2}-\left\|\mathbf{x}^{k+1}-\mathbf{u} \right\|^{2}-2\gamma_{2}\langle\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{ x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\mathbf{x}^{k}-\mathbf{u}\rangle\] \[+2\gamma_{2}^{2}\big{\|}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k })\big{\|}^{2}+2\gamma_{2}^{2}\big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})\big{\|}^{2}\] \[+\gamma_{1}\gamma_{2}\Big{(}\big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^ {k}}(\widetilde{\mathbf{x}}^{k})-\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|} ^{2}-\big{\|}\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}\Big{)}.\]

If \(\beta=\nicefrac{{\gamma_{2}}}{{\gamma_{1}}}\leq\nicefrac{{1}}{{2}}\)

\[2\gamma_{2}\langle\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{ \mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{u}\rangle \leq \left\|\mathbf{x}^{k}-\mathbf{u}\right\|^{2}-\left\|\mathbf{x}^{k+1}-\mathbf{u} \right\|^{2}-2\gamma_{2}\langle\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{ x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\mathbf{x}^{k}-\mathbf{u}\rangle\] \[+2\gamma_{2}^{2}\big{\|}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}) \big{\|}^{2}\] \[+\gamma_{1}\gamma_{2}\Big{(}\big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^ {k}}(\widetilde{\mathbf{x}}^{k})-\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|} ^{2}-\big{\|}\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}\Big{)}.\]

Combining the latter with the result of the following chain

\[\big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})- \widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2} \stackrel{{\eqref{eq:2}}}{{\leq}} 4\big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})-F (\widetilde{\mathbf{x}}^{k})\big{\|}^{2}+4\big{\|}F(\widetilde{\mathbf{x}}^{k})-F(\mathbf{x }^{k})\big{\|}^{2}\] \[+4\big{\|}F(\mathbf{x}^{k})-\overline{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k}) \big{\|}^{2}+4\big{\|}\overline{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})-\widehat{\mathbf{g} }_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}\] \[\stackrel{{\eqref{eq:2}}}{{\leq}} 4\big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})-F (\widetilde{\mathbf{x}}^{k})\big{\|}^{2}+4\big{\|}\overline{\mathbf{g}}_{\mathbf{\xi}^{k}}( \mathbf{x}^{k})-\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}\] \[= 4\big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})-F (\widetilde{\mathbf{x}}^{k})\big{\|}^{2}+4\big{\|}F(\mathbf{x}^{k})-\overline{\mathbf{g}}_{ \mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}\] \[+4\big{\|}\overline{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})-\widehat{\mathbf{g }}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}+4\gamma_{1}^{2}L^{2}\big{\|}\widehat{ \mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}\]

we obtain if \(\gamma_{1}\leq\nicefrac{{1}}{{2}}L\)

\[2\gamma_{2}\langle\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{ \mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{u}\rangle \leq \left\|\mathbf{x}^{k}-\mathbf{u}\right\|^{2}-\left\|\mathbf{x}^{k+1}-\mathbf{u} \right\|^{2}-2\gamma_{2}\langle\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}) -\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\mathbf{x}^{k}-\mathbf{u}\rangle\] (58) \[+2\gamma_{2}^{2}\big{\|}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{ \mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})\big{\|}^{2}+4 \gamma_{1}\gamma_{2}\big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})-F (\widetilde{\mathbf{x}}^{k})\big{\|}^{2}\] \[+4\gamma_{1}\gamma_{2}\big{\|}F(\mathbf{x}^{k})-\overline{\mathbf{g}}_{\mathbf{ \xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}+4\gamma_{1}\gamma_{2}\big{\|}\overline{\mathbf{g}}_{ \bm

#### e.3.2 Lipschitz Case

**Theorem 11**.: _Let Assumptions 1, 3, 5 hold. And let_

\[\gamma_{1} = \min\left\{\frac{1}{2L},\sqrt{\frac{(n-2B-m)R^{2}}{16\sigma^{2}K}},\sqrt{\frac{mR^{2}}{8\rho^{2}Bn}}\right\}, \tag{59}\] \[\gamma_{2} = \min\left\{\frac{1}{4L},\sqrt{\frac{m^{2}R^{2}}{64\rho^{2}B^{2}n^ {2}}},\sqrt{\frac{(n-2B-m)R^{2}}{64\sigma^{2}K}}\right\}, \tag{60}\]

_where \(\rho^{2}=q\sigma^{2}\) with \(q=2C^{2}+12+\frac{12}{n-2B-m}\) and \(C=\Theta(1)\) by Lemma E.1. Then iterations of_ SEG-CC _(Algorithm 7) satisfy for \(k\geq 1\)_

\[\mathbb{E}[R_{k}^{2}]\leq 2R, \tag{61}\]

_and_

\[\sum_{k=0}^{K-1}\mathbb{E}\big{[}\big{\langle}F(\widetilde{\mathbf{x}}^{k}), \widetilde{\mathbf{x}}^{k}-\mathbf{x}^{*}\big{\rangle}\big{]}\leq\frac{R^{2}}{\gamma_ {2}}.\]

_where \(R_{k}=\|\mathbf{x}^{k}-\mathbf{x}^{*}\|\) and \(R\geq\|\mathbf{x}^{0}-\mathbf{x}^{*}\|\)._

Proof.: Substituting \(\mathbf{u}=\mathbf{x}^{*}\) into the result of Lemma E.2 and taking expectation over \(\mathbf{\eta}^{k}\) one obtains

\[2\gamma_{2}\mathbb{E}_{\mathbf{\eta}^{k}}\big{[}\big{\langle}\mathbf{\bar {g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{x}^{*} \big{\rangle}\big{]}\] \[\stackrel{{\text{(\ref{eq:1})}}}{{\leq}} \big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2}-\mathbb{E}_{\mathbf{\eta}^{ k}}\Big{[}\big{\|}\mathbf{x}^{k+1}-\mathbf{x}^{*}\big{\|}^{2}\Big{]}-2\gamma_{2} \mathbb{E}_{\mathbf{\eta}^{k}}\big{[}\big{\langle}\mathbf{\hat{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})-\mathbf{\bar{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}), \mathbf{x}^{k}-\mathbf{x}^{*}\big{\rangle}\big{]}\] \[+\gamma_{1}\gamma_{2}4\mathbb{E}_{\mathbf{\eta}^{k}}\Big{[}\big{\|} \mathbf{\bar{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})-F(\widetilde{\mathbf{x}}^{k} )\big{\|}^{2}\Big{]}+\gamma_{1}\gamma_{2}4\big{\|}F(\mathbf{x}^{k})-\mathbf{\bar{g}}_{ \mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}\] \[+\gamma_{1}\gamma_{2}4\big{\|}\mathbf{\bar{g}}_{\mathbf{\xi}^{k}}(\mathbf{x} ^{k})-\mathbf{\hat{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}+2\gamma_{2}^{2} \mathbb{E}_{\mathbf{\eta}^{k}}\Big{[}\big{\|}\mathbf{\hat{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})-\mathbf{\bar{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}) \big{\|}^{2}\Big{]}\]

To estimate the inner product in the right-hand side we apply Cauchy-Schwarz inequality:

\[-2\gamma_{2}\mathbb{E}_{\mathbf{\eta}^{k}}\big{[}\big{\langle}\mathbf{\hat {g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})-\mathbf{\bar{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k}),\mathbf{x}^{k}-\mathbf{x}^{*}\big{\rangle}\big{]}\] \[\leq 2\gamma_{2}\mathbb{E}_{\mathbf{\eta}^{k}}\big{[}\big{\|}\mathbf{x}^{k}- \mathbf{x}^{*}\big{\|}\big{\|}\mathbf{\hat{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}) -\mathbf{\bar{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})\big{\|}\big{]}\] \[\stackrel{{ Lemma~{}E.1}}{{\leq}} 2\gamma_{2}\rho\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}1_{k}.\]

Then Assumption 6 implies

\[2\gamma_{2}\big{\langle}F(\widetilde{\mathbf{x}}^{k}),\widetilde{\bm {x}}^{k}-\mathbf{x}^{*}\big{\rangle} \leq \big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2}-\mathbb{E}_{\mathbf{\eta}^ {k}}\Big{[}\big{\|}\mathbf{x}^{k+1}-\mathbf{x}^{*}\big{\|}^{2}\Big{]}\] \[+2\gamma_{2}\rho\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}1_{k}+2 \gamma_{2}^{2}\rho^{2}1_{k}+\frac{4\gamma_{1}\gamma_{2}\sigma^{2}}{|\hat{G}_{ k}\setminus\hat{C}_{k}|}\] \[+4\gamma_{1}\gamma_{2}\big{\|}F(\mathbf{x}^{k})-\mathbf{\bar{g}}_{\mathbf{ \xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}+4\gamma_{1}\gamma_{2}\big{\|}\mathbf{\bar{g}}_{ \mathbf{\xi}^{k}}(\mathbf{x}^{k})-\mathbf{\hat{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}.\]Taking expectation over \(\mathbf{\xi}^{k}\) one obtains that

\[2\gamma_{2}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\big{[}\big{<}F( \widetilde{\mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{x}^{*}\big{>}\big{]}\] \[\leq \big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2}-\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\Big{[}\big{\|}\mathbf{x}^{k+1}-\mathbf{x}^{*}\big{\|}^{2}\Big{]}+2 \gamma_{2}\rho\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}1_{k}+2\gamma_{2}^{2}\rho^{ 2}1_{k}\] \[+\gamma_{1}\gamma_{2}\bigg{(}\frac{4\sigma^{2}}{|\mathbb{G}_{k} \setminus C_{k}|}+4\mathbb{E}_{\mathbf{\xi}^{k}}\Big{[}\big{\|}F(\mathbf{x}^{k})- \overline{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}\Big{]}+4\mathbb{E}_{ \mathbf{\xi}^{k}}\Big{[}\big{\|}\overline{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})- \widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}\Big{]}\bigg{)}\] \[\leq \big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2}-\mathbb{E}_{\mathbf{\xi}^{k },\mathbf{\eta}^{k}}\Big{[}\big{\|}\mathbf{x}^{k+1}-\mathbf{x}^{*}\big{\|}^{2}\Big{]}+2 \gamma_{2}\rho\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}1_{k}+2\gamma_{2}^{2}\rho^{ 2}1_{k}\] \[+\gamma_{1}\gamma_{2}\bigg{(}\frac{4\sigma^{2}}{|\mathbb{G}_{k} \setminus C_{k}|}+\frac{4\sigma^{2}}{|\mathbb{G}_{k+\frac{1}{2}}\setminus C_{k +\frac{1}{2}}|}+4\rho^{2}1_{k-\frac{1}{2}}\bigg{)}\] \[\leq \big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2}-\mathbb{E}_{\mathbf{\xi}^{k },\mathbf{\eta}^{k}}\Big{[}\big{\|}\mathbf{x}^{k+1}-\mathbf{x}^{*}\big{\|}^{2}\Big{]}+2 \gamma_{2}\rho\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}1_{k}+2\gamma_{2}^{2}\rho^{ 2}1_{k}\] \[+\gamma_{1}\gamma_{2}\bigg{(}\frac{4\sigma^{2}}{n-2B-m}+\frac{4 \sigma^{2}}{n-2B-m}+4\rho^{2}1_{k-\frac{1}{2}}\bigg{)}.\]

Finally taking the full expectation gives that

\[2\gamma_{2}\mathbb{E}\big{[}\big{<}F(\widetilde{\mathbf{x}}^{k}), \widetilde{\mathbf{x}}^{k}-\mathbf{x}^{*}\big{>}\big{]}\] \[\leq \mathbb{E}\Big{[}\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2}\Big{]} -\mathbb{E}\Big{[}\big{\|}\mathbf{x}^{k+1}-\mathbf{x}^{*}\big{\|}^{2}\Big{]}+2\gamma_{ 2}\rho\mathbb{E}\big{[}\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}1_{k}\big{]}+2 \gamma_{2}^{2}\rho^{2}\mathbb{E}[1_{k}]\] \[+\gamma_{1}\gamma_{2}\bigg{(}\frac{8\sigma^{2}}{n-2B-m}+4\rho^{2} \mathbb{E}\Big{[}1_{k-\frac{1}{2}}\Big{]}\bigg{)}.\]

Summing up the results for \(k=0,1,\ldots,K-1\) we derive

\[\frac{2\gamma_{2}}{K}\sum_{k=0}^{K-1}\mathbb{E}\big{[}\big{<}F( \widetilde{\mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{x}^{*}\big{>}\big{]}\] \[\leq \frac{1}{K}\sum_{k=0}^{K-1}\big{(}\mathbb{E}\left[\|\mathbf{x}^{k}- \mathbf{x}^{*}\|^{2}\right]-\mathbb{E}\left[\|\mathbf{x}^{k+1}-\mathbf{x}^{*}\|^{2}\right] \big{)}+\frac{8\gamma_{1}\gamma_{2}\sigma^{2}}{n-2B-m}\] \[+\frac{2\gamma_{2}\rho}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\|\mathbf{x }^{k}-\mathbf{x}^{*}\|1_{k}\right]+\frac{2\gamma_{2}^{2}\rho^{2}}{K}\sum_{k=0}^{ K-1}\mathbb{E}[1_{k}]+\frac{4\gamma_{1}\gamma_{2}\rho^{2}}{K}\sum_{k=0}^{K-1} \mathbb{E}\Big{[}1_{k-\frac{1}{2}}\Big{]}\] \[\leq \frac{\|\mathbf{x}^{0}-\mathbf{x}^{*}\|^{2}-\mathbb{E}[\|\mathbf{x}^{K}-\bm {x}^{*}\|^{2}]}{K}+\frac{8\gamma_{1}\gamma_{2}\sigma^{2}}{n-2B-m}\] \[+\frac{2\gamma_{2}\rho}{K}\sum_{k=0}^{K-1}\sqrt{\mathbb{E}\left[ \|\mathbf{x}^{k}-\mathbf{x}^{*}\|^{2}\right]\mathbb{E}[1_{k}]}+\frac{2\gamma_{2}^{2} \rho^{2}}{K}\sum_{k=0}^{K-1}\mathbb{E}[1_{k}]+\frac{4\gamma_{1}\gamma_{2}\rho^{ 2}}{K}\sum_{k=0}^{K-1}\mathbb{E}\Big{[}1_{k-\frac{1}{2}}\Big{]}.\]

Assumption 5 implies that \(0\leq\big{<}F(\mathbf{x}^{*}),\widetilde{\mathbf{x}}^{k}-\mathbf{x}^{*}\big{>}\leq\big{<}F (\widetilde{\mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{x}^{*}\big{>}\). Using this and a the notation \(R_{k}=\|\mathbf{x}^{k}-\mathbf{x}^{*}\|\), \(k>0\), \(R_{0}\geq\|\mathbf{x}^{0}-\mathbf{x}^{*}\|\) we get

\[0 \leq \frac{R_{0}^{2}-\mathbb{E}[R_{K}^{2}]}{K}+\frac{8\gamma_{1}\gamma_{ 2}\sigma^{2}}{n-2B-m} \tag{62}\] \[+\frac{2\gamma_{2}\rho}{K}\sum_{k=0}^{K-1}\sqrt{\mathbb{E}\left[\| \mathbf{x}^{k}-\mathbf{x}^{*}\|^{2}\right]\mathbb{E}[1_{k}]}+\frac{2\gamma_{2}^{2} \rho^{2}}{K}\sum_{k=0}^{K-1}\mathbb{E}[1_{k}]+\frac{4\gamma_{1}\gamma_{2}\rho^{ 2}}{K}\sum_{k=0}^{K-1}\mathbb{E}\Big{[}1_{k-\frac{1}{2}}\Big{]}\]

implying (after changing the indices) that

\[\mathbb{E}[R_{k}^{2}] \leq R_{0}^{2}+\frac{8\gamma_{1}\gamma_{2}\sigma^{2}k}{n-2B-m}+2 \gamma_{2}\rho\sum_{l=0}^{k-1}\sqrt{\mathbb{E}[R_{l}^{2}]\mathbb{E}[1_{l}]} \tag{63}\] \[+2\gamma_{2}^{2}\rho^{2}\sum_{l=0}^{k-1}\mathbb{E}[1_{l}]+4 \gamma_{1}\gamma_{2}\rho^{2}\sum_{l=0}^{k-1}\mathbb{E}\Big{[}1_{l-\frac{1}{2}} \Big{]} \tag{64}\]holds for all \(k\geq 0\). In the remaining part of the proof we derive by induction that

\[\mathbb{E}[R_{k}^{2}] \leq R_{0}^{2}+\frac{8\gamma_{1}\gamma_{2}\sigma^{2}k}{n-2B-m}+2\gamma_{ 2}\rho\sum_{l=0}^{k-1}\sqrt{\mathbb{E}[R_{l}^{2}]\mathbb{E}[1_{l}]} \tag{65}\] \[+2\gamma_{2}^{2}\rho^{2}\sum_{l=0}^{k-1}\mathbb{E}[\mathbb{1}_{l} ]+4\gamma_{1}\gamma_{2}\rho^{2}\sum_{l=0}^{k-1}\mathbb{E}\Big{[}\mathbb{1}_{l- \frac{1}{2}}\Big{]}\leq 2R_{0}^{2} \tag{66}\]

for all \(k=0,\ldots,K\). For \(k=0\) this inequality trivially holds. Next, assume that it holds for all \(k=0,1,\ldots,T-1\), \(T\leq K-1\). Let us show that it holds for \(k=T\) as well. From (64) and (66) we have that \(\mathbb{E}[R_{k}^{2}]\leq 2R_{0}^{2}\) for all \(k=0,1,\ldots,T-1\). Therefore,

\[\mathbb{E}[R_{T}^{2}] \leq R_{0}^{2}+\frac{8\gamma_{1}\gamma_{2}\sigma^{2}k}{n-2B-m}+2 \gamma_{2}\rho\sum_{l=0}^{k-1}\sqrt{\mathbb{E}[R_{l}^{2}]\mathbb{E}[1_{l}]}+2 \gamma_{2}^{2}\rho^{2}\sum_{l=0}^{k-1}\mathbb{E}[1_{l}]\] \[+4\gamma_{1}\gamma_{2}\rho^{2}\sum_{l=0}^{k-1}\mathbb{E}\Big{[} \mathbb{1}_{l-\frac{1}{2}}\Big{]}\] \[\leq R_{0}^{2}+\frac{8\gamma_{1}\gamma_{2}\sigma^{2}k}{n-2B-m}+2 \gamma_{2}\rho R_{0}\sum_{l=0}^{k-1}\sqrt{\mathbb{E}[1_{l}]}+2\gamma_{2}^{2} \rho^{2}\sum_{l=0}^{k-1}\mathbb{E}[\mathbb{1}_{l}]\] \[+4\gamma_{1}\gamma_{2}\rho^{2}\sum_{l=0}^{k-1}\mathbb{E}\Big{[} \mathbb{1}_{l-\frac{1}{2}}\Big{]}.\]

The latter together with the expected number of at least one peer violations (57) implies

\[\mathbb{E}[R_{T}^{2}] \leq R_{0}^{2}+\frac{8\gamma_{1}\gamma_{2}\sigma^{2}k}{n-2B-m}+2 \gamma_{2}\rho R_{0}\frac{nB}{m}+2\gamma_{2}^{2}\rho^{2}\frac{nB}{m}+4\gamma_ {1}\gamma_{2}\rho^{2}\frac{nB}{m}.\]

Taking

\[\gamma_{1}=\min\left\{\frac{1}{2L},\sqrt{\frac{(n-2B-m)R_{0}^{2}} {16\sigma^{2}K}},\sqrt{\frac{mR_{0}^{2}}{8\rho^{2}Bn}}\right\}\] \[\gamma_{2} = \min\left\{\sqrt{\frac{m^{2}R_{0}^{2}}{64\rho^{2}B^{2}n^{2}}}, \frac{1}{4L},\sqrt{\frac{(n-2B-m)R_{0}^{2}}{64\sigma^{2}K}},\sqrt{\frac{mR_{0 }^{2}}{32\rho^{2}Bn}}\right\}\] \[= \min\left\{\frac{1}{4L},\sqrt{\frac{m^{2}R_{0}^{2}}{64\rho^{2}B^ {2}n^{2}}},\sqrt{\frac{(n-2B-m)R_{0}^{2}}{64\sigma^{2}K}}\right\}\]

we satisfy conditions of Lemma E.2 and ensure that

\[\frac{8\gamma_{1}\gamma_{2}\sigma^{2}k}{n-2B-m}+2\gamma_{2}\rho R_{0}\frac{nB }{m}+2\gamma_{2}^{2}\rho^{2}\frac{nB}{m}+4\gamma_{1}\gamma_{2}\rho^{2}\frac{nB }{m}\leq\frac{R_{0}^{2}}{4}+\frac{R_{0}^{2}}{4}+\frac{R_{0}^{2}}{4}+\frac{R_{0 }^{2}}{4}=R_{0}^{2},\]

and, as a result, we get

\[\mathbb{E}[R_{T}^{2}]\leq 2R_{0}^{2}\equiv 2R. \tag{67}\]

Therefore, (66) holds for all \(k=0,1,\ldots,K\). Together with (62) it implies

\[\sum_{k=0}^{K-1}\mathbb{E}\big{[}\big{\langle}F(\widetilde{\mathbf{x}}^{k}), \widetilde{\mathbf{x}}^{k}-\mathbf{x}^{*}\big{\rangle}\big{]}\leq\frac{R_{0}^{2}}{ \gamma_{2}}.\]

#### e.3.3 Quasi-Strongly Monotone Case

**Lemma E.3**.: _Let Assumptions 3, 4 and Corollary 2 hold. If_

\[\gamma_{1}\leq\frac{1}{2L} \tag{68}\]

_then \(\overline{g}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})=\overline{g}_{\mathbf{\eta}^{k }}\left(\mathbf{x}^{k}-\gamma_{1}\widehat{g}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\right)\) satisfies the following inequality_

\[\gamma_{1}^{2}\mathbb{E}\left[\left\|\overline{g}_{\mathbf{\eta}^{k} }(\widetilde{\mathbf{x}}^{k})\right\|^{2}\mid\mathbf{x}^{k}\right] \leq 2\widehat{P}_{k}+\frac{8\gamma_{1}^{2}\sigma^{2}}{G}+4\gamma_{1} ^{2}\rho^{2}\mathbb{1}_{k-\frac{1}{2}}, \tag{69}\]

_where \(\widehat{P}_{k}=\gamma_{1}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\left< \overline{g}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\mathbf{x}^{k}-\mathbf{x}^{*} \right>\right]\) and \(\rho^{2}=q\sigma^{2}\) with \(q=2C^{2}+12+\frac{12}{n-2B-m}\) and \(C=\Theta(1)\) by Lemma E.1._

Proof.: Using the auxiliary iterate \(\widehat{\mathbf{x}}^{k+1}=\mathbf{x}^{k}-\gamma_{1}\overline{g}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})\), we get

\[\left\|\widehat{\mathbf{x}}^{k+1}-\mathbf{x}^{*}\right\|^{2} = \left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-2\gamma_{1}\langle\mathbf{x} ^{k}-\mathbf{x}^{*},\overline{g}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})\rangle+ \gamma_{1}^{2}\big{\|}\overline{g}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}) \big{\|}^{2} \tag{70}\] \[= \left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-2\gamma_{1}\left\langle \mathbf{x}^{k}-\gamma\widehat{g}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})-\mathbf{x}^{*},\overline{g }_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})\right\rangle\] (72) \[-2\gamma_{1}^{2}\langle\widehat{g}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k}), \overline{g}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})\rangle+\gamma_{1}^{2} \big{\|}\overline{g}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})\big{\|}^{2}.\]

Taking the expectation \(\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\cdot\right]=\mathbb{E}\left[ \cdot\mid\mathbf{x}^{k}\right]\) conditioned on \(\mathbf{x}^{k}\) from the above identity, using tower property \(\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\cdot\right]=\mathbb{E}_{\mathbf{\xi}^ {k}}\big{[}\mathbb{E}_{\mathbf{\eta}^{k}}\left[\cdot\right]\), and \(\mu\)-quasi strong monotonicity of \(F(x)\), we derive

\[\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\left\|\widehat{\mathbf{ x}}^{k+1}-\mathbf{x}^{*}\right\|^{2}\right]\] \[= \left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-2\gamma_{1}\mathbb{E}_{ \mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\langle\mathbf{x}^{k}-\gamma_{1}\widehat{g}_{\bm {\xi}^{k}}(\mathbf{x}^{k})-\mathbf{x}^{*},\overline{g}_{\mathbf{\eta}^{k}}(\widetilde{\bm {x}}^{k})\rangle\right]\] \[-2\gamma_{1}^{2}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[ \langle\widehat{g}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k}),\overline{g}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})\rangle\right]+\gamma_{1}^{2}\mathbb{E}_{\mathbf{\xi}^{k}, \mathbf{\eta}^{k}}\left[\left\|\overline{g}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k} )\right\|^{2}\right]\] \[= \left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}\] \[-2\gamma_{1}\mathbb{E}_{\mathbf{\xi}^{k}}\left[\langle\mathbf{x}^{k}- \gamma_{1}\widehat{g}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})-\mathbf{x}^{*},F\left(\mathbf{x}^{k}- \gamma_{1}\widehat{g}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\right)\rangle\right]\] \[-2\gamma_{1}^{2}\mathbb{E}_{\mathbf{\xi}^{k}}\left[\langle\widehat{ g}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k}),\overline{g}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}) \rangle\right]+\gamma_{1}^{2}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\left\| \overline{g}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})\right\|^{2}\right]\] \[\stackrel{{\text{(QSM),\eqref{eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eqTo upper bound the last term we use simple inequality (16), and apply \(L\)-Lipschitzness of \(F(x)\):

\[\mathbb{E}_{\boldsymbol{\xi}^{k},\boldsymbol{\eta}^{k}}\left[\left\| \widehat{\boldsymbol{x}}^{k+1}-\boldsymbol{x}^{*}\right\|^{2}\right] \stackrel{{\eqref{eq:L1}}}{{\leq}} \left\|\boldsymbol{x}^{k}-\boldsymbol{x}^{*}\right\|^{2}-\gamma_{ 1}^{2}\mathbb{E}_{\boldsymbol{\xi}^{k}}\left[\left\|\widehat{\boldsymbol{g}}_{ \boldsymbol{\xi}^{k}}(\boldsymbol{x}^{k})\right\|^{2}\right]\] \[+4\gamma_{1}^{2}\mathbb{E}_{\boldsymbol{\xi}^{k}}\left[\left\| \overline{\boldsymbol{g}}_{\boldsymbol{\xi}^{k}}(\boldsymbol{x}^{k})-\widehat{ \boldsymbol{g}}_{\boldsymbol{\xi}^{k}}(\boldsymbol{x}^{k})\right\|^{2}\right]\] \[+4\gamma_{1}^{2}\mathbb{E}_{\boldsymbol{\xi}^{k}}\left[\left\|F( \boldsymbol{x}^{k})-F\left(\widetilde{\boldsymbol{x}}^{k}\right)\right\|^{2}\right]\] \[+4\gamma_{1}^{2}\mathbb{E}_{\boldsymbol{\xi}^{k},\boldsymbol{\eta }^{k}}\left[\left\|\overline{\boldsymbol{g}}_{\boldsymbol{\eta}^{k}}\left( \widetilde{\boldsymbol{x}}^{k}\right)-F\left(\widetilde{\boldsymbol{x}}^{k} \right)\right\|^{2}\right]\] \[\stackrel{{\eqref{eq:L1},\eqref{eq:L2},\eqref{eq:L3}}}{{\leq}} \left\|\boldsymbol{x}^{k}-\boldsymbol{x}^{*}\right\|^{2}-\gamma_{ 1}^{2}\left(1-4L^{2}\gamma_{1}^{2}\right)\mathbb{E}_{\boldsymbol{\xi}^{k}} \left[\left\|\widehat{\boldsymbol{g}}_{\boldsymbol{\xi}^{k}}(\boldsymbol{x}^{k })\right\|^{2}\right]\] \[+4\gamma_{1}^{2}\mathbb{E}_{\boldsymbol{\xi}^{k}}\left[\left\| \overline{\boldsymbol{g}}_{\boldsymbol{\xi}^{k}}(\boldsymbol{x}^{k})-\widehat{ \boldsymbol{g}}_{\boldsymbol{\xi}^{k}}(\boldsymbol{x}^{k})\right\|^{2}\right]\] \[+\frac{4\gamma_{1}^{2}\sigma^{2}}{G}+\frac{4\gamma_{1}^{2}\sigma ^{2}}{G}\] \[\stackrel{{\eqref{eq:L1},\mathit{Lem.~{}D.1}}}{{\leq}} \left\|\boldsymbol{x}^{k}-\boldsymbol{x}^{*}\right\|^{2}-\gamma_{ 1}^{2}\left(1-4\gamma_{1}^{2}L^{2}\right)\mathbb{E}_{\boldsymbol{\xi}^{k}} \left[\left\|\widehat{\boldsymbol{g}}_{\boldsymbol{\xi}^{k}}(\boldsymbol{x}^{k })\right\|^{2}\right]\] \[+\frac{8\gamma_{1}^{2}\sigma^{2}}{G}+4\gamma_{1}^{2}\rho^{2} \mathbb{1}_{k-\frac{1}{2}}\] \[\stackrel{{\eqref{eq:L2}}}{{\leq}} \left\|\boldsymbol{x}^{k}-\boldsymbol{x}^{*}\right\|^{2}+\frac{8 \gamma_{1}^{2}\sigma^{2}}{G}+4\gamma_{1}^{2}\rho^{2}\mathbb{1}_{k-\frac{1}{2}}.\]

Finally, we use the above inequality together with (70):

\[\left\|\boldsymbol{x}^{k}-\boldsymbol{x}^{*}\right\|^{2}-2\widehat {P}_{k}+\gamma_{1}^{2}\mathbb{E}\left[\left\|\overline{\boldsymbol{g}}_{ \boldsymbol{\eta}^{k}}(\widetilde{\boldsymbol{x}}^{k})\right\|^{2}\mid \boldsymbol{x}^{k}\right] \leq \left\|\boldsymbol{x}^{k}-\boldsymbol{x}^{*}\right\|^{2}+\frac{8 \gamma_{1}^{2}\sigma^{2}}{G}+4\gamma_{1}^{2}\rho^{2}\mathbb{1}_{k-\frac{1}{2}},\]

where \(\widehat{P}_{k}=\gamma_{1}\mathbb{E}_{\boldsymbol{\xi}^{k},\boldsymbol{\eta }^{k}}\left[\left(\overline{\boldsymbol{g}}_{\boldsymbol{\eta}^{k}}( \widetilde{\boldsymbol{x}}^{k}),\boldsymbol{x}^{k}-\boldsymbol{x}^{*}\right)\right]\). Rearranging the terms, we obtain (69). 

**Lemma E.4**.: _Let Assumptions 3, 4 and Corollary 2 hold. If_

\[\gamma_{1}\leq\frac{1}{2\mu+2L}, \tag{73}\]

_then \(\overline{\boldsymbol{g}}_{\boldsymbol{\eta}^{k}}(\widetilde{\boldsymbol{x}} ^{k})=\overline{\boldsymbol{g}}_{\boldsymbol{\eta}^{k}}\left(\boldsymbol{x}^{k}- \gamma_{1}\widehat{\boldsymbol{g}}_{\boldsymbol{\xi}^{k}}(\boldsymbol{x}^{k})\right)\) satisfies the following inequality_

\[\widehat{P}_{k} \geq \frac{\mu\gamma_{1}}{2}\left\|\boldsymbol{x}^{k}-\boldsymbol{x}^ {*}\right\|^{2}+\frac{\gamma_{1}^{2}}{4}\mathbb{E}_{\boldsymbol{\xi}^{k}} \left[\left\|\overline{\boldsymbol{g}}_{\boldsymbol{\xi}^{k}}(\boldsymbol{x}^{k })\right\|^{2}\right]-\frac{8\gamma_{1}^{2}\sigma^{2}}{G}-\frac{9\gamma_{1}^{2} \rho^{2}\mathbb{1}_{k-\frac{1}{2}}}{2}, \tag{74}\]

_or simply_

\[-\widehat{P}_{k} \leq -\frac{\mu\gamma_{1}}{2}\left\|\boldsymbol{x}^{k}-\boldsymbol{x}^ {*}\right\|^{2}+\frac{4\gamma_{1}^{2}\sigma^{2}}{G}+4\gamma_{1}^{2}\rho^{2} \mathbb{1}_{k-\frac{1}{2}}\]

_where \(\widehat{P}_{k}=\gamma_{1}\mathbb{E}_{\boldsymbol{\xi}^{k},\boldsymbol{\eta}^{ k}}\left[\left(\overline{\boldsymbol{g}}_{\boldsymbol{\eta}^{k}}( \widetilde{\boldsymbol{x}}^{k}),\boldsymbol{x}^{k}-\boldsymbol{x}^{*}\right)\right]\), where \(\rho^{2}=q\sigma^{2}\) with \(q=2C^{2}+12+\frac{12}{n-2B-m}\) and \(C=\Theta(1)\) by Lemma E.1._

[MISSING_PAGE_EMPTY:53]

Proof of Theorem 6.: Since \(\mathbf{x}^{k+1}=\mathbf{x}^{k}-\gamma_{2}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{ \mathbf{x}}^{k}\right)\), we have

\[\left\|\mathbf{x}^{k+1}-\mathbf{x}^{*}\right\|^{2} = \left\|\mathbf{x}^{k}-\gamma_{2}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}\left( \widetilde{\mathbf{x}}^{k}\right)-\mathbf{x}^{*}\right\|^{2}\] \[= \left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-2\gamma_{2}\langle \widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right),\mathbf{x}^{k}- \mathbf{x}^{*}\rangle+\gamma_{2}^{2}\big{\|}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}\left( \widetilde{\mathbf{x}}^{k}\right)\big{\|}^{2}\] \[\leq \left\|\mathbf{x}^{k}-\mathbf{x}^{*}\right\|^{2}-2\gamma_{2}\langle \widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right),\mathbf{x}^{k} -\mathbf{x}^{*}\rangle+2\gamma_{2}^{2}\big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^{k}} \left(\widetilde{\mathbf{x}}^{k}\right)\big{\|}^{2}\] \[+2\gamma_{2}^{2}\big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}\left( \widetilde{\mathbf{x}}^{k}\right)-\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde {\mathbf{x}}^{k}\right)\big{\|}^{2}+2\gamma_{2}\langle\overline{\mathbf{g}}_{\mathbf{\eta} ^{k}}\left(\widetilde{\mathbf{x}}^{k}\right)-\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}} \left(\widetilde{\mathbf{x}}^{k}\right),\mathbf{x}^{k}-\mathbf{x}^{*}\rangle\] \[\leq (1+\lambda)\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2}-2\gamma_{2} \langle\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right), \mathbf{x}^{k}-\mathbf{x}^{*}\rangle+2\gamma_{2}^{2}\big{\|}\overline{\mathbf{g}}_{\mathbf{ \eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right)\big{\|}^{2}\] \[+\gamma_{2}^{2}\bigg{(}2+\frac{1}{\lambda}\bigg{)}\big{\|} \overline{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right)-\widehat {\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right)\big{\|}^{2}\]

Taking the expectation, conditioned on \(\mathbf{x}^{k}\),

\[\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\big{\|}\mathbf{x}^{k+1}-\mathbf{ x}^{*}\big{\|}^{2} \leq (1+\lambda)\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2}-2\beta \gamma_{1}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\langle\overline{\mathbf{g}}_{\bm {\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right),\mathbf{x}^{k}-\mathbf{x}^{*}\rangle\] \[+2\beta^{2}\gamma_{1}^{2}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}} \big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right) \big{\|}^{2}+\gamma_{2}^{2}\rho^{2}\bigg{(}2+\frac{1}{\lambda}\bigg{)}\mathbb{ I}_{k},\]

using the definition of \(\widehat{P}_{k}=\gamma_{1}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\langle \overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\mathbf{x}^{k}-\mathbf{x}^ {*}\rangle\right]\), we continue our derivation:

\[\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}}\left[\big{\|}\mathbf{x}^{k+1} -\mathbf{x}^{*}\big{\|}^{2}\right]\] \[= (1+\lambda)\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2}-2\beta \widehat{P}_{k}+2\beta^{2}\gamma_{1}^{2}\mathbb{E}_{\mathbf{\xi}^{k},\mathbf{\eta}^{k}} \big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}\left(\widetilde{\mathbf{x}}^{k}\right) \big{\|}^{2}\] \[+\gamma_{2}^{2}\rho^{2}\bigg{(}2+\frac{1}{\lambda}\bigg{)}\mathbb{ I}_{k}\] \[+\gamma_{2}^{2}\rho^{2}\bigg{(}2+\frac{1}{\lambda}\bigg{)}\mathbb{ I}_{k}\] \[\stackrel{{ 0\leq\beta\leq\nicefrac{{1}}{{2}}}}{{\leq}} (1+\lambda)\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2}-2\widehat{P}_{k} \big{(}\beta-2\beta^{2}\big{)}+\frac{16\gamma_{2}^{2}\sigma^{2}}{G}+8\gamma_{2} ^{2}\rho^{2}\mathbb{I}_{k-\frac{1}{2}}\] \[+\gamma_{2}^{2}\rho^{2}\bigg{(}2+\frac{1}{\lambda}\bigg{)}\mathbb{ I}_{k}\] \[\stackrel{{\eqref{eq:22}}}{{\leq}} (1+\lambda)\big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2}\] \[+2\beta(1-2\beta)\bigg{(}-\frac{\mu\gamma_{1}}{2}\big{\|}\mathbf{x}^{k }-\mathbf{x}^{*}\big{\|}^{2}+\frac{4\gamma_{1}^{2}\sigma^{2}}{G}+4\gamma_{1}^{2}\rho^ {2}\mathbb{I}_{k-\frac{1}{2}}\bigg{)}\] \[+\frac{16\gamma_{2}^{2}\sigma^{2}}{G}+8\gamma_{2}^{2}\rho^{2} \mathbb{I}_{k-\frac{1}{2}}+\gamma_{2}^{2}\rho^{2}\bigg{(}2+\frac{1}{\lambda} \bigg{)}\mathbb{I}_{k}\] \[\leq \Big{(}1+\lambda-2\beta(1-2\beta)\frac{\mu\gamma_{1}}{2}\Big{)} \big{\|}\mathbf{x}^{k}-\mathbf{x}^{*}\big{\|}^{2}\] \[+\frac{\gamma_{1}^{2}\sigma^{2}}{G}+\gamma_{1}^{2}\rho^{2}\mathbb{ I}_{k-\frac{1}{2}}+\frac{16\gamma_{2}^{2}\sigma^{2}}{G}+8\gamma_{2}^{2}\rho^{2} \mathbb{I}_{k-\frac{1}{2}}+\gamma_{2}^{2}\rho^{2}\bigg{(}2+\frac{1}{\lambda} \bigg{)}\mathbb{I}_{k}\] \[\stackrel{{ 0\leq\beta\leq\nicefrac{{1}}{{4}}}}{{\leq}} \Big{(}1+\lambda-\frac{\mu\gamma_{2}}{2}\Big{)}\big{\|}\mathbf{x}^{k}- \mathbf{x}^{*}\big{\|}^{2}\] \[\stackrel{{ 0\leq\beta\leq\nicefrac{{1}}{{4}}}}{{\leq}} \Big{(}1+\lambda-\frac{\mu\gamma_{2}}{2}\Big{)}\big{\|}\mathbf{x}^{k}- \mathbf{x}^{*}\big{\|}^{2}\] \[+\frac{\sigma^{2}}{G}\big{(}\gamma_{1}^{2}+16\gamma_{2}^{2}\big{)}+ \gamma_{1}^{2}\rho^{2}\mathbb{I}_{k-\frac{1}{2}}+8\gamma_{2}^{2}\rho^{2} \mathbb{I}_{k-\frac{1}{2}}+\gamma_{2}^{2}\rho^{2}\bigg{(}2+\frac{4}{\lambda} \bigg{)}\mathbb{I}_{k}.\]Next, taking the full expectation from the both sides and obtain

\[\mathbb{E}\left[\left\|\mathbf{x}^{k+1}-\mathbf{x}^{*}\right\|^{2}\right] \leq \Big{(}1-\frac{\mu\gamma_{2}}{4}\Big{)}\mathbb{E}\left[\left\|\mathbf{x }^{k}-\mathbf{x}^{*}\right\|^{2}\right]\] \[+\frac{\sigma^{2}}{G}\big{(}\gamma_{1}^{2}+16\gamma_{2}^{2}\big{)} +\rho^{2}\big{(}\gamma_{1}^{2}+8\gamma_{2}^{2}\big{)}\mathbb{E}1_{k-\frac{1}{2 }}+2\rho^{2}\bigg{(}\gamma_{2}^{2}+\frac{2\gamma_{2}}{\mu}\bigg{)}\mathbb{E}1_{k}.\]

Unrolling the recurrence, we derive the rest of the result:

\[\mathbb{E}\big{\|}\mathbf{x}^{K+1}-\mathbf{x}^{*}\big{\|}^{2} \leq \Big{(}1-\frac{\mu\gamma_{2}}{4}\Big{)}^{K+1}\big{\|}\mathbf{x}^{0}- \mathbf{x}^{*}\big{\|}^{2}+\frac{4\sigma^{2}\big{(}\gamma_{1}^{2}+16\gamma_{2}^{2} \big{)}}{\gamma_{2}\mu(n-2B-m)}\] \[+\rho^{2}\big{(}\gamma_{1}^{2}+8\gamma_{2}^{2}\big{)}\sum_{i}^{K} \Big{(}1-\frac{\mu\gamma_{2}}{4}\Big{)}^{K-i}\mathbb{E}1_{i-\frac{1}{2}}\] \[+2\rho^{2}\bigg{(}\gamma_{2}^{2}+\frac{2\gamma_{2}}{\mu}\bigg{)} \sum_{i}^{K}\Big{(}1-\frac{\mu\gamma_{2}}{4}\Big{)}^{K-i}\mathbb{E}1_{i}.\]

Since \(\gamma_{2}\leq\frac{4}{\mu}\) and that implies

\[\mathbb{E}\Bigg{[}\sum_{i}^{T}\mathbb{1}_{i}\Big{(}1-\frac{\gamma\mu}{2} \Big{)}^{T-i}\Bigg{]}\leq\mathbb{E}\Bigg{[}\sum_{i}^{T}\mathbb{1}_{i}\Bigg{]} \leq\frac{nB}{m}. \tag{75}\]

using the expected number of at least one peer violations (57) we derive

\[\mathbb{E}\big{\|}\mathbf{x}^{K+1}-\mathbf{x}^{*}\big{\|}^{2} \leq \Big{(}1-\frac{\mu\gamma_{2}}{4}\Big{)}^{K+1}\big{\|}\mathbf{x}^{0}- \mathbf{x}^{*}\big{\|}^{2}+\frac{4\sigma^{2}\big{(}\gamma_{1}^{2}+16\gamma_{2}^{2 }\big{)}}{\gamma_{2}\mu^{2}(n-2B-m)}\] \[+\rho^{2}\big{(}\gamma_{1}^{2}+8\gamma_{2}^{2}\big{)}\frac{nB}{m }+2\rho^{2}\bigg{(}\gamma_{2}^{2}+\frac{2\gamma_{2}}{\mu}\bigg{)}\frac{nB}{m}\] \[\leq \Big{(}1-\frac{\mu\gamma_{2}}{4}\Big{)}^{K+1}\big{\|}\mathbf{x}^{0}- \mathbf{x}^{*}\big{\|}^{2}+\frac{4\sigma^{2}\big{(}\gamma_{1}^{2}+16\gamma_{2}^{2 }\big{)}}{\gamma_{2}\mu(n-2B-m)}\] \[+\rho^{2}\big{(}\gamma_{1}^{2}+10\gamma_{2}^{2}\big{)}\frac{nB}{m }+\frac{4\rho^{2}\gamma_{2}}{\mu}\frac{nB}{m},\]

that together with \(\rho^{2}=q\sigma^{2}\) with \(q=2C^{2}+12+\frac{12}{n-2B-m}\) and \(C=\Theta(1)\) by Lemma E.1 give result of the theorem. 

**Corollary 9**.: _Let assumptions of Theorem 6 hold. Then \(\mathbb{E}\big{\|}\mathbf{x}^{T}-\mathbf{x}^{*}\big{\|}^{2}\leq\varepsilon\) holds after_

\[T=\widetilde{\Theta}\bigg{(}\frac{L}{\mu}+\frac{1}{\beta}+\frac{\sigma^{2}}{ \beta^{2}\mu^{2}(n-2B-m)\varepsilon}+\frac{q\sigma^{2}Bn}{\beta\mu^{2}m \varepsilon}+\frac{q\sigma^{2}Bn}{\beta^{2}\mu^{2}m\sqrt{\varepsilon}}\bigg{)}\]

_iterations of_ \(\mathsf{SEG-CC}\) _with_

\[\gamma=\min\left\{\frac{1}{2L+2\mu},\frac{4\ln\Big{(}\max\Bigl{\{}2,\min\Bigl{\{} \frac{m(n-2B-m)\beta^{2}\mu^{2}R^{2}K}{32m\sigma^{2}+4q\sigma^{2}\beta^{2}nB(n -2B-m)},\frac{m\mu^{2}\beta^{2}R^{2}K^{2}}{32qnB\sigma^{2}}\Bigr{\}}\Bigr{\}} \Bigr{)}}{\mu\beta(K+1)}\right\}.\]

Proof.: Next, we plug \(\gamma_{2}=\beta\gamma_{1}\leq\nicefrac{{\gamma_{1}}}{{4}}\) into the result of Theorem 6 and obtain

\[\mathbb{E}\left[\left\|\mathbf{x}^{k+1}-\mathbf{x}^{*}\right\|^{2}\right]\leq\left(1- \frac{\mu\beta\gamma_{1}}{4}\right)\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2}+ \frac{8\sigma^{2}\gamma_{1}}{\beta\mu(n-2B-m)}+2\rho^{2}\gamma_{1}^{2}\frac{nB} {m}+\frac{4\rho^{2}\beta\gamma_{1}}{\mu}\frac{nB}{m}. \tag{76}\]

Using the definition of \(\rho\) (\(\rho^{2}=q\sigma^{2}=\Theta(\sigma^{2})\)) from Lemma E.1 and if \(B\leq\frac{n}{4}\), \(m<<n\) the result of Theorem 6 can be simplified as

\[\mathbb{E}\big{\|}\mathbf{x}^{T}-\mathbf{x}^{*}\big{\|}^{2}\leq\ \left(1-\frac{\mu \beta\gamma_{1}}{4}\right)^{T}\big{\|}\mathbf{x}^{0}-\mathbf{x}^{*}\big{\|}^{2}+ \frac{8\sigma^{2}\gamma_{1}}{\beta\mu(n-2B-m)}+2q\sigma^{2}\gamma_{1}^{2}\frac {nB}{m}+\frac{4q\sigma^{2}\beta\gamma_{1}}{\mu}\frac{nB}{m}.\]

Applying Lemma C.4 to the last bound we get the result of the corollary.

#### e.3.4 Lipschitz Monotone Case

**Theorem 12**.: _Suppose the assumptions of Theorem 11 and Assumption 5 hold. Then after \(K\) iterations of \(\mathsf{SEG-CC}\) (Algorithm 7)_

\[\mathbb{E}\Big{[}\mathsf{Gap}_{B_{R}(x^{*})}\big{(}\overline{\mathbf{x}}^{K}\big{)} \Big{]}\leq\frac{3R^{2}}{2\gamma_{2}K}, \tag{77}\]

_where \(\mathsf{Gap}_{B_{R}(x^{*})}\big{(}\overline{\mathbf{x}}^{K}\big{)}=\max\limits_{u \in B_{R}(x^{*})}\big{\langle}F(\mathbf{u}),\overline{\mathbf{x}}^{K}-\mathbf{u}\big{\rangle}\), \(\overline{\mathbf{x}}^{K}=\frac{1}{K}\sum\limits_{k=0}^{K-1}\widetilde{\mathbf{x}}^{k}\) and \(R\leq\|\mathbf{x}^{0}-\mathbf{x}^{*}\|\)._

Proof.: We start the proof with the result of Lemma E.2

\[2\gamma_{2}\big{\langle}\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{u}\big{\rangle} \leq \big{\|}\mathbf{x}^{k}-\mathbf{u}\big{\|}^{2}-\big{\|}\mathbf{x}^{k+1}-\mathbf{u} \big{\|}^{2}-2\gamma_{2}\big{\langle}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^ {k}),\mathbf{x}^{k}-\mathbf{u}\big{\rangle}\] \[+2\gamma_{2}^{2}\big{\|}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^ {k})\big{\|}^{2}+4\gamma_{1}\gamma_{2}\big{\|}\overline{\mathbf{g}}_{\mathbf{\eta}^{k}} (\widetilde{\mathbf{x}}^{k})-F(\widetilde{\mathbf{x}}^{k})\big{\|}^{2}\] \[+4\gamma_{1}\gamma_{2}\big{\|}F(\mathbf{x}^{k})-\overline{\mathbf{g}}_{ \mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}+4\gamma_{1}\gamma_{2}\big{\|}\overline{ \mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})-\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k}) \big{\|}^{2},\]

that leads to the following inequality

\[2\gamma_{2}\big{\langle}F(\widetilde{\mathbf{x}}^{k}),\widetilde{\mathbf{ x}}^{k}-\mathbf{u}\big{\rangle}\] \[\leq \big{\|}\mathbf{x}^{k}-\mathbf{u}\big{\|}^{2}-\big{\|}\mathbf{x}^{k+1}-\mathbf{u} \big{\|}^{2}+2\gamma_{2}^{2}\big{\|}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^ {k})\big{\|}^{2}\] \[+2\gamma_{2}\big{\langle}F(\widetilde{\mathbf{x}}^{k})-\overline{\mathbf{ g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{u}\big{\rangle}-2 \gamma_{2}\big{\langle}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k} )-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\mathbf{x}^{k}-\mathbf{u} \big{\rangle}\] \[+4\gamma_{1}\gamma_{2}\Big{(}\big{\|}\overline{\mathbf{g}}_{\mathbf{\eta} ^{k}}(\widetilde{\mathbf{x}}^{k})-F(\widetilde{\mathbf{x}}^{k})\big{\|}^{2}+\big{\|}F( \mathbf{x}^{k})-\overline{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}+\big{\|} \overline{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})-\widehat{\mathbf{g}}_{\mathbf{\xi}^{k}}( \mathbf{x}^{k})\big{\|}^{2}\Big{)}.\]

Assumption 5 implies that

\[\big{\langle}F(\mathbf{u}),\widetilde{\mathbf{x}}^{k}-\mathbf{u}\big{\rangle}\leq\big{\langle} F(\widetilde{\mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{u}\big{\rangle} \tag{78}\]

and consequently by Jensen inequality

\[2\gamma_{2}K\big{\langle}F(\mathbf{u}),\overline{\mathbf{x}}^{K}-\mathbf{u} \big{\rangle}\] \[\leq \big{\|}\mathbf{x}^{0}-\mathbf{u}\big{\|}^{2}+2\gamma_{2}^{2}\sum_{k=0}^ {K-1}\big{\|}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})- \overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})\big{\|}^{2}\] \[+2\gamma_{2}\sum_{k=0}^{K-1}\big{(}\big{\langle}F(\widetilde{\bm {x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\widetilde{ \mathbf{x}}^{k}-\mathbf{u}\big{\rangle}-\big{\langle}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}), \mathbf{x}^{k}-\mathbf{u}\big{\rangle}\big{)}\] \[+4\gamma_{1}\gamma_{2}\sum_{k=0}^{K-1}\Big{(}\big{\|}\overline{ \mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})-F(\widetilde{\mathbf{x}}^{k})\big{\|}^{2 }+\big{\|}F(\mathbf{x}^{k})-\overline{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}+ \big{\|}\overline{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})-\widehat{\mathbf{g}}_{\mathbf{\xi} ^{k}}(\mathbf{x}^{k})\big{\|}^{2}\Big{)},\]

where \(\overline{\mathbf{x}}^{K}=\frac{1}{K}\sum\limits_{k=0}^{K-1}\widetilde{\mathbf{x}}^{k}\).

Then maximization in \(\mathbf{u}\) gives

\[2\gamma_{2}K\mathsf{Gap}_{B_{R}(x^{*})}\big{(}\overline{\mathbf{x}}^{ K}\big{)}\] \[\leq \max\limits_{u\in B_{R}(x^{*})}\big{\|}\mathbf{x}^{0}-\mathbf{u}\big{\|}^{2 }+2\gamma_{2}^{2}\sum_{k=0}^{K-1}\big{\|}\widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}) \big{\|}^{2}\] \[+4\gamma_{1}\gamma_{2}\sum_{k=0}^{K-1}\Big{(}\big{\|}\overline{ \mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})-F(\widetilde{\mathbf{x}}^{k})\big{\|}^{2 }+\big{\|}F(\mathbf{x}^{k})-\overline{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})\big{\|}^{2}+ \big{\|}\overline{\mathbf{g}}_{\mathbf{\xi}^{k}}(\mathbf{x}^{k})-\widehat{\mathbf{g}}_{\mathbf{\xi}^{ k}}(\mathbf{x}^{k})\big{\|}^{2}\Big{)}\] \[+2\gamma_{2}\max\limits_{u\in B_{R}(x^{*})}\sum_{k=0}^{K-1} \big{\langle}F(\widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{u}\big{\rangle}\] \[+2\gamma_{2}\max\limits_{u\in B_{R}(x^{*})}\sum_{k=0}^{K-1} \big{\langle}\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})- \widehat{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}),\mathbf{x}^{k}-\mathbf{u} \big{\rangle}.\]

[MISSING_PAGE_EMPTY:57]

we have

\[2\gamma_{2}\mathbb{E}\left[\max_{\mathbf{u}\in B_{R}(x^{*})}\sum_{k=0}^{ K-1}\langle F(\widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{u}\rangle\right]\] \[= 2\gamma_{2}\mathbb{E}\left[\sum_{k=0}^{K-1}\langle F(\widetilde{ \mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}), \widetilde{\mathbf{x}}^{k}\rangle\right]\] \[+2\gamma_{2}\mathbb{E}\left[\max_{\mathbf{u}\in B_{R}(x^{*})}\sum_{k=0 }^{K-1}\langle F(\widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k}),-\mathbf{u}\rangle\right]\] \[= 2\gamma_{2}\mathbb{E}\left[\max_{\mathbf{u}\in B_{R}(x^{*})}\sum_{k= 0}^{K-1}\langle F(\widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k}),-\mathbf{u}\rangle\right]\] \[= 2\gamma_{2}\mathbb{E}\left[\sum_{\mathbf{u}\in B_{R}(x^{*})}\left\langle \frac{1}{K}\sum_{k=0}^{K-1}(F(\widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{ \eta}^{k}}(\widetilde{\mathbf{x}}^{k})),\mathbf{x}^{0}-\mathbf{u}\right\rangle\right]\] \[\stackrel{{\eqref{eq:2}}}{{\leq}} 2\gamma_{2}K\mathbb{E}\left[\max_{\mathbf{u}\in B_{R}(x^{*})}\left\{ \frac{\gamma_{2}}{2}\left\|\frac{1}{K}\sum_{k=0}^{K-1}(F(\widetilde{\mathbf{x}}^{k })-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k}))\right\|^{2}+ \frac{1}{2\gamma_{2}}\|\mathbf{x}^{0}-\mathbf{u}\|^{2}\right\}\right]\] \[= \gamma_{2}^{2}\mathbb{E}\left[\left\|\sum_{k=0}^{K-1}(F( \widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^ {k}))\right\|^{2}\right]+\max_{\mathbf{u}\in B_{R}(x^{*})}\|\mathbf{x}^{0}-\mathbf{u}\|^{2}.\]

We notice that \(\mathbb{E}[F(\widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k})\mid F(\widetilde{\mathbf{x}}^{0})-\overline{\mathbf{g}}_{\mathbf{ \eta}^{0}}(\widetilde{\mathbf{x}}^{0}),\ldots,F(\widetilde{\mathbf{x}}^{k-1})-\overline {\mathbf{g}}_{\mathbf{\eta}^{k-1}}(\widetilde{\mathbf{x}}^{k-1})]=0\) for all \(k\geq 1\), i.e., conditions of Lemma C.2 are satisfied. Therefore, applying Lemma C.2, we get

\[2\gamma_{2}\mathbb{E}\left[\max_{\mathbf{u}\in B_{R}(x^{*})}\sum_{k= 0}^{K-1}\langle F(\widetilde{\mathbf{x}}^{k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}( \widetilde{\mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{u}\rangle\right] \tag{83}\] \[\leq \gamma_{2}^{2}\sum_{k=0}^{K-1}\mathbb{E}[\|F(\widetilde{\mathbf{x}}^ {k})-\overline{\mathbf{g}}_{\mathbf{\eta}^{k}}(\widetilde{\mathbf{x}}^{k})\|^{2}]\] (84) \[+\max_{\mathbf{u}\in B_{R}(x^{*})}\|\mathbf{x}^{0}-\mathbf{u}\|^{2}\] \[\leq \frac{\gamma_{2}^{2}K\sigma^{2}}{n-2B-m}+\max_{\mathbf{u}\in B_{R}(x^ {*})}\|\mathbf{x}^{0}-\mathbf{u}\|^{2}\] (86) \[\stackrel{{\eqref{eq:2},\eqref{eq:2}}}{{\leq}} \frac{9}{8}R^{2}. \tag{87}\]

Assembling the above results together gives

\[2\gamma_{2}K\mathbb{E}\mathsf{Gap}_{B_{R}(x^{*})}\big{(}\overline{\mathbf{x}}^{K} \big{)}\leq\frac{R^{2}}{32}+\frac{R^{2}}{5}+\frac{R^{2}}{4}+\frac{3}{4}R^{2}+ \frac{9}{8}R^{2}\leq 3R^{2}. \tag{88}\]

**Corollary 10**.: _Let assumptions of Theorem 12 hold. Then \(\mathbb{E}\Big{[}\sigma\mathsf{ap}_{B_{R}(x^{*})}\big{(}\overline{\mathbf{x}}^{K} \big{)}\Big{]}\leq\varepsilon\) holds after_

\[K=\Theta\bigg{(}\frac{LR^{2}}{\varepsilon}+\frac{\sigma^{2}R^{2}}{n \varepsilon^{2}}+\frac{\sigma n^{2}R}{m\varepsilon}\bigg{)}\]

_iterations of \(\mathsf{SEG-CC}\)._Proof.: \[\mathbb{E}\Big{[}\texttt{Gap}_{B_{R}(x^{*})}\big{(}\overline{\mathbf{x}}^ {K}\big{)}\Big{]}\leq\frac{3R^{2}}{2\gamma_{2}K} \leq \frac{3R^{2}}{2K}\Bigg{(}4L+\sqrt{\frac{64\sigma^{2}K}{(n-2B-m)R^{ 2}}}+\sqrt{\frac{64\rho^{2}B^{2}n^{2}}{m^{2}R^{2}}}\Bigg{)}\] \[\leq \frac{6R^{2}}{K}+\sqrt{\frac{144\sigma^{2}R^{2}}{(n-2B-m)K}}+ \frac{12\rho BnR}{mK}\]

Let us chose \(K\) such that each of the last three terms less or equal \(\nicefrac{{\varepsilon}}{{3}}\), then

\[K=\max\biggl{(}\frac{18LR^{2}}{\varepsilon},\frac{144\cdot 9\sigma^{2}R^{2}}{(n -2B-m)\varepsilon^{2}},\frac{36\rho BnR}{m\varepsilon}\biggr{)},\]

where \(\rho^{2}=q\sigma^{2}\) with \(q=2C^{2}+12+\frac{12}{n-2B-m}\) and \(C=\Theta(1)\) by Lemma E.1. The latter implies that

\[\mathbb{E}\Big{[}\texttt{Gap}_{B_{R}(x^{*})}\big{(}\overline{\mathbf{x}}^{K} \big{)}\Big{]}\leq\varepsilon.\]

Using the definition of \(\rho\) from Lemma E.1 and if \(B\leq\frac{n}{4}\), \(m<<n\) the bound for \(K\) can be easily derived. 

### Proofs for R-Seg-Cc

#### e.4.1 Quasi Strongly Monotone Case

```
0:\(\mathbf{x}^{0}\) - starting point, \(r\) - number of restarts, \(\{\gamma_{t}\}_{t=1}^{r}\) - stepsizes for SEG-CC (Alg. 7), \(\{K_{t}\}_{t=1}^{r}\) - number of iterations for SEG-CC (Alg. 7),
1:\(\widehat{\mathbf{x}}^{0}=\mathbf{x}^{0}\)
2:for\(t=1,2,\ldots,r\)do
3: Run SEG-CC (Alg. 7) for \(K_{t}\) iterations with stepsize \(\gamma_{t}\), starting point \(\widehat{\mathbf{x}}^{t-1}\),
4: Define \(\widehat{\mathbf{x}}^{t}\) as \(\widehat{\mathbf{x}}^{t}=\frac{1}{K_{t}}\sum\limits_{k=0}^{K_{t}}\mathbf{x}^{k,t}\), where \(\mathbf{x}^{0,t},\mathbf{x}^{1,t},\ldots,\mathbf{x}^{K_{t},t}\) are the iterates produced by SEG-CC.
5:endfor
5:\(\widehat{\mathbf{x}}^{r}\)
```

**Algorithm 8** R-SEG-CC

**Theorem** (Theorem 7 duplicate).: _Let Assumptions 1, 3, 4 hold. Then, after \(r=\left\lceil\log_{2}\frac{R^{2}}{\varepsilon}\right\rceil-1\) restarts R-SEG-CC (Algorithm 8) with \(\gamma_{1_{t}}=\min\left\{\frac{1}{2L},\sqrt{\frac{(G-B-m)R^{2}}{16\sigma^{2} ^{2}K_{t}}},\sqrt{\frac{mR^{2}}{8q\sigma^{2}2^{2}Bn}}\right\}\), \(\gamma_{2_{t}}=\min\left\{\frac{1}{4L},\sqrt{\frac{m^{2}R^{2}}{64q^{2}2^{2}B^{2} n^{2}}},\sqrt{\frac{(G-B-m)R^{2}}{64q^{2}K_{t}}}\right\}\) and \(K_{t}=\left\lceil\max\left\{\frac{8L}{\mu},\frac{16n\sigma B\sqrt{q^{2t}}}{m \mu R},\frac{256q^{2}t^{2}}{(G-B-m)\mu^{2}R^{2}}\right\}\right\rceil\), where \(R\geq\left\|\mathbf{x}^{0}-\mathbf{x}^{*}\right\|\) outputs \(\widehat{\mathbf{x}}^{r}\) such that \(\mathbb{E}\|\widehat{\mathbf{x}}^{r}-\mathbf{x}^{*}\|^{2}\leq\varepsilon\). Moreover, the total number of executed iterations of SEG-CC is_

\[\sum\limits_{t=1}^{r}K_{t}=\Theta\left(\frac{\ell}{\mu}\log\frac{\mu R_{0}^{2}} {\varepsilon}+\frac{\sigma^{2}}{(n-2B-m)\mu\varepsilon}+\frac{nB\sigma}{m \sqrt{\mu\varepsilon}}\right). \tag{89}\]

Proof of Theorem 7.: \(\overline{\mathbf{x}}^{K}=\frac{1}{K}\sum\limits_{k=0}^{K-1}\widehat{\mathbf{x}}^{k}\)

\[\mu\mathbb{E}\Big{[}\big{\|}\overline{\mathbf{x}}^{K}-\mathbf{x}^{*} \big{\|}^{2}\Big{]} = \mu\mathbb{E}\Bigg{[}\Bigg{\|}\frac{1}{K}\sum\limits_{k=0}^{K-1} \big{(}\widetilde{\mathbf{x}}^{k}-\mathbf{x}^{*}\big{)}\Bigg{\|}^{2}\Bigg{]}\leq\mu \mathbb{E}\Bigg{[}\frac{1}{K}\sum\limits_{k=0}^{K-1}\big{\|}\widetilde{\mathbf{x}}^ {k}-\mathbf{x}^{*}\big{\|}^{2}\Bigg{]}\] \[= \frac{\mu}{K}\sum\limits_{k=0}^{K-1}\mathbb{E}\Big{[}\big{\|} \widetilde{\mathbf{x}}^{k}-\mathbf{x}^{*}\big{\|}^{2}\Big{]}\overset{\text{(\text{ \text{\text{OSM}}})}}{\leq}\frac{1}{K}\sum\limits_{k=0}^{K-1}\mathbb{E}\big{[} \langle F(\widetilde{\mathbf{x}}^{k}),\widetilde{\mathbf{x}}^{k}-\mathbf{x}^{*}\rangle \big{]}.\]Theorem 11 implies that \(\mathsf{SEG-CC}\) with

\[\gamma_{1} = \min\left\{\frac{1}{2L},\sqrt{\frac{(n-2B-m)R^{2}}{16\sigma^{2}K}}, \sqrt{\frac{mR^{2}}{8\rho^{2}Bn}}\right\},\] \[\gamma_{2} = \min\left\{\frac{1}{4L},\sqrt{\frac{m^{2}R^{2}}{64\rho^{2}B^{2}n^ {2}}},\sqrt{\frac{(n-2B-m)R^{2}}{64\sigma^{2}K}}\right\},\]

guarantees

\[\mu\mathbb{E}\Big{[}\|\mathbf{\overline{x}}^{K}-\mathbf{x}^{*}\|^{2}\Big{]}\leq\frac{R_ {0}^{2}}{\gamma_{2}K}\]

after \(K\) iterations.

After the first restart we have

\[\mathbb{E}\Big{[}\|\mathbf{\widehat{x}}^{1}-\mathbf{x}^{*}\|^{2}\Big{]}\leq\frac{R_{0} ^{2}}{\mu\gamma_{2_{1}}K_{1}}\leq\frac{R_{0}^{2}}{2},\]

since \(\mu\gamma_{2_{1}}K_{1}\geq 2\).

Next, assume that we have \(\mathbb{E}[\|\mathbf{\widehat{x}}^{t}-\mathbf{x}^{*}\|^{2}]\leq\frac{R_{0}^{2}}{2^{t}}\) for some \(t\leq r-1\). Then, Theorem 11 implies that

\[\mathbb{E}\Big{[}\|\mathbf{\widehat{x}}^{t+1}-\mathbf{x}^{*}\|^{2}\mid\mathbf{\widehat{x}} ^{t}\Big{]}\leq\frac{\|\mathbf{\widehat{x}}^{t}-\mathbf{x}^{*}\|^{2}}{\mu\gamma_{2_{t} }K_{t}}.\]

Taking the full expectation from the both sides of previous inequality we get

\[\mathbb{E}\Big{[}\|\mathbf{\widehat{x}}^{t+1}-\mathbf{x}^{*}\|^{2}\Big{]}\leq\frac{ \mathbb{E}[\|\mathbf{\widehat{x}}^{t}-\mathbf{x}^{*}\|^{2}]}{\mu\gamma_{2_{t}}K_{t}} \leq\frac{R_{0}^{2}}{2^{t}\mu\gamma_{2_{t}}K_{t}}\leq\frac{R_{0}^{2}}{2^{t+1}}.\]

Therefore, by mathematical induction we have that for all \(t=1,\ldots,r\)

\[\mathbb{E}\left[\|\mathbf{\widehat{x}}^{t}-\mathbf{x}^{*}\|^{2}\right]\leq\frac{R_{0} ^{2}}{2^{t}}.\]

Then, after \(r=\left\lceil\log_{2}\frac{R_{0}^{2}}{\varepsilon}\right\rceil-1\) restarts of \(\mathsf{SEG-CC}\) we have \(\mathbb{E}\left[\|\mathbf{\widehat{x}}^{r}-\mathbf{x}^{*}\|^{2}\right]\leq\varepsilon\). The total number of iterations executed by \(\mathsf{SEG-CC}\) is

\[\sum_{t=1}^{r}K_{t} = \Theta\left(\sum_{t=1}^{r}\max\left\{\frac{L}{\mu},\frac{\sigma^ {2}2^{t}}{(n-2B-m)\mu^{2}R_{0}^{2}},\frac{nB\rho^{2}\frac{t}{2}}{m\mu R_{0}} \right\}\right)\] \[= \Theta\left(\frac{L}{\mu}r+\frac{\sigma^{2}2^{r}}{(n-2B-m)\mu^{2 }R_{0}^{2}}+\frac{nB\rho^{2}\frac{t}{2}}{m\mu R_{0}}\right)\] \[= \Theta\left(\frac{L}{\mu}\log\frac{\mu R_{0}^{2}}{\varepsilon}+ \frac{\sigma^{2}}{(n-2B-m)\mu^{2}R_{0}^{2}}\cdot\frac{\mu R_{0}^{2}}{ \varepsilon}+\frac{nB\rho}{m\mu R_{0}}\cdot\sqrt{\frac{\mu R_{0}^{2}}{ \varepsilon}}\right)\] \[= \Theta\left(\frac{L}{\mu}\log\frac{\mu R_{0}^{2}}{\varepsilon}+ \frac{\sigma^{2}}{(n-2B-m)\mu\varepsilon}+\frac{nB\rho}{m\sqrt{\mu\varepsilon }}\right),\]

that together with \(\rho^{2}=q\sigma^{2}\) with \(q=2C^{2}+12+\frac{12}{n-2B-m}\) and \(C=\Theta(1)\) given by Lemma E.1 implies the result of the theorem. 

**Corollary 11**.: _Let assumptions of 7 hold. Then \(\mathbb{E}\left[\|\mathbf{\widehat{x}}^{r}-\mathbf{x}^{*}\|^{2}\right]\leq\varepsilon\) holds after_

\[\sum_{t=1}^{r}K_{t}=\Theta\left(\frac{L}{\mu}\log\frac{\mu R^{2}}{\varepsilon} +\frac{\sigma^{2}}{n\mu\varepsilon}+\frac{n^{2}\sigma}{m\sqrt{\mu\varepsilon }}\right) \tag{90}\]

_iterations of \(\mathsf{SEG-CC}\)._

Proof.: Using the definition of \(\rho\) from Lemma E.1 and if \(B\leq\frac{n}{4}\), \(m<<n\) the bound for \(\sum\limits_{t=1}^{r}K_{t}\) can be easily derived.

[MISSING_PAGE_FAIL:61]

Figure 4: Distance to the solution BF attack with various batchsizes.

Figure 3: Distance to the solution ALIE attack with various of parameter values and batchsizes.

Figure 5: Distance to the solution under IPM attack with various parameter values and batchsizes.

Checks of Computations.Next, using the same setup, we compare M-SGDA-RA, which showed the best performance in the above experiments, with methods that check computations. With checks of computation the best strategy for attackers is that at each iteration only one peer attacks, since it maximizes the expected number of rounds with the presence of actively malicious peers. So the comparison in this paragraph is performed in this setup.

Figure 6: Distance to the solution under RN attack with various parameter values and batchsizes.

Figure 8: Distance to the solution under BF attack with various batchsizes.

Figure 7: Distance to the solution under ALIE attack with various parameter values and batchsizes.

Figure 9: Distance to the solution under IPM attack with various parameter values and batchsizes.

Learning rates.We conducted extra experiments to show the dependence on different learning rate values \(1e-5,2e-5,5e-6\).

Figure 11: Distance to the solution under various attacks, batchsizes (bs) and learning rates (lr).

Figure 10: Distance to the solution under RN attack with various parameter values and batchsizes.

### Generative Adversarial Networks

One of the most well-known frameworks in which the objective function is a variational inequality is generative adversarial networks (GAN) Goodfellow et al. (2014). In the simplest case of this setting, we have a generator \(G:\mathbb{R}^{z}\rightarrow\mathbb{R}^{d}\) and a discriminator \(D:\mathbb{R}^{d}\rightarrow\mathbb{R}\), where \(z\) denotes the dimension of the latent space. The objective function can be written as

\[\min_{G}\max_{D}\quad\mathop{\mathbb{E}}_{x}\log(D(x))+\mathop{\mathbb{E}}_{z} \log(1-D(G(z))). \tag{91}\]

Here, it is understood that \(D\) and \(G\) are modeled as neural nets and can be optimized in the distributed setting with gradient descent ascent algorithms. However, due to the complexity of the GANs framework, tricks and adjustments are being employed to ensure good results, such as the Wasserstein GAN formulation Gulrajani et al. (2017) with Lipschitz constraint on \(D\) and the spectral normalization Miyato et al. (2018) trick to ensure the Lipschitzness of \(D\). The discriminator can thus benefit in practice from multiple gradient ascent steps per gradient descent step on the generator. In addition, Adam Kingma and Ba (2014) is often used for GANs as they can be very slow to converge and not perform as well with vanilla SGD.

Therefore, in our implementation of GANs in the distributed setting, we employ all of these techniques and show improvements when we add checks of gradient computations to the server. As for the gradients in our implementation, we can think of the accumulated Adam steps within the clients as "generalized gradients" and aggregate them in the server with checks of computations (by rewinding model and optimizer state). We tried aggregation after each descent or ascent step, after full descent-ascent step, and after multiple descent-ascent steps. For the first case, we found that GANs converge much more slowly. For the third case, the performance is better but checks of computations take more time. Thus, we choose to report the performance for the second case: aggregations of a full descent-ascent step. Though, we note that experiments for the other cases suggest similar improvements.

The dataset we chose for this experiment is CIFAR-10 Krizhevsky et al. (2009) because it is more realistic than MNIST yet is still tractable to simulate in the distributed setting. We let \(n=10\), \(B=2\), and choose a learning rate of 0.001, \(\beta_{1}=0.5\), and \(\beta_{2}=0.9\) with a batch size of 64. We run the algorithms for 4600 epochs. We could not average across runs as the simulation is very compute intensive and the benefits are obvious. We compare SGDA-RA (RFA with bucket size 2) and SGDA-CC under the following byzantine attacks: i) no attack (NA), ii) label flipping (LF), iii) inner product manipulation (IPM) Xie et al. (2019), and iv) a little is enough (ALIE) Baruch et al. (2019). The architecture of the GAN follows Miyato et al. (2018).

We show the results in Figure 12. The improvements are most significant for the ALIE attack. Even when no attacks are present, checks of computations only slightly affects convergence speed. This experiment should further justify our proposed algorithm and its real-world benefits even for a setting as complex as distributed GANs. Code for GANs is available at [https://github.com/zeligism/vi-robust-agg](https://github.com/zeligism/vi-robust-agg).

Figure 12: Comparison of FID to CIFAR-10 per epoch between SGDA-CC and SGDA-RA. The FID is calculated on 50,000 samples. (lower = better).