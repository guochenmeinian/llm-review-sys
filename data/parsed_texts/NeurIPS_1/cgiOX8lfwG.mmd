# Discretely Beyond \(1/e\): Guided Combinatorial Algorithms for Submodular Maximization

Yixin Chen, Ankur Nath, Chunli Peng, Alan Kuhnle

Department of Computer Science & Engineering

Texas A&M University

College Station, TX

{chen777, anath, chunli.peng, kuhnle}@tamu.edu

###### Abstract

For constrained, not necessarily monotone submodular maximization, all known approximation algorithms with ratio greater than \(1/e\) require continuous ideas, such as queries to the multilinear extension of a submodular function and its gradient, which are typically expensive to simulate with the original set function. For combinatorial algorithms, the best known approximation ratios for both size and matroid constraint are obtained by a simple randomized greedy algorithm of Buchbinder et al. [9]: \(1/e\approx 0.367\) for size constraint and \(0.281\) for the matroid constraint in \(\mathcal{O}(kn)\) queries, where \(k\) is the rank of the matroid. In this work, we develop the first combinatorial algorithms to break the \(1/e\) barrier: we obtain approximation ratio of \(0.385\) in \(\mathcal{O}(kn)\) queries to the submodular set function for size constraint, and \(0.305\) for a general matroid constraint. These are achieved by guiding the randomized greedy algorithm with a fast local search algorithm. Further, we develop deterministic versions of these algorithms, maintaining the same ratio and asymptotic time complexity. Finally, we develop a deterministic, nearly linear time algorithm with ratio \(0.377\).

## 1 Introduction

A nonnegative set function \(f:2^{\mathcal{U}}\rightarrow\mathbb{R}^{+}\) is _submodular_ iff for all \(S\subseteq T\subseteq\mathcal{U}\), \(x\in\mathcal{U}\setminus T\), \(f\left(S\cup\{x\}\right)-f\left(S\right)\geq f\left(T\cup\{x\}\right)-f\left(T\right)\); and \(f\) is monotone iff \(f\left(S\right)\leq f\left(T\right)\) for all \(S\subseteq T\subseteq\mathcal{U}\). Submodular optimization plays an important role in data science and machine learning [3], particularly in tasks that involve selecting a representative subset of data or features. Its diminishing returns property makes it ideal for scenarios where the incremental benefit of adding an element to a set decreases as the set grows. Applications include sensor placement for environmental monitoring [19, 27], where the goal is to maximize coverage with limited sensors, feature selection [22, 18, 2] in machine learning to improve model performance and reduce overfitting, and data summarization [23, 31] for creating concise and informative summaries of large datasets. Further, many of these applications employ submodular objective functions that are non-monotone, _e.g._ Mirzasoleiman et al. [23], Tschiatschek et al. [31]. Formally, we study the optimization problem (SM): \(\max f\left(S\right),\)_s.t._\(S\in\mathcal{I}\), where \(f\) is nonnegative, submodular and not necessarily monotone; and \(\mathcal{I}\subseteq 2^{\mathcal{U}}\) is a family of feasible subsets. Specifically, we consider two cases: when \(\mathcal{I}\) is a size constraint (all sets of size at most \(k\)); and more generally, when \(\mathcal{I}\) is an arbitrary matroid of rank \(k\).

In this field, algorithms typically assume access to a _value oracle_ for the submodular function \(f\), and the efficiency of an algorithm is measured by the number of queries to the oracle, because evaluation of the submodular function is typically expensive and dominates other parts of the computation. In the general, not necessarily monotone case, the approximability of constrained submodular optimization in the value oracle model is not well understood. For several years, \(1/e\approx 0.367\) was conjecturedto be the best ratio, as this ratio is obtained by the measured continuous greedy [15] algorithm that also gets the \(1-1/e\) ratio in the monotone setting, which is known to be optimal [24]. However, in several landmark works, the \(1/e\) barrier was broken: first to \(0.371\) by Buchbinder et al. [9] (for size constraint only) and subsequently to \(0.372\) by Ene and Nguyen [13], then \(0.385\) by Buchbinder and Feldman [6]. Very recently, the best known approximation factor has been improved to 0.401 [7]. On the other hand, the best hardness result is \(0.478\)[16; 28].

All of the algorithms improving on the \(1/e\) ratio use oracle queries to the _multilinear extension_ of a submodular function and its gradient. The multilinear extension relaxes the submodular set function to allow choosing an element with probability in \([0,1]\). Although this is a powerful technique, the multilinear extension must be approximated by polynomially many random samples of the original set function oracle. Unfortunately, this leads to a high query complexity for these algorithms, which we term _continuous algorithms_; typically, the query complexity to the original submodular function is left uncomputed. As an illustration, we compute in Appendix B that the continuous algorithm of Buchbinder and Feldman [6] achieves ratio of \(0.385\) with query complexity of \(\mathcal{O}\left(n^{11}\log(n)\right)\) to the set function oracle. Consequently, these algorithms are of mostly theoretical interest - the time cost of running on tiny instances (say, \(n<100\)) is already prohibitive, as demonstrated by Chen and Kuhnle [12] where a continuous algorithm required more than \(10^{9}\) queries to the set function on an instance with \(n=87,k=10\).

For size and matroid constraints, the current state-of-the-art approximation ratio for a combinatorial algorithm (_i.e._ not continuous) is obtained by the RandomGreedy algorithm (Algorithm 1) of Buchbinder et al. [9]. RandomGreedy achieves ratio \(1/e\approx 0.367\) for size constraint, and \(0.283-\varepsilon\) ratio for matroid constraint; its query complexity is \(\mathcal{O}\left(kn\right)\). Thus, there is no known combinatorial algorithm that breaks the \(1/e\) barrier; and therefore, no such algorithm is available to be used in practice on any of the applications of SM described above.

Moreover, closing the gap between ratios achieved by deterministic and randomized algorithms for SM has been the focus of a number of recent works [5; 17; 11; 8]. In addition to theoretical interest, deterministic algorithms are desirable in practice, as a ratio that holds in expectation may fail on any given run with constant probability. Buchbinder and Feldman [5] introduced a linear programming method to derandomize the RandomGreedy algorithm (at the expense of additional time complexity), meaning that the best known ratios for deterministic algorithms are again given by RandomGreedy. There is no known method to derandomize continuous

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Constraint & Reference & Query & Ratio & Type \\ \hline \multirow{5}{*}{Size} & Buchbinder and Feldman [5] & \(\mathcal{O}\left(k^{3}n\right)\) & \(1/e\approx 0.367\) & Det \\  & Buchbinder et al. [9] & \(\mathcal{O}\left(kn\right)\) & \(1/e\) & Cmb \\  & Buchbinder and Feldman [7] & poly(\(n\)) & \(0.401\) & Cts \\ \cline{2-5}  & Algorithm 2 & \(\mathcal{O}\left(kn/\varepsilon\right)\) & \(0.385-\varepsilon\) & Cmb \\  & Algorithm 11 & \(\mathcal{O}\left(kn\left(\frac{10}{9\varepsilon}\right)^{\frac{30}{2\varepsilon }-1}\right)\) & \(0.385-\varepsilon\) & Det \\  & Algorithm 14 & \(\mathcal{O}\left(\log(k)n\left(\frac{10}{3\varepsilon}\right)^{\frac{30}{2 \varepsilon}}\left(\frac{10}{\varepsilon}\right)^{\frac{10}{1}-1}\right)\) & \(0.377-\varepsilon\) & Det \\ \hline \multirow{5}{*}{Matroid} & Sun et al. [30] & \(\mathcal{O}\left(k^{2}n^{2}\right)\) & \(0.283-\mathcal{O}\left(\frac{1}{k^{2}}\right)\) & Det \\  & Buchbinder et al. [9] & \(\mathcal{O}\left(kn\right)\) & \(0.283-\varepsilon\) & Cmb \\  & Buchbinder and Feldman [7] & poly(\(n\)) & \(0.401\) & Cts \\ \cline{2-5}  & Algorithm 2 & \(\mathcal{O}\left(kn/\varepsilon\right)\) & \(0.305-\varepsilon\) & Cmb \\  & Algorithm 11 & \(\mathcal{O}\left(kn\left(\frac{10}{9\varepsilon}\right)^{\frac{30}{2\varepsilon }-1}\right)\) & \(0.305-\varepsilon\) & Det \\ \hline \hline \end{tabular}
\end{table}
Table 1: The prior state-of-the-art and the ratios achieved in this paper, in each category: deterministic (det), randomized combinatorial (cmb), and continuous (cts).

algorithms, as the only known way to approximate the multilinear extension of a general submodular set function relies on random sampling methods. Ozcan et al. [26], however, introduced a deterministic estimation via Taylor series approximation, but this approach is limited to a specific class of submodular functions that can be expressed as weighted compositions of analytic and multilinear functions. Therefore, there is no known deterministic algorithm that breaks the \(1/e\) barrier. The best known ratio in each category of continuous, combinatorial, and deterministic algorithms is summarized in Table 1. In this work, we consider the following questions:

_Can combinatorial algorithms, and separately, deterministic algorithms, obtain approximation ratios for SM beyond \(1/e\)? If so, are the resulting algorithms practical and do they yield empirical improvements in objective value over existing algorithms?_

### Contributions

In this work, we improve the best known ratio for a combinatorial algorithm for size-constrained SM to \(0.385-\varepsilon\approx 1/e+0.018\). This is achieved by using the result of a novel local search algorithm to guide the RandomGreedy algorithm. Overall, we obtain query complexity of \(\mathcal{O}\left(kn/\varepsilon\right)\), which is at worst quadratic in the size of the ground set, since \(k\leq n\). Thus, this algorithm is practical and can run on moderate instance sizes; the first algorithm with ratio beyond \(1/e\) for which this is possible. Further, we extend this algorithm to the matroid constraint, where it improves the best known ratio of a combinatorial algorithm for a general matroid constraint from \(0.283\) of RandomGreedy to \(0.305-\varepsilon\).

Secondly, we obtain these same approximation ratios with deterministic algorithms. The ideas are similar to the randomized case, except we leverage a recently formulated algorithm InterpolatedGreedy[11] as a replacement for guided RandomGreedy. The analysis of InterpolatedGreedy has similar recurrences (up to low order terms) and the algorithm can be guided in a similar fashion to RandomGreedy, but is amenable to derandomization. The derandomization only adds a constant factor, albeit one that is exponential in \((1/\varepsilon)\).

Next, we seek to lower the query complexity further, while still improving the \(1/e\) ratio. As InterpolatedGreedy can be sped up to \(\mathcal{O}_{\varepsilon}(n\log k)\), the bottleneck becomes the local search procedure. Thus, we develop a faster way to produce the guiding set \(Z\) by exploiting a run of (unguided) InterpolatedGreedy and demonstrating that a decent guiding set is produced if the algorithm exhibits nearly worst-case behavior. With this method, we achieve a deterministic algorithm with ratio \(0.377\approx 1/e+0.01\) in \(\mathcal{O}_{\varepsilon}(n\log k)\) query complexity, which is nearly linear in the size of the ground set (since \(k=O(n)\)).

Finally, we demonstrate the practical utility of our combinatorial \(0.385\)-approximation algorithm by implementing it and evaluating in the context of two applications of size-constrained SM on moderate instance sizes (up to \(n=10^{4}\)). We evaluate it with parameters set to enforce a ratio \(>1/e\). It outperforms both the standard greedy algorithm and RandomGreedy by a significant margin in terms of objective value; moreover, it uses about twice the queries of RandomGreedy and is orders of magnitude faster than existing local search algorithms.

### Additional Related Work

**Derandomization.** Buchbinder and Feldman [5] introduced a linear programming (LP) method to derandomize the RandomGreedy algorithm, thereby obtaining ratio \(1/e\) with a deterministic algorithm. Further, Sun et al. [30] were able to apply this technique to RandomGreedy for matroids. A disadvantage of this approach is an increase in the query complexity over the original randomized algorithm. Moreover, we attempted to use this method to derandomize our guided RandomGreedy algorithm, but were unsuccessful. Instead, we obtained our deterministic algorithms by guiding the InterpolatedGreedy algorithm instead of RandomGreedy; this algorithm is easier to derandomize, notably without increasing the asymptotic query complexity.

**Relationship to Buchbinder and Feldman [6].** The continuous, \(0.385\)-approximation algorithm of Buchbinder and Feldman [6] guides the measured continuous greedy algorithm using the output of a continuous local search algorithm, in analogous fashion to how we guide RandomGreedy with the output of a combinatorial local search. However, the analysis of RandomGreedy is much different from that of measured continuous greedy, although the resulting approximation factor is the same.

Specifically, Buchbinder and Feldman [6] obtain their ratio by optimizing a linear program mixing the continous local search and guided measured continous greedy; in contrast, we use submodularity and the output of our fast local search to formulate new recurrences for guided RandomGreedy, which we then solve.

**Local search algorithms.** Local search is a technique widely used in combinatorial optimization. Nemhauser et al. [25] introduced a local search algorithm for monotone functions under size constraint; they showed a ratio of \(1/2\), but noted that their algorithm may run in exponential time. Subsequently, local search has been found to be useful, especially for non-monotone functions. Feige et al. [14] proposed a \(1/3\) approximation algorithm with \(\mathcal{O}\left(n^{4}/\varepsilon\right)\) queries for the unconstrained submodular maximization problem utilizing local search. Meanwhile, Lee et al. [21] proposed a local search algorithm for general SM with matroid constraint, attaining \(1/4-\varepsilon\) approximation ratio with a query complexity of \(\mathcal{O}\left(k^{5}\log(k)n/\varepsilon\right)\). We propose our own FastLS in Section 2.1, yielding a ratio of \(1/2\) for monotone cases and \(1/4\) for non-monotone cases through repeated applications of FastLS, while running in \(\mathcal{O}\left(kn/\varepsilon\right)\) queries.

**Fast approximation algorithms.** Buchbinder et al. [10] developed a faster version of RandomGreedy for size constraint that reduces the query complexity to \(\mathcal{O}_{\varepsilon}\left(n\right)\) with ratio of \(1/e-\varepsilon\). Chen and Kuhnle [11] proposed LinearCard, the first deterministic, linear-time algorithm with an \(1/11.657\)-approximation ratio for size constraints. Also, Han et al. [17] introduced TwinGreedy, a \(0.25\)-approximation algorithm with a query complexity of \(\mathcal{O}\left(kn\right)\) for matroid constraints. These algorithms are fast enough to be used as building blocks for our FastLS, which requires as an input a constant-factor approximation in \(\mathcal{O}\left(kn\right)\) queries.

**Relationship to Tukan et al. [32].** During the submission of this paper, we noticed an independent and parallel work by Tukan et al. [32], which proposed a different \(0.385\)-approximation algorithm. Both papers start from a similar idea-guiding the random greedy algorithm with a fast algorithm to find a local optimum. However, Tukan et al. [32] only considered size constraint and focused on algorithm speedup. They introduced a randomized local search algorithm and used its output to guide the stochastic greedy of Buchbinder et al. [9], achieving a query complexity of \(\mathcal{O}_{\varepsilon}\left(n+k^{2}\right)\). On the other hand, we 1) address a more general constraint-matroid constraint; 2) for size constraint, present an asymptotically faster algorithm that uses a novel way of guiding with partial solutions from random greedy itself, which are not local optima, thereby achieving ratio \(0.377-\varepsilon\) with \(\mathcal{O}_{\varepsilon}\left(n\log(k)\right)\) queries; and 3) derandomize these algorithms.

### Preliminaries

**Notation.** In this section, we establish the notations employed throughout the paper. We denote the marginal gain of adding \(A\) to \(B\) by \(\Delta(A|B)=f\left(A\cup B\right)-f\left(B\right)\). For every set \(S\subseteq\mathcal{U}\) and an element \(x\in\mathcal{U}\), we denote \(S\cup\left\{x\right\}\) by \(S+x\), and \(S\backslash\{x\}\) by \(S-x\). Given a constraint and its related feasible sets \(\mathcal{I}\), let \(O\in\arg\max_{S\in\mathcal{I}}f\left(S\right)\); that is, \(O\) is an optimal solution. To simplify the pseudocode and the analysis, we add \(k\)_dummy elements_ into the ground set, where the dummy element serves as a null element with zero marginal gain when added to any set. The symbol \(e_{0}\) is utilized to represent a dummy element.

**Submodularity.** A set function \(f:2^{\mathcal{U}}\rightarrow\mathbb{R}^{+}\) is submodular, if \(\Delta(x|S)\geq\Delta(x|T)\) for all \(S\subseteq T\subseteq\mathcal{U}\) and \(x\in\mathcal{U}\setminus T\), or equivalently, for all \(A,B\subseteq\mathcal{U}\), it holds that \(f\left(A\right)+f\left(B\right)\geq f\left(A\cup B\right)+f\left(A\cap B\right)\).

**Constraints.** In this paper, our focus lies on two constraints: size constraint and matroid constraint. For size constraint, we define the feasible subsets as \(\mathcal{I}(k)=\{S\subseteq\mathcal{U}:|S|\leq k\}\), where \(k\) is an input parameter. The matroid constraint is defined in Appendix A.

**Organization.** Our randomized algorithms are described in Section 2, with two subroutines, FastLS and GuidedRG, in Section 2.1 and 2.2, respectively. Due to space constraints, we provide only a sketch of the analysis for size constraint in the main text. The full pseudocodes and formal proofs for both size and matroid constraint are provided in Appendix C. Then, we briefly sketch the deterministic approximation algorithms in Section 2.3, with full details provided in Appendix D. Next, we introduce the nearly linear-time deterministic algorithm in Section 3, with omitted analysis provided in Appendix E. Our empirical evaluation is summarized in Section 4. In Section 5, we discuss limitations and future directions.

```
1Input: Instance \((f,\mathcal{I})\), a constant-factor approximation \(Z_{0}\), switch time \(t\in[0,1]\), accuracy \(\varepsilon>0\)
2\(Z\leftarrow\textsc{FastLS}(f,\mathcal{I},Z_{0},\varepsilon)\) /* find local optimum \(Z\) */
3\(A\leftarrow\textsc{GuidedRG}(f,\mathcal{I},Z,t)\) /* guided by local optimum \(Z\) */ return\(\arg\max\{f(Z)\,,f(A)\}\)
```

**Algorithm 2**Randomized combinatorial approximation algorithm.

A Randomized \((0.385-\varepsilon)\)-approximation in \(\mathcal{O}\left(kn/\varepsilon\right)\) Queries

In this section, we present our randomized approximation algorithm (Alg. 2) for both size and matroid constraints. This algorithm improves the state-of-the-art, combinatorial approximation ratio to \(0.385-\varepsilon\approx 1/e+0.018\) for size constraint, and to \(0.305-\varepsilon\approx 0.283+0.022\) for matroid constraint.

**Algorithm Overview.** In overview, Alg. 2 consists of two components, which are detailed below. The first component is a local search algorithm, FastLS (Alg. 4 in Appendix C.1), described in detail in Section 2.1. In brief, the local search algorithm takes an accuracy parameter \(\varepsilon>0\) and a constant-factor, approximate solution \(Z_{0}\) as input, which may be produced by any approximation algorithm with better than \(\mathcal{O}\left(kn\right)\) query complexity. The second component is a random greedy algorithm, GuidedRG (Alg. 6 in Appendix C.2), that is guided by the output \(Z\) of the local search, described in detail in Section 2.2. Also, GuidedRG takes a parameter \(t\in[0,1]\), which is the switching time (as fraction of the budget or rank \(k\)) from guided to unguided behavior. The candidate with best \(f\) value from the two subroutines is returned.

If \(f(Z)<\alpha\)OPT (otherwise, there is nothing to show), then the local search set satisfies our definition of \(((1+\varepsilon)\alpha,\alpha)\)-guidance set (Def. 2.1 below). Under this guidance, we show that GuidedRG produces a superior solution compared to its unguided counterpart. The two components, FastLS and GuidedRG are described in Sections 2.1 and 2.2, respectively. The following theorem is proven in Section 2.2 (size constraint) and Appendix C.2.2 (matroid constraint).

**Theorem 2.1**.: Let \((f,\mathcal{I})\) be an instance of SM. Let \(\varepsilon>0\), and \(k\geq 1/\varepsilon\). Algorithm 2 achieves an expected \((0.385-\varepsilon)\)-approximation ratio for size constraint with \(t=0.372\), and an expected \((0.305-\varepsilon)\)-approximation ratio for matroid constraint with \(t=0.559\). The query complexity of the algorithm is \(\mathcal{O}\left(kn/\varepsilon\right)\).

### The Fast Local Search Algorithm

In this section, we introduce FastLS (Alg. 4), which is the same for size or matroid constraints. There are several innovations in FastLS that result in \(\mathcal{O}\left(kn/\varepsilon\right)\) time complexity, where \(k\) is the maximum size of a feasible set, and \(\varepsilon>0\) is an input accuracy parameter.

In overview, the algorithm maintains a feasible set \(Z\); initially, \(Z=Z_{0}\), where \(Z_{0}\) is an input set which is a constant approximation to OPT. The value of \(Z\) is iteratively improved via swapping, which is done in the following way. For each element \(a\in\mathcal{U}\), we compute \(\Delta(a|Z\setminus a)\); if \(a\not\in Z\), this is just the gain of \(a\) to \(Z\); this requires \(\mathcal{O}\left(n\right)\) queries. Then, if \(a\in Z\) and \(e\not\in Z\) such that \(Z\setminus a+e\) is feasible, and \(\Delta(e|Z)-\Delta(a|Z\setminus a)\geq\frac{\varepsilon}{k}f(Z)\), then \(a\) is swapped in favor of \(e\). If no such swap exists, the algorithm terminates.

One can show that, for each swap, the value of \(Z\) increases by at least a multiplicative \((1+\varepsilon/k)\) factor. Since \(f(Z)\) is initialized to a constant fraction of OPT, it follows that we make at most \(\mathcal{O}(k/\varepsilon)\) swaps. Since each swap requires \(\mathcal{O}\left(n\right)\) queries, this yields the query complexity of the algorithm: \(\mathcal{O}\left(kn/\varepsilon\right)\). In addition, if \(f\) is monotone, FastLS gets ratio of nearly \(1/2\) for FastLS. A second repetition of FastLS yields a ratio of \(1/4\) in the case of general (non-monotone) \(f\), as shown in Appendix C.1.2. Thus, FastLS may be of independent interest, as local search obtains good objective values empirically and is commonly used in applications.

For our purposes, we want to use the output of FastLS to guide RandomGreedy. Since we will also use another algorithm for a similar purpose in Section 3, we abstract the properties needed for such a guidance set. Intuitively, a set \(Z\) is a good guidance set if it has a low \(f\)-value and also ensures that the value of its intersection and union with an optimal solution are poor.

**Definition 2.1**.: A set \(Z\) is a (\(\alpha,\beta\))-guidance set, if given constants \(\alpha,\beta\in(0,0.5)\) and optimum solution \(O\), it holds that: 1) \(f\left(Z\right)<\alpha f\left(O\right)\); 2) \(f\left(O\cap Z\right)\leq\alpha f\left(O\right)\); 3) \(f\left(O\cup Z\right)\leq\beta f\left(O\right)\), or alternatively, 3\({}^{\prime}\)) \(f\left(O\cap Z\right)+f\left(O\cup Z\right)\leq(\alpha+\beta)f\left(O\right)\).

Lemma 2.2 (proved in Appendix C.1.1) implies that for the FastLS output \(Z\), if \(f(Z)<\alpha\)OPT, then \(Z\) is \(((1+\varepsilon)\alpha,\alpha)\)-guidance set.

**Lemma 2.2**.: Let \(\varepsilon>0\), and let \((f,\mathcal{I}(\mathcal{M}))\) be an instance of SM. The input set \(Z_{0}\) is an \(\alpha_{0}\)-approximate solution to \((f,\mathcal{I}(\mathcal{M}))\). FastLS (Alg. 4) returns a solution \(Z\) with \(\mathcal{O}\left(kn\log(1/\alpha_{0})/\varepsilon\right)\) queries such that \(f\left(S\cup Z\right)+f\left(S\cap Z\right)<(2+\varepsilon)f\left(Z\right)\), where \(S\in\mathcal{I}(\mathcal{M})\).

### Guiding the RandomGreedy Algorithm

In this section, we discuss the guided RandomGreedy algorithm (Alg. 6) using an \(((1+\varepsilon)\alpha,\alpha)\)-guidance set \(Z\) returned by FastLS. Due to space constraints, we only consider the size constraint in the main text. The ideas for the matroid constraint are similar, although the final recurrences obtained differ. The version for matroid constraints is given in Appendix C.2.2.

The algorithm GuidedRG is simple to describe: it maintains a partial solution \(A\), initially empty. It takes as parameters the switching time \(t\) and guidance set \(Z\). While the partial solution satisfies \(\left|A\right|<tk\), the algorithm operates as RandomGreedy with ground set \(\mathcal{U}\setminus Z\); after \(\left|A\right|\geq tk\), it operates as RandomGreedy with ground set \(\mathcal{U}\). Pseudocode is provided in Appendix C.2.

**Overview of analysis.** For clarity, we first describe the (unguided) RandomGreedy analysis from Buchbinder et al. [9]. There are two recurrences: the first is the greedy gain:

\[\mathbb{E}\left[f\left(A_{i}\right)-f\left(A_{i-1}\right)\right]\geq\frac{1}{ k}\mathbb{E}\left[f\left(O\cup A_{i-1}\right)-f\left(A_{i-1}\right)\right].\]

Intuitively, the gain at iteration \(i\) is at least a \(1/k\) fraction of the difference between \(f(O\cup A)\) and \(A\), in expectation, where \(A\) is the partial solution. If \(f\) were monotone, the right hand side would be at least \((\text{OPT}-f(A))/k\). However, in the case that \(f\) is not monotone, the set \(O\cup A\) may have value smaller than OPT.

To handle this case, it can be shown that the expected value of \(f(O\cup A)\) satisfies a second recurrence:

\[\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\overset{(a)}{\geq}\left(1- \frac{1}{k}\right)\mathbb{E}\left[f\left(O\cup A_{i-1}\right)\right]+\frac{1}{ k}\mathbb{E}\left[f\left(O\cup A_{i-1}\cup M_{i}\right)\right]\overset{(b)}{ \geq}\left(1-\frac{1}{k}\right)\mathbb{E}\left[f\left(O\cup A_{i-1}\right) \right],\]

where \(M_{i}\) is the set of elements with the top \(k\) marginal gains at iteration \(i\), _(a)_ is from submodularity, and _(b)_ is from nonnegativity. Thus, this expected value, while initially OPT (since \(A_{0}=\emptyset\)), may degrade but is bounded.

Both of these recurrences are solved together to prove the expected ratio of \(1/e\) for RandomGreedy: the worst-case evolution of the expected values of \(f(A_{i})\), \(f(O\cup A_{i})\), according to this analysis,

Figure 1: **(a)**: The evolution of \(\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\) and \(\mathbb{E}\left[f\left(A_{i}\right)\right]\) in the worst case of the analysis of RandomGreedy, as the partial solution size increases to \(k\). **(b)**: Illustration of how the degradation of \(\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\) changes as we introduce an \((0.385+\varepsilon,0.385)\)-guidance set. **(c)**: The updated degradation with a switch point \(tk\), where the algorithm starts with guidance and then switches to running without guidance. The dashed curved lines depict the unguided values from **(a)**.

is illustrated in Fig. 1(a). Observe that \(f(A_{i})\) converges to OPT\(/e\) (as required for the ratio), and _observe that \(f(O\cup A_{i})\) also converges to OPT\(/e\)_. Thus, very little gain is obtained in the later stages of the algorithm, as illustrated in the plot. The overarching idea of the guided version of the algorithm is to obtain a better degradation of \(\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\), leading to better gains later in the algorithm that improve the worst-case ratio. In the following, we elaborate on this goal, the achievement of which is illustrated in Fig. 1(c).

**Stage 1: Recurrences when avoiding \(Z\).** Suppose \(Z\) is an \((\alpha,\beta)\)-guidance set, and that RandomGreedy selects elements as before, but excluding \(Z\) from the ground set. Then, the recurrences change as follows. The recurrence for the gain becomes:

\[\mathbb{E}\left[f\left(A_{i}\right)-f\left(A_{i-1}\right)\right]\geq\frac{1}{ k}\mathbb{E}\left[f\left(\left(O\setminus Z\right)\cup A_{i-1}\right)-f\left(A_{i- 1}\right)\right], \tag{1}\]

where \(O\setminus Z\) replaces \(O\) since we select elements outside of the set \(Z\). For the second recurrence, we can lower bound the term \(\mathbb{E}\left[f\left(O\cup A_{i-1}\cup M_{i}\right)\right]\) using submodularity and the fact that \(Z\cap A_{i-1}=\emptyset\):

\[\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\geq\left(1-\frac{1}{k} \right)\mathbb{E}\left[f\left(O\cup A_{i-1}\right)\right]+\frac{1}{k}\mathbb{ E}\left[f\left(O\right)-f\left(O\cup Z\right)\right]. \tag{2}\]

Finally, a similar recurrence to (2) also holds for \(f\left(\left(O\setminus Z\right)\cup A_{i}\right)\); both are needed for the analysis. Since \(Z\) is a guidance set, by submodularity, \(f(O\setminus Z)\geq f(O)-f(O\cap Z)\geq(1-\alpha)\)OPT, which ensures that some gain is available by selection outside of \(Z\). And \(f(O)-f(O\cup Z)\geq(1-\beta)\)OPT, which means that the degradation recurrences are improved.

The blue line in Figure 1(b) depicts this improved degradation with the size of the partial solution. However, this improvement comes at a cost: a smaller increase in \(\mathbb{E}\left[f\left(A_{i}\right)\right]\) is obtained over the unguided version. Therefore, to obtain an improved ratio we switch back to the regular behavior of RandomGreedy - intuitively, this shifts the relatively good, earlier behavior of RandomGreedy to later in the algorithm.

**Stage 2: Switching back to selection from whole ground set.** After the switch, the recurrences revert back to the original ones, but with different starting values. Since \(\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\) was significantly enhanced in the first stage, in the final analysis we get an overall improvement over the unguided version. The blue line in Figure 1(c) demonstrates the degradation of \(\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\) over two stages, while the orange line depicts how the approximation ratio converges to a value \(0.385>1/e\).

The above analysis sketch can be formalized and the resulting recurrences solved: the results are stated in the following lemma, which is formally proven in Appendix C.2.1.

**Lemma 2.3**.: With an input size constraint \(\mathcal{I}\) and a \(\left((1+\varepsilon)\alpha,\alpha\right)\)-guidance set \(Z\), GuideDREG returns set \(A_{k}\) with \(\mathcal{O}\left(kn\right)\) queries, _s.t._\(\mathbb{E}\left[f\left(A_{k}\right)\right]\geq\left[\left(2-t-\frac{1}{k} \right)\left(1-\frac{1}{k}\right)e^{t-1}-e^{-1}\right.\)\(-(1+\varepsilon)\alpha\left(\left(1-\frac{1}{k}\right)^{2}e^{t-1}-e^{-1} \right)-\alpha\left(\left(1+\frac{1-t}{1-\frac{1}{k}}\right)e^{t-1}-\left(2- \frac{1}{k}\right)e^{-1}\right)\right]f\left(O\right).\)

From Lemma 2.3, we can directly prove the main result for size constraint.

Proof of Theorem 2.1 under size constraint.: Let \(\left(f,\mathcal{I}\right)\) be an instance of SM, with optimal solution set \(O\). If \(f\left(Z\right)\geq(0.385-\varepsilon)f\left(O\right)\) under size constraint, the approximation ratio holds immediately. Otherwise, by Lemma 2.2, FastLS returns a set \(Z\) which is an \(((1+\varepsilon)\alpha,\alpha)\)-guidance set, where \(\alpha=0.385-\varepsilon\). By Lemma 2.3,

\[\mathbb{E}\left[f\left(A_{k}\right)\right]\geq\left[\left(2-t- \varepsilon\right)\left(1-\varepsilon\right)e^{t-1}-e^{-1}-(0.385-0.615 \varepsilon)\left((1-\varepsilon)^{2}e^{t-1}-e^{-1}\right)\right.\] \[\left.-(0.385-\varepsilon)\left(\left(1+\frac{1-t}{1-\varepsilon }\right)e^{t-1}-(2-\varepsilon)e^{-1}\right)\right]f\left(O\right) \left(\forall k\geq\frac{1}{\varepsilon}\right)\] \[\geq\left(0.385-\varepsilon\right)f\left(O\right).\] ( \[t=0.372\] )

### Deterministic approximation algorithms

In this section, we outline the deterministic algorithms, for size and matroid constraints. The main idea is similar, but we replace GuidedRG with a deterministic subroutine. For simplicity, we present a randomized version in Appendix D.2 as Alg. 10, which we then derandomize (Alg. 11 in Appendix D.3). Further discussion is provided in Appendix D.

**Algorithm overview.** Chen and Kuhnle [11] proposed a randomized algorithm, InterpolatedGreedy, which may be thought of as an interpolation between standard greedy [25] and RandomGreedy[9]. Instead of picking \(k\) elements, each randomly chosen from the top \(k\) marginal gains, it picks \(\ell=\mathcal{O}\left(1/\varepsilon\right)\) sets randomly from \(\mathcal{O}\left(\ell\right)\) candidates. Although it uses only a constant number of rounds, the recurrences for InterpolatedGreedy are similar to the RandomGreedy ones discussed above, so we can guide it similarly.

To select the candidate sets in each iteration, we replace InterlaceGreedy (the subroutine of InterpolatedGreedy proposed in Chen and Kuhnle [11]) with a guided version: GuidedIG-S (Alg. 9 in Appendix D.1.1) for size constraint, and GuidedIG-M (Alg. 8 in Appendix D.1) for matroid constraint. Since only \(\ell\) random choices are made, each from \(\mathcal{O}\left(\ell\right)\) sets, there are at most \(\mathcal{O}\left(\ell^{\mathcal{O}\left(\ell\right)}\right)\) possible solutions, where \(\ell\) is a constant number depending on \(\varepsilon\). Notably, we are still able to obtain the same approximation factors as in Section 2. The proof of Theorem 2.4 is provided in Appendices D.2 and D.3.

**Theorem 2.4**.: Let \((f,k)\) be an instance of SM, with the optimal solution set \(O\). Alg. 11 achieves a deterministic \((0.385-\varepsilon)\) approximation ratio with \(t=0.372\), and a deterministic \((0.305-\varepsilon)\) approximation ratio with \(t=0.559\). The query complexity of the algorithm is \(\mathcal{O}\left(kn\ell^{2\ell-1}\right)\) where \(\ell=\frac{10}{9\varepsilon}\).

## 3 Deterministic Algorithm with Nearly Linear Query Complexity

In this section, we sketch a deterministic algorithm with \((0.377-\varepsilon)\) approximation ratio and \(\mathcal{O}_{\varepsilon}\left(n\log(k)\right)\) query complexity for the size constraint. A full pseudocode (Alg. 14) and analysis is provided in Appendix E.

**Description of algorithm.** Our goal is to improve the asymptotic \(\mathcal{O}_{\varepsilon}(kn)\) query complexity. Recall that in Section 2, we described a deterministic algorithm that employed the output of local search to guide the InterpolatedGreedy algorithm, which obeys similar recurrences to RandomGreedy. To produce the \(\ell\) candidate sets for each iteration of InterpolatedGreedy, a greedy algorithm (guided InterlaceGreedy) is used. These algorithms can be sped up using a descending thresholds technique. This results in ThreshGuidedIG (Alg. 12 in Appendix E.1), which achieves \(\mathcal{O}_{\varepsilon}\left(n\log k\right)\) query complexity for the guided part of our algorithm.

However, the local search FastLS still requires \(\mathcal{O}\left(kn/\varepsilon\right)\) queries, so we seek to find a guidance set in a faster way. Recall that, in the definition of guidance set \(Z\), the value \(f(Z)\) needs to dominate both \(f(\mathcal{O}\cap Z)\) and \(f(\mathcal{O}\cup Z)\). To achieve this with faster query complexity, we employ a run of unguided InterpolatedGreedy. Consider the recurrences plotted in Fig. 1(a) - if the worst-case degradation occurs, then at some point the value of \(f(A_{i})\) becomes close to \(f(O\cup A_{i})\). On the other hand, if the worst-case degradation does not occur, then the approximation factor of InterpolatedGreedy is improved (see Fig. 2). Moreover, if we ensure that at every stage, \(A_{i}\) contains no elements that contribute a negative gain, then we will also have \(f(A_{i})\geq f(O\cap A_{i})\).

To execute this idea, we run (derandomized, unguided InterpolatedGreedy, and consider all \(\mathcal{O}\left(\ell^{\ell}\right)\) intermediate solutions. Each one of these is pruned (by which we mean, any element with negative contribution is discarded until none such remain). Then, the guided part of our algorithm executes with every possible candidate intermediate solution as the guiding set; finally, the feasible set encountered with maximum \(f\) value is returned.

Figure 2: Depiction of how analysis of InterpolatedGreedy changes if there is no \((0.377,0.46)\)-guidance set.

The tradeoff between the first and second parts of the algorithm is optimized with \(\alpha=0.377\) and \(\beta=0.46\). That is, if InterpolatedGreedy produces an \((\alpha,\beta)\)-guidance set, the guided part of our algorithm achieves ratio \(0.377\); otherwise, InterpolatedGreedy has ratio at least \(0.377\). We have the following theorem. The algorithms and analysis sketched here are formalized in Appendix E.

**Theorem 3.1**.: Let \((f,k)\) be an instance of SM, with the optimal solution set \(\mathcal{O}\). Algorithm 14 achieves a deterministic \((0.377-\varepsilon)\) approximation ratio with \(\mathcal{O}(n\log(k){\ell_{1}}^{2\ell_{1}}{\ell_{2}}^{2\ell_{2}-1})\) queries, where \(\ell_{1}=\frac{10}{3\varepsilon}\) and \(\ell_{2}=\frac{2}{\varepsilon}\).

## 4 Empirical Evaluation

In this section, along with Appendix F, we implement and empirically evaluate our randomized \((0.385-\varepsilon)\)-approximation algorithm (Alg. 2, FastLS+GuidedRG) on two applications of size-constrained SM, and compare to several baselines in terms of objective value of solution and number of queries to \(f\). In summary, our algorithm uses roughly twice the queries as the standard greedy algorithm, but obtains competitive objective values with an expensive local search that uses one to two orders of magnitude more queries. 1

Footnote 1: Our code is available at [https://gitlab.com/luciacyx/guided-rg.git](https://gitlab.com/luciacyx/guided-rg.git).

**Baselines.** 1) StandardGreedy: the classical greedy algorithm [25], which often performs well empirically on non-monotone objectives despite having no theoretical guarantee. 2) RandomGreedy, the current state-of-the-art combinatorial algorithm as discussed extensively above. 3) The local search algorithm of Lee et al. [21], which is the only prior polynomial-time local search algorithm with a theoretical guarantee: ratio \(1/4-\varepsilon\) in \(\mathcal{O}\left(k^{5}\log(k)n/\varepsilon\right)\) queries. As our emphasis is on theoretical guarantees above \(1/\varepsilon\), we set \(\varepsilon=0.01\) for our algorithm, which yields ratio at least \(0.375\) in this evaluation. For Lee et al. [21], we set \(\varepsilon=0.1\), which is the standard value of the accuracy parameter in the literature - running their algorithm with \(\varepsilon=0.01\) produced identical results.

**Applications and datasets.** For instances of SM upon which to evaluate, we chose video summarization and maximum cut (MC). For video summarization, our objective is to select a subset of frames from a video to create a summary. As in Banihashem et al. [1], we use a Determinantal Point Process objective function to select a diverse set of elements [20]. Maximum cut is a classical example of a non-monotone, submodular objective function. We run experiments on unweighted Erdos-Renyi (ER), Barabasi-Albert (BA) and Watts-Strogatz (WS) graphs which have been used to model many real-world networks. The formal definition of problems, details of datasets, and hyperparameters of graph generation can be found in the Appendix F. In video summarization, there are \(n=100\) frames. On all the instances of maximum cut, the number of vertices \(n=10000\). The mean of 20 independent runs is plotted, and the shaded region represents one standard deviation about the mean.

**Results.** As shown in Figure 3 in this section, and Figure 5 and 6 in Appendix F, on both applications, FastLS +GuidedRG produces solutions of higher objective value than StandardGreedy, and also higher than RandomGreedy. The objective values of FastLS +GuidedRG often matches with Lee et al. [21] which performs the best; this agrees with the intuition that, empirically, local search is nearly optimal. In terms of queries, our algorithm uses roughly twice the number of queries

Figure 3: The objective value (higher is better) and the number of queries (log scale, lower is better) are normalized by those of StandardGreedy. Our algorithm (blue star) outperforms every baseline on at least one of these two metrics.

as StandardGreedy, but we improve on Lee et al. [21] typically by at least a factor of \(10\) and often by more than a factor of \(100\).

## 5 Discussion and Limitations

Prior to this work, the state-of-the-art combinatorial ratios were \(1/e\approx 0.367\) and \(0.283\) for size constrained and matroid constrained SM, respectively, both achieved by the RandomGreedy algorithm. In this work, we show how to guide RandomGreedy with a fast local search algorithm to achieve ratios \(0.385\) and \(0.305\), respectively, in \(\mathcal{O}\left(kn/\varepsilon\right)\) queries. The resulting algorithm is practical and empirically outperforms both RandomGreedy and standard greedy in objective value on several applications of SM. However, if \(k\) is on the order of \(n\), the query complexity is quadratic in \(n\), which is too slow for modern data sizes. Therefore, an interesting question for future work is whether further improvements in the query complexity to achieve these ratios (or better ones) could be made.

In addition, we achieve the same approximation ratios and asymptotic query complexity with deterministic algorithms, achieved by guiding a different algorithm; moreover, we speed up the deterministic algorithm to \(\mathcal{O}_{\varepsilon}(n\log k)\) by obtaining the guidance set in another way. This result is a partial answer to the limitation in the previous paragraph, as we achieve a ratio beyond \(1/e\) in nearly linear query complexity. However, for all of our deterministic algorithms, there is an exponential dependence on \(1/\varepsilon\), which makes these algorithms impractical and mostly of theoretical interest.

## References

* [1]
* Banihashem et al. [2023] Banihashem, K., Biabani, L., Goudarzi, S., Hajiaghayi, M., Jabbarzade, P., and Monemizadeh, M. (2023). Dynamic non-monotone submodular maximization. _Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023_.
* Bao et al. [2022] Bao, W., Hang, J., and Zhang, M. (2022). Submodular feature selection for partial label learning. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2022_.
* Bilmes [2022] Bilmes, J. A. (2022). Submodularity in machine learning and artificial intelligence. _arXiv preprint arXiv:2202.00132_.
* Brualdi [1969] Brualdi, R. A. (1969). Comments on bases in dependence structures. _Bulletin of the Australian Mathematical Society_, 1(2):161-167.
* Buchbinder and Feldman [2018] Buchbinder, N. and Feldman, M. (2018). Deterministic algorithms for submodular maximization problems. _ACM Trans. Algorithms_, 14(3):32:1-32:20.
* Buchbinder and Feldman [2019] Buchbinder, N. and Feldman, M. (2019). Constrained submodular maximization via a nonsymmetric technique. _Math. Oper. Res._, 44(3):988-1005.
* Buchbinder and Feldman [2023] Buchbinder, N. and Feldman, M. (2023). Constrained submodular maximization via new bounds for dr-submodular functions. _arXiv preprint arXiv:2311.01129_.
* Buchbinder et al. [2023] Buchbinder, N., Feldman, M., and Garg, M. (2023). Deterministic (1/2 + \(\epsilon\))-approximation for submodular maximization over a matroid. _SIAM J. Comput._, 52(4):945-967.
* Buchbinder et al. [2014] Buchbinder, N., Feldman, M., Naor, J., and Schwartz, R. (2014). Submodular maximization with cardinality constraints. In _Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014_.
* Buchbinder et al. [2017] Buchbinder, N., Feldman, M., and Schwartz, R. (2017). Comparing apples and oranges: Query trade-off in submodular maximization. _Math. Oper. Res._, 42(2):308-329.
* Chen and Kuhnle [2023] Chen, Y. and Kuhnle, A. (2023). Approximation algorithms for size-constrained non-monotone submodular maximization in deterministic linear time. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023_.

* Chen and Kuhnle [2024] Chen, Y. and Kuhnle, A. (2024). Practical and parallelizable algorithms for non-monotone submodular maximization with size constraint. _J. Artif. Intell. Res._, 79:599-637.
* Ene and Nguyen [2016] Ene, A. and Nguyen, H. L. (2016). Constrained submodular maximization: Beyond 1/e. In _IEEE 57th Annual Symposium on Foundations of Computer Science, FOCS 2016_.
* Feige et al. [2011] Feige, U., Mirrokni, V. S., and Vondrak, J. (2011). Maximizing non-monotone submodular functions. _SIAM J. Comput._, 40(4):1133-1153.
* Feldman et al. [2011] Feldman, M., Naor, J., and Schwartz, R. (2011). A unified continuous greedy algorithm for submodular maximization. In _IEEE 52nd Annual Symposium on Foundations of Computer Science, FOCS 2011_.
* Gharan and Vondrak [2011] Gharan, S. O. and Vondrak, J. (2011). Submodular maximization by simulated annealing. In _Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2011_.
* Han et al. [2020] Han, K., Cao, Z., Cui, S., and Wu, B. (2020). Deterministic approximation for submodular maximization over a matroid in nearly linear time. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020_.
* Khanna et al. [2017] Khanna, R., Elenberg, E. R., Dimakis, A. G., Negahban, S. N., and Ghosh, J. (2017). Scalable greedy feature selection via weak submodularity. In _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017_.
* Krause et al. [2008] Krause, A., Singh, A. P., and Guestrin, C. (2008). Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. _J. Mach. Learn. Res._, 9:235-284.
* Kulesza et al. [2012] Kulesza, A., Taskar, B., et al. (2012). Determinantal point processes for machine learning. _Foundations and Trends(r) in Machine Learning_, 5(2-3):123-286.
* Lee et al. [2009] Lee, J., Mirrokni, V. S., Nagarajan, V., and Sviridenko, M. (2009). Non-monotone submodular maximization under matroid and knapsack constraints. In _Proceedings of the 41st Annual ACM Symposium on Theory of Computing, STOC 2009_.
* Liu et al. [2013] Liu, Y., Wei, K., Kirchhoff, K., Song, Y., and Bilmes, J. A. (2013). Submodular feature selection for high-dimensional acoustic score spaces. In _IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013_.
* Mirzasoleiman et al. [2018] Mirzasoleiman, B., Jegelka, S., and Krause, A. (2018). Streaming non-monotone submodular maximization: Personalized video summarization on the fly. In _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI 2018_.
* Nemhauser and Wolsey [1978] Nemhauser, G. L. and Wolsey, L. A. (1978). Best algorithms for approximating the maximum of a submodular set function. _Math. Oper. Res._, 3(3):177-188.
* I. _Math. Program._, 14(1):265-294.
* May 1, 2021_, pages 423-431. SIAM.
* Powers et al. [2016] Powers, T., Bilmes, J. A., Krout, D. W., and Atlas, L. E. (2016). Constrained robust submodular sensor selection with applications to multistatic sonar arrays. In _19th International Conference on Information Fusion, FUSION 2016_.
* Qi [2022] Qi, B. (2022). On maximizing sums of non-monotone submodular and linear functions. In _33rd International Symposium on Algorithms and Computation, ISAAC 2022_.
* Roy [2022] Roy (2022). Cooking video. [https://youtu.be/voUDP4rUKvQ?si=ZUezR4jMVzOz5Kcw](https://youtu.be/voUDP4rUKvQ?si=ZUezR4jMVzOz5Kcw).

- 28th International Conference, COCOON 2022_.
* Tschiatschek et al. [2014] Tschiatschek, S., Iyer, R. K., Wei, H., and Bilmes, J. A. (2014). Learning mixtures of submodular functions for image collection summarization. In _Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, NeurIPS 2014_.
* Tukan et al. [2024] Tukan, M., Mualem, L., and Feldman, M. (2024). Practical \(0.385\)-approximation for submodular maximization subject to a cardinality constraint. _arXiv preprint arXiv:2405.13994_.

[MISSING_PAGE_FAIL:13]

**Lemma A.4**.: \[1-\frac{1}{x}\leq\log(x)\leq x-1, \forall x>0\] \[1-\frac{1}{x+1}\geq e^{-\frac{1}{x}}, \forall x\in\mathbb{R}\] \[(1-x)^{y-1}\geq e^{-xy}, \forall xy\leq 1\]

## Appendix B Query Complexity Analysis of the Continuous Algorithm in Buchbinder and Feldman [6]

```
input : oracle \(f\), a solvable down-closed polytope \(P\), a set \(Z\in\mathcal{U}\), \(t_{s}\in(0,1)\) /* Initialization */
1 Let \(\bar{\delta}_{1}\gets t_{s}\cdot n^{-4}\) and \(\bar{\delta}_{2}\leftarrow(1-t_{s})\cdot n^{-4}\)
2 Let \(t\gets 0\) and \(y(t)\leftarrow\mathbf{1}_{\emptyset}\) /* Growing \(y(t)\)
3while\(t<1\)do
4foreach\(u\in\mathcal{U}\)do
5 Let \(w_{u}(t)\) be an estimate of \(\mathbb{E}\left[\Delta(u|R(y(t)))\right]\) obtained by averaging the values of \(\Delta(u|R(y(t)))\) for \(r=\left\lceil 48n^{6}\log(2n)\right\rceil\) independent samples of \(R(y(t))\)
6 end foreach
7 Let \(x(t)\leftarrow\begin{cases}\arg\max_{x\in P}\left\{\sum_{u\in\mathcal{U}\setminus Z }w_{u}(t)\cdot x_{u}(t)-\sum_{u\in Z}x_{u}(t)\right\}&\text{ if }t\in[0,t_{s} )\,,\\ \arg\max_{x\in P}\left\{\sum_{u\in\mathcal{U}}w_{u}(t)\cdot x_{u}(t)\right\}& \text{ if }t\in[t_{s},1)\,.\end{cases}\)
8 Let \(\delta_{t}\) be \(\bar{\delta}_{1}\) when \(t<t_{s}\) and \(\bar{\delta}_{2}\) when \(t\geq t_{s}\)
9 Let \(y\left(t+\delta_{t}\right)\gets y(t)+\delta_{t}\left(\mathbf{1}_{\mathcal{ U}}-y(t)\right)\circ x(t)\)
10 Update \(t\gets t+\delta_{t}\)
11 end while return\(y(1)\)
```

**Algorithm 3**Aided Measured Continuous Greedy \((f,P,Z,t_{s})\)[6]

In this section, we analyze the query complexity of Aided Measured Continuous Greedy (Alg. 3) proposed by Buchbinder and Feldman [6], which is \(\mathcal{O}\left(n^{11}\log(n)\right)\).

In Alg. 3, queries to the oracle \(f\) only occur on Line 5. For each element \(u\) in the ground set \(\mathcal{U}\), \(r=\left\lceil 48n^{6}\log(2n)\right\rceil\) queries are made. These queries correspond to \(r\) independent samples of \(R(y(t))\) to estimate \(\mathbb{E}\left[\Delta(u|R(y(t)))\right]\). Therefore, there are \(nr=\mathcal{O}\left(n^{7}\log(n)\right)\) queries for each iteration of the while loop (Line 3-11).

Time variable \(t\) is increased by \(\bar{\delta}_{1}=t_{s}\cdot n^{-4}\), when \(t<t_{s}\), and is increased by \(\bar{\delta}_{2}=(1-t_{s})\cdot n^{-4}\), when \(t\geq t_{s}\). Thus, there are a total of \(2n^{4}\) iterations within the while loop. In conclusion, the total number of queries made by the algorithm is \(\mathcal{O}\left(n^{11}\log(n)\right)\).

## Appendix C Analysis of Randomized Approximation Algorithm, Alg. 2

In this section, we provide a detailed analysis of our randomized approximation algorithm and its two components, FastLS and GuidedRG. This section is organized as follows: Appendix C.1 analyzes the theoretical guarantee of single run of FastLS (Appendix C.1.1), which is needed to show that it finds a good guidance set. Then, although it is not needed for our results, we show the FastLS independently achieves an approximation ratio achieved for SMCC under monotone and non-monotone objectives (Appendix C.1.2).

In Appendix C.2, we provide pseudocode and formally prove the results for GuidedRG. Specifically, we solve the recurrences for both size and matroid constraint.

[MISSING_PAGE_FAIL:15]

Then, by Lemma A.2, there exits a bijection \(\sigma:S\setminus Z\to Z\setminus S\) such that \(Z+e-\sigma(e)\) is a basis for all \(e\in S\setminus Z\). After the algorithm terminates, for every \(e\in S\setminus Z\), it holds that, \(\Delta(e|Z)-\Delta(\sigma(e)|Z-\sigma(e))<\frac{\varepsilon}{k}f\left(Z\right)\). Then,

\[\varepsilon f\left(Z\right)>\sum_{e\in S\setminus Z}\Delta(e|Z)-\sum_{a\in Z \setminus S}\Delta(a|Z-a)\]

Let \(\ell=|S\setminus Z|=|Z\setminus S|\), \(S\setminus Z=\{e_{1},\ldots,e_{\ell}\}\), \(Z\setminus S=\{a_{1},\ldots,a_{\ell}\}\). By submodularity,

\[\sum_{e\in S\setminus Z}\Delta(e|Z) =\left(f\left(Z+e_{1}\right)-f\left(Z\right)\right)+\left(f\left( Z+e_{2}\right)-f\left(Z\right)\right)+\ldots+\left(f\left(Z+e_{\ell}\right)-f \left(Z\right)\right)\] \[\geq\ldots\] \[\geq f\left(Z+e_{1}+\ldots+e_{\ell}\right)-f\left(Z\right)=f\left(S \cup Z\right)-f\left(Z\right).\]

Also by submodularity,

\[\sum_{a\in Z\setminus S}\Delta(a|Z-a) =\sum_{i=1}^{\ell}\Delta(a_{i}|Z-a_{i})\leq\sum_{i=1}^{\ell}\Delta (a_{i}|Z-a_{1}-\ldots-a_{i})=f\left(Z\right)-f\left(S\cap Z\right)\]

Thus,

\[\varepsilon f\left(Z\right)>f\left(S\cup Z\right)-f\left(Z\right)- \left(f\left(Z\right)-f\left(S\cap Z\right)\right)\] \[\Rightarrow \left(2+\varepsilon\right)f\left(Z\right)>f\left(S\cup Z\right)+ f\left(S\cap Z\right).\]

#### c.1.2 Approximation Ratio achieved by FastLS

In this section, we show that FastLS can be employed independently to achieves approximation ratios of nearly \(1/2\) and \(1/4\) for both the monotone and non-monotone versions of the problem, respectively.

**Monotone submodular functions.** By employing FastLS once, it returns an \(\frac{1}{2+\varepsilon}\)-approximation result in monotone cases.

**Theorem C.1**.: Let \(\varepsilon>0\), and let \(\left(f,\mathcal{I}(\mathcal{M})\right)\) be an instance of SM, where \(f\) is monotone. The input set \(Z_{0}\) is an \(\alpha_{0}\)-approximate solution to \(\left(f,\mathcal{I}(\mathcal{M})\right)\). FastLS (Alg. 4) returns a solution \(Z\) such that \(f\left(Z\right)\geq f\left(O\right)/(2+\varepsilon)\) with \(\mathcal{O}\left(kn\log(1/\alpha_{0})/\varepsilon\right)\) queries.

Proof.: By Lemma 2.2, set \(S=O\), it holds that

\[\left(2+\varepsilon\right)f\left(Z\right)>f\left(O\cup Z\right)+f\left(O\cap Z \right)\geq f\left(O\right),\]

where the last inequality follows by monotonicity and non-negativity. 

**Non-monotone submodular functions.** For the non-monotone problem, \(2\) repetitions of FastLS (Alg. 5) yields a ratio of \(\frac{1}{4+2\varepsilon}\). The theoretical guarantees and the corresponding analysis are provided as follows. We remark that this is a primitive implementation of the guiding idea: the second run of FastLS avoids the output of the first one.

```
1Input: oracle \(f\), constraint \(\mathcal{I}\), an approximation result \(Z_{0}\), switch point \(t\), error rate \(\varepsilon\)
2\(Z_{1}\leftarrow\textsc{FastLS}(f,\mathcal{U},\mathcal{I},Z_{0},\varepsilon)\) /* run FASTLS with ground set \(\mathcal{U}\) */
3\(Z_{2}\leftarrow\textsc{FastLS}(f,\mathcal{U}\setminus Z_{1},\mathcal{I},Z_{0}, \varepsilon)\) /* run FASTLS with ground set \(\mathcal{U}\setminus Z_{1}\) */ return\(Z\leftarrow\arg\max\{f\left(Z_{1}\right),f\left(Z_{2}\right)\}\)
```

**Algorithm 5**An \(1/(4+2\varepsilon)\)-approximation algorithm with \(\mathcal{O}\left(kn/\varepsilon\right)\)

**Theorem C.2**.: Let \(\varepsilon>0\), and let \(\left(f,\mathcal{I}(\mathcal{M})\right)\) be an instance of SM, where \(f\) is not necessarily monotone. The input set \(Z_{0}\) is an \(\alpha_{0}\)-approximate solution to \(\left(f,\mathcal{I}(\mathcal{M})\right)\). Alg. 5 returns a solution \(Z\) such that \(f\left(Z\right)\geq f\left(O\right)/(4+2\varepsilon)\) with \(\mathcal{O}\left(kn\log(1/\alpha_{0})/\varepsilon\right)\) queries.

Proof.: By repeated application of Lemma 2.2 for the two calls of FastLS in Alg. 5, it holds that

\[f\left(O\cup Z_{1}\right)+f\left(O\cap Z_{1}\right)<\left(2+ \varepsilon\right)f\left(Z_{1}\right)\] \[f\left(\left(O\setminus Z_{1}\right)\cup Z_{2}\right)+f\left( \left(O\setminus Z_{1}\right)\cap Z_{2}\right)<\left(2+\varepsilon\right)f \left(Z_{2}\right)\]

By summing up the above two inequalities, it holds that

\[\left(4+\varepsilon\right)f\left(Z\right) \geq\left(2+\varepsilon\right)f\left(Z_{1}\right)+\left(2+ \varepsilon\right)f\left(Z_{2}\right)\] \[\geq f\left(O\cup Z_{1}\right)+f\left(O\cap Z_{1}\right)+f\left( \left(O\setminus Z_{1}\right)\cup Z_{2}\right)+f\left(\left(O\setminus Z_{1} \right)\cap Z_{2}\right)\] \[\geq f\left(O\cup Z_{1}\right)+f\left(\left(O\setminus Z_{1} \right)\cup Z_{2}\right)+f\left(O\cap Z_{1}\right)\] (nonnegativity) \[\geq f\left(O\setminus Z_{1}\right)+f\left(O\cap Z_{1}\right)\] (submodularity) \[\geq f\left(O\right).\] (submodularity)

### Pseudocode of GuidedRG (Alg. 6) and its Analysis

```
1Procedure GuidedRG (\(f,\mathcal{I},Z,t\)):
2Input: oracle \(f\), constraint \(\mathcal{I}\), guidance set \(Z\), switch point \(t\)
3\(A_{0}\gets k\) dummy elements /* Equivalent to an empty set */
4for\(i\gets 1\) to \(k\)do
5if\(i\leq t\cdot k\)then\(M_{i}\leftarrow\arg\max_{M\subseteq\mathcal{U}\setminus\left(A_{i-1} \cup Z\right),M\text{ is a basis}}\sum_{a\in M}\Delta(a|A_{i-1})\)
6else\(M_{i}\leftarrow\arg\max_{M\subseteq\mathcal{U}\setminus A_{i-1},M\text{ is a basis}}\sum_{a\in M}\Delta(a|A_{i-1})\)
7if\(\mathcal{I}\) represents the size constraintthen
8\(a_{i}\gets\) randomly pick an element from \(M_{i}\)
9\(A_{i}\gets A_{i-1}+a_{i}-e_{0}\) /* \(e_{0}\) is the dummy element */
10elseif\(\mathcal{I}\) represents the matroid constraintthen
11\(\sigma_{i}\leftarrow\) a bijection from \(M_{i}\) to \(A_{i-1}\), where \(A_{i-1}+x-\sigma_{i}\left(x\right)\in\mathcal{I}(\mathcal{M}),\forall x\in M_{i}\)
12\(e_{i}\leftarrow\) randomly pick an element from \(M_{i}\)
13\(A_{i}\gets A_{i-1}+e_{i}-\sigma_{i}(e_{i})\)
14 end if
15return\(A_{k}\)
```

**Algorithm 6**An algorithm guided by an \((\alpha,\beta)\)-guidance set \(Z\) with \(\mathcal{O}\left(kn\right)\) queries

In this section, we present the pseudocode for GuidedRG as Alg. 6. Then, we provide the detailed proof of Lemma 2.3 in Appendix C.2.1, which addresses size constraints. Finally, we analyze the algorithm under matroid constraints and provide the guarantees and its analysis in Appendix C.2.2.

#### c.2.1 GuidedRG under Size Constraints

In Sec. 2.2, we introduce the intuition behind GuidedRG under size constraint. Below, we reiterate theoretical guarantees achieved by GuidedRG under size constraints and provide the detailed analysis.

**Lemma 2.3**.: With an input size constraint \(\mathcal{I}\) and a \(((1+\varepsilon)\alpha,\alpha)\)-guidance set \(Z\), Guide-DRG returns set \(A_{k}\) with \(\mathcal{O}\left(kn\right)\) queries, _s.t._\(\mathbb{E}\left[f\left(A_{k}\right)\right]\geq\left[\left(2-t-\frac{1}{k} \right)\left(1-\frac{1}{k}\right)e^{t-1}-e^{-1}\right.\right.\)

We provide the recurrence of \(f\left(\left(O\setminus Z\right)\cup A_{i}\right)\), \(f\left(O\cup A_{i}\right)\) and \(f\left(A_{i}\right)\) in Lemmata C.3 and C.4 and their analysis below to help prove Lemma 2.3 under size constraint.

**Lemma C.3**.: When \(0<i\leq t\cdot k\), it holds that

\[\mathbb{E}\left[f\left(\left(O\setminus Z\right)\cup A_{i}\right) \right]\geq\left(1-\frac{1}{k}\right)\mathbb{E}\left[f\left(\left(O\setminus Z \right)\cup A_{i-1}\right)\right]+\frac{1}{k}\left[f\left(O\setminus Z \right)-f\left(O\cup Z\right)\right],\] \[\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\geq\left(1- \frac{1}{k}\right)\mathbb{E}\left[f\left(O\cup A_{i-1}\right)\right]+\frac{1}{k }\left[f\left(O\right)-f\left(O\cup Z\right)\right].\]When \(t\cdot k<i\leq k\), it holds that

\[\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\geq\left(1-\frac{1}{k}\right) \mathbb{E}\left[f\left(O\cup A_{i-1}\right)\right]\]

Proof.: At iteration \(i\), condition on a given \(A_{i-1}\). When \(i\leq tk\), \(A_{i-1}\cap Z=\emptyset\) and \(M_{i}\) is selected out of \(A_{i-1}\cup Z\), so

\[\left(O\cup Z\right)\cap\left(\left(O\setminus Z\right)\cup A_{i-1}\cup M_{i} \right)=O\setminus Z \tag{3}\]

\[\left(\left(O\cup Z\right)\cap\left(O\cup A_{i-1}\cup M_{i}\right)=O.\right. \tag{4}\]

Then,

\[\mathbb{E}\left[f\left(\left(O\setminus Z\right)\cup A_{i}\right) \mid A_{i-1}\right]=\frac{1}{k}\sum_{x\in M_{i}}f\left(\left(O\setminus Z \right)\cup A_{i-1}\cup\{x\}\right)\] (selection of next element) \[\geq\frac{1}{k}\left[\left(k-1\right)f\left(\left(O\setminus Z \right)\cup A_{i-1}\right)+f\left(\left(O\setminus Z\right)\cup A_{i-1}\cup M _{i}\right)\right]\] (submodularity) \[\geq\frac{1}{k}\left[\left(k-1\right)f\left(\left(O\setminus Z \right)\cup A_{i-1}\right)+f\left(O\setminus Z\right)+f\left(O\cup Z\cup A_{i- 1}\cup M_{i}\right)-f\left(O\cup Z\right)\right]\] (submodularity) \[\geq\frac{1}{k}\left[\left(k-1\right)f\left(\left(O\setminus Z \right)\cup A_{i-1}\right)+f\left(O\setminus Z\right)-f\left(O\cup Z\right)\right]\] (nonnegativity) \[\mathbb{E}\left[f\left(O\cup A_{i}\right)\mid A_{i-1}\right]= \frac{1}{k}\sum_{x\in M_{i}}f\left(O\cup A_{i-1}\cup\{x\}\right)\] \[\geq\frac{1}{k}\left[\left(k-1\right)f\left(O\cup A_{i-1}\right)+ f\left(O\cup A_{i-1}\cup M_{i}\right)\right]\] (submodularity) \[\geq\frac{1}{k}\left[\left(k-1\right)f\left(O\cup A_{i-1}\right)+ f\left(O\cup Z\cup A_{i-1}\cup M_{i}\right)-f\left(O\cup Z\right)\right]\] (submodularity) \[\geq\frac{1}{k}\left[\left(k-1\right)f\left(O\cup A_{i-1}\right)+ f\left(O\right)-f\left(O\cup Z\right)\right]\] (nonnegativity)

When \(i>tk\), it holds that

\[\mathbb{E}\left[f\left(O\cup A_{i}\right)\mid A_{i-1}\right]= \frac{1}{k}\sum_{x\in M_{i}}f\left(O\cup A_{i-1}\cup\{x\}\right)\] \[\geq\frac{1}{k}\left[\left(k-1\right)f\left(O\cup A_{i-1}\right)+ f\left(O\cup A_{i-1}\cup M_{i}\right)\right]\] (submodularity) \[\geq\left(1-\frac{1}{k}\right)f\left(O\cup A_{i-1}\right)\] (nonnegativity)

By unconditioning \(A_{i-1}\), the lemma is proved. 

**Lemma C.4**.: When \(0<i\leq t\cdot k\), it holds that

\[\mathbb{E}\left[f\left(A_{i}\right)\right]-\mathbb{E}\left[f\left(A_{i-1} \right)\right]\geq\frac{1}{k}\left(\mathbb{E}\left[f\left(\left(O\setminus Z \right)\cup A_{i-1}\right)\right]-\mathbb{E}\left[f\left(A_{i-1}\right) \right]\right).\]

When \(t\cdot k<i\leq k\), it holds that

\[\mathbb{E}\left[f\left(A_{i}\right)\right]-\mathbb{E}\left[f\left(A_{i-1} \right)\right]\geq\frac{1}{k}\left(\mathbb{E}\left[f\left(O\cup A_{i-1}\right) \right]-\mathbb{E}\left[f\left(A_{i-1}\right)\right]\right).\]

Proof.: Given \(A_{i-1}\) at iteration \(i\). When \(i\leq t\cdot k\), it holds that

\[\mathbb{E}\left[f\left(A_{i}\right)-f\left(A_{i-1}\right)\mid A_{ i-1}\right]=\frac{1}{k}\sum_{x\in M_{i}}\Delta(x|A_{i-1})\] \[\geq\frac{1}{k}\sum_{x\in O\setminus\left(A_{i-1}\cup Z\right)} \Delta(x|A_{i-1})\] (Line 5 in Alg. 6) \[\geq\frac{1}{k}\left[f\left(\left(O\setminus Z\right)\cup A_{i-1} \right)-f\left(A_{i-1}\right)\right].\] (submodularity)

[MISSING_PAGE_FAIL:19]

where Inequality \((a)\) and \((b)\) follow from Inequality 5, Lemma C.4 and A.3. 

#### c.2.2 GuidedRG under Matroid Constraints

```
ProcedureRandomGreedy\((f,\mathcal{M})\):
1Input: oracle \(f\), matroid constraint \(\mathcal{M}\)
2Initialize:\(A_{0}\leftarrow\) arbitrary basis in \(\mathcal{I}(\mathcal{M})\)
3for\(i\gets 1\) to \(k\)do
4\(M_{i}\leftarrow\arg\max_{S\subseteq\mathcal{U},S\text{ is a basis }\sum_{x\in S}\Delta(x|A_{i-1})}\)
5\(\sigma\leftarrow\) a bijection from \(M_{i}\) to \(A_{i-1}\)
6\(x_{i}\leftarrow\) a uniformly random element from \(M_{i}\)
7\(A_{i}\gets A_{i-1}+x_{i}-\sigma(x_{i})\)
8 end for return\(A_{k}\)
```

**Algorithm 7**RandomGreedy for Matroid

**Discussion about Intuition behind GuidedRG under Matroid Constraints.** The pseudocode for RandomGreedy under matroid constraints is provided in Alg. 7. To deal with the feasibility for matroid constraints, Alg. 7 starts with an arbitrary basis and builds the solution by randomly swapping the elements in ground set with a candidate basis. The analysis of it proceeds according to two main recurrences.

1) \(\mathbb{E}\left[f\left(A_{i}\right)-f\left(A_{i-1}\right)\right]\geq\frac{1}{ k}\mathbb{E}\left[f\left(O\cup A_{i-1}\right)-2f\left(A_{i-1}\right)\right],\)
2) \(\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\geq\left(1-\frac{2}{k}\right) \mathbb{E}\left[f\left(O\cup A_{i-1}\right)\right]+\frac{1}{k}\mathbb{E}\left[ f\left(O\right)+f\left(O\cup A_{i-1}\cup M_{i}\right)\right].\)

Fig. 4(a) depicts the worse-case behavior of \(\mathbb{E}\left[f\left(A_{i}\right)\right]\) and \(\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\). As discussed in Section 2.2, we consider improving the degradation of \(\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\) by selecting elements from outside of an \((\alpha+\varepsilon,\alpha)\)-guidance set \(Z\) to enhance the lower bound of \(\mathbb{E}\left[f\left(O\cup A_{i-1}\cup M_{i}\right)\right]\). The blue line in Fig. 4(b) illustrated the improvement of \(\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\) with an \((\alpha+\varepsilon,\alpha)\)-guidance set. However, restricting the selection only to elements outside \(Z\) restricts the increase in \(\mathbb{E}\left[f\left(A_{i}\right)\right]\) to the difference between \(\mathbb{E}\left[f\left((O\setminus Z)\cup A_{i-1}\right)\right]\) and \(\mathbb{E}\left[f\left(A_{i-1}\right)\right]\). This restriction is illustrated by the red line in Figure 4(b), indicating degradation in \(\mathbb{E}\left[f\left((O\setminus Z)\cup A_{i}\right)\right]\).

To benefit from the improved degradation of \(\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\), we consider transitioning to selecting elements from the whole ground set at a suitable point. The blue line in Fig. 4(b) illustrates how \(\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\) degrades before and after we switch, and the orange line illustrates the evolution of \(\mathbb{E}\left[f\left(A_{i}\right)\right]\). Even starting with an inferior selection at the first stage, we still get an overall improvement on the objective value.

We provide the updated recursions for \(f\left((O\setminus Z)\cup A_{i}\right)\), \(f\left(O\cup A_{i}\right)\) and \(f\left(A_{i}\right)\) in Lemma C.5 and C.6 below. Then, the closed form of the solution value, derived from these lemmata, is presented in Lemma C.7. After that, we prove the approximation ratio of the randomized algorithm under matroid constraint.

**Lemma C.5**.: When \(0<i\leq t\cdot k\), it holds that

\[\mathbb{E}\left[f\left((O\setminus Z)\cup A_{i}\right)\right]\geq\left(1-\frac{ 2}{k}\right)\mathbb{E}\left[f\left((O\setminus Z)\cup A_{i-1}\right)\right]+ \frac{1}{k}\left[2f\left(O\setminus Z\right)-f\left(O\cup Z\right)\right],\]

\[\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\geq\left(1-\frac{2}{k}\right) \mathbb{E}\left[f\left(O\cup A_{i-1}\right)\right]+\frac{1}{k}\left[2f\left(O \right)-f\left(O\cup Z\right)\right].\]

When \(t\cdot k<i\leq k\), it holds that

\[\mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\geq\left(1-\frac{2}{k}\right) \mathbb{E}\left[f\left(O\cup A_{i-1}\right)\right]+\frac{1}{k}f\left(O\right)\]Proof.: When \(i\leq tk\), it holds that

\[\mathbb{E}\left[f\left(\left(O\setminus Z\right)\cup A_{i}\right) \mid A_{i-1}\right]=\frac{1}{k}\sum_{x\in M_{i}}f\left(\left(O\setminus Z \right)\cup\left(A_{i-1}+x-\sigma_{i}(x)\right)\right)\] \[\geq\frac{1}{k}\sum_{x\in M_{i}}\left[\Delta(x|\left(O\setminus Z \right)\cup A_{i-1})+f\left(\left(O\setminus Z\right)\cup\left(A_{i-1}-\sigma_ {i}(x)\right)\right)\right]\] (submodularity) \[\geq\frac{1}{k}\left[f\left(\left(O\setminus Z\right)\cup A_{i-1} \cup M_{i}\right)-f\left(\left(O\setminus Z\right)\cup A_{i-1}\right)+\left(k- 1\right)f\left(\left(O\setminus Z\right)\cup A_{i-1}\right)+f\left(O\setminus Z \right)\right]\] (submodularity) \[\geq\frac{1}{k}\left[\left(k-2\right)f\left(\left(O\setminus Z \right)\cup A_{i-1}\right)+2f\left(O\setminus Z\right)-f\left(O\cup Z\right) \right]\] (submodularity; nonnegativity) \[\mathbb{E}\left[f\left(O\cup A_{i}\right)\mid A_{i-1}\right]= \frac{1}{k}\sum_{x\in M_{i}}f\left(O\cup\left(A_{i-1}+x-\sigma_{i}(x)\right)\right)\] \[\geq\frac{1}{k}\sum_{x\in M_{i}}\left[\Delta(x|O\cup A_{i-1})+f \left(O\cup\left(A_{i-1}-\sigma_{i}(x)\right)\right)\right]\] (submodularity) \[\geq\frac{1}{k}\left[f\left(O\cup A_{i-1}\cup M_{i}\right)-f \left(O\cup A_{i-1}\right)+\left(k-1\right)f\left(O\cup A_{i-1}\right)+f\left(O \right)\right]\] (submodularity) \[\geq\frac{1}{k}\left[\left(k-2\right)f\left(O\cup A_{i-1}\right)+2 f\left(O\right)-f\left(O\cup Z\right)\right]\] (submodularity; nonnegativity)

When \(i>tk\), it holds that

\[\mathbb{E}\left[f\left(O\cup A_{i}\right)\mid A_{i-1}\right]= \frac{1}{k}\sum_{x\in M_{i}}f\left(O\cup\left(A_{i-1}+x-\sigma_{i}(x)\right)\right)\] \[\geq\frac{1}{k}\sum_{x\in M_{i}}\left[\Delta(x|O\cup A_{i-1})+f \left(O\cup\left(A_{i-1}-\sigma_{i}(x)\right)\right)\right]\] (submodularity) \[\geq\frac{1}{k}\left[f\left(O\cup A_{i-1}\cup M_{i}\right)-f \left(O\cup A_{i-1}\right)+\left(k-1\right)f\left(O\cup A_{i-1}\right)+f\left( O\right)\right]\] (submodularity) \[\geq\frac{1}{k}\left[\left(k-2\right)f\left(O\cup A_{i-1}\right)+f \left(O\right)\right]\] (nonnegativity)

By unconditioning \(A_{i-1}\), the lemma is proved.

Figure 4: This set of figures indicates how guiding benefits RandomGreedy under matroid constraints. The figure (a) depicts the evolution of \(f\left(O\cup A_{i}\right)\) and \(f\left(A_{i}\right)\) with RandomGreedy. The figure (b) illustrates how the degradation of \(f\left(O\cup A_{i}\right)\) changes as we introduce an \(\left(0.305+\varepsilon,0.305\right)\)-guidance set. Additionally, we also need to consider the degradation of \(f\left(\left(O\setminus Z\right)\cup A_{i}\right)\), which is the value that the solution approaches with the guidance. The figure (c) shows the updated degradation with a switch point \(tk\), where the algorithm starts with guidance and then switches to running without guidance. It demonstrates that even though the value of \(A_{i}\) decreases initially when the selection starts outside of \(Z\), it benefits from the improved degradation of \(f\left(O\cup A_{i}\right)\) upon switching back to the original algorithm.

**Lemma C.6**.: When \(0<i\leq t\cdot k\), it holds that

\[\mathbb{E}\left[f\left(A_{i}\right)\right]-\mathbb{E}\left[f\left(A_{i-1}\right) \right]\geq\frac{1}{k}\left(\mathbb{E}\left[f\left(\left(O\setminus Z\right) \cup A_{i-1}\right)\right]-2\mathbb{E}\left[f\left(A_{i-1}\right)\right]\right).\]

When \(t\cdot k<i\leq k\), it holds that

\[\mathbb{E}\left[f\left(A_{i}\right)\right]-\mathbb{E}\left[f\left(A_{i-1} \right)\right]\geq\frac{1}{k}\left(\mathbb{E}\left[f\left(O\cup A_{i-1}\right) \right]-2\mathbb{E}\left[f\left(A_{i-1}\right)\right]\right).\]

Proof.: Given \(A_{i-1}\) at iteration \(i\). When \(i\leq t\cdot k\), since \(O\) is a base, \(O\setminus\left(A_{i-1}\cup Z\right)\) with dummy elements is also a base. It holds that

\[\mathbb{E}\left[f\left(A_{i}\right)-f\left(A_{i-1}\right)\mid A_{ i-1}\right]=\frac{1}{k}\sum_{x\in M_{i}}\left[f\left(A_{i-1}+x-\sigma_{i}(x) \right)-f\left(A_{i-1}\right)\right]\] \[\geq\frac{1}{k}\sum_{x\in M_{i}}\left[\Delta(x|A_{i-1})+f\left(A_ {i-1}-\sigma_{i}(x)\right)-f\left(A_{i-1}\right)\right]\] (submodularity) \[\geq\frac{1}{k}\sum_{x\in O\setminus\left(A_{i-1}\cup Z\right)} \Delta(x|A_{i-1})+\frac{1}{k}\sum_{x\in M_{i}}\left[f\left(A_{i-1}-\sigma_{i}( x)\right)-f\left(A_{i-1}\right)\right]\] (Line 5 in Alg. 6) \[\geq\frac{1}{k}\left[f\left(\left(O\setminus Z\right)\cup A_{i-1} \right)-f\left(A_{i-1}\right)\right]-\frac{1}{k}f\left(A_{i-1}\right).\] (submodularity)

When \(i>t\cdot k\), it holds that

\[\mathbb{E}\left[f\left(A_{i}\right)-f\left(A_{i-1}\right)\mid A_ {i-1}\right]=\frac{1}{k}\sum_{x\in M_{i}}\left[f\left(A_{i-1}+x-\sigma_{i}(x) \right)-f\left(A_{i-1}\right)\right]\] \[\geq\frac{1}{k}\sum_{x\in M_{i}}\left[\Delta(x|A_{i-1})+f\left(A_ {i-1}-\sigma_{i}(x)\right)-f\left(A_{i-1}\right)\right]\] (submodularity) \[\geq\frac{1}{k}\sum_{x\in O}\Delta(x|A_{i-1})+\frac{1}{k}\sum_{x \in M_{i}}\left[f\left(A_{i-1}-\sigma_{i}(x)\right)-f\left(A_{i-1}\right)\right]\] (Line 6 in Alg. 6) \[\geq\frac{1}{k}\left[f\left(O\cup A_{i-1}\right)-f\left(A_{i-1} \right)\right]-\frac{1}{k}f\left(A_{i-1}\right).\] (submodularity)

**Lemma C.7**.: With an input matroid constraint \(\mathcal{I}\) and a \(((1+\varepsilon)\alpha,\alpha)\)-guidance set \(Z\), GuidedRG returns set \(A_{k}\) with \(\mathcal{O}\left(kn\right)\) queries, _s.t._\(\mathbb{E}\left[f\left(A_{k}\right)\right]\geq\frac{1}{2}\left(\frac{1}{2}+ \left(\frac{3}{2}-t-\frac{1}{k}\right)\left(1-\frac{2}{k}\right)e^{2\left(t-1 \right)}-e^{-2}-\left(1+\varepsilon\right)\alpha\left(\left(1-\frac{2}{k} \right)^{2}e^{2\left(t-1\right)}-e^{-2}\right)\right.\]

Proof.: It follows from Lemma C.5 and the closed form for a recurrence provided in Lemma A.3 that

\[\begin{cases}\mathbb{E}\left[f\left(\left(O\setminus Z\right)\cup A_{i} \right)\right]\geq f\left(O\setminus Z\right)-\frac{1}{2}\left(1-\left(1- \frac{2}{k}\right)^{i}\right)f\left(O\cup Z\right),&\forall 0<i\leq tk\\ \mathbb{E}\left[f\left(O\cup A_{i}\right)\right]\geq\frac{1}{2}\left(1+ \left(1-\frac{2}{k}\right)^{i-\left

[MISSING_PAGE_FAIL:23]

The algorithm for the size constraint closely resembles InterlaceGreedy in Chen and Kuhnle [11]. Hence, we provide the pseudocode (Alg. 9), guarantees, and analysis in Appendix D.1.1. In this section, we focus on presenting GuidedIG-M for matroid constraints as Alg. 8. This algorithm addresses the feasibility issue by incorporating InterlaceGreedy into matroid constraints. Moreover, while it compromises the approximation ratio over size constraint to some extent, it no longer has the drawback of low success probability, which the size-constrained version has.

```
ProcedureGuidedIG-M (\(f\), \(\mathcal{I}(\mathcal{M}),Z,G,\ell\)): Input: oracle \(f\), matroid constraint \(\mathcal{M}\), guidance set \(Z\), starting set \(G\), set size \(\ell\)
1 Initialize:\(A,A_{1},\ldots,A_{\ell}\leftarrow\emptyset\)for\(i\gets 1\) to \(k\)do
2\(X_{i}\leftarrow\{x\in\mathcal{U}\setminus(G\cup A\cup Z):A+x\in\mathcal{I}( \mathcal{M})\}\)\(j_{i}^{*},a_{i}^{*}\leftarrow\arg\max_{j\in[\ell],x\in X}\Delta(x|G\cup A_{j})\)\(A\gets A+a_{i}^{*}\), \(A_{j_{i}^{*}}\gets A_{j_{i}^{*}}+a_{i}^{*}\)
3 end for
4\(\sigma\leftarrow\) a bijection from \(G\) to \(A\)_s.t._\((G\cup A_{j})\setminus\left(\sum_{x\in A_{j}}\sigma^{-1}(x)\right)\) is a basis return\(\left\{(G\cup A_{j})\setminus\left(\sum_{x\in A_{j}}\sigma^{-1}(x)\right):1 \leq j\leq\ell\right\}\)
```

**Algorithm 8**A guided InterlaceGreedy subroutine for matroid constraints.

**Algorithm overview.** Under size constraints, InterpolatedGreedy[11] constructs the solution with \(\ell\) iterations, where each iteration involves calling the InterlaceGreedy subroutine and adding \(k/\ell\) elements into the solution. However, this approach is not applicable to matroid constraint due to the feasibility problem. Consequently, we propose GuidedIG-M for matroid constraints designed as follows: 1) consider adding a basis (\(k\) elements) \(A\) to \(\ell\) solution sets, where each addition dominates the gain of a distinct element in \(O\); 2) by exchange property, establish a bijection between the basis \(A\) and the starting set \(G\); 3) delete elements in each solution set that are mapped to by the basis \(A\). This procedure avoids the extensive guessing of GuidedIG-S for size constraints and reduces the number of potential solutions from \(\ell(\ell+1)\) to \(\ell\). We provide the theoretical guarantees and the detailed analysis below.

**Lemma D.1**.: Let \(O\in\mathcal{I}(\mathcal{M})\), and suppose GuidedIG-M(Alg. 8) is called with \((f,\mathcal{M},Z,G,\ell)\), where \(Z\cap G=\emptyset\). Then GuidedIG-M outputs \(\ell\) candidate sets with \(\mathcal{O}\left(\ell kn\right)\) queries. Moreover, a randomly selected set \(G^{\prime}\) from the output satisfies that:

1) \(\mathbb{E}\left[f(G^{\prime})\right]\geq\left(1-\frac{2}{\ell}\right)f(G)+\frac {1}{\ell+1}\left(1-\frac{1}{\ell}\right)f((O\setminus Z)\cup G)\);
2) \(\mathbb{E}\left[f(O\cup G^{\prime})\right]\geq\left(1-\frac{2}{\ell}\right)f(O \cup G)+\frac{1}{\ell}\left(f(O)+f\left(O\cup(Z\cap G)\right)-f(O\cup Z)\right)\);
3) \(\mathbb{E}\left[f\left((O\setminus Z)\cup G^{\prime})\right]\geq\left(1-\frac {2}{\ell}\right)f((O\setminus Z)\cup G)+\frac{1}{\ell}\left(f(O\setminus Z)+f \left((O\setminus Z)\cup(Z\cap G)\right)-f(O\cup Z)\right)\).

Proof.: \(A=\{a_{1}^{*},a_{2}^{*},\ldots,a_{k}^{*}\}\) be the sequence with the order of elements being added. Since \(A\) and \(O\setminus Z\) are basis of \(\mathcal{M}\) (by adding dummy elements into \(O\setminus Z\)), we can order \(O\setminus Z=\{o_{1},o_{2},\ldots,o_{k}\}\)_s.t._ for any \(1\leq i\leq k\), \(\{a_{1}^{*},\ldots,a_{i-1}^{*},o_{i}\}\) is an independent set. Thus, \(o_{i}\in X_{i}\). Let \(A_{j}^{(i)}\) be \(A_{j}\) after \(i\)-th iteration. Therefore, for any \(1\leq j\leq\ell\), by submodularity,

\[\Delta(o_{i}|G\cup A_{j})\leq\Delta\Big{(}o_{i}|G\cup A_{j}^{(i-1)}\Big{)}\leq \Delta\Big{(}a_{i}^{*}|G\cup A_{j_{i}^{*}}^{(i-1)}\Big{)}\]

\[\Rightarrow f\left((O\setminus Z)\cup G\cup A_{j}\right)-f\left(G\cup A_{j} \right)\leq\sum_{i=1}^{k}\Delta(o_{i}|G\cup A_{j})\leq\sum_{i=1}^{k}\Delta\Big{(} a_{i}^{*}|G\cup A_{j_{i}^{*}}^{(i-1)}\Big{)}=\sum_{l=1}^{\ell}\Delta(A_{l}|G)\]

By summing up the above inequality with \(1\leq j\leq\ell\),

\[(\ell+1)\sum_{j=1}^{\ell}\Delta(A_{j}|G)\geq\sum_{j=1}^{\ell}f\left((O\setminus Z )\cup G\cup A_{j}\right)-\ell f(G)\]

[MISSING_PAGE_FAIL:25]

Proof.: Let \(o_{\text{max}}=\arg\max_{o\in O\setminus(G\cup Z)}\Delta(o|G)\), and let \(\{a_{1},\ldots,a_{\ell}\}\) be the largest \(\ell\) elements of \(\{\Delta(x|G):x\in\mathcal{U}\setminus(G\cup Z)\}\), as chosen on Line 3. We consider the following two cases.

**Case \((O\setminus(G\cup Z))\cap\{a_{1},\ldots,a_{\ell}\}=\emptyset\).** Then, \(o_{\text{max}}\not\in\{a_{1},\ldots,a_{\ell}\}\) which implies that \(\Delta(a_{u}|G)\geq\Delta(o|G)\), for every \(1\leq u\leq\ell\) and \(o\in O\setminus(G\cup Z)\); and, after the first iteration of the **for** loop on Line 10 of Alg. 9, none of the elements in \(O\setminus(G\cup Z)\) is added into any of \(\{A_{0,i}\}_{i=1}^{\ell}\). We will analyze the iteration of the **for** loop on Line 4 with \(u=0\).

Since none of the elements in \(O\setminus(G\cup Z)\) is added into the collection when \(j=0\), we can order \(O\setminus(G\cup Z)=\{o_{1},o_{2},\ldots\}\) such that the first \(\ell\) elements are not selected in any set before we get to \(j=1\), the next \(\ell\) elements are not selected in any set before we get to \(j=2\), and so on. Let \(i\in\{1,\ldots,\ell\}\). Let \(A_{0,i}^{j}\) be the value of \(A_{0,i}\) after \(j\) elements are added into it, and define \(A_{0,i}=A_{0,i}^{k/\ell}\), the final value. Finally, denote by \(\delta_{j}\) the value \(\Delta\Big{(}x_{j,i}|A_{0,i}^{j}\Big{)}\). Then,

\[f\left((O\setminus Z)\cup A_{0,i}\right)-f\left(A_{0,i}\right) \leq\sum_{o\in O\setminus(A_{0,i}\cup Z)}\Delta(o|A_{0,i})\] (submodularity) \[\leq\sum_{o\in O\setminus(G\cup Z)}\Delta(o|A_{0,i})\] ( \[G\subseteq A_{0,i}\] ) \[\leq\sum_{l=1}^{\ell}\Delta\big{(}o_{l}|A_{0,i}^{0}\big{)}+\sum_{ l=\ell+1}^{2\ell}\Delta\big{(}o_{l}|A_{0,i}^{1}\big{)}+\ldots\] (submodularity) \[\leq\ell\sum_{j=1}^{k/\ell}\delta_{j}=\ell(f\left(A_{0,i}\right) -f\left(G\right)),\]

where the last inequality follows from the ordering of \(O\) and the selection of elements into the sets. By summing up the above inequality with all \(1\leq i\leq\ell\), it holds that,

\[\mathbb{E}\left[f(A)\right]=\frac{1}{\ell}\sum_{i=1}^{\ell}f\left( A_{0,i}\right)\geq\frac{1}{\ell(\ell+1)}\sum_{i=1}^{\ell}f((O\setminus Z) \cup A_{0,i})+\frac{\ell}{\ell+1}f(G)\] \[=\frac{1}{\ell+1}\mathbb{E}\left[f\left((O\setminus Z)\cup A \right)\right]+\frac{\ell}{\ell+1}f\left(G\right),\]Since \(A_{0,i_{1}}\cap A_{0,i_{2}}=G\) for any \(1\leq i_{1}\neq i_{2}\leq\ell\), and each \(x_{j,i}\) is selected outside of \(Z\), by repeated application of submodularity, it can be shown that

\[\mathbb{E}\left[f\left(\left(O\setminus Z\right)\cup A\right) \right]=\frac{1}{\ell}\sum_{i=1}^{\ell}f(\left(O\setminus Z\right)\cup A_{0,i})\] \[\geq\left(1-\frac{1}{\ell}\right)f\left(\left(O\setminus Z \right)\cup G\right)+\frac{1}{\ell}f\left(\left(O\setminus Z\right)\cup\left( \cup_{i=1}^{\ell}A_{0,i}\right)\right)\] \[\geq\left(1-\frac{1}{\ell}\right)f\left(\left(O\setminus Z \right)\cup G\right)+\frac{1}{\ell}\left(f\left(\left(O\setminus Z\right)\cup \left(Z\cap G\right)\right)-f(O\cup Z)\right)\]

\[\mathbb{E}\left[f\left(O\cup A\right)\right]=\frac{1}{\ell}\sum_{i=1}^{\ell}f( O\cup A_{0,i})\geq\left(1-\frac{1}{\ell}\right)f\left(O\cup G\right)+\frac{1}{ \ell}f\left(O\cup\left(\cup_{i=1}^{\ell}A_{0,i}\right)\right)\]

Therefore, if we select a random set from \(\left\{A_{0,i}:1\leq i\leq\ell\right\}\), the three inequalities in the Lemma hold and we have probability \(1/(\ell+1)\) of this happening.

**Case \(\left(O\setminus\left(G\cup Z\right)\right)\cap\left\{a_{1},\ldots,a_{\ell} \right\}\neq\emptyset\).** Then \(o_{\text{max}}\in\left\{a_{1},\ldots,a_{\ell}\right\}\), so \(a_{u}=o_{\text{max}}\), for some \(u\in 1,\ldots,\ell\). We analyze the iteration \(u\) of the **for** loop on Line 4. Similar to the previous case, let \(i\in\left\{1,\ldots,\ell\right\}\), define \(A_{u,i}^{j}\) be the value of \(A_{u,i}\) after we add \(j\) elements into it, and we will use \(A_{u,i}\) for \(A_{u,i}^{k/\ell}\), Also, let \(\delta_{j}=\Delta\Big{(}x_{j,i}|A_{u,i}^{j-1}\Big{)}\). Finally, let \(x_{1,i}=a_{u}\) and observe \(A_{u,i}^{1}=G\cup\left\{a_{u}\right\}\), for any \(i\in\left\{1,\ldots,\ell\right\}\).

Then, we can order \(O\setminus G=\left\{o_{1},o_{2},\ldots\right\}\) such that: 1) for the first \(\ell\) elements \(\left\{o_{l}\right\}_{l=1}^{\ell}\), \(\Delta(o_{\text{max}}|G)\leq\Delta(o_{\text{max}}|G)=\delta_{1}\); 2) the next \(\ell\) elements \(\left\{o_{l}\right\}_{l=\ell+1}^{2\ell}\) are not selected by any set before we get to \(j=2\), which implies that \(\Delta\big{(}o_{l}|A_{u,i}^{1}\big{)}\leq\delta_{2}\), and so on. Therefore, analagous to the the previous case, we have that

\[\mathbb{E}\left[f\left(A\right)\right]\geq\frac{1}{\ell+1}\mathbb{E}\left[f \left(\left(O\setminus Z\right)\cup A\right)\right]+\frac{\ell}{\ell+1}f \left(G\right) \tag{7}\]

Since, \(A_{u,i_{1}}\cap A_{u,i_{2}}=G\cup\left\{a_{u}\right\}\) for any \(1\leq i_{1}\neq i_{2}\leq\ell\), \(a_{u}\in O\setminus Z\), and each \(x_{j,i}\) is selected outside of \(Z\), by submodularity and nonnegativity of \(f\), it holds that

\[\mathbb{E}\left[f\left(\left(O\setminus Z\right)\cup A\right) \right]=\frac{1}{\ell}\sum_{i=1}^{\ell}f(\left(O\setminus Z\right)\cup A_{u,i})\] \[\geq\left(1-\frac{1}{\ell}\right)f\left(\left(O\setminus Z\right) \cup G\right)+\frac{1}{\ell}f\left(\left(O\setminus Z\right)\cup\left(\cup_{ i=1}^{\ell}A_{u,i}\right)\right)\] \[\geq\left(1-\frac{1}{\ell}\right)f\left(\left(O\setminus Z\right) \cup G\right)+\frac{1}{\ell}f\left(\left(O\setminus Z\right)\cup\left(Z\cap G \right)\right)-f(O\cup Z)\right)\]

\[\mathbb{E}\left[f\left(O\cup A\right)\right]=\frac{1}{\ell}\sum_{i=1}^{\ell}f( O\cup A_{u,i})\geq\left(1-\frac{1}{\ell}\right)f\left(O\cup G\right)+\frac{1}{ \ell}f\left(O\cup\left(\cup_{i=1}^{\ell}A_{u,i}\right)\right)\]

Therefore, if we select a random set from \(\left\{A_{u,i}:1\leq i\leq\ell\right\}\), the three inequalities in the lemma holds, and this happens with probability \((\ell+1)^{-1}\). 

### Randomized Version of our Deterministic Algorithm

In this section, we provide the randomized version (Alg 10) of our deterministic algorithm (Alg. 11, provided in Appendix D). The deterministic version simply evaluates all possible paths and returns the best solution. In the following, we provide the theoretical guarantee and its analysis under different constraints.

**Theorem D.3**.: Let \((f,\mathcal{I})\) be an instance of SM, with the optimal solution set \(O\). Algorithm 10 achieves an expected \((0.385-\varepsilon)\) approximation ratio with \((\ell+1)^{-\ell}\) success probability and input \(t=0.372\) under size constraint, where \(\ell=\frac{10}{9\varepsilon}\). Moreover, it achieves an expected \((0.305-\varepsilon)\) approximation ratio with \(t=0.559\) under matroid constraint. The query complexity of the algorithm is \(\mathcal{O}\left(kn/\varepsilon\right)\).

```
1Input: oracle \(f\), constraint \(\mathcal{I}\), an approximation result \(Z_{0}\), switch point \(t\), error rate \(\varepsilon\)
2\(Z\leftarrow\textsc{FastLS}(f,\mathcal{I},Z_{0},\varepsilon)\)
3Initialize\(\ell\leftarrow\frac{10}{9\varepsilon},A_{0}\leftarrow\emptyset\)
4if\(\mathcal{I}\) is a size constraintthen
5for\(i\gets 1\)to\(\ell\)do
6if\(i\leq t\ell\)then\(A_{i}\leftarrow\) a random set in GuidedIG-S\((f,\mathcal{I},Z,A_{i-1},\ell)\)
7else\(A_{i}\leftarrow\) a random set in GuidedIG-S\((f,\mathcal{I},\emptyset,A_{i-1},\ell)\)
8
9 end if
10
11else
12for\(i\gets 1\)to\(\ell\)do
13if\(i\leq t\ell\)then\(A_{i}\leftarrow\) a random set in GuidedIG-M\((f,\mathcal{I},Z,A_{i-1},\ell)\)
14else\(A_{i}\leftarrow\) a random set in GuidedIG-M\((f,\mathcal{I},\emptyset,A_{i-1},\ell)\)
15
16 end if
17
18 end for return\(A^{*}\leftarrow\arg\max\{f(Z),f(A_{\ell})\}\)
```

**Algorithm 10**The randomized algorithm suitable for derandomization.

#### d.2.1 Size constraints

By Lemma D.2 in Appendix D.1.1 and the closed form for a recurrence provided in Lemma A.3, the following corollary holds,

**Corollary D.1**.: After iteration \(i\) of the for loop in Alg. 10, the following inequalities hold with a probability of \((\ell+1)^{-i}\)

\[\mathbb{E}\left[f\left(A_{i}\right)\right] \geq\frac{\ell}{\ell+1}\mathbb{E}\left[f\left(A_{i-1}\right)\right] +\frac{1}{\ell+1}\left(f\left(O\setminus Z\right)-\left(1-\left(1-\frac{1}{ \ell}\right)^{i}\right)f\left(O\cup Z\right)\right), 1\leq i\leq t\ell\] \[\mathbb{E}\left[f\left(A_{i}\right)\right] \geq\frac{\ell}{\ell+1}\mathbb{E}\left[f\left(A_{i-1}\right)\right] +\frac{1}{\ell+1}\left(1-\frac{1}{\ell}\right)^{i-\left\lfloor t\ell\right\rfloor }\left(f(O)-\left(1-\left(1-\frac{1}{\ell}\right)^{\left\lfloor t\ell\right\rfloor }\right)f\left(O\cup Z\right)\right), t\ell<i\leq\ell\]

Proof of approximation ratio.: If \(f\left(Z\right)\geq\left(0.385-\varepsilon\right)f\left(O\right)\), the approximation ratio holds immediately. So, we analyze the case \(f\left(Z\right)<\left(0.385-\varepsilon\right)f\left(O\right)\) in the following.

Recall in Corollary C.1 that \(Z\) is a \((1+\varepsilon)\alpha,\alpha)\)-guidance set, it holds that \(f\left(O\cup Z\right)+f\left(O\cap Z\right)<(0.77-1.615\varepsilon)f\left(O\right)\) and \(f\left(O\cap Z\right)<(0.385-0.615\varepsilon)f\left(O\right)\).

By repeatedly implementing Lemma A.3 with the recursion in Corollary D.1, it holds that

\[\mathbb{E}\left[f\left(A_{\lfloor t\ell\rfloor}\right)\right] \geq\left(1-\left(1-\frac{1}{\ell+1}\right)^{\lfloor t\ell\rfloor}\right) \left(f\left(O\setminus Z\right)-f\left(O\cup Z\right)\right)+\frac{\left\lfloor t \ell\right\rfloor}{\ell+1}\left(1-\frac{1}{\ell}\right)^{\lfloor t\ell\rfloor }f\left(O\cup Z\right)\] \[\geq\left(1-\left(1-\frac{1}{\ell+1}\right)^{\lfloor t\ell\rfloor }\right)\left(f\left(O\right)-f\left(O\cap Z\right)-f\left(O\cup Z\right) \right)+\frac{\left\lfloor t\ell\right\rfloor}{\ell+1}\left(1-\frac{1}{\ell} \right)^{\lfloor t\ell\rfloor}f\left(O\cup Z\right)\] (submodularity) \[\mathbb{E}\left[f\left(A_{\ell}\right)\right]\geq\left(1-\frac{1 }{\ell+1}\right)^{\ell-\lfloor t\ell\rfloor}\mathbb{E}\left[f\left(A_{\lfloor t \ell\rfloor}\right)\right]+\frac{\ell-\lfloor t\ell\rfloor}{\ell+1}\left(1- \frac{1}{\ell}\right)^{\ell-\lfloor t\ell\rfloor}\left(f\left(O\right)- \left(1-\left(1-\frac{1}{\ell}\right)^{\lfloor t\ell\rfloor}\right)f\left(O \cup Z\right)\right)\] \[\geq\left(\left(1-\frac{1}{\ell+1}\right)^{(1-t)\ell+1}-\left(1- \frac{1}{\ell+1}\right)^{\ell}\right)\left(f\left(O\right)-f\left(O\cap Z \right)-f\left(O\cup Z\right)\right)\]\[\geq\left[c(c^{2}(1-t)+1)e^{t-1}-e^{-1}\right]f\left(O\right)-\left[c(c (1-t)+1)e^{t-1}-(c^{3}+1)e^{-1}\right]\left(f\left(O\cup Z\right)+f\left(O\cap Z \right)\right)\] \[\geq\left[c(c^{2}(1-t)+1)e^{t-1}-e^{-1}\right]f\left(O\cap Z\right) \text{(Let }c=1-\frac{9\epsilon}{10}=1-\frac{1}{\ell}\text{)}\] \[\geq\left[c(c^{2}(1-t)+1)e^{t-1}-e^{-1}\right]f\left(O\right)- \left[c(c(1-t)+1)e^{t-1}-(c^{3}+1)e^{-1}\right]\left(0.77-1.615\epsilon\right)f \left(O\right)\] \[\qquad-\left[c^{3}e^{-1}-c^{2}(1-t)e^{t-1}\right]\left(0.385-0.615 \epsilon\right)f\left(O\right)\] \[\qquad\left(f\left(O\cup Z\right)+f\left(O\cap Z\right)<(0.77-1.615 \epsilon)f\left(O\right)\text{; }f\left(O\cap Z\right)<(0.385-0.615\epsilon)f\left(O\right)\right)\] \[\geq\left(0.385-\epsilon\right)f\left(O\right)\] (0 \[<\epsilon<0.385;t=0.372\] )

#### d.2.2 Matroid Constraints

By Lemma D.1 in Appendix D.1 and the closed form for a recurrence provided in Lemma A.3, the following corollary holds,

**Corollary D.2**.: After iteration \(i\) of the for loop in Alg. 10, the following inequalities hold,

\[\mathbb{E}\left[f\left(A_{i}\right)\right]\geq\left(1-\frac{2}{ \ell}\right)\mathbb{E}\left[f\left(A_{i-1}\right)\right]\] \[\quad+\frac{1}{\ell+1}\left(1-\frac{1}{\ell}\right)\left(f\left(O \setminus Z\right)-\frac{1}{2}\left(1-\left(1-\frac{2}{\ell}\right)^{i-1} \right)f\left(O\cup Z\right)\right),1\leq i\leq t\ell\] \[\mathbb{E}\left[f\left(A_{i}\right)\right]\geq\left(1-\frac{2}{ \ell}\right)\mathbb{E}\left[f\left(A_{i-1}\right)\right]\] \[\quad+\frac{1}{\ell+1}\left(1-\frac{1}{\ell}\right)\left(\frac{1 }{2}\left(1+\left(1-\frac{2}{\ell}\right)^{i-t\ell}\right)f(O)-\frac{1}{2} \left(\left(1-\frac{2}{\ell}\right)^{i-t\ell}-\left(1-\frac{2}{\ell}\right)^{i }\right)f(O\cup Z)\right),t\ell<i\leq\ell\]

Proof of approximation ratio.: If \(f\left(Z\right)\geq(0.305-\varepsilon)f\left(O\right)\), the approximation ratio holds immediately. So, we analyze the case \(f\left(Z\right)<(0.305-\varepsilon)f\left(O\right)\) in the following.

Recall that \(Z\) is a \((1+\varepsilon)\alpha,\alpha)\)-guidance set in Corollary C.1, it holds that \(f\left(O\cup Z\right)+f\left(O\cap Z\right)<(0.61-1.695\varepsilon)f\left(O \right)\) and \(f\left(O\cap Z\right)<(0.305-0.695\varepsilon)f\left(O\right)\).

By repeatedly implementing Lemma A.3 with the recursion in Corollary D.2, it holds that

\[\mathbb{E}\left[f\left(A_{\lfloor\epsilon\ell\rfloor}\right)\right] \geq\frac{\ell-1}{2(\ell+1)}\left[\left(1-\left(1-\frac{2}{\ell}\right)^{ \lfloor\epsilon\ell\rfloor}\right)\left(f\left(O\setminus Z\right)-\frac{1}{2} f\left(O\cup Z\right)\right)+t\left(1-\frac{2}{\ell}\right)^{\lfloor\epsilon\ell \rfloor-1}f\left(O\cup Z\right)\right]\] \[\geq\frac{\ell-1}{2(\ell+1)}\left[\left(1-\left(1-\frac{2}{\ell} \right)^{\lfloor\epsilon\ell\rfloor}\right)\left(f\left(O\right)-f\left(O\cap Z \right)-\frac{1}{2}f\left(O\cup Z\right)\right)+t\left(1-\frac{2}{\ell}\right) ^{\lfloor\epsilon\ell\rfloor-1}f\left(O\cup Z\right)\right]\] (submodularity) \[\mathbb{E}\left[f\left(A_{\ell}\right)\right]\geq\left(1-\frac{2} {\ell}\right)^{\ell-\lfloor\epsilon\ell\rfloor}\mathbb{E}\left[f\left(A_{ \lfloor\epsilon\rfloor}\right)\right]+\frac{\ell-1}{2(\ell+1)}\left[\left(\frac {1}{2}+\left(\frac{1}{2}-t+\frac{1}{\ell}\right)\left(1-\frac{2}{\ell}\right) ^{\ell-\lfloor\epsilon\ell\rfloor-1}\right)f(O)\] \[\qquad-(1-t)\left(\left(1-\frac{2}{\ell}\right)^{\ell-\lfloor \epsilon\ell\rfloor-1}-\left(1-\frac{2}{\ell}\right)^{\ell-1}\right)f(O\cup Z)\] \[\geq\frac{\ell-1}{2(\ell+1)}\left[\left(\frac{1}{2}+\left(\frac{3 }{2}-t-\frac{1}{\ell}\right)\left(1-\frac{2}{\ell}\right)^{\ell-t\ell}-\left(1 -\frac{2}{\ell}\right)^{\ell}\right)f\left(O\right)\right.\] \[\qquad-\left(\left(1-\frac{2}{\ell}\right)^{(1-t)\ell}-\left(1- \frac{2}{\ell}\right)^{\ell}\right)f\left(O\cap Z\right)\] \[\qquad-\left(\left(\frac{1}{2}+\frac{1-t}{1-\frac{2}{\ell}} \right)\left(1-\frac{2}{\ell}\right)^{\ell-t\ell}-\left(\frac{3}{2}-\frac{1}{ \ell}\right)\left(1-\frac{2}{\ell}\right)^{\ell-1}\right)f\left(O\cup Z\right)\] (\[t\ell-1<\lfloor t\ell\rfloor\leq t\ell\] ) \[\geq\frac{\ell-1}{2(\ell+1)}\left[\left(\frac{1}{2}+\left(\frac{3 }{2}-t-\frac{1}{\ell}\right)\left(1-\frac{2}{\ell}\right)e^{2(t-1)}-e^{-2} \right)f\left(O\right)-\left(e^{2(t-1)}-\left(1-\frac{2}{\ell}\right)e^{-2} \right)f\left(O\cap Z\right)\right.\] (Lemma A.4; nonnegativity) \[\geq\frac{\ell-1}{2(\ell+1)}\left[\left(\frac{1}{2}+\left(\frac{3 }{2}-t-\frac{1}{\ell}\right)\left(1-\frac{2}{\ell}\right)e^{2(t-1)}-e^{-2} \right)f\left(O\right)-\left(\left(\frac{1}{2}+\frac{1}{\ell}\right)e^{-2}- \left(\frac{1}{2}-t\right)e^{2(t-1)}\right)f\left(O\cap Z\right)\right.\] \[\qquad-\left(\left(\frac{1}{2}+\frac{1-t}{1-\frac{2}{\ell}} \right)e^{2(t-1)}-\left(\frac{3}{2}-\frac{1}{\ell}\right)e^{-2}\right)\left(f \left(O\cup Z\right)+f\left(O\cap Z\right)\right)\] \[\geq\frac{\ell-1}{2(\ell+1)}\left[\frac{1}{2}+\left(\frac{3}{2}-t -\frac{1}{\ell}\right)\left(1-\frac{2}{\ell}\right)e^{2(t-1)}-e^{-2}-\left(0.3 05-0.695\varepsilon\right)\left(\left(\frac{1}{2}+\frac{1}{\ell}\right)e^{-2} -\left(\frac{1}{2}-t\right)e^{2(t-1)}\right)\right.\] \[\qquad-(0.61-1.695\varepsilon)\left(\left(\frac{1}{2}+\frac{1-t }{1-\frac{2}{\ell}}\right)e^{2(t-1)}-\left(\frac{3}{2}-\frac{1}{\ell}\right)e^{ -2}\right)\right]f\left(O\right)\] \[\geq(0.305-\varepsilon)f\left(O\right) (\ell=\frac{10}{9\varepsilon};0<\varepsilon<0.305;t=0.559)\]

### Derandomize Alg. 10 in Section 2.3

In this section, we present the deranmized version of Alg. 10, which simply evaluates all possible paths and returns the best solution. We reiterate the guarantee as follows.

**Theorem 2.4**.: Let \((f,k)\) be an instance of SM, with the optimal solution set \(O\). Alg. 11 achieves a deterministic \((0.385-\varepsilon)\) approximation ratio with \(t=0.372\), and a deterministic \((0.305-\varepsilon)\) approximation ratio with \(t=0.559\). The query complexity of the algorithm is \(\mathcal{O}\left(kn\ell^{2\ell-1}\right)\) where \(\ell=\frac{10}{9\varepsilon}\).

## Appendix E Proofs for Nearly Linear Time Deterministic Algorithm in Section 3

In this section, we provide the pseudocode and additional analysis of our nearly linear time deterministic algorithm introduced in Section 3. We organize this section as follows: in Appendix E.1, we provide a speedup version of GuidedIG-S which queries \(\mathcal{O}\left(n\log(k)\right)\) times; in Appendix E.2, we analyze the subroutine, Prune; at last, in Appendix E.3, we provide the pseudocode of nearly linear time deterministic algorithm and its analysis.

```
1Input: oracle \(f\), constraint \(\mathcal{I}\), an approximation result \(Z_{0}\), switch point \(t\), error rate \(\varepsilon\)
2\(Z\leftarrow\textsc{FastLS}(f,\mathcal{I},Z_{0},\varepsilon)\)
3Initialize\(\ell\leftarrow\frac{10}{9\varepsilon},G_{0}\leftarrow\{\emptyset\}\)
4if\(Z\) is a size constraintthen
5for\(i\gets 1\) to \(\ell\)do
6\(G_{i}\leftarrow\emptyset\)
7for\(A_{i-1}\in G_{i-1}\)do
8if\(i\leq t\ell\)then
9\(G_{i}\gets G_{i}\cup\textsc{GuidedIG-S}(f,k,Z,A_{i-1},\ell)\)
10else
11\(G_{i}\gets G_{i}\cup\textsc{GuidedIG-S}(f,k,\emptyset,A_{i-1},\ell)\)
12
13 end if
14
15 end for
16
17 end for
18
19 end for return\(A^{*}\leftarrow\arg\max\{f\left(Z\right),f\left(A_{\ell}\right):A_{\ell}\in G_{ \ell}\}\)
```

**Algorithm 11**Deterministic combinatorial approximation algorithm with the same ratio as Alg. 2

### GuidedIG-S Speedup

In this section, we provide the algorithm ThreshGuidedIG, which combines the guiding and descending threshold greedy techniques with InterlaceGreedy[11].

**Lemma E.1**.: Let \(O\subseteq\mathcal{U}\) be any set of size at most \(k\), and suppose ThreshGuidedIG(Alg. 12) is called with \((f,k,Z,G,\ell)\). Then ThreshGuidedIG outputs \(\ell(\ell+1)\) candidate sets with \(\mathcal{O}\left(\ell^{2}n\log(k)/\varepsilon\right)\) queries. Moreover, with a probability of \((\ell+1)^{-1}\), a randomly selected set \(A\) from the output satisfies that:

1. \(\left(\frac{\ell}{1-\varepsilon}+1\right)\mathbb{E}\left[f\left(A\right)\right] \geq\mathbb{E}\left[f\left(\left(O\setminus Z\right)\cup A\right)\right]+\frac{ \ell}{1-\varepsilon}f\left(G\right)-\varepsilon f(O);\)
2. \(\mathbb{E}\left[f(O\cup A)\right]\geq\left(1-\frac{1}{\ell}\right)f(O\cup G)+ \frac{1}{\ell}\left(f\left(O\cup(Z\cap G)\right)-f\left(O\cup Z\right)\right)\)
3. \(\mathbb{E}\left[f\left(\left(O\setminus Z\right)\cup A\right)\right]\geq\left( 1-\frac{1}{\ell}\right)f\left(\left(O\setminus Z\right)\cup G\right)+\frac{1}{ \ell}(f\left(\left(O\setminus Z\right)\cup(Z\cap G)\right)-f(O\cup Z)).\)

Proof.: Let \(o_{\text{max}}=\arg\max_{o\in O\setminus(G\cup Z)}\Delta(o|G)\), and let \(\{a_{1},\ldots,a_{\ell}\}\) be the largest \(\ell\) elements of \(\{\Delta(x|G):x\in\mathcal{U}\setminus(G\cup Z)\}\), as chosen on Line 13. We consider the following two cases.

**Case \((O\setminus(G\cup Z))\cap\{a_{1},\ldots,a_{\ell}\}=\emptyset\)**. Then, \(o_{\text{max}}\not\in\{a_{1},\ldots,a_{\ell}\}\) which implies that \(\Delta(a_{u}|G)\geq\Delta(o|G)\), for every \(1\leq u\leq\ell\) and \(o\in O\setminus(G\cup Z)\); and, after the first iteration of the **while** loop on Line 13, none of the elements in \(O\setminus(G\cup Z)\) is added into any of \(\{A_{0,i}\}_{i=1}^{\ell}\). We will analyze the iteration of the **for** loop on Line 4 with \(u=0\).

For any \(l\in[\ell]\), let \(A_{0,l}^{(j)}\) be \(A_{0,l}\) after we add \(j\) elements into it, \(\tau_{l}^{(j)}\) be \(\tau_{l}\) when we adopt \(j\)-th elements into \(A_{0,l}\), and \(\tau_{l}^{(1)}=M\). By Line 7, it holds that \(A_{0,l}^{(1)}=G\cup\{a_{l}\}\). Since \((O\setminus(G\cup Z))\cup\{a_{1},\ldots,a_{\ell}\}=\emptyset\), and we add elements to each set in turn, we can order \(O\setminus(G\cup Z)=\{o_{1},o_{2},\ldots\}\) such that the first \(\ell\) elements are not selected by any set before we get \(A_{0,l}^{(1)}\), the next \(\ell\) elements are not selected in any set before we get \(A_{0,l}^{(2)}\), and so on. Therefore, for any \(j\leq|A_{0,l}\setminus G|\) and \(\ell(j-1)+1\leq i\leq\ell j\), \(o_{i}\) are filtered out by \(A_{0,l}\) with threshold \(\tau_{l}^{(j)}/(1-\varepsilon)\), which follows that \(\Delta\Big{(}o_{i}|A_{0,l}^{(j)}\Big{)}<\tau_{l}^{(j)}/(1-\varepsilon)\leq(f( A_{0,l}^{(j)})-f(A_{0,l}^{(j-1)}))/(1-\varepsilon)\); for any \(\ell|A_{0,l}\setminus G|<i\leq|O\setminus(G\cup Z)|\), \(o_{i}\) are filtered out by \(A_{0,l}\) with threshold \(\frac{\varepsilon M}{k}\), which follows that \(\Delta(o_{i}|A_{0,l})<\varepsilon M/k\). Thus,

\[f\left((O\setminus Z)\cup A_{0,l}\right)-f\left(A_{0,l}\right) \leq\sum_{o\in O\setminus A_{0,l}}\Delta(o|A_{0,l})\] (submodularity) \[\leq\sum_{o\in O\setminus(G\cup Z)}\Delta(o|A_{0,l}) (G\subseteq A_{0,l})\]

[MISSING_PAGE_FAIL:33]

Proof.: Let \(A_{i}\) be \(A\) after we delete \(i\)-th element \(x_{i}\), \(A_{0}\) be the input set \(A\), \(A_{m}\) be the output set \(A^{\prime}\). Since any element \(x_{i}\) being deleted follows that \(\Delta(x_{i}|A_{i})<0\), it holds that \(f\left(A_{i}\right)>f\left(A_{i}+x_{i}\right)=f\left(A_{i-1}\right)\). Therefore, \(f\left(A^{\prime}\right)=f\left(A_{m}\right)>\ldots>f\left(A_{0}\right)=f\left( A\right)\). The first inequality holds.

For any \(x_{i}\in A\setminus A^{\prime}\), it holds that \(\Delta(x_{i}|A_{i})<0\). By submodularity,

\[f\left(S\cup A\right)-f\left(S\cup A^{\prime}\right)=\sum_{i=1}^{m}\Delta(x_{i }|S\cup A_{i})\leq\sum_{i=1}^{m}\Delta(x_{i}|A_{i})<0.\]

The second inequality holds.

For any \(y\in A^{\prime}\setminus T\), since it is not deleted, there exists \(0\leq i_{y}\leq m\) such that \(\Delta\big{(}y|A_{i_{y}}\big{)}\geq 0\). By submodularity,

\[f\left(A^{\prime}\right)-f\left(T\right)\geq\sum_{y\in A^{\prime}\setminus T} \Delta(y|A^{\prime})\geq\sum_{y\in A^{\prime}\setminus T}\Delta\big{(}y|A_{i_ {y}}\big{)}\geq 0.\]

The third inequality holds. 

### Proofs for Theorem 3.1 of Alg. 14

Prior to delving into the proof of Theorem 3.1, we provide the following corollary first. It demonstrates the progression of the intermediate solution of ThreshGuidedIG, after the pruning process, relying on Lemma E.2 and E.1.

**Corollary E.1**.: Let \(O\subseteq\mathcal{U}\) be any set of size at most \(k\). Then \(\textsc{Prune}(\textsc{ThreshGuidedIG}(f,k,\emptyset,G,\ell))\) outputs \(\ell(\ell+1)\) candidate sets with \(\mathcal{O}\left(\ell^{2}n\log(k)/\varepsilon\right)\) queries. Moreover, with a probability of \((\ell+1)^{-1}\), a randomly selected set \(A\) from the output satisfies that:

\[1)\ \left(\frac{\ell}{1-\varepsilon}+1\right)\mathbb{E}\left[f \left(A\right)\right]\geq\left(1-\frac{1}{\ell}\right)f(O\cup G)+\frac{\ell}{1 -\varepsilon}f\left(G\right)-\varepsilon f(O);\]

\[2)\ \mathbb{E}\left[f(O\cup A)\right]\geq\left(1-\frac{1}{\ell}\right)f(O \cup G);\]

\[3)\ f(O\cap A)\leq f(A).\]

**Theorem 3.1**.: Let \((f,k)\) be an instance of SM, with the optimal solution set \(O\). Algorithm 14 achieves a deterministic \((0.377-\varepsilon)\) approximation ratio with \(\mathcal{O}(n\log(k)\ell_{1}{}^{2\ell_{1}}\ell_{2}{}^{2\ell_{2}-1})\) queries, where \(\ell_{1}=\frac{10}{3\varepsilon}\) and \(\ell_{2}=\frac{5}{\varepsilon}\).

Proof.: Following the proof of Theorem 3.1, we consider two cases of the algorithm.

**Case 1.** For every \(A\in Z\), it holds that \(f(O\cup A)\geq 0.46f(O)\). Then, we prove that \(\max_{C\in Z}f\left(C\right)\geq(0.377-\varepsilon)f\left(O\right)\).

In the following, we prove the theorem by analyzing the random case of the algorithm, where we randomly select a set from the output of Prune (ThreshGuidedIG). Suppose that we successfullly select a set where the inequalities in Corollary E.1 hold. Let \(A_{i}\) and \(A_{i-1}\) be random sets in and \(Z_{i-1}\), respectively. Let \(\left(1-\frac{1}{\ell_{1}}\right)^{i^{*}-1}>0.46\geq\left(1-\frac{1}{\ell_{1}} \right)^{i^{*}}\). Then, by Inequality (2) in Corollary E.1, when \(i<i^{*}\), \(\mathbb{E}\left[f(O\cup A_{i})\right]\geq\left(1-\frac{1}{\ell_{1}}\right)^{i} f(O)\); when \(i\geq i^{*}\), \(f(O\cup A_{i})\geq 0.46f(O)\) by assumption. By applying Inequality (1) in Corollary E.1,

\[\mathbb{E}\left[f\left(A_{i^{*}}\right)\right] \geq\left[\frac{i^{*}}{\frac{\ell_{1}}{1-\varepsilon^{\prime}}+1} \left(1-\frac{1}{\ell_{1}}\right)^{i^{*}}-\varepsilon^{\prime}\left(1-\left(1- \frac{1}{\frac{\ell_{1}}{1-\varepsilon^{\prime}}+1}\right)^{i^{*}}\right) \right]f\left(O\right)\] \[\mathbb{E}\left[f\left(A_{\ell_{1}}\right)\right] \geq\left(1-\frac{1}{\frac{\ell_{1}}{1-\varepsilon^{\prime}}+1} \right)^{\ell_{1}-i^{*}}\mathbb{E}\left[f\left(A_{i^{*}}\right)\right]+\left( 1-\left(1-\frac{1}{\frac{\ell_{1}}{1-\varepsilon^{\prime}}+1}\right)^{\ell_{1 }-i^{*}}\right)(0.46-\varepsilon^{\prime})f(O)\] \[\geq\left[\frac{i^{*}}{\frac{\ell_{1}}{1-\varepsilon^{\prime}}+1} \left(1-\frac{1}{\ell_{1}}\right)^{i^{*}}\left(1-\frac{1}{\frac{\ell_{1}}{1- \varepsilon^{\prime}}+1}\right)^{\ell_{1}-i^{*}}+\left(1-\left(1-\frac{1}{ \frac{\ell_{1}}{1-\varepsilon^{\prime}}+1}\right)^{\ell_{1}-i^{*}}\right)0.46\right.\] \[\left.\quad-\varepsilon^{\prime}\left(1-\left(1-\frac{1}{\frac{ \ell_{1}}{1-\varepsilon^{\prime}}+1}\right)^{\ell_{1}}\right)\right]f(O)\] \[\geq\left[\frac{\log(0.46)}{\left(\frac{\ell_{1}}{1-\varepsilon^ {\prime}}+1\right)\log\left(1-\frac{1}{\ell_{1}}\right)}\left(1-\frac{1}{\ell_ {1}}\right)e^{-1}+\left(1-\frac{e^{\varepsilon^{\prime}-1}}{0.46\left(1-\frac {1}{\frac{\ell_{1}}{1-\varepsilon^{\prime}}+1}\right)}\right)0.46-\varepsilon ^{\prime}\left(1-e^{-1+\varepsilon^{\prime}}\right)\right]f(O)\] (Lemma A.4; \[\left(1-\frac{1}{\ell_{1}}\right)^{i^{*}-1}>0.46\geq\left(1-\frac{1}{\ell_ {1}}\right)^{i^{*}})\]

[MISSING_PAGE_FAIL:36]

\[\geq\left[\left(\frac{(1-t)(\ell_{2}-1)}{\frac{\ell_{2}}{1-\varepsilon^ {\prime}}+1}+1-\frac{1}{\ell_{2}}\right)\left(1-\frac{1}{\ell_{2}}\right)e^{t-1 }-e^{-1}-\varepsilon^{\prime}\left(1-e^{-1}\right)\right.\] \[\geq\left(0.377-\varepsilon\right)f\left(O\right)\] ( \[t=0.3;\ell_{2}=\frac{5}{2\varepsilon^{\prime}};\varepsilon^{ \prime}=\frac{\varepsilon}{2}\] )

## Appendix F Experiments

Experimental setupWe run all experiments on an Intel Xeon(R) W5-2445 CPU at 3.10 GHz with \(20\) cores, \(64\) GB of memory, and one NVIDIA RTX A4000 with \(16\) GB of memory. For Maximum Cut experiments, we use the standard multiprocessing provided in Python, which takes about \(20\) minutes to complete, while the video summarization finishes in under a minute.

### Additional tables and plots

In this section, you can find the tables and plots omitted in the main paper due to space constraints. In Figure 5, we compare the frames selected by FastLS +RandomGreedy and StandardGreedy, and in Figure 6, we report the results for Barab'asi-Albert and Watts-Strogatz models for Maximum Cut.

### Problem Formulation

In this section, we formally introduce video summarization and Maximum Cut.

#### f.2.1 Video summarization

Formally, given \(n\) frames from a video, we present each frame by a \(p\)-dimensional vector. Let \(X\in\mathcal{R}^{n\times n}\) be the Gramian matrix of the \(n\) resulting vectors so \(X_{ij}\) quantifies the similarity between two vectors through their inner product. The Determinantal Point Process (DPP) objective function is defined by the determinant function \(f:2^{n}\rightarrow\mathcal{R}:f(S)=\log(\det(X_{S})+1)\), where \(X_{S}\) is the principal submatrix of \(X\) indexed by \(S\) following Banihashem et al. [1] to make the objective function \(f\) a non-monotone non-negative submodular function.

#### f.2.2 Maximum cut

Given an undirected graph \(G(V,E)\), where \(V\) represents the set of vertices, \(E\) denotes the set of edges and weights \(w(u,v)\) on the edges \((u,v)\in E\), the goal of the Maximum Cut problem is to find a subset of nodes \(S\subseteq V\) that maximizes the objective function, \(f(S)=\sum_{u\in S,v\in V\setminus S}w(u,v)\).

### Hyperparameters

For all experiments, we set the error rate, \(\epsilon\), to \(0.01\) for FastLS +GuidedRG and to \(0.1\) for Lee et al. [21]. Additionally, for video summarization, we run RandomGreedy \(20\) times and report the standard deviation of these runs. For all other experiments, we run the algorithms once per instance and report the standard deviation over instances.

Figure 5: Frames selected for Video Summarization

### Datasets

The video we select [29] (available under CC BY license) for video summarization lasts for roughly \(4\) minutes, and we uniformly sample \(100\) frames from the video to form the ground set. For maximum cut, we run experiments on synthetic random graphs, each distribution consisting of \(20\) graphs of size \(10,000\) vertices generated using the Erdos-Renyi (ER), Barabasi-Albert (BA), and Watts-Strogatz (WS) models. The ER graphs are generated with \(p=0.001\), while the WS graphs are created with \(p=0.001\) and \(10\) edges per node. For the BA model, graphs are generated by adding \(m=2\) edges in each iteration. Data and code are provided in the supplementary material to regenerate the empirical results provided in the paper.

### Implementation of FastLS+GuidedRG

For our implementation of FastLS, we take the solution of StandardGreedy as our initial solution \(Z_{0}\); the theoretical guarantee is thus \(f(Z_{0})>\text{OPT}/k\), since \(Z_{0}\) has higher \(f\)-value than the maximum singleton. This increases the theoretical query complexity of our algorithm as implemented to \(\mathcal{O}\left(\frac{kn}{\varepsilon}\log(\frac{k}{\varepsilon})\right)\). Then, for each swap, we find the best candidates to remove from and add to the current solution set (including the dummy element), rather than any pair that satifies the criterion. For guided GuidedRG, we implement exactly as in the pseudocode (Alg. 6) - we remark that we could instead use (a guided version of) the linear-time variant of RandomGreedy[10] to reduce the empirical number of queries further, but for simplicity we did not do this in our evaluation.

Figure 6: The objective value (higher is better) and the number of queries (lower is better) are normalized by those of StandardGreedy. Our algorithm (blue star) outperforms every baseline on at least one of these two metrics.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims made in the abstract and introduction are matched by theorem statements with detailed proofs in the main text and appendices or by empirical evaluation. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: All formal theorem statements include the assumed hypotheses, and theoretical and empirical limitations are discussed explicitly in Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: All assumptions are clearly specified. In the main text, we discuss the intuition behind the analysis and sketch the main arguments, while complete detailed proofs are provided in the appendices. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.

4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All parameter settings and implementation choices for the evaluated algorithms are reported. Further, the source code and documentation is provided, with instructions for how to reproduce the results. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The complete data, source code, and instructions to reproduce the experimental results are provided in the supplementary material. Once this manuscript is published, we plan to make this available on an open-source repository. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: No training is involved in the evaluation. All hyperparameters for random graph generation and for the algorithms are provided. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In out plots, we plot the mean over 20 independent repetitions. The shaded regions of the plots indicate one standard deviation about the mean. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ** The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Our experiments are not large-scale, and modern desktop hardware should be sufficient to reproduce all results in a few hours. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: The research conforms in every respect to the code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: Ours is foundational research, not tied to any particular application or deployment. The applications upon which we implemented and empirically evaluated our algorithms are proof-of-concept and thus our provided implementations are unlikely to lead directly to negative societal impact.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not release any data or models that have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The only pre-existing asset used is the video in the summarization experiment. We explicitly mention the license in Appendix F and properly respect the terms. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. ** For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The released source code is well documented. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.