# Advancing Open-Set Domain Generalization Using Evidential Bi-Level Hardest Domain Scheduler

 Kunyu Peng\({}^{1}\), Di Wen\({}^{1}\), Kailun Yang\({}^{2}\), Ao Luo\({}^{3}\), Yufan Chen\({}^{1}\), Jia Fu\({}^{4,5}\),

M. Saquib Sarfraz\({}^{1,6}\), Alina Roitberg\({}^{7}\), Rainer Stiefelhagen\({}^{1}\)

\({}^{1}\)Karlsruhe Institute of Technology \({}^{2}\)Hunan University \({}^{3}\)Waseda University

\({}^{4}\)KTH Royal Institute of Technology \({}^{5}\)RISE Research Institutes of Sweden

\({}^{6}\)Mercedes-Benz Tech Innovation \({}^{7}\)University of Stuttgart

Correspondence: kailun.yang@hnu.edu.cn

###### Abstract

In Open-Set Domain Generalization (OSDG), the model is exposed to both new variations of data appearance (domains) and open-set conditions, where both known and novel categories are present at test time. The challenges of this task arise from the dual need to generalize across diverse domains and accurately quantify category novelty, which is critical for applications in dynamic environments. Recently, meta-learning techniques have demonstrated superior results in OSDG, effectively orchestrating the meta-train and -test tasks by employing varied random categories and predefined domain partition strategies. These approaches prioritize a well-designed training schedule over traditional methods that focus primarily on data augmentation and the enhancement of discriminative feature learning. The prevailing meta-learning models in OSDG typically utilize a predefined sequential domain scheduler to structure data partitions. However, a crucial aspect that remains inadequately explored is the influence brought by strategies of domain schedulers during training. In this paper, we observe that an adaptive domain scheduler benefits more in OSDG compared with prefixed sequential and random domain schedulers. We propose the **E**vidential **Bi-**L**evel **H**ardest **D**omain **S**cheduler (EBiL-HaDS) to achieve an adaptive domain scheduler. This method strategically sequences domains by assessing their reliabilities in utilizing a follower network, trained with confidence scores learned in an evidential manner, regularized by max rebiasing discrepancy, and optimized in a bi-level manner. We verify our approach on three OSDG benchmarks, _i.e._, PACS, DigitsDG, and OfficeHome. The results show that our method substantially improves OSDG performance and achieves more discriminative embeddings for both the seen and unseen categories, underscoring the advantage of a judicious domain scheduler for the generalizability to unseen domains and unseen categories. The source code is publicly available at [https://github.com/KPeng9510/EBiL-HaDS](https://github.com/KPeng9510/EBiL-HaDS).

## 1 Introduction

Open-Set Domain Generalization (OSDG) is a challenging task where the model is exposed to both: domain shift and category shift. Recent OSDG works often take a meta-learning approach [54, 46] which simulates different cross-domain learning tasks during training. These methods conventionally use a _predefined_ sequential domain scheduler to create meta-train and meta-test domains within each minibatch. But is fixing the meta-learning domain schedule a priori the best way to go? As a step to explore this, our work investigates the new idea of _adaptive domain scheduler_, which dynamically adjusts the training order based on ongoing model performance and domain difficulty.

OSDG is critical for many real-world applications with changing conditions, ranging from healthcare [33] and security [5] to autonomous driving [19]. Despite the remarkable success of deep learning, the recognition quality often deteriorates when facing out-of-distribution samples. This problem is amplified in OSDG settings, where the model faces a dual challenge of identifying and rejecting unseen categories, _e.g._, by delivering low confidence score in such cases [27; 46] while simultaneously generalizing well to unseen data appearances (domain shift). Historically, research efforts in OSDG have predominantly focused on the latter, developing methods to adapt models to varying domain conditions. Strategies to improve domain generalization include the use of Generative Adversarial Networks (GANs) [4], contrastive learning [65], and metric learning [27]. MLDG [46] for the first time proposed to use meta-learning to handle OSDG tasks. However, all these works follow the OSDG protocols where different source domains preserve different distributions of known categories, which diverges the domain gaps for different categories. This divergence makes the challenge more inclined towards the domain generalization aspect. Wang _et al._[54] revised these benchmarks for OSDG, standardizing the category distribution across source domains to achieve a more balanced evaluation of both domain generalization and open set recognition challenges. This revision includes established open-set recognition methods such as Adversarial Reciprocal Points Learning (ARPL) [8]. Recently, Wang _et al._[54] introduced an effective meta-learning approach named MEDIC, featuring a binary classification and a _predefined_ sequential domain scheduler for the data partition during meta-train and -test stages.

However, these existing meta-learning-based OSDG approaches, _i.e._, MEIDC [54] and MLDG [46], do not consider how the order in which domains are presented during training affects model generalization. We believe this overlooks the potential to dynamically adapt the domain scheduler used for data partition based on certain criteria, such as domain difficulty, which could result in a more targeted training strategy and, therefore, better outcomes. In this paper, we observe that different ordering strategies for domain presentation used for data partition during the meta-training and testing phases lead to significant variations in OSDG performance, emphasizing the critical role of domain scheduling in optimizing model generalization.

To bridge this gap, we introduce a new training strategy named the **E**vidential **Bi-**L**evel **H**ardest **D**omain **S**cheduler (EBiL-HaDS), which allows _dynamically adjusting the order of domain presentation_ during data partitioning in the meta-training and -testing phases. The key idea of our method is to quantify domain reliability, defined as the aggregated confidence of the model on the samples across unseen domains, which will then be used as the main criterion for the data partition. To assess the domain reliability, we incorporate a secondary follower network to assess the domain reliability alongside the primary network. This allows for prioritizing the optimization of meta-learning on less reliable domains, facilitating an adaptive domain scheduler-based data partitioning. This follower network is trained using bi-level optimization, which involves a hierarchical setup where the solution to a lower-level optimization problem (evaluating domain reliability) serves as a constraint in an upper-level problem (meta-learning objective). Optimization of the follower network is guided by confidence scores generated through our proposed max rebiased evidential learning method, which adjusts the confidence by amplifying the differences between the decision boundaries of different classes. As a result, the follower network can better quantify the reliability of each domain based on how distinct and consistent the classification boundaries are, improving the ability to generalize to unseen domains. EBiL-HaDS enhances cross-domain generalizability and differentiation of seen and unseen classes by prioritizing training on less reliable domains through adaptive domain scheduling.

Our experiments demonstrate the effectiveness of domain scheduling via EBiL-HaDS on three established datasets: PACS [31], DigitsDG [63], and OfficeHome [51], which span a variety of image classification tasks. Results demonstrate that EBiL-HaDS significantly improves model generalizability in open-set scenarios, enhancing domain generalization and the model's ability to distinguish between known and unknown categories in new domains. This performance surpasses that of both random and standard sequential domain scheduling methods during training, underscoring EBiL-HaDS's potential to advance current OSDG capabilities in deep learning.

## 2 Related Work

We simultaneously address two challenges: domain generalization and open-set recognition. Domain generalization is a task that expects a model to generalize well to unseen domains while leveragingmultiple seen domains for training [48; 52]. Open-set recognition, on the other hand, aims to reject unseen categories at test-time, _e.g._, by delivering low confidence scores in such cases [16].

Domain generalization methods usually alleviate the domain gap with techniques such as data augmentation [55; 40; 65; 18; 64; 34; 35], contrastive learning [57; 24; 28; 43], domain adversarial learning [15], domain-specific normalization [44], and GANs-based methods [10; 32]. For open-set recognition, common approaches include logits calibration [3; 42], evidential learning [59; 53; 62; 2], reconstruction-based approaches [58; 22], GANs-based methods [29], and reciprocal point-based approaches [8; 9].

A considerable cluster of research utilizes source domains that encompass diverse categories for training, as highlighted in [14; 47; 4; 8; 36; 61], where each category poses unique domain generalization challenges. The primary focus of these methodologies is to improve domain generalizability, and they tend to allocate less attention to the complexities associated with open-set scenarios. In this setting, the categories involved in each source domain may not be the same. ODG-Net proposed by Bose _et al._[4] leverages GAN to synthesize data from the merged training domains to improve cross-domain generalizability. SWAD proposed by Chen _et al._[8] uses models averaged across various training epochs. Katsumata _et al._[27] propose to use metric learning to get discriminative embedding space which benefits the open-set domain generalization. Wang _et al._[54] introduce a new MEDIC model together with a new formalization of the open-set domain generalization protocols, where the source domains share the same categories defined as seen. This benchmark definition balances the impact of the model's open-set recognition and domain generalization performance in evaluation, which is adopted in our work.

Newer OSDG approaches show great promise of meta-learning strategies for improving cross-domain generalization [54; 46]. Yet, these works mainly utilize a fixed, sequential scheduling of source domains during training [54; 46]. Existing works in curriculum learning indicate that using a specific training order at the instance level can benefit the model performance on various tasks [56; 26; 17; 37; 20; 41; 28; 45]. However, existing curriculum learning approaches usually operate at the instance level (scheduling individual dataset instances within standard training) and are not designed for OSDG tasks, while we focus on domain-based scheduling by quantifying the domain difficulty in meta-learning. The influence of domain scheduling in the OSDG task remains unexplored. This paper, for the first time, examines the effects of guiding the meta-learning process with an _adaptive domain scheduler_, named EBiL-HaDS, which achieves data partition based on a domain reliability measure estimated by a follower network, trained in a bi-level manner with the supervision from the confidence score optimized by a novel max rebiased discrepancy evidential learning.

## 3 Method

The most challenging domain is chosen to perform data partitioning for the meta-task reservation during meta-learning. In our proposed EBiL-HaDS, we first utilize max rebiased discrepancy evidential learning (Sec. 3.1) to achieve more reliable confidence acquisition, which is subsequently used as the supervision for the reliability prediction of the follower network. During the training stage, our method makes use of two networks with identical architectures: one serves as the main feature extraction network, and the other functions as the follower network, aiming to assess the domain reliability. The follower network is optimized in a bi-level manner alongside the main network (Sec. 3.2). Hardest domain selection is accomplished by aggregating votes for samples from each domain for the randomly selected reserved classes using the follower network (Sec. 3.3).

To optimize domain scheduling, we first define the term domain reliability, as the degree to which data from a domain consistently aids in improving the model's accuracy and generalizability across unseen domains. An important step is therefore to adaptively rate the domain reliability during training. To achieve this, we employ two parallel networks: the main network used for feature extraction, and the follower network, which assesses the reliability of different domains based on the refined confidence metrics. This follower network plays a central role in our adaptive domain scheduling strategy: it employs a voting process to identify and select the most challenging domains - those that exhibit the least reliability according to its assessments. After selecting the hardest domain, the data is divided into two sets. One set includes data from more challenging domains outside the reserved classes and data from more reliable domains within the reserved classes, which together form the meta-training set. The complementary partitions of the meta-training set are used as the meta-testing set. Both networks are simultaneously optimized through a bi-level training approach, meaning that the outcome of a lower-level optimization problem (evaluating domain reliability) is a constraint in an upper-level problem (meta-learning objective). The entire pipeline during training when using the proposed EBil-HaDS is depicted in Alg. 1.

### Max Rebiased Discrepancy Evidential Learning

The domain scheduler we propose leverages a follower network to ascertain reliability measurements, based on reliability evaluations conducted before the start of each epoch for all source domains. As such, confidence calibration is crucial for the main network's functionality. Evidential learning, which has been extensively applied across various domains such as action recognition [62] and image classification [25], effectively calibrates these confidence predictions. However, a notable limitation of evidential learning is its propensity for overfitting, leading to suboptimal performance [11].

To address these challenges, we propose to regularize the evidential learning by novel rebiased discrepancy maximization, which is employed for the confidence calibration of the main network to encourage diverse decision boundaries. This method involves training dual decision-making heads designed to exhibit rebiased maximized discrepancies. The aim is to foster the development of both informative and dependable decision-making capabilities within the leveraged deep learning model. Let \(\mathbf{x}\) denote a batch of data used in training, \(M_{\alpha}\) denote the feature extraction backbone, \(R_{\theta_{1}}\) and \(R_{\theta_{2}}\) denote the two rebiased layers, and \(\mathcal{K}\) denote the Gaussian kernel to reproduce the Hilbert space. We first calculate the max rebiased discrepancy regularization by Eq. 1,

\[\mathcal{R}_{RB}(\mathbf{x};\Theta)=\sum_{i\in\{1,2\}}\mathbb{E}\left[\mathcal{ K}(R_{\theta_{i}}(M_{\alpha}(\mathbf{x})),R_{\theta_{i}}(M_{\alpha}(\mathbf{x}))) \right]-2*\mathbb{E}[\mathcal{K}(R_{\theta_{1}}(M_{\alpha}(\mathbf{x})),R_{ \theta_{2}}(M_{\alpha}(\mathbf{x})))]. \tag{1}\]

We aim to maximize the above loss function to achieve the maximum discrepancy between the embeddings extracted from the two rebiased layers. This maximization encourages the learned evidence from the two layers to diverge from each other, thereby capturing open-set domain generalization cues from two different perspectives. Deep evidential learning is then applied to the conventional classification head, providing an additional constraint to achieve more reliable confidence calibration, as described in Eq. 2.

\[\mathcal{L}_{RBE}(\mathbf{y},\mathbf{x};\Theta)=\sum_{i\in\{1,2\}}\left[\sum_{ c=1}^{\mathcal{C}}\left[\mathbf{y}_{c}\left(\log S_{i}-\log(R_{\theta_{i}}\left(M_ {\alpha}(\mathbf{x})\right)_{c}+1)\right)\right]\right]-\mathcal{R}_{RB}( \mathbf{x};\Theta) \tag{2}\]

where \(S_{i}=\sum_{c=1}^{C}(Dir(p|R_{\theta_{i}}\left(M_{\alpha}(\mathbf{x})\right)_{ c}+1)\) denotes the strength of a Dirichlet distribution, \(\mathbf{y}_{c}\) is the one-hot annotation of sample \(\mathbf{x}\) from class \(c\), \(p\) is the predicted probability. The two rebiased layers are engineered to capture distinct evidence by employing max discrepancy regularization. By averaging the logits produced by the two prediction heads on the top of the two rebiased layers for the conventional classification on the seen categories, we can harvest the final estimated confidence score. This score is subsequently utilized to supervise the follower network, as elaborated in the following subsections.

### Follower Network for Reliability Learning

To establish an adaptive domain scheduler for the OSDG task, the most straightforward approach would involve training a network to directly predict the sequence in which domains are employed during the training phase for sample selection. However, this method does not facilitate gradient computation, thereby preventing the direct optimization of the scheduler network.

In this work, we propose an alternative method where a follower network is trained to assess the reliability of each sample, utilizing predicted confidence scores derived from max discrepancy evidential learning as supervision. Throughout the training process, we employ samples from various domains to collectively assess reliability. Additionally, we utilize a follower network, denoted as \(M_{\beta}\), which mirrors the architecture of the main network, but with classification heads replaced by one regression head. \(\Theta\) indicates all the parameters in the main network, including the parameters from the backbone, rebiased layers, and heads. We aim to solve the optimization task, as shown in Eq. 3.

\[\Theta^{*}=\operatorname*{arg\,min}_{\Theta}\mathcal{L}_{m}(M_{\Theta}(\mathbf{x} ),\omega^{*}\gets M_{\beta^{*}}(\mathbf{x}))\ \ \ \text{subject to}\ \ \beta^{*}=\operatorname*{arg\,min}_{\beta}L_{f}(M_{\Theta}(\mathbf{x}),M_{ \beta}(\mathbf{x})), \tag{3}\]

where \(\omega^{*}\) indicates the instance-wise reliability which serves as the weight for each instance during the loss calculation. Substituting the best response function \(\beta^{*}(\Theta)=\operatorname*{arg\,min}_{\beta}L_{f}(M_{\Theta}(\mathbf{x} ),M_{\beta}(\mathbf{x}))\) provides a single-level problem, as shown in Eq. 4.

\[\Theta^{*}=\operatorname*{arg\,min}_{\Theta}L_{m}(\Theta,\beta^{*}(\Theta)), \tag{4}\]

where \(L_{m}\) denotes classification loss (\(L_{CLS}\)) and \(L_{RBE}\). \(L_{f}\) denotes the regression loss (\(L_{REG}\)).

### Hardest Domain Scheduler during Training

We illustrate the details of the training procedure by the proposed domain scheduler in Alg. 1. We adopt the meta-training framework outlined by MLDG [46], integrating our proposed domain scheduler to facilitate the data partition of meta-tasks. In this approach, optimization is achieved using both the meta-train and -test sets, characterized by distinct data distributions. For each domain present in the training dataset, we sample a batch that encompasses the reserved categories. Subsequently, we identify the most challenging domain by determining which domain exhibits the lowest reliability under the selected seen categories. This procedure is accomplished by the calculation of the expected reliability as in Eq. 5.

\[d^{*}=\operatorname*{arg\,min}_{d}(\{\omega_{d}|d\in\mathcal{D}\}),\ \omega_{d}=\min_{c\in\mathcal{C}^{*}}\left[\exp\left[1+\sum_{i=1}^{N_{c}^{*}} \frac{(M_{\beta}(\mathbf{x}_{i}^{(c,d)}))}{N_{c}^{*}}\right]*(0.1+\sigma*\gamma_{d})\right], \tag{5}\]

where \(d^{*}\) denotes the estimated hardest domain. \(N_{c}^{*}\) and \(\mathcal{C}^{*}\) denote the number of samples from domain \(d\) and the number of selected known categories at the start of one epoch. \(\mathcal{D}\) denotes the known domains used during the training procedure. \(\mathbf{x}_{i}^{(c,d)}\) indicates the \(i\)-th sample from class \(c\) and domain \(d\). \(\gamma_{d}\) indicates the schedule frequency for domain \(d\) in the past training period, which considers the balance of different domains.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

training, enhancing generalization to unseen domains, which is observed from the above experimental analyses. Significant performance gains in challenging DG splits, such as the unseen Cartoon domain, demonstrate its effectiveness in handling extreme domain shifts. Consistent metric improvements highlight EBiL-HaDS's versatility across various OSDG challenges.

Further experiments on a different backbone, _i.e._, ResNet50 [21], are delivered in Table 2, where our method contributes \(1.08\%\), \(0.94\%\), and \(1.27\%\) performance improvements of close-set accuracy, H-score, and OSCR for binary classification head and consistent performance improvements for the conventional classification head. EBiL-HaDS contributes more performance improvements when we compare the experimental results on ResNet18 with ResNet50 [21] for the PACS dataset, which illustrates that the EBiL-HaDS is more helpful in alleviating the generalizability issue of model-preserving light-weight network structure since network with small size is hard to optimize and obtain the generalizable capabilities on challenging unseen domains.

We further conduct ablation on model architecture on ResNet152 and ViT base model [13], where our proposed method is compared with the MEDIC and other challenging baselines, _i.e._, ARPL, MLDG, SWAD, and ODG-Net. From Table 2 and Table 1 we can observe an obvious OSDG performance improvement by increasing the complexity of the leveraged feature extraction backbone. However, when we compare Table 3 and Table 2, some baseline approaches trained with ResNet152 backbone even show performance decay on the major evaluation metric OSCR. This observation demonstrates that most OSDG methods face the overfitting issue when using a very large backbone, which is a critical issue for open-set challenging to recognize samples from unseen categories, especially in an unseen domain. The MEDIC-bcls approach shows \(6.48\%\) performance degradation on OSCR when we replace the backbone from ResNet50 to ResNet152. Using the proposed BHiL-HaDS to achieve a more reasonable task reservation in meta-learning procedure, BHiL-HaDS-bcls delivers \(86.25\%\) in terms of OSCR on ResNet152 [21] backbone, where the OSCR performance of BHiL-HaDS-bcls on ResNet50 is \(86.12\%\).

This observation shows that our proposed adaptive domain scheduler can make the meta-learning effective on large complex models by reserving reasonable task for model optimization. Additional experiments on the ViT base model [13] (patch size 16 and window size 224) are provided in Table 4, where we observe that our proposed method can deliver consistent performance gains on the transformer architecture.

This observation is further validated by the experimental results on DigitsDG where a smaller network structure, _i.e._, ConvNet [66], is used, as shown in Table 5. Our method contributes \(3.74\%\), \(4.83\%\), and \(4.72\%\) performance improvements of close-set accuracy, H-score, and OSCR for binary classification head and \(3.74\%\), \(12.29\%\), and \(5.64\%\) performance improvements of close-set accuracy, H-score, and OSCR for the conventional classification head. Consistent performance improvements are shown in Table 6 on the OfficeHome. We further observe that using EBiL-HaDS, the optimized model can contribute a distinct separation between confidence scores of the model on the unseen categories and seen categories in the test unseen domain, showing the benefits of a reasonable domain scheduler for OSDG.

close-set accuracy, H-score, and OSCR for binary classification head and \(1.82\%\), \(2.73\%\), and \(3.68\%\) performance improvements of close accuracy, H-score, and OSCR for conventional classification head. The significant OSDG performance improvements highlight the importance of the confidence score learned by the max rebiased discrepancy evidential learning in supervising the follower network, ensuring the promising reliability prediction. Then we use a sequential scheduler and keep the \(L_{RBE}\) in the third part of Table 7 (w/o DGS), where our approach outperforms this variant by \(2.04\%\), \(3.92\%\), and \(3.89\%\) of close accuracy, H-score, and OSCR for binary classification head and \(2.04\%\), \(3.31\%\), and \(5.07\%\) of these metrics for conventional classification head. This observation shows the importance of the proposed domain scheduler for the OSDG task and highlights the effect of using meta-learning trained with a reasonable data partition. Both ablations show better OSDG performances compared with MEDIC, confirming the benefit of each component. We further deliver

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c c|c c|c c|c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c|}{**MNIST**} & \multicolumn{3}{c|}{**MNIST-M**} & \multicolumn{3}{c|}{**SVIN**} & \multicolumn{3}{c}{**SVIN**} & \multicolumn{3}{c}{**Avg**} \\  & Acc & H-score & OSCR & Acc & H-score & OSCR & Acc & H-score & OSCR & Acc & H-score & OSCR & Acc & H-score & OSCR \\ \hline SequentialSched-sh & 97.89 & 67.39 & 66.71 & 71.14 & 48.44 & 58.337 & 76.00 & 51.20 & 55.58 & 88.11 & 64.00 & 70.62 & 84.12 & 57.98 & 70.19 \\ SequentialSched-shcls & 97.89 & 83.20 & 96.58 & 71.14 & **64.08** & 58.26 & 76.00 & 58.77 & 57.60 & 88.11 & 62.24 & 72.91 & 83.28 & 66.30 & 71.15 \\ Random-sh & 98.39 & 52.93 & 94.21 & 20.92 & 52.70 & 52.41 & 77.92 & 59.68 & 57.95 & 88.33 & 44.11 & 75.66 & 83.89 & 52.35 & 70.06 \\ Random-sh & 98.39 & 75.67 & 94.22 & 70.92 & 57.23 & 54.87 & 77.92 & 57.54 & 61.06 & 88.33 & 68.34 & 74.81 & 83.39 & 64.20 & 71.25 \\ \hline EBL-HtPS-chs & **95.80** & 87.40 & 97.49 & **74.28** & 56.58 & **60.86** & **80.33** & 61.27 & 62.84 & **93.97** & **75.82** & 78.14 & **87.02** & 70.27 & 75.83 \\ EBL-HtPS-chs & **99.50** & **91.63** & **97.58** & **74.28** & 60.72 & 59.39 & **80.33** & **62.23** & **63.38** & **93.37** & 75.77 & **79.28** & **87.02** & **72.59** & **75.87** \\ \hline \hline \end{tabular}
\end{table}
Table 8: Comparison with different domain schedulers on DigidsDG with open-set ratio 6:4.

Figure 1: Comparison of open-set confidence using ResNet18 [21] on PACS. _Photo_ is the unseen domain. We use red and blue colors to denote unseen and seen categories.

Figure 3: Ablation of different open-set ratios on DigitsDG dataset by ConvNet [66] backbone, where SA indicates Split Ablation. Regarding all the splits, Case 1 (denoted by red color) indicates using 7, 8, 9, 10 as unseen categories. In (a) and (b), Case 2 (denoted by blue color) and Case 3 (denoted by gray color) indicate that using 0, 1, 2, 3 and 2, 3, 4, 5 as unseen categories. In (c) and (d), Case 2 and Case 3 indicate using 7, 8, 9 and 8, 9 as unseen categories, respectively.

more ablations, _e.g._, the benefits brought by the max discrepancy regularization term, comparison with recent curriculum learning approaches, and ablation of the rebiased layers in the appendix.

### Comparison of Different Domain Schedulers for OSDG Task

We present several comparison experiments in Table 8 to demonstrate the efficacy of various domain schedulers when applying meta-learning to the OSDG task. We compare our proposed EBiL-HaDS with both the sequential domain scheduler and the random domain scheduler. The sequential domain scheduler selects domains in a fixed order for batch data partitioning, while the random scheduler assigns domains randomly. Results show that EBiL-HaDS significantly outperforms both the random and sequential domain schedulers. Specifically, EBiL-HaDS achieves performance improvements of \(3.13\%\), \(8.39\%\), and \(4.63\%\) in closed accuracy, H-score, and OSCR for the binary classification head, and \(3.13\%\), \(17.92\%\), and \(5.77\%\) in these metrics for the conventional classification head compared to the random scheduler. This ablation demonstrates that our scheduler enables the model to converge to a more optimal region, which outperforms both predefined fixed-order (sequential) and maximally random (random) schedulers, underscoring the importance of a well-designed domain scheduler in meta-learning for the OSDG. More ablations on domain schedulers are supplemented in the appendix.

### Analysis of the TSNE Visualizations of the Latent Space

In Figure 2, we deliver the TSNE [49] visualization of the latent space of MEDIC and our approach on the OSDG splits, _i.e._, _photo_ and _art_ as unseen domains. Unseen and seen categories are denoted by red and other colors. we observe that the model trained by our method delivers a more compact cluster for each category and the unseen category is more separable regarding the decision boundary in the latent space. Our method's ability to improve the generalizability of the model is particularly noteworthy. The well-structured latent space facilitates better transfer learning capabilities, allowing the model to adapt more efficiently to new, unseen categories. This characteristic is especially beneficial in dynamic environments where the data distribution can change over time. In essence, the effectiveness of our approach in achieving a more discriminative and generalizable latent space can be directly linked to the sophisticated data partitioning achieved through EBiL-HaDS. This demonstrates the profound influence that carefully designed domain schedulers can have on the overall performance of deep learning models, emphasizing the need for thoughtful consideration in their implementation.

### Ablation of the Open-Set Ratios and the Number of Unseen Categories

We first conduct the ablation towards different unseen categories with a predefined open-set ratio in Figure 2(a) and Figure 2(b) of close-set accuracy and the OSCR for open-set evaluation, where the performance of the ODG-NET, binary classification head, and conventional classification head of MEDIC method and our method are presented. We then conduct the ablation towards different numbers of unseen categories in Figure 2(c) and Figure 2(d) of close-set accuracy and the OSCR for open-set evaluation, where the performance of the ODG-NET, binary classification head, and classification head of MEDIC method and our method are presented. From the experimental results and comparisons, we can find consistent performance improvements, indicating the high generalizability of our approach across different open-set ratios and unseen category settings.

## 5 Conclusion

In this study, we introduce the Evidential Bi-Level Hardest Domain Scheduler (EBiL-HaDS) for the OSDG task. EBiL-HaDS is designed to create an adaptive domain scheduler that dynamically adjusts to varying domain difficulties. Extensive experiments on diverse image recognition tasks across three OSDG benchmarks demonstrate that our proposed solution generates more discriminative embeddings. Additionally, it significantly enhances the performance of state-of-the-art techniques in OSDG, showcasing its efficacy and potential for broader applications for deep learning models.

**Limitations and Societal Impacts.** EBiL-HaDS positively impacts society by enhancing model awareness of out-of-distribution categories in unseen domains, leading to more reliable decisions. It emphasizes the importance of domain scheduling in OSDG. However, the method may still result in misclassification and biased predictions, potentially causing negative effects. EBiL-HaDS relies on source domains with unified categories and has so far only been tested on image classification.

## Acknowledgements

The project served to prepare the SFB 1574 Circular Factory for the Perpetual Product (project ID: 471687386), approved by the German Research Foundation (DFG, German Research Foundation) with a start date of April 1, 2024. This work was also partially supported by the SmartAge project sponsored by the Carl Zeiss Stiftung (P2019-01-003; 2021-2026). This work was performed on the HoreKa supercomputer funded by the Ministry of Science, Research and the Arts Baden-Wurttemberg and by the Federal Ministry of Education and Research. The authors also acknowledge support by the state of Baden-Wurttemberg through bwHPC and the German Research Foundation (DFG) through grant INST 35/1597-1 FUGG. This project is also supported by the National Natural Science Foundation of China under Grant No. 62473139. Lastly, the authors thank for the support of Dr. Sepideh Pashami, the Swedish Innovation Agency VINNOVA, the Digital Futures.

## References

* [1] Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk. Generalization on the unseen, logic reasoning and degree curriculum. In _ICML_, 2023.
* [2] Wentao Bao, Qi Yu, and Yu Kong. Evidential deep learning for open set action recognition. In _ICCV_, 2021.
* [3] Abhijit Bendale and Terrance E. Boult. Towards open set deep networks. In _CVPR_, 2016.
* [4] Shirsha Bose, Ankit Jha, Hitesh Kandala, and Biplab Banerjee. Beyond boundaries: A novel data-augmentation discourse for open domain generalization. _TMLR_, 2023.
* [5] Pau Panareda Busto, Ahsan Iqbal, and Juergen Gall. Open set domain adaptation for image and action recognition. _TPAMI_, 2018.
* [6] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. In _NeurIPS_, 2021.
* [7] Junbum Cha, Kyungjae Lee, Sungrae Park, and Sanghyuk Chun. Domain generalization by mutual-information regularization with pre-trained models. In _ECCV_, 2022.
* [8] Guangyao Chen, Peixi Peng, Xiangqian Wang, and Yonghong Tian. Adversarial reciprocal points learning for open set recognition. _TPAMI_, 2022.
* [9] Guangyao Chen, Limeng Qiao, Yemin Shi, Peixi Peng, Jia Li, Tiejun Huang, Shiliang Pu, and Yonghong Tian. Learning open set network with discriminative reciprocal points. In _ECCV_, 2020.
* [10] Keyu Chen, Di Zhuang, and J. Morris Chang. Discriminative adversarial domain generalization with meta-learning based cross-domain validation. _Neurocomputing_, 2022.
* [11] Danruo Deng, Guangyong Chen, Yang Yu, Furui Liu, and Pheng-Ann Heng. Uncertainty estimation by fisher information-based evidential deep learning. In _ICML_, 2023.
* [12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009.
* [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.
* [14] Bo Fu, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Learning to detect open classes for universal domain adaptation. In _ECCV_, 2020.
* [15] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. _JMLR_, 2016.

* [16] Chuanxing Geng, Sheng-jun Huang, and Songcan Chen. Recent advances in open set recognition: A survey. _TPAMI_, 2021.
* [17] Tiantian Gong, Kaixiang Chen, Liyan Zhang, and Junsheng Wang. Debiased contrastive curriculum learning for progressive generalizable person re-identification. _TCVST_, 2023.
* [18] Jintao Guo, Na Wang, Lei Qi, and Yinghuan Shi. Aloft: A lightweight mlp-like architecture with dynamic low-frequency transform for domain generalization. In _CVPR_, 2023.
* [19] Xiaoqing Guo, Jie Liu, Tongliang Liu, and Yixuan Yuan. Simt: Handling open-set noise for domain adaptive semantic segmentation. In _CVPR_, 2022.
* [20] Guy Hacohen and Daphna Weinshall. On the power of curriculum learning in training deep networks. In _ICML_, 2019.
* [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, 2016.
* [22] Hongzhi Huang, Yu Wang, Qinghua Hu, and Ming-Ming Cheng. Class-specific semantic reconstruction for open set recognition. _TPAMI_, 2023.
* [23] Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In _ECCV_, 2020.
* [24] Seogyku Jeon, Kibeom Hong, Pilhyeon Lee, Jewook Lee, and Hyeran Byun. Feature stylization and domain-aware contrastive learning for domain generalization. In _MM_, 2021.
* [25] Fengcheng Ji, Wenzhi Zhao, Qiao Wang, William J. Emery, Rui Peng, Yuanbin Man, Guoqiang Wang, and Kun Jia. Spectral-spatial evidential learning network for open-set hyperspectral image classification. _TGRS_, 2024.
* [26] Nazmul Karim, Niluthpol Chowdhury Mithun, Abhinav Rajvanshi, Han-pang Chiu, Supun Samarasekera, and Nazanin Rahnavard. C-sfda: A curriculum learning aided self-training framework for efficient source free domain adaptation. In _CVPR_, 2023.
* [27] Kai Katsumata, Ikki Kishida, Ayako Amma, and Hideki Nakayama. Open-set domain generalization via metric learning. In _ICIP_, 2021.
* [28] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised contrastive regularization for domain generalization. In _ICCV_, 2021.
* [29] Shu Kong and Deva Ramanan. Opengan: Open-set recognition via open data generation. In _CVPR_, 2021.
* [30] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domain generalization. In _AAAI_, 2018.
* [31] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier domain generalization. In _ICCV_, 2017.
* [32] Fan Li, Yanxiang Chen, Haiyang Liu, Zuxing Zhao, Yuanzhi Yao, and Xin Liao. Vocoder detection of spoofing speech based on gan fingerprints and domain generalization. _TOMM_, 2024.
* [33] Haoliang Li, YuFei Wang, Renjie Wan, Shiqi Wang, Tie-Qiang Li, and Alex Kot. Domain generalization for medical imaging classification with linear-dependency regularization. _NeurIPS_, 2020.
* [34] Lei Li, Ke Gao, Juan Cao, Ziyao Huang, Yepeng Weng, Xiaoyue Mi, Zhengze Yu, Xiaoya Li, and Boyang Xia. Progressive domain expansion network for single domain generalization. In _CVPR_, 2021.
* [35] Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M. Hospedales. A simple feature augmentation for domain generalization. In _ICCV_, 2021.

* [36] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In _ECCV_, 2018.
* [37] Kang Liao, Lang Nie, Chunyu Lin, Zishuo Zheng, and Yao Zhao. Recrecnet: Rectangling rectified wide-angle images by thin-plate spline model and dof-based curriculum learning. In _ICCV_, 2023.
* [38] Fangrui Lv, Jian Liang, Shuang Li, Bin Zang, Chi Harold Liu, Ziteng Wang, and Di Liu. Causality inspired representation learning for domain generalization. In _CVPR_, 2022.
* [39] Toshihiko Matsuura and Tatsuya Harada. Domain generalization using a mixture of multiple latent domains. In _AAAI_, 2020.
* [40] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In _CVPR_, 2021.
* [41] Vaasudev Narayanan, Aniket Anand Deshmukh, Urun Dogan, and Vineeth N. Balasubramanian. On challenges in unsupervised domain generalization. In _NeurIPSW_, 2022.
* [42] Kunyu Peng, Cheng Yin, Junwei Zheng, Ruiping Liu, David Schneider, Jiaming Zhang, Kailun Yang, M. Saquib Sarfraz, Rainer Stiefelhagen, and Alina Roitberg. Navigating open set scenarios for skeleton-based action recognition. In _AAAI_, 2024.
* [43] Mohamed Ragab, Zhenghua Chen, Wenyu Zhang, Emadeldeen Eldele, Min Wu, Chee-Keong Kwoh, and Xiaoli Li. Conditional contrastive domain generalization for fault diagnosis. _TIM_, 2022.
* [44] Seonguk Seo, Yumin Suh, Dongwan Kim, Geeho Kim, Jongwoo Han, and Bohyung Han. Learning to optimize domain specific normalization for domain generalization. In _ECCV_, 2020.
* [45] Yang Shu, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Transferable curriculum for weakly-supervised domain adaptation. In _AAAI_, 2019.
* [46] Yang Shu, Zhangjie Cao, Chenyu Wang, Jianmin Wang, and Mingsheng Long. Open domain generalization with domain-augmented meta-learning. In _CVPR_, 2021.
* [47] Mainak Singha, Ankit Jha, Shirsha Bose, Ashwin Nair, Moloud Abdar, and Biplab Banerjee. Unknown prompt, the only lacuna: Unveiling clip's potential for open domain generalization. _arXiv preprint arXiv:2404.00710_, 2024.
* [48] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In _ECCV_, 2016.
* [49] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _JMLR_, 2008.
* [50] Vladimir Vapnik. Statistical learning theory, 1998.
* [51] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In _CVPR_, 2017.
* [52] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. _TKDE_, 2023.
* [53] Ruofan Wang, Rui-Wei Zhao, Xiaobo Zhang, and Rui Feng. Towards evidential and class separable open set object detection. In _AAAI_, 2024.
* [54] Xiran Wang, Jian Zhang, Lei Qi, and Yinghuan Shi. Generalizable decision boundaries: Dualistic meta-learning for open set domain generalization. In _ICCV_, 2023.
* [55] Yufei Wang, Haoliang Li, and Alex C. Kot. Heterogeneous domain generalization via domain mixup. In _ICASSP_, 2020.
* [56] Yulin Wang, Yang Yue, Rui Lu, Tianjiao Liu, Zhao Zhong, Shiji Song, and Gao Huang. Efficienttrain: Exploring generalized curriculum learning for training visual backbones. In _ICCV_, 2023.

* [57] Xufeng Yao, Yang Bai, Xinyun Zhang, Yuechen Zhang, Qi Sun, Ran Chen, Ruiyu Li, and Bei Yu. Pcl: Proxy-based contrastive learning for domain generalization. In _CVPR_, 2022.
* [58] Ryota Yoshihashi, Wen Shao, Rei Kawakami, Shaodi You, Makoto Iida, and Takeshi Naemura. Classification-reconstruction learning for open-set recognition. In _CVPR_, 2019.
* [59] Yang Yu, Danruo Deng, Furui Liu, Qi Dou, Yueming Jin, Guangyong Chen, and Pheng Ann Heng. Anedl: Adaptive negative evidential deep learning for open-set semi-supervised learning. In _AAAI_, 2024.
* [60] Jian Zhang, Lei Qi, Yinghuan Shi, and Yang Gao. Mvdg: A unified multi-view framework for domain generalization. In _ECCV_, 2022.
* [61] Chao Zhao and Weiming Shen. Adaptive open set domain generalization network: Learning to diagnose unknown faults under unknown working conditions. _Reliability Engineering & System Safety_, 2022.
* [62] Chen Zhao, Dawei Du, Anthony Hoogs, and Christopher Funk. Open set action recognition via multi-label evidential learning. In _CVPR_, 2023.
* [63] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Deep domain-adversarial image generation for domain generalisation. In _AAAI_, 2020.
* [64] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to generate novel domains for domain generalization. In _ECCV_, 2020.
* [65] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In _ICLR_, 2020.
* [66] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain adaptive ensemble learning. _TIP_, 2021.
* [67] Ronghang Zhu and Sheng Li. Crossmatch: Cross-classifier consistency regularization for open-set single domain generalization. In _ICLR_, 2021.

## Appendix A Further Illustration of the Evaluation Methods and Protocols

We follow the same protocol according to the MEDIC approach [54]. For PACS dataset which is used in Table 1 and Table 2, we use the open-set ratio as \(6:1\), where the _elephant_, _horse_, _giraffe_, _dog_, _guitar_, _house_ are selected as seen categories and _person_ is selected as the unseen category. For the DigitsDG dataset, we leverage the open-set ratio as \(6:4\), where digits \(0,1,2,3,4,5\) are used as seen categories while digits \(6,7,8,9\) are selected as unseen categories, in Table 5. For the OfficeHome dataset, _Mop_, _Mouse_, _Mug_, _Notebook_, _Oven_, _Pan_, _PaperClip_, _Pen_, _Pencil_, _PositiNotes_, _Printer_, _PushPin_, _Radio_, _Refrigerator_, _Ruler_, _Scissors_, _Scevdriver_, _Shelf_, _Sink_, _Sneakers_, _Soda_, _Speaker_, _Spoon_, _TV_, _Table_, _Telephone_, _ToothBrush_, _Toys_, _TrashCan_, _Webcam_ are chosen as the \(30\) unseen categories. The _Acc_ indicates the close-set accuracy measured on the seen categories to assess the correctness of the classification. The _H-score_ and _OSCR_ are measurements for the open-set recognition which are widely used in the OSDG field. Since the _H-score_ relies on a predefined threshold derived from the source domain validation set to separate the seen categories and unseen categories, it is regarded as a secondary metric in our evaluation. MEDIC proposes OSCR for the OSDG evaluation where no predefined threshold is required, which is used as our primary evaluation metric.

Regarding the calculation of the H-Score, we first have a threshold ratio \(\lambda\) to separate the samples coming from seen and unseen classes. When the predicted confidence score is below \(\lambda\), we regard the corresponding samples as an unseen category. Then, we calculate the accuracy for all the samples regarded as seen categories according to their corresponding seen labels, which can be denoted as \(Acc_{k}\). The accuracy calculation of the unseen categories is conducted in a binary classification manner, where the label for the samples from the seen category is annotated as \(1\) and the label for the samples from the unseen category is annotated as \(0\). Then the accuracy for the unseen evaluation can be denoted as \(Acc_{u}\). The H-score is calculated as follows,

\[H_{score}=\frac{2*Acc_{u}*Acc_{k}}{Acc_{u}+Acc_{k}}. \tag{6}\]

OSCR is a combination of the accuracy and the AUROC via a moving threshold to measure the quality of the confidence score prediction for the OSDG task. Different from the AUROC, OSDG only calculates the samples that are correctly predicted using such moving threshold, which is a combination of the calculation manner from the H-score and AUROC.

## Appendix B More Ablations of the Proposed Method

### Ablation of the RBE

We deliver the ablation results of schedulers with different loss function components on the DigitsDG dataset with an open-set ratio of 6:4 for close-set accuracy, H-score, and OSCR of both the conventional and binary classification heads under different dataset partitions (Mnist, Mnist-m, SVHN, SYN). The experiments are conducted by removing the whole \(\mathcal{L}_{RBE}\) component or removing only the regularization term \(\mathcal{R}_{RB}\) from the EBiL-HaDS method. The results show that EBiL-HaDS performs better than the other two variations on almost all the dataset partitions except the H-score of Mnist-m, as shown in Table 9, demonstrating the effect of our \(\mathcal{L}_{RBE}\) and the importance of the regularization term \(\mathcal{R}_{RB}\) in the whole \(\mathcal{L}_{RBE}\) component.

Comparisons between EBiL-HaDS and the method without the \(\mathcal{L}_{RBE}\) illustrate the whole improvement of our deep evidential learning component with regularization term \(\mathcal{R}_{RB}\). EBiL-HaDS has in average \(2.04\%\), \(3.31\%\), \(5.07\%\) performance increase of the conventional classification head and \(2.04\%\), \(3.92\%\), \(3.89\%\) increase of the binary classification head for close-set accuracy, H-score, and OSCR, respectively. Differences between the models without \(\mathcal{L}_{RBE}\) and without \(\mathcal{R}_{RB}\) demonstrate the effect of our deep evidential learning itself. The performance with deep evidential learning improves \(0.66\%\), \(0.43\%\), \(2.47\%\) with the conventional classification head and \(0.66\%\), \(0.73\%\), \(0.83\%\) with the binary classification head for close-set accuracy, H-score, and OSCR in average. On the other hand, EBiL-HaDS has superior results than the model without regularization term \(\mathcal{R}_{RB}\) by \(1.38\%\), \(2.88\%\), \(2.60\%\) and \(1.38\%\), \(3.19\%\), \(3.06\%\) for close-set accuracy, H-score, OSCR with the

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_FAIL:18]

Figure 4: Experimental details for the ablation of different splits on 6:4 ratio on DigitsDG dataset. (Supplementary figure)

Figure 5: Experimental details for the ablation of different open-set ratios on DigitsDG dataset. (Supplementary figure)

Figure 6: Val and test accuracy on MnistDG, where the validation accuracy of MEDIC and our approach are indicated by lines in blue and orange colors, and the test accuracy of MEDIC and our approach are indicated by lines in gray and yellow colors. The horizontal axis indicates the evaluation step with stepsize 100 during the meta-learning procedure.

Figure 7: Ablation for the \(\sigma\), where the horizontal axis indicates the ablation cases. Case 1, 2, 3, 4, 5, and 6 indicate \(2e^{-1}\), \(2e^{-2}\), \(2e^{-3}\), \(2e^{-4}\), \(2e^{-5}\) and \(2e^{-6}\). The experiments are conducted on DigitsDG using a ConvNet architecture.

Figure 8: Ablation for the loss weight of the \(L_{REG}\), where the horizontal axis indicates the ablation cases. Case 1, 2, 3, and 4 indicate \(1e^{-3}\), \(1e^{-4}\), \(1e^{-5}\), and \(1e^{-6}\). The experiments are conducted on DigitsDG using a ConvNet architecture.

Figure 9: Ablation for the loss weight of the \(L_{RBE}\), where the horizontal axis indicates the ablation cases. Case 1, 2, 3, and 4 indicate \(5e^{-1}\), \(5e^{-2}\), \(5e^{-3}\), and \(5e^{-4}\). The experiments are conducted on DigitsDG using a ConvNet architecture.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The authors confirm that the main claim provided in the abstract and the introduction is verified through experiments and can demonstrate the contribution and the scope of this work. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The authors confirm that the limitation of the proposed approach is discussed in the conclusion section alongside the negative and positive society impacts. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]  Justification: The authors confirm that this submission does not contain theoretical result. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The authors confirm that a detailed algorithm description is provided in our main paper alongside with the implementation details. The code will be made publicly available to the community upon decision. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code link is provided in abstract. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The authors confirm that the detailed training scripts are illustrated in the appendix. The training details, _e.g._, optimizer, scheduler, and hyperparameters, are delivered in the implementation details section in the main paper. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.

7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The authors confirm that this submission does not provide error bars in our main experiments. However, our ablation experiments include cross backbone generalizability, cross split tests and cross open-set ratio tests, which can demonstrate the statistic significance of the proposed approach as shown in Figure 3 in our main paper. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The authors confirm that the details of the computational resources are provided in the implement details section in the main paper. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: The authors have reviewed the NeurIPS Code of Ethics and confirmed for all of the content. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The authors confirm that the potential positive and negative society impacts are discussed in the conclusion section. Guidelines: ** The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The authors confirm that this submission poses no such risk. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The authors confirm that the original papers that produced the code package or dataset are correctly cited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [No] Justification: The authors confirm that this submission does not contain new dataset. Protocols are demonstrated in the supplementary materials and code will be made publicly available upon decision. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The authors confirm that this submission does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The authors confirm that this submission does not involve crowdsourcing nor research with human subjects. Guidelines: ** The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.