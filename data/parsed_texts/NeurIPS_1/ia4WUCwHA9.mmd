# Theoretical guarantees in KL for Diffusion Flow Matching

Marta Gentiloni Silveri

Ecole polytechnique

Route de Saclay, 91120 Palaiseau, France

marta.gentiloni-silveri@polytechnique.edu

Giovanni Conforti

Universita degli Studi di Padova

Via Trieste, 63, 35131 Padova, Italia

giovanni.conforti@math.unipd.it &Alain Durmus

Ecole polytechnique

Route de Saclay, 91120 Palaiseau, France

alain.durmus@polytechnique.edu

###### Abstract

Flow Matching (FM) (also referred to as stochastic interpolants or rectified flows) stands out as a class of generative models that aims to bridge in finite time the target distribution \(\nu^{\star}\) with an auxiliary distribution \(\mu\), leveraging a fixed coupling \(\pi\) and a bridge which can either be deterministic or stochastic. These two ingredients define a path measure which can then be approximated by learning the drift of its Markovian projection. The main contribution of this paper is to provide relatively mild assumptions on \(\nu^{\star}\), \(\mu\) and \(\pi\) to obtain non-asymptotics guarantees for Diffusion Flow Matching (DFM) models using as bridge the conditional distribution associated with the Brownian motion. More precisely, we establish bounds on the Kullback-Leibler divergence between the target distribution and the one generated by such DFM models under moment conditions on the score of \(\nu^{\star}\), \(\mu\) and \(\pi\), and a standard \(\mathrm{L}^{2}\)-drift-approximation error assumption.

## 1 Introduction

A significant task in statistics and machine learning currently revolves around generating samples from a target distribution that is only accessible via a dataset. To tackle this challenge, generative models have become prominent as effective computational tools for learning to simulate new data. Essentially, these models involve learning a _generator_ capable of mapping a source distribution into new approximate samples from the target distribution.

One of the most productive approaches to generative modeling is based on deterministic and stochastic transport dynamics, that connect a target distribution with a base distribution. Typically, the target distribution represents the data set from which we want to generate new samples, while the base distribution is one that can be easily simulated or sampled. Regarding the dynamics, they correspond to SDEs Stochastic Differential Equations (SDEs) or Ordinary Differential Equations (ODEs), where the drift (for SDEs) or velocity field (for ODEs) is determined by solving a regression problem. This regression problem is usually addressed with appropriate neural networks and related training techniques [15, 14, 16, 17, 18, 19, 20, 21, 22].

Among these methods, score-base generative models (SGMs) and in particular diffusion models based on score matching [15, 2, 23, 24] was an important milestone. In a nutshell, these models involve transforming an arbitrary density into a standard Gaussian model and consists in learning the drift of the corresponding reversal process. More precisely, the idea is to first consideran Ornstein-Uhlenbeck (OU) process \((X^{\mathrm{OU}}_{t})_{t\in[0,T]}\), over a time interval \([0,T]\),

\[\mathrm{d}X^{\mathrm{OU}}_{t}=-(1/2)X^{\mathrm{OU}}_{t}\mathrm{d}t+\mathrm{d}B_{t }\,\quad X^{\mathrm{OU}}_{0}\sim\nu^{\star}\, \tag{1}\]

where \((B_{t})_{t\geqslant 0}\) is a \(d\)-dimensional Brownian motion. Then, the reversal process \((\overleftarrow{X}^{\mathrm{OU}}_{t})_{t\in[0,T]}\), which is defined from the non-homogeneous SDE [1]:

\[\mathrm{d}\overleftarrow{X}^{\mathrm{OU}}_{t}=\{(1/2)\overleftarrow{X}^{ \mathrm{OU}}_{t}+\nabla\log p^{\mathrm{OU}}_{T-t}(\overleftarrow{X}^{\mathrm{OU }}_{t})\}\mathrm{d}t+\mathrm{d}B_{t}\,\quad t\in[0,T]\quad\,\ \text{with}\ \overleftarrow{X}^{\mathrm{OU}}_{0}\sim\nu^{\star}P^{\mathrm{OU}}_{T}\, \tag{2}\]

allows the law of \(X^{\mathrm{OU}}_{T}\) to be transported to \(\nu^{\star}\): [1, Equations 3.11, 3.12] show that \(\overleftarrow{X}^{\mathrm{OU}}_{T}\) has distribution \(\nu^{\star}\). The initialization \(\nu^{\star}P^{\mathrm{OU}}_{T}\) in (2) is the distribution of \(X^{\mathrm{OU}}_{T}\) defined in (1). The drift in (2) can be decomposed as the sum of a linear function and the score associated with the density of \(X^{\mathrm{OU}}_{T-t}\) with respect to the Lebesgue measure, denoted by \(p^{\mathrm{OU}}_{T-t}\). From the Tweedie identity, this score is the solution to a regression problem that can be solved efficiently by score matching [1, 2, 20]. Learning this score at different times can also be formulated as a sequence of denoising problems. Once the drift of the reversal process is learned or equivalently the scores \((\nabla\log p^{\mathrm{OU}}_{t})_{t\in[0,T]}\), score-based generative models consist in following the reversal dynamics over \([0,T]\) or, more commonly, a discretization of it, starting with a sample from \(\mathrm{N}(0,\mathrm{Id})\). The final sample at time \(T\) is then approximatively distributed according to \(\nu^{\star}\). Note that an approximation is made even if the reversal dynamics were simulated exactly, because for full accuracy, the model would need to start from a sample of \(\nu^{\star}P^{\mathrm{OU}}_{T}\). However, it is well known that for sufficiently large \(T\), \(\nu^{\star}P^{\mathrm{OU}}_{T}\) is (exponentially) close to \(\mathrm{N}(0,\mathrm{Id})\).

When exploring diffusion models, it has been realized that the generation of approximate data samples could also be achieved using an ODE instead of the reversal diffusion:

\[\mathrm{d}\overleftarrow{X}^{\mathrm{OU}}_{t}/\mathrm{d}t=(1/2)(\overleftarrow {X}^{\mathrm{OU}}_{t}+\nabla\log p^{\mathrm{OU}}_{T-t}(\overleftarrow{X}^{ \mathrm{OU}}_{t}))\.\]

Similarly to the drift of the reversal diffusion, the velocity fields at time \(t\in[0,T]\) associated with this ODE is the sum of a linear function and the score of the density of \(X^{\mathrm{OU}}_{T-t}\). This observation has prompted the introduction of the Probability Flow ODE implementation of diffusion models [13]. SGMs in their standard and probability flow ODE implementations have achieved notable success in a range of applications; see e.g., [1, 15, 16, 17].

While diffusion-based methods have now become popular generative models, they can suffer from two limitations. First, there is a trade-off in selecting the time horizon \(T\) and second, they rely solely on Gaussian distributions as base distributions, in general. Therefore, there remains considerable interest in developing methods that consider a more general base distribution \(\mu\) and that accomplish the transport between \(\nu^{\star}\) and \(\mu\) relying on dynamics defined on a fixed finite time interval. Defining a generative process in finite time by means of a coupling and an interpolating process, [15], laid the foundation for Flow Matching (FM) models [1, 16, 17], LCBH\({}^{+}\)23, Liu22, LGL23], finally addressing this problem.

In its simplest form, the main strategy employed by FMs to bridge two distributions, involves a fixed coupling \(\pi\) between \(\nu^{\star}\) and \(\mu\) and the use of a bridge, _i.e._, a conditional distribution on the path space \(\mathrm{C}([0,1],\mathbb{R}^{d})\) of a reference process \((R_{t})_{t\in[0,1]}\) given its starting point \(R_{0}\) and end point \(R_{1}\). In case \(R_{t}\) is a deterministic function of \(R_{0}\) and \(R_{1}\), we say that the bridge is deterministic and stochastic otherwise. As \((R_{t})_{t\in[0,1]}\) corresponds to the solution of a stochastic differential equation, we coin the term Diffusion Flow Matching (DFM) to distinguish the latter case from the former one and focus on it. Then, this bridge and the coupling \(\pi\) between \(\nu^{\star}\) and \(\mu\) define an interpolated process \((X^{1}_{t})_{t\in[0,1]}\), referred to as an interpolant, defined as \((X^{1}_{0},X^{1}_{1})\sim\pi\) and \((X^{1}_{t})_{t\in[0,1]}\) given \((X^{1}_{0},X^{1}_{1})\) has the same conditional distribution as \((R_{t})_{t\in[0,1]}\) given \((R_{0},R_{1})\). However, \((X^{1}_{t})_{t\in[0,1]}\) does not correspond in general to the distribution of a diffusion or even to the one of a Markov process. This characteristic poses a challenge when dealing with potential stochastic sampling procedures: indeed, similarly to SGMs, FMs and DFMs aim to design a Markov process that approximatively transport \(\mu\) to \(\nu^{\star}\). To address this issue, most works proposing FM and DFM models [1, 17] rely on mimicking the marginal flow of the interpolated process \((X^{1}_{t})_{t\in[0,1]}\) through a diffusion process known as the Markovian projection. A remarkable feature of this diffusion lies in the fact that its drift is also a solution of a regression problem that can be approximatively solved using only samples from the interpolant \((X^{1}_{t})_{t\in[0,1]}\). Then, an approximate samples from \(\nu^{\star}\) is obtained by following a discretization of the dynamics associated with the considered Markovian projection, starting with a sample of \(\mu\).

While there exists now an important literature on theoretical guarantees for SGMs [1, 2, 3, 4, 5, 6, 7, 8, 9], only a few works have been considering FMs. In addition, up to our knowledge, these works on FMs only consider deterministic interpolants [1, 1, 2, 10]. The main objective of this paper is to fill this gap and to analyze DFMs using the \(d\)-dimensional Brownian motion as reference process, in which case the bridge is simply the \(d\)-dimensional standard Brownian bridge. We provide theoretical guarantees, upper bounding the Kullback Leibler divergence between the target distribution and the one resulting from the DFM. Our results consider the two sources of error coming from the DFM model, namely drift-estimation and time-discretization. This pursuit underscores the significance of comprehending and quantifying the factors influencing the performance of DFMs, paving the way for further advancements in generative modeling techniques.

Our contribution.In this work, we analyze a DFM model using as bridge the \(d\)-dimensional Brownian bridge and examine how it performs in two distinct scenarios: one without early-stopping and another with early-stopping. In our first main contribution Theorem 2, we establish an explicit and simple bound on the \(\mathrm{KL}\) divergence between the data distribution and the distribution at the final time of the DFM model. We achieve our bound without early stopping, by assuming only (1) moment conditions on the target \(\nu^{\star}\) and the base \(\mu\) (**H1**); (2) integrability conditions on the scores associated with the data distribution \(\nu^{\star}\), the base distribution \(\mu\) and the coupling \(\pi\) (**H2**); (3) a \(\mathrm{L}^{2}\)-drift-approximation error (**H3**) (an assumption commonly considered in previous works). Note that condition (2) implies that \(\nu^{\star}\) necessarily admits a density. We relax this condition in our second contribution. In Theorem 3, we establish an explicit bound on the \(\mathrm{KL}\) divergence between a smoothed version of the target distribution and an early stopped version of the DFM model, assuming (1) and (3), but replacing the condition (2) by assuming (4) \(\pi=\mu\otimes\nu^{\star}\) and integrability conditions only on the score associated with \(\mu\).

To the best of our knowledge, our paper provides the first convergence analysis for diffusion-type FMs, that tackles all the sources of error, _i.e._, the drift-approximation-error and the time-discretization error. In addition, previous studies concerning FMs and Probability Flow ODEs, with deterministic or mixed sampling procedure, either rely on at least some Lipschitz regularity of the flow velocity field or its estimator and/or do not take the time-discretization error into consideration. Also, in the context of SGMs with constant step-size, most of existing works without early-stopping are obtained assuming either the score (or its estimator) to be Lipschitz or the data distribution to satisfy some additional conditions (e.g., manifold hypothesis, bounded support, etc.); the unique exception being [1]. We refer to Section 3.2 for a more in depth literature comparison.

Notation.Given a measurable space \((\mathsf{E},\mathcal{E})\), we denote by \(\mathcal{P}(\mathsf{E})\) the set of probability measures of \(\mathsf{E}\). Also, given a topological space \((\mathsf{E},\tau)\), we use \(\mathcal{B}(\mathsf{E})\) to denote the Borel \(\sigma\)-algebra on \(\mathsf{E}\). Given two random variables \(Y,\tilde{Y}\), we write \(Y\perp\tilde{Y}\) to say that \(Y\) and \(\tilde{Y}\) are independent. Denote by \((B_{t})_{t\in[0,1]}\) a \(d\)-dimensional Brownian motion. We denote by \(\mathrm{Leb}^{d}\) the Lebesgue measure on \(\mathbb{R}^{d}\). Given two real numbers \(u,v\in\mathbb{R}\), we write \(u\lesssim v\) (resp. \(u\gtrsim v\)) to mean \(u\leq Cv\) (resp. \(u\geq Cv\)) for a universal constant \(C>0\). Also, we denote by \(\|x\|\) the Euclidean norm of \(x\in\mathbb{R}^{d}\), by \(\langle x,y\rangle\) the scalar product between \(x,y\in\mathbb{R}^{d}\), and by \(x^{T}\) the transpose of \(x\). Given a matrix \(\mathbf{A}\in\mathbb{R}^{d\times s}\), we denote by \(\left\|\mathbf{A}\right\|_{\mathrm{op}}\) the operator norm of \(\mathbf{A}\). For \(f:[0,1]\times\mathbb{R}^{d}\to\mathbb{R}\) regular enough, we denote by \(\nabla_{x}f(t,x),\nabla_{x}^{2}f(t,x)\) and \(\Delta_{x}f(t,x)\) respectively the gradient, hessian and laplacian of \(f\), defined for \(t,x\in[0,1]\times\mathbb{R}^{d}\) by \(\nabla_{x}f(t,x):=(\partial_{x_{i}}f(t,x))_{i}\), \(\nabla_{x}^{2}f(t,x):=(\partial_{x_{i}}\partial_{x_{j}}f(t,x))_{i,j}\), \(\Delta f(t,x):=\sum_{i=1}^{d}\partial_{x_{i}}^{2}f(t,x)\), where \(\partial_{x_{j}}\) denotes the partial derivative with respect to the \(j\)-th variable. For \(F:[0,1]\times\mathbb{R}^{d}\to\mathbb{R}^{d}\) regular enough, we denote by \(D_{x}F\), \(\operatorname{div}_{x}F\) and \(\Delta_{x}F\) respectively, the Jacobian matrix, the divergence and the vectorial laplacian of \(F\), defined for \(t,x\in[0,1]\times\mathbb{R}^{d}\) by \(D_{x}F(t,x)=(\partial_{x_{j}}F_{i}(t,x))_{i,j}\), \(\operatorname{div}_{x}F(t,x):=\sum_{j=1}^{d}\partial_{x_{j}}F_{j}(t,x)\), \(\Delta_{x}F(t,x)=(\Delta_{x}F_{1}(t,x),...,\Delta_{x}F_{d}(t,x))\).

## 2 Diffusion Flow Matching.

Given a target distribution \(\nu^{\star}\in\mathcal{P}(\mathbb{R}^{d})\) and a base distribution \(\mu\in\mathcal{P}(\mathbb{R}^{d})\), the idea at the core of FM models is intuitively to construct a path between these two by considering two ingredients (1) a coupling \(\pi\) and (2) a bridge (or an interpolant following [1]) between \(\mu\) and \(\nu^{\star}\) (or more precisely a bridge with foundations \(\pi\)). More formally, here we say that \(\pi\) is a coupling between \(\mu\) and \(\nu^{\star}\) if for any \(\mathsf{A}\in\mathcal{B}(\mathbb{R}^{d})\), \(\pi(\mathsf{A}\times\mathbb{R}^{d})=\mu(\mathsf{A})\) and \(\pi(\mathbb{R}^{d}\times\mathsf{A})=\nu^{\star}(\mathsf{A})\), and denote by \(\Pi(\mu,\nu^{\star})\) the set of coupling between \(\mu\) and \(\nu^{\star}\). Then, based on a probability measure on \(\mathbb{W}=\mathrm{C}([0,1]\,,\mathbb{R}^{d})\) the set of continuous functions from \([0,1]\) to \(\mathbb{R}^{d}\), we define the bridge \(\mathrm{b}\mathbb{Q}\) associated with \(\mathbb{Q}\) as the Markov kernel \(\bowtie\) on \(\mathbb{R}^{2d}\times\mathbb{W}\), such that, for any \(\mathsf{A}\in\mathcal{B}(\mathbb{W})\), \(\mathbb{Q}(\mathsf{A})=\int\mathbb{Q}_{0,1}\mathrm{d}(x_{0},x_{1})\mathsf{D} \mathbb{Q}((x_{0},x_{1}),\mathsf{A})\) (see e.g., [13, Theorem 8.37] for the existence of this kernel), where for any \(\mathsf{I}=\{t_{1},\ldots,t_{n}\}\subset[0,1]\), \(t_{1}<\cdots<t_{n}\), \(\mathbb{Q}_{\mathsf{I}}\) is the \(\mathsf{I}\)-marginal distribution of \(\mathbb{Q}\), _i.e._, the pushforward measure of \(\mathbb{Q}\) by \((x_{t})_{t\in[0,1]}\mapsto(x_{t_{1}},\ldots,x_{t_{n}})\). From a probabilistic perspective, it implies that if \((W_{t})_{t\in[0,1]}\sim\mathbb{Q}\), then \(\mathrm{b}\mathbb{Q}\) is a conditional distribution of \((W_{t})_{t\in[0,1]}\sim\mathbb{Q}\) given \((W_{0},W_{1})\): for any bounded and measurable function on \(\mathbb{W}\), \(\mathbb{E}[f((W_{t})_{t\in[0,1]})|X_{0},X_{1}]=\int f((w_{t})_{t\in[0,1]}) \mathbb{Q}((W_{0},W_{1}),\mathrm{d}(w_{t})_{t\in[0,1]})\).

### Definition of the interpolated process

Consider now a coupling \(\pi\) and a bridge \(\bowtie^{\beta}\) associated to \(\mathbb{Q}^{\beta}\in\mathcal{P}(\mathbb{W})\). We suppose here that \(\mathbb{Q}^{\beta}\) is the distribution of \((Y_{t})_{t\in[0,1]}\) solution of the stochastic differential equation

\[\mathrm{d}Y_{t}=\beta(Y_{t})\mathrm{d}t+\sqrt{2}\mathrm{d}B_{t}\,\quad t\in[0,1]\,\quad Y_{0}\sim\mu\in\mathcal{P}(\mathbb{R}^{d})\, \tag{3}\]

where \((B_{t})_{t\in\mathbb{R}_{+}}\) is a standard \(d\)-dimensional Brownian motion. In addition, we suppose \(\beta\in\mathrm{C}^{\infty}(\mathbb{R}^{d},\mathbb{R}^{d})\) for simplicity and that (3) admits a unique strong solution. Consider now, the interpolated measure \(\mathbb{I}(\pi,\mathbb{Q}^{\beta})\)1 corresponding to the distribution of the process defined by \((X_{0}^{\mathrm{I}},X_{1}^{\mathrm{I}})\sim\pi\) and \((X_{t}^{\mathrm{I}})_{t\in[0,1]}(X_{0}^{\mathrm{I}},X_{1}^{\mathrm{I}})\sim \mathsf{b}\mathbb{Q}^{\beta}((X_{0}^{\mathrm{I}},X_{1}^{\mathrm{I}}),\cdot)\). In [1], \((X_{t}^{\mathrm{I}})_{t\in[0,1]}\) is referred to as a stochastic interpolant. Denote by \((s,t,x,y)\mapsto p_{t|s}^{Y}(y|x)\) the conditional density of \(Y_{t}\) given \(Y_{s}\) with respect to the Lebesgue measure and by \((p_{t}^{\mathrm{I}})_{t\in[0,1]}\) the time marginal densities of \((X_{t}^{\mathrm{I}})_{t\in[0,1]}\) with respect to the Lebesgue measure, that is \(\mathbb{P}(Y_{t}\in\mathsf{A}|Y_{s})=\int_{\mathsf{A}}p_{t|s}(x|Y_{s})\mathrm{d}x\) and \(\mathbb{P}(X_{t}^{\mathrm{I}}\in\mathsf{A})=\int_{\mathsf{A}}p_{t}^{\mathrm{I}}(x)\mathrm{d}x\), for \(\mathsf{A}\in\mathcal{B}(\mathbb{R}^{d})\) and \(s,t\in[0,1]\), \(s\neq t\). Then, note that, as a straightforward consequence of the definition of \((X_{t}^{\mathrm{I}})_{t\in[0,1]}\), it holds

Footnote 1: It corresponds to \(\pi\otimes\mathsf{b}\mathbb{Q}^{\beta}\) the tensor product between \(\pi\) and \(\mathsf{b}\mathbb{Q}^{\beta}\): \(\mathbb{I}(\pi,\mathbb{Q}^{\beta})(\mathsf{A})=\int\mathsf{b}\mathbb{Q}^{ \beta}((x_{0},x_{1}),\mathsf{A})\mathrm{d}\pi(x_{0},x_{1})\), \(\mathsf{A}\in\mathcal{B}(\mathbb{R}^{d})\). In other words, \(\pi\)-mixture of \(\mathbb{Q}^{\beta}\)-bridges.

\[p_{t}^{\mathrm{I}}(x)=\int_{\mathbb{R}^{2d}}p_{t|0}^{Y}(x|x_{0})p_{1|t}^{Y}(x_{ 1}|x)\tilde{\pi}(\mathrm{d}x_{0},\mathrm{d}x_{1})\, \tag{4}\]

where

\[\tilde{\pi}(\mathrm{d}x_{0},\mathrm{d}x_{1})=\frac{\pi(\mathrm{d}x_{0},\mathrm{d}x_{1})}{p_{1|0}^{Y}(x_{1}|x_{0})}\.\]

An example that we will focus on in this paper is \(\mathbb{Q}^{\beta}=\mathbb{B}\) the distribution of the Brownian motion \((\sqrt{2}B_{t})_{t\in\mathbb{R}}\) solution of (3) with \(\beta\equiv 0\). Then, it is well known that \(\mathrm{b}\mathbb{B}\) is then the Markov kernel associated with the Brownian bridge and the resulting stochastic interpolant satisfies for any \(t\in[0,1]\)

\[X_{t}^{\mathrm{I}}\stackrel{{\text{dist}}}{{=}}(1-t)X_{0}^{ \mathrm{I}}+tX_{1}^{\mathrm{I}}+\sqrt{2t(1-t)}\mathrm{Z}\,\quad\mathrm{Z}\sim\mathrm{N}(0,\mathrm{Id})\, \tag{5}\]

where \(\stackrel{{\text{dist}}}{{=}}\) denotes the equality in distribution.

It is well-known that for any \(x_{0},x_{1}\in\mathbb{R}^{d}\), the distribution \(\mathsf{b}\mathbb{Q}^{\beta}((x_{0},x_{1}),\cdot)\) is diffusion-like under appropriate conditions. More precisely, \(\mathsf{b}\mathbb{Q}^{\beta}((x_{0},x_{1}),\cdot)\) is the distribution of \((\mathbf{Y}_{t})_{t\in[0,1]}\) solution to

\[\mathrm{d}\mathbf{Y}_{t}=\{\beta(\mathbf{Y}_{t})+2\nabla\Phi_{t}^{x_{1}}( \mathbf{Y}_{t})\}\mathrm{d}t+\sqrt{2}\mathrm{d}B_{t}\,\quad t\in[0,1]\,\quad\mathbf{Y}_{0}=x_{0}\, \tag{6}\]

where \(\Phi_{t}^{x_{1}}(y)=\log p_{1|t}^{Y}(x_{1}|y)\). For instance, for \(x_{0},x_{1}\in\mathbb{R}^{d}\), the bridge \(\mathrm{b}\mathbb{B}((x_{0},x_{1}),\cdot)\) associated to \(\mathbb{B}\) is the distribution of a Brownian bridge \((\bar{B}_{t}^{x_{0},x_{1}})_{t\in[0,1]}\) solution to the SDE

\[\mathrm{d}\bar{B}_{t}^{x_{0},x_{1}}=\frac{x_{1}-\bar{B}_{t}^{x_{0},x_{1}}}{1-t} \mathrm{d}t+\sqrt{2}\mathrm{d}B_{t}\,\quad t\in[0,1]\,\quad\bar{B}_{0}^{x_{0},x_{1}}=x_{0}\.\]

From (6), it turns out that \((X_{t}^{\mathrm{I}})_{t\in[0,1]}\) given \((X_{0}^{\mathrm{I}},X_{1}^{\mathrm{I}})\) is therefore solution to

\[\mathrm{d}X_{t}^{\mathrm{I}}=\{\beta(X_{t}^{\mathrm{I}})+2\nabla\Phi_{t}^{X_{1}}( X_{t}^{\mathrm{I}})\}\mathrm{d}t+\sqrt{2}\mathrm{d}B_{t}\,\quad t\in[0,1]\,\quad X_{0}^{\mathrm{I}}\sim\mu. \tag{7}\]Note that the drift coefficient in (7) depends on \(X^{1}_{1}\), and therefore, \((X^{1}_{t})_{t\in[0,1]}\) is not Markov, which is a natural property if we are interested in constructing a generative process. To circumvent this issue, based on \(\mathbb{I}(\pi,\mathbb{Q}^{\beta})\), we aim to define a distribution \(\mathbb{M}(\pi,\mathbb{Q}^{\beta})\) such that it has the same one-dimensional time marginals as \(\mathbb{I}(\pi,\mathbb{Q}^{\beta})\) and corresponds to a diffusion, _i.e._, if \((X^{1}_{t})_{t\in[0,1]}\sim\mathbb{I}(\pi,\mathbb{Q}^{\beta})\) and \((X^{\mathrm{M}}_{t})_{t\in[0,1]}\sim\mathbb{M}(\pi,\mathbb{Q}^{\beta})\), for any \(t\in[0,1]\), \(X^{1\text{ dist}}_{t}\stackrel{{\text{ dist}}}{{=}}X^{\mathrm{M}}_{t}\) and \((X^{\mathrm{M}}_{t})_{\in\mathbb{N}}\) is solution of a diffusion with Markov coefficient. This can be done trough the Markovian projection.

### Markovian projection and Diffusion Flow Matching

Markovian projection.The idea of Markovian projection originally dates back to [13] and [14]. Its main idea is in essence to define a diffusion Markov process which "mimics" the time-marginal of an Ito process:

\[\mathrm{d}\mathbf{X}_{t}=\mathbf{b}_{t}\mathrm{d}t+\sqrt{2}\mathrm{d}B_{t}\;, \quad t\in[0,1]\;,\quad\mathbf{X}_{0}\sim\mu\;,\]

for some adapted process \((\mathbf{b}_{t})_{t\in[0,1]}\) and initial distribution \(\mu\). Under appropriate conditions (see [1, Corollary 3.7]), this diffusion process, denoted by \((\mathbf{X}^{\mathrm{M}}_{t})_{t\in[0,1]}\), exists and is solution to an SDE with a (relatively) simple modification of the drift \(\mathbf{b}_{t}\), namely

\[\mathrm{d}\mathbf{X}^{\mathrm{M}}_{t}=\tilde{\mathbf{b}}_{t}(\mathbf{X}^{ \mathrm{M}}_{t})\mathrm{d}t+\sqrt{2}\mathrm{d}B_{t}\;,\quad t\in[0,1]\;,\quad \mathbf{X}^{\mathrm{M}}_{0}\sim\mu\;,\]

where \(\tilde{\mathbf{b}}_{t}(\mathbf{X}^{\mathrm{M}}_{t})=\mathbb{E}[\mathbf{b}_{t}| \mathbf{X}^{1}_{t}]\). This result can be applied to the non-Markov process \((X^{1}_{t})_{t\in[0,1]}\) solution of (7): its Markovian projection is solution of

\[\mathrm{d}X^{\mathrm{M}}_{t}=\tilde{\beta}_{t}(X^{\mathrm{M}}_{t})\mathrm{d}t+ \sqrt{2}\mathrm{d}B_{t}\;,\quad t\in[0,1]\;,\]

for some function \(\tilde{\beta}:[0,1]\times\mathbb{R}^{d}\to\mathbb{R}^{d}\). It turns out that we can identify \(\tilde{\beta}\), relying on the family of conditional densities \((p^{\prime}_{t|s})_{0\leq s\leq t\leq 1}\) and marginal densities \((p^{1}_{t})_{0\leq t\leq 1}\). This is the content of the following result.

**Theorem 1**.: _Consider a \(\pi\in\Pi(\mu,\nu^{\star})\) and \(\mathbb{Q}^{\beta}\) associated with (3). Consider the drift field_

\[\tilde{\beta}^{Y}_{t}(x)=\beta(x)+2\frac{\int\nabla_{x}\log p^{Y}_{1|t}(x_{1}| x)p^{Y}_{t|0}(x|x_{0})p^{Y}_{1|t}(x_{1}|x)\tilde{\pi}(\mathrm{d}x_{0}, \mathrm{d}x_{1})}{p^{1}_{t}(x)}\;. \tag{8}\]

_Under appropriate conditions (see Appendix A.1), the Markov process \((X^{\mathrm{M}}_{t})_{t\in[0,1]}\) solution of_

\[\mathrm{d}X^{\mathrm{M}}_{t}=\tilde{\beta}^{Y}_{t}(X^{\mathrm{M}}_{t})\mathrm{ d}t+\sqrt{2}\mathrm{d}B_{t}\;,\quad t\in[0,1)\;,\quad X^{\mathrm{M}}_{0}\sim \mu\;, \tag{9}\]

_mimics the one-dimensional time marginals of \(\mathbb{I}(\pi,\mathbb{Q}^{\beta})\), i.e., for any \(t\in[0,1)\), \(X^{1\text{ dist}}_{t}\stackrel{{\text{ dist}}}{{=}}X^{\mathrm{M}}_{t}\)._

This result is well-know, but, for sake of completeness, we provide the proof in Appendix A.1.

The process (9) is known as the _Markovian projection_ of \(\mathbb{I}(\pi,\mathbb{Q}^{\beta})\), and its drift (8) as _mimicking drift_. In what follows, we denote by \(\mathbb{M}(\pi,\mathbb{Q}^{\beta})\) the distribution of \((X^{\mathrm{M}}_{t})_{t\in[0,1]}\) on \(\mathbb{W}\).

_Remark 1_.: It can be easily shown by continuity that \(X^{\mathrm{M}}_{t}=X^{1}_{t}\Rightarrow X^{1}_{1}\) for \(t\to 1\), where \(\Rightarrow\) denotes the convergence in distribution. As \(\text{Law}(X^{1}_{1})=\nu^{\star}\), the Markovian projection therefore gives a an _ideal_ generative model which would consist in following the SDE (9) with initial distribution \(\mu\).

_Remark 2_.: Note that, because of (4), the mimicking drift (8) rewrites as

\[\tilde{\beta}^{Y}_{t}(X^{1}_{t})=\mathbb{E}\Big{[}2\nabla_{x}\log p^{Y}_{1|t}( X^{1}_{1}|X^{1}_{t})+\beta(X^{1}_{t})|X^{1}_{t}\Big{]}\;.\]

Diffusion Flow Matching.Eventually, as pointed out in Remark 1, the Markovian projection gives a an _ideal_ generative model. However, a) the mimicking drift (8) is intractable and b) the continuous-time SDE (9) can not be numerically simulated. Thus, in order to implement the proposed theoretical idea, we first need to address and overcome the aforementioned computational challenges. To circumvent a), observe that, because of Remark 2 and [13, Corollary 8.17], we can approximate the mimicking drift via solving

\[\min_{\theta\in\Theta}\mathbb{E}\Big{[}\left\|s^{Y}_{\theta}(t,X^{1}_{t})- \tilde{\beta}^{Y}_{t}(X^{1}_{t})\right\|^{2}\Big{]}\;, \tag{10}\]for a properly chosen class of neural networks \(\{(t,x)\mapsto s^{Y}_{\theta}\,(t,x)\}_{\theta\in\Theta}\), and replace \(\tilde{\beta}^{Y}\) in (9) with \(s^{Y}_{\theta^{\star}}\), where \(\theta^{\star}\in\Theta\) denotes a minimizer of (10). To deal with b), we simply make use of the Euler-Maruyama scheme, _i.e._, for a choice of sequence of step sizes \(\{h_{k}\}_{k=1}^{N}\), \(N\geqslant 1\), and the corresponding time discretization \(t_{k}=\sum_{i=1}^{k}h_{i}\), such that \(t_{0}=0\) and \(t_{N}=1\), we define the continuous process \((X^{\theta^{\star}}_{t})_{t\in[0,1]}\) recursively on the intervals \([t_{k},t_{k+1}]\) by

\[\mathrm{d}X^{\theta^{\star}}_{t}=s^{Y}_{\theta^{\star}}(t_{k},X^{\theta^{\star }}_{t_{k}})\mathrm{d}t+\sqrt{2}\mathrm{d}B_{t}\;,\quad t\in[t_{k},t_{k+1}]\;, \quad\text{with}\quad X^{\theta^{\star}}_{0}\sim\mu\;. \tag{11}\]

(11) is the DFM generative model we are going to analyze.

## 3 Main results

In this section, we provide convergence guarantees in Kullback-Leibler divergence for the Diffusion Flow Matching model (11), under mild assumptions on the \(\mu,\nu^{\star},\pi\) and \(s_{\theta^{\star}}\), either within a non-early-stopping regime or within a early-stopping regime.

From now on, we consider the case \(\beta\equiv 0\), _i.e._, \(\mathbb{Q}^{\beta}=\mathbb{B}\). We show in Appendix A.2, Remark 9, that, under out set of assumptions, the conditions of Theorem 1 hold for this setup. Moreover, in this case, for any \(s,t\in[0,1]\), \(s<t\) and \(x,y\in\mathbb{R}^{d}\), the conditional density \(p^{Y}_{t|s}(y|x)\equiv p_{t-s}(y|x)\) where \((t,x,y)\mapsto p_{t}(y|x)\) is the heat kernel:

\[p_{t}(y|x)=\frac{1}{(4\pi t)^{d/2}}\exp\Big{(}-\frac{\|y-x\|^{2}}{4t}\Big{)}\;,\quad t\in(0,1]\;. \tag{12}\]

In the following, we set \(\tilde{\beta}^{Y}\equiv\tilde{\beta}\) and \(s^{Y}_{\theta^{\star}}\equiv s_{\theta^{\star}}\).

### Convergence Bounds.

We assume moment conditions on the probability measures \(\mu\) and \(\nu^{\star}\), and mild integrability conditions on the probability distributions \(\mu,\nu^{\star}\) and on the coupling \(\pi\).

For \(p\geqslant 1\), we denote for \(\zeta\in\mathcal{P}(\mathbb{R}^{d})\),

\[\mathbf{m}_{p}[\zeta]=\int\|x\|^{p}\,\mathrm{d}\zeta(x)\;.\]

**H1**.: _The probability distributions \(\mu,\nu^{\star}\) satisfy \(\mathbf{m}_{8}[\mu]+\mathbf{m}_{8}[\nu^{\star}]<+\infty\)._

**H2**.: _The probability distributions \(\nu^{\star}\), \(\mu\) and \(\pi\) are absolutely continuous with respect to the Lebesgue measure on \(\mathbb{R}^{d}\) and \(\mathbb{R}^{2d}\) respectively, and satisfy_

_(i) The functions \(\log\mathrm{d}\mu/\mathrm{d}\mathrm{Leb}^{d}\) and \(\log\mathrm{d}\nu^{\star}/\mathrm{d}\mathrm{Leb}^{d}\) are continuously differentiable and satisfy \(\|\nabla\log\nu^{\star}\|^{8}_{\mathrm{L}^{8}(\nu^{\star})}+\|\nabla\log\mu\| ^{8}_{\mathrm{L}^{8}(\mu)}<+\infty\) where for \(\zeta\in\{\nu^{\star},\mu\}\),_

\[\|\nabla\log\zeta\|^{8}_{\mathrm{L}^{8}(\zeta)}=\int\left\|\nabla\log\Big{(} \frac{\mathrm{d}\zeta}{\mathrm{d}\mathrm{Leb}^{d}}\Big{)}(x_{0})\right\|^{8} \mathrm{d}\zeta(x_{0})\;.\]

_(ii) The function \(\log\mathrm{d}\pi/\mathrm{d}\mathrm{Leb}^{2d}\) is continuously differentiable and satisfies \(\|\nabla\log\tilde{\pi}\|^{8}_{\mathrm{L}^{8}(\pi)}<+\infty\) where_

\[\|\nabla\log\tilde{\pi}\|^{8}_{\mathrm{L}^{8}(\pi)}=\int\|\nabla\log\left( \tilde{\pi}\right)(x_{0},x_{1})\|^{8}\,\mathrm{d}\pi(x_{0},x_{1})\;,\quad \tilde{\pi}(x_{0},x_{1})=\frac{1}{p_{1}(x_{1}|x_{0})}\frac{\mathrm{d}\pi}{ \mathrm{d}\mathrm{Leb}^{2d}}(x_{0},x_{1})\;, \tag{13}\]

_and \(p_{1}\) is defined in (12)._

_Remark 3_.: Under **H**1, note that \(\|\nabla\log\tilde{\pi}\|^{8}_{\mathrm{L}^{8}(\pi)}<+\infty\) is equivalent by (12) and \(\pi\in\Pi(\mu,\nu^{\star})\) to

\[\int\|\nabla\log(\mathrm{d}\pi/\mathrm{d}\mathrm{Leb}^{2d})(x_{0},x_{1})\|^{8 }\mathrm{d}\pi(x_{0},x_{1})<+\infty\;.\]_Remark 4_.: We can relax the condition that \(\log\mathrm{d}\mu/\mathrm{d}\mathrm{Leb}^{d}\), \(\log\mathrm{d}\nu^{\star}/\mathrm{d}\mathrm{Leb}^{d}\) and \(\log\mathrm{d}\pi/\mathrm{d}\mathrm{Leb}^{2d}\) are continuously differentiable assuming that \(\sqrt[s]{\mathrm{d}\mu/\mathrm{d}\mathrm{Leb}^{d}}\), \(\sqrt[s]{\mathrm{d}\nu^{\star}/\mathrm{d}\mathrm{Leb}^{d}}\) and \(\sqrt[s]{\mathrm{d}\pi/\mathrm{d}\mathrm{Leb}^{2d}}\) belongs to some Sobolev space, but, for ease of presentation, we prefer not to delve into these technical details.

Moreover, we assume to have estimated the mimicking drift with an \(\varepsilon^{2}\)-precision, for some \(\varepsilon^{2}>0\) sufficiently small.

**H3**.: _There exist \(\theta^{\star}\in\Theta\) and \(\varepsilon^{2}>0\) such that_

\[\sum_{k=0}^{N-1}h_{k+1}\mathbb{E}\Big{[}\left\|s_{\theta^{\star}}(t_{k},X_{t_{ k}}^{\mathrm{M}})-\tilde{\beta}_{t_{k}}(X_{t_{k}}^{\mathrm{M}})\right\|^{2} \Big{]}\leq\varepsilon^{2}\;.\]

_Remark 5_.: To be coherent with the previous section, observe that, as a consequence of Theorem 1, for any \(k=0,\cdots,N\), it holds

\[\mathbb{E}\Big{[}\left\|s_{\theta^{\star}}(t_{k},X_{t_{k}}^{\mathrm{M}})- \tilde{\beta}_{t_{k}}(X_{t_{k}}^{\mathrm{M}})\right\|^{2}\Big{]}=\mathbb{E} \Big{[}\left\|s_{\theta^{\star}}(t_{k},X_{t_{k}}^{\mathrm{I}})-\tilde{\beta}_ {t_{k}}(X_{t_{k}}^{\mathrm{I}})\right\|^{2}\Big{]}\;.\]

Under such assumptions, we derive an upper bound on the KL divergence between the data distribution \(\nu^{\star}\) and the output of the DFM (11):

**Theorem 2**.: _Consider a uniform partition of \([0,1]\) with a constant stepsize \(h_{k}\equiv h\), \(h=1/N_{h}>0\), for \(N_{h}\in\mathbb{N}^{*}\) and consider the corresponding process \((X_{t}^{\theta^{\star}})_{t\in[0,1]}\) defined in (11). Assume **H1** to 3. Denoting by \(\nu_{1}^{\theta^{\star}}\) the distribution of \(X_{1}^{\theta^{\star}}\), we have that_

\[\mathrm{KL}(\nu^{\star}|\nu_{1}^{\theta^{\star}})\lesssim\varepsilon ^{2}+h(h^{1/8}+1)\Big{(}d^{4}+\mathbf{m}_{8}[\mu]+\mathbf{m}_{8}[\nu^{\star}]+ \|\nabla\log\tilde{\pi}\|_{\mathrm{L}^{8}(\pi)}^{8}\\ +\|\nabla\log\mu\|_{\mathrm{L}^{8}(\mu)}^{8}+\|\nabla\log\nu^{ \star}\|_{\mathrm{L}^{8}(\nu^{\star})}^{8}\Big{)}\;. \tag{14}\]

_Remark 6_.: Under almost the same conditions as Theorem 2, _i.e._, **H1**  **H2**, **H3**, replacing \(\tilde{\pi}\) in (13) by

\[\tilde{\pi}_{T}(x_{0},x_{1})=\frac{1}{p_{T}(x_{1}|x_{0})}\frac{\mathrm{d}\pi}{ \mathrm{d}\mathrm{Leb}^{2d}}(x_{0},x_{1})\;, \tag{15}\]

our proofs apply also to DFM using a time horizon \(T>0\) and the Brownian bridge on \([0,T]\). In particular, we would have obtained similar bounds as the ones derived in Theorem 2 but with a factor \(\max(1,T^{8})\) in front of the second addend.

_Remark 7_.: Choosing, in Theorem 2,

\[N_{h}=\frac{d^{4}+\mathbf{m}_{8}[\mu]+\mathbf{m}_{8}[\nu^{\star}]+\|\nabla \log\tilde{\pi}\|_{\mathrm{L}^{8}(\pi)}^{8}+\|\nabla\log\mu\|_{\mathrm{L}^{8}( \mu)}^{8}+\|\nabla\log\nu^{\star}\|_{\mathrm{L}^{8}(\nu^{\star})}^{8}}{ \varepsilon^{2}}\]

makes the approximation error of order \(\mathcal{O}(\varepsilon^{2})\) and the complexity of order \(\mathcal{O}(\varepsilon^{-2})\).

Using an early-stopping procedure, we obtain in the case \(\pi\) is the independent coupling:

**Theorem 3**.: _Fix \(0<\delta<1/2\). Consider a uniform partition of \([0,1]\) with a constant stepsize \(h_{k}\equiv h\), \(h=1/N_{h}>0\), for \(N_{h}\in\mathbb{N}^{*}\) and consider the corresponding process \((X_{t}^{\theta^{\star}})_{t\in[0,1]}\) defined in (11). Assume **H1**, **H3** and \(\pi=\mu\otimes\nu^{\star}\) to be the independent coupling. Suppose in addition that \(\mu\) is absolutely continuously with respect to the Lebesgue measure, \(\log\mathrm{d}\mu/\mathrm{d}\mathrm{Leb}^{d}\) is continuously differentiable and satisfies \(\|\nabla\log\mu\|_{\mathrm{L}^{8}(\mu)}^{8}<+\infty\)._

_Then, denoting by \(\nu_{1-\delta}^{\star}\) and \(\nu_{1-\delta}^{\theta^{\star}}\) the distribution of \(X_{1-\delta}^{\mathrm{M}}\) and \(X_{1-\delta}^{\theta^{\star}}\) respectively, we have that_

\[\mathrm{KL}(\nu_{1-\delta}^{\star}|\nu_{1-\delta}^{\theta^{\star}})\lesssim \varepsilon^{2}+h(h^{1/8}+1)\Big{(}\frac{d^{4}}{\delta^{4}}+\mathbf{m}_{8}[ \mu]+\mathbf{m}_{8}[\nu^{\star}]\frac{1}{\delta^{8}}+\|\nabla\log\mu\|_{ \mathrm{L}^{8}(\mu)}^{8}\Big{)}\;.\]

**Corollary 1**.: _Fix \(\delta=\mathcal{O}(\sqrt{\varepsilon})\). Consider a uniform partition of \([0,1]\) with a constant stepsize \(h=\mathcal{O}(\min(\varepsilon^{4}/d^{4},\varepsilon^{6}))\), and consider the corresponding process \((X_{t}^{\theta^{\star}})_{t\in[0,1]}\) defined in (11). Assume **H1**, **H3** and \(\pi=\mu\otimes\nu^{\star}\) to be the independent coupling. Suppose in addition that \(\mu\)is absolutely continuously with respect to the Lebesgue measure, \(\log\mathrm{d}\mu/\mathrm{d}\mathrm{Leb}^{d}\) is continuously differentiable and satisfies \(\|\nabla\log\mu\|_{\mathrm{L}^{8}(\mu)}^{8}<+\infty\). Then, denoting by \(\nu_{1-\delta}^{\star}\) and \(\nu_{1-\delta}^{\theta^{\star}}\) the distribution of \(X_{1-\delta}^{\mathrm{M}}\) and \(X_{1-\delta}^{\theta^{\star}}\) respectively, we have that_

\[\mathscr{W}_{2,\text{FM}}^{2}(\nu_{1-\delta}^{\star}|\nu_{1-\delta}^{\theta^{ \star}})\lesssim\varepsilon^{2}\,\]

_where \(\mathcal{W}_{2,\text{FM}}\) denotes the Fortet- Mourier distance of order 2, i.e._

\[\mathscr{W}_{2,\text{FM}}^{2}(\mu,\nu)=\inf_{\pi\in\Pi(\mu,\nu)}\int\min\{\|x- y\|^{2},1\}\mathrm{d}\pi(x,y)\.\]

### Related works and comparison with existing literature.

FMs stand at the forefront of innovation in generative modeling, offering a practical solution to the longstanding challenge of bridging two arbitrary distributions within a finite time interval. Their consequent immense potential has prompted substantial research efforts aimed at providing a theoretical explanation for their effectiveness and has put SGMs and Probability Flow ODEs all in perspective. In this section we report and discuss previous researches on FMs, SGMs and Probability Flow ODEs with the purpose of highlighting the links and differences with our work and contextualizing our contribution.

Non-early-stopping setting.In the context of FMs, [1] and [1] seek convergence guarantees in \(2\)-Wasserstein distance. Both consider more general designs for the stochastic interpolant and a deterministic sampling procedure, rather than a stochastic one. However, both works rely on some regularity condition on the approximated flow velocity filed, _i.e._, that it is Lipschitz. Namely, [1] works under a \(K\)-Lipschitz (uniform in time and space) assumption on the estimator of the exact flow velocity field. How such assumption pertains to the flow matching framework for generative modeling is not articulated and remains unclear. On the other hand, [1] assumes the estimator of the exact flow velocity field to be \(L_{t}\)-Lipschitz for any \(t\in[0,1]\) and discuss in [1], Theorem 2] how such assumption relate to the setting : under the additional assumption [1], Assumption 4], the true flow velocity field is proven to be \(L_{t}\)-Lipschitz in space for any \(t\in[0,1]\). Therefore, [1] enhances the findings of [1]. However, [1], Assumption 4] is not an usual conditions considered in papers about convergence guarantees for SGM and it is unclear which type of distributions satisfy [1, Assumption 4]. Moreover, both works [1, 1] do not take into account the discretization error in their analysis.

In the context of Probability Flow ODEs, [13] and [2] investigate the performance of such models in Total Variation distance and 2-Wasserstein distance. In contrast to [1] and [1], [1] and [2] examine the error coming from the (prerequisite when implementing an algorithm) introduction of a time-discretization scheme. However, once again, the provided bounds work under smoothness assumptions either on the score or on its estimator. More precisely, the result reported in [13] depends on a small \(\mathrm{L}^{2}\)-Jacobian-estimation error assumption, besides a classical small \(\mathrm{L}^{2}\)-score-estimation error assumption. As for [2], they assume the score to be Lipschitz in time and the data to be smooth and log-concave. In contrast, our result do not make such assumptions. Finally, to the best of our knowledge, the recent work [2] represents the state of art in the context of SGMs without early-stopping procedure: [2, Theorem 2.1] provides a sharp bound in \(\mathrm{KL}\) divergence between the data distribution and the law of the SGM both in the overdamped and kinetic setting under the sole assumptions of an \(\mathrm{L}^{2}\)-score-approximation error and that the data distribution has finite Fisher information with respect to the standard Gaussian distribution. All previous results are obtained assuming either some Lipschitz condition on the score and/or its estimator ([2], [3]) or a manifold hypothesis on the data distribution ([1]). However, we underline that the FM framework enables to consider a significantly wider range of interpolating paths compared to SGMs and to avoid the trade-off concerning the time horizon \(T\) which is inevitable when dealing with SGMs.

Early-stopping procedure.The recent work [1] establishes convergence guarantees in \(2\)-Wasserstein distance for FMs based on a deterministic sampling procedure. However, the results of [1] requires to interpolate with a Gaussian distribution and applies only to data distributions which either have a bounded support, are strongly log-concave, or are the convolution between a Gaussian and an other probability distribution supported on an Euclidean Ball. In contrast, for our bound to hold true we only need the data distribution \(\nu^{\star}\) and its score to have finite eight-ordermoment. Furthermore, even if [13] goes into depth when dealing with the statistical analysis of the estimator and the \(\mathrm{L}^{2}\)-estimation error, the entire investigation therein pursued depends on the choice of ReLUnetworks with Lipschitz regularity control to approximate the velocity field. They motivate such choice by proving (see [13, Theorem 5.1]) Lipschitz properties in time and space of the true velocity field under the aforementioned assumptions on the data. On the contrary, we do not assume any regularity on the estimator of the mimicking drift. In the context of Probability Flow ODEs, [10] provides bound in Total Variation distance, but assuming both the score and its estimator to be Lipschitz in space. So, also in the early-stopping regime, our bound improves previously obtained one.

To conclude, in the context of SGMs, [10, 11, 12] are able to cover any data distribution with bounded second moment at the cost of using exponentially decreasing step-sizes. However, [11, Corollary 2.4] and [1] improves upon [10, Theorem 2.2]: the term that takes track of the time-discretization error is linearly dependent on the dimension \(d\) in the former works, whereas quadratically dependent on \(d\) in the latter.

### The proposed methodology.

In what follows, we provide a sketch of the proofs of Theorem 2 and Theorem 3 in order to outline and delineate our methodology.

The starting point of our proof of Theorem 2 is the following (by now) standard [10, 10, 11] decomposition of the KL divergence which is derived from Girsanov theorem:

\[\mathrm{KL}(\nu^{\star}|\nu_{1}^{\theta^{\star}})\leq\mathrm{KL}(\mathbb{M}( \pi,\mathbb{B})|\mathrm{Law}(X_{[0,1]}^{\theta^{\star}}))\lesssim\sum_{k=0}^{ N-1}\int_{t_{k}}^{t_{k+1}}\mathbb{E}\Big{[}\,\Big{\|}\,\big{\|}_{\theta^{\star} }(t_{k},X_{t_{k}}^{\mathrm{M}})-\tilde{\beta}_{t}(X_{t}^{\mathrm{M}})\Big{\|} ^{2}\,\Big{]}\mathrm{d}t\]

\[\lesssim\varepsilon^{2}+\sum_{k=0}^{N-1}\int_{t_{k}}^{t_{k+1}}\mathbb{E}\Big{[} \,\Big{\|}\tilde{\beta}_{t_{k}}(X_{t_{k}}^{\mathrm{M}})-\tilde{\beta}_{t}(X_{t }^{\mathrm{M}})\Big{\|}^{2}\,\Big{]}\mathrm{d}t\,\]

where, for the first inequality, we used the data processing inequality [14, Lemma 1.6] and the last inequality follows from the triangle inequality and the assumption \(\mathbf{H}3\). In order to conclude, we should bound the \(\mathrm{L}^{2}\) norm of the adjoint process in the Pontryagin system associated with the Markovian projection of the interpolant. We do so by introducing a novel quantity in the generative model literature (see [14]), namely the so-called _reciprocal characteristic_ of the mimicking drift, _i.e._,

\[(\partial_{t}+\mathcal{L}_{t}^{\mathrm{M}})\tilde{\beta}_{t}\,\]

where \(\mathcal{L}^{\mathrm{M}}\) denotes the generator of \((X_{t}^{\mathrm{M}})_{t\in[0,1]}\). This quantity may be viewed as some sort of mean acceleration field and guides the time evolution of the mimicking drift, as

\[\mathrm{d}\tilde{\beta}_{t}(X_{t}^{\mathrm{M}})=(\partial_{t}+\mathcal{L}_{t}^ {\mathrm{M}})\tilde{\beta}_{t}(X_{t}^{\mathrm{M}})\mathrm{d}t+\sqrt{2}D_{x} \tilde{\beta}_{t}(X_{t}^{\mathrm{M}})\mathrm{d}B_{t}\,\quad t\in[0,1]\.\]

The main efforts in our proof are directed towards bounding the \(\mathrm{L}^{2}\) norm of the reciprocal characteristic whose representation in terms of either conditional moments or higher-order logarithmic derivatives of conditional densities is quite intricate, see (39). Trying to bound each of these terms separately requires strong assumptions on the initial distributions and couplings leading to sub-optimal results. However, using integration by parts both in time and space and a double change of measure argument, and profiting from symmetry properties of the heat kernel, we managed to bound these terms under assumptions comparable to the minimal ones required in the analysis of SGMs. Note that the analysis of the reciprocal characteristic is not required for SGMs (it is always 0) and that controlling it also requires new tricks and ideas, since its representation contains up to three logarithmic derivatives of conditional distributions, whereas the analysis of SGMs requires at most two such derivatives to be analyzed.

Regarding the proof of Theorem 3, we consider the interpolated process \((X_{t}^{1})_{t\in[0,\delta]}\) restricted to \([0,1-\delta]\). Denoting by \(\pi_{1-\delta}\), the coupling between \(\mu\) and \(\nu_{1-\delta}^{\star}\) corresponding to the distribution of the couple \((X_{0}^{1},X_{1-\delta}^{1})\). By the property of the Brownian bridge, \((X_{t}^{1})_{t\in[0,1-\delta]}\) is a stochastic interpolant resulting from \(\pi_{1-\delta}\) and the Brownian bridge on \([0,1-\delta]\). Therefore based on Remark 6,we only have to show \(\nu^{\star}_{1-\delta}\) and \(\pi_{1-\delta}\) satisfy \(\mathbf{H}\) and \(\mathbf{H}\), replacing \(\tilde{\pi}\) in \(\mathbf{H}\) by \(\tilde{\pi}_{1-\delta}\) defined in (15). More precisely, we show that they hold and that

\[\|\nabla\log\tilde{\pi}_{1-\delta}\|_{\mathrm{L}^{g}(\pi_{1-\delta })}^{8} \leqslant\|\nabla\log\mu\|_{\mathrm{L}^{g}(\mu)}^{8}+\mathbf{m}_{8}[ \mu]\frac{1}{(1-\delta)^{8}}+\mathbf{m}_{8}[\nu^{\star}]\frac{1}{\delta^{8}}+d ^{4}\frac{1}{\delta^{4}(1-\delta)^{4}}\] \[\big{\|}\nabla\log\nu^{\star}_{1-\delta}\|_{\mathrm{L}^{g}(\nu^{ \star}_{1-\delta})}^{8} \leqslant\mathbf{m}_{8}[\mu]\frac{1}{(1-\delta)^{8}}+\mathbf{m}_{8 }[\nu^{\star}]\frac{1}{\delta^{8}}+d^{4}\frac{1}{\delta^{4}(1-\delta)^{4}}\.\]

## 4 Conclusion

In this work, we have investigated a Diffusion Flow Matching model built around the Markovian projection of the \(d\)-dimensional Brownian bridge between the data distribution \(\nu^{\star}\) and the base distribution \(\mu\). In particular, we have derived convergence guarantees in Kullback-Leibler divergence, which take account of all the sources of error - time-discretization error and drift-estimation error - that arise when implementing the model, and which hold under mild moments conditions on \(\mu\), \(\nu^{\star}\), the scores of \(\mu\), \(\nu^{\star}\) and the score of the coupling \(\pi\) between \(\mu\) and \(\nu^{\star}\). However, there are several questions remaining open. First, it would be worthy to understand if we could lower the order of integrability of the score associated with \(\mu,\nu\) and \(\pi\). Second, it would be interesting to complement our analysis by a statistical analysis of DFM (11), similarly to what have been achieved in [1] for a particular deterministic FM model. Finally, it would be valuable to obtain better dimension dependence with respect to the space dimension \(d\) when applying early-stopping procedure.

## Acknowledgments and Disclosure of Funding

The work of Marta Gentiloni-Silveri has been supported by the Paris Ile-de-France Region in the framework of DIM AI4IDF. The work by Alain Durmus is partially funded by the European Union (ERC-2022-SYG-OCEAN-101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.

## References

* [ABVE23] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. _arXiv preprint arXiv:2303.08797_, 2023.
* [And82] Brian DO Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326, 1982.
* [AVE22] Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. _arXiv preprint arXiv:2209.15571_, 2022.
* [BB17] Paolo Baldi and Paolo Baldi. _Stochastic calculus_. Springer, 2017.
* [BDBDD23] Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Linear convergence bounds for diffusion models via stochastic localization. _arXiv preprint arXiv:2308.03686_, 2023.
* [BDD23] Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for flow matching methods. _arXiv preprint arXiv:2305.16860_, 2023.
* [BKRS15] Vladimir I. Bogachev, Nicolai V. Krylov, Michael Rockner, and Stanislav V. Shaposhnikov. Fokker-planck-kolmogorov equations. 2015.
* [Bor22a] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. _Transactions on Machine Learning Research_, 2022. Expert Certification.
* [Bor22b] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. _Transactions on Machine Learning Research_, 2022. Expert Certification.

- 1628, 2013.
* [CCL\({}^{+}\)23a] Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ODE is provably fast. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [CCL\({}^{+}\)23b] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R. Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. _International Conference on Learning Representations_, 2023.
* [CDS23] Giovanni Conforti, Alain Durmus, and Marta Gentiloni Silveri. Score diffusion models without early stopping: finite fisher information is all you need. _arXiv preprint arXiv:2308.12240_, 2023.
* [CLL23] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In _International Conference on Machine Learning_, pages 4735-4763. PMLR, 2023.
* [CLT22] Tianrong Chen, Guan-Horng Liu, and Evangelos Theodorou. Likelihood training of schrodinger bridge using forward-backward SDEs theory. In _International Conference on Learning Representations_, 2022.
* [DBMP19] Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. _Advances in neural information processing systems_, 32, 2019.
* [DBTHD21] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. _Advances in Neural Information Processing Systems_, 34:17695-17709, 2021.
* [FJNO20] Chris Finlay, Jorn-Henrik Jacobsen, Levon Nurbekyan, and Adam Oberman. How to train your neural ode: the world of jacobian and kinetic regularization. In _International conference on machine learning_, pages 3154-3164. PMLR, 2020.
* [GCBD19] Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, and David Duvenaud. Scalable reversible generative models with free-form continuous dynamics. In _International Conference on Learning Representations_, page 7, 2019.
* [GHJZ24] Yuan Gao, Jian Huang, Yuling Jiao, and Shurong Zheng. Convergence of continuous normalizing flows for learning probability distributions. _arXiv preprint arXiv:2404.00551_, 2024.
* [Gyo86] Istvan Gyongy. Mimicking the one-dimensional marginal distributions of processes having an ito differential. _Probability theory and related fields_, 71(4):501-516, 1986.
* [GZ24] Xuefeng Gao and Lingjiong Zhu. Convergence analysis for general probability flow odes of diffusion models in wasserstein distances. _arXiv preprint arXiv:2401.17958_, 2024.
* [HD05] Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4), 2005.
* [HJA20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.
* [Hyv05] Aapo Hyvarinen. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(24):695-709, 2005.
* [Kle13] Achim Klenke. _Probability theory: a comprehensive course_. Springer Science & Business Media, 2013.
* [Kre97] Arthur J Krener. Reciprocal diffusions in flat space. _Probability theory and related fields_, 107(2):243-281, 1997.

* [Kry] N. V. Krylov. On the relation between differential operators of second order and the solutions of stochastic differential equations. _Steklov Seminar_.
* [LCBH\({}^{+}\)23] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In _The Eleventh International Conference on Learning Representations_, 2023.
* [LGL23] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. _International Conference on Learning Representations (ICLR)_, 2023.
* [Liu22] Qiang Liu. Rectified flow: A marginal preserving approach to optimal transport. _arXiv preprint arXiv:2209.14577_, 2022.
* [LLT23] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In _International Conference on Algorithmic Learning Theory_, pages 946-985. PMLR, 2023.
* [LWCC24] Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards non-asymptotic convergence for diffusion-based generative models. In _The Twelfth International Conference on Learning Representations_, 2024.
* [LWYql23] Xingchao Liu, Lemeng Wu, Mao Ye, and qiang liu. Learning diffusion bridges on constrained domains. In _The Eleventh International Conference on Learning Representations_, 2023.
* [LYB\({}^{+}\)23] Sungbin Lim, EUN BI YOON, Taehyun Byun, Taewon Kang, Seungwoo Kim, Kyungjae Lee, and Sungjoon Choi. Score-based generative modeling through stochastic evolution equations in hilbert spaces. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 37799-37812. Curran Associates, Inc., 2023.
* [Nut21] Marcel Nutz. Introduction to entropic optimal transport. _Lecture notes, Columbia University_, 2021.
* [OFLR21] Derek Onken, Samy Wu Fung, Xingjian Li, and Lars Ruthotto. Ot-flow: Fast and accurate continuous normalizing flows via optimal transport. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 9223-9232, 2021.
* [Pel22] Stefano Peluchetti. Non-denoising forward-time diffusions, 2022.
* [PMM23] Francesco Pedrotti, Jan Maas, and Marco Mondelli. Improved convergence of score-based diffusion models via prediction-correction. _arXiv preprint arXiv:2305.14164_, 2023.
* [PVG\({}^{+}\)21] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts: A diffusion probabilistic model for text-to-speech. In _International Conference on Machine Learning_, pages 8599-8608. PMLR, 2021.
* [RBL\({}^{+}\)22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* [RDN\({}^{+}\)22] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.
* [RY13] Daniel Revuz and Marc Yor. _Continuous martingales and Brownian motion_, volume 293. Springer Science & Business Media, 2013.
* [SBCD23] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schrodinger bridge matching. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.

* [SDWMG15a] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 2256-2265, Lille, France, 07-09 Jul 2015. PMLR.
* [SDWMG15b] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015.
* [SE19] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.
* [SE20] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution, 2020.
* [VEH14] Tim Van Erven and Peter Harremos. Renyi divergence and kullback-leibler divergence. _IEEE Transactions on Information Theory_, 60(7):3797-3820, 2014.
* [Vin11] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural Computation_, 23:1661-1674, 2011.

Postponed proofs

### The Markovian Projection

First, we introduce the set of assumptions under which Theorem 1 holds true.

**H4**.: _For \(t>s\), \((s,t,x,y)\mapsto p^{Y}_{t|s}(y|x)\) is continuously differentiable in the \(t\) and \(s\) variables and twice continuously differentiable in the \(x\) and \(y\) variables. Furthermore, \(\partial_{t}p^{Y}_{t|s}(y|x)\), \(\partial_{s}p^{Y}_{t|s}(y|x)\), \(\nabla_{x}p^{Y}_{t|s}(y|x)\), \(\nabla_{y}p^{Y}_{t|s}(y|x)\), \(\nabla_{x}^{2}p^{Y}_{t|s}(y|x)\), \(\nabla_{y}^{2}p^{Y}_{t|s}(y|x)\) are bounded._

**H4** ensures that \((s,t,x,y)\mapsto p^{Y}_{t|s}(y|x)\) is enough regular to allow a series of algebraic manipulations.

**H5**.: \(\tilde{\beta}^{Y}\) _is a locally bounded Borel vector field on \(\mathbb{R}^{d}\times(0,1)\) such that, for at least one probability solution \(\mu_{t}\) to the Fokker-Planck equation_

\[\partial_{t}\mu_{t}+\mathrm{div}(\tilde{\beta}^{Y}_{t}\mu_{t})-\Delta\mu_{t}=0 \;,\quad t\in(0,1)\;,\quad\mu_{0}=\mu\;, \tag{16}\]

_it holds \(\int\|\tilde{\beta}^{Y}_{t}(x)\|\mu_{t}(\mathrm{d}x)\mathrm{d}t<+\infty\)._

**H5** provides uniqueness of the solution to the Fokker Planck equation with drift field \(\tilde{\beta}^{Y}\), see [3, Theorem 9.4.3].

We are now ready to rigorously state and prove Theorem 1:

**Theorem 4**.: _Consider a \(\pi\in\Pi(\mu,\nu^{\star})\), \(\mathbb{Q}^{\beta}\) associated with (3) and the drift field defined in (8). Under **H4** and 5, the Markov process \((X^{\mathrm{M}}_{t})_{t\in[0,1]}\) solution of (9) is such that, for any \(t\in[0,1)\), \(X^{\mathrm{1}}_{t}\stackrel{{\text{dist}}}{{=}}X^{\mathrm{M}}_{t}\)._

Proof of Theorem 4.: We start by reminding that \((s,t,x,y)\mapsto p^{Y}_{t|s}(y|x)\) satisfies for any \(x,y\in\mathbb{R}^{d}\) and \(s,t\in[0,1]\) with \(s<t\) both the Fokker-Planck equation

\[\partial_{t}p^{Y}_{t|s}(y|x)+\mathrm{div}_{y}(p^{Y}_{t|s}(y|x)\beta(x))-\Delta_ {y}p^{Y}_{t|s}(y|x)=0\;,\]

and the Kolmogorov backward equation

\[\partial_{s}p^{Y}_{t|s}(y|x)+\langle\beta(x),\nabla_{x}p^{Y}_{t|s}(y|x)\rangle +\Delta_{x}p^{Y}_{t|s}(y|x)=0\;.\]

If we exploit these well-known results, (4) and **H4**, we get that for \(t\in(0,1)\)

\[\partial_{t}p^{\mathrm{I}}_{t}(x) =\partial_{t}\Big{(}\int_{\mathbb{R}^{2d}}p^{Y}_{t|0}(x|x_{0})p^{ Y}_{1|t}(x_{1}|x)\tilde{\pi}(\mathrm{d}x_{0},\mathrm{d}x_{1})\Big{)}\] \[=\int_{\mathbb{R}^{2d}}\Big{(}\Delta_{x}p^{Y}_{t|0}(x|x_{0})p^{Y} _{1|t}(x_{1}|x)-p^{Y}_{t|0}(x|x_{0})\Delta_{x}p^{Y}_{1|t}(x_{1}|x)\Big{)}\tilde {\pi}(\mathrm{d}x_{0},\mathrm{d}x_{1})\] \[\quad-\int_{\mathbb{R}^{2d}}\mathrm{div}_{x}(p^{Y}_{t|0}(x|x_{0}) \beta(x))p^{Y}_{1|t}(x_{1}|x)\tilde{\pi}(\mathrm{d}x_{0},\mathrm{d}x_{1})\] \[\quad-\int_{\mathbb{R}^{2d}}\langle\beta(x),\nabla_{x}p^{Y}_{1|t} (x_{1}|x)\rangle p^{Y}_{t|0}(x|x_{0})\tilde{\pi}(\mathrm{d}x_{0},\mathrm{d}x_{1})\] \[=\int_{\mathbb{R}^{2d}}\Big{(}\Delta_{x}p^{Y}_{t|0}(x|x_{0})p^{Y}_{ 1|t}(x_{1}|x)+\nabla_{x}p^{Y}_{t|0}(x|x_{0})\nabla_{x}p^{Y}_{1|t}(x_{1}|x) \Big{)}\tilde{\pi}(\mathrm{d}x_{0},\mathrm{d}x_{1})\] \[\quad-\int_{\mathbb{R}^{2d}}\Big{(}\nabla_{x}p^{Y}_{t|0}(x|x_{0}) \nabla_{x}p^{Y}_{1|t}(x_{1}|x)+p^{Y}_{t|0}(x|x_{0})\Delta_{x}p^{Y}_{1|t}(x_{1} |x)\Big{)}\tilde{\pi}(\mathrm{d}x_{0},\mathrm{d}x_{1})\] \[\quad-\langle\beta(x),\nabla_{x}p^{\mathrm{I}}_{t}(x)\rangle- \mathrm{div}_{x}\,\beta(x)p^{\mathrm{I}}_{t}(x)\] \[=\mathrm{div}_{x}\,\Big{(}\int_{\mathbb{R}^{2d}}\{\nabla_{x}p^{Y} _{t|0}(x|x_{0})p^{Y}_{1|t}(x_{1}|x)-p^{Y}_{t|0}(x|x_{0})\nabla_{x}p^{Y}_{1|t}( x_{1}|x)\}\tilde{\pi}(\mathrm{d}x_{0},\mathrm{d}x_{1})\Big{)}\] \[\quad-\mathrm{div}_{x}(\beta(x)p^{\mathrm{I}}_{t}(x))\] \[=\mathrm{div}_{x}\,\Big{(}-\beta(x)p^{\mathrm{I}}_{t}(x)\] \[\quad+\int_{\mathbb{R}^{2d}}\{\nabla_{x}\log p^{Y}_{t|0}(x|x_{0}) -\nabla_{x}\log p^{Y}_{1|t}(x_{1}|x)\}p^{Y}_{t|0}(x|x_{0})p^{Y}_{1|t}(x_{1}|x) \tilde{\pi}(\mathrm{d}x_{0},\mathrm{d}x_{1})\Big{)}\;.\]Therefore, if we set

\[v_{t}^{Y}(x)=\frac{\int_{\mathbb{R}^{2d}}\{\nabla_{x}\log p_{1|t}^{Y}(x_{1}|x)- \nabla_{x}\log p_{t|0}^{Y}(x|x_{0})\}p_{t|0}^{Y}(x|x_{0})p_{1|t}^{Y}(x_{1}|x) \tilde{\pi}(\mathrm{d}x_{0},\mathrm{d}x_{1})}{p_{t}^{\mathrm{I}}(x)}+\beta(x)\;,\]

we have proven that \(p_{t}^{\mathrm{I}}(x)\) satisfies the following continuity equation

\[\partial_{t}p_{t}^{\mathrm{I}}(x)+\mathrm{div}_{x}(v_{t}^{Y}(x)p_{t}^{ \mathrm{I}}(x))=0\;,\quad t\in(0,1)\;,\;x\in\mathbb{R}^{d}\;.\]

Consequently, if we define

\[b_{t}^{Y}(x)=v_{t}^{Y}(x)+\nabla_{x}\log p_{t}^{\mathrm{I}}(x)\;,\]

we have that, under **H4**, \(p_{t}^{\mathrm{I}}(x)\) satisfies the following Fokker-Planck equation

\[\partial_{t}p_{t}^{\mathrm{I}}(x)+\mathrm{div}_{x}(b_{t}^{Y}(x)p_{t}^{ \mathrm{I}}(x))-\Delta_{x}p_{t}^{\mathrm{I}}(x)=0\;,\quad t\in(0,1)\;,\;x\in \mathbb{R}^{d}\;. \tag{17}\]

So, \(b_{t}(x)\) is a mimicking drift. It remains to show that \(b^{Y}\equiv\tilde{\beta}^{Y}\): the thesis will then follows from the uniqueness of the solution to the Fokker Planck equation (17) under **H5**, see [2, Theorem 9.4.3]. To this aim, note that

\[\nabla_{x}\log p_{t}^{\mathrm{I}}(x) =\frac{\int_{\mathbb{R}^{2d}}\{\nabla_{x}p_{t|0}^{Y}(x|x_{0})p_{1| t}^{Y}(x_{1}|x)+p_{t|0}^{Y}(x|x_{0})\nabla_{x}p_{1|t}^{Y}(x_{1}|x)\} \tilde{\pi}(\mathrm{d}x_{0},\mathrm{d}x_{1})}{p_{t}^{\mathrm{I}}(x)}\] \[=\frac{\int_{\mathbb{R}^{2d}}\{\nabla_{x}\log p_{t|0}^{Y}(x|x_{0})+ \nabla_{x}\log p_{1|t}^{Y}(x_{1}|x)\}p_{t|0}^{Y}(x|x_{0})p_{1|t}^{Y}(x_{1}|x) \tilde{\pi}(\mathrm{d}x_{0},\mathrm{d}x_{1})}{p_{t}^{\mathrm{I}}(x)}\;.\]

Therefore, for any \(t\in[0,1)\) and \(x\in\mathbb{R}^{d}\), we have that

\[b_{t}^{Y}(x)=2\frac{\int_{\mathbb{R}^{2d}}\nabla_{x}\log p_{1|t}^{Y}(x_{1}|x)p _{t|0}^{Y}(x|x_{0})p_{1|t}^{Y}(x_{1}|x)\tilde{\pi}(\mathrm{d}x_{0},\mathrm{d}x _{1})}{p_{t}^{\mathrm{I}}(x)}+\beta(x)=\tilde{\beta}_{t}^{Y}(x)\;.\]

### Preliminary results

We begin this section with two remarks aiming at highlighting some of the properties of the heat kernels (12) and some of their important consequences.

_Remark 8_.: It is well-known that \((s,x,y)\mapsto p_{s}(y|x)\) defined in (12) is twice continuously differentiable in the space variables \(x\) and \(y\) and satisfies for \(x,y\in\mathbb{R}^{d},s\in(0,1]\),

\[\nabla_{x}p_{s}(y|x)=-\frac{x-y}{2s}p_{s}(y|x)=-\nabla_{y}p_{s}(y|x)\;, \tag{18}\]

\[\nabla_{x}^{2}p_{s}(y|x)=-\frac{1}{2s}p_{s}(y|x)\operatorname{Id}+\frac{(x-y)( x-y)^{\mathrm{T}}}{4s^{2}}p_{s}(y|x)=\nabla_{y}^{2}p_{s}(y|x)\;, \tag{19}\]

\[\Delta_{x}p_{s}(y|x)=-\frac{d}{2s}p_{s}(y|x)+\left\|\frac{x-y}{2s}\right\|^{2 }p_{s}(y|x)=\Delta_{y}p_{s}(y|x)\;. \tag{20}\]

Moreover (12) satisfies the heat equation, _i.e._,

\[\partial_{s}p_{s}(y|x)=\Delta_{x}p_{s}(y|x)\;,\quad s\in(0,1]\;,\;x,y\in \mathbb{R}^{d}\;. \tag{21}\]

Thus, in particular, \((s,x,y)\mapsto p_{s}(y|x)\) is continuously differentiable in the time variable \(s\).

_Remark 9_.: In the case \(\beta\equiv 0\), _i.e._, \(\mathbb{Q}^{\beta}=\mathbb{B}\), under **H1**, the conditions of Theorem 1 hold. Indeed, as highlighted in Remark 8, \((s,x,y)\mapsto p_{s}(y|x)\) defined in (12) is continuously differentiable in the time variable \(s\) and twice continuously differentiable in the space variables \(x\) and \(y\). Also, using Equation (18), Equation (19) and (12), it's straightforward to verify that \(\nabla_{x}p_{t|s}^{Y}(y|x)\), \(\nabla_{y}p_{t|s}^{Y}(y|x)\), \(\nabla_{x}^{2}p_{t|s}^{Y}(y|x)\), \(\nabla_{y}^{2}p_{t|s}^{Y}(y|x)\) are bounded. Additionally, using (20) and (21), it's easy to argue that also \(\partial_{t}p_{t|s}^{Y}(y|x)\) and \(\partial_{s}p_{t|s}^{Y}(y|x)\) are bounded. So **H4** is verified and \(p_{t}^{\mathrm{I}}\) solves the Fokker-Planck equation (16). Moreover \(\tilde{\beta}\) is clearly locally bounded on \(\mathbb{R}^{d}\times(0,1)\) and, as a consequence of (18), (5), and Jensen inequality, it satisfies uniformly in time

\[\int_{\mathbb{R}^{d}}\left\|\tilde{\beta}_{t}(x)\right\|^{2}p_{t}^{ \mathrm{I}}(x)\mathrm{d}x =\int_{\mathbb{R}^{d}}\left\|\frac{1}{p_{t}^{\mathrm{I}}(x)}\int_{ \mathbb{R}^{2d}}\frac{x_{1}-x}{1-t}p_{t}(x|x_{0})p_{1-t}(x_{1}|x)\tilde{\pi}( \mathrm{d}x_{0},\mathrm{d}x_{1})\right\|^{2}p_{t}^{\mathrm{I}}(x)\mathrm{d}x\] \[=\mathbb{E}\Bigg{[}\left\|\mathbb{E}\Bigg{[}X_{1}^{\mathrm{I}}-X_ {0}^{\mathrm{I}}-\sqrt{2t}\mathrm{Z}\Big{\|}^{2}\,\right]\] \[\lesssim\mathbb{E}\Bigg{[}\left\|X_{1}^{\mathrm{I}}-X_{0}^{ \mathrm{I}}-\sqrt{2t}\mathrm{Z}\Big{\|}^{2}\,\right]\] \[\lesssim\mathbf{m}_{2}[\mu]+\mathbf{m}_{2}[\nu^{\star}]+d\;,\]

which is finite under \(\mathbf{H}1\). It follows that \(\int\|\tilde{\beta}_{t}^{\mathrm{Y}}(x)\|p_{t}^{\mathrm{I}}(x)\mathrm{d}x \mathrm{d}t<+\infty\). So, \(\mathbf{H}5\) is verified.

Additionally, hereunder, we state three lemmas that will be crucial in the derivation of the bounds provided in Theorems 2 and 3.

**Lemma 1**.: _For any \(p\geq 1\), they hold_

\[\mathbb{E}[\big{\|}X_{s}^{\mathrm{I}}-X_{0}^{\mathrm{I}}\big{\|}^{2p}]\lesssim s ^{2p}\mathbf{m}_{2p}[\mu]+s^{2p}\mathbf{m}_{2p}[\nu^{\star}]+d^{p}s^{p}(1-s)^{ p}\;,\]

_and_

\[\mathbb{E}[\big{\|}X_{1}^{\mathrm{I}}-X_{s}^{\mathrm{I}}\big{\|}^{2p}]\mathrm{ d}s\lesssim(1-s)^{2p}\mathbf{m}_{2p}[\mu]+(1-s)^{2p}\mathbf{m}_{2p}[\nu^{\star}]+d^{ p}s^{p}(1-s)^{p}\;.\]

Proof of Lemma 1.: As a direct consequence of (5) and Young inequality, it holds

\[\mathbb{E}[\big{\|}X_{s}^{\mathrm{I}}-X_{0}^{\mathrm{I}}\big{\|}^ {2p}] =\mathbb{E}\Big{[}\left\|s(X_{1}^{\mathrm{I}}-X_{0}^{\mathrm{I}})+ \sqrt{2s(1-s)}\mathrm{Z}\right\|^{2p}\Big{]}\] \[\lesssim s^{2p}\mathbb{E}[\big{\|}X_{0}^{\mathrm{I}}-X_{1}^{ \mathrm{I}}\big{\|}^{2p}]+(2s(1-s))^{p}\mathbb{E}[\|\mathrm{Z}\|^{2p}]\] \[\lesssim s^{2p}\mathbf{m}_{2p}[\mu]+s^{2p}\mathbf{m}_{2p}[\nu^{ \star}]+d^{p}s^{p}(1-s)^{p}\;.\]

A similar argument holds for \(\mathbb{E}[\big{\|}X_{1}^{\mathrm{I}}-X_{s}^{\mathrm{I}}\big{\|}^{2p}]\). 

We preface the next lemma with a definition.

**Definition 1**.: Consider \(\mathbb{Q}\in\mathcal{P}(\mathbb{W})\). The reverse time measure \(\mathbb{Q}^{\mathrm{R}}\in\mathcal{P}(\mathbb{W})\) of \(\mathbb{Q}\) is defined as follows: for any \(\mathsf{A}\in\mathcal{B}(\mathbb{W})\)

\[\mathbb{Q}^{\mathrm{R}}(\mathsf{A})=\mathbb{Q}(\mathsf{A}^{\mathrm{R}}),\;,\]

where \(\mathsf{A}^{\mathrm{R}}:=\{t\mapsto\omega(1-t)\;:\;\omega\in\mathsf{A}\}\).

**Lemma 2**.: _Assume \(\pi\ll\mathrm{Leb}^{2d}\) and \(\mu,\nu^{\star}\ll\mathrm{Leb}^{d}\). Then \((X_{t}^{\mathrm{I}})_{t\in[0,1]}\) solves weakly_

\[\mathrm{d}\overrightarrow{X}_{t}=2\overrightarrow{b}_{t}(\overrightarrow{X}_ {0},\overrightarrow{X}_{t})\mathrm{d}t+\sqrt{2}\mathrm{d}\overrightarrow{B}_ {t}\;,\quad t\in[0,1]\;,\quad\overrightarrow{X}_{0}\sim\mu\;. \tag{22}\]

_with \((\overrightarrow{B}_{t})_{t\in[0,1]}\)\(d\)-dimensional Brownian motion independent of \(\overrightarrow{X}_{0}\) and_

\[\overrightarrow{b}_{t}(x_{0},x)=\nabla_{x}\psi_{t}^{x_{0}}(x)\;,\]

_where \(\psi_{t}^{x_{0}}(x)\) solves_

\[\partial_{t}\psi_{t}^{x_{0}}+\Delta_{x}\psi_{t}^{x_{0}}+\|\nabla_{x}\psi_{t}^{ x_{0}}\|^{2}=0\;,\quad t\in[0,1)\;,\quad\psi_{1}^{x_{0}}=\log\tilde{\pi}_{0}^{x_{0}}\;, \tag{23}\]

_with_

\[\tilde{\pi}_{0}^{x_{0}}(x):=\;\frac{\mathrm{d}\pi(x_{0},x)}{\mathrm{d}\mathrm{ Leb}^{2d}}\frac{1}{p_{1}(x|x_{0})}\bigg{/}\frac{\mathrm{d}\mu(x_{0})}{\mathrm{d} \mathrm{Leb}^{d}}\;, \tag{24}\]_and \(p_{1}(x|x_{0})\) defined in (12). Similarly, \(((X^{1}_{t})_{t\in[0,1]})^{\mathrm{R}}\) solves weakly_

\[\mathrm{d}\overset{\leftarrow}{X}_{t}=2\overset{\leftarrow}{b}_{t}(\overset{ \leftarrow}{X}_{0},\overset{\leftarrow}{X}_{t})\mathrm{d}t+\sqrt{2}\mathrm{d} \overset{\leftarrow}{B}_{t}\;,\quad t\in[0,1]\;,\quad\overset{\leftarrow}{X}_ {0}\sim\nu^{\star}\;.\]

_with \((\overset{\leftarrow}{B}_{t})_{t\in[0,1]}\)\(d\)-dimensional Brownian motion independent of \(\overset{\leftarrow}{X}_{0}\) and_

\[\overset{\leftarrow}{b}_{t}(x_{1},x)=\nabla_{x}\varphi_{1-t}^{x_{1}}(x)\;,\]

_where \(\varphi_{t}^{x_{1}}(x)\) solves_

\[-\partial_{t}\varphi_{t}^{x_{1}}+\Delta_{x}\varphi_{t}^{x_{1}}+\left\|\nabla_{ x}\varphi_{t}^{x_{1}}\right\|^{2}=0\;,\quad t\in[0,1)\;,\quad\varphi_{0}^{x_{1 }}=\log\tilde{\pi}_{1}^{x_{1}}\;,\]

_with_

\[\tilde{\pi}_{1}^{x_{1}}(x):=\frac{\mathrm{d}\pi(x,x_{1})}{\mathrm{d}\mathrm{ Leb}^{2d}}\frac{1}{p_{1}(x_{1}|x)}\bigg{/}\frac{\mathrm{d}\nu^{\star}(x_{1})}{ \mathrm{d}\mathrm{Leb}^{d}}\;\;, \tag{25}\]

_and \(p_{1}(x_{1}|x)\) defined in (12)._

Proof of Lemma 2:.: We just show that \((X^{1}_{t})_{t\in[0,1]}\) solves weakly (22). The argument for \(((X^{1}_{t})_{t\in[0,1]})^{\mathrm{R}}\) is similar and therefore omitted.

For a fixed \(x_{0}\in\mathbb{R}^{d}\), we denote by \((x_{0},\mathsf{A})\mapsto\mathbb{I}_{\pi,\mathbb{B}}(x_{0},\mathsf{A})\) the conditional distribution of \((X^{1}_{t})_{t\in[0,1]}\) given \(X^{1}_{0}\) (see e.g. [13, Theorem 8.37] for the existence of this conditional distribution) and by \((B^{x_{0}}_{t})_{t\in[0,1]}\) the solution to

\[\mathrm{d}B^{x_{0}}_{t}=\sqrt{2}\mathrm{d}B_{t}\;,\quad t\in[0,1],\quad B^{x_{ 0}}_{0}=x_{0}\;.\]

Also, we denote by \((W_{t})_{t\in[0,1]}\) the canonical process on the Wiener space \(\mathbb{W}\) and by \((\mathcal{F}_{t})_{t\in[0,1]}\) the corresponding natural filtration. Note that, as a consequence of the very definition of \((X^{1}_{t})_{t\in[0,1]}\), (23) and Ito's formula applied to \((\psi_{t}^{x_{0}}(B^{x_{0}}_{t}))_{t\in[0,1]}\), it holds

\[\frac{\mathrm{d}\mathbb{I}_{\pi,\mathbb{B}}(x_{0},\cdot)}{\mathrm{ d}\mathrm{Law}((B^{x_{0}}_{t})_{t\in[0,1]})}((B^{x_{0}}_{t})_{t\in[0,1]})\] \[=\tilde{\pi}_{0}^{x_{0}}(B^{x_{0}}_{1})\] \[=\exp\Big{(}\psi_{1}^{x_{0}}(B^{x_{0}}_{1})\Big{)}\] \[=\exp\Big{(}\psi_{1}^{x_{0}}(B^{x_{0}}_{1})-\psi_{0}^{x_{0}}(x_{0 })\Big{)}\] \[=\exp\Big{(}\psi_{1}^{x_{0}}(B^{x_{0}}_{1})-\psi_{0}^{x_{0}}(B^{x _{0}}_{0})-\int_{0}^{1}\Big{\{}\partial_{t}\psi_{t}^{x_{0}}+\Delta_{x}\psi_{t} ^{x_{0}}+\left\|\nabla_{x}\psi_{t}^{x_{0}}\right\|^{2}\Big{\}}(B^{x_{0}}_{t}) \mathrm{d}t\Big{)}\] \[=\exp\Big{(}\int_{0}^{1}\nabla_{x}\psi_{t}^{x_{0}}(B^{x_{0}}_{t}) \mathrm{d}B^{x_{0}}_{t}-\int_{0}^{1}\left\|\nabla_{x}\psi_{t}^{x_{0}}(B^{x_{0 }}_{t})\right\|^{2}\mathrm{d}t\Big{)}\;.\]

Hence, for any \(t\in[0,1]\) we have that

\[\frac{\mathrm{d}\mathbb{I}_{\pi,\mathbb{B}}(x_{0},\cdot)}{\mathrm{d}\mathrm{ Law}((B^{x_{0}}_{t})_{t\in[0,1]})}\bigg{|}_{\mathcal{F}_{t}}=D_{t}\;,\]

where

\[D_{t}=\exp\Big{(}\int_{0}^{t}\nabla_{x}\psi_{t}^{x_{0}}(W_{s})\mathrm{d}W_{s} -\int_{0}^{1}\left\|\nabla_{x}\psi_{t}^{x_{0}}(W_{s})\right\|^{2}\mathrm{d}s \Big{)}\]

is continuous \(\mathrm{Law}((B^{x_{0}}_{t})_{t\in[0,1]})\)- almost surely and is such that

\[\mathrm{d}D_{t}=D_{t}\nabla_{x}\psi_{s}^{x_{0}}(W_{s})\mathrm{d}W_{t}\;.\]

Therefore, being \((W_{t})_{t\in[0,1]}\) a martingale under \(\mathrm{Law}((B^{x_{0}}_{t})_{t\in[0,1]})\), as a consequence of Girsanov theorem (see [13, Theorem 1.4]), we have that

\[\big{(}W_{t}-\big{(}D^{-1}\langle W,D\rangle\big{)}_{t}\big{)}_{t\in[0,1]}= \bigg{(}W_{t}-2\int_{0}^{t}\nabla_{x}\psi_{s}^{x_{0}}(W_{s})\mathrm{d}s\bigg{)}_ {t\in[0,1]}\]is a \(((\mathcal{F}_{t})_{t},\mathbb{I}_{\pi,\mathbb{B}}(x_{0},\cdot))\)- martingale with bracket \(\sqrt{2}t\). It follows that, under \(\mathbb{I}_{\pi,\mathbb{B}}(x_{0},\cdot)\), \((W_{t})_{t\in[0,1]}\) solves

\[\mathrm{d}W_{t}=2\nabla_{x}\psi_{t}^{x_{0}}(W_{s})\mathrm{d}t+\sqrt{2}\mathrm{d }B_{t}\,\quad t\in[0,1]\,\quad W_{0}=x_{0}\.\]

The thesis is now a direct consequence of the fact that

\[\mathbb{I}(\pi,\mathbb{B})(\mathsf{A})=\int_{\mathbb{R}^{d}}\mathbb{P}(X^{ \mathrm{I}}\in\mathsf{A}|X^{\mathrm{I}}_{0}=x_{0})\mu(\mathrm{d}x_{0})=\int_{ \mathbb{R}^{d}}\mathbb{I}_{\pi,\mathbb{B}}(x_{0},\mathsf{A})\mu(\mathrm{d}x_{0 })\,\]

for any measurable \(\mathsf{A}\in\mathbb{W}\). 

**Lemma 3**.: _Assume **H**2. Then, almost surely, it holds_

\[\overrightarrow{b}_{t}(X^{\mathrm{I}}_{0},X^{\mathrm{I}}_{t})=\mathbb{E} \left[\frac{\nabla\bar{\pi}_{0}^{x^{\mathrm{I}}_{0}}(X^{\mathrm{I}}_{1})}{\bar {\pi}_{0}^{x^{\mathrm{I}}_{0}}(X^{\mathrm{I}}_{1})}\bigg{|}(X^{\mathrm{I}}_{0},X^{\mathrm{I}}_{t})\right]\, \tag{26}\]

_where \(\bar{\pi}_{0}^{x_{0}}\) is defined as in (24). Moreover, for any \(u\in[0,1-s]\), \(s\in[0,1]\) and \(p\in\{2,4,8\}\) it holds_

\[\mathbb{E}\bigg{[}\left\|\int_{u}^{u+s}\overrightarrow{b}_{r}(\overrightarrow{ X}_{0},\overrightarrow{X}_{r})\mathrm{d}r\right\|^{p}\bigg{]}\lesssim s^{p} \Big{(}\left\|\nabla\log\bar{\pi}\right\|_{\mathrm{L}^{p}(\pi)}^{p}+\left\| \nabla\log\mu\right\|_{\mathrm{L}^{p}(\mu)}^{p}\Big{)}. \tag{27}\]

_Similarly, almost surely it holds_

\[\overleftarrow{b}_{t}(X^{\mathrm{I}}_{0},X^{\mathrm{I}}_{t})=\mathbb{E}\left[ \frac{\nabla\bar{\pi}_{1}^{x^{\mathrm{I}}_{1}}(X^{\mathrm{I}}_{0})}{\bar{\pi} _{1}^{x^{\mathrm{I}}_{1}}(X^{\mathrm{I}}_{0})}\bigg{|}(X^{\mathrm{I}}_{0},X^{ \mathrm{I}}_{t})\right]\, \tag{28}\]

_where \(\bar{\pi}_{1}^{x_{1}}\) is defined as in (25). Moreover, for any \(u\in[0,1-s]\), \(s\in[0,1]\) and \(p\in\{2,4,8\}\) it holds_

\[\mathbb{E}\bigg{[}\left\|\int_{u}^{u+s}\overleftarrow{b}_{r}( \overrightarrow{X}_{0},\overleftarrow{X}_{r})\mathrm{d}r\right\|^{p}\bigg{]} \lesssim s^{p}\Big{(}\left\|\nabla\log\bar{\pi}\right\|_{\mathrm{L}^{p}(\pi)} ^{p}+\left\|\nabla\log\nu^{*}\right\|_{\mathrm{L}^{p}(\nu^{*})}^{p}\Big{)}. \tag{29}\]

Proof of Lemma 3.: We just show (26) and (27). The proof of (28) and (29) is similar and therefore omitted. First of all, note that (26) is trivial for \(t=1\). We therefore focus on \(t\in[0,1)\). It's well known that the solution \(\psi_{t}^{x_{0}}(x)\) to the Hamilton-Jacobi-Bellman equation (23) is given by

\[\psi_{t}^{x_{0}}(x)=\log\Big{(}\int\tilde{\pi}_{0}^{x_{0}}(x_{1})p_{1-t}(x_{1} |x)\mathrm{d}x_{1}\Big{)}\,\quad t\in[0,1)\.\]

Therefore, we have that for \(t\in[0,1)\)

\[\overrightarrow{b}_{t}(x_{0},x)=\nabla_{x}\psi_{t}^{x_{0}}(x)=\frac{\int\tilde {\pi}_{0}^{x_{0}}(x_{1})\nabla_{x}p_{1-t}(x_{1}|x)\mathrm{d}x_{1}}{\int\bar{ \pi}_{0}^{x_{0}}(\tilde{x}_{1})p_{1-t}(\tilde{x}_{1}|x)\mathrm{d}\tilde{x}_{1}}\.\]

Using (18) and integrating by parts, we get for \(t\in[0,1)\) that

\[\overrightarrow{b}_{t}(x_{0},x)=-\frac{\int\tilde{\pi}_{0}^{x_{0}}(x_{1}) \nabla_{x_{1}}p_{1-t}(x_{1}|x)\mathrm{d}x_{1}}{\int\tilde{\pi}_{0}^{x_{0}}( \tilde{x}_{1})p_{1-t}(\tilde{x}_{1}|x)\mathrm{d}\tilde{x}_{1}}=\frac{\int( \nabla\tilde{\pi}_{0}^{x_{0}}(x_{1})/\tilde{\pi}_{0}^{x_{0}}(x_{1}))\tilde{\pi }_{0}^{x_{0}}(x_{1})p_{1-t}(x_{1}|x)\mathrm{d}x_{1}}{\int\tilde{\pi}_{0}^{x_{0} }(\tilde{x}_{1})p_{1-t}(\tilde{x}_{1}|x)\mathrm{d}\tilde{x}_{1}}\.\]

But then, it suffices to prove that for \(t\in[0,1)\) it holds

\[\frac{\tilde{\pi}_{0}^{x_{0}}(x_{1})p_{1-t}(x_{1}|x)}{\int\tilde{\pi}_{0}^{x_{0 }}(\tilde{x}_{1})p_{1-t}(\tilde{x}_{1}|x)\mathrm{d}\tilde{x}_{1}}=p_{1|0,t}^{ \mathrm{I}}(x_{1}|x_{0},x)\,\]

to conclude. To do so, we simply use (4).

\[p_{1|0,t}^{\mathrm{I}}(x_{1}|x_{0},x) =\frac{p_{1,1t}^{\mathrm{I}}(x_{0},x_{1},x)}{\int p_{1,t}^{\mathrm{ I}}(x_{0},\tilde{x}_{1},x)\mathrm{d}\tilde{x}_{1}}\] \[=\frac{\pi(x_{0},x_{1})p_{t}(x|x_{0})p_{1-t}(x_{1}|x)}{p_{1}(x_{1} |x_{0})}\bigg{/}\int\frac{\pi(x_{0},\tilde{x}_{1})p_{t}(x|x_{0})p_{1-t}(\tilde{x }_{1}|x)}{p_{1}(\tilde{x}_{1}|x_{0})}\mathrm{d}\tilde{x}_{1}\] \[=\frac{\pi(x_{0},x_{1})p_{1-t}(x_{1}|x)}{p_{1}(x_{1}|x_{0})} \bigg{/}\int\frac{\pi(x_{0},\tilde{x}_{1})p_{1-t}(\tilde{x}_{1}|x)}{p_{1}( \tilde{x}_{1}|x_{0})}\mathrm{d}\tilde{x}_{1}\] \[=\frac{\pi(x_{0},x_{1})p_{1-t}(x_{1}|x)}{\mu(x_{0})p_{1}(x_{1}|x _{0})}\bigg{/}\int\frac{\pi(x_{0},\tilde{x}_{1})p_{1-t}(\tilde{x}_{1}|x)}{\mu(x_ {0})p_{1}(\tilde{x}_{1}|x_{0})}\mathrm{d}\tilde{x}_{1}\] \[=\frac{\tilde{\pi}_{0}^{x_{0}}(x_{1})p_{1-t}(x_{1}|x)}{\int\tilde{ \pi}_{0}^{x_{0}}(\tilde{x}_{1})p_{1-t}(\tilde{x}_{1}|x)\mathrm{d}\tilde{x}_{1}}\.\]

[MISSING_PAGE_FAIL:19]

### Proof of Theorem 2

We fix \(0<\epsilon<\min\{h,1/2\}\) and, for any \(t\in[0,1-\epsilon]\), we denote by \(\nu_{t}^{\star}=\text{Law}(X_{t}^{\rm M})\) and \(\nu_{t}^{\theta^{\star}}=\text{Law}(X_{t}^{\theta^{\star}})\).

First, using the data processing inequality [14, Lemma 1.6], the standard decomposition of the KL divergence [13, 12, 11] based on Girsanov theorem, triangle inequality and \(\mathbf{H}\)3, we bound the KL divergence between \(\nu_{1-\epsilon}^{\star}\) and \(\nu_{1-\epsilon}^{\theta^{\star}}\) as follows

\[\begin{split}&\text{KL}(\nu_{1-\epsilon}^{\star}|\nu_{1-\epsilon}^{ \theta^{\star}})\\ &\leq\text{KL}(\text{Law}((X_{t}^{\rm M})_{t\in[0,1-\epsilon]})| \text{Law}((X_{t}^{\theta^{\star}})_{t\in[0,1-\epsilon]}))\\ &\lesssim\sum_{k=0}^{N-2}\int_{t_{k}}^{t_{k+1}}\mathbb{E}\Big{[} \left\|\tilde{s}_{\theta^{\star}}(t_{k},X_{t_{k}}^{\rm M})-\tilde{\beta}_{t}( X_{t}^{\rm M})\right\|^{2}\Big{]}\text{d}t+\int_{1-h}^{1-\epsilon}\mathbb{E} \Big{[}\left\|s_{\theta^{\star}}(1-h,X_{1-h}^{\rm M})-\tilde{\beta}_{t}(X_{t}^ {\rm M})\right\|^{2}\Big{]}\text{d}t\\ &\lesssim\varepsilon^{2}+\sum_{k=0}^{N-2}\int_{t_{k}}^{t_{k+1}} \mathbb{E}\Big{[}\left\|\tilde{\beta}_{t_{k}}(X_{t_{k}}^{\rm M})-\tilde{\beta}_ {t}(X_{t}^{\rm M})\right\|^{2}\Big{]}\text{d}t+\int_{1-h}^{1-\epsilon}\mathbb{ E}\Big{[}\left\|\tilde{\beta}_{1-h}(X_{1-h}^{\rm M})-\tilde{\beta}_{t}(X_{t}^{\rm M}) \right\|^{2}\Big{]}\text{d}t\.\end{split} \tag{30}\]

Second, we aim at bounding the RHS of (30) uniformly in \(\epsilon\). Indeed, if we assume to be able to bound it with a constant \(A\) independent of \(\epsilon\), then, using the weak convergence of \(X_{1-\epsilon}^{\rm M}\) to \(X_{1}^{1}\) (whose law is given by \(\nu^{\star}\)) as \(\epsilon\to 0\), the continuity of \((X_{t}^{\theta^{\star}})_{t\in[0,1]}\), hence the weak convergence of \(X_{1-\epsilon}^{\theta^{\star}}\) to \(X_{1}^{\theta^{\star}}\) (whose law is given by \(\nu_{1}^{\theta^{\star}}\)) as \(\epsilon\to 0\), and the lower semi-continuity of the \(\rm KL\)-divergence with respect to the weak convergence [15, Theorem 19], we will get

\[\begin{split}\text{KL}(\nu^{\star}|\nu_{1}^{\theta^{\star}})& \leq\liminf_{\epsilon\to 0}\text{KL}(\nu_{1-\epsilon}^{\star}|\nu_{1- \epsilon}^{\theta^{\star}})\lesssim\liminf_{\epsilon\to 0}A=A\.\end{split} \tag{31}\]

Let us therefore bound the RHS of (30). We will do so by using stochastic calculus tools, and, more precisely, Ito's formula. To this aim let us introduce the generator of \((X_{t}^{\rm M})_{t\in[0,1-\epsilon]}\), which is defined for any \(t\in[0,1-\epsilon]\) and \(\rho\in C^{2}(\mathbb{R}^{d})\) as

\[\mathcal{L}_{t}^{\rm M}\rho:=\langle\nabla_{x}\rho,\tilde{\beta}_{t}\rangle+ \Delta_{x}\rho\.\]

Using Ito's formula, we get that

\[\begin{split}&\text{d}\tilde{\beta}_{t}(X_{t}^{\rm M})=(\partial_{t }+\mathcal{L}_{t}^{\rm M})\tilde{\beta}_{t}(X_{t}^{\rm M})\text{d}t+\sqrt{2} D_{x}\tilde{\beta}_{t}(X_{t}^{\rm M})\text{d}B_{t}\,\quad t\in[0,1-\epsilon]\.\end{split}\]

So, applying Young inequality and Ito's isometry, we have that, for any \(k=0,\cdots,N-1\)

\[\begin{split}&\mathbb{E}\Big{[}\left\|\tilde{\beta}_{t_{k}}(X_{t_{k}}^ {\rm M})-\tilde{\beta}_{t}(X_{t}^{\rm M})\right\|^{2}\Big{]}\\ &=\mathbb{E}\Bigg{[}\left\|\int_{t_{k}}^{t}(\partial_{s}+\mathcal{ L}_{s}^{\rm M})\tilde{\beta}_{s}(X_{s}^{\rm M})\text{d}s+\sqrt{2}\int_{t_{k}}^{t}D_{x} \tilde{\beta}_{s}(X_{s}^{\rm M})\text{d}B_{s}\ \right\|^{2}\Bigg{]}\\ &\lesssim\mathbb{E}\Bigg{[}\left\|\int_{t_{k}}^{t}(\partial_{s}+ \mathcal{L}_{s}^{\rm M})\tilde{\beta}_{s}(X_{s}^{\rm M})\text{d}s\right\|^{2} \Bigg{]}+2\int_{t_{k}}^{t}\mathbb{E}\Big{[}\left\|D_{x}\tilde{\beta}_{s}(X_{s}^{ \rm M})\right\|^{2}\Big{]}\text{d}s\.\end{split}\]

We now bound separately the two upper addends. To do so, we introduce the auxiliary measures \(\lambda_{k}^{h}(\text{d}s)\in\mathcal{P}([t_{k},t_{k+1}])\) for \(k=0,...,N-2\) and \(\lambda_{N-1}^{h}(\text{d}s)\in\mathcal{P}([1-h,1-\epsilon])\) which will help us, via a double change of measure argument, to mitigate the bad behaviour at \(t=0\) and \(t=1\) of the reciprocal characteristic of the mimicking drift (_i.e._, \(\partial_{s}+\mathcal{L}_{s}^{\rm M}\)), which is the trickiest addend. Namely, for \(k=0,...,N-2\) we consider the measures \(\lambda_{k}^{h}(\text{d}s)\in\mathcal{P}([t_{k},t_{k+1}])\) defined as

\[\lambda_{k}^{h}(\text{d}s)=\frac{\rho(s)^{-1}}{Z_{k}}\mathbb{1}_{[t_{k},t_{k+1 }]}\text{d}s\,\]

with

\[\rho(s)^{-1}=s^{-7/8}\mathbb{1}_{\{s\leq 1/2\}}+(1-s)^{-7/8}\mathbb{1}_{\{s>1/2\}}\,\]

and

\[Z_{k}=\int_{\min\{t_{k+1},1/2\}}^{\min\{t_{k+1},1/2\}}r^{-7/8}\text{d}r+\int_{ \max\{t_{k},1/2\}}^{\max\{t_{k+1},1/2\}}(1-r)^{-7/8}\text{d}r\.\]Whereas, for \(k=N-1\), we consider the measures \(\lambda_{N-1}^{h}(\mathrm{d}s)\in\mathcal{P}([1-h,1-\epsilon])\) defined as

\[\lambda_{N-1}^{h}(\mathrm{d}s)=\frac{\rho(s)^{-1}}{Z_{N-1}}\mathbb{1}_{[1-h,1- \epsilon]}\mathrm{d}s\;,\]

with \(\rho(s)^{-1}\) as above and

\[Z_{N-1}=\int_{\min\{1-h,1/2\}}^{\min\{1-\epsilon,1/2\}}r^{-7/8}\mathrm{d}r+ \int_{\max\{1-h,1/2\}}^{\max\{1-\epsilon,1/2\}}(1-r)^{-7/8}\mathrm{d}r\;.\]

Note that, for any \(s\in[0,1-\epsilon]\) and for any \(k=0,...,N-1\), they hold

\[\rho(s)\lesssim 1\;,\quad Z_{k}\lesssim h^{1/8}\;,\quad\int_{0}^{1-\epsilon} \rho^{-1}(s)\mathrm{d}s=16\left(\frac{1}{2}\right)^{1/8}-8\epsilon^{1/8} \lesssim 1\;. \tag{32}\]

We start by bounding the first addend, that is the one that involves the reciprocal characteristic of the mimicking drift. With a first change of measure argument, we get for any \(k=0,...,N-1\),

\[\mathbb{E}\Bigg{[}\left\|\int_{t_{k}}^{t}(\partial_{s}+\mathcal{L}_{s}^{\mathrm{ M}})\tilde{\beta}_{s}(X_{s}^{\mathrm{M}})\mathrm{d}s\right\|^{2}\Bigg{]}=Z_{k}^{2} \mathbb{E}\Bigg{[}\left\|\int_{t_{k}}^{t}(\partial_{s}+\mathcal{L}_{s}^{ \mathrm{M}})\tilde{\beta}_{s}(X_{s}^{\mathrm{M}})\rho(s)\lambda_{k}^{h}( \mathrm{d}s)\right\|^{2}\Bigg{]}\;,\]

where, in the last inequality, we used (32). But then, if we apply Jensen inequality and use an other change of measure argument, we get

\[\mathbb{E}\Bigg{[}\left\|\int_{t_{k}}^{t}(\partial_{s}+\mathcal{L} _{s}^{\mathrm{M}})\tilde{\beta}_{s}(X_{s}^{\mathrm{M}})\mathrm{d}s\right\|^{2} \Bigg{]} \leq Z_{k}^{2}\mathbb{E}\Bigg{[}\int_{t_{k}}^{t}\left\|(\partial_ {s}+\mathcal{L}_{s}^{\mathrm{M}})\tilde{\beta}_{s}(X_{s}^{\mathrm{M}})\right\|^ {2}\rho(s)^{2}\lambda_{k}^{h}(\mathrm{d}s)\Bigg{]} \tag{33}\] \[\lesssim h^{1/8}\int_{t_{k}}^{t}\mathbb{E}\Big{[}\left\|(\partial _{s}+\mathcal{L}_{s}^{\mathrm{M}})\tilde{\beta}_{s}(X_{s}^{\mathrm{M}})\right\| ^{2}\Big{]}\rho(s)\mathrm{d}s\;.\]

Let us now focus on the second addend. Remarkably, this addend can be bounded via the reciprocal characteristic of \(\tilde{\beta}\): because of Ito's formula, for \(t\in[0,1-\epsilon]\), it holds true

\[\mathrm{d}\left\|\tilde{\beta}_{t}(X_{t}^{\mathrm{M}})\right\|^{2}=\left\{2 \langle\tilde{\beta}_{t},(\partial_{t}+\mathcal{L}_{t}^{\mathrm{M}})\tilde{ \beta}_{t}\rangle+2\left\|D_{x}\tilde{\beta}_{t}\right\|^{2}\right\}(X_{t}^{ \mathrm{M}})\mathrm{d}t+2\sqrt{2}\langle\tilde{\beta}_{t},D_{x}\tilde{\beta}_ {t}\rangle(X_{t}^{\mathrm{M}})\mathrm{d}B_{t}\;.\]

Consequently, if we assume that the process \((\int_{0}^{s}\langle\tilde{\beta}_{t},D_{x}\tilde{\beta}_{t}\rangle(X_{t}^{ \mathrm{M}})\mathrm{d}B_{t})_{s\in[0,1-\epsilon]}\) is a true martingale (see Lemma 4 below), we have that

\[2\int_{t_{k}}^{t}\mathbb{E}\Big{[}\left\|D_{x}\tilde{\beta}_{s} (X_{s}^{\mathrm{M}})\right\|^{2}\Big{]}\mathrm{d}s\] \[\leq\mathbb{E}\Big{[}\left\|\tilde{\beta}_{t}(X_{t}^{\mathrm{M}} )\right\|^{2}\Big{]}-\mathbb{E}\Big{[}\left\|\tilde{\beta}_{t_{k}}(X_{t_{k}}^{ \mathrm{M}})\right\|^{2}\Big{]}+2\Big{|}\int_{t_{k}}^{t}\mathbb{E}[(\tilde{\beta }_{s}(X_{s}^{\mathrm{M}}),(\partial_{s}+\mathcal{L}_{s}^{\mathrm{M}})\tilde{ \beta}_{s}(X_{s}^{\mathrm{M}}))]\mathrm{d}s\right|\;.\]

But then, using, as before, a double change of measure argument and applying Cauchy-Schwartz inequality, we can bound the above expression as follows

\[2\int_{t_{k}}^{t}\mathbb{E}\Big{[}\left\|D_{x}\tilde{\beta}_{s}(X _{s}^{\mathrm{M}})\right\|^{2}\Big{]}\mathrm{d}s\] \[\leq\mathbb{E}\Big{[}\left\|\tilde{\beta}_{t}(X_{t}^{\mathrm{M}} )\right\|^{2}\Big{]}-\mathbb{E}\Big{[}\left\|\tilde{\beta}_{t_{k}}(X_{t_{k}}^{ \mathrm{M}})\right\|^{2}\Big{]}\] \[\leq\mathbb{E}\Big{[}\left\|\tilde{\beta}_{t}(X_{t}^{\mathrm{M}} )\right\|^{2}\Big{]}-\mathbb{E}\Big{[}\left\|\tilde{\beta}_{t_{k}}(X_{t_{k}}^{ \mathrm{M}})\right\|^{2}\Big{]}+Z_{k}\int_{t_{k}}^{t}\mathbb{E}\Big{[}\left\| \tilde{\beta}_{s}(X_{s}^{\mathrm{M}})\right\|^{2}\Big{]}\lambda_{k}^{h}(\mathrm{d }s)\] \[\quad+Z_{k}\int_{t_{k}}^{t}\mathbb{E}\Big{[}\left\|(\partial_{s}+ \mathcal{L}_{s}^{\mathrm{M}})\tilde{\beta}_{s}(X_{s}^{\mathrm{M}})\right\|^{2} \Big{]}\rho(s)^{2}\lambda_{k}^{h}(\mathrm{d}s)\] \[=\mathbb{E}\Big{[}\left\|\tilde{\beta}_{t}(X_{t}^{\mathrm{M}}) \right\|^{2}\Big{]}-\mathbb{E}\Big{[}\left\|\tilde{\beta}_{t_{k}}(X_{t_{k}}^{ \mathrm{M}})\right\|^{2}\Big{]}+\int_{t_{k}}^{t}\mathbb{E}\Big{[}\left\|\tilde{ \beta}_{s}(X_{s}^{\mathrm{M}})\right\|^{2}\Big{]}\rho(s)^{-1}\mathrm{d}s\] \[\quad+\int_{t_{k}}^{t}\mathbb{E}\Big{[}\left\|(\partial_{s}+ \mathcal{L}_{s}^{\mathrm{M}})\tilde{\beta}_{s}(X_{s}^{\mathrm{M}})\right\|^{2} \Big{]}\rho(s)\mathrm{d}s\;.\]Plugging this bound and (33) in (30), we get

\[\mathrm{KL}(\nu_{1-\epsilon}^{\star}|\nu_{1-\epsilon}^{\theta^{\star }})\lesssim\varepsilon^{2}+h\mathbb{E}\Big{[}\left\|\tilde{\beta}_{1-\epsilon}(X _{1-\epsilon}^{\mathrm{M}})\right\|^{2}\Big{]}+h\int_{0}^{1-\epsilon}\mathbb{E} \Big{[}\left\|\tilde{\beta}_{s}(X_{s}^{\mathrm{M}})\right\|^{2}\Big{]}\rho(s)^{ -1}\mathrm{d}s\\ +h(h^{1/8}+1)\int_{0}^{1-\epsilon}\mathbb{E}\Big{[}\left\|(\partial _{s}+\mathcal{L}_{s}^{\mathrm{M}})\tilde{\beta}_{s}(X_{s}^{\mathrm{M}})\right\|^ {2}\Big{]}\rho(s)\mathrm{d}s. \tag{34}\]

We now compute explicitly and upper bound each term appearing in the RHS of (34), recalling that, because of Theorem 1, for any \(s\in[0,1-\epsilon]\), \(\nu_{s}^{\star}=\text{Law}(X_{s}^{1})\). We start with

\[\mathbb{E}\Big{[}\left\|\tilde{\beta}_{1-\epsilon}(X_{1-\epsilon}^{\mathrm{M}} )\right\|^{2}\Big{]}\lesssim\int_{\mathbb{R}^{d}}\left\|\frac{\int_{\mathbb{R} ^{2d}}p_{1-\epsilon}(x|x_{0})\nabla_{x}p_{\epsilon}(x_{1}|x)\tilde{\pi}(x_{0},x _{1})\mathrm{d}x_{1}\mathrm{d}x_{0}}{p_{1-\epsilon}^{\mathrm{I}}(x)}\right\|^{ 2}p_{1-\epsilon}^{\mathrm{I}}(x)\mathrm{d}x\.\]

First we use (18), second we integrate by part, third we apply Jensen inequality and last we rely on \(\mathbf{H}\) 2.

\[\mathbb{E}\Big{[}\left\|\tilde{\beta}_{1-\epsilon}(X_{1-\epsilon}^ {\mathrm{M}})\right\|^{2}\Big{]}\] \[\lesssim\int_{\mathbb{R}^{d}}\left\|\frac{\int_{\mathbb{R}^{2d}}p_ {1-\epsilon}(x|x_{0})p_{\epsilon}(x_{1}|x)(\nabla_{x_{1}}\tilde{\pi}(x_{0},x_{ 1})/\tilde{\pi}(x_{0},x_{1}))\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x _{1}}{p_{1-\epsilon}^{\mathrm{I}}(x)}\right\|^{2}p_{1-\epsilon}^{\mathrm{I}}(x )\mathrm{d}x\] \[=\mathbb{E}\Bigg{[}\left\|\mathbb{E}\Bigg{[}\frac{\nabla_{x_{1}} \tilde{\pi}}{\tilde{\pi}}(X_{0}^{1},X_{1}^{\mathrm{I}})\Bigg{|}X_{1-\epsilon}^ {\mathrm{I}}\Bigg{]}\right\|^{2}\Bigg{]}\leq\mathbb{E}\Bigg{[}\mathbb{E}\Bigg{[} \left\|\frac{\nabla_{x_{1}}\tilde{\pi}}{\tilde{\pi}}(X_{0}^{1},X_{1}^{\mathrm{ I}})\right\|^{2}\Bigg{|}X_{1-\epsilon}^{\mathrm{I}}\Bigg{]}\Bigg{]}\] \[=\mathbb{E}\Bigg{[}\left\|\frac{\nabla_{x_{1}}\tilde{\pi}}{\tilde{ \pi}}(X_{0}^{1},X_{1}^{\mathrm{I}})\right\|^{2}\Bigg{]}=\left\|\frac{\nabla_{x_ {1}}\tilde{\pi}}{\tilde{\pi}}\right\|^{2}_{\mathrm{L}^{2}(\pi)}\leq\left\| \nabla\log\tilde{\pi}\right\|^{2}_{\mathrm{L}^{2}(\pi)}\.\]

In the very same way we deal with the third term of the RHS of (34).

\[\int_{0}^{1-\epsilon}\mathbb{E}\Big{[}\left\|\tilde{\beta}_{s}(X_ {s}^{\mathrm{M}})\right\|^{2}\Big{]}\rho(s)^{-1}\mathrm{d}s\] \[\lesssim\int_{0}^{1-\epsilon}\rho(s)^{-1}\int_{\mathbb{R}^{d}} \left\|\frac{\int_{\mathbb{R}^{2d}}p_{s}(x|x_{0})\nabla_{x}p_{1-s}(x_{1}|x) \tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}}{p_{s}^{\mathrm{I}}(x)} \right\|^{2}p_{s}^{\mathrm{I}}(x)\mathrm{d}x\mathrm{d}s\] \[=\int_{0}^{1-\epsilon}\rho(s)^{-1}\int_{\mathbb{R}^{d}}\left\| \frac{1}{p_{s}^{\mathrm{I}}(x)}\int_{\mathbb{R}^{2d}}p_{s}(x|x_{0})p_{1-s}(x_ {1}|x)\frac{\nabla_{x_{1}}\tilde{\pi}}{\tilde{\pi}}(x_{0},x_{1})\tilde{\pi}(x_ {0},x_{1})\mathrm{d}x_{0},\mathrm{d}x_{1}\right\|^{2}p_{s}^{\mathrm{I}}(x) \mathrm{d}x\] \[=\int_{0}^{1-\epsilon}\rho(s)^{-1}\mathbb{E}\Bigg{[}\left\|\mathbb{ E}\Bigg{[}\frac{\nabla_{x_{1}}\tilde{\pi}}{\tilde{\pi}}(X_{0}^{1},X_{1}^{\mathrm{I}}) \right\|^{2}\Bigg{|}X_{s}^{\mathrm{I}}\Bigg{]}\Bigg{]}\mathrm{d}s\] \[=\int_{0}^{1-\epsilon}\rho(s)^{-1}\mathrm{d}s\ \left\|\nabla\log\tilde{\pi} \right\|^{2}_{\mathrm{L}^{2}(\pi)}\lesssim\left\|\nabla\log\tilde{\pi}\right\|^ {2}_{\mathrm{L}^{2}(\pi)}\, \tag{36}\]

where, in the last inequality, we used (32). We now turn to the last term, _i.e._, to

\[\int_{0}^{1-\epsilon}\mathbb{E}\Big{[}\left\|(\partial_{s}+\mathcal{L}_{s}^{ \mathrm{M}})\tilde{\beta}_{s}(X_{s}^{\mathrm{M}})\right\|^{2}\Big{]}\rho(s) \mathrm{d}s=\int_{0}^{1-\epsilon}\int_{\mathbb{R}^{d}}\left\|(\partial_{s}+ \mathcal{L}_{s}^{\mathrm{M}})\tilde{\beta}_{s}(x)\right\|^{2}\rho(s)p_{s}^{ \mathrm{I}}(x)\mathrm{d}x\ \mathrm{d}s\.\]

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_EMPTY:24]

\[A_{s}^{2}(x)=-4\frac{\int_{\mathbb{R}^{2d}}\left\langle\nabla_{x}p_{1-s }(x_{1}|x),\nabla_{x}p_{s}(x|x_{0})\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0} \mathrm{d}x_{1}\right.}{p_{s}^{1}(x)}\\ \cdot\frac{\int_{\mathbb{R}^{2d}}p_{s}(x|x_{0})\nabla_{x}p_{1-s}( x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}}{p_{s}^{1}(x)}\;,\]

\[A_{s}^{3}(x)=4\frac{\left\|\int_{\mathbb{R}^{2d}}\nabla_{x}p_{s}(x|x_{0})p_{1-s }(x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}\right\|^{2}} {(p_{s}^{1}(x))^{2}}\\ \cdot\frac{\int_{\mathbb{R}^{2d}}p_{s}(x|x_{0})\nabla_{x}p_{1-s}( x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}}{p_{s}^{1}(x)}\;,\]

\[A_{s}^{4}(x)=4\frac{\int_{\mathbb{R}^{2d}}p_{s}(x|x_{0})\nabla_{x}p_{1-s}(x_{1} |x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}}{p_{s}^{1}(x)}\\ \cdot\left(\frac{\int_{\mathbb{R}^{2d}}\nabla_{x}p_{s}(x|x_{0})p_{1- s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}}{p_{s}^{1}(x)} \right)^{\mathrm{T}}\int_{\mathbb{R}^{2d}}p_{s}(x|x_{0})\nabla_{x}p_{1-s}(x_{1} |x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}}{p_{s}^{1}(x)}\;.\]

\[A_{s}^{5}(x)=4\frac{\int_{\mathbb{R}^{2d}}\Delta_{x}p_{s}(x|x_{0})\nabla_{x}p_ {1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}}{p_{s}^{1 }(x)}\\ -4\frac{\int_{\mathbb{R}^{2d}}\Delta_{x}p_{s}(x|x_{0})p_{1-s}(x_{1 }|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}\int_{\mathbb{R}^{2d }}p_{s}(x|x_{0})\nabla_{x}p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_ {0}\mathrm{d}x_{1}}{(p_{s}^{1}(x))^{2}}\;,\]

\[A_{s}^{6}(x)=4\frac{\int_{\mathbb{R}^{2d}}\nabla_{x}^{2}p_{1-s}( x_{1}|x)\nabla_{x}p_{s}(x|x_{0})\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0} \mathrm{d}x_{1}}{p_{s}^{1}(x)}\\ -4\frac{\int_{\mathbb{R}^{2d}}p_{s}(x|x_{0})\nabla_{x}^{2}p_{1- s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}\To bound the second term we first split the time interval \([0,1-\epsilon]\) in two, \([0,1/2]\) and \([1/2,1-\epsilon]\), we second make either \(\nabla_{x}p_{s}(x|x_{0})\) or \(\nabla_{x}p_{1-s}(x_{1}|x)\) explicit and we last proceed as before, _i.e._, we exploit (18), we integrate by parts and we use Young and Jensen inequalities.

\[\int_{0}^{1-\epsilon}\int_{\mathbb{R}^{d}}\left\|\frac{\int_{ \mathbb{R}^{2d}}\nabla_{x}p_{1-s}(x_{1}|x)(\nabla_{x}p_{s}(x|x_{0}))^{\rm T} \tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}}{p_{s}^{\rm I}(x)} \right\|_{\rm op}^{4}p_{s}^{\rm I}(x)\mathrm{d}x\mathrm{d}s\] \[=\int_{0}^{1/2}\int_{\mathbb{R}^{d}}\left\|\frac{\int_{\mathbb{R} ^{2d}}\nabla_{x}p_{s}(x|x_{0})(\nabla_{x}p_{1-s}(x_{1}|x))^{\rm T}\tilde{\pi} (x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}}{p_{s}^{\rm I}(x)}\right\|_{\rm op }^{4}p_{s}^{\rm I}(x)\mathrm{d}x\mathrm{d}s\] \[\quad+\int_{1/2}^{1-\epsilon}\int_{\mathbb{R}^{d}}\left\|\frac{ \int_{\mathbb{R}^{2d}}\nabla_{x}p_{1-s}(x_{1}|x)(\nabla_{x}p_{s}(x|x_{0}))^{ \rm T}\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}}{p_{s}^{\rm I}(x)} \right\|_{\rm op}^{4}p_{s}^{\rm I}(x)\mathrm{d}x\mathrm{d}s\] \[\lesssim\int_{0}^{1/2}\int_{\mathbb{R}^{d}}\left\|\frac{1}{p_{s}^ {\rm I}(x)}\int_{\mathbb{R}^{2d}}\frac{\nabla_{x_{0}}\tilde{\pi}}{\tilde{\pi}}( x_{0},x_{1})\frac{(x_{1}-x)^{\rm T}}{1-s}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\tilde{ \pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}\right\|_{\rm op}^{4}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\Proceeding in a similar way (we omit the argument, as it is almost a duplication of the previous one), we get

\[\int_{0}^{1-\epsilon}\int_{\mathbb{R}^{d}}\left\|A_{s}^{4}(x)\right\|^{2}\rho(s)p _{s}^{\mathrm{I}}(x)\mathrm{d}x\;\mathrm{d}s\lesssim\left\|\nabla\log\tilde{\pi} \right\|_{\mathrm{L}^{8}(\pi)}^{8}+\left\|\nabla\log\tilde{\pi}\right\|_{ \mathrm{L}^{4}(\pi)}^{4}\;.\]

We now turn to \(A_{s}^{5}\). Because of (20), \(A_{s}^{5}\) rewrites as

\[A_{s}^{5}(x) =\frac{1}{p_{s}^{\mathrm{I}}(x)}\int_{\mathbb{R}^{2d}}\Big{\{} \frac{-d}{2s}+\frac{\left\|x-x_{0}\right\|^{2}}{4s^{2}}\Big{\}}\frac{x_{1}-x}{ 2(1-s)}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0} \mathrm{d}x_{1}\] \[\quad-\Big{(}\frac{1}{p_{s}^{\mathrm{I}}(x)}\int_{\mathbb{R}^{2d}} \Big{\{}\frac{-d}{2s}+\frac{\left\|x-x_{0}\right\|^{2}}{4s^{2}}\Big{\}}p_{s}(x |x_{0})p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1} \Big{)}\] \[\quad\cdot\Big{(}\frac{1}{p_{s}^{\mathrm{I}}(x)}\int_{\mathbb{R}^{ 2d}}\frac{x_{1}-x}{2(1-s)}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1} )\mathrm{d}x_{0}\mathrm{d}x_{1}\Big{)}\] \[\lesssim\frac{1}{p_{s}^{\mathrm{I}}(x)}\int_{\mathbb{R}^{2d}} \frac{\left\|x-x_{0}\right\|^{2}}{s^{2}}\frac{x_{1}-x}{1-s}p_{s}(x|x_{0})p_{1- s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}\] \[\quad-\Big{(}\frac{1}{p_{s}^{\mathrm{I}}(x)}\int_{\mathbb{R}^{2d} }\frac{\left\|x-x_{0}\right\|^{2}}{s^{2}}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\tilde{ \pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}\Big{)}\] \[\quad\cdot\Big{(}\frac{1}{p_{s}^{\mathrm{I}}(x)}\int_{\mathbb{R}^ {2d}}\frac{x_{1}-x}{1-s}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1}) \mathrm{d}x_{0}\mathrm{d}x_{1}\Big{)}\;.\]

Therefore, we have that

\[\int_{0}^{1-\epsilon}\int_{\mathbb{R}^{d}}\left\|A_{s}^{5}(x) \right\|^{2}\rho(s)p_{s}^{\mathrm{I}}(x)\mathrm{d}x\;\mathrm{d}s \lesssim\int_{0}^{1-\epsilon}\mathbb{E}\Bigg{[}\left\|\mathbb{E} \Bigg{[}\frac{\left\|X_{s}^{\mathrm{I}}-X_{0}^{\mathrm{I}}\right\|^{2}}{s^{2}} \frac{X_{1}^{\mathrm{I}}-X_{s}^{\mathrm{I}}}{1-s}\Bigg{|}X_{s}^{\mathrm{I}} \right]\] \[\quad-\mathbb{E}\Bigg{[}\frac{\left\|X_{s}^{\mathrm{I}}-X_{0}^{ \mathrm{I}}\right\|^{2}}{s^{2}}\Bigg{|}X_{s}^{\mathrm{I}}\Bigg{]}\mathbb{E} \Bigg{[}\frac{X_{1}^{\mathrm{I}}-X_{s}^{\mathrm{I}}}{1-s}\Bigg{|}X_{s}^{ \mathrm{I}}\Bigg{]}\Bigg{\|}^{2}\Bigg{]}\rho(s)\mathrm{d}s\;.\]

We now split the time interval \([0,1-\epsilon]\) in two, \([0,1/2]\), \([1/2,1-\epsilon]\) and we focus on the first one. So we look at \(s\in[0,1/2]\) and we try to bound the integrand. Using Lemma 1, Lemma 2, Lemma 3 and standard and well-known inequalities (Cauchy-Schwarz, Young and Jensen inequalities), we get that for \(s\in[0,1/2]\)

\[\mathbb{E}\Bigg{[}\Bigg{\|}\mathbb{E}\Bigg{[}\frac{\big{\|}\overset{ \overleftarrow{f}}{f}_{1-s}^{s}\big{\|}^{2}+2\langle\overleftarrow{f}_{1-s}^{s},\overleftarrow{g}_{1-s}^{s}\rangle}{s^{2}}\bigg{|}\widetilde{X}_{1-s}\bigg{|} \widetilde{X}_{1-s}\bigg{]}\Bigg{\|}^{2}\Bigg{]}s^{7/8}\] \[=\mathbb{E}\Bigg{[}\Bigg{\|}\mathbb{E}\Bigg{[}\frac{\big{\|} \overset{\overleftarrow{f}}{f}_{1-s}^{s}\big{\|}^{2}+2\langle\overleftarrow{f} _{1-s}^{s},\overleftarrow{g}_{1-s}^{s}\rangle+\big{\|}\overleftarrow{g}_{1-s} ^{s}\big{\|}^{2}}{s^{2}}(\widetilde{X}_{0}-\overleftarrow{X}_{1-s})\Bigg{|} \widetilde{X}_{1-s}\Bigg{]}\] \[\quad-\mathbb{E}\Bigg{[}\frac{\big{\|}\overset{\overleftarrow{f}} {f}_{1-s}^{s}\big{\|}^{2}+2\langle\overleftarrow{f}_{1-s}^{s},\overleftarrow{g }_{1-s}^{s}\rangle+\big{\|}\overleftarrow{g}_{1-s}^{s}\big{\|}^{2}}{s^{2}} \bigg{|}\widetilde{X}_{1-s}\Bigg{]}\Bigg{\|}\mathbb{E}\Big{[}\widetilde{X}_{0}- \overleftarrow{X}_{1-s}\Big{|}\widetilde{X}_{1-s}\Bigg{]}\Bigg{\|}^{2}\Bigg{]} s^{7/8}\] \[=\mathbb{E}\Bigg{[}\Bigg{\|}\mathbb{E}\Bigg{[}\frac{\big{\|} \overset{\overleftarrow{f}}{f}_{1-s}^{s}\big{\|}^{2}+2\langle\overleftarrow{f} _{1-s}^{s},\overleftarrow{g}_{1-s}^{s}\rangle}{s^{2}}(\widetilde{X}_{0}- \overleftarrow{X}_{1-s})\bigg{|}\widetilde{X}_{1-s}\Bigg{]}\] \[\quad-\mathbb{E}\Bigg{[}\frac{\big{\|}\overset{\overleftarrow{f}} {f}_{1-s}^{s}\big{\|}^{2}+2\langle\overleftarrow{f}_{1-s}^{s},\overleftarrow{g }_{1-s}^{s}\rangle}{s^{2}}\bigg{|}\widetilde{X}_{1-s}\Bigg{]}\mathbb{E}\Big{[} \widetilde{X}_{0}-\overleftarrow{X}_{1-s}\Big{|}\widetilde{X}_{1-s}\Big{]} \Bigg{\|}^{2}\Bigg{]}s^{7/8}\] \[\lesssim\mathbb{E}\Bigg{[}\Bigg{\|}\mathbb{E}\Bigg{[}\frac{ \big{\|}\overset{\overleftarrow{f}}{f}_{1-s}^{s}\big{\|}^{2}+2\langle \overleftarrow{f}_{1-s}^{s},\overleftarrow{g}_{1-s}^{s}\rangle}{s^{2}}( \widetilde{X}_{0}-\overleftarrow{X}_{1-s})\bigg{|}\mathbb{E}\Big{[} \widetilde{X}_{0}-\overleftarrow{X}_{1-s}\Big{]}\Bigg{\|}^{2}\Bigg{]}s^{7/8}\] \[\lesssim\mathbb{E}\Bigg{[}\Bigg{\|}\frac{\big{\|}\overset{ \overleftarrow{f}}{f}_{1-s}^{s}\big{\|}^{2}+2\langle\overleftarrow{f}_{1-s}^{ s},\overleftarrow{g}_{1-s}^{s}\rangle}{s^{2}}(\widetilde{X}_{0}- \overleftarrow{X}_{1-s})\Bigg{\|}^{2}\Bigg{]}s^{7/8}\] \[\quad+\mathbb{E}\Bigg{[}\Bigg{\|}\mathbb{E}\Bigg{[}\frac{\big{\|} \overset{\overleftarrow{f}}{f}_{1-s}^{s}\big{\|}^{2}+2\langle\overleftarrow{f} _{1-s}^{s},\overleftarrow{g}_{1-s}^{s}\rangle}{s^{2}}\bigg{|}\widetilde{X}_{1- s}\Bigg{]}\Bigg{\|}^{4}\Bigg{]}s^{7/4}+\mathbb{E}\Bigg{[}\Big{\|}\mathbb{E}\Big{[} \widetilde{X}_{0}-\overleftarrow{X}_{1-s}\Big{|}\widetilde{X}_{1-s}\Big{]} \Big{\|}^{4}\Bigg{]}\] \[\lesssim\mathbb{E}\Bigg{[}\Bigg{\|}\frac{\big{\|}\overset{ \overleftarrow{f}}{f}_{1-s}^{s}\big{\|}^{2}+2\langle\overleftarrow{f}_{1-s}^{ s},\overleftarrow{g}_{1-s}^{s}\rangle}{s^{2}}\Bigg{\|}^{4}\Bigg{]}s^{7/4}+ \mathbb{E}\Big{[}\big{\|}\widetilde{X}_{0}-\overleftarrow{X}_{1-s}\Big{\|}^{4} \Big{]}\] \[\lesssim\mathbb{E}\Bigg{[}\frac{\big{\|}\overset{\overleftarrow{f}} {f}_{1-s}^{s}\big{\|}^{8}}{s^{8}}\Bigg{]}+\mathbb{E}\Bigg{[}\frac{\langle \overleftarrow{f}_{1-s}^{s},\overleftarrow{g}_{1-s}^{s}\rangle^{4}}{s^{8}} \Bigg{]}s^{7/4}+\mathbb{E}\Big{[}\big{\|}\widetilde{X}_{0}-\overleftarrow{X}_{1- s}\Big{\|}^{4}\Big{]}\]\[\lesssim\mathbb{E}\left[\left\|\frac{\left\|\overleftarrow{f}_{1-s}^{s} \right\|^{8}}{s^{8}}\right\|+\mathbb{E}\left[\frac{\left\|\overleftarrow{g}_{1-s} ^{s}\right\|^{8}}{s^{8}}\right]s^{7/2}+\mathbb{E}\Big{[}\left\|\overleftarrow{ X}_{0}-\overleftarrow{X}_{1-s}\right\|^{4}\Big{]}\right.\] \[\lesssim\mathbb{E}\left[\left\|\frac{\left\|\overleftarrow{f}_{1-s} ^{s}\right\|^{8}}{s^{8}}\right\|+d^{4}s^{-1/2}+\mathbb{E}\Big{[}\left\|X_{1}^{ 1}-X_{s}^{1}\right\|^{4}\Big{]}\right.\] \[\lesssim\left\|\nabla\log\tilde{\pi}\right\|_{\mathrm{L}^{8}(\pi)} ^{8}+\left\|\nabla\log\nu^{\star}\right\|_{\mathrm{L}^{8}(\nu^{\star})}^{8}+d^ {4}s^{-1/2}+d^{2}+\mathbf{m}_{4}[\mu]+\mathbf{m}_{4}[\nu^{\star}]\.\]

But then, we obtain that

\[\int_{0}^{1/2}\int_{\mathbb{R}^{d}}\left\|A_{s}^{5}(x)\right\|^{2 }\rho(s)p_{s}^{\mathrm{I}}(x)\mathrm{d}x\ \mathrm{d}s\lesssim d^{4}+\mathbf{m}_{4}[ \mu]+\mathbf{m}_{4}[\nu^{\star}]+\left\|\nabla\log\tilde{\pi}\right\|_{\mathrm{ L}^{8}(\pi)}^{8}\\ +\left\|\nabla\log\nu^{\star}\right\|_{\mathrm{L}^{8}(\nu^{\star}) }^{8}\.\]

We now focus on the second time interval, that is we look at \(s\in[1/2,1-\epsilon]\) and try to bound the integrand. Using (32) and proceeding in a similar way, we get that

\[\mathbb{E}\Bigg{[}\left\|\mathbb{E}\Bigg{[}\frac{\left\|X_{s}^{1}- X_{0}^{1}\right\|^{2}}{s^{2}}\frac{X_{1}^{1}-X_{s}^{1}}{1-s}\right\|X_{s}^{1} \Bigg{]}-\mathbb{E}\left[\frac{\left\|X_{s}^{1}-X_{0}^{1}\right\|^{2}}{s^{2}}X _{s}^{1}\right]\mathbb{E}\Bigg{[}\frac{X_{1}^{1}-X_{s}^{1}}{1-s}\Bigg{|}X_{s}^ {1}\Bigg{]}\Bigg{]}^{2}\Bigg{]}\rho(s)\] \[\lesssim\mathbb{E}\Bigg{[}\left\|\mathbb{E}\Bigg{[}\left\| \overrightarrow{X}_{s}-\overrightarrow{X}_{0}\right\|^{2}\overrightarrow{f} _{s}^{1-s}+\overrightarrow{g}_{s}^{1-s}\right\|\overrightarrow{X}_{s}\Bigg{]} \Bigg{]}^{2}\Bigg{]}\] \[\lesssim\mathbb{E}\Big{[}\left\|\overrightarrow{X}_{s}- \overrightarrow{X}_{0}\right\|^{8}\Big{]}+\mathbb{E}\Bigg{[}\frac{\left\| \overrightarrow{f}_{s}^{1-s}\right\|^{4}}{(1-s)^{4}}\Bigg{]}=\mathbb{E}\Big{[} \left\|X_{s}^{1}-X_{0}^{1}\right\|^{8}\Big{]}+\mathbb{E}\Bigg{[}\frac{\left\| \overrightarrow{f}_{s}^{1-s}\right\|^{4}}{(1-s)^{4}}\Bigg{]}\] \[\lesssim d^{4}+\mathbf{m}_{8}[\mu]+\mathbf{m}_{8}[\nu^{\star}]+ \left\|\nabla\log\tilde{\pi}\right\|_{\mathrm{L}^{4}(\pi)}^{4}+\left\|\nabla \log\mu\right\|_{\mathrm{L}^{4}(\mu)}^{4}\.\]

But then, we have that

\[\int_{1/2}^{1-\epsilon}\int_{\mathbb{R}^{d}}\left\|A_{s}^{5}(x) \right\|^{2}\rho(s)p_{s}^{\mathrm{I}}(x)\mathrm{d}x\ \mathrm{d}s\lesssim d^{4}+ \mathbf{m}_{8}[\mu]+\mathbf{m}_{8}[\nu^{\star}]+\left\|\nabla\log\tilde{\pi} \right\|_{\mathrm{L}^{8}(\pi)}^{8}\\ +\left\|\nabla\log\mu\right\|_{\mathrm{L}^{8}(\mu)}^{8}\.\]

To conclude, we can bound \(A_{s}^{5}\) as follows

\[\int_{0}^{1-\epsilon}\int_{\mathbb{R}^{d}}\left\|A_{s}^{5}(x) \right\|^{2}\rho(s)p_{s}^{\mathrm{I}}(x)\mathrm{d}x\ \mathrm{d}s\lesssim d^{4}+ \mathbf{m}_{8}[\mu]+\mathbf{m}_{8}[\nu^{\star}]+\left\|\nabla\log\tilde{\pi} \right\|_{\mathrm{L}^{8}(\pi)}^{8}\\ +\left\|\nabla\log\mu\right\|_{\mathrm{L}^{8}(\mu)}^{8}+\left\| \nabla\log\nu^{\star}\right\|_{\mathrm{L}^{8}(\nu^{\star})}^{8}\.\]We are left with \(A_{s}^{6}\). Using (19), we can rewrite \(A_{s}^{6}\) as follows

\[A_{s}^{6}(x)\] \[=\frac{1}{p_{s}^{\mathrm{t}}(x)}\int_{\mathbb{R}^{2d}}\Big{\{}\frac {-1}{2(1-s)}\,\mathrm{Id}+\frac{(x_{1}-x)(x_{1}-x)^{\mathrm{T}}}{4(1-s)^{2}} \Big{\}}\frac{x-x_{0}}{2s}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1} )\mathrm{d}x_{0}\mathrm{d}x_{1}\] \[\quad-\Big{(}\frac{1}{p_{s}^{\mathrm{I}}(x)}\int_{\mathbb{R}^{2d}} \Big{\{}\frac{-1}{2(1-s)}\,\mathrm{Id}+\frac{(x_{1}-x)(x_{1}-x)^{\mathrm{T}}}{4 (1-s)^{2}}\Big{\}}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{ d}x_{0}\mathrm{d}x_{1}\Big{)}\] \[\quad\cdot\Big{(}\frac{1}{p_{s}^{\mathrm{I}}(x)}\int_{\mathbb{R}^ {2d}}\frac{x-x_{0}}{2s}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1}) \mathrm{d}x_{0}\mathrm{d}x_{1}\Big{)}\] \[\lesssim(\frac{1}{p_{s}^{\mathrm{t}}(x)}\int_{\mathbb{R}^{2d}} \frac{(x_{1}-x)(x_{1}-x)^{\mathrm{T}}}{(1-s)^{2}}\frac{x-x_{0}}{s}p_{s}(x|x_{0} )p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}\] \[\quad-\Big{(}\frac{1}{p_{s}^{\mathrm{I}}(x)}\int_{\mathbb{R}^{2d} }\frac{(x_{1}-x)(x_{1}-x)^{\mathrm{T}}}{(1-s)^{2}}p_{s}(x|x_{0})p_{1-s}(x_{1}| x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}\Big{)}\] \[\quad\cdot\Big{(}\frac{1}{p_{s}^{\mathrm{I}}(x)}\int_{\mathbb{R} ^{2d}}\frac{x-x_{0}}{s}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1}) \mathrm{d}x_{0}\mathrm{d}x_{1}\Big{)}\;.\]

It follows that

\[\int_{0}^{1-\epsilon}\int_{\mathbb{R}^{d}}\big{\|}A_{s}^{6}(x) \big{\|}^{2}\,\rho(s)p_{s}^{\mathrm{I}}(x)\mathrm{d}x\;\mathrm{d}s\] \[\lesssim\,\int_{0}^{1-\epsilon}\mathbb{E}\Bigg{[}\,\Bigg{\|} \mathbb{E}\Bigg{[}\frac{(X_{1}^{1}-X_{s}^{\mathrm{I}})(X_{1}^{1}-X_{s}^{ \mathrm{I}})^{\mathrm{T}}}{(1-s)^{2}}\frac{X_{s}^{\mathrm{I}}-X_{0}^{1}}{s} \Bigg{|}X_{s}^{\mathrm{I}}\Bigg{]}\] \[\qquad-\,\mathbb{E}\Bigg{[}\frac{(X_{1}^{1}-X_{s}^{\mathrm{I}})(X _{1}^{\mathrm{I}}-X_{s}^{\mathrm{I}})^{\mathrm{T}}}{(1-s)^{2}}\Bigg{|}X_{s}^{ \mathrm{I}}\Bigg{]}\mathbb{E}\Bigg{[}\frac{X_{s}^{\mathrm{I}}-X_{0}^{1}}{s} \Bigg{|}X_{s}^{\mathrm{I}}\Bigg{]}\Bigg{\|}^{2}\,\Bigg{]}\rho(s)\mathrm{d}s\]

At this point, we proceed as for \(A_{s}^{5}\), that is we split the time interval in two and we use Lemma 1, Lemma 2 and Lemma 3. By doing so and by using (32), we get that for \(s\in[0,1/2]\)

\[\mathbb{E}\Bigg{[}\,\Bigg{\|}\mathbb{E}\Bigg{[}\frac{(X_{1}^{1}-X _{s}^{\mathrm{I}})(X_{1}^{1}-X_{s}^{\mathrm{I}})^{\mathrm{T}}}{(1-s)^{2}} \frac{X_{s}^{\mathrm{I}}-X_{0}^{1}}{s}\Bigg{|}X_{s}^{\mathrm{I}}\Bigg{]}\] \[\qquad-\,\mathbb{E}\Bigg{[}\frac{(X_{1}^{1}-X_{s}^{\mathrm{I}})( X_{1}^{1}-X_{s}^{\mathrm{I}})^{\mathrm{T}}}{(1-s)^{2}}\Bigg{|}X_{s}^{\mathrm{I}} \Bigg{]}\mathbb{E}\Bigg{[}\frac{X_{s}^{\mathrm{I}}-X_{0}^{1}}{s}\Bigg{|}X_{s} ^{\mathrm{I}}\Bigg{]}\Bigg{\|}^{2}\,\Bigg{]}\rho(s)\] \[\lesssim\mathbb{E}\Bigg{[}\,\Bigg{\|}\mathbb{E}\Bigg{[}(\overset{ \leftarrow}{X}_{0}-\overset{\leftarrow}{X}_{1-s})(\overset{\leftarrow}{X}_{0}- \overset{\leftarrow}{X}_{1-s})^{\mathrm{T}}\frac{\overset{\leftarrow}{s}_{1-s} }{s}\Bigg{|}\overset{\leftarrow}{X}_{1-s}\Bigg{]}\] \[\qquad-\,\mathbb{E}\Big{[}(\overset{\leftarrow}{X}_{0}-\overset{ \leftarrow}{X}_{1-s})(\overset{\leftarrow}{X}_{0}-\overset{\leftarrow}{X}_{1-s })^{\mathrm{T}}\Big{|}\overset{\leftarrow}{X}_{1-s}\Big{]}\mathbb{E}\Bigg{[} \frac{\overset{\leftarrow}{f}_{1-s}}{s}\Bigg{|}\overset{\leftarrow}{X}_{1-s} \Bigg{]}\Bigg{\|}^{2}\,\Bigg{]}\] \[\lesssim\mathbb{E}\Big{[}\,\Big{\|}(\overset{\leftarrow}{X}_{0}- \overset{\leftarrow}{X}_{1-s})(\overset{\leftarrow}{X}_{0}-\overset{\leftarrow}{X}_{1-s })^{\mathrm{T}}\Big{\|}_{\mathrm{op}}\,\Big{]}+\mathbb{E}\Bigg{[}\,\Bigg{\|} \frac{\overset{\leftarrow}{f}_{1-s}}{s}\Bigg{|}^{4}\,\Bigg{]}\] \[=\mathbb{E}[\|X_{1}^{\mathrm{I}}-X_{s}^{\mathrm{I}}\|^{8}]+ \mathbb{E}\Bigg{[}\,\Bigg{\|}\frac{\overset{\leftarrow}{f}_{1-s}}{s}\Bigg{|}^{4} \,\Bigg{]}\] \[\lesssim d^{4}+\mathbf{m}_{8}[\mu]+\mathbf{m}_{8}[\nu^{\star}]+ \|\nabla\log\tilde{\pi}\|^{8}_{\mathrm{L}^{8}(\pi)}+\|\nabla\log\nu^{\star}\|^{8 }_{\mathrm{L}^{8}(\nu^{\star})}\;.\]Whereas, for \(s\in[1/2,1-\epsilon]\), we get

\[\mathbb{E}\bigg{[}\bigg{\|}\mathbb{E}\bigg{[}\frac{(X_{1}^{1}-X_{s} ^{1})(X_{1}^{1}-X_{s}^{1})^{\mathrm{T}}}{(1-s)^{2}}\frac{X_{s}^{1}-X_{0}^{1}}{s }\bigg{|}X_{s}^{1}\bigg{]}\] \[\quad-\,\mathbb{E}\bigg{[}\frac{(X_{1}^{1}-X_{s}^{1})(X_{1}^{1}-X_ {s}^{1})^{\mathrm{T}}}{(1-s)^{2}}\bigg{|}X_{s}^{1}\bigg{]}\mathbb{E}\bigg{[} \frac{X_{s}^{1}-X_{0}^{1}}{s}\bigg{|}X_{s}^{1}\bigg{]}\bigg{\|}^{2}\ \bigg{]} \rho(s)\] \[\lesssim\mathbb{E}\bigg{[}\bigg{\|}\mathbb{E}\bigg{[}\frac{( \overrightarrow{X}_{1}-\overrightarrow{X}_{s})(\overrightarrow{X}_{1}- \overrightarrow{X}_{s})^{\mathrm{T}}}{(1-s)^{2}}(\overrightarrow{X}_{s}- \overrightarrow{X}_{0})\bigg{|}\overrightarrow{X}_{s}\bigg{]}\] \[\quad\quad-\,\mathbb{E}\bigg{[}\frac{(\overrightarrow{X}_{1}- \overrightarrow{X}_{s})(\overrightarrow{X}_{1}-\overrightarrow{X}_{s})^{ \mathrm{T}}}{(1-s)^{2}}\bigg{|}\overrightarrow{X}_{s}\bigg{]}\mathbb{E}\big{[} \overrightarrow{X}_{s}-\overrightarrow{X}_{0}\big{|}\overrightarrow{X}_{s} \big{]}\bigg{\|}^{2}\ \bigg{]}(1-s)^{7/8}\] \[=(1-s)^{7/8}\mathbb{E}\bigg{[}\bigg{\|}\mathbb{E}\bigg{[}\frac{ \overrightarrow{f}_{s}^{1-s}(\overrightarrow{f}_{s}^{1-s})^{\mathrm{T}}+ \overrightarrow{f}_{s}^{1-s}(\overrightarrow{g}_{s}^{1-s})^{\mathrm{T}}+ \overrightarrow{g}_{s}^{1-s}(\overrightarrow{f}_{s}^{1-s})^{\mathrm{T}}}{(1-s )^{2}}(\overrightarrow{X}_{s}-\overrightarrow{X}_{0})\bigg{|}\overrightarrow{X }_{s}\bigg{]}\] \[\quad\quad\quad-\,\mathbb{E}\bigg{[}\frac{\overrightarrow{f}_{s}^ {1-s}(\overrightarrow{f}_{s}^{1-s})^{\mathrm{T}}+\overrightarrow{f}_{s}^{1-s} (\overrightarrow{g}_{s}^{1-s})^{\mathrm{T}}+\overrightarrow{g}_{s}^{1-s}( \overrightarrow{f}_{s}^{1-s})^{\mathrm{T}}}{(1-s)^{2}}\bigg{|}\overrightarrow{ X}_{s}\bigg{]}\mathbb{E}\Big{[}\overrightarrow{X}_{s}-\overrightarrow{X}_{0}\big{|} \overrightarrow{X}_{s}\Big{]}\bigg{\|}^{2}\ \bigg{]}\] \[\lesssim\mathbb{E}\bigg{[}\frac{\bigg{\|}\overrightarrow{f}_{s}^ {1-s}(\overrightarrow{f}_{s}^{1-s})^{\mathrm{T}}+\overrightarrow{f}_{s}^{1-s} (\overrightarrow{g}_{s}^{1-s})^{\mathrm{T}}+\overrightarrow{g}_{s}^{1-s}( \overrightarrow{f}_{s}^{1-s})^{\mathrm{T}}\bigg{\|}_{\mathrm{op}}^{4}}{(1-s )^{8}}\bigg{]}(1-s)^{7/4}+\mathbb{E}\big{[}\,\Big{\|}\overrightarrow{X}_{s}- \overrightarrow{X}_{0}\Big{\|}^{4}\ \bigg{]}\] \[\lesssim\mathbb{E}\bigg{[}\frac{\bigg{\|}\overrightarrow{f}_{s}^ {1-s}\bigg{\|}^{8}}{(1-s)^{8}}\bigg{]}+\mathbb{E}\bigg{[}\frac{\bigg{\|} \overrightarrow{g}_{s}^{1-s}\bigg{\|}^{8}}{(1-s)^{8}}\bigg{]}(1-s)^{7/2}+ \mathbb{E}\Big{[}\,\big{\|}X_{s}^{1}-X_{0}^{1}\big{\|}^{4}\ \bigg{]}\] \[\lesssim\|\nabla\log\tilde{\pi}\|_{\mathrm{L}^{8}(\pi)}^{8}+\| \nabla\log\mu\|_{\mathrm{L}^{8}(\mu)}^{8}+d^{4}(1-s)^{-1/2}+d^{2}+\mathbf{m}_{ 4}[\mu]+\mathbf{m}_{4}[\nu^{\star}]\.\]

Consequently, we have that

\[\int_{0}^{1-\epsilon}\int_{\mathbb{R}^{d}}\big{\|}A_{s}^{6}(x) \big{\|}^{2}\,\rho(s)p_{s}^{\mathrm{I}}(x)\mathrm{d}x\ \mathrm{d}s\lesssim d^{4}+\mathbf{m}_{8}[\mu] +\mathbf{m}_{8}[\nu^{\star}]+\|\nabla\log\tilde{\pi}\|_{\mathrm{ L}^{8}(\pi)}^{8}\\ +\|\nabla\log\mu\|_{\mathrm{L}^{8}(\mu)}^{8}+\|\nabla\log\nu^{ \star}\|_{\mathrm{L}^{8}(\nu^{\star})}^{8}\.\]

Putting together the bounds on the \(\{A_{s}^{k}\}_{k=1}^{6}\) derived so far, we eventually obtain

\[\int_{0}^{1-\epsilon}\mathbb{E}\Big{[}\,\Big{\|}(\partial_{s}+ \mathcal{L}_{s}^{\mathrm{M}})\tilde{\beta}_{s}(\overrightarrow{X}_{s})\Big{\|} ^{2}\,\Big{]}\rho(s)\mathrm{d}s \tag{41}\] \[\lesssim d^{4}+\mathbf{m}_{8}[\mu]+\mathbf{m}_{8}[\nu^{\star}]+ \|\nabla\log\tilde{\pi}\|_{\mathrm{L}^{8}(\pi)}^{8}+\|\nabla\log\mu\|_{ \mathrm{L}^{8}(\mu)}^{8}+\|\nabla\log\nu^{\star}\|_{\mathrm{L}^{8}(\nu^{ \star})}^{8}\.\]

Plugging (41), (35) and (36) into (34), we get

\[\mathrm{KL}(\nu^{\star}_{1-\epsilon}|\nu^{\theta^{\star}}_{1- \epsilon})\lesssim\varepsilon^{2}+h(h^{1/8}+1)\Big{(}d^{4}+\mathbf{m}_{8}[\mu] +\mathbf{m}_{8}[\nu^{\star}]+\|\nabla\log\tilde{\pi}\|_{\mathrm{L}^{8}(\pi)}^ {8}\\ +\|\nabla\log\mu\|_{\mathrm{L}^{8}(\mu)}^{8}+\|\nabla\log\nu^{ \star}\|_{\mathrm{L}^{8}(\nu^{\star})}^{8}\ )\.\]

The estimate (14) then follows from the above estimate and (31).

However, for (14) to hold true, we still need to prove that

**Lemma 4**.: \((\int_{0}^{s}(\tilde{\partial}_{t},D_{x}\tilde{\partial}_{t})(X_{t}^{\mathrm{M}} )\mathrm{d}B_{t})_{s\in[0,1-\epsilon]}\) _is a martingale._

Proof of Lemma 4.: If we show that for any \(s\in[0,1-\epsilon]\), it holds \(\mathbb{E}[\|\langle\tilde{\beta}_{s},D_{x}\tilde{\beta}_{s}\rangle(X_{t}^{ \mathrm{M}})\|^{2}]<C\), for some \(C>0\) independent of time, then by Fubini's theorem and [1, Theorem 7.3] we are done. To do so, by the Cauchy-Schwarz inequality, we just need to show that \(\mathbb{E}[\|\tilde{\beta}_{s}(X_{s}^{\mathrm{M}})\|^{4}]\) and \(\mathbb{E}[\left\|D_{x}\tilde{\beta}_{s}(X_{s}^{\rm M})\right\|_{\rm op}^{4}]\) are bounded from above by constants which are independent of time. To this aim, note that, as a direct consequence of (8), Theorem 1, (18), Jensen inequality and **H2**, it holds

\[\mathbb{E}\Big{[}\left\|\tilde{\beta}_{s}(X_{s}^{\rm M})\right\|^{ 4}\Big{]} \tag{42}\] \[\lesssim\int_{\mathbb{R}^{d}}\left\|\frac{f_{\mathbb{R}^{2d}}\,p_ {s}(x|x_{0})\nabla_{x}p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0} \mathrm{d}x_{1}}{p_{s}^{\rm I}(x)}\right\|^{4}p_{s}^{\rm I}(x)\mathrm{d}x\] \[=\int_{\mathbb{R}^{d}}\left\|\frac{1}{p_{s}^{\rm I}(x)}\int_{ \mathbb{R}^{2d}}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\frac{\nabla_{x_{1}}\tilde{\pi}} {\tilde{\pi}}(x_{0},x_{1})\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0},\mathrm{d}x_ {1}\right\|^{4}p_{s}^{\rm I}(x)\mathrm{d}x\] \[=\mathbb{E}\Bigg{[}\left\|\mathbb{E}\Bigg{[}\frac{\nabla_{x_{1}} \tilde{\pi}}{\tilde{\pi}}(X_{0}^{\rm I},X_{1}^{\rm I})\Bigg{|}X_{s}^{\rm I} \Bigg{]}\right\|^{4}\Bigg{]}\leq\mathbb{E}\Bigg{[}\mathbb{E}\Bigg{[}\left\|\frac {\nabla_{x_{1}}\tilde{\pi}}{\tilde{\pi}}(X_{0}^{\rm I},X_{1}^{\rm I})\right\|^ {4}\Bigg{|}X_{s}^{\rm I}\Bigg{]}\Bigg{]}\] \[=\mathbb{E}\Bigg{[}\left\|\frac{\nabla_{x_{1}}\tilde{\pi}}{\tilde{ \pi}}(X_{0}^{\rm I},X_{1}^{\rm I})\right\|^{4}\Bigg{]}=\left\|\nabla\log\tilde{ \pi}\right\|_{\mathrm{L}^{4}(\pi)}^{4}\;.\]

Similarly, recalling (38), we have that

\[\mathbb{E}\Big{[}\left\|D_{x}\tilde{\beta}_{s}(X_{s}^{\rm M}) \right\|_{\rm op}^{4}\Big{]}\] \[\lesssim\int_{\mathbb{R}^{d}}\left\|\frac{\int_{\mathbb{R}^{2d}} \nabla_{x}p_{1-s}(x_{1}|x)(\nabla_{x}p_{s}(x|x_{0}))^{\rm T}\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{d}x_{1}}{p_{s}^{\rm I}(x)}\right\|_{\rm op}^{4} p_{s}^{\rm I}(x)\mathrm{d}x\] \[+\int_{\mathbb{R}^{d}}\left\|\frac{\int_{\mathbb{R}^{2d}}p_{s}(x|x _{0})\nabla_{x}^{2}p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0} \mathrm{d}x_{1}}{p_{s}^{\rm I}(x)}\right\|_{\rm op}^{4}p_{s}^{\rm I}(x)\mathrm{ d}x\] \[+\int_{\mathbb{R}^{d}}\left\|\frac{\int_{\mathbb{R}^{2d}}p_{s}(x|x _{0})\nabla_{x}p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{ d}x_{1}}{p_{s}^{\rm I}(x)}\right\|^{8}p_{s}^{\rm I}(x)\mathrm{d}x\] \[+\int_{\mathbb{R}^{d}}\left\|\frac{\int_{\mathbb{R}^{2d}}\nabla_{ x}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x_{1})\mathrm{d}x_{0}\mathrm{ d}x_{1}}{p_{s}^{\rm I}(x)}\right\|^{8}p_{s}^{\rm I}(x)\mathrm{d}x\;.\]

To bound the first term of the RHS of the above expression, we integrate by parts and use Lemma 1 and **H2**.

\[\int_{\mathbb{R}^{d}}\left\|\frac{\int_{\mathbb{R}^{2d}}\nabla_{x} p_{1-s}(x_{1}|x)(\nabla_{x}p_{s}(x|x_{0}))^{\rm T}\tilde{\pi}(x_{0},x_{1}) \mathrm{d}x_{0}\mathrm{d}x_{1}}{p_{s}^{\rm I}(x)}\right\|_{\rm op}^{4}p_{s}^{ \rm I}(x)\mathrm{d}x\] \[=\int_{\mathbb{R}^{d}}\left\|\frac{1}{p_{s}^{\rm I}(x)}\int_{ \mathbb{R}^{2d}}\frac{\nabla_{x_{0}}\tilde{\pi}}{\tilde{\pi}}(x_{0},x_{1}) \frac{(x_{1}-x)^{\rm T}}{1-s}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\tilde{\pi}(x_{0},x _{1})\mathrm{d}x_{0}\mathrm{d}x_{1}\right\|_{\rm op}^{4}p_{s}^{\rm I}(x)\mathrm{ d}x\] \[\lesssim\mathbb{E}\Bigg{[}\left\|\frac{\nabla_{x_{0}}\tilde{\pi} }{\tilde{\pi}}(X_{0}^{\rm I},X_{1}^{\rm I})\frac{(X_{1}^{\rm I}-X_{s}^{\rm I} )^{\rm T}}{\epsilon}\right\|_{\rm op}^{4}\Bigg{]}\] \[\lesssim\frac{1}{\epsilon}(d^{4}+\mathbf{m}_{8}[\mu]+\mathbf{m}_{ 8}[\nu^{\star}])+\left\|\nabla\log\tilde{\pi}\right\|_{\mathrm{L}^{8}(\pi)}^{8}\;.\]

[MISSING_PAGE_EMPTY:33]

But then, using Jensen inequality we obtain that

\[\int_{\mathbb{R}^{d}}\left\|\nabla_{x_{1-\delta}}\log\frac{\mathrm{d }\nu_{1-\delta}^{\star}}{\mathrm{d}\mathrm{L}\mathrm{e}\mathrm{b}^{d}}\right\|^ {8}\mathrm{d}\nu_{1-\delta}^{\star} =\mathbb{E}\Bigg{[}\left\|\mathbb{E}\Bigg{[}\left\|\mathbb{E}\Bigg{[} \frac{X_{1-\delta}^{\mathrm{I}}-\delta X_{0}^{\mathrm{I}}+(1-\delta)X_{1}^{ \mathrm{I}}}{\delta(1-\delta)}\Bigg{|}X_{1-\delta}^{\mathrm{I}}\Bigg{]}\right\|^ {8}\Bigg{]}\right.\] \[\lesssim\mathbb{E}\Bigg{[}\left\|\frac{X_{1-\delta}^{\mathrm{I}}- \delta X_{0}^{\mathrm{I}}+(1-\delta)X_{1}^{\mathrm{I}}}{\delta(1-\delta)}\right\| ^{8}\Bigg{]}\] \[\lesssim\mathbf{m}_{8}[\nu_{1-\delta}^{\star}]\frac{1}{\delta^{8} (1-\delta)^{8}}+\mathbf{m}_{8}[\mu]\frac{1}{(1-\delta)^{8}}+\mathbf{m}_{8}[\nu ^{\star}]\frac{1}{\delta^{8}}\] \[\lesssim\mathbf{m}_{8}[\mu]\frac{1}{(1-\delta)^{8}}+\mathbf{m}_{8 }[\nu^{\star}]\frac{1}{\delta^{8}}+d^{4}\frac{1}{\delta^{4}(1-\delta)^{4}}\;.\]

Also, consider

\[\pi_{1-\delta}(x_{0},x_{1-\delta})=\mu(x_{0})\int_{\mathbb{R}^{d}}p_{1-\delta|0,1}^{\mathrm{I}}(x_{1-\delta}|x_{0},x_{1})\nu^{\star}(\mathrm{d}x_{1})\;,\]

where \((x_{0},x_{1},x_{1-\delta})\mapsto p_{1-\delta|0,1}^{\mathrm{I}}(x_{1-\delta}| x_{0},x_{1})\) denotes the density of \(X_{1-\delta}^{\mathrm{I}}\) given \((X_{0}^{\mathrm{I}},X_{1}^{\mathrm{I}})\) with respect to the Lebesgue measure. Then \(\pi_{1-\delta}\in\Pi(\mu,\nu_{1-\delta}^{\star})\) and \(\pi_{1-\delta}\ll\mathrm{Leb}^{2d}\). Moreover

\[\nabla\log\Big{(}\frac{1}{p_{1-\delta}}\frac{\mathrm{d}\pi_{1-\delta}}{\mathrm{ d}\mathrm{Leb}^{2d}}\Big{)}\in\mathrm{L}^{8}(\pi_{1-\delta})\;. \tag{45}\]

Indeed, because of (5),

\[p_{1-\delta|0,1}^{\mathrm{I}}(x_{1-\delta}|x_{0},x_{1})=\frac{1}{(4\pi\delta( 1-\delta))^{d/2}}\exp\Bigg{(}-\frac{\left\|x_{1-\delta}-\delta x_{0}-(1-\delta )x_{1}\right\|^{2}}{4\delta(1-\delta)}\Bigg{)}\;.\]

Therefore

\[\nabla_{x_{0}}p_{1-\delta|0,1}^{\mathrm{I}}(x_{1-\delta}|x_{0},x_{1})=\frac{x_{ 1-\delta}-\delta x_{0}-(1-\delta)x_{1}}{2(1-\delta)}p_{1-\delta|0,1}^{\mathrm{ I}}(x_{1-\delta}|x_{0},x_{1})\;,\]

and

\[\nabla_{x_{1-\delta}}p_{1-\delta|0,1}^{\mathrm{I}}(x_{1-\delta}|x_{0},x_{1})=- \frac{x_{1-\delta}-\delta x_{0}-(1-\delta)x_{1}}{2\delta(1-\delta)}p_{1-\delta |0,1}^{\mathrm{I}}(x_{1-\delta}|x_{0},x_{1})\;.\]

Furthermore

\[\frac{p_{1-\delta|0,1}^{\mathrm{I}}(x_{1-\delta}|x_{0},x_{1})\nu^ {\star}(\mathrm{d}x_{1})}{\int_{\mathbb{R}^{d}}p_{1-\delta|0,1}^{\mathrm{I}}(x _{1-\delta}|x_{0},\bar{x}_{1})\nu^{\star}(\mathrm{d}\bar{x}_{1})} =\frac{p_{1-\delta|0,1}^{\mathrm{I}}(x_{1-\delta}|x_{0},x_{1})\mu (x_{0})\nu^{\star}(\mathrm{d}x_{1})}{\int_{\mathbb{R}^{d}}p_{1-\delta|0,1}^{ \mathrm{I}}(x_{1-\delta}|x_{0},\bar{x}_{1})\mu(x_{0})\nu^{\star}(\mathrm{d}\bar {x}_{1})}\] \[=p_{1|0,1-\delta}^{\mathrm{I}}(x_{1}|x_{0},x_{1-\delta})\mathrm{d}x _{1}\;.\]

Consequently, we have that

\[\frac{\nabla_{x_{0}}\pi_{1-\delta}}{\pi_{1-\delta}}(x_{0},x_{1- \delta})\] \[=\frac{\nabla\mu(x_{0})\int_{\mathbb{R}^{d}}p_{1-\delta|0,1}^{ \mathrm{I}}(x_{1-\delta}|x_{0},x_{1})\nu^{\star}(\mathrm{d}x_{1})+\mu(x_{0}) \int_{\mathbb{R}^{d}}\nabla_{x_{0}}p_{1-\delta|0,1}^{\mathrm{I}}(x_{1-\delta}| x_{0},x_{1})\nu^{\star}(\mathrm{d}x_{1})}{\mu(x_{0})\int_{\mathbb{R}^{d}}p_{1-\delta|0,1}^{ \mathrm{I}}(x_{1-\delta}|x_{0},\bar{x}_{1})\nu^{\star}(\mathrm{d}\bar{x}_{1})}\] \[=\frac{\nabla\mu(x_{0})}{\mu(x_{0})}+\int_{\mathbb{R}^{d}}\frac{x_ {1-\delta}-\delta x_{0}-(1-\delta)x_{1}}{2(1-\delta)}p_{1|0,1-\delta}^{\mathrm{ I}}(x_{1}|x_{0},x_{1-\delta})\mathrm{d}x_{1}\;,\]

and that

\[\frac{\nabla_{x_{1-\delta}}\pi_{1-\delta}}{\pi_{1-\delta}}(x_{0},x_ {1-\delta}) =\frac{\mu(x_{0})\int_{\mathbb{R}^{d}}\nabla_{x_{1-\delta}}p_{1- \delta|0,1}^{\mathrm{I}}(x_{1-\delta}|x_{0},x_{1})\nu^{\star}(\mathrm{d}x_{1})}{ \mu(x_{0})\int_{\mathbb{R}^{d}}p_{1-\delta|0,1}^{\mathrm{I}}(x_{1-\delta}|x_{0},\bar{x}_{1})\nu^{\star}(\mathrm{d}\bar{x}_{1})}\] \[=-\int_{\mathbb{R}^{d}}\frac{x_{1-\delta}-\delta x_{0}-(1-\delta)x _{1}}{2\delta(1-\delta)}p_{1|0,1-\delta}^{\mathrm{I}}(x_{1}|x_{0},x_{1-\delta}) \mathrm{d}x_{1}\;.\]But then, if we use Jensen inequality and (43), we get

\[\int_{\mathbb{R}^{2d}}\left\|\nabla_{x_{0}}\log\frac{\mathrm{d}\pi_{ 1-\delta}}{\mathrm{d}\mathrm{L}\mathrm{e}^{\mathrm{D}^{2d}}}\right\|^{8}\mathrm{ d}\pi_{1-\delta} \lesssim\int_{\mathbb{R}^{d}}\left\|\nabla_{x_{0}}\log\frac{\mathrm{d} \mu}{\mathrm{d}\mathrm{L}\mathrm{e}^{\mathrm{D}^{d}}}\right\|^{8}\mathrm{d}\mu\] \[\quad+\mathbb{E}\Bigg{[}\left\|\mathbb{E}\Bigg{[}\frac{X_{1- \delta}^{1}-\delta X_{0}^{1}-(1-\delta)X_{1}^{1}}{1-\delta}\Bigg{|}(X_{0}^{1},X _{1-\delta}^{1})\right]\right\|^{8}\Bigg{]}\] \[\lesssim\left\|\nabla\log\mu\right\|_{\mathrm{L}^{8}(\mu)}^{8}+ \mathbb{E}\Bigg{[}\left\|\frac{X_{1-\delta}^{1}-\delta X_{0}^{1}-(1-\delta)X_{ 1}^{1}}{1-\delta}\right\|^{8}\Bigg{]}\] \[\lesssim\left\|\nabla\log\mu\right\|_{\mathrm{L}^{8}(\mu)}^{8}+ \mathbf{m}_{8}[\nu_{1-\delta}^{\star}]\frac{1}{(1-\delta)^{8}}+\mathbf{m}_{8} [\mu]\frac{\delta^{8}}{(1-\delta)^{8}}+\mathbf{m}_{8}[\nu^{\star}]\] \[\lesssim\left\|\nabla\log\mu\right\|_{\mathrm{L}^{8}(\mu)}^{8}+ \mathbf{m}_{8}[\mu]\frac{\delta^{8}}{(1-\delta)^{8}}+\mathbf{m}_{8}[\nu^{\star }]+d^{4}\frac{\delta^{4}}{(1-\delta)^{4}}\] \[\lesssim\left\|\nabla\log\mu\right\|_{\mathrm{L}^{8}(\mu)}^{8}+ \mathbf{m}_{8}[\mu]\frac{1}{(1-\delta)^{8}}+\mathbf{m}_{8}[\nu^{\star}]\frac{1 }{\delta^{8}}+d^{4}\frac{1}{\delta^{4}(1-\delta)^{4}}\,\]

and (similarly)

\[\int_{\mathbb{R}^{2d}}\left\|\nabla_{x_{1-\delta}}\log\frac{ \mathrm{d}\pi_{1-\delta}}{\mathrm{d}\mathrm{L}\mathrm{e}^{\mathrm{D}^{2d}}} \right\|^{8}\mathrm{d}\pi_{1-\delta} \lesssim\mathbb{E}\Bigg{[}\left\|\mathbb{E}\Bigg{[}\frac{X_{1- \delta}^{1}-\delta X_{0}^{1}-(1-\delta)X_{1}^{1}}{\delta(1-\delta)}\Bigg{|}(X _{0}^{1},X_{1-\delta}^{1})\right]\right\|^{8}\Bigg{]}\] \[\lesssim\mathbf{m}_{8}[\mu]\frac{1}{(1-\delta)^{8}}+\mathbf{m}_{ 8}[\nu^{\star}]\frac{1}{\delta^{8}}+d^{4}\frac{1}{\delta^{4}(1-\delta)^{4}}\.\]

Additionally, because of Remark 8 and (43), they hold

\[\int_{\mathbb{R}^{2d}}\left\|\nabla_{x_{0}}\log p_{1-\delta}(x_{1 -\delta}|x_{0})\right\|^{8}\mathrm{d}\pi_{1-\delta}(x_{0},x_{1-\delta})\] \[=\int_{\mathbb{R}^{2d}}\left\|\nabla_{x_{1-\delta}}\log p_{1- \delta}(x_{1-\delta}|x_{0})\right\|^{8}\mathrm{d}\pi_{1-\delta}(x_{0},x_{1- \delta})\] \[=\mathbb{E}\Bigg{[}\left\|\frac{X_{1-\delta}^{1}-X_{0}^{1}}{1- \delta}\right\|^{

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims made in the abstract and introduction make clear the goals attained by the paper and match the results therein provided. Additionally, a discussion on the contributions made by the paper and the comparison with the literature is held. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper discusses the limitations of the work performed in Section 4. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The paper provides the full set of assumptions for either Theorem 2 and Theorem 3, see Section 3.1. Moreover, it includes the detailed proofs of all the stated theorems in Appendix A and a sketch of the proof of our main contributions Theorem 2, Theorem 3 in Section 3.3. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.

**Experimental Result Reproducibility**

Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

Answer: [NA] Justification: Since this work is of theoretical nature, we do not provide experimental data, hence our answer. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: Since this work is of theoretical nature, we do not provide experimental data, hence our answer. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: Since this work is of theoretical nature, we do not provide experimental data, hence our answer. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Since this work is of theoretical nature, we do not provide experimental data, hence our answer. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Since this work is of theoretical nature, we do not provide experimental data, hence our answer. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We confirm that the research conducted our paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Since this work is of theoretical nature, it does not present societal impacts up to our knowledge. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Since this work is of theoretical nature, it does not present such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: In the present work, we do not use any assets to be credited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: In the present work, we do not introduce new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The present work does not include any experiment involving human subjects, hence our answer. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The present work does not include any experiment involving human subjects, hence our answer. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.