[MISSING_PAGE_FAIL:1]

### Learning from Vision-Language Representations

Prior research has shown that vision-language representations such as embeddings from contrastive language-image pretraining (CLIP) [10] can be used to identify novelty of an image relative to a set (and, as a bonus, can be decoded into a verbal explanation of novelty) [11]. In our research, we utilize this representation and corresponding ability to select novel images as a proxy for the amount of useful, previously-unexplored information within a complete multimodal driving scene, allowing for an active learning query to select diverse samples based on vision-language encodings of scene images.

## 3 Algorithm

Here, we present our algorithm named Vision-Language Embedding Diversity Querying (VisLED-Querying), which can be viewed in Figure 1. The algorithm can be used in two different settings:

1. Open-World Exploring: this method imposes no particular class expectations on the data. It is suitable for cases when the model seeks to include information which is most novel relative to data it has seen previously.
2. Closed-World Mining: this method utilizes a zero-shot learning [10] step to sort data between a fixed set of classes before evaluating for novelty, filtering any points estimated to not belong to one of the closed-set classes. This method is suitable for mining new and different instances of existing classes, but may also filter out the most difficult or unusual instances even from known classes if the zero-shot method fails to recognize the object.
3. Open-World Exploring VisLED-Querying
4. Unlabeled pool of egocentric driving scene images
5. Updated training set
6. Embed each egocentric driving scene image from the unlabeled pool using CLIP;
7. Use hierarchical clustering to separate the embeddings;
8. Sample new data points from the unclustered set for addition to the training set;
9. When employing CLIP's [12] zero-shot learning technique for classification, the algorithm examines each sample image to identify objects, that are most likely to belong to predefined classes. As a result, each sample is assigned to a single class, as the zero-shot learning method predominantly identifies one class with high accuracy. In instances where other classes may also be identified, their confidence scores are typically low enough to risk false positives, rendering them inadequate for threshold-based classification. Therefore, a single-class assignment is favored for simplicity and accuracy.

Once the samples for each class have been identified, embeddings will be generated separately for each class, followed by hierarchical clustering. Subsequently, a number of samples will be selected from each class, with a focus on sampling from clusters with minimal data representation. Initially, the algorithm will prioritize unique samples (clusters with only one sample present), matching them with corresponding scene names until the desired number of unique scenes is achieved in the training set. Upon inclusion of all scene-names from unique samples, the algorithm will proceed to clusters containing pairs of images, and so on, until the required number of scenes have been sampled for the training set.

```
Input: Unlabeled pool of egocentric driving scene images Output: Updated training set Embed each egocentric driving scene image from the unlabeled pool using CLIP;
10. Encode each class label using a text encoding;
11. Applying zero-shot learning by maximizing the product of the embeddings, sort the embedded images by class;
12. For each class, apply hierarchical clustering;
13. Sample new data points from the unclustered set associated with the desired class, and add to the training set;
14.
```

**Algorithm 2**Closed-World Mining VisLED-Querying

## 4 Experimental Evaluation

### Dataset

We use the nuScenes object detection dataset [13] for our experiments. nuScenes contains 1.4M camera images and 400k LIDAR sweeps of driving data, originally labeled by expert annotators from an annotation partner. 1.4M objects are labeled with a 3D bounding box, semantic category (among 23 classes), and additional attributes. NuScenes comprises 1000 scenes. In order to maintain complete control over the scenes within the dataset, we modify the fundamental database setup slightly, using the method introduced in [14, 15] to accommodate active learning queries. We use the _trainval_ split of the dataset for public reproducibility.

### 3D Object Detection Model

We explore the BEVFusion approach to 3D object detection [16], which has demonstrated notable performance, ranking third in the NuScenes tracking challenge and seventh in the detection challenge. While various methods exist to integrate image and LiDAR data into a unified representation, LiDAR-to-Camera projection methods often introduce geometric distortions, and Camera-to-LiDAR projections face challenges in semantic-orientation tasks. BEVFusion aims to address these issues by creating a unified representation that preserves both geometric structure and semantic density.

In our implementation, we utilize the Swin-Transformer [17] as the image backbone and VoxelNet [18] as the LiDAR backbone. To generate bird's-eye-view (BEV) features for images, we employ a Feature Pyramid Network (FPN) [19] to fuse multi-scale camera features, resulting in a feature map one-eighth of the original size. Subsequently, images are down-sampled to 256x704 pixels, and LiDAR point clouds are voxelized to 0.075 meters to obtain the BEV features necessary for object detection. These modalities are integrated using a convolution-based BEV encoder to mitigate local misalignment between LiDAR-BEV and camera-BEV features, particularly in scenarios of depth estimation uncertainty from the camera mode. For a comprehensive overview of the architecture, including its integration with VisLED-Querying, refer to Figure 1.

### Experiments

We train the BEVFusion model in increasing training set sizes, using three different acquisition modes: (1) Random Sampling, (2) Entropy-Querying, and (3) VisLED-Querying with Closed-Set Mining setting. As expected, active learning strategies outperform the random baseline, and the entropy-querying method is dominant due to its nature of optimizing uncertainty with respect to the model, as opposed to VisLED's model-agnostic sampling. Yet, as illustrated in Table 1, VisLED still stays consistently ahead of random sampling, and offers a 1% gain over random sampling mAP at 50% of the data pool, all without requiring _any_ model training or inference.

## 5 Discussion and Conclusion

Our presented learning method, VisLED-Querying, samples without any information about the model. This enables VisLED to select novel, informative data points, to the extent that novelty is visibly identifiable, for _any_ model. The benefit this offers is that a data point may need to be annotated only once, and can then be used in a variety of models for additional autonomous driving tasks instead of

Figure 1: VisLED System Overview. For both Open-World Exploring and Closed-World Mining, the system begins with the processing of the unlabeled data pool into vision-language embedding representations. In Open-World Exploring, these embeddings are clustered and used as the basis for a query. In Closed-World Mining, the embeddings are first used in zero-shot learning to classify scenes based on object appearance, and then further clustered per-class, offering a chance to sample from particular classes which are known to be minority in the labeled training set.

sampling and possibly forming an entirely different set for annotation. While these gains may be marginal in the current data setting (\(<\) 1000 scenes), at scale, these performance gains may translate to serious reductions in annotation costs and safety-critical detection failures. Further, VisLED offers one key possibility that is otherwise limited on uncertainty-driven approaches: VisLED will recommend unique samples without any prior assumptions on class taxonomy, making it especially suited to open-set learning, where new classes may be introduced at any time. This capability, when paired with methods of self- or semi-supervised learning for object detection by fusing LiDAR and camera [20], may prove especially beneficial in identifying and learning from novel encounters. In future research, we plan to experiment on the effectiveness of VisLED in multi-task learning settings [21], experiments on other benchmark datasets [22], and experiments in open-set and continual learning.

## References

* [1] Valerie Chen, Man-Ki Yoon, and Zhong Shao. Task-aware novelty detection for visual-based deep learning in autonomous systems. In _2020 IEEE International Conference on Robotics and Automation (ICRA)_, pages 11060-11066. IEEE, 2020.
* [2] Walter J Scheirer, Anderson de Rezende Rocha, Archana Sapkota, and Terrance E Boult. Toward open set recognition. _IEEE transactions on pattern analysis and machine intelligence_, 35(7):1757-1772, 2012.
* [3] Ross Greer, Jason Isa, Nachiket Deo, Akshay Rangesh, and Mohan M Trivedi. On salience-sensitive sign classification in autonomous vehicle path planning: Experimental explorations with a novel dataset. In _Proceedings of the IEEE/CVF Winter Conference

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline \multicolumn{2}{c|}{Labeled Pool} & \multicolumn{2}{c|}{mAP} & \multicolumn{2}{c}{NDS} & 378 \\ \hline \hline Rounds & \% & Random & Entropy & VisLED & Random & Entropy & VisLED \\ \hline \hline
1 & 10\% & 30.95 & 31.06 (+1.06) & 29.14 (-1.81) & 33.53 & 34.09 (+0.56) & 32.16 (+1.37) \\ \hline
2 & 20\% & 38.00 & 40.41 (\(\pm\)2.41) & 40.76 (\(\pm\)2.76) & 40.14 & 41.85 (+1.71) & 41.18 (+1.94) \\ \hline
3 & 30\% & 44.94 & 45.57 (+0.63) & 45.01 (+0.07) & 48.41 & 50.11 (+1.7) & 49.40 (+0.99) \\ \hline
4 & 40\% & 47.73 & 49.24 (+1.51) & 49.21 (+1.48) & 53.10 & 53.80 (+0.7) & 53.64 (+0.51) \\ \hline
5 & 50\% & **49.90** & **63.88** (+13.98) & **51.05** (+1.15) & **55.64** (**64.85** (+9.21) & **56.45** (+0.81) \\ \hline \hline  & 100\% & \multicolumn{2}{c|}{**52.88**} & \multicolumn{2}{c|}{**58.73**} \\ \hline \hline \end{tabular}
\end{table}
Table 1: This table shows the mean average precision (mAP) and nuScenes driving score (NDS) metrics for the random sampling, entropy-querying, and VisLED-querying (Closed-World Mining) in every round. It also shows the mAP and NDS scores for the full training split when trained using one GPU. Both the entropy-querying and VisLED methods outperform random sampling consistently, and reach nearly the same level of performance as 100% of the data at just the 50% data point, showing faster learning than the baseline method.

Figure 2: Performance of BEVFusion in 3D Object Detection on nuScenes at different training set sizes, using three different learning strategies. Simultaneously, we chart the learning of BEVFusion on the full training set, over the course of six epochs (top horizontal axis) to give an impression of the asymptotic performance limit that may be expected of the model. We observe that the active learning methods move towards this asymptote sooner than random sampling, and that VisLED maintains a margin over random sampling throughout.

on Applications of Computer Vision_, pages 636-644, 2022.
* [43] Eshed Ohn-Bar and Mohan M Trivedi. What makes an on-road object important? In _2016 23rd International Conference on Pattern Recognition (ICPR)_, pages 3392-3397. IEEE, 2016.
* [44] Ross Greer, Akshay Gopalkrishnan, Nachiket Deo, Akshay Rangesh, and Mohan Trivedi. Salient sign detection in safe autonomous driving: Ai which reasons over full visual context. In _27th International Technical Conference on the Enhanced Safety of Vehicles (ESV) National Highway Traffic Safety Administration_, number 23-0333, 2023.
* [45] Ross Greer, Akshay Gopalkrishnan, Jacob Landgren, Lulua Rakla, Anish Gopalan, and Mohan Trivedi. Robust traffic light detection using salience-sensitive loss: Computational framework and evaluations. In _2023 IEEE Intelligent Vehicles Symposium (IV)_, pages 1-7. IEEE, 2023.
* [46] Aseem Behl, Kashyap Chitta, Aditya Prakash, Eshed Ohn-Bar, and Andreas Geiger. Label efficient visual abstractions for autonomous driving. In _2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 2338-2345. IEEE, 2020.
* [47] N Kulkarni, A Rangesh, J Buck, J Feltracco, M Trivedi, N Deo, R Greer, S Sarraf, and S Sathyanarayana. Create a large-scale video driving dataset with detailed attributes using amazon sagemaker ground truth. 2021.
* [48] Sanjoy Dasgupta. Two faces of active learning. _Theoretical computer science_, 412(19):1767-1781, 2011.
* [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sasstr, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [50] Ross Greer and Mohan Trivedi. Towards explainable, safe autonomous driving with language embeddings for novelty identification and active learning: Framework and experimental analysis with real-world data sets. _arXiv preprint arXiv:2402.07320_, 2024.
* [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sasstr, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. _International Conference on Machine Learning_, 2021.
* [52] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11621-11631, 2020.
* [53] Ahmed Ghita, Bjork Antoniussen, Walter Zimmer, Ross Greer, Christian Cress, Andreas Mogelmose, Mohan M Trivedi, and Alois C Knoll. Activeanno3d-an active learning framework for multi-modal 3d object detection. _arXiv preprint arXiv:2402.03235_, 2024.
* [54] Ross Greer, Bjork Antoniussen, Mathias V Andersen, Andreas Mogelmose, and Mohan M Trivedi. The why, when, and how to use active learning in large-data-driven 3d object detection for safe autonomous driving: An empirical exploration. _arXiv preprint arXiv:2401.16634_, 2024.
* [55] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L Rus, and Song Han. Bevfusion: Multi-task multi-sensor fusion with unified bird's-eye view representation. In _2023 IEEE international conference on robotics and automation (ICRA)_, pages 2774-2781. IEEE, 2023.
* [56] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted window. _ICCV_, 2021.
* [57] Yan Yan, Yuxing Mao,, and Bo Li. Second: Sparsely embedded convolutional detection. _Sensors_, 2018.
* [58] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detectio. _CVPR_, 2017.
* [59] Aral Hekimoglu, Michael Schmidt, and Alvaro Marcos-Ramiro. Monocular 3d object detection with lidar guided semi supervised active learning. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 2346-2355, 2024.
* [60] Aral Hekimoglu, Philipp Friedrich, Walter Zimmer, Michael Schmidt, Alvaro Marcos-Ramiro, and Alois Knoll. Multi-task consistency for active learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3415-3424, 2023.

* [22] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [23] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [24] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [25] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [26] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [27] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [28] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [29] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [30] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [31] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [32] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [33] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [34] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [35] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [36] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [37] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [38] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [39] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [40] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [41] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [42] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [43] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [44] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [45] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [46] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [47] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [48] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [49] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [50] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [51] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [52] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [53] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [54] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [55] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [56] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [57] Walter Zimmer, Christian Cress, Huu Tung Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [58] Walter Zimmer, Christian Cress, Huu Tung Nguyen Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)_, pages 1030-1037. IEEE, 2023.
* [59] Walter Zimmer, Christian Cress, Huu Tung Nguyen Nguyen, and Alois C Knoll. Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception. In _2023 IEEE 26th International Conference on Intelligent