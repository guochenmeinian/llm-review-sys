# RepMedSAM: Segment Anything in Medical Images with Lightweight CNN

Zehan Zhang0000-0002-0451-985X

1Hangzhou Genlight Medtech Co. Ltd., Zhejiang, China

1

Rui Huang0

1Hangzhou Genlight Medtech Co. Ltd., Zhejiang, China

1

Ning Huang0

1Hangzhou Genlight Medtech Co. Ltd., Zhejiang, China

1

Footnote 1: [https://www.face.com/](https://www.face.com/)

Footnote 2: [https://www.face.com/](https://www.face.com/)

###### Abstract

Traditional deep learning segmentation models require designing network structures and loss functions specific to different tasks, followed by training dedicated models, which leads to a significant amount of repetitive work. The Segment Anything Model (SAM) provides a unified framework for handling segmentation tasks. However, the current SAM model is mainly applicable to natural images and may require substantial computational resources during inference, posing challenges for widespread clinical implementation. In this work, we utilize RepViT as the Image Encoder to develop a lightweight SAM structure. The training phase consists of two main parts: knowledge distillation and fine-tuning. During the inference phase, reparameterization is employed to optimize inference speed. The proposed method achieves an average DSC of 0.8688 and an average NSD of 0.8746 on the validation set, and it improves inference speed while increasing the number of parameters compared to the baseline.

Keywords:Segment Anything Model Medical image Lightweight Knowledge distillation.

## 1 Introduction

Medical image segmentation is a crucial component in clinical practice, where the accuracy of segmentation results is essential for ensuring the safety and efficacy of medical diagnosis and treatment. Existing methods typically design network structures and loss functions tailored to specific tasks and train dedicated models, resulting in poor generalization performance across different segmentation tasks and modalities.

Recently, various foundational models have garnered significant attention from researchers in the field of computer science due to their outstanding performance. In natural images, the Segment Anything Model (SAM) [1] performs interactive segmentation using prompts such as points, boxes, masks, and text. The introduction of such prompt engineering enables SAM to adapt to nearly all downstream segmentation tasks, achieving impressive results comparable to models specifically trained for particular tasks. However, unlike natural images, medical images comprise multiple modalities with significant differences betweenthem. Therefore, the performance of the foundational SAM model in medical image segmentation is quite limited. Works like MedSAM [3] have successfully transferred SAM to the medical image segmentation domain, achieving excellent performance. Nonetheless, these models generally require substantial computational resources during inference, and their inference speed does not meet the real-time requirements of clinical applications. Hence, developing a lightweight SAM for efficient deployment in medical image segmentation is a meaningful and promising research area.

Currently, many lightweight SAM models have emerged in natural images to enable these applications to run on resource-constrained terminal devices. Mobile-SAM [6] replaces SAM's image encoder with a lighter structure and completes training within a day using a decoupled distillation approach. Compared to SAM, MobileSAM achieves comparable performance with 60 times fewer parameters and can run stably on a CPU. EfficientViT-SAM [6] proposes a pre-training framework called SAMI with masked images, which significantly enhances the performance of image mask pre-training methods and extends well to various tasks such as image classification, object detection, and semantic segmentation. Compared to the original SAM, EfficientSAM reduces the number of parameters by 20 times, accelerates the running speed by 20 times, and surpasses models like MobileSAM and FastSAM [7] in performance. RepViT [4], which incorporates re-parameterized convolutions into the MobileNetV3 [2] architecture, forms a lightweight CNN resembling a ViT structure. When used as the image encoder and combined with SAM, it achieves faster and better results than MobileSAM.

In this paper, we build upon RepViT and SAM, implementing several minor architectural and preprocessing adjustments to significantly reduce the overall model parameters and computational load. The model undergoes training in two stages: distillation of the image encoder and overall fine-tuning. During the inference phase, we leverage structural re-parameterization to further reduce the number of parameters and enhance speed, all while maintaining accuracy.

## 2 Method

We propose a lightweight foundational model for medical image segmentation based on SAM and RepViT. The details of the proposed method are described as follows.

### Preprocessing

We primarily follow the baseline data processing approach for three-channel 2D data. The inference phase consists of the following steps:

* Resize the longest edge to 256 while maintaining the aspect ratio of the image.
* Normalize the image pixel value to the [0, 1] range using Max-Min Normalization.

* Pad the image to [256; 256].
* Align the coordinates of the box with the resized image.

For 3D data preprocessing during the inference phase, we perform slice-by-slice operations for each box. Each slice is expanded to three channels, and the data preprocessing method then follows the same procedure as described above for 2D data.

During the training phase, to avoid excessive disk usage, we directly read npz files for training. For 3D data, one random slice in the stack is read per iteration. To utilize more slices in the same stack, one 3D data is read multiple times during each training epoch.

### Proposed Method

The overall architecture of the proposed method is identical to MedSAM, as illustrated in Figure 1. It mainly consists of three parts: the Image Encoder, the Mask Decoder, and the Prompt Encoder. For each box, the method predicts the corresponding mask individually.

We replace the Image Encoder with the lightweight RepViT model, the overall structure of which is shown in Figure 2. This structure comprises multiple stacked RepViTBlocks.

The structure of a single RepViTBlock is shown in Figure 3. Figure 3(a) depicts the structure of the MobileNet Block. The RepViT Block is an improved version based on this structure. Its architecture during training and inference is illustrated in Figure 3(b).

Additionally, the training of RepSAM is divided into two stages. The first stage involves knowledge distillation for the Image Encoder. In this stage, instead of using a larger parameter Image Encoder as the teacher model, we directly use the pre-trained TinyViT. Despite the smaller parameter count of the teacher

Figure 1: The network architecture of MedSAM.

model, the RepViT obtained by distillation learning achieves better results compared to training from scratch. The second stage involves fine-tuning the overall structure of RepSAM.

Loss function: We employ KLDivLoss as the loss function for the first stage of knowledge distillation. During the fine-tuning stage, the loss function is a combination of DiceLoss, BCELoss, and MSELoss.

### Post-processing

We maintain the same approach as the baseline by post-processing the predicted masks through cropping and resizing to align the results with the input images.

## 3 Experiments

### Dataset and evaluation measures

We use the dataset provided by the challenge, along with an additional public dataset, ToothSeg, for model training. Furthermore, during the distillation stage of the Image Encoder, we also include the validation set provided by the challenge in the training process. Throughout training, we maintain the data format as.npz. For 3D data, a random slice is read in each iteration, and during each epoch, each 3D dataset is traversed multiple times to read and train on multiple slices from the same data.

Figure 3: The structure of a single RepViT Block

Figure 2: The network architecture of RepViT.

The evaluation metrics include two accuracy measures--Dice Similarity Coefficient (DSC) and Normalized Surface Dice (NSD)--alongside one efficiency measure--running time. These metrics collectively contribute to the ranking computation.

### Implementation details

#### 3.2.1 Environment settings

The development environments and requirements are presented in Table 1.

#### 3.2.2 Training protocols

The training process is divided into two stages. The first stage involves the distillation learning of the Image Encoder. The training protocol for this stage is shown in Table 2. In this stage, RepViT learns relevant knowledge from the pre-trained TinyViT and encodes the images. We use KLD-Loss as the loss function. Through knowledge distillation, RepViT can quickly converge within 10 epochs, and we select the best distillation model based on the loss value.

The second stage involves fine-tuning the entire RepSAM. First, we load the pre-trained weights for the three components: the Image Encoder loads the

\begin{table}
\begin{tabular}{l l} \hline System & Ubuntu 18.04.6 LTS \\ \hline CPU & Intel(R) Core(TM) i9-10900K CPU @ 3.70GHz \\ \hline RAM & 64GB \\ \hline GPU (number and type) & One NVIDIA GeForce RTX 3090 24G \\ \hline CUDA version & 11.8 \\ \hline Programming language & Python 3.9 \\ \hline Deep learning framework PyTorch(torch 2.1.2, torchvision 0.16.2) \\ \hline Code \\ \hline \end{tabular}
\end{table}
Table 1: Development environments and requirements. (mandatory table)

\begin{table}
\begin{tabular}{l l} \hline Pre-trained Model & MedSAM [3] \\ \hline Batch size & 4 \\ \hline Patch size & 256\(\times\)256\(\times\)3 \\ \hline Total epochs & 10 \\ \hline Optimizer & AdamW \\ \hline Initial learning rate (lr) & 1e-4 \\ \hline Lr decay schedule & CosineAnnealingLR \\ \hline Loss function & KLDivLoss \\ \hline \end{tabular}
\end{table}
Table 2: Training protocols. (mandatory table)weights from the knowledge-distilled RepViT, and the Prompt Encoder and Mask Decoder directly load the provided baseline weights. The remaining training protocol details are shown in Table 3, with model parameters and computational load only calculated for the Image Encoder. During the training phase, we apply a random jitter of 5 pixels to the input boxes and randomly flip the images for augmentation. Additionally, we employ the EMA (Exponential Moving Average) strategy to enhance the model's robustness. The data is split into training and validation sets in a 4:1 ratio. The optimal model is selected based on the dice metric evaluated on the validation set.

## 4 Results and discussion

### Quantitative results on validation set

We compare our method with the baseline, MedSAM, and other models. The quantitative evaluation results are shown in Table 4, listing the DSC and NSD for nine modalities, along with the average DSC and NSD across all modalities. Our proposed method achieves an average Dice of 0.8688 and an average NSD of 0.8746 on the validation set. Additionally, we compare our method with RepMedSAM trained from scratch. The table shows that RepMedSAM trained from scratch already achieves results surpassing both the baseline and the larger MedSAM. Moreover, incorporating knowledge distillation in the initial phase significantly enhances the final segmentation results.

The proposed method achieves good segmentation results across various modalities, including CT, MR, US, X-Ray, Dermatology, Endoscopy, and Fundus. However, its performance is relatively poorer on PET and Microscopy, possibly due to the data quality and the inherent difficulty of these segmentation

\begin{table}
\begin{tabular}{p{113.8pt} p{113.8pt}} \hline Pre-trained Model & MedSAM [3] RepViT [4] \\ \hline Batch size & 4 \\ \hline Patch size & 256\(\times\)256\(\times\)3 \\ \hline Total epochs & 30 \\ \hline Optimizer & AdamW \\ \hline Initial learning rate (lr) & 2e-5 \\ \hline Lr decay schedule & CosineAnnealingLR \\ \hline Training time & 61 hours \\ \hline Loss function & DiceLoss + BCELoss + MSELoss \\ \hline Number of model parameters & 23.16M\({}^{1}\) 23.04M(inference) \\ \hline Number of flops & 7.23G\({}^{2}\) 7.08G(inference) \\ \hline CO\({}_{2}\)eq & 16.8Kg\({}^{3}\) \\ \hline \end{tabular}
\end{table}
Table 3: Training protocols. (mandatory table)

[MISSING_PAGE_FAIL:7]

Figure 4: Comparison between segmentation results of different methods.

Figure 5: Segmentation Results for Specific Modalities.

on the validation set. Including more datasets for training could enhance the model's generalization ability further. Additionally, we attempted to use the ONNX engine for inference, but the runtime did not show significant reduction. Further work on deployment could focus on reducing the required inference time.

## 5 Conclusion

We combine the lightweight CNN structure, RepViT, with SAM and apply it to medical image segmentation. Through training in two stages, knowledge distillation and fine-tuning, RepMedSAM achieves higher segmentation accuracy and faster segmentation speed compared to the baseline.

#### Acknowledgements

We thank all the data owners for making the medical images publicly available and CodaLab [5] for hosting the challenge platform.

## References

* [1] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W. Y. Lo, P. Dollar, R. Girshick, and R. Segment anything. In: Proceedings of the International Conference on Computer Vision. pp. 4015-4026 (2023)
* [2] Koonce, B., Koonce, B.: Mobilenetv3. Convolutional Neural Networks with Swift for Tensorflow: Image Recognition and Dataset Categorization pp. 125-144 (2021)
* [3] Ma, J., He, Y., Li, F., Han, L., You, C., Wang, B.: Segment anything in medical images. Nature Communications **15**(1), 654 (2024)

\begin{table}
\begin{tabular}{l c c c c c} \hline Case ID & Size & Num. Objects & Baseline w/o rep & RepMedSAM \\ \hline
3DBox\_CT\_0566 & (287, 512, 512) & 6 & 296.77 & 241.14 & 170.87 \\
3DBox\_CT\_0888 & (237, 512, 512) & 6 & 79.30 & 64.61 & 45.88 \\
3DBox\_CT\_0860 & (246, 512, 512) & 1 & 10.95 & 8.98 & 6.48 \\
3DBox\_MR\_0621 & (115, 400, 400) & 6 & 124.53 & 101.81 & 72.35 \\
3DBox\_MR\_0121 & (64, 290, 320) & 6 & 80.05 & 65.18 & 45.53 \\
3DBox\_MR\_0179 & (84, 512, 512) & 1 & 10.62 & 8.80 & 6.28 \\
3DBox\_PET\_0001 & (264, 200, 200) & 1 & 6.69 & 5.48 & 3.98 \\
2DBox\_US\_0525 & (256, 256, 3) & 1 & 0.54 & 0.44 & 0.32 \\
2DBox\_X-Ray\_0053 & (320, 640, 3) & 34 & 1.47 & 1.32 & 1.17 \\
2DBox\_Dermoscopy\_0003 & (3024, 4032, 3) & 1 & 0.80 & 0.70 & 0.52 \\
2DBox\_Endoscopy\_0086 & (480, 560, 3) & 1 & 0.55 & 0.44 & 0.32 \\
2DBox\_Fundus\_0003 & (2048, 2048, 3) & 1 & 0.60 & 0.48 & 0.36 \\
2DBox\_Microscope\_0008 & (1536, 2040, 3) & 19 & 1.15 & 1.04 & 0.90 \\
2DBox\_Microscope\_0016 & (1920, 2560, 3) & 241 & 8.55 & 8.44 & 8.39 \\ \hline \end{tabular}
\end{table}
Table 5: Quantitative evaluation of segmentation efficiency in terms of running time (s).

* [4] Wang, A., Chen, H., Lin, Z., Pu, H., Ding, G.: Repvit: Revisiting mobile cnn from vit perspective. arXiv preprint arXiv:2307.09283 (2023)
* [5] Xu, Z., Escalera, S., Pavao, A., Richard, M., Tu, W.W., Yao, Q., Zhao, H., Guyon, I.: Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform. Patterns **3**(7), 100543 (2022)
* [6] Zhang, C., Han, D., Qiao, Y., Kim, J.U., Bae, S.H., Lee, S., Hong, C.S.: Faster segment anything: Towards lightweight sam for mobile applications. arXiv preprint arXiv:2306.14289 (2023)
* [7] Zhao, X., Ding, W., An, Y., Du, Y., Yu, T., Li, M., Tang, M., Wang, J.: Fast segment anything. arXiv preprint arXiv:2306.12156 (2023)

\begin{table}
\begin{tabular}{l l} \hline Requirements & Answer \\ \hline A meaningful title & Yes \\ \hline The number of authors (\(\leq\)6) & 3 \\ \hline Author affiliations and ORCID & Yes \\ \hline Corresponding author email is presented & Yes \\ \hline Validation scores are presented in the abstract & Yes \\ \hline Introduction includes at least three parts: & Yes \\ background, related work, and motivation & \\ \hline A pipeline/network figure is provided & Figure 2 \\ \hline Pre-processing & Page 2 \\ \hline Strategies to data augmentation & Page 3 \\ \hline Strategies to improve model inference & Page 3 \\ \hline Post-processing & Page 4 \\ \hline Environment setting table is provided & Table 1 \\ \hline Training protocol table is provided & Table 2 3 \\ \hline Ablation study & Page 7 \\ \hline Efficiency evaluation results are provided & Table 5 \\ \hline Visualized segmentation example is provided & Figure 4 5 \\ \hline Limitation and future work are presented & Yes \\ \hline Reference format is consistent. & Yes \\ \hline Main text \textgreater{}= 8 pages (not include references and appendix) & Yes \\ \hline \end{tabular}
\end{table}
Table 6: Checklist Table. Please fill out this checklist table in the answer column.