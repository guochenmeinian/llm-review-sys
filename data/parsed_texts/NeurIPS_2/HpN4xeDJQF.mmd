# Beyond Single Stationary Policies: Meta-Task Players as Naturally Superior Collaborators

Haoming Wang

Equal contribution. MOE KLINNS Lab, Xi'an Jiaotong University, Xi'an, Shaanxi, China

Zhaoming Tian

Equal contribution. MOE KLINNS Lab, Xi'an Jiaotong University, Xi'an, Shaanxi, China

Yunpeng Song

MOE KLINNS Lab, Xi'an Jiaotong University, Xi'an, Shaanxi, China

Xiangliang Zhang

Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA

{wanghm,tzm9802}@stu.xjtu.edu.cn, yunpengs@xjtu.edu.cn, xzhang33@nd.edu, zmcai@sei.xjtu.edu.cn

Zhongmin Cai

Corresponding author. MOE KLINNS Lab, Xi'an Jiaotong University, Xi'an, Shaanxi, China

###### Abstract

In human-AI collaborative tasks, the distribution of human behavior, influenced by mental models, is non-stationary, manifesting in various levels of initiative and different collaborative strategies. A significant challenge in human-AI collaboration is determining how to collaborate effectively with humans exhibiting non-stationary dynamics. Current collaborative agents involve initially running self-play (SP) multiple times to build a policy pool, followed by training the final adaptive policy against this pool. These agents themselves are a single policy network, which is **insufficient for handling non-stationary human dynamics**. We discern that despite the inherent diversity in human behaviors, the **underlying meta-tasks within specific collaborative contexts tend to be strikingly similar**. Accordingly, we propose **C**ollaborative **B**aysian **P**olicy **R**euse (**CBPR1**), a novel Bayesian-based framework that **adaptively selects optimal collaborative policies matching the current meta-task from multiple policy networks** instead of just selecting actions relying on a single policy network. We provide theoretical guarantees for CBPR's rapid convergence to the optimal policy once human partners alter their policies. This framework shifts from directly modeling human behavior to identifying various meta-tasks that support human decision-making and training meta-task playing (MTP) agents tailored to enhance collaboration. Our method undergoes rigorous testing in a well-recognized collaborative cooking simulator, _Overcooked_. Both empirical results and user studies demonstrate CBPR's superior competitiveness compared to existing baselines.

Footnote 1: We make our code publicly available [https://github.com/AlexWanghaoming/CBPR](https://github.com/AlexWanghaoming/CBPR)

## 1 Introduction

An ongoing challenge in artificial intelligence (AI) involves training agents capable of effective collaboration with humans Klien et al. (2004); Bard et al. (2020); Dafoe et al. (2020). Unlike typical AI-only multi-agent collaboration, human-AI collaborative scenarios such as two-player cooking games, autonomous driving, and managing power grid stability incorporates a non-stationary component, humans Jagerman et al. (2019); Chandak et al. (2020); Chandak (2022). As humans may vary in their level of initiative, alter their collaboration strategies, or sometimes even do not collaborate at all. This variability suggests that for cooperative agents, the probability distribution \(P(A|s_{t})\) of a human action \(A\) given an environmental state \(s_{t}\) changes over time, reflecting differentmental states. Such non-stationarity poses a significant challenge in training collaborative agents, as it requires strategies that can adapt to the unpredictable nature of human behavior, which departs from the stable action-outcome associations expected in scenarios dominated by AI.

Recent works mainly develop collaborative agents through two workflows: (1) explicitly model human behavior by using real human trajectories Carroll et al. (2019), and then train a collaborator by teaming up with human models. (2) train Self-Play (SP) agents to form a policy pool (a diverse set of AI agents assumed to encompass all potential human policies) and then train a collaborator pairing with policies in the policy pool Strouse et al. (2021); Yu et al. (2023); Zhao et al. (2023). However, despite their ability to achieve commendable performance by amassing extensive human data collection or SP agent training, these collaborators share a common fundamental flaw: _they are essentially policy networks following a stationary distribution, thus making it difficult to cope with non-stationary human dynamics._

In this work, we propose Collaborative Bayesian Policy Reuse (CBPR), which reuses multiple stationary policies tailored to meta-tasks within a specific collaborative scenario. CBPR builds upon Bayesian Policy Reuse (BPR) Rosman et al. (2016); Chen et al. (2022), extending its application to human-AI collaborative tasks with theoretical guarantees. CBPR avoids modeling the non-stationary dynamics of human collaborators, focusing instead on heuristically modeling available meta-tasks within defined collaborative contexts. For example, in the multi-player cooking game _Overcooked_, meta-tasks include {_place onions in pot_, _deliver soup_, _place onions in pot & deliver soup_, _others_} (Figure 1) are available. Noticing that for a complex human-AI collaborative task, all of the undefined meta-tasks are categorized as "others," we subsequently train stationary meta-task-playing (MTP) collaborators using reinforcement learning (RL) to precisely match meta-task models on a one-to-one basis. During collaboration, CBPR identifies the meta-task being performed by the human partner based on recent actions, subsequently adapting the optimal MTP collaborator for use.

We evaluate CBPR in a fully-observable two-player common-payoff collaborative cooking simulator based on the game Overcooked Carroll et al. (2019), which has recently been proposed as a coordination challenge for AI Carroll et al. (2019); McKee et al. (2022); Wang et al. (2020); Wu et al. (2021); Knott et al. (2021). State-of-the-art performance of this game was achieved in Carroll et al. (2019); Strouse et al. (2021); Yu et al. (2023) via training stationary cooperation policy. Both simulated experiments and user studies show that the proposed CBPR agent can collaborate effectively with non-stationary agents and real humans. The novel contributions of this paper can be summarized as follows:

Figure 1: _Left_: The drawbacks of current collaborative agents, which train a stationary policy to manage the non-stationary dynamics of human collaborators but fail to determine the specific collaborative policies executed by humans. _Right_: Our approach focuses on identifying the meta-tasks underlying human decision-making and trains collaborators to match these meta-tasks in a one-to-one manner. This strategy enables effective ad-hoc collaboration with non-stationary humans.

1. We introduce a human-AI collaboration framework, CBPR, which addresses the challenge of modeling non-stationary human dynamics. This framework identifies the meta-tasks performed by human partners and reuses the optimal collaborative policy.
2. Theoretically, based on the Non-Stationary Markov Decision Process (NS-MDP), we provide theorems on _Collaboration Convergence_ and _Collaboration Optimality_ to support CBPR's convergence to the optimal collaborative policy over time in human-AI collaboration.
3. Empirically, we demonstrate CBPR's capability to collaborate effectively with non-stationary agents who frequently switch strategies, agents with various collaboration skills, and real humans.

## 2 Related Work

### Human-AI Collaboration

Training agents to collaborate with humans has been extensively studied. Recent research can be categorized into two groups based on whether human data is used during training. BCP Carroll et al. (2019) is trained by pairing with a supervised human model, while Boltzmann Policy Distribution (BPD) Laidlaw and Dragan (2022) updates its prior based on online human actions. These approaches require human data collection and are prone to distributional shifts. In contrast, another category focuses on achieving zero-shot coordination without extensive human data Hu et al. (2020). These works (e.g., PCP Strouse et al. (2021), Hidden-Utility Self-Play (HSP) Yu et al. (2023), and Maximum Entropy Population-based Training (MEP) Zhao et al. (2023)) train Self-Play (SP) agents to form a policy pool--a diverse set of AI agents assumed to encompass all potential human policies--and then train a collaborator to pair with policies in this pool. However, these collaborative agents remain single _stationary_ models despite their diverse training partners.

Our work represents a fundamental departure from previous studies by avoiding the need to model human behavior and instead focusing on constructing meta-tasks that underpin human decision-making. Furthermore, our CBPR framework does not restrict the construction of meta-tasks, which can be categorized into two streams: reliant on human data (e.g., behavior cloning) and independent of human data (e.g., rule-based methods).

### Policy Reuse

Policy reuse is a kind of transfer learning method that can greatly speed up reinforcement learning for a new task by using policies for relevant tasks. Initial methods like PRQL Fernandez and Veloso (2013) and OPS-TL Li and Zhang (2018), Li et al. (2018) integrated source policies with limitations in transfer efficiency. Subsequent approaches such as CAPS and CUP Zhang et al. (2022) improved policy selection and introduced more efficient algorithms without the need for extra training components.

Bayesian policy reuse (BPR) Rosman et al. (2016) represents a specialized stream within policy reuse. Utilizing a Bayesian optimization approach, BPR efficiently computes posteriors for novel tasks. Extensions like BPR+ Hernandez-Leal et al. (2016, 2016) and Bayes-Pepper Hernandez-Leal and Kaisers (2017) adapt BPR to multiagent scenarios, aligning tasks with opponent strategies and policies with optimal responses to these strategies. However, most BPR methodologies Rosman et al. (2016), Hernandez-Leal et al. (2016), Hernandez-Leal and Kaisers (2017), Zheng et al. (2018, 2021), Chen et al. (2022), Xie et al. (2022) primarily address multi-task problems or copy with competitive scenarios. Several studies, such as Zheng et al. (2018, 2021), investigated deep BPR+ in collaborative games.

However, these approaches primarily rely on policy inference to adjust to the changing strategies of opponents (or partners), which may not be optimal for human-AI collaboration given the wide spectrum of potential human policies. To our knowledge, our research is pioneering in applying and tailoring Bayesian policy reuse-based algorithms specifically for the human-AI collaboration challenge.

## 3 Collaborative Bayesian Policy Reuse

### Vanilla Bayesian Policy Reuse

Bayesian policy reuse is a general framework of transfer learning to cope with unknown tasks or frequently changing opponents. These classes of methods typically involve two phases: an offline learning phase and an online reusing phase. The workflow of a typical BPR can be summarized as follows: In the offline phase, it is presupposed that there exists a library of tasks \(\mathcal{T}\) and a corresponding library of learned policies \(\Pi\). Through conducting multiple simulations with varied policies across different tasks, a performance model \(P(U\mid\mathcal{T},\Pi)\) is derived, where \(U=\Sigma_{i=0}^{k}r_{i}\) is cumulative utility. This model works as a mapping operator, associating each task and policy with a distribution of a predefined utility measure, such as reward.

During the online phase, BPR identifies the current task or opponent policy by maintaining a belief model \(\beta(\cdot)\). This model is periodically updated based on observations, as defined by the observation model \(P(\sigma\mid\tau,\pi)\), where \(\sigma\) represents any signal aiding cooperation, such as reward or interaction trajectory. Significantly, this update adheres to Bayes' rule as follows:

\[\beta_{k}(\tau)=\frac{\mathrm{P}\left(\sigma_{k}\mid\tau,\pi_{k}\right)\beta_{ k-1}(\tau)}{\sum_{\tau^{\prime}\in\mathcal{T}}\mathrm{P}\left(\sigma_{k}\mid \tau^{\prime},\pi_{k}\right)\beta_{k-1}\left(\tau^{\prime}\right)} \tag{1}\]

With this belief model, the BPR agent can select the optimal response policy by solving the following optimization problem:

\[\pi^{\star}=\mathrm{argmax}_{\pi\in\Pi}\int_{U}^{U^{\mathrm{max}}}\sum_{\tau \in\mathcal{T}}\beta(\tau)\mathrm{P}\left(U^{+}\mid\tau,\pi\right)\mathrm{d}U ^{+} \tag{2}\]

where \(\bar{U}=\max_{\pi\in\Pi}\sum_{\tau\in\mathcal{T}}\beta(\tau)\mathbb{E}[U\mid \tau,\pi]\) represents the average performance of a single policy across all tasks. It's important to note that using \(\bar{U}\) as the lower limit of the integral, this optimization problem essentially seeks the policy with the highest likelihood of achieving utility above the average.

### CBPR Framework

#### Offline stage

Initially, we train meta-task processing (MTP) agents \(\pi\in\Pi\) using the Proximal Policy Optimization (PPO) algorithm by individually pairing them with meta-tasks within a specific

Figure 2: Overview of the CBPR Framework. This framework is divided into two main phases. _Left:_ Offline Training Phase. This includes **(1)** constructing meta-task models using collected data and creating a meta-task library; **(2)** developing cooperative policies for each meta-task to compile an AI policy library; **(3)** establishing a performance model by evaluating each meta-task and AI policy pair. _Right:_ Online Collaboration Phase. During a collaboration round, the process involves **(a)** gathering a list of historical and current human data; **(b)** determining the current meta-task undertaken by the human using Bayesian policy inference (refer to Equation 3-4); **(c)** selecting the most suitable AI policy for cooperation (as per Equation 5); and finally, **(d)** the AI collaborator executes actions according to the chosen policy.

collaborative context, as exemplified by tasks such as _place onions in pot_, _deliver soup_, _place onions in pot & deliver soup_, and _others_ in the _Overcooked_ collaboration benchmark. Meta-task models \(\tau\in\mathcal{T}\) are constructed through supervised learning, utilizing trajectories from either _rule-based agents enhanced with noise_ or _real humans performing the tasks_. In this study, we employ the rule-based agents developed by Yu et al. (2023). Subsequently, we construct the performance model \(P(U\mid\mathcal{T},\Pi)\) (i.e., observation model) by fitting a Gaussian distribution over the mean episodic return given a stochastic AI policy \(\pi\) and a noisy rule-based agent \(\tau\).

In previous BPR-based algorithms, the belief is designed for measuring the similarity between different tasks or opponents in transfer learning. These algorithms update belief using a observation model \(\mathrm{P}\left(\sigma\mid\tau,\pi\right)\) which only considers the game result but overlooks opponent's behavior. This leads to a poor collaborative performance when humans switch policy in a long-episode game. In this study, we used intra-episode belief \(\xi^{t}(\tau)\) at timestep \(t\) to measure the similarity between current meta-task \(\tau\) and \(\tau^{\prime}\) in meta-task model library \(\mathcal{T}\). The intra-episode belief was firstly proposed in Chen et al. (2022) and we extend it to the human-AI collaborative scenario.

**Online policy reuse** At the beginning of online policy reuse, the inter-episode belief \(\beta_{0}(\tau)\) is initialized with a uniform distribution. For each episode, CBPR maintains a first-in-first-out (FIFO) human behavior queue \(\mathcal{Q}\) of length \(l\), which records the latest human behavior tuples \((s_{i},a_{i})\). The AI selects initial response MTP agents according to the inter-episode belief \(\beta_{0}(\tau)\) (line 5 in Algorithm 1). CBPR collects human state-action pairs and updates the intra-episode belief \(\xi_{t}(\tau)\):

\[\xi_{t}(\tau)=\frac{P(\mathcal{Q}\mid\tau)\xi_{t-1}(\tau)}{\sum_{\tau^{\prime} \in\mathcal{T}}P\left(\mathcal{Q}\mid\tau^{\prime}\right)\xi_{t-1}\left(\tau^{ \prime}\right)} \tag{3}\]

where \(P(\mathcal{Q}\mid\tau)=\frac{\exp\left(\sum_{i=0}^{l}\log\tau(a_{i}|s_{i}) \right)}{\sum_{\tau^{\prime}\in\mathcal{T}}\exp\left(\sum_{i=0}^{l}\log\tau^{ \prime}(a_{i}|s_{i})\right)}\). Then the intra-episode belief and inter-episode belief are integrated:

\[\xi_{t}(\tau)=\rho^{t}\beta_{k-1}(\tau)+\left(1-\rho^{t}\right)\xi_{t}(\tau) \tag{4}\]

Where \(\rho\in[0,1]\) is a hyperparameter controlling the weight of the inter-episode and intra-episode beliefs. As the timestep \(t\) increases in a game with a long episode, the integrated belief \(\zeta_{t}(\tau)\) primarily depends on the intra-episode belief \(\xi_{t}(\tau)\). The AI then uses the integrated belief \(\zeta_{t}(\tau)\) to select a policy to cooperate with the human at each timestep.

\[\pi_{t}^{\star}=\arg\max_{\pi\in\Pi}\int_{\tilde{U}}^{U^{\max}}\sum_{\tau\in \mathcal{T}}\zeta_{t}(\tau)P\left(U^{+}\mid\tau,\pi\right)\mathrm{d}U^{+} \tag{5}\]

At the end of each episode, CBPR collects the episodic return \(u_{k}\) and updates the inter-episode belief \(\beta_{k}(\tau)\). To adapt to non-stationary human dynamics, we store human-AI trajectories in a replay buffer \(\mathcal{R}\) of the current MTP agent and update its policy. The detailed pseudo-code for the policy reuse of CBPR is presented in Algorithm 1.

### Theory Analysis of CBPR

The selection of cooperative policies (line 11 in the Algorithm 1) is crucial to the performance of CBPR in collaborating with humans. In this section, we propose theorems on the convergence and optimality of CBPR to support our viewpoint: CBPR will converge to the optimal cooperative strategy during the human-AI interaction process. We formulate collaborative process between humans and AI as a Non-Stationary MDP (NS-MDP) Chandak et al. (2020). In this process, the non-stationarity, resulting from the dynamic nature of human policy, can be mitigated by decomposing the entire non-stationary decision process into several stationary ones. Each stationary MDP corresponds to a specific meta-task executed by the human. Specifically, for a given NS-MDP \(\{M_{i}\}_{i=1}^{\infty}\), the transition function integrates human actions as part of the environment itself, which can be denoted as \(\mathcal{P}_{i}:\mathcal{S}\times\mathcal{A}_{AI}\times\mathcal{A}_{hu}\to \Delta(\mathcal{S})\). Within each stationary MDP \(M_{i}\), the human policy \(\pi_{hu,i}:\mathcal{S}\to\Delta(\mathcal{A})\) is assumed to be stationary, although it may exhibit variations across different stationary MDPs. Under this assumption, the CBPR agent could establish a convergent human-AI collaboration:

**Theorem 1** (Collaboration Convergence of CBPR Agent).: _Let \(H_{i}:=\{S_{i}^{j},\pi_{hu,i}(S_{i}^{j}),R^{j}\}_{j=0}^{\infty}\) be a trajectory collected from a single stationary MDP \(M_{i}\) within the overall NS-MDP \(\{M_{i}\}_{i=1}^{\infty}\) under the human meta-task policy \(\pi_{hu,i}\). Denote \(\mathcal{D}:=\{(i,H_{i}):i\in[1,k]\}\) as a random variable representing a set of trajectories observed prior to the most recently completed stationary MDP \(M_{k}\). Given \(\mathcal{D}\), the response policy of CBPR agent could almost sure converge when interacting with a human partner, even when the human's policy is non-stationary._

We provide all proofs and a detailed explanation in Appendix A. In addition to being able to converge in cooperation with non-stationary humans, the CBPR agent can also establish the optimal collaboration policy:

**Theorem 2** (Collaboration Optimality of CBPR Agent).: _Denoting \(\mathrm{CBPR}\) for CBPR algorithm, let \(\rho(\pi,m):=\mathbb{E}[\int_{0}^{U^{\mathrm{max}}}\mathrm{P}\left(U^{+}\mid \tau(m),\pi\right)\mathrm{d}U^{+}]\) be the expected return of exploiting AI policy \(\pi\) with human meta-task policy \(\tau(m)\) in MDP \(M_{m}\). Given a positive integer \(k\) and a set of trajectories \(\mathcal{D}\) observed prior to the MDP \(M_{k}\), it follows that for any subsequent stationary MDP \(M_{k+\delta}\), we have:_

\[\Pr\Bigl{(}\rho\bigl{(}\mathrm{CBPR}(\mathcal{D}),k+\delta\bigr{)}\geq\rho(\pi _{k}^{\star},k+\delta)\Bigr{)}\to 1 \tag{6}\]

_when \(k\to\infty\), where \(\pi_{k}^{\star}\) is the optimal response policy for human meta-task policy at MDP \(M_{k}\)._

## 4 Experiments

In the context of _Overcooked_, we use rule-based policies developed in Yu et al. (2023) for each game layout (see Appendix C.1). These rule-based policies such as _place onions in pot_, _deliver soup_ are used to train corresponding MTP agents in a one-to-one manner. In this section, we conduct extensive experiments to answer the following questions:

**Q1**: When interacting with non-stationary agents who switch their strategies, can CBPR outperform established baselines? Additionally, can CBPR adapt its collaborative strategies to better synchronize with partner behaviors?

**Q2**: When interacting with non-stationary agents of various collaboration skills, can CBPR surpass other baselines?

**Q3**: Can CBPR exceed the performance of other baselines in collaboration with real humans?

**Q4**: How do hyperparameters and number of predefined meta-tasks influence the collaborative performance (mean reward) of CBPR agents?

_Overcooked_ environment _Overcooked_ is a popular two-player common-payoff game. It has become a typical environment for studying human-AI collaboration Carroll et al. (2019); Knott et al. (2021); Strouse et al. (2021); McKee et al. (2022); Yu et al. (2023). In this game, players should place three onions or tomatoes in a pot and deliver as many cooked soups as possible within a time limit. Good coordination between two players is crucial for achieving a high score. We employed four layouts in our experiments: _Cramped Room_, _Coordination Ring_, _Asymmetric Advantage_ and _Soup Coordination_ (Figure 8 in Appendix) in our experiments. Notably, in the _Asymm. Adv._ and _Soup Coord._, the players do not interfere with each other, and their movements are unobstructed by their partners.

**Baselines** We compare CBPR against three well-established baselines: (1) the Behavioral Cloning Play (BCP) Carroll et al. (2019), a human model-based method designed for human-AI collaboration; (2) Fictitious Co-Play (FCP) Strouse et al. (2021), a two-stage approach trained with partners of varying skill levels; (3) Self-Play (SP) Silver et al. (2017), a common RL method trained by playing against itself. For a fair comparison, we employed PPO Schulman et al. (2017) as the underlying algorithm of CBPR and reimplemented all baselines using identical hyperparameters in our experiments. Further details about environment setting and agents are illustrated in Appendix C.

### Cooperating with Rule-Based Agents Under Dynamic Policy Switching

To answer question **Q1**, we conduct a thorough investigation into the collaboration performance of CBPR when paired with non-stationary agents. These agents exhibited changes in their rule-based policies (Appendix Table 3), both inter-episodically and intra-episodically. We maintained a consistent random seed for policy switching during the evaluations to ensure fairness when comparing CBPR with baseline methods.

In our experiment, we evaluate the collaborative performance of agents at four different policy switching frequencies, as shown in Figure 3. The results show that CBPR consistently outperforms baseline methods in most cases. In particular, BCP, which was trained using a stationary human model, exhibited significantly poorer performance compared to CBPR. In addition, FCP and SP agents show greater fluctuations in episodic rewards, primarily due to their inability to effectively collaborate with all agents. In some instances, SP agents opted not cooperate, resulting in zero reward.

Our findings indicate that CBPR is particularly effective at collaborating with partners exhibiting varying degrees of non-stationarity. For a detailed overview of the results across the additional three policy switching frequencies (i.e., _per 2 episodes_, _per 200 timesteps_, and _per 100 timesteps_), please refer to the Appendix C.

### Cooperation with Partners of Various Collaboration Skills

The cooperative capacity of non-stationary humans is typically suboptimal. A generalized agent must be capable of collaborating with partners possessing diverse collaboration skills.

During the initial training phase of FCP Strouse et al. (2021), a policy pool is created by preserving various agent "checkpoints" that represent different levels of expertise. To answer question **Q2**, we pair CBPR with agents with varying collaboration skills preserved during the first stage of the FCP training. We evaluate collaborative performance over 50 episodes on four layouts. The results show that CBPR consistently achieved higher mean episode rewards than FCP, particularly when collaborating with lower-skilled partners (Figure 4). It is noteworthy that BCP performs better in the _Asymm. Adv._ and _Soup Coord._ in which players' movements are not hindered by their partners. We replayed the trajectories of BCP in _Cramped Rm._ and _Coord. Ring_ and observed that BCP occasionally became immobilized and failed to collaborate with partners (Figure 4b).

Figure 3: Comparative performance analysis against baselines when collaborators switch their rule-based policies _per episode_. All agents were evaluated over 50 continuous episodes. The shaded areas denote standard deviation calculated from five random seeds.

### Cooperation with Real Humans

To address question **Q3**, we recruited 25 volunteers from a local university, comprising 5 females and 20 males, ranging in age from 21 to 34 years, to participate in a study involving collaboration with CBPR and baseline agents. These volunteers were randomly assigned to one of four groups, each corresponding to a different game layout. Prior to the experiment, nearly all volunteers were unfamiliar with _Overcooked_. We provided comprehensive instructions from scratch and allowed them to play at least five practice rounds before beginning the evaluation. Subsequently, participants were instructed to interact with both the CBPR and baseline agents through the human-AI web applications developed by Carroll et al. (2019). Each volunteer participated in two episodes, during which we recorded the average reward obtained.

According to the reward distribution (Figure 5), we observe that CBPR achieves more efficient collaboration than other baselines. In most comparisons, CBPR displays significant higher reward according to the one-sided Mann-Whitney U test.

**Case study** To further demonstrate how the CBPR is more superior than baseline algorithms when collaborating with real humans, we present a case in Figure 6. In this case, we record five frames from the _Overcooked_ game interface to show that the ability of CBPR to adaptively adjust cooperative policies. Initially, CBPR agent is ready to use a dish to serve the soon-to-be-ready soup. When the human partner picks the soup, CBPR will set down the dish and continue to place onions to the pot for a new round. Meanwhile, FCP, after putting down the dish, will appear confused until the human served the soup. BCP, on the other hand, will not put down the dish and stubbornly prepare to serve the soup, ignoring the fact that the soup had already been served.

### Ablation Study

**Ablation on the queue size \(l\) and inter-episodic belief weight \(\rho\).** In CBPR, the length \(l\) of human behavior queue and weight \(\rho\) of inter-episodic belief mainly influence the collaborative performance. The larger \(l\) in \(P(\mathcal{Q}\mid\tau)\) of Eq. 3 means that CBPR chooses policy considering more past human behaviors. The larger \(\rho\) determines that CBPR needs to consider inter-episodic belief more at the beginning of an episode. To answer question **Q4**, we expand on the experiments from section 4.2 demonstrate the results in Figure 7 and Appendix D.2. Overall, the results show that \(l\)=20 performs best, and in a relative simple layout (i.e., _Cramped Rm._), since the belief of cooperative policy converges easily, variation in \(\rho\) has little impact on the reward. However, in complex layout (e.g., _Soup Coord._) (Figure 16), adjusting \(\rho\) can enhance cooperative performance to a certain extent.

**Ablation on the number of predefined meta-tasks.** The performance of CBPR depends on the design of the meta-tasks. To address the challenge of predefined meta-tasks not covering all

Figure 4: Comparative performance analysis against baselines in cooperation with partners of diverse skill levels (low, medium and high). All agents were evaluated over 50 episodes and errors bars denote 95% confidence intervals.

Figure 5: Rewards distribution of agents collaborating with real humans over four layouts. *, \(p<0.05\); **, \(p<0.01\); ***, \(p<0.001\), and n.s., not significant. (Statistical significance was assessed by a one-sided Mann-Whitney U test.)

[MISSING_PAGE_FAIL:9]

in front of the serving areas) can obstruct their partners from completing the task in the non-separated layouts. Therefore, non-separated layouts require more cooperation between players compared to separated layouts. As shown in Figure 4, CBPR's better performance in _Cramped Rm._ and _Coord. Ring_ suggests its advantage in collaborative tasks.

**The double-edged sword of SP's simple policy.** In _Asymm. Adv._, SP agent exhibits outstanding performance when it cooperates with the agent of high skill level (Figure 3(c)). We replayed the game and found that the SP agent learned the simplest and most effective policy (i.e., in the right room, just pick an onion from onion dispenser and then place it in a pot within the shortest path). On the contrary, other agents exhibit some superfluous actions due to their own complexity. However, when cooperating with the agent of low skill level, SP performs poorly because the SP agent on the right only learned the simplest policy (putting onions in the pot), and when the agent with low skill level on the left does not deliver the cooked soup, SP will wait in place rather than deliver the cooked soup. In a more complex layout _Soup Coord._, we found that the SP agent learned a policy of putting only one onion in the pot and starting to cook, leaving its partner confused and uncertain about what went wrong. Therefore, cooperation with SP agents leads to low performance (Figure 3(d)).

## 5 Conclusion and Discussion

**Conclusion** In this work, we proposed CBPR framework and evaluated it in the well-known game _Overcooked_. CBPR could effectively tackle the challenge of collaborating with humans by utilizing a suite of meta-task aware agents. In response to the non-stationary nature of human behavior, CBPR adeptly selects MTP agent based on the most recent human actions and episodic returns. We have theoretically underpinned the collaborative efficacy of the CBPR approach. Empirically, we demonstrated that CBPR outperforms baselines when collaborates with simulated humans that change their policies frequently, simulated humans that employ different skill levels and real human players. We remark our primary argument that, given the non-stationary inherent in human behaviors, it is more effective to design various agents tailored to corresponding humans in different mental and behavioral states, rather than relying on a seemingly omnipotent single agent. After all, two heads are better than one.

**Limitations and future work** In this work, meta-tasks are modeled by manually-designed rule-based policies. In real-world application domains such as assessing power system transient stability in power grid dispatching and autonomous driving, it is time consuming to design various rule-based policies.CBPR offers a viable strategy to model meta-tasks, facilitating the training of multiple specialized experts to handle distinct meta-tasks. A notable challenge, however, is the manual summarization of domain experts' meta-tasks. As a direction for future research, we are keen to address the task of clustering policies automatically based on human trajectories. While this study Zhang et al. [2023] has made strides in this direction, the clustering approach adopted therein tends to obscure semantic understanding, presenting hurdles for AI in comprehending human behaviors. Splitting human trajectories according to the key state may be a possible solution. Additionally, perceiving the acquisition of a specific class of shaped rewards by an agent as the execution of a meta-task merits future consideration. This approach also does not depend on human data or models and offers enhanced universality and interpretability.

## Acknowledgements

We are grateful to Professor Xiaohong Guan for his kind support of this work and anonymous reviewers for their insightful comments. This work was supported by the National Key R&D Program of China (2021YFB2400800).

Figure 7: Episodic reward by using different length \(l\) of human behavior queue and weight \(\rho\) of inter-episodic belief in _Cramped Rm._ layout. All agents are evaluated over 50 episodes and error bars denote 95% confidence intervals.

## References

* Bard et al. (2020) Nolan Bard, Jakob N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, et al. The hanabi challenge: A new frontier for ai research. _Artificial Intelligence_, 280:103216, 2020.
* Carroll et al. (2019) Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca Dragan. On the utility of learning about humans for human-ai coordination. _Advances in neural information processing systems_, 32, 2019.
* Chandak (2022) Yash Chandak. _Reinforcement Learning for Non-stationary problems_. PhD thesis, PhD thesis, University of Massachusetts Amherst, 2022.
* Chandak et al. (2020) Yash Chandak, Scott Jordan, Georgios Theocharous, Martha White, and Philip S Thomas. Towards safe policy improvement for non-stationary mdps. _Advances in Neural Information Processing Systems_, 33:9156-9168, 2020.
* Chen et al. (2022) Hao Chen, Quan Liu, Ke Fu, Jian Huang, Chang Wang, and Jianxing Gong. Accurate policy detection and efficient knowledge reuse against multi-strategic opponents. _Knowledge-Based Systems_, 242:108404, 2022.
* Dafoe et al. (2020) Allan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R McKee, Joel Z Leibo, Kate Larson, and Thore Graepel. Open problems in cooperative ai. _arXiv preprint arXiv:2012.08630_, 2020.
* Fernandez and Veloso (2013) Fernando Fernandez and Manuela Veloso. Learning domain structure through probabilistic policy reuse in reinforcement learning. _Progress in Artificial Intelligence_, 2(1):13-27, 2013.
* Gupta et al. (2022) Abhishek Gupta, Aldo Pacchiano, Yuexiang Zhai, Sham Kakade, and Sergey Levine. Unpacking reward shaping: Understanding the benefits of reward engineering on sample complexity. _Advances in Neural Information Processing Systems_, 35:15281-15295, 2022.
* Hernandez-Leal and Kaisers (2017) Pablo Hernandez-Leal and Michael Kaisers. Towards a fast detection of opponents in repeated stochastic games. In _International Conference on Autonomous Agents and Multiagent Systems_, pages 239-257. Springer, 2017.
* Hernandez-Leal et al. (2016a) Pablo Hernandez-Leal, Benjamin Rosman, Matthew E Taylor, L Enrique Sucar, and Enrique Munoz de Cote. A bayesian approach for learning and tracking switching, non-stationary opponents. In _Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems_, pages 1315-1316, 2016a.
* Hernandez-Leal et al. (2016b) Pablo Hernandez-Leal, Matthew E Taylor, Benjamin Rosman, L Enrique Sucar, and Enrique Munoz De Cote. Identifying and tracking switching, non-stationary opponents: A bayesian approach. In _Workshops at the Thirtieth AAAI Conference on Artificial Intelligence_, 2016b.
* Hu et al. (2020) Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. "other-play" for zero-shot coordination. In _International Conference on Machine Learning_, pages 4399-4410. PMLR, 2020.
* Jagerman et al. (2019) Rolf Jagerman, Ilya Markov, and Maarten de Rijke. When people change their mind: Off-policy evaluation in non-stationary recommendation environments. In _Proceedings of the twelfth ACM international conference on web search and data mining_, pages 447-455, 2019.
* Klien et al. (2004) Glen Klien, David D Woods, Jeffrey M Bradshaw, Robert R Hoffman, and Paul J Feltovich. Ten challenges for making automation a" team player" in joint human-agent activity. _IEEE Intelligent Systems_, 19(6):91-95, 2004.
* Knott et al. (2021) Paul Knott, Micah Carroll, Sam Devlin, Kamil Ciosek, Katja Hofmann, Anca D Dragan, and Rohin Shah. Evaluating the robustness of collaborative agents. _arXiv preprint arXiv:2101.05507_, 2021.
* Laidlaw and Dragan (2022) Cassidy Laidlaw and Anca Dragan. The boltzmann policy distribution: Accounting for systematic suboptimality in human models. _arXiv preprint arXiv:2204.10759_, 2022.
* Laidlaw and Dragan (2020)Siyuan Li and Chongjie Zhang. An optimal online method of selecting source policies for reinforcement learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* Li et al. (2018) Siyuan Li, Fangda Gu, Guangxiang Zhu, and Chongjie Zhang. Context-aware policy reuse. _arXiv preprint arXiv:1806.03793_, 2018.
* McKee et al. (2022) Kevin R McKee, Joel Z Leibo, Charlie Beattie, and Richard Everett. Quantifying the effects of environment and population diversity in multi-agent reinforcement learning. _Autonomous Agents and Multi-Agent Systems_, 36(1):21, 2022.
* Rosman et al. (2016) Benjamin Rosman, Majd Hawsly, and Subramanian Ramamoorthy. Bayesian policy reuse. _Machine Learning_, 104(1):99-127, 2016.
* Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Silver et al. (2017) David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. _arXiv preprint arXiv:1712.01815_, 2017.
* Strouse et al. (2021) DJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, and Richard Everett. Collaborating with humans without human data. _Advances in Neural Information Processing Systems_, 34:14502-14515, 2021.
* Wang et al. (2020) Rose E Wang, Sarah A Wu, James A Evans, Joshua B Tenenbaum, David C Parkes, and Max Kleiman-Weiner. Too many cooks: Coordinating multi-agent collaboration through inverse planning. 2020.
* Wu et al. (2021) Sarah A Wu, Rose E Wang, James A Evans, Joshua B Tenenbaum, David C Parkes, and Max Kleiman-Weiner. Too many cooks: Bayesian inference for coordinating multi-agent collaboration. _Topics in Cognitive Science_, 13(2):414-432, 2021.
* Xie et al. (2022) Donghan Xie, Zhi Wang, Chunlin Chen, and Daoyi Dong. Efficient bayesian policy reuse with a scalable observation model in deep reinforcement learning. _arXiv preprint arXiv:2204.07729_, 2022.
* Yu et al. (2022) Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative multi-agent games. _Advances in Neural Information Processing Systems_, 35:24611-24624, 2022.
* Yu et al. (2023) Chao Yu, Jiaxuan Gao, Weilin Liu, Botian Xu, Hao Tang, Jiaqi Yang, Yu Wang, and Yi Wu. Learning zero-shot cooperation with humans, assuming humans are biased. _arXiv preprint arXiv:2302.01605_, 2023.
* Zhang et al. (2022) Jin Zhang, Siyuan Li, and Chongjie Zhang. Cup: Critic-guided policy reuse. _arXiv preprint arXiv:2210.08153_, 2022.
* Zhang et al. (2023) Ziqian Zhang, Lei Yuan, Lihe Li, Ke Xue, Chengxing Jia, Cong Guan, Chao Qian, and Yang Yu. Fast teammate adaptation in the presence of sudden policy change. _arXiv preprint arXiv:2305.05911_, 2023.
* Zhao et al. (2023) Rui Zhao, Jinming Song, Yufeng Yuan, Haifeng Hu, Yang Gao, Yi Wu, Zhongqian Sun, and Wei Yang. Maximum entropy population-based training for zero-shot human-ai coordination. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 6145-6153, 2023.
* Zheng et al. (2018) Yan Zheng, Zhaopeng Meng, Jianye Hao, Zongzhang Zhang, Tianpei Yang, and Changjie Fan. A deep bayesian policy reuse approach against non-stationary agents. _Advances in neural information processing systems_, 31, 2018.
* Zheng et al. (2021) Yan Zheng, Jianye Hao, Zongzhang Zhang, Zhaopeng Meng, Tianpei Yang, Yanran Li, and Changjie Fan. Efficient policy detecting and reusing for non-stationarity in markov games. _Autonomous Agents and Multi-Agent Systems_, 35(1):1-29, 2021.
* Zheng et al. (2021)Proof of collaboration performance

### Proof of collaboration convergence

**Theorem 1** (Collaboration Convergence of CBPR Agent).: _Let \(H_{i}:=\{S_{i}^{j},\pi_{hu,i}(S_{i}^{j}),R^{j}\}_{j=0}^{\infty}\) be a trajectory collected from a single stationary MDP \(M_{i}\) within the overall NS-MDP \(\{M_{i}\}_{i=1}^{\infty}\) under the human meta-task policy \(\pi_{hu,i}\). Denote \(\mathcal{D}:=\{(i,H_{i}):i\in[1,k]\}\) as a random variable representing a set of trajectories observed prior to the most recently completed stationary MDP \(M_{k}\). Given \(\mathcal{D}\), the response policy of CBPR agent could almost sure converge when interacting with a human partner, even when the human's policy is non-stationary._

To establish the convergence of the posterior distribution, we first note that Doob's Martingale Convergence Theorem applies to our setting. Specifically, we have the following theorem:

**Theorem 3** (Doob's Martingale Convergence Theorem).: _Let \(X_{n}\) be a martingale (or sub-martingale or super-martingale) with respect to the sequence of sigma-algebras \(\mathcal{F}_{n}\), such that \(E[|X_{n}|]<\infty\) for all \(n\). If there exists a constant \(C\) such that \(E[|X_{n+1}-X_{n}||\mathcal{F}_{n}|]\leq C\) for all \(n\), then there exists a random variable \(X\) such that \(X_{n}\) converges to \(X\) almost surely and in \(L^{1}\)._

With the aforementioned theorem, we can readily establish the proof of our theorem.

Proof.: For non-stationary MDPs, demonstrating convergence involves showing that the algorithm can adapt to changing convergence points and ultimately reach them. Therefore, we will first establish the convergence property of the Bayesian update. Specifically, it will be demonstrated that the posterior distribution converges almost surely to the true parameter value. Subsequently, we will prove that, when using Bayesian updates, CBPR algorithms always converge to a fixed response policy, provided that the human policy remains unchanged before reaching the fixed response policy.

To establish the convergence of posterior distribution, we first proof that the Doob's Martingale Convergence Theorem holds for the Bayesian updating: \(\beta_{k}(\tau)=\frac{\mathrm{P}(\sigma_{k}|\tau,\pi_{k})\beta_{k-1}(\tau)}{ \sum_{\tau^{\prime}\in\mathcal{T}}\mathrm{P}(\sigma_{k}|\tau^{\prime},\pi_{k}) \beta_{k-1}(\tau^{\prime})}\).

Consider \(\mathcal{F}_{k}\) as the sequence of sigma-algebras generated by observations up to time \(k\). A fundamental property of Bayesian updating is that the expected value of the posterior distribution conditioned on past data equals the current posterior distribution, expressed as \(E[\beta_{k+1}(\tau)|\mathcal{F}_{k}]=\beta_{k}(\tau)\). This holds because the posterior distribution \(\beta_{k}(\tau)\) encapsulates all relevant information up to time \(k\). Thus, conditioning on \(\mathcal{F}_{k}\) accounts for all past observations, and in the absence of new data, the expected future posterior must align with the current posterior. This relationship signifies that, given the information available up to time \(k\), the expectation of the next posterior does not deviate from the current posterior, establishing \(\beta_{k}(\tau)\) as a martingale with respect to \(\mathcal{F}_{k}\).

Moreover, the bounded nature of \(\beta_{k}(\tau)\) within the interval [0, 1] ensures that the Bayesian update satisfies the conditions of Doob's Martingale Convergence Theorem. Since \(\beta_{k}(\tau)\) represents a probability, it is inherently bounded, which guarantees that the expected absolute change \(E[|\beta_{k+1}(\tau)-\beta_{k}(\tau)||\mathcal{F}_{k}]\) remains bounded. Additionally, with \(E[\beta_{k}(\tau)]=1\), the integrability condition required for martingale convergence is also satisfied. This combination of boundedness and integrability provides the mathematical foundation that guarantees the convergence of the sequence \(\beta_{k}(\tau)\).

In conclusion, the sequence of Bayesian updates \(\beta_{k}(\tau)\) adheres to the defining properties of a martingale and satisfies the conditions of Doob's Martingale Convergence Theorem through its inferent property and boundedness. As a result, we can conclude that the belief \(\beta_{k}(\tau)\) regarding the human meta-task will converge as \(k\rightarrow\infty\):

\[\mathrm{Pr}\Big{(}\beta_{k}(\tau)\Big{)}\to 1 \tag{7}\]

Secondly, to prove that the calculated best response policy of AI \(\pi^{\star}\) converges to a fixed value as \(k\rightarrow\infty\), we consider both the structure of the Bayesian update and the decision-making process in CBPR framework.

Given \(\beta_{k}(\tau)\) converges, we note that the uncertainty about the human behavior policy \(\tau\) diminishes with an increasing number of observations. The convergence of \(\beta_{k}(\tau)\) to a specific distribution implies that the belief about the human's policy stabilizes. In mathematical terms, as \(k\rightarrow\infty\), \(\beta_{k}(\tau)\rightarrow\beta(\tau)\) for some fixed distribution \(\beta(\tau)\).

Then the stabilized response policy of AI \(\pi^{\star\star}\) is given by:

\[\pi^{\star\star}=\operatorname*{argmax}_{\pi\in\Pi}\int_{\tilde{U}}^{U^{\max}} \sum_{\tau\in\mathcal{T}}\beta(\tau)\mathrm{P}\left(U^{+}|\tau,\pi\right)\mathrm{ d}U^{+} \tag{8}\]

Here, the decision-making is a function of both the belief \(\beta(\tau)\) and the expected utility \(\mathrm{P}\left(U^{+}|\tau,\pi\right)\) for each AI response policy \(\pi\). As \(\beta_{k}(\tau)\) converges to \(\beta(\tau)\), the decision-making process becomes increasingly dependent on a stable belief about the human's policy. Thus, the variability in the choice of \(\pi^{\star}\) diminishes, leading to a convergence of \(\pi^{\star}\) as well.

Formally, the convergence of \(\pi^{\star}\) can be shown by demonstrating that the integral expression defining \(\pi^{\star}\) becomes stable as \(k\to\infty\). Since \(\beta(\tau)\) stabilizes, the integral's value, which depends on the belief about \(\tau\), also stabilizes. Consequently, by the linearity of convergence, the policy that maximizes this expression, \(\pi^{\star}\), will almost sure converge to a fixed policy.

Given the convergence property of \(\pi^{\star}\), the almost sure convergence for the response policy of our CBPR agent is established.

### Proof of collaboration optimality

**Theorem 2** (Collaboration Optimality of CBPR Agent).: _Denoting \(\mathrm{CBPR}\) for CBPR algorithm, let \(\rho(\pi,m):=\mathbb{E}[\int_{U}^{U^{\max}}\mathrm{P}\left(U^{+}\mid\tau(m), \pi\right)\mathrm{d}U^{+}]\) be the expected return of exploiting AI policy \(\pi\) with human meta-task policy \(\tau(m)\) in MDP \(M_{m}\). Given a positive integer \(k\) and a set of trajectories \(\mathcal{D}\) observed prior to the MDP \(M_{k}\), it follows that for any subsequent stationary MDP \(M_{k+\delta}\), we have:_

\[\mathrm{Pr}\Big{(}\rho\big{(}\mathrm{CBPR}(\mathcal{D}),k+\delta\big{)}\geq \rho(\pi_{k}^{\star},k+\delta)\Big{)}\to 1 \tag{9}\]

_when \(k\to\infty\), where \(\pi_{k}^{\star}\) is the optimal response policy for human meta-task policy at MDP \(M_{k}\)._

Proof.: Considering the CBPR algorithm within the framework of MDPs, we define the expected return \(\rho(\pi,m)\) as the integral of the probability of achieving utility \(U^{+}\) given the AI policy \(\pi\) and the human meta-task policy \(\tau(m)\) in MDP \(M_{m}\).

Assuming that the human policy library and AI policy library encompass all possible human meta-task policies and their corresponding best AI response policies. Then, we need to prove that the expected return of exploiting the CBPR algorithm's policy in any subsequent stationary MDP \(M_{k+\delta}\) will be greater than or equal to that of the optimal response policy \(\pi_{k}^{\star}\) at \(M_{k}\). Formally, we can express this and derive it as follows:

\[\mathrm{Pr}\Big{(}\rho\big{(}\mathrm{CBPR}(\mathcal{D}),k+\delta \big{)}\geq\rho(\pi_{k}^{\star},k+\rho)\Big{)}\] \[= \mathrm{Pr}\Bigg{(}\int_{U}^{U^{\max}}\sum_{\tau\in\mathcal{T}} \beta(\tau)\mathrm{P}\left(U^{+}\mid\tau,\pi_{\mathrm{CBPR}}\right)\mathrm{d}U ^{+}\geq\int_{U}^{U^{\max}}\mathrm{P}\left(U^{+}\mid\tau(k+\delta),\pi(k^{ \star})\,\mathrm{d}U^{+}\right)\] \[= \mathrm{Pr}\Bigg{(}\int_{U}^{U^{\max}}\sum_{\tau\in\mathcal{T}} \beta(\tau)\mathrm{P}\left(U^{+}\mid\tau,\pi_{\mathrm{CBPR}}\right)\mathrm{d}U ^{+}-\int_{U}^{U^{\max}}\mathrm{P}\left(U^{+}\mid\tau(k+\delta),\pi(k^{\star} )\,\mathrm{d}U^{+}\geq 0\right)\] \[= \mathrm{Pr}\Bigg{(}\int_{U}^{U^{\max}}\Big{[}\beta(\tau(k+ \delta))\mathrm{P}\left(U^{+}\mid\tau(k+\delta),\pi_{\mathrm{CBPR}}\right)- \mathrm{P}\left(U^{+}\mid\tau(k+\delta),\pi_{k}^{\star}\right)\Big{]}\mathrm{d}U ^{+}\] \[\qquad+\int_{U}^{U^{\max}}\sum_{\tau\in\mathcal{T}-\{\tau(k+ \delta)\}}\beta(\tau)\mathrm{P}\left(U^{+}\mid\tau,\pi_{\mathrm{CBPR}}\right) \mathrm{d}U^{+}\geq 0\Bigg{)} \tag{10}\]

Where \(\tau(k+\delta)\) represent the true stationary human meta-task policy at MDP \(M_{k+\delta}\), \(\pi(k^{\star})\) is the best response of AI at MDP \(M_{k}\), \(\pi_{\mathrm{CBPR}}\) is the response policy generated by CBPR algorithm.

From theorem 1, we have \(\mathrm{Pr}\Big{(}\beta_{k}(\tau(k+\delta))\Big{)}\to 1\), when \(k\to\infty\).

[MISSING_PAGE_EMPTY:15]

Implementation details

In our study, we rigorously implemented MTP within the CBPR framework and ensured that all baselines (BCP, FCP, and SP) adhered to a unified methodology. This approach utilized the Proximal Policy Optimization (PPO) algorithm, a widely acclaimed reinforcement learning technique Schulman et al. (2017), under a standardized set of parameters (refer to Table 2). The adoption of PPO was motivated by its balance between sample efficiency and simplicity, making it a popular choice in recent multi-agent learning research Yu et al. (2022). To optimize the learning process and mitigate the often challenging exploration in the environment, we incorporated tailored reward shaping parameters as delineated in Table 3. This strategy aligns with the established practices in reinforcement learning that emphasize the importance of structured rewards in complex environments Gupta et al. (2022). Additionally, our empirical analyses revealed a distinct performance advantage of feature-based observation models over the image-based ones, leading to their adoption across all agents. The entire training process was facilitated by the computational prowess of an NVIDIA 3080 GPU.

### Collaborative Bayesian Policy Reuse (CBPR)

The CBPR's offline phase is a multi-faceted process encompassing meta-task modeling, MTP, and performance modeling.

Initial efforts involved the manual definition of rule-based policies for each layout (Table 4), a step inspired by the scripted policies detailed in Yu et al. (2023).

This was followed by the training of MTP agents \(\pi\in\Pi\), which were systematically paired with rule-based agents to facilitate robust policy development. The training phase, as illustrated in Figure 9, was underpinned by a commitment to capturing a diverse range of strategic interactions. Subsequently, we developed meta-task models \(\tau\in\mathcal{T}\), leveraging a two-layer feed-forward neural network. This network, initialized orthogonally and optimized at a learning rate of 1e-3, was instrumental in deciphering the nuanced mappings from observations to actions.

In the final stage, performance models were crafted by pairing each MTP agent \(\pi\) with rule-based meta-tasks across 50 episodes, adopting a Gaussian distribution approach to model episodic rewards.

\begin{table}
\begin{tabular}{l c} \hline \hline
**Parameter** & **Value** \\ \hline Learning rate & 5e-4 \\ Entropy coefficient & 0.01 \\ Epsilon & 0.05 \\ Gamma & 0.99 \\ Lambda & 0.95 \\ Batch size & 4096 \\ Clipping & 0.05 \\ Hidden dim of actor and critic & 128 \\ Reward shaping horizons & 0.5 * total timesteps \\ \hline \hline \end{tabular}
\end{table}
Table 2: PPO hyperparameters for MTP, BCP, FCP and SP agents. _Lambda_ is used in generalized advantage estimation (GAE) to calculate advantage function. Reward shaping parameters in Table 3 gradually anneals to zero over _Reward shaping horizons_.

\begin{table}
\begin{tabular}{l c} \hline \hline
**Action** & **Reward** \\ \hline Place in pot & 3 \\ Dish pickup & 3 \\ Soup pickup & 5 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Reward shaping parameters for PPO.

### Baselines

#### c.2.1 Behavior Cloning (BC) and Behavioral Cloning Play (BCP) Carroll et al. (2019)

The BC models were trained using human-human trajectory data from Carroll et al. (2019). This process, partitioning 85% of data for training and 15% for validation, aligns with the standard practices in supervised learning. The neural network, characterized by two layers with a hidden size of 64 and an orthogonal initialization, was optimized for performance with a learning rate of 1e-4 and an Adam epsilon of 1e-8. Each model underwent a rigorous 120-epoch training regimen across four layouts and five seeds, reflecting a commitment to robustness and generalizability in agent training. The BCP agents, trained in tandem with BC partners, represent a novel amalgamation of cloning and playing strategies, with training curves depicted in Figure 10.

#### c.2.2 Self-Play (SP) and Fictitious Co-Play (FCP) Strouse et al. (2021)

The training of FCP agents, utilizing a pool size of 36 in the initial stage, was a strategic choice to ensure a diverse range of policy interactions. This diversity was further augmented by selecting five seeds from the first stage of FCP training for SP.

The second stage of training, involving a prolonged and intensive regimen over 50,000 episodes (amounting to 3e7 timesteps), was designed to refine and solidify the agents' strategies. Such extensive training is critical in environments characterized by high complexity and variability, as it allows agents to encounter and adapt to a wide array of scenarios. This comprehensive approach to training is evident in the detailed training curves presented in Figures 11 and 12, which provide insights into the progression and refinement of agent strategies over time.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Layouts** & **Meta-tasks** \\ \hline Cramped Room & 1. Place onion in pot \\  & 2. Deliver soup \\  & 3. Place onion and deliver soup \\  & 4. Others \\ Coordination Ring & 1. Place onion in pot \\  & 2. Deliver soup \\  & 3. Place onion and deliver soup \\  & 4. Others \\ Asymmetric Advantage & 1. Place onion in pot \\  & 2. Deliver soup \\  & 3. Place onion and deliver soup \\  & 4. Others \\ Soup Coordination & 1. Place tomato in pot \\  & 2. Deliver soup \\  & 3. Mixed order \\  & 4. Others \\ \hline \hline \end{tabular}
\end{table}
Table 4: Predefined rule-based meta-tasks.

Figure 9: Training curves of meta-task playing (MTP) agents over five random seeds. The shaded area denotes the standard deviation. Noticing that the reward should not be directly compared to each other because agents vary in the partners they train with.

## Appendix D Additional results

### Collaborating with rule-based agents with various policy switching frequencies

In this section, we delve deeper into the dynamics of collaboration with rule-based agents under different policy switching frequencies. We present a series of additional experiments to complement the findings discussed in Subsection 4.1. These experiments are critical in understanding how frequent policy shifts impact the overall performance and coordination in multi-agent environments.

Figure 13 illustrates the comparative performance when rule-based agents switch policies every 2 episodes. Notably, the frequent policy changes introduce a unique set of challenges and opportunities for adaptation, as evidenced by the performance fluctuations across 50 continuous episodes. The standard error shaded areas, based on five random seeds, highlight the variability in performance under these conditions.

Similarly, Figures 14 and 15 offer insights into the performance impacts when the policy switching occurs every 200 and 100 timesteps, respectively. These results are pivotal in understanding the optimal frequency of policy shifts to achieve efficient collaboration without overwhelming the learning agents with too frequent changes.

Figure 11: Training curves of FCP over five random seeds. The shaded area denotes the standard deviation. Noticing that the reward should not be directly compared to each other because the difficulty of the task varies with different game layouts.

Figure 10: Training curves of BCP agents over five random seeds. The shaded area denotes the standard deviation. Noticing that the reward should not be directly compared to each other because the difficulty of the task varies with different game layouts.

### Ablation study: collaborating with partners of diverse skill levels

In the following ablation study, we focus on the aspect of collaborating with partners exhibiting diverse skill levels. This study is vital to assess how agents adapt to varying competencies within a team setting. The results of this study are shown in Figures 16 and 17, where we examine different weights and behavioral queue lengths.

Figure 14: Comparative performance analysis against baselines when rule-based agents swift policies every _200 timesteps_. All agents were evaluated over 50 continuous episodes. The shaded areas denote standard errors over five random seeds.

Figure 12: Training curves of self-play agents over five random seeds. The shaded area denotes the standard deviation. Noticing that the reward should not be directly compared to each other because the difficulty of the task varies with different game layouts.

Figure 13: Comparative performance analysis against baselines when rule-based agents swift policies every _2 episodes_. All agents were evaluated over 50 continuous episodes. The shaded areas denote standard errors over five random seeds.

Figure 15: Comparative performance analysis against baselines when rule-based agents swift policies every _100 timesteps_. All agents were evaluated over 50 continuous episodes. The shaded areas denote standard errors over five random seeds.

In Figure 16, we explore the episodic rewards obtained by varying the weight \(\rho\) of the inter-episodic belief across three different layouts - _Coordination Ring_, _Asymmetric Advantage_, and _Soup Coordination_. Each layout presents a unique challenge and thus allows us to evaluate the adaptability of the agents to different team dynamics over 50 episodes. The 95% confidence intervals depicted here underscore the consistency of our findings.

Additionally, Figure 17 presents the effects of altering the length \(l\) of the human behavior queue. This modification helps us understand how the memory of past interactions influences current decision-making processes in different environmental layouts. The episodic rewards over 50 episodes, along with the error bars, provide a clear depiction of the performance trends under these varied conditions.

Figure 16: Episodic reward by using different weight \(rho\) of inter-episodic belief in other three layouts. All agents were evaluated over 50 episodes and error bars denote 95% confidence intervals.

Figure 17: Episodic reward by using different length \(l\) of human behavior queue in other three layouts. All agents were evaluated over 50 episodes and error bars denote 95% confidence intervals.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We summarized our contributions at the end of the introduction. Please see Section 1 Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed the limitations of the work in Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provided the theoretical result in Section 3.3 and complete proof in Appendix A. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have placed all the code for our algorithms and experiments in an anonymous repository ([https://github.com/AlexWanghaoming/CBPR](https://github.com/AlexWanghaoming/CBPR)) to facilitate the reproduction of our work. In addition, we provide the implementation details in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have placed all the code for our algorithms and experiments in an anonymous repository ([https://github.com/AlexWanghaoming/CBPR](https://github.com/AlexWanghaoming/CBPR)) to facilitate the reproduction of our work. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: we provide the implementation details (hyperparameters included) in Appendix C Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See section 4.3 Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the type of compute workers in section C Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Our research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our models do not have this kind of risk. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We provide MIT License of our released code. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [Yes] Justification: We provide the screenshots of the game and details about compensation in user study section. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.