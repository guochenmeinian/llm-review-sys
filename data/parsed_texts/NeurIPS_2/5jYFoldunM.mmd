# On the Adversarial Robustness of Benjamini Hochberg

Louis L Chen

Operations Research Department

Naval Postgraduate School

Monterey, CA 93943

louis.chen@nps.edu

&Roberto Szechtman

Operations Research Department

Naval Postgraduate School

Monterey, CA 93943

rszechtm@nps.edu

&Matan Seri

Operations Research Department

Naval Postgraduate School

Monterey, CA 93943

matan.seri@gmail.com

website: [https://louislichen.github.io/](https://louislichen.github.io/)

###### Abstract

The Benjamini-Hochberg (BH) procedure is widely used to control the false detection rate (FDR) in multiple testing. Applications of this control abound in drug discovery, forensics, anomaly detection, and, in particular, machine learning, ranging from nonparametric outlier detection to out-of-distribution detection and one-class classification methods. Considering this control could be relied upon in critical safety/security contexts, we investigate its adversarial robustness. More precisely, we study under what conditions BH does and does not exhibit adversarial robustness, we present a class of simple and easily implementable adversarial test-perturbation algorithms, and we perform computational experiments. With our algorithms, we demonstrate that there are conditions under which BH's control can be significantly broken with relatively few (even just one) test score perturbation(s), and provide non-asymptotic guarantees on the expected adversarial-adjustment to FDR. Our technical analysis involves a combinatorial reframing of the BH procedure as a "balls into bins" process, and drawing a connection to generalized ballot problems to facilitate an information-theoretic approach for deriving non-asymptotic lower bounds.

## 1 Introduction

Multiple testing has broad applications in drug discovery, forensics, candidate screening, anomaly detection, and in particular, machine learning. Indeed, recent works [5, 18, 28, 32], in nonparametric outlier detection, _out-of-distribution detection_ (OOD), and one-class classification have all adopted multiple testing methodology in developing principled decision rules with statistical guarantees. In fact, the Benjamini-Hochberg (BH) multiple testing procedure, widely used to control the _false detection rate_ (FDR), is either used or modified in all these recent methods. Considering this FDR control could be relied upon in some critical (safety/security) contexts, for which false positives incur costs, we investigate its adversarial robustness.

Adversarial corruption presents a challenge to statistical methodology, and is a modern-day concern due to not only the ease with which high volumes of data can now be accessed/processed but also the increasingly widespread use of statistical procedures. This threat poses vulnerabilities to machine learning tasks like OOD, which would aim to fortify security systems like fraud detection [5].

Manipulation of data and experimental results are common means by which incorrect conclusions can be reached. Worse, strategic perturbation can dramatically decrease the fidelity of the models and methods used. A burgeoning field of adversarial corruption has gained traction in recent years to meet this concern, most notably in the area of (deep) machine learning; see, for example, [27; 16; 20]. In this work we address adversarial corruption in hypothesis testing, specifically in the large-scale context in which the primary focus is on the aggregate metric: FDR.

BH [6] is one of the most widely used multiple testing procedures, which upon input of a collection of p-values, outputs a rejection region ensuring that the FDR is no greater than a user-defined threshold \(q\in(0,1)\). This control of FDR holds under independently generated p-values - as well as some restricted forms of dependency like _positive regression dependent on a subset_ (PRDS) [7] - but it generally holds without strong assumptions on the alternative distributions. This degree of distributional robustness, however, could be said to come at the cost of adversarial robustness, as we show in this work.

### Literature Review

Although OOD methods [23; 12; 21; 22] are often complex and not always supported by statistical guarantees, conformal inference has made possible the use of one-class classifiers to generate conformal p-values for which OOD can now be conducted via multiple testing. This has led to the adoption of the BH procedure in OOD. Indeed, [5] leverages the FDR control afforded by BH over conformal p-values (shown to be PRDS) to test for outliers. More precisely, given a test set of observations for which we wish to identify as inliers or outliers (out of distribution), a conformal p-value is generated for each observation, which is then processed by BH to decide which are likely outliers. We refer the reader to [18; 28] for other recent works along this vein.

In recent years, concerns have risen over the possibility of adversarial manipulation of statistical methodologies. This manipulation commonly occurs at the level of data collection and training, often invalidating the assumptions made regarding how data is drawn, but it can also occur at test time. There is a growing literature on _adversarial robustness_, which is concerned with securing statistical methods like (deep) machine learning [27; 16; 20], linear regression [8], M-estimation [9], and online learning [26; 17; 1]. In particular, [13] considers contamination models that incorporate (adaptive) adversarial perturbation of up to an \(\epsilon-\) fraction of drawn data. Indeed, we adopt this modeling in our own study - see (c-Perturb). As well, a similar concept to the notion of adversarial robustness that we adopt in this paper is one the literature refers to as _perturbation resilience_. Generally speaking, a problem instance is called \(\alpha-\) perturbation resilient when despite a degree (parameterized by \(\alpha\)) of perturbation to the instance, the optimal solution does not change. First introduced in [10] for combinatorial optimization (in particular, MAX-CUT), the concept has since also inspired research into devising resilient unsupervised learning, particularly in clustering [4; 3; 2].

With respect to the hypothesis testing literature, there are recent adversarial robust studies focused on simple [19] and sequential hypothesis testing [11] from a game theoretic perspective, in which protection of statistical power, risk, or sample size from corruption is of chief concern. Complementing the adversarial robust perspective are several distributionally robust studies, in which the data-generating distribution is known only to lie in a parametric family. Recent works include [11; 24; 15] which focus on test risk in single and sequential hypothesis testing settings employing uncertainty sets of distributions of fixed distance (e.g. Wasserstein, phi-divergence) for the null and alternative hypotheses. In contrast to these works, this paper is focused on FDR, not individual test risk. Furthermore, distributional robustness is not equivalent to the perturbation-robustness that this paper and other adversarial robust studies seek in general. Indeed, [30] shows that the BH procedure's FDR control exhibits a distributional robustness to possible dependence between null and non-null hypotheses. On the other hand, our work would illustrate that, distributional robustness aside, BH can lack adversarial robustness.

### Preliminaries

Let \(\mathcal{N}:=\{1,\ldots,N\}\), where \(N\in\mathbb{Z}_{+}\), be a finite set for which each member \(i\in\mathcal{N}\) denotes a binary hypothesis test deciding between a null and alternative hypothesis. Further, there exists a partitioning, \(\mathcal{N}=\mathcal{H}_{0}\uplus\mathcal{H}_{1}\), such that the correct decision for test \(i\in\mathcal{N}\) is either null if \(i\in\mathcal{H}_{0}\), or alternative if \(i\in\mathcal{H}_{1}\). Here, \(\mathcal{H}_{0}\) and \(\mathcal{H}_{1}\) are the (unknown) sets of null and alternative test indices, respectively;consequently, for any test \(i\), the correct decision (i.e. set membership) is unknown to the decision maker. In fact, while the number of tests \(N\) is known (and large, on the order of thousands), neither \(N_{0}:=|\mathcal{H}_{0}|\) nor \(\pi_{0}:=\frac{N_{0}}{N}\) is known to the decision maker, although \(\pi_{0}\geq 0.90\) "is reasonable in most large-scale testing situations" - ([14], p. 285).

For each test \(i\in\mathcal{N}\), p-value \(p_{i}\in[0,1]\) is randomly generated (independent of all other \(p_{j}\), \(j\neq i\)), which we model as a draw from either \(U(0,1)\) when \(i\in\mathcal{H}_{0}\) (\(p_{i}\) then referred to as a _null p-value_) or some alternative distribution \(\mathbb{P}^{1}_{i}\) on \([0,1]\) when \(i\in\mathcal{H}_{1}\) (\(p_{i}\) then referred to as an _alternative p-value_). A multiple-testing algorithm \(\mathcal{A}\) takes as input a randomly generated collection of p-values \(p=\{p_{i}\}_{i\in\mathcal{N}}\) and outputs for each test \(i\) a determination \(\mathcal{A}(i)\in\{0,1\}\) with \(\mathcal{A}(i)=1\) iff the determination is to reject the null hypothesis (i.e., claim \(i\in\mathcal{H}_{1}\)), or sometimes referred to as "make a discovery" for the \(i\)-th test. With \(a_{p}:=\sum_{i\in\mathcal{H}_{0}}\mathcal{A}(i)\) denoting the number of false discoveries, and \(R_{p}:=|\mathcal{A}^{-1}(1)|\) denoting the number of rejections/discoveries made, we refer to \(FDP[\mathcal{A};p]:=\frac{a_{p}}{R_{p}\lor 1}\) as the false detection proportion summarizing \(\mathcal{A}\)'s decisions on p-values \(p\), where \(x\lor y\) is shorthand for \(\max(x,y)\) for any \(x,y\in\mathbb{R}\). We refer to its expectation with respect to the random generation of \(p\) as the _false detection rate_, \(FDR(\mathcal{A}):=\mathbb{E}_{p}FDP[\mathcal{A};p]\).

In this work, we focus on the Benjamini Hochberg procedure, a widely-used multiple-testing algorithm.

### The Benjamini Hochberg (BH) Procedure

Given a collection of p-values \(p=\{p_{i}\}_{i\in\mathcal{N}}\) and desired control level \(q\in(0,1)\) to bound FDR, the BH procedure \(BH_{q}\) operates as follows:

1. The p-values are sorted in increasing order, \(p_{(1)}\leq p_{(2)}\leq\ldots\leq p_{(N)}\).
2. The index \(i_{\max}:=\max\Bigl{\{}i\in[0,N]_{\mathbb{Z}}:p_{(i)}\leq i\frac{q}{N}\Bigr{\}}\) is identified, with \(p_{(0)}:=0\).
3. Reject the tests corresponding to the smallest \(i_{\max}\) p-values: \(p_{(1)},p_{(2)},\ldots,p_{(i_{\max})}\).

\(BH_{q}\) provides provable FDR control at the level of \(q\) without any assumptions on the alternative distributions \(\{\mathbb{P}^{1}_{i}\}_{i\in\mathcal{H}_{1}}\).

**Lemma 1.1** (Theorem 4.1 from [14]).: _If every null p-value is super-uniform, equiv., \(p_{i}\sim\mathbb{P}^{0}_{i}\geq U(0,1)\) for all \(i\in\mathcal{H}_{0},\) and the collection is jointly independent, then regardless of the collection of alternative distributions \(\{\mathbb{P}^{1}_{i}\}_{i\in\mathcal{H}_{1}}\),_

\[FDR(BH_{q})=\pi_{0}q\leq q,\ \ \ \ \forall q\in(0,1). \tag{1}\]

_Remark 1.2_.: In fact, the assumption of joint independence of the null p-values can be relaxed to a form of dependency known as PRDS - see [7] for this generalization of Lemma 1.1.

### The Adversary and the c-Perturbation Problem

We model an (_omniscient_) adversary with knowledge of \(\mathcal{H}_{0}\), \(\mathcal{H}_{1}\), and that knows the decision maker's choice of control level \(q\). The adversary receives the p-values \(p=\{p_{i}\}_{i=1}^{N}\)_after_ they are generated but _before_ they are received by the decision maker, or before test time. Given the ability to perturb \(c\geq 1\) p-values, the adversary solves

\[\max_{p^{\prime}:|p-p^{\prime}|_{0}\leq c}FDP[BH_{q};p^{\prime}],\] (c-Perturb)

where \(p^{\prime}\) denotes the p-values derived from an adjusted collection of p-values \(p^{\prime}=(p^{\prime}_{i})_{i=1}^{N}\). In words, the adversary finds the perturbation of at most \(c\)-many p-values before the execution of \(BH_{q}\) so as to maximize the _adversarially-adjusted_ false detection proportion \(FDP[BH_{q},p^{\prime}]\). Perturbation of p-values is implicitly the result of data perturbation, and we refer to both Remark 4.6 and Section 5.2 for examples and experiments involving direct perturbation of data.

While we assume omniscience for the adversary throughout, we will briefly address modifications in analysis for an _oblivious_ adversary that has no knowledge of \(\mathcal{H}_{0}\), nor \(\mathcal{H}_{1}\). Indeed, our algorithms to be presented can be modified naturally for implementation by an oblivious adversary (see comments in Section 3); further, there is nearly equivalent performance when \(\pi_{0}\) is large, as is typical. Hence, it is for the sake of brevity that we omit explicit analysis of the oblivious adversary.

### Main Results

Intuitively, the effect of a small number of p-value perturbations becomes insignificant in settings where a large number of tests are rejected (see Theorem 3.2). This happens, for instance, when either the number of tests \(N\), or the control level \(q\), or the distance (e.g. KL-divergence) between the null and alternative distributions, is large. For this reason, we focus on results that are non-asymptotic in the number of tests \(N\).

In Section 3, we present the algorithm INCREASE-c that uses strategic increases to \(c\) null p-values to induce expansion of the BH rejection region. We also present an efficient, optimal algorithm MOVE-1 (Appendix Section 7.2) for the adversary's maximization of FDR with at most one (i.e. \(c=1\)) p-value perturbation.

In Section 4 we discuss the adversarial robustness of BH through study of the adversarial adjustments to FDR by the INCREASE-c algorithms, revealing where its control is and isn't adversarially robust.

In Section 5 we provide accompanying numerical experiments on i.i.d. as well as PRDS p-values.

## 2 BH as Balls into Bins

In this section we will establish important notation for the discussions to follow. We reduce the real line to a collection of \(N+1\) "bins". \(N_{0}\) and \(N_{1}\) balls will each be assigned to one of these bins independently of each other, from discrete distributions that are specified in the next subsection. The main motivation for this reduction is to facilitate discussion of effective perturbations in Section 3, and for the technical analysis in Section 4.

### The Balls into Bins System

We partition the segment \([0,1]\) into \(N\) equiprobable segments that will be referred to as _bins_. We define the \(i\)-th bin \(B_{i}:=\left\{p\in\mathbb{R}:(i-1)\frac{q}{N}\leq p<i\frac{q}{N}\right\}\), for \(i=1,\ldots,N\). What remains forms bin \(N+1\), i.e., \(B_{N+1}:=\left\{p\in\mathbb{R}:1\geq p\geq 1-q\right\}\). For shorthand, we write \(B_{1:i}:=\cup_{i=1}^{i}B_{l}=\left\{p\in\mathbb{R}:0\leq p<i\frac{q}{N}\right\}\), Finally, we write \(B_{i}^{0}:=\left|B_{i}\cap\left\{p_{j}\right\}_{j\in\mathcal{H}_{0}}\right|\) and \(B_{i}^{\mathcal{N}}:=\left|B_{i}\cap\left\{p_{j}\right\}_{j\in\mathcal{N}}\right|\) for bin \(i\)'s, respectively, _null-load_ and _total load_. The _alternative- load_\(B_{i}^{1}\) is defined analogously, as are \(B_{1i}^{0}\)\(B_{1i}^{\mathcal{N}}\), and \(B_{1i}^{1}\).

Rejection Count:Borrowing terminology from the classic _balls into bins_ problem of probability theory [29], this framework facilitates a re-interpretation of the random drawing of p-values as balls being randomly placed into an ordered collection of bins, enumerated 1 up to \(N+1\). Framed in this way, we see that \(BH_{q}\) operates by identifying the _rejection count_

\[\tilde{k}=\max\Bigl{\{}i\in[0,N]_{\mathbb{Z}}:B_{1:i}^{\mathcal{N}}=i\Bigr{\}}, \tag{2}\]

which corresponds to the largest collection of consecutive bins \(1,\ldots,i\) that collectively contain precisely \(i\) balls, so that \(BH_{q}\) rejects all tests with p-values lying in the first \(\tilde{k}\) bins. The case \(\tilde{k}=0\) corresponds to rejecting no tests. In fact, \(\tilde{k}\) is a stopping time under a filtration \(\mathcal{F}\) that we define next.

Filtration \(\mathcal{F}=\{\mathcal{F}_{i}\}_{i=0}^{N}\):Let \(\Omega:=[0,1]^{N}\) be a sample space with the classical Borel \(\sigma\)- algebra \(\mathcal{B}\) and (with slight abuse of notation) probability measure \(\mathbb{P}:=(\otimes_{i\in\mathcal{H}_{0}}U(0,1))\otimes\bigl{(}\otimes_{i \in\mathcal{H}_{1}}\mathbb{P}_{i}^{1}\bigr{)}\). We define a filtration beginning with \(\mathcal{F}_{N}:=\sigma(B_{N+1}^{\mathcal{N}},B_{N+1}^{0})\), and continuing inductively (\(N\) towards \(0\)), let \(\mathcal{F}_{i}\) be the \(\sigma\)-algebra generated by \(\{B_{i}^{\mathcal{N}}\}_{i=i+1}^{N+1}\) and \(\{B_{i}^{0}\}_{i=i+1}^{N+1}\). In words, this filtration corresponds to what is cumulatively learned about the bin loads (null and total) upon examination of the bins in sequence starting with bin \(N+1\) and concluding with bin \(1\), assuming each observed p value comes with correct identification of whether or not \(i\in\mathcal{H}_{0}\).

We note the fact that for \(\ell>i\), it follows that \(E\Bigl{[}\frac{B_{1i}^{0}}{i}\Bigr{\|}\mathcal{F}_{\ell}\Bigr{]}=E\Bigl{[} \frac{B_{1i}^{0}}{i}\Bigr{\|}\frac{B_{1i}^{0}}{\ell}\Bigr{]}=\frac{B_{1i}^{0} }{\ell}\) a.s., so that \(\frac{B_{1i}^{0}}{N},\frac{B_{1i}^{0}=1}{N-1},\ldots,\frac{B_{1i}^{0}}{1}\) form a martingale sequence adapted to the filtration. This fact will prove useful when combined with the optional stopping theorem to facilitate several results in this work.

Under this lens, the adversary's (algorithmic) task reduces to reshuffling p-values among the bins, and in Section 3 we demonstrate there are indeed simple, tractable ways of performing this to manipulateBH, with potentially great effect on FDR control. The key insight is that there exist alternative stopping times that present alternative rejection counts (/regions) that can break FDR control.

## 3 Adversarial Algorithm: INCREASE-c

Throughout this section, we don the role of the adversary and study the c-Perturb problem, in which we are given \(q\in(0,1)\), a realized collection \(p=\{p_{i}\}_{i\in\mathcal{N}}\) (along with the labels of null or alternative for each \(p_{i}\)), and a budget \(c\geq 1\), and our task is to produce a perturbed collection \(p^{\prime}\). Toward this, we focus on a procedure called INCREASE-c that despite its sub-optimality (see Appendix Section 7.2) is intuitive and simple to execute; further, as theoretical and empirical analysis in sections 4 and 5 respectively show, it has strong performance in expectation.

We begin by defining a random variable that is a stopping time adapted to \(\mathcal{F}\); given an integer \(c\geq 1\), let

\[\tilde{k}_{+c}:=\begin{cases}\max\{i\in[c,N]_{\mathbbm{Z}}:B^{\mathcal{N}}_{1: i}=i-c\}&B^{0}_{N+1}\geq c\\ \tilde{k}&B^{0}_{N+1}<c,\end{cases} \tag{3}\]

and we choose to write \(\tilde{k}_{+}\) in place of \(\tilde{k}_{+1}\).

Increasing the Rejection CountThe interest in \(\tilde{k}_{+c}\) is that if we moved any selection of \(c\) null p-values from bin \(N+1\) into bin \(\tilde{k}_{+c}\) (in fact any bin \(i\leq\tilde{k}_{+c}\)), then \(BH_{q}\) would output a new, increased rejection count \(\tilde{k}_{+c}\). We formally study this in Section 4. In the meantime, we comment on the increase \(\tilde{k}_{+c}-\tilde{k}\), which is a difference between two stopping times

It is easy to see that \(\tilde{k}_{+c}-\tilde{k}\geq c\) whenever \(B^{0}_{N+1}\geq c\); hence, the increase in the rejection count is at least \(c\), but possibly more. We provide a stronger lower bound on this increase by utilizing the ratio between the number \(B^{0}_{k+2:N}\) of nulls not rejected by \(BH_{q}\) and the number \(N-(\tilde{k}+1)\) of bins left outside of the \(BH_{q}\) rejection region in the case of no corruption. Computational experiments indicate comparable performance of this bound with those of simulations presented in Section 5's Table 1.

**Theorem 3.1**.: _If \(c\geq 1,\) then_

\[\mathbb{E}\left[\tilde{k}_{+c}-\tilde{k}\|B^{0}_{N+1}\geq c\right]\geq\frac{c- 1}{1-\mathbb{E}\left[\frac{B^{0}_{k+2:N}}{N-(k+1)}\ \parallel\ B^{0}_{N+1}\geq c \right]}+1 \tag{4}\]

_for any collection of alternative hypothesis distributions \(\{\mathbb{P}^{1}_{i}\}_{i\in\mathcal{H}_{1}}\)._

INCREASE-c runs as follows:

1. IF \(B^{0}_{N+1}\geq c\), then move the largest \(c\) (ties broken arbitrarily) in the (N+1)-th bin to bin \(\tilde{k}_{+c}\).
2. ELSE leave the p-values unperturbed.

It in fact suffices for the \(c\)-many p-values to be placed in any bin \(i\leq\tilde{k}_{+c}\). We remark that since an oblivious adversary cannot discern null-drawn from alternative-drawn in the collection \(p\), INCREASE-c as written is unimplementable in such a case. Hence, for the oblivious adversary, we modify INCREASE-c's criterion to \(B^{\mathcal{N}}_{N+1}\geq c\) and have the oblivious adversary now take the \(c-\)many p-values uniformly at random from among the p-values in the \((N+1)\)-th bin. Intuitively, this modification for the oblivious adversary should yield nearly \(c\) null p-values being moved (on average) just as in the non-oblivious case, assuming the proportion of nulls among the \(B^{\mathcal{N}}_{N+1}\) - many p-values is high, as a typically large \(\pi_{0}\) would entail.

We conclude this section with a characterization of the average increase in FDR, denoted \(\Delta_{c}\) that INCREASE-c induces.

**Theorem 3.2**.: _Given \(c\geq 1,\) let \(p_{+c}\) denote the perturbed form of \(p\) that INCREASE-c produces. Then the adversarially-adjusted FDR induced by INCREASE-c is_

\[\mathbb{E}FDP[BH_{q};p_{+c}]=\mathbb{E}FDP[BH_{q};p]+\Delta_{c},\]

_for any collection of alternative distributions \(\{\mathbb{P}^{1}_{i}\}_{i\in\mathcal{H}_{1}}\), where_

\[\Delta_{c}:=\mathbb{E}\left[\frac{c}{\tilde{k}_{+c}};B^{0}_{N+1}\geq c\right]. \tag{5}\]In Section 4 to follow, we provide analytical lower bounds for \(\Delta_{c}\) as part of a discussion on BH's adversarial robustness. Section 5 presents computational experiments (e.g. Table 1). We remark that INCREASE-c is not optimal for all instances of c-Perturb; indeed, for \(c=1\), we present a provably optimal algorithm MOVE-1 in Appendix Section 7.2, which in contrast to INCREASE-1 sometimes induces a reduced rejection count. However, INCREASE-c remains a formidable adversarial procedure, as the results of Section 5 demonstrate on not only i.i.d. p-values but also PRDS conformal p-values.

## 4 Theoretical Analysis: Performance Guarantees and Insights into Adversarial Robustness

BH's FDR control - Lemma 1.1 - is (distributionally) robust in the sense that it holds no matter the alternative distributions \(\{\mathbb{P}_{i}^{1}\}_{i\in\mathcal{H}_{1}}\). However, as Theorem 3.2 indicates, the degree to which this control can withstand data perturbations at test time, i.e., its adversarial robustness, very much depends on \(\{\mathbb{P}_{i}^{1}\}_{i\in\mathcal{H}_{1}}\).

Recalling Lemma 1.1, we may assume without loss of generality that no alternative distribution \(\mathbb{P}_{i}^{1}\) stochastically dominates \(U(0,1)\) (equiv., \(\mathbb{P}_{i}^{1}\succeq U(0,1)\)). That being said, the "degree" to which the alternative distributions \(\{\mathbb{P}_{i}^{1}\}_{i\in\mathcal{H}_{1}}\) are (stoch.) dominated by the null distribution \(U(0,1)\) (equiv., \(\mathbb{P}_{i}^{1}\preccurlyeq U(0,1)\)) is critical. We briefly preview two regimes of special interest for which each of the next two subsections cover.

**High sub-uniformity:** When the alternatives are sub-uniform \(\mathbb{P}_{i}^{1}\preccurlyeq U(0,1)\) for all \(i\in\mathcal{H}_{1}\), and highly so, such that for all \(i\in\mathcal{H}_{1}\) it holds that \(\mathbb{P}_{i}^{1}\left(p_{i}<\epsilon\right)\approx 1\) for some small \(\epsilon>0\), then it follows that \(\tilde{k}\) is large and \(B_{1:\tilde{k}}\) should contain most alternative p-values. Consequently, in order for INCREAES-c to induce any sizeable increase to the FDR, the adversary will need to expand the BH rejection region significantly so as to introduce a commensurate number of nulls. Table 1 indicates \(c\) may need to be quite large to make a dent in FDR control. This message is made more precise in Theorem 4.1.

**Low sub-uniformity:** As we will see, when the alternative p-values are barely dominated by \(U(0,1)\), \(\Delta_{c}\) can be rather large. In fact, in the special case that there is no dominance such that \(\mathbb{P}_{i}^{1}=U(0,1)\) for all \(i\in\mathcal{H}_{1}\), a strikingly vulnerable state occurs with high probability. Indeed, in this case where nulls and alternatives are virtually indistinguishable, \(BH_{q}\) (in fact any \(\mathcal{A}\)) admits an FDR of \(\pi_{0}\) whenever any rejections are made (i.e., \(\mathbb{E}\left[FDP[BH_{q};p]]\|\tilde{k}\geq 1\right]=\pi_{0}\)) so that \(BH_{q}\) accordingly compensates by making no rejections with high probability (\(\mathbb{P}\left(\tilde{k}=0\right)=1-q\)), which follows by the distributional robust control (1) from Lemma 1.1. But those times when \(\tilde{k}=0\) is precisely when INCREASE-c's simultaneous expansion of the rejection region and injection of nulls into this region is most damaging. That this event and other similarly vulnerable events occurs with high probability is the fault of the distributional robustness. This message is made rigorous in the forthcoming Theorem 4.5.

### Case of High Sub-Uniformity in Alternatives \(\{\mathbb{P}_{i}^{1}\}_{i\in\mathcal{H}_{1}}\)

If \(\mathbb{P}_{i}^{1}\preccurlyeq U(0,1)\) for all \(i\in\mathcal{H}_{1},\) with \(\mathbb{P}_{i}^{1}\left(p_{i}<\epsilon\right)\approx 1\) for some small \(\epsilon>0\), then it is clear that the number of alternatives rejected by \(BH_{q}\) should be nearly the maximum number \(N_{1}\) of correct rejections possible (i.e., \(B_{1:\tilde{k}}^{1}\approx N_{1}\)) with high probability, limiting any potential impact of INCREASE-c.

The following bounds are formulated to elaborate on such dynamics in this case of large separation between alternatives and nulls, for which the event \([B_{1:c}^{1}=N_{1}]\) has probability close to 1.

**Theorem 4.1**.: _If \(c\geq 1,\) then_

\[\Delta_{c}\leq\mathbb{P}\left(B_{1:c}^{1}=N_{1}\right)\mathbb{E} \left[\frac{c}{c+N_{1}+B_{1:N_{1}+c}^{0}}\|B_{N+1}^{0}\geq c\right]+1-\mathbb{ P}\left(B_{1:c}^{1}=N_{1}\right)\]

_and_

\[\mathbb{E}\left[\tilde{k}_{+c}\|B_{N+1}^{0}\geq c\right]\geq \frac{(N_{1}+c)\cdot\mathbb{P}\left(B_{1:c}^{1}=N_{1}\right)}{1-\mathbb{E} \left[\frac{B_{1:N}^{0}}{N}\|B_{1:N}^{0}\leq N_{0}-c\right]}. \tag{6}\]In words, for fixed \(c\), as the alternative distributions concentrate more and more on \(0\), it follows that \(\mathbb{P}\left(B^{1}_{1:c}=N_{1}\right)\uparrow 1,\) so that the effect \(\Delta_{c}\) of INCREASE-c on BH's FDR is dampened. And this occurs despite the fact that the increase \(\tilde{k}_{+c}-\tilde{k}\) in rejection count produced by INCREASE-c consists of mostly the introduction of nulls, and tends to a magnification of \((N_{1}+c)\) by at least a factor of the inverse of \(1-\mathbb{E}\left[\frac{B^{0}_{1:N}}{N}\|B^{0}_{1:N}\leq N_{0}-c\right],\) which is straightforward to compute since \(B^{0}_{1:N}\sim Binom(N_{0},q)\). We refer the reader to Section 5's Table 1 for simulations illustrating the above.

### Case of Low Sub-Uniformity in Alternatives \(\{\mathbb{P}^{1}_{i}\}_{i\in\mathcal{H}_{1}}\)

In the study of this regime, we aim to demonstrate that the adversarial increase \(\Delta_{c}\) can be rather large. We provide a lower bound \(L_{c}\) on \(\Delta_{c}\) that will be a function of parameters \(q,N,N_{0}\), as well as the alternative distributions \(\{\mathbb{P}^{1}_{i}\}_{i\in\mathcal{H}_{1}}\).

#### 4.2.1 Lower Bounding \(\Delta_{c}\)

In view of \(5\), the event \([\tilde{k}_{+c}=c]\) is clearly of significance in the computation of \(\Delta_{c}\). In words, this event describes the case that the \(BH_{q}\) rejection region captures nothing, and yet when the adversary successfully executes INCREASE-c the rejection will now capture only nulls - generating a false detection proportion of 1. Hence, we lower bound the adjustment \(\Delta_{c}\) of Theorem 3.2 by lower-bounding the probability of this event.

Our strategy: (1) first, characterize this probability under the special case that \(\mathbb{P}^{1}_{i}=U(0,1)\) for all \(i\in\mathcal{H}_{1}\); (2) second, to handle when \(\mathbb{P}^{1}_{i}\leq U(0,1)\) for some \(i\in\mathcal{H}_{1}\), we translate the KL divergence of the resulting discrete, bin-assignment distributions into a bound via Pinsker's Inequality.

The key to the first step will be to recognize that when \(\mathbb{P}^{1}_{i}=U(0,1)\) for all \(i\in\mathcal{H}_{1}\), the vector of total loads \((B^{\mathcal{N}}_{1},\ldots,B^{\mathcal{N}}_{N})\) is exchangeable (i.e., its law is invariant under permutations), given \(B^{\mathcal{N}}_{1:N}\). Indeed, exchangeability, combined with the generalized Ballot Theorem of [31] yields the following result:

**Corollary 4.2**.: _Let \(n\geq 1\), \(p\in[0,1],\) and \(x\) a non-negative integer such that \(0\leq x\leq n\). If \(\tilde{B}=(\tilde{B}_{1},\ldots,\tilde{B}_{n})\sim Multinomial\Big{(}x,(p, \ldots,p)\Big{)}\), then \(\mathbb{P}_{\tilde{B}}\left(\cap_{r=1}^{n}\left[\sum_{i=1}^{r}\tilde{B}_{i}<r \right]\right)=1-\frac{x}{n}.\)_

Armed with Corollary 4.2, we can begin to estimate the probability laws of \(\tilde{k}\) and \(\tilde{k}_{+c}\).

**Corollary 4.3**.: _If \(\mathbb{P}^{1}_{i}=U(0,1)\) for all \(i\in\mathcal{H}_{1}\), then \(\mathbb{P}\left(\tilde{k}=\ell\right)=\left(\frac{1-q}{1-q+\frac{N_{0}-q}{1-q +}}\right)\cdot\mathbb{P}\left(B^{\mathcal{N}}_{1:\ell}=\ell\right)\) for \(\ell=0,\ldots,N,\) where \(0^{0}=1\), and \(\mathbb{P}\left(B^{\mathcal{N}}_{1:\ell}=\ell\right)=\binom{N}{\ell}\binom{ \frac{q}{N}}{N}\binom{1-\frac{q\ell}{N}}{N-\ell}^{N-\ell}.\)_

_If \(B^{0}_{N+1}\geq c\), then \(\mathbb{P}\left(\tilde{k}_{+c}=c\ \parallel\ B^{0}_{N+1}\right)=\left(1-\frac{cq}{ N}\right)^{N_{1}}\cdot\left(1-\frac{c}{N}\right)^{N_{0}-B^{0}_{N+1}}\left(1- \frac{N_{0}-B^{0}_{N+1}}{N-c}-\frac{N_{1}q}{N-cq}\right)\)._

Next, we carry out the second step of our plan. Towards this, we make the following assumption.

**Assumption 4.4**.: Suppose the alternative p-values are independent and identically distributed, with common distribution \(\mathbb{P}^{1}\), i.e., \(\{p_{i}\}_{i\in\mathcal{H}_{1}}\overset{iid}{\sim}\mathbb{P}^{1}\). We write \(\delta:=(1-q)-\mathbb{P}^{1}\left(p_{i}\in B_{N+1}\right)\), and \(\delta_{j}:=\mathbb{P}^{1}\left(p_{i}\in B_{j}\right)-\frac{q}{N}\), and we assume that \(\mathbb{P}^{1}\left(p_{i}\in B_{j}\right)>0\) for all \(j\in[N]\), as well as \(\mathbb{P}^{1}\left(p_{i}\in B_{E:N}\right)>0\) for arbitrary \(\ell<N\).

This assumption not only reduces the notational burden (otherwise documenting \(N_{1}\) many alternative distributions) but it also allows the leveraging of KL divergences between the different bin assignment distributions implied by the alternative \(\mathbb{P}^{1}\) versus the null \(U(0,1)\).

**Theorem 4.5**.: _Suppose Assumption 4.4. Then_

\[\Delta_{c}\geq L_{c}(q,N,N_{0},\mathbb{P}^{1}):=\left(1-\frac{cq}{N}-\delta_{1: c}\right)^{N_{1}}\left[\Big{(}1-\frac{c}{N}\Big{)}^{N_{0}}\left(1-\pi_{c}-V_{c} \right)M_{c}+Z_{c}\right]\]

_where \(\pi_{c}(q,N,N_{0},\mathbb{P}^{1}):=\frac{N_{0}+\mathbb{E}\left[B^{1}_{c+1:N} \right]\|B^{1}_{1:c}=0}{N-cq-N\delta_{1:c}},\)\(\mathbb{E}\left[B^{1}_{c+1:N}\right]\|B^{1}_{1:c}=0\big{]}=N_{1} \frac{(N-c)q+N\delta_{\delta_{\pm 1:N}}}{N-cq-N\delta_{1:c}},\)\(V_{c}(q,N,N_{0},\mathbb{P}^{1}):=\sqrt{\frac{ln2}{2}\mathbb{E}\left[B^{1}_{c+1:N}\right]\|B^{1}_{1:c}=0 \big{]}D_{KL}(\mathbb{P}^{1},c),\)_\[M_{c}(q,N,N_{0}):=\frac{N-cq}{N-c}-\mathbb{E}\left[\left(1-\frac{c}{N} \right)^{-B_{N+1}^{0}};B_{N+1}^{0}\leq c-1\right],\] \[Z_{c}(q,N,N_{0}):=\mathbb{P}\left(B_{N+1}^{0}\geq c\right)\left(1- \frac{c}{N}\right)^{N_{0}-\mathbb{E}\left[B_{N+1}^{0}\|B_{N+1}^{0}\geq c\right] }\frac{\mathbb{E}\left[B_{N+1}^{0}\|B_{N+1}^{0}\geq c\right]}{N-c}\] \[D_{KL}(\mathbb{P}^{1},c):=\sum_{j}\frac{1}{N-c}log\left(\frac{q+ \delta-\sum_{j=1}^{c}(q/N+\delta_{j})}{(N-c)(q/N+\delta_{j})}\right)\]

_Remark 4.6_.: For a more concrete application/example, consider when the p-values \(\{p_{i}\}_{i\in\mathcal{N}}\) are derived from z-scores \(\{z_{i}\}_{i\in\mathcal{N}}\) via \(p_{i}=\mathbb{P}(Z>z_{i})\), where \(Z\sim N(0,1)\), with null z-scores \(\{z_{i}\}_{i\in\mathcal{H}_{0}}\) i.i.d. \(N(0,1)\) and alternative z-scores \(\{z_{i}\sim N(\mu_{1}^{i},1)\}_{i\in\mathcal{H}_{1}}\) (with all \(\mu_{1}^{i}\geq 0\)). Indeed, under this framework, we may view the \(\mu_{1}^{i}\) as the distance between \(\mathbb{P}_{1}^{1}\) and \(U(0,1)\).

Towards satisfying Assumption 4.4, we can assume throughout that there exists a nonnegative parameter \(\mu_{1}\) such that the alternative parameters \((\mu_{1}^{i},\sigma^{i})=(\mu_{1},1)\) for all \(i\in\mathcal{H}_{1}\). Indeed, if every member of the collection \(\{\mu_{1}^{i}\}_{i\in\mathcal{H}_{1}}\) is in reality only close to zero, then the forthcoming estimates (lower bounds) would provide close, conservative estimates when setting \(\mu_{1}=\max\{\mu_{1}^{i}:i\in\mathcal{H}_{1}\}\).

When \(\mu_{1}>0\), it follows that \(\delta(\mu_{1}):=(1-q)-\Phi\left(\frac{\Phi^{-1}(1-q)-\mu_{1}}{\sigma_{1}} \right)>0\). As well, \(\delta_{j}(\mu_{1}):=\Phi\left(\frac{\Phi^{-1}(1-\frac{(j-1)q}{N})-\mu_{1}}{ \sigma_{1}}\right)-\Phi\left(\frac{\Phi^{-1}(1-j\frac{q}{N})-\mu_{1}}{\sigma_ {1}}\right)-\frac{q}{N}\). Intuitively, BH is adversarially robust (\(\Delta_{c}\) small) when the alternative distributions are "far" (\(\mu_{1}>>0\)) from the theoretical null, and the opposite holds when they are "close" (\(\mu_{1}=0\)) - see Figures 2 and 3.

## 5 Simulations and Data Experiments

In this section, we provide computations (performed in R and Python on a Macbook Air-M2 chip, 8GB memory, with no experiment time exceeding 5 minutes) to demonstrate the performance of the adversarial algorithm INCREASE-c. We demonstrate its performance through simulation on synthetic data to make comparisons to the theoretical estimates provided in Section 4. We then demonstrate its performance on a real-data experiment in outlier detection.

### INCREASE-c Simulations on Synthetic Data

#### 5.1.1 INCREASE-c Simulations on i.i.d. p-values

Following the framework from Remark 4.6, we simulated \(10^{4}\) replications of the following experiment: (1) \(N=1000\) p-values are generated, with \(\{p_{i}\}_{i\in\mathcal{N}_{0}}\stackrel{{ iid}}{{\sim}}U(0,1)\), and each \(p_{i}\) among \(\{p_{i}\}_{i\in\mathcal{N}_{1}}\) generated via \(p_{i}=1-\Phi\left(\frac{z_{i}-\mu_{1}}{1}\right)\), with \(\{z_{i}\}_{i\in\mathcal{N}_{0}}\stackrel{{ iid}}{{\sim}}N(\mu_{1},1)\); (2) \(FDP[BH_{q};p]\) and \(FDP[BH_{q};p_{+c}]\) are calculated. In Figure 1 each of the \(10^{4}\)- many (\(FDP[BH_{q};p]\), \(FDP[BH_{q};p_{+c}]\)) pairs are plotted. As can be seen, the vast majority of the pairs satisfy \(FDP[BH_{q};p_{+c}]>FDP[BH_{q};p]\), and, further, all pairs lie above the horizontal line situated at the level of the BH control level \(\pi_{0}\cdot q=.09\), i.e, \(FDP[BH_{q};p_{+c}]>\pi_{0}\cdot q\).

In Table 1, we present the effectiveness of INCREASE-c over ranges of corruption budget \(c\) and \(\mu_{1}\) from small to large. As can be seen, when \(\mu_{1}=0\), any amount of corruption budget \(c\) yields large post-corruption FDR \(\mathbb{E}_{z}FDP[BH_{q};z_{+c}]\). When \(\mu_{1}>0\) grows, however, the budget \(c\) must correspondingly grow in order for there to be nontrivial post-corruption FDR. Finally, we note that for any fixed \(c\), the increase in rejection count \(\tilde{k}_{+c}-\tilde{k}\) is on the average larger when \(\mu_{1}\) is larger. For experiments on non- i.i.d., PRDS p-values, we refer the interested reader to Section 5.2 or Appendix Section 7.4.

#### 5.1.2 INCREASE-1 Simulations versus Theoretical Bounds Under Small \(\mu_{1}\)

In Figures 2 and 3 we illustrate how well the insights discussed in Section 4.2 capture the sensitivity of BH to adversarial perturbations when \(\mu_{1}\) is near 0. Specifically, for each \(q\) in the grid \(\{.01,.02,\ldots,.99\}\), we computed the difference \(FDP[BH_{q};p_{+}]-FDP[BH_{q};p]\) across \(10^{3}\) replications of the setup as in Section 5.1.1, with the average of this difference being an estimate of \(\Delta_{1}\). The plot of these \(\Delta_{1}\) estimates with respect to \(q\) is then compared with the plot of our lower bound \(L_{1}\) as a function of \(q\).

Figures 2 and 3 illustrate that the stricter the control, i.e., the smaller \(q\) is, the more effective INCREASE-c becomes. In fact, the bound indicates high vulnerability for the typical use case

[MISSING_PAGE_FAIL:9]

### Training

We begin by training an unsupervised decision-tree-based algorithm on a training set. From the set \(n_{0}\) of genuine transactions, we uniformly at random select a subset \(n_{train}\subseteq n_{0}\) of size \(141,758\) to form a training set \(D^{train}:=\{X_{i}\}_{ien_{train}}\) upon which we train an isolation forest [25]\(\tilde{s}:\mathbb{R}^{30}\rightarrow\mathbb{R}_{+}\) using the R library isotree3, where, in principle, \(\hat{s}(X_{i})\) returns an _isolation depth_ that is smaller if \(Y_{i}=1\) (i.e. is an outlier) and larger if \(Y_{i}=0\) (i.e. is an inlier)

Footnote 3: [https://cran.r-project.org/web/packages/isotree/vignettes/An_Introduction_to_Isolation_Forests.html](https://cran.r-project.org/web/packages/isotree/vignettes/An_Introduction_to_Isolation_Forests.html)

### Calibration and (Adversarially-Perturbed) Testing

Then for each of \(10^{2}\) simulations, we uniformly at random selected a subset \(\tilde{n}_{cal}\subseteq n_{0}\setminus n_{train}\) of size \(141,657\) to form a calibration set \(\tilde{D}^{cal}\coloneqq\{X_{i}\}_{i\in\tilde{n}_{cal}}\) of strictly genuine transactions. As well, we uniformly at random selected a subset \(\tilde{n}_{test,1}\in n_{1}\) of \(100\) fraudulent transactions to append to the \(900\) remaining genuine transactions comprising \(\tilde{n}_{test,0}\coloneqq n_{0}\setminus n_{train}\setminus\tilde{n}_{cal}\) to form a test set \(\tilde{D}^{test}:=\{X_{i}\}_{i\in\tilde{n}_{test}}\), where \(\tilde{n}_{test}\coloneqq\tilde{n}_{test,0}\ \ \tilde{n}_{test,1}\). Finally, we transformed \(X_{i}\mapsto\tilde{p}_{i}\in(0,1)\) for each \(i\in\tilde{n}_{test}\) via \(\tilde{p}_{i}=\frac{1+[\{j\in\tilde{n}_{cal}\cup\{X_{i}\}\leq S(X_{i})\}]}{| \tilde{n}_{cal}|}\). The resulting collection of conformal p-values \(\tilde{p}:=(\tilde{p}_{i})_{i\in\tilde{n}_{test}}\) is PRDS, as proven in [5], and hence the FDR control of Lemma 1.1 holds (see [7]). In contrast, upon executing INCREASE-c to generate a corresponding adversarially-perturbed collection \(\tilde{p}_{+c}:=(\tilde{p}_{+c,i})_{i\in\tilde{n}_{test}}\), we obtain a collection for which BH's FDR control no longer holds.

### Experimental Results

We executed \(BH_{0.1}\), on both \(\tilde{p}\) and \(\tilde{p}_{+c}\), with an execution providing the decision for each \(i\in\tilde{n}_{test}\) whether to report it as genuine (null) or fraudulent (alternative). We report the average (over the \(10^{2}\) simulations) false detection proportion (FDP) produced by \(BH_{0.1}\), i.e., both \(E[FDP[BH_{0.1};\tilde{p}]]\) and \(E[FDP[BH_{0.1};\tilde{p}_{+c}]]\) (for \(c=1,5,10\)). As well, we report the average number of alleged frauds \(\mathbb{E}\left[\tilde{k}\right]\) and \(\mathbb{E}\left[\tilde{k}_{+c}\right]\). As Table 2 indicates, although the method of [5] can ordinarily control the FDR below the explicit \(0.10\) level, INCREASE-c has the ability to push the FDR above this control level.

## 6 Conclusions

This is the first work to consider adversarial corruption of the popular Benjamini Hochberg multiple testing procedure to break its FDR control. While BH may exhibit robustness when the alternative distributions are "far" from the null, it exhibits great sensitivity in practical cases when the alternatives are "closer" to the null. In such cases, with the modification of few p-values (as few as one), the attacker can increase the expected FDR well past the guarantee stipulated by the BH procedure. This study suggests some caution may be necessary when using BH, especially in safety-security settings. Numerical experiments support the analytical results. Finally, BH is but one member of the family of step-up multiple testing procedures, which generally entail rejection regions decided via a stopping time, which since our paper shows can be manipulated in the case of BH, it means other step-up procedures can be similarly prone.

\begin{table}
\begin{tabular}{|l|r r r r|} \hline \(c\) & \(E[FDP[BH_{0.1};\tilde{p}]]\) & \(E[FDP[BH_{0.1};\tilde{p}_{+c}]]\) & \(E[\tilde{k}]\) & \(E[\tilde{k}_{+c}]\) \\ \hline
1 &.09 &.10 & 60.21 & 60.21 \\
5 &.08 &.17 & 48.69 & 57.12 \\
10 &.09 &.23 & 56.39 & 72.85 \\
20 & 0.09 & 0.31 & 58.12 & 89.06 \\ \hline \end{tabular}
\end{table}
Table 2: Credit Card Fraud Detection Experiment

## Acknowledgments and Disclosure of Funding

We wish to thank Wang Chi Cheung for a stimulating conversation. This work was supported by the Air Force Office of Scientific Research (Mathematical Optimization Program) under the grant: "Optimal Decision Making under Tight Performance Requirements in Adversarial and Uncertain Environments: Insight from Rockafellian Functions".

## References

* [1]I. Amir, I. Attias, T. Koren, Y. Mansour, and R. Livni (2020) Prediction with corrupted expert advice. Advances in Neural Information Processing Systems33, pp. 14315-14325. Cited by: SS1.
* [2]H. Angelidakis, K. Makarychev, and Y. Makarychev (2017) Algorithms for stable and perturbation-resilient problems. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pp. 438-451. Cited by: SS1.
* [3]P. Awasthi, A. Blum, and O. Sheffet (2012) Center-based clustering under perturbation stability. Information Processing Letters112 (1-2), pp. 49-54. Cited by: SS1.
* [4]M. Balcan, N. Haghtalab, and C. White (2020-02) K-center clustering under perturbation resilience. ACM Trans. Algorithms16 (2), pp. mar. Cited by: SS1.
* [5]S. Bates, E. Candes, L. Lei, Y. Romano, and M. Sesia (2023) Testing for outliers with conformal p-values. The Annals of Statistics51 (1), pp. 149-178. Cited by: SS1.
* [6]Y. Benjamini and Y. Hochberg (1995) Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal statistical society: series B (Methodological)57 (1), pp. 289-300. Cited by: SS1.
* [7]Y. Benjamini and D. Yekutieli (2001) The control of the false discovery rate in multiple testing under dependency. The Annals of Statistics29 (4), pp. 1165-1188. Cited by: SS1.
* [8]D. Berend, A. Kontorovich, L. Reyzin, and T. Robinson (2020) On biased random walks, corrupted intervals, and learning under adversarial design. Annals of Mathematics and Artificial Intelligence88, pp. 887-905. Cited by: SS1.
* [9]S. Bhatt, G. Fang, P. Li, and G. Samorodnitsky (2022) Minimax m-estimation under adversarial corruption. In Proceedings of the 39th International Conference on Machine Learning (ICML), Baltimore, MD, Cited by: SS1.
* [10]Y. Bilu and N. Linial (2012) Are stable instances easy?. Combinatorics, Probability and Computing21 (5), pp. 643-660. Cited by: SS1.
* [11]S. Cao, R. Zhang, and S. Zou (2022) Adversarially robust sequential hypothesis testing. Sequential Analysis41 (1), pp. 81-103. Cited by: SS1.
* [12]D.Hendrycks and K.Gimpel (2017) A baseline for detecting misclassified and out-of-distribution examples in neural networks. In Proceedings of International Conference on Learning Representations, Cited by: SS1.
* [13]I. Diakonikolas and D. M. Kane (2023) Algorithmic high-dimensional robust statistics. Cambridge university press. Cited by: SS1.
* [14]B. Efron and T. Hastie (2013) Computer age statistical inference. Cited by: SS1.
* [15]R. Gao, L. Xie, Y. Xie, and H. Xu (2018) Robust hypothesis testing using wasserstein uncertainty sets. Advances in Neural Information Processing Systems31. Cited by: SS1.
* [16]I. Goodfellow, P. McDaniel, and N. Papernot (2018-06) Making machine learning robust against adversarial inputs. Commun. ACM61 (7), pp. 56-66. External Links: ISSN 0018-9219 Cited by: SS1.
* [17]A. Gupta, T. Koren, and K. Talwar (2019) Better algorithms for stochastic bandits with adversarial corruptions. In Conference on Learning Theory, pp. 1562-1578. Cited by: SS1.

* [18] Ying Jin and Emmanuel J Candes. Selection by prediction with conformal p-values. _Journal of Machine Learning Research_, 24(244):1-41, 2023.
* [19] Yulu Jin and Lifeng Lai. On the adversarial robustness of hypothesis testing. _IEEE Transactions on Signal Processing_, 69:515-530, 2021.
* [20] Akshay Krishnamurthy, Thodoris Lykouris, Chara Podimata, and Robert Schapire. Contextual search in the presence of adversarial corruptions. _Operations Research_, 71(4):1120-1135, 2023.
* [21] K. Lee, K. Lee, H. Lee, and J. Shin. A simple unified framework for detecting out-ofdistribution samples and adversarial attacks. In _NeurIPS_, 2018.
* [22] K. Lee, K. Lee, H. Lee, and J. Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. In _International Conference on Learning Representations_, 2018.
* [23] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. _Advances in neural information processing systems_, 31, 2018.
* [24] Zishuo Li, Yilin Mo, and Fei Hao. Game theoretical approach to sequential hypothesis test with byzantine sensors. In _2019 IEEE 58th Conference on Decision and Control (CDC)_, pages 2654-2659. IEEE, 2019.
* [25] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In _2008 eighth ieee international conference on data mining_, pages 413-422. IEEE, 2008.
* [26] Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial corruptions. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pages 114-122, 2018.
* [27] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018.
* [28] Akshayaa Magesh, Venugopal V Veeravalli, Anirban Roy, and Susmit Jha. Principled out-of-distribution detection via multiple testing. _Journal of Machine Learning Research_, 24(378):1-35, 2023.
* [29] Martin Raab and Angelika Steger. "balls into bins"--a simple and tight analysis. In _International Workshop on Randomization and Approximation Techniques in Computer Science_, pages 159-170. Springer, 1998.
* [30] Weijie J Su. The fdr-linking theorem. _arXiv preprint arXiv:1812.08965_, 2018.
* [31] Lajos Takacs. A generalization of the ballot problem and its application in the theory of queues. _Journal of the American Statistical Association_, 57(298):327-337, 1962.
* [32] Weiwei Liu Xinsong Ma, Xin Zou. A provable decision rule for out-of-distribution detection. _Proceedings of the 41st International Conference on Machine Learning (ICML), Vienna, Austria_, 2024.

Appendix / supplemental material

### Adversarial Algorithm: INCREASE-c

**Theorem 3.1**.: _If \(c\geq 1,\) then_

\[\mathbb{E}\left[\tilde{k}_{+c}-\tilde{k}\|B^{0}_{N+1}\geq c\right]\geq\frac{c-1} {1-\mathbb{E}\left[\frac{B^{0}_{k+2\tilde{k}}}{N-(\tilde{k}+1)}\ \parallel\ B^{0}_{N+1}\geq c\right]}+1 \tag{4}\]

_for any collection of alternative hypothesis distributions \(\{\mathbb{P}^{1}_{i}\}_{i\in\mathcal{H}_{1}}\)._

Proof.: Since the statement in the case of \(c=1\) is trivially true, we henceforth assume that \(c>1\). Recall \(\tilde{k}_{+c}=\max\{i\in[c,N]_{\mathbb{Z}}:B_{1:i}=i-c\}\). Let us define \(\tilde{k}^{0}_{+c}:=\max\{i\in[c,N]_{\mathbb{Z}}:B^{0}_{1:i}=i-(B^{1}_{1:\tilde {k}}+c)\}\). Then we begin by establishing that

\[\tilde{k}_{+c}\geq\tilde{k}^{0}_{+c}>\tilde{k}+1.\]

To see why this holds, we note that \(B^{0}_{1:\tilde{k}+1}=\tilde{k}-B^{1}_{1:\tilde{k}+1}=\tilde{k}+1-(B^{1}_{1: \tilde{k}+1}+1)>\tilde{k}+1-(B^{1}_{1:\tilde{k}-1}+c)\), and this implies \(\tilde{k}+1<\tilde{k}^{0}_{+c}\). Further, \(B_{1:\tilde{k}^{0}_{+c}}=B^{0}_{1:\tilde{k}^{0}_{+c}}+B^{1}_{1:\tilde{k}^{0}_ {+c}}=\tilde{k}^{0}_{+c}-(B^{1}_{1:\tilde{k}}+c)+B^{1}_{1:\tilde{k}^{0}_{+c}}= \tilde{k}^{0}_{+c}-c+(B^{1}_{1:\tilde{k}^{0}_{+c}}-B^{1}_{1:\tilde{k}})\geq \tilde{k}^{0}_{+c}-c\), which implies \(\tilde{k}_{+c}\geq\tilde{k}^{0}_{+c}\).

Next, we justify the relation

\[\mathbb{E}\left[\frac{B^{0}_{k+2\tilde{k}^{0}_{+c}}}{N-(\tilde{k}+1)}\ \parallel\ \tilde{k},B^{1}_{1:\tilde{k}},[B^{0}_{N+1}\geq c]\right]=\mathbb{E}\left[\frac{B^{0}_{k+2\tilde{k}^{0}_{+c}}}{\tilde{k}^{0}_{+c}-(\tilde{k}+1)}\ \parallel\ \tilde{k},B^{1}_{1:\tilde{k}},[B^{0}_{N+1}\geq c]\right].\]

It suffices to establish it for any fixed, joint realization of \(\tilde{k},B^{1}_{1:\tilde{k}}\) that occurs consistent with the event \([B^{0}_{N+1}\geq c]\) with positive probability. With slight abuse of notation, we will continue to use \(\tilde{k},B^{1}_{1:\tilde{k}}\) for such a fixed realization, and write \(\mathbb{E}\left[\cdot\right]\) for \(\mathbb{E}\left[\cdot\|\tilde{k},B^{1}_{1:\tilde{k}},[B^{0}_{N+1}\geq c]\right]\). The plan is to apply the Optional Stopping Theorem on a martingale sequence. To do so, let us form a (backwards-running) filtration: let \(\mathcal{F}_{N}\) be the sigma-algebra generated by the event \([B^{0}_{N+1}\geq c]\) as well as the random variable \(B^{0}_{N+1}\), and let \(\mathcal{F}_{i}\) be the sigma-algebra generated by the event \([B^{0}_{N+1}\geq c]\) as well as the random variables \(\{B^{0}_{j}\}_{j=i+1}^{N+1}\). Then for any integers \(k,\ell\in\{N,N-1,\ldots 1\}\) such that \(\ell>k\),

\[\mathbb{E}\left[B^{0}_{k+2\{k\vee(\tilde{k}+2)\}}\parallel\mathcal{F}_{\ell} \right]=\mathbb{E}\left[B^{0}_{k+2\{k\vee(\tilde{k}+2)\}}\parallel\!\!\!B^{0}_{k+2\{k\vee(\tilde{k}+2)\}}\ \!\!we derive

\[\frac{B^{0}_{k+2\hat{k}^{0}_{+e}}}{\tilde{k}^{0}_{+e}-(\tilde{k}+1)} =\frac{\tilde{k}^{0}_{+e}-(\tilde{k}+c)}{\tilde{k}^{0}_{+e}-(\tilde{k}+1)}=\frac {\tilde{k}^{0}_{+e}-(\tilde{k}+1)-c+1}{\tilde{k}^{0}_{+e}-(\tilde{k}+1)}=1-\frac {c-1}{\tilde{k}^{0}_{+e}-(\tilde{k}+1)}\] \[\implies 0<\mathbb{E}\left[\frac{B^{0}_{k+2;N}}{N-(\tilde{k}+1)}\; \parallel\;\tilde{k},B^{1}_{1:\tilde{k}},[B^{0}_{N+1}\geq c]\right]=1-(c-1) \mathbb{E}\left[\frac{1}{\tilde{k}^{0}_{+e}-(\tilde{k}+1)}\;\parallel\;\tilde{ k},B^{1}_{1:\tilde{k}},[B^{0}_{N+1}\geq c]\right]<1\] (*) \[\implies \frac{1}{\mathbb{E}\left[\tilde{k}^{0}_{+e}-(\tilde{k}+1)\; \parallel\;\tilde{k},B^{1}_{1:\tilde{k}},[B^{0}_{N+1}\geq c]\right]}\leq\mathbb{ E}\left[\frac{1}{\tilde{k}^{0}_{+e}-(\tilde{k}+1)}\;\parallel\;\tilde{k},B^{1}_{1: \tilde{k}},[B^{0}_{N+1}\geq c]\right]\] \[= \frac{1-\mathbb{E}\left[\frac{B^{0}_{k+2N}}{N-(\tilde{k}+1)}\; \parallel\;\tilde{k},B^{1}_{1:\tilde{k}},[B^{0}_{N+1}\geq c]\right]}{c-1}.\] Hence, \[\frac{c-1}{1-\mathbb{E}\left[\frac{B^{0}_{k+2N}}{N-(k+1)}\; \parallel\;\tilde{k},B^{1}_{1:\tilde{k}},[B^{0}_{N+1}\geq c]\right]}+1\leq \mathbb{E}\left[\tilde{k}^{0}_{+e}-\tilde{k}\;\parallel\;\tilde{k},B^{1}_{1: \tilde{k}},[B^{0}_{N+1}\geq c]\right]\leq\mathbb{E}\left[\tilde{k}_{+c}-\tilde {k}\;\parallel\;\tilde{k},B^{1}_{1:\tilde{k}},[B^{0}_{N+1}\geq c]\right].\] It follows that \[\frac{c-1}{1-\mathbb{E}\left[\frac{B^{0}_{k+2N}}{N-(k+1)}\; \parallel\;B^{0}_{N+1}\geq c\right]}+1 =\frac{c-1}{\mathbb{E}\left[1-\mathbb{E}\left[\frac{B^{0}_{k+2N}} {N-(\tilde{k}+1)}\;\parallel\;\tilde{k},B^{1}_{1:\tilde{k}},[B^{0}_{N+1}\geq c ]\right]\;\parallel\;B^{0}_{N+1}\geq c\right]}+1\] \[\leq \mathbb{E}\left[\frac{c-1}{1-\mathbb{E}\left[\frac{B^{0}_{k+2N}} {N-(\tilde{k}+1)}\;\parallel\;\tilde{k},B^{1}_{1:\tilde{k}},[B^{0}_{N+1}\geq c ]\right]}\;\parallel\;B^{0}_{N+1}\geq c\right]+1\] \[\leq \mathbb{E}\left[\tilde{k}_{+e}-\tilde{k}\;\parallel\;B^{0}_{N+1} \geq c\right],\] where we have used (conditional) Jensen's Inequality - valid because \[c>1\] and (*) ensures \[1-\mathbb{E}\left[\frac{B^{0}_{k+2;N}}{N-(\tilde{k}+1)}\; \parallel\;\tilde{k},B^{1}_{1:\tilde{k}},[B^{0}_{N+1}\geq c]\right]>0.\]

**Theorem 3.2**.: _Given \(c\geq 1,\) let \(p_{+c}\) denote the perturbed form of \(p\) that INCREASE-c produces. Then the adversarially-adjusted FDR induced by INCREASE-c is_

\[\mathbb{E}FDP[BH_{q};p_{+c}]=\mathbb{E}FDP[BH_{q};p]+\Delta_{c},\]

_for any collection of alternative distributions \(\{\mathbb{P}^{1}_{i}\}_{i\in\mathcal{H}_{i}}\), where_

\[\Delta_{c}:=\mathbb{E}\left[\frac{c}{\tilde{k}_{+c}};B^{0}_{N+1}\geq c\right]. \tag{5}\]

Proof.: We derive

\[\mathbb{E}FDP[BH_{q};p_{+}] =\mathbb{E}\left[\frac{B^{0}_{1:\tilde{k}_{+c}}+c}{\tilde{k}_{+c}} ;B^{0}_{N+1}\geq c\right]+\mathbb{E}\left[\frac{B^{0}_{1:\tilde{k}\lor 1}}{ \tilde{k}\lor 1};B^{0}_{N+1}<c\right]\] \[=\mathbb{E}\left[\frac{B^{0}_{1:\tilde{k}_{+c}}}{\tilde{k}_{+c}} ;B^{0}_{N+1}\geq c\right]+\mathbb{E}\left[\frac{c}{\tilde{k}_{+c}};B^{0}_{N+1} \geq c\right]+\mathbb{E}\left[\frac{B^{0}_{1:\tilde{k}\lor 1}}{\tilde{k}\lor 1};B^{0}_{N+1}<c\right]\] \[=\mathbb{P}\left(B^{0}_{N+1}\geq c\right)\mathbb{E}\left[\frac{B^ {0}_{1:N}}{N}\left\|B^{0}_{N+1}\geq c\right]+\mathbb{E}\left[\frac{c}{\tilde{k} _{+c}};B^{0}_{N+1}\geq c\right]+\mathbb{P}\left(B^{0}_{N+1}<c\right)\mathbb{E} \left[\frac{B^{0}_{1:N}}{N}\left\|B^{0}_{N+1}<c\right]\right]\] \[=\mathbb{E}\left[\frac{B^{0}_{1:N}}{N}\right]+\mathbb{E}\left[ \frac{c}{\tilde{k}_{+c}};B^{0}_{N+1}\geq c\right],\]

as desired.

### Move-1

MOVE-1 is an efficient, optimal algorithm for an (omniscient) adversary with budget \(c=1\). We begin with several small insights en route to describing MOVE-1 in full.

#### Perturbing the Rejection Count

If \(p^{\prime}=p\), i.e., the sample statistics are left unperturbed, we obtain a false detection proportion of \(FDP\left[BH_{q};p\right]=\frac{B_{1:i}^{\mathcal{N}}}{k}\). If it is possible to improve on this, it can be easily shown that in the case of \(c=1\) we must induce an altered rejection count

\[\tilde{k}^{\prime}:=\max\Bigl{\{}i\in[0,N]_{\mathbb{Z}}:|\{p^{\prime}_{i}\}_{i \in\mathcal{N}}\cap B_{1:i}\}|=i\Bigr{\}}, \tag{7}\]

henceforth referred to as the _(adversarially) adjusted rejection count_ resulting from the adversary's choice of \(p^{\prime}\). As it turns out, for the special case of \(c=1\), \(\tilde{k}^{\prime}\neq\tilde{k}\) is necessary if a larger FDP is to be obtained, as the following lemma indicates.

**Lemma 7.1**.: _If \(\|p-p^{\prime}\|_{0}\leq 1\) and \(FDP\left[BH_{q};p^{\prime}\right]\neq FDP\left[BH_{q};p\right],\) then \(\tilde{k}^{\prime}\neq\tilde{k}\)._

Proof.: We suppose the contrary for the sake of a contradiction, i.e., \(\tilde{k}^{\prime}=\tilde{k}\). It follows that

\[|\{j\in\mathcal{N}:0\leq p^{\prime}_{j}<\tilde{k}\frac{q}{N}\}|=\tilde{k}^{ \prime}=\tilde{k}=|\{j\in\mathcal{N}:0\leq p_{j}<\tilde{k}\frac{q}{N}\}|,\]

and then by \(FDP\left[BH_{q};p^{\prime};\right]\neq FDP\left[BH_{q};p\right],\) it follows that

\[|\{j\in\mathcal{H}_{0}:0\leq p^{\prime}_{j}<\tilde{k}\frac{q}{N}\}|\neq|\{j\in \mathcal{H}_{0}:0\leq p_{j}<\tilde{k}\frac{q}{N}\}|,\]

Since \(\|p-p^{\prime}\|_{0}\leq 1\), these two conclusions are at odds, presenting a contradiction. 

Considering Lemma 7.1, the adversary's \(p^{\prime}\in[0,1]^{N}\) decision simplifies to deciding on an adjusted rejection count \(\tilde{k}^{\prime}\) from among a constrained set of integers consistent with the constraint \(\|p^{\prime}-p\|_{0}\leq 1\). An optimal \(\tilde{k}^{\prime}\) can indeed be larger or smaller than, or even equal to \(\tilde{k}\). Hence, towards understanding this new search space, it suffices to characterize the set of feasible \(\tilde{k}^{\prime}\) that are larger than \(\tilde{k}\), and smaller.

#### Increasing the Rejection Count (c = 1)

Towards understanding how to increase the rejection count, we first highlight the following fact that follows immediately from (2).

**Lemma 7.2**.: _If \(i\in\{\tilde{k}+1,\ldots,N\},\) then \(B_{1:i}^{\mathcal{N}}\leq i-1\). In particular, \(B_{1:\tilde{k}+1}^{\mathcal{N}}=i-1\)._

Proof.: Suppose there exists \(i\in\{\tilde{k}+1,\ldots,N\}\) such that \(B_{1:i}^{\mathcal{N}}>i-1\). If \(B_{1:i}^{\mathcal{N}}=i\), then \(i>\tilde{k}\) presents a contradiction of (2). However, proceeding with \(B_{1:i}^{\mathcal{N}}\geq i+1\), we see that there necessarily exists \(j\in\{i+1,\ldots,N\}\) for which \(B_{1:j}^{\mathcal{N}}=j\), for if this were not the case, then \(B_{1:j}^{\mathcal{N}}\geq j+1\) for all \(j\in\{i+1,\ldots,N\}\), meaning \(B_{1:N}^{\mathcal{N}}\geq N+1,\) yet another contradiction since there are only \(N\) p-values. 

Consequently, for \(\tilde{k}^{\prime}>\tilde{k}\), we require a perturbed collection \(p^{\prime}\) for which bin counts increase. This necessitates a _decrease_ of a p-value - see Lemma 7.5 for details. Hence, we define

\[\mathcal{L}:=\{i\in\{\tilde{k}+1,\ldots,N\}:B_{1:i}^{\mathcal{N}}=i-1\} \tag{8}\]

and note that these are precisely the positions \(i\) for which the addition of one (and only one) p-value into \(B_{1:i}\) through the _decrease_ of a p-value brings \(i\) into candidacy for the new rejection count - see equation (2).

**Proposition 7.3**.: \(\mathcal{L}=\{\tilde{k}^{\prime}:\tilde{k}^{\prime}>\tilde{k},\|p-p^{\prime} \|_{0}\leq 1\}\)Proof.: To prove the first statement, we recall that by Lemma 7.2, if \(i>\tilde{k}\), then \(B^{\mathcal{N}}_{1:i}\leq i-1\). So, if \(i\geq\tilde{k}+1\) with \(B^{\mathcal{N}}_{1:i}<i-1\), then no decrease of a single p-value could increase \(B^{\mathcal{N}}_{1:i}\) to \(i\) (Lemma 7.5), precluding the possibility of \(\tilde{k}^{\prime}=i\). In other words, no \(i\notin\mathcal{L}\) is achievable for the new rejection count \(\tilde{k}^{\prime}\).

On the other hand, say we enumerate \(\mathcal{L}\) with

\[i_{L}>i_{L-1}>\ldots>i_{1}=\tilde{k}+1.\]

For the case of \(i_{L}\), \(\tilde{k}^{\prime}=i_{L}\) is achieved if and only if a p-value is decreased from any bin \(B_{j}\) with \(j>i_{\ell}\) to a bin \(B_{j-s}\) where \(i_{L}\geq j-s\geq 1\). For the case of \(i_{\ell}\), \(\tilde{k}^{\prime}=i_{\ell}\) is achieved if and only if a p-value is decreased from any bin \(B_{j}\) with \(i_{\ell+1}\geq j>i_{\ell}\) to a bin \(B_{j-s}\) where \(i_{\ell}\geq j-s\geq 1\). Finally, all these movements of p-value just described are always possible for any given \(p=\{p_{i}\}_{i\in\mathcal{N}}\). 

#### Decreasing the Rejection Count (c = 1)

Analogously, it follows that the increase of a p-value is necessary for the _decrease_ of the rejection count - see Lemma 7.6, and we define

\[\mathcal{R}:=\Big{\{}i\in\{i^{*}+1,\ldots,\tilde{k}-1\}:B^{\mathcal{N}}_{1:i}= i\Big{\}}\cup\{i^{*}\}, \tag{9}\]

where \(i^{*}:=0\vee\max\Big{\{}i\in\{1,\ldots,\tilde{k}-1\}:B^{\mathcal{N}}_{1:i}=i+1 \Big{\}}\). We note that \(\mathcal{R}\) is precisely the set of all the achievable new (decreased) rejection counts \(\tilde{k}^{\prime}<\tilde{k}\) we could induce with the perturbation of a single p-value.

**Proposition 7.4**.: \(\mathcal{R}=\{\tilde{k}^{\prime}:\tilde{k}>\tilde{k}^{\prime},\lfloor p-p^{ \prime}\rfloor_{0}\leq 1\}\)__

Proof.: We begin by proving the first statement that \(\tilde{k}^{\prime}<\tilde{k}\) implies \(\tilde{k}^{\prime}\in\mathcal{R}\). To do so, we proceed in two steps. First we show that \(\tilde{k}\geq i^{*}\), so that \(i^{*}\leq\tilde{k}^{\prime}\leq\tilde{k}-1\). Then we show that if \(\tilde{k}^{\prime}=i\) for some \(i\in\{i^{*}+1,\ldots,\tilde{k}-1\}\) in which \(B^{\mathcal{N}}_{1:i}\neq i\), we arrive at a contradiction.

To see that \(\tilde{k}^{\prime}\geq i^{*}\), if \(i^{*}>\tilde{k}^{\prime}\), then \(B^{\mathcal{N}}_{1:i^{*}}=i^{*}+1\) becomes \(B^{\mathcal{N}}_{1:i^{*}}\leq i^{*}-1\) by Lemma 7.2; in other words, the change in magnitude of \(B^{\mathcal{N}}_{1:i^{*}}\) is at least 2, which contradicts Lemma 7.6. Next, if \(i\in\{i^{*}+1,\ldots,\tilde{k}-1\}\), then \(B^{\mathcal{N}}_{1:i}\leq i\) by the definition of \(i^{*}\). This means if \(B^{\mathcal{N}}_{1:i}\neq i\), then \(B^{\mathcal{N}}_{1:i}<i\), so that Lemma 7.6 indicates \(\tilde{k}^{\prime}\) could not be \(i\).

As for the second statement, let \(\mathcal{R}\) be enumerated

\[i_{R}>i_{R-1}>\ldots>i_{1}=i^{*}.\]

For the case of \(i_{R}\), \(\tilde{k}^{\prime}=i_{R}\) is achieved if and only if a p-value is moved from bin \(B_{j}\) with \(\tilde{k}\geq j>i_{R}\) to a bin \(B_{j+s}>\tilde{k}\). For \(R-1\geq r\geq 2\), by Lemma 7.6 it holds that \(\tilde{k}^{\prime}=i_{r}\) is achieved if and only if a p-value is moved from a bin \(B_{j}\) with \(i_{r+1}\geq j>i_{r}\) to a bin \(B_{j+s}\) with \(j+s>\tilde{k}\). Finally, for the case of \(i_{1}\), \(\tilde{k}^{\prime}=i_{1}=i^{*}\) is achieved if and only if a p-value is moved from \(B_{j}\) with \(i^{*}\geq j\) to a \(B_{j+s}\) with \(j+s>\tilde{k}\). Finally, all these movements of p-values just described are always possible for any given \(p=\{p_{i}\}_{i\in\mathcal{N}}\). 

It follows that \(\tilde{k}^{\prime}\neq\tilde{k}\) if and only if \(\tilde{k}^{\prime}\in\mathcal{L}\cup\mathcal{R}\), so an efficient, optimal search procedure becomes straightforward. Informally, we iterate over the bins in reverse order, beginning with \(N+1\) and terminating with \(i^{*}\). At iteration _(_b_in number) \(i\), if \(i\in\mathcal{L}\cup\mathcal{R}\), then the trivial subproblem \(\max_{p^{\prime}\|p-p^{\prime}\|_{0}\leq 1,\tilde{k}^{\prime}=i}FDP[BH_{q};p^{ \prime}]\) is solved; otherwise, nothing is done. Upon termination, the best FDP encountered is the answer. This is summarized in Theorem 7.7

**Lemma 7.5** (Decreasing a p-value).: _Let \(p^{\prime}\) be such that \(\|p-p^{\prime}\|_{0}\leq 1\). If \(p^{\prime}\) is the decrease of a single p-value in \(p\) from \(B_{j}\) to \(B_{j-s}\), where \(j\in\{2,\ldots,N+1\}\) and \(1\leq s\leq j-1\), then_

* \(i\in\{1,\ldots,j-s-1\}\implies B^{\mathcal{N}}_{1:i}\) _remains constant_
* \(i\in\{j-s,\ldots,j-1\}\implies B^{\mathcal{N}}_{1:i}\) _increases by 1_* \(i\in\{j,\ldots,N+1\}\implies B_{1:i}^{\mathcal{N}}\) _remains constant._

**Lemma 7.6** (Increasing a p-value).: _Let \(p^{\prime}\) be such that \(\|p-p^{\prime}\|_{0}\leq 1\). If \(p^{\prime}\) is the increase of a single p-value in \(p\) from \(B_{j}\) to \(B_{j+s}\), where \(j\in\{1,\ldots,N\}\) and \(1\leq s\leq N+1-j\), then_

* \(i\in\{1,\ldots,j-1\}\implies B_{1:i}^{\mathcal{N}}\) _remains constant_
* \(i\in\{j,\ldots,j+s-1\}\implies B_{1:i}^{\mathcal{N}}\) _decreases by 1_
* \(i\in\{j+s,\ldots,N+1\}\implies B_{1:i}^{\mathcal{N}}\) _remains constant._

**Theorem 7.7** (MOVE-1).: _Let \(q\in(0,1)\), \(p\)-values \(\{p_{i}\}_{i\in\mathcal{N}}\) and sets \(\mathcal{H}_{0}\), \(\mathcal{H}_{1}\) be given. For each \(i\in\mathcal{L}\cup\mathcal{R}\), let_

\[FDP_{i}:=\max_{p^{\prime}\|p-p^{\prime}\|_{0}\leq 1,k^{\prime}=i}FDP[BH_{q};p^{ \prime}].\]

_Then_

\[\max_{p^{\prime}\|p-p^{\prime}\|_{0}\leq 1}FDP[BH_{q};p^{\prime}]=\left(\max_{i \in\mathcal{L}\cup\mathcal{R}\cup\{k\}}FDP_{i}\right)\]

This result explains that with one pass of the p-values from the largest to the smallest in the collection, we can ascertain the optimal perturbation \(p^{\prime}\). As the execution based on this result is straightforward, we omit the pseudocode for the sake of brevity.

In Table 3, we compare the average performance of INCREASE-1 against that of the optimal MOVE-1 over \(10^{4}\) simulations. The experiments followed the setup described in Remark 4.6, in which \(N=10^{3}\) p-values are derived from independently generated z-scores, with \(N_{0}\)(\(=900\)) null z-scores i.i.d. \(N(0,1)\) and \(N_{1}\)(\(=100\)) alternative z-scores i.i.d. \(N(\mu_{1},1)\), for \(\mu_{1}=1,2\). As Table 3 indicates, INCREASE-1 can provide nearly identical performance in adjustment to FDR; however, the perturbation distance \(\left|z-z^{\prime}\right|\) is on the average much greater than in MOVE-1.

\begin{table}
\begin{tabular}{|c|c|c|} \cline{2-3} \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{MOVE-1(INCREASE-1)} \\ \cline{2-3} \multicolumn{1}{c|}{} & \(\mathbb{E}_{z}FDP[BH_{q};z^{\prime}]\) & Average \(\|z-z^{\prime}\|_{1}\) \\ \hline \(\mu_{1}=2\) & 0.140 (0.139) & 0.139 (1.563) \\ \hline \(\mu_{1}=1\) & 0.775 (0.751) & 0.492 (2.297) \\ \hline \(\mu_{1}=0\) &.992 (0.990) & 0.551 (2.41) \\ \hline \end{tabular}
\end{table}
Table 3: Sample Average (\(10^{4}\)-batch) estimates of \(\mathbb{E}_{z}FDP[BH_{q};z^{\prime}]\) under MOVE-1 and INCREASE-1 (in parentheses) when \(\mu_{1}=0,1,2\) and \(N=10^{3},q=0.10,\pi_{0}=0.90,\) and all \(\sigma^{i}=1\).

### Theoretical Analysis: Performance Guarantees and Insights into Adversarial Robustness

**Theorem 4.1**.: _If \(c\geq 1,\) then_

\[\Delta_{c}\leq\mathbb{P}\left(B_{1:c}^{1}=N_{1}\right)\mathbb{E} \left[\frac{c}{c+N_{1}+B_{1:N_{1}+c}^{0}}\|B_{N+1}^{0}\geq c\right]+1-\mathbb{ P}\left(B_{1:c}^{1}=N_{1}\right)\]

_and_

\[\mathbb{E}\left[\tilde{k}_{+c}\|B_{N+1}^{0}\geq c\right]\geq\frac {\left(N_{1}+c\right)\cdot\mathbb{P}\left(B_{1:c}^{1}=N_{1}\right)}{1-\mathbb{ E}\left[\frac{B_{1:N}^{0}}{N}\|B_{1:N}^{0}\leq N_{0}-c\right]}. \tag{6}\]

Proof.: When \(B_{1:c}^{1}=N_{1},\) it easily follows that \(\tilde{k}_{+c}\geq c+N_{1}+B_{1:N_{1}+c}^{0}\). This combined with Theorem 3.2 easily yields the first inequality.

As for the second inequality, let

\[\tilde{k}_{+c}^{0}:=\max\{i\in[0,N]_{\mathbb{Z}}:B_{1:i}^{0}=i- \left(N_{1}+c\right)\}\geq N_{1}+c>0.\]

Then

\[\mathbb{E}\left[\frac{B_{1:N}^{0}}{N}\ \ \|\ B_{N+1}^{0}\geq c \right]=\mathbb{E}\left[\frac{B_{1:\tilde{k}_{+c}^{0}}^{0}}{\tilde{k}_{+c}^{0} }\ \ \|\ B_{N+1}^{0}\geq c\right]=\mathbb{E}\left[\frac{\tilde{k}_{+c}^{0}- \left(N_{1}+c\right)}{\tilde{k}_{+c}^{0}}\ \ \|\ B_{N+1}^{0}\geq c\right]=1- \left(N_{1}+c\right)\mathbb{E}\left[\frac{1}{\tilde{k}_{+c}^{0}}\ \ \|\ B_{N+1}^{0}\geq c\right]\]

implies that

\[\frac{1}{\mathbb{E}\left[\tilde{k}_{+c}^{0}\ \ \|\ B_{N+1}^{0}\geq c \right]}\leq\mathbb{E}\left[\frac{1}{\tilde{k}_{+c}^{0}}\ \ \|\ B_{N+1}^{0}\geq c \right]=\frac{1}{N_{1}+c}\cdot\left(1-\mathbb{E}\left[\frac{B_{1:N}^{0}}{N}\ \ \|\ B_{N+1}^{0}\geq c\right]\right),\]

so that

\[\mathbb{E}\left[\tilde{k}_{+c}\ \ \|\ B_{N+1}^{0}\geq c,B_{1:c}^{1}=N_{1}\right]=\mathbb{E}\left[\tilde{k}_{+c}^{0}\ \ \|\ B_{N+1}^{0}\geq c\right]\geq\frac{N_{1}+c}{1-\mathbb{E}\left[\frac{B_{1:N}^{0}}{N}\ \ \|\ B_{N+1}^{0}\geq c\right]},\]

since whenever \(B_{1:c}^{1}=N_{1}\), it follows that \(\tilde{k}_{+c}=\tilde{k}_{+c}^{0}\).

Consequently, we derive

\[\frac{N_{1}+c}{1-\mathbb{E}\left[\frac{B_{1:N}^{0}}{N}\ \ \|\ B_{N+1}^{0}\geq c\right]}\leq\mathbb{E}\left[\tilde{k}_{+c}\ \ \|\ B_{N+1}^{0}\geq c,B_{1:c}^{1}=N_{1}\right]\] \[=\frac{\mathbb{E}\left[\tilde{k}_{+c};B_{1:c}^{1}=N_{1},B_{N+1}^{0} \geq c\right]}{\mathbb{P}\left(B_{1:c}^{1}=N_{1}\right)\mathbb{P}\left(B_{N+1}^{0}\geq c\right)}\] \[=\frac{\mathbb{E}\left[\tilde{k}_{+c};B_{N+1}^{0}\geq c\right]- \mathbb{E}\left[\tilde{k}_{+c};B_{1:c}^{1}<N_{1},B_{N+1}^{0}\geq c\right]}{ \mathbb{P}\left(B_{1:c}^{1}=N_{1}\right)\mathbb{P}\left(B_{N+1}^{0}\geq c \right)}\] \[=\mathbb{E}\left[\tilde{k}_{+c}\ \ \|\ B_{N+1}^{0}\geq c \right]\frac{1}{\mathbb{P}\left(B_{1:c}^{1}=N_{1}\right)}-\mathbb{E}\left[ \tilde{k}_{+c}\ \|\ B_{1:c}^{1}<N_{1},B_{N+1}^{0}\geq c\right]\frac{\mathbb{P}\left(B_{1:c}^{1}<N_{1}\right)}{ \mathbb{P}\left(B_{1:c}^{1}=N_{1}\right)},\]

yielding

\[\mathbb{P}\left(B_{1:c}^{1}=N_{1}\right)\cdot\frac{N_{1}+c}{1- \mathbb{E}\left[\frac{B_{1:N}^{0}}{N}\ \ \|\ B_{N+1}^{0}\geq c\right]}+\mathbb{E}\left[\tilde{k}_{+c}\ \|\ B_{1:c}^{1}<N_{1},B_{N+1}^{0}\geq c\right]\mathbb{P}\left(B_{1:c}^{1}<N_{1}\right)\leq \mathbb{E}\left[\tilde{k}_{+c}\ \|\ B_{N+1}^{0}\geq c\right].\]

**Corollary 4.2**.: _Let \(n\geq 1\), \(p\in[0,1]\), and \(x\) a non-negative integer such that \(0\leq x\leq n\). If \(\tilde{B}=(\tilde{B}_{1},\ldots,\tilde{B}_{n})\sim Multinomial\Big{(}x,(p, \ldots,p)\Big{)}\), then \(\mathbb{P}_{\tilde{B}}\left(\cap_{r=1}^{n}\big{[}\sum_{i=1}^{r}\tilde{B}_{i}<r \big{]}\right)=1-\frac{x}{n}.\)_

Proof.: We will appeal to the following result of [31]:

**Lemma 7.8** (Theorem 1 of [31]).: _Let there be \(n\geq 1\) non-negative integers \(z_{1},\ldots,z_{n}\) summing to \(x\). If \(\tilde{\pi}\) denotes a permutation drawn uniformly at random, then_

\[\mathbb{P}_{\tilde{\pi}}\left(\cap_{r=1}^{n}\left[\sum_{i=1}^{r}z_{\tilde{\pi}( i)}<r\right]\right)=\left[1-\frac{x}{n}\right]^{+}.\]

Let \(\tilde{\pi}\) be a random permutation on \(\{1,\ldots,n\}\) that is drawn uniformly at random, independently of \(\tilde{B}\). Since \(\tilde{B}=(\tilde{B}_{1},\ldots,\tilde{B}_{n})\sim Multinomial\Big{(}x,(p, \ldots,p)\Big{)}\), then \(\tilde{B}_{\tilde{\pi}}:=(\tilde{B}_{\tilde{\pi}(1)},\ldots,\tilde{B}_{\tilde {\pi}(n)})\) is equivalent in distribution to \(\tilde{B}\), or \(\tilde{B}_{\tilde{\pi}}\stackrel{{ D}}{{=}}\tilde{B}\). Therefore,

\[\mathbb{P}_{\tilde{B}}\left(\cap_{r=1}^{n}\left[\sum_{i=1}^{r} \tilde{B}_{i}<r\right]\right) =\mathbb{E}_{\tilde{B}}\left[\mathbb{P}_{\tilde{\pi}}\left(\cap_{ r=1}^{n}\left[\sum_{i=1}^{r}\tilde{B}_{\tilde{\pi}(i)}<r\right]\right)\right]\] \[=\mathbb{E}_{\tilde{B}}\left[1-\frac{x}{n}\right]=1-\frac{x}{n}.\]

**Corollary 4.3**.: _If \(\mathbb{P}_{i}^{1}=U(0,1)\) for all \(i\in\mathcal{H}_{1}\), then \(\mathbb{P}\left(\tilde{k}=\ell\right)=\left(\frac{1-q}{1-q+\frac{(N-\ell)q}{N }}\right)\cdot\mathbb{P}\left(B_{1:\ell}^{\mathcal{N}}=\ell\right)\) for \(\ell=0,\ldots,N,\) where \(0^{0}=1\), and \(\mathbb{P}\left(B_{1:\ell}^{\mathcal{N}}=\ell\right)=\left(N\atop\ell\right) \left(\frac{q\ell}{N}\right)^{\ell}\left(1-\frac{q\ell}{N}\right)^{N-\ell}\). If \(B_{N+1}^{0}\geq c\), then \(\mathbb{P}\left(\tilde{k}_{+c}=c\ \parallel\ B_{N+1}^{0}\right)=\left(1-\frac{cq}{N} \right)^{N_{1}}\cdot\left(1-\frac{c}{N}\right)^{N_{0}-B_{N+1}^{0}}\left(1- \frac{N_{0}-B_{N+1}^{0}}{N-c}-\frac{N_{1}q}{N-cq}\right)\)._

Proof.: The event \([\tilde{k}=\ell]\) is equivalent to \(\left\{\cap_{j=\ell+1}^{N}\left[B_{\ell+1;j}^{\mathcal{N}}<j-\ell\right]\right\} \cap\left[B_{1:\ell}^{\mathcal{N}}=\ell\right]\), an intersection of two events. Regarding the event \(\left[B_{1:\ell}^{\mathcal{N}}=\ell\right]\), because \(\mu_{1}=0\), it is clear that \(\mathbb{P}\left(B_{1:\ell}^{\mathcal{N}}=\ell\right)=\binom{N}{\ell}\left( \frac{q\ell}{N}\right)^{\ell}\left(1-\frac{q\ell}{N}\right)^{N-\ell}\). To complete the characterization of \(\mathbb{P}\left(\tilde{k}=\ell\right)\) it suffices to find \(\mathbb{P}\left(\cap_{j=\ell+1}^{N}\left[B_{\ell+1;j}^{\mathcal{N}}<j-\ell \right]\ \parallel\ \left[B_{1:\ell}^{\mathcal{N}}=\ell\right]\right)\). Towards doing so, we note that conditioned on \(\left[B_{1:\ell}^{\mathcal{N}}=\ell\right]\), we have

\[B_{\ell+1:N}^{\mathcal{N}}\sim Binom\left(N-\ell,\frac{\frac{(N-\ell)q}{N}}{1 -q+\frac{(N-\ell)q}{N}}\right),\]

and, upon further conditioning on \(B_{\ell+1:N}^{\mathcal{N}}\),

\[\left(B_{\ell+1}^{\mathcal{N}},\ldots,B_{N}^{\mathcal{N}}\right)\sim Multinom \left(B_{\ell+1:N}^{\mathcal{N}},\left(\frac{1}{N-\ell},\ldots,\frac{1}{N- \ell}\right)\right).\]

Hence, we derive

\[\mathbb{P}\left(\cap_{j=\ell+1}^{N}\left[B_{\ell+1;j}^{\mathcal{N}}<j-\ell \right]\ \parallel\ \left[B_{1:\ell}^{\mathcal{N}}=\ell\right],B_{\ell+1:N}^{ \mathcal{N}}\right)=1-\frac{B_{\ell+1:N}^{\mathcal{N}}}{N-\ell},\]

by Corollary 4.2. To conclude, we integrate out \(B_{\ell+1:N}^{\mathcal{N}}\) from this derivation; more precisely,

\[\mathbb{P}\left(\cap_{j=\ell+1}^{N}\left[B_{\ell+1;j}^{\mathcal{N }}<j-\ell\right]\ \parallel\ \left[B_{1:\ell}^{\mathcal{N}}=\ell\right]\right)\] \[=1-\frac{\mathbb{E}\left[B_{\ell+1:N}^{\mathcal{N}}\ \parallel\ \left[B_{1:\ell}^{\mathcal{N}}=\ell\right]\right]}{N-\ell}=1-\frac{\frac{(N- \ell)q}{N}}{1-q+\frac{(N-\ell)q}{N}}=\frac{1-q}{1-q+\frac{(N-\ell)q}{N}}.\]

As for the second statement, in similar fashion to above, Corollary 4.2 yields

\[\mathbb{P}\left(\cap_{j=c+1}^{N}\left[B_{c+1;j}^{\mathcal{N}}<j-c\right]\ \parallel\ \left[B_{1:c}^{\mathcal{N}}=0\right],B_{N+1}^{0},B_{c+1:N}^{1}\right)=1-\frac{N_{0}-B_{N+1}^{0}+B_{c+1:N}^{1}}{N-c},\]

implying

\[\mathbb{P}\left(\cap_{j=c+1}^{N}\left[B_{c+1;j}^{\mathcal{N}}<j-c\right] \ \parallel\ \left[B_{1:c}^{\mathcal{N}}=0\right],B_{N+1}^{0}\right)=1-\frac{N_{0}-B_{N+1}^{0}+E[B_{c+1:N}^{1}\ \parallel\ B_{1:c}^{1}=0]}{N-c}.\]Hence,

\[\mathbb{P}\left(\tilde{k}_{+c}=c\ \parallel\ B_{N+1}^{0}\right)= \mathbb{P}\left(B_{1:c}^{\mathcal{N}}=0\ \parallel\ B_{N+1}^{0}\right) \mathbb{P}\left(\cap_{j=c+1}^{N}[B_{c+1;j}^{\mathcal{N}}<j-1]\ \parallel\ B_{1:c}^{ \mathcal{N}}=0,B_{N+1}^{0}\right)\] \[=(1-\frac{cq}{N})^{N_{1}}\cdot(1-\frac{c}{N})^{N_{0}-B_{N+1}^{0} }\left(1-\frac{N_{0}-B_{N+1}^{0}}{N-c}-\frac{N_{1}q}{N-cq}\right).\]

**Theorem 4.5**.: _Suppose Assumption 4.4. Then_

\[\Delta_{c}\geq L_{c}(q,N,N_{0},\mathbb{P}^{1}):=\left(1-\frac{cq}{N}-\delta_{1: c}\right)^{N_{1}}\left[\left(1-\frac{c}{N}\right)^{N_{0}}\left(1-\pi_{c}-V_{c} \right)M_{c}+Z_{c}\right]\]

_where \(\pi_{c}(q,N,N_{0},\mathbb{P}^{1}):=\frac{N_{0}+\mathbb{E}[B_{c+1:N}^{1}\|B_{1: c}^{0}=0]}{N-c},\quad\mathbb{E}\left[B_{c+1:N}^{1}\|B_{1:c}^{1}=0\right]=N_{1} \frac{(N-c)q+N\delta_{c+1:N}}{N-cq-N\delta_{1:c}},\)_

\(V_{c}(q,N,N_{0},\mathbb{P}^{1}):=\sqrt{\frac{ln2}{2}\mathbb{E}\left[B_{c+1:N}^{ 1}\|B_{1:c}^{1}=0\right]D_{KL}(\mathbb{P}^{1},c)},\)__

\(M_{c}(q,N,N_{0}):=\frac{N-cq}{N-c}-\mathbb{E}\left[\left(1-\frac{c}{N}\right)^ {-B_{N+1}^{0}};B_{N+1}^{0}\leq c-1\right]\)_,_

\(Z_{c}(q,N,N_{0}):=\mathbb{P}\left(B_{N+1}^{0}\geq c\right)\left(1-\frac{c}{N} \right)^{N_{0}-\mathbb{E}\left[B_{N+1}^{0}\|B_{N+1}^{0}\geq c\right]}\frac{ \mathbb{E}\left[B_{N+1}^{0}\|B_{N+1}^{0}\geq c\right]}{N-c}\)__

\(D_{KL}(\mathbb{P}^{1},c):=\sum_{j}\frac{1}{N-c}log\left(\frac{q+\delta-\sum_{j =1}^{c}(q/N+\delta_{j})}{(N-c)(q/N+\delta_{j})}\right)\)__

Proof.: Observe that \(\Delta_{c}=\mathbb{E}\left[\frac{c}{k_{+c}};B_{N+1}^{0}\geq c\right]\geq \mathbb{P}\left(\tilde{k}_{+c}=c,B_{N+1}^{0}\geq c\right).\)

By Theorem 4.3,

\[\mathbb{P}\left(\tilde{k}_{+c}=c,B_{N+1}^{0}\geq c\right) =\mathbb{E}\left[\mathbb{P}\left(\tilde{k}_{+c}=c\ \parallel\ B_{N+1}^{0}\right);B_{N+1}^{0}\geq c\right]\] \[=\mathbb{E}\left[\mathbb{P}\left(B_{1:c}=0\ \parallel\ B_{N+1}^{0} \right)\cdot\underbrace{\mathbb{P}\left(\cap_{j=c+1}^{N}[B_{c+1;j}^{\mathcal{N }}<j-c]\ \parallel\ [B_{1:c}^{\mathcal{N}}=0],B_{N+1}^{0}\right)}_{(\#)};B_{N+1}^{0}\geq c\right]\]

where we now proceed to lower bound (#). The following discussion outlines how we proceed in the case of \(c=1,\) but this is without loss of generality.

Let \(B_{N+1}^{0}\) and \(B_{2:N}^{1}\) be given, along with the event \([B_{1}^{\mathcal{N}}=0].\) Then there are \(N_{0}-B_{N+1}^{0}\) many null p-values and \(B_{2:N}^{1}\) many alternative p-values each of whose assignment to one of bin number \(2,\ldots,N\) remains stochastic. Let there be an arbitrary enumeration of these null p-values, upon which we let the collection of their bin-assignment random variables be denoted \((\alpha_{i}^{0})_{i=1}^{N_{0}-B_{N+1}^{0}}.\) Let there also be an arbitrary enumeration of these alternative p-values, upon which we let the collection of their bin-assignment random variables be denoted \((\alpha_{i}^{1})_{i=1}^{B_{2:N}^{1}}\). More precisely, \(\alpha_{i}^{0}\sim Unif(\{2,\ldots,N\})\), and \(\alpha_{i}^{1}=j\) with probability \(\frac{q/N+\delta_{j}}{q+\delta-(q/N+\delta_{1})}\), for \(j=2,\ldots,N\). Finally, we will write \(\mathbb{P}_{1}^{bins}\) for the joint distribution derived from the independent coupling of all the random variables \((\alpha_{i}^{0})_{i=1}^{N_{0}-B_{N+1}^{0}}\) and \((\alpha_{i}^{1})_{i=1}^{B_{2:N}^{1}}\). For contrast, let \(\tilde{\alpha}_{i}^{1}\sim Unif(\{2,\ldots,N\})\), which expresses the random bin assignment of any alternative p-value to one of \(B_{2},\ldots,B_{N}\) given that \(B_{1}^{\mathcal{N}}=0\), were \(\mathbb{P}^{1}=U(0,1)\). Then \(\mathbb{P}_{0}^{bins}\), the joint derived from the independent coupling of \((\alpha_{i}^{0})_{i=1}^{N_{0}-B_{N+1}^{0}}\) and \((\tilde{\alpha}_{i}^{1})_{i=1}^{B_{2:N}^{1}}\), offers a useful counterpoint to \(\mathbb{P}_{1}^{bins}\). By the chain rule for KL-divergence, we find

\[D_{KL}\left(\mathbb{P}_{0}^{bins}\|\mathbb{P}_{1}^{bins}\right) =\sum_{i=1}^{N_{0}-B_{N+1}^{0}}D_{KL}\left(\alpha_{i}^{0}\|\alpha _{i}^{0}\right)+\sum_{i=1}^{B_{1}^{1}}D_{KL}\left(\tilde{\alpha}_{i}^{1}\| \alpha_{i}^{1}\right)\] \[=B_{2:N}^{1}\underbrace{\sum_{j}\frac{1}{N-1}log\left(\frac{ \frac{1}{N-1}}{\frac{q/N+\delta_{j}}{q+\delta-(q/N+\delta_{1})}}\right)}_{=:D_{ KL}(\mathbb{P}^{1},1)},\]and by Pinsker's Inequality, for any event \(E\),

\[\mathbb{P}_{1}^{bins}\left(E\right)\geq\mathbb{P}_{0}^{bins}\left(E\right)-\sqrt{ \frac{ln2}{2}D_{KL}\left(\mathbb{P}_{0}^{bins}\|\mathbb{P}_{1}^{bins}\right)}.\]

Let \(a_{i}^{0}\) (respectively \(a_{i}^{1}\)) denote the realization of the \(i\)-th null (respectively alternative) p-value's bin assignment. Then if we set \(E\) to be the collection of realizations \(\left(a_{i}^{0}\right)_{i=1}^{N_{0}-B_{N+1}^{0}}\) and \(\left(a_{i}^{1}\right)_{i=1}^{B_{2N}^{1}}\) that are consistent with \(\cap_{j=2}^{N}[B_{2j}<j-1]\), we find

\[\mathbb{P}\left(\cap_{j=2}^{N}[B_{2j}^{\mathcal{N}}<j-1]\ \|\ B_{1}^{\mathcal{N}}=0,B_{N+1}^{0},B_{2N}^{1}\right)\] \[=\mathbb{P}_{\mu_{1}}\left(E\right)\geq\mathbb{P}_{0}\left(E\right)- \sqrt{\frac{ln2}{2}D_{KL}\left(\mathbb{P}_{0}^{bins}\|\mathbb{P}_{1}^{bins} \right)}\] \[=\left(1-\frac{N_{0}-B_{N+1}^{0}+B_{2:N}^{1}}{N-1}\right)-\sqrt{ \frac{ln2}{2}B_{2N}^{1}D_{KL}(\mathbb{P}^{1},1)}.\] (Corollary 4.2)

Hence, for the case of general \(c\),

\[\left(\#\right)=\mathbb{E}\left[\mathbb{P}\left(\cap_{j=c+1}^{N}[B_{c+1:j}^{ \mathcal{N}}<j-c]\ \|\ [B_{1:c}^{\mathcal{N}}=0],B_{N+1}^{0},B_{c+1:N}^{1} \right)\ \|\ [B_{1:c}^{\mathcal{N}}=0],B_{N+1}^{0}\right]\] \[\geq\mathbb{E}\left[\left(1-\frac{N_{0}-B_{N+1}^{0}+B_{c+1:N}^{1} }{N-c}\right)-\sqrt{\frac{ln2}{2}B_{c+1:N}^{1}D_{KL}(\mathbb{P}^{1},c)}\ \|\ [B_{1:c}^{\mathcal{N}}=0],B_{N+1}^{0}\right]\] (Pinsker's Inequality) \[\geq 1-\frac{N_{0}-B_{N+1}^{0}+\mathbb{E}\left[B_{c+1:N}^{1}\|B_{1:c}^{1}=0\right]}{N-c}-\sqrt{\frac{ ln2}{2}\mathbb{E}\left[B_{c+1:N}^{1}\|B_{1:c}^{1}=0\right]D_{KL}(\mathbb{P}^{1},c)}.\]

It follows that

\[\mathbb{P}\left(\tilde{k}_{+c}=c,B_{N+1}^{0}\geq c\right)\geq\] \[\mathbb{E}\left[\underbrace{\mathbb{P}\left(B_{1:c}^{\mathcal{N}}=0\ \|\ B_{N+1}^{0}\right)}_{\left(1-\frac{c}{N}-\delta_{1:c}\right)^{N_{1}}(1- \frac{c}{N})^{N_{0}-B_{N+1}^{0}}}\left(1-\frac{N_{0}-B_{N+1}^{0}+\mathbb{E} \left[B_{c+1:N}^{1}\|B_{1}^{1}=0\right]}{N-c}-\sqrt{\frac{ln2}{2}\mathbb{E} \left[B_{c+1:N}^{1}\|B_{1}^{1}=0\right]D_{KL}(\mathbb{P}^{1},c)}\right);B_{N+1}^{0}\geq c\right].\]

To conclude,

\[\frac{\mathbb{P}\left(\tilde{k}_{+c}=c,B_{N+1}^{0}\geq 1 \right)}{\left(1-\frac{cq}{N}-\delta_{1:c}\right)^{N_{1}}}\] \[\geq\mathbb{E}\left[\left(1-\frac{c}{N}\right)^{N_{0}-B_{N+1}^{0} }\left(1-\frac{N_{0}-B_{N+1}^{0}+\mathbb{E}\left[B_{c+1:N}^{1}\|B_{1:c}^{1}=0 \right]}{N-c}-\sqrt{\frac{ln2}{2}\mathbb{E}\left[B_{2:N}^{1}\|B_{1}^{1}=0 \right]D_{KL}(\mathbb{P}^{1},c)}\right);B_{N+1}^{0}\geq c\right]\] \[=\left(1-\frac{c}{N}\right)^{N_{0}}\left(1-\underbrace{\frac{N_{0}+ \mathbb{E}\left[B_{c+1:N}^{1}\|B_{1:c}^{1}=0\right]}{N-c}}_{\pi(c,\mu_{1})}- \underbrace{\sqrt{\frac{ln2}{2}\mathbb{E}\left[B_{c+1:N}^{1}\|B_{1:c}^{1}=0 \right]D_{KL}(\mathbb{P}^{1},c)}}_{V(c,\mu_{1})}\right)\underbrace{\mathbb{E} \left[(1-\frac{c}{N})^{-B_{N+1}^{0}};B_{N+1}^{0}\geq c\right]}_{(**)}\] \[+\underbrace{\mathbb{E}\left[(1-\frac{c}{N})^{N_{0}-B_{N+1}^{0}} \frac{B_{N+1}^{0}}{N-c};B_{N+1}^{0}\geq c\right]}_{(***)}\!\!\!,\]

where

\[(**)=\mathbb{E}\left[\left(1-\frac{c}{N}\right)^{-B_{N+1}^{0}} \right]-\mathbb{E}\left[\left(1-\frac{c}{N}\right)^{-B_{N+1}^{0}};B_{N+1}^{0} \leq c-1\right]=\frac{N-cq}{N-c}-\mathbb{E}\left[\left(1-\frac{c}{N}\right)^{-B_{N+1}^{0}} ;B_{N+1}^{0}\leq c-1\right],\] \[(***)\geq\mathbb{P}\left(B_{N+1}^{0}\geq c\right)\left(1-\frac{c}{N} \right)^{N_{0}-\mathbb{E}\left[B_{N+1}^{0}\|B_{N+1}^{0}\geq c\right]}\frac{ \mathbb{E}\left[B_{N+1}^{0}\|B_{N+1}^{0}\geq c\right]}{N-c}\ \ \text{(Jensen's)}\]

### Simulations and Data Experiments

#### 7.4.1 INCREASE-c Simulations on PRDS p-values

In Table 4 we illustrate the effectiveness of INCREASE-c in disrupting the nonparametric outlier detection method of [5] that is based on the application of BH on conformal p-values, and in doing so, demonstrate effectiveness of INCREASE-c on PRDS p-values. We follow the simulation setting of Section 5.2 in [5], using their publicly available source code to generate the conformal p-values. In short, a data set is generated in \(\mathbb{R}^{50}\), along with \(10^{3}\) training observations used to fit a one-class SVM classifier, as well as \(10^{3}\) observations forming a calibration set to be used with a test set to derive (marginal) conformal p-values. In each of \(10^{3}\) independent replications, INCREASE-c was applied to a new test set consisting of \(10^{3}\) conformal p-values, designed to discern inliers (signals drawn from a mixture of multivariate gaussians with identity covariance matrices) from outliers (signals drawn from a mixture of multivariate gaussians with identity covariance matrices scaled by a strength \(\sqrt{a}\)). This was performed for \(a\in\{1,1.5,2,2.5,3\}\); a signal strength \(a=1\) corresponds to identical null and alternative distributions, while larger values of \(a\) make it easier to detect outliers. We set the fraction of outliers in each test set to \(\pi_{1}=.1\), so that a fraction \(\pi_{0}=.90\) of the observations are inliers in each data set.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract and introduction precisely summarize the results of our paper's content.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, we discuss to what extent our analysis could be extended and further generalized, but were not done due to space constraints.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Our appendix contains the full proofs of all theoretical results.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All such information is disclosed in our Experimental Results section.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: Experiments conducted were either with simulations on synthetic data, or a Kaggle data set in which the URL is provided. They are simple to set up, and we described them precisely. The codes we implemented were either our own scripts, or code that we cited from [5].
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Training of an isolation forest was done in Section 5.2. Training of a one-class SVM was also done in Appendix Section 7.4 when we implemented the simulations of [5] as executed with their publicly available code at: [https://github.com/msesia/conditional-conformal-pvalues.git](https://github.com/msesia/conditional-conformal-pvalues.git)
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We found it unnecessary to report error bars as the purpose behind our simulations was to showcase/illustrate how large the false detection proportion could be made as a result of small perturbation. Further, the simulation averages were consistent with our theory.

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Section 5
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We affirm that the research conforms to the code of ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: In the Introduction and Conclusion we discuss potential concerns for safety-security settings
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer:[NA] Justification: [NA]
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We give credit to [5] for the code we implemented in our own experiments.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: [NA]
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA]
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer:[NA] Justification: [NA]