# The Sample Complexity of Gradient Descent

in Stochastic Convex Optimization

 Roi Livni

School of Electrical Engineering

Tel Aviv University

rlivni@tauex.tau.ac.il

###### Abstract

We analyze the sample complexity of full-batch Gradient Descent (GD) in the setup of non-smooth Stochastic Convex Optimization. We show that the generalization error of GD, with common choice of hyper-parameters, can be \(\tilde{\Theta}(d/m+1/\sqrt{m})\), where \(d\) is the dimension and \(m\) is the sample size. This matches the sample complexity of _worst-case_ empirical risk minimizers. That means that, in contrast with other algorithms, GD has no advantage over naive ERMs. Our bound follows from a new generalization bound that depends on both the dimension as well as the learning rate and number of iterations. Our bound also shows that, for general hyper-parameters, when the dimension is strictly larger than number of samples, \(T=\Omega(1/\epsilon^{4})\) iterations are necessary to avoid overfitting. This resolves an open problem by Amir, Koren, and Livni [3], Schliserman, Sherman, and Koren [20], and improves over previous lower bounds that demonstrated that the sample size must be at least square root of the dimension.

## 1 Introduction

Stochastic Convex Optimization (SCO) is a theoretical model that depicts a learner that minimizes a (Lipschitz) convex function, given finite noisy observations of the objective [22]. While often considered simplistic, in recent years SCO has become a focus of theoretical research, partly, because of its importance to the study of first-order optimization methods. But, also, it has become focus of study because it is one of few theoretical settings that exhibit _overparameterized learning_. In more detail, classical learning theory often focuses on the tension between number of samples, or training data, and the complexity of the model to be learnt. A common wisdom of classical theories [1, 7, 14, 24] is that, to avoid overfitting, the complexity of a model should be adjusted in proportion to the amount of training data. However, recent advances in Machine Learning have challenged this viewpoint. Evidently [18, 25], state-of-the-art algorithms generalize well but without, explicitly, controlling the capacity of the model to be learnt. In turn, today, it is one of the most emerging challenges, for learning theory, to understand learnability when the number of parameters in a learnt model exceeds the number of examples, and when, seemingly, nothing withholds the algorithm from overfitting.

Towards this, we look at SCO. In SCO, Shalev-Shwartz, Shamir, Srebro, and Sridharan [22] showed how algorithms can overfit with _dimension dependent_ sample size. But, at the same time, it was known [8, 26] that there are algorithms that provably avoid overfitting with far fewer examples than dimensions. As such, SCO became a canonical model to study how a well-designed algorithm can avoid overfitting even when the number of examples is too small to guarantee generalization by an algorithmic-independent argument [2, 3, 4, 5, 11, 12, 16, 20, 21, 22]. A step towards understanding _what_ induces generalization is to identify _which_ algorithms generalize. Then, we can ask what yields the separation. Surprisingly, for many well-studied algorithms this question is not always answered.

Perhaps the simplest algorithm, whose sample complexity is not yet understood, is Gradient Descent (GD). And we turn to the basic question of the sample complexity of gradient descent.

While this question remained open, there have been several advancements and intermediate answers: The first, dimension independent, generalization bound was given by Bassily, Feldman, Guzman, and Talwar [6] that provided stability bounds [8]. The result of Bassily et al. demonstrated that, GD can have _dimension-independent_ sample complexity rate. However, to achieve that, one has to use non-standard choice of hyperparameters which affects the efficiency of the algorithm. In particular, the number of rounds becomes quadratic in the size of the sample (as opposed to linear, with standard choice). On the other hand, a classical covering argument shows that linear dependence in the dimension is the worst possible, for any empirical risk minimizer, irrespective of properties such as stability.

In terms of lower bounds, Amir, Koren, and Livni [3] were the first to show that GD may have a dimension dependence in the sample complexity. They showed that, with natural hyperparameters, the algorithm must observe number of samples that is at least logarithmic in the dimension. This result was recently improved by Schliserman, Sherman, and Koren [20] that showed that at least square root of the dimension is required. Taken together, so far it was shown that either the algorithm's hyperparameters are tuned to achieve stability, at a cost in running time, or the algorithm must suffer _some_ dimension dependence, linear at worst square root at best.

Here, we close the gap and show that linear dependence is necessary. Informally, we provide the following generalization error bound, in terms of dimension \(d\), sample size, \(m\), and hyperparameters of the algorithm, \(\eta\) and \(T\) (the learning rate and number of iterations). We show that when \(T\) is at most cubic in the dimension (see Theorem 1 for a formal statement):

\[\text{Generalization gap of GD}=\Omega\left(\min\left\{\frac{d}{m},1\right\} \cdot\min\left\{\eta\sqrt{T},1\right\}\right).\]

The first factor in the RHS describes the linear dependence of the generalization error in the dimension, and corresponds to the optimal sample complexity of empirical risk minimizers, as demonstrated by Carmon, Livni, and Yehudayoff [11]. The second term lower bounds the stability of the algorithm [6], and played a similar role in previous bounds [3, 20]. Each factor is optimal at a certain regime, and cannot be improved. Most importantly, for a standard choice of \(\eta=O(1/\sqrt{T})\), the first term is dominant, and the aformentioned lower bound is complemented with the upper bound of Carmon et al. [11]. Our result implies, then, a sample complexity of \(\widetilde{\Theta}(d/m+1/\sqrt{m})\). When \(d\geq m\), the second factor is dominant. When running time is at most quadratic in number of examples, this term also governs the stability of the algorithm, hence the result of Bassily et al. [6] provides a complementary upper bound (see further discussion in Section 3.1).

## 2 Background

We consider the standard setup of Stochastic Convex Optimization (SCO) as in [22]. Set \(\mathcal{W}=\{w:\|w\|\leq 1\}\), and let \(\mathcal{Z}\) be an arbitrary, finite domain (our main result is a lower bound, hence finiteness of \(\mathcal{Z}\) is without loss of generality). We assume that there exists a function \(f(w,z)\) that is convex and \(L\)-Lipschitz in \(w\in\mathcal{W}\) for every choice of \(z\in\mathcal{Z}\). Recall that a function \(f\) is convex and \(L\)-Lipschitz if for any \(w_{1},w_{2}\in\mathcal{W}\) and \(0\leq\lambda\leq 1\):

\[f(\lambda w_{1}+(1-\lambda)w_{2})\leq\lambda f(w_{1})+(1-\lambda)f(w_{2}), \text{ and, }|f(w_{1})-f(w_{2})|\leq L\|w_{1}-w_{2}\|. \tag{1}\]

First order optimizationAlgorithmically we require further assumptions concerning any interaction with the function to be optimized. Recall [19] that, for fixed \(z\), the sub-gradient set of \(f(w,z)\) at point \(w\) is the set:

\[\partial f(w,z)=\left\{g:f(w^{\prime},z)\geq f(w,z)+g^{\top}(w^{\prime}-w), \forall w^{\prime}\in\mathcal{W}\right\}.\]

A first order oracle for \(f\) is a mapping \(\mathcal{O}_{z}(w)\) such that \(\mathcal{O}_{z}(w)\in\partial f(w,z).\) Our underlying assumption is that a learner has a first order oracle access. In other words, given a function \(f(w,z)\), we will assume that there is a procedure \(\mathcal{O}_{z}\) that calculates and returns a subgradient at every \(w\) for every \(z\). Recall [10, 19] that when \(|\partial f(w,z)|=1\), the function is differentiable, at \(w\), and in that case, the unique subdifferential is the gradient \(\nabla f(w,z)\).

### Learning

A learning algorithm \(A\), in SCO, is any algorithm that receives as input a sample \(S=\{z_{1},\ldots,z_{m}\}\in\mathbb{Z}^{m}\) of \(m\) examples, and outputs a parameter \(w_{S}\). An underlying assumption in learning is that there exists a distribution \(D\), unknown to the learner \(A\), and that the sample \(S\) is drawn i.i.d from \(D\). The goal of the learner is to minimize the population loss:

\[F(w)=\operatorname*{\mathbb{E}}_{z\sim D}[f(w,z)],\]

More concretely, We will say that the learner has sample complexity \(m(\varepsilon)\) if, assuming \(|S|\geq m(\varepsilon)\), then w.p. \(1/2\) (Again, because we mostly care about lower bounds, fixing the confidence will not affect the generality of our result):

\[F(w_{S})-\min_{w\in W}F(w)\leq\varepsilon. \tag{2}\]

Empirical Risk MinimizationA natural approach to perform learning is by _Empirical Risk Minimization_ (ERM). Given a sample \(S\), the empirical risk is defined to be:

\[F_{S}(w)=\frac{1}{|S|}\sum_{z\in S}f(w,z).\]

An \(\varepsilon\)-ERM is any algorithm that, given sample \(S\), returns a solution \(w_{S}\in W\) that minimizes the empirical risk up to additive error \(\varepsilon>0\). Recently, Carmon et al. [11] showed that any \(\varepsilon\)-ERM algorithm has a sample complexity bound of

\[m(\varepsilon)=\tilde{O}\left(\frac{d}{\varepsilon}+\frac{1}{\varepsilon^{2} }\right), \tag{3}\]

The above rate is optimal up to logarithmic factor [12]. Namely, there exists a construction and an ERM that will fail, w.p. \(1/2\), unless \(m=\Omega(d/\varepsilon)\) examples are provided1. Importantly, though, there are algorithms that can learn with much smaller sample complexity. In particular SGD [26], stable-GD [6] and regularized ERMs [8].

Footnote 1: The \(\Omega(1/\varepsilon^{2})\) sample complexity bound is more straightforward and follows from standard information-theoretic arguments

#### Gradient Descent

We next depict Gradient Descent whose sample complexity is the focus of this work. GD depends on hyperparameters \(T\in\mathbb{N}\) and \(\eta\geq 0\) and operates as follows on the empirical risk. The algorithm receives as input a sample \(S=\{z_{1},\ldots,z_{m}\}\), defines \(w_{0}=0\), and operates for \(T\) iterations according to the following recursion:

\[w_{t}=\Pi\left[w_{t-1}-\frac{\eta}{|S|}\sum_{z\in S}\mathcal{O}_{z}(w_{t}) \right]\Rightarrow\ w_{S}^{GD}:=\frac{1}{T}\sum_{t=1}^{T}w_{t}, \tag{4}\]

where \(\Pi\) is the projection onto the unit ball, and \(\mathcal{O}_{z}(w_{t})\) is a subgradient of the loss function \(f(w,z)\) at \(w_{t}\). The final output, \(w_{S}^{GD}\), of the algorithm is the averaged iterate (our result, though, can be generalized to other possible suffix-averages such as, say, outputting the last iterate, see Theorem 10). GD constitutes an \(\varepsilon\)-ERM. Concretely, it is known [10, 17] that GD minimizes the empirical risk and its optimization error is given by:

\[F_{S}(w_{S}^{GD})-\min_{w\in W}F_{S}(w)=\Theta\left(\min\left(\eta+\frac{1}{ \eta T},1\right)\right). \tag{5}\]

The above bound is tight irrespective of the dimension2. The population loss have also been studied, and Bassily et al. [6] demonstrated the following learning guarantee:

Footnote 2: For completeness, we demonstrate the lower bound for \(d=1\) at Appendix E

\[\operatorname*{\mathbb{E}}_{S\sim D^{m}}\left[F(w_{S}^{GD})-\min_{w\in W}F(w) \right]=O\left(\eta\sqrt{T}+\frac{1}{\eta T}+\frac{\eta T}{m}\right). \tag{6}\]

The last two terms in the RHS follow from a stability argument, provided in [6], and the first term follows from the optimization error of GD as depicted in Eq. (5). Notice that there is always an \(O(\eta\sqrt{T})\) gap between the generalization error and empirical error of gradient descent.

## 3 Main Result

**Theorem 1**.: _For every \(d\geq 4096,T\geq 10,m\geq 1\) and \(\eta>0\), there exists a distribution \(D\), and a \(4\)-Lipschitz convex function \(f(w,z)\) in \(\mathbb{R}^{d+1}\), such that for any first order oracle of \(f(w,z)\), with probability \(1/2\), if we run GD with \(\eta\) as a learning rate then:_

\[F(w_{S}^{GD})-\min_{w\in\mathcal{W}}F(w)\geq\frac{1}{2\cdot 272\cdot 16^{2}} \cdot\min\left\{\frac{d}{1032m},1\right\}\cdot\min\left\{\eta\sqrt{\min\{\lfloor d ^{3}/136\rfloor,T\}},1\right\}.\]

We remark, that the above theorem is true for _any_ suffix averaging (e.g. last iterate), and not restricted to the averaged iterate (see Theorem 10). We now specialize our bound for two interesting regimes. First, we improve previous dependence in the dimension in [3, 20] and obtain a generalization error bound for \(d=\Omega(m+T^{1/3})\):

**Corollary 2**.: _Fix \(\eta\), and suppose \(d=\Omega\left(m+T^{1/3}\right)\). There exists a distribution \(D\), and an \(O(1)\)-Lipschitz convex function \(f(w,z)\) in \(\mathbb{R}^{d}\), such that for any first order oracle of \(f(w,z)\), with probability \(1/2\), if we run GD for \(T\) iterations, then:_

\[F(w_{S}^{GD})-\min_{w\in\mathcal{W}}F(w)\geq\Omega\left(\min\left\{\eta\sqrt{ T}+\frac{1}{\eta T},1\right\}\right). \tag{7}\]

The first term follows from Theorem 1, the second term follows from the optimization error in Eq. (5). Equation (7) does not hold for \(d<m\), and the linear improvement over [2, 20] is tight. This can be seen from Eq. (5) that shows that GD achieves \(\varepsilon\) empirical excess error when \(\eta=O(1/\sqrt{T})\) and \(T=O(1/\varepsilon^{2})\). Equation (7) becomes vacuous for such choice of parameters, but Carmon et al. [11] showed that the sample complexity of _any ERM_ is bounded by \(\tilde{O}((d+\sqrt{m})/m)\). However, as depicted next, this upper bound becomes tight and GD does not improve over a worst-case ERM:

**Corollary 3**.: _Suppose \(T=O(m^{1.5})\), and \(\eta=\Theta(1/\sqrt{T})\). There exists a distribution \(D\), and an \(O(1)\)-Lipschitz convex function \(f(w,z)\) in \(\mathbb{R}^{d}\), such that for any first order oracle of \(f\), with probability \(1/2\), if we run GD with \(\eta\), for \(T\) iterations:_

\[F(w_{S}^{GD})-\min_{w\in\mathcal{W}}F(w)\geq\Omega\left(\min\left\{\frac{d}{m }+\frac{1}{\sqrt{m}},1\right\}\right).\]

Corollary 3 complements Carmon et al. [11] upper bound, and improves over Feldman [12] lower bound that only showed existence of _some_ ERM with the aforementioned sample complexity. To see that Corollary 3 follows from Theorem 1, notice that when \(d\leq\sqrt{m}\), then \(d/m<1/\sqrt{m}\) and the bound is dominated by the second term, which is a well known-information theoretic lower bound for learning. When \(d>\sqrt{m}\), and \(T<m^{1.5}\) we have that \(T\leq d^{3}\), plugging \(\eta=O(1/\sqrt{T})\) yields the bound.

### Discussion

Theorem 1 provides a new generalization error bound for GD. It shows that the worst case sample complexity for ERMs, derived by Feldman [12], is in fact applicable also to a very natural first order algorithm and not just to abstract ERMs. This Highlights the importance of choosing the right algorithm for learning in SCO. As discussed, the bound is tight in several regimes, nevertheless still there are unresolved open problems.

Stability in low dimensionWhen GD is treated as a naive empirical risk minimizer, and \(\eta=O(1/\sqrt{T})\), \(T=O(m)\), there is no improvement, when using GD, over a worst-case ERM. In the other direction, for dimension that is linear in \(m\), one cannot improve over the \(\Omega(\eta\sqrt{T})\) term that governs stability. Our bound, though, provide a hope that stability in low dimension can yield an improved bound. In particular, consider the case where \(\eta=1/T^{1/4}\) and \(d<m\). This is a case where we apply a stable algorithm in small dimensions. Our bound does not negate the possibility of an improved generalization bound. That would mean that, at least at some regime, GD can improve over the worst-case ERM behaviour. We leave it as an open problem for future study

**Open Question 4**.: _Is there a generalization bound for GD such that_

\[\mathop{\mathbb{E}}_{S\sim D^{m}}\left[F(w_{S}^{GD})-\min_{w\in\mathcal{W}}F(w) \right]=O\left(\frac{d\eta\sqrt{T}}{m}+\frac{1}{\sqrt{m}}\right).\]

_Alternatively, can we prove an improved generalization error bound such that:_

\[\mathop{\mathbb{E}}_{S\sim D^{m}}\left[F(w_{S}^{GD})-\min_{w\in\mathcal{W}}F(w) \right]=\Omega\left(\min\left\{\frac{d}{m},\eta\sqrt{T},1\right\}\right).\]

Late stoppingAnother regime where there is a gap between known upper bound and lower bound appears when \(T=\Omega(m^{2})\). Specifically, the stability upper bound for GD by Bassily et al. [6] gives

\[\mathop{\mathbb{E}}_{S\sim D^{m}}\left[F(w_{S}^{GD})-\min_{w\in\mathcal{W}}F(w )\right]=O\left(\eta\sqrt{T}+\frac{\eta T}{m}+\frac{1}{\eta T}\right).\]

By Corollary 2, for large enough dimension:

\[\mathop{\mathbb{E}}_{S\sim D^{m}}\left[F(w_{S}^{GD})-\min_{w\in\mathcal{W}}F(w )\right]=\Omega\left(\min\left\{\eta\sqrt{T}+\frac{1}{\eta T},1\right\}\right).\]

When \(T=O(m^{2})\), the two bounds coincide. Indeed, for the \(\eta T/m\) term to dominate the \(\eta\sqrt{T}\) term, we must have \(T=\Omega(m^{2})\). One has to take at least \(T=O(m^{2})\) iterations in order to generalize with GD (in fact, any full batch method [2]), however \(T=O(m^{2})\) iterations are sufficient. Nevertheless, the above gap does yield the possibility of an _unstable_ GD method that does generalize. Particularly, if we just regulate the term \(\eta\sqrt{T}\), but allow \(\eta T/m=\Omega(1)\), then this may yield a regime where GD is unstable (and ERM bounds do not apply) and yet generalize.

**Open Question 5**.: _Are there choices of \(\eta\) and \(T\) (that depend on \(m\)) such that \(\eta T/m\in\Omega(1)\), but GD has dimension independent sample complexity?_

Notice that the \(\eta T/m\) term also governs stability in the _smooth_ convex optimization setup [13]. Recall that a function \(f(w,z)\) is said to be \(\beta\)-smooth if for all \(z\), \(f(w,z)\) is differentiable, and the gradient is an \(\beta\)-Lipschitz mapping [10, 15]. For smooth optimization, even if \(\eta\sqrt{T}=\Omega(1)\), GD is still stable. Hardt, Recht, and Singer [13] showed that the stability of GD in the smooth case is governed by \(O\left(\frac{\eta T}{m}\right)\) for \(\eta<1/\beta\). Therefore, the question of generalization when \(\eta T/m\in\Omega(1)\) remains open, even under smoothness assumptions:

**Open Question 6**.: _Assume that \(f(w,z)\) is \(\Theta(1)\)-smooth. What is the sample complexity of GD, when \(\eta\) and \(T\) are chosen so that \(\eta+\frac{1}{\eta T}=o(1)\), but \(\frac{\eta T}{m}=\Omega(1)\)._

## 4 Technical Overview

We next provide a high level overview of our proof technique. For simplicity of exposition we begin with the case \(T=m=d\). We begin by a brief overview of previous construction by Amir et al. [3] that demonstrated Corollary 2 when \(m=\Omega(\log d)\). The construction in [3] can be decomposed into three terms:

\[f(w,z)=g(w,z)+N_{0}(w)+h(w,z).\]

The function \(g\) has the property that an ERM may fail to learn, unless dimension dependent sample size is considered. Amir et al. [3] incorporated Shalev-Shwartz et al. [22] construction. Later, [20] used Feldman's function [12] to construct \(g\). The shift from the construction depicted in Shalev-Shwartz et al. [22] to Feldman's function is the first step that allows to move from logarithmic to polynomial dependence in the dimension. In both constructions an underlying property of \(g\) is that there exists a distribution \(D\) such that, for small samples, there are overfitting minima. Concretely, there exists a \(w_{S}\in\{0,1/\sqrt{d}\}^{d}\) such that

\[\frac{1}{|S|}\sum_{z\in S}g(w_{S},z)-\mathop{\mathbb{E}}_{z\sim D}[g(w_{S},z) ]=\Omega(1). \tag{8}\]

The challenge is then, to make gradient descent's trajectory move towards the point \(w_{S}\). The idea can be decomposed into two parts:

#### Simplifying with an adversarial subgradient:

To simplify the problem, let us first ease the challenge and suppose we can choose our subgradient oracle in a way that depends on the observed sample. Let \(N_{0}\) be the Nemirovski function [9]:

\[N_{0}(w)=\max\{-w(i),0\}.\]

Notice that \(N_{0}\) is not differentiable and the choice of subgradient at certain points is not apriori determined. For example, notice that every standard basis vector \(-e_{i}\in\partial N_{0}(0)\). More generally, given a sample \(S\), let \(I=i_{1},\ldots,i_{d^{\prime}}\) be exactly the set of indices such that \(w_{S}\), from Eq. (8), \(w_{S}(i)\neq 0\). Now assume by induction that \(w_{t}(i)>0\) exactly for \(i=i_{1},\ldots i_{t}\), then one can show that we can define the subgradient oracle of \(N_{0}\):

\[\mathcal{O}(w_{t})=-e_{i_{t+1}}\in\partial N_{0}(w_{t}).\]

In that case \(w_{t+1}\) will satisfy our assumption for \(i_{t+1}\) and we can continue to follow this dynamic for \(T\) steps.

Notice that, in this case, GD will converge to \(w_{S}\) (if \(\eta=1/\sqrt{d}\) which we assume now for concreteness). One can also show that the output of GD (the averaged iterate) will overfit. The caveat is that our subgradient oracle depends on the sample \(S\). In reality, the sample is drawn independent of the subgradient oracle. and all previous constructions, as well as ours need to handle this. This is discussed in the next section. But before that, let us review another challenge which is when \(T\neq d\):

When \(d\ll T\)Another challenge we face with the construction above is that it works when we assume that \(T\approx d\). That is because, in Nemirovski's function, the number of iterates we can perform is bounded by the dimension. After \(d\) iterations we will end up with the vector \(v=\sum_{t=1}^{d}\eta e_{i_{t}}\). If \(T=\omega(d)\) then \(\eta=o(1/\sqrt{d})\), and the dynamic will end up with a too small norm vector to induce a sizeable population loss. This strategy will provide, at best, with a factor of the form \(\Omega\left(\eta\sqrt{\min\{d,T\}}\right)\). Such a factor may be unsatisfactory in a very natural setting where, say, \(T=O(m)\), \(\eta=O(1/\sqrt{m})\), and \(d=\Omega(\sqrt{m})\). To obtain the \(d^{3}\) dependence, we perform the following alternation over the Nemirovski function. Consider the function:

\[N(w)=\max\{0,\max_{i\leq d}\{-w(i)\},\max_{i\leq j\leq d}\{w(j)-w(i)\}\}. \tag{9}\]

And suppose that at each iteration we return a subgradient as follows:

* If there is \(i\leq d\), such that \(w(i)=w(i+1)>\eta\), return subgradient \(e_{i+1}-e_{i}\) and \(w\) is updated by \(w_{t+1}=w_{t}-\eta e_{i+1}+\eta e_{i}\).
* If there is no such \(i\), then take the minimal \(i\) (if exists) such that \(w(i)=0\), and return subgradient \(-e_{i}\) and update \(w_{t+1}=w_{t}+\eta e_{i}\).
* When non of the above is met, return subgradient \(0\).

The dynamic of the above scheme is depicted in Fig. 1, and solves the problem when \(T\approx d^{3}\). One can show that GD will run for at least \(d^{3}\approx T\) iterations, and will increase \(O(d)\) coordinates, each, on average, by an order of \(O(\eta d)\). This is better than the increase of \(\eta\) in each coordinate that we get from Nemirovski's function. In this way we obtain the improved result of \(\eta\sqrt{T}\), even when \(T\approx d^{3}\).

When \(T\ll d\),when the number of iterations is smaller than \(d\) we face a different challenge. The immediate solution is to embed in \(\mathbb{R}^{d}\) a construction from \(\mathbb{R}^{T}\), this will provide us with the \(\Omega(\eta\sqrt{T})\) term but, on the other hand, such a construction will not yield a \(\Omega(d/m)\) term. A different approach, that exploits the dimension to its fullest, is to consider blocks of coordinates and operate on those instead of single coordinates.

The conclusive outcome incorporates both ideas together, and we replace the Nemirovski function with a version of Eq. (9) that operates on \(O(T^{1/3})\) blocks of coordinates. And this concludes our construction. We next move on to the challenge of replacing the data dependent oracle with a standard first order oracle.

#### Reduction to sample dependent oracle:

As discussed, the construction above does not yield a lower bound as it relies on a subgradient oracle that is dependent on the whole sample. To avoid such dependence of the oracle on the sample, we observe that if we can infer the sample \(S\) from the trajectory, i.e. if the state \(w_{t}\) "encodes" the sample, then formally the subgradient is allowed to "decipher" the sample from the point \(w_{t}\). In that way we achieve this behaviour of sample dependent subgradient oracle. This part becomes challenging and may depend on the way we choose \(g\), and \(N\). The simplest case, studied by Amir et al. [3], introduced the third function, \(h\), which was a small perturbation function that elevates coordinates in \(I\) and inhibits coordinates not in \(I\). The function \(h\) depends on \(z\) and not on \(S\), hence it cannot know apriori \(I\). But, an important observation is that, in Shalev-Shwartz et al. [22] construction, if \(i\notin I\), there exists \(z\in S\) that "certifies" that. In fact, each \(z\) can be thought of as a subset of indices, and if an index appears in \(z\), then it cannot be in \(I\). So we can build the perturbation in a way that every coordinate is elevated, unless \(z\) certifies \(i\notin I\): In that case we define \(h(w,z)\) so that its gradient will radically inhibit \(i\).

The last observation is what becomes challenging in our case. As discussed, to achieve improved rate, we need to use Feldman's function. When using Feldman's function the coordinates cannot be ruled out, or identified, by a single \(z\) but one has to look at the whole sample to identify \(I\). While Schliserman et al. [20] tackle a similar problem, we take a slightly different approach described next: For each \(z\) assign a random, positive, number \(\alpha(z)\). We can think of this \(\alpha\) as a hash function. Let us add another coordinate to the vector \(w\), \(w(d+1)\). Consider the function

\[h(w,z)=\gamma\alpha(z)\cdot w(d+1).\]

Then \(\partial h(w,z)=\gamma\alpha(z)e_{d+1}\). Write \(\alpha(S)=\frac{1}{|S|}\sum_{z\in S}\alpha(z)\) then in turn:

\[w_{t}(d+1)=w_{t-1}(d+1)-\partial\frac{\gamma}{|S|}\sum_{z\in S}h(w_{t-1},z)=- t\cdot\gamma\alpha(S)e_{d+1}.\]

If \(\gamma\), \(\alpha(z)\) are chosen correctly, \(\alpha(S)\) is a one to one mapping from samples to real numbers, and small \(\gamma\) ensures that the overall addition of \(h\) has negligible affect on the outcome. Then, we may define the subgradient oracle to be dependent on coordinate \(d+1\) which encodes the whole sample. Our final construction will take a different \(h\), which adds small strong convexity in this coordinate, for reasons next explained:

#### Working with any first order oracle

Notice that our statement is slightly stronger than what we so far illustrated. Theorem 1 states that, for _any_ subgradient oracle, GD will fail. For that, we need to be a little bit more careful, and we want to replace our function with a function that leads to the

Figure 1: Depiction of the dynamics induced by Eq. (16) and our choice of sub-differentials

same guaranteed trajectory, but at the same time it should be differentiable at visited points. This will ensure a unique derivative, making the construction independent of the choice of (sub)gradient oracle.

Towards this goal, we start with the construction depicted so far and consider the set of all values, gradients, and points \(\{f_{j},g_{j},w_{j}\}_{j\in J}\) that our algorithm may visit, for any possible time step and any possible sample, with our construction. Notice that, while this set may be big and even exponential, it is nevertheless finite. What we want is to interpolate a new function through these triplets. In contrast with our original construction, we require a differentiable function at the designated points. Notice, that such an interpolation will have the exact same behaviour when implementing GD on it (with the added feature that the oracle is well defined and unique).

The problem of convex interpolation is well studied, for example Taylor et al. [23] shows sufficient and necessary conditions for interpolation of a smooth function. Our case is slightly easier as we do not care about the smoothness parameter. On the other hand we do require Lipschitzness of the interpolation. We therefore provide an elementary, self-contained, proof to the following easy to prove Lemma, (proof is provided in Appendix B)

**Lemma 7**.: _Let \(G=\{f_{j},g_{j},w_{j}\}_{j\in J}\subseteq\mathbb{R}\times\mathbb{R}^{d}\times \mathbb{R}^{d}\) be a triplet of values in \(\mathbb{R}\), and gradients and points in \(\mathbb{R}^{d}\), such that \(\|g_{j}\|\leq L\). Suppose that for every \(i,j\in J\):_

\[f_{i}\geq f_{j}+g_{j}^{\top}(w_{i}-w_{j}), \tag{10}\]

_and let_

\[I_{\text{diff}}=\{i:f_{i}=f_{j}+g_{j}^{\top}(w_{i}-w_{j})\Rightarrow g_{i}=g_{ j}\}.\]

_Then there exists a convex \(L\)-Lipschitz function \(\hat{f}\) such that for all \(j\in J\): \(\hat{f}(w_{j})=f_{j}\), and for all \(i\in I_{\text{diff}}\), \(\hat{f}\) is differentiable at \(w_{i}\) and:_

\[\nabla f(w_{i})=g_{i}.\]

With Lemma 7 at hand, consider the function

\[h(w,z)=\frac{1}{2}(w(d+1))^{2}+\alpha(z)\cdot w(d+1).\]

The above function encodes in \(w(d+1)\) the sample and time-step as before. Moreover, because it is slightly strongly convex (in coordinate \(d+1\)), \(w_{1}(d+1)\neq w_{2}(d+1)\) ensures that

\[h(w_{1},z)>h(w_{2},z)+\nabla h(w_{2},z)^{\top}(w_{1}-w_{2}),\]

Then the term \(h\) in \(f\) ensures that the triples \(\{f_{j},g_{j},w_{j}\}\) along the trajectory generate gradient vectors that satisfy strict inequality in Eq. (10) and in turn, our interpolation from Lemma 7 will be differentiable at these points. There's some technical subtlety because the interpolation needs to also take the averaged iterate into account, but this is handled in a similar fashion.

In the next two sections we provide more formal statements of the two main ingredients: First, we define a setup of optimization with a sample-dependent first order Oracle and state a lower bound for the generalization error in this setup. The second ingredient is a reduction from the standard setup of first order optimization.

### Sample-dependent Oracle

As discussed, the first step in our proof is to consider a slightly weaker setup where the first-order oracle may depend on the whole sample. Let us formally define what we mean by that. Define

\[\mathcal{S}_{m}^{T}=\{\mathbf{S}=(S_{1},\ldots,S_{t}),S_{i}\in\cup_{i=1}^{m} \mathbb{Z}^{m},t\leq T\},\]

the set of all subseqences of samples of size at most \(m\). Given a function \(f(w,z)\), a sample dependent oracle, \(\mathcal{O}_{\mathcal{S}}\), is a finite sequence of first order oracles

\[\mathcal{O}_{\mathcal{S}}=\{\mathcal{O}^{(t)}(S;w,z)\}_{t=1}^{T},\]

that each receive as input a finite sample \(S\), as well as \(w\) and returns a subgradient:

\[\mathcal{O}^{(t)}(S,w,z)\in\partial f(w,z).\]The sequence of samples can be thought of as the past samples that were observed by the algorithm. In the case of full-batch GD these will be the whole sample, and for SGD, each \(S\) provided to \(\mathcal{O}^{(t)}\) will be all previously observed samples. Given \(\mathbf{S}\in\mathcal{S}_{m}^{T}\) let us also denote

\[\mathcal{O}^{(t)}(\mathbf{S},w)=\frac{1}{|S_{t}|}\sum_{z\in S_{t}}\mathcal{O}^ {(t)}(S_{1:t-1},w,z)\in\partial\left(\frac{1}{|S_{t}|}\sum_{z\in S_{t}}f(w,z) \right), \tag{11}\]

where we let \(\mathbb{S}_{1:0}=\emptyset\), and \(S_{1:t-1}=(S_{1},\ldots,S_{t-1})\) is the concatenated subsample of all previously observed samples in the sequence. As discussed, working with a sample-dependent oracle is easier (for lower bounds). And indeed, our first result shows that, if the subgradient can be chosen in a way that depends on the sample, we can provide the desired lower bound. For fixed and known \(\eta>0\), \(T\), a sample dependent first order oracle \(\mathcal{O}_{\mathbb{S}}\), and a sequence of samples \(\mathbf{S}=(S_{1},S_{2},\ldots,S_{T})\), define \(w_{0}=0\) and inductively:

\[w_{t}^{\mathbf{S}}=\Pi\left[w_{t-1}^{\mathbf{S}}-\eta\mathcal{O}^{(t)}( \mathbf{S},w_{t-1}^{\mathbf{S}})\right],\]

and for every suffix \(\mathbf{s}<T\):

\[w_{\mathbf{S},\mathbf{s}}^{GD}=\frac{1}{T-\mathbf{s}}\sum_{t=\mathbf{s}+1}^{T} w_{t}^{\mathbf{S}} \tag{12}\]

**Lemma 8**.: _For every \(m,d,T\geq 18\) and \(\eta>0\) there are a distribution \(D\), a \(3\)-Lipschitz convex function \(f(w,z)\) in \(\mathbb{R}^{d}\), as well as a sample dependent first order oracle \(\mathcal{O}_{\mathbb{S}}\) such that: if \(\mathbf{S}=(S,S,\ldots S)\in\mathcal{S}_{m}^{T}\) for \(S\sim D^{m}\) i.i.d, then w.p. \(1/2\), for every suffix averaging \(\mathbf{s}\):_

\[F(w_{\mathbf{S},\mathbf{s}}^{GD})-F(0)\geq\frac{1}{\sqrt{2}\cdot 272\cdot 16^{2} }\cdot\min\left\{\frac{d}{1032m},1\right\}\cdot\min\left\{\eta\sqrt{\min\{ \lfloor d^{3}/136\rfloor,T\}},1\right\}.\]

The proof of Lemma 8 is provided in Appendix A.1. We next move to describe the second ingredient of our proof.

### Reduction to sample-dependent oracles

As discussed, the second ingredient of our proof is a reduction to the sample-dependent setup. Instead of using a perturbation function as in [3], we take a more black box approach and show that, given a sample dependent first order oracle, there exists a function that basically induces the same trajectory. Proof is provided in Appendix A.2.

**Lemma 9**.: _Suppose \(q\in\mathbb{R}^{T}\), \(\|q\|_{\infty}\leq 1\). And suppose that \(f(w,z)\) is a convex, \(L\)-Lipschitz, function over \(w\in\mathbb{R}^{d}\), let \(\eta>0\), let \(\mathcal{O}_{\mathbb{S}}\) be a sample dependent first order oracle, and for every sequence of samples \(\mathbf{S}=(S_{1},S_{2},\ldots,S_{T})\) define the sequence \(\{w_{t}^{\mathbf{S}}\}_{t=1}^{T}\) as in Eq. (12)._

_Then, for every \(\varepsilon>0\) there exists an \(L+1\) Lipschitz convex function3\(\tilde{f}((w,x),z)\) over \(\mathbb{R}^{d+1}\) (that depends on \(q,f,T,\eta,m,\mathcal{O}_{\mathbb{S}},\varepsilon\))._

Footnote 3: i.e. \(w\in\mathbb{R}^{d}\) and \(x\in\mathbb{R}\)

_such that for any first order oracle \(\mathcal{O}_{z}\) for \(\tilde{f}\), define \(u_{0}=0\in\mathbb{R}^{d}\) and \(x_{0}=0\in\mathbb{R}\), and:_

\[(u_{t},x_{t})=(u_{t-1},x_{t-1})-\frac{\eta}{|S_{t}|}\sum_{z\in S_{t}}\mathcal{O }_{z}((u_{t},x_{t}))\]

_then if we define:_

\[u_{q}=\sum_{t=1}^{T}q(t)u_{t},\text{ and, }\quad x_{q}=\sum_{t=1}^{T}q(t)x_{t}, \text{ and }w_{q}^{\mathbf{S}}=\sum_{t=1}^{T}q(t)w_{t}^{\mathbf{S}}.\]

_then, we have that \(u_{q}=w_{q}^{\mathbf{S}}\) and:_

\[|\tilde{f}((u_{q},x_{q}),z)-f(w_{q}^{\mathbf{S}},z)|\leq\varepsilon.\]

_and,_

\[|\tilde{f}((0,0),z)-f(0,z)|\leq\varepsilon.\]AcknowledgmentsThe author would like to thank Tamar Livni for creating Figure 1. Tamar holds all copyrights to the artwork. The author would also like to thank Tomer Koren and Yair Carmon for several discussions. This research was funded in part by an ERC grant (FOG, 101116258), as well as an ISF Grant (2188 \(\backslash\) 20).

## References

* [1] N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler. Scale-sensitive dimensions, uniform convergence, and learnability. _Journal of the ACM (JACM)_, 44(4):615-631, 1997.
* [2] I. Amir, Y. Carmon, T. Koren, and R. Livni. Never go full batch (in stochastic convex optimization). _Advances in Neural Information Processing Systems_, 34:25033-25043, 2021.
* [3] I. Amir, T. Koren, and R. Livni. Sgd generalizes better than gd (and regularization doesn't help). In _Conference on Learning Theory_, pages 63-92. PMLR, 2021.
* [4] I. Amir, R. Livni, and N. Srebro. Thinking outside the ball: Optimal learning with gradient descent for generalized linear stochastic convex optimization. _Advances in Neural Information Processing Systems_, 35:23539-23550, 2022.
* [5] I. Attias, G. K. Dziugaite, M. Haghifam, R. Livni, and D. M. Roy. Information complexity of stochastic convex optimization: Applications to generalization and memorization. _arXiv preprint arXiv:2402.09327_, 2024.
* [6] R. Bassily, V. Feldman, C. Guzman, and K. Talwar. Stability of stochastic gradient descent on nonsmooth convex losses. _Advances in Neural Information Processing Systems_, 33:4381-4391, 2020.
* [7] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the vapnik-chervonenkis dimension. _Journal of the ACM (JACM)_, 36(4):929-965, 1989.
* [8] O. Bousquet and A. Elisseeff. Stability and generalization. _The Journal of Machine Learning Research_, 2:499-526, 2002.
* [9] S. Bubeck, Q. Jiang, Y.-T. Lee, Y. Li, and A. Sidford. Complexity of highly parallel non-smooth convex optimization. _Advances in neural information processing systems_, 32, 2019.
* [10] S. Bubeck et al. Convex optimization: Algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 8(3-4):231-357, 2015.
* [11] D. Carmon, R. Livni, and A. Yehudayoff. The sample complexity of errns in stochastic convex optimization. _arXiv preprint arXiv:2311.05398_, 2023.
* [12] V. Feldman. Generalization of erm in stochastic convex optimization: The dimension strikes back. _Advances in Neural Information Processing Systems_, 29, 2016.
* [13] M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient descent. In _International conference on machine learning_, pages 1225-1234. PMLR, 2016.
* [14] D. Haussler and M. Warmuth. The probably approximately correct (pac) and other learning models. _The Mathematics of Generalization_, pages 17-36, 2018.
* [15] E. Hazan et al. Introduction to online convex optimization. _Foundations and Trends(r) in Optimization_, 2(3-4):157-325, 2016.
* [16] R. Livni. Information theoretic lower bounds for information theoretic upper bounds. _Advances in Neural Information Processing Systems_, 36, 2023.
* [17] Y. Nesterov. _Introductory lectures on convex optimization: A basic course_, volume 87. Springer Science & Business Media, 2013.
* [18] B. Neyshabur, R. Tomioka, and N. Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. _arXiv preprint arXiv:1412.6614_, 2014.

* [19] R. T. Rockafellar. _Convex Analysis:(PMS-28)_. Princeton university press, 2015.
* [20] M. Schliserman, U. Sherman, and T. Koren. The dimension strikes back with gradients: Generalization of gradient methods in stochastic convex optimization. _arXiv preprint arXiv:2401.12058_, 2024.
* [21] A. Sekhari, K. Sridharan, and S. Kale. Sgd: The role of implicit regularization, batch-size and multiple-epochs. _Advances In Neural Information Processing Systems_, 34:27422-27433, 2021.
* [22] S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization. In _COLT_, volume 2, page 5, 2009.
* [23] A. B. Taylor, J. M. Hendrickx, and F. Glineur. Smooth strongly convex interpolation and exact worst-case performance of first-order methods. _Mathematical Programming_, 161:307-345, 2017.
* [24] V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. In _Measures of complexity: festschrift for alexey chervonenkis_, pages 11-30. Springer, 2015.
* [25] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.
* [26] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In _Proceedings of the 20th international conference on machine learning (icml-03)_, pages 928-936, 2003.

## Appendix A Proof of Theorem 1

The proof is an immediate corollary of Lemmas 8 and 9, which we prove in Appendices A.1 and A.2 respectively. To see that Theorem 1 indeed follows from these Lemmas, start with \(\eta,d,m,T\) that satisfy the conditions. Let \(f(w,z)\) be the function and \(\O_{\S}\) the sample dependent first order oracle, whose existence follows from Lemma 8 with suffix \(\mathbf{s}=0\). And let \(\bar{f}\) be the function whose existence follows from Lemma 9 to some arbitrarily small \(\epsilon_{0}\), with \(q(t)=\frac{1}{T}\) for all \(t\). It is easy to see that if we apply GD on \(\bar{f}\) and define its output \((u^{GD},x^{GD})\) then: \((u^{GD},x^{GD})=(u_{q},x_{q})\), and \(w_{q}^{\S}=w_{\S}^{GD}\).

Then, we have that w.p. \(1/2\):

\[\bar{F}((u^{GD},x^{GD}))-\bar{F}(0) =\bar{F}(u_{q},x_{q})-\bar{F}(0)\] \[\geq F(w_{\S}^{GD})-2\epsilon_{0}-F(0)\] \[\geq\frac{1}{\sqrt{2}\cdot 272\cdot 16^{2}}\cdot\min\left\{ \frac{d}{1032m},1\right\}\cdot\min\left\{\eta\sqrt{\min\{\lfloor d^{3}/136\rfloor,T\}},1\right\}-2\epsilon_{0}\] \[\geq\frac{1}{2\cdot 272\cdot 16^{2}}\cdot\min\left\{\frac{d}{1032m},1\right\}\cdot\min\left\{\eta\sqrt{\min\{\lfloor d^{3}/136\rfloor,T\}},1 \right\}.\]

Where in the last equation, we assume \(\epsilon_{0}\) to be sufficiently small. Finally, note that \(\bar{F}(0)\geq\min_{w\in\N}\bar{F}(w)\).

Notice, that by the same argument, by taking any suffix \(\mathbf{s}<T\), and setting \(q(t)=0\) for \(t\leq\mathbf{s}\), and \(q(t)=\frac{1}{T-\mathbf{s}}\) for \(t\geq\mathbf{s}+1\), we can obtain the following stronger result for any suffix averaging:

**Theorem 10**.: _For every \(d\geq 4096,T\geq 10,m\geq 1\) and \(\eta>0\) and suffix \(\mathbf{s}<T\), there exists a distribution \(D\), and a \(4\)-Lipschitz convex function \(f(w,z)\) in \(\R^{d}\), such that for any first order oracle of \(f(w,z)\), with probability \(1/2\), if we run GD with \(\eta\) as a learning rate then:_

\[F\left(\frac{1}{T-\mathbf{s}}\sum_{t=\mathbf{s}+1}^{T}w_{t}^{\S}\right)-F(0) \geq\frac{1}{2\cdot 272\cdot 16^{2}}\cdot\min\left\{\frac{d}{1032m},1 \right\}\cdot\min\left\{\eta\sqrt{\min\{\lfloor d^{3}/136\rfloor,T\}},1\right\}.\]

### Proof of Lemma 8

For simplicity we will assume that \(d=2^{n}\) for some \(n\in\mathbb{N}\), the final result will be obtained by embedding a construction in a subspace of size at least half the original dimension.

We start by recalling Feldman's construction [12]: There exists a set \(\mathcal{V}\subseteq\{0,1\}^{d}\), such that:

\[\left(\forall v_{1}\neq v_{2}\in\mathcal{V},\ v_{1}\cdot v_{2}\leq\frac{5d}{16 }\right),\text{ and }\left(\|v\|^{2}\geq\frac{7d}{16}\right),\text{ and }\left(\left| \mathcal{V}\right|>e^{d/258}\right). \tag{13}\]

Indeed, suppose we pick randomly \(w\in\{0,1\}^{d}\) according to probability \(P\) where each coordinate \(P(w(i))=1\) with probability \(1/2\) (independently). Then, by Hoeffding's inequality for two \(w_{1},w_{2}\sim P\) independently:

\[P\left(w_{1}\cdot w_{2}>\frac{d}{4}+\frac{d}{16}\right)\leq e^{-\frac{d}{128}},\]

\[P\left(w_{1}\cdot w_{1}<\frac{d}{2}-\frac{d}{16}\right)\leq e^{-\frac{d}{128}}.\]

Thus, picking \(\mathcal{V}\) elements i.i.d according to \(P\), randomly, of size \(|\mathcal{V}|\geq e^{d/258}\) we can show by union bound that with positive probability, all \(|\mathcal{V}|^{2}\) pairs in \(\mathcal{V}\) will satisfy the requirement. Next, we define a distribution \(D_{\varepsilon}\) supported on subsets of \(\mathcal{V}\) such that for a random variable \(V\subseteq\mathcal{V}\) each \(v\in V\) w.p. \(\varepsilon\) independently. We start by assuming that \(T\leq\frac{1}{17}d^{3}\) (the case \(T>d^{3}\) is handled at the end), and let \(k\in\mathbb{N}\) be such that:

\[d\leq k\left(\frac{T}{17}\right)^{1/3}<2d.\]

One can show that without loss of generality we can assume \(k\) is also a power of \(2\) (in particular, \(d\) is divisble by \(k\) for large enough \(T\)). We next follow the idea depicted in Section 4, but we want to handle the case \(k\gg 1\). For that, we redefine the function in Eq. (9), and take blocks of coordinates. To simplify notations, let us define for two set of indices \(I,J\) of elements in \([d]\): \(I=\{i_{1},\ldots,i_{k}\}\), \(J=\{j_{1},\ldots,i_{k}\}\), \(I<J\) if \(\max\{i\in I\}<\min\{j\in J\}\). and we will also write:

\[e_{I}=\frac{1}{\sqrt{|I|}}\sum_{i\in I}e_{i},\quad\text{and},w(I)=w\cdot e_{I} =\frac{1}{\sqrt{I}}\sum_{i\in I}w(i).\]

then we define our final function as:

\[N(w)=\max\left\{0,\max_{|I|=k}\{-w(I)\},\max\left\{-(w(I)-w(J)):I<J,|I|=|J|=k \right\}\right\} \tag{14}\]

Define \(\alpha=\min\{\frac{1}{\eta\sqrt{27}},1\}\), and let:

\[f(w,V)=g(w,V)+\alpha N(w).\]

where \(N\) is defined in Eq. (14), and \(g\) is defined to be Feldman's function with a suitable choice of threshold:

\[g(w,V)=\frac{1}{\sqrt{d}}\max_{v\in V}\left\{\frac{45\eta\alpha d^{2}}{2\cdot 1 6^{2}k^{1.5}},w\cdot v\right\}. \tag{15}\]

Notice that \(N\) is \(2\)-Lipschitz, and \(g\) is \(1\)-Lipschitz.

To obtain the trajectory, we next define a sample dependent oracle. We only define it for samples \(\mathbf{S}\) such that there exists \(v^{\star}\not\in V_{i}\) for all \(V_{i}\in S\) (define it arbitrarily to any other type of sample). Let \(\mathcal{I}=\{i_{1},\ldots,i_{d^{\prime}}\}\) be a set of \(\frac{7d}{16}\) indices such that \(v^{\star}(i_{j})\neq 0\). Divide the elements of \(\mathcal{I}\) into \(d^{\prime}/k\) subsets. Namely, let

\[I_{j}=\{i_{(j-1)\cdot k+1},i_{(j-1)\cdot k+2},\ldots,i_{j\cdot k}\},\quad j=1, \ldots,d^{\prime}/k.\]

We start by defining only an oracle for the function \(\alpha N\). We will later show that the trajectory induced by this oracle stays in the minima of \(g\), and that will show that, for our purposes, we can choose the

[MISSING_PAGE_EMPTY:13]

\[w^{\mathbf{S}}_{t}\cdot v \leq\sum_{i=1}^{d^{\prime}/k}w^{\mathbf{S}}_{t}(i_{t})\mathbf{1}\{v( i_{t})=1\}\] \[\leq\max\left\{\sum_{i\in I_{B}}w^{\mathbf{S}}_{T}(i):I_{B}\subseteq I,|I|\leq\frac{5d}{16}\right\}\qquad\qquad Eqs.\,(\ref{eq:13})and\,(\ref{eq:17})\] \[\leq\sum_{t=1}^{\frac{5d}{16k}}\sum_{i\in I_{t}}w^{\mathbf{S}}_{T} (i)\] \[\leq\sum_{t=1}^{\frac{5d}{16k}}\sqrt{k}\eta\alpha((d^{\prime}/k)+1-t) \qquad\qquad\qquad\qquad Eq.\,(\ref{eq:16})\] \[\leq\sum_{t=0}^{\frac{5d}{16k}}\sqrt{k}\eta\alpha((d^{\prime}/k)-t)\] \[\leq\sqrt{k}\eta\alpha\left(\frac{5

[MISSING_PAGE_EMPTY:15]

To simplify notation, let us denote:

\[w^{\mathbf{S}}_{T+1}=w^{\mathbf{S}}_{q},\text{ and }x^{\mathbf{S}}_{T+1}=x^{ \mathbf{S}}_{q},\]

and assume without loss of generality that \(\max\{q(i)\neq 0\}=T\) (Otherwise, we look only at the sequence up to point \(\max\{q(i)\neq 0\}\)). Consider now the sets of triplets:

\[G\left(z\right)=\left\{\left(v,g,u\right)=\left(f(w^{\mathbf{S}}_{t},z)+h_{z}( x^{\mathbf{S}}_{t}),(\mathcal{O}_{\mathbf{S},t,z},\nabla h_{z}(x^{\mathbf{S}}_{t})),(w^{ \mathbf{S}}_{t},x^{\mathbf{S}}_{t})\right):\mathbf{S}^{m}_{T}\in\mathcal{S},t \leq T+1\right\},\]

where \(\mathcal{O}_{\mathbf{S},T+1,z}\in\partial(f(w^{\mathbf{S}}_{q})+h_{z}(x^{ \mathbf{S}}_{q}))\) is chosen arbitrarily.

Convexity of \(f+h_{z}\) ensure that the triplets in \(G\left(z\right)\) satisfy Eq. (10) for all \(t\leq T+1\), as in Lemma 7. To apply the Lemma, we also want to achieve differentiability at points such that \(t<T\). Therefore, take any two triplets

\[\left(v_{i},g_{i},u_{i}\right)=\left(f(w^{\mathbf{S}}_{t_{i}},z)+h_{z}(x^{ \mathbf{S}}_{t_{i}}),\left(\mathcal{O}_{\mathbf{S},t,z},\nabla h_{z}(x^{ \mathbf{S}}_{t_{i}})\right),\left(w^{\mathbf{S}}_{t_{i}},x^{\mathbf{S}}_{t_{i} }\right)\right),i=1,2,\]

where \(t_{1}<T\) and \(t_{2}\leq T+1\), and suppose \(g_{1}\neq g_{2}\). To simplify notations, let us write \(w^{\mathbf{S}}_{t_{i}}=w_{i}\) and \(x^{\mathbf{S}}_{t_{i}}=x_{i}\).

First, by convexity of \(f\) we have that:

\[v_{1}-v_{2}+g^{\top}_{2}(u_{2}-u_{1})= f(w_{1},z)+h_{z}(x_{1})-f(w_{2},z)-h_{z}(x_{2})-\left( \mathcal{O}_{\mathbf{S}_{2},t_{2},z},\nabla h_{z}(x_{2})\right)^{\top}\left((w _{1},x_{1})-(w_{2},x_{2})\right)\] \[= f(w_{1},z)-f(w_{2},z)-\mathcal{O}^{\top}_{\mathbf{S}_{2},t_{2},z }\left(w_{1}-w_{2}\right)+h_{z}(x_{1})-h_{z}(x_{2})-\nabla h_{z}(x_{2})^{\top }\left(x_{1}-x_{2}\right)\] \[\geq h_{z}(x_{1})-h_{z}(x_{2})-\nabla h_{z}(x_{2})^{\top}\left(x_{1}-x _{2}\right).\]

Next, because \(g_{1}\neq g_{2}\), either \(\nabla h_{z}(x_{1})\neq\nabla h_{z}(x_{2})\), which implies \(x_{1}\neq x_{2}\), or \(\mathcal{O}_{\mathbf{S}_{1},t_{1},z}\neq\mathcal{O}_{\mathbf{S}_{2},t_{2},z}\) which implies \((\mathbf{S}_{1},t_{1})\neq(\mathbf{S}_{2},t_{2})\) which again implies \(x_{1}\neq x_{2}\) by Claim 1. In other words, if \(g_{1}\neq g_{2}\) then \(x_{1}\neq x_{2}\):

\[h_{z}(x_{1})-h_{z}(x_{2})-\nabla h_{z}(x_{2})^{\top}\left(x_{1}-x _{2}\right) =\gamma\left(x_{1}^{2}-2\alpha(z)x_{1}-x_{2}^{2}-2\alpha(z)x_{2}- \left(2x_{1}-2\alpha(z)\right)\cdot\left(x_{1}-x_{2}\right)\right)\] \[=\gamma\left(x_{1}^{2}+x_{2}^{2}-2x_{1}\cdot x_{2}\right)\] \[=\gamma\left(x_{1}-x_{2}\right)^{2}\] \[>0 x_{1}\neq x_{2}\]

We showed then, that \(v_{1}-v_{2}+g^{\top}_{2}(u_{2}-u_{1})=0\), implies \(g_{1}=g_{2}\). We obtain, by Lemma 7, that there exists a function \(\tilde{f}((w,x),z)\) such that for all \(t\) and \(\mathbf{S}\):

\[\tilde{f}(w^{\mathbf{S}}_{t},x^{\mathbf{S}}_{t},z)=f(w^{\mathbf{S}}_{t},z)+h_{ z}(x^{\mathbf{S}}_{t}) \tag{23}\]

and for all \(t\leq T\),

\[\nabla\tilde{f}((w^{\mathbf{S}}_{t},x^{\mathbf{S}}_{t}))=(\mathcal{O}_{ \mathbf{S},t,z},\nabla h_{z}(x^{\mathbf{S}}_{t})). \tag{24}\]

This proves Lemma 9. Indeed. By the Lipschitzness of \(h\) in the unit ball, we have that \(|x_{t}|\leq\gamma\eta T\). For sufficiently small \(\gamma\), from Eq. (23), since \(\{(f(0,z)+h_{z}(0),(0,\nabla h_{z}(0)),(0,0))\}\in G(z)\), we obtain that

\[|\tilde{f}((0,0),z)-f(0,z)|=\gamma|h_{z}(0)|\leq\gamma^{2}\eta T\leq\varepsilon.\]

\[|\tilde{f}((w^{\mathbf{S}}_{q},x^{\mathbf{S}}_{q}),z)-f((w^{\mathbf{S}}_{q},z) )|=\gamma|h_{z}(x^{\mathbf{S}}_{q})|\leq\varepsilon.\]

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_EMPTY:19]

we have that:

\[w_{T_{d_{0},d_{1}}}(I_{t})=\alpha\eta\begin{cases}(d_{0}+2-t)&t\leq d_{1}\\ (d_{0}+1-t)&d_{1}<t\leq d_{0}\\ 0&\text{o.w.}\end{cases}\]

For the case \(d_{1}\)=0, \(T_{d_{0},d_{1}}=d_{0}\), and it follows from our outer-induction step. Assume the statement is true for \(d_{1}\) and we will prove it for \(d_{1}+1\), here, yet again, we use induction. And we will show that for \(1\leq d_{2}<d_{0}+2-d_{1}\) we have at time

\[T_{d_{0},d_{1},d_{2}}=T_{d_{0}}+T_{d_{1}}+d_{2},\]

we have that:

\[w_{T_{d_{0},d_{1},d_{2}}}(I_{t})=\alpha\eta\begin{cases}(d_{0}+2-t)&t\leq d_{1} \\ (d_{0}+1-t)&d_{1}<t<d_{0}+2-d_{2}\\ (d_{0}+2-t)&t=d_{0}+2-d_{2}\\ (d_{0}+1-t)&d_{0}+2-d_{2}<t\leq d_{0}\\ 0&\text{o.w.}\end{cases} \tag{28}\]

We start the induction with the case \(d_{2}=1\), in that case notice that \(T_{d_{0},d_{1},d_{2}}=T_{d_{0},d_{1}}+1\), and by induction hypothesis:

\[w_{T_{d_{0},d_{1}}}(I_{t})=\alpha\eta\begin{cases}(d_{0}+2-t)&t\leq d_{1}\\ (d_{0}+1-t)&d_{1}<t\leq d_{0}\\ 0&\text{o.w.}\end{cases}\]

In this case, note that there are no two consecutive coordinates that are equal, hence our choice of oracle is defined so that \(\mathcal{O}^{(t)}=-e_{I_{d_{0}+1}}\). Hence, by our update rule (and the lack of projections which we proved at the beginning):

\[w_{T_{d_{0},d_{1},1}}(I_{t})=\alpha\eta\begin{cases}(d_{0}+2-t)&t\leq d_{1}\\ (d_{0}+1-t)&d_{0}<t\leq d_{0}\\ 1&t=d_{0}+1\\ 0&\text{o.w.}\end{cases}\]

Which satisfies Eq. (28). Now assume that Eq. (28) holds for \(d_{2}\), and take \(d_{2}+1<d_{0}+2-d_{1}\) (otherwise, we are done). Notice that \(T_{d_{0},d_{1},d_{2}+1}=T_{d_{0},d_{1},d_{2}}+1\). Observe that \(w_{T_{0},d_{1},d_{2}}(d_{0}+2-d_{2})=w_{T_{0},d_{1},d_{2}}(d_{0}+1-d_{2})\) (notice that \(d_{0}+1-d_{2}>d_{1}\)), and our update rule is such that \(\mathcal{O}^{(t)}=e_{I_{d_{0}+2-d_{2}}}-e_{I_{d_{0}+1-d_{2}}}\) and we obtain then:

\[w_{T_{d_{0},d_{1},d_{2}+1}}=w_{T_{d_{0},d_{1},d_{2}}}-\eta\alpha e_{I_{d_{0},d _{1},d_{2}}}+\eta\alpha e_{I_{d_{0}+2-d_{2}}}=\alpha\eta\begin{cases}(d_{0}+2 -t)&t\leq d_{1}\\ (d_{0}+1-t)&d_{1}<t<d_{0}+2-(d_{2}+1)\\ (d_{0}+2-t)&t=d_{0}+2-(d_{2}+1)\\ (d_{0}+1-t)&d_{0}+2-(d_{2}+1)<t\leq d_{0}\\ 0&\text{o.w.}\end{cases}\]

The most inner induction step is now complete. We now notice that \(T_{d_{0},d_{1},d_{0}+1-d_{1}}=T_{d_{0},d_{1}+1}\), which proves the middle-induction step. And we notice that \(T_{d_{0},d_{0}+1}=T_{d_{0}+1}\), which proves the whole induction argument.

### Proof of Eq. (17)

We only need to show that the following quantity is increasing in \(t\)

\[X_{t}=\max\left\{\sum_{i\in I_{R}}w_{i}^{\mathcal{S}}(t):I_{B}\subseteq I,|I|= B\right\}.\]

[MISSING_PAGE_FAIL:21]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification:

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [NA]  Justification:  Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA]  Justification:  Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA]  Justification:  Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. ** It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.