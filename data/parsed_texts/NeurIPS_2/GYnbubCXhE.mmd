# Conditional Matrix Flows for

Gaussian Graphical Models

Marcello Massimo Negri

University of Basel

marcellomassimo.negri@unibas.ch &Fabricio Arend Torres

University of Basel

fabricio.arendtorres@unibas.ch &Volker Roth

University of Basel

volker.roth@unibas.ch

###### Abstract

Studying conditional independence among many variables with few observations is a challenging task. Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through \(l_{q}\) regularization with \(q\leq 1\). However, most GMMs rely on the \(l_{1}\) norm because the objective is highly non-convex for sub-\(l_{1}\) pseudo-norms. In the frequentist formulation, the \(l_{1}\) norm relaxation provides the solution path as a function of the shrinkage parameter \(\lambda\). In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different \(\lambda\) requires repeated runs of expensive Gibbs samplers. Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks. As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters \(\lambda\) and all \(l_{q}\) norms, including non-convex sub-\(l_{1}\) pseudo-norms. Within one model we thus have access to (i) the evolution of the posterior for any \(\lambda\) and any \(l_{q}\) (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit.

## 1 Introduction

Estimating complex relationships between random variables is a central problem in science. When only few observations are available, the problem becomes even more challenging. Examples include functional connectivity in fMRI data [Smith et al., 2013], networks of interactions from microarray data [Castelo and Roverato, 2006], or correlation patterns in longitudinal studies [Diggle, 2002]. In such cases, it is particularly challenging to infer the conditional independence among random variables. Graphical models are commonly used to represent such an independence structure in the form of network graphs. In this work, we focus specifically on Gaussian Graphical Models (GGMs). GGMs assume observations \(\mathbf{X}\in\mathbb{R}^{n\times d}\) to be generated from a multivariate Gaussian distribution \(\mathcal{X}\sim\mathcal{N}(\mathbf{\mu},\mathbf{\Sigma})\) with \(\mathbf{X}_{i,:}\) being realizations of \(\mathcal{X}\) for \(i\in\{1,\ldots,n\}\). Here \(\mathbf{\mu}\in\mathbb{R}^{d}\) denotes the mean and \(\mathbf{\Sigma}\in\mathbb{R}^{d\times d}\) the covariance matrix, which is symmetric positive definite \(\mathbf{\Sigma}\succ 0\). GGMs provide a simple interpretation of conditional independence through the precision matrix \(\mathbf{\Omega}=\mathbf{\Sigma}^{-1}\), whenever \(\mathbf{\Sigma}\) is non-singular. Specifically, \(\mathbf{\Omega}_{i,j}=0\) implies that the pair of variables \((i,j)\) is conditionally independent given all remaining variables. That is to say, there is no edge between nodes \(i\) and \(j\) in the underlying undirected graph.

Penalized likelihood formulationGiven the centered observations \(\mathbf{X}\) and the associated sample covariance matrix \(\mathbf{S}=\mathbf{X}^{T}\mathbf{X}\), we are interested in reconstructing the precision matrix \(\mathbf{\Omega}\), hence the underlying graph structure. This is particularly challenging when \(d>n\) because the sample covariance matrix becomes singular and the precision matrix can no longer be obtained by simply inverting the MLE of the covariance matrix. One way to overcome this problem is to consider likelihood-penalized models that encourage sparsity in the precision matrix. In other words, we trade off the likelihood with the number of zeros in \(\mathbf{\Omega}\):

\[\mathbf{\hat{\Omega}}=\operatorname*{arg\,max}_{\mathbf{\Omega}>0}\{\log\det\mathbf{\Omega }-\text{Tr}(\tfrac{1}{2}\mathbf{S}\mathbf{\Omega})-\lambda\|\mathbf{\Omega}\|_{0}\}\;, \tag{1}\]

where \(\log\det\mathbf{\Omega}-\text{Tr}(\tfrac{1}{2}\mathbf{S}\mathbf{\Omega})\) is the log-likelihood term and \(\|\mathbf{\Omega}\|_{0}=\sum_{i<j}1[w_{ij}\neq 0]\) is the \(l_{0}\) norm, which counts the number of non-zero off-diagonal elements of \(\mathbf{\Omega}\). The trade-off between the two terms is controlled by the parameter \(\lambda\geq 0\). Note that the optimization must be performed over the space of symmetric positive definite matrices \(\mathbf{\Omega}\succ 0\). In practice, the objective in Eq. (1) is highly non-convex and cannot be optimized easily. Even in the simpler linear regression setting, \(l_{0}\) regularization can be computed exactly only for few features (Hastie et al., 2015). Similar problems arise for all non-convex sub-\(l_{1}\) pseudo-norms.

Lasso relaxation: Frequentist and Bayesian approachesIn order to simplify Eq. (1), most approaches replace the highly non-convex \(l_{0}\) norm with the closest convex norm, the \(l_{1}\) norm: \(\|\mathbf{\Omega}\|_{1}=\sum_{i<j}|w_{ij}|\). Meinshausen and Buhlmann (2006) first proposed using \(l_{1}\) regularization in the Graphical Lasso model, which sparked the development of several likelihood penalized algorithms (Friedman et al., 2007; Yuan and Lin, 2007; Banerjee et al., 2007). These frequentist approaches make it possible to elegantly compute the solution path as a function of the shrinkage parameter \(\lambda\)(Mazumder and Hastie, 2012). The \(l_{1}\)-relaxed problem can also be easily extended to a Bayesian formulation. Wang (2012) assumed a Laplace prior on the off-diagonal elements of the precision matrix and showed that the frequentist solution can be recovered as the maximum a posteriori estimate (MAP). In the Bayesian framework, it is possible to explore the full posterior distribution, to formulate the posterior predictive, and to use the marginal likelihood for model selection. But in high-dimensional settings, MCMC samplers suffer from poor mixing behaviour, which becomes increasingly difficult to diagnose in higher dimensions. Furthermore, different hyperparameters values, e.g., \(\lambda\), require independent expensive Markov chains. In the literature, alternative priors have also been proposed (Li et al., 2019). However, the dependence of Gibbs samplers on tractable posterior conditionals typically restricts the choice of priors significantly.

Variational InferenceVariational inference approaches (Blei et al., 2017) approximate intractable densities with tractable parameterized distributions. Compared to MCMC samplers, variational inference turns a sampling problem into an optimization one, which generally requires less computational time. Let \(\mathbf{x}\) be the observed variables and \(\mathbf{z}\) the unobserved ones. Given a family \(\mathcal{Q}=\{q_{\theta}(\mathbf{z})|\theta\in\Theta\}\) of parameterized distributions, variational inference involves finding the distribution \(q_{\theta^{*}}(\mathbf{z})\) that best approximates the posterior \(p(\mathbf{z}|\mathbf{x})\). The distance between the two distributions is usually measured as the Kullback-Leibler divergence, which defines the following optimization problem:

\[q_{\theta^{*}}(\mathbf{z})=\operatorname*{arg\,min}_{\theta\in\Theta}\operatorname {KL}\bigl{(}q_{\theta}(\mathbf{z})||p(\mathbf{z}|\mathbf{x})\bigr{)}=\operatorname*{arg\, min}_{\theta\in\Theta}\operatorname{\mathbb{E}}_{\mathbf{z}\sim q_{\theta}} \left[\,\log\frac{q_{\theta}(\mathbf{z})}{p(\mathbf{z}|\mathbf{x})}\right]. \tag{2}\]

The goodness of \(q_{\theta^{*}}(\mathbf{z})\) as an approximation of the posterior \(p(\mathbf{z}|\mathbf{x})\) creates a trade-off between the expressive power of the variational family \(\mathcal{Q}\) and the tractability of the optimization in Eq. (2). Common approaches rely on the so-called mean field approximation, which assumes mutual independence among variables: \(q_{\theta}(\mathbf{z})=\prod_{i}q_{\theta_{i}}(z_{i})\). But this is not viable in GGMs as we intend to model precisely the dependence structure. Several approaches have been proposed for Bayesian Lasso (Alves et al., 2021) and for its group-sparse variant (Babacan et al., 2014) Yet, to the best of our knowledge, variational approaches have not been studied in the context of Bayesian GGMs.

ContributionWe present a unified approach to sparse regression for the whole family of all \(l_{q}\) (pseudo-) norms with \(0<q<\infty\), which allows for a fully Bayesian and frequentist-type interpretation. Specifically, we propose a very general framework for variational inference in Bayesian GGMs through a Normalizing Flow defined over the space of symmetric positive definite matrices. By conditioning the flow on \(\lambda\) and on \(q\), we simultaneously train a continuum of sparse regression models for all choices of shrinkage parameters \(0<\lambda<\lambda_{\text{max}}\) and all \(l_{q}\) (pseudo-) norms with \(0<q<q_{\text{max}}\), including the highly non-convex sub \(l_{1}\) pseudo-norms. We use as prior the generalized Normal distribution, which for \(q=1\) recovers the Laplace prior and the Lasso regularization in the MAP limit. On the one hand, our approach inherits the advantages of the Bayesian framework while altogether avoiding problems arising from Gibbs sampling strategies. On the other hand, we can still recover the frequentist solution path in the MAP limit by training through simulated annealing.

In summary, our main contributions are the following:

1. We propose a general framework for variational inference in GGMs through a normalizing flow defined directly over the space of symmetric positive definite matrices. We condition such a flow on the shrinkage parameter \(\lambda\) and on \(q>0\) to model \(l_{q}\) (pseudo-) norms.
2. We combine the advantages of the frequentist and Bayesian frameworks: in a single model we have access to posterior inference and to the marginal likelihood as a function of \(\lambda\) and \(q\). We can further recover the frequentist solution path as the MAP by annealing the system.
3. To the best of our knowledge, we are the first to propose a unified framework for sub-\(l_{1}\) pseudo-norms that does not require surrogate penalties and that enables consistent exploration of the full solution paths in terms of \(\lambda\) and \(q\).

## 2 Related Work

In this section, we provide an overview of the Bayesian Graphical Lasso and illustrate the limitations of current inference methods. We then outline the limitations of Lasso regularization and briefly review existing approaches for sparsity with sub-\(l_{1}\) pseudo-norms. Lastly, as an alternative to MCMC approaches, we briefly review variational inference with normalizing flows.

Bayesian Graphical LassoSimilarly to the Bayesian Lasso (Park and Casella, 2008), Wang (2012) provided a Bayesian interpretation of the Graphical Lasso (BGL) for posterior inference. Specifically, they showed that the \(l_{1}\)-relaxed optimization of Eq. (1) is equivalent to the maximum a posteriori estimate (MAP) of the model defined by \(p(\mathbf{\Omega}|\mathbf{S},\lambda)\propto p(\mathbf{S}|\mathbf{\Omega})\ p(\mathbf{\Omega}|\lambda)\), with

\[p(\mathbf{S}|\mathbf{\Omega}) \propto\mathcal{W}_{d}(n,\mathbf{\Omega}^{-1}) \tag{3}\] \[p(\mathbf{\Omega}|\lambda) \propto\mathcal{W}_{d}(d+1,\lambda\mathbf{1}_{d})\prod_{i<j}\operatorname {DE}(\omega_{ij}|\lambda)I[\mathbf{\Omega}\succ 0]\;,\]

where the indicator function \(I[\mathbf{\Omega}\succ 0]\) imposes positive definiteness on \(\mathbf{\Omega}\). The likelihood term reflects that the sample covariance matrix \(\mathbf{S}=\mathbf{X}^{T}\mathbf{X}\) is distributed according to the Wishart distribution \(\mathcal{W}_{d}(n,\mathbf{\Omega}^{-1})\propto\det(\mathbf{\Omega})^{n/2}\det(\mathbf{S}) ^{(n-d-1)/2}\exp\operatorname{Tr}(-\frac{1}{2}\mathbf{\Omega}\mathbf{S})\). The prior is instead composed of two terms. The Wishart term \(\mathcal{W}_{d}(d+1,\lambda\mathbf{1}_{d})\) imposes symmetry and positive definiteness on \(\mathbf{\Omega}\). In addition, the double exponential (or Laplace) prior \(\operatorname{DE}(\omega_{ij}|\lambda)=\frac{\lambda}{2}\exp\{-\lambda|\omega_ {ij}|\}\) encourages sparsity on the off-diagonal elements of \(\mathbf{\Omega}\).

Inference in the BGL, and more generally for Bayesian Lassos, is performed through Gibbs samplers. However, these MCMC strategies are computationally expensive and can suffer from high rejection rates, especially in high dimensions (Mohammadi and Wit, 2015). Furthermore, to recover the frequentist solution path in the MAP limit, the Markov Chain must be restarted for each \(\lambda\) value. Its derivation also requires to expand the Laplace prior as an infinite mixture of Gaussians (Andrews and Mallows, 1974; West, 1987) and to define an ad hoc mixing density. Lastly, the obtained Gibbs sampler does not generalize to other priors.

Sparsity with sub-\(l_{1}\) pseudo-normsThe \(l_{0}\) norm is particularly suited to enforce sparsity because it directly penalizes the non-zero entries. In other words, it is equivalent to best subset selection of the variables. Convex relaxation with the \(l_{1}\) norm makes the problem tractable, but comes at the cost of encouraging shrinkage also on the retained variables (Hastie et al., 2015). This motivated interest in sub-\(l_{1}\) pseudo-norms, which in the limit reduce to the original \(l_{0}\) norm formulation. Due to their non-convexity and combinatorial complexity, multiple surrogate objectives have been proposed. Notably, with the proposed approach we do not have to resort to alternative objectives or penalties, as we can directly and exactly enforce sub-\(l_{1}\) pseudo-norm regularization with a suitable prior. In the context of linear regression, several algorithms have been proposed. The popular SCAD (Fan and Li,2001] guarantees unbiasedness, sparsity, and continuity while reducing the overshrinking behaviour. Zhang [2010] proposed the nearly unbiased MC+ method, which consists of a concave penalty and a selection algorithm that bridges the gap between \(l_{1}\) and \(l_{0}\). In the literature, other penalties that approximate the \(l_{0}\) penalty have been proposed, such as the SICA penalty [Lv and Fan, 2009], the log-penalty [Mazumder et al., 2011], the seamless-\(l_{0}\) penalty [Dicker et al., 2013] and the atan penalty [Wang and Zhu, 2016]. In the Bayesian framework, Ishwaran and Rao [2005] used a rescaled spike and slab prior to encourage variable selection and drew connections to Ridge regularization.

Normalizing Flows for variational inferenceNormalizing Flows (NFs) are flexible models that can perform accurate density estimation For this reason, NFs represent a very attractive solution to perform efficient and accurate variational inference [Rezende and Mohamed, 2015, van den Berg et al., 2019]. Suppose \(\mathcal{X}\) is a continuous \(d\)-dimensional random variable with unknown distribution \(p_{\mathbf{x}}\), and let \(\mathcal{Z}\) be any continuous \(d\)-dimensional random variable with some known base distribution \(\mathbf{z}\sim p_{\mathcal{Z}}\). The key idea of NFs is to construct a diffeomorphism \(\mathcal{T}:\operatorname{supp}(\mathcal{X})\mapsto\operatorname{supp}( \mathcal{Z})\), i.e., a differentiable bijection, in order to rewrite \(p_{\mathcal{X}}\) through the change of variable formula as

\[p_{\mathcal{X}}(\mathbf{x})=p_{\mathcal{Z}}\big{(}\mathcal{T}(\mathbf{x})\big{)}\left| \det\mathcal{J}_{\mathcal{T}}(\mathbf{x})\right|\;, \tag{4}\]

where \(\mathcal{J}_{\mathcal{T}}\) is the Jacobian of the transformation \(\mathcal{T}\). Assuming that we have successfully learned the transformation \(\mathcal{T}\), we can then evaluate the target density through Eq. (4). We can further sample from the target distribution by simply transforming samples of the base distribution through \(\mathcal{T}\), namely \(\mathbf{x}=\mathcal{T}(\mathbf{x})\) with \(\mathbf{z}\) being realizations of \(p_{\mathcal{Z}}\). In practice, the crucial part of designing NFs is to construct arbitrarily complicated bijections. To do so, NFs exploit that compositions \(\mathcal{T}=\mathcal{T}_{1}\circ\cdots\circ\mathcal{T}_{m}\) of bijections \(\{\mathcal{T}_{i}\}_{i=1}^{m}\) remain bijections. The determinant of the resulting transformation decomposes into the determinants of each \(\mathcal{T}_{i}\) as

\[\det\mathcal{J}_{\mathcal{T}}(\mathbf{x})=\prod_{i=1}^{m}\det\mathcal{J}_{\mathcal{ T}_{i}}(\mathbf{u}_{i-1})\quad\mathrm{with}\quad\mathbf{u}_{i-1}=\mathcal{T}_{1}(\mathbf{x}) \circ\cdots\circ\mathcal{T}_{i-1}(\mathbf{u}_{i-2})\;. \tag{5}\]

Therefore, by composing computationally tractable non-linear bijections, it is possible to define arbitrarily expressive bijections and hence to transform the base distribution \(p_{\mathcal{Z}}\) into arbitrarily complicated distributions \(p_{\mathcal{X}}\). Among others, Huang et al. [2018] proved that autoregressive flows are universal density approximators of continuous random variables. For a comprehensive review of NFs and their different architectures, see Papamakarios et al. [2021] and Kobyzev et al. [2020]. As first proposed by Atanov et al. [2020], it is also possible to condition the transformation \(\mathcal{T}\) on some parameter \(c\in\mathbb{R}^{n}\). The resulting Conditional NF [Kobyzev et al., 2020] models with one flow the family of conditional distributions \(p_{\mathcal{X}}(\mathbf{x}|c)\) for continuous values of \(c\).

## 3 Proposed approach

In this section, we describe how to infer the posterior of \(\mathbf{\Omega}\) in GGMs with conditional NFs and how to do so as a function of the shrinkage parameter \(\lambda\) and of the \(l_{q}\) (pseudo-) norm regularization, all within a single model. We illustrate how to define the conditional flow directly over the space of positive definite matrices and argue why training and posterior inference is particularly efficient. Furthermore, we show that NFs provide direct access to the marginal log-likelihood for model selection and show how to recover the frequentist solution path by training through simulated annealing. We term the resulting model Conditional Matrix Flow (CMF).

### Generalized Normal distribution for \(\mathbf{l_{q}}\) (pseudo-) norms

Unlike previous Bayesian approaches to GGMs, we can flexibly specify any prior (and likelihood) in Eq. (3) without worrying about the existence of a suitable Gibbs sampler. We exploit such flexibility to extend the model beyond the standard \(l_{1}\) relaxation. Wang [2012] showed that in the MAP limit \(l_{1}\) regularization is recovered with a Laplace prior on the off-diagonals of \(\mathbf{\Omega}\). We generalize this idea to any \(l_{q}\) (pseudo-) norm with the generalized Normal distribution as prior, which reduces to Laplace and Normal distributions for \(q=1\) and \(q=2\), respectively. Its probability density is defined as:

\[f(x|\alpha,\beta)=\frac{\beta}{2\alpha\Gamma(1/\beta)}\exp\bigg{\{}-\frac{|x|^ {\beta}}{\alpha^{\beta}}\bigg{\}}\;, \tag{6}\]where \(\Gamma(\cdot)\) is the Gamma function, \(\alpha,\beta>0\) are the scale and shape parameters, respectively. Here, we assumed the distribution to be centered around zero, i.e. \(\mu=0\). In order to link the prior to the \(l_{q}\) norm, we rename its parameters as \(\lambda=\alpha^{-\beta}>0\) and \(q=\beta\). Our full model can now be conditioned through the prior on both \(\lambda\) and \(q\) and is defined as \(p(\mathbf{\Omega}|\mathbf{S},\lambda,q)\propto p(\mathbf{S}|\mathbf{\Omega})\:p(\mathbf{\Omega}| \lambda,q)\) with

\[p(\mathbf{S}|\mathbf{\Omega}) \propto\mathcal{W}_{d}(n,\mathbf{\Omega}^{-1}) \tag{7}\] \[p(\mathbf{\Omega}|\lambda,q) \propto\mathcal{W}_{d}(d+1,\lambda\mathbf{1}_{d})\prod_{i<j}\frac{q \lambda^{1/q}}{2\Gamma(1/q)}\exp\{-\lambda|\omega_{ij}|^{q}\}I[\mathbf{\Omega} \succ 0]\:.\]

If we now consider the log-probability and drop the normalization constant, which does not affect the KL optimization, we recover the \(l_{q}\) (pseudo-) norm. For \(q=1\) the prior on the off-diagonals reduces to the Laplace distribution and we recover the Bayesian Graphical Lasso in Eq. (3). For \(q=2\) we recover instead the normal distribution, i.e., Ridge regularization. Overall, when employing the distribution in Eq. (7), we can effectively model solutions corresponding to \(l_{q}\) (pseudo-) norm regularization for \(q>0\), including the non-convex sub-\(l_{1}\) pseudo-norms.

### Variational inference with conditional flows for GGMs

As we want to model the posterior \(p(\mathbf{\Omega}|\mathbf{S})\) over the precision matrix \(\mathbf{\Omega}\), we design flows directly over the space of symmetric positive definite matrices. Specifically, we train a NF conditioned on \(\lambda\in[\lambda_{1},\lambda_{2}]\) and on \(q\in[q_{1},q_{2}]\) with \(\lambda_{1},\lambda_{2},q_{1},q_{2}>0\) in order to study the evolution of the posterior as a function of \(\lambda\) and \(q\). From Eq. (4) we see that the resulting flow \(\mathcal{T}_{\mathbf{\theta}(\lambda,q)}\) implicitly defines the probability \(q_{\mathbf{\theta}(\lambda,q)}(\mathbf{\Omega})=p_{base}(\mathcal{T}_{\mathbf{\theta}( \lambda,q)}(\mathbf{\Omega}))|\det\mathcal{J}_{\mathbf{\mathcal{T}_{\mathbf{\theta}}( \lambda,q)}}(\mathbf{\Omega})|\), which we want to train to approximate the posterior \(p(\mathbf{\Omega}|\mathbf{S},\lambda,q)\propto p(\mathbf{S}|\mathbf{\Omega})\:p(\mathbf{\Omega}| \lambda,q)\). We minimize the KL divergence in Eq. (2) and approximate the expectation with Monte Carlo samples:

\[\mathcal{L}(\mathbf{\theta};\lambda,q) =\mathrm{KL}\big{(}q_{\mathbf{\theta}(\lambda,q)}(\mathbf{\Omega})||p( \mathbf{\Omega}|\mathbf{S},\lambda,q)\big{)} \tag{8}\] \[\approx\frac{1}{M}\sum_{i=1}^{M}\log\frac{q_{\mathbf{\theta}(\lambda,q)}(\mathbf{\Omega}_{i})}{p(\mathbf{S}|\mathbf{\Omega}_{i})\:p(\mathbf{\Omega}_{i}|\lambda,q )}\:+N_{C}(\lambda,q).\]

Note that this is particularly efficient in NFs because it only requires sampling from the base distribution \(p_{base}(\mathcal{T}_{\mathbf{\theta}(\lambda,q)}(\mathbf{\Omega}))\), which is computationally cheap. We further need to evaluate the unnormalized posterior \(p(\mathbf{\Omega}|\mathbf{S},\lambda,q)\propto p(\mathbf{S}|\mathbf{\Omega})\:p(\mathbf{\Omega}| \lambda,q)\), i.e. the product of the likelihood and the prior, which is also trivial. Relevantly, in Eq. (8) we do not need to compute the normalization constant \(N_{C}(\lambda,q)=\log p(\mathbf{S}|\lambda,q)\) because it does not influence the optimization.

Unlike standard Bayesian approaches, the proposed conditional flow provides access directly to the marginal log-likelihood \(N_{C}(\lambda,q)=\log p(\mathbf{S}|\lambda,q)\) as a function of \(\lambda\) and \(q\) without further calculations. This is particularly interesting because the marginal log-likelihood is extremely expensive to compute with classical approaches. If we assume that the flow is expressive enough and that the optimization reached the global minimum, then \(\mathrm{KL}\big{(}q_{\mathbf{\theta}(\lambda,q)}(\mathbf{\Omega})||p(\mathbf{\Omega}|\mathbf{S },\lambda,q)\big{)}\approx 0\). In this case, the loss function at the end of training gives us access directly to the negative marginal log-likelihood for continuous values of \(\lambda\) (and \(q\)). We can then perform model selection by simply choosing \(\lambda^{*}=\max_{\lambda}\log p(\mathbf{S}|\lambda,q)\).

### Conditional Matrix Flow

We now describe the proposed Conditional Matrix Flow (CMF) and how we define it over the space of symmetric positive definite matrices by construction. We exploit the well-known Cholesky decomposition (Horn and Johnson, 1985), which states that any symmetric positive definite matrix \(\mathbf{M}\in\mathbb{R}^{d\times d}\) can be decomposed as \(\mathbf{M}=\mathbf{L}\mathbf{L}^{T}\), where \(\mathbf{L}\in\mathbb{R}^{d\times d}\) is a lower triangular matrix. The decomposition is unique if \(L\) has positive diagonal elements. As a result, the symmetric positive definite \(\mathbf{M}\) can be fully and uniquely identified by \(d(d+1)/2\) numbers, \(d\) of which are constrained to be positive. We can then use a \(d(d+1)/2\)-dimensional normalizing flow to model probability densities over symmetric-positive matrices. We now illustrate how to transform the initial vector \(\mathbf{z}\in\mathbb{R}^{d(d+1)/2}\) into a symmetric positive definite matrix \(\mathbf{\Omega}\in\mathbb{R}^{d\times d}\). To the best of our knowledge, this is the first attempt to define a flow \(\mathcal{T}(\mathbf{\Omega})\) on the space of symmetric positive definite matrices by construction.

A high-level visualization of the proposed architecture is shown in Figure 1. The first \(n\) layers of the network consist of arbitrary NF transformations and map the initial vector \(\mathbf{z}\) to \(\mathbf{z}^{(n)}_{\lambda,q}\). We propose to use a class of flexible transformations that we called Sum-of-Sigmoids, which worked particularly well for our applications. We provide further insight on the implementation in Appendix A.1. Furthermore, we condition the flow on \(\lambda\) and \(q\) via a hypernetwork. The hypernetwork takes \(\lambda\) and \(q\) as inputs and returns the parameters of the Matrix Flow. We transform \(\mathbf{z}^{(n)}_{\lambda,q}\) into a symmetric positive definite matrix in three steps. First, the vector is raveled into a lower triangular matrix \(\mathbf{L}\in\mathbb{R}^{d\times d}\), which we call _Fill-Triangular_. This transformation has a unit Jacobian determinant because it just reshapes the vector into a matrix. In a second step, we bijectively map the diagonal of the resulting lower triangular matrix to positive values with a softplus activation \(\mathrm{Softplus}(\mathbf{x})=\log(1+\exp\mathbf{x})\). This transformation, which we call _Positive-Diagonal_, acts element-wise and hence admits a cheap Jacobian (log) determinant. Lastly, we compute the _Cholesky product_\(\mathrm{Chol}(\mathbf{L}):\mathbf{L}\mapsto\mathbf{L}\mathbf{L}^{T}\), which again has an inexpensive Jacobian (log) determinant (Gupta and Nagar, 1999):

\[\det\mathcal{J}_{\text{Chol}}(\mathbf{L})=2^{d}\prod_{i=1}^{d}(\mathbf{L}_{ii})^{d-i+ 1}\, \tag{9}\]

which depends only on the \(d\) diagonal elements of \(\mathbf{L}\). Note that by enforcing the triangular matrix to be positive, we make sure that the Cholesky decomposition is unique, and hence a bijection.

We open-source the implementation of the conditional bijective layers.1 We based our library, which was first used in Torres et al. (2023), on the high-level structure provided by the _nflows_ library (Durkan et al., 2020).

Footnote 1: **FlowConductor**: (Conditional) Normalizing Flows and bijective Layers for Pytorch [https://github.com/FabricioArendTorres/FlowConductor](https://github.com/FabricioArendTorres/FlowConductor)

### Training through simulated annealing

With our CMF, we can perform posterior inference as a function of \(\lambda\) and \(q\) and to perform model selection. These are key advantages of the Bayesian perspective. In addition, we can also recover the frequentist solution path as a function of \(\lambda\) without modifying the model. Specifically, we employ optimization through Simulated Annealing (Kirkpatrick et al., 1983) to approximately sample from the global maxima of the distribution, which for posteriors is the maximum a posteriori estimate (MAP). The idea comes from statistical mechanics, where slowly cooling processes are used to study the ground (optimal) state of the system. When applied to more general optimization tasks, simulated annealing consists in accepting iterative improvements if they lead to a lower cost function. Meanwhile, the temperature is slowly decreased from the initial value \(T_{0}\) to \(T_{n}\approx 0\), where the system is frozen and no further changes occur. In our setting, this corresponds to introducing an artificial temperature \(T_{i}\) for the target posterior in Eq. (7):

\[p^{(i)}(\mathbf{\Omega})=p(\mathbf{\Omega}|\mathbf{S},\lambda,q)^{1/T_{i}}\, \tag{10}\]

where \(T_{i}\) is the temperature at the \(i\)-th iteration. If the initial temperature is high enough, \(p^{(i)}(\mathbf{\Omega})\) will likely be very flat, allowing for a better exploration of the support of the distribution. As the temperature decreases, the distribution becomes more peaked. In the limit \(T_{n}\to 0\) the distribution

Figure 1: Architecture of the proposed Conditional Matrix Flow (CMF) model. Light grey is used to denote unconstrained values, dark grey for positive values, and white for zeros.

\(p^{(i)}(\mathbf{\Omega})\) should concentrate on the global maximum, hence on the MAP solution. Geman and Geman (1984) formally showed that convergence to the set of global minima is guaranteed for logarithmic cooling schedules. Unfortunately, logarithmic cooling schedules are not viable in practice, so alternative schemes have been explored (Abramson et al., 1999). In this paper, we use the popular geometric cooling schedule (Andrieu and Doucet, 2000; Yuan et al., 2004), which works well in practice: \(T_{i}=T_{0}a^{i/n}\) for some \(a>0\) and \(i\in\{0,\ldots,n\}\). In practice, we train the objective in Eq. (8) while slowly cooling down the system, i.e. decreasing \(T_{i}\) in Eq. (10), until we reach a sufficiently low temperature \(T_{n}\). By saving the model at \(T_{i}=1\) and \(T_{n}\) we have access to both the Bayesian and frequentist solutions, respectively.

### Limitations

The proposed approach provides a general framework for posterior inference in GGMs and generalizes to a large family of likelihood and priors, which requires an efficient evaluation of their analytic expression. In some cases, e.g. for posterior predictive distributions, this would require an additional integration step, which could become computationally expensive. The proposed approach is also limited by the expressive power of the bijective layers of the flow. Even though state-of-the-art layers are extremely powerful in modeling high-dimensional distributions, each layer might still be limited in terms of the number of modes that can be modeled (Liao and He, 2021). Lastly, the proposed model assumes that we can model a family of posterior distributions as a function of the conditioning parameters \(q\) and \(\lambda\), which ultimately depends on the flexibility of the hypernetwork used.

## 4 Experiments

In this section, we showcase the effectiveness of the proposed CMF first on artificial data and then on a real application. In particular, we study the evolution of the variational posterior as a function of \(\lambda\) and \(q\). We then perform model selection on \(\lambda\) through marginal likelihood maximization. We further illustrate the effect of training through simulated annealing and show that we recover the frequentist solution path through the MAP. Lastly, we show that the proposed method can be readily applied to real data in higher-dimensional settings. Results show that sub-\(l1\) pseudo-norms provide sparser solutions and contrast the well-known overshrinking effect of Lasso relaxation. We provide the code to reproduce the experiments at [https://github.com/marcello-negri/CMF/](https://github.com/marcello-negri/CMF/).

### Toy example: synthetic data

We illustrate how the proposed CMF works on artificially generated sparse precision matrices. The data generation process consists of sampling a sparse precision matrix (Pedregosa et al., 2011) with \(d\) features and sparsity level \(\alpha\), and then generating \(n\) Gaussian samples accordingly. For illustrative purposes, we show results for one precision matrix with \(\alpha=90\%\) and \(n=d=15\) (\(n=d\) ensures the invertibility of the empirical covariance). We trained our model for \(10^{\prime}000\) epochs through simulated annealing with an initial temperature of \(T_{0}=5\) to \(T_{n}=0.01\) and performed \(100\) geometric cooling steps with \(T_{i}=T_{0}a^{i/n}\) for \(a=T_{n}/T_{0}\).

Figure 2 shows the effect of simulated annealing on the posterior. At temperature \(T=1\) the conditional flow approximates the true posterior in Eq. (7), which for \(q=1\) coincides with the Bayesian Graphical Lasso (BGL) model. As we decrease the temperature, the (unnormalized) target posterior in Eq. (10) becomes more peaked and we observe shrinking credible intervals. At the final temperature \(T_{n}\), the distribution converges to the MAP and we recover the frequentist path. As shown in Figure 2 (_right_), our model accurately reconstructs the solution path over \(\lambda\) (MSE \(=0.052\)). Furthermore, we perform model selection on \(\lambda\) for the model in Eq. (7), which is obtained at temperature \(T=1\). In the Appendix A.4 in Figure 7, we show the (approximate) marginal log-likelihood as a function of \(\lambda\) and the resulting optimal \(\lambda^{*}_{\text{CMF}}=3.52\) that maximizes it. We compare the result with the frequentist estimate obtained through MLE with 5-fold cross validation \(\lambda^{*}_{\text{GLasso}}=3.36\). Again, the model agrees with the frequentist solution.

One of the most significant drawbacks of relaxing the \(l_{0}\)-norm regularization with \(l_{1}\) is that sparsity is achieved through shrinkage. This means that the selected relevant features are over-shrunk. With the proposed CMF we show that by exploring sub-\(l_{1}\) pseudo-norms we can reduce the shrinkage effect and virtually overcome it in the \(q\to 0\) limit. In Figure 3 (_left_) we show \(95\%\) posterior credibility intervals for one entry of the reconstructed precision matrix. We can observe that, as the \(l_{q}\) pseudo-norm gets closer to \(l_{1}\), the posterior median is progressively shrunk towards zero. With the proposed CMF we can prevent this shrinkage effect by exploring the posterior solution path for sub-\(l_{1}\) pseudo-norms. In particular, note that for \(\lambda=0.3\) the posterior \(95\%\) credibility interval does not include the value 0 for \(q=\{0.25,0.5,0.75\}\), as opposed to the BGL (or \(q=1\)). In Figure 3 (_right_) we show the posterior solution path of the CMF for \(q=0.5\) in the MAP limit (\(T=0.01\)). Compared to the \(l_{1}\) solution path, sub-\(l_{1}\) solution paths show less shrinkage effect (higher values for the selected features) and steepest decrease towards zero. This behaviour becomes more evident as \(q\) decreases, as we show more extensively in the Appendix A.4 in Figure 6 for \(q=\{1,0.75,0.5,0.25\}\). These results support the interest in sub-\(l_{1}\) pseudo-norms, and in \(l_{0}\) norm in the limit.

### Edge recovery with sub-\(l_{1}\) pseudo-norms

We now study the behaviour of the proposed CMF in terms of F1 score for edge recovery as a function of the number of samples. We show that the proposed CMF outperforms competing methods, especially in the low sample regime. We compare the CMF with the BGL, which is the only alternative Bayesian model. Then, we compare the CMF in the \(q\to 0\) limit against frequentist approaches that use approximate \(l_{0}\) norm penalties: Atan ("atan") (Wang and Zhu, 2016), Seamless \(l_{0}\) ("selo") (Dicker et al., 2013), log-penalty ("log") (Mazumder et al., 2011), SICA ("sica") (Lv and Fan, 2009).

We use 10 ground truth precision matrices of dimension \(d=30\) and generate \(n\) Gaussian samples accordingly. We show results for the relevant \(n<d\) regime and around \(n=d\), namely for samples

Figure 3: _Left: \(95\%\) posterior credibility intervals of the proposed CMF for one entry of the precision matrix as a function of the pseudo-norm \(q=\{1,0.75,0.5,0.25\}\). Results are compared with the BGL and the frequentist solution (GLasso). Right: MAP estimate of the CMF posterior as a function of \(\lambda\) for \(q=0.5\) (\(T=0.01\)). The dashed line is the frequentist solution path for \(q=1\)._

Figure 2: \(75\%\) posterior credible intervals as a function of \(\lambda\) for \(q=1\). Left: at \(T=1.0\) the CMF reduces to the BGL. Right: at \(T=0.01\) the CMF reduces to the frequentist solution path (dashed).

\(n=\{15,25,35,45\}\). For Bayesian approaches (the proposed CMF and the BGL) we draw 1000 samples from the (approximate) posteriors and consider \(90\%\) credibility intervals. All results are averaged over the 10 precision matrices. The proposed CMF is trained for 5000 epochs to a final temperature \(T=1\), which corresponds to the Bayesian model in Eq. (7). For the BGL we run the Gibbs sampler for 4000 iterations with a burn-in of 1000 and keep every fourth sample. The frequentist approaches were run with the specific hyper-parameters suggested in their original papers.

As the proposed method is inherently Bayesian we first compare it against the BGL. We show that for \(q=1\) we obtain results compatible with the BGL. Furthermore, with the proposed method we can additionally infer the posterior for \(q<1\). The results in Figure 4 (_left_) show that in the low sample regime (\(n<d\)) sub-\(l_{1}\) pseudo-norms are beneficial and result in a higher F1 score. The effect is stronger as \(q\to 0\), while in the \(n>d\) regime sub-\(l_{1}\) pseudo-norms do not provide a significant advantage. We also compare the proposed CMF with classical frequentist approaches with surrogate penalties that approximate the \(l_{0}\) norm. The results in Figure 4 (_right_) show that the proposed CMF with \(q=0.25\) outperforms all competing methods across all sample regimes, but especially for very low sample sizes (\(n=15\)). Note that in the \(q<1\) regime frequentist algorithms require an ad hoc initialization of the precision matrix, which we provided through the Ledoit-Wolf shrinkage estimator.

### Real data application

We now consider a real-world application to showcase that the proposed CMF can be easily used in high-dimensional settings. We consider a setting in which we are interested in studying only a subset of query variables. In particular, we show that we can avoid inferring the posterior on the full precision matrix, which is expensive, by simply redefining the target posterior, i.e. the prior and likelihood. We consider a colorectal cancer dataset (Sheffer et al., 2009), which contains measurements of \(7\) clinical variables together with \(312\) gene measurements from biopsies for \(260\) cancer patients. As the dataset contains many missing values, we drop the _p53 mutation status_ clinical variable and only consider \(n=190\) fully measured patients. We study the connections between the \(s=6\) clinical variables and the \(t=312\) gene expression measurements. Like Kaufmann et al. (2015), we consider the partition

\[\mathbf{\Omega}=\begin{pmatrix}s&t&s&t\\ \mathbf{\Omega}_{11}&\mathbf{\Omega}_{12}\\ \mathbf{\Omega}_{12}^{T}&\mathbf{\Omega}_{22}\end{pmatrix}\begin{matrix}s\\ t\\ \mathbf{S}=\begin{pmatrix}\mathbf{S}_{11}&\mathbf{S}_{12}\\ \mathbf{S}_{12}^{T}&\mathbf{S}_{22}\end{pmatrix}\begin{matrix}s\\ t\\ \end{matrix}\end{matrix} \tag{11}\]

where \(s\ll t\). Instead of inferring the expensive \((s+t)\times(s+t)\) precision matrix \(\mathbf{\Omega}\), we study the \(\mathbf{\Omega}_{11}\) and \(\mathbf{\Omega}_{12}\) sub-matrices, which overall require only \(s\times(s+t)\) dimensions. Following Kaufmann et al. (2015), we show in Appendix A.2 that we can infer the posterior \(p(\mathbf{\Omega}_{11},\mathbf{\Omega}_{12}|\mathbf{S},\lambda)\) independently of the large \(\mathbf{\Omega}_{22.1}\). But in contrast to Kaufmann et al. (2015), we (i) can enforce the correct double-exponential prior on \(\mathbf{\Omega}_{11}\), we (ii) do not need to invert the \(st\times st\) matrix, which takes \(O(st^{3})\) operations, and we (iii) do not require a Gibbs sampler, which can still suffer from poor mixing behaviour. In our framework, we only need to define a different posterior as the target

Figure 4: F1 score for edge recovery. _Left:_ the proposed CMF for \(q=\{0.25,0.5,0.75,1\}\) against the Bayesian Graphical Lasso (BGL). _Right:_ the proposed CMF for \(q=0.25\) against various frequentist approaches with penalties that approximate the \(l_{0}\) norm. Results are averaged over 10 precision matrices with \(d=30\).

distribution. In practice, we define the flow jointly over the sub-matrices \(\mathbf{\Omega}_{11}\), which must be positive definite, and \(\mathbf{\Omega}_{12}\), which is unconstrained. It is then sufficient to define the bijective layers on the \(s(s+1)/2\) plus \(s\times t\) dimensions and to perform the Cholesky product only on the dimensions encoding \(\mathbf{\Omega}_{11}\). Note that the determinant of the Jacobian of the full transformation is still the same as in Eq. (9) because the \(s\times t\) dimensions encoding \(\mathbf{\Omega}_{12}\) are just raveled into a matrix.

In Figure 5 we show the inferred network structure with the proposed CMF for \(q=1.0\) (_left_) and for \(q=0.6\) (_right_). Significant edges are obtained by considering \(80\%\) credible intervals on the posterior of \(\mathbf{\Omega}_{11}\) and \(\mathbf{\Omega}_{12}\). The clinical variables in the data are age, sex, cancer group stage (GS), and TNM classification for colorectal cancer, which measures its size (T), whether it spreads to lymph nodes (N) and if metastases develop (M). Note that, in contrast to (Kaufmann et al., 2015), we can infer the posterior on the \(\mathbf{\Omega}_{11}\) block as well, which allows to study the conditional independence structure among clinical variables. For instance, results suggest strong connections between the cancer group stage (GS) and each variable of the TNM classification (T, N, M). More interestingly, with the proposed CMF we can infer the posterior for different sub-\(l_{1}\) pseudo-norms. As expected, Figure 5 shows that with lower \(q\) values we obtain sparser solutions. Additional quantitative comparisons for \(q=\{1.0,0.9,0.8,0.7,0.6\}\) are included in the Appendix A.5 in Figure 8.

## 5 Conclusions

We propose a very general framework for variational inference in Gaussian Graphical Models through conditional normalizing flows. Our model unifies the benefits of Bayesian and frequentist approaches while avoiding most of their specific problems. Compared to existing approaches, the most important advantage of our method is that it can jointly train a continuum of sparse regression models for all regularization parameters and all \(l_{q}\) (pseudo-) norms. All of these models can be analyzed both in a Bayesian fashion (at temperature \(T=1\)) or, alternatively, in the frequentist limit (i.e. penalized likelihood) as \(T\) approaches \(0\). To the best of our knowledge, this is the first Gaussian Graphical Model that can continuously explore all sparsity-inducing priors from the \(l_{q}\)-norm family. Moreover, thanks to our variational formalism, we can integrate out the model parameters and use the (approximate) marginal likelihood for model selection without any additional computational costs.

Figure 5: Inferred network structure with the proposed CMF for \(q=1.0\) (_left_) and \(q=0.6\) (_right_). Positive edges are shown in black and negative ones in grey. Clinical variables are highlighted in red.

## References

* Smith et al. (2013) Stephen M Smith, Christian F Beckmann, Jesper Andersson, Edward J Auerbach, Janine Bijsterbosch, Gwenaelle Douaud, Eugene Duff, David A Feinberg, Ludovica Griffanti, Michael P Harms, et al. Resting-state fmri in the human connectome project. _Neuroimage_, 80:144-168, 2013.
* Castelo and Roverato (2006) Robert Castelo and Alberto Roverato. A robust procedure for gaussian graphical model search from microarray data with p larger than n. _Journal of Machine Learning Research_, 7(94):2621-2650, 2006.
* Diggle (2002) P. Diggle. _Analysis of Longitudinal Data_. OUP Oxford, 2002. ISBN 9780198524847.
* Hastie et al. (2015) Trevor Hastie, Robert Tibshirani, and Martin Wainwright. _Statistical Learning with Sparsity: The Lasso and Generalizations_. Chapman &; Hall/CRC, 2015. ISBN 1498712169.
* 1462, 2006. doi: 10.1214/00905360600000281.
* Friedman et al. (2007) Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation with the graphical lasso. _Biostatistics_, 9(3):432-441, 12 2007. ISSN 1465-4644. doi: 10.1093/biostatistics/kxm045.
* Yuan and Lin (2007) Ming Yuan and Yi Lin. Model selection and estimation in the gaussian graphical model. _Biometrika_, 94(1):19-35, 2007. ISSN 00063444, 14643510.
* Banerjee et al. (2007) Onureena Banerjee, Laurent El Ghaoui, and Alexandre d'Aspremont. Model selection through sparse maximum likelihood estimation, 2007.
* 2149, 2012. doi: 10.1214/12-EJS740.
* 886, 2012. doi: 10.1214/12-BA729.
* Li et al. (2019) Yunfan Li, Bruce A. Craig, and Anindya Bhadra. The graphical horseshoe estimator for inverse covariance matrices, 2019.
* Blei et al. (2017) David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians. _Journal of the American Statistical Association_, 112(518):859-877, apr 2017. doi: 10.1080/01621459.2017.1285773.
* Alves et al. (2021) Larissa Alves, Ronaldo Dias, and Helio S. Migon. Variational full bayes lasso: Knots selection in regression splines, 2021.
* Babacan et al. (2014) S. Derin Babacan, Shinichi Nakajima, and Minh N. Do. Bayesian group-sparse modeling and variational inference. _IEEE Transactions on Signal Processing_, 62(11):2906-2921, 2014. doi: 10.1109/TSP.2014.2319775.
* Park and Casella (2008) Trevor Park and George Casella. The bayesian lasso. _Journal of the American Statistical Association_, 103(482):681-686, 2008. doi: 10.1198/016214508000000337.
* 138, 2015. doi: 10.1214/14-BA889.
* Andrews and Mallows (1974) D. F. Andrews and C. L. Mallows. Scale mixtures of normal distributions. _Journal of the Royal Statistical Society. Series B (Methodological)_, 36(1):99-102, 1974. ISSN 00359246.
* West (1987) Mike West. On scale mixtures of normal distributions. _Biometrika_, 74(3):646-648, 09 1987. ISSN 0006-3444. doi: 10.1093/biomet/74.3.646.
* Fan and Li (2001) Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle properties. _Journal of the American Statistical Association_, 96:1348-1360, 02 2001.
* 942, 2010. doi: 10.1214/09-AOS729.
* Zhang and Wang (2012)Jinchi Lv and Yingying Fan. A unified approach to model selection and sparse recovery using regularized least squares. _The Annals of Statistics_, 37(6A), dec 2009. doi: 10.1214/09-aos683.
* Mazumder et al. (2011) Rahul Mazumder, Jerome H. Friedman, and Trevor Hastie. Sparsenet: Coordinate descent with nonconvex penalties. _Journal of the American Statistical Association_, 106(495):1125-1138, 2011.
* Dicker et al. (2013) Lee Dicker, Baosheng Huang, and Xihong Lin. Variable selection and estimation with the seamless-l 0 penalty. _Statistica Sinica_, 23, 04 2013. doi: 10.5705/ss.2011.074.
* Wang and Zhu (2016) Yanxin Wang and Li Zhu. Variable selection and parameter estimation with the atan regularization method. _Journal of Probability and Statistics_, 2016:1-12, 01 2016. doi: 10.1155/2016/6495417.
* Ishwaran and Rao (2005) Hemant Ishwaran and J. Sunil Rao. Spike and slab variable selection: Frequentist and bayesian strategies. _The Annals of Statistics_, 33(2), apr 2005. doi: 10.1214/00905360400001147.
* Rezende and Mohamed (2015) Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Francis Bach and David Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning_, volume 37, pages 1530-1538. PMLR, 07-09 Jul 2015.
* van den Berg et al. (2019) Rianne van den Berg, Leonard Hasenclever, Jakub M. Tomczak, and Max Welling. Sylvester normalizing flows for variational inference, 2019.
* Huang et al. (2018) Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive flows, 2018.
* Papamakarios et al. (2021) George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. _J. Mach. Learn. Res._, 22(1), jan 2021. ISSN 1532-4435.
* Kobyzev et al. (2020) Ivan Kobyzev, Simon Prince, and Marcus Brubaker. Normalizing flows: An introduction and review of current methods. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, PP:1-1, 05 2020. doi: 10.1109/TPAMI.2020.2992934.
* Atanov et al. (2020) Andrei Atanov, Alexandra Volokhova, Arsenii Ashukha, Ivan Sosnovik, and Dmitry Vetrov. Semi-conditional normalizing flows for semi-supervised learning, 2020.
* Horn and Johnson (1985) Roger A. Horn and Charles R. Johnson. _Matrix analysis_. Cambridge University Press, repr. with corr. edition, 1985.
* Gupta and Nagar (1999) Arjun K Gupta and Daya K Nagar. _Matrix variate distributions_, volume 104. CRC Press, 1999.
* Torres et al. (2023) Fabricio Arend Torres, Marcello Massimo Negri, Marco Inversi, Jonathan Aellen, and Volker Roth. Lagrangian flow networks for conservation laws, 2023.
* Durkan et al. (2020) Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. nflows: normalizing flows in pytorch, 2020.
* Kirkpatrick et al. (1983) S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. _Science_, 220 (4598):671-680, 1983. doi: 10.1126/science.220.4598.671.
* Geman and Geman (1984) Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, PAMI-6 (6):721-741, 1984. doi: 10.1109/TPAMI.1984.4767596.
* Abramson et al. (1999) David Abramson, Mohan Krishnamoorthy, and Henry Dang. Simulated annealing cooling schedules for the school timetabling problem. _Asia-Pacific Journal of Operational Research_, 1999.
* 1004, 06 2000. doi: 10.1109/18.841176.
* Yuan et al. (2004) Changhe Yuan, Tsai-Ching Lu, and Marek J. Druzdzel. Annealed map. In _Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence_, page 628-635. AUAI Press, 2004.
* Liao and He (2021) Huadong Liao and Jiawei He. Jacobian determinant of normalizing flows, 2021.
* Liu et al. (2018)F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* Sheffer et al. (2009) Michal Sheffer, Manny D. Bacolod, Or Zuk, Sarah F. Giardina, Hanna Pincas, Francis Barany, Philip B. Paty, William L. Gerald, Daniel A. Notterman, and Eytan Domany. Association of survival and disease progression with chromosomal instability: A genomic exploration of colorectal cancer. _Proceedings of the National Academy of Sciences_, 106(17):7131-7136, 2009. doi: 10.1073/pnas.0902232106.
* Kaufmann et al. (2015) Dinu Kaufmann, Sonali Parbhoo, Aleksander Wieczorek, Sebastian Keller, David Adametz, and Volker Roth. Bayesian markov blanket estimation, 2015.
* Kingma et al. (2016) Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.
* Papamakarios et al. (2018) George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation, 2018.
* Torres (2018) Fabricio Arend Torres. Sampling and annealing for dependency subnetwork estimation. Master's thesis, University of Basel, 2018.

Appendix

### Sum-of-Sigmoids layers

We implement the proposed model with a conditional flow architecture that relies on element-wise transformations through monotonic functions, which we term _Sum-of-Sigmoids_. We further combine these element-wise monotonic transformations in an autoregressive fashion using a MADE-like approach (Kingma et al., 2016; Huang et al., 2018). The proposed layers are light and very flexible and work particularly well for our purposes already with only 4 layers. Note that, even though it is not needed in our framework, the inverse can be computed numerically, which is relatively cheap since the transformation is element-wise monotonic. Specifically, we implement flexible monotonic transformations by combining shifted and scaled sigmoid activations. Differently from Huang et al. (2018), we also add shifted (and flipped) softplus functions to the element-wise activations. This leads to a linear behaviour outside a specific range \([-s,s],s>0\), alleviating the effect of inputs that lie outside the seen training data. The strictly monotonic element-wise transformation is given by

\[\phi_{sos}(z^{(i)})=\underbrace{\left[a\sum_{j=1}^{k}v_{j}\;\sigma(w_{j}z^{(i) }+b_{j})\right]}_{\text{sum of monotonic functions}}+\underbrace{\left[\ln\left(1+e^{(z^{(i)} -s)}\right)-\ln\left(1+e^{(-z^{(i)}-s)}\right)\right]}_{\text{approx. linear for }|z^{(i)}|\gg s,\text{and zero for }|z^{(i)}|\ll s}, \tag{12}\]

where \(\sigma\) is the sigmoid function and \(\{v_{j},w_{j},b_{j}\}_{j=1}^{k}\) and \(a\) are learnable parameters such that \(w_{k},v_{j},a>0\) and \(\sum_{j=1}^{k}v_{j}=1\). The advantage of using an element-wise transformation is that the associated Jacobian is diagonal and therefore its log-determinant is straightforward to compute. The proposed Sum-of-Sigmoids layers provide a very flexible transformation, but due to the element-wise nature, different dimensions are not mixed together. We overcome this by predicting the parameters \(\{v_{j},w_{j},b_{j}\}_{j=1}^{k}\) and \(a\) of the Sum-of-Sigmoids with masked autoregressive hypernetworks, similar to other masked autoregressive flows. When implemented through masking (Papamakarios et al., 2018), autoregressive flows allow for an elegant extension to conditional settings as well. Relevantly, this autoregressive structure admits a simple log-determinant Jacobian since the Jacobian is lower-triangular by construction.

### Joint posterior for \(\boldsymbol{\Omega_{11}}\) and \(\boldsymbol{\Omega_{12}}\)

We show that given the partition in Eq. (11) and the prior in Eq. (7), the joint posterior factorizes as

\[p(\boldsymbol{\Omega}_{11},\boldsymbol{\Omega}_{12},\boldsymbol{\Omega}_{22.1}| \boldsymbol{S},\lambda,q)=p(\boldsymbol{\Omega}_{11},\boldsymbol{\Omega}_{12}| \boldsymbol{S},\lambda,q)\;p(\boldsymbol{\Omega}_{22.1}|\boldsymbol{S}, \lambda,q)\;. \tag{13}\]

We further provide the analytic expression for \(p(\boldsymbol{\Omega}_{11},\boldsymbol{\Omega}_{12}|\boldsymbol{S},\lambda,q)\). The latter is used as target distribution in the training loss in Eq. (8) for the real data application in Section 4. For this proof we followed the approach of Torres (2018).

Let \(\boldsymbol{X}\in\mathbb{R}^{n\times(s+t)}\) be the design matrix containing independent observations. We are interested in estimating the connections between \(s\) query variables with respect to the \(t\) remaining ones, typically with \(s\ll t\). Given the partition of \(\boldsymbol{\Omega}\) and \(\boldsymbol{S}\) in Eq. (11), we define the full posterior \(p(\boldsymbol{\Omega},\boldsymbol{S}|\lambda,q)=p(\boldsymbol{\Omega}_{11}, \boldsymbol{\Omega}_{12},\boldsymbol{\Omega}_{22},\boldsymbol{S}|\lambda,q)\) as the product of the Wishart likelihood \(p(\boldsymbol{S}|\boldsymbol{\Omega})\) and a suitable prior \(p(\boldsymbol{\Omega}|\lambda,q)\):

\[\begin{split} p(\boldsymbol{S}|\boldsymbol{\Omega})& \propto\mathcal{W}_{s+t}(n,\boldsymbol{\Omega}^{-1})=\det( \boldsymbol{\Omega})^{n/2}\;\exp\{\mathrm{Tr}(-\tfrac{1}{2}\boldsymbol{ \Omega}\boldsymbol{S})\}\\ p(\boldsymbol{\Omega}|\lambda,q)&\propto\mathcal{W }_{s+t}(s+t+1,\lambda\boldsymbol{1}_{s+t})\,p(\boldsymbol{\Omega}_{11}| \lambda,q)\;p(\boldsymbol{\Omega}_{12}|\lambda,q)\;.\end{split} \tag{14}\]

Specifically, we select a Wishart prior on \(\boldsymbol{\Omega}\) to ensure positive definiteness on the full matrix. In order to encourage sparsity we choose a generalized Normal distribution prior over the off-diagonal elements of \(\boldsymbol{\Omega}_{11}\) and over the full \(\boldsymbol{\Omega}_{12}\):

\[p(\boldsymbol{\Omega}_{11}|\lambda,q)\propto\prod_{i<j}\exp\big{(}-\lambda|( \boldsymbol{\Omega}_{11})_{ij}|^{q}\big{)}\qquad p(\boldsymbol{\Omega}_{12}| \lambda,q)\propto\prod_{i,j}\exp\big{(}-\lambda|(\boldsymbol{\Omega}_{12})_{ ij}|^{q}\big{)}\;. \tag{15}\]

As a first step we explicitly write down the expression for the likelihood

\[\begin{split} p(\boldsymbol{S}|\boldsymbol{\Omega})& =p(\boldsymbol{S}|\boldsymbol{\Omega}_{11},\boldsymbol{\Omega}_{12}, \boldsymbol{\Omega}_{22.1})\\ &=\mathcal{W}_{s+t}(n,\boldsymbol{\Omega}^{-1})\\ &\propto\det(\boldsymbol{\Omega})^{n/2}\det(\boldsymbol{S})^{(n-( s+t)-1)/2}\exp\big{(}-\tfrac{1}{2}\mathrm{Tr}\big{[}\boldsymbol{\Omega} \boldsymbol{S}\big{]}\big{)}\end{split} \tag{16}\]and for the prior

\[\begin{split} p(\mathbf{\Omega}|\lambda,q)&=p(\mathbf{\Omega}_{1 1},\mathbf{\Omega}_{12},\mathbf{\Omega}_{22.1}|\lambda,q)\\ &\propto\mathcal{W}_{s+t}(s+t+1,\lambda\mathbf{1}_{s+t})\,p(\mathbf{\Omega }_{11}|\lambda)\,p(\mathbf{\Omega}_{12}|\lambda)\\ &=\exp\big{(}-\tfrac{\lambda}{2}\mathrm{Tr}\big{[}\mathbf{\Omega} \big{]}\big{)}\,\prod_{i<j}\exp\big{(}-\lambda|(\mathbf{\Omega}_{11})_{ij}|^{q} \big{)}\,\prod_{i,j}\exp\big{(}-\lambda|(\mathbf{\Omega}_{12})_{ij}|^{q}\big{)}\;. \end{split} \tag{17}\]

Overall, the posterior reads as

\[\begin{split} p(\mathbf{\Omega}_{11},\mathbf{\Omega}_{12},\mathbf{\Omega}_{2 2.1},\mathbf{S}|\lambda,q)&\propto\det(\mathbf{\Omega})^{n/2}\det(\mathbf{S} )^{(n-(s+t)-1)/2}\\ &\times\exp\Big{(}-\frac{1}{2}\mathrm{Tr}\big{[}\mathbf{\Omega}\mathbf{S} +\lambda\mathbf{\Omega}\big{]}\Big{)}\\ &\times\prod_{i<j}\exp\big{(}-\lambda|(\mathbf{\Omega}_{11})_{ij}|^{q }\big{)}\;\prod_{i,j}\exp\big{(}-\lambda|(\mathbf{\Omega}_{12})_{ij}|^{q}\big{)} \;.\end{split} \tag{18}\]

Following Kaufmann et al. (2015), we now re-write the posterior through the change of variables \((\mathbf{\Omega}_{11},\mathbf{\Omega}_{12},\mathbf{\Omega}_{22})\to(\mathbf{\Omega}_{11},\mathbf{ \Omega}_{12},\mathbf{\Omega}_{22.1})\) involving the Schur component \(\mathbf{\Omega}_{22.1}=\mathbf{\Omega}_{22}-\mathbf{\Omega}_{12}^{T}\mathbf{\Omega}_{11}^{-1} \mathbf{\Omega}_{12}\). Note that the transformation has unit Jacobian: \(J\big{(}(\mathbf{\Omega}_{11},\mathbf{\Omega}_{12},\mathbf{\Omega}_{22})\to(\mathbf{\Omega}_{1 1},\mathbf{\Omega}_{12},\mathbf{\Omega}_{22.1})\big{)}=\mathbf{1}\). We can now explicitly re-write the posterior as \(p(\mathbf{\Omega}_{11},\mathbf{\Omega}_{12},\mathbf{\Omega}_{22.1},\mathbf{S}|\lambda,q)\) by using the following substitutions, which are straightforward to show:

\[\begin{split}\det(\mathbf{\Omega})&=\det(\mathbf{\Omega}_{ 11})\det(\mathbf{\Omega}_{22}-\mathbf{\Omega}_{12}^{T}\mathbf{\Omega}_{11}^{-1}\mathbf{\Omega} _{12})=\det(\mathbf{\Omega}_{11})\det(\mathbf{\Omega}_{22.1})\;,\\ \mathrm{Tr}(\mathbf{\Omega}\mathbf{S})&=\mathrm{Tr}\big{[} \mathbf{\Omega}_{11}\mathbf{S}_{11}+\mathbf{\Omega}_{12}\mathbf{S}_{12}^{T}+\mathbf{\Omega}_{12}^{ T}\mathbf{S}_{12}+\mathbf{\Omega}_{22.1}\mathbf{S}_{22}+\mathbf{\Omega}_{12}^{T}\mathbf{\Omega}_{11}^{-1} \mathbf{\Omega}_{12}\mathbf{S}_{22}\big{]}\;,\\ \mathrm{Tr}(\lambda\mathbf{\Omega})&=\mathrm{Tr}\big{[} \lambda\mathbf{\Omega}_{11}+\lambda\mathbf{\Omega}_{22.1}+\lambda\mathbf{\Omega}_{12}^{T} \mathbf{\Omega}_{11}^{-1}\mathbf{\Omega}_{12}\big{]}\;.\end{split} \tag{19}\]

By re-arranging the terms the posterior simplifies to:

\[\begin{split} p(\mathbf{\Omega}_{11},\mathbf{\Omega}_{12},\mathbf{\Omega}_{2 2.1},&\mathbf{S}|\lambda,q)\propto\det(\mathbf{\Omega}_{11})^{n/2}\det( \mathbf{S})^{\frac{n-(s+t)-1}{2}}\\ &\times\det(\mathbf{\Omega}_{22.1})^{n/2}\exp\Big{(}-\frac{1}{2} \mathrm{Tr}\big{[}\mathbf{\Omega}_{22.1}(\mathbf{S}_{22}+\lambda\mathbf{1}_{t})\big{]} \Big{)}\\ &\times\exp\Big{(}-\frac{1}{2}\mathrm{Tr}\big{[}\mathbf{\Omega}_{11}( \mathbf{S}_{11}+\lambda\mathbf{1}_{s})+2(\mathbf{\Omega}_{12}^{T}\mathbf{S}_{12})+\mathbf{\Omega}_ {12}^{T}\mathbf{\Omega}_{11}^{-1}\mathbf{\Omega}_{12}(\mathbf{S}_{22}+\lambda\mathbf{1}_{t}) \big{]}\Big{)}\\ &\times\prod_{i<j}\exp\big{(}-\lambda|(\mathbf{\Omega}_{11})_{ij}|^{q }\big{)}\;\prod_{i,j}\exp\big{(}-\lambda|(\mathbf{\Omega}_{12})_{ij}|^{q}\big{)}\;. \end{split}\]

If we now condition on \(\mathbf{S}\), and specifically on \(\mathbf{S}_{22}\), we can clearly see that the posterior factorizes as \(p(\mathbf{\Omega}_{11},\mathbf{\Omega}_{12},\mathbf{\Omega}_{22.1}|\mathbf{S},\lambda,q)\propto p (\mathbf{\Omega}_{11},\mathbf{\Omega}_{12}|\mathbf{S},\lambda,q)\;p(\mathbf{\Omega}_{22.1}| \mathbf{S},\lambda,q)\). In particular, we are interested in estimating the joint posterior \(p(\mathbf{\Omega}_{11},\mathbf{\Omega}_{12}|\mathbf{S},\lambda,q)\), which reads as

\[\begin{split} p(\mathbf{\Omega}_{11},\mathbf{\Omega}_{12}|\mathbf{S},\lambda, q)&\propto\det(\mathbf{\Omega}_{11})^{n/2}\\ &\times\exp\Big{(}-\frac{1}{2}\mathrm{Tr}\big{[}\mathbf{\Omega}_{11} (\mathbf{S}_{11}+\lambda\mathbf{1}_{s})+2(\mathbf{\Omega}_{12}^{T}\mathbf{S}_{12})+\mathbf{\Omega} _{12}^{T}\mathbf{\Omega}_{11}^{-1}\mathbf{\Omega}_{12}(\mathbf{S}_{22}+\lambda\mathbf{1}_{t}) \big{]}\Big{)}\\ &\times\prod_{i<j}\exp\big{(}-\lambda|(\mathbf{\Omega}_{11})_{ij}|^{q }\big{)}\;\prod_{i,j}\exp\big{(}-\lambda|(\mathbf{\Omega}_{12})_{ij}|^{q}\big{)} \end{split} \tag{20}\]

### Run-time comparison with Gibbs sampler

The proposed CMF provides a significant speed-up in terms of sampling with respect to standard Gibbs sampling algorithms for posterior inference in Gaussian Graphical Models. For a realistic and fair comparison, we measure run-time on our real data experiment. For this purpose, we trained the Conditional Matrix Flow for \(q=1\) and compare its sampling speed against the Gibbs sampler introduced in Kaufmann et al. (2015). The setting presented here is the same one used to obtain the results shown in the Experiment section. The Gibbs sampler takes 89 seconds to generate \(500\) samples, which translates to about \(5.6\) samples per second. This result was obtained on a Intel(R) Xeon(R) CPU E5-1660 v3 @ 3.00GHz. On the other hand, on the consumer-grade GPU NVIDIA TITAN X (12GB VRAM) sampling is extremely efficient: each second we can generate \(2000\) independent samples from the approximate posterior. Relevantly, in order to retrieve the full posterior path as a function of \(\lambda\), we would need independent Markov Chains for each \(\lambda\) value, rendering the approach infeasible. In contrast, with the proposed framework we can draw independent samples for different \(\lambda\) values at the same computational cost.

### Artificial data

Figure 6: MAP estimate as a function of \(\lambda\) for different sub-\(l_{1}\) pseudo-norms. The dashed line is the frequentist solution path for \(q=1\).

Figure 7: Evolution of the (approximate) marginal log-likelihood of the CMF as a function of \(\lambda\) for \(T=1.00\) and \(q=1\). We select the optimal \(\lambda\) as its maximum (\(\lambda_{\text{CMF}}=3.52\), in blue) and compare with the frequentist estimate obtained through cross validation (\(\lambda_{\text{GLasso}}=3.36\), in red).

### Real data

Figure 8: 95\(\%\) posterior credibility intervals of the proposed CMF for the 4 entries with highest absolute median posterior as a function of the pseudo-norm \(q=\{1,0.9,0.8,0.7,0.6\}\).