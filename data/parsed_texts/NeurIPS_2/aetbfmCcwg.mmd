# Debiasing Synthetic Data Generated by Deep Generative Models

Alexander Decruyenaere \({}^{*}\)

Ghent University Hospital - SYNDARA

&Heidelinde Dehaene \({}^{*}\)

Ghent University Hospital - SYNDARA

&Paloma Rabaey

Ghent University - imec

&Christiaan Polet

Ghent University Hospital - SYNDARA

&Johan Decruyenaere

Ghent University Hospital - SYNDARA&Thomas Demeester

Ghent University - imec

&Stijn Vansteelandt

Ghent University - Department of Applied Mathematics,

Computer Science and Statistics

\({}^{*}\)Joint first authors and corresponding authors

{firstname.lastname}@ugent.be

###### Abstract

While synthetic data hold great promise for privacy protection, their statistical analysis poses significant challenges that necessitate innovative solutions. The use of deep generative models (DGMs) for synthetic data generation is known to induce considerable bias and imprecision into synthetic data analyses, compromising their inferential utility as opposed to original data analyses. This bias and uncertainty can be substantial enough to impede statistical convergence rates, even in seemingly straightforward analyses like mean calculation. The standard errors of such estimators then exhibit slower shrinkage with sample size than the typical 1 over root-\(n\) rate. This complicates fundamental calculations like p-values and confidence intervals, with no straightforward remedy currently available. In response to these challenges, we propose a new strategy that targets synthetic data created by DGMs for specific data analyses. Drawing insights from debiased and targeted machine learning, our approach accounts for biases, enhances convergence rates, and facilitates the calculation of estimators with easily approximated large sample variances. We exemplify our proposal through a simulation study on toy data and two case studies on real-world data, highlighting the importance of tailoring DGMs for targeted data analysis. This debiasing strategy contributes to advancing the reliability and applicability of synthetic data in statistical inference.

## 1 Introduction

The concept of generating synthetic data as a means of privacy protection was initially introduced by Rubin (1993) within the framework of multiple imputation, a widely used technique for managing the statistical analysis of incomplete data sets. Since its inception, a substantial body of literature on synthetic data has emerged (Raghunathan et al., 2003; Raghunathan, 2021; Drechsler, 2011; Raab et al., 2016; Reiter, 2005), with a recent surge in interest propelled by advancements in deep generative modelling technology (Raghunathan, 2021; van Breugel et al., 2023; Wan et al., 2017; Yan et al., 2022; Nowok et al., 2016; Endres et al., 2022; Hernandez et al., 2022). In this work, we focus on tabular synthetic data and their inferential utility, which captures whether synthetic data can be used to obtain valid estimates and inference for a population parameter (Decruyenaere et al., 2024).

The substantial privacy protection potential offered by synthetic data is marred by significant challenges that undermine their inferential utility (Raab et al., 2016). Previous work by Decruyenaere et al. (2024) has shown that these challenges are much more severe when using deep generative models (DGMs), rather than parametric statistical models, which is why we focus on the former. First, standard confidence intervals and p-values obtained on synthetic data may drastically underestimate the uncertainty in synthetic data as these ignore the size of the original data. Indeed, synthetic data obtained via generators trained on small datasets will unsurprisingly deliver much worse quality than synthetic data obtained via generators trained on large datasets. This uncertainty in the generator must therefore be translated into analysis results, such as confidence intervals and p-values. Standard confidence intervals and p-values ignore this, as they do not distinguish whether the data are synthetic or real. Second, it is well known from the literature on plug-in estimation that data-adaptive methods (such as DGMs) cannot succeed to estimate _all_ features of the data-generating distribution well (Bickel et al., 1993; van der Laan and Rose, 2011; Chernozhukov et al., 2018; Hines et al., 2022). These methods are designed to optimally balance bias and variance w.r.t. a chosen criterion, such as prediction error. However, they cannot guarantee that such an optimal trade-off is simultaneously made w.r.t. all possible discrepancy measures that exist, such as mean squared error in specific functionals (mean, variance, least squares projections...) of the observed data distribution. Such data-adaptive methods therefore leave non-negligible bias in estimators of such functionals, leading to excess variability, slow convergence, and confidence intervals that do not cover the truth at nominal level (and may even never contain the truth, even in large samples) (Decruyenaere et al., 2024).

Related work.While several approaches have been proposed to account for the uncertainty arising from synthetic data generation, we are not aware of strategies for generating and analysing DGM-based synthetic data that guarantee valid inference. Raghunathan et al. (2003) developed a framework inspired by the work on multiple imputation for missing data, by combining the results of multiple synthetic datasets, but this is not readily applicable to DGM-based synthetic data. Rai≈°a et al. (2023) extended this work for differentially private (DP) synthetic data, acknowledging the additional DP noise, but continue to consider parametric (Bayesian) data generation strategies.

Our work instead focuses on obtaining valid inference from a single synthetic dataset, which is arguably more attractive for use by practitioners. Raab et al. (2016) derived alternative combining rules that reduce to the correction factor \(\sqrt{1+m/n}\) for the standard error (SE) of an estimator in the case of inference from a single (non-DP) synthetic dataset of size \(m\) generated from an original dataset of size \(n\). The method suggested by Awan and Cai (2020) to preserve efficient estimators in a single (DP or non-DP) synthetic dataset relies on generating data conditional on the estimate in the original data. Both procedures are only applicable to parametric generative models and therefore suffer from the same limitation as the aforementioned approaches. To enable Bayesian inference from a single DP synthetic dataset, Wilde et al. (2021) proposed a corrected analysis that relies on the availability of additional public data, while Ghalebikesabi et al. (2022) investigated importance weighting methods to remove the noise-related bias, but they do not study the impact on inference.

Contributions.Our work is the first to propose a generator-agnostic solution that mitigates the impact of the typical slower-than-\(\sqrt{n}\)-convergence of estimators in synthetic data created by DGMs. As far as we are aware, our approach is thus the only one that provides some formal guarantees for (more) honest inference in this setting. In this paper, we show how the statistical bias in estimators can be removed, by adapting results on debiased or targeted machine learning (van der Laan and Rose, 2011; Chernozhukov et al., 2018) to the current setting where machine learning is not necessarily used in the analysis, but rather in the generation of synthetic data. Although we build upon ideas from existing work, our extension is non-trivial, since (1) previous work did not consider synthetic data; and (2) we demonstrate, with significant generality, how to mitigate the estimation errors in the DGM that would otherwise propagate into the estimator calculated on synthetic data.

In Section 2 and 3, we propose a generator-agnostic debiasing strategy, directed towards the downstream statistical analysis of the synthetic data. As such, we obtain estimators that are less sensitive to the typical slow (statistical) convergence affecting the generators. We illustrate this with a simulation study in Section 4, showing that the coverage of both the mean and linear regression coefficient estimators indeed improves. In Section 5, we further cement the utility of our debiasing strategy in a practical setting through two case studies. While the proposed strategy is generator-agnostic, we focus our analyses on two DGMs for tabular data: CTGAN and TVAE (Xu et al., 2019). Finally, Section 6 concludes with a discussion on our method and its limitations.

Notation and Set-Up

The aim of this paper is to use synthetic data in order to learn a specific functional \(\theta(.)\) of the observed data distribution \(P\). We formalise the problem of learning \(\theta(P)\) based on synthetic data as follows. Suppose that, based on \(n\) independent (possibly high-dimensional) samples \(O_{1},...,O_{n}\) from \(P\), we estimate the observed data distribution as \(\widehat{P}_{n}\). Here, \(\widehat{P}_{n}\) may be obtained by fitting parametric models to the distribution of the observed data, or alternatively by training DGMs. Based on \(m\) independent (synthetic) random samples \(S_{1},...,S_{m}\) from \(\widehat{P}_{n}\), we then estimate \(\theta(P)\). For this, the data analyst (to whom \(\widehat{P}_{n}\) is unknown) uses the \(m\) synthetic samples \(S_{1},...,S_{m}\) to approximate \(\widehat{P}_{n}\) as \(\widetilde{P}_{m}\), and obtains an estimator of \(\theta(P)\) given by \(\theta(\widetilde{P}_{m})\). We use \(\mathbb{P}_{n}\) to denote the empirical distribution of the observed data and \(\widetilde{\mathbb{P}}_{m}\) to denote the empirical distribution of the synthetic data. We index expectations by the distribution under which they are taken; e.g., \(E_{P}(Y)\) denotes the population expectation of \(Y\).

Throughout, we assume that the parameter of interest is sufficiently smooth in the data-generating distribution so that root-\(n\) consistent estimators (i.e., with SEs shrinking at 1 over root-\(n\) rate) can be obtained. Formally, we assume the \(\theta(P)\) is pathwise differentiable with efficient influence curve (EIC) \(\phi(.,P)\) under the nonparametric model, as is satisfied for (most) standard statistical analyses. The EIC is a functional derivative of \(\theta(P)\) w.r.t. the data-generating distribution \(P\) (in the sense of a Gateaux derivative), which has mean zero under \(P\)(Fisher and Kennedy, 2021; Hines et al., 2022).

In what follows, we illustrate our debiasing strategy via two examples. Here, we briefly outline some of their theoretical foundations we build upon. Appendix A.1 clarifies how these debiased estimators originate from their EIC. We further refer to them as debiased or EIC-based estimators.

Population mean.Adapting the traditional formulation of the population mean to the context of synthetic data, we choose \(\widetilde{P}_{m}=\widetilde{\mathbb{P}}_{m}\). As a result, we will study the large sample behaviour of the synthetic data sample average:

\[\theta(\widetilde{P}_{m})=\frac{1}{m}\sum_{i=1}^{m}S_{i}.\]

The EIC of this sample average is \(\phi(S,\widehat{P}_{n})=S-\theta(\widehat{P}_{n})\).

Linear regression coefficient.Second, suppose we are interested in a specific coefficient \(\theta\equiv\theta(P)\) of some exposure \(A\) for the outcome \(Y\) in the linear model \(E_{P}(Y|A,X)=\theta A+\omega(X)\), where \(X\) is a possibly high-dimensional vector of covariates and \(\omega(.)\) is an unknown function. Our proposal allows \(\omega(X)\) to correspond to a standard linear model in all covariates, but is more generally valid. Building upon the nonparametric definition of \(\theta\) as given in Appendix A.1, we adjust it to obtain an estimator in the setting of synthetic data. Let \(Y\), \(X\) and \(A\) be jointly or sequentially modelled by some generative model, from which a single synthetic dataset \(S_{i}=(\widetilde{Y}_{i},\widetilde{A}_{i},\widetilde{X}_{i})\) is sampled (\(i=1,\ldots,m\)). We then consider the estimated regression coefficient of exposure \(A\) given by

\[\theta(\widetilde{P}_{m})=\frac{\frac{1}{m}\sum_{i=1}^{m}\left\{\widetilde{A}_ {i}-E_{\widetilde{P}_{m}}(A|\widetilde{X}_{i})\right\}\left\{\widetilde{Y}_{i }-E_{\widetilde{P}_{m}}(Y|\widetilde{X}_{i})\right\}}{\frac{1}{m}\sum_{i=1}^{m }\left\{\widetilde{A}_{i}-E_{\widetilde{P}_{m}}(A|\widetilde{X}_{i})\right\} ^{2}},\]

where the nuisance parameters \(E_{\widetilde{P}_{m}}(A|\widetilde{X}_{i})\) and \(E_{\widetilde{P}_{m}}(Y|\widetilde{X}_{i})\) are estimated based on synthetic data. This EIC-based estimator coincides with the Maximum Likelihood Estimator (MLE) when least squares predictions for these nuisance parameters are used. Its EIC is (Vansteelandt and Dukes, 2022)

\[\phi(S,\widehat{P}_{n})=\frac{\left\{\widetilde{A}-E_{\widehat{P}_{n}}(A| \widetilde{X})\right\}\left[Y-E_{\widehat{P}_{n}}(Y|\widetilde{X})-\theta( \widehat{P}_{n})\left\{\widetilde{A}-E_{\widehat{P}_{n}}(A|\widetilde{X}) \right\}\right]}{E_{\widehat{P}_{n}}\left[\left\{\widetilde{A}-E_{\widehat{P} _{n}}(A|\widetilde{X})\right\}^{2}\right]}

In Appendix A.2 we derive that

\[\theta(\widetilde{P}_{m})-\theta(P) = \frac{1}{m}\sum_{i=1}^{m}\phi(S_{i},\widehat{P}_{n})-\frac{1}{m} \sum_{i=1}^{m}\phi(S_{i},\widetilde{P}_{m})+R(\widetilde{P}_{m},\widehat{P}_{n}) \tag{1}\] \[+\int\left\{\phi(S,\widetilde{P}_{m})-\phi(S,\widehat{P}_{n}) \right\}d(\widetilde{\mathbb{P}}_{m}-\widehat{P}_{n})\] \[+\frac{1}{n}\sum_{i=1}^{n}\phi(O_{i},P)-\frac{1}{n}\sum_{i=1}^{n} \phi(O_{i},\widehat{P}_{n})+R(\widehat{P}_{n},P)\] \[+\int\left\{\phi(O,\widehat{P}_{n})-\phi(O,P)\right\}d(\mathbb{P }_{n}-P).\]

We now examine the eight terms of this equation and discuss why some may be negligible while other may introduce bias. First, \(R(\cdot)\) are remainder terms, which can generally be shown to be small, but must be studied on a case-by-case basis; see Hines et al. (2022) for worked out examples. In order for these remainder terms to be \(o_{p}(m^{-1/2})\) and \(o_{p}(n^{-1/2})\), we generally need that faster than \(n^{-1/4}\) convergence rates are obtained for the unknown functionals of \(\widetilde{P}_{m}\) and \(\widehat{P}_{n}\) that appear in the EICs. It is unknown whether these convergence rates are attainable for DGMs; whether they are, will partly depend on the number of parameters in the DGM itself, the dimension of the data and the complexity of the observed data distribution. The simulation study in Section 4 will give further insight into this.

Second, the two empirical process terms (1) and (2) in the von Mises expansion can be shown to be \(o_{p}(m^{-1/2})\) and \(o_{p}(n^{-1/2})\) by Markov's inequality, under weak conditions. For (1) to converge to zero, we will need the difference between \(\widetilde{P}_{m}\) and \(\widehat{P}_{n}\) to converge to zero (in \(L_{2}(\widehat{P}_{n})\) at any rate), and the estimator to be calculated on a different part of the data than the one on which \(\widetilde{P}_{m}\) was estimated (Chernozhukov et al., 2018). For (2) to converge to zero, we will need \(\widehat{P}_{n}\) (e.g., the DGM) to consistently estimate \(P\) (in \(L_{2}(P)\) at any rate). In addition, it can be argued that the DGM needs to be trained on a different part of the data than that on which the debiasing step (see later) will be applied. In Appendix A.3, we elaborate on the necessity of sample splitting.

We thus foresee that the two remainder and two empirical process terms are negligible under certain conditions. Third, as elaborated in Appendix A.2, we show that the large sample behaviour of both

\[\frac{1}{n}\sum_{i=1}^{n}\phi(O_{i},P)\hskip 56.905512pt(3)\qquad\text{and} \hskip 56.905512pt\frac{1}{m}\sum_{i=1}^{m}\phi(S_{i},\widehat{P}_{n}) \tag{4}\]

is standard, and well understood; in particular, these terms vary around zero with variance that can be estimated well. By contrast, the terms

\[-\frac{1}{m}\sum_{i=1}^{m}\phi(S_{i},\widetilde{P}_{m})\hskip 56.905512pt(5)\qquad\text{and} \hskip 56.905512pt-\frac{1}{n}\sum_{i=1}^{n}\phi(O_{i},\widehat{P}_{n}) \tag{6}\]

need some further discussion. Term (5) generally fails to have mean zero because the synthetic data \(S_{i}\) do not originate from the distribution \(\widetilde{P}_{m}\). This term therefore induces a bias in \(\theta(\widetilde{P}_{m})\) that results from using data-adaptive estimates \(\widetilde{P}_{m}\) on the synthetic data; a similar term would appear if we instead analysed the real data. Term (6) likewise fails to have mean zero because the observed data \(O_{i}\) do not originate from the distribution \(\widehat{P}_{n}\). Also this term thus induces a bias, now resulting from the use of a generative model to obtain \(\widehat{P}_{n}\). It may be large relative to (3) when DGMs are used, because of slow convergence of \(\widehat{P}_{n}\). It is precisely this term that causes estimators based on synthetic data to converge slowly with increasing sample size, as observed in Decruyenaere et al. (2024).

After identifying the two problematic terms, we now propose in a second step a targeting or debiasing strategy to remove these bias terms (5) and (6). As in van der Laan and Rose (2011) and Chernozhukov et al. (2018), we will remove bias term (5) by analysing the data with debiased estimators based on the EIC that ensure that this bias term then becomes zero. Novel to our proposal is that we will additionally shift the generated data to ensure that also bias term (6) becomes zero. This bias term depends on the EIC, which itself depends on the target parameter of interest. In the next two paragraphs, we discuss how this can be done for the two considered estimators. Note that the proposed strategy does not require any actual finetuning or retraining of the DGM. For a given parameter of interest, a mere post-processing of the synthetic samples, based on access to the DGM as well as the original data it was trained on, allows eliminating the bias for a given parameter of interest. A graphical summary of the problem setting and our debiasing strategy can be found in Appendix A.4.

### Population mean

For the population mean, the debiasing step with respect to term (5) is implicit since the traditional estimator as given in Section 2 is a debiased estimator. Therefore, the proposal to debias a given DGM with respect to the population mean \(\theta(P)=\int odP(o)\) amounts to first training the DGM and then augmenting the output for the variable of interest to ensure that bias term (6) is zero, or hence

\[\frac{1}{n}\sum_{i=1}^{n}\phi(O_{i},\widehat{P}_{n})=\frac{1}{n}\sum_{i=1}^{n} O_{i}-\theta(\widehat{P}_{n})=\overline{O}-\theta(\widehat{P}_{n})=0.\]

In other words, the population mean of the synthetic data under the DGM should match the sample average of the real data. Here, \(\theta(\widehat{P}_{n})\) can be approximated by generating a very large sample \(k\) (e.g., one million samples) based on the DGM and calculating their sample mean \(\overline{Y}\). The generative model must then be corrected, to ensure that this sample mean equals \(\overline{O}\). To obtain a debiased synthetic sample, we shift all samples generated by the given DGM by adding \(\overline{O}-\overline{Y}\) to the considered variable. Note that the sample average of such a set of \(m\) corrected synthetic samples will generally differ from \(\overline{O}\).

### Linear regression coefficient

For linear regression coefficients, this section shows how the samples generated by a given DGM need to be adapted to eliminate bias term (6), written as follows (see Appendix A.5):

\[b=\frac{\sum_{i=1}^{n}\left\{A_{i}-E_{\widehat{P}_{n}}(A|X_{i})\right\}\left\{ Y_{i}-E_{\widehat{P}_{n}}(Y|X_{i})\right\}}{\sum_{i=1}^{n}\left\{A_{i}-E_{ \widehat{P}_{n}}(A|X_{i})\right\}^{2}}-\theta(\widehat{P}_{n}). \tag{7}\]

To compute this bias, we must generate, for each observed value \(X_{i}\), a very large sample of measurements of \(A\) and \(Y\) with the given level \(X_{i}\), based on the DGM. Then \(E_{\widehat{P}_{n}}(Y|X_{i})\) and \(E_{\widehat{P}_{n}}(A|X_{i})\) can be estimated as the sample average of those values for respectively \(Y\) and \(A\). Further based on a very large DGM-generated sample \(k\) (e.g. one million samples), we calculate \(\theta(\widehat{P}_{n})\) as follows:

\[\frac{\sum_{j=1}^{k}\left\{A_{j}-E_{\widehat{P}_{n}}(A|X_{j})\right\}\left\{Y_ {j}-E_{\widehat{P}_{n}}(Y|X_{j})\right\}}{\sum_{j=1}^{k}\left\{A_{j}-E_{ \widehat{P}_{n}}(A|X_{j})\right\}^{2}}. \tag{8}\]

Debiasing of the DGM can now be done by adding the product \(b\big{\{}\widetilde{A}_{i}-E_{\widehat{P}_{n}}(A|\widetilde{X}_{i})\big{\}}\) to the synthetic outcome observations \(\widetilde{Y}_{i}\) generated by the DGM. An in-depth elaboration is provided in Appendix A.5. With the proposed shifting, we ensure that the debiasing with respect to term (6) is completed. We then proceed our analysis with these shifted synthetic observations and employ the EIC-based estimator of Section 2, which ensures that bias term (5) equals zero as well.

### Properties

To summarise, when the remainder terms \(R(.)\) and the empirical process terms (1) and (2) are all \(o_{p}(m^{-1/2})\) and \(o_{p}(n^{-1/2})\) and, furthermore, the suggested debiasing approach is used so as to remove terms (5) and (6), then we expect that

\[\theta(\widetilde{P}_{m})-\theta(P) = \frac{1}{m}\sum_{i=1}^{m}\phi(S_{i},\widehat{P}_{n})+\frac{1}{n }\sum_{i=1}^{n}\phi(O_{i},P)+o_{p}(n^{-1/2})+o_{p}(m^{-1/2}).\]

In particular, the resulting estimator \(\theta(\widetilde{P}_{m})\) may even converge at root-\(n\) rates (under standard conditions of pathwise differentiability (Bickel et al., 1993; Hines et al., 2022)) and has an easy-to-calculate variance that acknowledges the uncertainty in the generation of synthetic data (provided that the statistical convergence of the generator is not too slow). In Appendix A.6, we show that the variance of \(\theta(\widetilde{P}_{m})\) may be approximated by

\[\left(\frac{1}{m}+\frac{1}{n}\right)E\left\{\phi(O,P)^{2}\right\},\]thereby generalising results known for parametric synthetic data generators (Raab et al., 2016; Decruyenaere et al., 2024), where the correction factor \(\sqrt{1+m/n}\) was proposed. Thus, when \(m\rightarrow\infty\), the debiased estimator \(\theta(\widetilde{P}_{m})\) based on debiased synthetic data has the same distribution as when the real data were analysed. Therefore, the proposed debiasing strategy delivers analysis results that are asymptotically equivalent to those obtained from the same analysis on the real data, provided that \(n/m=o(1)\). This means that results of the same quality and confidence intervals of the same expected length are then obtained, as will be illustrated in the case study in Section 5.2.

Since \(E\left\{\phi(O,P)^{2}\right\}\) is unknown, it merely remains to estimate it as

\[\frac{1}{m}\sum_{i=1}^{m}\phi(S_{i},\widetilde{P}_{m})^{2}.\]

This is a consistent estimator when \(\widetilde{P}_{m}\) converges to \(\widehat{P}_{n}\) as \(m\) goes to infinity, and moreover, \(\widehat{P}_{n}\) converges to \(P\) as \(n\) goes to infinity. We note that this sample variance will be subject to bias that results from 'poor' tuning of the DGM. Removing this bias is not required, because this variance will be scaled by \(1/m+1/n\) so that any bias becomes negligible in large samples. While the use of debiased estimators based on the EIC of \(E\left\{\phi(O,P)^{2}\right\}\) may potentially improve performance, this goes beyond the scope of this work.

Practical implications.Sections 3.1 and 3.2 show how bias term (6) is eliminated by shifting the synthetic variable of interest. We describe how bias term (5) is also removed by using debiased estimators, which we referred to as EIC-based estimators (see Section 2). As mentioned earlier, the EIC-based estimator for the population mean always coincides with the sample average, while for the linear regression coefficient it only reduces to the ordinary least squares estimator when data-adaptive estimation of the nuisance parameters is not used. This implies that it may suffice for the applied researcher to 1) shift the synthetic data and apply the traditional estimators, and 2) to multiply the SE of the estimator with \(\sqrt{1+m/n}\) to obtain valid inference from a single synthetic dataset. However, the EIC-based estimators are recommended since they are robust against model misspecification by allowing for more flexibility in the estimation of the nuisance parameters.

## 4 Simulation study

Our proposed debiasing strategy is empirically validated by a simulation study that covers both estimators 3.1 (sample mean) and 3.2 (linear regression coefficient). Having full control over the data generating process allows us to calculate the bias, SE and convergence rate of both estimators in synthetic data, with and without our debiasing strategy. The data generating process consists of the following four variables: _age_ (normally distributed), atherosclerosis _stage_ (ordinal with four categories), _therapy_ (binary), and _blood pressure_ (normally distributed). The Directed Acyclic Graph (DAG) in Figure 1 represents the dependency structure and we refer to Appendix A.7.1 for more details. This setting allows us to simultaneously target the population mean of _age_ and the population effect of _therapy_ (\(A\)) on _blood pressure_ (\(Y\)) adjusted for _stage_ (\(X\)).

### Set-up

We conduct a Monte Carlo simulation study, where \(n\) independent records are sampled from the data generating process, forming the observed **original dataset**\(O_{1},...,O_{n}\). This process is repeated \(250\) times, with the sample size \(n\) varying log-uniformly between \(50\) and \(5000\) (i.e., \(n\in\left\{50,160,500,1600,5000\right\}\)). Per original dataset, following DGMs are trained: CTGAN and TVAE (Xu et al., 2019), of which a detailed explanation can be found in Appendix A.7.2. From these DGMs \(m\) synthetic data records are sampled that constitute the **default synthetic dataset**\(S_{1},...,S_{m}\). We set \(m=n\) to retain the dimensionality of the original data. Subsequently, each default synthetic dataset is debiased with respect to both estimands using the steps provided in Section 3, leading to a **debiased synthetic dataset**. Finally, two estimators are calculated in each of three datasets: the sample mean of _age_ and the linear regression coefficient of _therapy_ on _blood pressure_ adjusted for

Figure 1: DAG for the variables in the simulation study.

stage_. We always report the maximum likelihood estimation (MLE)-based estimators, as used in traditional statistical analysis, of which the standard errors (SEs) are inflated with the correction factor \(\sqrt{1+m/n}\) to acknowledge the sampling variability of synthetic data. These estimators will deliver similar estimates as the EIC-based estimators since no data-adaptive estimation is used (see Section 3.3 and Appendix A.7.5).

### Results

We now present the results of our simulation study. The DGMs were trained using the default hyperparameters as suggested by the package Synthcity(Qian et al., 2023). We also show results obtained for other hyperparameters (the default in the package SDV(Patki et al., 2016)) in Appendix A.7.4. In Section 4.2.1 we evaluate the empirical coverage of the \(95\)% confidence interval (CI) for the population parameters. Our debiasing strategy should enhance the coverage, preferably to the nominal level, allowing for (more) honest inference, which is the main contribution of our strategy. Then, we analyse step by step the various components that may influence the coverage by investigating the bias and SE of the estimators in Section 4.2.2, and their convergence rates in Section 4.2.3. Additional results are presented in Appendix A.7.4, including the convergence rates of the nuisance parameters estimated by the DGMs. Our code is available on Github: [https://github.com/syndara-lab/debiased-generation](https://github.com/syndara-lab/debiased-generation).

#### 4.2.1 Coverage

By definition, \(95\)% (_empirical_) of the \(95\)% CIs (_nominal_) should cover the population parameter. Figure 2 depicts the empirical coverage of the \(95\)% CIs obtained from both original and synthetic samples for the population mean of _age_ and the population effect of _therapy_ on _blood pressure_ adjusted for _stage_. The results indicate that our debiasing strategy delivers empirical coverage levels for the population mean that approximate the nominal level for all sample sizes and DGMs considered. By contrast, the coverage based on the default synthetic datasets decreases with increasing \(n\) due to slower shrinkage of the SE than the typical 1 over root-\(n\) rate, as calculated in Section 4.2.3 and previously elaborated in Decruyenaere et al. (2024). For the population regression coefficient, debiasing delivers coverage at the nominal level for TVAE across all sample sizes, but not for CTGAN, although it clearly provides more honest inferences than based on the default synthetic datasets. The residual loss of coverage likely results from not using (efficient) sample splitting (see Appendix A.3).

#### 4.2.2 Bias and Standard Error

Figures (a)a and (b)b depict the estimates and their SE, respectively, for the sample mean of _age_ obtained in the default and debiased synthetic datasets. Figures A5 and A6 in the appendix show these for the linear regression coefficient of _therapy_ on _blood pressure_ adjusted for _stage_. In Figure (a)a, each dot is an estimate per Monte Carlo run and the true population parameters are represented by the horizontal dashed line. This figure allows a qualitative assessment of two key properties of estimators: empirical bias (i.e., the average difference between the estimates and the population parameter, as represented by the solid line) and empirical SE (i.e., the standard deviation of the estimates, as indicated by the vertical spread of the estimates). Ideally, both converge to zero as the sample size grows larger. The convergence rate conveys the rate at which this happens. The funnel represents the default behaviour of an unbiased estimator based on original data of which the SE diminishes at a rate of 1 over root-\(n\).

As shown in Figure (a)a, the sample mean of _age_ is unbiased in the default synthetic datasets, but exhibits a large empirical SE that shrinks slowly with sample size due to the data-adaptive nature of

Figure 2: Empirical coverage of the \(95\)% confidence interval for the population mean of _age_ and the population effect of _therapy_ on _blood pressure_ adjusted for _stage_.

DGMs. It is exactly this variability that is, on average, underestimated by the MLE-based SE in Figure 2(b). Debiasing reduces the empirical variability and accelerates its shrinkage, such that the average MLE-based SE approximates the empirical SE. For the linear regression coefficient of _therapy_ on _blood pressure_ adjusted for _stage_, debiasing reduces finite-sample bias and also improves shrinkage of the empirical SE. Albeit less pronounced, the average MLE-based SE still underestimates the empirical SE with CTGAN despite debiasing (see Figures A5 and A6 in the appendix).

#### 4.2.3 Convergence Rate of Standard Error

Assuming a power law \(n^{-a}\) in convergence rate for the empirical SE, we estimate the exponent \(a\) from five logarithmically spaced sample sizes \(n\) between 50 and 5000, shown in Table 1 and Figure A7 in the appendix. Standard statistical analysis assumes that the bias converges faster than the SE with the latter diminishing at a rate of \(1/\sqrt{n}\). However, the SEs produced by DGMs converge much slower (i.e., \(a_{SE}<0.5\)), leading to a progressively increasing underestimation of the empirical SE by the MLE-based SE (which assumes \(\sqrt{n}\)-convergence) as the sample size grows larger. In turn, this results in too narrow CIs and poor coverage, as observed in Section 4.2.1. By contrast, our debiasing strategy renders estimators of which the SE converges at approximately root-\(n\) rates (i.e., \(a_{SE}=0.5\)), explaining the improvement in coverage of the CIs as a result of debiasing.

## 5 Case studies

To illustrate our findings and highlight their implications for the applied researcher, we conduct two case studies. First, Section 5.1 transfers the framework from our simulation study to the International Stroke Trial (IST) dataset (Sandercock et al., 2011). Second, Section 5.2 describes whether analysis results from the Adult Census Income dataset (Becker and Kohavi, 1996) are similar to those obtained from the real data, when the sample size \(m\) of the generated synthetic data is very large. In both case studies, estimated SEs in the default synthetic and debiased synthetic datasets are corrected by multiplying with factor \(\sqrt{1+m/n}\) to acknowledge the sampling variability of synthetic data.

### International Stroke Trial

We adapt the framework discussed in Section 4.1 to the IST dataset, one of the biggest randomised trials in acute stroke research (Sandercock et al., 2011). The dataset with \(19285\) complete cases now

\begin{table}
\begin{tabular}{l r r r r} \hline \hline
**Estimator** & **CTGAN** & **TVAE** & **CTGAN** & **TVAE** \\ \hline \multicolumn{5}{c}{**Default synthetic datasets**} & \multicolumn{2}{c}{**Debiased synthetic datasets**} \\ Mean age & 0.01 [-0.16; 0.18] & 0.21 [0.04; 0.39] & 0.50 [0.46; 0.54] & 0.47 [0.44; 0.50] \\ Effect therapy & 0.31 [0.22; 0.40] & 0.09 [-0.13; 0.31] & 0.43 [0.31; 0.55] & 0.46 [0.38; 0.55] \\ \hline \hline \end{tabular}
\end{table}
Table 1: Estimated exponent \(a\) [\(95\)% CI] for the convergence rate \(n^{-a}\) for empirical SE.

Figure 3: Each dot in Figure (a) is an estimate for the population mean of _age_ per Monte Carlo run. The funnel indicates the behaviour of an unbiased and \(\sqrt{n}\)-consistent estimator based on observed data. Figure (b) depicts the empirical and average MLE-based SE for the sample mean of _age_.

constitutes our _population_. We mimic different hypothetical settings where an institution only has access to a limited _sample_ of observations, with the sample size \(n\) varying between \(50\) and \(5000\). In order to easily share the data with other researchers, the institution generates a synthetic dataset with sample size \(m\), where \(m=n\). We repeated this process \(100\) times per sample size \(n\) to be able to calculate the empirical coverage levels. For illustration purposes, we focus on the effect of aspirin on the outcome at \(6\) months and report the proportion of deaths for the two treatment arms (aspirin and no aspirin), and its corresponding risk difference. For each value of \(n\), two default synthetic datasets were generated using both CTGAN and TVAE. Next, for each of these, we first split the default synthetic dataset by treatment, debias the data with relation to the population mean within each treatment arm, and then combine them back into one debiased synthetic dataset. We noticed that using the same hyperparameters as in the simulation study resulted in biased estimates, as can be seen in Figure A12 in the appendix. For this reason, we highlight the results obtained by training with the default hyperparameters suggested by the package SDV(Patki et al., 2016) instead.

One of the original research questions in Sandercock et al. (2011) was whether or not there is a difference in risk of death between the treatment arms. Suppose a researcher can repeatedly collect information on 500 subjects and uses original, default synthetic and debiased synthetic data to make an inferential statement about this risk difference. Figure 4 depicts the confidence intervals for the first 15 repetitions, with the vertical dashed lines representing the true risk difference of \(-0.009\) as calculated based on our population (the full dataset). Should the researcher use the default synthetic data, they would falsely conclude (in 7 out of these 15 repetitions) that the risk is significantly different from \(-0.009\), while using the debiased synthetic dataset basically eliminates this high number of false-positives (with all these 15 intervals containing the population parameter), as is the case in the original data as well. More results can be found in Appendix A.8.1.

### Adult Census Income Dataset

We also perform a case study on the Adult Census Income dataset, which comprises \(45222\) complete cases and \(14\) unique variables (Becker and Kohavi, 1996). We assume the researcher's interest lies in inferring the population mean of _hours_ worked per week (estimated via the sample mean) and the average _sex_-adjusted difference in _age_ between persons with an _income_ of \(>\$50\)K a year vs. not (estimated via a linear regression model \(age\left(Y\right)\sim income\left(A\right)+sex\left(X\right)\)). Our goal in this case study is to confirm whether inferential results obtained using the debiased synthetic dataset are asymptotically equivalent (i.e. with \(m>>n\) in our debiasing strategy) to those obtained using the original data. To test this across different sample sizes, the original data constitute five different samples of the Adult Census Income dataset with sizes \(n\) varying log-uniformly between \(50\) and \(45222\). For each original dataset, a default synthetic dataset of size \(m=10^{6}\) was generated by TVAE. Subsequently, this dataset was debiased following the steps described in Section 3.1 (sample mean) and Section 3.2 (linear regression coefficient).

Figure 5 depicts the \(95\)% CIs for both estimators, the five original sample sizes and the three versions of datasets. This indeed confirms that analysis on the debiased synthetic dataset leads to results of similar quality and CIs of similar length compared to the original dataset. By contrast, the analysis on the default synthetic dataset may yield results of inferior quality and even incorrect conclusions.

Discussion

In this paper, we propose a new debiasing strategy that targets synthetic data created by DGMs towards the downstream task of statistical inference from the resulting synthetic data. We establish our theory for two estimators by applying insights from debiased or targeted machine learning literature (van der Laan and Rose, 2011; Chernozhukov et al., 2018) to the current setting where machine learning is not necessarily used in the analysis, but rather in the generation of synthetic data. We obtain estimators that are less sensitive to the typical slow (statistical) convergence affecting DGMs and thereby improve the inferential utility of the synthetic data.

We illustrated the impact of our proposal through a simulation study on toy data and two case studies on real-world data. Our debiasing strategy results in root-\(n\) consistent estimators based on the synthetic data and thereby better coverage of the confidence intervals, allowing for more honest inference. While coverage was clearly improved, it was not guaranteed to be at the nominal level. Indeed, it may remain anti-conservative for some estimators and DGMs, due to slow convergence inherent to these models and/or due to residual overfitting bias that could not be removed since sample splitting was not performed. Future work should focus on efficient sample splitting, where the resulting bias reduction outweighs the increase in finite-sample bias that arises from training on smaller sample sizes. Alternatively, findings from Ghalebikesabi et al. (2022) on importance weighting could be incorporated, with the weights being targeted to eliminate the impact of the data-adaptive estimation of the weights. This may potentially relax the fast baseline convergence assumption, and enable the same debiased synthetic data to be used for multiple downstream analyses.

A key advantage of our debiasing strategy is that it may deliver synthetic data created by DGMs of which the analysis is equivalent to the original data analysis, provided that the synthetic sample size is chosen to be much larger than the original sample size. Although the risk of disclosure may increase with the size of non-DP synthetic data (Reiter and Drechsler, 2010), this trade-off is beyond the scope of our paper. More interestingly, in the case of DP synthetic data, our debiasing strategy may exploit their post-processing immunity that allows for data transformations without compromising privacy guarantees (Dwork and Roth, 2014). However, our strategy needs to be extended to incorporate the DP-constraints when studying the difference between \(\theta(\widetilde{P}_{m})\) and \(\theta(P)\) in DP synthetic data.

Limitations of our proposal include the low-dimensional setting of our simulation and case studies, for which DGMs might be less well suited. The positive results found for two widely used estimators in this simple setting highlight the utility of a debiasing approach and are encouraging in terms of future larger-scale applications. However, before addressing these, it is important to first understand low-dimensional settings, where valid inference is already challenging to attain. While our debiasing strategy boils downs to a post-processing step, one could argue that the lack of change to the DGM's training strategy itself is actually a strength, since it renders our strategy generator-agnostic.

Another limitation concerns the fact that our debiasing strategy for a regression coefficient requires sampling of synthetic data conditional on a covariate, which is not available in all DGMs. However, this issue is partially mitigated in the case of conditioning on categorical variables, since one can always generate a synthetic dataset unconditionally and then only select samples that fit the condition - though this approach also has its limits, especially when conditioning on multiple covariates at once. Zhou et al. (2023) propose a deep generative approach to sample from a conditional distribution, even when working with high-dimensional data. Future work could explore this strategy.

Finally, our proposal requires that the person generating synthetic data is aware of the analyses that will be run on those data, and has access to the corresponding EICs needed for debiasing (which in particular rules out the possibility for debiasing w.r.t. non-pathwise differentiable parameters, such as conditional means or predictions). For each parameter of interest, the data generated by the DGM will then need to be debiased (simultaneously) w.r.t. that parameter's EIC, which is left for future research. In the case of original data, several debiased estimation strategies that do not require exact knowledge about the EIC already exist. These methods include a) approximating the EIC through finite-differencing (Carone et al., 2019; Jordan et al., 2022) or stochastic approximations via Monte Carlo (Agrawal et al., 2024), and b) automatically estimating the EIC from the data through auto-DML (Chernozhukov et al., 2022). Alternatively, kernel debiased plug-in estimation methods (Cho et al., 2023) enable simultaneous debiasing of all pathwise differentiable target parameters that meet certain regularity conditions, without requiring any knowledge about the EIC. Integrating their insights could further strengthen the foundations of our current work on the interplay between synthetic data, deep generative modelling, and debiased machine learning.

## Acknowledgments and Disclosure of Funding

Paloma Rabaey's research is funded by the Research Foundation Flanders (FWO-Vlaanderen) with grant number 1170124N. This research also received funding from the Flemish government under the "Onderzoeksprogramma Artificiele Intelligentie (AI) Vlaanderen" programme.

## References

* Agrawal et al. (2024) Agrawal, R., Witty, S., Zane, A., and Bingham, E. (2024). Automated efficient estimation using monte carlo efficient influence functions. _arXiv preprint arXiv:2403.00158_.
* Awan and Cai (2020) Awan, J. and Cai, Z. (2020). One step to efficient synthetic data. _arXiv preprint arXiv:2006.02397_.
* Becker and Kohavi (1996) Becker, B. and Kohavi, R. (1996). Adult. UCI Machine Learning Repository.
* Bickel et al. (1993) Bickel, P., Klaassen, C., Ritov, Y., and Wellner, J. (1993). _Efficient and Adaptive Estimation for Semiparametric Models_, volume 4. Baltimore Johns Hopkins University Press.
* Carone et al. (2019) Carone, M., Luedtke, A. R., and van der Laan, M. J. (2019). Toward computerized efficient estimation in infinite-dimensional models. _Journal of the American Statistical Association_.
* Chernozhukov et al. (2018) Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. _The Econometrics Journal_, 21(1):C1-C68.
* Chernozhukov et al. (2022) Chernozhukov, V., Newey, W. K., and Singh, R. (2022). Automatic debiased machine learning of causal and structural effects. _Econometrica_, 90(3):967-1027.
* Cho et al. (2023) Cho, B., Gan, K., Malenica, I., and Mukhin, Y. (2023). Kernel Debiased Plug-in Estimation. _arXiv preprint arXiv:2306.08598_.
* Decruyenaere et al. (2024) Decruyenaere, A., Dehaene, H., Rabaey, P., Polet, C., Decruyenaere, J., Vansteelandt, S., and Demeester, T. (2024). The real deal behind the artificial appeal: Inferential utility of tabular synthetic data. In Kiyavash, N. and Mooij, J. M., editors, _Proceedings of the Fortieth Conference on Uncertainty in Artificial Intelligence_, volume 244 of _Proceedings of Machine Learning Research_, pages 966-996. PMLR.
* Drechsler (2011) Drechsler, J. (2011). _Synthetic datasets for statistical disclosure control: theory and implementation_, volume 201. Springer Science & Business Media.
* Dwork and Roth (2014) Dwork, C. and Roth, A. (2014). The Algorithmic Foundations of Differential Privacy. _Foundations and Trends in Theoretical Computer Science_, 9(3-4):211-407.
* Endres et al. (2022) Endres, M., Mannarapotta Venugopal, A., and Tran, T. S. (2022). Synthetic data generation: a comparative study. In _Proceedings of the 26th International Database Engineered Applications Symposium_, pages 94-102.
* Fisher and Kennedy (2021) Fisher, A. and Kennedy, E. H. (2021). Visually communicating and teaching intuition for influence functions. _The American Statistician_, 75(2):162-172.
* Ghalebikesabi et al. (2022) Ghalebikesabi, S., Wilde, H., Jewson, J., Doucet, A., Vollmer, S., and Holmes, C. (2022). Mitigating statistical bias within differentially private synthetic data. In Cussens, J. and Zhang, K., editors, _Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence_, volume 180 of _Proceedings of Machine Learning Research_, pages 696-705. PMLR.
* Goodfellow et al. (2014) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. _Advances in neural information processing systems_, 27.
* Hernandez et al. (2022) Hernandez, M., Epelde, G., Alberdi, A., Cilla, R., and Rankin, D. (2022). Synthetic data generation for tabular health records: A systematic review. _Neurocomputing_, 493:28-45.
* Hines et al. (2022) Hines, O., Dukes, O., Diaz-Ordaz, K., and Vansteelandt, S. (2022). Demystifying statistical learning based on efficient influence functions. _American Statistician_, 76(3):292-304.
* Hines et al. (2020)

[MISSING_PAGE_FAIL:12]

Appendix

### Elaboration estimators for synthetic data

Population mean.The population mean of the observed data \(o\) is defined as

\[\theta(P)=\int o\frac{dP(o)}{do}do=\int odP(o),\]

which we will denote short as \(\int OdP\) and its efficient influence curve (EIC) is \(\phi(O,P)=O-\theta(P)\). Choosing \(\widetilde{P}_{m}=\widetilde{\mathbb{P}}_{m}\), we will then study the large sample behaviour of the synthetic data estimator:

\[\theta(\widetilde{P}_{m})=\int sd\widetilde{\mathbb{P}}_{m}(s)=\frac{1}{m} \sum_{i=1}^{m}S_{i}.\]

Linear regression.Suppose we are interested in a specific coefficient \(\theta\equiv\theta(P)\) of some exposure \(A\) in the linear model

\[E_{P}(Y|A,X)=\theta A+\omega(X),\]

where \(X\) is a possibly high-dimensional covariate and \(\omega(.)\) is an unknown function. Our proposal allows \(\omega(X)\) to correspond to a standard linear model, but is less restrictive. The parameter \(\theta\) in this linear model can be nonparametrically defined as

\[\theta(P)=\frac{E_{P}\left[\left\{A-E_{P}(A|X)\right\}\left\{Y-E_{P}(Y|X) \right\}\right]}{E_{P}\left[\left\{A-E_{P}(A|X)\right\}^{2}\right]}\]

in the sense that it reduces to \(\theta\) when the above model holds. Its EIC is (Vansteelandt and Dukes, 2022)

\[\phi(O,P)=\frac{\left\{A-E_{P}(A|X)\right\}\left[Y-E_{P}(Y|X)-\theta(P)\left\{ A-E_{P}(A|X)\right\}\right]}{E_{P}\left[\left\{A-E_{P}(A|X)\right\}^{2}\right]}\]

Denote the obtained synthetic data samples as \(S=(\widetilde{Y},\widetilde{A},\widetilde{X})\). An estimator \(\theta(\widetilde{P}_{m})\) is then obtained by substituting in the above expression for \(\theta(P)\), the first expectation in the numerator and denominator by a sample average, \(E_{P}(A|X)\) and \(E_{P}(Y|X)\) by data-adaptive predictions \(E_{\widetilde{P}_{m}}(A|\widetilde{X}_{i})\) and \(E_{\widetilde{P}_{m}}(Y|\widetilde{X}_{i})\) obtained based on the synthetic data. This delivers estimator

\[\theta(\widetilde{P}_{m})=\frac{\frac{1}{m}\sum_{i=1}^{m}\left\{\widetilde{A}_ {i}-E_{\widetilde{P}_{m}}(A|\widetilde{X}_{i})\right\}\left\{\widetilde{Y}_{i} -E_{\widetilde{P}_{m}}(Y|\widetilde{X}_{i})\right\}}{\frac{1}{m}\sum_{i=1}^{m} \left\{\widetilde{A}_{i}-E_{\widetilde{P}_{m}}(A|\widetilde{X}_{i})\right\}^{ 2}}.\]

[MISSING_PAGE_FAIL:14]

induce excess variability. Next letting also the sample size \(n\) go to infinity, we obtain convergence of \(m^{-1/2}\sum_{i=1}^{m}\phi(S_{i},\widehat{P}_{n})\) to \(E\left\{\phi(O,P)^{2}\right\}N(0,1)\) under the assumption that \(E\left\{\phi(S,\widehat{P}_{n})^{2}|\widehat{P}_{n}\right\}\) converges in probability to \(E\left\{\phi(O,P)^{2}\right\}\). This is a weak assumption because

\[E\left\{\phi(S,\widehat{P}_{n})^{2}|\widehat{P}_{n}\right\}=\int\phi(o, \widehat{P}_{n})^{2}d\widehat{P}_{n}(o)\]

is generally smooth (continuous) in the distribution of the data (as in the considered two examples). Moreover, the flexibility offered by deep generative models (DGMs) makes it reasonable to assume that \(\widehat{P}_{n}\) converges to \(P\) (e.g., in \(L_{2}(P)\)); while this convergence may be slow, no requirements on the rate of convergence are needed for the above assumption to be satisfied.

### Note on sample splitting

In order to let

\[\int\left\{\phi(O,\widehat{P}_{n})-\phi(O,P)\right\}d(\mathbb{P}_{n}-P)\]

converge to zero, we will need the DGM to be trained on a different part of the data than the one on which the debiasing step will be applied. Such sample splitting is needed to prevent overfitting bias that may otherwise result from the highly data-adaptive nature of DGMs. In addition, we need \(\widehat{P}_{n}\) (e.g., the DGM) to consistently estimate \(P\) in the sense that the squared mean (under \(P\)) of \(\phi(O,\widehat{P}_{n})-\phi(O,P)\) (at fixed \(\widehat{P}_{n}\)) converges to zero in probability.

To prevent efficiency loss with sample splitting, one may consider the use of \(k\)-fold cross-fitting. For this, we randomly split the data in \(k\) folds, each time train the DGM on \(k-1\) folds to obtain an estimator of the observed data distribution \(\widehat{P}_{(k-1)n/k}\) and calculate the bias

\[-\frac{1}{n/k}\sum_{i}\phi(O_{i},\widehat{P}_{(k-1)n/k}),\]

based on the \(n/k\) data points \(O_{i}\) in the remaining fold. The average of the \(k\) obtained bias estimates can next be used for debiasing (see the next section for specific examples).

However, it remains to be seen from future work if the resulting bias reduction outweighs the increase in finite-sample bias that may result from training the DGM on smaller sample sizes. Preliminary simulations with a simple implementation suggested that this was not the case, which is why our results in the main text are reported without the use of sample splitting. Furthermore, the remainder of the theory discards this nuance for now as well.

### Graphical summary

Figure A1: Suppose that interest lies in inferring the mean \(\theta(.)\) (orange solid vertical line) of the ground truth distribution \(P\) (orange solid curve) from synthetic data. **(1)** First, a random sample of original data of size \(n\) with empirical distribution \(\mathbb{P}_{n}\) (orange histogram) is collected from this ground truth. Theory on asymptotic linearity prescribes that the sample mean (orange dashed vertical line) will deviate from the population mean by order of 1 over root-\(n\) (grey arrow). **(2)** Subsequently, a deep generative model is trained on these original data, yielding an estimated distribution \(\widehat{P}_{n}\) (blue solid curve). Its mean (blue solid vertical line) may in turn differ from the original sample mean by an order larger than 1 over root-\(n\) (red arrow), which is referred to as regularisation bias in Decruyenaere et al. (2024). **(3)** Finally, synthetic data of size \(m\) (here \(m=n\)) are sampled from the estimated distribution \(\widehat{P}_{n}\), forming the empirical distribution \(\widetilde{\mathbb{P}}_{m}=\widetilde{P}_{m}\) (blue histogram with sample mean indicated by the blue dashed vertical line). The mean of both distributions \(\widehat{P}_{n}\) and \(\widetilde{P}_{m}\) will again differ by order of 1 over root-\(m\) (green arrow). Ideally, the data analyst, who uses the synthetic data to estimate \(\theta(P)\) by \(\theta(\widetilde{P}_{m})\), needs to take into account these three sources of random variability. **(4)** The large sample behaviour of the synthetic data estimator \(\theta(\widetilde{P}_{m})\) is depicted by repeating the above procedure multiple times across increasing sample sizes of \(n=m\) and storing each estimate of the synthetic sample mean. Although the estimator remains unbiased for \(\theta(P)\) (dashed line), its empirical standard error becomes larger than in the original data due to the additional sources of variability. However, the correction factor \(\sqrt{1+m/n}\) to the model-based standard error previously proposed by Raab et al. (2016) only captures the original data sampling variability (grey funnel) and a lower bound of the synthetic data sampling variability (green funnel), while the uncertainty associated with the regularisation bias (red funnel) remains unaccounted for. Since it cannot readily be expressed analytically, our debiasing strategy will eliminate the latter by shifting the mean of the distribution \(\theta(\widehat{P}_{n})\) estimated by the generative model towards the original sample mean (thereby removing the red arrow and funnel). Additionally, choosing synthetic sample sizes of \(m\rightarrow\infty\) will shrink the synthetic data sampling variability (ultimately removing the green arrow and funnel), such that the synthetic data estimator exhibits similar large sample behaviour as in original data.

### Elaboration on debiased strategy for linear regression coefficient

For the linear regression coefficient case, this section shows how the samples generated by a given DGM need to adapted to eliminate the bias \(b\), given by

\[b = \frac{\sum_{i=1}^{n}\left\{A_{i}-E_{\widehat{P}_{n}}(A|X_{i})\right\} \left[Y_{i}-E_{\widehat{P}_{n}}(Y|X_{i})-\theta(\widehat{P}_{n})\left\{A_{i}-E _{\widehat{P}_{n}}(A|X_{i})\right\}\right]}{\sum_{i=1}^{n}\left\{A_{i}-E_{ \widehat{P}_{n}}(A|X_{i})\right\}^{2}}\] \[= \frac{\sum_{i=1}^{n}\left\{A_{i}-E_{\widehat{P}_{n}}(A|X_{i}) \right\}\left\{Y_{i}-E_{\widehat{P}_{n}}(Y|X_{i})\right\}}{\sum_{i=1}^{n} \left\{A_{i}-E_{\widehat{P}_{n}}(A|X_{i})\right\}^{2}}-\theta(\widehat{P}_{n})\]

To compute this bias, we must generate, for each observed value \(X_{i}\), a very large sample of measurements of \(A\) and \(Y\) with the given level \(X_{i}\), based on the DGM. Then \(E_{\widehat{P}_{n}}(Y|X_{i})\) can be calculated as the sample average of those values for \(Y\), and likewise \(E_{\widehat{P}_{n}}(A|X_{i})\) can be calculated as the sample average of those values for \(A\). Further based on a large sample generated based on the DGM, we calculate \(\theta(\widehat{P}_{n})\) as the sample average of \(\left\{A-E_{\widehat{P}_{n}}(A|X)\right\}\left\{Y-E_{\widehat{P}_{n}}(Y|X)\right\}\) divided by the sample average of \(\left\{A-E_{\widehat{P}_{n}}(A|X)\right\}^{2}\).

Debiasing of the DGM can now be done by adding \(b\{\widetilde{A}_{i}-E_{\widehat{P}_{n}}(A|\widetilde{X}_{i})\}\) to the synthetic outcome observations generated by the DGM. This change does not affect the predictions \(E_{\widehat{P}_{n}}(Y|X_{i})\) from the DGM (because \(\widetilde{A}_{i}-E_{\widehat{P}_{n}}(A|X_{i})\) averages to zero for each choice of \(X_{i}\)). Further, with this change, \(\theta(\widehat{P}_{n})\) also increases with \(b\) units as follows:

\[\frac{\sum_{i=1}^{N}\left\{\widetilde{A}_{i}-E_{\widehat{P}_{n}}(A|\widetilde{ X}_{i})\right\}\left\{\widetilde{Y}_{i}+b\left\{\widetilde{A}_{i}-E_{\widehat{P} _{n}}(A|\widetilde{X}_{i})\right\}-E_{\widehat{P}_{n}}(Y|\widetilde{X}_{i}) \right\}}{\sum_{i=1}^{N}\left\{\widetilde{A}_{i}-E_{\widehat{P}_{n}}(A| \widetilde{X}_{i})\right\}^{2}},\]

where the sum runs over a large sample of synthetic observations. With this change in \(\theta(\widehat{P}_{n})\), the previously calculated bias becomes

\[\frac{\sum_{i=1}^{n}\left\{A_{i}-E_{\widehat{P}_{n}}(A|X_{i})\right\}\left\{Y _{i}-E_{\widehat{P}_{n}}(Y|X_{i})\right\}}{\sum_{i=1}^{n}\left\{A_{i}-E_{ \widehat{P}_{n}}(A|X_{i})\right\}^{2}}-\left\{\theta(\widehat{P}_{n})+b\right\}\]

which is zero, and as such the debiasing with respect to term (6) is complete. Based on a new sample of synthetic observations (independent of those generated to calculate the bias), the synthetic data estimator is

\[\theta(\widetilde{P}_{m}) = \frac{\frac{1}{m}\sum_{i=1}^{m}\left\{\widetilde{A}_{i}-E_{\widehat {P}_{m}}(A|\widetilde{X}_{i})\right\}\left[\widetilde{Y}_{i}+b\left\{ \widetilde{A}_{i}-E_{\widehat{P}_{m}}(A|\widetilde{X}_{i})\right\}-E_{\widehat {P}_{m}}(Y|\widetilde{X}_{i})\right]}{\frac{1}{m}\sum_{i=1}^{m}\left\{ \widetilde{A}_{i}-E_{\widehat{P}_{m}}(A|\widetilde{X}_{i})\right\}^{2}}\] \[= \frac{\frac{1}{m}\sum_{i=1}^{m}\left\{\widetilde{A}_{i}-E_{ \widehat{P}_{m}}(A|\widetilde{X}_{i})\right\}\left\{\widetilde{Y}_{i}-E_{ \widehat{P}_{m}}(Y|\widetilde{X}_{i})\right\}}{\frac{1}{m}\sum_{i=1}^{m} \left\{\widetilde{A}_{i}-E_{\widetilde{P}_{m}}(A|\widetilde{X}_{i})\right\}^ {2}}+b.\]

This ensures that bias term (5) equals zero. Please note that this estimator coincides with the standard linear regression estimator on the debiased synthetic data when least squares predictions for \(E_{\widetilde{P}_{m}}(A|\widetilde{X}_{i})\) and \(E_{\widetilde{P}_{m}}(Y|\widetilde{X}_{i})\) are used.

### Derivation of variance of debiased estimator \(\theta(\widetilde{P}_{m})\)

With the suggested debiasing, we thus expect that

\[\theta(\widetilde{P}_{m})-\theta(P) = \frac{1}{m}\sum_{i=1}^{m}\phi(S_{i},\widehat{P}_{n})+\frac{1}{n} \sum_{i=1}^{n}\phi(O_{i},P)+o_{p}(n^{-1/2})+o_{p}(m^{-1/2}).\]

Since the synthetic data are independently drawn from the observed data, any covariance between the 2 leading terms must originate from the fact that \(\widehat{P}_{n}\) depends on the observed data. This covariance equals zero since

\[E\left\{\phi(S_{i},\widehat{P}_{n})\phi(O_{j},P)\right\} = E\left[E\left\{\phi(S_{i},\widehat{P}_{n})|O_{1},...,O_{n}\right\} \phi(O_{j},P)\right]\] \[= E\left[E\left\{\phi(S_{i},\widehat{P}_{n})|\widehat{P}_{n},O_{1 },...,O_{n}\right\}\phi(O_{j},P)\right]\] \[= E\left[E\left\{\phi(S_{i},\widehat{P}_{n})|\widehat{P}_{n} \right\}\phi(O_{j},P)\right]=0\]

where in the second equality we use that \(\widehat{P}_{n}\) is determined by \(O_{1},...,O_{n}\), in the third equality we use that \(S_{i}\) only depends on \(O_{1},...,O_{n}\) via \(\widehat{P}_{n}\), and in the final equality we use that \(\phi(S_{i},\widehat{P}_{n})\) has mean zero when the synthetic data are sampled from \(\widehat{P}_{n}\). This renders these terms asymptotically independent and their sum, hence, asymptotically normal. Moreover, since the variance of \(\phi(S_{i},\widehat{P}_{n})\) converges to \(E\left\{\phi(O,P)^{2}\right\}\) (see the previous section; Appendix A.2), the variance of \(\theta(\widetilde{P}_{m})\) may thus be approximated by

\[\left(\frac{1}{m}+\frac{1}{n}\right)E\left\{\phi(O,P)^{2}\right\},\]

thereby generalising results known for parametric synthetic data generators (Raab et al., 2016; Decruyenaere et al., 2024).

### Simulation study

#### a.7.1 Data generating process

```
input :Requested number of data records \(n\). output :Dataframe \(D\) with \(n\) records, each made up of 4 attributes: \(age\), \(stage\), \(therapy\), \(bp\). \(D\gets Empty\)\(dataframe\) for\(i\gets 1\)to\(n\)do \(age\gets Normal(mean=50,std=10)\) \(\nu_{age}\gets 0.05\) \(\nu_{I},\nu_{II},\nu_{III}\gets 2,3,4\) \(c_{PI},c_{PII},c_{PIII}\leftarrow\) \(Sigmoid(\nu_{I}-\nu_{age}\times age),Sigmoid(\nu_{II}-\nu_{age}\times age),Sigmoid( \nu_{III}-\nu_{age}\times age)\) \(stage\gets Categorical(cat=[I,II,III,IV],probs=[c_{PI},c_{PII}-cp_{I},cp_{III}- c_{PII},1-cp_{III}])\) \(therapy\gets Categorical(cat=[False,True],p=[0.5,0.5])\) \(\beta_{therapy}\leftarrow-20\) \(\beta_{I},\beta_{II},\beta_{III},\beta_{IV}\gets 0,10,20,30\) \(\mu_{bp}\gets 120+\beta_{stage}+\beta_{therapy}\times therapy\) \(bp\gets Normal(mean=\mu_{bp},std=10)\) \(D_{i}\leftarrow\{age,stage,therapy,bp\}\)  end while
```

**Algorithm 1**Data generating process for hypothetical disease.

Inspired by an applied medical setting, we create a hypothetical disease, defined by a low-dimensional tabular data generation process. The dependency structure depicted by the directed acyclic graph (DAG) in Figure 1 in the main text displays the presence of four variables, each of them chosen to obtain a mix of data types. In our hypothetical disease, it is assumed that a patient is observed at a given point in time. At this time, patient data about _age_, atherosclerosis _stage_, and the random assignment of _therapy_ is gathered. The continuous outcome variable _blood pressure_ is evaluated at a later time point, making this design a simplification, since we do not consider the data as longitudinal.

The exact routine to reconstruct this data generating process is presented in the pseudo-code in Algorithm 1. _Age_ (continuous) follows a normal distribution with mean \(50\) and standard deviation \(10\). Atherosclerosis _stage_ (ordinal) was generated according to a proportional odds cumulative logit model where an increase in _age_ causes an increase in the odds of having a _stage_ higher than a given stage \(k\) (\(\nu_{age}=-0.05\) and intercepts \(\nu_{stage}=\{2,3,4\}\) for stage I-III). _Therapy_ (binary) is considered to be 1:1 randomly assigned and is therefore sampled from a Bernouilli distribution with a probability of \(0.50\). The last variable, _blood pressure_ (continuous), is sampled from a normal distribution with standard deviation \(10\) and where the baseline average of \(120\) increases with higher atherosclerosis _stage_ (\(\beta_{stage}=\{0,10,20,30\}\) for stage I-IV, respectively) and absence of _therapy_ (\(\beta_{therapy}=-20\)).

#### a.7.2 Deep Generative Models

We elaborate on the DGMs used to create synthetic data in our simulation study and cases studies. All experiments were run on our institutional high performance computing cluster using a single GPU (NVIDIA Ampere A100; \(80\)GB GPU memory) and single CPU (AMD EPYC 7413), taking less than \(24\) hours to complete (simulation study: less than \(15\) minutes per individual run across \(5\) sample sizes; International Stroke Trial case study: less than \(75\) minutes per individual run across \(5\) sample sizes; Adult Census Income Dataset case study: less than \(4\) hours). We focus on two commonly used DGMs, namely Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) and Variational Autoencoders (VAEs) (Kingma and Welling, 2013).

A GAN consists of two competing neural networks, a generator and discriminator, and aims to achieve an equilibrium between both (Hernandez et al., 2022). This translates to a mini-max game, since the generator aims to minimise the difference between the real and generated data, while the discriminator aims to maximise the possibility to distinguish the real and generated data (Goodfellowet al., 2014). We use the CTGAN implementation that was designed specifically for tabular data, proposed by Xu et al. (2019).

A VAE is a deep latent variable model, consisting of an encoder and a decoder (Kingma and Welling, 2013). The encoder models the approximate posterior distribution of the latent variables given an input instance, whereby typically a standard normal prior is assumed for the latent variables. The decoder allows reconstructing an input instance, based on a sample from the predicted latent space distribution. Encoder and decoder can be jointly trained by maximising the evidence lower bound (ELBO), i.e. the marginal likelihood of the training instances. Maximising the ELBO corresponds to minimising the Kullback-Leibler (KL) divergence between the predicted latent variable distribution for a given input instance and the standard normal priors, and minimising the reconstruction error of the input instance at the decoder output. Once again, we use the tabular implementation of a VAE (TVAE) proposed by Xu et al. (2019).

The results presented in the rest of the Appendix are obtained by training both types of DGM with default hyperparameters as suggested by the packages Synthcity and SDV (for both CTGAN and TVAE). A comparison of the default hyperparameters in both packages is provided in Tables A1 and A2. Note that both packages implement the CTGAN and TVAE modules as originally proposed by Xu et al. (2019), where a cluster-based normaliser is used to preprocess numerical features.

#### a.7.3 Quality of synthetic data

We performed some additional analyses to assess the quality of the synthetic data obtained in our simulation study.

Average IKLD.The inverse of the KL divergence (IKLD) between original and synthetic data, averaged over 250 Monte Carlo runs and standardised between \(0\) and \(1\), is presented in Table A3, where the debiased synthetic datasets have slightly higher IKLD than their default versions for all DGMs considered.

[MISSING_PAGE_FAIL:22]

et al. (2024). While our debiasing strategy clearly improves the empirical coverage for the population regression coefficient, it does, however, not guarantee this to be at the nominal level for all sample sizes and DGMs considered. In particular, debiasing only seems to achieve coverage at the nominal level for \(\mathtt{TVAE}\) (Synthcity) across all sample sizes and for \(\mathtt{TVAE}\) (SDV) at sufficiently large sample sizes. Our approach still falls short for \(\mathtt{CTGAN}\) (Synthcity) and \(\mathtt{CTGAN}\) (SDV), although providing more honest inference than the default synthetic datasets.

Bias and SE for mean _age_.As shown in Figure A3, the sample mean of _age_ is unbiased in the default synthetic datasets, but exhibits large variability that shrinks slowly with sample size due to the data-adaptive nature of DGMs. Debiasing reduces this variability and accelerates shrinkage. Figure A4 indicates that the empirical SE for the sample mean of _age_ is indeed, on average, underestimated by the MLE-based SE in default synthetic datasets. After debiasing, the average MLE-based SE approximates the empirical SE, albeit with minor deviations at smaller sample sizes.

Bias and SE for effect _therapy_ on _blood pressure_ adjusted for _stage_.It can be seen from Figure A5 that the linear regression coefficient of _therapy_ on _blood pressure_ adjusted for _stage_ in the default synthetic datasets has finite-sample bias towards the null that converges to zero as the sample size grows larger. Additionally, its variability seems to diminish slower than expected. Debiasing reduces the finite-sample bias and improves the shrinkage of the SE. However, Figure A6 shows that the average MLE-based SE in the debiased synthetic datasets still underestimates the empirical SE with \(\mathtt{CTGAN}\) (SDV) and \(\mathtt{CTGAN}\) (Synthcity) despite debiasing, albeit less pronounced.

Figure A4: The empirical and MLE-based standard error for the sample mean of _age_.

Figure A5: The horizontal dashed line represents the population effect of _therapy_ on _blood pressure_ adjusted for _stage_ and each dot is a MLE-based estimate per Monte Carlo run (250 dots in total per value of \(n\)). The dashed funnel indicates the behaviour of an unbiased and \(\sqrt{n}\)-consistent estimator based on observed data.

[MISSING_PAGE_FAIL:25]

(Syntchity) and, to a lesser extent, for CTGAN (SDV) and TVAE (Syntchity). Nevertheless, a root-\(n\) consistent estimator was still obtained after debiasing these DGMs (see Table A4).

Summary.Table A7 summarises the effect of our debiasing strategy on bias, SE and coverage in the simulation study. Our strategy results in uniformly valid coverage for the population mean, allowing for honest inference. For the regression coefficient, coverage was clearly improved but may remain anti-conservative for some DGMs. This may originate from residual overfitting bias inherent to these DGMs that could not be removed since (efficient) sample splitting was not performed (see Appendix A.3).

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & & \multicolumn{2}{c}{**Mean age**} & \multicolumn{3}{c}{**Effect therapy**} \\ \cline{2-7}
**Model** & **Bias** & **SE** & **Coverage** & **Bias** & **SE** & **Coverage** \\ \hline
**CTGAN (SDV)** & unbiased & root-\(n\) \& nominal & bias at & root-\(n\) \& anti-conser- & \\  & unbiased & at all \(n\) & small \(n\) & underestimated & active at all \(n\) \\
**CTGAN (Syntchity)** & unbiased & root-\(n\) \& nominal & & root-\(n\) \& anti-conser- & \\  & unbiased & at all \(n\) & unbiased & underestimated & active at all \(n\) \\
**TVAE (SDV)** & unbiased & root-\(n\) \& nominal & bias at & root-\(n\) \& nominal only & \\  & unbiased & at all \(n\) & small \(n\) & unbiased & at large \(n\) \\
**TVAE (Syntchity)** & unbiased & root-\(n\) \& nominal & & root-\(n\) \& nominal & \\  & unbiased & at all \(n\) & unbiased & & unbiased & at all \(n\) \\ \hline \hline \end{tabular}
\end{table}
Table A7: Behaviour of estimators in debiased synthetic data in the simulation study.

[MISSING_PAGE_FAIL:27]

Figure A9: The horizontal dashed line represents the population parameter and each dot is a maximum likelihood estimation (MLE)-based or efficient influence curve (EIC)-based estimate per Monte Carlo run (250 dots in total per value of \(n\)). The dashed funnel indicates the behaviour of an unbiased and \(\sqrt{n}\)-consistent estimator based on observed data.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Estimator** & **CTGAN (SDV)** & **CTGAN (Synthecity)** & **TVAE (SDV)** & **TVAE (Synthecity)** \\ \hline \multicolumn{4}{c}{**Default synthetic datasets**} \\ Mean age (MLE) & 0.33 [0.04; 0.62] & 0.01 [-0.16; 0.18] & 0.10 [-0.13; 0.32] & 0.21 [0.04; 0.39] \\ Mean age (EIC) & 0.33 [0.04; 0.62] & 0.01 [-0.16; 0.18] & 0.10 [-0.13; 0.32] & 0.21 [0.04; 0.39] \\ Effect therapy (MLE) & 0.23 [-0.09; 0.56] & 0.31 [0.22; 0.40] & 0.28 [0.10; 0.46] & 0.09 [-0.13; 0.31] \\ Effect therapy (EIC) & 0.33 [-0.11; 0.78] & 0.31 [0.24; 0.38] & 0.30 [0.13; 0.48] & 0.10 [-0.13; 0.32] \\ \hline \multicolumn{4}{c}{**Debiased synthetic datasets**} \\ Mean age (MLE) & 0.42 [0.36; 0.48] & 0.50 [0.46; 0.54] & 0.45 [0.44; 0.47] & 0.47 [0.44; 0.50] \\ Mean age (EIC) & 0.42 [0.36; 0.48] & 0.50 [0.46; 0.54] & 0.45 [0.44; 0.47] & 0.47 [0.44; 0.50] \\ Effect therapy (MLE) & 0.58 [0.43; 0.73] & 0.43 [0.31; 0.55] & 0.61 [0.43; 0.79] & 0.46 [0.38; 0.55] \\ Effect therapy (EIC) & 0.63 [0.50; 0.75] & 0.43 [0.32; 0.54] & 0.64 [0.48; 0.79] & 0.46 [0.38; 0.55] \\ \hline \hline \end{tabular}
\end{table}
Table A8: Estimated exponent \(a\) [\(95\)% CI] for the power law convergence rate \(n^{-a}\) for empirical SE of the maximum likelihood estimation (MLE)-based and efficient influence curve (EIC)-based estimators.

Figure A11: Convergence rate of the empirical standard error (SE) for the maximum likelihood estimation (MLE)-based and efficient influence curve (EIC)-based estimators. If the SE is of the form \(\text{SE}=cn^{-a}\), where \(c\) is a constant, then \(\log\left(SE\right)=\log(c)+(-a)\log\left(n\right)\). Therefore slope \(a\) represents the convergence rate and the vertical offset \(\log(c)\) indicates the log asymptotic variance. The dashed line indicates the behaviour of the SE of an unbiased and \(\sqrt{n}\)-consistent estimator based on observed data, whereas the dotted line indicates the assumed behaviour of the SE of the same estimator based on synthetic data, following the correction proposed by Raab et al. (2016).

### Case studies

#### a.8.1 International Stroke Trial

We adapt the framework discussed in Section 4.1 to the International Stroke Trial (IST), one of the biggest randomised trials in acute stroke research (Sandercock et al., 2011). The dataset with \(19285\) complete cases now constitutes our _population_. We mimic different hypothetical settings where an institution only has access to a limited _sample_ of observations, with the sample size \(n\) varying between \(50\) and \(5000\).

In order to easily share the data with other researchers, the institution generates a synthetic dataset with sample size \(m\), where \(m=n\). Similarly to the simulation study, we repeated this process \(100\) times per sample size \(n\), to be able to calculate the empirical coverage levels. For illustration purposes, we focus on the effect of aspirin on the outcome at \(6\) months and report the proportion of deaths for the two treatment arms (aspirin and no aspirin), and its corresponding risk difference.

For each value of \(n\), two default synthetic datasets were generated using both CTGAN and TVAE. Given the interest in the proportion of death in the group with and without aspirin, we use the debiasing strategy with respect to the population mean. This implies that the default synthetic dataset was first split by treatment and then debiased with relation to the population mean within each treatment arm. The two debiased subdatasets were then afterwards combined into one debiased synthetic dataset for each generative model. For both the default and debiased synthetic dataset, the sampling variability of synthetic data is acknowledged by inflating the standard errors (SEs) by the correction factor \(\sqrt{1+m/n}\).

The funnel plots for the proportion of deaths in both treatment arms and the risk difference are shown in Figure A12. We noticed that using the same hyperparameters as in the simulation study resulted in biased estimates, as can be seen in Figure A12. For this reason, we highlight the results obtained by training with the default hyperparameters suggested by the package SDV (Patki et al., 2016) instead. Analogously to the simulation study, our debiasing strategy decreases the variance of the mean estimator in both treatment arms, remedying the slower-than-\(\sqrt{n}\)-convergence observed in the default synthetic datasets. The impact for the applied researcher can be better understood by looking at the empirical coverage levels of the \(95\%\) CI for the true proportion of deaths in the aspirin arm, for all sample sizes and DGMs considered. Figure A13a illustrates that in contrast to the default synthetic datasets, the coverage levels based on the debiased synthetic datasets are all positioned around the nominal level.

One of the original research questions in Sandercock et al. (2011) was whether or not there is a difference in risk of death between the treatment arms. Figure A13b depicts the empirical type 1 error rate for the risk difference in death between aspirin and no aspirin group based on original data, default and debiased synthetic data. For the aforementioned reason, we focus on the results obtained by training with the default hyperparameters suggested by the package SDV (Patki et al., 2016). Should the researcher use the default synthetic data, they would very often falsely conclude that the risk is significantly different from the true difference of \(-0.009\), as calculated based on our population (the full dataset), while using the debiased synthetic dataset basically eliminates this high number of false-positives, as is the case in the original data as well.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Estimator** & **Original** & **CTGAN (SDV)** & **CTGAN (Synthectiv)** & **TVAE (SDV)** & **TVAE (Synthectiv)** \\ \hline \multicolumn{5}{c}{**Default synthetic datasets**} \\ Proportion death aspirin group & 0.54 [0.50; 0.59] & 0.02 [-0.06; 0.11] & Nau [Nau; Nau] & 0.23 [-0.17; 0.62] & Nau [Nau; Nau] \\ Proportion death no aspirin group & 0.54 [0.52; 0.57] & 0.02 [-0.15; 0.18] & Nau [Nau; Nau] & 0.23 [-0.14; 0.61] & Nau [Nau; Nau] \\ Risk difference death & 0.54 [0.51; 0.56] & 0.01 [-0.24; 0.26] & Nau [Nau; Nau] & 0.46 [0.14; 0.78] & Nau [Nau; Nau] \\ \hline \multicolumn{5}{c}{**Debiased synthetic datasets**} \\ Proportion death aspirin group & - & 0.54 [0.48; 0.61] & 0.59 [0.52; 0.67] & 0.53 [050; 0.56] & 0.60 [0.51; 0.70] \\ Proportion death no aspirin group & - & 0.52 [0.46; 0.59] & 0.58 [0.55; 0.62] & 0.53 [0.49; 0.58] & 0.58 [0.49; 0.67] \\ Risk difference death & - & 0.55 [0.48; 0.62] & 0.57 [0.53; 0.60] & 0.53 [0.50; 0.57] & 0.57 [0.53; 0.61] \\ \hline \hline \end{tabular}
\end{table}
Table A9: Estimated exponent \(a\) [\(95\)% CI] for the power law convergence rate \(n^{-a}\) for empirical SE. Note that a convergence rate could not be estimated for the default synthetic data when hyperparameters suggested by the package Synthcity. This occurred because there was no variance in the estimates for sample sizes of \(1600\) and \(5000\).

Figure A12: Estimates for the proportion of death in both treatment arms and their corresponding risk difference estimates. We show results for original data, and both default synthetic data (left) and debiased synthetic data (right) for all four generators. The horizontal dashed line represents the population proportion of death in each group and the corresponding risk difference, and each dot is an estimate per Monte Carlo run (100 dots in total per value of n). The dashed funnel indicates the behaviour of an unbiased and \(\sqrt{n}\)-consistent estimator based on observed data.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We state why our research is relevant (bias in synthetic data generated by deep generative models, leading to imprecision in statistical analysis and wrong conclusions), and propose a new strategy to debias this synthetic data. While this methodology is general, we clearly state that we apply it on two Deep Generative Models (CTGAN and TVAE), in a simulation study and two case studies to convince the reader of its merits. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We list limitations that are related to several facets of this paper. We refer to the assumption that is needed for the formal derivation of our strategy but frame this in the context of well-known analyses. We also discuss the low-dimensional setting of our simulation and case studies, for which DGMs might be less suited. Nevertheless, the positive results for two widely used estimators in this simple setting highlights the utility of a debiased approach and is simultaneously encouraging in terms of future larger-scale applications. In contrast to other work, we are aware that we do not suggest a tuning strategy of the DGM but instead rely on the debiasing of the generated synthetic data in a post-processing step. Nonetheless, we perceive this as a strength, since it renders our strategy generator-agnostic. However, the debiasing method of the regression coefficient still requires sampling of synthetic data conditional on a covariate, which is not a given in all types of DGM. Finally, when multiple parameters are of interest, the data generated by the DGM will need to be debiased to ensure that several such restrictions hold (simultaneously) w.r.t. several EICs \(\phi(.)\), which is left for future research. In the Discussion section, we offer various suggestions and ideas for future research aimed at addressing the limitations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: In Section 2 we clearly present our set up and its assumptions. Section 3 contains a detailed theoretical proof, including reasoning steps along the way, of our debiasing strategy, complemented by additional notes in the supplementary material (referred to as 'Appendix' in the manuscript). Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Besides fully describing our debiasing methodology, we also provide ample details on the simulation study and case studies. Moreover, we have made our code for the simulation and case studies available online through our GitHub page, with the link included in the main text. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The simulation study is built on a fictitious yet realistic data generating process, which is provided in full in Appendix A.7.1. As DGMs, we use the default implementations provided in the code libraries Synthcity and SDV, which are open-source. The case studies are based on two public datasets: the International Stroke Trial dataset (Sandercock et al., 2011) and the Adult Census Income dataset (Becker and Kohavi, 1996). While we aimed to provide all details necessary to reproduce our experiments in the paper itself, we additionally make all code available in a public Github repository (the link is included in the main text). These code enables reproduction of all results for the simulation study and case studies. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?Answer: [Yes] Justification: Since the focus is not on building novel generative models, but rather on proposing a general methodology for debiasing the data generated by a generic generator, we simply use default hyperparameters for the DGMs as suggested by two code libraries (SynthCity and SDV). In our simulation study, our training data is obtained by sampling from the data generating process. We do not use any test splits, since the evaluation is done in terms of utility of statistical analyses, where we compare the obtained estimands with their ground truth values. Similar considerations are valid for the case studies. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: While we do include experimental results, we do not compare multiple generative models or methods. Rather, we present a novel methodology for debiasing synthetic data, and use various experiments to verify that it works as expected, by comparing the statistical utility of synthetic data with and without our debiasing strategy. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All experiments were run on our institutional high performance computing cluster using a single GPU (NVIDIA Ampere A100; \(80\)GB GPU memory) and single CPU (AMD EPYC 7413), taking less than \(24\) hours to complete (simulation study: less than \(15\) minutes per individual run across \(5\) sample sizes; International Stroke Trial case study: less than \(75\) minutes per individual run across \(5\) sample sizes; Adult census Income Dataset case study: less than \(4\) hours). This is stated as such in the appendix.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: This research did not involve human subjects or participants. Furthermore, there are no data-related concerns: the simple data generating process that is proposed in the simulation study is fictitious, and apart from that we use public datasets. There are potential harmful consequences related to synthetic data, such as privacy disclosure risk and incorrect statistical inference from tabular synthetic datasets. In fact, our work addresses the second risk by debiasing synthetic datasets towards improving their inferential utility. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Positive societal impacts stem from the increased reliability of statistical analyses on synthetic data, which is made possible by our debiasing method. We do not foresee major negative societal impacts of our method. General concerns regarding tabular synthetic data do apply, though we believe our method remedies part of them (see our answer to Question 9). Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not believe our paper poses such risks, since we propose a generic debiasing strategy for synthetic data, without releasing any new models or datasets. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: In the paper, we credit the creators of the deep generative models used in our work (i.e. CTGAN and TVAE), as well as the open source code libraries whose implementations we used (Synthecity and SDV). Furthermore, we cite the sources of the public datasets we used. The International Stroke Trial exists under an Open Database License, and the Adult Census Income dataset exists under a Creative Commons Attribution 4.0 International License. Both are free to use, but require attribution, which we provided. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA] Justification: We do not release any new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.