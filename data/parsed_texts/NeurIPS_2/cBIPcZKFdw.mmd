# Strategic Apple Tasting

 Keegan Harris

Carnegie Mellon University

Pittsburgh, PA 15213

keeganh@cmu.edu &Chara Podimata

MIT & Archimedes/Athena RC

Cambridge, MA 02142

podimata@mit.edu &Zhiwei Steven Wu

Carnegie Mellon University

Pittsburgh, PA 15213

zstevenwu@cmu.edu

###### Abstract

Algorithmic decision-making in high-stakes domains often involves assigning _decisions_ to agents with _incentives_ to strategically modify their input to the algorithm. In addition to dealing with incentives, in many domains of interest (e.g. lending and hiring) the decision-maker only observes feedback regarding their policy for rounds in which they assign a positive decision to the agent; this type of feedback is often referred to as _apple tasting_ (or _one-sided_) feedback. We formalize this setting as an online learning problem with apple-tasting feedback where a _principal_ makes decisions about a sequence of \(T\)_agents_, each of which is represented by a _context_ that may be strategically modified. Our goal is to achieve sublinear _strategic regret_, which compares the performance of the principal to that of the best fixed policy in hindsight, _if the agents were truthful when revealing their contexts_. Our main result is a learning algorithm which incurs \(\tilde{\mathcal{O}}(\sqrt{T})\) strategic regret when the sequence of agents is chosen _stochastically_. We also give an algorithm capable of handling _adversarially-chosen_ agents, albeit at the cost of \(\tilde{\mathcal{O}}(T^{(d+1)/(d+2)})\) strategic regret (where \(d\) is the dimension of the context). Our algorithms can be easily adapted to the setting where the principal receives _bandit_ feedback--this setting generalizes both the linear contextual bandit problem (by considering agents with incentives) and the strategic classification problem (by allowing for partial feedback).

## 1 Introduction

Algorithmic systems have recently been used to aid in or automate decision-making in high-stakes domains (including lending and hiring) in order to, e.g., improve efficiency or reduce human bias [12, 1]. When subjugated to algorithmic decision-making in high-stakes settings, individuals have an incentive to _strategically_ modify their observable attributes to appear more qualified. Such behavior is often observed in practice. For example, credit scores are often used to predict the likelihood an individual will pay back a loan on time if given one. Online articles with titles like _"9 Ways to Build and Improve Your Credit Fast"_ are ubiquitous and offer advice such as "pay credit card balances strategically" in order to improve one's credit score with minimal effort [46]. In hiring, common advice ranges from curating a list of keywords to add to one's resume, to using white font in order to "trick" automated resume scanning software [23, 2]. If left unaccounted for, such strategic manipulations could result in individuals being awarded opportunities for which they are not qualified for, possibly at the expense of more deserving candidates. As a result, it is critical to keep individuals' incentives in mind when designing algorithms for learning and decision-making in high-stakes settings.

In addition to dealing with incentives, another challenge of designing learning algorithms for high-stakes settings is the possible _selection bias_ introduced by the way decisions are made. In particular, decision-makers often only have access to feedback about the deployed policy from individuals that have received positive decisions (e.g., the applicant is given the loan, the candidate is hired to the job and then we can evaluate how good our decision was). In the language of online learning, this type of feedback is known as _apple tasting_ (or _one-sided_) feedback. _When combined, these two complications (incentives & one-sided feedback) have the potential to amplify one other, as algorithms can learnonly when a positive decision is made, but individuals have an incentive to strategically modify their attributes in order to receive such positive decisions, which may interfere with the learning process._

### Contributions

We formalize our setting as a game between a _principal_ and a sequence of \(T\)_strategic agents_, each with an associated _context_\(\mathbf{x}_{t}\) which describes the agent. At every time \(t\in\{1,\ldots,T\}\), the principal deploys a _policy_\(\pi_{t}\), a mapping from contexts to binary _decisions_ (e.g., whether to accept/reject a loan applicant). Given policy \(\pi_{t}\), agent \(t\) then presents a (possibly modified) context \(\mathbf{x}_{t}^{\prime}\) to the algorithm, and receives a decision \(a_{t}=\pi_{t}(\mathbf{x}_{t}^{\prime})\). If \(a_{t}=1\), the principal observes _reward_\(r_{t}(a_{t})=r_{t}(1)\); if \(a_{t}=0\) they receive no feedback. \(r_{t}(0)\) is assumed to be known and constant across rounds.1 Our metric of interest is _strategic regret_, i.e., regret with respect to the best fixed policy in hindsight, _if agents were truthful when reporting their contexts_.

Footnote 1: We relax this assumption at later parts of the paper with virtually no impact on our results.

Our main result is an algorithm which achieves \(\tilde{O}(\sqrt{T})\) strategic regret (with polynomial per-round runtime) when there is sufficient randomness in the distribution over agents (Algorithm 1). At a high level, our algorithm deploys a linear policy at every round which is appropriately shifted to account for the agents' strategic behavior. We identify a _sufficient_ condition under which the data received by the algorithm at a given round is "clean", i.e. has not been strategically modified. Algorithm 1 then online-learns the relationship between contexts and rewards by only using data for which it is sure is clean.

In contrast to performance of algorithms which operate in the non-strategic setting, the regret of Algorithm 1 depends on an exponentially-large constant \(c(d,\delta)\approx(1-\delta)^{-d}\) due to the one-sided feedback available for learning, where \(d\) is the context dimension and \(\delta\in(0,1)\) is a parameter which represents the agents' ability to manipulate. While this dependence on \(c(d,\delta)\) is insignificant when the number of agents \(T\to\infty\) (i.e. is very large), it may be problematic for the principal whenever \(T\) is either small or unknown. To mitigate this issue, we show how to obtain \(\tilde{O}(d\cdot T^{2/3})\) strategic regret by playing a modified version of the well-known _explore-then-commit_ algorithm (Algorithm 2). At a high level, Algorithm 2 "explores" by always assigning action \(1\) for a fixed number of rounds (during which agents do not have an incentive to strategize) in order to collect sufficient information about the data-generating process. It then "exploits" by using this data learn a strategy-aware linear policy. Finally, we show how to combine Algorithm 1 and Algorithm 2 to achieve \(\tilde{O}(\min\{c(d,\delta)\cdot\sqrt{T},d\cdot T^{2/3}\})\) strategic regret whenever \(T\) is unknown.

While the assumption of stochastically-chosen agents is well-motivated in general, it may be overly restrictive in some specific settings. Our next result is an algorithm which obtains \(\tilde{O}(T^{(d+1)/(d+2)})\) strategic regret when agents are chosen _adversarially_ (Algorithm 4). Algorithm 4 uses a variant of the popular Exp3 algorithm to trade off between a carefully constructed set of (exponentially-many) policies [6]. As a result, it achieves sublinear strategic regret when agents are chosen adversarially, but requires an exponentially-large amount of computation at every round.

Finally, we note that while our primary setting of interest is that of one-sided feedback, all of our algorithms can be easily extended to the more general setting in which the principal receives _bandit feedback_ at each round, i.e. \(r_{t}(0)\) is not constant and must be learned from data. To the best of our knowledge, we are the first to consider strategic learning in the contextual bandit setting.

### Related work

**Strategic responses to algorithmic decision-making** There is a growing line of work at the intersection of economics and computation on algorithmic decision-making with incentives, under the umbrella of _strategic classification_ or _strategic learning_[27, 3, 41, 22] focusing on online learning settings [21, 19], causal learning [52, 31, 29, 34], incentivizing desirable behavior [39, 28, 11, 42], incomplete information [30, 25, 37]. In its most basic form, a principal makes either a binary or real-valued prediction about a strategic agent, and receives _full feedback_ (e.g., the agent's _label_) after the decision is made. While this setting is similar to ours, it crucially ignores the one-sided feedback structure present in many strategic settings of interest. In our running example of hiring, full feedback would correspond to a company not offering an applicant a job, and yet still getting to observe whether they would have been a good employee! As a result, such methods are not applicable in our setting. Concurrent work [18] studies the effects of bandit feedback in the related problem of _performative prediction_[47], which considers data distribution shifts at the _population level_ in response to the deployment of a machine learning model. In contrast, our focus is on strategic responses to machine learning models at the _individual level_ under apple tasting and bandit feedback. Ahmadi et al. [4] study an online strategic learning problem in which they consider "bandit feedback" _with respect to the deployed classifier_. In contrast, we use the term "bandit feedback" to refer to the fact that we only see the outcome when for the action/decision taken.

**Apple tasting and online learning** Helmbold et al. [32] introduce the notion of apple-tasting feedback for online learning. In particular, they study a binary prediction task over "instances" (e.g., fresh/rotten apples), in which a positive prediction is interpreted as accepting the instance (i.e. "tasting the apple") and a negative prediction is interpreted as rejecting the instance (i.e., _not_ tasting the apple). The learner only gets feedback when the instance is accepted (i.e., the apple is tasted). While we are the first to consider classification under incentives with apple tasting feedback, similar feedback models have been studied in the context of algorithmic fairness [9], partial-monitoring games [5], and recidivism prediction [24]. A related model of feedback is that of _contaminated controls_[40], which considers learning from (1) a treated group which contains only _treated_ members of the agent population and (2) a "contaminated" control group with samples from the _entire_ agent population (not just those under _control_). Technically, our results are also related to a line of work in contextual bandits which shows that greedy algorithms without explicit exploration can achieve sublinear regret as long as the underlying context distribution is sufficiently diverse [49, 8, 38, 53, 48].

**Bandits and agents** A complementary line of work to ours is that of _Bayesian incentive-compatible_ (BIC) exploration in multi-armed bandit problems [43, 35, 50, 36, 44, 45]. Under such settings, the goal of the principal is to _persuade_ a sequence of \(T\) agents with incentives to explore across several different actions with bandit feedback. In contrast, in our setting it is the principal, not the agents, who is the one taking actions with partial feedback. As a result there is no need for persuasion, but the agents now have an incentive to strategically modify their behavior in order to receive a more desirable decision/action.

**Other related work** Finally, our work is broadly related to the literature on learning in repeated Stackelberg games [7, 55], online Bayesian persuasion [16, 17, 13], and online learning in principal-agent problems [20, 33, 56]. In the repeated Stackelberg game setting, the principal (leader) commits to a mixed strategy over a finite set of actions, and the agent (follower) best-responds by playing an action from a finite set of best-responses. Unlike in our setting, both the principal's and agent's payoffs can be represented by matrices. In contrast, in our setting the principal commits to a pure strategy from a continuous set of actions, and the agent best-responds by playing an action from a continuous set. In online Bayesian persuasion, the principal (sender) commits to a "signaling policy" (a random mapping from "states of the world" to receiver actions) and the agent (receiver) performs a posterior update on the state based on the principal's signal, then takes an action from a (usually finite) set. In both this setting and ours, the principal's action is a policy. However in our setting the policy is a linear decision rule, whereas in the Bayesian persuasion setting, the policy is a set of conditional probabilities which form an "incentive compatible" signaling policy. This difference in the policy space for the principal typically leads to different algorithmic ideas being used in the two settings. Strategic learning problems are, broadly speaking, instances of principal-agent problems. In contract design, the principal commits to a contract (a mapping from "outcomes" to agent payoffs). The agent then takes an action, which affects the outcome. In particular, they take the action which maximizes their expected payoff, subject to some cost of taking the action. The goal of the principal is to design a contract such that their own expected payoff is maximized. While the settings are indeed similar, there are several key differences. First, in online contract design the principal always observes the outcome, whereas in our setting the principal only observes the reward if a positive decision is made. Second, the form of the agent's best response is different, which leads to different agent behavior and, as a result, different online algorithms for the principal.

## 2 Setting and background

We consider a game between a _principal_ and a sequence of \(T\)_agents_. Each agent is associated with a _context_\(\mathbf{x}_{t}\in\mathcal{X}\subseteq\mathbb{R}^{d}\), which characterizes their attributes (e.g., a loan applicant's credit history/report). At time \(t\), the principal commits to a _policy_\(\pi_{t}:\mathcal{X}\rightarrow\{1,0\}\), which maps from contexts to binary _decisions_ (e.g., whether to accept/reject the loan application). We use \(a_{t}=1\) to denote the the principal's positive decision at round \(t\) (e.g., agent \(t\)'s loan application is approved), and \(a_{t}=0\) to denote a negative decision (e.g., the loan application is rejected). Given \(\pi_{t}\), agent \(t\)_best-responds_ by strategically modifying their context within their _effort budget_ as follows:

**Definition 2.1** (Agent best response; lazy tiebreaking).: _Agent \(t\) best-responds to policy \(\pi_{t}\) by modifying their context according to the following optimization program._

\[\mathbf{x}^{\prime}_{t}\in \arg\max_{\mathbf{x}^{\prime}\in\mathcal{X}}\ \mathbb{1}\{\pi_{t}( \mathbf{x}^{\prime})=1\}\] \[s.t. \|\mathbf{x}^{\prime}-\mathbf{x}_{t}\|_{2}\leq\delta\]

_Furthermore, we assume that if an agent is indifferent between two (modified) contexts, they choose the one which requires the least amount of effort to obtain (i.e., agents are lazy when tiebreaking)._

In other words, every agent wants to receive a positive decision, but has only a limited ability to modify their (initial) context (represented by \(\ell_{2}\) budget \(\delta\)).2 Such an effort budget may be induced by time or monetary constraints and is a ubiquitous model of agent behavior in the strategic learning literature (e.g., [39, 28, 19, 10]). We focus on _linear thresholding policies_ where the principal assigns action \(\pi(\mathbf{x}^{\prime})=1\), if and only if \(\langle\mathbf{\beta},\mathbf{x}^{\prime}\rangle\geq\gamma\) for some \(\mathbf{\beta}\in\mathbb{R}^{d}\), \(\gamma\in\mathbb{R}\). We refer to \(\langle\mathbf{\beta},\mathbf{x}^{\prime}_{t}\rangle=\gamma\) as the _decision boundary_. For linear thresholding policies, the agent's best-response according to Definition 2.1 is to modify their context in the direction of \(\mathbf{\beta}/\|\mathbf{\beta}\|_{2}\) until the decision-boundary is reached (if it can indeed be reached). While we present our results for _lazy tiebreaking_ for ease of exposition, all of our results can be readily extended to the setting in which agents best-respond with a "trembling hand", i.e. _trembling hand tiebreaking_. Under this setting, we allow agents who strategically modify their contexts to "overshoot" the decision boundary by some bounded amount, which can be either stochastic or adversarially-chosen. See Appendix D for more details.

Footnote 2: Our results readily extend to the setting in which the agent’s effort constraint takes the form of an ellipse rather than a sphere. Under this setting, the agent effort budget constraint in Definition 2.1 would be \(\|A^{1/2}(\mathbf{x}^{\prime}-\mathbf{x}_{t})\|_{2}\leq\delta\), where \(A\in\mathbb{R}^{d\times d}\) is some positive definite matrix. If \(A\) is known to the principal, this may just be viewed as a linear change in the feature representation.

The principal observes \(\mathbf{x}^{\prime}_{t}\) and plays action \(a_{t}=\pi_{t}(\mathbf{x}^{\prime}_{t})\) according to policy \(\pi_{t}\). If \(a_{t}=0\), the principal receives some known, _constant_ reward \(r_{t}(0):=r_{0}\in\mathbb{R}\). On the other hand, if the principal assigns action \(a_{t}=1\), we assume that the reward the principal receives is linear in the agent's _unmodified_ context, i.e.,

\[r_{t}(1):=\langle\mathbf{\theta}^{(1)},\mathbf{x}_{t}\rangle+\epsilon_{t} \tag{1}\]

for some _unknown_\(\mathbf{\theta}^{(1)}\in\mathbb{R}^{d}\), where \(\epsilon_{t}\) is i.i.d. zero-mean sub-Gaussian random noise with (known) variance \(\sigma^{2}\). Note that \(r_{t}(1)\) is observed _only_ when the principal assigns action \(a_{t}=1\), and _not_ when \(a_{t}=0\). Following Helmbold et al. [32], we refer to such feedback as _apple tasting_ (or _one-sided_) feedback. Mapping to our lending example, the reward a bank receives for rejecting a particular loan applicant is the same across all applicants, whereas their reward for a positive decision could be anywhere between a large, negative reward (e.g., if a loan is never repaid) to a large, positive reward (e.g., if the loan is repaid on time, with interest).

The most natural measure of performance in our setting is that of _Stackelberg regret_, which compares the principal's reward over \(T\) rounds with that of the optimal policy _given that agents strategic_.

**Definition 2.2** (Stackelberg regret).: _The Stackelberg regret of a sequence of policies \(\{\pi_{t}\}_{t\in[T]}\) on agents \(\{\mathbf{x}_{t}\}_{t\in[T]}\) is_

\[\operatorname{Reg}_{\mathtt{Stackel}}(T):=\sum_{t\in[T]}r_{t}(\tilde{\pi}^{*} (\tilde{\mathbf{x}}_{t}))-\sum_{t\in[T]}r_{t}(\pi_{t}(\mathbf{x}^{\prime}_{t}))\]

_where \(\tilde{\mathbf{x}}_{t}\) is the best-response from agent \(t\) to policy \(\tilde{\pi}^{*}\) and \(\tilde{\pi}^{*}\) is the optimal-in-hindsight policy, given that agents best-respond according to Definition 2.1._

A stronger measure of performance is that of _strategic regret_, which compares the principal's reward over \(T\) rounds with that of the optimal policy _had agents reported their contexts truthfully_.

**Definition 2.3** (Strategic regret).: _The strategic regret of a sequence of policies \(\{\pi_{t}\}_{t\in[T]}\) on agents \(\{\mathbf{x}_{t}\}_{t\in[T]}\) is_

\[\operatorname{Reg}_{\mathtt{strat}}(T):=\sum_{t\in[T]}r_{t}(\pi^{*}(\mathbf{x} _{t}))-\sum_{t\in[T]}r_{t}(\pi_{t}(\mathbf{x}^{\prime}_{t}))\]

_where \(\pi^{*}(\mathbf{x}_{t})=1\) if \(\langle\mathbf{\theta}^{(1)},\mathbf{x}_{t}\rangle\geq r_{0}\) and \(\pi^{*}(\mathbf{x}_{t})=0\) otherwise._

[MISSING_PAGE_FAIL:5]

_for exploration_; a growing area of interest in the online learning literature (see references in Section 1.2). Moreover, such assumptions often hold in practice. For example, in the related problem of (non-strategic) contextual bandits (we will later show how our results extend to the strategic version of this problem), Bietti et al. [14] find that a greedy algorithm with no explicit exploration achieved the second-best empirical performance across a large number of datasets when compared to many popular contextual bandit algorithms. In our settings of interest (e.g. lending, hiring), such an assumption is reasonable if there is sufficient diversity in the applicant pool. In Section 4 we show how to remove this assumption, albeit at the cost of worse regret rates and exponential computational complexity.

At a high level, our algorithm (formally stated in Algorithm 1) relies on three key ingredients to achieve sublinear strategic regret:

1. A running estimate of \(\mathbf{\theta}^{(1)}\) is used to compute a linear policy, which separates agents who receive action \(1\) from those who receive action \(0\). Before deploying, we shift the decision boundary by the effort budget \(\delta\) to account for the agents strategizing.
2. We maintain an estimate of \(\mathbf{\theta}^{(1)}\) (denoted by \(\widehat{\mathbf{\theta}}^{(1)}\)) and only updating it when \(a_{t}=1\) and we can ensure that \(\mathbf{x}_{t}^{\prime}=\mathbf{x}_{t}\).
3. We assign actions "greedily" (i.e. using no explicit exploration) w.r.t. the shifted linear policy.

**Shifted linear policy** If agents were _not_ strategic, assigning action \(1\) if \(\langle\widehat{\mathbf{\theta}}^{(1)}_{t},\mathbf{x}_{t}\rangle\geq r_{0}\) and action \(0\) otherwise would be a reasonable strategy to deploy, given that \(\widehat{\mathbf{\theta}}^{(1)}_{t}\) is our "best estimate" of \(\mathbf{\theta}^{(1)}\) so far. Recall that the strategically modified context \(\mathbf{x}_{t}^{\prime}\) is s.t., \(\|\mathbf{x}_{t}^{\prime}-\mathbf{x}_{t}\|\leq\delta\). Hence, in Algorithm 1, we shift the linear policy by \(\delta\|\widehat{\mathbf{\theta}}^{(1)}\|_{2}\) to account for strategically modified contexts. Now, action \(1\) is only assigned if \(\langle\widehat{\mathbf{\theta}}^{(1)}_{t},\mathbf{x}_{t}\rangle\geq\delta\| \widehat{\mathbf{\theta}}^{(1)}\|_{2}+r_{0}\). This serves two purposes: (1) It makes it so that any agent with unmodified context \(\mathbf{x}\) such that \(\langle\widehat{\mathbf{\theta}}^{(1)}_{t},\mathbf{x}\rangle<r_{0}\) cannot receive action \(1\), no matter how they strategize. (2) It forces some agents with contexts in the band \(r_{0}\leq\langle\widehat{\mathbf{\theta}}^{(1)}_{t},\mathbf{x}\rangle<\delta\| \widehat{\mathbf{\theta}}^{(1)}\|_{2}+r_{0}\) to strategize in order to receive action \(1\). **Estimating \(\mathbf{\theta}^{(1)}\)** After playing action \(1\) for the first \(d\) rounds, Algorithm 1 forms an initial estimate of \(\mathbf{\theta}^{(1)}\) via ordinary least squares (OLS). Note that since the first \(d\) agents will receive action \(1\) regardless of their context, they have no incentive to modify and thus \(\mathbf{x}_{t}^{\prime}=\mathbf{x}_{t}\) for \(t\leq d\). In future rounds, the algorithm's estimate of \(\mathbf{\theta}^{(1)}\) is only updated whenever \(\mathbf{x}_{t}^{\prime}\) lies _strictly_ on the positive side of the linear decision boundary. We call these contexts _clean_, and can infer that \(\mathbf{x}_{t}^{\prime}=\mathbf{x}_{t}\) due to the lazy tiebreaking assumption in Definition 2.1 (i.e. agents will not strategize more than is necessary to receive the positive classification).

**Condition 3.2** (Sufficient condition for \(\mathbf{x}^{\prime}=\mathbf{x}\)).: _Given a shifted linear policy parameterized by \(\mathbf{\beta}^{(1)}\in\mathbb{R}^{d}\), we say that a context \(\mathbf{x}^{\prime}\) is clean if \(\langle\mathbf{\beta}^{(1)},\mathbf{x}^{\prime}\rangle>\delta\|\mathbf{\beta}^{(1)}\|_ {2}+r_{0}\)._

**Greedy action assignment** By assigning actions greedily according to the current (shifted) linear policy, we are relying on the diversity in the agent population for implicit exploration (i.e., to collect more datapoints to update our estimate of \(\mathbf{\theta}^{(1)}\)). As we will show, this implicit exploration is sufficient to achieve \(\widetilde{\mathcal{O}}(\sqrt{T})\) strategic regret under Assumption 3.1, albeit at the cost of an exponentially-large (in \(d\)) constant which depends on the agents' ability to manipulate (\(\delta\)).

We are now ready to present our main result: strategic regret guarantees for Algorithm 1 under apple tasting feedback.

**Theorem 3.3** (Informal; detailed version in Theorem B.1).: _With probability \(1-\gamma\), Algorithm 1 achieves the following performance guarantee:_

\[\operatorname{Reg}(T)\leq\widetilde{\mathcal{O}}\left(\frac{1}{c_{0}\cdot c_{ 1}(d,\delta)\cdot c_{2}(d,\delta)}\sqrt{d\sigma^{2}T\log(4dT/\gamma)}\right)\]

_where \(c_{0}\) is a lower bound on the density ratio as defined in Assumption 3.1, \(c_{1}(d,\delta):=\mathbb{P}_{\mathbf{x}\sim U^{d}}(\mathbf{x}[1]\geq\delta)\geq \Theta\left(\frac{(1-\delta)^{d/2}}{d^{2}}\right)\) for sufficiently large \(d\) and \(c_{2}(d,\delta):=\mathbb{E}_{\mathbf{x}\sim U^{d}}[\mathbf{x}[2]^{2}]\mathbf{x }[1]\geq\delta]\geq\left(\frac{3}{4}-\frac{1}{2}\delta-\frac{1}{4}\delta^{2} \right)^{3}\), where \(\mathbf{x}[i]\) denotes the \(i\)-th coordinate of a vector \(\mathbf{x}\).4_

Footnote 4: While we assume that \(\delta\) is known to the principal, Algorithm 1 is fairly robust to overestimates of \(\delta\), in the sense that (1) it will still produce a consistent estimate for \(\boldsymbol{\theta}^{(1)}\) (albeit at a rate which depends on the over-estimate instead of the actual value of \(\delta\)) and (2) it will incur a constant penalty in regret which is proportional to the amount of over-estimation.

Proof sketch.: Our analysis begins by using properties of the strategic agents and shifted linear decision boundary to upper-bound the per-round strategic regret for rounds \(t>d\) by a term proportional to \(\|\widehat{\boldsymbol{\theta}}_{t}^{(1)}-\boldsymbol{\theta}^{(1)}\|_{2}\), i.e., our instantaneous estimation error for \(\boldsymbol{\theta}^{(1)}\). Next we show that

\[\|\widehat{\boldsymbol{\theta}}_{t}^{(1)}-\boldsymbol{\theta}^{(1)}\|_{2}\leq \frac{\left\|\sum_{s=1}^{t}\mathbf{x}_{s}\epsilon_{s}1\!\left\{\mathcal{I}_{s} ^{(1)}\right\}\right\|_{2}}{\lambda_{min}(\sum_{s=1}^{t}\mathbf{x}_{s}\mathbf{ x}_{s}^{\top}\!\!\!1\!\left\{\mathcal{I}_{s}^{(1)}\right\})}\]

where \(\lambda_{min}(M)\) is the minimum eigenvalue of (symmetric) matrix \(M\), and \(\mathcal{I}_{s}^{(1)}=\{\widehat{\boldsymbol{\theta}}_{s}^{(1)},\mathbf{x}_{s }\rangle\geq\delta\|\widehat{\boldsymbol{\theta}}_{s}^{(1)}\|_{2}+r_{0}\}\) is the event that Algorithm 1 assigns action \(a_{s}=1\) and can verify that \(\mathbf{x}_{s}^{\prime}=\mathbf{x}_{s}\). We upper-bound the numerator using a variant of Azuma's inequality for martingales with subgaussian tails. Next, we use properties of Hermitian matrices to show that \(\lambda_{min}(\sum_{s=1}^{t}\mathbf{x}_{s}\mathbf{x}_{s}^{\top}\!\!\!1\!\left\{ \mathcal{I}_{s}^{(1)}\right\})\) is lower-bounded by two terms: one which may be bounded vs.h.p. by using the extension of Azuma's inequality for matrices, and one of the form \(\sum_{s=1}^{t}\lambda_{min}(\mathbb{E}_{s-1}[\mathbf{x}_{s}\mathbf{x}_{s}^{ \top}\!\!\!1\!\left\{\mathcal{I}_{s}^{(1)}\right\}])\), where \(\mathbb{E}_{s-1}\) denotes the expected value conditioned on the filtration up to time \(s\). Note that up until this point, we have only used the fact that contexts are drawn i.i.d. from a _bounded_ distribution.

Using Assumption 3.1 on the bounded density ratio, we can lower bound \(\lambda_{min}(\mathbb{E}_{s-1}[\mathbf{x}_{s}\mathbf{x}_{s}^{\top}\!\!\!1\! \left\{\mathcal{I}_{s}^{(1)}\right\}])\) by \(\lambda_{min}(\mathbb{E}_{U^{d},s-1}[\mathbf{x}_{s}\mathbf{x}_{s}^{\top}\!\! \!1\!\left\{\mathcal{I}_{s}^{(1)}\right\}])\), _where the expectation is taken with respect to the uniform distribution over the \(d\)-dimensional ball_. We then use properties of the uniform distribution to show that \(\lambda_{min}(\mathbb{E}_{U^{d},s-1}[\mathbf{x}_{s}\mathbf{x}_{s}^{\top}\!\!\!1 \!\left\{\mathcal{I}_{s}^{(1)}\right\}])\geq\mathcal{O}(c_{0}\cdot c(d,\delta))\). Putting everything together, we get that \(\|\widehat{\boldsymbol{\theta}}_{t}^{(1)}-\boldsymbol{\theta}^{(1)}\|_{2}\leq (c_{0}\cdot c(d,\delta)\cdot\sqrt{t})^{-1}\) with high probability. Via a union bound and the fact that \(\sum_{t\in[T]}\frac{1}{\sqrt{t}}\leq 2T\), we get that \(\operatorname{Reg}(T)\leq\widetilde{\mathcal{O}}(\frac{1}{c_{0}\cdot c(d, \delta)}\sqrt{T})\). Finally, we use tools from high-dimensional geometry to lower bound the volume of a spherical cap and we show that for sufficiently large \(d\), \(c_{1}(d,\delta)\geq\Theta\left(\frac{(1-\delta)^{d/2}}{d^{2}}\right)\). 

### High-dimensional contexts

While we typically think of the number of agents \(T\) as growing and the context dimension \(d\) as constant in our applications of interest, there may be situations in which \(T\) is either unknown or small. Under such settings, the \(\nicefrac{{1}}{{c}}(d,\delta)\) dependence in the regret bound (where \(c(d,\delta)=c_{1}(d,\delta)\cdot c_{2}(d,\delta)\)) may become problematic if \(\delta\) is close to \(1\). This begs the question: "Why restrict the OLS estimator in Algorithm 1 to use only clean contexts (as defined in Condition 3.2)?" Perhaps unsurprisingly, we show in Appendix B that the estimate \(\widehat{\boldsymbol{\theta}}^{(1)}\) given by OLS will be inconsistent if even a constant fraction of agents strategically modify their contexts.

Given the above, it seems reasonable to restrict ourselves to learning procedures which only use data from agents for which the principal can be sure that \(\mathbf{x}^{\prime}=\mathbf{x}\). Under such a restriction, it is natural to ask whether there exists some sequence of linear polices which maximizes the number of points of the form \((\mathbf{x}^{\prime}_{t},r_{t}(1))\) for which the principal can be sure that \(\mathbf{x}^{\prime}_{t}=\mathbf{x}_{t}\). Again, the answer is no:

**Proposition 3.4**.: _For any sequence of linear policies \(\{\boldsymbol{\beta}_{t}\}_{t}\), the expected number of clean points is:_

\[\mathbb{E}_{\mathbf{x}_{1},\ldots,\mathbf{x}_{T}\sim U^{d}}\left[\sum_{t\in[T] }\mathbbm{1}\left\{\langle\mathbf{x}_{t},\boldsymbol{\beta}_{t}\rangle>\delta \|\boldsymbol{\beta}_{t}\|_{2}\right\}\right]=c_{1}(d,\delta)\cdot T\]

_when (initial) contexts are drawn uniformly from the \(d\)-dimensional unit sphere._

The proof follows from the rotational invariance of the uniform distribution over the unit sphere. Intuitively, Proposition 3.4 implies that any algorithm which wishes to learn \(\boldsymbol{\theta}^{(1)}\) using clean samples will only have \(c_{1}(d,\delta)\cdot T\) datapoints in expectation. Observe that this dependence on \(c_{1}(d,\delta)\) arises as a direct result of the agents' ability to strategize. We remark that a similar constant often appears in the regret analysis of BIC bandit algorithms (see Section 1.2). Much like our work, [43] find that their regret rates depend on a constant which may be arbitrarily large, depending on how hard it is to persuade agents to take the principal's desired action in their setting. The authors conjecture that this dependence is an inevitable "price of incentive-compatibility". While our results do not rule out better strategic regret rates in \(d\) for more complicated algorithms (e.g., those which deploy non-linear policies), it is often unclear how strategic agents would behave in such settings, both in theory (Definition 2.1 would require agents to solve a non-convex optimization with potentially no closed-form solution) and in practice, making the analysis of such nonlinear policies difficult in strategic settings.

We conclude this section by showing that polynomial dependence on \(d\) is possible, at the cost of \(\widetilde{\mathcal{O}}(T^{2/3})\) strategic regret. Specifically, we provide an algorithm (Algorithm 3) which obtains the following regret guarantee whenever \(T\) is small or unknown, which uses Algorithm 1 and a variant of the explore-then-commit algorithm (Algorithm 2) as subroutines:

**Theorem 3.5** (Informal; details in Theorem B.13).: _Algorithm 3 incurs expected strategic regret_

\[\mathbb{E}[\operatorname{Reg}(T)]=\widetilde{\mathcal{O}}\left(\min\left\{ \frac{d^{5/2}}{(1-\delta)^{d/2}}\cdot\sqrt{T},d\cdot T^{2/3}\right\}\right),\]

_where the expectation is taken with respect to the sequence of contexts \(\{\mathbf{x}_{t}\}_{t\in[T]}\) and random noise \(\{\epsilon_{t}\}_{t\in[T]}\)._

The algorithm proceeds by playing a "strategy-aware" variant of explore-then-commit (Algorithm 2) with a doubling trick until the switching time \(\tau^{*}=g(d,\delta)\) is reached. Note that \(g(d,\delta)\) is a function of both \(d\) and \(\delta\), _not_\(c_{0}\). If round \(\tau^{*}\) is indeed reached, the algorithm switches over to Algorithm 1 for the remaining rounds.

```
Input : Time horizon \(T\), failure probability \(\gamma\)  Set \(T_{0}\) according to Theorem B.9  Assign action \(1\) for the first \(T_{0}\) rounds  Estimate \(\boldsymbol{\theta}^{(1)}\) as \(\hat{\boldsymbol{\theta}}^{(1)}_{T_{0}}\) via OLS for \(t=T_{0}+1,\ldots,T\)do  Assign action \(a_{t}=1\) if \(\langle\hat{\boldsymbol{\theta}}^{(1)}_{T_{0}},\mathbf{x}_{t}\rangle\geq\delta \cdot\|\hat{\boldsymbol{\theta}}^{(1)}_{T_{0}}\|_{2}\) and action \(a_{t}=0\) otherwise
```

**ALGORITHM 2**Explore-Then-Commit

**Extension to bandit feedback** Algorithm 1 can be extended to handle bandit feedback by explicitly keeping track of an estimate \(\widehat{\boldsymbol{\theta}}^{(0)}\) of \(\boldsymbol{\theta}^{(0)}\) via OLS, assigning action \(a_{t}=1\) if and only if \(\langle\widehat{\boldsymbol{\theta}}^{(1)}_{t}-\widehat{\boldsymbol{\theta}}^{ (0)}_{t},\mathbf{x}^{\prime}_{t}\rangle\geq\delta\cdot\|\widehat{\boldsymbol{ \theta}}^{(1)}_{t}-\widehat{\boldsymbol{\theta}}^{(0)}_{t}\|_{2}\), and updating the OLS estimate of \(\widehat{\boldsymbol{\theta}}^{(0)}\) whenever \(a_{t}=0\) (since agents will not strategize to receive action 0). Algorithm 3 may be extended to bandit feedback by "exploring" for twice as long in Algorithm 2, in addition to using the above modifications. In both cases, the strategic regret rates are within a constant factor of the rates obtained in Theorem 3.3 and Theorem 3.5.

## 4 Beyond stochastic contexts

In this section, we allow the sequence of initial agent contexts to be chosen by an (oblivious) _adversary_. This requires new algorithmic ideas, as the regression-based algorithms of Section 3 suffer _linear_ strategic regret under this adversarial setting. Our algorithm (Algorithm 4) is based on the popular EXP3 algorithm [6]. At a high level, Algorithm 4 maintains a probability distribution over "experts", i.e., a discretized grid \(\mathcal{E}\) over carefully-selected policies. In particular, each grid point \(\mathbf{e}\in\mathcal{E}\subseteq\mathbb{R}^{d}\) represents an "estimate" of \(\mathbf{\theta}^{(1)}\), and corresponds to a slope vector which parameterizes a (shifted) linear threshold policy, like the ones considered in Section 3. We use \(a_{t,\mathbf{e}}\) to refer to the action played by the principal at time \(t\), had they used the linear threshold policy parameterized by expert \(\mathbf{e}\). At every time-step, (1) the adversary chooses an agent \(\mathbf{x}_{t}\), (2) a slope vector \(\mathbf{e}_{t}\in\mathcal{E}\) is selected according to the current distribution, (3) the principal commits to assigning action \(1\) if and only if \(\langle\mathbf{e}_{t},\mathbf{x}_{t}^{\prime}\rangle\geq\delta\|\mathbf{e}_{t }\|_{2}\), (4) the agent strategically modifies their context \(\mathbf{x}_{t}\rightarrow\mathbf{x}_{t}^{\prime}\), and (5) the principal assigns an action \(a_{t}\) according to the policy and receives the associated reward \(r_{t}(a_{t})\) (under apple tasting feedback).

Algorithm EXP4, which maintains a distribution over experts and updates the loss of _all_ experts based on the current action taken, is not directly applicable in our setting as the strategic behavior of the agents prevents us from inferring the loss of each expert at every time-step [9]. This is because if \(\mathbf{x}_{t}^{\prime}\neq\mathbf{x}_{t}\) under the thresholding policy associated with expert \(\mathbf{e}\)), it is generally not possible to "back out" \(\mathbf{x}_{t}\) given \(\mathbf{x}_{t}^{\prime}\), which prevents us from predicting the counterfactual context the agent would have modified to had the principal been using expert \(\mathbf{e}^{\prime}\) instead. As a result, we use a modification of the standard importance-weighted loss estimator to update the loss of _only the policy played by the algorithm_ (and therefore the distribution over policies). Our regret guarantees for Algorithm 4 are as follows:

**Theorem 4.1** (Informal; detailed version in Theorem C.1).: _Algorithm 4 incurs expected strategic regret \(\mathbb{E}[\mathrm{Reg}(T)]=\widetilde{\mathcal{O}}(T^{(d+1)/(d+2)})\)._

We remark that Algorithm 4 may be extended to handle settings in which agents are selected by an _adaptive_ adversary by using EXP3.P [6] in place of EXP3.

Proof sketch.: The analysis is broken down into two parts. In the first part, we bound the regret w.r.t. the best policy on the grid. In the second, we bound the error incurred for playing policies on the grid, rather than the continuous space of policies. We refer to this error as the _Strategic Discretization Error_ (\(SDE(T)\)). The analysis of the regret on the grid mostly follows similar steps to the analysis of EXP3 / EXP4. The important difference is that we shift the reward obtained by \(a_{t}\), by a factor of \(1+\lambda\), where \(\lambda\) is a (tunable) parameter of the algorithm. This shifting (which does not affect the regret, since all the losses are shifted by the same fixed amount) guarantees that the losses at each round are non-negative and bounded with high probability. Technically, this requires bounding the tails of the subgaussian of the noise parameters \(\epsilon_{t}\).

We now shift our attention to bounding \(SDE(T)\). The standard analysis of the discretization error in the non-strategic setting does not go through for our setting, since an agent may strategize very differently with respect to two policies which are "close together" in \(\ell_{2}\) distance, depending on the agent's initial context. Our analysis proceeds with a case-by-case basis. Consider the best expert \(\mathbf{e}^{*}\) in the grid. If \(a_{t,\mathbf{e}^{*}}=\pi^{*}(\mathbf{x}_{t})\) (i.e., the action of the best expert matches that of the optimal policy), there is no discretization error in round \(t\). Otherwise, if \(a_{t,\mathbf{e}^{*}}\neq\pi^{*}(\mathbf{x}_{t})\), we show that the per-round \(SDE\) is upper-bounded by a term which looks like twice the discretization upper-bound for the non-strategic setting, plus an additional term. We show that this additional term must always be non-positive by considering two subcases (\(a_{t,\mathbf{e}^{*}}=1\), \(\pi^{*}(\mathbf{x}_{t})=0\) and \(a_{t,\mathbf{e}^{*}}=0\), \(\pi^{*}(\mathbf{x}_{t})=1\)) and using properties about how agents strategize against the deployed algorithmic policies.

**Algorithm 4**EXP3 with strategy-aware experts (EXP3-SAE)

**Computational complexity** While both Algorithm 1 and Algorithm 3 have \(\mathcal{O}(d^{3})\) per-iteration computational complexity, Algorithm 4 must maintain and update a probability distribution over a grid of size exponential in \(d\) at every time-step, making it hard to use in practice if \(d\) is large. We view the design of computationally efficient algorithms for adversarially-chosen contexts as an important direction for future research.

**Extension to bandit feedback** Algorithm 4 may be extended to the bandit feedback setting by maintaining a grid over estimates of \(\boldsymbol{\theta}^{(1)}-\boldsymbol{\theta}^{(0)}\) (instead of over \(\boldsymbol{\theta}^{(1)}\)). No further changes are required.

## 5 Conclusion

We study the problem of classification under incentives with apple testing feedback. Such one-sided feedback is often what is observed in real-world strategic settings including lending and hiring. Our main result is a "greedy" algorithm (Algorithm 1) which achieves \(\widetilde{\mathcal{O}}(\sqrt{T})\) strategic regret when the initial agent contexts are generated _stochastically_. The regret of Algorithm 1 depends on a constant \(c_{1}(d,\delta)\) which scales exponentially in the context dimension, which may be problematic in settings for which the number of agents is small or unknown. To address this, we provide an algorithm (Algorithm 3) which combines Algorithm 1 with a strategy-aware version of the explore-then-commit algorithm using a doubling trick to achieve \(\widetilde{\mathcal{O}}(\min\{\frac{\sqrt{dT}}{c_{1}(d,\delta)},d\cdot T^{2/3 }\})\) expected strategic regret whenever \(T\) is unknown. Finally, we relax the assumption of stochastic contexts and allow for contexts to be generated adversarially. Algorithm 4 achieves \(\widetilde{\mathcal{O}}(T^{\frac{d+1}{d+2}})\) expected strategic regret whenever agent contexts are generated adversarially by running EXP3 over a discretized grid of strategy-aware policies, but has exponential-in-\(d\) per-round computational complexity. All of our results also apply to the more general setting of bandit feedback, under slight modifications to the algorithms. There are several directions for future work:

**Unclean data** The regret of Algorithm 1 depends on a constant which is exponentially large in \(d\), due to the fact that it only learns using clean data (Condition 3.2). While learning using unclean data will generally produce an inconsistent estimator, it would be interesting to see if the principal could leverage this data to remove the dependence on this constant. Alternatively, lower bounds which show that using unclean data will not improve regret would also be interesting.

**Efficient algorithms for adversarial contexts** Our algorithm for adversarially-chosen agent contexts suffers exponential-in-\(d\) per-round computational complexity, which makes it unsuitable for use in settings with high-dimensional contexts. Deriving polynomial-time algorithms with sublinear strategic regret for this setting is an exciting (but challenging) direction for future research.

**More than two actions** Finally, it would be interesting to extend our algorithms for strategic learning under bandit feedback to the setting in which the principal has _three or more_ actions at their disposal. While prior work [29] implies an impossibility result for strategic regret minimization with three or more actions, other (relaxed) notions of optimality (e.g., sublinear _Stackelberg_ regret; recall Definition 2.2) may still be possible.

## Acknowledgements

KH is supported in part by an NDSEG Fellowship. KH and ZSW are supported in part by the NSF FAI Award #1939606. For part of this work, CP was supported by a FODSI postdoctoral fellowship from UC Berkeley. The authors would like to thank the anonymous NeurIPS reviewers for valuable feedback.

## References

* [1] Algorithmic hiring: Complex hiring by numbers? URL [https://hiring.monster.com/resources/recruiting-strategies/workforce-planning/hiring-algorithms/](https://hiring.monster.com/resources/recruiting-strategies/workforce-planning/hiring-algorithms/).
* [2] Using white font on a cv to trick ats. URL [https://www.thecvstore.net/blog/cv-ats-white-font/#:~text=Even%20if%20you%20get%20past,harm%20your%20future%20employment%20prospects](https://www.thecvstore.net/blog/cv-ats-white-font/#:~text=Even%20if%20you%20get%20past,harm%20your%20future%20employment%20prospects).
* [3] Saba Ahmadi, Hedyeh Beyhaghi, Avrim Blum, and Keziah Naggita. The strategic perceptron. In _Proceedings of the 22nd ACM Conference on Economics and Computation_, pages 6-25, 2021.
* [4] Saba Ahmadi, Avrim Blum, and Kunhe Yang. Fundamental bounds on online strategic classification. In _Proceedings of the 24th ACM Conference on Economics and Computation, EC 2023, London, United Kingdom, July 9-12, 2023_, pages 22-58. ACM, 2023.
* [5] Andras Antos, Gabor Bartok, David Pal, and Csaba Szepesvari. Toward a classification of finite partial-monitoring games. _Theoretical Computer Science_, 473:77-99, 2013.
* [6] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. _SIAM journal on computing_, 32(1):48-77, 2002.
* [7] Maria-Florina Balcan, Avrim Blum, Nika Haghtalab, and Ariel D Procaccia. Commitment without regrets: Online learning in stackelberg security games. In _Proceedings of the sixteenth ACM conference on economics and computation_, pages 61-78, 2015.
* [8] Hamsa Bastani, Mohsen Bayati, and Khashayar Khosravi. Mostly exploration-free algorithms for contextual bandits. _Management Science_, 67(3):1329-1349, 2021. doi: 10.1287/mnsc.2020.3605. URL [https://doi.org/10.1287/mnsc.2020.3605](https://doi.org/10.1287/mnsc.2020.3605).
* [9] Yahav Bechavod, Katrina Ligett, Aaron Roth, Bo Waggoner, and Steven Z Wu. Equal opportunity in online classification with partial feedback. _Advances in Neural Information Processing Systems_, 32, 2019.
* [10] Yahav Bechavod, Katrina Ligett, Steven Wu, and Juba Ziani. Gaming helps! learning from strategic interactions in natural dynamics. In _International Conference on Artificial Intelligence and Statistics_, pages 1234-1242. PMLR, 2021.
* [11] Yahav Bechavod, Chara Podimata, Steven Wu, and Juba Ziani. Information discrepancy in strategic learning. In _International Conference on Machine Learning_, pages 1691-1715. PMLR, 2022.
* [12] Jillian Berman. Do ai-powered lending algorithms silently discriminate? this initiative aims to find out, Nov 2021. URL [https://www.marketwatch.com/story/do-ai-powered-lending-algorithms-silently-discriminate-this-initiative](https://www.marketwatch.com/story/do-ai-powered-lending-algorithms-silently-discriminate-this-initiative)\-aims-to-find-out-11637246524.
* [13] Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Alberto Marchesi, Francesco Trovo, and Nicola Gatti. Optimal rates and efficient algorithms for online bayesian persuasion. In _International Conference on Machine Learning_, pages 2164-2183. PMLR, 2023.
* [14] Alberto Bietti, Alekh Agarwal, and John Langford. A contextual bandit bake-off. _The Journal of Machine Learning Research_, 22(1):5928-5976, 2021.
* [15] Avrim Blum, John Hopcroft, and Ravindran Kannan. _Foundations of data science_. Cambridge University Press, 2020.

* [16] Matteo Castiglioni, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Online bayesian persuasion. _Advances in Neural Information Processing Systems_, 33:16188-16198, 2020.
* [17] Matteo Castiglioni, Alberto Marchesi, Andrea Celli, and Nicola Gatti. Multi-receiver online bayesian persuasion. In _International Conference on Machine Learning_, pages 1314-1323. PMLR, 2021.
* [18] Yatong Chen, Wei Tang, Chien-Ju Ho, and Yang Liu. Performative prediction with bandit feedback: Learning through reparameterization. _arXiv preprint arXiv:2305.01094_, 2023.
* [19] Yiling Chen, Yang Liu, and Chara Podimata. Learning strategy-aware linear classifiers. _Advances in Neural Information Processing Systems_, 33:15265-15276, 2020.
* [20] Vincent Conitzer and Nikesh Garera. Learning algorithms for online principal-agent problems (and selling goods online). In _Proceedings of the 23rd international conference on Machine learning_, pages 209-216, 2006.
* [21] Jinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, and Zhiwei Steven Wu. Strategic classification from revealed preferences. In _Proceedings of the 2018 ACM Conference on Economics and Computation_, pages 55-70, 2018.
* [22] Itay Eilat, Ben Finkelshtein, Chaim Baskin, and Nir Rosenfeld. Strategic classification with graph neural networks. _arXiv preprint arXiv:2205.15765_, 2022.
* [23] Christian Eilers. Resume keywords: List by industry [for use to pass the usts], Jan 2023. URL [https://zety.com/blog/resume-keywords](https://zety.com/blog/resume-keywords).
* [24] Danielle Ensign, Sorelle A Friedler, Scott Neville, Carlos Scheidegger, and Suresh Venkatasubramanian. Decision making with limited feedback: Error bounds for predictive policing and recidivism prediction. In _Proceedings of Algorithmic Learning Theory,_, volume 83, 2018.
* [25] Ganesh Ghalme, Vineet Nair, Itay Eilat, Inbal Talgam-Cohen, and Nir Rosenfeld. Strategic classification in the dark. In _International Conference on Machine Learning_, pages 3672-3681. PMLR, 2021.
* [26] Nika Haghtalab, Thodoris Lykouris, Sloan Nietert, and Alexander Wei. Learning in stackelberg games with non-myopic agents. In _Proceedings of the 23rd ACM Conference on Economics and Computation_, pages 917-918, 2022.
* [27] Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classification. In _Proceedings of the 2016 ACM conference on innovations in theoretical computer science_, pages 111-122, 2016.
* [28] Keegan Harris, Hoda Heidari, and Steven Z Wu. Stateful strategic regression. _Advances in Neural Information Processing Systems_, 34:28728-28741, 2021.
* [29] Keegan Harris, Anish Agarwal, Chara Podimata, and Zhiwei Steven Wu. Strategyproof decision-making in panel data settings and beyond. _arXiv preprint arXiv:2211.14236_, 2022.
* [30] Keegan Harris, Valerie Chen, Joon Kim, Ameet Talwalkar, Hoda Heidari, and Steven Z Wu. Bayesian persuasion for algorithmic recourse. _Advances in Neural Information Processing Systems_, 35:11131-11144, 2022.
* [31] Keegan Harris, Dung Daniel T Ngo, Logan Stapleton, Hoda Heidari, and Steven Wu. Strategic instrumental variable regression: Recovering causal relationships from strategic responses. In _International Conference on Machine Learning_, pages 8502-8522. PMLR, 2022.
* [32] David P Helmbold, Nicholas Littlestone, and Philip M Long. Apple tasting. _Information and Computation_, 161(2):85-139, 2000.
* [33] Chien-Ju Ho, Aleksandrs Slivkins, and Jennifer Wortman Vaughan. Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems. In _Proceedings of the fifteenth ACM conference on Economics and computation_, pages 359-376, 2014.

* Horowitz and Rosenfeld [2023] Guy Horowitz and Nir Rosenfeld. Causal strategic classification: A tale of two shifts. In _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 13233-13253. PMLR, 2023.
* Hu et al. [2022] Xinyan Hu, Dung Ngo, Aleksandrs Slivkins, and Steven Z Wu. Incentivizing combinatorial bandit exploration. _Advances in Neural Information Processing Systems_, 35:37173-37183, 2022.
* Immorlica et al. [2019] Nicole Immorlica, Jieming Mao, Aleksandrs Slivkins, and Zhiwei Steven Wu. Bayesian exploration with heterogeneous agents. In _The world wide web conference_, pages 751-761, 2019.
* Jagadeesan et al. [2021] Meena Jagadeesan, Celestine Mendler-Dunner, and Moritz Hardt. Alternative microfoundations for strategic classification. In _International Conference on Machine Learning_, pages 4687-4697. PMLR, 2021.
* Kannan et al. [2018] Sampath Kannan, Jamie Morgenstern, Aaron Roth, Bo Waggoner, and Zhiwei Steven Wu. A smoothed analysis of the greedy algorithm for the linear contextual bandit problem. In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_, NIPS'18, page 2231-2241, Red Hook, NY, USA, 2018. Curran Associates Inc.
* Kleinberg and Raghavan [2020] Jon Kleinberg and Manish Raghavan. How do classifiers induce agents to invest effort strategically? _ACM Transactions on Economics and Computation (TEAC)_, 8(4):1-23, 2020.
* Lancaster and Imbens [1996] Tony Lancaster and Guido Imbens. Case-control studies with contaminated controls. _Journal of Econometrics_, 71(1-2):145-160, 1996.
* Levanon and Rosenfeld [2021] Sagi Levanon and Nir Rosenfeld. Strategic classification made practical. In _International Conference on Machine Learning_, pages 6243-6253. PMLR, 2021.
* Levanon and Rosenfeld [2022] Sagi Levanon and Nir Rosenfeld. Generalized strategic classification and the case of aligned incentives. In _International Conference on Machine Learning_, pages 12593-12618. PMLR, 2022.
* Mansour et al. [2015] Yishay Mansour, Aleksandrs Slivkins, and Vasilis Syrgkanis. Bayesian incentive-compatible bandit exploration. In _Proceedings of the Sixteenth ACM Conference on Economics and Computation_, pages 565-582, 2015.
* Ngo et al. [2021] Daniel Ngo, Logan Stapleton, Vasilis Syrgkanis, and Zhiwei Steven Wu. Incentivizing bandit exploration: Recommendations as instruments. In _Proceedings of the 2021 International Conference on Machine Learning (ICML'21)_, 2021.
* Ngo et al. [2023] Daniel Ngo, Keegan Harris, Anish Agarwal, Vasilis Syrgkanis, and Zhiwei Steven Wu. Incentive-aware synthetic control: Accurate counterfactual estimation via incentivized exploration. 2023. URL [https://keeganharris.github.io/IE_SC.pdf](https://keeganharris.github.io/IE_SC.pdf).
* O'Shea [2022] Bev O'Shea. 9 ways to build and improve your credit fast, Nov 2022. URL [https://www.nerdwallet.com/article/finance/raise-credit-score-fast](https://www.nerdwallet.com/article/finance/raise-credit-score-fast).
* Perdomo et al. [2020] Juan Perdomo, Tijana Zrnic, Celestine Mendler-Dunner, and Moritz Hardt. Performative prediction. In _International Conference on Machine Learning_, pages 7599-7609. PMLR, 2020.
* Raghavan et al. [2018] Manish Raghavan, Aleksandrs Slivkins, Jennifer Wortman Vaughan, and Zhiwei Steven Wu. The externalities of exploration and how data diversity helps exploitation. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, _Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018_, volume 75 of _Proceedings of Machine Learning Research_, pages 1724-1738. PMLR, 2018. URL [http://proceedings.mlr.press/v75/raghavan18a.html](http://proceedings.mlr.press/v75/raghavan18a.html).
* Raghavan et al. [2023] Manish Raghavan, Aleksandrs Slivkins, Jennifer Wortman Vaughan, and Zhiwei Steven Wu. Greedy algorithm almost dominates in smoothed contextual bandits. _SIAM Journal on Computing_, 52(2):487-524, 2023. doi: 10.1137/19M1247115. URL [https://doi.org/10.1137/19M1247115](https://doi.org/10.1137/19M1247115).

* Sellke and Slivkins [2021] Mark Sellke and Aleksandrs Slivkins. The price of incentivizing exploration: A characterization via thompson sampling and sample complexity. In _Proceedings of the 22nd ACM Conference on Economics and Computation_, pages 795-796, 2021.
* Shamir [2011] Ohad Shamir. A variant of azuma's inequality for martingales with subgaussian tails. _arXiv preprint arXiv:1110.2392_, 2011.
* Shavit et al. [2020] Yonadav Shavit, Benjamin Edelman, and Brian Axelrod. Causal strategic linear regression. In _International Conference on Machine Learning_, pages 8676-8686. PMLR, 2020.
* Sivakumar et al. [2020] Vidyashankar Sivakumar, Zhiwei Steven Wu, and Arindam Banerjee. Structured linear contextual bandits: A sharp and geometric smoothed analysis. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 9026-9035. PMLR, 2020. URL [http://proceedings.mlr.press/v119/sivakumar20a.html](http://proceedings.mlr.press/v119/sivakumar20a.html).
* Tropp [2012] Joel A Tropp. User-friendly tail bounds for sums of random matrices. _Foundations of computational mathematics_, 12:389-434, 2012.
* Zhao et al. [2023] Geng Zhao, Banghua Zhu, Jiantao Jiao, and Michael Jordan. Online learning in stackelberg games with an omniscient follower. In _International Conference on Machine Learning_, pages 42304-42316. PMLR, 2023.
* Zhu et al. [2023] Banghua Zhu, Stephen Bates, Zhuoran Yang, Yixin Wang, Jiantao Jiao, and Michael I. Jordan. The sample complexity of online contract design. In _Proceedings of the 24th ACM Conference on Economics and Computation, EC 2023, London, United Kingdom, July 9-12, 2023_, page 1188. ACM, 2023.

[MISSING_PAGE_FAIL:15]

Proof.: If \(a_{t}=a_{t}^{*}\), the condition is satisfied trivially. If \(a_{t}\neq a_{t}^{*}\), then either (1) \(\langle\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{\mathbf{\theta}}_{t}^{(0)},\mathbf{x} _{t}^{\prime}\rangle-\delta\|\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{\mathbf{ \theta}}_{t}^{(0)}\|_{2}\geq 0\) and \(\langle\mathbf{\theta}^{(1)}-\mathbf{\theta}^{(0)}\rangle<0\) or (2) \(\langle\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{\mathbf{\theta}}_{t}^{(0)},\mathbf{x }_{t}^{\prime}\rangle-\delta\|\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{\mathbf{ \theta}}_{t}^{(0)}\|_{2}<0\) and \(\langle\mathbf{\theta}^{(1)}-\mathbf{\theta}^{(0)}\rangle\geq 0\).

**Case 1:**\(\langle\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{\mathbf{\theta}}_{t}^{(0)},\mathbf{x }_{t}^{\prime}\rangle-\delta\|\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{\mathbf{ \theta}}_{t}^{(0)}\|_{2}\geq 0\) and \(\langle\mathbf{\theta}^{(1)}-\mathbf{\theta}^{(0)}\rangle<0\). (\(a_{t}^{*}=0\), \(a_{t}=1\))

By Definition 2.1, we can rewrite

\[\langle\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{\mathbf{\theta}}_{t}^{(0)},\mathbf{ x}_{t}^{\prime}\rangle-\delta\|\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{\mathbf{ \theta}}_{t}^{(0)}\|_{2}\geq 0\]

as

\[\langle\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{\mathbf{\theta}}_{t}^{(0)},\mathbf{ x}_{t}\rangle+(\delta^{\prime}-\delta)\|\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{ \mathbf{\theta}}_{t}^{(0)}\|_{2}\geq 0\]

for some \(\delta^{\prime}\leq\delta\). Since \((\delta^{\prime}-\delta)\|\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{\mathbf{\theta} }_{t}^{(0)}\|_{2}\leq 0\), \(\langle\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{\mathbf{\theta}}_{t}^{(0)},\mathbf{ x}_{t}\rangle\geq 0\) must hold.

**Case 2:**\(\langle\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{\mathbf{\theta}}_{t}^{(0)},\mathbf{ x}_{t}^{\prime}\rangle-\delta\|\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{\mathbf{ \theta}}_{t}^{(0)}\|_{2}<0\) and \(\langle\mathbf{\theta}^{(1)}-\mathbf{\theta}^{(0)}\rangle\geq 0\). (\(a_{t}^{*}=1\), \(a_{t}=0\))

Since modification did not help agent \(t\) receive action \(a_{t}=1\), we can conclude that \(\langle\widehat{\mathbf{\theta}}_{t}^{(1)}-\widehat{\mathbf{\theta}}_{t}^{(0)}, \mathbf{x}_{t}\rangle<0\). 

**Lemma B.3**.: _Let \(f_{U^{d}}:\mathcal{X}\rightarrow\mathbb{R}_{>0}\) denote the density function of the uniform distribution over the \(d\)-dimensional unit sphere. If \(T\geq d\) and agent contexts are drawn from a distribution over the \(d\)-dimensional unit sphere with density function \(f:\mathcal{X}\rightarrow\mathbb{R}_{\geq 0}\) such that \(\frac{f(\mathbf{x})}{f_{U^{d}}(\mathbf{x})}\geq c_{0}\in\mathbb{R}_{>0}\), \(\forall\mathbf{x}\in\mathcal{X}\), then the following guarantee holds under apple tasting feedback._

\[\|\mathbf{\theta}^{(1)}-\hat{\mathbf{\theta}}_{t+1}^{(1)}\|_{2}\leq\frac{2}{c_{0}\cdot c _{1}(\delta,d)\cdot c_{2}(\delta,d)}\sqrt{\frac{14d\sigma^{2}\log(2d/\gamma_{ t})}{t}}\]

_with probability \(1-\gamma_{t}\)._

Proof.: Let \(\mathcal{I}_{s}^{(1)}=\{\langle\hat{\mathbf{\theta}}_{s}^{(1)},\mathbf{x}_{s} \rangle\geq\delta\|\hat{\mathbf{\theta}}_{s}^{(1)}\|_{2}+r_{0}\}\). Then, from the definition of \(\mathbf{\theta}_{t+1}^{(1)}\) we have:

\[\hat{\mathbf{\theta}}_{t+1}^{(1)} :=\left(\sum_{s=1}^{t}\mathbf{x}_{s}\mathbf{x}_{s}^{\top}\mathbb{ 1}\left\{\mathcal{I}_{s}^{(1)}\right\}\right)^{-1}\sum_{s=1}^{t}\mathbf{x}_{s }r_{s}(1)\mathbb{1}\left\{\mathcal{I}_{s}^{(1)}\right\}\] (closed form solution of OLS) \[=\left(\sum_{s=1}^{t}\mathbf{x}_{s}\mathbf{x}_{s}^{\top}\mathbb{ 1}\left\{\mathcal{I}_{s}^{(1)}\right\}\right)^{-1}\sum_{s=1}^{t}\mathbf{x}_{s }(\mathbf{x}_{s}^{\top}\mathbf{\theta}^{(1)}+\epsilon_{s})\mathbb{1}\left\{ \mathcal{I}_{s}^{(1)}\right\}\] (plug in

\[r_{s}(1)\]

)

Re-arranging the above and taking the \(\ell_{2}\) norm on both sides we get:

\[\left\|\mathbf{\theta}^{(1)}-\hat{\mathbf{\theta}}_{t+1}^{(1)}\right\|_{2} =\left\|\left(\sum_{s=1}^{t}\mathbf{x}_{s}\mathbf{x}_{s}^{\top} \mathbb{1}\left\{\mathcal{I}_{s}^{(1)}\right\}\right)^{-1}\sum_{s=1}^{t} \mathbf{x}_{s}\epsilon_{s}\mathbb{1}\left\{\mathcal{I}_{s}^{(1)}\right\}\right\| _{2}\] (Cauchy-Schwarz) \[=\frac{\left\|\sum_{s=1}^{t}\mathbf{x}_{s}\epsilon_{s}\mathbb{1} \left\{\mathcal{I}_{s}^{(1)}\right\}\right\|_{2}}{\sigma_{min}\left(\sum_{s=1 }^{t}\mathbf{x}_{s}\mathbf{x}_{s}^{\top}\mathbb{1}\left\{\mathcal{I}_{s}^{(1)} \right\}\right)}\] \[=\frac{\left\|\sum_{s=1}^{t}\mathbf{x}_{s}\epsilon_{s}\mathbb{1} \left\{\mathcal{I}_{s}^{(1)}\right\}\right\|_{2}}{\lambda_{min}\left(\sum_{s=1 }^{t}\mathbf{x}_{s}\mathbf{x}_{s}^{\top}\mathbb{1}\left\{\mathcal{I}_{s}^{(1)} \right\}\right)}\]

[MISSING_PAGE_FAIL:17]

\[\lambda_{min}(\mathbb{E}_{s-1}[\mathbf{x}_{s}\mathbf{x}_{s}^{\top} \mathbb{1}\{\mathcal{I}_{s}^{(1)}\}]) :=\min_{\mathbf{\omega}\in S^{d-1}}\mathbf{\omega}^{\top}\mathbb{E}_{s-1}[ \mathbf{x}_{s}\mathbf{x}_{s}^{\top}\mathbb{1}\{\mathcal{I}_{s}^{(1)}\}]\mathbf{\omega}\] \[=\min_{\mathbf{\omega}\in S^{d-1}}\mathbf{\omega}^{\top}\left(\int\mathbf{ x}_{s}\mathbf{x}_{s}^{\top}\mathbb{1}\{\mathcal{I}_{s}^{(1)}\}f(\mathbf{x}_{s})d \mathbf{x}_{s}\right)\mathbf{\omega}\] \[=\min_{\mathbf{\omega}\in S^{d-1}}\mathbf{\omega}^{\top}\left(\int\mathbf{ x}_{s}\mathbf{x}_{s}^{\top}\mathbb{1}\{\mathcal{I}_{s}^{(1)}\}f(\mathbf{x}_{s}) \cdot\frac{f_{U^{d}}(\mathbf{x}_{s})}{f(\mathbf{x}_{s})}d\mathbf{x}_{s}\right) \mathbf{\omega}\] \[\geq c_{0}\cdot\min_{\mathbf{\omega}\in S^{d-1}}\mathbf{\omega}^{\top} \mathbb{E}_{s-1,U^{d}}[\mathbf{x}_{s}\mathbf{x}_{s}^{\top}\mathbb{1}\{ \mathcal{I}_{s}^{(1)}\}]\mathbf{\omega}\] \[=c_{0}\min_{\mathbf{\omega}\in S^{d-1}}\mathbb{E}_{s-1,U^{d}}[\langle \mathbf{\omega},\mathbf{x}_{s}\rangle^{2}|\langle\widehat{\mathbf{\beta}}_{s},\mathbf{ x}_{s}\rangle\geq\delta\|\widehat{\mathbf{\beta}}_{s}\|_{2}]\cdot\underbrace{\mathbb{P}_{s-1,U^{d}}( \langle\widehat{\mathbf{\beta}}_{s},\mathbf{x}_{s}\rangle\geq\delta\|\widehat{\mathbf{ \beta}}_{s}\|_{2})}_{c_{1}(\delta,d)}\] \[=c_{0}\cdot c_{1}(\delta,d)\min_{\mathbf{\omega}\in S^{d-1}}\mathbb{E }_{s-1,U^{d}}[\langle\mathbf{\omega},\mathbf{x}_{s}\rangle^{2}|\langle\widehat{\bm {\beta}}_{s},\mathbf{x}_{s}\rangle\geq\delta\|\widehat{\mathbf{\beta}}_{s}\|_{2}] \tag{3}\]

Throughout the remainder of the proof, we surpress the dependence on \(U^{d}\) and note that unless stated otherwise, all expectations are taken with respect to \(U^{d}\). Let \(B_{s}\in\mathbb{R}^{d\times d}\) be the orthonormal matrix such that the first column is \(\widehat{\mathbf{\beta}}_{s}/\|\widehat{\mathbf{\beta}}_{s}\|_{2}\). Note that \(B_{s}e_{1}=\widehat{\mathbf{\beta}}_{s}/\|\widehat{\mathbf{\beta}}_{s}\|_{2}\) and \(B_{s}\mathbf{x}\sim U^{d}\).

\[\mathbb{E}_{s-1}[\mathbf{x}_{s}\mathbf{x}_{s}^{\top}|\langle \widehat{\mathbf{\beta}}_{s},\mathbf{x}_{s}\rangle\geq\delta\|\widehat{\mathbf{\beta} }_{s}\|] =\mathbb{E}_{s-1}[(B_{s}\mathbf{x}_{s})(B_{s}\mathbf{x}_{s})^{\top}| \langle\widehat{\mathbf{\beta}}_{s},B_{s}\mathbf{x}_{s}\rangle\geq\delta]\] \[=B_{s}\mathbb{E}_{s-1}[\mathbf{x}_{s}\mathbf{x}_{s}^{\top}| \mathbf{x}_{s}^{\top}B_{s}^{\top}\|\widehat{\mathbf{\beta}}_{s}\|_{2}B_{s}e_{1} \geq\delta\cdot\|\widehat{\mathbf{\beta}}_{s}\|_{2}|B_{s}^{\top}\] \[=B_{s}\mathbb{E}_{s-1}[\mathbf{x}_{s}\mathbf{x}_{s}^{\top}| \mathbf{x}_{s}[1]\geq\delta]B_{s}^{\top}\]

Observe that for \(j\neq 1\), \(i\neq j\), \(\mathbb{E}[\mathbf{x}_{s}[j]\mathbf{x}_{s}[i]|\mathbf{x}_{s}[1]\geq\delta]=0\). Therefore,

\[\mathbb{E}_{s-1}[\mathbf{x}_{s}\mathbf{x}_{s}^{\top}|\langle \widehat{\mathbf{\beta}}_{s},\mathbf{x}_{s}\rangle\geq\delta\|\widehat{\mathbf{\beta} }_{s}] =B_{s}(\mathbb{E}[\mathbf{x}_{s}[2]^{2}|\mathbf{x}_{s}[1]\geq\delta]I_ {d}\] \[+(\mathbb{E}[\mathbf{x}_{s}[1]^{2}|\mathbf{x}_{s}[1]\geq\delta]- \mathbb{E}[\mathbf{x}_{s}[2]^{2}|\mathbf{x}_{s}[1]\geq\delta])\mathbf{e}_{1} \mathbf{e}_{1}^{\top})B_{s}^{\top}\] \[=\mathbb{E}[\mathbf{x}_{s}[2]^{2}|\mathbf{x}_{s}[1]\geq\delta]I_ {d}\] \[+(\mathbb{E}[\mathbf{x}_{s}[1]^{2}|\mathbf{x}_{s}[1]\geq\delta]- \mathbb{E}[\mathbf{x}_{s}[2]^{2}|\mathbf{x}_{s}[1]\geq\delta])\frac{\widehat{ \mathbf{\beta}}_{s}}{\|\widehat{\mathbf{\beta}}_{s}\|_{2}}\left(\frac{\widehat{\mathbf{ \beta}}_{s}}{\|\widehat{\mathbf{\beta}}_{s}\|_{2}}\right)^{\top}\]

and

\[\lambda_{min}(\mathbb{E}_{s-1}[\mathbf{x}_{s}\mathbf{x}_{s}^{\top} \mathbb{1}\{\mathcal{I}_{s}^{(1)}\}]) \geq c_{0}\cdot c_{1}(\delta,d)\min_{\mathbf{\omega}\in S^{d-1}}\left( \mathbb{E}[\mathbf{x}_{s}[2]^{2}|\mathbf{x}_{s}[1]\geq\delta]\|\mathbf{\omega}\|_{2}\] \[+(\mathbb{E}[\mathbf{x}_{s}[1]^{2}|\mathbf{x}_{s}[1]\geq\delta]- \mathbb{E}[\mathbf{x}_{s}[2]^{2}|\mathbf{x}_{s}[1]\geq\delta])\left\langle\bm {\omega},\frac{\widehat{\mathbf{\beta}}_{s}}{\|\widehat{\mathbf{\beta}}_{s}\|_{2}} \right\rangle^{2})\] \[\geq c_{0}\cdot c_{1}(\delta,d)\cdot c_{2}(\delta,d)\]

**Lemma B.6**.: _For sufficiently large values of \(d\),_

\[c_{1}(\delta,d)\geq\Theta\left(\frac{(1-\delta)^{d/2}}{d}\right).\]

Proof.: Lemma B.6 is obtained via a similar argument to Theorem 2.7 in Blum et al. [15]. As in Blum et al. [15], we are interested in the volume of a hyperspherical cap. However, we are interested in a lower-bound, not an upper-bound (as is the case in [15]). Let A denote the portion of the \(d\)-dimensional hypersphere with \(\mathbf{x}[1]\geq\frac{\sqrt{c}}{d-1}\) and let H denote the upper hemisphere.

\[c_{1}(\delta,d):=\mathbb{P}_{\mathbf{x}\sim U^{d}}(\mathbf{x}[1]\geq\delta)= \frac{vol(A)}{vol(H)}\]

In order to lower-bound \(c_{1}(\delta,d)\), it suffices to lower bound \(vol(A)\) and upper-bound \(vol(H)\). In what follows, let \(V(d)\) denote the volume of the \(d\)-dimensional hypersphere with radius \(1\).

[MISSING_PAGE_FAIL:19]

Proof.: Let \(B_{t}\in\mathbb{R}^{d\times d}\) be the orthonormal matrix such that the first column is \(\mathbf{\beta}_{t}/\|\mathbf{\beta}_{t}\|_{2}\). Note that \(B_{t}\mathbf{x}\sim U^{d}\) if \(\mathbf{x}\sim U^{d}\) and \(B_{t}e_{1}=\mathbf{\beta}_{t}/\|\mathbf{\beta}_{t}\|_{2}\).

\[\mathbb{E}_{\mathbf{x}_{1},\ldots,\mathbf{x}_{T}\sim U^{d}}[\sum_ {t=1}^{T}\mathbb{1}\{\langle\mathbf{x}_{t},\mathbf{\beta}_{t}\rangle\geq\delta\| \mathbf{\beta}_{t}\|_{2}\}] =\sum_{t=1}^{T}\mathbb{P}_{\mathbf{x}_{t}\sim U^{d}}(\langle \mathbf{x}_{t},\mathbf{\beta}_{t}\rangle\geq\delta\|\mathbf{\beta}_{t}\|_{2})\] \[=\sum_{t=1}^{T}\mathbb{P}_{\mathbf{x}_{t}\sim U^{d}}(\langle B_{t }\mathbf{x}_{t},\mathbf{\beta}_{t}\rangle\geq\delta\|\mathbf{\beta}_{t}\|_{2})\] \[=\sum_{t=1}^{T}\mathbb{P}_{\mathbf{x}_{t}\sim U^{d}}(\mathbf{x}_{ t}^{\top}B_{t}^{\top}\|\mathbf{\beta}_{t}\|_{2}B_{t}e_{1}\geq\delta\|\mathbf{\beta}_{t }\|_{2})\] \[=\sum_{t=1}^{T}\mathbb{P}_{\mathbf{x}_{t}\sim U^{d}}(\mathbf{x}_{ t}^{\top}I_{d}e_{1}\geq\delta\|_{2})\] \[=T\cdot\mathbb{P}_{\mathbf{x}\sim U^{d}}(\mathbf{x}[1]\geq\delta\| _{2})\]

### Explore-Then-Commit Analysis

**Theorem B.9**.: _Let \(f_{U^{d}}:\mathcal{X}\rightarrow\mathbb{R}_{\geq 0}\) denote the density function of the uniform distribution over the \(d\)-dimensional unit sphere. If agent contexts are drawn from a distribution over the \(d\)-dimensional unit sphere with density function \(f:\mathcal{X}\rightarrow\mathbb{R}_{\geq 0}\) such that \(\frac{f(\mathbf{x})}{f_{U}(\mathbf{x})}\geq c_{0}>0\), \(\forall\mathbf{x}\in\mathcal{X}\), then Algorithm 2 achieves the following performance guarantee_

\[\mathrm{Reg}_{\mathrm{ETC}}(T)\leq\frac{8\cdot 63^{1/3}}{c_{0}}d\sigma^{2/3}T^{2 /3}\log^{1/3}(4d/\gamma)\]

_with probability \(1-\gamma\) if \(T_{0}:=4\cdot 63^{1/3}\sigma^{2/3}dT^{2/3}\log^{1/3}(4d/\gamma)\)._

Proof.: \[\mathrm{Reg}_{\mathrm{ETC}}(T) :=\sum_{t=1}^{T}\langle\mathbf{\theta}^{(a_{t}^{*})}-\mathbf{\theta}^{(a_ {t})},\mathbf{x}_{t}\rangle\] \[\leq T_{0}+\sum_{t=1}^{T}\langle\mathbf{\theta}^{(a_{t}^{*})}-\mathbf{ \theta}^{(a_{t})},\mathbf{x}_{t}\rangle\] \[=T_{0}+\sum_{t=T_{0}+1}^{T}\langle\hat{\mathbf{\theta}}^{(a_{t}^{*})} _{T_{0}/2}-\hat{\mathbf{\theta}}^{(a_{t})}_{T_{0}/2},\mathbf{x}_{t}\rangle+\langle \mathbf{\theta}^{(a_{t}^{*})}-\hat{\mathbf{\theta}}^{(a_{t}^{*})}_{T_{0}/2},\mathbf{x}_ {t}\rangle+\langle\hat{\mathbf{\theta}}^{(a_{t})}_{T_{0}/2}-\mathbf{\theta}^{(a_{t})},\mathbf{x}_{t}\rangle\] \[\leq T_{0}+\sum_{t=T_{0}+1}^{T}|\langle\mathbf{\theta}^{(1)}-\hat{ \mathbf{\theta}}^{(1)}_{T_{0}/2},\mathbf{x}_{t}\rangle|+|\langle\mathbf{\theta}^{(0)}- \hat{\mathbf{\theta}}^{(0)}_{T_{0}/2},\mathbf{x}_{t}\rangle|\] \[\leq T_{0}+\sum_{t=T_{0}+1}^{T}\|\mathbf{\theta}^{(1)}-\hat{\mathbf{\theta }}^{(1)}_{T_{0}/2}\|_{2}\|\mathbf{x}_{t}\|_{2}+\|\mathbf{\theta}^{(0)}-\hat{\mathbf{ \theta}}^{(0)}_{T_{0}/2}\|_{2}\|\mathbf{x}_{t}\|_{2}\] \[\leq T_{0}+T\cdot\|\mathbf{\theta}^{(1)}-\hat{\mathbf{\theta}}^{

[MISSING_PAGE_EMPTY:21]

**Lemma B.12**.: _The following bound holds on the minimum eigenvalue of \(\sum_{s=1}^{T_{0}/2}\mathbf{x}_{s+k}\mathbf{x}_{s+k}^{\top}\) with probability \(1-\gamma\):_

\[\lambda_{min}\left(\sum_{s=1}^{T_{0}/2}\mathbf{x}_{s+k}\mathbf{x}_{s+k}^{\top} \right)\geq\frac{T_{0}}{6d}+4\sqrt{T_{0}\log(d/\gamma)}\]

Proof.: \[\lambda_{min}(\sum_{s=1}^{T_{0}/2}\mathbf{x}_{s+k}\mathbf{x}_{s+k}^{\top}) \geq\lambda_{min}(\sum_{s=1}^{T_{0}/2}\mathbf{x}_{s+k}\mathbf{x}_{s +k}^{\top}-\mathbb{E}[\mathbf{x}_{s+k}\mathbf{x}_{s+k}^{\top}])+\lambda_{min}( \sum_{s=1}^{T_{0}/2}\mathbb{E}[\mathbf{x}_{s+k}\mathbf{x}_{s+k}^{\top}])\] \[\geq\lambda_{min}(\sum_{s=1}^{T_{0}/2}\mathbf{x}_{s+k}\mathbf{x}_{ s+k}^{\top}-\mathbb{E}[\mathbf{x}_{s+k}\mathbf{x}_{s+k}^{\top}])+\sum_{s=1}^{T_{0 }/2}\lambda_{min}(\mathbb{E}[\mathbf{x}_{s+k}\mathbf{x}_{s+k}^{\top}])\]

where the inequalities follow from the fact that \(\lambda_{min}(A+B)\geq\lambda_{min}(A)+\lambda_{min}(B)\) for two Hermitian matrices \(A\), \(B\). Let \(Y_{T_{0}/2}:=\sum_{s=1}^{T_{0}/2}\mathbf{x}_{s+k}\mathbf{x}_{s+k}^{\top}- \mathbb{E}[\mathbf{x}_{s+k}\mathbf{x}_{s+k}^{\top}]\). Note that \(\mathbb{E}Y_{T_{0}/2}=\mathbb{E}Y_{0}=0\), \(-X_{s+k}:=\mathbb{E}[\mathbf{x}_{s+k}\mathbf{x}_{s+k}^{\top}]\), \(\mathbb{E}[-X_{s+k}]=0\), and \((-X_{s+k})^{2}\preceq 4I_{d}\). By Theorem A.1,

\[\mathbb{P}(\lambda_{max}(-Y_{T_{0}/2})\geq\alpha)\leq d\cdot\exp(-\alpha^{2}/1 6T_{0}).\]

Since \(-\lambda_{max}(-Y_{T_{0}/2})=\lambda_{min}(Y_{T_{0}/2})\),

\[\mathbb{P}(\lambda_{max}(Y_{T_{0}/2})\leq\alpha)\leq d\cdot\exp(-\alpha^{2}/16T _{0}).\]

Therefore, \(\lambda_{min}(Y_{T_{0}/2})\geq 4\sqrt{T_{0}\log(d/\gamma)}\) with probability \(1-\gamma\). We now turn our attention to lower bounding \(\lambda_{min}(\mathbb{E}[\mathbf{x

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_EMPTY:24]

We next analyze how much the potential changes per-round:

\[\log\left(\frac{W_{t+1}}{W_{t}}\right) =\log\left(\frac{\sum_{e\in\mathcal{E}}w_{t}(e)\exp\left(-\eta\widehat {\ell}_{t}(e)\right)}{W_{t}}\right)=\log\left(\sum_{e\in\mathcal{E}}p_{t}(e) \exp\left(-\eta\widehat{\ell}_{t}(e)\right)\right)\] \[\leq\log\left(\sum_{e\in\mathcal{E}}p_{t}(e)\cdot\left(1-\eta \widehat{\ell}_{t}(e)+\eta^{2}\widehat{\ell}_{t}^{2}(e)\right)\right)\qquad \qquad(e^{-x}\leq 1-x+x^{2},x>0)\] \[=\log\left(1-\eta\sum_{e\in\mathcal{E}}p_{t}(e)\widehat{\ell}_{t}( e)+\eta^{2}\sum_{e\in\mathcal{E}}p_{t}(e)\widehat{\ell}_{t}^{2}(e)\right)\qquad \qquad\qquad(\sum_{e\in\mathcal{E}}p_{t}(e)=1)\] \[\leq-\eta\sum_{e\in\mathcal{E}}p_{t}(e)\widehat{\ell}_{t}(e)+ \eta^{2}\sum_{e\in\mathcal{E}}p_{t}(e)\widehat{\ell}_{t}^{2}(e)\] \[=-\eta\sum_{e\in\mathcal{E}}\frac{q_{t}(e)-\gamma/|\mathcal{E}|} {(1-\gamma)}\widehat{\ell}_{t}(e)+\eta^{2}\sum_{e\in\mathcal{E}}\frac{q_{t}(e )-\gamma/|\mathcal{E}|}{(1-\gamma)}\widehat{\ell}_{t}^{2}(e)\] \[\leq-\eta\sum_{e\in\mathcal{E}}\frac{q_{t}(e)-\gamma/|\mathcal{E} |}{(1-\gamma)}\widehat{\ell}_{t}(e)+\eta^{2}\sum_{e\in\mathcal{E}}\frac{q_{t}( e)}{(1-\gamma)}\widehat{\ell}_{t}^{2}(e) \tag{11}\]

where the second inequality is due to the fact that \(\log x\leq x-1\) for \(x\geq 0\). In order for this inequality to hold we need to verify that:

\[1-\eta\sum_{e\in\mathcal{E}}p_{t}(e)\widehat{\ell}_{t}(e)+\eta^{2}\sum_{e\in \mathcal{E}}p_{t}(e)\widehat{\ell}_{t}^{2}(e)\geq 0,\]

or equivalently, that:

\[1-\eta\sum_{e\in\mathcal{E}}p_{t}(e)\widehat{\ell}_{t}(e)\geq 0 \tag{12}\]

We do so after we explain how to tune \(\eta\) and \(\gamma\).

We return to Equation (11); summing up for all rounds \(t\in[T]\) in Equation (11) we get:

\[\log\left(\frac{W_{T}}{W_{0}}\right)\leq-\eta\sum_{t\in[T]}\sum_{e\in \mathcal{E}}\frac{q_{t}(e)-\gamma/|\mathcal{E}|}{(1-\gamma)}\widehat{\ell}_{t }(e)+\eta^{2}\sum_{t\in[T]}\sum_{e\in\mathcal{E}}\frac{q_{t}(e)}{(1-\gamma)} \widehat{\ell}_{t}^{2}(e) \tag{13}\]

Using Equation (9) and Equation (10) we have that: \(\log(W_{T}/W_{0})\geq-\eta\sum_{t\in[T]}\widehat{\ell}_{t}(e^{*})-\log| \mathcal{E}|\). Combining this with the upper bound on \(\log(W_{T}/W_{0})\) from Equation (13) and multiplying both sides by \((1-\gamma)/\eta\) we get:

\[\sum_{t\in[T]}\sum_{e\in\mathcal{E}}\left(q_{t}(e)-\frac{\gamma}{|\mathcal{E} |}\right)\widehat{\ell}_{t}(e)-(1-\gamma)\sum_{t\in[T]}\widehat{\ell}_{t}(e^{* })\leq\eta\sum_{t\in[T]}\sum_{e\in\mathcal{E}}q_{t}(e)\widehat{\ell}_{t}^{2}( e)+(1-\gamma)\frac{\log(|\mathcal{E}|)}{\eta}\]

We can slightly relax the right hand side using the fact that \(\gamma<1\) and get:

\[\sum_{t\in[T]}\sum_{e\in\mathcal{E}}\left(q_{t}(e)-\frac{\gamma}{|\mathcal{E} |}\right)\widehat{\ell}_{t}(e)-(1-\gamma)\sum_{t\in[T]}\widehat{\ell}_{t}(e^{ *})\leq\eta\sum_{t\in[T]}\sum_{e\in\mathcal{E}}q_{t}(e)\widehat{\ell}_{t}^{2}( e)+\frac{\log(|\mathcal{E}|)}{\eta}\]

Taking expectations (wrt the draw of the algorithm) on both sides of the above expression and using our derivations for the first and second moment (Equation (7) and Equation (8) respectively) we get:

\[\sum_{t\in[T]}\sum_{e\in\mathcal{E}}\left(q_{t}(e)-\frac{\gamma}{|\mathcal{E} |}\right)\ell_{t}(e)-(1-\gamma)\sum_{t\in[T]}\ell_{t}(e^{*})\leq\eta\sum_{t \in[T]}\sum_{e\in\mathcal{E}}q_{t}(e)\frac{\lambda^{2}}{q_{t}(e)}+\frac{\log( |\mathcal{E}|)}{\eta}\]

Using the fact that \(\ell_{t}(\cdot)\in[0,\lambda]\) the above becomes:

\[\mathbb{E}\left[\operatorname{Reg}^{\varepsilon}(T)|\mathcal{C}\right]=\sum_{ t\in[T]}\sum_{e\in\mathcal{E}}q_{t}(e)\ell_{t}(e)-\sum_{t\in[T]}\ell_{t}(e^{*}) \leq\eta T\lambda^{2}|\mathcal{E}|+\frac{\log(|\mathcal{E}|)}{\eta}+\gamma T\]

[MISSING_PAGE_FAIL:26]

Adding and subtracting \(\mathbf{x}_{t}^{\prime}\) from quantity \(Q_{t}\), we have:

\[Q_{t} =\left\langle\widehat{\boldsymbol{\theta}}^{(0)}-\widehat{ \boldsymbol{\theta}}^{(1)},\mathbf{x}_{t}-\mathbf{x}_{t}^{\prime}\right\rangle+ \left\langle\widehat{\boldsymbol{\theta}}^{(0)}-\widehat{\boldsymbol{\theta}}^ {(1)},\mathbf{x}_{t}^{\prime}\right\rangle\] \[\leq\left\langle\widehat{\boldsymbol{\theta}}^{(0)}-\widehat{ \boldsymbol{\theta}}^{(1)},\mathbf{x}_{t}-\mathbf{x}_{t}^{\prime}\right\rangle- \delta\|e^{*}\|\] (Equation (16)) \[\leq\left\|\widehat{\boldsymbol{\theta}}^{(0)}-\widehat{ \boldsymbol{\theta}}^{(1)}\right\|\cdot\|\mathbf{x}_{t}-\mathbf{x}_{t}^{\prime }\|-\delta\|e^{*}\|\] (Cauchy-Schwarz) \[\leq\|e^{*}\|\cdot\delta-\delta\|e^{*}\|.\]

As a result:

\[SDE(G_{2})\leq 2\varepsilon T \tag{17}\]

Moving on to the analysis of \(SDE(G_{3})\):

\[SDE(G_{3})=\sum_{t\in G_{3}}\left\langle\boldsymbol{\theta}^{(0)}-\boldsymbol {\theta}^{(1)},\mathbf{x}_{t}\right\rangle\]

Again, we use \(\widehat{\boldsymbol{\theta}}^{(1)}\) and \(\widehat{\boldsymbol{\theta}}^{(0)}\) the points that \(e^{*}=\widehat{\boldsymbol{\theta}}^{(1)}-\widehat{\boldsymbol{\theta}}^{(0)}\). Adding and subtracting \(\langle e^{*},\mathbf{x}_{t}\rangle\) and following the same derivations as in \(SDE(G_{3})\), we have that:

\[SDE(G_{3})\leq 2\varepsilon T+\sum_{t\in G_{3}}\underbrace{\left\langle \widehat{\boldsymbol{\theta}}^{(1)}-\widehat{\boldsymbol{\theta}}^{(0)}, \mathbf{x}_{t}\right\rangle}_{Q_{t}} \tag{18}\]

Since \(a_{t,e^{*}}=0\), then it must have been the case that \(\mathbf{x}_{t}^{\prime}=\mathbf{x}_{t}\); this is because the agent would not spend effort to strategize if they would still be assigned action \(0\). For this reason, it must be that \(Q_{t}\leq 0\).

Combining the upper bounds for \(SDE(G_{2})\) and \(SDE(G_{3})\) in Equation (15), we have that \(SDE(T)\leq 4\varepsilon T\).

Putting everything together, we have that the regret is comprised by the regret incurred on the discretized grid and the strategic discretization error, i.e.,

\[\mathbb{E}[\mathrm{Reg}(T)]\leq 3\sqrt{T|\mathcal{E}|\sigma\log(T)\log(| \mathcal{E}|)}+4\varepsilon T=3\sqrt{Td\left(\frac{1}{\varepsilon}\right)^{d} \sigma\log(T)\log(1/\varepsilon)}+4\varepsilon T\]

Tuning \(\varepsilon=\left(\frac{d\sigma\log T}{T}\right)^{1/(d+2)}\) we get that the regret is:

\[\mathbb{E}[\mathrm{Reg}(T)]\leq 6T^{(d+1)/(d+2)}(d\sigma\log T)^{1/(d+2)}= \widetilde{\mathcal{O}}\left(T^{(d+1)/(d+2)}\right).\]

## Appendix D Extension to trembling hand best-response

Observe that when lazy tiebreaking (Definition 2.1), if agent \(t\) modifies their context they modify it by an amount \(\delta_{L}\) such that

\[\delta_{L,t}:=\min_{0\leq\eta\leq\delta}\eta\] \[\text{s.t. }\pi_{t}(\mathbf{x}_{t}^{\prime})=1\] \[\|\mathbf{x}_{t}^{\prime}-\mathbf{x}_{t}\|_{2}=\eta.\]

We define \(\gamma\)_-trembling hand_ tiebreaking as \(\delta_{TH,t}=\delta_{L,t}+\alpha_{t}\), where \(\alpha_{t}\in[0,\min\{\delta-\delta_{L,t},\gamma\}]\) may be chosen arbitrarily. Our results in Section 3 may be extended to trembling hand tiebreaking by considering the following redefinition of a clean point:

**Condition D.1** (Sufficient condition for \(\mathbf{x}^{\prime}=\mathbf{x}\)).: _Given a shifted linear policy parameterized by \(\boldsymbol{\beta}^{(1)}\in\mathbb{R}^{d}\), we say that a context \(\mathbf{x}^{\prime}\) is clean if \(\langle\boldsymbol{\beta}^{(1)},\mathbf{x}^{\prime}\rangle>(\delta+\gamma)\| \boldsymbol{\beta}^{(1)}\|_{2}+r_{0}\)._No further changes are required. This will result in a slightly worse constant in Theorem 3.3 (i.e. all instances of \(\delta\) will be replaced by \(\delta+\gamma\)). Our algorithms and results in Section 4 do not change. Our definition of trembling hand best-response is similar in spirit to the \(\epsilon\)-best-response in Haghtalab et al. [26]. Specifically, Haghtalab et al. [26] study a Stackelberg game setting in which the follower best-responds \(\epsilon\)-optimally. In our trembling hand setting, the strategic agent can also be thought of as \(\epsilon\)-best responding (using the language of [26]), although it is important to note that an \(\epsilon\)-best response for the agent in our setting will cause the agent to only strategize _more_ than necessary.