# Near-Optimal Distributionally Robust Reinforcement Learning with General \(L_{p}\) Norms

 Pierre Clavier

Ecole Polytechnique, Inria

Laixi Shi

Caltech

&Erwan Le Pennec

Ecole polytechnique

&Eric Mazumdar

Ecole polytechnique

&Adam Wierman

Caltech

&Matthieu Geist

Cohere

CMAP, CNRS, Ecole Polytechnique, Institut Polytechnique de Paris, 91120 Palaiseau

Inria Paris, HeKA, 75015 Paris, France

###### Abstract

To address the challenges of sim-to-real gap and sample efficiency in reinforcement learning (RL), this work studies distributionally robust Markov decision processes (RMDPs) -- optimize the worst-case performance when the deployed environment is within an uncertainty set around some nominal MDP. Despite recent efforts, the sample complexity of RMDPs has remained largely undetermined. While the statistical implications of distributional robustness in RL have been explored in some specific cases, the generalizability of the existing findings remains unclear, especially in comparison to standard RL. Assuming access to a generative model that samples from the nominal MDP, we examine the sample complexity of RMDPs using a class of generalized \(L_{p}\) norms as the 'distance' function for the uncertainty set, under two commonly adopted \(sa\)-rectangular and \(s\)-rectangular conditions. Our results imply that RMDPs can be more sample-efficient to solve than standard MDPs using generalized \(L_{p}\) norms in both \(sa\)- and \(s\)-rectangular cases, potentially inspiring more empirical research. We provide a near-optimal upper bound and a matching minimax lower bound for the \(sa\)-rectangular scenarios. For \(s\)-rectangular cases, we improve the state-of-the-art upper bound and also derive a lower bound using \(L_{\infty}\) norm that verifies the tightness.

## 1 Introduction

Reinforcement learning (RL) (Sutton, 1988) is a popular paradigm in machine learning, particularly noted for its success in practical applications. The RL framework, usually modeled within the context of a Markov decision process (MDP), focuses on learning effective decision-making strategies based on interactions with a fixed environment. However, the work of Mannor et al. (2004), among others, has highlighted a vulnerability in RL strategies, revealing the sensitivity to inherent shift or estimation errors in the reward and transition probabilities. A specific example of this is when, because of a sim-to-real gap, policies learned in idealized environments fail when deployed in environments with slight changes or adversarial perturbations (Klopp et al., 2017; Mahmood et al., 2018).

To address this issue, distributionally robust RL, usually formulated as robust MDPs (RMDPs), proposed by Iyengar (2005) and Nilim and El Ghaoui (2005), have attracted considerable attention. RMDPs are formulated as max-min problems, seeking policies that are resilient to model environmentperturbations within a specified uncertainty set. Despite the robustness benefits, solving RMDPs is NP-hard for general uncertainty sets (Nilim and El Ghaoui, 2005). To overcome this challenge, the rectangularity condition is often adopted so that the uncertainty set can be decomposed as products of independent subsets for each state or state-action pair, denoted as \(s\)-rectangular or \(sa\)-rectangular assumptions (see Definition 4 and 5). These assumptions facilitate computation traceability of methods such as robust value iteration and robust policy iteration, preserving many structural properties of MDPs (Ho et al., 2021). The \(s\)-rectangularity condition, though with less restrictive structure assumption, impose more challenges for algorithm design, while the \(sa\)-rectangularity condition allows for deterministic optimal policies akin to non-robust MDPs (Wiesemann et al., 2013). Note that dealing with uncertainty in transition kernels is much more difficult than that in rewards (Kumar et al., 2022; Derman et al., 2021).

The question of sample efficiency is central in RL problems ranging from practice to theory. Although minimax sample efficiency has been achieved for standard MDPs (Azar et al., 2013; Li et al., 2023), this goal in general remains open in RMDPs. Specifically, there exists prior work studying the sample complexity of distributionally robust RL for a few specific divergences such as total variation (\(TV\)) distance, \(\chi^{2}\) divergence, Kullback-Leibler divergence (\(KL\)) divergence, and Wasserstein distance (see discussions in Appendix A) (Yang et al., 2022; Zhou et al., 2021; Panaganti and Kalathil, 2022). While such results remain unclear for more general class, such as the general smooth \(L_{p}\) norms (see Def. 1). To the best of our knowledge, minimax optimal sample complexity for the full range of uncertainty level has only been achieved for one case \(-TV\) distance (Shi et al., 2023). In this work, we focus on understanding the sample complexity of RMDPs with a general smooth \(L_{p}\) that will be defined in Def. 1. This generalized result is appealing for both practice and theory. In practice, numerous applications are based on optimizations or learning approaches that involve general norms beyond those specific cases that have been studied in prior works. Additionally, optimizing \(L_{p}\) norm weighted ambiguity sets for robust MDPs has been proposed in the context of RMDPs in Russel et al. (2019), which justifies our formulation. Theoretically, prior work has characterized the sample complexity of RMDPs for some specific norms have suggested intriguing insights about the statistical implications of distributional robustness in RL. It is interesting to further understand the statistical cost of robust RL in more general scenarios. One area of focus is the contrast between the sample efficiency of solving distributionally robust RL and solving standard RL. In particular, for the specific case of \(TV\) distance, Shi et al. (2023) shows that the sample complexity for solving robust RL is at least the same as and sometimes (when the uncertainty level is relatively large) could be smaller than that of standard RL. This motivates the following open question:

_Is distributionally robust RL more sample efficient than standard RL for some general class of norms (Def. (1))_?

A second question is about the comparisons between the sample complexity of solving \(s\)-rectangular RMDPs and that of solving \(sa\)-rectangular RMDPs. Note that \(s\)-rectangular RMDPs involve more complex optimization problems with additional variables (uncertainty levels for each action) to optimize. This leads to a richer class of optimal policy candidates--stochastic policies in \(s\)-rectangular cases, in contrast to the class of deterministic policies for \(sa\)-rectangular cases. In addition, existing sample complexity upper bounds for solving \(s\)-rectangular RMDPs are larger than that for solving \(sa\)-rectangularity (Yang et al., 2022) for the investigated cases. This motivates the curious question:

_Does solving \(s\)-rectangular RMDPs require more samples than solving \(sa\)-rectangular RMDPs with general smooth \(L_{p}\) norms defined in Def. 1?_

**Main contributions.** In this paper, we address each of the two questions discussed above. In particular, we provide the first sample complexity analysis for RMDPs with general \(L_{p}\) norms (cf. Def. 1) under both the \(s\)- and \(sa\)-rectangularity conditions. For convenience, we present detailed comparisons between the prior arts and our results in Table 1 for quick reference and discuss the contributions and their implications as below.

\(\bullet\) Considering the first question, we illustrate our results in both \(sa\)- and \(s\)-rectangular cases in Figure 1. In the case of \(sa\)-rectangularity, we derive a sample complexity upper bound for RMDPs using general smooth \(L_{p}\) norms (cf. Theorem 1) in the order of \(\widetilde{O}\left(\frac{SA}{\left(1-\gamma\right)^{2}\max\{1-\gamma,C_{g} \sigma\}\varepsilon^{2}}\right)\). Here, \(\sigma\) is the uncertainty level/radius of the uncertainty set, and \(C_{g}>0\) is a positive constant related to the geometry of the norm defined in Def. 1. For classical \(L_{p}\) norms, \(C_{g}\geq 1\) so we can directly relax this constant to \(1\) to obtain the result in Table 1. In addition, we provide a matching minimax lower bound (cf. Theorem 2) that confirms the near-optimality of the upper bound for almost full range of the uncertainty level. Our results match the near-optimal sample complexity derived in Shi et al. [2023] for the specific case using TV distance, while holding for broader cases using general \(L_{p}\) norms. The results rely on a new dual optimization form for \(sa\)-rectangular RMDPs and reveal the relationship between the sample complexity and this new dual form -- the infinite span seminorm (controlled in Lemma 5), which may be of independent interest.

In the case of \(s\)-rectangularity, we provide a sample complexity upper bound for solving RMDPs with general smooth \(L_{p}\) norms in the order of \(\widetilde{O}\left(\frac{SA}{(1-\gamma)^{2}\max\{1-\gamma,C_{g}\min_{s}\|\pi_{ s}\|,\tilde{\sigma}\}^{2}}\right)\) with \(\|.\|_{*}\) the dual norm and \(\tilde{\sigma}\) the radius of the ball in the \(s\)-rectangular uncertainty set. This result improves the prior art \(\widetilde{O}\left(\frac{SA}{(1-\gamma)^{4}e^{2}}\right)\) in Clavier et al. [2023] for classical \(L_{p}\) -- by at least a factor of \(O\left(\frac{1}{1-\gamma}\right)\) when \(\tilde{\sigma}\lesssim 1-\gamma\). Furthermore, we present a lower bound for a representative case with \(L_{\infty}\) norm, which corroborates the tightness of the upper bound. To the best of our knowledge, this is the first lower bound for solving RMDPs with \(s\)-rectangularity.

\(\bullet\) We highlight the technical contributions as below. For the upper bounds, regarding optimization contribution, we derive new dual optimization problem forms for both \(sa-\) and \(s-\) rectangular cases (Lemma 3 and 4), which is the foundation of the covering number argument in finite-sample analysis. From a statistical point of view, a new concentration lemma (See Lemma 8 for dual forms) is introduced to obtain a lower sample complexity than standard RL, controlling the infinite span semi norm of the value function, both for \(sa-\) and \(s-\) rectangular case are derived (See Lemma 5 and 6). For the lower bound, the technical contributions are mainly in \(s\)-rectangular cases, which involves entire new challenges compared to \(sa\)-rectangularity case: the optimal policies can be stochastic and hard to be characterized as a closed form, compared to the deterministic one in \(sa\)-rectangular cases. Therefore, we construct new hard instances for \(s\)-rectangular cases that is distinct from those used in \(sa\)-rectangular cases or standard RL.

\(\bullet\) Considering the second question, as illustrated in Figure 1, our results highlight that robust RL is at least the same as and sometimes can be more sample-efficient to solve than standard RL for general smooth \(L_{p}\) norms (cf. Def. 1). This insight is of significant practical importance and serves to provide crucial motivation for the use and study of distributionally robustness in RL. Notably, robust RL does not only reduce the vulnerability of RL policy to estimation errors and sim-to-real gaps, but also leads to better data efficiency. In terms of comparing the statistical implications of \(sa\)- and \(s\)- rectangularity, our results show that solving \(s\)-rectangular RMDPs is not harder than solving \(sa\)-rectangular RMDPs in terms of sample requirement (See Theorem 3 and Figure 2, Right).

## 2 Problem Formulation: Robust Markov Decision Processes

In this section, we formulate distributionally robust Markov decision processes (RMDPs) in the discounted infinite-horizon setting, introduce the sampling mechanism, and describe our goal.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \multirow{3}{*}{Result type} & \multirow{3}{*}{Reference} & \multirow{3}{*}{Distance} & \multicolumn{2}{c|}{\(sa\)-rectangularity} & \multicolumn{2}{c|}{\(s\)-rectangularity} \\ \cline{4-6}  & & & \(0<\sigma\lesssim 1-\gamma\) & \(1-\gamma\leq\sigma<\sigma_{\max}\) & \(0<\tilde{\sigma}\lesssim 1-\gamma\) & \(1-\gamma\leq\tilde{\sigma}<\tilde{\sigma}_{\max}\) \\ \hline \multirow{7}{*}{Upper bound} & Yang et al. [2022a] & TV & \(\frac{S^{4}A_{2}+s^{2}}{S^{(1-\gamma)^{2}}}\) & \(\frac{S^{4}A_{2}+s^{2}}{S^{(1-\gamma)^{2}}}\) & \(\frac{S^{4}A_{2}^{2}+s^{2}}{S^{(1-\gamma)^{2}}}\) & \(\frac{S^{4}A_{2}^{2}+s^{2}}{S^{(1-\gamma)^{2}}}\) \\ \cline{2-6}  & Panaganti and Kaluthi [2022] & TV & \(\frac{S^{4}A_{2}}{S^{(1-\gamma)^{2}}}\) & \(\frac{S^{4}A_{2}}{(1-\gamma)^{2}}\) & \(\times\) & \(\times\) \\ \cline{2-6}  & Shi et al. [2023] & TV & \(\frac{S^{4}A_{2}}{(1-\gamma)^{2}}\) & \(\frac{S^{4}A_{2}}{(1-\gamma)^{2}}\) & \(\times\) & \(\times\) \\ \cline{2-6}  & Clavier et al. [2023] & \(L_{p}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) \\ \cline{2-6}  & **This paper** & \(L_{p}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) \\ \cline{2-6}  & **This paper** & General \(L_{p}\) [1] & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) \\ \hline \multirow{7}{*}{Lower bound} & Yang et al. [2022a] & TV & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}(1-\gamma)^{2}}{S^{(1-\gamma)^{2}}}\) & \(\times\) & \(\times\) \\ \cline{2-6}  & Shi et al. [2023] & TV & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\times\) & \(\times\) \\ \cline{1-1} \cline{2-6}  & **This paper** & \(L_{p}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\times\) & \(\times\) \\ \cline{1-1} \cline{2-6}  & **This paper** & \(L_{\infty}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) & \(\frac{S_{4}}{(1-\gamma)^{2}}\) \\ \hline \end{tabular}
\end{table}
Table 1: Comparisons with prior results (up to log terms) regarding finding an \(\varepsilon\)-optimal policy for the distributionally RMDP, where \(\sigma\) is the radius of the uncertainty set and \(\sigma_{\max}\) defined in Theorem 1.

Standard Markov decision processes (MDPs).A discounted infinite-horizon MDP is represented by \(\mathcal{M}=(\mathcal{S},\mathcal{A},\gamma,P,r)\), where \(\mathcal{S}=\{1,\cdots,S\}\) and \(\mathcal{A}=\{1,\cdots,A\}\) are the finite state and action spaces, respectively, \(\gamma\in[0,1)\) is the discounted factor, \(P:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\) denotes the probability transition kernel, and \(r:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) is the immediate reward function, which is assumed to be deterministic. Moreover, we assume that the reward function is bounded in \((0,1)\) without loss of generality of the results due to the variance reward invariance. Finally we denote \(1_{A}\) or \(1_{S}\) the unitary vector of respectively dimension \(A\) or \(S\). Moreover, \(e_{s}\) is the standard unitary vector supported on \(s\). The policy we are looking for is denoted by \(\pi:\mathcal{S}\rightarrow\Delta(\mathcal{A})\), which specifies the probability of action selection over the action space in any state. Note that if the policy is deterministic in the \(sa\)-rectangular case, we overload the notation and refer to \(\pi(s)\) as the action selected by the policy \(\pi\) in state \(s\). Finally, to characterize the cumulative reward, the value function \(V^{\pi,P}\) for any policy \(\pi\) under the transition kernel \(P\) is defined by \(\forall s\in\mathcal{S}\)

\[V^{\pi,P}(s)\coloneqq\mathbb{E}_{\pi,P}\left[\sum_{t=0}^{\infty}\gamma^{t}r \big{(}s_{t},a_{t}\big{)}\,\Big{|}\,s_{0}=s\right]. \tag{1}\]

The expectation is taken over the randomness of the trajectory \(\{s_{t},a_{t}\}_{t=0}^{\infty}\) generated by executing the policy \(\pi\) under the transition kernel \(P\), such that \(a_{t}\sim\pi(\cdot\,|\,s_{t})\) and \(s_{t+1}\sim P(\cdot\,|\,s_{t},a_{t})\) for all \(t\geq 0\). In the same way, the Q function \(Q^{\pi,P}\) associated with any policy \(\pi\) under the transition kernel \(P\) is defined using expectation taken over the randomness of the trajectory under policy \(\pi\) as

\[Q^{\pi,P}(s,a)\coloneqq\mathbb{E}_{\pi,P}\left[\sum_{t=0}^{\infty}\gamma^{t}r \big{(}s_{t},a_{t}\big{)}\,\Big{|}\,s_{0},a_{0}=s,a\right], \tag{2}\]

Distributionally robust MDPs.We consider distributionally robust MDPs (RMDPs) in the discounted infinite-horizon setting, denoted by \(\mathcal{M}_{\text{rob}}=\{\mathcal{S},\mathcal{A},\gamma,\mathcal{U}_{\| \cdot\|}^{\sigma}(P^{0}),r\}\), where \(\mathcal{S},\mathcal{A},\gamma,r\) are the same sets and parameters as in standard MDPs. The main difference compared to standard MDPs is that instead of assuming a fixed transition kernel \(P\), it allows the transition kernel to be arbitrarily chosen from a prescribed uncertainty set \(\mathcal{U}_{\|\cdot\|}^{\sigma}(P^{0})\) centered around a _nominal_ kernel \(P^{0}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\), where the uncertainty set is specified using some called \(L_{p}\) smooth norm denoted \(\left\|\cdot\right\|\) defined in of radius \(\sigma>0\) defined in 1.

**Definition 1** (General smooth \(L_{p}\) norms and dual norms).: _A norm \(\|\cdot\|\) is said to be a general smooth \(L_{p}\), norm, \(p>1\) if_

* _for all_ \(x\in\mathbb{R}^{n}\)_,_ \(\left\|x\right\|\coloneqq\left\|x\right\|_{p,w}=(\sum_{k=1}^{n}w_{k}(|x_{k}|) ^{p})^{1/p}\) _for some_ \(w\in\mathbb{R}^{n}_{+},\) _being an arbitrary positive vector._
* _it is twice continuously differentiable Rudin et al._ _[_1964_]_ _with the supremum of the Hessian Matrix over the simplex_ \(C_{S}=\sup_{x\in\Delta_{S}}\left\|\nabla^{2}\left\|x\right\|\right\|_{2^{\prime }}\)_, where_ \(\left\|\cdot\right\|_{2}\) _here is the spectral norm.

Figure 1: **Left**: Sample complexity results for RMDPs with \(sa\)- and \(s\)-rectangularity with \(L_{p}\) with comparisons to prior arts [23] (for \(L_{1}\) norm, or called total variation distance) and [19] ; **Right:** The data and instance-dependent sample complexity upper bound of solving \(s\)-rectangular dependency RMDPs with \(L_{p}\) norms.

_Finally, we denote the dual norm of \(\left\lVert\cdot\right\rVert\) as \(\left\lVert\cdot\right\rVert_{*}\) s.t. \(\left\lVert y\right\rVert_{*}\coloneqq\max_{x}x^{T}y:\left\lVert x\right\rVert\leq 1\). Moreover, for any metric \(\left\lVert\cdot\right\rVert\), we define \(C_{g}\)\(C_{g}\coloneqq 1/\min_{s}\left\lVert e_{s}\right\rVert\) where \(e_{s}\in\mathbb{R}^{S}\) is the standard basis vector with only \(1\) at the \(s\)-th entry, otherwise \(0\)._

Note that the quantity \(C_{S}\) exists, as the Hessian of a \(C^{2}\) functional is continuous and because the simplex is a compact set, so by Extreme Value Theorem Rudin et al. (1964), \(C_{S}\) is finite. For example, considering \(L_{p}\) norms with any \(p\geq 2\), \(C_{S}\) is bounded by \((p-1)S^{1/q}\). (See (154) ) This definition is general and includes \(L_{p}\) norms (Rudin et al., 1964) for any \(p\geq 2\) and all rescaled and weighted norms. Moreover, we could extend our results to a larger set than the one of the norms defined in Def. 1, where the further discussion can be found in Appendix B. However, it does not include divergences such as \(KL\) and \(\chi^{2}\). Not that the case of \(TV\) which is not \(C^{2}\) smooth is treated independently with different arguments the proof but has the same sample complexity. In particular, given the nominal transition kernel \(P^{0}\) and some uncertainty level \(\sigma\), the uncertainty set--with arbitrary smooth \(L_{p}\) norm metric \(\left\lVert\ \right\rVert\coloneqq\mathbb{R}^{S}\times\to\mathbb{R}^{+}\) in \(sa\) rectangular case or from \(\mathbb{R}^{S\times\mathcal{A}}\) in the \(s\)-rectangular case, is specified as \(\mathcal{U}^{\sigma}_{\parallel\cdot\parallel}(P^{0})\coloneqq\otimes_{s,a} \mathcal{U}^{\sigma,\sigma}_{\parallel\cdot\parallel}(P^{0}_{s,a})\)

\[\mathcal{U}^{\text{sa},\sigma}_{\parallel\cdot\parallel}(P^{0}_{ s,a})\coloneqq\left\{P_{s,a}\in\Delta(\mathcal{S}):\left\lVert P_{s,a}-P^{0}_{ s,a}\right\rVert\leq\sigma\right\}, \tag{3}\] \[P_{s,a}\coloneqq P(\cdot\left\lVert\,s,a\right)\in\mathbb{R}^{1 \times S},P^{0}_{s,a}\coloneqq P^{0}(\cdot\left\lVert\,s,a\right)\in\mathbb{R }^{1\times S}, \tag{4}\]

where we denote a vector of the transition kernel \(P\) or \(P^{0}\) at state-action pair \((s,a)\). In other words, the uncertainty is imposed in a decoupled manner for each state-action pair, obeying the so-called \(sa\)-rectangularity (Zhou et al., 2021; Wiesemann et al., 2013). More generally, we define \(s\)-rectangular MDPs as \(\mathcal{U}^{\sigma}_{\parallel\cdot\parallel}(P)=\otimes_{s}\mathcal{U}^{ \sigma,\widetilde{\sigma}}_{\parallel\cdot\parallel}(P_{s}),\) for the general smooth \(L_{p}\) norm \(\left\lVert\cdot\right\rVert\). The uncertainty is imposed in a decoupled manner for each state pair, and a fixed budget given a state for all action is defined. To get a similar meaning for the radius of the ball between \(sa\)-rectangular and \(s\)-rectangular assumptions, we need to rescale the radius depending on the norm like in Yang et al. (2022b). The \(s\)-uncertainty set is then defined using the rescaled radius \(\tilde{\sigma}\) as

\[\mathcal{U}^{\text{a},\tilde{\sigma}}_{\parallel\cdot\parallel} (P_{s})\coloneqq\Big{\{}P^{\prime}_{s}\in\Delta(\mathcal{S})^{\mathcal{A}}: \left\lVert P^{\prime}_{s}-P_{s}\right\rVert\leq\tilde{\sigma}=\sigma\left\lVert 1_{ \mathcal{A}}\right\rVert\Big{\}}, \tag{5}\] \[P_{s}\coloneqq P(\cdot,\cdot\left\lvert\,s\right\rvert)\in \mathbb{R}^{1\times SA},\quad P^{0}_{s}\coloneqq P^{0}(\cdot,\cdot\left\lvert \,s\right\rvert)\in\mathbb{R}^{1\times SA}. \tag{6}\]

where \(1_{A}\in\mathbb{R}^{A}\) denotes the unitary vector. For the specific case of respectively \(L_{1}\),\(L_{p}\) and \(L_{\infty}\) norm, \(\tilde{\sigma}\) is equal to \(|\sigma\mathcal{A}|,\sigma|\mathcal{A}|^{1/p}\) and \(\sigma\). Note that this scaling allows for a fair comparison between \(sa\)- and \(s\)-rectangular MDPs. In RMDPs, we are interested in the worst-case performance of a policy \(\pi\) over all the possible transition kernels in the uncertainty set. This is measured by the _robust value function_\(V^{\pi,\sigma}\) and the _robust Q-function_\(Q^{\pi,\sigma}\) in \(\mathcal{M}_{\text{rob}}\), defined respectively as \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\)

\[V^{\pi,\sigma}(s)\coloneqq\inf_{P\in\mathcal{U}^{\text{a},\sigma}_{\parallel \cdot\parallel}(P^{0})}V^{\pi,P}(s),\quad Q^{\pi,\sigma}(s,a)\coloneqq\inf_{P \in\mathcal{U}^{\text{a},\sigma}_{\parallel\cdot\parallel}(P^{0})}Q^{\pi,P}( s,a). \tag{7}\]

Similarly for \(s\)-rectangularity, the value function is denoted \(V^{\pi,\sigma}_{s}(s)\coloneqq\inf_{P\in\mathcal{U}^{\sigma,\widetilde{\sigma }}_{\parallel\cdot\parallel}(P^{0})}V^{\pi,P}(s)\).

Optimal robust policy and robust Bellman operator.As a generalization of properties of standard MDPs in the \(sa\)-rectangular robust case, it is well-known that there exists at least one deterministic policy that maximizes the robust value function (resp. robust Q-function) simultaneously for all states (resp. state-action pairs) (Iyengar, 2005; Nilim and El Ghaoui, 2005) but not in the \(s\)-rectangular case. Therefore, we denote the _optimal robust value function_ (resp. _optimal robust Q-function_) as \(V^{*,\sigma}\) (resp. \(Q^{*,\sigma}\)), and the optimal robust policy as \(\pi^{\star}\), which satisfy \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\)

\[V^{*,\sigma}(s)\coloneqq V^{\pi^{\star},\sigma}(s)=\max_{\pi}V^{ \pi,\sigma}(s),\quad Q^{*,\sigma}(s,a)\coloneqq Q^{\pi^{*},\sigma}(s,a)= \max_{\pi}Q^{\pi,\sigma}(s,a). \tag{8a}\]

A key concept in RMDPs is a generalization of Bellman's optimality principle, encapsulated in the following _robust Bellman consistency equation_ (resp. _robust Bellman optimality equation_):

\[\forall(s,a)\in\mathcal{S}\times\mathcal{A},\quad Q^{\pi,\sigma }(s,a)=r(s,a)+\gamma\inf_{\mathcal{P}\in\mathcal{U}^{\text{a},\sigma}_{ \parallel\cdot\parallel}(P^{0}_{s,a})}\mathcal{P}V^{\pi,\sigma}, \tag{9a}\] \[\forall(s,a)\in\mathcal{S}\times\mathcal{A},\quad Q^{*,\sigma}(s,a )=r(s,a)+\gamma\inf_{\mathcal{P}\in\mathcal{U}^{\text{a},\sigma}_{\parallel \cdot\parallel}(P^{0}_{s,a})}\mathcal{P}V^{*,\sigma}. \tag{9b}\]for the \(sa\)-rectangular case and same equation replacing \(P^{0}_{s,a}\) by \(P^{0}_{s}\) and \(\sigma\) by \(\tilde{\sigma}\). The robust Bellman operator (Iyengar, 2005; Nilim and El Ghaoui, 2005) is denoted by \(\mathcal{T}^{\sigma}(\cdot):\mathbb{R}^{SA}\rightarrow\mathbb{R}^{SA}\)

\[\mathcal{T}^{\sigma}(Q^{\pi})(s,a)\coloneqq r(s,a)+\gamma\inf_{ \mathcal{P}\in\mathcal{U}^{\pi,\sigma}_{\|\cdot\|}(P^{0}_{s,a})}\mathcal{P}V, \quad\text{with}\quad V(s)\coloneqq\max_{\pi}Q^{\pi}(s,a)^{\cdot} \tag{10}\]

for \(sa\)-rectangular MDPs. Given that \(Q^{\star,\sigma}\) is the unique-fixed point of \(\mathcal{T}^{\sigma}\) one can recover the optimal robust value function and Q-function using a procedure termed _distributionally robust value iteration_ (\(DRVI\)). Generalizing the standard value iteration, \(DRVI\) starts from some given initialization and recursively applies the robust Bellman operator until convergence. As has been shown previously, this procedure converges rapidly due to the \(\gamma\)-contraction property of \(\mathcal{T}^{\sigma}\) with respect to the \(L_{\infty}\) norm (Iyengar, 2005; Nilim and El Ghaoui, 2005).

## 3 Distributionally Robust Value Iteration

Generative model-based sampling.Following Zhou et al. (2021); Panaganti and Kalathil (2022), we assume access to a generative model or a simulator (Kearns and Singh, 1999), which allows us to collect \(N\) independent samples for each state-action pair generated based on the _nominal_ kernel \(P^{0}\): \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\), \(s_{i,s,a}\stackrel{{ i.i.d}}{{\sim}}P^{0}(\cdot\,|\,s,a),\quad i=1,2,\cdots,N.\) The total sample size is, therefore, \(NSA\). We consider a model-based approach tailored to RMDPs, which first constructs an empirical nominal transition kernel based on the collected samples and then applies distributionally robust value iteration (DRVI) to compute an optimal robust policy. As we decouple the statistical estimation error and the optimization error, we exhibit an algorithm that can achieve arbitrary small error \(\epsilon_{opt}\) in the empirical MDP defined as an empirical nominal transition kernel \(\widehat{P}^{0}\in\mathbb{R}^{SA\times S}\) that can be constructed on the basis of the empirical frequency of state transitions, i.e. \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\)

\[\widehat{P}^{0}(s^{\prime}\,|\,s,a)\coloneqq\frac{1}{N}\sum_{i=1}^ {N}\mathds{1}\big{\{}s_{i,s,a}=s^{\prime}\big{\}}, \tag{11}\]

which leads to an empirical RMDP \(\widehat{\mathcal{M}}_{\text{rob}}=\{\mathcal{S},\mathcal{A},\gamma,\mathcal{ U}^{\sigma}_{\|\cdot\|}(\widehat{P}^{0}),r\}\). Analogously, we can define the corresponding robust value function (resp. robust Q-function) of policy \(\pi\) in \(\widehat{\mathcal{M}}_{\text{rob}}\) as \(\widehat{V}^{\pi,\sigma}\) (resp. \(\widehat{Q}^{\pi,\sigma}\)) (cf. (8)). In addition, we denote the corresponding _optimal robust policy_ as \(\widehat{\pi}^{\star}\) and the _optimal robust value function_ (resp. _optimal robust Q-function_) as \(\widehat{V}^{\star,\sigma}\) (resp. \(\widehat{Q}^{\star,\sigma}\)) (cf. (9)), which satisfies the robust Bellman optimality equation \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\):

\[\widehat{Q}^{\star,\sigma}(s,a)=r(s,a)+\gamma\inf_{\mathcal{P}\in\mathcal{U}^ {\star,\sigma}_{\|\cdot\|}(\widehat{P}^{0}_{\pi,a})}\mathcal{P}\widehat{V}^{ \star,\sigma}. \tag{12}\]

Equipped with \(\widehat{P}^{0}\), we can define the empirical robust Bellman operator \(\widehat{\mathcal{T}}^{\sigma}\) as \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\)

\[\widehat{\mathcal{T}}^{\sigma}(Q^{\pi})(s,a)\coloneqq r(s,a)+ \gamma\inf_{\mathcal{P}\in\mathcal{U}^{\pi,\sigma}_{\|\cdot\|}(\widehat{P}^{ 0}_{\pi,a})}\mathcal{P}V, \tag{13}\]

with \(V(s)\coloneqq\max_{\pi}Q^{\pi}(s,a)\). The aim of this work is given the collected samples, to learn the robust optimal policy for the RMDP w.r.t. some prescribed uncertainty set \(\mathcal{U}^{\sigma}(P^{0})\) around the nominal kernel using as few samples as possible. Specifically, given some target accuracy level \(\varepsilon>0\), the goal is to seek an \(\varepsilon\)-optimal robust policy \(\widehat{\pi}\) obeying

\[\forall s\in\mathcal{S}:\quad V^{\star,\sigma}(s)-V^{\widehat{ \pi},\sigma}(s)\leq\varepsilon, \tag{14}\] \[\widehat{V}^{\widehat{\pi}^{\star},\sigma}-\widehat{V}^{\widehat {\pi},\sigma}\leq\varepsilon_{\text{opt}}. \tag{15}\]

This formulation allows plugging any solver of RMDPs in this bound, for instance, the distributionally robust value iteration (DRVI) algorithm detailed in Appendix G.

## 4 Theoretical guarantees

In this section, we present our main results characterizing the sample complexity of solving RMDPs with \(sa\)-and \(s\)-rectangularity. Additionally, we discuss the implications of our results for the comparisons between standard and robust RL, and for comparisons between \(sa\)- versus \(s\)-rectangularity.

### \(sa\)-rectangular uncertainty set with general smooth norms

To begin, we consider the RMDPs with \(sa\)-rectangularity with general norms. We first provide the following sample complexity upper bound for certain oracle planning algorithms, whose proof is postponed to Appendix D.2. Technically, we derive two new dual forms for RMDPs problems using arbitrary norms in Lemmas 3 and 4 for respectively \(sa\)- and \(s\)-rectangular RMDPS. In these dual forms, a central quantity denoted \(\mathrm{sp}(.)_{*}\), representing the dispersion of the value function, appears and is the dual span semi-norm associated with the considered general \(L_{p}\) norm \(\|.\|\) defined in 1 in the initial primal problem. The main challenge in this analysis is to derive a tight upper bound on this quantity in Lemmas (5) and (6), leading to the following sample complexity.

**Theorem 1** (Upper bound for \(sa\)-rectangularity).: _Consider the uncertainty set \(U_{\|.\|}^{\mathbf{a},\sigma}(\cdot)\) associated with arbitrary \(L_{p}\) smooth norm \(\|\cdot\|\) defined in 1. We denote \(\sigma_{\max}\coloneqq\max_{p_{1},p_{2}\in\Delta(\mathcal{S})}\|p_{1}-p_{2}\|\) as the accessible maximal uncertainty level. Consider any \(\delta\in(0,1)\), discount factor \(\gamma\in\left[\frac{1}{4},1\right)\), and uncertainty level \(\sigma\in(0,\sigma_{\max}]\). Let \(\widehat{\pi}\) be the output policy of some oracle planning algorithm with optimization error \(\varepsilon_{\text{opt}}\) introduced in (15). With introduced in 1, one has with probability at least \(1-\delta\),_

\[\forall s\in\mathcal{S}:\quad V^{\star,\sigma}(s)-V^{\widehat{\pi},\sigma}(s) \leq\varepsilon+\frac{8\varepsilon_{\text{opt}}}{1-\gamma} \tag{16}\]

_for any \(\varepsilon\in(0,\sqrt{1/\max\{1-\gamma,\sigma C_{g}\}}]\), as long as the total number of samples obeys_

\[NSA\gtrsim\frac{c_{1}SA}{(1-\gamma)^{2}\max\{1-\gamma,C_{g}\sigma\}\varepsilon ^{2}}+\frac{c_{2}SAC_{S}\left\|1_{S}\right\|_{*}}{(1-\gamma)^{2}\epsilon} \tag{17}\]

_with \(c_{1},c_{2},c_{3}\) a universal positive constant. For a sufficiently small level of accuracy \(\epsilon\leq(\max\{1-\gamma,C_{g}\sigma\})/(C_{S}\left\|1_{S}\right\|)\), the sample complexity is_

\[NSA\gtrsim\frac{c_{3}SA}{(1-\gamma)^{2}\max\{1-\gamma,C_{g}\sigma\}\varepsilon ^{2}}. \tag{18}\]

Note that this result is also true for \(TV\) without the geometric smooth term depending on \(C_{S}\). Considering \(L_{p}\) norms, \(C_{g}\geq 1\) and \(C_{S}\leq S^{1/q}(p-1)\). In Theorem 1, we introduce the following minimax-optimal lower bound to verify the tightness of the above upper bound; a proof is provided in Appendix E.

**Theorem 2** (Lower bound for \(sa\)-rectangularity).: _Consider the uncertainty set \(U_{\|.\|}^{\mathbf{a},\sigma}(\cdot)\) associated with arbitrary \(L_{P}\) norm \(\|\cdot\|\) defined in \(1\). We denote \(\sigma_{\max}\coloneqq\max_{p_{1},q_{1}\in\Delta(\mathcal{S})}\|p_{1}-p_{2}\|\) as the accessible maximal uncertainty level. Consider any tuple \((S,A,\gamma,\sigma,\varepsilon)\), where \(\gamma\in\left[\frac{1}{2},1\right)\), \(\sigma\in(0,\sigma_{\max}(1-c_{0})]\) with \(0<c_{0}\leq\frac{1}{8}\) being any small enough positive constant, and \(\varepsilon\in\left(0,\frac{c_{0}}{256(1-\gamma)}\right]\). We can construct two infinite-horizon RMDPs \(\mathcal{M}_{0},\mathcal{M}_{1}\) such that giving a dataset with \(N\) independent samples for each state-action pair over the nominal transition kernel (for either \(\mathcal{M}_{0}\) or \(\mathcal{M}_{1}\) respectively), one has_

\[\inf_{\widehat{\pi}}\max_{\mathcal{M}\in\left\{\mathcal{M}_{0},\mathcal{M}_{1} \right\}}\left\{\mathbb{P}_{\mathcal{M}}\Big{(}\max_{s\in\mathcal{S}}\left[V^ {\star,\sigma}(s)-V^{\widehat{\pi},\sigma}(s)\right]>\varepsilon\Big{)} \right\}\geq\frac{1}{8},\]

_where the infimum is taken over all estimators \(\widehat{\pi}\), \(\mathbb{P}_{0}\) (resp. \(\mathbb{P}_{1}\)) are the probability when the RMDP is \(\mathcal{M}_{0}\) (resp. \(\mathcal{M}_{1}\)), as long as, for \(c_{7}\) is a universal positive constant,_

\[NSA\leq\frac{c_{7}SA}{(1-\gamma)^{2}\max\{1-\gamma,C_{g}\sigma\}\varepsilon^{2 }}. \tag{19}\]

\(\bullet\) Near minimax-optimal sample complexity with general \(L_{p}\) norms.Recall that Theorem 1 shows that the sample complexity upper bound of oracle algorithms for RMDPs is in the order of \(\widetilde{O}\left(\frac{SA}{(1-\gamma)^{2}\max\{1-\gamma,C_{g}\sigma\} \varepsilon^{2}}\right).\) Combined with the lower bound in Theorem 2, we observe that the above sample complexity is near minimax-optimal, in almost the full range of uncertainty.

\(\bullet\) Solving RMDPs with general \(L_{p}\) norms can be easier than solving standard RL.Recall that the sample complexity of solving standard RL with a generative model [Agarwal et al., 2020, Liet al., 2024; Azar et al., 2013a) is: \(\widetilde{O}\left(\frac{SA}{(1-\gamma)^{2}\sigma^{2}}\right).\) Comparing this with the sample complexity in (18), it highlights that solving robust MDPs (cf. (18)) using any norm as the divergence function for the uncertainty set is not harder than (and is sometimes easier than) solving standard RL (cf. (4.1)). Specifically, when the uncertainty level is small \(\sigma\lesssim 1-\gamma\), the sample complexity of solving robust MDPs matches that of standard MDPs. While when the uncertainty level is relatively larger \(1-\gamma\lesssim\sigma\leq\sigma_{\max}\), the sample complexity of solving robust MDPs is smaller than that of standard MDPs by a factor or \(\frac{\sigma}{1-\gamma}\), which goes to \(\frac{1}{1-\gamma}\) when \(\sigma=O(1)\).

\(\bullet\)Comparisons with prior arts.In Figure 1, we illustrate the comparisons with two state-of-the-arts (Clavier et al., 2023; Shi et al., 2023) which use some divergence functions belonging to the class of general norms considered in this work. In particular, Shi et al. [2023] achieved the state-of-the-art minimax-optimal sample complexity \(\widetilde{O}\left(\frac{SA}{(1-\gamma)^{2}\max\{1-\gamma,\sigma\}\varepsilon ^{2}}\right)\) for specific \(L_{1}\) norm (or called total variation distance). In this work, we attain near minimax-optimal sample complexity for any general norm (including \(L_{1}\)) which matches the one in Shi et al. [2023] when narrowing down to \(L_{1}\) norm. Note that in \(TV\) case, \(C_{g}=1\). This reveals that the finding of robust MDPs can be easier than standard MDPs (Shi et al., 2023) in terms of sample requirement does not only hold for \(L_{1}\) norm, but for any general norm. In addition, compared to Clavier et al. [2023] which focuses on \(L_{p}\) norms for any \(1\leq p\leq\infty\): when \(1-\gamma\lesssim\sigma\leq\sigma_{\max}\), we improve the sample complexity \(\widetilde{O}(\frac{SA}{(1-\gamma)^{2}\sigma^{2}})\) to \(\widetilde{O}(\frac{SA}{(1-\gamma)^{2}\sigma\varepsilon^{2}})\) by at least a factor of \(\frac{1}{1-\gamma}\); otherwise, we match the results in Clavier et al. [2023].

\(\bullet\)Burn-in Condition, \(C_{g}\) factor and \(TV\) case :In Th. 1 and 3 we need a sufficiently small level of accuracy \(\epsilon\leq(\max\{1-\gamma,C_{g}\sigma\})/(C_{s}\left\|1_{S}\right\|)\), to obtain the sample complexity. This type of condition is usual in MDPS analysis Shi et al. [2022] and is equivalent to burn in term. Moreover, the quantity \(C_{S}\) exists (see 1) and for example, considering \(L_{p}\) norms, \(C_{S}\) is bounded by \(S^{1/q}\). (See (154)) and the product \(C_{S}\left\|1_{S}\right\|\) is upper bounded by \(S\) for \(L_{2}\) norm. Moreover, note that our theorem for the smooth norm is also true for \(TV\) which is not \(C^{2}\) and has the same complexity as (Shi et al. [2023]). In this case, the burn-in condition is not needed. (See Lemma D.3.3). Finally, the factor \(C_{g}=1/\min_{s}\left\|e_{s}\right\|\) is norm dependent and depends on how big the vector \(e_{s_{0}}\) is in the considered norm. Note for classical \(L_{p}\) this quantity is bigger than \(1\), which reduces the sample complexity.

### \(s\)-rectangular uncertainty set with general norms

To continue, we move on to the case when the uncertainty set is constructed under \(s\)-rectangularity smooth norm. The following theorem presents the sample complexity upper bound for learning an \(\epsilon\)-optimal policy for RMDPs with \(s\)-rectangularity. A proof is shown in Appendix D.2.

**Theorem 3** (Upper bound for \(s\)-rectangularity).: _Consider the uncertainty set \(\mathcal{U}_{\|\cdot\|}^{s,\widetilde{\sigma}}_{\cdot\|\cdot}(\cdot)\) with \(s\)-rectangularity. Consider any discount factor \(\gamma\in\left[\frac{1}{4},1\right)\), the rescaled uncertainty level \(\tilde{\sigma}=\sigma\left\|1_{A}\right\|\), and denote \(\tilde{\sigma}_{\max}\coloneqq\left\|1_{A}\right\|\max_{p_{1},p_{2}\in \Delta_{S}(\tilde{\mathbf{y}})}\left\|p_{1}-p_{2}\right\|\) and \(\delta\in(0,1)\). Let \(\widehat{\pi}\) be the output policy of an arbitrary optimization algorithm with error \(\varepsilon_{\text{opt}}\), with probability at least \(1-\delta\), one has for any \(\varepsilon\in(0,\sqrt{1/\max\{1-\gamma,C_{g}\min_{s}\left\|\pi_{s}\right\|_{ \star}\sigma\}}]\), \(\forall s\in\mathcal{S}:\quad V^{\star,\widetilde{\sigma}}(s)-V^{\widehat{ \pi},\widetilde{\sigma}}(s)\leq\varepsilon+\frac{8\varepsilon_{\text{opt}}} {1-\gamma}\) as long as the total number of samples obeys_

\[NSA\gtrsim\frac{c_{4}SA}{(1-\gamma)^{2}\varepsilon^{2}}\min\Biggl{\{}\frac{1} {\max\{1-\gamma,C_{g}\sigma\}},\frac{1}{\sigma C_{g}\min_{s\in\mathcal{S}} \left\{\left\|\pi_{s}^{*}\right\|_{\star}\left\|1_{A}\right\|,\left\|\hat{\pi} _{s}\right\|_{\star}\left\|1_{A}\right\|\right\}}\Biggr{\}}+\frac{c_{5}SAC_{ S}\left\|1_{S}\right\|_{\star}}{(1-\gamma)^{2}\epsilon} \tag{20}\]

_For a sufficiently small accuracy, \(\epsilon\leq(\max\{1-\gamma,C_{g}\tilde{\sigma}\})/(C_{s}\left\|1_{S}\right\|)\) the sample complexity is_

\[NSA\gtrsim\frac{c_{6}SA}{(1-\gamma)^{2}\varepsilon^{2}}\min\Biggl{\{}\frac{1} {\max\{1-\gamma,C_{g}\sigma\}},\frac{1}{\sigma C_{g}\min_{s\in\mathcal{S}} \left\{\left\|\pi_{s}^{*}\right\|_{\star}\left\|1_{A}\right\|,\left\|\hat{\pi} _{s}\right\|_{\star}\left\|1_{A}\right\|\right\}}\Biggr{\}} \tag{21}\]

where \(\hat{\pi}_{s}\in\Delta_{A}\) denote the policy of the empirical RMPDs at state \(s\), \(\pi_{s}^{*}\in\Delta_{A}\) the optimal policy given \(s\) of the true RMPDs, \(\left\|.\right\|_{\star}\) the dual norm and \(c_{4},c_{5},c_{6}\) are universal constant. Note that this result is also true for \(TV\) without the term depending on smoothness \(C_{S}\). In addition, we provide the lower bounds for a representative divergence function -- \(L_{\infty}\) norm in the following. Note that for classical \(L_{p},C_{S}=S^{1/q}(p-1)\) and \(C_{g}\) can be lower bounded by \(1\). A proof is provided in Appendix F.

**Theorem 4** (Lower bound for \(s\)-rectangularity).: _Consider the uncertainty set \(\mathcal{U}_{L_{\infty}}^{\mathcal{S},\tilde{\sigma}}\left(\cdot\right)\) associated with the \(L_{\infty}\) norm. Consider any tuple \((S,A,\gamma,\sigma,\varepsilon)\) and \(0<c_{0}\leq\frac{1}{8}\) being any small enough positive constant, where \(\gamma\in\left[\frac{1}{2},1\right)\), and \(\varepsilon\in\left(0,\frac{c_{0}}{256(1-\gamma)}\right]\). Correspondingly, we denote the accessible maximal uncertainty level for \(\mathcal{U}_{L_{\infty}}^{\mathcal{S},\tilde{\sigma}}(\cdot)\) as \(\sigma_{\max}^{\infty}\coloneqq\max_{p_{1},p_{1}\in\Delta(\mathcal{S})^{A}}\|p _{1}-p_{2}\|_{\infty}=1\). Then we can construct a collection of infinite-horizon RMDPs \(\mathcal{M}_{L_{\infty}}\) defined by the uncertainty set with \(\mathcal{U}_{L_{\infty}}^{\mathcal{S},\tilde{\sigma}}\left(\cdot\right)\) so that for any \(\sigma\in\left(0,\sigma_{\max}^{\infty}(1-c_{0})\right]\), and any dataset with in total \(N_{\text{all}}\) independent samples for all state-action pairs over the nominal transition kernel (for any RMDP inside \(\mathcal{M}_{L_{\infty}}\)), one has_

\[\inf_{\tilde{\pi}}\max_{\mathcal{M}\in\mathcal{M}_{L_{\infty}}}\left\{\mathbb{P }_{\mathcal{M}}\big{(}\max_{s\in\mathcal{S}}\left[V^{\star,\sigma}(s)-V^{ \tilde{\pi},\sigma}(s)\right]>\varepsilon\big{)}\right\}\geq\frac{1}{8}, \tag{22}\]

_provided that for \(c_{8}\) is a universal positive constant,_

\[N_{\text{all}}\leq\frac{c_{8}SA}{(1-\gamma)^{2}\max\{1-\gamma,\tilde{\sigma} \}\varepsilon^{2}}, \tag{23}\]

_with \(\mathbb{P}_{\mathcal{M}}\) the probability when the RMDP is \(\mathcal{M}\), and the infimum is taken over all estimators \(\widehat{\pi}\)._

Now we can present some implications of Theorem 3 and Theorem 4.

\(\bullet\) Robust MDPs with \(s\)-rectangularity are at least as easy as \(sa\)-rectangularity.Theorem 3 shows that the sample complexity of solving RMDPs with \(s\)-rectangularity does not exceed the order of \(\widetilde{O}\left(\frac{SA}{(1-\gamma)^{2}\max\{1-\gamma,C_{g}\sigma\} \varepsilon^{2}}\right).\) This matches the sample complexity for \(sa\)-rectangularity (cf. (18)) and indicates that although \(s\)-rectangular RMDPs are of a more complicated formulation, solving \(s\)-rectangular RMDPs is at least as easy as solving \(sa\)-rectangular RMDPs in terms of the sample complexity. In addition to the worst-case sample complexity upper bound, Theorem 3 also provides a data and instance-dependent sample complexity upper bound for \(s\)-rectangular RMDPs (cf. in (20)).Taking the divergence function \(\|\cdot\|=L_{p}\) for instance, the data and instance-dependent sample complexity upper bound is

\[\begin{cases}\widetilde{O}\left(\frac{SA}{(1-\gamma)^{2}\varepsilon^{2}}\frac {1}{\max\{1-\gamma,\sigma\}}\right)&\text{if }\widehat{\pi}_{s}(a\,|\,s)=\pi_{s}^{ \ast}(a\,|\,s)=\frac{1}{A},\quad\forall(s,a)\in\mathcal{S}\times\mathcal{A}\\ \widetilde{O}\left(\frac{SA}{(1-\gamma)^{2}\varepsilon^{2}}\frac{1}{\max\{1- \gamma,\sigma A^{1/p}\}}\right)&\text{if }\|\widehat{\pi}_{s}(\cdot\,|\,s)\|_{0}=\|\pi_{s}^{ \ast}(\cdot\,|\,s)\|_{0}=1,\quad\forall s\in\mathcal{S}.\end{cases}\]

where \(\left\|.\right\|_{0}\) corresponds to the total number of nonzero elements in a vector.The intuition beyond this theorem is that when the policy becomes proportional to uniform, the uncertainty budget of the \(s\)-rectangular MDPs is equally spread into all actions, and we retrieve the \(sa\)-rectangular case. When the policy becomes deterministic, all the uncertainty budget concentrates on one action. In this case, most of the actions are not robust except one, and the problem is simpler than classical MDP for this only specific action. An illustration of this result can be found in Fig. 2.

\(\bullet\) Comparisons with prior arts.In Figure 1, we illustrate the comparisons with Clavier et al. (2023) which use \(L_{p}\) norms functions belonging to the class of general norms considered in this work. We do not compare in this section to Yang et al. (2022) as it is not anymore state-of-the-art with regard to the work of Clavier et al. (2023). In particular, the latest achieves in the \(s\)-rectangular case at sample complexity of \(\widetilde{O}\left(\frac{SA}{(1-\gamma)^{3}\varepsilon^{2}}\right)\) in the regime where \(\tilde{\sigma}\lesssim 1-\gamma\). In this regime, our result is the same but more general but in the regime where \(\tilde{\sigma}\gtrsim 1-\gamma\), they achieve sample complexity of \(\widetilde{O}\left(\frac{SA}{(1-\gamma)^{4}\varepsilon^{2}}\right)\) which is bigger than our result \(\widetilde{O}\left(\frac{SA}{(1-\gamma)^{2}\max\{1-\gamma,\tilde{\sigma}\} \varepsilon^{2}}\right)\) by a factor at least \(\frac{1}{1-\gamma}\).

## 5 Conclusion

This work refined sample complexity bounds to learn robust Markov decision processes when the uncertainty set is characterized by an general \(L_{p}\) metric, assuming the presence of a generative model. Our findings not only strengthen the current knowledge by improving both the upper and lower bounds, but also highlight that learning \(s\)-rectangular MDPs is less challenging in terms of sample complexity compared to classical \(sa\)-rectangular MDPs. This work is the first to provide results with a minimax bound, as prior results concerning \(s\)-rectangular cases were not minimax optimal. Additionally, wehave established the minimax sample complexity for RMDPs using a general \(L_{p}\) norm, demonstrating that it is never larger than that required for learning standard MDPs. Our research identifies potential avenues for future work, such as exploring the characterization of tight sample complexity for RMDPs under a broader family of uncertainty sets, such as those defined by \(f\)-divergence. It would be highly desirable for a more unified theoretical foundation, as the distance between probability measures is more natural to define using divergence. Moreover, it would be interesting to focus on the finite-horizon Setting and linear setting, as our current analytical framework opens the door for potential extensions to address finite-horizon RMDPs. Such an extension would contribute to a more comprehensive understanding of tabular cases. Finally, the case of linear MDPs would be interesting to explore.

## 6 Acknowledgements

Fondation Mathematique Jacques Hadamard supported this work during Pierre Clavier's visiting the California Institute of Technology. Pierre Clavier has been supported by a grant from Region Ile-de-France; DIM Math Innov. The work of L. Shi is supported in part by the Resnick Institute and Computing, Data, and Society Postdoctoral Fellowship at California Institute of Technology. The work of E. Mazumdar is supported in part from NSF-2240110. The work of A. Wierman is supported in part from CNS-2146814, CPS-2136197, CNS-2106403, and NGSDI-2105648.

## References

* Agarwal et al. (2020) Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based reinforcement learning with a generative model is minimax optimal. In _Conference on Learning Theory_, pages 67-83. PMLR, 2020.
* Azar et al. (2013a) Mohammad Azar, Remi Munos, and Hilbert J Kappen. Minimax pac bounds on the sample complexity of reinforcement learning with a generative model. _Machine learning_, 91:325-349, 2013a.
* Azar et al. (2013b) Mohammad Gheshlaghi Azar, Remi Munos, and Hilbert J Kappen. Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model. _Machine learning_, 91(3):325-349, 2013b.
* Badrinath and Kalathil (2021) Kishan Panaganti Badrinath and Dileep Kalathil. Robust reinforcement learning using least squares policy iteration with provable performance guarantees. In _International Conference on Machine Learning_, pages 511-520. PMLR, 2021.
* Bai et al. (2019) Yu Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang. Provably efficient Q-learning with low switching cost. _arXiv preprint arXiv:1905.12849_, 2019.
* Beck and Srikant (2012) Carolyn L Beck and Rayadurgam Srikant. Error bounds for constant step-size Q-learning. _Systems & control letters_, 61(12):1203-1208, 2012.
* Blanchet and Murthy (2019) Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport. _Mathematics of Operations Research_, 44(2):565-600, 2019.
* Blanchet et al. (2023) Jose Blanchet, Miao Lu, Tong Zhang, and Han Zhong. Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage. _arXiv preprint arXiv:2305.09659_, 2023.
* Chen et al. (2020) Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finite-sample analysis of stochastic approximation using smooth convex envelopes. _arXiv preprint arXiv:2002.00874_, 2020.
* Clavier et al. (2022) Pierre Clavier, Stephanie Allassoniere, and Erwan Le Pennec. Robust reinforcement learning with distributional risk-averse formulation. _arXiv preprint arXiv:2206.06841_, 2022.
* Clavier et al. (2023) Pierre Clavier, Erwan Le Pennec, and Matthieu Geist. Towards minimax optimality of model-based robust reinforcement learning. _arXiv preprint arXiv:2302.05372_, 2023.
* Derman and Mannor (2020) Esther Derman and Shie Mannor. Distributional robustness and regularization in reinforcement learning. _arXiv preprint arXiv:2003.02894_, 2020.
* Derman et al. (2021) Esther Derman, Matthieu Geist, and Shie Mannor. Twice regularized MDPs and the equivalence between robustness and regularization. _Advances in Neural Information Processing Systems_, 34, 2021.
* Dong et al. (2022) Jing Dong, Jingwei Li, Baoxiang Wang, and Jingzhao Zhang. Online policy optimization for robust MDP. _arXiv preprint arXiv:2209.13841_, 2022.
* Dong et al. (2019) Kefan Dong, Yuanhao Wang, Xiaoyu Chen, and Liwei Wang. Q-learning with UCB exploration is sample efficient for infinite-horizon MDP. _arXiv preprint arXiv:1901.09311_, 2019.
* Duchi and Namkoong (2018) John Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust optimization. _arXiv preprint arXiv:1810.08750_, 2018.
* Gao (2020) Rui Gao. Finite-sample guarantees for wasserstein distributionally robust optimization: Breaking the curse of dimensionality. _arXiv preprint arXiv:2009.04382_, 2020.
* Goyal and Grand-Clement (2022) Vineet Goyal and Julien Grand-Clement. Robust markov decision processes: Beyond rectangularity. _Mathematics of Operations Research_, 2022.
* Han et al. (2022) Songyang Han, Sanbao Su, Sihong He, Shuo Han, Haizhao Yang, and Fei Miao. What is the solution for state adversarial multi-agent reinforcement learning? _arXiv preprint arXiv:2212.02705_, 2022.
* Ho et al. (2018) Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Fast bellman updates for robust MDPs. In _International Conference on Machine Learning_, pages 1979-1988. PMLR, 2018.
* Ho et al. (2018)Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Partial policy iteration for l1-robust markov decision processes. _Journal of Machine Learning Research_, 22(275):1-46, 2021.
* Iyengar (2005) Garud N Iyengar. Robust dynamic programming. _Mathematics of Operations Research_, 30(2):257-280, 2005.
* Jafarnia-Jahromi et al. (2020) Mehdi Jafarnia-Jahromi, Chen-Yu Wei, Rahul Jain, and Haipeng Luo. A model-free learning algorithm for infinite-horizon average-reward MDPs with near-optimal regret. _arXiv preprint arXiv:2006.04354_, 2020.
* Jin et al. (2018) Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient? In _Advances in Neural Information Processing Systems_, pages 4863-4873, 2018.
* Jin et al. (2020) Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for reinforcement learning. In _International Conference on Machine Learning_, pages 4870-4879. PMLR, 2020.
* Jin et al. (2021) Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline RL? In _International Conference on Machine Learning_, pages 5084-5096, 2021.
* Karush (2013) William Karush. Minima of functions of several variables with inequalities as side conditions. In _Traces and emergence of nonlinear programming_, pages 217-245. Springer, 2013.
* Kaufman and Schaefer (2013) David L Kaufman and Andrew J Schaefer. Robust modified policy iteration. _INFORMS Journal on Computing_, 25(3):396-410, 2013.
* Kearns and Singh (1999) Michael J Kearns and Satinder P Singh. Finite-sample convergence rates for Q-learning and indirect algorithms. In _Advances in neural information processing systems_, pages 996-1002, 1999.
* Klopp et al. (2017) Olga Klopp, Karim Lounici, and Alexandre B Tsybakov. Robust matrix completion. _Probability Theory and Related Fields_, 169(1-2):523-564, 2017.
* Kumar et al. (2022) Aounon Kumar, Alexander Levine, Tom Goldstein, and Soheil Feizi. Certifying model accuracy under distribution shifts. _arXiv preprint arXiv:2201.12440_, 2022.
* Kumar et al. (2023) Navdeep Kumar, Esther Derman, Matthieu Geist, Kfir Levy, and Shie Mannor. Policy gradient for s-rectangular robust markov decision processes. _arXiv preprint arXiv:2301.13589_, 2023.
* Li et al. (2021) Gen Li, Laixi Shi, Yuxin Chen, Yuantao Gu, and Yuejie Chi. Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning. _Advances in Neural Information Processing Systems_, 34, 2021.
* Li et al. (2022a) Gen Li, Yuejie Chi, Yuting Wei, and Yuxin Chen. Minimax-optimal multi-agent RL in Markov games with a generative model. _Neural Information Processing Systems_, 2022a.
* Li et al. (2022b) Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, and Yuting Wei. Settling the sample complexity of model-based offline reinforcement learning. _arXiv preprint arXiv:2204.05275_, 2022b.
* Li et al. (2023a) Gen Li, Changxiao Cai, Yuxin Chen, Yuting Wei, and Yuejie Chi. Is Q-learning minimax optimal? a tight sample complexity analysis. _Operations Research_, 2023a.
* Li et al. (2023b) Gen Li, Yuting Wei, Yuejie Chi, and Yuxin Chen. Breaking the sample size barrier in model-based reinforcement learning with a generative model. _accepted to Operations Research_, 2023b.
* Li et al. (2023c) Gen Li, Yuling Yan, Yuxin Chen, and Jianqing Fan. Minimax-optimal reward-agnostic exploration in reinforcement learning. _arXiv preprint arXiv:2304.07278_, 2023c.
* Li et al. (2024) Gen Li, Yuting Wei, Yuejie Chi, and Yuxin Chen. Breaking the sample size barrier in model-based reinforcement learning with a generative model. _Operations Research_, 72(1):203-221, 2024.
* Li et al. (2022c) Yan Li, Tuo Zhao, and Guanghui Lan. First-order policy optimization for robust markov decision process. _arXiv preprint arXiv:2209.10579_, 2022c.
* Mahmood et al. (2018) A Rupam Mahmood, Dmytro Korenkevych, Gautham Vasan, William Ma, and James Bergstra. Benchmarking reinforcement learning algorithms on real-world robots. In _Conference on robot learning_, pages 561-591. PMLR, 2018.
* Li et al. (2020)Shie Mannor, Duncan Simester, Peng Sun, and John N Tsitsiklis. Bias and variance in value function estimation. In _Proceedings of the twenty-first international conference on Machine learning_, page 72, 2004.
* McDiarmid et al. (1989) Colin McDiarmid et al. On the method of bounded differences. _Surveys in combinatorics_, 141(1):148-188, 1989.
* Moos et al. (2022) Janosch Moos, Kay Hansel, Hany Abdulsamad, Svenja Stark, Debora Clever, and Jan Peters. Robust reinforcement learning: A review of foundations and recent advances. _Machine Learning and Knowledge Extraction_, 4(1):276-315, 2022.
* Nilim and Ghaoui (2005) Arnab Nilim and Laurent El Ghaoui. Robust control of Markov decision processes with uncertain transition matrices. _Operations Research_, 53(5):780-798, 2005.
* Panaganti and Kalathil (2022) Kishan Panaganti and Dileep Kalathil. Sample complexity of robust reinforcement learning with a generative model. In _International Conference on Artificial Intelligence and Statistics_, pages 9582-9602. PMLR, 2022.
* Qiaoben et al. (2021) You Qiaoben, Xinning Zhou, Chengyang Ying, and Jun Zhu. Strategically-timed state-observation attacks on deep reinforcement learning agents. In _ICML 2021 Workshop on Adversarial Machine Learning_, 2021.
* Rahimian and Mehrotra (2019) Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. _arXiv preprint arXiv:1908.05659_, 2019.
* Rashidinejad et al. (2021) Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. _Neural Information Processing Systems (NeurIPS)_, 2021.
* Roy et al. (2017) Aurko Roy, Huan Xu, and Sebastian Pokutta. Reinforcement learning under model mismatch. _Advances in neural information processing systems_, 30, 2017.
* Rudin et al. (1964) Walter Rudin et al. _Principles of mathematical analysis_, volume 3. McGraw-hill New York, 1964.
* Russel et al. (2019) Reazul Hasan Russel, Bahram Behzadian, and Marek Petrik. Optimizing norm-bounded weighted ambiguity sets for robust mdps. _arXiv preprint arXiv:1912.02696_, 2019.
* Shi and Chi (2022) Laixi Shi and Yuejie Chi. Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity. _arXiv preprint arXiv:2208.05767_, 2022.
* Shi et al. (2022) Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic Q-learning for offline reinforcement learning: Towards optimal sample complexity. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162, pages 19967-20025. PMLR, 2022.
* Shi et al. (2023) Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, Matthieu Geist, and Yuejie Chi. The curious price of distributional robustness in reinforcement learning with a generative model. _arXiv preprint arXiv:2305.16589_, 2023.
* Sidford et al. (2018) Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, and Yinyu Ye. Near-optimal time and sample complexities for solving Markov decision processes with a generative model. In _Advances in Neural Information Processing Systems_, pages 5186-5196, 2018.
* Smirnova et al. (2019) Elena Smirnova, Elvis Dohmatob, and Jeremie Mary. Distributionally robust reinforcement learning. _arXiv preprint arXiv:1902.08708_, 2019.
* Sutton (1988) Richard S Sutton. Learning to predict by the methods of temporal differences. _Machine learning_, 3(1):9-44, 1988.
* Tamar et al. (2014) Aviv Tamar, Shie Mannor, and Huan Xu. Scaling up robust MDPs using function approximation. In _International conference on machine learning_, pages 181-189. PMLR, 2014.
* Tan et al. (2020) Kai Liang Tan, Yasaman Esfandiari, Xian Yeow Lee, and Soumik Sarkar. Robustifying reinforcement learning agents via action space adversarial training. In _2020 American control conference (ACC)_, pages 3959-3964. IEEE, 2020.
* Tan et al. (2020)Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applications in continuous control. In _International Conference on Machine Learning_, pages 6215-6224. PMLR, 2019.
* Tsybakov (2009) A. B. Tsybakov. _Introduction to nonparametric estimation_, volume 11. Springer, 2009.
* Neumann (1928) J v. Neumann. Zur theorie der gesellschaftsspiele. _Mathematische annalen_, 100(1):295-320, 1928.
* Vershynin (2018) Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* Wainwright (2019) Martin J Wainwright. Stochastic approximation with cone-contractive operators: Sharp \(\ell_{\infty}\)-bounds for Q-learning. _arXiv preprint arXiv:1905.06265_, 2019.
* Wang et al. (2023) Shengbo Wang, Nian Si, Jose Blanchet, and Zhengyuan Zhou. A finite sample complexity bound for distributionally robust q-learning. _arXiv preprint arXiv:2302.13203_, 2023.
* Wang and Zou (2021) Yue Wang and Shaofeng Zou. Online robust reinforcement learning with model uncertainty. _Advances in Neural Information Processing Systems_, 34, 2021.
* Wiesemann et al. (2013) Wolfram Wiesemann, Daniel Kuhn, and Berc Rustem. Robust markov decision processes. _Mathematics of Operations Research_, 38(1):153-183, 2013.
* Wolff et al. (2012) Eric M Wolff, Ufuk Topcu, and Richard M Murray. Robust control of uncertain markov decision processes with temporal logic specifications. In _2012 IEEE 51st IEEE Conference on Decision and Control (CDC)_, pages 3372-3379. IEEE, 2012.
* Xie et al. (2021) Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. _Advances in neural information processing systems_, 34, 2021.
* Xu and Mannor (2012) Huan Xu and Shie Mannor. Distributionally robust Markov decision processes. _Mathematics of Operations Research_, 37(2):288-300, 2012.
* Xu et al. (2023) Zaiyan Xu, Kishan Panaganti, and Dileep Kalathil. Improved sample complexity bounds for distributionally robust reinforcement learning. _arXiv preprint arXiv:2303.02783_, 2023.
* Yan et al. (2022) Yuling Yan, Gen Li, Yuxin Chen, and Jianqing Fan. The efficacy of pessimism in asynchronous Q-learning. _arXiv preprint arXiv:2203.07368_, 2022.
* Yan et al. (2023) Yuling Yan, Gen Li, Yuxin Chen, and Jianqing Fan. The efficacy of pessimism in asynchronous q-learning. _IEEE Transactions on Information Theory_, 2023.
* Yang et al. (2021) Kunhe Yang, Lin Yang, and Simon Du. Q-learning with logarithmic regret. In _International Conference on Artificial Intelligence and Statistics_, pages 1576-1584. PMLR, 2021.
* Yang (1991) Wei H Yang. On generalized holder inequality. 1991.
* Yang et al. (2022a) Wenhao Yang, Liangyu Zhang, and Zhihua Zhang. Toward theoretical understandings of robust Markov decision processes: Sample complexity and asymptotics. _The Annals of Statistics_, 50(6):3223-3248, 2022a.
* Yang et al. (2022b) Wenhao Yang, Liangyu Zhang, and Zhihua Zhang. Toward theoretical understandings of robust markov decision processes: Sample complexity and asymptotics. _The Annals of Statistics_, 50(6):3223-3248, 2022b.
* Yang et al. (2023) Wenhao Yang, Han Wang, Tadashi Kozuno, Scott M Jordan, and Zhihua Zhang. Avoiding model estimation in robust markov decision processes with a generative model. _arXiv preprint arXiv:2302.01248_, 2023.
* Yin et al. (2021) Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal offline reinforcement learning via double variance reduction. _arXiv preprint arXiv:2102.01748_, 2021.
* Zhang et al. (2020a) Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui Hsieh. Robust deep reinforcement learning against adversarial perturbations on state observations. _Advances in Neural Information Processing Systems_, 33:21024-21037, 2020a.
* Zhang et al. (2020b)Huan Zhang, Hongge Chen, Duane Boning, and Cho-Jui Hsieh. Robust reinforcement learning on state observations with learned optimal adversary. _arXiv preprint arXiv:2101.08452_, 2021.
* Zhang et al. (2020) Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning via reference-advantage decomposition. _Advances in Neural Information Processing Systems_, 33, 2020b.
* Zhou et al. (2021) Zhengqing Zhou, Qinxun Bai, Zhengyuan Zhou, Linhai Qiu, Jose Blanchet, and Peter Glynn. Finite-sample regret bound for distributionally robust offline tabular reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_, pages 3331-3339. PMLR, 2021.

## Appendix A Other related works

Here we provide additional discussion of related work that could not be fit into the main paper due to space considerations. We limit our discussions to the tabular setting with finite state and action spaces provable RL algorithms.

Classical reinforcement learning with finite-sample guarantees.A recent surge in attention for RL has leveraged the methodologies derived from high-dimensional probability and statistics to analyze RL algorithms in non-asymptotic scenarios. Substantial efforts have been devoted to conducting non-asymptotic sample analyses of standard RL in many settings. Illustrative instances encompass investigations employing Probably Approximately Correct (PAC) bonds in the context of _generative model_ settings (Kearns and Singh, 1999; Beck and Srikant, 2012; Li et al., 2022a; Chen et al., 2020; Azar et al., 2013b; Sidford et al., 2018; Agarwal et al., 2020; Li et al., 2023a, b; Wainwright, 2019) and the _online setting_ via both in PAC-base or regret-based analyses (Jin et al., 2018; Bai et al., 2019; Li et al., 2021; Zhang et al., 2020b; Dong et al., 2019; Jin et al., 2020; Li et al., 2023c; Jafarnia-Jahromi et al., 2020; Yang et al., 2021) and finally _offline setting_(Rashidinejad et al., 2021; Xie et al., 2021; Yin et al., 2021; Shi et al., 2022; Li et al., 2022b; Jin et al., 2021; Yan et al., 2022).

Robustness in reinforcement learning.Reinforcement learning has had notable achievements but has also exhibited significant limitations, particularly when the learned policy is susceptible to deviations in the deployed environment due to perturbations, model discrepancies, or structural modifications. To address these challenges, the idea of robustness in RL algorithms has been studied. Robustness could concern uncertainty or perturbations across different Markov Decision Processes (MDPs) components, encompassing reward, state, action, and the transition kernel. Moos et al. (2022) gives a recent overview of the different work in this field.

The distributionally robust MDP (RMDP) framework has been proposed (Iyengar, 2005) to enhance the robustness of RL has been proposed. In addition to this work, various other research efforts, including, but not limited to, Zhang et al. (2020a, 2021), Han et al. (2022), Clavier et al. (2022), Qiaoben et al. (2021), explore robustness regarding state uncertainty. In these scenarios, the agent's policy is determined on the basis of perturbed observations generated from the state, introducing restricted noise, or undergoing adversarial attacks. Finally, robustness considerations extend to uncertainty in the action domain. Works such as Tessler et al. (2019), Tan et al. (2020) consider the robustness of actions, acknowledging potential distortions introduced by an adversarial agent.

Given the focus of our work, we provide a more detailed background on progress related to distributionally robust RL. The idea of distributionally robust optimization has been explored within the context of supervised learning (Rahimian and Mehrotra, 2019; Gao, 2020; Duchi and Namkoong, 2018; Blanchet and Murthy, 2019) and has also been extended to distributionally robust dynamic programming and Distributionally Robust Markov Decision Processes (DRMDPs) such as in (Iyengar, 2005; Xu and Mannor, 2012; Wolff et al., 2012; Kaufman and Schaefer, 2013; Ho et al., 2018; Smirnova et al., 2019; Ho et al., 2021; Goyal and Grand-Clement, 2022; Derman and Mannor, 2020; Tamar et al., 2014; Badrinath and Kalathil, 2021). Despite the considerable attention received, both empirically and theoretically, most previous theoretical analyses in the context of RMDPs adopt an asymptotic perspective (Roy et al., 2017) or focus on planning with exact knowledge of the uncertainty set (Iyengar, 2005; Xu and Mannor, 2012; Tamar et al., 2014). Many works have focused on the finite-sample performance of verifiable robust Reinforcement Learning (RL) algorithms. These investigations encompass various data generation mechanisms and uncertainty set formulations over the transition kernel. Closelyrelated to our work, various forms of uncertainty sets have been explored, showcasing the versatility of approaches. Divergence such as Kullback-Leibler (KL) divergence is another prevalent choice, extensively studied by Yang et al. (2022); Panaganti and Kalathil (2022); Zhou et al. (2021); Shi and Chi (2022); Xu et al. (2023); Wang et al. (2023); Blanchet et al. (2023), who investigated the sample complexity of both model-based and model-free algorithms in simulator or offline settings. Xu et al. (2023) considered various uncertainty sets, including those associated with the Wasserstein distance. The introduction of an R-contamination uncertainty set Wang and Zou (2021), has been proposed to tackle a robust Q-learning algorithm for the online setting, with guarantees analogous to standard RL. Finally, the finite-horizon scenario has been studied by Xu et al. (2023); Dong et al. (2022) with finite-sample complexity bounds for (RMDPs) using TV and \(\chi^{2}\) divergence. More broadly, other related topics have been explored, such as the iteration complexity of policy-based methods (Li et al., 2022; Kumar et al., 2023), and regularization-based robust RL (Yang et al., 2023). Finally, Badrinath and Kalathil (2021) examined a general \(sa\)-rectangular form of the uncertainty set, proposing a model-free algorithm for the online setting with linear function approximation to address large state spaces.

## Appendix B Further discussions of Theorem 1 and Theorem 3

* _What norms are included in the Definition 1?_ In our upper bound result Theorems 3 and 1, we upper bound the sample complexity for \(C^{2}\) norms and TV. The set of \(C^{2}\) smooth norm is very large as it includes all, \(L_{p}\) norm, weighted, rescaled \(L_{p}\) norms for \(p\geq 2\). Weighted norms can be useful in practice, to get more weights on dangerous specific states in Robust MDPs formulation such as in Russel et al. (2019). Moreover, note that our result can generalize to metric or pseudo metric (which are not homogeneous ie \(\left\|\lambda\right\|=\left|\lambda\right|\left\|x\right\|\forall x\in\mathbb{ R}^{n},\lambda\in\mathbb{R}\)) with norms of the form \(x\mapsto\phi^{-1}(\sum_{k=1}^{n}\phi(\left|x_{k}\right|))\) with \(\phi\) a convex incising function such as the norm is still positive, definite positive. Choosing \(\phi(x)=x^{p}\) leads to the \(L_{p}\) norms.
* _Assumptions on \(\gamma\) in Theorems 1 and 3, and Assumptions on \(\gamma\) for lower bound._ When \(\gamma\) is small (e.g., \(\gamma\in(0,\frac{1}{2}]\) leads to the effective horizon length is at most \(2\)), the sequential structure almost disappears and is much less of interest for RL community. So people Li et al. (2023); Yan et al. (2023) usually focus on reasonable range \(\gamma\in(c,1)\) for some small positive constant \(c\), such as \(\gamma\in[\frac{1}{2},1)\). However, the theorems can be directly extended to a broader range of \(\gamma\in(c,1)\) along with \(c\) as small as desired so that almost cover the full range \((0,1)\).
* _Why final results on \(s\) depend on \(\hat{\pi}\)?_ Theorem 3 is \(\hat{\pi}\) data dependent which is randomness-dependent measure. However, taking the minimum of this quantity leads to the same bound as is \(sa\)-rectangular, so to illustrate that it is possible to get tighter bounds for \(s\)-rectangular with instance-dependent RMDPs, we decide to write also randomness-dependent quantity, while the less tight upper bound is written also in the theorem, taking the first term in the min operator in (21).
* _Why our results are still true for \(Tv\)?_ Theorems 1 and 3 are stated for \(C^{2}\) smooth norms, however, our result is still true for \(TV\) which is not \(C^{2}\) as in this specific case, the dual of the optimization problem becomes a \(1-\)dimensional problem. In this case in the main concentration lemma 8, the additional term involving smoothness term denoted \(C_{S}\) is not present and the bound is simpler as is not required this additional term.
* _Why burn-in or sufficiently small \(\epsilon\) condition is not too restrictive?_ The burn-in term in Th. 1 and 3 is proportional to \(1/\epsilon\) where the "sample complexity" term is proportional to \(1/\epsilon^{2}\). The smooth term depending on \(C_{S}\) or burn-in is then not too large for sufficiently small \(\epsilon\) compared to the other term, which will give final sample complexity.
* _Why this is not extendable to \(f\)-divergence currently?_ The f-divergence as a distinct family of divergence is beyond the scope of this paper. Current proof for arbitrary norms cannot be directly extended since the key phenomenon of shrinking range of the robust value function has not been verified for \(f\)-divergence yet, while it is promising as an interesting future direction.

Preliminaries

These quantities appear in the dual formulation of the robust optimization problem and more preciously the dual span semi norm \(\mathrm{sp}(.)_{*}\) note that for \(L_{2}\), we retrieve the classical mean with the definition of \(\omega\)) With slight abuse of notation, we denote \(0\) (resp. \(1\)) as the all-zero (resp. all-one) vector. We then introduce the notation \([T]:=\{1,\cdots,T\}\) for any positive integer \(T>0\). Then, for all \(1\leq i\leq n\), for two vectors \(x=[x_{i}]_{1\leq i\leq n}\) and \(y=[y_{i}]_{1\leq i\leq n}\), the notation \(x\leq y\) (resp. \(x\geq y\)) means \(x_{i}\leq y_{i}\) (resp. \(x_{i}\geq y_{i}\)). Finally, for any vector \(x\), the notation is overloaded by letting \(x^{\circ 2}=\big{[}x(s,a)^{2}\big{]}_{(s,a)\in\mathcal{S}\times\mathcal{A}}\) (resp. \(x^{\circ 2}=\big{[}x(s)^{2}\big{]}_{s\in\mathcal{S}}\)), Finally, we drop the subscript \(\|.\|\) to write \(\mathcal{U}^{\sigma}_{\|.\|}(\cdot)=\mathcal{U}^{\sigma}(\cdot)\) for both \(sa\)- and \(s\)- rectangular assumptions such that we write uncertainty set in the for \(sa\)-rectangular case \(\mathcal{U}^{\mathbf{a},\sigma}(.)\) or \(\mathcal{U}^{\mathbf{\epsilon},\widetilde{\sigma}}(.)\) in the \(s\)-rectangular assumptions.

Matrix and Vector Notations.We define the following notation.

* \(r\in\mathbb{R}^{SA}\) the reward function, such that \(r_{(s,a)}=r(s,a)\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\).
* \(P^{0}\in\mathbb{R}^{SA\times S}\) the nominal transition kernel matrix using \(P^{0}_{s,a}\) as the \((s,a)\)-th row.
* \(\widehat{P}^{0}\in\mathbb{R}^{SA\times S}\) the estimated nominal transition kernel matrix with \(\widehat{P}^{0}_{s,a}\) as the \((s,a)\)-th row.
* \(\Pi^{\pi}\in\{0,1\}^{S\times SA}\) the projection matrix associated with a policy \(\pi\) \[\Pi^{\pi}=\begin{pmatrix}1^{\top}_{\pi(1)}&0^{\top}&\cdots&0^{\top}\\ 0^{\top}&1^{\top}_{\pi(2)}&\cdots&0^{\top}\\ \vdots&\vdots&\ddots&\vdots\\ 0^{\top}&0^{\top}&\cdots&1^{\top}_{\pi(S)}\end{pmatrix},\] (24) where \(1^{\top}_{\pi(1)},1^{\top}_{\pi(2)},\ldots,1^{\top}_{\pi(S)}\in\mathbb{R}^{A}\) are simplex vector such as \[1^{\top}_{\pi(1)}=(\pi(a_{1}|s_{1}),\pi(a_{2}|s_{1}),...,\pi(a_{A}|s_{1})).\]
* The two matrices \(P^{V}\in\mathbb{R}^{SA\times S}\), \(\widehat{P}^{V}\in\mathbb{R}^{SA\times S}\) represent the probability transition kernel in the uncertainty set that leads to the worst-case value for any vector \(V\in\mathbb{R}^{S}\). Moreover, the quantities \(P^{V}_{s,a}\) (resp. \(\widehat{P}^{V}_{s,a}\)) stands for the \((s,a)\)-th row of the transition matrix \(P^{V}\) (resp. \(\widehat{P}^{V}\)). In \(sa\)-rectangular case, the \((s,a)\)-th rows of these transition matrices are defined as \[P^{V}_{s,a}=\mathrm{argmin}_{\mathcal{P}\in\mathcal{U}^{\mathbf{a},\pi}(P^{0}_ {s,a})}\mathcal{P}V,\qquad\text{and}\qquad\widehat{P}^{V}_{s,a}=\mathrm{ argmin}_{\mathcal{P}\in\mathcal{U}^{\mathbf{a},\sigma}(\widehat{P}^{0}_{s,a})} \mathcal{P}V.\] (25a) Moreover, the shorthand notation defined below is used \[P^{\pi,V}_{s,a} :=P^{V,\sigma}_{s,a}=\mathrm{argmin}_{\mathcal{P}\in\mathcal{U}^ {\mathbf{a},\sigma}(P^{0}_{s,a})}\mathcal{P}V^{\pi,\sigma},\] (25b) \[P^{\pi,\widehat{V}}_{s,a} :=P^{\widehat{V}^{\pi,\sigma}}_{s,a}=\mathrm{argmin}_{\mathcal{P} \in\mathcal{U}^{\mathbf{a},\sigma}(P^{0}_{s,a})}\widehat{P}\widehat{V}^{\pi, \sigma},\] (25c) \[\widehat{P}^{\pi,V}_{s,a} :=\widehat{P}^{V,\sigma}_{s,a}=\mathrm{argmin}_{P\in\mathcal{U}^ {\mathbf{a},\sigma}(\widehat{P}^{0}_{s,a})}PV^{\pi,\sigma},\] (25d) \[\widehat{P}^{\pi,\widehat{V}}_{s,a} :=\widehat{P}^{\pi,\sigma}_{s,a}=\mathrm{argmin}_{P\in\mathcal{U}^ {\mathbf{a},\sigma}(\widehat{P}^{0}_{s,a})}P\widehat{V}^{\pi,\sigma}.\] (25e) In the following, we define the corresponding probability transition matrices which are denoted by \(P^{\pi,V}\in\mathbb{R}^{SA\times S}\), \(P^{\pi,\widehat{V}}\in\mathbb{R}^{SA\times S}\), \(\widehat{P}^{\pi,V}\in\mathbb{R}^{SA\times S}\) and \(\widehat{P}^{\pi,\widehat{V}}\in\mathbb{R}^{SA\times S}\).
* Using the projection over \(\pi\), the matrices \(P^{\pi}\in\mathbb{R}^{S\times S}\), \(\widehat{P}^{\pi}\in\mathbb{R}^{S\times S}\), \(\underline{P}^{\pi,V}\in\mathbb{R}^{S\times S}\), \(\underline{P}^{\pi,\widehat{V}}\in\mathbb{R}^{S\times S}\), \(\underline{\widehat{P}}^{\pi,V}\in\mathbb{R}^{S\times S}\) and \(\underline{\widehat{P}}^{\pi,\widehat{V}}\in\mathbb{R}^{S\times S}\) represent probability transition matrices w.r.t. policy \(\pi\). \[P^{\pi}\coloneqq\Pi^{\pi}P^{0},\qquad\widehat{P}^{\pi}\coloneqq\Pi^{\pi} \widehat{P}^{0},\qquad\underline{P}^{\pi,V}\coloneqq\Pi^{\pi}P^{\pi,V},\qquad \underline{P}^{\pi,\widehat{V}}\coloneqq\Pi^{\pi}P^{\pi,\widehat{V}},\] \[\underline{\widehat{P}}^{\pi,V}\coloneqq\Pi^{\pi}\widehat{P}^{\pi, V},\qquad\text{and}\qquad\underline{\widehat{P}}^{\pi,\widehat{V}}\coloneqq\Pi^{\pi} \widehat{P}^{\pi,\widehat{V}}.\] (26) For \(s\)-rectangular, we will use the same notation for these transition matrices. Finally, we denote \(P^{\pi}_{s}\) as the \(s\)-th row of the transition matrix \(P^{\pi}\).

* \(r_{\pi}\in\mathbb{R}^{S}\) is the reward function restricted to the actions chosen by \(\pi\), \(r_{\pi}=\Pi^{\pi}r\).
* \(\operatorname{Var}_{P}(V)\in\mathbb{R}^{SA}\) is the variance for a given transition kernel \(P\in\mathbb{R}^{SA\times S}\) and vector \(V\in\mathbb{R}^{S}\), we denote the \((s,a)\)-th row of \(\operatorname{Var}_{P}(V)\) as \[\operatorname{Var}_{P}(s,a)\coloneqq\operatorname{Var}_{P_{s,a}}(V).\] (27)

### Additional definitions and basic facts

For any norm smooth \(\|.\|\) introduced in 1, we define the span semi norm as

**Definition 2** (Span semi norm).: _Given any norm \(\|\cdot\|\), we define the span semi norm as: \(\operatorname{sp}(x)=\min_{\omega\in\mathbb{R}}\|v-\omega\mathbf{1}\|\) and the generalized mean as \(\omega(x):=\operatorname{arg\,min}_{\omega\in\mathbb{R}}\|x-\omega\mathbf{1}\|\)._

Let vector \(P\in\mathbb{R}^{1\times S}\) and vector \(V\in\mathbb{R}^{S}\), we define the variance

\[\operatorname{Var}_{P}(V)\coloneqq P(V\circ V)-(PV)\circ(PV). \tag{28}\]

The following lemma bounds the Lipschitz constant of the variance function.

**Lemma 1**.: _(Shi et al. (2023), Lemma 2 ) Assuming \(0\leq V_{1},V_{2}\leq\frac{1}{1-\gamma}\) which obey \(\|V_{1}-V_{2}\|_{\infty}\leq x\), then for \(P\in\Delta(S)\), one has_

\[|\operatorname{Var}_{P}(V_{1})-\operatorname{Var}_{P}(V_{2})|\leq\frac{2x}{(1 -\gamma)}. \tag{29}\]

**Lemma 2**.: _(Panaganti and Kalathil, 2022, Lemma 6) Consider any \(\delta\in(0,1)\). For any fixed policy \(\pi\) and fixed value vector \(V\in\mathbb{R}^{S}\), one has with probability at least \(1-\delta\),_

\[\left|\sqrt{\operatorname{Var}_{\widehat{P}^{\pi}}(V)}-\sqrt{\operatorname{ Var}_{P^{\pi}}(V)}\right|\leq\sqrt{\frac{2\|V\|_{\infty}^{2}\log(\frac{2SA}{ \delta})}{N}}1.\]

### Empirical robust MDP \(\widehat{\mathcal{M}}_{\text{rob}}\) Bellman equations

We define the robust MDP \(\widehat{\mathcal{M}}_{\text{rob}}=\{\mathcal{S},\mathcal{A},\gamma,\mathcal{ U}^{\sigma}(\widehat{P}^{0}),r\}\) based on the estimated nominal distribution \(\widehat{P}^{0}\) in (11). Then, we denote the associated robust value function (resp. robust Q-function) are \(\widehat{V}^{\pi,\sigma}\) (resp. \(\widehat{Q}^{\pi,\sigma}\)) qnd we can notice that that \(\widehat{Q}^{*,\sigma}\) is the unique-fixed point of \(\widehat{\mathcal{T}}^{\sigma}(\cdot)\) (see Lemma C.3), the empirical robust Bellman operator constructed using \(\widehat{P}^{0}\). Finally, similarly to (9), for \(\widehat{\mathcal{M}}_{\text{rob}}\), the Bellman's optimality principle gives the following _robust Bellman consistency equation_ (resp. _robust Bellman optimality equation_) for _sa_-rectangular assumptions:

\[\widehat{Q}^{\pi,\sigma}(s,a) =r(s,a)+\gamma\inf_{\mathcal{P}\in\mathcal{U}^{\pi,\sigma}( \widehat{P}^{0}_{s,a})}\mathcal{P}\widehat{V}^{\pi,\sigma}, \tag{30a}\] \[\widehat{Q}^{*,\sigma}(s,a) =r(s,a)+\gamma\inf_{\mathcal{P}\in\mathcal{U}^{\pi,\sigma}( \widehat{P}^{0}_{s,a})}\mathcal{P}\widehat{V}^{*,\sigma}. \tag{30b}\]

Using matrix notation, we can write the 

[MISSING_PAGE_FAIL:19]

where we use the change of variable \(y(s^{\prime})=\mathcal{P}(s^{\prime})-P(s^{\prime})\) for all \(s^{\prime}\in\mathcal{S}\). Then the Lagrangian function of the above optimization problem can be written as follows:

\[\inf_{\mathcal{P}\in\mathcal{U}_{s,a}^{\mathcal{P}}(P)}\mathcal{P}V= PV+\sup_{\mu\geq 0,\nu\in\mathbb{R}}\inf_{\{y:\left\|y\right\|\leq\sigma\}}- \sum_{s^{\prime}}\mu(s)P(s^{\prime})+\sum_{s^{\prime}}(y(s^{\prime})(V(s^{ \prime})-\mu(s^{\prime})-\nu) \tag{41}\] \[\stackrel{{(a)}}{{=}}PV+\sup_{\mu\geq 0,\nu\in\mathbb{ R}}-\sum_{s^{\prime}}\mu(s^{\prime})P(s^{\prime})-\sigma\left\|(V(s^{\prime})- \mu(s^{\prime})-\nu\mathbf{1})\right\|_{*}\] (42) \[\stackrel{{(b)}}{{=}}\sup_{\mu\geq 0}P(V-\mu)- \sigma\mathrm{sp}(V-\mu)_{*} \tag{43}\]

where \(\mu\in\mathbb{R}_{+}^{\mathcal{S}}\), \(\nu\in\mathbb{R}\) are Lagrangian variables, (a) is true using the equality case of Cauchy-Swartz inequality for dual norm Yang (1991), and (b) is due to is the definition of the span semi-norm (see (C)). The value that maximizes the inner maximization problem in (42) in \(\omega(V,\mu)\) is the generalized-mean by definition denoted with abbreviate notation \(\omega\). If the norm is differentiable, then we have that the equality (a) comes from the generalized Holder's inequality for arbitrary norms Yang (1991), namely, defining \(z=(V-\mu-\omega)\), it satisfies

\[z=\left\|z\right\|_{*}\nabla\left\|y\right\| \tag{44}\]

The quantity \(\nu\) is replaced by the generalized mean for equality in (b) while (44) comes from Yang (1991). Using complementary slackness Karush (2013)stackness let \(\mathcal{B}=\{s\in\mathcal{S}:\mu(s)>0\}\)

\[\forall s\in\mathcal{B}:\quad y^{*}(s)=-P(s), \tag{45}\]

which leads to the following equality by plugging the previous (45) in (44) and defining \(z^{*}=V-\mu^{*}-\omega\):

\[\forall s\in\mathcal{B},\quad z^{*}(s)=\left\|z^{*}\right\|_{*} \nabla\left\|P\right\|(s) \tag{46}\]

or

\[\forall s\in\mathcal{B},\quad V(s)-\mu^{*}(s)=\omega+\lambda\nabla \left\|P\right\|(s)\dot{=}\alpha_{P}^{\lambda,\omega} \tag{47}\]

by letting \(\lambda=\left\|z^{*}\right\|_{*}\in\mathbb{R}^{+}\). Note that here the hypothesis of 1 are use and especially separability is needed to ensure that for \(s\in\mathcal{B}\), \(\nabla\left\|y\right\|=\nabla\left\|P\right\|\) only depend on \(P(s)\) and not on other coordinates, which is true form generalized \(L_{p}\) norms. We can remark that \(v-\mu^{*}\) is \(P\) dependent, but if \(P\) is known, the best \(\mu^{*}\) is only determined by one 2 dimensional parameters \(\lambda=\left\|v-\mu^{*}-\nu\right\|_{*}\) and \(\omega\in\mathbb{R}^{+}\). Moreover, when \(P\) is fixed, the scalar \(\omega\) is a constant is fully determined by \(P\), \(v\) and \(\mu^{*}\). This is why the quantity defined \(\alpha_{P}^{\lambda}\) varies through 2 parameter \(\lambda\) and \(\omega\). Given this observation, we can rewrite the optimization problem as :

\[\sup_{\mu\geq 0}P(V-\mu)-\sigma\mathrm{sp}(V-\mu)_{*}=\sup_{ \mu_{P}^{\lambda,\omega}\in\mathcal{M}_{P}^{\lambda,\omega}}P(V-\mu_{P}^{ \lambda,\omega})-\sigma\mathrm{sp}((V-\mu_{P}^{\lambda,\omega}))_{*} \tag{48}\] \[=\sup_{\alpha_{P}^{\lambda,\omega}\in\Lambda_{P}^{\lambda,\omega} }P[V]_{\alpha_{P}^{\lambda,\omega}}-\sigma\mathrm{sp}([V]_{\alpha_{P}^{ \lambda,\omega}})_{*} \tag{49}\]

where we defined the maximization problem on \(\mu\) not in \(\mathbb{R}^{S}\) but at the optimal in the variational family denote \(\mathcal{M}_{P}^{\lambda,\omega}=\{v-\alpha_{P}^{\lambda,\omega},(\lambda, \omega)\in\mathbb{R}_{+}^{2},P\in\Delta(S)\}\). We can rewrite the optimization problem in terms of \(\alpha_{P}\) with \(\left[V\right]_{\alpha_{P}^{\lambda,\omega}}\) defined in 35. Contrary to the \(TV\) case, \(\alpha\) is not a scalar but \(\alpha_{P}^{\lambda,\omega}\) belongs to a variational family only determined by two parameter. Note that this lemma is still true writing subgradient and not gradient of \(P\). As we assume \(C^{2}\)-regularity on norms, the subgradient space of the norm reduce to the singleton of the gradient in our case. \(C^{2}\) smoothness will be needed in concentration part while it is possible to be more general in optimization lemmas. Note that for \(TV\) or \(L_{1}\), this lemma holds, but the vector \(\alpha_{P}^{\lambda,\omega}\) reduces to a positive scalar denoted \(\alpha\) which is equal to \(\left\|v-\mu^{*}\right\|_{\infty}\) according to Iyengar (2005).

**Lemma 4** (Strong duality for the distance induced by the norm \(\left\|\right\|\) in the \(s\)-rectangular case.).: _Consider any probability vector \(P^{\pi}\coloneqq\Pi^{\pi}P\in\Delta_{s}\) for \(P\in\Delta(S)^{A}\), any fixed uncertainty level \(\bar{\sigma}\)_and the uncertainty set \(\mathcal{U}^{\mathfrak{s},\tilde{\sigma}}_{\|.\|}(P)\), we abbreviate the subscript to use \(\mathcal{U}^{\mathfrak{s},\tilde{\sigma}}(P)\coloneqq\mathcal{U}^{\mathfrak{s}, \tilde{\sigma}}_{\|.\|}(P)\). Then for any vector \(V\in\mathbb{R}^{S}\) obeying \(V\geq 0\), recalling the definition of \([V]_{\alpha}\) in (35), one has_

\[\inf_{\mathcal{P}\in\mathcal{U}^{\mathfrak{s},\tilde{\sigma}}(P)} \mathcal{P}^{\pi}V=\sum_{a}\pi(a|s)(\Big{(}\max_{\alpha^{\lambda,\omega}_{P_{ a}}\in\mathbb{A}^{\lambda,\omega}_{P_{a}}}P_{sa}[V]_{\alpha^{\lambda,\omega}_{P_{ a}}}-\tilde{\sigma}\left\|\pi_{s}\right\|_{*}\operatorname{sp}([V]_{\alpha^{ \lambda,\omega}_{P_{sa}}})_{*}\Big{)}. \tag{50}\]

_with the definition of \(\operatorname{sp}()_{*}\) in \(\mathcal{C}\) and where the variational family \(\Lambda^{\lambda,\omega}_{P}\) is defined as :_

\[\Lambda^{\lambda,\omega}_{P}=\{\alpha\in\left[0,1/(1-\gamma)\right]^{S},\alpha =\omega+\lambda|\nabla\left\|P\right\|\mid:=\alpha^{\lambda,\omega}_{P}\} \tag{51}\]

_with \(\omega\) is the generalized mean defined as the argmin in the definition of the span semi norm in 2 and \(\lambda,\omega\) a positive scalar. Moreover, for \(L_{1}\) or \(TV\), case, the vector \(\alpha^{\lambda,\omega}_{P}\) reduces to a \(1\) dimensional scalar such as \(\alpha\in[0,1/(1-\gamma)]\)._

In the proof of the previous lemma, we decompose this problem \(s\)-rectangular radius \(\tilde{\sigma}\) into \(sa\)-rectangular sub-problem with respectively radius \(\sigma_{sa}\).

Proof.: \[\inf_{\mathcal{P}^{\pi}\in\mathcal{U}^{\mathfrak{s},\tilde{\sigma }}(P^{*})}\mathcal{P}^{\pi}V=\inf_{\{\sigma_{sa}:\left\|\sigma_{sa}\right\|\leq \tilde{\sigma}\}\ \mathcal{P}^{\prime}\in\mathcal{U}^{\mathfrak{s},\sigma}(P_{sa})} \sum_{a}\pi(a|s)\mathcal{P}^{\prime}V\] \[\stackrel{{(a)}}{{=}}\sum_{a}\pi(a|s)P_{sa}V+\min_{ \{\sigma_{sa}:\left\|\sigma_{sa}\right\|\leq\tilde{\sigma}\}}\sum_{a}\pi(a|s) \sum_{\{y:\left\|y\right\|\leq\sigma_{sa},1y=0,y\geq-P_{sa}\}}\sum_{s^{\prime }}y(s^{\prime})V\]

where we use the change of variable \(y(s^{\prime})=\mathcal{P}_{sa}(s^{\prime})-P_{sa}(s^{\prime})\) in (a). Then we case use the previous lemma for \(sa\) rectangular assumption, Lemma 3. Then,

\[\min_{\{\sigma_{sa}:\left\|\sigma_{sa}\right\|\leq\tilde{\sigma}\} }\sum_{a}\pi(a|s)\min_{\{y,\left\|y\right\|\leq\sigma_{sa},1y=0,y\geq-P_{sa}\} }\sum_{s^{\prime}}y(s^{\prime})V\] \[=\min_{\{\sigma_{sa}:\left\|\sigma_{sa}\right\|\leq\tilde{ \sigma}\}}\sum_{a}\pi(a|s)\max_{\mu\geq 0}\Big{(}-P_{sa}\mu-\sigma_{sa} \operatorname{sp}(V-\mu)_{*}\Big{)}\] \[=\left(\sum_{a}\pi(a|s)\max_{\mu\geq 0}\Big{\{}(-P_{sa}\mu)- \max_{\{\sigma_{sa}:\left\|\sigma_{sa}\right\|\leq\tilde{\sigma}\}}\sum_{a} \pi(a|s)\sigma\operatorname{sp}(V-\mu)_{*}\Big{\}}\right)\] \[=\sum_{a}\pi(a|s)\max_{\mu\geq 0}\Big{\{}(-P_{sa}\mu)-\tilde{ \sigma}\left\|\pi_{s}\right\|_{*}\operatorname{sp}(V-\mu)_{*}\Big{\}}.\]

We can exchange the min and the max as we get concave-convex problems in \(\sigma\) and \(\mu\) in the second line according to minimax theorem (v. Neumann, 1928) and using Cauchy Swartz inequality which is attained in the last equality. Finally, we obtain:

\[\inf_{\mathcal{P}\in\mathcal{U}^{\mathfrak{s},\tilde{\sigma}}(P)} \mathcal{P}^{\pi}V= \sum_{a}\pi(a|s)\Big{(}\max_{\mu\geq 0}P_{sa}(V-\mu)-\tilde{ \sigma}\left\|\pi_{s}\right\|_{*}\operatorname{sp}(V-\mu)_{*}\Big{)}\] \[\stackrel{{(a)}}{{=}} \sum_{a}\pi(a|s)\Big{(}\max_{\alpha^{\lambda,\omega}_{P_{sa}}\in \Lambda^{\lambda,\omega}_{P_{sa}}}P_{sa}[V]_{\alpha^{\lambda,\omega}_{P_{sa}}} -\tilde{\sigma}\left\|\pi_{s}\right\|_{*}\operatorname{sp}([V]_{\alpha^{ \lambda,\omega}_{P_{sa}}})_{*}\Big{)}\]

where in (a) we use the previous lemma for \(sa-\) rectangular case. Note that as we are using \(sa\)-rectangular case, for \(TV\) or \(L_{1}\), this lemma holds, but the vector \(\alpha^{\lambda}_{P}\) reduces to a positive scalar denoted \(\alpha\) which is equal to \(\left\|v-\mu^{*}\right\|_{\infty}\). (See also Iyengar (2005)).

Proof of the upper bound : Theorem 1 and 3

### Technical lemmas

We begin with a key lemma concerning the dynamic range of the robust value function \(V^{\pi,\sigma}\) (cf. (7)), which produces tighter control when \(\sigma\) is large; the proof is deferred to Appendix D.3.1. This lemma allows tighter control compared to Clavier et al. (2023).

**Lemma 5**.: _In \(sa-\)rectangular case (see (3), for any nominal transition kernel \(P\in\mathbb{R}^{SA\times S}\), any fixed uncertainty level \(\sigma\), and any policy \(\pi\), its corresponding robust value function \(V^{\pi,\sigma}\) (cf. (7)) satisfies_

\[\mathrm{sp}(V^{\pi,\sigma})_{\infty}\leq\frac{1}{\gamma\max\{1-\gamma,C_{g} \sigma\}} \tag{53}\]

where \(C_{g}=1/(\min_{s}\left\|e_{s}\right\|)\) is a geometric constant depending on the geometry of the norm. For example, for \(L_{p}\), norms \(p\geq 1\), \(C_{g}\geq 1\) which reduce the sample complexity. In \(s\)-rectangular case, we obtain a slightly different lemma because of the dependency on \(\pi\).

**Lemma 6**.: _The infinite span semi norm can be controlled as follows for every \(s\) in \(s\)-rectangular case (See (5)):_

\[\mathrm{sp}(V^{\pi,\sigma})_{\infty}\leq\frac{1}{\gamma\max\{1-\gamma,\left\| \pi_{s}\right\|_{*}C_{g}\tilde{\sigma}\}}\leq\frac{1}{\gamma\max\{1-\gamma, \min_{s}\left\|\pi_{s}\right\|_{*}C_{g}\tilde{\sigma}\}} \tag{54}\]

where \(C_{g}=\frac{1}{\min_{s}\left\|e_{s}\right\|}\) is a geometric constant depending on the geometry of the norm. These lemmas are required to get tight bounds for the sample complexity. The main difference between \(sa\)- and \(s\)-rectangular case is that we have an extra dependency on \(\left\|\pi_{s}\right\|_{*}\), which represents how stochastic the policy can be in \(s\) rectangular MDPs.

**Lemma 7**.: _Consider an MDP with transition kernel matrix \(P\) and reward function \(0\leq r\leq 1\). For any policy \(\pi\) and its associated state transition matrix \(P_{\pi}\coloneqq\Pi^{\pi}P\) and value function \(0\leq V^{\pi,P}\leq\frac{1}{1-\gamma}\) (cf. (1)), one has for \(sa\)- and \(s\)- rectangular assumptions._

\[\left(I-\gamma P_{\pi}\right)^{-1}\sqrt{\mathrm{Var}_{P_{\pi}}(V^{\pi,P})}\leq \sqrt{\frac{8}{\gamma^{2}(1-\gamma)^{2}}\mathrm{sp}(V^{\pi,P})_{\infty}}1.\]

_See D.3.7 for the proof_

### Proof of Theorem 1 and Theorem 3

The first decomposition of the proof of Theorem 1 and Theorem 3 Agarwal et al. (2020) while the argument needs essential adjustments in order to adapt to the robustness setting. One has by assumptions using any planner in empirical RMDPs :

\[\left\|\widehat{V}^{\star,\sigma}-\widehat{V}^{\widehat{\pi},\sigma}\right\|_ {\infty}\leq\varepsilon_{\mathsf{opt}}, \tag{55}\]

using previous inequality, performance gap \(\left\|V^{\star,\sigma}-V^{\widehat{\pi},\sigma}\right\|_{\infty}\), can be upper bounded using 3 steps.

First step: subdivide the performance gap in 3 terms.We recall the definition of the optimal robust policy \(\pi^{\star}\) with regard to \(\mathcal{M}_{\text{rob}}\) and the optimal robust policy \(\widehat{\pi}^{\star}\), the optimal robust value function \(\widehat{V}^{\star,\sigma}\) (resp. robust value function \(\widehat{Q}^{\pi,\sigma}\)) w.r.t. \(\widehat{\mathcal{M}}_{\text{rob}}\). Then, the performance gap \(V^{\star,\sigma}-V^{\widehat{\pi},\sigma}\) can be decomposed in one optimization term and two statistical error terms

\[V^{\star,\sigma}-V^{\widehat{\pi},\sigma} =\left(V^{\pi^{\star},\sigma}-\widehat{V}^{\pi^{\star},\sigma} \right)+\left(\widehat{V}^{\pi^{\star},\sigma}-\widehat{V}^{\widehat{\pi}^{ \star},\sigma}\right)+\left(\widehat{V}^{\widehat{\pi}^{\star},\sigma}- \widehat{V}^{\widehat{\pi},\sigma}\right)+\left(\widehat{V}^{\widehat{\pi}, \sigma}-V^{\widehat{\pi},\sigma}\right)\] \[\stackrel{{\mathrm{(i)}}}{{\leq}}\left(V^{\pi^{\star},\sigma}-\widehat{V}^{\pi^{\star},\sigma}\right)+\left(\widehat{V}^{\widehat{ \pi}^{\star},\sigma}-\widehat{V}^{\widehat{\pi},\sigma}\right)+\left(\widehat{V} ^{\widehat{\pi},\sigma}-V^{\widehat{\pi},\sigma}\right)\] \[\stackrel{{\mathrm{(ii)}}}{{\leq}}\left(V^{\pi^{ \star},\sigma}-\widehat{V}^{\pi^{\star},\sigma}\right)+\varepsilon_{\mathsf{opt }}+\left(\widehat{V}^{\widehat{\pi},\sigma}-V^{\widehat{\pi},\sigma}\right) \tag{56}\]where (i) holds by \(\widehat{V}^{\pi^{*},\sigma}-\widehat{V}^{\hat{\pi}^{*},\sigma}\leq 0\) since \(\widehat{\pi}^{*}\) is the robust optimal policy for \(\widehat{\mathcal{M}}_{\text{rob}}\), and (ii) comes from (55) and definition of optimization error. The proof aims to control the last remaining terms in (56) using concentration theory and sufficiently big number of step \(N\). To do so, we will consider a more general term \(\widehat{V}^{\pi,\sigma}-V^{\pi,\sigma}\) for any policy \(\pi\) even if control of these two terms slightly differ at the end. Using (32), it holds that for both \(sa\)- and \(s\)-rectangular assumptions:

\[\widehat{V}^{\pi,\sigma}-V^{\pi,\sigma} =r_{\pi}+\gamma\widehat{\underline{P}}^{\pi,\widehat{V}}\widehat{ V}^{\pi,\sigma}-\big{(}r_{\pi}+\gamma\underline{P}^{\pi,V}V^{\pi,\sigma}\big{)}\] \[=\Big{(}\gamma\widehat{\underline{P}}^{\pi,\widehat{V}}\widehat{ V}^{\pi,\sigma}-\gamma\underline{P}^{\pi,\widehat{V}}\widehat{V}^{\pi,\sigma} \Big{)}+\Big{(}\gamma\underline{P}^{\pi,\widehat{V}}\widehat{V}^{\pi,\sigma}- \gamma\underline{P}^{\pi,V}V^{\pi,\sigma}\Big{)}\] \[\overset{\text{(i)}}{\leq}\gamma\Big{(}\underline{P}^{\pi,V} \widehat{V}^{\pi,\sigma}-\underline{P}^{\pi,V}V^{\pi,\sigma}\Big{)}+\Big{(} \gamma\widehat{\underline{P}}^{\pi,\widehat{V}}\widehat{V}^{\pi,\sigma}- \gamma\underline{P}^{\pi,\widehat{V}}\widehat{V}^{\pi,\sigma}\Big{)},\]

where (i) holds because \(\underline{P}^{\pi,\widehat{V}}\widehat{V}^{\pi,\sigma}\leq\underline{P}^{\pi, V}\widehat{V}^{\pi,\sigma}\) because of the optimality of \(\underline{P}^{\pi,\widehat{V}}\) (see. (25)). Factorizing terms leads to the following equation

\[\widehat{V}^{\pi,\sigma}-V^{\pi,\sigma}\leq\gamma\left(I-\gamma \underline{P}^{\pi,V}\right)^{-1}\Big{(}\widehat{\underline{P}}^{\pi,\widehat {V}}\widehat{V}^{\pi,\sigma}-\underline{P}^{\pi,\widehat{V}}\widehat{V}^{\pi, \sigma}\Big{)}. \tag{57}\]

In the same manner, we can also obtain a lower bound of this quantity:

\[\widehat{V}^{\pi,\sigma}-V^{\pi,\sigma} =r_{\pi}+\gamma\widehat{\underline{P}}^{\pi,\widehat{V}}\widehat{ V}^{\pi,\sigma}-\big{(}r_{\pi}+\gamma\underline{P}^{\pi,V}V^{\pi,\sigma}\big{)}\] \[=\Big{(}\gamma\widehat{\underline{P}}^{\pi,\widehat{V}}\widehat{ V}^{\pi,\sigma}-\gamma\underline{P}^{\pi,\widehat{V}}\widehat{V}^{\pi,\sigma} \Big{)}+\Big{(}\gamma\underline{P}^Second step: bound first term and second term in (62) to control \(\left\|\widehat{V}^{\pi^{*},\sigma}-V^{\pi^{*},\sigma}\right\|_{\infty}\)To control the two terms in (62), we use lemma 8 based Bernstein's concentration argument and whose proof is in Appendix D.3.3.

**Lemma 8**.: _For both \(sa-\) and \(s\)-rectangular setting, consider any \(\delta\in(0,1)\), with probability \(1-\delta\), it holds:_

\[\left|\underline{\widehat{P}}^{\pi^{*},V}V^{\pi^{*},\sigma}-\underline{P}^{\pi ^{*},V}V^{\pi^{*},\sigma}\right|\leq 2\sqrt{\frac{L}{N}}\sqrt{\operatorname{Var}_{P^ {\pi^{*}}}\left(V^{*,\sigma}\right)}+\frac{3LC_{S}\left\|1\right\|_{*}}{N(1- \gamma)}1 \tag{63}\]

_with \(L=2\log(18\left\|1\right\|_{*}SAN/\delta)\) and where \(\operatorname{Var}_{P^{\pi^{*}}}\left(V^{*,\sigma}\right)\) is defined in (27). Moreover, for the specific case of \(TV\), this lemma is true without the smoothness term \(\frac{3LC_{S}\left\|1\right\|_{*}}{N(1-\gamma)}\)._

Armed with the above lemma, now we control the **first term** on the right-hand side of (62) as follows:

\[\left(I-\gamma\underline{\widehat{P}}^{\pi^{*},V}\right)^{-1} \Big{(}\underline{\widehat{P}}^{\pi^{*},V}V^{\pi^{*},\sigma}-\underline{P}^{\pi ^{*},V}V^{\pi^{*},\sigma}\Big{)}\] \[\overset{\text{(a)}}{\leq}\Big{(}I-\gamma\underline{\widehat{P}} ^{\pi^{*},V}\Big{)}^{-1}\Big{\|}\underline{\widehat{P}}^{\pi^{*},V}V^{\pi^{*}, \sigma}-\underline{P}^{\pi^{*},V}V^{\pi^{*},\sigma}\Big{\|}_{\infty}\] \[\overset{\text{(b)}}{\leq}\Big{(}I-\gamma\underline{\widehat{P}} ^{\pi^{*},V}\Big{)}^{-1}\bigg{(}2\sqrt{\frac{L}{N}}\sqrt{\operatorname{Var}_{P ^{\pi^{*}}}\left(V^{*,\sigma}\right)}+\frac{3LC_{S}\left\|1\right\|_{*}}{N(1- \gamma)}\bigg{)}\] \[\leq\Big{(}I-\gamma\underline{\widehat{P}}^{\pi^{*},V}\Big{)}^{-1 }\frac{3LC_{S}\left\|1\right\|_{*}}{N(1-\gamma)}1+\underbrace{2\sqrt{\frac{L} {N}}\Big{(}I-\gamma\underline{\widehat{P}}^{\pi^{*},V}\Big{)}^{-1}\sqrt{ \operatorname{Var}_{\widehat{P}^{\pi^{*},V}}\left(V^{*,\sigma}\right)}}_{=: \mathcal{R}_{1}}\] \[\quad+\underbrace{2\sqrt{\frac{L}{N}}\Big{(}I-\gamma\underline{ \widehat{P}}^{\pi^{*},V}\Big{)}^{-1}\sqrt{\big{|}\operatorname{Var}_{\widehat {P}^{\pi^{*}}}\left(V^{*,\sigma}\right)-\operatorname{Var}_{\widehat{P}^{\pi^{ *},V}}\left(V^{*,\sigma}\right)}\Big{]}}_{=:\mathcal{R}_{3}} \tag{64}\]

where (a) holds as the matrix \(\left(I-\gamma\underline{\widehat{P}}^{\pi^{*},V}\right)^{-1}\) is positive definite, (b) holds due to Lemma 8, and the last point holds from the following decomposition for variance and triangular inequality

\[\sqrt{\operatorname{Var}_{P^{\pi^{*}}}\left(V^{*,\sigma}\right)} =\Big{(}\sqrt{\operatorname{Var}_{P^{\pi^{*}}}\left(V^{*,\sigma}\right)}- \sqrt{\operatorname{Var}_{\widehat{P}^{\pi^{*}}}\left(V^{*,\sigma}\right)} \Big{)}+\sqrt{\operatorname{Var}_{\widehat{P}^{\pi^{*}}}\left(V^{*,\sigma} \right)}\] \[\leq\Big{(}\sqrt{\operatorname{Var}_{P^{\pi^{*}}}\left(V^{*, \sigma}\right)}-\sqrt{\operatorname{Var}_{\widehat{P}^{\pi^{*}}}\left(V^{*, \sigma}\right)}\Big{)}\] \[+\sqrt{\big{|}\operatorname{Var}_{\widehat{P}^{\pi^{*}}}\left(V^ {*,\sigma}\right)-\operatorname{Var}_{\widehat{P}^{\pi^{*},V}}\left(V^{*, \sigma}\right)\big{|}}+\sqrt{\operatorname{Var}_{\widehat{P}^{\pi^{*},V}}\left( V^{*,\sigma}\right)}.\]

Finally, the fact that \(\underline{\widehat{P}}^{\pi^{*},V}\) is a stochastic matrix, so

\[\Big{(}I-\gamma\underline{\widehat{P}}^{\pi^{*},V}\Big{)}^{-1}1=\Big{(}I+\sum _{t=1}^{\infty}\gamma^{t}\Big{(}\underline{\widehat{P}}^{\pi^{*},V}\Big{)}^{ t}\Big{)}1\leq\frac{1}{1-\gamma}1. \tag{65}\]

Armed with these inequalities, the three terms \(\mathcal{R}_{1},\mathcal{R}_{2},\mathcal{R}_{3}\) in (64) can be controlled separately.

* Consider \(\mathcal{R}_{1}\). We first introduce the following lemma, whose proof is postponed to Appendix D.3.4.

[MISSING_PAGE_EMPTY:25]

[MISSING_PAGE_FAIL:26]

[MISSING_PAGE_FAIL:27]

[MISSING_PAGE_EMPTY:28]

[MISSING_PAGE_FAIL:29]

where (i) and (ii) hold by the fact that each row of \((1-\gamma)\left(I-\gamma\underline{P}^{\widehat{\pi},\widehat{V}}\right)^{-1}\) is a probability vector that falls into \(\Delta(\mathcal{S})\). The remainder of the proof will focus on controlling the three terms in (93) separately.

* For \(\mathcal{S}_{1}\), we introduce the following lemma, whose proof is postponed to D.3.6. **Lemma 11**.: _Consider any \(\delta\in(0,1)\). Taking \(N\geq\frac{L^{\prime\prime}}{(1-\gamma)^{2}}\) one has with probability at least \(1-\delta\), for \(sa-\) rectangular_ \[\left(I-\gamma\underline{P}^{\widehat{\pi},\widehat{V}}\right)^{-1} \sqrt{\operatorname{Var}_{\underline{P}^{\widehat{\pi},\widehat{V}}}(\widehat{ V}^{\widehat{\pi},\sigma})} \leq 6\sqrt{\frac{\left(1+\varepsilon_{\mathsf{opt}}+\frac{L^{\prime \prime}C_{\widehat{\pi}}\|1\|_{*}}{N(1-\gamma)}\right)}{\gamma^{3}(1-\gamma)^ {2}\max\{1-\gamma,\sigma\}}}1\] \[\leq 6\sqrt{\frac{\left(1+\varepsilon_{\mathsf{opt}}+\frac{L^{ \prime\prime}C_{\widehat{\pi}}\|1\|_{*}}{N(1-\gamma)}\right)}{(1-\gamma)^{3} \gamma^{3}}}1.\] _and for_ \(s\)_-rectangular_ \[\left(I-\gamma\underline{P}^{\widehat{\pi},\widehat{V}}\right)^{-1} \sqrt{\operatorname{Var}_{\underline{P}^{\widehat{\pi},\widehat{V}}}(\widehat{ V}^{\widehat{\pi},\sigma})} \leq 6\sqrt{\frac{L^{\prime\prime}\Big{(}1+\varepsilon_{\mathsf{opt }}+\frac{C_{\widehat{\pi}}\|1\|_{*}}{N(1-\gamma)}\Big{)}}{\gamma^{3}(1-\gamma)^ {2}\max\{1-\gamma,C_{g}\tilde{\sigma}\min_{s}\|\hat{\pi}_{s}\|_{\infty}\}}}1\] \[\leq 6\sqrt{\frac{L^{\prime\prime}\Big{(}1+\varepsilon_{\mathsf{ opt}}+\frac{C_{\widehat{\pi}}\|1\|_{*}}{N(1-\gamma)}\Big{)}}{(1-\gamma)^{3}\gamma^{2}}}1.\] Applying Lemma 11 and (65) to (93) leads to \[\mathcal{S}_{1} =2\sqrt{\frac{L}{N}}\Big{(}I-\gamma\underline{P}^{\widehat{\pi}, \widehat{V}}\Big{)}^{-1}\sqrt{\operatorname{Var}_{\underline{P}^{\widehat{\pi},\widehat{V}}}(\widehat{V}^{\widehat{\pi},\sigma})}\] \[\leq 12\sqrt{\frac{L^{\prime\prime}}{\gamma^{3}(1-\gamma)^{2}\max \{1-\gamma,C_{g}\sigma\}N}}1.\] (94) for \(sa\)-rectangular and the same quantity replacing \(\max\{1-\gamma,C_{g}\sigma\}\) by \(\max\{1-\gamma,C_{g}\tilde{\sigma}\min_{s}\|\hat{\pi}_{s}\|_{*}\}\) for \(s-\) rectangular case.
* Applying Lemma 1 with \(\|\widehat{V}^{\star,\sigma}-\widehat{V}^{\widehat{\pi},\sigma}\|_{\infty}\leq \varepsilon_{\mathsf{opt}}\) and (65), \(\mathcal{S}_{2}\) can be controlled as \[\mathcal{S}_{2} =2\sqrt{\frac{L^{\prime\prime}}{N}}\Big{(}I-\gamma\underline{P}^{ \widehat{\pi},\widehat{V}}\Big{)}^{-1}\sqrt{\left|\operatorname{Var}_{ \underline{P}^{\widehat{\pi},\widehat{V}}}(\widehat{V}^{\star,\sigma})- \operatorname{Var}_{\underline{P}^{\widehat{\pi},\widehat{V}}}(\widehat{V}^{ \widehat{\pi},\sigma})\right|}\] \[\leq 4\sqrt{\frac{L^{\prime\prime}}{N}}\Big{(}I-\gamma\underline{P }^{\widehat{\pi},\widehat{V}}\Big{)}^{-1}\sqrt{\varepsilon_{\mathsf{opt}}\frac {1}{1-\gamma}^{2}}\leq 8\sqrt{\frac{\varepsilon_{\mathsf{opt}}L^{\prime\prime}}{(1- \gamma)^{4}N}}1.\] (95)
* \(\mathcal{S}_{3}\) can be controlled similar to \(\mathcal{R}_{2}\) in (76) as follows: \[\mathcal{S}_{3} =2\sqrt{\frac{L^{\prime\prime}}{N}}\left(I-\gamma\underline{P}^{ \widehat{\pi},\widehat{V}}\right)^{-1}\sqrt{\left|\operatorname{Var}_{P^{ \widehat{\pi}}}(\widehat{V}^{\star,\sigma})-\operatorname{Var}_{\underline{P}^ {\widehat{\pi},\widehat{V}}}(\widehat{V}^{\star,\sigma})\right|}\] \[\leq 4\sqrt{\frac{L^{\prime\prime}}{N}}\left(I-\gamma\underline{P}^{ \widehat{\pi},\widehat{V}}\right)^{-1}\sqrt{\frac{1}{\gamma^{2}\max\{1-\gamma,C_ {g}\sigma\}}}1\] (96) \[\leq 8\sqrt{\frac{L^{\prime\prime}}{\gamma^{2}(1-\gamma)^{2}\max\{1 -\gamma,C_{g}\sigma\}N}}1\] (97) for \(sa\)-rectangular and replacing \(\max\{1-\gamma,\sigma\}\) by \(\max\{1-\gamma,\tilde{\sigma}\min_{s}\|\hat{\pi}_{s}\|_{*}\}\) for \(s-\) rectangular case.

[MISSING_PAGE_FAIL:31]

* For \(\mathcal{S}_{5}\), it is observed that \[\mathcal{S}_{5} =2\sqrt{\frac{L^{\prime\prime}}{N}}\Big{(}I-\gamma\underline{P}^{ \widehat{\pi},V}\Big{)}^{-1}\sqrt{\text{Var}_{\underline{P}^{\widehat{\pi},V}}( \widehat{V}^{\widehat{\pi},\sigma}-V^{\widehat{\pi},\sigma})}\] \[\leq 2\sqrt{\frac{L^{\prime\prime}}{(1-\gamma)^{2}N}}\left\|V^{ \widehat{\pi},\sigma}-\widehat{V}^{\widehat{\pi},\sigma}\right\|_{\infty}1.\] (104)
* Next, observing that \(\mathcal{S}_{6}\) and \(\mathcal{S}_{7}\) are almost the same as the terms \(\mathcal{S}_{2}\) (controlled in (95)) and \(\mathcal{S}_{3}\) (controlled in (97)) in (93), it is easily verified that they can be controlled as follows \[\mathcal{S}_{6}\leq 4\sqrt{\frac{\varepsilon_{\mathsf{opt}}L^{\prime\prime}}{( 1-\gamma)^{4}N}}1,\qquad\qquad\mathcal{S}_{7}\leq 4\sqrt{\frac{L^{\prime \prime}}{\gamma^{2}(1-\gamma)^{2}\max\{1-\gamma,C_{g}\sigma\}N}}1.\] (105)

for \(sa\)-rectangular and the same quantity replacing \(\max\{1-\gamma,\sigma\}\) by \(\max\{1-\gamma,\min_{s}\left\|\widehat{\pi}_{s}\right\|_{*}\tilde{\sigma}\}\) for \(s-\) rectangular case. Then inserting the results in (103), (104), and (105) back to (102) leads to

\[\Big{(}I-\gamma\underline{P}^{\widehat{\pi},V}\Big{)}^{-1}\Big{(} \underline{\widehat{P}}^{\widehat{\pi},\widehat{V}}\widehat{V}^{\widehat{\pi}, \sigma}-\underline{P}^{\widehat{\pi},\widehat{V}}\widehat{V}^{\widehat{\pi}, \sigma}\Big{)}\] (106) \[\leq\left(\frac{2\varepsilon_{\mathsf{opt}}}{(1-\gamma)}\right)1 +8\sqrt{\probability at least \(1-\delta\), for \(sa\)-rectangular

\[\left\|V^{\star,\sigma}-V^{\widehat{\pi},\sigma}\right\|_{\infty} \leq\left\|V^{\pi^{\star},\sigma}-\widehat{V}^{\pi^{\star},\sigma} \right\|_{\infty}+\varepsilon_{\mathsf{opt}}+\left\|\widehat{V}^{\widehat{\pi}, \sigma}-V^{\widehat{\pi},\sigma}\right\|_{\infty}\] \[\leq\varepsilon_{\mathsf{opt}}+48\sqrt{\frac{L^{\prime\prime} \big{(}1+\varepsilon_{\mathsf{opt}}+\frac{C_{S}\left\|1\right\|_{\mathsf{t}}}{N (1-\gamma)}\big{)}}{\gamma^{3}(1-\gamma)^{2}\max\{1-\gamma,C_{g}\sigma\}N}+ \frac{6\varepsilon_{\mathsf{opt}}}{(1-\gamma)}+\frac{28L^{\prime\prime}C_{S} \left\|1\right\|_{\mathsf{t}}}{N(1-\gamma)^{2}}\] \[+160\sqrt{\frac{L(1+\frac{C_{S}\left\|1\right\|_{\mathsf{t}}}{N(1 -\gamma)})}{(1-\gamma)^{2}\max\{1-\gamma,C_{g}\sigma\}N}+\frac{14LC_{S}\left\| 1\right\|_{\mathsf{t}}}{N(1-\gamma)^{2}}}\] \[\leq\frac{8\varepsilon_{\mathsf{opt}}}{1-\gamma}+\frac{42L^{ \prime\prime}C_{S}\left\|1\right\|_{\mathsf{t}}}{N(1-\gamma)^{2}}+1508\sqrt{ \frac{L^{\prime\prime}(1+\frac{C_{S}\left\|1\right\|_{\mathsf{t}}}{N(1-\gamma) })}{(1-\gamma)^{2}\max\{1-\gamma,C_{g}\sigma\}N}} \tag{112}\]

where the last inequality holds by \(\gamma\geq\frac{1}{4}\) and \(N\geq\frac{16L^{\prime\prime}}{(1-\gamma)^{2}}\) for \(sa\)-rectangular and the same quantity replacing \(\max\{1-\gamma,\sigma\}\) by \(\max\{1-\gamma,\sigma\min_{s}\{\left\|\pi^{\star}_{s}\right\|_{\mathsf{t}}\}\}\) for \(s-\) rectangular case. The proof is similar for \(TV\) without the geometric term depending on \(C_{S}\).

### Proof of the auxiliary lemmas

#### d.3.1 Proof of Lemma 5

Similarly to Shi et al. [2023], denoting \(s_{0}\) the argmax of \(V^{\pi,\sigma}\) such that \(V^{\pi,\sigma}\left(s_{0}\right)=\min_{s\in\mathcal{S}}V^{\pi,\sigma}(s)\) using recursive Bellman's equation

\[\max_{s\in\mathcal{S}}V^{\pi,\sigma}(s) =\max_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi(\cdot\left|s\right)} \left[r(s,a)+\gamma\inf_{\mathcal{P}\in\mathcal{U}^{\sigma}\left(P_{s,a} \right)}\mathcal{P}V^{\pi,\sigma}\right] \tag{113}\] \[\leq\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\left(1+\gamma\inf _{\mathcal{P}\in\mathcal{U}^{\sigma}\left(P_{s,a}\right)}\mathcal{P}V^{\pi, \sigma}\right) \tag{114}\]

where the second line holds since the reward function \(r(s,a)\in[0,1]\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\).

Then we construct for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), \(\widetilde{P}_{s,a}\in\mathbb{R}^{S}\) by reducing the values of some elements of \(P_{s,a}\) such that \(P_{s,a}\geq\widetilde{P}_{s,a}\geq 0\) and \(\sum_{s^{\prime}}\left(P_{s,a}\left(s^{\prime}\right)-\widetilde{P}_{s,a} \left(s^{\prime}\right)\right)=\sigma C_{g}^{s,a}\). with \(C_{g}^{s,a}=\frac{1}{\left\|e_{a_{0}}\right\|}\) It lead to \(\widetilde{P}_{s,a}+\sigma C_{g}^{s,a}e_{s_{0}}^{\top}\in\mathcal{U}_{\left\| 1\right\|}^{\sigma}\left(P_{s,a}\right)\), where \(e_{s_{0}}\) is the standard basis vector supported on \(s_{0}\), since

\[\frac{1}{2}\left\|\widetilde{P}_{s,a}+\sigma C_{g}^{s,a}e_{s_{0}}^{\top}-P_{s, a}\right\|\leq\frac{1}{2}\left\|\widetilde{P}_{s,a}-P_{s,a}\right\|+\frac{C_{g}^{s,a}\sigma\left\|e_{s_{0}}\right\|}{2}=\sigma/2+\sigma/2=\sigma \tag{115}\]

Consequently,

\[\inf_{\mathcal{P}\in\mathcal{U}_{\left\|\cdot\right\|}^{\sigma} \left(P_{s,a}\right)}\mathcal{P}V^{\pi,\sigma} \leq\left(\widetilde{P}_{s,a}+\sigma C_{g}^{s,a}e_{s_{0}}^{\top} \right)V^{\pi,\sigma}\leq\left\|\widetilde{P}_{s,a}\right\|_{1}\left\|V^{\pi, \sigma}\right\|_{\infty}+\sigma V^{\pi,\sigma}\left(s_{0}\right)C_{g}^{s,a} \tag{116}\] \[\leq(1-C_{g}^{s,a}\sigma)\max_{s\in\mathcal{S}}V^{\pi,\sigma}(s)+ \sigma C_{g}^{s,a}\min_{s\in\mathcal{S}}V^{\pi,\sigma}(s) \tag{117}\]

where the second inequality holds by

\[\left\|\widetilde{P}_{s,a}\right\|_{1}=\sum_{s^{\prime}}\widetilde{P}_{s,a} \left(s^{\prime}\right)=-\sum_{s^{\prime}}\left(P_{s,a}\left(s^{\prime}\right)- \widetilde{P}_{s,a}\left(s^{\prime}\right)\right)+\sum_{s^{\prime}}P_{s,a} \left(s^{\prime}\right)=1-\sigma C_{g}^{s,a} \tag{118}\]

Plugging this back to the previous relation gives\[\max_{s\in\mathcal{S}}V^{\pi,\sigma}(s)\leq 1+\gamma(1-C_{g}^{s,a}\sigma)\max_{s \in\mathcal{S}}V^{\pi,\sigma}(s)+\gamma C_{g}^{s,a}\sigma\min_{s\in\mathcal{S}}V ^{\pi,\sigma}(s) \tag{119}\]

which, by rearranging terms, yields

\[\max_{s\in\mathcal{S}}V^{\pi,\sigma}(s) \leq\frac{1+\gamma C_{g}^{s,a}\sigma\min_{s\in\mathcal{S}}V^{\pi, \sigma}(s)}{1-\gamma(1-C_{g}^{s,a}\sigma)} \tag{120}\] \[\leq\frac{1}{(1-\gamma)+\gamma C_{g}^{s,a}\sigma}+\min_{s\in \mathcal{S}}V^{\pi,\sigma}(s)\leq\frac{1}{\gamma\max\{1-\gamma,C_{g}^{s,a} \sigma\}}+\min_{s\in\mathcal{S}}V^{\pi,\sigma}(s) \tag{121}\]

So rearranging terms it holds :

\[\mathrm{sp}(V^{\pi,\sigma})_{\infty}\leq\frac{1}{\gamma\max\{1-\gamma,C_{g}^{s,a}\sigma\}} \tag{122}\]

or taking the sup over \(s\):

\[\mathrm{sp}(V^{\pi,\sigma})_{\infty}\leq\frac{1}{\gamma\max\{1-\gamma,C_{g} \sigma\}} \tag{123}\]

As we pick the supreme over \(s\), the quantity, \(C_{g}^{s,a}\) is replaced by \(C_{g}=1/(\min_{s}\|e_{s}\|)\) to obtain a control for every \(s\).

#### d.3.2 Proof of Lemma 6

Similarly to 5 denoting \(s_{0}\) the argmax of \(V^{\pi,\sigma}\) such that \(V^{\pi,\sigma}\left(s_{0}\right)=\min_{s\in\mathcal{S}}V^{\pi,\sigma}(s)\) using recursive Bellman's equation

\[\max_{s\in\mathcal{S}}V^{\pi,\sigma}(s) =\max_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi\left(\cdot\right|s \right)}\left[r(s,a)+\gamma\inf_{\mathcal{P}\in\mathcal{U}^{\sigma}\left(P_{s }\right)}\mathcal{P}V^{\pi,\tilde{\sigma}}\right] \tag{124}\] \[\leq\max_{s\in\mathcal{S}}\left(1+\gamma\inf_{\mathcal{P}^{\pi} \in\mathcal{U}^{\tilde{\sigma}}\left(P_{s}^{\pi}\right)}\mathcal{P}^{\pi}V^{ \pi,\tilde{\sigma}}\right) \tag{125}\]

where the second line holds since the reward function \(r(s,a)\in[0,1]\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\).Then we construct for any \(s\in\mathcal{S}\)\(\widetilde{P}_{s}\in\mathbb{R}^{S\times A}\) by reducing the values of some elements of \(P_{s}\) such that \(P_{s}\geq\widetilde{P}_{s}\geq 0\) and

\[\forallas \(C_{g}^{s}\left\|\sigma_{s,a}e_{s_{0},a}\right\|\) is equal to \(C_{g}^{s}\tilde{\sigma}\left\|e_{s_{0}}\right\|\) Consequently,

\[\inf_{\mathcal{P}^{\pi}\in\mathcal{U}^{\pi}\left(P_{s}\right)} \mathcal{P}^{\pi}V^{\pi,\tilde{\sigma}} \leq\Pi^{\pi}\left(\widetilde{P}_{s}^{\pi}+\sigma C_{g}^{s} \varepsilon_{s_{0}}^{\top}\right)V^{\pi,\tilde{\sigma}} \tag{128}\] \[=\sum_{a}\sum_{s^{\prime}}\widetilde{P}_{s}(s^{\prime},a)\pi(a|s)V ^{\pi,\tilde{\sigma}}(s^{\prime})+\sigma e_{s_{0},a}C_{g}^{s}V^{\pi,\tilde{ \sigma}}\left(s_{0}\right)\pi(a|s)\] (129) \[\leq\sum_{a}\sup_{s^{\prime}}[V^{\pi,\tilde{\sigma}}(s^{\prime})] (\sum_{s^{\prime}}\widetilde{P}_{s}(s^{\prime},a)))\pi(a|s)+V^{\pi,\tilde{ \sigma}}\left(s_{0}\right)\pi(a|s)\sigma_{s,a}C_{g}^{s}\] (130) \[\overset{(a)}{=}\max_{s\in\mathcal{S}}V^{\pi,\sigma}(s)\sum_{a}( 1-\sigma C_{g}^{s})\pi(a|s)+\sum_{a}V^{\pi,\tilde{\sigma}}\left(s_{0}\right) \pi(a|s)\sigma_{s,a}C_{g}^{s}\] (131) \[\overset{(b)}{=}\max_{s\in\mathcal{S}}V^{\pi,\sigma}(s)(1-\tilde{ \sigma}C_{g}^{s})\left\|\pi_{s}\right\|_{*}+\left\|\pi_{s}\right\|_{*}\tilde{ \sigma}C_{g}^{s}\min_{s\in\mathcal{S}}V^{\pi,\tilde{\sigma}}(s)\] (132) \[\leq(1-C_{g}^{s}\tilde{\sigma})\max_{s\in\mathcal{S}}V^{\pi, \sigma}(s)+\sigma C_{g}^{s}\min_{s\in\mathcal{S}}V^{\pi,\tilde{\sigma}}(s) \tag{133}\]

where \(\left\|\pi\right\|_{\infty}\) is the norm of the vector \(\pi(.|s)\) and where (a) holds because

\[\sum_{s^{\prime}}\widetilde{P}_{s}\left(s^{\prime}\right)=-\sum_{s^{\prime}} \left(P_{s}\left(s^{\prime}\right)-\widetilde{P}_{s}\left(s^{\prime}\right) \right)+\sum_{s^{\prime}}P_{s}\left(s^{\prime}\right)=1-\sigma_{s,a}C_{g}^{s} \tag{134}\]

Finally (b) is due to (126) and using Holder's inequality in the second term. Plugging this back to the previous relation gives

\[\max_{s\in\mathcal{S}}V^{\pi,\tilde{\sigma}}(s)\leq 1+\gamma(1-\tilde{ \sigma}C_{g}^{s}\left\|\pi_{s}\right\|_{*})\max_{s\in\mathcal{S}}V^{\pi, \sigma}(s)+\gamma\left\|\pi_{s}\right\|_{*}\tilde{\sigma}C_{g}^{s}\min_{s\in \mathcal{S}}V^{\pi,\tilde{\sigma}}(s) \tag{135}\]

which, by rearranging terms, yields

\[\max_{s\in\mathcal{S}}V^{\pi,\tilde{\sigma}}(s) \leq\frac{1+\gamma\tilde{\sigma}\left\|\pi_{s}\right\|_{*}C_{g}^ {s}\min_{s\in\mathcal{S}}V^{\pi,\tilde{\sigma}}(s)}{1-\gamma(1-C_{g}^{s} \tilde{\sigma}\left\|\pi_{s}\right\|_{*})} \tag{136}\] \[\leq\frac{1}{(1-\gamma)+\left\|\pi_{s}\right\|_{*}\gamma C_{g}^ {s}\tilde{\sigma}}+\min_{s\in\mathcal{S}}V^{\pi,\tilde{\sigma}}(s)\] (137) \[\leq\frac{1}{(1-\gamma)+\gamma\left\|\pi_{s}\right\|_{*}C_{g}^ {s}\tilde{\sigma}}+\min_{s\in\mathcal{S}}V^{\pi,\tilde{\sigma}}(s)\] (138) \[\leq\frac{1}{\gamma\max\{1-\gamma,C_{g}^{s}\left\|\pi_{s}\right\| _{*}\tilde{\sigma}\}}+\min_{s\in\mathcal{S}}V^{\pi,\tilde{\sigma}}(s). \tag{139}\]

So rearranging and taking the sumpremum over all sterm it holds :

\[\mathrm{sp}(V^{\pi,\tilde{\sigma}})_{\infty}\leq\frac{1}{\gamma\max\{1-\gamma, \min_{s}\left\|\pi_{s}\right\|_{*}C_{g}\tilde{\sigma}\}}. \tag{140}\]

As we pick the supreme over \(s\) ovf this quantity, \(C_{g}^{s}\) is replaced by \(C_{g}=1/\min_{s}\left\|e_{s}\right\|\).

#### d.3.3 Proof of Lemma 8

Proof.: Concentration of the robust values function. with probability \(1-\delta\), it holds:

\[\left|P_{s,a}^{\pi,V}V-\widehat{P}_{s,a}^{\pi,V}V\right|\leq 2\sqrt{\frac{L}{N}} \sqrt{\mathrm{Var}_{P_{s,a}^{0}}(V)}+\frac{3LC_{S}\left\|1\right\|_{*}}{N(1- \gamma)}\]with \(L=2\log(18\left\|1\right\|_{*}SAN/\delta)\) and First we can use optimization duality such as in (50):

\[\left|P_{s,a}^{\pi,V}V-\widehat{P}_{s,a}^{\pi,V}V\right| \tag{141}\] \[=\Big{|}\max_{\mu_{P_{a}^{\lambda,\omega}}^{\lambda,\omega}\in \mathcal{M}_{P_{s,a}^{\rho}}^{0}}\left\{P_{s,a}^{0}(V-\mu)-\sigma\left(\text{ sp}((V-\mu))_{*}\right)\right\}\] \[-\max_{\mu_{P_{s,a}^{\lambda,\omega}}^{\lambda,\omega}\in\mathcal{M }_{P_{s,a}^{\rho}}^{\lambda,\omega}}\left\{\widehat{P}_{s,a}^{0}(V-\mu_{P_{s,a} ^{\rho}}^{\lambda,\omega})-\sigma\left(\text{sp}((V-\mu_{P_{s,a}^{\rho}}^{ \lambda,\omega}))_{*}\right)\right\}\Big{|}\] \[\leq\max\left\{\Big{|}\max_{\mu_{P_{a}^{\lambda,\omega}}^{\lambda,\omega}\in\mathcal{M}_{P_{s,a}^{\rho}}^{\lambda,\omega}}\left\{P_{s,a}^{0}(V -\mu_{P_{s,a}^{\rho}}^{\lambda,\omega})-\sigma\left(\text{sp}((V-\mu_{P_{s,a} ^{\rho}}^{\lambda,\omega}))_{*}\right)\right\}\right.\] \[-\max_{\mu_{P_{s,a}^{\lambda,\omega}}^{\lambda,\omega}\in\mathcal{ M}_{P_{s,a}^{\rho}}^{\lambda,\omega}}\left\{\widehat{P}_{s,a}^{0}(V-\mu_{P_{s,a} ^{\rho}}^{\lambda,\omega})-\sigma\left(\text{sp}((V-\mu_{P_{s,a}^{\rho}}^{ \lambda,\omega}))_{*}\right)\right\}\Big{|};\] (142) \[\Big{|}\max_{\mu_{P_{s,a}^{\lambda,\omega}}^{\lambda,\omega}\in \mathcal{M}_{P_{s,a}^{\rho}}^{\lambda,\omega}}\left\{\widehat{P}_{s,a}^{0}(V -\mu_{P_{s,a}^{\rho}}^{\lambda,\omega})-\sigma\left(\text{sp}((V-\mu_{P_{s,a} ^{\rho}}^{\lambda,\omega}))_{*}\right)\right\}\] (143) \[-\max_{\mu_{P_{s,a}^{\lambda,\omega}}^{\lambda,\omega}\in \mathcal{M}_{P_{s,a}^{\rho}}^{\lambda,\omega}}\left\{P_{s,a}^{0}(V-\mu_{P_{s,a}^{\rho}}^{\lambda,\omega})-\sigma\left(\text{sp}((V-\mu_{P_{s,a}^{\rho}}^{ \lambda,\omega}))_{*}\right)\right\}\qquad\Big{|}\Big{\}}\] \[\leq\max\left\{\underbrace{\left|\max_{\mu\in\mu_{P_{s,a}^{\lambda,\omega}}^{\lambda,\omega}}\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)(V- \mu_{P_{s,a}^{\rho}}^{\lambda,\omega})\right|}_{=:g_{s,a}(\alpha_{P}^{\lambda,\omega},V)},\underbrace{\left|\max_{\mu_{P_{s,a}^{\lambda,\omega}}^{\lambda, \omega}}\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)(V-\mu_{P_{s,a}^{0}}^{ \lambda,\omega})\right|}_{=:g_{s,a}(\alpha_{P}^{\lambda,\omega},V)}\right\} \tag{144}\]

where in the first equality we use Lemma 3. The final inequality is a consequence of the \(1\)-Lipschitzness of the max operator. First, we control \(g_{s,a}(\alpha_{P}^{\lambda,\omega},V)\). To do so, we use for a fixed \(\alpha_{P}^{\lambda,\omega}\) and any vector \(V\) that is independent with \(\widehat{P}^{0}\), the Bernstein's inequality, one has with probability at least \(1-\delta\) with \(sa\)-rectangular notations,

\[g_{s,a}(\alpha_{P}^{\lambda,\omega},V)=\left|\left(P_{s,a}^{0}-\widehat{P}_{ s,a}^{0}\right)\left[V\right]_{\alpha_{P}^{\lambda,\omega}}\right|\leq \sqrt{\frac{2\log(\frac{2}{\delta})}{N}}\sqrt{\text{Var}_{P_{s,a}^{0}}(V)}+ \frac{2\log(\frac{2}{\delta})}{3N(1-\gamma)}. \tag{145}\]

Once pointwise concentration derived, we will use uniform concentration to yield this lemma. First, union bound, is obtained noticing that \(g_{s,a}(\alpha_{P}^{\lambda,\omega},V)\) is \(1\)-Lipschitz w.r.t. \(\lambda\) and \(\omega\) as it is linear in \(\lambda\) and \(\omega\). Moreover, \(\lambda^{*}=\left\|V-\mu^{*}-\omega\right\|_{*}\) obeying \(\lambda^{*}\leq\frac{\left\|1\right\|_{*}}{1-\gamma}\). The quantity \(\omega\in[0,1/(1-\gamma)]\) as it is always smaller that \(V\) by definition. We construct then a \(2\)-dimensional a \(\varepsilon_{1}\)-net \(N_{\varepsilon_{1}}\) over \(\lambda^{*}\in[0,\frac{\left\|1\right\|_{*}}{1-\gamma}]\) and \(\omega\in[0,1/(1-\gamma)]\) whose size satisfies \(\left|N_{\varepsilon_{1}}\right|\leq\left(\frac{\left\|1\right\|_{*}}{ \varepsilon_{1}(1-\gamma)}\right)^{2}\)[Vershynin, 2018]. Using union bound and (145), it holds with probability at least \(1-\frac{\delta}{SA}\) that for all \(\lambda\in N_{\varepsilon_{1}}\),

\[g_{s,a}(\alpha_{P}^{\lambda},V)\leq\sqrt{\frac{2\log(\frac{2SA|N_{\varepsilon_{1 }}|}{\delta})}{N}}\sqrt{\text{Var}_{P_{s,a}^{0}}(V)}+\frac{2\log(\frac{2SA|N_{ \varepsilon_{1}}|}{\delta})}{3N(1-\gamma)}. \tag{146}\]Using the previous equation and also (144), it results in using notation \(2\log(\frac{18SAN\|1\|_{*}}{\delta})=L\),

\[g_{s,a}(\alpha_{P}^{\lambda},V) \stackrel{{\rm(a)}}{{\leq}}\sup_{\alpha_{P}^{\lambda} \in N_{e_{1}}}\left|\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)[V]_{\alpha_ {P}^{\lambda}}\right|+\varepsilon_{1}\] \[\stackrel{{\rm(b)}}{{\leq}}\sqrt{\frac{2\log(\frac{ SA|N_{e_{1}}|}{\delta})}{N}}\sqrt{\mathrm{Var}_{P_{s,a}^{0}}(V)}+\frac{2\log(\frac{ 2SA|N_{e_{1}}|}{\delta})}{3N(1-\gamma)}+\varepsilon_{1} \tag{147}\] \[\stackrel{{\rm(c)}}{{\leq}}\sqrt{\frac{2\log(\frac{2 SA|N_{e_{1}}|}{\delta})}{N}}\sqrt{\mathrm{Var}_{P_{s,a}^{0}}(V)}+\frac{\log( \frac{2SA|N_{e_{1}}|}{\delta})}{N(1-\gamma)}\] \[\stackrel{{\rm(d)}}{{\leq}}\sqrt{\frac{L}{N}}\sqrt{ \mathrm{Var}_{P_{s,a}^{0}}(V)}+\frac{L}{N(1-\gamma)}\] (148) \[\leq\sqrt{\frac{L}{N}}\|V\|_{\infty}+\frac{L}{N(1-\gamma)}\] \[\leq 2\sqrt{\frac{L}{(1-\gamma)^{2}N}} \tag{149}\]

where (a) is because the optimal \(\alpha\) falls into the \(\varepsilon_{1}\)-ball centered around some point inside \(N_{e_{1}}\) and \(g_{s,a}(\alpha_{P}^{\lambda},V)\) is 1-Lipschitz with regard to \(\lambda\) and \(\omega\), (b) is due to Eq. (146), (c) arises from taking \(\varepsilon_{1}=\frac{\log(\frac{2SA|N_{e_{1}}|}{3N(1-\gamma)})}{}\), (d) is verified by \(\left|N_{e_{1}}\right|\leq\left(\frac{3\|1\|_{*}}{\varepsilon_{1}(1-\gamma)} \right)^{2}\leq 9N\left\|1\right\|\) and that variance of a ceiling function of a vector is smaller than the variance of non-ceiling vector, and the last inequality comes from the fact \(\|V^{*,\sigma}\|_{\infty}\leq\frac{1}{1-\gamma}\) and taking \(N\geq 2\log(\frac{18SAN\|1\|_{*}}{\delta})=L\).

Contrary to the previous term, the second term \(g_{s,a}(\alpha_{P}^{\lambda},V)\) is more difficult as we need concentration. Still, the data has an extra dependency through the parameter \(\alpha_{P}^{\lambda}\). We need to decouple this problem using absorbing MDPs. Then it leads to

\[g_{s,a}(\alpha_{P}^{\lambda,\omega},V) \tag{150}\] \[=|\max_{\mu_{P_{s,a}^{\lambda,\omega}}\in\mathcal{M}_{P_{s,a}^{ \rho}}^{\lambda,\omega}}\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)(V-\mu_ {P_{s,a}^{0}}^{\lambda,\omega})|\] (151) \[=|\max_{\mu\in\mathcal{M}_{P_{s,a}^{0}}^{\lambda,\omega}}\left(P_ {s,a}^{0}-\widehat{P}_{s,a}^{0}\right)(V-\mu_{P_{s,a}^{0}}^{\lambda,\omega})+ \left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)(\mu_{P_{s,a}^{0}}^{\lambda, \omega}-\mu_{\widehat{P}_{s,a}^{0}}^{\lambda,\omega})|\] (152) \[\leq|\max_{\mu_{P_{s,a}^{0}}^{\lambda,\omega}\in\mathcal{M}_{P_{ s,a}^{0}}^{\lambda,\omega}}\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)(V-\mu_ {P_{s,a}^{0}}^{\lambda,\omega})+\max_{\mu_{P_{s,a}^{0}}^{\lambda,\omega}\in \mathcal{M}_{P_{s,a}^{0}}^{\lambda,\omega}}\left(P_{s,a}^{0}-\widehat{P}_{s,a }^{0}\right)(\mu_{P_{s,a}^{0}}^{\lambda,\omega}-\mu_{\widehat{P}_{s,a}^{0}}^{ \lambda,\omega})|. \tag{153}\]

In the first equality, we add the term \(\mu_{P_{s,a}^{0}}^{\lambda,\omega}\) to retrieve the previous concentration problem, fixing \(P_{s,a}^{0}\) and optimizing \(\lambda,\omega\). In the second, we extend the max using triangular inequality. The first term in the last equality is exactly the term we have controlled previously, while the second one needs more attention. We decouple the data's dependency, then control the difference between the \(\mu\). Then using the characterization of the optimal \(\mu\) from equation (47):

\[\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)(\mu_{P_{s,a}^{0}}^{\lambda, \omega}-\mu_{P_{s,a}^{0}}^{\lambda,\omega})=\sum_{s^{\prime}}\lambda\left(P_{s,a}^{0}(s^{\prime})-\widehat{P}_{s,a}^{0}(s^{\prime})\right)(\nabla\big{\|}P_{ s,a}^{0}\big{\|}-\nabla\big{\|}\hat{P}_{s,a}^{0}\big{\|})\]

Here we assume that the subgradient is a gradient as we assume that the norm is \(C^{2}\). The question that arises is whether the gradient of the norm is Lipschitz.

Note that we are considering the worst case as \((\mu_{P_{s,a}^{0}}^{\lambda,\omega}-\mu_{\widehat{P}_{s,a}^{0}}^{\lambda,\omega})\) can be zero in the case where \(\mu\) the Lagrangian variable is equal to zero. Finally, note that we can also control this term when one of the two terms \(\mu_{P_{\rho,a}}^{\lambda,\omega}\) or \(\mu_{P_{\rho,a}}^{\lambda,\omega}\) is equal to zero as \(\mu_{P_{\rho,a}}^{\lambda,\omega}\) and \(\mu_{P_{\rho,a}}^{\lambda,\omega}\) smaller that \(V\) because \(V-\mu\) need to be positive in equation (43). In this case, classical control using Bernstein's inequality without uniform concentration can be applied, giving the same result. In the worst case where all terms in \((\mu_{P_{\rho,a}}^{\lambda,\omega}-\mu_{P_{\rho,a}}^{\lambda,\omega})\) are non zero, assuming that the norm is \(C^{2}\), using mean value theorem, we know that

\[\left\|\left(\nabla\middle\|P_{s,a}^{0}\right\|-\nabla\middle\|\hat{P}_{s,a}^{ 0}\right\|\right)\leq\sup_{x\in\Delta\left(S\right)}\left\|\nabla^{2}\left\|x \right\|\right\|_{2}\left\|\left(P_{s,a}^{0}-\hat{P}_{s,a}^{0}\right)\right\|_ {2}.\]

As the norm is \(C^{2}\), is continuous and as the simplex is bounded, this quantity exists according to the Extreme value theorem. It is possible to compute this contact depending on \(S\) for explicit norms such as \(L_{p}\). Indeed, for \(L_{2}\):

\[\nabla^{2}\left\|x\right\|_{2}=\frac{\left(I-\frac{x\bigotimes x}{\left\|x \right\|_{2}^{2}}\right)}{\left\|x\right\|_{2}}\leq\frac{1}{\left\|x\right\|_ {2}}I\leq\frac{1}{\min_{x\in\Delta\left(S\right)}\left\|x\right\|_{2}}I=\sqrt{S}\]

where \(\bigotimes\) is the Kronecker product. So we have an upper bound independent of \(x\). For \(L_{p}=\left\|x\right\|_{p}\) norms, \(p\geq 2\), we have simple taking derivative twice:

\[\nabla^{2}\left\|x\right\|_{p}=\frac{p-1}{L_{p}}\left(\mathcal{A}^{p-2}-g_{p}g _{p}^{T}\right)\]

with

\[\mathcal{A} =\text{Diag}\left(\frac{\text{abs}(x)}{L_{p}}\right)\] \[g_{p} =\mathcal{A}^{p-2}\left(\frac{x}{L_{p}}\right).\]

where Diag is the diagonal matrix. However, as \(x\leq L_{p}\), \(\mathcal{A}\leq I\), we get

\[H\leq\frac{p-1}{\left\|x\right\|_{p}}\leq(p-1)S^{1/q}=C_{S} \tag{154}\]

where the \(1/L_{p}\) is minimized for the uniform distribution. Then using Cauchy Swartz inequality, it holds

\[\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)(\mu_{P_{s,a}^{0}}^{\lambda, \omega}-\mu_{P_{\rho,a}^{0}}^{\lambda,\omega})\leq\lambda\left\|\left(P_{s,a} ^{0}-\widehat{P}_{s,a}^{0}\right)\right\|_{2}^{2}. \tag{155}\]

Then the question is how to bound the quantity \(\left\|\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)\right\|_{2}^{2}\). To do so, we will use McDiarmid inequality.

**Definition 3**.: _Bounded difference property_

_A function \(f:\mathcal{X}_{1}\times\ldots\mathcal{X}_{n}\rightarrow\mathbb{R}\) satisfies the bounded difference property if for each \(i=1,\ldots,n\) the change of coordinate from \(s_{i}\) to \(s_{i}^{\prime}\) may change the value of the function at most on \(c_{i}\)_

\[\forall i\in[n]:\sup_{x_{i}^{\prime}\in\mathcal{X}_{i}}\left|f\left(x_{1}, \ldots,x_{i},\ldots,x_{n}\right)-f\left(x_{1},\ldots,x_{i}^{\prime},\ldots,x _{n}\right)\right|\leq c_{i}\]

In our case, we consider \(f\left(X_{1},\ldots,X_{n}\right)=\left\|\sum_{k=1}^{n}X_{k}\right\|_{2}\). Then we can notice that by triangle inequality for any \(x_{1},\ldots,x_{n}\) and \(x_{k}^{\prime}\) with \(X_{i,s,\prime}=P_{i,s,a}^{0}(s^{\prime})-P_{s,a}^{0}(s^{\prime})\) ( index \(i\) holds for index of sample generated from the generative model) that

\[f\left(x_{1},\ldots,x_{k},\ldots,x_{n}\right)=\left\|x_{1}+\ldots +x_{n}\right\|_{2}\leq\left\|x_{1}+\ldots+x_{n}-x_{k}+x_{k}^{\prime}\right\|_{ 2}+\left\|x_{k}-x_{k}^{\prime}\right\|_{2}\] \[\leq f\left(x_{1},\ldots,x_{k}^{\prime},\ldots,x_{n}\right)+2\]

[MISSING_PAGE_FAIL:39]

Finally, we obtain:

\[\max_{\lambda}\Big{\|}\Big{(}P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\Big{)} \Big{\|}_{2}^{2}C_{S}\lambda\leq\frac{L^{\prime}C_{S}\left\|11\right\|_{*}}{N(1 -\gamma)}.\]

Regrouping all terms:

\[g_{s,a}(\alpha_{\hat{P}}^{\lambda},V) \leq\big{|}\max_{\mu_{P_{s,a}^{0}}^{\lambda}\in\mathcal{M}_{P_{s,a }^{0}}^{\lambda}}\Big{(}P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\Big{)}\left(V-\mu_{P_ {s,a}^{0}}^{\lambda}\right)+\max_{\mu_{P_{s,a}^{0}}^{\lambda}\in\mathcal{M}_{P _{s,a}^{0}}^{\lambda}}\Big{(}P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\Big{)}\left(\mu_{ P_{s,a}^{0}}^{\lambda}-\mu_{\widehat{P}_{s,a}^{0}}^{\lambda}\right)\] \[\leq 2\sqrt{\frac{L}{N}}\sqrt{\text{Var}_{P_{s,a}^{0}}(V)}+\frac{ L^{\prime}C_{S}\left\|11\right\|_{*}}{N(1-\gamma)}+\frac{L}{N(1-\gamma)}\] \[\leq 2\sqrt{\frac{L}{N}}\sqrt{\text{Var}_{P_{s,a}^{0}}(V)}+\frac{ 3LC_{S}\left\|11\right\|_{*}}{N(1-\gamma)} \tag{159}\]

We can recognize that the second term is a second-order term as long as \(N\geq(C_{S}\left\|11\right\|_{*})^{2}\), we can regroup the two terms. Finally, as \(g_{s,a}(\alpha_{\hat{P}}^{\lambda},V)\geq g_{s,a}(\alpha_{\hat{P}}^{\lambda},V)\), we obtain

\[\Big{|}P_{s,a}^{\pi,V}V-\widehat{P}_{s,a}^{\pi,V}V\Big{|}\leq 2\sqrt{\frac{L}{N}} \sqrt{\text{Var}_{P_{s,a}^{0}}(V)}+\frac{3LC_{S}\left\|11\right\|_{*}}{N(1- \gamma)} \tag{160}\]

It is important to note that the geometry of the norm is present in the second order term \(\frac{3LC_{S}\left\|11\right\|}{N(1-\gamma)}\) but this term is negligible as it is proportional to \(1/N\) with regard to the variance term in \(1/\sqrt{N}\). Moreover, note that the quantity \(C_{S}\left\|11\right\|_{*}=S\) for \(L_{2}\) norms.

For the specific case of \(TV\) which is not \(C^{2}\) smooth, this lemma still holds as in (144), we only need to control one term without the dependency on data in the supremum as \(\alpha_{P}^{\lambda}\) reduces to a scalar \(\alpha\) which does not depend on \(P\). Then extra decomposition using smoothness of the norm is not needed, as the only remaining term in the max in (144) is the left-hand side term.

For the \(s\)-rectangular case, the first equation can be rewritten simply by factorizing by \(\pi(a|s)\) using lemma 4.

\[\Big{|}P_{s,a}^{\pi,V}V-\widehat{P}_{s,a}^{\pi,V}V\Big{|} =\Big{|}\sum_{a}\pi(a|s)\max_{\mu_{P_{s,a}^{0}}^{\lambda}\in \mathcal{M}_{P_{s,a}^{0}}^{\lambda}}\left\{P_{s,a}^{0}(V-\mu)-\sigma\left( \text{sp}((V-\mu))_{*}\right)\right\}\] \[-\max_{\mu_{P_{s,a}^{0}}^{\lambda}\in\mathcal{M}_{P_{s,a}^{0}}^{ \lambda}}\left\{\widehat{P}_{s,a}^{0}(V-\mu_{P_{s,a}^{0}}^{\lambda})-\sigma \left(\text{sp}((V-\mu_{P_{s,a}^{0}}^{\lambda})_{*}\right)\right\}\Big{|} \tag{162}\] \[\leq\sum_{a}\pi(a|s)\Big{(}2\sqrt{\frac{L}{N}}\sqrt{\text{Var}_{ P_{s,a}^{0}}(V)}+\frac{LC_{S}\left\|11\right\|_{*}}{N(1-\gamma)}\Big{)}\] (163) \[=2\sqrt{\frac{L}{N}}\sqrt{\text{Var}_{P_{s,a}^{0}}(V)}+\frac{3LC _{S}\left\|11\right\|_{*}}{N(1-\gamma)} \tag{164}\]

using \(sa\)-rectangular results, which gives the result for \(s\)-rectangular case.

Combining this lemma with a matrix notation using union bound, one has with probability \(1-\delta\):

\[\Big{|}\hat{\underline{P}}^{\pi^{*},V}V^{\pi^{*},\sigma}-\underline{P}^{\pi^{* },V}V^{\pi^{*},\sigma}\Big{|}\leq 2\sqrt{\frac{L}{N}}\sqrt{\text{Var}_{P^{*}} \left(V^{*,\sigma}\right)}+\frac{3LC_{S}\left\|11\right\|_{*}}{N(1-\gamma)}1\] (165) (166)

#### d.3.4 Proof of Lemma 9

Using the same argument as in (216), it holds that for any \(\alpha^{*}\) solution of (53)

\[\Big{(}I-\gamma\underline{\widehat{\underline{B}}}^{\pi^{*},V}\Big{)}^{-1}\sqrt{ \operatorname{Var}_{\underline{B}^{\pi^{*},V}}(V^{*,\sigma})}=\sqrt{\frac{1}{1 -\gamma}}\sqrt{\sum_{t=0}^{\infty}\gamma^{t}\left(\underline{\widehat{\underline {B}}}^{\pi^{*},V}\right)^{t}\operatorname{Var}_{\underline{\widehat{\underline{ B}}}^{\pi^{*},V}}(V^{*,\sigma})}. \tag{167}\]

Then we can control \(\operatorname{Var}_{\underline{\widehat{\underline{B}}}^{\pi^{*},V}}(V^{*, \sigma})\). Defining \(V^{\prime}\coloneqq V^{*,\sigma}-\eta 1,\eta\in\mathbb{R}\), we use Bellman's equation in (32)) which lead to

\[V^{\prime} =V^{*,\sigma}-\eta 1\leq V^{*,\sigma}-\eta 1=r_{\pi^{*}}+\gamma \underline{P}^{\pi^{*},V}V^{*,\sigma}-\eta 1\] (168) \[=r_{\pi^{*}}+\gamma P^{\pi^{*},V}V^{*,\sigma}-\gamma\text{osp}(V ^{*,\sigma})_{*}-\eta 1\] (169) \[=r^{\prime}_{\pi^{*}}+\gamma\underline{\widehat{\underline{P}}}

Finally, the inequality is due to Lemma 8. Plugging (176) into (167) gives,

\[\left(I-\gamma\underline{\widehat{\underline{P}}}^{\pi^{\star},V} \right)^{-1}\sqrt{\operatorname{Var}_{\underline{P}^{\pi^{\star},V}}(V^{\star, \sigma})} \tag{177}\] \[\leq\sqrt{\frac{1}{1-\gamma}}\Big{(}\sum_{t=0}^{\infty}\gamma^{t} \left(\underline{\widehat{\underline{P}}}^{\pi^{\star},V}\right)^{t}\left( \underline{\widehat{\underline{P}}}^{\pi^{\star},V}\left(V^{\prime}\circ V^{ \prime}\right)-\frac{1}{\gamma}V^{\prime}\circ V^{\prime}+\frac{2}{\gamma^{2}} \|V^{\prime}\|_{\infty}1\right.\] (178) \[+\frac{2}{\gamma}\|V^{\prime}\|_{\infty}\Big{(}2\sqrt{\frac{L}{(1 -\gamma)^{2}N}}+\frac{3C_{S}\left\|1\right\|_{*}L}{N(1-\gamma)}\Big{)}1\Big{)} \Big{)}^{1/2}\] \[\overset{\rm(i)}{\leq}\sqrt{\frac{1}{1-\gamma}}\sqrt{\bigg{|} \sum_{t=0}^{\infty}\gamma^{t}\left(\underline{\widehat{\underline{P}}}^{\pi^{ \star},V}\right)^{t}\left(\underline{\widehat{\underline{P}}}^{\pi^{\star},V} \left(V^{\prime}\circ V^{\prime}\right)-\frac{1}{\gamma}V^{\prime}\circ V^{ \prime}\right)\bigg{|}}\] \[\qquad+\sqrt{\frac{1}{1-\gamma}}\sqrt{\sum_{t=0}^{\infty}\gamma^{ t}\left(\underline{\widehat{\underline{P}}}^{\pi^{\star},V}\right)^{t}\left( \frac{2}{\gamma^{2}}\|V^{\prime}\|_{\infty}1+\frac{2}{\gamma}\|V^{\prime}\|_{ \infty}\Big{(}2\sqrt{\frac{L}{(1-\gamma)^{2}N}}+\frac{3C_{S}\left\|1\right\|_ {*}L}{N(1-\gamma)}\Big{)}1\right)}\] \[\leq\sqrt{\frac{1}{1-\gamma}}\sqrt{\bigg{|}\sum_{t=0}^{\infty} \gamma^{t}\left(\underline{\widehat{\underline{P}}}^{\pi^{\star},V}\right)^{t} \left[\underline{\widehat{\underline{P}}}^{\pi^{\star},V}\left(V^{\prime} \circ V^{\prime}\right)-\frac{1}{\gamma}V^{\prime}\circ V^{\prime}\right] \bigg{|}}\] (179) \[+\sqrt{\frac{\left(2+2\Big{(}2\sqrt{\frac{L}{(1-\gamma)^{2}N}}+ \frac{3C_{S}\|1\|_{*}L}{N(1-\gamma)}\Big{)}\right)\|V^{\prime}\|_{\infty}}{(1 -\gamma)^{2}\gamma^{2}}}1, \tag{180}\]

using in (i) the triangle inequality. The final part of the proof focuses on the first term, which follows

\[\bigg{|}\sum_{t=0}^{\infty}\gamma^{t}\Big{(}\underline{\widehat{ \underline{P}}}^{\pi^{\star},V}\Big{)}^{t}\Big{(}\underline{\widehat{\underline {P}}}^{\pi^{\star},V}\left(V^{\prime}\circ V^{\prime}\right)-\frac{1}{\gamma}V ^{\prime}\circ V^{\prime}\Big{)}\bigg{|}\] \[=\bigg{|}\bigg{(}\sum_{t=0}^{\infty}\gamma^{t}\Big{(}\underline{ \widehat{\underline{P}}}^{\pi^{\star},V}\Big{)}^{t+1}-\sum_{t=0}^{\infty} \gamma^{t-1}\Big{(}\underline{\widehat{\underline{P}}}^{\pi^{\star},V}\Big{)} ^{t}\bigg{)}\left(V^{\prime}\circ V^{\prime}\right)\bigg{|}\leq\frac{1}{ \gamma}\|V^{\prime}\|_{\infty}^{2}1 \tag{181}\]

using recursion between the two sums. Then, using (181) back to (180) leads to

\[\left(I-\gamma\underline{\widehat{\underline{P}}}^{\pi^{\star},V} \right)^{-1}\sqrt{\operatorname{Var}_{\underline{P}^{\pi^{\star},V}}(V^{\star, \sigma})}\] \[\leq\sqrt{\frac{\|V\|_{\infty}^{2}}{\gamma(1-\gamma)}}1+3\sqrt{ \frac{\Big{(}1+\Big{(}\sqrt{\frac{L}{(1-\gamma)^{2}N}}+\frac{C_{S}\|1\|_{*}L} {N(1-\gamma)}\Big{)}\Big{)}\|V^{\prime}\|_{\infty}}{(1-\gamma)^{2}\gamma^{2}}}1\] \[\leq 4\sqrt{\frac{\Big{(}1+\Big{(}\sqrt{\frac{L}{(1-\gamma)^{2}N} +\frac{C_{S}\|1\|_{*}L}{N(1-\gamma)}\Big{)}\Big{)}\|V^{\prime}\|_{\infty}}}{(1 -\gamma)^{2}\gamma^{2}}}1 \tag{182}\] \[\leq 4\sqrt{\frac{\Big{(}1+\Big{(}1\sqrt{\frac{L}{(1-\gamma)^{2}N} +\frac{C_{S}\|1\|_{*}L}{N(1-\gamma)}\Big{)}\Big{)}\|V^{\prime}\|_{*}}}{(1- \gamma)^{2}\gamma^{2}}}1 \tag{183}\]

[MISSING_PAGE_EMPTY:43]

where the last inequality follow the decomposition of (150). Finally, to control the remaining term

\[\max_{\mu^{\lambda}_{P^{0}_{s,a}}\in\mathcal{M}^{\lambda}_{P^{0}_{s,a}}}\left(P^{0} _{s,a}-\widehat{P}^{0}_{s,a}\right)(\widehat{V}^{\star,\sigma}-\mu^{\lambda}_{P ^{0}_{s,a}})=\max_{\alpha^{\lambda}_{P}\in\Lambda^{\lambda}_{P}}\left\{\left(P^{ 0}_{s,a}-\widehat{P}^{0}_{s,a}\right)[V]_{\alpha^{\lambda}_{P}}\right\} \tag{192}\]

(191) for any given \(\alpha\in[0,\alpha^{\lambda\lambda\omega\star}_{P_{\text{rob}}}[\subset\left[0, \frac{1}{1-\gamma}\right]^{S}\) in the variational family with one parameter \(\lambda\), with the dependency between \(\widehat{V}^{\star,\sigma}\) and \(\widehat{P}^{0}\), we resort to the following leave-one-out argument or absorbing MDPs used in (Agarwal et al., 2020; Li et al., 2022; Shi and Chi, 2022; Clavier et al., 2023). To begin, we create a collection of auxiliary RMDPs that exhibit the intended statistical independence between robust value functions and the estimated nominal transition kernel. These auxiliary RMDPs are designed to be minimally distinct from the initial RMDPs, subsequently, we manage to control the relevant term within these auxiliary RMDPs and demonstrate that its value closely approximates the target quantity for the desired RMDP. Recall that the empirical infinite-horizon robust MDP \(\widehat{\mathcal{M}}_{\text{rob}}\) is defined using the nominal transition kernel \(\widehat{P}^{0}\). Inspired by Agarwal et al. (2020), we can construct an auxiliary absorbing robust MDP \(\widehat{\mathcal{M}}^{s,u}_{\text{rob}}\) for each state \(s\) and any non-negative scalar \(u\geq 0\), so that it is the same as \(\widehat{\mathcal{M}}_{\text{rob}}\) except for the transition properties in state \(s\). These auxiliary MDPs are called absorbing MDPs are have been used for the first time in the context of RMDPS in Clavier et al. (2023). Defining the reward function and nominal transition kernel of \(\widehat{\mathcal{M}}^{s,u}_{\text{rob}}\) as \(P^{s,u}\) and \(r^{s,u}\), which are expressed as follows using the same notation as Shi et al. (2023):

\[\begin{cases}r^{s,u}(s,a)=u&\forall a\in\mathcal{A},\\ r^{s,u}(\widetilde{s},a)=r(\widetilde{s},a)&\forall(\widetilde{s},a)\in \mathcal{S}\times\mathcal{A}\text{ and }\widetilde{s}\neq s.\end{cases} \tag{193}\]

\[\begin{cases}P^{s,u}(s^{\prime}\,|\,s,a)=\mathds{1}(s^{\prime}=s)&\forall(s^{ \prime},a)\in\mathcal{S}\times\mathcal{A},\\ P^{s,u}(\cdot\,|\,\widetilde{s},a)=\widehat{P}^{0}(\cdot\,|\,\widetilde{s},a)& \forall(\widetilde{s},a)\in\mathcal{S}\times\mathcal{A}\text{ and } \widetilde{s}\neq s,\end{cases} \tag{194}\]

Nominal transition probability at state \(s\) of the auxiliary \(\widehat{\mathcal{M}}^{s,u}_{\text{rob}}\) never leaves state \(s\) once entered, which gives the name absorbing to these auxiliary RMPDs. Finally, we define the robust Bellman operator \(\widehat{\mathcal{T}}^{\sigma}_{s,u}(\cdot)\) associated \(\widehat{\mathcal{M}}^{s,u}_{\text{rob}}\) as

\[\widehat{\mathcal{T}}^{\sigma}_{s,u}(Q)(\tilde{s},a)=r^{s,u}(\tilde{s},a)+ \gamma\inf_{\mathcal{P}\in\mathcal{U}^{\star,\sigma}(P^{s,u}_{\tilde{s},a})} \mathcal{P}V,\qquad\text{with }V(\tilde{s})=\max_{a}Q(\tilde{s},a). \tag{195}\]

in \(sa\)-rectangular case and with stochastic policy in \(s\)-rectangular case. Using these auxiliary RMDPs we can remark equivalence between \(\widehat{\mathcal{M}}_{\text{rob}}\) and the auxiliary RMDP \(\widehat{\mathcal{M}}^{s,u}_{\text{rob}}\) fixed-point. First, \(\widehat{Q}^{\star,\sigma}\) is the unique-fixed point of \(\widehat{\mathcal{T}}^{\sigma}(\cdot)\) with associated value \(\widehat{V}^{\star,\sigma}\). We will show that the robust value function \(\widehat{V}^{\star,\sigma}_{s,u^{\star}}\) obtained from the fixed point of \(\widehat{\mathcal{T}}^{\sigma}_{s,u}(\cdot)\)is the same as the the robust value function \(\widehat{V}^{\star,\sigma}\) derived from \(\widehat{\mathcal{T}}^{\sigma}(\cdot)\), as long as we choose \(u\) as

\[u^{\star}\coloneqq u^{\star}(s)=\widehat{V}^{\star,\sigma}(s)-\gamma\inf_{ \mathcal{P}\in\mathcal{U}^{\star,\sigma}(e_{s})}\mathcal{P}\widehat{V}^{\star, \sigma}. \tag{196}\]

with \(e_{s}\) is the \(s\)-th standard basis vector in \(\mathbb{R}^{S}\). This assertion is verified as:

* **First for state \(s^{\prime}\neq s\), for all \(a\in\mathcal{A}\)**: it holds \[r^{s,u^{\star}}(s^{\prime},a)+\gamma\inf_{\mathcal{P}\in \mathcal{U}^{\star,\sigma}(P^{s,u^{\star}}_{\varphi^{\prime},a})}\mathcal{P} \widehat{V}^{\star,\sigma} =r(s^{\prime},a)+\gamma\inf_{\mathcal{P}\in\mathcal{U}^{\star, \sigma}(\widetilde{P}^{0}_{\varphi^{\prime},a})}\mathcal{P}\widehat{V}^{\star,\sigma}\] \[=\widehat{\mathcal{T}}^{\sigma}(\widehat{Q}^{\star,\sigma})(s^{ \prime},a)=\widehat{Q}^{\star,\sigma}(s^{\prime},a),\] (197) where the first equality holds because of (193) and (194), and the last inequality comes from that \(\widehat{Q}^{\star,\sigma}\) is the fixed point of \(\widehat{\mathcal{T}}^{\sigma}(\cdot)\) (see Lemma C.3) and the definition of the robust Bellman operator in (13).
* **Then for state \(s\), for any \(a\in\mathcal{A}:\)** \[r^{s,u^{\star}}(s,a)+\gamma\inf_{\mathcal{P}\in\mathcal{U}^{\star,\sigma}(P^{s,u^{\star}}_{\varphi,a})}\mathcal{P}\widehat{V}^{\star,\sigma}=u^ {\star}+\gamma\inf_{\mathcal{P}\in\mathcal{U}^{\star,\sigma}(e_{s})}\mathcal{P} \widehat{V}^{\star,\sigma}\] \[=\widehat{V}^{\star,\sigma}(s)-\gamma\inf_{\mathcal{P}\in \mathcal{U}^{\star,\sigma}(e_{s})}\mathcal{P}\widehat{V}^{\star,\sigma}+ \gamma\inf_{\mathcal{P}\in\mathcal{U}^{\star,\sigma}(e_{s})}\mathcal{P} \widehat{V}^{\star,\sigma}=\widehat{V}^{\star,\sigma}(s),\] (198)using in the first equality is the definition of \(P^{s,u^{*}}_{s,a}\) in (194) and where we use the definition of \(u^{*}\) in (196) in the second one.

Finally, we have proved that there exists a fixed point \(\widehat{Q}^{\star,\sigma}_{s,u^{*}}\) of the operator \(\widehat{\mathcal{T}}^{\sigma}_{s,u^{*}}(\cdot)\) by taking

\[\begin{cases}\widehat{Q}^{\star,\sigma}_{s,u^{*}}(s,a)=\widehat{V}^{\star, \sigma}(s)&\forall a\in\mathcal{A},\\ \widehat{Q}^{\star,\sigma}_{s,u^{*}}(s^{\prime},a)=\widehat{Q}^{\star,\sigma}(s ^{\prime},a)&\forall s^{\prime}\neq s\text{ and }a\in\mathcal{A}.\end{cases} \tag{199}\]

we have confirmed the existence of a fixed point of the operator \(\widehat{\mathcal{T}}^{\sigma}_{s,u^{*}}(\cdot)\) with corresponding value function \(\widehat{V}^{\star,\sigma}_{s,u^{*}}\) that coincide with \(\widehat{V}^{\star,\sigma}\). Note that the corresponding properties between \(\widehat{\mathcal{M}}_{\text{rob}}\) and \(\widehat{\mathcal{M}}^{s,u}_{\text{rob}}\) in Step 1 and Step 2 hold in fact for any uncertainty set and \(s\)- or \(sa\)-rectangular assumptions. Equipped with these fixed point equalities, we can use concentration inequalities to show this lemma.

Concentration inequality using an \(\varepsilon\)-net for all reward values \(u\).First we can verify that

\[0\leq u^{\star}\leq\left[\widehat{V}^{\star,\sigma}(s)\right]_{\alpha^{ \lambda,\sigma*}_{P_{sa}}}\leq\widehat{V}^{\star,\sigma}(s)\leq\frac{1}{1-\gamma}. \tag{200}\]

Then, we define a \(N_{\varepsilon_{2}}\)-net over the interval \(\left[0,1/(1-\gamma)\right]\), where \(|N_{\varepsilon_{2}}|\) the size of the net can be controlled by \(|N_{\varepsilon_{2}}|\leq\frac{3}{\varepsilon_{2}(1-\gamma)}\)(Vershynin, 2018). The only parameter that varies is \(\lambda\) in the variation family, \(\alpha^{\lambda}_{P_{sa}}\) so we have \(1\)-dimensional control and not a vector in \(\mathbb{R}^{S}\). Then similarly to Lemma C.3, it holds that for each \(u\in N_{\varepsilon_{2}}\), there exists a unique fixed point \(\widehat{Q}^{\star,\sigma}_{s,u}\) of the operator \(\widehat{\mathcal{T}}^{\sigma}_{s,u}(\cdot)\), which satisfies \(0\leq\widehat{Q}^{\star,\sigma}_{s,u}\leq\frac{1}{1-\gamma}\cdot 1\). Consequently, the corresponding robust value function can be upper bounded by \(\left\|\widehat{V}^{\star,\sigma}_{s,u}\right\|_{\infty}\leq\frac{1}{1-\gamma}\). Using (194) and (193) by construction for all \(u\in N_{\varepsilon_{2}}\), \(\widehat{\mathcal{M}}^{s,u}_{\text{rob}}\) is statistically independent of \(\widehat{P}^{0}_{s,a}\). This independence indicates that \([\widehat{V}^{\star,\sigma}_{s,u}]_{\alpha}\) and \(\widehat{P}^{0}_{s,a}\) are independent for a fixed \(\alpha\). Using (148) and (149) and taking the union bound over all \((s,a,\alpha)\in\mathcal{S}\times\mathcal{A}\times N_{\varepsilon_{1}}\), \(u\in N_{\varepsilon_{2}}\) gives that, with probability at least \(1-\delta\), it holds for all \((s,a,u)\in\mathcal{S}\times\mathcal{A}\times N_{\varepsilon_{2}}\) that

\[\max_{\alpha^{\lambda,\nu}_{P_{sa}}\in\Lambda^{\lambda,\nu}_{P_{ sa}}}\left|\left(P^{0}_{s,a}-\widehat{P}^{0}_{s,a}\right)\left[\widehat{V}^{ \star,\sigma}_{s,u}\right]_{\alpha^{\lambda,\sigma*}_{P_{sa}}}\right| \leq 2\sqrt{\frac{2\log(\frac{18\|\|_{1}\mathcal{S}AN|N_{ \varepsilon_{2}}|}{\delta})}{N}}\sqrt{\operatorname{Var}_{P^{0}_{s,a}}( \widehat{V}^{\star,\sigma}_{s,u})} \tag{201}\] \[+\varepsilon_{2}\] \[\leq 2\sqrt{\frac{2\log(\frac{18\|_{1}\mathcal{S}AN|N_{ \varepsilon_{2}}|}{\delta})}{(1-\gamma)^{2}N}}+\varepsilon_{2}, \tag{202}\]

Finally, we use **uniform concentration** to obtain the lemma. Recalling that \(u^{\star}\in\left[0,\frac{1}{1-\gamma}\right]\) (see (200)), we can always find some \(\overline{u}\in N_{\varepsilon_{2}}\) such that \(|\overline{u}-u^{\star}|\leq\varepsilon_{2}\). Consequently, plugging in the operator \(\widehat{\mathcal{T}}^{\sigma}_{s,u}(\cdot)\) in (195) yields

\[\forall Q\in\mathbb{R}^{SA}:\quad\left\|\widehat{\mathcal{T}}^{\sigma}_{s, \overline{u}}(Q)-\widehat{\mathcal{T}}^{\sigma}_{s,u^{*}}(Q)\right\|_{\infty} =|\overline{u}-u^{\star}|\leq\varepsilon_{2}\]

We can then remark that the fixed points of \(\widehat{\mathcal{T}}^{\sigma}_{s,\overline{u}}(\cdot)\) and \(\widehat{\mathcal{T}}^{\sigma}_{s,u^{*}}(\cdot)\) obey

\[\left\|\widehat{Q}^{\star,\sigma}_{s,\overline{u}}-\widehat{Q}^{ \star,\sigma}_{s,u^{*}}\right\|_{\infty} =\left\|\widehat{\mathcal{T}}^{\sigma}_{s,\overline{u}}(\widehat {Q}^{\star,\sigma}_{s,u})-\widehat{\mathcal{T}}^{\sigma}_{s,u^{*}}(\widehat {Q}^{\star,\sigma}_{s,u^{*}})\right\|_{\infty}\] \[\leq\left\|\widehat{\mathcal{T}}^{\sigma}_{s,\overline{u}}(\widehat {Q}^{\star,\sigma}_{s,\overline{u}})-\widehat{\mathcal{T}}^{\sigma}_{s,\overline{u}}( \widehat{Q}^{\star,\sigma}_{s,u^{*}})\right\|_{\infty}+\left\|\widehat{ \mathcal{T}}^{\sigma}_{s,\overline{u}}(\widehat{Q}^{\star,\sigma}_{s,u^{*}})- \widehat{\mathcal{T}}^{\sigma}_{s,u^{*}}(\widehat{Q}^{\star,\sigma}_{s,u^{*}}) \right\|_{\infty}\] \[\leq\gamma\left\|\widehat{Q}^{\star,\sigma}_{s,\overline{u}}- \widehat{Q}^{\star,\sigma}_{s,u^{*}}\right\|_{\infty}+\varepsilon_{2},\]

where we use that the operator \(\widehat{\mathcal{T}}^{\sigma}_{s,\overline{u}}(\cdot)\) is a \(\gamma\)-contraction. It gives that:

\[\left\|\widehat{Q}^{\star,\sigma}_{s,\overline{u}}-\widehat{Q}^{\star,\sigma}_{s,u^{*}}\right\|_{\infty}\leq\frac{\varepsilon_{2}}{(1-\gamma)}\quad\text{and} \quad\left\|\widehat{V}^{\star,\sigma}_{s,\overline{u}}-\widehat{V}^{\star, \sigma}_{s,u^{*}}\right\|_{\infty}\leq\left\|\widehat{Q}^{\star,\sigma}_{s, \overline{u}}-\widehat{Q}^{\star,\sigma}_{s,u^{*}}\right\|_{\infty}\leq\frac{ \varepsilon_{2}}{(1-\gamma)}. \tag{203}\]Finally to control the first term in (191), using the identity \(\widehat{V}^{\star,\sigma}=\widehat{V}^{\star,\sigma}_{s,u^{\star}}\) or fixed point relation between the two RMPDS, established in previous step of the proof gives that: for all \((s,a)\in\mathcal{S}\times\mathcal{A}\),

\[\max_{\alpha_{P_{s,a}^{\lambda,\omega}}^{\lambda,\omega}\in\mathrm{ \mathbb{A}}^{\lambda,\omega}_{P_{s,a}}}\left|\left(P_{s,a}^{0}-\widehat{P}_{s,a }^{0}\right)[\widehat{V}^{\star,\sigma}]_{\alpha_{P_{s,a}}^{\lambda,\omega}}\right|\] \[\leq\max_{\alpha_{P_{s,a}^{\lambda,\omega}}^{\lambda,\omega}\in \mathrm{\mathbb{A}}^{\lambda,\omega}_{P_{s,a}}}\left|\left(P_{s,a}^{0}-\widehat {P}_{s,a}^{0}\right)[\widehat{V}^{\star,\sigma}]_{\alpha_{P_{s,a}}^{\lambda, \omega}}\right|\] \[\stackrel{{\mathrm{(a)}}}{{\leq}}\max_{\alpha_{P_{s,a }^{\lambda,\omega}}^{\lambda,\omega}\in\mathrm{\mathbb{A}}^{\lambda,\omega}_{P_ {s,a}}}\left\{\left|\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)[\widehat{V }^{\star,\sigma}_{s,\overline{u}}]_{\alpha_{P_{s,a}}^{\lambda,\omega}}\right|+ \left|\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)\left([\widehat{V}^{\star, \sigma}_{s,\overline{u}}]_{\alpha_{P_{s,a}}^{\lambda,\omega}}-[\widehat{V}^{ \star,\sigma}_{s,u^{\star}}]_{\alpha_{P_{s,a}}^{\lambda}}\right)\right|\right\}\] \[\stackrel{{\mathrm{(b)}}}{{\leq}}\max_{\alpha_{P_{s,a }^{\lambda,\omega}}^{\lambda,\omega}\in\mathrm{\mathbb{A}}^{\lambda,\omega}_{P_ {s,a}}}\left|\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)[\widehat{V}^{\star,\sigma}_{s,\overline{u}}]_{\alpha_{P_{s,a}}^{\lambda}}\right|+\frac{2\varepsilon _{2}}{(1-\gamma)}\] \[\stackrel{{\mathrm{(c)}}}{{\leq}}\frac{2\varepsilon_ {2}}{(1-\gamma)}+\varepsilon_{2}+2\sqrt{\frac{2\log(\frac{18\|1\|_{s}SAN|N_{ \varepsilon_{2}}|}{\delta})}{N}}\sqrt{\mathrm{Var}_{P_{s,a}^{0}}(\widehat{V}^{ \star,\sigma}_{s,u})}+\frac{4\log(\frac{18\|1\|_{s}SAN|N_{\varepsilon_{2}}|} {\delta})}{3N(1-\gamma)}\] \[\leq\frac{3\varepsilon_{2}}{(1-\gamma)}+2\sqrt{\frac{2\log(\frac{ 18\|1\|_{s}SAN|N_{\varepsilon_{2}}|}{\delta})}{N}}\sqrt{\mathrm{Var}_{P_{s,a}^ {0}}(\widehat{V}^{\star,\sigma})}+\frac{4\log(\frac{18\|1\|_{s}SAN|N_{ \varepsilon_{2}}|}{\delta})}{3N(1-\gamma)}\] \[\leq 2\sqrt{\frac{L^{\prime\prime}}{N}}\sqrt{\mathrm{Var}_{P_{s,a} ^{0}}(\widehat{V}^{\star,\sigma})}+\frac{14\log(\frac{54\|1\|_{s}SAN|N_{ \varepsilon_{2}}|}{\delta})}{N(1-\gamma)} \tag{204}\] \[\leq 16\sqrt{\frac{L^{\prime\prime}}{(1-\gamma)^{2}N}}, \tag{206}\]

with \(L^{\prime\prime}=\log\left(\frac{54\|1\|_{s}SAN^{2}}{(1-\gamma)\delta}\right)\) where (a) comes from triangular inequality, (b) is due (203), for any \(\alpha\in\mathbb{R}^{S}\)

\[\left|\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)\left([\widehat {V}^{\star,\sigma}_{s,\overline{u}}]_{\alpha}-[\widehat{V}^{\star,\sigma}_{s,u ^{\star}}]_{\alpha}\right)\right| \leq\left\|P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right\|_{1}\left\|[ \widehat{V}^{\star,\sigma}_{s,\overline{u}}]_{\alpha}-[\widehat{V}^{\star, \sigma}_{s,u^{\star}}]_{\alpha}\right\|_{\infty}\] \[\leq 2\left\|\widehat{V}^{\star,\sigma}_{s,\overline{u}}-\widehat {V}^{\star,\sigma}_{s,u^{\star}}\right\|_{\infty}\leq\frac{2\varepsilon_{2}}{( 1-\gamma)}, \tag{207}\]

(c) follows from (201), (d) holds using Lemma 1 with (203). Here, the two last inequalities hold by letting \(\varepsilon_{2}=\frac{2\log(\frac{18\|1\|_{s}SAN|N_{\varepsilon_{2}}|}{N})}{N}\), which gives \(|N_{\varepsilon_{2}}|\leq\frac{3}{\varepsilon_{2}(1-\gamma)}\leq\frac{3N}{1-\gamma}\), and the last inequality holds by the fact \(\mathrm{Var}_{P_{s,a}^{0}}(\widehat{V}^{\star,\sigma})\leq\|\widehat{V}^{\star, \sigma}\|_{\infty}\leq\frac{1}{1-\gamma}\) and letting \(N\geq 2\log\left(\frac{54\|1\|_{s}SAN^{2}}{(1-\gamma)\delta}\right)=L^{\prime\prime}\).

Rewriting (186), the first term of the max is controlled.

\[\max\Big{\{}\left|\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)\left[\widehat {V}^{\widehat{\pi},\sigma}\right]_{\alpha_{P_{s,a}}^{\lambda\star}}\right|, \left|\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)\left[\widehat{V}^{\widehat {\pi},\sigma}\right]_{\alpha_{P_{s,a}}^{\lambda\star}}\right|\Big{\}}\]

The second term can be controlled by the same term as the first one plus an additional term with

\[\left|\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)[\widehat{V }^{\widehat{\pi},\sigma}]_{\alpha_{P_{s,a}}^{\lambda\star}}\right|\leq\] \[\max_{\mu_{P_{s,a}^{0}}^{\lambda\lambda}\in\mathrm{\mathbb{M}}^{ \lambda}_{P_{s,a}^{0}}}\left(P_{s,a}^{0}-\widehat{P}_{s,a}^{0}\right)( \widehat{V}^{\star,\sigma}-\mu_{P_{s,a}^{0}}^{\lambda})+\max_{\mu_{P_{s,a}^{0}}^{ \lambda}\in\mathrm{\mathbb{M}}^{\lambda}_{P_{s,a}^{0}}}\left(P_{s,a}^{0}- \widehat{P}_{s,a}^{0}\right)(\mu_{P_{s,a}^{0}}^{\lambda}-\mu_{P_{s,a}^{0}}^{ \lambda})\right|\]

[MISSING_PAGE_EMPTY:47]

[MISSING_PAGE_EMPTY:48]

[MISSING_PAGE_EMPTY:49]

However, we also \(\|V^{\prime}\|_{\infty}\leq\|V^{\pi,P}\|_{\infty}\leq\frac{1}{1-\gamma}\) in (219). So finally, the result is

\[\left(I-\gamma P_{\pi}\right)^{-1}\sqrt{\operatorname{Var}_{P_{\pi}}(V^{\pi,P}) }\leq\sqrt{\frac{8}{\gamma^{2}(1-\gamma)^{2}}\mathrm{sp}(V^{\pi,P})_{\infty}}1.\]

## Appendix E Proof of Theorem 2

In this section, we focus on the scenarios in the uncertainty sets are constructed with \((s,a)\)-rectangularity condition with some general norms. Towards this, we firstly observe that for the two limiting cases \(\ell_{1}\) norm and \(\ell_{\infty}\) norm, one has \(\|p_{1}-p_{2}\|_{1}\leq 2\) and \(\|p_{1}-p_{2}\|_{\infty}\leq 1\) for any two probability distribution \(p_{1},p_{2}\in\mathbb{R}^{S}\). Namely, the accessible ranges of the uncertainty level \(\sigma\) for \(\ell_{1}\) norm and \(\ell_{\infty}\) norm are \((0,2]\) and \((0,1]\), respectively. In addition, we have

\[\forall p_{1},p_{2}\in\mathbb{R}^{S}:\quad\|p_{1}-p_{2}\|_{\infty}\leq\|p_{1}- p_{2}\|\leq\|p_{1}-p_{2}\|_{1} \tag{221}\]

for any norm \(\|\cdot\|\). It indicates that the accessible range of the uncertainty level \(\sigma_{\|\cdot\|}\) for any given norm \(\|\cdot\|\) is between \(\left(0,\sigma_{\|\cdot\|}^{\max}\right)\), where \(1\leq\sigma_{\|\cdot\|}^{\max}\leq 2\).

To continue, we specify the definition of the uncertainty set with \(sa\)-rectangularity condition with some given general norm \(\|\cdot\|\) as below: for any nominal transition kernel \(P\in\mathbb{R}^{SA\times S}\),

\[\mathcal{U}_{\|\cdot\|}^{\sigma}(P)\coloneqq\mathcal{U}_{\|\cdot\|}^{\sigma}(P )=\otimes\,\mathcal{U}_{p}^{\sigma}(P_{s,a}),\qquad\mathcal{U}_{\|\cdot\|}^{ \sigma}(P_{s,a})\coloneqq\Big{\{}P_{s,a}^{\prime}\in\Delta(\mathcal{S}):\big{\|} P_{s,a}^{\prime}-P_{s,a}\big{\|}\leq\sigma_{\|\cdot\|}\Big{\}}. \tag{222}\]

Then, we recall the assumption of the uncertainty radius \(\sigma_{\|\cdot\|}\in\left(0,\sigma_{\|\cdot\|}^{\max}(1-c_{0})\right]\) with \(0<c_{0}<1\). Then, resorting to the same class of hard MDPs in (Shi et al., 2023, Section C.1), we can complete the proof by directly following the same proof pipeline of Shi et al. (2023, Section C) by replacing \(\sigma\) with \(\sigma_{\|\cdot\|}^{\max}\sigma_{\|\cdot\|}\).

## Appendix F Proof of Theorem 4

Developing the lower bound for the cases with \(s\)-rectangular uncertainty set involves several new challenges compared to that of \((s,a)\)-rectangular cases. Specifically, the first challenge is that the optimal policy can be stochastic and hard to be characterized with a closed form for the RMDPs with a \(s\)-rectangular uncertainty set, rather than deterministic polices in \((s,a)\)-rectangular cases. Such richer and smoother class of optimal policies makes slightly changing the transition kernel generally could only leads to a smoothly changed stochastic optimal policy instead of a completely different one. Such reduced changing of optimal policy further gives smaller performance gap, thus challenges of a tighter lower bound. Second, most of the hard instances in the literature are constructed as \(SA\) states with a constant number of action spaces without loss of generality. While when it comes to \(s\)-rectangular uncertainty set, the action space size becomes important and can't be assumed as a constant anymore. So a new class of instances are required.

To address these challenges, in this section, we construct a new set of hard RMDP instances for two limiting cases: \(\ell_{1}\) norm and \(\ell_{\infty}\) norm.

### Construction of the hard problem instances

Before proceeding, we introduce two useful sets related to the state space and action space as below:

\[\mathcal{S}=\{0,1,\ldots,S\},\qquad\text{and}\qquad\mathcal{A}=\{0,1,\cdots,A -1\}.\]

In this section, we construct a set of RMDPs termed as \(\mathcal{M}_{\ell_{\infty}}\), which consists of \(S(A-1)\) components including \(S(A-1)\) components, each associates with some different state-action pair. Specifically, it is defined as

\[\mathcal{M}_{\ell_{\infty}}\coloneqq\left\{\mathcal{M}_{\theta}=\left( \mathcal{S},\mathcal{A},\mathcal{U}^{\sigma}(P^{\theta}),r,\gamma\right)\mid \theta\in\Theta=\left\{(i,j):(i,j)\in\mathcal{S}\times\mathcal{A}\setminus\{0 \}\right\}\right\}. \tag{223}\]

We introduce the detailed definition of \(\mathcal{M}_{\ell_{\infty}}\) by introducing several key components of it sequentially. In particular, for any RMDP \(\mathcal{M}_{\theta}\in\mathcal{M}_{\ell_{\infty}}\), the state space is of size \(2S\), which includes two classes of states \(\mathcal{X}=\{x_{0},x_{1},\cdots,x_{S-1}\}\) and \(\mathcal{Y}=\{y_{0},y_{1},\cdots,y_{S-1}\}\). The action space for each state is \(\mathcal{A}\) of \(A\) possible actions. So we have totally \(2S\) states and there is in total \(2SA\) state-action pairs.

Armed with the above definitions, we can first introduce the following nominal transition kernel: for all \((s,a)\in\mathcal{X}\cup\mathcal{Y}\times\mathcal{A}\)

\[P^{(0,0)}(s^{\prime}\,|\,s,a)=\left\{\begin{array}{ll}p\mathds{1}(s^{\prime }=y_{i})+(1-p)\mathds{1}(s^{\prime}=x_{i})&\text{if}\quad s=x_{i},a=0,\quad \forall i\in\mathcal{S}\\ q\mathds{1}(s^{\prime}=y_{i})+(1-q)\mathds{1}(s^{\prime}=x_{i})&\text{if}\quad s =x_{i},a\neq 0,\quad\forall i\in\mathcal{S}\\ \mathds{1}(s^{\prime}=s)&\text{if}\quad s\in\mathcal{Y}\end{array}\right. \tag{224}\]

Here, \(p\) and \(q\) are set according to

\[0\leq p\leq 1\quad\text{ and }\quad 0\leq q=p-\Delta \tag{225}\]

for some \(p\) and \(\Delta>0\) that will be introduced momentarily.

Then we introduce the \(S(A-1)\) components inside \(\mathcal{M}_{\infty}\). Namely, for any \((i,j)\in\mathcal{S}\times\mathcal{A}\setminus\{0\}\), the nominal transition kernel of \(\mathcal{M}_{(i,j)}\) is specified as

\[P^{(i,j)}(s^{\prime}\,|\,s,a)=\left\{\begin{array}{ll}p\mathds{1}(s^{\prime }=y_{i})+(1-p)\mathds{1}(s^{\prime}=x_{i})&\text{if}\ s=x_{i},a=j\\ q\mathds{1}(s^{\prime}=y_{i})+(1-q)\mathds{1}(s^{\prime}=x_{i})&\text{if}\ s=x _{i}\in\mathcal{X},a=0\\ P^{(0,0)}(s^{\prime}\,|\,s,a)&\text{otherwise}\end{array}\right. \tag{226}\]

In words, the nominal transition kernel of each variant \(\mathcal{M}_{(i,j)}\) only differs slightly from that of the basic nominal transition kernel \(P^{(0,0)}\) when \(s=x_{i}\) and \(a=\{0,j\}\), which makes all the components inside \(\mathcal{M}_{\ell_{\infty}}\) closed to each other.

In addition, the reward function is defined as

\[\forall a\in\mathcal{A}:\quad r(s,a)=\left\{\begin{array}{ll}1&\text{if}\ s \in\mathcal{Y}\\ 0&\text{otherwise}.\end{array}\right. \tag{227}\]

Uncertainty set of the transition kernels.Recall the following useful notation for any transition probability \(P\), i.e., the transition vector associated with some state \(s\) is denoted as:

\[P_{s}\coloneqq P(\cdot,\cdot\,|\,s)\in\mathbb{R}^{1\times SA},\quad P_{s}^{0} \coloneqq P^{0}(\cdot,\cdot\,|\,s)\in\mathbb{R}^{1\times SA}. \tag{228}\]

With this in hand, the uncertainty set (definition in (5)) with \(\ell_{\infty}\) norm for any \(P^{\theta}\) with \(\theta\in\Theta\) can be represented as:

\[\mathcal{U}_{\infty}^{\mathbf{\delta},\widetilde{\sigma}}(P_{s}^{\theta}) \coloneqq\mathcal{U}_{\|.\|}^{\mathbf{\delta},\widetilde{\sigma}}(P_{s}^{\theta} )=\Big{\{}P_{s}^{\prime}\in\Delta(\mathcal{S})^{\mathcal{A}}:\big{\|}P_{s}^{ \prime}-P_{s}^{\theta}\big{\|}\leq\tilde{\sigma}=\sigma\,\|1\|_{\infty}= \sigma\Big{\}}. \tag{229}\]

So without loss of generality, we set the radius \(\Value functions and optimal policies.For each RMDP instance \(\mathcal{M}_{\theta}\in\mathcal{M}_{\ell_{\infty}}\), with some abuse of notation, we denote \(\pi_{\theta}^{\star}\) as the optimal policy. In addition, let \(V_{\theta}^{\pi,\sigma}\) (resp. \(\overline{V}_{\theta}^{\star,\sigma}\)) represent the corresponding robust value function of any policy \(\pi\) (resp. \(\pi_{\theta}^{\star}\)) with uncertainty level \(\sigma\). Armed with these notations, the following lemma shows some essential properties concerning the value functions and optimal policies; the proof is postponed to Appendix F.3.

**Lemma 12**.: _Consider any \(\mathcal{M}_{\theta}\in\mathcal{M}_{\ell_{\infty}}\) and any policy \(\pi\), one has_

\[\forall(i,j)\in\Theta:\quad V_{(i,j)}^{\pi,\sigma}(x_{i})\leq\frac {\gamma\big{(}z_{(i,j)}^{\pi}-\sigma\big{)}}{(1-\gamma)\bigg{(}1+\frac{\gamma \big{(}z_{(i,j)}^{\pi}-\sigma\big{)}}{1-\gamma(1-\sigma)}\bigg{)}\left(1- \gamma\left(1-\sigma\right)\right)}, \tag{233}\]

_where \(z_{(i,j)}^{\pi}\) is defined as_

\[\forall(i,j)\in\Theta:\quad z_{(i,j)}^{\pi}\coloneqq p\pi(j\,|\,x _{i})+q\left[1-\pi(j\,|\,x_{i})\right]. \tag{234}\]

_In addition, the robust optimal value functions and the robust optimal policies satisfy_

\[\forall(i,j)\in\Theta,s\in\mathcal{X}:\quad V_{(i,j)}^{\star, \sigma}(s)=\frac{\gamma\left(p-\sigma\right)}{(1-\gamma)\left(1+\frac{\gamma \left(p-\sigma\right)}{1-\gamma(1-\sigma)}\right)\left(1-\gamma\left(1-\sigma \right)\right)} \tag{235}\]

_and_

\[\pi_{(i,j)}^{\star}(j\,|\,x_{i})=1\qquad\text{and}\qquad\pi_{(i,j)}^{\star}(0\,|\,s)=1\quad\forall s\in\mathcal{X}\setminus\{x_{i}\}. \tag{236}\]

In words, this lemma shows that for any RMDP \(\mathcal{M}_{(i,j)}\), the optimal policy on state \(x_{i}\) satisfies \(\pi_{(i,j)}^{\star}(j\,|\,x_{i})=1\) and will focus on \(a=0\) for all other states \(s\in\mathcal{X}\setminus\{x_{i}\}\).

### Establishing the minimax lower bound

Step 1: converting the goal to estimate \((i,j)\).Now we are in position to derive the lower bound. Recall the goal is to control the following quantity associated with any policy estimator \(\widehat{\pi}\) based on the dataset with in total \(N_{\text{all}}\) samples:

\[\max_{(i,j)\in\Theta}\mathbb{P}_{(i,j)}\left\{\max_{s\in\mathcal{ X}\cup\mathcal{Y}}\left(V_{(i,j)}^{\star,\sigma}(s)-V_{(i,j)}^{\widehat{\pi}, \sigma}(s)\right)\right\}\geq\max_{(i,j)\in\Theta}\mathbb{P}_{(i,j)}\left\{ \max_{s\in\mathcal{X}}\left(V_{(i,j)}^{\star,\sigma}(s)-V_{(i,j)}^{\widehat{ \pi},\sigma}(s)\right)\right\}. \tag{237}\]

To do so, we can invoke a key claim in Shi et al. (2023) here since our problem setting can be reduced to the same one in Shi et al. (2023): With \(\varepsilon\leq\frac{c_{1}}{32(1-\gamma)}\), letting

\[\Delta=32(1-\gamma)\max\{1-\gamma,\sigma\}\varepsilon\leq c_{1} \max\{1-\gamma,\sigma\} \tag{238}\]

which satisfies (231), it leads to that for any policy \(\widehat{\pi}\) and all \((i,j)\in\Theta\),

\[V_{(i,j)}^{\star,\sigma}(x_{i})-V_{(i,j)}^{\widehat{\pi},\sigma }(x_{i})\geq 2\varepsilon\big{(}1-\widehat{\pi}(j\,|\,x_{i})\big{)},\] \[\forall s\in\mathcal{X}\setminus\{x_{i}\}:\quad V_{(i,j)}^{\star, \sigma}(s)-V_{(i,j)As a result, taking

\[j^{\prime}=\arg\max_{a\in\mathcal{A}}\,\widehat{\pi}(a\,|\,x_{i}), \tag{242}\]

we are motivated to construct the following estimate of \(\theta\):

\[\widehat{\theta}\begin{cases}=(i,j^{\prime})&\text{if}\quad j^{\prime}>0\\ \in\mathcal{G}_{-w}&\text{if}\quad j^{\prime}=0,\end{cases} \tag{243}\]

which satisfies

\[\mathbb{P}_{(i,j)}\big{\{}\widehat{\theta}=(i,j)\big{\}}\geq\mathbb{P}_{(i,j)} \big{\{}j^{\prime}=j\big{\}}\geq\mathbb{P}_{(i,j)}\big{\{}\widehat{\pi}(j\,|\, x_{i})>\frac{1}{A}\big{\}}\geq\frac{3}{4}. \tag{244}\]

Step 2: developing the probability of error in testing multiple hypotheses.Before proceeding, we discuss the dataset consisting of in total \(N_{\text{all}}\) independent samples. Observing that each RMDP inside the set \(\mathcal{M}_{\ell_{\infty}}\) are constructed symmetrically associated with one pair of states \((x_{i},y_{i})\) for all \(i\in\mathcal{S}\) and another action \(j\in\mathcal{A}\times\{0\}\), respectively. Therefore, it is obvious that the dataset is supposed to be generated uniformly on each \((x_{i},y_{i},j)\) to maximize the information gain, leading to \(\frac{N_{\text{all}}}{S(A-1)}\) samples for any states-action \((x_{i},y_{i},j)\) with \(i\in\mathcal{S},j\in\mathcal{A}\setminus\{0\}\).

Then we are ready to turn to the hypothesis testing problem over \((i,j)\in\Theta\). Towards this, we consider the minimax probability of error defined as follows:

\[p_{\text{e}}\coloneqq\inf_{\phi}\max_{(i,j)\in\Theta}\big{\{} \mathbb{P}_{(i,j)}\big{(}\phi\neq(i,j)\big{)}\big{\}}, \tag{245}\]

where the infimum is taken over all possible tests \(\phi\) constructed from the dataset introduced above.

To continue, armed with the above dataset with \(N_{\text{all}}\) independent samples, we denote \(\mu^{i,j}\) (resp. \(\mu^{i,j}(s,a)\)) as the distribution vector (resp. distribution) of each sample tuple \((s,a,s^{\prime})\) under the nominal transition kernel \(P^{(i,j)}\) associated with \(\mathcal{M}_{(i,j)}\). With this in mind, combined with Fano's inequality from Tsybakov (2009, Theorem 2.2) and the additivity of the KL divergence (cf. Tsybakov (2009, Page 85)), we obtain

\[p_{\text{e}} \geq 1-N_{\text{all}}\frac{\max_{(i,j),(i^{\prime},j^{\prime})\in \Theta,(i,j)\neq(i^{\prime},j^{\prime})}\mathsf{KL}\big{(}\mu^{i,j}\,|\,\mu^{i ^{\prime},j^{\prime}}\big{)}+\log 2}{\log|\Theta|}\] \[\stackrel{{\text{(i)}}}{{\geq}}1-N_{\text{all}}\max_ {(i,j),(i^{\prime},j^{\prime})\in\Theta,(i,j)\neq(i^{\prime},j^{\prime})} \mathsf{KL}\big{(}\mu^{i,j}\,|\,\mu^{i^{\prime},j^{\prime}}\big{)}-\frac{1}{2}\] \[=\frac{1}{2}-N_{\text{all}}\max_{(i,j),(i^{\prime},j^{\prime})\in \Theta,(i,j)\neq(i^{\prime},j^{\prime})}\mathsf{KL}\big{(}\mu^{i,j}\,|\,\mu^{ i^{\prime},j^{\prime}}\big{)} \tag{246}\]

where (i) holds by \(\log|\Theta|\geq 2\log 2\) as long as \(S(A-1)\) are large enough. Then following the same proof pipeline of Shi et al. (2023, Section C.2), we can arrive at

\[p_{\text{e}}\geq\frac{1}{2}-\frac{N_{\text{all}}}{S(A-1)}\frac{4096}{c_{1}}(1 -\gamma)^{2}\max\{1-\gamma,\sigma\}\varepsilon^{2}\geq\frac{1}{4}, \tag{247}\]

if the sample size is selected as

\[N_{\text{all}}\leq\frac{c_{1}S(A-1)}{16396(1-\gamma)^{2}\max\{1-\gamma,\sigma\} \varepsilon^{2}}. \tag{248}\]

Step 3: summing up the results together.Finally, we suppose that there exists an estimator \(\widehat{\pi}\) such that

\[\max_{(i,j)\in\Theta}\mathbb{P}_{(i,j)}\left[\max_{s\in\mathcal{X}\cup \mathcal{Y}}\Big{(}V^{\star,\sigma}_{(i,j)}(s)-V^{\widehat{\pi},\sigma}_{(i, j)}(s)\Big{)}\geq\varepsilon\right]<\frac{1}{4}, \tag{249}\]

then according to (237), we necessarily have

\[\forall s\in\mathcal{X}:\quad\max_{(i,j)\in\Theta}\mathbb{P}_{(i,j)}\left[V^ {\star,\sigma}_{(i,j)}(s)-V^{\widehat{\pi},\sigma}_{(i,j)}(s)\geq\varepsilon \right]<\frac{1}{4}, \tag{250}\]which indicates

\[\forall s\in\mathcal{X}:\quad\max_{(i,j)\in\Theta}\mathbb{P}_{(i,j)}\left[V^{\star, \sigma}_{(i,j)}(s)-V^{\widehat{\pi},\sigma}_{(i,j)}(s)<\varepsilon\right]\geq \frac{3}{4}. \tag{251}\]

As a consequence, (244) shows we must have

\[\forall(i,j)\in\Theta:\quad\mathbb{P}_{(i,j)}\left[\widehat{\theta}=(i,j) \right]\geq\frac{3}{4} \tag{252}\]

to achieve (249). However, this would contract with (247) if the sample size condition in (248) is satisfied. Thus, we complete the proof.

### Proof of Lemma 12

Without loss of generality, we first consider any \(\mathcal{M}_{(i,j)}\) with \((i,j)\in\mathcal{S}\times\mathcal{A}\setminus\{0\}\). Following the same routine of Shi et al. (2023, Section C.3.1), we can verify that the order of the robust value function \(V^{\pi,\sigma}_{(i,j)}\) over different states satisfies

\[\forall k\in\mathcal{S}:\quad V^{\pi,\sigma}_{(i,j)}(x_{k})\leq V^{\pi,\sigma }_{(i,j)}(y_{k}), \tag{253}\]

which means the robust value function of the states inside \(\mathcal{X}\) are always not larger than the corresponding states inside \(\mathcal{Y}\).

Then we denote the minimum of the robust value function over states as below:

\[V^{\pi,\sigma}_{(i,j),\min}\coloneqq\min_{s\in\mathcal{S}}V^{\pi,\sigma}_{(i, j)}(s). \tag{254}\]

In the following arguments, we first take a moment to assume \(V^{\pi,\sigma}_{(i,j),\min}=V^{\pi,\sigma}_{(i,j)}(x_{i})\). With this in mind, we arrive at

\[V^{\pi,\sigma}_{(i,j)}(y_{i})=1+\gamma\left(1-\sigma\right)V^{\pi,\sigma}_{(i, j)}(y_{i})+\gamma\sigma V^{\pi,\sigma}_{(i,j),\min}=\frac{1+\gamma\sigma V^{ \pi,\sigma}_{(i,j)}(x_{i})}{1-\gamma\left(1-\sigma\right)}. \tag{255}\]

Then, when we move on to the characterization of the robust value function at state \(x_{i}\). To do so, we notice two important facts:

1. The nominal transition probability \(P^{(i,j)}_{x_{i},a}\) at state-action pair \((x_{i},a)\) for any \(a\in\mathcal{A}\) is a Bernoulli distribution (see (226) and (224)). The TV distance and the \(\ell_{\infty}\) norm between two Bernoulli distribution are the same.
2. Invoking the definitions of the nominal transition probability in (226) and (224), we have \[P^{(i,j)}_{x_{i},j} =p\mathds{1}(s^{\prime}=y_{i})+(1-p)\mathds{1}(s^{\prime}=x_{i})\] \[P^{(i,j)}_{x_{i},a} =q\mathds{1}(s^{\prime}=y_{i})+(1-q)\mathds{1}(s^{\prime}=x_{i}) \quad\forall a\in\mathcal{A}\setminus\{j\}.\] (256)

With the above two facts in hand, our problem setting is reduced to the same one in Shi et al. (2023) and can reuse the results in Shi et al. (2023, Section C.3.1) to achieve

\[V^{\pi,\sigma}_{(i,j)}(x_{i})\leq\frac{\frac{\gamma\left(z^{\pi}_{(i,j)}- \sigma\right)}{1-\gamma(1-\sigma)}}{(1-\gamma)\bigg{(}1+\frac{\gamma\left(z^ {\pi}_{(i,j)}-\sigma\right)}{1-\gamma(1-\sigma)}\bigg{)}}. \tag{257}\]

and

\[\pi^{\star}_{(i,j)}(j\,|\,x_{i}) =1\] \[V^{\star,\sigma}_{(i,j)}(x_{i}) =\frac{\frac{\gamma\left(z^{\pi^{\star}}_{(i,j)}-\sigma\right)}{ 1-\gamma(1-\sigma)}}{(1-\gamma)\left(1+\frac{\gamma\left(z^{\pi^{\star}}_{(i,j)}-\sigma\right)}{1-\gamma(1-\sigma)}\right)}=\frac{\frac{\gamma(p-\sigma)}{ 1-\gamma(1-\sigma)}}{(1-\gamma)\left(1+\frac{\gamma(p-\sigma)}{1-\gamma(1- \sigma)}\right)}. \tag{258}\]

Analogously, we can verify that for other \(x_{k}\in\mathcal{X}\setminus\{x_{i}\}\),

\[\pi^{\star}_{(i,j)}(0\,|\,x_{k}) =1\] \[V^{\star,\sigma}_{(i,j)}(x_{k}) =\frac{\frac{\gamma(p-\sigma)}{1-\gamma(1-\sigma)}}{(1-\gamma) \left(1+\frac{\gamma(p-\sigma)}{1-\gamma(1-\sigma)}\right)}. \tag{259}\]DRVI for \(sa-\) rectangular algorithm for arbitrary norm

In order to compute the fixed point of \(\widehat{\mathcal{T}}^{\sigma}\), distributionally robust value iteration (DRVI), is defined in Algorithm 1. For \(sa\)-rectangularity, starting from an initialization \(\widehat{Q}_{0}=0\), the update rule at the \(t\)-th (\(t\geq 1\)) iteration is the following \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\):

\[\widehat{Q}_{t}^{\pi}(s,a)=\widehat{\mathcal{T}}^{\sigma}\widehat{Q}_{t-1}^{ \pi}(s,a)=r(s,a)+\gamma\inf_{\mathcal{P}\in\mathcal{U}_{t\cdot\|\cdot\|}^{ \mathbf{n},\sigma}(\widehat{P}_{s,a}^{0})}\mathcal{P}\widehat{V}_{t-1}, \tag{260}\]

where \(\widehat{V}_{t-1}(s)=\max_{\pi}\widehat{Q}_{t-1}^{\pi}(s,a)\) for all \(s\in\mathcal{S}\).

Directly solving (260) is computationally expensive since it involves optimization over a \(S\)-dimensional probability simplex at each iteration, especially when the dimension of the state space \(\mathcal{S}\) is large. Fortunately, given strong duality (260) can be equivalently solved using its dual problem, which concerns optimizing a two variable (\(\lambda\) and \(\omega\)) and thus can be solved efficiently. The specific form of the dual problem depends on the choice of the norm \(\|.\|\), which we shall discuss separately in Appendix C.3. To complete the description, we output the greedy policy of the final Q-estimate \(\widehat{Q}_{T}\) as the final policy \(\widehat{\pi}\), namely,

\[\forall s\in\mathcal{S}:\quad\widehat{\pi}(s)=\arg\max_{a}\widehat{Q}_{T}(s,a). \tag{261}\]

Encouragingly, the iterates \(\big{\{}\widehat{Q}_{t}\big{\}}_{t\geq 0}\) of \(DRVI\) converge linearly to the fixed point \(\widehat{Q}^{*,\sigma}\), owing to the appealing \(\gamma\)-contraction property of \(\widehat{\mathcal{T}}^{\sigma}\).

```
input: empirical nominal transition kernel \(\widehat{P}^{0}\); reward function \(r\); uncertainty level \(\sigma\); number of iterations \(T\). initialization:\(\widehat{Q}_{0}(s,a)=0\), \(\widehat{V}_{0}(s)=0\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\). for\(t=1,2,...,T\)do for\(s\in\mathcal{S},a\in\mathcal{A}\)do  Set \(\widehat{Q}_{t}(s,a)\) according to (260);  end for for\(s\in\mathcal{S}\)do  Set \(\widehat{V}_{t}(s)=\max_{a}\widehat{Q}_{t}(s,a)\);  end for  end for output:\(\widehat{Q}_{T}\), \(\widehat{V}_{T}\) and \(\widehat{\pi}\) obeying \(\widehat{\pi}(s)\coloneqq\arg\max_{a}\widehat{Q}_{T}(s,a)\). Algorithm 1: Distributionally robust value iteration (\(DRVI\)) for infinite-horizon RMDPs for \(sa\)-rectangular for arbitrary norm
```

Using Algorithm 1, it allows getting an \(\epsilon_{opt}\) error in the empirical MDP in the \(sa\)-rectangular case. In the \(s\)-rectangular case, finding an algorithm to get \(\epsilon_{opt}\) is more difficult to use, as the policy is not deterministic anymore and 1 cannot anymore be applied. For \(L_{p}\) norms, Clavier et al. [2023] derived an algorithm but for arbitrary norm we need to consider a more general problem for arbitrary norm in Appendix G 

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Yes Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See conclusion. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Assumptions are stated in lemmas and Theorems.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: Theoretical paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [NA]

Justification: Theoretical paper.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: Theoretical paper. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Theoretical paper. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. ** It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Theoretical paper. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification:Done Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification:Theoretical paper. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Theoretical paper. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Theoretical paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA] Justification:Theoretical paper. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Theoretical paper. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Theoretical paper. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.