[MISSING_PAGE_FAIL:1]

performance across a large number of possible subgroups. Each subgroup test is equivalent to evaluating a separate hypothesis about the model's performance on that specific slice of data. This raises a subtle but important problem--from the perspective of hypothesis testing, such methods _implicitly test multiple hypotheses_ (Sec. 3.1). The more subgroups we test, the more hypotheses we implicitly evaluate. Consequently, this results in critical problems associated with multiple testing: (i) _high false positive_ and (ii) _high false negative rates_ (Sec. 3.2). A third complementary challenge is that each data slice tested is (iii) _not necessarily practically meaningful_. The practical implications of these drawbacks are quite concerning--they hinder our ability to accurately identify where the model performs poorly, thereby undermining the reliability of our model evaluations.

**Can we do better?** To address these three challenges, we propose to _loosen the restrictive assumption of reliance only on data_ and propose a new paradigm of testing called Context-Aware Testing (CAT) to offer an alternative to the dominant data-only view (Sec. 3.3). CAT provides a principled approach to incorporate external knowledge--or context-- to the ML testing process. With the view that each evaluated data slice corresponds to an implicit hypothesis test, we propose that ML evaluation on observational datasets can be achieved via a _context-guided sampling mechanism_ (Definition 1). This mechanism is a sampling procedure that uses context as an inductive bias to prioritize specific data slices to test for which have a higher chance of surfacing meaningful model failures. Therefore, CAT fundamentally helps to answer the question of "what should we test for?".

Let's consider an example of building an ML model to predict prostate cancer [11]. Data-only methods employ a search procedure over the dataset to find divergence across a large number of possible feature combinations which may lead to (i) _high false positive rates_ by identifying spurious underperforming subgroups (e.g. based on eye color or patient ID); (ii) _high false negative rates_ by failing to identify true underperforming subgroups due to the large number of combinations tested and applied testing corrections; and (iii) _testing subgroups which are not practically meaningful_ (e.g. interaction between eye color and height). In contrast, a CAT-based approach would define and target task-relevant subgroups, limiting the number of tests conducted with better false positive control and greater statistical power. As we empirically show in Sec. 5, obtaining many false positives and false negatives is overwhelmingly common in current testing practices.

In bringing CAT to reality, we develop the framework called **SMART**2**Testing, which performs automated ML model evaluation by actively identifying potential failure cases (Sec. 4). SMART uses large language models (LLMs) to generate contextually relevant failure hypotheses to test. We further introduce a _self-falsification mechanism_, to automatically validate the generated failure hypotheses using the available data, allowing efficient pruning of spurious hypotheses. Finally, SMART generates comprehensive model reports that provide insights into the identified failure modes, their impact, and potential root causes, enabling stakeholders to make informed decisions.

Footnote 2: systematic, modular, automated, requirements-responsive, transferable

**Contributions.**1 We identify critical gaps in predominant data-only ML testing, illustrating they miss important dimensions (Sec. 3). 2 We formalize the _Context-Aware Testing_ paradigm, providing a principled framework to incorporate context in addition to data into the testing process, which is then used to guide the generation of targeted tests (Sec. 3). 3 We build the first context-aware testing system, _SMART_ Testing, which employs LLMs to hypothesize likely and relevant model failures and empirically refutes them with data using a novel _self-falsification mechanism_ (Sec. 4). 4 We demonstrate the value of context for effective testing, challenging the de facto data-only paradigm by showing how SMART identifies impactful model failures while avoiding false positives across diverse settings, when compared to data-only testing. Additionally, SMART identifies failures on important societal groups and generates comprehensive model reports (Sec. 5).

## 2 Related work

To highlight the need for SMART Testing, we contrast it with other ML testing paradigms -- specifically Data-only testing methods which address the same testing problem as SMART. We provide an overview in Table 6 and an extended discussion in Appendix A.

_Data-only testing methods_[12, 13, 14, 15]: Address the question: "what should we test?". Data-only methods search the data to find "slices" where the model's predictions underperform compared to average performance, deeming those slices as model failures. Although automated, data-onlyapproaches operate _only_ on raw data without accounting for the problem context. Consequently, they must search across a large space of potential failures, usually covering all subsets of features and their distinct values. While an exhaustive search may seem beneficial, as we show in Sec. 3, the reality is that performing many tests on a finite dataset risks discovering slices where model failure is irrelevant or due to random variability, i.e. the multiple testing problem. SMART Testing addresses this challenge by prioritizing relevant and likely model failures through contextual awareness.

_Orthogonal testing dimensions:_ While SMART addresses what to test, several other dimensions of model testing exist that are orthogonal to our approach (detailed in Appendix A). (i) Behavioral Testing [16; 17; 5] evaluates model behaviors (i.e. responses to data) by operationalizing tests along pre-defined dimensions (often defined by humans) -- rather than discovering the test dimensions. SMART fundamentally differs by addressing the core issue of "what to test". (ii) Software functional testing [18; 19], aims to primarily test functional correctness (e.g. input-output functionality such as monotonicity), rather than testing for failures. In addition, test cases are either pre-specified or specified with an approximation of the underlying model to probe for functional correctness.

## 3 A context-aware testing framework for ML

The prevailing paradigm for testing ML models relies on _data-only_ methods which exclusively use data to surface model failures. In this section, we explore the limitations of data-only methods by viewing ML testing as a multiple hypothesis testing problem. We explore why data-only methods are uniquely prone to finding _false positive_ and _false negative_ model failures. To address this, we introduce a new paradigm of testing called **context-aware testing** which relies on external knowledge, or _context_, as an inductive bias to better identify where models fail.

### A multiple hypothesis testing view of ML evaluation

**Preliminaries.** Denote the feature space by \(\mathcal{X}\) and the label space by \(\mathcal{Y}\), and \(\mathcal{P}\) the joint probability distribution over \(\mathcal{X}\times\mathcal{Y}\). We wish to test a fixed, trained black-box model \(f:\mathcal{X}\rightarrow\mathcal{Y}\), using a finite dataset \(\mathcal{D}\subset\mathcal{X}\times\mathcal{Y}\) usually split into \(\mathcal{D}_{train}=\{(x_{i},y_{i})\}_{i=1}^{N_{train}}\) and \(\mathcal{D}_{test}=\{(x_{i},y_{i})\}_{i=1}^{N_{test}}\). We assume the existence of a loss function \(\ell:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}\) which measures the discrepancy between the model's prediction and the true labels point-wise.

The primary goal of ML testing is to identify _meaningful failure modes_--subgroups (data slices) of the data distribution where the model's performance is significantly worse than its average behavior. Formally, let \(\mathcal{S}\subseteq\mathcal{X}\times\mathcal{Y}\) denote a _data slice_ and let \(\mathcal{P}_{\mathcal{S}}=\mathcal{P}(\cdot|(x,y)\in\mathcal{S})\) be the conditional distribution induced by \(\mathcal{S}\). We aim to identify slices where the slice-specific expected loss \(\mu_{\mathcal{S}}=\mathbb{E}_{(x,y)\sim\mathcal{P}_{\mathcal{S}}}[\ell(f(x),y)]\) significantly exceeds the population-level expected loss \(\mu_{\mathcal{P}}=\mathbb{E}_{(x,y)\sim\mathcal{P}}[\ell(f(x),y)]\)3.

Footnote 3: while technically any subset of the dataset can be considered a data slice, we are practically interested in _meaningful_ failures, such as model failures on vulnerable socio-economic groups.

**Testing ML models is a multiple hypothesis testing problem**. We are interested in identifying failure modes that generalize beyond the training dataset. We can interpret the empirical dataset \(\mathcal{D}\) as a sample from a broader distribution \(\mathcal{P}\), and our goal is to make an _inferential_ claim on the performance on the data slices with respect to \(\mathcal{P}\). Suppose we have a candidate data slice \(\hat{\mathcal{S}}\subseteq\mathcal{D}\). We can evaluate the empirical slice-specific loss as \(\hat{\mu}_{\mathcal{S}}=|\hat{\mathcal{S}}|^{-1}\sum_{(x,y)\in\mathcal{S}} \ell(f(x),y)\) and compare it to the empirical loss over the entire dataset \(\hat{\mu}_{\mathcal{D}}=|\mathcal{D}|^{-1}\sum_{(x,y)\in\mathcal{D}}\ell(f(x),y)\). To make an inferential claim about the model's performance on the data slice \(\mathcal{S}\) w.r.t. \(\mathcal{P}\), we can follow the frequentist testing paradigm and formulate a hypothesis test where we evaluate whether the performance is significantly different. Therefore, \(H_{0}:\mu_{\mathcal{S}}=\mu_{\mathcal{D}}\) and alternative hypothesis \(H_{1}:\mu_{\mathcal{S}}\neq\mu_{\mathcal{D}}\), where \(\mu_{\mathcal{S}}=\mathbb{E}_{(x,y)\sim\mathcal{P}_{\mathcal{S}}}[\ell(f(x),y)]\) and \(\mu_{\mathcal{D}}=\mathbb{E}_{(x,y)\sim\mathcal{P}}[\ell(f(x),y)]\) denote the _true_ slice-specific and population-level losses. In practice, this evaluation could be done by running an appropriate frequentist statistical test and evaluating whether \(p<\alpha\) for each slice, given some pre-defined \(\alpha\).

However, in realistic testing scenarios, we evaluate the model's performance not just on a single slice but on a large collection of candidates \(\{\mathcal{S}_{j}\}_{j=1}^{m}\). This amounts to conducting many simultaneous hypothesis tests. Accounting for multiple testing is important. A naive testing procedure that does not adjust for multiplicity could surface a large number of spurious failure modes simply by chance (Type I error). Conversely, controlling the false discovery rate [20] may involve adjusting the per-test significance threshold to \(\alpha^{\prime}\ll\alpha\), potentially sacrificing power to detect true failures (Type II error).

The multiple hypothesis testing viewpoint reveals a key challenge in ML model evaluation: To reliably surface meaningful failures, we require a principled procedure for generating a relatively small number of promising hypotheses (candidate slices) to test.

### The failures of data-only testing

Existing ML testing methodologies are _data-only_ in that they only use the available empirical data to test for model failures and vary in their optimization objective [14, 12, 13, 21, 15]. However, data-only methods operate under the restrictive assumption that the available empirical data is the sole input for testing ML model performance. In practice, this is almost never the case. It is common to have an _a priori_ understanding of where models are likely to fail given the data distribution, model class, training algorithm, and deployment context. This restrictive assumption results in three challenges: **(i) High false positive rate**: data-only methods search over a large space of data slices and each evaluation amounts to an implicit hypothesis test (Sec. 3.1). Therefore, the probability of observing a false failure increases with every test performed. **(ii) High false negative rate**: The naive testing procedure can be made robust by correcting for the number of tests performed which reduces the statistical power to detect true failures. **(iii) Lack of meaningful failures**: Data-only methods are fundamentally limited by the fact that not all statistically significant slices are practically meaningful. We empirically validate these claims in Sec. 5.

The core limitation of data-only testing is the lack of a principled failure mode _discovery_ procedure that can incorporate prior knowledge to guide the search for meaningful errors.

### Formulating context-aware testing

To address the limitations of data-only testing, we introduce **context-aware testing** (CAT), a principled framework for identifying meaningful model failures using context. This could be the context implicitly encoded in the dataset (i.e. via meaningful feature names) or available external input (i.e. external contextual knowledge, such as a string of input information from a human). Let \(\mathcal{C}\) denote the space of all possible contextual information and \(c\in\mathcal{C}\) be specific external input. Our core insight is that we can use \(\mathcal{C}\) as an inductive bias to select which slices to test for.

**Definition 1** (Context-Aware Testing).: _Let \(\mathcal{X}\), \(\mathcal{Y}\), \(\mathcal{P}\), \(f\), \(\mathcal{D}=\{(x_{i},y_{i})\}_{i=1}^{N}\), and \(\ell\) be defined as in the standard supervised learning setup. Let \(\mathcal{C}\) be a space of contexts._

_Context-aware testing is defined by two procedures:_

1. _A context-guided slice sampling mechanism_ \(\pi:\mathcal{C}\times(\mathcal{X}\times\mathcal{Y})\times\mathbb{N}\to 2^{ \mathcal{X}\times\mathcal{Y}}\) _such that_ \(\pi(c,\mathcal{D},m)=\{\mathcal{S}_{1},\ldots,\mathcal{S}_{m}\}\)_, where_ \(c\) _is used as an inductive bias for function_ \(\pi\) _to prioritize slices likely to contain meaningful failures, and_ \(m\in\mathbb{N}\) _are the number of slices to evaluate._
2. _A multiple hypothesis testing procedure:_ \(\forall\mathcal{S}_{i}\in\pi(c,\mathcal{D},m)\)_, test_ \(H_{0}:\mu_{\mathcal{S}_{i}}=\mu_{\mathcal{D}}\) _vs._ \(H_{1}:\mu_{\mathcal{S}_{i}}\neq\mu_{\mathcal{D}}\)_, where_ \(\mu_{\mathcal{S}_{i}}=\mathbb{E}_{(x,y)\sim\mathcal{P}_{\mathcal{S}_{i}}}[\ell (f(x),y)]\)_,_ \(\mu_{\mathcal{D}}=\mathbb{E}_{(x,y)\sim\mathcal{P}}[\ell(f(x),y)]\)__

_A meaningful failure is characterized by statistical significance and practical relevance._

The targeted sampling mechanism \(\pi\) uses the context \(\mathcal{C}\) as an inductive bias to prioritize testing of slices that are (i) _more relevant_ to the deployment context, and (ii) _more likely_ to exhibit significant performance gaps.

This principled slice selection offers several key advantages over data-only methods: _(i) Improved false positive control_: by limiting the number of tests conducted to \(m\), CAT controls the risk of spurious discoveries that arise when naively testing all possible slices. _(ii) Improved true positive rate_: The targeted selection of slices likely to contain failures maintains test power by avoiding the need to aggressively correct for multiple testing. _(iii) Meaningful failures_: context-guided sampling identifies failures that are both statistically significant and practically relevant.

Context-aware testing overcomes the limitations of data-only methods by employing a principled, context-guided slice sampling mechanism \(\pi\) to prioritize the discovery of meaningful model failures.

The core technical challenge in realizing CAT is the development of an effective context-conditional sampling algorithm \(\pi\). In the following section, we propose a concrete instantiation of \(\pi\) using a large language model to generate plausible failure modes and guide testing of ML models.

## 4 SMART Testing

We now instantiate the CAT framework outlined in Sec. 3.3 with a method called **SMART** testing (**s**ystematic, **m**odular, **a**utomated, **r**o**uirements-responsive, and **t**n**fnerable). SMART generates relevant and likely hypotheses about potential model failures and empirically evaluates these hypotheses on available data. SMART follows a four-step procedure: (1) Hypothesis generation; (2) Operationalization; (3) Self-falsification; (4) Reporting. Table 1 provides an early illustration of what it practically means to generate hypotheses and test them with SMART. The procedure below details how the four steps are implemented.

**Step 1: Hypothesis Generation.** Recall from Sec. 3.3, we wish to define a sampling mechanism \(\pi\) to sample slices \(\mathcal{S}\) which are both relevant and have a high relative likelihood of failure -- where \(\pi\) should be both contextually-aware and able to integrate requirements to guide sampling.

We posit that LLMs have the potential to satisfy these properties due to the following capabilities: _Contextual understanding_: LLMs have been pretrained with a vast corpus of information and hence have extensive prior knowledge around different contexts and settings [22, 23, 24, 25, 22]. _Integrate requirements_: LLMs are adept at integrating requirements or additional information about the problem via natural language [26, 24]. _Hypothesis proposers_: In proposing likely failure modes, LLMs have also been shown to be "phenomenonal hypothesis proposers" [27].

An LLM is defined as a probabilistic mapping \(l:\Sigma^{*}\to P(\Sigma)\), where \(\Sigma\) denotes the vocabulary space, and \(P(\Sigma)\) represents the probability distributions over \(\Sigma\). The model processes input sequences \(s\in\Sigma^{*}\), each a concatenation of tokens representing external (contextual) input \(\mathcal{C}_{e}\) (which can be null) and dataset contextualization \(\mathcal{C}_{D}\) is formalized as \(s=(\mathcal{C}_{e},\mathcal{C}_{D})\). We extract the contextualized description \(\mathcal{C}_{D}\) from the dataset \(\mathcal{D}\) using

\begin{table}
\begin{tabular}{l l l l l l} \hline Hypothesis & Verification & p-value & \(|\Delta Acc|\) & Evidence \\ \hline \(f\) underperforms an elderly pe-fent-safety & Elderly patients may have more complex health histories due to age-related comorbidities, which could make predictions less accurate. & age \(>\) = 72 & 0.000 & 0.194 & Supported \\ \(f\) underperforms on patients with multiple comorbidities could complicate the prediction multiple comorbidities. & The presence of multiple comorbidities could complicate the prediction model for an illustration because different labels conditions. & comorbidities \(>\) = 2 & 0.900 & 0.0270 & Not supported \\ \(f\) underperforms on patients under-going conservative management. & Conservative management might be choices for patients with more complex weights & treatment,conservative management = 1 & 0.000 & 0.200 & Supported \\ \(\phi\) conservative management. & or less predictable cases, which could lead to worse predictive performance. & & & & \\ \hline \end{tabular}
\end{table}
Table 1: Example hypotheses on model failure, justifications, and operationalizations generated by the SMART framework on a healthcare dataset. The p-values show whether the model’s performance significantly differs from average performance with \(|\Delta Acc|\) measuring the effect size.

Figure 1: Overview of the SMART Testing Framework showing the four steps. All steps are automatically executed by an LLM.

Figure 2: SMART uses an LLM to integrate context \(\mathcal{C}\), and data context \(\mathcal{D}_{c}\). Relevant and likely failure hypotheses are then generated by LLM (i.e. sampling mechanism). The hypotheses are then operationalized in \(\mathcal{D}\) and evaluated. In contrast, data-only methods are not guided by context and requirements, searching more exhaustively in \(\mathcal{D}\) for divergent slices.

an extractor function \(\mathcal{E}:\mathcal{D}\rightarrow\mathcal{C}_{D}\), which captures essential dataset characteristics (e.g. feature relationships and high-level dataset information). Additionally, we highlight the the LLM will implicitly extract context based on the context encoded in the dataset (e.g. via meaningful feature names). Based on input \(s\), the LLM predicts a distribution over \(\Sigma\) from which hypotheses of model failure and corresponding justifications are sampled.

As depicted in Fig. 2, given \(\mathcal{C}_{e}\) and \(\mathcal{C}_{D}\), we sample the \(N\) most likely hypotheses of failures, \(\mathcal{H}=\{H_{1},H_{2},\ldots,H_{N}\}\), and corresponding justifications \(\mathcal{J}=\{J_{1},J_{2},\ldots,J_{N}\}\), to provide explainability. This process is formalized using the LLM's mapping \(l\) as follows:

\[(H_{i},J_{i})\sim l(s),\text{where }s=(\mathcal{C}_{e},\mathcal{C}_{D}),\; \forall i\in\{1,2,\ldots,N\}.\]

**Step 2: Operationalization.** The process of operationalizing each hypothesis \(H_{i}\in\mathcal{H}\) involves translating its natural language expression into a form that can directly operate on the training dataset \(\mathcal{D}_{train}\) (an example is provided in Table 1). To achieve this, we define an interpreter function \(I:\mathcal{H}\rightarrow\{0,1\}^{\mathcal{X}}\) that maps each natural language hypothesis \(H_{i}\) to a corresponding binary-valued function \(g_{i}:\mathcal{X}\rightarrow\{0,1\}\) on the feature space \(\mathcal{X}\), where \(g_{i}(x)=1\) if \(x\) satisfies the criteria and \(g_{i}(x)=0\) otherwise. Each function \(g_{i}\) induces a data slice \(\mathcal{S}_{i}\subseteq\mathcal{D}_{\text{train}}\) consisting of data points that satisfy the criteria of hypothesis \(H_{i}\), such that \(\mathcal{S}_{i}=\{(x,y)\in\mathcal{D}_{\text{train}}:g_{i}(x)=1\}\). Therefore, each hypothesis, after operationalization, corresponds to a specific slice that is being tested on. Steps 1 and 2 serve to practically instantiate \(\pi\) from Sec. 3.3.

**Step 3: Self-falsification** We introduce a novel self-falsification mechanism to empirically evaluate (or _refute_) the generated hypotheses4. Specifically, for each feasible hypothesis \(H_{i}\in\mathcal{H}\), we attempt to falsify the hypothesis with observed empirical data 5. This involves evaluating the model \(f\) over the data slice \(\mathcal{S}_{i}\) operationalized from \(H_{i}\). We then assess whether the slice performance on \(f\) has a significant deviation from the model's overall performance. For instance, in Table 1, this is done by computing \(|\Delta\text{Acc}|\) and the p-value. The significance of this deviation is determined through frequentist statistical testing, i.e. when \(p<\alpha\) for any \(\alpha\) which might also be adjusted for multiple hypothesis testing. This step effectively "reshuffles/reranks" the hypotheses based on their likelihood on the observed data. For example, when benchmarking we select the top \(n\) hypotheses based on statistical significance: \(\mathcal{H}^{*}=\underset{h_{i}\in\mathcal{H}_{\mathcal{T}}}{\text{arg min}}\)\(\{p_{i}<\alpha\}\).

Footnote 4: self-falsification is rooted in scientific philosophy and the seminal ideas of Popper [28] who proposed the principle of falsification—the ability to refute hypotheses with empirical observations—as driving science.

Footnote 5: We assume data for self-falsification is available. If not, discovered slices could guide new data collection.

_Remark:_ As shown in Fig. 1, we can exclude the self-falsification from SMART in cases of small-sample sizes. We denote this ablation of SMART as \(SMART_{NSF}\).

**Step 4. Model Performance Evaluation and Reporting.** Finally, SMART automatically generates a report of the overall performance of the model under varying conditions generated by the LLM, including a summary report, a complete summary of the tests carried out, intermediate and final results, and potential failure modes of the ML model.

\(\mathcal{Q}\) SMART is a tabular CAT method which (i) directly _searches for model failures_, sampling targeted tests (Sec. 3.3) and (ii) _incorporates data, and context_ into ML testing.

### Practical use of SMART.

We highlight the practical use of SMART testing and emphasize SMART's ease of use and minimal input requirements needed. In particular, as shown in the example below, users do not need any prior knowledge to use SMART. Rather, we make use of the context inherently encoded in the dataset, feature names and task.

Figure 3: A self-falsification module within the SMART framework. A hypothesis generator \(l\) generates plausible hypotheses and justifications for when the model might fail. This is operationalized with \(\phi_{i}\) and tested against the empirical data.

### Robustness to False Positives

**Goal.** We aim to underscore the role of contextual awareness in preventing false positives in model testing. In particular, we consider the scenario when dealing with tabular data that may contain many irrelevant or uninformative features [30], persisting even post-feature selection [31; 32]. We contrast SMART which explicitly accounts for context, to data-only approaches which are context-unaware and can only operate on the numerical data.

**Setup.** We fit a predictive model to the training dataset, varying the number of irrelevant, synthetically generated features contained in the dataset. The irrelevant features are drawn from different distributions. We then quantify the proportion of conditions in the identified slices that _falsely_ include the irrelevant synthetic features. We evaluate using five real-world tabular datasets spanning diverse domains, namely finance, healthcare, criminal justice and education: loan, breast cancer, diabetes, COMPAS recidivism [33] and OULAD education [34]. These datasets have varying characteristics, from sample size to number of features and are representative of different contexts pertinent to tabular ML, to demonstrate the effectiveness of SMART across various real-world contexts

**Analysis.** Fig. 4 shows the proportions of irrelevant features included in slices for increasing numbers of irrelevant features. Data-only methods are unaware of context and are shown to spuriously include high proportions of irrelevant features in their slices; i.e. _false positives (FPs)_. Additionally, these false discoveries increase for data-only methods as the number of irrelevant features increases.

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Sec.** & **Experiment** & **Tategory** \\ \hline Figure 4 & Access databases to irrelevant features & SMART consistently needs relevant features, outperforming data-only methods. \\ Table 3 & Access inhibitors to identify significant model failures & SMART demonstrates kits with larger performance discrepancies across models. \\ Table 4 & Measure PSRs in identifying underperforming subgroups & SMART attributes include the larger negative in all settings. \\ Table 5 & Access inhibitors to potential LLM biases & SMART effectively mitigate biases is identified in implementing subgroups. \\ Table 10 & Evaluate ability to safely rely on hyperparameters & SMART attribute and representation while maintaining statistical significance. \\ Figure 8 & Access inhibitors to suffer with irrelevant feature detection & SMART consistently avoid relevant feature profiles of sample size. \\ Table 11 & Evaluate performance in different deployment environments & SMART facilitates more significant false positives over environments. \\ Figure 9 & Access inhibitors of capsule and co-ordinates & SMART consistently outperform data-only methods. \\ Table 12 & Evaluate stability to the general features. & SMART avoids question features, unlike other methods. \\ Table 13 & Compare performance of GPT-3.5 and in SMART & SMART obtains full GPT-truth captions and irrelevant labels. \\ Table 14 & Evaluate SMART across different datasets. & SMART identifies positive feature descriptors. \\ Table 15 & Explain SMART across different datasets. & SMART identifies positive feature descriptors. \\ Table 16 & Explain the number of hypotheses or "yes-weight models" & SMART aims to predict the number of relevant features in the dataset. \\ Table 17 & Explain the number of hypotheses or "yes-weight models" & SMART identifies positive feature descriptors across various models. \\ Table 18 & Evaluate SMART. & SMART promote comprehensive multi-task learning, providing clear justifications for each hypothesis. \\ \hline \hline \end{tabular}
\end{table}
Table 2: Summary of experiments and takeaways.

Consequently, the sensitivity to FPs of data-only methods risks that slices identified are neither empirically relevant nor meaningful. In contrast, SMART, by virtue of contextual-awareness when generating hypotheses is _not_ sensitive to extraneous and contextually irrelevant features. Importantly, SMART also maintains its robustness to FPs even with an increasing number of irrelevant features.

_Remark._ We also assess sensitivity to the number of data samples. Fig. 9, Appendix D.6 shows that SMART remains robust to variations irrespective of sample size. In contrast, data-only methods have variable performance with false discoveries sensitive to the sample size.

**Avoiding non-existent failure slices.** It is important that model testing does not flag failures when there are none. In Appendix D.5, we demonstrate that SMART's contextual awareness means that it is resistant to spurious failure slices that have no underlying relationships. In contrast, data-only approaches implicitly assume the existence of problematic slices, which we empirically show makes them prone to spuriously flagging non-existent failure slices.

**Takeaway 1.** SMART's contextual awareness ensures testing relevance, thereby reducing false positives across different scenarios, in contrast to data-only methods.

### Targeting model failures

**Goal.** We assess whether the identified failure slices persist when evaluated on new, unseen data across different tabular models. This evaluates the generalization of the identified model failures across multiple different ML models.

**Setup.** We use the prostate cancer dataset [35] and aim to discover slices indicative of underperformance. Thereafter, we assess generalizability of the identified failures across four different ML models (logistic regression, SVM, XGBoost and MLP). Specifically, we compare the top identified slice from each method and compute the absolute difference between the accuracies of that slice and the remainder of the dataset. We posit that a testing framework should be able to identify slices with high-performance discrepancies on unseen data across multiple ML models.

**Analysis.** Table 3 shows that SMART slices exhibit the greatest discrepancies in model accuracies on unseen test data across different ML models -- indicating the discovered failure modes are generalizable. We find that SMART surfaces slices with greater performance differences compared to \(SMART_{NSF}\) (ablation without self-falsification), highlighting the importance of the introduced self-falsification mechanism. In contrast, data-only methods fail to identify slices where the performance discrepancy is as large as SMART. This limitation can be attributed to the tendency of data-only methods to overfit the training data, leading to high false discovery rates.

**Takeaway 2.** SMART discovers failure slices where the model substantially underperforms, generalizing to unseen test data across different ML models. In contrast, data-only approaches fail to find slices where the difference in accuracies is as large, highlighting the lack of generalizability and reliability of their findings.

### Robustness to False Negatives

In our setup, false negatives (FNs) are directly tied to true positives (TPs). i.e. the more true positives we find, the fewer FNs we miss. Across multiple experiments (Fig. 4, Table 10, Table 3, Table 16), we consistently show that SMART identifies TPs at substantially higher rates than data-only. For example, in Table 16, SMART identifies an average of 9.6 out of 10 subgroups where the ML

Figure 4: Contextual-awareness in SMART reduces FPs, i.e. reducing the proportion of irrelevant features in slices. SMART is not sensitive to the number of irrelevant features, unlike data-only methods. \(\downarrow\) is better

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & **LogisticRegression** & **SVM** & **XGBoostlier** & **MLPClassifier** \\ \hline \hline Australian & 0.24 \(\pm\) 0.02 & 0.24 \(\pm\) 0.02 & 0.09 \(\pm\) 0.09 & 0.24 \(\pm\) 0.02 \\ \(PSG\_{,B}\) & 0.23 \(\pm\) 0.01 & 0.23 \(\pm\) 0.01 & 0.11 \(\pm\) 0.07 & 0.23 \(\pm\) 0.01 \\ \(PSG\_{,A}\) & 0.23 \(\pm\) 0.01 & 0.23 \(\pm\) 0.01 & 0.11 \(\pm\) 0.07 & 0.23 \(\pm\) 0.01 \\ Divesfology & 0.05 \(\pm\) 0.11 & 0.09 \(\pm\) 0.13 & 0.14 \(\pm\) 0.15 & 0.02 \(\pm\) 0.05 \\ Stickender & 0.01 \(\pm\) 0.00 & 0.01 \(\pm\) 0.00 & 0.00 \(\pm\) 0.00 & 0.01 \(\pm\) 0.01 \\ Stickender & 0.26 \(\pm\) 0.06 & 0.25 \(\pm\) 0.06 & 0.18 \(\pm\) 0.09 & 0.26 \(\pm\) 0.06 \\ \hline SMART\({}_{\text{NSF}}\) & 0.17 \(\pm\) 0.01 & 0.17 \(\pm\) 0.01 & 0.09 \(\pm\) 0.05 & 0.17 \(\pm\) 0.01 \\
**SMART** & **0.37 \(\pm\) 0.03** & **0.37 \(\pm\) 0.03** & **0.26 \(\pm\) 0.06** & **0.37 \(\pm\) 0.03** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Identifying slices with the highest performance discrepancies. We show differences in accuracies (\(|\Delta Acc|\)) between the top identified divergent slice and average performance across four classifiers (over 5 runs). \(\uparrow\) is better.

model significantly underperforms. The fact that data-driven methods discover fewer such subgroups implies that they are missing the ones SMART uncovers.

**Goal.** That said, we conduct an additional experiment to directly assess the false negative rate, wherein we can control issues (as FNs are naturally unspecified in real data).

**Setup.** We simulate a dataset to predict recidivism (\(Y\)) based on five covariates: gender, race, age, income, and education. \(\log(\frac{P(Y_{i}|X_{i}=j)}{P(Y_{i}\neq j)})=\alpha_{j}-(\delta_{1}X_{\text{ age}}+\delta_{2}C_{\text{income}}+\delta_{3}C_{\text{education}})+\epsilon,\text{ where }\epsilon\sim\mathcal{N}(\mu_{0},\sigma_{0})\). We then train a predictor function \(\hat{f}\) on the data and synthetically introduce underperformance on certain corrupted subgroups. For an individual \(i\), if they belong to corrupted subgroup \(j\), the prediction \(\hat{Y}_{i}\) is equal to \(\hat{f}(X_{i})\) with probability \(1-p\), and a random prediction sampled from a Bernoulli distribution with probability \(p\). If the individual does not belong to subgroup \(j\), the prediction is simply \(\hat{f}(X_{i})\). Finally, we measure how often each testing method identifies that the model \(\hat{f}\) underperforms on a subgroup.

**Analysis.** Table 4 demonstrates results where we synthetically manipulate/corrupt the performance of an ML model on a single subgroup (n=1), two subgroups (n=2), and three subgroups (n=3) out of a total of five. The results show the average of 20 runs, where the corrupted groups are randomly selected within each run. We find that SMART consistently is least susceptible to false negatives across all corrupted variables, when compared to data-only methods which struggle especially once more than one variable is corrupted. This serves to corroborate our earlier results.

**Takeaway 3.** SMART is less prone to FNs, compared to data-only methods across all settings.

### Assessing and mitigating potential LLM challenges and biases.

**Background.** In many settings, we want SMART to be part of a human-in-the-loop model evaluation, particularly to address challenges with LLMs, such as biases or missing dimensions. Let us first discuss how SMART addresses some issues by design and then perform an experimental assessment.

\(\blacktriangleright\)**Using data to mitigate LLM challenges**: We use data in two ways _(i) Data usage in the generation of hypotheses:_ Before generating explicit hypotheses of where the model is likely to fail, we provide the LLM with additional information about the data description and model failures of the training dataset. The hypotheses sampled are therefore reflective of the inductive bias of the LLM as well as being conditioned on the data itself; _(ii) Data usage in falsifying hypotheses:_ core to SMART is the self-falsification mechanism where we iteratively generate hypotheses and test them on a validation dataset. Data is therefore used to filter out hypotheses which are not supported by the data. Hence, even if "incorrect" hypotheses about group failures are proposed, this step ensures they are discarded.

\(\blacktriangleright\)**SMART provides clear and transparent testing**: This is done in two ways: _(i) SMART's model reports:_ that document specific failure cases with natural language justifications (see Appendix D.10 for examples). Automatically generated reports can be a useful tool for humans-in-the-loop experts to audit and validate the testing process, such as evaluating whether tests should be added or removed. For example, a domain expert (e.g. a clinician) could review the report to assess whether the identified failure modes are truly relevant and concerning in that specific context. _(ii) Tests include justifications:_ The justifications for the tests allow human users to inspect the model's testing procedures, understand the reasons, and audit for biases or missed dimensions.

**Goal.** Going beyond the mitigation strategies by design, we also assess SMART's robustness to prior biases. Specifically, we assess common ethnicity related biases of LLMs.

**Setup.** We use the same data generating process as Sec. 5.3.

We train a predictor function \(\hat{f}\) and simulate a scenario where we intentionally corrupt a model's predictions for a proportion \(\tau\) of a minority subgroup ("white" or "black" ethnicity).

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & **FNR (n=1)** & **FNR (n=2)** & **FNR (n=3)** \\ \hline Autostrat & \(0.75\pm 0.44\) & \(0.88\pm 0.22\) & \(0.92\pm 0.15\) \\ pysubgroup\_beam & \(0.65\pm 0.49\) & \(0.75\pm 0.26\) & \(0.68\pm 0.17\) \\ pysubgroup\_apriori & \(0.65\pm 0.49\) & \(0.75\pm 0.26\) & \(0.68\pm 0.17\) \\ Diveyloper & \(0.05\pm 0.22\) & \(0.40\pm 0.35\) & \(0.72\pm 0.31\) \\ Sliceline & \(0.25\pm 0.44\) & \(0.50\pm 0.36\) & \(0.70\pm 0.28\) \\ \hline
**SMART** & **0.00 \(\pm\) 0.00** & **0.05 \(\pm\) 0.15** & **0.38 \(\pm\) 0.27** \\ \hline \hline \end{tabular}
\end{table}
Table 4: False Negative Rate (FNR) for different methods at various settings. \(\downarrow\) is better.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multicolumn{4}{c}{**Corrupted White**} & \multicolumn{2}{c}{**Corrupted black**} \\ \hline \(\tau\) & \(P_{\text{white}}\) & \(P_{\text{dark}}\) & \(P_{\text{white}}\) & \(P_{\text{dark}}\) \\ \hline \(0.01\) & \(0.78\pm 0.04\) & \(0.15\pm 0.04\) & \(0.16\pm 0.04\) & \(0.78\pm 0.04\) \\ \(0.02\) & \(0.88\pm 0.03\) & \(0.08\pm 0.03\) &\(\tau\) ranges from 0 (no corruption) to 1 (completely random predictions for the subgroup). SMART aims to identify the top underperforming subgroup without bias based on historical patterns. We measure the proportion of times the top subgroup contains the "white" (\(P_{\text{white}}\)) or "black" (\(P_{\text{black}}\)) minority, averaged over 20 runs and 5 seeds, separately corrupting "white" and "black" ethnicities.

**Analysis.** We show in Table 5 that SMART is able to identify where models underperform even in scenarios such as ethnicity bias, where LLMs exhibit prior biases from the training dataset. This links to the above discussion that SMART mitigates such biases by design both using the training dataset to guide hypothesis generation and using the self-falsification mechanism to empirically evaluate hypotheses (and discard those that aren't reflective of the data).

**Takeaway 4**. SMART mitigates potential biases in the LLM, both by using the real data to guide hypothesis generation, as well as using the self-falsification mechanism to filter spurious hypotheses.

## 6 Discussion and limitations

Responding to recent calls for better model testing [36, 6], we formalize **Context-Aware Testing**; a new testing paradigm, actively seeking out relevant and likely model failures based on contextual awareness -- going beyond data alone. We develop **SMART Testing**, using LLMs to hypothesize likely and relevant model failures providing _improved_ and _automated_ testing of tabular ML models, compared to data-only methods in various scenarios.

**Model reports.** SMART produces comprehensive and automated model reports documenting failure cases and justifications, thereby providing data scientists, ML engineers and stakeholders increased visibility into model failures. We provide an example SMART report in Appendix D.10.

**Practical considerations**. Given the potential utility of SMART we highlight the following five practical considerations: _Hypothesis generation._ While CAT offers a principled framework for context-guided testing, LLMs present challenges in hypothesis generation. Although SMART has mechanisms to address these (see Sec. 5.4), it cannot guarantee the absence of biases. _Use with small datasets._ SMART may be limited in some cases by insufficient real data to operationalize and test hypotheses, suggesting future work could explore targeted data collection or synthetic data generation to enhance testing. _Extensions to other modalities._ SMART is formalized to test tabular ML models, due to the interpretable and structured features in tabular data to guide hypothesis generation. While extensions to other modalities such as image and text is beyond the scope of this work, this is a promising future research direction that would require addressing the lack of explicitly interpretable features possibly via external metadata and developing new ways to operationalize hypotheses on unstructured data. That said, tabular data is ubiquitous in real-world applications [37, 38] with approximately 79% of data scientists working on tabular problems daily, vastly surpassing other modalities [39, 40]. This highlights the immediate impact and relevance of SMART. _Need for interpretable/meaningful feature names._ Feature names play an important role in finding model failures (see Appendix D.9). The need for interpretable/meaningful feature names (e.g. column labels such as sex, age, race etc) as a source of context is similar to human requirements of interpretable feature names to understand what the data refers to. While feature names are typically the de facto in research and industry datasets, in the rare occasions they are not, this will affect the performance of SMART. _Cost of SMART._ SMART is extremely accessible and cheap to use, approximately \(<\)0.10 USD for 5 hypotheses and \(<\)0.5 USD for 100 hypotheses for state-of-the-art models (see Appendix D.7).

**Broader impact**. Better testing practices can help ensure models are reliable, safe, and beneficial before being deployed in real-world applications. We hope that our work can help mitigate model testing risks in real-world applications as well as spur new testing regimes which are _context aware_.

## Acknowledgments and Disclosure of Funding

We would like to thank the reviewers, Fergus Imrie, Nicolas Astorga, Kasia Kobalczyk, Tennison Liu and Andrew Rashbass for their helpful feedback. PR is supported by GSK, ML is supported by AstraZeneca, NS by the Cystic Fibrosis Trust. This work was supported by Microsoft's Accelerate Foundation Models Academic Research initiative.

## References

* [1] Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Re. Hidden stratification causes clinically meaningful failures in machine learning for medical imaging. In _Proceedings of the ACM Conference on Health, Inference, and Learning_, pages 151-159, 2020.
* [2] Harini Suresh, Jen J Gong, and John V Guttag. Learning tasks for multitask learning: Heterogeneous patient populations in the ICU. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 802-810, 2018.
* [3] Karan Goel, Albert Gu, Yixuan Li, and Christopher Re. Model patching: Closing the subgroup performance gap with data augmentation. In _International Conference on Learning Representations_, 2020.
* [4] Angel Alexander Cabrera, Minsuk Kahng, Fred Hohman, Jamie Morgenstern, and Duen Horng Chau. Discovery of intersectional bias in machine learning using automatic subgroup generation. In _ICLR Debugging Machine Learning Models Workshop_, 2019.
* [5] Boris van Breugel, Nabeel Seedat, Fergus Imrie, and Mihaela van der Schaar. Can you rely on your model evaluation? improving model evaluation with synthetic test data. _Advances in Neural Information Processing Systems_, 36, 2024.
* [6] Nabeel Seedat, Fergus Imrie, and Mihaela van der Schaar. Navigating data-centric artificial intelligence with DC-Check: Advances, challenges, and opportunities. _IEEE Transactions on Artificial Intelligence_, 2023.
* [7] Oleg S Pianykh, Georg Langs, Marc Dewey, Dieter R Enzmann, Christian J Herold, Stefan O Schoenberg, and James A Brink. Continuous learning AI in radiology: Implementation principles and early applications. _Radiology_, 297(1):6-14, 2020.
* [8] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In _International Conference on Machine Learning_, pages 5637-5664. PMLR, 2021.
* [9] Kayur Patel, James Fogarty, James A Landay, and Beverly Harrison. Investigating statistical machine learning as a tool for software development. In _Proceedings of the SIGCHI Conference on Human Factors in Computing Systems_, pages 667-676, 2008.
* [10] Lea Goetz, Nabeel Seedat, Robert Vandersluis, and Mihaela van der Schaar. Generalization--a key challenge for responsible ai in patient-facing clinical applications. _npj Digital Medicine_, 7(1):126, 2024.
* [11] Maire A Duggan, William F Anderson, Sean Altekruse, Lynne Penberthy, and Mark E Sherman. The surveillance, epidemiology and end results (SEER) program and pathology: towards strengthening the critical relationship. _The American Journal of Surgical Pathology_, 40(12):e94, 2016.
* [12] Yeounoh Chung, Tim Kraska, Neoklis Polyzotis, Ki Hyun Tae, and Steven Euijong Whang. Slice finder: Automated data slicing for model validation. In _2019 IEEE 35th International Conference on Data Engineering (ICDE)_, pages 1550-1553. IEEE, 2019.
* [13] Svetlana Sagadeeva and Matthias Boehm. Sliceline: Fast, linear-algebra-based slice finding for ml model debugging. In _Proceedings of the 2021 International Conference on Management of Data_, pages 2290-2299, 2021.
* [14] Adebayo Oshingbesan, Winslow Georgos Omondi, Girmaw Abebe Tadesse, Celia Cintas, and Skyler Speakman. Beyond protected attributes: Disciplined detection of systematic deviations in data. In _Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS 2022_, 2022.

* [15] Eliana Pastor, Luca De Alfaro, and Elena Baralis. Looking for trouble: Analyzing classifier behavior via pattern divergence. In _Proceedings of the 2021 International Conference on Management of Data_, pages 1400-1412, 2021.
* [16] Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of nlp models with checklist. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 4902-4912, 2020.
* [17] Paul Rottger, Bertie Vidgen, Dong Nguyen, Zeerak Waseem, Helen Margetts, Janet Pierrehumbert, et al. Hatecheck: Functional tests for hate speech detection models. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, page 41. Association for Computational Linguistics, 2021.
* [18] Maria Christakis, Hasan Ferit Eniser, Jorg Hoffmann, Adish Singla, and Valentin Wustholz. Specifying and testing k-safety properties for machine-learning models. In _Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence_, pages 4748-4757, 2023.
* [19] Arnab Sharma and Heike Wehrheim. Higher income, larger loan? monotonicity testing of machine learning models. In _Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis_, pages 200-210, 2020.
* [20] Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: a practical and powerful approach to multiple testing. _Journal of the Royal statistical society: series B (Methodological)_, 57(1):289-300, 1995.
* [21] Florian Lemmerich and Martin Becker. pysubgroup: Easy-to-use subgroup discovery in python. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_, pages 658-662, 2018.
* [22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* [23] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. _Nature_, pages 1-9, 2023.
* [24] Nabeel Seedat, Nicolas Huynh, Boris van Breugel, and Mihaela van der Schaar. Curated LLM: Synergy of LLMs and data curation for tabular augmentation in low-data regimes. In _Forty-first International Conference on Machine Learning_, 2024.
* [25] Nicolas Astorga, Tennison Liu, Nabeel Seedat, and Mihaela van der Schaar. Partially observable cost-aware active-learning with large language models. _Advances in Neural Information Processing Systems_, 38, 2024.
* [26] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. _arXiv preprint arXiv:2309.03409_, 2023.
* [27] Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. _arXiv preprint arXiv:2310.08559_, 2023.
* [28] Karl R Popper. Science as falsification. _Conjectures and refutations_, 1(1963):33-39, 1963.
* [29] OpenAI. Gpt-4 technical report, 2023.
* [30] Md Rezault Karim, Md Shajalal, Alexander Grass, Till Dohmen, Sisay Adapna Chala, Alexander Boden, Christian Beecks, and Stefan Decker. Interpreting black-box machine learning models for high dimensional datasets. In _2023 IEEE 10th International Conference on Data Science and Advanced Analytics (DSAA)_, pages 1-10. IEEE, 2023.

* Varsas et al. [2013] Danilo Vasconcellos Vargas, Hirotaka Takano, and Junichi Murata. Contingency training. In _The SICE Annual Conference 2013_, pages 1361-1366. IEEE, 2013.
* Grinsztajn et al. [2022] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? _Advances in Neural Information Processing Systems_, 35:507-520, 2022.
* Angwin et al. [2016] Julia Angwin, Jeff Larson, Lauren Kirchner, and Surya Mattu. Machine bias. _ProPublica: [https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing_](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing_), May 2016.
* Kuzilek et al. [2017] Jakub Kuzilek, Martin Hlosta, and Zdenek Zdrahal. Open university learning analytics dataset. _Scientific data_, 4(1):1-8, 2017.
* Cancer UK PCUK [2019] Prostate Cancer UK PCUK. Cutract. _[https://prostatecanceruk.org_](https://prostatecanceruk.org_), 2019.
* Rostamzadeh et al. [2021] Negar Rostamzadeh, Ben Hutchinson, Christina Greer, and Vinodkumar Prabhakaran. Thinking beyond distributions in testing machine learned models. In _NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications_, 2021.
* Borisov et al. [2021] Vadim Borisov, Tobias Leemann, Kathrin Segler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data: A survey. _arXiv preprint arXiv:2110.01889_, 2021.
* Shwartz-Ziv and Armon [2022] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. _Information Fusion_, 81:84-90, 2022.
* Kaggle [2017] Kaggle. Kaggle machine learning and data science survey, 2017. URL [https://www.kaggle.com/datasets/kaggle/kaggle-survey-2017](https://www.kaggle.com/datasets/kaggle/kaggle-survey-2017).
* Hansen et al. [2023] Lasse Hansen, Nabeel Seedat, Mihaela van der Schaar, and Andrija Petrovic. Reimagining synthetic tabular data generation through data-centric ai: A comprehensive benchmark. _Advances in Neural Information Processing Systems_, 37:33781-33823, 2023.
* Hendrycks and Dietterich [2018] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _International Conference on Learning Representations_, 2018.
* d'Eon et al. [2022] Greg d'Eon, Jason d'Eon, James R Wright, and Kevin Leyton-Brown. The spotlight: A general method for discovering systematic errors in deep learning models. In _Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency_, pages 1962-1981, 2022.
* Kim et al. [2019] Michael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for fairness in classification. In _Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society_, pages 247-254, 2019.
* Singla et al. [2021] Sahil Singla, Besmira Nushi, Shital Shah, Ece Kamar, and Eric Horvitz. Understanding failures of deep networks via robust feature extraction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12853-12862, 2021.
* Eyuboglu et al. [2022] Sabri Eyuboglu, Maya Varma, Khaled Saab, Jean-Benoit Delbrouck, Christopher Lee-Messer, Jared Dunnmon, James Zou, and Christopher Re. Domino: Discovering systematic errors with cross-modal embeddings. _arXiv preprint arXiv:2203.14960_, 2022.
* Shankar et al. [2023] Shreya Shankar, Labib Fawaz, Karl Gyllstrom, and Aditya Parameswaran. Automatic and precise data validation for machine learning. In _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_, pages 2198-2207, 2023.
* Polyzotis et al. [2019] Neoklis Polyzotis, Martin Zinkevich, Sudip Roy, Eric Breck, and Steven Whang. Data validation for machine learning. _Proceedings of machine learning and systems_, 1:334-347, 2019.
* Collins et al. [2015] Gary S Collins, Johannes B Reitsma, Douglas G Altman, and Karel GM Moons. Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (tripod) the tripod statement. _Circulation_, 131(2):211-219, 2015.

## Appendix: Context-aware testing: a new paradigm for testing with large language models

### Table of Contents

* A Extended related work
* A.1 Enhanced overview of relevant literature
* A.2 Components of the ML Testing pipeline
* A.3 Comparison of the features of slice discovery methods
* B SMART Details
* B.1 Contextual understanding
* B.2 Operationalizing variables
* B.3 Feasibility checks
* B.4 Data adjustment queries
* B.5 Requirements, automation, and outputs
* B.6 Moving outside of IID data
* B.7 SMART and multiple testing
* C Benchmarks & Experimental Details
* C.1 Datasets
* C.2 Baselines
* C.3 LLM prompts
* C.3.1 Generic templates.
* C.3.2 Example prompts: OULAD Education.
* C.4 Experimental details
* C.4.1 Context-aware testing (Sec. 5.1.)
* C.4.2 Requirements-constrained testing (Sec. 5.2)
* C.4.3 Targeting model failures (Sec. 5.3.)
* C.4.4 Adaptive testing for a deployment environment (Sec. 5.4.)
* C.4.5 Discovery of societally important groups and failure understanding (Sec. 5.5.)
* D Additional experiments
* D.1 Requirements-constrained testing
* D.2 Adaptive testing for a deployment environment
* D.3 Effects of LLMs
* D.3.1 Comparison over identified divergent slices
* D.3.2 Performance across different models
* D.3.3 Hypothesis generation
* D.4 Effects of different tabular machine learning models
* D.5 On the inductive biases of ML model testing
* D.6 Context aware sensitivity
* D.7 Cost of SMART
* D.8 SMART with open-weight models
* D.9 Understanding the importance of feature names
* D.10 Example of a model report
Extended related work

### Enhanced overview of relevant literature

This work primarily engages with works on ML model testing. Consequently, we detail different paradigms of ML model testing next, which we summarize in Table 6 and Figure 5 showing that none of the existing paradigms satisfy all the properties for ML testing neither in terms of automation nor from the perspective of relevance as they are unable to incorporate context and/or requirements.

**Existing paradigms in ML model testing.** The ML community has predominatly approached ML model evaluation/testing via the use of held-out test datasets. On the basis of the test dataset a single performance metric (accuracy, AUC etc) is computed. This single average evaluation may mask nuances of the model's performance along various dimensions. One approach to address this is to create better benchmark datasets when evaluating models on common benchmarking tasks. For example, manual corruptions like Imagenet-C [41] or by collecting additional real data such as the Wilds benchmark [8]. Benchmark datasets are labor-intensive to collect and their utility is limited to the specific benchmark tasks.

What if we want to evaluate models not confined to benchmarking tasks? To test in cases beyond benchmark tasks, the community has proposed trying to find slices or regions wherein the model fails (i.e. via stress tests). As mentioned in Sec. 1, this could be **Behavioral testing**: which requires human expertise and intuition to define the test scenarios (e.g. Checklist [16], HateCheck [17] or 3S-Testing [5]) -- which is not automated and does not scale. Moreover, it runs a high risk of overlooking critical weaknesses due to human cognitive biases. **Data-only testing**: which does not account for context or requirements and searches exhaustively (e.g. Autostrat [14], SliceFinder [12] or DivExplorer [15]). This may slices focus on arbitrary, less important, or unrealistic/implausible scenarios that are unlikely to be seen in reality. Moreover, we run the risk of the multiple testing problem, where by virtue of the large number of tests evaluated, we might discover a divergent group by chance.

**Hypothesis-driven ML model testing.** In contrast, a hypothesis-driven approach to ML model testing brings about the falsifiability approach widely adopted in science. It begins with the formulation of specific, testable hypotheses based on theoretical understanding, context/domain knowledge, and the intended application of the model (i.e. requirements). The concept of hypothesis-driven ML model testing is deeply rooted in work by Popper [28] who proposed the principle of falsifiability as a driver of scientific progress. The progression of knowledge hinges on the formulation and rigorous testing of hypotheses which can either be falsified or supported by empirical evidence. In the context of hypotheses in ML model testing, testable statements about the model's performance under various conditions, fairness, and robustness can improve our understanding of the model's performance. By rigorously testing these hypotheses, we can uncover the strengths and limitations of ML models.

**Contrast to data-only methods on unstructured data.**

In this paper, we have discussed data-only methods for slice discovery or blindspot discovery applicable to structured data. Specifically, we focus on tabular data where metadata in the form of column names is explicitly encoded into the data. The data-only approaches covered in this work directly search over the raw feature space to identify slices with similar attributes wherein the model would exhibit underperforming predictions.

For completeness, we contrast to data-only approaches often applicable to data without explicit structure and metadata (images, text, audio). There have been numerous methods including [42, 43, 44, 45]

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline \multirow{2}{*}{Paradigm} & \multirow{2}{*}{Objective} & \multicolumn{4}{c}{Search space} \\ \cline{3-8}  & & & \multirow{2}{*}{Context-aware} & \begin{tabular}{l} Integure \\ Requirements \\ \end{tabular} & \begin{tabular}{l} Automated \\ testing \\ \end{tabular} & \multirow{2}{*}{Outputs} & \multirow{2}{*}{Examples} \\ \cline{1-2} \cline{4-8} Average testing & Overall performance & & \begin{tabular}{l} Data split \\ \end{tabular} & \begin{tabular}{l} N.A. \\ \end{tabular} & \begin{tabular}{l} N.A. \\ \end{tabular} & 
\begin{tabular}{l} \(\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{ \boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{ \boldsymbol{\boldsymbol{\boldsymbol{\boldsymbol{ \boldsymbol{\boldsymbol{ \boldsymbol{ \boldsymbol{ \boldsymbol{ \boldsymbol{          }}}}}}}}}}} \,to identify slices of unstructured data with systematic failures. They all follow a similar pattern: (1) the data is embedded (often with a pre-trained) model into a representation space and (2) the underperforming slices are identified by clustering on the raw embedding space or after dimensionality reduction. We then need to post-hoc interpret the clusters in order to understand what they represent. This contrasts the tabular data setting where we find cohesive groups of features that provide an explicit interpretation. Moreover, in tabular regimes, we do not have access to pre-trained models in the same way as for domains such as images or text.

Specifically, to context-aware testing we also note that tabular data inherently includes context through interpretable/meaningful feature names and metadata. This context is not naturally present in images (which are tensors of pixel intensities). Hence, extending CAT to other domains like images or text would require incorporating metadata to provide necessary context -- which is often unavailable. In addition, one would need to develop new ways to operationalize hypotheses on unstructured data.

**Contrast to software testing.**

Going beyond ML model testing, the idea of testing is also prevalent in software systems. For example, unit tests of functions in a codebase. We highlight that software testing of functional input-output correctness is also an area proposed in software testing, which could also theoretically be applied to ML systems. That said, we contrast between SMART and these paradigms in Table 7 below, demonstrating that we tackle a different testing problem.

Additionally, a further orthogonal area to ML model testing is that of data validation [46, 47]. In contrast, to testing an ML model for failures, data validation aims to test input data pipelines for data quality problems or drift.

\begin{table}
\begin{tabular}{p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt}} \hline
**Criteria** & SMART & Christakis et al. [18] & Sharma et al [19] & **How is SMART different?** \\ \hline Test case generation & Automatic & LLM-based test cases & Pre-Specified dimensions of functional correctness (\(\upk\)-safety properties) & Approximates the lack-box model to test with a white-box model & SMART uses the LLM to automatically hypothesize the test cases \& execute them \\ \hline What is the focus of testing & Identify model failures & Identifiers & Assesses I/O functional correctness & Assesses I/O functional correctness & SMT solvers find violations to monotonicity properties. i.e. input increase?=output & SMART failures encompass a broader range of possibilities \\ \hline Are tests context-aware? & Yes & No & No & SMART uniquely incorporates context awareness into testing, leading to more realistic assessments. & SMART uniquely incorporates context awareness into testing assessments. \\ \hline \end{tabular}
\end{table}
Table 7: Comparison of SMART testing with software testing works

Figure 5: Contrasting paradigms of ML model testing along (i) Testing relevance which accounts for requirements and/or context and (ii) Degree of automation and adaptability when carrying out testing. We desire a new paradigm of ML testing to address both.

### Components of the ML Testing pipeline

* Test search: First, we need to decide "what to test". This is a search problem to identify test dimensions. This could either be done via SMART which is targeted (via context and requirements) or data-only methods (where the space is larger). Alternatively, if human experts are available, humans could define the test dimensions.
* Operationalize: Second, we need to operationalize the test and address the challenge of "how to carry out this test on data". If we have sufficient data -- SMART could operationalize the tests on the data via an interpreter. Alternatively, if we don't have sufficient data to run the test -- then SMART could be augmented by behavioral testing approaches such as van Breugel et al. [5] -- which once the test has been defined use synthetic data to augment small subgroups/slices.
* Evaluation report: Third, we carry out the test and evaluate the model to answer the question, "how does the model perform". SMART can be used to produce a comprehensive report of failures and justifications in an automated manner.

### Comparison of the features of slice discovery methods

Even as we zoom into the task of discovering slices where the model might underperform, we observe that SMART has features which are not supported by most of other discovery methods. We exemplify some of these features in Table 8

\begin{table}
\begin{tabular}{l c c c c c} Criteria & SliceFinder & Pysubgroup & DivExplorer & Autostat & SMART Testing \\ Integrates custom domain knowledge & ✗ & ✗ & ✗ & ✗ & \(\blacktriangleright\) \\ Constant slice discovery time & ✗ & ✗ & ✗ & ✗ & \(\blacktriangleright\) \\ Performance is resistant to irrelevant data & ✗ & ✗ & ✗ & ✗ & \(\blacktriangleright\) \\ Can capture rare slices & ✗ & ✗ & ✗ & ✗ & \(\blacktriangleright\) \\ Inherently supports logical ORs & ✗ & ✗ & ✗ & \(\blacktriangleright\) & \(\blacktriangleright\) \\ Resistant to overfitting the training set & ✗ & ✗ & ✗ & ✗ & \(\blacktriangleright\) \\ \end{tabular}
\end{table}
Table 8: Comparison of slice discovery methods

Figure 6: Components of ML Testing — (i) Test search, (ii) Operationalize, (iii) Evaluation report, with example approaches for each component.

SMART Details

We present a block diagram of the key components of SMART Testing in Figure 7. In addition, for each component we provide additional motivations and technical details not covered in the main paper. We further provide more technical information on certain implementation details.

### Contextual understanding

A core component of SMART is leveraging LLMs to strategically identify _what to test for_ in tabular ML models. This process is anchored on the premise that LLMs can effectively navigate the space of potential model failure slices, harnessing their contextual understanding to pinpoint slices where the model is most susceptible to failure. The operation of LLMs within the SMART framework involves three critical inputs:

1. **Context** (\(\mathcal{C}\)): A string describing the overarching scenario and the task at hand.
2. **Dataset Information** (\(\mathcal{D}_{c}\)): Extracted from the training data, this includes a description of the observations where the model did not fail (\(\hat{y}=y\)), or where it failed \(\hat{y}\neq y\)). This could be characterized as a string with a description of the covariates of each group (e.g., mean, median, mode, and a textual description of the distribution).

Utilizing these inputs, the LLM generates a set of hypotheses (\(\mathcal{H}\)) and corresponding justifications (\(\mathcal{J}\)) regarding potential model failures. The framework also incorporates a _self-refine mechanism_ to enhance hypothesis generation. This mechanism iteratively refines hypotheses based on the observed or provided context (\(\mathcal{C}\)) and data (\(\mathcal{D}_{c}\)), re-ranking them by their likelihood. The self-refine mechanism is introduced in order to generate hypotheses that are more likely to target specific model failures.

### Operationalizing variables

Once the hypotheses have been proposed, it is important to operationalize them. This operationalization can be achieved through one of two methods:

1. **LLM-based Operationalization**: Hypothesizing possible operationalizations (e.g., \(age>70\) as a way to operationalize "elderly people"). In this case, the previous interactions and information is provided to the LLM, together with the relevant description of the data. It is then asked to provide possible ways to operationalize a specific hypothesis. This is done by passing an "operationalization prompt" which contains the aforementioned information.

Figure 7: A strong machine learning testing framework should incorporate textual understanding during testing, should meet requirements, should be automated, and provide contextual reports with an emphasis on model failures.

The following is an example of an operationalization prompt in Python:

This is then converted into a specific operationalization for each hypothesis using an external compiler which maps the strings to a function.
2. **Data-driven Operationalization**: Utilizing training data to identify optimal splits for given covariates (e.g., \(age>=82.32\) as a condition to split data based on the "age" covariate). This method receives the covariate (or set of covariates) as an input and returns the optimal split. The optimal split is defined as the split which can identify two groups which have the largest absolute difference in accuracies given some requirements (e.g. a minimum or maximum group size in each group). This splitting can be done by _any black box splitting function_ which takes in a set of inputs and returns a splitting mechanism.

**Black-box splitting function for data-driven operationalization**. While any black-box splitting function could perform on the data, we provide details on the specific function used in SMART.

The function implemented is based on a decision tree model, which can be either a regressor or a classifier, depending on the nature of the outcome variable. In all our experiments, we perform classification-based tasks.

The function operates by fitting a decision tree to the data and recursively traversing the tree to find the split that yields the largest absolute difference in the outcome variable's mean value between two slices. The split must also satisfy group size constraints, specified as minimum and maximum group sizes. The traversal process evaluates each potential split, calculating the mean outcome for each slice and the discrepancy between these means. The optimal split is the one that maximizes this discrepancy while adhering to the group size requirements.

The function returns a query string that represents this optimal split. This string can then be used to segment the dataset into the identified slices for further analysis or testing. The following is pseudocode for the splitting algorithm.

```
Data:\(dataframe\), \(features\), \(outcome\), \(min\_group\_size\), \(max\_group\_size\) Result: Optimal query string for data split
1FunctionGetOptimalSplitQuery(\(dataframe\), \(features\), \(outcome\), \(min\_group\_size\), \(max\_group\_size\)):
2 Validate input features and outcome in \(dataframe\) Determine the type of decision tree model based on \(outcome\) type Fit the decision tree model to \(dataframe\) using \(features\) and \(outcome\) Initialize an empty list \(conditions\) for tracking split conditions return TraverseTree(\(root\), \(0\), \(conditions\))
3FunctionTraverseTree(\(node\), \(depth\), \(conditions\)):
4if\(node\) is a leafthen
5 Calculate the discrepancy in \(outcome\) between the two slices return the condition and discrepancy if group size constraints are met
6
7 end if
8 Determine left and right conditions based on the threshold at \(node\)return TraverseTree(\(node.left\), \(depth+1\), \(conditions\)\(\cup\)\(\{left\_condition\}\)) // or the right conditions, depending on which has the greater discrepancy
```

**Algorithm 1** Optimal splitting mechanism within the SMART framework

This splitting mechanism is used for continuous features. A separate black-box splitting mechanism is developed for categorical features based on iterating on different permutations of these features and evaluating them that helps with variable selection.

In practice, we employ the LLM-based operationalization for the SMART ablation and the data-driven operationalization within the original SMART framework. However, we highlight that this is a design choice that we have found works in practice; there is nothing stopping from using any operationalization framework within the main SMART framework.

### Feasibility checks

SMART is built on many modules which can be toggled on or off. One of such modules is a module called "feasibility check" which evaluates whether there are any hypotheses which should be tested in the first place. The experiments presented in Sec. 5.1 highlight the importance of being able to identify when no relationship between covariates exist.

The feasibility check contains three steps. First, an LLM is queried to evaluate whether any relationships could exist between the covariates and an outcome variable. For instance, this could be whether a relationship could exist between loan default and the annual rainfall in a given region. Second, the answer is self-refined. This helps to evaluate feasibility because, in practice, initial responses tend to be over-optimistic (such as hypothesizing that annual rainfall is associated with loan default via a geographic proxy). This self-refinement helps to critically evaluate the previous answer. The number of steps in the self-refinement process is a hyperparameter. Lastly, the answer (whether or not a relationship could exist between the two variables and, hence, should be inspected) is converted to a boolean value via an external function.

The following is pseudocode which implements the feasibility module.

```
Input:\(unique\_values\), \(context\), \(context\_target\), \(system\_message\), \(n\_refine\) Output:\(feasibility\_boolean\_response\)
8FunctionFeasibilityCheck(\(unique\_values\), \(context\), \(context\_target\), \(system\_message\), \(n\_refine\)): // Construct the feasibility task prompt
9\(task\leftarrow\) "Evaluate subgroups for model performance" \(task\gets task\cup\) "Context: " \(\cup context\cup\) "Target: " \(\cup context\_target\)\(task\gets task\cup\) "Columns: " \(\cup\) join(\(unique\_values\).keys()\) // Get initial feasibility response
10\(feasibility\_response\leftarrow\)GetLLMRResponse(\(task\), \(system\_message\)) // Refine the answer
11\(feasibility\_response\leftarrow\) SelfRefine(\(unique\_values\), \(context\), \(context\_target\), \(feasibility\_response\), \(system\_message\), \(n\_refine\)) // Convert to boolean
12\(boolean\_task\leftarrow\) "Based on analysis, provide yes/no answer" \(boolean\_task\gets boolean\_task\cup\) "Analysis: " \(\cup feasibility\_response\)
13\(feasibility\_boolean\_response\leftarrow\)GetLLMRResponse(\(boolean\_task\))
```

**Algorithm 2** Feasibility for evaluating slices

### Data adjustment queries

Given that SMART operates with an LLM, sometimes the framework outputs proposals which do not operationalize on the data well. For instance, even if a column "age" is a categorical variable, SMART might propose to operationalize the hypothesis "elderly people" as \(age>72\) which would cause an error.

To avoid this, we implement an additional data adjustment module which can handle such cases. It catches the error and re-prompts the LLM to find a group which could be operationalized given the data structure, and does so iteratively until such a group is found.

The following is a pseudocode function that explains how data adjustment is performed.

```
Data:X (DataFrame), n_subgroups (integer) Result:Adjusted subgroup queries in the dataset
14FunctionAdjustSubgroupQuery(X, n_subgroups):
15foreachsubgroupconditiondo
16ifCheckQueryExistence(X, condition)then
17 UpdateSubgroupCondition(condition)
18else
19 // Condition yields no rows, adjust it
20\(adjusted\_condition\leftarrow\)GetLLMRresponse(condition) // Update the condition in the subgroup
21 UpdateSubgroupCondition(adjusted_condition)
22
23 end if
24
25 end foreach
26
27return
```

**Algorithm 3** Pseudocode for adjusting subgroup/slice queries

### Requirements, automation, and outputs

**Requirements**. SMART can natively integrate user requirements into its framework. This is done by inputting requirements as a string and concatenating it together with the context. Some requirements are directly integrated into the framework itself (e.g. functionalities for determining the minimum or maximum sample size of a data split).

**Automation**. Fig. 1 showcases the pipeline of SMART and which components are automated. SMART is developed using an "sklearn" style fit, predict framework. The fit method automatically performs a feasibility check, generates hypotheses, justifications, operationalizes them, performs _self-falsification_ using empirical data, re-ranks the hypotheses, and saves all intermediate results. The method can then automatically be used on any piece of data to evaluate whether it underperforms on the groups that have been found to underperform.

**Outputs**. In addition to SMART outputting subgroups/slices or a scalar number, it can output a model report. An example report is provided in D.10. We note that this report is simply an example which employs both the findings of the fitting procedure and an additional LLM to summarize the outputs. More fine-grained outputs can be constructed.

### Moving outside of IID data

In the paper, we propose that SMART can move outside of IID data and generalize better in the presence of covariate shift (refer to Sec. D.2). We highlight that this is done by performing an analysis on the original data and using an LLM propose possible hypotheses and splits for a different target domain where no data is present (hence, operationalizing only using the LLM with access to previous operationalizations). The following is an example prompt which is designed to do this.

```
1forall
2You have access to the following information.
3
4Dataset information: {context}. {context_target}
5
6The dataset contains {len(unique_values)} columns. The columns are {', '.join(unique_values.keys())}. The values are {str(unique_values.items())}
7
8However, you are no longer working with the same data as just described. Rather, this is the context: {new_context}.
9
10These are the hypotheses: {self_updated_hypotheses}.
11
12TASK: Propose specific variable ranges for each hypothesis such that they are clearly operationalizable and defined. Use this format: Hypothesis: <>; Operationalization: <>.
3"""

Code Listing 3: Operationalization (LLM knowledge): General template

### SMART and multiple testing

The reason why SMART performs testing via hypothesis generation is because testing for all slices is equivalent to generating and testing a hypothesis on the data. Here, we outline in greater detail how hypothesis generation is connected to multiple testing.

As outlined in the main manuscript, searching for \(f\) failures may bring up the challenge of multiple hypothesis testing. Specifically, when we evaluate the failure rate of model \(f\) across different slices \(\mathcal{S}_{i}\subseteq\mathcal{D}\), we are testing the null hypothesis \(H_{0}^{(i)}:\mu_{\mathcal{S}_{i}}=\mu_{\mathcal{D}}\) against the alternative \(H_{1}^{(i)}:\mu_{\mathcal{S}_{i}}\neq\mu_{\mathcal{D}}\). Then, the probability of making a Type I error increases with each test. This drastically inflates the family-wise error rate (FWER). For instance, assuming that each slice is independent, the probability of making one or more Type I errors across all tests is given by \(1-(1-\alpha)^{m}\), where \(m\) is the total number of slices tested. While this can be addressed by adjusting for multiple testing, we run into the trade-off between the FWER control and statistical power. As we employ statistical correction methods to account for Type I errors, we increase the probability of Type II errors.

Benchmarks & Experimental Details

We summarize all experimental details, datasets and benchmarks.

Code can be found at: [https://github.com/pauliusrauba/SMART_Testing](https://github.com/pauliusrauba/SMART_Testing) or [https://github.com/vanderschaarlab/SMART_Testing](https://github.com/vanderschaarlab/SMART_Testing)

### Datasets

We summarize the different datasets we use in this paper in Table 9. The datasets vary in number of samples, number of features and domain.

Note that, depending on the experiment, we might vary the sample size or features used. This can be because of the added synthetic variables or certain slice discovery algorithms taking too long to run.

### Baselines

We compare SMART testing with the following data-only baselines in Section 5.

* Autostrat [14]. We use the implementation from 7 Footnote 7: [https://github.com/Adebayo-Oshingbean/BPS_Autostrat_Code/](https://github.com/Adebayo-Oshingbean/BPS_Autostrat_Code/)
* Pysubgroup [21]. We use the implementation from 8 Footnote 8: [https://github.com/flemmerich/pysubgroup](https://github.com/flemmerich/pysubgroup)
* Divexplorer [15]. We use the implementation from 9 Footnote 9: [https://github.com/elianap/divexplorer](https://github.com/elianap/divexplorer)
* Slicefinder [12]. We use the implementation from 10 Footnote 10: [https://github.com/yeounoh/slicefinder](https://github.com/yeounoh/slicefinder)
* Sliceline [12]. We use the implementation from 11 Footnote 11: [https://github.com/DataDome/sliceline](https://github.com/DataDome/sliceline)

### LLM prompts

#### c.3.1 Generic templates.

```
1"""ConstractsthetaskdescriptionfortheLLM."""
2task=f"""Yourtaskistoproposepossiblehypothesesastowhichsubgroupswithinthedatasetmighthaveworsepredictiveperformancethanonaveragebecauseofsocietalbiasinthedataset,insufficientdata,otherrelationships,orothers.Thesubgroupsmightbebasedonanyoftheprovidedcharacteristics,aswellasonanycombinationofsuchcharacteristics.
3
4Datasetinformation:{context}.{context_target}
5
6Thedatasetcontains{len(unique_values)}columns.Thecolumnsare{','.join(unique_values.keys())}.
7

\begin{table}
\begin{tabular}{c c c c c} \hline Name & \(n\) samples & \(n\) features & Domain & Task \\ \hline Loan & 5000 & 8 & Finance & Loan default \\ Breast & 116 & 8 & Healthcare/Medicine & Breast cancer \\ Compas [33] & 4743 & 8 & Criminal justice & Criminal recidivism \\ OULAD [34] & 5000 & 8 & Education & Exam score \\ Diabetes & 768 & 8 & Healthcare/Medicine & Diabetes label \\ SEER [35] & 2000 & 12 & Healthcare/Medicine & Prostate cancer morality \\ CUTRACT [35] & 20000 & 12 & Healthcare/Medicine & Prostate cancer morality \\ \hline \end{tabular}
\end{table}
Table 9: Summary of the datasets used.

```
8Task:Create{n}hypothesesastowhichsubgroupswithinthedatasetthemodelwillperformworsethanonaveragebecauseofsocietalbissoresorotherreasons.Import:Yourhypothesiscancontaineitheronevariableortwovariablesinthecondition.Therefore,yourgoalistotfinddiscrepanciesinthemodel'sperformancetheunderlyingdataoutcomes.Justifywhyyouthinkthatforeachofthe{n}hypotheses.Formatoftheoutput:Hypothesis:<>;Justification:<>.
9
10"""
11"""
12Thefollowingarehypothesesaboutwhichpeoplewithinadatasetthemodelmightunderperformon.
13Proposespecificrangesforeachhypothesis.Hypotheses:{hypotheses}.
14
15TASK:returnadictionarythatcontainsanindexnumberasthekeyandthecolumnvalueasthevalue.Iftherearemultiplecolumnsinthathypothesis,returntheminalist.Therearethecolumnnames:{','.join(unique_values.keys())}.
16"""
17CodeListing 5: Generic operationalization prompt

#### c.3.2 Example prompts: OULAD Education.
```
1"""
2-------------INPUTTEXT-------------
3Yourtaskistotproposesposiblesprobethesesastowhichsubgroupswithinthedatasetmighthaveworsepredictiveperformancethanonaveragebecauseofsocietalbissinthedataset,insufficientdata,otherrelationships,orothers.Thesubgroupsmightbebasedonanyoftheprovidedcharacteristics,aswellasonanycombinationofsuchcharacteristics.
4
5Datasetinformation:
6OpenUniversityLearningAnalyticsDataset(OULAD)containsdataaboutcourses,studentsandtheirinteractionswithVirtualLearningEnvironment(VLE)forsevenselectedcourses(calledmodules).PresentationsofcoursesstartinFebruaryandOctober-theyaremarkedbyBandJrespectively.Thedatasetconsistsoftablesconnectedusinguniqueidentifiers.Alltablesarestoredinthecsvformat.
7
8Thetaskistotpredictthestudentswhoarelikelytofailorwithdrawfromacourse.
9
10
11Thedatasetcontains14columns.Thecolumnsaregender,region,highest_education,imd_band,age_band,num_of_prev_attempts,studied_credits,disability,test,group_0,group_1,group_2,group_3,group_4.Thevaluesaredictitems[('gender',['M','F']),{'region',['SouthRegion','Scotland','YorkshireRegion','WestMidlandsRegion','EastAnglianRegion','EastMidlandsRegion','SouthEastRegion','Wales','NorthWesternRegion','LondonRegion','SouthWestRegion','Ireland','NorthRegion']),('highest_education',['LowerThanALevel','ALevelorEquivalent','HEQualification','NoFormaluals','PostGraduateQualification']),{'ind_band',['70-80%','40-50%','0-10%','10-20','50-60%','20-30%','60-70%','80-90%','90-100%','30-40%',nan]),('age_band',['0-35','35-55','55<']),{'num_of_prev_attempts',[0,1,2,3,5,4,6]),('studied_credits',{'min':30,'mean':79.77122804326149,'max':655}),('disability', ['N', 'Y']), ('test', [False, True]), ('group_0', [False, True ]), ('group_1', [True, False]), ('group_2', [True, False]), ('group_3', [False, True]), ('group_4', [False, True])])
12
13Task: Create 5 hypotheses as to which subgroups within the dataset the model will perform worse than on average because of societal biases or other reasons. Therefore, your goal is to find discrepancies in the model's performance, not the underlying data outcomes. Justify why you think that. Format of the output: Hypothesis: <>; Justification: <>.
14"""
15
16
17OUTPUTTEXT
17Hypothesis: Students with disabilities will have worse predictive performance than average.
18Justification: Students with disabilities may face additional challenges in their learning process, which could affect their performance in the course. They may require additional support or accommodations that are not adequately captured in the dataset, leading to lower predictive performance for this subgroup.
19
19
20Hypothesis 2: Students from lower socioeconomic backgrounds (indicated by imd_band) will have worse predictive performance than average.
21Justification: Students from lower socioeconomic backgrounds may have limited access to resources and support systems that are necessary for successful completion of the course. This lack of resources and support may not be adequately captured in the dataset, leading to lower predictive performance for this subgroup.
22
23Hypothesis 3: Students with lower levels of education (indicated by highest_education) will have worse predictive performance than average.
24Justification: Students with lower levels of education may have less prior knowledge and skills that are necessary for success in the course. This lack of foundational knowledge may not be adequately captured in the dataset, leading to lower predictive performance for this subgroup.
25
26Hypothesis 4: Students who have attempted the course multiple times ( indicated by num_of_prev_attempts) will have worse predictive performance than average.
27Justification: Students who have attempted the course multiple times may have encountered difficulties or challenges in their previous attempts that hindered their success. These challenges may not be adequately captured in the dataset, leading to lower predictive performance for this subgroup.
28
29Hypothesis 5: Students in certain regions (indicated by region) will have worse predictive performance than average.
20Justification: Students in certain regions may have different educational systems, resources, and support networks that can impact their performance in the course. These regional differences may not be adequately captured in the dataset, leading to lower predictive performance for subgroups based on region.

Code Listing 6: Hypothesis generation: OULAD Dataset

```
1"""
2-------------INPUTTEXT
3
4The following are hypotheses about which people within a dataset the model might underperform on.
5Propose specific ranges for each hypothesis. Hypotheses: Hypothesis 1: Students with disabilities will have worse predictive performance than average. than average.

* 7Justification: Students with disabilities may face additional challenges in their learning process, which could affect their performance in the course. They may require additional support or accommodations that are not adequately captured in the dataset, leading to lower predictive performance for this subgroup.
* 8
* 9Hypothesis 2: Students from lower socioeconomic backgrounds (indicated by imd_band) will have worse predictive performance than average.
* 10Justification: Students from lower socioeconomic backgrounds may have limited access to resources and supports systems that are necessary for successful completion of the course. This lack of resources and support may not be adequately captured in the dataset, leading to lower predictive performance for this subgroup.
* 11
* 12Hypothesis 3: Students with lower levels of education (indicated by highest_education) will have worse predictive performance than average.
* 13Justification: Students with lower levels of education may have less prior knowledge and skills that are necessary for success in the course. This lack of foundational knowledge may not be adequately captured in the dataset, leading to lower predictive performance for this subgroup.
* 14
* 15Hypothesis 4: Students who have attempted the course multiple times ( indicated by num_of_prev_attempts) will have worse predictive performance than average.
* 16Justification: Students who have attempted the course multiple times may have encountered difficulties or challenges in their previous attempts that hindered their success. These challenges may not be adequately captured in the dataset, leading to lower predictive performance for this subgroup.
* 17
* 18Hypothesis 5: Students in certain regions (indicated by region) will have worse predictive performance than average.
* 19Justification: Students in certain regions may have different educational systems, resources, and support networks that can impact their performance in the course. These regional differences may not be adequately captured in the dataset, leading to lower predictive performance for subgroups based on region..
* 20
* 21Dataset information:
* they are marked by B and J respectively. The dataset consists of tables connected using unique identifiers. All tables are stored in the csv format.
* 23
* 24The task is to predict the students who are likely to fail or withdraw from a course.
* 25
* 26
* 27The dataset contains 14 columns. The columns are gender, region, highest_education, imd_band, age_band, num_of_prev_attempts, studied_credits, disability, test, group_0, group_1, group_2, group_3, group_4. The values are dict_items[('gender', ['M', 'F 'J]), ('region', ['South Region', 'Scotland', 'Yorkshire Region', 'West Midlands Region', 'East Anglian Region', 'East Midlands Region', 'South East Region', 'Wales', 'North Western Region', 'London Region', 'South West Region', 'Ireland', 'North Region']), ('highest_education', ['Lower Than A Level', 'A Level or Equivalent', 'HE Qualification', 'No Formal quals', 'Post Graduate Qualification']), ('imd_band', ['70-80%', '40-60%', '0-10%','10-20', '50-60%', '20-30%', '60-70%', '80-90%', '90-100%', '30-40%', manly, ('age_band', ['0-35', '35-65', '55=']), ('num_of_prev_attempts', [0, 1, 2, 3, 5, 4, 6]), ('studied_credits', '[min':30,'mean': 79.77122804326149,'max':655), ('disability ', ['N', 'Y']), ('test', [False, True]), ('group_0', [False, True ]), ('group_1', [True, False]), ('group_2', [True, False]), ('group_3', [False, True]), ('group_4', [False, True])])
28
29TASK: Propose specific variable ranges for each hypothesis such that they are clearly operationalizable and defined. Use this format: Hypothesis: <>; Operationalization: <>.
30"""
31
32
33Hypothesis 1: Students with disabilities will have worse predictive performance than average.
34Operationalization: Students with disabilities can be identified by the "disability" variable in the dataset. To test this hypothesis, we can define the range for students with disabilities as those who have a value of "Y" in the "disability" column.
35
36Hypothesis 2: Students from lower socioeconomic backgrounds (indicated by imd_band) will have worse predictive performance than average.
37Operationalization: Socioeconomic background can be measured using the "imd_band" variable in the dataset. To test this hypothesis, we can define the range for students from lower socioeconomic backgrounds as those who have an "imd_band" value in the range of "0-30%".
38
39Hypothesis 3: Students with lower levels of education (indicated by highest_education) will have worse predictive performance than average.
30Operationalization: Education level can be determined using the " highest_education" variable in the dataset. To test this hypothesis, we can define the range for students with lower levels of education as those who have a value of "Lower Than A Level" or "No Formal quals" in the "highest_education" column.
41
42Hypothesis 4: Students who have attempted the course multiple times ( indicated by num_of_prev_attempts) will have worse predictive performance than average.
43Operationalization: Previous attempts can be measured using the " num_of_prev_attempts" variable in the dataset. To test this hypothesis, we can define the range for students who have attempted the course multiple times as those who have a value greater than 1 in the "num_of_prev_attempts" column.
44
45Hypothesis 5: Students in certain regions (indicated by region) will have worse predictive performance than average.
46Operationalization: Region can be determined using the "region" variable in the dataset. To test this hypothesis, we can define the range for students in certain regions as those who belong to the regions of "North Region" or "Wales".

Code Listing 7: Operationalization (LLM knowledge): OULAD Dataset

```
1"""
2-------------INPUTTEXT------------
3
4The following are groups that are defined based on the dataset. Convert them into a Python dictionary format. Each group should be represented as a key-value pair in the dictionary, where the key is an index (0 to 4), and the value is a string representing the group using Python syntax and logical operators. For multiple conditions, use Python's logical 'and' ('&&') or 'or' ('||'). Ensure the format is a valid Python dictionary.
6Examples:
7- Single Condition: {0: 'X > 45'}
8- Multiple Conditions: {1: '(X > 45) and (Y < 20)'}
9
10Groups to summarize: Hypothesis 1: Students with disabilities will have worse predictive performance than average.
11
12Operationalization: Students with disabilities can be identified by the "disability" variable in the dataset. To test this hypothesis, we can define the range for students with disabilities as those who have a value of "Y" in the "disability" column.
13
14Hypothesis 2: Students from lower socioeconomic backgrounds (indicated by imd_band) will have worse predictive performance than average.
15Operationalization: Socioeconomic background can be measured using the "imd_band" variable in the dataset. To test this hypothesis, we can define the range for students from lower socioeconomic backgrounds as those who have an "imd_band" value in the range of "0-30%".
16
17Hypothesis 3: Students with lower levels of education (indicated by highest_education) will have worse predictive performance than average.
18Operationalization: Education level can be determined using the " highest_education" variable in the dataset. To test this hypothesis, we can define the range for students with lower levels of education as those who have a value of "Lower Than A Level" or "No Formal quals" in the "highest_education" column.
19
20Hypothesis 4: Students who have attempted the course multiple times ( indicated by num_of_prev_attempts) will have worse predictive performance than average.
21Operationalization: Previous attempts can be measured using the " num_of_prev_attempts" variable in the dataset. To test this hypothesis, we can define the range for students who have attempted the course multiple times as those who have a value greater than 1 in the "num_of_prev_attempts" column.
22
23Hypothesis 5: Students in certain regions (indicated by region) will have worse predictive performance than average.
24Operationalization: Region can be determined using the "region" variable in the dataset. To test this hypothesis, we can define the range for students in certain regions as those who belong to the regions of "North Region" or "Wales".
25Column names: gender, region, highest_education, imd_band, age_band, num_of_prev_attempts, studied_credits, disability, test, group_0, group_1, group_2, group_3, group_4
26Column values: dict_items([('gender', ['M', 'F']), ('region', ['South Region', 'Scotland', 'Yorkshire Region', 'West Midlands Region', 'East Anglian Region', 'East Midlands Region', 'South East Region', 'Wales', 'North Western Region', 'London Region', 'South West Region', 'Ireland', 'North Region'), ('highest_education', [' Lower Than A Level', 'A Level or Equivalent', 'HE Qualification', 'No Formal quals', 'Post Graduate Qualification']), ('imd_band', ['70-80%', '40-50%', '0-10%', '10-20', '50-60%', '20-30%', '60-70%', '80-90%', '90-100%', '30-40%', nan]), ('age_band', ['0-35', '35-55', '55<=']), ('num_of_prev_attempts', [0, 1, 2, 3, 5, 4, 61), ('studied_credits', {'min': 30,'mean': 79.77122804326149,'max': 655}), ('disability', ['N', 'Y']), ('test', [False, True]), ('group_0', [False, True]), ('group_1', [ True, False]), ('group_2', [True, False]), ('group_3', [False, True]), ('group_4', [False, True])])

#### c.4 Experimental details

We note that all experiments were performed using two compute resources: a server with NVIDIA RTX A4000 GPU and 18-Core Intel Core i9-10980XE, as well as an Apple M1 Pro 32GB RAM. We exemplify SMART Testing using GPT-4 [29] as the LLM but run further experiments to test the sensitivity to the type of language model in Appendix D.3.

#### c.4.1 Context-aware testing (Sec. 5.1.)

**Goal.** We aim to underscore the role of context in ML model testing to prevent false positives, especially when dealing with tabular data where data may contain many irrelevant or uninformative features [30], persisting even post-feature selection [31, 32]. We contrast SMART which explicitly accounts for context, in contrast to data-only approaches which are context-unaware only operating on the data.

**Setup.** We fit a predictive model to the training dataset, varying the number of irrelevant, synthetically generated features contained in the dataset -- where irrelevant features are drawn from different distributions. We then quantify the proportion of conditions in the identified slices that falsely include the irrelevant synthetic synthetically features.

Because different methods are sensitive to different types of irrelevant features, we developed a data generating processes that encompasses many types of variables. Over many runs, different data-only methods pick up on some of these variables, showcasing that all methods are susceptible to randomly sampled irrelevant features in the dataset.

**Sampling mechanism.** To evaluate the impact of irrelevant features, we enrich the dataset by adding synthetic categorical variables. The number of new variables is equal to the number of existing features in the dataset. For each new variable \(x_{i}\), we determine its type by sampling from a Bernoulli distribution with probability 0.5. If the sampled value is 0, \(x_{i}\) is a Bernoulli variable with success probability 0.1; otherwise, \(x_{i}\) is a categorical variable with four categories, following a predefined probability distribution (e.g., {0.1, 0.3, 0.4, 0.2}):

\[\text{Type}(x_{i}) \sim\text{Bernoulli}(0.5),\] \[x_{i} \sim\begin{cases}\text{Bernoulli}(0.1)&\text{if Type}(x_{i})=0,\\ \text{Categorical}(0.1,0.3,0.4,0.2)&\text{if Type}(x_{i})=1.\end{cases}\]

We note that there are synthetic data generating processes that completely break other data-only methods. As an example, creating a unique ID column for each sample breaks the Autostrat algorithm, as all the subgroups/slices identified are the unique IDs. The data generating process employed in our experiment reflects a broad variety of commonly encountered DGPs.

#### c.4.2 Requirements-constrained testing (Sec. 5.2.)

We test whether each of the methods can fulfil three requirements.

The first requirement involved the use of the variable "age" in each detected slice. This was passed as an input to SMART. The other methods do not accept context as input and, therefore, it was notpossible to fulfil these requirements. The numbers provided for other methods are simply how often they fulfilled the requirements by chance.

The second and third requirements involved obtaining a minimum and maximum sample size. This was passed as an input to SMART and the ablated SMART version. Based on this, SMART changed its hyperparameter within its function which asks to indicate a minimum and maximum sample size for the discovered slices. As with the previous experiments, this was not adjusted for the other groups because they do not take textual input.

#### c.4.3 Targeting model failures (Sec. 5.3.)

In order to evaluate the targeting of model failures, we try four different tabular models with pre-specified hyperparameters. We find discrepant slices on the training dataset and evaluate them on the testing dataset.

#### c.4.4 Adaptive testing for a deployment environment (Sec. 5.4.)

The goal of the adaptive testing experiment is to understand the extent to which SMART, as well as other data-only methods, can use data in a source domain to generalize to a new, target domain where a covariate shift has been detected. To this end, the datasets provided have a known covariate shift and can be evaluated.

Each method was trained on the UK dataset and the discovered slices were evaluated on the US dataset. No additional context was provided to data-only methods since they do not accept any text or context as inputs.

In contrast, we have provided SMART with the previously discovered slices and hypotheses, and have asked to re-evaluate these hypotheses in the context of the US dataset. Specifically, SMART re-hypothesized possible model failures for the US market but used the UK data to operationalize the variables. The ablated version, \(SMART_{NSF}\), achieved the best overall performance on the US market. The ablated version (i) did not have access to the UK data and the failures of the models; and (ii) operationalized each covariate using the LLM alone (refer to Sec. B.2 for a discussion on operationalizations with different SMART versions). This provides evidence that, in the presence of covariate shift, using inductive knowledge or domain expertise might be more useful to finding meaningful model failures.

#### c.4.5 Discovery of societally important groups and failure understanding (Sec. 5.5.)

As discussed in the main paper, SMART provides both possible hypotheses and justifications for model failures which can be evaluated using a simple "fit" method. Furthermore, SMART prioritizes meaningful data slices which are of societal importance. Such slices can be inspected for any data input.

[MISSING_PAGE_FAIL:31]

[MISSING_PAGE_FAIL:32]

### Effects of LLMs

In this section, we provide more experimental details which compare the effectiveness of two GPT models, GPT3.5 and GPT4. **We highlight that the goal is not to exhaustively test the framework with every LLM.** Rather, the goal is to showcase that SMART is feasible with at least the capabilities of GPT-4. We provide this section as a way to measure the sensitivity of the model's performance with lower LLMs but highlight that we _do not_ recommend using it with smaller LLMs, especially LLMs with fewer than 7B parameters.

#### d.3.1 Comparison over identified divergent slices

The following table reproduces the experiment from Sec. D.2 by directly comparing two models - GPT3.5 and GPT4. The setup is the same as in the original experiment.

The table provides a measure of the model's performance on the training dataset from the same environment (\(\mathcal{D}^{\text{UX}}_{\text{train}}\)), the testing dataset from the same environment (\(\mathcal{D}^{\text{UX}}_{\text{test}}\)), and a different deployment environment (\(\mathcal{D}^{\text{US}}_{\text{test}}\)).

**Takeaway**. Both GPT3.5 and GPT4 provide strong increases over benchmark methods with little variability between the two LLMs. One of the possible reasons why is that the hypothesis space of possible model failures is somewhat limited. This can be seen by the similar hypotheses that are generated by both GPT models.

#### d.3.2 Performance across different models

In this section, we vary different tabular machine learning model types and identify how well the ablated and original SMART, identified with GPT3.5 and GPT4, can identify slices with large performance discrepancies.

**SMART with deep learning models.** SMART's targeted sampling of hypotheses, is entirely independent of the downstream model used. i.e. SMART's context-guided slice sampling mechanism is used to generate hypotheses independently of the downstream model.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{Logistic Regression} & \multicolumn{2}{c}{SVM} & \multicolumn{2}{c}{XGBoost} & \multicolumn{2}{c}{Multi-layer Perceptron} \\ \cline{2-9}  & \(|\Delta Acc|\) & p-value & \(|\Delta Acc|\) & p-value & \(|\Delta Acc|\) & p-value & \(|\Delta Acc|\) & p-value \\ \hline SMART\_NSF\_GPT3.5 & 0.23 \(\pm\) 0.03 & 0.00 \(\pm\) 0.00 & 0.23 \(\pm\) 0.03 & 0.00 \(\pm\) 0.00 & 0.12 \(\pm\) 0.06 & 0.14 \(\pm\) 0.27 & 0.23 \(\We extend our analysis with Logistic Regression, SVM, XGBoost, and MLP to further include two tabular deep learning method: TabPFN and TabNet. As shown in Table 14, across all models SMART is the best at finding subgroups where the models are least reliable.

**Takeaway.** GPT4 adds additional value when it comes to identifying slices with performance discrepancies. However, GPT3.5 is still able to find significant performance discrepancies across divergent slices, competitive and in many cases exceeding the performance of other methods (refer to Sec. D.4).

#### d.3.3 Hypothesis generation

A key component which determines the quality of the SMART method is the ability to navigate the search space by generating hypotheses. Therefore, we overview the kinds of hypotheses which are generated and how they differ based on the type of LLM used.

The table below provides an example of the top 10 hypotheses and justifications generated by GPT3.5 and GPT4 for the prostate cancer example in the UK, where the goal is to find specific slices where the model is likely to fail, and propose justifications for them.

**Key takeaway**. Many of the hypotheses for both models are similar. The underlying difference is in the ordering and justification of the hypotheses.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{Classifier} & \multicolumn{8}{c}{Evaluation Method} \\ \cline{2-9}  & Autostat & PSG\_B & PSG\_A & Divexplorer & Slicefinder & Sliceline & SMART \\ \hline TabPFNClassifier & 0.20 \(\pm\) 0.10 & 0.19 \(\pm\) 0.05 & 0.18 \(\pm\) 0.05 & 0.00 \(\pm\) 0.00 & 0.00 \(\pm\) 0.00 & 0.23 \(\pm\) 0.05 & **0.28 \(\pm\) 0.17** \\ TabNet & 0.10 \(\pm\) 0.09 & 0.10 \(\pm\) 0.04 & 0.10 \(\pm\) 0.04 & 0.02 \(\pm\) 0.04 & 0.00 \(\pm\) 0.00 & 0.17 \(\pm\) 0.08 & **0.17 \(\pm\) 0.12** \\ \hline \hline \end{tabular}
\end{table}
Table 14: Identifying slices with the highest performance discrepancies. We show differences in accuracies (\(|\Delta Acc|\)) between the top identified divergent slice and average performance across two state-of-the-art deep learning classifiers (over 5 runs) on the SEER dataset. \(\uparrow\) is better. 0.00 implies the evaluation method does not support the model.

[MISSING_PAGE_FAIL:35]

[MISSING_PAGE_FAIL:36]

\[N_{\text{runs}} \sim\text{Uniform}\{1,499\},\] \[M_{\text{pref}} \sim\text{Uniform}\{1,5\},\] \[A_{\text{rainfall}} \sim\text{Uniform}\{20000,99999\},\] \[F_{\text{color}} \sim\text{Uniform}\{1,6\},\] \[P_{\text{season}} \sim\text{Uniform}\{0,3\},\] \[Y \sim\text{Uniform}\{0,1\}.\]

The second scenario is given by the variables sampled from the following DGPs:

\[N_{\text{runs}} \sim\text{Uniform}\{1,499\},\] \[M_{\text{pref}} \sim\text{Binomial}(1,0.5),\] \[A_{\text{rainfall}} \sim\text{Categorical}(0.1,0.3,0.4,0.2),\] \[F_{\text{color}} \sim\text{Binomial}(1,0.1),\] \[P_{\text{season}} \sim\text{Binomial}(1,0.05),\] \[Y \sim\text{Uniform}\{0,1\}.\]

The third scenario is given by the variables sampled from the following DGPs:

\[N_{\text{runs}} \sim\text{Uniform}\{1,499\},\] \[M_{\text{pref}} \sim\text{Binomial}(1,0.5),\] \[A_{\text{rainfall}} \sim\text{Categorical}(0.1,0.3,0.4,0.2),\] \[A_{\text{music\_hap}} =M_{\text{pref}}\times A_{\text{rainfall}},\] \[A_{\text{run\_hap}} =N_{\text{runs}}\times M_{\text{pref}},\] \[Y \sim\text{Uniform}\{0,1\}.\]

**Analysis.** Table 18 shows the number of slices spuriously discovered, while Table 19 outlines the number of conditions within the slices. We can clearly see the pitfalls of data-only approaches which detect slices which in reality have no relation to one another -- often surfacing few conditions per group which suggests they arise by chance. The rationale for this failure is simply because data-only approaches do not and cannot reason about the features and/or understand context and simply aim to find slices with discrepancies in performance -- which of course could arise by chance. In contrast, we see that SMART by virtue of context-awareness can avoid surfacing groups -- which in reality have no relationships.

### Context aware sensitivity

We provide an additional experiment where we vary the sample size in the training dataset and observe how that affects the number of slices discovered for each method. We show that the SMART is not affected by irrelevant features regardless of the sample size of the training dataset. The result is shown in Figure 9.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Method & \(\mathcal{S}_{\text{uniform}}\) & \(\mathcal{S}_{\text{skewed}}\) & \(\mathcal{S}_{\text{interactions}}\) \\ \hline Autostrat & 1.00 \(\pm\).0 & 1.00 \(\pm\).0 & 1.00 \(\pm\).0 \\ \(PSG\_B\) & 20.00 \(\pm\).0 & 20.00 \(\pm\).0 & 20.00 \(\pm\).0 \\ \(PSG\_A\) & 20.00 \(\pm\).0 & 20.00 \(\pm\).0 & 20.00 \(\pm\).0 \\ divexplorer & 20.00 \(\pm\).0 & 20.00 \(\pm\).0 & 20.00 \(\pm\).0 \\ slicefinder & 20.00 \(\pm\).0 & 20.00 \(\pm\).0 & 20.00 \(\pm\).0 \\ \hline SMART & **0.00 \(\pm\).0** & **0.00 \(\pm\).0** & **0.00 \(\pm\).0** \\ \hline \hline \end{tabular}
\end{table}
Table 18: Number of discovered slices on a synthetic dataset with no prior relationships in three data generating process scenarios. slices capped at most 20. Average of 50 runs \(\pm\) standard deviations is shown.

### Cost of SMART

We assess the cost of LLM hypothesis generation and scalability to larger datasets. Specifically, we demonstrate not only that SMART is cheap but also easily scalable to large datasets

* Scalability: SMART's scalability depends on the number of hypotheses generated, not dataset size (unlike data-only methods). This allows SMART to easily scale to arbitrarily large datasets.
* Cost Analysis: In practical terms, cost then also scales primarily with the number of hypotheses generated, not dataset size. We provide a rough estimate based on token counts of input and outputs for 2 datasets (SEER and OULAD) in Table 20. This would be less than 0.1 USD for 5 hypotheses and less than 0.5 USD for 100 hypotheses for state-of-the-art (USD) for different GPT LLMs and different numbers of hypotheses generated. The cost is estimated based on token counts. Note: GPT-4o models are post our paper and are even cheaper.

### SMART with open-weight models

SMART ideally should be used with the most capable LLM possible.That said, we assess the differences in hypotheses between open-weight models and GPT-4.

We assess Mistral-7b, Qwen-1.5-7b, Llama-3-8b, Llama-70b, where for the OULAD and SEER datasets we generate 5 hypotheses and assess overlap to the hypotheses generated by GPT-4. This is presented in Table 21.

To summarize, the overlap between open-source models and GPT-4 is between 60-80%. We find that open-source models propose similar hypotheses, but they are not replacements for more capable models. This highlights that less capable models might propose similar hypotheses, yet they still catch fewer model failures.

### Understanding the importance of feature names

SMART uses the implicit context encoded in the interpretable feature names as a source of contextual information to guide hypothesis generation. For instance, in a medical dataset, features with names

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model & Cost SER & Cost SER & Cost OULAD & Cost SER \\
5 Hypothesis (USD) & 100 Hypothesis (USD) & 5 Hypothesis (USD) & 100 Hypothesis (USD) \\ \hline GPT+4 & 0.017 & 0.249 & 0.022 & 0.316 \\ GPT-3.5 & 0.004 & 0.050 & 0.005 & 0.064 \\ GPT-4o Mini & 0.0003 & 0.005 & 0.0005 & 0.006 \\ GPT-4o & 0.008 & 0.125 & 0.011 & 0.158 \\ \hline \hline \end{tabular}
\end{table}
Table 20: Models SMART (USD) for different GPT LLMs and different numbers of hypotheses generated. The cost is estimated based on token counts. Note: GPT-4o models are post our paper and are even cheaper.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Method & \(\mathcal{S}_{\text{uniform}}\) & \(\mathcal{S}_{\text{skewed}}\) & \(\mathcal{S}_{\text{interactions}}\) \\ \hline Autostrat & 2.17 \(\pm\) 0.46 & 1.73 \(\pm\) 1.01 & 1.13 \(\pm\) 0.35 \\ \(PSG\_B\) & 1.03 \(\pm\) 0.18 & 1.90 \(\pm\) 0.71 & 1.27 \(\pm\) 0.45 \\ \(PSG\_A\) & 1.03 \(\pm\) 0.18 & 1.90 \(\pm\) 0.71 & 1.27 \(\pm\) 0.45 \\ divexplorer & 2.10 \(\pm\) 0.31 & 2.87 \(\pm\) 0.68 & 2.40 \(\pm\) 0.50 \\ slicefinder & 1.40 \(\pm\) 0.50 & 1.50 \(\pm\) 0.57 & 1.00 \(\pm\) 0.00 \\ \hline SMART & **0.00 \(\pm\) 0.00** & **0.00 \(\pm\) 0.00** & **0.00 \(\pm\) 0.00** \\ \hline \hline \end{tabular}
\end{table}
Table 19: Number of conditions per discovered slice (false positives) in three data generating process scenarios. Average of 50 runs +- standard deviations is shown. Lower is better.

Figure 9: Proportion of irrelevant features (y) for each slice discovery method, based on the sample size. Lower is better.

like age, sex, or patient covariate features provide context to guide LLM hypothesis generation. This contrasts with data-only approaches which only use the numerical data values alone and ignore the context surrounding the feature names.

We aim to assess the sensitivity to interpretable feature names to provide guidance on the use of SMART. First, we perform a qualitative study where we limit the data schema by hiding the feature names (such that they become uninformative) and inspect the hypotheses and justifications generated. We find that in the limited-schema case, SMART generates hypotheses based on inferences about the feature information (e.g. "the model might fail on feature_4 if feature_4 represents gender"). In contrast, informative names guide meaningful hypothesis generation. Such hypotheses and justifications are illustrated in Table 22.

Second, we evaluate whether limiting the data schema by hiding some feature names and leaving minimal external context affects detection rates of model failures. We compare two versions of SMART, original and with corrupted feature labels, in identifying data slices with high performance discrepancies from average (Fig. 10. We find that across two real-world private datasets, hiding the feature names hinders model evaluation. This highlights that feature names play an important role in finding model failures.

These results highlight while SMART does not rely on any additional feature descriptions, feature names play an important role in finding model failures, just as any human requires interpretable feature names to understand the data. That said, feature names (e.g. column labels such as sex, age, race etc) are present in almost all tabular datasets both in the research field and in the real world where data is stored in SQL tables with column names.

\begin{table}
\begin{tabular}{l l c c c c} \hline
**Dataset** & **Factors (GPT Hypotheses)** & **Mistral-7b** & **Llama 3-8b** & **Qwen 1.5-7b** & **Llama 70b** \\ \hline \multirow{4}{*}{**SERT**} & Disability & ✓ & ✓ & ✓ & ✓ \\  & IMD band & ✓ & ✓ & & ✓ \\  & Ago & & ✓ & ✓ & ✓ \\  & Number of previous attempts & ✓ & ✓ & ✓ & \\  & Test (boolean) & & & & \\ \cline{2-6}  & **Owald Overlap Percentage** & 60\% & 70\% & 60\% & 60\% \\ \hline \multirow{4}{*}{**SERT**} & Ago & & ✓ & ✓ & ✓ \\  & Prostate-specific antigen (PSA) & & & & \\  & Comorbidities & ✓ & ✓ & ✓ & ✓ \\  & Treatment (conservative management) & ✓ & ✓ & ✓ & ✓ \\  & Cancer stage & & & ✓ & ✓ \\ \cline{1-1}  & **SEER Overlap Percentage** & 40\% & 60\% & 80\% & 80\% \\ \hline \end{tabular}
\end{table}
Table 21: Comparison hypotheses by GPT-4 and overlap w/ open-weight models

Figure 10: Identifying the importance of feature names as a source of information for context-aware testing. We compare two versions of SMART on CUTRACT and SEER datasets. The first version, “base”, is the normal SMART method. The second version, “corrupt”, has feature names changed to uninformative ones. The goal is to identify slices with the highest performance discrepancies between the average & subgroup prediction, calculated as the difference in their respective accuracies (y-axis). We see showing feature names enables to find subgroups that have a higher performance discrepancy from baseline performance than without feature names. \(\uparrow\) is better.

[MISSING_PAGE_FAIL:40]

Recommendations:

1. The model appears to be less reliable for older patients (\(age>75\)), patients with lower levels of prostate-specific antigen (\(<10\)), and those who have undergone conservative management treatment. It also shows decreased performance for patients with more than two comorbidities and those with a Gleason score of 4. However, the model is more reliable when these conditions are not met.

2. Before deploying the model, the end user should be aware of the following: - The model's performance may be compromised for older patients and those with multiple comorbidities. Consider additional validation or alternative models for these groups. - Patients with lower prostate-specific antigen levels and those who have undergone conservative management treatment may also experience less accurate predictions. Additional clinical insights may be needed for these cases. - Although the model shows decreased performance for patients with a Gleason score of 4, this group is relatively small, so the impact on overall model performance may be limited. However, caution should be exercised when interpreting results for these patients.

Remember, these recommendations are based on the training and test datasets from the UK. If deploying in a different geographical context, consider revalidating the model with local data.

--

Definitions of the metrics:

* **Group Size**: \[\text{group\_size}=|\text{slice}|\]
* **Support**: \[\text{support}=\frac{|\text{slice}|}{|\text{dataset}|}\]
* **Number of Criteria**: \[\text{num\_criteria}=\text{Count}(\text{"and"})+1\]
* **Outcome Difference**: \[\text{outcome\_diff}=|\text{avg\_outcome\_dataset}-\text{avg\_outcome\_slice}|\]
* **Accuracy Difference**: \[\text{accuracy\_diff}=|\text{accuracy\_dataset}-\text{accuracy\_slice}|\]
* **Odds Ratio (Outcome)**: \[\text{odds\_ratio\_outcome}=\frac{p_{1}(1-p_{1})}{p_{0}(1-p_{0})}\]
* **Odds Ratio (Accuracy)**: \[\text{odds\_ratio\_acc}=\frac{p_{1}(1-p_{1})}{p_{0}(1-p_{0})}\] _(where \(p_{1}\) and \(p_{0}\) are accuracies in the slice and the rest of the dataset, respectively)_* **Lift (Outcome)**: \[\text{lift\_outcome}=\frac{p_{1}}{p}\]
* **Lift (Accuracy)**: \[\text{lift\_acc}=\frac{p_{1}}{p}\] _(where \(p_{1}\) is accuracy in the slice and \(p\) is accuracy in the entire dataset)_
* **Weighted Relative Outcome**: \[\text{weighted\_relative\_outcome}=\text{support}\times\text{diff\_outcomes}\]
* **Weighted Relative Accuracy**: \[\text{weighted\_relative\_accuracy}=\text{support}\times\text{diff\_accuracy}\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract accurately reflects the claims made in the paper. Our paper introduces the new paradigm of context-aware testing which we discuss in Sec. 3. We introduce our context-aware testing instantiation SMART in Sec. 4. We experimentally show how under a variety of different experimental conditions SMART outperforms data-only benchmarks in Sec. 5. We therefore believe the provided evidence fully supports the claims that context-aware testing is a new alternative to testing ML models relative to data-only methods. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our work in Sec. 6. We also provide ways of addressing these limitations in Sec. 5.4 with an extended discussion on possible biases, mitigation strategies, and best practices, including how SMART provides unique opportunities to address such challenges with features such as model reports or transparent hypothesis testing and operationalization. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.

* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The paper makes a claim that context can be used to guide the search for relevant and meaningful model failures via a context-guided slice sampling mechanism which uses context to prioritize likely slices. This is defined and discussed in Sec. 3.3. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can appear in either the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper provides the key experimental setups in the main experimental section (Sec. 5) together with additional experimental details in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. *3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide details about the algorithms and data in Appendix C, Sec. B and Sec. D. Code can be found at: [https://github.com/pauliusrauba/SMART_Testing](https://github.com/pauliusrauba/SMART_Testing) or [https://github.com/vanderschaarlab/SMART_Testing](https://github.com/vanderschaarlab/SMART_Testing) Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the details on the experiments are either provided within Section 5, with full details provided in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?Answer: [Yes]

Justification: Error bars (standard deviation) are included as relevant over multiple seeds for the experiments in Section 5 and additional experiments in Appendix D.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All the compute details on the experiments are provided in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We have read the code of ethics do not violate any of the dimensions. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts**Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Broader impacts of testing and implications are outlined in Sec 1 and 6. We discuss potential impacts and mitigations in Sec 5.4 and Sec 6. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Not applicable -- our paper presents a new method for reliable and safe ML model testing. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Appendix C provides details and citations for all assets (data and baselines) used in the paper.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not produce new assets such as datasets, but uses existing benchmarks/datasets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not have crowdsourcing experiments or research with humans. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA]

Justification: We do not have crowdsourcing experiments or research with humans that would need an IRB.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.