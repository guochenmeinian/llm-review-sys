# Lie Point Symmetry and Physics Informed Networks

Tara Akhound-Sadegh

School of Computer Science, McGill University,

Mila - Quebec Artificial Intelligence Institute,

Montreal, Quebec, Canada

Laurence Perreault-Levasseur

Universite de Montreal, Montreal, Quebec, Canada

Ciala Institute, Montreal, Quebec, Canada

Mila - Quebec Artificial Intelligence Institute, Montreal, Quebec, Canada

Trottier Space Institute, Montreal, Quebec, Canada

CCA, Flatiron Institute, New York, USA

Perimeter Institute, Waterloo, Ontario, Canada

Johannes Brandstetter

Microsoft Research AI4Science,

Amsterdam, Netherlands

Max Welling

University of Amsterdam, \({}^{\dagger}\)

Amsterdam, Netherlands

&Siamak Ravanbakhsh

School of Computer Science, McGill University,

Mila - Quebec Artificial Intelligence Institute,

Montreal, Quebec, Canada

###### Abstract

Symmetries have been leveraged to improve the generalization of neural networks through different mechanisms from data augmentation to equivariant architectures. However, despite their potential, their integration into neural solvers for partial differential equations (PDEs) remains largely unexplored. We explore the integration of PDE symmetries, known as Lie point symmetries, in a major family of neural solvers known as physics-informed neural networks (PINNs). We propose a loss function that informs the network about Lie point symmetries in the same way that PINN models try to enforce the underlying PDE through a loss function. Intuitively, our symmetry loss ensures that the infinitesimal generators of the Lie group conserve the PDE solutions. Effectively, this means that once the network learns a solution, it also learns the neighbouring solutions generated by Lie point symmetries. Empirical evaluations indicate that the inductive bias introduced by the Lie point symmetries of the PDEs greatly boosts the sample efficiency of PINNs.

## 1 Introduction

In recent years, deep learning has accelerated data-driven approaches to science and engineering. A prominent example is the role of deep learning in solving partial differential equations (PDEs), which are ubiquitous in many scientific disciplines.

This is mainly driven by the fact that traditional handcrafted numerical solvers can be prohibitively expensive, and thus, learning to solve PDEs has the potential to significantly impact various areas of science, ranging from quantum chemistry to biology to climate science to cosmology, (Wang et al., 2020; Kochkov et al., 2021; Kashinath et al., 2021; Gupta and Brandstetter, 2022; Nguyen et al., 2023; Bi et al., 2022). Like any other machine learning problem, learning to solve PDEs can benefit from inductive biases to boost sample efficiency and generalization capabilities. As such, the PDE itself and its respective Lie point symmetries, i.e. joint symmetries of coordinate and field transformations, are two natural inductive biases for constructing efficient neural PDE solvers.

For parametric PDEs, Physics-Informed Neural Networks (PINNs) have emerged as an efficient new learning paradigm. PINNs constrain the solution network to satisfy the underlying PDE via the PINN loss terms (Raissi et al., 2019), which comprise a residual loss, as well as loss terms for initial and boundary points. As such, PINNs can be seen as a data-free learning approach, utilizing available physics information to guide neural network training. Lie point symmetries are uniquely special in that it is possible to characterize the full set of permissible transformations for each PDE, and thus naturally offer additional physics information which piggy-backs the data-free learning objective. Lie point symmetries have been introduced to deep learning by Brandstetter et al. (2022) for Lie point symmetry data augmentation which places a firm mathematical footing on data augmentation pipelines for neural PDE solvers. Further, Lie point symmetries showed promising results when used in extending the framework to self-supervised learning for neural PDE solvers (Mialon et al., 2023). In this work, for the first time we integrate symmetries into PINNs and demonstrate the effectiveness of this symmetry regularization on generalization capabilities.

Lie point symmetries of a PDE, by definition, map a solution to a solution, preserving the PDE. Lie point symmetry transformations act on both independent (_e.g._, time and space), and dependent (PDE solution) fields and, by extension, on all partial derivatives. To enforce a Lie point symmetry, we first need to _prolong_ a symmetry transformation to find how it transforms the partial derivatives of the respective PDE. We show how to do this using automatic differentiation, which enables us to transform the entire PDE for each infinitesimal generator of each Lie point symmetry. "Imposing a symmetry" means that by transforming the PDE via a symmetry transformation, the PDE is required to be preserved. In practical terms, Lie point symmetries should be orthogonal to the PDE gradient. Each Lie Point symmetry results in one such orthogonality constraint, which we enforce using a penalty term within the PINN framework. We can enforce these on the same points where the PINN loss is imposed. Importantly, our Lie Point symmetry regularization is complementary to the PINN loss: while the PDE loss regularizes the neural network to satisfy the PDE at select points, the Lie point symmetry loss regularizes the neural network such that infinitesimal changes in different directions continue to satisfy the underlying PDE.

## 2 Background

### Partial Differential Equations

Partial differential equations (PDEs) are used to mathematically describe the dynamics of various physical systems. In a PDE, the evolution of a function that involves several variables is described in terms of local updates expressed by partial derivatives. Since temporal PDEs are especially prevalent

Figure 1: All the solutions of the differential equation of a harmonic oscillator can be reached via symmetry transformations of a single solution. Its Lie-point symmetry group is the special linear group, \(SL(3)\), corresponding to eight one-parameter subgroups transforming a given solution, identified using a different colour in this figure. While, in general, Lie-point symmetries form more than one orbit – that is, we cannot reach all solutions using such transformations – they can significantly narrow down the solution space.

in physical sciences, we consider PDEs of the following general form:

\[\Delta=\mathbf{u}_{t}+\mathcal{D}_{\mathbf{x}}[\mathbf{u}] =0, t\in[0,T],\mathbf{x}\in\Omega \tag{1}\] \[\mathbf{u}(0,\mathbf{x}) =f(\mathbf{x}), \mathbf{x}\in\Omega\] \[\mathbf{u}(t,\mathbf{x}) =g(t,\mathbf{x}), \mathbf{x}\in\partial\Omega,t\in[0,T]\]

where \(\mathbf{u}(t,\mathbf{x})\in\mathbb{R}^{D_{u}}\) is the solution to the PDE, \(t\) denotes the time, and \(\mathbf{x}\) is a vector of possibly multiple spatial coordinates. \(\Omega\subset\mathbb{R}^{D}\) is the domain and \(\mathcal{D}_{\mathbf{x}}[\cdot]\) is a non-linear _differential operator_. \(f(\mathbf{x})\) is known as the initial condition function and \(g(t,\mathbf{x})\) describes the boundary conditions. In the following, we sometimes explicitly separate the time from other independent variables and sometimes for convenience group them together.

**Example 1** (Heat Equation).: _The one-dimensional heat equation describes the heat conduction in a one-dimensional rod, with viscosity coefficient \(\nu\). Its differential operator is given by \(\mathcal{D}_{x}[u]=-\nu u_{xx}\)._

Using Neural Networks for Solving PDEs.For many systems, obtaining an analytical solution to the PDE is impossible; hence, numerical methods are traditionally used to obtain approximate solutions. Numerical solvers, such as finite element methods (FEM) or finite difference methods (FDM) rely on discretizing the space \(T\times\Omega\)(Quarteroni, 2009). The topology of the space has to be taken into account when constructing the mesh, and the resolution of the discretization will affect the accuracy of the predicted solutions. Additionally, these solvers are often computationally expensive, especially for complex dynamics, and each time the initial or boundary conditions of the PDE change the solver must be rerun. These considerations and constraints make designing numerical solvers difficult, and scientists often need to handcraft a specific solver for an application (Quarteroni, 2009).

Given the recent successes of neural networks, especially in dealing with large datasets, using deep learning to solve PDEs has become a promising direction. The idea is to learn a function or, more generally, a functional that bypasses the numerical computation and produces the solution to the PDE in a single shot or by progression through time. Broadly, there are two approaches to solving PDEs with neural networks: neural operator methods which approximate the solution operator that maps between solutions of the underlying PDEs, and direct methods which learn the underlying solution function. The most prominent representative of the latter are PINNs.

PINNs.In contrast to neural operator methods (Lu et al., 2021; Li et al., 2021) where the main idea is to generalize neural networks to obtain mappings between function space, PINNs directly learn the solution function of the underlying PDE. Consequently instead of relying on large training sets,

PINNs operate as a surrogate model for the PDE solution, trained directly with the PDE equation itself. The simplicity of the PINN idea has made it the subject of many follow-up improvements; see Krishnapriyan et al. (2021); Wang et al. (2020, 2022, 2023)

In PINNs, the PDE solution \(u(t,\mathbf{x})\) of Eq. (1) is a neural network \(u_{\theta}(t,\mathbf{x})\) with parameters \(\theta\). The loss function is then compromised of two parts:

\[\mathcal{L}(\theta)=\mathcal{L}_{\mathrm{PDE}}+\mathcal{L}_{\mathrm{data-fit}} \tag{2}\]

The first term is the _physics-informed_ objective, ensuring that the function learned by the neural network satisfies the PDE Eq. (1). Let \(r(t,\mathbf{x})\) denote the residual error in agreement with the PDE equation:

\[r_{\theta}(t,\mathbf{x})=\frac{\partial}{\partial t}u_{\theta}(t,\mathbf{x})+ \mathcal{D}_{\mathbf{x}}[u_{\theta}(t,\mathbf{x})]\]

where the derivatives of the solution network \(u_{\theta}\) are calculated using automatic differentiation (Raissi et al., 2019). This penalty is then imposed on a finite set of points \((t,\mathbf{x})_{1:N_{r}}\) which are sampled from inside the domain \([0,T]\times\Omega\) to obtain the _PDE loss_:

\[\mathcal{L}_{\mathrm{PDE}}=\frac{1}{N_{r}}\sum_{i=1}^{N_{r}}\|r_{\theta}(t_{i },\mathbf{x}_{i})\|_{2}^{2} \tag{3}\]

The second term in Eq. (2), is a supervised loss which ensures that the function learned by the neural network satisfies the initial and boundary conditions of the problem - that is

\[\mathcal{L}_{\mathrm{data-fit}}=\frac{1}{N_{s}}\sum_{i=1}^{N_{s}}\|u_{\theta}( 0,\mathbf{x}_{i}^{0})-f(x_{i}^{0})\|_{2}^{2}+\frac{1}{N_{b}}\sum_{i=1}^{N_{b}} \|u_{\theta}(t_{i}^{b},\mathbf{x}_{i}^{b})-g(t_{i}^{b},x_{i}^{b})\|_{2}^{2} \tag{4}\]where \((\mathbf{x}^{0})_{1:N_{s}}\in\Omega\) are \(N_{s}\) samples at which the initial condition function \(f\) is sampled. \((t^{b},\mathbf{x}^{b})_{1:N_{b}}\) are \(N_{b}\) points sampled on the boundary (from \([0,T]\times\partial\Omega\)).

There have also been multiple models that combine the operator learning approach with the physics-informed loss. For example, in Li et al. (2021), the NO approach is combined with the PINN loss. Another example is Wang et al. (2021), which we describe further in Section 3.1.

### Symmetries

Groups.Symmetries are essentially transformations of the object that leave an aspect of it invariant: for example, the nature of an object does not change if we translate or rotate it. In mathematics, symmetries of an object are described by abstract objects known as _groups_. More concretely, a group is a set \(G\) and a group operation \(\cdot:G\times G\to G\) satisfying: associativity, the existence of an identity element, and the presence of an inverse element for every element in the set.

Lie Groups.Groups that are also differentiable manifolds are known as _Lie groups_, and are important in the study of continuous symmetries. In Lie groups, in addition to satisfying the three properties, the group operation \(\cdot\) and its inverse are smooth maps. Each Lie group has an associated _Lie algebra_ which is its tangent space, as a vector space, at identity. 3 Intuitively, Lie algebras describe the smooth transformations of the Lie groups in the limit of infinitesimal transformations. The elements of the Lie algebra are vectors describing the direction of the infinitesimal symmetry transformation.

Footnote 3: Formally, the Lie algebra is a vector space equipped with a binary operation known as the Lie bracket.

One Parameter Subgroups.The Lie group, \(\mathcal{G}\), can be multi-dimensional and complex. When the \(n\)-dimensional group \(\mathcal{G}\) is _simply connected_, it is often represented in terms of a series of \(n\)_one-parameter transformations_, \(g=g_{1}(\epsilon_{1})g_{2}(\epsilon_{2})\dots g_{n}(\epsilon_{n})\), where \(g_{i}:\mathbb{R}\to\mathcal{G}\), \(i=1,\dots,n\) and such that \(g_{i}(\epsilon)g_{i}(\delta)=g_{i}(\epsilon+\delta)\). The \(\epsilon\)'s are the real parameters of the transformation, and each \(g_{i}\) is a continuous group homomorphism (_i.e._, a smooth, group-structured map) from this parameter to the symmetry group.

### Symmetries of Partial Differential Equations

In this section, we will provide the mathematical background of symmetries of differential equations. We will mostly follow the exposition presented in Olver (1986) and refer the reader to this original text for a more in-depth treatment of this topic.

In the context of differential equations, symmetries are transformations that map a solution of the PDE to another solution. For example, in Fig. 1, we can see how the solutions of a simple harmonic oscillator can be obtained via symmetry transformations of a given solution, where the symmetry group is \(SL(3)\). Consider the PDE \(\Delta\), involving \(p\) independent variables \(\mathbf{x}=(x_{1},\dots,x_{p})\in X\) and \(q\) dependent variables \(\mathbf{u}=(u_{1},\dots,u_{q})\in U\). The solutions to the PDE will be of the form \(u_{i}=f_{i}(x_{1},\dots,x_{p})\) for \(i=1,\dots,q\), for \(\mathbf{x}\in\Omega\), where \(\Omega\subset X\) is the domain of the function \(f\). The symmetry group \(\mathcal{G}\), of \(\Delta\), is the local group of transformations on an open subset of the space of dependent and independent variables, \(M\subset X\times U\), transforming solutions of \(\Delta\) to other solutions.

Prolongations.To formalize this abstract definition of PDE symmetries, Lie proposed viewing \(\Delta\) as a concrete geometric object and introduced the concept of _prolongation_(Olver, 1986). The idea is to _prolong_ the space of independent and dependent variables, \(X\times U\), to a space that also represents the partial derivatives involved in the PDE. More concretely, we have the following definition:

**Definition 1**.: _The **n-th order prolongation** (or n-th order jet space) of \(X\times U\) is denoted as \(X\times U^{(n)}=X\times U_{1}\times\dots\times U_{n}\), whose coordinates represent the independent and dependent variables as well as all the partial derivatives of the dependent variables up to order n._

Equivalently we have the notion of _prolongation_ of \(\mathbf{u}\) as \(\mathbf{u}^{(n)}=(\mathbf{u}_{\mathbf{x}},\mathbf{u}_{\mathbf{xx}},\dots, \mathbf{u}_{n\mathbf{x}})\) where \(\mathbf{u}_{i\mathbf{x}}\) is all the unique \(i^{\mathrm{th}}\) derivatives of \(u\), for \(i=1,\dots,n\). For example, if \(\mathbf{x}=(x,y)\), then \(\mathbf{u}_{\mathbf{xx}}=(\partial_{xx}\mathbf{u},\partial_{xy}\mathbf{u}, \partial_{yy}\mathbf{u})\).

Using this notion of prolongation, we can represent a PDE as an algebraic equation, \(\Delta(\mathbf{x},\mathbf{u}^{(n)})=0\), where \(\Delta\) is the map that determines the PDE, _i.e._, \(\Delta:X\times U^{(n)}\to\mathbb{R}\). In other words, the PDE tells us where the map \(\Delta\) vanishes on \(X\times U^{(n)}\).

For example, for the heat equation described in 1, \(\Delta\) is given by:

\[\Delta((x,t),\mathbf{u}^{(2)})=u_{t}-\nu u_{xx}\;, \tag{5}\]

The graph of all prolonged solutions is the set \(\mathcal{S}_{\Delta}\subset X\times U^{(n)}\), and is defined as \(\mathcal{S}_{\Delta}=\{(\mathbf{x},\mathbf{u}^{(n)}):\Delta(\mathbf{x}, \mathbf{u}^{(n)})=0\}\). In this new notation, we can say that \(\mathbf{u}(\mathbf{x})\) is a solution of the PDE if \(\Gamma_{u}^{(n)}=\{(\mathbf{x},\mathrm{pr}^{(n)}\mathbf{u}(\mathbf{x}))\} \subset\mathcal{S}_{\Delta}\). where \(\mathrm{pr}^{(n)}\mathbf{u}(\mathbf{x}):X\to U^{n}\), is a vector-valued function whose entries represent all derivatives of \(\mathbf{u}\) wrt \(\mathbf{x}\) up to order \(n\).

Prolongations of the Infinitesimal Generators.Let \(\mathbf{v}\) be the vector field on the subspace \(M\subset X\times U\) with corresponding one-parameter subgroup \(\exp{(e\mathbf{v})}\). In other words, the vector field \(\mathbf{v}\) is the _infinitesimal generator_ of the one-parameter subgroup. Intuitively, this vector field describes the infinitesimal transformations of the group to the independent and dependent variables, and we can write it as:

\[\mathbf{v}=\sum_{i=1}^{p}\xi_{i}(\mathbf{x},\mathbf{u})\frac{\partial}{ \partial x^{i}}+\sum_{\alpha=1}^{q}\phi_{\alpha}(\mathbf{x},\mathbf{u})\frac{ \partial}{\partial u^{\alpha}}\;, \tag{6}\]

where \(\xi^{i}(\mathbf{x},\mathbf{u})\) and \(\phi_{\alpha}(\mathbf{x},\mathbf{u})\) are coordinate-dependent coefficients. To study how symmetries transform a solution to another solution, we need to know how they transform the partial derivatives and, therefore, the _jet space_\(X\times U^{(n)}\).

A symmetry transformation of the independent (\(\mathbf{x}\)) and dependent (\(\mathbf{u}\)) variables will also induce transformations in the partial derivatives \(\mathbf{u}_{\mathbf{x}},\mathbf{u}_{\mathbf{x}\mathbf{x}},\ldots\). The _prolongation of the infinitesimal generator_, \(\mathrm{pr}^{(n)}\mathbf{v}\) is a generalization of the generator \(\mathbf{v}\) which describes these induced transformations in these partial derivatives.

This prolongation will be defined on the jet-space \(X\times U^{(n)}\) and it is given by:

\[\mathrm{pr}^{(n)}\mathbf{v}=\sum_{i=1}^{p}\xi_{i}(\mathbf{x},\mathbf{u})\frac {\partial}{\partial x^{i}}+\sum_{\alpha=1}^{q}\sum_{J}\phi_{\alpha}^{(J)}( \mathbf{x},\mathbf{u})\frac{\partial}{\partial u_{J}^{\alpha}}\;, \tag{7}\]

where we have used the notation \(J=(i_{1},\ldots,i_{k})\) for the multi-indices, with \(0\leq i_{k}\leq p\) and \(0\leq k\leq n\) and \(\mathbf{u}_{J}^{\alpha}=\frac{\partial^{k}\mathbf{u}^{\alpha}}{\partial x^{i_ {1}}\cdots\partial x^{i_{k}}}\). Calculating \(\phi_{\alpha}^{(J)}\), the coefficients of \(\partial_{\mathbf{u}_{\mathbf{x}}^{\alpha}}\), can be done using the prolongation formula, which involves the total derivative operator \(D\) (see Olver [1986] for a derivation):

\[\phi_{\alpha}^{(J)}=D_{J}Q_{\alpha}+\sum_{i=1}^{p}\xi_{i}\frac{\partial\mathbf{ u}_{J}^{\alpha}}{\partial x^{i}}\qquad\text{where}\qquad Q_{\alpha}=\phi_{ \alpha}-\sum_{i=1}^{p}\xi_{i}\frac{\partial\mathbf{u}^{\alpha}}{\partial x^{i }}\;. \tag{8}\]

The upshot is that we can mechanically calculate the prolonged vector field using partial derivatives of \(\mathbf{u}\), which are, in turn, produced by automatic differentiation. In practice, prolonged vector fields are implemented as vector-valued functions (or functionals) of \(\mathbf{x}\) and \(\mathbf{u}\). Therefore, the implementation of this mechanical process is generic and can be applied to any PDE. See the Appendix A for examples.

Lie Point Symmetries of PDEs.We can now define the _prolongation of the action of \(\mathcal{G}\)_:

**Definition 2**.: _For symmetry group \(\mathcal{G}\) acting on \(M\), the **prolongation of action of \(\mathcal{G}\)** on the open subset \(M\subset X\times U\) is the induced action on \(M^{(n)}=M\times U^{(n)}\) which transforms derivatives of a solution, \(\mathbf{u}=f(\mathbf{x})\), into corresponding derivatives of another solution, \(\mathbf{u}^{\prime}=f^{\prime}(\mathbf{x}^{\prime})\). We can write this as:_

\[\mathrm{pr}^{(n)}g\cdot(\mathbf{x},\mathbf{u}^{(n)})=(g\cdot\mathbf{x}, \mathrm{pr}^{(n)}g\cdot\mathbf{u}^{(n)})\;.\]

Using the definition above, we can 

**Theorem 2.1**.: \(\mathcal{G}\) _is the symmetry group of the \(n\)-th order PDE \(\Delta(\mathbf{x},\mathbf{u}^{n})\), if \(\mathcal{G}\) acts on \(M\), and its prolongation leaves the solution set \(\mathcal{S}_{\Delta}\) invariant: \(\mathrm{pr}^{(n)}g\cdot(\mathbf{x},\mathbf{u}^{(n)})\in\mathcal{S}_{\Delta}, \quad\forall g\in\mathcal{G}\)._

Finally, we can express the symmetry condition in terms of the infinitesimal generators \(\mathbf{v}\) of \(\mathcal{G}\):

**Theorem 2.2** (Infinitesimal Criterion).: \(\mathcal{G}\) _is a symmetry group of the PDE \(\Delta(\mathbf{x},\mathbf{u}^{n})\) if for every infinitesimal generator \(\mathbf{v}\) of \(\mathcal{G}\), we have that_

\[\mathrm{pr}^{(n)}\mathbf{v}[\Delta]=0\quad\mathrm{when}\quad\Delta=0\;.\]

**Example 2** (A Symmetry of the Heat Equation).: _As an illustrative example, we can consider the heat Eq. (5) and will show that the following vector field generates a symmetry group for this PDE: \(\mathbf{v}=2\nu t\partial_{x}-xu\partial_{u}\). We need to find the first prolongation \(\phi^{(t)}\) and the second prolongation \(\phi^{(xx)}\), where \(\phi=-xu\). Using the prolongation formula given in Eq. (7), we get:_

\[\phi^{(t)}=-xu_{t}-2\nu u_{x}\qquad\text{and}\qquad\phi^{(xx)}=-2u_{x}-xu_{xx}\]

_Now: \(\mathrm{pr}^{(2)}\mathbf{v}[\Delta]=\phi^{(t)}-\nu\phi^{(xx)}=xu_{t}+2\nu u_{ x}-\nu\big{(}2\nu u_{x}-x\nu u_{xx}\big{)}=x(u_{t}-\nu u_{xx})\;.\)_

_Clearly \(\mathrm{pr}^{(2)}\mathbf{v}[\Delta]=0\) when \(\Delta=0\), hence \(\mathbf{v}\) is a symmetry of the heat equation._

## 3 Methods

### Solving PDEs with Different Initial/Boundary Conditions with PINNs

Wang et al. (2021) combines the approach introduced in Lu et al. (2021) with the PINN loss to solve PDEs with different initial or boundary conditions without requiring retraining of the model as the original PINN model does. We will also use this proposed framework to examine the effect of enforcing the symmetry condition of the PDE on the model.

Recall that we want to model the operator \(\mathcal{O}:\mathcal{A}\rightarrow\mathcal{U}\), where \(\mathcal{A}\) is the space of initial condition functions and \(\mathcal{U}\) is the space of PDE solutions. Our model consists of two neural networks: \(e_{\theta_{1}}\) embeds the initial condition function, and \(g_{\theta_{2}}\) embeds the independent variables, \([t,\mathbf{x}]\in\mathbb{R}^{p}\).

In particular, to embed the initial condition function \(f(\mathbf{x})=\mathbf{u}(0,\mathbf{x})\in\mathbb{R}^{p}\), it is sampled at fixed points \(\{\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\}\), and the concatenated values are fed to \(f_{\theta_{1}}\)

The final prediction is the inner product of these embedding vectors:

\[\mathcal{O}_{\theta}(f)(\mathbf{x},t)=e_{\theta_{1}}^{\top}\big{(}\mathbf{u}(0,\mathbf{x}_{1}),\ldots,\mathbf{u}(0,\mathbf{x}_{n})\big{)}\;g_{\theta_{2}}( \mathbf{x},t)\;, \tag{9}\]

where \(\theta=(\theta_{1},\theta_{2})\). In Algorithm 1, we use the notation \(u_{\theta}\) to denote the operator \(\mathcal{O}_{\theta}\) and use \((\mathbf{x}l,\mathbf{u}^{\prime})_{1:N_{l}}\) to include both boundary and initial condition samples.

While we acknowledge recent architectural improvements to DeepONets (Krishnapriyan et al., 2021), since our goal is to showcase the effectiveness of symmetries, we deploy MLPs for both networks.

### Imposing the Symmetry Criterion

To further inform PINNs about the symmetries of the PDE, we use an additional loss term \(\mathcal{L}_{\mathrm{sym}}\). Conveniently, this loss sometimes also contains the PDE loss of Eq. (2).

Recall that the infinitesimal criterion of Theorem 2.2 requires that by acting on a solution \((\mathbf{x},\mathbf{u}^{n})\) in the jet space using the prolonged infinitesimal generator \(\mathrm{pr}^{(n)}\mathbf{v}\), the PDE equation should remain satisfied. In simple terms, our symmetry loss encourages the orthogonality of \(\mathrm{pr}^{(n)}\mathbf{v}\) and the gradient of \(\Delta\) - in other words, infinitesimal symmetry transformations are encouraged to fall on the level-sets of \(\Delta\), maintaining \(\Delta=0\). Next, we elaborate on this procedure.

Assume that the Lie algebra of the symmetry group of the \(n\)-th order PDE, \(\Delta\), is spanned by \(K\) independent vector fields, \(\{\mathbf{v}_{1},\ldots,\mathbf{v}_{K}\}\), where each \(\mathbf{v}_{k}\) is defined as in Eq. (6). As noted earlier, for each \(\mathbf{v}_{k}\), we can obtain their prolongations using automatic differentiation and create a vector of the corresponding coefficients:

\[\mathrm{coef}\big{(}\mathrm{pr}^{(n)}\mathbf{v}_{k}\big{)}=\left[\xi_{0}^{k}, \ldots\xi_{p}^{k},\phi_{0}^{k},\ldots,\phi_{q}^{k},(\phi_{0}^{x_{0}})^{k}, \ldots,(\phi_{q}^{x_{1}^{1},\ldots x^{p}})^{k}\right]\,. \tag{10}\]

This is a vector of infinitesimal transformations in the jet space.5We also use the notation \(J_{\Delta}\) for the gradient of \(\Delta\) wrt all independent and dependent variables: \(J_{\Delta}=\big{(}\frac{\partial\Delta}{\partial x^{i}},\frac{\partial\Delta}{ \partial u_{\beta}^{2}}\big{)}\).

The symmetry loss encourages the orthogonality of each of \(K\) prolonged vector fields and the gradient vector above on points inside the domain:

\[\mathcal{L}_{\mathrm{sym}}=\sum_{k=1}^{K}(J_{\Delta}^{\top}\mathrm{coef}( \mathrm{pr}^{(n)}\mathbf{v}_{k}))^{2}\;. \tag{11}\]

An alternative is to minimize the absolute value of cosine similarity. We found both of these to work well in practice.

Therefore, the total loss we use to train the two networks consists of the PINN loss introduced in Eq. (2) and the symmetry loss: \(\mathcal{L}=\alpha\mathcal{L}_{\mathrm{PDE}}+\beta\mathcal{L}_{\mathrm{data- fit}}+\gamma\mathcal{L}_{\mathrm{sym}}\), where \(\alpha\), \(\beta\) and \(\gamma\) are hyperparameters. However, as we see through examples, one or more symmetries of a PDE often simplify to a constant times the \(\mathcal{L}_{PDE}\), removing the need to separate treatment of the PDE loss. Algorithm 1 summarizes our training algorithm.

```
inputs: \(\Delta\): the PDE equation of order \(n\), \((\mathbf{v}_{k})_{1:K}\) : infinitesimal generators of symmetries of \(\Delta\) \(\mathcal{D}=\big{\{}\big{(}\mathbf{x}_{1:N_{r}},(\mathbf{x},\mathbf{u})_{1:N_ {l}},\mathbf{u}_{1:N_{s}}\big{)}\big{\}}_{1:N_{f}}\): dataset for \(N_{f}\) different initial conditions init: initialize parameters \(\theta\) of network \(u_{\theta}\) for iteration do  Sample from \(\mathcal{D}\) calculate \(\mathcal{L}_{\mathrm{sym}}\):  calculate \(\mathbf{u}^{(n)}\) using automatic differentiation for \(\mathbf{x}_{1:N_{r}}\)  calculate \(\mathrm{coef}(\mathrm{pr}^{(n)}\mathbf{v}_{k}),\) using \(\mathbf{x}_{1:N_{r}},(\mathbf{u}^{(n))_{1:N_{r}}},\) auto diff and Eq. (7)  calculate \(\mathcal{L}_{\mathrm{sym}}\) using \(\mathrm{coef}(\mathrm{pr}^{(n)}\mathbf{v}_{k})\) and \(\Delta\) and equation Eq. (11) calculate \(\mathcal{L}_{\mathrm{PDE}}\):  using \(\Delta,(\mathbf{u}^{(n)})_{1:N_{r}}\) and equation Eq. (3): calculate \(\mathcal{L}_{\mathrm{data-fit}}\):  calculate \(\tilde{\mathbf{u}}=\mathbf{u}_{\theta}(\mathbf{x},\mathbf{u}_{1:N_{s}})\) for \(\mathbf{x}_{1:N_{l}}\)  calculate \(\mathcal{L}_{\mathrm{data-fit}}\) using \(\mathbf{u}_{1:N_{l}}\), \(\tilde{\mathbf{u}}_{1:N_{l}}\) and equation Eq. (4) \(\mathcal{L}=\alpha\mathcal{L}_{\mathrm{PDE}}+\beta\mathcal{L}_{\mathrm{data- fit}}+\gamma\mathcal{L}_{\mathrm{sym}}\) \(\theta\leftarrow\theta-\nabla_{\theta}\mathcal{L}\) endfor return\(u_{\theta}\)
```

**Algorithm 1** PINN with Lie Point Symmetry

## 4 Experiments

### Heat Equation

First, we study the effectiveness of imposing the symmetry constraint on the heat equation, described in Eq. (5). This equation is a simple linear PDE with a rich symmetry group.

**Symmetries.** The following \(6\)-dimensional lie algebra spans the symmetry group of the heat equation:

\[\mathbf{v}_{1}=\partial_{x}\quad\mathbf{v}_{3}=\partial_{u} \mathbf{v}_{5}=2\nu t\partial_{x}-xu\partial_{u} \tag{12}\] \[\mathbf{v}_{2}=\partial_{t}\quad\mathbf{v}_{4}=x\partial_{x}+2t \partial_{t}\quad\mathbf{v}_{6}=4\nu tx\partial_{x}-4\nu t^{2}\partial_{t}-( x^{2}+2\nu t)u\partial_{u}\;.\]

For example, the infinitesimal generator \(v_{1}\) corresponds to space translation, and \(v_{5}\) to Galilean boost. We refer the reader to Olver (1986) for more details on the derivation.

Data.We generate simulated solutions, which we use to test the models' performance. We use \(\Omega=[0,L]=[0,2\pi]\), discretized uniformly into \(256\) points and assume periodic spatial boundaries. We also use \([0,T]=[0,16]\), discretized into \(100\) points. The viscosity coefficient is set to \(\nu=0.01\)

[MISSING_PAGE_EMPTY:8]

### Burgers' Equation

The second PDE we analyze is Burgers' Eq. (14), which combines diffusion (with thermal diffusivity \(\nu\)) and non-linear advection (wave motion). The nonlinearity of this equation makes it more complex, resulting in shock formation.

\[u_{t}=\nu u_{xx}-uu_{x} \tag{14}\]

**Symmetries.** Burgers' equation in the form described in Eq. (14) has a symmetry group spanned by the following \(5\)-dimensional vector space.

\[\mathbf{v}_{1} =\partial_{x}\quad\mathbf{v}_{3}=t\partial_{x}+\partial_{u} \mathbf{v}_{5}=tx\partial_{x}+t^{2}\partial_{t}-(x-tu)\partial_{u} \tag{15}\] \[\mathbf{v}_{2} =\partial_{t}\quad\mathbf{v}_{4}=x\partial_{x}+2t\partial t-u \partial_{u}\]

However, only the last generator \(\mathbf{v}_{5}\) results in a useful training signal. The first three generators give \(\mathcal{L}_{\mathrm{sym}}=0\) and \(\mathbf{v}_{4}\) gives \(\mathcal{L}_{\mathrm{sym}}=c\Delta=c\mathcal{L}_{\mathrm{PDE}}\), for a constant \(c\). As with the heat equation experiment, we can eliminate the PDE loss and only use symmetry and supervised losses for training.

Data.The data used to evaluate the model is obtained using the Fourier Spectral method with periodic spatial boundaries. Initial conditions are obtained similarly to the heat equation experiment, described in Eq. (13). We use \(\nu=0.1\) as the diffusion coefficient. The domain is \([0,L]=[0,2\pi]\) and \([0,T]=[0,2.475]\) discretized uniformly into \(256\) and \(100\) points respectively.

Training and Experiments.For Burgers' equation, we train the model on datasets of \(N_{f}=500\) initial conditions and \(N_{r}=5000,25000\) and \(100000\) samples. We found that for \(\mathcal{L}_{\mathrm{sym}}\), cosine similarity works better in this case. The models' architectures are similar to those used for the heat equation described in Appendix C.

Results.Table 2 shows the average mean-squared errors on the test dataset for the two models as \(N_{r}\) increases. We can see that, even with one symmetry group useful for training, the model trained with \(\mathcal{L}_{\mathrm{sym}}\) performs better. The predictions on a single instance of the test dataset can also be seen in Section 4.2. Again, we see that the symmetry loss especially improves the model's performance for larger values of \(t\). See Appendix B for additional plots of the prediction results. We also note that the high standard deviations in Table 2 are because, compared to the heat equation, the behaviour of the solution (specifically shock formation) varies a lot based on the initial conditions.

## Conclusion and Limitations

Lie groups and continuous symmetries are historically rooted in the study of differential equations, yet to this day, their application to Neural PDE solvers has been limited. Our work presents the

\begin{table}
\begin{tabular}{c c c} \hline \hline Number of Points (\(N_{r}\)) & No Symmetry & Symmetry \\ \hline \(500\) & \(1.12\pm 0.58\) & \(\mathbf{0.30\pm 0.15}\) \\ \(2000\) & \(0.36\pm 0.19\) & \(\mathbf{0.24\pm 0.14}\) \\ \(10000\) & \(0.22\pm 0.14\) & \(\mathbf{0.21\pm 0.13}\) \\ \hline \hline \end{tabular} 
\begin{tabular}{c c c} \hline \hline Number of Points (\(N_{r}\)) & No Symmetry & Symmetry \\ \hline \(5000\) & \(0.041\pm 0.042\) & \(\mathbf{0.034\pm 0.039}\) \\ \(25000\) & \(0.030\pm 0.038\) & \(\mathbf{0.017\pm 0.020}\) \\ \(100000\) & \(0.018\pm 0.022\) & \(\mathbf{0.013\pm 0.020}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: The average test set mean-squared error for the Heat equation.

Figure 3: (a) predictions of the network trained with a total of \(100000\) points inside the domain (\(500\) initial conditions and only \(200\) points for each PDE), with and without the symmetry loss, for Burgers’ equation on a test data. (b) corresponding predictions at different time slices showing that the model trained with symmetry loss performs better, especially at larger \(t\).

foundations for leveraging Lie point symmetry in a large family of Neural PDE solvers that do not require access to accurate simulations. Using available machinery of automatic differentiation, we show that local symmetry constraints can improve PDE solutions found using PINN models.

The method we propose to leverage local symmetries has some limitations: 1) while the Lie point symmetries of important PDEs are well-known, in general, one needs to analytically derive them for a given PDE to use our approach; 2) as we mentioned in Section 4, not all symmetries of the equation will necessarily be useful for constraining the PINN. Fortunately, the usefulness of symmetries is obvious from the corresponding infinitesimal criterion, and one could limit the symmetry loss to useful symmetries; 3) while symmetries can significantly improve performance, based on our empirical observations (see Table 1), one could achieve a similar effect with PINN by increasing the sample size. These limitations motivate our future direction, which builds on our current understanding, to impose symmetry constraints through equivariant architectures.

#### Acknowledgments

The authors would like to thank Prakash Panangaden for insightful discussions on symmetries and Lie Group theory. We also thank Oumar Kaba for his helpful feedback on this manuscript. This research is in part supported by Canada CIFAR AI Chair and Microsoft Research. Computational resources are provided by Mila and Digital Research Alliance of Canada.

## References

* Bar-Sinai et al. (2019) Yohai Bar-Sinai, Stephan Hoyer, Jason Hickey, and Michael P. Brenner. Learning data-driven discretizations for partial differential equations. _Proceedings of the National Academy of Sciences_, 116(31):15344-15349, 2019. doi: 10.1073/pnas.1814058116.
* Bi et al. (2022) Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Pangu-weather: A 3d high-resolution model for fast and accurate global weather forecast. _arXiv preprint arXiv:2211.02556_, 2022.
* Brandstetter et al. (2022a) Johannes Brandstetter, Max Welling, and Daniel E. Worrall. Lie point symmetry data augmentation for neural PDE solvers. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162, pages 2241-2256. PMLR, 2022a.
* Brandstetter et al. (2022b) Johannes Brandstetter, Daniel E. Worrall, and Max Welling. Message passing neural PDE solvers. In _The 10th International Conference on Learning Representations_. OpenReview.net, 2022b.
* Gupta and Brandstetter (2022) Jayesh K Gupta and Johannes Brandstetter. Towards multi-spatiotemporal-scale generalized pde modeling. _arXiv preprint arXiv:2209.15616_, 2022.
* Kashinath et al. (2021) K. Kashinath, M. Mustafa, A. Albert, J-L. Wu, C. Jiang, S. Esmaeilzadeh, K. Azizzadenesheli, R. Wang, A. Chattopadhyay, A. Singh, A. Manepalli, D. Chirila, R. Yu, R. Walters, B. White, H. Xiao, H. A. Tchelepi, P. Marcus, A. Anandkumar, P. Hassanzadeh, and Prabhat. Physics-informed machine learning: case studies for weather and climate modelling. _Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 379(2194):20200093, February 2021. doi: 10.1098/rsta.2020.0093.
* accelerated computational fluid dynamics. _Proceedings of the National Academy of Sciences_, 118(21), 2021. doi: 10.1073/pnas.2101784118.
* Krishnapriyan et al. (2021) Aditi S. Krishnapriyan, Amir Gholami, Shandian Zhe, Robert M. Kirby, and Michael W. Mahoney. Characterizing possible failure modes in physics-informed neural networks. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021_, pages 26548-26560, 2021.
* Li et al. (2021a) Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. In _Proceedings of the 9th International Conference on Learning Representations_, 2021a.
* Liu et al. (2021b)Zongyi Li, Hongkai Zheng, Nikola B. Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial differential equations. _CoRR_, abs/2111.03794, 2021b. URL [https://arxiv.org/abs/2111.03794](https://arxiv.org/abs/2111.03794).
* Lu et al. (2021) Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. _Nature Machine Intelligence_, 3:218-229, 2021.
* Mialon et al. (2023) Gregoire Mialon, Quentin Garrido, Hannah Lawrence, Danyal Rehman, Yann LeCun, and Bobak T Kiani. Self-supervised learning with lie symmetries for partial differential equations. _arXiv preprint arXiv:2307.05432_, 2023.
* Nguyen et al. (2023) Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, and Aditya Grover. Climax: A foundation model for weather and climate. _arXiv preprint arXiv:2301.10343_, 2023.
* Olver (1986) Peter J. Olver. _Applications of Lie Groups to Differential Equations_. Springer, 1986.
* Quarteroni (2009) Alfio Quarteroni. _Numerical Models for Differential Problems_, volume 2. Springer, 2009.
* Raissi et al. (2019) Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. _Journal of Computational physics_, 378:686-707, 2019.
* Wang et al. (2020a) Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards physics-informed deep learning for turbulent flow prediction, 2020a.
* Wang et al. (2020b) Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient pathologies in physics-informed neural networks, 2020b.
* Wang et al. (2021) Sifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric partial differential equations with physics-informed deeponets. _Science Advances_, 7(40), 2021.
* Wang et al. (2022) Sifan Wang, Shyam Sankaran, and Paris Perdikaris. Respecting causality is all you need for training physics-informed neural networks, 2022.
* Wang et al. (2023) Sifan Wang, Shyam Sankaran, Hanwen Wang, and Paris Perdikaris. An expert's guide to training physics-informed neural networks. _arXiv preprint arXiv:2308.08468_, 2023.

Illustrative Examples

**Example 3** (Obtaining the Prolongation of \(SO(2)\)).: _We can consider If \(X\times U=\mathbb{R}\times\mathbb{R}\) and the infinitesimal generator of the 2-dimensional rotation group, \(SO(2)\):_

\[\mathbf{v}_{SO(2)} =\xi(x,u)\partial_{x}+\phi(x,u)\partial_{u}\] \[=-u\partial_{x}+x\partial_{u}\]

_In this 2-dimensional case, the calculation of the prolonged generator is simple:_

\[\phi^{(x)} =D_{x}(\phi-\xi u_{x})+\xi u_{xx}\] \[=D_{x}(x+uu_{x})-uu_{xx}\] \[=(1+u_{x}^{2}+uu_{xx})-uu_{xx}\] \[=1+u_{x}^{2}\]

_Therefore:_

\[\mathrm{pr}^{(1)}\mathbf{v}_{SO(2)}=-u\partial_{x}+x\partial_{u}+(1+u_{x}^{2}) \partial_{u_{x}}\]

We will work through another example of obtaining the prolongation of an infinitesimal generator of the heat equation:

**Example 4** (Obtaining the Prolongation of an Infinitesimal Generator).: _As an example, we will consider \(X\times U=\mathbb{R}^{2}\times\mathbb{R}\) and the following infinitesimal generator, which is a symmetry of the heat equation:_

\[\mathbf{v} =\xi_{1}(x,t,u)\partial_{x}+\xi_{2}(x,t,u)\partial_{t}+\phi(x,t,u )\partial_{u}\] \[=2\nu t\partial_{x}-xu\partial_{u}\]

_where \(x,t\) denote the independent variables, \(u\) is the dependent variable and \(\nu\) is a positive constant. By the prolongation formula, Eq. (7), the first prolongation in \(t\) is given by:_

\[\phi^{t} =D_{t}(\phi-\xi_{1}u_{x}-\xi_{2}u_{t})+\xi_{1}u_{xt}+\xi_{2}u_{tt}\] \[=D_{t}(-xu-2\nu tu_{x})+2\nu tu_{xt}\] \[=-xu_{t}-2\nu u_{x}\]

As a final illustrative example of the symmetry criterion, we will follow Olver's example below:

**Example 5**.: _As an illustrative example of the infinitesimal criterion, we can consider a simple DE:_

\[\Delta(x,u,u_{x})=(u-x)u_{x}+u+x=0\]

_We can easily see that \(SO(2)\) is a symmetry group of this differential equation, using the prolongation of the generator we calculated in Example 3:_

\[\mathrm{pr}^{(1)}\mathbf{v}[\Delta] =-u\Delta_{x}+x\Delta_{u}+(1+u_{x}^{2})\Delta_{u_{x}}\] \[=-u(1-u_{x})+x(1+u_{x})+(1+u_{x}^{2})(u-x)\] \[=u_{x}\Delta\]

Figure 4: Various solutions of the PDE \(\Delta(x,u,u_{x})=(u-x)u_{x}+u+x=0\) obtained via symmetry transformation (rotation) of a know solution (in red).

_Since \(\Delta u_{x}=0\) when \(\Delta=0\), we can conclude that \(SO(2)\) is indeed a symmetry group of the equation. In fact, we can see that it transforms solutions of this differential equation to other solutions in Fig. 4._

## Appendix B Implementation and Training Details

We model the two networks, \(g_{\theta_{1}}\) and \(e_{\theta_{2}}\) in Eq. (9) with MLPs consisting of \(7\) hidden layers of width \(100\). This choice was based on the previous research using PINN and DeepONets for solving Burgers' equation (Wang et al., 2021). We used \(\mathrm{elu}\) activation as differentiable activations are required for the PDE loss. The output of the embedding vectors from both networks is \(100\) dimensional. We used ADAM optimizer with learning rate of \(0.001\) for the training and performed early stopping using the validation dataset.

For both the Heat equation and Burgers' equation experiments, we perform hyper-parameter tuning on the coefficients of the loss terms from the set \([0.1,\dots,1,\dots,10,\dots,100,\dots 200]\). This is done separately for the baseline model and the model trained with symmetry loss, \(\mathcal{L}_{\mathrm{sym}}\), as we varied the number of samples, \(N_{r}\). We found that similar to PINNs, the model is sensitive to the weight given to the supervised loss for the initial conditions vs the symmetry/PINN loss. The specific coefficient values for the models trained with and without symmetry loss for the heat equation are in Table 3 and Table 4.

We also note that for Burgers' equation, we found that cosine similarity for \(\mathcal{L}_{\mathrm{sym}}\) works better than the dot product. The results reported in Section 4 use cosine-similarity.

We will make the data and the code available on GitHub.

## Appendix C Additional Results

In the figure below, we can see the behaviour of the two models, trained with and without symmetry loss for Burgers' equation, as we increase the number of training samples. It can be seen that, as expected, in the model trained with \(\mathcal{L}_{\mathrm{sym}}\) performs significantly better with low samples inside the domain. The corresponding mean-squared errors are reported in Table 2.

To show the effect of the number of symmetry groups of a PDE in the performance of the model trained with the symmetry loss, we analyzed the performance on (average MSE over the test dataset) as we increased the number of infinitesimal generators that were used to calculate \(\mathcal{L}_{\mathrm{sym}}\) for the Heat equation. The two tables below show this effect for two different models that are trained with \(N_{r}=500\) and \(N_{r}=10000\) unique points sampled from inside the grid, respectively. As it can be seen, including both of the useful symmetries of the Heat equation leads to the best performance compared to the models that are trained with 1 or 0 infinitesimal generators. Furthermore, as shown in previous results, we can see that the model's overall performance in all 3 cases increases as we

\begin{table}
\begin{tabular}{l l l l} \hline \hline loss coefficient & \(N_{r}=500\) & \(N_{r}=2000\) & \(N_{r}=10000\) \\ \hline \(\alpha\) & \(150\) & \(150\) & \(130\) \\ \(\gamma\) & \(20\) & \(20\) & \(20\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: he loss coefficients for the model trained without the symmetry loss (i.e. \(\mathcal{L}=\alpha\mathcal{L}_{\mathrm{PINN}}+\gamma\mathcal{L}_{\mathrm{ data-fit}}\) for the heat equation for various number of unique points sampled inside the grid for training, \(N_{r}\)

\begin{table}
\begin{tabular}{l l l l} \hline \hline loss coefficient & \(N_{r}=500\) & \(N_{r}=2000\) & \(N_{r}=10000\) \\ \hline \(\beta\) & \(100\) & \(80\) & \(40\) \\ \(\gamma\) & \(20\) & \(20\) & \(20\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: The loss coefficients for the model trained with the symmetry loss (i.e. \(\mathcal{L}=\beta\mathcal{L}_{\mathrm{sym}}+\gamma\mathcal{L}_{\mathrm{ data-fit}}\) for the heat equation for various number of unique points sampled inside the grid for training, \(N_{r}\)sample more points inside the grid and that the effect of the symmetry loss is more pronounced in the low-data regime. We also tried training models with and without a symmetry loss for the

Heat equation using a modified MLP architecture, as suggested in Wang et al. (2020). We found that, unexpectedly, this architecture was performing worse than a simple MLP architecture in our experiments. However, as it can be seen in Table 6, the model trained with the symmetry loss still performs better than the one trained with just the PINN loss.

\begin{table}
\begin{tabular}{c c c} \hline \hline Number of Points & No Symmetry & Symmetry \\ (\(N_{r}\)) & & \\ \hline \(500\) & \(1.15\pm 0.754\) & \(\mathbf{0.859\pm 0.689}\) \\ \(2000\) & \(0.974\pm 0.667\) & \(\mathbf{0.520\pm 0.488}\) \\ \(10000\) & \(0.391\pm 0.577\) & \(\mathbf{0.357\pm 0.363}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: The average test set mean-squared error for the Heat equation as a function of increasing the number of unique points sampled inside the grid. The model architecture is a modified MLP, as suggested in Wang et al. (2020). It can be seen that the model trained with the symmetry loss performs betters than that trained without, especially in the low-data regime.

Figure 5: The effect of training the PDE solver for the Burgers’ equation with and without the symmetry loss for one of the PDEs in the test dataset. (a) shows the ground truth solution and the predictions of the two models as the number of samples inside the domain increases from \(5000\) to \(25000\) and \(100000\).(b) shows the corresponding predictions and the ground truth solution at different time slices.

\begin{table}
\begin{tabular}{l l l} \hline \hline Number of Symmetries (\(K\)) & MSE when \(N_{r}=500\) & MSE when \(N_{r}=10000\) \\ \hline \(0\) & \(0.73\pm 0.38\) & \(0.26\pm 0.21\) \\ \(1\) & \(0.61\pm 0.33\) & \(0.25\pm 0.17\) \\ \(2\) & \(\mathbf{0.44\pm 0.26}\) & \(\mathbf{0.20\pm 0.12}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: The average test set mean-squared error for the Heat equation as a function of increasing the number of infinitesimal generators used to compute the symmetry loss. The MSE is reported for models trained with different numbers of unique points sampled from inside the grid. It can be seen that including both of the useful symmetries of the Heat equation leads to the best performance compared to the models that are trained with 1 or 0 infinitesimal generators.