**USDC: A Dataset of User Stance and Dogmatism in Long Conversations**

**Anonymous Author(s)**

Affiliation

Address

email

Identifying user's opinions and stances in long conversation threads on various topics can be extremely critical for enhanced personalization, market research, political campaigns, customer service, conflict resolution, targeted advertising and content moderation. Hence, training language models to automate this task is critical. However, to train such models, gathering manual annotations has multiple challenges: 1) It is time-consuming and costly; 2) Conversation threads could be very long, increasing chances of noisy annotations; and 3) Interpreting instances where a user changes their opinion within a conversation is difficult because often such transitions are subtle and not expressed explicitly. Inspired by the recent success of large language models (LLMs) for complex natural language processing (NLP) tasks, we leverage Mistral Large and GPT-4 to automate the human annotation process on the following two tasks while also providing reasoning: i) User Stance classification, which involves labeling a user's Stance of a post in a conversation on a five-point scale; ii) User Dogmatism classification, which deals with labeling a user's overall opinion in the conversation on a four-point scale. The majority voting on zero-shot, one-shot, and few-shot annotations from these two LLMs on 764 multi-user Reddit conversations helps us curate the USDC dataset. USDC is then used to finetune and instruction-tune multiple deployable small

Figure 1: Sample Reddit conversation on “Capitalism vs. Socialism” with Stance (for every comment \(\{c_{i}\}_{i=1}^{6}\)) and Dogmatism (for every author \(\{a_{j}\}_{j=1}^{3}\)) labels from Mistral Large and GPT-4. The submission content favors to socialism and examines how the authors position their opinions regarding socialism vs. capitalism.

language models for the 5-class stance and 4-class dogmatism classification tasks. We make the code and dataset publicly available 1.

Footnote 1: [https://anonymous.4open.science/r/USDC-OFFF](https://anonymous.4open.science/r/USDC-OFFF)

## 1 Introduction

Understanding the user's (or author's) opinion in a conversation is a fundamental aspect of successful interpersonal interactions, and it is essential for developing better interpersonal communication skills, empathy development, and informed decision-making. This user understanding is particularly relevant in the context of dogmatism, a phenomenon observed in various areas such as politics, religion, culture, intellect, and science, where rigid adherence to beliefs often hinders open-mindedness and empathy (Rokeach, 1954). Advertisers can target their campaigns more effectively by aligning with the opinions and stances of potential customers. Companies can use this information for market research to tailor products and services to meet consumer needs and preferences. Political groups can gauge public reaction to policies and campaigns and adjust their strategies accordingly. Identifying differing opinions can help conflict resolution by understanding the perspectives of all parties' perspectives. Society can promote tolerance and maintain social harmony by recognizing and respecting diverse opinions.

Fig. 1 shows a sample Reddit conversation on the topic of _Capitalism vs. Socialism_. We refer to an author's initial post (containing title and body) as a submission. Multiple authors can then share their opinions as comments on the submission. Specifically this example contains 6 comments \(\{c_{i}\}_{i=1}^{6}\) from 3 authors \(\{a_{j}\}_{j=1}^{3}\). We also show stance and dogmatism predictions from two large language models (LLMs): Mistral Large and GPT-4. Some authors like \(a_{1}\) change their views during the discussion based on the beliefs or opinions of others. At the beginning of the dialogue, we note that author \(a_{1}\) is somewhat favoring socialism (in submission and \(c_{2}\)). But the author shifts their stance to somewhat favors capitalism (in \(c_{4}\)) after considering the viewpoints of author \(a_{2}\) in comments \(c_{1}\) and \(c_{3}\), illustrating author \(a_{1}\)'s firm yet open-minded approach. On the other hand, author \(a_{3}\) seems very flexible based on their comment \(c_{5}\). Understanding conversations requires understanding the fine-grained topics being discussed and the dynamic viewpoints of the individual users.

Given the importance of understanding these user dynamics in conversations, training language models to perform this task automatically at scale is critical. While numerous datasets are available for analyzing individual user posts (Fast & Horvitz, 2016; Sakketou et al., 2022), typically through random subsampling of posts or selecting posts with a limited number of tokens, the exploration of a specific user's opinion across each post within an entire conversational thread remains under-explored.

Crowdsourcing is one possible approach to address the need for a suitable dataset. However, a significant limitation in manually annotating datasets for user opinions is the time-consuming nature of the process, as annotators must read entire conversations to label each user's post, making data acquisition costly. Additionally, manual annotation often faces challenges related to quality, as accurately labeling opinions requires understanding demographic details and domain-specific knowledge. Given these limitations, achieving a comprehensive and accurate set of user opinions corresponding to posts about a topic often requires multiple annotators or iterative rounds of annotation. Since users could change their opinion (often times with subtle transitions and not with explicit statements) within a conversation, tracking such changes across multiple users manually becomes very cumbersome.

Recently, large language models (LLMs), especially those built on Transformer architectures (Vaswani et al., 2017) and pretrained on large datasets, have resulted in state-of-the-art accuracies on several complex natural language processing (NLP) tasks (Brown et al., 2020; Chung et al., 2024). LLMs are also being frequently used for dialog response generation (Zhang et al., 2020; Bao et al., 2019; Roller et al., 2021; Adiwardana et al., 2020). Given the complex and cumbersome nature of conversation understanding, we hypothesize that LLMs can be effective in capturing nuances involved in understanding user opinions and their shifts in multi-user conversational contexts. Also, since these models possess long-range memory capabilities, we believe that they can reason over extended conversational threads involving numerous participants, as good as human annotators, if not better.

In this work, we leverage LLMs like Mistral Large and GPT-4 to perform two tasks: i) User Stance classification, which involves labeling a user's stance of a post in a conversation on a five-pointscale; ii) User Dogmatism classification, which deals with labeling a user's overall opinion in the conversation on a four-point scale. Besides the class labels, we also obtain reasoning behind these labels from these LLMs. We experiment with these two models as human-like annotators to generate user opinions in full-length, multi-user Reddit conversations in a zero-shot, one-shot as well as few-shot setup. Thus, overall for every sample, we obtain six annotations ({Mistral Large, GPT-4} \(\times\) {zero-shot, one-shot, few-shot}). Fig. 2 presents our LLM-based annotation pipeline for user-level Stance and Dogmatism tasks. We consider majority voting over these six as our final annotations. Overall, this helps us curate our USDC (a dataset of user stance and dogmatism in conversations) dataset, which consists of 764 multi-user conversations from 22 subreddits, including 1,528 user-level dogmatism samples and 9,618 stance samples across all posts from selected users. Overall, the annotations in the dataset highlight specific user opinions in each post related to stance, track opinion fluctuations leading to a dogmatic nature, and provide reasoning about why users hold specific opinions.

USDC addresses several weaknesses of existing post-level stance and dogmatism datasets. First, the full-length multi-user conversation aspect of USDC enables it to capture contextual and opinion shifts of multiple users. This feature allows it to serve as both an instruction-tuning user opinion dataset and an evaluation benchmark. We believe that the ability to perform instruction tuning for user opinions at a large scale can bridge the gap between open-source and commercial user trait understanding models. Additionally, the in-context learning annotations using state-of-the-art LLMs in USDC make it a more comprehensive measure of how current LLMs understand complex tasks like capturing opinions. This aspect makes it a valuable resource, especially for social media agents seeking deeper insights into user behavior.

In this work, we utilize our USDC dataset to finetune as well as instruction-tune open-source LLMs for generating stance and dogmatism labels for users. We experiment with three pretrained small language models (SLMs) like LLaMA-2-7B, LLaMA-3-8B, and Falcon-7B. We also experiment with four instruction-tuned SLMs like LLaMA-2-chat-7B, LLaMA-3-8B-instruct, Vicuna-7B-v.1.5, and Falcon-7B-instruct. We report weighted F1 scores obtained using these models for both the tasks.

We make the following contributions: 1) We contribute USDC (a dataset of user stance and dogmatism in conversations) dataset consisting of 764 multi-user conversations labeled with 1,528 user-level dogmatism samples and 9,618 stance samples. 2) We report initial results for the stance and dogmatism detection tasks using seven small language models for the UDSC dataset. We find that stance detection performance improves with instruction-tuning (F1-score of 56.2) compared to finetuning (F1-score of 54.9). However, dogmatism detection performs worse with instruction-tuning (F1-score of 49.2) compared to fine-tuning (F1-score of 51.4), highlighting the complexity of this task. 3) We make the code and dataset publicly available1. Also, the finetuned and instruction-tuned models are made available as well.

Figure 2: Generating annotations using LLMs: We pass the entire conversation for each Reddit thread in JSON format. The JSON highlights the top two authors who posted the most comments, alongside annotation guidelines for stance and dogmatism labels in the system prompt.

Related Work

**Opinion fluctuations in user conversations.** Our work is closely related to previous studies (Fast and Horvitz, 2016; Sakketou et al., 2022), which explore Stance and Dogmatism at the post level, where posts are randomly sampled from conversation threads. Fast and Horvitz (2016) predicted user dogmatism on randomly sampled Reddit posts from conversations, with each post limited to 200-300 characters. One major limitation of this work is the unavailability of a public dataset and missing annotator demographic details. Sakketou et al. (2022) created the post-level Stance dataset, SPINOS, where each post is considered independent, and submission posts are missing while annotators label the data. Additionally, the quality of the dataset is not validated due to missing demographic details of these annotators. Our work overcomes the limitations of previous studies and presents Stance detection for posts and Dogmatism labels of users in conversations, considering the entire context, while preserving submission IDs. Hence, our dataset provides clear user-level posts and dogmatism data, which are useful for modeling dynamic user representations.

**Generating annotations for NLP tasks using Large Language Models** Our work also relates to a growing body of literature suggesting that large language models can perform similarly to human annotators in labeling complex NLP tasks (Zhou et al., 2022; Zhang et al., 2023; Bansal and Sharma, 2023; Lowmanstone et al., 2023; Wadhwa et al., 2023; Honovich et al., 2023; Zheng et al., 2024; Ye et al., 2022a; Meng et al., 2022). Several studies have explored LLM-based annotation generation in zero-shot or few-shot task settings (Ye et al., 2022a; Meng et al., 2022; Ye et al., 2022b), while others have compared pairs of language models to assess the quality of annotations generated by these LLMs (Zheng et al., 2024). However, these studies focused on generating annotations for NLP tasks such as sentiment analysis, natural language inference (Gilardi et al., 2023; Alizadeh et al., 2023), or creating synthetic dialogues, but only for dyadic conversations (Lee et al., 2023). Our approach complements these previous studies by focusing on generating annotations of user opinions in complex multi-user conversations.

## 3 USDC Dataset Curation

In this section, we will discuss three main things: 1) Collection of Reddit conversations, 2) Obtaining LLM annotations, and 3) Inter-annotator agreement with LLMs as annotators.

### Collection of Reddit Conversation Threads

**Initial crawl.** We crawl an year (2022) worth of multi-user conversation data from 22 subreddits of Reddit 2 using praw API 3. This dataset includes submissions and all associated user comments. Each submission, which serves as the initial message of the conversation, contains a title and content body. This is followed by comments and replies to the submission or other comments. Overall, we crawled 3,619 Reddit conversations across the 22 subreddits. A sample Reddit conversation is displayed in Fig. 1.

Footnote 2: [https://www.reddit.com/](https://www.reddit.com/)

Footnote 3: [https://github.com/praw-dev/praw](https://github.com/praw-dev/praw)

**Quality filtering of conversations.** Since submission content on Reddit can sometimes include videos, we perform the following filtering steps. 1) We only consider submissions where the content is text. 2) We remove conversations with [deleted] tags and empty content. 3) We exclude conversations where the posts were either discarded by users or removed by moderators.

Reddit user conversations can be very long and we observed up to 591 comments in a single crawled conversation data. Considering the maximum sequence length allowed by various language models, we retained only those conversations that contain at least 20 and at most 70 comments. Considering conversations with fewer than 20 comments results in too few comments to accurately gauge user opinions based on small samples. Further, we ensure that at least two users covering \(\sim\)50% of the comments in the conversations. We did not remove any comments or reduce the post length in the selected conversations. Out of the initial 3,619 conversations, these filtering steps result into 764 conversations getting selected. Table. 4 in the Appendix shows detailed subreddit level statistics.

### Obtaining LLM Annotations

**Representing Reddit conversations in JSON format.**

To create the prompt, we follow the nested hierarchical structure of Reddit conversations to maintain the context. Specifically, we maintain a JSON structure for each conversation, where each author has their post IDs, and comments or replies are available in the body section. An example of a Reddit conversation in JSON format is provided in Appendix D. Note that the JSON explicitly includes the top-2 authors who posted the most comments in the conversation, as well as their respective post IDs. Our emphasis on these top-2 users (covering 47% posts of total posts on average) aimed at accurately assigning Stance and Dogmatism labels, acknowledging the challenge of modeling a user's opinion belief based on a very number of posts within a conversation.

**Using LLMs as human-like annotators.** To annotate the position (or Stance) of a user towards a subreddit topic at each post and the overall opinion (or Dogmatism level) of a user in a conversation, we employ two well-known commercialized API-based LLMs: GPT-4 (OpenAI, 2023) and Mistral Large (Jiang et al., 2024). OpenAI GPT-4 is a decoder-based language model which features a context window of 32k to 128k tokens. Mistral Large features a context window of 32k tokens. Additionally, we also examined other versions of these models, such as GPT-3.5 and Mistral-small and medium, but found that these models failed to produce annotations in the desired format. We briefly discuss these limitations in Section 6.

For both GPT-4 and Mistral Large, we supplied a system prompt that contains the definition of Stance and Dogmatism, guidelines for annotating each user conversation, and the necessary labels for Stance and Dogmatism, as shown in Fig 2. The system prompt is detailed in the Appendix B. Along with the system prompt, we provided a user prompt comprising the entire user conversation in a structured JSON format, as discussed above. Additionally, we prompted the model to generate reasoning for each label, explaining why the LLMs assigned a particular label to a specific user post. We used zero-shot, one-shot, and few-shot settings to get the LLM-based annotations. For the few-shot setting, we added two examples in the prompt. Samples of generated outputs using GPT-4 in zero-shot, one-shot, and few-shot settings are shown in Appendix E.1, E.2, E.3 respectively. Similarly, samples of generated outputs using Mistral Large in zero-shot, one-shot, and few-shot settings are shown in Appendix E.4, E.5, E.6 respectively.

**Annotation tasks.** We prompt the LLMs to perform two annotation tasks: 1) Stance detection, which determines if a user comment or post is _Strongly In Favor_, _Strongly Against_, _Stance Not Inferrable_, _Somewhat In Favor_, or _Somewhat Against_ towards specific subreddit submission content; 2) Dogmatism identification, which evaluates the user's overall opinion in conversation and categorizes them into one of four categories: _Firm but Open_, _Open to Dialogue_, _Flexible_ or _Deeply Rooted_. This assessment reveals whether a user is open to changing their beliefs or remains steadfast in their opinions based on interactions with other users.

**Addressing LLM response and JSON parsing failures.** Sometimes the LLMs got confused with the author IDs and missed Stance labels for some author IDs (Fig. 3 (left)). Sometimes, there were minor errors in key naming ('label' vs 'body' in Fig. 3 (right)). For each LLM setting, we observed

Figure 3: Failure cases of LLMs: Mistral Large few-shot output (left), the ids (‘f6mmzx1’,‘f6mma88”) were mismatched with generated ids (‘f9mmzx1’,‘f9mna88”), GPT-4 zero-shot output (right), the key **“label”** was mismatched with generated key “body”.

such errors in around 15 cases on average. We manually fixed such JSON parse errors and missing Stance labels for some author IDs.

**Majority voting.** After obtaining six annotations ([Mistral Large, GPT-4]\(\times\){zero-shot, one-shot, few-shot}) for each sample, we aggregate using majority voting to determine the final gold annotations for the Stance and Dogmatism tasks. Fig. 4 presents the class distributions for both the annotation tasks. Additionally, we present the class distributions obtained from each model with the three settings (zero-shot, one-shot and few-shot) for two tasks in Appendix Figs. 5 and 6 respectively.

### Inter-annotator Agreement with LLMs as Annotators

As the quality of labeling on subjective tasks is challenging, we validated the inter-annotator agreement (IAA) between the six LLMs (GPT-4 Zero-shot, GPT-4 One-shot, GPT-4 Few-shot, Mistral Large Zero-shot, Mistral Large One-shot, and Mistral Large Few-shot) for the Stance as well as Dogmatism tasks. We perform IAA using two approaches: i) Cohen's kappa score (Cohen, 1960) and ii) Fleiss' kappa score (Fleiss, 1971). Cohen's kappa measures the agreement between two raters, while Fleiss' kappa extends this to multiple raters. Hence, we employed Cohen's kappa for pairwise comparisons and Fleiss' kappa for overall agreement across all models.

Fig. 7 in Appendix shows the pairwise Cohen's kappa values for both Stance and Dogmatism tasks. We observe that Cohen's kappa values range from 0.36 to 0.72 for Stance and 0.31 to 0.61 for dogmatism, indicating moderate agreement between the models. Broadly kappa values are higher for model pairs within a family (GPT-4 or Mistral large). Thus, the large variance in the kappa scores is not due to the various in-context learning settings (ZS, OS, FS) but rather due to architectural differences.

The overall Fleiss' kappa value was calculated as 0.485 for Stance and 0.435 for Dogmatism, suggesting moderate agreement among all six models. Comparing LLM IAA with previous studies, we observe that for dogmatism, the LLM IAA of 0.435 matches with 0.44 as mentioned in Fast & Horvitz (2016). Similarly, for Stance, the LLM IAA of 0.485 is much higher than 0.34 as reported in Sakketou et al. (2022). It is important to note that previous studies on Stance and Dogmatism datasets were created on post-level data with limited token lengths, whereas our work focuses on entire user conversations. This suggests that LLMs can be considered as competent annotators for complex subjective tasks. However, the moderate agreement levels indicate potential areas for improvement and align with the observed performance variations among the models.

## 4 Training Small Language Models

In this section, we briefly discuss the small language models that we experiment with. We also discuss their finetuning and instruction tuning details.

Figure 4: Distribution of class labels for Stance (left) and Dogmatism (right) tasks. These class labels are determined by majority voting across GPT-4 and Mistral Large models.

### Small Language Models

we train three pretrained small language models (LLaMA-2-7B, LLaMA-3-8B, Falcon-7B) and four instruction-tuned small language models (LLaMA-2-chat-7B, LLaMA-3-8B-instruct, Vicuna-7B-v.1.5, and Falcon-7B-instruct). We finetune as well as instruction tune these models using the proposed USDC dataset. We use pretrained models checkpoints from Hugging Face. All of these LLMs have context length of 4096 tokens.

**LLaMA** models (Touvron et al., 2023a) are decoder-only LLMs trained on 1.6 trillion tokens from a mixture of corpora including C4, English CommonCrawl, Wikipedia, Github, and more. We use two versions of models in our study: LLaMa-2-7B (Touvron et al., 2023b) and LLaMa-3-8B and their instruction tuned variants.

**Falcon** models (Almazrouei et al., 2023) are decoder-only LLMs trained on \(\geq\) 1 trillion tokens of text, with a particular emphasis on the RefinedWeb corpus. For Falcon, we use both the pretrained and instruction tuned 7B parameter variants in our study.

**Vicuna** model (Chiang et al., 2023) is finetuned from the LLaMA 7B model on approximately 70K user-shared conversations gathered from ShareGPT.com and we used the 7B parameter variants.

### Experimental Setup

**Train-test setup.** We conducted both finetuning and instruction-tuning of small language models. For this purpose, we divided the dataset of 764 conversations into train (\(\sim\) 75%) and test splits (\(\sim\) 25%). The training dataset comprised 564 conversations, including 1128 samples of Dogmatism labels and 7520 samples of Stance labels. Conversely, the testing dataset consisted of 200 conversations, with 400 samples of Dogmatism labels and 1831 samples of Stance labels across two authors posts.

**Implementation details for reproducibility.** All experiments were conducted on a machine equipped with an NVIDIA A100 GPU with 80 GB of GPU RAM, partitioned into two devices of 40 GB each. We employed 4-bit quantization with normalized floating precision (nf4) from the bitsandbytes library 4. Additionally, we utilized LoRA (Hu et al., 2021) with a rank of 64 and an alpha value of 16 during task-based instruction tuning. Finally, we use PEFT (Parameter Efficient Finetuning) 5 library to train large language models with SFTT (Supervised Finetuning Trainer) setting. To further enhance performance, we divided the training dataset into a validation set, comprising a randomly chosen 10% subset from the training set, used exclusively for hyperparameter tuning. More details about bitsandbytes, PEFT and SFTT parameters are reported in Appendix.

Footnote 4: [https://pypi.org/project/bitsandbytes/](https://pypi.org/project/bitsandbytes/)

Footnote 5: [https://github.com/huggingface/peft](https://github.com/huggingface/peft)

### Finetuning and Instruction Tuning of Small Language Models (SLMs)

**Finetuning of SLMs.** For Stance classification, we treat each user post as an independent sample. In contrast, for Dogmatism classification, we consider the entire user conversation as a single sample by concatenating all the threads from a user in that conversation. To load the pretrained SLMs, we perform 4-bit quantization, apply the LoRA technique (Hu et al., 2021), and fine-tune the models with SFTT before saving the fine-tuned model. For finetuning, we used prompt for Stance classification as shown in Fig. 8 (see Appendix). Similarly, Fig. 9 (see Appendix) displays prompt for Dogmatism identification.

**Instruction tuning of SLMs.** We instruction tune the SLMs on user conversations along with their gold labels from the training part of the USDC dataset. For instruction tuning, we use the same prompt as used for LLMs to generate the USDC dataset (also shown in Appendix B). Similar to finetuning, we use same train-test splits for instruction tuning.

## 5 Results

**Do SLMs finetuned with task-specific LLM annotations accurately perform Stance and Dogmatism tasks on user opinions?** We show the weighted F1 of various SLMs finetuned with task-specific LLM annotations on the stance and dogmatism detection tasks on the USDC test set in Table 1. Wereport AUC scores and other qualitative analysis in Appendix F (Fig. 11 and 12). We make the following observations from these results: 1) For both tasks, the majority voting labels as ground truth, has a relatively high performance, scoring above 50% weighted F1-score across several models. 2) LLaMa-3 models (LLaMA-3-8B and LLaMA-3-8B-instruct) perform better across both the tasks. 3) For GPT-4 annotations, in most cases, SLMs finetuned with few-shot annotations outperform those trained with zero and one-shot annotations. For Mistral Large annotations, typically SLMs finetuned with one-shot annotations performs the best. 4) Specifically, for Stance detection task, Vicuna-7B-v.1.5 finetuned using few-shot annotations is the best model trained with GPT-4 annotations. Similarly, LLaMA-3-8B-instruct finetuned with one-shot annotations is the best model trained with Mistral Large annotations. 5) For the Dogmatism detection task, LLaMA-3-8B-instruct finetuned using few-shot annotations is the best model trained with GPT-4 annotations. Similarly, LLaMA-2-chat-7B finetuned with one-shot annotations is the best model trained with Mistral Large annotations. 6) Overall, we observe that instruction tuned SLMs perform better than the pretrained SLMs.

**Do SLMs instruction-tuned with task-specific LLM annotations perform better than SLMs finetuned with task-specific LLM annotations for the Stance and Dogmatism tasks?** We show the weighted F1 of various SLMs instruction-tuned with task-specific LLM annotations, on the stance and dogmatism detection tasks on the USDC test set in Table 2. We report AUC scores and other qualitative analysis in Appendix F (see Fig. 13). We make the following observations from these results: 1) SLMs with instruction-tuning result in higher weighted F1-scores than SLMs with finetuning for stance detection, while SLMs with finetuning outperform SLMs with instruction-tuning in dogmatism detection. 2) Contrary to results in Table 1, Table 2 demonstrates that using majority voting labels as ground truth, SLM instruction-tuning yields relatively high performance only for the stance detection task, but not for the dogmatism detection. 3) Similar to results in Table 1, LLaMA-3 models (LLaMA-3-8B and LLaMA-3-8B-instruct) perform better across both tasks. Additionally, GPT-4 annotations yield the best results in the few-shot setting, while Mistral Large annotations perform best in the one-shot setting.

Overall, we draw the following conclusions when comparing SLM finetuning and instruction-tuning: (1) Since dogmatism detection is inherently a more complex and varied task than stance detection, the model might struggle to generalize from the instructional data. (2) The system prompt used in finetuning is much simpler than the original system prompt for instruction-tuning, making it challenging to handle the context length for longer conversations. We perform an error analysis to further analyze the results in the next subsection.

**Error Analysis** Table 3 illustrates the confusion matrix for stance detection for LLaMa-3-8B finetuning and instruction-tuning. We make the following observations this table: 1) For both finetuning and instruction-tuning, there is a significant misclassification between "Somewhat Against" and "Somewhat In Favor," as well as between "Somewhat In Favor" and "Stance Not Inferrable." These overlaps suggest challenges in distinguishing moderate stances, indicating a need for enhanced

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \multirow{3}{*}{**Model**} & \multicolumn{4}{c|}{**Stance Classification**} & \multicolumn{6}{c|}{**Dogmatism Classification**} \\ \cline{2-13}  & \multicolumn{2}{c|}{**GPT-14**} & \multicolumn{2}{c|}{**Mistral Large**} & \multicolumn{2}{c|}{**GPT-4**} & \multicolumn{2}{c|}{**Mistral Large**} \\ \cline{2-13}  & **2S** & **0S** & **1S** & **2S** & **0S** & **2S** & **0S** & **1S** & **2S** & **0S** & **1Sfeature representation and clearer class definitions to improve model performance. We report the confusion matrix for dogmatism detection task in Appendix Fig. 10. Fig. 10 shows significant misclassifications, especially for the "Deeply Rooted" and "Flexible" labels, with both having zero accuracy and F1-scores. On the other hand, the model performs moderately better for "Firm but Open" and "Open to Dialogue" classes with accuracies of 48.7% and 64.4% respectively. The confusion matrix also indicates substantial confusion to distinguish between intermediate levels of dogmatism, such as "Firm but Open" and "Open to Dialogue". The area under the ROC curve (AUC) is a measure of the model's ability to distinguish between classes. Hence, we further report the ROC curve which shows the trade-off between the true positive rate (TPR) and false positive rate (FPR) for each class for stance and dogmatism tasks, see Figs. 11 and 12 in Appendix F.

**Verification using Human Interaction.** Due to the time-consuming nature of the manual annotation process, we perform human annotations on the set of 200 test conversations. In the forms for human annotations, we displayed the top 2 author's Reddit posts from the conversation, along with the submission title and content. We also provided a link to the original Reddit URL for annotators to look at the full conversation. We provided detailed annotation guidelines (similar to the ones mentioned in the prompt in Appendix B) to instruct human annotators in carrying out these tasks. Here is a sample Google form6. With three human annotators on a sample of 10 conversations, the agreement of majority labels (i.e., USDC test set labels) with human labels is 0.56 for the stance detection task and 0.45 for the dogmatism task. The annotators included two males and one female, affiliated with both academia and industry, aged between 20 and 40, and were very familiar with Reddit topics.

Footnote 6: [https://forms.gle/dbPQBsNyfNJjvUeR9](https://forms.gle/dbPQBsNyfNJjvUeR9)

## 6 Conclusion

In this paper, we focused on the problems of 5-class stance and 4-class dogmatism classification in long conversations. Using LLMs as human-like annotators, we introduced USDC, a large-scale dataset of user stance and dogmatism in conversations. This is achieved by providing detailed annotation guidelines in the system prompt and full-length conversation as user prompt. Commercialized API-based LLMs generate author-level stance and dogmatism labels via zero, one and few-shot settings. The full-length multi-user conversation aspect of USDC allows it to capture the contextual and opinion shifts of multiple users in a conversation. We believe that the ability to perform finetuning or instruction tuning SLMs for user opinions at a large scale can bridge the gap between SLMs and commercial LLMs for understanding user traits. While finetuning SLMs shows F1-score on both stance and dogmatism tasks, the F1-score remains below 60% (54.9% for Stance and 51.4% for Dogmatism). On the other hand, instruction tuning of SLMs only improves F1-score performance on stance, not the dogmatism task. Further, the performance still falls short of 60%, with weighted F1-scores of 56.2% for stance and 49.2% for dogmatism. These findings indicate that there is still significant room for improvement in understanding user opinions from a text segment.

**Limitations.** We plan to extend this work along the following directions in the future. 1) We performed this work on English conversations only. It would be nice to extend this to multi-lingual conversations and verify how accurately SLMs and LLMs perform on the Stance and Dogmatism tasks in the multi-lingual scenario. 2) We analyzed user dogmatism based on their posts within a single conversation. This approach could be extended to include posts across multiple conversations and utilize similar profile information if available. 3) We analyzed dogmatism information for only the top two authors. This was mainly because considering more authors increases the output generation length, and we were constrained by our budget. This implies that our current models have not been evaluated for authors who do not post frequently.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline \hline  & & & \multicolumn{3}{c|}{Predicted} & & \\ \hline  & SOA & SOIF & SNII & SGA & SIF \\ \hline \multirow{4}{*}{Actual} & SOA & 151 & 132 & 34 & 44 & 2 \\ \cline{2-7}  & SOIF & 93 & **537** & 113 & 17 & 14 \\ \cline{2-7}  & SNI & 23 & 78 & **259** & 5 & 0 \\ \cline{2-7}  & SCA & 52 & 35 & 13 & 115 & 17 \\ \cline{2-7}  & SIF & 18 & 50 & 12 & 25 & 27 \\ \hline \end{tabular}
\end{table}
Table 3: Confusion matrix for LLMa-3-8B Stance detection models on USDC test set: finetuning (left) and instruction-tuning (right). SOA: Somewhat Against, SOIF: Somewhat In Favor, SNI: Stance Not Inferrable, SGA: Strongly Against, SIF: Strongly In Favor.

## References

* Adiwardana et al. [2020] Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like open-domain chatbot. _arXiv preprint arXiv:2001.09977_, 2020.
* Alizadeh et al. [2023] Meysam Alizadeh, Mael Kubli, Zeynab Samei, Shirin Dehghani, Juan Diego Bermeo, Maria Korobeynikova, and Fabrizio Gilardi. Open-source large language models outperform crowd workers and approach chatgpt in text-annotation tasks. _arXiv preprint arXiv:2307.02179_, 2023.
* Almazrouei et al. [2023] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon series of open language models. _arXiv preprint arXiv:2311.16867_, 2023.
* Bansal and Sharma [2023] Parikshit Bansal and Amit Sharma. Large language models as annotators: Enhancing generalization of nlp models at minimal cost. _arXiv preprint arXiv:2306.15766_, 2023.
* Bao et al. [2019] Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. Plato: Pre-trained dialogue generation model with discrete latent variable. _arXiv preprint arXiv:1910.07931_, 2019.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).
* Chung et al. [2024] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _Journal of Machine Learning Research_, 25(70):1-53, 2024.
* Cohen [1960] Jacob Cohen. A coefficient of agreement for nominal scales. _Educational and psychological measurement_, 20(1):37-46, 1960.
* Fast and Horvitz [2016] Ethan Fast and Eric Horvitz. Identifying dogmatism in social media: Signals and models. In _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pp. 690-699, 2016.
* Fleiss [1971] Joseph L Fleiss. Measuring nominal scale agreement among many raters. _Psychological bulletin_, 76(5):378, 1971.
* Gilardi et al. [2023] Fabrizio Gilardi, Meysam Alizadeh, and Mael Kubli. Chatgpt outperforms crowd workers for text-annotation tasks. _Proceedings of the National Academy of Sciences_, 120(30):e2305016120, 2023.
* Honovich et al. [2023] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 14409-14428, 2023.
* Hu et al. [2021] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2021.
* Jiang et al. [2024] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. _arXiv preprint arXiv:2401.04088_, 2024.
* Lee et al. [2023] Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen White, and Sujay Jauhar. Making large language models better data creators. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 15349-15360, 2023.

* Lowmanstone et al. [2023] London Lowmanstone, Ruyuan Wan, Risako Owan, Jaehyung Kim, and Dongyeop Kang. Annotation imputation to individualize predictions: Initial studies on distribution dynamics and model predictions. _arXiv preprint arXiv:2305.15070_, 2023.
* Meng et al. [2022] Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Generating training data with language models: Towards zero-shot language understanding. _Advances in Neural Information Processing Systems_, 35:462-477, 2022.
* OpenAI [2023] R OpenAI. Gpt-4 technical report. arxiv 2303.08774. _View in Article_, 2(5), 2023.
* Rokeach [1954] Milton Rokeach. The nature and meaning of dogmatism. _Psychological Review_, 61(3), 1954.
* Roller et al. [2021] Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. Recipes for building an open-domain chatbot. In _Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume_, pp. 300-325, Online, 2021. Association for Computational Linguistics.
* Sakketou et al. [2022] Flora Sakketou, Allison Lahnala, Liane Vogel, and Lucie Flek. Investigating user radicalization: A novel dataset for identifying fine-grained temporal shifts in opinion. _arXiv preprint arXiv:2204.10190_, 2022.
* Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023a.
* Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023b.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Wadhwa et al. [2023] Manya Wadhwa, Jifan Chen, Junyi Jessy Li, and Greg Durrett. Using natural language explanations to rescale human judgments. _arXiv preprint arXiv:2305.14770_, 2023.
* Ye et al. [2022a] Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong. Zerogen: Efficient zero-shot learning via dataset generation. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pp. 11653-11669, 2022a.
* Ye et al. [2022b] Jiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Progen: Progressive zero-shot dataset generation via in-context feedback. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pp. 3671-3683, 2022b.
* Zhang et al. [2023] Ruoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, and Lei Zou. Llmaaa: Making large language models as active annotators. _arXiv preprint arXiv:2310.19596_, 2023.
* Zhang et al. [2020] Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. DIALOGPT : Large-scale generative pre-training for conversational response generation. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations_, pp. 270-278, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-demos.30. URL [https://aclanthology.org/2020.acl-demos.30](https://aclanthology.org/2020.acl-demos.30).
* Zheng et al. [2024] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _Advances in Neural Information Processing Systems_, 36, 2024.
* Zhou et al. [2022] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. _arXiv preprint arXiv:2211.01910_, 2022.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have ensured that the main claims made in the abstract and introduction are directly correlating to the research findings and the methods we have employed. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper discusses the main limitations of the work performed by the authors in the discussion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]

Justification: Our paper does not require any explicit theorems and proofs.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]

Justification: The paper has delineated all the information related to the experimental setup in the experimental setup section.

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

Answer: [Yes]

Justification: We have released the code and dataset, making the dataset publicly available under a license.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

6. **Experimental Setting/Details**

Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?

Answer: [Yes]

Justification: We provided all the training and testing details in the experimental setup.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.

7. **Experiment Statistical Significance**

Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?

Answer: [Yes]

Justification: We conducted our experiments on all LLM-generated annotations across zero-shot, one-shot, and few-shot settings, using majority voting as labels. Our reported results represent the average performance across all test samples.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have included the specifications of the hardware and software environments to ensure the reproducibility of our results. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: The research conducted in this paper fully conforms with the NeurIPS Code of Ethics in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper explores how advancements and applications of our findings could benefit society by capturing opinions of users in conversation benefit interpersonal skills. Specifically, we investigate the effectiveness of current state-of-the-art large language models in this context.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our research does not pose any risks for misuse. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have explicitly cited the crawled websites, code and models used. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We open-source the code and the new USDC dataset, and we provide complete documentation on how the dataset was created. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [Yes] Justification: We provide full instructions on how we surveyed our LLM generated annotations using human participants in the Results section. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We use publicly available Reddit user conversations to create the USDC dataset, and we do not collect any new data that would require IRB approval. Guidelines: ** The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_FAIL:21]

The output should follow this structure:
866 (''json
867 {
868 "author1": {
869 "name": "[author_name]",
860 "stance_annotations": [
861 {
862 "id": "[segment_id]",
863 "label": "[chosen_label]",
864 "reason": "[Justification in <50 words]"
865 },
866...
867 ],
868 "dogmatism_label": "[chosen_dogmatism_label]",
869 "dogmatism_reason": "[Justification in <50 words]"
870 },
871 "author2": {
872 "name": "[author_name]",
873 "stance_annotations": [
874 {
875 "id": "[segment_id]",
876 "label": "[chosen_label]",
877 "reason": "[Justification in <50 words]"
878 },
879...
880 ],
881 "dogmatism_label": "[chosen_dogmatism_label]",
882 "dogmatism_reason": "[Justification in <50 words]"
883 }
884 },
885 },
886 """Instructions for Effective Annotation
887
888 1. **Labeling Stance**: For each segment (including the original Reddit submission, comments, or replies) where "author1" or "author2" is mentioned, assign a stance label that best represents the stance
881 expressed towards the discussed topic in the submission. This
882 comprehensive approach ensures no relevant contribution by "author1" or
883 "author2" is overlooked. Evaluate the stance based on the content's tone,
884 argumentation, and engagement level with the topic.
885 2. **Providing Justification**: For each label assigned, include a concise
886 reason, aiming for less than 50 words. Focus on the stance and
887 argumentative indicators present in the text.
888 3. **Dogmatism Assessment**: After reviewing all segments from "author1" and
889 "author2", assign a single dogmatism label reflecting the overall tone
890 and approach in their contributions.
801 """

## Appendix C Prompts for Finetuning SLMs

Fig. 8 and 9 shows the prompts used for finetuning SLMs for the stance and dogmatism classification tasks respectively.

[MISSING_PAGE_FAIL:23]

something. Now days pro gun is interpreting the
* 932Constitution more literal, which leaves even the most
* 933mild policies of anti gun as infringements. To further
* 934compound this anti gun is only considering the most
* 935extreme measures. "Assault Weapons" bans, mandatory
* 936buybacks, red flag laws, etc.. I think at this point
* 937there is just nothing left to talk about. The middle
* 938ground is gone..",
* 939"replies":[
* 940{
* 941":"f3pati9",
* 942"replies":[
* 943{
* 944":"f3pdu44",
* 945"body":"You are exactly right. I'm done with the
* 946idea that there can be real compromise. We
* 947should have at least gotten national
* 948reciprocity and shall-issue in every state in
* 949exchange for what we've given up. Now you
* 950have to be a gooddamn lawyer to exercise your
* 951rights without violating the law."
* 952},
* 953{
* 954"author2":"MikeWillTerminate",
* 955"id":"f3foct0B",
* 956"body":"I am prepared for UBCs, if they do this:
* 95711. Lower the age to buy handguns to 18,
* 958nationwide.
* 9592Repeal the Hughes Amendment:
* 96033. A FOPA-like ban on assault weapon bans (what
* 961the FOPA did with a registry)
* 96244. The punishment for violation is a monetary
* 963fine only
* 96455. A repeal of the GCA ban on foreign NFA weapons
* 9666A repeal of the National Minimum Drinking Age
* 967Act of 1984"
* 968 ]
* 969},
* 970{
* 971"id":"f3pd55z",
* 972"body":"Everytime there is a "compromise" pro gun loses
* 973something. That and today's compromise is tomorrow
* 974's loophole to be closed. All such compromises do
* 975is push that policy off until the next round."
* 976"}
* 977 ]
* 978 {
* 980"id":"f3paf0j",
* 981"body":"Yealth this sub it's not conducive to conversion. Its
* 982quickly devolving to little more than "Boogaloo" memes
* 983and shouting "SHALL. NOT." at each other. However, as
* 984far as I know, the mods won't delete your thread and ban
* 985you from the sub for trying to have a good faith
* 986discussion, like some of the gun control subs will.",
* 987"replies":[
* 988 {
* 989"id":"f3psbm",
* 990"body":"Unfortunately this sub's mod team takes a very
* 991passive approach to moderation. With very little
* 992effort they could make this sub into a quality
* 993progun meeting ground *without having to resort to
* 994censorship*. Instead they promote low-effort memes
* 995and endless duplication of posts through their 996 ination. whubbard has the chops to resurrect this sub. Let's see if he's up to the challenge.",
998 "replies":[
999 {
1000 "id":"f3q8xj6",
1001 "body":"We voted to ban memes last week. All
1002 "body":"We've tried to ban memes last week. All
1002 "replies":[
1004 {
1005 "id":"f3qn4p8",
1006 "body":"Damn I might have to eat some crow here then..."
1008 }
1009 ]
1010 ]
1011 ]
1012 ]
1013 ]
1014 },
1015 {
1016 "id":"f3pafqa",
1017 "body":"Found the gun grabber!!",
1018 "replies":[
1019 {
1020 "id":"f3pcw4h",
1021 "body":"Witch hunter."
1022 }
1023 ]
1024 },
1025 {
1026 "id":"f3pa151",
1027 "body":"I see people have discussions when it makes sense to.
1028 Not much reason to spend time responding to the same gun
1029 "control measures over and over though."
1030 },
1031 {
1032 "id":"f3paw3h",
1033 "body":"I get where you're coming from, but people's ability
1034 "to protect themselves and own their own property isn't
1035 "something that is compromisingable. Anything less, and they
1036 "cease to own their own property. It's like breathing,
1037 "there can be nothing less than total ability to breath
1038 "when and how someone wants. It's just that simple."
1039 },
1040 {
1041 "id":"f3pax9m",
1042 "body":"My take on this, What kind of open discussion is
1043 "possible for a right that is guaranteed and most
1044 "importantly, not to be infringed upon? They're making all
1045 "thes unlawful laws to portray it as it's somehow
1046 "legitimate. They are not, We are at an apex, to which
1047 "both political spectrums and even us to a degree are
1048 "liable for.\nI certainly believe both sides are waiting
1049 "for this to boil over so each can finger point. I just
1050 "speculate it's going to be the hell humanity been
1051 "whispering about but never thought it would ever occur."
1052 },
1053 {
1054 "id":"f3bp6ny",
1055 "body":"The time for discussion is over."
1056 },
1057 {
1058 "id":"f3pfqwq",
1059 "body":"I don't know what you're talking about. Sure people
1060 "downvote, but they also talk. We get "why do you need guns" posts at least weekly, and several people will
1062 engage in actual conversation with them, citing facts,
1063 clearing up statistics, and telling stories to illustrate
1064 why this is important to them, but they are usually met
1065 with "you stupid @#$%, you think you're Rambo" or
1066 something equally clever. People who come here to discuss
1067 and learn will be treated well. People who are just
1068 trolling are treated like trolls.",
1069 "replies":[
1070 { { "author1":"rookerin0",
1072 "id":"f3pghji",
1073 "body":"I made this post because I'm always seeing
1074 rational, conversation seeking comments getting
1075 blown to downvote hell.",
1076 "replies":[
1077 { "id":"f3p19xv",
1079 "body":"[Like this one?]([https://www.reddit.com/r/](https://www.reddit.com/r/)
1080 proof/comments/dhcu92/yup/f3p75tg/)> One smart
1081 man in a sub full of... welp... "strong opinions
1082 ". You start off with arrogance, as the sole
1083 arbiter of what constitutes a "smart man". Then
1084 you back it up with a dismissive swipe at what
1085 you term "strong opinions".> Every other country
1086 can see that PROPER gun control reduces gun
1087 violence by a ton, More arrogance. False
1088 equivalence. Unsupported claims.> but the US
1089 refuses to let go of it's antique laws In a
1090 shocking turn of events, more arrogance.> Fully
1091 aware that this is a fully pro gun sub, willing
1092 to take the downvotes in order to spark a
1093 discussion and crack some heads. You aren't the
1094 first arrogant asshole to grace this sub with
1095 posts like this. Try bringing something other
1096 than your own self-importance to the discussion.
1097 Edit: And then there's [this gen]([https://www.reddit.com/r/unpopularopinion/comments/d3w5z1/people_living_in_the_us_are_living_in_one_of_the/f06r3sg/](https://www.reddit.com/r/unpopularopinion/comments/d3w5z1/people_living_in_the_us_are_living_in_one_of_the/f06r3sg/).> Wanna feel like you could be shot at
1101 very single moment? Move to the US, it'll prob
1102 happen to you either as a bystander, or you'd be
1103 shot by a random citizen (sometimes police)."
1104 },
1105 {
1106 "id":"f3pj8k0",
1107 "body":"As is tradition. We're done with that
1108 condescending bullshit from antis, you dont
1109 come here for good faith discussion and
1110 whether you get a reasonable response or not,
1111 nothing ever changes, easier to downvote you
1112 and move on because we get the same
1113 treatment anytime we attempt to speak out in
1114 antisbs."
1115 },
1116 { "id":"f3plgf4",
1118 "body":"If downvotes hurt your feelings, you
1119 shouldn't be on reddit. People tend to
1120 downvote anything they disagree with (which
1121 is why some subs specifically ask you to only
1122 downvote things that contribute nothing to
1123 the discussion). It's a bad habit, but that's
1124 the way it is. People downvote and *still*
1125 engage. You want to post a view contrary to 1126 the prevailing view of the sub, take your
1127 lumps and participate in what conversation
1128 you are offered. But if you're only here to
1129 preach about how stupid, misguided, unevolved
1130, uneducated, irrational, and/or violent we
1131,
1132 {
1133 {
1134 "id":"f3tcf1",
1135 "body":"An arrogant Israeli trying to tell
1136 another nation how they should be run. You're
1137 just a walking stereotype aren't you? And
1138 before you say anything, I popped into your
1139 comment history. That's where the calling you
1140 Israel comes from.",
1141 "replies":[
1142 {
1143 "author1":"rookerin0",
1144 "id":"f3tyvb4",
1145 "body":"I thought that trying to tell other
1146 mentions how they should run was your
1147 guys's stereotype.",
1148 "replies":[
1149 {
1150 "id":"f3u0vkq",
1151 "body":"No we go in and try to make
1152 them work our way."
1153 }
1154 ]
1155 ]
1156 ]
1157 ]
1158 ]
1159 ]
1160 ]
1161 },
1162 {
1163 "id":"f3pzseh",
1164 "body":"It's a little unfortunate but the grabbers who come
1165 on heret to be intellectually dishonest and/or
1166 uninformed. There was some Australian post a few days ago
1167 that pretty much asked why we like our guns more than
1168 children. No discussion to be had there. There's also
1169 some posts that clearly demonstrate the poster should
1170 inform himself or herself a little."
1171 },
1172 {
1173 "author2":"MikeWillTerminate",
1174 "id":"f3rqu2u",
1175 "body":"Actually, do that. It shows everyone that they tend
1176 to be crazy, unstable, ignorant, stereotyping, arrogant
1177 - bastards who hate black people with a hair trigger."
1178 },
1179 {
1180 "id":"f3t7tgg",
1181 "body":"Welcome to reddit, home of every single safe place
1182 for anything that doesn't violate the TOS. At least its
1183 slightly better than r/politics"
1184 },
1185 {
1186 "id":"f3unt9z",
1187 "body":"This isn't r/gundebate. This is a pro gun subreddit.
1188 That said, we do allow some debate provided it remains
1189 civil.",
1190 "replies":[1191 {  "authorl":"rookerin0",  "id":"f3uonn2",  "body":"Sallytho,r/gundebateisprettydead..."  "195 }  1196 ]  1197 },  1198 {  "id":"f44dip6o",  "body":"Anythingelseyouwanttogiveawayforfree?"  1201 }  1202 ]  1203 }
1204 ### Response
1207 E Samples of JSON Outputs from LLMs
1208 E.1 Sample JSON Output from LLM: GPT-4 Zero-shot
1209
1210 {  "authorl":{  "name":"rookerin0",  "stance_annotations":[  1215 {  "id":"dhoxyz",  "label":"somewhat_in_favor",  "reason":"Advocatesforopendiscussions,showinginterestin  1219  dialogue."  1220 },  1221 {  "id":"f3pghi",  1223 "label":"somewhat_in_favor",  1224 "reason":"Expressdesireforrationaldiscussions,despite  1225 downvotes."  1226 },  1227 {  1228 "id":"f3tyvb4",  1229 "label":"somewhat_against",  1230 "reason":"Sarcasticretort,butlacksstrongemotional  1231 language."  1232 },  1233 {  1234 "id":"f3uom2",  1235 "label":"stance_not_inferrable",  1236 "reason":"Commentisfactual,noclearstanconthedebate  1237 itiself."  1238 },  1239 ],  1240 "dogmatism_label":"OpentoDialogue",  1241 "dogmatism_reason":"Showsinterestindiscussion,acknowledges  1242 downvotes."  1243 },  1244 "author2":{  1245 "name":"MikeWillTerminate",  1246 "stance_annotations":[  1247 {  1248 "id":"f3rt0bf",  1249 "label":"somewhat_in_favor",  1250 "reason":"ProposesconditionsforUBCs,indicatingsome  1251  openness."  1252 },

[MISSING_PAGE_FAIL:29]

1316 ],
1317 "dogmatism_label": "Firm but Open",
1318 "dogmatism_reason": "While showing strong opinions, also lists specific conditions for compromise."
1319 }
1320 }
1321
1322
1323

### Sample JSON Output from LLM: GPT-4 large Few-shot
1324 {
1325 {
1326 "author1": {
1327 "name": "rookerin0",
1328 "stance_annotations": [
1329 {
1330 "id": "dhoxyz",
1331 "label": "somewhat_in_favor",
1332 "reason": "Expresses desire for open discussions, indicating a stance in favor of dialogue."
1333 },
1334 {
1335 {
1336 "id": "f3pghji",
1337 "label": "somewhat_in_favor",
1338 "reason": "Advocates for rational conversation despite
1339 },
1340 },
1341 {
1342 "id": "f3tywb4",
1343 "label": "somewhat_against",
1344 "reason": "Uses sarcasm to counter a stereotype, showing a
1345  stance against it."
1346 },
1347 {
1348 "id": "f3uom2",
1349 "label": "stance_not_inferrable",
1350 "reason": "Comment on the state of another subreddit does not
1351  reveal a stance on the gun debate."
1352 }
1353 ],
1354 "dogmatism_label": "Open to Dialogue",
1355 "dogmatism_reason": "Shows a consistent
1356  preference for discussion over confrontation."
1357 },
1358 "author2": {
1359 "name": 'MikeWillTerminate",
1360 "stance_annotations": [
1361 {
1362 "id": 'f3rt0bf",
1363 "labe":'somewhat_in_favor",
1364 "reason": 'Lists conditions for accepting UBCs, indicating
1365  some openness to compromise."
1366 },
1367 {
1368 "id": 'f3rqu2u",
1369 "label": "strongly_against",
1370 "reason": "Uses derogatory language to describe opponents,
1371 } showing strong opposition."
1372 }
1373 ],
1374 "dogmatism_label": "Firm but Open",
1375 "dogmatism_reason": "While showing strong opinions, also suggests
1376  conditions for policy acceptance."
1377 }
1378 }

1380 E.4 Sample JSON Output from LLM: Mistral Large Zero-shot
1381 {
1382 {
1383 "author1":{
1384 "name": "rookerin0",
1385 "stance_annotations":[
1386 {
1387 "id": "dhoxyz",
1388 "label": "somewhat_in_favor",
1389 "reason": "Author expresses desire for open discussions on anti-gun ideals."
1391 },
1392 {
1393 "id": "f3pghi",
1394 "label": "stance_not_inferrable",
1395 "reason": "Author is commenting on downvotes, not expressing stance."
1396 },
1397 },
1398 {
1399 "id": "f3tywb4",
1400 "label": "stance_not_inferrable",
1401 "reason": "Author is making a stereotypical comment, not expressing stance."
1402 },
1403 },
1404 {
1405 "id": "f3uom2",
1406 "label": "stance_not_inferrable",
1407 "reason": "Author is commenting on the state of another
1408 ubertdit, not expressing stance."
1409 }
1410 ],
1411 "dogmatism_label": "Open to Dialogue",
1412 "dogmatism_reason": "Author expresses desire for open discussions
1413  and does not dismiss opposing views."
1414 },
1415 "author2":{
1416 "name": "MikeWillTerminate",
1417 "stance_annotations":[
1418 {
1419 "id": "f3rt0bf",
1420 "label": "strongly_in_favor",
1421 "reason": "Author lists conditions for accepting UBCs, showing
1422 strong pro-gun stance."
1423 },
1424 {
1425 "id": "f3ru2u",
1426 "label": "strongly_in_favor",
1427 "reason": "Author's comment is dismissive of anti-gun
1428 advocates, showing strong pro-gun stance."
1429 }
1430 ],
1431 "dogmatism_label": "Deeply Rooted",
1432 "dogmatism_reason": "Author's comments are dismissive of opposing
1433  views and show a strong, unchangeable belief in pro-gun ideals
1434."
1435 }
1439 }
1438

### Sample JSON Output from LLM: Mistral Large One-shot
1439
1440 {
1441 "author1":{
1442 "name": "rookerin0","stance_annotations":[
1444 {
1445 "id": "dhoxyz",
1446 "label": "somewhat_in_favor",
1447 "reason": "Expresses desire for open discussions on gun
1448 {
1449 },
1450 {
1451 "id": "f3pghji",
1452 "label": "stance_not_inferrable",
1453 "reason": "Comments on downvotes, but no clear stance on gun
1454 control."
1455 },
1456 {
1457 "id": "f3tywb4",
1458 "label": "stance_not_inferrable",
1459 "reason": "Comment about stereotypes, but no clear stance on
1460 {
1461 },
1462 {
1463 "id": "f3uomm2",
1464 "label": "stance_not_inferrable",
1465 "reason": "Mentions r/gundebate being dead, but no clear
1466 {
1467 }
1468 ],
1469 "dogmatism_label": "Open to Dialogue",
1470 "dogmatism_reason": "Seeks open discussions and does not express
1471 strong dogmatic views."
1472 },
1473 "author2": {
1474 "name": "MikeWillTerminate",
1475 "stance_annotations":[
1476 {
1477 "id": "f3rt0bf",
1478 "label": "somewhat_in_favor",
1479 "reason": "Lists conditions for supporting UBCs, showing
1480 {
1481 },
1482 {
1483 "id": "f3ru2u",
1484 "label": "strongly_in_favor",
1485 "reason": "Strong negative language against gun control
1486 advocates, suggesting strong pro-gun stance."
1487 }
1488 ],
1489 "dogmatism_label": "Firm but Open",
1490 "dogmatism_reason": "Expresses strong pro-gun views but open to
1491 {
1492 }
1493 }
1495 ```
LAM

### Sample JSON Output from LLM: Mistral Large Few-shot
```
1496 {
1497 {
1498 "author1":{
1499 "name": "rookerin0",
1500 "stance_annotations":[
1501 {
1502 "id": "dhoxyz",
1503 "label": "somewhat_in_favor",
1504 "reason": "Expresses desire for open discussions on anti vs.
1505 "pro-gun debates."* 1506 },
* 1507 {
* 1508 "id": "f3pghi",
* 1509 "label": "stance_not_inferrable",
* 1510 "reason": "Comments on downvotes, but no clear stance on the topic."
* 1511 },
* 1513 {
* 1514 "id": "f3tyvb4",
* 1515 "label": "stance_not_inferrable",
* 1516 "reason": "Makes a stereotypical comment, but no clear stance
* 1517 "on the topic."
* 1518 },
* 1519 {
* 1520 "id": "f3uom2",
* 1521 "label": "stance_not_inferrable",
* 1522 "reason": "Mentions r/gundebate being dead, but no clear
* 1523 "stance on the topic."
* 1524 }
* 1525 ],
* 1526 "dogmatism_label": "Open to Dialogue",
* 1527 "dogmatism_reason": "Seeks open discussions and engages in conversation without strong dogmatic language."
* 1529 },
* 1530 "author2": {
* 1531 "name": "MikeWillTerminate",
* 1532 "stance_annotations": [
* 1533 {
* 1534 "id": "f3rt0bf",
* 1535 "label": "somewhat_in_favor",
* 1536 "reason": "Lists conditions for supporting UBCs, showing openness to discussion."
* 1537 },
* 1539 {
* 1540 "id": "f3ru2u",
* 1541 "label": "strongly_against",
* 1542 "reason": "Uses derogatory language to express strong opposition to 'grabbers'."
* 1544 }
* 1545 ],
* 1546 "dogmatism_label": "Firm but Open",
* 1547 "dogmatism_reason": "Expresses strong opinions but also shows willingness to consider certain conditions for compromise."
* 1549 }
* 1550 }

Figure 10: Confusion matrix for LLaMa-3-8B Dogmatism detection models on USDC test set: finetuning (left) and instruction-tuning (right). Here, DR: Deeply Rooted, FX: Flexible, FBO: Firm but Open, OTD: Open to Dialogue

[MISSING_PAGE_FAIL:34]

Figure 12: LLaMa-3-8B finetuning for dogmatism task: Visualize the ROC curves for each class along with their AUC values for GPT-4 Annotations across zero-shot, one-shot, few-shot and majority labels.

Figure 13: LLaMa-3-8B instruction-tuning for stance detection task: Visualize the ROC curves for each class along with their AUC values for GPT-4 Annotations across zero-shot, one-shot, few-shot and majority labels.