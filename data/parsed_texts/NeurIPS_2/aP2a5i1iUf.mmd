# Understanding Mode Connectivity via

Parameter Space Symmetry

 Bo Zhao

University of California San Diego

bozhao@ucsd.edu

&Nima Dehmamy

IBM Research

nima.dehmamy@ibm.com

Robin Walters

Northeastern University

r.walters@northeastern.edu

&Rose Yu

University of California San Diego

roseyu@ucsd.edu

###### Abstract

It has been observed that the global minimum of neural networks is connected by curves on which train and test loss is almost constant. This phenomenon, often referred to as mode connectivity, has inspired various applications such as model ensembling and fine-tuning. Despite empirical evidence, a theoretical explanation is still lacking. We explore the connectedness of minimum through a new approach, parameter space symmetry. By relating topology of symmetry groups to topology of minima, we provide the number of connected components of full-rank linear networks. In particular, we show that skip connections reduce the number of connected components. We then prove mode connectivity up to permutation for linear networks. We also provide explicit expressions for connecting curves in minimum induced by symmetry.

## 1 Introduction

Among recent studies on the loss landscape, a particularly interesting discovery is mode connectivity [5, 10], which refers to the phenomenon that distinct minima found by stochastic gradient descent (SGD) can be connected by continuous paths through the high-dimensional parameter space of neural networks. Mode connectivity has implications on other phenomena in deep learning such as the lottery ticket hypothesis [8] and loss landscape and training trajectory analysis [11]. Additionally, mode connectivity has inspired applications in diverse fields, including model ensembling [10, 2, 3], model averaging [15, 30], pruning [8], improving adversarial robustness [34], and fine-tuning for altering prediction mechanism [20].

Discrete symmetry, especially permutation, is well-known to be related to mode connectivity. In particular, the neural network output is invariant to permuting the neurons [13]. [6] conjectures that all minima found by SGD are linearly connected up to permutation. Various algorithms have since been developed to find the optimal permutation for linear mode connectivity [1]. However, compared to discrete symmetry, the role of continuous symmetry remains less studied. Continuous symmetry groups with continuous actions define positive dimensional connected spaces in the minimum [32]. We explore the connectedness of minimum through continuous symmetries in the parameter space.

We reveal the role of symmetry in the connectivity of minimum by relating properties of topological groups to their orbits and the minimum. Our results show that both continuous and discrete symmetry are important and useful in understanding the origin and failure cases of mode connectivity. Our work highlights a new approach towards understanding the topology of the minimum and complements previous theories on mode connectivity [31, 9, 23, 24, 18, 27, 25].

Connectedness of minima

### Linear network with invertible weights

Let **Param** be the space of parameters. Consider the multi-layer loss function \(L:\textbf{Param}\rightarrow\mathbb{R}\),

\[L:\textbf{Param}\rightarrow\mathbb{R},\hskip 56.905512pt(W_{1},...,W_{l}) \mapsto||Y-W_{1}...W_{1}X||_{2}^{2}. \tag{1}\]

where \(X,Y\in\mathbb{R}^{h\times h}\) are the input and output of the network. In this subsection, we assume that both \(X,Y\) have rank \(h\), and \(\textbf{Param}=(\mathbb{R}^{h\times h})^{l}\). Then \(L\) has a \(GL_{h}(\mathbb{R})^{l-1}\) symmetry, which acts on **Param** by \(g\cdot(W_{1},...,W_{l})=(g_{1}W_{1},g_{2}W_{2}g_{1}^{-1},...,g_{l-1}W_{l-1}g_{ l-2}^{-1},W_{l}g_{l-1}^{-1})\), for \((g_{1},...,g_{l-1})\in GL_{h}(\mathbb{R})^{l-1}\).

Let \(L^{-1}(c)=\{\theta\in\textbf{Param}:L(\theta)=c\}\) be a level set of \(L\). Since \(\|\cdot\|_{2}\geq 0\) and \(L^{-1}(0)\neq\emptyset\), the minimum value of \(L\) is 0. By relating the topology of \(GL(\mathbb{R})\) and \(L^{-1}(0)\), we have the following observations on the structure of the minimum of \(L\).

**Proposition 2.1**.: _There is a homeomorphism between \(L^{-1}(0)\) and \((\mathrm{GL}_{h})^{l-1}\)._

Since \((\mathrm{GL}_{h})^{l-1}\) has \(2^{l-1}\) connected components and homeomorphism preserves topological properties, \(L^{-1}(0)\) also has \(2^{l-1}\) connected components.

**Corollary 2.2**.: _The minimum of \(L\) has \(2^{l-1}\) connected components._

### ResNet with 1D weights

The topological properties of the minimum depend on the architecture. As an example of this dependency, we show that adding a skip connection changes the number of connected components of the minimum.

Consider a residual network \(W_{3}(W_{2}W_{1}X+\varepsilon X)\) and loss function

\[L(W_{3},W_{2},W_{1})=||Y-W_{3}(W_{2}W_{1}X+\varepsilon X)||_{2}, \tag{2}\]

where \((W_{1},W_{2},W_{3})\in\textbf{Param}=\mathbb{R}^{n\times n}\times\mathbb{R}^{n \times n}\times\mathbb{R}^{n\times n}\), \(\varepsilon\in\mathbb{R}\), and data \(X\in\mathbb{R}^{n\times n},Y\in R^{n\times n}\). The following proposition states that for a three-layer residual network with weight matrices of dimension \(1\times 1\), the number of components of the minimum is smaller than that of a linear network without the skip connection.

**Proposition 2.3**.: _Let \(n=1\). Assume that \(X,Y\neq 0\). When \(\varepsilon=0\), the minimum of \(L\) has 4 connected components. When \(\varepsilon\neq 0\), the minimum of \(L\) has 3 connected components._

The \(\varepsilon=0\) case follows from Corollary 2.2. For the \(\varepsilon\neq 0\) case, the proof decomposes the minimum of \(L\) into two sets \(S_{1}\) and \(S_{0}\), corresponding to the minima without the skip connection and an extra set of solutions because of the skip connection. \(S_{1}\) is homeomorphic to \(GL_{1}\times GL_{1}\) and has 4 connected components. \(S_{0}\) is a line and has 1 connected component. Two components of \(S_{1}\) are connected to \(S_{0}\), while the other two components of \(S_{1}\) are not. Therefore, \(S_{0}\) connects two components of \(S_{1}\). As a result, the minimum of \(L\) has 3 connected components. Full proof can be found in Appendix C.3.

Figure 1 visualizes the minimum without and with the skip connection. This result reveals the effect of skip connection on the connectedness of minimum, which may lead to a new explanation of the effectiveness of ResNets [12] and DenseNets [14]. We leave the connection between the topology of minimum and the optimization and generalization property of neural networks to future work.

## 3 Mode connectivity

From the examples in the previous section, the connectedness of the minimum is related to the symmetry of the loss function under certain conditions. In this section, we explore applications of this insight in explaining mode connectivity.

### Mode connectivity up to permutation

For the family of linear neural networks defined in Section 2.1, we show that permutation allows us to connect points in the minimum that are not connected without permutation. Our results support the empirical observation that neuron alignment by permutation improves mode connectivity [29].

[MISSING_PAGE_EMPTY:3]

[MISSING_PAGE_FAIL:4]

## Acknowledgments and Disclosure of Funding

We thank Iordan Ganev for helpful comments on proofs in Appendix B. This work was supported in part by the U.S. Army Research Office under Army-ECASE award W911NF-07-R-0003-03, the U.S. Department Of Energy, Office of Science, IARPA HAYSTAC Program, NSF Grants #2205093, #2146343, and #2134274. R. Walters is supported by NSF grants #2107256 and #2134178.

## References

* [1] Samuel K. Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. GitHub re-basin: Merging models modulo permutation symmetries. _International Conference on Learning Representations_, 2023.
* [2] Gregory Benton, Wesley Maddox, Sanae Lotfi, and Andrew Gordon Wilson. Loss surface simplexes for mode connecting volumes and fast ensembling. In _International Conference on Machine Learning_, pages 769-779. PMLR, 2021.
* [3] Frederik Benzing, Simon Schug, Robert Meier, Johannes Von Oswald, Yassir Akram, Nicolas Zucchet, Laurence Aitchison, and Angelika Steger. Random initialisations performing above chance and how to find them. _14th Annual Workshop on Optimization for Machine Learning (OPT2022)_, 2022.
* [4] Johanni Brea, Berlin Simsek, Bernd Illing, and Wulfram Gerstner. Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape. _arXiv preprint arXiv:1907.02911_, 2019.
* [5] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In _International conference on machine learning_, pages 1309-1318. PMLR, 2018.
* [6] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. _International Conference on Learning Representations_, 2022.
* [7] Damien Ferbach, Baptiste Goujaud, Gauthier Gidel, and Aymeric Dieuleveut. Proving linear mode connectivity of neural networks via optimal transport. _arXiv preprint arXiv:2310.19103_, 2023.
* [8] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In _International Conference on Machine Learning_, pages 3259-3269. PMLR, 2020.
* [9] C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. In _5th International Conference on Learning Representations, ICLR_, 2017.
* [10] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. _Advances in neural information processing systems_, 31, 2018.
* [11] Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. Using mode connectivity for loss landscape analysis. _35th International Conference on Machine Learning's Workshop on Modern Trends in Nonconvex Optimization for Machine Learning_, 2018.
* [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [13] Robert Hecht-Nielsen. On the algebraic structure of feedforward network weight spaces. In _Advanced Neural Computers_, pages 129-135. Elsevier, 1990.
* [14] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4700-4708, 2017.

* [15] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. _Conference on Uncertainty in Artificial Intelligence_, 2018.
* [16] Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, and Behnam Neyshabur. Repair: Renormalizing permuted activations for interpolation repair. _International Conference on Learning Representations_, 2023.
* [17] Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, Joao Sedoc, and Naomi Saphra. Linear connectivity reveals generalization strategies. _International Conference on Learning Representations_, 2023.
* [18] Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Rong Ge, and Sanjeev Arora. Explaining landscape connectivity of low-cost solutions for multilayer nets. _Advances in neural information processing systems_, 32, 2019.
* [19] John Lee. _Introduction to topological manifolds_, volume 202. Springer Science & Business Media, 2010.
* [20] Ekdeep Singh Lubana, Eric J Bigelow, Robert P Dick, David Krueger, and Hidenori Tanaka. Mechanistic mode connectivity. In _International Conference on Machine Learning_, pages 22965-23004. PMLR, 2023.
* [21] Hancheng Min, Salma Tarmoun, Rene Vidal, and Enrique Mallada. On the explicit role of initialization on the convergence and implicit bias of overparametrized linear networks. In _International Conference on Machine Learning_, pages 7760-7768. PMLR, 2021.
* [22] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? _Advances in neural information processing systems_, 33:512-523, 2020.
* [23] Quynh Nguyen. On connected sublevel sets in deep learning. In _International conference on machine learning_, pages 4790-4799. PMLR, 2019.
* [24] Quynh Nguyen. A note on connectivity of sublevel sets in deep learning. _arXiv preprint arXiv:2101.08576_, 2021.
* [25] Quynh N Nguyen, Pierre Brechet, and Marco Mondelli. When are solutions connected in deep networks? _Advances in Neural Information Processing Systems_, 34:20956-20969, 2021.
* [26] Fabrizio Pittorino, Antonio Ferraro, Gabriele Perugini, Christoph Feinauer, Carlo Baldassi, and Riccardo Zecchina. Deep networks on toroids: Removing symmetries reveals the structure of flat regions in the landscape geometry. In _Proceedings of the 39th International Conference on Machine Learning_, pages 17759-17781, 2022.
* [27] Alexander Shevchenko and Marco Mondelli. Landscape connectivity and dropout stability of sgd solutions for over-parameterized neural networks. In _International Conference on Machine Learning_, pages 8773-8784. PMLR, 2020.
* [28] Berlin Simsek, Francois Ged, Arthur Jacot, Francesco Spadaro, Clement Hongler, Wulfram Gerstner, and Johanni Brea. Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances. In _International Conference on Machine Learning_, pages 9722-9732. PMLR, 2021.
* [29] Norman Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, and Rongjie Lai. Optimizing mode connectivity via neuron alignment. _Advances in Neural Information Processing Systems_, 33:15300-15311, 2020.
* [30] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In _International Conference on Machine Learning_, pages 23965-23998. PMLR, 2022.

* [31] David Yunis, Kumar Kshitij Patel, Pedro Henrique Pamplona Savarese, Gal Vardi, Jonathan Frankle, Matthew Walter, Karen Livescu, and Michael Maire. On convexity and linear mode connectivity in neural networks. In _OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)_, 2022.
* [32] Bo Zhao, Iordan Ganev, Robin Walters, Rose Yu, and Nima Dehmamy. Symmetries, flat minima, and the conserved quantities of gradient flow. _International Conference on Learning Representations_, 2023.
* [33] Bo Zhao, Robert M Gower, Robin Walters, and Rose Yu. Improving convergence and generalization using parameter symmetries. _arXiv preprint arXiv:2305.13404_, 2023.
* [34] Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, and Xue Lin. Bridging mode connectivity in loss landscapes and adversarial robustness. _International Conference on Learning Representations_, 2020.
* [35] Zhanpeng Zhou, Yongyi Yang, Xiaojiang Yang, Junchi Yan, and Wei Hu. Going beyond linear mode connectivity: The layerwise linear feature connectivity. _arXiv preprint arXiv:2307.08286_, 2023.

## Appendix A Related Work

Mode connectivity[10] and [5] discover empirically that the global minimum of neural networks are connected by curves on which train and test loss is almost constant. It is then observed that SGD solutions are linearly connected if they are trained from pre-trained weights [22] or share a short period of training at the beginning [8]. Additionally, neuron alignment by permutation improves mode connectivity [29]. Then, [6] conjecture that all minima found by SGD are linearly connected up to permutation. Following the conjecture, [1] develop algorithms that finds the optimal alignment for linear mode connectivity, and [16] further reduce the barrier by rescaling the preactivations of interpolated networks. A few papers propose theoretical explanation of linear mode connectivity using different tools. [35] shows that the feature maps of each layer are also linearly connected and identify conditions that guarantees linear connectivity. [31] seeks to explain linear mode connectivity through finding a convex hull defined by SGD trajectory endpoints. [7] uses optimal transport theory to prove that wide two-layer neural networks trained using SGD are linearly connected with high probability. It is worth noting that linear mode connectivity does not always hold outside of computer vision. Language models that are not linearly connected have different generalization strategies [17]. [20] further show that the lack of linear connectivity indicates that the two models rely on different attributes to make predictions.

Theory on connectedness of minimumSeveral work explores the theoretical explanation of mode connectivity by studying the connectedness of sub-level sets. [9] shows that the minimum is connected for 2-layer linear network without regularization, and for deeper linear networks with \(L2\) regularization. Futhermore, they show that the minimum of a two-layer ReLU network is asymptotically connected, that is, there exists a path connecting any two solutions with bounded error. [23] proves that the sublevel sets are connected in pyramidal networks with piecewise linear activation functions and first hidden layer wider than \(2N\), where \(N\) is the number of training data). The width requirement is later improved to \(N+1\)[24]. [18] prove the existence of a piece-wise linear path between two solutions for ReLU networks, if they are both dropout stable, or both noise stable and sufficiently overparametrized. [27] generalizes this proof to show that wider neural networks are more connected, following the observation that SGD solutions for wider neural network are more dropout stable. [25] gives a new upper bound of the loss barrier between solutions using the loss of sparse subnetworks that are optimized, which is a milder condition than dropout stability.

Symmetry in the loss landscapeDiscrete symmetries have inspired a line of work on loss landscape topology. [4] shows that permutations of a given layer are connected within a loss level set. Through examining the permutation symmetries, [28] characterize the geometry of the global minima manifold for networks without other symmetries and show that adding one neuron to each layer in a minimal network connects the permutation equivalent global minima. By removing permutation and rescaling symmetries, [26] study the geometry of minima in the functional space. [32] finds a set of nonlinear continuous symmetries that partially parametrizes the minimum. [33] uses symmetry induced curves to approximate the curvature of the minimum.

## Appendix B Background

In this section, we review mathematical concepts used in the paper and list some useful results on the number of connected components of topological spaces. We refer readers to [19] for a more detailed introduction to this topic.

### Connected components

Consider two topological spaces \(X\) and \(Y\). A map \(f:X\to Y\) is _continuous_ if for every open subset \(U\subseteq Y\), its preimage \(f^{-1}(U)\) is open in \(X\). If \(X\) and \(Y\) are metric spaces with metrics \(d_{X}\) and \(d_{Y}\) respectively, this is equivalent to the delta-epsilon definition. That is, \(f\) is continuous if at every \(x\in X\), for any \(\epsilon>0\) there exists \(\delta>0\) such that \(d_{X}(x,y)<\delta\) implies \(d_{Y}(f(x),f(y))<\epsilon\) for all \(y\in Y\).

A topological space is _connected_ if it cannot be expressed as the union of two disjoint, nonempty, open subsets. A topological space \(X\) is _path connected_ if for every \(p,q\in X\), there is a continuous map \(f:[0,1]\to X\) such that \(f(0)=p\) and \(f(1)=q\). Path connectedness implies connectedness, but the converse is not true [19]. [23] studies the path connectedness of sublevel sets of loss functions.

The following theorem is the main intuition of this paper and will appear frequently in proofs.

**Theorem B.1** (Theorem 4.7 in [19]).: _Let \(X,Y\) be topological spaces and let \(f:X\to Y\) be a continuous map. If \(X\) is connected, then \(f(X)\) is connected._

A map \(f\) is a _homeomorphism_ from \(X\) to \(Y\) if \(f\) is bijective and both \(f\) and \(f^{-1}\) are continuous. \(X\) and \(Y\) are _homeomorphic_ if such a map exists. A _(connected) component_ of a topological space \(X\) is a maximal nonempty connected subset of \(X\). The components of \(X\) form a partition of \(X\). The next two corollaries of Theorem B.1 show that connectedness and the number of connected components are topological properties. That is, they are preserved under homeomorphisms.

**Corollary B.2**.: _Let \(f:X\to Y\) be a homeomorphism from \(X\) to \(Y\), and let \(U\subseteq X\) be a subset of \(X\) with the subspace topology. Then \(U\) is connected if and only if \(f(U)\subseteq Y\) is connected._

Proof.: By the definition of homeomorphism, \(f\) and \(f^{-1}\) are continuous. From Theorem B.1, if \(U\in X\) is connected, then \(f(U)\in Y\) is connected. Similarly, if \(f(U)\) is connected, then \(f^{-1}(f(U))=U\) is connected. 

**Corollary B.3**.: _Let \(X\) be a topological space that has \(N\) components. Let \(Y\) be a topological space homeomorphic to \(X\). Then \(Y\) has \(N\) components._

Proof.: Let \(C_{1},...,C_{N}\) be the components of \(X\). Let \(f\) be a homeomorphism from \(X\) to \(Y\). Since \(f\) is bijective and \(C_{1},...,C_{N}\) is a partition of \(X\), \(f(C_{1}),...,f(C_{N})\) is a partition of \(Y\). From Theorem B.1, since \(C_{1},...,C_{N}\) are all connected, so are \(f(C_{1}),...,f(C_{N})\).

Lastly, we need to show that \(f(C_{1}),...,f(C_{N})\) are maximally connected. Suppose there exists a set \(U\subseteq Y\), such that \(U\not\subseteq f(C_{i})\) and \(f(C_{i})\cup U\) is connected for some \(i\). Then by Theorem B.1, \(f^{-1}(f(C_{i})\cup U)\supset C_{i}\) is connected in \(X\). This contradicts the fact that \(C_{i}\) is a maximal component in \(X\). Therefore, \(f(C_{1}),...,f(C_{N})\) are maximally connected.

Since \(f(C_{1}),...,f(C_{N})\) partitions \(Y\) and are maximally connected, \(Y\) has \(N\) components. 

Another consequence of Theorem B.1 is the following upper bound on the number of components of the image of a continuous map.

**Proposition B.4**.: _Let \(f:X\to Y\) be a continuous map. The number of components of the image \(f(X)\subseteq Y\) is at most the number of components of \(X\)._Proof.: Let \(C_{1},...,C_{N}\) be the components of \(X\). Since \(C_{i}\) is continuous and the action is continuous, according to Theorem B.1, \(f(C_{i})\) is continuous for all \(i\in\{1,...,N\}\). Additionally, since \(\bigcup_{i=1}^{N}C_{i}=X\), we have \(\bigcup_{i=1}^{N}f(C_{i})=f(X)\). Therefore, there is a surjective map from \(\{f(C_{1}),...,f(C_{N})\}\) to the set of components of \(f(X)\), which implies that \(f(X)\) has at most \(N\) components. 

Let \(X_{1},...,X_{n}\) be topological spaces. The _product space_ is their Cartesian product \(X_{1}\times...\times X_{n}\) endowed with the product topology. Denote \(\pi_{0}(X)\) as the set of connected components of a space \(X\). The following proposition provides a way to count the components of a product space.

**Proposition B.5**.: _Consider \(n\) topological spaces \(X_{1},...,X_{n}\). Then \(|\pi_{0}(X_{1}\times...\times X_{n})|=\prod_{i=0}^{n}|\pi_{0}(X_{i})|\)._

Proof.: When \(n=1\), the number of components of the product space is \(|\pi_{0}(X_{1})|\).

For the \(n>1\) case, since \(X_{1}\times...\times X_{n}=(X_{1}\times...\times X_{n-1})\times X_{n}\), it suffices to show that \(|\pi_{0}(A\times B)|=|\pi_{0}(A)||\pi_{0}(B)|\) for any topological spaces \(A\) and \(B\). Let \(f:\pi_{0}(A)\times\pi_{0}(B)\rightarrow\pi_{0}(A\times B)\) be the map that assigns \(C\in\pi_{0}(A)\times\pi_{0}(B)\) to the element in \(\pi_{0}(A\times B)\) that contains \(C\). Then \(f\) is surjective because \(\pi_{0}(A)\times\pi_{0}(B)\) forms a partition of \(A\times B\). To prove that \(f\) is injective, suppose that \(f(C_{1})=f(C_{2})\) for \(C_{1},C_{2}\in\pi_{0}(A)\times\pi_{0}(B)\). Consider the projection \(\pi_{A}:A\times B\to A\). Since \(\pi_{A}\) is continuous and \(C_{1},C_{2}\) belong to the same component of \(A\times B\), \(\pi_{A}(C_{1})\) and \(\pi_{A}(C_{2})\) belong to the same component of \(A\). Similarly, \(\pi_{B}(C_{1})\) and \(\pi_{B}(C_{2})\) belong to the same component of \(B\) under the projection \(\pi_{B}:A\times B\to B\). Since all components of \(A\) and \(B\) are maximally connected, we have \(C_{1}=C_{2}\), which implies that \(f\) is injective. Since \(f\) is a bijection from \(\pi_{0}(A)\times\pi_{0}(B)\) to \(\pi_{0}(A\times B)\), \(|\pi_{0}(A\times B)|=|\pi_{0}(A)||\pi_{0}(B)|\). 

### Groups

A _group_ is a set \(G\) together with a law of composition, that satisfies \((ab)c=a(bc)\)\(\forall a,b,c\in G\), \(\exists 1\) such that \(1a=a1=a\)\(\forall a\in G\), and \(\forall a\in G\), \(\exists b\) such that \(ab=ba=1\). An _action_ of a group \(G\) on a set \(S\) is a map \(\cdot:G\times S\to S\), that satisfies id \(\cdot s=s\) for all \(s\in S\) and \((gg^{\prime})\cdot s=g\cdot(g^{\prime}\cdot s)\) for all \(g,g^{\prime}\) in \(G\) and all \(s\) in \(S\). The _orbit_ of \(s\in S\) is the set \(O(s)=\{s^{\prime}\in S\mid s^{\prime}=gs\text{ for some }g\text{ in }G\}\).

A _topological group_ is a group \(G\) endowed with a topology such that multiplication and inverse are both continuous. A recurring example is the general linear group \(GL_{n}(\mathbb{R})\), with the subspace topology obtained from \(\mathbb{R}^{n^{2}}\). \(GL_{n}(\mathbb{R})\) has two connected components, which correspond to the preimages of the positive and negative reals under the determinant map.

The _product_ of groups \(G_{1},...,G_{n}\) is a group denoted by \(G_{1}\times...\times G_{n}\). The elements in \(G_{1}\times...\times G_{n}\) is the product set of \(G_{1},...,G_{n}\). The group structure is defined by identity \((1,...,1)\), inverse \((g_{1},...,g_{n})^{-1}=(g_{1}^{-1},...,g_{n}^{-1})\), and multiplication rule \((g_{1},...,g_{n})(g_{1}^{\prime},...,g_{n}^{\prime})=of \(L\) consists of a single orbit, Corollary B.6 extends immediately to the number of components of the minimum.

**Corollary B.7**.: _Let \(L\) be a function with a symmetry group \(G\). If the minimum of \(L\) consists of a single \(G\)-orbit, then the number of connected components of the minimum is smaller or equal to the number of connected components of \(G\)._

Generally, symmetry groups do not act transitively on a level set \(L^{-1}(c)\in\mathbf{Param}\). In this case, the connectedness of the orbits does not directly inform the connectedness of the level set.

**Proposition B.8**.:
1. _There exists a space_ \(X\)_, a group_ \(G\)_, and an action of_ \(G\)_, such that each orbit for the action of_ \(G\) _is connected and_ \(X\) _is not connected._
2. _There exists a space_ \(X\)_, a group_ \(G\)_, and an action of_ \(G\)_, such that each orbit for the action of_ \(G\) _is disconnected and_ \(X\) _is connected._

Proof.: For part (a), consider a subspace of \(\mathbb{R}^{2}\), \(X=X_{1}\cup X_{2}\) where \(X_{1}=\{(x,y):x=0,y>0\}\) and \(X_{2}=\{(x,y):x=1,y>0\}\). The space \(X\) is not connected. Let \(G\) be the multiplicative group of positive real numbers and act on \(X\) by multiplication on the second coordinate. Then there are two orbits, \(X_{1}\) and \(X_{2}\), which are both connected.

For part (b), consider the space \(X=\mathbb{R}^{2}\setminus\{0\}\). Then \(X\) is connected. Let \(G\) be the multiplicative group of real numbers, which acts on \(X\) by multiplication on both coordinates. That is, \(g\cdot(x_{1},x_{2})=(gx,gx_{2}),\forall(x_{1},x_{2})\in X,\forall g\in G\). The orbit of any point \((x_{1},x_{2})\in X\) is not connected. 

Nevertheless, since the set of orbits partitions the space, we can use the following bound on the number of components of the space.

**Proposition B.9**.: _Let \(X\) be a topological space and let \(X=\coprod_{i}X_{i}\) be a partition of \(X\) into disjoint subspaces. Then \(|\pi_{0}(X)|\leq\sum_{i}|\pi_{0}(X_{i})|\)._

Proof.: Let \(S=\{A\subseteq X:\exists i,A\text{ is a component of }X_{i}\}\) be the union of the components of the subspaces. Then \(S\) is a partition of \(X\), and every element in \(S\) is connected. Therefore, there is a surjective map from \(S\) to \(\pi_{0}(X)\), defined by mapping each \(s\in S\) to the element of \(\pi_{0}(X)\) that includes \(s\). This implies that \(|\pi_{0}(X)|\leq|S|=\sum_{i=1}^{n}|\pi_{0}(X_{i})|\). 

Consider a topological space \(X\) and a group \(G\) that acts on \(X\). Let \(O=\{O_{1},...,O_{n}\}\) be the set of orbits. By Proposition B.9, the number of components of the orbits give the following upper bound on the number of components of the space: \(|\pi_{0}(X)|\leq\sum_{i=1}^{n}|\pi_{0}(O_{i})|\).

## Appendix C Missing Proofs

### Proof of Proposition 2.1

Proof.: Recall that \(W_{1},...,W_{n},X,Y\) are matrices in \(\mathbb{R}^{h\times h}\), and \(X,Y\) are both full rank. Consider the map

\[f:(\mathrm{GL}_{h})^{l-1}\to L^{-1}(0),\quad(g_{1},...,g_{l-1})\mapsto(g_{1}X ^{-1},g_{2},...,g_{l-1},Y\prod_{i}^{l-1}g_{i}^{-1}). \tag{7}\]

The inverse \(f^{-1}:(W_{1},...,W_{l})\mapsto(W_{1}X,W_{2},W_{3},...,W_{l-1})\) is well defined, because \(X\), \(W_{1},W_{2},W_{3},...,W_{l-1}\) are all full-rank. Since both \(f\) and \(f^{-1}\) are continuous, \(f\) is a homeomorphism between \((\mathrm{GL}_{h})^{l-1}\) and \(L^{-1}(0)\). 

### Proof of Corollary 2.2

Proof.: From Proposition \(2.1\), \(L^{-1}(0)\) is homeomorphic to \((\mathrm{GL}_{h})^{l-1}\). According to Corollary B.3, this implies that \(L^{-1}(0)\) has the same number of connected components as \((\mathrm{GL}_{h})^{l-1}\). From Proposition B.5, \(GL_{h}(\mathbb{R})^{l-1}\) has \(2^{l-1}\) connected components. Therefore, \(L^{-1}(0)\) has \(2^{l-1}\) connected components.

### Proof of Proposition 2.3

Proof.: When \(\varepsilon=0\), the skip connection is effectively removed, and the loss function (2) reduces to (1). By Corollary 2.2, the minimum of \(L\) has 4 connected components. In the rest of the proof, we consider the case where \(\varepsilon\neq 0\).

Let \((W_{1_{0}},W_{2_{0}},W_{3_{0}})=(I,(\alpha-\varepsilon)I,\alpha^{-1}YX^{-1})\), where \(\alpha\in\mathbb{R}\) is an arbitrary number such that \(\alpha\neq\varepsilon\) and \(\alpha\neq 0\). Then \((W_{1_{0}},W_{2_{0}},W_{3_{0}})\) is a point in \(L^{-1}(0)\). Define set \(G_{1}=\{g\in R^{h\times h}:\det\left(gW_{2_{0}}W_{1_{0}}X+\varepsilon X\right)\neq 0\}\). Let \(a:GL_{1}\times G_{1}\rightarrow\mathbf{Param}\) be the following map:

\[g_{1},g_{2}\mapsto(g_{1}W_{1_{0}},\] \[g_{2}W_{2_{0}}g_{1}^{-1},\] \[W_{3_{0}}(W_{2_{0}}W_{1_{0}}X+\varepsilon X)(g_{2}W_{2_{0}}W_{1_ {0}}X+\varepsilon X)^{-1}). \tag{8}\]

From the definition of \(G_{1}\), \((g_{2}W_{2_{0}}W_{1_{0}}X+\varepsilon X)\) is invertible, so \(a\) is well defined. Additionally, we have \(L(a(g_{1},g_{2}))=L(W_{1_{0}},W_{2_{0}},W_{3_{0}})=0,\forall g_{1},g_{2}\in GL _{1}\times G_{1}\). Therefore, denoting the image of \(a\) as \(S_{1}\), we have \(S_{1}\subseteq L^{-1}(0)\).

Let \(S_{0}=\{(W_{1},W_{2},W_{3}):W_{3}=Y(\varepsilon X)^{-1}\text{ and }W_{1}=0\}\) if \(\varepsilon\neq 0\), or \(\emptyset\) otherwise. For \((W_{1},W_{2},W_{3})\in S_{0}\), we have \(L(W_{1},W_{2},W_{3})=||Y-Y(\varepsilon X)^{-1}(0+\varepsilon X)||_{2}=0\). Therefore, \(S_{0}\subseteq L^{-1}(0)\).

We then show that the minimum of \(L\) is the union of \(S_{1}\) and \(S_{0}\). Consider a point \((W_{1},W_{2},W_{3})\in L^{-1}(0)\). If \(W_{1}=0\), then \(\varepsilon\neq 0\), otherwise \((W_{1},W_{2},W_{3})\) cannot be in \(L^{-1}(0)\). In this case, \(W_{3}\) must equal to \(Y(\varepsilon X)^{-1}\), and \((W_{1},W_{2},W_{3})\in S_{0}\). If \(W_{1}\neq 0\), then \(W_{1}W_{1_{0}}^{-1}\in GL_{1}\) and \(W_{2}W_{1}W_{1_{0}}^{-1}W_{2_{0}}^{-1}\in G_{1}\). The second part is due to \(W_{2}W_{1}W_{1_{0}}^{-1}W_{2_{0}}^{-1}W_{2_{0}}W_{1_{0}}X+\varepsilon X=W_{2}W _{1}X+\varepsilon X\neq 0\) since \((W_{1},W_{2},W_{3})\in L^{-1}(0)\). In this case we have \((W_{1},W_{2},W_{3})=a(W_{1}W_{1_{0}}^{-1},W_{2}W_{1}W_{1_{0}}^{-1}W_{2_{0}}^{-1})\), which means that \((W_{1},W_{2},W_{3})\in S_{1}\).

The number of connected components of \(S_{1}\) and \(S_{0}\) can be obtained from their structures. Since \(W_{2_{0}}W_{1_{0}}X\neq 0\), there is a homeomorphism between \(G_{1}\) and \(GL_{1}\) defined by the map

\[f:G_{1}\to GL_{1},g\mapsto gW_{2_{0}}W_{1_{0}}X+\varepsilon X \tag{9}\]

with inverse \(f^{-1}:GL_{1}\to G_{1},g\mapsto\varepsilon(g-\varepsilon X)(W_{2_{0}}W_{1_{0} }X)^{-1}\). Since \(a\) is also a homeomorphism, its image \(S_{1}\) is homeomorphic to \(GL_{1}\times GL_{1}\) and has 4 connected components. When \(\varepsilon\neq 0\), \(S_{0}\) is a line and thus has 1 connected component.

The last part of the proof shows the connectedness of the connected components of \(S_{1}\) and \(S_{0}\). Let \(G_{1}^{+}=\{g_{2}\in G_{1}:f(g_{2})\in GL^{sign(\varepsilon X)}\}\) be the connected component in \(G_{1}\) that correspond to \(GL^{sign(\varepsilon X)}\), and \(G_{1}^{-}=\{g_{2}\in G_{1}:f(g_{2})\in GL^{-sign(\varepsilon X)}\}\) be the component that correspond to \(GL^{-sign(\varepsilon X)}\). For convenience, we name the connected components of \(Im(a)\) as follows:

\[C_{1}=\{(W_{1},W_{2},W_{3})\in\mathbf{Param}:(W_{1},W_{2},W_{3})=a (g_{1},g_{2}),g_{1}\in GL^{+},g_{2}\in G_{1}^{+}\}\] \[C_{2}=\{(W_{1},W_{2},W_{3})\in\mathbf{Param}:(W_{1},W_{2},W_{3})=a (g_{1},g_{2}),g_{1}\in GL^{-},g_{2}\in G_{1}^{+}\}\] \[C_{3}=\{(W_{1},W_{2},W_{3})\in\mathbf{Param}:(W_{1},W_{2},W_{3})=a (g_{1},g_{2}),g_{1}\in GL^{+},g_{2}\in G_{1}^{-}\}\] \[C_{4}=\{(W_{1},W_{2},W_{3})\in\mathbf{Param}:(W_{1},W_{2},W_{3})=a (g_{1},g_{2}),g_{1}\in GL^{-},g_{2}\in G_{1}^{-}\}\]

Note that for \((W_{1},W_{2},W_{3})\in S_{1}\), there exists a (unique) \(g_{2}\in G_{1}\) such that we can write \(W_{3}\) as

\[W_{3}=W_{3_{0}}[W_{2_{0}}W_{1_{0}}X+\varepsilon X][g_{2}W_{2_{0}}W_{1_{0}}X+ \varepsilon X]^{-1})=Yf(g_{2})^{-1}.\]

Following from the definition of \(G_{1}^{+}\), for a point \((W_{1},W_{2},W_{3})\) in \(C_{1}\) or \(C_{2}\), \(sign(W_{3})=sign(Y(\varepsilon X)^{-1})\). Additionally, when \(g_{2}\) is close to 0, \(g_{2}\) belongs to \(G_{1}^{+}\). The boundary of both \(C_{1}\) and \(C_{2}\) contain a point in \(S_{0}\):

\[\lim_{g_{1}\to 0^{+}}a(g_{1},g_{1})=\lim_{g_{1}\to 0^{-}}a(g_{1},g_{1})=(0, \alpha-\varepsilon,Y(\varepsilon X)^{-1})\in S_{0}.\]

Therefore, both \(C_{1}\) and \(C_{2}\) are connected to \(S_{0}\).

For points in \(C_{3}\) and \(C_{4}\), \(sign(W_{3})\neq sign(Y(\varepsilon X)^{-1})\). Therefore, no point in \(C_{3}\) or \(C_{4}\) can be sufficiently close to \(S_{0}\). As a result, these components are not connected to \(S_{0}\). In summary, when \(\varepsilon\neq 0\), \(S_{0}\) connects 2 components of \(S_{1}\), and the minimum of \(L\) has 3 connected components.

[MISSING_PAGE_EMPTY:12]