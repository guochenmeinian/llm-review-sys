# Achieving Constant Regret in Linear Markov Decision Processes

 Weitong Zhang

School of Data Science and Society

University of North Carolina at Chapel Hill

Chapel Hill, NC 27599

weitongz@unc.edu

&Zhiyuan Fan

EECS

Massachusetts Institute of Technology

Cambridge, MA 02139

fanzy@mit.edu

&Jiafan He

Department of Computer Science

University of California, Los Angeles

Los Angeles, CA 90095

jiafanhe19@ucla.edu &Quanquan Gu

Department of Computer Science

University of California, Los Angeles

Los Angeles, CA 90095

qgu@cs.ucla.edu

equal contribution

###### Abstract

We study the constant regret guarantees in reinforcement learning (RL). Our objective is to design an algorithm that incurs only finite regret over infinite episodes with high probability. We introduce an algorithm, \(\mathsf{Cert\_LSVI\_UCB}\), for misspecified linear Markov decision processes (MDPs) where both the transition kernel and the reward function can be approximated by some linear function up to misspecification level \(\zeta\). At the core of \(\mathsf{Cert\_LSVI\_UCB}\) is an innovative certified estimator, which facilitates a fine-grained concentration analysis for multi-phase value-targeted regression, enabling us to establish an instance-dependent regret bound that is constant w.r.t. the number of episodes. Specifically, we demonstrate that for a linear MDP characterized by a minimal suboptimality gap \(\Delta\), \(\mathsf{Cert\_LSVI\_UCB}\) has a cumulative regret of \(\widetilde{\mathcal{O}}(d^{3}H^{5}/\Delta)\) with high probability, provided that the misspecification level \(\zeta\) is below \(\widetilde{\mathcal{O}}(\Delta/(\sqrt{d}H^{2}))\). Here \(d\) is the dimension of the feature space and \(H\) is the horizon. Remarkably, this regret bound is independent of the number of episodes \(K\). To the best of our knowledge, \(\mathsf{Cert\_LSVI\_UCB}\) is the first algorithm to achieve a constant, instance-dependent, high-probability regret bound in RL with linear function approximation without relying on prior distribution assumptions.

## 1 Introduction

Reinforcement learning (RL) has been a popular approach for teaching agents to make decisions based on feedback from the environment. RL has shown great success in a variety of applications, including robotics (Kober et al., 2013), gaming (Mnih et al., 2013), and autonomous driving. In most of these applications, there is a common expectation that RL agents will master tasks after making only a bounded number of mistakes, even over indefinite runs. However, theoretical support for this expectation is limited in RL literature: in the worst case, existing works such as Jin et al. (2020); Ayoub et al. (2020); Wang et al. (2019) only provided \(\widetilde{\mathcal{O}}(\sqrt{K})\) regret upper bounds with \(K\) being the number of episodes; in the instance-dependent case, Simchowitz and Jamieson (2019); Yang et al. (2021); He et al. (2021) achieved logarithmic high-probability regret upper bounds (e.g.,\(\tilde{\mathcal{O}}(\Delta^{-1}\log K)\)) for both tabular MDPs and MDPs with linear function approximations, provided a minimal suboptimality gap \(\Delta\). However, these findings suggest that an agent's regret increases with the number of episodes \(K\), contradicting to the expectation of finite mistakes in practice. To close this gap between theory and practice, there is a recent line of work proving constant regrets bound for RL and bandits, suggesting that an RL agent's regret may remain bounded even when it encounters an indefinite number of episodes. Papini et al. (2021); Zhang et al. (2021) have provided instance-dependent constant regret bound under certain coverage assumptions on the data distribution. However, verifying these data distribution assumptions can be difficult or even infeasible. On the other hand, it is known that high-probability constant regret bound can be achieved unconditionally in multi-armed bandits (Abbasi-Yadkori et al., 2011) and contextual linear bandits if and only if the misspecification is sufficiently small with respect to the minimal sub-optimality gap (Zhang et al., 2023). This raises a critical question:

_Is it possible to design a reinforcement learning algorithm that incurs only constant regret under minimal assumptions?_

To answer this question, we introduce a novel algorithm, which we refer to as \(\mathsf{Cert\text{-}LSVI\text{-}UCB}\), for reinforcement learning with linear function approximation. To encompass a broader range of real-world scenarios characterized by large state-action spaces and the need for function approximation, we consider the _misspecified linear MDP_(Jin et al., 2020) setting, where both the transition kernel and reward function can be approximated by a linear function with approximation error \(\zeta\). We show that, with our innovative design of certified estimator and novel analysis, \(\mathsf{Cert\text{-}LSVI\text{-}UCB}\) achieves constant regret without relying on any prior assumption on data distributions. Our key contributions are summarized as follows:

* We introduce a parameter-free algorithm, referred to as \(\mathsf{Cert\text{-}LSVI\text{-}UCB}\), featuring a novel certified estimator for testing when the confidence set fails. This certified estimator enables \(\mathsf{Cert\text{-}LSVI\text{-}UCB}\) to achieve a constant, instance-dependent, high probability regret bound of \(\tilde{\mathcal{O}}(d^{3}H^{5}/\Delta)\) for tasks with a suboptimality gap \(\Delta\), under the condition that the misspecification level \(\zeta\) is bounded by \(\zeta<\tilde{\mathcal{O}}\big{(}\Delta/(\sqrt{d}H^{2})\big{)}\). This bound is termed a _high probability constant regret bound_, indicating that it does not depend on the number of episodes \(K\). We note that this constant regret bound matches the logarithmic expected regret lower bound of \(\Omega(\Delta^{-1}\log K)\), suggesting that our result is valid and optimal in terms of the dependence on the suboptimality gap \(\Delta\).
* When restricted to a well-specified linear MDP (i.e., \(\zeta=0\)), the constant high probability regret bound improves the previous logarithmic result \(\tilde{\mathcal{O}}(d^{3}H^{5}\Delta^{-1}\log K)\) in He et al. (2021) by a \(\log K\) factor. Our results suggest that the total suboptimality incurred by \(\mathsf{Cert\text{-}LSVI\text{-}UCB}\) remains constantly bounded, regardless of the number of episodes \(K\). In contrast to the previous constant regret bound achieved by Papini et al. (2021), our regret bound does not require any prior assumption on the feature mapping, such as the UniSOFT assumption made in Papini et al. (2021). To the best of our knowledge, \(\mathsf{Cert\text{-}LSVI\text{-}UCB}\) is the first algorithm to achieve a _high probability constant regret bound_ for MDPs without prior assumptions on data distributions. We further show that this constant regret high-probability bound does not violate the logarithmic expected regret bound by letting \(\delta=1/K\)2. Footnote 2: The detailed conversion is presented in Remark 5.2.

**Notation.** Vectors are denoted by lower case boldface letters such as \(\mathbf{x}\), and matrices by upper case boldface letters such as \(\mathbf{A}\). We denote by \([k]\) the set \(\{1,2,\cdots,k\}\) for positive integers \(k\). We use \(\log x\) to denote the logarithm of \(x\) to base \(2\). For two non-negative sequence \(\{a_{n}\},\{b_{n}\},a_{n}\leq\mathcal{O}(b_{n})\) means that there exists a positive constant \(C\) such that \(a_{n}\leq Cb_{n}\); \(a_{n}\leq\tilde{\mathcal{O}}(b_{n})\) means there exists a positive constant \(k\) such that \(a_{n}\leq\mathcal{O}(b_{n}\log^{k}b_{n})\); \(a_{n}\geq\Omega(b_{n})\) means that there exists a positive constant \(C\) such that \(a_{n}\geq Cb_{n}\); \(a_{n}\geq

## 2 Related Work

Instance-dependent regret bound in RL.Although most of the theoretical RL works focus on worst-case regret bounds, instance-dependent (a.k.a., problem-dependent, gap-dependent) regret bound is another important bound to understanding how the hardness of different instance can affect the sample complexity of the algorithm. For tabular MDPs, Jaksch et al. (2010) proved a \(\widetilde{\mathcal{O}}(D^{2}S^{2}A\Delta^{-1}\log K)\) instance-dependent regret bound for average-reward MDP where \(D\) is the diameter of the MDP and \(\Delta\) is the policy suboptimal gap. Simchowitz and Jamieson (2019) provided a lower bound for episodic MDP which suggests that the any algorithm will suffer from \(\Omega(\Delta^{-1})\) regret bound. Yang et al. (2021) analyzed the optimistic \(Q\)-learning and proved a \(\mathcal{O}(SAH^{6}\Delta^{-1}\log K)\) logarithmic instance-dependent regret bound. In the domain of linear function approximation, He et al. (2021) provided instance-dependent regret bounds for both linear MDPs (i.e., \(\widetilde{\mathcal{O}}(d^{3}H^{5}\Delta^{-1}\log K)\)) and linear mixture MDPs (i.e., \(\widetilde{\mathcal{O}}(d^{2}H^{5}\Delta^{-1}\log K)\)). Furthermore, Dann et al. (2021) provided an improved analysis for this instance-dependent result with a redefined suboptimal gap. Zhang et al. (2023) proved a similar logarithmic instance-dependent bound with He et al. (2021) in misspecified linear MDPs, showing the relationship between misspecification level and suboptimality bound. Despite all these bounds are logarithmic depended on the number of episode \(K\), many recent works are trying to remove this logarithmic dependence. Papini et al. (2021) showed that under the linear MDP assumption, when the distribution of contexts \(\mathbf{\phi}(s,a)\) satisfies the "diversity assumption" (Hao et al., 2020) called 'UniSOFT', then LSVI-UCB algorithm may achieve an expected constant regret w.r.t. \(K\). Zhang et al. (2021) showed a similar result on bilinear MDP (Yang and Wang, 2020), and extended this result to offline setting, indicating that the algorithm only need a finite offline dataset to learn the optimal policy. Table 1 summarizes the most relevant results mentioned above for the ease of comparison with our results.

RL with model misspecification.All of the aforementioned works consider the well-specified setting and ignore the approximation error in the MDP model. To better understand this misspecification issue, Du et al. (2019) showed that having a good representation is insufficient for efficient RL unless the approximation error (i.e., misspecification level) by the representation is small enough. In particular, Du et al. (2019) showed that an \(\widetilde{\Omega}(\sqrt{H/d})\) misspecification will lead to \(\Omega(2^{H})\) sample complexity for RL to identify the optimal policy, even with a generative model. On the other hand, a series of work (Jin et al., 2020; Zanette et al., 2020, 2020) provided \(\widetilde{\mathcal{O}}(\sqrt{K}+\zeta K)\)-type regret bound for RL in various settings, where \(\zeta\) is the misspecification level3 and we ignore the dependence on the dimension of the feature mapping \(d\) and the planing horizon \(H\) for simplicity. These algorithms, however, require the knowledge of misspecification level \(\zeta\), thus are not _parameter-free_. Another concern for these algorithms is that some of the algorithms (Jin et al., 2020) would possibly suffer from a _trivial asymptotic regret_, i.e., \(\text{Regret}(k)>\omega(k\zeta\cdot\text{poly}(d,H,\log(1/\delta)))\), as suggested by Vial et al. (2022). This means the performance of the RL algorithm will possibly degenerate as the number of episodes \(k\) grows. To tackle these two issues, Vial et al. (2022) propose the \(\text{Sup-LSVI-UCB}\) algorithm which requires a parameter \(\varepsilon_{\text{tol}}\). When \(\varepsilon_{\text{tol}}=d/\sqrt{K}\), the proposed

\begin{table}
\begin{tabular}{c c c} \hline \hline Algorithm & Misspecified MDP? & Result \\ \hline LSVI-UCB (He et al., 2021) & \(\times\) & \(\widetilde{\mathcal{O}}(d^{3}H^{5}\Delta^{-1}\log(K))\) \\ LSVI-UCB (Papini et al., 2021) & \(\times\) & \(\widetilde{\mathcal{O}}(d^{3}H^{5}\Delta^{-1}\log(1/\lambda))\) \\ Cert-LSVI-UCB (ours, Theorem 5.1) & \(\checkmark\) & \(\widetilde{\mathcal{O}}(d^{3}H^{5}\Delta^{-1})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Instance-dependent regret bounds for different algorithms under the linear MDP setting. Here \(d\) is the dimension of the linear function \(\mathbf{\phi}(s,a)\), \(H\) is the horizon length, \(\Delta\) is the minimal suboptimality gap. All results in the table represent high probability regret bounds. The regret bound depends the number of episodes \(K\) in He et al. (2021) and the minimum positive eigenvalue \(\lambda\) of features mapping in Papini et al. (2021). **Misspecified MDP?** indicates if the algorithm can (\(\checkmark\)) handle the misspecified linear MDP or not (\(\times\)).

algorithm is _parameter-free_ but will have a trivial _asymptotic regret bound_. When \(\varepsilon_{\text{tol}}=\zeta\), the algorithm will have a non-trivial _asymptotic regret bound_ but is not _parameter-free_ since it requires knowledge of the misspecification level. Another series of works (He et al., 2022; Lykouris et al., 2021; Wei et al., 2022) are working on the _corruption robust_ setting. In particular, Lykouris et al. (2021); Wei et al. (2022) are using the _model-selection_ technique to ensure the robustness of RL algorithms under adversarial MDPs.

## 3 Preliminaries

We consider episodic Markov Decision Processes, which are denoted by \(\mathcal{M}(\mathcal{S},\mathcal{A},H,\{r_{h}\},\{\mathbb{P}_{h}\})\). Here, \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the finite action space, \(H\) is the length of each episode, \(r_{h}:\mathcal{S}\times\mathcal{A}\mapsto[0,1]\) is the reward function at stage \(h\) and \(\mathbb{P}_{h}(\cdot|s,a)\) is the transition probability function at stage \(h\). The policy \(\pi=\{\pi_{h}\}_{h=1}^{H}\) denotes a set of policy functions \(\pi_{h}:\mathcal{S}\mapsto\mathcal{A}\) for each stage \(h\). For given policy \(\pi\), we define the state-action value function \(Q_{h}^{\pi}(s,a)\) and the state value function \(V_{h}^{\pi}(s)\) as

\[Q_{h}^{\pi}(s,a)=r_{h}(s,a)+\mathbb{E}\left[\sum_{h^{\prime}=h+1}^{H}r_{h^{ \prime}}\big{(}s_{h^{\prime}},\pi_{h^{\prime}}(s_{h^{\prime}})\big{)}\;\Big{|} \;s_{h}=s,a_{h}=a\Big{]},V_{h}^{\pi}(s)=Q_{h}^{\pi}\big{(}s,\pi_{h}(s)\big{)},\]

where \(s_{h^{\prime}+1}\sim\mathbb{P}_{h}(\cdot|s_{h^{\prime}},a_{h^{\prime}})\). The optimal state-action value function \(Q_{h}^{*}\) and the optimal state value function \(V_{h}^{*}\) are defined by \(Q_{h}^{*}(s,a)=\max_{\pi}Q_{h}^{\pi}(s,a),V_{h}^{*}(s)=\max_{\pi}V_{h}^{\pi}(s)\).

By definition, both the state-action value function \(Q_{h}^{\pi}(s,a)\) and the state value function \(V_{h}^{\pi}(s)\) are bounded by \([0,H]\) for any state \(s\), action \(a\) and stage \(h\). For any function \(V:\mathcal{S}\mapsto\mathbb{R}\), we denote by \([\mathbb{P}_{h}V](s,a)=\mathbb{E}_{s^{\prime}\sim\mathbb{P}_{h}(\cdot|s,a)}V(s ^{\prime})\) the expected value of \(V\) after transitioning from state \(s\) given action \(a\) at stage \(h\) and \([\mathbb{B}_{h}V](s,a)=r_{h}(s,a)+[\mathbb{P}_{h}V](s,a)\) where \(\mathbb{B}\) is referred to as the _Bellman operator_. For each stage \(h\in[H]\) and policy \(\pi\), the Bellman equation, as well as the Bellman optimality equation, are presented as follows

\[Q_{h}^{\pi}(s,a) =r_{h}(s,a)+[\mathbb{P}_{h}V_{h+1}^{\pi}](s,a):=[\mathbb{B}_{h}V_ {h+1}^{\pi}](s,a),\] \[Q_{h}^{\pi}(s,a) =r_{h}(s,a)+[\mathbb{P}_{h}V_{h+1}^{*}](s,a):=[\mathbb{B}_{h}V_{h +1}^{*}](s,a).\]

We use regret to measure the performance of RL algorithms. It is defined as \(\text{Regret}(K)=\sum_{k=1}^{K}\big{(}V_{1}^{*}(s_{1}^{k})-V_{1}^{\pi^{k}}(s_{1 }^{k})\big{)}\), where \(\pi^{k}\) represents the agent's policy at episode \(k\). This definition quantifies the cumulative difference between the expected rewards that could have been obtained by following the optimal policy and those achieved under the agent's policy across the first \(K\) episodes, measuring the total loss in performance due to suboptimal decisions.

We consider linear function approximation in this work, where we adopt the _misspecified linear MDP_ assumption, which is firstly proposed in Jin et al. (2020).

**Assumption 3.1** (\(\zeta\)-Approximate Linear MDP, Jin et al. 2020).: For any \(\zeta\leq 1\), we say a MDP \(\mathcal{M}(\mathcal{S},\mathcal{A},H,\{r_{h}\},\{\mathbb{P}_{h}\})\) is a \(\zeta\)_-approximate linear MDP_ with a feature map \(\mathbf{\phi}:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}^{d}\), if for any \(h\in[H]\), there exist \(d\)_unknown_ (signed) measures \(\mathbf{\mu}_{h}=\big{(}\mu_{h}^{(1)},\cdots,\mu_{h}^{(d)}\big{)}\) over \(\mathcal{S}\) and an unknown vector \(\mathbf{\theta}_{h}\in\mathbb{R}^{d}\) such that for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), we have

\[\big{\|}\mathbb{P}_{h}(\cdot|s,a)-\langle\mathbf{\phi}(s,a),\mathbf{\mu}_{h}(\cdot) \rangle\,\big{\|}_{\text{TV}}\leq\zeta,\quad\big{|}r_{h}(s,a)-\langle\mathbf{\phi} (s,a),\mathbf{\theta}_{h}\rangle\,\big{|}\leq\zeta,\]

w.l.o.g. we assume \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}:\|\mathbf{\phi}(s,a)\|\leq 1\) and \(\forall h\in[H]:\|\mathbf{\mu}_{h}(\mathcal{S})\|\leq\sqrt{d},\|\mathbf{\theta}_{h}\| \leq\sqrt{d}\).

The \(\zeta\)_-approximate linear MDP_ suggests that for any policy \(\pi\), the state-action value function \(Q_{h}^{\pi}\) can be approximated by a linear function of the given feature mapping \(\mathbf{\phi}\) up to some misspecification level, which is summarized in the following proposition.

**Proposition 3.2** (Lemma C.1, Jin et al. 2020).: For a \(\zeta\)_-approximate linear MDP_, for any policy \(\pi\), there exist corresponding weights \(\{\mathbf{w}_{h}^{\pi}\}_{h\in[H]}\) where \(\mathbf{w}_{h}^{\pi}=\mathbf{\theta}_{h}+\int V_{h+1}^{\pi}(s^{\prime})\mathrm{d} \mathbf{\mu}_{h}(s^{\prime})\) such that for any \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\), \(\big{|}Q_{h}^{\pi}(s,a)-\langle\mathbf{\phi}(s,a),\mathbf{w}_{h}^{\pi}\rangle\, \big{|}\leq 2H\zeta\). We have \(\|\mathbf{w}_{h}^{\pi}\|_{2}\leq 2H\sqrt{d}\).

Next, we introduce the definition of the suboptimal gap as follows.

**Definition 3.3** (Minimal suboptimality gap).: For each \(s\in\mathcal{S},a\in\mathcal{A}\) and step \(h\in[H]\), the suboptimality gap \(\text{gap}_{h}(s,a)\) is defined by \(\Delta_{h}(s,a)=V_{h}^{*}(s)-Q_{h}^{\pi}(s,a)\) and the minimal suboptimality gap \(\Delta\) is defined by \(\Delta=\min_{h,s,a}\big{\{}\Delta_{h}(s,a):\Delta_{h}(s,a)\neq 0\big{\}}\).

Notably, a task with a larger \(\Delta\) means it is easier to distinguish the optimal action \(\pi_{h}^{*}(s)\) from other actions \(a\in\mathcal{A}\), while a task with lower gap \(\Delta\) means it is more difficult to distinguish the optimal action.

## 4 Proposed Algorithms

### Main algorithm: \(\texttt{Cert-LSVI-UCB}\)

We begin by introducing our main algorithm \(\texttt{Cert-LSVI-UCB}\), which is a modification of the Sup-LSVI-UCB (Vial et al., 2022). As presented in Algorithm 1, for each episode \(k\), our algorithm maintains a series of index sets \(\mathcal{C}^{l}_{k,h}\) for each stage \(h\in[H]\) and phase \(l\). The algorithm design ensures that for any episode \(k\), the maximum number of phases \(l\) is bounded by \(L_{k}\leq\max\{\lceil\log_{4}(k/d)\rceil,0\}\). During the exploitation step, for each phase \(l\) associated with the index set \(\mathcal{C}^{l}_{k-1,h}\), the algorithm constructs the estimator vector \(\mathbf{w}^{k}_{h,l}\) by solving the following ridge regression problem in Line 6 and Line 7:

\[\mathbf{w}^{k}_{h,l}\leftarrow\operatorname*{argmin}_{\mathbf{w}\in\mathbb{R} ^{d}}\lambda\|\mathbf{w}\|_{2}^{2}+\sum_{\tau\in\mathcal{C}^{k-1}_{h,l}}\left( \mathbf{w}^{\top}\mathbf{\phi}^{\tau}_{h}-r^{\tau}_{h}-\widehat{V}^{k}_{h+1}(s^{ \tau}_{h+1})\right)^{2}.\]

After calculating the estimator vector \(\mathbf{w}^{k}_{h,l}\) in Line 8, the algorithm quantizes \(\mathbf{w}^{k}_{h,l}\) and \((\mathbf{U}^{k}_{h,l})^{-1}\) to the precision of \(\kappa_{l}\). Similar to Sup-LSVI-UCB (Vial et al., 2022), we note \(\widetilde{\mathbf{U}}^{k,-1}_{h,l}\) is the quantized version of inverse covariance matrix \((\mathbf{U}^{k}_{h,l})^{-1}\) rather than the inverse of quantized covariance matrix \((\widetilde{\mathbf{U}}^{k}_{h,l})^{-1}\). The main difference between our implementation and that in Vial et al. (2022) is that we use a layer-dependent quantification precision \(\kappa_{l}\) instead of the global quantification precision \(\kappa=2^{-4L}/d\), which enables our algorithm get rid of the dependence on \(\mathcal{O}(\log K)\) in the maximum number of phases \(L_{k}\).

After obtaining \(\widetilde{\mathbf{w}}^{k}_{h,l}\) and \(\widetilde{\mathbf{U}}^{k,-1}_{h,l}\), a subroutine, \(\texttt{Cert-LinUCB}\), is called to calculate an optimistic value function \(\widehat{V}^{k}_{h}(s^{\tau}_{h})\) for all historical states \(s^{\tau}_{h}\) in Line 10. Then the algorithm transits to stage \(h-1\) and iteratively computes \(\widetilde{\mathbf{w}}^{k}_{h,l}\) and \(\widetilde{\mathbf{U}}^{k,-1}_{h,l}\) for all phase \(l\) and stage \(h\in[H]\).

In the exploration step, the algorithm starts to do planning from the initial state \(s^{k}_{1}\). For each observed state \(s^{k}_{h}\), the same subroutine, \(\texttt{Cert-LinUCB}\), will be called in Line 14 for the policy \(\pi^{k}_{h}(s^{k}_{h})\), the corresponding phase \(l_{h}^{k}(s_{h}^{k})\), and a flag \(f_{h}^{k}(s_{h}^{k})\). If the flag \(f_{h}^{k}(s_{h}^{k})=1\), the algorithm adds the index \(k\) to the index set \(\mathcal{C}_{h,l_{h}^{k}(s_{h}^{k})}^{k}\) in Line 15. Otherwise, the algorithm skips the current index \(k\) and all index sets remain unchanged. Finally, the algorithm executes policy \(\pi_{h}^{k}(s_{h}^{k})\), receives reward \(r_{h}^{k}\) and observes the next state \(s_{h+1}^{k}\) in Line 17.

```
1:input:\(s\in\mathcal{S},\forall l:\widetilde{\mathbf{w}}_{h,l}^{k}\in\mathbb{R}^{d}, \widetilde{\mathbf{U}}_{h,l}^{k,-1}\in\mathbb{R}^{d\times d},L\in\mathbb{N}^{+}\)
2:output:\(\widehat{V}_{h}^{k}(s)\in\mathbb{R},\pi_{h}^{k}(s)\in\mathcal{A},l_{h}^{k}(s )\in\mathbb{N}^{+},f_{h}^{k}(s)\in\{0,1\}\)
3:\(\mathcal{A}_{h,1}^{k}(s)=\mathcal{A},\widetilde{V}_{h,0}^{k}(s)=0,\widehat{V} _{h,0}^{k}(s)=H\)
4:for phase \(l=1,\cdots,L+1\)do
5: Set \(Q_{h,l}^{k}(s,a)=\left\langle\mathbf{\phi}(s,a),\widetilde{\mathbf{w}}_{h,l}^{k}\right\rangle\)
6: Set \(\pi_{h,l}^{k}(s)=\operatorname{argmax}_{a\in\mathcal{A}_{h,l}^{k}}Q_{h,l}^{k}( s,a),V_{h,l}^{k}(s)=Q_{h,l}^{k}\big{(}s,\pi_{h,l}^{k}(s)\big{)}\)
7:if\(l>L\)then
8:return\(\left(\widetilde{V}_{h}^{k}(s),\pi_{h}^{k}(s),l_{h}^{k}(s),f_{h}^{k}(s)\right)= \left(\widehat{V}_{h,l-1}^{k}(s),\pi_{h,l-1}^{k}(s),l,1\right)\)
9:elseif\(\gamma_{l}\cdot\max_{a\in\mathcal{A}_{h,l}^{k}(s)}\left\|\mathbf{\phi}(s,a)\right\|_ {\widetilde{\mathbf{U}}_{h,l}^{k,-1}}2\geq 2^{-l}\)then
10:return\(\left(\widehat{V}_{h}^{k}(s),\pi_{h}^{k}(s),l_{h}^{k}(s),f_{h}^{k}(s)\right)= \left(\widehat{V}_{h,l-1}^{k}(s),\operatorname{argmax}_{a\in\mathcal{A}_{h,l} ^{k}(s)}\left\|\mathbf{\phi}(s,a)\right\|_{\widetilde{\mathbf{U}}_{h,l}^{k,-1}},l,1\right)\)
11:elseif\(\max\left\{V_{h,l}^{k}(s)-3\cdot 2^{-l},\widetilde{V}_{h,l-1}^{k}(s)\right\}>\min \left\{V_{h,l}^{k}(s)+3\cdot 2^{-l},\widehat{V}_{h,l-1}^{k}(s)\right\}\)then
12:return\(\left(\widehat{V}_{h}^{k}(s),\pi_{h}^{k}(s),l_{h}^{k}(s),f_{h}^{k}(s)\right)= \left(\widehat{V}_{h,l-1}^{k}(s),\pi_{h,l-1}^{k}(s),l,0\right)\)
13:else
14:\(\widetilde{V}_{h,l}^{k}(s)=\min\left\{V_{h,l}^{k}(s)+3\cdot 2^{-l},\widehat{V}_{h,l-1}^{k}(s)\right\}\)
15:\(\widetilde{V}_{h,l}^{k}(s)=\max\left\{V_{h,l}^{k}(s)-3\cdot 2^{-l}, \widetilde{V}_{h,l-1}^{k}(s)\right\}\)
16:\(\mathcal{A}_{h,l+1}^{k}(s)=\left\{a\in\mathcal{A}_{h,l}^{k}(s):Q_{h,l}^{k}(s,a) \geq V_{h,l}^{k}(s)-4\cdot 2^{-l}\right\}\)
17:endif
18:endfor
```

**Algorithm 2**\(\texttt{Cert-LinUCB}:(s;\{\widetilde{\mathbf{w}}_{h,l}^{k}\}_{l},l_{\{}},\{ \widetilde{\mathbf{U}}_{h,l}^{k,-1}\}_{l},L)\mapsto\left(\widehat{V}_{h}^{k}(s ),\pi_{h}^{k}(s),l_{h}^{k}(s),f_{h}^{k}(s)\right)\)

### Subroutine: \(\texttt{Cert-LinUCB}\)

Next we introduce subroutine \(\texttt{Cert-LinUCB}\), improved from \(\texttt{Sup-Lin-UCB-Var}\)(Vial et al., 2022) that computes the optimistic value function \(\widehat{V}_{h}^{k}\). The algorithm is described as follows. Starting from phase \(l=1\), the algorithm first calculates the estimated state-action function \(Q_{h,l}^{k}(s,a)\) as a linear function over the quantified parameter \(\widetilde{\mathbf{w}}_{h,l}^{k}\) and feature mapping \(\mathbf{\phi}(s,a)\), following Proposition 3.2. After calculating the estimated state-action value function \(Q_{h,l}^{k}(s)\), the algorithm computes the greedy policy \(\pi_{h,l}^{k}(s)\) and its corresponding value function \(V_{h,l}^{k}(s)\).

Similar to \(\texttt{Sup-Lin-UCB-Var}\)(Vial et al., 2022), our algorithm has several conditions starting from Line 7 to determine whether to stop at the current phase or to eliminate the actions and proceed to the next phase \(l+1\), which are listed in the following conditions.

* **Condition 1**: In Line 7, if the current phase \(l\) is greater than the maximum phase \(L\), we directly stop at that phase and take the greedy policy on previous phase \(\pi_{h}^{k}(s)=\pi_{h,l-1}^{k}(s)\).
* **Condition 2**: In Line 9, if there exists an action whose uncertainty \(\left\|\mathbf{\phi}(s,a)\right\|_{\widetilde{\mathbf{U}}_{h,l}^{k,-1}}\) is greater than the threshold \(2^{-l}\gamma_{l}^{-1}\), our algorithm will perform exploration by selecting that action.
* **Condition 3**: In Line 11, we compare the value of the pessimistic value function \(\widetilde{V}_{h,l}^{k}(s)\) and the optimistic value function \(\widehat{V}_{h,l}^{k}(s)\) which will be assigned in Line 14 and Line 15, if the pessimistic estimation will be greater than the optimistic estimation, we will stop at that phase and take the greedy policy on previous phase \(\pi_{h}^{k}(s)=\pi_{h,l-1}^{k}(s)\). Only in this case, the Algorithm 2 outputs flag \(f_{h}^{k}(s)=0\), which means this observation will not be used in Line 15 in Algorithm 1.
* **Condition 4**: In the default case in Line 16, the algorithm proceeds to the next phase after eliminating actions.

Notably, in **Condition 4**, since the expected estimation precision in the \(l\)-th phase is about \(\widetilde{\mathcal{O}}(2^{-l})\), our algorithm can eliminate the actions whose state-action value is significantly less than others, i.e., less than \(\widetilde{\mathcal{O}}(2^{-l})\), while retaining the remaining actions for the next phase.

Specially, our algorithm differs from that in Vial et al. (2022) in terms of **Condition 3** to certify the performance of the estimation. In particular, a well-behaved estimation should always guarantee that the optimistic estimation is greater than the pessimistic estimation. According to Line 14 and Line 15, this is equivalent to the confidence region for \(l\)-th phase has intersection of the previous confidence region \([\widetilde{V}^{k}_{h,l-1}(s),\widetilde{V}^{k}_{h,l-1}(s)]\). Otherwise, we hypothesis the estimation on \(l\)-th phase is corrupted by either misspecification or bad concentration event, thus will stop the algorithm. We will revisit the detail of this design later.

It's important to highlight that our algorithms provide unique approaches when compared with previous works. In particular, He et al. (2021) does not eliminate actions and combines estimations from all layers by considering the minimum estimated optimistic value function. This characteristic prevents their algorithm from achieving a uniform PAC guarantee in the presence of misspecification. For a more detailed comparison with He et al. (2021), please refer to Appendix B.1. Additionally, Lykouris et al. (2021); Wei et al. (2022) focus on a model-selection regime where a set of base learners are employed in the algorithms, whereas we adopt a multi-phase approach similar with SupLinUCB rather than conducting model selection over base learners.

## 5 Constant Regret Guarantee

**Theorem 5.1**.: Under Assumption 3.1, let \(\gamma_{l}=5(l+20+\lceil\log(ld)\rceil)dH\sqrt{\log(16ldH/\delta)}\) for some fixed \(0<\delta<1/4\). With probability at least \(1-4\delta\), if misspecification level \(\zeta\) is below \(\widetilde{\mathcal{O}}\big{(}\Delta/(\sqrt{d}H^{2})\big{)}\) where \(\Delta\) is the minimal suboptimality gap, then for all \(K\in\mathbb{N}^{+}\), the regret of Algorithm 1 is upper bounded by

\[\text{Regret}(K)\leq\widetilde{\mathcal{O}}\big{(}d^{3}H^{5}\Delta^{-1}\log(1/ \delta)\big{)}.\]

This regret bound is constant w.r.t. the episode \(K\).

Theorem 5.1 demonstrates a constant regret bound with respect to number of episodes \(K\). Compared with Papini et al. (2021), our regret bound does not require any prior assumption on the feature mapping \(\mathbf{\phi}\), such as the _UniSOFT_ assumption made in Papini et al. (2021). In addition, compared with the previous logarithmic regret bound He et al. (2021) in the well-specified setting, our constant regret bound removes the \(\log K\) factor, indicating the cumulative regret no longer grows w.r.t. the number of episode \(K\), with high probability.

**Remark 5.2**.: As discussed in Zhang et al. (2023) in the misspecified linear bandits, Our _high probability_ constant regret bound does not violate the lower bound proved in Papini et al. (2021), which says that certain diversity condition on the contexts is necessary to achieve an _expected_ constant regret bound. When extending this high probability constant regret bound to the expected regret bound, we have

\[\mathbb{E}[\text{Regret}(K)]\leq\widetilde{\mathcal{O}}\big{(}d^{3}H^{5} \Delta^{-1}\log(1/\delta)\big{)}\cdot(1-\delta)+\delta K,\]

which depends on the number of episodes \(k\). To obtain a sub-linear expected regret, we can choose \(\delta=1/K\), which yields a logarithmic expected regret \(\widetilde{\mathcal{O}}(d^{3}H^{5}\Delta^{-1}\log K)\) and does not violate the lower bound in Papini et al. (2021).

**Remark 5.3**.: Du et al. (2019) provide a lower bound showing the interplay between the misspecification level \(\zeta\) and suboptimality gap \(\Delta\) in a weaker setting, which we discuss in detail in Appendix B.2. Along with the result from Du et al. (2019), our results suggests that ignoring the dependence on \(H\), \(\zeta=\widetilde{\mathcal{O}}(\Delta/\sqrt{d})\) plays an important seperation for if a misspeficied model can be efficiently learned. This result is also aligned with the positive result and negative result for linear bandits (Lattimore et al., 2020; Zhang et al., 2023).

## 6 Technical Challenges and Highlight of Proof Techniques

In this section, we highlight several major challenges in obtaining the constant regret under misspecified linear MDP assumption and how our method, especially the certified estimator, tackles these challenges.

[MISSING_PAGE_FAIL:8]

Lemma 6.3 delivers a clear message: In the well-specified setting, Line 11 will never be triggered (\(l\geq\infty\)). When the misspecification level is large, then Line 11 will be more likely triggered, indicating it's harder for the algorithm to proceed to deeper layer. The contribution of the certified estimator yields the following important lemma regarding the 'local estimation error':

**Lemma 6.4** (Lemma C.12, Informal).: With high probability, for any \(\varepsilon>\widetilde{\Omega}(\sqrt{d}H^{2}\zeta)\) and \(h\in[H]\), \(\texttt{Cert-LSVI-UCB}\) ensures \(\sum_{k=1}^{\infty}\mathds{1}\left[V_{h}^{*}(s_{h}^{k})-V_{h}^{\pi^{k}}(s_{h}^{ k})\geq\varepsilon\right]\leq\widetilde{\mathcal{O}}\big{(}d^{3}H^{4} \varepsilon^{-2}\big{)}\).

**Remark 6.5**.: He et al. (2021b) achieved a similar \(\mathcal{O}\big{(}d^{3}H^{5}\varepsilon^{-2}\big{)}\)_uniform-PAC_ bound for (well-specified) linear MDP. Comparing with Lemma 6.4 with \(\zeta=0\), one can find that our result is better than He et al. (2021b). In addition, Lemma 6.4 ensures this _uniform-PAC_ result under all stage \(h\in[H]\) while He et al. (2021b) only ensure the \(h=1\). This improvement is achieved by a more efficient data selection strategy which we will discuss in detail in Appendix B.1.

### Challenge 2. Achieving constant regret from local estimation error

In misspecified linear bandits, Zhang et al. (2023b) concludes their proof by controlling \(\sum_{k=1}^{\infty}\mathds{1}[V_{1}^{*}(s_{1}^{k})-V_{1}^{\pi}(s_{1}^{k})\geq \Delta]\)4. Although it is trivial showing that rounds with instantaneous regret \(V_{1}^{*}(s_{1}^{k})-V_{1}^{\pi}(s_{1}^{k})<\Delta\) is optimal in bandits (i.e., \(V_{1}^{*}(s_{1}^{k})=V_{1}^{\pi}(s_{1}^{k})\)), previous works fail to reach a similar result for RL settings. This difficulty arises from the randomness inherent in MDPs: Consider a policy \(\pi\) that is optimal at the initial stage \(h=1\). After the initial state and action, the MDP may transition to a state \(s_{2}^{\prime}\) with a small probability \(p\) where the policy \(\pi\) is no longer optimal, or to another state \(s_{2}\) where \(\pi\) remains optimal until the end. In this context, the gap between \(V_{1}^{*}(s_{1})\) and \(V_{1}^{\pi}(s_{1})\) can be arbitrarily small, given a sufficiently small \(p>0\):

Footnote 4: We employ the RL notations and set \(h=1\) for the ease of comparison.

\[V_{1}^{*}(s_{1})-V_{1}^{\pi}(s_{1})=p\big{(}V_{2}^{*}(s_{2}^{\prime})-V_{2}^{ \pi}(s_{2}^{\prime})\big{)}+(1-p)\big{(}V_{2}^{*}(s_{2})-V_{2}^{\pi}(s_{2}) \big{)}=p\big{(}V_{2}^{*}(s_{2}^{\prime})-V_{2}^{\pi}(s_{2}^{\prime})\big{)}.\]

Therefore, one cannot easily draw a constant regret conclusion simply by controlling \(\sum_{k=1}^{\infty}\mathds{1}[V_{1}^{*}(s_{1}^{k})-V_{1}^{\pi}(s_{1}^{k})\geq\Delta]\) since the gap between \(V_{1}^{*}(s_{1}^{k})-V_{1}^{\pi}(s_{1}^{k})\) needs to be further fine-grained controlled. In short, the existence of \(\Delta\) describing the minimal gap between \(V^{*}(s)-Q^{*}(s,a)\) cannot be easily applied to controlling regret \(V^{*}(s)-V^{\pi}(s)\).

Our approach: A fine-grained concentration analysisWe address this challenge by providing a fine-grained concentration analysis in connecting the gap with the regret. Notice that the regret \(V_{h}^{*}(s_{h})-V_{h}^{\pi^{k}}(s_{h})\) in episode \(k\) is the expectation of cumulative suboptimality gap \(\mathbb{E}[\sum_{h=1}^{H}\Delta_{h}^{k}]\) taking over trajectory \(\{s_{h}^{k}\}_{h=1}^{H}\). In addition, the variance of the random variable can be self-bounded according to

\[\mathrm{Var}\left[\sum_{h=1}^{H}\Delta_{h}^{k}\right]\leq\mathbb{E}\left[ \left(\sum_{h=1}^{H}\Delta_{h}^{k}\right)^{2}\right]\leq H^{2}\,\mathbb{E} \left[\sum_{h=1}^{H}\Delta_{h}^{k}\right]=H^{2}\big{(}V_{1}^{*}(s_{1}^{k})-V_{ 1}^{\pi^{k}}(s_{1}^{k})\big{)}.\]

Denote \(\eta_{k}\) be the difference between \(V_{h}^{*}(s_{h})-V_{h}^{\pi^{k}}(s_{h})\) and the actual \(\sum_{h=1}^{H}\Delta_{h}^{k}\). Freedman inequality (Lemma H.5) implies that \(\sum_{t=1}^{T}\eta^{t}\geq aC\) and \(\sum_{t=1}^{T}\mathrm{Var}[\eta^{t}]\leq vC\) happens at the same with a small probability for certain constant \(a\) and \(v\). Using a fine-grained union bound statement over \(C\), we can reach the following statement indicates the cumulative regret can be upper bounded using the cumulative suboptimality gap:

**Lemma 6.6** (Lemma C.14, Informal).: The following statement holds with high probability:

\[\sum_{k=1}^{K}\big{(}V_{h}^{*}(s_{h})-V_{h}^{\pi^{k}}(s_{h})\big{)}\leq \widetilde{\mathcal{O}}\Big{(}\sum_{k=1}^{K}\sum_{h=1}^{H}\Delta_{h}^{k}+H^ {2}\Big{)}.\]

Comparing with Lemma 6.1 in He et al. (2021a), Lemma 6.6 eliminates the \(\log K\) dependence, which is achieved by the aforementioned fine-grained union bound. As a result, together with Lemma 6.4, we reach the desired statement that \(\texttt{Cert-LSVI-UCB}\) achieves constant regret bound when the misspecification is sufficiently small against the minimal suboptimality gap.

## 7 Conclusions and Limitations

In this work, we proposed a new algorithm, called certified estimator, for reinforcement learning with a misspecified linear function approximation. Our algorithm is parameter-free and does not require prior knowledge of misspecification level \(\zeta\) or the suboptimality \(\Delta\). Our algorithm is based on a novel certified estimator and provides the first constant regret guarantee for misspecified linear MDPs and (well-specified) linear MDPs.

Limitations.Despite these advancements, several aspects of our algorithm and analysis warrant further investigation. One significant open question is whether the dependency on the planning horizon and dimension \(d,H\) can achieve optimal instance-dependent regret bounds. For the gap-independent regret bounds, the regret lower bound is \(\Omega(d\sqrt{H^{3}K})\) as shown by Zhou et al. (2021), and this benchmark has recently been met by works such as He et al. (2022); Agarwal et al. (2022). Additionally, our analysis assumes uniform misspecification across all actions. Investigating other types of misspecifications could lead to more sophisticated results, enhancing the algorithm's robustness and applicability to diverse real-world scenarios. This exploration remains an important direction for future research.

## Acknowledgments and Disclosure of Funding

We thank the anonymous reviewers for their helpful comments. This work was done while WZ was a PhD student at UCLA. WZ is partially supported by UCLA dissertation year fellowship and the research fund from UCLA-Amazon Science Hub. JH and QG are partially supported by the research fund from UCLA-Amazon Science Hub. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.

## References

* Abbasi-Yadkori et al. (2011)Abbasi-Yadkori, Y., Pal, D. and Szepesvari, C. (2011). Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_**24** 2312-2320.
* Agarwal et al. (2022)Agarwal, A., Jin, Y. and Zhang, T. (2022). Vo \(q\) l: Towards optimal regret in model-free rl with nonlinear function approximation. _arXiv preprint arXiv:2212.06069_.
* Ayoub et al. (2020)Ayoub, A., Jia, Z., Szepesvari, C., Wang, M. and Yang, L. (2020). Model-based reinforcement learning with value-targeted regression. In _International Conference on Machine Learning_. PMLR.
* Cesa-Bianchi and Lugosi (2006)Cesa-Bianchi, N. and Lugosi, G. (2006). _Prediction, learning, and games_. Cambridge university press.
* Chu et al. (2011)Chu, W., Li, L., Reyzin, L. and Schapire, R. (2011). Contextual bandits with linear payoff functions. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_. JMLR Workshop and Conference Proceedings.
* Dann et al. (2021)Dann, C., Marinov, T. V., Mohri, M. and Zimmert, J. (2021). Beyond value-function gaps: Improved instance-dependent regret bounds for episodic reinforcement learning. _Advances in Neural Information Processing Systems_**34**.
* Du et al. (2019)Du, S. S., Kakade, S. M., Wang, R. and Yang, L. F. (2019). Is a good representation sufficient for sample efficient reinforcement learning? In _International Conference on Learning Representations_.
* Hao et al. (2020)Hao, B., Lattimore, T. and Szepesvari, C. (2020). Adaptive exploration in linear contextual bandit. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* He et al. (2022)He, J., Zhao, H., Zhou, D. and Gu, Q. (2022a). Nearly minimax optimal reinforcement learning for linear markov decision processes. _arXiv preprint arXiv:2212.06132_.
* He et al. (2021a)He, J., Zhou, D. and Gu, Q. (2021a). Logarithmic regret for reinforcement learning with linear function approximation. In _International Conference on Machine Learning_. PMLR.
* He et al. (2021b)He, J., Zhou, D. and Gu, Q. (2021b). Uniform-PAC bounds for reinforcement learning with linear function approximation. In _Advances in Neural Information Processing Systems_.
* He et al. (2021)He, J., Zhou, D., Zhang, T. and Gu, Q. (2022b). Nearly optimal algorithms for linear contextual bandits with adversarial corruptions. In _Advances in Neural Information Processing Systems_.
* Hoeffding (1963)Hoeffding, W. (1963). Probability inequalities for sum of bounded random variables.
* Ishfaq et al. (2021)Ishfaq, H., Cui, Q., Nguyen, V., Ayoub, A., Yang, Z., Wang, Z., Precup, D. and Yang, L. (2021). Randomized exploration in reinforcement learning with general value function approximation. In _International Conference on Machine Learning_. PMLR.
* Jaksch et al. (2010)Jaksch, T., Ortner, R. and Auer, P. (2010). Near-optimal regret bounds for reinforcement learning. _Journal of Machine Learning Research_**11**.
* Jia et al. (2020)Jia, Z., Yang, L., Szepesvari, C. and Wang, M. (2020). Model-based reinforcement learning with value-targeted regression. In _Learning for Dynamics and Control_. PMLR.
* Jin et al. (2020)Jin, C., Yang, Z., Wang, Z. and Jordan, M. I. (2020). Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_. PMLR.
* Kober et al. (2013)Kober, J., Bagnell, J. A. and Peters, J. (2013). Reinforcement learning in robotics: A survey. _The International Journal of Robotics Research_**32** 1238-1274.
* Lattimore et al. (2020)Lattimore, T., Szepesvari, C. and Weisz, G. (2020). Learning with good feature representations in bandits and in rl with a generative model. In _International Conference on Machine Learning_. PMLR.
* Lykouris et al. (2021)Lykouris, T., Simchowitz, M., Slivkins, A. and Sun, W. (2021). Corruption-robust exploration in episodic reinforcement learning. In _Conference on Learning Theory_. PMLR.
* Mnih et al. (2013)Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D. and Riedmiller, M. (2013). Playing atari with deep reinforcement learning. _arXiv preprint arXiv:1312.5602_.
* Modi et al. (2020)Modi, A., Jiang, N., Tewari, A. and Singh, S. (2020). Sample complexity of reinforcement learning using linearly combined model ensembles. In _International Conference on Artificial Intelligence and Statistics_.
* Papini et al. (2021a)Papini, M., Tirinzoni, A., Pacchiano, A., Restelli, M., Lazaric, A. and Pirotta, M. (2021a). Reinforcement learning in linear mdps: Constant regret and representation selection. _Advances in Neural Information Processing Systems_**34** 16371-16383.
* Papini et al. (2021b)Papini, M., Tirinzoni, A., Restelli, M., Lazaric, A. and Pirotta, M. (2021b). Leveraging good representations in linear contextual bandits. In _International Conference on Machine Learning_. PMLR.
* Simchowitz and Jamieson (2019)Simchowitz, M. and Jamieson, K. G. (2019). Non-asymptotic gap-dependent regret bounds for tabular mdps. _Advances in Neural Information Processing Systems_**32** 1153-1162.
* Takemura et al. (2021)Takemura, K., Ito, S., Hatano, D., Sumita, H., Fukunaga, T., Kakimura, N. and Kawarabayashi, K.-i. (2021). A parameter-free algorithm for misspecified linear contextual bandits. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Vial et al. (2022)Vial, D., Parulekar, A., Shakkottai, S. and Srikant, R. (2022). Improved algorithms for misspecified linear markov decision processes. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Wang et al. (2019)Wang, Y., Wang, R., Du, S. S. and Krishnamurthy, A. (2019). Optimism in reinforcement learning with generalized linear function approximation. In _International Conference on Learning Representations_.
* Wei et al. (2022)Wei, C.-Y., Dann, C. and Zimmert, J. (2022). A model selection approach for corruption robust reinforcement learning. In _International Conference on Algorithmic Learning Theory_. PMLR.
* Yang et al. (2021)Yang, K., Yang, L. and Du, S. (2021). Q-learning with logarithmic regret. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Yang et al. (2021)Yang, L. and Wang, M. (2020). Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound. In _International Conference on Machine Learning_. PMLR.
* Zanette et al. (2020a)Zanette, A., Brandfonbrener, D., Brunskill, E., Pirotta, M. and Lazaric, A. (2020a). Frequentist regret bounds for randomized least-squares value iteration. In _International Conference on Artificial Intelligence and Statistics_.
* Zanette et al. (2020b)Zanette, A., Lazaric, A., Kochenderfer, M. and Brunskill, E. (2020b). Learning near optimal policies with low inherent bellman error. In _International Conference on Machine Learning_. PMLR.
* Zhang et al. (2023a)Zhang, W., He, J., Fan, Z. and Gu, Q. (2023a). On the interplay between misspecification and sub-optimality gap: From linear contextual bandits to linear MDPs.
* Zhang et al. (2023b)Zhang, W., He, J., Fan, Z. and Gu, Q. (2023b). On the interplay between misspecification and sub-optimality gap in linear contextual bandits. _arXiv preprint arXiv:2303.09390_.
* Zhang et al. (2021)Zhang, W., He, J., Zhou, D., Zhang, A. and Gu, Q. (2021). Provably efficient representation learning in low-rank markov decision processes. _arXiv preprint arXiv:2106.11935_.
* Zhou et al. (2021a)Zhou, D., Gu, Q. and Szepesvari, C. (2021a). Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory_. PMLR.
* Zhou et al. (2021b)Zhou, D., He, J. and Gu, Q. (2021b). Provably efficient reinforcement learning for discounted mdps with feature mapping. In _International Conference on Machine Learning_. PMLR.

## Appendix A Additional Related Work

RL with linear function approximation.Recent years have witnessed a line of work focusing on RL with linear function approximation to tackle RL tasks in large state space. A widely studied MDP model is linear MDP (Jin et al., 2020), where both the transition kernel and the reward function are linear functions of a given feature mapping of the state-action pairs \(\mathbf{\phi}(s,a)\). Several works have developed RL algorithms with polynomial sample complexity or sublinear regret bound in this setting. For example, LSVI-UCB (Jin et al., 2020) has an \(\widetilde{\mathcal{O}}(\sqrt{d^{3}H^{4}K})\) regret bound, randomized LSVI (Zanette et al., 2020a) has an \(\widetilde{\mathcal{O}}(\sqrt{d^{3}H^{4}K})\) regret bound and Ishfaq et al. (2021) achieved an \(\widetilde{\mathcal{O}}(\sqrt{d^{3}H^{4}K})\). He et al. (2022a) then improves this regret bound to a nearly minimax-optimal result \(\widetilde{\mathcal{O}}(d\sqrt{H^{3}K})\) while Agarwal et al. (2022) provides a general function approximation extension given the above result. Linear mixture/kernel MDPs (Modi et al., 2020; Jia et al., 2020; Ayoub et al., 2020; Zhou et al., 2021b) have also emerged as another model that enables model-based RL with linear function approximation. In this setting, the transition kernel is a linear function of a feature mapping on the triplet of state, action, and next state \(\mathbf{\phi}(s,a,s^{\prime})\). Nearly minimax optimal regrets can be achieved for both finite-horizon episodic MDPs (Ayoub et al., 2020; Zhou et al., 2021a) and infinite-horizon discounted MDPs (Zhou et al., 2021b) under this assumption.

## Appendix B Additional Discussions on Algorithm Design and Result

### Comparison with He et al. (2021b)

It is worth comparing our algorithm with He et al. (2021b), which also provides a uniform PAC bound for linear MDPs. Both our algorithm and theirs utilize a multi-phase structure that maintains multiple regression-based value function estimators at different phases. Despite this similarity, there are several major differences between our algorithm and that in He et al. (2021b), which are highlighted as follows:

1. In Line 7 of Algorithm 1, when calculating the regression-based estimator, for different phase \(l\), we use the same regression target \(\widehat{V}_{h+1}^{k}\), while their algorithm uses different \(V_{h+1,l}^{k}\) for different phase \(l\).
2. When aggregating the regression estimators over all different \(L_{k}\) phases, we follow the arm elimination method as in Chu et al. (2011), while He et al. (2021b) simply take the point-wise minimum of all estimated state-action functions, i.e., \(Q(s,a)=\min_{l\in[L_{k}]}Q_{k,h}^{l}(s,a)\).

3. When calculating the phase \(l_{h}^{k}(s_{h}^{k})\) for a trajectory \(s_{1}^{k},s_{2}^{k},\cdots,s_{H}^{k}\), He et al. (2021b) require that the phase \(l_{h}^{k}(s_{h}^{k})\) to be monotonically decreasing with respect to the stage \(h\), i.e., \(l_{h}^{k}(s_{h}^{k})\leq l_{h-1}^{k}(s_{h-1}^{k})\) (see line 19 in Algorithm 2 in He et al. (2021b)). Such a requirement will lead to a poor estimation for later stages and thus increase the sample complexity. In contrast, we do not have this requirement or any other requirements related to \(l_{h}^{k}(s_{h}^{k})\) and \(l_{h-1}^{k}(s_{h-1}^{k})\).

As a result, by (3), He et al. (2021b) have to sacrifice some sample complexity to make their algorithm work for different target value functions \(V_{h+1,l}^{k}\). As a comparison, since we use the same regression target for different phase \(l\), we do not have to make such a sacrifice in (3). Moreover, by (2), He et al. (2021b) cannot deal with linear MDPs with misspecification, while our algorithm can handle misspecification as in Vial et al. (2022).

### Discussion on Lower Bounds of Sample Complexity

We present a lower bound from Du et al. (2019) to better illustrate the interplay between the misspecification level \(\zeta\) and the suboptimality gap \(\Delta\).

**Assumption B.1** (Assumption 4.3, Du et al. 2019, \(\zeta\)-Approximate Linear MDP).: There exists \(\zeta>0\), \(\mathbf{\theta}_{h}\in\mathbb{R}^{d}\) and \(\mathbf{\mu}_{h}:\mathcal{S}\mapsto\mathbb{R}^{d}\) for each stage \(h\in[H]\) such that for any \((s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}\), we have \(\left\lvert\mathbb{P}_{h}(s^{\prime}|s,a)-\langle\mathbf{\phi}(s,a),\mathbf{\mu}_{h}(s ^{\prime})\rangle\right\rvert\leq\zeta\) and \(\left\lvert r(s,a)-\langle\mathbf{\phi}(s,a),\mathbf{\theta}_{h}\rangle\right\rvert\leq\zeta\).

**Theorem B.2** (Theorem 4.2, Du et al. 2019).: There exists a family of hard-to-learn linear MDPs with action space \(|\mathcal{A}|=2\) and a feature mapping \(\mathbf{\phi}(s,a)\) satisfying Assumption B.1, such that for any algorithm that returns a \(1/2\)-optimal policy with probability \(0.9\) needs to sample at least \(\Omega(\min\{|\mathcal{S}|,2^{H},\exp(d\zeta^{2}/16)\})\) episodes.

**Remark B.3**.: As claimed in Du et al. (2019), Theorem B.2 suggests that when misspecification in the \(\ell_{\infty}\) norm satisfies \(\zeta=\Omega(\Delta\sqrt{H/d})\), the agent needs an exponential number of episodes to find a near-optimal policy, where \(\Delta=1/2\) in their setting. It is worth noting that Assumption B.1 is a \(\ell_{\infty}\) approximation for the transition matrix. Such a \(\ell_{\infty}\) guarantee (\(\|\cdot\|_{\infty}\leq\zeta\)) is weaker than the \(\ell_{1}\) guarantee (\(\|\cdot\|_{1}\leq\zeta\)) provided in Assumption 3.1. So it's natural to observe a positive result when making a stronger assumption and a negative result when making a weaker assumption. In addition, despite of this difference, one could find that \(\zeta\sim\Delta/\sqrt{d}\) plays a vital role in determining if the task can be efficiently learned. Similar positive and negative results are also provided in Lattimore et al. (2020); Zhang et al. (2023b) in the linear contextual bandit setting (a special case of linear MDP with \(H=1\)).

## Appendix C Constant Regret Guarantees for \(\mathsf{Cert\text{-}LSVI\text{-}UCB}\)

In this section, we present the proof of Theorem 5.1. To begin with, we recap the notations used in the algorithm and introduce several shorthand notations that would be employed for the simplicity of latter proof. The notation table is presented in Table 2.Any proofs not included in this section are deferred to Appendix D.

### Quantized State Value Function set \(\mathcal{V}_{h,l}^{k}\).

To begin our proof, we first extend the definition of \(\widehat{V}_{h,l}^{k}\) to arbitrary \(l\) and give a formal definition of the state value function class \(\mathcal{V}_{h,l}^{k}\) as we skip the detail of this definition in Section 6.

**Definition C.1**.: We extend the definition of state value function \(\widehat{V}_{h,l}^{k}\) to any tuple \((k,h,l)\in[K]\times[H]\times\mathbb{N}^{+}\) by

\[\widehat{V}_{h,l}^{k},\cdot,\cdot,\cdot=\mathsf{Cert\text{-}LinUCB}\big{(}s; \{\widetilde{\mathbf{w}}_{h,\ell}^{k}\}_{\ell=1}^{l},\{\widetilde{\mathbf{U}} _{h,\ell}^{k,-1}\}_{\ell=1}^{l},l\big{)}\]

We also define the state value function family \(\mathcal{V}_{h,l}^{k}\) be the set of all possible \(\widehat{V}_{h,l}^{k}\).

\[\mathcal{V}_{h,l}^{k}=\Big{\{}\widehat{V}_{h,l}^{k}\;\Big{|}\;\widehat{V}_{h,l} ^{k},\cdot,\cdot,\cdot=\mathsf{Cert\text{-}LinUCB}\big{(}s;\{\widetilde{ \mathbf{w}}_{\cdot,\ell}\}_{\ell=1}^{l},\{\widetilde{\mathbf{U}}_{\cdot,\ell}^{ \cdot,-1}\}_{\ell=1}^{l},l\big{)}\Big{\}}\]

where \(\{\widetilde{\mathbf{w}}_{\cdot,\ell}\}_{\ell=1}^{l}\) and \(\{\widetilde{\mathbf{U}}_{\cdot,\ell}^{\cdot,-1}\}_{\ell=1}^{l}\) are referring to _any_ possible parameters generated by Line 8 in Algorithm 1.

It is worth noting that one can check the definition of \(\widehat{V}^{k}_{h,l}\) here is consistent with those computed in Algorithm 2 with \(l<l^{k}_{h}(s)\). Therefore, we will not distinguish between the notations in the remainder of the proof.

The following lemma controls the distance between \(\widehat{V}^{k}_{h}(s)\) and \(\widehat{V}^{k}_{h,l}(s)\) for any phase \(l\).

**Lemma C.2**.: For any \((k,h,s)\in[K]\times[H]\times\mathcal{S},l\in[l^{k}_{h}(s)-1]\), it holds that

\[\widetilde{V}^{k}_{h,l}(s)\leq\widehat{V}^{k}_{h}(s)\leq\widehat{V}^{k}_{h,l}( s),\quad|\widehat{V}^{k}_{h}(s)-\widehat{V}^{k}_{h,l}(s)|\leq 6\cdot 2^{-l}.\]

Moreover, for any tuple \((k,h,s,l_{+})\in[K]\times[H]\times\mathcal{S}\times\mathbb{N}^{+}\), the difference \(|\widehat{V}^{k}_{h}(s)-\widehat{V}^{k}_{h,l_{+}}(s)|\) is bounded by \(6\cdot 2^{-l_{+}}\), following the extension of the definition scope of \(\widehat{V}^{k}_{h,l_{+}}\) as outlined in Definition C.1.

Lemma C.2 suggests that given any phase \(l_{+}\), \(\widehat{V}^{k}_{h,l}\) is close to \(\widehat{V}^{k}_{h}\). This enables us to construct covering on \(\widehat{V}^{k}_{h}\) using the covering on \(\widehat{V}^{k}_{h,l}\).

### Concentration of State Value Function \(\widehat{V}^{k}_{h}(s)\)

In this subsection, we provide a new analysis for bounding the self-normalized concentration of \(\left\|\sum_{\tau}\mathbf{\phi}^{\tau}_{h}\big{(}[\mathbb{P}_{h}\widehat{V}^{k}_{h }](s^{\tau}_{h},a^{\tau}_{h})-\widehat{V}^{k}_{h}(s^{\tau}_{h+1})\big{)} \right\|_{\mathbf{U}^{-1}}\) to get rid of the \(\log k\) factor in Vial et al. (2022).

\begin{table}
\begin{tabular}{c l} \hline Notation & Meaning \\ \hline \(\zeta\) & Misspecification level of feature map \(\phi_{h}\). (see Definition 3.1) \\ \(\Delta\) & Minimal suboptimality gap among \(\Delta_{h}\). (see Definition 3.3) \\ \(s^{k}_{h},a^{k}_{h}\) & States and actions introduced in the episode \(k\) by the policy \(\pi_{k}\). \\ \(Q^{\pi}_{h}(s,a),V^{\pi}_{h}(s)\) & Ground-truth state-action value function and state value function of policy \(\pi\). \\ \(Q^{\pi}_{h}(s,a),V^{\pi}_{h}(s)\) & The optimal ground-truth state-action value function and state value function. \\ \(\Delta_{h}(s,a)\) & Suboptimal gap with respect to the optimal policy \(\pi^{*}\). (see Definition 3.3) \\ \(\mathbb{P}_{h},B_{h}\) & The ground-truth transition kernel and the Bellman operator. \\ \hline \(\kappa_{l}\) & The quantification precision in the phase \(l\). (see Algorithm 1) \\ \(\gamma_{l}\) & The confidence radius in the phase \(l\). (see Theorem 5.1) \\ \(\mathbf{C}^{k}_{h,l}\) & Index sets during phase \(l\) in the episode \(k\). (see Algorithm 1) \\ \(\mathbf{w}^{k}_{h,l},\mathbf{U}^{k}_{h,l}\) & Empirical weights and covariance matrix in the phase \(l\). (see Algorithm 1) \\ \(\widetilde{\mathbf{w}}^{k}_{h,l},\widetilde{\mathbf{U}}^{k}_{h,l}\) & Quantified version of \(\mathbf{w}^{k}_{h,l}\) and \(\mathbf{U}^{k}_{h,l}\). (see Algorithm 1) \\ \(V^{k}_{h}(s)\) & The overall optimistic state value function. (see Algorithm 2) \\ \(Q^{k}_{h,l}(s,a)\) & Empirical state-action value function in phase \(l\). (see Algorithm 2) \\ \(V^{k}_{h,l}(s)\) & Empirical state value function in phase \(l\). (see Algorithm 2) \\ \(\widehat{\mathcal{V}}^{k}_{h,l}(s)\) & Optimistic state value function in phase \(l\). (see Definition C.1) \\ \(\widehat{V}^{k}_{h,l}(s)\) & Pessimistic state value function in phase \(l\). (see Algorithm 2) \\ \(\pi^{k}_{h}\) & Policy played in the episode \(k\). (see Algorithm 2) \\ \(\pi^{k}_{h,l}\) & Policy induced at state \(s\) during phase \(l\) of episode \(k\). (see Algorithm 2) \\ \(l^{k}_{h}(s)\) & The index of the phase at which state \(s\) stops in episode \(k\). (see Algorithm 2) \\ \(\phi^{k}_{h}\) & The feature vector observed in the episode \(k\). (see Algorithm 1) \\ \(\mathbf{V}^{k}_{h,l}\) & Function family of all optimistic state function \(\widehat{V}^{k}_{h,l}\). (see Definition C.1) \\ \(\gamma_{l,l_{+}}\) & The confidence radius with covering on phase \(l_{+}\). (see Definition C.3) \\ \(L_{+}\) & The phase offsets for the covering statement. (see Lemma C.5) \\ \(\chi\) & The inflation on misspecification. (see Lemma C.6) \\ \(L_{\zeta}\) & The deepest phase that tolerance \(\zeta\) misspecification. (see Lemma C.8) \\ \(L_{\varepsilon}\) & The shallowest phase that guarantees \(\varepsilon\) accuracy. (see Lemma C.9). \\ \(\Delta^{k}_{h}\) & The suboptimality gap of played policy \(\pi^{k}_{h}\) at state \(s^{k}_{h}\). (see Lemma C.13) \\ \(\mathcal{G}_{1}\) & The event defined in Definition C.3. \\ \(\mathcal{G}_{2}\) & The event defined in Definition D.13. \\ \(\mathcal{G}_{\varepsilon}\) & The condition defined in Definition C.10. \\ \hline \end{tabular}
\end{table}
Table 2: Notations used in algorithm and proof To facilitate our proof, we define the filtration list \(\mathcal{F}_{h}^{k}=\left\{\left\{s_{i}^{j},a_{i}^{j}\right\}_{i=1,j=1}^{H,k-1}, \left\{s_{i}^{k},a_{i}^{k}\right\}_{i=1}^{h}\right\}\). It is easy to verify that \(s_{h}^{k},a_{h}^{k}\) are both \(\mathcal{F}_{h}^{k}\)-measurable. Also, for any function \(V\) built on \(\mathcal{F}_{h}^{k}\), \([\mathbb{P}_{h}V](s_{h}^{k},a_{h}^{k})-V(s_{h+1}^{k})\) is \(\mathcal{F}_{h+1}^{k}\)-measurable and it is also a zero-mean random variable conditioned on \(\mathcal{F}_{h}^{k}\).

The first lemma we provide is similar with Vial et al. (2022), which shows the self-normalized concentration property for each phase \(l\) and any function \(V\in\mathcal{V}_{h,l}^{k}\).

**Definition C.3**.: For some fixed mapping \(l\mapsto l_{+}=l_{+}(l)\) that \(l_{+}\geq l\), we define the bad event as

\[\mathcal{B}_{1}(k,h,l,V)=\left\{\left\|\sum_{\tau\in\mathcal{C}_{h,l}^{k-1}} \phi_{h}^{\tau}\big{(}[\mathbb{P}_{h}V](s_{h}^{\tau},a_{h}^{\tau})-V(s_{h+1}^{ \tau})\big{)}\right\|_{(\mathbf{U}_{h,l}^{k})^{-1}}>\gamma_{l,l_{+}}\right\}.\]

The good event is defined by \(\mathcal{G}_{1}=\bigcap_{k=1}^{K}\bigcap_{h=1}^{H}\bigcap_{l\geq 1}\bigcap_{V\in \mathcal{V}_{h,l_{+}}^{k}}\mathcal{B}_{1}^{\complement}(k,h,l,V)\) where we define \(\gamma_{l,l_{+}}=5l_{+}dH\sqrt{\log(16dH/\delta)}=\tilde{\mathcal{O}}(ldH\log( \delta^{-1}))\).

**Lemma C.4**.: The good event \(\mathcal{G}_{1}\) defined in Definition C.3 happens with probability at least \(1-2\delta\).

Lemma C.4 establishes the concentration bounds for any given phase \(l\). However, the total number of phases for the state value function \(V_{h}^{k}(s)\) can be bounded only trivially by\(l=\mathcal{O}(\log K)\), resulting in \(\log K\) dependence. To address this issue, the following lemma proposes a method to eliminate this logarithmic factor:

**Lemma C.5**.: Under event \(\mathcal{G}_{1}\), for any \((k,h,l)\in[K]\times[H]\times\mathbb{N}^{+}\),

\[\left\|\sum_{\tau\in\mathcal{C}_{h,l}^{k-1}}\phi_{h}^{\tau}\big{(}[\mathbb{P} _{h}\widehat{V}_{h+1}^{k}](s_{h}^{\tau},a_{h}^{\tau})-\widehat{V}_{h+1}^{k}( s_{h+1}^{\tau})\big{)}\right\|_{(\mathbf{U}_{h,l}^{k})^{-1}}\leq 1.1\gamma_{l}.\] (C.1)

where we set \(\gamma_{l}=\gamma_{l,l_{+}}\) with \(l_{+}=l+20+\lceil\log(ld)\rceil\).

Then Lemma C.5 immediately yields the following lemma regarding the estimation error of the state-action value function \(Q_{h,l}^{k}\):

**Lemma C.6**.: Under event \(\mathcal{G}_{1}\), for any \((k,h,s)\in[K]\times[H]\times\mathcal{S},l\in[l_{h}^{k}(s)-f_{h}^{k}(s)],a_{l} \in\mathcal{A}_{h,l}^{k}(s)\),

\[\left|Q_{h,l}^{k}(s,a)-[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a)\right|\leq 2 \cdot 2^{-l}+\chi\sqrt{l}\zeta\] (C.2)

where we define \(\chi=12\sqrt{d}H\).

Lemma C.6 build an estimation error for any \(l\in[l_{h}^{k}(s)-1]\). As we mentioned in the algorithm design, a larger \(l\) here will lead to more precise estimation (a smaller \(2^{-l}\) term in (C.2)) but will suffer from a larger covering number (a larger \(\gamma_{l}\) term in (C.2)). Following a similar proof sketch from Vial et al. (2022), the next lemma shows that any action that is not eliminated has a low regret,

**Lemma C.7**.: Fix some arbitrary \(L_{0}\geq 1\) and let \(\chi=12\sqrt{d}H\). Under event \(\mathcal{G}_{1}\), for any \((k,h,s)\in[K]\times[H]\times\mathcal{S},l\in[\min\{L_{0},l_{h}^{k}(s)-f_{h}^{k }(s)\}],a_{l+1}\in\mathcal{A}_{h,l+1}^{k}(s)\),

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a)-[\mathbb{B}_{ h}\widehat{V}_{h+1}^{k}](s,a_{l+1})\leq 8\cdot 2^{-l}+2l\cdot\chi\sqrt{L_{0}}\zeta.\]

### The Impact of Misspecification Level \(\zeta\)

Next, we are ready to show the criteria where Line 11 in Algorithm 2 will be triggered, which shows the impact of misspecification on this multi-phased estimation.

**Lemma C.8**.: Under event \(\mathcal{G}_{1}\), for any \((k,h)\in[K]\times[H]\) such that \(f_{h}^{k}(s_{h}^{k})=0\), we have \(l_{h}^{k}(s_{h}^{k})>L_{\zeta}\) where \(L_{\zeta}\) is the maximal integer satisfying \(2^{-L_{\zeta}}\geq\chi L_{\zeta}^{1.5}\zeta\) for \(\chi=12\sqrt{d}H\), i.e., \(L_{\zeta}=\Omega(\log(1/\zeta))\).

Equipped with Lemma C.8, the following lemma suggests that how much estimation precision \(\varepsilon\) can be achieved by accumulating the error \(2^{-l_{h}^{k}(s_{h}^{k})}\) that occurred in Lemma C.6.

**Lemma C.9**.: Under event \(\mathcal{G}_{1}\) and for all \(\varepsilon>0\), define \(L_{\varepsilon}\) to be the minimal integer satisfying \(2^{-L_{\varepsilon}}\leq 0.01\varepsilon/H\), i.e., \(L_{\varepsilon}=\lceil-\log(0.01\varepsilon/H)\rceil\). When \(L_{\varepsilon}\leq L_{\zeta}\), then for any \(\mathcal{K}\subseteq[K],h\in[H]\),

\[\sum_{k\in\mathcal{K}}2^{-l_{h}^{k}(s_{h}^{k})}\leq 0.01|\mathcal{K}|\cdot \varepsilon/H+2^{12}L_{\varepsilon}dH\gamma_{L_{\varepsilon}}^{2}\cdot \varepsilon^{-1}.\]

The relationship between \(L_{\varepsilon}\leq L_{\zeta}\) can be translated to the relationship between \(\varepsilon\) and \(\zeta\). We characterize this condition as follows:

**Definition C.10**.: Condition \(\mathcal{G}_{\varepsilon}\) is defined for a given \(\varepsilon\), and is satisfied if \(L_{\zeta}\geq L_{\varepsilon}\) where \(L_{\varepsilon}\) is the minimal integer satisfying \(2^{-L_{\varepsilon}}\leq 0.01\varepsilon/H\) and \(L_{\zeta}\) is the maximal integer satisfying \(2^{-L_{\zeta}}\geq\chi L_{\zeta}^{1.5}\zeta\).

**Lemma C.11**.: If \(\varepsilon\geq\Omega\big{(}\sqrt{d}H^{2}\zeta\log^{2}(1/\zeta)\big{)}\), then \(\mathcal{G}_{\varepsilon}\) is satisfied.

Proof.: If \(\varepsilon\geq\Omega\big{(}\sqrt{d}H^{2}\zeta\log^{2}(1/\zeta)\big{)}\), we have

\[2^{-L_{\varepsilon}}\geq 0.005\varepsilon/H\geq 2\chi L_{\zeta}^{1.5}\zeta\geq 2 ^{-L_{\zeta}}.\]

where the first inequality is given by the definition of \(L_{\varepsilon}\), the last inequality is given by the definition of \(L_{\zeta}\), and the second inequality holds since \(H\chi L_{\zeta}^{1.5}\leq\mathcal{O}\big{(}\sqrt{d}H^{2}\log^{2}(1/\zeta)\big{)}\), and the last inequality is given by the definition of \(L_{\varepsilon}\) and \(L_{\zeta}\), respectively. Since \(2^{-l}\) decreases as \(l\) increases, we can conclude that \(L_{\varepsilon}\leq L_{\zeta}\). 

The above analysis of the interplay between misspecification level \(\zeta\) and precision \(\varepsilon\) yields the following important lemma in our proof, showing a local decision error across all \(h\in[H]\):

**Lemma C.12**.: Under Assumption 3.1, let \(\gamma_{l}=5(l+20+\lceil\log(ld)\rceil)dH\sqrt{\log(16dH/\delta)}\), for some fixed \(0<\delta<1/3\). With probability at least \(1-3\delta\), for any \(\varepsilon>\Omega\big{(}\sqrt{d}H^{2}\zeta\log^{2}(1/\zeta)\big{)}\) and \(h\in[H]\), we have

\[\sum_{k=1}^{\infty}\mathds{1}\left[V_{h}^{*}(s_{h}^{k})-V_{h}^{ \pi^{k}}(s_{h}^{k})\geq\varepsilon\right]\leq\mathcal{O}\big{(}d^{3}H^{4} \varepsilon^{-2}\log^{4}(dH\varepsilon^{-1})\log(\delta^{-1})\iota\big{)},\]

where \(\iota\) refers to some polynomial of \(\log\log(dH\varepsilon^{-1}\delta^{-1})\). This can also be written as

\[\Pr\Big{[}\exists\varepsilon>\varepsilon_{0},h\in[H],\sum_{k=1}^{\infty} \mathds{1}\left[V_{h}^{*}(s_{h}^{k})-V_{h}^{\pi^{k}}(s_{h}^{k})>\varepsilon \right]>f(\varepsilon,\delta)\Big{]}\leq\delta.\]

with \(\varepsilon_{0}=\widetilde{\Omega}(\sqrt{d}H^{2}\zeta)\) and \(f(\varepsilon,\delta)=\widetilde{\mathcal{O}}(d^{3}H^{4}\varepsilon^{-2}\log( \delta^{-1}))\).

### From Local Step-wise Decision Error to Constant Regret

The next lemma shows that the total incurred suboptimality gap is constant if the minimal suboptimality gap \(\Delta\) satisfies \(\Delta>\varepsilon_{0}\).

**Lemma C.13**.: Suppose an RL algorithm Alg. satisfies

\[\Pr\Big{[}\exists\varepsilon>\varepsilon_{0},h\in[H],\sum_{k=1}^{\infty} \mathds{1}\left[V_{h}^{*}(s_{h}^{k})-V_{h}^{\pi^{k}}(s_{h}^{k})>\varepsilon \right]>f(\varepsilon,\delta)\Big{]}\leq\delta,\]

such that \(f(\varepsilon,\delta)=\widetilde{\mathcal{O}}(C_{1}/\varepsilon+C_{2}/ \varepsilon^{2})\) where \(C_{1},C_{2}>0\) are constant in \(\varepsilon\), but may depend on other quantities such as \(d,H,\log(\delta^{-1})\). If the minimal suboptimality gap \(\Delta\) satisfies \(\Delta>\varepsilon_{0}\), then

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\Delta_{h}^{k}\leq\widetilde{\mathcal{O}}(C_{2}H/ \Delta+C_{1}H)\]

where \(\Delta_{h}^{k}=\Delta_{h}\big{(}s_{h}^{k},\pi_{h}^{k}(s_{h}^{k})\big{)}=V_{h}^ {*}(s_{h}^{k})-Q_{h}^{*}\big{(}s_{h}^{k},\pi_{h}^{k}(s_{h}^{k})\big{)}\) is the suboptimality gap suffered in stage \(h\) of episode \(k\).

The following Lemma is a refined version of Lemma 6.1 in He et al. (2021) that removes the dependence between regret and number of episodes \(K\).

**Lemma C.14**.: For each MDP \(\mathcal{M}(\mathcal{S},\mathcal{A},H,\{r_{h}\},\{\mathbb{P}_{h}\})\) and any \(\delta>0\), with probability at least \(1-\delta\), we have

\[\text{Regret}(K)<\widetilde{\mathcal{O}}\bigg{(}\sum_{k=1}^{K}\sum_{h=1}^{H} \Delta_{h}^{k}+H^{2}\log(1/\delta)\bigg{)}.\]

We are now ready to prove Theorem 5.1:

Proof of Theorem 5.1.: By plugging in Lemma C.12 and Lemma C.13 into Lemma C.14, we can reach the desired statement. 

## Appendix D Proof of Lemmas in Appendix C

In this section, we prove lemmas outlined in Appendix C. Any proofs not included in this section are deferred to Appendix E.

### Proof of Lemma c.2

Proof of Lemma c.2.: According to the criteria for Line 11, we have \(\widetilde{V}_{h,l}^{k}(s)\leq\widehat{V}_{h,l}^{k}(s)\) for any \(l\in[l_{h}^{k}(s)-1]\). From the definition of \(\widetilde{V}_{h,l}^{k}(s)\) and \(\widehat{V}_{h,l}^{k}(s)\), they are monotonic in \(l\) that \(\widehat{V}_{h,l-1}^{k}(s)\leq\widehat{V}_{h,l}^{k}(s)\) and \(\widehat{V}_{h,l}^{k}(s)\leq\widehat{V}_{h,l-1}^{k}(s)\) hold. Combining with \(\widehat{V}_{h+1}^{k}(s)=\widehat{V}_{h,l_{h}^{k}(s)-1}^{k}\), we have

\[\forall l\in[l_{h}^{k}(s)-1],\widetilde{V}_{h,l}^{k}(s)\leq\widehat{V}_{h}^{k} (s)\leq\widehat{V}_{h,l}^{k}(s)\] (D.1)

From the definition of \(\widehat{V}_{h,l}^{k}(s)\) and \(\widehat{V}_{h,l}^{k}(s)\), we have

\[0\leq\widehat{V}_{h,l}^{k}(s)-\widetilde{V}_{h,l}^{k}(s)\leq\big{(}\widehat{V }_{h,l}^{k}(s)-V_{h,l}^{k}(s)\big{)}+\big{(}V_{h,l}^{k}(s)-\widetilde{V}_{h,l} ^{k}(s)\big{)}\leq 6\cdot 2^{-l}.\] (D.2)

Plugging (D.1) into (D.2), we conclude that for any phase \(l\in[l_{h}^{k}(s)-1]\), it holds that \(|\widehat{V}_{h}^{k}(s)-\widehat{V}_{h,l}^{k}(s)|\leq 6\cdot 2^{-l}\).

Now consider the extended state value function \(\widehat{V}_{h,l_{+}}^{k}\) with an arbitrary \(l_{+}\in\mathbb{N}^{+}\). For every \(s\) where \(l_{+}\leq l_{h}^{k}(s)-1\), we have \(|\widehat{V}_{h}^{k}(s)-V_{h,l_{+}}^{k}(s)|\leq 6\cdot 2^{-l_{+}}\) as reasoned above. For the other \(s\in\mathcal{S}\) where \(l_{+}\geq l_{h}^{k}(s)\), we have \(\widehat{V}_{h,l}^{k}(s)=\widehat{V}_{h}^{k}(s)\) following the procedure of Algorithm 2. This suggest that \(|\widehat{V}_{h}^{k}(s)-\widehat{V}_{h,l_{+}}^{k}(s)|\leq 6\cdot 2^{-l_{+}}\) always holds. 

### Proof of Lemma c.4

The following Lemma shows the rounding only cast bounded effects on the recovered parameters.

**Lemma D.1**.: For any \((k,h,s)\in[K]\times[H]\times\mathcal{S},l\in[l_{h}^{k}(s)-f_{h}^{k}(s)],a\in \mathcal{A}_{h,l}^{k}(s)\), it holds that

\[\big{|}\big{\langle}\boldsymbol{\phi}(s,a),\mathbf{w}_{h,l}^{k}\big{\rangle}- \big{\langle}\boldsymbol{\phi}(s,a),\widetilde{\mathbf{w}}_{h,l}^{k}\big{\rangle} \big{|}\leq 0.01\cdot 2^{-4l},\big{|}\|\boldsymbol{\phi}(s,a)\|_{(\mathcal{ U}_{h,l}^{k})^{-1}}-\|\boldsymbol{\phi}(s,a)\|_{\widehat{\mathcal{U}}_{h,l}^{k-1}} \big{|}\leq 0.1\cdot 2^{-2l}.\]

The following lemma shows the number of episodes that are taken into regression \(|\mathcal{C}_{h,l}^{k}|\) is bounded independently from the number of episodes \(k\).

**Lemma D.2**.: For any tuple \((k,h,l)\in[K]\times[H]\times\mathbb{N}^{+}\), we have \(|\mathcal{C}_{h,l}^{k}|\leq 16l\cdot 4^{l}\gamma_{l}^{2}d\).

The following lemma shows the number of possible state value functions \(|\mathcal{V}_{h,l}^{k}|\) is bounded independently from the number of episodes \(k\).

**Lemma D.3**.: For any tuple \((k,h,l)\in[K]\times[H]\times\mathbb{N}^{+}\), we have \(|\mathcal{V}_{h,l}^{k}|\leq(2^{22}d^{6}H^{4})^{l^{2}d^{2}}\).

Now we are ready to prove Lemma C.4.

Proof of Lemma c.4.: Recall in Definition C.3, the good event defined by the union of each single bad event:

\[\mathcal{G}_{1}=\bigcap_{k=1}^{K}\bigcap_{h=1}^{H}\bigcap_{l\geq 1}\bigcap_{V\in \mathcal{V}_{h,l_{+}}^{k}}\mathcal{B}_{1}^{\complement}(k,h,l,V),\]

where each single bad event is given by

\[\mathcal{B}_{1}(k,h,l,V)=\Bigg{\{}\Bigg{\|}\sum_{\tau\in\mathcal{C}_{h,l}^{k-1 }}\mathbf{\phi}_{h}^{\tau}\big{(}[\mathbb{P}_{h}V](s_{h}^{\tau},a_{h}^{\tau})-V(s_{ h+1}^{\tau})\big{)}\Bigg{\|}_{(\mathbf{U}_{h,l}^{k})^{-1}}>\gamma_{l}\Bigg{\}},\]

in which \([\mathbb{P}_{h}V](s,a)=\mathbb{E}_{s^{\prime}\sim\mathbb{P}_{h}(\cdot|s,a)}\,V (s)\).

Consider some fixed \((h,l)\in[H]\times\mathbb{N}^{+}\), \(V\in\mathcal{V}_{h,l_{+}}^{K}\). Arrange elements of \(\mathcal{C}_{h,l}^{k}\) in ascending order as \(\{\tau_{i}\}_{i}\). Since the environment sample \(s_{h+1}^{\tau_{i}}\) according to \(\mathbb{P}_{h}(\cdot|s_{h}^{\tau_{i}},a_{h}^{\tau_{i}})\), we have \([\mathbb{P}_{h}V](s_{h}^{\tau_{i}},a_{h}^{\tau_{i}})-V(s_{h+1}^{\tau_{i}})\) is \(\mathcal{F}_{h}^{\tau_{i}}\)-measurable with \(\mathbb{E}\left[[\mathbb{P}_{h}V](s_{h}^{\tau_{i}},a_{h}^{\tau_{i}})-V(s_{h+1} ^{\tau_{i}})\big{|}\mathcal{F}_{h}^{\tau_{i}}\right]=0\). Since \(0\leq V(s_{h+1}^{\tau_{i}})\leq H\), we have \(|[\mathbb{P}_{h}V](s_{h}^{\tau_{i}},a_{h}^{\tau_{i}})-V(s_{h+1}^{\tau_{i}})|\leq H\). This further leads to

\[\Bigg{\|}\sum_{\tau\in\mathcal{C}_{h,l}^{k-1}}\mathbf{\phi}_{h}^{\tau} \big{(}[\mathbb{P}_{h}V](s_{h}^{\tau},a_{h}^{\tau})-V(s_{h+1}^{\tau})\big{)} \Bigg{\|}_{(\mathbf{U}_{h,l}^{k})^{-1}}\] \[=\Bigg{\|}\sum_{i=1}^{|\mathcal{C}_{h,l}^{k-1}|}\mathbf{\phi}_{h}^{ \tau_{i}}\big{(}[\mathbb{P}_{h}V](s_{h}^{\tau_{i}},a_{h}^{\tau_{i}})-V(s_{h+1} ^{\tau_{i}})\big{)}\Bigg{\|}_{(\mathbf{U}_{h,l}^{k})^{-1}}\] \[\leq H\sqrt{2d\ln\big{(}1+|\mathcal{C}_{h,l}^{k}|/(d\lambda)\big{)} +2\ln(l^{2}H|\mathcal{V}_{h,l_{+}}^{K}|/\delta)}\] \[\leq H\sqrt{2d\ln(1+l\cdot 4^{l}\gamma_{l}^{2})+2\ln(l^{2}H(2^{2 2}d^{6}H^{4})^{l_{+}^{2}d^{2}}/\delta)}\] \[\leq\gamma_{l,l_{+}},\]

where the first inequality holds following from the good event of probability \(1-\delta/(l^{2}H|\mathcal{V}_{h,l_{+}}^{K}|)\) defined in Lemma H.2 over filtration \(\{\mathcal{F}_{h}^{\tau_{i}}\}_{i}\), the second inequality is derived from combining Lemma D.2 and Lemma D.3, and the last inequality is given by Lemma G.3. According to Lemma H.2, we have the bad event \(\bigcup_{k=1}^{K}\mathcal{B}_{1}(k,h,l,V)\) happens with probability at most \(\delta/(l^{2}H|\mathcal{V}_{h,l_{+}}^{K}|)\). Taking union bound over all \((h,l)\in[H]\times\mathbb{N}^{+}\), \(V\in\mathcal{V}_{h,l_{+}}^{K}\), we have the bad event happens with probability at most

\[\Pr[\mathcal{G}_{1}^{\complement}]\leq\sum_{h=1}^{K}\sum_{l=1}^{ \infty}\sum_{V\in\mathcal{V}_{h,l_{+}}^{K}}\Pr\Big{[}\bigcup_{k=1}^{K}\mathcal{ B}_{1}(k,h,l,V)\Big{]}\leq\sum_{h=1}^{H}\sum_{l=1}^{\infty}\sum_{V\in \mathcal{V}_{h,l_{+}}^{K}}\frac{\delta}{l^{2}H|\mathcal{V}_{h,l_{+}}^{K}|}\leq 2\delta,\]

where the last inequality holds due to \(\sum_{n\geq 1}n^{-2}=\pi^{2}/6\). This completes our proof. 

### Proof of Lemma c.5

Proof of Lemma c.5.: Denote the martingale difference between \(\widehat{V}_{h,l_{+}}^{k}-\widehat{V}_{h}^{k}\) as:

\[\mu_{h,l}^{k}=[\mathbb{P}_{h}(\widehat{V}_{h,l_{+}}^{k}-\widehat{V}_{h+1}^{k})] (s_{h}^{k},\pi_{h}^{k}(s_{h}^{k}))-(\widehat{V}_{h,l_{+}}^{k}(s_{h+1}^{k})- \widehat{V}_{h+1}^{k}(s_{h+1}^{k})).\]

By triangle inequality:

\[\Bigg{\|}\sum_{\tau\in\mathcal{C}_{h,l}^{k-1}}\mathbf{\phi}_{h}^{\tau} \big{(}[\mathbb{P}_{h}\widehat{V}_{h+1}^{k}](s_{h}^{\tau},a_{h}^{\tau})- \widehat{V}_{h+1}^{k}(s_{h+1}^{\tau})\big{)}\Bigg{\|}_{(\mathbf{U}_{h,l}^{k})^{- 1}}\] \[\leq\Bigg{\|}\sum_{\tau\in\mathcal{C}_{h,l}^{k-1}}\mathbf{\phi}_{h}^{ \tau}\big{(}[\mathbb{P}_{h}V_{h,l_{+}}^{k}](s_{h}^{\tau},a_{h}^{\tau})-\widehat {V}_{h,l_{+}}^{k}(s_{h+1}^{\tau})\big{)}\Bigg{\|}_{(\mathbf{U}_{h,l}^{k})^{-1}}+ \Bigg{\|}\sum_{\tau\in\mathcal{C}_{h,l}^{k-1}}\mathbf{\phi}_{h}^{\tau}\mu_{h,l_{+}}^ {\tau}\Bigg{\|}_{(\mathbf{U}_{h,l}^{k})^{-1}}.\] (D.3)According to the definition of event \(\mathcal{G}_{1}\), we can upper bound the first term by

\[\Bigg{\|}\sum_{\tau\in\mathcal{C}^{k-1}_{h,l}}\phi_{h}^{\tau}\big{(}[\mathbb{P}_{ h}V^{k}_{h,l_{+}}](s_{h}^{\tau},a_{h}^{\tau})-\widehat{V}^{k}_{h,l_{+}}(s_{h+1}^{ \tau})\big{)}\Bigg{\|}_{(\mathbf{U}^{k}_{h,l})^{-1}}\leq\gamma_{l,l_{+}}=\gamma _{l}.\] (D.4)

According to Lemma C.2, we have \(|\widehat{V}^{k}_{h,l_{+}}(s)-\widehat{V}^{k}_{h+1}(s)|\leq 6\cdot 2^{-l_{+}}\) for any \(s\in\mathcal{S}\). Thus, the difference can be bounded by \(|\mu^{\tau}_{h,l_{+}}|\leq 6\cdot 2^{-l_{+}}\). Consequently, we can bound the second term by

\[\Bigg{\|}\sum_{\tau\in\mathcal{C}^{k-1}_{h,l}}\mathbf{\phi}_{h}^{\tau }\mu^{\tau}_{h,l_{+}}\Bigg{\|}_{(\mathbf{U}^{k}_{h,l})^{-1}} \leq 6\cdot 2^{-l_{+}}\sqrt{|\mathcal{C}^{k}_{h,l}|}\] \[\leq 6\cdot 2^{-l_{+}}\sqrt{16l\cdot 4^{l}\gamma_{l}^{2}d}\] \[=24\cdot 2^{l-l_{+}}\gamma_{l}\sqrt{ld},\] (D.5)

where the first inequality is provided by Lemma H.3, utilizing the condition \(|\mu^{\tau}_{h,l_{+}}|\leq 6\cdot 2^{-l_{+}}\), the second inequality is from Lemma D.2. By plugging in the definition of \(l_{+}\), we can further bound the final term of (D.5) by

\[\Bigg{\|}\sum_{\tau\in\mathcal{C}^{k-1}_{h,l}}\mathbf{\phi}_{h}^{\tau }\mu^{\tau}_{h,l_{+}}\Bigg{\|}_{(\mathbf{U}^{k}_{h,l})^{-1}}\leq 24\cdot 2^{l-l_{+} }\gamma_{l}\sqrt{ld}\leq 24\cdot 2^{-20}\gamma_{l}\leq 0.1\gamma_{l}.\] (D.6)

Plugging (D.4) and (D.6) into (D.3) yields the desired statement such that

\[\Bigg{\|}\sum_{\tau\in\mathcal{C}^{k-1}_{h,l}}\mathbf{\phi}_{h}^{\tau }\big{(}[\mathbb{P}_{h}\widehat{V}^{k}_{h+1}](s_{h}^{\tau},a_{h}^{\tau})- \widehat{V}^{k}_{h+1}(s_{h+1}^{\tau})\big{)}\Bigg{\|}_{(\mathbf{U}^{k}_{h,l}) ^{-1}}\leq 1.1\gamma_{l},\]

which concludes our proof. 

### Proof of Lemma c.6

The following lemma shows the state-action value function \(Q^{k}_{h,l}(s,a)\) is always well estimated.

**Lemma D.4**.: Under event \(\mathcal{G}_{1}\), for any \((k,h,l,s,a)\in[K]\times[H]\times\mathbb{N}^{+}\times\mathcal{S}\times\mathcal{A}\),

\[\big{|}Q^{k}_{h,l}(s,a)-[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)\big{|}\leq \big{(}1.2+8\sqrt{ld}H\cdot 2^{l}\zeta\big{)}\gamma_{l}\|\mathbf{\phi}(s,a)\|_{(\mathbf{U}^{k}_{h,l}) ^{-1}}+0.01\cdot 2^{-4l}+2H\zeta.\]

Equipped with Lemma D.1 and Lemma D.4, we are ready to prove Lemma C.6.

Proof of Lemma c.6.: In case that \(l\leq l^{k}_{h}(s)-f^{k}_{h}(s)\), for any \(a\in\mathcal{A}^{k}_{h,l}(s)\), we have that

\[\|\mathbf{\phi}(s,a)\|_{(\mathbf{U}^{k}_{h,l})^{-1}} \leq\|\mathbf{\phi}(s,a)\|_{\widetilde{\mathbf{U}}^{k,-1}_{h,l}}+\big{|} \|\mathbf{\phi}(s,a)\|_{(\mathbf{U}^{k}_{h,l})^{-1}}-\|\mathbf{\phi}(s,a)\|_{ \widetilde{\mathbf{U}}^{k,-1}_{h,l}}\big{|}\] \[\leq 2^{-l}\gamma_{l}^{-1}+0.1\cdot 2^{-2l}\leq 1.1\cdot 2^{-l} \gamma_{l}^{-1},\] (D.7)

where the first inequality holds due to triangle inequality, and in the second inequality, the first term is satisfied since state \(s\) passes the criterion in Line 9 in phase \(l\) and the second term follows from Lemma D.1, and the last inequality is given by Lemma G.2 which implies \(2^{l}>\gamma_{l}\). Plugging (D.7) into Lemma D.4 gives

\[\big{|}Q^{k}_{h,l}(s,a)-[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a) \big{|} \leq 0.01\cdot 2^{-4l}+1.32\cdot 2^{-l}+8.8\sqrt{ld}H\zeta+2H\zeta\] \[\leq 2\cdot 2^{-l}+12\sqrt{ld}H\zeta,\]

which proves the desired statement.

### Proof of Lemma C.7

Equipped with Lemma C.6, we are able to show several properties of the state value function \(V^{k}_{h,l}\) throught the arm-elimination process. The first lemma suggests that for any action \(a_{l}\in\mathcal{A}^{k}_{h,l}(s)\), there is at least one action \(a_{l+1}\in\mathcal{A}^{k}_{h,l+1}(s)\) close to \(a_{l}\) in terms of the Bellman operator \([\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)\) after the elimination.

**Lemma D.5**.: Under event \(\mathcal{G}_{1}\), for any \((k,h,s)\in[K]\times[H]\times\mathcal{S}\), \(l\in[\min\{L_{0},l^{k}_{h}(s)-f^{k}_{h}(s)\}],a_{l}\in\mathcal{A}^{k}_{h,l}(s)\), there exists \(a_{l+1}\in\mathcal{A}^{k}_{h,l+1}(s)\) that

\[[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l})-[\mathbb{B}_{h}\widehat{V}^{k}_ {h+1}](s,a_{l+1})\leq 2\chi\sqrt{L_{0}}\zeta\]

where \(\chi=12\sqrt{d}H\) for arbitrary \(L_{0}\geq 1\).

Then the following lemma shows that by induction on stage \(h\in[H]\), we can show the elimination process keep at least one near-optimal action \(a_{l+1}\in\mathcal{A}^{k}_{h,l+1}(s)\).

**Lemma D.6**.: Under event \(\mathcal{G}_{1}\), for any \((k,h,s)\in[K]\times[H]\times\mathcal{S},l\in[\min\{L_{0},l^{k}_{h}(s)-f^{k}_{h }(s)\}]\), there exists \(a_{l+1}\in\mathcal{A}^{k}_{h,l+1}(s)\) that,

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)-[\mathbb{B}_{ h}\widehat{V}^{k}_{h+1}](s,a_{l+1})\leq 2l\cdot\chi\sqrt{L_{0}}\zeta.\]

where \(\chi=12\sqrt{d}H\) for arbitrary \(L_{0}\geq 1\).

The following two lemmas indicate that the state value function \(V^{k}_{h,l}(s)\) on stage \(h\) is a good estimation for the state value function given by Bellman operator \(V(s)=\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)\).

**Lemma D.7**.: Under event \(\mathcal{G}_{1}\), for any \((k,h,s)\in[K]\times[H]\times\mathcal{S},l\in[\min\{L_{0},l^{k}_{h}(s)-f^{k}_{h }(s)\}]\),

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)-V^{k}_{h,l}(s )\leq 2\cdot 2^{-l}+(2l-1)\chi\sqrt{L_{0}}\zeta.\]

where \(\chi=12\sqrt{d}H\) for arbitrary \(L_{0}\geq 1\).

**Lemma D.8**.: Under event \(\mathcal{G}_{1}\), for any \((k,h,s)\in[K]\times[H]\times\mathcal{S},l\in[\min\{L_{0},l^{k}_{h}(s)-f^{k}_{ h}(s)\}]\),

\[V^{k}_{h,l}(s)-\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a) \leq 2\cdot 2^{-l}+\chi\sqrt{L_{0}}\zeta,\]

where \(\chi=12\sqrt{d}H\) for arbitrary \(L_{0}\geq 1\).

Now we are ready to show any actions remaining in the elimination process are near-optimal.

Proof of Lemma c.7.: First, according to Lemma D.7, we can write

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)-V^{k}_{h,l}( s)\leq 2\cdot 2^{-l}+(2l-1)\chi\sqrt{L_{0}}\zeta.\] (D.8)

Any action \(a_{l+1}\in\mathcal{A}^{k}_{h,l+1}(s)\) passes the elimination process will satisfy:

\[Q^{k}_{h,l}(s,a_{l+1})\geq V^{k}_{h,l}(s)-4\cdot 2^{-l}.\] (D.9)

According to Lemma C.6 with the condition that \(l\leq L_{0}\), we have that the empirical state-action value function \(Q^{k}_{h,l}(s,\cdot)\) is a good estimation for \([\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,\cdot)\) among every \(a_{l+1}\in\mathcal{A}^{k}_{l}(s)\) under event \(\mathcal{G}_{1}\):

\[\big{|}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l+1})-Q^{k}_{h,l}(s,a_{l+1}) \big{|}\leq 2\cdot 2^{-l}+\chi\sqrt{L_{0}}\zeta.\] (D.10)

Combining (D.8), (D.9), and (D.10) gives

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)- [\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l+1})\] \[= \big{(}\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1} ](s,a)-V^{k}_{h,l}(s)\big{)}+\big{(}V^{k}_{h,l}(s)-Q^{k}_{h,l}(s,a_{l+1})\big{)} +\big{(}Q^{k}_{h,l}(s,a_{l+1})-[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l+1} )\big{)}\] \[\leq \big{(}2\cdot 2^{-l}+(2l-1)\chi\sqrt{L_{0}}\zeta\big{)}+4\cdot 2^ {-l}+\big{(}2\cdot 2^{-l}+\chi\sqrt{L_{0}}\zeta\big{)}\] \[= 8\cdot 2^{-l}+2l\cdot\chi\sqrt{L_{0}}\zeta,\]

which proves the desired statement.

### Proof of Lemma c.8

The following two lemmas demonstrate that, at stage \(h\), both the optimistic state value function, \(\widehat{V}_{h,l}^{k}(s)\), and the pessimistic state value function, \(\widehat{V}_{h,l}^{k}(s)\), exhibit a gap relative to the state value function determined by the Bellman operator, given as \(V(s)=\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a)\).

**Lemma D.9**.: Under event \(\mathcal{G}_{1}\), for any \((k,h,s)\in[K]\times[H]\times\mathcal{S},l\in[\min\{L_{0},l_{h}^{k}(s)-f_{h}^{k }(s)\}]\),

\[\min\big{\{}V_{h,l}^{k}(s)+3\cdot 2^{-l},\widehat{V}_{h,l-1}^{k}(s)\big{\}}- \max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a)\geq 2^{-l}-(2 l-1)\chi\sqrt{L_{0}}\zeta,\]

where \(\chi=12\sqrt{d}H\) for arbitrary \(L_{0}\geq 1\). In case that \(l\leq l_{h}^{k}(s)-1\), the inequality is equivalent to

\[\widehat{V}_{h,l}^{k}(s)-\max_{a\in\mathcal{A}}[\mathbb{B}_{h} \widehat{V}_{h+1}^{k}](s,a)\geq 2^{-l}-(2l-1)\chi\sqrt{L_{0}}\zeta.\]

**Lemma D.10**.: Under event \(\mathcal{G}_{1}\), for any \((k,h,s)\in[K]\times[H]\times\mathcal{S},l\in[\min\{L_{0},l_{h}^{k}(s)-f_{h}^{k }(s)\}]\),

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a)- \max\big{\{}V_{h,l}^{k}(s)-3\cdot 2^{-l},\widetilde{V}_{h,l-1}^{k}(s)\big{\}} \geq 2^{-l}-\chi\sqrt{L_{0}}\zeta,\]

where \(\chi=12\sqrt{d}H\) for arbitrary \(L_{0}\geq 1\). In case that \(l\leq l_{h}^{k}(s)-1\), the inequality is equivalent to

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a) -\widetilde{V}_{h,l}^{k}(s)\geq 2^{-l}-\chi\sqrt{L_{0}}\zeta.\]

Proof of Lemma c.8.: Set \(L_{0}=L_{\zeta}\) be the maximal integer satisfying \(2^{-L_{\zeta}}-\chi L_{\zeta}^{1.5}\zeta\geq 0\). Combining Lemma D.10 and Lemma D.9, for any \(l\in[\min\{L_{0},l_{h}^{k}(s)-f_{h}^{k}(s)\}]\), we have that

\[\min\big{\{}V_{h,l}^{k}(s)+3\cdot 2^{-l},\widehat{V}_{h,l-1}^{k}(s )\big{\}}-\max\big{\{}V_{h,l}^{k}(s)-3\cdot 2^{-l},\widetilde{V}_{h,l-1}^{k}(s) \big{\}}\] \[=\big{(}\widehat{V}_{h,l}^{k}(s)-\max_{a\in\mathcal{A}}[\mathbb{B }_{h}\widehat{V}_{h+1}^{k}](s,a)\big{)}+\big{(}\max_{a\in\mathcal{A}}[ \mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a)-\widetilde{V}_{h,l}^{k}(s)\big{)}\] \[\geq\big{(}2^{-l}-(2l-1)\chi\sqrt{L_{0}}\zeta\big{)}+\big{(}2^{- l}-\chi\sqrt{L_{0}}\zeta\big{)}\] \[=2\cdot 2^{-l}-2l\cdot\chi\sqrt{L_{0}}\zeta\] \[\geq 2\cdot 2^{-L_{0}}-2\chi L_{0}^{1.5}\zeta\geq 0.\]

where the second inequality holds since \(2^{-l}\) decreases as \(l\) increases and the last inequality holds according to the selection of \(L_{0}\).

When \(f_{h}^{k}(s)=0\), consider \(l=l_{h}^{k}(s)\). The above reasoning indicates the criterion in Line 11 can never satisfied. Thus \(f_{h}^{k}(s)=0\) can only happen if \(l_{h}^{k}(s)>L_{0}=L_{\zeta}\). 

### Proof of Lemma c.9

By partitioning \([K]\) based on whether Algorithm 2 stops before phase \(L_{\varepsilon}\), we can prove Lemma C.9. Specifically, Lemma D.2 bounds the number of episodes in which Algorithm 2 stops before phase \(L_{\varepsilon}\). This allows us to establish an upper bound for the desired summation over these episodes. Furthermore, for episodes that stop after phase \(L_{\varepsilon}\), the contribution of \(2^{-l_{h}^{k}(s_{h}^{k})}\gamma_{l_{h}^{k}(s_{h}^{k})}\) is small according to the definition of \(L_{\varepsilon}\).

Proof of Lemma c.9.: Denote \(\mathcal{C}_{h,+}^{K}=[K]-\bigcup_{l=1}^{L_{\varepsilon}-1}\mathcal{C}_{h,l}^{K}\). In this sense, we have

\[\sum_{k\in\mathcal{K}}2^{-l_{h}^{k}(s_{h}^{k})}=\sum_{k\in\mathcal{K}\cap \mathcal{C}_{h,+}^{K}}2^{-l_{h}^{k}(s_{h}^{k})}+\sum_{l=1}^{L_{\varepsilon}-1} \sum_{k\in\mathcal{K}\cap\mathcal{C}_{h,l}^{K}}2^{-l_{h}^{k}(s_{h}^{k})}.\] (D.11)

From the construction of \(\mathcal{C}_{h,l}^{K}\), we have \(l_{h}^{k}(s_{h}^{k})=l\) for any \(k\in\mathcal{C}_{h,l}^{K}\). Fix some \(k\in\mathcal{C}_{h,+}^{K}\). If \(f_{h}^{k}(s_{h}^{k})=0\), we have \(l_{h}^{k}(s_{h}^{k})\geq L_{\zeta}\geq L_{\varepsilon}\) where the first inequality is given by Lemma C.8 and the second inequality is given by the assignment of \(L_{\varepsilon}\). Otherwise, we have \(l_{h}^{k}(s_{h}^{k})\geq L_{\varepsilon}\) according tothe definition of \(\mathcal{C}^{K}_{h,l}\). This indicates \(l^{k}_{h}(s^{k}_{h})\geq L_{\varepsilon}\) holds for any \(k\in\mathcal{C}^{K}_{h,+}\). This allow is to bound the first term by

\[\sum_{k\in\mathcal{K}\cap\mathcal{C}^{K}_{h,+}}2^{-l^{k}_{h}(s^{k}_{h})}\leq \sum_{k\in\mathcal{K}\cap\mathcal{C}^{K}_{h,+}}2^{-L_{\varepsilon}}\leq 0.01| \mathcal{K}|\cdot\varepsilon/H,\] (D.12)

where the first inequality holds since \(l^{k}_{h}(s^{k}_{h})>L_{\varepsilon}\) and the second inequality holds from both \(2^{-L_{\varepsilon}}\leq 0.01\varepsilon/H\) and \(|\mathcal{K}\cap\mathcal{C}^{K}_{h,+}|\leq|\mathcal{K}|\).

Furthermore, we can bound the second term by

\[\sum_{l=1}^{L_{\varepsilon}-1}\sum_{k\in\mathcal{K}\cap\mathcal{C }^{K}_{h,l}}2^{-l^{k}_{h}(s^{k}_{h})} \leq\sum_{l=1}^{L_{\varepsilon}-1}|\mathcal{K}\cap\mathcal{C}^{K} _{h,l}|\cdot 2^{-l}\] \[\leq\sum_{l=1}^{L_{\varepsilon}-1}16l\cdot 4^{l}\gamma_{l}^{2}d \cdot 2^{-l}\] \[\leq 16L_{\varepsilon}d\cdot 2^{L_{\varepsilon}}\gamma_{L_{ \varepsilon}}^{2}\leq 2^{12}L_{\varepsilon}dH\gamma_{L_{\varepsilon}}^{2} \varepsilon^{-1}.\] (D.13)

where the second inequality is given by Lemma D.2, and the last inequality holds due to \(0.005\varepsilon/H\leq 2^{-L_{\varepsilon}}\) which is because \(L_{\varepsilon}\) is a minimal integer such that \(2^{-L_{\varepsilon}}\leq 0.01\varepsilon/H\).

Finally, plugging (D.12) and (D.13) into (D.11) gives

\[\sum_{k\in\mathcal{K}}2^{-l^{k}_{h}(s^{k}_{h})}\leq 0.01|\mathcal{K}|\cdot \varepsilon/H+2^{12}L_{\varepsilon}dH\gamma_{L_{\varepsilon}}^{2}\varepsilon^ {-1}.\]

### Proof of Lemma c.12

The following lemma provides an upper bound for the underestimation error of the empirical state value function \(\widehat{V}^{k}_{h}\) with respect to the optimal state value function \(V^{*}_{h}\).

**Lemma D.11**.: Under event \(\mathcal{G}_{1}\) and for all \(\varepsilon>0\) that \(\mathcal{G}_{\varepsilon}\) is satisfied, for any \((k,h,s)\in[K]\times[H]\times\mathcal{S}\),

\[V^{*}_{h}(s)-\widehat{V}^{k}_{h}(s)\leq 0.07\varepsilon.\]

As \(\widehat{V}^{k}_{h}\) represents an empirical state value function with potentially optimal policy \(\pi^{k}_{h}(s)\), the following lemma provides an upper bound for the overestimation error of \(\widehat{V}^{k}_{h}\) with respect to deploying the policy \(\pi^{k}_{h}(s)\) on the ground-truth transition kernel.

**Lemma D.12**.: Under event \(\mathcal{G}_{1}\) and for all \(\varepsilon>0\) that \(\mathcal{G}_{\varepsilon}\) is satisfied, for any \((k,h,s)\in[K]\times[H]\times\mathcal{S}\),

\[\widehat{V}^{k}_{h}(s)-[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,\pi^{k}_{h}(s) )\leq 20\cdot 2^{-l^{k}_{h}(s)}+0.16\varepsilon/H.\]

To start with, we define a good event according to:

**Definition D.13**.: For some \(\varepsilon>0\), let \(\mathcal{K}^{\varepsilon}_{h}=\{k\in[K]:V^{*}_{h}(s^{k}_{h})-{V^{\pi^{k}}_{h }(s^{k}_{h})}\geq\varepsilon\}\). We define the bad event as

\[\mathcal{B}_{2}(h,\varepsilon)=\Bigg{\{}\sum_{k\in\mathcal{K}^{\varepsilon}_ {h}}\sum_{h^{\prime}=h}^{H}\eta^{k}_{h^{\prime}}>4\sqrt{H^{3}|\mathcal{K}^{ \varepsilon}_{h}|\log(4H|\mathcal{K}^{\varepsilon}_{h}|\log(\varepsilon^{-1}) /\delta)}\Bigg{\}}.\]

where \(\eta^{k}_{h}=[\mathbb{P}_{h}(\widehat{V}^{k}_{h+1}-{V^{\pi^{k}}_{h+1}})](s^{k} _{h},\pi^{k}_{h}(s^{k}_{h}))-\big{(}\widehat{V}^{k}_{h+1}(s^{k}_{h+1})-{V^{\pi^ {k}}_{h+1}}(s^{k}_{h+1})\big{)}\). The good event is defined as \(\mathcal{G}_{2}=\bigcap_{h=1}^{H}\bigcap_{l\geq 1}\mathcal{B}^{\mathsf{C}}_{2}(h,2 ^{-l})\).

The following lemma provides the concentration property such that the cumulative sample error is small with high probability.

**Lemma D.14**.: Event \(\mathcal{G}_{2}\) happens with probability at least \(1-\delta\).

Using the above results, we can bound the instantaneous regret of any subsets once the misspecification level is appropriately controlled,

**Lemma D.15**.: Under event \(\mathcal{G}_{1},\mathcal{G}_{2}\) and for all \(\varepsilon>0\) that \(\mathcal{G}_{\varepsilon}\) is satisfied, for any \(\mathcal{K}\subseteq[K]\) and \(h\in[H]\), it satisfies that

\[\sum_{k\in\mathcal{K}}\left(V_{h}^{*}(s_{h}^{k})-V_{h}^{\pi^{k}}(s_{h}^{k}) \right)\leq 0.49|\mathcal{K}|\varepsilon+2^{17}L_{\varepsilon}dH^{2}\gamma_{L_ {\varepsilon}}^{2}\varepsilon^{-1}+4\sqrt{H^{3}|\mathcal{K}|\log(4H|\mathcal{K} |\log(\varepsilon^{-1})/\delta)}.\]

With all lemmas stated above, we can show \(\mathtt{Cert\_-LSVI\_-UCBachieves}\) constant step-wise decision error. The following lemma gives a sufficient condition that \(\mathcal{G}_{\varepsilon}\) defined in Definition C.10 is satisfied.

Now, we are ready to prove Lemma C.12.

Proof of Lemma c.12.: We focus on the case where the good event \(\mathcal{G}_{1}\cap\mathcal{G}_{2}\cap\mathcal{G}_{\varepsilon}\) occurs. By the union bound statement over Lemma C.4 and Lemma D.14, and Lemma C.11, this good event happens with a probability of at least \(1-3\delta\) and requires \(\varepsilon\geq\Omega(\zeta\sqrt{d}H^{2}\log^{2}(dH\zeta^{-1}))\). W.l.o.g, consider \(\mathcal{K}_{h}^{\varepsilon}\) for some \(h\in[H]\) and \(\varepsilon=2^{-l}\) where \(l>0\) is an integer. On the one hand, we have

\[\sum_{k\in\mathcal{K}_{h}^{\varepsilon}}\left(V_{h}^{*}(s_{h}^{k})-V_{h}^{\pi^ {k}}(s_{h}^{k})\right)\geq|\mathcal{K}_{h}^{\varepsilon}|\varepsilon.\] (D.14)

On the other hand, Lemma D.15 gives

\[\sum_{k\in\mathcal{K}_{h}^{\varepsilon}}\left(V_{h}^{*}(s_{h}^{k} )-V_{h}^{\pi^{k}}(s_{h}^{k})\right) \leq 0.49|\mathcal{K}_{h}^{\varepsilon}|\varepsilon+2^{17}L_{ \varepsilon}dH^{2}\gamma_{L_{\varepsilon}}^{2}\varepsilon^{-1}\] \[\quad+4\sqrt{H^{3}|\mathcal{K}_{h}^{\varepsilon}|\log(4H|\mathcal{ K}_{h}^{\varepsilon}|\log(\varepsilon^{-1})/\delta)}.\] (D.15)

Combining (D.14) and (D.15) gives

\[0.51|\mathcal{K}_{h}^{\varepsilon}|\varepsilon\leq 2^{17}L_{\varepsilon}dH^{2 }\gamma_{L_{\varepsilon}}^{2}\varepsilon^{-1}+4\sqrt{H^{3}|\mathcal{K}_{h}^{ \varepsilon}|\log(4H|\mathcal{K}_{h}^{\varepsilon}|\log(\varepsilon^{-1})/ \delta)}.\]

Plugging the value of \(\gamma_{L_{\varepsilon}}\), we have

\[0.51|\mathcal{K}_{h}^{\varepsilon}|\varepsilon \leq 2^{22}L_{\varepsilon}(L_{\varepsilon}+\log(2^{20}dH))^{2}d^{3 }H^{4}\varepsilon^{-1}\log(16L_{\varepsilon}d/\delta)\] \[\quad+4\sqrt{H^{3}|\mathcal{K}_{h}^{\varepsilon}|\log(4H| \mathcal{K}_{h}^{\varepsilon}|\log(\varepsilon^{-1})/\delta)}.\] (D.16)

According to Lemma G.4, (D.16) implies

\[|\mathcal{K}_{h}^{\varepsilon}|\leq\mathcal{O}(L_{\varepsilon}(L_{ \varepsilon}+\log(dH))^{2}d^{3}H^{4}\varepsilon^{-2}\log(L_{\varepsilon}d) \log(\delta^{-1})\iota),\]

where \(\iota\) refers to a polynomial of \(\log\log(dH\varepsilon^{-1}\delta^{-1})\). With the definition of \(L_{\varepsilon}\), we conclude that

\[|\mathcal{K}_{h}^{\varepsilon}|\leq\mathcal{O}(d^{3}H^{4}\varepsilon^{-2} \log^{4}(dH\varepsilon^{-1})\log(\delta^{-1})\iota).\]

### Proof of Lemma c.13

Proof of Lemma c.13.: From the definition of suboptimality gap, we have

\[\Delta_{h}^{k} =V_{h}^{*}(s_{h}^{k})-[\mathbb{B}_{h}V_{h+1}^{*}](s_{h}^{k},\pi_{ h}^{k}(s_{h}^{k}))\] \[\leq V_{h}^{*}(s_{h}^{k})-[\mathbb{B}_{h}V_{h+1}^{\pi^{k}}](s_{h} ^{k},\pi_{h}^{k}(s_{h}^{k}))\] \[=V_{h}^{*}(s_{h}^{k})-V_{h}^{\pi^{k}}(s_{h}^{k}).\] (D.17)

According to the assumption,

\[\sum_{k=1}^{K}\mathds{1}\left[V_{h}^{*}(s_{1}^{k})-V_{h}^{\pi^{k}}(s_{h}^{k}) \geq\varepsilon\right]\leq\left(\frac{C_{1}}{\varepsilon}+\frac{C_{2}}{ \varepsilon^{2}}\right)\log^{a}\left(\frac{C_{1}}{\varepsilon}+\frac{C_{2}}{ \varepsilon^{2}}\right)\]holds for every \(\varepsilon>\varepsilon_{0}\) with probability at least \(1-\delta\), replacing the \(V_{h}^{*}(s_{1}^{k})-V_{h}^{\pi^{k}}(s_{h}^{k})\) with its lower bound \(\Delta_{h}^{k}\) yields for every \(\varepsilon>\varepsilon_{0}\),

\[\sum_{k=1}^{K}\mathds{1}\left[\Delta_{h}^{k}\geq\varepsilon\right]\leq\Big{(} \frac{C_{1}}{\varepsilon}+\frac{C_{2}}{\varepsilon^{2}}\Big{)}\log^{a}\Big{(} \frac{C_{1}}{\varepsilon}+\frac{C_{2}}{\varepsilon^{2}}\Big{)}.\]

In addition, according to the definition of minimal suboptimality gap \(\Delta\) in Definition 3.3, we have \(\Delta_{h}^{k}\) is either \(0\) or no less than \(\Delta\). Since for any \(x\in\{0\}\cup[\Delta,H]\), it holds that \(x\leq\Delta\cdot\mathds{1}[x\geq\Delta]+\int_{\Delta}^{H}\mathds{1}[x\geq \varepsilon]\,\mathrm{d}\varepsilon\), we decompose the total suboptimality incurred in stage \(h\) by

\[\sum_{k=1}^{K}\Delta_{h}^{k} \leq\sum_{k=1}^{K}\left(\Delta\cdot\mathds{1}\left[\Delta_{h}^{k }\geq\Delta\right]+\int_{\Delta}^{H}\mathds{1}\left[\Delta_{h}^{k}\geq \varepsilon\right]\mathrm{d}\varepsilon\right)\] \[=\Delta\sum_{k=1}^{K}\mathds{1}\left[\Delta_{h}^{k}\geq\Delta \right]+\int_{\Delta}^{H}\sum_{k=1}^{K}\mathds{1}\left[\Delta_{h}^{k}\geq \varepsilon\right]\mathrm{d}\varepsilon.\] (D.18)

In case that \(\varepsilon_{0}\leq\Delta\), the first term in (D.18) can be bounded by

\[\Delta\sum_{k=1}^{K}\mathds{1}\left[\Delta_{h}^{k}\geq\Delta\right]\leq\Delta \Big{(}\frac{C_{1}}{\Delta}+\frac{C_{2}}{\Delta^{2}}\Big{)}\log^{a}\Big{(} \frac{C_{1}}{\Delta}+\frac{C_{2}}{\Delta^{2}}\Big{)}.\] (D.19)

We can further bound the second term by

\[\int_{\Delta}^{H}\sum_{k=1}^{K}\mathds{1}\left[V_{1}^{*}(s_{1}^{k })-V_{1}^{\pi^{k}}(s_{1}^{k})\geq\varepsilon\right]\mathrm{d}\varepsilon \leq\int_{\Delta}^{H}\Big{(}\frac{C_{1}}{\varepsilon}+\frac{C_{ 2}}{\varepsilon^{2}}\Big{)}\log^{a}\Big{(}\frac{C_{1}}{\varepsilon}+\frac{C_{ 2}}{\varepsilon^{2}}\Big{)}\mathrm{d}\varepsilon\] \[\leq\log^{a}\Big{(}\frac{C_{1}}{\Delta}+\frac{C_{2}}{\Delta^{2}} \Big{)}\cdot\Big{(}C_{1}\ln\frac{H}{\Delta}+\frac{C_{2}}{\Delta}\Big{)}\] \[\leq(C_{1}\log H+C_{2}/\Delta)\cdot\mathrm{polylog}(C_{1},C_{2}, \Delta^{-1}).\] (D.20)

Plugging (D.19) and (D.20) into (D.18) with summation over \(h\in[H]\), we conclude that the total suboptimality incurred in stage \(h\) is bounded by

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\Delta_{h}^{k} \leq H\cdot(C_{1}+C_{2}/\Delta+C_{1}\log H+C_{2}/\Delta)\cdot \mathrm{polylog}(C_{1},C_{2},\Delta^{-1})\] \[\leq\widetilde{\mathcal{O}}(C_{2}H/\Delta+C_{1}H).\]

### Proof of Lemma c.14

Proof of Lemma c.14.: For a given policy \(\pi\) and any state \(s_{h}\in\mathcal{S}\), we have

\[V_{h}^{*}(s_{h})-V_{h}^{\pi}(s_{h})\] \[=\big{(}V_{h}^{*}(s_{h})-[\mathbb{B}_{h}V_{h+1}^{*}](s_{h},\pi_{h }(s_{h}))\big{)}+\big{(}[\mathbb{B}_{h}V_{h+1}^{*}](s_{h},\pi_{h}(s_{h}))-[ \mathbb{B}_{h}V_{h+1}^{*}](s_{h},\pi_{h}(s_{h}))\big{)}\] \[=\Delta_{h}(s_{h},\pi_{h}(s_{h}))+[\mathbb{P}_{h}(V_{h+1}^{*}-V_{ h+1}^{*})](s_{h},\pi_{h}(s_{h})),\]

where the second equality is given by the definition of suboptimality gap \(\Delta_{h}(\cdot,\cdot)\) in Definition 3.3. Taking expectation on both sides with respect to the randomness of state-transition and taking telescoping sum over all \(h\in[H]\) gives

\[V_{1}^{*}(s_{1})-V_{h}^{\pi}(s_{1})=\mathbb{E}\,\bigg{[}\sum_{h=1}^{H}\Delta_{h }(s_{h},\pi_{h}(s_{h}))\bigg{]},\]

where \(s_{h+1}\sim\mathbb{P}_{h}(\cdot|s_{h},\pi_{h}(s_{h}))\). Let the filtration list be \(\mathcal{F}^{k}=\Big{\{}\{s_{i}^{j},a_{i}^{j}\}_{i=1,j=1}^{H,k-1}\Big{\}}\), we have

\[\mathbb{E}\,\Big{[}\sum_{h=1}^{H}\Delta_{h}^{k}\Big{|}\mathcal{F}^{k}\Big{]}=V _{1}^{*}(s_{1}^{k})-V_{h}^{\pi^{k}}(s_{1}^{k}).\]Denote random variable \(\eta^{k}=\left(V_{1}^{*}(s_{1}^{k})-V_{h}^{\pi^{k}}(s_{1}^{k})\right)-\sum_{h=1}^{H }\Delta_{h}^{k}\). We can see \(\eta^{k}\) is \(\mathcal{F}_{k+1}\)-measurable with \(|\operatorname{\mathbb{E}}[\eta^{k}|\mathcal{F}^{k}]|=0\). Furthermore, for the variance of \(\eta^{k}\), we have

\[\operatorname{Var}[\eta^{k}|\mathcal{F}^{k}] \leq\operatorname{\mathbb{E}}\Big{[}\Big{(}\sum_{h=1}^{H}\Delta_{ h}^{k}\Big{)}^{2}\Big{|}\mathcal{F}^{k}\Big{]}\] \[\leq H^{2}\operatorname{\mathbb{E}}\Big{[}\sum_{h=1}^{H}\Delta_{ h}^{k}\Big{|}\mathcal{F}^{k}\Big{]}\] \[=H^{2}\big{(}V_{1}^{*}(s_{1}^{k})-V_{h}^{\pi^{k}}(s_{1}^{k})\big{)},\]

where the first inequality holds due to \(\operatorname{Var}[X]\leq\operatorname{\mathbb{E}}[(X-t)^{2}]\) for any fixed \(t\), the second inequality follows \(0\leq\Delta_{h}^{k}\leq H\). As a result, the total variance of the random variables \(\{\eta^{k}\}\) can be bounded by

\[\sum_{k=1}^{K}\operatorname{Var}[\eta^{k}|\mathcal{F}^{k}]\leq\sum_{k=1}^{K}H^ {2}\big{(}V_{1}^{*}(s_{1}^{k})-V_{h}^{\pi^{k}}(s_{1}^{k})\big{)}=H^{2}\text{ Regret}(K).\]

Let \(F(x)=\sqrt{2xH^{2}\log(x/\delta)}+H^{2}\log(x/\delta)\), using peeling technique, we can write

\[\Pr\Big{[}\Big{(}\sum_{k=1}^{K}\eta^{k}\Big{)}\geq F(\text{Regret }(K)),1<\text{Regret}(K)\Big{]}\] \[=\Pr\Big{[}\Big{(}\sum_{k=1}^{K}\eta^{k}\Big{)}\geq F(\text{Regret }(K)),1<\text{Regret}(K),\sum_{k=1}^{K}\operatorname{Var}[\eta^{k}|\mathcal{F }^{k}]\leq H^{2}\text{Regret}(K)\Big{]}\] \[\leq\sum_{i=1}^{\infty}\Pr\Big{[}\Big{(}\sum_{k=1}^{K}\eta^{k} \Big{)}\geq F(2^{i}),\sum_{k=1}^{K}\operatorname{Var}[\eta^{k}|\mathcal{F}^{k }]\leq 2^{i}H^{2}\Big{]}\] \[\leq\sum_{i=1}^{\infty}\exp\Big{(}\frac{-F(2^{i})^{2}}{2^{i+1}H^ {2}+2F(2^{i})H^{2}/3}\Big{)},\] (D.21)

where the last inequality follows Lemma H.5. Plugging \(F(x)=\sqrt{2xH^{2}\log(x/\delta)}+H^{2}\log(x/\delta)\) back into (D) yields

\[\Pr\Big{[}\Big{(}\sum_{k=1}^{K}\eta^{k}\Big{)}\geq\sqrt{2\text{ Regret}(K)H^{2}\log(\text{Regret}(K)/\delta)}+H^{2}\log(\text{Regret}(K)/ \delta),1<\text{Regret}(K)\Big{]}\] \[\leq\sum_{i=1}^{\infty}\exp(-\log(2^{i}/\delta))=\sum_{i=1}^{ \infty}\delta/2^{i}=\delta.\]

Therefore, whenever \(\text{Regret}(K)>1\), with probability at least \(1-\delta\), we have

\[\sum_{k=1}^{K}\eta^{k}<\sqrt{2\text{Regret}(K)H^{2}\log(\text{Regret}(K)/ \delta)}+H^{2}\log(\text{Regret}(K)/\delta).\]

Combining with the fact that \(\text{Regret}(K)=\sum_{k=1}^{K}\eta^{k}+\sum_{k=1}^{K}\sum_{h=1}^{H}\Delta_{h} ^{k}\), we have

\[\text{Regret}(K)<\sum_{k=1}^{K}\sum_{h=1}^{H}\Delta_{h}^{k}+\sqrt{2\text{Regret }(K)H^{2}\log(\text{Regret}(K)/\delta)}+H^{2}\log(\text{Regret}(K)/\delta),\]whenever \(\text{Regret}(K)>1\). Taking \(x=\text{Regret}(K)\), \(a=\sum_{k=1}^{K}\sum_{h=1}^{H}\Delta_{h}^{k}+H^{2}\log(\text{Regret}(K)/\delta)\), and \(b=2H^{2}\log(\text{Regret}(K)/\delta)\), inequality (1) yields

\[\text{Regret}(K) \leq 2\sum_{k=1}^{K}\sum_{h=1}^{H}\Delta_{h}^{k}+2H^{2}\log(\text{ Regret}(K)/\delta)+4H^{2}\log(\text{Regret}(K)/\delta)\] \[=2\sum_{k=1}^{K}\sum_{h=1}^{H}\Delta_{h}^{k}+6H^{2}\log(1/\delta)+ 6H^{2}\log(\text{Regret}(K))\] (D.22)

According to the fact that \(x\leq a\log x+b\Rightarrow x\leq 4a\log(2a)+2b\), letting \(x=\text{Regret}(K),a=2\sum_{k=1}^{K}\sum_{h=1}^{H}\Delta_{h}^{k}+6H^{2}\log(1/ \delta)\) and \(b=6H^{2}\), (D.22) becomes

\[\text{Regret}(K) \leq\left(8\sum_{k=1}^{K}\sum_{h=1}^{H}\Delta_{h}^{k}+24H^{2} \log(1/\delta)\right)\log\left(4\sum_{k=1}^{K}\sum_{h=1}^{H}\Delta_{h}^{k}+12H ^{2}\log(1/\delta)\right)+12H^{2}.\]

Hiding the logarithmic factors within the \(\widetilde{O}\) notation yields

\[\text{Regret}(K)<\widetilde{\mathcal{O}}\Big{(}\sum_{k=1}^{K}\sum_{h=1}^{H} \Delta_{h}^{k}+H^{2}\log(1/\delta)\Big{)}.\]

## Appendix E Proof of Lemmas in Appendix D

In this section, we prove lemmas outlined in Appendix D. Any proofs not included in this section are deferred to Appendix F.

### Proof of Lemma d.1

We first introduce the claim from Vial et al. (2022) controlling the rounding error:

**Lemma E.1** (Claim 1, Vial et al. 2022, restate).: For any \((k,h,l,s,a)\in[K]\times[H]\times\mathbb{N}^{+}\times\mathcal{S}\times\mathcal{A}\), we have

\[\mathbf{\phi}(s,a)^{\top}(\mathbf{w}_{h,l}^{k}-\widetilde{\mathbf{w}}_{h,l}^{k}) \leq\sqrt{d}\kappa_{l},\big{\|}\mathbf{\phi}(s,a)_{(\mathbf{U}_{h,l}^{k})^{-1}}- \|\mathbf{\phi}(s,a)\|_{\widetilde{\mathbf{U}}_{h,l}^{k,-1}}\big{\|}\leq\sqrt{d \kappa_{l}},\]

where \(\kappa_{l}\) is used to quantify the vector \(\mathbf{w}_{h,l}^{k}\) and inverse matrix \((\mathbf{U}_{h,l}^{l})^{-1}\).

Proof of Lemma d.1.: From Lemma E.1 we have

\[\big{|}\big{\langle}\mathbf{\phi}(s,a),\mathbf{w}_{h,l}^{k}\big{\rangle}-\big{ \langle}\mathbf{\phi}(s,a),\widetilde{\mathbf{w}}_{h,l}^{k}\big{\rangle}\big{|} \leq\sqrt{d}\kappa_{l}\leq 0.01\cdot 2^{-4l}\]

where the first inequality is due to Lemma E.1, and the second inequality is valid due to \(\kappa_{l}=0.01\cdot 2^{-4l}\). Similarly, we have

\[\big{\|}\mathbf{\phi}(s,a)\|_{(\mathbf{U}_{h,l}^{k})^{-1}}-\|\mathbf{\phi}(s,a)\|_{ \widetilde{\mathbf{U}}_{h,l}^{k-1}}\big{|}\leq\sqrt{d\kappa_{l}}\leq 0.1 \cdot 2^{-2l}.\]

### Proof of Lemma d.2

Proof of Lemma d.2.: First, both \(l_{h}^{\tau}(s_{h}^{\tau})=l\) and \(f_{h}^{\tau}(s_{h}^{\tau})=1\) held for any \(\tau\in\mathcal{C}_{h,l}^{k}\). This implies that the criteria for either Line 7 or Line 9 holds as \(l=l_{h}^{\tau}(s_{h}^{\tau})\). For \(\tau\) that satisfies the first criterion, we have \(l_{h}^{\tau}(s_{h}^{\tau})>L_{\tau}\). Note that \(L_{\tau}=\max\{\lceil\log_{4}(\tau/d)\rceil,0\}\), so this only happens for \(\tau<4^{l}d\). For other \(\tau\) that satisfies the second criterion, we have that

\[\|\mathbf{\phi}_{h}^{\tau}\|_{(\mathbf{U}_{h,l}^{\tau})^{-1}}\geq\|\mathbf{\phi}_{h}^{ \tau}\|_{\widetilde{\mathbf{U}}_{h,l}^{\tau,-1}}-\big{\|}\mathbf{\phi}_{h}^{\tau} \|_{\widetilde{\mathbf{U}}_{h,l}^{\tau,-1}}-\|\mathbf{\phi}_{h}^{\tau}\|_{(\mathbf{ U}_{h,l}^{\tau})^{-1}}\big{\|}\geq 2^{-l}\gamma_{l}^{-1}-0.1\cdot 2^{-l} \gamma_{l}^{-1}=0.9\cdot 2^{-l}\gamma_{l}^{-1},\]where the first inequality holds due to the triangle inequality. In the second inequality, the first term \(\|\mathbf{\phi}_{h}^{\tau}\|_{\widetilde{\mathbf{U}}_{h,l}^{\tau-1}}\) is bounded by criterion in Line 9 while the second term \(\big{|}\|\mathbf{\phi}_{h}^{\tau}\|_{\widetilde{\mathbf{U}}_{h,l}^{\tau-1}}-\|\mathbf{ \phi}_{h}^{\tau}\|(\mathbf{U}_{h,l}^{\tau})^{-1}\big{|}\) follows from Lemma D.1.

Arrange elements of \(\mathcal{C}_{h,l}^{k}\) in ascending order as \(\{\tau_{i}\}_{i}\). According to the above reasoning, the number of elements \(\tau\in\mathcal{C}_{h,l}^{k}\) that \(\|\mathbf{\phi}_{h}^{\tau}\|_{(\mathbf{U}_{h,l}^{\tau})^{-1}}\geq 0.9\cdot 2^{-l} \gamma_{l}^{-1}\) is at least \(|\mathcal{C}_{h,l}^{k}|-4^{l}d\). This gives

\[\sum_{i=1}^{|\mathcal{C}_{h,l}^{k}|}\min\{1,\|\mathbf{\phi}_{h}^{\tau}\|_{( \mathbf{U}_{h,l}^{\tau})^{-1}}^{2}\}\geq(0.9\cdot 2^{-l}\gamma_{l}^{-1})^{2} \cdot(|\mathcal{C}_{h,l}^{k}|-4^{l}d).\] (E.1)

On the other hand, Lemma H.1 upper bounds the LHS of (E.1) by

\[\sum_{i=1}^{|\mathcal{C}_{h,l}^{k}|}\min\{1,\|\mathbf{\phi}_{h}^{\tau}\|_{( \mathbf{U}_{h,l}^{\tau})^{-1}}^{2}\}\leq 2d\ln\big{(}1+|\mathcal{C}_{h,l}^{k}|/ (d\lambda)\big{)}.\] (E.2)

Combining (E.1) and (E.2) gives

\[0.81\cdot 4^{-l}\gamma_{l}^{-2}(|\mathcal{C}_{h,l}^{k}|-4^{l}d)\leq 2d\ln \big{(}1+|\mathcal{C}_{h,l}^{k}|/(16d)\big{)}.\] (E.3)

From algebra analysis in Lemma G.1, a necessary condition for (E.3) is \(|\mathcal{C}_{h,l}^{k}|\leq 16l\cdot 4^{l}\gamma_{l}^{2}d\). 

### Proof of Lemma D.3

We first present a claim from Vial et al. (2022) controlling the infinite norm of coefficient \(\mathbf{w}\).

**Lemma E.2** (Claim 10, Vial et al. 2022).: For any \((k,h,l)\in[K]\times[H]\times\mathbb{N}^{+}\), we have \(\|\mathbf{w}_{h,l}^{k}\|_{\infty}\leq\|\mathbf{w}_{h,l}^{k}\|_{2}\leq(2^{l}dH) ^{4}\).

Proof of Lemma D.3.: Denote \(\mathcal{X}_{\ell}\) as the set of all \(\widetilde{\mathbf{w}}_{h,\ell}^{k}\) and \(\mathcal{Y}_{\ell}\) as the set of all \(\widetilde{\mathbf{U}}_{h,\ell}^{k,-1}\). From the definition of \(\mathcal{V}_{h,l}^{k}\), we have that \(|\mathcal{V}_{h,l}^{k}|\leq\prod_{\ell=1}^{l}\big{(}|\mathcal{X}_{\ell}|\cdot| \mathcal{Y}_{\ell}|\big{)}\). From Lemma E.2, we have \(\|\mathbf{w}_{h,\ell}^{k}\|_{\infty}\leq(2^{\ell}dH)^{4}\). Note that \(\mathbf{w}_{h,\ell}^{k}\in\mathbb{R}^{d}\), we have the number of different \(\widetilde{\mathbf{w}}_{h,\ell}^{k}\) controlled by

\[|\mathcal{X}_{\ell}|\leq(1+2\cdot(2^{\ell}dH)^{4}/\kappa_{\ell})^{d}\leq(2 \cdot(2^{\ell}dH)^{4}\cdot 2^{6+4\ell}d)^{d}\leq 2^{(7+8\ell)d}d^{5d}H^{4d}.\]

In addition, we have \(\|(\mathbf{U}_{h,l}^{k})^{-1}\|_{\infty}\leq 1/\lambda=1/16\). So we can bound the number of \(\widetilde{\mathbf{U}}_{h,\ell}^{k,-1}\) by

\[|\mathcal{Y}_{\ell}|\leq(1+2\cdot 1/(16\kappa_{\ell}))^{d^{2}}\leq(2\cdot 2^{2+4 \ell}d)^{d^{2}}\leq 2^{(3+4\ell)d^{2}}d^{d^{2}}.\]

As a result, we can conclude that

\[|\mathcal{V}_{h,l}^{k}|\leq\prod_{\ell=1}^{l}\big{(}|\mathcal{X}_{\ell}|\cdot| \mathcal{Y}_{\ell}|\big{)}\leq\prod_{\ell=1}^{l}\big{(}2^{(7+8\ell)d}d^{5d}H^{ 4d}\cdot 2^{(3+4\ell)d^{2}}d^{d^{2}}\big{)}\leq(2^{22}d^{5}H^{4})^{l^{2}d^{2}}.\]

### Proof of Lemma D.4

Proof of Lemma D.4.: According to Proposition 3.2, there exists a parameter \(\mathbf{w}_{h}\) such that for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), it holds that \(\big{|}\langle\mathbf{\phi}(s,a),\mathbf{w}_{h}\rangle-[\mathbb{B}_{h}\widehat{ \mathbf{V}}_{h+1}^{k}](s,a)\big{|}\leq 2H\zeta\). Denoting \(\eta_{h}^{\tau}=\langle\mathbf{\phi}_{h}^{\tau},\mathbf{w}_{h}\rangle-[\mathbb{B}_{h }\widehat{\mathbf{V}}_{h+1}^{k}](s_{h}^{\tau},a_{h}^{\tau})\) and \(\varepsilon_{h}^{\tau}=\big{(}\widehat{\mathbf{V}}_{h+1}^{k}(s_{h+1}^{\tau})- [\mathbb{P}_{h}\widehat{\mathbf{V}}_{h+1}^{k}](s_{h}^{\tau},a_{h}^{\tau})\big{)}\), we have

\[\mathbf{U}_{h,l}^{k}(\mathbf{w}_{h,l}^{k}-\mathbf{w}_{h}) =\sum_{\tau\in\mathcal{C}_{h,l}^{k-1}}\mathbf{\phi}_{h}^{\tau}\Big{(}r _{h}^{\tau}+\widehat{\mathbf{V}}_{h+1}^{k}(s_{h+1}^{\tau})\Big{)}-\Big{(} \lambda\mathbf{I}+\sum_{\tau\in\mathcal{C}_{h,l}^{k-1}}\mathbf{\phi}_{h}^{\tau}( \mathbf{\phi}_{h}^{\tau})^{\top}\Big{)}\mathbf{w}_{h}\] \[=-\lambda\mathbf{w}_{h}+\sum_{\tau\in\mathcal{C}_{h,l}^{k-1}}\mathbf{ \phi}_{h}^{\tau}\Big{(}r_{h}^{\tau}+\widehat{\mathbf{V}}_{h+1}^{k}(s_{h+1}^{ \tau})-\langle\mathbf{\phi}_{h}^{\tau},\mathbf{w}_{h}\rangle\Big{)}\] \[=-\lambda\mathbf{w}_{h}+\sum_{\tau\in\mathcal{C}_{h,l}^{k-1}}\mathbf{ \phi}_{h}^{\tau}\Big{(}r_{h}^{\tau}+\widehat{\mathbf{V}}_{h+1}^{k}(s_{h+1}^{ \tau})-[\mathbb{B}_{h}\widehat{\mathbf{V}}_{h+1}^{k}](s_{h}^{\tau},a_{h}^{\tau} )\Big{)}+\sum_{\tau\in\mathcal{C}_{h,l}^{k-1}}\mathbf{\phi}_{h}^{\tau}\eta_{h}^{\tau}\] \[=-\lambda\mathbf{w}_{h}+\sum_{\tau\in\mathcal{C}_{h,l}^{k-1}}\mathbf{ \phi}_{h}^{\tau}\varepsilon_{h}^{\tau}+\sum_{\tau\in\mathcal{C}_{h,l}^{k-1}}\mathbf{ \phi}_{h}^{\tau}\eta_{h}^{\tau},\] (E.4)where the first equality holds due to the definition of \(\mathbf{U}^{k}_{h,l},\mathbf{w}^{k}_{h,l}\), the second equality holds by rearranging the terms, the third equality holds according the definition of \(\eta^{n}_{h}\), and the last equality holds from the relationship that \([\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s^{r}_{h},a^{r}_{h})=r^{r}_{h}+[\mathbb{P }_{h}\widehat{V}^{k}_{h+1}](s^{r}_{h},a^{r}_{h})\). Therefore, for any vector \(\mathbf{\phi}\in\mathbb{R}^{d}\), it holds that

\[\big{|}\big{\langle}\mathbf{\phi},\mathbf{w}^{k}_{h,l}-\mathbf{w}_{h} \big{\rangle}\big{|} =\big{|}\mathbf{\phi}^{\top}\big{(}\mathbf{U}^{k}_{h,l}\big{)}^{-1} \mathbf{U}^{k}_{h,l}(\mathbf{w}^{k}_{h,l}-\mathbf{w}_{h})\big{|}\] \[=\left|\mathbf{\phi}^{\top}(\mathbf{U}^{k}_{h,l})^{-1}\cdot\bigg{(}- \lambda\mathbf{w}_{h}+\sum_{\tau\in\mathcal{C}^{k-1}_{h,l}}\mathbf{\phi}^{\tau}_{h }\varepsilon^{\tau}_{h}+\sum_{\tau\in\mathcal{C}^{k-1}_{h,l}}\mathbf{\phi}^{\tau}_ {h}\eta^{\tau}_{h}\bigg{)}\right|\] \[\leq\|\mathbf{\phi}\|_{(\mathbf{U}^{k}_{h,l})^{-1}}\bigg{\|}- \lambda\mathbf{w}_{h}+\sum_{\tau\in\mathcal{C}^{k-1}_{h,l}}\mathbf{\phi}^{\tau}_{h }\varepsilon^{\tau}_{h}+\sum_{\tau\in\mathcal{C}^{k-1}_{h,l}}\mathbf{\phi}^{\tau}_ {h}\eta^{\tau}_{h}\bigg{\|}_{(\mathbf{U}^{k}_{h,l})^{-1}},\] (E.5)

where the second equality follows (E.4) and the inequality holds from Cauchy-Schwarz inequality (i.e., \(|\mathbf{x}^{\top}\mathbf{U}\mathbf{y}|\leq\|\mathbf{x}\|_{\mathbf{U}}\|\mathbf{ y}\|_{\mathbf{U}}\)). From the triangle inequality, we have

\[\bigg{\|}-\lambda\mathbf{w}_{h}+\sum_{\tau\in\mathcal{C}^{k-1}_{ h,l}}\mathbf{\phi}^{\tau}_{h}\varepsilon^{\tau}_{h}+\sum_{\tau\in\mathcal{C}^{k-1} _{h,l}}\mathbf{\phi}^{\tau}_{h}\eta^{\tau}_{h}\bigg{\|}_{(\mathbf{U}^{k}_{h,l})^{- 1}}\] \[\leq\lambda\|\mathbf{w}_{h}\|_{(\mathbf{U}^{k}_{h,l})^{-1}}+\bigg{ }\bigg{\|}\sum_{\tau\in\mathcal{C}^{k-1}_{h,l}}\mathbf{\phi}^{\tau}_{h}\varepsilon ^{\tau}_{h}\bigg{\|}_{(\mathbf{U}^{k}_{h,l})^{-1}}+\bigg{\|}\sum_{\tau\in \mathcal{C}^{k-1}_{h,l}}\mathbf{\phi}^{\tau}_{h}\eta^{\tau}_{h}\bigg{\|}_{(\mathbf{ U}^{k}_{h,l})^{-1}}.\] (E.6)

There are three terms which we will bound respectively. For the first term, we have

\[\lambda\|\mathbf{w}_{h}\|_{(\mathbf{U}^{k}_{h,l})^{-1}}\leq 2\sqrt{d \lambda}H\leq 0.1\gamma_{l},\] (E.7)

where the first inequality holds due to the fact that \(\|\mathbf{w}_{h}\|_{2}\leq 2H\sqrt{d}\) as of Proposition 3.2 and the fact that \(\mathbf{U}^{k}_{h,l}\succeq\lambda\mathbf{I}\). Under the good event \(\mathcal{G}_{1}\) and Lemma C.5, the second term can be bounded by the following:

\[\bigg{\|}\sum_{\tau\in\mathcal{C}^{k-1}_{h,l}}\mathbf{\phi}^{\tau}_{ h}\varepsilon^{\tau}_{h}\bigg{\|}_{(\mathbf{U}^{k}_{h,l})^{-1}}\leq 1.1\gamma_{l}.\] (E.8)

And the last term can be bounded by:

\[\bigg{\|}\sum_{\tau\in\mathcal{C}^{k-1}_{h,l}}\mathbf{\phi}^{\tau}_ {h}\eta^{\tau}_{h}\bigg{\|}_{(\mathbf{U}^{k}_{h,l})^{-1}}\leq 2H\zeta\sqrt{| \mathcal{C}^{k}_{h,l}|}\leq 2H\zeta\sqrt{16l\cdot 4^{l}\gamma_{l}^{2}d}=8\sqrt{l}d \hskip 0.569055ptH\cdot 2^{l}\gamma_{l}\zeta,\] (E.9)

where the first inequality is due to Lemma H.3, and the second inequality follows from Lemma D.2. Plugging (E.6), (E.7), (E.8), and (E.9) into (E.5) gives

\[\big{|}\big{\langle}\mathbf{\phi},\mathbf{w}^{k}_{h,l}-\mathbf{w}_{h} \big{\rangle}\big{|}\leq\big{(}1.2\gamma_{l}+8\sqrt{l}d\hskip 0.569055ptH\cdot 2^{l} \gamma_{l}\zeta\big{)}\|\mathbf{\phi}\|_{(\mathbf{U}^{k}_{h,l})^{-1}}.\] (E.10)

So for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), we have

\[\big{|}Q^{k}_{h,l}(s,a)-[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a )\big{|}=\big{|}\big{\langle}\mathbf{\phi}(s,a),\widetilde{\mathbf{w}}^{k}_{h,l} \big{\rangle}-[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)\big{|}\] \[\leq 0.01\cdot 2^{-4l}+\big{(}1.2+8\sqrt{l}d\hskip 0.569055ptH \cdot 2^{l}\zeta\big{)}\gamma_{l}\|\mathbf{\phi}(s,a)\|_{(\mathbf{U}^{k}_{h,l})^{-1}}+2H\zeta.\] (E.11)

where the first inequality holds from the triangle inequality, and there are three terms in the second inequality which we will bound them respectively: the first term is given by Lemma D.1, the second term follows (E.10), and the third term holds from the definition of \(\mathbf{w}_{h}\)

### Proof of Lemma D.5

Proof of Lemma d.5.: We prove by doing case analysis. In case that action \(a_{l}\in\mathcal{A}^{k}_{h,l+1}(s)\), we can assign \(a_{l+1}=a_{l}\in\mathcal{A}^{k}_{h,l+1}(s)\) so that

\[[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l})-[\mathbb{B}_{h} \widehat{V}^{k}_{h+1}](s,a_{l+1})=0.\] (E.12)

On the other hand, in the case that \(a_{l}\notin\mathcal{A}^{k}_{h,l+1}(s)\), the action \(a_{l}\) is eliminated with \(Q^{k}_{h,l}(s,a_{l})<V^{k}_{h,l}(s)-4\cdot 2^{-l}\). Note in this case, there exists \(a_{l+1}=\pi^{k}_{h,l}(s)\in\mathcal{A}^{k}_{h,l+1}(s)\) such that

\[Q^{k}_{h,l}(s,a_{l})+4\cdot 2^{-l}<V^{k}_{h,l}(s)=Q^{k}_{h,l}(s,a_{l+1}).\] (E.13)

According to Lemma C.6 and the condition that \(l\leq L_{0}\), we have that empirical state-value function \(Q^{k}_{h,l}(s,\cdot)\) is a good estimation for \([\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,\cdot)\) on actions \(a_{l},a_{l+1}\in\mathcal{A}^{k}_{l}(s)\) under event \(\mathcal{G}_{1}\):

\[\big{|}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l})-Q^{k}_{h,l} (s,a_{l})\big{|} \leq 2\cdot 2^{-l}+\chi\sqrt{L_{0}}\zeta\] (E.14) \[\big{|}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l+1})-Q^{k}_{h, l}(s,a_{l+1})\big{|} \leq 2\cdot 2^{-l}+\chi\sqrt{L_{0}}\zeta.\] (E.15)

Moreover,

\[[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l})-[\mathbb{B}_{h} \widehat{V}^{k}_{h+1}](s,a_{l+1})\] \[=\big{(}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l})-Q^{k}_{h,l }(s,a_{l})\big{)}\] \[\quad+\big{(}Q^{k}_{h,l}(s,a_{l})-Q^{k}_{h,l}(s,a_{l+1})\big{)}+ \big{(}Q^{k}_{h,l}(s,a_{l+1})-[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l+1}) \big{)}\] \[\leq 2\cdot\big{(}2\cdot 2^{-l}+\chi\sqrt{L_{0}}\zeta\big{)}-4 \cdot 2^{-l}\] \[=2\chi\sqrt{L_{0}}\zeta.\] (E.16)

where the first inequality is derived from combining (E.13), (E.14), and (E.15). So from (E.12) and (E.16), we have that \([\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l})-[\mathbb{B}_{h}\widehat{V}^{k}_ {h+1}](s,a_{l+1})\leq 2\chi\sqrt{L_{0}}\zeta\) holds in both cases. 

### Proof of Lemma d.6

Proof of Lemma d.6.: We prove by induction on \(l\). The induction basis holds at \(l=0\) by selecting \(a_{1}=\operatorname*{argmax}_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_ {h+1}](s,a)\in\mathcal{A}\) which ensures \(\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)-[\mathbb{B}_{ h}\widehat{V}^{k}_{h+1}](s,a_{1})=0\). Additionally, if the induction hypothesis holds for \(l-1\), we have that

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)-[ \mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l+1})\] \[=\big{(}\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1 }](s,a)-[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l})\big{)}+\big{(}[\mathbb{ B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l})-[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l+1}) \big{)}\] \[\leq 2(l-1)\chi\sqrt{L_{0}}\zeta+2\chi\sqrt{L_{0}}\zeta\] \[=2l\cdot\chi\sqrt{L_{0}}\zeta,\]

where the first inequality term is derived from combining induction hypothesis with Lemma D.5. We can then reach desired statement holds for all \(l\) in the range by induction. 

### Proof of Lemma d.7

Proof of Lemma d.7.: According to Lemma D.6, there exists some action \(a_{l}\in\mathcal{A}^{k}_{h,l}(s)\) that

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)-[ \mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l})\leq 2(l-1)\chi\sqrt{L_{0}}\zeta.\] (E.17)

Moreover, we have

\[[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l})-V^{k}_{h,l}(s)\leq[ \mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a_{l})-Q^{k}_{h,l}(s,a_{l})\leq 2\cdot 2^{-l} +\chi\sqrt{L_{0}}\zeta,\] (E.18)

where the first inequality comes from the definition \(V^{k}_{h,l}(s)=\max_{a\in\mathcal{A}^{k}_{h,l}}Q^{k}_{h,l}(s,a)\) and the second inequality holds according to Lemma C.6 with \(l\leq L_{0}\). Adding up (E.17) and (E.18) leads to

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)-V^{k}_{h,l} \leq 2\cdot 2^{-l}+(2l-1)\chi\sqrt{L_{0}}\zeta.\]

This completes the proof.

### Proof of Lemma d.8

Proof of Lemma d.8.: The statement holds by simply checking:

\[V^{k}_{h,l}(s)-\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k }_{h+1}](s,a) \leq V^{k}_{h,l}(s)-[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,\pi^{k }_{h,l}(s))\] \[=Q^{k}_{h,l}(s,\pi^{k}_{h,l}(s))-[\mathbb{B}_{h}\widehat{V}^{k}_{h +1}](s,\pi^{k}_{h,l}(s))\] \[\leq 2\cdot 2^{-l}+\chi\sqrt{L_{0}}\zeta,\]

where the first inequality holds from \(\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)\geq[\mathbb{B }_{h}\widehat{V}^{k}_{h+1}](s,\pi^{k}_{h,l}(s))\), the equality is from the definition \(V^{k}_{h,l}(s)=Q^{k}_{h,l}(s,\pi^{k}_{h,l}(s))\), and the last inequality holds according to Lemma C.6 with the condition \(l\leq L_{0}\). 

### Proof of Lemma d.9

Proof of Lemma d.9.: The statement holds by checking

\[\min\left\{V^{k}_{h,l}(s)+3\cdot 2^{-l},\widehat{V}^{k}_{h,l-1}(s )\right\}-\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)\] \[=\min_{\ell=1}^{l}\{V^{k}_{h,\ell}(s)+3\cdot 2^{-\ell}\}-\max_{a \in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a)\] \[\geq\min_{\ell=1}^{l}\{3\cdot 2^{-\ell}-(2\cdot 2^{-l}+(2\ell-1) \chi\sqrt{L_{0}}\zeta)\}\] \[=2^{-l}-(2l-1)\chi\sqrt{L_{0}}\zeta,\]

where the first equality holds due to \(\widehat{V}^{k}_{h,l}(s)=\min_{\ell=1}^{l}\{V^{k}_{h,\ell}(s)+3\cdot 2^{-\ell}\}\), the inequality holds according to Lemma D.7, and the last equality holds since \(2^{-l}\) decreases as \(l\) increases. 

### Proof of Lemma d.10

Proof of Lemma d.10.: The statement holds by checking

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a) -\max\left\{V^{k}_{h,l}(s)-3\cdot 2^{-l},\widetilde{V}^{k}_{h,l-1}(s)\right\}\] \[=\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a) -\max_{\ell=1}^{l}\{V^{k}_{h,\ell}(s)-3\cdot 2^{-\ell}\}\] \[=\min_{\ell=1}^{l}\left\{\,\max_{a\in\mathcal{A}}[\mathbb{B}_{h} \widehat{V}^{k}_{h+1}](s,a)-V^{k}_{h,\ell}(s)+3\cdot 2^{-\ell}\right\}\] \[\geq\min_{\ell=1}^{l}\{-(2\cdot 2^{-l}+\chi\sqrt{L_{0}}\zeta)+3 \cdot 2^{-\ell}\}\] \[=2^{-l}-\chi\sqrt{L_{0}}\zeta,\]

where the first equality holds due to the design of Algorithm 2 such that \(\widetilde{V}^{k}_{h,l}(s)=\max_{\ell=1}^{l}\{V^{k}_{h,\ell}(s)-3\cdot 2^{- \ell}\}\), the inequality holds according to Lemma D.8, and the last equality holds since \(2^{-l}\) decreases as \(l\) increases. 

### Proof of Lemma d.11

We prove Lemma D.11 in this subsection. The first lemma which we introduce establishes an upper bound on the underestimation of the state value function \(\widehat{V}^{k}_{h}\) for every action and every state through a categorised discussion based on whether Algorithm 2 reaches phase \(L_{\varepsilon}\) for state \(s\). Specifically, if the process does not reach phase \(L_{\varepsilon}\), we can substantiate the statement by applying Lemma D.9 to phase \(l^{k}_{h}(s)-1\). Conversely, if the process reaches phase \(L_{\varepsilon}\), the statement can be proven by applying Lemma D.7 to phase \(L_{\varepsilon}\).

**Lemma E.3**.: Under event \(\mathcal{G}_{1}\) and for all \(\varepsilon>0\) that \(\mathcal{G}_{\varepsilon}\) is satisfied, for any \((k,h,s)\in[K]\times[H]\times\mathcal{S}\),

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}^{k}_{h+1}](s,a) -\widehat{V}^{k}_{h}(s)\leq 0.07\varepsilon/H.\]Now we are ready to prove Lemma D.11 by induction.

Proof of Lemma d.11.: We prove by induction on stage \(h\in[H]\). It is sufficient to show for any \(h\in[H],s\in\mathcal{S}\),

\[V_{h}^{*}(s)-\widehat{V}_{h}^{k}(s)\leq 0.07\varepsilon\cdot(H+1-h)/H.\] (E.19)

We use induction on \(h\) from \(H+1\) to \(1\) to prove the statement. The induction basis holds from the definition that \(V_{H+1}^{*}(s)=\widehat{V}_{H+1}^{k}(s)=0\). Assume the induction hypothesis (E.19) holds for \(h+1\), we have

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}V_{h+1}^{*}](s,a)-\max_{a\in \mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a) \leq\max_{a\in\mathcal{A}}[\mathbb{B}_{h}(V_{h+1}^{*}-\widehat{V} _{h+1}^{k})](s,a)\] \[\leq\max_{s^{\prime}\in\mathcal{S}}\big{(}V_{h+1}^{*}(s^{\prime} )-\widehat{V}_{h+1}^{k}(s^{\prime})\big{)}\] \[\leq 0.07\varepsilon\cdot(H-h)/H.\] (E.20)

So for level \(h\), it holds that

\[V_{h}^{*}(s)-\widehat{V}_{h}^{k}(s)\] \[=\big{(}\max_{a\in\mathcal{A}}[\mathbb{B}_{h}V_{h+1}^{*}](s,a)- \max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a)\big{)}+\big{(} \max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a)-\widehat{V}_ {h}^{k}(s)\big{)}\] \[\leq 0.07\varepsilon\cdot(H-h)/H+0.07\varepsilon/H\leq 0.07 \varepsilon\cdot(H+1-h)/H,\]

where the first inequality holds by combining (E.20) with Lemma E.3. This proves the induction statement (E.19) for \(h\), which leads to the desired statement. 

### Proof of Lemma d.12

We prove Lemma D.12 in this subsection, the first lemma we use establishes an upper bound on the overestimation of the state value function \(\widehat{V}_{h}^{k}\) for the executed policy \(\pi_{h}^{k}(s)\) across all states.

**Lemma E.4**.: Under event \(\mathcal{G}_{1}\) and for all \(\varepsilon>0\) that \(\mathcal{G}_{\varepsilon}\) is satisfied, for any \((k,h,s)\in[K]\times[H]\times\mathcal{S}\),

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a)-[\mathbb{B}_{ h}\widehat{V}_{h+1}^{k}](s,\pi_{h}^{k}(s))\leq 16\cdot 2^{-l_{h}^{k}(s)}+0.10 \varepsilon/H.\]

Then the following lemma establishes an upper bound on the decision error induced by the arm-elimination process with respect to the state-action value function given by the ground-truth transform.

**Lemma E.5**.: Under event \(\mathcal{G}_{1}\) and for all \(\varepsilon>0\) that \(\mathcal{G}_{\varepsilon}\) is satisfied, for any \((k,h,s)\in[K]\times[H]\times\mathcal{S}\),

\[\widehat{V}_{h}^{k}(s)-\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{ k}](s,a)\leq 10\cdot 2^{-l_{h}^{k}(s)}+0.06\varepsilon/H.\]

Proof of Lemma d.12.: We can directly reach the desired result by taking summation on Lemma E.4 and Lemma E.5:

\[\widehat{V}_{h}^{k}(s)-[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s, \pi_{h}^{k}(s))\] \[\leq\big{(}\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1 }^{k}](s,a)-[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,\pi_{h}^{k}(s))\big{)}+ \big{(}\widehat{V}_{h}^{k}(s)-\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V }_{h+1}^{k}](s,a)\big{)}\] \[\leq\big{(}16\cdot 2^{-l_{h}^{k}(s)}+0.10\varepsilon/H\big{)}+ \big{(}10\cdot 2^{-l_{h}^{k}(s)}+0.06\varepsilon/H\big{)}\] \[=26\cdot 2^{-l_{h}^{k}(s)}+0.16\varepsilon/H.\]

### Proof of Lemma d.14

We can prove the statement by applying a union bound to the concentration event, as given by the Azuma-Hoeffding inequality.

Proof of Lemma d.14.: Consider some fixed \(h\in[H]\) and \(\varepsilon=2^{-l}>0\). List the episodes index \(k\) such that \(V_{h}^{*}(s_{h}^{k})-V_{h}^{\pi^{k}}(s_{h}^{k})>\varepsilon\) holds in ascending order as \(\{\tau_{i}\}_{i}\). Recall that

\[\eta_{h}^{\tau_{i}}=[\mathbb{P}_{h}(\widehat{V}_{h+1}^{\tau_{i}}-V_{h+1}^{\tau_ {i}})](s_{h}^{\tau_{i}},\pi_{h}^{\tau_{i}}(s_{h}^{\tau_{i}}))-\big{(}\widehat{V }_{h+1}^{\tau_{i}}(s_{h+1}^{\tau_{i}})-V_{h+1}^{\pi_{i}^{\tau_{i}}}(s_{h+1}^{ \tau_{i}})\big{)}.\]

Since the environment sample \(s_{h^{\prime}+1}^{\tau_{i}}\) according to \(\mathbb{P}_{h^{\prime}}(\cdot|s_{h^{\prime}}^{\tau_{i}},a_{h^{\prime}}^{\tau_{i }})\), we have \(\eta_{h^{\prime}}^{\tau_{i}}\) is \(\mathcal{F}_{h^{\prime}+1}^{\tau_{i}}\)-measurable with \(\mathbb{E}\left[\eta_{h^{\prime}}^{\tau_{i}}|\mathcal{F}_{h^{\prime}}^{\tau_{i }}\right]=0\). Since both \(0\leq\widehat{V}_{h^{\prime}+1}^{\tau_{i}}(s_{h^{\prime}+1}^{\tau_{i}})\leq H\) and \(0\leq V_{h^{\prime}+1}^{\pi_{i}^{\tau_{i}}}(s_{h^{\prime}+1}^{\tau_{i}})\leq H\) hold, we have \(|\eta_{h^{\prime}}^{\tau_{i}}|\leq 2H\). According to Lemma H.4 over filtration

\[\mathcal{F}_{h}^{\tau_{1}}\subseteq\mathcal{F}_{h+1}^{\tau_{1}}\subseteq \cdots\subseteq\mathcal{F}_{H}^{\tau_{1}}\subseteq\mathcal{F}_{h}^{\tau_{2}} \subseteq\mathcal{F}_{h+1}^{\tau_{2}}\subseteq\cdots\subseteq\mathcal{F}_{H}^ {\tau_{2}}\subseteq\cdots\subseteq\mathcal{F}_{h^{\prime}}^{\tau_{i}}\subseteq\cdots\]

for some fixed \(S=|\mathcal{K}_{h}^{\varepsilon}|\), the good event that

\[\sum_{i=1}^{|\mathcal{K}_{h}^{\varepsilon}|}\sum_{h^{\prime}=h}^{H}\eta_{h^{ \prime}}^{\tau_{i}}\leq 2H\sqrt{2HS\log(4HS^{2}l^{2}/\delta)}=4\sqrt{H^{3}| \mathcal{K}_{h}^{\varepsilon}|\log(4H|\mathcal{K}_{h}^{\varepsilon}|\log( \varepsilon^{-1})/\delta)}\]

happens with probability at least \(1-\delta/(4HS^{2}l^{2})\). By the union bound statement over all \((h,S,l)\in[H]\times[K]\times\mathbb{N}^{+}\), we have the bad event happens with probability at most

\[\Pr[\mathcal{G}_{2}^{\mathbf{6}}]\leq\sum_{h=1}^{H}\sum_{S=1}^{K}\sum_{l=1}^{ \infty}\Pr[\mathcal{B}_{2}(h,2^{-l})]\leq\sum_{h=1}^{H}\sum_{S=1}^{K}\sum_{l= 1}^{\infty}\frac{\delta}{4HS^{2}l^{2}}\leq\delta,\]

where the last inequality holds from \(\sum_{n\geq 1}n^{-2}=\pi^{2}/6\), which reach the desired statement. 

### Proof of Lemma d.15

We first provide the following instantaneous regret upper bound by combining Lemma d.11 and Lemma d.12.

**Lemma E.6**.: Under event \(\mathcal{G}_{1}\) and for all \(\varepsilon>0\) that \(\mathcal{G}_{\varepsilon}\) is satisfied, for any \((k,h)\in[K]\times[H]\),

\[V_{h}^{*}(s_{h}^{k})-V_{h}^{\pi^{k}}(s_{h}^{k})\leq 0.23\varepsilon+26\sum_{h^{ \prime}=h}^{H}2^{-l_{h^{\prime}}^{k}(s_{h^{\prime}}^{k})}+\sum_{h^{\prime}=h} ^{H}\eta_{h^{\prime}}^{k},\]

where \(\eta_{h}^{k}=[\mathbb{P}_{h}(\widehat{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}})](s_{h}^ {k},\pi_{h}^{k}(s_{h}^{k}))-\big{(}\widehat{V}_{h+1}^{k}(s_{h+1}^{k})-V_{h+1}^ {\pi^{k}}(s_{h+1}^{k})\big{)}\) is a \(\mathcal{F}_{h+1}^{k}\)-measurable random variable that \(\mathbb{E}[\eta_{h}^{k}|\mathcal{F}_{h}^{k}]=0\) and \(|\eta_{h}^{k}|\leq H\).

Together with Lemma C.9 and the definition of \(\mathcal{G}_{2}\), we can provide an upper bound for arbitrary subsets.

Proof of Lemma d.15.: Taking summation on result given by Lemma E.6 to all \(k\in\mathcal{K}\) gives

\[\sum_{k\in\mathcal{K}}\big{(}V_{h}^{*}(s_{h}^{k})-V_{h}^{\pi^{k}}(s_{h}^{k}) \big{)}\leq 0.23|\mathcal{K}|\varepsilon+26\sum_{k\in\mathcal{K}}\sum_{h^{ \prime}=h}^{H}2^{-l_{h}^{k}(s_{h^{\prime}}^{k})}+\sum_{k\in\mathcal{K}}\sum_{h ^{\prime}=h}^{H}\eta_{h^{\prime}}^{k}.\] (E.21)

We can bound the second term according to Lemma C.9,

\[26\sum_{k\in\mathcal{K}}\sum_{h^{\prime}=h}^{H}2^{-l_{h^{\prime}}^{k}(s_{h^{ \prime}}^{k})}\leq 0.26|\mathcal{K}|\varepsilon+2^{17}L_{\varepsilon}dH^{2} \gamma_{L_{\varepsilon}}^{2}\varepsilon^{-1}.\] (E.22)

Under event \(\mathcal{G}_{2}\), the third term satisfies that

\[\sum_{k\in\mathcal{K}}\sum_{h^{\prime}=h}^{H}\eta_{h^{\prime}}^{k}\leq 4\sqrt{H^{3}| \mathcal{K}|\log(4H|\mathcal{K}|\log(\varepsilon^{-1})/\delta)}.\] (E.23)

Plugging (E.22) and (E.23) into (E.21) gives

\[\sum_{k\in\mathcal{K}}\big{(}V_{h}^{*}(s_{h}^{k})-V_{h}^{\pi^{k}}(s_{h}^{k}) \big{)}\leq 0.49|\mathcal{K}|\varepsilon+2^{17}L_{\varepsilon}dH^{2}\gamma_{L_{ \varepsilon}}^{2}\varepsilon^{-1}+4\sqrt{H^{3}|\mathcal{K}|\log(4H|\mathcal{K}| \log(\varepsilon^{-1})/\delta)}.\] (E.24)Proof of Lemmas in Appendix E

### Proof of Lemma e.3

Proof of Lemma e.3.: We start the proof by discussing different cases. First, if \(l_{h}^{k}(s)\leq L_{\varepsilon}\), we have \(l_{h}^{k}(s)-1\leq\min\{L_{\varepsilon},l_{h}^{k}(s)-1\}\), according to the definition of \(\widehat{V}_{h,l}^{k}(s)\),

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a)- \widehat{V}_{h}^{k}(s) =\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a) -\widehat{V}_{h,l_{h}^{k}(s)-1}^{k}(s)\] \[\leq-2^{-(l_{h}^{k}(s)-1)}+2(l_{h}^{k}(s)-1)\chi\sqrt{L_{ \varepsilon}}\zeta\] \[\leq 0+2\chi L_{\varepsilon}^{1.5}\zeta\] \[\leq 0.02\varepsilon/H,\] (F.1)

where the first inequality holds from Lemma D.9, and the last inequality holds due to \(\chi L_{\varepsilon}^{1.5}\zeta\leq 2^{-L_{\varepsilon}}\leq 0.01\varepsilon/H\) given by \(\mathcal{G}_{\varepsilon}\).

On the other hand, when \(l_{h}^{k}(s)>L_{\varepsilon}\), we have \(L_{\varepsilon}\leq\min\{L_{\varepsilon},l_{h}^{k}(s)-1\}\) and thus

\[\widehat{V}_{h}^{k}(s)\geq\widehat{V}_{h,L_{\varepsilon}}^{k}(s) \geq V_{h,L_{\varepsilon}}^{k}(s)-3\cdot 2^{-L_{\varepsilon}}\] (F.2)

where the first inequality is due to Lemma C.2 and the second inequality holds due to the definition of \(\widehat{V}_{h,L_{\varepsilon}}^{k}(s)\). Therefore, \(L_{\varepsilon}\leq\min\{L_{\varepsilon},l_{h}^{k}(s)-1\}\) yields

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a) -\widehat{V}_{h}^{k}(s) \leq\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a)-V_{h,L_{\varepsilon}}^{k}(s)+3\cdot 2^{-L_{\varepsilon}}\] \[\leq 5\cdot 2^{-L_{\varepsilon}}+(2L_{\varepsilon}-1)\chi\sqrt{L_{ \varepsilon}}\zeta\] \[\leq 0.05\varepsilon/H+0.02\varepsilon/H=0.07\varepsilon/H,\] (F.3)

where the first inequality is given by (F.2), the second inequality is given by Lemma D.7, and the last inequality holds from \(\chi L_{\varepsilon}^{1.5}\zeta\leq 2^{-L_{\varepsilon}}\leq 0.01\varepsilon/H\) given by \(\mathcal{G}_{\varepsilon}\). So considering both (F.1) and (F.3), we have the first statement

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a) -\widehat{V}_{h}^{k}(s)\leq 0.07\varepsilon/H\]

always holds under event \(\mathcal{G}_{1}\). 

### Proof of Lemma e.4

We prove Lemma E.4 by applying Lemma C.7 on phase \(\min\{L_{\varepsilon},l_{h}^{k}(s)-1\}\), in this subsection.

Proof of Lemma e.4.: Note we have \(\pi_{h,l_{h}^{k}(s)-1}^{k}(s)\in\mathcal{A}_{h,l_{h}^{k}(s)}^{k}(s)\) according to the definition of \(\mathcal{A}_{h,l+1}^{k}(s)\). This implies \(\pi_{h}^{k}(s)\in\mathcal{A}_{h,l_{h}^{k}(s)}^{k}(s)\) during the elimination process.

If \(l_{h}^{k}(s)\leq L_{\varepsilon}\), we have \(l_{h}^{k}(s)-1\leq\min\{L_{\varepsilon},l_{h}^{k}(s)-1\}\). Thus,

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a) -[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,\pi_{h}^{k}(s)) \leq 8\cdot 2^{-(l_{h}^{k}(s)-1)}+2l_{h}^{k}(s)\cdot\chi\sqrt{L_{ \varepsilon}}\zeta\] \[\leq 16\cdot 2^{-l_{h}^{k}(s)}+2\chi L_{\varepsilon}^{1.5}\zeta\] \[\leq 16\cdot 2^{-l_{h}^{k}(s)}+0.02\varepsilon/H,\] (F.4)

where the first inequality follows from Lemma C.7 with \(\pi_{h}^{k}(s)\in\mathcal{A}_{h,l_{h}^{k}(s)}^{k}(s)\) and the last inequality holds due to \(\chi L_{\varepsilon}^{1.5}\zeta\leq 0.01\varepsilon/H\) given by \(\mathcal{G}_{\varepsilon}\).

Otherwise, we have \(L_{\varepsilon}\leq\min\{L_{\varepsilon},l_{h}^{k}(s)-1\}\). In this case, we have

\[\max_{a\in\mathcal{A}}[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,a) -[\mathbb{B}_{h}\widehat{V}_{h+1}^{k}](s,\pi_{h}^{k}(s)) \leq 8\cdot 2^{-L_{\varepsilon}}+2\chi L_{\varepsilon}^{1.5}\zeta\] \[\leq 0.08\varepsilon/H+0.02\varepsilon/H=0.10\varepsilon/H,\] (F.5)

[MISSING_PAGE_FAIL:34]

[MISSING_PAGE_FAIL:35]

[MISSING_PAGE_EMPTY:36]

**Lemma H.3** (Lemma 8, Zanette et al. (2020b)).: Let \(\{\phi^{k},\eta^{k}\}_{k=1}^{\infty}\) be any bounded sequence satisfying \(\phi^{k}\in\mathbb{R}^{d}\) and \(|\eta^{k}|\leq\zeta\) for some constant \(\zeta>0\). For \(k\geq 1\), let \(\mathbf{U}^{k}=\lambda\mathbf{I}+\sum_{\tau=1}^{k-1}\mathbf{\phi}^{\tau}(\mathbf{\phi}^ {\tau})^{\top}\). Then, for all \(k\geq 1\), we have that

\[\left\|\sum_{\tau=1}^{k-1}\eta^{\tau}\mathbf{\phi}^{\tau}\right\|_{(\mathbf{U}^{k} )^{-1}}\leq\zeta\sqrt{k}.\]

**Lemma H.4** (Azuma-Hoeffding inequality, Hoeffding (1963)).: Let \(\{\eta^{k}\}_{k=1}^{K}\) be a martingale difference sequence with respect to a filtration \(\{\mathcal{F}^{k}\}_{k=1}^{K}\) satisfying \(|\eta^{k}|\leq M\) for some constant \(M>0\) and \(\eta^{k}\) is \(\mathcal{F}^{k+1}\)-measurable with \(|\mathbb{E}[\eta^{k}|\mathcal{F}^{k}]|=0\). Then for some fixed \(k\in[K]\) and any \(\delta\in(0,1)\), with probability at least \(1-\delta\), we have

\[\sum_{\tau=1}^{k}\eta^{\tau}\leq M\sqrt{2k\ln\delta^{-1}}.\]

**Lemma H.5** (Freedman inequality, Cesa-Bianchi and Lugosi (2006)).: Let \(\{\eta^{k}\}_{k=1}^{K}\) be a martingale difference sequence with respect to a filtration \(\{\mathcal{F}^{k}\}_{k=1}^{K}\) satisfying \(|\eta^{k}|\leq M\) for some constant \(M>0\) and \(\eta^{k}\) is \(\mathcal{F}^{k+1}\)-measurable with \(|\mathbb{E}[\eta^{k}|\mathcal{F}^{k}]|=0\). Then for some fixed \(k\in[K]\), \(a>0\) and \(v>0\), we have

\[\Pr\Big{(}\sum_{\tau=1}^{k}\eta^{\tau}\geq a,\sum_{\tau=1}^{k}\operatorname{ Var}[\eta^{\tau}|\mathcal{F}^{\tau}]\leq v\Big{)}\leq\exp\Big{(}\frac{-a^{2}}{2v +2aM/3}\Big{)}.\]

## Appendix I Numerical Simulation

We added experiments on synthetic datasets to verify the performance of the algorithm and the contribution of each component. Specifically, we consider a linear MDP with \(S=4\), \(A=5\), \(H=2\), and \(d=8\). Each element in the feature vector \(\mathbf{\phi}(s,a)\) and \(\mathbf{\mu}(s^{\prime})\) is generated by a uniform distribution \(U(0,1)\). Subsequently, \(\mathbf{\phi}\) is normalized to ensure that \(\mathbb{P}(s^{\prime}|s,a)\) is a probability measure, i.e., \(\mathbf{\phi}(s,a)=\mathbf{\phi}(s,a)/\sum_{s^{\prime}}\mathbf{\phi}^{\top}(s,a)\mathbf{\mu}(s ^{\prime})\). The reward is defined by \(r(s,a)=\mathbf{\phi}^{\top}(s,a)\mathbf{\theta}\), where \(\mathbf{\theta}\sim N(0,I_{d})\). The model misspecification is also added to the transition \(\mathbb{P}\) and reward function \(r\). For a given misspecification \(\zeta\), the ground truth reward function is defined by \(r(s,a)=\phi^{\top}(s,a)\theta+Z(s,a)\), where \(Z(s,a)\sim U(-\zeta,\zeta)\). When adding the model misspecification to the transition kernel, we first random sample a subset \(\mathcal{S}_{+}\subset\mathcal{S}\) such that \(|\mathcal{S}_{+}|=|S|/2\). Then the misspecified transition kernel is then generated by

\[\mathbb{P}^{\prime}(s^{\prime}|s,a)=\mathbb{P}(s^{\prime}|s,a)+2\frac{\zeta}{S }\operatorname{\mathds{1}}[s^{\prime}\in\mathcal{S}_{+}]-\frac{\zeta}{S},\]

we can verify that \(\|\mathbb{P}(\cdot|s,a)-\mathbb{P}^{\prime}(\cdot|s,a)\|_{\mathrm{TV}}=\zeta\). We investigated the misspecification level from \(\zeta=0,0.01,\cdots,0.3\) in \(16\) randomly generated environments over \(2000\) episodes. We report the cumulative regret and runtime with respect to different misspecification levels. Additionally, we performed an ablation study by 1) removing the certified estimation (Algorithm 2, Line 11) and 2) removing the quantization (Algorithm 1, Line 8).

The results of these configurations are presented in the following table. The detailed regret for all misspecification level is presented in Table 3 and Figure 1, we plot the cumulative regret for 2000 episodes with respect to the misspecification level \(\zeta\). The cumulative regret curve is plotted in Figure 2.

The experimental results suggest several key findings that support our theoretical analysis:

* When the misspecification level is low, it is possible to achieve constant regret, where the instantaneous regret in the final rounds is approximately zero.
* The certified estimator and the quantization do not significantly affect the algorithm's runtime. In contrast, the certified estimator provides an 'early-stopping' condition in Algorithm 2, which slightly reduces the algorithm's runtime. In particular, our algorithm yields a computational complexity of \(O(d^{2}AHK^{2}\log K)\), which is the same as Vial et al. (2022) and only \(\log K\) greater than the vanilla LSVI-UCB (Jin et al., 2020) due to the multi-phased algorithm.

[MISSING_PAGE_FAIL:38]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We study the constant regret analysis in reinforcement learning from a theoretical perceptive. The contribution, assumptions and scope are clearly claimed in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed the limitations and future potential directions in Section 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.

Figure 2: Cumulative regret with respect to the number of episodes. We reported the median cumulative regret with the shadow area as the region from 25% percentage to 75% percentage statistics over 16 runs.

* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]. Justification: We provide a detailed proof for all theorems in the appendix, starting from Appendix C. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: Our submission paper does not include experiments Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: Our paper does not include experiments requiring code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: Our paper does not include experiments. Guidelines: ** The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics.

Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work provides the theoretical understanding of reinforcement learning. Although there might be some potential social impacts on reinforcement learning applications, according to the guidelines, we believe our result does not have a direct connection with these issues. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We place this paper on the theoretical understand of reinforcement learning thus the paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.

* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We do not use existing assets in the paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.

* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.