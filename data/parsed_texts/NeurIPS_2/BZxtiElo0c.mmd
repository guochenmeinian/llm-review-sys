# GeSS: Benchmarking Geometric Deep Learning under Scientific Applications with Distribution Shifts

Deyu Zou1, Shikun Liu2, Siqi Miao2, Victor Fung2, Shiyu Chang3, Pan Li2

1University of Science and Technology of China, zoudeyu2020@mail.ustc.edu.cn,

2Georgia Institute of Technology, {shikun.liu,siqi.miao,victorfung,panli}@gatech.edu

3University of California, Santa Barbara, chang87@ucsb.edu

Equal contribution. Code and data are available at [https://github.com/Graph-COM/GESS](https://github.com/Graph-COM/GESS).

###### Abstract

Geometric deep learning (GDL) has gained significant attention in scientific fields, for its proficiency in modeling data with intricate geometric structures. Yet, very few works have delved into its capability of tackling the distribution shift problem, a prevalent challenge in many applications. To bridge this gap, we propose GeSS, a comprehensive benchmark designed for evaluating the performance of GDL models in scientific scenarios with distribution shifts. Our evaluation datasets cover diverse scientific domains from particle physics, materials science to biochemistry, and encapsulate a broad spectrum of distribution shifts including conditional, covariate, and concept shifts. Furthermore, we study three levels of information access from the out-of-distribution (OOD) test data, including no OOD information, only unlabeled OOD data, and OOD data with a few labels. Overall, our benchmark results in 30 different experiment settings, and evaluates 3 GDL backbones and 11 learning algorithms in each setting. A thorough analysis of the evaluation results is provided, poised to illuminate insights for GDL researchers and domain practitioners who are to use GDL in their applications.

## 1 Introduction

Machine learning (ML) techniques, as a powerful and efficient approach, have been widely used in diverse scientific fields, including high energy physics (HEP) [15], materials science [20], and drug discovery [81], propelling ML4S (ML for Science) into a promising direction. In particular, geometric deep learning (GDL) is gaining much focus in scientific applications because many scientific data can be represented as point cloud data embedded in a complex geometric space. Current GDL research mainly focuses on neural network architectures design [79, 19, 36, 73, 78, 47], capturing geometric properties (_e.g.,_ invariance and equivariance properties), to learn useful representations for geometric data, and these backbones have shown to be successful in various GDL scenarios.

However, ML models in scientific applications consistently face challenges related to data distribution shifts (\(\mathbb{P}_{\mathcal{S}}(X,Y)\neq\mathbb{P}_{\mathcal{T}}(X,Y)\)) between the training (source) domain \(\mathcal{S}\) and the test (target) domain \(\mathcal{T}\). In particular, the regime expected to have new scientific discoveries has often been less explored and thus holds limited data with labels. To apply GDL techniques to such a regime, researchers often resort to training models over labeled data from well-explored regimes or theory-guided simulations, whose distribution may not align well with the real-world to-be-explored regime of scientific interest. In materials science, for example, the OC20 dataset [10] covers a broad space of catalyst surfaces and adsorbates. ML models trained over this dataset may be expected to extrapolate to new catalyst compositions such as oxide electrocatalysts [80]. Additionally, in HEP, models are often trained based on simulated data and are expected to generalize to real experiments, which hold more variable conditions and may differ substantially from simulations [50].

Despite the significance, scant research has systematically explored the distribution shift challenges specific to GDL. Findings from earlier studies on CV and NLP tasks [9; 13; 99] might not be directly applicable to GDL models, due to the substantially distinct model architectures.

In the context of ML4S, several studies address model generalization issues, but there are two prominent _disparities_ in these works. First, previous studies are often confined to specific scientific scenarios that have different types of distribution shifts. For example, [96] concentrated exclusively on drug-related shifts such as scaffold shift, while [33] investigated model generalization to deal with the label-fidelity shifts in the application of materials property prediction. Due to the disparity in shift types, the findings effective for one application might be ineffectual for another.

Moreover, existing studies often assume different levels of the availability of target-domain data information. Specifically, while some studies assume some availability of the data from the target domain [33], they differ on whether such data is labeled or not. On the other hand, certain investigations presume total unavailability of the target-domain data [55]. These varying conditions often dictate the selection of corresponding methodologies.

To address the above disparities, this paper presents **GeSS**, a benchmark to evaluate GDL models' capability of dealing with various types of distribution shifts across scientific applications. Specifically, the datasets cover three scientific fields: HEP, biochemistry, and materials science, and are collected from either real experimental scenarios exhibiting distribution shifts, or simulated scenarios designed to mimic real-world distribution shifts. Moreover, we leverage the specific generation process of geometric data, i.e., _the inherent causality_ of these applications to categorize their distribution shifts into different categories: conditional shift (\(\mathbb{P}_{\mathcal{S}}(X|Y)\neq\mathbb{P}_{\mathcal{T}}(X|Y)\) and \(\mathbb{P}_{\mathcal{S}}(Y)=\mathbb{P}_{\mathcal{T}}(Y)\)), covariate shift (\(\mathbb{P}_{\mathcal{S}}(Y|X)=\mathbb{P}_{\mathcal{T}}(Y|X)\) and \(\mathbb{P}_{\mathcal{S}}(X)\neq\mathbb{P}_{\mathcal{T}}(X)\)), and concept shift (\(\mathbb{P}_{\mathcal{S}}(Y|X)\neq\mathbb{P}_{\mathcal{T}}(Y|X)\)). Furthermore, to address the disparity of assumed available out-of-distribution (OOD) information across previous works, we study three levels: no OOD information (No-Info), only OOD features without labels (O-Feature), and OOD features with a few labels (Par-Label). We evaluate representative methodologies across these three levels, specifically, OOD generalization methods for the No-Info level, domain adaptation (DA) methods for the O-Feature level, and transfer learning (TL) methods for the Par-Label level.

Our experiments are conducted over 6 datasets, in 30 different settings with 10 different distribution shifts times 3 levels of OOD info, covering 3 GDL backbones and 11 learning algorithms in each setting. According to our experiments, we observe that no approach can be the best for all types of shifts, and the levels of OOD info may benefit GDL models to various extents across different applications. In the meantime, our comprehensive evaluation also yields three valuable takeaways to guide the selection of practical solutions depending on the availability of OOD data:

* For Par-Label level, TL strategies show advantages under concept shifts, particularly when there are significant changes in the marginal label distribution.
* For O-Feature level, DA strategies excel when the distribution shifts happen to the geometric characteristics of features that are critical for label determination compared with other features.
* For No-Info level, OOD generalization methods will have some improvements if the training dataset can be properly partitioned into valid groups that reflect the shifts.

In addition to offering domain practitioners guidance on handling distribution shift issues, our new proposed HEP datasets and 10 curated distribution shift scenarios can also facilitate the development and evaluation of new algorithms within the GDL community for various scientific applications.

## 2 Comparison with Existing Benchmarks on Distribution Shifts

Prior research has constructed benchmarks tailored to diverse research fields, shifts, and knowledge levels, and some representative works are summarized in Table 1. In this section, we discuss how GeSS is compared to existing distribution-shift benchmarks in the following three perspectives.

**Application Domain.** Recent works have introduced benchmarks on distribution shifts across various application domains, including tabular data [23; 49], CV [34; 30; 109], OCR [43], GraphML [27; 14; 5], NLP [95; 104], and LLMs [76]. Regarding ML4S, OOD issues have been discussed across various prediction tasks, such as retrosynthesis predictions [102] and property predictions on drugs [35; 86; 108], proteins [41], and materials [56], most of which are built upon GraphML settings. However, no benchmark studies have been conducted in numerous scientific applications, let alone with a focus on GDL models and a broad range of methodologies to deal with distribution shifts like this benchmark.

**Distribution Shift.** Previous works explored various distribution shifts. [77; 39; 28; 14] benchmark domain generalization methods. [39; 97; 70] study subpopulation shift. [100] categorizes and quantifies diversity and correlation shifts. [89] jointly analyzes spurious correlation, low-data drift and unseen data shift. [107; 53] benchmark spurious correlations in more diverse and realistic settings. [49] benchmarks a \(Y|X\)-shift (_aka._, concept shift in our work) which is shown to be prevalent in tabular data. [27] specifies covariate and concept shifts on the GraphML setting. However, many scientific application scenarios involve distribution shifts with mechanisms that differ from those mentioned above due to their specific data generation processes. Regarding ML4S, previous OOD benchmarks have proposed several data-split strategies to reflect distribution shifts in realistic scientific scenarios, such as molecular sizes and scaffolds [35], protein sequences and structures [41], and chemical reaction conditions [86]. Compared to these works, our benchmark not only collects datasets that can reflect distribution shifts in real-world scientific challenges but also, from an ML perspective, leverages the inherent causality of the specific geometric data generation processes in these applications to categorize their distribution shifts for an in-depth analysis.

**Available OOD Info.** In addition to the level without any OOD data [39; 27], some studies assume the availability of OOD features and benchmark DA methods [69; 24], while others assume the availability of OOD labels to investigate the model transferability [8; 88; 17; 46]. Compared to previous works, we aim to understand the benefits of different levels of OOD data across various distribution shifts, so our benchmark integrates three information levels.

## 3 Benchmark Design

### Distribution Shift Categories

Let \(\mathcal{X}\) be the input space, \(\mathcal{Y}\) be the output space, \(h:\mathcal{X}\rightarrow\mathcal{Y}\) be the labeling rule. Under the OOD assumption, we have joint distribution shift, _i.e._, \(\mathbb{P}_{\mathcal{S}}(X,Y)\neq\mathbb{P}_{\mathcal{T}}(X,Y)\). We denote \(f(\cdot;\Theta)\) as the GDL model with parameters \(\Theta\), and \(\ell:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}\) as the loss function. Our objective is to find an optimal model \(f^{*}\) with parameters \(\Theta^{*}\), which can be best generalized to target distribution \(\mathbb{P}_{\mathcal{T}}\):

\[\Theta^{*}=\arg\min_{\Theta}\mathbb{E}_{(X,Y)\sim\mathbb{P}_{\mathcal{T}}}[ \ell(f(X;\Theta),Y)] \tag{1}\]

For an in-depth analysis dedicated to scientific applications studied in this work, we consider the following data model [60; 9; 91; 45; 96]. The input variable \(X\in\mathcal{X}\) consists of two disjoint parts, namely the causal part \(X_{c}\) and the independent part \(X_{i}\), which satisfies conditional independence with \(Y\) given \(X_{c}\), _i.e._, \(X_{i}\perp\!\!\!\perp Y|X_{c}\). Next, we categorize various types of distribution shifts.

First, the above data model satisfies \(\mathbb{P}(X,Y)=\mathbb{P}(Y|X)\mathbb{P}(X)=\mathbb{P}(Y|X_{c})\mathbb{P}(X)\). Thus, we define covariate and concept shifts as follows.

\(\dagger\) Covariate Shift holds if \(\mathbb{P}_{\mathcal{S}}(Y|X_{c})=\mathbb{P}_{\mathcal{T}}(Y|X_{c})\), and \(\mathbb{P}_{\mathcal{S}}(X)\neq\mathbb{P}_{\mathcal{T}}(X)\).

\(\dagger\) Concept Shift holds if \(\mathbb{P}_{\mathcal{S}}(Y|X_{c})\neq\mathbb{P}_{\mathcal{T}}(Y|X_{c})\). Note that the shift of \(\mathbb{P}(Y|X_{c})\) is also characterized by the change of labeling rule \(h\) between \(\mathcal{S}\) and \(\mathcal{T}\).

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline \multirow{2}{*}{**Benchmark**} & \multirow{2}{*}{**Application Domain**} & \multicolumn{3}{c}{**Distribution Shift**} & \multicolumn{3}{c}{**Available OOD Info**} \\ \cline{3-8}  & & Covariate & Conditional & Concept & No-Info & O-Feature & Par-Label \\ \hline WILDS [39; 31] & CV and NLP & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) \\ \hline OoD-Bench [100] & CV & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) \\ \hline WILDS 2.0 [69; 103] & CV and NLP & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) \\ \hline Wild-Time [98] & CV and NLP & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) \\ \hline IGLUE [8] & NLP & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) \\ \hline GOOD [27], GDS [14] & Graph ML & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) \\ \hline \multirow{2}{*}{DrugOOD [35]} & ML4S (Graph ML) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) \\  & only Biochemistry & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) \\ \hline \multirow{2}{*}{**GeSS (Ours)**} & ML4S (GDL) & \multirow{2}{*}{\(\boldsymbol{\checkmark}\)} & \multirow{2}{*}{\(\boldsymbol{\checkmark}\)} & \multirow{2}{*}{\(\boldsymbol{\checkmark}\)} & \multirow{2}{*}{\(\boldsymbol{\checkmark}\)} & \multirow{2}{*}{\(\boldsymbol{\checkmark}\)} & \multirow{2}{*}{\(\boldsymbol{\checkmark}\)} & \multirow{2}{*}{\(\boldsymbol{\checkmark}\)} & \multirow{2}{*}{\(\boldsymbol{\checkmark}\)} \\  & HEP, Biochemistry & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) & \(\boldsymbol{\checkmark}\) \\ \multicolumn{1}{c}{} & and Materials Science & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with existing benchmarks under distribution shifts from three perspectives: Application Domain, Distribution Shift, and Available OOD Info. “Available OOD Info” refers to what type of OOD-Info has been used in the evaluation of these benchmarks.

On the other hand, we have \(\mathbb{P}(X,Y)=\mathbb{P}(X|Y)\mathbb{P}(Y)\). This induces the scenario of Conditional Shift, which holds if \(\mathbb{P}_{\mathcal{S}}(X|Y)\neq\mathbb{P}_{\mathcal{T}}(X|Y)\) and \(\mathbb{P}_{\mathcal{S}}(X)=\mathbb{P}_{\mathcal{T}}(Y)\), and Label Shift if \(\mathbb{P}_{\mathcal{S}}(X|Y)=\mathbb{P}_{\mathcal{T}}(X|Y)\) and \(\mathbb{P}_{\mathcal{S}}(Y)\neq\mathbb{P}_{\mathcal{T}}(Y)\). But as label shift does not arise in our datasets, we later only focus on Conditional Shift. The conditional probability can be decomposed into two parts due to our data model, _i.e._, \(\mathbb{P}(X|Y)=\mathbb{P}(X_{c}|Y)\mathbb{P}(X_{i}|X_{c})\). This enables us to further categorize Conditional Shift into two sub-types based on the specific factor experiencing shifts, and we observe that these two sub-types exhibit distinct characteristics in our experiments.

\(\dagger\)\(\mathcal{I}\)-Conditional Shift holds if \(\mathbb{P}_{\mathcal{S}}(X_{i}|X_{c})\neq\mathbb{P}_{\mathcal{T}}(X_{i}|X_{c})\), and \(\mathbb{P}_{\mathcal{S}}(X_{c}|Y)=\mathbb{P}_{\mathcal{T}}(X_{c}|Y)\).

\(\dagger\)\(\mathcal{C}\)-Conditional Shift holds if \(\mathbb{P}_{\mathcal{S}}(X_{i}|X_{c})=\mathbb{P}_{\mathcal{T}}(X_{i}|X_{c})\), and \(\mathbb{P}_{\mathcal{S}}(X_{c}|Y)\neq\mathbb{P}_{\mathcal{T}}(X_{c}|Y)\).

Given that each category mentioned above has only one factor experiencing shifts, we naturally partition sub-groups within the source domain \(\mathcal{S}\) based on the specific factor undergoing changes.

Note that our considered data models as above do not aim to cover all possible causal relationships. Some other models studied in previous literature [1, 11, 4] are discussed in Appendix A.1, but they do not correspond to GDL applications and thus fall outside the scope of this study. Our data models best describe the mechanisms of applications studied in this work. Besides, the categorization of distribution shifts is determined by the shifted probability term which _directly_ manifests through the data generation processes of the studied applications. These processes typically align with scientific experiment procedures or domain-specific theories. More details are in Appendix A.2.

### Dataset Curation and Shift Creation

In this section, we introduce the datasets in this study. Table 2 gives a summary. For each dataset, we first introduce the significance of its associated scientific application. Then, we delve into how the distribution shift of each dataset comes from in practice, and categorize the distribution shift according to the definition in Sec. 3.1. Additionally, we elaborate on the selection of domains \(\mathcal{S}\) and \(\mathcal{T}\), along with partitioning subgroups in the source domain \(\mathcal{S}\) for our later experimental setup.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline
**Scientific Field** & **Dataset** & **Domain** & **Shift Case** & **Shift Category** & **Evaluation Metrics** & **Applicable Method** \\ \hline \multirow{4}{*}{HEP} & \multirow{2}{*}{Track-.Pileup} & \multirow{2}{*}{Pileup} & PUSO & \(\mathcal{I}\)-Conditional Shift & ACC & Mixup \\  & & & & \(\tau\to 3\mu\) & & \\ \cline{2-7}  & & & & \(\tau_{50}\to 2\mu\) & \(\mathcal{C}\)-Conditional Shift & ACC & DANN \\ \hline \multirow{2}{*}{Maternikh Science} & \multirow{2}{*}{Qitter} & \multirow{2}{*}{Fidelity} & BES06 & \multirow{2}{*}{Concept Shift} & \multirow{2}{*}{MAE} & TL Methods \\  & & & & & & \\ \hline \multirow{3}{*}{Biochemistry} & DrugGOD-3D-Assay & Assay & \multirow{3}{*}{1bap-core-ic50-assay} & \multirow{3}{*}{Concept Shift} & \multirow{3}{*}{AUC} & GroupDBO \\  & DrugGOD-3D-Size & Size & & & & \\ \cline{1-1} \cline{3-4}  & DrugGOD-3D-Scaffold & Scaffold & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 2: Summary of distribution shifts in this study. We also recommend applicable methods for each scenario according to our experimental results, which are shown comprehensively in Table 3.

Figure 1: Overview of distribution shifts in this study. The upper (green-colored) and lower (blue-colored) instances represent the scenarios in domains \(\mathcal{S}\) and \(\mathcal{T}\), respectively. (a) Three-dimensional trajectories of particles in a collision event, which are simulated with a magnetic field parallel to the \(z\) axis and plotted on a 2D plane; (b) For the same set of MOFs, the distribution of calculated band gap values exhibits a bimodal (unimodal) nature with lower (higher) expectations under PBE (HSE06) estimation; (c) Molecular three-dimensional stick models with different scaffold IDs across \(\mathcal{S}\) and \(\mathcal{T}\).

#### 3.2.1 Track: Particle Tracking Simulation -- High Energy Physics

**Motivations.** ML techniques have long been employed and have played a significant role in diverse applications of particle physics [63], including particle flow reconstruction [37], jet tagging [61], and pileup mitigation [54], _etc._ Typically, ML models rely on simulation data for training due to the scarcity of labeled data from real-world experiments. However, the intricate and time-varying nature of experimental environments often leads to distinct physical characteristics that differ from simulated data used for training. For example, the pileup (PU) level, is defined as the number of noisy collisions around the primary collision in Large Hadron Collider experiments [32]. The PU level during the real deployment phase can differ from the PU level used to train the ML model.

**Dataset.** We create Track, a particle tracking simulation dataset, and propose Track-Pileup and Track-Signal datasets. A data sample corresponds to a collision event, which generates numerous particles. Each particle will leave multiple detector hits when traversing the detector. Each point in a data sample represents a detector hit generated by a particle associated with a 3D coordinate. The task is to predict the existence of a specific decay of interest (referred to as _signal_ in our work) in a given event, denoted by \(Y\), like the decay of \(z\rightarrow\mu\mu\). This can be formulated as a binary classification task in differentiating the detector hits left by the signal particles plus backgrounds (\(X_{c}+X_{i}\)) from those only left by background particles (\(X_{i}\)). This application scenario naturally involves a data generation process \(Y\to X\) because the detector hit patterns are determined by whether some type of collision happens. Such generation process leads to conditional shift: The variation in the number of PU particles (Pileup Shift) causes a shift in \(\mathbb{P}(X_{i}|X_{c})\), and the change in the type of signal particles (Signal Shift) causes a shift in \(\mathbb{P}(X_{c}|Y)\). We further categorize the two shifts as follows.

**Pileup Shift -- \(\mathcal{I}\)-Conditional Shift.** As shown in the bottom left of Fig. 1a, a higher PU level results in more background particle tracks in a collision while keeping the signal particle track the same. This mechanism aligns with our definition of \(\mathcal{I}\)-Conditional Shift as \(\mathbb{P}_{\mathcal{S}}(X_{i}|X_{c})\neq\mathbb{P}_{\mathcal{T}}(X_{i}|X_{c})\) and \(\mathbb{P}_{\mathcal{S}}(X_{c}|Y)=\mathbb{P}_{\mathcal{T}}(X_{c}|Y)\). We train the model on source-domain data with the PU level of 10 (PU10) and evaluate its generalizability on PU50 and PU90 target-domain data, respectively. The division of source-domain subgroups is based on the number of points present in the data entry (one collision event) as it can mimic Pileup Shift in terms of varying particle counts across different PU levels.

**Signal Shift -- \(\mathcal{C}\)-Conditional Shift.** As depicted in the bottom right of Fig. 1a, we alter the geometric characteristics of signal tracks by introducing signal particles with varying momenta, which leads to changes in the _curvature_ of signal tracks, while leaving the background particle tracks unchanged. Therefore, we categorize this shift as \(\mathcal{C}\)-Conditional Shift, as it satisfies \(\mathbb{P}_{\mathcal{S}}(X_{i}|X_{c})=\mathbb{P}_{\mathcal{T}}(X_{i}|X_{c})\) and \(\mathbb{P}_{\mathcal{S}}(X_{c}|Y)\neq\mathbb{P}_{\mathcal{T}}(X_{c}|Y)\). We train the model on source-domain data, where the positive samples consist of 5 different types of signal decays, all characterized by large signal track radii, making them easier to distinguish from background tracks. We evaluate the model on target-domain data with signal decays of \(z_{20}^{\prime}\to 2\mu\), \(z_{10}^{\prime}\to 2\mu\), \(\tau\to 3\mu\), respectively, whose radii of signal tracks are smaller. We split the source \(\mathcal{S}\) into 5 sub-groups, each corresponding to a specific type of signal decay.

#### 3.2.2 Qmof: Quantum Metal-organic Frameworks -- Materials Science

**Motivations.** Materials property prediction plays a crucial role in discovering new materials with favorable properties [62, 93]. Training ML models using data with labels calculated from theory-grounded methods, such as DFT [57], to predict important materials properties, such as band gap, has been an emerging trend, accelerating the process of materials discovery. Among DFT methods, PBE techniques are popular for their cost-effectiveness. However, they are noted for producing low-fidelity results, particularly in the underestimation of band gaps [111, 7]. Conversely, high-fidelity methods exhibit highly accurate calculations but come at the cost of extensive computational resources, resulting in a scarcity of high-fidelity labeled data. Hence, there's a need for methods that allow ML models trained on low-fidelity data to generalize to high-fidelity prediction.

**Dataset.** We introduce the Quantum MOF (QMOF) [66, 65], a publicly available dataset comprising over 20,000 metal-organic frameworks (MOFs), coordination polymers, and their quantum-chemical properties calculated from high-throughput periodic DFT. Each point in a sample represents an atom associated with a 3D coordinate. The task is to predict the band gap value of a given material as a regression problem that can be evaluated with MAE metrics. The dataset includes band gap values calculated by 4 different DFT methods (PBE, HLE17, HSE06*, and HSE06) ranging from low-fidelity to high-fidelity over the same set of input materials. This naturally forms the shifts across DFT methods at different fidelity levels (Fidelity Shift) categorized as follows.

**Fidelity Shift -- Concept Shift.** As illustrated in Fig. 0(b), DFT methods at different fidelity levels tend to yield varying distributions of the band gap estimation \(Y\) given the same set of input data \(X\), thus reflecting the shift of \(\mathbb{P}(Y|X)\) characterized by concept shift. Namely, the expected estimation band gap values tend to increase sequentially from PBE (the lowest estimation) to HLE17, HSE06*, and HSE06 (the highest estimation). We construct 2 separate shift cases: one with HSE06 as the target domain \(\mathcal{T}\) and the other with HSE06* as the target domain. In both cases, the remaining three levels are used as the source domain \(\mathcal{S}\), each serving as a subgroup in the source-domain splits.

#### 3.2.3 DrugOOD-3D: 3D Conformers of Drug Molecules -- Biochemistry

**Motivations.** ML techniques have been applied to various biochemical scenarios, such as protein design [3], molecular docking [12], _etc._, thereby catalyzing the process of drug discovery. Despite the success, the performance of ML-aided drug discovery easily degrades due to the underlying distribution shifts. Unpredictable public health events like COVID-19 may introduce entirely new targets from unseen domains. Besides, the assay environments, where biochemical properties are measured, may also largely diverge. These challenges related to the distribution shift spur a need for generalizable ML models to further advance drug discovery.

**Dataset.** We adapt DrugOOD [35] and propose DrugOOD-3D, with our main focus on the geometric structure of molecules and GDL models. We adopt the task of Ligand Based Affinity Prediction (LBAP) in predicting the binding affinity of a given ligand molecule. We transition the task to a binary classification problem, using AUC scores as evaluation metrics, following DrugOOD. We built DrugOOD-3D-Scaffold, DrugOOD-3D-Size, and DrugOOD-3D-Assay datasets, corresponding to shift cases of lbap-core-ic50-scaffold, lbap-core-ic50-size, and lbap-core-ic50-assay, which cover scaffold, assay, and size shifts introduced as follows.

**Scaffold & Size Shift -- Covariate Shift.** The scaffold pattern, illustrated in Fig. 0(c), is a significant structural characteristic to describe the core structure of a molecule [101]. Analogously, molecular size is also an important biochemical characteristic. We categorize the two shifts as covariate shifts because the shift in scaffold and size primarily reflects a shift in the marginal input distribution \(\mathbb{P}(X)\) across domains, while the labeling rule \(h\) and \(\mathbb{P}(Y|X)\) are kept invariant.

**Assay Shift -- Concept Shift.** We classify assay shift as concept shift since shifts in assays can be viewed as modifying the experimental procedures and conditions. Such modifications could alter the resulting binding affinity value for the same set of molecules, described as a change in \(\mathbb{P}(Y|X)\). Note that we follow the same design of domain splits and sub-group splits as DrugOOD.

## 4 Experiments

### Experimental Settings

We briefly introduce the experimental settings and leave more details about dataset splits in Appendix C, backbones and learning algorithms in Appendix D, and hyperparameter tuning in Appendix F.

**Backbones.** We include three GDL backbones widely used in various scientific applications: EGNN [71], DGCNN [85], and Point Transformer [110].

**Learning Algorithms.** We select **11** most representative OOD methods to compare. These methods cover general, GNN-grounded, and GDL-grounded algorithms, and span a broad range of learning strategies under different levels of OOD info, _i.e._, No-Info, O-Feature, and Par-Label levels. For **No-Info** level, we select 1) _vanilla_: ERM [82]; 2) _invariant learning_: VREx [40]; 3) _data augmentation_: MixUp [105]; 4) _subgroup robust method_: GroupDRO [68]; 5) _causal inference_: DIR [91]; 6) _information bottleneck_: LRI [55]. Note that DIR is a well-known graph-based OOD baseline and LRI is a novel algorithm grounded in GDL. We refer to the above-mentioned methods as _OOD generalization methods_ for simplicity. For **O-Feature** level, we select _domain-invariant methods_: 7) DANN [22] and 8) DeepCoral [75]. For **Par-Label** level, we conduct _full_ fine-tuning, _i.e._, all model parameters get fine-tuned, with 9) 100, 10) 500, and 11) 1000 labels, denoted as \(\mathrm{TL}_{100},\mathrm{TL}_{500},\mathrm{TL}_{1000}\) respectively. Regarding Fidelity Shift, we select a subset of OOD generalization methods (VREx and GroupDRO) that are compatible with regression tasks to evaluate.

We provide a detailed discussion on the rationale behind the selection of methods and GDL backbones in Appendices D.1 and D.2. Additionally, we have developed a highly modular codebase, allowing for the seamless evaluation of new algorithms tailored for the GDL setting using this benchmark.

**Dataset Splits.** For each dataset, we first divide it into the ID dataset and the OOD dataset based on our characterization of \(\mathbb{P}_{\mathcal{S}}\) and \(\mathbb{P}_{\mathcal{T}}\). The resulting dataset in the source domain contains multiple subgroups following our split covered in Sec. 3.2, for the operation of OOD methods that rely on subgroup splits. Subsequently, the ID and OOD datasets are randomly segmented into Train-ID, Val-ID, and Test-ID, and Train-OOD, Val-OOD, and Test-OOD, respectively.

**Model Training & Evaluation.** For fair comparisons across the three info levels, we meticulously set up both the model training and evaluation processes: In No-Info level, we train the model solely on the Train-ID dataset via OOD methods. In O-Feature level, we apply DA algorithms and train

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline  & & \multicolumn{4}{c}{**PIleup Shift \(-2\)-Conditional Shift (ACC\(\uparrow\))**} & \multicolumn{4}{c}{**DOCCNN**} \\ \cline{3-11} \multirow{2}{*}{Level} & \multirow{2}{*}{Algorithm} & \multicolumn{4}{c}{**EGNN**} & \multicolumn{4}{c}{**FUS0**} & \multicolumn{4}{c}{**FUS0**} \\ \cline{3-11}  & & & \multicolumn{2}{c}{Test-ID} & \multicolumn{2}{c}{Test-OOD} & \multicolumn{2}{c}{Test-ID} & \multicolumn{2}{c}{Test-OOD} & \multicolumn{2}{c}{Test-ID} & \multicolumn{2}{c}{Test-OOD} \\ \hline \multirow{8}{*}{No-Info} & ERM & 95.7(0.08) & 87.65(0.30) & 86.1(0.15) & 80.99(1.40) & 94.5(0.24) & **86.36(0.96)** & 94.35(0.2) & 79.84(0.08) \\  & VREx & 95.49(0.32) & 87.45(0.30) & 95.49(0.32) & 80.7(0.93) & 94.5(0.17) & 86.37(0.38) & 94.51(0the model on the _same_ Train-ID used in No-Info level _plus_ the extra data feature info of the entire OOD dataset. In Par-Label level, we use the Train-OOD dataset to fine-tune the model which has already been pre-trained on the _same_ Train-ID used in the two previous levels. Although the training datasets may vary due to the need to contain different levels of OOD info, we achieve a fair comparison by keeping Train-ID data invariant. Across all levels of OOD info, we evaluate the model's ID performance using the _same_ Val-ID and Test-ID datasets, and its OOD performance using the _same_ Val-OOD and Test-OOD datasets. For hyperparameter tuning, we tune a predefined set of hyperparameters and select the model with the best metric score of Val-OOD for the ultimate evaluation. Also, we thoroughly discuss the motivation and insights of our design of model training and evaluation processes, and put details in Appendix E.

### Results Analysis -- General Tendency

We put experimental results on 2 of 3 backbones in Table 3. Complete results can be found in Appendix G. We begin by presenting overall comparisons and general findings: Although \(\mathrm{TL}_{1000}\) outperforms ERM by a notable margin in several cases, fine-tuning can sometimes result in negative effects when the labeled OOD data is quite limited, particularly in cases involving a smaller degree of distribution shifts (_cf._ Fig. 2). This is consistent with [38], where fine-tuning a large model based on a small set of labels may lead to catastrophic forgetting. To mitigate this issue, robust fine-tuning strategies, such as weight-space ensembles [90], regularization [94] and surgical fine-tuning [44], could be potential solutions.

### Results Analysis -- Insightful Conclusions

Besides the general observations exhibited above, our experiments also yield some intriguing conclusions that may be widely applicable. We structure this subsection by first presenting our conclusions, exemplified by representative observations and rational explanations.

\(\bullet\)**Conclusion 1. DA strategies excel in \(\mathcal{C}\)-Conditional Shift, where some variation arises in the geometric characteristics of the causal component \(X_{c}\).**

A great example illustrating this conclusion comes from Signal Shift. As shown in Fig. 4, DANN, a DA method, performs particularly well in the case of \(z^{\prime}_{10}\to 2\mu\), largely outperforming ERM (\(\uparrow\) 9.4% in the EGNN model) and all OOD generalization methods without OOD info (\(\uparrow\) at least 6.6% in EGNN). As introduced before, Signal Shift represents a \(\mathcal{C}\)-conditional shift, which in this case arises from the shift in the curvature of the signal tracks, whose points collectively form the causal component \(X_{c}\). The access to OOD features enables the model to align the latent representations of the causal components with varying distributions of geometries (curvatures here) across the source and target domains, thereby aiding in the correct identification of unseen signal types.

In contrast, Fig. 4 shows that DA strategies yield performance very close to ERM in Pileup Shift. Although both Pileup and Signal shifts are categorized as Conditional Shifts (_cf._ Sec. 3.2.1), they mainly differ in two aspects: 1) Pileup Shift represents an \(\mathcal{I}\)-Conditional Shift, occurring exclusively in the independent part, _i.e._\(\mathbb{P}(X_{i}|X_{c})\), and 2) it involves a variation in the number of particle tracks rather than geometric characteristics, which is different from Signal Shift. We propose two plausible explanations for the challenges faced by DA strategies in Pileup Shift, centered around these disparities, and recommend interested readers see **H2** in Appendix H.

\(\bullet\)**Conclusion 2. TL methods excel under Concept Shift, particularly when the shift of the marginal label distribution \(\mathbb{P}(Y)\) is large.**

Here we examine two cases of Fidelity Shift, where the TL strategy demonstrates contrasting results (_cf._ Fig. 2(a)): TL performs particularly well in the case of HSE06, where it largely outperforms all other methods with the MAE score increased by at least 40%. However, it exhibits only limited improvement in the case of HSE06*. We explain the difference by analyzing the marginal label distributions \(\mathbb{P}_{\mathcal{S}}(Y)\) and \(\mathbb{P}_{\mathcal{T}}(Y)\). As mentioned before, Fidelity Shift can be characterized by a change in the labeling rule, _i.e._, two similar inputs may be mapped to very different \(Y\) values.

Figure 2: Test-OOD improvements (%) over ERM for \(\mathrm{TL}_{100}\) and \(\mathrm{TL}_{1000}\) across different shift cases (in the EGNN backbone).

Specifically, fidelity levels of PBE, HLE17, and HSE06* provide estimations that are closer to each other, while the fidelity level of HSE06 significantly exceeds the other three. Therefore, the case with HSE06 as the target domain \(\mathcal{T}\) but the other three as the source \(\mathcal{S}\), yields a large gap in the distribution of \(\mathbb{P}(Y)\) between the two domains, _i.e.,_\(\mathbb{P}_{S}(Y)\neq\mathbb{P}_{\mathcal{T}}(Y)\) (_cf._ Fig. 2(b)). Therefore, the OOD labels are crucial to finetune the model predictions to match the aimed distribution \(\mathbb{P}_{\mathcal{T}}(Y)\). In contrast, the case when HSE06* is the target domain \(\mathcal{T}\) but the others as \(\mathcal{S}\), yields closer distributions of \(\mathbb{P}(Y)\) between the two domains, _i.e.,_\(\mathbb{P}_{\mathcal{S}}(Y)\approx\mathbb{P}_{\mathcal{T}}(Y)\) (_cf._ Fig. 2(c)). Therefore, only using a few OOD labels to finetune the model tends to have a limited impact on its performance.

\(\bullet\)**Conclusion 3. For the OOD generalization methods to learn robust representations, the more informatively the groups obtained by splitting the source domain \(\mathcal{S}\) indicate the distribution shift, the better performance these methods may achieve.**

This observation is related to GroupDRO, an OOD method that is to learn robust representation across different group splits of the source \(\mathcal{S}\). As illustrated in Fig. 4, GroupDRO almost consistently outperforms ERM in all cases with Signal Shift (\(\tau\), \(z^{\prime}_{10},z^{\prime}_{20}\)) while it largely under-performs ERM in the cases of Pileup Shift (PUS0, PUS0). GroupDRO captures robustness by increasing the importance of subgroups with larger errors and thus highly relies on the assumption that the shift between the splits of in-domain data can to some extent reflect the distribution shift between the source \(\mathcal{S}\) and the target \(\mathcal{T}\). In the cases with Signal Shift, the way to split subgroups of the source domain aligns well with the distribution shift: Each split represents a distinct type of decay (5 types in total). By learning robust representations across these subgroups, GroupDRO yields better OOD generalization. In contrast, in the case of Pileup Shift, varying the number of points in a collision event is used as a proxy of the shift to achieve the group splits of the training dataset, based on the fact that the PU level is positively correlated with the number of particles. This way of subgroup splits is subjective, which is limited by the availability of data and may not fully reflect Pileup Shift between domain \(\mathcal{S}\) and \(\mathcal{T}\).

More analysis (observations, conclusions, and conjectures) of experimental results are put in Appendix H. Also, we identify conclusions that align with existing OOD literature and others that are novel and _specific_ to the GDL setting. We conduct comparisons (including consistency and disparity) between our findings and previous ones in Appendix I.

## 5 Conclusion

This work systematically evaluates the performance of GDL models when scientific applications meet distribution shifts. Our benchmark has 30 distinct scenarios with 10 shift cases times 3 levels of available OOD info, covering 3 GDL backbones and 11 learning algorithms. Based on our evaluation, we reveal several intriguing discoveries. In particular, our results may help select applicable solutions based on the causal mechanism behind the distribution shift and the availability of OOD info. Moreover, our work encourages more realistic and rigorous evaluations of GDL used in scientific applications, and may inspire methodological advancements for GDL to deal with distribution shifts.

Figure 4: Test-OOD improvements (%) over ERM for GroupDRO, MixUp, LRI, and DANN methods across Pileup Shifts (cases of PUS0/90) and Signal Shifts (cases of \(\tau\to 3\mu\) and \(z^{\prime}_{10}\to 2\mu\)) in the EGNN backbone.

Figure 3: (a) Test-OOD improvements (%) over ERM for VREx, DeepCoral, \(\mathrm{TL}_{100}\) and \(\mathrm{TL}_{1000}\) methods in Fidelity Shifts (including HSE06 and HSE06* cases) in the EGNN backbone; (b)/(c) KDE [67, 58] curves of the marginal label distribution \(\mathbb{P}(Y)\) across the source \(\mathcal{S}\) and target \(\mathcal{T}\) in the cases of HSE06 / HSE06*.

## Acknowledgments

The authors would like to thank Dr. Callie Hao and the Sharc Lab at Georgia Tech. for their computing resources (NVIDIA RTX A6000) to support this work, and also thank Dr. Mia Liu at Purdue University for suggestions about distribution shifts in HEP, and Dr. Jan-Frederik Schulte at Purdue University for the help with the generation of some of HEP data. Deyu Zou, Shikun Liu, Siqi Miao and Pan Li are partially supported by the NSF awards PHY-2117997, IIS-2239565, IIS-2428777, and CCF-2402816; DOE award DE-FOA-0002785; JPMC faculty awards; OpenAI Researcher Access Program Credits; and Microsoft Azure Research Credits for Generative AI.

## References

* [1] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution generalization. _Advances in Neural Information Processing Systems_, 34:3438-3450, 2021.
* [2] Xiaocong Ai, Corentin Allaire, Noemi Calace, Angela Czirkos, Irina Ene, Markus Elsing, Ralf Farkas, Louis-Guillaume Gagnon, Rocky Garg, Paul Gessinger, Hadrien Grasland, Heather M. Gray, Christian Gumpert, Julia Hrdika, Benjamin Huth, Moritz Kiehn, Fabian Klimpel, Attila Krasznahorkay, Robert Langenberg, Charles Leggett, Joana Niermann, Joseph D. Osborn, Andreas Salzburger, Bastian Schlag, Lauren Tompkins, Tomohiro Yamazaki, Beomki Yeo, Jin Zhang, Georgiana Mania, Bernadette Kolbinger, Edward Moyse, and David Rousseau. A common tracking software project, 2021.
* [3] Namrata Anand and Tudor Achim. Protein structure and sequence generation with equivariant denoising diffusion probabilistic models. _arXiv preprint arXiv:2205.15019_, 2022.
* [4] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint arXiv:1907.02893_, 2019.
* [5] Gleb Bazhenov, Denis Kuznedelev, Andrey Malinin, Artem Babenko, and Liudmila Prokhorenkova. Evaluating robustness and uncertainty of graph models under structural distributional shifts. _Advances in Neural Information Processing Systems_, 36, 2024.
* [6] Christian Bierlich, Smita Chakraborty, Nishita Desai, Leif Gellersen, Ilkka Helenius, Philip Ilten, Leif Lonnblad, Stephen Mrenna, Stefan Prestel, Christian Tobias Preuss, et al. A comprehensive guide to the physics and usage of pythia 8.3. _SciPost Physics Codebases_, page 008, 2022.
* [7] Pedro Borlido, Thorsten Aull, Ahmad W Huran, Fabien Tran, Miguel AL Marques, and Silvana Botti. Large-scale benchmark of exchange-correlation functionals for the determination of electronic band gaps of solids. _Journal of chemical theory and computation_, 15(9):5069-5079, 2019.
* [8] Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott, Edoardo Maria Ponti, and Ivan Vulic. Iglue: A benchmark for transfer learning across modalities, tasks, and languages. In _International Conference on Machine Learning_, pages 2370-2392. PMLR, 2022.
* [9] Shiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. Invariant rationalization. In _International Conference on Machine Learning_, pages 1448-1458. PMLR, 2020.
* [10] Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, et al. Open catalyst 2020 (oc20) dataset and community challenges. _Acs Catalysis_, 11(10):6059-6072, 2021.
* [11] Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, MA Kaili, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-of-distribution generalization on graphs. _Advances in Neural Information Processing Systems_, 35:22131-22148, 2022.

* Corso et al. [2023] Gabriele Corso, Bowen Jing, Regina Barzilay, Tommi Jaakkola, et al. Diffdock: Diffusion steps, twists, and turns for molecular docking. In _International Conference on Learning Representations (ICLR 2023)_, 2023.
* Creager et al. [2021] Elliot Creager, Jorn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. In _International Conference on Machine Learning_, pages 2189-2200. PMLR, 2021.
* Ding et al. [2021] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In _NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications_, 2021.
* Duarte and Vlimant [2022] Javier Duarte and Jean-Roch Vlimant. Graph neural networks for particle tracking and reconstruction. In _Artificial intelligence for high energy physics_, pages 387-436. World Scientific, 2022.
* Duval et al. [2023] Alexandre Duval, Simon V Mathis, Chaitanya K Joshi, Victor Schmidt, Santiago Miret, Fragkiskos D Malliaros, Taco Cohen, Pietro Lio, Yoshua Bengio, and Michael Bronstein. A hitchhiker's guide to geometric gnns for 3d atomic systems. _arXiv preprint arXiv:2312.07511_, 2023.
* Fang et al. [2024] Alex Fang, Simon Kornblith, and Ludwig Schmidt. Does progress on imagenet transfer to real-world datasets? _Advances in Neural Information Processing Systems_, 36, 2024.
* Fey and Lenssen [2019] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. _arXiv preprint arXiv:1903.02428_, 2019.
* Fuchs et al. [2020] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d roto-translation equivariant attention networks. _Advances in neural information processing systems_, 33:1970-1981, 2020.
* Fung et al. [2021] Victor Fung, Jiaxin Zhang, Eric Juarez, and Bobby G Sumpter. Benchmarking graph neural networks for materials chemistry. _npj Computational Materials_, 7(1):84, 2021.
* Gama et al. [2014] Joao Gama, Indre Zliobaite, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. A survey on concept drift adaptation. _ACM computing surveys (CSUR)_, 46(4):1-37, 2014.
* Ganin et al. [2016] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. _The journal of machine learning research_, 17(1):2096-2030, 2016.
* Gardner et al. [2024] Josh Gardner, Zoran Popovic, and Ludwig Schmidt. Benchmarking distribution shift in tabular data with tableshift. _Advances in Neural Information Processing Systems_, 36, 2024.
* Garg et al. [2023] Saurabh Garg, Nick Erickson, James Sharpnack, Alex Smola, Sivaraman Balakrishnan, and Zachary Chase Lipton. Rlspench: Domain adaptation under relaxed label shift. In _International Conference on Machine Learning_, pages 10879-10928. PMLR, 2023.
* Gasteiger et al. [2019] Johannes Gasteiger, Janek Gross, and Stephan Gunnemann. Directional message passing for molecular graphs. In _International Conference on Learning Representations_, 2019.
* Gretton et al. [2009] Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, Bernhard Scholkopf, et al. Covariate shift by kernel mean matching. _Dataset shift in machine learning_, 3(4):5, 2009.
* Gui et al. [2022] Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. Good: A graph out-of-distribution benchmark. _Advances in Neural Information Processing Systems_, 35:2059-2073, 2022.
* Gulrajani and Lopez-Paz [2020] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. _arXiv preprint arXiv:2007.01434_, 2020.
* Halgren [1999] Thomas A Halgren. Mmff vi. mmff94s option for energy minimization studies. _Journal of computational chemistry_, 20(7):720-729, 1999.

* [30] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 8340-8349, 2021.
* [31] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _International Conference on Learning Representations_, 2018.
* [32] Roger Highfield. Large hadron collider: Thirteen ways to change the world. _The Daily Telegraph. London. Retrieved_, pages 10-10, 2008.
* [33] Noah Hoffmann, Jonathan Schmidt, Silvana Botti, and Miguel AL Marques. Transfer learning on large datasets for the accurate prediction of material properties. _arXiv preprint arXiv:2303.03000_, 2023.
* [34] Mark Ibrahim, Quentin Garrido, Ari S Morcos, and Diane Bouchacourt. The robustness limits of sota vision models to natural variation. _Transactions on Machine Learning Research_, 2023.
* [35] Yuanfeng Ji, Lu Zhang, Jiaxiang Wu, Bingzhe Wu, Long-Kai Huang, Tingyang Xu, Yu Rong, Lanqing Li, Jie Ren, Ding Xue, et al. Drugood: Out-of-distribution (ood) dataset curator and benchmark for ai-aided drug discovery-a focus on affinity prediction problems with noise annotations. _arXiv preprint arXiv:2201.09637_, 2022.
* [36] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. In _International Conference on Learning Representations_, 2020.
* [37] Jan Kieseler. Object condensation: one-stage grid-free multi-object reconstruction in physics detectors, graph, and image data. _The European Physical Journal C_, 80:1-12, 2020.
* [38] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _Proceedings of the national academy of sciences_, 114(13):3521-3526, 2017.
* [39] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In _International Conference on Machine Learning_, pages 5637-5664. PMLR, 2021.
* [40] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In _International Conference on Machine Learning_, pages 5815-5826. PMLR, 2021.
* [41] Tim Kucera, Carlos Oliver, Dexiong Chen, and Karsten Borgwardt. Proteinshake: Building datasets and benchmarks for deep learning on protein structures. _Advances in Neural Information Processing Systems_, 36, 2024.
* [42] Greg Landrum et al. Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling. _Greg Landrum_, 8:31, 2013.
* [43] Stefan Larson, Yi Yang Gordon Lim, Yutong Ai, David Kuang, and Kevin Leach. Evaluating out-of-distribution performance on document image classifiers. _Advances in Neural Information Processing Systems_, 35:11673-11685, 2022.
* [44] Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgical fine-tuning improves adaptation to distribution shifts. In _The Eleventh International Conference on Learning Representations_, 2022.
* [45] Haoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Learning invariant graph representations for out-of-distribution generalization. _Advances in Neural Information Processing Systems_, 35:11828-11841, 2022.

* [46] Wenxuan Li, Alan Yuille, and Zongwei Zhou. How well do supervised models transfer to 3d image segmentation? In _The Twelfth International Conference on Learning Representations_, 2023.
* [47] Yi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic graphs. In _The Eleventh International Conference on Learning Representations_, 2022.
* [48] Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. Zin: When and how to learn invariance without environment partition? _Advances in Neural Information Processing Systems_, 35:24529-24542, 2022.
* [49] Jiashuo Liu, Tianyu Wang, Peng Cui, and Hongseok Namkoong. On the need for a language describing distribution shifts: Illustrations on tabular datasets. _Advances in Neural Information Processing Systems_, 36, 2024.
* [50] Shikun Liu, Tianchun Li, Yongbin Feng, Nhan Tran, Han Zhao, Qiang Qiu, and Pan Li. Structural re-weighting improves graph domain adaptation. In _International Conference on Machine Learning_, pages 21778-21793. PMLR, 2023.
* [51] Yi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message passing for 3d molecular graphs. In _International Conference on Learning Representations_, 2021.
* [52] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. Learning under concept drift: A review. _IEEE transactions on knowledge and data engineering_, 31(12):2346-2363, 2018.
* [53] Aengus Lynch, Gbetomji JS Dovonon, Jean Kaddour, and Ricardo Silva. Spawrious: A benchmark for fine control of spurious correlation biases. _arXiv preprint arXiv:2303.05470_, 2023.
* [54] J Arjona Martinez, Olmo Cerri, Maria Spiropulu, JR Vlimant, and M Pierini. Pileup mitigation at the large hadron collider with graph neural networks. _The European Physical Journal Plus_, 134(7):333, 2019.
* [55] Siqi Miao, Yunan Luo, Mia Liu, and Pan Li. Interpretable geometric deep learning via learnable randomness injection. In _The Eleventh International Conference on Learning Representations_, 2022.
* [56] Sadaman Sadeed Omee, Nihang Fu, Rongzhi Dong, Ming Hu, and Jianjun Hu. Structure-based out-of-distribution (ood) materials property prediction: a benchmark study. _arXiv preprint arXiv:2401.08032_, 2024.
* [57] Maylis Orio, Dimitrios A Pantazis, and Frank Neese. Density functional theory. _Photosynthesis research_, 102:443-453, 2009.
* [58] Emanuel Parzen. On estimation of a probability density function and mode. _The annals of mathematical statistics_, 33(3):1065-1076, 1962.
* [59] Judea Pearl et al. Models, reasoning and inference. _Cambridge, UK: CambridgeUniversityPress_, 19(2):3, 2000.
* [60] Judea Pearl, Madelyn Glymour, and Nicholas P Jewell. _Causal inference in statistics: A primer_. John Wiley & Sons, 2016.
* [61] Huilin Qu and Loukas Gouskos. Jet tagging via particle clouds. _Physical Review D_, 101(5):056019, 2020.
* [62] Paul Raccuglia, Katherine C Elbert, Philip DF Adler, Casey Falk, Malia B Wenny, Aurelio Mollo, Matthias Zeller, Sorelle A Friedler, Joshua Schrier, and Alexander J Norquist. Machine-learning-assisted materials discovery using failed experiments. _Nature_, 533(7601):73-76, 2016.

* [63] Alexander Radovic, Mike Williams, David Rousseau, Michael Kagan, Daniele Bonacorsi, Alexander Himmel, Adam Aurisano, Kazuhiro Terao, and Taritree Wongjirad. Machine learning at the energy and intensity frontiers of particle physics. _Nature_, 560(7716):41-48, 2018.
* [64] Sereina Riniker and Gregory A Landrum. Better informed distance geometry: using what we know to improve conformation generation. _Journal of chemical information and modeling_, 55(12):2562-2574, 2015.
* [65] Andrew S Rosen, Victor Fung, Patrick Huck, Cody T O'Donnell, Matthew K Horton, Donald G Truhlar, Kristin A Persson, Justin M Notestein, and Randall Q Snurr. High-throughput predictions of metal-organic framework electronic properties: theoretical challenges, graph neural networks, and data exploration. _npj Computational Materials_, 8(1):112, 2022.
* [66] Andrew S Rosen, Shaelyn M Iyer, Debmalya Ray, Zhenpeng Yao, Alan Aspuru-Guzik, Laura Gagliardi, Justin M Notestein, and Randall Q Snurr. Machine learning the quantum-chemical properties of metal-organic frameworks for accelerated materials discovery. _Matter_, 4(5):1578-1597, 2021.
* [67] Murray Rosenblatt. Remarks on some nonparametric estimates of a density function. _The annals of mathematical statistics_, pages 832-837, 1956.
* [68] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks. In _International Conference on Learning Representations_, 2019.
* [69] Shiori Sagawa, Pang Wei Koh, Tony Lee, Irene Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, et al. Extending the wild's benchmark for unsupervised adaptation. In _International Conference on Learning Representations (ICLR)_, 2022.
* [70] Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation shift. In _International Conference on Learning Representations_, 2021.
* [71] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In _International conference on machine learning_, pages 9323-9332. PMLR, 2021.
* [72] Kristof Schutt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert Muller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. _Advances in neural information processing systems_, 30, 2017.
* [73] Kristof Schutt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In _International Conference on Machine Learning_, pages 9377-9388. PMLR, 2021.
* [74] Jonathan Shlomi, Peter Battaglia, and Jean-Roch Vlimant. Graph neural networks in particle physics. _Machine Learning: Science and Technology_, 2(2):021001, 2020.
* [75] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In _Computer Vision-ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14_, pages 443-450. Springer, 2016.
* [76] Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et al. TrustlIm: Trustworthiness in large language models. _arXiv preprint arXiv:2401.05561_, 2024.
* [77] Xiaoxiao Sun, Xingjian Leng, Zijian Wang, Yang Yang, Zi Huang, and Liang Zheng. Cifar-10-warehouse: Broad and more realistic testbeds in model generalization analysis. _arXiv preprint arXiv:2310.04414_, 2023.
* [78] Philipp Tholke and Gianni De Fabritiis. Equivariant transformers for neural network based molecular potentials. In _International Conference on Learning Representations_, 2021.

* [79] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. _arXiv preprint arXiv:1802.08219_, 2018.
* [80] Richard Tran, Janice Lan, Muhammed Shuaibi, Brandon M Wood, Siddharth Goyal, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, et al. The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysts. _ACS Catalysis_, 13(5):3066-3084, 2023.
* [81] Jessica Vamathevan, Dominic Clark, Paul Czodrowski, Ian Dunham, Edgardo Ferran, George Lee, Bin Li, Anant Madabhushi, Parantu Shah, Michaela Spitzer, et al. Applications of machine learning in drug discovery and development. _Nature reviews Drug discovery_, 18(6):463-477, 2019.
* [82] Vladimir N Vapnik. An overview of statistical learning theory. _IEEE transactions on neural networks_, 10(5):988-999, 1999.
* [83] Limei Wang, Yi Liu, Yuchao Lin, Haoran Liu, and Shuiwang Ji. Comenet: Towards complete and efficient message passing for 3d molecular graphs. _Advances in Neural Information Processing Systems_, 35:650-664, 2022.
* [84] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph classification. In _Proceedings of the Web Conference 2021_, pages 3663-3674, 2021.
* [85] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. _ACM Transactions on Graphics (tog)_, 38(5):1-12, 2019.
* [86] Zihao Wang, Yongqiang Chen, Yang Duan, Weijiang Li, Bo Han, James Cheng, and Hanghang Tong. Towards out-of-distribution generalizable predictions of chemical kinetics properties. _arXiv preprint arXiv:2310.03152_, 2023.
* [87] David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. _Journal of chemical information and computer sciences_, 28(1):31-36, 1988.
* [88] Florian Wenzel, Andrea Dittadi, Peter Gehler, Carl-Johann Simon-Gabriel, Max Horn, Dominik Zietlow, David Kernert, Chris Russell, Thomas Brox, Bernt Schiele, et al. Assaying out-of-distribution generalization in transfer learning. _Advances in Neural Information Processing Systems_, 35:7181-7198, 2022.
* [89] Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena, Krishnamurthy Dj Dvijotham, and Ali Taylan Cemgil. A fine-grained analysis on distribution shift. In _International Conference on Learning Representations_, 2021.
* [90] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 7959-7971, 2022.
* [91] Yingxin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. In _International Conference on Learning Representations_, 2021.
* [92] Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In _International Conference on Learning Representations_, 2020.
* [93] Tian Xie and Jeffrey C Grossman. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. _Physical review letters_, 120(14):145301, 2018.

* [94] LI Xuhong, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning with convolutional networks. In _International Conference on Machine Learning_, pages 2825-2834. PMLR, 2018.
* [95] Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization perspective. _arXiv preprint arXiv:2211.08073_, 2022.
* [96] Nianzu Yang, Kaipeng Zeng, Qitian Wu, Xiaosong Jia, and Junchi Yan. Learning substructure invariance for out-of-distribution molecular representations. _Advances in Neural Information Processing Systems_, 35:12964-12978, 2022.
* [97] Yuzhe Yang, Haoran Zhang, Dina Katabi, and Marzyeh Ghassemi. Change is hard: A closer look at subpopulation shift. _arXiv preprint arXiv:2302.12254_, 2023.
* [98] Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei W Koh, and Chelsea Finn. Wild-time: A benchmark of in-the-wild distribution shift over time. _Advances in Neural Information Processing Systems_, 35:10309-10324, 2022.
* [99] Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Improving out-of-distribution robustness via selective augmentation. In _International Conference on Machine Learning_, pages 25407-25437. PMLR, 2022.
* [100] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and Jun Zhu. Ood-bench: Quantifying and understanding two dimensions of out-of-distribution generalization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7947-7958, 2022.
* [101] Austin B Yongye, Jacob Waddell, and Jose L Medina-Franco. Molecular scaffold analysis of natural products databases in the public domain. _Chemical biology & drug design_, 80(5):717-724, 2012.
* [102] Yemin Yu, Luotian Yuan, Ying Wei, Hanyu Gao, Fei Wu, Zhihua Wang, and Xinhai Ye. Retroood: Understanding out-of-distribution generalization in retrosynthesis prediction. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 374-382, 2024.
* [103] Yongcan Yu, Lijun Sheng, Ran He, and Jian Liang. Benchmarking test-time adaptation against distribution shifts in image classification. _arXiv preprint arXiv:2307.03133_, 2023.
* [104] Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, and Maosong Sun. Revisiting out-of-distribution robustness in nlp: Benchmarks, analysis, and llms evaluations. _Advances in Neural Information Processing Systems_, 36, 2024.
* [105] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In _International Conference on Learning Representations_, 2018.
* [106] Kun Zhang, Bernhard Scholkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under target and conditional shift. In _International conference on machine learning_, pages 819-827. PMLR, 2013.
* [107] Min Zhang, Haoxuan Li, Fei Wu, and Kun Kuang. Metacoco: A new few-shot classification benchmark with spurious correlation. _arXiv preprint arXiv:2404.19644_, 2024.
* [108] Ziqiao Zhang, Bangyi Zhao, Ailin Xie, Yatao Bian, and Shuigeng Zhou. Activity cliff prediction: Dataset and benchmark. _arXiv preprint arXiv:2302.07541_, 2023.
* [109] Bingchen Zhao, Shaozuo Yu, Wufei Ma, Mingxin Yu, Shenxiao Mei, Angtian Wang, Ju He, Alan Yuille, and Adam Kortylewski. Ood-cv: A benchmark for robustness to out-of-distribution shifts of individual nuisances in natural images. In _European conference on computer vision_, pages 163-180. Springer, 2022.
* [110] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 16259-16268, 2021.

* [111] Yan Zhao and Donald G Truhlar. Calculation of semiconductor band gaps with the m06-l density functional. _The Journal of chemical physics_, 130(7), 2009.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] As mentioned in Section 3.1, our considered data models do not aim to cover all possible causal relationships. Some other models have been discussed in Appendix A but they do not correspond to our proposed GDL applications and thus fall outside the scope of this study. We leave benchmarks for GDL scenarios with other data models as a future work. 3. Did you discuss any potential negative societal impacts of your work? [N/A] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See the URL in the first page. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 3.2, Appendix C and F for details. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] Standard deviation across 3 replicates are shown in Table 3, 9, 10, 11, and 12. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Acknowledgments.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? See Section 3.2. 2. Did you mention the license of the assets? See Appendix C.5. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? See Appendix C.5. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Appendix C.5.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]More Discussions about Distribution Shift Formulations

### Details of the Data Model

Here we give a complementary discussion about the established data model that are discussed in Sec. 3.1. Concretely, we adopt the Structure Causal Model (SCM) [59; 60] to represent \(\mathcal{I}\)-conditional, \(\mathcal{C}\)-conditional, covariate, and concept shifts from a causal view. As illustrated in Fig. 5, five variables, including input \(X\), causal part \(X_{c}\), independent part \(X_{i}\), domain \(D\) and label \(Y\), are linked by the direct causal correlation "\(\rightarrow\)". For some variable \(A\) in the SCM, \(\textbf{Pa}(A)\to A\) denotes as the direct causal link from its parent variables \(\textbf{Pa}(A)\) to \(A\). According to the causal theory [59; 60], there exists the correlation \(\textbf{Pa}(A)\to A\), if and only if there exists a function \(f_{A}\), _s.t._, \(A=f_{A}(\textbf{Pa}(A),\epsilon_{A})\), where \(\epsilon_{A}\) is exogenous noise satisfying \(\epsilon_{A}\perp\!\!\!\perp\textbf{Pa}(A)\), and we omit the exogenous noise in this study for simplification. Plus note we treat \(D\) as an additional variable that exerts an influence on the other variables and thus induces a shift in the corresponding probability distribution between the domains \(\mathcal{S}\) and \(\mathcal{T}\).

We start with the correlation that is shared across four categories. The input variable \(X\) consists of two disjoint parts \(X_{i}\) and \(X_{c}\), \(i.e.\), \(X_{i}\to X\gets X_{c}\). For a convenient analysis of the proposed application scenarios, we introduce certain assumptions or specifications. However, it's important to note that our data model and its associated assumptions do not aim to cover all possible causal relationships. To provide a more comprehensive view, each specification or assumption is accompanied by some examples (from other works) that challenge it.

* We do not further discuss potential causal dependencies between \(X_{i}\) and \(X_{c}\) for simplicity and use a dashed arrow to represent such potential dependencies, following [91]. However, some works [1; 11] involved the explicit discussions of latent interactions between \(X_{i}\) and \(X_{c}\).
* We assume the independent part \(X_{i}\) and the label variable \(Y\) to be conditionally independent given \(X_{c}\), \(i.e.\), \(X_{i}\perp\!\!\!\perp Y|X_{c}\), as discussed in Sec. 3.1. Also, there exists some causal relationships which violate this assumption of conditional independence. A typical example lies in the PIIF SCM[4; 1], where the correlation \(X_{c}\to Y\to X_{i}\) breaks the assumption. However, we haven't identified a scenario in this work that adheres to such a setting.

In particular, the causal part \(X_{c}\) shares the causal correlation with \(Y\), represented as either \(X_{c}\to Y\) (which is assumed by many previous works), or \(Y\to X_{c}\) (which appears in our study), corresponding to the aforementioned data generating process \(X\to Y\) and \(Y\to X\).

### Details of the Shift Categories

Concretely, we classify the following distribution shifts based on their distinct data generation process between \(X\) and \(Y\) (specifically, the correlation between \(X_{c}\) and \(Y\)) as well as how the domain variable \(D\) affects individual variables like \(X_{i},X_{c}\) or \(Y\). Note that we follow the well-established definitions of these shifts and further extend the definitions to what we present in our work based on our established data model to better align with the application scenarios we propose.

**Covariate Shift**[26] is initially defined as \(\mathbb{P}_{\mathcal{S}}(X)\neq\mathbb{P}_{\mathcal{T}}(X)\) and \(\mathbb{P}_{\mathcal{S}}(Y|X)=\mathbb{P}_{\mathcal{T}}(Y|X)\). In the context of our work, the data generating process of \(X\to Y\) induces the assumption of covariate and concept shifts, and covariate shift holds if \(\mathbb{P}_{\mathcal{S}}(X)\neq\mathbb{P}_{\mathcal{T}}(X)\) and \(\mathbb{P}_{\mathcal{S}}(Y|X_{c})=\mathbb{P}_{\mathcal{T}}(Y|X_{c})\), which are

Figure 5: SCMs of covariate, concept, \(\mathcal{I}\)-conditional, and \(\mathcal{C}\)-conditional shifts.

achieved by the variable \(D\) exclusively impacting \(X\) without affecting \(Y\), as shown in Fig. 4(a). Note that we do not further specify which part of \(X\), \(X_{c}\) or \(X_{i}\), is impacted by the variable \(D\). This is in line with real-world scenarios where the specific shifts in \(\mathbb{P}(X_{c})\) or \(\mathbb{P}(X_{i})\) may not be apparent, such as in our instances of the scaffold shift and size shift based on the DrugOOD-3D dataset. Therefore, we present both \(D\to X_{c}\) and \(D\to X_{i}\) in Fig. 4(a) for simplicity.

**Concept Shift**[52, 21] is initially formalized as \(\mathbb{P}_{S}(Y|X)\neq\mathbb{P}_{\mathcal{T}}(Y|X)\). Under our formulation, concept shift holds if \(\mathbb{P}_{\mathcal{S}}(Y|X_{c})\neq\mathbb{P}_{\mathcal{T}}(Y|X_{c})\). This is characterized by the correlation \(D\to Y\gets X_{c}\), as shown in Fig. 4(b), which means the label \(Y\) is determined collectively by the input causal part \(X_{c}\) and the domain variable \(D\), and more importantly, there is a change of the labeling rule \(h\) across domains \(\mathcal{S}\) and \(\mathcal{T}\). Note that in our study we do not further assume if the shift in \(\mathbb{P}(X)\) exists. In the fidelity shift, DFT methods with varying fidelity levels calculate band gap values of the same set of MOFs, where we consider that \(\mathbb{P}(X)\) remains invariant across domains, while the assay shift, also categorized as concept shift, may involve the shift in \(\mathbb{P}(X)\). So we do not explicitly present the correlation between \(D\) and \(X_{i}\) or \(X_{c}\) in Fig. 4(b).

**Conditional Shift**, as proposed in [106], is induced by the data generating process of \(Y\to X\) and holds if \(\mathbb{P}_{S}(X|Y)\neq\mathbb{P}_{\mathcal{T}}(X|Y)\). We follow this formulation in our work. Note that \(Y\to X\) aligns well with the scenario of the Track dataset, where the simulated physical event \(X\) is controlled by multiple parameters, including one representing the label \(Y\) as positive or negative. The conditional distribution could be decomposed into two distinct parts based on the data model of \(X_{i}\perp\!\!\!\perp Y|X_{c}\): \(\mathbb{P}(X|Y)=\mathbb{P}(X_{c}|Y)\mathbb{P}(X_{i}|X_{c},Y)=\mathbb{P}(X_{c} |Y)\mathbb{P}(X_{i}|X_{c})\), which serves as the basis to further categorize conditional shift into the two following sub-types.

* \(\mathcal{I}\)**-Conditional Shift** holds if \(\mathbb{P}_{\mathcal{S}}(X_{i}|X_{c})\neq\mathbb{P}_{\mathcal{T}}(X_{i}|X_{c})\) and \(\mathbb{P}_{\mathcal{S}}(X_{c}|Y)=\mathbb{P}_{\mathcal{T}}(X_{c}|Y)\). As shown in Fig. 4(c), the domain variable \(D\) exclusively affects the independent part \(X_{i}\), _i.e._, \(D\to X_{i}\). In this case, only the conditional distribution \(\mathbb{P}(X_{i}|X_{c})\) changes across domains \(\mathcal{S}\) and \(\mathcal{T}\). Note that there does not exist the causal link of \(X_{i}\to X_{c}\) in this scenario to hold the assumption of \(X_{i}\perp\!\!\!\perp Y|X_{c}\), so the distribution \(\mathbb{P}(X_{c}|Y)\) will not be indirectly influenced by \(D\) and thus keeps invariant across the domains.
* \(\mathcal{C}\)**-Conditional Shift** holds if \(\mathbb{P}_{\mathcal{S}}(X_{i}|X_{c})=\mathbb{P}_{\mathcal{T}}(X_{i}|X_{c})\) and \(\mathbb{P}_{\mathcal{S}}(X_{c}|Y)\neq\mathbb{P}_{\mathcal{T}}(X_{c}|Y)\). As shown in Fig. 4(d), the domain variable \(D\) exclusively affects the causal part \(X_{c}\), which forms the structure of \(Y\to X_{c}\gets D\), representing the distribution of \(X_{c}\) is determined by both \(Y\) and \(D\). That means only the conditional distribution \(\mathbb{P}(X_{c}|Y)\) changes across the domains \(\mathcal{S}\) and \(\mathcal{T}\) while the distribution \(\mathbb{P}(X_{i}|X_{c})\) keeps invariant.

**Discussions**. In terms of covariate shift and concept shift, our extended formulations, compared to the initial definitions, put greater emphasis on the analysis of \(\mathbb{P}(Y|X_{c})\) rather than \(\mathbb{P}(Y|X)\), due to the assumption of conditional independence in our data model. Such extension is beneficial because the underlying rationale or causal correlation, often rooted in well-established scientific rules or theories like Density Functional Theory (DFT), holds significant importance for scientific discovery and ML4S. Therefore, it deserves more attention.

### Additional Comparisons with Distribution Shifts in Related Studies

Here we conduct additional comparisons with some distribution shifts which have been proposed by related works.

**Concept Shift.**[27] and [100] also involved shift cases explicitly formalized as concept shift. However, we claim it operates under a different mechanism than the one formalized in our study. In our work, concept shift particularly denotes the change in causal correlation between \(X_{c}\) and \(Y\), _i.e._, the shift in \(\mathbb{P}(Y|X_{c})\). This definition aligns perfectly with a real-world scenario of fidelity shift observed in materials science. In contrast, in GOOD's context [27], concept shift corresponds to changes in statistical rather than causal correlation. For instance, it may involve correlations between color and digit in datasets like ColoredMNIST [27, 14].

**Covariate Shift.** Diversity shift [100], low-data drift [89], and unseen data shift [89] can be explicitly formalized as covariate shift. However, our work does not aim to conduct a more find-grained analysis within a single shift category. Instead, our goal is to formalize and categorize diverse distribution-shift mechanisms across various scientific application scenarios.

## Appendix B Preliminaries for Geometric Deep Learning

**Notations**. We consider a geometric data sample \(g=(\mathcal{V},\mathbf{X},\mathbf{r})\), where \(\mathcal{V}=\{v_{1},\cdots,v_{n}\}\) is a set of points with the size \(n\), \(\mathbf{X}\in\mathbb{R}^{n\times m}\) denotes as \(m\)-dimensional point features, and \(\mathbf{r}\in\mathbb{R}^{n\times d}\) denotes as \(d\)-dimensional spacial coordinates of points. We specifically focus on 3D coordinates of scientific data in our study, _i.e._, \(d=3\). We build the GDL model \(\hat{y}=f(g;\Theta)\) to predict the ground-truth label \(y\) of data \(g\), where \(y\) is categorical for classification tasks and continuous for regression tasks. The model in our study consists of two parts, _i.e._, \(f=\omega\circ\Phi\), including the GDL component \(\Phi\), which is based on multiple GDL layers, and the MLP component \(\omega\), which gives the final prediction. And we hope the GDL models maintain strong predictive performance even when \(g\) is drawn from a distribution differing from the one during training, which motivates our study.

**Pipelines**. Here we present _how GDL backbones handle geometric data_ in this study. Given \(N\) samples of \(\{g_{i}\}_{i=1}^{N}\), we begin with constructing a \(k\)-nn graph for each data entry based on the spacial distances, _i.e._, \(\|\mathbf{r}_{v}-\mathbf{r}_{u}\|_{2}\) between any pair of points \(u,v\in\mathcal{V}\), where \(k\) is a hyperparameter. The GDL model then iteratively updates the representation of the point \(v\) via aggregation \(\text{AGG}_{u\in\mathcal{N}(v)}(\mathbf{m}_{uv})\), where AGG denotes as the aggregation operator (_e.g._, \(\sum\) or max), \(\mathcal{N}(v)\) denotes as the neighbors of point \(v\) in the \(k\)-nn graph, and \(\mathbf{m}_{uv}\) denotes as the message passing from the point \(u\) to \(v\). The GDL models typically need to capture geometric properties (_e.g._, invariance properties), and this has caused GDL models to often process geometric features carefully. Beyond basic spatial coordinates, the GDL models that achieve the _invariance merit_ often incorporate relative geometric information between points into the message \(\mathbf{m}_{uv}\), such as distance [72], angle [25], torsion [51], and rotation angle [83] information. Note that the selected backbones in this study only involve the distance information. Investigation into how capturing higher-order geometric information (such as certain kinds of angles with special scientific meanings) affects the generalization ability of GDL models remains a topic for future work. Also, we refer interested readers to [16] for more detailed descriptions of different types of GDL models.

After several GDL layers, there is a pooling operator used to aggregate all point representations, to obtain the representation of the geometric data. Then an additional MLP component is needed to generate the predicted labels.

## Appendix C Details of Datasets

### Dataset Statistics

The statistics of the covered datasets are shown in Table 4. Strategies of domain splits and sub-group splits for each distribution shift scenario, which have been discussed in Sec. 3.2, are detailed in Table

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline
**Dataset** & **Shift** & **Shift Case** & **Train-ID** & **Val-ID** & **Test-ID** & **OOD** & **Val-OOD** & **Test-OOD** \\ \hline \multirow{2}{*}{Track-Pileup} & \multirow{2}{*}{Pileup} & PU50 & \multirow{2}{*}{14814/15634} & \multirow{2}{*}{2469/2605} & \multirow{2}{*}{2470/2607} & \multirow{2}{*}{10000/10000} & \multirow{2}{*}{2500/2500} & \multirow{2}{*}{2500/2500} \\  & & PU90 & & & & & & & 7700/7700 & 2500/2500 & 2500/2500 \\ \hline \multirow{3}{*}{Track-Signal} & \multirow{3}{*}{Signal} & \(\tau\to 3\mu\) & & & & & & & 12000/15000 & 1975/2500 & 1977/2500 \\  & & \(z_{10}^{\prime}\to 2\mu\) & & & & & & & 12000/15000 & 1975/2500 & 1977/2500 \\  & & \(z_{10}^{\prime}\to 2\mu\) & & & & & & 12000/15000 & 1975/2500 & 1977/2500 \\ \hline \multirow{2}{*}{QHOF} & \multirow{2}{*}{Fidelity} & HSE06 & 10781 & 1796 & 1798 & 6000 & 2000 & 2000 \\  & & HSE06* & 10781 & 1796 & 1798 & 6000 & 2000 & 2000 \\ \hline DrugGOD-3D-Assay & Assay & lhpc-core+i0-assay & 29060/3861 & 9611/1295 & 99451/1232 & 3327/14687 & 1709/1957 & 1527/1310 \\ DrugGOD-3D-Size & Size & lhpc-core+i50-size & 32686/2542 & 10872/857 & 11003/846 & 26426/6921 & 14657/2706 & 11769/4215 \\ DrugGOD-3D-Scaffold & Scaffold & lhpc-core+i50-scaffold & 19455/1116 & 4473/211 & 26670/3015 & 30389/8824 & 1620/20678 & 14369/4146 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Dataset statistics for each dataset and distribution shift scenario. We evaluate the ID performance of models using Val-ID and Test-ID Datasets, and the OOD performance using Val-OOD and Test-OOD Datasets. The “OOD” column in this table presents the total number of OOD data entries whose features (but not labels) are used in the O-Feature level. Note that this table does not include statistics of Train-OOD that is specifically used for fine-tuning models in the Par-label level as mentioned in Sec. 4.1, because we utilize a fixed number of 100, 500, 1000 labels in this case, corresponding to \(\mathrm{TL}_{100}\), \(\mathrm{TL}_{500}\), \(\mathrm{TL}_{1000}\) baselines respectively. For bi-classification tasks, we list the number of positive data points (left) and negative data points (right), separated by “\(\prime\)”.

6. Note that for distribution shifts in DrugOOD-3D, we follow the same dataset splits and sub-group splits as the original benchmark DrugOOD. But in the Par-Label setting, we split 1000 samples from both Val-OOD and Test-OOD datasets, create the Train-OOD dataset, sample a specific number (100, 500, and 1000) for model fine-tuning, and evaluate the OOD performance of the fine-tuned models on the remaining OOD data. We also ensure a fair comparison here, as the number of removed samples is significantly smaller than the size of the OOD dataset itself. In the following three sub-sections, we make a complementary introduction to the studied scientific datasets.

Besides, we provide more granular information that better reflects the characteristics of the constructed datasets and distribution shifts. The detailed statistics can be seen in Table 5, covering

* The average number of tracks for each pileup level in Pileup Shift (Track-Pileup Dataset). Note that a higher PU level results in more background particle tracks in the collision while keeping the signal particle track the same.
* The average signal radius of each type of signal in Signal Shift (Track-Signal Dataset). Note that from \(z\to 2\mu\), \(z^{\prime}_{20}\to 2\mu\), \(z^{\prime}_{10}\to 2\mu\), to \(\tau\to 3\mu\), the average radius of signal tracks progressively approaches 2724.96 (the average radius of background tracks), which means it is getting harder to distinguish signals from backgrounds.
* The average number of atoms for different domains in Size Shift (DrugOOD-3D-Size Dataset).
* The average band gap value for each fidelity level in Fidelity Shift (QMOF Dataset). Note that the distinction between these fidelity levels extends beyond the mean of bandgap values. Specifically, the distribution of calculated band gap values displays varying properties across different levels, as illustrated in Fig. 0(c).

### Track Dataset

Here we employ the term _event_ to refer to the comprehensive recording of an entire physics process by an experiment [74]. As mentioned in Sec. 3.2.1, a signal event (labeled as _positive_) involves the existence of a particular decay of interest (_i.e._, signal). Here we are interested in multiple types of signals, including \(z\to\mu\mu\), \(\tau\to\mu\mu\mu\) (which have been widely observed), and \(z^{\prime}_{K}\to\mu\mu\) (which is a theoretical possibility) decays. This motivates us to construct the signal shift, where we expect the models trained with multiple types of signals to generalize to new signals that are different but to some extent related to the seen types. _Invariant mass_ is a crucial physical quantity that characterizes the distinct decay type. Specifically, when ranked from the largest to the smallest, \(z\to\mu\mu\) has an invariant mass of 91.19 GeV, \(z^{\prime}_{K}\to\mu\mu\) (where we consider \(K=80,70,60,50\) for model training and \(K=10,20\) for evaluation of model generalizability) has an invariant mass of \(K\) GeV, and \(\tau\to\mu\mu\mu\) has an invariant mass of 1.777 GeV. In our study, the disparities in invariant mass manifest through changes in the momenta of the signal particles and the radii of signal tracks (tracks left by signal particles). In the \(z\to\mu\mu\) decay, the expected radius of the signal tracks is significantly

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \multicolumn{4}{c}{1) **Pileup Shift** — the average number of tracks} \\ \hline Domain & PU-10 & PU-50 & PU-90 & \\ \#Tracks & 55.76 & 232.58 & 408.38 & \\ \hline \hline \multicolumn{4}{c}{2) **Signal Shift** — the average radius of signal tracks} \\ \hline Domain & \(\tau\to 3\mu\) & \(z^{\prime}_{10}\to 2\mu\) & \(z^{\prime}_{20}\to 2\mu\) & \(z\to 2\mu\) \\ \#Radius & 3979.66 & 8754.34 & 16014.98 & 58092.27 \\ \hline \hline \multicolumn{4}{c}{3) **Size Shift** — the average number of atoms} \\ \hline Domain & Domain-8 & Domain-37 & Domain-95 & Domain-157 & Domain-157 \\ \#Atoms & 25 & 46 & 105 & 276 \\ \hline \hline \multicolumn{4}{c}{4) **Fidelity Shift** — the average bandgap value} \\ \hline Domain & PBE & HLE17 & HSE06* & HSE06 \\ \#Bandgap & 2.09 & 2.68 & 2.95 & 3.86 \\ \hline \hline \end{tabular}
\end{table}
Table 5: More granular information that better reflects the characteristics of the constructed datasets and distribution shifts.

larger, making it easily distinguishable from the background tracks, while in the \(\tau\to\mu\mu\mu\) decay, the expected radius of the signal tracks is very close to that of the background tracks.

All events are simulated using the PYTHIA generator [6] with the addition of soft QCD pileup events, and particle tracks are generated using Acts [2]. Each point in a data entry is associated with a 3D coordinate, as well as other physical quantities measured by detectors, such as momenta. However, we use a dummy feature with all ones as the point feature for model training, following [55]. The model takes 3D coordinates and the dummy features of each point in data as input and predicts the existence of the signal in the given data.

### QMOF Dataset

We obtain 3D coordinates of each point in the materials data via the DFT-optimized structures provided by the QMOF Database. For point features, we associate each point in a sample with a categorical feature indicating the atom type for model training. The model takes 3D coordinates and atom-type categorical features as input and predicts the band gap value of given materials data.

### DrugOOD-3D Dataset

We first present how we adapt DrugOOD [35] and perform the GDL tasks over the dataset. We pre-process the SMILES [87] string of data provided in the dataset via the RDKit package [42], generating a conformer for each molecule, so as to assign each atom with a 3D coordinate. Concretely, we begin with generating a molecular object based on the SMILES string. Then we add hydrogens to the molecule and employ the ETKDG method [64] to obtain the initial conformer, which is further refined using the MMFF94 force field [29]. Note that we drop a data entry if it fails in conformer generation after the above process. The model takes 3D coordinates and atom-type categorical features as input, which is analogous to the scenario of the QMOF dataset, and predicts the binding affinity values of given ligands in a form of the binary classification task, as mentioned in Sec. 3.2.3.

### License

For the newly created Track-Pileup and Track-Signal datasets, we've got permission from the HEP community and utilized Acts to create them. Acts is licensed under the Mozilla Public License Version 2.0. Others are collected from public datasets and can be found at QMOF and DrugOOD. The data underlying the QMOF Database is made publicly available under a CC BY 4.0 license. DrugOOD is licensed under the GNU GENERAL PUBLIC LICENSE 3.0.

Also, note that the data we are using and curating do not contain any personally identifiable information or offensive content.

\begin{table}
\begin{tabular}{c c c c c c c} \hline
**Dataset** & **Shift** & **Shift Case** & \(\mathcal{S}\)**-Component** & **[Sub-groups]** & **Criterion** & \(\mathcal{T}\)**-Component** \\ \hline \multirow{2}{*}{Track-Pileup} & \multirow{2}{*}{Pileup} & \multirow{2}{*}{Pileup} & \multirow{2}{*}{Pileup} & \multirow{2}{*}{Pileup} & \multirow{2}{*}{5} & \multirow{2}{*}{The number of points} & \multirow{2}{*}{\(\tau\to 3\mu\)} \\  & & & & & & & \\  & & & & & & & \\  & & & & & & \\ \hline \multirow{3}{*}{Track-Signal} & \multirow{3}{*}{Signal} & \multirow{3}{*}{Signal} & \(\tau\to 3\mu\) & Mixed Signals: \(\tau\to 3\mu\) & Mixed Signals: \(\tau\to 80\mu,0.0,50,60,70\) & \multirow{3}{*}{5} & \multirow{3}{*}{The signal type} & \multirow{3}{*}{\(\tau\to 2\mu\)} \\  & & & & & & & \\  & & & & & & & \\  & & & & & & & \\  & & & & & & & \\ \hline \multirow{3}{*}{QOF} & \multirow{3}{*}{Fidelity} & \multirow{3}{*}{HSEBO*} & Mixed Fidelity: **PBE**, HLE17, HSD6* & \multirow{3}{*}{3} & \multirow{3}{*}{The fidelity level} & \multirow{3}{*}{\(\tau\to 2\mu\)} \\  & & & & & & & \\  & & & & & & & \\ \hline DrugOOD-30-Assay & Assay & Bug-core-x50-assay & \multirow{3}{*}{Following DrugOOD} & \multirow{3}{*}{91} & \multirow{3}{*}{The molecular size} & \multirow{3}{*}{Following DrugOOD} \\  & & & & & & & \\  & & & & & & \\ \hline \multirow{3}{*}{DrugOOD-30-Searfield} & \multirow{3}{*}{Searfield} & \multirow{3}{*}{Inb-core-x50-scaffield} & \multirow{3}{*}{6462} & \multirow{3}{*}{The softfold pattern} & \multirow{3}{*}{Following DrugOOD} \\  & & & & & & & \\ \hline \end{tabular}
\end{table}
Table 6: The criteria of domain splits and sub-group splits in each distribution shift scenario. The “\(\mathcal{S}\)-Component” and “\(\mathcal{T}\)-Component” columns provide a description of the composition of the data in the domains \(\mathcal{S}\) and \(\mathcal{T}\). We denote the number of sub-group splits in the source domain \(\mathcal{S}\) as \(|\)Sub-groups\(|\). The criterion of the sub-group splits for each scenario is also summarized in the “Criterion” column.

Details of Algorithms and Backbones

### Backbone Details

Our benchmark contains **3** backbones which have been widely used in scenarios of geometric deep learning. Here we give detailed descriptions for each backbone in this study as follows.

* **DGCNN** (Dynamic Graph CNN), introduced by [85], is a GDL architecture aimed at exploiting local geometric structures of geometric data while maintaining permutation invariance. Specifically, it constructs a local neighborhood graph and applies edge convolution, with dynamic graph updates after each layer of the network.
* **Point Transformer**[110] is an architecture applying self-attention networks to 3D point cloud processing. It is built based on a highly expressive Point Transformer layer, which is invariant to permutation and cardinality of geometric data.
* **EGNN** (\(E(n)\) Equivariant Graph Neural Networks), proposed by [71], is an architecture that preserves equivariance to rotations, translations and reflections on the coordinates of points when handling GDL data, _i.e._, \(E(n)\) equivariance, and that also preserves equivariance to permutations on the set of points.

**Discussions**. It is noteworthy that any specific application indicated by the datasets may have more advanced model architectures than these three architectures. We choose the above three as they are the most general, applicable to diverse scientific application scenarios, and most cutting-edge architectures are built upon them.

### Algorithm Details

Our benchmark contains **11** baselines spanning the No-Info, O-Feature, and Par-Label levels. We group them according to their distinct learning strategies and provide detailed descriptions for each algorithm as follows. We use \(\bullet\) to represent algorithms from the No-Info level, \(\dagger\) for O-Feature, and \(\ddagger\) for the Par-Label level, respectively.

* _Vanilla_: The empirical risk minimization (ERM) [82] minimizes the sum of errors across all samples.
* _Subgroup robustness_: Group distributionally robust optimization (GroupDRO) [68] aims to minimize worst-case losses and capture subgroup robustness by increasing the importance of groups with larger errors.
* _Invariant learning_: Variance Risk Extrapolation (VREx) [40] captures group invariance by specifically minimizing the risk variances of training domains.
* _Augmentation_: Mixup [105] improves model generalization by linearly interpolating two training samples randomly drawn from the training distribution. We follow [84] to perform Mixup specifically in the embedding space for the classification of geometric data.
* _Causal Inference_: DIR [91] captures the causal rationales for graph-structured data, mainly by conducting interventional augmentation on training data to create multiple interventional distributions, and then filtering out the parts of data that are unstable for model predictions. It is a well-known graph-based OOD baseline.
* _Information bottleneck_: LRI [55] is a novel geometric deep learning strategy grounded on a variational objective derived from the principle of information bottleneck. It injects learnable randomness to each node of geometric data, aimed at capturing minimal sufficient information to make correct and stable predictions. We adopt its _LRI-Bernoulli_ framework, which specifically injects Bernoulli randomness to each point.
* _Domain Invariance for Unsupervised Domain Adaptation_: Domain-Adversarial Neural Network (DANN) [22] encourages feature representations to be consistent across the source and the target domain by adversarially training the normal label predictor and a special domain classifier; Deep correlation alignment (DeepCoral) [75] also encourages domain invariance by penalizing the deviation of covariance matrices between the source and the target domain.

* _Vanilla Fine-tuning_: We fine-tune all parameters of the GDL model using a small amount of OOD data, after it has been pre-trained on ID data via the ERM algorithm. Specifically, we conduct 3 baselines here, fine-tuning the model using 100, 500, and 1000 labeled target samples, respectively.

**Discussions**. Here we explain the rationale behind the selection of methods in this benchmark.

Firstly, the baselines need to include diverse learning strategies, as listed above. This means that if multiple methods fundamentally adopt similar ideas or strategies, we will select the most representative one among them.

Secondly, to our best knowledge, there are no OOD baselines specifically designed for scientific GDL, except LRI which assesses OOD generalization performance in its paper. To build this benchmark, it is necessary to extend general-purpose and foundational methodologies to the GDL setting. Therefore, the selected baselines cover 1) general-purpose methods, which can be applied to various scenarios such as CV, GraphML, and GDL; 2) graph-specific methods, applicable to GraphML but also feasible in GDL; and 3) GDL-specific methods, feasible only in GDL. That is why DIR and LRI are essential components of our selected baselines.

## Appendix E Motivation and Insights from Experimental Design

Since we aim to understand how different Info levels and their associated algorithms affect the model generalizability across different application scenarios, it is important to carefully design experimental settings for a fair comparison among the three Info levels. As introduced in Sec 4.1,

* In the training stage, across the three levels, the model is trained (or pre-trained in Par-Label level) on the _same_ amount of ID data, and gains additional access to some OOD features (in O-Feature level) or a few OOD features & labels (in Par-Label level). In this way, we demonstrate the effect of extra OOD info on model generalizability given that the factor of ID data is controlled invariant across the three levels.
* In the evaluation stage, across the three levels, we evaluate the model's ID performance using the _same_ Val-ID and Test-ID datasets, and its OOD performance using the _same_ Val-OOD and Test-OOD datasets, for a fair evaluation.

We consider the following cases: If additional OOD information does not improve generalization performance, it is unnecessary to consider the corresponding Info level. Conversely, if it enhances model generalizability, we need to further evaluate the practicality of utilizing this Info level and its associated algorithms by assessing: 1) the expected performance gain from the additional OOD information, and 2) the costs related to acquiring such info. Regarding point 1), the potential of extra OOD info to enhance generalization performance relies on the underlying mechanism of the distribution shift, as analyzed in Section 4.2 and 4.3. This requires evaluating the type and mechanism of the studied distribution shift using domain-specific knowledge. For point 2), we need to assess the availability and the cost of collecting some labeled or unlabeled OOD data in the considered application.

Accordingly, we recommend three steps to GDL practitioners in handling distribution shift issues: 1) Assess the type of shift by leveraging domain-specific knowledge; 2) Assess the availability of collecting some labeled or unlabeled OOD data; 3) Select the suitable Info level and its associated method by considering the trade-off between the expenses of gathering extra OOD information (dependent on step2) and the expected performance gain resulting from such info (dependent on step1). Therefore, our experimental design and results provide practitioners with insights for making the most suitable choice in handling scientific distribution shift issues.

## Appendix F Details of Experimental Implementation

We conduct experiments on **3** scientific datasets and **10** cases of distribution shifts, covering **3** GDL backbones and **11** baselines from **3** knowledge levels. We implement our codes based on PyTorch Geometric [18]. We provide details of experimental implementation as follows.

**Basic Setup**. For all the experiments, we use the Adam optimizer, with a learning rate of 1e-3 and a weight decay of 1e-5. For each backbone, we use a fixed setting across various scenarios, all with the sum global pooling and the RELU activation function. The settings of batch size, maximum number of epochs, and the number of iterations per epoch for the O-Feature level are consistent across different algorithms for a fair comparison in this study. Details are shown in Table 8. Note that the batch size is set to 128 instead of 256 in the O-Feature level of the pileup shift due to the memory constraints, and maximum number of epochs is set to 75 in the pileup shift because the model has been trained to converge under this setting.

**Hyperparameter Tuning**. For each knowledge level and each algorithm, we search from a set of one specific hyperparameter to tune, and select the optimal one based on Val-OOD metric scores for a fair comparison. For VREx, we tune the weight of its variance penalty loss; For GroupDRO, we tune the Exponential coefficient; For Mixup, we tune the probability value that a certain batch data performs mixup augmentation; For DIR, we tune the causal ratio for selecting causal edges; For LRI, we tune the weight of the KL divergence regularizer; For DANN, we tune the weight of the domain classification loss; For DeepCoral, we tune the weight of covariance penalty loss. We detail the search space for each hyperparameter in Table 7.

## Appendix G Complete Experimental Results

Here we present complementary baseline results that are not shown in the main text due to space limit in Table 9, 10, 11, and 12.

## Appendix H Complementary Analysis of Experimental Results

In addition to representative analysis shown in the main text (Sec 4.2 and 4.3), here we present complementary analysis (observations, conclusions, and conjectures) of experimental results to provide further insights to the community.

\(\bullet\)**H1**. _Complementary to General Findings in Sec. 4.2_

We observe that OOD generalization methods in No-Info level find it hard to improve generalizability across various applications, which implies that the assumptions adopted by these methods may be kind of strong and not really match practical scenarios. Therefore, we recommend that future studies pay attention to 1) collecting some data information from the target domain \(\mathcal{T}\) if possible, and 2) proposing novel OOD methods based on assumptions that better match the scientific applications.

\(\bullet\)**H2**. _Complementary to Conclusion 1 in Sec. 4.3_

Based on the distinctions between Pileup and Signal shifts outlined in Conclusion 1, we propose two plausible explanations for the failure of DA strategies in Pileup Shift. One explanation lies in their distinct mechanisms (Sec. 3.2.1): Pileup Shift represents \(\mathcal{I}\)-Conditional Shift, a shift occurring

\begin{table}
\begin{tabular}{c c c} \hline \hline Algorithm & Hyperparameter & Search Space \\ \hline VREx & Penalty Weight & \(\{0.001,0.01,0.1,1.0\}\) \\ GroupDRO & Exponential Coefficient & \(\{0.001,0.01,0.1,1.0\}\) \\ Mixup & Probability & \(\{0.25,0.5,0.75,1.0\}\) \\ DIR & Causal Ratio & \(\{0.3,0.4,0.5\}\) \\ LRI & Information Loss Coefficient & \(\{0.01,0.1,1.0,10.0\}\) \\ DANN & Domain Loss Weight & \(\{0.001,0.01,0.1,1.0,5.0\}\) \\ DeepCoral & Penalty Weight & \(\{0.001,0.01,0.1\}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Hyperparameters search space for all algorithms.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{Salit} & \multirow{2}{*}{No-Info} & \multicolumn{3}{c}{O-Feature} & \multicolumn{3}{c}{Par-Label} \\ \cline{3-10}  & & & Batch Size & \# Max Epochs & Batch Size & \# Max Epochs & \# Iterations per Epoch & Batch Size & \# Max Epochs \\ \hline \multirow{2}{*}{Task} & Pileup & 256 & 200 & 128 & 200 & 150 & 256 & 75 \\  & Signal & 256 & 100 & 256 & 100 & 150 & 256 & 100 \\ \hline QMOF & Fidelity & 256 & 100 & 256 & 100 & 150 & 256 & 100 \\ \hline DrugOD-3D & Size, Scaffold, and Assay & 256 & 100 & 256 & 100 & 150 & 256 & 100 \\ \hline \hline \end{tabular}
\end{table}
Table 8: General hyperparameters of the datasets in this study.

exclusively in the independent part, _i.e._\(\mathbb{P}(X_{i}|X_{c})\). Therefore, the OOD features, unlike in Signal Shift, cannot provide sufficient information to guide model predictions in the target domain.

Another conjecture is that, in Pileup Shift, the variation occurs in the number of tracks instead of geometric characteristics (such as the curvature of signal tracks in Signal Shift). Despite sensitive to geometric properties, the GDL model may struggle to handle the variation of such non-geometric information, which makes it more challenging to align the representation space between the source domain \(\mathcal{S}\) and the target domain \(\mathcal{T}\), when given the additional OOD feature info.

We are currently unable to control either of these two disparitiy factors for further exploration because it would destroy the inherent scientific implications of the Track-Pileup dataset. Therefore, we leave more investigation to future work.

\(\bullet\)**H3**. _Complementary to Conclusion 1 in Sec. 4.3_

The DANN algorithm (and its corresponding O-Feature Level) shows a lower performance gain in the case of \(\tau\to 3\mu\) compared to \(z_{10}^{\prime}\to 2\mu\) (_cf._ Fig. 4), although they are both categorized as \(\mathcal{C}\)-Conditional Shift. We explain this disparity by analyzing the geometric characteristics of the data causal component \(X_{c}\). As shown in Table 5, the average radius of signal tracks in the \(\tau\to 3\mu\) decay is very close to that of the background tracks, which means the signal and background tracks share very similar curvature in this case. Therefore, distinguishing signals from backgrounds is much more challenging in the case of \(\tau\to 3\mu\), even when the model has access to the feature information of the \(\tau\to 3\mu\) event in O-Feature Level.

\(\bullet\)**H4**. _Complementary to Conclusion 2 in Sec. 4.3_

Additionally, we observe that TL strategies cannot yield a large improvement over ERM in Assay Shift (_cf._ Table 10), which is another scenario of Concept Shift (_cf._ Sec. 3.2.3). Following the analysis in Conclusion 2, a plausible explanation lies in how \(\mathbb{P}_{\mathcal{S}}(Y)\) and \(\mathbb{P}_{\mathcal{T}}(Y)\) varies in this case: Although there is a large divergence in \(\mathbb{P}(Y)\) between different assay subgroups (_cf._ Fig. 5(a)), the distribution \(\mathbb{P}(Y)\) is quite similar between the source and target domain (_cf._ Fig. 5(b)), _i.e._\(\mathbb{P}_{\mathcal{S}}(Y)\approx\mathbb{P}_{\mathcal{T}}(Y)\), which stands in contrast to the scenarios of the scaffold shift and size shift shown in Fig. 5(c) and 5(d).

However, to provide a comprehensive answer to this question, it's crucial to consider other factors as well. For example, we simplify the affinity prediction by following DrugOOD, transitioning it to a binary classification task. Besides, the mechanism of the assay shift, unlike the fidelity shift scenario, may involve more than just a mismatch of label distribution \(\mathbb{P}(Y)\) but could also entail a substantial shift in the input distribution \(\mathbb{P}(X)\) between the domains \(\mathcal{S}\) and \(\mathcal{T}\). This can pose a significant challenge, particularly when the amount of labeled OOD data is limited. We leave a further in-depth analysis of this case to future work.

## Appendix I Comparisons with Previous Findings

During result analysis, we identify conclusions that align with existing OOD literature and others that are novel and specific to the GDL setting. We conduct comparisons, including both consistency and disparity, between our observations and conclusions and those from previous findings related to OOD, such as in CV tasks.

### Comparison -- Consistency

* In Sec. 4.2, we observe that fine-tuning can sometimes result in negative effects when the labeled OOD data is quite limited, particularly in cases involving a smaller degree of distribution shifts. This is consistent with [38], where fine-tuning a large model based on a small set of labels may lead to catastrophic forgetting. To mitigate this issue, robust fine-tuning strategies, such as weight-space ensembles [90], regularization [94] and surgical fine-tuning [44], could be potential solutions.
* In **H1** of Appendix H, we observe that multiple OOD generalization methods in our No-Info level find it hard to provide significant improvement across various applications. We can find consistent observations in existing works [14, 28, 39].
* In Sec. 4.3, we conclude that, "For the OOD generalization methods to learn robust representations, the more informatively the groups obtained by splitting the source domain \(\mathcal{S}\) indicate the distribution shift, the better performance these methods may achieve." This is consistent with [13], which revealed the importance of appropriate subgroup partitioning for invariant learning.
* Some works focus on leveraging additional auxiliary variables for OOD generalization. [92] used auxiliary information to help improve OOD performance in a semi-supervised scenario. [48] recently proposed to leverage such additional variables to encode information about the latent distribution shift, and to jointly learn group splits and invariant representation. How to leverage these auxiliary variables to enhance OOD generalization is an interesting topic for the GDL settings.

### Comparison -- Disparity

* Conclusion 1 in Sec. 4.3 reveals how an algorithm can achieve superior performance by analyzing shifts in the geometric or non-geometric characteristics of the causal (\(X_{c}\)) or non-causal (\(X_{i}\)) components in geometric data. Such an analysis is tailored for GDL settings.
* The second point in Sec. I.1 could be even more severe in GDL compared to CV tasks considering the intricate nature of irregularity and geometric prior (information on the structure space and symmetry properties like invariance or equivariance) inherent in geometric data.

Besides, certain shifts in scientific GDL are infrequent or even unique in CV. This indicates the challenges faced by several methods initially proposed for CV tasks in addressing these shifts, and the necessity to develop OOD methods specifically designed for scientific GDL. We list some examples in our work as follows.

* Size shift, despite categorized as covariate shift, is a unique case where the model trained in data with lower size is to generalize to data with larger size. Methods designed for CV might struggle to capture this mechanism, potentially explaining why several methods do not perform well in the context of size shift in our study.
* Fidelity shift, which indicates the variation in the causal correlation between \(X_{c}\) and \(Y\), poses a challenge in material property prediction. However, such a shift in \(\mathbb{P}(Y|X_{c})\) is rare in CV tasks because \(Y\) is typically derived from human annotations based on the input \(X\). Therefore, most methods in our benchmark struggle to handle this type of shift, with the exception of the pretraining-finetuning strategy.

[MISSING_PAGE_FAIL:28]

[MISSING_PAGE_FAIL:29]

[MISSING_PAGE_FAIL:30]