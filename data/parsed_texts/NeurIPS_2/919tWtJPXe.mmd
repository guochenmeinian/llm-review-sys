# Self-supervised Object-Centric Learning for Videos

Gorkay Aydemir\({}^{1}\) Weidi Xie\({}^{3,4}\) Fatma Guney\({}^{1,2}\)

\({}^{1}\) Department of Computer Engineering, Koc University \({}^{2}\) KUIS AI Center

\({}^{3}\) CMIC, Shanghai Jiao Tong University \({}^{4}\) Shanghai AI Laboratory

{gaydemir23, fguney}@ku.edu.tr weidi@sjtu.edu.cn

###### Abstract

Unsupervised multi-object segmentation has shown impressive results on images by utilizing powerful semantics learned from self-supervised pretraining. An additional modality such as depth or motion is often used to facilitate the segmentation in video sequences. However, the performance improvements observed in synthetic sequences, which rely on the robustness of an additional cue, do not translate to more challenging real-world scenarios. In this paper, we propose the first fully unsupervised method for segmenting multiple objects in real-world sequences. Our object-centric learning framework spatially binds objects to slots on each frame and then relates these slots across frames. From these temporally-aware slots, the training objective is to reconstruct the middle frame in a high-level semantic feature space. We propose a masking strategy by dropping a significant portion of tokens in the feature space for efficiency and regularization. Additionally, we address over-clustering by merging slots based on similarity. Our method can successfully segment multiple instances of complex and high-variety classes in YouTube videos.1

Footnote 1: Project page: [https://kuis-ai.github.io/solv](https://kuis-ai.github.io/solv)

## 1 Introduction

Given a video sequence of a complex scene, our goal is to train a vision system that can discover, track, and segment objects, in a way that abstracts the visual information from millions of pixels into semantic components, _i.e._, objects. This is commonly referred to as object-centric visual representation learning in the literature. By learning such abstractions of the visual scene, the resulting object-centric representation acts as fundamental building blocks that can be processed independently and recombined, thus improving the model's generalization and supporting high-level cognitive vision tasks such as reasoning, control, _etc._[13, 29].

The field of object-centric representation learning in computer vision has made significant progress over the years, starting from synthetic images [34, 37], and has since shifted towards in-the-wild image [20, 49] and real-world videos [65, 85, 87, 46, 60]. In general, existing approaches typically follow an autoencoder training paradigm [52, 26], _i.e._, reconstructing the input signals with certain bottlenecks, with the hope to group the regional pixels into semantically meaningful objects based on the priors of architecture or data. In particular, for images, low-level features like color, and semantic features from pretrained deep networks, are often used to indicate the assignment of pixels to objects, while for videos, additional modalities/signals are normally incorporated, such as optical flow [40], depth map [17], with segmentation masks directly available from the discontinuities. Despite being promising, using additional signals in videos naturally incurs computation overhead and error accumulation, for example, optical flow may struggle with static or deformable objects and large displacements between frames, while depth may not be readily available in generic videos and its estimation can suffer in low-light or low-contrast environments.

In this paper, we introduce SOLV, a self-supervised model capable of discovering multiple objects in real-world video sequences without using additional modalities [2; 36; 40; 17] or any kind of weak supervision such as first frame initialization [40; 17]. To achieve this, we adopt axial spatial-temporal slot attentions, that first groups spatial regions within frame, then followed by enriching the slot representations using additional cues from interactions with neighboring frames. We employ masked autoencoder (MAE)-type training, with the objective of reconstructing dense visual features from masked inputs. This approach has two benefits: first, it acts as an information bottleneck, forcing the model to learn the high-level semantic structures, given only partial observations; second, it alleviates memory constraints, which helps computational efficiency. Additionally, due to the complexity of visual scenes, a fixed number of slots often leads to an over-segmentation issue. We address this issue by merging similar slots with a simple clustering algorithm. Experimentally, our method significantly advances the state-of-the-art without using information from additional modalities on a commonly used synthetic video dataset, MOVi-E [25] and a subset of challenging Youtube-VIS 2019 [87] with in-the-wild videos (Fig. 1).

To summarize, we make the following contributions: (i) We propose a self-supervised multi-object segmentation model on real-world videos, that uses axial spatial-temporal slots attention, effectively grouping visual regions with similar property, without relying on any additional signal; (ii) We present an object-centric learning approach based on masked feature reconstruction, and slot merging; (iii) Our model achieves state-of-the-art results on the MOVi-E and Youtube-VIS 2019 datasets, and competitive performance on DAVIS2017.

## 2 Related Work

Object-centric Learning:The field of unsupervised learning of object-centric representations from images and videos has gained considerable attention in recent years. Several approaches have been proposed to address this problem with contrastive learning [31; 41; 84] or more recently with reconstruction objectives. An effective reconstruction-based approach first divides the input into a set of region identifier variables in the latent space, _i.e._ slots, that bind to distinctive parts corresponding to objects. Slot attention has been applied to both images [26; 5; 10; 12; 18; 50; 89; 19; 27; 15; 52; 69] and videos [35; 71; 28; 40; 33; 75; 90; 43]. However, these methods are typically evaluated on synthetic data and struggle to generalize to real-world scenarios due to increasing complexity. To address this challenge, previous works explore additional information based on the 3D structure [8; 58; 32] or reconstruct different modalities such as flow [40] or depth [17]. Despite these efforts, accurately identifying objects in complex visual scenes without explicit guidance remains an open challenge. The existing work relies on guided initialization from a motion segmentation mask [1; 2] or initial object locations [40; 17]. To overcome this limitation, DINOSAUR [67] performs reconstruction in the feature space by leveraging the inductive biases learned by recent self-supervised models [7]. We also follow this strategy which has proven highly effective in learning object-centric representations in real-world data without any guided initialization or explicit supervision.

Object Localization from DINO Features:The capabilities of Vision Transformers (ViT) [14] have been comprehensively investigated, leading to remarkable findings when combined with self-supervised features from DINO [7]. By grouping these features with a traditional graph partitioning method [57; 68; 81], impressive results can be achieved compared to earlier approaches [76; 77].

Figure 1: We introduce SOLV, an object-centric framework for instance segmentation in real-world videos. This figure depicts instance segmentation results of first, middle, and last frames of videos on Youtube-VIS 2019 [87]. Without any supervision both in training and inference, we manage to segment object instances accurately.

Recent work, CutLER [80], extends this approach to segment multiple objects with series of normalized cuts. The performance of these methods without any additional training shows the power of self-supervised DINO features for segmentation and motivates us to build on it in this work.

**Video Object Segmentation:** Video object segmentation (VOS) [86; 4; 22; 23; 42; 38; 44; 56; 59; 61; 72; 78; 79; 88] aims to identify the most salient object in a video without relying on annotations during evaluation in unsupervised setting [73; 21; 47; 53] and only annotation of the first frame in semi-supervised setting [6]. Even if the inference is unsupervised, ground-truth annotations can be used during training in VOS [11; 16; 63; 54; 6]. Relying on labelled data during training might introduce a bias towards the labelled set of classes that is available during training. In this paper, we follow a completely unsupervised approach without using any annotations during training or testing.

Motion information is commonly used in unsupervised VOS to match object regions across time [36; 9; 51; 66]. Motion cues particularly come in handy when separating multiple instances of objects in object-centric approaches. Motion grouping [86] learns an object-centric representation to segment moving objects by grouping patterns in flow. Recent work resorts to sequential models while incorporating additional information [70; 40; 17; 1]. In this work, we learn temporal slot representations from multiple frames but we do not use any explicit motion information. This way, we can avoid the degradation in performance when flow cannot be estimated reliably.

## 3 Methodology

In this section, we start by introducing the considered problem scenario, then we detail the proposed object-centric architecture, that is trained with self-supervised learning.

### Problem Scenario

Given an RGB video clip as input, _i.e._, \(\mathcal{V}_{t}=\left\{\mathbf{v}_{t-n},\ldots,\mathbf{v}_{t},\ldots\mathbf{v }_{t+n}\right\}\in\mathbb{R}^{(2n+1)\times H\times W\times 3}\), our goal is to train an object-centric model that processes the clip, and outputs all object instances in the form of segmentation masks, _i.e._, discover and track the instances in the video, via _self-supervised learning_, we can formulate the problem as :

\[\mathbf{m}_{t}=\Phi\left(\mathcal{V}_{t};\Theta\right)=\Phi_{\text{vis-dec}} \circ\Phi_{\text{st-bind}}\circ\Phi_{\text{vis-enc}}\left(\mathcal{V}_{t}\right) \tag{1}\]

where \(\mathbf{m}_{t}\in\mathbb{R}^{K_{t}\times H\times W}\) refers to the output segmentation mask for the middle frame with \(K_{t}\) discovered objects. After segmenting each frame, we perform Hungarian matching to track the objects across frames in the video. \(\Phi\left(\cdot;\Theta\right)\) refers to the proposed segmentation model that will be detailed in the following section. Specifically, it consists of three core components (see Fig. 2), namely, visual encoder that extracts frame-wise visual features (Section 3.2.1), spatial-temporal axial binding that first groups pixels into slots within frames, then followed by joining the slots across temporal frames (Section 3.2.2), and visual decoder that decodes the spatial-temporal slots to reconstruct the dense visual features, with the segmentation masks of objects as by-products (Section 3.2.3).

### Architecture

Generally speaking, our proposed architecture is a variant of Transformer, that can be trained with simple masked autoencoder, _i.e._, reconstructing the full signals given partial input observation. However, unlike standard MAE [30] that recovers the image in pixel space, we adopt an information bottleneck design, that first assigns spatial-temporal features into slots, then reconstructs the dense visual features from latent slots. As a result, each slot is attached to one semantically meaningful object, and the segmentation masks can be obtained as by-products from reconstruction, _i.e._, without relying on manual annotations.

#### 3.2.1 Visual Encoder

Given the RGB video clip, we divide each image into regular non-overlapping patches. Following the notation introduced in Section 3.1, _i.e._, \(\mathcal{V}_{t}=\left\{\mathbf{v}_{t-n},\ldots,\mathbf{v}_{t},\ldots\mathbf{v }_{t+n}\right\}\in\mathbb{R}^{(2n+1)\times N\times(3P^{2})}\), where \(N=HW/P^{2}\) is the number of tokens extracted from each frame with patches of size \(P\). The visual encoder consists of token drop and feature extraction, as detailed below.

**Token Drop:** As input to the encoder, we only sample a subset of patches. Our sampling strategy is straightforward: we randomly drop the input patches with some ratio for each frame,

\[\mathcal{V}^{\prime}_{t}=\left\{\mathbf{v}^{\prime}_{t-n},\ldots,\mathbf{v}^{ \prime}_{t+n}\right\}=\left\{\mathrm{drop}\left(\mathbf{v}_{t-n}\right),\ldots, \mathrm{drop}\left(\mathbf{v}_{t+n}\right)\right\}\in\mathbb{R}^{(2n+1)\times N ^{\prime}\times(3P^{2})},\quad N^{\prime}<N \tag{2}\]

where \(N^{\prime}\) denotes the number of tokens after random sampling.

**Feature Extraction:** We use a frozen Vision Transformer (ViT) [14] with parameters initialized from DINov2 [62], a self-supervised model that has been pretrained on a large number of images:

\[\mathcal{F}=\left\{\mathbf{f}_{t-n},\ldots,\mathbf{f}_{t+n}\right\}=\left\{ \phi_{\text{DINO}}\left(\mathbf{v}^{\prime}_{t-n}\right),\ldots,\phi_{\text{ DINO}}\left(\mathbf{v}^{\prime}_{t+n}\right)\right\}\in\mathbb{R}^{(2n+1)\times N ^{\prime}\times D} \tag{3}\]

where \(D\) refers to the dimension of output features from the last block of DINov2, right before the final layer normalization.

**Discussion:** Our design of token drop serves two purposes: _Firstly_, masked autoencoding has been widely used in natural language processing and computer vision, acting as a proxy task for self-supervised learning, our token drop can effectively encourage the model to acquire high-quality visual representation; _Secondly_, for video processing, the extra temporal axis brings a few orders of magnitude of more data, processing sparsely sampled visual tokens can substantially reduce memory budget, enabling computation-efficient learning, as will be validated in our experiments.

#### 3.2.2 Spatial-temporal Binding

After extracting visual features for each frame, we first spatially group the image regions into slots, with each specifying a semantic object, _i.e_., discover objects within single image; then, we establish the temporal bindings between slots with a Transformer, _i.e_., associate objects within video clips.

\[\Phi_{\text{st-bind}}\left(\mathcal{F}\right)=\psi_{\text{t-bind}}\left(\psi_{ \text{s-bind}}\left(\mathbf{f}_{t-n}\right),\ldots,\psi_{\text{s-bind}}\left( \mathbf{f}_{t+n}\right)\right)\in\mathbb{R}^{K\times D_{\text{int}}} \tag{4}\]

**Spatial Binding (\(\psi_{\text{s-bind}}\)):** The process of spatial binding is applied to each frame **independently**. We adopt the invariant slot attention proposed by Biza et al. [3], with one difference, that is, we use a shared initialization \(\mathcal{Z}_{\tau}\) at each time step \(\tau\in\left\{t-n,\ldots,t+n\right\}\). Specifically, given features after token drop at a time step \(\tau\) as input, we learn a set of initialization vectors to translate and scale the input position encoding for each slot separately with \(K\) slot vectors \(\mathbf{z}^{j}\in\mathbb{R}^{D_{\text{int}}}\), \(K\) scale vectors

Figure 2: **Overview. In this study, we introduce SOLV, an autoencoder-based model designed for object-centric learning in videos. Our model consists of three components: (i) Visual Encoder for extracting features for each frame using \(\phi_{\text{DINO}}\); (ii) Spatial-temporal Binding module for generating temporally-aware object-centric representations by binding them spatially and temporally using \(\psi_{\text{s-bind}}\) and \(\psi_{\text{b-bind}}\), respectively; (iii) Visual Decoder for estimating segmentation masks and feature reconstructions for the central frame with \(\psi_{\text{dec}}\), after merging similar slots using \(\psi_{\text{merge}}\)**\(\mathbf{S}^{j}_{s}\in\mathbb{R}^{2}\), \(K\) position vectors \(\mathbf{S}^{j}_{p}\in\mathbb{R}^{2}\), and one absolute positional embedding grid \(\mathbf{G}_{\text{abs}}\in\mathbb{R}^{N\times 2}\). We mask out the patches corresponding to the tokens dropped in feature encoding (Section 3.2.1) and obtain absolute positional embedding for each frame \(\tau\) as \(\mathbf{G}_{\text{abs},\tau}=\operatorname{drop}\left(\mathbf{G}_{\text{abs}} \right)\in\mathbb{R}^{N^{\prime}\times 2}\). Please refer to the original paper [3] or the Supplementary Material for details of invariant slot attention on a single frame. This results in the following set of learnable vectors for each frame:

\[\mathcal{Z}_{\tau}=\left\{\left(\mathbf{z}^{j},\mathbf{S}^{j}_{s},\mathbf{S}^{ j}_{p},\mathbf{G}_{\text{abs},\tau}\right)\right\}_{j=1}^{K} \tag{5}\]

Note that these learnable parameters are shared for all frames and updated with respect to the dense visual features of the corresponding frame. In other words, slots of different frames start from the same representation but differ after binding operation due to interactions with frame features. The output of \(\psi_{\text{s-bind}}\) is the updated slots corresponding to independent object representations at frame \(\tau\):

\[\{\mathbf{z}^{j}_{\tau}\}_{j=1}^{K}=\psi_{\text{s-bind}}\left(\mathbf{f}_{\tau }\right)\in\mathbb{R}^{K\times D_{\text{abs}}},\quad\tau\in\{t-n,\dots,t+n\} \tag{6}\]

In essence, given that consecutive frames typically share a similar visual context, the use of learned slots inherently promotes temporal binding, whereby slots with the same index can potentially bind to the same object region across frames. Our experiments demonstrate that incorporating invariant slot attention aids in learning slot representations that are based on the instance itself, rather than variable properties like size and location.

**Temporal Binding (\(\psi_{\text{t-bind}}\)):** Up until this point, the model is only capable of discovering objects by leveraging information from individual frames. This section focuses on how to incorporate temporal context to enhance the slot representation. Specifically, given the output slots from the spatial binding module, denoted as \(\left\{\{\mathbf{z}^{j}_{t-n}\}_{j=1}^{K},\dots,\{\mathbf{z}^{j}_{t+n}\}_{j=1 }^{K}\right\}\in\mathbb{R}^{(2n+1)\times K\times D_{\text{abs}}}\), we apply a transformer encoder to the output slots with same index across different frames. In other words, the self-attention mechanism only computes a \((2n+1)\times(2n+1)\) affinity matrix across \(2n+1\) time steps. This approach helps to concentrate on the specific region of the image through time. Intuitively, each slot learns about its future and past in the temporal window by attending to each other, which helps the model to create a more robust representation by considering representations of the same object at different times.

To distinguish between time stamps, we incorporate learnable temporal positional encoding onto the slots, _i.e._, all slots from a single frame receive the same encoding. As the result of temporal transformer, we obtain the updated slots \(\mathbf{c}\) at the target time step \(t\):

\[\mathbf{c}=\Phi_{\text{st-bind}}\left(\mathcal{F}\right)\in\mathbb{R}^{K\times D _{\text{abs}}} \tag{7}\]

#### 3.2.3 Visual Decoder

The spatial-temporal binding process yields a set of slot vectors \(\mathbf{c}\in\mathbb{R}^{K\times D_{\text{abs}}}\) at target frame \(t\) that are aware of temporal context. However, in natural videos, the number of objects within a frame can vary significantly, leading to over-clustering when a fixed number of slots is used, _i.e._, as we can always initialize slots in an over-complete manner.

To overcome this challenge, we propose a simple solution for slot merging through Agglomerative Clustering. Additionally, we outline the procedure for slot decoding to reconstruct video features, which resembles a masked auto-encoder in feature space.

\[\Phi_{\text{vis-dec}}\left(\mathbf{c}\right)=\psi_{\text{dec}}\circ\psi_{ \text{merge}}\left(\mathbf{c}\right) \tag{8}\]

**Slot Merging (\(\psi_{\text{merge}}\)):** As expected, the problem of object segmentation is often a poorly defined problem statement in the self-supervised scenario, as there can be multiple interpretations of visual regions. For instance, in an image of a person, it is reasonable to group all person pixels together or group the person's face, arms, body, and legs separately. However, it is empirically desirable for pixel embeddings from the same object to be closer than those of different objects. To address this challenge, we propose to merge slots using Agglomerative Clustering (AC), which does not require a predefined number of clusters. As shown in Fig. 3, we first compute the affinity matrix between all slots based on cosine similarity, then use this affinity matrix to cluster slots into groups, and compute the mean slot for each resulting cluster:

\[\mathbf{c}^{\prime}=\psi_{\text{merge}}\left(\mathbf{c}\right)\in\mathbb{R}^{ K_{t}\times D_{\text{abs}}},\quad\quad\quad K_{t}\leq K \tag{9}\]

[MISSING_PAGE_FAIL:6]

recent works such as Karazija et al. [36], Bao et al. [2], and Seitzer et al. [67]. This approach also allows us to compare to state-of-the-art image-based segmentation methods, such as DINOSAUR [67]. For real-world datasets, we use the mean Intersection-over-Union (mIoU) metric, which is widely accepted in segmentation, by considering only foreground objects. To ensure temporal consistency of assignments between frames, we apply Hungarian Matching between the predicted and ground-truth masks of foreground objects in the video, following the standard practice [65].

### Results on Synthetic Data: MOVi-E

The results on the MOVi-E dataset are presented in Table 1, with methods divided based on whether they use an additional modality. For example, the sequential extensions of slot attention, such as SAVi [40] and SAVi++ [17], reconstruct different modalities like flow and sparse depth, respectively, their performance falls behind other approaches despite additional supervision. On the other hand, STEVE [69] improves results with a transformer-based decoder for reconstruction. PPMP [36] significantly improves performance by predicting probable motion patterns from an image with flow supervision during training. MoTok [2] outperforms other methods, but relies on motion segmentation masks to guide the attention maps of slots during training, with a significant drop in performance without motion segmentation. The impressive performance of DINOSAUR [67] highlights the importance of self-supervised training for segmentation. Our method stands out by using the spatial-temporal slot binding, and autoencoder training in the feature space, resulting in significantly better results than all previous work (see Fig. 5).

### Results on Real Data: DAVIS17 and Youtube-VIS 2019

Due to a lack of multi-object segmentation methods evaluated on real-world data, we present a baseline approach that utilizes spectral clustering on the DINOv2 features [62]. To define the number of clusters, we conduct an oracle test by using the ground-truth number of objects. After obtaining masks independently for each frame, we ensure temporal consistency by applying Hungarian Matching between frames based on the similarity of the mean feature of each mask. However, as shown in Table 2, directly clustering the DINOv2 features produces poor results despite the availability of privileged information on the number of objects.

\begin{table}
\begin{tabular}{l c|c} \hline \hline Model & +Modality & FG-ARI \(\uparrow\) \\ \hline SAVi [40] & Flow & 39.2 \\ SAVi++ [17] & Sparse Depth & 41.3 \\ PPMP [36] & Flow & 63.1 \\ MoTok [2] & Motion Seg. & 66.7 \\ \hline MoTok [2] & & 52.4 \\ STEVE [70] & - & 54.1 \\ DINOSAUR [67] & - & 65.1 \\ Ours & & **80.8** \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Quantitative Results on MOVi-E.** This table shows results in comparison to the previous work in terms of FG-ARI on MOVi-E.

\begin{table}
\begin{tabular}{l c|c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Supervision} & \multicolumn{2}{c}{DAVIS17} & \multicolumn{2}{c}{YTVIS19} \\ \cline{3-5}  & & mIoU \(\uparrow\) & FG-ARI \(\uparrow\) & mIoU \(\uparrow\) & FG-ARI \(\uparrow\) \\ \hline DINOv2 [62] + Clustering & \#Objects & 14.34 & 24.19 & 28.17 & 16.19 \\ OCLR [83] & Flow & **34.60** & 14.67 & 32.49 & 15.88 \\ Ours & - & 30.16 & **32.15** & **45.32** & **29.11** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Quantitative Results on Real-World Data.** These results show the video multi-object evaluation results on the validation split of DAVIS17 and a subset of the YTVIS19 train split.

Figure 5: **Qualitative Results on MOVi-E.**

We also compare our method to the state-of-the-art approach OCLR [83], which is trained using a supervised objective on synthetic data with ground-truth optical flow. OCLR [83] slightly outperforms our method on DAVIS17, where the optical flow can often be estimated accurately, that greatly benefits the segmentation by allowing it to align with the real boundaries, resulting in better mIoU performance. However, our method excels at detecting and clustering multiple objects, even when the exact boundaries are only roughly located, as evidenced by the higher FG-ARI. The quality of optical flow deteriorates with increased complexity in YTVIS videos, causing OCLR [83] to fall significantly behind in all metrics. The qualitative comparison in Fig. 6 demonstrates that our method can successfully segment multiple objects in a variety of videos. Additional qualitative results can be found in the Supplementary Material.

### Ablation Study

**On the Effectiveness of Architecture:** We conducted an experiment to examine the impact of each proposed component on the performance. _Firstly_, we replaced the spatial binding module, \(\psi_{\text{s-bind}}\), with the original formulation in Slot Attention [52]. _Secondly_, we removed the temporal binding module, \(\psi_{\text{t-bind}}\), which prevents information sharing between frames at different time steps. _Finally_, we eliminated the slot merging module, \(\psi_{\text{merge}}\).

We can make the following observations from the results in Table 3: (i) as shown by the results of Model-A, a simple extension of DINOSAUR [67] to videos, by training with DINOv2 features and matching mask indices across frames, results in poor performance; (ii) only adding slot merging (Model-B) does not significantly improve performance, indicating that over-clustering is not the primary issue; (iii) significant mIoU gains can be achieved with temporal binding (Model-C) or

\begin{table}
\begin{tabular}{c c|c c} \hline \hline \#Slots & Merging & mIoU \(\uparrow\) & FG-ARI \(\uparrow\) \\ \hline \multirow{2}{*}{6} & ✗ & 43.29 & 27.73 \\  & ✓ & 44.90 & 28.52 \\ \hline \multirow{2}{*}{8} & ✗ & 39.90 & 22.55 \\  & ✓ & **45.32** & **29.11** \\ \hline \multirow{2}{*}{12} & ✗ & 36.04 & 20.20 \\  & ✓ & 43.19 & 27.94 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Number of Slots.** The effect of varying the number of slots with/without slot merging.

Figure 6: **Qualitative Results of Multi-Object Video Segmentation on YTVIS19.** We provide the first, middle, and last frames along with their corresponding segmentation after the Hungarian Matching step. Our model is proficient in accurately identifying objects in their entirety, irrespective of their deformability or lack of motion. This capability is highlighted in contrast to OCLR [83], whose results are displayed in the third row for comparison.

spatial binding (Model-D), which shows the importance of spatially grouping pixels within each frame as well as temporally relating slots in the video. Finally, the best performance is obtained with Model-E in the last row by combining all components.

**Number of Slots:** We perform an experiment by varying the number of slots with and without slot merging in Table 4. Consistent with the findings in previous work [67], the performance is highly dependent on the number of slots, especially when slot merging is not applied. For example, without slot merging, increasing the number of slots leads to inferior results in terms of both metrics, due to the over-segmentation issue in some videos. While using slot merging, we can better utilize a larger number of slots as can be seen from the significantly improved results of 8 or 12 slots with slot merging. This is also reflected in FG-ARI, pointing to a better performance in terms of clustering. We set the number of slots to 8 and use slot merging in our experiments. We provide a visual comparison of the resulting segmentation of an image with and without slot merging in Fig. 4. Our method can successfully merge parts of the same object although each part is initially assigned to a different slot.

**Token Drop Ratio:** We ablated the ratio of tokens to drop in Fig. 7. As our motivation for the token drop is two-fold (Section 3.2.1), we show the effect of varying the token drop ratio on both performance and memory usage. Specifically, we plot mIoU on the vertical axis versus the memory consumption on the horizontal axis. We report the memory footprint as the peak in the GPU memory usage throughout a single pass with constant batch size. Fig. 7 confirms the trade-off between memory usage and performance. As the token drop ratio decreases, mIoU increases due to less information loss from tokens dropped. However, this comes at the cost of significantly more memory usage. We cannot report results by using all tokens, _i.e_. token drop ratio of 0, due to memory constraints. Interestingly, beyond a certain threshold, _i.e_. at 0.5, the token drop starts to act as regularization, resulting in worse performance despite increasing the number of tokens kept. We use 0.5 in our experiments, which reaches the best mIoU with a reasonable memory requirement. In summary, the token drop provides not only efficiency but also better performance due to the regularization effect.

**Visual Encoder:** We conducted experiments to investigate the effect of different visual encoders in Table 5. We adjust the resize values to maintain a consistent token count of 864 for different architectures. These findings underscore the critical role of pretraining methods. Please see Supplementary for a qualitative comparison. Notably, the DINov2 model [62] outperforms its predecessor, DINO [7], in terms of clustering efficiency. On the other hand, models pretrained using supervised methods results in the weakest performance, with a substantial gap in the FG-ARI metric when compared to self-supervised pretraining methods. Furthermore, no specific patch size offers a clear advantage within the same pretraining type. For instance, DINO pretraining with ViT-B/8 exhibits superior performance in the FG-ARI metric but lags behind its 16-patch-sized counterpart in the mIoU metric.

## 5 Discussion

We presented the first fully unsupervised object-centric learning method for scaling multi-object segmentation to in-the-wild videos. Our method advances the state-of-the-art significantly on commonly used simulated data but more importantly, it is the first fully unsupervised method to report state-of-the-art performance on unconstrained videos of the Youtube-VIS dataset.

This work takes a first step towards the goal of decomposing the world into semantic entities from in-the-wild videos without any supervision. The performance can always be improved with better

\begin{table}
\begin{tabular}{l|c c} \hline \hline Model & mIoU & FG-ARI \(\uparrow\) \\ \hline Supervised ViT-B/16 & 37.58 & 19.94 \\ DINO ViT-B/8 & 39.53 & 24.70 \\ DINO ViT-B/16 & 41.91 & 24.01 \\ DINov2 ViT-B/14 & **45.32** & **29.11** \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Visual Encoder**. The effect of different types of self-supervised pretraining methods and architectures for feature extraction.

features from self-supervised pretraining as shown in the comparison of different DINO versions. While we obtain significant performance boosts from self-supervised pretraining, it also comes with some limitations. For example, our method can locate objects roughly, but it fails to obtain pixel-accurate boundaries due to features extracted at patch level. Furthermore, nearby objects might fall into the same patch and cause them to be assigned to the same slot. Our slot merging strategy, although has been proved effective as a simple fix for the issue of over-segmentation, remains not differentiable. Future work needs to go beyond that assumption and develop methods that can adapt to a varying number of objects in a differentiable manner.

## 6 Acknowledgements

Weidi Xie would like to acknowledge the National Key R&D Program of China (No. 2022ZD0161400).

## References

* [1]Z. Bao, P. Tokmakov, A. Jabri, Y. Wang, A. Gaidon, and M. Hebert (2022) Discovering objects that can move. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Vol., pp. 2022. Cited by: SS1, SS2.
* [2]Z. Bao, P. Tokmakov, Y. Wang, A. Gaidon, and M. Hebert (2023) Object discovery from motion-guided tokens. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Vol., pp. 2023. Cited by: SS1, SS2.
* [3]O. Biza, S. van Steenkiste, M. S. Sajjadi, G. F. Elsayed, A. Mahendran, and T. Kipf (2023) Invariant slot attention: object discovery with slot-centric reference frames. In Proc. of the International Conf. on Machine learning (ICML), Cited by: SS1, SS2.
* [4]T. Brox and J. Malik (2010) Object segmentation by long term analysis of point trajectories. In Proc. of the European Conf. on Computer Vision (ECCV), Cited by: SS1, SS2.
* [5]C. P. Burgess, L. Matthey, N. Watters, R. Kabra, I. Higgins, M. Botvinick, and A. Lerchner (2019) Monet: unsupervised scene decomposition and representation. arXiv.org. Cited by: SS1, SS2.
* [6]S. Caelles, K. Maninis, J. Pont-Tuset, L. Leal-Taixe, D. Cremers, and L. Van Gool (2017) One-shot video object segmentation. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1, SS2.
* [7]M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin (2021) Emerging properties in self-supervised vision transformers. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), Cited by: SS1, SS2.
* [8]C. Chen, F. Deng, and S. Ahn (2021) Roots: object-centric representation and rendering of 3d scenes. In Journal of Machine Learning Research (JMLR), Cited by: SS1, SS2.
* [9]S. Choudhury, L. Karazija, I. Laina, A. Vedaldi, and C. Rupprecht (2022) A simple and powerful global optimization for unsupervised video object segmentation. In Proc. of the British Machine Vision Conf. (BMVC), Cited by: SS1, SS2.
* [10]E. Crawford and J. Pineau (2019) Spatially invariant unsupervised object detection with convolutional neural networks. In Proc. of the Conf. on Artificial Intelligence (AAAI), Cited by: SS1, SS2.
* [11]A. Dave, P. Tokmakov, and D. Ramanan (2019) Towards segmenting anything that moves. In Proc. of the IEEE International Conf. on Computer Vision (ICCV) Workshops, Cited by: SS1, SS2.
* [12]F. Deng, Z. Zhi, D. Lee, and S. Ahn (2021) Generative scene graph networks. In Proc. of the International Conf. on Learning Representations (ICLR), Cited by: SS1, SS2.
* [13]A. Dittadi, S. Papa, M. De Vita, B. Scholkopf, O. Winther, and F. L. (2022) Generalization and robustness implications in object-centric learning. In Proc. of the International Conf. on Machine learning (ICML), Cited by: SS1, SS2.
* [14]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. In Proc. of the International Conf. on Learning Representations (ICLR), Cited by: SS1, SS2.
* [15]Y. Du, S. Li, Y. Sharma, J. Tenenbaum, and I. Mordatch (2021) Unsupervised learning of compositional energy concepts. In Advances in Neural Information Processing Systems (NeurIPS), Cited by: SS1, SS2.
* [16]S. Dutt Jain, B. Xiong, and K. Grauman (2017) Fusionseg: learning to combine motion and appearance for fully automatic segmentation of generic objects in videos. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1, SS2.
* [17]G. Elsayed, A. Mahendran, S. van Steenkiste, K. Greff, M. C. Mozer, and T. Kipf (2022) Savit++: towards end-to-end object-centric learning from real-world videos. In Advances in Neural Information Processing Systems (NeurIPS), Cited by: SS1, SS2.

* [18] Martin Engelcke, Adam R Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative scene inference and sampling with object-centric latent representations. In _Proc. of the International Conf. on Learning Representations (ICLR)_, 2020.
* [19] SM Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton, et al. Attend, infer, repeat: Fast scene understanding with generative models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2016.
* [20] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. In _International Journal of Computer Vision (IJCV)_, 2010.
* [21] Alon Faktor and Michal Irani. Video segmentation by non-local consensus voting. In _Proc. of the British Machine Vision Conf. (BMVC)_, 2014.
* [22] Deng-Ping Fan, Wenguan Wang, Ming-Ming Cheng, and Jianbing Shen. Shifting more attention to video salient object detection. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2017.
* [23] Katerina Fragkiadaki, Geng Zhang, and Jianbo Shi. Video segmentation by tracing discontinuities in a trajectory embedding. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2012.
* [24] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In _Journal of Machine Learning Research (JMLR)_, 2010.
* [25] Klaus Greff, Francois Belleti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapragasam, Florian Gollemo, Charles Herrmann, et al. Kubric: A scalable dataset generator. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [26] Klaus Greff, Raphael Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with iterative variational inference. In _Proc. of the International Conf. on Machine learning (ICML)_, 2019.
* [27] Klaus Greff, Antti Rasmus, Mathias Berglund, Tele Hao, Harri Valpola, and Jurgen Schmidhuber. Tagger: Deep unsupervised perceptual grouping. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2016.
* [28] Klaus Greff, Sjoerd Van Steenkiste, and Jurgen Schmidhuber. Neural expectation maximization. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* [29] Klaus Greff, Sjoerd Van Steenkiste, and Jurgen Schmidhuber. On the binding problem in artificial neural networks. In _arXiv.org_, 2020.
* [30] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [31] Henaff, Olivier J and Koppula, Skanda and Shelhamer, Evan and Zoran, Daniel and Jaegle, Andrew and Zisserman, Andrew and Carreira, Joao and Arandjelovic, Relja. Object discovery and representation networks. In _Proc. of the European Conf. on Computer Vision (ECCV)_, 2022.
* [32] Paul Henderson and Christoph H Lampert. Unsupervised object-centric video generation and decomposition in 3d. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [33] Jindong Jiang, Sepehr Janghorbani, Gerard De Melo, and Sungjin Ahn. Scalor: Generative world models with scalable object representations. In _Proc. of the International Conf. on Learning Representations (ICLR)_, 2020.
* [34] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2017.
* [35] Rishabh Kabra, Daniel Zoran, Goker Erdogan, Loic Matthey, Antonia Creswell, Matt Botvinick, Alexander Lerchner, and Chris Burgess. Simone: View-invariant, temporally-abstracted object representations via unsupervised video decomposition. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2012.
* [36] Laurynas Karazija, Subhabrata Choudhury, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Unsupervised Multi-object Segmentation by Predicting Probable Motion Patterns. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [37] Laurynas Karazija, Iro Laina, and Christian Rupprecht. Clevrtex: A texture-rich benchmark for unsupervised multi-object segmentation. In _arXiv.org_, 2021.
* [38] Margret Keuper, Bioern Andres, and Thomas Brox. Motion trajectory segmentation via minimum cost multicuts. In _Proc. of the IEEE International Conf. on Computer Vision (ICCV)_, 2015.
* [39] Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _Proc. of the International Conf. on Learning Representations (ICLR)_, 2015.
* [40] Thomas Kipf, Gamaleldin F. Elsayed, Aravindh Mahendran, Sara Stone, Austinand Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional object-centric learning from video. In _Proc. of the International Conf. on Learning Representations (ICLR)_, 2022.
* [41] Thomas Kipf, Elise Van der Pol, and Max Welling. Contrastive learning of structured world models. In _Proc. of the International Conf. on Learning Representations (ICLR)_, 2020.

* [42] Yeong Jun Koh and Chang-Su Kim. Primary object segmentation in videos based on region augmentation and reduction. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2017.
* [43] Adam Kosiorek, Hyunjik Kim, Yee Whye Teh, and Ingmar Posner. Sequential attend, infer, repeat: Generative modelling of moving objects. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2018.
* [44] Zihang Lai, Erika Lu, and Weidi Xie. Mast: A memory-augmented self-supervised tracker. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2020.
* [45] Zihang Lai and Weidi Xie. Self-supervised learning for video correspondence flow. In _Proc. of the British Machine Vision Conf. (BMVC)_, 2019.
* [46] Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, and James M Rehg. Video segmentation by tracking many figure-ground segments. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2013.
* [47] Siyang Li, Bryan Seybold, Alexey Vorobyov, Alireza Fathi, Qin Huang, and C-C Jay Kuo. Instance embedding transfer to unsupervised video object segmentation. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2018.
* [48] Huaijin Lin, Ruizheng Wu, Shu Liu, Jiangbo Lu, and Jiaya Jia. Video instance segmentation with a propose-reduce paradigm. In _Proc. of the IEEE International Conf. on Computer Vision (ICCV)_, 2021.
* [49] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Proc. of the European Conf. on Computer Vision (ECCV)_, 2014.
* [50] Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, and Sungjin Ahn. Space: Unsupervised object-oriented scene representation via spatial attention and decomposition. In _Proc. of the International Conf. on Learning Representations (ICLR)_, 2020.
* [51] Runtao Liu, Zhirong Wu, Stella Yu, and Stephen Lin. The emergence of objectness: Learning zero-shot segmentation from videos. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [52] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [53] Xiankai Lu, Wenguan Wang, Chao Ma, Jianbing Shen, Ling Shao, and Fatih Porikli. See more, know more: Unsupervised video object segmentation with co-attention siamese networks. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2019.
* [54] Jonathon Luiten, Paul Voigtlaender, and Bastian Leibe. Premvos: Proposal-generation, refinement and merging for video object segmentation. In _Proc. of the Asian Conf. on Computer Vision (ACCV)_, 2018.
* [55] Jonathon Luiten, Idil Esen Zulfikar, and Bastian Leibe. Unovost: Unsupervised offline video object segmentation and tracking. In _Proc. of the IEEE Winter Conference on Applications of Computer Vision (WACV)_, 2020.
* [56] K.-K. Maninis, S. Caelles, Y. Chen, J. Pont-Tuset, L. Leal-Taixe, D. Cremers, and L Van Gool. Video object segmentation without temporal information. In _IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI)_, 2018.
* [57] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [58] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2021.
* [59] Peter Ochs and Thomas Brox. Object segmentation in video: a hierarchical variational approach for turning point trajectories into dense regions. In _Proc. of the IEEE International Conf. on Computer Vision (ICCV)_, 2011.
* [60] Peter Ochs, Jitendra Malik, and Thomas Brox. Segmentation of moving objects by long term video analysis. In _IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI)_, 2013.
* [61] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory networks. In _Proc. of the IEEE International Conf. on Computer Vision (ICCV)_, 2019.
* [62] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. In _arXiv.org_, 2023.
* [63] Anestis Papazoglou and Vittorio Ferrari. Fast object segmentation in unconstrained video. In _Proc. of the IEEE International Conf. on Computer Vision (ICCV)_, 2013.
* [64] Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, and David Cournapeau. Scikit-learn: Machine learning in Python. In _Journal of Machine Learning Research (JMLR)_, 2011.
** [65] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2016.
* [66] Georgy Poniantink, Nermin Samet, Yang Xiao, Yuming Du, Renaud Marlet, and Vincent Lepetit. A simple and powerful global optimization for unsupervised video object segmentation. In _Proc. of the IEEE Winter Conference on Applications of Computer Vision (WACV)_, 2022.
* [67] Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann Simon-Gabriel, Tong He, Zheng Zhang, Bernhard Scholkopf, Thomas Brox, et al. Bridging the gap to real-world object-centric learning. In _Proc. of the International Conf. on Learning Representations (ICLR)_, 2023.
* [68] Oriane Simeoni, Gilles Puy, Huy V Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Perez, Renaud Marlet, and Jean Ponce. Localizing objects with self-supervised transformers and no label. In _Proc. of the British Machine Vision Conf. (BMVC)_, 2021.
* [69] Gautam Singh, Fei Deng, and Sungjin Ahn. Illiterate dall-e learns to compose. In _Proc. of the International Conf. on Learning Representations (ICLR)_, 2022.
* [70] Gautam Singh, Yi-Fu Wu, and Sungjin Ahn. Simple unsupervised object-centric learning for complex and naturalistic videos. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [71] Karl Stelzner, Robert Peharz, and Kristian Kersting. Faster attend-infer-repeat with tractable probabilistic models. In _Proc. of the International Conf. on Learning Representations (ICLR)_, 2019.
* [72] Pavel Tokmakov, Cordelia Schmid, and Karteek Alahari. Learning to segment moving objects. In _International Journal of Computer Vision (IJCV)_, 2019.
* [73] Yi-Hsuan Tsai, Ming-Hsuan Yang, and Michael J Black. Video segmentation via object flow. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2016.
* [74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* [75] Rishi Veerapaneni, John D Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu, Joshua Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement learning. In _Proc. Conf. on Robot Learning (CoRL)_, 2020.
* [76] Huy V Vo, Patrick Perez, and Jean Ponce. Toward unsupervised, multi-object discovery in large-scale image collections. In _Proc. of the European Conf. on Computer Vision (ECCV)_, 2020.
* [77] Van Huy Vo, Elena Sizikova, Cordelia Schmid, Patrick Perez, and Jean Ponce. Large-scale unsupervised object discovery. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [78] Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, and Liang-Chieh Chen. Feelvos: Fast end-to-end embedding learning for video object segmentation. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2019.
* [79] Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, and Kevin Murphy. Tracking emerges by colorizing videos. In _Proc. of the European Conf. on Computer Vision (ECCV)_, 2018.
* [80] Xudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2023.
* [81] Yangtao Wang, Xi Shen, Shell Xu Hu, Yuan Yuan, James L Crowley, and Dominique Vaufreydaz. Self-supervised transformers for unsupervised object discovery using normalized cut. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [82] Nicholas Watters, Loic Matthey, Christopher P Burgess, and Alexander Lerchner. Spatial broadcast decoder: A simple architecture for learning disentangled representations in vaes. In _arXiv.org_, 2019.
* [83] Junyu Xie, Weidi Xie, and Andrew Zisserman. Segmenting moving objects via an object-centric layered representation. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [84] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [85] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. Youtube-vos: A large-scale video object segmentation benchmark. In _Proc. of the European Conf. on Computer Vision (ECCV)_, 2018.
* [86] Charig Yang, Hala Lamdouar, Erika Lu, Andrew Zisserman, and Weidi Xie. Self-supervised video object segmentation by motion grouping. In _Proc. of the IEEE International Conf. on Computer Vision (ICCV)_, 2021.
* [87] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2019.
* [88] Zhao Yang, Qiang Wang, Luca Bertinetto, Weiming Hu, Song Bai, and Philip HS Torr. Anchor diffusion for unsupervised video object segmentation. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2019.

* [89] Peiyu Yu, Sirui Xie, Xiaojian Ma, Yixin Zhu, Ying Nian Wu, and Song-Chun Zhu. Unsupervised foreground extraction via deep region competition. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [90] Daniel Zoran, Rishabh Kabra, Alexander Lerchner, and Danilo J Rezende. Parts: Unsupervised segmentation with slots, attention and independence maximization. In _Proc. of the IEEE International Conf. on Computer Vision (ICCV)_, 2021.

## Appendix A Broader Impact

We proposed an object-oriented approach that can be applied to videos of real-world environments. We trained our model on the Youtube-VIS 2019 dataset that includes videos of humans. The proposed work can be used to locate and segment multiple instances of a wide variety of objects including all kinds of animals and humans from videos. While progress in this area can be used to improve not only human life but also, for example, wildlife, we acknowledge that it could also inadvertently assist in the creation of computer vision applications that could potentially harm society.

## Appendix B Experiment Details

### Dataset Details

We eliminated the black borders for all videos in YTVIS19 dataset. Since we propose a self-supervised model, we merge the available splits on datasets for training. For all datasets except YTVIS, we use the available validation splits for evaluation. The annotations for the validation split are missing on the YTVIS, therefore, we use a subset of 300 videos from the train split for evaluation. We will share the indices of selected videos for future comparisons together with the code. For evaluation, we upsample the segmentation masks to match the resolution of the original input frames by using bilinear interpolation.

### Model Details

**Feature Extractor (\(\phi_{\text{DINO}}\)):** We use the ViT-B/14 architecture with DINOv2 pretraining [62] as our default feature extractor. Our feature vector is the output of the last block without the CLS token. We add positional embeddings to the patches and then drop the tokens.

**Spatial Binding(\(\psi_{\text{s-bind}}\)):** We project the feature tokens from \(\phi_{\text{DINO}}\) to slot dimension \(D_{\text{slot}}=128\), with a 2-layer-MLP, followed by layer normalization. Then the slots and the projected tokens are passed to the Invariant Slot Attention (ISA) (as detailed in Section C) as input. After slot attention, slots are updated with a GRU cell. Following the sequential update, slots are passed to a residual MLP with a hidden size of \(4\times D_{\text{slot}}\). All projection layers (\(p\), \(q\), \(k\), \(v\), \(g\)) have the same size as slots, _i.e_. \(D_{\text{slot}}\). We repeat the binding operation 3 times. We multiply the scale parameter \(\mathbf{S}_{s}\) by \(\delta=5\) to prevent relative grid \(\mathbf{G}_{\text{rel}}\) from containing large numbers.

We use the following initializations for the learnable parameters: \(\mathbf{G}_{\text{abs}}\), a coordinate grid in the range \([-1,1]\); slots \(\mathbf{z}\), Xavier initialization [24]; slot scale \(\mathbf{S}_{s}\) and slot position \(\mathbf{S}_{p}\), a normal distribution.

**Temporal Binding (\(\psi_{\text{t-bind}}\)):** We use a transformer encoder with 3 layers and 8 heads [74] for temporal binding. The hidden dimension of encoder layers is set to \(4\times D_{\text{slot}}\). We initialize the temporal positional embedding with a normal distribution. We masked the slots of not available frames, _i.e_. frames with indices that are either less than 0 or exceed the frame number, in transformer layers.

**Slot Merging (\(\psi_{\text{merge}}\)):** We use the implementation of Agglomerative Clustering in the sklearn library [64] with complete linkage. For each cluster, we compute the mean slot and the sum of attention matrices, _i.e_. matrix \(\mathbf{A}\) in (13), for the associated slots. We determine the scale \(\mathbf{S}_{s}\) and position \(\mathbf{S}_{p}\) parameters for the merged attention values to calculate the relative grid \(\mathbf{G}_{\text{rel}}\) of the newslots. Then, \(\mathbf{G}_{\text{rel}}\) is projected onto \(D_{\text{slot}}\) using a linear layer \(h\) and added to the broadcasted slots before decoding.

Decoder Mapper (\(\psi_{\text{mapper}}\)):The mapper \(\psi_{\text{mapper}}\) consists of 5 linear layers with ReLU activations and a hidden size of 1024. The final layer maps the activations to the dimension of ViT-B tokens with an extra alpha value _i.e._ 768 + 1.

### Training Details

In all our experiments, unless otherwise specified, we employ DINOv2 [62] with the ViT-B/14 architecture. We set the number of consecutive frame range \(n\) to 2 and drop half of the tokens before the slot attention step. We train our models on \(2\times\)V100 GPUs using the Adam [39] optimizer with a batch size of 48. We clip the gradient norms at 1 to stabilize the training. We match mask indices of consecutive frames by applying Hungarian Matching on slot similarity to provide temporal consistency. To prevent immature slots in slot merging, we apply merging with a probability that is logarithmically increasing through epochs.

Movi-E:We train our model from scratch for a total of 60 epochs, which is equivalent to approximately 300K iterations. We use a maximum learning rate of \(4\times 10^{-4}\) and an exponential decay schedule with linear warmup steps constituting 5% of the overall training period. The model is trained using 18 slots and the input frames are adjusted to a size of \(336\times 336\), leading to 576 feature tokens for each frame. The slot merge coefficient in \(\psi_{\text{merge}}\) is configured to \(0.12\).

Ytvis19:Similar to MOVi-E, we train the model from scratch for 180 epochs, corresponding to approximately 300K iterations with a peak learning rate of \(4\times 10^{-4}\) and decay it with an exponential schedule. Linear warmup steps are introduced for 5% of the training timeline. The model training involves 8 slots, and the input frames are resized to dimensions of \(336\times 504\), resulting in 864 feature tokens for each frame. The slot merge coefficient in \(\psi_{\text{merge}}\) is set to be \(0.12\).

Davis17:Due to the small size of DAVIS17, we fine-tune the model pretrained on the YTVIS19 dataset explained above. We finetune on DAVIS17 for 300 epochs, corresponding to approximately 40K iterations with a reduced learning rate of \(1\times 10^{-4}\). We use the same learning rate scheduling strategy as in YTVIS19. During the fine-tuning process, we achieve the best result without slot merging, likely due to the fewer number of objects, typically one object at the center, on DAVIS17 compared to YTVIS19.

## Appendix C Invariant Slot Attention

In this section, we provide the details of invariant slot attention (ISA), initially proposed by Biza et al. [3]. We use ISA in our Spatial Binding module \(\psi_{\text{s-bind}}\) with shared initialization as explained in the main paper. Given the shared initialization \(\mathcal{Z}_{\tau}=\left\{\left(\mathbf{z}^{j},\mathbf{S}_{s}^{j},\mathbf{S}_{ p}^{j},\mathbf{G}_{\text{abs},\tau}\right)\right\}_{j=1}^{K}\), our goal is to update the \(K\) slots: \(\left\{\mathbf{z}^{j}\right\}_{j=1}^{K}\). For clarification, in the following, we focus on the computation of single-step slot attention for time step \(\tau\):

\[\mathbf{A}^{j}:=\operatorname*{softmax}_{K}\left(\mathbf{M}^{j}\right)\in \mathbb{R}^{N^{\prime}},\qquad\mathbf{M}^{j}:=\frac{1}{\sqrt{D_{\text{slot}}}} p\bigg{(}k\left(\mathbf{f}_{r}\right)+g\left(\mathbf{G}_{\text{rel},\tau}^{j} \right)\bigg{)}q\left(\mathbf{z}^{j}\right)^{T}\in\mathbb{R}^{N^{\prime}} \tag{13}\]

where \(p\), \(g\), \(k\), and \(q\) are linear projections while the relative grid of each slot is defined as:

\[\mathbf{G}_{\text{rel},\tau}^{j}:=\frac{\mathbf{G}_{\text{abs},\tau}-\mathbf{ S}_{p}^{j}}{\mathbf{S}_{s}^{j}}\in\mathbb{R}^{N^{\prime}\times 2} \tag{14}\]

The slot attention matrix \(\mathbf{A}\) from (13) is used to compute the scale \(\mathbf{S}_{s}\) and the positions \(\mathbf{S}_{p}\) of slots following Biza et al. [3]:

\[\mathbf{S}_{s}^{j}:=\sqrt{\frac{\operatorname*{sum}\left(\mathbf{A}\odot \left(\mathbf{G}_{\text{abs},\tau}-\mathbf{S}_{p}^{j}\right)^{2}\right)}{ \operatorname*{sum}\left(\mathbf{A}^{j}\right)}}\in\mathbb{R}^{2},\qquad\mathbf{ S}_{p}^{j}:=\frac{\operatorname*{sum}\left(\mathbf{A}^{j}\odot\mathbf{G}_{\text{abs}, \tau}\right)}{\operatorname*{sum}\left(\mathbf{A}^{j}\right)}\in\mathbb{R}^{2} \tag{15}\]

After this step, following the original slot attention, input features are aggregated to slots using the weighted mean with another linear projection \(v\):

\[\mathbf{U}:=\mathbf{W}^{T}p\bigg{(}v\left(\mathbf{f}_{r}\right)+g\left(\mathbf{ G}_{\text{rel},\tau}^{j}\right)\bigg{)}\in\mathbb{R}^{K\times D_{\text{slot}}}, \qquad\mathbf{W}^{j}:=\frac{\mathbf{A}^{j}}{\operatorname*{sum}\left(\mathbf{A }^{j}\right)}\in\mathbb{R}^{N^{\prime}} \tag{16}\]Then, \(\mathbf{U}\) from (16) is used to update slots \(\{\mathbf{z}^{j}\}_{j=1}^{K}\) with GRU followed by an additional MLP as residual connection as shown in (17). This operation is repeated 3 times.

\[\mathbf{z}:=\mathbf{z}+\mathrm{MLP}\left(\left(\mathrm{norm}(\mathbf{z})\right), \right.\qquad\mathbf{z}:=\mathrm{GRU}\left(\mathbf{z},\mathbf{U}\right)\in \mathbb{R}^{K\times D_{\text{data}}} \tag{17}\]

## Appendix D Additional Ablation Studies

In this section, we provide additional experiments to show the effect of resolution on the segmentation quality. We also report the results by varying the evaluation split on the YTVIS to confirm the generalization capability of our model.

Effect of Resolution:We conducted experiments to investigate the effect of the input frame resolution, _i.e_. the number of input tokens, in Fig. 8. Specifically, we experimented with resolutions of \(168\times 252\), \(224\times 336\), and our default resolution \(336\times 506\), corresponding to 216, 384, and 864 input feature tokens, respectively. These experiments show that the input resolution is crucial for segmentation performance.

Varying The Evaluation Split:As stated before, we cannot use the validation split of YTVIS19 due to manual annotations that are not publicly available. For evaluation, we only choose a subset of 300 videos. Here, we perform an experiment to examine the effect of varying the set of evaluation videos on performance.

We repeat the experiment in Table 3 by choosing mutually exclusive subsets of 300 videos, 3 times. We report mean and standard deviation (\(\mu\pm\sigma\)) of experiments for each model in Table 6. These results are coherent with the reported result on the fixed subset, showing that segmentation performance peaks when all of our components are combined, _i.e_. model E. Similarly, removing our components \(\psi_{\text{b-bind}}\) (model D) and \(\psi_{\text{b-bind}}\) (model C) one at a time leads to a performance drop, as shown in Table 3. Removing both (model B), results in the worst performance, even falling behind its counterpart without slot merging (model A). Overall, the results of varying the evaluation set agree with the reported performance on the selected subset in the main paper. This shows that the performance of our model generalizes over different evaluation subsets.

## Appendix E Additional Comparisons

We compare the performance of our model to available supervised models, exploiting pre-trained object detectors on large data in a supervised manner, for unsupervised video object segmentation on DAVIS17 in Table 7. The results underscore a notable gap between supervised and unsupervised approaches.

\begin{table}
\begin{tabular}{l c c} \hline \hline Model & mIoU \(\uparrow\) & FG-ARI \(\uparrow\) \\ \hline A & 37.72 \(\pm\) 0.82 & 29.06 \(\pm\) 2.24 \\ B & 37.20 \(\pm\) 1.53 & 30.15 \(\pm\) 1.90 \\ C & 42.76 \(\pm\) 2.09 & 30.03 \(\pm\) 1.67 \\ D & 42.98 \(\pm\) 2.01 & 29.69 \(\pm\) 2.02 \\ E & 43.28 \(\pm\) 2.57Failure Analysis

Although our model can detect in-the-wild objects in different scales, segmentation boundaries are not perfectly aligned with the object due to patch-wise segmentation, as has been pointed out in the Discussion (Section 5). In addition to these observations, we identify three types of commonly occurring failure cases: (i) The over-clustering issue that remains unresolved in some cases even with slot merging. (ii) The tendency to cluster nearby instances of the same class into a single slot. (iii) Failure to detect small objects, particularly when they are situated near large objects. We provide visual examples of these cases in Fig. 9.

## Appendix G Additional Visualizations

We provide additional qualitative results in Fig. 10. Our model can recognize not only the most salient object in the middle but also small, subtle objects in the background. Furthermore, it can handle a varying number of objects in the scene with slot merging.

**Feature Extractor:** In Fig. 11, we provide visualizations of different feature extractors, corresponding to the quantitative evaluation in Table 5, including DINOv2 [62], DINO [7], and Supervised. Self-supervised models performs better than the supervised one, also qualitatively. In particular, DINOv2 stands out for its exceptional capability to learn object representations across a diverse range

Figure 9: Failure cases of our model with potential reasons, grouped into three.

of categories. It also effectively identifies and segments intricate details that are missed by other feature extractors such as tree branches on the left and the bag on the table on the right.

**Components:** In Fig. 12, we visually compare the segmentation results of the models corresponding to the component ablation study in the main paper (Table 3 ). First of all, model A, which corresponds to the temporally consistent DINOSAUR [67], struggles to cluster the instances as a whole and also fails to track all objects, for instance, the human on the right, due to discrepancies in mask index across three frames. With the help of slot merging, model B effectively addresses the over-clustering issue, as observed in the mask of the calf on the left. Integrating our binding modules \(\psi_{\text{b-bind}}\) and \(\psi_{\text{b-bind}}\), resulting in model C and D, respectively, leads to a marked improvement in both segmentation and tracking quality. On the other hand, both models C and D have shortcomings in detecting the human in certain frames of the right video. Finally, combining them, our full model, _i.e._ model E, excels at segmenting and tracking not only labeled objects but also other objects, such as the car visible in the first frame of the second video.

**Comparison:** We provide a visual comparison between OCLR [83] and our model in Fig. 13 after matching the ground-truth and predictions. OCLR fails to detect static objects (top-left, middle-left, bottom-left) and deformable objects (middle-right) due to failures of optical flow in these cases. Moreover, it considers moving regions as different objects such as the water waves (top-right). On the other hand, SOLV can accurately detect all objects as a whole.

Figure 10: Qualitative results of multi-object video segmentation on YTVIS19

Figure 11: Qualitative comparison of different pretraining methods for visual encoder.

Figure 12: Qualitative results of different models in the component ablation study (Table 3).

Figure 13: Qualitative results of multi-object video segmentation on YTVIS19 after Hungarian Matching is applied. Segmentation results of OCLR [83] are provided in third row.