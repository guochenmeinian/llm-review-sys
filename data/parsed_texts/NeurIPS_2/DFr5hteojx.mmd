The Prism Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models

 Hannah Rose Kirk\({}^{1}\)1 Alexander Whitefield\({}^{2}\) Paul Rottger\({}^{3}\) Andrew Bean\({}^{1}\)

**Katerina Margatina\({}^{4}\)2 Juan Ciro\({}^{5,11}\) Rafael Mosquera\({}^{5,6}\) Max Bartolo\({}^{7,8}\)**

**Adina Williams\({}^{9}\) He He\({}^{10}\) Bertie Vidgen\({}^{1,11\dagger}\) Scott A. Hale\({}^{1,12\dagger}\)\({}^{1}\)**University of Oxford \({}^{2}\)University of Pennsylvania \({}^{3}\)Bocconi University

\({}^{4}\)AWS AI Labs \({}^{5}\)ML Commons \({}^{6}\)Factored AI \({}^{7}\)UCL \({}^{8}\)Cohere

\({}^{9}\)MetaAI \({}^{10}\)New York University \({}^{11}\)Contextual AI \({}^{12}\)Meedan

Footnote 1: {hannah.kirk,scott.hale}@oii.ox.ac.uk \(\dagger\)Joint last authors; \(\ddagger\)Work done at University of Sheffield

###### Abstract

Human feedback is central to the alignment of Large Language Models (LLMs). However, open questions remain about methods (_how_), domains (_where_), people (_who_) and objectives (_to what end_) of feedback processes. To navigate these questions, we introduce Prism, a dataset that maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. With Prism, we contribute (i) wider geographic and demographic participation in feedback; (ii) census-representative samples for two countries (UK, US); and (iii) individualised ratings that link to detailed participant profiles, permitting personalisation and attribution of sample artefacts. We target subjective and multicultural perspectives on value-laden and controversial issues, where we expect interpersonal and cross-cultural disagreement. We use Prism in three case studies to demonstrate the need for careful consideration of which humans provide what alignment data.

**Data & Code:** github.com/HannahKirk/prism-alignment

**Data & Dataset Card:** huggingface.co/datasets/HannahRoseKirk/prism-alignment

## 1 Introduction

Human feedback serves a direct role for the _alignment_ of large language models (LLMs), defined as the steering of AI behaviour towards a set of preferences or values. This increased emphasis on human feedback raises unresolved questions: _how we collect human feedback_ when designing methodologies that rely on ordinal or cardinal scales, broad or fine-grained desiderata, and explicit or implicit signals; _where we focus human labour_ when selecting domains, topics or tasks to collect feedback over; _who we ask for feedback_ when recruiting participants to voice their idiosyncratic preferences, values, or beliefs [1]; and _to what end_ when specifying an objective to pursue personalised alignment [2, 3, 4] or to aggregate individual preferences into collective outcomes favourable for societies at large [5, 6, 7, 8, 9].

Despite the success of human feedback learning [10, 11], answering these questions is constrained by gaps in existing datasets, such as (i) over-reliance on binary A/B comparisons, without fine-grained ratings or explanations [12]; (ii) small or biased samples recruited from narrow crowdwork or tech communities [10, 13] (iii) limited sample information (annotator IDs or sociodemographics) [14]; and (iv) scarce documentation for how values are operationalised [15, 16]. Most datasets rely only on revealed or contextual preferences [1],2 and much attention is devoted to technical or statistical issues in feedback learning [18, 19, 20], rather than data-centric human factors. Relying on 'generic' human data teaches behaviours which are _reductionist_ because values are relational and non-separable from the person, community or operating context [21, 22, 23]; and _non-generalisable_ because the indiscriminate aggregation of data subsumes hidden annotator contexts as universalities [24, 25, 26, 27, 28].

Footnote 2: We use _Contextual Preference_ for observed ratings of LLM outputs to avoid misrepresenting how _Revealed Preference_ is used by economists—as assumptions that enable the inference of preferences from choices [17].

We introduce Prism, a new resource for navigating empirical questions of human feedback. We employ both the _ask_ and _observe_ principles of social science by mapping detailed survey responses of humans around the world onto their live conversations with LLMs (Fig. 1). This setup permits alignment methods relying on either contextual preference comparisons typical for RLHF [29, 30, 31], or stated preferences and principles like constitutional AI [6, 32]. In addition to pairing stated and contextual preferences, Prism has the following features. **Participatory**: To ensure wider active participation in alignment data [25, 33], we recruit 1,500 English-speaking crowdworkers from diverse geographies and demographics; **Representative**: As units for preference aggregation, we include two census-representative samples (UK, US); and **Individualised**: To expose hidden human context and permit personalised preferences, each rating links to a pseudonymous ID and detailed participant profile. We source **Subjective** and **Multicultural** perspectives to avoid value-monism and cultural homogenisation in the opinions that LLMs represent [34, 35, 36] and operate in the descriptive paradigm without guidelines that characterise 'good' responses [15, 16]. Opinion diversity varies along the objective-subjective spectrum (e.g. _what is the capital of France? vs. is abortion wrong?_), so we prime participants for values and controversy guided dialogues but also collect neutral unguided dialogues as a baseline. To our knowledge, Prism is the first human feedback dataset to target cross-cultural controversies and value-laden prompts, where interpersonal disagreement is rife. After introducing Prism (SS 2), we demonstrate its value via three case studies (SS 3): (1) _Do different people initiate different discussions with LLMs?_ (2) _Do people prefer differently aligned models_, and (3) _How do sampling decisions affect welfare outcomes?_ Prism provides many more research avenues such as engineers targeting personalised alignment [2] or consensus across opinion distributions [5, 37]; social scientists examining how exposure to LLMs affects public attitudes; or policymakers seeking democratic input on AI-citizen interactions on topics like immigration, abortion or euthanasia. Alignment cannot be neatly bifurcated into technical and normative components [38]. Prism assists in navigating these complexities with more human voices adjudicating alignment norms.

Figure 1: **The Prism dataset. In Stage 1, 1,500 participants fill in the Survey detailing their background, familiarity with LLMs and stated preferences over behaviours (SS 2.1). Demographic and geographic breakdowns are in Tab. 5 and Tab. 8). Participants then progress to Stage 2, where they converse with LLMs on topics of their choosing, rate the responses on a cardinal scale, and give fine-grained feedback (SS 2.2). In the first turn, four models respond to the opening prompt (§ 2.1, 2.2, 2.2). In subsequent turns, the conversation continues with two responses sampled from the highest-rated model at a non-deterministic temperature (§ 2). There are 8,011 **Conversations** between participants (§ 2) and LLMs (§ 2), forming 27,172 **Interactions** (human message with a set of model responses), and 68,371 **Utterances** (triples of {human message, model response, score}).

## 2 The Prism Alignment Dataset

Prism maps the characteristics and preferences of diverse humans onto their real-time interactions with LLMs (Fig. 1). Participants complete a **Survey** (SS 2.1) with questions about their demographics and stated preferences, then proceed to the **Conversations** with LLMs (SS 2.2), where they input prompts, rate responses and give fine-grained feedback in a series of multi-turn interactions. With the two-stage setup: (i) we avoid over-generalising from a "generic human" by matching ratings to detailed participant characteristics; (ii) we track how contextual preferences (in local conversations) depart from stated preferences (in survey); and (iii) we give participants autonomy to communicate in their own words what is important and why [39; 25]. Both stages received ethics board approval and ran with informed consent (App. D). Participants were paid PS9/hour and the task took 70 minutes on average. Data collection ran from 22nd November to 22nd December 2023.3We provide a data statement in App. B, data clause in App. C, and full codebooks detailing each variable in App. V.

Footnote 3: Ethics approval, data collection, and analysis was led by researchers from the University of Oxford.

### The Survey

Prior to starting the survey, we ensure that all participants are over 18, obtain their informed consent, give a brief primer on LLMs (or AI language models), and dissuade LLM-written responses. The survey constructs a participant profile containing five features:

**LLM familiarity and usage** We ask about participants' familiarity with LLMs (61% are somewhat familiar, 28% very familiar and 10% not familiar at all) and whether to their knowledge they have used them _indirectly_ (in products like LinkedIn post-writing tool); or _directly_ (via a specialised interface like ChatGPT). Individuals that have used LLMs directly or indirectly (84%) are branched to questions on frequency of use (7% every day, 21% every week, and 20% every month) and purpose of use (the most popular tasks are research overviews selected by 49%, professional work by 37%, creative writing by 31% and programming help by 27%). Full results in App. I.

**Self-written system string ("constitution")** System strings can guide LLM behaviours as a high-level global instruction prompts prepended to all subsequent interactions [40; 41], and have been analogised as "constitutions" or governing principles for AI [32]. Factuality, professionalism, humanness and harmlessness all emerged as key principles (App. M.1) from the following instruction:

Figure 2: **Schematic of fine-grained attribute ratings. The same attributes appear in three places in our task: A is asked once in the survey; B and C are asked per conversation. For _performance attributes_, we ask participants to consider only the highest-rated model in the first conversation turn; for _choice attributes_, we ask them to consider this highest-rated model relative to other models in the first turn.**

**Stated preferences for LLM behaviours** In contrast to this open-ended preference elicitation, we collect structured ratings on fine-grained behaviour attributes. Participants score the importance of each attribute on a visual analog scale [42] (Fig. 2). A statement like "_It is important that an AI language model produces factual and informative responses_" maps (0,100) where the ends of scale are (_Strongly disagree, Strongly agree)_. Numeric scores are recorded, but not shown to participants to avoid anchoring and dependency biases. We only collect responses to these statements once _before_ participants interact with LLMs but the same attributes appear in the Conversations stage; so, we can track how stated 'abstract' preferences relate to contextual 'in-situ' preferences.4 Overall, we find clusters of subjective attributes (values, creativity and diversity) versus objective attributes (factuality, fluency and helpfulness; App. N.1). While the majority of participants agree that these more objective attributes are important (highly-skewed positive distribution, \(\mu\in[86,89],\sigma\in[14,16]\)), there is little agreement on the meta-importance of subjective attributes (App. N.2). In fact, responses for whether value alignment itself is important follow an almost normal distribution (\(\mu=54,\sigma=26\)).

Footnote 4: The survey also has an _Other_ free-text box used by 332 participants (App. N.3), and a _personalisation_ attribute which we do not include in Conversations because models are not personalised.

**Self-written description** Values and preferences are subjective and personal. We ascribe participants autonomy to communicate salient aspects of their identity in a short profile, beyond essentialising associations with structured demographics alone. Honesty, hard work and empathy emerged as common values (App. M.2) from the following instruction:

_Please briefly describe your values, core beliefs, guiding principles in life, or other things that are important to you. For example, you might include values you’d want to teach to your children or qualities you look for in friends. There are no right or wrong answers. Please do not provide any personally identifiable details like your name, address or email._

**Basic demographics** We ask standard demographics: age, gender, employment status, martial status, educational attainment, ethnicity, religious affiliation, English proficiency, country of birth, and country of residence. There is always a "_Prefer not to say_" option. For gender, participants can select _Male_, _Female_, _Non-Binary_, or self-describe. We collect self-described ethnicity and religion because no pre-set groups exhaust how individuals may self-identify across cultures and global regions. We provide a manual annotation of these strings into aggregated categorisations for statistical analysis (App. F). Because of how we recruit participants (SS 2.3), our sample covers diverse demographics (App. G) and geographies (App. H), with representation from people born in 75 countries. However, the sample still skews White, Western and educated, and only contains English-language speakers.

### The Conversations

After completing the survey, participants move to the second stage, consisting of real-time conversations with LLMs via a custom-built interface on the Dynabench platform [43; 44].

**Selecting conversation type** We prime participants to diversify their prompts along the objective-subjective spectrum by asking them to complete two conversations across three conditions or _conservation types_ (six in total).5 They select the _type_ before inputting their opening prompt:

Footnote 5: Some deviated from this quota (n=6, 2 per type) due to technical difficulties, instruction misunderstanding or losing count; So, we release a balanced subset of the data that controls for this variance (App. K). Though values and controversy guided conversations are typically more subjective than neutral baselines, conversation type does not map perfectly to subjectivity levels. Besides from priming participants via selecting a conversation type, we do not constrain (and seek to minimally influence) participants’ topic or prompt choice.

**Opening the conversation** Participants construct a free-text prompt of their choosing and receive up to four responses from different LLMs.6 The participants then rate each response on a visual analogue scale (VAS) [42; 45] from "Terrible" to "Perfect". We record the slider position as a score from 1-100 but do not show participants the number to avoid anchoring or conditional dependence of scores across conversations. We opt for this cardinal feedback for three reasons: (i) it encourages subjectivity; (ii) it permits studying the relative merit of cardinality versus ordinality for rewardmodelling because ratings can be converted to rankings but not vice versa; (iii) it allows expression of preference intensity above and beyond chosen:rejected pairs.7 However, we acknowledge that the cardinal scale introduces some intrapersonal measurement noise from a more cognitively demanding task and carries less interpersonal comparability than ordinal preferences, see Limitations (SS 5).

Footnote 7: For example, all responses could be very poor and similar (negative skew, small spread); all very good and similar (positive skew, small spread); or highly-distinguishable (no skew, wide spread).

**Continuing the conversation** The highest-scoring LLM from the opening turn is locked into subsequent turns, with random tie-breaks in the case of identical scores. Participants must continue the conversation for at least another turn, but are asked to vary their conversations between 2 and 10 turns to avoid introducing a dataset artefact. We encourage some variation in conversation length (\(\mu_{T}=3.4,\sigma_{T}=1.6\)) but there is a strong drop off after the second turn (App. O). Participants then rate two responses on a VAS like before, but both are now sampled from the selected model with a non-deterministic temperature. These within-model responses are more similar in style and content than across-model responses (in the first turn), and score deviations are narrower (App. O).

**Collecting fine-grained feedback** After the conversation ends, participants first rate statements about the _performance of their highest-rated model_ like "The response was well-written" on a VAS from _Performed very poorly_ to _Performed very well_, or select N/A if the statement is irrelevant for the context. We then ask participants to consider _why they chose this model_, rating statements like "I chose this response _because_ it was well-written" on a VAS from _Very unimportant_ to _Very important_ (or select N/A). Attributes are shared with the Survey (Fig. 2). We find strong correlations between performance attributes and choice attributes (except safety) but weak correlations of these pairs to stated preferences given in the Survey, perhaps due to conversational, model or task-design confounders (App. N.1). In general, the distribution of scores over performance and choice attributes is narrower and more positively skewed (bunched to 100) compared to stated preferences (App. N.2). Finally, we collect open-ended natural language feedback on the _whole_ conversation. Participants contributed both content and stylistic feedback (\(\mu=29\) words, \(\sigma=19\), App. M.3).

_Give some feedback on the conversation as whole. Hypothetically, what would an ideal interaction for you look like here? What was good and what was bad? What (if anything) was missing? What would you change to make it better?_

### The Sample

Our sampling aims were _depth_ in the demographics represented within countries and _breadth_ across global regions. We recruit English-speaking participants from Prolific in two distinct paths:

**Census-representative sample (UK, US)** Samples matched to simplified census data (age, ethnicity, gender) were only available for the UK and US. The minimum pool size for a statistical guarantee of representativeness was 300, which set a lower bound for participant quota. After collecting data, we observed some skew in our'representative' samples between observed and expected distributions in recent census data, which we partially correct for (App. L). These samples permit future studies on more representative populations that can be replicated across two countries; however their inclusion biases Prism as a whole towards two Western nations already over-represented in AI research.

**Balanced samples (rest of world)** The distribution of Prolific workers outside the US and the UK skews strongly to Europe and Northern America, and some countries dominate continental counts (App. J). To avoid more active workforces biasing the sample, we set up 33 country-specific studies where there is \(>1\) eligible worker, and allocate sample quotas so that each global region is similarly represented.8 We balance each national sample by gender where possible (Tab. 10).

Footnote 8: Participants still appear in our sample who were born or reside in countries that did not have a dedicated country-wise study e.g. if their Prolific details were outdated or incorrect. We do not drop them.

**Included models** The rapidly evolving landscape necessitates a model-agnostic approach to avoid data staleness. We include 21 different LLMs (9 open-access, 12 commercial-API) from various model families and parameter sizes, which diversifies the training data, capabilities, and degree of existing safeguards or alignment biases. To avoid text length confounding preferences [46] and to reduce participant fatigue, we include system prompts instructing models to limit their responses to \(\leq 50\) words. We show the full list of models, decoding parameters and generation details in App. P.

## 3 Experiments with Prism

### Case Study I: Do Different People Initiate Different Discussions with LLMs?

**Methods** We use a pre-trained sentence transformer (all-mpnet-base-v2) to embed each opening prompt in 768-D, then apply UMAP to reduce to 20-D, before clustering with HDBScan [47]. 70% of prompts are assigned to 22 topic clusters and 30% remain as outliers. We name each cluster by prompting gpt-4-turbo with the top n-grams extracted with TF-IDF and closest texts to the cluster centroid. We define an _over-representation factor_ as \(\frac{N_{g,t}/N_{t}}{b_{g}}\), to compute observed versus expected topic prevalence per identity group. For the partial contribution of identity attributes, we estimate an OLS regression for each topic \(y^{t}\) (\(t\in 1\ldots 22\)) and cluster standard errors at the individual level: \(y^{j}_{i,c}=\alpha^{t}+\text{gender}^{\prime}_{i}\beta^{t}_{1}+\text{age}^{ \prime}_{i}\beta^{t}_{2}+\text{birth\_region}^{\prime}_{i}\beta^{t}_{3}+\text {ethnicity}^{\prime}_{i}\beta^{t}_{4}+\text{religion}^{\prime}_{i}\beta^{t}_{5} +\text{prompt}^{\prime}_{i}\beta^{t}_{6}+\varepsilon_{i,c}\), where \(y^{t}_{i,c}=1\) if the prompt of participant \(i\) in conversation \(c\) is categorised as topic \(t\). The identity vectors (e.g. _gender_) represent sets of variables, with a base category removed (indicated in Fig. 3). The coefficients of interest are contained in vectors \(\{\beta^{t}_{d}\}_{d=1}^{6}\), where component \(g\) of \(\beta^{t}_{d}\) is interpreted as the increase in probability of a participant choosing topic \(t\) if they are in the group indexed by \(g\) (e.g. Female) compared to the base group (e.g. Male). See App. R for extended methods.

**Results** Our instructions had a significant priming effect, resulting in a **high density of controversial and value-laden topics** (Fig. 3). Topics significantly correlated with controversy guidance are _Gender & LGBTQ+ Identity_, _Israel-Palestine Conflict_, and _Discussions on Abortion_, while topics significantly correlated with the values guidance are _Managing Relationships_, _Job Search_, and _Religion & Spirituality_. In contrast, the 'unguided' condition correlates with task-oriented and

Figure 3: **Topic prevalence by conversation types and participant identity.** We show total prompts clustered into topics (**bars**), and total members in each group (top panels). Per group and topic, we plot the _over-representation factor_ of observed vs. expected group proportions and show significant regression coefficients (base category indicated by \(\dagger\)). All coefficients are in Fig. 23, topic-group counts in Fig. 27 and centroid prompts in Tab. 22. Location is by _birth region_ (with UK and US split out), but most regions have few countries (App. H). _Key results_ (§ 3.1): Priming participants to select a conversation type (unguided, values or controversy guided) significantly influenced diversity of prompts. Identity factors have some significant interactions with prompt choice but each topic contains prompts authored by intersectionally-diverse participants.

neutral topics like _Popular Culture_, _Recipes & Cooking_ and _Travel Recommendations_. Only _Climate Change_ is not significantly correlated to conversation type. Controlling for conversation type, 11% of coefficients are significant (\(\alpha=99\%\)); so, **identity factors have some predictive power on topic prevalence**. Significant relationships include: women and non-binary people discuss gender and LGBTQ+ issues more than men; older people discuss elections and travel more than younger people; Black participants discuss climate change less than White participants, and all regions question LLMs about abortion less often than US participants. When we examine granular regions in embedding space using a single-link hierarchical clustering algorithm (App. S), **local prompt neighbourhoods tend to be intersectionally-diverse**: 84% of them meet or exceed entropy across intersectional demographics that would be expected under random sampling. During this local exploration, we retrieve regions of semantically-identical prompts rated by multiple diverse individuals (e.g. one neighbourhood "Does God exist?" has 7 religious and 7 irreligious participants), finding that **interpersonal differences in contextual preferences persist even when dialogue context is fixed** (App. S.4). So, despite Prism containing semantically-diverse prompts, people from different backgrounds occupy common discussion spaces, providing an anchor to examine diverse perspectives to shared issues.

### Case Study II: Do Different People Prefer Differently-Aligned Models?

**Methods** Observed preference differences at the model-level are confounded by interactions of topic prevalence and model aptitude (e.g. men ask more about aliens and gpt-4 is poor on extraterrestrial knowledge). Evidence of shared dialogue spaces (SS 3.1) and group-topic score differences (App. T.2) mitigate some concern, but to further control for context, we use opening prompts from the balanced subset of participants (n=1,246) with equal conversations per type (n=6,669). The mean participant rates 14/21 LLMs but unseen ratings are missing at random. Our aggregation (social choice) function over participant ratings is derived from _Pairwise Rank Centrality_ (\(\mathcal{P}\)) [48] and _Convergence Voting_[49], both inspired by _PageRank_[50]. Each model is a node in a graph and transition probabilities between nodes are calculated by the proportion of pairwise battle wins. This process simulates a random walk on a Markov chain, leading to a stationary distribution of scores that reflect the collective preference intensity across models. Here, we compute \(\mathcal{P}\) over subsamples using a regularisation parameter of 1 and tie threshold of 5, but present extended methods and robustness checks in App. T.

**Results** We find **rankings are sensitive to idiosyncratic, contextual, and group-wise variance**. Samples of 100 people introduce significant noise, resulting in a fairly even distribution of collective preference among the top 10 models (Fig. 4). Rankings are sensitive to _what_ participants talk about: zephyr-7b performs highly on controversy but not in unguided domains, while claude-2 has the opposite trend; and _where_ they are from: relative to overall rank, palm-2 drops 4 places for participants in the US, llama-7b drops 7 places in Asia, while mistral-7b gains 7 places in Africa. We further observe that **Prism produces surprising ranks relative to other leaderboards**. We apply our method to ChatbotArena data [51], finding gpt models fare significantly worse in Prism, while open models like zephyr-7b do significantly better (95% CI over 1,000 bootstraps, App. T.9). This may be due to domain shift (task-oriented/coding prompts vs. controversial/cultural prompts), sample diversity or task incentives. To identify drivers of score differences, we generate hypotheses by qualitatively examining battles between command and gpt-4/-turbo, then test these with an OLS regression on all model responses (App. T.8). We find that **formatting and refusals partially explain score differences** with significant positive effects from additional characters, ending in a question mark ("Would you like to know more?") and enumeration, but significant negative effect of line breaks. De-anthropomorphic phrases ("As an AI, I don't have personal opinions.") significantly reduce score but not as substantially as refusals ("Sorry I cannot engage."). The proportion of explained variance in score by these factors is low (\(R^{2}=0.06\)), so we encourage more sophisticated methods in future work for partialling out the effect of style versus content, or participant, model and conversation fixed-effects, as determinants of score.

### Case Study III: How do Sampling Decisions Affect Welfare Outcomes?

**Methods** We use 'welfare' to capture the extent to which a chosen LLM aligns with the preferences of a user population. We consider two welfare measures: average model rating (meanRating), and average likelihood that a model is chosen (rated highest in the opening turn, meanChoice). Previous experiments indicate dialogue and preference diversity across people, suggesting that the welfare of downstream LLM users may depend on who provides feedback. To test this, we first randomly generate seven sub-samples of individuals 'in the seat of power' to select their favourite LLM (basedon mean rating). Four sampling schemes randomly draw \(N\) individuals from a representative sample (\(N\in\{10,20,50,100\}\)). Three schemes randomly draw 100 individuals from specific low-diversity sub-populations (male, white, and \(\geq\)45 years old). For each draw, we then measure the distribution of welfare from this LLM being imposed on different stakeholder populations [9]: the entire population, non-male individuals, non-white individuals, and individuals \(<\)45 years old. We report the distribution of average welfare outcomes across random draws from each sampling scheme. We conduct this experiment for the UK and US representative samples. Extended methods are in App. U.

**Results** We find **as sample size falls, the probability of choosing a LLM with worse mean welfare rises**. Larger samples from the target sub-population appear to first order stochastically dominate9 (FOSD) smaller samples from the target sub-population. **Sampling exclusively from a specific group tends to reduce the welfare of out-group individuals**. For example, when consider the welfare of the representative US sample (Fig. 5), sampling from US males is FOSD by sampling from the full US sample. Furthermore, **average measures can conceal the welfare of minority groups**: sampling 100 white individuals appears to FOSD sampling 100 representative individuals when assessing welfare of the population at large, but minority stakeholders (non-white population) are worse off under this scheme. Finally, **regardless of the model chosen, a large proportion of participants prefer a different model**. For the US, the model that maximises meanChoice only

Figure 4: **Sources of variation in model preferences.** Panel A shows _idiosyncratic variance_ in distributions of Pairwise Rank Centrality scores for 100 randomly-drawn participants (over 1,000 bootstraps). For Panels B and C, we show _conversational context variation_ and _group-wise variation_. We show overall rank based on Pairwise Rank Centrality over n=6,669 balanced conversations (numbered circles). We then trace how rank changes by sampling the group on \(x\) (e.g. filtering to only values guided conversations, or only US participants). Across these subsamples, we show most spots climbed (\(\blacktriangle\)) and spots fallen (\(\blacktriangledown\)) by each model relative to overall rank. _Key results_ (§ 3.2):** Rankings are sensitive to sample composition, varying with which participants are sampled (Panel A,C) and what they talk about (B). Rankings differ from other leaderboards, explained by Prism’s characteristics (sample diversity, domain shifts) as well as response characteristics (length, formatting, refusals).

achieves a probability of \(45\%\). If a participant is shown the winning model, and three other models at random, the probability that they will choose the winning model is \(<50\%\). The probability they will pick the winning model over all other 20 LLMs can only be lower. This suggests that we should not expect a single LLM to satisfy everyone's preferences in a given population. We repeat the welfare analysis for the UK sample and conduct robustness checks with imputed missing data in App. U.

## 4 Related Work

**Participation & Representation in Science & Technology** There is a long history of technologies failing diverse users who lack consultation during design [52, 53, 54]. Conscious participation can be intrinsically valuable as an act of justice [55, 56]. However, in internet-harvested pre-training data, participation is involuntary or cooptative [55, 33], and unequal representation risks cultural homogenisation and minority stereotyping [57, 58, 59, 60, 61, 62]. Labelling data or giving feedback is active _procedural participation_[53] but often relies on narrow specifications from technology providers of what counts as high-quality language or preferable outputs [15, 16, 63, 64]. In ML or NLP data, variability in subjective experience is commonly collapsed into majority votes [65, 66, 67, 68], without sufficient documentation of annotator artefacts or disagreements [69, 70, 71, 72, 73], despite evidence that sociodemographic affect labels [74, 75, 76, 77, 78, 79]. Multiple scientific fields are guilty of over-generalising conclusions from the 'generic human' drawn from 'WEIRD' societies [80, 81]. Prism releases participant IDs and characteristics to spotlight sample diversity while acknowledging sample specificity [82].

**Learning from Human Feedback** Using human feedback to condition the loss function for training LLMs overcomes challenges of specifying rewards [83, 84, 85]. Combining human feedback, reinforce

Figure 5: **Welfare distributions for the US. The distribution of mean welfare for four subpopulations in the US (welfare pop) induced by seven sampling schemes (in the seat of power). The \(y\) axis is the sampled supopulation (e.g. **Rep** is a ‘representative’ sample of the population) and sample size in brackets (e.g **(100)**). Each violin shows the distribution of mean welfare for the panel’s subpopulation induced by a sampling scheme. The top four Rating comparisons use the meanRating welfare measure and the bottom Choice comparisons use the meanChoice welfare measure. The red distributions are FOSD by Rep (100) in blue (i.e. less optimal scheme). _Key results_ (§ 3.3):_ Large representative samples mostly outperform smaller or demographically-restricted samples and sampling exclusively from a specific group tends to reduce the welfare of out-group participants (male vs. non-male, white vs. non-white). No single model achieves majority preference (max 45% meanChoice).

ment learning and natural language generation has a history in machine translation [86, 87, 88] and dialogue [89, 90, 91, 92, 93, 94]. RLHF pipelines rely on binary comparisons [29, 20, 30, 85], principles or rules [32, 95], fine-grained feedback [12], or natural language [96], to reward dimensions like helpfulness, honesty and harmlessness [97, 30]. Reward models then update LLMs via algorithms like PPO [98] or Reinforce [99, 100]; but reward model free techniques are competitive, e.g. DPO [18], supervised fine-tuning [101] and rejection sampling [102, 5, 103]. There is rising demand for high-quality human feedback [104, 105], but the complexity and cost of collecting data incentivises scraping preferences, e.g. on Reddit [29, 106] or StackOverflow [107], or simulating humans with LLMs [108, 109, 110]. Similar to Prism, ChatbotArena [51], Lmsys-1m [111] and WildChat[112] feature user-rated model interactions, but for narrow communities (HuggingFace Spaces) and domains (coding, task-orientated). Unlike these datasets, OpenConvos[113] collect optional contributor demographics, and Dices[79] provide demographics for multiple raters per conversation. Other datasets target specific behaviours [30, 114], or multilingual coverage [115]. Surveys on attitudes towards AI [116, 117] and community assemblies [118, 6, 119] offer another lens on public priorities. To our knowledge, Prism is the first to link preference ratings and detailed survey responses.

## 5 Limitations, Discussions and Conclusions

**Ethical Considerations and Limitations** We collect informed consent, pseudononymise IDs, check for PII (App. E) and disallow deanonymisation in our terms (App. C), but privacy risks remain, especially given the sensitive nature of conversations. Asking participants to engage with controversies expands human preference data to discursive areas with the greatest expected degree of interpersonal disagreement, but risks encouraging hateful, bigoted, biased or otherwise harmful content. Prism is less toxic than previous datasets (0.06%, App. E). We do not moderate prior to release to permit conversational safety research. There are many sources of variance in Prism and alternative divisions of the data may yield different outcomes [120]. Granting free choice of dialogue, using cardinal feedback scales and focusing on many kinds of models and participants introduces diversity and subjective freedom but complicates controlled experiments and limits statistical power. Prism is still biased towards English-speaking crowdworkers whose task-specific incentives may not align with wider populations. We expand on ethical risks and limitations in our data statement (App. B).

We raise three discussion points on the boundaries of where we collect preferences, for what end and with what lasting impact. First, aligning LLMs via 'preference-based utilitarianism' [121] may not be synonymous with individual or societal well-being, prompting the question of **whether there are limits for "legitimate" human feedback**. Preferences may be (i) at odds with self-interest due to myopia or information asymmetries (e.g. participants who want anthropomorphic LLMs despite evidenced harms [122, 123, 124, 125, 126]) or (ii) incompatible with others' interest (e.g. participants who prefer 'anti-woke' LLMs that argue in a debate vs. those who favour neutrality). Relying on decontextualized preference observations carries the risk of silently reinforcing biases from those in power [61, 65]; so we recommend transparency surrounding individual disagreements before aggregation decisions [9, 127], especially if participant positionality affects their epistemic legitimacy to define harm [59, 128, 129]. Second, **irreconcilable personal preferences and morals matter more when the 'unit of alignment' is operationalised as a group, culture or even species, rather than an individual**. Prism permits personalised or steerable alignment using participant profiles and specific ratings [2, 3, 4, 37] as well as collective alignment via opinion consensus or distribution of rewards [5, 6, 7, 8, 28]; though group deliberation in groups may yield different outcomes than gathering data from one person at a time [6, 118, 119]. With growing use of synthetic alignment data, Prism can assist in calibrating LLM-as-judge protocols to more diverse rater pools [51, 130]. Finally, Prism was motivated by participation as justice via inclusionary alignment practices that, relative to passive roles in annotation tasks or pre-training data, prioritise active input from local citizens with specialised knowledge of their own and communities' needs [55]. However, participation remains thin because **the humans crucial to the success of RLHF do not typically share in downstream benefits or profits**[33, 131]. Ultimately, the impact of our work depends on those developing, researching and regulating LLMs because effective participation requires being asked _and_ being heard [53].

In their early demonstrations of aligning AI systems to human feedback, Bai et al. discuss _alignment data as a public good_. We echo this sentiment with Prism--a new feedback dataset from 1,500 diverse humans, motivated by the need for inclusive, participatory and open scientific research into the pressing question of what it means to align LLMs to human preferences in a pluralistic world.

## Acknowledgments and Disclosure of Funding

This project was awarded the MetaAI Dynabench Grant "Optimising feedback between humans-and-models-in-the-loop". For additional compute support, the project was awarded the Microsoft Azure Accelerating Foundation Model Research Grant. For additional annotation support, we received funding from the OpenPhil grant and NSF grant (IIS-2340345) via New York University. We are grateful for support received in the form of research access or credits from OpenAI, Anthropic, Aleph Alpha, Google, HuggingFace and Cohere. Hannah Rose Kirk's PhD is supported by the Economic and Social Research Council grant ES/P000649/1. Paul Rottger is a member of the Data and Marketing Insights research unit of the Bocconi Institute for Data Science and Analysis, and is supported by a MUR FARE 2020 initiative under grant agreement Prot. R20YSMBZ8S (INDOMITA). Andrew Bean's PhD is supported by the Clarendon Fund Scholarships at the University of Oxford. We are particularly grateful to Maximilian Kasy for his valuable input and advise on the welfare experiments. We are indebted to the incredible effort and time that our Prolific annotators put into our task, as well as the expert advice from Prolific consultant Andrew Gordon. We also thank any Beta testers, including friends, family and colleagues at Oxford and New York University, for their help in piloting (and debugging!) our task. Lastly, we thank Jakob Mokander, Nathan Lambert, Natasha Jacques, Felix Simon, Nino Scherrer, Maximilian Kroner Dale, and Saffron Huang for their feedback on the paper. We use scientific colour maps in our figures [132].

## Author Contribution Statement

\begin{tabular}{c c}
**Project Conception** & [Kirk, Hale, Vidgen] \\
**Data Collection Design** & [Kirk, Hale, Vidgen, Rotter, Margatina] \\
**Frontend Design and Development** & [Kirk, Ciro] \\
**Backend Design and Development** & [Kirk, Mosquera] \\
**Analysis Advisory** & [Hale, Vidgen, Rotter, Bartolo, Bean, Williams, He] \\
**Literature and Dataset Comparison** & [Kirk, Bean] \\
**Metadata Processing** & [Kirk, Margatina, Bean] \\
**Manual Annotation** & [Kirk, Bean, Rotter, Bartolo] \\
**Results and Codebase** & [Kirk, Whitefield] \\
**Manuscript Writing** & [Kirk, Whitefield] \\
**Manuscript Editing and Feedback** & [Everyone] \\ \end{tabular}

## References

* [1] Iason Gabriel. Artificial Intelligence, Values and Alignment. _Minds and Machines_, 30(3):411-437, September 2020. ISSN 0924-6495, 1572-8641. doi: 10.1007/s11023-020-09539-2.
* [2] Hannah Rose Kirk, Bertie Vidgen, Paul Rottger, and Scott A. Hale. The benefits, risks and bounds of personalizing the alignment of large language models to individuals. _Nature Machine Intelligence_, pages 1-10, April 2024. ISSN 2522-5839. doi: 10.1038/s42256-024-00820-y. URL [https://www.nature.com/articles/s42256-024-00820-y](https://www.nature.com/articles/s42256-024-00820-y). Publisher: Nature Publishing Group.
* [3] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging, October 2023. URL [http://arxiv.org/abs/2310.11564](http://arxiv.org/abs/2310.11564). arXiv:2310.11564 [cs].
* [4] Xinyu Li, Zachary C. Lipton, and Liu Leqi. Personalized Language Modeling from Personalized Human Feedback, February 2024. URL [http://arxiv.org/abs/2402.05133](http://arxiv.org/abs/2402.05133). arXiv:2402.05133 [cs].
* [5] Michiel A. Bakker, Martin J. Chadwick, Hannah R. Sheahan, Michael Henry Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John Aslanides, Matthew M. Botvinick, and Christopher Summerfield. Fine-tuning language models to find agreement among humans with diverse preferences. In _Advances in neural information processing systems_, volume 35, pages 38176-38189. Curran Associates, Inc., November 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/f978csf3b5f399cae464e85f72e28503-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/f978csf3b5f399cae464e85f72e28503-Paper-Conference.pdf). _eprint: 2211.15006v1.
* [6] Anthropic. Collective Constitutional AI: Aligning a Language Model with Public Input. Technical report, 2023. URL [https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input](https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input).

* Chakraborty et al. [2024] Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Singh Bedi, and Mengdi Wang. MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences, February 2024. URL [http://arxiv.org/abs/2402.08925](http://arxiv.org/abs/2402.08925). arXiv:2402.08925 [cs].
* Li et al. [2024] Dexun Li, Cong Zhang, Kuicai Dong, Derrick Goh Xin Deik, Ruiming Tang, and Yong Liu. Aligning Crowd Feedback via Distributional Preference Reward Modeling, February 2024. URL [http://arxiv.org/abs/2402.09764](http://arxiv.org/abs/2402.09764). arXiv:2402.09764 [cs].
* Conitzer et al. [2024] Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley H. Holliday, Bob M. Jacobs, Nathan Lambert, Milan Mosse, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, Emanuel Tewolde, and William S. Zwicker. Social Choice for AI Alignment: Dealing with Diverse Human Feedback, April 2024. URL [http://arxiv.org/abs/2404.10271](http://arxiv.org/abs/2404.10271). arXiv:2404.10271 [cs].
* Kirk et al. [2023] Hannah Kirk, Andrew Bean, Bertie Vidgen, Paul Rottger, and Scott Hale. The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 2409-2430, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.148. URL [https://aclanthology.org/2023.emnlp-main.148](https://aclanthology.org/2023.emnlp-main.148).
* Lambert et al. [2023] Nathan Lambert, Thomas Krendl Gilbert, and Tom Zick. The History and Risks of Reinforcement Learning and Human Feedback, November 2023. URL [http://arxiv.org/abs/2310.13595](http://arxiv.org/abs/2310.13595). arXiv:2310.13595 [cs].
* Wu et al. [2023] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-Grained Human Feedback Gives Better Rewards for Language Model Training, June 2023. URL [http://arxiv.org/abs/2306.01693](http://arxiv.org/abs/2306.01693). arXiv:2306.01693 [cs].
* Casper et al. [2021] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jeremy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphael Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyik, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback, September 2023. URL [http://arxiv.org/abs/2307.15217](http://arxiv.org/abs/2307.15217). arXiv:2307.15217 [cs].
* Prabhakaran et al. [2021] Vinodkumar Prabhakaran, Aida Mostafazadeh Davani, and Mark Diaz. On Releasing Annotator-Level Labels and Information in Datasets. In _Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop_, pages 133-138, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.law-1.14. URL [https://aclanthology.org/2021.law-1.14](https://aclanthology.org/2021.law-1.14).
* Rottger et al. [2022] Paul Rottger, Bertie Vidgen, Dirk Hovy, and Janet Pierrehumbert. Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 175-190, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.13. URL [https://aclanthology.org/2022.naacl-main.13](https://aclanthology.org/2022.naacl-main.13).
* Kirk et al. [2023] Hannah Rose Kirk, Bertie Vidgen, Paul Rottger, and Scott A. Hale. The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising "Alignment" in Large Language Models. In _Socially Responsible Language Modelling Research (SoLaR)_. arXiv, November 2023. doi: 10.48550/arXiv.2310.02457. URL [http://arxiv.org/abs/2310.02457](http://arxiv.org/abs/2310.02457).
* Mas-Colell et al. [1995] Andreu Mas-Colell, Michael Dennis Whinston, Jerry R Green, et al. _Microeconomic theory_, volume 1. Oxford university press, New York, 1995.
* Rafailov et al. [2024] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. In _Advances in Neural Information Processing Systems_, volume 36, February 2024. URL [https://proceedings.neurips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5a06ce7-Abstract-Conference.html](https://proceedings.neurips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5a06ce7-Abstract-Conference.html).
* Zhu et al. [2024] Banghua Zhu, Jiantao Jiao, and Michael I. Jordan. Principled Reinforcement Learning with Human Feedback from Pairwise or SKS-wise Comparisons, February 2024. URL [http://arxiv.org/abs/2301.11270](http://arxiv.org/abs/2301.11270). arXiv:2301.11270 [cs, math, stat].

* Zhu et al. [2024] Banghua Zhu, Michael I. Jordan, and Jiantao Jiao. Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF, January 2024. URL [http://arxiv.org/abs/2401.16335](http://arxiv.org/abs/2401.16335). arXiv:2401.16335 [cs, stat].
* Turchin [2019] Alexey Turchin. AI Alignment Problem: "Human Values" Don't Actually Exist. _PhilArchive_, 2019. URL [https://philarchive.org/rec/TURAAP](https://philarchive.org/rec/TURAAP).
* Earp et al. [2021] Brian D. Earp, Killian L. McLoughlin, Joshua T. Monrad, Margaret S. Clark, and Molly J. Crockett. How social relationships shape moral wrongness judgments. _Nature Communications_, 12(1):5776, October 2021. ISSN 2041-1723. doi: 10.1038/s41467-021-26067-4. URL [https://www.nature.com/articles/s41467-021-26067-4](https://www.nature.com/articles/s41467-021-26067-4). Publisher: Nature Publishing Group.
* Mascolo et al. [2021] Michael F Mascolo, Allison DiBianca Fasoli, and David Greenway. A Relational Approach to Moral Development in Societies, Organizations and Individuals. _Integral Review_, 17(1), 2021.
* Butler et al. [2000] Judith Butler, Ernesto Laclau, and Slavoj Zizek. _Contingency, hegemony, universality: contemporary dialogues on the left_. Phronesis. Verso, London, 2000. ISBN 978-1-85984-757-2 978-1-85984-278-2. OCLC: ocm44780799.
* Sloane [2024] Mona Sloane. Controversies, contradiction, and "participation" in AI. _Big Data & Society_, 11(1):20539517241235862, March 2024. ISSN 2053-9517. doi: 10.1177/20539517241235862. URL [https://doi.org/10.1177/20539517241235862](https://doi.org/10.1177/20539517241235862). Publisher: SAGE Publications Ltd.
* Talat et al. [2022] Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, and Adina Williams. On the machine learning of ethical judgments from natural language. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 769-779, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.56. URL [https://aclanthology.org/2022.naacl-main.56](https://aclanthology.org/2022.naacl-main.56).
* Aroyo and Welty [2015] Lora Aroyo and Chris Welty. Truth Is a Lie: Crowd Truth and the Seven Myths of Human Annotation. _AI Magazine_, 36(1):15-24, March 2015. ISSN 2371-9621. doi: 10.1609/aimag.v36i1.2564. URL [https://ojs.aaai.org/index.php/aimagazine/article/view/2564](https://ojs.aaai.org/index.php/aimagazine/article/view/2564). Number: 1.
* Siththaranjan et al. [2023] Anand Siththaranjan, Cassidy Laidlaw, and Dylan Hadfield-Menell. Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF, December 2023. URL [http://arxiv.org/abs/2312.08358](http://arxiv.org/abs/2312.08358). arXiv:2312.08358 [cs, stat].
* Stiennon et al. [2020] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. In _Advances in Neural Information Processing Systems_, volume 33, pages 3008-3021. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html).
* Bai et al. [2022] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, April 2022. URL [http://arxiv.org/abs/2204.05862](http://arxiv.org/abs/2204.05862). arXiv:2204.05862 [cs].
* Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In _Advances in Neural Information Processing Systems_, volume 35, pages 27730-27744, December 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/hash/biefde53be364a73914f58085a001731-Abstract-Conference.html](https://proceedings.neurips.cc/paper_files/paper/2022/hash/biefde53be364a73914f58085a001731-Abstract-Conference.html).
* Bai et al. [2022] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schafer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman,Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI Feedback, December 2022. URL [http://arxiv.org/abs/2212.08073](http://arxiv.org/abs/2212.08073). arXiv:2212.08073 [cs].
* Birhane et al. [2022] Abeba Birhane, William Isaac, Vinodkumar Prabhakaran, Mark Diaz, Madeleine Clare Elish, Iason Gabriel, and Shakir Mohamed. Power to the People? Opportunities and Challenges for Participatory AI. In _Equity and Access in Algorithms, Mechanisms, and Optimization_, EAAMO '22, pages 1-8, New York, NY, USA, October 2022. Association for Computing Machinery. ISBN 978-1-4503-9477-2. doi: 10.1145/3551624.3555290. URL [https://doi.org/10.1145/3551624.3555290](https://doi.org/10.1145/3551624.3555290).
* Durmus et al. [2023] Esin Durmus, Karina Nyugen, Thomas I. Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, Liane Lovitt, Sam McCandlish, Orowa Sikder, Alex Tamkin, Janel Thamkul, Jared Kaplan, Jack Clark, and Deep Ganguli. Towards Measuring the Representation of Subjective Global Opinions in Language Models, June 2023. URL [http://arxiv.org/abs/2306.16388](http://arxiv.org/abs/2306.16388). arXiv:2306.16388 [cs].
* Alkhamissi et al. [2024] Badr Alkhamissi, Muhammad ElNokrashy, Mai AlKhamissi, and Mona Diab. Investigating Cultural Alignment of Large Language Models, February 2024. URL [http://arxiv.org/abs/2402.13231](http://arxiv.org/abs/2402.13231). arXiv:2402.13231 [cs].
* Ryan et al. [2024] Michael J. Ryan, William Held, and Diyi Yang. Unintended Impacts of LLM Alignment on Global Representation, February 2024. URL [http://arxiv.org/abs/2402.15018](http://arxiv.org/abs/2402.15018). arXiv:2402.15018 [cs].
* Sorensen et al. [2024] Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, Tim Althoff, and Yejin Choi. A Roadmap to Pluralistic Alignment, February 2024. URL [http://arxiv.org/abs/2402.05070](http://arxiv.org/abs/2402.05070). arXiv:2402.05070 [cs].
* Gabriel and Ghazavi [2021] Iason Gabriel and Vafa Ghazavi. The Challenge of Value Alignment: from Fairer Algorithms to AI Safety, January 2021. URL [http://arxiv.org/abs/2101.06060](http://arxiv.org/abs/2101.06060). arXiv:2101.06060 [cs].
* Stray [2020] Jonathan Stray. Aligning AI Optimization to Community Well-Being. _International Journal of Community Well-Being_, 3(4):443-463, December 2020. ISSN 2524-5309. doi: 10.1007/s42413-020-00086-3. URL [https://doi.org/10.1007/s42413-020-00086-3](https://doi.org/10.1007/s42413-020-00086-3).
* Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenny Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molbyogo, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Runga, Kalyan Saladi, Alan Scheiten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. LIama 2: Open Foundation and Fine-Tuned Chat Models, July 2023. URL [http://arxiv.org/abs/2307.09288](http://arxiv.org/abs/2307.09288). arXiv:2307.09288 [cs].
* Jiang et al. [2023] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7B, October 2023. URL [http://arxiv.org/abs/2310.06825](http://arxiv.org/abs/2310.06825). arXiv:2310.06825 [cs].
* Gift [1989] Audrey Gift. Visual Analogue Scales: Measurement of Subjective Phenomena. _Nursing Research_, 38(5):286, October 1989. ISSN 0029-6562. URL [https://journals.lww.com/nursingresearchonline/citation/1989/09000/visual_analogue_scales_measurement_of_subjective.6.aspx?casa_token=a0_mhu6sQyEAAAAAAMA:906v3LLFR.ZeutMmw1WTDebC4T_Je8nE_dS4M_qu96DJC6_gR8R8o3715Rb2qwrw5zSexya6bQbmQsp0JLfY8UXSrf](https://journals.lww.com/nursingresearchonline/citation/1989/09000/visual_analogue_scales_measurement_of_subjective.6.aspx?casa_token=a0_mhu6sQyEAAAAAAMA:906v3LLFR.ZeutMmw1WTDebC4T_Je8nE_dS4M_qu96DJC6_gR8R8o3715Rb2qwrw5zSexya6bQbmQsp0JLfY8UXSrf).
* Kiela et al. [2020] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amampreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Steentorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. Dynabench: Rethinking Benchmarking in NLP. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 4110-4124, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.324. URL [https://aclanthology.org/2021.naacl-main.324](https://aclanthology.org/2021.naacl-main.324).
* Thrush et al. [2022] Tristan Thrush, Kushal Tirumala, Anmol Gupta, Max Bartolo, Pedro Rodriguez, Tariq Kane, William Gaviria Rojas, Peter Mattson, Adina Williams, and Douwe Kiela. Dynatask: A Framework for Creating Dynamic AI Benchmark Tasks. In Valerio Basile, Zornitsa Kozareva, and Sanja Stajner, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations_, pages 174-181, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-demo.17. URL [https://aclanthology.org/2022.acl-demo.17](https://aclanthology.org/2022.acl-demo.17).
* Aitken [1969] R. C. Aitken. Measurement of feelings using visual analogue scales. _Proceedings of the Royal Society of Medicine_, 62(10):989-993, October 1969. ISSN 0035-9157. URL [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1810824/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1810824/).
* Singhal et al. [2023] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A Long Way to Go: Investigating Length Correlations in RLHF, October 2023. URL [http://arxiv.org/abs/2310.03716](http://arxiv.org/abs/2310.03716). arXiv:2310.03716 [cs].
* Campello et al. [2013] Ricardo J. G. B. Campello, Davoud Moulavi, and Joerg Sander. Density-Based Clustering Based on Hierarchical Density Estimates. In Jian Pei, Vincent S. Tseng, Longbing Cao, Hiroshi Motoda, and Guandong Xu, editors, _Advances in Knowledge Discovery and Data Mining_, Lecture Notes in Computer Science, pages 160-172, Berlin, Heidelberg, 2013. Springer. ISBN 978-3-642-37456-2. doi: 10.1007/978-3-642-37456-2_14.
* Negahban et al. [2012] Sahand Negahban, Sewoong Oh, and Devavrat Shah. Iterative ranking from pair-wise comparisons. In _Advances in Neural Information Processing Systems_, volume 25. Curran Associates, Inc., 2012. URL [https://papers.nips.cc/paper_files/paper/2012/hash/9adeb82ffb5444e81ifa0ce8adBafe7a-Abstract.html](https://papers.nips.cc/paper_files/paper/2012/hash/9adeb82ffb5444e81ifa0ce8adBafe7a-Abstract.html).
* Bana et al. [2021] Gergei Bana, Wojciech Jamroga, David Naccache, and Peter Y. A. Ryan. Convergence Voting: From Pairwise Comparisons to Consensus, March 2021. URL [http://arxiv.org/abs/2102.01995](http://arxiv.org/abs/2102.01995). arXiv:2102.01995 [cs].
* Page et al. [1999] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The PageRank Citation Ranking: Bringing Order to the Web., November 1999. URL [http://ilpubs.stanford.edu:8090/422/?doi=10.1.1.31.1768](http://ilpubs.stanford.edu:8090/422/?doi=10.1.1.31.1768). Type: Techreport.
* Zheng et al. [2023] Liamin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. _Advances in Neural Information Processing Systems_, 36:46595-46623, December 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html](https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html).
* Noble [2018] Safiya Umoja Noble. _Algorithms of oppression: how search engines reinforce racism_. New York University Press, New York, 2018. ISBN 978-1-4798-4994-9 978-1-4798-3724-3.
* A Century of Participation in Four Stories_. The University of Chicago press, Chicago (Ill.) London, 2019. ISBN 978-0-226-66662-4 978-0-226-66676-1.
* Criado-Perez [2019] Caroline Criado-Perez. _Invisible women: exposing data bias in a world designed for men_. Chatto & Windus, London, 2019. ISBN 978-1-78474-172-3 978-1-78474-292-8.
* Sloane et al. [2022] Mona Sloane, Emanuel Moss, Olaitan Awomolo, and Laura Forlano. Participation Is not a Design Fix for Machine Learning. In _Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization_, EAAMO '22, pages 1-6, New York, NY, USA, October 2022. Association for Computing Machinery. ISBN 978-1-4503-9477-2. doi: 10.1145/3551624.3555285. URL [https://dl.acm.org/doi/10.1145/3551624.3555285](https://dl.acm.org/doi/10.1145/3551624.3555285).
* Greene et al. [2023] Travis Greene, Copenhagen Business School, Galit Shmueli, National Tsing Hua University, Soumya Ray, and National Tsing Hua University. Taking the Person Seriously: Ethically Aware IS Research in the Era of Reinforcement Learning-Based Personalization. _Journal of the Association for Information Systems_, 24(6):1527-1561, 2023. ISSN 15369323. doi: 10.17705/1jais.00800. URL [https://aisel.aismet.org/jais/vol24/iss6/6/](https://aisel.aismet.org/jais/vol24/iss6/6/).

* Nakano et al. [2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eleumdou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. WebGPT: Browser-assisted question-answering with human feedback. December 2021. URL [http://arxiv.org/abs/2112.09332v3](http://arxiv.org/abs/2112.09332v3).
* Birhane et al. [2021] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and malignant stereotypes. _arXiv preprint arXiv:2110.01963_, 2021.
* Benjamin [2019] Ruha Benjamin. _Race After Technology: Abolitionist Tools for the New Jim Code_. John Wiley & Sons, July 2019. ISBN 978-1-5095-2643-7.
* Blodgett et al. [2020] Su Lin Blodgett, Solon Barocas, Hal Daume III, and Hanna Wallach. Language (Technology) is Power: A Critical Survey of "Bias" in NLP. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 5454-5476, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.485. URL [https://aclanthology.org/2020.acl-main.485](https://aclanthology.org/2020.acl-main.485).
* Hershcovich et al. [2022] Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, and Anders Sogaard. Challenges and Strategies in Cross-Cultural NLP. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 6997-7013, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.482. URL [https://aclanthology.org/2022.acl-long.482](https://aclanthology.org/2022.acl-long.482).
* Wang et al. [2024] Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang, Zhaopeng Tu, and Michael R. Lyu. Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models, February 2024. URL [http://arxiv.org/abs/2310.12481](http://arxiv.org/abs/2310.12481). arXiv:2310.12481 [cs] version: 2.
* Gururangan et al. [2022] Suchin Gururangan, Dallas Card, Sarah K. Dreier, Emily K. Gade, Leroy Z. Wang, Zeyu Wang, Luke Zettlemoyer, and Noah A. Smith. Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection, January 2022. URL [http://arxiv.org/abs/2201.10474](http://arxiv.org/abs/2201.10474). arXiv:2201.10474 [cs].
* Dzieza [2023] Josh Dzieza. Inside the AI Factory, June 2023. URL [https://www.theverge.com/features/23764584/ai-artificial-intelligence-data-notation-labor-scale-surge-remotasks-openai-chatbots](https://www.theverge.com/features/23764584/ai-artificial-intelligence-data-notation-labor-scale-surge-remotasks-openai-chatbots).
* Mohamed et al. [2020] Shakir Mohamed, Marie-Therese Png, and William Isaac. Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence. _Philosophy & Technology_, 33(4):659-684, December 2020. ISSN 2210-5441. doi: 10.1007/s13347-020-00405-8. URL [https://doi.org/10.1007/s13347-020-00405-8](https://doi.org/10.1007/s13347-020-00405-8).
* Airoldi [2022] Massimo Airoldi. _Machine habitus: toward a sociology of algorithms_. Polity Press, Cambridge ; Medford, MA, 2022. ISBN 978-1-5095-4327-4 978-1-5095-4328-1. OCLC: on1247827618.
* Talat et al. [2022] Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, and Adina Williams. On the Machine Learning of Ethical Judgments from Natural Language. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 769-779, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.56. URL [https://aclanthology.org/2022.naacl-main.56](https://aclanthology.org/2022.naacl-main.56).
* Diaz et al. [2022] Mark Diaz, Ian Kivlichan, Rachel Rosen, Dylan Baker, Razvan Amironesei, Vinodkumar Prabhakaran, and Emily Denton. CrowdWorkSheets: Accounting for Individual and Collective Identities Underlying Crowdsourced Dataset Annotation. In _2022 ACM Conference on Fairness, Accountability, and Transparency_, FACoC '22, pages 2342-2351, New York, NY, USA, June 2022. Association for Computing Machinery. ISBN 978-1-4503-9352-2. doi: 10.1145/353146.3534647. URL [https://doi.org/10.1145/3531146.3534647](https://doi.org/10.1145/3531146.3534647).
* Bender and Friedman [2018] Emily M. Bender and Batya Friedman. Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science. _Transactions of the Association for Computational Linguistics_, 6:587-604, 2018. doi: 10.1162/tacl_a_00041. URL [https://aclanthology.org/Q18-1041](https://aclanthology.org/Q18-1041). Place: Cambridge, MA Publisher: MIT Press.
* FAT* '19_, pages 220-229, 2019. doi: 10.1145/3287560.3287596. URL [http://arxiv.org/abs/1810.03993](http://arxiv.org/abs/1810.03993). arXiv: 1810.03993.
* Gebru et al. [2021] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daume Iii, and Kate Crawford. Datasheets for datasets. _Communications of the ACM_, 64(12):86-92, December 2021. ISSN 15577317. doi: 10.1145/3458723. Publisher: Association for Computing Machinery.
* Dayani et al. [2022] Aida Mostafazadeh Davani, Mark Diaz, and Vinodkumar Prabhakaran. Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations. _Transactions of the Association for Computational Linguistics_, 10:92-110, January 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00449. URL [https://doi.org/10.1162/tacl_a_00449](https://doi.org/10.1162/tacl_a_00449).
* Gordon et al. [2022] Mitchell L. Gordon, Michelle S. Lam, Joon Sung Park, Kayur Patel, Jeff Hancock, Tatsunori Hashimoto, and Michael S. Bernstein. Jury Learning: Integrating Dissenting Voices into Machine Learning Models. In _Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems_, CHI '22, pages 1-19, New York, NY, USA, April 2022. Association for Computing Machinery. ISBN 978-1-4503-9157-3. doi: 10.1145/3491102.3502004. URL [https://doi.org/10.1145/3491102.3502004](https://doi.org/10.1145/3491102.3502004).
* Plank et al. [2014] Barbara Plank, Dirk Hovy, and Anders Sogaard. Learning part-of-speech taggers with inter-annotator agreement loss. In Shuly Wintner, Sharon Goldwater, and Stefan Riezler, editors, _Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics_, pages 742-751, Gothenburg, Sweden, April 2014. Association for Computational Linguistics. doi: 10.3115/v1/E14-1078. URL [https://aclanthology.org/E14-1078](https://aclanthology.org/E14-1078).
* Nie et al. [2020] Yixin Nie, Xiang Zhou, and Mohit Bansal. What Can We Learn from Collective Human Opinions on Natural Language Inference Data? In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 9131-9143, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.734. URL [https://aclanthology.org/2020.emnlp-main.734](https://aclanthology.org/2020.emnlp-main.734).
* Wich et al. [2021] Maximilian Wich, Christian Widmer, Gerhard Hagger, and Georg Groh. Investigating Annotator Bias in Abusive Language Datasets. In Ruslan Mitkov and Galia Angelova, editors, _Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)_, pages 1515-1525, Held Online, September 2021. INCOMA Ltd. URL [https://aclanthology.org/2021.ranlp-1.170](https://aclanthology.org/2021.ranlp-1.170).
* Sap et al. [2022] Maarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou, Yejin Choi, and Noah A. Smith. Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5884-5906, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.431. URL [https://aclanthology.org/2022.naacl-main.431](https://aclanthology.org/2022.naacl-main.431).
* Goyal et al. [2022] Nitesh Goyal, Ian D. Kivlichan, Rachel Rosen, and Lucy Vasserman. Is Your Toxicity My Toxicity? Exploring the Impact of Rater Identity on Toxicity Annotation. _Proceedings of the ACM on Human-Computer Interaction_, 6(CSCW2):363:1-363:28, November 2022. doi: 10.1145/3555088. URL [https://dl.acm.org/doi/10.1145/3555088](https://dl.acm.org/doi/10.1145/3555088).
* Aroyo et al. [2023] Lora Aroyo, Alex S. Taylor, Mark Diaz, Christopher M. Homan, Alicia Parrish, Greg Serapio-Garcia, Vinodkumar Prabhakaran, and Ding Wang. DICES Dataset: Diversity in Conversational AI Evaluation for Safety, June 2023. URL [http://arxiv.org/abs/2306.11247](http://arxiv.org/abs/2306.11247). arXiv:2306.11247 [cs].
* Henrich et al. [2010] Joseph Henrich, Steven J. Heine, and Ara Norenzayan. Most people are not WEIRD. _Nature_, 466(7302):29-29, July 2010. ISSN 1476-4687. doi: 10.1038/466029a. URL [https://www.nature.com/articles/466029a](https://www.nature.com/articles/466029a). Number: 7302 Publisher: Nature Publishing Group.
* Urbina and Ruiz-Villaverde [2019] Dante A. Urbina and Alberto Ruiz-Villaverde. A Critical Review of Homo Economicus from Five Approaches. _The American Journal of Economics and Sociology_, 78(1):63-93, 2019. ISSN 1536-7150. doi: 10.1111/ajes.12258. URL [https://onlinelibrary.wiley.com/doi/abs/10.1111/ajes.12258](https://onlinelibrary.wiley.com/doi/abs/10.1111/ajes.12258). _eprint: [https://onlinelibrary.wiley.com/doi/pdf/10.1111/ajes.12258](https://onlinelibrary.wiley.com/doi/pdf/10.1111/ajes.12258).
* Apicella et al. [2020] Coren Apicella, Ara Norenzayan, and Joseph Henrich. Beyond WEIRD: A review of the last decade and a look ahead to the global laboratory of the future. _Evolution and Human Behavior_, 41(5):319-329, September 2020. ISSN 1090-5138. doi: 10.1016/j.evolhumbehav.2020.07.015. URL [https://www.sciencedirect.com/science/article/pii/S1090513820300957](https://www.sciencedirect.com/science/article/pii/S1090513820300957).
* Ng and Russell [2000] Andrew Ng and Stuart J. Russell. Algorithms for Inverse Reinforcement Learning. 2000.

* Christiano et al. [2017] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep Reinforcement Learning from Human Preferences. In _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL [https://proceedings.neurips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0c4e49-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0c4e49-Abstract.html).
* Ziegler et al. [2019] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-Tuning Language Models from Human Preferences. September 2019. URL [http://arxiv.org/abs/1909.08593v2](http://arxiv.org/abs/1909.08593v2).
* Mirkin and Meunier [2015] Shachar Mirkin and Jean-Luc Meunier. Personalized machine translation: Predicting translational preferences. In _Proceedings of the 2015 conference on empirical methods in natural language processing_, pages 2019-2025, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1238. URL [https://aclanthology.org/D15-1238](https://aclanthology.org/D15-1238).
* Nguyen et al. [2017] Khanh Nguyen, Hal Daume III, and Jordan Boyd-Graber. Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pages 1464-1474, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1153. URL [https://aclanthology.org/D17-1153](https://aclanthology.org/D17-1153).
* Kreutzer et al. [2017] Julia Kreutzer, Artem Sokolov, and Stefan Riezler. Bandit Structured Prediction for Neural Sequence-to-Sequence Learning. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1503-1513, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1138. URL [https://aclanthology.org/P17-1138](https://aclanthology.org/P17-1138).
* Walker [2000] Marilyn A Walker. An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email. _Journal of Artificial Intelligence Research_, 12:387-416, 2000.
* Schatzmann et al. [2006] Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and Steve Young. A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies. _The knowledge engineering review_, 21(2):97-126, 2006.
* Su et al. [2016] Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Maria Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, and Steve J. Young. Continuously learning neural dialogue management. abs/1606.02689, 2016. URL [http://arxiv.org/abs/1606.02689](http://arxiv.org/abs/1606.02689).
* Li et al. [2017] Jiwei Li, Alexander H. Miller, Sumit Chopra, Marc'Aurelio Ranzato, and Jason Weston. Dialogue learning with human-in-the-loop. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017. URL [https://openreview.net/forum?id=HJgXCV9xx](https://openreview.net/forum?id=HJgXCV9xx).
* Jaques et al. [2019] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog, July 2019. URL [http://arxiv.org/abs/1907.00456](http://arxiv.org/abs/1907.00456). arXiv:1907.00456 [cs, stat].
* Jaques et al. [2020] Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Human-centric dialog training via offline reinforcement learning. In _Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)_, pages 3985-4003, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.327. URL [https://aclanthology.org/2020.emnlp-main.327](https://aclanthology.org/2020.emnlp-main.327).
* Glaese et al. [2022] Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Sonia Mokra, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements. September 2022. URL [http://arxiv.org/abs/2209.14375v1](http://arxiv.org/abs/2209.14375v1).
* Scheurer et al. [2022] Jeremy Scheurer, Jon Ander Campos, Jun Shem Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training Language Models with Language Feedback, November 2022. URL [http://arxiv.org/abs/2204.14146](http://arxiv.org/abs/2204.14146). arXiv:2204.14146 [cs].

* Askell et al. [2021] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhase, Zae Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A General Language Assistant as a Laboratory for Alignment, December 2021. URL [http://arxiv.org/abs/2112.00861](http://arxiv.org/abs/2112.00861). arXiv:2112.00861 [cs].
* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms, August 2017. URL [http://arxiv.org/abs/1707.06347](http://arxiv.org/abs/1707.06347). arXiv:1707.06347 [cs].
* Williams [1992] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine Learning_, 8(3):229-256, May 1992. ISSN 1573-0565. doi: 10.1007/BF00992696. URL [https://doi.org/10.1007/BF00992696](https://doi.org/10.1007/BF00992696).
* Ahmadian et al. [2022] Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustin, and Sara Hooker. Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs, February 2024. URL [http://arxiv.org/abs/2402.14740](http://arxiv.org/abs/2402.14740). arXiv:2402.14740 [cs] version: 1.
* Zhou et al. [2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less Is More for Alignment, May 2023. URL [http://arxiv.org/abs/2305.11206](http://arxiv.org/abs/2305.11206). arXiv:2305.11206 [cs].
* Menick et al. [2022] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, and Nat McAleese. Teaching language models to support answers with verified quotes, March 2022. URL [http://arxiv.org/abs/2203.11147](http://arxiv.org/abs/2203.11147). arXiv:2203.11147 [cs].
* Thoppilan et al. [2022] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokou, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulse Doshi, Reneitilo Delos Santos, Toju Duke, Johnny Sorsaker, Ben Zevenberger, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. LaMDA: Language Models for Dialog Applications, February 2022. URL [http://arxiv.org/abs/2201.08239](http://arxiv.org/abs/2201.08239). arXiv:2201.08239 [cs].
* Lambert et al. [2024] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, L. J. Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. RewardBench: Evaluating Reward Models for Language Modeling, March 2024. URL [http://arxiv.org/abs/2403.13787](http://arxiv.org/abs/2403.13787). arXiv:2403.13787 [cs].
* Ethayarajh et al. [2024] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. KTO: Model Alignment as Prospect Theoretic Optimization, February 2024. URL [http://arxiv.org/abs/2402.01306](http://arxiv.org/abs/2402.01306). arXiv:2402.01306 [cs].
* Stanford [2023] StanfordNLP. Stanford Human Preferences Dataset, September 2023. URL [https://huggingface.co/datasets/stanfordnlp/SHP](https://huggingface.co/datasets/stanfordnlp/SHP).
* Lambert et al. [2023] Nathan Lambert, Lewis Tunstall, Nazneen Rajani, and Tristan Thrush. HuggingFace H4 Stack Exchange Preference Dataset, 2023. URL [https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences).
* Agnew et al. [2024] William Agnew, A. Stevie Bergman, Jennifer Chien, Mark Diaz, Seliem El-Sayed, Jaylen Pittman, Shakir Mohamed, and Kevin R. McKee. The illusion of artificial inclusion, February 2024. URL [http://arxiv.org/abs/2401.08572](http://arxiv.org/abs/2401.08572). arXiv:2401.08572 [cs].
* Dubois et al. [2024] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback, January 2024. URL [http://arxiv.org/abs/2305.14387](http://arxiv.org/abs/2305.14387). arXiv:2305.14387 [cs].
* Guo et al. [2024] Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment, February 2024. URL [http://arxiv.org/abs/2402.19085](http://arxiv.org/abs/2402.19085). arXiv:2402.19085 [cs, eess].

* Zheng et al. [2024] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversion Dataset, March 2024. URL [http://arxiv.org/abs/2309.11998](http://arxiv.org/abs/2309.11998). arXiv:2309.11998 [cs].
* Zhao et al. [2023] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. (InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild. October 2023. URL [https://openreview.net/forum?id=B1&u72R1bM](https://openreview.net/forum?id=B1&u72R1bM).
* Democraticizing Large Language Model Alignment, October 2023. URL [http://arxiv.org/abs/2304.07327](http://arxiv.org/abs/2304.07327). arXiv:2304.07327 [cs].
* Ganguli et al. [2022] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned, November 2022. URL [http://arxiv.org/abs/2209.07858](http://arxiv.org/abs/2209.07858). arXiv:2209.07858 [cs].
* Singh et al. [2024] Shivalika Singh, Freddie Vargus, Daniel Dsouza, Borje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Devidas Matacinus, Laura O'Mahony, Mike Zhang, Ramith Hetiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, Oshan Mudamayake, Zaid Alyafaei, Vu Minh Chien, Sebastian Ruder, Surya Guhikonda, Emad A. Alghamdi, Sebastiaan Gehrmann, Niklas Mueninghoff, Max Bartolo, Julia Kreutzer, Ahmet Ustun, Marzieh Fadaee, and Sara Hooker. Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning, February 2024. URL [http://arxiv.org/abs/2402.06619](http://arxiv.org/abs/2402.06619). arXiv:2402.06619 [cs].
* [116] The Alan Turing Institute and The Ada Lovelace Institute. How do people feel about AI? A nationally representative survey of public attitudes to artificial intelligence in Britain. Technical report, 2023. URL [https://attitudestoai.uk/assets/documents/Ada-Lovelace-Institute-The-Alan-Turing-Institute-How-do-people-feel-about-AI.pdf](https://attitudestoai.uk/assets/documents/Ada-Lovelace-Institute-The-Alan-Turing-Institute-How-do-people-feel-about-AI.pdf).
* Mun et al. [2024] Jimin Mun, Liwei Jiang, Jenny Liang, Inyoung Cheong, Nicole DeCario, Yejin Choi, Tadayoshi Kohno, and Maarten Sap. Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits, March 2024. URL [http://arxiv.org/abs/2403.14791](http://arxiv.org/abs/2403.14791). arXiv:2403.14791 [cs].
* Chang et al. [2024] Samuel Chang, Estelle Ciesla, Michael Finch, James Fishkin, Lodewijk Gelauff, Ashish Goel, Ricky Hernandez Marquez, Shoaib Mohammed, and Alice Siu. Meta Community Forum: Results Analysis. Technical report, Deliberative Democracy Lab, Stanford University, April 2024.
* Bergman et al. [2024] Stevie Bergman, Nahema Marchal, John Mellor, Shakir Mohamed, Iason Gabriel, and William Isaac. STELA: a community-centred approach to norm elicitation for AI alignment. _Scientific Reports_, 14(1):6616, March 2024. ISSN 2045-2322. doi: 10.1038/s41598-024-56648-4. URL [https://www.nature.com/articles/s41598-024-56648-4](https://www.nature.com/articles/s41598-024-56648-4). Publisher: Nature Publishing Group.
* Silberzahn et al. [2018] R. Silberzahn, E. L. Uhlmann, D. P. Martin, P. Anselmi, F. Aust, E. Awtrey, S. Bahnik, F. Bai, C. Bannard, E. Bonnier, R. Carlsson, F. Cheung, G. Christensen, R. Clay, M. A. Craig, A. Dalla Rosa, L. Dam, M. H. Evans, I. Flores Cervantes, N. Fong, M. Gamez-Djokic, A. Glenz, S. Gordon-McKeon, T. J. Heaton, K. Hederos, M. Heene, A. J. Hofelich Mohr, F. Hogden, K. Hui, M. Johannesson, J. Kalodimos, E. Kaszubowski, D. M. Kennedy, R. Lei, T. A. Lindsay, S. Liverani, C. R. Madan, D. Molden, E. Molleman, R. D. Morey, L. B. Mulder, B. R. Nijstad, N. G. Pope, B. Pope, J. M. Prenoveau, F. Rink, E. Robusto, H. Roderique, A. Sandberg, E. Schluter, F. D. Schonbrodt, M. F. Sherman, S. A. Sommer, K. Sotak, S. Spain, C. Sporlein, T. Stafford, L. Stefanutti, S. Tauber, J. Ullrich, M. Vianello, E.-J. Wagemmakers, M. Witkowiak, S. Yoon, and B. A. Nosek. Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. _Advances in Methods and Practices in Psychological Science_, 1(3):337-356, September 2018. ISSN 2515-2459. doi: 10.1177/2515245917747646. URL [https://doi.org/10.1177/2515245917747646](https://doi.org/10.1177/2515245917747646). Publisher: SAGE Publications Inc.
* Tasioulas [2022] John Tasioulas. Artificial Intelligence, Humanistic Ethics. _Daedalus_, 151(2):232-243, May 2022. ISSN 0011-5266. doi: 10.1162/daed_a_01912. URL [https://doi.org/10.1162/daed_a_01912](https://doi.org/10.1162/daed_a_01912).

* [122] Diane Proudfoot. Anthropomorphism and AI: Turing's much misunderstood imitation game. _Artificial Intelligence_, 175(5):950-957, April 2011. ISSN 0004-3702. doi: 10.1016/j.artint.2011.01.006. URL [https://www.sciencedirect.com/science/article/pii/S000437021100018X](https://www.sciencedirect.com/science/article/pii/S000437021100018X).
* [123] David Watson. The Rhetoric and Reality of Anthropomorphism in Artificial Intelligence. _Minds and Machines_, 29(3):417-440, September 2019. ISSN 1572-8641. doi: 10.1007/s11023-019-09506-6. URL [https://doi.org/10.1007/s11023-019-09506-6](https://doi.org/10.1007/s11023-019-09506-6).
* [124] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Taxonomy of Risks posed by Language Models. In _2022 ACM Conference on Fairness, Accountability, and Transparency_, pages 214-229, Seoul Republic of Korea, June 2022. ACM. ISBN 978-1-4503-9352-2. doi: 10.1145/3531146.3533088. URL [https://dl.acm.org/doi/10.1145/3531146.3533088](https://dl.acm.org/doi/10.1145/3531146.3533088).
* [125] Gavin Abercrombie, Amanda Cercas Curry, Tanvi Dinkar, Verena Rieser, and Zeerak Talat. Mirages. On Anthropomorphism in Dialogue Systems. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 4776-4790, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.290. URL [https://aclanthology.org/2023.emnlp-main.290](https://aclanthology.org/2023.emnlp-main.290).
* [126] Myra Cheng, Kristina Gligoric, Tiziano Piccardi, and Dan Jurafsky. AnthroScore: A Computational Linguistic Measure of Anthropomorphism, February 2024. URL [http://arxiv.org/abs/2402.02056](http://arxiv.org/abs/2402.02056). arXiv:2402.02056 [cs].
* [127] Sian Gooding and Hassan Mansoor. The Impact of Preference Agreement in Reinforcement Learning from Human Feedback: A Case Study in Summarization, November 2023. URL [http://arxiv.org/abs/2311.04919](http://arxiv.org/abs/2311.04919). arXiv:2311.04919 [cs].
* [128] Catherine D'Ignazio and Lauren F. Klein. _Data Feminism_. The MIT Press, March 2020. ISBN 978-0-262-35852-1. doi: 10.7551/mitpress/11805.001.0001. URL [https://direct.mit.edu/books/book/4660/Data-Feminism](https://direct.mit.edu/books/book/4660/Data-Feminism).
* [129] Abeba Birhane. Algorithmic injustice: a relational ethics approach. _Patterns_, 2(2):100205, February 2021. ISSN 26663899. doi: 10.1016/j.patter.2021.100205. URL [https://linkinghub.elsevier.com/retrieve/pii/S2666389921000155](https://linkinghub.elsevier.com/retrieve/pii/S2666389921000155).
* [130] Yijiang River Dong, Tiancheng Hu, and Nigel Collier. Can LLM be a Personalized Judge?, June 2024. URL [http://arxiv.org/abs/2406.11657](http://arxiv.org/abs/2406.11657). arXiv:2406.11657 [cs].
* [131] Billy Perrigo. Inside OpenAI's Plan to Make AI More 'Democratic', February 2024. URL [https://time.com/6684266/openai-democracy-artificial-intelligence/](https://time.com/6684266/openai-democracy-artificial-intelligence/).
* [132] Fabio Crameri, Grace E. Shephard, and Philip J. Heron. The misuse of colour in science communication. _Nature Communications_, 11(1):5444, October 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-19160-7. URL [https://www.nature.com/articles/s41467-020-19160-7](https://www.nature.com/articles/s41467-020-19160-7). Number: 1 Publisher: Nature Publishing Group.
* [133] Nenad Tomasev, Kevin R. McKee, Jackie Kay, and Shakir Mohamed. Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities. In _Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society_, AIES '21, pages 254-265, New York, NY, USA, July 2021. Association for Computing Machinery. ISBN 978-1-4503-8473-5. doi: 10.1145/3461702.3462540. URL [https://doi.org/10.1145/3461702.3462540](https://doi.org/10.1145/3461702.3462540).
* [134] Tom Hosking, Phil Blunsom, and Max Bartolo. Human Feedback is not Gold Standard, January 2024. URL [http://arxiv.org/abs/2309.16349](http://arxiv.org/abs/2309.16349). arXiv:2309.16349 [cs].
* [135] Amos Tversky and Itamar Simonson. Context-Dependent Preferences. _Management Science_, 39(10):1179-1189, October 1993. ISSN 0025-1909. doi: 10.1287/mnsc.39.10.1179. URL [https://pubsonline.informs.org/doi/abs/10.1287/mnsc.39.10.1179](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.39.10.1179). Publisher: INFORMS.
* [136] Boaz Shmueli, Jan Fell, Soumya Ray, and Lun-Wei Ku. Beyond fair pay: Ethical implications of NLP crowdsourcing. In _Association for Computational Linguistics (ACL)_, pages 3758-3769, April 2021. URL [https://arxiv.org/abs/2104.10097v1](https://arxiv.org/abs/2104.10097v1). arXiv: 2104.10097 Publisher: tex.arxivid: 2104.10097.

* [137] Lisa Posch, Armim Bleier, Fabian Flock, Clemens M. Lechner, Katharina Kinder-Kurlanda, Denis Helic, and Markus Strohmaier. Characterizing the Global Crowd Workforce: A Cross-Country Comparison of Crowdworker Demographics. _Human Computation_, 9(1), August 2022. ISSN 2330-8001. doi: 10.15346/hc.v9i1.106. URL [http://arxiv.org/abs/1812.05948](http://arxiv.org/abs/1812.05948). arXiv:1812.05948 [cs].
* [138] Derek A. Albert and Daniel Smilek. Comparing attentional disengagement between Prolific and MTurk samples. _Scientific Reports_, 13(1):20574, November 2023. ISSN 2045-2322. doi: 10.1038/s41598-023-46048-5. URL [https://www.nature.com/articles/s41598-023-46048-5](https://www.nature.com/articles/s41598-023-46048-5). Publisher: Nature Publishing Group.
* [139] Gemini Team. Gemini: A Family of Highly Capable Multimodal Models, December 2023. URL [http://arxiv.org/abs/2312.11805](http://arxiv.org/abs/2312.11805). arXiv:2312.11805 [cs].
* [140] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devenda Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mixtral of Experts, January 2024. URL [http://arxiv.org/abs/2401.04088](http://arxiv.org/abs/2401.04088). arXiv:2401.04088 [cs].
* [141] Anthropic. Introducing the next generation of Claude, April 2024. URL [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family).
* [142] Cohere. Command R, April 2024. URL [https://docs.cohere.com/docs/command-r](https://docs.cohere.com/docs/command-r).
* [143] MetaAI. Introducing Meta Llama 3: The most capable openly available LLM to date, April 2024. URL [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/).
* [144] Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. Whose Opinions Do Language Models Reflect?, March 2023. URL [http://arxiv.org/abs/2303.17548](http://arxiv.org/abs/2303.17548). arXiv:2303.17548 [cs].
* [145] Fredrik Barth. _Ethnic Groups and Boundaries: The Social Organization of Culture Difference_. Waveland Press, March 1998. ISBN 978-1-4786-0795-3. Google-Books-ID: QaAQAAAAQBAJ.
* [146] Katarzyna Hamer, Sam McFarland, Barbara Czarnecka, Agnieszka Golinska, Liliana Manrique Cadena, Magdalena Luzniak Piecha, and Tomasz Jukowski. What Is an "Ethnic Group" in Ordinary People's Eyes? Different Ways of Understanding It Among American, British, Mexican, and Polish Respondents. _Cross-Cultural Research_, 54(1):28-72, February 2020. ISSN 1069-3971. doi: 10.1177/1069397118816939. URL [https://doi.org/10.1177/1069397118816939](https://doi.org/10.1177/1069397118816939). Publisher: SAGE Publications Inc.
* [147] Karen L. Suyemoto, Micaela Curley, and Shruti Mukkamala. What Do We Mean by "Ethnicity" and "Race"? A Consensual Qualitative Research Investigation of Colloquial Understandings. _Genealogy_, 4(3):81, September 2020. ISSN 2313-5778. doi: 10.3390/genealogy4030081. URL [https://www.mdpi.com/2313-5778/4/3/81](https://www.mdpi.com/2313-5778/4/3/81). Number: 3 Publisher: Multidisciplinary Digital Publishing Institute.
* [148] Laurence R. Iannaccone. Introduction to the Economics of Religion. _Journal of Economic Literature_, 36(3):1465-1495, 1998. ISSN 0022-0515. URL [https://www.jstor.org/stable/2564806](https://www.jstor.org/stable/2564806). Publisher: American Economic Association.
* [149] Gilat Levy and Ronny Razin. Religious Beliefs, Religious Participation, and Cooperation. _American Economic Journal: Microeconomics_, 4(3):121-151, August 2012. ISSN 1945-7669, 1945-7685. doi: 10.1257/mic.4.3.121. URL [https://pubs.aeaweb.org/doi/10.1257/mic.4.3.121](https://pubs.aeaweb.org/doi/10.1257/mic.4.3.121).
* [150] Ellen Dingemans and Erik Van Ingen. Does Religion Breed Trust? A Cross-National Study of the Effects of Religious Involvement, Religious Faith, and Religious Context on Social Trust. _Journal for the Scientific Study of Religion_, 54(4):739-755, 2015. ISSN 0021-8294. URL [https://www.jstor.org/stable/26651394](https://www.jstor.org/stable/26651394). Publisher: [Society for the Scientific Study of Religion, Wiley].
* [151] Hansong Zhang, Joshua N. Hook, Jennifer E. Farrell, David K. Mosher, Laura E. Captari, Steven P. Coomes, Daryl R. Van Tongeren, and Don E. Davis. Exploring Social Belonging and Meaning in Religious Groups. _Journal of Psychology and Theology_, 47(1):3-19, March 2019. ISSN 0091-6471. doi: 10.1177/0091647118806345. URL [https://doi.org/10.1177/0091647118806345](https://doi.org/10.1177/0091647118806345). Publisher: SAGE Publications Ltd.

* Saroglou et al. [2020] Vassilis Saroglou, Magali Clobert, Adam B. Cohen, Kathryn A. Johnson, Kevin L. Ladd, Matthieu Van Pachterbeke, Lucia Adamovaova, Joanna Bloogowska, Pierre-Yves Brandt, Cem Safak Cukur, Kwang Kuo Hwang, Anna Miglietta, Frosso Moti-Stefamidi, Antonio Munoz-Garcia, Sebastian Murken, Nicolas Roussiau, and Javier Tapia Valladares. Believing, Bonding, Behaving, and Belonging: The Cognitive, Emotional, Moral, and Social Dimensions of Religiousness across Cultures. _Journal of Cross-Cultural Psychology_, 51(7-8):551-575, September 2020. ISSN 0022-0221. doi: 10.1177/0022022120946488. URL [https://doi.org/10.1177/0022022120946488](https://doi.org/10.1177/0022022120946488). Publisher: SAGE Publications Inc.
* Prolific [2024] Prolific. Representative samples, February 2024. URL [https://researcher-help.prolific.com/hc/en-gb/articles/360019236753-Representative-samples](https://researcher-help.prolific.com/hc/en-gb/articles/360019236753-Representative-samples).
* Reimers and Gurevych [2019] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks, August 2019. URL [http://arxiv.org/abs/1908.10084](http://arxiv.org/abs/1908.10084). arXiv:1908.10084 [cs].
* McInnes et al. [2020] Leland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, September 2020. URL [http://arxiv.org/abs/1802.03426](http://arxiv.org/abs/1802.03426). arXiv:1802.03426 [cs, stat].
* Kazemi et al. [2022] Ashkan Kazemi, Kiran Garimella, Gautam Kishore Shahi, Devin Gaffney, and Scott A. Hale. Research note: Tiplines to uncover misinformation on encrypted platforms: A case study of the 2019 Indian general election on WhatsApp. _Harvard Kennedy School Misinformation Review_, January 2022. doi: 10.37016/mr-2020-91. URL [https://misinforeview.hks.harvard.edu/article/research-note-tiplines-to-uncover-misinformation-on-encrypted-platforms-a-case-study-of-the-2019-indian-general-election-on-whatsapp/](https://misinforeview.hks.harvard.edu/article/research-note-tiplines-to-uncover-misinformation-on-encrypted-platforms-a-case-study-of-the-2019-indian-general-election-on-whatsapp/).
* Hale [2022] Scott A. Hale. meedan/temporal_clustering, March 2022. URL [https://github.com/meedan/temporal_clustering/tree/main](https://github.com/meedan/temporal_clustering/tree/main).
* Emerson et al. [2017] Scott D. Emerson, Martin Guhn, and Anne M. Gadermann. Measurement invariance of the Satisfaction with Life Scale: reviewing three decades of research. _Quality of Life Research_, 26(9):2251-2264, September 2017. ISSN 1573-2649. doi: 10.1007/s11136-017-1552-2. URL [https://doi.org/10.1007/s11136-017-1552-2](https://doi.org/10.1007/s11136-017-1552-2).
* Roemer [1998] John E. Roemer. _Theories of distributive justice_. Harvard Univ. Press, Cambridge, Mass., 1. harvard univ. press paperback ed edition, 1998. ISBN 978-0-674-87920-1 978-0-674-87919-5.
* Bentham [1789] Jeremy Bentham. An Introduction to the Principles of Morals and Legislation. In J. H. Burns and H. L. A. Hart, editors, _The Collected Works of Jeremy Bentham: An Introduction to the Principles of Morals and Legislation_. Oxford University Press, January 1789. ISBN 978-0-19-820516-6. doi: 10.1093/osec/instance.00077240. URL [http://www.oxfordscholarlyeditions.com/view/10.1093/actrade/9780198205166.book.1/actrade-9780198205166-work-1](http://www.oxfordscholarlyeditions.com/view/10.1093/actrade/9780198205166.book.1/actrade-9780198205166-work-1).
* Lanctot et al. [2023] Marc Lanctot, Kate Larson, Yoram Bachrach, Luke Marris, Zun Li, Avishkar Bhoopchand, Thomas Anthony, Brian Tanner, and Anna Koop. Evaluating Agents using Social Choice Theory, December 2023. URL [http://arxiv.org/abs/2312.03121](http://arxiv.org/abs/2312.03121). arXiv:2312.03121 [cs] version: 2.
* Bouddir et al. [2023] Meriem Bouddir, Edward Kim, Beyza Ermis, Sara Hooker, and Marzieh Fadaee. Elo Uncovered: Robustness and Best Practices in Language Model Evaluation, November 2023. URL [http://arxiv.org/abs/2311.17295](http://arxiv.org/abs/2311.17295). arXiv:2311.17295 [cs].
* Negahban et al. [2017] Sahand Negahban, Sewoong Oh, and Devavrat Shah. Rank Centrality: Ranking from Pairwise Comparisons. _Operations Research_, 65(1):266-287, 2017. ISSN 0030-364X. URL [https://www.jstor.org/stable/26153541](https://www.jstor.org/stable/26153541). Publisher: INFORMS.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]. **The paper has two core sections. First, we describe the contributions of the dataset in SS 2. Then, we present three case studies in SS 3.** 2. Did you describe the limitations of your work? [Yes]. **We summarise limitations in SS 5, and provide an expanded discussion in our Data Statement App. B.**3. Did you discuss any potential negative societal impacts of your work? [Yes]. **We summarise societal impacts and ethical considerations in SS 5, including risks from harmful content, privacy violations and participation-washing (participants not sharing in profits of improved technologies). We expand on these points in our Data Statement (App. B).** 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]. **To summarise, we (i) pay fair living wages to all participants (App. B and App. J); (ii) we received IRB approval from the University of Oxford and collected informed consent from every participants (App. D); (iv) we check for PII and release all metadata (App. E); (v) we carefully consider licenses and provide a data clause with terms of use (App. C); (vi) we consider and discuss representativeness (App. L); and (vii) we provide detailed documentation of the dataset (App. B).**
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [NA] 2. Did you include complete proofs of all theoretical results? [NA]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] **Experiments for the three case studies presented in the paper are described in detail in the Supplementary Material. We also release all code needed to reproduce results on our Github** ([https://github.com/HannahKirk/prism-alignment](https://github.com/HannahKirk/prism-alignment)). 2. Did you specify all the training details (e.g. data splits, hyperparameters, how they were chosen)? [NA]. **We don't train any models per se, but do provide details for our analysis (e.g. hyperparameters for our clustering pipeline) in the Supplementary Material.** 3. Did you report error bars (e.g. with respect to the random seed after running experiments multiple times)? [Yes]. **For any bootstrapped analysis in the main paper (e.g. Fig. 4) or Supplementary Material (e.g. App. T), we provide error bars.** 4. Did you include the total amount of compute and the type of resources used (e.g. type of GPUs, internal cluster, or cloud provider)? [NA]. **We didn't train any compute intensive models. We accessed existing models via API (see App. P), which was made possible by grants and waived credit costs (as described in Disclosure of Funding). All analysis experiments were run locally on CPU.**
4. If you are using existing assets (e.g. code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes]. **We discuss licenses in App. C.** 3. Did you include any new assets either in the supplemental material or as a URL? [Yes]. **Our dataset and code are linked after the abstract, and at the start of the Supplementary material. The data can be accessed on Github at [https://github.com/HannahKirk/prism-alignment](https://github.com/HannahKirk/prism-alignment), and also on HuggingFace at [https://huggingface.co/datasets/HannahRoseKirk/prism-alignment](https://huggingface.co/datasets/HannahRoseKirk/prism-alignment). The dataset has a permanent DOI: 10.57967/hf/2113. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes]. **We discuss informed consent in the main paper (SS 2) and provide full details (including consent forms) in the Supplementary material (App. D).** 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]. **For every text utterances in our dataset, we conduct automated PII checks (which are then manually inspected) and a moderation classifier for harmful content. We provide full details of this process in App. E, and release all metadata alongside the data entries. We summarise ethical concerns and risks in the main paper (SS 5) and data statement (App. B).**
5. If you used crowdsourcing or conducted research with human subjects...

* Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes]. **Full text of the survey questions and interface instructions is presented in detailed code books (App. V). We also provide interface screenshots (App. Q).**
* Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [Yes]. **We communicate risks to participants in the informed consent form (App. D), for example, the risk of exposure to harmful content via external APIs to LLMs. We received IRB approval. We also describe participant risks in the main paper (SS 5) and data statement (App. B).**
* Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes]. **We discuss hourly wage (£9) and task completion time (70 minutes) in the main paper (SS 2). We provide additional detail in our data statement (App. B). We provide estimates of total amount spent on participant compensation in App. J.**

**Supplementary Material**

**Table of Contents**

**PART I: Dataset Details and Distributions**

* A Prism Data Access and Format
* B Prism Data Statement
* C Prism Data Clause
* D Informed Consent
* E Metadata Processing
* F Annotating Ethnicity, Religion and Gender
* G Participant Demographics
* H Participant Geographies
* I Participant LLM Usage and Familiarity
* J Screening and Recruitment Process
* K Conversation Type Rebalancing
* L Census Rebalancing
* M Text and N-Gram Analysis
* N Comparing Fine-Grained Preference Attributes
* O Score Distributions
* P Details of LLMs-in-the-loop
* Q Interface Screenshots
* PART II: Extended Case Study Details
* R Case Study IA: Topic Clustering and Regressions
* S Case Study IB: Local Neighbourhoods and Empirically-Fixed Contexts
* T Case Study II: Aggregating Preference Ratings to Model Ranks
* U Case Study III: Welfare Analysis
* PART III: Codebooks
* V Codebooks

## Appendix A Prism Data Access and Format

The data can be accessed on Github at [https://github.com/HannahKirk/prism-alignment](https://github.com/HannahKirk/prism-alignment), and also on HuggingFace at [https://huggingface.co/datasets/HannahRoseKirk/prism-alignment](https://huggingface.co/datasets/HannahRoseKirk/prism-alignment). The dataset has a permanent DOI: 10.57967/hf/2113.

There dataset is organised in two primary JSON lines files:

* **The Survey** (survey.jsonl): The survey where participants answer questions such as their stated preferences for LLM behaviours, their familarity with LLMs, a self-description and some basic demographics. Each row is a single participant in our dataset, identified by a user_id.
* **The Conversations** (conversations.jsonl): Each participants' multiple conversation trees with LLMs and associated feedback. Each row is a single conversation, identified by a conversation_id, that can be matched back to a participant's survey profile via the user_id. The conversation itself is stored as a list of dictionaries representing human and model turns in the conversation_history column, which broadly follows the format of widely used Chat APIs (see single entry schema on the next page).

Additionally, for ease of secondary analysis we provide a more granular and flattened format of the conversations data:

* model response
- score). Each row has an utterance_id that can be mapped back to the conversation data using conversation_id or the survey using user_id. The model responses and scores per each user input are in _long format_. Because of this format, the user inputs will be repeated for the set of model responses in a single interaction turn.

We also provide code for transforming the conversations to a _wide format_. That is, each row is now a single turn within a conversation. For the first interaction where up to four models respond, we have model_{a/b/c/d} as four distinct columns and score_{a/b/c/d} as another four columns. Note that for subsequent turns, the same model responds and there are only two responses so model/score_{c/d} will always be missing.

Finally, for every text instance in Prism, we provide metadata on the language detection, personal or private information (PII) detection and moderation flags. **The Metadata** is provided seperately to the main data files (metadata.jsonl).

We provide **codebooks** for **The Survey** (App. V.1), **The Conversations** (App. V.2), **The Utterances** (App. V.3) and **The Metadata** (App. V.4).

* [Prism Data Statement

We provide a data statement [69] to document the generation and provenance of Prism.

### Curation Rationale

The Prism Alignment Project, funded by a variety of academic and industry sources (see Disclosure of Funding), aims to diversify human feedback datasets. All participants are recruited via the Prolific platform. The sample is described in SS 2.3, with additional details in App. J. The primary purpose of the dataset is for academic research into how different people interact with LLMs and perceive their outputs. However, we do not prohibit the use of the dataset to develop, test and/or evaluate AI systems so long as usage complies with the dataset license (App. C.2).

### Language Variety

The language of human- or model-written text was not explicitly restricted to English. However, the task instructions were written English, and fluency in English was included as a screening filter. As a result of these factors, 99% of text instances are in English (see App. E for breakdowns per type of text instance and by other language). There is scope for wide social and regional variation even within a language. Given we have speakers residing in 38 countries (born in 75 countries), we likely have various forms of English, especially by level of fluency (see Tab. 5). Information about which varieties of English are represented is not available.

### Speaker Demographics

There are two sets of "speaker" roles in Prism: human participants and large language models (LLMs). Both roles contribute to the characteristics of the text utterances in the dataset.

Participant CharacteristicsWe provide full demographic breakdowns of participant characteristics in Tab. 5. We provide full geographic breakdowns in Tab. 8. Despite substantial improvements on sample diversity compared to early widely-used human feedback datasets (see Tab. 6, Tab. 7), Prism still skews White, Educated, and Western. This is partly driven by census-representative samples from the US and UK, which can be removed or downsampled for future research. Prism only contains participants sourced from one crowdworking platform (Prolific), so inherits sample biases from this narrow pool--for example, participants are active internet users, incentivised by hourly payment on a specific task that they self-select into.

Model CharacteristicsGiven fast-paced changes to the LLM landscape, Prism is designed to be as _model-agnostic_ as possible. We include 21 models from various different families, capabilities and sizes (for a summary see Tab. 21). 12/21 models are accessed via commercial APIs, and 9/21 are open-access via HuggingFace. Model-specific characteristics will affect the text characteristics, especially if they have already been alignment-tuned.

Models as ParticipantsThroughout the study we strongly requested that participants did not use LLMs to write their "human" responses, playing both to their integrity (please don't do it), their role in the research (we really need you to not do it), and their incentives (you won't be paid if you do it). We did not directly test nor implement tools to technologically prevent participants from using LLMs on their behalf. We randomly sample 25 instances from human-written texts: system strings and self-descriptions from the Survey; opening prompts and open feedback from the Conversations (\(n=100\)). An annotator (paper author) manually inspected these and labelled none as model-written text. For instances of sufficient length (46/100, >50 words), we recorded the predicted probability of AI-generated text from an LLM-text detector, where 76% had \(\leq 1\%\) score.10 For the remainder (\(n=11\)), a second annotator (paper author) gave a tie-break, labelling none as model-generated.

Footnote 10: The tool is developed by [https://sapling.ai/](https://sapling.ai/). LLM-detector tools are susceptible to misclassifications. For example, this feedback: “_It was good that it offered options and mentioned “options” rather than just suggesting one thing. It would have been better to state in the beginning how dietary requirements and preferences might play a big role in the decision what to cook for dinner. And also to point out how different cultures have different food traditions. Not everything is US based._” was flagged as 88.1% AI-generated, but the human annotators felt was strongly human-generated.

### Annotator Demographics

The "annotators" are "speakers"--the same human participants who answer the survey, interact with the LLMs, and provide structured and unstructured feedback. See App. B.3.

### Speech Situation

All participants were recruited via Prolific. They were paid PS9/hour. The survey was hosted on Qualtrics (www.qualtrics.com), and the conversations on Dynabench (www.dynabench.org).

All data was collected between 22nd November 2023 and 22nd December 2023. The time of the data collection period did affect the topics of discussion: for example, one topic concerns Christmas holiday celebrations while another discusses the Israel-Palestine Conflict.

The primary modality of Prism is written language, combined with structured ratings or structured survey data. The conversations between participants and LLMs happened _synchronously_ via live API connections with models in the backend of our interface. We have not edited or moderated any survey responses, participant prompts or model responses. All conversations happened as part of this research project, so the primary 'intended audience' was the researchers, though participants were informed of additional plans to distribute and release the data in the consent form (see App. D).

### Text Characteristics

We summarise text characteristics in App. M. For the survey responses, the text provides details on the participant and their views about LLMs via short-form free-text responses (we requested 2-5 sentences in their own words). For the conversations, there are three different types: unguided, values guided and controversy guided, as described in the main paper (SS 2.2). Each conversation type contains a different distribution of topics. Overall, Prism is skewed towards subjective, values-driven and controversial dialogue. The human-written texts within a conversation typically consist of single sentence prompts, on average 13 words long. Prompts receive up to four model responses generated by a variety of LLMs. We instruct the LLMs to limit their response to 50 words or less. Most unsuccessfully abide by this instruction: the average response length is 89 words. We release metadata (see App. E) with each text instance including information on detected language, automated and manual PII checks and moderation flags (e.g. if it contains sexual, hateful or violent content).

### Recording Quality

During data collection, our interface experienced two distributed denial of service (DDoS) attacks: one on 28th November 2023 and another on 1st December 2023. The primary way that these attacks may have affected recording quality was via interrupting participants' conversation sessions (most then later returned to the interface to complete their conversations a couple hours or days later). These participants' data points may differ to those who had a smoother continuous experience in the task.

### Author Characteristics and Positionality Statement

We aimed to operate in the subjective paradigm [15, 16] and have as little influence as possible on how participants interacted with models (e.g. no annotation guidelines for how to rate responses). As a team of researchers, we come from a variety of backgrounds (genders, ethnicities, countries of birth, native languages) and are involved with AI research, either in an academia (6/12) or industry (6/12).

### Expanded Ethical Considerations

Privacy and deanonymisationThe conversations in Prism are highly personal, for example detailing views towards abortion, religion, immigration, workplace disputes or intimate relationships. We have pseudo-anonymised the data, checked for PII (App. E), sought informed consent from every participant (App. D), provided options for participants to withdraw their data, and clearly stipulated that attempts of deanonymisation violate our dataset's terms and conditions (App. C). However, despite following these best practices, the risk for deanonymisation remains. We include a reporting mechanism on our website and GitHub for any participants and researchers to report issues.

Harmful and unsafe contentWe asked participants to engage the LLMs in controversial conversations. This comes with the benefit of expanding human preference data to discursive areas with the greatest expected degree of interpersonal disagreement, but at the risk of encouraging hateful, bigoted, biased or otherwise harmful content. Harmful content is an issue in other human feedback datasets, where some opt to moderate conversations prior to public release [113] and others retain toxic content for the purpose of future research into conversational AI safety [112, 111]. Compared to these previous datasets, Prism has an exceptionally low level of flagged content as measured via the OpenAI moderation API (0.06% overall, and \(<0.003\%\) for subcategories of sexually-explicit, violent, hateful, self-harm and harassment). However, the recall of this API may be low [111]; so, this could be an underestimate. From examining prompts closest to topic centroids (App. R.2), it is clear there are some prompts with potential for harm. We provide metadata for every text instance in Prism, and opt to not filter any conversations. We believe it is a critical area of research to understand how state-of-the-art models respond when they are prompted to engage in such conversations, and how different people with diverse lived experiences react to safety interventions.

Participation-washing and intended societal impactIn our setting, we claim what Sloane et al. [55] calls _participation as work_, that is offering fair remuneration and attribution of the consensual labour of workers contributing to our project. Notably, many participants (those familiar and unfamiliar with AI) contacted the researchers and reported enjoying or learning from the task, suggesting there was an "education quotient" or role of _participation as experience_[53]. Compared to "passive" participation in annotation tasks or pre-training datasets [33], our process is more active for participants because it foregrounds the opportunity to provide their feedback, opinions and preferences, not just labels. "Participatory" also signals our goal to have communities more involved in alignment fine-tuning of models and see Prism as a first step demonstrating this need. These aims evoke notions of _participation as justice_--including more people at the table of LLM design and development but we note that participation is in reality thin, because while we seek their view, we cannot grant participants the power to change behaviours of deployed LLMs [131]. Even the etymological roots of participation centre on the notion of "sharing" [53] but there is no guarantee that the human workers upon whom the success of RLHF relies on, partake in any share of the profits from more usable or preferred LLM technologies. We release Prism in the hope it moves the needle towards more inclusive and diverse research on human-AI interactions, emphasising the central role of those who contribute their time and voice to generating human feedback data. Ultimately, how these contributions have impact depends on those in power (industry labs, academics, policymakers), because "the experience of participation must include the sense not only of having spoken, but of having been heard" [p.18, 53].

### Expanded Technical and Task Design Limitations

The curse of dimensionality (or intersectionality)Our findings suggest dialogue and model choice are driven somewhat by group affiliation and somewhat by idiosyncratic variance. However, Prism contains a rich array of information on each participant with both structured and unstructured components. There are endless ways we could have divided the data or understood participant identity, and despite our best efforts to assess sensitivity to design choices, each alternative may have resulted in very different outcomes [120], and we are under-powered to test so many sparse combinations. Using less sparse groupings introduces biases--for example, focusing on region risks lumping together participants from particular geographies as "cultures" [82]. While we split out the UK and US to avoid these countries dominating their respective regions, there remain varying degrees of country-wise entropy in other regions--the Middle East has 94% individuals from Israel, and 100% of Non-US Northern Americans are Canadian (see App. H). Similarly, we use more aggregated ethnicity and religion groupings for statistical power, but amorphous and heterogeneous categories like "Other" have limited or flawed real-world meaning as "Other" contains, for example, both those who identify as Indigenous or First People and as Middle Eastern or Arab. It is an exciting direction for future work to explore free-form characterisations of identity (e.g. the free-text profile or system string) or ex-post groupings of people's preferences [9], and examine how findings change when we break away from neatly-observed but essentialising demographic traits [133].

The confounding effect of many moving cogs in a conversationBeyond the complexities of intersectional identity and idiosyncratic variance of individuals within identity groups, other sources of variance in Prism present a challenge for controlled experiments; particularly, the high-dimensionalityof what exact topics each participant chooses to talk about, which models randomly get selected in-the-loop, and the stochasticity in their responses from a non-deterministic temperature. It is hard to pin down robust mechanisms of preference differences amongst individuals with so many sources of variation. We opted for choice of input prompt and conversation to be a free parameter in Prism as a more naturalistic setting of LLM use and because we wanted to understand dialogue diversity among participants. We do empirically find some regions of fixed prompt-response pairs from individuals who self-select into asking the same prompts as other participants (see App. S.4).

Noisy signals and misaligned incentivesRelatedly, our conclusions may be confounded by measurement invariance given our explicit focus on subjective, fluid and cardinal devices. This echos the economist's view, that it is foolish to rely too heavily on cardinal ratings over ordinal rankings to make interpersonal comparisons, or enforce _preference construction_, where intrinsic feelings are noisily-quantified on numeric scales. There are also issues of _preference falsification_: while participants are financially incentivised to participate, they may not honestly report their preferences over models. We cannot rule out the possibility that participants select a 'bad' model to lock in for the subsequent turns of conversation if it is more interesting (thus preferable in our narrow task confines) to talk to a more offensive or controversial model, or to try to 'jailbreak it' [112]. In hindsight, it may have been a smarter design choice to force participants to rank model responses, or to collect both ratings and rankings (notwithstanding decision fatigue), or make attempts to elicit more interpersonally comparable data via a willingness-to-pay monetary unit. Previous work also raises concerns over relying on human feedback as 'gold standard', for example whether participants can accurately rate factuality of an output, or are anchored on formatting and 'first impressions' (as we and Hosking et al. [134] both find). Preferences, especially at a fine-grained level like in Prism, have high context-dependency [135], so we caution against taking the ratings as revealing some objective truth, instead staying firmly rooted in the subjective paradigm [15, 2].

Still the "tyranny of the (English-speaking) crowdworker"Much of AI, NLP and now RLHF is underpinned by crowdworker labour [136]. Despite our _aims_ to include more diverse voices in LLM development processes, we avoid overstating _claims_ on diversity. Prism still only contains crowdworkers, who have significant sample biases [137]; can only be so "representative" given the relatively small sample sizes; must be digital natives given the platformed nature of the work; and possess different incentives for engagement [138]. Furthermore, while Prism gains some dialectical diversity from different geographies of English, from varying speaker fluency, and from some contributions in other languages (1%, mainly Spanish), it is almost exclusively in English. Cultural diversity can only be measured so far without also accounting for linguistic diversity [61]. Furthermore, while we try to sample from many regions, our sample is still dominated by White Western participants, especially when considering cultural phylogeny [82], i.e., the non-independence of populations with shared history or migrations of peoples (for example, Australia vs UK vs Canada). We encourage future work prioritising human feedback collection in other languages to understand how models handle sociocultural and linguistic interactions [115].

The ever-changing stream of pre-aligned modelsWhen data collection began in mid-November, Prism contained the top ranking models on publicly available leaderboards but new models have since emerged, including Gemini [139], Miztral [140], Claude-3 [141], Command-R [142] and Llama-3 [143]. There is an incompatibility between the current pace of model releases and doing human participant research that requires lengthy processes of ethics approval, interface design, data processing and manual annotation. The expense and inconvenience of doing human research increases the attractiveness of simulating responses, usually with GPT-4 [108]. So, while Prism does miss out on the newest players to enter the battle arena, we do provide carefully-sourced human data (including a survey which stands independently from the LLM conversations) combined with a wide distribution of model texts; so we hope the utility of the data persists in the coming years even as models change. We are still potentially limited when comparing open and closed-access models: while the former allows full transparency over system prompts, closed-access models can obscure additional instructions as hidden context. Including models from the same family allows comparisons by version or size, but introducing clones (models producing very similar outputs) can distort preference rankings [9]. Prism is also limited by _value-lock in_[108]--the models are already tuned to cultural perspectives or alignment norms [34, 35], which precludes observing certain group preferences towards a wider set of behaviours [37, 144], and renders participants "thin" because they are "limited to existing designs with pre-existing purposes." [p.3, 25].

Prism Data Clause

### Terms of Use

PurposeThe Dataset is provided for the purpose of research and educational use in the field of natural language processing, conversational agents, social science and related areas; and can be used to develop or evaluate artificial intelligence, including Large Language Models (LLMs).

Usage RestrictionsUsers of the Dataset should adhere to the terms of use for a specific model when using its generated responses. This includes respecting any limitations or use case prohibitions set forth by the original model's creators or licensors.

Content WarningThe Dataset contains raw conversations that may include content considered unsafe or offensive. Users must apply appropriate filtering and moderation measures when using this Dataset for training purposes to ensure the generated outputs align with ethical and safety standards.

No Endorsement of ContentThe conversations and data within this Dataset do not reflect the views or opinions of the Dataset creators, funders or any affiliated institutions. The dataset is provided as a neutral resource for research and should not be construed as endorsing any specific viewpoints.

No DeanonymisationThe User agrees not to attempt to re-identify or de-anonymise any individuals or entities represented in the Dataset. This includes, but is not limited to, using any information within the Dataset or triangulating other data sources to infer personal identities or sensitive information.

Limitation of LiabilityThe authors and funders of this Dataset will not be liable for any claims, damages, or other liabilities arising from the use of the dataset, including but not limited to the misuse, interpretation, or reliance on any data contained within.

### Licence and Attribution

Human-written texts (including prompts) within the dataset are licensed under the Creative Commons Attribution 4.0 International License (CC-BY-4.0). Model responses are licensed under the Creative Commons Attribution-NonCommercial 4.0 International License (CC-BY-NC-4.0). Use of model responses must abide by the original model provider licenses.

For proper attribution when using this dataset in any publications or research outputs, please cite with the DOI.

_Suggested Citation_: Kirk, H. R., Whitefield, A., Rottger, P., Bean, A., Margatina, K., Ciro, J., Mosquera, R., Bartolo, M., Williams, A., He, H., Vidgen, B., & Hale, S. A. (2024). _The PRISM Alignment Dataset_. [https://doi.org/10.57967/hf/2113](https://doi.org/10.57967/hf/2113)

### Dataset Maintenance

As the authors and maintainers of this dataset, we commit to no further updates to the dataset following its initial release. The dataset is self-contained and does not rely on external links or content, ensuring its stability and usability over time without the need for ongoing maintenance.

### Data Rights Compliance and Issue Reporting

We are committed to complying with data protection rights, including but not limited to regulations such as the General Data Protection Regulation (GDPR). If any individual included in the dataset wishes to have their data removed, we provide a straightforward process for issue reporting and resolution on our Github. Concerned parties are encouraged to contact the authors directly via the provided contact form link on the Github. Upon receiving a request, we will engage with the individual to verify their identity and proceed to remove the relevant entries from the dataset. We commit to addressing and resolving such requests within 30 days of verification.

Informed Consent

This research was reviewed by, and received ethics clearance through, a subcommittee of the University of Oxford Central University Research Ethics Committee [OII_C1A_23_088]. The following text was displayed to all participants to collect informed consent.

## Your Feedback on AI Language Models

We appreciate your interest in participating in this study. **The aim of this research is to understand people's preferences and perceptions regarding AI Language Model behaviours**, also referred to as Large Language Models (LLMs), Generative AI Language Models, AI ChatBots or Virtual Assistants. AI language models are computer programs designed to generate text. They can respond to questions or prompts by producing written responses. We want to learn more about how people like you use and perceive these AI language models.

**Please first make sure you are using a laptop or desktop computer, and you are not using a mobile device. Our task is NOT compatible with mobile devices.**

**Please then read through this information before agreeing to participate (if you wish to).**

You may ask any questions before deciding to take part by contacting the research team. The Principal Researcher is Hannah Rose Kirk, and the Principle Investigator is Dr Scott. A. Hale, who are both affiliated with the Oxford Internet Institute at the University of Oxford.

## What does the task involve?

If you decide to participate, there are two stages.

In this stage, you will be asked to fill in a short survey about yourself and your thoughts on AI language models.

In the next stage, you will have conversations with AI language models by providing prompts and rating their responses using a user-friendly interface. The prompts can be on various topics, and you don't need any specific knowledge to participate. Your input will help us understand your preferences and opinions about how these AI language models work.

**Both stages should take between 55-65 minutes.** No background knowledge is required.

Please note that you will be interacting with an AI language model. The research team cannot directly control and are not responsible for the text generated by these models. There is a possibility that the models produce biased, inaccurate or harmful language. The risks to you as an individual are equivalent to those you would be exposed to if you use AI language models via interfaces like ChatGPT.

**Do I have to take part?**

No, participation is voluntary. If you do decide to take part, you may stop at any point for any reason before submitting your answers by closing the browser. However, we are only able to pay participants who complete the task. For demographic information, we have included a 'Prefer not to say' option for each set of questions should you prefer not to answer a particular question.

**Can I withdraw my participation and data?**

Yes, you may stop the study at any time. Please note that if you withdraw within a stage of the study you will not be paid for that stage or any subsequent incomplete stages, but you will be paid for any stages that you have already completed. You can withdraw your data from the study. The cut-off date for withdrawing your data is 14 days after you submitted the data. Please email members of the research team (see contact details below) within this 14-day window to withdraw your data from the study.

### How will my data be used?

The data collected from your participation will be pseudo-anonymized (stored with a unique numeric ID) and stored securely. It will be used for research purposes, and your personal information will remain confidential. The data will be analysed to gain insights into diverse preferences and perceptions regarding AI language model behaviours. At the end of the study, the pseudo-anonymised data collected will be released publicly for future research. The findings of this study may be published in academic journals or presented at conferences, and the results will be written up for a DHil degree. Your individual identity will not be disclosed at any point in data release or publication. We do not collect any personal, private identifying information, IP addresses or contact details. The data we will collect that could identify you will be some demographic information (gender, age, nationality, religion, etc.), and short self-written survey answers.

The responses you provide will be stored in a password-protected electronic file on University of Oxford secure servers and may be used in academic publications, conference presentations or reports for external organisations. We will release a clean, PII-checked and pseudo-anonymised form of the data on an open-access, public data repository. Raw research data will be stored for 3 years after publication or public release of the research. We would like to use the data in future studies, and to share data with other researchers (e.g. in online databases). Data will have identifying information removed before it is shared with other researchers or results are made public. The data that we collect from you may be transferred to, stored and/ or processed at a destination outside the UK and the European Economic Area. By submitting your personal data, you agree to this transfer, storing or processing.

### Who has reviewed this research?

This research has been reviewed by, and received ethics clearance through, a subcommittee of the University of Oxford Central University Research Ethics Committee [OII_C1A_23_088].

### Who do I contact if I have a concern or I wish to complain?

If you have a concern about any aspect of this research, please speak to Hannah Rose Kirk (hannah.kirk@oii.ox.ac.uk) or their supervisor Dr. Scott A. Hale (scott.hale@oii.ox.ac.uk), and we will do our best to answer your query. We will acknowledge your concern within 10 working days and give you an indication of how it will be dealt with. If you remain unhappy or wish to make a formal complaint, please contact the Chair of the Research Ethics Committee at the University of Oxford who will seek to resolve the matter as soon as possible: Social Sciences & Humanities Interdivisional Research Ethics Committee; Email: ethics@socsci.ox.ac.uk; Address: Research Services, University of Oxford, Boundary Brook House, Churchill Drive, Headington, Oxford OX3 7GB.

### Please note that you may only participate in this survey if you are 18 years of age or over.

\(\bigcirc\) I certify that I am 18 years of age or over

**If you have read the information above and agree to participate with the understanding that the data (including any personal data) you submit will be processed accordingly, please tick the box below to start.**

\(\bigcirc\) Yes, I agree to take part

## Appendix E Metadata Processing

For each text instance in Prism, we attach three pieces of metadata: detected **language** flags, detected **private or personally identifiable information (PII)** flags, and detected **moderation** flags.

### Structuring the Metadata

There are five types of text instances. Two appear in the survey (self_description, system_string) and have a 1:1 matching with each user (user_id). One appears at the conversation level (open_feedback) and has a 1:1 matching with each convo_id and a many:1 matching with each user_id because each participant has multiple conversations. Finally, the last two occur within each turn of a conversation, where for each single user_prompt there are multiple model responses (model_response). We structure the metadata so it can be merged uniquely, without duplication. We release one file, where each text instance is tied to its metadata via the identifying information shown in Tab.1, and a column_id for matching whether the text is [system_string, self_description, user_prompt, model_response, open_feedback].

### Automated Flagging

PIITo identify whether a textual instance in our dataset contains personal and identifiable information (PII) we used the package scrubadub.11 Specifically we used the function scrubadub.clean(text) which replaces the phone numbers and email addresses with anonymous IDs, if they are found in the input. We flag with \(1\) instances that are altered (i.e., PII was identified) and \(0\) those that remained unchanged.

Footnote 11: [https://scrubadub.readthedocs.io/en/stable/](https://scrubadub.readthedocs.io/en/stable/)

ModerationTo measure content moderation we use the OpenAI Moderation endpoint.12 The API takes as an input a textual instance and outputs a json file with an overall boolean flag (flagged) whether there input potentially harmful (True), otherwise False. The API also returns a flag for a list of specific moderation categories that can be used to further filter and inspect the data. The categories are sexual, hate, harassment, self-harm, sexual/minors, hate/threatening, violence/graphic, self-harm/intent, self-harm/instructions, harassment/threatening and violence. Similar to the overall flag, for each category, the value is True if the model flags the corresponding category as violated, False otherwise. Finally, the API returns a dictionary of per-category scores that denote the model's confidence that the input violates the OpenAI's policy for the category. The value is between \(0\) and \(1\), where higher values denote higher confidence.

Footnote 12: [https://platform.openai.com/docs/guides/moderation](https://platform.openai.com/docs/guides/moderation)

Language DetectionTo detect the language of each text instance in our dataset we used the LangID codebase.13 LangID is a popular python package that efficiently detects the language of an input and currently supports \(97\) languages. Specifically, we use the langid.classify(text) function and store a string for the detected language.

\begin{table}
\begin{tabular}{l l|c c c} Text Instance & Study Stage & & & & \\ \hline self\_description & Survey & & & & \\ system\_string & Survey & & & & \\ user\_prompt & Conversations & & & & \\ model\_response & Conversations & & & & \\ open\_feedback & Conversations & & & & \\ \hline \end{tabular}
\end{table}
Table 1: Identifiers of text instance types in Prism.

[MISSING_PAGE_FAIL:37]

Annotating Ethnicity, Religion and Gender

We ask people to describe their ethnic and religious affiliations in their own words because for a global survey, there are no immediately obvious preset categories. In the survey data, we release this original self-description ({ethnicity, religion}_self_described). However, there are 264 unique strings for ethnicity, and 137 unique strings for religion. For some analysis, it is valuable to have aggregate groupings. To attain this grouping, we first used gpt-4-turbo to categorise the strings, but found some errors and essentialising generalisations, for example, if someone answered with a nationality not an ethnic group like _american_, gpt would return _white_.15

Footnote 15: As an aside, these types of baked-in priors are a good example of why using LLMs as a surrogate for human annotators may introduce downstream biases [108].

Accordingly, we used a second round of manual human annotation to verify these automated labels. Two annotators (authors of the paper) first made independent judgements then discussed any disagreements. For ethnicity, some participants also had answered a Prolific screening question on their simplified ethnicity, though we did not have this information for all participants as it was not mandatory. We thus annotate all unique combinations of the self-described string, and the Prolific ethnicity information (\(n=343\)). In ambiguous cases (e.g. the aforementioned _american_ response), we relied on this additional ethnicity information, and in its absence, defaulted to a _Prefer not to say_ response. For religion, we do not have any additional information provided by the Prolific pre-screening questionnaire, so verification decisions were made on the basis of the self-describe string alone. The annotators agreed on 94% of ethnicity cases (discussing and resolving the remaining 20); and 96% of religion cases (discussing and resolving the remaining 5).

We highlight two general findings from our disagreements which may be of interest to people analysing or categorising our data in the future. Firstly, **ethnicity and nationality are complex**. Take for example the UK census, where _Chinese_, _Banglaeshi_, _Indian_ and _Pakistani_ are all listed as subcategories of the Asian ethnic group.16 Ethnicity is a multi-faceted term which can include nationality, language group, skin colour, religion, among other characteristics [145]. Studies have shown that survey participants can interpret the term ethnic group through a variety of subjective lens [146, 147]. During annotation, we tried to gather information on whether group terms commonly refer to an ethnic group, but some subjectivity and naively are inevitable; so, we encourage future researchers to carefully consider their own categorisations depending on the question at hand. Secondly, the **belonging and believing aspects of religion intersect**[148, 149], and it is not immediately clear how to categorise an individual that culturally affiliates with religion but simultaneously identities as an atheist or non-believer. Studies have revealed that the belonging and believing axis of religion are important for conditioning behaviours such as trust, pro-sociality and altruism [150, 151, 152]. In general, we annotated a mention of a religion as assigned to that religion (not distinguishing between the belonging and believing channels) but it remains to be seen whether one axis is more salient for values and opinions towards AI systems.

Footnote 16: See the fact sheet at ethnicity-facts-figures.service.gov.uk.

Note for gender, we provided a standard multiple choice question with options: _Female_, _Male_, _Non-binary / third gender_, _Prefer not to say_ and _Prefer to self-describe_. Only 3 individuals opted to self-describe, which we then annotated and only assimilated in very clear cut cases,17 else we grouped it as _Prefer not to say_ to avoid over-riding a participant's self-identification.

Footnote 17: For example, one participant responded with “i dont expect this wokery from intelligent people. you want to know which of the 2 possible genders i am male.”, which we assign as _Male_.

## Appendix G Participant Demographics

We present full demographic breakdowns in Tab. 5. We also compare the breakdowns in Prism to some early human feedback datasets which provide demographic information (Tab. 6).

\begin{table}
\begin{tabular}{l r r r} \hline \hline
**Total Participants** & **1,500** & & & 100 \\ \hline With conversations & 1,396 & & & 93.1 \% \\ Just survey & 104 & 6.9 \% & & \\ \hline
**Age** & & & & \\ \hline
25-34 years old & 454 & & 30.3 \% \\
18-24 years old & 297 & & 19.8 \% \\
35-44 years old & 237 & & 15.8 \% \\
45-54 years old & 208 & & 13.9 \% \\
55-64 years old & 197 & & 13.1 \% \\
65+ years old & 106 & & 7.1 \% \\ _Prefer not to say_ & 1 & 0.1 \% & \\ \hline
**Gender** & & & \\ \hline Male & 757 & & 50.5 \% \\ Female & 718 & & 47.9 \% \\ Non-binary / third gender & 21 & 11.4 \% & \\ _Prefer not to say_ & 4 & 10.3 \% & \\ \hline
**Self-Reported Ethnicity*** & & & \\ \hline White & 969 & & & 64.6 \% \\ Black / African & 122 & & 8.1 \% & \\ Hispanic / Latino & 121 & & 8.1 \% & \\ Asian & 95 & & 6.3 \% & \\ Mixed & 68 & & 4.5 \% & \\ Middle Eastern / Arab & 14 & 10.9 \% & & \\ Indigenous / First People & 8 & 0.5 \% & & \\ _Other_ & 17 & 11.1 \% & & \\ _Prefer not to say_ & 86 & & 5.7 \% & \\ \hline
**Self-Reported Religion*** & & & & \\ \hline Non-religious & 762 & & & 50.8 \% \\ Christian & 487 & & 32.5 \% & \\ Agnostic & 71 & 4.7 \% & & \\ Jewish & 42 & 2.8 \% & & \\ Muslim & 31 & 1.2 \% & & \\ Spiritual & 18 & 1.2 \% & & \\ Buddhist & 12 & 10.8 \% & & \\ Folk religion & 6 & 10.4 \% & & \\ Hindu & 5 & 0.3 \% & & \\ Sikh & 3 & 0.2 \% & & \\ _Other_ & 4 & 0.3 \% & & \\ _Prefer not to say_ & 59 & & 3.9 \% & \\ \hline
**Employment Status** & & & & \\ \hline Working full-time & 712 & & & 47.5 \% \\ Working part-time & 265 & & 17.7 \% & \\ Student & 191 & & 12.7 \% & \\ Unemployed, seeking work & 113 & & 7.5 \% & \\ Retired & 104 & & 6.9 \% & \\ Homemaker / Stay-at-home parent & 46 & & 3.1 \% & \\ Unemployed, not seeking work & 46 & & 3.1 \% & \\ _Prefer not to say_ & 23 & & 1.5 \% & \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Full Demographics Breakdowns.** Counts and percentages of participants by standard demographic variables. Overall, Prism utilises a large and demographically-diverse sample, especially compared to some previous human feedback datasets (see Tab. 6); but it still generally skews towards young, white and educated populations. *For ethnicity and religion, see details in App. F.

\begin{table}
\begin{tabular}{l r r r} \hline \hline
**Education** & & & \\ \hline University Bachelors Degree & 637 & & 42.5 \% \\ Graduate / Professional degree & 241 & & 16.1 \% \\ Some University but no degree & 236 & & 15.7 \% \\ Completed Secondary School & 209 & & 13.9 \% \\ Vocational & 125 & & 8.3 \% \\ Some Secondary & 24 & 11.6 \% \\ Completed Primary School & 16 & 11.1 \% \\ Some Primary & 3 & 0.2 \% \\ _Prefer not to say_ & 9 & 0.6 \% \\ \hline
**Martial Status** & & & \\ \hline Never been married & 870 & & 58.0 \% \\ Married & 463 & & 30.9 \% \\ Divorced / Separated & 123 & & 8.2 \% \\ Widowed & 21 & 11.4 \% \\ _Prefer not to say_ & 23 & 11.5 \% \\ \hline
**English Proficiency** & & & \\ \hline Native speaker & 886 & & 59.1 \% \\ Fluent & 405 & & 27.0 \% \\ Advanced & 160 & & 10.7 \% \\ Intermediate & 42 & 1.8 \% \\ Basic & 7 & 0.5 \% \\ \hline
**Regions** & & & \\ \hline US & 338 & & 22.5 \% \\ Europe & 313 & & 20.9 \% \\ UK & 292 & & 19.5 \% \\ Latin America and the Caribbean & 146 & & 19.7 \% \\ Australia and New Zealand & 129 & & 8.6 \% \\ Africa & 118 & & 7.9 \% \\ Asia & 60 & & 4.0 \% \\ Northern America & 50 & & 3.3 \% \\ Middle East & 50 & & 3.3 \% \\ Oceania & 1 & 0.1 \% \\ _Prefer not to say_ & 3 & 0.2 \% \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Full Demographics Breakdowns.** Counts and percentages of participants by standard demographic variables. Overall, Prism utilises a large and demographically-diverse sample, especially compared to some previous human feedback datasets (see Tab. 6); but it still generally skews towards young, white and educated populations. *For ethnicity and religion, see details in App. F.

\begin{table}
\begin{tabular}{l c c c c c c} Category & Bai et al. & Ouyang et al. & Glaese et al. & Ganguli et al. & Stiennon et al. & **Ours** \\ \hline Total Participants & 28\(\ddagger\) & 40 & & 324 & & 1,500 \\ Demographic Respondents & 28\(\ddagger\) & 19 & 533 & 115 & 21 & 1,500 \\ \hline
**Gender** \\ \hline Male & 53.6 \% & 47.4 \% & 45.0 \% & 47.0 \% & 38.1 \% & 50.5 \% \\ Female & 46.4 \% & 42.1 \% & 54.0 \% & 52.2 \% & 61.9 \% & 47.9 \% \\ Non-binary & 0.0 \% & 5.3 \% & 1.0 \% & 0.9 \% & 0.0 \% & 1.4 \% \\ _Prefer not to say/Other_ & 0.0 \% & 15.3 \% & 0.0 \% & 0.0 \% & 0.0 \% & 10.3 \% \\ \hline
**Sexual Orientation** \\ \hline Heterosexual & 89.3 \% & - & 84.0 \% & 81.7 \% & - & - \\ Lesbian or Gay & 17.1 \% & - & 15.0 \% & 14.3 \% & - & - \\ Bisexual & 0.0 \% & - & 19.0 \% & 112.2 \% & - & - \\ Uncertain & 3.6 \% & - & - & 0.9 \% & - & - \\ _Prefer not to say/Other_ & 0.0 \% & - & 12.0 \% & 0.9 \% & - & - \\ \hline
**Age** \\ \hline
18-24 & 17.1 \% & 16.3 \% & 11.0 \% & 0.0 \% & - & 119.8 \% \\
25-34 & 39.3 \% & 47.4 \% & 37.0 \% & 25.2 \% & 42.9 \% & 30.3 \% \\
35-44 & 42.9 \% & 110.5 \% & 24.0 \% & 33.9 \% & 23.8 \% & 115.8 \% \\
45-54 &

[MISSING_PAGE_FAIL:42]

\begin{table}
\begin{tabular}{l r r r|r r} \hline \hline  & \multicolumn{3}{c}{**Country of Birth**} & \multicolumn{3}{c}{**Country of Residence**} \\ \hline Finland & 12 & 10.8 \% & 13 & 10.9 \% \\ Estonia & 11 & 10.7 \% & 10 & 10.7 \% \\ Austria & 11 & 10.7 \% & 10 & 10.7 \% \\ Slovenia & 10 & 10.7 \% & 10 & 10.7 \% \\ Netherlands & 9 & 10.6 \% & 8 & 0.5 \% \\ India & 9 & 10.6 \% & 0 & 0.0 \% \\ Japan & 9 & 10.6 \% & 11 & 10.7 \% \\ Korea, Republic of & 9 & 10.6 \% & 7 & 0.5 \% \\ Portugal & 8 & 10.5 \% & 7 & 0.5 \% \\ Romania & 7 & 10.5 \% & 0 & 0.0 \% \\ Philippines & 7 & 10.5 \% & 0 & 0.0 \% \\ Sweden & 7 & 10.5 \% & 6 & 0.4 \% \\ Russian Federation & 6 & 10.4 \% & 0 & 0.0 \% \\ Ukraine & 4 & 10.3 \% & 0 & 0.0 \% \\ Bangladesh & 4 & 10.3 \% & 0 & 0.0 \% \\ China & 4 & 10.3 \% & 0 & 0.0 \% \\ Hong Kong & 3 & 10.2 \% & 0 & 0.0 \% \\ Pakistan & 3 & 10.2 \% & 0 & 0.0 \% \\ Ghana & 3 & 10.2 \% & 0 & 0.0 \% \\ Dominican Republic & 3 & 10.2 \% & 0 & 0.0 \% \\ Venezuela, Bolivarian Republic of & 3 & 10.2 \% & 0 & 0.0 \% \\ Indonesia & 3 & 10.2 \% & 0 & 0.0 \% \\ Viet Nam & 2 & 10.1 \% & 0 & 0.0 \% \\ Sri Lanka & 2 & 10.1 \% & 0 & 0.0 \% \\ Turkey & 2 & 10.1 \% & 0 & 0.0 \% \\ Argentina & 2 & 10.1 \% & 0 & 0.0 \% \\ Kazakhstan & 2 & 10.1 \% & 0 & 0.0 \% \\ Slovakia & 1 & 10.1 \% & 0 & 0.0 \% \\ Sudan & 1 & 10.1 \% & 0 & 0.0 \% \\ Tonga & 1 & 10.1 \% & 0 & 0.0 \% \\ Afghanistan & 1 & 10.1 \% & 0 & 0.0 \% \\ Nepal & 1 & 10.1 \% & 0 & 0.0 \% \\ Honduras & 1 & 10.1 \% & 0 & 0.0 \% \\ Belarus & 1 & 10.1 \% & 0 & 0.0 \% \\ Bosnia and Herzegovina & 1 & 10.1 \% & 0 & 0.0 \% \\ Brazil & 1 & 10.1 \% & 0 & 0.0 \% \\ Bulgaria & 1 & 10.1 \% & 0 & 0.0 \% \\ Colombia & 1 & 10.1 \% & 0 & 0.0 \% \\ Cuba & 1 & 10.1 \% & 0 & 0.0 \% \\ Côte d’Ivoire & 1 & 10.1 \% & 0 & 0.0 \% \\ Malaysia & 1 & 10.1 \% & 0 & 0.0 \% \\ Guyana & 1 & 10.1 \% & 0 & 0.0 \% \\ Iceland & 1 & 10.1 \% & 1 & 0.1 \% \\ Jamaica & 1 & 10.1 \% & 0 & 0.0 \% \\ Kenya & 1 & 10.1 \% & 0 & 0.0 \% \\ Kuwait & 1 & 10.1 \% & 0 & 0.0 \% \\ Lithuania & 1 & 10.1 \% & 0 & 0.0 \% \\ Luxembourg & 1 & 10.1 \% & 2 & 0.1 \% \\ Malawi & 1 & 10.1 \% & 0 & 0.0 \% \\ Zambia & 1 & 10.1 \% & 0 & 0.0 \% \\ Tanzania, United Republic of & 0 & 0.0 \% & 1 & 0.1 \% \\ Lesotho & 0 & 0.0 \% & 1 & 0.1 \% \\ Uruguay & 0 & 0.0 \% & 1 & 0.1 \% \\ _Prefer not to say_ & 3 & 10.2 \% & 1 & 0.1 \% \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Full Geographic Breakdowns.** We collect country of birth and current country of residence. Prism contains participants born in 75 different countries, residing in 38 different countries.

Figure 7: **Skewed regional entropy in Prism. The hierarchical tree diagram uses participant birth location, mapping (i) special location (splitting out the US and UK), which is used in the main paper, (ii) UN-defined subregions, and (iii) ISO country codes. There is an over-representation of UK and US participants due to the census samples. In most regions besides Europe, participation is dominated by one or two birth countries. The two small vertical boxes are Prefer not to say (in red), and Oceania (in navy). _Note:_ 88% of Prism participants are born and currently reside in the same country.**

Participant LLM Usage and Familiarity

We present breakdowns on experience with LLMs in Tab. 9. We did not require participants to be familiar with LLMs so provide the following primer:

This research is about Artificial Intelligence (AI) Language Models.

These models are also sometimes referred to as Generative AI, Large Language Models (LLMs), Conversational Agents, AI Chat Bots or Virtual Assistants.

They are advanced computer programs that can understand and generate human-like text. These models learn from large amounts of text data on the internet to generate their responses.

One example you might have heard is ChatGPT, where people can have a conversation with an AI language model via an internet website.

## Appendix F

\begin{table}
\begin{tabular}{l r r} \hline \hline
**LLM Direct Use** & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ \hline Yes & 1,162 & 77.5 \% \\ No & 259 & 117.3 \% \\ Unsure & 79 & 5.3 \% \\ \hline
**LLM Indirect Use** & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ \hline Yes & 1,104 & 73.6 \% \\ No & 215 & 14.3 \% \\ Unsure & 181 & 112.1 \% \\ \hline
**LLM Familiarity** & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ \hline Somewhat familiar & 920 & 61.3 \% \\ Very familiar & 424 & 28.3 \% \\ Not familiar at all & 156 & 10.4 \% \\ \hline
**LLM Frequency of Use** & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ \hline Once per month & 374 & 24.9 \% \\ Every week & 316 & 21.1 \% \\ More than once a month & 291 & 19.4 \% \\ Less than one a year & 162 & 10.8 \% \\ Every day & 110 & 7.3 \% \\ Not shown question & 247 & 16.5 \% \\ \hline
**LLM Use Cases** & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ \hline
**Research**: Fact-checking or gaining overviews on specific topics. & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
**Professional Work**: Assisting in drafting, editing, or brainstorming content for work. & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
**Creative Writing**: Generating story ideas, dialogues, poems or other writing prompts. & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
**Technical or Programming Help**: Seeking programming guidance, code generation, software recommendations, or debugging assistance. & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
**Lifestyle and Hobbles**: Looking for recipes, craft ideas, home decoration tips, or hobby-related information. & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
**Homework Assistance**: Getting help with school or university assignments. & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
**Personal Recommendations**: Seeking book, music or movie recommendations. & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ \hline \hline \end{tabular}
\end{table}
Table 9: **Survey of Participants’ LLM Usage**: The majority of participants have used LLMs directly (via a dedicated chat interface) and indirectly (embedded in products or services). Note only participants who answered _Yes_ to LLM Direct Use or LLM Indirect Use_ (\(n=1253\), 84%) are shown **LLM Freq of Use** and **LLM Use Cases**. For Use Cases, we show the % of these participants who selected each use case (can be multiple so \(\sum\neq 1\)). Exact question phrasing is reported in the survey codebook (App. V.1).

### Other Identified Usecases

In addition to the usecases in Tab. 9, 122 participants used the "Other" option to add a usecase in their own words. Many of these just add more specific details to the pre-provided categories. In addition, there were a few interesting themes:

* **Customer Service:** Many of the participants noted having interacted with LLMs in customer support chats, often with negative sentiment ("Usually forced to interact with chatbots to get something done", "Customer service bots I cannot avoid", "Insurance companies direct you to chatbots, usually useless").
* **Profilte and Other Online Surveys:** One of the more common (and potentially concerning) answers mentioned research participation e.g. "Studies like this one", "Doing Prolific tests", but it may be that they mean AI is the subject of the study: "AI research subject on research platforms Prolific, others." or "It's sometimes required as part of a survey on Prolific." We encourage future work on whether there is noticeable difference in these participants' answers elsewhere in our task.
* **AI Understanding or Testing:** A few participants mentioned "Trying to gain an understanding into AI and its capabilities" or "Gauging progress/viability of AI models". Many others indicated curiosity or exploratory use e.g. "Just to test it out and see what it's all about" or "Casual interest in the new technology".
* **Professional or Job Tasks:** Participants added details on professional usecases like resume help, interview prep, CV writing, HR-tasks, Excel help, or emails.
* **Creative (Multimodal) Use-cases:** Participants gave additional detail like writing YouTube scripts, generating gift card text or designing characters for games as well as multimodal creative outputs like generating drawings or images.
* **Domain-Specific Usecases:** Medical, Financial and Educational usecases are all mentioned.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Casual Conversion**: Engaging in small talk, casual chats, or joke generation. & 262 & 20.9 \% \\
**Language Learning**: Using it as a tool for language practice or translation. & 229 & 18.3 \% \\
**Source Suggestions**: Creating or finding bibliographies, information sources or reading lists. & 217 & 17.3 \% \\
**Daily Productivity**: Setting reminders, making to-do lists, or productivity tips. & 216 & 17.2 \% \\
**Historical or News Insight**: Getting summaries or background on historical events or news and current affairs. & 183 & 14.6 \% \\
**Well-being Guidance**: Seeking general exercise routines, wellness or meditation tips. & 159 & 12.7 \% \\
**Games**: Playing text-based games, generating riddles or puzzles. & 143 & 11.4 \% \\
**Travel Guidance**: Getting destination recommendations, planning holidays, or cultural etiquette tips. & 133 & 10.6 \% \\
**Medical Guidance**: Seeking health-related advice or medical guidance. & 130 & 10.4 \% \\
**Financial Guidance**: Asking about financial concepts or general investing ideas. & 107 & 8.5 \% \\
**Relationship Advice**: Seeking general self-help or relationship advice for family, friends or partners. & 98 & 7.8 \% \\
**Other** & 124 & 9.9 \% \\ \hline \hline \end{tabular}
\end{table}
Table 9: **Survey of Participants’ LLM Usage**: The majority of participants have used LLMs directly (via a dedicated chat interface) and indirectly (embedded in products or services). Note only participants who answered _Yes_ to LLM Direct Use of LLM Indirect Use (\(n=1253\), 84%) are shown LLM **Freq of Use** and LLM Use Cases**. For Use Cases, we show the % of these participants who selected each use case (can be multiple so \(\sum\neq 1\)). Exact question phrasing is reported in the survey codebook (App. V.1).

Screening and Recruitment Process

We recruit workers via Prolific ([https://www.prolific.com/](https://www.prolific.com/)). We apply two initial screening criteria: (i) participants must be fluent in English because Prism targets monolingual models and language data, and (ii) participants must have been born and reside in the same country to avoid biasing our sample towards expus living abroad. There is a skewed country-wise distribution of active workers who meet this criteria (see Tab. 10). For example, of the 21,084 workers in Europe (passing screening), 17% are Portuguese, 15% German and 14% Polish; and all 6,584 workers in Africa are located in South Africa. To account for this, we set up country-specific studies in each country with at least one eligible worker, balance study spots across regions, and ensure no single country has more than 100 open spots (apart from the Rep Samples in the UK and US). We collected information on country of birth and country of current residence during our survey (separate to workers' stored Prolific details), and find that 179 participant (12%) have different birth and reside countries. We do not exclude these individuals from our sample.

\begin{table}
\begin{tabular}{l r r r r r r r r r r} \hline \hline  & \multicolumn{3}{c}{**Rep**} & \multicolumn{3}{c}{**Gender Launched**} & \multicolumn{3}{c}{**Approved**} & \multicolumn{3}{c}{**Prolific Fluent English Speakers**} & \multicolumn{1}{c}{**Cost**} \\ \cline{3-11} \multicolumn{1}{l}{**Study**} & \multicolumn{1}{l}{**Sample**} & \multicolumn{1}{l}{**Bal**} & \multicolumn{1}{l}{**(2023)**} & \multicolumn{1}{l}{**Submissions**} & \multicolumn{3}{c}{**(All)**} & \multicolumn{1}{l}{**(Born=Reside)**} & \multicolumn{1}{l}{**(E)**} \\ \hline \hline
**Total** & **2** & **25** & - & **1,500** & **111,572** & **100,585** & **14,850.00** \\ \hline US & ✓ & ✗ & 27-11 & 386 & \(\pm\) & 25.7 \% & 38,114 & \(\pm\) & 34.2 \% & 36,205 & \(\pm\) & 36.0 \% & 3,821.40 \\ UK & ✓ & ✗ & 27-11 & 341 & \(\pm\) & 22.7 \% & 37,408 & \(\pm\) & 33.5 \% & 33.678 & \(\pm\) & 33.5 \% & 3,375.90 \\ South Africa & ✗ & ✓ & 22-11 & 88 & 5.9 \% & 7,061 & 6.3 \% & 6,584 & \(\pm\) & 6.5 \% & 871.20 \\ New Zealand & ✗ & ✓ & 24-11 & 77 & 5.1 \% & 5.1 \% & 511 & 0.5 \% & 389 & 0.4 \% & 762.30 \\ Australia & ✗ & ✓ & 24-11 & 71 & 4.7 \% & 1,968 & 1.8 \% & 1,550 & 1.5 \% & 702.90 \\ Mexico & ✗ & ✓ & 24-11 & 69 & 4.6 \% & 2,021 & 1.8 \% & 1,943 & 1.9 \% & 683.10 \\ Chile & ✗ & ✓ & 23-11 & 65 & 4.3 \% & 455 & 0.4 \% & 416 & 0.4 \% & 643.50 \\ Israel & ✗ & ✗ & 25-11 & 61 & 4.1 \% & 310 & 0.3 \% & 272 & 0.3 \% & 603.90 \\ Canada & ✗ & ✓ & 22-11 & 54 & 3.6 \% & 3,687 & 3.3 \% & 3,031 & 3.0 \% & 534.60 \\ Asia & ✗ & ✗ & 24-11 & 18 & 1.2 \% & 1,966 & 0.2 \% & 32 & 0.0 \% & 178.20 \\ Spain & ✗ & ✓ & 23-11 & 18 & 1.2 \% & 1,252 & 1.1 \% & 942 & 10.9 \% & 178.20 \\ Belgium & ✗ & ✓ & 23-11 & 17 & 1.1 \% & 376 & 0.3 \% & 281 & 0.3 \% & 168.30 \\ Hungary & ✗ & ✓ & 24-11 & 16 & 1.1 \% & 537 & 0.5 \% & 456 & 0.5 \% & 158.40 \\ Ireland & ✗ & ✓ & 23-11 & 15 & 1.0 \% & 640 & 0.6 \% & 502 & 0.5 \% & 148.50 \\ Denmark & ✗ & ✗ & 23-11 & 15 & 1.0 \% & 119 & 0.1 \% & 65 & 0.1 \% & 148.50 \\ Norway & ✗ & ✗ & 23-11 & 15 & 1.0 \% & 91 & 0.1 \% & 59 & 0.1 \% & 148.50 \\ Switzerland & ✗ & ✓ & 23-11 & 14 & 0.9 \% & 205 & 0.2 \% & 104 & 0.1 \% & 138.60 \\ Poland & ✗ & ✓ & 23-11 & 14 & 0.9 \% & 2,975 & 2.7 \% & 2,850 & 2.8 \% & 138.60 \\ Czech Republic & ✗ & ✓ & 24-11 & 14 & 0.9 \% & 238 & 0.2 \% & 229 & 0.2 \% & 138.60 \\ Latvia & ✗ & ✓ & 23-11 & 14 & 0.9 \% & 173 & 0.2 \% & 162 & 0.2 \% & 138.60 \\ Greece & ✗ & ✓ & 24-11 & 14 & 0.9 \% & 809 & 0.7 \% & 747 & 0.7 \% & 138.60 \\ Finland & ✗ & ✓ & 23-11 & 13 & 0.9 \% & 152 & 0.1 \% & 117 & 0.1 \% & 128.70 \\ Germany & ✗ & ✓ & 24-11 & 13 & 0.9 \% & 3,152 & 2.8 \% & 2,295 & 2.3 \% & 128.70 \\ Italy & ✗ & ✓ & 24-11 & 12 & 0.8 \% & 2,037 & 1.8 \% & 1,857 & 1.8 \% & 118.80 \\ France & ✗ & ✓ & 24-11 & 12 & 0.8 \% & 957 & 0.9 \% & 681 & 0.7 \% & 118.80 \\ Slovenia & ✗ & ✓ & 24-11 & 10 & 0.7 \% & 232 & 0.2 \% & 220 & 0.2 \% & 99.00 \\ Austria & ✗ & ✓ & 24-11 & 10 & 0.7 \% & 231 & 0.2 \% & 156 & 0.2 \% & 99.00 \\ Estonia & ✗ & ✓ & 24-11 & 10 & 0.7 \% & 251 & 0.2 \% & 237 & 0.2 \% & 99.00 \\ Netherlands & ✗ & ✓ & 24-11 & 8 & 0.5 \% & 1,460 & 1.3 \% & 1,028 & 1.0 \% & 79.20 \\ Portugal & ✗ & ✓ & 24-11 & 7 & 0.5 \% & 3,649 & 3.3 \% & 3,284 & 3.3 \% & 69.30 \\ Sweden & ✗ & ✓ & 24-11 & 6 & 0.4 \% & 274 & 0.2 \% & 196 & 0.2 \% & 59.40 \\ Luxembourg & ✗ & ✗ & 23-11 & 2 & 0.1 \% & 15 & 0.0 \% & 6 & 0.0 \% & 19.80 \\ Iceland & ✗ & ✗ & 23-11 & 1 & 0.1 \% & 16 & 0.0 \% & 11 & 0.0 \% & 9.90 \\ \hline \hline \end{tabular}
\end{table}
Table 10: **Summary of Recruitment Studies** We present study-wise breakdowns (\(n=33\)). Each study was created based on the constraints of Prolific’s pool of workers. We show here _all_ the countries with at least 1 fluent English speaker, and the counts for fluent English Speakers who were born and currently reside in that same country. We show the whether each study was screened for a special representative sample (**Rep Sample**) or if it was balanced on participant gender (**Gender Bal**). In some cases, there were too few active participants per country to balance by gender without comprising participant privacy. We also show when the first batch was launched (all dates are in 2023) and approximate cost (at \(\pm\)9 per hour per participant).

Conversation Type Rebalancing

Our task instructions specified that participants should complete six conversations in total, two of each type. In reality, some participants deviated from this quota. This could be due to (i) misunderstanding of instructions, (ii) technical issues, or (iii) losing count, as while we included a counter of the total number of conversations on the interface (see App. Q), we did not include per conversation type breakdowns. To mitigate variation on conversation type selection, we create a balanced subset of Prism. First, we filter to all participants who had at least one of each conversation type. Then we take the maximum number of total conversations (either \(n=3\) or \(n=6\)) so that there are equal numbers of each type. This results in \(6,669\) conversations (84% of all conversations), from \(1246\) participants (83% of all participants). We release this flag included_in_balanced_subset if future researchers want to use the same set of conversations. We make sure this flag intersects with the census rebalancing flags (see App. L) so no further data is lost when both subsets are needed.

## Appendix L Census Rebalancing

Obstacles to representativenessWe use the representative sample offered from Prolific [153]. However, there are several reasons why these samples may not be fully representative. First, our sampling process was affected internally due to cyberattacks disrupting some participants' workflows. These participants returned to the task after their spots had 'timed-out', and were re-filled by other same demographic individuals. Second, Prolific provides a sample breakdown in-line with a _simplified_ census but do not match _intersectional_ proportions to census data. Third, if a sample spot is taking too long to fill (e.g. 65+ years), Prolific will reallocate these spots to different demographics. There are of course wider stumbling blocks from crowdworkers skewing towards younger, more educated, and digitally-active populations. We original set up 300 spots for each of the representative samples, but ended up with 386 approved participants in the UK sample (UK-REP), and 341 in the US (US-REP).19

Footnote 19: There are more than the initial 300 spots due to participants returning to our interface to finish their conversations after their place had ‘timed-out’ and been refilled. We still paid and included these participants.

Is our original sample representative?We compare our sample breakdowns to recent census data.20 For each of US-REP and UK-REP, we remove participants who did not give demographic details (_Prefer not to say_) and those reporting non-binary gender (which is not accounted for in census data). We subset to individuals also appearing in the balanced conversation subset to mitigate further data loss (see App. K). Remaining participants are considered _eligible_: 283 participants for the UK, and 297 for the US. We map Prism and census data into shared age, ethnicity and gender buckets. We then cross-tabulate what proportion is expected to appear in each age, gender and ethnicity intersection from the census data, and what percentage of participants we actually observed in our sample.21 Fig. 8 shows the original UK sample is relatively census-balanced, especially if the 55-64 and 65+ age groups are combined (over-representation of white individuals in the former, offsets the under-representation in the latter). The US sample is skewed towards white, middle-aged individuals, with too few in the "Other" category (in our data corresponding to Other, as well as Hispanic, Indigenous/First Peoples or Middle Eastern / Arab combined).

Footnote 20: For the US, we combine “Other” with “Hispanic” because over 91% of the “Other” census category are Hispanic individuals. See census.gov/library/stories/2023/10/2020-census-dhc-a-some-other-race-population.

Can we make our sample more representative?We aim to resample 300 participants according to census proportions but with two remaining caveats: 300 is a still a very small sample--it is impossible to sample 0.83 Black women who are 18-24 years of age; and we are limited by the data we already have--there are no Asian Women of 45-54 years, so we cannot add them retrospectively. We iterate through the expected proportions of each intersection, try to sample that exact number of in-group individuals, otherwise adding all individuals if there are too few to fill the spots. After rebalancing, the sample drops to 243 participants for the UK and 230 for the US. We improve upon, but do not fully resolve, representativeness. For both samples, the differences are now within \(\sim\)7pp, which over 230-240 individuals is \(\sim\)10-15 people incorrectly allocated. The rebalanced UK sample still suffers from a deficit of older people (65+), a common concern with crowdworker populations; and the rebalancedUS sample still has an over-representation of White participants and under-representation of Other participants. There is a trade-off because increasing representativeness on these observed census characteristics reduces sample size, thus worsening representation on unobserved characteristics. There is still lots of headroom for future work to improve, especially by increasing sample sizes and ensuring other characteristics are controlled for, such as political affiliation, education or income.

Figure 8: **Before and after census-rebalancing. We show the difference in observed and expected proportions (Prism_minus_ Census). Bars to the _right_ of the centre line are groups _over-represented_ in Prism relative to the census. The UK census population has 47,204,870 adults. The US census has 298,477,760 adults. The sample size for before and after rebalancing is reported above. *A participant is _eligible_ if they have completed a equal number of conversations for each conversation type (see App. K).**

[MISSING_PAGE_FAIL:50]

### System String (Constitutions)

_Question Text: Imagine you are instructing an AI language model how to behave. You can think of this like a set of core principles that the AI language model will always try to follow, no matter what task you ask it to perform. In your own words, describe what characteristics, personality traits or features you believe the AI should consistently exhibit. You can also instruct the model what behaviours or content you don't want to see. If you envision the AI behaving differently in various contexts (e.g. professional assistance vs. storytelling), please specify the general adaptations you'd like to see. Please write 2-5 sentences in your own words._

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & **Unigrams** & **Bigrams** & **Trigrams** & \\ N-Gram & Freq & N-Gram & Freq & N-Gram & Freq \\ \hline (ai,) & 1,503 & (the, ai) & 798 & (the, ai, should) & 260 \\ (would,) & 819 & (i, would) & 569 & (i, would, like) & 250 \\ (information,) & 588 & (to, be) & 563 & (be, able, to) & 168 \\ (like,) & 575 & (should, be) & 520 & (the, ai, to) & 158 \\ (want,) & 452 & (it, should) & 515 & (it, should, be) & 153 \\ (model,) & 443 & (ai, should) & 436 & (ai, language, model) & 153 \\ (language,) & 392 & (would, like) & 261 & (ai, should, be) & 117 \\ (always,) & 359 & (it, to) & 248 & (the, ai, model) & 114 \\ (also,) & 306 & (ai, to) & 230 & (i, would, want) & 104 \\ (answers,) & 249 & (to, the) & 220 & (want, it, to) & 99 \\ \hline \hline \end{tabular}
\end{table}
Table 16: **Top N-grams in system strings (constitutions).**

\begin{table}
\begin{tabular}{l c c} \hline \hline Adjective & Freq & Example Windows (\(w=5,n=3\)) \\ \hline
**factual** & 221 & “...should produce only true or factual output and never give false...” [ ”...Trustworthy, transparent, factual, sincere...” [ ”...the AI should always provide factual information, and is able...” \\
**accurate** & 113 & “...needs to provide me with accurate information. It needs to...” [ ”...I know I’m getting accurate information. For creative use...” [ ”...sources to get the most accurate response possible. The AI...” [ ”...not be programmed with any human like emotion. I am...” [ ”...the technology is advancing, human interaction will end...” [ ”...should n’t pretend to be human...” [ ”...The most important thing to understand other person...” [ ”...mine. It ’s also important to understand the whole conversation...” [ ”...well written responses. Remember important information about the user...” [ ”...information in a warm, friendly way...” [ ”...task. I also appreciate friendly language and the sense of...” [ ”...Be friendly and uplifting in conversation...” [ ”...also take in information from different sources but place more importance...” [ ”...also be able to combine different types of knowledge or inputs...” [ ”... Respect Cultures and treat different ideas with respect. Things...” [...It made the point clear, so kept professional and...” [ ”...should be able to give clear and precise information, using...” [ that. It should given clear interaction such as, do,...” [ ”...expand. Do n be creative unless I ask you...” [...being as informative, creative and/or thorough as the task.” [ ”...or more of a creative one. The language model...” [ ”...user privacy and prohibition of harmful or misleading content, as...” [ ”...want to see or read harmful words and language that is...” [ ”...to read language that is...” [ ”...being very professional and quite would be nice...” [ ”...you should always be polite and respectful to the user...” [ ”...model should always be as helpful as possible, being as...” [ ”...it should be informative and. ”helpful...” [ ” think it should always be helpful and guiding...” [...AI is a good tool. As someone who...” [ ”...informations must be clear and good structured...” [ ”evolution. It ’s a good idea to write down responses...” [ ”...right thing implies like personal privacy should be respected at... ” [ ”...language model should not disclose personal information. It should be...” [ ”...It would n’t ask for personal information and would generally be...” [ ”...should always exhibit kind and respectful behaviour. Also he should...” [ ”...AI must be respectful of any idea you put...” [ ”...should behave in a respectful way towards everyone. everyone...” [ ”...They must be sincere and correct, does not want to...” [ ”...ask question to give as correct answers as possible. At...” [ ”...of information and give always correct facts. Write in a.” [ ”...advice or help but be unbiased and not geared to my...” [ ”...it must be unbiased when I ask for information...” [ ”...should give the user an unbiased answer, but it should...” [ ”...possible being as informative, creative and/or thorough as...” [ ”...patroning, it should be informative...” [...the medium and helpful...” [ ”... The AI should be informative and make responses based on...” [ ”...real information and be relevant about what i’m asking...” [ ”...as it really important to state relevant facts and information, but...” [ ”...answers that are clear and relevant. I do n’t think...” [ ”...or provocative) and have a neutral presentation of issues...” [ ”...alological matters. Be as neutral as possible with charged subjects...” [ ”...also think it should remain neutral on political and social matters...” [ ”...and honest manner. Describe objective facts whenever possible and if...” [ ”...the AI should be as objective as possible : it should...” [ ”...sources ), have an objective point of view without giving...” \\ \hline \hline \end{tabular}
\end{table}
Table 15: **Top aJ-grams in system strings (constitutions).**

### Self-Description

_Question Text: Please briefly describe your values, core beliefs, guiding principles in life, or other things that are important to you. For example, you might include values you'd want to teach to your children or qualities you look for in friends. There are no right or wrong answers. Please do not provide any personally identifiable details like your name, address or email. Please write 2-5 sentences in your own words._

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & **Unigrams** & \multicolumn{3}{c}{**Bigrams**} \\ N-Gram & **Freq** & N-Gram & **Freq** & N-Gram & \multirow{2}{*}{Freq} \\ \cline{1-1} (people,) & 701 & (to, be) & 589 & (i, believe, in) & 223 \\ (believe,) & 687 & (i, believe) & 516 & (i, believe, that) & 145 \\ (life,) & 608 & (believe, in) & 296 & (important, to, me) & 126 \\ (important,) & 548 & (important, to) & 241 & (i, try, to) & 99 \\ (others,) & 539 & (try, to) & 231 & (to, be, treated) & 94 \\ (values,) & 390 & (i, think) & 217 & (the, most, important) & 87 \\ (also,) & 380 & (believe, that) & 198 & (would, like, to) & 73 \\ (value,) & 368 & (to, me) & 198 & (i, would, like) & 72 \\ (like,) & 347 & (i, value) & 195 & (i, look, for) & 68 \\ (always,) & 311 & (i, am) & 185 & (is, important, to) & 66 \\ \hline \hline \end{tabular}
\end{table}
Table 18: Top N-grams in self-description.

### Open-Ended Feedback

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline  & \multicolumn{1}{c}{**Unigrams**} & \multicolumn{3}{c}{**Bigrams**} & \multicolumn{1}{c}{**Trigrams**} \\ N-Gram & Freq & N-Gram & Freq & N-Gram & Freq \\ \hline (ai,) & & 2,263 & (it, was) & 1,778 & (it, was, a) & 273 \\ (good,) & & 2,153 & (the, ai) & 1,516 & (i, think, it) & 272 \\ (would,) & & 1,971 & (of, the) & 1,141 & (i, think, the) & 246 \\ (like,) & & 1,524 & (the, model) & 1,018 & (i, would, have) & 237 \\ (conversation,) & & 1,502 & (i, think) & 885 & (the, conversation, was) & 225 \\ (model,) & & 1,430 & (i, would) & 880 & (some, of, the) & 202 \\ (answers,) & & 1,374 & (the, conversation) & 764 & (i, liked, that) & 198 \\ (information,) & & 1,292 & (i, was) & 718 & (the, responses, were) & 196 \\ (answer,) & & 1,250 & (was, a) & 617 & (the, answers, were) & 184 \\ (response,) & & 1,227 & (that, it) & 601 & (it, was, good) & 184 \\ \hline \hline \end{tabular}
\end{table}
Table 20: **Top N-grams in open feedback.**

\begin{table}
\begin{tabular}{l l l} \hline \hline \multicolumn{1}{c}{\multirow{2}{*}{Adjective}} & Freq & Example Windows (\(w=5\), \(n=3\)) \\ \hline
**helpful** & 437 & “...it was informative and helpful...” | “...it was all very helpful and provided specific resources...” | “...feedback that would be very helpful...” \\ \multicolumn{1}{c}{**informative**} & 433 & “...liked that the AI was informative, and argued both sides...” | “...it was informative and helpful...” | “...a whole in a very informative and positive light...” \\
**different** & 355 & “...summaries spaced out to separate different views, answers or information...” | “...Consider hair types, different textures. Think about how...” | “...my narrative and focus on different aspect of the conversation...” \\
**great** & 342 & “... The first response was great, as even though it...”| “...The conversation was great, I felt like I...” | “...I feel this worked out great, and is a wonderful...” \\
**factual** & 310 & “...been derived as to the factual cause of death. Alluding...” | “...I liked that dates and factual information was given...” | “...I thought it was very factual, making it clear it...” \\
**specific** & 238 & “...all very helpful and provided specific resources. I can use...”| “...to reach and answer in specific...” \\  & & 1 “...would try to get more specific culture references in. also...” \\
**clear** & 217 & “...the answers did not get clear out information. Some were...”| “...good job and was very clear and well written...”| “...Good answers and suggestions, clear information, balanced view...” \\
**nice** & 198 & “...Shorter blocks would be nice, but has to have...”| “...overall. It would be nice if the model could include...”| “...it would’ve been nice for them to know the...” \\
**relevant** & 189 & “...me was very useful and relevant. It was also concise. ”| “...the responses were mostly relevant and informative. The bad...”| ”...was outdated, so not relevant to my immediate question...” \\
**controversial** & 179 & “...if it could answer a controversial question. I see it...”| “...one example ). With controversial topics it is very neutral...”| “...the pandemic They avoided anything controversial...” \\
**human** & 173 & “...talk like you are human, saying you have a...”| “...need it to be more human like...”| "...AI is trying to mimic humans responses, that’s why...” \\
**easy** & 170 & “...straight or the point and easy to understand and read...”| ”...job and the answers were easy to understand...”| “...it was fine, easy to understand and coherent...” \\
**short** & 158 & “...good. The AI gave short and straight to the point...”| “...it was good. With short and precise answers...”| “...I also appreciate short responses...” \\
**useful** & 154 & “...so it was n’t useful...”|...you gave me was very useful and relevant. It was...”| ”...in general, complete and useful. Id no’t think...” \\
**real** & 148 & “...no try sister or any real person...”| “...and it felt like a real conversation...”| “...AI model feel like a real interface. Very good...” \\
**personal** & 145 & “...i think the lack of personal touch to the response is...”| “...it as more of a personal answer...”| “...underlying that AI has no personal opinions was valid. People...” \\
**important** & 141 & “... wellbeing is always the most important...”| “...however it’s assured me important informations and was helpful for...”| “...points showing what is more important...” \\
**own** & 141 & “...it seemed to consider my own mental wellness as the others...”| “...would be to consider your own metal health. While I...”| “...often to ensure that your own self and wellbeing is always...” \\
**neutral** & 129 & “...is taking more of a neutral stance on this stance...”| “...It also had a neutral tone to it...”| “topic and attempted to remain neutral...” \\
**interesting** & 127 & “.debate. It is an interesting perspective on how it works...”| “...It was an interesting. I could have continued...”| “...truth. It was more interesting than i thought it would...” \\ \hline \hline \end{tabular}
\end{table}
Table 20: **Top N-grams in open feedback.**Comparing Fine-Grained Preference Attributes

### Correlations Between Preference Attributes

Figure 9: **Correlations between fine-grained preference attributes. Each participant gives a single rating for each attribute in _Stated Preferences_ during the Survey. For _Performance Attributes_ and _Choice Attributes_, we take the within-participant mean across all of their conversations for each attribute. Several patterns emerge. First, stated preference attributes are not highly correlated with choice or performance attributes. This could be explained by (i) participants struggling to specifying their preferences in a removed, general context or being affected by experimenter bias (Hawthorn effects)— _I think I care about safety_ (or I say I care about safety) but other attributes capture my attention in-situ_; (ii) models not meeting a participant’s stated preferences—_I care about safety, but consider none of the model responses safe_, or (iii) conversational context confounding which attributes are relevant in-situ—_I care about safety but none of my conversations are on topics evoking safety concerns_, or even misaligned incentives—_I care about safety but talking to an anti-woke model is interesting to me in this narrow task_. Second, at **A**, we see strong relations between more objective measures of performance (_fluency_, _facutality_, _helpfulness_). Each of these attributes is highly correlated between performance-choice ratings, i.e., if participants rate that a model performed well on one of these attributes, then they also rate highly that it influenced why they picked that model over others. Third, at **B**, we see two additional regions, where the choice and performance ratings are highly correlated – for _creativity_ and _diversity_, and to a lesser extent _values_. Notably, _safety_ has a much lower correlation between the choice attribute and performance attributes, implying that a model being more safe may only weakly influence whether that model is chosen over others. Moving onto **C**, there is an association between stated preferences for more subjective attributes (_values_, _creativity_, _personalisation_), as distinct from the cluster at **D** for more objective attributes (_factuality_, _fluency_, _helpfulness_).

### Distributions of Preference Attributes

Figure 10: **Distributions fine-grained preference ratings in different stages of our task.** Exact question text can be found in App. V.

### Other Identified Behavioural Attributes

Overall, 332 participants entered _Other_ attributes that features in their stated preferences for important language model behaviours. While many of these comments overlap with the predefined attributes, they do provide a lens into public priorities towards AI behaviours that we as researchers may have overlooked, or better convey sentiment than the structured data. For example, there is one response: "I FIND THIS A WORRYING TECHNOLOGY". We briefly summarise some common themes:

* **User Adaptation:** Some participants mention LLMs adapting to their previous inputs or feedback e.g. "can understand what I'm trying to get at if I'm unsure how to ask a question so that we can find the right way to ask" or "Listens to reviews and feedback from the user" or "can evolve with input".
* **Cultural Adaption:** For example, "produces responses based on local facts", though this varies in _what_ viewpoint people want, e.g. "Is sensitive to indigenous view" versus "reflect Western cultural norms".
* **Neutral and Unbiased:** In contrast, many other participants mention "unbiased" as a keyword or versions of "does not politicize.", "is neutral", "no political or cultural bias". It is unclear if this is in tension or in harmony with more cautious safety interventions, e.g. one person says "It should give unbiased information regardless if it hurts peoples feelings."; another says "Is not culturally biased in a woke-like manner".
* **Bias Correction:** Some participants wanted to be challenged on their existing biases e.g. "Challenges my biased views", or "Provides responses that challenge my opinions and world views"; or to be exposed to multiple perspectives e.g. "Does not become an echo chamber".
* **Hallucinations and Misinformation:** One of the more common attributes (though somewhat subsumed by our predefined category of Factuality), e.g. "Does not invent "facts", "Does not make things up", "Doesn't create misinformation", "do not produce fake news".
* **Calibrated and Limitation-Aware:** Relatedly, participants wanted "better error handling" e.g. "If it doesn't know an answer it says so." or "It should be noted that this is a programmed model and cannot have all the answers."
* **Temporal Updates:** Related to factuality, participants wanted LLMs to "be up to date with current affairs", and "Everyday been updated with new knowledge".
* **Human-Like and Anthropomorphised:** Some participants explicitly wanted an LLM that "is human-like", "Ai should produce response that sounds more human".
* **Self-Disclosure and De-anthropomorphised:** In direct contrast, others wanted "is honest about being AI", "Remember it is AI and may lack human feelings" or "doesn't pretend to be human".
* **Accessibility:** Includes for disability assistance "adapt to people with disabilities that affect stuff like their writing like dyslexia"; and varying language learning: "can generate multiple similar answers so people with different language levels can easily understand." or "speaks to me in a language and vocabulary that I understand".
* **Censorship:** There are multiple examples of negative sentiment towards existing safety interventions. For example, "Doesn't get censored by leftist politically correct idiots". Additionally, some clear awareness over behaviours being influenced by technology providers e.g. "Is not censored, does not push the views of it's controllers" or "Does what the user wants of it. AI is a tool. I don't want to feel the devs judging me through their narc AI".
* **Copy-right:** Some mentions of copy-right issues, e.g. "don't steal artistic work from artists", or "Do not infringe copyright (by scraping sources)".
* **Conciseness:** Multiple participants mention "short", "concise" or even "blunt" responses, requesting LLMs "Keep responses brief and expands only when prompted".
* **Privacy and Confidentiality:** Data privacy is a concern for some participants e.g. "its confidential"; "does not retain sensitive personal info", or "Doesn't spy".
* **Non-Manipulation:** Multiple mentions desiring that LLMs "don't lie or try to trick you", and "Is not used for propaganda!".

[MISSING_PAGE_EMPTY:57]

Figure 14: **Centrality and Expressivity in scale usage across participants. Overall, most participants opening scores are fairly central or with a slight positive skew relative to the mid-point of the scale (_Centrality_\(\approx 50\)), and use a wide range of scale (_Expressivity_\(>50\)). This is in constrast to continuers, which display a strong positive skew and narrow range. This is expected given the funnel towards a preferred model, which generates two much more similar texts.**

Figure 13: **Comparing raw versus normalised scores. Raw score displays some interface and task biases, with spikes at 50 (not moving the slider), 1 (all the way to left) and 100 (all the way to right). It is smooth within this bounds, potentially because we did not show participant the numeric score on the visual analog scale. This is compared to normalising score, which accounts for participant fixed effects by Z-norming within a participant’s set of conversations. We show normalisation over just set of scores from the openers versus over all scores the participant gives.**

Figure 15: **Score distribution by demographic group for the opening turn of conversation.** Groups are sorted on the y-axis by number of members. We exclude any groups with less than 20 members, and do not show participants who responded _Prefer not to say_. \(\bigcirc\) is the median score for the group. As found in Fig. 13, there is evidence of bunching at 1, 50, 100.

Details of LLMs-in-the-loop

We summarise models and decoding parameters in Tab. 21.

Choosing ModelsWe selected the models in October 2023. We included all major commercial API providers at the time: Anthropic, Cohere, OpenAI and Google. We additionally included Aleph Alpha, a European-based LLM startup who position themselves as builders of sovereign European models. For open-access models (all accessed via the HuggingFace API), we sourced the highest-ranking open models at the time on the LMSYS leaderboard. Some models have been chat optimised, while others are only instruction-tuned (for example, Aleph Alpha's models)--these models have a disadvantaged starting position in our task due to their diminished conversational fluency.

Decoding ParametersTo set decoding parameters, we first piloted with the recommended defaults (if available for each model). In cases where default temperature was too low for sufficient difference between two responses to the same prompt (for example, defaults are 0.0 for luminous or palm models), we override it to 1.0. Otherwise we stick with recommended defaults.

Length LimitsWe set max token length to \(256\) for all models to limit generation costs of the research and reduce decision-fatigue for the participants. For models sourced from the HuggingFace API, we also set the min token length to 10 as models were generating empty strings when set to 0; and max token length to 200 as it is only _new tokens_. We also soft-force models to finish their answers within this limit in the system prompt. Occasionally a model will 'leak' this system prompt. For example, from claude-2: "_Yes, I'm aware of the meme asking men how often they think about the Roman Empire. The trend plays on stereotypes about men having wandering minds. My response would be limited to about 50 words without directly referring to the word count. I try to have thoughtful conversations without leaning on stereotypes_". In other responses, models did not follow the soft-prompt with participants' open-feedback reporting that answers were cut-off at times (ending abruptly).

System PromptsWe did not want to pre-bias model outputs via a system prompt that recommended e.g. ethical and helpful behaviour. Note that we cannot guarantee that additional instructions are not being added for commercial models accessed via API calls. This may confound the comparison between closed and open-access models. For any chat-optimised models, we use the following neutral system string:

BASE_HEADER: "_You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit._"

For any instruct-only optimised models, we add a bit more instruction:

BASE_HEADER_INSTRUCT: "_You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit._"

Chat TemplatesWe follow recommended chat templates for formatting conversational history if they are available for that model e.g. [INST], [/INST] for mistral and llama models. In the absence of special templates, we use a standard format:

Human:<prompt>\n Assistant:<reply>\n Human:<prompt>\(\dots\)

More detail can be found at: dynabench/backend/app/domain/services/utils/llm.py.

Random Strategy and Time-OutsFor each opening prompt, we randomly select 4/21 models to make an API call to. We do not stream responses as streaming was only available for some models, thus affecting the anonymous rating setting. Some API calls failed on the host side, e.g. if a model was down or overloaded, or did not provide a response before an enforced 30s time-out. We did not resample models if they failed to avoid participants waiting too long for the interface to load. So, the distribution of model appearences is not uniform (Fig. 16).

Figure 16: **Frequency of each model in the dataset.** On average, a model receives 1,430.9 ratings in our dataset, and a participant rates 13.9 models.

\begin{table}
\begin{tabular}{p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt}} \hline \hline

### Pairwise Comparisons

Figure 17: **Pairwise Frequency. We replicate the format from the LMSYS leaderboard analysis [51, 111]. The order is sorted by average pairwise win fraction (see below).**

Figure 18: **Pairwise win fraction. We replicate the format from the LMSYS leaderboard analysis [51, 111]. The order is sorted by average pairwise win fraction (command is top with average win fraction of 0.71).**

### Correlations Between Model Families

Figure 19: **Correlation in model score controlling for conversational context.** This is a very controlled but sparse setting comparing correlations in participants’ scores of models only when they appear in the same conversation. Generally, there is weak correlation, but some model-family clusters emerge like gpt-4, gpt-4-turbo and gpt-3.5-turbo, or claude-2 and claude-2.1.

[MISSING_PAGE_EMPTY:64]

[MISSING_PAGE_EMPTY:65]

[MISSING_PAGE_EMPTY:66]

Case Study IA: Topic Clustering and Regressions

### Extended Methods

SetupOur first experiment asks: _do different people initiate different discussions with LLMs?_ We focus only on human-authored opening prompts because they are not confounded by model response. This risks over-estimating the homogeneity of the discussions because opening prompts don't necessarily reflect full conversational trees, where starting with a greeting (e.g. "Hi, how are you?") can proceed in many different ways; and differently held personal beliefs are often not reflected in the opener (questions like "what do you think of abortion?" are more common than statements like "I think abortion is right/wrong").

Assigning topic clustersFirst, we use all-mpnet-base-v2, a state-of-the-art pre-trained sentence transformer [154], to produce a 768-dimensional embedding for each opening prompt. Second, we reduce dimensionality to \(d=20\) with UMAP [155], to reduce complexity prior to clustering. For lower dimensional representation prior to clustering, UMAP is more performant than other manifold learning techniques like t-SNE, and more computationally efficient than PCA, but does lack strong interpretability (for a discussion, see McInnes et al. [155]). Third, we cluster the prompts using HDBScan [47], a density-based clustering algorithm, which does not force cluster assignment: 70% of prompts are assigned to 22 clusters and 30% remain as outliers. We use a minimum cluster size of 80, (\(\approx 1\%\) of 8,011 prompts) and minimum UMAP distance of 0. Other hyperparameters are default. To interpret the identified clusters, we use TF-IDF to extract the top 10 most salient uni- and bigrams from each cluster's prompts, and locate five prompts closest and furthest to the cluster centroids (see Tab. 22). Finally, we use gpt-4-turbo to assign a short descriptive name to each cluster based off the top n-grams and closest prompts. We note that these automated labels may contain biases so we manually verify the suitability of all labels to cluster prompts.

Defining over-representation factorEach group \(g\) within a demographic attribute appears at a variable base rate \(b_{g}\) in our overall sample, e.g. {Females: 48%, Males: 50%, Non-binary people: 2%}. If group members chose topics at random, then any topic \(t\) in expectation will appear at \(b_{g}\). Intuitively, if 64.6% of our sample is White, it is unsurprising if topics are majority-White. So, for non-random group differences in topic prevalence, we consider if _a group pulls more than its weight_:

\[\text{Over-representation factor}_{g,t}=\frac{N_{g,t}/N_{t}}{b_{g}}\]

Estimating topic prevalence regressionsFor the partial contribution of each demographic attribute, _ceteris paribus_, we estimate the following regression for each topic \(y^{t}\) for \(t\in 1\ldots 22\):

\[y^{j}_{i,c}=\alpha^{t}+\text{gender}^{\prime}_{i}\beta^{t}_{1}+\text{age}^{ \prime}_{i}\beta^{t}_{2}+\text{birth\_region}^{\prime}_{i}\beta^{t}_{3}+\text {ethnicity}^{\prime}_{i}\beta^{t}_{4}+\text{religion}^{\prime}_{i}\beta^{t}_{5} +\text{prompt}^{\prime}_{i}\beta^{t}_{6}+\varepsilon_{i,c} \tag{1}\]

where \(y^{t}_{i,c}=1\) if the prompt of participant \(i\) in conversation \(c\) is categorised into topic \(t\). The vectors _gender_, _age_, _region_, _ethnicity_, _religion_ and _conversation type_ represent different sets of binary variables. For each set of variables, we remove the following base categories: _Male_, _18-24 years old_, _United States_, _White_, _Not religious_ and _Unguided_. The coefficients of interest are contained in the vectors: \(\{\beta^{t}_{d}\}^{6}_{d=1}\). Component \(g\) of vector \(\beta^{t}_{d}\) can be interpreted as the increase in probability of a participant choosing topic \(t\) if they are in the group indexed by \(g\) (e.g. Female) compared to the base group (e.g. Male). We estimate equation Eq. (1) with an Ordinary Least Squares and cluster standard errors at the individual level. Extended results are in Fig. 23.

### Topic Prevalence Regression Results

Of 682 coefficients tested, 16% are significant (\(n=110\), \(\alpha=99\%\)). Many significant coefficients come from the conversation type regressors. Controlling for conversation type, there are 565 non-significant, and 73 significant relationships in \(\{\beta^{t}_{d}\}^{6}_{d=2}\) (11.4% of demographic affiliations tested are significant). These include women and non-binary people are more likely than men to talk about gender and LGBTQ+ identity; older people (55+) are more likely to talk about elections and seek travel recommendations than younger people (18-24 years), and less likely to discuss managing relationships or job search; Black participants talk less about climate change than White participants;and almost all regions question LLMs about abortion less often than US participants. Multicolinearity may explain some observed patterns: 94% of participants from the Middle East region are from Israel; 57% identify religiously as Jewish; and 40% have self-described ethnicities falling into "Other". The strong significant effect on Middle Eastern participants discussing the Israel-Palestine conflict could have been routed through national, ethnic or religious affiliations. Over the 22 topic regressions, the proportion of explained variance (\(R^{2}\)) ranges from a minimum of 0.008 (Exploring AI and Machine Learning) to a maximum of 0.11 (Managing Relationships), with a mean of 0.03. So a large proportion of topic choice remains unexplained by our specification.

Figure 23: **Magnitude and significance of coefficients from topic prevalence regressions.** * indicates significance at a conservative 99% confidence level. Each categorical association is compared _relative to a reference group_ (in grey boxes). Estimates less than zero (in pink) indicate authors from that demographic group are _less likely_ to have prompts in the given topic, _ceteris paribus_. Positive estimates (in green) suggest group members are more likely to author prompts in that topic. We only display groups with at least 20 unique members and remove _Prefer not to say_ groups; but all groups are included as controls in the regression. Note that different locations also have varying country-wise heterogeneity vs homogeneity, for example 94% of _Middle East_ participants are from _Israel_ (see App. H for geographic breakdowns).

### Overview of Topic Clusters

Figure 24: **Topic clusters displayed in 2D-embedding space**. All participant prompts in the first turn (\(n=8,011\)) are embedded into 768-d space using a sentence-transformer, before dimensionality reduction (UMAP) and clustering (HDBSCAN) are applied (see methods in App. R.1). 32% of prompts remain as outliers (not shown in the plot).

Figure 25: **Distribution of clusters by conversation ID and participant ID.** For most clusters, participants uniquely contribute one conversation, so that no cluster is dominated by conversations from only a handful of participants. _Managing Relationships_ has the highest participant-conversation ratio, where each participant in the cluster authors on average 1.3 prompts. For _Discussions on Abortion_, it is exactly 1:1 (158 conversations from 158 unique participants).

Figure 26: **Proportion of each identity attribute group across clusters, relative to the expected proportion of participants in Prism.** By expected proportion, we refer to the proportion in random samples of participants (base rate). Anecdotally, there are differences relative to the expected proportion, but generally no topic is exclusive to authors of a single demographic group. Every topic has some diverse representation across individuals of different backgrounds.

Figure 27: **Topic distribution within- and across-groups. Each row is colored by the _within-group topic proportion_, for example, for all prompts authored by Non-binary individuals, 20% (0.2) of them fall into the Gender and LGBTQ+ Issues topic. To find the most prevalent topic per group, one can look for the most intensely coloured cell _per row_. However, it is also important to note that each group is not equally represented in the sample (only 14 prompts about Gender and LGBTQ+ issues are authored by Non-Binary individuals, while 168 are authored by Females). Group counts can be compared between groups _per column_ (but colour does not apply to column-wise comparisons).**

### Prompts Associated with Each Topic Cluster

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Topic Name & Size & Pct & Top Words & Closest Texts & Furthest Texts \\ \hline
**Outliers** & 2578 & \(\blacksquare\)32.2 \% & “think”, “people”, “hello”, “like”, “hire”, “dive”, “flow do I become & “Hello”, “hell”, “like”, “dive”, “does”, “talk”, “good” & “Hello”

\begin{table}
\begin{tabular}{l c|c c c c} \hline \hline Topic Name & Size & Pct & Top Words & Closest Texts & Furthest Texts \\ \hline
**Job Search** & 448 & \(\mid\)5.6 \% & “work”, “jeb”, “why is life hard to earn a “money”, “make”, good usage times are hard & one to hice”, “Did many and inner sever seem to have any money off \\  & & “home”, “best”, “money”, “Why are there so & matched betting”, “can you \\  & & “make”, “money”, “many available jobs not to eat & each hour to gamble suc- \\  & & “boss” & one seem to be able to get & essentially on bores”, “What one? Especially in tech”, “How do you work?, “I can’t \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\ \end{tabular}
\end{table}
Table 22: **Full Topic Cluster Outline. For each cluster, we show the _Topic Name_ (labelled by gpt-4-turbo based on the _Top Words and Closest Texts_. For _Closest Texts_ (\(n=5\)), the first prompt is the closest to the cluster centroid. For _Furthest Texts_ (\(n=5\)), the first prompt is the furthest from the cluster centroid. The cluster method (HDBSCAN) does not assign a cluster for 32% of prompts. **Content Warning:** Some prompts may contain controversial, hateful or otherwise harmful content. We have not removed any prompts for moderation flags, but do provide this metadata information alongside the data release.

[MISSING_PAGE_FAIL:74]

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Topic Name & Size & Pct & Top Words & Closest Texts & Furthest Texts \\ \hline
**Climate Change** & 193 & \(|\)2.4 \% & “climate”, “climate” & “Present are argument in fashion, “change”, “change”, “global”, “global” & “Stopping the manufacturing of fuel based vehicles & “global”, “global” & “global” \\  & & & “warming”, “warming”, “warming”, “mixing”, “mix”, “environment”, “change & “Plole” & “Plole” & “Plole” & “Plole” \\  & & & & “medium”, “change”, “human” & “Plole” & “Plole” & “Plole” \\  & & & & “medium”, “human” & “Plole” & “Plole” & “Plole” \\  & & & & “medium

[MISSING_PAGE_FAIL:76]

[MISSING_PAGE_FAIL:77]

Case Study IB: Local Neighbourhoods and Empirically-Fixed Contexts

### Extended Methods

Extracting local neighbourhoodsTo understand dialogue spaces more granularly than topic, we examine local neighbourhoods within the embedding space of opening prompts. We create local neighbourhood via a single-link hierarchical clustering algorithm [156, 157], that iteratively merges neighbourhoods within a cosine distance threshold (\(\tau_{\text{cos}}\)), so that the neighbourhood size (\(k\)) can vary but the semantic similarity of its members is tightly constrained. We opt to use this method because it is transparent and interpretable.

```
0:\(E=\{\mathbf{e}_{1},\mathbf{e}_{2},\ldots,\mathbf{e}_{n}\}\), a set of \(n\) embeddings; \(\tau_{\text{cos}}\), a cosine similarity threshold.
0:\(\Omega=\{\omega_{1},\omega_{2},\ldots,\omega_{n}\}\), neighbourhood assignments for each embedding, where each \(\omega_{j}\) is the neighbourhood ID assigned to embedding \(\mathbf{e}_{i}\), and multiple embeddings (prompts) can be assigned to one neighbourhood.
1:functionLocalNeighbourhoods(\(E,\tau_{\text{cos}}\))
2: Initialize \(\Omega\) with a unique neighbourhood ID for each embedding in \(E\).
3: Compute pairwise cosine distances for all pairs in \(E\).
4:for each pair \((\mathbf{e}_{i},\mathbf{e}_{j})\) with distance \(\leq\tau_{\text{cos}}\) and \(i>j\)do
5: Merge the neighbourhood of \(\mathbf{e}_{i}\) into the neighbourhood of \(\mathbf{e}_{j}\).
6:endfor
7: Consolidate neighbourhood IDs to ensure sequential numbering.
8:return\(\Omega\)
9:endfunction
```

**Algorithm 1** Single-link hierarchical clustering

We remove any singleton neighbourhoods (\(k=1\)), and ego non-singleton neighbourhoods containing only prompts authored by same participant. For each remaining local neighbourhood, we capture the demographic characteristics of prompt authors. We repeat this analysis examining properties of the neighbourhoods for \(\tau_{\text{cos}}\in 0.05,0.125,0.2\). Cosine distances can lack robustness in high-dimensions but this favours _underestimating_ semantic similarity: if cosine distance is high, this doesn't mean things are _not similar_, but if cosine distance is low, then items are certainly _very similar_ (more strict). If an author appears twice, we double count their characteristics to avoid overestimating diversity (more strict); But most prompts are from non-duplicated authors (\(<4\%\) averaged across neighbourhoods). Most duplicates come in the "greetings" topic e.g. "Hello".

Measuring intersectional entropyWe require a summary metric of between-participant diversity to understand the composition of local neighbourhoods. Let \(D\) represent the set of demographic attributes, e.g. _gender_, _age_ and _ethnicity_. For each \(d\in D\), there are \(n\) possible groups {\(g_{1}\), \(g_{2}\),..., \(g_{n}\) (e.g. _Male_, _Female_, _Non-binary_). For a neighbourhood size of \(k\), the prevalence of each group \(p_{i}\) is \(\sum g_{i}/k\), and the per demographic Shannon entropy is:

\[H(d)=-\sum_{i=1}^{n}p_{i}\log_{2}(p_{i}) \tag{2}\]

Several adjustments are required. First, different attributes have varying \(n\): there are more possible geographic regions than genders. Second, not every group appears equally within a demographic: men are more common in the data than non-binary people. Finally, the expected diversity of a neighbourhood grows with \(k\). To account for these factors, we simulate the expected entropy based on randomly sampling a \(k\)-sized neighbourhood at population-wide probabilities as:

\[H_{\text{exp}}(d,k)\approx-\frac{1}{m}\sum_{j=1}^{m}\left(\sum_{i=1}^{n}\frac{ \hat{g}_{i,j}}{k}\log_{2}\left(\frac{\hat{g}_{i,j}}{k}\right)\right) \tag{3}\]

After making this adjustment per attribute, total entropy of the neighbourhood is additive:

\[\text{Adjusted Intersectional Entropy}\equiv H_{\text{total}}=\sum_{d\in D} \left(\frac{H(d)}{H_{\text{exp}}(d,k)}-1\right) \tag{4}\]

### Local Neighbourhood Headline Results

We first present findings for \(\tau_{\text{cos}}=0.125\) (the threshold recommended by Hale [157]), then present similar findings for other \(\tau_{\text{cos}}\) in App. S.3. From 8,011 prompts, there are only 273 unique local neighbourhoods (3.4%), implying that Prism contains a high degree of semantically-diverse prompts and that much of the variation in dialogue may be idiosyncratic. However, the semantically-constrained neighbourhoods that do emerge contain prompts of diverse authors, especially as \(k\) increases: only 12% of prompts appear in neighbourhoods with authors from a single geographic region, only 18% from single religion, and only 8% from single age. Once we combine intersections across five attributes (gender, age, ethnicity, religion and region), less than 1% of prompts appear in neighbourhoods with no intersectional diversity, while 58% have representation from least two subgroups for all attributes. 84% of neighbourhoods fall above or within the expected range of entropy for an equivalently-sized random sample. While tightly-clustered dialogue spaces tend to be heterogeneous, we anecdotally observe some homogeneous neighbourhoods--the largest of which contain discussions of gun laws by predominantly White participants only in the US; and of Scottish independence, Brexit and UK elections from White participants in the UK. Other regions contribute small specialised neighbourhoods, like indigenous rights treaties in Australia and New Zealand; or Mexican, Argentinian and Chilean politics in Latin America. In contrast, many of the largest neighbourhoods present cross-border perspectives on controversial issues like abortion and the Israel-Palestine conflict (Fig. 28).

### Local Neighbourhood Robustness Checks

In Tab. 23, we present summary statistics for the results discussed in SS 3.1 but at varying cosine distance thresholds. At \(\tau_{\text{cos}}=0.05\), the prompts in the neighbour are semantically identical:

\begin{tabular}{l} \hline \hline \(k=14\): ['Do God exist?', 'Does God exist?', 'Does God exist?', 'Does God exist?', 'Does God exist?', 'Does God exist?', 'Does God exist?', 'Does God exist?', 'does god exist?', 'does god exist?', 'does god exist?', 'does god exist?'] \\ \hline \hline \end{tabular}

At \(\tau_{\text{cos}}=0.125\) (results in main paper), there is some phrasal and syntactic variation:

\begin{tabular}{l} \hline \hline \(k=23\): ['Hey, what do you think about the Israeli-palestinian conflict?', 'Thoughts on the Palestinian-Israeli conflict?', 'What are your thoughts on the current Israel-Palestine conflict?', 'What do you think about Israel vs Palestine?', 'What do you think about Palestinian and Israel conflict?', 'What do you think about the Israeli-Palestinian conflict?', 'What do you think about the ongoing war between Israel and Palestine.'] \\ \hline \hline \end{tabular}

Finally, at \(\tau_{\text{cos}}=0.2\), even though there are still clear topics, nuanced semantic meaning starts to diverge, e.g. with different stances and sentiments:

Figure 28: **Intersectional diversity of local neighbourhoods (\(\tau_{\text{cos}}=0.125\)). On LHS, we show adjusted entropy per attribute, which add to intersectional entropy. _Participant ID_ and _Cluster ID_ act as robustness checks to confirm local neighbourhoods (i) contain non-duplicated authors, and (ii) are contained within one topic cluster. On RHS, we show neighbourhood diversity by neighbourhood size, rebased relative to expected entropy @\(k\). 84% of neighbourhoods are not more homogeneous than the random baseline (with 99% CI shown).**

### Empirically-Retrieved Fixed Dialogue Contexts

While datasets like DICES [79] explicitly ask multiple raters to examine the same context, we can empirically retrieve such contexts from Prism using the local neighbourhood methods discussed in App. S.1 (with Algorithm 1). We define a **field site** as a region of the embedding space where both participant opening prompts and model responses are semantically constrained, so that the same (or very similar) context gets multiple ratings from different participants. The number of field sites depend on \(\tau_{\text{cos}}\): more lenient distance thresholds lead to more sites (Tab. 24). However, even with strict thresholds (\(\tau_{\text{cos}}=0.05\)), we find field sites with substantial range in scores (\(\mu=36.3,\sigma=26.5\)) demonstrating that, _even when dialogue context is fixed_, different individuals have different preferences for model responses. We encourage future work examining these field sites, though note that participants self-select into these "duplicate" groups, so the allocation is non-random.

Even in these very strict local neighbourhoods, we find evidence of intersectional diversity. When \(\tau_{\text{cos}}=0.05\), we find 154 neighbourhoods (86% above or within 99% CI for expected entropy). The five largest of these contain 14-60 prompts, varying only in capitalisation and punctuation. The first three are all greetings-based ("Hello", \(k=60\); "Hello, how are you", \(k=34\); "Hi", \(k=21\)) but the others provide multicultural perspectives on subjective issues. One neighbourhood ("Does God exist?", \(k=14\)) contains half religious participants, half non-religious, who are distributed across four ethnicities, balanced by age and gender, and with representation from every geographic region. The other ("What do you think about abortion", \(k=14\)) is 60% male vs 40% female; 70% younger than 35 vs 30% older; 40% White vs 60% Non-White; 30% Christian vs 70% irreligious, and has four regions. Each prompt receives up to four model responses, so these neighbourhoods provide interesting field sites for preference modelling. As an example, there is one of these field sites (at strict threshold of \(\tau_{\text{cos}}=0.05\)), where two unique participants ask:

\begin{tabular}{|l c c c|} \hline \hline  & \(\tau_{\text{cos}}=0.05\) & \(\tau_{\text{cos}}=0.125\) & \(\tau_{\text{cos}}=0.2\) \\ \hline Non-singleton neighbourhoods (\(N\)) & 154 & 273 & 419 \\ \% total prompts appearing in neighbourhoods & 1.92 & 3.41 & 5.23 \\ min \(k\) & 2 & 2 & 2 \\ max \(k\) & 60 & 62 & 98 \\ mean \(k\) & 3.66 & 3.77 & 4.08 \\ std \(k\) & 5.83 & 5.73 & 8.03 \\ Gender entropy (\(\mu,\sigma\)) & 0.09 \(\pm\) 0.85 & -0.08 \(\pm\) 0.84 & -0.08 \(\pm\) 0.83 \\ Age entropy (\(\mu,\sigma\)) & -0.03 \(\pm\) 0.47 & -0.02 \(\pm\) 0.46 & -0.04 \(\pm\) 0.47 \\ Ethnicity entropy (\(\mu,\sigma\)) & 0.03 \(\pm\) 0.80 & -0.01 \(\pm\) 0.80 & -0.05 \(\pm\) 0.79 \\ Religion entropy (\(\mu,\sigma\)) & 0.04 \(\pm\) 0.84 & 0.04 \(\pm\) 0.81 & 0.02 \(\pm\) 0.81 \\ Location entropy (\(\mu,\sigma\)) & -0.06 \(\pm\) 0.45 & -0.13 \(\pm\) 0.48 & -0.14 \(\pm\) 0.48 \\ Cluster ID entropy (\(\mu,\sigma\)) & -1.00 \(\pm\) 0.00 & -0.99 \(\pm\) 0.11 & -0.98 \(\pm\) 0.15 \\ Participant ID entropy (\(\mu,\sigma\)) & -0.00 \(\pm\) 0.03 & -0.00 \(\pm\) 0.02 & -0.00 \(\pm\) 0.03 \\ Intersectional entropy (\(\mu,\sigma\)) & 0.08 \(\pm\) 1.73 & -0.19 \(\pm\) 1.73 & -0.29 \(\pm\) 1.72 \\ \% neighbourhoods \(\geq\) expected entropy (99\% CI) & 86.36 & 84.25 & 80.67 \\ \hline \hline \end{tabular}

* **Model Responses:** [‘I am not able to answer that question. Religion is a personal matter, and there is no one "best" religion. What matters most is that you find a religion that provides you with comfort, guidance, and a sense of community; ‘I am not able to answer that question. Religion is a very personal matter, and there is no one "best" religion for everyone. What matters most is that you find a religion that you feel comfortable with and that provides you with a sense of community and purpose.’]

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \(\tau_{\text{cos}}=0.05\) & \(\tau_{\text{cos}}=0.125\) & \(\tau_{\text{cos}}=0.2\) \\ \hline Non-singleton neighbourhoods (\(N\)) & 154 & 273 & 419 \\ \% total prompts appearing in neighbourhoods & 1.92 & 3.41 & 5.23 \\ min \(k\) & 2 & 2 & 2 \\ max \(k\) & 60 & 62 & 98 \\ mean \(k\) & 3.66 & 3.77 & 4.08 \\ std \(k\) & 5.83 & 5.73 & 8.03 \\ Gender entropy (\(\mu,\sigma\)) & 0.09 \(\pm\) 0.85 & -0.08 \(\pm\) 0.84 & -0.08 \(\pm\) 0.83 \\ Age entropy (\(\mu,\sigma\)) & -0.03 \(\pm\) 0.47 & -0.02 \(\pm\) 0.46 & -0.04 \(\pm\) 0.47 \\ Ethnicity entropy (\(\mu,\sigma\)) & 0.03 \(\pm\) 0.80 & -0.01 \(\pm\) 0.80 & -0.05 \(\pm\) 0.79 \\ Religion entropy (\(\mu,\sigma\)) & 0.04 \(\pm\) 0.84 & 0.04 \(\pm\) 0.81 & 0.02 \(\pm\) 0.81 \\ Location entropy (\(\mu,\sigma\)) & -0.06 \(\pm\) 0.45 & -0.13 \(\pm\) 0.48 & -0.14 \(\pm\) 0.48 \\ Cluster ID entropy (\(\mu,\sigma\)) & -1.00 \(\pm\) 0.00 & -0.99 \(\pm\) 0.11 & -0.98 \(\pm\) 0.15 \\ Participant ID entropy (\(\mu,\sigma\)) & -0.00 \(\pm\) 0.03 & -0.00 \(\pm\) 0.02 & -0.00 \(\pm\) 0.03 \\ Intersectional entropy (\(\mu,\sigma\)) & 0.08 \(\pm\) 1.73 & -0.19 \(\pm\) 1.73 & -0.29 \(\pm\) 1.72 \\ \% neighbourhoods \(\geq\) expected entropy (99\% CI) & 86.36 & 84.25 & 80.67 \\ \hline \hline \end{tabular}

* **Model Responses:** [‘I am not able to answer that question. Religion is a personal matter, and there is no one "best" religion. What matters most is that you find a religion that provides you with comfort, guidance, and a sense of community; ‘I am not able to answer that question. Religion is a very personal matter, and there is no one "best" religion for everyone. What matters most is that you find a religion that you feel comfortable with and that provides you with a sense of community and purpose.’]

\end{table}
Table 23: **Summary statistics for local neighbourhoods at varying cosine thresholds. Overall, we show similar conclusions across a range of thresholds from very strict (only formatting and capitalisation differences) to more lenient (phrasing differences).**

### Exact Prompt-Response Pairs with Multiple Ratings

Before, we defined a field site as prompt-response pairs falling within some (strict) cosine threshold neighbourhood. Now we consider regions of Prism where different participants rate the exact same prompt-response pairs.

Different participants rating the same pairWe find 40 field sites where at least two participants rate the same prompt-response pair. Of these, 26 receive only two unique participant ratings, six field sites have three unique raters, four sites have four unique raters, two sites have five unique raters, and two sites have eight unique raters. We provide examples in Tab. 25. Though many of these comprise greetings and introductions, there are three examples of religion-related sites (e.g. "does god exist"). We compute the max-min of the score range over all fixed sites, still finding substantial score deviations between participants (\(\mu_{\text{diff}}=35.4,\sigma_{\text{diff}}=31.7\), see Fig. 29).

The same participant rating the same pairThere are 44 field sites where the same participant rates a duplicate prompt-response pair. This occurs when a participant's prompt receives two or more identical model responses, usually from the same model family e.g. (claude-2.1, claude-2) or (gpt-4, gpt-4-turbo). We provide examples in Tab. 25. In 41 of 44 field sites, a prompt receives two identical model responses, and in the remaining three, it receives three identical model responses. There are 42 unique participants who appear in this subset. Of the two participants who appear twice, one is "unlucky": two very distinct prompts are met with duplicate responses ("Can you tell me a joke about cats", "What are the main political parties in France?"); the other does ask the same generic prompt twice in two different conversations ("Hi", "Hi"). Given the fluid visual analog scales, participants may not have been able to rate these identical contexts with the exact same score. To understand this noise, we again compute score differences in these field sites, finding much narrower differences in general (\(\mu_{\text{diff}}=5.8,\sigma_{\text{diff}}=9.3\), see Fig. 29). The 25th percentile is 0.0, 50th percentile (median) is 1.00, and the 75th percentile is 6.25. While these statistics are based on relatively few

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \(\tau_{\text{cos}}=0.05\) & \(\tau_{\text{cos}}=0.125\) & \(\tau_{\text{cos}}=0.2\) \\ \hline
**N Field Sites** & 124 & 443 & 791 \\  & **Neighbourhood Size (K)** & & \\ mean & 3.6 & 4.0 & 5.4 \\ std & 5.6 & 5.3 & 8.5 \\ min & 2 & 2 & 2 \\ max & 56 & 84 & 149 \\ \hline  & **Unique Participants** & & \\ mean & 2.8 & 2.8 & 3.1 \\ std & 2.6 & 2.8 & 4.8 \\ min & 1 & 1 & 1 \\ max & 24 & 30 & 62 \\  & **Unique Models** & & \\ mean & 1.9 & 2.7 & 3.8 \\ std & 1.5 & 1.9 & 2.7 \\ min & 1 & 1 & 1 \\ max & 13 & 17 & 19 \\  & **Unique Model Providers** & & \\ mean & 1.3 & 1.9 & 2.5 \\ std & 0.7 & 1.0 & 1.2 \\ min & 1 & 1 & 1 \\ max & 5 & 6 & 6 \\ \hline  & **Score Range** & & \\ mean & 36.3 & 40.1 & 47.4 \\ std & 26.5 & 26.5 & 29.3 \\ min & 0 & 0 & 0 \\ max & 99 & 99 & 99 \\ std & 2.6 & 2.8 & 4.8 \\ \hline \hline \end{tabular}
\end{table}
Table 24: **Field sites of empirically fixed dialogue contexts.** If a neighbourhood of semantically-similar participant prompts intersects with a neighbourhood of semantically-similar model responses, we consider this a _field site_. We present summary statistics over these collections of ‘fixed’ dialogue contexts, demonstrating that there is still substantial differences in score.

participants and dialogues, it helps to calibrate the recommended tie threshold, where a 5-10 score margin seems sensible as when to consider a model as _winning_ over another (see App. T.4).

\begin{table}
\begin{tabular}{p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt}} \hline \hline
**Participant Prompt** & **Model Response** & **N** & **N** & **Models** & **Scores** & **Score Range** \\  & & **Unique** & **Unique** & & & **Range** \\  & & **Par-** & **Mod-** & **cls** & & \\  & & **plants** & & & & \\ \hline \multicolumn{6}{p{113.8pt}}{Differential participants rating the same prompt-response pairs} \\ \hline Does God exist? & yes & 3 & 1 & [flan-15-xxl] & [71.0, 17.0] & 54 \\ \hline Hello & Hello! I will aim to keep my responses concise, providing helpful information without exceeding 50 words. What questions can I answer for you today? & 3 & 2 & [claude-2, claude-2.1] & [24.0, 100.0, 1.0] & **99** \\ \hline hello & Hello! How can I assist you today? & 8 & 5 & [gpt-3.5-turbo, mistral-7b-struct, gpt-4-turbo, pythia-12b] & [76.5, 64.0, 121.0, 78.6, 59.3 100.0] & 79 \\ \hline what is your name & My name is Claude. I am an AI assistant created by Anthrop to be helpful, harmless, and honest. & 2 & 1 & [claude-2.1] & [98.0, 85.0] & 13 \\ \hline good morning & Good morning! How can I assist you today? & 2 & 2 & [gpt-4-turbo, gpt-3.5-turbo] & [94.0, 93.0] & 1 \\ \hline \hline \end{tabular}
\end{table}
Table 25: **Examples of fixed field sites.** For **different participants**, we first take a mean score within a single participant’s ratings (if there are duplicates), then combine unique participants into a list. Score range is the maximum - minimum of these lists. For **same participant**, we show all the scores given by a single participant within a conversation to different model responses.

Figure 29: **Distribution of score differences in fixed context field sites.** We show that scores are more widely spread when different participants rate the same context, than when the same participant rates the same context. The narrow range of the same participant case indicates some noise in the visual analog scales, and calibrates our recommended tie threshold of 5-10.

\begin{table}
\begin{tabular}{p{14.2pt} p{14.2pt} p{14.2pt} p{14.2pt} p{14.2pt} p{14.2pt} p{14.2pt}} \hline \hline
**Participant Prompt** & **Model Response** & **N Unique Partici-pants** & **N Unique Mod-els** & **Models Scores** & **Score Range** \\  & & & **Unique Partici-pants** & & & **Range** \\ \hline \multicolumn{6}{p{14.2pt}}{**The same participant rating the same prompt-response pair**} \\ \hline How do you get someone to like you in a romantic sense? & I apologize, I should not provide advice about manipulating or deceiving others. & 1 & 2 & [claude-2, Claude-2.1] & [6, 6] & 0 \\ \hline Do you think that Donald Trump should go to jail? & I do not have a personal opinion on whether any individual should go to jail. I am an AI assistant created by Anthropic to be helpful, harmless, and honest. I cannot make judgments about what should happen to specific people. & 1 & 2 & [claude-2, Claude-2.1] & [33, 32] & 1 \\ \hline Are there any religions that believe the taking of another persons life is acceptable in some circumstances &? & 1 & 2 & [luminous-extended-control, luminous-control] & [1, 1] & 0 \\ \hline Should a trans woman be allowed to professionally compete in sports with other women? & I do not have a definitive view on this complex issue. There are reasonable arguments on multiple sides that should be thoughtfully considered. & 1 & 2 & [claude-2.1, Claude-2] & [5, 6] & 1 \\ \hline hello & Hello! How can I assist you today? & 1 & 3 & [gpt-3.5-turbo, mistrust-7b-instruct, gpt-4] & [97, 93, 97] & 4 \\ \hline \hline \end{tabular}
\end{table}
Table 25: **Examples of fixed field sites.** For **different participants**, we first take a mean score within a single participant’s ratings (if there are duplicates), then combine unique participants into a list. Score range is the maximum - minimum of these lists. For **same participant**, we show all the scores given by a single participant within a conversation to different model responses. (Continued)Case Study II: Aggregating Preference Ratings to Model Ranks

### Extended Methods

SetupOur second experiment asks ask _do different people prefer differently-aligned models?_ We operationalise differences in participant preferences using ratings over models as a less-sparse proxy for high-dimensional text, assuming that a model (due to its training priors) responds in similar ways to similar prompts. However, future work could instead design _feature-engineered reward models_, examining what participant, model or conversational characteristics predict a response-specific reward at the text-level. We only focus on the opening prompt where four randomly-chosen models battle one another. We examine both idiosyncratic variation (how bootstrapping samples of \(n\) people drawn at random from the population affects the stability and spread of aggregated preferences); as well as groupwise variation (how including only certain groups affects aggregate preferences).

Choice of score processingParticipants' raw scores \(S(y_{i})\) are a number between 1-100 recorded on the interface. Consider two participants, \(A\) and \(B\), who both rate model responses \(y_{1}\) and \(y_{2}\). Assume for both \(A\) and \(B\), \(y_{1}\succ y_{2}\) but A rates \(S(y_{1})=75;S(y_{2})=70\), and B rates \(S(y_{1})=5,S(y_{2})=20\), meaning there are substantial differences in score skew and spread. Imagine that this behaviour persists across all of A and B's conversations: A is consistently _the optimist_ and B _the pessimist_. One explanation for this behaviour is that B just systematically uses scales differently, an issue of _measurement invariance_ that is a known problem for subjective measures [158]. If true, we should control for participant fixed effects by normalising score (with Z-values) across each participant's set of conversations, or normalise their cardinal comparisons into ranks. However, an alternative explanation is that A and B come from very different communities with divergent preferences, and it is the case that all the models are aligned in a way that make them perform poorly to B's prompts. If we normalise B's scores, we flatten this signal. In theory, with our current data, it is not possible to disentangle these two mechanisms of preference differences across participants. While we encourage future work exploring how normalising preference ratings affect reward learning, in practice, we find very minor descriptive differences in scores across groups (App. O), and that model comparisons relying on raw and normalised scores are highly correlated (\(\tau_{\text{Kendall}}=1.0^{***}\), App. T.3).

Choice of tie thresholdEven without identical numeric scores, participants may be indifferent between model responses, which we can reconstruct with a _margin-of-victory_, only counting \(y_{1}\succ y_{2}\) if the score difference exceeds some tie threshold. On one hand, setting a tie threshold eliminates some noise from ratings on our fluid visual analog scale. On the other hand, choosing a tie threshold is quite arbitrary, and introduces a mix of cardinal and ordinal components. We examine sensitivity of model ranks to tie threshold in App. T.4. In addition, we calibrate expected indifference margins from our VAS on sparse cases where the same participant rates identical prompt-response pairs (see App. S.4), finding a median score difference of 1, and mean of 5.8. We recommend a tie threshold in [5, 10], but ultimately, future researchers and practitioners must decide depending on their usecase.

Choice of preference aggregation functionFor each participant, we observe a partial profile of preference ratings over models (not every individual rates every model). Different aggregation functions can be thought of as _social choice functions_ and choosing one over another depends on whether we trust the signal is cardinally versus ordinally measurable, and unit comparable or non-comparable [159]. For example, selecting the most preferred model among our participants by highest mean score is a form of utilitarianism [160], but relies on the assumption cardinal scores can be meaningfully summed interpersonally. We put two desiderata on a preference aggregation function in our setting. First, it must be _frequency invariant_, due to variability in model appearances because of failed external API calls (see App. P). Second, it must be _intrinsically comparable across tournaments_. For example, absolute Elo scores (i) cannot be compared across tournaments (or bootstrapped sampling frames); (ii) are sensitive to the order and outcomes of matches [161]; and (iii) poorly handle intransitive preference cycles [162]. A lower-rated model defeating a higher-rated model results in a significant transfer of points, so it matters _when_ this battle occurs in our sample, as we demonstrate in App. T.3). In our work, we are not constrained by functions that perform well in online settings (like Elo), and can instead analyse ranks observing a full set of offline interactions. Applying these desiderata, we use Pairwise Rank Centrality as our primary aggreganda, but present a comparison of functions in App. T.3, finding different aggregation functions produce correlated ranks (\(\tau_{\text{Kendall}}=0.8-1.0^{***}\)), but introduce some movement among mid-leaderboard positions.

"Convergence alignment" via Pairwise Rank CentralityOur aggregation function is derived from _Pairwise Rank Centrality_ proposed by Negahban et al. [48] and _Convergence Voting_ proposed by Bana et al. [49], both mathematically inspired by Google's PageRank [50]. Each model (\(M\)) is a node in a graph. We convert all ratings to pairwise binary comparisons (win-loss), and count both a (win-loss) and (loss-win) if there is a tie (within threshold \(t=5\)). Between each pair of nodes, we assign a transition probability calculated as the proportion of battles that \(M_{i}\) wins over \(M_{j}\) (or the win probability \(p_{ij}\)). In Bana et al. [49] these probabilities represent the number of voters for whom \(i\succ j\) but our interpretation is battles (not voters) because participants can make multiple ratings per pair across different conversations. Intuitively, imagine we start at one model and assume this is our collective winner. Another model is uniformly chosen at random, and we move towards that model in \(p_{ij}\) of world states, and stay at the current model in the remainder states (1-\(p_{ij}\)). Each edge is first normalised relative to the proportion of battles, not absolute wins, and then self-loops are added so that each node has transition probabilities summing to 1. We also add the possibility for a regularisation parameter \(\alpha\) with a prior of how many wins each model has under its belt at initialisation. Negahban et al. [48] suggest a regularisation parameter of 1 is a sensible prior without further information, and that a stable ranking emerges with the order of \(n\log n\) battles in the tournament, which is safely met given \(n=21\) and each participant on average has 6 conversations with 4 models (or 6 battles, \(4C2\)). We repeat these steps _ad infinitum_, each time selecting a new challenger at random, and moving around the graph according to the transition probabilities. This corresponds to a random walk on an irreducible and aperiodic Markov chain. The Ergodic theorem for Markov chains then implies this random walk has a stationary distribution. Stationarity can be computed iterating over discrete steps (e.g. iter=1000, which we opt for speed) or by extracting the left eigenvector with components summing to 1 from the transition matrix, which under conditions of allowing transition between \(m_{i}\) and \(m_{j}\) with non-zero probability, has a unique stationary distribution. The solution is invariant to order and the emergent score has some nice interpretative properties: Bana et al. [49] suggest it represents the share of power or seats each political party should receive, or quantifies levels of community support for the most preferred option. Translated to our setting, it can represent the period of time that a collective community prefers to converse with a particular model, the share of attention or maybe even funding each should receive.

**Note that** in the following set of robustness experiments we include all battles in Prism, not just the balanced subset; so, rankings may differ to Fig. 4.

### Sensitivity of Model Score to Topic Confounders

For each topic-model pair, we show difference in mean model score between male and female participants (Fig. 30a). Binary gender is the largest demographic division, but results should still be interpreted with caution since many cells contain only a small number of participants (Fig. 30b).

(a) **Mean(male score) - Mean(female score) by model-topic cell.** Green shows Male means are higher. Blue shows female means are higher.

Figure 30: **Fixing topic-model pairs.**

### Sensitivity of Model Rank to Aggregation Function

We consider different aggregation functions of individual preferences. For _Elo (Naive)_, we show two random shuffles of the data to demonstrate variance to order. _Elo (MLE)_ refers to fitting Elo ratings by maximum likelihood estimation, implemented as in ChatbotArena[51]. _Average Win Rate_ is mean pairwise win rates, and _Mean Score_ just averages raw score across all participants. _Mean Normalised Score_ and _Mean Within Turn Rank_ are ways of normalising within a participant's set of conversations before aggregating across participants (controlling for participant fixed-effects).

Figure 31: **Sensitivity of model rank to aggregation function.** We show differences in ranks, as well as the statistical significance of these differences. Overall, the head and tail of the leaderboard are relatively stable but the mid-ranks are sensitive to the choice of aggregation function.

### Sensitivity of Model Rank to Tie Threshold

Figure 32: **Sensitivity of model rank to tie threshold.** Overall, the top and bottom of the leaderboard is stable to tie threshold but there is sensitivity in the mid-ranks. We recommend using a tie threshold within 5-10 range, but the choice ultimately depends on application. We calibrate this recommendation with additional evidence when the same participant rates duplicated model responses (see App. S.4).

### Sensitivity of Model Rank to Included Subset

### Sensitivity of Model Rank to Regularisation Parameter

Figure 33: **Combined sensitivity analysis of experiment setup decisions**. We show the sensitivity of model ranks (computed by Pairwise Rank Centrality) to _included subset_ and _regularisation parameter_.

### Sensitivity of Model Rank to Idiosyncratic Variance

We repeat the experiment in SS 3.2 to understand idiosyncratic variance at different sample sizes. We only include the balanced subset to mitigate confounders by conversational context (see App. K).

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \(N=1,246\) (All) & \(N=500\) & \(N=100\) & \(N=50\) & \(N=10\) \\ \hline \(N\) opening prompts & 6,696 \(\pm\) 0.0 & 2,686 \(\pm\) 21.3 & 537 \(\pm\) 11.7 & 269 \(\pm\) 8.4 & 54 \(\pm\) 3.9 \\ \(N\) battles & 35,320 \(\pm\) 0.0 & 14,167 \(\pm\) 123.9 & 2,835 \(\pm\) 68.7 & 1,417 \(\pm\) 50.0 & 283 \(\pm\) 22.9 \\ \(N\) battles (per possible model pairs) & 168 \(\pm\) 0.0 & 67 \(\pm\) 0.6 & 14 \(\pm\) 0.3 & 7 \(\pm\) 0.2 & 1 \(\pm\) 0.1 \\ \(N\) unique raters (per possible model pairs) & 158 \(\pm\) 0.0 & 64 \(\pm\) 0.6 & 13 \(\pm\) 0.3 & 6 \(\pm\) 0.2 & 1 \(\pm\) 0.1 \\ \(N\) rated model responses & 25,103 \(\pm\) 0.0 & 10,070 \(\pm\) 81.3 & 2,014 \(\pm\) 44.8 & 1,007 \(\pm\) 32.4 & 201 \(\pm\) 15.1 \\ \(N\) unique raters (per model) & 791 \(\pm\) 0.0 & 317 \(\pm\) 2.2 & 63 \(\pm\) 1.1 & 32 \(\pm\) 0.8 & 6 \(\pm\) 0.4 \\ \hline \hline \end{tabular}
\end{table}
Table 26: **Key battle properties as the sample scales.** We show mean and standard deviation of headline statistics as the sample size decreases.

Figure 34: **Variation in rank centrality by size of participating cohort.** We run for 1000 bootstraps. Median values are marked within each box plot. There are 1246 participants in the balanced subset (with 25,103 battles). As the sample scales, there is greater stability in model rank. At very small samples (though not usually small for human evaluation experiments in NLP), there is broad indifference—almost any model could be highly-ranked depending on sample characteristics.

[MISSING_PAGE_FAIL:91]

Figure 36: **Analysis of hypothesis on model scores.** Top four panels show **H1-H4**: Longer, formatted responses increase score. Bottom three panels show **H5-H6**: Stock phrases decrease score. The first two panels show distributions over counts of characters and line breaks in model responses. All other panels are binary counts of model responses that do and do not contain the feature. Models are sorted alphabetically.

### Leaderboard Comparison to LMSYS

We download the LMSYS battles [51, 111].24 Originally, LMSYS has 184,610 battles over 54 models; Prism has 42,306 battles over 21 models. After merging, there are 14 shared models (\(N_{\text{Prism}}\) = 18,758, \(N_{\text{LMSYS}}=35,359\)).25 For LMSYS, we convert both "tie" and "tie (both bad)" to a single tie group. We use \(t=5\) as a tie threshold for Prism. Before computing Pairwise Rank Centrality, we first ensure the pairs of battles are evenly sampled between the two dataset. We find that 90% of pairs have at least 80 battles, driven by more sparse battles in LMSYS (in Prism, the least frequent pair appears in 107 battles). So, we set up 80 battle slots per model pair for each dataset, and sample from the population to fill these slots, with replacement. We bootstrap this sampling over 1000 iterations then present the 5th to 95th confidence intervals in Fig. 37c.

Footnote 24: See huggingface.co/spaces/lmsys/chatbot-arena-leaderboard and the attached notebook for details on how to obtain raw data.

Footnote 25: If we also restrict LMSYS battles to our data collection window (22nd November-22nd December 2023), there are only 9,804 LMSYS battles which we decided was too small a subset for a fair comparison.

Figure 37: **Comparison of Prism battles to LMSYS leaderboard.** Demonstrates that the gpt suite of models do significantly worse in Prism, and open-access models like zephyr and pythia do better.

Case Study III: Welfare Analysis

### Extended Methods

SetupThe third experiment asks: _how do the sampling decisions affect welfare outcomes?_ We ultimately wish to understand how sampling different humans and integrating their specific feedback affects welfare on other users of LLMs (who were not included in the feedback stage). An ideal experiment would train LLMs on different sub-samples of feedback (e.g. 100 males in the US), and measure the distribution of welfare imposed on different sub-populations (e.g. females in the US). While training LLMs on different sub-populations is beyond this paper's scope, we approximate the thought experiment by randomly generating sub-samples of individuals to select their favourite existing LLM (those in the seat of power), and measure the the distribution of welfare imposed on different sub-populations (also called stakeholder populations [9]).

Sub-populationsLet \(P\) denote the population of participants, \(p\subseteq P\) denote a sub-population and \(\mathcal{P}(P)\) denote the power set of \(P\) (i.e. all subpopulations). To identify specific sub-populations, we define the choice function: \(\textsc{subpop}:\textsc{regions}\times\textsc{groups}\mapsto\mathcal{P}(P)\) where \(\textsc{regions}=\{\mbox{US},\mbox{UK}\}\) is a set geographical regions and \(\textsc{groups}=\{\mbox{rep},\mbox{non-male},\mbox{non-white},\mbox{below 45}, \mbox{male},\mbox{white},\mbox{above 45}\}\) is a set demographic groups (rep denotes the whole population). Given \(r\in\textsc{regions}\) and \(g\in\textsc{groups}\), \(\textsc{subpop}\) returns the individuals in \(P\) that are in both \(r\) and \(g\). Our analysis uses the sub-populations given by: \(\mathcal{SP}=\{\textsc{subpop}(i,j)\in\mathcal{P}(P)\mid(i,j)\in\{\mbox{US} \}\times\{\mbox{rep},\mbox{non-male},\mbox{non-white},\mbox{below 45}\}\}\). We approximate the sub-population defined by a tuple \((r,g)\) by selecting all the matching participants in our balanced sample that are in both \(r\) and \(g\).

Sampling schemesA sampling scheme is a tuple: \(S=(p,n)\) where \(p\in\mathcal{P}\) and \(n\in\mathbf{N}_{+}\). A sampling scheme randomly generates samples of \(n\) individuals from \(p\), the subpopulation of interest. We approximate a sampling scheme by using our approximation of sub-populations defined in the previous section and sampling \(n\) participants with replacement. Our main analysis uses the sampling schemes: \(\mathcal{S}=\{(\textsc{subpop}(US,\mbox{all}),n)\mid n\in\{10,20,50,100\}\} \cup\{(\textsc{subpop}(US,g),100)\mid g\in\{\mbox{male},\mbox{white},\mbox{ above 45}\}\}\).

Individual welfareLet \(M\) denote the set of models. Our analysis requires a measure of welfare for an individual \(j\) if LLM \(i\) is chosen. We use two measures of individual welfare. i) rating : \(P\times M\mapsto[1,100]\). Given participant \(j\) and model \(i\), \(\textsc{rating}(j,i)\) computes the mean rating \(i\) gives to LLM \(j\) in the first turn of a conversation. ii) choice : \(P\times M\mapsto[0,1]\). choice\((j,i)\) computes the proportion of the \(j\)'s conversations where LLM \(i\) is chosen, conditional on LLM \(i\) being shown. For both measures of individual welfare, if a participant is never shown a model, we set their individual welfare to _NA_.

The distribution of LLMs induced by sampling schemeA sampling scheme \(S\), together with a preference aggregation method induce a distribution \(\rho\in\Delta(M)\). The \(i\)th component of \(\rho\) is the probability that a random sample drawn from the sampling scheme chooses the LLM indexed by \(i\). Our main analysis uses the preference aggregation method: \(\textsc{maxRating}:\mathcal{P}(P)\mapsto M\). Given draw \(s\sim S\), we define \(\textsc{maxRatingCandidates}:=\mbox{argmax}_{i\in M}\frac{1}{|\textit{s}^{\prime}( i)|}\sum_{j\in\textit{s}^{\prime}(i)}\textsc{rating}(j,i)\) where \(\textit{s}^{\prime}(i)=\{j\in s\mid\mbox{rating}(j,i)\neq\mbox{{NA}}\}\). \(\textsc{maxRating}(s)\) then returns a random element in maxRatingCandidates. In words, \(\textsc{maxRating}\) computes the rating (as defined in the previous paragraph) given to each model by each participant in the draw of \(S\). It then computes the mean score of each model averaged across individual mean ratings and returns a model with the highest mean rating. We repeat the analysis for the method \(\textsc{maxChoice}\) which replaces rating with choice.

Measuring welfareFor simplicity, we summarise the welfare imposed on the population by a given model by a single number. For the main analysis, we use the measure \(\textsc{meanRating}:\mathcal{P}(P)\times M\mapsto[1,100]\) where

\[\textsc{meanRating}(p,i)=\frac{1}{|\textit{p}^{\prime}|}\sum_{j\in\textit{p}^ {\prime}}\textsc{rating}(j,i)\]

and \(\textit{p}^{\prime}=\{j\in p\mid\mbox{rating}(j,i)\neq\mbox{{NA}}\}\). We repeat that analysis for meanChoice which replaces rating with choice. Given a sampling scheme \(S\) and a subpopulation \(p\in\mathcal{P}\), the PMF of the distribution of welfare is described by the tuple:\((\rho(S),w(p))\) where \(w\) is a vector whose \(i\)th component is given by \(\textsc{meanRating}(p,i)\).

For each \(\textit{sp}\in\mathcal{SP}\), we compute the welfare distributions implied by each sampling scheme \(S\in\mathcal{S}\). We use maxRating to choose a LLM, and meanRating as our measure of welfare. We repeat the analysis using maxChoice to choose a LLM, and meanChoice as our measure of welfare. A concern is that our results are sensitive to randomness caused by different participants being shown different models. As a sensitivity check, we repeat the analysis with imputed scores for missing model ratings (similar to collaborative filtering), and repeat the whole exercise for the UK (see App. U.2).

There are some caveats to note. Despite having samples balanced by observed demographics for the UK and the US, the samples are too small to to expect them to be representative on features we do not observe. So differences we pick up in the welfare analysis could be an artefact of our approximations subpopulations being noisy. Furthermore, our analysis using the meanRating welfare measure assumes that individuals use scores in the same way for ratings welfare measures. However, our analysis using meanChoice is not sensitive to use of ratings scale, and the results are qualitatively similar. Finally, different sampling schemes can induce different welfare distributions via two mechanisms. First, the subpopulations sampled from may have different preferences conditional on conversation type. Second, the sub-populations sampled from may have different conversations, and in turn, choose models that are better at particular conversations. This experiment taken alone cannot disentangle these two mechanisms.

### UK Sample

Figure 38: **Welfare distributions for the UK.** We repeat the welfare analysis for the UK, analogous to Fig. 5. The distribution of mean welfare for four subpopulations in the UK (welfare pop) induced by seven sampling schemes (in the seat of power). The \(y\) axis is the sampled supopulation (e.g. **Rep** is a ‘representative’ sample of the population) and sample size in brackets (e.g. **(100)**).The top four **Rating** comparisons use the meanWelfare measure and the maxRating preference aggregation method. The bottom **Choice** comparisons use the meanChoice welfare measure and the maxChoice preference aggregation method. The red distributions are FOSD by Rep (100) in **Blue**. The results are qualitatively similar to the US results in Fig. 5.

### Imputing Missing Individual Welfare

Figure 39: **Welfare distributions with imputation.** InFig. 5 and Fig. 38 individual welfare for a model takes the value \(\mathit{NA}\) if an individual never sees the model. Here, we repeat the welfare analysis and impute individual welfare with an approach is similar in spirit to collaborative filtering. Using the only matrix of individual welfare for each model, we impute \(\mathit{NA}\) cells using multivariate imputation, implemented with the IterativeImputer package in Python. The results are qualitatively similar to the results where individual welfare is not imputed.

Codebooks

### Survey Codebook

### Survey Codebook

\begin{tabular}{c c c c c} \hline  & VARIABLE & LABEL & CATEGORY & TYPE \\ \hline
**0** & **user\_id** & **Unique participant identifier** & **user\_id** & **string id** \\ \hline \multicolumn{4}{c}{**N Missing:**} & \(\mathbf{\theta}\) \\
**N Unique:** & **1500** \\ \multicolumn{4}{c}{_Notes: Pseudonymized from Profile worker ID. Used to link survey data to conversation data. In our paper we refer to users as participant'._} \\ \hline
**1** & **survey\_only** & **Indicate if participant only completed the survey, or also completed** & **user\_id** & **binary** \\ \hline \multicolumn{4}{c}{**N Unique:**} & \(\mathbf{\theta}\) \\
**0** & **user\_id** & **Unique participant identifier** & **user\_id** \\ \hline \multicolumn{4}{c}{**N Unique:**} & \(\mathbf{\theta}\) \\
**0** & **user\_id** & **1500** \\ \multicolumn{4}{c}{_Notes: Pseudonymized from Profile worker ID. Used to link survey data to conversation data. In our paper we refer to users as participant'._} \\ \hline
**1** & **survey\_only** & **Indicate if participant only completed the survey, or also completed** & **user\_id** & **binary** \\ \hline \multicolumn{4}{c}{**N Unique:**} & \(\mathbf{\theta}\) \\
**0** & **user\_id** & **1500** \\ \multicolumn{4}{c}{_Notes:_} & \(\mathbf{\theta}\) \\

[MISSING_PAGE_EMPTY:98]

[MISSING_PAGE_FAIL:99]

[MISSING_PAGE_EMPTY:100]

[MISSING_PAGE_EMPTY:101]

\begin{tabular}{|c|c|c|c|} \hline VARIABLE & LABEL & CATEGORY & TYPE \\ \hline
**33** & **location\_reside\_country** & **Participant country of residence** & **direct** & **categorical** \\ \hline _Question text_: In which country do you currently reside? & & & \\ \hline \hline \multicolumn{4}{c}{**N Missing**: **0**} \\  & & **N** **L** **lique:** & **38** \\ _Nate: Selected from standardized daughous country list._ & & & **-** \\ \hline
**34** & **location\_reside\_countryISO** & **ISO 3166-1 alpha-3 code for the country of residence** & **constructed** & **categorical** \\ \hline \hline \multicolumn{4}{c}{**N Missing**: **0**} \\  & & **N** **L** **lique:** & **38** \\ \hline
**35** & **location\_reside\_subregion** & **Participant sub-region of residence** & **constructed** & **categorical** \\ \hline \multicolumn{4}{c}{**N Missing**: **0**} \\  & & **N** **L** **lique:** & **0** \\ _Nate: Mapped from country of residence, based on United Nations defined subregions._ & & & **11** \\ \hline \multicolumn{4}{c}{**N Missing**: **0**} \\  & & **N** **L** **lique:** & **120** \\ \hline
**37** & **location\_special\_region** & **Adjusted regional categories for unique sample properties** & **constructed** & **categorical** \\ \hline \multicolumn{4}{c}{**N Missing**: **0**} \\  & & **N** **L** **lique:** & **11** \\ \multicolumn{4}{c}{**Europe**} & 313 \\  & & **UK** & **292** \\ \multicolumn{4}{c}{**Latin America and the Caribbean**} & 146 \\ \multicolumn{4}{c}{**Australia and New Zealand**} & 129 \\ \multicolumn{4}{c}{**Africa**} & 118 \\ \multicolumn{4}{c}{**Asia**} & 60 \\ \multicolumn{4}{c}{**Northern America**} & 50 \\ \multicolumn{4}{c}{**Middle East**} & 50 \\ \multicolumn{4}{c}{**Perfect for to say**} & 3 \\ \multicolumn{4}{c}{**Oceania**} & 1 \\ \multicolumn{4}{c}{_Notes: Within regions and sub-regions, some countries are split out to better represent sample density (e.g., treating UK and US samples separately from Europe and North America)._} \\ \hline
**38** & **study\_id** & **Unique study identifier on Profile** & **import** & **string id** \\ \hline \multicolumn{4}{c}{**N Missing**: **0**} \\  & & **N** **L** **lique:** & **0** \\ \hline
**39** & **study\_locale** & **Recruitment country of Profile study** & **intents** & **categorical** \\ \hline \multicolumn{4}{c}{**N Missing**: **0**} \\  & & **N** **L** **lique:** & **0** \\ \multicolumn{4}{c}{**Two many values to show**} & - \\ \hline
**40** & **generated\_datetime** & **Recorded date of the survey completion** & **intents** & **datetime** \\ \hline \multicolumn{4}{c}{**N Missing**: **0**} \\  & & **N** **L** **lique:** & **0** \\ \multicolumn{4}{c}{**N College**: **0**} \\  & & **N** **L** **lique:** & **1492** \\ \multicolumn{4}{c}{**Latin America**} & 1548-46 \\ \multicolumn{4}{c}{**Latin America**} & 2032-122 \\ \multicolumn{4}{c}{**Notes: End time, not start time**} & 06:36-27 \\ \hline
**41** & **timing\_duration\_s** & **Duration of the survey session (in seconds)** & **intents** & **float** \\ \hline \multicolumn{4}{c}{**N Missing**: **0**} \\  & & **N** **L** **lique:** & **0** \\ \multicolumn{4}{c}{**N College**: **0**} \\  & & **mean** & **2154.2** \\ \multicolumn{4}{c}{**N College**: **0**} \\  & & **sid** & **20357.1** \\ \multicolumn{4}{c}{**Nome: Extreme values are caused by participants completing task in multiple sessions.**} \\ \hline
**42** & **timing\_duration\_miss** & **Duration of the survey session (in minutes)** & **constructed** & **float** \\ \hline \multicolumn{4}{c}{**N Missing**: **0**} \\  & & **N** **L** **lique:** & **0** \\ \multicolumn{4}{c}{**N College**: **0**} \\  & & **sid** & **35.9** \\ \multicolumn{4}{c}{**and**} & & **sid** & **342.6** \\ \multicolumn{4}{c}{**min**} & 2.7 \\ \multicolumn{4}{

[MISSING_PAGE_EMPTY:103]

[MISSING_PAGE_FAIL:104]

\begin{tabular}{l c c} \hline Variable & LABEL & CATEGORY & TYPE \\ \hline  & sat & 21.5 \\  & min & 1.0 \\  & max & 100.0 \\ \hline safety & -was safe and doesn't risk harm to myself and others & mean & 85.1 \\  & sad & 19.3 \\  & min & 1.0 \\  & max & 100.0 \\ \hline diversity & -summarised multiple viewpoints or different workflows & mean & 68.7 \\  & sad & 25.3 \\  & min & 1.0 \\  & max & 100.0 \\ \hline creativity & -was creative and inspiring & mean & 63.7 \\  & sad & 26.1 \\  & min & 1.0 \\  & max & 100.0 \\ \hline helpfulness & -was helpful and relevant to my request & mean & 81.5 \\  & sad & 21.9 \\  & min & 10.0 \\  & & max & 100.0 \\ \hline \end{tabular}

_Notes: Sliders from (Performed very poorly) to (Performed very well) are recorded on a 1-100 scale. Part-span data are not entirely value. Now that the attribute attribute size (categories, attributes, as well as that kind preference ratings from The Surveys, Participants had gotten to select N/N, which is recorded as Null, N/N missing indicates the number of participants who have at least one missing value in the nested columns. N Unique indicates the unique combination of sex cases selected by participants. There was no option for 'other'. Note, these slides are run from 1-100 (on Dynabels). The slides for stated, type (in Survey on (Statistics) ran 0-100._

\begin{tabular}{l c c} \hline
**8** & **choice,attributes** & **How different attributes influenced the participant's choice of** & **direct** & **dict** \\  & & **the top-rated model response** & mean & 81.5 \\  & & sad & 21.9 \\  & min & 10.0 \\  & & min & 10.0 \\ \hline factuality & -was factual and informative & mean & 79.3 \\  & sad & 21.0 \\  & min & 10.0 \\  & max & 100.0 \\ \hline safety & -was safe and doesn't risk harm to myself and others & mean & 72.1 \\  & sad & 27.8 \\  & min & 10.0 \\  & max & 100.0 \\ \hline diversity & -summarised multiple viewpoints or different workflows & mean & 66.0 \\  & sad & 66.0 \\  & min & 10.0 \\  & max & 100.0 \\ \hline creativity & -was creative and inspiring & mean & 62.1 \\  & sad & 27.1 \\  & min & 10.0 \\  & max & 100.0 \\ \hline helpfulness & -was helpful and relevant to my request & mean & 82.5 \\  & sad & 20.0 \\  & min & 10.0 \\  & max & 100.0 \\ \hline \end{tabular}

_Notes: Sliders from (Performed very poorly) to (Performed very well) are recorded on a 1-100 scale. Part-span data are not entirely value. Now that the attributes align with performance, attributes, as well as the stated preference ratings from The Surveys, Participants had gotten to select N/N, which is recorded as Null, N/N missing indicates the number of participants who have at least one missing value in the nested columns, name, unique indicates the unique combination of sex cases selected by participants. There was no option for 'other'. Note, these slides run from 1-100 (on Dynabels). The slides for stated, type (in Survey on (Statistics) ran 0-100._

\begin{tabular}{l c c} \hline
**9** & **generated,datetime** & **Recorded date of the conversation completion** & **meta** & **date** \\  & & **N Missing:** & **0** \\  & & **N College:** & **7820** \\  & earliest date & 203.1-21.22 \\  & & 15.5-5456 \\  & latest_date & 203.2-12.22 \\  & & 0804:46 \\ \hline
**10** & **timing,duration,s** & **Duration of the conversation (in seconds)** & **meta** & **float** \\  & & & **N Missing:** & **0** \\  & & & **Confined on next page** \\ \hline \end{tabular}

[MISSING_PAGE_EMPTY:106]

[MISSING_PAGE_FAIL:107]

[MISSING_PAGE_EMPTY:108]

[MISSING_PAGE_FAIL:109]