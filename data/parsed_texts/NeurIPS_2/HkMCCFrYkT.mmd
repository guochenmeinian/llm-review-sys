# HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting

 Yuanhao Cai \({}^{1}\), Zihao Xiao \({}^{1}\), Yixun Liang \({}^{2}\), Minghan Qin \({}^{3}\),

**Yulun Zhang \({}^{4,}\)1, Xiaokang Yang \({}^{4}\), Yaoyao Liu \({}^{5}\), Alan Yuille \({}^{1}\) \({}^{1}\)** Johns Hopkins University, \({}^{2}\) HKUST, \({}^{3}\) Tsinghua University,

\({}^{4}\) Shanghai Jiao Tong University, \({}^{5}\) University of Illinois Urbana-Champaign

Footnote 1: Corresponding Author.

###### Abstract

High dynamic range (HDR) novel view synthesis (NVS) aims to create photorealistic images from novel viewpoints using HDR imaging techniques. The rendered HDR images capture a wider range of brightness levels containing more details of the scene than normal low dynamic range (LDR) images. Existing HDR NVS methods are mainly based on NeRF. They suffer from long training time and slow inference speed. In this paper, we propose a new framework, High Dynamic Range Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views and reconstruct LDR images with a user input exposure time. Specifically, we design a Dual Dynamic Range (DDR) Gaussian point cloud model that uses spherical harmonics to fit HDR color and employs an MLP-based tone-mapper to render LDR color. The HDR and LDR colors are then fed into two Parallel Differentiable Rasterization (PDR) processes to reconstruct HDR and LDR views. To establish the data foundation for the research of 3D Gaussian splatting-based methods in HDR NVS, we recalibrate the camera parameters and compute the initial positions for Gaussian point clouds. Comprehensive experiments show that HDR-GS surpasses the state-of-the-art NeRF-based method by 3.84 and 1.91 dB on LDR and HDR NVS while enjoying 1000\(\times\) inference speed and only costing 6.3% training time. Code and data are released at [https://github.com/caiyuanhao1998/HDR-GS](https://github.com/caiyuanhao1998/HDR-GS)

## 1 Introduction

Compared to normal low dynamic range (LDR) images, high dynamic range (HDR) images capture a broader range of luminance levels to retain the details in dark and bright regions, allowing for more accurate representation of real-world scenes. Novel view synthesis (NVS) aims to produce photo-realistic images of a scene at unobserved viewpoints, given a set of posed images of the same scene. NVS has been widely applied in autonomous driving [1, 2, 3, 4], image editing [5, 6, 7, 8], digital human [9, 10, 11, 12], _etc._ NVS is a challenging topic in computer vision because the limited capacity of camera sensor usually leads to a low dynamic range (from 0 to 255) of luminance in rendered images. This results in the loss of image details in very bright or dark areas, color distortions, and a limited capacity to display subtle gradations in light and shadow that the human eye can normally perceive. Hence, there is a growing demand to render HDR (from 0 to +\(\infty\)) views for better image quality and visualization performance.

Figure 1: HDR-GS _vs._ HDR-NeRF. Our HDR-GS achieves better PSNR in dB, SSIM, and LPIPS performance with shorter training time in minutes and faster inference speed in fps.

Existing HDR NVS methods are mainly based on neural radiance fields (NeRF) [13]. However, the ray tracing scheme in NeRF is very time-consuming because it needs to sample many 3D points and then compute their densities and colors for every single ray, severely slowing down the training and inference processes. For instance, the state-of-the-art NeRF-based method HDR-NeRF [14] takes 9 hours to train and 8.2 s to infer an image at the spatial size of 400\(\times\)400. This limitation impedes the application of NeRF-based algorithms in rendering real-time dynamic scenes.

Recently, 3D Gaussian Splitting (3DGS) [15] has achieved impressive inference speed than NeRF-based methods while yielding comparable results on LDR NVS, which inspires another technical route for HDR NVS. However, directly applying the original 3DGS to HDR imaging may encounter three issues. **Firstly**, the dynamic range of the rendered image is still limited to [0, 255], which severely degrades the visual quality. **Secondly**, training 3DGS with images under different exposures may lead to a non-convergence problem because the spherical harmonics (SH) of 3D Gaussians can not adaptively model the change of exposures. This results in artifacts, blur, and color distortion in the rendered images, as shown in the upper part of Fig. 2. **Thirdly**, 3DGS cannot adapt the exposure level of the synthesized views, which limits its applications, especially in AR/VR, film, and gaming where specific moods and atmospheres are usually evoked by controlling the lighting condition.

To cope with these problems, we propose a novel 3DGS-based method, namely High Dynamic Range Gaussian Splatting (HDR-GS), for 3D HDR imaging. More advanced than the original 3DGS, our HDR-GS can not only render HDR views but also reconstruct LDR images with a controllable exposure time, as depicted in the lower part of Fig. 2. Specifically, we present a Dual Dynamic Range (DDR) Gaussian point cloud model that can jointly model the HDR and LDR colors. We achieve this by using the SH of 3D point clouds to model the HDR color. Then three independent MLPs are employed to model the classical nonparametric camera response function (CRF) calibration process [16] in RGB channels, respectively. By this means, the HDR color of the 3D point is tone-mapped to the corresponding LDR color with the user input exposure time. Subsequently, the HDR and LDR colors are fed into two Parallel Differentiable Rasterization (PDR) processes to render the HDR and LDR images. In addition, we also notice that existing datasets only provide the camera poses in the normalized device coordinate (NDC) system, which is not suitable for 3DGS-based methods. To establish the data foundation for the research of 3DGS-based methods in HDR imaging, we recalibrate the camera parameters and compute SfM [17] points to initialize 3D Gaussians. With the proposed techniques, HDR-GS outperforms state-of-the-art (SOTA) NeRF-based methods by 1.91 dB on the HDR novel view synthesis task while enjoying 1000\(\times\) inference speed and only requiring 6.3% training time, as shown in Fig. 1. In a nutshell, our contributions can be summarized as:

Figure 2: Comparisons of point clouds (left) and rendered views (right) between the original 3DGS [15] (top) and our HDR-GS (bottom). (i) 3DGS [15] renders blurry LDR views when training with images under different exposures. Its point clouds suffer from severe color distortion and can not accurately represent the scene. In addition, 3DGS cannot control the exposure of the rendered images. (ii) Our HDR-GS can not only reconstruct clear HDR images with 3D consistency but also render LDR views with controllable exposure time \(\Delta t\).

**(i)** We propose a novel framework, HDR-GS, for 3D HDR imaging. To the best of our knowledge, this is the first attempt to explore the potential of Gaussian splatting in 3D HDR reconstruction.

**(ii)** We present a Dual Dynamic Range Gaussian point cloud model with two Parallel Differentiable Rasterization processes that can render HDR images and LDR views with controllable exposure time.

**(iii)** We establish a data foundation by recalibrating camera parameters and computing initial points for 3DGS-based methods on the multi-view HDR datasets [14]. Experiments show that our HDR-GS dramatically outperforms SOTA methods while enjoying much faster training and inference speed.

## 2 Related Work

**High Dynamic Range Imaging.** Conventional HDR imaging [18] techniques recover HDR images by directly fusing a series of LDR images under different exposure levels at a fixed pose [19] or calibrating the camera response function (CRF) from the LDR images [16; 20]. These traditional methods yield compelling results in static scenes but produce unpleasant ghost artifacts in dynamic scenes. To address this issue, later works [21; 22; 23; 24; 25] adopt an optical estimator to detect motion regions in the LDR images and then remove or align these regions in further fusion. With the development of deep learning, convolutional neural networks (CNNs) [26; 27; 28; 29] and Transformers [30; 31; 32; 33] have been used to learn an implicit mapping function from an LDR image to its HDR counterpart. Yet, these 2D HDR imaging methods lack 3D perception capabilities and are unable to render novel HDR views.

**Neural Radiance Field.** NeRF [13] learns an implicit mapping function from the position of a 3D point and view direction to the point color and volume density. NeRF achieves impressive performance on the NVS task, inspiring many follow-up works to improve its reconstruction quality [34; 35; 36; 37; 38] and inference speed [39; 40; 41; 42; 43; 44; 45] or expand its application area [14; 46; 47; 48; 49]. For example, Huang _et al._ present HDR-NeRF [14] that employs an MLP following the vanilla NeRF to learn an implicit mapping from physical radiance to digital color. Although good results are achieved, these NeRF-based methods suffer from slow training and inference speed due to their time-consuming ray-tracing scheme.

**Gaussian Splatting.** 3DGS [15] explicitly represents a scene by millions of Gaussian point clouds. Its parallelized rasterization in view rendering allows it to enjoy much faster inference speed than NeRF-based methods that suffer from the time-consuming ray-tracing scheme. Thus, 3DGS has been rapidly and widely applied in many areas such as dynamic scene rendering [50; 51; 52], SLAM [53; 54; 55; 56], inverse rendering [57; 58; 59], digital human [60; 61; 62], 3D generation [63; 64; 65], medical imaging [66], _etc._ However, the illuminance modeled by 3DGS is still limited to a low dynamic range. The potential of 3DGS in HDR imaging still remains under-explored. This work aims to fill this research gap.

## 3 Method

Figure 3 depicts the overall framework of our HDR-GS. To begin with, we use the structure-from-motion (SfM) [17] algorithm to recalibrate the camera parameters of the scene and initialize the Gaussian point clouds, as shown in Fig. 3 (a). Then we propose a Dual Dynamic Range (DDR) Gaussian point cloud model to jointly fit the HDR and LDR colors, as illustrated in Fig. 3 (b). The 3D Gaussians directly use the spherical harmonics (SH) to model the HDR color. Then three independent MLPs are employed to learn the tone-mapping operation in RGB channels. This tone-mapper renders the LDR color from the corresponding HDR color and a controllable exposure time \(\Delta t\). Subsequently, the LDR and HDR colors are fed into two Parallel Differentiable Rasterization (PDR) processes to render the HDR and LDR images, as depicted in Fig. 3 (c). In this section, we first introduce the DDR point cloud model, then PDR processes, and finally the initialization and optimization of HDR-GS.

### Dual Dynamic Range Gaussian Point Cloud Model

A scene can be represented by a set of our Dual Dynamic Range (DDR) Gaussian point clouds \(\mathcal{G}\) as

\[\mathcal{G}=\{G_{i}(\mathbf{\mu}_{i},\mathbf{\Sigma}_{i},\alpha_{i},\mathbf{c}_{i}^{l},\bm {c}_{i}^{h},\Delta t,\theta)\mid i=1,2,\ldots,N_{p}\}, \tag{1}\]

where \(N_{p}\) is the number of 3D Gaussians and \(G_{i}\) represents the \(i\)-th Gaussian. Its center position, covariance, opacity, LDR color, and HDR color are denoted as \(\mathbf{\mu}_{i}\in\mathbb{R}^{3}\), \(\mathbf{\Sigma}_{i}\in\mathbb{R}^{3\times 3}\), \(\alpha_{i}\in\mathbb{R}\)\(\mathbf{c}_{i}^{l}\in\mathbb{R}^{3}\), and \(\mathbf{c}_{i}^{h}\in\mathbb{R}^{3}\). Besides these attributes, each \(G_{i}\) also contains an exposure time \(\Delta t\in\mathbb{R}\) that controls the light intensity of the LDR view and three global-shared MLPs with parameters \(\theta\).

\(\mathbf{\Sigma}_{i}\) is represented by a rotation matrix \(\mathbf{R}_{i}\in\mathbb{R}^{3}\) and a scaling matrix \(\mathbf{S}_{i}\in\mathbb{R}^{3}\) as

\[\mathbf{\Sigma}_{i}=\mathbf{R}_{i}\mathbf{S}_{i}\mathbf{S}_{i}^{\top}\mathbf{R}_{i }^{\top}. \tag{2}\]

\(\mathbf{\mu}_{i}\), \(\mathbf{R}_{i}\), \(\mathbf{S}_{i}\), \(\alpha_{i}\), and \(\theta\) are learnable parameters. The tone-mapping operation \(f_{TM}(\cdot)\) models the camera response function (CRF) that non-linearly maps the HDR color \(\mathbf{c}_{i}^{h}\) into the LDR color \(\mathbf{c}_{i}^{l}\) as

\[\mathbf{c}_{i}^{l}=f_{TM}(\mathbf{c}_{i}^{h}\cdot\Delta t), \tag{3}\]

where the exposure time \(\Delta t\) can be read from the metadata of photos. We propose to employ MLPs to learn the tone-mapping process. There are two options. The first option is to directly model \(f_{TM}(\cdot)\), which may result in the vanishing gradient problem because the multiplication operations may cause numerical overflow or underflow. Besides, the multiplication also leads to the nonlinearity and discontinuity of the input signal of MLPs, which also exacerbates the training instability. The second option is following the traditional non-parametric CRF calibration method of Debevec and Malik (2016) that transforms \(f_{TM}(\cdot)\) from linear domain to logarithmic domain to enhance the stability of MLP training. We adopt the second option. Specifically, \(f_{TM}(\cdot)\) in Eq. (3) is inversed and transformed as

\[\text{log }f_{TM}^{-1}(\mathbf{c}_{i}^{l})=\text{log }\mathbf{c}_{i}^{h}+\text{log }\Delta t, \tag{4}\]

where \(\text{log}(\cdot)\) refers to the natural logarithm function and its base is \(e=2.71828\cdots\). Subsequently, we take the inverse function of Eq. (4) on both sides and reformulate it as

\[\mathbf{c}_{i}^{l}=(\text{log }f_{TM}^{-1})^{-1}(\text{log }\mathbf{c}_{i}^{h}+\text{log }\Delta t). \tag{5}\]

Then we use three MLPs \(\theta\) to model the function \((\text{log }f_{TM}^{-1})^{-1}\) in RGB channels independently because the RGB colors are tone-mapped by different CRFs. For simplicity, we define the mapping function of our tone-mapper \(\theta\) as \(g_{\theta}(x)\triangleq(\text{log }f_{TM}^{-1})^{-1}(x)\). Then Eq. (5) is reformulated as

\[\mathbf{c}_{i}^{l}=g_{\theta}(\text{log }\mathbf{c}_{i}^{h}+\text{log }\Delta t), \tag{6}\]

here \(\text{log }\mathbf{c}_{i}^{h}\) is modeled by the spherical harmonics (SH) with a set of coefficients \(\mathbf{k}=\{k_{l}^{m}|0\leq l\leq L,-l\leq m\leq l\}\in\mathbb{R}^{(L+1)^{2 }\times 3}\). Each \(k_{l}^{m}\in\mathbb{R}^{3}\) is a set of three coefficients corresponding to the RGB components. \(L\) is the degree of SH. Then \(\mathbf{c}_{i}^{h}\) at the view direction \(\mathbf{d}=(\theta,\phi)\) is derived by

\[\mathbf{c}_{i}^{h}(\mathbf{d},\mathbf{k})=\text{exp}(\sum_{l=0}^{L}\sum_{m=-l}^{l }k_{l}^{m}\ Y_{l}^{m}(\theta,\phi)), \tag{7}\]

Figure 3: Pipeline of our method. (a) SfM (2017) algorithm is used to recalibrate camera parameters and initialize 3D Gaussians. (b) Dual Dynamic Range Gaussian point clouds use spherical harmonics to model the HDR color. Three MLPs are employed to tone-map the LDR color from the HDR color and user input exposure time. (c) The HDR and LDR colors are fed into two Parallel Differentiable Rasterization to render the HDR and LDR views.

where \(Y_{l}^{m}:\mathbb{S}^{2}\rightarrow\mathbb{R}\) is the SH function that maps 3D points on the sphere to real numbers and \(\exp(\cdot)\) represents the the exponential function. By substituting Eq. (7) into Eq. (6), we obtain \(\mathbf{c}_{i}^{l}\) as

\[\mathbf{c}_{i}^{l}(\mathbf{d},\mathbf{k},\Delta t)=g_{\theta}(\sum_{l=0}^{L}\sum_{m =-l}^{l}k_{l}^{m}\:Y_{l}^{m}(\theta,\phi)+\text{log}\:\Delta t+b), \tag{8}\]

here we add a constant bias \(b\in\mathbb{R}\) that helps adjust the SH function to better fit the data. The detailed architecture of our MLP-based tone-mapper \(\theta\) is shown in Fig. 3 (b). \(g_{\theta}(\cdot)\) equals to the process that the RGB channels of log \(\mathbf{c}_{i}^{h}\) respectively undergo an independent MLP containing a fully connected (\(fc\)) layer, a ReLU activation, an \(fc\) layer, and a sigmoid activation to produce the LDR color \(\mathbf{c}_{i}^{l}\).

### Parallel Differentiable Rasterization

The computed HDR color \(\mathbf{c}_{i}^{h}\) in Eq. (7) and LDR color \(\mathbf{c}_{i}^{l}\) in Eq. (8) of Gaussian point clouds are fed into two parallel differentiable rasterization processes to jointly render the HDR and LDR views, as shown in Fig. 3 (c). The HDR rasterization \(F_{\text{HDR}}\) and LDR rasterization \(F_{\text{LDR}}\) are represented as

\[\begin{split}\mathbf{I}^{h}&=F_{\text{HDR}}(\mathbf{ M}_{int},\mathbf{M}_{ext},\{\mathbf{\mu}_{i},\mathbf{\Sigma}_{i},\alpha_{i},\mathbf{c}_{i}^ {h}\}_{i=1}^{N_{p}}),\\ \mathbf{I}^{l}(\Delta t)&=F_{\text{LDR}}(\mathbf{M}_{ int},\mathbf{M}_{ext},\{\mathbf{\mu}_{i},\mathbf{\Sigma}_{i},\alpha_{i},\mathbf{c}_{i}^{l}( \Delta t)\}_{i=1}^{N_{p}}),\end{split} \tag{9}\]

where \(\mathbf{I}^{h}\) and \(\mathbf{I}^{l}(\Delta t)\in\mathbb{R}^{H\times W\times 3}\) denote the rendered HDR image and LDR image with the exposure time \(\Delta t\), \(H\) and \(W\) refers to the height and width of the images, \(\mathbf{M}_{ext}\in\mathbb{R}^{4\times 4}\) represents the extrinsic matrix, and \(\mathbf{M}_{int}\in\mathbb{R}^{3\times 4}\) refers to the intrinsic matrix. Please note that we omit \(\mathbf{d}\) and \(\mathbf{k}\) in \(\mathbf{c}_{i}^{h}\) and \(\mathbf{c}_{i}^{l}\) for simplicity. Then we introduce the details of the parallel rasterization processes. First of all, we derive the possibility value of the \(i\)-th 3D Gaussian distribution at the point position \(\mathbf{x}\in\mathbb{R}^{3}\) as

\[P(\mathbf{x}|\mathbf{\mu}_{i},\mathbf{\Sigma}_{i})=\exp\big{(}-\frac{1}{2}(\mathbf{x} -\mathbf{\mu}_{i})^{\top}\mathbf{\Sigma}_{i}^{-1}(\mathbf{x}-\mathbf{\mu}_{i})\big{)}. \tag{10}\]

Subsequently, the splatting operation projects the 3D Gaussians to the 2D imaging plane. In this projection process, the center position \(\mathbf{\mu}_{i}\) is firstly transferred from the world coordinate system to the camera coordinate system and then projected to the image coordinate system as

\[\widetilde{\mathbf{v}}_{i}=[\mathbf{v}_{i},1]^{\top}=\mathbf{M}_{ext}\: \widetilde{\mathbf{\mu}}_{i}=\mathbf{M}_{ext}\:[\mathbf{\mu}_{i},1]^{\top},\qquad \widetilde{\mathbf{u}}_{i}=[\mathbf{u}_{i},1]^{\top}=\mathbf{M}_{int}\: \widetilde{\mathbf{v}}_{i}=\mathbf{M}_{int}\:[\mathbf{v}_{i},1]^{\top}, \tag{11}\]

where \(\mathbf{u}_{i}\in\mathbb{R}^{2}\) and \(\mathbf{v}_{i}\in\mathbb{R}^{3}\) refer to the image coordinate and camera coordinate of \(\mathbf{\mu}_{i}\). \(\widetilde{\mathbf{u}}_{i}\), \(\widetilde{\mathbf{v}}_{i}\), and \(\mathbf{\widetilde{\mu}}_{i}\) are the homogeneous versions of \(\mathbf{u}_{i}\), \(\mathbf{v}_{i}\), and \(\mathbf{\mu}_{i}\), respectively. The 3D covariance \(\mathbf{\Sigma}_{i}\) is also transferred from the world coordinate system to \(\mathbf{\Sigma}_{i}^{{}^{\prime}}\in\mathbb{R}^{3\times 3}\) in the camera coordinate system as

\[\mathbf{\Sigma}_{i}^{{}^{\prime}}=\mathbf{J}_{i}\mathbf{W}_{i}\mathbf{\Sigma}_{i}\mathbf{ W}_{i}^{\top}\mathbf{J}_{i}^{\top}, \tag{12}\]

where \(\mathbf{J}_{i}\in\mathbb{R}^{3\times 3}\) represents the Jacobian matrix of the affine approximation of the projective transformation \(\mathbf{M}_{int}\mathbf{M}_{ext}\). \(\mathbf{W}_{i}\in\mathbb{R}^{3\times 3}\) is the viewing transformation obtained by taking the first three rows and columns of \(\mathbf{M}_{ext}\). Similar to previous works [15; 66; 67; 68], the 2D covariance matrix \(\mathbf{\Sigma}_{i}^{{}^{\prime\prime}}\in\mathbb{R}^{2\times 2}\) is derived by directly skipping the third row and column of \(\mathbf{\Sigma}_{i}^{{}^{\prime}}\). Subsequently, the 2D projection is divided into non-overlapping tiles. The 3D Gaussians (\(\mathbf{\mu}_{i}\),\(\mathbf{\Sigma}_{i}\)) are assigned to the tiles where their 2D projections (\(\mathbf{u}_{i}\),\(\mathbf{\Sigma}_{i}^{{}^{\prime\prime}}\)) cover. For each tile, the assigned 3D Gaussians are sorted according to the view space depth. Then the RGB value \(\mathbf{I}^{h}(p)\) and \(\mathbf{I}^{l}(p|\Delta t)\in\mathbb{R}^{3}\) at pixel \(p\) is obtained by blending \(\mathcal{N}\) ordered points overlapping pixel \(p\) in the corresponding tile as

\[\mathbf{I}^{h}(p)=\sum_{j\in\mathcal{N}}\mathbf{c}_{j}^{h}\:\sigma_{j}\prod_{k=1}^{ j-1}(1-\sigma_{k}),\qquad\mathbf{I}^{l}(p|\Delta t)=\sum_{j\in\mathcal{N}}\mathbf{c}_{j}^ {l}(\Delta t)\:\sigma_{j}\prod_{k=1}^{j-1}(1-\sigma_{k}), \tag{13}\]

where \(\sigma_{j}=\alpha_{j}P(\mathbf{x}_{j}|\mathbf{\mu}_{j},\mathbf{\Sigma}_{j})\) and \(\mathbf{x}_{j}\) is the \(j\)-th intersection 3D point between the ray, which starts from the optical center of the camera and lands at pixel \(p\), and the Gaussian point clouds in 3D space. \(\mathbf{c}_{j}^{h}\) and \(\mathbf{c}_{j}^{l}(\Delta t)\) are the HDR color and LDR color with exposure time \(\Delta t\) of \(\mathbf{x}_{j}\), respectively.

### Initialization and Optimization

An obstacle to the research of 3DGS-based methods in 3D HDR imaging is that the original multi-view HDR datasets [14] only provide the camera poses in the normalized device coordinate (NDC) system. This NDC system is not suitable for 3DGS-based methods for two main reasons. **Firstly**, NDC focuses on describing the positions on the 2D screen after perspective projection. However, 3D Gaussian is an explicit 3D representation. 3DGS requires transforming and projecting Gaussian point clouds in 3D space. **Secondly**, NDC rescales the coordinates into the range [-1, 1] or [0, 1]. The voxel resolution is limited, making it challenging to capture fine details in the scene. **Besides**, the original datasets [14] do not provide SfM [17] point clouds for the initialization of 3DGS.

To address these issues and establish a data foundation for the research of 3DGS-based algorithms in 3D HDR imaging, we use the SfM algorithm [17] to recalibrate the camera parameters and compute the initial positions for 3D Gaussians. The mapping function of SfM \(F_{\text{SfM}}\) is summarized as

\[\mathbf{M}_{int},\ \{\mathbf{M}_{ext}^{j}\}_{j=1}^{N_{v}},\ N_{p},\ \{ \boldsymbol{\mu}_{i}\}_{i=1}^{N_{p}}\ =\ F_{\text{SfM}}(\{\hat{\mathbf{t}}_{j}^{i}(t_{s})\}_{j=1}^{N_{v}}), \tag{14}\]

where \(N_{v}\) represents the number of viewpoints and \(\hat{\mathbf{I}}_{j}^{i}(t_{s})\in\mathbb{R}^{H\times W\times 3}\) refers to the LDR image at the \(j\)-th viewpoint with the exposure time \(t_{s}\) in the multi-view HDR datasets [14]. The intrinsic matrix \(\mathbf{M}_{int}\) does not change with the viewpoint. Please note that we take the HDR views under the same exposure time as the inputs of SfM algorithms because SfM is based on multi-view feature detection and matching. Changes in exposure conditions may degrade the accuracy of SfM. Then we use the computed \(N_{p}\) and \(\{\boldsymbol{\mu}_{i}\}_{i=1}^{N_{p}}\) in Eq. (14) to initialize \(\mathcal{G}\) in Eq. (1). Other learnable parameters of \(\mathcal{G}\) are randomly initialized. The recalibrated camera pose-image data pairs \(\{\mathbf{M}_{ext}^{j},\hat{\mathbf{I}}_{j}^{i}(\Delta t)\}_{j=1}^{N_{v}}\) in Eq. (14) are used to train our HDR-GS with the weighted sum of \(\mathcal{L}_{1}\) loss and D-SSIM loss as

\[\mathcal{L}_{p}=\sum_{j=1}^{B}\mathcal{L}_{1}(\mathbf{I}_{j}^{i}(\Delta t_{j}),\hat{\mathbf{I}}_{j}^{i}(\Delta t_{j}))+\lambda\cdot\mathcal{L}_{\text{D-SSIM} }(\mathbf{I}_{j}^{l}(\Delta t_{j}),\hat{\mathbf{I}}_{j}^{l}(\Delta t_{j})), \tag{15}\]

where \(B\) is the training batch size and \(\lambda\) is a hyperparameter. Similar to HDR-NeRF [14] that uses the ground truth CRF correction coefficient \(C_{0}\) to restrict the HDR color on the synthetic scenes, we also enforce a constraint to the rendered HDR image in the \(\mu\)-law [14, 23, 69, 70] LDR domain as

\[\mathcal{L}_{c}=\sum_{j=1}^{B}\Big{|}\Big{|}\frac{\text{log}(1+\mu\cdot\text{ norm}(\mathbf{I}_{j}^{h}))}{\text{log}(1+\mu)}-\frac{\text{log}(1+\mu\cdot\text{ norm}(\hat{\mathbf{I}}_{j}^{h}))}{\text{log}(1+\mu)}\Big{|}\Big{|}_{2}^{2}\,, \tag{16}\]

where \(\mu\) is the amount of compression. \(\text{norm}(\cdot)\) is the min-max normalization. \(\mathbf{I}_{j}^{h}\) and \(\hat{\mathbf{I}}_{j}^{h}\in\mathbb{R}^{H\times W\times 3}\) denote the rendered and ground-truth HDR image at the \(j\)-th viewpoint. The overall training loss is

\[\mathcal{L}=\mathcal{L}_{p}+\gamma\cdot\mathcal{L}_{c}, \tag{17}\]

where \(\gamma\) is a hyperparameter that controls the relative importance between \(\mathcal{L}_{p}\) and \(\mathcal{L}_{c}\). We do not use \(\mathcal{L}_{c}\) in the real scenes since the ground truth HDR images are not provided in the real datasets. Therefore, we set \(\gamma=0.6\) and 0 in the experiments on the synthetic and real datasets, respectively.

## 4 Experiments

### Experimental Settings

**Dataset.** We adopt the multi-view image datasets collected by [14], including 4 real scenes captured by a camera and 8 synthetic scenes created by the software Blender [71]. Images with 5 different exposure time \(\{t_{1},t_{2},t_{3},t_{4},t_{5}\}\) are captured at 35 different viewpoints. Following HDR-NeRF [14], images at 18 views with exposure time randomly selected from \(\{t_{1},t_{3},t_{5}\}\) are used for training while other 17 views at exposure time \(\{t_{1},t_{3},t_{5}\}\) and \(\{t_{2},t_{4}\}\) and HDR images are used for testing.

**Implementation Details.** We implement HDR-GS by PyTorch [72]. The models are trained with the Adam optimizer [73] (\(\beta_{1}\) = 0.9, \(\beta_{2}\) = 0.999, and \(\epsilon\) = 1\(\times\)10\({}^{-15}\)) for 3\(\times\)10\({}^{4}\) iterations. The learning rate for point cloud position is initially set to 1.6\(\times\)10\({}^{-4}\) and exponentially decays to 1.6\(\times\)10\({}^{-6}\). The learning rates for point feature, opacity, scaling, and rotation are set to 2.5\(\times\)10\({}^{-3}\), 5\(\times\)10\({}^{-2}\), 5\(\times\)10\({}^{-3}\), and 1\(\times\)10\({}^{-3}\). The learning rate for the tone mapper network is initially set as 5\(\times\)10\({}^{-4}\) and exponentially decays to 5\(\times\)10\({}^{-5}\). All experiments are conducted on a single RTX A5000 GPU.

**Evaluation Metrics.** We adopt the peak signal-to-noise ratio, PSNR (higher is better), and structural similarity index measure, SSIM [74] (higher is better), to quantitatively evaluate the objective performance. Learned perceptual image patch similarity, LPIPS [75] (lower is better), is adopted as the perceptual metric. Similar to [14], we also quantitatively evaluate the rendered HDR images in the tone-mapped domain and qualitatively show HDR results tone-mapped by Photomatix pro [76]. In addition, frames per second, fps (higher is faster), is used to measure the model inference speed.

### Quantitative Results

**Comparisons on the Synthetic Datasets.** The quantitative results of LDR and HDR NVS on the synthetic datasets are reported in Table 1. We compare our HDR-GS with three NeRF-based methods (NeRF [13], NeRF-W [77], and HDR-NeRF [14]) and the original 3DGS [15]. Table 1 lists the training time, inference speed, PSNR, SSIM, and LPIPS results on LDR-OE, LDR-NE, and HDR. LDR-OE represents the LDR NVS results with exposure time \(t_{1},t_{3},\) and \(t_{5}\). LDR-NE denotes the LDR NVS results with exposure time \(t_{2}\) and \(t_{4}\). HDR refers to the HDR NVS results. Please note that only HDR-NeRF and HDR-GS can output both LDR and HDR views. Other methods can only render LDR images. Our method outperforms SOTA methods on all tracks except for the PSNR on LDR-NE. **(i)** When compared to the recent best method HDR-NeRF, our HDR-GS outperforms it by 2.03 and 1.91 dB on LDR-OE and HDR tracks while enjoying 1000\(\times\) faster inference speed and only costing 6.3% training time. **(ii)** When compared to the original 3DGS, our HDR-GS is 21.64 and 17.36 dB higher on LDR-OE and LDR-NE, respectively. Interestingly, HDR-GS is slightly faster

\begin{table}
\begin{tabular}{l|c c|c c c|c c c|c c c} \hline \multirow{2}{*}{Method} & Training & Inference & \multicolumn{3}{c|}{LDR-OE (\(t_{1},t_{3},t_{3}\))} & \multicolumn{3}{c|}{LDR-NE (\(t_{2},t_{4}\))} & \multicolumn{3}{c}{HDR} \\  & Time (min) & Speed (fps) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) \\ \hline NeRF [13] & 405 & 0.190 & 13.97 & 0.555 & 0.376 & 14.51 & 0.522 & 0.428 & — & — & — \\
3DGS [15] & 38 & 121 & 19.46 & 0.690 & 0.276 & 18.97 & 0.778 & 0.309 & — & — & — \\ NeRF-W [77] & 437 & 0.178 & 29.83 & 0.936 & 0.047 & 29.22 & 0.927 & 0.050 & — & — & — \\ HDR-NeRF [14] & 542 & 0.122 & 39.07 & 0.973 & 0.026 & **37.53** & 0.966 & 0.024 & 36.40 & 0.936 & 0.018 \\ \hline HDR-GS (Ours) & **34** & **126** & **41.10** & **0.982** & **0.011** & 36.33 & **0.977** & **0.016** & **38.31** & **0.972** & **0.013** \\ \hline \end{tabular}
\end{table}
Table 1: Quantitative results on the synthetic datasets. Metrics are averaged over all scenes. LDR-OE denotes the LDR results with exposure time \(t_{1}\), \(t_{3}\), and \(t_{5}\). LDR-NE denotes the LDR results with exposure time \(t_{2}\) and \(t_{4}\). HDR denotes the HDR results. HDR-GS yields the best results on all tracks.

Figure 4: LDR visual comparisons on the synthetic scenes. Previous methods introduce unpleasant black spots or render blurry images. Our method controls the exposure better while reconstructing more detailed structures.

than 3DGS. This is because 3DGS cannot adapt to different exposure levels. It is fragile and hard to converge when training with LDR images under different lighting intensities. The color change of the scene misleads the adaptive density control in 3DGS to split more Gaussian point clouds with distorted color to represent the complex variances in exposure levels, prolonging the training process.

To intuitively show the superiority of our method, Fig. 1 plots a radar chart that features concentric polygons representing the HDR NVS performance across 5 metrics of the SOTA method HDR-NeRF and our HDR-GS. It can be observed that our HDR-GS forms a much larger outermost polygon fully enclosing that of HDR-NeRF, indicating superior performance across all evaluated aspects. These results strongly demonstrate the advantages of our method in effectiveness and model efficiency.

**Comparisons on the Real Datasets.** Table 2 shows the quantitative comparisons on the real datasets. Please note that the real datasets do not provide HDR ground truth for quantitative evaluation. Hence, we only report the LDR NVS results in Table 2. When compared to the recent best method HDR-NeRF, our HDR-GS is 3.84 and 0.23 dB higher in PSNR on LDR-OE and LDR-NE. When compared to the original 3DGS, HDR-GS significantly surpasses it by 18.28 and 12.16 dB on LDR-OE and LDR-NE. These results suggest the outstanding generalization ability and effectiveness of our method.

### Qualitative Results

**LDR Novel View Rendering.** The comparisons of LDR novel view rendering with different exposure times on the synthetic (dog and sofa) and real (box and luckycat) scenes are shown in Fig. 4 and 5. It can be observed that NeRF, 3DGS, and NeRF-W fail to control the exposure. They either introduce black stripes and spots, or over-smooth the image, or over-enhance the image while distorting the color. HDR-NeRF can adapt the light intensity but it also produces blurry images. In contrast, our

\begin{table}
\begin{tabular}{l|c c c|c c c} \hline \multirow{2}{*}{Method} & \multicolumn{3}{c|}{LDR-OE (\(t_{1},t_{3},t_{5}\))} & \multicolumn{3}{c}{LDR-NE (\(t_{2},t_{4}\))} \\  & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) \\ \hline NeRF [13] & 14.95 & 0.661 & 0.308 & 14.44 & 0.731 & 0.255 \\
3DGS [15] & 17.19 & 0.806 & 0.103 & 19.50 & 0.727 & 0.152 \\ NeRF-W [77] & 28.55 & 0.927 & 0.094 & 28.64 & 0.923 & 0.089 \\ HDR-NeRF [14] & 31.63 & 0.948 & 0.069 & 31.43 & 0.943 & 0.069 \\ \hline HDR-GS (Ours) & **35.47** & **0.970** & **0.022** & **31.66** & **0.965** & **0.030** \\ \hline \end{tabular}
\end{table}
Table 2: Quantitative results on the real datasets. Metrics are averaged across all scenes. LDR-OE represents the LDR results with exposure time \(t_{1}\), \(t_{3}\), and \(t_{5}\). LDR-NE denotes the LDR results with exposure time \(t_{2}\) and \(t_{4}\). HDR refers to the HDR results. HDR-GS yields the best results on all tracks.

Figure 5: LDR visual comparisons on the real scenes. Previous methods introduce unpleasant black spots or render blurry images. Our method controls the exposure better while reconstructing more detailed structures.

HDR-GS can not only control the exposure level of LDR views according to the user input time but also reconstruct clearer images with high-frequency textures and structural contents.

**HDR Novel View Rendering.** The comparisons of HDR novel view synthesis on the synthetic (upper) and real (lower) datasets are depicted in Fig. 6. Please note that only HDR-NeRF and our HDR-GS can reconstruct HDR images. As can be seen, HDR-NeRF yields low-contrast and over-smooth images while sacrificing fine-grained details and introducing undesirable chromatic artifacts and black spots. On the contrary, our HDR-GS can render more perceptually pleasing HDR images with sharper textures and preserve the color and spatial smoothness of homogeneous regions.

### Ablation Study

In this section, we adopt the synthetic datasets to conduct ablation study. Table 3 lists the PSNR results averaged across all scenes on the LDR-OE, LDR-NE, and HDR tracks, respectively.

**Break-down Ablation.** We adopt 3DGS [15] trained with the original coordinates (NDC) as the baseline to conduct a break-down ablation on the synthetic datasets. Our goal is to study the effect of each component towards higher performance. The results are reported in Table 2(a). **(i)** The baseline model can only render LDR views. It achieves 12.35 and 11.83 dB on LDR-OE and LDR-NE. **(ii)** When using the recalibrated camera poses, the model yields an improvement of 2.27 and 2.58 dB on LDR-OE and LDR-NE because it is liberated from the constraint of the NDC system. **(iii)** When we apply the SfM points for the initialization of 3D Gaussians, the model gains by 4.84 dB and 4.56 dB

\begin{table}

\end{table}
Table 3: Ablations on the synthetic datasets. The PSNR results on HDR, LDR-OE, and LDR-NE are reported.

Figure 6: HDR visual comparisons on the synthetic (upper) and real (lower) scenes. Our method can recover the details in both dark and bright regions while suppressing color distortion. Please zoom in for a better view.

because the SfM points provide a general shape of Gaussian point clouds to alleviate the overfitting issues of 3DGS. However, the model still cannot render HDR views nor change the exposure level of the LDR views until now, leading to limited LDR NVS performance. **(iv)** Then we apply our DDR point clouds, the model is enabled to render HDR views with 38.31 dB in PSNR performance. Besides, the model yields 21.64 and 17.36 dB improvements on LDR-OE and LDR-NE because our DDR point clouds allow the model to adapt the lighting intensity with controllable exposure time.

**CRF Domain.** We conduct experiments to compare the effects of modeling CRF in linear domain and logarithmic domain. As shown in Table 3, when the MLPs \(\theta\) directly models \(f_{TM}(\cdot)\) in Eq. (3), our method yields poor results of only 26.18, 29.53, and 27.44 dB on HDR, LDR-OE, and LDR-NE. In contrast, when the MLPs \(\theta\) models \(g_{\theta}(\cdot)\) in Eq. (6), the performance is 12.13, 11.57, and 8.89 dB higher on HDR, LDR-OE, and LDR-NE. This is because the multiplication in \(f_{TM}(\cdot)\) is transferred into the addition in \(g_{\theta}(\cdot)\), which enhances the training stability by suppressing the numerical nonlinearity and discontinuity problems. This evidence verifies our analysis in Sec. 3.1.

**Exposure Time Used for Training.** We conduct experiments in Table 3c to study the effect of the number of exposure times used in training. **(i)** According to the research of Debevec and Malik [16], modeling CRF requires at least two exposures. Thus, when we only use a single exposure \(\{t_{3}\}\), HDR-GS fails to reconstruct HDR views and LDR images with novel exposure time. **(ii)** When two exposures \(\{t_{1},t_{5}\}\) are used for training, HDR-GS gains by 9.20, 11.62, and 10.53 dB on HDR, LDR-OE, and LDR-NE. **(iii)** The performance of using three exposures \(\{t_{1},t_{3},t_{5}\}\) is close to that of using five exposures \(\{t_{1},t_{2},t_{3},t_{4},t_{5}\}\). Hence, it is a reasonable choice to use three exposure times.

**Recalibration of Camera Parameters.** In Eq. (14), we use the SfM algorithm to recalibrate the camera parameters and compute the initial positions of 3D Gaussians at the same exposure time \(t_{s}\). Here, we conduct experiments to study the effect of \(t_{s}\) in Table 3d. The performance achieves its maximum value when \(t_{s}=t_{4}=8\) seconds. Therefore, the optimal choice of \(t_{s}\) is \(t_{4}=8\) s.

## 5 Limitation and Broader Impact

The main limitation of this work is that the memory usage of 3DGS-based methods is non-trivial and maybe unaffordable to some low-RAM mobile devices. HDR imaging is a very important topic in computational photography. Nowadays, billions of LDR images are captured by mobile phones and cameras. Therefore, how to enhance the quality of these images, adapt the exposure level, and render HDR views is worth studying. Our HDR-GS is capable of reconstructing better HDR and LDR views with controllable exposure time at 1000\(\times\) speed than SOTA methods, showing great value in practical applications. Until now, 3D HDR imaging techniques have no negative social impact yet. Our proposed HDR-GS does not present any negative foreseeable societal consequences, either.

## 6 Conclusion

This paper focuses on studying the efficiency problem of 3D HDR imaging. We propose the first Gaussian Splatting-based framework, HDR-GS, for HDR novel view synthesis. Our HDR-GS is based on the Dual Dynamic Range Gaussian point clouds that can jointly model HDR color and LDR color with user input exposure time. Then, the HDR and LDR colors are fed into two Parallel Differentiable Rasterization processes to render the HDR and LDR views. To avoid the limitations of NDC system and establish a data foundation for the research of 3DGS-based methods, we recalibrate the camera parameters and compute the initial positions for Gaussian point clouds. Experiments show that our HDR-GS outperforms the SOTA NeRF-based method by 1.91 and 3.84 dB on HDR and LDR novel view rendering, while enjoying 1000\(\times\) inference speed and requiring only 6.3% training time.

## Acknowledgement

This work was supported by the office of Naval Research with award N000142412696.

## References

* [1] Z. Yang, Y. Chai, D. Anguelov, Y. Zhou, P. Sun, D. Erhan, S. Rafferty, and H. Kretzschmar, "Surfelgan: Synthesizing realistic sensor data for autonomous driving," in _CVPR_, 2020.
* [2] S. Huang, Z. Gojcic, Z. Wang, F. Williams, Y. Kasten, S. Fidler, K. Schindler, and O. Litany, "Neural lidar fields for novel view synthesis," in _ICCV_, 2023.
* [3] G. Wang, Z. Chen, C. C. Loy, and Z. Liu, "Sparsenerf: Distilling depth ranking for few-shot novel view synthesis," in _ICCV_, 2023.
* [4] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P. Srinivasan, J. T. Barron, and H. Kretzschmar, "Block-nerf: Scalable large scene neural view synthesis," in _CVPR_, 2022.
* [5] S. Liu, X. Zhang, Z. Zhang, R. Zhang, J.-Y. Zhu, and B. Russell, "Editing conditional radiance fields," in _ICCV_, 2021.
* [6] J. Sun, X. Wang, Y. Shi, L. Wang, J. Wang, and Y. Liu, "Ide-3d: Interactive disentangled editing for high-resolution 3d-aware portrait synthesis," _ACM ToG_, 2022.
* [7] Y.-J. Yuan, Y.-T. Sun, Y.-K. Lai, Y. Ma, R. Jia, and L. Gao, "Nerf-editing: geometry editing of neural radiance fields," in _CVPR_, 2022.
* [8] S. Kobayashi, E. Matsumoto, and V. Sitzmann, "Decomposing nerf for editing via feature field distillation," in _NeurIPS_, 2022.
* [9] L. Liu, M. Habermann, V. Rudnev, K. Sarkar, J. Gu, and C. Theobalt, "Neural actor: Neural free-view synthesis of human actors with pose control," _ACM TOG_, 2021.
* [10] T. Hu, K. Sarkar, L. Liu, M. Zwicker, and C. Theobalt, "Egorenderer: Rendering human avatars from egocentric camera images," in _ICCV_, 2021.
* [11] J. Zheng, Y. Jang, A. Papaioannou, C. Kampouris, R. A. Potamias, F. P. Papantoniou, E. Galanakis, A. Leonardis, and S. Zafeiriou, "llsh: The imperial light-stage head dataset for human head view synthesis," in _ICCV_, 2023.
* [12] Z. Zheng, H. Huang, T. Yu, H. Zhang, Y. Guo, and Y. Liu, "Structured local radiance fields for human avatar modeling," in _CVPR_, 2022.
* [13] B. Mildenhall, P. Srinivasan, M. Tancik, J. Barron, R. Ramamoorthi, and R. Ng, "Nerf: Representing scenes as neural radiance fields for view synthesis," in _ECCV_, 2020.
* [14] X. Huang, Q. Zhang, Y. Feng, H. Li, X. Wang, and Q. Wang, "Hdr-nerf: High dynamic range neural radiance fields," in _CVPR_, 2022.
* [15] B. Kerbl, G. Kopanas, T. Leimkuhler, and G. Drettakis, "3d gaussian splatting for real-time radiance field rendering," _ACM Transactions on Graphics_, 2023.
* [16] P. E. Debevec and J. Malik, "Recovering high dynamic range radiance maps from photographs," in _SIGGRAPH_, 1997.
* [17] J. L. Schonberger and J.-M. Frahm, "Structure-from-motion revisited," in _CVPR_, 2016.
* [18] R. Szeliski, _Computer vision: algorithms and applications_. Springer Nature, 2022.
* [19] T. Mertens, J. Kautz, and F. Van Reeth, "Exposure fusion," in _Pacific Conference on Computer Graphics and Applications_, 2007.
* [20] G. Ward, E. Reinhard, and P. Debevec, "High dynamic range imaging & image-based lighting," in _SIGGRAPH_, 2008.
* [21] T. Grosch _et al._, "Fast and robust high dynamic range image generation with camera and object movement," _Vision, Modeling and Visualization, RWTH Aachen_, 2006.
* [22] K. Jacobs, C. Loscos, and G. Ward, "Automatic high-dynamic range image generation for dynamic scenes," _IEEE Computer Graphics and Applications_, 2008.
* [23] N. K. Kalantari, R. Ramamoorthi, _et al._, "Deep high dynamic range imaging of dynamic scenes.," _ACM ToG_, 2017.
* [24] O. T. Tursun, A. O. Akyuz, A. Erdem, and E. Erdem, "The state of the art in hdr deghosting: A survey and evaluation," in _Computer Graphics Forum_, 2015.
* [25] Q. Yan, Y. Zhu, and Y. Zhang, "Robust artifact-free high dynamic range imaging of dynamic scenes," _Multimedia Tools and Applications_, 2019.
* [26] G. Eilertsen, J. Kronander, G. Denes, R. K. Mantiuk, and J. Unger, "Hdr image reconstruction from a single exposure using deep cnns," _ACM TOG_, 2017.
* [27] Z. Khan, M. Khanna, and S. Raman, "Fhdr: Hdr image reconstruction from a single ldr image using feedback network," in _IEEE Global Conference on Signal and Information Processing_, 2019.

* [28] J. Kim, S. Lee, and S.-J. Kang, "End-to-end differentiable learning to hdr image synthesis for multi-exposure images," in _AAAI_, 2021.
* [29] Z. Liu, W. Lin, X. Li, Q. Rao, T. Jiang, M. Han, H. Fan, J. Sun, and S. Liu, "Adnet: Attention-guided deformable convolutional network for high dynamic range imaging," in _CVPRW_, 2021.
* [30] R. Chen, B. Zheng, H. Zhang, Q. Chen, C. Yan, G. Slabaugh, and S. Yuan, "Improving dynamic hdr imaging with fusion transformer," in _AAAI_, 2023.
* [31] Z. Liu, Y. Wang, B. Zeng, and S. Liu, "Ghost-free high dynamic range imaging with context-aware transformer," in _ECCV_, 2022.
* [32] J. W. Song, Y.-I. Park, K. Kong, J. Kwak, and S.-J. Kang, "Selective transhdr: Transformer-based selective hdr imaging using ghost region mask," in _ECCV_, 2022.
* [33] Y. Yu, H. Wang, T. Luo, H. Fan, and L. Zhang, "Magic: Multi-modality guided image completion," in _ICLR_, 2024.
* [34] J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla, and P. P. Srinivasan, "Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields," in _ICCV_, 2021.
* [35] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, "Mip-nerf 360: Unbounded anti-aliased neural radiance fields," in _CVPR_, 2022.
* [36] D. Verbin, P. Hedman, B. Mildenhall, T. Zickler, J. T. Barron, and P. P. Srinivasan, "Ref-nerf: Structured view-dependent appearance for neural radiance fields," in _CVPR_, 2022.
* [37] W. Hu, Y. Wang, L. Ma, B. Yang, L. Gao, X. Liu, and Y. Ma, "Tri-miprf: Tri-mip representation for efficient anti-aliasing neural radiance fields," in _ICCV_, 2023.
* [38] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, "Zip-nerf: Anti-aliased grid-based neural radiance fields," in _ICCV_, 2023.
* [39] T. Muller, A. Evans, C. Schied, and A. Keller, "Instant neural graphics primitives with a multiresolution hash encoding," _ACM ToG_, 2022.
* [40] C. Reiser, R. Szeliski, D. Verbin, P. Srinivasan, B. Mildenhall, A. Geiger, J. Barron, and P. Hedman, "Merf: Memory-efficient radiance fields for real-time view synthesis in unbounded scenes," _TOG_, 2023.
* [41] A. Chen, Z. Xu, A. Geiger, J. Yu, and H. Su, "Tensorf: Tensorial radiance fields," in _ECCV_, 2022.
* [42] R. Li, H. Gao, M. Tancik, and A. Kanazawa, "Nerfacc: Efficient sampling accelerates nerfs," in _ICCV_, 2023.
* [43] L. Yariv, P. Hedman, C. Reiser, D. Verbin, P. P. Srinivasan, R. Szeliski, J. T. Barron, and B. Mildenhall, "Bakedsdf: Meshing neural sdfs for real-time view synthesis," in _SIGGRAPH_, 2023.
* [44] Z. Chen, T. Funkhouser, P. Hedman, and A. Tagliasacchi, "Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures," in _CVPR_, 2023.
* [45] T. Hu, S. Liu, Y. Chen, T. Shen, and J. Jia, "Efficientnerf efficient neural radiance fields," in _CVPR_, 2023.
* [46] Z. Cui, L. Gu, X. Sun, X. Ma, Y. Qiao, and T. Harada, "Aleth-nerf: Illumination adaptive nerf with concealing field assumption," in _AAAI_, 2024.
* [47] Y. Cai, J. Wang, A. Yuille, Z. Zhou, and A. Wang, "Structure-aware sparse-view x-ray 3d reconstruction," in _CVPR_, 2024.
* [48] L. Ma, X. Li, J. Liao, Q. Zhang, X. Wang, J. Wang, and P. V. Sander, "Deblur-nerf: Neural radiance fields from blurry images," in _CVPR_, 2022.
* [49] N. Pearl, T. Treibitz, and S. Korman, "Nan: Noise-aware nerfs for burst-denoising," in _CVPR_, 2022.
* [50] Z. Yang, H. Yang, Z. Pan, X. Zhu, and L. Zhang, "Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting," _arXiv preprint arXiv:2310.10642_, 2023.
* [51] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, and X. Wang, "4d gaussian splatting for real-time dynamic scene rendering," _arXiv preprint arXiv:2310.08528_, 2023.
* [52] J. Luiten, G. Kopanas, B. Leibe, and D. Ramanan, "Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis," _arXiv preprint arXiv:2308.09713_, 2023.
* [53] N. Keetha, J. Karhade, K. M. Jatvallabhula, G. Yang, S. Scherer, D. Ramanan, and J. Luiten, "Splatam: Splat, track & map 3d gaussians for dense rgb-d slam," _arXiv preprint arXiv:2312.02126_, 2023.
* [54] V. Yugay, Y. Li, T. Gevers, and M. R. Oswald, "Gaussian-slam: Photo-realistic dense slam with gaussian splatting," _arXiv preprint arXiv:2312.10070_, 2023.
* [55] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, "Gaussian splatting slam," _arXiv preprint arXiv:2312.06741_, 2023.

* [56] C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, and X. Li, "Gs-slam: Dense visual slam with 3d gaussian splatting," _arXiv preprint arXiv:2311.11700_, 2023.
* [57] Z. Liang, Q. Zhang, Y. Feng, Y. Shan, and K. Jia, "Gs-ir: 3d gaussian splatting for inverse rendering," _arXiv preprint arXiv:2311.16473_, 2023.
* [58] T. Xie, Z. Zong, Y. Qiu, X. Li, Y. Feng, Y. Yang, and C. Jiang, "Physgaussian: Physics-integrated 3d gaussians for generative dynamics," _arXiv preprint arXiv:2311.12198_, 2023.
* [59] Y. Jiang, J. Tu, Y. Liu, X. Gao, X. Long, W. Wang, and Y. Ma, "Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces," _arXiv preprint arXiv:2311.17977_, 2023.
* [60] X. Liu, X. Zhan, J. Tang, Y. Shan, G. Zeng, D. Lin, X. Liu, and Z. Liu, "Humangaussian: Text-driven 3d human generation with gaussian splatting," _arXiv preprint arXiv:2311.17061_, 2023.
* [61] M. Kocabas, J.-H. R. Chang, J. Gabriel, O. Tuzel, and A. Ranjan, "Hugs: Human gaussian splats," _arXiv preprint arXiv:2311.17910_, 2023.
* [62] S. Hu and Z. Liu, "Gauhuman: Articulated gaussian splatting from monocular human videos," _arXiv preprint arXiv:_, 2023.
* [63] J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng, "Dreamgaussian: Generative gaussian splatting for efficient 3d content creation," _arXiv preprint arXiv:2309.16653_, 2023.
* [64] T. Yi, J. Fang, G. Wu, L. Xie, X. Zhang, W. Liu, Q. Tian, and X. Wang, "Gaussiandreamer: Fast generation from text to 3d gaussian splatting with point cloud priors," _arXiv preprint arXiv:2310.08529_, 2023.
* [65] Y. Liang, X. Yang, J. Lin, H. Li, X. Xu, and Y. Chen, "Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching," _arXiv preprint arXiv:2311.11284_, 2023.
* [66] Y. Cai, Y. Liang, J. Wang, A. Wang, Y. Zhang, X. Yang, Z. Zhou, and A. Yuille, "Radiative gaussian splatting for efficient x-ray novel view synthesis," in _ECCV_, 2024.
* [67] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, "Ewa volume splatting," in _Proceedings Visualization, 2001. VIS'01._, IEEE, 2001.
* [68] G. Kopanas, J. Philip, T. Leimkuhler, and G. Drettakis, "Point-based neural rendering with per-view optimization," in _Computer Graphics Forum_, 2021.
* [69] K. R. Prabhakar, S. Agrawal, D. K. Singh, B. Ashwath, and R. V. Babu, "Towards practical and efficient high-resolution hdr deghosting with cnn," in _ECCV_, 2020.
* [70] Q. Yan, D. Gong, Q. Shi, A. v. d. Hengel, C. Shen, I. Reid, and Y. Zhang, "Attention-guided network for ghost-free high dynamic range imaging," in _CVPR_, 2019.
* [71] "Blender."[https://www.blender.org/](https://www.blender.org/).
* [72] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, _et al._, "Pytorch: An imperative style, high-performance deep learning library," in _NeurIPS_, 2019.
* [73] D. P. Kingma and J. L. Ba, "Adam: A method for stochastic optimization," in _ICLR_, 2015.
* [74] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncell, "Image quality assessment: from error visibility to structural similarity," _TIP_, 2004.
* [75] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, "The unreasonable effectiveness of deep features as a perceptual metric," in _CVPR_, 2018.
* [76] "Photomatix Pro 6." [https://www.hdrsoft.com/](https://www.hdrsoft.com/).
* [77] R. Martin-Brualla, N. Radwan, M. S. Sajjadi, J. T. Barron, A. Dosovitskiy, and D. Duckworth, "Nerf in the wild: Neural radiance fields for unconstrained photo collections," in _CVPR_, 2021.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Please refer to the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please refer to Sec. 5 Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to addressing problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: This paper does not include any theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in the appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Please refer to Sec. 3 and Sec. 4.1 for details. Code and data are publicly available at [https://github.com/caiyuanhao1998/HDR-GS](https://github.com/caiyuanhao1998/HDR-GS) Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: Code and data are available at [https://github.com/caiyuanhao1998/HDR-GS](https://github.com/caiyuanhao1998/HDR-GS) Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Please refer to Sec. 4.1 for the implementation details of training and testing. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Please refer to the experiment part, _i.e._, Sec. 4. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please refer to Sec. 4.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Please refer to Sec. 5 Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have carefully credited all previous works we used in the paper. The license and terms are properly respected. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide our source code with instructions. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.