Gaussian Partial Information Decomposition: Bias Correction and Application to High-dimensional Data

 Praveen Venkatesh\({}^{1,2,*}\), Corbett Bennett\({}^{1}\), Sam Gale\({}^{1}\), Tamina K. Ramirez\({}^{3}\),

**Greggory Heller\({}^{4}\), Severine Durand\({}^{1}\), Shawn Olsen\({}^{1}\), Stefan Mihalas\({}^{1,\dagger}\)\({}^{1}\)**

\({}^{1}\)Allen Institute; \({}^{2}\)University of Washington; \({}^{3}\)Columbia University;

\({}^{4}\)Massachusetts Institute of Technology

\({}^{*}\)praveen.venkatesh@alleninstitute.org; \({}^{\dagger}\)stefanm@alleninstitute.org

###### Abstract

Recent advances in neuroscientific experimental techniques have enabled us to simultaneously record the activity of thousands of neurons across multiple brain regions. This has led to a growing need for computational tools capable of analyzing how task-relevant information is represented and communicated between several brain regions. Partial information decompositions (PIDs) have emerged as one such tool, quantifying how much unique, redundant and synergistic information two or more brain regions carry about a task-relevant message. However, computing PIDs is computationally challenging in practice, and statistical issues such as the bias and variance of estimates remain largely unexplored. In this paper, we propose a new method for efficiently computing and estimating a PID definition on multivariate Gaussian distributions. We show empirically that our method satisfies an intuitive additivity property, and recovers the ground truth in a battery of canonical examples, even at high dimensionality. We also propose and evaluate, for the first time, a method to correct the bias in PID estimates at finite sample sizes. Finally, we demonstrate that our Gaussian PID effectively characterizes inter-areal interactions in the mouse brain, revealing higher redundancy between visual areas when a stimulus is behaviorally relevant.

## 1 Introduction

Neuroscientific experiments are increasingly collecting large-scale datasets with simultaneous recordings of multiple brain regions with single-unit resolution [1, 2, 3]. These experimental advances call for new computational tools that can allow us to probe how multiple brain regions jointly process relevant information in a behaving animal.

Partial Information Decompositions (PIDs) offer a new method for studying how different brain regions carry task-relevant information: they provide measures to quantify the amount of _unique_, _redundant_ and _synergistic_ information that one region has with respect to another. The information itself could pertain to task-relevant variables such as stimuli, behavioral responses, or information contained in a third region. For example, we may be interested in how much information about a stimulus is communicated or shared (i.e., redundantly present) between two brain regions over time. Or, we might be interested in the extent to which one region's activity uniquely explains that of another, while excluding information corresponding to spontaneous behaviors.

Ideas such as redundancy and synergy have a long history in neuroscience, having been proposed for understanding noise correlations [4] and to understand differences in encoding complexity between different brain regions [5]. PIDs have also been suggested for quantifying how much sensory information is used to execute behaviors [6] and for tracking stimulus-dependent information flows between brain regions [7, 8]. Outside of neuroscience, PID has been used to understand interactionsbetween different variables in financial markets [9], to quantify the relevance of different features for the purpose of feature selection in machine learning [10], and to define and quantify bias in the field of fair Machine Learning [11].

An important constraint that has limited the broader adoption of PIDs in neuroscience is the computational difficulty of estimating PIDs for high-dimensional data. Many PID definitions that are operationally well-motivated involve solving an optimization problem over a space of probability distributions: the number of optimization variables can thus be exponential in the number of neurons [12]. This has led to the use of poorly motivated PID definitions that are easy to compute (such as the "MMI-PID" of [13], in works such as [9; 14; 15; 16]), or limited analyses to very few dimensions [17]. Furthermore, due to the limited exploration of estimators for PIDs, issues such as the bias and variance of estimates have received no attention so far, to our knowledge.

In this paper, we make the following contributions:

1. We provide a new and efficient method for computing and estimating a well-known PID definition called the \(\sim\)-PID or the BROJA-PID [18] on Gaussian distributions (Section 3). By restricting our attention to Gaussian distributions, we are able to significantly reduce the number of optimization variables, so that this is just quadratic in the number of neurons, rather than exponential.
2. We present a set of canonical examples for Gaussian distributions where ground truth is known, and show that our method outperforms others (Section 4).
3. We also raise (for what we believe is the first time) the issue of bias in PID estimates, propose a method for correcting the bias, and empirically evaluate its performance (Section 5).
4. Finally, we show that our Gaussian PID estimator closely agrees with ground truth, even on non-Gaussian distributions, and show an example of its use on real neural data (Section 6).

#### Related work

Our method is based on our earlier work [12], where we also examined PIDs for Gaussian distributions. Our current work differs in a few key aspects: (i) we estimate the PID of a different PID definition, the \(\sim\)-PID rather than the \(\delta\)-PID, because the \(\delta\)-PID does not satisfy a basic property called additivity [19] (defined in Sec. 2); (ii) our current method provides an exact upper bound to the PID definition being computed, rather than an approximate upper bound; (iii) we now consider the problem of estimation, not just computation, and explore the issue of the bias of PID estimates; and (iv) our current method is much faster, and we demonstrate agreement with ground truth at much higher dimensionality.

Several other studies have also considered methods for efficiently estimating PIDs: Banerjee et al. [20] and Makkeh et al. [21] address computing _discrete_ PIDs, but their method does not scale to higher dimensions; Pakman et al. [17] estimate PIDs using copulas, but their method would also potentially be computationally prohibitive at high dimensionalities; Liang et al. [22] use convex optimization to directly estimate the \(\sim\)-PID for general high-dimensional distributions, but they do not compare with ground truth at high dimensionality or examine bias in their estimates.

## 2 Background: An Introduction to PIDs and the \(\sim\)-PID

In this section, we provide an introduction to the concept of partial information decomposition along with an illustrative example. Let \(M\), \(X\) and \(Y\) be three random variables with joint distribution \(P_{MXY}\). A PID decomposes the total mutual information between the _message_\(M\) and two _constituent_ random variables \(X\) and \(Y\) into a sum of four non-negative components that satisfy [18; 23]:

\[I\big{(}M;(X,Y)\big{)} =UI(M:X\setminus Y)+UI(M:Y\setminus X)+RI(M:X;Y)+SI(M:X;Y) \tag{1}\] \[I(M;X) =UI(M:X\setminus Y)+RI(M:X;Y)\] (2) \[I(M;Y) =UI(M:Y\setminus X)+RI(M:X;Y) \tag{3}\]

Here, \(I(A;B)\) is the _Shannon mutual information_[24] between the random variables \(A\) and \(B\), and the four terms in the RHS of (1) are respectively the information about \(M\) that is (i) _uniquely_ present in \(X\) and not in \(Y\); (ii) _uniquely_ present in \(Y\) and not in \(X\); (iii) _redundantly_ present in both \(X\) and \(Y\) and can be extracted from either; and (iv) _synergistically_ present in \(X\) and in \(Y\), i.e., information which cannot be extracted from either of them individually, but can be extracted from their interaction. For the sake of brevity, we may also refer to these partial information components as \(UI_{X}\), \(UI_{Y}\), \(RI\) and \(SI\) respectively. Notwithstanding notation, they should all properly be understood to be functions of the joint distribution \(P_{MXY}\).

Now, \(UI_{X}\), \(UI_{Y}\), \(RI\) and \(SI\) consist of four undefined quantities, subject to the three equations in (1)-(3). In addition, they are typically assumed to be non-negative, \(RI\) and \(SI\) are each constrained to be symmetric in \(X\) and \(Y\), and the functional forms of \(UI_{X}\) and \(UI_{Y}\) should be identical when exchanging \(X\) for \(Y\). Despite the number of constraints, many definitions satisfy all of them, each differing in its motivation and interpretation [18; 23; 25; 26; 27] (see [27; 28] for a review), and we need to formally define one of these partial information components to determine the other three.

**Example 1**.: Before we jump into a specific definition, we provide an intuition into what these terms mean using a simple example. Suppose \(M=[A,B,C]\), \(X=[A,B,C\oplus Z]\), and \(Y=[B,Z]\), where \(A,B,C,Z\sim\) i.i.d. Ber(0.5).1 Then, \(X\) has 1 bit of unique information about \(M\), i.e., \(A\); \(Y\) has no unique information; \(X\) and \(Y\) both have 1 bit of redundant information, i.e., \(B\), since it can be obtained from either \(X\) or \(Y\); and \(X\) and \(Y\) have 1 bit of synergistic information, i.e., \(C\), which cannot be obtained from either \(X\) or \(Y\) individually (since \(C\oplus Z\perp C\)), but can only be recovered when both \(X\) and \(Y\) are known. For more examples on binary variables, we refer the reader to [18]. 

Footnote 1: i.i.d. stands for “independent and identically distributed”; \(X\perp Y\) means \(X\) and \(Y\) are independent.

In this manuscript, we consider a definition that we refer to as the \(\sim\)-PID2[18; 25], which is defined below. We chose to build an estimator for _this_ definition for two reasons: (i) it is a _Blackwellian PID_ definition, i.e., it has well-defined operational interpretations based on concepts from statistical decision theory (e.g., see [18; 27; 29] for details); and (ii) it satisfies many desirable properties (e.g., see [18; 30]), and in particular, a property that we call _additivity of independent components_.

Footnote 2: This PID is also sometimes referred to as the BROJA PID (after the authors of [18]), or the minimum-synergy PID in the literature. We prefer to use an author-agnostic nomenclature as introduced in our earlier work [12], because this PID was also introduced contemporaneously by [25].

**Definition 1** (\(\sim\)-PID [18]).: _The unique information about \(M\) present in \(X\) and not in \(Y\) is given by_

\[\widetilde{UI}(M:X\setminus Y)\coloneqq\min_{Q\in\Delta_{P}}I_{Q}(M;X\,|\,Y), \tag{4}\]

_where \(\Delta_{P}\coloneqq\{Q_{MXY}:Q_{MX}=P_{MX},\;Q_{MY}=P_{MY}\}\) and \(I_{Q}(\,\cdot\,;\cdot\,|\,\cdot)\) is the conditional mutual information over the joint distribution \(Q_{MXY}\). The remaining \(\sim\)-PID components, \(\widetilde{UI}(M:Y\setminus X)\), \(\widetilde{RI}(M:X;Y)\) and \(\widetilde{SI}(M:X;Y)\), follow from equations (1)-(3)._

**Property 1** (Additivity of independent components).: _Suppose \(M=[M_{1},M_{2}]\), \(X=[X_{1},X_{2}]\), and \(Y=[Y_{1},Y_{2}]\), such that \((M_{1},X_{1},Y_{1})\perp(M_{2},X_{2},Y_{2})\). Then, additivity implies that_

\[UI(M:X\setminus Y)=UI(M_{1}:X_{1}\setminus Y_{1})+UI(M_{2}:X_{2}\setminus Y_{2 }), \tag{5}\]

_and similarly for the other three partial information components, \(UI_{Y}\), \(RI\) and \(SI\)._

Property 1 stipulates that we should be able to compute the PIDs of two independent systems _separately_, and then add the components across both systems. In effect, additivity implies that the PID of an isolated system should not depend on the PID of another isolated system, making it an intuitive and highly desirable property (see App. A.1 for concrete examples). Of the many PID definitions examined by Rauh et al. [19], only the \(\sim\)-PID satisfied additivity (as proved in [18]).

## 3 Computing the \(\sim\)-PID for Gaussian Distributions

The first contribution of this paper is a method to efficiently compute bounds on the \(\sim\)-PID for jointly Gaussian random vectors \(M\), \(X\) and \(Y\). To be precise, our method computes an upper bound for \(\widetilde{UI}_{X}\) and \(\widetilde{UI}_{Y}\), and lower bounds for \(\widetilde{RI}\) and \(\widetilde{SI}\). Similar to our earlier work [12], we present a new PID definition that we call the \(\sim\)-PID, which characterizes an upper bound on the unique information of the \(\sim\)-PID by restricting the optimization space to jointly Gaussian \(Q_{MXY}\):

**Definition 2** (\(\sim_{G}\)-Pid).: _Let \(P_{MXY}\) be jointly Gaussian. Then, the unique information about \(M\) present in \(X\) and not in \(Y\) is given by_

\[\widetilde{UI}_{G}(M:X\setminus Y)\coloneqq\min_{Q\in\Delta_{P}}I_{Q}(M;X\,|\, Y), \tag{6}\]

_where \(\Delta_{P}\coloneqq\{Q_{MXY}:Q_{MXY}\) jointly Gaussian, \(Q_{MX}=P_{MX},\;Q_{MY}=P_{MY}\}\) and \(I_{Q}\) is the conditional mutual information over the joint distribution \(Q_{MXY}\)._

If the optimal \(Q_{MXY}\) in the unrestricted optimization of Definition 1 happens to be Gaussian for some \(P_{MXY}\), then the \(\sim_{G}\)-PID would be identical to the \(\sim\)-PID for that \(P_{MXY}\). We conjecture that this happens whenever \(P_{MXY}\) is Gaussian: for example, in a similar optimization problem for computing the information bottleneck [31], the optimal distribution is Gaussian whenever \(P\) is Gaussian [32; 33]. We provide empirical evidence in favor of this conjecture through a sequence of examples with Gaussian \(P\) in Sec. 4, where we recover the ground truth even when \(Q\) is restricted to be Gaussian. However, we leave a theoretical examination of this conjecture for future work.

In practical terms, restricting the search space to Gaussian \(Q_{MXY}\) reduces the number of optimization variables from being exponential in the dimensionality to quadratic (see Appendix A.2), allowing us to compute the \(\sim_{G}\)-PID for much higher dimensionalities of \(M\), \(X\) and \(Y\). In what follows, we show how the optimization problem for the \(\sim_{G}\)-PID can be written out in closed-form and then solved using projected gradient descent.

### Notation and Preliminaries

Suppose \(M\), \(X\) and \(Y\) are jointly Gaussian random vectors of dimensions \(d_{M}\), \(d_{X}\) and \(d_{Y}\) respectively, with a joint covariance matrix given by \(\Sigma_{MXY}\). We will make extensive use of the submatrices of \(\Sigma_{MXY}\), so we explain their notation here:

* \(\Sigma_{XY}\) will denote the \((d_{X}+d_{Y})\times(d_{X}+d_{Y})\) joint (auto-)covariance matrix of the vector \([X^{\mathsf{T}},Y^{\mathsf{T}}]^{\mathsf{T}}\).
* \(\Sigma_{X,Y}\) (note the comma) will denote the \(d_{X}\times d_{Y}\) cross-covariance matrix between \(X\) and \(Y\).
* \(\Sigma_{XY,M}\) will denote the \((d_{X}+d_{Y})\times d_{M}\) cross-covariance matrix between the concatenated vector \([X^{\mathsf{T}},Y^{\mathsf{T}}]^{\mathsf{T}}\) and the vector \(M\).

In general, groupings of vectors without commas represent joint covariance, while a comma represents a cross-covariance between the groups on either side of the comma. The same notation will also be used for conditional covariance matrices: for example, \(\Sigma_{XY|M}\) is the conditional _joint_ covariance of \((X,Y)\) given \(M\), while \(\Sigma_{X,Y|M}\) is the conditional _cross_-covariance _between_\(X\) and \(Y\) given \(M\).

We will also use an equivalent notation for the joint distribution [12], where \(P_{MXY}\) is parameterized as a "broadcast channel" [24, Ch. 15.6] from \(M\) to \(X\) and \(Y\):

\[X=H_{X}M+N_{X}\quad\text{and}\quad Y=H_{Y}M+N_{Y}. \tag{7}\]

Here, \(H_{X}\coloneqq\Sigma_{X,M}\) and \(H_{Y}\coloneqq\Sigma_{Y,M}\) represent _channel gain matrices_, while \(N_{X}\) and \(N_{Y}\) represent additive noise and are not necessarily independent of each other: \([N_{X}^{\mathsf{T}},N_{Y}^{\mathsf{T}}]^{\mathsf{T}}\sim\mathcal{N}(0,\Sigma_ {XY|M})\).

**Remark 1**.: Without loss of generality, we can assume that \(M\), \(X\) and \(Y\) are all zero-mean, and that \(\Sigma_{M}=I\). Further, we explicitly assume that the \(X\) and \(Y\) channels are individually whitened, i.e., that \(\Sigma_{X|M}=I\) and \(\Sigma_{Y|M}=I\). This assumption precludes deterministic relationships between \(M\) and \(X\) or \(Y\), and is required to ensure that information quantities remain finite [12]. 

### Optimizing the Union Information

Bertschinger et al. [18] showed that the minimizer for the unique information is also the minimizer for the "union information", \(I^{\cup}(M:X;Y)\coloneqq UI_{X}+UI_{Y}+RI\). In other words, we can also solve the following optimization problem, which yields simpler expressions for the objective and gradient:

\[\widetilde{I^{\cup}}(M:X;Y)\coloneqq\min_{Q_{MXY}}I_{Q}(M;X,Y) \quad\text{s.t.}\quad Q_{MX}=P_{MX},\;Q_{MY}=P_{MY} \tag{8}\]

Now, suppose \(P_{MXY}\) is Gaussian with covariance \(\Sigma_{MXY}^{P}\) and the solution \(Q_{MXY}\) is also assumed to be Gaussian with covariance \(\Sigma_{MXY}^{Q}\). Then, the constraint in (8) implies that \(\Sigma_{MX}^{Q}=\Sigma_{MX}^{P}\) and \(\Sigma_{MY}^{Q}=\Sigma_{MY}^{P}\). In other words, \(\Sigma_{M}\), \(\Sigma_{X}\), \(\Sigma_{Y}\), and \(\Sigma_{M,XY}\) are all constant across \(P\) and \(Q\). Therefore, the only part of \(\Sigma_{MXY}^{Q}\) that is variable is \(\Sigma_{X,Y}^{Q}\), or equivalently, \(\Sigma_{X,Y|M}^{Q}\).3 In what follows, we will drop the superscripts denoting the distribution, as this will be clear from context. Generally speaking, we will discuss the optimization problem and thus the distribution will be \(Q\).

Footnote 3: We can use \(\Sigma_{X,Y|M}\) in place of \(\Sigma_{X,Y}\) because they differ by a constant: \(\Sigma_{X,Y|M}-\Sigma_{X,Y}\) is an off-diagonal block in \(\Sigma_{XY}-\Sigma_{XY|M}\), which is equal to \(\Sigma_{XY,M}\Sigma_{M}^{-1}\Sigma_{XY,M}^{\mathsf{T}}\), which is constant across \(P\) and \(Q\).

**Proposition 1**.: _The union information for the \(\sim_{G}\)-PID of Definition 2 is given by_

\[\widetilde{I^{\cup}_{G}}\coloneqq\min_{\Sigma_{X,Y|M}}\frac{1}{2} \log\det\bigl{(}I+\Sigma_{M}^{-1}\Sigma_{XY,M}^{\mathsf{T}}\Sigma_{XY|M}^{-1} \Sigma_{XY,M}\bigr{)}\quad\text{s.t.}\quad\Sigma_{XY|M}\succcurlyeq 0 \tag{9}\]_where the optimization variable \(\Sigma_{X,Y|M}\) is an off-diagonal block embedded within \(\Sigma_{XY|M}\); all other matrices in the objective are constants that are derived from \(\Sigma_{MXY}^{\mathrm{P}}\)._

We solve the above optimization problem using projected gradient descent: we analytically derive the gradient and the projection operator for the constraint set as shown below. Then, we use the RProp algorithm [34; 35] for gradient descent, which independently adjusts the learning rates for each optimization parameter (derivations, and details of implementation and complexity are in App. A). Code for our implementation is available on GitHub [36], and details of the compute configuration are given in App. E.

**Proposition 2**.: _The **objective** in Proposition 1 can be simplified to_

\[f(\Sigma_{X,Y|M})=\frac{1}{2}\log\det\bigl{(}I+H_{Y}^{\mathsf{T}}H_{Y}+B^{ \mathsf{T}}S^{-1}B\bigr{)}, \tag{10}\]

_where \(B\coloneqq(H_{X}-\Sigma_{X,Y|M}H_{Y})\) and \(S\coloneqq(I-\Sigma_{X,Y|M}\Sigma_{X,Y|M}^{\mathsf{T}})\)._

_The **gradient** of the objective with respect to \(\Sigma_{X,Y|M}\) is given by_

\[\nabla f(\Sigma_{X,Y|M})=S^{-1}B\bigl{(}I+H_{Y}^{\mathsf{T}}H_{Y}+B^{\mathsf{ T}}S^{-1}B\bigr{)}^{-1}\bigl{(}B^{\mathsf{T}}S^{-1}\Sigma_{X,Y|M}-H_{Y}^{ \mathsf{T}}\bigr{)}. \tag{11}\]

_A **projection operator** on to the constraint set \(\Sigma_{XY|M}\succcurlyeq 0\) can be obtained as follows: let \(\Sigma_{XY|M}\eq:V\Lambda V^{\mathsf{T}}\) be the eigenvalue decomposition of \(\Sigma_{XY|M}\), with \(\Lambda\coloneqq\mathsf{diag}(\lambda_{i})\). Let \(\overline{\lambda}_{i}\coloneqq\max(0,\lambda_{i})\) represent the rectified eigenvalues, and \(\overline{\Lambda}\coloneqq\mathsf{diag}(\overline{\lambda}_{i})\). Then, define_

\[\overline{\Sigma}_{XY|M} \coloneqq V\overline{\Lambda}V^{\mathsf{T}}, \tag{12}\] \[\Sigma_{X,Y|M}^{\mathsf{m}pij} \coloneqq\overline{\Sigma}_{X|M}^{-1/2}\overline{\Sigma}_{X,Y|M} \overline{\Sigma}_{Y|M}^{-1/2}, \tag{13}\]

_where \(\overline{\Sigma}_{X|M}\), \(\overline{\Sigma}_{Y|M}\) and \(\overline{\Sigma}_{X,Y|M}\) are submatrices of \(\overline{\Sigma}_{XY|M}\)._

## 4 Canonical Gaussian Examples

In this section, we show how well our \(\sim_{G}\)-PID estimator performs on a series of Gaussian examples of increasing complexity, which have known ground truth. Barrett [13] showed that, for Gaussian distributions, the \(\sim\)-PID reduces to the MMI-PID (defined below), whenever \(M\) is scalar. These also happen to be cases when the optimal distribution \(Q_{MXY}\) is Gaussian [12], and thus the \(\sim_{G}\)-PID should recover the ground truth. We then leverage additivity (Property 1) to combine two or more simple examples into complex ones, where ground truth continues to be known.

**Definition 3** (Minimum Mutual Information (MMI) PID).: _Let the redundant information be defined as the minimum of the two mutual informations:_

\[RI_{\text{MM}}(M:X;Y)=\min\{I(M;X),I(M;Y)\}. \tag{14}\]

_The remaining MMI-PID components, \(UI_{\text{MM}}(M:X\setminus Y)\), \(UI_{\text{MM}}(M:Y\setminus X)\) and \(SI_{\text{MM}}(M:X;Y)\), follow from equations (1)-(3)._

We first provide a Gaussian analog of Example 1 in Examples 2-4 (for \(d_{M}=d_{X}=d_{Y}=1\)). We will use the channel notation described in Equation (7). Complete derivations for these examples (and some nuances that are omitted here) are presented in App. B.

**Example 2** (Pure uniqueness: variable \(A\) from Example 1).: Suppose \(M\sim\mathcal{N}(0,1)\), \(H_{X}=1\) and \(H_{Y}=0\), with \(N_{X},N_{Y}\sim\text{i.i.d.}\,\mathcal{N}(0,1)\). Here, only \(X\) receives information about \(M\), while \(Y\) is pure noise. Thus, \(X\) has unique information about \(M\) (\(UI_{X}=I(M;X)>0\)), with no unique information in \(Y\), and no redundancy or synergy (\(UI_{Y}=RI=SI=0\)). 

**Example 3** (Pure redundancy: variable \(B\) from Example 1).: Ideally, we would set \(M\sim\mathcal{N}(0,1)\), \(X=M\) and \(Y=M\). However, for continuous random variables, \(I(M;X)=\infty\) when \(M=X\). So instead, we set \(M\sim\mathcal{N}(0,1)\), \(H_{X}=1\) and \(H_{Y}=1\), with \(N_{X}\sim\mathcal{N}(0,1)\) while \(N_{Y}=N_{X}\) (i.e., \(X=Y\), so they are both the same noisy version of \(M\)). In this case, \(X\) and \(Y\) are fully redundant since they both contain exactly the same information about \(M\). Thus, \(RI=I(M;(X,Y))>0\), while \(UI_{X}=UI_{Y}=SI=0\). 

**Example 4** (Pure synergy: variable \(C\) from Example 1).: We cannot replicate pure synergy for Gaussian variables, but we can approach it in a limit. Let \(M\sim\mathcal{N}(0,1)\), \(H_{X}=1\) and \(H_{Y}=0\), with \(N_{X}\sim\mathcal{N}(0,\sigma^{2})\) and \(N_{Y}=N_{X}\) (i.e., \(X=M+Y\)). Further, let \(\sigma^{2}\to\infty\). In this case, \(I(M;Y)=0\)and \(I(M;X)\to 0\) as \(\sigma^{2}\to\infty\), so \(X\) and \(Y\)_individually_ convey little to no information about \(M\). However, we can recover information about \(M\) from \(X\) and \(Y\)_together_ by taking their difference, since \(X-Y=M\). Thus, \(SI>0\), while \(UI_{Y}=RI=0\) and \(UI_{X}\to 0\). 

Examples 2, 3 and 4 have been provided solely for intuition. Their PIDs can be inferred directly from Equations (1)-(3). We next describe three one-dimensional examples that each have _two_ non-zero PID components. For lack of space, we only provide a brief description and defer details to App. B. We estimate the \(\sim_{G}\)-PID (as well as the \(\delta_{G}\)-PID [12] and the ground-truth MMI-PID [13]) for these examples and show that all three are equal (see Fig. 1).

**Example 5** (Unique and redundant information).: Let \(X\) be a noisy representation of \(M\), and let \(Y\) be a noisy representation of \(X\) with standard deviation \(\sigma_{Y|X}\). When \(Y=X\) (zero noise), this example reduces to Example 3. As \(\sigma_{Y|X}\to\infty\), \(RI\) reduces while \(UI_{X}\) approaches \(I(M;X)\). 

**Example 6** (Unique and synergistic information).: Let \(M\sim\mathcal{N}(0,1)\), \(H_{X}=1\), \(H_{Y}=0\) and \(N_{X},N_{Y}\sim\mathcal{N}(0,\sigma^{2})\) such that their correlation is \(\rho\). When \(\sigma^{2}\) is finite and \(\rho=0\), this example reduces to Example 2, since there can be no synergy between \(X\) and \(Y\). As \(\rho\to 1\), \(X-Y\to M\); so the total mutual information \(I(M;(X,Y))\to\infty\), driven by synergy growing unbounded, while the unique component remains constant at \(I(M;X)\). 

**Example 7** (Redundant and synergistic information).: Let \(M\sim\mathcal{N}(0,1)\), \(H_{X}=H_{Y}=1\) and \(N_{X},N_{Y}\sim\mathcal{N}(0,1)\) such that their correlation is \(\rho\). When \(\rho<1\), \(I(M;X)\) and \(I(M;Y)\) are both equal by symmetry, and thus equal to \(RI\) (see Def. 3 for the MMI-PID, which is ground truth here). As \(\rho\) reduces, the two channels \(X\) and \(Y\) have noisy representations of \(M\) with increasingly independent noise terms. Averaging the two, \((X+Y)/2\), will provide more information about \(M\) than either one of them individually (i.e., synergy), and thus \(SI\) increases as \(\rho\) reduces. 

Figure 1: PID values for Examples 5, 6 and 7. The \(\sim_{G}\)-PID and the \(\delta_{G}\)-PID agree exactly with the MMI-PID, which is known to be the ground truth, since \(M\) is scalar [13].

Figure 2: Left and right panels respectively show PID values for Examples 8 and 9, which combine two scalar examples with known ground truth, using Property 1. The middle panel shows the absolute error between each PID definition and the ground truth. The \(\sim_{G}\)-PID diverges from the \(\delta_{G}\)- and MMI-PIDs, and is the only one that agrees with the ground truth. The \(\sim_{G}\)-PID maintains an error less than \(10^{-7}\) bits, whereas other definitions have errors greater than \(0.1\) bits.

The next set of examples will use the examples presented above in different combinations. This ensures that, where possible, the ground truth remains known in accordance with Property 1. These examples are also designed to reveal the differences between the \(\sim\)-PID, the MMI-PID and the \(\delta\)-PID: in particular, they show how the MMI-PID and the \(\delta\)-PID fail where the \(\sim\)-PID does not. These examples use two-dimensional \(M\), \(X\) and \(Y\), i.e., \((d_{M},d_{X},d_{Y})=(2,2,2)\). A diagrammatic representation of Examples 8 and 9 is given in App. B.2.

**Example 8**.: Let \(X_{1}=\alpha M_{1}+N_{X,1}\), \(Y_{1}=M_{1}+N_{Y,1}\), \(X_{2}=M_{2}+N_{X,2}\) and \(Y_{2}=3M_{2}+N_{Y,2}\), where \(M_{1},M_{2},N_{X,i},N_{Y,i}\sim\) i.i.d. \(\mathcal{N}(0,1)\), \(i=1,2\). Here, \((M_{1},X_{1},Y_{1})\) is independent of \((M_{2},X_{2},Y_{2})\), therefore using Property 1, we can add the PID values from their individual decompositions (which each have known ground truth via the MMI-PID since \(M_{1}\) and \(M_{2}\) are scalar). Fig. 2(l) compares the \(\sim_{G}\)-PID, the \(\delta_{G}\)-PID and the MMI-PID for the joint decomposition of \(I(M;(X,Y))\), at different values of \(\alpha\), the gain in \(X_{1}\). Only the \(\sim_{G}\)-PID matches the ground truth, as it is the only definition here that is additive. 

**Example 9**.: Let \(M\) and \(Y\) be as in Example 8. Suppose \(X=H_{X}\)\(R(\theta)\)\(M\), where \(H_{X}\) is a diagonal matrix with diagonal entries 3 and 1, and \(R(\theta)\) is a \(2\times 2\) rotation matrix that rotates \(M\) by an angle \(\theta\). When \(\theta=0\), \(X\) has higher gain for \(M_{1}\) while \(Y\) has higher gain for \(M_{2}\). When \(\theta\) increases to \(\pi/2\), \(X\) and \(Y\) have equal gains for both \(M_{1}\) and \(M_{2}\) (barring a difference in sign). Since \((M_{1},X_{1},Y_{1})\) is not independent of \((M_{2},X_{2},Y_{2})\) for all \(\theta\), we know the ground truth only at the end-points. Nonetheless, the example shows a difference between the three definitions, as shown in Fig. 2(r). 

**Example 10**.: In this example, we test the stability of the \(\sim_{G}\)-PID as the dimensionality, \(d\coloneqq d_{M}=d_{X}=d_{Y}\) increases. By Property 1, if we take two i.i.d. systems of variables \((M,X,Y)\) at dimensionality \(d\) and concatenate their respective variables, every PID component of the composite system of dimensionality \(2d\) should be double that of the original. This process can be repeated, taking two independent \(2d\)-dimensional systems and concatenating them to create a \(4d\)-dimensional system. Fig. 3 shows precisely this process starting with the system in Example 8 with \(d=2\), and continually doubling its size until \(d=1024\). The \(\sim_{G}\)-PID accurately matches ground truth by doubling in value, and remains stable with small relative errors (shown in App. B.5). The \(\sim_{G}\)-PID also runs much faster than the \(\delta_{G}\)-PID on this example, providing a speed-up of more than \(1000\times\) at \(d=64\) (see right-most plot in Fig. 3, and extended results in App. B.4). 

**Remark 2**.: In the above examples, the ground truth referred to the \(\sim\)-PID of Def. 1, since it was inferred using Def. 3 and additivity (Property 1). Since our method for computing the \(\sim_{G}\)-PID recovers the ground truth \(\sim\)-PID, we can infer that the distribution of the optimal \(Q_{MXY}\) for the \(\sim\)-PID is in fact Gaussian in these examples. This provides empirical evidence in support of the conjecture stated in Sec. 3. 

## 5 Estimation and Bias-correction for the \(\sim_{G}\)-PID

Having discussed how to compute the \(\sim_{G}\)-PID and shown that it agrees well with ground truth in several canonical examples, we discuss how the \(\sim_{G}\)-PID may be estimated from data. Given a sample

Figure 3: The first five plots on the left show PID values for Example 10. Different shadings represent different values of gain in \(X_{1}\) (\(\alpha\)) from Example 8. The \(\sim_{G}\)-PID doubles every time \(d\) doubles as seen by the constant 45\({}^{\circ}\) slope on the base-2 log-log plot, even when \(d_{M}=d_{X}=d_{Y}=1024\). The right-most plot shows a timing comparison between the \(\sim_{G}\)- and the \(\delta_{G}\)-PIDs on this example. The \(\delta_{G}\)-PID failed for \(d>64\); the \(\sim_{G}\)-PID’s timing performance up to \(d=1024\) is shown in App. B.4.

of \(n\) realizations of \(M\), \(X\) and \(Y\) drawn from \(P_{MXY}\), we may estimate the sample joint covariance matrix \(\hat{\Sigma}_{MXY}\). We therefore use the straightforward "plug-in" estimator for the \(\sim_{G}\)-PID, by using \(\hat{\Sigma}_{MXY}\) in place of \(\Sigma_{MXY}\) in the optimization problem in equation (9).

However, it is well-known that estimators of information-theoretic quantities suffer from large biases for moderate sample sizes [37]. Cai et al. [38] characterized the bias in the entropy of a \(d\)-dimensional Gaussian random vector, for a fixed sample size \(n\).

**Proposition 3** (Bias in Gaussian entropy [38]).: _Suppose \(M\in\mathbb{R}^{d_{M}}\) has an auto-covariance matrix \(\Sigma_{M}\). The entropy of \(M\) is \(H(M)=\frac{1}{2}\log\det(2\pi e\Sigma_{M})\) when \(\Sigma_{M}\) is known [24]. For the sample covariance matrix \(\hat{\Sigma}_{M}\), the bias is given by:_

\[\mathrm{Bias}\big{[}\hat{H}(M)\big{]}=\sum_{k=1}^{d_{M}}\log(1-k/n). \tag{15}\]

For a proof, we refer the reader to [38, Corollary 2]. This result may be naturally extended to compute the bias of each of the mutual information quantities in the LHS of equations (1)-(3):

**Corollary 4** (Bias in Gaussian mutual information).: _For the joint mutual information \(I(M;(X,Y))\),_

\[\mathrm{Bias}\big{[}\hat{I}\big{(}M;(X,Y)\big{)}\big{]}=\sum_{k=1}^{d_{M}} \log(1-k/n)\ +\sum_{k=1}^{d_{X}+d_{Y}}\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!100 runs of two configurations called "Bit-of-all" (with a little bit of each PID component) and "Fully-redundant" (which has predominantly redundancy), with \(d_{M}=d_{X}=d_{Y}=10\) (details and additional setups in App. C). We find that bias correction brings the PID values closer to their true values even at small sample sizes. In App. C.3, we also include a preliminary analysis of the variance of PID estimates using bootstrap [39, Ch. 8].

## 6 Application to Simulated and Real Neural Data

So far, we have only considered the \(\sim_{G}\)-PID when applied to _Gaussian_\(P_{MXY}\). Although Def. 2 strictly applies only to Gaussian \(P_{MXY}\), the estimation process in Sec. 5 relies only on a sample covariance matrix, which is well-defined for a wide variety of non-Gaussian distributions. Many applications where PIDs could be useful have non-Gaussian data. For instance, there is great interest in applying PIDs in neuroscience (e.g., to understand how multiple brain regions jointly encode and communicate information [6, 40]), but spike-count distributions are non-Gaussian.

**Simulated neural data.** To show that our \(\sim_{G}\)-PID estimates provide reasonable results on non-Gaussian spiking neural data, we first simulate spike-count data using Poisson random variables (following [12]; described in App. D.1). We evaluate the ground truth \(\sim\)-PID for this distribution using the discrete PID estimator of Banerjee et al. [20]. The \(\sim_{G}\)-PID is estimated from a sample covariance matrix using \(10^{6}\) realizations of \(M\), \(X\) and \(Y\). We find that the \(\sim_{G}\)-PID closely matches

Figure 5: A comparison of the \(\sim_{G}\)-PID, the \(\delta_{G}\)-PID and the MMI-PID for a multivariate Poisson system. The ground truth is a discrete \(\sim\)-PID computed using the package of Banerjee et al. [30]. The \(\sim_{G}\)-PID comes closest to the ground truth (possibly because they compute the same PID definition), despite the fact that the \(\sim_{G}\)-PID only uses the covariance matrix of the Poisson distribution, whereas the ground truth uses knowledge of the distribution itself.

Figure 6: Bias-corrected redundancy estimates for information about VISp activity that is shared between VISI and VISal: redundancy in bits (left) and redundancy as a fraction of total mutual information (right). Data spread is across 40 mice. Statistical comparisons use a two-sided Mann-Whitney-Wilcoxon test. Observe that there is greater and more sustained redundancy on flashes corresponding to image changes, which are behaviorally relevant and linked to rewards in this task.

the ground truth for a range of parameter values, despite the fact that the \(\sim_{G}\)-PID is effectively computed on a Gaussian approximation of a Poisson distribution (Fig. 5). More examples of the \(\sim_{G}\)-PID applied to non-Gaussian data are provided in App. D.2 (also see App. A.2). We conclude that it is reasonable to use and interpret the \(\sim_{G}\)-PID on non-Gaussian spike count data.

**Real neural data.** We then applied our bias-corrected \(\sim_{G}\)-PID estimator to the Visual Behavior Neuropixels dataset collected by us at the Allen Institute [41]. We recorded over 80 mice using six neuropixels probes targeting various regions of visual cortex, while the mice were engaged in a visual change-detection task. In the task, images from a set of 8 natural scenes were presented in 250 ms flashes, at intervals of 750 ms; the image would stay the same for a variable number of flashes after which it would change to a new image. The mouse had to kick to receive a water reward when the image changed. Thus, a given image flash could be a behaviorally relevant target if the previous image was different, or not, if the previous image was the same.

We used our bias-corrected PID estimator to understand how information is processed along the visual hierarchy during this task. We estimated the \(\sim_{G}\)-PID to understand how information contained in the spiking activity of primary visual cortex (VISp) was represented in two higher-order visual cortical areas, VISI and VISal. We aligned trials to the onset of a stimulus flash, binned spikes in 50 ms intervals and considered the top 20 principal components (to achieve reasonable estimates at these sample sizes; explained further in App. D.5) from each region in each time bin. We computed the \(\sim_{G}\)-PID on the sample covariance matrix of these principal components (shown in Fig. 6). We found that there was a significantly larger amount of redundant information about VISp activity between VISI and VISal for stimulus flashes corresponding to an image change, compared to flashes that were not changes (Fig. 6(l)). The larger redundancy was also sustained slightly longer for flashes corresponding to changes, than non-change flashes. Both of these effects were maintained even when the redundancy was normalized by the joint mutual information, suggesting that the effect was not purely due to an increase in the total amount of information (Fig. 6(r)). Our results suggest that the visual cortex propagates information throughout the hierarchy more robustly when such information is relevant for behavior.

## 7 Discussion

In this paper, we proposed a new and efficient method for estimating the \(\sim_{G}\)-PID for Gaussian distributions. We showed that our method recovers the ground truth and suitably corrects for bias through a series of examples. In particular, Fig. 4 showed how large the biases can be at small sample sizes, which makes bias correction particularly important in neuroscientific settings where sample sizes are often small.

We focused on Gaussian distributions, as they have historically been a starting point for many estimators (e.g., correlation is used as a measure of dependence, but zero correlation implies independence only in the Gaussian case). We were able to show ground-truth validation for our \(\sim_{G}\)-PID estimator at high dimensionalities only thanks to the existence of closed-form results on scalar Gaussians.

While our central claims and results applied to Gaussian distributions, our method for computing PIDs did not immediately break down for distributions close to Gaussian in some limit (e.g., Poisson). The effective spiking rate used in our multivariate Poisson distribution was also not particularly large (see App. D.1); we would expect our estimates to improve for higher firing rates, since the Poisson distribution will then be more Gaussian.

**Limitations.** Our work has several limitations that require further theory and simulations to resolve, the most important of which are: (1) Our estimator is technically a bound on the PID values because we assume Gaussian optimality in Definition 2; (2) Our bias-correction method is heuristic: we do not provide a rigorous theoretical characterization of the bias of PID values.

**Broader impacts.** Our work is mainly methodological, so the scope for negative impacts depends on how the methods might be used. For example, incorrect interpretations drawn from the use of our PID estimators may affect scientific conclusions. In particular, the PID is inherently a correlational quantity and carries the same caveats: it may not be appropriate to make causal interpretations on the basis of observed PID values. Also, despite our best efforts to explore a variety of systems, we cannot tell how accurate our bias-correction method will be in novel configurations.

## Acknowledgments and Disclosure of Funding

We thank Lukasz Kusmierz for providing a valuable reference on the bias of Shannon entropy estimates. We also thank Gabe Schamberg and Christof Koch for helpful discussions.

We wish to thank the Allen Institute founder, Paul G. Allen, for his vision, encouragement, and support. Praveen Venkatesh was supported by a Shanahan Family Foundation Fellowship at the Interface of Data and Neuroscience, supported in part by the Allen Institute. Stefan Mihalas was in part supported by NSF 2223725, NIH R01EB029813, and RF1DA055669 grants.

## References

* [1]S. E. de Vries, J. A. Lecoq, M. A. Buice, P. A. Groblewski, G. K. Ocker, M. Oliver, D. Feng, N. Cain, P. Ledochowitsch, D. Millman, et al. A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex (1):138-151, 2020.
* [2] Joshua H. Siegle, X. Jia, S. Durand, S. Gale, C. Bennett, N. Graddis, G. Heller, T. Ramirez, H. Choi, J. Harter A. Luviano, et al. Survey of spiking in the mouse visual system reveals functional hierarchy. _Nature_, 592(7852):86-92, 2021.
* [3] C. Stringer, M. Pachitariu, N. Steinmetz, Charu Bai Reddy, Matteo Carandini, and Kenneth D Harris. Spontaneous behaviors drive multidimensional, brainwide activity. _Science_, 364(6437):eaav7893, 2019.
* [4] Elad Schneidman, William Bialek, and Michael J Berry. Synergy, redundancy, and independence in population codes. _Journal of Neuroscience_, 23(37):11539-11553, 2003.
* [5] Itay Gat and N.Nafali Tishby. Synergy and redundancy among brain cells of behaving monkeys. _Advances in neural information processing systems_, 11, 1998.
* [6] Giuseppe Pica, Eugenio Pisani, Houman Safaai, Caroline Runyan, Christopher Harvey, Mathew Diamond, Christoph Kayser, Tommaso Fellin, and Stefano Panzeri. Quantifying how much sensory information in a neural code is relevant for behavior. _Advances in Neural Information Processing Systems_, 30, 2017.
* [7] Giuseppe Pica, Mohammadreza Soltanipour, and Stefano Panzeri. Using intersection information to map stimulus information transfer within neural networks. _BioSystems_, 185:104028, 2019.
* [8] Jan Bim, Vito De Feo, Daniel Chicharro, Malte Bieler, Ileana L. Hanganu-Opatz, Andrea Brovelli, and Stefano Panzeri. A non-negative measure of feature-related information transfer between neural signals. _BioRxiv_, page 758128, 2019.
* [9] Tomas Scagliarini, Luca Faes, Daniele Marinazzo, Sebastiano Stramaglia, and Rosario N Mantegna. Synergistic information transfer in the global system of financial markets. _Entropy_, 22(9):1000, 2020.
* [10] Patricia Wollstadt, Sebastian Schmitt, and Michael Wibral. A rigorous information-theoretic definition of redundancy and relevancy in feature selection based on (partial) information decomposition. _arXiv preprint arXiv:2105.04187_, 2021.
* [11] Sanghamitra Dutta, Praveen Venkatesh, Piotr Mardziel, Anupam Datta, and Pulkit Grover. An information-theoretic quantification of discrimination with exempt features. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 3825-3833, 2020.
* [12] Praveen Venkatesh and Gabriel Schamberg. Partial information decomposition via deficiency for multivariate Gaussians. In _2022 IEEE International Symposium on Information Theory (ISIT)_, pages 2892-2897. IEEE, 2022.
* [13] Adam B Barrett. Exploration of synergistic and redundant information sharing in static and dynamical Gaussian systems. _Physical Review E_, 91(5):052802, 2015.
* [14] Nigel Colenbier, Frederik Van de Steen, Lucina Q Uddin, Russell A Poldrack, Vince D Calhoun, and Daniele Marinazzo. Disambiguating the role of blood flow and global signal with partial information decomposition. _Neuroimage_, 213:116699, 2020.
* [15] Tjoerd W Boonstra, Luca Faes, Jennifer N Kerkman, and Daniele Marinazzo. Information decomposition of multichannel EMG to map functional interactions in the distributed motor system. _NeuroImage_, 202:116093, 2019.
* [16] Jana Krohova, Luca Faes, Barbora Czippelova, Zuzana Turianikova, Nikoleta Zagutova, Riccardo Pernice, Alessandro Busacca, Daniele Marinazzo, Sebastiano Stramaglia, and Michal Javorka. Multiscale information decomposition dissects control mechanisms of heart rate variability at rest and during physiological stress. _Entropy_, 21(5):526, 2019.

* Pakman et al. [2021] Ari Pakman, Amin Nejatbakhsh, Dar Gilboa, Abdullah Makkeh, Luca Mazzucato, Michael Wibral, and Elad Schneidman. Estimating the unique information of continuous variables. _Advances in neural information processing systems_, 34:20295-20307, 2021.
* Bertschinger et al. [2014] Nils Bertschinger, Johannes Rauh, Eckehard Olbrich, Jurgen Jost, and Nihat Ay. Quantifying unique information. _Entropy_, 16(4):2161-2183, 2014.
* Rauh et al. [2022] Johannes Rauh, Pradeep Kr Banerjee, Eckehard Olbrich, Guido Montufar, and Jurgen Jost. Continuity and additivity properties of information decompositions. _arXiv preprint arXiv:2204.10982_, 2022.
* Banerjee et al. [2018] Pradeep Kr Banerjee, Johannes Rauh, and Guido Montufar. Computing the unique information. In _2018 IEEE International Symposium on Information Theory (ISIT)_, pages 141-145. IEEE, 2018.
* Makkeh et al. [2018] Abdullah Makkeh, Dirk Oliver Theis, and Raul Vicente. BROJA-2PID: A robust estimator for bivariate partial information decomposition. _Entropy_, 20(4):271, 2018.
* Liang et al. [2023] Paul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard Chen, Zihao Deng, Faisal Mahmood, Ruslan Salakhutdinov, and Louis-Philippe Morency. Quantifying & modeling feature interactions: An information decomposition framework. _arXiv preprint arXiv:2302.12247_, 2023.
* Williams and Beer [2010] Paul L Williams and Randall D Beer. Nonnegative decomposition of multivariate information. _arXiv preprint arXiv:1004.2515_, 2010.
* Cover and Thomas [2012] Thomas M Cover and Joy A Thomas. _Elements of Information Theory_. John Wiley & Sons, 2012.
* Griffith and Koch [2014] Virgil Griffith and Christof Koch. Quantifying synergistic mutual information. In _Guided self-organization: inception_, pages 159-190. Springer, 2014.
* Harder et al. [2013] Malte Harder, Christoph Salge, and Daniel Polani. Bivariate measure of redundant information. _Physical Review E_, 87(1):012130, 2013.
* Kolchinsky [2022] Artemy Kolchinsky. A novel approach to the partial information decomposition. _Entropy_, 24(3):403, 2022.
* Lizier et al. [2018] Joseph T Lizier, Nils Bertschinger, Jurgen Jost, and Michael Wibral. Information decomposition of target effects from multi-source interactions: perspectives on previous, current and future work. _Entropy_, 20(4):307, 2018.
* Venkatesh et al. [2023] Praveen Venkatesh, Keerthana Gurushankar, and Gabriel Schamberg. Capturing and interpreting unique information. _arXiv preprint arXiv:2302.11873_, 2023.
* Banerjee et al. [2018] Pradeep Kr Banerjee, Eckehard Olbrich, Jurgen Jost, and Johannes Rauh. Unique informations and deficiencies. In _2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 32-38. IEEE, 2018.
* Tishby et al. [2000] Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. _arXiv preprint physics/0004057_, 2000.
* Chechik et al. [2003] Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss. Information bottleneck for Gaussian variables. _Advances in Neural Information Processing Systems_, 16, 2003.
* Globerson and Tishby [2004] Amir Globerson and Naftali Tishby. On the optimality of the Gaussian information bottleneck curve. _The Hebrew University of Jerusalem, Tech. Rep_, page 22, 2004.
* Riedmiller and Braun [1993] Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In _IEEE International Conference on Neural Networks_, pages 586-591. IEEE, 1993.
* Hinton [2018] Geoffrey Hinton. Coursera neural networks for machine learning, lecture 6, 2018. URL [https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf). Also see [https://optimization.cbe.cormell.edu/index.php?title=RMSProp](https://optimization.cbe.cormell.edu/index.php?title=RMSProp).
* [36] Code for this paper. URL [https://github.com/praveenv253/gpid](https://github.com/praveenv253/gpid).
* Paninski [2003] Liam Paninski. Estimation of entropy and mutual information. _Neural computation_, 15(6):1191-1253, 2003.
* Cai et al. [2015] T Tony Cai, Tengyuan Liang, and Harrison H Zhou. Law of log determinant of sample covariance matrix and optimal estimation of differential entropy for high-dimensional Gaussian distributions. _Journal of Multivariate Analysis_, 137:161-172, 2015.
* Wasserman [2004] Larry Wasserman. _All of statistics: a concise course in statistical inference_, volume 26. Springer, 2004.
* Timme and Lapish [2018] Nicholas M Timme and Christopher Lapish. A tutorial for information theory in neuroscience. _eneuro_, 5(3), 2018.
* [41] Allen Institute. Visual behavior neuropixels dataset overview, 2022. URL [https://portal.brain-map.org/explore/circuits/visual-behavior-neuropixels](https://portal.brain-map.org/explore/circuits/visual-behavior-neuropixels).
* Strassen [1969] Volker Strassen. Gaussian elimination is not optimal. _Numerische mathematik_, 13(4):354-356, 1969.

**Gaussian Partial Information Decomposition:**

**Bias Correction and Application to High-dimensional Data**

**Appendices**

Praveen Venkatesh, Corbett Bennett, Sam Gale, Tamina K. Ramirez, Gregory Heller,

**Severine Durand, Shawn Olsen, Stefan Mihalas**

## Appendix A Supplementary Material for Sections 2 and 3

### Implications of the Additivity Property

As mentioned in the main text, additivity states that the PID values of an isolated system should not depend on the PID values of another isolated system. Without additivity, it is not possible to examine independent systems in isolation, since broadening your view to include a different isolated system could change the PID values of the first system.

Hypothetically, suppose we have two separate individuals (labeled 1 and 2) receiving completely independent stimuli, and we examine the PID between the activity in brain regions \(M\), \(X\) and \(Y\) in each of their brains. Then the _total_ unique information that \(X_{1}\) and \(X_{2}\) have about \(M_{1}\) and \(M_{2}\) with respect to \(Y_{1}\) and \(Y_{2}\) should be equal to the _sum_ of the unique information in each of the individuals taken separately.

As another example, suppose we are trying to examine visual information flow and auditory information flow in a multi-sensory integration task. We may want to understand the degree to which the activity in the sub-regions of the visual and auditory systems depend on each other. A reasonable null model of independence between the two systems would be that the PID values of the joint system will be equal to the sum of the PID values in the two individual systems. Then, measuring the actual degree to which the joint PID value is not equal to the sum will be a meaningful measure of dependence between the systems. This would only be possible with a PID definition that guaranteed additivity of independent sub-systems. However, since the \(\delta\)-PID does not satisfy the additivity (Property 1), we cannot guarantee that the aforementioned null would be the correct null model, and we would not be able to perform such an analysis.

### Explaining the Exponential Reduction in the Number of Optimization Variables

In Def. 2, we restrict \(Q_{MXY}\) to be jointly Gaussian. This reduces the number of optimization variables from being exponential in the dimensionality to quadratic, as we show here.

Previous discrete \(\sim\)-PID estimators [20; 21] have found efficient methods to solve the optimization problem of Def. 1 without reducing the number of optimization variables. If \(P_{MXY}\) is discrete in Def. 1, and each dimension of \(M\), \(X\) and \(Y\) has a support of size \(K\) (i.e., \(M_{i}\), \(X_{i}\) and \(Y_{i}\) can each take one of \(K\) discrete values), then the total number of degrees of freedom in \(\Delta_{P}\) is \(\mathcal{O}(K^{(d_{M}+d_{X}+d_{Y})})\). This corresponds to the total support of a discrete \(Q_{MXY}\), which can be an arbitrarily complex discrete distribution.

In contrast, the \(\sim_{G}\)-PID in Def. 2 reduces the dimensionality of the optimization space by assuming that \(Q_{MXY}\) is Gaussian. \(Q_{MXY}\) is then completely parameterized by its covariance matrix, \(\Sigma_{MXY}^{Q}\). In fact, given the additional constraints in Def. 2, the only part of \(\Sigma_{MXY}^{Q}\) that is variable is \(\Sigma_{X,Y}^{Q}\), as explained in Sec. 3.2. This matrix has only \(d_{X}\times d_{Y}\) variables. Thus, by restricting the optimization space to jointly Gaussian distributions, we have reduced the number of variables from exponential in the dimensionality to quadratic.

As a concrete example, consider a neuroscientific setting (as in Sec. 6) where \(M\), \(X\) and \(Y\) represent the spiking activity of neurons in three different brain regions, so that \(d_{M}\), \(d_{X}\) and \(d_{Y}\) represent the number of neurons, and \(K\) represents the maximum number of spikes. Makkeh et al. [21] demonstrated their method for a maximum support size of \(K=18\) with \(d_{M}=d_{X}=d_{Y}=1\), for a total support of approximately \(18^{3}-1=5831\) in \(Q_{MXY}\). If we assume a single neuron can have at most \(K=10\) spikes, then we get an effective dimensionality of \(\log_{10}(5831)<4\). In other words,these discrete \(\sim\)-PID estimators can handle around 4 neurons across all three regions \(M\), \(X\) and \(Y\) (which is also what we use in the simulation of Fig. 5 in Sec. 6), whereas our \(\sim_{G}\)-PID estimator can handle several hundreds if not thousands of neurons (as seen in Example 10).

### Proofs of Propositions 1 and 2

Proof of Proposition 1.: Firstly, the differential entropy of a Gaussian random variable \(M\) with covariance matrix \(\Sigma_{M}\) is given by [43, Thm. 8.4.1]:

\[h(M)=\frac{1}{2}\log\det(2\pi e\Sigma_{M}). \tag{18}\]

Secondly, for a joint Gaussian distribution \(P_{MYY}\) parameterized by a covariance matrix \(\Sigma_{MXY}\), the conditional covariance matrix can be written as [44, Sec. 8.1.3]:

\[\Sigma_{XY|M} =\Sigma_{XY}-\Sigma_{XY,M}\Sigma_{M}^{-1}\Sigma_{XY,M}^{\mathsf{ T}} \tag{19}\] \[\Rightarrow\qquad\Sigma_{XY} =\Sigma_{XY|M}+\Sigma_{XY,M}\Sigma_{M}^{-1}\Sigma_{XY,M}^{\mathsf{ T}} \tag{20}\]

Using these two equations, we can derive the mutual information between \(M\) and \((X,Y)\) as follows:

\[I(M;(X,Y)) = h(X,Y)-h(X,Y\,|\,M) \tag{21}\] \[\stackrel{{(a)}}{{=}} \frac{1}{2}\log\det(2\pi e\Sigma_{XY})-\frac{1}{2}\log\det(2\pi e \Sigma_{XY|M})\] (22) \[= \frac{1}{2}\log\bigl{(}(2\pi e)^{d_{M}}\det(\Sigma_{XY})\bigr{)} -\frac{1}{2}\log\bigl{(}(2\pi e)^{d_{M}}\det(\Sigma_{XY|M})\bigr{)}\] (23) \[= \frac{1}{2}\log\biggl{(}\frac{\det(\Sigma_{XY})}{\det(\Sigma_{XY| M})}\biggr{)}\] (24) \[\stackrel{{(b)}}{{=}} \frac{1}{2}\log\biggl{(}\frac{\det(\Sigma_{XY|M}+\Sigma_{XY,M} \Sigma_{M}^{-1}\Sigma_{XY,M}^{\mathsf{T}})}{\det(\Sigma_{XY|M})}\biggr{)}\] (25) \[= \frac{1}{2}\log\biggl{(}\frac{\det(\Sigma_{XY|M})\det(I+\Sigma_{ XY|M}^{-1}\Sigma_{XY,M}\Sigma_{M}^{-1}\Sigma_{XY,M}^{\mathsf{T}})}{\det(\Sigma_{ XY|M})}\biggr{)}\] (26) \[= \frac{1}{2}\log\det(I+\Sigma_{XY|M}^{-1}\Sigma_{XY,M}\Sigma_{M}^{ -1}\Sigma_{XY,M}^{\mathsf{T}})\] (27) \[\stackrel{{(c)}}{{=}} \frac{1}{2}\log\det(I+\Sigma_{M}^{-1}\Sigma_{XY,M}^{\mathsf{T}} \Sigma_{XY|M}^{-1}\Sigma_{XY,M}), \tag{28}\]

where in (a) we used equation (18), in (b) we used equation (20), and in (c) we used the fact that \(\det(I+AB)=\det(I+BA)\).

The remainder of the proof follows from the arguments presented below equation (8). The constraint in Proposition 1 arises because, when optimizing over \(\Sigma_{X,Y|M}\), we require \(\Sigma_{MXY}\) to be a valid positive semidefinite covariance matrix, i.e., \(\Sigma_{MXY}\succcurlyeq 0\). This happens if and only if \(\Sigma_{M}\) and its Schur complement in \(\Sigma_{MXY}\) are both positive semidefinite, i.e., \(\Sigma_{M}\succcurlyeq 0\) and \(\Sigma_{M}-\Sigma_{M,XY}\Sigma_{XY}^{-1}\Sigma_{M,XY}^{\mathsf{T}}=\Sigma_{XY |M}\succcurlyeq 0\). 

Proof of Proposition 2.: The proof is divided into three parts consisting of derivations for the objective, the gradient and the projection operator.

**Objective.** After whitening the \(P_{X|M}\) and the \(P_{Y|M}\) channels, and assuming that \(\Sigma_{M}=I\) (see Remark 1), without loss of generality we have that

\[\Sigma_{X,M} =\mathbb{E}\bigl{[}(H_{X}M+N_{X})M^{\mathsf{T}}\bigr{]}=H_{X} \mathbb{E}[MM^{\mathsf{T}}]=H_{X} \tag{29}\] \[\Sigma_{XY|M} =\left[\begin{array}{cc}I&\Sigma_{X,Y|M}\\ \Sigma_{X,Y|M}^{\mathsf{T}}&I\end{array}\right]\] (30) \[\Rightarrow\quad\Sigma_{M}^{-1}\Sigma_{XY,M}^{\mathsf{T}}\Sigma_{ XY|M}^{-1}\Sigma_{XY,M} =\left[\begin{array}{cc}H_{X}^{\mathsf{T}}&H_{Y}^{\mathsf{T}}\end{array} \right]\left[\begin{array}{cc}I&\Sigma_{X,Y|M}\\ \Sigma_{X,Y|M}^{\mathsf{T}}&I\end{array}\right]^{-1}\left[\begin{array}{cc} H_{X}\\ H_{Y}\end{array}\right] \tag{31}\]For the sake of brevity, let \(\Sigma\) represent the optimization variable \(\Sigma_{X,Y|M}\), and let \(S\) be its Schur complement in \(\Sigma_{XY|M}\), \(I-\Sigma\Sigma^{\mathsf{T}}\). Then, the inverse of \(\Sigma_{XY|M}\) can be written as [44, Sec. 9.1.5]:

\[\Sigma_{XY|M}^{-1}=\left[\begin{array}{cc}S^{-1}&-S^{-1}\Sigma\\ -\Sigma^{\mathsf{T}}S^{-1}&I+\Sigma^{\mathsf{T}}S^{-1}\Sigma\end{array}\right] \tag{32}\]

Therefore, we get:

\[\Sigma_{M}^{-1}\Sigma_{XY,M}^{\mathsf{T}}\Sigma_{XY|M}^{-1}\Sigma_ {XY,M} =\left[\begin{array}{cc}H_{X}^{\mathsf{T}}&H_{Y}^{\mathsf{T}} \end{array}\right]\left[\begin{array}{cc}S^{-1}&-S^{-1}\Sigma\\ -\Sigma^{\mathsf{T}}S^{-1}&I+\Sigma^{\mathsf{T}}S^{-1}\Sigma\end{array}\right] \left[\begin{array}{c}H_{X}\\ H_{Y}\end{array}\right] \tag{33}\] \[=H_{Y}^{\mathsf{T}}H_{Y}+(H_{X}-\Sigma H_{Y})^{\mathsf{T}}S^{-1}( H_{X}-\Sigma H_{Y}) \tag{34}\]

Thus, setting \(B\coloneqq H_{X}-\Sigma H_{Y}\), the optimization problem in Proposition 1 reduces to

\[\min_{\Sigma} \frac{1}{2}\log\det\bigl{(}I+H_{Y}^{\mathsf{T}}H_{Y}+B^{\mathsf{ T}}S^{-1}B\bigr{)}\] (35) s.t. \[\Sigma_{XY|M}\succcurlyeq 0\]

Gradient.Let the objective derived in the previous section be called \(f(\Sigma)\), where \(\Sigma\coloneqq\Sigma_{X,Y|M}\) as before. We can compute the gradient of \(f\) with respect to \(\Sigma\) using standard identities from matrix calculus. First, note that the gradient of a scalar function with respect to a matrix is itself a matrix with entries as follows:

\[\nabla f(\Sigma)\Big{|}_{ij}=\frac{\partial f}{\partial\Sigma_{ij}}(\Sigma). \tag{36}\]

Considering each element of this matrix:

\[\frac{\partial f}{\partial\Sigma_{ij}}(\Sigma) =\left.\frac{1}{2}\frac{\partial}{\partial\Sigma_{ij}}\log\det(I+ H_{Y}^{\mathsf{T}}H_{Y}+B^{\mathsf{T}}S^{-1}B)\right|_{\Sigma} \tag{37}\] \[\stackrel{{(a)}}{{=}}\frac{1}{2}\mathrm{Tr}\Bigl{\{} (I+H_{Y}^{\mathsf{T}}H_{Y}+B^{\mathsf{T}}S^{-1}B)^{-1}\frac{\partial}{\partial \Sigma_{ij}}(I+H_{Y}^{\mathsf{T}}H_{Y}+B^{\mathsf{T}}S^{-1}B)\Bigr{\}}\Big{|}_{\Sigma}\] (38) \[\stackrel{{(b)}}{{=}}\frac{1}{2}\mathrm{Tr}\Bigl{\{} (I+H_{Y}^{\mathsf{T}}H_{Y}+B^{\mathsf{T}}S^{-1}B)^{-1}\frac{\partial}{\partial \Sigma_{ij}}(B^{\mathsf{T}}S^{-1}B)\Bigr{\}}\Big{|}_{\Sigma}, \tag{39}\]

where in (a), we have used the identity \(\partial\log\det(X)=\mathrm{Tr}\{X^{-1}\partial(X)\}\)[44, Sec. 2], while in (b), we use the fact that only \(B\) and \(S\) depend on \(\Sigma\) implicitly, with the other terms being constants.

Expanding the partial derivative alone, we get:

\[\frac{\partial}{\partial\Sigma_{ij}}(B^{\mathsf{T}}S^{-1}B)\Big{|}_{\Sigma}= \left[\frac{\partial}{\partial\Sigma_{ij}}(B^{\mathsf{T}})\!\cdot\!S^{-1}B\,+\,B ^{\mathsf{T}}\!\cdot\!\frac{\partial}{\partial\Sigma_{ij}}(S^{-1})\!\cdot\!B\,+ \,B^{\mathsf{T}}\!S^{-1}\!\cdot\!\frac{\partial}{\partial\Sigma_{ij}}(B) \right]_{\Sigma}, \tag{40}\]

wherein

\[\frac{\partial}{\partial\Sigma_{ij}}(B)\Big{|}_{\Sigma} =\frac{\partial}{\partial\Sigma_{ij}}(H_{X}-\Sigma H_{Y})\Big{|}_{\Sigma} \tag{41}\] \[\stackrel{{(b)}}{{=}}-J^{ij}H_{Y},\] (42) \[\frac{\partial}{\partial\Sigma_{ij}}(S^{-1})\Big{|}_{\Sigma} \stackrel{{(c)}}{{=}}-S^{-1}\frac{\partial S}{\partial \Sigma_{ij}}S^{-1}\Big{|}_{\Sigma}\] (43) \[=-S^{-1}\frac{\partial}{\partial\Sigma_{ij}}(I-\Sigma\Sigma^{ \mathsf{T}})S^{-1}\Big{|}_{\Sigma}\] (44) \[\stackrel{{(d)}}{{=}}-S^{-1}(-J^{ij}\Sigma^{\mathsf{T }}-\Sigma J^{ij}{}^{\mathsf{T}})S^{-1}, \tag{45}\]

where \(J^{ij}\) is the _single-entry matrix_, containing a 1 at location \((i,j)\) and 0's everywhere else; in (b) and (d), we use the fact that \(\partial X/\partial X_{ij}=J^{ij}\)[44, Sec. 9.7.6]; and in (c) we use the identity \(\partial(X^{-1})=X^{-1}\partial(X)X^{-1}\). Therefore, (40) becomes

\[\frac{\partial}{\partial\Sigma_{ij}}(B^{\mathsf{T}}S^{-1}B)\Big{|} _{\Sigma} \tag{46}\] \[\qquad=-(J^{ij}H_{Y})^{\mathsf{T}}S^{-1}B\;+\;B^{\mathsf{T}}S^{-1} (J^{ij}\Sigma^{\mathsf{T}}\:+\;\Sigma J^{ij\mathsf{T}})S^{-1}B\;+\;B^{\mathsf{ T}}S^{-1}(-J^{ij}H_{Y})\] (47) \[\qquad=-H_{Y}^{\mathsf{T}}J^{ij\mathsf{T}}S^{-1}B\;+\;B^{\mathsf{ T}}S^{-1}J^{ij}\Sigma^{\mathsf{T}}S^{-1}B\;+\;B^{\mathsf{T}}S^{-1}\Sigma J^{ ij\mathsf{T}}S^{-1}B\;-\;B^{\mathsf{T}}S^{-1}J^{ij}H_{Y}. \tag{48}\]

Putting it all together, and letting \(A\coloneqq(I+H_{Y}^{\mathsf{T}}H_{Y}+B^{\mathsf{T}}S^{-1}B)\), (39) becomes

\[\frac{\partial f}{\partial\Sigma_{ij}}(\Sigma) =\frac{1}{2}\mathrm{Tr}\Big{\{}A^{-1}\big{(}-H_{Y}^{\mathsf{T}}J^ {ij\mathsf{T}}S^{-1}B\;+\;B^{\mathsf{T}}S^{-1}J^{ij}\Sigma^{\mathsf{T}}S^{-1}B \tag{49}\] \[\qquad\qquad\qquad+\;B^{\mathsf{T}}S^{-1}\Sigma J^{ij\mathsf{T}} S^{-1}B\;-\;B^{\mathsf{T}}S^{-1}J^{ij}H_{Y}\big{)}\Big{\}}\] \[=\frac{1}{2}\Big{[}-\mathrm{Tr}\big{\{}A^{-1}H_{Y}^{\mathsf{T}}J^ {ij\mathsf{T}}S^{-1}B\big{\}}\;+\;\mathrm{Tr}\big{\{}A^{-1}B^{\mathsf{T}}S^{-1 }J^{ij}\Sigma^{\mathsf{T}}S^{-1}B\big{\}}\] \[\qquad\qquad+\;\mathrm{Tr}\big{\{}A^{-1}B^{\mathsf{T}}S^{-1} \Sigma J^{ij\mathsf{T}}S^{-1}B\big{\}}\;-\;\mathrm{Tr}\big{\{}A^{-1}B^{\mathsf{ T}}S^{-1}J^{ij}H_{Y}\big{\}}\Big{]}\] (50) \[\overset{(e)}{=}\frac{1}{2}\Big{[}-\mathrm{Tr}\big{\{}S^{-1}BA^{- 1}H_{Y}^{\mathsf{T}}J^{ij\mathsf{T}}\big{\}}\;+\;\mathrm{Tr}\big{\{}\Sigma^{ \mathsf{T}}S^{-1}BA^{-1}B^{\mathsf{T}}S^{-1}J^{ij}\big{\}}\] \[\qquad\qquad+\;\mathrm{Tr}\big{\{}S^{-1}BA^{-1}B^{\mathsf{T}}S^{- 1}\Sigma J^{ij\mathsf{T}}\big{\}}\;-\;\mathrm{Tr}\big{\{}H_{Y}A^{-1}B^{\mathsf{ T}}S^{-1}J^{ij}\big{\}}\Big{]}, \tag{51}\]

where in (e), we have used the fact that the trace of a matrix product is invariant under cyclic permutations of the matrices within the product.

Finally, using the fact that \(\mathrm{Tr}\{W^{\mathsf{T}}J^{ij}\}=\mathrm{Tr}\{WJ^{ij\mathsf{T}}\}=W_{ij}\) for any matrix \(W\)[44, Sec. 9.7.5],

\[\frac{\partial f}{\partial\Sigma_{ij}}(\Sigma) =\frac{1}{2}\Big{[}-2\big{(}S^{-1}BA^{-1}H_{Y}^{\mathsf{T}}\big{)} _{ij}\;+\;2\big{(}S^{-1}BA^{-1}B^{\mathsf{T}}S^{-1}\Sigma\big{)}_{ij}\Big{]} \tag{52}\] \[=\Big{[}S^{-1}BA^{-1}\big{(}B^{\mathsf{T}}S^{-1}\Sigma-H_{Y}^{ \mathsf{T}}\big{)}\Big{]}_{ij}\] (53) \[\Rightarrow\quad\nabla f(\Sigma) =S^{-1}BA^{-1}\big{(}B^{\mathsf{T}}S^{-1}\Sigma-H_{Y}^{\mathsf{T}} \big{)}\] (54) \[=S^{-1}B\big{(}I+H_{Y}^{\mathsf{T}}H_{Y}+B^{\mathsf{T}}S^{-1}B \big{)}^{-1}\big{(}B^{\mathsf{T}}S^{-1}\Sigma-H_{Y}^{\mathsf{T}}\big{)}. \tag{55}\]

**Projection operator.** Recall that the optimization variable, \(\Sigma\coloneqq\Sigma_{X,Y|M}\) is an off-diagonal block of \(\Sigma_{XY|M}\), which is the matrix upon which the constraint is defined:

\[\Sigma_{XY|M}=\left[\begin{array}{cc}I&\Sigma\\ \Sigma^{\mathsf{T}}&I\end{array}\right], \tag{56}\]

wherein the diagonal blocks are identity due to Remark 1. For the purposes of this section, let us suppose \(\Sigma_{XY|M}\) is a function of \(\Sigma\), \(\Sigma_{XY|M}\eqqcolon g(\Sigma)\), so that the constraint may be written as \(g(\Sigma)\succcurlyeq 0\). A suitable projection operator, therefore, will accept a value \(\Sigma_{0}\) (that may violate \(g(\Sigma_{0})\succcurlyeq 0\)) and find a point \(\Sigma^{proj}\) close to it that satisfies the constraint, i.e., \(g(\Sigma^{proj})\succcurlyeq 0\).

We do not find the "orthogonal" projection operator, which has the minimum distance \(\|\Sigma^{proj}-\Sigma_{0}\|\) in some norm. Instead, we propose a simple heuristic to find a \(\Sigma^{proj}\) which satisfies the constraint.

If \(\Sigma_{0}\) satisfies the constraint, then we are done, so let us assume that \(g(\Sigma_{0})\not\succcurlyeq 0\). Then, we can find a matrix \(\overline{\Sigma}_{XY|M}\) which is close to \(g(\Sigma_{0})\) and satisfies \(\overline{\Sigma}_{XY|M}\succcurlyeq 0\) as follows: let the eigenvalue decomposition of \(g(\Sigma_{0})\) be given by \(V\Lambda V^{\mathsf{T}}\), with \(\Lambda\coloneqq\mathrm{diag}(\lambda_{i})\) being the diagonal matrix consisting of its eigenvalues \(\lambda_{i}\). Then, since \(g(\Sigma_{0})\) is not positive semidefinite, \(\exists\;i\) s.t. \(\lambda_{i}<0\). We set \(\overline{\lambda}_{i}\coloneqq 0\) for all such \(i\); effectively, \(\overline{\lambda}_{i}=\max\{0,\lambda_{i}\}\;\forall\;i\). We then reconstruct the matrix using these "rectified" eigenvalues and set it to be \(\overline{\Sigma}_{XY|M}\coloneqq V\overline{\Lambda}V^{\mathsf{T}}\), where \(\overline{\Lambda}=\mathrm{diag}(\overline{\lambda}_{i})\).

Now, we need to find \(\Sigma\) such that \(g(\Sigma)=\overline{\Sigma}_{XY|M}\). However, \(\overline{\Sigma}_{XY|M}\) may not have identity matrices on its diagonal blocks, i.e., it might not correspond to a whitened channel. We therefore whiten \(\overline{\Sigma}_{XY|M}\) as follows:

\[\overline{\Sigma}_{XY|M}^{\textit{whitened}}=\left[\begin{array}{cc}\overline{ \Sigma}_{X|M}^{-1/2}&0\\ 0&\overline{\Sigma}_{Y|M}^{-1/2}\end{array}\right]\Sigma_{XY|M}\left[\begin{array} []{cc}\overline{\Sigma}_{X|M}^{-1/2}&0\\ 0&\overline{\Sigma}_{Y|M}^{-1/2}\end{array}\right], \tag{57}\]

where \(\overline{\Sigma}_{X|M}\) and \(\overline{\Sigma}_{Y|M}\) are the diagonal blocks of \(\overline{\Sigma}_{XY|M}\). Crucially, since the matrix multiplying \(\overline{\Sigma}_{XY|M}\) on either side is itself (the inverse square-root of) a covariance matrix (and hence positive semidefinite), \(\overline{\Sigma}_{XY|M}^{\textit{whitened}}\) is also positive semidefinite.

Now, the off-diagonal block of \(\overline{\Sigma}_{XY|M}^{\textit{whitened}}\) will satisfy \(g(\cdot)=\overline{\Sigma}_{XY|M}^{\textit{whitened}}\succcurlyeq 0\). This off-diagonal block forms the output of our projection operation and can be written as

\[\Sigma_{X,Y|M}^{proj}=\overline{\Sigma}_{X|M}^{-1/2}\overline{\Sigma}_{XY|M} \overline{\Sigma}_{Y|M}^{-1/2}, \tag{58}\]

which comes directly from equation (57). 

### Details of \(\sim_{G}\)-PID Optimization and RProp Implementation

The optimization problem for the \(\sim_{G}\)-PID, using projected gradient descent with RProp (mentioned in Section 3), is implemented as follows:

1. Let \(\Sigma\coloneqq\Sigma_{X,Y|M}\) be shorthand for the optimization variable, and let \(\mathrm{Proj}(\cdot)\) represent the projection operator defined in Prop. 2. Let \(\Sigma^{(i)}\) represent the value of \(\Sigma\) at iteration \(i\) of the optimization. Initialize \(\Sigma^{(0)}=\mathrm{Proj}(H_{X}H_{Y}^{+})\), where \(H_{Y}^{+}\) is the pseudoinverse of \(H_{Y}\).
2. Evaluate the objective and the gradient as defined in Prop. 2, at the current value of \(\Sigma^{(i)}\). Compute the sign of (each element of) the gradient, \[\psi(\Sigma^{(i)})\coloneqq\mathrm{Sgn}\big{(}\nabla f(\Sigma^{(i)})\big{)}.\] (59) When computing the objective and the gradient, add a small regularization term to the computation of \(S^{-1}\) (as defined in Prop. 2): \(S^{-1}=\big{(}(1+\epsilon)I-\Sigma\Sigma^{\mathsf{T}}\big{)}^{-1}\), where we take \(\epsilon=10^{-7}\).
3. Update: \[\Sigma^{(i+1)}=\mathrm{Proj}\big{(}\Sigma^{(i)}-\alpha^{i}\eta^{(i)}\odot\psi( \Sigma^{(i)})\big{)},\] (60) where \(\mathrm{Proj}\) is the projection operator defined in Prop. 2; \(\eta^{(i)}\) is a time-varying learning rate vector of the same dimension as \(\Sigma\), describing the learning rate for each element of \(\Sigma\); \(\odot\) represents an element-wise (or Hadamard) product between vectors; and \(\alpha\coloneqq 0.999\) is a constant, which when raised to the power of \(i\), imposes a slow overall decay of the learning rate to promote convergence. The matrix inverses embedded in the projection operator are also regularized by modifying Equation (58) as follows: \[\Sigma_{X,Y|M}^{proj}=\big{(}\gamma I+\overline{\Sigma}_{X|M}^{1/2}\big{)}^{- 1}\overline{\Sigma}_{XY|M}\big{(}\gamma I+\overline{\Sigma}_{Y|M}^{1/2}\big{)} ^{-1},\] (61) where \(\gamma\) is slowly increased from \(10^{-12}\), by a factor of 10 in each step, until \(g(\Sigma_{X,Y|M}^{proj})\succcurlyeq 0\).
4. \(\eta^{(0)}\) is initialized to \(10^{-3}\) and \(\eta^{(i)}\) is updated as follows: \[\eta^{(i+1)}=\eta^{(i)}\odot\beta^{-\psi(\Sigma^{(i+1)})\odot\psi(\Sigma^{(i) })},\] (62) where \(\beta\coloneqq 0.9\) is a constant that determines how fast the learning rate increases or decreases; and all operations are carried out element-wise. Note that when some element of the gradient changes in sign, that element of \(-\psi(\Sigma^{(i+1)})\odot\psi(\Sigma^{(i)})\) will be positive, resulting in a decrease in that element of \(\eta^{(i)}\). On the other hand, if the sign of some element of the gradient remains the same, then the learning rate for that 

### Computational Complexity of the \(\sim_{G}\)-PID

Suppose for simplicity that \(M\), \(X\) and \(Y\) all have the same dimensionality \(d\coloneqq d_{M}=d_{X}=d_{Y}\). Then, \(\Sigma_{MXY}\) and all its submatrices have a side of dimensionality \(\mathcal{O}(d)\). Thus, the computational complexity of each gradient descent iteration is determined by the complexity of the matrix operations in equations (10)-(13). These operations include matrix multiplication, matrix inverses, \(\log\det(\cdot)\) and eigenvalue decomposition, which have a worst case complexity of \(\mathcal{O}(d^{3})\).

Matrix multiplication can potentially be performed at lower complexity for large matrix sizes (e.g., see Strassen [42]), which has downstream implications for each of the other operations as well. However, for our purposes, we take the computational complexity of the objective, the gradient and the projection operator to all be \(\mathcal{O}(d^{3})\).

## Appendix B Supplementary Material for Section 4

First, observe that by subtracting equation (2) from equation (1), we have

\[\begin{split} I(M;(X,Y))-I(M;X)&=UI_{Y}+SI\\ \Rightarrow&\qquad I(M;Y\,|\,X)=UI_{Y}+SI.\end{split} \tag{63}\]

Similarly, subtracting equations (1) and (3), we get that \(I(M;X\,|\,Y)=UI_{X}+SI\). These two equations hold in general, and will be used in what follows.

### Details and Derivations for Examples in Section 4

**Example 2** (Pure uniqueness).: \[M \sim\mathcal{N}(0,1)\] (64) \[X =M+N_{X} N_{X},N_{Y} \sim\,\text{i.i.d.}\,\,\mathcal{N}(0,1)\] (65) \[Y =N_{Y} (N_{X},N_{Y})\,\,\perp\!\!\perp M\] (66)

_Derivation of PID values in Example 2._

\[Y\,\,\perp\!\!\perp\,M \Rightarrow\quad I(M;Y) =0 \tag{67}\] \[\Rightarrow\quad UI_{Y}+RI =0\] (68) \[UI_{Y},RI\geq 0 \Rightarrow\quad UI_{Y}=RI =0\] (69) \[\Rightarrow\qquad UI_{X} =I(M;X)\] (70) \[\Rightarrow\qquad SI =I(M;(X,Y))-I(M;X)\] (71) \[=I(M;X)-I(M;X)=0. \tag{72}\]

**Example 3** (Pure redundancy).: \[M \sim\mathcal{N}(0,1)\] (73) \[X =M+N_{X} N_{X} \sim\mathcal{N}(0,1)\] (74) \[Y =M+N_{X} N_{X}\,\,\perp\!\!\perp M\] (75)

_Derivation of PID values in Example 3._

\[I(M;X\,|\,Y)=0 \Rightarrow\quad UI_{X}+SI =0 \tag{76}\] \[I(M;Y\,|\,X)=0 \Rightarrow\quad UI_{Y}+SI =0\] (77) \[\Rightarrow\qquad RI =I(M;(X,Y))=I(M;X). \tag{78}\]

**Example 4** (Pure synergy).: \[M \sim\mathcal{N}(0,1)\] (79) \[X =M+N_{X} N_{X} \sim\mathcal{N}(0,\sigma^{2})\] (80)\[Y=N_{X} N_{X} \perp\!\!\!\perp M \tag{81}\]

_Derivation of PID values in Example 4._

\[Y\perp\!\!\!\perp M \Rightarrow I(M;Y)=0 \tag{82}\] \[\Rightarrow UI_{Y}+RI=0\] (83) \[UI_{Y},RI\geq 0 \Rightarrow UI_{Y}=RI=0\] (84) \[\Rightarrow UI_{X}=I(M;X)\] (85) \[\Rightarrow SI=I(M;(X,Y))-I(M;X)\] (86) \[=\infty-I(M;X)=\infty. \tag{87}\]

It should be noted that certain nuances have been omitted in discussing Examples 2-4 above. For instance, in Example 3, \(\Sigma_{XY|M}\) is rank deficient and hence non-invertible, which would be an issue when computing the objective in Equation (9). Also, in Example 4, \(I(M;(X,Y))=\infty\), however this could be corrected by adding some noise to either \(X\) or \(Y\) so that their difference is a noisy representation of \(M\).

**Example 5** (Unique and redundant information).: \[M \sim\mathcal{N}(0,1)\] (88) \[X =M+N_{X} N_{X} \sim\mathcal{N}(0,1) N_{X}\perp\!\!\!\perp M\] (89) \[Y =M+N_{X}+N_{Y}^{\prime} N_{Y}^{\prime} \sim\mathcal{N}(0,\sigma^{2}) N_{Y}^{\prime}\perp(N_{X},M)\] (90)

_Derivation of PID values in Example 5._ Essentially, \(X\) is a noisy representation of \(M\), while \(Y\) is a noisy representation of \(X\). Since \(M\)--\(X\)--\(Y\) forms a Markov chain, \(I(M;Y\,|\,X)=0\), and hence \(UI_{Y}=SI=0\). When \(\sigma^{2}=0\), this example reduces to Example 3 with only redundancy being present. For any finite non-zero value of \(\sigma^{2}\), both \(RI\) and \(UI_{X}\) are present and are non-zero. Since \(M\) is scalar, the redundancy for the \(\sim\)-PID is identical to the MMI-PID's redundancy [13]:

\[RI=\min\{I(M;X),I(M;Y)\}=I(M;Y), \tag{91}\]

since \(I(M;Y)<I(M;X)\), by the data processing inequality. At the limit when \(\sigma^{2}\to\infty\), \(I(M;Y)\to 0\), and therefore \(RI\to 0\), while \(UI_{X}\) will become equal to \(I(M;X)\). 

**Example 6** (Unique and synergistic information).: \[M \sim\mathcal{N}(0,1)\] (92) \[X =M+N_{X} N_{X},N_{Y} \sim\mathcal{N}(0,\sigma^{2}), (N_{X},N_{Y})\perp\!\!\!\perp M\] (93) \[Y =N_{Y} \mathrm{Corr}(N_{X},N_{Y}) =\rho\] (94)

_Derivation of PID values in Example 6._ When \(\rho=1\) and \(\sigma^{2}\to\infty\), this example reduces to Example 4, with only synergy being present. In general, \(Y\perp\!\!\!\perp M\), therefore, \(I(M;Y)=UI_{Y}+RI=0\), meaning \(UI_{Y}=RI=0\). For any finite value of \(\sigma^{2}\), \(X\) will have some unique information about \(M\) given by \(UI_{X}=I(M;X)>0\). Correspondingly,

Figure 7: Diagrams explaining Examples 8 and 9. See App. B.2 for details.

\(I(M;X\,|\,Y)-I(M;X)\). When \(\rho=0\), \(I(M;X\,|\,Y)=I(M;X)\) and therefore \(SI=0\). As \(\rho\to 1\), \(X-Y\to M\); so the total mutual information \(I(M;(X,Y))\to\infty\), driven by synergy growing unbounded, while the unique component remains finite at \(I(M;X)\). 

**Example 7** (Redundant and synergistic information).: \[M \sim\mathcal{N}(0,1)\] (95) \[X =M+N_{X} N_{X},N_{Y} \sim\mathcal{N}(0,1),\qquad(N_{X},N_{Y})\perp\!\!\!\perp M\] (96) \[Y =M+N_{Y} \operatorname{Corr}(N_{X},N_{Y}) =\rho\] (97)

Derivation of PID values in Example 7.: When \(\rho=1\), we once again reduce to Example 3 with only redundancy. When \(\rho<1\), we cannot infer the PID values using Equation (1) and non-negativity alone, since none of the individual mutual information values (or conditional mutual information values) go to zero. Instead, we can determine the redundancy using the MMI-PID since \(M\) is scalar. Note that \(I(M;X)\) and \(I(M;Y)\) are both equal by symmetry, and thus equal to \(RI\). This also implies that both \(UI_{X}\) and \(UI_{Y}\) must be equal to zero. As \(\rho\) reduces, the two channels \(X\) and \(Y\) have noisy representations of \(M\) with increasingly independent noise terms. Therefore, their average, \((X+Y)/2\) will be more informative about \(M\) than either one of them individually, meaning that \(X\) and \(Y\) jointly contain more information than any one individually. This extra information about \(M\) is synergistic, given by \(SI=I(M;X\,|\,Y)\), and increases as \(\rho\) decreases, attaining its maximum possible value at \(\rho=0\). 

### Diagrams Explaining Examples 8 and 9

Examples 8 and 9 can be understood diagrammatically as shown in Fig. 7(l) and Fig. 7(r), respectively. In both diagrams, we represent the two-dimensional plane describing \(M\), with axes \(m_{1}\) and \(m_{2}\). The colored vectors shown on this plane represent \(H_{X}\) and \(H_{Y}\), i.e., the gain with which \(X\) and \(Y\) represent each value of \(M\). For example, \(Y_{2}\) captures only \(M_{2}\), with a gain corresponding to its length. The gains are directly representative of the signal-to-noise ratio (and hence the amount of information) in each variable, since the noise in each variable is i.i.d., with unit variance. In Example 8, the gain in \(X_{1}\) is variable, while in Example 9, the angle at which \(X_{1}\) and \(X_{2}\) sample \(M_{1}\) and \(M_{2}\) is variable.

### Comparison of the \(\sim_{G^{*}}\) and \(\delta_{G}\)-PIDs in Example 8 at Higher Dimensionality

For completeness, we also present a comparison between the \(\sim_{G^{*}}\) and \(\delta_{G}\)-PIDs for Example 8 at a higher dimensionality of \(d=64\). The results can be found in Fig. 8, and show that both PIDs scale proportionally. We use a gain of \(\alpha=0.99\) in place of \(\alpha=1\), since a gain of unity causes certain matrices to become ill-conditioned.

Although the \(\delta_{G}\)-PID does not obey the additivity property in general (which is why it does not agree with the \(\sim_{G}\)-PID at gains of \(\alpha>1\)), it appears to double along with the \(\sim_{G}\)-PID (i.e., additivity across identical independent copies appears to hold, at least in this specific example).

Figure 8: A comparison of the \(\sim_{G^{*}}\) and \(\delta_{G^{*}}\)-PIDs at a dimensionality of \(d=64\). The plot is virtually identical to that in Fig. 2 (which uses \(d=2\)), except that the \(y\)-axis is scaled up by a factor of 32.

### Run Time Analysis of the \(\sim_{G}\)-PID in Example 10

In Fig. 9, we present a complete picture of the time taken to compute the \(\sim_{G}\)-PID, as a function of dimensionality \(d\), in Example 10. Note that we cannot compare with the \(\delta_{G}\)-PID beyond a dimensionality of \(d=64\), because the computation for the \(\delta_{G}\)-PID failed in our setup for \(d>64\). Neither the \(\delta_{G}\)-PID nor the \(\sim_{G}\)-PID were carefully profiled and optimized, therefore, the runtime analysis in both Fig. 9 and Fig. 3 should be considered preliminary. We leave a more thorough comparison of the run time of both these methods to future work.

### Absolute and Relative Errors in Example 10

Figure 10 shows how the absolute and relative errors in PID values scale with increasing dimensionality in Example 10. The absolute errors increase in proportion to dimensionality, starting under

Figure 10: Absolute (top) and relative (bottom) errors in computed PID values from Example 10.

Figure 9: A plot showing the time taken to compute the \(\sim_{G}\)-PID at different dimensionalities in Example 10. This plot extends what is shown in the right-most plot of Fig. 3 for the \(\sim_{G}\)-PID.

at \(d=2\) and remaining under \(10^{-4}\) at \(d=1024\). The relative errors are all roughly constant, and remain under \(10^{-6}\).

## Appendix C Supplementary Material for Section 5

### Implementation Details for Bias-correction

We use a number of different setups based on sampling from random connectivity matrices for bias correction in Section 5. All of these setups assume that \(d_{X}=d_{Y}\).

The **both-unique, fully-redundant** and **high-synergy** setups have the following in common:

\[\Sigma_{M} =I \tag{98}\] \[\Sigma_{X|M} =I\] (99) \[\Sigma_{Y|M} =I\] (100) \[\Sigma_{MXY} =\left[\begin{array}{ccc}I&H_{X}^{\mathsf{T}}&H_{Y}^{\mathsf{T} }\\ H_{X}&H_{X}H_{X}^{\mathsf{T}}+\Sigma_{X|M}&H_{X}H_{Y}^{\mathsf{T}}+\Sigma_{W}\\ H_{X}&H_{Y}H_{X}^{\mathsf{T}}+\Sigma_{W}^{\mathsf{T}}&H_{Y}H_{Y}^{\mathsf{T}}+ \Sigma_{Y|M}\end{array}\right]. \tag{101}\]

Also, the elements of \(H_{X}\) are either zero or one, \(H_{X}(i,j)\sim\) i.i.d. Ber(0.1). These three setups differ in their definitions of \(H_{Y}\) (the channel gain from \(M\) to \(Y\)) and \(\Sigma_{W}\) (which controls the extent of correlation between \(X\) and \(Y\)).

The **both-unique** setup draws \(H_{Y}(i,j)\sim\) i.i.d. Ber(0.1), with all elements of \(H_{Y}\) independent of the elements of \(H_{X}\), and sets \(\Sigma_{W}=0\).

The **fully-redundant** setup is similar to Example 7, by setting \(H_{Y}=H_{X}\) and \(\Sigma_{W}=0.9I\) (note that \(\Sigma_{W}\) is square, since \(d_{X}=d_{Y}\)). By keeping \(\Sigma_{W}\) close to the identity matrix, we are effectively in the regime with high correlation \(\rho\) in Example 7. This allows us to come close to emulating Example 3, without suffering from the issue of non-invertibility of \(\Sigma_{XY|M}\), mentioned in App. B.

The **high-synergy** setup is similar to Example 6, by setting \(H_{Y}=0\) and \(\Sigma_{W}=0.8I\). As with the fully-redundant setup, by keeping \(\Sigma_{W}\) close to the identity matrix, we are in the high-\(\rho\) regime. This allows us to come close to emulating Example 4, while not making the synergy or the total mutual information infinite.

The **zero-synergy** setup is similar to Example 5, and uses the following setup:

\[\Sigma_{M} =\Sigma_{X|M}=I \tag{102}\] \[\Sigma_{X} =H_{X}H_{X}^{\mathsf{T}}+\Sigma_{X|M}\] (103) \[\Sigma_{Y|X} =I\] (104) \[\Sigma_{MXY} =\left[\begin{array}{ccc}I&H_{X}^{\mathsf{T}}&H_{Y}^{\mathsf{T }}\\ H_{X}&\Sigma_{X}&\Sigma_{X}H_{Y}^{\mathsf{T}}\\ H_{X}&H_{Y}^{\mathsf{T}}\Sigma_{X}^{\mathsf{T}}&H_{Y}^{\prime}\Sigma_{X}H_{Y}^{ \prime\mathsf{T}}+\Sigma_{Y|X}\end{array}\right]. \tag{105}\]

Here, \(H_{X}(i,j)\sim\) i.i.d. Ber(0.1), while \(H_{Y}=H_{Y}^{\prime}H_{X}\), with \(H_{Y}^{\prime}(i,j)\sim\) i.i.d. Ber(0.1), \(H_{Y}^{\prime}\perp\!\!\!\!\!\perp H_{X}\). Defined this way, \(M\)--\(X\)--\(Y\) form a Markov chain, ensuring that \(I(M;X\,|\,Y)=0\), so that \(SI=0\) (Refer equation (63)).

The **bit-of-all** setup is a combination of equal parts of the high-synergy and zero-synergy setups. The variables \(X\) and \(Y\) are swapped in the zero-synergy setup, so that both \(X\) and \(Y\) can have some unique information.

**Remark 3** (Rectification).: In practice, we observed that the bias correction procedure prescribed in Definition 4 could lead to negative values for certain PID quantities. This occurred because the bias-corrected union information was not guaranteed to satisfy certain bounds, which we enforce below. To prevent the occurrence of negative PID values after bias-correction, we require a form of rectification:

\[\widetilde{I_{G}^{O}}\Big{|}_{\text{rect}}^{(1)}\coloneqq\max\Bigl{\{} \widetilde{I_{G}^{C}}|_{\text{bias-corr}}\,\ \hat{I}(M;X)\big{|}_{\text{bias-corr}}\,\ \hat{I}(M;Y)\big{|}_{\text{bias-corr}}\Bigr{\}} \tag{106}\]\[\widetilde{I}_{G}^{\widetilde{\cup}}\big{|}_{\text{rect}}^{(2)}\coloneqq\min\Bigl{\{} \widetilde{I}_{G}^{\widetilde{\cup}}|_{\text{neck}}^{(1)}\,,\;\hat{I}(M;X)\big{|} _{\text{bias-corr}}+\hat{I}(M;Y)\big{|}_{\text{bias-corr}}\,,\;\hat{I}(M;(X,Y)) \big{|}_{\text{bias-corr}}\Bigr{\}}, \tag{107}\]

where \(\hat{I}(\cdot)|_{\text{bias-corr}}\) represents a bias-corrected mutual information estimate. After the second rectification equation above, the union information is bounded from below by the individual (bias-corrected) mutual information values, and bounded from above by the sum of the individual mutual information values, and by the total mutual information. 

### Bias-correction Performance in Additional Setups and at Higher Dimensionality

Plots showing bias correction performance for all setups described in App. C.1 are shown in Fig. 11 for 10-dimensional, and in Fig. 12 for 20-dimensional \(M\), \(X\) and \(Y\).

Of the setups we examine, only the case with both \(X\) and \(Y\) having purely unique information appears to have somewhat poor performance, where our bias-correction method appears to over-correct the bias in unique information, while insufficiently correcting the bias in redundancy and synergy.

### A Preliminary Analysis of the Variance of PID Estimates

In Figures 13 and 14, we present a preliminary analysis of the variance of our PID estimates using bootstrap. The figures represent the true distribution of the PID estimates over multiple sample draws, or over multiple bootstrap sample draws, in the form of box plots. In what follows, we colloquially refer to these box plots as "confidence intervals". The true "confidence intervals" were estimated using 100 runs of bias-corrected PID estimates, i.e., by drawing 100 different samples, each of size \(n\). The bootstrap "confidence intervals" were estimated using 100 bootstrap samples that were _resampled_ from a _single_ randomly drawn sample of size \(n\).

When correcting for bias in the PID estimates on bootstrap samples, we use the number of _unique_ data points in each bootstrap sample in place of \(n\) (refer Corollary 4), rather than the total sample size. This leads to more stable bootstrap-PID estimates.

The quality of the bootstrap "confidence interval" is affected greatly by the quality of the individual sample used for bootstrap resampling. Nevertheless, we observe a reasonable degree of qualitative agreement between the true "confidence interval" and the bootstrap "confidence interval", particularly as the sample size increases. Future work will assess confidence intervals with greater care, using well-defined metrics, and assess how well these confidence intervals are calibrated.

## Appendix D Supplementary Material for Section 6

### Details Regarding the Multivariate Poisson Spike-count Simulation

We follow our previous paper [12], where this analysis was first presented. In this simulation, \(M\) is two-dimensional, consisting of two independent and identically distributed Poisson random variables, \(M_{1}\) and \(M_{2}\). \(X\) and \(Y\) are each generated through a linear combination of Binomially thinning \(M_{1}\) and \(M_{2}\), along with some Poisson noise:

\[M_{1},M_{2} \sim\operatorname{Poiss}(2) \tag{108}\] \[X \sim\operatorname{Binom}(M_{1},\alpha)+\operatorname{Binom}(M_{ 2},0.5)+\operatorname{Poiss}(1)\] (109) \[Y \sim\operatorname{Binom}(M_{1},0.5)+\operatorname{Binom}(M_{2},0. 5)+\operatorname{Poiss}(1) \tag{110}\]

While our results in Fig. 5 show that our \(\sim_{G}\)-PID estimator comes closest to obtaining the same values as the estimator of Banerjee et al. [20], several remarks are warranted:

1. The method of Banerjee et al. [20] estimates the \(\sim\)-PID and not the \(\delta\)-PID. Since there is no discrete estimator for the \(\delta\)-PID, it is entirely possible that the \(\delta_{G}\)-PID is equally (or more) accurate with respect to its own true value. That is, the difference between the \(\delta_{G}\)-PID and the Banerjee et al. [20] estimator is entirely due to a difference in definitions;
2. We stated in the introduction that the \(\delta_{G}\)-PID estimates an _approximate_ upper bound, whereas our \(\sim_{G}\)-PID estimates an _exact_ upper bound. The difference due to the approximate nature of the \(\delta_{G}\)-PID upper bound is also not captured here, nor in Examples 8 and 9. In all these examples, we cannot tell how much of the difference is due to the difference in definitions and the approximate nature of the estimators.

Figure 11: Bias correction for various setups described in App. C.1 with \(d\coloneqq d_{M}=d_{X}=d_{Y}=10\).

Figure 12: Bias correction for various setups described in App. C.1 with \(d\coloneqq d_{M}=d_{X}=d_{Y}=20\).

Figure 13: Bootstrap “confidence intervals” for various setups described in App. C.1 with \(d\coloneqq d_{M}=d_{X}=d_{Y}=10\). Note that these are not true confidence intervals, but box-plot representations of the true variance of the estimator (over 100 runs) and the bootstrap estimate of the estimate’s variance (from 100 bootstrap resamplings of a single random sample).

Figure 14: Bootstrap “confidence intervals” for various setups described in App. C.1 with \(d\coloneqq d_{M}=d_{X}=d_{Y}=20\). Note that these are not true confidence intervals, but box-plot representations of the true variance of the estimator (over 100 runs) and the bootstrap estimate of the estimate’s variance (from 100 bootstrap resamplings of a single random sample).

3. Finally, we do not know how accurate the "ground truth" itself is, since this is also assessed using the estimator of Banerjee et al. [20]. Differences between the various estimators could also be due to a poor estimate produced by the discrete PID estimator.

### Additional Simulations on Non-Gaussian Data

In order to evaluate how the \(\sim_{G}\)-PID performs on a greater variety of non-Gaussian distributions, we considered three more setups similar to the one described in App. D.1.

The first additional setup is a multivariate Binomial distribution:

\[M_{1},M_{2} \sim\mathrm{Binom}(4,0.5) \tag{111}\] \[X \sim\mathrm{Binom}(M_{1},\alpha)+\mathrm{Binom}(M_{2},0.5)+ \mathrm{Binom}(2,0.5)\] (112) \[Y \sim\mathrm{Binom}(M_{1},0.5)+\mathrm{Binom}(M_{2},0.5)+\mathrm{ Binom}(2,0.5) \tag{113}\]

This setup has the same mean as the multivariate Poisson distribution, but has a smaller variance and is therefore more peaked, further from zero. The multivariate Binomial distribution is closer to Gaussian than the multivariate Poisson, and indeed, we see good agreement between the \(\sim_{G}\)-PID and the "ground truth", as assessed by the estimator of Banerjee et al. [20] (see Fig. 15, top panels).

The next two setups are "zero-inflated" versions of the multivariate Poisson and Binomial distributions, which are produced by passing each of the aforementioned variables through a "Z-channel", with zero-out probability 0.3. That is,

\[M_{1}^{\prime} =M_{1}\times Z_{M_{1}} X^{\prime} =X\times Z_{X} \tag{114}\] \[M_{2}^{\prime} =M_{2}\times Z_{M_{2}} Y^{\prime} =Y\times Z_{Y} \tag{115}\]

where \(Z_{i}\sim\) i.i.d. \(\mathrm{Ber}(0.7)\), \(i\in\{M_{1},M_{2},X,Y\}\). The primed versions of these variables have an additional probability mass at zero, with a probability of 0.3. This is said to better reflect the statistics of Calcium imaging [45], although we use a different base distribution than the one in the cited paper. The zero-inflated versions of the Poisson and Binomial distributions are bimodal, with one of the peaks at zero, and the Binomial version having more well separated peaks. For both zero-inflated versions, the \(\sim_{G}\)-PID performs poorly at recovering the absolute \(\sim\)-PID values (Fig. 15, middle-left and bottom-left panels). However, the relative PID values (normalized by the total mutual information) are closer to their true values (Fig. 15, middle-right and bottom-right panels), providing hope that a better mutual information estimate (e.g., [46, 47]) could correct the absolute PID values to some extent.

Further validation on non-Gaussian data is necessary to understand the impact of non-Gaussianity on \(\sim_{G}\)-PID estimates. However, a more extensive evaluation of non-Gaussian distributions (especially at higher dimensionalities) is made difficult by the unavailability of ground truth. The "ground truth" in these examples is obtained using the discrete \(\sim\)-PID estimator of Banerjee et al. [20] (which we assume is more accurate for discrete variables). Our method provides a better result than the \(\delta\)-PID and the MMI-PID in Fig. 5, at least part of which is probably due to the difference in definitions. Nevertheless, we believe these examples demonstrate that applying our method to non-Gaussian data need not be ruled out, although care should always be taken in interpreting results.

### Implementation Details of the Analysis Pipeline

The Visual Behavior Neuropixels data was analyzed as follows:

1. We selected mice that had at least 20 units in each brain region of interest. Only mice with both familiar and novel sessions were selected.
2. From each region we selected units of 'good' quality, with SNR at least 1, and with fewer than 1 inter-spike interval violations.
3. Trials were aligned to the start of each stimulus flash, and spikes were counted in bins of 50 ms, between 0 and 250 ms after stimulus onset (0-50 ms, 50-100 ms, etc.).
4. Trials corresponding to a non-change flash were defined as those that occurred between 4 and 10 flashes after the start of a behavioral trial, such that the image remained the same as the original image in this behavioral trial. Flashes corresponding to an omission, flashes after an omission, and flashes during which the animal lacked, were all removed. Only flashes that occurred while Figure 15: PID values and normalized PID values for three different non-Gaussian distributions, as described in App. D.2. The top, middle and bottom panels are respectively for a multivariate Binomial, a zero-inflated Poisson, and a zero-inflated Binomial distribution. The left column represents the PID values in bits, while the right column represents the PID values normalized by the total mutual information. All values are plotted as a function of the gain from \(M_{1}\) to \(X\) (\(\alpha\)) on the \(x\)-axis. An explanation of the results can be found in the text.

[MISSING_PAGE_EMPTY:30]

[MISSING_PAGE_EMPTY:31]

the animal was engaged (as measured by an average reward rate of at least 2 rewards/min) were selected.
5. Trials corresponding to a change flash were defined as those during which an image change occurred, and the animal was engaged (as above).
6. The top 10 or 20 principal components of neural activity were selected at each time bin, and for each brain region under consideration. Principal component analysis was carried out using the Scikit-learn [48] package in Python.
7. \(\sim_{G}\)-PID estimates were computed on the covariance matrix between principal components across regions, for each time bin.
8. Data were aggregated across 40 or 41 mice for the figures with VISal, and across 38 mice for the figures with VISam.
9. Statistical significance was assessed using a two-sided unpaired Mann-Whitney-Wilcoxon test.

### Additional Results

Fig. 16 shows the redundancy about VISp activity, between VISl and VISal. It is identical to Fig. 6, except that it uses only the top-10 PCA components in each of the three visual cortical regions. This figure shows that the result presented in Fig. 6 gracefully degrades with a reduction in the number of PCA components. When using fewer PCA components, the increase in redundancy on change trials is still statistically significant, but the effect size is not as large, and not as sustained, as it is in Fig. 6.

Fig. 17 shows the redundancy about VISp activity between VISl and VISam, the latter of which is a different higher-order cortical region (see, e.g., [49]). This figure shows that the result in Fig. 6 can also be seen for another choice of higher-order cortical region (i.e., VISam rather than VISal).

Figures 18 and 19 show all PID components, not just the redundancy, for the same settings as in Figures 6 and 17 respectively. These show that redundancy is the dominant partial information component, and appears to be the main driver of changes in the overall mutual information. This justifies why we include only a plot of redundancy in Figs. 6, 16 and 17.

### The Necessity for PCA, and Maximizing Dimensionality

In our analysis of real neural data in Sec. 6, we used principal components analysis to reduce dimensionality, despite the fact that our method could scale to much larger dimensionalities. As mentioned briefly in Sec. 6, this was due to the limited number of trials we had. For many mice, there were a larger number of neurons in each region than trials with which to compute the covariance matrix. In other words, directly computing the covariance of the neural activity would have resulted in a covariance matrix that was rank-deficient. We used PCA to reduce dimensionality and ensure that we obtained a reasonable estimate of the covariance matrix, and to minimize the error in our PID estimate, which would naturally be higher with a greater number of PCA dimensions.

For completeness, we analyze here what would happen if we used the maximum possible number of PCA components for each mouse, while ensuring that the covariance matrix is not rank deficient. We also ensure that we had the same number of components across all regions and across both change and non-change conditions (within each mouse), so as to perform a fair comparison. This gives us on average 53\(\pm\)16 PCA components, with a minimum of 22 and a maximum of 84 PCA components. We find that the results of higher and more sustained redundancy on change flashes continues to hold when using the maximum possible number of PCA components for each mouse (see Fig. 20). However, as expected, there is much greater variance across mice, possibly due to variability in the number of PCA components chosen across mice, and due to greater errors in our PID estimates. For the main results presented in Fig. 6, in order to be consistent across mice, we rounded down from the minimum and used the top 20 PCA components for all mice.

### Differences between Change and Non-change Conditions are not an Artifact of Bias-correction

The number of trials corresponding to change flashes is much smaller than the number of trials corresponding to non-change flashes. Accordingly, the sample size used to estimate the covariance matrix is different in each of the two conditions. Bias correction was performed using the appropriate Figure 21: Redundancy about VISp activity between VISl and VISal, as a function of time, for flashes corresponding to an image change (blue) and flashes corresponding to a non-change (orange) with an equal number of samples. Data points are across 41 mice. The plot on the left shows the raw redundancy in bits, while the plot on the right shows the redundancy normalized by the total mutual information. The observations made in the other figures continue to hold; the differences seen between the two conditions are, therefore, not a result of differences in sample size.

Figure 20: Redundancy about VISp activity between VISl and VISal, using the maximum possible number of PCA components for each mouse. Data points are across 42 mice, however, not all mice are represented at all points in time and for both conditions, since individual data points may not have converged due to ill-conditioned matrices. The plot on the left shows the raw redundancy in bits, while the plot on the right shows the redundancy normalized by the total mutual information. The increase in redundancy on change trials is still statistically significant, however, the errorbars have a greater spread for reasons described in the text.

sample size; however, as noted in Section 5, our bias correction process is not perfect, and may leave some residual bias.

In order to show that the results we observed were not an artifact of differences in residual bias caused by different sample sizes, we randomly subsampled the non-change flashes to produce a dataset with equal numbers of trials for change and non-change flashes. Repeating the analysis as before, we found that our conclusions continued to hold even in the setting where both conditions have equal sample sizes, as shown in Figure 21.

### A Note on Inhomogeneous Poisson Processes

Inhomogeneous Poisson processes are those whose mean firing- (or "emission-") rates change with time. Our method for estimating the PID does not consider the temporal characteristics of the original signal. Rather, the data analyst can choose the random variables \(M\), \(X\) and \(Y\) to span some time range (or different time ranges) as they please.

In our analysis of real neural data in Sec. 6, we counted the number of spikes in a 50-125 ms window after stimulus onset. Even if the underlying spiking process was an inhomogeneous Poisson process, this spike count would be Poisson distributed, with a mean given by the integral of the emission rate over the fixed window. In general, while analyzing data and computing PID values, one will have to be aware of severe inhomogeneities (e.g., if the distribution changes not just in the same way in each trial, but changes across trials), and account for them separately. For example, in our analysis, we excluded time periods when the mice were not "engaged" in the task, as defined by not actively consuming rewards at a rate of at least 2 rewards per minute.

## Appendix E Compute Configuration Used and Code Availability

All analyses were performed on a workstation equipped with an Intel Core i7-10700KF CPU with 8 cores (16 threads), 48 GiB of RAM and data stored on a 1 TB PCIe NVMe solid state drive.

Analysis of the Visual Behavior Neuropixels data (for 84 sessions) with \(d=20\) took approximately 9 minutes to run. This included loading data for each session and computing 840 PID values on \(60\times 60\) covariance matrices, implying an average run-time of about 0.64s for each PID estimate (including amortized data-load time).

All code used to compute and estimate the \(\sim_{G}\)-PID and correct for bias, including all examples in this paper and code for neural data analysis, is available on GitHub [36].

## Additional References

* [43] Thomas M Cover and Joy A Thomas. _Elements of Information Theory_. John Wiley & Sons, 2012.
* [44] K. B. Petersen and M. S. Pedersen. _The Matrix Cookbook_. Technical University of Denmark, 2012. URL [http://www2.compute.dtu.dk/pubdb/pubs/3274-full.html](http://www2.compute.dtu.dk/pubdb/pubs/3274-full.html). Version: November 15, 2012.
* [45] Xue-Xin Wei, Ding Zhou, Andres Grosmark, Zaki Ajabi, Fraser Sparks, Pengcheng Zhou, Mark Brandon, Attila Losonczy, and Liam Paninski. A zero-inflated gamma model for post-deconvolved calcium imaging traces. _Neurons, Behavior, Data Analysis, and Theory_, 3(2):1-21, 2020.
* [46] Alexander Kraskov, Harald Stosbauer, and Peter Grassberger. Estimating mutual information. _Physical review E_, 69(6):066138, 2004.
* [47] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In _International conference on machine learning_, pages 531-540. PMLR, 2018.
* [48] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* [49] Joshua H Siegle, Xiaoxuan Jia, Severine Durand, Sam Gale, Corbett Bennett, Nile Graddis, Gregory Heller, Tamina K Ramirez, Hannah Choi, Jennifer A Luviano, et al. Survey of spiking in the mouse visual system reveals functional hierarchy. _Nature_, 592(7852):86-92, 2021.