# Geometry-Aware Adaptation for Pretrained Models

Nicholas Roberts, Xintong Li, Dyah Adila, Sonia Cromp,

**Tzu-Heng Huang, Jitian Zhao, Frederic Sala**

University of Wisconsin-Madison

{nicki1iroberts, fredsala}@cs.wisc.edu

{xli2224, adila, cromp, thuang273, jzhao326}@wisc.edu

###### Abstract

Machine learning models--including prominent zero-shot models--are often trained on datasets whose labels are only a small proportion of a larger label space. Such spaces are commonly equipped with a metric that relates the labels via distances between them. We propose a simple approach to exploit this information to adapt the trained model to reliably predict new classes--or, in the case of zero-shot prediction, to improve its performance--without any additional training. Our technique is a drop-in replacement of the standard prediction rule, swapping \(\arg\max\) with the Frechet mean. We provide a comprehensive theoretical analysis for this approach, studying (i) learning-theoretic results trading off label space diameter, sample complexity, and model dimension, (ii) characterizations of the full range of scenarios in which it is possible to predict any unobserved class, and (iii) an optimal active learning-like next class selection procedure to obtain optimal training classes for when it is not possible to predict the entire range of unobserved classes. Empirically, using easily-available external metrics, our proposed approach, Loki, gains up to 29.7% relative improvement over SimCLR on ImageNet and scales to hundreds of thousands of classes. When no such metric is available, Loki can use self-derived metrics from class embeddings and obtains a 10.5% improvement on pretrained zero-shot models such as CLIP.

## 1 Introduction

The use of pretrained models is perhaps the most significant recent shift in machine learning. Such models can be used out-of-the-box, either to predict classes observed during pretraining (as in ImageNet-trained ResNets [14]) or to perform zero-shot classification on any set of classes [32]. While this is exciting, label spaces are often so huge that models are unlikely to have seen _even a single point_ with a particular label. Without additional modification, pretrained models will simply fail to reliably predict such classes. Instead, users turn to fine-tuning--which requires additional labeled data and training cycles and so sacrifices much of the promise of zero-shot usage.

How can we adapt pretrained models to predict new classes, without fine-tuning or retraining? At first glance, this is challenging: predictive signal comes from labeled training data. However, _relational_ information between the classes can be exploited to enable predicting a class even when there are no examples with this label in the training set. Such relational data is commonly available, or can be constructed with the help of knowledge bases, ontologies, or powerful foundation models [39].

How to best exploit relational structure remains unclear, with a number of key challenges: We might wish to know what particular subset of classes is rich enough to enable predicting many (or all) remaining labels. This is crucial in determining whether a training set is usable or, even with the aid of structure, insufficient. It is also unclear how approaches that use relational information interact with the statistical properties of learning, such as training sample complexity. Finally, performing adaptation requires an efficient and scalable algorithm.

This work addresses these challenges. It proposes a simple and practical approach to learning in structured label spaces, with theoretical guarantees. First, we offer a simple way to translate the soft outputs (i.e., probability vectors) produced by _any_ supervised learning model into a more general model that can exploit geometric information for label structure. In other words, our approach, called Loki,1 is a simple _adaptor_ for pretrained models. Loki can be applied via a fixed linear transformation of the model outputs. Loki's simplicity makes it applicable to a broad range of settings while enabling very high-cardinality predictions subject to a potentially small model output budget--we provide a visualization of this key idea in Figure 1.

Footnote 1: Refers to the ‘locus’ (plural: _loci_) of the Fréchet mean.

Theoretically, we provide a rich set of results for the metric-based adaptation setting. First, we introduce a learning-theoretic result in terms of the sample complexity of the pretrained model. It captures a key tradeoff between the number of classes and metric structure, the problem dimensionality, and the number of samples used to train the model prior to adaptation. Next we exhaustively study the properties of training sets, determining for a wide class of relational structures the minimum number (and structure) of subsets that enable reliable prediction. Finally we show how to exploit this result in an active learning-like approach to selecting points that will improve deficient training datasets.

Experimentally, we show that using structural information improves prediction in high-cardinality settings. We demonstrate the strength of the active learning-based approach to dataset expansion over random baselines. Finally, and most excitingly, we show that even in zero-shot models like CLIP, where it is possible to produce probability vectors over any possible class, the use of our adaptor leads to a **19.53%** relative improvement.

## 2 Background

First, we introduce the problem setting, notation, and mathematical tools that we will use. Afterward, we discuss how Loki relates to prior work.

Problem SettingAs in conventional supervised learning, we have a dataset \((x_{1},y_{1}),\ldots,(x_{n},y_{n})\) drawn from a distribution on \(\mathcal{X}\times\mathcal{Y}\), where \(\mathcal{X}\) and \(\mathcal{Y}\) are the input and label spaces. In our setting, \(N:=|\mathcal{Y}|\) is finite but large; often \(N\gg n\)--so that many labels will simply not be found in the training dataset. We let \(\Lambda\subseteq\mathcal{Y}\) with \(|\Lambda|=K\) be the set of observed labels. For convenience of notation, we also define \(\mathbf{\lambda}:=[\lambda_{i}]_{i=1}^{K}\), \(\mathbf{y}:=[y_{i}]_{j=1}^{N}\) to be the vectors of elements in \(\Lambda\) and \(\mathcal{Y}\).

In addition to our dataset, we have access to a relational structure on \(\mathcal{Y}\). We assume that \(\mathcal{Y}\) is a metric space with metric (distance) \(d:\mathcal{Y}^{2}\rightarrow\mathbb{R}\); \(d\) encodes the relational structure of the label space. Specifically, we model this metric space using a graph, \(G=(\mathcal{Y},\mathcal{E})\) where \(\mathcal{E}\subseteq\mathcal{Y}^{2}\times\mathbb{R}_{+}\) is a set of edges relating the labels and the standard shortest-path distance \(d:\mathcal{Y}^{2}\rightarrow\mathbb{R}_{\geq 0}\). In addition to its use in prediction, the metric \(d\) can be used for evaluating a model by measuring, for example, \(\frac{1}{n}\sum_{i=1}^{n}d^{2}(f(x_{i}),y)\)--the analogue of the empirical square loss.

Frechet Mean EstimatorDrawing on ideas from structured prediction [5; 35], we use a simple predictor that exploits the label metric. It relies on computing the _Frechet mean_[8], given by

\[m_{\mathbf{y}}(\mathbf{w}):=\operatorname*{arg\,min}_{y\in\mathcal{Y}}\sum_{i=1}^{K} \mathbf{w}_{i}d^{2}(y,\mathbf{y}_{i}), \tag{1}\]

Figure 1: Classification regions in the probability simplex of 3-class classifiers faced with a 100-class problem. The probability simplex using \(\operatorname*{arg\,max}\) prediction can only output one of three classes. Loki uses the entire probability vector to navigate the class metric space, leading to more prediction regions. (Left) regions from \(\operatorname*{arg\,max}\) prediction. (Centers, Right) classification regions from Loki.

where \(\mathbf{w}\in\mathbb{R}_{\geq 0}^{K}\) is a set of weights. The Frechet mean generalizes the barycenter to metric spaces and is often used in geometric machine learning [26].

Locus of the Frechet mean.The _locus of the Frechet mean_ is the set of all Frechet means under different weights [29]. We write it as \(\Pi(\mathbf{y}):=\cup_{\mathbf{w}\in\Delta^{K-1}}m_{\mathbf{y}}(\mathbf{w})\).

\(\Pi(\mathbf{y})\) can be thought of the set of all labels in \(\mathcal{Y}\) that are reachable by the Frechet mean given \(\{\mathbf{y}_{i}\}_{i=1}^{K}\subseteq\mathcal{Y}\) and different choices of its parameter \(\mathbf{w}\). Intuitively, we can think of the locus for a given dataset as describing how usable it is for predicting beyond the observed labels. Trivially, if \(\{\mathbf{y}_{i}\}_{i=1}^{K}=\mathcal{Y}\), then \(\Pi(\mathbf{y})=\mathcal{Y}\). We are primarily interested in the case in which \(\{\mathbf{y}_{i}\}_{i=1}^{K}\subset\mathcal{Y}\) yet we still have \(\Pi(\mathbf{y})=\mathcal{Y}\), or that \(|\Pi(\mathbf{y})|\) is at least sufficiently large.

### Relation to Prior work

Loki is primarily related to two areas: zero-shot learning and structured prediction.

Zero-Shot LearningLike zero-shot learning (ZSL), Loki is capable of predicting unobserved classes. Our framework is most closely related to _generalized_ ZSL, which uses side information to predict both observed and unobserved classes. Many types of external knowledge are used in ZSL, including text [28; 38; 9], attributes [22; 23; 7], knowledge graphs [42; 34; 10], ontologies [25], and logical rules [33]. Our work is most closely related to ZSL approaches that rely on knowledge graph information. Often, ZSL methods that use knowledge graph information rely on the use of graph neural network architectures [42; 17; 18]. _However, we note that these architectures can be heavyweight and can be challenging to scale up to extremely large graphs, whereas Loki does not have architectural requirements and scales linearly in the size of the graph when \(K\) is small._

Structured PredictionStructured prediction (SP) operates in label spaces that are endowed with algebraic or geometric structure [1; 21]. This includes problems such as predicting permutations [19], non-Euclidean and manifold regression [31; 36], and learning on graphs [12]. Loki is the most immediately related to the latter, however, any finite metric space can be represented as a graph, which further lends to the flexibility of our approach. Even in discrete structured prediction settings, the cardinality of the label space may be combinatorially large. _As such, Loki can be viewed as a simple method for adapting classifiers to structured prediction._

The Frechet mean has been used in structured prediction--but in approaches requiring training. In [5], \(\hat{f}(x)=\arg\min_{y\in\mathcal{Y}}\frac{1}{n}\sum_{i=1}^{n}\alpha_{i}(x)d^{ 2}(y,\mathbf{y}_{i})\), where \(\alpha(x)=(K+n\nu I)^{-1}K_{x}\). \(K\) is the kernel matrix for a kernel \(k:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}\), so that \(K_{i,j}=k(x_{i},x_{j})\), \((K_{x})_{i}=k(x,x_{i})\). \(\nu\) is a regularization parameter. In other words, the weight \(w_{i}\) corresponding to \(\mathbf{y}_{i}\) is the average produced by solving a kernel regression problem at all points \(x_{k}\) in the dataset where \(y_{k}=\mathbf{y}_{i}\). It has also used in weak supervision (WS) for metric-equipped label spaces [41; 37], where the goal is to produce labeled data for training structured prediction models.

## 3 Framework

We introduce our framework--Loki. We show how Loki can be used to adapt any supervised classifier over a set of \(K\) classes to a much richer set of possible class predictions. It does so by weighting the Frechet mean by the classifier's per-class prediction probabilities or logits, allowing it to predict any class in the locus of the Frechet mean--potentially far more classes than the initial \(K\). Next, we show how Loki can be expressed as a _fixed_ linear transformation of a model's outputs. Finally, we show that Loki relates to standard classification.

### Loki: Adapting Pretrained Models

We describe our approach to adapting pretrained classifiers-trained on a set of classes \(\Lambda\)--to the metric geometry of the label space \(\mathcal{Y}\), enabling the prediction of unobserved classes.

We model unobserved classes \(\mathcal{Y}\setminus\Lambda\) using the Frechet mean among observed classes weighted by their _prediction probabilities_\(P(y=\lambda_{i}|x\text{ and }y\in\Lambda)\). We denote the vector of model outputs as \(\mathbf{P}_{y|x}:=[\mathbf{P}_{\lambda_{i}|x}]_{i=1}^{K}=[P(y=\lambda_{i}|x\text{ and }y\in\Lambda)]_{i=1}^{K}\). Then predictions using Loki are given by

\[\hat{y}\in m_{\mathbf{\lambda}}(\mathbf{P}_{y|x})=\operatorname*{arg\,min}_{y\in \mathcal{Y}}\sum_{i=1}^{K}\mathbf{P}_{\lambda_{i}|x}d^{2}(y,\lambda_{i}).\]

### Loki as a linear transformation of model outputs

Most standard classifiers output a vector of prediction probabilities, \(\mathbf{P}_{y|x}\), whose entries correspond to the confidence of predicting a specific class. Predictions are typically given by \(\hat{y}\in\operatorname*{arg\,max}_{i\in[K]}(\mathbf{P}_{y|x})_{i}\). Loki generalizes this prediction rule when viewed as a linear transformation of \(\mathbf{P}_{y|x}\). Consider the Loki prediction rule \(\hat{y}\in m_{\mathbf{\lambda}}(\mathbf{P}_{y|x})=\operatorname*{arg\,min}_{y\in \mathcal{Y}}\sum_{i=1}^{K}\mathbf{P}_{\lambda_{i}|x}d^{2}(y,\lambda_{i})= \operatorname*{arg\,max}_{j\in[N]}(\mathbf{D}\mathbf{P}_{y|x})_{j}\), where \(\mathbf{D}_{j,i}=\left[-d^{2}(y_{j},\lambda_{i})\right]\); \(\mathbf{D}\in\mathbb{R}^{N\times K}\) is the matrix of negative squared distances between the observed classes and the rest of the label space. Thus Loki can be used within standard classification pipelines when the model output \(\mathbf{P}_{y|x}\) is multiplied by the fixed matrix \(\mathbf{D}\).

### Generalizing Standard Classification

We provide a simple intuition for our approach. The fact that Loki reduces to standard classification among the observed classes has several implications. This includes the idea that under our framework, forms of few-shot, zero-shot, hierarchical, and partial label learning all reduce to standard classification when additional metric information is introduced.

Generalizing the arg max prediction ruleIn the absence of this metric information--a situation that we model using the complete graph and setting \(\Lambda=\mathcal{Y}\)--our framework also recovers standard classification. Indeed, both in terms of error modeling and in terms of inter-class similarity, the intuition of standard classification and of the 0-1 loss are captured well by the unweighted complete graph--simply treat all differences equally. This graph is given as \(K_{N}:=(\mathcal{Y},\mathcal{Y}^{2}\times\{1\})\)--i.e., every label is equidistant from every other label. Plugging this into Loki, we obtain the following:

\[\hat{y}\in m_{\mathbf{\lambda}}(\mathbf{P}_{y|x})=\operatorname*{arg\,min}_{y\in \mathcal{Y}}\sum_{i=1}^{K}\mathbf{P}_{\lambda_{i}|x}d^{2}(y,\lambda_{i})= \operatorname*{arg\,min}_{y\in\mathcal{Y}}\sum_{i=1}^{K}\mathbf{P}_{\lambda_{i }|x}\mathbf{1}\{y\neq\lambda_{i}\}=\operatorname*{arg\,max}_{i\in[K]}\mathbf{ P}_{\lambda_{i}|x},\]

which is exactly the standard classification prediction rule.

Generalizing the 0-1 loss via the expected squared distance_The expected squared distance_\(\mathbb{E}[d^{2}(y,\hat{y})]\)_is the standard loss function for many structured prediction problems_. Note that **accuracy fails in such settings**--since it cannot distinguish between small and large errors. This is most clearly seen in the extreme case of regression, where test accuracy will virtually always be zero no matter how good a trained model is. At the other extreme, the complete graph, this loss function becomes the standard 0-1 loss: \(\mathbb{E}[d^{2}(y,\hat{y})]=\mathbb{E}[\mathbf{1}\{y\neq\hat{y}\}]\). As an adaptor for structured label spaces, we use the empirical version of this loss to evaluate Loki. Note that the expected squared distance subsumes other metrics as well. For example, when \(\mathcal{Y}=\mathbb{R}\), we can derive the standard MSE by setting \(d(y,\hat{y})=|y-\hat{y}|\), which is just the standard L1 distance metric. Other scores such as recall at top-k can be similarly obtained at the cost of \(d(\cdot,\cdot)\) being a true metric. In other words, \(\mathbb{E}[d^{2}(y,\hat{y})]\) is an very general metric that supports any metric space, and we use it throughout this work.

## 4 Theoretical Results

Challenges and OpportunitiesThe \(\operatorname*{arg\,max}\) of per-class model probabilities is a ubiquitous component of classification pipelines in machine learning. In order to predict unobserved classes using metric space information, Loki replaces this standard component. As a simple but significant change to standard pipelines, Loki opens up a new area for fundamental questions. There are three main flavors of theoretical questions that arise in this setting:

1. How does the performance of Loki change as a function of the number of samples?
2. What minimal sets of observed classes are required to predict any class in the metric space?
3. How can we acquire new classes that maximize the total number of classes we can predict?

[MISSING_PAGE_FAIL:5]

**Definition 4.3** (Trivial locus cover).: If \(\Lambda=\mathcal{Y}\), then \(\Lambda\) is the trivial locus cover.

This Definition captures the notion of observing all of the classes in the label space. Here, all of the elements of \(\mathcal{Y}\) are trivially reachable using Loki.

**Definition 4.4** (Nontrivial locus cover).: A locus cover \(\Lambda\) is nontrivial if \(\Lambda\neq\mathcal{Y}\).

Loki is more useful and interesting when faced with a nontrivial locus cover--under Definition 4.4, we can use some subset of classes \(\Lambda\) to predict any label in \(\mathcal{Y}\).

**Definition 4.5** (Minimum locus cover).: Given a set \(\Lambda\subseteq\mathcal{Y}\), if \(\Lambda\) is the smallest set that is still a locus cover, then it is a minimum locus cover.

In cases involving an extremely large number of classes, it is desirable to use Loki on the smallest possible set of observed classes \(\Lambda\) such that all labels in \(\mathcal{Y}\) can still be predicted. Definition 4.5 characterizes these situations--later, we obtain the minimum locus covers for all trees and grid graphs. It is worth noting that the minimum locus cover need not be unique for a fixed graph.

**Definition 4.6** (Identifying locus cover).: Given a set \(\Lambda\subseteq\mathcal{Y}\), if \(\Lambda\) is a locus cover where \(\forall\,y\in\mathcal{Y},\;\exists\,\mathbf{w}\in\Delta^{|\Lambda|-1}\) such that \(m_{\mathbf{\Lambda}}(\mathbf{w})=\{y\}\), then \(\Lambda\) is an identifying locus cover.

The Frechet mean need not be unique--as an \(\arg\min\), it returns a set of minimizers. In certain metric spaces, the minimum locus cover can yield large sets of minimizers--this is undesirable, as it makes predicting a single class challenging. Definition 4.6 appeals to the idea of finding some set of classes for which the Frechet mean _always_ returns a unique minimizer--this is desirable in practice, and in some cases, moreso than Definition 4.5.

**Definition 4.7** (Pairwise decomposable).: Given \(\Lambda\subseteq\mathcal{Y}\), \(\Pi(\Lambda)\) is called pairwise decomposable when it holds that \(\Pi(\Lambda)=\cup_{\lambda_{1},\lambda_{2}\in\Lambda}\Pi(\{\lambda_{1},\lambda _{2}\})\).

In many cases, the locus can be written in a more convenient form--the union of the locus of pairs of nodes. We refer to this definition as pairwise decomposability. Later, we shall see that pairwise decomposability is useful in computing the locus in polynomial time.

TreesMany label spaces are endowed with a tree metric in practice: hierarchical classification, in which the label space includes both classes and superclasses, partial labeling problems in which internal nodes can represent the prediction of a set of classes, and the approximation of complex or intractable metrics using a minimum spanning tree. We show that for our purposes, trees have certain desirable properties that make them easy to use with Loki--namely that we can easily identify a locus cover that satisfies both Definition 4.5 and Definition 4.6. Conveniently, we also show that any locus in any tree satisfies Definition 4.7.

We first note that the leaves of any tree yield the minimum locus cover. This is a convenient property--any label from any label space endowed with a tree metric can be predicted using Loki using only classifiers trained using labels corresponding to the leaves of the metric space. This can be especially useful if the tree has long branches and few leaves. Additionally, for tree metric spaces, the minimum locus cover (Definition 4.5) is also an identifying locus cover (Definition 4.6). This follows from the construction of the weights in the proof of Theorem A.4 (shown in the Appendix) and the property that all paths in trees are unique. Finally, we note that any locus in any tree is pairwise decomposable--the proof of this is given in the Appendix (Lemma A.5). We will see later that this property yields an efficient algorithm for computing the locus.

Phylogenetic TreesImage classification datasets often have a hierarchical tree structure, where only the leaves are actual classes, and internal nodes are designated as superclasses--examples include the ImageNet [6] and CIFAR-100 datasets [20]. Tree graphs in which only the leaf nodes are labeled are referred to as phylogenetic trees [3]. Often, these graphs are weighted, but unless otherwise mentioned, we assume that the graph is unweighted.

For any arbitrary tree \(T=(\mathcal{V},\mathcal{E})\), the set of labels induced by phylogenetic tree graph is \(\mathcal{Y}=\text{Leaves}(T)\). We provide a heuristic algorithm for obtaining locus covers for arbitrary phylogenetic trees in Algorithm B.1 (see Appendix). We prioritize adding endpoints of long paths to \(\Lambda\), and continue adding nodes in this way until \(\Pi(\Lambda)=\mathcal{Y}\). Similarly to tree metric spaces, any phylogenetic tree metric space is pairwise decomposable. We prove the correctness of Algorithm B.1 and pairwisedecomposability of phylogenetic trees in the Appendix (Theorem A.6 and Lemma A.7). Later, we give algorithms for computing the set of nodes in an arbitrary locus in arbitrary graphs--if the locus is pairwise decomposable, the algorithm for doing so is efficient, and if not, it has time complexity exponential in \(K\). Due to the pairwise decomposability of phylogenetic trees, this polynomial-time algorithm to compute \(\Pi(\Lambda)\) applies.

Grid GraphsClasses often have a spatial relationship. For example, classification on maps or the discretization of a manifold both have spatial relationships--grid graphs are well suited to these types of spatial relationships. We obtain minimum locus covers for grid graphs satisfying Definition 4.5, but we find that these are not generally identifying locus covers. On the other hand, we give an example of a simple identifying locus cover satisfying Definition 4.6. Again, we find that grid graphs are in general pairwise decomposable and hence follow Definition 4.7.

We find that the pair of vertices on furthest opposite corners yields the minimum locus cover. While the set of vertices given by Theorem A.8 (found in the Appendix) satisfies Definition 4.5, this set does not in general satisfy Definition 4.6. This is because the path between any two vertices is not unique, so each minimum path of the same length between the pair of vertices can have an equivalent minimizer. On contrast, the following example set of vertices satisfies Definition 4.6 but it clearly does not satisfy Definition 4.5. _Example_: Given a grid graph, the set of all corners is an identifying locus cover. On the other hand, the vertices given by Theorem A.8 can be useful for other purposes. Lemma A.9 (provided in the Appendix) shows that subspaces of grid graphs can be formed by the loci of pairs of vertices in \(\Lambda\). This in turn helps to show that loci in grid graphs are pairwise decomposable in general (see Lemma A.10 in the Appendix).

The Complete GraphThe standard classification setting does not use relational information between classes. As before, we model this setting using the complete graph, and we show the expected result that in the absence of useful relational information, Loki cannot help, and the problem once again becomes standard multiclass classification among observed classes. To do so, we show that there is no nontrivial locus cover for the complete graph (Theorem A.11 in the Appendix).

### Label Subspaces in Practice

While it is desirable for the set of observed classes to form a minimum or identifying locus cover, it is often not possible to choose the initial set of observed classes a priori--these are often random. In this section, we describe the more realistic cases in which a random set of classes are observed and an active learning-based strategy to choose the next observed class. The aim of our active learning approach is, instead of randomly selecting the next observed class, to actively select the next class so as to maximize the total size of the locus--i.e., the number of possible classes that can be output using Loki. Before maximizing the locus via active learning, we must first address a much more basic question: can we even efficiently compute the locus?

Computing the LocusWe provide algorithms for obtaining the set of all classes in the locus, given a set of classes \(\Lambda\). We show that when the locus is pairwise decomposable (Definition 4.7), we can compute the locus efficiently using a polynomial time algorithm. When the locus is not pairwise decomposable, we provide a general algorithm that has time complexity exponential in \(|\Lambda|\)--we are not aware of a more efficient algorithm. We note that any locus for every type of graph that we consider in Section 4.2 is pairwise decomposable, so our polynomial time algorithm applies. Algorithms B.2 and B.3 along with their time complexity analyses can be found in the Appendix.

Large Locus via Active Next-Class SelectionWe now turn to actively selecting the next class to observe in order to maximize the size of the locus. For this analysis, we focus on the active learning setting when the class structure is a tree graph, as tree graphs are generic enough to apply to a wide variety of cases--including approximating other graphs using the minimum spanning tree. Assume the initial set of \(K\) observed classes are sampled at random from some distribution. We would like to actively select the \(K+1\)st class such that \(|\Pi(\Lambda)|\) with \(\Lambda=\{\lambda\}_{i=1}^{K+1}\) is as large as possible.

**Theorem 4.8**.: _Let \(T=(\mathcal{Y},\mathcal{E})\) be a tree graph and let \(\Lambda\subseteq\mathcal{Y}\) with \(K=|\Lambda|\). Let \(T^{\prime}\) be the subgraph of the locus \(\Pi(\Lambda)\). The vertex \(v\in\mathcal{Y}\setminus\Lambda\) that maximizes \(|\Pi(\Lambda\cup\{v\})|\) is the solution to the following optimization problem: \(\operatorname*{arg\,max}_{y\in\mathcal{Y}\setminus\Pi(\Lambda)}d(y,b)\) s.t. \(b\in\partial_{in}T^{\prime}\) and \(\Gamma(y,b)\setminus\{b\}\subseteq\mathcal{Y}\setminus\Pi(\Lambda)\). where \(\partial_{in}T^{\prime}\) is the inner boundary of \(T^{\prime}\) (all vertices in \(T^{\prime}\) that share an edge with vertices not in \(T^{\prime}\))._This procedure can be computed in polynomial time--solving the optimization problem in Theorem 4.8 simply requires searching over pairs of vertices. Hence we have provided an efficient active learning-based strategy to maximize the size of the locus for trees.

## 5 Experimental Results

In this section, we provide experimental results to validate the following claims:

1. Loki improves performance of zero-shot foundation models even with no external metric.
2. Loki adapts to label spaces with a large number of unobserved classes.
3. The active approach given in Theorem 4.8 yields a larger locus than the passive baseline.
4. The same active approach yields better performance on ImageNet.
5. With Loki, calibration can improve _accuracy_, even with no external metric.

### Loki Improves Zero-Shot Models

We evaluate the capability of Loki to improve upon zero-shot models where all classes are observed.

SetupOur experiment compares the zero-shot prediction performance of CLIP [32] on CIFAR-100 [20] to CLIP logits used with Loki. First, we consider the setting in which no external metric relating the labels is available, and instead derive internal metric information from Euclidean distances between text embeddings from the models using their respective text encoders. Second, we consider three external metric spaces for use with Loki: the complete graph \(K_{100}\) and two phylogenetic trees: the default CIFAR-100 superclasses [20] and WordNet [2].

ResultsThe results of this experiment are given in Table 1. When no external metric information is available, Loki still outperforms CLIP-like models that use the standard prediction rule--in other words, Loki seems to unconditionally improve CLIP. As expected, under the complete graph, our method becomes equivalent to the standard prediction mechanism used by CLIP. On the other hand, Loki outperforms CLIP when using the default CIFAR-100 tree hierarchy and even more so when using the WordNet geometry, with a \(\mathbf{19.53\%}\) relative improvement in mean squared distance over the CLIP baseline. We postulate that the strong performance using WordNet is due to the richer geometric structure compared to that of the default CIFAR-100 hierarchy.

### Loki on Partially-Observed Label Spaces

To validate our approach on partially observed label spaces, we evaluate the performance of adapting a logistic classifier trained on SimCLR embeddings of ImageNet [6], 5-NN models trained on a 9,419 class subset of the PubMed dataset,2 and the 325,056-class LSHTC dataset [30].

\begin{table}
\begin{tabular}{c c|c c c} \hline \hline Model & Metric Space & \(\operatorname{arg\,max}\) & Loki & Relative Improvement \\ \hline CLIP-RN50 [32] & Internal & 0.2922 & **0.2613** & **10.57\%** \\ \hline CLIP-ViT-L-14 [32] & Internal & 0.1588 & **0.1562** & **1.63\%** \\ \hline ALIGN [16] & Internal & 0.1475 & **0.1430** & **3.02\%** \\ \hline CLIP-RN50 [32] & \(K_{100}\) & 0.5941\({}^{*}\) & 0.5941\({}^{*}\) & 0.0\%\({}^{*}\) \\ CLIP-RN50 [32] & Default tree & 7.3528 & **7.1888** & **2.23\%** \\ CLIP-RN50 [32] & WordNet tree & 24.3017 & **19.5549** & **19.53\%** \\ \hline \hline \end{tabular}

* \({}^{*}\) methods equivalent under the complete graph as Loki reduces to \(\operatorname{arg\,max}\) prediction.

\end{table}
Table 1: **CIFAR-100. Improving CLIP predictions using Loki. Results are reported as \(\mathbb{E}[d^{2}(y,\hat{y})]\) in the respective metric space. CLIP-like zero-shot models can be improved using Loki even without access to an external metric, and internal class embedding distances are used. When an external metric is available, Loki outperforms CLIP using the default CIFAR-100 hierarchy and WordNet.**SetupFor ImageNet, we use the WordNet phylogenetic tree as the metric space [2]. In this setting, we sample random subsets of size \(K\) of the 1000 ImageNet classes and compare a baseline one-vs-rest classifier to the same classifier but using Loki to adapt predictions to classes beyond the original \(K\). We conduct two sets of experiments. In the first, we sample \(K\) classes uniformly, while in the second, we adopt a more realistic sampler--we sample from a Gibbs distribution: \(P(\lambda|\theta)=\frac{1}{Z}\exp(-\theta d(\lambda,\lambda^{c}))\), where \(\lambda^{c}=m_{\mathbf{y}}(\mathbf{1}_{N})\) is the centroid of the metric space, \(\theta\) is the concentration parameter around the centroid, and \(Z\) is the normalizer. While the Gibbs distribution sampler is more realistic, it is also the more challenging setting--classes which have low probability according to this distribution are less likely to appear in the locus. For PubMed, we derive our metric from Euclidean distances between SimCSE class embeddings [11]. Finally for LSHTC, we summarize the default graph by randomly selecting nodes and merging them with their neighbors until we obtain a graph with 10,000 supernodes representing sets of classes.

ResultsFigure 2 shows the mean squared distances compared to the baseline one-vs-rest classifiers, across various settings of \(K\). We find that Loki always significantly outperforms the baseline, even in the more challenging setting of sampling according to a Gibbs distribution. Tables 2 and 3 show our improvements when using Loki over the baseline 5-NN models. While Loki consistently yields an improvement on PubMed and LSHTC, the improvement is more dramatic on LSHTC.

### Large Loci via Active Learning

We evaluate our active next class selection approach on increasing the locus size. We expect that compared to passive (random) selection, our active approach will lead to larger loci, and in practice, a larger set of possible classes that can be predicted using Loki while observing fewer classes during training.

SetupWe compare with a baseline that randomly selects the next class. We first generate a synthetic random tree with size \(N\) and fix an initial \(K\). In active selection, we use the approach described in Theorem 4.8 to select the next node. As a passive baseline, we randomly sample (without replacement) the remaining nodes that are not in \(\Lambda\).

\begin{table}
\begin{tabular}{r|c c} \hline \hline \(K\) & 5-NN & 5-NN + Loki \\ \hline
100 & 1.68666 & **1.42591** \\
250 & 1.52374 & **1.47801** \\
500 & 1.64074 & **1.45921** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **PubMed. Loki improves baseline for all settings of \(K\). The metric space is Euclidean distances applied to SimCSE embeddings.**

Figure 2: (Top left) **ImageNet. Mean squared distances under uniform class sampling and under the Gibbs distribution—Loki improves upon the baseline SimCLR one-vs-rest classifier. (Top right) **Synthetic. Active class selection consistently leads to larger loci compared to uniform sampling.** (Bottom) **Active selection on ImageNet.** Active class selection improves performance on ImageNet.

ResultsThe results are shown in Figure 2. We set \(N=100\) and the initial \(K=3\), then we average the result over 10 independent trials. The active approach consistently outperforms the random baseline as it attains a larger locus with fewer nodes selected.

### Improving Performance via Active Learning

Next, we evaluate the our active next class selection approach on improving error on ImageNet. While we found that the the active approach indeed leads to an increased locus size compared to the passive baseline, we expect that this increased locus size will lead to improved performance.

SetupWe randomly sample 500 ImageNet classes, and using the WordNet metric space, we use our active approach to iteratively sample 50 more classes. We compare this to a passive baseline in which the 50 classes are sampled randomly. We repeat this experiment over 10 independent trials.

ResultsFigure 2 shows that our active approach yields improved error over the passive baseline. The gap between the active approach and the passive baseline widens with more selection rounds.

### Improving Accuracy via Calibration

Finally, we evaluate the effect of calibrating the Softmax outputs on the performance of Loki.

SetupWe calibrate via Softmax temperature scaling [13] using CLIP on CIFAR-100. We do not use an external metric space, and instead use Euclidean distance applied to the CLIP text encoder.

ResultsThe reliability diagram in Figure 3 shows that the optimal Softmax temperature for Loki is both close to the default temperature used by CLIP and to the optimally-calibrated temperature. In Figure 3 (right), we find that appropriate tuning of the temperature parameter _can lead to improved accuracy with CLIP_, even when no external metric space is available.

## 6 Conclusion

In this work, we proposed Loki--a simple adaptor for pretrained models to enable the prediction of additional classes that are unobserved during training by using metric space information. We comprehensively answered the space of new questions that arise under Loki in terms of learning, optimal metric space settings, and a practical active selection strategy. Experimentally, we showed that Loki can be used to improve CLIP even without external metric space information, can be used to predict a large number of unobserved classes, and a validation of our active selection strategy.

Figure 3: **CLIP on CIFAR-100 with no external metric.** (Left) Reliability diagrams across a range of Softmax temperatures, highlighting the CLIP default temperature, the optimal temperature for Loki, and the minimizer of the Expected Calibration Error (ECE). All three are well-calibrated. (Center) Tradeoff between optimizing for ECE and the expected squared distance. As with the reliability diagrams, the CLIP default temperature, the Loki-optimal temperature, and the ECE-optimal temperature are similar. (Right) Tradeoff between optimizing for zero-one error and the expected squared distance. Temperature can be tuned to improve _accuracy_ when using Loki.

## Acknowledgements

We are grateful for the support of the NSF under CCF2106707 (Program Synthesis for Weak Supervision) and the Wisconsin Alumni Research Foundation (WARF).

## References

* [1] Gokhan Bakir, Thomas Hofmann, Bernhard Scholkopf, Alexander J. Smola, Ben Taskar, and S.V.N Vishwanathan. _Predicting Structured Data_. The MIT Press, July 2007.
* [2] Bjorn Barz and Joachim Denzler. Hierarchy-based image embeddings for semantic image retrieval. In _2019 IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 638-647. IEEE, 2019.
* [3] Louis J. Billera, Susan P. Holmes, and Karen Vogtmann. Geometry of the space of phylogenetic trees. _Advances in Applied Mathematics_, 27(4):733-767, 2001.
* [4] Yeshwanth Cherapanamjeri, Nicolas Flammarion, and Peter L. Bartlett. Fast mean estimation with sub-gaussian rates. In Alina Beygelzimer and Daniel Hsu, editors, _Proceedings of the Thirty-Second Conference on Learning Theory_, volume 99 of _Proceedings of Machine Learning Research_, pages 786-806. PMLR, 25-28 Jun 2019.
* [5] Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi. A consistent regularization approach for structured prediction. In _Advances in Neural Information Processing Systems 30 (NIPS 2016)_, volume 30, 2016.
* [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pages 248-255, 2009.
* [7] Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. Describing objects by their attributes. In _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pages 1778-1785, 2009.
* [8] Maurice R. Frechet. Les elements aleatoires de nature quelconque dans un espace distancie. _Annales de l'institut Henri Poincare_, 1948.
* [9] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc' Aurelio Ranzato, and Tomas Mikolov. Devise: A deep visual-semantic embedding model. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 26. Curran Associates, Inc., 2013.
* [10] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. I Know the Relationships: Zero-Shot Action Recognition via Two-Stream Graph Convolutional Networks and Knowledge Graphs. _Proceedings of the AAAI Conference on Artificial Intelligence_, 33(01):8303-8311, July 2019.
* [11] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In _Empirical Methods in Natural Language Processing (EMNLP)_, 2021.
* [12] Colin Graber and Alexander Schwing. Graph structured prediction energy networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [13] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 1321-1330. PMLR, 06-11 Aug 2017.
* [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2016.

* [15] Daniel Hsu and Sivan Sabato. Loss minimization and parameter estimation with heavy tails. _Journal of Machine Learning Research_, 17(18):1-40, 2016.
* [16] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 4904-4916. PMLR, 18-24 Jul 2021.
* [17] Michael C. Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, and Eric P. Xing. Rethinking knowledge graph propagation for zero-shot learning. _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11479-11488, 2018.
* [18] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations_, 2017.
* [19] Anna Korba, Alexandre Garcia, and Florence d'Alche-Buc. A structured prediction approach for label ranking. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [21] Volodymyr Kuleshov and Percy S Liang. Calibrated structured prediction. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015.
* [22] Christoph H. Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pages 951-958, 2009.
* [23] Christoph H. Lampert, Hannes Nickisch, and Stefan Harmeling. Attribute-based classification for zero-shot visual object categorization. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 36(3):453-465, 2014.
* [24] M. Lerasle and R. I. Oliveira. Robust empirical mean estimators, 2011.
* [25] Juan Li, Ruoxu Wang, Ningyu Zhang, Wen Zhang, Fan Yang, and Huajun Chen. Logic-guided semantic representation learning for zero-shot relation classification. In _Proceedings of the 28th International Conference on Computational Linguistics_, pages 2967-2978, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics.
* [26] Aaron Lou, Isay Katsman, Qingxuan Jiang, Serge Belongie, Ser-Nam Lim, and Christopher De Sa. Differentiating through the frechet mean. In _Proceedings of the 37th International Conference on Machine Learning_, pages 6393-6403, 2020.
* 2335, 2015.
* [28] Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg Corrado, and Jeffrey Dean. Zero-shot learning by convex combination of semantic embeddings. In Yoshua Bengio and Yann LeCun, editors, _ICLR_, 2014.
* [29] Tom M W Nye, Xiaoxian Tang, Grady Weyenberg, and Ruriko Yoshida. Principal component analysis and the locus of the frechet mean in the space of phylogenetic trees. _Biometrika_, 104(4):901-922, Dec 2017.
* [30] Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis, Thierry Artieres, Georgios Paliouras, Eric Gaussier, Ion Androutsopoulos, Massih-Reza Amini, and Patrick Gallinari. Lshtc: A benchmark for large-scale text classification. _ArXiv_, abs/1503.08581, 2015.
* [31] Alexander Petersen and Hans-Georg Muller. Frechet regression for random objects with euclidean predictors. _The Annals of Statistics_, 2016.

* [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* [33] Tim Rocktaschel, Sameer Singh, and Sebastian Riedel. Injecting logical background knowledge into embeddings for relation extraction. In _Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 1119-1129, Denver, Colorado, May-June 2015. Association for Computational Linguistics.
* [34] Abhinaba Roy, Deepanway Ghosal, Erik Cambria, Navonil Majumder, Rada Mihalcea, and Soujanya Poria. Improving Zero-Shot Learning Baselines with Commonsense Knowledge. _Cognitive Computation_, 14(6):2212-2222, November 2022.
* [35] Alessandro Rudi, Carlo Ciliberto, GianMaria Marconi, and Lorenzo Rosasco. Manifold structured prediction. In _Advances in Neural Information Processing Systems 32 (NeurIPS 2018)_, volume 32, 2018.
* [36] Alessandro Rudi, Carlo Ciliberto, GianMaria Marconi, and Lorenzo Rosasco. Manifold structured prediction. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [37] Changho Shin, Winfred Li, Harit Vishwakarma, Nicholas Carl Roberts, and Frederic Sala. Universalizing weak supervision. In _International Conference on Learning Representations_, 2022.
* [38] Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng. Zero-shot learning through cross-modal transfer. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 26. Curran Associates, Inc., 2013.
* [39] Russell Stewart and Stefano Ermon. Label-free supervision of neural networks with physics and domain knowledge. In _Thirty-First AAAI Conference on Artificial Intelligence_, 2017.
* [40] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In _Compressed Sensing_, 2010.
* [41] Harit Vishwakarma and Frederic Sala. Lifting weak supervision to structured prediction. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [42] X. Wang, Y. Ye, and A. Gupta. Zero-shot recognition via semantic embeddings and knowledge graphs. In _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6857-6866, Los Alamitos, CA, USA, jun 2018. IEEE Computer Society.

The appendix is organized as follows: in Appendix A, we provide proofs of the claims made in the main text. Next, in Appendix B, we provide algorithms and time complexity analyses for the algorithms referenced in the main text. Appendix C contains additional experimental results. In Appendix D, we provide additional details about our experimental setup. Then in Appendix E, we discuss the broader impacts and limitations of Loki. Finally, in Appendix F, we provide more details and examples of the locus visualizations shown in Figure 1.

## Appendix A Deferred Proofs

### Proof of Theorem 4.1 (Loki sample complexity)

In this section, we provide a formal statement and proof of our sample complexity result for Loki, including additional required definitions and assumptions.

First, we define the required tools to prove the sample complexity bound for Loki. For the purposes of this proof, we define the Frechet variance as the function over which the Frechet mean returns the minimizer.

**Definition A.1** (Frechet variance).: The Frechet variance is defined as

\[\Psi_{\mathbf{w}}(V):=\sum_{i\in[K]}\mathbf{w}_{i}d^{2}(V,V_{i}).\]

Additionally, we will require a technical assumption related to the sensitivity of the Frechet variance to different node choices.

**Assumption A.2** (Frechet variance is \(\frac{1}{\alpha}\)-bi-Lipschitz).: For a metric space defined by a graph \(G=(\mathcal{V},\mathcal{E})\), the Frechet variance is \(K\)-bi-Lipschitz if there exists a \(K\geq 1\) such that

\[\frac{1}{K}d^{2}(V,\tilde{V})\leq|\Psi_{\mathbf{w}}(V)-\Psi_{\mathbf{w}}( \tilde{V})|\leq Kd^{2}(V,\tilde{V})\]

for all \(V,\tilde{V}\in\mathcal{V}\), and a fixed \(\mathbf{w}\in\Delta^{K-1}\). For our purposes, such a \(K\) always exists: consider setting \(K=\text{diam}(G)^{2}\max_{V_{1},V_{2}\in\mathcal{V}}|\Psi_{\mathbf{w}}(V_{1}) -\Psi_{\mathbf{w}}(V_{2})|\). However, this is a very conservative bound that holds for all graphs that we consider. Instead, we assume access to the largest \(\alpha=\frac{1}{K}\leq 1\) such that

\[\alpha d^{2}(V,\tilde{V})\leq|\Psi_{\mathbf{w}}(V)-\Psi_{\mathbf{w}}(\tilde{V} )|,\]

which may be problem dependent.

**Theorem A.3** (Loki sample complexity).: _Let \(\mathcal{Y}\) be a set of points on the \(d\) dimensional 2-norm ball of radius \(R\), and let \(\Lambda\subseteq\mathcal{Y}\) be the set of \(K\) observed classes. Assume that \(\Lambda\) forms a \(2R/(\sqrt{d}K-1)\)-net under the Euclidean distance. Assume that training examples are generated by drawing \(n\) samples from the following process: draw \(x\sim\mathcal{N}(y,I)\) where \(y\sim\text{Unif}(\Lambda)\), and at test time, draw \(x\sim\mathcal{N}(y,I)\) where \(y\sim\text{Unif}(\mathcal{Y})\). Assume that we estimate a Gaussian mixture model with \(K\) components (each having identity covariance) on the training set and obtain probability estimates \(\hat{\mathbb{P}}(y_{i}|x)\) for \(i\in[K]\) for a sample \((x,y_{*})\) from the test distribution. Then with high probability, under the following model,_

\[\hat{y}_{*}\in m_{\Lambda}([\hat{\mathbb{P}}(y_{i}|x)]_{i\in[K]})=\operatorname {arg\,min}_{y\in\mathcal{Y}}\sum_{i\in[K]}\hat{\mathbb{P}}(y_{i}|x)d^{2}(y,y_{ i})\]

_the sample complexity of estimating target \(y_{*}\) from the test distribution \(\mathcal{D}_{\text{test}}\) with prediction \(\hat{y}_{*}\) is:_

\[\mathbb{E}_{(x,y_{*})\sim\mathcal{D}_{\text{test}}}[d^{2}(y_{*},\hat{y}_{*})] \leq O\left(\frac{d}{\alpha}\sqrt{\frac{\log K/\delta}{n}}\left(\frac{1}{\left( R^{1-\frac{2}{d}}-\frac{\log R}{R}\right)}+\sqrt{d}\right)\right)\]

_where \(d\) is the dimensionality of the input and \(\alpha\) is our parameter under Assumption A.2._

Proof.: We begin by detailing the data-generating process.

**At training time,** our underlying data-generating process, \(\mathcal{D}_{\text{train}}\), is as follows:

[MISSING_PAGE_FAIL:15]

\[=\Bigg{|}\sum_{i\in[K]}\Bigg{(}\frac{\exp\{-\frac{1}{2}||x-\hat{y}_{i}||_ {2}^{2}\}}{\sum_{j\in[K]}\exp\{-\frac{1}{2}||x-\hat{y}_{j}||_{2}^{2}\}\frac{1}{ K}}\] \[\qquad\qquad-\frac{\exp\{-\frac{1}{2}||x-y_{i}||_{2}^{2}\}\frac{1 }{K}}{\sum_{j\in[K]}\exp\{-\frac{1}{2}||x-y_{j}||_{2}^{2}\}\frac{1}{K}}\Bigg{)} ||y_{*}-y_{i}||_{2}^{2}\Bigg{|}.\] \[=\Bigg{|}\sum_{i\in[K]}\Bigg{(}\frac{\exp\{-\frac{1}{2}||x-\hat{y }_{i}||_{2}^{2}\}}{\sum_{j\in[K]}\exp\{-\frac{1}{2}||x-\hat{y}_{j}||_{2}^{2}\}}\] \[\qquad\qquad-\frac{\exp\{-\frac{1}{2}||x-y_{i}||_{2}^{2}\}}{\sum_ {j\in[K]}\exp\{-\frac{1}{2}||x-y_{j}||_{2}^{2}\}}\Bigg{)}||y_{*}-y_{i}||_{2}^{ 2}\Bigg{|}. \tag{4}\]

For notational convenience, we define the following:

\(a_{i}:=\exp\{-\frac{1}{2}||x-y_{i}||_{2}^{2}\}\),

\(\hat{a}_{i}:=\exp\{-\frac{1}{2}||x-\hat{y}_{i}||_{2}^{2}\}\),

\(b:=\sum_{j\in[K]}\exp\{-\frac{1}{2}||x-y_{j}||_{2}^{2}\}\),

\(\hat{b}:=\sum_{j\in[K]}\exp\{-\frac{1}{2}||x-\hat{y}_{j}||_{2}^{2}\}\), and

\(c_{i}:=||\hat{y}_{*}-y_{i}||_{2}^{2}\).

Then (4) becomes:

\[\Bigg{|}\sum_{i\in[K]}\bigg{(}\frac{\hat{a}_{i}}{\hat{b}}-\frac{a _{i}}{b}\bigg{)}\,c_{i}\Bigg{|} =\Bigg{|}\sum_{i\in[K]}\bigg{(}\frac{\hat{a}_{i}}{\hat{b}}-\frac{ \hat{a}_{i}}{b}+\frac{\hat{a}_{i}}{b}-\frac{a_{i}}{b}\bigg{)}\,c_{i}\Bigg{|}\] \[=\Bigg{|}\sum_{i\in[K]}\bigg{(}\frac{\hat{a}_{i}-a_{i}}{b}+\hat{a }_{i}\left(\frac{1}{\hat{b}}-\frac{1}{b}\right)\bigg{)}\,c_{i}\Bigg{|}\] \[\leq\Bigg{|}\sum_{i\in[K]}\bigg{(}\frac{\hat{a}_{i}-a_{i}}{b} \bigg{)}\,c_{i}\Bigg{|}+\Bigg{|}\sum_{i\in[K]}\bigg{(}\hat{a}_{i}\left(\frac{ 1}{\hat{b}}-\frac{1}{b}\right)\bigg{)}\,c_{i}\Bigg{|}\] \[=\Bigg{|}\sum_{i\in[K]}\frac{a_{i}}{b}\left(\frac{\hat{a}_{i}}{a_ {i}}-1\right)c_{i}\Bigg{|}+\Bigg{|}\Bigg{(}\sum_{i\in[K]}\frac{a_{i}}{b}\left( \frac{\hat{a}_{i}}{a_{i}}-1\right)\Bigg{)}\left(\sum_{i\in[K]}\frac{\hat{a}_{i} }{\hat{b}}c_{i}\right)\Bigg{|}. \tag{5}\]

Now we define the following: \(L:=||x-y_{z}||_{2}^{2}\) with \(z\in\arg\min_{j}||x-y_{j}||_{2}^{2}\) is the smallest distance to a class mean, and \(L+E_{i}:=||x-y_{i}||_{2}^{2}\) with \(E_{i}>0\). Similarly, define \(\hat{L}:=||x-\hat{y}_{z}||_{2}^{2}\) with \(z\in\arg\min_{j}||x-\hat{y}_{j}||_{2}^{2}\) is the smallest distance to an estimated class mean, and \(\hat{L}+\hat{E}_{i}:=||x-\hat{y}_{i}||_{2}^{2}\) with \(\hat{E}_{i}>0\). Finally, we define \(T:=\sqrt{\hat{L}}+\sqrt{\hat{L}+\hat{E}_{i}}+\sqrt{L}+\sqrt{L+E_{i}}\).

Then we can bound the parts separately:

For \(i\neq z\), we have

\[\frac{a_{i}}{b} =\Bigg{(}\frac{\exp\{-\frac{1}{2}||x-y_{i}||_{2}^{2}\}}{\sum_{j \in[K]}\exp\{-\frac{1}{2}||x-y_{j}||_{2}^{2}\}}\Bigg{)}\] \[\leq\bigg{(}\frac{\exp\{-\frac{1}{2}(L+E_{i})\}}{\exp\{-\frac{1} {2}L\}+\exp\{-\frac{1}{2}(L+E_{i})\}}\bigg{)}\] \[=\frac{1}{\exp\{\frac{1}{2}E_{i}\}} \tag{6}\]

and in the case of \(i=z\), we bound \(\frac{a_{i}}{b}\leq 1\). So overall, we have \(\frac{a_{i}}{b}\leq\frac{1}{\exp\{\mathbf{1}_{i\neq z,\frac{1}{2}E_{i}}\}}\) for all \(i\in[K]\).

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_EMPTY:22]

### Proof of Lemma a.9 (loci of grid subspaces)

**Lemma A.9** (Locus of grid subspaces).: _Given any pair of vertices in \(\Lambda\), we can find a subset \(G^{\prime}\) of the original grid graph \(G=(\mathcal{Y},\mathcal{E})\) which takes the given pair as two corner. \(\Pi(\mathbf{\Lambda})\) equals to all the points inside \(G^{\prime}\)._

Proof.: The result follows from application of Theorem A.8 to the choice of metric subspace. 

### Proof of Lemma a.10 (grid pairwise decomposability)

**Lemma A.10** (Grid pairwise decomposability).: _Let \(G=(\mathcal{Y},\mathcal{E})\) be a grid graph and \(\Lambda\subseteq\mathcal{Y}\). Then \(\Pi(\Lambda)\) is pairwise decomposable._

Proof.: Suppose we have a grid graph \(G=(\mathcal{Y},\mathcal{E})\) and a vertex \(\hat{y}\in\Pi(\Lambda)\) with \(\Lambda\subseteq\mathcal{Y}\). Due to the fact that \(\hat{y}\in\Pi(\Lambda)\) and the fact that \(G\) is a grid graph, we have that \(\hat{y}\in\Gamma(\lambda_{\alpha},\lambda_{\beta})\) for some \(\lambda_{\alpha},\lambda_{\beta}\in\Lambda\). Then by Lemma A.10,

\[\hat{y}\in\Pi(\{\lambda_{\alpha},\lambda_{\beta}\})\subseteq\cup_{\lambda_{i },\lambda_{j}\in\Lambda}\Pi(\{\lambda_{i},\lambda_{j}\}).\]

Therefore loci on grid graphs are pairwise decomposable. 

### Proof of Theorem a.11 (no nontrivial locus covers for complete graphs)

**Theorem A.11** (Trivial locus cover for the complete graph).: _There is no non-trivial locus cover for the complete graph._

Proof.: We show that there is no nontrivial locus cover for complete graphs by showing that removing any vertex from the trivial locus cover renders that vertex unreachable. We proceed by strong induction on the number of vertices, \(n\).

**Base case** We first set \(n=3\). Let \(\mathbf{K_{3}}=(\mathcal{V},\mathcal{V}^{2})\) be the complete graph with three vertices: \(\mathcal{V}=\{v_{1},v_{2},v_{3}\}\), and without loss of generality, let \(\Lambda=\{v_{2},v_{3}\}\) be our set of observed classes. There are two cases on the weight vector \(\mathbf{w}=[w_{2},w_{3}]\):

Case 1: Suppose \(\mathbf{w}\not\in\text{int}\Delta^{2}\). This means that either \(w_{2}=1\) or \(w_{3}=1\)--which leads to the Frechet mean being either \(v_{2}\) or \(v_{3}\), respectively. Neither of these instances correspond to \(v_{1}\) being a minimizer.

Case 2: Suppose \(\mathbf{w}\in\text{int}\Delta^{2}\). Then the Frechet mean is given by

\[m_{\mathbf{\lambda}}(\mathbf{w})=\operatorname*{arg\,min}_{y\in\mathcal{Y}}w_{2}d^{2} (y,v_{2})+w_{3}d^{2}(y,v_{3})\]

Assume for contradiction that \(v_{1}\in\Pi(\mathbf{\Lambda})\):

\[w_{2}d^{2}(v_{1},v_{2})+w_{3}d^{2}(v_{1},v_{3}) =w_{2}+w_{3}\] \[>w_{3}\] because

\[\mathbf{w}\in\text{int}\Delta^{2}\] \[=w_{2}d^{2}(v_{2},v_{2})+w_{3}d^{2}(v_{2},v_

[MISSING_PAGE_EMPTY:24]

Algorithms and Time Complexity Analyses

We provide time complexity analyses for Algorithms B.1, B.2, and B.3.

### Analysis of Algorithm B.1 (locus cover for phylogenetic trees)

We provide Algorithm B.1 with comments corresponding to the time complexity of each step.

```
\(\text{phylogenetic tree}\)\(T=(\mathcal{V},\mathcal{E})\), \(\mathcal{Y}=\text{Leaves}(T)\) \(N\leftarrow|\mathcal{Y}|\) \(P\leftarrow\text{sortbylength}([\Gamma(y_{i},y_{j})]_{i,i\in[N]})\)\(\triangleright\)\(N|\mathcal{E}|+N^{2}\log N\) \(P\leftarrow\text{reverse}(P)\)\(\triangleright\)\(O(N^{2})\) \(\Lambda\leftarrow\emptyset\) for\(\Gamma(y_{i},y_{j})\) in \(P\)do\(\triangleright\)\(O(N^{2})\) if\(\Pi(\Lambda)=\mathcal{Y}\)then\(\triangleright\)\(O(K^{2}D\max\{N|\mathcal{E}|,N^{2}\log N\})\) return\(\Lambda\) else\(\Lambda\leftarrow\Lambda\cup\{y_{i},y_{j}\}\) endif endfor
```

**Algorithm 1** Locus cover for phylogenetic trees

Combining these, we obtain the following time complexity:

\[O(N|\mathcal{E}|+N^{2}\log N+N^{2}+N^{2}K^{2}D\max\{N|\mathcal{E}|,N^{2}\log N \})=O(N^{2}K^{2}D\max\{N|\mathcal{E}|,N^{2}\log N\}).\]

### Analysis of Algorithm B.2 (computing a pairwise decomposable locus)

We first provide Algorithm B.2 here, with comments corresponding to the time complexity of each step.

```
\(\Lambda\), \(\mathcal{Y}\), \(G=(\mathcal{V},\mathcal{E})\) \(\Pi\leftarrow\emptyset\) \(D\leftarrow\text{diam}(G)\)\(\triangleright\)\(O(N|\mathcal{E}|+N^{2}\log N)\) for\(\lambda_{i},\lambda_{j}\in\Lambda\)do\(\triangleright\)\(O(K^{2})\) for\(w_{1}\) in \(\left\{\frac{0}{D},\frac{1}{D},...,\frac{D}{D}\right\}\)do\(\triangleright\)\(O(D)\) \(\mathbf{w}\leftarrow[w_{1},1-w_{1}]\)\(\Pi\leftarrow\Pi\cup m(\lambda_{i},\lambda_{j})(w)\)\(\triangleright\)\(O(N|\mathcal{E}|+N^{2}\log N)\) endfor endfor return\(\Pi\)
```

**Algorithm 2** Computing a pairwise decomposable locus

We first compute the diameter of the graph \(G=(\mathcal{Y},\mathcal{E})\) with \(N=|\mathcal{Y}|\), which is done in \(O(N|\mathcal{E}|+N^{2}\log N)\) time using Dijkstra's algorithm to compute the shortest paths between all pairs of vertices. We then iterate over all pairs of elements in \(\Lambda\) with \(K=|\Lambda|\), which amounts to \(O(K^{2})\) iterations. Within this, we perform \(O(D)\) computations of the Frechet mean, for which each iteration requires \(O(N|\mathcal{E}|+N^{2}\log N)\) arithmetic operations or comparisons. Combining these, the final time complexity is

\[O(N|\mathcal{E}|+N^{2}\log N+K^{2}D(N|\mathcal{E}|+N^{2}\log N))=O(K^{2}D\max\{ N|\mathcal{E}|,N^{2}\log N\}).\]

### Analysis of Algorithm B.3 (computing a generic locus)

We provide Algorithm B.3 with comments corresponding to the time complexity of each step.

Following a similar argument from the analysis of Algorithm B.2, the time complexity is

\[O(N|\mathcal{E}|+N^{2}\log N+D^{K}(N|\mathcal{E}|+N^{2}\log N))=O(D^{K}),\]where \(K\) is the number of observed classes.

## Appendix C Additional Experiments

### Additional calibration experiments

In the main text, we evaluated the effect of calibrating the Softmax outputs on the performance of Loki using a metric space based on the internal features of the CLIP model. Here, we perform the same analysis using two external metric spaces.

SetupWe perform our calibration analysis using the default and WordNet tree.

ResultsOur results are shown in Figure 4. Like our experiment using an internally-derived metric, we find that the optimal Softmax temperature is close to the CLIP default and the optimal temperature for calibration. For the default tree, we again found that temperature scaling can be used to improve accuracy using Loki.

SetupWe calibrate via Softmax temperature scaling [13] using CLIP on CIFAR-100. We do not use an external metric space, and instead use Euclidean distance applied to the CLIP text encoder.

Figure 4: **CLIP on CIFAR-100 with the WordNet hierarchy.** (Left) Reliability diagrams across a range of Softmax temperatures, highlighting the CLIP default temperature, the optimal temperature for Loki, and the minimizer of the Expected Calibration Error (ECE). All three are well-calibrated. (Center) Tradeoff between optimizing for ECE and the expected squared distance. As with the reliability diagrams, the CLIP default temperature, the Loki-optimal temperature, and the ECE-optimal temperature are similar. (Right) Tradeoff between optimizing for zero-one error and the expected squared distance. Depending on the metric space, temperature scaling can improve _accuracy_.

ResultsThe reliability diagram in Figure 3 shows that the optimal Softmax temperature for Loki is both close to the default temperature used by CLIP and to the optimally-calibrated temperature. In Figure 3 (right), we find that appropriate tuning of the temperature parameter _can lead to improved accuracy with CLIP_, even when no external metric space is available.

### Ablation of Loki formulation

Loki is based on the Frechet mean, which is defined as \(\arg\min_{y\in\mathcal{Y}}\sum_{i=1}^{K}\mathbf{P}_{\lambda_{i}|x}d^{2}(y, \lambda_{i})\). However, this is not the only approach that can be considered. For example, the Frechet _median_, often used in robust statistics, is defined as \(\arg\min_{y\in\mathcal{Y}}\sum_{i=1}^{K}\mathbf{P}_{\lambda_{i}|x}d(y,\lambda _{i})\), without squaring the distances. More generally, we define \(\arg\min_{y\in\mathcal{Y}}\sum_{i=1}^{K}\mathbf{P}_{\lambda_{i}|x}d^{\beta}(y,\lambda_{i})\) and evaluate different choices of \(\beta\).

SetupWe conduct this experiment on ImageNet using SimCLR as our pretrained classifier with \(K=\) 250, 500, and 750 randomly selected classes.

ResultsFrom this analysis, we conclude that using the Frechet mean is the optimal formulation for Loki, as it achieved the lowest mean squared distance for all settings of \(K\).

### Comparison across metric spaces

Expected squared distances cannot be directly compared across metric spaces, as they may be on different scales. Our solution is to use a form of normalization: we divide the expected squared distance by the square of the graph diameter. This brings all of the values to the 0-1 range, and since \(\mathbb{E}[d^{2}(y,\hat{y})]/\text{diam}(G)^{2}\) indeed also generalizes the 0-1 error, this enables comparison between 0-1 errors and those from different metric spaces. We provide these results in Table 1, again for our CLIP experiments on CIFAR-100. This evaluation metric enables us to determine which metric spaces have geometry best 'fit' to our pretrained models.

SetupUsing \(\mathbb{E}[d^{2}(y,\hat{y})]/\text{diam}(G)^{2}\), we re-evaluate our CLIP results on CIFAR-100 shown in Table 1 for the ResNet-50 architecture.

ResultsFor CIFAR-100, we observe that the WordNet metric space resulted in the lowest error and therefore has the best geometry.

\begin{table}
\begin{tabular}{l|l|l|l} \hline \hline \(\beta=\) & \(K=250\) & \(K=500\) & \(K=750\) \\ \hline \hline
0.5 & 61.28 & 46.57 & 36.31 \\
1 & 52.98 & 41.06 & 33.14 \\
**2 (Loki)** & **46.78** & **37.76** & **29.88** \\
4 & 47.99 & 39.58 & 34.65 \\
8 & 65.29 & 58.42 & 54.90 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Expected squared distances of SimCLR+Loki alternatives on ImageNet. We ablate over the choice of distance exponent in Loki (where \(\beta=2\), corresponding to the Fréchet mean), including the Fréchet median (\(\beta=1\)). That is, we tune \(\beta:\hat{y}\in\text{arg min}_{y\in\mathcal{Y}}\sum_{i=1}^{K}\mathbf{P}_{ \lambda_{i}|x}d^{\beta}(y,\lambda_{i})\) and find that the optimal setting is \(\beta=2\), corresponding to Loki.

\begin{table}
\begin{tabular}{l|l|l} \hline \hline Metric space \(G\) & \(\text{diam}(G)^{2}\) & \(\mathbb{E}[d^{2}(y,\hat{y})]/\text{diam}(G)^{2}\) \\ \hline \hline Complete graph & 1 & 0.5941 (0-1 error) \\ Default tree & 16 & 0.4493 \\
**WordNet tree** & 169 & **0.1157** \\ CLIP features & 0.9726 & 0.2686 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison across metric spaces for CLIP on CIFAR-100 by normalizing by the squared diameter of the metric space: \(\mathbb{E}[d^{2}(y,\hat{y})]/\text{diam}(G)^{2}\).

Experimental Details

In this section, we provide additional details about our experimental setup.3

Footnote 3: Code implementing all of our experiments is available here: [https://github.com/SproketLab/loki](https://github.com/SproketLab/loki).

### CLIP experiments

All experiments are carried out using CLIP frozen weights. There are no training and hyperparameters involved in experiments involving CLIP, except for the Softmax temperature in the calibration analysis. We evaluted using the default CIFAR-100 test set. The label prompt we use is "a photo of a [class_name]."

### ImageNet experiments

To construct our datasets, we randomly sample 50 images for each class from ImageNet as our training dataset then use the validation dataset in ImageNet to evaluate Loki's performance. We extract the image embeddings by using SimCLRv14 and train a baseline one-vs-rest classifier. We use WordNet phylogenetic tree as the metric space. The structure of phylogenetic tree can be found at here5. We test different numbers of observed classes, \(K\), from 1000 classes. Observed classes are sampled in two ways, uniformly and Gibbs distribution with the concentration parameter 0.5.

Footnote 5: [https://github.com/cvjena/semantic-embeddings/tree/master/ILSVRC](https://github.com/cvjena/semantic-embeddings/tree/master/ILSVRC)

### LSHTC experiments

We generate a summary graph of the LSHTC class graph (resulting in supernodes representing many classes) by iteratively:

1. randomly selecting a node or supernode
2. merging its neighbors into the node to create a supernode

until the graph contains at most 10,000 supernodes. In the LSHTC dataset, each datapoint is assigned to multiple classes. We push each class to its supernode, then apply majority vote to determine the final class. We test different numbers of observed classes, \(K\), from 10,000 classes. We collect datapoints which are in the observed classes. Then split half of dataset as training dataset and make the rest as testing dataset, including those datapoints which are not in the observed classes. We train a baseline classifier using a 5-NN model and compare its performance with Loki.

## Appendix E Broader Impacts and Limitations

As a simple adaptor of existing classifiers, it is possible for our method to inherit biases and failure modes that might be present in existing pretrained models. On the other hand, if the possibility of harmful mis-classifications are known a priori, information to mitigate these harms can be baked into the metric space used by Loki. Finally, while we have found that Loki often works well in practice, it is possible for the per-class probabilities that are output by a pretrained model to be sufficiently mis-specified relative to the metric over the label space, or for the metric itself to be mis-specified. Nonetheless, we have found that off-the-shelf or self-derived metric spaces to work well in practice.

## Appendix F Random Locus Visualizations

In Figures 1, 5, 6, 7, 8, and 9, we provide visualizations of classification regions of the probability simplex when using Loki with only three observed classes out of 100 total classes, and different types of random metric spaces. The first example in Figure 1 shows the prediction regions on the probability simplex when using standard \(\operatorname*{arg\,max}\) prediction--the three regions correspond to predicting one of the three classes (0, 39, and 99) and no regions corresponding to any of the other classes \(\{1,...,38,40,...,98\}\). We compute these regions using a version of Algorithm B.3, and while 

[MISSING_PAGE_FAIL:29]

Figure 8: Classification regions in the probability simplex of 3-class classifiers faced with a 100-class problem where the classes are related by random Erdős-Rényi graphs.

Figure 7: Classification regions in the probability simplex of 3-class classifiers faced with a 100-class problem where the classes are related by random small-world graphs generated by a Watts–Strogatz model.

Figure 9: Classification regions in the probability simplex of 3-class classifiers faced with a 100-class problem where the classes are related by random Barabási–Albert graphs.