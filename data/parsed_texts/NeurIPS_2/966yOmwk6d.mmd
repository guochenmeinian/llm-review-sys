# Towards Data-Algorithm Dependent Generalization:

a Case Study on Overparameterized Linear Regression

 Jing Xu

IIIS, Tsinghua University

xujing21@mails.tsinghua.edu.cn &Jiaye Teng

IIIS, Tsinghua University

tjy20@mails.tsinghua.edu.cn &Yang Yuan

IIIS, Tsinghua University

Shanghai Artificial Intelligence Laboratory

Shanghai Qi Zhi Institute

yuanyang@tsinghua.edu.cn &Andrew Chi-Chih Yao

IIIS, Tsinghua University

Shanghai Artificial Intelligence Laboratory

Shanghai Qi Zhi Institute

andrewcyao@tsinghua.edu.cn

Equal Contribution

###### Abstract

One of the major open problems in machine learning is to characterize generalization in the overparameterized regime, where most traditional generalization bounds become inconsistent even for overparameterized linear regression [46]. In many scenarios, this failure can be attributed to obscuring the crucial interplay between the training algorithm and the underlying data distribution. This paper demonstrate that the generalization behavior of overparameterized model should be analyzed in a both data-relevant and algorithm-relevant manner. To make a formal characterization, We introduce a notion called data-algorithm compatibility, which considers the generalization behavior of the entire data-dependent training trajectory, instead of traditional last-iterate analysis. We validate our claim by studying the setting of solving overparameterized linear regression with gradient descent. Specifically, we perform a data-dependent trajectory analysis and derive a sufficient condition for compatibility in such a setting. Our theoretical results demonstrate that if we take early stopping iterates into consideration, generalization can hold with significantly weaker restrictions on the problem instance than the previous last-iterate analysis.

## 1 Introduction

Although deep neural networks achieve great success in practice [13, 14, 63], their remarkable generalization ability is still among the essential mysteries in the deep learning community. One of the most intriguing features of deep neural networks is overparameterization, which confers a level of tractability to the training problem, but leaves traditional generalization theories failing to work. In generalization analysis, both the training algorithm and the data distribution play essential roles [23, 28]. For instance, a line of work [46, 74] highlights the role of the algorithm by showing that the algorithm-irrelevant uniform convergence bounds can become inconsistent in deep learning regimes. Another line of work [8, 68] on benign overfitting emphasizes the role of data distribution via profound analysis of specific overparameterized models.

Despite the significant role of data and algorithm in generalization analysis, existing theories usually focus on either the data factor (_e.g._, uniform convergence [46] and last iterate analysis [8, 68]) or the algorithm factor (_e.g._, stability-based bounds [24]). Combining both data and algorithm factor intogeneralization analysis can help derive tighter generalization bounds and explain the generalization ability of overparameterized models observed in practice. In this sense, a natural question arises:

_How to incorporate both data factor and algorithm factor into generalization analysis?_

To gain insight into the interplay between data and algorithms, we provide motivating examples of a synthetic overparameterized linear regression task and a classification task on the corrupted MNIST dataset in figure 1. In both scenarios, the final iterate with less algorithmic information, which may include the algorithm type (_e.g._, GD or SGD), hyperparameters (_e.g._, learning rate, number of epochs), generalizes much worse than the early stopping solutions (see the Blue Line). In the linear regression case, the generalization error of the final iterate can be more than \(\times 100\) larger than that of the early stopping solution. In the MNIST case, the final iterate on the SGD trajectory has 19.9% test error, much higher than the 2.88% test error of the best iterate on the GD trajectory. Therefore, the almost ubiquitous strategy of early stopping is a key ingredient in generalization analysis for overparameterized models, whose benefits have been demonstrated both theoretically and empirically [1, 2, 27, 34, 41, 72]. By focusing on the entire optimization trajectory and performing data-dependent trajectory analysis, both data information and the dynamics of the training algorithm can be exploited to yield consistent generalization bounds.

When we take the algorithm into consideration and analyze the data-dependent training trajectory, generalization occurs if the minimum excess risk of the iterates on the training trajectory converges to zero, as the sample size tends to infinity. This accords with the real practice of training deep neural networks, where one can pick up the best parameter on the training trajectory, by calculating its loss on a validation dataset. We dub this notion of generalization as as _data-algorithm-compatibility_, which is formally defined in Section 3.2.

The significance of compatibility comes in three folds. Firstly, it incorporates both data and algorithm factors into generalization analysis, and is suitable for the overparameterization regime (see Definition 3.1). Secondly, it serves as a minimal condition for generalization, without which one cannot expect to find a consistent solution via standard learning procedures. Consequently, compatibility holds with only mild assumptions and applies to a wide range of problem instances (see Theorem 4.1). Thirdly, it captures the algorithmic significance of early stopping in generalization. By exploiting the algorithm information along the entire trajectory, we arrive at better generalization bounds than the last-iterate analysis (see Table 1 and 2 for examples).

To theoretically validate compatibility, we study it under overparameterized linear regression setting. Analysis of the overparameterized linear regression is a reasonable starting point to study more complex models like deep neural networks [17, 57], since many phenomena of the high dimensional non-linear model are also observed in the linear regime (_e.g._, Figure 1). Furthermore, the neural

Figure 1: **(a) The training plot for linear regression with spectrum \(\lambda_{i}=1/i^{2}\) using GD.** Note that the axes are in the log scale. **(b) The training plot of CNN on corrupted MNIST with 20% label noise using SGD.** Both models successfully learn the useful features in the initial phase of training, but it takes a long time for them to fit the noise in the dataset. The observations demonstrate the power of data-dependent trajectory analysis, since the early stopping solutions on the trajectory generalize well but the final iterate fails to generalize. See Appendix C for details.

tangent kernel (NTK) framework [4; 26] demonstrates that very wide neural networks trained using gradient descent with appropriate random initialization can be approximated by kernel regression in a reproducing kernel Hilbert space, which rigorously establishes a close relationship between overparameterized linear regression and deep neural network training.

Specifically, we investigate solving overparameterized linear regression using gradient descent with constant step size, and prove that under some mild regularity conditions, gradient descent is compatible with overparameterized linear regression if the effective dimensions of the feature covariance matrix are asymptotically bounded by the sample size. Compared with the last-iterate analysis [8], the main theorems in this paper require significantly weaker assumptions, which demonstrates the benefits of data-relevant and algorithm-relevant generalization analysis.

We summarize our contributions as follows:

* We formalize the notion of data-algorithm-compatibility, which highlights the interaction between data and algorithm and serves as a minimal condition for generalization.
* We derive a sufficient condition for compatibility in solving overparameterized linear regression with gradient descent. Our theory shows that generalization of early-stopping iterates requires much weaker restrictions in the considered setting.
* Technically, we derive time-variant generalization bounds for overparameterized linear regression via data-dependent trajectory analysis. Empirically, we conduct the various experiments to verify the the theoretical results and demonstrate the benefits of early stopping.

## 2 Related Works

**Data-Dependent Techniques** mainly focus on the data distribution condition for generalization. One of the most popular data-dependent methods is uniform convergence [7; 29; 74; 75]. However, recent works [46; 47] point out that uniform convergence may not be powerful enough to explain generalization, because it may only yield inconsistent bound in even linear regression cases. Another line of works investigates benign overfitting, which mainly studies generalization of overfitting solutions [8; 21; 35; 68; 70; 76; 77].

**Algorithm-Dependent Techniques** measure the role of the algorithmic information in generalization. A line of works derives generalization bounds via algorithm stability [9; 12; 18; 19; 24; 31; 32; 45; 67]. A parallel line of works analyzes the implicit bias of algorithmic information [11; 25; 39; 40; 59; 64], which are mainly based on analyzing a specific data distribution (_e.g._, linear separable).

**Other Generalization Techniques.** Besides the techniques above, there are many other approaches. For example, PAC-Bayes theory performs well empirically and theoretically [16; 42; 43; 48; 49; 58; 61] and can yield non-vacuous bounds in deep learning regimes [50; 54]. Furthermore, there are other promising techniques including information theory [6; 56; 71] and compression-based bounds [3].

**Early Stopping** has the potential to improve generalization for various machine learning problems [5; 30; 33; 38; 53; 62; 69; 74]. A line of works studies the rate of early stopping in linear regression and kernel regression with different algorithms, _e.g._, gradient descent [72], stochastic gradient descent [15; 37; 51; 55; 66], gradient flow [2], conjugate gradient [10] and spectral algorithms [22; 36]. Beyond linear models, early-stopping is also effective for training deep neural networks [27; 34]. Another line of research focuses on the signal for early stopping [20; 52].

## 3 Preliminaries

In this section, we formally define compatibility between the data distribution and the training algorithm, starting from the basic notations.

### Notations

**Data Distribution.** Let \(\mathcal{D}\) denote the population distribution and \(\mathbf{z}\sim\mathcal{D}\) denote a data point sampled from distribution \(\mathcal{D}\). Usually, \(\mathbf{z}\) contains a feature and its corresponding response. Besides, we denote the dataset with \(n\) samples as \(\mathbf{Z}\triangleq\{\mathbf{z}_{i}\}_{i\in[n]}\), where \(\mathbf{z}_{i}\sim\mathcal{D}\) are i.i.d. sampled from distribution \(\mathcal{D}\).

**Loss and Excess Risk.** Let \(\ell(\mathbf{\theta};\mathbf{z})\) denote the loss on sample \(\mathbf{z}\) with parameter \(\mathbf{\theta}\in\mathbb{R}^{p}\). The corresponding population loss is defined as \(L(\mathbf{\theta};\mathcal{D})\triangleq\mathbb{E}_{\mathbf{z}\sim\mathcal{D}}\ell(\mathbf{ \theta};\mathbf{z})\). When the context is clear, we omit the dependency on \(\mathcal{D}\) and denote the population loss by \(L(\mathbf{\theta})\). Our goal is to find the optimal parameter \(\mathbf{\theta}^{*}\) which minimizes the population loss, i.e., \(L(\mathbf{\theta}^{*})=\min_{\mathbf{\theta}}L(\mathbf{\theta})\). Measuring how a parameter \(\mathbf{\theta}\) approaches \(\mathbf{\theta}^{*}\) relies on a term _excess risk_\(R(\mathbf{\theta})\), defined as \(R(\mathbf{\theta})\triangleq L(\mathbf{\theta})-L(\mathbf{\theta}^{*})\).

**Algorithm.** Let \(\mathcal{A}(\cdot)\) denote a iterative algorithm that takes training data \(\mathbf{Z}\) as input and outputs a sequence of parameters \(\{\mathbf{\theta}_{n}^{(t)}\}_{t\geq 0}\), where \(t\) is the iteration number. The algorithm can be either deterministic or stochastic, _e.g._, variants of (S)GD.

### Definitions of Compatibility

Based on the above notations, we introduce the notion of compatibility between data distribution and algorithm in Definition 3.1. Informally, compatibility measures whether a consistent excess risk can be reached along the training trajectory. Note that we omit the role of the loss function in the definition, although the algorithm depends on the loss function.

**Definition 3.1** (Compatibility).: _Given a loss function \(\ell(\cdot)\) with corresponding excess risk \(R(\cdot)\), a data distribution \(\mathcal{D}\) is compatible with an algorithm \(\mathcal{A}\) if there exists nonempty subsets \(T_{n}\) of \(\mathbb{N}\), such that \(\sup_{t\in T_{n}}R(\mathbf{\theta}_{n}^{(t)})\) converges to zero in probability as sample size \(n\) tends to infinity, where \(\{\mathbf{\theta}_{n}^{(t)}\}_{t\geq 0}\) denotes the output of algorithm \(\mathcal{A}\), and the randomness comes from the sampling of training data \(\mathbf{Z}\) from distribution \(\mathcal{D}\) and the execution of algorithm \(\mathcal{A}\). That is to say, \((\mathcal{D},\mathcal{A})\) is compatible if there exists nonempty sets \(T_{n}\), such that_

\[\sup_{t\in T_{n}}R(\mathbf{\theta}_{n}^{(t)})\overset{P}{\rightarrow}0\quad\text{ as}\quad n\rightarrow\infty. \tag{1}\]

_We call \(\{T_{n}\}_{n>0}\) the compatibility region of \((\mathcal{D},\mathcal{A})\). The distribution \(\mathcal{D}\) is allowed to change with \(n\). In this case, \(\mathcal{D}\) should be understood as a sequence of distributions \(\{\mathcal{D}_{n}\}_{n\geq 1}\). We also allow the dimension of model parameter \(\mathbf{\theta}\) to be infinity or to grow with \(n\). We omit this dependency on \(n\) when the context is clear._

Compatibility serves as a minimal condition for generalization, since if a data distribution is incompatible with the algorithm, one cannot expect to reach a small excess risk even if we allow for _arbitrary_ early stopping. However, we remark that considering only the minimal excess risk is insufficient for a practical purpose, as one cannot exactly find the \(t\) that minimizes \(R(\mathbf{\theta}_{n}^{(t)})\) due to the noise in the validation set. Therefore, it is meaningful to consider a region of time \(t\) on which the excess risk is consistent as in Definition 3.1. The larger the region is, the more robust the algorithm will be to the noise in its execution.

**Comparisons with Other Notions.** Compared to classic definitions of learnability, _e.g._, PAC learning, the definition of compatibility is data-specific and algorithm-specific, and is thus a more fine-grained notion. Compared to the concept of _benign_ proposed in [8], which studies whether the excess risk at \(t=\infty\) converges to zero in probability as the sample size goes to infinity, compatibility only requires that there exists a time to derive a consistent excess risk. We will show later in Section 4.2 that in the overparameterized linear regression setting, there exist cases such that the problem instance is compatible but not benign.

## 4 Analysis of Overparameterized Linear Regression with Gradient Descent

To validate the meaningfulness of compatibility, we study it in the overparameterized linear regression regime. We first introduce the data distribution, loss, and training algorithm, and then present the main theorem, which provides a sufficient condition for compatibility in this setting.

### Preliminaries for Overparameterized Linear Regression

**Notations.** Let \(O,o,\Omega,\omega\) denote asymptotic notations, with their usual meaning. For example, the argument \(a_{n}=O(b_{n})\) means that there exists a large enough constant \(C\), such that \(a_{n}\leq Cb_{n}\). We use \(\lesssim\) with the same meaning as the asymptotic notation \(O\). Besides, let \(\|\mathbf{x}\|\) denote the \(\ell_{2}\) norm for vector \(\mathbf{x}\), and \(\|\mathbf{A}\|\) denote the operator norm for matrix \(\mathbf{A}\). We allow the vector to belong toa countably infinite-dimensional Hilbert space \(\mathcal{H}\), and with a slight abuse of notation, we use \(\mathbb{R}^{\infty}\) interchangeably with \(\mathcal{H}\). In this case, \(x^{\top}z\) denotes inner product and \(xz^{\top}\) denotes tensor product for \(x,z\in\mathcal{H}\). A random variable \(X\) is called \(\sigma\)-subgaussian if \(\mathbb{E}[e^{\lambda X}]\leq e^{\lambda^{2}\sigma^{2}/2}\) for any \(\lambda\).

**Data Distribution.** Let \((\mathbf{x},y)\in\mathbb{R}^{p}\times\mathbb{R}\) denote the feature vector and the response, following a joint distribution \(\mathcal{D}\). Let \(\mathbf{\Sigma}\triangleq\mathbb{E}[\mathbf{x}\mathbf{x}^{\top}]\) denote the feature covariance matrix, whose eigenvalue decomposition is \(\mathbf{\Sigma}=\mathbf{V}\mathbf{\Lambda}\mathbf{V}^{\top}=\sum_{i>0}\lambda_{i}\mathbf{v}_{i}\bm {v}_{i}^{\top}\) with decreasing eigenvalues \(\lambda_{1}\geq\lambda_{2}\geq\cdots\). We make the following assumptions on the distribution of the feature vector.

**Assumption 1** (Assumptions on feature distribution).: _We assume that_

1. \(\mathbb{E}[\mathbf{x}]=0\)_._
2. \(\lambda_{1}>0,\sum_{i>0}\lambda_{i}<C\) _for some absolute constant_ \(C\)_._
3. _Let_ \(\tilde{\mathbf{x}}=\mathbf{\Lambda}^{-\frac{1}{2}}\mathbf{V}^{\top}\mathbf{x}\)_. The random vector_ \(\tilde{\mathbf{x}}\) _has independent_ \(\sigma_{x}\)_-subgaussian entries._

**Loss and Excess Risk.** We choose square loss as the loss function \(\ell\), i.e. \(\ell(\mathbf{\theta},(\mathbf{x},y))=1/2(y-\mathbf{x}^{\top}\mathbf{\theta})^{2}\). The corresponding population loss is denoted by \(L(\mathbf{\theta})=\mathbb{E}\ell(\mathbf{\theta},(\mathbf{x},y))\) and the optimal parameter is denoted by \(\mathbf{\theta}^{*}\triangleq\operatorname*{argmin}_{\mathbf{\theta}\in\mathbb{R}^{p}} L(\mathbf{\theta})\). We assume that \(\|\mathbf{\theta}^{*}\|<C\) for some absolute constant \(C\). If there are multiple such minimizers, we choose an arbitrary one and fix it thereafter. We focus on the excess risk of parameter \(\mathbf{\theta}\), defined as

\[R(\mathbf{\theta})=L(\mathbf{\theta})-L(\mathbf{\theta}^{*})=\frac{1}{2}(\mathbf{\theta}-\mathbf{ \theta}^{*})^{\top}\mathbf{\Sigma}(\mathbf{\theta}-\mathbf{\theta}^{*}). \tag{2}\]

Let \(\varepsilon=y-\mathbf{x}^{\top}\mathbf{\theta}^{*}\) denote the noise in data point \((\mathbf{x},y)\). The following assumptions involve the conditional distribution of the noise.

**Assumption 2** (Assumptions on noise distribution).: _We assume that_

1. _The conditional noise_ \(\varepsilon|\mathbf{x}\) _has zero mean._
2. _The conditional noise_ \(\varepsilon|\mathbf{x}\) _is_ \(\sigma_{y}\)_-subgaussian._

Note that both Assumption 1 and Assumption 2 are commonly considered in the related literatures [8, 68, 76].

**Training Set.** Given a training set \(\{(\mathbf{x}_{i},y_{i})\}_{1\leq i\leq n}\) with \(n\) pairs independently sampled from the population distribution \(\mathcal{D}\), we define \(\mathbf{X}\triangleq(\mathbf{x}_{1},\cdots,\mathbf{x}_{n})^{\top}\in\mathbb{R}^{n\times p}\) as the feature matrix, \(\mathbf{Y}\triangleq(y_{1},\cdots,y_{n})^{\top}\in\mathbb{R}^{n}\) as the corresponding noise vector, and \(\mathbf{\varepsilon}\triangleq\mathbf{Y}-\mathbf{X}\mathbf{\theta}^{*}\) as the residual vector. Let the singular value decomposition (SVD) of \(\mathbf{X}\) be \(\mathbf{X}=\mathbf{U}\mathbf{\tilde{\Lambda}}^{\frac{1}{2}}\mathbf{W}^{\top}\), with \(\mathbf{\tilde{\Lambda}}=\text{diag}\{\mu_{1}\,\cdots,\mu_{n}\}\in\mathbb{R}^{n \times n}\), \(\mu_{1}\geq\cdots\geq\mu_{n}\).

We consider the overparameterized regime where the feature dimension is larger than the sample size, namely, \(p>n\). In this regime, we assume that \(\text{rank}(\mathbf{X})=n\) almost surely as in Bartlett et al. [8]. This assumption is equivalent to the invertibility of \(XX^{\top}\).

**Assumption 3** (Linear independent training set).: _For any \(n<p\), we assume that the features in the training set \(\{\mathbf{x}_{1},\mathbf{x}_{2},\cdots,\mathbf{x}_{n}\}\) is linearly independent almost surely._

**Algorithm.** Given the dataset \((\mathbf{X},\mathbf{Y})\), define the empirical loss function as \(\hat{L}(\mathbf{\theta})\triangleq\frac{1}{2n}\|\mathbf{X}\mathbf{\theta}-\mathbf{Y}\|^{2}\). We choose full-batch gradient descent on the empirical risk with a constant learning rate \(\lambda\) as the algorithm \(\mathcal{A}\) in the previous template. In this case, the update rule for the optimization trajectory \(\{\mathbf{\theta}_{t}\}_{t\geq 0}\) is formulated as

\[\mathbf{\theta}_{t+1}=\mathbf{\theta}_{t}-\frac{\lambda}{n}\mathbf{X}^{\top}(\mathbf{X}\mathbf{ \theta}_{t}-\mathbf{Y}). \tag{3}\]

Without loss of generality, we consider zero initialization \(\mathbf{\theta}_{0}=\mathbf{0}\) in this paper. In this case, for a sufficiently small learning rate \(\lambda\), \(\mathbf{\theta}_{t}\) converges to the _min-norm interpolator_\(\hat{\mathbf{\theta}}=\mathbf{X}^{\top}(\mathbf{X}\mathbf{X}^{\top})^{-1}\mathbf{Y}\) as \(t\) goes to infinity, which was well studied previously [8]. This paper takes one step further and discuss the excess risk along the entire training trajectory \(\{R(\mathbf{\theta}_{t})\}_{t\geq 0}\).

**Effective Rank and Effective Dimensions.** We define the effective ranks of the feature matrix \(\mathbf{\Sigma}\) as \(r(\mathbf{\Sigma})\triangleq\frac{\sum_{i>0}\lambda_{i}}{\lambda_{1}}\) and \(R_{k}(\mathbf{\Sigma})\triangleq\frac{\left(\sum_{i>k}\lambda_{i}\right)^{2}}{\sum_{ i>k}\lambda_{1}^{2}}\). Our results depend on two notions of effective dimension of the feature covariance \(\mathbf{\Sigma}\), defined as

\[k_{0} \triangleq\min\left\{l\geq 0:\lambda_{l+1}\leq\frac{c_{0}\sum_{i>l} \lambda_{i}}{n}\right\}, \tag{4}\] \[k_{1} \triangleq\min\left\{l\geq 0:\lambda_{l+1}\leq\frac{c_{1}\sum_{i>0} \lambda_{i}}{n}\right\}, \tag{5}\]

where \(c_{0},c_{1}\) are constants independent of the dimension \(p\), sample size \(n\), and time \(t\)1. We omit the dependency of \(k_{0},k_{1}\) on \(c_{0},c_{1},n,\mathbf{\Sigma}\) when the context is clear.

Footnote 1: Constants may depend on \(\sigma_{x}\), and we omit this dependency thereafter for clarity.

### Main Theorem for Overparameterized Linear Regression with Gradient Descent

Next, we present the main result of this section, which provides a clean condition for compatibility between gradient descent and overparameterized linear regression.

**Theorem 4.1**.: _Consider the overparameterized linear regression setting defined in section 4.1. Let Assumption 1,2 and 3 hold. Assume the learning rate satisfies \(\lambda=O\left(\frac{1}{\operatorname{Tr}(\mathbf{\Sigma})}\right)\)._

* _If the covariance satisfies_ \(k_{0}=o(n),R_{k_{0}}(\mathbf{\Sigma})=\omega(n),\;r(\mathbf{\Sigma})=o(n)\)_, it is compatible in the region_ \(T_{n}=\left(\omega\left(\frac{1}{\lambda}\right),\infty\right)\)_._
* _If the covariance satisfies_ \(k_{0}=O(n),k_{1}=o(n),r(\mathbf{\Sigma})=o(n)\)_, it is compatible in the region_ \(T_{n}=\left(\omega\left(\frac{1}{\lambda}\right),o\left(\frac{n}{\lambda}\right)\right)\)_._
* _If the covariance does not change with_ \(n\)_, and satisfies_ \(k_{0}=O(n)\) _and_ \(p=\infty\)_, it is compatible in the region_ \(T_{n}=\left(\omega\left(\frac{1}{\lambda}\right),o\left(\frac{n}{\lambda} \right)\right)\)_._

The proof of Theorem 4.1 is given in Appendix A and sketched in Section 5. The theorem shows that gradient descent is compatible with overparameterized linear regression under some mild regularity conditions on the learning rate, effective rank and effective dimensions. The condition on the learning rate is natural for optimizing a smooth objective. We conjecture that the condition \(k_{0}=O(n)\) can not be removed in general cases, since the effective dimension \(k_{0}\) characterizes the concentration of the singular values of the data matrix \(\mathbf{X}\) and plays a crucial role in the excess risk of the gradient descent dynamics.

**Comparison with Benign Overfitting.** The paper [8] studies overparameterized linear regression and gives the condition for min-norm interpolator to generalize. They prove that the feature covariance \(\mathbf{\Sigma}\) is benign if and only if

\[k_{0}=o(n),\;R_{k_{0}}(\mathbf{\Sigma})=\omega(n),\;r(\mathbf{\Sigma})=o(n) \tag{6}\]

As discussed in Section 3.2, benign problem instance also satisfies compatibility, since benign overfitting requires a stronger condition on \(k_{0}\) and an additional assumption on \(R_{k_{0}}(\mathbf{\Sigma})\). The following example shows that this inclusion relationship is strict.

**Example 4.1**.: _Under the same assumption as in Theorem 4.1, if the spectrum of \(\mathbf{\Sigma}\) satisfies_

\[\lambda_{k}=\frac{1}{k^{\alpha}}, \tag{7}\]

_for some \(\alpha>1\), we derive that \(k_{0}=\Theta(n)\). Therefore, this problem instance satisfies compatibility, but does not satisfy benign overfitting._

Example 4.1 shows the existence of a case where the early stopping solution can generalize but interpolating solution fails. Therefore, Theorem 4.1 can characterize generalization for a much wider range of problem instances.

[MISSING_PAGE_FAIL:7]

_Furthermore, for any positive constant \(\delta\), with probability at least \(1-\delta\), the minimal excess risk on the training trajectory can be bounded as_

\[\min_{t}R(\mathbf{\theta}_{t})\lesssim\frac{\max\{\sqrt{r(\mathbf{\Sigma})},1\}}{\sqrt{n} }+\frac{\max\{k_{1},1\}}{n}. \tag{11}\]

Lemma 5.1 below shows that \(k_{1}=o(n)\) always holds for fixed distribution. Therefore, combining Corollary 5.1 and the following Lemma 5.1 completes the proof of Theorem 4.1.

**Lemma 5.1**.: _For any fixed (i.e. independent of sample size \(n\)) feature covariance \(\mathbf{\Sigma}\) satisfying assumption 1, we have \(k_{1}(n)=o(n)\)._

Next we apply the bound in Corollary 5.1 to several data distributions. These distributions are widely discussed in [8, 76]. We also derive the existing excess risk bounds, which focus on the min-norm interpolator [8] and one-pass SGD iterate [76], of these distributions and compare them with our theorem. The results are summarized in Table 1, which shows that the bound in Corollary 5.1 outperforms previous results for a general class of distributions.

**Example 5.1**.: _Under the same conditions as Theorem 5.1, let \(\mathbf{\Sigma}\) denote the feature covariance matrix. We show the following examples:_

1. **(Inverse Polynomial).** _If the spectrum of_ \(\mathbf{\Sigma}\) _satisfies_ \(\lambda_{k}=\frac{1}{k^{\alpha}}\) _for some_ \(\alpha>1\)_, we derive that_ \(k_{0}=\Theta(n)\)_,_ \(k_{1}=\Theta\left(n^{\frac{1}{\alpha}}\right)\)_. Therefore,_ \(\min_{t}V(\mathbf{\theta}_{t})=O\left(n^{\frac{1-\alpha}{\alpha}}\right)\) _and_ \(\min_{t}R(\mathbf{\theta}_{t})=O\left(n^{-\min\left\{\frac{n-1}{\alpha},\frac{1}{2} \right\}}\right)\)_._
2. **(Inverse Log-Polynomial).** _If the spectrum of_ \(\mathbf{\Sigma}\) _satisfies_ \(\lambda_{k}=\frac{1}{k\log^{q}(k+1)}\) _for some_ \(\beta>1\)_, we derive that_ \(k_{0}=\Theta\left(\frac{n}{\log n}\right)\)_,_ \(k_{1}=\Theta\left(\frac{n}{\log^{q}n}\right)\)_. Therefore,_ \(\min_{t}V(\mathbf{\theta}_{t})=O\left(\frac{1}{\log^{q}n}\right)\) _and_ \(\min_{t}R(\mathbf{\theta}_{t})=O\left(\frac{1}{\log^{q}n}\right)\)_._
3. **(Constant).** _If the spectrum of_ \(\mathbf{\Sigma}\) _satisfies_ \(\lambda_{k}=\frac{1}{n^{1+\varepsilon}},1\leq k\leq n^{1+\varepsilon}\)_, for some_ \(\varepsilon>0\)_, we derive that_ \(k_{0}=0\)_,_ \(k_{1}=0\)_. Therefore,_ \(\min_{t}V(\mathbf{\theta}_{t})=O\left(\frac{1}{n}\right)\) _and_ \(\min_{t}R(\mathbf{\theta}_{t})=O\left(\frac{1}{\sqrt{n}}\right)\)_._
4. **(Piecewise Constant).** _If the spectrum of_ \(\mathbf{\Sigma}\) _satisfies_ \(\lambda_{k}=\left\{\frac{1}{\frac{s}{d-s}}\quad\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,

[MISSING_PAGE_EMPTY:9]

Conclusion

In this paper, we investigate how to characterize and analyze generalization in a data-dependent and algorithm-dependent manner. We formalize the notion of data-algorithm compatibility and study it under the regime of overparameterized linear regression with gradient descent. Our theoretical and empirical results demonstrate that one can ease the assumptions and broaden the scope of generalization by fully exploiting the data information and the algorithm information. Despite linear cases in this paper, compatibility can be a much more general concept. Therefore, we believe this paper will motivate more work on data-dependent trajectory analysis.

## Acknowledgement

The authors would like to acknowledge the support from the 2030 Innovation Megaprojects of China (Programme on New Generation Artificial Intelligence) under Grant No. 2021AAA0150000.

## References

* Advani et al. [2020] Madhu S. Advani, Andrew M. Saxe, and Haim Sompolinsky. High-dimensional dynamics of generalization error in neural networks. _Neural Networks_, 132:428-446, 2020. doi: 10.1016/j.neunet.2020.08.022. URL [https://doi.org/10.1016/j.neunet.2020.08.022](https://doi.org/10.1016/j.neunet.2020.08.022).
* Ali et al. [2019] Alnur Ali, J. Zico Kolter, and Ryan J. Tibshirani. A continuous-time view of early stopping for least squares regression. In Kamalika Chaudhuri and Masashi Sugiyama, editors, _The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan_, volume 89 of _Proceedings of Machine Learning Research_, pages 1370-1378. PMLR, 2019. URL [http://proceedings.mlr.press/v89/ali19a.html](http://proceedings.mlr.press/v89/ali19a.html).
* Arora et al. [2018] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In Jennifer G. Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 254-263. PMLR, 2018. URL [http://proceedings.mlr.press/v80/arora18b.html](http://proceedings.mlr.press/v80/arora18b.html).
* Arora et al. [2019] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 322-332. PMLR, 2019. URL [http://proceedings.mlr.press/v97/arora19a.html](http://proceedings.mlr.press/v97/arora19a.html).
* Bai et al. [2021] Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels. _CoRR_, abs/2106.15853, 2021. URL [https://arxiv.org/abs/2106.15853](https://arxiv.org/abs/2106.15853).
* Banerjee and Montufar [2021] Pradeep Kr. Banerjee and Guido Montufar. Information complexity and generalization bounds. _CoRR_, abs/2105.01747, 2021. URL [https://arxiv.org/abs/2105.01747](https://arxiv.org/abs/2105.01747).
* Bartlett et al. [2017] Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 6240-6249, 2017. URL [https://proceedings.neurips.cc/paper/2017/hash/b22b257ad0519d4500539da3c8bcf4dd-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/b22b257ad0519d4500539da3c8bcf4dd-Abstract.html).
* Bartlett et al. [2019] Peter L. Bartlett, Philip M. Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. _CoRR_, abs/1906.11300, 2019. URL [http://arxiv.org/abs/1906.11300](http://arxiv.org/abs/1906.11300).

* Bassily et al. [2020] Raef Bassily, Vitaly Feldman, Cristobal Guzman, and Kunal Talwar. Stability of stochastic gradient descent on nonsmooth convex losses. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/2e2c4bf7ceaa4712a72dd5ee136dc9a8-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/2e2c4bf7ceaa4712a72dd5ee136dc9a8-Abstract.html).
* Blanchard and Kramer [2016] Gilles Blanchard and Nicole Kramer. Convergence rates of kernel conjugate gradient for random design regression. _Analysis and Applications_, 14(06):763-794, 2016.
* Bousquet and Elisseeff [2002] Olivier Bousquet and Andre Elisseeff. Stability and generalization. _J. Mach. Learn. Res._, 2:499-526, 2002. URL [http://jmlr.org/papers/v2/bousquet02a.html](http://jmlr.org/papers/v2/bousquet02a.html).
* Bousquet et al. [2020] Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable algorithms. In Jacob D. Abernethy and Shivani Agarwal, editors, _Conference on Learning Theory, COLT 2020, 9-12 July 2020, Virtual Event [Graz, Austria]_, volume 125 of _Proceedings of Machine Learning Research_, pages 610-626. PMLR, 2020. URL [http://proceedings.mlr.press/v125/bousquet20b.html](http://proceedings.mlr.press/v125/bousquet20b.html).
* Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/1457cd06bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457cd06bfcb4967418bfb8ac142f64a-Abstract.html).
* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pages 4171-4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423).
* Dieuleveut and Bach [2016] Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-sizes. _The Annals of Statistics_, 44(4):1363-1399, 2016.
* Dziugaite and Roy [2017] Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Gal Elidan, Kristian Kersting, and Alexander T. Ihler, editors, _Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017, Sydney, Australia, August 11-15, 2017_. AUAI Press, 2017. URL [http://auai.org/uai2017/proceedings/papers/173.pdf](http://auai.org/uai2017/proceedings/papers/173.pdf).
* Emami et al. [2020] Melikasadat Emami, Mojtaba Sahraee-Ardakan, Parthe Pandit, Sundeep Rangan, and Alyson K. Fletcher. Generalization error of generalized linear models in high dimensions. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 2892-2901. PMLR, 2020. URL [http://proceedings.mlr.press/v119/emami20a.html](http://proceedings.mlr.press/v119/emami20a.html).
* Feldman and Vondrak [2018] Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 9770-9780, 2018. URL [https://proceedings.neurips.cc/paper/2018/hash/05a624166c8eb8273b8464e8d9cb5bbd9-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/05a624166c8eb8273b8464e8d9cb5bbd9-Abstract.html).

* Feldman and Vondrak [2019] Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In Alina Beygelzimer and Daniel Hsu, editors, _Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA_, volume 99 of _Proceedings of Machine Learning Research_, pages 1270-1279. PMLR, 2019. URL [http://proceedings.mlr.press/v99/feldman19a.html](http://proceedings.mlr.press/v99/feldman19a.html).
* European Conference, ECML PKDD 2021, Bilbao, Spain, September 13-17, 2021, Proceedings, Part II_, volume 12976 of _Lecture Notes in Computer Science_, pages 217-232. Springer, 2021. doi: 10.1007/978-3-030-86520-7_14. URL [https://doi.org/10.1007/978-3-030-86520-7_14](https://doi.org/10.1007/978-3-030-86520-7_14).
* Frei et al. [2022] Spencer Frei, Niladri S. Chatterji, and Peter L. Bartlett. Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. _CoRR_, abs/2202.05928, 2022. URL [https://arxiv.org/abs/2202.05928](https://arxiv.org/abs/2202.05928).
* de Forgo et al. [2008] L. Lo Gerfo, Lorenzo Rosasco, Francesca Odone, Ernesto De Vito, and Alessandro Verri. Spectral algorithms for supervised learning. _Neural Comput._, 20(7):1873-1897, 2008. doi: 10.1162/neco.2008.05-07-517. URL [https://doi.org/10.1162/neco.2008.05-07-517](https://doi.org/10.1162/neco.2008.05-07-517).
* Goldt et al. [2019] Sebastian Goldt, Madhu Advani, Andrew M. Saxe, Florent Krzakala, and Lenka Zdeborova. Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 6979-6989, 2019. URL [https://proceedings.neurips.cc/paper/2019/hash/cab070d53bd0d200746fb852a922064a-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/cab070d53bd0d200746fb852a922064a-Abstract.html).
* Hardt et al. [2016] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, _Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016_, volume 48 of _JMLR Workshop and Conference Proceedings_, pages 1225-1234. JMLR.org, 2016. URL [http://proceedings.mlr.press/v48/hardt16.html](http://proceedings.mlr.press/v48/hardt16.html).
* Hu et al. [2020] Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the early-time learning dynamics of neural networks. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/c6dfc6b7c601ac2978357b7a81e2d7ae-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/c6dfc6b7c601ac2978357b7a81e2d7ae-Abstract.html).
* Jacot et al. [2018] Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and generalization in neural networks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 8580-8589, 2018. URL [https://proceedings.neurips.cc/paper/2018/hash/5a4be1fa34e62bb8a6ec6b91d246215a-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/5a4be1fa34e62bb8a6ec6b91d246215a-Abstract.html).
* Ji et al. [2021] Ziwei Ji, Justin D. Li, and Matus Telgarsky. Early-stopped neural networks are consistent. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 1805-1817, 2021. URL [https://proceedings.neurips.cc/paper/2021/hash/0e1ebad68af770ae4830b7ac92bc3c6f-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/0e1ebad68af770ae4830b7ac92bc3c6f-Abstract.html).
* Jiang et al. [2020] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020. URL [https://openreview.net/forum?id=SJgIPJBFvH](https://openreview.net/forum?id=SJgIPJBFvH)* Koltchinskii and Panchenko [2000] Vladimir Koltchinskii and Dmitriy Panchenko. Rademacher processes and bounding the risk of function learning. In _High dimensional probability II_, pages 443-457. Springer, 2000.
* Kuzborskij and Szepesvari [2021] Ilja Kuzborskij and Csaba Szepesvari. Nonparametric regression with shallow overparameterized neural networks trained by GD with early stopping. In Mikhail Belkin and Samory Kpotufe, editors, _Conference on Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado, USA_, volume 134 of _Proceedings of Machine Learning Research_, pages 2853-2890. PMLR, 2021. URL [http://proceedings.mlr.press/v134/kuzborskij21a.html](http://proceedings.mlr.press/v134/kuzborskij21a.html).
* Lei and Ying [2020] Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic gradient descent. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 5809-5819. PMLR, 2020. URL [http://proceedings.mlr.press/v119/lei20c.html](http://proceedings.mlr.press/v119/lei20c.html).
* Li et al. [2020] Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods for non-convex learning. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020._ OpenReview.net, 2020. URL [https://openreview.net/forum?id=SkxxtgHKPS](https://openreview.net/forum?id=SkxxtgHKPS).
* Li et al. [2021] Jiangyuan Li, Thanh V. Nguyen, Chinmay Hegde, and Ka Wai Wong. Implicit sparse regularization: The impact of depth and early stopping. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 28298-28309, 2021. URL [https://proceedings.neurips.cc/paper/2021/hash/ee188463935a061dee6df8bf449cb882-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/ee188463935a061dee6df8bf449cb882-Abstract.html).
* Li et al. [2020] Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. In Silvia Chiappa and Roberto Calandra, editors, _The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy]_, volume 108 of _Proceedings of Machine Learning Research_, pages 4313-4324. PMLR, 2020. URL [http://proceedings.mlr.press/v108/li20j.html](http://proceedings.mlr.press/v108/li20j.html).
* Li et al. [2020] Zhu Li, Weijie Su, and Dino Sejdinovic. Benign overfitting and noisy features. _CoRR_, abs/2008.02901, 2020. URL [https://arxiv.org/abs/2008.02901](https://arxiv.org/abs/2008.02901).
* Lin and Cevher [2018] Junhong Lin and Volkan Cevher. Optimal rates for spectral-regularized algorithms with least-squares regression over hilbert spaces. _CoRR_, abs/1801.06720, 2018. URL [http://arxiv.org/abs/1801.06720](http://arxiv.org/abs/1801.06720).
* Lin and Rosasco [2017] Junhong Lin and Lorenzo Rosasco. Optimal rates for multi-pass stochastic gradient methods. _J. Mach. Learn. Res._, 18:97:1-97:47, 2017. URL [http://jmlr.org/papers/v18/17-176.html](http://jmlr.org/papers/v18/17-176.html).
* Liu et al. [2023] Chaoyue Liu, Amirhesam Abedsoltan, and Mikhail Belkin. On emergence of clean-priority learning in early stopped neural networks. _CoRR_, abs/2306.02533, 2023. doi: 10.48550/arXiv.2306.02533. URL [https://doi.org/10.48550/arXiv.2306.02533](https://doi.org/10.48550/arXiv.2306.02533).
* Lyu and Li [2020] Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020._ OpenReview.net, 2020. URL [https://openreview.net/forum?id=SJeLfgBKPS](https://openreview.net/forum?id=SJeLfgBKPS).
* Lyu et al. [2021] Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on two-layer nets: Margin maximization and simplicity bias. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 12978-12991, 2021. URL [https://proceedings.neurips.cc/paper/2021/hash/6c351da15b5e8a743a21ee96a86e25df-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/6c351da15b5e8a743a21ee96a86e25df-Abstract.html).

* Mallinar et al. [2022] Neil Mallinar, James B. Simon, Amirhesam Abedsoltan, Parthe Pandit, Misha Belkin, and Preetum Nakkiran. Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting. In _NeurIPS_, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/08342dc6ab69f23167b4123086ad4d38-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/08342dc6ab69f23167b4123086ad4d38-Abstract-Conference.html).
* McAllester [2003] David A. McAllester. Simplified pac-bayesian margin bounds. In Bernhard Scholkopf and Manfred K. Warmuth, editors, _Computational Learning Theory and Kernel Machines, 16th Annual Conference on Computational Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003, Proceedings_, volume 2777 of _Lecture Notes in Computer Science_, pages 203-215. Springer, 2003. doi: 10.1007/978-3-540-45167-9_16. URL [https://doi.org/10.1007/978-3-540-45167-9_16](https://doi.org/10.1007/978-3-540-45167-9_16).
* McAllester [2013] David A. McAllester. A pac-bayesian tutorial with A dropout bound. _CoRR_, abs/1307.2118, 2013. URL [http://arxiv.org/abs/1307.2118](http://arxiv.org/abs/1307.2118).
* Mohri et al. [2012] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of Machine Learning_. Adaptive computation and machine learning. MIT Press, 2012. ISBN 978-0-262-01825-8. URL [http://mitpress.mit.edu/books/foundations-machine-learning-0](http://mitpress.mit.edu/books/foundations-machine-learning-0).
* Mou et al. [2018] Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of SGLD for non-convex learning: Two theoretical viewpoints. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, _Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018_, volume 75 of _Proceedings of Machine Learning Research_, pages 605-638. PMLR, 2018. URL [http://proceedings.mlr.press/v75/mou18a.html](http://proceedings.mlr.press/v75/mou18a.html).
* Nagarajan and Kolter [2019] Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain generalization in deep learning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 11611-11622, 2019. URL [https://proceedings.neurips.cc/paper/2019/hash/05e97c207235d63ceb1db43c60db7bbb-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/05e97c207235d63ceb1db43c60db7bbb-Abstract.html).
* Negrea et al. [2020] Jeffrey Negrea, Gintare Karolina Dziugaite, and Daniel Roy. In defense of uniform convergence: Generalization via derandomization with an application to interpolating predictors. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 7263-7272. PMLR, 2020. URL [http://proceedings.mlr.press/v119/negrea20a.html](http://proceedings.mlr.press/v119/negrea20a.html).
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL [https://openreview.net/forum?id=Skz_WfbCZ](https://openreview.net/forum?id=Skz_WfbCZ).
* Parrado-Hernandez et al. [2012] Emilio Parrado-Hernandez, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. Pac-bayes bounds with data dependent priors. _J. Mach. Learn. Res._, 13:3507-3531, 2012. URL [http://dl.acm.org/citation.cfm?id=2503353](http://dl.acm.org/citation.cfm?id=2503353).
* Perez-Ortiz et al. [2021] Maria Perez-Ortiz, Omar Rivasplata, John Shawe-Taylor, and Csaba Szepesvari. Tighter risk certificates for neural networks. _J. Mach. Learn. Res._, 22:227:1-227:40, 2021. URL [http://jmlr.org/papers/v22/20-879.html](http://jmlr.org/papers/v22/20-879.html).
* Pillaud-Vivien et al. [2018] Loucas Pillaud-Vivien, Alessandro Rudi, and Francis R. Bach. Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 8125-8135, 2018. URL [https://proceedings.neurips.cc/paper/2018/hash/10ff0b5e85e8b85cc3095d431d8c08b4-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/10ff0b5e85e8b85cc3095d431d8c08b4-Abstract.html).

- but when? In Gregoire Montavon, Genevieve B. Orr, and Klaus-Robert Muller, editors, _Neural Networks: Tricks of the Trade
- Second Edition_, volume 7700 of _Lecture Notes in Computer Science_, pages 53-67. Springer, 2012. doi: 10.1007/978-3-642-35289-8_5. URL [https://doi.org/10.1007/978-3-642-35289-8_5](https://doi.org/10.1007/978-3-642-35289-8_5).
* Raskutti et al. [2014] Garvesh Raskutti, Martin J. Wainwright, and Bin Yu. Early stopping and non-parametric regression: an optimal data-dependent stopping rule. _J. Mach. Learn. Res._, 15(1):335-366, 2014. doi: 10.5555/2627435.2627446. URL [https://dl.acm.org/doi/10.5555/2627435.2627446](https://dl.acm.org/doi/10.5555/2627435.2627446).
* Rivasplata et al. [2020] Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesvari, and John Shawe-Taylor. Pac-bayes analysis beyond the usual bounds. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/c3992e9a68c5ae12bd18488bc579b30d-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/c3992e9a68c5ae12bd18488bc579b30d-Abstract.html).
* Rosasco and Villa [2015] Lorenzo Rosasco and Silvia Villa. Learning with incremental iterative regularization. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada_, pages 1630-1638, 2015. URL [https://proceedings.neurips.cc/paper/2015/hash/1587965fb4d4b5afe8428a4a024feb0d-Abstract.html](https://proceedings.neurips.cc/paper/2015/hash/1587965fb4d4b5afe8428a4a024feb0d-Abstract.html).
* Russo and Zou [2016] Daniel Russo and James Zou. Controlling bias in adaptive data analysis using information theory. In Arthur Gretton and Christian C. Robert, editors, _Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016_, volume 51 of _JMLR Workshop and Conference Proceedings_, pages 1232-1240. JMLR.org, 2016. URL [http://proceedings.mlr.press/v51/russo16.html](http://proceedings.mlr.press/v51/russo16.html).
* Sahraee-Ardakan et al. [2022] Mojtaba Sahraee-Ardakan, Melikasadat Emami, Parthe Pandit, Sundeep Rangan, and Alyson K. Fletcher. Kernel methods and multi-layer perceptrons learn linear models in high dimensions. _CoRR_, abs/2201.08082, 2022. URL [https://arxiv.org/abs/2201.08082](https://arxiv.org/abs/2201.08082).
* Seeger [2002] Matthias W. Seeger. Pac-bayesian generalisation error bounds for gaussian process classification. _J. Mach. Learn. Res._, 3:233-269, 2002. URL [http://jmlr.org/papers/v3/seeger02a.html](http://jmlr.org/papers/v3/seeger02a.html).
* Shah et al. [2020] Harshay Shah, Kaustav Tampuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/6cfe0e6127fa25df2a0ef2ae1067d915-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/6cfe0e6127fa25df2a0ef2ae1067d915-Abstract.html).
* Shankar et al. [2020] Vaishaal Shankar, Rebecca Roelofs, Horia Mania, Alex Fang, Benjamin Recht, and Ludwig Schmidt. Evaluating machine accuracy on imagenet. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 8634-8644. PMLR, 2020. URL [http://proceedings.mlr.press/v119/shankar20c.html](http://proceedings.mlr.press/v119/shankar20c.html).
* Shawe-Taylor and Williamson [1997] John Shawe-Taylor and Robert C. Williamson. A PAC analysis of a bayesian estimator. In Yoav Freund and Robert E. Schapire, editors, _Proceedings of the Tenth Annual Conference on Computational Learning Theory, COLT 1997, Nashville, Tennessee, USA, July 6-9, 1997_, pages 2-9. ACM, 1997. doi: 10.1145/267460.267466. URL [https://doi.org/10.1145/267460.267466](https://doi.org/10.1145/267460.267466).
* Shen et al. [2022] Ruoqi Shen, Liyao Gao, and Yi-An Ma. On optimal early stopping: Over-informative versus under-informative parametrization. _CoRR_, abs/2202.09885, 2022. URL [https://arxiv.org/abs/2202.09885](https://arxiv.org/abs/2202.09885).

* Silver et al. [2017] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. _Nat._, 550(7676):354-359, 2017. doi: 10.1038/nature24270. URL [https://doi.org/10.1038/nature24270](https://doi.org/10.1038/nature24270).
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL [https://openreview.net/forum?id=r1q7n9gAb](https://openreview.net/forum?id=r1q7n9gAb).
* ECCV 2018
- 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VI_, volume 11210 of _Lecture Notes in Computer Science_, pages 504-519. Springer, 2018. doi: 10.1007/978-3-030-01231-1_31. URL [https://doi.org/10.1007/978-3-030-01231-1_31](https://doi.org/10.1007/978-3-030-01231-1_31).
* Tarres and Yao [2014] Pierre Tarres and Yuan Yao. Online learning as stochastic approximation of regularization paths: Optimality and almost-sure convergence. _IEEE Trans. Inf. Theory_, 60(9):5716-5735, 2014. doi: 10.1109/TIT.2014.2332531. URL [https://doi.org/10.1109/TIT.2014.2332531](https://doi.org/10.1109/TIT.2014.2332531).
* Teng et al. [2021] Jiaye Teng, Jianhao Ma, and Yang Yuan. Towards understanding generalization via decomposing excess risk dynamics. _CoRR_, abs/2106.06153, 2021. URL [https://arxiv.org/abs/2106.06153](https://arxiv.org/abs/2106.06153).
* Tsigler and Bartlett [2020] Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. _arXiv preprint arXiv:2009.14286_, 2020.
* Vaskevicius et al. [2020] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. The statistical complexity of early-stopped mirror descent. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/024d2d699e6c1a82c9ba98638f4d824-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/024d2d699e6c1a82c9ba98638f4d824-Abstract.html).
* Wang and Thrampoulidis [2021] Ke Wang and Christos Thrampoulidis. Benign overfitting in binary classification of gaussian mixtures. In _IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021_, pages 4030-4034. IEEE, 2021. doi: 10.1109/ICASSP39728.2021.9413946. URL [https://doi.org/10.1109/ICASSP39728.2021.9413946](https://doi.org/10.1109/ICASSP39728.2021.9413946).
* Xu and Raginsky [2017] Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning algorithms. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 2524-2533, 2017. URL [https://proceedings.neurips.cc/paper/2017/hash/ad71c82b22f4f65b9398f76d8be4c615-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/ad71c82b22f4f65b9398f76d8be4c615-Abstract.html).
* Yao et al. [2007] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. _Constructive Approximation_, 26(2):289-315, 2007.
* Yun et al. [2021] Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon Han, Junsuk Choe, and Sanghyuk Chun. Re-labeling imagenet: From single to multi-labels, from global to localized labels. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021_, pages 2340-2350. Computer Vision Foundation / IEEE, 2021. URL [https://openaccess.thecvf.com/content/CVPR2021/html/Yun_Re-Labeling_ImageNet_From_Single_to_Multi-Labels_From_Global_to_Localized_CVPR_2021_paper.html](https://openaccess.thecvf.com/content/CVPR2021/html/Yun_Re-Labeling_ImageNet_From_Single_to_Multi-Labels_From_Global_to_Localized_CVPR_2021_paper.html).

* Zhang et al. [2021] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Commun. ACM_, 64(3):107-115, 2021. doi: 10.1145/3446776. URL [https://doi.org/10.1145/3446776](https://doi.org/10.1145/3446776).
* Zhou et al. [2020] Lijia Zhou, Danica J. Sutherland, and Nati Srebro. On uniform convergence and low-norm interpolation learning. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/4cc5400e63624c44fadeda9f57588a6-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/4cc5400e63624c44fadeda9f57588a6-Abstract.html).
* Zou et al. [2021] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham M. Kakade. Benign overfitting of constant-stepsize SGD for linear regression. In Mikhail Belkin and Samory Kpotufe, editors, _Conference on Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado, USA_, volume 134 of _Proceedings of Machine Learning Research_, pages 4633-4635. PMLR, 2021. URL [http://proceedings.mlr.press/v134/zou21a.html](http://proceedings.mlr.press/v134/zou21a.html).
* Zou et al. [2022] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham M. Kakade. Risk bounds of multi-pass SGD for least squares in the interpolation regime. In _NeurIPS_, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/543924ff260ba990f2ef84f940f3db2-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/543924ff260ba990f2ef84f940f3db2-Abstract-Conference.html).

**Supplementary Materials**

## Appendix A Proofs for the Main Results

We first sketch the proof in section A.1 and give some preliminary lemmas A.2. The following sections A.3, A.4 and A.5 are devoted to the proof of Theorem 5.1. The proof of Theorem 4.1 is given in A.6.

### Proof Sketch

We start with a standard bias-variance decomposition following Bartlett et al. [8], which derives that the time-variant excess risk \(R(\mathbf{\theta}_{t})\) can be bounded by a bias term and a variance term. We refer to Appendix A.3 for more details.

For the bias part, we first decompose it into an optimization error and an approximation error. For the optimization error, we use the spectrum analysis to bound it with \(O\left(1/t\right)\) where \(t\) denotes the time. For the approximation error, we bound it with \(O\left(1/\sqrt{n}\right))\) where \(n\) denotes the sample size, inspired by Bartlett et al. [8]. We refer to Appendix A.4 for more details.

For the variance part, a key step is to bound the term \((\mathbf{I}-\frac{\lambda}{n}\mathbf{X}\mathbf{X}^{\top})^{t}\), where \(\mathbf{X}\) is the feature matrix. The difficulty arises from the different scales of the eigenvalues of \(\mathbf{X}\mathbf{X}^{\top}\), where the largest eigenvalue has order \(\Theta(n)\) while the smallest eigenvalue has order \(O(1)\), according to Lemma 10 in Bartlett et al. [8]. To overcome this issue, we divide the matrix \(\mathbf{X}\mathbf{X}^{\top}\) based on whether its eigenvalues is larger than \(c(t,n)\), which is a flexible term dependent on time \(t\) and sample size \(n\). Therefore, we split the variance term based on eigenvalues of covariance matrix \(\mathbf{\Sigma}\) (leading to the \(k_{1}\)-related term) and based on the eigenvalues of \(\mathbf{X}\mathbf{X}^{\top}\) (leading to the \(k_{2}\)-related term). We refer to Appendix A.5 for more details.

### Preliminaries

The following result comes from Bartlett et al. [8], which bounds the eigenvalues of \(\mathbf{X}\mathbf{X}^{\top}\).

**Lemma A.1**.: _(Lemma 10 in Bartlett et al. [8]) For any \(\sigma_{x}\), there exists a constant \(c\), such that for any \(0\leq k<n\), with probability at least \(1-e^{-\frac{n}{c}}\),_

\[\mu_{k+1}\leq c\left(\sum_{i>k}\lambda_{i}+\lambda_{k+1}n\right). \tag{12}\]

This implies that as long as the step size \(\lambda\) is small than a threshold independent of sample size \(n\), gradient descent is stable.

**Corollary A.1**.: _There exists a constant \(c\), such that with probability at least \(1-e^{-\frac{n}{c}}\), for any \(0\leq\lambda\leq\frac{1}{c\sum_{i>0}\lambda_{i}}\) we have_

\[\mathbf{O}\preceq\mathbf{I}-\frac{\lambda}{n}\mathbf{X}^{\top}\mathbf{X}\preceq\mathbf{I}. \tag{13}\]

Proof.: The right hand side of the inequality is obvious since \(\lambda>0\). For the left hand side, we have to show that the eigenvalues of \(\mathbf{I}-\frac{\lambda}{n}\mathbf{X}^{\top}\mathbf{X}\) is non-negative. since \(\mathbf{X}^{\top}\mathbf{X}\) and \(\mathbf{X}\mathbf{X}^{\top}\) have the same non-zero eigenvalues, we know that with probability at least \(1-e^{-\frac{n}{c}}\), the smallest eigenvalue of \(\mathbf{I}-\frac{\lambda}{n}\mathbf{X}^{\top}\mathbf{X}\) can be lower bounded by

\[1-\frac{\lambda}{n}\mu_{1}\geq 1-c\lambda\left(\frac{\sum_{i>0}\lambda_{i}}{n }+\lambda_{k+1}\right)\geq 1-2c\lambda\sum_{i>0}\lambda_{i}\geq 0. \tag{14}\]

where the second inequality uses lemma A.1, and the last inequality holds if \(\lambda\leq\frac{1}{2c\sum_{i>0}\lambda_{i}}\).

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_EMPTY:20]

Plug it into the above expression for \(R(\mathbf{\theta}_{t})\), we have

\[R(\mathbf{\theta}_{t}) =\frac{1}{2}\mathbb{E}\left[\mathbf{x}^{\top}\left[\mathbf{I}-\left(\mathbf{I} -\frac{\lambda}{n}\mathbf{X}^{\top}\mathbf{X}\right)^{t}\right]\mathbf{X}^{\dagger}(\mathbf{X} \mathbf{\theta}^{*}+\mathbf{\varepsilon})-\mathbf{x}^{\top}\mathbf{\theta}^{*}\right]^{2}\] \[=\frac{1}{2}\mathbb{E}\left[\mathbf{x}^{\top}\left(\mathbf{X}^{\dagger} \mathbf{X}-\left(\mathbf{I}-\frac{\lambda}{n}\mathbf{X}^{\top}\mathbf{X}\right)^{t}\mathbf{X}^{ \dagger}\mathbf{X}-\mathbf{I}\right)\mathbf{\theta}^{*}\right. \tag{28}\] \[\left.+\mathbf{x}^{\top}\left[\mathbf{I}-\left(\mathbf{I}-\frac{\lambda}{n} \mathbf{X}^{\top}\mathbf{X}\right)^{t}\right]\mathbf{X}^{\dagger}\mathbf{\varepsilon}\right]^ {2}.\]

Applying lemma A.3, we obtain

\[R(\mathbf{\theta}_{t}) =\frac{1}{2}\mathbb{E}\left[-\mathbf{x}^{\top}\left(\mathbf{I}-\frac{ \lambda}{n}\mathbf{X}^{\top}\mathbf{X}\right)^{t}\mathbf{\theta}^{*}+\mathbf{x}^{\top}\mathbf{X}^ {\top}\left[\mathbf{I}-\left(\mathbf{I}-\frac{\lambda}{n}\mathbf{X}\mathbf{X}^{\top}\right)^{t }\right](\mathbf{X}\mathbf{X}^{\top})^{-1}\mathbf{\varepsilon}\right]^{2}\] \[\leq\mathbb{E}\left[\mathbf{x}^{\top}\left(\mathbf{I}-\frac{\lambda}{n} \mathbf{X}^{\top}\mathbf{X}\right)^{t}\mathbf{\theta}^{*}\right]^{2}+\mathbb{E}\left[\mathbf{x }^{\top}\mathbf{X}^{\top}\left[\mathbf{I}-\left(\mathbf{I}-\frac{\lambda}{n}\mathbf{X}\mathbf{X}^ {\top}\right)^{t}\right](\mathbf{X}\mathbf{X}^{\top})^{-1}\mathbf{\varepsilon}\right]^{2}\] \[:=\mathbf{\theta}^{*\top}\mathbf{B}\mathbf{\theta}^{*}+\mathbf{\varepsilon}^{\top} \mathbf{C}\mathbf{\varepsilon}. \tag{29}\]

which proves the first claim in the lemma. The second part of the theorem directly follows from lemma 18 in Bartlett et al. [8]. 

### Proof for the Bias Upper Bound

The next lemma guarantees that the sample covariace matrix \(\frac{1}{n}\mathbf{X}^{\top}\mathbf{X}\) concentrates well around \(\mathbf{\Sigma}\).

**Lemma A.5**.: _(Lemma 35 in Bartlett et al. [8]) There exists constant \(c\) such that for any \(0<\delta<1\) with probability as least \(1-\delta\),_

\[\left\|\mathbf{\Sigma}-\frac{1}{n}\mathbf{X}^{\top}\mathbf{X}\right\|\leq c\|\mathbf{\Sigma} \|\max\left\{\sqrt{\frac{r(\mathbf{\Sigma})}{n}},\frac{r(\mathbf{\Sigma})}{n},\sqrt{ \frac{\log(\frac{1}{\delta})}{n}},\frac{\log(\frac{1}{\delta})}{n}\right\}. \tag{30}\]

The following inequality will be useful in 

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_EMPTY:23]

\[\begin{split}&\operatorname{Tr}[\mathbf{C}]\\ =&\operatorname{Tr}\left[\left[\mathbf{I}-\left(\mathbf{I}-\frac{ \lambda}{n}\mathbf{X}\mathbf{X}^{\top}\right)^{t}\right]^{2}\left(\mathbf{X}\mathbf{X}^{\top} \right)^{-2}\mathbf{X}\mathbf{\Sigma}\mathbf{X}^{\top}\right]\\ \leq&\operatorname{Tr}\underbrace{\left[\mathbf{U}\text{diag }\left\{\frac{k_{3}\text{ times }}{1,\cdots 1},\ \overline{0},\cdots 0\right\}\right\}\mathbf{U}^{\top}\left(\mathbf{X}\mathbf{X}^{\top}\right)^{-2}\mathbf{X}\mathbf{ \Sigma}\mathbf{X}^{\top}\right]\\ +&\operatorname{Tr}\left[\mathbf{U}\text{diag }\left\{\frac{k_{3}\text{ times }}{0,\cdots 0},\left[\overline{1-\left(1-\frac{\lambda}{n}\mu_{k_{3}+1}\right)^{t}}\right]^{2},\cdots \left[1-\left(1-\frac{\lambda}{n}\mu_{n}\right)^{t}\right]^{2}\right\}\mathbf{U}^{ \top}\left(\mathbf{X}\mathbf{X}^{\top}\right)^{-2}\mathbf{X}\mathbf{\Sigma}\mathbf{X}^{\top}\right].\end{split} \tag{45}\]

#### Bounding 1

Noticing \(\mathbf{X}=\mathbf{U}\tilde{\mathbf{\Lambda}}^{\frac{1}{2}}\mathbf{W}^{\top}\) and \(\mathbf{\Sigma}=\sum_{i\geq 1}\lambda_{i}\mathbf{v}_{i}\mathbf{v}_{i}^{\top}\), we express the first term as sums of eigenvector products,

\[\begin{split}\text{\text{\text{1}}}&=\operatorname{Tr}\left[\mathbf{U}\text{diag }\left\{\frac{k_{3}\text{ times }}{1,\cdots 1},\ \overline{0},\cdots 0\right\}\right\}\mathbf{U}^{\top}\left(\mathbf{X}\mathbf{X}^{\top}\right)^{-2}\mathbf{X}\mathbf{ \Sigma}\mathbf{X}^{\top}\right]\\ &=\operatorname{Tr}\left[\mathbf{U}\text{diag }\left\{\frac{k_{3}\text{ times }}{1,\cdots 1},\ \overline{0},\cdots 0\right\}\right\}\mathbf{U}^{\top}\mathbf{U}\tilde{\mathbf{\Lambda}}^{-2}\mathbf{U}^{\top}\mathbf{U} \tilde{\mathbf{\Lambda}}^{\frac{1}{2}}\mathbf{W}^{\top}\mathbf{\Sigma}\mathbf{W}\tilde{\mathbf{ \Lambda}}^{\frac{1}{2}}\mathbf{U}^{\top}\right]\\ &=\operatorname{Tr}\left[\text{diag }\left\{\frac{k_{3}\text{ times }}{1,\cdots 1},\ \overline{0},\cdots 0 \right\}\tilde{\mathbf{\Lambda}}^{-1}\mathbf{W}^{\top}\mathbf{\Sigma}\mathbf{W}\right]\\ &=\sum_{i\geq 1}\lambda_{i}\operatorname{Tr}\left[\text{diag }\left\{\frac{k_{3}\text{ times }}{1,\cdots 1},\ \overline{0},\cdots 0\right\}\tilde{\mathbf{\Lambda}}^{-1}\mathbf{W}^{\top}\mathbf{v}_{i}\mathbf{v}_{i}^{\top}\mathbf{W}\right]\\ &=\sum_{i\geq 1}\sum_{1\leq j\leq k_{3}}\frac{\lambda_{i}}{\mu_{j}} \left(\mathbf{v}_{i}^{\top}\mathbf{w}_{j}\right)^{2}.\end{split} \tag{46}\]

Next we divide the above summation into \(1\leq i\leq k_{1}\) and \(i>k_{1}\). For the first part, notice that

\[\begin{split}\sum_{1\leq j\leq k_{3}}\frac{\lambda_{i}}{\mu_{j}} \left(\mathbf{v}_{i}^{\top}\mathbf{w}_{j}\right)^{2}&\leq\sum_{1\leq j \leq n}\frac{\lambda_{i}}{\mu_{j}}\left(\mathbf{v}_{i}^{\top}\mathbf{w}_{j}\right)^{2} \\ &=\lambda_{i}\mathbf{v}_{i}^{\top}\left(\sum_{1\leq j\leq n}\frac{1}{ \mu_{j}}\mathbf{w}_{j}\mathbf{w}_{j}^{\top}\right)\mathbf{v}_{i}\\ &=\lambda_{i}\mathbf{v}_{i}^{\top}\mathbf{W}\tilde{\mathbf{\Lambda}}^{-1}\mathbf{W }^{\top}\mathbf{v}_{i}\\ &=\lambda_{i}\mathbf{v}_{i}^{\top}\mathbf{W}\tilde{\mathbf{\Lambda}}^{\frac{1} {2}}\mathbf{U}^{\top}\mathbf{U}\tilde{\mathbf{\Lambda}}^{-2}\mathbf{U}^{\top}\mathbf{U}\tilde{\mathbf{ \Lambda}}^{\frac{1}{2}}\mathbf{W}^{\top}\mathbf{v}_{i}\\ &=\lambda_{i}^{2}\tilde{\mathbf{x}}_{i}^{\top}(\mathbf{X}\mathbf{X}^{\top})^ {-2}\tilde{\mathbf{x}}_{i},\end{split} \tag{47}\]

where \(\tilde{\mathbf{x}}_{i}\) is defined as \(\tilde{\mathbf{x}}_{i}=\frac{\mathbf{X}\mathbf{v}_{i}}{\sqrt{\lambda_{i}}}=\frac{U\mathbf{ \Lambda}^{\frac{1}{2}}\mathbf{W}^{\top}\mathbf{v}_{i}}{\sqrt{\lambda_{i}}}\).

From the proof of lemma 11 in Bartlett et al. [8], we know that for any \(\sigma_{x}\), there exists a constant \(c_{0}\) and \(c\) such that if \(k_{0}\leq\frac{n}{c}\), with probability at least \(1-e^{-\frac{n}{c}}\) the first part can be bounded as \[\sum_{1\leq i\leq k_{1}}\sum_{1\leq j\leq k_{3}}\frac{\lambda_{i}}{\mu_{j}}\left( \boldsymbol{v}_{i}^{\top}\boldsymbol{w}_{j}\right)^{2}\leq\sum_{1\leq i\leq k_ {1}}\lambda_{i}^{2}\boldsymbol{\tilde{x}}_{i}(\boldsymbol{X}\boldsymbol{X}^{ \top})^{-2}\boldsymbol{\tilde{x}}_{i}\leq c\frac{k_{1}}{n}, \tag{48}\]

which gives a bound for the first part.

For the second part we interchange the order of summation and have

\[\begin{split}\sum_{i\geq k_{1}}\sum_{1\leq j\leq k_{3}}\frac{ \lambda_{i}}{\mu_{j}}\left(\boldsymbol{v}_{i}^{\top}\boldsymbol{w}_{j}\right)^ {2}&=\sum_{1\leq j\leq k_{3}}\sum_{i\geq k_{1}}\frac{\lambda_{i}}{ \mu_{j}}\left(\boldsymbol{v}_{i}^{\top}\boldsymbol{w}_{j}\right)^{2}\\ &\leq\frac{1}{c_{3}c(t,n)\sum_{i>0}\lambda_{i}}\sum_{1\leq j\leq k _{3}}\sum_{i\geq k_{1}}\lambda_{i}\left(\boldsymbol{v}_{i}^{\top}\boldsymbol{ w}_{j}\right)^{2}\\ &=\frac{\lambda_{k_{1}+1}}{c_{3}c(t,n)\sum_{i>0}\lambda_{i}}\sum_ {1\leq j\leq k_{3}}\sum_{i\geq k_{1}}\left(\boldsymbol{v}_{i}^{\top}\boldsymbol {w}_{j}\right)^{2}\\ &\leq\frac{\lambda_{k_{1}+1}}{c_{3}c(t,n)\sum_{i>0}\lambda_{i}} \sum_{1\leq j\leq k_{3}}1\\ &=\frac{\lambda_{k_{1}+1}k_{3}}{c_{3}c(t,n)\sum_{i>0}\lambda_{i}} \\ &\leq c\frac{k_{3}}{c(t,n)n}.\end{split} \tag{49}\]

for \(c\) large enough.

Putting 48 and 49 together, and noting that \(k_{3}\leq k_{2}\) with high probability as given in lemma A.8, we know there exists a constant \(c\) that with probability at least \(1-e^{-\frac{n}{c}}\),

\[\textcircled{1}\leq c\frac{k_{1}}{n}+c\frac{k_{2}}{c(t,n)n}. \tag{50}\]

Bounding 2

Similar to the first step in bounding 1, we note that

\[\begin{split}\textcircled{2}&=\operatorname{Tr} \left[\boldsymbol{U}\text{diag}\left\{\overbrace{0,\cdots 0}^{k_{3}\text{ times}},\overbrace{\left[1-\left(1-\frac{\lambda}{n}\mu_{k_{3}+1} \right)^{t}\right]^{2},\cdots,\left[1-\left(1-\frac{\lambda}{n}\mu_{n}\right)^{t} \right]^{2}}^{n-k_{3}\text{ times}}\right.\right.\\ &\qquad\left.\left.\boldsymbol{U}\tilde{\boldsymbol{\Lambda}}^{-2} \boldsymbol{U}^{\top}\boldsymbol{U}\tilde{\boldsymbol{\Lambda}}^{\frac{1}{2}} \boldsymbol{W}^{\top}\boldsymbol{\Sigma}\boldsymbol{W}\tilde{\boldsymbol{ \Lambda}}^{\frac{1}{2}}\boldsymbol{U}^{\top}\right]\\ &=\operatorname{Tr}\left[\text{diag}\left\{\overbrace{0,\cdots 0}^{k_{3}\text{ times}},\overbrace{\frac{1}{\mu_{k_{3}+1}}\left[1-\left(1- \frac{\lambda}{n}\mu_{k_{3}+1}\right)^{t}\right]^{2},\cdots,\frac{1}{\mu_{n}} \left[1-\left(1-\frac{\lambda}{n}\mu_{n}\right)^{t}\right]^{2}}^{2}\right\} \right.\\ &\qquad\left.\boldsymbol{W}^{\top}\boldsymbol{\Sigma}\boldsymbol{W} \right].\end{split} \tag{51}\]

From Bernoulli's inequality and the definition of \(k_{3}\), for any \(k_{3}+1\leq j\leq n\), we have

\[\frac{1}{\mu_{k}}\left[1-\left(1-\frac{\lambda}{n}\mu_{k}\right)^{t}\right]^{2} \leq\frac{1}{\mu_{k}}\left(\frac{\lambda}{n}\mu_{k}t\right)^{2}=\left(\frac{ \lambda t}{n}\right)^{2}\mu_{k}\leq c_{3}\left(\frac{\lambda t}{n}\right)^{2}c (t,n)\sum_{i>0}\lambda_{i}, \tag{52}\]

Hence,\[\begin{split}\textcircled{2}&\leq c_{3}c(t,n)\left( \frac{\lambda t}{n}\right)^{2}\sum_{i>0}\lambda_{i}\operatorname{Tr}[\boldsymbol{ W}^{\top}\boldsymbol{\Sigma}\boldsymbol{W}]\\ &=c_{3}c(t,n)\left(\frac{\lambda t}{n}\sum_{i>0}\lambda_{i} \right)^{2}.\end{split} \tag{53}\]

#### Putting things together

From the bounds for 1 and 2 given above, we know that there exists a constant \(c\) such that with probability at least \(1-e^{-\frac{n}{c}}\), the trace of the variance matrix \(C\) has the following upper bound

\[\operatorname{Tr}[C]\leq c\left(\frac{k_{1}}{n}+\frac{k_{2}}{c(t,n)n}+c(t,n) \left(\frac{\lambda t}{n}\sum_{i>0}\lambda_{i}\right)^{2}\right). \tag{54}\]

Proof of theorem 5.1.: Lemma A.4, A.7 and Theorem A.1 gives the complete proof. Note that the high probability events in the proof are independent of the epoch number \(t\), and this implies that the theorem holds uniformly for all \(t\in\mathbb{N}\). 

### Proof of Compatibility Results

**Corollary A.2** (Corollary 5.1 restated).: _Let Assumption 1, 2 and 3 hold. Fix a constant \(c(t,n)\). Suppose \(k_{0}=O(n)\), \(k_{1}=o(n)\), \(r(\boldsymbol{\Sigma})=o(n)\), \(\lambda=O\left(\frac{1}{\sum_{i>0}\lambda_{i}}\right)\). Then there exists a sequence of positive constants \(\{\delta_{n}\}_{n\geq 0}\) which converge to 0, such that with probability at least \(1-\delta_{n}\), the excess risk is consistent for \(t\in\left(\omega\To conclude, \(\delta_{n}\) can be chosen such that

\[\log\left(\frac{1}{\delta_{n}}\right)=\omega(1),\log\left(\frac{1}{\delta_{n}} \right)=O(n),\log\left(\frac{1}{\delta_{n}}\right)=O\left(\frac{1}{\frac{k_{1}} {n}+\frac{\lambda^{2}t^{2}}{n^{2}}}\right), \tag{59}\]

and then with probability at least \(1-\delta_{n}\), the excess risk is consistent for all \(t\in\left(\omega\left(\frac{1}{\lambda}\right),o\left(\frac{n}{\lambda}\right)\right)\):

\[R(\mathbf{\theta}_{t})=B(\mathbf{\theta}_{t})+V(\mathbf{\theta}_{t})=o(1). \tag{60}\]

This completes the proof for the first claim. The second claim follows from Equation 55 and 57 by setting \(t=\Theta\left(\frac{\sqrt{n}}{\lambda}\right)\). 

**Lemma A.9** (Lemma 5.1 restated).: _For any fixed (i.e. independent of sample size \(n\)) feature covariance \(\mathbf{\Sigma}\) satisfying assumption 1, we have \(k_{1}(n)=o(n)\)._

Proof.: Suppose there exists constant \(c\), such that \(k_{1}(n)\geq cn\). By definition of \(k_{1}\), we know that \(\lambda_{l}\geq\frac{c_{1}\sum_{i>0}\lambda_{i}}{n}\) holds for \(1\leq l\leq k_{1}(n)\). Hence we have

\[\sum_{l=\lfloor cn^{2^{l}+1}\rfloor}^{\lfloor cn^{2^{l}+1}\rfloor}\lambda_{l} \gtrsim\frac{c_{1}\sum_{i>0}\lambda_{i}}{n^{2i+1}}cn2^{i}\gtrsim\sum_{i>0} \lambda_{i}. \tag{61}\]

summing up all \(l\) leads to a contradiction since \(\sum_{i>0}\lambda_{i}<\infty\), which finishes the proof. 

**Theorem A.2** (Theorem 4.1 restated).: _Consider the overparameterized linear regression setting defined in section 4.1. Let Assumption 1,2 and 3 hold. Assume the learning rate satisfies \(\lambda=O\left(\frac{1}{\operatorname{Tr}(\mathbf{\Sigma})}\right)\)._

* _If the covariance satisfies_ \(k_{0}=o(n),R_{k_{0}}(\mathbf{\Sigma})=\omega(n),\;r(\mathbf{\Sigma})=o(n)\)_, it is compatible with the region_ \(T_{n}=\left(\omega\left(\frac{1}{\lambda}\right),\infty\right)\)_._
* _If the covariance satisfies_ \(k_{0}=O(n),k_{1}=o(n),r(\mathbf{\Sigma})=o(n)\)_, it is compatible with the region_ \(T_{n}=\left(\omega\left(\frac{1}{\lambda}\right),o\left(\frac{n}{\lambda} \right)\right)\)_._
* _If the covariance does not change with_ \(n\)_, and satisfies_ \(k_{0}=O(n)\) _and_ \(p=\infty\)_, it is compatible with the region_ \(T_{n}=\left(\omega\left(\frac{1}{\lambda}\right),o\left(\frac{n}{\lambda} \right)\right)\)_._

Proof.: For the first argument, notice that (a) the bias term can still be bounded when \(t=\omega(1)\) and \(r(\Sigma)=o(n)\), according to Lemma A.4; (b) the variance term can be bounded with \(t\to\infty\) (that is, the variance loss would increase with time \(t\)). Therefore, the first argument directly follows Theorem 4 in Bartlett et al. [8].

The second argument follows Corollary 5.1, and the third argument follows Corollary 5.1 and Lemma 5.1. Specifically, for any \(\varepsilon>0\), there exists \(\{\delta_{n}\}_{n>0}\) and \(N\) such that for any sample size \(n>N\), we have

\[\Pr\left[\left|\sup_{t\in T_{n}}R(\mathbf{\theta}_{t})\right|>\varepsilon\right] \leq\delta_{n}. \tag{62}\]

Let \(n\to\infty\) shows that \(\sup_{t\in T_{n}}R(\mathbf{\theta}_{t})\) converges to \(0\) in probability, which completes the proof for the second and the third claim. 

## Appendix B Comparisons and Discussions

In this section, we provide additional discussions and calculations for the main results, and compare it with previous works, including benign overfitting (Section B.1), stability-based bounds (Section B.2), uniform convergence (Section B.3), and early-stopping bounds (Section B.4).

### Comparisons with Benign Overfitting

We summarize the results in Bartlett et al. [8], Zou et al. [76] and our results in Table 1, and provide a detailed comparison with them below.

**Comparison to Bartlett et al. [8].** In this seminal work, the authors study the excess risk of the min-norm interpolator. As discussed before, gradient descent converges to the min-norm interpolator in the overparameterized linear regression setting. One of the main results in Bartlett et al. [8] is to provide a tight bound for the variance part in excess risk as

\[V(\boldsymbol{\hat{\theta}})=O\left(\frac{k_{0}}{n}+\frac{n}{R_{k_{0}}( \boldsymbol{\Sigma})}\right), \tag{63}\]

where \(\boldsymbol{\hat{\theta}}=\boldsymbol{X}^{\top}(\boldsymbol{X}\boldsymbol{X}^ {\top})^{-1}\boldsymbol{Y}\) denotes the min-norm interpolator, and \(R_{k}(\boldsymbol{\Sigma})=(\sum_{i>k}\lambda_{i})^{2}/\)\((\sum_{i>k}\lambda_{i}^{2})\) denote another type of effective rank.

By introducing the time factor, Theorem 5.1 improves over Equation (63) in at least two aspects. Firstly, Theorem 5.1 guarantees the consistency of the gradient descent dynamics for a broad range of step number \(t\), while Bartlett et al. [8] study the limiting behavior of the dynamics of \(t\to\infty\). Secondly, Theorem 5.1 implies that the excess risk of early stopping gradient descent solution can be much better than the min-norm interpolator. Compared to the bound in Equation (63), the bound in Corollary 5.1 (a.) replaces \(k_{0}\) with a much smaller quantity \(k_{1}\); and (b.) drops the second term involving \(R_{k_{0}}(\boldsymbol{\Sigma})\). Therefore, we can derive a consistent bound for an early stopping solution, even though the excess risk of limiting point (min-norm interpolator) can be \(\Omega(1)\).

**Comparison to Zou et al. [76].** Zou et al. [76] study a different setting, which focuses on the one-pass stochastic gradient descent solution of linear regression. The authors prove a bound for the excess risk as

\[R(\boldsymbol{\tilde{\theta}}_{t})=O\left(\frac{k_{1}}{n}+\frac{n\sum_{i>k_{1} }\lambda_{i}^{2}}{(\sum_{i>0}\lambda_{i})^{2}}\right), \tag{64}\]

where \(\boldsymbol{\tilde{\theta}}_{t}\) denotes the parameter obtained using stochastic gradient descent (SGD) with constant step size at epoch \(t\). Similar to our bound, Equation 64 also uses the effective dimension \(k_{1}\) to characterize the variance term. However, we emphasize that Zou et al. [76] derive the bound in a pretty different scenario from ours, which is one-pass SGD scenario. During the one-pass SGD training, one uses a fresh data point to perform stochastic gradient descent in each epoch, and therefore they set \(t=\Theta(n)\) by default. As a comparison, we apply the standard full-batch gradient descent, and thus the time can be more flexible. Besides, our results in Corollary 5.1 improve the bound in Equation (64) by dropping the second term. We refer to the third and fourth example in Example 5.1 for a numerical comparison of the bounds2.

Footnote 2: Due to the bias term in Theorem 5.1, the overall excess risk bound cannot surpass the order \(O(1/\sqrt{n})\), which leads to the cases that Zou et al. [76] outperforms our bound. However, we note that such differences come from the intrinsic property of GD and SGD, which may be unable to avoid in the GD regimes.

### Comparisons with Stability-Based Bounds

In this section, we show that Theorem 5.1 gives provably better upper bounds than the stability-based method. We cite a result from Teng et al. [67], which uses stability arguments to tackle overparameterized linear regression under similar assumptions.

**Theorem B.1** (modified from Theorem 1 in Teng et al. [67]).: _Under the overparameterized linear regression settings, assume that \(\|\boldsymbol{x}\|\leq 1\), \(|\varepsilon|\leq V\), \(w=\frac{\boldsymbol{\theta}^{*,\top}\boldsymbol{x}}{\sqrt{\boldsymbol{\theta}^ {*,\top}\boldsymbol{\Sigma}\boldsymbol{\theta}^{*}}}\) is \(\sigma_{w}^{2}\)-subgaussian. Let \(B_{t}=\sup_{\tau\in[t]}\|\boldsymbol{\theta}_{t}\|\). the following inequality holds with probability at least \(1-\delta\):_

\[R(\boldsymbol{\theta}_{t})=\tilde{O}\left(\max\{1,\boldsymbol{\theta}^{*,\top }\boldsymbol{\Sigma}\boldsymbol{\theta}^{*}\sigma_{w}^{2},(V+B_{t})^{2}\} \sqrt{\frac{\log(4/\delta)}{n}}+\frac{\|\boldsymbol{\theta}^{*}\|^{2}}{\lambda t }+\frac{\lambda t(V+B_{t})^{2}}{n}\right). \tag{65}\]

Theorem B.1 applies the general stability-based results [19, 24] in the overparameterized linear regression setting, by replacing the bounded Lipschitz condition with the bounded domain condition. A fine-grained analysis [31] may remove the bounded Lipschitz condition, but it additionally requires

[MISSING_PAGE_FAIL:29]

Proof.: We begin with the calculation of \(\|\mathbf{\theta}_{t}\|^{2}\). By Lemma A.2, the conditional unbiasedness of noise in Assumption 2 and the noise variance lower bound, we have

\[\mathbb{E}\|\mathbf{\theta}_{t}\|^{2} =\left\|\left(\mathbf{I}-\frac{\lambda}{n}\mathbf{X}^{\top}\mathbf{X}\right)^{ t}\left(\mathbf{\theta}_{0}-\mathbf{X}^{\dagger}\mathbf{Y}\right)+\mathbf{X}^{\dagger}Y\right\|^{2}\] \[=\mathbb{E}\left\|\left(\mathbf{I}-\left(\mathbf{I}-\frac{\lambda}{n}\mathbf{ X}^{\top}\mathbf{X}\right)^{t}\right)\mathbf{X}^{\dagger}\left(\mathbf{X}\mathbf{\theta}^{*}+ \mathbf{\varepsilon}\right)\right\|^{2}\] \[=\mathbb{E}\left\|\left(\mathbf{I}-\left(\mathbf{I}-\frac{\lambda}{n}\mathbf{ X}^{\top}\mathbf{X}\right)^{t}\right)\mathbf{X}^{\dagger}\mathbf{X}\mathbf{\theta}^{*}\right\|^{2}+ \mathbb{E}\left\|\left(\mathbf{I}-\left(\mathbf{I}-\frac{\lambda}{n}\mathbf{X}^{\top}\mathbf{X }\right)^{t}\right)\mathbf{X}^{\dagger}\mathbf{\varepsilon}\right\|^{2}\] \[\geq\mathbb{E}\left\|\left(\mathbf{I}-\left(\mathbf{I}-\frac{\lambda}{n} \mathbf{X}^{\top}\mathbf{X}\right)^{t}\right)\mathbf{X}^{\dagger}\mathbf{\varepsilon}\right\|^ {2}\] \[=\mathbb{E}\operatorname{Tr}\left[\left(\mathbf{I}-\left(\mathbf{I}-\frac{ \lambda}{n}\mathbf{X}^{\top}\mathbf{X}\right)^{t}\right)\mathbf{X}^{\dagger}\mathbf{ \varepsilon}\mathbf{\varepsilon}^{\top}\mathbf{X}^{\dagger,\top}\left(\mathbf{I}-\left(\bm {I}-\frac{\lambda}{n}\mathbf{X}^{\top}\mathbf{X}\right)^{t}\right)\right]\] \[\geq\sigma_{\varepsilon}^{2}\mathbb{E}\operatorname{Tr}\left[ \left(\mathbf{I}-\left(\mathbf{I}-\frac{\lambda}{n}\mathbf{X}^{\top}\mathbf{X}\right)^{t} \right)\mathbf{X}^{\dagger}\mathbf{X}^{\dagger,\top}\left(\mathbf{I}-\left(\mathbf{I}-\frac{ \lambda}{n}\mathbf{X}^{\top}\mathbf{X}\right)^{t}\right)\right]\] \[=\sigma_{\varepsilon}^{2}\sum_{i=1}^{n}\frac{[1-(1-\frac{\lambda} {n}\mu_{i})t^{2}]^{2}}{\mu_{i}}. \tag{69}\]

When \(\mu_{i}=o\left(\frac{n}{t}\right)\), we have

\[1-(1-\frac{\lambda}{n}\mu_{i})^{t}=1-1+\frac{\lambda}{n}\mu_{i}t+O\left(\left( \frac{\lambda}{n}\mu_{i}t\right)^{2}\right)=\Theta\left(\frac{\lambda}{n}\mu_ {i}t\right). \tag{70}\]

Plugging it into Equation 69 and then use Lemma B.1, B.2, we know that under the high probability event in Lemma B.1 and B.2,

\[\mathbb{E}\|\mathbf{\theta}_{t}\|^{2}=\Omega\left((n-l)\frac{\lambda^{2}}{n^{2}} \mu_{n}t^{2}\right)=\Omega\left(\frac{\lambda^{2}}{n}\mu_{n}t^{2}\right)= \Omega\left(\frac{\lambda^{2}t^{2}}{n}\left(\sum_{i>k_{0}}\lambda_{i}\right)\right) \tag{71}\]

Therefore, the stability-based bound, i.e., the right hand side of Equation 65, can be lower bounded in expectation as \(\Omega\left(\frac{\lambda^{2}t^{3}}{n^{2}}\sum_{i>k_{0}}\lambda_{i}\right)\). This implies that the stability-based bound is vacuous when \(t=\Omega\left(\frac{n^{\frac{3}{3}\left(\sum_{i>k_{0}}\lambda_{i}\right)^{- \frac{1}{3}}}}{\lambda}\right)\). Thus, stability-based methods will provably yield smaller compatibility region than \(\left(\omega\left(\frac{1}{\lambda}\right),o\left(\frac{n}{\lambda}\right)\right)\) in Theorem 4.1 when \(\sum_{i>k_{0}}\lambda_{i}\) is not very small, as demonstrated in the examples below.

**Example B.1**.: _Let Assumption 1, 2, 3 holds. Assume without loss of generality that \(\lambda=\Theta(1)\). We have the following examples:_

1. **(Inverse Polynomial).** _If the spectrum of_ \(\mathbf{\Sigma}\) _satisfies_ \[\lambda_{k}=\frac{1}{k^{\alpha}},\] _for some_ \(\alpha>1\)_, we derive that_ \(k_{0}=\Theta(n)\)_,_ \(\sum_{i>k_{0}}\lambda_{i}=\Theta(\frac{1}{n^{\alpha-1}})\)_. Therefore, the stability bound in Theorem B.1 is vacuous when_ \[t=\Omega\left(n^{\frac{\alpha+1}{3}}\right),\] _which is outperformed by the region in Theorem_ 5.1 _when_ \(\alpha<2\)2. **(Inverse Log-Polynominal).** _If the spectrum of_ \(\mathbf{\Sigma}\) _satisfies_ \[\lambda_{k}=\frac{1}{k\log^{\beta}(k+1)},\] _for some_ \(\beta>1\) _, we derive that_ \(k_{0}=\Theta\left(\frac{n}{\log n}\right)\)_,_ \(\sum_{i>k_{0}}\lambda_{i}=\tilde{\Theta}(1)\)_. Therefore, the stability bound in Theorem_ B.1 _is vacuous when_ \[t=\tilde{\Omega}\left(n^{\frac{2}{3}}\right),\] _which is outperformed by the region in Theorem_ 5.1_._
3. **(Constant).** _If the spectrum of_ \(\mathbf{\Sigma}\) _satisfies_ \[\lambda_{k}=\frac{1}{n^{1+\varepsilon}},1\leq k\leq n^{1+\varepsilon},\] _for some_ \(\varepsilon>0\)_, we derive that_ \(k_{0}=0\)_,_ \(\sum_{i>k_{0}}\lambda_{i}=1\)_. Therefore, the stability bound in Theorem_ B.1 _is vacuous when_ \[t=\Omega\left(n^{\frac{2}{3}}\right),\] _which is outperformed by the region in Theorem_ 5.1_._
4. **(Piecewise Constant).** _If the spectrum of_ \(\mathbf{\Sigma}\) _satisfies_ \[\lambda_{k}=\begin{cases}\frac{1}{s}&1\leq k\leq s,\\ \frac{1}{d-s}&s+1\leq k\leq d,\end{cases}\] _where_ \(s=n^{r},d=n^{q},0<r\leq 1,q\geq 1\)_, we derive that_ \(k_{0}=n^{r}\)_,_ \(\sum_{i>k_{0}}\lambda_{i}=1\)_. Therefore, the stability bound in Theorem_ B.1 _is vacuous when_ \[t=\Omega\left(n^{\frac{2}{3}}\right),\] _which is outperformed by the region in Theorem_ 5.1_._

### Comparisons with Uniform Convergence Bounds

We first state a standard bound on the Rademacher complexity of linear models.

**Theorem B.3** (Theorem in Mohri et al. [44]).: _Let \(S\subseteq\{\mathbf{x}:\|x\|_{2}\leq r\}\) be a sample of size \(n\) and let \(\mathcal{H}=\{x\mapsto\langle w,x\rangle:\|w\|_{2}\leq\Lambda\}\). Then, the empirical Rademacher complexity of \(\mathcal{H}\) can be bounded as follows:_

\[\hat{\mathcal{R}}_{S}(\mathcal{H})\leq\sqrt{\frac{r^{2}\Lambda^{2}}{n}}. \tag{72}\]

Furthermore, Talagrand's Lemma (See Lemma 5.7 in Mohri et al. [44]) indicates that

\[\hat{\mathcal{R}}_{S}(l\circ\mathcal{H})\leq L\hat{\mathcal{R}}_{S}(\mathcal{ H})=\frac{\Theta(\Lambda^{2})}{\sqrt{n}}, \tag{73}\]

where \(L=\Theta(\Lambda)\) is the Lipschitz coefficient of the square loss function \(l\) in our setting. Therefore, the Rademacher generalization bound is vacuous when \(\Lambda=\Omega(n^{\frac{1}{4}})\). By Theorem B.2, we know that \(\mathbb{E}\|\mathbf{\theta}_{t}\|^{2}=\Omega(n^{\frac{1}{2}})\) when \(t=\Omega\left(\frac{n^{\frac{3}{4}}}{\lambda\left(\sum_{i>k_{0}}\lambda_{i} \right)^{\frac{1}{2}}}\right)\). A similar comparison as in Example B.1 can demonstrate that uniform stability arguments will provably yield smaller compatibility region than that in Theorem 5.1 for example distributions.

### Comparison with Previous Works on Early Stopping

A line of works focuses on deriving the excess risk guarantee of linear regression or kernel regression with early stopping (stochastic) gradient descent. We refer to Section 2 for details. Here we compare our results with some most relevant works, including [37, 51, 72].

**Comparison with Yao et al. [72].** Yao et al. [72] study kernel regression with early stopping gradient descent. Their approaches are different from ours in the following aspects.

Firstly, the assumptions used in the two approaches are different, due to different goals and techniques. Yao et al. [72] assume that the input feature and data noise have bounded norm (see Section 2.1 in Yao et al. [72]), while we require that the input feature is subgaussian with independent entries.

Furthermore, although Yao et al. [72] obtain a minimax bound in terms of the convergence rate, it is suboptimal in terms of compatibility region. Specifically, The results in our paper show a region like \((0,n)\) while the techniques Yao et al. [72] can only lead to a region like \((0,\sqrt{n})\). See Proof of the Main Theorem in section 2 in Yao et al. [72] for details. Such differences come from different goals of the two approaches, where Yao et al. [72] focus on providing the optimal early-stopping time while we focus on providing a larger time region in which the loss is consistent.

**Comparison with Lin and Rosasco [37].** Lin and Rosasco [37] study stochastic gradient descent with arbitrary batchsize, which is reduced to full batch gradient descent when setting the batchsize to sample size \(n\). Their results are different from ours, since they require the boundness assumption, and focus more on the optimal early stopping time rather than the largest compatibility region, in the same spirit of Yao et al. [72]. Specifically, Lin and Rosasco [37] derive a region like \((0,n^{\frac{\zeta+1}{2\alpha+\gamma}})\), where \(\zeta\) and \(\gamma\) are problem dependent constants (See Theorem 1 in Lin and Rosasco [37] for details). The following examples demonstrate that this paper's results yield larger regions for a wide range of distribution classes.

**Example B.2**.: **(Inverse Polynomial).** _If the spectrum of \(\mathbf{\Sigma}\) satisfies_

\[\lambda_{k}=\frac{1}{k^{\alpha}},\]

_for some \(\alpha>1\). For this distribution, we have \(\zeta=\frac{1}{2}\), \(\gamma=\frac{1}{\alpha}\), and their region is \((0,n^{\frac{3\alpha}{2\alpha+1}})\), which is smaller than \((0,n^{\frac{3\alpha+1}{2\alpha+1}})\) given in Example 5.2._

**Example B.3**.: **(Inverse Log-Polynomial).** _If the spectrum of \(\mathbf{\Sigma}\) satisfies_

\[\lambda_{k}=\frac{1}{k\log^{\beta}(k+1)},\]

_for some \(\beta>1\). For this distribution, we have \(\zeta=\frac{1}{2}\), \(\gamma=1\), and their region is \((0,n^{\frac{3}{4}})\), which is smaller than \((0,n)\) given Corollary 5.1._

### Calculations in Example 5.1

We calculate the quantities \(r(\Sigma),k_{0},k_{1},k_{2}\) for the example distributions in 5.1. The results validate that \(k_{

[MISSING_PAGE_FAIL:33]

[MISSING_PAGE_FAIL:34]

[MISSING_PAGE_FAIL:35]

Figure 4: **The training plot for corrupted MNIST with 20% label noise using GD.**

Figure 3: **The training plot for corrupted MNIST with different levels of label noise using SGD. Figure (c) is copied from Figure 1.**