# Calibrate and Boost Logical Expressiveness of GNN Over Multi-Relational and Temporal Graphs

Dingmin Wang

University of Oxford

dingmin.wang@cs.ox.ac.uk

Equal contribution, listed in alphabetical order.

Yeyuan Chen

University of Michigan

yeyuanch@umich.edu

Equal contribution, listed in alphabetical order.

###### Abstract

As a powerful framework for graph representation learning, Graph Neural Networks (GNNs) have garnered significant attention in recent years. However, to the best of our knowledge, there has been no formal analysis of the logical expressiveness of GNNs as Boolean node classifiers over multi-relational graphs, where each edge carries a specific relation type. In this paper, we investigate \(\mathcal{FOC}_{2}\), a fragment of first-order logic with two variables and counting quantifiers. On the negative side, we demonstrate that the R\({}^{2}\)-GNN architecture, which extends the local message passing GNN by incorporating global readout, fails to capture \(\mathcal{FOC}_{2}\) classifiers in the general case. Nevertheless, on the positive side, we establish that R\({}^{2}\)-GNN models are equivalent to \(\mathcal{FOC}_{2}\) classifiers under certain restricted yet reasonable scenarios. To address the limitations of R\({}^{2}\)-GNN regarding expressiveness, we propose a simple _graph transformation_ technique, akin to a preprocessing step, which can be executed in linear time. This transformation enables R\({}^{2}\)-GNN to effectively capture any \(\mathcal{FOC}_{2}\) classifiers when applied to the "transformed" input graph. Moreover, we extend our analysis of expressiveness and _graph transformation_ to temporal graphs, exploring several temporal GNN architectures and providing an expressiveness hierarchy for them. To validate our findings, we implement R\({}^{2}\)-GNN and the _graph transformation_ technique and conduct empirical tests in node classification tasks against various well-known GNN architectures that support multi-relational or temporal graphs. Our experimental results consistently demonstrate that R\({}^{2}\)-GNN with the graph transformation outperform the baseline methods on both synthetic and real-world datasets. The code is available at [https://github.com/hdmmblz/multi-graph](https://github.com/hdmmblz/multi-graph).

## 1 Introduction

Graph Neural Networks (GNNs) have become a standard paradigm for learning with graph structured data, such as knowledge graphs Park et al. (2019); Tena Cucala et al. (2021); Wang et al. (2023) and molecules Hao et al. (2020); Gasteiger et al. (2021); Guo et al. (2021). GNNs take as input a graph where each node is labelled by a feature vector, and then they recursively update the feature vector of each node by processing a subset of the feature vectors from the previous layer. For example, many GNNs update a node's feature vector by combining its value in the previous layer with the output of some aggregation function applied to its _neighbours_' feature vectors in the previous layer; in this case, after \(k\) iterations, a node's feature vector can capture structural information about the node's \(k\)-hop neighborhood. GNNs have proved to be very efficient in many applications like knowledge graph completion and recommender systems. Most previous work on GNNs mainly revolves around findingGNN architectures (e.g. using different aggregation functions or graph-level pooling schemes) which offer good empirical performance Kipf and Welling (2016); Xu et al. (2018); Corso et al. (2020). The theoretical properties of different architectures, however, are not yet well understood.

In Xu et al. (2018), the authors first proposed a theoretical framework to analyze the expressive power of GNNs by establishing a close connection between GNNs and the Weisfeiler-Lehman (1-WL) test for checking graph isomorphism. Similarly, Geerts and Reutter (2022) provides an elegant way to easily obtain bounds on the separation power of GNNs in terms of the Weisfeiler-Leman (k-WL) tests. However, the characterization in terms of the Weisfeiler-Lehman test only calibrates distinguishing ability. It cannot answer _which Boolean node classifier can be expressed by GNNs._ To this end, Barcelo et al. (2020) consider a class of GNNs named ACR-GNNs proposed in Battaglia et al. (2018), where the update function uses a "global" aggregation of the features of all nodes in the graph in addition to the typical aggregation of feature vectors of neighbour nodes. Then, the authors of the paper prove that in the single-relational 2 scenario, ACR-GNNs can capture every Boolean node classifier expressible in the logic \(\mathcal{FOC}_{2}\).

Footnote 2: The “single-relational” means there is only one type of edges in the graph.

However, most knowledge graphs need multiple relation types. For example, in a family tree, there are multiple different relation types such as "father" and "spouse". In this paper, we consider the abstraction of a widely used GNN architecture called R-GCN Schlichtkrull et al. (2018), which is applicable to multi-relational graphs. Following Barcelo et al. (2020), we define R\({}^{2}\)-GNN as a generalization of R-GCN by adding readout functions to the neighborhood aggregation scheme. We show that although adding readout functions enables GNNs to aggregate information of isolated nodes that can not be collected by the neighborhood-based aggregation mechanism, R\({}^{2}\)-GNN are still unable to capture all Boolean node classifiers expressible as formulas in logic \(\mathcal{FOC}_{2}\) in multi-relational scenarios if applied "directly" to the input. This leaves us with the following questions: (1) Are there reasonable and practical sub-classes of multi-relational graphs for which \(\mathcal{FOC}_{2}\) can be captured by R\({}^{2}\)-GNN? (2) Is there some simple way to encode input graphs, so that all \(\mathcal{FOC}_{2}\) node classifiers can be captured by R\({}^{2}\)-GNN for all multi-relational graphs?

In this paper, we provide answers to the above questions. Moreover, we show that our theoretical findings also transfer to temporal knowledge graphs, which are studied extensively in Park et al. (2022) and Gao and Ribeiro (2022). In particular, we leverage the findings from Gao and Ribeiro (2022) which shows that a temporal graph can be transformed into an "equivalent" static multi-relational graph. Consequently, our results, originally formulated for static multi-relational graphs, naturally extend to the domain of temporal knowledge graphs. Our contributions are as follows:

* We calibrate the logic expressiveness of R\({}^{2}\)-GNN as node classifiers over different sub-classes of multi-relational graphs.
* In light of some negative results about the expressiveness of R\({}^{2}\)-GNN found in the multi-relational scenario, there is a compelling need to boost the power of R\({}^{2}\)-GNN. To address this challenge, we propose a _graph transformation_ and show that such a transformation enables R\({}^{2}\)-GNN to capture each classifier expressible as a \(\mathcal{FOC}_{2}\) formula in all multi-relational graphs.
* We expand the scope of expressiveness results and graph transformation from static multi-relational graphs to _temporal_ settings. Within this context, we propose several temporal GNN architectures and subject them to a comparative analysis with frameworks outlined in Gao and Ribeiro (2022). Ultimately, we derive an expressiveness hierarchy.

## 2 Preliminaries

### Multi-relational Graphs

A _multi-relational_ graph is a \(4\)-tuple \(G=(V,\mathcal{E},P_{1},P_{2})\), where \(V\), \(P_{1}\), \(P_{2}\) are finite sets of _nodes_, _types_ and _relations_ (a.k.a, unary/binary predicates)3, respectively, and \(\mathcal{E}\) is a set of triples of the form \((v_{1},p_{2},v_{2})\) or \((v,type,p_{1})\), where \(p_{1}\in P_{1}\), \(p_{2}\in P_{2}\), \(v_{1},v_{2},v\in V\), and \(type\) is a special symbol.

Footnote 3: For directed graphs, we assume \(P_{2}\) contains relations both in two directions (with inverse-predicates). Moreover, we assume there exists an “equality relation” \(\mathsf{EQ}\in P_{2}\) such that \(\forall x,y\in V,x=y\Leftrightarrow\mathsf{EQ}(x,y)=1\).

Next, given arbitrary (but fixed) finite sets \(P_{1}\) and \(P_{2}\) of unary and binary predicates, respectively, we define the following three kinds of graph classes:

[MISSING_PAGE_FAIL:3]

In the definitions (Equations (1) and (2)), the functions can be set as any functions, such as matrix multiplications or QKV-attentions. Most commonly used GNN such as R-GCN (Schlichtkrull et al. [2018]) and R-GAT (Busbridge et al. [2019]) are captured (upper-bounded) within our R-GNN frameworks. Other related works, such as (Barcelo et al. [2020], Huang et al. [2023], Qiu et al. [2023]) also use intrinsically the same framework as our R-GNN/R\({}^{2}\)-GNN, which has been widely adopted and studied within the GNN community. We believe that analyzing these frameworks can yield common insights applicable to numerous existing GNNs

GNN-based Boolean node classifierIn order to translate the output of a GNN to a Boolean value, we apply a Boolean classification function \(CLS:\mathbb{R}^{d}\rightarrow\{true,false\}\), where \(d\) is the dimension of the feature vectors \(\mathbf{x}_{v}^{L}\). Hence, a Boolean node classifier based on an R\({}^{2}\)-GNN \(\mathcal{M}\) proceeds in three steps: (1) encode the input multi-relational graph \(G\) as described above, (2) apply the R\({}^{2}\)-GNN, and (3) apply \(CLS\) to the output of the R\({}^{2}\)-GNN. This produces a \(true\) or \(false\) value for each node of \(G\). In what follows, we abuse the language and represent a family of GNN-based Boolean node classifiers by the name of the corresponding GNN architecture; for example, R\({}^{2}\)-GNN is the set of all R\({}^{2}\)-GNN-based Boolean node classifiers.

### Logic \(\mathcal{FOC}_{2}\) Formulas

In this paper, we focus on the logic \(\mathcal{FOC}_{2}\), a fragment of first-order logic that only allows formulas with at most two variables, but in turn permits to use _counting quantifiers_. Formally, given two finite sets \(P_{1}\) and \(P_{2}\) of _unary_ and _binary_ predicates, respectively, a \(\mathcal{FOC}_{2}\) formula \(\varphi\) is inductively defined according to the following grammar:

\[\varphi::=A(x)\mid r(x,y)\mid\varphi\wedge\varphi\mid\varphi\vee\varphi\mid \neg\varphi\mid\exists^{\geq n}y(\varphi)\text{ where }A\in P_{1}\text{ and }r\in P_{2} \tag{3}\]

where \(x/y\) in the above rules can be replaced by one another. But please note that \(x\) and \(y\) are the only variable names we are allowed to use (Though we can reuse these two names). In particular, a \(\mathcal{FOC}_{2}\) formula \(\varphi\) with exactly one free variable \(x\) represents a Boolean node classifier for multi-relational graphs as follows: a node \(v\) is assigned to \(true\) iff the formula \(\varphi_{v}\) obtained by substituting \(x\) by \(v\) is satisfied by the (logical) model represented by the multi-relational graph. Similarly as the GNN-based Boolean node classifiers, in what follows, we abuse the language and represent the family of \(\mathcal{FOC}_{2}\) Boolean node classifiers by its name \(\mathcal{FOC}_{2}\).

### Inclusion and Equality Relationships

In this paper, we will mainly talk about inclusion/non-inclusion/equality/strict-inclusion relationships between different node classifier families on certain graph classes. To avoid ambiguity, we give formal definitions of these relationships here. These definitions are all quite natural.

**Definition 2**.: _For any two sets of node classifier \(A,\)\(B\), and graph class \(\mathcal{G}\), We say:_

* \(A\subseteq B\) _on_ \(\mathcal{G}\)_, iff for any node classifier_ \(a\in A\)_, there exists some node classifier_ \(b\in B\) _such that for all graph_ \(G\in\mathcal{G}\) _and_ \(v\in V(G)\)_, it satisfies_ \(a(G,v)=b(G,v)\) _(Namely,_ \(a\) _and_ \(b\) _evaluate the same for all instances in_ \(\mathcal{G}\)_). It implies_ \(B\) _is more expressive than_ \(A\) _on_ \(\mathcal{G}\)_._
* \(A\nsubseteq B\) _on_ \(\mathcal{G}\)_, iff the above condition in item_ 1 _doesn't hold._
* \(A\subsetneq B\) _on_ \(\mathcal{G}\)_, iff_ \(A\subseteq B\) _but_ \(B\nsubseteq A\)_. It implies_ \(B\) _is_ _strictly more expressive than_ \(A\) _on_ \(\mathcal{G}\)_._
* \(A=B\) _on_ \(\mathcal{G}\)_, iff_ \(A\subseteq B\) _and_ \(B\subseteq A\)_. It implies_ \(A\) _and_ \(B\) _has the same expressivity on_ \(\mathcal{G}\)_._

## 3 Related Work

The relationship between first-order logic and the Weisfeiler-Lehman test was initially established by Cai et al. [1989]. Subsequently, more recent works such as Xu et al. [2018], have connected the Weisfeiler-Lehman test with expressivity of GNN. This line of research has been followed by numerous studies, including Maron et al. [2020], which explore the distinguishability of GNNs using the Weisfeiler-Lehman test technique. In particular, Barcelo et al. [2020] introduced the calibration of logical expressivity in GNN-based classifiers and proposed a connection between \(\mathcal{FOC}_{2}\) and R\({}^{2}\)-GNN in single-relational scenario. This led to the emergence of related works, such as Huang et al. [2023], Geerts and Reutter [2021], and Qiu et al. [2023], all of which delve into the logical expressivity of GNNs. Moreover, the theoretical analysis provided in Gao and Ribeiro [2022] has inspired us to extend our results to temporal graph scenarios.

## 4 Logic expressiveness of \(\mathbf{R}^{2}\)-GNN in multi-relational graphs

Our analysis begins with the observation that certain Boolean classifiers can be represented as \(\mathcal{FOC}_{2}\) formulas, but remain beyond the expressiveness of any \(\mathbf{R}^{2}\)-GNN (and consequently, any R-GNN or R-GCN). An illustrative example of this distinction is provided in Figure 1. In this example, we make the assumption that \(P_{1}\) is empty, thereby ensuring that all nodes in both \(G_{1}\) and \(G_{2}\) possess identical initial feature vectors. Additionally, \(P_{2}\) is defined to comprise precisely two relations, namely, \(p_{1}\) and \(p_{2}\). It is evident that no \(\mathbf{R}^{2}\)-GNN can distinguish the node \(a\) in \(G_{1}\) from node \(a\) in \(G_{2}\) - that is, when an \(\mathbf{R}^{2}\)-GNN performs the neighbour-based aggregation, it cannot distinguish whether the \(p_{1}\)-neighbour of \(a\) and the \(p_{2}\)-neighbour of \(a\) are the same. Moreover, the global readout aggregation cannot help in distinguishing those nodes because all nodes have the same feature vector.

We proceed to formalize this intuition and, in the reverse direction, offer a corresponding result. We demonstrate that there exist Boolean classifiers that fall within the scope of \(\mathbf{R}^{2}\)-GNN but elude capture by any \(\mathcal{FOC}_{2}\) formula.

**Proposition 3**.: \(\mathcal{FOC}_{2}\not\subseteq\mathbf{R}^{2}\)_-GNN and \(\mathbf{R}^{2}\)-GNN \(\not\subseteq\mathcal{FOC}_{2}\) on some universal graph class._

We prove Proposition 3 in the Appendix. Here, we give some intuition about the proof. The first result is proved using the example shown in Figure 1, which we have already discussed. To show \(\mathbf{R}^{2}\)-GNN \(\not\subseteq\mathcal{FOC}_{2}\), we construct a classifier \(c\) which classifies a node into true iff _the node has a larger number of \(r_{1}\)-type neighbors than that of \(r_{2}\)-type neighbors_. We can prove that we can easily construct an \(\mathbf{R}^{2}\)-GNN to capture \(c\). However, for \(\mathcal{FOC}_{2}\), this cannot be done, since we can only use counting quantities expressing that there exist at most or at least a specific number of neighbours connected via a particular relation, but our target classifier requires comparing indefinite numbers of neighbours via two relations. Thus, we proceed by contradiction, assume that there exists a \(\mathcal{FOC}_{2}\) classifier equivalent to \(c\), and then find two large enough graphs with nodes that cannot be distinguished by the classifier (but can be distinguished by \(c\)).

In some real-world applications, it is often possible to find an upper bound on the size of any possible input graph or to ensure that any input graph will contain at most one relation between every two nodes. For this reason, we next present restricted but positive&practical expressiveness results on bounded and simple graph classes.

**Theorem 4**.: \(\mathcal{FOC}_{2}\subseteq\mathbf{R}^{2}\)_-GNN on any simple graph class, and \(\mathcal{FOC}_{2}\subsetneq\mathbf{R}^{2}\)-GNN on some simple graph class._

The key idea of the construction is that we will first transform the \(\mathcal{FOC}_{2}\) formula into a new form which we call _relation-specified_\(\mathcal{FOC}_{2}\) (an equivalent form to \(\mathcal{FOC}_{2}\), see more details in our Appendix), and then we are able to construct an equivalent \(\mathbf{R}^{2}\)-GNN inductively over the parser tree of the transformed formula.

Having Theorem 4, one may wonder about the inclusion relationship of \(\mathbf{R}^{2}\)-GNN and \(\mathcal{FOC}_{2}\) in the backward direction. In Proposition 3, we showed that for arbitrary universal graph classes, this inclusion relationship fails. However, given a bounded graph class, we can show that for each \(\mathbf{R}^{2}\)-GNN Boolean node classifier, one can write an equivalent \(\mathcal{FOC}_{2}\) classifier. An intuition about why this is the case is that all graphs in a bounded graph class will have at most \(n\) constants, for some known \(n\in\mathbb{N}\), so for each \(\mathbf{R}^{2}\)-GNN classifier, we can construct an equivalent \(\mathcal{FOC}_{2}\) classifier with a finite number of sub-formulas to recover the features obtained at different layers of \(\mathbf{R}^{2}\)-GNN.

**Theorem 5**.: \(\mathbf{R}^{2}\)_-GNN \(\subseteq\mathcal{FOC}_{2}\) on any bounded graph class, and \(\mathbf{R}^{2}\)-GNN \(\subsetneq\mathcal{FOC}_{2}\) on some bounded graph class._

Figure 1: Multi-edge graphs \(G_{1}\) and \(G_{2}\), and a \(\mathcal{FOC}_{2}\) formula \(\varphi(x)\) that distinguishes them; \(\varphi(x)\) evaluates node \(a\) in \(G_{1}\) to \(true\) and node \(a\) in \(G_{2}\) to \(false\).

Combining Theorem 4 and Theorem 5, we have the following corollary.

**Corollary 5.1**.: \(R^{2}\)_-\(G\!N\!N=\mathcal{FOC}_{2}\) on any bounded simple graph class._

At Last, one may be curious about the complexity of logical classifier in Theorem 5. Here we can give a rather loose bound as follows:

**Theorem 6**.: _For any bounded graph class \(\mathcal{G}_{b}\). Suppose any \(G\in\mathcal{G}_{b}\) has no more than \(N\) nodes, and \(\mathcal{G}_{b}\) has unary predicate set \(P_{1}\) and relation (binary predicate) set \(P_{2}\). Let \(m_{1}:=|P_{1}|,\)\(m_{2}:=|P_{2}|\), then for any node classifier \(c\), suppose \(c\) can be represented as an \(R^{2}\)-GNN with depth (layer number) \(L\), then by Theorem 5 there is a \(\mathcal{FOC}_{2}\) classifier \(\varphi\) equivalent to \(c\) over \(\mathcal{G}_{b}\), and the following hold:_

* _The quantifier depth of_ \(\varphi\) _is no more than_ \(L\)_._
* _The size of_ \(\varphi\) _(quantified by the number of nodes of_ \(\varphi\)_'s parse tree) is no more than_ \(2^{2f(L)}\)_, where_ \(f(L):=2^{2^{2(N+1)f(L-1)}}\)_,_\(f(0)=O(2^{2^{2(m_{1}+m_{2})}})\)_._

The key idea of Theorem 6 is the following: First, by Lemma 27 in our appendix, the combination of **ALL**\(\mathcal{FOC}_{2}\) logical classifiers with quantifier depth no more than \(L\) can already distinguish accepting and rejecting instances of \(c\). Then by Proposition 26 (This is a key point of this bound; please refer to our appendix), We know the number of intrinsically different bounded-depth \(\mathcal{FOC}_{2}\) classifiers is finite, so we only need to get an upper bound on this number. Finally, we can get the desired bound by iteratively using the fact that a boolean combination of a set of formulas can be always written as DNF (disjunctive normal form). The tower of power of two comes from \(L\) rounds of DNF enumerations. Although the bound seems scary, it is a rather loose bound. We give a detailed proof of Theorem 6 in the appendix along with the proof of Theorem 5.

## 5 R\({}^{2}\)-GNN capture \(\mathcal{FOC}_{2}\) over transformed multi-relational graphs

As we pointed out in the previous section, one of the reasons why R\({}^{2}\)-GNN cannot capture \(\mathcal{FOC}_{2}\) classifiers over arbitrary universal graph classes is that in multi-relational graphs, they cannot distinguish whether information about having a neighbour connected via a particular relation comes from the same neighbour node or different neighbour nodes. Towards solving this problem, we propose a _graph transformation_\(F\) (see Definition 7), which enables R\({}^{2}\)-GNN to capture all \(\mathcal{FOC}_{2}\) classifiers on multi-relational graphs. Similar transformation operations have also been used and proved to be an effective way to encode multi-relational graphs in previous studies, e.g., MGNNs Tena Cucala et al. (2021), Indigo Liu et al. (2021) and Time-then-Graph Gao and Ribeiro (2022).

**Definition 7**.: _Given a multigraph \(G=(V,\mathcal{E},P_{1},P_{2})\), the transformation \(F\) will map \(G\) to another graph \(F(G)=(V^{\prime},\mathcal{E}^{\prime},P^{\prime}_{1},P^{\prime}_{2})\) with changes described as follows:_

* _for any two nodes_ \(a,b\in V\)_, if there exists at least one relation_ \(p\in P_{2}\) _between_ \(a\) _and_ \(b\)_, we add two new nodes_ \(ab\) _and_ \(ba\) _to_ \(V^{\prime}\)_._
* _we add a new unary predicate {_primal_} and two new binary predicates {_aux1,aux2_}. Hence,_ \(F(P_{1}):=P^{\prime}_{1}=P_{1}\cup\{\text{primal}\}\)_, and_ \(F(P_{2}):=P^{\prime}_{2}=P_{2}\cup\{\text{aux1,aux2}\}\)_. For each node_ \(v^{\prime}\in V^{\prime}\)_,_ \(\text{primal}(v^{\prime})=1\) _iff_ \(v^{\prime}\) _is also in_ \(V\)_; otherwise,_ \(\text{primal}(v^{\prime})=0\)_;_
* _for each triplet of the form_ \((a,\!p_{2},b)\) _in_ \(\mathcal{E}\)_, we add to_ \(\mathcal{E}^{\prime}\) _four new triples:_ \((ab,\!\text{aux1},a)\)_,_ \((ba,\!\text{aux1},b)\) _and_ \((ab,\!\text{aux2},ba)\) _as well as_ \((ab,\!p_{2},ba)\)_._

An example is in Figure 2. We can see that after applying the _graph transformation_, we need to execute two more hops to propagate information from node \(a\) to node \(b\). However, now we are able to distinguish whether the information about different relations comes from the same node or different nodes. This transformation can be implemented and stored in linear time/space complexity \(O(|V|+|\mathcal{E}|)\), which is very efficient.

Figure 2: Graph Transformation.

**Definition 8**.: _Given a classifier \(\mathcal{C}\) and a transformation function \(F\), we define \(\mathcal{C}\circ F\) to be a new classifier, an extension of \(\mathcal{C}\) with an additional transformation operation on the input graph._

With _graph transformation_\(F\), we get a more powerful class of classifiers than R\({}^{2}\)-GNN. We analyze the logical expressiveness of R\({}^{2}\)-GNN \(\circ F\) in multi-relational graphs, which means first transform a graph \(G\) to \(F(G)\) and then run an R\({}^{2}\)-GNN on \(F(G)\). We will see in the following that this transformation \(F\) boosts the logical expressiveness of R\({}^{2}\)-GNN prominently.

**Theorem 9**.: \(R^{2}\)_-GNN \(\subseteq R^{2}\)-GNN \(\circ F\) on any universal graph class._

**Theorem 10**.: \(\mathcal{FOC}_{2}\subseteq R^{2}\)_-GNN \(\circ F\) on any universal graph class._

Theorem 9 demonstrates that R\({}^{2}\)-GNN with _graph transformation_\(F\) have more expressiveness than R\({}^{2}\)-GNN; and Theorem 10 shows the connection between \(\mathcal{FOC}_{2}\) and R\({}^{2}\)-GNN equipped with _graph transformation_\(F\). We depict their relations in Figure 3. Theorem 9 is a natural result since no information is lost in the process of transformation, while Theorem 10 is an extension on Theorem 4, whose formal proofs can be found in the Appendix. As for the backward direction, we have the result shown in Theorem 11.

**Theorem 11**.: \(R^{2}\)_-GNN \(\circ F\subseteq\mathcal{FOC}_{2}\) on any bounded graph class._

The proof of the theorem is relatively straightforward based on previous results: by Theorem 5, it follows that R\({}^{2}\)-GNN \(\circ F\subseteq\mathcal{FOC}_{2}\circ F\) on any bounded graph class. Then, it suffices to prove \(\mathcal{FOC}_{2}\circ F\subseteq\mathcal{FOC}_{2}\), which we do by using induction over the quantifier depth.

By combining Theorem 10 and Theorem 11, we obtain Corollary 11.1, stating that \(\mathcal{FOC}_{2}\) and R\({}^{2}\)-GNN \(\circ F\) have the same expressiveness with respect to bounded graph classes. Corollary 11.1 does not hold for arbitrary universal graph classes, but our finding is nevertheless exciting because, in many real-world applications there are upper bounds over input graph size.

**Corollary 11.1**.: \(R^{2}\)_-GNN \(\circ F=\mathcal{FOC}_{2}\) on any bounded graph class._

To show the strict separation as in Figure 3, we can combine Proposition 3 and theorems 4 and 9 and Theorem 10 to directly get the following:

**Corollary 11.2**.: \(R^{2}\)_-GNN \(\subseteq R^{2}\)-GNN \(\circ F\) on some universal graph class, and \(\mathcal{FOC}_{2}\subseteq R^{2}\)-GNN \(\circ F\) on some simple graph class._

One may think after transformation \(F\), the logic \(\mathcal{FOC}_{2}\circ F\) with new predicateds becomes stronger as well. However by a similar proof as for Theorem 10 and Lemma 28, we can actually show \(\mathcal{FOC}_{2}\circ F\subseteq\mathcal{FOC}_{2}\) always holds, so \(F\) won't bring added power for \(\mathcal{FOC}_{2}\). However, it indeed make R\({}^{2}\)-GNN strictly more expressive.

## 6 Temporal Graphs

As stated in Gao and Ribeiro (2022), a temporal knowledge graph, composed of multiple snapshots, can consistently undergo transformation into an equivalent static representation as a multi-relational graph. Consequently, this signifies that our theoretical results initially devised for multi-relational graphs can be extended to apply to temporal graphs, albeit through a certain manner of transfer.

Following previous work Jin et al. (2019); Pareja et al. (2020); Park et al. (2022); Gao and Ribeiro (2022), we define a temporal knowledge graph as a set of graph "snapshots" distributed over a sequence of **finite** and **discrete** time points \(\{1,2,\ldots,T\}\). Formally, a temporal knowledge graph is a set \(G=\{G_{1},\cdots,G_{T}\}\) for some \(T\in\mathbb{N}\), where each \(G_{t}\) is a static multi-relational graph. All these \(G_{t}\) share the same node set and predicate set.

In a temporal knowledge graph, a relation or unary fact between two nodes might hold or disappear across the given timestamps. For example, a node \(a\) may be connected to a node \(b\) via a relation \(p\) in the first snapshot, but not in the second; in this case, we have \((a,p,b)\) in \(G_{1}\) not in \(G_{2}\). To keep track of which relations hold at which snapshots, we propose _temporal predicates_, an operation which we define in Definition 12.

**Definition 12**.: _Given a temporal graph \(G=\{G_{1},\cdots,G_{T}\}\), where each \(G_{t}\) is of the form \((V_{t},\mathcal{E}_{t},P_{1},P_{2})\), temporal predicates are obtained from \(G\) by replacing, for each \(t\in\{1,\ldots,T\}\) and each \(p\in P_{2}\), each triple \((v_{a},p,v_{b})\in\mathcal{E}_{t}\) with \((v_{a},p^{t},v_{b})\), where \(p^{t}\) is a fresh predicate, unique for \(p\) and \(t\). Similarly, each unary fact \((v_{a},q)\in\mathcal{E}_{t},q\in P_{1}\) should be replaced by \((v_{a},q^{t})\)._

Note that temporalising introduces \(T\times|P|\) new predicates in total. By _temporalizing predicates_, we assign a superscript to each predicate and use it to distinguish relations over different timestamps.

**Definition 13**.: _Given a temporal knowledge graph \(G=\{G_{1},\ldots,G_{T}\}\), the collapse function \(H\) maps \(G\) to the static graph \(H(G)\) obtained by taking the union of graphs over all timestamps in the temporalization of \(G\)._

As we have proved in Section 5, for multi-relational graphs, R\({}^{2}\)-GNN with _graph transformation_ is more powerful than the pure R\({}^{2}\)-GNN. Here, we transfer these theoretical findings in multi-relational graphs to the setting of temporal knowledge graphs. To be more specific, after _temporalizing predicates_, we apply a _graph transformation_ to each graph snapshot.

**Definition 14**.: _We define \(F^{T}\) to be the temporal graph transformation that takes any temporal knowledge graph as input, applies graph transformation to each snapshot and outputs. Specially, non-primal nodes, aux1 and aux2 edges added in any snapshot should be added into all snapshots._

R\({}^{2}\)-TgnnGao and Ribeiro (2022) casts node representation in temporal graphs into two frameworks: _time-and-graph_ and _time-then-graph_. Due to space constraints, we refer interested readers to Gao and Ribeiro (2022) for more details about the two frameworks. Here, we define a more general GNN-based framework abbreviated as R\({}^{2}\)-TGNN, where each R\({}^{2}\)-TGNN is a sequence \(\{\mathcal{A}_{t}\}_{t=1}^{T}\), where each \(\mathcal{A}_{t}\) is an R\({}^{2}\)-GNN model.Given a temporal knowledge graph \(G=\{G_{1},\ldots,G_{T}\}\), where \(G_{t}=(V_{t},\mathcal{E}_{t},P_{1},P_{2})\) for each \(t\in\{1,\ldots,T\}\). The updating rule is as follows:

\[\mathbf{x}_{v}^{t}=\mathcal{A}_{t}\bigg{(}G_{t},\!v,\!\mathbf{y}^{t}\bigg{)} \quad\text{ where }\quad\mathbf{y}_{v}^{t}=[I_{G_{t}}(v):\mathbf{x}_{v}^{t-1}], \forall v\in V(G_{t}) \tag{4}\]

where \(I_{G_{t}}(v)\) is the one-hot initial feature vector of node \(v\) at timestamp \(t\), and \(\mathcal{A}_{t}(G_{t},\!v,\!\mathbf{y}^{t})\) calculates the new feature vector of \(v\) by running the R\({}^{2}\)-GNN model \(\mathcal{A}_{t}\) on \(G_{t}\), but using \(\mathbf{y}^{t}\) as the initial feature vectors. As shown in Theorem 15, R\({}^{2}\)-TGNN composed with \(F^{T}\) have the same expressiveness as _time-then-graph4_, while being more powerful than _time-and-graph_.

Footnote 4: Since temporalized predicates and timestamps make the definitions of bounded/simple/universal graph class vague, we no longer distinguish them in temporal settings. In theorem statements of this section, \(=\), \(\subseteq\) always hold for any temporal graph class, and \(\subsetneq\), \(\not\subseteq\) hold for some temporal graph class

**Theorem 15**.: _time-and-graph \(\subsetneq\) R\({}^{2}\)-TGNN \(\circ F^{T}=\) time-then-graph._

We also establish the validity of Theorem 16, which asserts that R\({}^{2}\)-TGNN with _graph transformation_ maintains the same expressive power, whether it is applied directly to the temporal graph or to the equivalent collapsed static multi-relational graph

**Theorem 16**.: \(R^{2}\)_-TGNN \(\circ F^{T}=\) R\({}^{2}\)-GNN \(\circ F\circ H\)_

We also prove a strict inclusion that R\({}^{2}\)-TGNN \(\subsetneq\) R\({}^{2}\)-TGNN\(\circ H\). Finally we get the following hierarchy of these frameworks as in Figure 6. the proof of Theorem 17 is in the appendix.

**Theorem 17**.: _The following hold:_

* \(R^{2}\)_-GNN_ \(\subsetneq\)__\(R^{2}\)_-GNN_ \(\circ H\subsetneq\)__\(R^{2}\)_-TGNN_ \(\circ F\circ H\)_=_ \(R^{2}\)_-TGNN_ \(\circ F^{T}\)_= time-then-graph._
* \(time\)_-and-graph_ \(\subsetneq\)__\(R^{2}\)_-TGNN_ \(\circ F^{T}\)_._
* \(R^{2}\)_-TGNN_ \(\not\subseteq\)__\(time\)_-and-graph._

Figure 4: Hierarchic expressiveness.

## 7 Experiment

We empirically verify our theoretical findings for multi-relational graphs by evaluating and comparing the testing performance of R\({}^{2}\)-GNN with _graph transformation_ and less powerful GNNs (R-GNN and R\({}^{2}\)-GNN). We did two groups of experiments on synthetic datasets and real-world datasets, respectively. Details for datasets generation and statistical information as well as hyper-parameters can be found in the Appendix.

### Synthetic Datasets

We first define three simple \(\mathcal{FOC}_{2}\) classifiers

\[\varphi_{\mathbf{1}} \coloneqq\exists^{\geq 2}y(p_{1}^{1}(x,y)\wedge Red^{1}(y))\wedge \exists^{\geq 1}y(p_{1}^{2}(x,y)\wedge Blue^{2}(y))\] \[\varphi_{\mathbf{2}} \coloneqq\exists^{[10,20]}y(\neg p_{1}^{2}(x,y)\wedge\varphi_{1} (y))\qquad\quad\varphi_{\mathbf{3}}\coloneqq\exists^{\geq 2}y(p_{1}^{1}(x,y) \wedge p_{1}^{2}(x,y))\]

Besides, we define another complicate \(\mathcal{FOC}_{2}\) classifier denoted as \(\varphi_{4}\) shown as follows:

\[\varphi_{\mathbf{4}}\coloneqq\bigvee_{3\leq t\leq 10}(\exists^{ \geq 2}y(Black^{t}(y)\wedge Red^{t-1}(y)\wedge Blue^{t-2}(y))\wedge p_{1}^{t}(x,y) \wedge p_{2}^{t-1}(x,y)\wedge p_{3}^{t-2}(x,y)\wedge\varphi^{t}(y))\] \[\text{where}\quad\varphi^{t}(y)\coloneqq\exists^{\geq 2}x(p_{1}^{t} (x,y)\wedge Red^{t}(x))\wedge\exists^{\geq 1}x(p_{2}^{t-1}(x,y)\wedge Blue^{t-2}(x))\]

For each of them, we generate an independent dataset containing 7k multi-relational graphs of size up to 50-1000 nodes for training and 500 multi-relational graphs of size similar to the train set. We tried different configurations for the aggregation functions and evaluated the node classification performances of three temporal GNN methods (R-TGNNs, R\({}^{2}\)-TGNNs and R\({}^{2}\)-TGNNs \(\circ F^{T}\)) on these datasets.

We verify our hypothesis empirically according to models' actual performances of fitting these three classifiers. Theoretically, \(\varphi_{1}\) should be captured by all three models because the classification result of a node is decided by the information of its neighbor nodes, which can be accomplished by the general neighborhood based aggregation mechanism. \(\varphi_{2}\) should not be captured by R-TGNN because the use of \(\neg p_{1}^{2}(x,y)\) as a guard means that the classification result of a node depends on the global information including those isolated nodes, which needs a global readout. For \(\varphi_{3}\) and \(\varphi_{4}\), they should only be captured by R\({}^{2}\)-TGNNs \(\circ F^{T}\). An intuitive explanation for this argument is that if we _temporalise predicates_ and then collapse the temporal graph into its equivalent static multi-relational graph using \(H\), we will encounter the same issue as in the Figure 1. Thus we can't distinguish expected nodes without _graph transformation_.

Results for temporal GNN methods and static GNN methods on four synthetic datasets can be found in Table 1. We can see that R\({}^{2}\)-GNN with _graph transformation_ achieves the best performance. Our theoretical findings show that it is a more expressive model, and the experiments indeed suggest that the model can exploit this theoretical expressiveness advantage to produce better results. Besides, we can also see that R\({}^{2}\)-TGNN \(\circ F\circ H\) and R\({}^{2}\)-TGNN \(\circ F^{T}\) achieve almost the same performance, which is in line with Theorem 16.

\begin{table}
\begin{tabular}{l|c c c|c c c|c c c|c c c} \hline \hline \(\mathcal{FOC}_{2}\) classifier & \multicolumn{4}{c|}{\(\varphi_{\mathbf{1}}\)} & \multicolumn{4}{c|}{\(\varphi_{\mathbf{2}}\)} & \multicolumn{4}{c|}{\(\varphi_{\mathbf{3}}\)} & \multicolumn{4}{c}{\(\varphi_{\mathbf{4}}\)} \\ Aggregation & sum & max & mean & sum & max & mean & sum & max & mean & sum & max & mean \\ \hline \multicolumn{12}{c}{Temporal Graphs Setting} \\ \hline R-TGNN & 100 & 60.7 & 65.4 & 61.0 & 51.3 & 52.4 & 93.7 & 82.3 & 84.4 & 83.5 & 60.0 & 61.3 \\ R\({}^{2}\)-TGNN & 100 & 63.5 & 66.8 & 93.1 & 57.7 & 60.2 & 94.5 & 83.3 & 85.9 & 85.0 & 62.3 & 66.2 \\ R\({}^{2}\)-TGNN \(\circ F^{T}\) & **100** & 67.2 & 68.1 & **99.0** & 57.6 & 62.2 & **100** & 88.8 & 89.2 & **98.1** & 73.4 & 77.5 \\ \hline \mul

### Real-world Datasets

For real-world static multi-relational graphs benchmarks, we used AIFB and MUTAG from Ristoski and Paulheim (2016). Since open source datasets for the node classification on temporal knowledge graphs are rare, we only tried one dataset Brain-10 Gao and Ribeiro (2022) for temporal settings.5

Footnote 5: The other three temporal dataset mentioned in Gao and Ribeiro (2022) are not released.

For static multi-relational graphs, we compare the performances of our methods with RGCN Schlichtkrull et al. (2018). Note that RGCN assigns each node an index and the initial embedding of each node is initialised based on the node index, so the initialisation functional is not permutation-equivariant Chen et al. (2019) and RGCN cannot be used to perform an isomorphism test. However, from Table 3, we can see that R\({}^{2}\)-GNN with _graph transformation_ still achieves the highest accuracy while being able to be used for the graph isomorphism test. Besides, R\({}^{2}\)-GNN \(\circ F\) also performs better compared with both R-GNN and R\({}^{2}\)-GNN. This again suggests that the extra expressive power gained by adding a _graph transformation_ step to R\({}^{2}\)-GNN can be exploited by the model to obtain better results.

For temporal graphs, Gao and Ribeiro (2022) have classified existing temporal models into two categories, _time-and-graph_ and _time-then-graph_, and shown that _time-then-graph_ models have better performance. We choose five models mentioned in Gao and Ribeiro (2022) as our baseline and include the best accuracy of the dataset Brain-10 reported in Gao and Ribeiro (2022). As we expected, R\({}^{2}\)-TGNNand R\({}^{2}\)-TGNN \(\circ F^{T}\) achieve better performance than that of the baseline models and R-TGNN scoring to Table 2. However, we observed that although in theory, R\({}^{2}\)-TGNN \(\circ F^{T}\) has stronger expressive power than R\({}^{2}\)-TGNN, we did not see an improvement when using R\({}^{2}\)-TGNN \(\circ F^{T}\) (\(0.8\%\) accuracy drop). To some extent, it may show that some commonly used benchmarks are inadequate for testing advanced GNN variants. Similar phenomena have also been observed in previous works Chen et al. (2019), Barcelo et al. (2020).

## 8 Conclusion

We analyze expressivity of R\({}^{2}\)-GNNs with and without _graph transformation_ in multi-relational graphs under different situations. Furthermore, we extend our theoretical findings to the temporal graph setting. Our experimental results confirm our theoretical insights, particularly demonstrating the state-of-the-art performance achieved by our _graph transformation_ technique.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{2}{c}{AIFB} & \multicolumn{2}{c}{MUTAG} \\  & sum & max & mean & sum & max & mean \\ \hline R-GNN & **91.7** & 73.8 & 82.5 & **76.5** & 63.3 & 73.2 \\ R\({}^{2}\)-GNN & **91.7** & 73.8 & 82.5 & **85.3** & 62.1 & 79.5 \\ R\({}^{2}\)-GNN \(\circ F\) & **97.2** & 75.0 & 89.2 & **88.2** & 65.5 & 82.1 \\ R-GCN & **95.8** & 77.9 & 86.3 & **73.2** & 65.7 & 72.1 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results on two static multi-relational graphs.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Models} & \multirow{2}{*}{Category} & \multirow{2}{*}{Source} & \multicolumn{3}{c}{Brain-10} \\  & & & sum & max & mean \\ \hline GCRN-M2 & time-and-graph & Seo et al. (2018) & 77.0 & 61.2 & 73.1 \\ DCRNN & time-and-graph & Li et al. (2018) & 84.0 & 70.1 & 66.5 \\ TGAT & time-then-graph & Xu et al. (2020) & 80.0 & 72.3 & 79.0 \\ TGN & time-then-graph & Rossi et al. (2020) & 91.2 & **88.5** & 89.2 \\ GRU-GCN & time-then-graph & Gao and Ribeiro (2022) & 91.6 & 88.2 & 87.1 \\ \hline R-TGNN & – & – & 85.0 & 82.3 & 82.8 \\ R\({}^{2}\)-TGNN & – & – & **94.8** & 82.3 & 91.0 \\ R\({}^{2}\)-TGNN \(\circ F^{T}\) & – & – & 94.0 & 83.5 & **92.5** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results on temporal graphs.

Acknowledgements

The authors extend their gratitude to Bernardo Cuenca Grau and David Tena Cucala for their valuable insights, stimulating discussions, and support.

## References

* Park et al. (2019) Namyong Park, Andrey Kan, Xin Luna Dong, Tong Zhao, and Christos Faloutsos. Estimating node importance in knowledge graphs using graph neural networks. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 596-606, 2019.
* Cucala et al. (2021) David Jaime Tena Cucala, Bernardo Cuenca Grau, Egor V Kostylev, and Boris Motik. Explainable gnn-based models over knowledge graphs. In _International Conference on Learning Representations_, 2021.
* Wang et al. (2023) Dingmin Wang, Yeyuan Chen, and Bernardo Cuenca Grau. Efficient embeddings of logical variables for query answering over incomplete knowledge graphs. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 4652-4659, 2023.
* Hao et al. (2020) Zhongkai Hao, Chengqiang Lu, Zhenya Huang, Hao Wang, Zheyuan Hu, Qi Liu, Enhong Chen, and Cheekong Lee. Asgn: An active semi-supervised graph neural network for molecular property prediction. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 731-752, 2020.
* Gasteiger et al. (2021) Johannes Gasteiger, Florian Becker, and Stephan Gunnemann. Gemnet: Universal directional graph neural networks for molecules. _Advances in Neural Information Processing Systems_, pages 6790-6802, 2021.
* Guo et al. (2021) Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest, Meng Jiang, and Nitesh V Chawla. Few-shot graph learning for molecular property prediction. In _Proceedings of the Web Conference 2021_, pages 2559-2567, 2021.
* Kipf and Welling (2016) Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* Xu et al. (2018) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _arXiv preprint arXiv:1810.00826_, 2018.
* Corso et al. (2020) Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, and Petar Velivckovic. Principal neighbourhood aggregation for graph nets. In _NeurIPS_, 2020.
* Geerts and Reutter (2022) Floris Geerts and Juan L Reutter. Expressiveness and approximation properties of graph neural networks. _arXiv preprint arXiv:2204.04661_, 2022.
* Barcelo et al. (2020) Pablo Barcelo, Egor V. Kostylev, Mikael Monet, Jorge Perez, Juan Reutter, and Juan Pablo Silva. The logical expressiveness of graph neural networks. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/forum?id=r1LZ7AEKvB](https://openreview.net/forum?id=r1LZ7AEKvB).
* Battaglia et al. (2018) Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. _arXiv preprint arXiv:1806.01261_, 2018.
* Schlichtkrull et al. (2018) Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In _European semantic web conference_, pages 593-607. Springer, 2018.
* Park et al. (2022) Namyong Park, Fuchen Liu, Purvanshi Mehta, Dana Cristofor, Christos Faloutsos, and Yuxiao Dong. Evokg: Jointly modeling event time and network structure for reasoning over temporal knowledge graphs. In _Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining_, pages 794-803, 2022.
* Park et al. (2021)Jianfei Gao and Bruno Ribeiro. On the equivalence between temporal and static equivariant graph representations. In _International Conference on Machine Learning_, pages 7052-7076. PMLR, 2022.
* Geerts and Reutter (2021) Floris Geerts and Juan L Reutter. Expressiveness and approximation properties of graph neural networks. In _International Conference on Learning Representations_, 2021.
* Liu et al. (2021) Shuwen Liu, Bernardo Grau, Ian Horrocks, and Egor Kostylev. Indigo: Gnn-based inductive knowledge graph completion using pair-wise encoding. _Advances in Neural Information Processing Systems_, 34:2034-2045, 2021.
* Busbridge et al. (2019) Dan Busbridge, Dane Sherburn, Pietro Cavallo, and Nils Y. Hammerla. Relational graph attention networks, 2019.
* Huang et al. (2023) Xingyue Huang, Miguel Romero Orth, Ismail Ilkan Ceylan, and Pablo Barcelo. A theory of link prediction via relational weisfeiler-leman, 2023.
* Qiu et al. (2023) Haiquan Qiu, Yongqi Zhang, Yong Li, and Quanming Yao. Logical expressiveness of graph neural network for knowledge graph reasoning, 2023.
* Cai et al. (1989) J.-Y. Cai, M. Furer, and N. Immerman. An optimal lower bound on the number of variables for graph identification. In _30th Annual Symposium on Foundations of Computer Science_, pages 612-617, 1989. doi: 10.1109/SFCS.1989.63543.
* Maron et al. (2020) Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks, 2020.
* Jin et al. (2019) Woojeong Jin, Meng Qu, Xisen Jin, and Xiang Ren. Recurrent event network: Autoregressive structure inference over temporal knowledge graphs. _arXiv preprint arXiv:1904.05530_, 2019.
* Pareja et al. (2020) Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, Tao Schardl, and Charles Leiserson. Evolvegcn: Evolving graph convolutional networks for dynamic graphs. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 5363-5370, 2020.
* Seo et al. (2018) Youngjoo Seo, Michael Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence modeling with graph convolutional recurrent networks. In _International conference on neural information processing_, pages 362-373. Springer, 2018.
* Li et al. (2018) Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. In _International Conference on Learning Representations_, 2018.
* Xu et al. (2020) Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. Inductive representation learning on temporal graphs. In _International Conference on Learning Representations_, 2020.
* Rossi et al. (2020a) Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael Bronstein. Temporal graph networks for deep learning on dynamic graphs. _arXiv preprint arXiv:2006.10637_, 2020a.
* Ristoski and Paulheim (2016) Petar Ristoski and Heiko Paulheim. Rdf2vec: Rdf graph embeddings for data mining. In _International Semantic Web Conference_, pages 498-514. Springer, 2016.
* Chen et al. (2019a) Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph isomorphism testing and function approximation with gnns. _Advances in neural information processing systems_, 32, 2019a.
* Chen et al. (2019b) Ting Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? a dissection on graph classification. _arXiv preprint arXiv:1905.04579_, 2019b.
* Davey and Priestley (2002) Brian A Davey and Hilary A Priestley. _Introduction to lattices and order_. Cambridge university press, 2002.
* Li et al. (2019) Jia Li, Zhichao Han, Hong Cheng, Jiao Su, Pengyun Wang, Jianfeng Zhang, and Lujia Pan. Predicting path failure in time-evolving graphs, 2019.
* Li et al. (2019)Youngjoo Seo, Michael Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence modeling with graph convolutional recurrent networks, 2016. URL [https://arxiv.org/abs/1612.07659](https://arxiv.org/abs/1612.07659).
* Chen et al. (2018) Jinyin Chen, Xueke Wang, and Xuanheng Xu. Gc-lstm: Graph convolution embedded lstm for dynamic link prediction, 2018. URL [https://arxiv.org/abs/1812.04206](https://arxiv.org/abs/1812.04206).
* Manessi et al. (2020) Franco Manessi, Alessandro Rozza, and Mario Manzo. Dynamic graph convolutional networks. _Pattern Recognition_, 97:107000, jan 2020. doi: 10.1016/j.patcog.2019.107000. URL [https://doi.org/10.1016%2Fj.patcog.2019.107000](https://doi.org/10.1016%2Fj.patcog.2019.107000).
* Sankar et al. (2018) Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao Yang. Dynamic graph representation learning via self-attention networks, 2018. URL [https://arxiv.org/abs/1812.09430](https://arxiv.org/abs/1812.09430).
* Rossi et al. (2020b) Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael Bronstein. Temporal graph networks for deep learning on dynamic graphs, 2020b. URL [https://arxiv.org/abs/2006.10637](https://arxiv.org/abs/2006.10637).
* Siegelmann and Sontag (1992) Hava T. Siegelmann and Eduardo D. Sontag. On the computational power of neural nets. In _Proceedings of the Fifth Annual Workshop on Computational Learning Theory_, COLT '92, page 440-449, New York, NY, USA, 1992. Association for Computing Machinery. ISBN 089791497X. doi: 10.1145/130385.130432. URL [https://doi.org/10.1145/130385.130432](https://doi.org/10.1145/130385.130432).

## Appendix

### Table of Contents

* A Preliminaries for Proofs
* B Proof of Proposition 3
* C Proof of Theorem 4
* D Proof of Theorem 5 and Theorem 6
* E Proof of Theorem 9
* F Proof of Theorem 10
* G Proof of Theorem 11
* H Proof of Theorem 15
* I Proof of Theorem 16
* J Proof of Theorem 17
* K Experiment Supplementary
* K.1 Synthetic dataset generation
* K.2 Statistical Information for Datasets
* K.3 Hyper-parameters
* K.4 More Results.
Preliminaries for Proofs

In this section, we give some preliminaries which will be used to prove the theorems, propositions and lemmas shown in our main body. In what follows, we fix a unary predicate set \(P_{1}\) and a binary predicate set \(P_{2}\).

**Definition 18**.: _For an \(R^{2}\)-GNN, we say it is a 0/1-GNN if the recursive formula used to compute vectors \(\mathbf{x}_{v}^{(i)}\) for each node \(v\) in a multi-relational graph \(G=\{V,\mathcal{E},P_{1},P_{2}\}\) on each layer \(i\) is in the following form_

\[\mathbf{x}_{v}^{(i)}=f\left(C^{(i)}\left(\mathbf{x}_{v}^{(i-1)}+\sum_{r\in P_{ 2}}\sum_{u\in V}A_{r}^{(i)}\mathbf{x}_{u}^{(i-1)}+R^{(i)}\left(\sum_{u\in V} \mathbf{x}_{u}^{(i-1)}\right)+b^{(i)}\right)\right) \tag{5}\]

_where \(C^{(i)}\),\(A_{j}^{(i)}\),\(R^{(i)}\) are all integer matrices of size \(d_{i}\times d_{i-1}\), \(b^{(i)}\) is bias column vector with size \(d_{i}\times 1\), where \(d_{i-1}\) and \(d_{i}\) are input/output dimensions, and \(f\) is defined as \(max(0\),\(min(x\),\(1))\)._

_Furthermore, we restrict the final output dimension be \(d_{L}=1\). Since all matrices have integer elements, initial vectors are integer vectors by initialisation function \(I(\cdot)\) (Section 2.2), and \(max(0\),\(min(x,1))\) will map all integers to \(0/1\), it's easy to see that the output of this kind of model is always \(0/1\), which can be directly used as the classification result. We call such model 0/1-GNN. A model instance can be represented by \(\{C^{(i)},(A_{j}^{(i)})_{j=1}^{K},R^{(i)},\)\(b^{(i)}\}_{i=1}^{L}\), where \(K=|P_{2}|\)_

**Lemma 19**.: _Regard 0/1-GNN as node classifier, then the set of node classifiers represented by 0/1-GNN is closed under \(\wedge,\vee\),\(\neg\)._

Proof.: Given two 0/1-GNN \(\mathcal{A}_{1}\),\(\mathcal{A}_{2}\), it suffices to show that we can construct \(\neg\mathcal{A}_{1}\) and \(\mathcal{A}_{1}\wedge\mathcal{A}_{2}\) in 0/1-GNN framework. That's because construction of \(\mathcal{A}_{1}\vee\mathcal{A}_{2}\) can be reduced to constructions of \(\wedge\),\(\neg\) by De Morgan's law, e.g., \(a\lor b=\neg(\neg a\wedge\neg b)\).

1. Construct \(\neg\mathcal{A}_{1}\). Append a new layer to \(\mathcal{A}_{1}\) with dimension \(d_{L+1}=1\). For matrices and bias \(C^{(L+1)}\),\((A_{j}^{(L+1)})_{j=1}^{K}\),\(R^{(L+1)}\),\(b^{(L+1)}\) in layer \(L+1\), set \(C_{1,1}^{L+1}=-1\) and \(b_{1}^{L+1}=1\) and other parameters \(0\). Then it follows \(\mathbf{x}_{v}^{(L+1)}=max(0\),\(min(-\mathbf{x}_{v}^{(L)}+1,1))\). Since \(\mathbf{x}_{v}^{(L)}\) is the 0/1 classification result outputted by \(\mathcal{A}_{1}\). It's easy to see that the above equation is exactly \(\mathbf{x}_{v}^{(L+1)}=\neg\mathbf{x}_{v}^{(L)}\)

2. Construct \(\mathcal{A}_{1}\wedge\mathcal{A}_{2}\). Without loss of generality, we can assume two models have same layer number \(L\) and same feature dimension \(d_{l}\) in each layer \(l\in\{1......L\}\). Then, we can construct a new 0/1-GNN \(\mathcal{A}\). \(\mathcal{A}\) has \(L+1\) layers. For each of the first \(L\) layers, say \(l\)-th layer, it has feature dimension \(2d_{l}\). Let \(\{C_{1}^{(l)},(A_{j,1}^{(l)})_{j=1}^{K}\),\(R_{1}^{(l)}\),\(b_{1}^{(l)}\}\),\(\{C_{2}^{(l)},(A_{j,2}^{(l)})_{j=1}^{K}\),\(R_{2}^{(l)}\),\(b_{2}^{(l)}\}\) be parameters in layer \(l\) of \(\mathcal{A}_{1}\),\(\mathcal{A}_{2}\) respectively. Parameters for layer \(l\) of \(\mathcal{A}\) are defined below

\[\mathbf{C}^{(l)}:=\begin{bmatrix}\mathbf{C}_{1}^{(l)}\\ &\mathbf{C}_{2}^{(l)}\end{bmatrix}\mathbf{A}_{j}^{(l)}:=\begin{bmatrix} \mathbf{A}_{j,1}^{(l)}\\ &\mathbf{A}_{j,2}^{(l)}\end{bmatrix}\mathbf{R}^{(l)}:=\begin{bmatrix} \mathbf{R}_{1}^{(l)}\\ &\mathbf{R}_{2}^{(l)}\end{bmatrix}\mathbf{b}^{(l)}:=\begin{bmatrix} \mathbf{b}_{1}^{(l)}\\ \mathbf{b}_{2}^{(l)}\end{bmatrix} \tag{6}\]

Initialization function of \(\mathcal{A}\) is concatenation of initial feature of \(\mathcal{A}_{1}\),\(\mathcal{A}_{2}\). Then it's easy to see that the feature \(\mathbf{x}_{v}^{L}\) after running first \(L\) layers of \(\mathcal{A}\) is a two dimension vector, and the two dimensions contains two values representing the classification results outputted by \(\mathcal{A}_{1}\),\(\mathcal{A}_{2}\) respectively.

For the last layer \(L+1\), it has only one output dimension. We just set \(\mathbf{C}_{1,1}^{L+1}=\mathbf{C}_{1,2}^{L+1}=1\),\(\mathbf{b}_{1}^{L+1}=-1\) and all other parameters \(0\). Then it's equivalent to \(\mathbf{x}_{v}^{(L+1)}=max(0\),\(min(\mathbf{x}_{v,1}^{(L)}+\mathbf{x}_{v,2}^{(L)}-1\),\(1)) where \(\mathbf{x}_{v,1}^{(L)}\),\(\mathbf{x}_{v,2}^{(L)}\) are output of \(\mathcal{A}_{1}\),\(\mathcal{A}_{2}\) respectively. It's easy to see that the above equation is equivalent to \(\mathbf{x}_{v}^{(L+1)}=\mathbf{x}_{v,1}^{(L)}\wedge\mathbf{x}_{v,2}^{(L)}\) so the \(\mathcal{A}\) constructed in this way is exactly \(\mathcal{A}_{1}\wedge\mathcal{A}_{2}\) 

**Definition 20**.: _A \(\mathcal{FOO}_{2}\) formula is defined inductively according to the following grammar:_

\[A(x),r(x,y),\varphi_{1}\wedge\varphi_{2},\varphi_{1}\vee\varphi_{2},\neg \varphi_{1},\exists^{\geq n}y(\varphi_{1}(x,y))\text{ where }A\in P_{1}\text{ and }r\in P_{2} \tag{7}\]

**Definition 21**.: _For any subset \(S\subseteq P_{2}\), let \(\varphi_{S}(x,y)\) denote the \(\mathcal{FOC}_{2}\) formula \((\bigwedge_{r\in S}r(x,y))\wedge(\bigwedge_{r\in P_{2}\setminus S}\neg r(x,y))\). Note that \(\varphi_{S}(x,y)\) means there is a relation \(r\) between \(x\) and \(y\) if and only if \(r\in S\), so \(\varphi_{S}(x,y)\) can be seen as a formula to restrict specific relation distribution between two nodes. \(\mathcal{RSFOC}_{2}\) is inductively defined according to the following grammar:_

\[A(x),\varphi_{1}\wedge\varphi_{2},\ \varphi_{1}\vee\varphi_{2},\ \neg\varphi_{1}, \exists^{\geq n}y\bigg{(}\varphi_{S}(x,y)\wedge\varphi_{1}(y)\bigg{)}\text{ where }A\in P_{1}\text{ and }S\subseteq P_{2} \tag{8}\]

Next, we prove that \(\mathcal{FOC}_{2}\) and \(\mathcal{RSFOC}_{2}\) have the same expressiveness, namely, each \(\mathcal{FOC}_{2}\) node classifier can be rewritten in the form \(\mathcal{RSFOC}_{2}\).

**Lemma 22**.: \(\mathcal{FOC}_{2}=\mathcal{RSFOC}_{2}\)_._

Proof.: Comparing the definitions of \(\mathcal{RSFOC}_{2}\) and \(\mathcal{FOC}_{2}\), it is obvious that \(\mathcal{RSFOC}_{2}\subseteq\mathcal{FOC}_{2}\) trivially holds, so we only need to prove the other direction, namely, \(\mathcal{FOC}_{2}\subseteq\mathcal{RSFOC}_{2}\). In particular, a Boolean logical classifier only contains one free variable, we only need to prove that for any one-free-variable \(\mathcal{FOC}_{2}\) formula \(\varphi(x)\), we can construct an equivalent \(\mathcal{RSFOC}_{2}\) formula \(\psi(x)\).

We prove Lemma 22 by induction over \(k\), where \(k\) is the quantifier depth of \(\varphi(x)\).

In the base case where \(k=0\), \(\varphi(x)\) is just the result of applying conjunction, disjunction or negation to a bunch of unary predicates \(A(x)\), where \(A\in P_{1}\). Given that the grammar of generating \(\varphi(x)\) is the same in \(\mathcal{RSFOC}_{2}\) and \(\mathcal{FOC}_{2}\) when \(k=0\), so the lemma holds for \(k=0\).

For the indutive step, we assume that Lemma 22 holds for all \(\mathcal{RSFOC}_{2}\) formula with quantifier depth no more than \(m\), we next need to consider the case when \(k=m+1\).

We can decompose \(\varphi(x)\) to be boolean combination of a bunch of \(\mathcal{FOC}_{2}\) formulas \(\varphi_{1}(x),\ldots,\varphi_{N}(x)\), each of which is in the form \(\varphi_{i}(x):=A(x)\) where \(A\in P_{1}\) or \(\varphi_{i}(x):=\exists^{\geq n}y(\varphi^{\prime}(x,y))\). See the following example for reference.

**Example 23**.: _Assume \(\varphi(x):=\big{(}A_{1}(x)\wedge\exists y(r_{1}(x,y))\big{)}\vee\big{(}\exists y \big{(}A_{2}(y)\wedge r_{2}(x,y)\big{)}\wedge\exists y(r_{3}(x,y))\big{)}\). It can be decomposed into boolean combination of four subformulas shown as follows:_

* \(\varphi_{1}(x)=A_{1}(x)\)__
* \(\varphi_{2}(x)=\exists y(r_{1}(x,y))\)__
* \(\varphi_{3}(x)=\exists y\big{(}A_{2}(y)\wedge r_{2}(x,y)\big{)}\)__
* \(\varphi_{4}(x)=\exists y(r_{3}(x,y))\)__

We can see that grammars of \(\mathcal{FOC}_{2}\) and \(\mathcal{RSFOC}_{2}\) have a common part: \(A(x),\varphi_{1}\wedge\varphi_{2},\varphi_{1}\vee\varphi_{2},\neg\varphi_{1}\), so we can only focus on those subformulas \(\varphi_{i}(x)\) in the form of \(\exists^{\geq n}y\varphi^{\prime}(x,y)\). In other words, if we can rewrite these \(\mathcal{FOC}_{2}\) subformulas into another form satisfying the grammar of \(\mathcal{RSFOC}_{2}\), we can naturally construct the desired \(\mathcal{RSFOC}_{2}\) formula \(\psi(x)\) equivalent to \(\mathcal{FOC}_{2}\) formula \(\varphi(x)\).

Without loss of generality, in what follows, we consider the construction for \(\varphi(x)=\exists^{\geq n}y(\varphi^{\prime}(x,y))\). Note that \(\varphi(x)\) has quantifier depth no more than \(m+1\), and \(\varphi^{\prime}(x,y)\) has quantifier depth no more than \(m\).

We can decompose \(\varphi^{\prime}(x,y)\) into three sets of subformulas \(\{\varphi_{i}^{x}(x)\}_{i=1}^{N_{x}}\),\(\{\varphi_{i}^{y}(y)\}_{i=1}^{N_{y}}\),\(\{r_{i}(x,y)\}_{i=1}^{|P_{2}|}\), where \(N_{x}\) and \(N_{y}\) are two natural numbers, \(\varphi_{i}^{x},\varphi_{i}^{y}\) are its maximal subformulas whose free variable is assigned to \(x\) and \(y\), respectively. \(\varphi^{\prime}(x)\) is the combination of these sets of subformulas using \(\wedge\), \(\vee\),\(\neg\).

**Example 24**.: _Assume that we have a \(\mathcal{FOC}_{2}\) formula in the form of \(\varphi^{\prime}(x,y)=\Big{(}r_{1}(x,y)\wedge\exists x(r_{2}(x,y))\Big{)} \vee\Big{(}\exists y\big{(}\exists x(r_{3}(x,y))\vee\exists y(r_{1}(x,y))\big{)} \wedge\exists y\big{(}A_{2}(y)\wedge r_{2}(x,y)\big{)}\Big{)}\)_

_It can be decomposed into the following subformulas:_

* \(\varphi_{1}^{x}(x):=\exists y\big{(}\exists x(r_{3}(x,y))\vee\exists y(r_{1}(x,y ))\big{)}\)_;_
* \(\varphi_{2}^{x}(x):=\exists y\big{(}A_{2}(y)\wedge r_{2}(x,y)\big{)}\)_;_
* \(\varphi_{1}^{y}(y):=\exists x(r_{2}(x,y))\)_;_
* \(r_{1}(x,y)\)Assume that \(N:=\{1,\ldots,N_{x}\}\), we construct a \(\mathcal{RSFOC}_{2}\) formula \(\varphi_{T}^{x}(x):=\big{(}\bigwedge_{i\in N}\varphi_{i}^{x}(x)\big{)}\wedge( \bigwedge_{i\in N\setminus T}\neg\varphi_{i}^{x}(x))\), where \(T\subseteq N\). It is called the _x-specification_ formula, which means \(\varphi_{T}^{x}(x)\) is _true_ iff the following condition holds: for all \(i\in T\), \(\varphi_{i}^{x}(x)\) is _true_ and for all \(i\in N\setminus T\), \(\varphi_{i}^{x}(x)\) is _false_.

By decomposing \(\varphi^{\prime}(x,\!y)\) into three subformula sets, we know Boolean value of \(\varphi^{\prime}(x,\!y)\) can be decided by Boolean values of these formulas \(\{\varphi_{i}^{x}(x)\}_{i=1}^{N_{x}}\),\(\{\varphi_{i}^{y}(y)\}_{i=1}^{N_{y}}\),\(\{r_{i}(x,\!y)\}_{i=1}^{|P_{2}|}\). Now for any two specific subsets \(S\subseteq P_{2},T\subset N\), we assume \(\varphi_{S}(x,\!y)\) and \(\varphi_{T}^{x}(x)\) are all _true_ (Recall the definition of \(\varphi_{S}(x,\!y)\) in Definition 21). Then Boolean values for formulas in \(\{\varphi_{i}^{x}(x)\}_{i=1}^{N_{x}}\),\(\{r_{i}(x,\!y)\}_{i=1}^{|P_{2}|}\) are determined and Boolean value of \(\varphi^{\prime}(x,\!y)\) depends only on Boolean values of \(\{\varphi_{i}^{y}(y)\}_{i=1}^{N_{y}}\). Therefore, we can write a new \(\mathcal{FOC}_{2}\) formula \(\varphi_{S,T}^{y}(y)\) which is a boolean combination of \(\{\varphi_{i}^{y}(y)\}_{i=1}^{N_{y}}\). This formula should satisfy the following condition: For any graph \(G\) and two nodes \(a,\!b\) on it, the following holds,

\[\varphi_{S}(a,\!b)\wedge\varphi_{T}^{x}(a)\Rightarrow\Big{(}\varphi^{\prime}(a,\!b)\Leftrightarrow\varphi_{S,T}^{y}(b)\Big{)} \tag{9}\]

By our inductive assumption, \(\varphi^{\prime}(x,\!y)\) has a quantifier depth which is no more than \(m\), so \(\{\varphi_{i}^{x}(y)\}_{i=1}^{N_{x}}\),\(\{\varphi_{i}^{y}(y)\}_{i=1}^{N_{y}}\) also have quantifier depths no more than \(m\). Therefore, each of them has \(\mathcal{RSFOC}_{2}\) correspondence. Furthermore, since \(\wedge\), \(\vee\),\(\neg\) are allowed operation in \(\mathcal{RSFOC}_{2}\), \(\varphi_{T}^{x}(x)\) and \(\varphi_{S,T}^{y}(y)\) can also be rewritten as \(\mathcal{RSFOC}_{2}\) formulas.

Given that \(\varphi_{S}(x,\!y)\) and \(\varphi_{T}^{x}(y)\) specify the boolean values for all \(\{\varphi_{i}^{x}(y)\}_{i=1}^{N_{x}}\),\(\{\varphi_{i}^{x}(x,\!y)\}_{i=1}^{|P_{2}|}\) formulas, so we can enumerate all possibilities over \(S\subseteq P_{2}\) and \(T\subseteq N\). Obviously for any graph \(G\) and a node pair (\(a,\!b\)), there exists an unique \((S,\!T)\) pair such that \(\varphi_{S}(a,\!b)\wedge\varphi_{T}^{x}(a)\) holds.

Hence, combining Equation (9), \(\varphi^{\prime}(x,\!y)\) is true only when there exists a \((S,\!T)\) pair such that \(\varphi_{S}(x,\!y)\wedge\varphi_{T}^{x}(x)\wedge\varphi_{S,T}^{y}(y)\) is _true_. Formally, we can rewrite \(\varphi^{\prime}(x,\!y)\) as following form:

\[\varphi^{\prime}(x,\!y)\equiv\bigvee_{S\subseteq P_{2},T\subseteq N}\Big{(} \varphi_{S}(x,\!y)\wedge\varphi_{T}^{x}(x)\wedge\varphi_{S,T}^{y}(y)\Big{)} \tag{10}\]

In order to simplify the formula above, let \(\phi_{T}(x)\) denote the following formula:

\[\phi_{T}(x,\!y)\coloneqq\bigvee_{S\subseteq P_{2}}\Big{(}\varphi_{S}(x,\!y) \wedge\varphi_{S,T}^{y}(y)\Big{)} \tag{11}\]

Then we can simplify Equation (10) to the following form:

\[\varphi^{\prime}(x,\!y)\equiv\bigvee_{T\subseteq N}\Big{(}\varphi_{T}^{x}(x) \wedge\phi_{T}(x,\!y)\Big{)} \tag{12}\]

Recall that \(\varphi(x)=\exists^{\geq n}y(\varphi^{\prime}(x,\!y))\), so it can be rewritten as:

\[\varphi(x)\equiv\exists^{\geq n}y\bigg{(}\bigvee_{T\subseteq N}\big{(}\varphi _{T}^{x}(x)\wedge\phi_{T}(x,\!y)\big{)}\bigg{)} \tag{13}\]

Since for any graph \(G\) and its node \(a\), there exists exactly one \(T\) such that \(\varphi_{T}^{x}(a)\) is _true_. Therefore, Equation (13) can be rewritten as the following formula:

\[\varphi(x)\equiv\bigvee_{T\subseteq N}\bigg{(}\varphi_{T}^{x}(x)\wedge\exists^ {\geq n}y(\phi_{T}(x,\!y))\bigg{)} \tag{14}\]

Let \(\widehat{\varphi}_{T}(x)\coloneqq\exists^{\geq n}y(\phi_{T}(x,\!y))\). Since \(\wedge,\!\vee\) are both allowed in \(\mathcal{RSFOC}_{2}\). If we want to rewrite \(\varphi(x)\) in the \(\mathcal{RSFOC}_{2}\) form, it suffices to rewrite \(\widehat{\varphi}_{T}(x)\) as a \(\mathcal{RSFOC}_{2}\) formula, which is shown as follows,

\[\widehat{\varphi}_{T}(x)\coloneqq\exists^{\geq n}y(\phi_{T}(x,\!y))=\exists^{ \geq n}y\bigg{(}\bigvee_{S\subseteq P_{2}}\Big{(}\varphi_{S}(x,\!y)\wedge \varphi_{S,T}^{y}(y)\Big{)}\bigg{)} \tag{15}\]

Similar to the previous argument, since for any graph \(G\) and of of its node pairs \((a,\!b)\), the _relation-specification_ formula \(\varphi_{S}(x,\!y)\) restricts exactly which types of relations exists between \((a,\!b)\), there is exactly one subset \(S\subseteq P_{2}\) such that \(\varphi_{S}(a,\!b)\) holds.

Therefore, for all \(S\subseteq P_{2}\), we can define \(n_{S}\) as the number of nodes \(y\) such that \(\varphi_{S}(x,y)\wedge\varphi_{S,T}^{y}(y)\) holds. Since for two different subsets \(S_{1},S_{2}\subseteq P_{2}\) and a fixed \(y\), \(\varphi_{S_{1}}(x,y)\) and \(\varphi_{S_{2}}(x,y)\) can't hold simultaneously, the number of nodes \(y\) that satisfies \(\varphi_{S}(x,y)\wedge\varphi_{S,T}^{y}(y)\) is exactly the sum \(\sum_{S\subseteq P_{2}}n_{S}\). Therefore, in order to express Equation (15), which means there exists at least \(n\) nodes \(y\) such that \(\bigvee_{S\subseteq P_{2}}(\varphi_{S}(x,y)\wedge\varphi_{S,T}^{y}(y))\) holds, it suffices to enumerate all possible values for \(\{n_{S}|S\subseteq P_{2}\}\) that satisfies \((\sum_{S\subseteq P_{2}}n_{S})=n,n_{S}\in\mathbb{N}\). Formally, we can rewrite \(\widehat{\varphi}_{T}(x)\) as follows:

\[\widehat{\varphi}_{T}(x)\equiv\bigvee_{(\sum_{S\subseteq P_{2}}n_{S})=n} \Big{(}\bigwedge_{S\subseteq P_{2}}\exists^{\geq n_{S}}y(\varphi_{S}(x,y) \wedge\varphi_{S,T}^{y}(y))\Big{)} \tag{16}\]

Note that \(\exists^{\geq n_{S}}y(\varphi_{S}(x,y)\wedge\varphi_{S,T}^{y}(y))\) satisfies the grammar of \(\mathcal{RSFC}\mathcal{O}_{2}\), so \(\widehat{\varphi}_{T}(x)\) can be rewritten as \(\mathcal{RSFC}\mathcal{O}_{2}\). Then, since \(\varphi_{T}^{x}(x)\) can also be rewritten as \(\mathcal{RSFC}\mathcal{O}_{2}\) by induction, combining Equation (14) and Equation (15), \(\varphi(x)\) is in \(\mathcal{RSFC}\mathcal{O}_{2}\). We finish the proof. 

## Appendix B Proof of Proposition 3

**Proposition 3**.: \(\mathcal{FOC}_{2}\not\subseteq R^{2}\)_-GNN and \(R^{2}\)_-GNN \(\not\subseteq\mathcal{FOC}_{2}\) on some universal graph class \(\mathcal{G}_{u}\)._

Proof.: First, we prove \(\mathcal{FOC}_{2}\nsubseteq\text{R}^{2}\)-GNN.

Consider the two graphs \(G_{1}\),\(G_{2}\) in Figure 1. \((G_{1}\),\(a)\),\((G_{2}\),\(a)\) can be distinguished by the \(\mathcal{FOC}_{2}\) formula \(\varphi(x):=\exists^{\geq 1}y(p_{1}(x,y)\wedge p_{2}(x,y))\). However, we will prove that any \(\text{R}^{2}\)-GNN can't distinguish any node in \(G_{1}\) from any node in \(G_{2}\).

Let's prove it by induction over the layer number \(L\) of \(\text{R}^{2}\)-GNN. That's to say, we want to show that for any \(L\geq 0\), \(\text{R}^{2}\)-GNN with no more than \(L\) layers can't distinguish any node of \(G_{1}\) from that of \(G_{2}\).

For the base case where \(L=0\), since each node feature vector is initialized by the unary predicate information, so the result trivially holds.

Assume any \(\text{R}^{2}\)-GNN with no more than \(L=m\) layers can't distinguish nodes of \(G_{1}\) from nodes of \(G_{2}\). Then we want to prove the result for \(L=m+1\).

For any \(\text{R}^{2}\)-GNN model \(\mathcal{A}\) with \(m+1\) layers, let \(\mathcal{A}^{\prime}\) denote its first \(m\) layers, we know outputs of \(\mathcal{A}^{\prime}\) on any node from \(G_{1}\) or \(G_{2}\) are the same, suppose the common output feature is \(\mathbf{x}^{(m)}\).

Recall the updating rule of \(\text{R}^{2}\)-GNN in Equation (2).We know the output of \(\mathcal{A}\) on any node \(v\) in \(G_{1}\) or \(G_{2}\) is defined as follows,

\[\mathbf{x}^{(m+1)}_{v}=C^{(m+1)}\bigg{(}\mathbf{x}^{(m)}_{v},\!\!\big{(}A^{(m+ 1)}_{1}(\{\!\!\{\mathbf{x}^{(m)}_{u_{1}(v)}\}\!\!\})\big{)},\!A^{(m+1)}_{2}(\{ \!\!\{\mathbf{x}^{(m)}_{u_{2}(v)}\}\!\!\})\bigg{)},\!R^{(m+1)}(\{\!\!\{\mathbf{ x}^{(m)}_{u_{2}(v)}\}\!\!\})\bigg{)},R^{(m+1)}(\{\!\!\{\mathbf{f}\mathbf{x}^

[MISSING_PAGE_FAIL:19]

numbers no more than \(m\) and quantifier depth no more than \(k\). Since these two forms are symmetrical, without loss of generality, we only consider the case \(\exists^{\geq N}y\phi^{\prime}(x,y)\),\(N\leq m\).

Let \(N_{1}\) denote the number of nodes \(v^{\prime}_{1}\in V(G)\) such that \((G,\)\(u_{1},\)\(v^{\prime}_{1})\)\(\models\)\(\phi^{\prime}\) and \(N_{2}\) denote the number of nodes \(v^{\prime}_{2}\in V(H)\) such that \((H,\)\(u_{2},\)\(v^{\prime}_{2})\)\(\models\)\(\phi^{\prime}\). Let's compare values of \(N_{1}\) and \(N_{2}\). First, By induction, since we have \(CLS_{G}(u_{1})=CLS_{H}(u_{2})\) from precondition, so for any \(v^{\prime}_{1}\in V(G)\), \(v^{\prime}_{2}\in V(H)\), which satisfies \(CLS_{G}(v^{\prime}_{1})=CLS_{H}(v^{\prime}_{2})\), \(\phi^{\prime}(x,y)\) can't distinguish \((u_{1},\)\(v^{\prime}_{1})\) and \((u_{2},\)\(v^{\prime}_{2})\). Second, isomorphism tells us \(\phi^{\prime}\) can't distinguish node pairs from the same graph if they share the same \(CLS\) values. Combining these two facts, there has to be a subset \(S\subseteq\{1,2,3\}\), such that \(N_{1}=\sum_{a\in S}N_{G}(a)\) and \(N_{2}=\sum_{a\in S}N_{H}(a)\), where \(N_{G}(a)\) denotes the number of nodes \(u\) on \(G\) such that \(CLS_{G}(u)=a\), (\(N_{H}(a)\) is defined similarly).

It's easy to see that \(N_{G}(1)=N_{H}(1)=1\), and \(N_{G}(a)\),\(N_{H}(a)>m\) for \(a\in\{2,3\}\). Therefore, at least one of \(N_{1}=N_{2}\) and \(m<min\{N_{1},\)\(N_{2}\}\) holds. In neither case \(\exists^{\geq N}y\phi^{\prime}(x,y)\),\(N\leq m\) can distiugush \((u_{1},\)\(v_{1})\) and \((u_{2},\)\(v_{2})\). 

Note that in the above proof our graph class \(\{G(n),\)\(H(n)|n\in\mathbb{N}\}\) is actually a _simple_ graph class, so we can actually get the following stronger argument.

**Corollary 24.1**.: \(R^{2}\)_-GNN \(\not\subseteq\mathcal{FOC}_{2}\) on some simple graph class._

## Appendix C Proof of Theorem 4

**Theorem 4**.: \(\mathcal{FOC}_{2}\subseteq R^{2}\)_-GNN on any simple graph class, and \(\mathcal{FOC}_{2}\subsetneq R^{2}\)-GNN on some simple graph class._

Proof.: We just need to show \(\mathcal{FOC}_{2}\subseteq\) R\({}^{2}\)-GNN on any simple graph class, and the second part can be just concluded from Corollary 24.1. By Lemma 22, \(\mathcal{FOC}_{2}=\mathcal{RSFC}_{2}\), so it suffices to show \(\mathcal{RSFC}_{2}\subseteq\) 0/1-GNN. By Lemma 19, 0/1-GNN is closed under \(\wedge\), \(\vee\),\(\neg\), so we can only focus on formulas in \(\mathcal{RSFC}_{2}\) of form \(\varphi(x)=\exists^{\geq n}y(\varphi_{S}(x,\)\(y)\)\(\varphi^{\prime}(y))\),\(S\subseteq P_{2}\). If we can construct an equivalent 0/1-GNN \(\mathcal{A}\) for all formulas of above form, then we can capture all formulas in \(\mathcal{RSFC}_{2}\) since other generating rules \(\wedge\), \(\vee\),\(\neg\) is closed under 0/1-GNN. In particular, for the setting of _single-edge_ graph class, \(\varphi\) is meaningful only when \(|S|\leq 1\). That's because \(|S|>2\) implies that \(\varphi\) is just the trivial \(\bot\) in any _single-edge_ graph class \(\mathcal{G}_{s}\).

Do induction over quantifier depth \(k\) of \(\varphi(x)\). In the base case where \(k=0\), the result trivially holds since in this situation, the only possible formulas that needs to consider are unary predicates \(A(x)\), where \(A\in P_{1}\), which can be captured by the initial one-hot feature. Next, assume our result holds for all formulas with quantifier depth \(k\) no more than \(m\), it suffices to prove the result when quantifier depth of \(\varphi(x)=\exists^{\geq n}y(\varphi_{S}(x,\)\(y)\wedge\varphi^{\prime}(y))\) is \(m+1\). It follows that quantifier depth of \(\varphi^{\prime}(y)\) is no more than \(m\).

By induction, there is a 0/1-GNN model \(\mathcal{A}^{\prime}\) such that \(\mathcal{A}^{\prime}=\varphi^{\prime}\) on single-edge graph class. To construct \(\mathcal{A}\), we only need to append another layer on \(\mathcal{A}^{\prime}\). This layer \(L+1\) has dimension \(1\), whose parameters \(C^{(L+1)}\),\((A^{(L+1)}_{j})^{K}_{j=1}\),\(R^{(L+1)}\),\(b^{(L+1)}\) are set as follows:

1. When \(|S|=1\): Suppose \(S=\{j\}\), set \(A^{L+1}_{j,(1,1)}=1\),\(b^{L+1}=1-n\), where \(A^{L+1}_{j,(1,1)}\) denotes the element on the first row and first column of matrix \(A^{(L+1)}_{j}\). Other parameters in this layer are \(0\). This construction represents \(\mathbf{x}^{(L+1)}_{v}=max(0\),\(min((\sum_{u\in\mathcal{N}_{G,j}(v)}\mathbf{x}^{(L)}_{u})-(n-1)\),\(1))\). Since \(\mathbf{x}^{(L)}_{u}\) is classification result outputted by \(\mathcal{A}^{\prime}\) which is equivalent to \(\varphi^{\prime}\), \(\sum_{u\in\mathcal{N}_{G,j}(v)}\mathbf{x}^{(L)}_{u}\) counts the number of \(j\)-type neighbor \(u\) of \(v\) that satisfies \(\varphi^{\prime}(u)\). Therefore \(\mathbf{x}^{(L+1)}_{v}=1\) if and only if there exists at least \(n\)\(j\)-type neighbors satisfying the condition \(\varphi^{\prime}\), which is exactly what \(\varphi(x)\) means.
2. When \(|S|=0\): Let \(K=|P_{2}|\), for all \(j\in[K]\), set \(A^{L+1}_{j,(1,1)}=-1\), \(R^{(L+1)}_{1,1}=1\),\(b^{L+1}=1-n\) and all other parameters \(0\). This construction represents \(\mathbf{x}^{(L+1)}_{v}=max(0\),\(min((\sum_{u\in V(G)}\mathbf{x}^{(L)}_{u})-(\sum_{j=1}^{K}\sum_{u \in\mathcal{N}_{G,j}(v)}\mathbf{x}^{(L)}_{u})-(n-1)\),\(1))\). Since we only consider single-edge graph, \((\sum_{u\in V(G)}\mathbf{x}_{u}^{(L)})-(\sum_{j=1}^{K}\sum_{u\in\mathcal{N}_{G,j}(v )}\mathbf{x}_{u}^{(L)})\) exactly counts the number of nodes \(u\) that satisfies \(\varphi^{\prime}(y)\) and doesn't have any relation with \(v\). It's easy to see that \(\mathbf{x}_{v}^{(L+1)}=1\) iff there exists at least \(n\) such nodes \(u\), which is exactly what \(\varphi(x)\) means.

Hence, we finish the proof for Theorem 4 - for each \(\mathcal{FOC}_{2}\) formula over the single-edge graph class, we can construct an R\({}^{2}\)-GNN to capture it.

## Appendix D Proof of Theorem 5 and Theorem 6

**Theorem 5**.: \(R^{2}\)_-GNN \(\subseteq\mathcal{FOC}_{2}\) on any bounded graph class, and \(R^{2}\)-GNN \(\subsetneq\mathcal{FOC}_{2}\) on some bounded graph class._

**Theorem 6**.: _For any bounded graph class \(\mathcal{G}_{b}\). Suppose any \(G\in\mathcal{G}_{b}\) has no more than \(N\) nodes, and \(\mathcal{G}_{b}\) has unary predicate set \(P_{1}\) and relation (binary predicate) set \(P_{2}\). Let \(m_{1}:=|P_{1}|,m_{2}:=|P_{2}|\), then for any node classifier \(c\), suppose \(c\) can be represented as an R\({}^{2}\)-GNN with depth (layer number) \(L\), then by Theorem 5 there is a \(\mathcal{FOC}_{2}\) classifier \(\varphi\) equivalent to \(c\) over \(\mathcal{G}_{b}\). Moreover, the followings hold:_

_1. The quantifier depth of \(\varphi\) is no more than \(L\)._

_2. The size of \(\varphi\) (quantified by the number of nodes of \(\varphi\)'s parse tree) is no more than \(2^{2f(L)}\), where \(f(L):=2^{2^{2(N+1)f(L-1)}}\),\(f(0)=O(2^{2^{2(m_{1}+m_{2})}})\)._

For Theorem 5, we just need to show R\({}^{2}\)-GNN \(\subseteq\mathcal{FOC}_{2}\) on any bounded graph class. The second part can then be shown by the fact that the graph class \(\{G_{1},G_{2}\}\) in Figure 1 is a bounded graph class but \(\mathcal{FOC}_{2}\nsubseteq\) R\({}^{2}\)-GNN still holds. In the following proof, we also show how to get the complexity upper bound claimed in Theorem 6. If we want to prove R\({}^{2}\)-GNN \(\subseteq\mathcal{FOC}_{2}\), it suffices to show that for any R\({}^{2}\)-GNN \(\mathcal{A}\), there exists an equivalent \(\mathcal{FOC}_{2}\) formula \(\varphi\) on any bounded graph class \(\mathcal{G}_{b}\). It implies that for two graphs \(G_{1},G_{2}\) and their nodes \(a,b\), if they are classified differently by \(\mathcal{A}\), there exists some \(\mathcal{FOC}_{2}\) formula \(\varphi\) that can distinguish them. Conversly, if \(a,b\) can't be distinguished by any \(\mathcal{FOC}_{2}\) formula, then they can't be distinguished by any R\({}^{2}\)-GNN as well.

**Definition 25**.: _For a set of classifiers \(\Psi=\{\psi_{1}.....\psi_{m}\}\), \(a\)\(\Psi\)-truth-table \(T\) is a 0/1 string of length \(m\). \(T\) can be seen as a classifier, which classifies a node \(v\) to be true if and only if for any \(1\leq i\leq m\), the classification result of \(\psi_{i}\) on \(v\) equals to \(T_{i}\), where \(T_{i}\) denotes the \(i\)-th bit of string \(T\). We define \(\mathcal{T}(\Psi):=\{0,1\}^{m}\) as the set of all \(\Psi\)-truth-tables. We have that for any graph \(G\) and its node \(v\), \(v\) satisfies exactly one truth-table \(T\)._

**Proposition 26**.: _Let \(\mathcal{FOC}_{2}(n)\) denote the set of formulas of \(\mathcal{FOC}_{2}\) with quantifier depth no more than \(n\). For any bounded graph class \(\mathcal{G}_{b}\) and \(n\), only finitely many intrinsically different node classifiers on \(\mathcal{G}_{b}\) can be represented by \(\mathcal{FOC}_{2}(n)\). Furthermore, define \(N,m_{1},m_{2}\) as in Theorem 6, the number of intrinsically different \(\mathcal{FOC}_{2}(n)\) node classifiers on \(\mathcal{G}_{b}\) and their parse tree sizes are all upper bounded by \(f(n)\) as defined in Theorem 6._

Proof.: Suppose all graphs in \(\mathcal{G}_{b}\) have no more than \(N\) constants, then for any natural number \(m>N\), formulas of form \(\exists^{\geq m}y(\varphi(x,y))\) are always false. Therefore, it's sufficient only to consider \(\mathcal{FOC}_{2}\) logical classifiers with threshold numbers no more than \(N\) on \(\mathcal{G}_{b}\).

There are only \(m_{1}+m_{2}\) predicates, and each boolean combination of unary predicates using \(\wedge\), \(\vee\),\(\neg\) can be rewritten in the form of Disjunctive Normal Form (DNF) (Davey and Priestley [2002]). So there are only at most \(f(0)=2^{2^{2(m_{1}+m_{2})}}\) intrinsically different formulas in \(\mathcal{FOC}_{2}\) with quantifier depth \(0\). Note that \(2(m_{1}+m_{2})\) is the number of terms, \(2^{2(m_{1}+m_{2})}\) is the number of different truth-table conjunctions on these terms, and \(2^{2(m_{1}+m_{2})}\) is the number of different DNFs on these conjunctions. Each DNF has parse tree of size at most \(1+2^{2(m_{1}+m_{2})}(1+2m_{1}+2m_{2})\leq 1000\cdot 2^{2^{2(m_{1}+m_{2})}}\). Therefore, define \(f(0)=1000\cdot 2^{2^{2(m_{1}+m_{2})}}=O(2^{2^{2(m_{1}+m_{2})}})\), we know the number of different \(\mathcal{FOC}_{2}\) formulas with quantifier depth \(0\) and parse tree size of these formulas can both be upper bounded by \(f(0)\).

By induction, suppose there are only \(f(k)\) intrinsically different \(\mathcal{FOC}_{2}(k)\) formulas on \(\mathcal{G}_{b}\). and each meaningful \(\mathcal{FOC}_{2}(k+1)\) formula is generated by the following grammar

\[\varphi_{1}\wedge\varphi_{2},\!\varphi_{1}\vee\varphi_{2},\neg\varphi_{2},\! \exists^{\geq m}y(\varphi^{\prime}(x,\!y)),\!m\leq N \tag{18}\]

where \(\varphi_{1},\!\varphi_{2}\) are \(\mathcal{FOC}_{2}(k+1)\) formulas and \(\varphi^{\prime}\) is \(\mathcal{FOC}_{2}(k)\) formulas.

Given that only the rule \(\exists^{\geq m}y(\varphi^{\prime}(x,\!y))\) can increase the quantifier depth from \(k\) to \(k+1\), \(m\leq N\), and there are only \(f(k)\) intrinsically different \(\varphi^{\prime}(x,\!y)\in\mathcal{FOC}_{2}(k)\) on \(\mathcal{G}_{b}\) by induction. Therefore, there are only \((2N+2)f(k)\) intrinsically different \(\mathcal{FOC}_{2}(k+1)\) formulas of form \(\exists^{\geq m}y(\varphi^{\prime}(x,\!y)),\!\exists^{\geq m}x(\varphi^{ \prime}(x,\!y))\) or in \(\mathcal{FOC}_{2}(k)\) on \(\mathcal{G}_{b}\). Moreover, their boolean combination using \(\wedge\), \(\vee\),\(\neg\) can be always rewritten in the DNF form, So there are also finitely many intrinsically different \(\mathcal{FOC}_{2}(k+1)\) logical classifiers on \(\mathcal{G}_{b}\). Similarly, we can bound the number of different DNF by \(f(k+1)=2^{2^{2(N+1)f(k)}}\), where \(2(N+1)f(k)\) is the number of "building blocks" which are sub-formulas with smaller quantifier depth or outermost symbol \(\exists\), \(2^{2(N+1)f(k)}\) is the number of different conjunctions on these building blocks, and \(f(k+1)=2^{2^{2(N+1)f(k)}}\) is the number of different DNFs on these conjunctions. Parse tree size of each of these DNFs is at most \(1+2^{2(N+1)f(k)}(1+2(N+1)f(k)(1+f(k)))\leq 2^{2^{2(N+1)f(k)}}=f(k+1)\). The LHS is from the inductive assumption that each \(\mathcal{FOC}_{2}(k)\) formula has a equivalent representation within \(f(k)\) parse tree size. The inequality is because we know \(f(k)\geq 1000\). Thus, we can upper bound the number of intrinsically different \(\mathcal{FOC}_{2}(k+1)\) formulas on \(\mathcal{G}_{b}\) and their parse tree size both by \(f(k+1)\). 

**Lemma 27**.: _For any two pairs \((G_{1},v_{1})\) and \((G_{2},v_{2})\), where \(G_{1}\) and \(G_{2}\) are two bounded graphs from \(\mathcal{G}_{b}\) and \(v_{1}\) and \(v_{2}\) are two nodes in \(G_{1}\) and \(G_{2}\), respectively. If all logical classifiers in \(\mathcal{FOC}_{2}(L)\) can't distinguish \(v_{1},\!v_{2}\), then any R\({}^{2}\)-GNN with layer no more than \(L\) can't distinguish them as well._

Proof.: By one-hot feature initialization function of R\({}^{2}\)-GNN, \(\mathcal{FOC}_{2}(0)\) can distinguish all different one-hot intial features, so the lemma trivially holds for the base case (\(L=0\)).

For the inductive step, we suppose Lemma 27 holds for all \(L\leq k\), then we can assume \(v_{1},\!v_{2}\) can't be distinguished by \(\mathcal{FOC}_{2}(k+1)\). Let \(N=k+1\)

\(G_{1}\) and \(G_{2}\) are _bounded_ graphs from \(\mathcal{G}_{b}\), so \(\mathcal{FOC}_{2}(N)\) has finitely many intrinsically different classifiers according to Proposition 26. Let \(\mathcal{TT}_{N}(v)\) denote the \(\mathcal{FOC}_{2}(N)\)-truth-table satisfied by \(v\). According to Definition 25, we know that for any \(T\in\mathcal{T}(\mathcal{FOC}_{2}(N))\), there exists a \(\mathcal{FOC}_{2}(N)\) classifier \(\varphi_{T}\) such that for any node \(v\) on \(G_{i}\), where \(i\in 1,2\), \(\mathcal{TT}_{N}(v)=T\Leftrightarrow(G_{i},\!v)\models\varphi_{T}\).

Assume there is an R\({}^{2}\)-GNN \(\mathcal{A}\) that distinguish \(v_{1},\!v_{2}\) with layer \(L=k+1\). Let \(\widehat{\mathcal{A}}\) denote its first \(k\) layers. By update rule of R\({}^{2}\)-GNN illustrated in Equation 2, output of \(\mathcal{A}\) on node \(v\) of graph \(G\), \(\mathbf{x}_{v}^{(k+1)}\) only dependent on the following three things:

* output of \(\widehat{\mathcal{A}}\) on \(v\), \(\mathbf{x}_{v}^{(k)}\)
* multiset of outputs of \(\widehat{\mathcal{A}}\) on \(r\)-type neighbors of \(v\) for each \(r\in P_{2}\), \(\{\mathbf{x}_{u}^{(k)}|u\in\mathcal{N}_{G,r}(v)\}\)
* multiset of outputs of \(\widehat{\mathcal{A}}\) on all nodes in the graph, \(\{\mathbf{x}_{u}^{(k)}|u\in\mathcal{N}_{G,r}(v)\}\)a \(\mathcal{FOC}_{2}(k+1)\) formula \(\exists^{\geq n_{2}}y_{\mathcal{T}}(y)\) that distinguishes \(v_{1}\) and \(v_{2}\), which contradicts the precondition that they can't be distinguished by \(\mathcal{FOC}_{2}(k+1)\) classifiers.

Since all possibilities contradicts the precondition that \(v_{1}\),\(v_{2}\) can't be distinguished by \(\mathcal{FOC}_{2}(k+1)\), such an \(\mathcal{A}\) that distinguishes \(v_{1}\),\(v_{2}\) doesn't exist. 

We can now gather all of these to prove Theorem 5 and Theorem 6.

Proof.: For any R\({}^{2}\)-GNN \(\mathcal{A}\), suppose it has \(L\) layers. For any graph \(G\in\mathcal{G}_{b}\) and its node \(v\), let \(\mathcal{TT}_{L}(v)\) denote the \(\mathcal{FOC}_{2}(L)\)-truth-table satisfied by \(v\). For any \(T\in\mathcal{T}(\mathcal{FOC}_{2}(L))\), since \(\mathcal{G}_{b}\) is a bounded graph class, using Proposition 26, there exists a \(\mathcal{FOC}_{2}(L)\) classifier \(\varphi_{T}\) such that for any node \(v\) in graph \(G\in\mathcal{G}_{b}\), \(\mathcal{TT}_{L}(v)=T\Leftrightarrow(G,v)\models\varphi_{T}\). Moreover, by Proposition 26, since \(T\) is a truth table on at most \(f(L)\) formulas, \(\varphi_{T}\) can be written as a conjunction over \(f(L)\) literals, which means \(\varphi_{T}\) has parse tree size at most \(1+f(L)^{2}\) since by Proposition 26, every formula in \(\mathcal{FOC}_{2}(L)\) is equivalent to some \(\mathcal{FOC}_{2}\) formula with parse tree size at most \(f(L)\).

By Lemma 27, If two nodes \(v_{1}\),\(v_{2}\) have same \(\mathcal{FOC}_{2}(L)\)-truth-table (\(\mathcal{TT}_{L}(v_{1})=\mathcal{TT}_{L}(v_{2})\)), they can't be distinguished by \(\mathcal{A}\). Let \(S\) denote the subset of \(\mathcal{T}(\mathcal{FOC}_{2}(L))\) that satisfies \(\mathcal{A}\). By Proposition 26 and Definition 25, \(\Phi:=\{\varphi_{T}|T\in S\}\) is a finite set with \(|\Phi|\leq 2^{f(L)}\), then disjunction of formulas in \(\Phi\), \((\bigvee_{T\in S}\varphi_{T})\) is a \(\mathcal{FOC}_{2}\) classifier that equals to \(\mathcal{A}\) under bounded graph class \(\mathcal{G}_{b}\). Furthermore, by the above upper bound of parse tree size of any \(\varphi_{T}\), \((\bigvee_{T\in S}\varphi_{T})\) has parse tree size no more than \(1+2^{f(L)}(1+f(L)^{2})\leq 2^{2f(L)}\), where the inequality is from \(f(L)\geq 1000\). We complete the proof. 

## Appendix E Proof of Theorem 9

See 9

Proof.: Assume that we have a predicate set \(P=P_{1}\cup P_{2}\), \(K=|P_{2}|\) and let \(P^{\prime}=P\cup\{primal\text{,}aux1\text{,}aux2\}\) denote the predicate set after transformation \(F\). For any R\({}^{2}\)-GNN \(\mathcal{A}\) under \(P\), we want to construct another R\({}^{2}\)-GNN \(\mathcal{A}^{\prime}\) under \(P^{\prime}\), such that for any graph \(G\) under \(P\) and its node \(v\), \(v\) has the same feature outputted by \(\mathcal{A}(G,\)\(v)\) and \(\mathcal{A}^{\prime}(F(G),\)\(v)\). Let \(L\) denote the layer number of \(\mathcal{A}\).

We prove this theorem by induction over the number of layers \(L\). In the base (\(L=0\)), our result trivially holds since the one-hot initialization over \(P^{\prime}\) contains all unary predicate information in \(P\). Now suppose the result holds for \(L\leq k\), so it suffices to prove it when \(L=k+1\).

For the transformed graph \(F(G)\), _primal_(v) is _true_ if and only if \(v\) is the node in the original graph \(G\). Without loss of generality, if we use one-hot feature initialization on \(P^{\prime}\), we can always keep an additional dimension in the node feature vector \(\mathbf{x}_{v}\) to show whether _primal_(v) is _true_, its value is always \(0/1\), in the proof below when we use \(\mathbf{x}\) to denote the feature vectors, we omit this special dimension for simplicity. But keep in mind that this dimension always keeps so we can distinguish original nodes and added nodes.

Recall that an R\({}^{2}\)-GNN is defined by \(\{C^{(i)},(A^{(i)}_{j})_{i=1}^{K},R^{(i)}\}_{i=1}^{L}\). By induction, let \(\widehat{\mathcal{A}}\) denote the first \(k\) layers of \(\mathcal{A}\), and let \(\widehat{\mathcal{A}}^{\prime}\) denote the R\({}^{2}\)-GNN equivalent with \(\widehat{\mathcal{A}}\) on \(F\) transformation such that \(\widehat{\mathcal{A}}=\widehat{\mathcal{A}}^{\prime}\circ F\). We will append three layers to \(\widehat{\mathcal{A}}^{\prime}\) to construct \(\mathcal{A}^{\prime}\) that is equivalent to \(\mathcal{A}\). Without loss of generality, we can assume all layers in \(\mathcal{A}\) have same dimension length \(d\). Suppose \(L^{\prime}\) is the layer number of \(\widehat{\mathcal{A}}^{\prime}\), so we will append layer \(L^{\prime}+1\),\(L^{\prime}+2\),\(L^{\prime}+3\). for all \(l\in\{L^{\prime}+1\),\(L^{\prime}+2\),\(L^{\prime}+3\}\), let \(\{C^{a,(l)}\),\(C^{p,(l)}\),\((A^{*,(l)}_{j})_{j=1}^{K}\),\(A^{*,(l)}_{aux1}\),\(A^{*,(l)}_{aux2}\),\(R^{*,(l)}\}\) denote the parameters in \(l\)-th layer of \(\mathcal{A}\). Here, \(A^{*,(l)}_{aux2}\) denotes the aggregation function corresponding to two new predicates _aux1_,_aux2_, added in transformation \(F\), and \(C^{p,(l)}\),\(C^{a,(l)}\) are different combination function that used for primal nodes and non-primal nodes. Note that with the help of the special dimension mentioned above, we can distinguish primal nodes and non-primal nodes. Therefore, It's safe to use different combination functions for these two kinds of nodes. Note that here since we add two predicates _aux1_,_aux2_, the input for combination function should be in the form \(C^{p}(\mathbf{x}_{0},(\mathbf{x}_{j})_{j=1}^{K}\mathbf{x}_{aux1}\mathbf{x}_{aux2 }\mathbf{x}_{g})\) where \(\mathbf{x}_{0}\) is the feature vector of the former layer, and \(\mathbf{x}_{j},1\leq j\leq K\) denote the output of aggregation function \(A^{*,(l)}_{j}\), \(\mathbf{x}_{aux1}\),\(\mathbf{x}_{aux2}\) denote the output of aggregation function \(A^{*,(l)}_{aux1}\),\(A^{*,(l)}_{aux2}\), and \(\mathbf{x}_{g}\) denotes the feature outputted by global readout function \(R^{*,(l)}\). For aggregation function and global readout function, their inputs are denoted by \(\mathbf{X}\), meaning a multiset of feature vector. Note that all aggregation functions and readout functions won't change the feature dimension, only combination functions \(C^{p,(l)}\),\(C^{a,(l)}\) will transform \(d_{l-1}\) dimension features to \(d_{l}\) dimension features.

1). layer \(L^{\prime}+1\): input dimension is \(d\), output dimension is \(d^{\prime}=Kd\). For feature vector \(\mathbf{x}\) with length \(d^{\prime}\), let \(\mathbf{x}^{(i)},i\in\{1,\ldots,K\}\) denote its \(i\)-th slice in dimension \([(i-1)d+1\),\(id]\). Let \([\mathbf{x}_{1},\ldots,\mathbf{x}_{m}]\) denote concatenation of \(\mathbf{x}_{1},\ldots,\mathbf{x}_{m}\), and let \([\mathbf{x}]^{n}\) denote concatenation of \(n\) copies of \(\mathbf{x}\), \(\mathbf{0}^{n}\) denote zero vectors of length \(n\). parameters for this layer are defined below:

\[C^{p,(L^{\prime}+1)}(\mathbf{x}_{0},(\mathbf{x}_{j})_{j=1}^{K},\mathbf{x}_{aux1},\mathbf{x}_{aux2},\mathbf{x}_{g})=[\mathbf{x}_{0},\mathbf{0} ^{d^{\prime}-d}] \tag{19}\] \[C^{a,(L^{\prime}+1)}(\mathbf{x}_{0},(\mathbf{x}_{j})_{j=1}^{K},\mathbf{x}_{aux1},\mathbf{x}_{aux2},\mathbf{x}_{g})=[\mathbf{x}_{aux1}]^{K}\] (20) \[A^{*,(L^{\prime}+1)}_{aux1}(\mathbf{X})=\sum_{\mathbf{x}\in \mathbf{X}}\mathbf{x} \tag{21}\]

Other parameters in this layer are set to functions that always output zero-vector.

We can see here that the layer \(L^{\prime}+1\) do the following thing:

For all primal nodes \(a\) and its non-primal neighbor \(e_{ab}\), pass concatenation of \(K\) copies of \(\mathbf{x}_{a}\) to \(\mathbf{x}_{e_{ab}}\), and remains the feature of primal nodes unchanged.

2). layer \(L^{\prime}+2\), also has dimension \(d^{\prime}=Kd\), has following parameters.

\[C^{p,(L^{\prime}+2)}(\mathbf{x}_{0},(\mathbf{x}_{j})_{j=1}^{K},\mathbf{x}_{aux1},\mathbf{x}_{aux2},\mathbf{x}_{g})=\mathbf{x}_{0} \tag{22}\] \[C^{a,(L^{\prime}+2)}(\mathbf{x}_{0},(\mathbf{x}_{j})_{j=1}^{K},\mathbf{x}_{aux1},\mathbf{x}_{aux2},\mathbf{x}_{g})=\sum_{j=1}^{K}\mathbf{x} _{j}\] (23) \[\forall j\in[1,K],A^{*,(L^{\prime}+2)}_{j}(\mathbf{X})=[\mathbf{0 }^{(j-1)d},\sum_{\mathbf{x}\in\mathbf{X}}\mathbf{x}^{(j)},\mathbf{0}^{(K-j)d}] \tag{24}\]

All other parameters in this layer are set to function that always outputs zero vectors. This layer do the following thing:

For all primal nodes, keep the feature unchanged, for all added node pair \(e_{ab}\),\(e_{ba}\). Switch their feature, but for all \(r_{i}\in P_{2}\), if there is no \(r_{i}\) relation between \(a\),\(b\), the \(i\)-th slice of \(\mathbf{x}_{e_{ab}}\) and \(\mathbf{x}_{e_{ba}}\) will be set to \(\mathbf{0}\).

3). layer \(L^{\prime}+3\), has dimension \(d\), and following parameters.

\[C^{p,(L^{\prime}+3)}(\mathbf{x}_{0},(\mathbf{x}_{j})_{j=1}^{K},\mathbf{x}_{aux1},\mathbf{x}_{aux2},\mathbf{x}_{g})=C^{(L)}(\mathbf{x}_{0}^{ (1)},(\mathbf{x}_{aux1}^{(j)})_{j=1}^{K}\mathbf{x}_{g}^{(1)}) \tag{25}\] \[R^{*,(L^{\prime}+3)}(\mathbf{X})=[R^{(L)}(\{\mathbf{x}_{v}^{(1)}| \mathbf{x}_{v}\in\mathbf{X},\mathbf{primal}(v)\}),\mathbf{0}^{d^{\prime}-d}]\] (26) \[A^{*,(L^{\prime}+3)}_{aux1}(\mathbf{X})=[A^{(L)}_{1}(\{\mathbf{x }^{(1)}|\mathbf{x}\in\mathbf{X}\}).......A^{(L)}_{K}(\{\mathbf{x}^{(K)}| \mathbf{x}\in\mathbf{X}\})] \tag{27}\]

Note that \(C^{(L)}\),\(A^{(L)}_{j}\),\(R^{(L)}\) are all parameters in the last layer of \(\mathcal{A}\) mentioned previously. All other parameters in this layer are set to functions that always output zero vectors. We can see that this layer simulates the work of last layer of \(\mathcal{A}\) as follows:

* For all \(1\leq j\leq K\)By construction above, \(\mathcal{A}^{\prime}\) is a desired model that have the same output as \(\mathcal{A}\).

## Appendix F Proof of Theorem 10

**Theorem 10**.: \(\mathcal{FOC}_{2}\subseteq R^{2}\)_-GNN \(\circ F\) on any universal graph class \(\mathcal{G}_{u}\)._

Proof.: For any \(\mathcal{FOC}_{2}\) classifier \(\varphi\) under predicate set \(P\), we want to construct a 0/1-GNN \(\mathcal{A}\) on \(P^{\prime}=P\cup\{primal,\)_aux1,_aux2_\} equivalent to \(\varphi\) with _graph transformation_\(F\).

Recall that \(\mathcal{FOC}_{2}=\mathcal{RSFOC}_{2}\) shown in Lemma 22 and 0/1-GNNs \(\subseteq\) R\({}^{2}\)-GNNs, it suffices to prove that 0/1-GNN\(\circ F\) capture \(\mathcal{RSFOC}_{2}\). By Lemma 19, since \(\wedge\), \(\vee\),\(\neg\) are closed under 0/1-GNN it suffices to show that when \(\varphi\) is in the form \(\exists^{\geq n}\big{(}\varphi_{S}(x,y)\wedge\varphi^{\prime}(y)\big{)},S \subseteq P_{2}\), we can capture it.

We prove by induction over quantifier depth \(m\) of \(\varphi\). Since \(0\)-depth formulas are only about unary predicate that can be extracted from one-hot initial feature, our theorem trivially holds for \(m=0\). Now, we assume it also holds for \(m\leq k\), it suffices to prove the case when \(m=k+1\). Then there are two possibilities:

1. When \(S\neq\emptyset\):

Consider the following logical classifier under \(P^{\prime}\):

\[\widehat{\varphi}_{S}(x):=\Big{(}\bigwedge_{r\in S}\exists xr(x,y)\Big{)} \wedge\Big{(}\bigwedge_{r\notin S}\neg\exists xr(x,y)\Big{)} \tag{28}\]

\(\widehat{\varphi}_{S}(x)\) restricts that for any \(r\in P^{\prime}\), \(x\) has \(r\)-type neighbor if and only if \(r\in S\). Review the definition of transformation \(F\), we know that for any added node \(e_{ab}\), \((F(G),e_{ab})\models\widehat{\varphi}_{S}\) if and only if \((G,\)_a_,_b_\()\models\varphi_{S}(a,b)\), where \(\varphi_{S}(x,y)\) is the _relation-specification_ formula defined in Definition 21 That is to say for any \(r_{i},1\leq i\leq K\), there is relation \(r_{i}\) between \(a,\)\(b\) if and only if \(i\in S\).

Now consider the following formula:

\[\widehat{\varphi}:=\exists^{\geq n}y\bigg{(}\textit{aux1}(x,y)\wedge\widehat{ \varphi}_{S}(y)\wedge\Big{(}\exists x\big{(}\textit{aux2}(x,y)\wedge(\exists y (\textit{aux1}(x,y)\wedge\varphi^{\prime}(y)))\big{)}\Big{)}\bigg{)}\bigg{)} \tag{29}\]

For any graph \(G\) and its node \(v\), it's easy to see that \((G,\)\(v)\models\varphi\Leftrightarrow(F(G),\)\(v)\models\widehat{\varphi}\). Therefore we only need to capture \(\widehat{\varphi}\) by 0/1-GNN on every primal node of transformed graphs. By induction, since quantifier depth of \(\varphi^{\prime}(y)\) is no more than \(k\), we know \(\varphi^{\prime}(y)\) is in 0/1-GNN. \(\widehat{\varphi}\) is generated from \(\varphi^{\prime}(y)\) using rules \(\wedge\) and \(\exists y\big{(}r(x,y)\wedge\varphi^{\prime}(y)\big{)}\). By Lemma 19, \(\wedge\) is closed under 0/1-GNN. For \(\exists y\big{(}r(x,y)\wedge\varphi^{\prime}(y)\big{)}\), we find that the construction needed is the same as construction for single-element \(S\) on single-edge graph class \(\mathcal{G}_{s}\) used in Theorem 4. Therefore, since we can manage these two rules, we can also finish the construction for \(\widehat{\varphi}\), which is equivalent to \(\varphi\) on primal nodes of transformed graph.

2. When \(S=\emptyset\)

First, consider the following two logical classifiers:

\[\bar{\varphi}(x):=\Big{(}\textit{primal}(x)\wedge\varphi^{\prime}(x)\Big{)} \tag{30}\]

\(\bar{\varphi}\) says a node is primal, and satisfies \(\varphi^{\prime}(x)\). Since \(\varphi^{\prime}(x)\) has quantifier depth no more than \(k\), and \(\wedge\) is closed under 0/1-GNN. There is a 0/1-GNN \(\mathcal{A}_{1}\) equivalent to \(\bar{\varphi}\) on transformed graph. Then, consider the following formula.

\[\bar{\varphi}(x):=\exists y\big{(}\textit{aux2}(x,y)\wedge(\exists x,\textit{ aux1}(1(x,y)\wedge\varphi^{\prime}(x))\big{)} \tag{31}\]

\(\bar{\varphi}(x)\) evaluates on added nodes \(e_{ab}\) on transformed graph, \(e_{ab}\) satisfies it iff \(b\) satisfies \(\varphi^{\prime}\)

Now for a graph \(G\) and its node \(v\), define \(n_{1}\) as the number of nodes on \(F(G)\) that satisfies \(\bar{\varphi}\), and define \(n_{2}\) as the number of _aux1_-type neighbors of \(v\) on \(F(G)\) that satisfies \(\bar{\varphi}\). Since \(\varphi(x)=\exists^{\geq n}y(\varphi_{\emptyset}(x,y)\wedge\varphi^{\prime}(y))\) It's easy to see that \((G,\)\(v)\models\varphi\) if and only if \(n_{1}-n_{2}\geq n\).

Formally speaking, for a node set \(S\), let \(|S|\) denote number of nodes in \(S\), we define the following classifier \(c\) such that for any graph \(G\) and its node \(a\), \(c(F(G),a)=1\Leftrightarrow(G,a)\models\varphi\)

\[c(F(G),a)=1\Leftrightarrow|\{v|v\in V(F(G)),(F(G),v)\models\bar{\varphi}\}|-| \{v|v\in\mathcal{N}_{F(G),aux1}(v),(F(G),v)\models\bar{\varphi}\}|\geq n \tag{32}\]

So how to construct a model \(\mathcal{A}\) to capture classifier \(c\)? First, by induction \(\bar{\varphi},\bar{\varphi}\) are all formulas with quantifier depth no more than \(k\) so by previous argument there are 0/1-GNN models \(\mathcal{\tilde{A}},\mathcal{\tilde{A}}\) that capture them respectively. Then we can use feature concatenation technic introduced in Equation (6) to construct a model \(\mathcal{\tilde{A}}\) based on \(\mathcal{\tilde{A}},\mathcal{\tilde{A}}\), such that \(\mathcal{\tilde{A}}\) has two-dimensional output, whose first and second dimensions have the same output as \(\mathcal{\tilde{A}},\mathcal{\tilde{A}}\) respectively.

Then, suppose \(\mathcal{\tilde{A}}\) has \(L\) layers, The only thing we need to do is to append a new layer \(L+1\) to \(\mathcal{\tilde{A}}\), it has output dimension \(1\). parameters of it are \(\{C^{(L+1)},(A^{(L+1)}_{j})^{K},A^{(L+1)}_{aux1},A^{(L+1)}_{aux2},R^{(L+1)}\}\) as defined in Equation (5). The parameter settings are as follows:

\(\textbf{R}^{(L+1)}_{1,1}=1,\textbf{A}^{(L+1)}_{aux1,(1,2)}=-1,\textbf{b}^{(L+ 1)}_{1}=1-n\). Other parameters are set to \(0\), where \(\textbf{A}^{(L+1)}_{aux1,(1,2)}\) denotes the value in the first row and second column of \(\textbf{A}^{(L+1)}_{aux1}\).

In this construction, we have

\(\textbf{x}^{(L+1)}_{v}=max(0,min(1,\sum_{u\in V(F(G))}\textbf{x}^{(L)}_{u,1}- \sum_{u\in\mathcal{N}_{F(G),aux1}(v)}\textbf{x}^{(L)}_{u,2}-(n-1)))\), which has exactly the same output as classifier \(c\) defined above in Equation (32). Therefore, \(\mathcal{A}\) is a desired model. 

## Appendix G Proof of Theorem 11

**Theorem 11**.: \(R^{2}\)_-GNN \(\circ F\subseteq\mathcal{FOC}_{2}\) on any bounded graph class \(\mathcal{G}_{b}\)._

Before we go into theorem itself, we first introduce Lemma 28 that will be used in following proof.

**Lemma 28**.: _Let \(\varphi(x,y)\) denote a \(\mathcal{FOC}_{2}\) formula with two free variables, for any natural number \(n\), the following sentence can be captured by \(\mathcal{FOC}_{2}\):_

_There exists no less than \(n\) ordered node pairs \((a,b)\) such that \((G,a,b)\models\varphi\)._

_Let \(c\) denote the graph classifier such that \(c(G)=1\) iff \(G\) satisfies the sentence above._

Proof.: The basic intuition is to define \(m_{i},1\leq i<n\) as the number of nodes \(a\), such that there are **exactly**\(i\) nodes \(b\) that \(\varphi(a,b)\) is _true_. Specially, we define \(m_{n}\) as the number of nodes \(a\), such that there are **at least**\(n\) nodes \(b\) that \(\varphi(a,b)\) is _true_. Since \(\sum_{i=1}^{n}im_{i}\) exactly counts the number of valid ordered pairs when \(m_{n}=0\), and it guarantees the existence of at least \(n\) valid ordered pairs when \(m_{n}>0\). It's not hard to see that for any graph \(G\), \(c(G)=1\Leftrightarrow\sum_{i=1}^{n}im_{i}\geq n\). Futhermore, fix a valid sequence \((m_{1}......m_{n})\) such that \(\sum_{i=1}^{n}im_{i}\geq n\), there has to be another sequence \((k_{1}......k_{n})\) such that \(n\leq\sum_{i=1}^{n}ik_{i}\leq 2n\) and \(k_{i}\leq m_{i}\) for all \(1\leq i\leq n\). Therefore, We can enumerate all possibilities of valid \((k_{1}......k_{n})\), and for each valid \((k_{1}......k_{n})\) sequence, we judge whether there are **at least**\(k_{i}\) such nodes \(a\) for every \(1\leq i\leq n\).

Formally, \(\varphi_{i}(x):=\exists^{[i]}y\varphi(x,y)\) can judge whether a node \(a\) has exactly \(i\) partners \(b\) such that \(\varphi(a,b)=1\), where \(\exists^{[i]}y\varphi(x,y)\) denotes "there are exactly \(i\) such nodes \(y\)" which is the abbreviation of formula \((\exists^{2i}y\varphi(x,y))\wedge(\neg\exists^{2i+1}y\varphi(x,y))\). The \(\mathcal{FOC}_{2}\) formula equivalent to our desired sentence \(c\) is as follows:

\[\bigvee_{\sum_{i=1}^{n}n\leq ik_{i}\leq 2n}\bigg{(}\bigwedge_{i=1}^{n-1}\exists^{2 \leq k_{i}}x\Big{(}\exists^{[i]}y\varphi(x,y)\Big{)}\bigg{)}\wedge\bigg{(} \exists^{2\geq k_{n}}x\Big{(}\exists^{2\geq n}y\varphi(x,y)\Big{)}\bigg{)} \tag{33}\]

This \(\mathcal{FOC}_{2}\) formula is equivalent to our desired classifier \(c\). 

With the Lemma 28, we now start to prove Theorem 11.

Proof.: By Theorem 5, it follows that R\({}^{2}\)-GNNs \(\circ F\subseteq\mathcal{FOC}_{2}\circ F\). Therefore it suffices to show \(\mathcal{FOC}_{2}\circ F\subseteq\mathcal{FOC}_{2}\).

By Lemma 22, it suffices to show \(\mathcal{RSFOC}_{2}\circ F\subseteq\mathcal{FOC}_{2}\). Since \(\wedge,\vee,\neg\) are common rules. We only need to show for any \(\mathcal{RSFOC}_{2}\) formula of form \(\varphi(x):=\exists^{\geq n}y(\varphi_{S}(x,y)\wedge\varphi^{\prime}(y))\) under transformed predicate set \(P^{\prime}=P\cup\{aux1,aux2,primal\}\), there exists an \(\mathcal{FOC}_{2}\) formula \(\varphi^{1}\) such that for any graph \(G\) under \(P\) and its node \(v\), \((G,v)\models\varphi^{1}\Leftrightarrow(F(G),v)\models\varphi\).

In order to show this, we consider a stronger result:

For any such formula \(\varphi\), including the existence of valid \(\varphi^{1}\), we claim there also exists an \(\mathcal{FOC}_{2}\) formula \(\varphi^{2}\) with two free variables such that the following holds: for any graph \(G\) under \(P\) and its added node \(e_{ab}\) on \(F(G)\), \((G,a,b)\models\varphi^{2}\Leftrightarrow(F(G),e_{ab})\models\varphi\). Call \(\varphi^{1}\),\(\varphi^{2}\) as first/second discriminant of \(\varphi\).

Now we need to prove the existence of \(\varphi^{1}\) and \(\varphi^{2}\).

We prove by induction over quantifier depth \(m\) of \(\varphi\), Since we only add a single unary predicate \(primal\) in \(P^{\prime}\), any \(\varphi(x)\) with quantifier depth \(0\) can be rewritten as \((primal(x)\wedge\varphi^{1}(x))\vee(\neg primal(x)\wedge\varphi^{2}(x))\), where \(\varphi^{1}(x)\),\(\varphi^{2}(x)\) are two formulas that only contain predicates in \(P\). Therefore, \(\varphi^{1}\) can be naturally seen as the first discriminant of \(\varphi\). Moreover, since \(\varphi^{2}(x)\) always evaluates on non-primal nodes, it is equivalent to \(\bot\) or \(\top\) under \(\neg primal(x)\) constraint. Therefore, the corresponding \(\bot\) or \(\top\) can be seen as the second discriminant, so our theorem trivially holds for \(m=0\). Now assume it holds for \(m\leq k\), we can assume quantifier depth of \(\varphi=\exists^{\geq n}y(\varphi_{S}(x,y)\wedge\varphi^{\prime}(y))\) is \(m=k+1\).

Consider the construction rules of transformation \(F\), for any two primal nodes in \(F(G)\), there is no relation between them, for a primal node \(a\) and an added node \(e_{ab}\), there is exactly a single relation of type \(aux1\) between them. For a pair of added nodes \(e_{ab}\),\(e_{ba}\), there are a bunch of relations from the original graph \(G\) and an additional \(aux2\) relation between them. Therefore, it suffices to only consider three possible kinds of \(S\subseteq P_{2}\cup\{aux1,aux2\}\) according to three cases mentioned above. Then, we will construct first/second determinants for each of these three cases. Since \(\varphi^{\prime}(y)\) has quantifier depth no more than \(k\), by induction let \(\widehat{\varphi}^{1}\),\(\widehat{\varphi}^{2}\) be first/second discriminants of \(\varphi^{\prime}\) by induction.

1. \(S=\{\textbf{aux1}\}\):

for primal node \(a\), \(\varphi(a)\) means the following: there exists at least \(n\) nodes \(b\), such that there is some relation between \(a\),\(b\) on \(G\) and the added node \(e_{ab}\) on \(F(G)\) satisfies \(\varphi^{\prime}\). Therefore, the first determinant of \(\varphi\) can be defined as following:

\[\varphi^{1}(x):=\exists^{\geq n}y,\Big{(}\bigvee_{r\in P_{2}}r(x,y)\Big{)} \wedge\widehat{\varphi}^{2}(x,y) \tag{34}\]

for added nodes \(e_{ab}\) on \(F(G)\), \(\varphi(e_{ab})\) means \(a\) satisfies \(\varphi^{\prime}\), so the second determinant of \(\varphi\) is the following:

\[n=1:\varphi^{2}(x,y):=\widehat{\varphi}^{1}(x),\ n>1:\varphi^{2}(x,y):=\bot \tag{35}\]

\(2.S=\{\textbf{aux2}\}\cup T,T\subseteq P_{2},T\neq\emptyset\)

primal nodes don't have _aux2_ neighbors, so first determinant is trivially _false_.

\[\varphi^{1}(x):=\bot \tag{36}\]

For added node \(e_{ab}\), \(e_{ab}\) satisfies \(\varphi\) iff there are exactly relations between \(a\),\(b\) of types in \(T\), and \(e_{ba}\) satisfies \(\varphi^{\prime}\). Therefore the second determinant is as follows, where \(\varphi_{T}(x,y)\) is the _relation-specification_ formula under \(P\) introduced in Definition 21

\[n=1:\varphi^{2}(x,y):=\varphi_{T}(x,y)\wedge\widehat{\varphi}^{2}(y,x),\,n>1: \varphi^{2}(x,y):=\bot \tag{37}\]

3. \(S=\emptyset\)

For a subset \(S\subseteq P_{2}\cup\{aux1,aux2\}\), let \(\varphi_{S}(x,y)\) denote the _relation-specification_ formula under \(P_{2}\cup\{aux1,aux2\}\) defined in Definition 21.

Since we consider on bounded graph class \(\mathcal{G}_{b}\), node number is bounded by a natural number \(N\). For any node \(a\) on \(F(G)\), let \(m\) denote the number of nodes \(b\) on \(F(G)\) such that \(\varphi^{\prime}(b)=1\), let \(m_{0}\) denote the number of nodes \(b\) on \(F(G)\) such that \(\varphi^{\prime}(b)=1\) and there is a single relation \(aux1\), between \((a,b)\) on \(F(G)\), (That is equivalent to \(\varphi_{\{aux1\}}(a,b)=1\)). For any \(T\subseteq P_{2}\), let \(m_{T}\) denote the number of nodes \(b\) on \(F(G)\) such that \(\varphi^{\prime}(b)=1\) and \(a\),\(b\) has exactly relations of types in \(T\cup\{aux2\}\) on \(F(G)\), (That is equivalent to \(\varphi_{T\cup\{aux2\}}(a,b)=1\)).

Note that the number of nodes \(b\) on \(F(G)\) such that \(a\),\(b\) don't have any relation, (That is equivalent to \(\varphi_{\emptyset}(a\),\(b)=1\)) and \(\varphi^{\prime}(b)=1\) equals to \(m-m_{0}-\sum_{T\subseteq P_{2}}m_{T}\). Therefore, for any transformed graph \(F(G)\) and its node \(v\), (\(F(G)\),\(v\models\varphi\Leftrightarrow m-m_{0}-\sum_{T\subseteq P_{2}}m_{T}\geq n\). Since \(|V(G)|\leq N\) for all \(G\) in bounded graph class \(\mathcal{G}_{b}\), transformed graph \(F(G)\) has node number no more than \(N^{2}\). Therefore, we can enumerate all possibilities of \(m\),\(m_{0}\),\(m_{T}\leq N^{2}\),\(T\subset P_{2}\) such that the above inequality holds, and for each possibility, we judge whether there exists exactly such number of nodes for each corresponding parameter. Formally speaking, \(\varphi\) can be rewritten as the following form:

\[\tilde{\varphi}_{m,m_{0}}(x):=\big{(}\exists^{[m]}y\varphi^{\prime}(y)\big{)} \wedge(\exists^{[m_{0}]}y(\varphi_{\{aux1\}}(x\),\(y\))\(\wedge\)\(\varphi^{\prime}(y)))\big{)} \tag{38}\]

\[\varphi(x)\equiv\bigvee_{m-m_{0}-\sum_{T\subseteq P_{2}}m_{T}\geq n,0\leq m,m_ {0},m_{T}\leq N^{2}}\bigg{(}\tilde{\varphi}_{m,m_{0}}(x)\wedge\big{(}\bigwedge_ {T\subseteq P_{2}}\exists^{[m_{T}]}y\),\((\varphi_{T\cup\{aux2\}}(x\),\(y\))\(\wedge\)\(\varphi^{\prime}(y))\big{)}\bigg{)}\]

where \(\exists^{[m]}y\) denotes there are exactly \(m\) nodes \(y\).

Since first/second determinant can be constructed trivially under combination of \(\wedge\), \(\vee\),\(\neg\), and we've shown how to construct determinants for formulas of form \(\exists^{\geq n}y(\varphi_{S}(x,y)\wedge\)\(\varphi^{\prime}(y))\) when \(S=\{aux1\}\) and \(S=\{aux2\}\cup T\),\(T\subseteq P_{2}\) in the previous two cases. Therefore, in Equation (38) and Equation (39), the only left part is the formula of form \(\exists^{[m]}y\varphi^{\prime}(y)\). The only remaining work is to show how to construct first/second determinants for formula in form \(\varphi(x):=\exists^{\geq n}y\varphi^{\prime}(y)\).

Let \(m_{1}\) denote the number of primal nodes \(y\) that satisfies \(\varphi^{\prime}(y)\) and let \(m_{2}\) denote the number of non-primal nodes \(y\) that satisfies \(\varphi^{\prime}(y)\). It's not hard to see that for any node \(v\) on \(F(G)\), (\(F(G)\),\(v\)) \(\models\varphi\Leftrightarrow m_{1}+m_{2}\geq n\). Therefore, \(\varphi(x)=\exists^{\geq n}y\varphi^{\prime}(y)\) that evaluates on \(F(G)\) is equivalent to the following sentence that evaluates on \(G\): _"There exists two natural numbers \(m_{1}\),\(m_{2}\) such that the following conditions hold: **1.**\(m_{1}+m_{2}=n\). **2.** There are at least \(m_{1}\) nodes \(b\) on \(G\) that satisfies \(\widehat{\varphi}^{1}\), (equivalent to \((F(G)\),\(b)\models\varphi^{\prime}\)). **3.** There are at least \(m_{2}\) ordered node pairs \(a\),\(b\) on \(G\) such that \(a\),\(b\) has some relation and \((G\),\(a\),\(b)\models\widehat{\varphi}^{2}\), (equivalent to \((F(G)\),\(e_{ab})\models\varphi^{\prime}\))."_

Formally speaking, rewrite the sentence above as formula under \(P\), we get the following construction for first/second determinants of \(\varphi\).

\[\varphi^{1}(x)=\varphi^{2}(x\text{,}y)=\bigvee_{m_{1}+m_{2}=n}\Big{(}(\exists^{ \geq m_{1}}y\text{,}\widehat{\varphi}^{1}(y))\wedge\overline{\varphi}_{m_{2}} \Big{)} \tag{40}\]

where \(\overline{\varphi}_{m_{2}}\) is the \(\mathcal{FOC}_{2}\) formula that expresses _"There exists at least \(m_{2}\) ordered node pairs \((a\),\(b)\) such that \((G\),\(a\),\(b)\models\widehat{\varphi}^{2}\)_(\(x\),\(y\))\(\wedge\)\((\bigvee_{r\in P_{2}}r(x\),\(y))\)". We've shown the existence of \(\overline{\varphi_{m_{2}}}\) in Lemma 28

## Appendix H Proof of Theorem 15

**Theorem 15**.: _time-and-graph \(\subsetneq\) R\({}^{2}\)-TGNN \(\circ F^{T}=\) time-then-graph._

For a graph \(G\) with \(n\) nodes, let \(\mathbb{H}^{V}\in\mathbb{R}^{n\times d_{v}}\) denote node feature matrix, and \(\mathbb{H}^{E}\in\mathbb{R}^{n\times n\times d_{v}}\) denote edge feature matrix, where \(\mathbb{H}^{E}_{ij}\) denote the edge feature vector from \(i\) to \(j\).

First we need to define the GNN used in their frameworks. Note that for the comparison fairness, we add the the global readout to the node feature update as we do in R\({}^{2}\)-GNNs. It recursively calculates the feature vector \(\mathbb{H}^{V,(l)}_{i}\) of the node i at each layer \(1\leq l\leq L\) as follows:

\[\mathbb{H}^{V,(l)}_{i}=u^{(l)}\Big{(}g^{(l)}\big{(}\{\!\{\mathbb{H}^{V,(l-1)} _{i},\!\mathbb{H}^{V,(l-1)}_{j},\!\mathbb{H}^{E}_{ij}\}\mid j\in\mathcal{N}(i )\!\}\big{)},r^{(l)}\big{(}\{\!\{\mathbb{H}^{V,(l-1)}_{j}|j\in V\!\}\!\}\big{)} \Big{)} \tag{41}\]

where \(\mathcal{N}(i)\) denotes the set of all nodes that adjacent to \(i\), and \(u^{(l)}\),\(g^{(l)}\),\(r^{(l)}\) are learnable functions. Note that here the GNN framework is a little different from the general definition defined in Equation (2). However, this framework is hard to fully implement and many previous works implementing _time-and-graph_ or _time-then-graph_Gao and Ribeiro (2022)Li et al. (2019); Seo et al. (2016); Chen et al. (2018); Manessi et al. (2020); Sankar et al. (2018); Rossi et al. (2020)) don't reach the expressiveness of Equation (41). This definition is more for the theoretical analysis. In contrast, our definition for GNN in Equation (1) and Equation (2) is more practical since it is fully captured by a bunch of commonly used models such as Schlichtkrull et al. (2018). For notation simplicity, for a GNN \(\mathcal{A}\), let \(\mathbb{H}^{V,(L)}=\mathcal{A}(\mathbb{H}^{V},\mathbb{H}^{E})\) denote the node feature outputted by \(\mathcal{A}\) using \(\mathbb{H}^{V},\mathbb{H}^{E}\) as initial features.

**Proposition 29**.: _(Gao and Ribeiro (2022)):time-and-graph \(\subsetneq\) time-then-graph_

The above proposition is from **Theorem 1** of Gao and Ribeiro (2022). Therefore, in order to complete the proof of Theorem 15, we only need to prove R\({}^{2}\)-TGNN \(\circ F^{T}=\)_time-then-graph_.

Let \(G=\{G_{1},\ldots,G_{T}\}\) denote a temporal knowledge graph, and \(\mathbb{A}^{t}\in\mathbb{R}^{n\times|P_{1}|},\mathbb{E}^{t}\in\mathbb{R}^{n \times n\times|P_{2}|},1\leq t\leq T\) denote one-hot encoding feature of unary facts and binary facts on timestamp \(t\), where \(P_{1},P_{2}\) are unary and binary predicate sets.

The updating rule of a _time-then-graph_ model can be generalized as follows:

\[\forall i\in V,\;\mathbb{H}^{V}_{i} =\textbf{RNN}([\mathbb{A}^{1}_{i}......\mathbb{A}^{T}_{i}]) \tag{42}\] \[\forall i,j\in V,\;\mathbb{H}^{E}_{i,j} =\textbf{RNN}([\mathbb{E}^{1}_{i,j}.....\mathbb{E}^{T}_{i,j}])\] (43) \[\textbf{X} :=\mathcal{A}(\mathbb{H}^{V},\mathbb{H}^{E}) \tag{44}\]

where \(\mathcal{A}\) is a GNN defined above, **RNN** is an arbitrary Recurrent Neural Network. \(\textbf{X}\in\mathbb{R}^{n\times d}\) is the final node feature output of _time-then-graph_.

First we need to prove _time-then-graph_\(\subseteq\) R\({}^{2}\)-TGNN\(\circ F^{T}\). That is, for any _time-then-graph_ model, we want to construct an equivalent R\({}^{2}\)-TGNN \(\mathcal{A}^{\prime}\) to capture it on transformed graph. We can use nodes added after transformation to store the edge feature \(\mathbb{H}^{E}\), and use primal nodes to store the node feature \(\mathbb{H}^{V}\). By simulating **RNN** through choosing specific functions in R\({}^{2}\)-TGNN, we can easily construct a R\({}^{2}\)-TGNN \(\mathcal{A}^{\prime}\) such that for any node \(i\), and any node pair \(i,j\) with at least one edge in history, \(\textbf{x}_{i}=\mathbb{H}^{V}_{i}\) and \(\textbf{x}_{e_{ij}}=\mathbb{H}^{E}_{i,j}\) hold, where \(\textbf{x}_{i}\) and \(\textbf{x}_{e_{ij}}\) are features of corresponding primal node \(i\) and added node \(e_{ij}\) outputted by \(\mathcal{A}^{\prime}\).

Note that \(\mathcal{A}^{\prime}\) is a R\({}^{2}\)-TGNN, it can be represented as \(\mathcal{A}^{\prime}_{1}......\mathcal{A}^{\prime}_{T}\), where each \(\mathcal{A}^{\prime}_{t},1\leq t\leq T\) is a R\({}^{2}\)-GNN. \(\mathcal{A}^{\prime}\) has simulated work of **RNN**, so the remaining work is to simulate \(\mathcal{A}(\mathbb{H}^{V},\mathbb{H}^{E})\). We do the simulation over induction on layer number \(L\) of \(\mathcal{A}\).

When \(L=0\), output of \(\mathcal{A}\) is exactly \(\mathbb{H}^{V}\), which has been simulated by \(\mathcal{A}^{\prime}\) above.

Suppose \(L=k+1\), let \(\tilde{\mathcal{A}}\) denote R\({}^{2}\)-GNN extracted from \(\mathcal{A}\) but without the last layer \(k+1\). By induction, we can construct a R\({}^{2}\)-TGNN \(\tilde{\mathcal{A}}^{\prime}\) that simulates \(\tilde{\mathcal{A}}(\mathbb{H}^{V},\mathbb{H}^{E})\). Then we need to append three layers to \(\tilde{\mathcal{A}}^{\prime}\) to simulate the last layer of \(\mathcal{A}\).

Let \(u^{(L)},g^{(L)},r^{(L)}\) denote parameters of the last layer of \(\mathcal{A}\). Using notations in Equation (2), let \(\{C^{(l)},(A^{(l)}_{j})_{j=1}^{|P_{2}|},A^{(l)}_{aux1},A^{(l)}_{aux2},R^{(l)}\} _{l=1}^{3}\) denote parameters of the three layers appended to \(\tilde{\mathcal{A}}^{\prime}_{T}\). They are defined as follows:

First, we can choose specific function in the first two added layers, such that the following holds:

**1.** For any added node \(e_{ij}\), feature outputted by the new model is \(\textbf{x}^{(2)}_{e_{ij}}=[\mathbb{H}^{E}_{ij},\textbf{x}^{\prime}_{i},\textbf {x}^{\prime}_{j}]\), where \(\textbf{x}^{(2)}\) denotes the feature outputted by the second added layer, and \(\textbf{x}^{\prime}_{i},\textbf{x}^{\prime}_{j}\) are node features of \(i,j\) outputted by \(\tilde{\mathcal{A}}^{\prime}\). For a feature **x** of added node of this form, we define \(\textbf{x}_{0},\textbf{x}_{1},\textbf{x}_{2}\) as corresponding feature slices where \(\mathbb{H}^{E}_{ij},\textbf{x}^{\prime}_{i},\textbf{x}^{\prime}_{j}\) have been stored.

**2.** For any primal node, its feature **x** only stores \(\textbf{x}^{\prime}_{i}\) in \(\textbf{x}_{1}\), and \(\textbf{x}_{0},\textbf{x}_{2}\) are all slices of dummy bits.

Let **X** be a multiset of features that represents function input. For the last added layer, we can choose specific functions as follows:

\[R^{(3)}(\textbf{X}):=r^{(L)}(\{\hskip-2.845276pt\{\textbf{x}_{1}|\textbf{x} \in\textbf{X},\textbf{primal}(\textbf{x})\}\hskip-2.845276pt\}) \tag{45}\] \[A^{(3)}_{aux1}(\textbf{X}):=g^{(L)}(\{\hskip-2.845276pt\{\textbf{f} (\textbf{x}_{1},\textbf{x}_{2},\textbf{x}_{0})|\textbf{x}\in\textbf{X}\} \hskip-2.845276pt\})\] (46) \[C^{(3)}(\textbf{x}_{aux1},\textbf{x}_{g}):=u^{(L)}(\textbf{x}_{ aux1},\textbf{x}_{g}) \tag{47}\]where \(\mathbf{x}_{aux1}\),\(\mathbf{x}_{g}\) are outputs of \(R^{(3)}\) and \(A^{(3)}_{aux1}\), and all useless inputs of \(C^{(3)}\) are omitted. Comparing this construction with Equation (41). It's east to see that after the last layer appended, we can construct an equivalent R\({}^{2}\)-TGNN \(\mathcal{A}^{\prime}\) that captures \(\mathcal{A}\) on transformed graph. By inductive argument, we prove _time-then-graph_\(\subseteq\)R \({}^{2}\)-TGNN \(\circ F^{T}\).

Then we need to show R\({}^{2}\)-TGNN \(\circ F^{T}\subseteq\)_time-then-graph_.

In Theorem 16, we will prove R\({}^{2}\)-TGNN \(\circ F^{T}=\)R\({}^{2}\)-GNN \(\circ F\circ H\). Its proof doesn't dependent on Theorem 15, so let's assume it's true for now. Then, instead of proving R\({}^{2}\)-TGNN \(\circ F^{T}\), it's sufficient to show R\({}^{2}\)-GNN \(\circ F\circ H\subseteq\)_time-then-graph_.

Let \(P_{1}^{T},P_{2}^{T}\) denote the set of temporalized unary and binary predicate sets defined in Definition 12. Based on _most expressive ability_ of Recurrent Neural Networks shown in Siegelmann and Sontag [1992], we can get a _most expressive representation_ for unary and binary fact sequences through **RNN**. A _most expressive_ RNN representation function is always injective, thus there exists a decoder function translating most-expressive representations back to raw sequences. Therefore, we are able to find an appropriate **RNN** such that its output features \(\mathbb{H}^{V}\),\(\mathbb{H}^{E}\) in Equation (42), Equation (43) contain all information needed to reconstruct all temporalized unary and binary facts related to the corresponding nodes.

For any R\({}^{2}\)-GNN \(\mathcal{A}\) on transformed collpsed temporal knowledge graph, we want to construct an equivalent _time-then-graph_ model \(\{\textbf{RNN},\mathcal{A}^{\prime}\}\) to capture \(\mathcal{A}\). In order to show the existence of the _time-then-graph_ model, we will do an inductive construction over layer number \(L\) of \(\mathcal{A}\). Here in order to build inductive argument, we will consider a following stronger result and aim to prove it: In additional to the existence of \(\mathcal{A}^{\prime}\), we claim there also exists a function \(f_{\mathcal{A}}\) with the following property: For any two nodes \(a\),\(b\) with at least one edge, \(f_{\mathcal{A}}(\mathbf{x}_{a}^{V,(l)},\mathbb{H}_{ab}^{E})=\mathbf{x}_{e_{ab}}\), where \(\mathbf{x}_{a}^{V},\mathbf{x}_{b}^{V},\mathbb{H}_{ab}^{E}\) are features of \(a\), \(b\) and edge information between \(a\),\(b\) outputted by \(\mathcal{A}^{\prime}\), and \(\mathbf{x}_{e_{ab}}\) is the feature of added node \(e_{ab}\) outputted by \(\mathcal{A}\circ F\circ H\). It suffices to show that there exists such function \(f_{\mathcal{A}}\) as well as a _time-then-graph_ model \(\{\textbf{RNN},\mathcal{A}^{\prime}\}\) such that the following conditions hold:

For any graph \(G\) and its node \(a\),\(b\in V(G)\),

1. \(\mathbb{H}_{a}^{V,(l)}=[\mathbf{x}_{a},Enc(\{\!\!\{\mathbf{x}_{e_{aj}}|j\in \mathcal{N}(a)\}\!\!\})]\).

2.If there is at least one edge between \(a\),\(b\) in history, \(f_{\mathcal{A}}(\mathbb{H}_{a}^{V,(l)},\mathbb{H}_{b}^{V,(l)},\mathbb{H}_{ab}^{ E})=\mathbf{x}_{e_{ab}}\). Otherwise, \(f_{\mathcal{A}}(\mathbb{H}_{a}^{V,(l)},\mathbb{H}_{b}^{V,(l)},\mathbb{H}_{ab}^ {E})=\mathbf{0}\)

where \(\mathbb{H}_{a}^{V,(l)},\mathbb{H}_{b}^{V,(l)}\) are node features outputted by \(\mathcal{A}^{\prime}\), while \(\mathbf{x}_{a}\),\(\mathbf{x}_{e_{ab}}\) are node features outputted by \(\mathcal{A}\) on transformed collpsed graph. \(Enc(\mathbf{X})\) is some injective encoding that stores all information of multiset \(\mathbf{X}\). For a node feature \(\mathbb{H}_{a}^{V,(l)}\) of above form, let \(\mathbb{H}_{a,0}^{V,(l)}:=\mathbf{x}_{a},\mathbb{H}_{a,1}^{V,(l)}=Enc(\{\!\!\{ \mathbf{x}_{e_{aj}}|j\in\mathcal{N}(a)\}\!\!\})\) denote two slices that store independent information in different positions.

For the base case \(L=0\). the node feature only depends on temporalized unary facts related to the corresponding node. Since by **RNN** we can use _most expressiveness representation_ to capture all unary facts. A specific **RNN** already captures \(\mathcal{A}\) when \(L=0\). Moreover, there is no added node \(e_{ab}\) that relates to any unary fact, so a constant function already satisfies the condition of \(f_{\mathcal{A}}\) when \(L=0\). Therefore, our result holds for \(L=0\)

Assume \(L=k+1\), let \(\widehat{\mathcal{A}}\) denote the model generated by the first \(k\) layers of \(\mathcal{A}\). By induction, there is _time-then-graph_ model \(\widehat{\mathcal{A}}^{\prime}\) and function \(f_{\widehat{\mathcal{A}}^{\prime}}\) that captures output of \(\widehat{\mathcal{A}}^{\prime}\) on transformed collapsed graph. We can append a layer to \(\widehat{\mathcal{A}}^{\prime}\) to build \(\mathcal{A}^{\prime}\) that simulates \(\mathcal{A}\). Let \(\{C^{(L)},(A^{(L)}_{j})_{j=1}^{T|P_{2}|},A^{(L)}_{aux1},A^{(L)}_{aux2},R^{(L)}\}\) denote the building blocks of layer \(L\) of \(\mathcal{A}\), and let \(u^{*}\),\(g^{*}\),\(r^{*}\) denote functions used in the layer that will be appended to \(\widehat{\mathcal{A}}^{\prime}\). They are defined below:

\[g^{*}(\{\!\!\{\mathbb{H}_{i}^{V,(l-1)},\!\!\mathbb{H}_{j}^{V,(l-1)},\!\! \mathbb{H}_{i}^{V,(l-1)},\!\!\mathbb{H}_{i}^{E}|j\in\mathcal{N}(i)\}\!\!\}):=A^{ (L)}_{aux1}(\{\!\!\{f_{\widehat{\mathcal{A}}^{\prime}}(\mathbb{H}_{i}^{V,(l-1)},\!\!\mathbb{H}_{j}^{V,(l-1)},\!\!\mathbb{H}_{ij}^{E})|j\in\mathcal{N}(i)\}\!\!\}) \tag{48}\]

\[r^{*}(\{\!\!\{\mathbb{H}_{j}^{V,(l-1)}|j\in V(G)\}\!\!\})=R^{(L)}\Big{(}\{\!\! \{\mathbb{H}_{j,0}^{V,(l-1)}|j\in V(G)\}\!\!\}\cup(\bigcup_{j\in V(G)}Dec( \mathbb{H}_{j,1}^{V,(l-1)}))\Big{)} \tag{49}\]

\[u^{*}(\mathbf{x}_{g},\!\mathbf{x}_{r})=C^{(L)}(\mathbf{x}_{g},\!\mathbf{x}_{r}) \tag{50}\]

where \(\mathbf{x}_{g}\),\(\mathbf{x}_{r}\) are outputs of \(g^{*}\) and \(r^{*}\). \(Dec(\mathbf{X})\) is a decoder function that do inverse mapping of \(Enc(\mathbf{X})\) mentioned above, so \(Dec(\mathbb{H}_{j,1}^{V,(l-1)})\) is actually \(\{\!\!\{\mathbf{x}_{e_{aj}}|j\in\mathcal{N}(a)\}\!\!\}\). Note that primal nodesin transformed graph only has type _aux1_- neighbors, so two inputs \(\mathbf{x}_{g}\),\(\mathbf{x}_{r}\), one for _aux1_ aggregation output and one for global readout are already enough for computing the value. Comparing the three rules above with Equation (2), we can see that our new model \(\mathcal{A}^{\prime}\) perfectly captures \(\mathcal{A}\).

We've captured \(\mathcal{A}\), and the remaining work is to construct \(f_{\mathcal{A}}\) defined above to complete inductive assumption. We can just choose a function that simulates message passing between pairs of added nodes \(e_{ab}\) and \(e_{ba}\) as well as message passing between \(e_{ab}\) and \(a\), and that function satisfies the condition for \(f_{\mathcal{A}}\). Formally speaking, \(f_{\mathcal{A}}\) can be defined below:

\[f_{\mathcal{A}}(\mathbb{H}_{i}^{V,(l)},\mathbb{H}_{j}^{V,(l)}, \mathbb{H}_{ij}^{E}):=\mathbf{Sim}_{\mathcal{A}_{L}}(\mathbb{H}_{i}^{V,(l-1)},\mathbb{H}_{g}^{(l-1)},g_{ij},g_{ji},\mathbb{H}_{ij}^{E}) \tag{51}\] \[g_{ij}:=f_{\widehat{\mathcal{A}}}(\mathbb{H}_{i}^{V,(l-1)}, \mathbb{H}_{j}^{V,(l-1)},\mathbb{H}_{ij}^{E}),\mathbb{H}_{g}^{(l-1)}:=\{ \mathbb{H}_{i}^{V,(l-1)}|i\in V(G)\} \tag{52}\]

Let's explain this equation, \(\mathbf{Sim}_{\mathcal{A}_{L}}(a,g,s,b,e)\) is a local simulation function which simulates single-iteration message passing in the following scenario:

Suppose there is a graph \(H\) with three constants \(V(H)=\{a,\)\(e_{ab},\)\(e_{ba}\}\). There is an _aux1_ edge between \(a\) and \(e_{ab}\), an _aux2_ edge between \(e_{ab}\) and \(e_{ba}\), and additional edges of different types between \(e_{ab}\) and \(e_{ba}\). The description of additional edges can be founded in \(e\). Initial node features of \(a,\)\(e_{ab},\)\(e_{ba}\) are set to \(a,\)\(s,\)\(b\) respectively. and the global readout output is \(g\). Finally, run \(L\)-th layer of \(\mathcal{A}\) on \(H\), and \(\mathbf{Sim}_{\mathcal{A}_{L}}\) is node feature of \(e_{ab}\) outputted by \(\mathcal{A}_{L}\).

Note that if we use appropriate injective encoding or just use concatenation technic, \(\mathbb{H}_{g}^{(l-1)},\mathbb{H}_{i}^{V,(l-1)},\mathbb{H}_{j}^{V,(l-1)}\) can be accessed from \(\mathbb{H}_{i}^{V,(l)},\mathbb{H}_{i}^{V,(l)}\). Therefore the above definition for \(f_{\mathcal{A}}\) is well-defined. Moreover, in the above explanation we can see that \(f_{\mathcal{A}}(\mathbb{H}_{i}^{V,(l-1)},\mathbb{H}_{j}^{V,(l-1)},\mathbb{H}_{ ij}^{E})\) is exactly node feature of \(e_{ij}\) outputted by \(\mathcal{A}\) on the transformed collapsed graph, so our proof finishes.

## Appendix I Proof of Theorem 16

**Theorem 16**.: \(R^{2}\)_-Tgnn \(\circ F^{T}=R^{2}\)_-Tgnn \(\circ F\circ H\)._

First, we recall the definition for R\({}^{2}\)-Tgnn as in Equation (53):

\[\mathbf{x}_{v}^{t}=\mathcal{A}_{t}\bigg{(}G_{t},\)\(v,\)\(\mathbf{y}^{t}\bigg{)}\quad\text{ where}\quad\mathbf{y}_{v}^{t}=[I_{G_{t}}(v):\mathbf{x}_{v}^{t-1}],\forall v\in V (G_{t}) \tag{53}\]

We say a R\({}^{2}\)-Tgnn is _homogeneous_ if \(\mathcal{A}_{1},\ldots,\mathcal{A}_{T}\) share the same parameters. In particular, we first prove Lemma 30, namely, _homogeneous_ R\({}^{2}\)-Tgnn and R\({}^{2}\)-Tgnn (where paramters in \(\mathcal{A}_{1},\ldots,\mathcal{A}_{T}\) may differ) have the same expressiveness.

**Lemma 30**.: _homogenous R\({}^{2}\)-Tgnn \(=R^{2}\)-Tgnn_

Proof.: The forward direction _homogeneous_ R\({}^{2}\)-Tgnn\(\subseteq\) R\({}^{2}\)-Tgnn trivially holds. It suffices to prove the backward direction.

Let \(\mathcal{A}:\{\mathcal{A}_{t}\}_{t=1}^{T}\) denote a R\({}^{2}\)-Tgnn. Without loss of generality, we can assume all models in each timestamps have the same layer number \(L\). Then for each \(1\leq t\leq T\), we can assume all \(\mathcal{A}_{t}\) can be represented by \(\{C_{t}^{(1)},(A_{t,j}^{(l)})_{j=1}^{|P_{2}|},R_{t}^{(l)}\}_{l=1}^{L}\). Futhorrome, without loss of generality, we can assume all output dimensions for \(A_{t,j}^{(l)},R_{t}^{(l)}\) and \(C_{t}^{(l)}\) are \(d\). As for input dimension, all of these functions also have input dimension \(d\) for \(2\leq l\leq L\). Specially, by updating rules of R\({}^{2}\)-Tgnn Equation (53), in the initialization stage of each timestamp we have to concat a feature with length \(|P_{1}|\) to output of the former timestamp, so the input dimension for \(A_{t,j}^{(1)},R_{t}^{(1)},\)\(C_{t}^{(1)}\) is \(d+|P_{1}|\).

We can construct an equivalent _homogeneous_ R\({}^{2}\)-Tgnn with \(L\) layers represented by \(\{C^{*,(l)},(A_{j}^{*,(l)})_{j=1}^{|P_{2}|},R^{*,(l)}\}_{l=1}^{L}\). For \(2\leq l\leq L\), \(C^{*,(l)}A_{j}^{*,(l)},R^{*,(l)}\) use output and input feature dimension \(d^{\prime}=Td\). Similar to the discussion about feature dimension above, since we need to concat the unary predicates information before each timestamp, for layer \(l=1\), \(C^{*,(1)},A_{j}^{*,(1)},R^{*,(1)}\) have input dimension \(d^{\prime}+|P_{1}|\) and output dimension \(d^{\prime}\). For dimension alignment, \(\mathbf{x}_{v}^{0}\) used in Equation (53) is defined as zero-vector with length \(d^{\prime}\).

[MISSING_PAGE_FAIL:32]

\(e_{ab}\) as destination, it can be divided into messages from \(a\) and messages from \(e_{ba}\). The first class of messages is easy to simulate since the \(aux1\) edge between \(e_{ab}\) and \(a\) is the same on \(F^{T}(G)_{T}\) and \(F(H(G))\).

For the second class of messages, since edges of type \(r_{i}\),\(1\leq i\leq T|P_{2}|\) may be lost in \(F^{T}(G)_{T}\), we have to simulate these messages only by the unchanged edge of type **aux2**. It can be realized by following construction:

\[1\leq l\leq L,A^{*,(l)}_{aux2}(\textbf{X})=[[A^{\prime,(l)}_{j}(\textbf{X})]] ^{K}_{j=1},A^{(l)}_{aux2}(\textbf{X})] \tag{60}\]

where \(K=T|P_{2}|,A^{\prime,(l)}_{j}(\textbf{X}):=A^{(l)}_{j}(\textbf{X})\) if and only if \(e_{ba}\) has neighbor \(r_{j}\) on \(F(H(G))\), otherwise \(A^{\prime(l)}_{j}(\textbf{X}):=\textbf{0}\). Note that **X** is exactly the feature of \(e_{ba}\), and we can access the information about its \(r_{j}\) neighbors from feature since \(\mathcal{A}^{\prime}\) has stored information about these facts.

In conclusion, we've simulated all messages between neighbors. Furthermore, since node sets on \(F^{T}(G)_{T}\) and \(F(H(G))\) are the same, global readout \(R^{(l)}\) is also easy to simulate by \(R^{*,(l)}\). Finally, using the original combination function \(C^{(l)}\), we can construct an R\({}^{2}\)-TGNN on \(F^{T}\) equivalent to \(\mathcal{A}\) on \(F(H(G))\) for any temporal knowledge graph \(G\).

## Appendix J Proof of Theorem 17

Based on Theorem 15, Theorem 16 and Corollary 11.2, in order to prove Theorem 17, it suffices to show the following theorems.

**Theorem 31**.: _If time range \(T>1\)\(R^{2}\)-TGNN \(\subsetneq\)\(R^{2}\)-GNN \(\circ H\)._

**Theorem 32**.: _If time range \(T>1\)\(R^{2}\)-TGNN \(\nsubseteq\) time-and-graph._

Proof.: Since a formal proof Theorem 32 relates to too many details in definition of time-and-graph (Please refer to Gao and Ribeiro [2022]) which is not the focus here. We will just a brief proof sketch of Theorem 32: That's because time-and-graph can not capture a chain of information that is continuously scattered in time intervals. Specifically, \(\varphi(x):=\exists^{\geq 1}y\), \(\left(r_{1}^{2}(x,y)\wedge(\exists^{\geq 1}x,r_{1}^{1}(y,x))\right)\) can't be captured by time-and-graph but \(\varphi(x)\) is in R\({}^{2}\)-TGNN.

We mainly give a detaild proof of Theorem 31: Since in each timestamp \(t\), R\({}^{2}\)-TGNN only uses a part of predicates in temporalized predicate set \(P^{\prime}=H(P)\), R\({}^{2}\)-TGNN \(\subseteq\) R\({}^{2}\)-GNN \(\circ H\) trivially holds. To show R\({}^{2}\)-TGNN is strictly weaker than R\({}^{2}\)-GNN \(\circ H\). Consider the following classifier:

Let time range \(T=2\), and let \(r\) be a binary predicate in \(P_{2}\). Note that there are two different predicates \(r^{1}\),\(r^{2}\) in \(P^{\prime}=H(P)\). Consider the following temporal graph \(G\) with \(5\) nodes {1,2,3,4,5}. its two snapshots \(G_{1}\),\(G_{2}\) are as follows:

\(G_{1}=\{r(1,2)\),\(r(4,5)\}\)

\(G_{2}=\{r(2,3)\}\).

It follows that after transformation \(H\), the static version of \(G\) is:

\(H(G)=\{r_{1}(1,2)\),\(r_{1}(4,5)\),\(r_{2}(2,3)\}\).

Consider the logical classifier \(\exists y\Big{(}r_{1}(x,y)\wedge(\exists rr_{2}(x,y))\Big{)}\) under \(P^{\prime}\).It can be captured by some R\({}^{2}\)-GNN under \(P^{\prime}\). Therefore, R\({}^{2}\)-GNN \(\circ H\) can distinguish nodes \(1\),\(4\).

Figure 6: Hierarchic expressiveness.

However, any R\({}^{2}\)-TGNN based on updating rules in Equation (53) can't distinguish these two nodes, so R\({}^{2}\)-TGNN is strictly weaker than R\({}^{2}\)-GNN \(\circ H\). 

Based on Theorem 31, we can consider logical classifier \(\varphi_{\mathbf{3}}\coloneqq\exists^{\geq 2}y(p_{1}^{1}(x,y)\wedge p_{1}^{2}(x, y))\). Note that this classifier is just renaming version of Figure 1. Therefore \(\varphi_{3}\) can't be captured by R\({}^{2}\)-GNN \(\circ H\), not to say weaker framework R\({}^{2}\)-GNN by Theorem 31.

## Appendix K Experiment Supplementary

### Synthetic dataset generation

For each synthetic datasets, we generate 7000 graphs as tranining set and 500 graphs as test set. Each graph has \(50-1000\) nodes. In graph generation, we fix the expected edge density \(\delta\). In order to generate a graph with \(n\) nodes, we pick \(\delta n\) pairs of distinct nodes uniformly randomly. For each selected node pair \(a\),\(b\), each timestamp \(t\) and each binary relation type \(r\), we add \(r^{t}(a,b)\) and \(r^{t}(a,b)\) into the graph with independent probability \(\frac{1}{2}\).

### Statistical Information for Datasets

We list the information for synthetic dataset in Table 4 and real-world dataset in Table 5. Note that synthetic datasets contains many graphs, but real-world datasets only contains a single graph. Therefore, for real-world dataset, we have two disjoint node set as train split and test split for training and testing respectively. In training, the model can see the subgraph induced by train split and unlabelled nodes, in testing, the model can see the whole graph but only evaluate the performance on test split.

\begin{table}
\begin{tabular}{c|c} \hline hyper-parameter & range \\ \hline learning rate & 0.01 \\ \hline combination & mean/max/add \\ \hline aggregation/readout & mean/max/add \\ \hline layer & \(1\),\(2\),\(3\) \\ \hline hidden dimension & \(10\),\(64\),\(100\) \\ \hline \end{tabular}
\end{table}
Table 6: Hyper-parameters.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline datasets & CIFAR1 & CIFAR10 & CIFAR10 & CIFAR10 \\ \hline \# Nodes & 8285 & 23644 & 5000 \\ \hline Time\_Range & \(\backslash\) & \(\backslash\) & 12 \\ \hline \# Relation types & 45 & 23 & 20 \\ \hline \# Edges & 29043 & 74227 & 1761414 \\ \hline \# Classes & 4 & 2 & 10 \\ \hline \# Train Nodes & 140 & 272 & 4500 \\ \hline \# Test Nodes & 36 & 68 & 500 \\ \hline \end{tabular}
\end{table}
Table 4: statistical information for synthetic datasets.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline datasets & CIFAR10 & CIFAR10 & CIFAR10 & CIFAR10 \\ \hline \# Nodes & 477 & 477 & 477 & 477 \\ \hline Time\_range & 2 & 2 & 2 & 10 \\ \hline \# Unary predicate & 2 & 2 & 2 & 3 \\ \hline \# Binary predicate(non-temporalized) & 1 & 1 & 1 & 3 \\ \hline Avg \# Degree (in single timestamp) & 3 & 3 & 3 & 5 \\ \hline Avg \# positive percentage & 50.7 & 52 & 25.3 & 73.3 \\ \hline \end{tabular}
\end{table}
Table 5: statistical information for Real datasets.

### Hyper-parameters

For all experiments, we did grid search according to Table 6.

### More Results

Apart from those presented in main part, we have some extra experimental results here:

1. Extra results on synthetic datasets but using different base model architecture, where R-GAT refers to Busbridge et al. (2019) and R\({}^{2}\)-GAT refers to its extension with global readout. Please Refer to Table 7. These results show the generality of our results on different base models within the framework.

2. Extra results for static real-world datasets. Add a base model R-GATBusbridge et al. (2019) and two larger real-world datasets DGS and AM from Schlichtkrull et al. (2018). Please refer to Table 8. From the results for two bigger datasets DGM and AM, we can see our framework outperforms the other baselines, which confirms the scalability of our method and theoretical results. These results show our method is effective both on small and large graphs.

3. Extra results for temporal real-world dataset Brain-10. Please refer to Table 9. These results implies that our method is effective on different base models in temporal settings. Moreover, we can see separate improvements from global readout and graph transformation respectively. As we said in the main part, the drop in the last column may be due to the intrinsic drawbacks of current real-world datasets. Many real-world datasets can not be perfectly modeled as first-order-logic classifier. This non-logical property may lead to less convincing experimental results. As Barcelo et al. (2020) commented, these commonly used benchmarks are inadequate for testing advanced GNN variants.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \(\mathcal{FOC}_{2}\) classifier & \(\varphi_{1}\) & \(\varphi_{2}\) & \(\varphi_{3}\) & \(\varphi_{4}\) \\ \hline R-GAT \(\circ H\) & 100 & 61.4 & 88.6 & 82.0 \\ \hline R\({}^{2}\)-GAT \(\circ H\) & 100 & 93.5 & 95.0 & 82.2 \\ \hline R\({}^{2}\)-GAT \(\circ F\circ H\) & **100** & **98.2** & **100** & **95.8** \\ \hline \end{tabular}
\end{table}
Table 7: Extra results on synthetic datasets

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline  & AIFB & MUTAG & DGS & AM \\ \hline \# of nodes & 8285 & 23644 & 333845 & 1666764 \\ \hline \# of edges & 29043 & 74227 & 916199 & 5988321 \\ \hline R-GCN & 95.8 & 73.2 & 83.1 & 89.3 \\ \hline R-GAT & 96.9 & 74.4 & 86.9 & 90.0 \\ \hline R-GNN & 91.7 & 76.5 & 81.2 & 89.5 \\ \hline R\({}^{2}\)-GNN & 91.7 & 85.3 & 85.5 & 89.9 \\ \hline R\({}^{2}\)-GNN \(\circ F\) & **97.2** & **88.2** & **88.0** & **91.4** \\ \hline \end{tabular}
\end{table}
Table 8: Extra results for static real-world datasets.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline Models & GRU-GCN\(\circ F^{T}\) & TGN \(\circ F^{T}\) & R-TGNN & R-TGNN \(\circ F^{T}\) & R\({}^{2}\)-TGNN & R\({}^{2}\)-TGNN\(\circ F^{T}\) \\ \hline Brain-10 & 95.0 & 94.2 & 85.0 & 90.9 & 94.8 & 94.0 \\ \hline \end{tabular}
\end{table}
Table 9: Extra results for temporal real-world dataset Brain-10.