# Anonymous and Copy-Robust Delegations

for Liquid Democracy+
Footnote â€ : Part of this research was carried out while both authors were affiliated with TU Berlin and Universidad de Chile, and Markus Uke was affiliated with University of Amsterdam.

Markus Utke

Department of Mathematics and Computer Science

TU Eindhoven

Eindhoven, The Netherlands

m.utke@tue.nl

&Ulrike Schmidt-Kraepelin

Simons Laufer Mathematical Sciences

Institute (SLMath)

Berkeley, CA, United States

uschmidt@slash.org

###### Abstract

Liquid democracy with ranked delegations is a novel voting scheme that unites the practicability of representative democracy with the idealistic appeal of direct democracy: Every voter decides between _casting_ their vote on a question at hand or _delegating_ their voting weight to some other, trusted agent. Delegations are transitive, and since voters may end up in a delegation cycle, they are encouraged to indicate not only a single delegate, but a set of potential delegates and a ranking among them. Based on the delegation preferences of all voters, a _delegation rule_ selects one representative per voter. Previous work has revealed a trade-off between two properties of delegation rules called _anonymity_ and _copy-robustness_.

To overcome this issue we study two _fractional_ delegation rules: Mixed Borda Branching, which generalizes a rule satisfying copy-robustness, and the Random Walk Rule, which satisfies anonymity. Using the _Markov chain tree theorem_, we show that the two rules are in fact equivalent, and simultaneously satisfy generalized versions of the two properties. Combining the same theorem with _Fulkerson's algorithm_, we develop a polynomial-time algorithm for computing the outcome of the studied delegation rule. This algorithm is of independent interest, having applications in semi-supervised learning and graph theory.

## 1 Introduction

Today, democratic decision-making in legislative bodies, parties, and non-profit organizations is often done via one of two extremes: In _representative democracy_, the constituents elect representatives who are responsible for deciding upon all upcoming issues for a period of several years. In _direct democracy_, the voters may vote upon every issue themselves. While the latter is distinguished by its idealistic character, it may suffer from low voter turnout as voters do not feel sufficiently informed. Liquid democracy aims to provide the best of both worlds by letting voters decide whether they want to _cast_ their opinion on an issue at hand, or prefer to _delegate_ their voting weight to some other, trusted voter. Delegations are transitive, i.e., if voter \(v_{1}\) delegates to voter \(v_{2}\), and \(v_{2}\) in turn delegates to voter \(v_{3}\), who casts its vote, then \(v_{3}\) receives the voting weight of both \(v_{1}\) and \(v_{2}\). Liquid democracy has been implemented, for example, by political parties [14] and Google [1]. From a theoretic viewpoint, liquid democracy has been studied intensively by the social choice community in the last decade [13].

Earlier works on liquid democracy [15, 16] have pointed towards the issue of _delegation cycles_, e.g., the situation that occurs when voter \(v_{3}\) in the above example decides to delegate to voter \(v_{1}\) instead of casting its vote. If this happens, none of the three voters reaches acasting voter via a chain of trusted delegations, and therefore their voting weight would be lost. In order to reduce the risk of the appearance of such so-called _isolated_ voters, several scholars suggested to allow voters to indicate _back-up_ delegations (Brill, 2018; Golz et al., 2021; Kavitha et al., 2022) that may be used in case there is no delegation chain using only top-choice delegations. In _liquid democracy with ranked delegations_(Brill et al., 2022; Colley et al., 2022; Kavitha et al., 2022), voters are assumed to indicate a set of trusted delegates together with a ranking (preference order) among them. In fact, Brill et al. (2022) showed empirically that in many random graph models, one to two back-up delegations per voter suffice in order to avoid the existence of isolated voters almost entirely.

Allowing the voters to indicate multiple possible delegations calls for a principled way to decide between multiple possible delegation chains. For example, consider Figure 1: Should voter \(v_{1}\)'s weight be assigned to casting voter \(s_{1}\), via \(v_{1}\)'s second-ranked delegation, or should it rather be assigned to voter \(s_{2}\) by following \(v_{1}\)'s first-ranked delegation to voter \(v_{2}\), and then following \(v_{2}\)'s second-ranked delegation? Brill et al. (2022) introduced the concept of _delegation rules_, which take as input a _delegation graph_ (i.e., a digraph with a rank function on the edges) and output an assignment of each (non-isolated) delegating voter to a casting voter. In order to navigate within the space of delegation rules, they apply the axiomatic method (Thomson, 2001), as commonly used in social choice theory. In particular, the authors argue that the following three axioms are desirable:

* _Confluence_: A delegation rule selects one path from every delegating voter to a casting voter and these paths are "consistent" with one another. That is, when the path of voter \(v_{1}\) reaches some other delegating voter \(v_{2}\), the remaining subpath of \(v_{1}\) coincides with the path of \(v_{2}\). This was argued to increase accountability of delegates (Brill et al., 2022; Golz et al., 2021).
* _Anonymity_: A delegation rule should make decisions solely on the structure of the graph, not on the identities of the voters, i.e. it should be invariant under renaming of the voters.
* _Copy-robustness_: If a delegating voter \(v_{1}\) decides to cast her vote herself instead of delegating, this should not change the sum of the voting weight assigned to \(v_{1}\)'s representative and herself. This property was emphasized by practitioners (Behrens and Swierczek, 2015) to avoid manipulations in the system by delegating voters acting as casting voters but actually _copying_ the vote of their former representative.

For any pair of axioms (i) to (iii), Brill et al. (2022) provide a delegation rule that satisfies both of them. In contrast, we prove in Section 7 that there exists no delegation rule that satisfies all three properties simultaneously, thereby strengthening an impossibility result by Brill et al. (2022).2

Footnote 2: Brill et al. (2022) show that there exists no delegation rule belonging to the subclass of _sequence rules_ that is both confluent and copy-robust. Any sequence rule is in particular anonymous.

Our ContributionWe show that the above impossibility is due to the restriction that delegation rules may not distribute the voting weight of a delegating voter to more than one casting voter.

* We generalize the definition of delegation rules to _fractional delegation rules_ (Section 3) and provide generalizations of all three axioms above (Section 7).
* We introduce a natural variant of the Borda Branching rule (Brill et al., 2022), which we call Mixed Borda Branching. We show that this rule is equivalent to the Random Walk Rule, a fractional delegation rule that has been suggested by Brill (2018).
* In our main result, we build upon _Fulkerson's algorithm_(Fulkerson, 1974) and the _Markov chain tree theorem_(Leighton and Rivest, 1986) and show the existence of a polynomial-time algorithm for Mixed Borda Branching. This algorithm is of independent interest, as it computes the probability of two nodes being connected, when sampling a min-cost branching in a digraph uniformly at random. This problem features in semi-supervised learning, under the name _directed power watershed_(Fita Sanmartin et al., 2021), a directed

Figure 1: Example of a delegation graph. Delegating voters (\(v_{1}\) and \(v_{2}\)) are indicated by circles and casting voters (\(s_{1}\) and \(s_{2}\)) by squares. Solid edges represent the first-ranked delegations and dashed edges second-ranked delegations.

variant of the _power watershed_(Couprie et al., 2010). To the best of our knowledge, we provide the first efficient algorithm.
* In Section 7, we show that the Random Walk Rule (and thus Mixed Borda Branching) satisfies the generalizations of all three axioms. We also formalize the impossibility for non-fractional delegation rules. Beyond that, we show that the Random Walk Rule satisfies a generalization of a further axiom (_guru participation_) which has been studied in the literature (Kotsialou and Riley, 2020, Colley et al., 2022, Brill et al., 2022) (Appendix C).

The proofs (or their completions) for results marked by (\(\bigstar\)) can be found in the appendix.

Related Work_Liquid democracy._ The idea to let agents rank potential delegates in liquid democracy was first presented by the developers of the liquid democracy platform _Liquid Feedback_(Behrens and Swierczek, 2015), who presented seven properties that cannot be satisfied simultaneously. Some of these properties, such as _copy-robustness_ and _guru participation_, have been picked up in the social choice literature (Kotsialou and Riley, 2020, Brill et al., 2022, Colley et al., 2022). The connection of confluent delegation rules to branchings in a digraph was first emphasized by Kavitha et al. (2022) and later built upon in (Brill et al., 2022, Natsui and Takazawa, 2022). We base our model on (Brill et al., 2022), as their model captures all rules and axioms from the literature. Fractional delegations were studied by Degrave (2014) and Bersetche (2022), however, here agents indicate a desired distribution among their delegates instead of a ranking. While the two approaches are orthogonal, we argue in Appendix B that they could be easily combined (and our algorithm could be adjusted).

_Branchings and matrix tree theorems._ Our algorithm for computing Mixed Borda Branching is based on an algorithm for computing min-cost branchings in directed trees. This can be done, e.g., via Edmond's algorithm (Edmonds, 1967) or Fulkerson's algorithm (Fulkerson, 1974). The latter comes with a characterization of min-cost branchings in terms of dual certificates, which we utilize in Algorithm 2. We refer to Kamiyama (2014) for a comprehensive overview on the literature of min-cost branchings. Our algorithm makes use of (a directed version of) the _matrix tree theorem_(Tutte, 1948), which allows to count directed trees in a digraph. An extension of this theorem is the _Markov chain tree theorem_(Leighton and Rivest, 1986), which we use for the construction of our algorithm as well as for proving the equivalence of Mixed Borda Branching and the Random Walk Rule. A comprehensive overview of the literature is given by Pitman and Tang (2018).

_Semi-supervised learning._ There is a connection of our setting to graph-based semi-supervised learning. In particular, Algorithm 2 is related to the _power watershed algorithm_(Couprie et al., 2010) and the _probabilistic watershed algorithm_(Fita Sanmartin et al., 2019, 2021). We elaborate on this connection in Section 4.

## 2 Preliminaries

The main mathematical concepts used in this paper are directed graphs (also called digraphs), branchings, in-trees, and Markov chains, all of which we briefly introduce below.

We assume that a digraph \(G\) has no parallel edges, and denote by \(V(G)\) the set of nodes and by \(E(G)\) the set of edges of \(G\). We use \(\delta_{G}^{+}(v)\) to indicate the set of outgoing edges of node \(v\in V(G)\), i.e., \(\delta_{G}^{+}(v)=\{(v,u)\in E(G)\}\). For a set of nodes \(U\subseteq V(G)\), we define the outgoing cut of \(U\) by \(\delta_{G}^{+}(U)=\{(u,v)\in E(G)\mid u\in U,v\in V(G)\setminus U\}\). A walk \(W\) is a node sequence \((W_{1},\ldots,W_{|W|})\), such that \((W_{i},W_{i+1})\in E(G)\) for \(i\in\{1,\ldots,|W|-1\}\). We omit \(G\) if it is clear from the context.

**Branchings and in-trees.** Given a digraph \(G\), we say that \(B\subseteq E\) is a branching (or in-forest) in \(G\), if \(B\) is acyclic, and the out-degree of any node \(v\in V(G)\) in \(B\) is at most one, i.e., \(|B\cap\delta^{+}(v)|\leq 1\). Throughout the paper, we use the term _branching_ to refer to _maximum-cardinality branchings_, i.e., branchings \(B\) maximizing \(|B|\) among all branchings in \(G\). For a given digraph \(G\) we define \(\mathcal{B}(G)\) as the set of all (max-cardinality) branchings and \(\mathcal{B}_{v,s}(G)\) as the set of all (max-cardinality) branchings in which node \(v\in V(G)\) has a path to the node \(s\in V(G)\). For any branching \(B\) in any digraph \(G\) it holds that \(|B|\leq|V(G)|-1\). If in fact \(|B|=|V(G)|-1\), then \(B\) is also called an in-tree. For every in-tree \(T\subseteq E\), there exists exactly one node \(v\in V(G)\) without outgoing edge. In this case, we also say that \(v\) is the sink of \(T\) and call \(T\) a \(v\)-tree. For \(v\in V(G)\), we let \(\mathcal{T}_{v}(G)\) be the set of \(v\)-trees in \(G\).

**Matrix tree theorem.** For our main result we need to count the number of in-trees in a weighted digraph, which can be done with help of the matrix tree theorem. For a digraph \(G\) with weight function \(w:E\to\mathbb{N}\), we define the weight of a subgraph \(T\subseteq E\) as \(w(T)=\prod_{e\in T}w(e)\) and the weight of a collection of subgraphs \(\mathcal{T}\) as \(w(\mathcal{T})=\sum_{T\in\mathcal{T}}w(T)\). Then, we define the _Laplacian_ matrix of \((G,w)\) as \(L=D-A\), where \(D\) is a diagonal matrix containing the weighted out-degree of any node \(v\) in the corresponding entry \(D_{v,v}\) and \(A\) is the weighted adjacency matrix, given as \(A_{u,v}=w((u,v))\) for any edge \((u,v)\in E\) and zero everywhere else. Moreover, we denote by \(L^{(v)}\) the matrix resulting from \(L\) when deleting the row and column corresponding to \(v\).

**Lemma 1** (Matrix tree theorem (Tutte, 1948; De Leenheer, 2020)3).: _Let \((G,w)\) be a weighted digraph and let \(L\) be its Laplacian matrix. Then,_

Footnote 3: Tutte (1948) proves the theorem for digraphs, the weighted version can be found in (De Leenheer, 2020).

\[\text{det}(L^{(v)})=\sum_{T\in\mathcal{T}_{v}}\prod_{e\in T}w(e)=w(\mathcal{T }_{v}).\]

If we interpret the weight of an edge as its _multiplicity_ in a multigraph, then \(\text{det}(L^{(v)})\) equals the total number of distinct \(v\)-trees.

Markov Chains.A Markov chain is a tuple \((G,P)\), where \(G\) is a digraph and the matrix \(P\in[0,1]^{|V|\times|V|}\) encodes the transition probabilities. That is, the entry \(P_{u,v}\) indicates the probability with which the Markov chain moves from state \(u\) to state \(v\) in one timestep. For a given edge \(e=(u,v)\in E\), we also write \(P_{e}\) to refer to \(P_{u,v}\). For all \(v\in V\) it holds that \(\sum_{e\in\delta^{+}(v)}P_{e}=1\). Moreover, if \((u,v)\notin E\), we assume \(P_{u,v}=0\). We define the matrix \(Q\in[0,1]^{|V|\times|V|}\) as \(Q=\lim_{\tau\to\infty}\frac{1}{\tau}\sum_{i=0}^{\tau}P^{\tau}\). If \((G,P)\) is an absorbing Markov chain, \(Q_{u,v}\) corresponds to the probability for a random walk starting in \(u\) to end in absorbing state \(v\). In contrast, if \(G\) is strongly connected and \(P\) is positive for all edges in \(G\), then \(Q_{u,v}\) corresponds to the relative number of times \(v\) is visited in an infinite random walk independent of the starting state (Grinstead and Snell, 1997).

## 3 Liquid Democracy with Fractional Delegations

A _delegation graph_\(G=(N\cup S,E)\) is a digraph with a _cost function4\(c:E\to\mathbb{N}\) (called _rank function_ before), representing the preferences of the voters over their potential delegates (lower numbers are preferred). Nodes correspond to voters and an edge \((u,v)\in E\) indicates that \(u\) accepts \(v\) as a delegate. By convention, the set of nodes \(S\) corresponds exactly to the sinks of the digraph, i.e., the set of nodes without outgoing edges. Thus, \(S\) captures the casting voters, and \(N\) the delegating voters. We assume for all \(v\in N\) that they reach some element in \(S\).5 A _delegation rule_ maps any delegation graph to a _fractional assignment_, i.e., a matrix \(A\in[0,1]^{N\times S}\), where, for every \(v\in N\), \(s\in S\), the entry \(A_{v,s}\) indicates the fraction of delegating voter \(v\)'s weight that is allocated to casting voter \(s\in S\).6 For any \(v\in N\) we refer to any casting voter \(s\in S\) with \(A_{v,s}>0\) as a _representative_ of \(v\). For assignment \(A\), the _voting weight_ of a casting voter \(s\in S\) is defined as \(\pi_{s}(A)=1+\sum_{v\in N}A_{v,s}\). A _non-fractional_ delegation rule is a special case of a delegation rule, that always returns assignments \(A\in\{0,1\}^{N\times S}\).

Footnote 4: We remark that in this paper cost functions and weight functions play different roles.

Footnote 5: If this is not the case, such a voter is _isolated_, i.e., there is no chain of trusted delegations to a casting voter, thus we cannot meaningfully assign its voting weight. We remove isolated nodes in a preprocessing step.

Footnote 6: While this definition allows \(A_{v,s}>0\) for any \(v\in N\), \(s\in S\), for a sensible delegation rule, this should only be the case if there exists a path from \(v\) to \(s\). _Confluence_ (defined in Section 7) implies this restriction.

**Mixed Borda Branching**. The output of any non-fractional, confluent delegation rule can be represented as a branching: Any branching in the delegation graph consists of \(|N|\) edges, and every delegating voter has a unique path to some casting voter. Brill et al. (2022) suggested to select min-cost branchings, i.e., those minimizing \(\sum_{e\in B}c(e)\). The authors call these objects Borda branchings and show that selecting them yields a copy-robust rule. As this rule is inherently non-anonymous, we suggest to mix uniformly over all Borda branchings, hoping to gain anonymity without losing the other properties7. Formally, for a given delegation graph \((G,c)\), let \(\mathcal{B}^{*}(G)\) be the set of all Borda branchings and let \(\mathcal{B}^{*}_{v,s}(G)\) be the set of all Borda branchings in which delegating voter \(v\in N\) is connected to casting voter \(s\in S\). Mixed Borda Branching returns the assignment \(A\) defined as

Footnote 7: Confluence and copy-robustness are not directly inherited from the non-fractional counterpart of the rule.

\[A_{v,s}=\frac{|\mathcal{B}^{*}_{v,s}(G)|}{|\mathcal{B}^{*}(G)|}\quad\text{ for all }v\in V,s\in S.\]

[MISSING_PAGE_EMPTY:5]

[MISSING_PAGE_FAIL:6]

\(Y\) of \(X\) and every node \(v\in Y\), the number of min-cost \(v\)-trees inside the graph \(G[Y]=(Y,E[Y])\), where \(E[Y]=\{(u,v)\in E\mid u,v\in Y\}\). This number, denoted by \(t_{Y}(v)\), can be computed by recursively applying the matrix tree theorem. Coming back to the graph \(G_{X}\), however, we need a more powerful tool since we need to compute the (relative) number of weighted branchings in \(G_{X}\) connecting any node to a sink node. Thus, we introduce the Markov chain tree theorem (in a slightly modified version so that it can deal with Markov chains induced by weighted digraphs).

For a weighted digraph \((G,w)\) we define the corresponding Markov chain \((G^{\prime},P)\) as follows: The digraph \(G^{\prime}\) is derived from \(G\) by adding self-loops, and for \((u,v)\in E(G^{\prime})\) we let

\[P_{u,v}=\begin{cases}\frac{w(u,v)}{\Delta}&\text{if }u\neq v\\ 1-\frac{\sum_{e\in\delta^{+}(u)}w(e)}{\Delta}&\text{if }u=v,\end{cases}\]

where \(\Delta=\max_{v\in V}\sum_{e\in\delta^{+}(v)}w(e)\).

**Lemma 3** (Markov chain tree theorem (Leighton and Rivest (1986))).: _Consider a weighted digraph \((G,w)\) and the corresponding Markov chain \((G^{\prime},P)\) and let \(Q=\lim_{\tau\to\infty}\frac{1}{\tau}\sum_{i=0}^{\tau}P^{\tau}\). Then, the entries of the matrix \(Q\) are given by_

\[Q_{u,v}=\frac{\sum_{B\in\mathcal{B}_{u,v}}\prod_{e\in B}P_{e}}{\sum_{B\in \mathcal{B}}\prod_{e\in B}P_{e}}=\frac{\sum_{B\in\mathcal{B}_{u,v}}\prod_{e\in B }w(e)}{\sum_{B\in\mathcal{B}}\prod_{e\in B}w(e)}=\frac{\sum_{B\in\mathcal{B}_{ u,v}}w(B)}{\sum_{B\in\mathcal{B}}w(B)}\quad.\]

We formalize Algorithm 2, which takes as input a delegation graph \((G,c)\) and, in contrast to the intuition above, works in a bottom-up fashion. We refer to Figure 2 for an illustration.

**Theorem 4** (\(\bigstar\)).: _Algorithm 2 returns Mixed Borda Branching and runs in \(\text{poly}(n)\)._

Proof sketch.: The main part of the proof, shows by induction over the laminar hierarchy of \(\mathcal{F}\), that for every \(X\in\mathcal{F}\) and \(v\in X\), the value \(t_{X}(v)\) equals the number of min-cost \(v\)-trees in \(G[X]\). Given that this is true, one can show that \(w_{X}(\mathcal{B}_{Y,\{s\}}(G_{X}))\) equals the number of min-cost branchings in \(G\) that connect any node in \(v\in Y\) to the sink \(s\) and that \(w_{X}(\mathcal{B}(G_{X}))\) equals the number of all min-cost branchings in \(G\). Hence, we can utilize the Markov chain tree theorem on the graph \(G_{X}\) to compute

Figure 2: Two iterations of Algorithm 2. Costs are depicted by edge patterns (solid equals cost \(1\), dashed equals cost \(2\), and dotted equals cost \(3\)) and weights are depicted by numbers on the edges.

the outcome of Mixed Borda Branching.

As for the running time, the main observation is that the computation of the number of branchings in a weighted digraph can be done in time logarithmic in the highest weight of an edge (and polynomial in the number of edges). Since all of our weights are bounded by the maximum number of branchings in the original graph (which is bounded by \(|N|^{|N|}\)), we can show the polynomial running time. 

## 6 Equivalence of Mixed Borda Branching and Random Walk Rule

With the help of the Markov chain tree theorem, as stated in Section 5, we can show the equivalence of the Random Walk Rule and Mixed Borda Branching.

**Theorem 5**.: _Let \((G,c)\) be a delegation graph and \(A\) and \(\hat{A}\) be the assignments returned by Mixed Borda Branching and the Random Walk Rule, respectively. Then, \(A=\hat{A}\)._

Proof.: Let \(v\in N\) and \(s\in S\), then,

\[\hat{A}_{v,s} =\lim_{\varepsilon\to 0}\Big{(}\frac{1}{\tau}\sum_{\tau=1}^{ \infty}(P^{(\varepsilon)})^{\tau}\Big{)}_{v,s}=\lim_{\varepsilon\to 0}\frac{\sum_{B \in\mathcal{B}_{v,s}}\prod_{e\in B}P^{(\varepsilon)}_{e}}{\sum_{B\in\mathcal{B }}\prod_{e\in B}P^{(\varepsilon)}_{e}}=\lim_{\varepsilon\to 0}\frac{\sum_{B\in \mathcal{B}_{v,s}}\prod_{(u,v)\in B}\frac{\varepsilon^{c(u,v)}}{\varepsilon_{u }}}{\sum_{B\in\mathcal{B}}\prod_{(u,v)\in B}\frac{\varepsilon^{c(u,v)}}{ \varepsilon_{u}}}\] \[=\lim_{\varepsilon\to 0}\frac{\prod_{u\in N}(\varepsilon_{u})^{-1} \sum_{B\in\mathcal{B}_{v,s}}\varepsilon^{c(B)}}{\prod_{u\in N}(\varepsilon_{u })^{-1}\sum_{B\in\mathcal{B}}\varepsilon^{c(B)}}=\frac{\sum_{B\in\mathcal{B}^{ *}_{v,s}}1}{\sum_{B\in\mathcal{B}^{*}}1}=A_{v,s}.\]

We first use the definition of the Random Walk Rule, and then apply the Markov chain tree theorem (Lemma 3) for fixed \(\varepsilon\in(0,1]\) to obtain the second equality. For the third equality, we use the definition of \(P^{(\varepsilon)}\), and then factor out the normalization factor \(\bar{\varepsilon}_{u}\) for every \(u\in N\). For doing so, it is important to note that for every \(v\in N\) and \(s\in S\), every branching in \(\mathcal{B}_{v,s}\) and every branching in \(\mathcal{B}\) contains exactly one outgoing edge per node in \(N\). We also remind the reader that \(c(B)=\sum_{e\in B}c(e)\). Finally, we resolve the limit in the fifth equality by noting that the dominant parts of the polynomials are those corresponding to min-cost (Borda) branchings. 

Given Theorem 5, we can interpret Algorithm 2 as an algorithm computing limit absorbing probabilities of a class of parametric Markov chains. We explain this reinterpretation in Appendix B.

```
1: compute \((\mathcal{F},E_{y},y)\), set \(\mathcal{F}^{\prime}=\mathcal{F}\cup\{N\cup S\}\) and label its elements "unprocessed" \(\triangleright\) Algorithm 1
2:\(t_{\{v\}}(v)\gets 1\) for all \(v\in N\cup S\), label singletons as "processed"
3:do pick unprocessed \(X\in\mathcal{F}^{\prime}\) for which all children are processed, label \(X\) as "processed"
4: set \(w_{X}(Y,Y^{\prime})\leftarrow\sum_{(u,v)\in E_{y}\cap(Y\times Y^{\prime})}t_{Y} (u)\), for all children \(Y\) and \(Y^{\prime}\) of \(X\)
5:if\(X\neq N\cup S\)then
6:for all children \(Y\) of \(X\)do
7:\(t_{X}(v)\gets w_{X}(\mathcal{T}_{Y}(G_{X}))\cdot t_{Y}(v)\) for all \(v\in Y\)\(\triangleright\) Lemma 1
8:else compute absorbing probability matrix \(Q\) for the Markov chain\(\triangleright\) Lemma 3
9: corresponding to \((G_{X},w_{X})\)
10:while\(X\neq N\cup S\)
11:return for all \(v\in N\) and \(s\in S\): \(A_{v,s}\gets Q_{Y_{v},\{s\}}\), where \(Y_{v}\) child of \(N\cup S\) with \(v\in Y_{v}\)
```

**Algorithm 2** Computation of Mixed Borda Branching

## 7 Axiomatic Analysis

In this section, we generalize and formalize the axioms mentioned in Section 1 and show that the Random Walk Rule (and hence Mixed Borda Branching) satisfies all of them. In particular, our version of confluence (copy-robustness, respectively) reduces to (is stronger than, respectively) the corresponding axiom for the non-fractional case by Brill et al. (2022) (see Appendix C.1). We first define _anonymity_, which prescribes that a delegation rule should not make decisions based on the identity of the voters. Given a digraph with a cost function \((G,c)\) and a bijection \(\sigma:V(G)\to V(G)\), we define the graph \(\sigma((G,c))\) as \((G^{\prime},c^{\prime})\), where \(V(G^{\prime})=V(G)\), \(E(G^{\prime})=\{(\sigma(u),\sigma(v)\mid(u,v)\in E(G)\}\) and \(c^{\prime}(\sigma(u),\sigma(v))=c(u,v)\) for each edge \((u,v)\in E(G)\).

**Anonymity:** For any delegation graph \((G,c)\), any bijection \(\sigma:V(G)\to V(G)\), and any \(v\in N,s\in S\), it holds that \(A_{v,s}=A^{\prime}_{\sigma(v),\sigma(s)}\), where \(A\) and \(A^{\prime}\) are the outputs of the delegation rule for \((G,c)\) and \(\sigma((G,c))\), respectively.

**Theorem 6** (\(\bigstar\)).: _The Random Walk Rule satisfies anonymity._

We now define _copy-robustness_, which intuitively demands that if a delegating voter \(v\in N\) decides to cast their vote instead, the total voting weight of \(v\) and all its representatives should not change. This lowers the threat of manipulation by a voter deciding whether to cast or delegate their vote depending on which gives them and their representatives more total voting weight. This axiom was introduced (under a different name) by Behrens and Swierczek (2015) and defined for non-fractional delegation rules in Brill et al. (2022). We strengthen10 and generalize the version of Brill et al. (2022).

Footnote 10: Brill et al. (2022) restrict the condition to voters that have a direct connection to their representative.

**Copy-robustness:** For every delegation graph \((G,c)\) and delegating voter \(v\in N\), the following holds: Let \((\hat{G},c)\) be the graph derived from \((G,c)\) by removing all outgoing edges of \(v\), let \(A\) and \(\hat{A}\) be the output of the delegation rule for \((G,c)\) and \((\hat{G},c)\), respectively and let \(S_{v}=\{s\in S\mid A_{v,s}>0\}\) be the set of representatives of \(v\) in \((G,c)\). Then \(\sum_{s\in S_{v}}\pi_{s}(A)=\pi_{v}(\hat{A})+\sum_{s\in S_{v}}\pi_{s}(\hat{A})\).

**Theorem 7** (\(\bigstar\)).: _The Random Walk Rule satisfies copy-robustness._

_Proof sketch._ We show a statement that is slightly stronger than the condition for copy-robustness. Namely, the voting weight of every casting voter from \(S\setminus S_{v}\) remains equal when \(v\) changes from being a delegating voter to becoming a casting voter. Let \((G_{X},w_{X})\), with \(X=N\cup\hat{S}\), be the contracted graph constructed in Algorithm 2 for \((G,c)\). Analogously, let \((\hat{G}_{X},\hat{w}_{X})\) be the contracted graph constructed for \((\hat{G},c)\). In order to argue about their relation, we first show: If \(y\) and \(\hat{y}\) are the functions returned by Algorithm 1 for \(G\) and \(\hat{G}\), respectively, then \(\hat{y}(Y)=y(Y)\) for every set \(Y\subseteq 2^{N\setminus\{v\}}\) not containing \(v\), \(\hat{y}(\{v\})=1\), and \(\hat{y}(Y)=0\) for all other sets containing \(v\). Now, let \(Y_{v}\) be the node in \(G_{X}\) containing \(v\) and let \(\mathcal{U}\subseteq V(G_{X})\) be the subset of nodes in \(G_{X}\) that are not reachable from \(Y_{v}\). We argue that for any \(s\in\hat{S}\setminus S_{v}\), the Markov chain induced by \((G_{X},w_{X})\) can only reach \(\{s\}\) from a starting node in \(\mathcal{U}\). However, for all nodes in \(\mathcal{U}\), all of their walks to any \(\{s\},s\in S\setminus S_{v}\) are still existent in the graph \((\hat{G}_{X},\hat{w}_{X})\), and still have the same weight. Hence, the voting weight of any \(s\in S\setminus S_{v}\) remains unchanged when moving from \(G\) to \(\hat{G}\). 

To capture the requirement that the voting weight of different voters is assigned to casting voters in a "consistent" way, Brill et al. (2022) define confluence as follows: A delegation rule selects, for every voter \(u\in N\), one walk in the delegation graph starting in \(u\) and ending in some sink \(s\in S\), and assigns voter \(u\) to casting voter \(s\). A delegation rule satisfies confluence, if, as soon as the walk of \(u\) meets some other voter \(v\), the remaining subwalk of \(u\) equals the walk of \(v\).11 Below, we provide a natural generalization of the property by allowing a delegation rule to specify a probability distribution over walks. Then, conditioned on the fact that the realized walk of some voter \(u\) meets voter \(v\), the probability that \(u\) reaches some sink \(s\in S\) should equal the probability that \(v\) reaches \(s\).

Footnote 11: Brill et al. (2022) assume paths instead of walks. The two definitions are equivalent (see Appendix C.1).

**Confluence:** For every delegation graph \((G,c)\), there exists a probability distribution \(f_{v}\) for all \(v\in N\) over the set of walks in \(G\) that start in \(v\) and end in some sink, which is consistent with the assignment \(A\) of the delegation rule (i.e., \(\mathbb{P}_{W\sim f_{v}}[s\in W]=A_{v,s}\) for all \(v\in N,s\in S\)), and,

\[\mathbb{P}_{W\sim f_{u}}[s\in W\mid v\in W]=\mathbb{P}_{W\sim f_{v}}[s\in W] \quad\text{ for all }u,v\in N,s\in S.\]

Note that the requirement that \(A_{v,s}=\mathbb{P}_{W\sim f_{v}}[s\in W]\) implies that for any \(v\in V\) and \(s\in S\) we can have \(A_{v,s}>0\) only if there is a path from \(v\) to \(s\) in \(G\).

**Theorem 8** (\(\bigstar\)).: _The Random Walk Rule satisfies confluence._

_Proof sketch._ One can verify that every delegation rule that can be formalized via a Markov chain on the delegation graph \((G,c)\) satisfies confluence. In Section 5, we showed that the Random Walk Rule can be computed by solving a Markov chain \((G_{X},P)\) on the contracted graph \(G_{X}\) (for \(X=N\cup S\)). We utilize \((G_{X},P)\) to define a probability distribution over walks in \(G\) that satisfies confluence: Every walk in \(G\) can be mapped to a walk in \(G_{X}\) (by ignoring edges inside children of \(X\)), but there may exist many walks in \(G\) that map to the same walk in \(G_{X}\). We pick, for every walk\(\hat{W}\) in \(G_{X}\), one representative walk \(W\) in \(G\) and give it the same probability as \(\hat{W}\) in \((G_{X},P)\). All other walks in \(G\) get zero probability. When picking the representative walks, we can ensure that for every two nodes \(u,v\in N\cup S\), the probability that the walk of \(u\) reaches \(v\) equals the probability that \(Y_{u}\) reaches \(Y_{v}\) in \((G_{X},P)\), where \(Y_{u}\) and \(Y_{v}\) are the nodes in \(V(G_{X})\) containing \(u\) and \(v\), respectively. For the constructed probability distribution, we then show that the confluence condition is met. 

With the formal definition of confluence, anonymity, and copy-robustness we can now show that these properties altogether are impossible to achieve in the non-fractional case. Recall, that a non-fractional delegation rule is defined as a delegation rule, that returns assignments \(A\in\{0,1\}^{N\times S}\).

**Theorem 9**.: _No non-fractional delegation rule satisfies confluence, anonymity, and copy-robustness._

Proof.: Consider the graph \((G_{1},c_{1})\) in Figure 3. There are four non-fractional assignments in \((G_{1},c_{1})\): Both \(v_{1}\) and \(v_{2}\) can either be assigned \(v_{3}\) or \(v_{4}\). Suppose a rule chooses assignment \(A\) with \(A_{v_{1},v_{4}}=A_{v_{2},v_{3}}=1\). This rule cannot satisfy confluence, as any walk from \(v_{2}\) to \(v_{3}\) includes \(v_{1}\) and confluence requires \(1=A_{v_{2},v_{3}}=\mathbb{P}_{W\sim f_{v_{2}}}[v_{3}\in W]=\mathbb{P}_{W\sim f _{v_{2}}}[v_{3}\in W\mid v_{1}\in W]=\mathbb{P}_{W\sim f_{v_{1}}}[v_{3}\in W]= A_{v_{1},v_{3}}=0\). Now, suppose a delegation rule chooses assignment \(A\) with \(A_{v_{1},v_{3}}=A_{v_{2},v_{3}}=1\). We define the bijection \(\sigma\) mapping \(v_{1}\) to \(v_{2}\), \(v_{2}\) to \(v_{1}\), \(v_{3}\) to \(v_{4}\) and \(v_{4}\) to \(v_{3}\). Then, \(\sigma((G_{1},c))=(G_{1},c)\) and thus \(A^{\prime}_{v_{1},v_{3}}=A^{\prime}_{v_{2},v_{3}}=1\) in the assignment \(A^{\prime}\) that the rule chooses for \(\sigma((G_{1},c))\). This contradicts anonymity, since \(1=A_{v_{1},v_{3}}\neq A^{\prime}_{\sigma(v_{1}),\sigma(v_{3})}=A^{\prime}_{v_{ 2},v_{4}}=0\). We can make the same argument in the case of \(A_{v_{1},v_{4}}=A_{v_{2},v_{4}}=1\). For any rule satisfying anonymity and confluence the chosen assignment \(A\) must therefore have \(A_{v_{1},v_{2}}=A_{v_{2},v_{4}}=1\).

The above arguments are independent of the cost function \(c\), so long as we have \(c(v_{1},v_{2})=c(v_{2},v_{1})\) and \(c(v_{1},v_{3})=c(v_{2},v_{4})\), needed for the equality of \(\sigma((G_{1},c))\) and \((G_{1},c)\). Thus, any rule satisfying anonymity and confluence must choose the assignment \(A\) with \(A_{v_{1},v_{2}}=A_{v_{3},v_{4}}=1\) for \((G_{2},c_{2})\).

We modify \((G_{1},c_{1})\) by making \(v_{2}\) a casting voter (as in the definition of copy-robustness) and retrieve \((G_{3},c_{3})\). Copy robustness requires that the assignment from \(v_{1}\) to \(v_{4}\) in \(G_{1}\) (which is zero) must be the same as the sum of assignments from \(v_{1}\) to \(v_{4}\) and \(v_{2}\). Thus, we have \(A_{v_{1},v_{3}}=1\) for the assignment \(A\), that any confluent, anonymous, and copy-robust rule chooses for \((G_{3},c_{3})\). However, we can also construct \((G_{3},c_{3})\) from \((G_{2},c_{2})\) by making \(v_{3}\) a casting voter. Then, analogously, copy-robustness requires \(A_{v_{1},v_{2}}=1\) for the assignment of \((G_{3},c_{3})\), leading to a contradiction. 

Since the Random Walk Rule (and thus Mixed Borda Branching) satisfies generalizations of the three axioms, the above impossibility is due to its restriction to non-fractional rules.

## 8 Concluding Remarks

We generalized the setting of liquid democracy with ranked delegations to allow for fractional delegation rules. Beyond that, we presented a delegation rule that can be computed in polynomial time and satisfies a number of desirable properties. A natural follow-up question is to understand the entire space of delegation rules satisfying these properties.

Fractional delegations have been recently implemented (see electric.vote) and studied by Degrave (2014) and Bersetche (2022). In contrast to our setting, these approaches let agents declare a desired _distribution_ over their delegates (instead of rankings). We remark that one could easily combine the two approaches by letting agents declare their desired split within each equivalence class of their ranking. Our algorithm can be extended for this setting (see Appendix B).

There exists a line of research which aims to understand liquid democracy from an epistemic viewpoint (Kahng et al., 2021; Caragiannis and Micha, 2019; Halpern et al., 2023). Here, many of the negative results stem from the fact that voting weight is concentrated on few casting voters. Since, intuitively, ranked delegations can help to distribute the voting weight more evenly, it would be interesting to study these through the epistemic lense

Figure 3: Situation in the proof of Theorem 9. Solid edges correspond to first-choice delegations, dashed edges to second-choice delegations.

## Acknowledgements

This work was supported by the _Deutsche Forschungsgemeinschaft_ (under grant BR 4744/2-1), the _Centro de Modelamiento Matematico (CMM)_ (under grant FB210005, BASAL funds for center of excellence from ANID-Chile), _ANID-Chile_ (grant ACT210005), and the _Dutch Research Council (NWO)_ (project number 639.023.811, VICI "Collective Information"). Moreover, this work was supported by the National Science Foundation under Grant No. DMS-1928930 and by the Alfred P. Sloan Foundation under grant G-2021-16778, while Ulrike Schmidt-Kraepelin was in residence at the Simons Laufer Mathematical Sciences Institute (formerly MSRI) in Berkeley, California, during the Fall 2023 semester.

We would like to thank Markus Brill for suggesting the setting to us as well as insightful discussions. Moreover, we thank Jannik Matuschke for helpful discussions on min-cost branchings. Also, we thank our colleagues from Universidad de Chile, Martin Lackner, and Theo Delemazure for their valuable feedback.

## References

* Behrens and Swierczek (2015) J. Behrens and B. Swierczek. Preferential delegation and the problem of negative voting weight. _The Liquid Democracy Journal_, 3:6-34, 2015.
* Bersetche (2022) F. Bersetche. A Voting Power Measure for Liquid Democracy with Multiple Delegation. Technical report, arXiv: 2209.14128 [cs.MA], 2022.
* Brill (2018) M. Brill. Interactive democracy. In _Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems (AAMAS) Blue Sky Ideas track_, pages 1183-1187. IFAAMAS, 2018.
* Brill et al. (2022) M. Brill, T. Delemazure, A.-M. George, M. Lackner, and U. Schmidt-Kraepelin. Liquid democracy with ranked delegations. In _Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI)_, pages 4884-4891. AAAI Press, 2022.
* Caragiannis and Micha (2019) I. Caragiannis and E. Micha. A contribution to the critique of liquid democracy. In _Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI)_, pages 116-122. IJCAI, 2019.
* Christoff and Grossi (2017) Z. Christoff and D. Grossi. Binary voting with delegable proxy: An analysis of liquid democracy. In _Proceedings of the 16th Conference on Theoretical Aspects of Rationality and Knowledge (TARK)_, pages 134-150, 2017.
* Colley et al. (2022) R. Colley, U. Grandi, and A. Novaro. Unravelling multi-agent ranked delegations. _Autonomous Agents and Multi-Agent Systems_, 36(1):1-35, 2022.
* Couprie et al. (2010) C. Couprie, L. Grady, L. Najman, and H. Talbot. Power watershed: A unifying graph-based optimization framework. _IEEE transactions on pattern analysis and machine intelligence_, 33(7):1384-1399, 2010.
* De Leenheer (2020) P. De Leenheer. An Elementary Proof of a Matrix Tree Theorem for Directed Graphs. _SIAM Review_, 62(3):716-726, 2020.
* Degrave (2014) J. Degrave. Resolving multi-proxy transitive vote delegation. Technical report, arXiv: 1412.4039 [cs.MA], 2014.
* Edmonds (1967) J. Edmonds. Optimum branchings. _Journal of Research of the National Bureau of Standards_, 71(4):233-240, 1967.
* Fanamartin et al. (2019) E. Fita Sanamartin, S. Damrich, and F. A. Hamprecht. Probabilistic Watershed: Sampling all spanning forests for seeded segmentation and semi-supervised learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* Fita Sanmartin et al. (2021) E. Fita Sanmartin, S. Damrich, and F. A. Hamprecht. Directed probabilistic watershed. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 20076-20088, 2021.
* Fita et al. (2019)D. R. Fulkerson. Packing rooted directed cuts in a weighted directed graph. _Mathematical Programming_, 6(1):1-13, 1974.
* Gathen and Gerhard (2013) J. Gathen and J. Gerhard. _Modern Computer Algebra_. Cambridge University Press, 2013. ISBN 978-1-107-03903-2.
* Golz et al. (2021) P. Golz, A. Kahng, S. Mackenzie, and A. D. Procaccia. The fluid mechanics of liquid democracy. _ACM Transactions on Economics and Computation_, 9(4):1-39, 2021.
* Grinstead and Snell (1997) C. M. Grinstead and J. L. Snell. _Introduction to Probability_. American Mathematical Society, second edition, 1997.
* Hahn et al. (2011) E. M. Hahn, H. Hermanns, and L. Zhang. Probabilistic reachability for parametric Markov models. _International Journal on Software Tools for Technology Transfer_, 13(1):3-19, 2011.
* Halpern et al. (2023) D. Halpern, J. Halpern, A. Jadbabaie, E. Mossel, A. Procaccia, and M. Revel. In defense of liquid democracy. In _Proceedings of the 24th ACM Conference on Economics and Computation (ACM-EC)_, 2023. Forthcoming.
* Hardt and Lopes (2015) S. Hardt and L. Lopes. Google votes: A liquid democracy experiment on a corporate social network. Technical report, Technical Disclosure Commons, 2015.
* Hopcroft et al. (1983) J. E. Hopcroft, J. D. Ullman, and A. V. Aho. _Data structures and algorithms_, volume 175. Addisonwesley Boston, 1983.
* Kahng et al. (2021) A. Kahng, S. Mackenzie, and A. D. Procaccia. Liquid democracy: An algorithmic perspective. _Journal of Artificial Intelligence Research_, 70:1223-1252, 2021.
* Kamiyama (2014) N. Kamiyama. Arborescence problems in directed graphs: Theorems and algorithms. _Interdisciplinary information sciences_, 20(1):51-70, 2014.
* Kavitha et al. (2022) T. Kavitha, T. Kiraly, J. Matuschke, I. Schlotter, and U. Schmidt-Kraepelin. Popular branchings and their dual certificates. _Mathematical Programming_, 192(1):223-237, 2022.
* Kling et al. (2015) C. Kling, J. Kunegis, H. Hartmann, M. Strohmeier, and S. Staab. Voting behaviour and power in online democracy: A study of liquidfeedback in germany's pirate party. In _International Conference on Weblogs and Social Media_, pages 208-217, 2015.
* Kotsialou and Riley (2020) G. Kotsialou and L. Riley. Incentivising participation in liquid democracy with breadth first delegation. In _Proceedings of the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)_, pages 638-644. IFAAMAS, 2020.
* Leighton and Rivest (1986) F. Leighton and R. Rivest. Estimating a probability using finite memory. _IEEE Transactions on Information Theory_, 32(6):733-742, 1986.
* Natsui and Takazawa (2022) K. Natsui and K. Takazawa. Finding popular branchings in vertex-weighted digraphs. In _Proceedings of the 16th International Conference and Workshops on Algorithms and Computation (WALCOM)_, pages 303-314. Springer, 2022.
* Paulin (2020) A. Paulin. Ten years of liquid democracy research an overview. _Central and Eastern European eDem and eGov Days_, 2020.
* Pitman and Tang (2018) J. Pitman and W. Tang. Tree formulas, mean first passage times and Kemeny's constant of a Markov chain. _Bernoulli_, 24(3):1942-1972, 2018.
* Thomson (2001) W. Thomson. On the axiomatic method and its recent applications to game theory and resource allocation. _Social Choice and Welfare_, 18(2):327-386, 2001.
* Tutte (1948) W. Tutte. The dissection of equilateral triangles into equilateral triangles. 44(4):463-482, 1948.

## Appendix A Missing Proofs of Section 5

**Lemma 2** (\(\bigstar\)).: _Let \((G,c)\) be a delegation graph and let \((\mathcal{F},E_{y},y)\) be the output of Algorithm 1. Then:_

1. _For every_ \((G,c)\)_, the output of the algorithm is unique, i.e., it does not depend on the choice of the strongly connected component in line_ 3_._
2. \(\mathcal{F}\) _is laminar, i.e., for any_ \(X,Y\in\mathcal{F}\) _it holds that either_ \(X\subseteq Y\)_,_ \(Y\subseteq X\)_, or_ \(X\cap Y=\emptyset\)_._
3. _Branching_ \(B\) _in_ \((G,c)\) _is min-cost iff (a)_ \(B\subseteq E_{y}\)_, and (b)_ \(|B\cap\delta^{+}(X)|=1\) _for all_ \(X\in\mathcal{F},X\subseteq N\)_._
4. _For every_ \(X\in\mathcal{F}\)_, an in-tree_ \(T\) _in_ \(G[X]=(X,E[X])\)_, where_ \(E[X]=\{(u,v)\in E\mid u,v\in X\}\)_, is min-cost iff (a)_ \(T\subseteq E_{y}\)_, and (b)_ \(|T\cap\delta^{+}(Y)|=1\) _for all_ \(Y\in\mathcal{F}\) _such that_ \(Y\subset X\)_._

Proof.: Let \((G,c)\) be a delegation graph and let \((\mathcal{F},E_{y},y)\) be the output of Algorithm 1.

We start by proving statement (ii). The sets in \(\mathcal{F}\) correspond exactly to those sets with positive \(y\)-value. Assume for contradiction that there exist two sets \(X,Y\in\mathcal{F}\) with \(X\cap Y\neq\emptyset\) and none of the subsets is a subset of the other. Assume without loss of generality that \(X\) was selected before \(Y\) by the algorithm and let \(y_{1}\) and \(y_{2}\) be the status of the function \(y\) in each of the two situation. Then, by construction of the algorithm it holds that \(G_{1}=(N\cup S,E_{y_{1}})\) is a subgraph of \(G_{2}=(N\cup S,E_{y_{2}})\). This is because once an edge is added to the set of tight edges (denoted by \(E_{y}\)) it remains in this set. Since \(Y\) is a strongly connected component in \(G_{2}\) without outgoing edge, it holds that for every \(z\in X\setminus Y\) and \(z^{\prime}\in X\cap Y\), the node \(z^{\prime}\) does not reach \(z\) in \(G_{2}\). However, this is a contradiction to the fact that \(X\) is a strongly connected component in the graph \(G_{1}\), which concludes the proof of statement (ii).

We now turn to prove statement (i) and already assume that \(\mathcal{F}\) is laminar. We fix an order of the selected strongly connected components in line 3 of the algorithm. Then, suppose that for some other choices in line 3, the algorithm returns some other output \((\hat{\mathcal{F}},E_{\hat{y}},\hat{y})\). Note that \(\hat{\mathcal{F}}\neq\mathcal{F}\) or \(E_{\hat{y}}\neq E_{y}\) implies that \(\hat{y}\neq y\). Thus, it suffices to assume for contradiction that \(\hat{y}\neq y\). Then there must be a smallest set \(X\), that has \(y(X)\neq\hat{y}(X)\) (without loss of generality we assume \(y(X)>\hat{y}(X)\)). Let \(\mathcal{X}=2^{X}\setminus\{X\}\) be the set of all strict subsets of \(X\). Since we defined \(X\) to be of minimal cardinality, we have \(y[\mathcal{X}]=\hat{y}[\mathcal{X}]\), where \(y[\mathcal{X}]\), and \(\hat{y}[\mathcal{X}]\) denote the restriction of \(y\) and \(\hat{y}\) to \(\mathcal{X}\), respectively. Because \(y(X)>0\), all children of \(X\) are strongly connected by tight edges with respect to \(y[\mathcal{X}]\) and have no tight edges pointing outside of \(X\). Now, consider the iteration of the alternative run of the Algorithm 2, in which the algorithm added the last set in \(\mathcal{X}\cup\{X\}\). Since \(\hat{y}(X)<y(X)\), for every further iteration of the algorithm, a chosen set \(X^{\prime}\neq X\) cannot contain any node in \(X\) (because otherwise \(X^{\prime}\) cannot form a strongly connected component without outgoing edge). However, since the nodes in \(X\) cannot reach a sink via tight edges, this is a contradiction to the termination of the algorithm.

We now prove statement (iii). The plan of attack is the following: First we define a linear program that captures the min-cost branchings in a delegation graph. Second, we dualize the linear program and show that \(y\) (more precisely a minor variant of \(y\)) is an optimal solution to the dual LP, and third, utilize complementary slackness to prove the claim. For a given delegation graph \((G,c)\) with \(V(G)=N\cup S\) we define the following linear program, also denoted by (LP):

\[\min\sum_{e\in E}c(e)x_{e}\] \[\sum_{e\in\delta^{+}(X)}x_{e}\geq 1 \forall\ X\subseteq N\] \[x_{e}\geq 0 \forall e\in E\]We claim that every branching \(B\) in \(G\) induces a feasible solution to (LP). More precisely, given a branching \(B\), let

\[x_{e}=\begin{cases}1&\text{if }e\in B\\ 0&\text{if }e\notin B.\end{cases}\]

The last constraint is trivially satisfied. Now, assume for contradiction that there exists \(X\subseteq N\) such that the corresponding constraint in (LP) is violated. In this case the nodes in \(X\) have no path towards some sink node in \(B\), a contradiction to the fact that \(B\) is a (maximum cardinality) branching. In particular, this implies that the objective value of (LP) is at most the minimum cost of any branching in \(G\) (in fact the two values are equal, but we do not need to prove this at this point). We continue by deriving the dual of (LP), to which we refer to as (DLP):

\[\max\sum_{X\subseteq N}y_{X}\] \[\sum_{X\subseteq N\mid e\in\delta^{+}(X)}y_{X}\leq c(e) \forall\ e\in E\] \[y_{X}\geq 0 \forall X\subseteq N\]

Now, let \(y\) be the function returned by Algorithm 1. We define \(\hat{y}\), which is intuitively \(y\) restricted to all subsets on \(N\), more precisely, \(\hat{y}(X)=y(X)\) for all \(X\subseteq N\). We claim that \(\hat{y}\) is a feasible solution to (DLP). This can be easily shown by induction. More precisely, we fix any \(e\in E\) and show that the corresponding constraint in (DLP) is satisfied throughout the execution of the algorithm. At the beginning of the algorithm \(y\) (and hence \(\hat{y}\)) is clearly feasible for (DLP). Now, consider any step in the algorithm and let \(X\) be the selected strongly connected component. If \(e\in\delta^{+}(X)\), then we know that the constraint corresponding to \(e\) is not tight (since \(X\) has no tight edge in its outgoing cut). Moreover, \(y\) is only increased up to the point that some edge in \(\delta^{+}(X)\) becomes tight (and not higher than that). Hence, after this round, the constraint for \(e\) is still satisfied. If, on the other hand, \(e\notin\delta^{+}(X)\), then the left-hand-side of \(e\)'s constraint remains equal when \(y(X)\) is increased. Hence, the constraint of \(e\) is still satisfied.

Next, we claim that there exists a branching \(B\) in \(G\), such that for the resulting primal solution \(x\), it holds that \(\sum_{e\in E}c(e)x_{e}=\sum_{X\subseteq N}\hat{y}(X)\). The branching \(B\) will be constructed in a top-down fashion by moving along the laminar hierarchy of \(\mathcal{F}\). To this end let \(G_{X}\) be the contracted graph as defined in Algorithm 2. We start by setting \(X=N\cup S\). Since every node in \(N\) can reach some sink via tight edges, we also know that every node in \(G_{X}\) can reach some sink. Hence, a branching in \(G_{X}\) has exactly one edge per node in \(V_{X}\) that is not a sink. Let's pick such a branching \(B_{X}\). We know that for every edge in \(B_{X}=(Y,Z)\) there exists some edge in the original graph \(G\) that is also tight, i.e., \(u\in Y\) and \(v\in Z\) such that \((u,v)\in E_{y}\). For every edge in \(B_{X}\) pick an arbitrary such edge and add it to \(B\). Now, pick an arbitrary node \(Y\in V(G_{X})\). By construction, we know that exactly one edge from \(B\) is included in \(\delta^{+}(Y)\), call this edge \((u,v)\). Then, within the graph \(G_{Y}\), there exists exactly one node \(Z\in V(G_{Y})\), that contains \(u\). We are going to search for a \(Z\)-tree within \(G_{Y}\). We know that such a tree exists since \(G_{Y}\) is strongly connected by construction. We follow the pattern from before, i.e., finding a \(Z\)-tree, mapping the edges back to the original graph (arbitrarily), and then continuing recursively. For proving our claim, it remains to show that \(\sum_{e\in E}c(e)x_{e}=\sum_{X\subseteq N}\hat{y}(X)\). The crucial observation is that, by construction, every set in \(\hat{\mathcal{F}}=\mathcal{F}\setminus\{\{s\}\mid s\in S\}\) is left by exactly one edge in \(B\). Hence, we can partition the set \(\hat{\mathcal{F}}\) into sets \(\bigcup_{e\in B}\hat{F}_{e}\), where \(\hat{\mathcal{F}}_{e}=\{X\in\hat{\mathcal{F}}\mid e\in\delta^{+}(X)\}\). Moreover, observe that every edge in \(B\) is tight. As a result we get that

\[\sum_{e\in B}c(e)x_{e}=\sum_{e\in B}\sum_{X\in\hat{\mathcal{F}}_{e}}\hat{y}(X) =\sum_{X\subseteq N}\hat{y}(X),\]

proving the claim.

As a result, note that we found a primal solution \(B\) (precisely, the \(x\) induced by \(B\)), and a dual solution \(\hat{y}\) having the same objective value. By weak duality, we can conclude that both solutions are in particular optimal. It only remains to apply complementary slackness to conclude the claim. To this end, let \(B\) be a min-cost branching and \(x\) be the induced primal solution. By the argument above we know that \(x\) is optimal. Now, for any \(X\subseteq N\) for which \(\hat{y}(X)>0\) (hence \(X\in\mathcal{F}\)), complementary slackness prescribes that the corresponding primal constraint is tight, i.e., \(\sum_{e\in\delta^{+}(X)}x_{e}=1\). Hence, the branching corresponding to \(x\) leaves the set \(X\) exactly once, and part (b) of statement (iii) is satisfied. For statement (a) we apply complementary slackness in the other direction. That is, when \(x_{e}>0\), this implies that the corresponding dual constraint is tight, implying that \(e\) has to be tight with respect to \(\hat{y}\) and therefore also with respect to \(y\) (recall that \(y\) and \(\hat{y}\) only differ with respect to the sink nodes).

We now turn to proving statement (iv). This is done almost analogously to statement (iii). Fix \(X\in\mathcal{F}\). In the following we argue about the min-cost in-trees in \(G[X]\) and how to characterize these via a linear program. To this end, we add a dummy sink node \(r\) to the graph \(G[X]\) and call the resulting graph \(\hat{G}\). More precisely, \(\hat{G}=(X\cup\{r\},E[X]\cup\{(u,r)\mid u\in X\})\). The cost of any edge \((u,r),u\in X\) is set to \(c^{*}:=\max_{e\in E(G)}c(e)+1\), where it is only important that this value is larger than any other cost in the graph. We define the following LP:

\[\min\sum_{e\in E(\hat{G})} c(e)x_{e}\] \[\sum_{e\in\delta^{+}_{\hat{G}}(Z)} x_{e}\geq 1 \forall\ Z\subseteq X\] \[x_{e}\geq 0 \forall e\in E(\hat{G})\]

For every min-cost in-tree \(T\) in \(G[X]\) we obtain a feasible solution to (LP). To this end, let \(u\in X\) be the sink node of \(T\) and define \(\hat{T}=T\cup\{(u,r)\}\). Then, translate \(\hat{T}\) to its incidence vector \(x\). Given this observation, we again derive the dual of (LP), to which we refer to as (DLP):

\[\max\sum_{Z\subseteq X}y_{Z}\] \[\sum_{Z\subseteq X\mid e\in\delta^{+}_{\hat{G}}(Z)} y_{Z}\leq c(e) \forall\ e\in E(\hat{G})\] \[y_{Z}\geq 0 \forall Z\subseteq X\]

Now, let \(y\) be the output of Algorithm 1 for the original graph \(G\). We derive \(\hat{y}:2^{X}\rightarrow\mathbb{R}\) as follows:

\[\hat{y}(Z)=\begin{cases}y(Z)&\text{ if }Z\subset X\\ c^{*}-\max_{u\in X}\sum_{Z\subset X\mid e\in\delta^{+}_{\hat{G}}(Z)}y(Z)&\text{ if }Z=X\end{cases}\]

First, analogously to (iii), it can be verified that \(\hat{y}\) is a feasible solution to (DLP). Moreover, again analogously to (iii), there exists some min-cost \(r\)-tree in \(\hat{G}\) and a corresponding primal solution \(x\), such that \(\sum_{e\in E(\hat{G})}c(e)x_{e}=\sum_{Z\subseteq X}\hat{y}_{X}\). (This tree is derived by first chosing a tight edge towards the dummy root node \(r\) and then again reverse over the laminar family \(\mathcal{F}\) restricted to \(X\).) This implies by weak duality that \(\hat{y}\) is an optimal solution to (DLP) and any min-cost \(r\)-tree in \(\hat{G}\) is an optimal solution to (LP). As a result, we can again apply complementary slackness in both directions: Let \(T\) be a min-cost in-tree in \(G[X]\) with sink node \(u\in X\). Then let \(\hat{T}=T\cup\{(u,r)\}\) be the corresponding min-cost \(r\)-tree in \(\hat{G}\) and \(x\) be the corresponding incidence vector. Then, complementary slackness implies that for any \(e\in E[X]\) for which \(x_{e}>0\) (and hence \(e\in T\)), it holds that the corresponding constraint in (DLP) is tight with respect to \(\hat{y}\) (and also \(y\)). This implies that \(e\in E_{y}\). On the other hand, for any \(Z\subset X\), if \(\hat{y}_{Z}>0\), and hence \(X\in\mathcal{F}\), complementary slackness prescribes that the corresponding primal constraint is tight, and hence \(|T\cap\delta^{+}_{G[X]}(Z)|=1\), concluding the proof. 

For the proof of the next theorem, we first explain how to compute the absorbing probabilities of an absorbing Markov chain \((G,P)\) and show a related lemma that we need in Appendix C. W.l.o.g. we assume that the states \(V(G)\) are ordered such that the non-absorbing states \(N\) come first and the absorbing states \(S\) last. We can then write the transition matrix as

\[P=\begin{bmatrix}D&C\\ 0&I_{|S|}\end{bmatrix}\quad,\]

where \(D\) is the \(|N|\times|N|\) transition matrix from non-absorbing states to non-absorbing states and \(C\) is the \(|N|\times|S|\) transition matrix from non-absorbing states to absorbing states. \(I_{|S|}\) denotes the \(|S|\times|S|\) identity matrix. The absorbing probability of an absorbing state \(s\in S\), when starting a random walk in a state \(v\in N\) is then given as the entry in the row corresponding to \(v\) and the column corresponding to \(s\) in the \(|N|\times|S|\) matrix \((I_{|N|}-D)^{-1}C\)(Grinstead and Snell, 1997).

[MISSING_PAGE_FAIL:16]

Now, let \(X=N\cup S\), i.e., we are in the last iteration of the algorithm. Due to an analogous reasoning as before, there is a many-to-one mapping from the min-cost branchings in \(G\) to branchings in \(G_{X}\). More precisely, for every branching \(B\in\mathcal{B}_{Y,\{s\}}(G_{X})\), there exist

\[\prod_{(Y,Y^{\prime})\in B}w_{X}(Y,Y^{\prime})=w_{X}(B)\]

branchings in \(G\) that map to \(B\). Hence, by the Markov chain tree theorem (Lemma 3), we get

\[A_{v,s}=Q_{v,s}=\frac{\sum_{B\in\mathcal{B}_{Y_{v},\{s\}}(G_{X})}w_{X}(B)}{\sum _{B\in\mathcal{B}(G_{X})}w_{X}(B)}=\frac{\sum_{B\in\mathcal{B}^{*}_{v,s}(G)}1} {\sum_{B\in\mathcal{B}^{*}(G)}1}\quad,\]

where \((G^{\prime}_{X},P)\) is the Markov chain corresponding to \(G_{X}\) and \(Q=\lim_{\tau\to\infty}\frac{1}{\tau}\sum_{i=0}^{\tau}P^{\tau}\). This equals the definition of Mixed Borda Branching.

Lastly, we argue about the running time of the algorithm. For a given delegation graph \((G,c)\), let \(n=V(G)\), i.e., the number of voters. Algorithm 1 can be implemented in \(\mathcal{O}(n^{3})\). That is because, the while loop runs for \(\mathcal{O}(n)\) iterations (the laminar set family \(\mathcal{F}\) can have at most \(2n-1\) elements), and finding all strongly connected components in a graph can be done in \(\mathcal{O}(n^{2})\) (e.g., with Kosaraju's algorithm [Hopcroft et al., 1983]). Coming back to the running time of Algorithm 2, we note that the do-while loop runs for \(\mathcal{O}(n)\) iterations, again, due to the size of \(\mathcal{F}^{\prime}\). In line 7, the algorithm computes \(\mathcal{O}(n)\) times the number of weighted spanning trees with the help of Lemma 1 (Tutte [1948]). Hence, the task is reduced to calculating the determinant of a submatrix of the laplacian matrix. Computing an integer determinant can be done in polynomial time in \(n\) and \(\log(M)\), if \(M\) is an upper bound of all absolute values of the matrix12. Note, that all values in every Laplacian (the out-degrees on the diagonals and the multiplicities in the other entries) as well as the results of the computation are upper-bounded by the total number of branchings in the original graph \(G\) (this follows from our argumentation about the interpretation of \(t_{X}(v)\) in the proof of Theorem 4), hence in particular by \(n^{n}\). Therefore, the running time of each iteration of the do-while loop is polynomial in \(n\). In the final step we compute the absorbing probabilities of the (scaled down version) of the weighted graph \((G_{X},w_{X})\) (where \(X=N\cup S\)). For that, we need to compute the inverse of a \(\mathcal{O}(n)\times\mathcal{O}(n)\) matrix, which can be done using the determinant and the adjuate of the matrix. Computing these comes down to computing \(\mathcal{O}(n^{2})\) determinants, for which we argued before that it is possible in polynomial time13. Summarizing, this gives us a running time of Algorithm 1 in \(\mathcal{O}((n^{7}\log(n)+n^{4}\log(n\log(n)))*(\log^{2}n+(\log(n\log(n)))^{2}))\). 

Footnote 12: More precisely, it can be computed in \(\mathcal{O}((n^{4}\log(nM)+n^{3}\log^{2}(nM))*(\log^{2}n+(\log\log M)^{2}))\)[Gathen and Gerhard, 2013]

Footnote 13: We argued this only for integer matrices, but we can transform the rational matrix into an integer one by scaling it up by a factor which is bounded by \(n^{n}\).

## Appendix B Further Remarks on Section 6

Alternative Interpretation of Algorithm 2We stated Algorithm 2 in terms of counting min-cost branchings. There exists a second natural interpretation that is closer to the definition of the Random Walk Rule, in which we want to compute the limit of the absorbing probabilities of a parametric Markov chain. We give some intuition on this reinterpretation of the algorithm with the example in Figure 2, and later extend this interpretation to a larger class of parametric Markov chains.

Intuitively speaking, every set \(X\in\mathcal{F}\) in the Markov chain \((G,P^{(\varepsilon)})\) corresponding to the delegation graph \(G\) is a strongly connected component whose outgoing edges have an infinitely lower probability than the edges inside of \(X\) as \(\varepsilon\) approaches zero. We are therefore interested in the behavior of an infinite random walk in \(G[X]\). While in the branching interpretation, the node weight \(t_{X}(v)\) can be interpreted as the number of min-cost \(v\)-arborescences in \(G[X]\), in the Markov chain interpretation we think of \(t_{X}(v)\) as an indicator of the relative time an infinite random walk spends in \(v\) (or the relative number of times \(v\) is visited) in the Markov chain given by the strongly connected graph \(G[X]\). Consider the example iteration depicted in Figure 1(a), where we are given an unprocessed \(X\in\mathcal{F}\) whose children \(Y_{1},Y_{2}\) are all processed. When contracting \(Y_{1}\) and \(Y_{2}\) the weights on the edges should encode how likely a transition is from one set to another, which is achieved by summing over the relative time spent in each node with a corresponding edge. We then translate the resulting graph (Figure 1(b)) into a Markov chain and again compute the relative time spend in each node. This computation is equivalent to calculating the sum of weights of all in-trees (up to a scaling factor, see Theorem 3). Indeed, we get a ratio of one to three for the time spend in \(Y_{1}\) and \(Y_{2}\). To compute \(t_{X}(v)\) we multiply the known weight \(t_{Y_{e}}(v)\) by the newly calculated weight of \(Y_{v}\). In the example this means that since we know, we spend three times as much time in \(Y_{2}\) as in \(Y_{1}\) all weights of nodes in \(Y_{2}\) should be multiplied by three (see Figure 1(c)).

Extension of Algorithm 2In addition, we remark that our algorithm could be extended to a larger class of parametric Markov chains, namely, to all Markov chains \((G,P^{(\varepsilon)})\), where \(G\) is a graph in which every node has a path to some sink node, and, for every \(e\in E(G)\), \(P^{(\varepsilon)}_{e}\) is some rational fraction in \(\varepsilon\), i.e., \(\frac{f_{e}(e)}{g_{e}(e)}\), where both \(f_{e}\) and \(g_{e}\) are polynomials in \(\varepsilon\) with positive coefficients.14 Now, we can construct a cost function \(c\) on \(G\), by setting \(c(e)=x_{e}-z_{e}+1\), where \(x_{e}\) is the smallest exponent in \(f_{e}(\varepsilon)\) and \(z_{e}\) is the smallest exponent in \(g_{e}(\varepsilon)\). Note that, if \(c(e)<1\), then the Markov chain cannot be well defined for all \(\varepsilon\in(0,1]\). Now, we run Algorithm 2 for the delegation graph \((G,c)\) with the only one difference, i.e., the weight function \(w_{X}\) also has to incorporate the coefficients of the polynomials \(f_{e}(\varepsilon)\) and \(g_{e}(\varepsilon)\). More precisely, we define for every \(e\in E\), the number \(q_{e}\) as the ratio between the coefficient corresponding to the smallest exponent in \(f_{e}\) and the coefficient corresponding to the smallest exponent in \(g_{e}\). Then, we redefine line 4 in the algorithm to be

Footnote 14: This class is reminiscent of a class of parametric Markov chains studied by Hahn et al. (2011).

\[w_{X}(Y,Y^{\prime})\leftarrow\sum_{(u,v)\in E_{y}\cap(Y\times Y^{\prime})}t_{Y} (u)\cdot q_{(u,v)}.\]

One can then verify with the same techniques as in Section 5 and Section 6, that this algorithm returns the outcome of the above defined class of Markov chains.

## Appendix C Missing Proofs and Further Results of Section 7

**Theorem 6** (\(\bigstar\)).: _The Random Walk Rule satisfies anonymity._

Proof.: Given a delegation graph \((G,c)\) and a bijection \(\sigma:V(G)\to V(G)\), we know that for all \(v\in V(G)\) it holds that \(|\delta^{\leftarrow}_{G}(v)|=|\delta^{\leftarrow}_{G^{\prime}}(\sigma(v))|\) and \(c(v,w)=c^{\prime}(\sigma(v),\sigma(w))\) for any edge \((v,w)\in\delta^{+}(v)\), where \((G^{\prime},c^{\prime})=\sigma((G,c))\). In the corresponding Markov chains \(M_{\varepsilon}\) and \(M^{\prime}_{\varepsilon}\) we therefore get \(P^{(\varepsilon)}_{(v,w)}=P^{\prime(\varepsilon)}_{(\sigma(v),\sigma(w))}\) (see Equation 1). Since through the bijection between the edges of the graph, we also get a bijection between all walks in the graph \(\mathcal{W}\) and for every \(s\in S\) and walk in \(\mathcal{W}[s,v]\) there is a corresponding walk in \(\mathcal{W}[\sigma(v),\sigma(s)]\) of the same probability. Therefore we have

\[A_{v,s}=\lim_{\varepsilon\to 0}\sum_{W\in\mathcal{W}[v,s]}\prod_{e\in W}P^{( \varepsilon)}_{e}=\lim_{\varepsilon\to 0}\sum_{W\in\mathcal{W}[\sigma(v), \sigma(s)]}\prod_{e\in W}P^{\prime(\varepsilon)}_{e}=A^{\prime}_{\sigma(v), \sigma(s)}\quad,probabilities on the edges of \(G_{X}\) are strictly positive as well. This gives \(\{s\}\) a strictly positive absorbing probability when starting a random walk in \(Y_{v}\) exactly if \(s\) is reachable from \(v\) in \(G_{y}\).

Our next observation is that \(\hat{\mathcal{F}}=\mathcal{F}\setminus\{Y\in\mathcal{F}\mid v\in Y\}\cup\{\{v\}\}\), \(\hat{y}(\{v\})=1\) and \(y(Y)=\hat{y}(Y)\) for all \(Y\in\hat{\mathcal{F}}\setminus\{\{v\}\}\). Consider the computation of \(\mathcal{F}\) in Algorithm 1. Since the output is unique (see Lemma 2 statement (i)), we can assume without loss of generality that after initializing \(\mathcal{F}\), all sets in \(\{Y\in\mathcal{F}\mid v\notin Y\}\) are added to \(\mathcal{F}\) first and then the remaining sets \(\{Y\in\mathcal{F}\mid v\in Y\}\). In \(\hat{G}\), the only edges missing are the outgoing edges from \(v\), therefore, when applying Algorithm 1 to \(\hat{G}\) all sets in \(\{Y\in\mathcal{F}\mid v\notin Y\}\) can be added to \(\hat{\mathcal{F}}\) first (with \(\hat{y}(Y)=y(Y)\)). Note, that the set \(\{v\}\) with \(y(\{v\})=1\) was added to \(\hat{\mathcal{F}}\) in the initialization. We claim, that the algorithm terminates at that point. Suppose not, then there must be another strongly connected component \(X\subseteq N\) with \(\delta^{+}(X)\cap E_{\hat{y}}=\emptyset\). If \(v\in X\) then since \(v\) has no outgoing edges \(X=\{v\}\), which is already in \(\mathcal{F}\). If \(v\notin X\) then \(X\) would have already been added.

With these two observations, we can show the following claim: For every casting voter \(s\in S\setminus S_{v}\) the voting weight remains equal, when \(v\) turns into a casting voter, i.e., \(\pi_{s}(A)=\pi_{s}(\hat{A})\). Fix \(s\in S\setminus S_{v}\) and let \(U\subset N\) be the set of nodes not reachable from \(v\) in \(G_{y}\). We know that \(\hat{\mathcal{F}}=\mathcal{F}\setminus\{Y\in\mathcal{F}\mid v\in Y\}\cup\{v\}\), which implies that for every node \(u\in U\) the sets containing \(u\) are equal in \(\mathcal{F}\) and \(\hat{\mathcal{F}}\), i.e., \(\{Y\in\mathcal{F}\mid u\in Y\}=\{Y\in\hat{\mathcal{F}}\mid u\in Y\}\). Therefore, the outgoing edges from any \(u\in U\) are equal in \(G_{y}\) and \(\hat{G}_{\hat{y}}\). Since \(\hat{\mathcal{F}}\subseteq\mathcal{F}\), the edges in \(\hat{G}_{\hat{y}}\) are a subset of the edges in \(G_{y}\) and therefore the set \(U\) is not reachable from \(v\) in \(\hat{G}_{\hat{y}}\). When translating \(\hat{G}_{\hat{y}}\) into the Markov chain \((\hat{G}_{\hat{y}},\hat{P}^{(\varepsilon)})\) (see Equation 1), we get for the probability of any tight out-edge \(e\) of \(u\) and any \(\varepsilon>0\), that \(P^{(\varepsilon)}_{e}=\hat{P}^{(\varepsilon)}_{e}\), where \(P^{(\varepsilon)}\) is the transition matrix induced by the original graph \(G_{y}\). In the following we argue about the set of walks in \(G_{y}\) and \(G_{\hat{y}}\). To this end we define for every \(u\in N\), the set \(\mathcal{W}[u,s]\) (\(\hat{\mathcal{W}}[u,s]\), respectively) as the set of walks in \(G_{y}\) (in \(G_{\hat{y}}\), respectively) that start in \(u\) and end in sink \(s\). Since all walks from any \(u\in U\) to \(s\) contain only outgoing edges from nodes in \(U\), we have \(\hat{\mathcal{W}}[u,s]=\mathcal{W}[u,s]\). For any other voter \(w\in N\setminus U\) we have \(\hat{\mathcal{W}}[w,s]=\mathcal{W}[w,s]=\emptyset\) and therefore

\[\pi_{s}(\hat{A})=1+\sum_{u\in U}\lim_{\varepsilon\to 0}\sum_{\hat{W}\in \hat{\mathcal{W}}[u,s]}\prod_{e\in W}P^{(\varepsilon)}_{e}=1+\sum_{u\in U} \lim_{\varepsilon\to 0}\sum_{W\in\mathcal{W}[u,s]}\prod_{e\in W}P^{( \varepsilon)}_{e}=\pi_{s}(A)\quad,\]

which concludes the proof of the claim.

Summarizing, we know that that for any casting voter \(s\in S\setminus S_{v}\) we have \(\pi_{s}(A)=\pi_{s}(\hat{A})\), which directly implies that \(\sum_{s\in S_{v}}\pi_{s}(A)=\pi_{v}(\hat{A})+\sum_{s\in S_{v}}\pi_{s}(\hat{A})\). 

**Theorem 8** (\(\bigstar\)).: _The Random Walk Rule satisfies confluence._

_Proof._ Before proving the claim, we introduce notation. For any walk \(W\) in some graph \(G\), and some node \(v\in V(G)\), we define \(W[v]\) to be the subwalk of \(W\) that starts at the first occasion of \(v\) in \(W\). For two nodes \(u,v\in V(G)\), we define \(W[u,v]\) to be the subwalk of \(W\) that starts at the first occasion of \(u\) and ends at the first occasion of \(v\). (Note that \(W[v]\) and \(W[u,v]\) might be empty.) Now, for a set of walks \(\mathcal{W}\) and \(u,v,s\in V(G)\), we define \(\mathcal{W}[v]=\{W[v]\mid W\in\mathcal{W}\}\) and \(\mathcal{W}[u,v]=\{W[u,v]\mid W\in\mathcal{W}\}\). Lastly, we define \(\mathcal{W}[u,v,s]=\{W\in W[u,s]\mid v\in W[u,s]\}\). We usually interpret a walk \(W\) as a sequence of nodes. In order to facilitate notation, we abuse notation and write \(v\in W\) for some node \(v\in V(G)\) in order to indicate that \(v\) appears in \(W\), and for an edge \(e\in E(G)\), we write \(e\in W\) to indicate that tail and head of \(e\) appear consecutively in \(W\).

For the remainder of the proof we fix \(\mathcal{W}\) to be the set of walks in the input delegation graph \(G\) starting in some node from \(N\) and ending in some sink node \(S\). Moreover, let \(G_{X}\) be the graph at the end of Algorithm 2, i.e., \(G_{X}\) for \(X=N\cup S\). We fix \(\hat{\mathcal{W}}\) to be the set of walks which start in some node of \(G_{X}\) and end in some sink node of \(G_{X}\) (which are exactly the nodes in \(\{\{s\}\mid s\in S\}\)).

In the following, we define for every \(v\in N\) a probability distribution \(f_{v}:\mathcal{W}[v]\rightarrow[0,1]\), such that it witnesses the fact that the Random Walk Rule is confluent. To this end, we define a mapping \(\gamma_{v}:\hat{\mathcal{W}}[Y_{v}]\rightarrow\mathcal{W}[v]\), where \(Y_{v}\) is the node in \(G_{X}\) that contains \(v\). Given a walk \(\hat{W}\in\hat{\mathcal{W}}[Y_{v}]\), we construct \(\gamma_{v}(\hat{W})\in\mathcal{W}[v]\) as follows: Let \(\hat{W}=Y^{(1)},\ldots Y^{(k)}\). By construction of \(G_{X}\) we know that for every \(i\in\{1,\ldots,k\}\), the fact that \((Y^{(i)},Y^{(i+1)})\in E_{X}\) implies that there exists \((b^{(i)},a^{(i+1)})\in E\) with \(b^{(i)}\in Y^{(i)}\) and \(a^{(i+1)}\in Y^{(i+1)}\). Moreover, we define \(a^{(1)}=v\) and \(b^{(n)}=s\)

[MISSING_PAGE_FAIL:20]

This concludes the proof. 

The next axiom was in its essence first introduced by Behrens and Swierczek (2015) and first given the name _guru-participation_ in Kotsialou and Riley (2020). The idea is that a representative (the _guru_) of a voter, should not be worse off if said voter abstains from the vote. Brill et al. (2022) define this property for non-fractional ranked delegations by requiring that any casting voter that was not a representative of the newly abstaining voter should not loose voting weight. This definition translates well into the setting of fractional delegations where we can have multiple representatives per voter. For simplicity, we made a slight modification to the definition15, resulting in a slightly stronger axiom.

Footnote 15: More specifically, Brill et al. (2022) use the notion of _relative_ voting weight between the casting voters in the definition of the axiom, which follows from our version of the axiom using absolute voting weight.

Previously, we stated the general assumption that every delegating voter in a delegation graph \((G,c)\) has a path to some casting voter in \(G\). In this section we modify given delegation graphs by removing nodes or edges, which may result in an invalid delegation graph not satisfying this assumption. To prevent this, we implicitly assume that after modifying a delegation graph, all nodes in \(N\) not connected to any sink in \(S\) (we call them _isolated_) are removed from the graph.

**Guru Participation:** A delegation rule satisfies _guru-participation_ if the following holds for every instance \((G,c)\): Let \((\hat{G},c)\) be the instance derived from \((G,c)\) by removing a node \(v\in N\) (and all newly isolated nodes), let \(S_{v}=\{s\in S\mid A_{v,s}>0\}\) be the set of representatives of \(v\) and let \(A\) and \(\hat{A}\) be the assignments returned by the delegation rule for \((G,c)\) and \((\hat{G},c)\), respectively. Then

\[\pi_{s}(\hat{A})\geq\pi_{s}(A)\quad\forall s\in S\setminus S_{v}\quad.\]

In particular, this implies

\[\sum_{s\in S_{v}}\pi_{s}(\hat{A})+1\leq\sum_{s\in S_{v}}\pi_{s}(A)\quad.\]

In order to prove that the Random Walk Rule satisfies guru-participation we first show the following lemma, saying that the voting weight of no casting voter decreases, when the in-edges of another casting voter are removed from the graph.

**Lemma C.1**.: _For the Random Walk Rule, removing the incoming edges of some casting voter \(s\in S\) (and all newly isolated voters) does not decrease the absolute voting weight of any casting voter \(s^{\prime}\in S\setminus\{s\}\)._

Proof.: Let \((G,c)\) be a delegation graph and \(s\in S\) a sink. Let \((\hat{G},c)\) be the delegation graph, where the in-edges of \(s\) and all voters disconnected from casting voters are removed. Let \(P^{(\varepsilon)}\) and \(\hat{P}^{(\varepsilon)}\) be the transition matrices of the corresponding Markov chains \(M_{\varepsilon}\) and \(\hat{M}_{\varepsilon}\). Then, for any \(\varepsilon>0\) and edge \(e\) in \(\hat{G}\) we have \(P^{(\varepsilon)}_{e}\leq\hat{P}^{(\varepsilon)}_{e}\). Since no edge on a path from any \(v\in N\) to any \(s^{\prime}\in S\setminus\{s\}\) was removed, we have \(\hat{\mathcal{W}}[v,s^{\prime}]=\mathcal{W}[v,s^{\prime}]\) and \(\hat{P}^{(\varepsilon)}_{e}\geq P^{(\varepsilon)}_{e}\) for every edge \(e\) in \(\hat{G}\) and \(\varepsilon>0\). Therefore, for the absolute voting weight of any \(s^{\prime}\in S\setminus\{s\}\) in \(\hat{G}\) we get

\[\pi_{s^{\prime}}(\hat{A})=1+\sum_{v\in N}\lim_{\varepsilon\to 0}\sum_{\hat{W} \in\hat{\mathcal{W}}[v,s^{\prime}]}\prod_{e\in\hat{W}}P^{(\varepsilon)}_{e} \geq 1+\sum_{v\in N}\lim_{\varepsilon\to 0}\sum_{W\in\mathcal{W}[v,s^{ \prime}]}\prod_{e\in W}P^{(\varepsilon)}_{e}=\pi_{s^{\prime}}(A)\quad,\]

which concludes the proof. 

Using Lemma C.1 and the proof of Theorem 7, we can show that guru-participation is satisfied by the Random Walk Rule by removing a delegating voter step by step.

**Theorem C.2**.: _The Random Walk Rule satisfies guru participation._

Proof.: Let \((G,c)\) be a delegation graph and \(v\in N\) a delegating voter. We remove \(v\) from \(G\) in three steps. First, we remove all out-edges of \(v\), making \(v\) a casting voter and call the new delegation graph \((\hat{G}_{1},c)\). Then we remove the in-edges of \(v\) (and all newly isolated voters) and get \((\hat{G}_{2},c)\). Finally, we remove \(v\) itself to retrieve \((\hat{G},c)\) as in the definition of guru-participation. Let \(A\), \(\hat{A}_{1}\), \(\hat{A}_{2}\) and \(\hat{A}\) be the assignments returned by the Random Walk Rule for \((g,c)\), \((\hat{G}_{1},c)\), \((\hat{G}_{2},c)\) and \((\hat{G},c)\), respectively. From the proof of Theorem 7 we know that for every casting voter \(s\in S\setminus S_{v}\) the voting weight in the instances \((G,c)\) and \((\hat{G}_{1},c)\) is equal, i.e., \(\pi_{s}(\hat{A}_{1})=\pi_{s}(A)\). From Lemma C.1 it follows that the voting weight of these voters can only increase if also the in-edges of \(v\) are removed, i.e., \(\pi_{s}(\hat{A}_{2})\geq\pi_{s}(\hat{A}_{1})\). Finally, removing the now completely isolated (now casting) voter \(v\) does not change the absolute voting weight of any other voter and therefore \(\pi_{s}(\hat{A})\geq\pi_{s}(A)\). 

### Relation to the Axioms of Brill et al. (2022)

First, we remark that the definition of a non-fractional delegation rule varies slightly from the definition of a delegation rule in Brill et al. (2022). That is, Brill et al. (2022) define the output of a delegation rule as a mapping from each delegating voter to some _path_ to a casting voter. Here, on the other hand, we define the output of a non-fractional delegation rule as a (non-fractional) assignment of delegating voters to casting voters. Hence, the definition of a delegation rule by Brill et al. (2022) is slightly more restrictive than our definition, hence, ceteris paribus, the impossibility result holds in particular for the smaller set of delegation rules. In the following, we refer to our definition as non-fractional delegation rules and to the definition of Brill et al. (2022) as non-fractional* delegation rules. We say that a non-fractional delegation rule is _consistent_ to a non-fractional* delegation rule if, for any input, the assignment in the former corresponds to the induced assignment in the latter.

Copy-robustnessNext, consider the copy-robustness axioms. The axiom by Brill et al. (2022) for non-fractional* delegation rules differs to our copy-robustness axiom restricted to non-fractional delegation rules in two technicalities: First, Brill et al. (2022) consider the relative voting weight instead of the absolute voting weight. However, since the number of voters does not change from \((G,c)\) to \((\hat{G},c)\), this does not change the axiom. The other difference is that their axiom requires that the delegating voter \(v\) under consideration has a direct path (in the output of the non-fractional* delegation rule) to its assigned casting voter. Since a non-fractional delegation rule only outputs an assignment and no path, we relaxed this assumption. Hence, our copy-robustness axiom is slightly stronger than the one presented by Brill et al. (2022). Nevertheless, it is easy to see that also the weaker version of the axiom is necessarily violated within the proof of the impossibility theorem when we utilize the definition of a delegation rule by Brill et al. (2022).

ConfluenceLastly, consider the confluence axiom. Brill et al. (2022) define their confluence axiom, which we denote by _confluence*_ as follows: A non-fractional* delegation rule satisfies confluence* if, for every delegating voter \(v\in N\) exactly one outgoing edge of \(v\) appears within the union of paths returned by the delegation rule. In particular, this is equivalent to the fact that the union of the returned paths forms a branching in the delegation graph. We prove below that the two axioms are in fact equivalent (within the restricted domain of non-fractional delegation rules).

**Proposition 1**.: _A non-fractional* delegation rule satisfies confluence* if and only if there exists a consistent non-fractional delegation rule that satisfies confluence._

Proof.: We start by proving the forward direction. Consider a non-fractional* delegation rule that satisfies confluence*. We define a consistent non-fractional delegation rule by simply returning the assignment induced by the returned paths instead of the paths. For showing that the rule satisfies confluence, we define the probability distributions \(f_{v},v\in N\) by setting \(f_{v}(W)=1\) if and only if \(W\) is the path returned for \(v\) by the delegation rule. All other probabilities are set to zero. Then, confluence* directly implies that the distributions \(f_{v},v\in N\) witness confluence.

For the other direction, consider a non-fractional delegation rule satisfying confluence, and let \(f_{v},v\in N\) be the probability distributions that witness this fact. Building upon that, we define a branching in \(G\) that is consistent with the outcome of the delegation rule. Interpreting this branching as the output of a non-fractional* delegation rule then proves the claim. We construct the branching as follows: We first set \(B=\emptyset\). In the beginning, set all delegating voters to be "active", while all casting voters are "inactive". Now, pick some arbitrary active voter \(v\in N\) and consider some arbitrary walk \(W\) that obtains non-zero probability by the distribution \(f_{v}\). Construct a path \(P\) from \(W\) by cutting the walk in the first appearance of some inactive voter, and then short cutting all remaining cycles (if existent). Now, add all edges in \(P\) to \(B\) and set all delegating voters on \(P\) to be "inactive". Note that, by confluence, for each newly inactive voter \(u\in N\) it holds that the casting voter assigned by the delegation rule corresponds to the sink node at the end of the unique maximal path in \(B\) starting from \(u\). We continue this process until all voters are inactive. As a result, we created a branching that is consistent with the original delegation rule, hence, there exists a consistent non-fractional* delegation rule that implicitly returns branchings, i.e., is confluent. 

## Appendix D Broader Impact

We are aware of the fact that any delegation rule, and in particular the one suggested in this paper, may be implemented in a liquid democracy system and could thereby have real world impact. In this paper, we chose the axiomatic method in order to evaluate the suggested rule in a principled way. While, with respect to the axioms considered in the literature so far, our delegation rule performs very well, we want to point out that this is the very first paper introducing fractional delegation rules for ranked delegations. In particular, there is a risk of some unforeseen disadvantages of the rule that could possibly be used for manipulations or lead to other negative societal effects. Therefore, we think that further theoretical and also empirical research is necessary before recommending our suggested delegation rule for (high-stake) real-world decision making.