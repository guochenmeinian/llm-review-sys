# Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances

 Mikhail Khodak

CMU

khodak@cmu.edu

&Edmond Chow

Georgia Tech.

&Maria-Florina Balcan

CMU

&Ameet Talwalkar

CMU

###### Abstract

Solving a linear system \(\mathbf{A}\mathbf{x}=\mathbf{b}\) is a fundamental scientific computing primitive for which numerous solvers and preconditioners have been developed. These come with parameters whose optimal values depend on the system being solved and are often impossible or too expensive to identify; thus in practice sub-optimal heuristics are used. We consider the common setting in which many related linear systems need to be solved, e.g. during a single numerical simulation. In this scenario, can we sequentially choose parameters that attain a near-optimal overall number of iterations, without extra matrix computations? We answer in the affirmative for Successive Over-Relaxation (SOR), a standard solver whose parameter \(\omega\) has a strong impact on its runtime. For this method, we prove that a bandit online learning algorithm--using only the number of iterations as feedback--can select parameters for a sequence of instances such that the overall cost approaches that of the best fixed \(\omega\) as the sequence length increases. Furthermore, when given additional structural information, we show that a _contextual_ bandit method asymptotically achieves the performance of the _instance-optimal_ policy, which selects the best \(\omega\) for each instance. Our work provides the first learning-theoretic treatment of high-precision linear system solvers and the first end-to-end guarantees for data-driven scientific computing, demonstrating theoretically the potential to speed up numerical methods using well-understood learning algorithms.

## 1 Introduction

The bottleneck subroutine in many science and engineering computations is a solver that returns a vector \(\mathbf{x}\) approximating the solution \(\mathbf{A}^{-1}\mathbf{b}\) to a linear system. A prominent example is in partial differential equations (PDEs), whose solutions often involve solving sequences of high-dimensional linear systems, often to very high precision [58]. As a result, a vast array of solvers and preconditioners have been developed, many of which have tunable parameters; these can have a strong, quantifiable effect on runtime, e.g. via their impact on condition numbers or the spectral radius of an iteration matrix [30, 32]. There is a long literature analyzing these algorithms, and indeed for some problems we have a strong understanding of the optimal parameters for a given matrix. However, computing them can sometimes be more costly than solving the original system, leading to an assortment of heuristics for setting good parameters [24, 29].

Our goal will be to provide an alternative to (possibly suboptimal) heuristics by taking advantage of the fact that, in practice, we often solve many linear systems at a time. A natural approach is to treat these instances as data to be passed to a machine learning (ML) algorithm; in particular, due to the sequential nature of many scientific computing tasks, the framework of online learning [17] provides a natural language for reasoning about it. For example, if we otherwise would solve a sequence of \(T\) linear systems \((\mathbf{A}_{1},\mathbf{b}_{1}),\ldots,(\mathbf{A}_{T},\mathbf{b}_{T})\) using a given solver with a fixed parameter, can we use ML to do as well as the best choice of that parameter, i.e. can we _minimize regret_? Or, if the matrices are all diagonal shifts of single matrix \(\mathbf{A}\), can we learn the functional relationship between the shift \(c_{t}\) and the optimal solver parameter for \(\mathbf{A}_{t}=\mathbf{A}+c_{t}\mathbf{I}_{n}\), i.e. can we predict using _context_?We investigate these questions for the Successive Over-Relaxation (SOR) solver, a generalization of Gauss-Seidel whose relaxation parameter \(\omega\in(0,2)\) dramatically affects the number of iterations (c.f. Figure 1, noting the log-scale). SOR is well-studied and often used as a preconditioner for Krylov methods (e.g. conjugate gradient (CG), as a basis for semi-iterative approaches, and as a multigrid smoother. Analogous to some past setups in data-driven algorithms [8; 36], we will sequentially set relaxation parameters \(\omega_{t}\) for SOR to use when solving each linear system \((\mathbf{A}_{t},\mathbf{b}_{t})\). Unlike past theoretical studies of related methods [31; 12; 11], we aim to provide _end-to-end_ guarantees--covering the full pipeline from data-intake to efficient learning to execution--while minimizing dependence on the dimension (\(n\) can be \(10^{6}\) or higher) and precision (\(1/\varepsilon\) can be \(10^{8}\) or higher). We emphasize that we do _not_ seek to immediately improve the empirical state of the art, and also that existing research on saving computation when solving sequences of linear systems (recycling Krylov subspaces, reusing preconditioners, etc.) is complementary to our own, i.e. it can be used in addition to the ideas presented here.

### Core contributions

We study two distinct theoretical settings, corresponding to views on the problem from two different approaches to data-driven algorithms. In the first we have a deterministic sequence of instances and study the spectral radius of the iteration matrix, the main quantity of interest in classical analysis of SOR [65]. We show how to convert its asymptotic guarantee into a surrogate loss that upper bounds the number of iterations via a quality measure of the chosen parameter, in the style of _algorithms with predictions_[49]. The bound holds under a _near-asymptotic_ condition implying that convergence occurs near the asymptotic regime, i.e. when the spectral radius of the iteration matrix governs the convergence. We verify the assumption and show that one can learn the surrogate losses using only bandit feedback from the original costs; notably, despite being non-Lipschitz, we take advantage of the losses' unimodal structure to match the optimal \(\tilde{\mathcal{O}}(T^{2/3})\) regret for Lipschitz bandits [38]. Our bound also depends only logarithmically on the precision and not at all on the dimension. Furthermore, we extend to the diagonally shifted setting described before, showing that an efficient, albeit pessimistic, contextual bandit (CB) method has \(\tilde{\mathcal{O}}(T^{3/4})\) regret w.r.t. the instance-optimal policy that always picks the best \(\omega_{t}\). Finally, we show a similar analysis of learning a relaxation parameter for the more popular (symmetric SOR-preconditioned) CG method.

Our second setting is _semi-stochastic_, with target vectors \(\mathbf{b}_{t}\) drawn i.i.d. from a (radially truncated) Gaussian. This is a reasonable simplification, as convergence usually depends more strongly on \(\mathbf{A}_{t}\), on which we make no extra assumptions. We show that the expected cost of running a symmetric variant of SOR (SSOR) is \(\mathcal{O}(\sqrt{n})\text{polylog}(\frac{n}{\varepsilon})\)-Lipschitz w.r.t. \(\omega\), so we can (a) compete with the optimal number of iterations--rather than with the best upper bound--and (b) analyze more practical, regression-based CB algorithms [26; 55]. We then show \(\tilde{\mathcal{O}}(\sqrt[3]{T^{2}\sqrt{n}})\) regret when comparing to the single best \(\omega\) and \(\tilde{\mathcal{O}}(T^{9/11}\sqrt{n})\) regret w.r.t. the instance-optimal policy in the diagonally shifted setting using a novel, Chebyshev regression-based CB algorithm. While the results do depend on the dimension \(n\), the dependence is much weaker than that of past work on data-driven tuning of a related regression problem [11].

**Remark 1.1**.: _Likely the most popular algorithms for linear systems are Krylov subspace methods such as CG. While an eventual aim of our line of work is to understand how to tune (many) parameters of (preconditioned) CG and other algorithms, SOR is a well-studied method and serves as a meaningful starting point. In fact, in Appendix A.3 we show that our near-asymptotic analysis extends directly, and in the semi-stochastic setting there is a natural path to (e.g.) SSOR-preconditioned CG, as it can be viewed as computing polynomials of iteration matrices where SSOR just takes powers. Lastly, apart from its use as a preconditioner and smoother, SOR is still sometimes preferred for direct use as well [27; 61; 37; 63; 64]._

### Technical and theoretical contributions

By studying a scientific computing problem through the lens of data-driven algorithms and online learning, we also make the following contributions to the latter two fields:

1. Ours is the first comparison of two leading theoretical approaches to data-driven algorithms applied to the same problem. While the algorithms with predictions [49] approach in Section 2 takes better advantage of the existing scientific computing literature to obtain (arguably) more interpretable and dimension-independent bounds, data-driven algorithm design [7] competes directly with the quantity of interest in Section 3 and enables provable guarantees for modern CB algorithms.
2. For algorithms with predictions, our approach of showing near-asymptotic performance bounds may be extendable to other iterative algorithms, as we demonstrate with CG. We also show that such performance bounds on a (partially-observable) cost function are learnable even when the bounds themselves are too expensive to compute.
3. In data-driven algorithm design, we take the novel approach of proving continuity of _the expectation of_ a discrete cost, rather than showing dispersion of its discontinuities [8] or bounding predicate complexity [12].
4. We introduce the idea of using CB to sequentially set instance-optimal algorithmic parameters.
5. We show that standard discretization-based bandit algorithms are optimal for sequences of adversarially chosen _semi-Lipschitz_ losses that generalize regular Lipschitz functions (c.f. Appendix C).
6. We introduce a new CB method that combines SquareCB [26] with Chebyshev polynomial regression to get sublinear regret on Lipschitz losses (c.f. Appendix D).

### Related work and comparisons

We discuss the existing literature on solving sequences of linear systems [51; 57; 25], work integrating ML with scientific computing to amortize cost [2; 4], and past theoretical studies of data-driven algorithms [31; 11] in Appendix A. For the latter we include a detailed comparison of the generalization implications of our work with the GJ framework [12]. Lastly, we address the baseline of approximating the spectral radius of the Jacobi iteration matrix.

## 2 Asymptotic analysis of learning the relaxation parameter

We start this section by going over the problem setup and the SOR solver. Then we consider the asymptotic analysis of the method to derive a reasonable performance upper bound to target as a surrogate loss for the true cost function. Finally, we prove and analyze online learning guarantees.

### Setup

At each step \(t=1,\ldots,T\) of (say) a numerical simulation we get a linear system instance, defined by a matrix-vector pair \((\mathbf{A}_{t},\mathbf{b}_{t})\in\mathbb{R}^{n\times n}\times\mathbb{R}^{n}\), and are asked for a vector \(\mathbf{x}\in\mathbb{R}^{n}\) such that the norm of its _residual_ or _defect_\(\mathbf{r}=\mathbf{b}_{t}-\mathbf{A}_{t}\mathbf{x}\) is small. For now we define "small" in a relative sense, specifically \(\|\mathbf{A}_{t}\mathbf{x}-\mathbf{b}_{t}\|_{2}\leq\varepsilon\|\mathbf{b}_{t }\|_{2}\) for some _tolerance_\(\varepsilon\in(0,1)\); note that when using an iterative method initialized at \(\mathbf{x}=\mathbf{0}_{n}\) this corresponds to reducing the residual by a factor \(1/\varepsilon\), which we call the _precision_. In applications it can be quite high, and so we will show results whose dependence on it is at worst logarithmic. To make the analysis tractable, we make two assumptions (for now) about the matrices \(\mathbf{A}\): they are symmetric positive-definite and consistently-ordered (c.f. Hackbusch [32], Definition 4.23). We emphasize that, while not necessary for convergence, both are standard in the analysis of SOR [65]; see Hackbusch [32; Criterion 4.24] for multiple settings where they holds.

To find a suitable \(\mathbf{x}\) for each instance in the sequence we apply Algorithm 3 (SOR), which at a high-level works by multiplying the current residual \(\mathbf{r}\) by the inverse of a matrix \(\mathbf{W}_{\omega}\)--derived from the diagonal \(\mathbf{D}\) and lower-triangular component \(\mathbf{L}\) of \(\mathbf{A}\)--and then adding the result to the current iterate \(\mathbf{x}\). Note that multiplication by \(\mathbf{W}_{\omega}^{-1}\) is efficient because \(\mathbf{W}_{\omega}\) is triangular. We will measure the cost of this algorithm by the number of iterations it takes to reach convergence, which we denote by \(\mathsf{SOR}(\mathbf{A},\mathbf{b},\omega)\), or \(\mathsf{SOR}_{t}(\omega)\) for short when it is run on the instance \((\mathbf{A}_{t},\mathbf{b}_{t})\). For simplicity, we will assume that the algorithm is always initialized at \(\mathbf{x}=\mathbf{0}_{n}\), and so the first residual is just \(\mathbf{b}\).

Having specified the computational setting, we now turn to the learning objective, which is to sequentially set the parameters \(\omega_{1},\ldots,\omega_{T}\) so as to minimize the total number of iterations:

\[\sum\nolimits_{t=1}^{T}\mathsf{SOR}_{t}(\omega_{t})=\sum\nolimits_{t=1}^{T} \mathsf{SOR}(\mathbf{A}_{t},\mathbf{b}_{t},\omega_{t}) \tag{1}\]

Figure 1: **Left:** comparison of different cost estimates. **Center-left:** mean performance of different parameters across forty instances of form \(\mathbf{A}+\frac{12\varepsilon\sim 3}{20}\mathbf{I}_{n}\), where \(c\sim\mathsf{Beta}(2,6)\). **Center-right:** the same but for \(c\sim\mathsf{Beta}(1/2,3/2)\), which is relatively higher-variance. In both cases the dashed line indicates instance-optimal performance, the matrix \(\mathbf{A}\) is a discrete Laplacian of a \(100\times 100\) square domain, and the targets \(\mathbf{b}\) are truncated Gaussians. **Right:** asymptocity as measured by the difference between the spectral norm at iteration \(k\) and the spectral radius, together with its upper bound \(\tau(1-\rho(\mathbf{C}_{\omega}))\).

To set \(\omega_{t}\) at some time \(t>1\), we allow the learning algorithm access to the costs \(\mathsf{SOR}_{s}(\omega_{s})\) incurred at the previous steps \(s=1,\ldots,t-1\); in the literature on online learning this is referred to as the _bandit_ or _partial feedback_ setting, to distinguish from the (easier, but unreasonable for us) _full information_ case where we have access to the cost function \(\mathsf{SOR}_{s}\) at every \(\omega\) in its domain.

Selecting the optimal \(\omega_{t}\) using no information about \(\mathbf{A}_{t}\) is impossible, so we must use a _comparator_ to obtain an achievable measure of performance. In online learning this is done by comparing the total cost incurred (1) to the counterfactual cost had we used a _single_, best-in-hindsight \(\omega\) at every timestep \(t\). We take the minimum over some domain \(\Omega\subset(0,2)\), as SOR diverges outside it. While in some settings we will compete with every \(\omega\in(0,2)\), we will often algorithmically use \([1,\omega_{\text{max}}]\) for some \(\omega_{\text{max}}<2\). The upper limit ensures a bound on the number of iterations--required by bandit algorithms--and the lower limit excludes \(\omega<1\), which is rarely used because theoretical convergence of vanilla SOR is worse there for realistic problems, e.g. those satisfying our assumptions.

This comparison-based approach for measuring performance is standard in online learning and effectively assumes a good \(\omega\in\Omega\) that does well-enough on all problems; in Figure 1 (center-left) we show that this is sometimes the case. However, the center-right plot in the same figure shows we might do better by using additional knowledge about the instance; in online learning this is termed a _context_ and there has been extensive development of contextual bandit algorithms that do as well as the best fixed policy mapping contexts to predictions. We will study an example of this in the _diagonally shifted_ setting, in which \(\mathbf{A}_{t}=\mathbf{A}+c_{t}\mathbf{I}_{n}\) for scalars \(c_{t}\in\mathbb{R}\); while mathematically simple, this is a well-motivated structural assumption in applications [28, 14, 13, 3, 62]. Furthermore, the same learning algorithms can also be extended to make use of other context information, e.g. rough spectral estimates.

### Establishing a surrogate upper bound

Our first goal is to solve \(T\) linear systems almost as fast as if we had used the best fixed \(\omega\in\Omega\). In online learning, this corresponds to minimizing _regret_, which for cost functions \(\ell_{t}:\Omega\mapsto\mathbb{R}\) is defined as

\[\operatorname{Regret}_{\Omega}(\{\ell_{t}\}_{t=1}^{T})=\sum\nolimits_{t=1}^{T }\ell_{t}(\omega_{t})-\min_{\omega\in\Omega}\sum\nolimits_{t=1}^{T}\ell_{t}( \omega) \tag{2}\]

In particular, since we can upper-bound the objective (1) by \(\operatorname{Regret}_{\Omega}(\{\mathsf{SOR}_{t}\}_{t=1}^{T})\) plus the optimal cost \(\min_{\omega\in\Omega}\sum_{t=1}^{T}\mathsf{SOR}_{t}(\omega)\), if we show that regret is _sublinear_ in \(T\) then the leading-order term in the upper bound corresponds to the cost incurred by the optimal fixed \(\omega\).

Many algorithms attaining sublinear regret under different conditions on the losses \(\ell_{t}\) have been developed [17, 16]. However, few handle losses with discontinuities--i.e. most algorithmic costs--and those that do (necessarily) need additional conditions on their locations [8, 9]. At the same time, numerical analysis often deals more directly with continuous asymptotic surrogates for cost, such as convergence rates. Taking inspiration from this, and from the algorithms with predictions idea of deriving surrogate loss functions for algorithmic costs [36], in this section we instead focus on finding _upper bounds_\(U_{t}\) on \(\mathsf{SOR}_{t}\) that are both (a) learnable and (b) reasonably tight in-practice. We can then aim for overall performance nearly as good as the optimal \(\omega\in\Omega\) as measured by these upper bounds:

\[\sum_{t=1}^{T}\mathsf{SOR}_{t}(\omega_{t})\leq\sum_{t=1}^{T}U_{t}(\omega_{t}) =\operatorname{Regret}_{\Omega}(\{U_{t}\}_{t=1}^{T})+\min_{\omega\in\Omega} \sum_{t=1}^{T}U_{t}(\omega)=o(T)+\min_{\omega\in\Omega}\sum_{t=1}^{T}U_{t}(\omega) \tag{3}\]

A natural approach to get a bound \(U_{t}\) is via the _defect reduction matrix_\(\mathbf{C}_{\omega}=\mathbf{I}_{n}-\mathbf{A}(\mathbf{D}/\omega+\mathbf{L})^{-1}\), so named because the residual at iteration \(k\) is equal to \(\mathbf{C}_{\omega}^{k}\mathbf{b}\) and \(\mathbf{b}\) is the first residual. Under our assumptions on \(\mathbf{A}\), Young [65] shows that the spectral radius \(\rho(\mathbf{C}_{\omega})\) of \(\mathbf{C}_{\omega}\) is a (nontrivial to compute) piecewise function of \(\omega\) with a unique minimum in \([1,2)\). Since we have error \(\|\mathbf{C}_{\omega}^{k}\mathbf{b}\|_{2}/\|\mathbf{b}\|_{2}\leq\|\mathbf{C}_{ \omega}^{k}\|_{2}\) at iteration \(k\), \(\rho(\mathbf{C}_{\omega})=\lim_{k\to\infty}\sqrt[k]{\|\mathbf{C}_{\omega}^{k}\| _{2}}\) asymptotically bounds how much the error is reduced at each step. It is thus often called the _asymptotic convergence rate_ and the number of iterations is said to be roughly bounded by \(\frac{-\log\varepsilon}{-\log\rho(\mathbf{C}_{\omega})}\) (e.g. Hackbusch [32, Equation 2.31b]). However, while it is tempting to use this as our upper bound \(U\), in fact it may not upper bound the number of iterations at all, since \(\mathbf{C}_{\omega}\) is not normal and so in-practice the iteration often goes through a transient phase where the residual norm first _increases_ before decreasing [60, Figure 25.6].

Thus we must either take a different approach or make some assumptions. Note that one can in-fact show an \(\omega\)-dependent, finite-time convergence bound for SOR via the energy norm [32, Corollary 3.45], but this can give rather loose upper bounds on the number of iterations (c.f. Figure 1 (left)). Instead, we make the following assumption, which roughly states that convergence always occurs _near_ the asymptotic regime, where nearness is measured by a parameter \(\tau\in(0,1)\):

**Assumption 2.1**.: _There exists \(\tau\in(0,1)\) s.t. \(\forall\ \omega\in\Omega\) the matrix \(\mathbf{C}_{\omega}=\mathbf{I}_{n}-\mathbf{A}(\mathbf{D}/\omega+\mathbf{L})^{-1}\) satisfies \(\|\mathbf{C}_{\omega}^{k}\|_{2}\leq(\rho(\mathbf{C}_{\omega})+\tau(1-\rho( \mathbf{C}_{\omega})))^{k}\) at \(k=\min_{\|\mathbf{C}_{\omega}^{k+1}\|_{2}<\varepsilon\|\mathbf{b}\|_{2}}i\)._

This effectively assumes an upper bound \(\rho(\mathbf{C}_{\omega})+\tau(1-\rho(\mathbf{C}_{\omega}))\) on the empirically observed convergence rate, which gives us a measure of the quality of each parameter \(\omega\) for the given instance \((\mathbf{A},\mathbf{b})\). Note that the specific form of the surrogate convergence rate was chosen both because it is convenient mathematically--it is a convex combination of 1 and the asymptotic rate \(\rho(\mathbf{C}_{\omega})\)--and because empirically we found the degree of "asymptocity" as measured by \(\|\mathbf{C}_{\omega}^{k}\|_{2}^{1/k}-\rho(\mathbf{C}_{\omega})\) for \(k\) right before convergence to vary reasonably similarly to a fraction of \(1-\rho(\mathbf{C}_{\omega})\) (c.f. Figure 1 (right)). This makes intuitive sense, as the parameters \(\omega\) for which convergence is fastest have the least time to reach the asymptotic regime. Finally, note that since \(\lim_{k\to\infty}\|\mathbf{C}_{\omega}^{k}\|_{2}^{1/k}=\rho(\mathbf{C}_{\omega})\), for every \(\gamma>0\) there always exists \(k^{\prime}\) s.t. \(\|\mathbf{C}_{\omega}^{k}\|_{2}\leq(\rho(\mathbf{C}_{\omega})+\gamma)^{k}\)\(\forall\ k\geq k^{\prime}\); therefore, since \(1-\rho(\mathbf{C}_{\omega})>0\), we view Assumption 2.1 not as a restriction on \(\mathbf{C}_{\omega}\) (and thus on \(\mathbf{A}\)), but rather as an assumption on \(\varepsilon\) and \(\mathbf{b}\). Specifically, the former should be small enough that \(\mathbf{C}_{\omega}^{t}\) reaches that asymptotic regime for some \(i\) before the criterion \(\|\mathbf{C}_{\omega}^{k}\|_{2}\leq\varepsilon\|\mathbf{b}\|_{2}\) is met; for similar reasons, the latter should not happen to be an eigenvector corresponding to a tiny eigenvalue of \(\mathbf{C}_{\omega}\) (c.f. Figure 2 (left)).

Having established this surrogate of the spectral radius, we can use it to obtain a reasonably tight upper bound \(U\) on the cost (c.f. Figure 1 (left)). Crucially for learning, we can also establish the following properties via the functional form of \(\rho(\mathbf{C}_{\omega})\) derived by Young [65]:

**Lemma 2.1**.: _Define \(U(\omega)=1+\frac{-\log\varepsilon}{-\log(\rho(\mathbf{C}_{\omega})+\tau(1- \rho(\mathbf{C}_{\omega})))}\), \(\alpha=\tau+(1-\tau)\max\{\beta^{2},\omega_{\text{max}}-1\}\), and \(\omega^{*}=1+\beta^{2}/(1+\sqrt{1-\beta^{2}})^{2}\), where \(\beta=\rho(\mathbf{I}_{n}-\mathbf{D}^{-1}\mathbf{A})\). Then the following holds:_

1. \(U\) _bounds the number of iterations and is itself bounded:_ \(\mathsf{SOR}(\mathbf{A},\mathbf{b},\omega)<U(\omega)\leq 1+\frac{-\log \varepsilon}{-\log\alpha}\)__
2. \(U\) _is decreasing towards_ \(\omega^{*}\)_, and_ \(\frac{-(1-\tau)\log\varepsilon}{\alpha\log^{2}\alpha}\)_-Lipschitz on_ \(\omega\geq\omega^{*}\) _if_ \(\tau\geq\frac{1}{e^{2}}\) _or_ \(\beta^{2}\geq\frac{4}{e^{2}}(1-\frac{1}{e^{2}})\)__

Lemma 2.1 introduces a quantity \(\alpha=\tau+(1-\tau)\max\{\beta^{2},\omega_{\text{max}}-1\}\) that appears in the upper bounds on \(U(\omega)\) and in its Lipschitz constant. This quantity will in some sense measure the difficulty of learning: if \(\alpha\) is close to 1 for many of the instances under consideration then learning will be harder. Crucially, all quantities in the result are spectral and do not depend on the dimensionality of the matrix.

### Performing as well as the best fixed \(\omega\)

Having shown these properties of \(U\), we now show that it is learnable via Tsallis-INF [1, 67], a bandit algorithm which at each instance \(t\) samples \(\omega_{t}\) from a discrete probability distribution over a grid of \(d\) relaxation parameters, runs SOR with \(\omega_{t}\) on the linear system \((\mathbf{A}_{t},\mathbf{b}_{t})\), and uses the number of iterations required \(\mathsf{SOR}_{t}(\omega_{t})\) as feedback to update the probability distribution over the grid. The scheme is described in full in Algorithm 1. Note that it is a relative of the simpler and more familiar Exp3 algorithm [5], but has a slightly better dependence on the grid size \(d\). In Theorem 2.1, we bound the cost of using the parameters \(\omega_{t}\) suggested by Tsallis-INF by the total cost of using the best fixed parameter \(\omega\in\Omega\) at all iterations--as measured by the surrogate bounds \(U_{t}\)--plus a term that increases sublinearly in \(T\) and a term that decreases in the size of the grid.

**Theorem 2.1**.: _Define \(\alpha_{t}=\tau_{t}+(1-\tau_{t})\max\{\beta_{t}^{2},\omega_{\text{max}}-1\}\), where \(\beta_{t}=\rho(\mathbf{I}_{n}-\mathbf{D}_{t}^{-1}\mathbf{A}_{t})\) and \(\tau_{t}\) is the minimal \(\tau\) satisfying Assumption 2.1 and the second part of Lemma 2.1. If we run Algorithm 1 using SOR initialized at \(\mathbf{x}=\mathbf{0}_{n}\) as the solver, \(\mathbf{g}_{[i]}=1+(\omega_{\text{max}}-1)\frac{i}{d}\) as the parameter grid, normalization \(K\geq\frac{-\log\varepsilon}{-\log\alpha_{\text{max}}}\) for \(\alpha_{\text{max}}=\max_{t}\alpha_{t}\), and step-size \(\eta=1/\sqrt{T}\) then the expected number of iterations is bounded as_

[MISSING_PAGE_EMPTY:6]

\(\mathbf{b}\) can be described as coming from a radially truncated normal distribution. Note also that the exact choice of truncation was done for convenience; any finite bound \(\geq n\) yields similar results.

We also make two other changes: (1) we study _symmetric_ SOR (SSOR) and (2) we use an absolute convergence criterion, i.e. \(\|\mathbf{r}_{k}\|_{2}\leq\varepsilon\), not \(\|\mathbf{r}_{k}\|_{2}\leq\varepsilon\|\mathbf{r}_{0}\|_{2}\). Symmetric SOR (c.f. Algorithm 8) is very similar to the original, except the linear system being solved at every step is now symmetric: \(\hat{\mathbf{W}}_{\omega}=\frac{\omega\omega}{2-\omega}\mathbf{W}_{\omega} \mathbf{D}^{-1}\mathbf{W}_{\omega}^{T}\). Note that the defect reduction matrix \(\tilde{\mathbf{C}}_{\omega}=\mathbf{I}_{n}-\mathbf{A}\hat{\mathbf{W}}_{\omega }^{-1}\) is still not normal, but it _is_ (non-orthogonally) similar to a symmetric matrix, \(\mathbf{A}^{-1/2}\tilde{\mathbf{C}}_{\omega}\mathbf{A}^{1/2}\). SSOR is twice as expensive per-iteration, but often converges in fewer steps, and is commonly used as a base method because of its spectral properties (e.g. by the Chebyshev semi-iteration, c.f. Hackbusch [32, Section 8.4.1]).

### Regularity of the expected cost function

We can then show that the expected cost \(\mathbb{E}_{\mathbf{b}}\texttt{SSOR}(\mathbf{A},\mathbf{b},\omega)\) is Lipschitz w.r.t. \(\omega\) (c.f. Corollary G.1). Our main idea is the observation that, whenever the error \(\|\tilde{\mathbf{C}}_{\omega}^{k}\mathbf{b}\|_{2}\) falls below the tolerance \(\varepsilon\), randomness should ensure that it does not fall so close to the threshold that the error \(\|\tilde{\mathbf{C}}_{\omega^{\prime}}^{k}\mathbf{b}\|_{2}\) of a nearby \(\omega^{\prime}\) is not also below \(\varepsilon\). Although clearly related to dispersion [8], here we study the behavior of a continuous function around a threshold, rather than the locations of the costs' discontinuities.

Our approach has two ingredients, the first being Lipschitzness of the error \(\|\tilde{\mathbf{C}}_{\omega}^{k}\mathbf{b}\|_{2}\) at each iteration \(k\) w.r.t. \(\omega\), which ensures \(\|\tilde{\mathbf{C}}_{\omega^{\prime}}^{k}\mathbf{b}\|_{2}\in(\varepsilon, \varepsilon+\mathcal{O}(|\omega-\omega^{\prime}|)]\) if \(\|\tilde{\mathbf{C}}_{\omega}^{k}\mathbf{b}\|_{2}\leq\varepsilon<\|\tilde{ \mathbf{C}}_{\omega^{\prime}}^{k}\mathbf{b}\|_{2}\). The second ingredient is anti-concentration, specifically that the probability that \(\|\tilde{\mathbf{C}}_{\omega}^{k}\mathbf{b}\|_{2}\) lands in \((\varepsilon,\varepsilon+\mathcal{O}(|\omega-\omega^{\prime}|)]\) is \(\mathcal{O}(|\omega-\omega^{\prime}|)\). While intuitive, both steps are made difficult by powering: for high \(k\) the random variable \(\|\tilde{\mathbf{C}}_{\omega}^{k}\mathbf{b}\|_{2}\) is highly concentrated because \(\rho(\tilde{\mathbf{C}}_{\omega})\ll 1\); in fact its measure over the interval is \(\mathcal{O}(|\omega-\omega^{\prime}|/\rho(\tilde{\mathbf{C}}_{\omega})^{k})\). To cancel this, the Lipschitz constant of \(\|\tilde{\mathbf{C}}_{\omega}^{k}\mathbf{b}\|_{2}\) must scale with \(\rho(\tilde{\mathbf{C}}_{\omega})^{k}\), which we can show because switching to SSOR makes \(\tilde{\mathbf{C}}_{\omega}^{k}\) is similar to a normal matrix. The other algorithmic modification we make--using absolute rather than relative tolerance--is so that \(\|\tilde{\mathbf{C}}_{\omega}^{k}\mathbf{b}\|_{2}^{2}\) is (roughly) a sum of i.i.d. \(\chi^{2}\) random variables; note that the square of relative tolerance criterion \(\|\tilde{\mathbf{C}}_{\omega}^{k}\mathbf{b}\|_{2}^{2}/\|\mathbf{b}\|_{2}^{2}\) does not admit such a result. At the same time, absolute tolerance does not imply an a.s. bound on the number of iterations if \(\|\mathbf{b}\|_{2}\) is unbounded, which is why we truncate its distribution.

Lipschitzness follows because \(|\mathbb{E}_{\mathbf{b}}\texttt{SSOR}(\omega)-\mathbb{E}_{\mathbf{b}}\texttt{ SSOR}(\omega^{\prime})|\) can be bounded using Jensen's inequality by the probability that \(\omega\) and \(\omega^{\prime}\) have different costs \(k\neq l\), which is at most the probability that \(\|\tilde{\mathbf{C}}_{\omega}^{k}\mathbf{b}\|_{2}\) or \(\|\tilde{\mathbf{C}}_{\omega^{\prime}}^{l}\mathbf{b}\|_{2}\) land in an interval of length \(\mathcal{O}(|\omega-\omega^{\prime}|)\). Note that the Lipschitz bound includes an \(\tilde{\mathcal{O}}(\sqrt{n})\) factor, which results from \(\tilde{\mathbf{C}}_{\omega}^{k}\) having stable rank \(\ll n\) due to powering. Regularity of \(\mathbb{E}_{\mathbf{b}}\texttt{SSOR}\) leads directly to regret guarantee for the same algorithm as before, Tsallis-INF:

**Theorem 3.1**.: _Define \(\kappa_{\max}=\max_{t}\kappa(\mathbf{A}_{t})\) to be the largest condition number and \(\beta_{\min}=\min_{t}\rho(\mathbf{I}_{n}-\mathbf{D}_{t}^{-1}\mathbf{A}_{t})\). Then there exists \(K=\Omega(\log\frac{n}{\varepsilon})\) s.t. running Algorithm 1 with SSOR has regret_

\[\mathbb{E}\sum_{t=1}^{T}\texttt{SSOR}_{t}(\omega_{t})-\min_{\omega\in[1, \omega_{\max}]}\sum_{t=1}^{T}\texttt{SSOR}_{t}(\omega)\leq 2K\sqrt{2dT}+\frac{32K^{4}T}{ \beta_{\min}^{4}d}\sqrt{\frac{2n\kappa_{\max}}{\pi}} \tag{7}\]

Setting \(d=\Theta(K^{2}\sqrt[3]{nT})\) yields a regret bound of \(\mathcal{O}(\log^{2}\frac{n}{\varepsilon}\sqrt[3]{T^{2}\sqrt{n}})\). Note that, while this shows convergence to the true optimal parameter, the constants in the regret term are much worse, not just due to the dependence on \(n\) but also in the powers of the number of iterations. Thus this result can be viewed as a proof of the asymptotic (\(T\to\infty\)) correctness of Tsallis-INF for tuning SSOR.

Figure 2: **Left:** solver cost for b drawn from a truncated Gaussian v.s. b a small eigenvector of \(\mathbf{C}_{1.4}\). **Center-left:** cost to solve 5K diagonally shifted systems \(\mathbf{A}_{t}=\mathbf{A}+\frac{12c_{t}-3}{20}\mathbf{I}_{n}\) for \(c_{t}\sim\text{Beta}(2,6)\). **Center-right:** total SSOR-preconditioned CG iterations taken while solving the 2D heat equation with a time-varying diffusion coefficient (used as context) on different grids, as a function of the linear system dimension. **Right:** (smoothed) parameters chosen at each timestep of one such simulation, overlaid on a contour plot of the cost of solving the system at step \(t\) with parameter \(\omega\) (c.f. Appendix H).

```
0: solver \(\mathsf{SOLVE}:\mathbb{R}^{n\times n}\times\mathbb{R}^{n}\times\Omega\mapsto \mathbb{Z}_{>0}\), instance sequence \(\{(\mathbf{A}_{t},\mathbf{b}_{t})\}_{t=1}^{T}\subset\mathbb{R}^{n\times n} \times\mathbb{R}^{n}\),  context sequence \(\{c_{t}\}_{t=1}^{T}\subset[c_{\text{min}},c_{\text{min}}+C]\), learning rate \(\eta>0\), parameter grid \(\mathbf{g}\in\Omega^{d}\),  Chebyshev polynomial features \(\mathbf{f}:[c_{\text{min}},c_{\text{min}}+C]\mapsto\mathbb{R}^{m+1}\), normalizations \(K,L,N>0\) for\(t=1,\ldots,T\)do \(\theta_{i}\leftarrow\underset{|\theta_{0}|\leq\frac{1}{N},|\theta_{i}|\leq \frac{2C\mu}{K\lambda\eta}}{\arg\min}\underset{i_{*}=i}{\sum}^{t-1}\left(( \theta,\mathbf{f}(c_{s}))-\frac{k}{K\lambda N}\right)^{2}\ \forall\ i\in[d]\)// update models \(\mathbf{s}_{[i]}\leftarrow\langle\theta_{i},\mathbf{f}(c_{t})\rangle\ \forall\ i\in[d]\)// compute model predictions \(i^{*}\leftarrow\arg\min_{[d]}|\mathbf{s}_{[i]}|\)\(\mathbf{P}_{[i]}\leftarrow\frac{1}{d+\eta(\mathbf{s}_{[i]}-\mathbf{s}_{[i^{*}]})}\ \forall\ i\neq i^{*}\)// compute probability of each action \(\mathbf{p}_{[i^{*}]}\gets 1-\sum_{i\neq i^{*}}\mathbf{p}_{[i]}\) sample \(i_{t}\in[d]\) w.p. \(\mathbf{p}_{[i_{t}]}\) and set \(\omega_{t}=\mathbf{g}_{[i_{t}]}\)// sample action \(k_{t}\leftarrow\mathsf{SOLVE}(\mathbf{A}_{t},\mathbf{b}_{t},\omega_{t})-1\)// run solver and update cost
```

**Algorithm 2**ChebC: SquareCB with a follow-the-leader oracle and polynomial regressor class.

### Chebyshev regression for diagonal shifts

For the shifted setting, we can use the same approach to prove that \(\mathbb{E}_{\mathsf{b}}\mathsf{SSOR}(\mathbf{A}+c\mathbf{I}_{n},\mathbf{b},\omega)\) is Lipschitz w.r.t. the diagonal offset \(c\) (c.f. Corollary G.2); for \(n=O(1)\) this implies regret \(\tilde{\mathcal{O}}(T^{3/4}\sqrt{n})\) for the same discretization-based algorithm as in Section 2.4. While optimal for Lipschitz functions, the method does not readily adapt to nice data, leading to various smoothed comparators [39, 47, 66]; however, as we wish to compete with the true optimal policy, we stay in the original setting and instead highlight how this section's semi-stochastic analysis allows us to study a very different class of bandit algorithms.

In particular, since we are now working directly with the cost function rather than an upper bound, we are able to utilize a more practical regression-oracle algorithm, SquareCB [26]. It assumes a class of regressors \(h:[c_{\text{min}},c_{\text{min}}+C]\times[d]\mapsto[0,1]\) with at least one function that perfectly predicts the expected performance \(\mathbb{E}_{\mathsf{b}}\mathsf{SSOR}(\mathbf{A}+c\mathbf{I}_{n},\mathbf{b}, \mathbf{g}_{[i]})\) of each action \(\mathbf{g}_{[i]}\) given the context \(c\); a small amount of model misspecification is allowed. If there exists an online algorithm that can obtain low regret w.r.t. this function class, then SquareCB can obtain low regret w.r.t. any policy.

To apply it we must specify a suitable class of regressors, bound its approximation error, and specify an algorithm attaining low regret over this class. Since \(m\) terms of the Chebyshev series suffice to approximate a Lipschitz function with error \(\tilde{\mathcal{O}}(1/m)\), we use Chebyshev polynomials in \(c\) with learned coefficients--i.e. models \(\langle\theta,\mathbf{f}(c)\rangle=\sum_{j=0}^{m}\theta_{[j]}P_{j}(c)\), where \(P_{j}\) is the \(j\)th Chebyshev polynomial--as our regressors for each action. To keep predictions bounded, we add constraints \(|\theta_{[j]}|=\mathcal{O}(1/j)\), which we can do without losing approximation power due to the decay of Chebyshev series coefficients. This allows us to show \(\mathcal{O}(dm\log T)\) regret for Follow-The-Leader via Hazan et al. [33, Theorem 5] and then apply Foster & Rakhlin [26, Theorem 5] to obtain the following guarantee:

**Theorem 3.2** (Corollary of Theorem D.4).: _Suppose \(c_{\text{min}}>-\lambda_{\text{min}}(\mathbf{A})\). Then Algorithm 2 with appropriate parameters has regret w.r.t. any policy \(f:[c_{\text{min}},c_{\text{min}}+C]\mapsto\Omega\) of_

\[\mathbb{E}\sum_{t=1}^{T}\mathsf{SSOR}_{t}(\omega_{t})-\sum_{t=1}^{T}\mathsf{SSOR }_{t}(f(c_{t}))\leq\tilde{\mathcal{O}}\left(d\sqrt{mnT}+\frac{T\sqrt{n}}{m}+ \frac{T\sqrt{n}}{d}\right) \tag{8}\]

Setting \(d=\Theta(T^{2/11})\) and \(m=\Theta(T^{3/11})\) yields \(\tilde{\mathcal{O}}(T^{9/11}\sqrt{n})\) regret, so we asymptotically attain instance-optimal performance, albeit at a rather slow rate. The rate in \(n\) is also worse than e.g. our semi-stochastic result for comparing to a fixed \(\omega\) (c.f. Theorem 3.1), although to obtain this the latter algorithm uses \(d=\mathcal{O}(\sqrt[3]{n})\) grid points, making its overhead nontrivial. We compare ChebCB to the Section 2.4 algorithm based on Tsallis-INF (among other methods), and find that, despite the former's worse guarantees, it seems able to converge to an instance-optimal policy much faster than the latter.

## Acknowledgments

This work was supported in part by the National Science Foundation grants IIS-1705121, IIS-1838017, IIS-2046613, IIS-2112471, and OAC-2203821, along with funding from Meta, Morgan Stanley, Amazon, Google, and Jane Street. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of any of these funding agencies.

## References

* Abernethy et al. [2015] Jacob Abernethy, Chansoo Lee, and Ambuj Tewari. Fighting bandits with a new kind of smoothness. In _Advances in Neural Information Processing Systems_, 2015.
* Amos [2023] Brandon Amos. Tutorial on amortized optimization. _Foundations and Trends in Machine Learning_, 16(5):592-732, 2023.
* Anzt et al. [2016] Hartwig Anzt, Edmond Chow, Jens Saak, and Jack Dongarra. Updating incomplete factorization preconditioners for model order reduction. _Numerical Algorithms_, 73:611-630, 2016.
* Arisaka and Li [2023] Sohei Arisaka and Qianxiao Li. Principled acceleration of iterative numerical methods using machine learning. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* Auer et al. [2002] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit problem. _SIAM Journal of Computing_, 32:48-77, 2002.
* Axelsson [1994] Owe Axelsson. _Iterative Solution Methods_. Cambridge University Press, 1994.
* Balcan [2021] Maria-Florina Balcan. Data-driven algorithm design. In Tim Roughgarden (ed.), _Beyond the Worst-Case Analysis of Algorithms_. Cambridge University Press, Cambridge, UK, 2021.
* Balcan et al. [2018] Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm design, online learning, and private optimization. In _59th Annual Symposium on Foundations of Computer Science_, 2018.
* Balcan et al. [2020] Maria-Florina Balcan, Travis Dick, and Wesley Pegden. Semi-bandit optimization in the dispersed setting. In _Proceedings of the Conference on Uncertainty in Artificial Intelligence_, 2020.
* Balcan et al. [2021] Maria-Florina Balcan, Dan DeBlasio, Travis Dick, Carl Kingsford, Tuomas Sandholm, and Ellen Vitercik. How much data is sufficient to learn high-performing algorithms? Generalization guarantees for data-driven algorithm design. In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computingg_, 2021.
* Balcan et al. [2022] Maria-Florina Balcan, Mikhail Khodak, Dravyansh Sharma, and Ameet Talwalkar. Provably tuning the ElasticNet across instances. In _Advances in Neural Information Processing Systems_, 2022.
* Bartlett et al. [2022] Peter Bartlett, Piotr Indyk, and Tal Wagner. Generalization bounds for data-driven numerical linear algebra. In _Proceedings of the 35th Annual Conference on Learning Theory_, 2022.
* Baumann and van Gijzen [2015] Manuel Baumann and Martin B. van Gijzen. Nested Krylov methods for shifted linear systems. _SIAM Journal on Scientific Computing_, 37:S90-S112, 2015.
* Bellavia et al. [2011] Stefania Bellavia, Valentina De Simone, Daniela di Serafina, and Benedetta Morini. Efficient preconditioner updates for shifted linear systems. _SIAM Journal on Scientific Computing_, 33:1785-1809, 2011.
* Beygelzimer et al. [2011] Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandit algorithms with supervised learning guarantees. In _Proceedings of the 14th International Conference on Artificial Intelligence and Statistics_, 2011.
* Bubeck and Cesa-Bianchi [2012] Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends in Machine Learning_, 5(1):1-122, 2012.

* [17] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, Learning, and Games_. Cambridge University Press, 2006.
* [18] Justin Y. Chen, Sandeep Silwal, Ali Vakilian, and Fred Zhang. Faster fundamental graph algorithms via learned predictions. In _Proceedings of the 40th International Conference on Machine Learning_, 2022.
* [19] Xinyi Chen and Elad Hazan. A nonstochastic control approach to optimization. arXiv, 2023.
* [20] Elliott Ward Cheney. _Introduction to Approximation Theory_. Chelsea Publishing Company, 1982.
* [21] Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn stochastic gradient descent with biased regularization. In _Proceedings of the 36th International Conference on Machine Learning_, 2019.
* [22] Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster matchings via learned duals. In _Advances in Neural Information Processing Systems_, 2021.
* [23] Paul Dutting, Guru Guruganesh, Jon Schneider, and Joshua R. Wang. Optimal no-regret learning for one-sided lipschitz functions. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* [24] Louis W. Ehrlich. An ad hoc SOR method. _Journal of Computational Physics_, 44:31-45, 1981.
* [25] Lakhdar Elbouyahyaoui, Mohammed Heyouni, Azita Tajaddini, and Farid Saberi-Movahed. On restarted and deflated block FOM and GMRES methods for sequences of shifted linear systems. _Numerical Algorithms_, 87:1257-1299, 2021.
* [26] Dylan J. Foster and Alexander Rakhlin. Beyond UCB: Optimal and efficient contextual bandits with regression oracles. In _Proceedings of the 37th International Conference on Machine Learning_, 2020.
* [27] Isaac Fried and Jim Metzler. SOR vs. conjugate gradients in a finite element discretization. _International Journal for Numerical Methods in Engineering_, 12:1329-1332, 1978.
* [28] Andreas Frommer and Uew Glassner. Restarted GMRES for shifted linear systems. _SIAM Journal on Scientific Computing_, 19:15-26, 1998.
* [29] Gene H. Golub and Qiang Ye. Inexact preconditioned conjugate gradient method with inner-outer iteration. _SIAM Journal on Scientific Computing_, 21:1305-1320, 1999.
* [30] Anne Greenbaum. _Iterative Methods for Solving Linear Systems_. Society for Industrial and Applied Mathematics, 1997.
* [31] Rishi Gupta and Timothy Roughgarden. A PAC approach to application-specific algorithm selection. _SIAM Journal on Computing_, 46(3):992-1017, 2017.
* [32] Wolfgang Hackbusch. _Iterative Solution of Large Sparse Systems of Equations_. Springer International Publishing, 2016.
* [33] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. _Machine Learning_, 69:169-192, 2007.
* [34] George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. _Nature Reviews Physics_, 3:422-440, 2021.
* [35] Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta-learning methods. In _Advances in Neural Information Processing Systems_, 2019.
* [36] Mikhail Khodak, Maria-Florina Balcan, Ameet Talwalkar, and Sergei Vassilvitskii. Learning predictions for algorithms with predictions. In _Advances in Neural Information Processing Systems_, 2022.

* [37] John B. King, Samim Anghaie, and Henry M. Domanus. Comparative performance of the conjugate gradient and SOR methods for computational thermal hydraulics. In _Proceedings of the Joint Meeting of the American Nuclear Society and the Atomic Industrial Forum_, 1987.
* [38] Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In _Advances in Neural Information Processing Systems_, 2004.
* [39] Akshay Krishnamurthy, John Langord, Alexandrs Slivkins, and Chicheng Zhang. Contextual bandits with continuous actions: Smoothing, zooming, and adapting. In _Proceedings of the 32nd Conference on Learning Theory_, 2019.
* [40] John Lafferty, Han Liu, and Larry Wasserman. Statistical machine learning. [https://www.stat.cmu.edu/](https://www.stat.cmu.edu/) larry/=sml/Concentration.pdf, 2010.
* [41] Soren Laue, Matthias Mitterreiter, and Joachim Giesen. Computing higher order derivatives of matrix and tensor expressions. In _Advances in Neural Information Processing Systems_, 2018.
* [42] Randall J. LeVeque. _Finite Difference Methods for Ordinary and Partial Differential Equations_. SIAM, 2007.
* [43] Yichen Li, Peter Yichen Chen, Tao Du, and Wojciech Matusik. Learning preconditioners for conjugate gradient PDE solvers. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* [44] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. In _Proceedings of the 9th International Conference on Learning Representations_, 2021.
* [45] Tyler Lu, David Pal, and Martin Pal. Contextual multi-armed bandits. In _Proceedings of the 13th International Conference on Artificial Intelligence and Statistics_, 2010.
* [46] Ilay Luz, Meirav Galun, Haggai Maron, Ronen Basri, and Irad Yavneh. Learning algebraic multigrid using graph neural networks. In _Proceedings of the 37th International Conference on Machine Learning_, 2020.
* [47] Maryam Majzoubi, Chicheng Zhang, Rajan Chari, Akshay Krishnamurthy, John Langford, and Alexandrs Slivkins. Efficient contextual bandits with continuous actions. In _Advances in Neural Information Processing Systems_, 2020.
* [48] Tanya Marwah, Zachary C. Lipton, and Andrej Risteski. Parametric complexity bounds for approximating PDEs with neural networks. In _Advances in Neural Information Processing Systems_, 2021.
* [49] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. In Tim Roughgarden (ed.), _Beyond the Worst-Case Analysis of Algorithms_. Cambridge University Press, Cambridge, UK, 2021.
* [50] Cameron Musco and Christopher Musco. Randomized block Krylov methods for stronger and faster approximate singular value decomposition. In _Advances in Neural Information Processing Systems_, 2015.
* [51] Michael L. Parks, Eric de Sturler, Greg Mackey, Duane D. Johnson, and Spandan Maiti. Recycling Krylov subspaces for sequences of linear systems. _SIAM Journal on Scientific Computing_, 28(5):1651-1674, 2006.
* [52] Shinsaku Sakaue and Taihei Oki. Discrete-convex-analysis-based framework for warm-starting algorithms with predictions. In _Advances in Neural Information Processing Systems_, 2022.
* [53] Rajiv Sambharya, Georgina Hall, Brandon Amos, and Bartolomeo Stellato. End-to-end learning to warm-start for real-time quadratic optimization. In _Proceedings of the 5th Annual Conference on Learning for Dynamics and Control_, 2023.

* [54] Nikunj Saunshi, Yi Zhang, Mikhail Khodak, and Sanjeev Arora. A sample complexity separation between non-convex and convex meta-learning. In _Proceedings of the 37th International Conference on Machine Learning_, 2020.
* [55] David Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability. _Mathematics of Operations Research_, 47, 2021.
* [56] Ali Taghibakhshi, Scott MacLachlan, Luke Olson, and Matthew West. Optimization-based algebraic multigrid coarsening using reinforcement learning. In _Advances in Neural Information Processing Systems_, 2021.
* [57] Jurjen D. Tebbens and Miroslav Tuma. Efficient preconditioning of sequences of nonsymmetric linear systems. _SIAM Journal on Scientific Computing_, 29:1918-1941, 2007.
* [58] James William Thomas. _Numerical Partial Differential Equations_. Springer Science+Business Media, 1999.
* [59] Lloyd N. Trefethen. Is Gauss quadrature better than Clenshaw-Curtis? _SIAM Review_, 50(1): 67-87, 2008.
* [60] Lloyd N. Trefethen and Mark Embree. _Spectra and Pseudospectra: The Behavior of Nonnormal Matrices and Operators_. Princeton University Press, 2005.
* [61] L. Dale Van Vleck and D. J. Dwyer. Successive overrelaxation, block iteration, and method of conjugate gradients for solving equations for multiple trait evaluation of sires. _Journal of Dairy Science_, 68:760-767, 1985.
* [62] Rui-Rui Wang, Qiang Niu, Xiao-Bin Tang, and Xiang Wang. Solving shifted linear systems with restarted GMRES augmented with error approximations. _Computers & Mathematics with Applications_, 78:1910-1918, 2019.
* [63] Zbigniew I. Woznicki. On numerical analysis of conjugate gradient method. _Japan Journal of Industrial and Applied Mathematics_, 10:487-519, 1993.
* [64] Zbigniew I. Woznicki. On performance of SOR method for solving nonsymmetric linear systems. _Journal of Computational and Applied Mathematics_, 137:145-176, 2001.
* [65] David M. Young. _Iterative Solution of Large Linear Systems_. Academic Press, 1971.
* [66] Yinglun Zhu and Paul Mineiro. Contextual bandits with smooth regret: Efficient learning in continuous action spaces. In _Proceedings of the 39th International Conference on Machine Learning_, 2022.
* [67] Julian Zimmert and Yevgeny Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. _Journal of Machine Learning Research_, 22:1-49, 2021.

Related work and comparisons

Our analysis falls mainly into the framework of data-driven algorithm design, which has a long history [31, 7]. Closely related is the study by Gupta & Roughgarden [31] of the sample complexity of learning the step-size of gradient descent, which can also be used to solve linear systems. While their sample complexity guarantee is logarithmic in the precision \(1/\varepsilon\), directly applying their Lipschitz-like analysis in a bandit setting yields regret with a polynomial dependence; note that a typical setting of \(\varepsilon\) is \(10^{-8}\). Mathematically, their analysis relies crucially on the iteration reducing error at every step, which is well-known _not_ to be the case for SOR (e.g. Trefethen & Embree [60, Figure 25.6]). Data-driven numerical linear algebra was studied most explicitly by Bartlett et al. [12], who provided sample complexity framework applicable to many algorithms; their focus is on the offline setting where an algorithm is learned from a batch of samples. While they do not consider linear systems directly, in Appendix A.1 we do compare to the guarantee their framework implies for SOR; we obtain similar sample complexity with an efficient learning procedure, at the cost of a strong distributional assumption on the target vector. Note that generalization guarantees have been shown for convex quadratic programming--which subsumes linear systems--by Sambharya et al. [53]; they focus on learning-to-initialize, which we do not consider because for high precisions the initialization quality usually does not have a strong impact on cost. Note that all of the above work also does not provide end-to-end guarantees, only e.g. sample complexity bounds.

Online learning guarantees were shown for the related problem of tuning regularized regression by Balcan et al. [11], albeit in the easier full information setting and with the target of reducing error rather than computation. Their approach relies on the dispersion technique [8], which often involves showing that discontinuities in the cost are defined by bounded-degree polynomials [9]. While possibly applicable in our setting, we suspect using it would lead to unacceptably high dependence on the dimension and precision, as the power of the polynomials defining our decision boundaries is \(\mathcal{O}(n^{-\log\varepsilon})\). Lastly, we believe our work is notable within this field as a first example of using contextual bandits, and in doing so competing with the provably instance-optimal policy.

Iterative (discrete) optimization has been studied in the related area of learning-augmented algorithms (a.k.a. algorithms with predictions) [22, 18, 52], which shows data-dependent performance guarantees as a function of (learned) predictions [49]; these can then be used as surrogate losses for learning [36]. Our construction of an upper bound under asymptotic convergence is inspired by this, although unlike previous work we do not assume access to the bound directly because it depends on hard-to-compute spectral properties. Algorithms with predictions often involve initializing a computation with a prediction of its outcome, e.g. a vector near the solution \(\mathbf{A}^{-1}\mathbf{b}\); we do not consider this because the runtime of SOR and other solvers depends fairly weakly on the distance to the initialization.

A last theoretical area is that of gradient-based meta-learning, which studies how to initialize and tune other parameters of gradient descent and related methods [35, 21, 54, 19]. This field focuses on learning-theoretic notions of cost such as regret or statistical risk. Furthermore, their guarantees are usually on the error after a fixed number of gradient steps rather than the number of iterations required to converge; targeting the former can be highly suboptimal in scientific computing applications [4]. This latter work, which connects meta-learning and data-driven scientific computing, analyzes specific case studies for accelerating numerical solvers, whereas we focus on a general learning guarantee.

Empirically, there are many learned solvers [46, 56, 43] and even full simulation replacements [34, 44]; to our knowledge, theoretical studies of the latter have focused on expressivity [48]. Amortizing the cost on future simulations [2], these approaches use offline computation to train models that integrate directly with solvers or avoid solving linear systems altogether. In contrast, the methods we propose are online and lightweight, both computationally and in terms of implementation; unlike many deep learning approaches, the additional computation scales slowly with dimension and needs only black-box access to existing solvers. As a result, our methods can be viewed as reasonable baselines, and we discuss an indirect comparison with the CG-preconditioner-learning approach of Li et al. [43] in Appendix H. Finally, note that improving the performance of linear solvers across a sequence of related instances has seen a lot of study in the scientific computing literature [51, 57, 25]. To our knowledge, this work does not give explicit guarantees on the number of iterations, and so a direct theoretical comparison is challenging.

### Sample complexity and comparison with the Goldberg-Jerrum framework

While not the focus of our work, we briefly note the generalization implications of our semi-stochastic analysis. Suppose for any \(\alpha>0\) we have \(T=\tilde{\mathcal{O}}(\frac{1}{\alpha^{2}}\text{polylog}\frac{n}{2})\) i.i.d. samples from a distribution \(\mathcal{D}\) over matrices \(\mathbf{A}_{t}\) satisfying the assumptions in Section 2.1 and truncated Gaussian targets \(\mathbf{b}_{t}\). Then empirical risk minimization \(\hat{\omega}=\arg\min_{\hat{\omega}\in\mathbf{g}}\sum_{t=1}^{T}\texttt{SSOR}( \mathbf{A}_{t},\mathbf{b}_{t},\omega)\) over a uniform grid \(\mathbf{g}\in[1,\omega_{\text{max}}]^{d}\) of size \(d=\tilde{\mathcal{O}}(\sqrt{nT})\) will be \(\alpha\)-suboptimal w.p. \(\geq 1-\delta\):

**Corollary A.1**.: _Let \(\mathcal{D}\) be a distribution over matrix-vector pairs \((\mathbf{A},\mathbf{b})\in\mathbb{R}^{n\times n}\times\mathbb{R}^{n}\) where \(\mathbf{A}\) satisfies the SOR conditions and for every \(\mathbf{A}\) the conditional distribution of \(\mathcal{D}\) given \(\mathbf{A}\) over \(\mathbb{R}^{n}\) is the truncated Gaussian. For every \(T\geq 1\) consider the algorithm that draws \(T\) samples \((\mathbf{A}_{t},\mathbf{b}_{t})\sim\mathcal{D}\) and outputs \(\hat{\omega}=\arg\min_{\hat{\omega}\in\mathbf{g}}\sum_{t=1}^{T}\texttt{SSOR} _{t}(\omega)\), where \(\mathbf{g}_{[i]}=1+(\omega_{\text{max}}-1)\frac{i-1/2}{d}\) and \(d=\frac{L\sqrt{T}}{\mathbf{A}}\) for \(L\) as in Corollary G.1. Then \(T=\tilde{\mathcal{O}}\left(\frac{1}{\alpha^{2}}\text{polylog}\frac{n}{\hat{ \omega}\delta}\right)\) samples suffice to ensure \(\mathbb{E}_{\mathcal{D}}\texttt{SSOR}(\mathbf{A},\mathbf{b},\hat{\omega})\leq \min_{\omega\in[1,\omega_{\text{max}}]}\texttt{SSOR}(\mathbf{A},\mathbf{b}, \omega)+\alpha\) holds w.p. \(\geq 1-\delta\)._

Proof.: A standard covering bound (see e.g. Lafferty et al. [40, Theorem 7.82]) followed by an application of Corollary G.1 implies that w.p. \(\geq 1-\delta\)

\[\mathbb{E}_{\mathcal{D}}\texttt{SSOR}(\mathbf{A},\mathbf{b}, \hat{\omega}) \leq\min_{\omega\in\mathbf{g}}\mathbb{E}_{\mathcal{D}}\texttt{SSOR }(\mathbf{A},\mathbf{b},\omega)+3K\sqrt{\frac{2}{T}\log\frac{2d}{\delta}} \tag{9}\] \[=\min_{\omega\in\mathbf{g}}\mathbb{E}_{\mathbf{A}}[\mathbb{E}_{ \mathbf{b}}\texttt{SSOR}(\mathbf{A},\mathbf{b},\omega)|\mathbf{A}]+3K\sqrt{ \frac{2}{T}\log\frac{2d}{\delta}}\] \[\leq\min_{\omega\in[1,\omega_{\text{max}}]}\mathbb{E}_{\mathbf{A }}\left[\mathbb{E}_{\mathbf{b}}\texttt{SSOR}(\mathbf{A},\mathbf{b},\omega)+ \frac{L}{d}\bigg{|}\mathbf{A}\right]+3K\sqrt{\frac{2}{T}\log\frac{2d}{\delta}}\] \[=\min_{\omega\in[1,\omega_{\text{max}}]}\mathbb{E}_{\mathcal{D}} \texttt{SSOR}(\mathbf{A},\mathbf{b},\omega)+\frac{L}{d}+3K\sqrt{\frac{2}{T} \log\frac{2d}{\delta}}\] \[\leq\min_{\omega\in[1,\omega_{\text{max}}]}\mathbb{E}_{\mathcal{ D}}\texttt{SSOR}(\mathbf{A},\mathbf{b},\omega)+4K\sqrt{\frac{2}{T}\log\frac{2LT}{K \delta}}\]

Noting that by Corollary G.1 we have \(L=\mathcal{O}(K^{4}\sqrt{n})=\mathcal{O}(\sqrt{n}\log^{4}\frac{n}{\varepsilon})\) yields the result. 

This matches directly applying the GJ framework of Bartlett et al. [12, Theorem 3.3] to our problem:

**Corollary A.2**.: _In the same setting as Corollary A.1 but generalizing the distribution to any one whose target vector support is \(\sqrt{n}\)-bounded, empirical risk minimization (running \(\hat{\omega}=\arg\min_{\omega\in[1,\omega_{\text{max}}]}\sum_{t=1}^{T}\texttt{SSOR }_{t}(\omega)\)) has sample complexity \(\tilde{\mathcal{O}}\left(\frac{1}{\alpha^{2}}\text{polylog}\frac{n}{\varepsilon \delta}\right)\)._

Proof.: For every \((\mathbf{A},\mathbf{b})\) pair in the support of \(\mathcal{D}\) and any \(r\in\mathbb{R}\) it is straightforward to define a GJ algorithm [12, Definition 3.1] that checks if \(\texttt{SSOR}(\mathbf{A},\mathbf{b},\omega)>r\) by computing \(\|\mathbf{r}_{k}(\omega)\|_{2}^{2}=\|\hat{\mathbf{C}}_{\omega}^{k}\mathbf{b} \|_{2}^{2}\)--a degree \(2k\) polynomial--for every \(k\leq\lfloor r\rfloor\) and returning "True" if one of them satisfies \(\|\mathbf{r}_{k}(\omega)\|_{2}^{2}\leq\varepsilon^{2}\) and "False" otherwise (and automatically return "True" for \(r\geq K\) and "False" for \(r<1\)). Since the degree of this algorithm is at most \(2K\), the predicate complexity is at most \(K\), and the parameter size is \(1\), by Bartlett et al. [12, Theorem 3.3] the pseudodimension of \(\{\texttt{SSOR}(\cdot,\cdot,\omega):\omega\in[1,\omega_{\text{max}}]\}\) is \(\mathcal{O}(\log K)\). Using the bounded assumption on the target vector--\(\texttt{SSOR}\leq K=\mathcal{O}(\log\frac{n}{\varepsilon})\)--completes the proof. 

At the same, recent generalization guarantees for tuning regularization parameters of linear regression by Balcan et al. [11, Theorem 3.2]--who applied dual function analysis [10]--have a quadratic dependence on the instance dimension. Unlike both results--which use uniform convergence--our bound also uses a (theoretically) efficient learning procedure, at the cost of a strong (but in our view reasonable) distributional assumption on the target vectors.

### Approximating the spectral radius of the Jacobi iteration matrix

Because the asymptotically optimal \(\omega\) is a function of the spectral radius \(\beta=\rho(\mathbf{M}_{1})\) of the Jacobi iteration matrix, a reasonable baseline is to simply approximate \(\beta\) using an eigenvalue solver and then run SOR with the corresponding approximately best \(\omega\). It is difficult to compare our results to this approach directly, since the baseline will always run extra matrix iterations while bandit algorithms will asymptotically run no more than the comparator. Furthermore, \(\mathbf{M}_{1}\) is not a normal matrix, a class for which it turns out to be surprisingly difficult to find bounds on the number of iterations required to approximate its largest eigenvalue within some tolerance \(\alpha>0\).

A comparison can be made in the diagonal offset setting by modifying this baseline somewhat and making the assumption that \(\mathbf{A}\) has a constant diagonal, so that \(\mathbf{M}_{1}\) is symmetric and we can use randomized block-Krylov to obtain a \(\hat{\beta}\) satisfying \(|\hat{\beta}^{2}-\beta^{2}|=\mathcal{O}(\varepsilon)\) in \(\tilde{\mathcal{O}}(1/\sqrt{\varepsilon})\) iterations w.h.p. [50, Theorem 1]. To modify the baseline, we consider a _preprocessing_ algorithm which discretizes \([c_{\text{min}},c_{\text{min}}+C]\) into \(d\) grid points, runs \(k\) iterations of randomized block-Krylov on the Jacobi iteration matrix of each matrix \(\mathbf{A}+c\mathbf{I}_{n}\) corresponding to offsets \(c\) in this grid, and then for each new offset \(c_{t}\) we set \(\omega_{t}\) using the optimal parameter implied by the approximate spectral radius of the Jacobi iteration matrix of \(\mathbf{A}+c\mathbf{I}_{n}\) corresponding to the closest \(c\) in the grid. This algorithm thus does \(\tilde{\mathcal{O}}(dk)\) matrix-vector products of preprocessing, and since the upper bounds \(U_{t}\) are \(\frac{1}{2}\)-Holder w.r.t. \(\omega\) while the optimal policy is Lipschitz w.r.t. \(\beta^{2}\) over an appropriate domain \([1,\omega_{\text{max}}]\) it will w.h.p. use at most \(\tilde{\mathcal{O}}(\sqrt{1/k^{2}+1/d})\) more iterations at each step \(t\in[T]\) compared to the optimal policy. Thus w.h.p. the total regret compared to the optimal policy \(\omega^{*}\) is

\[\sum_{t=1}^{T}\mathsf{SOR}_{t}(\omega_{t})=\tilde{\mathcal{O}} \left(dk+T/d+T/\sqrt{k}\right)+\sum_{t=1}^{T}U_{t}(\omega^{*}(c_{t})) \tag{10}\]

Setting \(d=\sqrt[4]{T}\) and \(k=\sqrt{T}\) yields the rate \(\tilde{\mathcal{O}}(T^{3/4})\), which can be compared directly to our \(\tilde{\mathcal{O}}(T^{3/4})\) rate for the discretized Tsallis-INF algorithm in Theorem 2.2. The rate of approximating \(\rho(\mathbf{M}_{1})\) thus matches that of our simplest approach, although unlike the latter (and also unlike ChebCB) it does not guarantee performance as good as the optimal policy in the semi-stochastic setting, where \(\omega^{*}\) might not be optimal. Intuitively, the randomized block-Krylov baseline will also suffer from spending computation on points \(c\in[c_{\text{min}},c_{\text{min}}+C]\) that it does not end up seeing.

### Tuning preconditioned conjugate gradient

CG is perhaps the most-used solver for positive definite systems; while it can be run without tuning, in practice significant acceleration can be realized via a good preconditioner such as (symmetric) SOR. The effect of \(\omega\) on CG performance can be somewhat distinct from that of regular SOR, requiring a separate analysis. We use the condition number analysis of Axelsson [6, Theorem 7.17] to obtain an upper bound \(U^{\complement}(\omega)\) on the number of iterations required \(\mathsf{CG}(\mathbf{A},\mathbf{b},\omega)\) to solve a system. While the resulting bounds match the shape of the true performance less exactly than the SOR bounds (c.f. Figure 4), they still provide a somewhat reasonable surrogate. After showing that these functions are also semi-Lipschitz (c.f. Lemma F.2), we can bound the cost of tuning CG using Tsallis-INF:

**Theorem A.1**.: _Set \(\mu_{t}=\rho(\mathbf{D}_{t}\mathbf{A}_{t}^{-1})\), \(\mu_{\text{max}}=\max_{t}\mu_{t}\), \(\overline{\sqrt{\mu}}=\frac{1}{T}\sum_{t=1}^{T}\sqrt{\mu}\), and \(\kappa_{\text{max}}=\max_{t}\kappa(\mathbf{A}_{t})\). If \(\min_{t}\mu_{t}-1\) is a positive constant then for Algorithm 1 using preconditioned CG as the solver there exists a parameter grid \(\mathbf{g}\in[2\sqrt{2}+2,\omega_{\text{max}}]^{d}\) and normalization \(K>0\) such that_

\[\mathbb{E}\sum_{t=1}^{T}\mathcal{G}\!\ell_{t}(\omega_{t})=\mathcal{O}\left( \sqrt[3]{\frac{\log^{2}\frac{\sqrt{\kappa_{\text{max}}}}{\varepsilon}}{\log^ {2}\frac{\sqrt{\mu_{\text{max}}}}{\sqrt{\mu_{\text{max}}}+1}}\sqrt{\overline{ \mu}}T^{2}}\right)+\min_{\omega\in(0,2)}\sum_{t=1}^{T}U_{t}(\omega) \tag{11}\]

Observe that the rate in \(T\) remains the same as for SOR, but the difficulty of learning now scales mainly with the spectral radii of the matrices \(\mathbf{D}_{t}\mathbf{A}_{t}^{-1}\).

[MISSING_PAGE_EMPTY:16]

[MISSING_PAGE_FAIL:17]

For contextual bandits, we restrict to \((L_{t},b)\)-semi-Lipschitz functions and \(L_{f}\)-Lipschitz policies, obtaining \(\mathcal{O}(T^{3/4})\) regret; this rate matches known upper and lower bounds for the case where losses are Lipschitz in both actions and contexts [45, Theorem 1], although this does not imply optimality of our result.

**Theorem C.4**.: _If \(u_{t}\geq\ell_{t}\) is \((L_{t},b)\)-semi-Lipschitz and \(c_{t}\in[c,c+C]\)\(\forall\)\(t\in[T]\) then Algorithm 5 using action space \(\mathbf{g}_{[i]}=a+\frac{b-a}{d}i\) and \(\mathbf{h}_{[j]}=c+\frac{C}{m}(j-\frac{1}{2})\) as the grid of contexts has regret w.r.t. any \(L_{f}\)-Lipschitz policy \(f:[c,c+C]\mapsto[a,b]\) of_

\[\mathbb{E}\sum_{t=1}^{T}\ell_{t}(\mathbf{g}_{[i_{t}]})-\sum_{t=1}^{T}u_{t}(\pi (c_{t}))\leq m+4K\sqrt{dmT}+\left(\frac{CL_{f}}{m}+\frac{b-a}{d}\right)\sum_{t =1}^{T}L_{t} \tag{14}\]

_Setting \(d=\sqrt[4]{\frac{(b-a)^{3}L^{2}T}{4CL_{f}K^{2}}}\), \(m=\sqrt[4]{\frac{C^{3}L_{f}^{3}L^{2}T}{4(b-a)K^{2}}}\) yields regret \(4\sqrt[4]{4K^{2}L^{2}(b-a)CL_{f}T^{3}}+\sqrt[4]{\frac{C^{3}L_{f}^{3}L^{2}T}{4( b-a)K^{2}}}\)._

Proof.: Define \(\lceil\cdot\rceil_{\mathbf{h}}\) to be the operation of rounding to the closest element of \(\mathbf{h}\), breaking ties arbitrarily, and set \([T]_{j}=\{t\in[T]:\lceil c_{t}\rfloor_{\mathbf{h}}=\mathbf{h}_{[j]}\}\). Furthermore, define \(\lceil x\rceil_{\mathbf{g}}\) to be the smallest element \(\mathbf{g}_{[i]}\) in \(\mathbf{g}\) s.t. \(x+\frac{CL_{f}}{2m}\leq\mathbf{g}_{[i]}\) (or \(\max_{i\in[d]}\mathbf{g}_{[i]}\) if such an element does not exist).

\[\mathbb{E}\sum_{t=1}^{T}\ell_{t}(\mathbf{g}_{[i_{t}]}) =\mathbb{E}\sum_{j=1}^{m}\sum_{t\in[T]_{j}}\ell_{t}(\mathbf{g}_{[ i_{t}]})-\min_{i\in[d]}\sum_{t\in[T]_{j}}\ell_{t}(\mathbf{g}_{[i]})+\min_{i\in[d] }\sum_{t\in[T]_{j}}\ell_{t}(\mathbf{g}_{[i]}) \tag{15}\] \[\leq m+4K\sqrt{dmT}+\sum_{j=1}^{m}\min_{i\in[d]}\sum_{t\in[T]_{j} }u_{t}(\mathbf{g}_{[i]})\] \[\leq m+4K\sqrt{dmT}+\sum_{t=1}^{T}u_{t}(\lceil f(\lceil c_{t} \rfloor_{\mathbf{h}})\rceil_{\mathbf{g}})\]

where the first inequality follows by Theorem C.2, the second applies Jensen's inequality to the left term and \(u_{t}\geq\ell_{t}\) on the right, and the last uses optimality of each \(i\) for each \(j\). Now since \(f\) is \(L_{f}\)-Lipschitz we have by definition of \(\lceil\cdot\rceil_{\mathbf{h}}\) that \(|f(c_{t})-f(\lceil c_{t}\rfloor_{\mathbf{h}})|\leq\frac{CL_{f}}{2m}\). This in turn implies that \(f(c_{t})\leq\lceil f(\lceil c_{t}\rfloor_{\mathbf{h}})\rceil_{\mathbf{g}}\leq f (c_{t})+\frac{CL_{f}}{m}+\frac{b-a}{d}\) by definition of \(\mathbf{g}\) and \(\lceil\cdot\rceil_{\mathbf{g}}\). Since \(u_{t}\) is \((L_{t},b)\)-semi-Lipschitz, the result follows.

Chebyshev regression for contextual bandits

### Preliminaries

We first state a Lipschitz approximation result that is standard but difficult-to-find formally. For all \(j\in\mathbb{Z}_{\geq 0}\) we will use \(P_{j}(x)=\cos(j\arccos(x))\) to denote the \(j\)th Chebyshev polynomial of the first kind.

**Theorem D.1**.: _Let \(f:[\pm 1]\mapsto[\pm K]\) be a \(K\)-bounded, \(L\)-Lipschitz function. Then for each integer \(m\geq 0\) there exists \(\theta\in\mathbb{R}^{m+1}\) satisfying the following properties:_

1. \(|\theta_{[0]}|\leq K\) _and_ \(|\theta_{[j]}|\leq 2L/j\ \forall\ j\in[m]\)__
2. \(\max_{x\in[\pm 1]}\left|f(x)-\sum_{j=0}^{m}\theta_{[j]}P_{j}(x)\right|\leq \frac{\pi+\frac{2}{\pi}\log(2m+1)}{m+1}L\)__

Proof.: Define \(\theta_{[0]}=\frac{1}{\pi}\int_{-1}^{1}\frac{f(x)}{\sqrt{1-x^{2}}}dx\) and for each \(j\in[m]\) let \(\theta_{[j]}=\frac{2}{\pi}\int_{-1}^{1}\frac{f(x)P_{j}(x)}{\sqrt{1-x^{2}}}dx\) be the \(j\)th Chebyshev coefficient. Since \(\int_{-1}^{1}\frac{dx}{\sqrt{1-x^{2}}}=\pi\) we trivially have \(|\theta_{[0]}|\leq K\) and by Trefethen [59, Theorem 4.2] we also have

\[|\theta_{[j]}|\leq\frac{2}{\pi j}\int_{-1}^{1}\frac{|f^{\prime}(x)|}{\sqrt{1-x ^{2}}}dx\leq\frac{2L}{\pi j}\int_{-1}^{1}\frac{dx}{\sqrt{1-x^{2}}}=2L/j \tag{16}\]

for all \(j\in[m]\). This shows the first property. For the second, by Trefethen [59, Theorem 4.4] we have that

\[\max_{x\in[-1,1]}\left|f(x)-\sum_{j=0}^{m}\theta_{[j]}P_{j}(x)\right| \leq\left(2+\frac{4\log(2m+1)}{\pi^{2}}\right)\max_{x\in[\pm 1]} \left|f(x)-p_{m}^{*}(x)\right|\] \[\leq\left(2+\frac{4\log(2m+1)}{\pi^{2}}\right)\frac{L\pi}{2(m+1) }=\frac{\pi+\frac{2}{\pi}\log(2m+1)}{m+1}L \tag{17}\]

where \(p_{m}^{*}\) is the (at most) \(m\)-degree algebraic polynomial that best approximates \(f\) on \([\pm 1]\) and the second inequality is Jackson's theorem [20, page 147]. 

**Corollary D.1**.: _Let \(f:[a,b]\mapsto[\pm K]\) be a \(K\)-bounded, \(L\)-Lipschitz function on the interval \([a,b]\). Then for each integer \(m\geq 0\) there exists \(\theta\in\mathbb{R}^{m+1}\) satisfying the following properties:_

1. \(|\theta_{[0]}|\leq K\) _and_ \(|\theta_{[j]}|\leq\frac{L(b-a)}{j}\)__
2. \(\max_{x\in[a,b]}\left|f(x)-\sum_{j=0}^{m}\theta_{[j]}P_{j}(\frac{2}{b-a}(x-a)- 1)\right|\leq\frac{\pi+\frac{2}{2}\log(2m+1)}{2(m+1)}L(b-a)\)__

Proof.: Define \(g(x)=f(\frac{b-a}{2}(x+1)+a)\), so that \(g:[\pm 1]\mapsto[\pm K]\) is \(K\)-bounded and \(L\frac{b-a}{2}\). Lipschitz. Applying Theorem D.1 yields the result. 

We next state regret guarantees for the SquareCB algorithm of Foster & Rakhlin [26] in the non-realizable setting:

**Theorem D.2** (Foster & Rakhlin [26, Theorem 5]).: _Suppose for any sequence of actions \(a_{1},\ldots,a_{T}\) an online regression oracle \(\mathcal{A}\) playing regressors \(h_{1},\ldots,h_{T}\in\mathcal{H}\) has regret guarantee_

\[R_{T}\geq\sum_{t=1}^{T}(\ell_{t}(c_{t},a_{t})-h_{t}(c_{t},a_{t}))^{2}-\min_{h \in\mathcal{H}}\sum_{t=1}^{T}(\ell_{t}(c_{t},a_{t})-h(c_{t},a_{t}))^{2} \tag{18}\]

_If all losses and regressors have range \([0,1]\) and \(\exists\ h\in\mathcal{H}\) s.t. \(\mathbb{E}\ell_{t}(a)=h(c_{t},a)+\alpha_{t}(c_{t},a)\) for \(|\alpha_{t}(a)|\leq\alpha\) then Algorithm 6 with learning rate \(\eta=2\sqrt{dT/(R_{T}+2\alpha^{2}T)}\) has expected regret w.r.t the the optimal policy \(f:[a,b]\mapsto\mathbf{g}\) bounded as_

\[\mathbb{E}\sum_{t=1}^{T}\ell_{t}(\mathbf{g}_{[i_{t}]})-\sum_{t=1}^{T}\ell_{t}(h (c_{t}))\leq 2\sqrt{dTR_{T}}+5\alpha T\sqrt{d} \tag{19}\]SquareCB requires an online regression oracle to implement, for which we will use the Follow-the-Leader scheme. It has the following guarantee for squared losses:

**Theorem D.3** (Corollary of Hazan et al. [33, Theorem 5]).: _Consider the follow-the-leader algorithm, which sequentially sees feature-target pairs \((\mathbf{x}_{1},y_{1}),\cdots,(\mathbf{x}_{T},y_{T})\in\mathcal{X}\times[0,1]\) for some subset \(\mathcal{X}\subset[0,1]^{n}\) and at each step sets \(\theta_{t+1}=\arg\min_{\theta\in\Theta}\sum_{t=1}^{T}(\langle\mathbf{x}_{t}, \theta\rangle-y_{t})^{2}\) for some subset \(\Theta\subset\mathbb{R}^{n}\). This algorithm has regret_

\[\sum_{t=1}^{T}(\langle\mathbf{x}_{t},\theta_{t}\rangle-y_{t})^{2}-\min_{\theta \in\Theta}(\langle\mathbf{x}_{t},\theta\rangle-y_{t})^{2}\leq 4B^{2}n\left(1+\log \frac{XDT}{2B}\right) \tag{20}\]

_for \(D_{\Theta}\) the diameter \(\max_{\theta,\theta^{\prime}}\|\theta-\theta^{\prime}\|_{2}\) of \(\Theta\), \(X=\max_{t\in[T]}\|\mathbf{x}_{t}\|_{2}\), and \(B=\max_{t\in[T],\theta\in\Theta}|\langle\mathbf{x}_{t},\theta\rangle|\)._

### Regret of ChebCB

**Theorem D.4**.: _Suppose \(\mathbb{E}\ell_{t}(x)\) is an \(L_{x}\)-Lipschitz function of actions \(x\in[a,b]\) and an \(L_{c}\)-Lipschitz function of contexts \(c_{t}\in[c,c+C]\). Then Algorithm 7 run with learning rate \(\eta=2\sqrt{dT/(R_{T}+2\alpha^{2}T)}\) for \(R_{T}\) and \(\alpha\) as in Equations 22 and 23, respectively, action set \(\mathbf{g}_{[i]}=a+(b-a)\frac{i-1/2}{d}\), Chebyshev features \(\mathbf{f}_{[j]}(c_{t})=P_{j}(c_{t})\), and normalizations \(L=L_{c}\) and \(N=2+\frac{4CL_{c}}{K}(1+\log m)\) has regret w.r.t. any policy \(f:[c,c+C]\mapsto[a,b]\) of_

\[\mathbb{E}\sum_{t=1}^{T}\ell_{t}(\mathbf{g}_{[i_{t}]})-\ell_{t}(f(c_{t}))= \tilde{\mathcal{O}}\left(L_{c}d\sqrt{mT}+\frac{L_{c}T\sqrt{d}}{m}+\frac{L_{x} T}{d}\right) \tag{21}\]

_Setting \(d=\Theta(T^{2/11})\) and \(m=\Theta(T^{3/11})\) yields a regret \(\tilde{\mathcal{O}}(\max\{L_{c},L_{x}\}T^{9/11})\)._Proof.: Observe that the above algorithm is equivalent to running Algorithm 6 with the follow-the-leader oracle over an \(d(m+1)\)-dimensional space \(\Theta\) with diameter \(\sqrt{\frac{d}{N^{2}}\left(1+\frac{4C^{2}L_{c}^{2}}{K^{2}}\sum_{j=1}^{m}\frac{1} {j^{2}}\right)}\leq\frac{\sqrt{dK^{2}+2dC^{2}L_{c}^{2}\pi^{2}/3}}{KN}\), features bounded by \(\sqrt{1+\sum_{j=1}^{m}P_{j}(c_{t})}\leq\sqrt{m+1}\), and predictions bounded by \(|\mathbf{f}(c),\theta\rangle|\leq\|\theta\|_{1}\|\mathbf{f}(c)\|_{\infty}\leq \frac{1}{N}+\frac{2C\ell_{c}}{KN}\sum_{j=1}^{m}\leq\frac{1}{2}\). Thus by Theorem D.3 the oracle has regret at most

\[R_{T}=d(m+1)\left(1+\log\frac{T\sqrt{d(m+1)(K^{2}+2C^{2}L_{c}^{2}\pi^{2}/3)}}{ KN}\right) \tag{22}\]

Note that, to ensure the regressors and losses have range in \([0,1]\) we can define the former as \(h(c,\mathbf{g}_{[i]})=\langle\mathbf{f}(c),\theta_{i}\rangle+\frac{1}{2}\) and the latter as \(\frac{\ell_{c}}{KN}+\frac{1}{2}\) and Algorithm 7 remains the same. Furthermore, the error of the regression approximation is then

\[\alpha=\frac{\pi+\frac{2}{\pi}\log(2m+1)}{2KN(m+1)}CL_{c} \tag{23}\]

We conclude by applying Theorem D.2, unnormalizing by multiplying the resulting regret by \(KN\), and adding the approximation error \(\frac{L_{x}(b-a)}{2d}\) due to the discretization of the action space.

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_EMPTY:25]

**Lemma F.2**.: _Let \(\mathbf{A}\) be a positive-definite matrix and \(\mathbf{b}\in\mathbb{R}^{n}\) any vector. Define_

\[U^{\varpi}(\omega)=1+\frac{\tau\log\left(\frac{\sqrt{\kappa(\mathbf{A})}}{ \varepsilon}+\sqrt{\frac{\kappa(\mathbf{A})}{\varepsilon^{2}}-1}\right)}{-\log \left(1-\frac{4}{2+\sqrt{\frac{4}{2-\omega}}+\frac{\mu(2-\omega)}{2-\omega}+ \frac{4\omega\omega}{2-\omega}}\right)} \tag{31}\]

_for \(\mu=\lambda_{\max}(\mathbf{D}\mathbf{A}^{-1})\geq 1\), \(\nu=\lambda_{\max}((\mathbf{L}\mathbf{D}^{-1}\mathbf{L}^{T}-\mathbf{D}/4) \mathbf{A}^{-1})\in[-1/4,0]\), and \(\tau\) the smallest constant (depending on \(\mathbf{A}\) and \(\mathbf{b}\)) s.t. \(U^{\varpi}\geq\mathcal{CG}(\mathbf{A},\mathbf{b},\cdot)\). Then the following holds_

1. \(\tau\in(0,1]\)__
2. _if_ \(\mu>1\) _then_ \(U^{\varpi}\) _is minimized at_ \(\omega^{*}=\frac{2}{1+\sqrt{\frac{2}{n}(1+2\nu)}}\) _and monotonically increases away from_ \(\omega^{*}\) _in both directions_
3. \(U^{\varpi}\) _is_ \(\left(\frac{\mu+4\omega+4}{4\mu\nu+2\mu-1}\tau\sqrt{\mu\sqrt{2}},2\sqrt{2}-2 \right)\)_-semi-Lipschitz on_ \([2\sqrt{2}-2,2)\)__
4. _if_ \(\mu\leq\mu_{\max}\) _then_ \(U^{\varpi}\leq 1+\frac{\tau\log\left(\frac{2}{n}\sqrt{\kappa(\mathbf{A})}\right)}{ -\log\left(1-\frac{2}{1+\sqrt{\mu_{\max}}}\right)}\) _on_ \([2\sqrt{2}-2,\frac{2}{1+1/\sqrt{\mu_{\max}}}]\)_, where_ \(\gamma\leq\frac{7+3\mu_{\max}}{8}\)_._

Proof.: By Hackbusch [32, Theorem 10.17] we have that the \(k\)th residual of SSOR-preconditioned CG satisfies

\[\|\mathbf{r}_{k}(\omega)\|_{2}=\|\mathbf{b}-\mathbf{A}\mathbf{x}_ {k}\|_{2}\leq\sqrt{\|\mathbf{A}\|}\|\mathbf{A}^{-1}\mathbf{b}-\mathbf{x}_{k}\|_ {\mathbf{A}} \leq\sqrt{\|\mathbf{A}\|}\frac{2x^{k}}{1+x^{2k}}\|\mathbf{A}^{-1} \mathbf{b}-\mathbf{x}_{0}\|_{\mathbf{A}} \tag{32}\] \[\leq\frac{2\sqrt{\kappa(\mathbf{A})}x^{k}}{1+x^{2k}}\|\mathbf{r}_ {0}\|_{2}\]

for \(x=\frac{\sqrt{\kappa(\mathbf{W}_{\omega}^{-1}\mathbf{A})}-1}{\sqrt{\kappa( \mathbf{W}_{\omega}^{-1}\mathbf{A})}+1}=1-\frac{2}{\sqrt{\kappa(\mathbf{W}_{ \omega}^{-1}\mathbf{A})}+1}\). By Axelsson [6, Theorem 7.17] we have

\[\kappa(\mathbf{\hat{W}_{\omega}^{-1}\mathbf{A}})\leq\frac{1+\frac{\mu}{4\omega }(2-\omega)^{2}+\omega\nu}{2-\omega} \tag{33}\]

Combining the two inequalities above yields the first result. For the second, we compute the derivative w.r.t. \(\omega\):

\[\frac{\partial_{\omega}U^{\varpi}}{\tau}=\frac{8(2\nu+1)\omega^{2}-4\mu(2- \omega)^{2}}{(2-\omega)\omega\sqrt{\frac{4}{2-\omega}+\frac{\mu(2-\omega)}{ \omega}+\frac{4\nu\omega}{2-\omega}}(\mu(2-\omega)^{2}+4\omega(\nu\omega+ \omega-1))} \tag{34}\]

Since \(\nu\in[-1/4,0]\) and \(\mu>1\), we have that \(\mu(2-\omega)^{2}+

#### f.4.1 Proof of Theorem a.1

Proof.: By Lemma F.2 the functions \(U_{t}-1\geq\texttt{CG}_{t}-1\) are \(\left(\frac{\mu_{t}+4\nu_{t}+4}{4\mu_{t}\nu_{t}+2\mu_{t}-1}\sqrt{\mu_{t}\sqrt{2 }},2\sqrt{2}-2\right)\)-semi-Lipschitz and \(\frac{\log\left[2\sqrt{2}\right]}{\log\sqrt{\frac{\mu_{\texttt{max}}}{\mu_{ \texttt{max}}+1+4}}}\)-bounded on \([2\sqrt{2}-2,\frac{2}{1+1/\sqrt{\mu_{\texttt{max}}}}]\); note that by the assumption on \(\min_{t}\mu_{t}\) and the fact that \(\nu_{t}\geq 1/4\) the semi-Lipschitz constant is \(\mathcal{O}(\sqrt{\mu_{t}})\). Therefore the desired regret w.r.t. any \(\omega\in[2\sqrt{2}-2,\frac{2}{1+1/\sqrt{\mu_{\texttt{max}}}}]\) follows, and extends to the rest of the interval because Lemma F.2.2 also implies all functions \(U_{t}\) are increasing away from this interval.

[MISSING_PAGE_FAIL:28]

**Lemma G.2**.: \(\|\tilde{\mathbf{C}}^{k}_{\omega}(c)\mathbf{b}\|_{2}\) _is \(\frac{10}{\lambda_{\min}(\mathbf{A})+c_{\min}}\rho(\tilde{\mathbf{C}}_{\omega})^{ k-1}(c)\|\mathbf{b}\|_{2}k\sqrt{\kappa(\mathbf{A})}\)-Lipschitz w.r.t. all \(c\geq c_{\min}>-\lambda_{\min}(\mathbf{A}(c))\), where \((c)\) denotes matrices derived from \(\mathbf{A}(c)=\mathbf{A}+c\mathbf{I}_{n}\)._

Proof.: We take the derivative as in the above proof of Lemma G.1:

\[|\partial_{c}\|\tilde{\mathbf{C}}^{k}_{\omega}(c)\mathbf{b}\|_{2}|=\rho(\tilde {\mathbf{C}}_{\omega}(c))^{k-1}\|\mathbf{b}\|_{2}k\|\mathbf{A}^{-\frac{1}{2}}( c)(\partial_{c}\tilde{\mathbf{C}}_{\omega}(c))\mathbf{A}^{\frac{1}{2}}(c)\|_{2} \sqrt{\kappa(\mathbf{A}(c))} \tag{40}\]

We then again apply the matrix calculus tool of Laue et al. [41] to get

\[\partial_{c}\tilde{\mathbf{C}}_{\omega}(c) =-\frac{2-\omega}{\omega}(\mathbf{D}(c)/\omega+\mathbf{L}^{T})^{- 1}\mathbf{D}(c)(\mathbf{D}(c)/\omega+\mathbf{L})^{-1}\] (41) \[\quad+\frac{2-\omega}{\omega^{2}}\mathbf{A}(c)(\mathbf{D}(c)/ \omega+\mathbf{L}^{T})^{-2}\mathbf{D}(c)(\mathbf{D}(c)/\omega+\mathbf{L})^{-1}\] \[\quad-\frac{2-\omega}{\omega}

### Anti-concentration

**Lemma G.3**.: _Let \(\mathbf{X}\in\mathbb{R}^{n\times n}\) be a nonzero matrix and \(\mathbf{b}=m\mathbf{u}\) be a product of independent random variables \(m\geq 0\) and \(\mathbf{u}\in\mathbb{R}^{n}\) with \(m^{2}\in[0,n]\) a \(\chi^{2}\)-squared random variable with \(n\) degrees of freedom truncated to the interval \([0,n]\) and \(\mathbf{u}\) distributed uniformly on the surface of the unit sphere. Then for any interval \(I=(\varepsilon,\varepsilon+\Delta]\subset\mathbb{R}\) for \(\varepsilon,\Delta>0\) we have that \(\Pr(\|\mathbf{X}\mathbf{b}\|_{2}\in I)\leq\frac{2\Delta}{\rho(\mathbf{X})}\sqrt {\frac{2}{\pi}}\)._

Proof.: Let \(f\) be the p.d.f. of \(\mathbf{b}\) and \(g\) be the p.d.f. of \(\mathbf{g}\sim\mathcal{N}(\mathbf{0}_{n},\mathbf{I}_{n})\). Then by the law of total probability and the fact that \(\mathbf{b}\) follows the distribution of \(\mathbf{g}\) conditioned on \(\|\mathbf{b}\|_{2}^{2}\leq n\) we have that

\[\begin{split}\Pr(\|\mathbf{X}\mathbf{b}\|_{2}\in I)& =\int_{\|\mathbf{x}\|_{2}^{2}\leq n}\Pr(\|\mathbf{X}\mathbf{b}\|_{ 2}\in I|\mathbf{b}=\mathbf{x})df(\mathbf{x})\\ &=\frac{\int_{\|\mathbf{x}\|_{2}^{2}\leq n}\Pr(\|\mathbf{X}\mathbf{ b}\|_{2}\in I|\mathbf{b}=\mathbf{x})dg(\mathbf{x})}{\int_{\|\mathbf{x}\|_{2}^{2} \geq n}dg(\mathbf{x})}\\ &\leq 2\int_{\|\mathbf{x}\|_{2}^{2}\leq n}\Pr(\|\mathbf{X}\mathbf{g} \|_{2}\in I|\mathbf{g}=\mathbf{x})dg(\mathbf{x})\\ &\leq 2\int_{\mathbb{R}^{n}}\Pr(\|\mathbf{X}\mathbf{g}\|_{2}\in I| \mathbf{g}=\mathbf{x})dg(\mathbf{x})=2\Pr(\|\mathbf{X}\mathbf{g}\|_{2}\in I) \end{split} \tag{45}\]

where the second inequality uses the fact that a \(\chi^{2}\) random variable with \(n\) degrees of freedom has more than half of its mass below \(n\). Defining the orthogonal diagonalization \(\mathbf{Q}^{T}\Lambda\mathbf{Q}=\mathbf{X}^{T}\mathbf{X}\) and noting that \(\mathbf{Q}\mathbf{g}\sim\mathcal{N}(\mathbf{0}_{n},\mathbf{I}_{n})\), we then have that

\[\|\mathbf{X}\mathbf{g}\|_{2}^{2}=(\mathbf{Q}\mathbf{g})^{T}\Lambda\mathbf{Q} \mathbf{g}=\sum_{i=1}^{n}\Lambda_{[i,i]}\chi_{i}^{2} \tag{46}\]

for i.i.d. \(\chi_{1},\ldots,\chi_{n}\sim\mathcal{N}(0,1)\). Let \(h\), \(h_{1}\), and \(h_{-1}\) be the densities of \(\sum_{i=1}^{n}\Lambda_{[i,i]}\chi_{i}^{2}\), \(\Lambda_{[1,1]}\chi_{1}^{2}\), and \(\sum_{i=2}^{n}\Lambda_{[i,i]}\chi_{i}^{2}\), respectively, and let \(u(a)\) be the uniform measure on the interval \((a,a+2\varepsilon\Delta+\Delta^{2}]\). Then since the density of the sum of independent random variables is their convolution, we can apply Young's inequality to obtain

\[\begin{split}\Pr(\|\mathbf{X}\mathbf{g}\|_{2}\in I)& =\Pr(\|\mathbf{X}\mathbf{g}\|_{2}^{2}\in(\varepsilon^{2},( \varepsilon+\Delta)^{2}])\\ &\leq\max_{a\geq\varepsilon^{2}}\int_{a}^{a+2\varepsilon\Delta+ \Delta^{2}}

### Lipschitz expectation

**Lemma G.4**.: _Suppose \(\mathbf{b}=m\mathbf{u}\), where \(m\) and \(\mathbf{u}\) are independent random variables with \(\mathbf{u}\) distributed uniformly on the surface of the unit sphere and \(m^{2}\in[0,n]\) a \(\chi^{2}\)-squared random variable with \(n\) degrees of freedom truncated to the interval \([0,n]\). Define \(K\) as in Corollary E.2, \(\beta=\min_{x}\rho(\mathbf{I}_{n}-\mathbf{D}_{x}^{-1}\mathbf{A}_{x})\), and \(\textsf{SSOR}(x)=\min_{\|\tilde{\mathbf{C}}_{x}^{k}\mathbf{b}\|_{2}\leq\varepsilon}k\) to be the number of iterations to convergence when the defect reduction matrix depends on some scalar \(x\in\mathcal{X}\) for some bounded interval \(\mathcal{X}\subset\mathbb{R}\). If \(\|\tilde{\mathbf{C}}_{x}^{k}\mathbf{b}\|_{2}\) is \(L\rho(\tilde{\mathbf{C}}_{x})^{k-1}\)-Lipschitz a.s. w.r.t. any \(x\in\mathcal{X}\) then \(\mathbb{ESOR}\) is \(\frac{32K^{3}L\sqrt{2/\pi}}{\beta^{4}}\)-Lipschitz w.r.t. \(x\)._

Proof.: First, note that by Hackbusch [32, Theorem 6.26]

\[\rho(\tilde{\mathbf{C}}_{x})=\rho(\tilde{\mathbf{M}}_{x})=\|\tilde{\mathbf{M}} _{x}^{2}\|_{\mathbf{A}_{x}}\geq\rho(\mathbf{M}_{x})^{2}\geq\left(\frac{\beta}{ 1+\sqrt{1-\beta^{2}}}\right)^{4}\geq\frac{\beta^{4}}{16} \tag{48}\]

Now consider any \(x_{1},x_{2}\in\mathcal{X}\) s.t. \(|x_{1}-x_{2}|\leq\frac{\varepsilon\beta^{4}\sqrt{\pi/2}}{2K^{3}L}\), assume w.l.o.g. that \(x_{1}<x_{2}\), and pick \(x^{\prime}\in[x_{1},x_{2}]\) with maximal \(\rho(\tilde{\mathbf{C}}_{x})\). Then setting \(\rho_{x^{\prime}}=\rho(\tilde{\mathbf{C}}_{x^{\prime}})\) we have that \(\|\tilde{\mathbf{C}}_{x_{i}}^{k}\mathbf{b}\|_{2}\) is \(L\rho_{x^{\prime}}^{k-1}\)-Lipschitz for both \(i=1,2\) and all \(k\in[K]\). Therefore starting with Jensen's inequality we have that

\[|\mathbb{ESSOR}(x_{i})-\mathbb{ESSOR}(x^{\prime})|\] \[\leq\mathbb{ESSOR}(x_{i})-\mathbb{SSOR}(x^{\prime})|\] \[=\sum_{k=1}^{K}\sum_{l=1}^{K}|k-l|\Pr(\textsf{SSOR}(x_{i})=k\cap \textsf{SSOR}(x^{\prime})=l)\] \[\leq K\sum_{k=1}^{K}\left(\sum_{l<k}\Pr(\|\tilde{\mathbf{C}}_{x_ {i}}^{l}\mathbf{b}\|_{2}>\varepsilon\cap\|\tilde{\mathbf{C}}_{x^{\prime}}^{l} \mathbf{b}\|_{2}\leq\varepsilon)+\sum_{l>k}\Pr(\|\tilde{\mathbf{C}}_{x_{i}}^{ k}\mathbf{b}\|_{2}\leq\varepsilon\cap\|\tilde{\mathbf{C}}_{x^{\prime}}^{k} \mathbf{b}\|_{2}>\varepsilon)\right)\] \[\leq K\sum_{k=1}^{K}\sum_{l<k}\Pr(\|\tilde{\mathbf{C}}_{x_{i}}^{ l}\mathbf{b}\|_{2}\in(\varepsilon-L\rho_{x^{\prime}}^{l-1}|x_{i}-x^{\prime}|, \varepsilon])\] \[\quad+K\sum_{k=1}^{K}\sum_{l>k}\Pr(\|\tilde{\mathbf{C}}_{x^{ \prime}}^{k}\mathbf{b}\|_{2}\in(\varepsilon,\varepsilon+L\rho_{x^{\prime}}^{k- 1}|x_{i}-x^{\prime}|))\] \[\leq K\sum_{k=1}^{K}\left(\sum_{l<k}\frac{2L\rho_{x^{\prime}}^{l -1}\sqrt{2/\pi}}{\rho(\tilde{\mathbf{C}}_{x^{\prime}}^{l})}|x_{i}-x^{\prime}|+ \sum_{k=1}^{K}\sum_{l>k}\frac{2L\rho_{x^{\prime}}^{k-1}\sqrt{2/\pi}}{\rho( \tilde{\mathbf{C}}_{x^{\prime}}^{k})}|x_{i}-x^{\prime}|\right)\] \[\leq\frac{2K^{3}L\sqrt{2/\pi}}{\rho_{x^{\prime}}}|x_{i}-x^{\prime }|\leq\frac{32K^{3}L\sqrt{2/\pi}}{\beta^{4}} \tag{49}\]

where the second inequality follows by the definition of SSOR, the third by Lipschitzness, and the fourth by the anti-concentration result of Lemma G.3. Since this holds for any nearby pairs \(x_{1}<x_{2}\), taking the summation over the interval \(\mathcal{X}\) completes the proof. 

**Corollary G.1**.: _Under the assumptions of Lemma F.1, the function \(\mathbb{E}_{\mathbf{b}}\textsf{SSOR}(\mathbf{A},\mathbf{b},\omega)\) is \(\frac{32K^{4}\sqrt{2n\kappa(\mathbf{A})/\pi}}{\beta^{4}}\left(\frac{1}{2-\omega _{\max}}+2\rho(\mathbf{D}\mathbf{A}^{-1})\right)\)-Lipschitz w.r.t. \(\omega\in[1,\omega_{\max}]\subset(0,2)\)._

Proof.: Apply Lemmas G.1 and F.1, noting that \(\|\mathbf{b}\|_{2}\leq\sqrt{n}\) by definition. 

**Corollary G.2**.: _Under the assumptions of Lemma F.1, the function \(\mathbb{E}_{\mathbf{b}}\textsf{SSOR}(\mathbf{A}(c),\mathbf{b},\omega)\) is \(\max_{c}\frac{320K^{4}\sqrt{2n\kappa(\mathbf{A}(c))/\pi}}{\beta^{4}(\lambda_{ \min}(\mathbf{A})+c_{\min})}\)-Lipschitz w.r.t. \(c\geq c_{\min}>-\lambda_{\min}\)._

Proof.: Apply Lemma G.2 and F.1, noting that \(\|\mathbf{b}\|_{2}\leq\sqrt{n}\) by definition.

Experimental details

All numerical results were generated in MATLAB on a laptop; code is available at [https://github.com/mkhodak/learning-to-relax](https://github.com/mkhodak/learning-to-relax). Note that, since we do not have access to problem parameters, we experimented with a few approaches to setting them automatically or heuristically on the simplest (low variance) setting below and then used the same settings for the rest of the experiments (high variance and heat equation). Furthermore, because the default step-size/learning rate settings in both algorithms are rather pessimistic, we use more aggressive time-varying approaches in practice. For Tsallis-INF we set \(\eta_{t}=2/\sqrt{t}\), which is what is used in the anytime variant [67]. As for ChebCB, we use an increasing schedule \(\eta_{t}=\mathcal{O}(t)\); note that Simchi-Levi & Xu [55] also use (a different) increasing learning rate schedule for setting inverse gap-weighted probabilities.

### Basic experiments

For the experiments in Figure 2 (center-left), we sample \(T=5\)K scalars \(c_{t}\sim\text{Beta}(2,6)\) and run Tsallis-INF, Tsallis-INF-CB ChebCB, the instance optimal policy \(\omega^{*}(c)\), and five values of \(\omega\)--evenly spaced on \([1,1.8]\)--on all instances \(\mathbf{A}_{t}=\mathbf{A}+\frac{12c_{t}-3}{20}\mathbf{I}_{n}\), in random order. Note that Figure 5 (left) contains results of the same setup, except with \(c_{t}\sim\text{Beta}(\frac{1}{2},\frac{3}{2})\), the higher-variance setting from Figure 1. The center and right figures contain results for the sub-optimal fixed \(\omega\) parameters, compared to Tsallis-INF. For both experiments the matrix \(\mathbf{A}\) is again the \(100\times 100\) Laplacian of a square-shaped domain generated in MATLAB, and the targets \(\mathbf{b}\) are re-sampled at each instance from the Gaussian truncated radially to have norm \(\leq n\). The reported results are averaged of forty trials.

### Accelerating a 2D heat equation solver

We then consider applying our methods to the task of numerical simulation of the 2D heat equation

\[\partial_{t}u(t,\mathbf{x})=\kappa(t)\Delta_{\mathbf{x}}u(t,\mathbf{x})+f(t, \mathbf{x}) \tag{50}\]

over the domain \(\mathbf{x}\in[0,1]^{2}\) and \(t\in[0,5]\). We use a five-point finite difference discretization with size denoted \(n_{\mathbf{x}}=1/\Delta_{\mathbf{x}}\), so that when an implicit time-stepping method such as Crank-Nicolson is applied with timestep \(\Delta_{t}\) the numerical simulation requires sequentially solving a sequence of linear systems \((\mathbf{A}_{t},\mathbf{b}_{t})\) with \(\mathbf{A}_{t}=\mathbf{I}_{(n_{\mathbf{x}}-1)^{2}}-\kappa((t+1/2)\Delta_{t}) \mathbf{A}\) for a fixed matrix \(\mathbf{A}\) (that depends on \(\Delta_{t}\) and \(\Delta_{\mathbf{x}}\)) corresponding to the discrete Laplacian of the system [42, Equation 12.29]. Each \(\mathbf{A}_{t}\) is positive definite, and moreover note that mathematically the setting is equivalent to an instantiation of the diagonal offset setting introduced in Section 2.4, since the linear system is equivalent to \(c_{t}\mathbf{I}_{(n_{\mathbf{x}}-1)^{2}}-\mathbf{A}=c_{t}\mathbf{b}_{t}\) for \(c_{t}=1/\kappa((t+1/2)\Delta_{t})\). However, for simplicity we will simply pass \(\kappa((t+1/2\Delta_{t}))\) as contexts to CB methods.

To complete the problem specification, define the _bump function_\(b_{\text{e},r}(\mathbf{x})\) centered at \(\mathbf{c}\in\mathbb{R}^{2}\) with radius \(r>0\) to be \(\exp\left(-\frac{1}{1-\|\mathbf{x}-\mathbf{c}\|_{2}^{2}/r^{2}}\right)\) if \(\|\mathbf{x}-\mathbf{c}\|_{2}<r\) and \(0\) otherwise. We set the initial condition \(u(0,\mathbf{x})=\mathbf{b}_{(\frac{1}{2}\)},\frac{1}{2},\frac{1}{2}(\mathbf{x})\), forcing function \(f(t,\mathbf{x})=32\mathbf{b}_{(\frac{1}{2}+\cos(16\pi t)/4,\frac{1}{2}+\cos(16 \pi t)/4),1/8}(\mathbf{x})\), and diffusion coefficient \(\kappa(t)=\exp(-2\cos(2\pi t))+B_{t}\), where \(B_{t}\) is a random process generated by simulating a Brownian motion on \(t\in[0,5]\) and then normalizing the result to be in the range \([0,e]\). The forcing function--effectively a bump circling rapidly around the center of the domain--is chosen to ensure that the linear system solutions are not too close to each other or to zero, and the diffusion coefficient function--plotted in Figure 6 (left)--is chosen to make the instance-optimal \(\omega\) behave roughly sinusoidally without being exactly so (c.f. Figure 2 (right)).

We set \(\Delta_{t}=10^{-3}\), thus making \(T=5000\), and evaluate our approach across five spatial discretizations: \(n_{\mathbf{x}}=25,50,100,200,400\). The resulting linear systems have size \(n=(n_{\mathbf{x}}-1)^{2}\). At each timestep, we solve each linear system using CG to relative precision \(\varepsilon=10^{-8}\). The baselines we consider are vanilla (unpreconditioned) CG and SSOR-CG with \(\omega=1\) or \(\omega=1.5\); as comparators we also evaluate performance when using the best fixed \(\omega\) in hindsight at each round, and when using the _instance-optimal_\(\omega\) at each round. Recall that we showed that Tsallis-INF has sublinear regret w.r.t. the surrogate cost of the best fixed \(\omega\) of SSOR-CG (Theorem A.1), and that ChebCB has sublinear regret w.r.t. the instance-optimal \(\omega\) for SOR in the semi-stochastic setting of Section 3. Since both methods are randomized, we take the average of three runs.

In Figure 2 (center-right) we show that both methods substantially outperform all three baselines, except at \(n_{\mathbf{x}}=50\) when \(\omega=1.5\) is the best fixed parameter in hindsight; furthermore, ChebCB does better then the best fixed \(\omega\) in hindsight in most cases. In Figure 6 (right) we also show that--at high-enough dimensions--this reduction in the number of iterations leads to an overall improvement in the _runtime_ of the simulation. Several other pertinent notes include:

1. At lower dimensions the learning-based approaches have slower overall runtime because of overhead associated with learning; ChebCB in particular solves a small constrained linear regression at each step. However, this overhead does _not_ scale with matrix dimension, and we expect data-driven approaches to have the greatest impact in higher dimensions.
2. Vanilla (unpreconditioned) CG is faster than SSOR-preconditioned CG with \(\omega=1\) despite having more iterations because each iteration is more costly.
3. To get a comparative sense of the scale of the improvement, we can consider the results in Li et al. [43, Table 1], who learn a (deep-learning-based) preconditioner for CG to simulate the 2D heat equation. In the precision \(10^{-8}\) case their solver takes 2.3 seconds, while Gauss-Seidel (i.e. SSOR with \(\omega=1\)) takes 2.995 seconds, a roughly 1.3x improvement. In our most closely comparable setting, Tsallis-INF and ChebCB are at least 3x faster than Gauss-Seidel (and have other advantages such as simplicity and being deployable in an online fashion without pretraining). We caveat this comparison by noting that Li et al. [43] consider a statistical, not online, learning setup, and their matrix structure may be significantly different--it results from a finite element method rather than finite differences. The only way to achieve a direct comparisons is via access to code; as of this writing it is not public.

Lastly, we give additional details for the plot in Figure 2 (right), which shows the actions taken by the various algorithms for a simulation at \(n_{\mathbf{x}}=100\). For clarity all lines are smoothed using a moving average with a window of 100, and for Tsallis-INF and ChebCB we also shade \(\pm\) one standard deviation computed over this window. The plot shows that Tsallis-INF converges to an action close to the best fixed \(\omega\) in hindsight, and that ChebCB fairly quickly follows the instance-optimal path, with the standard deviation of both decreasing over time.

Figure 6: Diffusion coefficient as a function of time (left) and normalized total wallclock time required to run 5K steps of the numerical simulation (right). The numbers within the plot corresponding to the average number of seconds required to run a step of the simulation using vanilla CG.