# Scale-invariant Optimal Sampling for Rare-events

Data with Sparse Models

 Jing Wang

Department of Statistics

University of Connecticut

Storrs, CT 06269

jing.7.wang@uconn.edu

&HaiYing Wang

Department of Statistics

University of Connecticut

Storrs, CT 06269

haiying.wang@uconn.edu

&Hao Helen Zhang

Department of Mathematics

University of Arizona

hzhang@math.arizona.edu

###### Abstract

Subsampling is effective in tackling computational challenges for massive data with rare events. Overly aggressive subsampling may adversely affect estimation efficiency, and optimal subsampling is essential to mitigate the information loss. However, existing optimal subsampling probabilities depends on data scales, and some scaling transformations may result in inefficient subsamples. This problem is more significant when there are inactive features, because their influence on the subsampling probabilities can be arbitrarily magnified by inappropriate scaling transformations. We tackle this challenge and introduce a scale-invariant optimal subsampling function in the context of sparse models, where inactive features are commonly assumed. Instead of focusing on estimating model parameters, we define an optimal subsampling function to minimize the prediction error, using adaptive lasso as an example to outline the estimation procedure and study its theoretical guarantee. We first introduce the adaptive lasso estimator for rare-events data and establish its oracle properties, thereby validating the use of subsampling. Then we derive a scale-invariant optimal subsampling function that minimizes the prediction error of the inverse probability weighted (IPW) adaptive lasso. Finally, we present an estimator based on the maximum sampled conditional likelihood (MSCL) to further improve the estimation efficiency. We conduct numerical experiments using both simulated and real-world data sets to demonstrate the performance of the proposed methods.

## 1 Introduction

Rare-events data refer to binary-response data that are highly imbalanced, i.e., the number of zeros (a.k.a "controls" or "negative instances") are possibly hundreds or thousands of times as large as the number of ones (a.k.a. "cases" or "positive instances"). This type of data is common in various fields, such as medicine, natural science, political science, and social science, where examples of rare events can be rare diseases, natural disasters, wars, and financial crises, respectively. Modern technologies also prompt us to pay more attention to rare-events data. For example, in modern online recommendation systems, clicks are usually rare events compared with nonclicks. Statistical analyses, including parameter estimation and inferences, pose unique challenges for rare-events data because of high imbalance. In addition, rare-events data often involve sparse models. For instance,rare diseases might be linked to a limited number of key genes. Therefore, researchers frequently adopt sparse models in genome-wide association studies for analyzing rare diseases. A different yet related example is the use of deep neural networks to predict click-through rates in modern online recommendation systems. These networks are typically overparameterized, necessitating methods that balance rare-events data with the sparsity of the underlying models. Data balancing is a popular approach to overcome challenges caused by imbalanced data and is usually accomplished through subsampling the zeros [5, 15] or oversampling the ones [3, 12, 16, 4]. In addition, rare-events data are often massive in order to obtain an adequate number of ones, and computation is demanding. Therefore, we focus on the subsampling approach since it addresses the imbalance issue and reduce the computational burden simultaneously.

It is shown in [20] that the efficiency of parameter estimation is essentially determined by the number of ones for rare-events logistic regression, and subsampling does not reduce the estimation efficiency as long as sufficient zeros are kept. In case of excessive removal of zeros, [22] developed an optimal sampling approach to minimize information loss. However, the optimal sampling probabilities in [22] are scale-dependent, which may lead to inefficient results. Figure 1 illustrates the issue using a simulated example, with details in Section D.1 of the appendix. We generate the data from the same logistic regression model and transform one of the covariates with different scales \(s=0.01,0.1,1,10\), and \(100\). Then we apply two optimal subsampling methods in [22], labeled with "A-OS" and "L-OS" in Figure 1. It is observed that the prediction errors of A-OS and L-OS are significantly impacted by the data scaling. The A-OS may perform similarly to the Uni (simple random sampling or uniform sampling) in Figure 1a when \(s=0.01\); so is the L-OS in Figure 1b when \(s=100\). This scale-dependent issue is not specific to logistic regression and rare-events data in [22]; it is a wide concern in literature for various data types and models, including but not limited to [1, 29, 21, 14, 26, 25, 24]. In this paper, we propose a scale-invariant optimal subsampling method to overcome the issue. It is labeled "P-OS" in Figure 1.

The scale-dependence issue can seriously impact variable selection results for sparse models, where true parameters are zero for inactive covariates. In this case, inactive variables may be arbitrarily transformed without changing the underlying model, but the A-OS or L-OS would be highly influenced and may lead to misleading results. To resolve this issue, we investigate scale-invariant optimal subsampling in the context of variable selection, for which one main goal is to distinguish active and inactive features.

Penalty-based feature selection methods are widely used. Specifically, the adaptive lasso is a popular choice due to its oracle properties, convexity, and practical ease of implementation [see 30, 28]. While penalization methods have been used for bias reduction in rare-events analysis [7], variable selection for rare-events data has not been investigated. Conducting effective variable selection is difficult in the context of rare-events data analysis, mainly due to the scarcity of information available for ones. An inaccurate variable selection result can subsequently impact both the effectiveness of optimal subsampling and the efficiency of parameter estimation. In this paper, we address the challenge of variable selection in the context of rare-events data. First, we propose the full data adaptive lasso and study its theoretical properties. Next, we introduce a novel subsampling estimator that seamlessly combines penalty-based variable selection and optimal sampling into one unified framework for rare-events data. The implementation of the adaptive lasso requires a pilot estimator to construct

Figure 1: Prediction errors with different scale transformation of the same model. (a): with non-sparse parameter \((-1,-1,-0.01,-0.01,-0.01,-0.01)^{\mathrm{T}}\). (b): with sparse parameter \((-1,0,0,0,0,0)^{\mathrm{T}}\).

data-dependent weights for covariates. Given that optimal sampling also relies on pilot estimates [see 23, 1], the adaptive lasso emerges as a natural choice for conducting variable selection method in the context of subsampled rare-events data. We validate the new estimators by proving their oracle properties and also develop an efficient algorithm to facilitate their practical implementation when handling massive real-world data sets. In summary, our main contributions are listed as follows:

* We propose scale-invariant optimal subsampling to enhance parameter estimation and variable selection. Existing optimal subsampling methods are scale-dependent, which may lead to unreliable or misleading results.
* We define adaptive lasso and establish its oracle properties for rare-events data, which show that the asymptotic variances are determined by the number of ones in the data and the active features in the model.
* We present a practical subsampling algorithm based on optimal probabilities that significantly reduces the computational burden and accelerates the optimization for penalty-based feature selection methods.

The rest of the paper is organized as follows. Section 2 introduces the model setup. Section 3 investigates nonuniform sampling and variable selection tailored for rare-events data. We propose new methods to construct scale-invariant optimal probabilities. Section 4 discusses theoretical properties of the MSCL estimator and presents a two-step algorithm to implement the proposed methods. Section 5 conducts numerical experiments on simulated and real data sets. Section 6 concludes the paper. Proofs and mathematical details are presented in the appendix.

## 2 Background and model setup

We use the subscript \({}_{\rm t}\) to indicate the true parameters. For a \(p\)-dimensional vector \(\mathbf{x}\), we use \(x_{(i)}\) to represent its \(i\)-th element. For an index subset \(\mathcal{A}\subset\{i:1,2,...,p\}\), we use \(\mathbf{x}_{(\mathcal{A})}\) to denote the subvector of \(\mathbf{x}\), whose elements correspond to the indexes in \(\mathcal{A}\). Furthermore, we use \(\mathbf{x}^{\otimes 2}\) to denote \(\mathbf{x}\mathbf{x}^{\rm T}\), use "\(\mathbf{\sim}\)" to denote convergence in distribution, use "\(\overset{P}{\longrightarrow}\)" to denote convergence in probability, and use "\(\overset{a,s}{\longrightarrow}\)" to denote convergence almost surely. We use \(\mathbf{I}\) to denote an identity matrix of a suitable dimension and use \(\mathbf{0}\) to denote a vector of zeros of a suitable dimension.

Let \((\mathbf{x}_{1},y_{1}),(\mathbf{x}_{2},y_{2}),...,(\mathbf{x}_{N},y_{N})\) denote \(N\) sample points from the joint distribution of \((\mathbf{x},y)\), where \(\{\mathbf{x}_{i}\}_{i=1}^{N}\) denote the \(p\)-dimensional predictors and \(\{y_{i}\}_{i=1}^{N}\) the binary responses. Assume that the probability of \(y\) being a one (\(y=1\)) given \(\mathbf{x}\) is

\[p(\mathbf{x};\mathbf{\theta}_{\rm t}):=\mathbb{P}(y=1|\mathbf{x})=\frac{e^{\alpha_{\rm t}+ f(\mathbf{x};\mathbf{\beta}_{\rm t})}}{1+e^{\alpha_{\rm t}+f(\mathbf{x};\mathbf{\beta}_{\rm t})}}= \frac{e^{g(\mathbf{x};\mathbf{\theta}_{\rm t})}}{1+e^{g(\mathbf{x};\mathbf{\theta}_{\rm t})}},\]

where \(\mathbf{\theta}_{\rm t}=(\alpha_{\rm t},\mathbf{\beta}_{\rm t}^{\rm T})^{\rm T}\) is the vector of true parameters and \(f(\mathbf{x};\mathbf{\beta}_{\rm t})\) is a smooth function of \(\mathbf{\beta}_{\rm t}\). For rare-events data, \(N_{1}\ll N_{0}\), where \(N_{1}=\sum_{i=1}^{N}y_{i}\) is the number of ones (i.e. \(y_{i}=1\)) and \(N_{0}=N-N_{1}\) is the number of zeros (i.e. \(y_{i}=0\)). Following the model setup used in [22], we assume that \(\alpha_{\rm t}\rightarrow-\infty\) as \(N\rightarrow\infty\), which implies that, under appropriate moment conditions,

\[\frac{N_{1}}{N_{0}}=\frac{\mathbb{E}\{p(\mathbf{x};\mathbf{\theta}_{\rm t})\}}{1- \mathbb{E}\{p(\mathbf{x};\mathbf{\theta}_{\rm t})\}}+o(1)=\mathbb{E}\{e^{\alpha_{\rm t }+f(\mathbf{x};\mathbf{\beta}_{\rm t})}\}+o(1)\to 0,\ \text{almost surely}. \tag{1}\]

Under this assumption, the asymptotic variance of the full data maximum likelihood estimator (MLE) is of order \(1/N_{1}\) instead of \(1/N\), indicating that the estimation efficiency is determined by the number of rare ones. Therefore, we can keep all the ones and sample the zeros to save computational costs. There could be a variance inflation due to aggressive subsampling, and [22] developed optimal subsampling functions to reduce the variance inflation. Specifically, the authors proposed non-uniform optimal sampling functions under the A- and L-optimality criteria, respectively, as follows: \(\varphi_{\rm A-OS}^{\rm scale}(\mathbf{x})\propto p(\mathbf{x};\mathbf{\theta}_{\rm t})\| \mathbf{M}^{-1}\hat{g}(\mathbf{x};\mathbf{\theta}_{\rm t})\|\) and \(\varphi_{\rm L-OS}^{\rm scale}(\mathbf{x})\propto p(\mathbf{x};\mathbf{\theta}_{\rm t})\| \hat{g}(\mathbf{x};\mathbf{\theta}_{\rm t})\|\), where \(\mathbf{M}=\mathbb{E}\{e^{f(\mathbf{x};\mathbf{\beta}_{\rm t})}\hat{g}^{\otimes 2}(\mathbf{x};\mathbf{ \theta}_{\rm t})\}\) and \(\hat{g}(\mathbf{x};\mathbf{\theta})\) denotes the derivative of \(g(\mathbf{x};\mathbf{\theta})\) with respect to \(\mathbf{\theta}\). However

[MISSING_PAGE_FAIL:4]

\(\lim_{N\to\infty}e^{\alpha_{\mathrm{t}}}/\rho\), and \(\hat{g}_{(\mathcal{A})}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\) consists of the elements of gradient vector \(\hat{g}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\) with indexes in the active set \(\mathcal{A}\)._

**Remark 1**.: _Theorem 1 shows that the estimation efficiency of \(\mathbf{\hat{\theta}}^{\mathrm{adp}}_{\mathrm{w}(\mathcal{A})}\) is predominantly determined by the number of ones instead of the full data size. The term \(c\mathbf{V}_{\mathrm{sub}(\mathcal{A})}\) is the variation inflation due to subsampling. The full data adaptive lasso in (2) correspond to the scenario with \(\rho=1\) and \(\varphi(\mathbf{x})=1\), for which \(c=\lim_{N\to\infty}e^{\alpha_{\mathrm{t}}}/\rho=0\). Intuitively, \(c\) can be interpreted as the imbalance rate in the subsample. If we include sufficient zeros (\(c=0\)), the subsampling does not reduce the estimation efficiency of \(\mathbf{\hat{\theta}}^{\mathrm{adp}}_{\mathrm{w}(\mathcal{A})}\)._

From Theorem 1, we see that there maybe information loss reflected as an inflated variance if \(c\neq 0\). To minimize the information loss due to sampling, we derive optimal functions as follows, where \(\varphi^{\mathrm{adp}}_{\mathrm{A-OS}}(\mathbf{x})\) corresponds to the A-optimality criterion [17] and \(\varphi^{\mathrm{adp}}_{\mathrm{L-OS}}(\mathbf{x})\) corresponds to the L-optimality criterion [17] in design of experiments. Here, the A-optimality minimizes the trace of the asymptotic variance of \(\hat{\mathbf{\theta}}^{\mathrm{adp}}_{\mathrm{w}(\mathcal{A})}\); the L-optimality focuses on the asymptotic variance of a linearly transformed estimator \(\mathbf{M}_{(\mathcal{A})}\hat{\mathbf{\theta}}^{\mathrm{adp}}_{\mathrm{w}(\mathcal{A })}\), which is proportional to \(\mathbf{M}_{\mathrm{w}(\mathcal{A})}\). The A-optimality criterion has a more direct interpretation, while an advantage of the L-optimality criterion is that the resulting optimal function is often faster to calculate.

**Proposition 1**.: _The A-optimal function that minimizes \(\text{tr}(\mathbf{V}_{\mathrm{w}(\mathcal{A})})\) is_

\[\varphi^{\mathrm{adp}}_{\mathrm{A-OS}}(\mathbf{x})=\frac{p(\mathbf{x};\mathbf{\theta}_{ \mathrm{t}})\|\mathbf{M}_{(\mathcal{A})}^{-1}\hat{g}_{(\mathcal{A})}(\mathbf{x};\mathbf{ \theta}_{\mathrm{t}})\|}{\mathbb{E}\left\{p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}}) \|\mathbf{M}_{(\mathcal{A})}^{-1}\hat{g}_{(\mathcal{A})}(\mathbf{x};\mathbf{\theta}_{ \mathrm{t}})\right\}}. \tag{4}\]

_The L-optimal function that minimizes \(\text{tr}(\mathbf{M}_{\mathrm{w}(\mathcal{A})})\) is_

\[\varphi^{\mathrm{adp}}_{\mathrm{L-OS}}(\mathbf{x})=\frac{p(\mathbf{x};\mathbf{\theta}_{ \mathrm{t}})\|\hat{g}_{(\mathcal{A})}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\|}{ \mathbb{E}\left\{p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\|\hat{g}_{(\mathcal{A})}( \mathbf{x};\mathbf{\theta}_{\mathrm{t}})\right\}}. \tag{5}\]

Unlike the optimal sampling function in [22], \(\varphi^{\mathrm{adp}}_{\mathrm{A-OS}}(\mathbf{x})\) (or \(\varphi^{\mathrm{adp}}_{\mathrm{L-OS}}(\mathbf{x})\)) relies only on the active variables. This implies that a first-step pilot estimator given by the adaptive lasso algorithm can benefit from sparse estimation methods when calculating optimal probabilities. For example, employing the standard lasso can effectively eliminate a large number of inactive variables to facilitate the computation of optimal \(\varphi^{\mathrm{adp}}_{\mathrm{A-OS}}(\mathbf{x})\) and \(\varphi^{\mathrm{adp}}_{\mathrm{L-OS}}(\mathbf{x})\). However, in practice, pilot estimators are often obtained from a small subsample size, introducing additional uncertainty. Therefore, it becomes crucial to exercise caution and be conservative by over-selecting variables during the first step to prevent the exclusion of important variables. As a consequence, although theoretically \(\varphi^{\mathrm{adp}}_{\mathrm{A-OS}}(\mathbf{x})\) and \(\varphi^{\mathrm{adp}}_{\mathrm{L-OS}}(\mathbf{x})\) do not depend on inactive variables, they are affected by inactive variables in practical implementations.

### Scale invariant optimal function

As discussed in Section 1, scaling dependent optimal probabilities may impact the performance of variable selection in practice. To address the issue, we propose to construct a scale invariant optimal function by focusing on the prediction error of an estimator \(\hat{\mathbf{\theta}}\), defined below.

\[\mathrm{MSPE}(\hat{\mathbf{\theta}})=\mathbb{E}_{\mathbf{x}}\left[\left\{p(\mathbf{x};\hat {\mathbf{\theta}})-p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\right\}^{2}\right]=\int \left\{p(\mathbf{x};\hat{\mathbf{\theta}})-p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\right\} ^{2}\mathrm{d}\mathbb{P}_{\mathbf{x}},\]

where \(\mathbb{P}_{\mathbf{x}}\) is the probability measure of \(\mathbf{x}\). The probability term \(p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\) involves both the covariates \(\mathbf{x}\) and the parameter vector \(\mathbf{\theta}_{\mathrm{t}}\), and it often does not depend on the scale of \(\mathbf{x}\). For example, in the logistic regression model, the value \(p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\) is only related to \(\mathbf{x}^{\mathrm{T}}\mathbf{\beta}_{\mathrm{t}}\). If we change the scale of \(x_{(j)}\), the value of \(\mathbf{\theta}_{\mathrm{t}}\) would change accordingly under the same data-generating model and so \(p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\) remains the same. Thus, re-scaling covariates would not affect this criterion. In the following, we give an optimal function that minimizes the prediction error.

**Theorem 2**.: _Under the assumptions of Theorem 1, for the IPW adaptive lasso estimator defined in (3), its prediction error satisfies_

\[N_{1}e^{-2\alpha_{\mathrm{t}}}\mathrm{MSPE}(\hat{\mathbf{\theta}}^{\mathrm{adp}}_{ \mathrm{w}(\mathcal{A})})\rightsquigarrow\mathbb{E}^{-1}\left\{e^{f(\mathbf{x}; \mathbf{\beta}_{\mathrm{t}})}\right\}\mathbf{Z}^{\mathrm{T}}_{(\mathcal{A})}\mathbf{M}^{1/ 2}_{\mathrm{w}(\mathcal{A})}\mathbf{M}^{-1}_{(\mathcal{A})}\mathbf{M}^{-1}_{(\mathcal{A })}\mathbf{\Omega}_{(\mathcal{A})}\mathbf{M}^{-1}_{(\mathcal{A})}\mathbf{M}^{1/2}_{\mathrm{ w}(\mathcal{A})}\mathbf{Z}_{(\mathcal{A})}. \tag{6}\]_where \(\mathbf{Z}_{(\mathcal{A})}\sim\mathbb{N}(\mathbf{0},\mathbf{I})\), and \(\mathbf{\Omega}_{(\mathcal{A})}=\mathbb{E}\left[e^{2f(\mathbf{x};\mathbf{\theta}_{\mathrm{i} })}\hat{y}_{(\mathcal{A})}^{\otimes 2}(\mathbf{x},\mathbf{\theta}_{\mathrm{t}})\right]\). The optimal function that minimizes the asymptotic mean of the prediction error in (6) is given as_

\[\varphi_{\mathrm{P-OS}}^{\mathrm{adp}}(\mathbf{x})=\frac{p(\mathbf{x};\mathbf{\theta}_{ \mathrm{t}})\|\mathbf{\Omega}_{(\mathcal{A})}^{\frac{1}{2}}\mathbf{M}_{(\mathcal{A})}^ {-1}\hat{y}_{(\mathcal{A})}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\|}{\mathbb{E} \left[p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\|\mathbf{\Omega}_{(\mathcal{A})}^{\frac{1 }{2}}\mathbf{M}_{(\mathcal{A})}^{-1}\hat{y}_{(\mathcal{A})}(\mathbf{x};\mathbf{\theta}_{ \mathrm{t}})\|\right]}. \tag{7}\]

We refer this prediction oriented criterion as P-optimality criterion. As we expect, the optimal function in (7) is unaffected by the scale of \(\mathbf{x}\) for a class of functions \(g\). The following proposition proves that \(\varphi_{\mathrm{P-OS}}^{\mathrm{adp}}(\mathbf{x})\) is invariant to rescaling of \(\mathbf{x}\).

**Proposition 2**.: _If \(g(\mathbf{x};\mathbf{\theta})\) satisfies that for every non-singular matrix \(\mathbf{A}\) there exists a non-singular matrix \(\mathbf{B}\), such that_

\[g(\mathbf{A}\mathbf{x};\mathbf{B}^{\mathrm{T}}\mathbf{\theta})=g(\mathbf{x};\mathbf{\theta}), \tag{8}\]

_then, \(\varphi_{\mathrm{P-OS}}^{\mathrm{adp}}(\mathbf{x})\) is invariant to scale changes of \(\mathbf{x}\)._

**Remark 2**.: _The condition in (8) is not restrictive and it is quite easy to satisfy. One simple example of \(g(\mathbf{x};\mathbf{\theta})\) that satisfies the condition is a linear function \(g(\mathbf{x};\mathbf{\theta})=\alpha+\mathbf{x}^{\mathrm{T}}\beta\), which corresponds to the logistic regression. The condition is also satisfied by more complex models. For example, consider an \(L\)-layer neural network_

\[g(\mathbf{x};\mathbf{W}^{1},\mathbf{W}^{2},...,\mathbf{W}^{L},\mathbf{b}^{1},...,\mathbf{b}^{L})=f^{L} (f^{L-1}(...f^{1}(\mathbf{x}^{\mathrm{T}}\mathbf{W}^{1}+\mathbf{b}^{1}))^{\mathrm{T}}\mathbf{W }+\mathbf{b}^{L}),\]

_where \(\mathbf{W}^{l}\) are the weights and \(\mathbf{b}^{l}\) are the biases in each layer, \(l=1,2,...,L\). If \(\mathbf{x}\) is rescaled to \(\mathbf{A}\mathbf{x}\), we can change \(\mathbf{W}^{1}\) to \((\mathbf{A}^{T})^{-1}\mathbf{W}^{1}\) so that the value of \(g\) does not change. That is_

\[g(\mathbf{A}\mathbf{x};(\mathbf{A}^{T})^{-1}\mathbf{W}^{1},\mathbf{W}^{2},...,\mathbf{W}^ {L},\mathbf{b}^{1},...,\mathbf{b}^{L})=f^{L}(f^{L-1}(...f^{1}(\mathbf{x}^{\mathrm{T}}\mathbf{ W}^{1}+\mathbf{b}^{1}))^{\mathrm{T}}\mathbf{W}+\mathbf{b}^{L})\] \[=g(\mathbf{x};\mathbf{W}^{1},\mathbf{W}^{2},...,\mathbf{W}^{L},\mathbf{b}^{1},...,\bm {b}^{L}).\]

## 4 Penalized MSCL estimator

The IPW estimator in (3) is not the most efficient estimator, because it assigns smaller weights for more informative data points with larger sampling probabilities. To improve the estimation efficiency, we propose the penalized MSCL estimator for variable selection given as

\[\hat{\mathbf{\theta}}_{\mathrm{mscl}}^{\mathrm{adp}}:=\operatorname*{arg\,max}_{ \mathbf{\theta}}\left\{\sum_{i=1}^{N_{\mathrm{snl}}^{\mathrm{sub}}}g(\mathbf{x}_{i}^{ \mathrm{sub}};\mathbf{\theta})-\log\{1+e^{g(\mathbf{x}_{i}^{\mathrm{sub}},\mathbf{\theta}) +I_{i}^{\mathrm{sub}}}\}]-\lambda_{N}\sum_{j=1}^{p}\frac{|\beta_{(j)}|}{|\hat{ \beta}_{\mathrm{pl}}(j)|^{\gamma}}\right\}, \tag{9}\]

where \(l_{i}^{\mathrm{sub}}=-\log\left\{\rho\varphi(\mathbf{x}_{i}^{\mathrm{sub}})\right\}\). The MSCL estimator introduced in [22] is defined as the minimizer of the objective function in (9), excluding the penalization term. In this paper, we extend this approach by proposing a penalized MSCL estimator to ensure model sparsity. We present the oracle properties of the penalized MSCL estimator in the following theorem.

**Theorem 3**.: _Let \(\hat{\mathbf{\beta}}_{\mathrm{pl}}\) be a consistent pilot estimate such that \(\lambda_{N}/(\sqrt{N_{1}}|\hat{\beta}_{\mathrm{pl}(j)}|^{\gamma})\overset{P} {\longrightarrow}\infty\) for \(j\in\mathcal{A}^{c}\). Under Assumptions 1-3 and 5, if \(\lambda_{N}/\sqrt{N_{1}}\to 0\), the estimator based on MSCL function with adaptive lasso penalty defined in (9) have the following properties:_

1. _Consistency in variable selection: The estimated active set_ \(\hat{\mathcal{A}}_{\mathrm{mscl}}:=\{j:\hat{\beta}_{\mathrm{mscl}(j)}^{\mathrm{ adp}}\neq 0\}\) _satisfies that_ \(\lim_{N\rightarrow\infty}\mathbb{P}(\hat{\mathcal{A}}_{\mathrm{mscl}}=\mathcal{A})=1\)__
2. _Asymptotic normality: The estimator of the active parameter vector satisfies that_ \[\sqrt{N_{1}}\mathbf{V}_{\mathrm{mscl}(\mathcal{A})}^{-1/2}(\hat{\mathbf{\theta}}_{ \mathrm{mscl}(\mathcal{A})}^{\mathrm{adp}}-\mathbf{\theta}_{\mathrm{t}(\mathcal{A}) })\rightsquigarrow\mathbb{N}(\mathbf{0},\mathbf{I}),\] (10) _where_ \(\mathbf{V}_{\mathrm{mscl}(\mathcal{A})}=\mathbb{E}\left\{e^{f(\mathbf{x};\mathbf{\beta}_{ \mathrm{t}})}\right\}\mathbf{\Lambda}_{\mathrm{mscl}(\mathcal{A})}^{-1}\) _and_ \(\mathbf{\Lambda}_{\mathrm{mscl}(\mathcal{A})}=\mathbb{E}\left[\frac{e^{f(\mathbf{x};\mathbf{ \beta}_{\mathrm{t}})}\hat{y}_{(\mathcal{A})}^{\otimes 2}(\mathbf{x};\mathbf{\beta}_{ \mathrm{t}})}{1+c\varphi^{-1}(\mathbf{x})\mathbf{e}^{f(\mathbf{x};\mathbf{\beta}_{\mathrm{t}}) }}\right]\)_._

The penalized MSCL estimator has the same asymptotic variance as the MSCL estimator under the true model, indicating that it is more efficient than the penalized IPW estimator [22]. We prove this by comparing the asymptotic variances and present the result in the following theorem.

**Theorem 4**.: _If the asymptotic variances \(\mathbf{V}_{\text{w}(\mathcal{A})}\) for \(\hat{\mathbf{\theta}}_{\text{w}(\mathcal{A})}^{\mathrm{adp}}\) in (2) and \(\mathbf{V}_{\mathrm{mscl}(\mathcal{A})}\) for \(\hat{\mathbf{\theta}}_{\mathrm{mscl}(\mathcal{A})}^{\mathrm{adp}}\) in (9), are finite, i.e., \(0<\mathbf{V}_{\text{w}(\mathcal{A})},\mathbf{V}_{\mathrm{mscl}(\mathcal{A})}<\infty\), then \(\mathbf{V}_{\mathrm{mscl}(\mathcal{A})}\leq\mathbf{V}_{\text{w}(\mathcal{A})}\), where the inequalities hold in the sense of Loewner ordering._

Thus, we give a practical two-step algorithm based on the penalized MSCL estimator. Since the optimal sampling functions contain unknown values and the adaptive lasso penalty also requires a consistent pilot estimator to build weights, it is natural to combine optimal sampling and the adaptive lasso into one unified framework. We recommend to use the lasso for pilot estimation. One reason is that it does estimation and variable selection simultaneously, and excluding some inactive variables improves the estimation accuracy of optimal probabilities. This also reduces the computational burden for subsequent steps. Another reason is that the lasso estimator tends to include more variables in practice and therefore has a low risk of excluding important variables in the pilot step. We present an outline of the practical implementation in Algorithm 2. More details are given in Section C.

```
1: First stage screening: * Take a pilot sample of expected sample size \(N_{\mathrm{pl}}\) using \(\{\pi(y_{i})=\rho_{0}+y_{i}(\rho_{1}-\rho_{0})\}_{i=1}^{N}\) and obtain a lasso penalized MSCL pilot estimator and an estimated active set \(\hat{\mathcal{A}}_{\mathrm{pl}}\). * Calculate approximate optimal sampling probabilities \(\{\hat{\pi}(\mathbf{x}_{i},y_{i})=y_{i}+(1-y_{i})\rho\hat{\varphi}(x_{i})\}_{i=1}^ {N}\) based on (4), (5), or (7).
2: Second stage screening: Use Algorithm 1 with the estimated optimal sampling probabilities to obtain a subsample of expected sample size \(N_{\mathrm{sub}}\) and compute the adaptive lasso penalized MSCL estimator based on \(\hat{\mathcal{A}}_{\mathrm{pl}}\).
```

**Algorithm 2** Two-step subsampling adaptive lasso algorithm

## 5 Numerical experiments

In this section, we use numerical experiments on both simulated and real data to investigate the performances of our proposed optimal subsampling and variable selection procedures.

### Simulation design

We consider a logistic regression with \(g(\mathbf{x};\mathbf{\theta})=\alpha+\mathbf{x}^{\mathrm{T}}\mathbf{\beta}\) and the following three true parameters \(\mathbf{\beta}_{\mathrm{t}}\) of dimension 50. We set different \(\alpha_{\mathrm{t}}\) so that the proportion of ones is \(0.005\):

1. **Case A:**\(\mathbf{\beta}_{\mathrm{t}}=(0.75,0.75,\mathbf{0}_{7}^{\mathrm{T}},0.75,0,0.75,0 75,\mathbf{0}_{37}^{\mathrm{T}})^{\mathrm{T}}\) and \(\alpha_{\mathrm{t}}=-5.8\).
2. **Case B:**\(\mathbf{\beta}_{\mathrm{t}}=(3,-2,\mathbf{0}_{7}^{\mathrm{T}},0.85,0,-0.75, \mathbf{0}_{38}^{\mathrm{T}})^{\mathrm{T}}\) and \(\alpha_{\mathrm{t}}=-6.2\).
3. **Case C:**\(\mathbf{\beta}_{\mathrm{t}}=(3,2,\mathbf{0}_{7}^{\mathrm{T}},0.85,\mathbf{0}_{40}^ {\mathrm{T}})^{\mathrm{T}}\) and \(\alpha_{\mathrm{t}}=-7.5\).

Here, \(\mathbf{0}_{d}\) denotes the zero vector of dimension \(d\). We use \(p_{\mathcal{A}}\) and \(p_{\mathcal{A}^{c}}\) to denote the number of active and inactive variables, respectively, and assume that \(\mathbf{x}\) is a normal random vector. The active components \(x_{(\mathcal{A},j)}\), \(1\leq j\leq p_{\mathcal{A}}\) of \(\mathbf{x}\) have variances 0.25 and the inactive components \(x_{(\mathcal{A}^{c},j)}\), \(1\leq j\leq p_{\mathcal{A}^{c}}\)of \(\mathbf{x}\) have variances \(100/p_{\mathcal{A}^{c}}^{3},100/(p_{\mathcal{A}^{c}}-1)^{3},...,100/3^{3},100/2 ^{3},100/1^{3}\). The correlation between the \(i\)-th and \(j\)-th elements of \(\mathbf{x}\) is \(0.5^{|i-j|},1\leq i,j\leq p\). We repeat our experiments \(S=500\) times generating \(N=500000\) data points in each run and use a pilot sample of size \(N_{\mathrm{pl}}=500\) for obtaining pilot estimates based on the lasso. We consider uniform sampling, the full data lasso, and the full data adaptive lasso for comparison. We use the 5-fold cross-validation and Bayesian information criterion (BIC) to determine the tuning parameter \(\lambda\) for the lasso and the adaptive lasso, and choose \(\gamma=1\) for the adaptive lasso.

#### 5.1.1 Estimation and prediction efficiency

We present the empirical median squared error (eMSE) for parameter estimation in Figure 2. All optimal sampling estimators outperform the uniform sampling. As the sampling rate increases, sampling estimators outperform the full data lasso estimator eventually in all of the three cases. Among the three optimal subsampling methods, \(\hat{\mathbf{\beta}}_{\mathrm{P-OS}}^{\mathrm{adp}}\) performs better than the other two subsampling methods.

[MISSING_PAGE_FAIL:8]

#### 5.1.3 Computational time

We present the mean computational times of different algorithms in Table 3. Our codes are written in the _julia_ programming language [2] and implemented on a Linux workstation. The lasso pathes are solved with _Lasso.jl_[13]. As shown in Table 3, subsampling algorithms significantly reduce the computational times compared with full data estimators. Although optimal sampling requires to calculate sampling probabilities, they use only about 0.77% of the computational time that the full data adaptive lasso requires. As we discussed in Section C.2, optimal sampling algorithms reduce both sample size and the data dimension. Therefore, the computational cost of the coordinate decent algorithm, which often requires a large number of iterations, is significantly reduced.

### Real data

We evaluate the performances of proposed estimators on two real data sets.

1. **Covtype data set:** It is available at [https://archive.ics.uci.edu/ml/datasets/covertype](https://archive.ics.uci.edu/ml/datasets/covertype), with \(N=581012\) observations and 54 covariates - 10 being quantitative and 44 being qualitative with dummy coding. We drop the 14th and 54th columns to avoid exact colinearity of the dummy variables. Our goal is to classify whether the forest cover type is Cottonwood/Willow (labeled as 1) or not (labeled as 0). The proportion of Cottonwood/Willow is 0.473%, which is highly imbalanced.
2. **Font data set:** It is available at [https://archive.ics.uci.edu/ml/datasets/Character+Font+Images](https://archive.ics.uci.edu/ml/datasets/Character+Font+Images), with 0.50% of the \(N=832670\) responses being the GADUGI font. The first 10 covariates are about the value, size, and style of the characters and there are additional 400 pixel values of the \(20\times 20\) images. We remove the 4th, 9th, and 10th covariates because they are constants.

For both data sets, we apply Algorithm 2 on the logarithmic-transformed data. We use pilot samples of size \(N_{\rm pl}=1000\) for the covtype data and \(N_{\rm pl}=1500\) for the font data due to its higher dimension. Since we do not know the true parameter for real data, we use area under the curve (AUC) to measure the performances of subsampling algorithms. We repeat the experiment for \(S=500\) and compute the empirical median AUC using the full data. The results are summarized in Figure 4. As shown in Figure 4, nonuniform sampling outperforms uniform sampling in general. There is one case for font data set that \(\hat{\mathbf{\beta}}^{\rm adp}_{\rm A-OS}\) is worse than the uniform sampling when the sampling rate is high. For the covtype data set, among the three estimators based on optimal sampling, \(\hat{\mathbf{\beta}}^{\rm adp}_{\rm P-OS}\) performs the best and \(\hat{\mathbf{\beta}}^{\rm adp}_{\rm L-OS}\) is worst. For the font data set, \(\hat{\mathbf{\beta}}^{\rm adp}_{\rm A-OS}\) and \(\hat{\mathbf{\beta}}^{\rm adp}_{\rm L-OS}\) are similar, and \(\hat{\mathbf{\beta}}^{\rm adp}_{\rm P-OS}\) based on the scale invariant optimal sampling function is significantly better.

## 6 Conclusion and limitations

In this paper, we investigated the problem of scale-invariant optimal subsampling in the context of variable selection for rare-events data. We derived optimal probabilities based on the A- and L

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \(\rho\) & Uni & A-OS & L-OS & P-OS \\ \hline
0.0025 & 0.168(0.017) & 0.086(0.013) & 0.088(0.013) & 0.084(0.013) \\
0.005 & 0.100(0.013) & 0.068(0.011) & 0.066(0.011) & 0.066(0.011) \\
0.0075 & 0.066(0.011) & 0.046(0.009) & 0.048(0.010) & 0.046(0optimality criteria, and discussed their limitations. Furthermore, we proposed scale-invariant optimal probabilities based on prediction errors to overcome the limitations. Both analytical and numerical results show the desirable properties of the proposed methods.

Our investigation has the following limitations.

* Our proposed criterion optimizes the probabilities by minimizing the asymptotic mean squared error in estimating rare-event probabilities. While this prioritizes the accuracy of estimation, it puts less emphasis on the quality of variable selection. Further research is needed to devise optimal probabilities that focus on variable selection performance metrics.
* Our theoretical analysis is based on asymptotic properties, with optimal probabilities defined through the asymptotic normality. Although our results may hold for sufficiently sparse models, they may not generalize to cases where the model is dense or over-parameterized, because asymptotic normality may no longer be applicable. Therefore, an important direction for future research is to study the non-asymptotic properties of our estimators, such as prediction error bounds. Non-asymptotic behaviors are particularly of interest in high-dimensional regimes.
* We employ Lasso as the pilot estimator. However, other variable selection methodologies, such as sure independence screening, can also be considered. Exploring the impact of different pilot estimators on our method's performance represents another avenue for future investigations.
* We assume that the underlying full model is correctly specified and possesses a sparse structure. Our analysis does not account for model misspecification. Further research is required to address scenarios where the model is possibly misspecified or where the number of features vastly exceeds the number of observations.

## Acknowledgments and Disclosure of Funding

The authors are grateful to Professor Kun Chen for the insightful comments and suggestions on the development of the manuscript. Funding in direct support of this work: NEI grant R21EY035710, NSF grant 2105571, UConn CLAS Research Funding in Academic Themes, GPUs donated by NVIDIA.

## References

* [1] M. Ai, J. Yu, H. Zhang, and H. Wang. Optimal subsampling algorithms for big data regressions. _Statistica Sinica_, 31(2):749-772, 2021.
* [2] J. Bezanson, A. Edelman, S. Karpinski, and V. B. Shah. Julia: A fresh approach to numerical computing. _SIAM review_, 59(1):65-98, 2017.

Figure 4: Empirical median AUCs for two real data sets

* [3] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: synthetic minority over-sampling technique. _Journal of artificial intelligence research_, 16:321-357, 2002.
* [4] G. Douzas and F. Bacao. Self-organizing map oversampling (somo) for imbalanced data set learning. _Expert systems with Applications_, 82:40-52, 2017.
* [5] C. Drummond, R. C. Holte, et al. C4. 5, class imbalance, and cost sensitivity: why undersampling beats over-sampling. In _Workshop on learning from imbalanced datasets II_, volume 11, 2003.
* [6] J. Fan and J. Lv. Sure independence screening for ultrahigh dimensional feature space. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 70(5):849-911, 2008.
* [7] D. Firth. Bias reduction of maximum likelihood estimates. _Biometrika_, 80(1):27-38, 03 1993.
* 332, 2007.
* [9] J. H. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordinate descent. _Journal of Statistical Software_, 33(1):1-22, 2010.
* [10] W. Fu and K. Knight. Asymptotics for lasso-type estimators. _The Annals of statistics_, 28(5):1356-1378, 2000.
* [11] C. J. Geyer. On the asymptotics of constrained m-estimation. _The Annals of statistics_, pages 1993-2010, 1994.
* [12] H. Han, W.-Y. Wang, and B.-H. Mao. Borderline-smote: a new over-sampling method in imbalanced data sets learning. In _International conference on intelligent computing_, pages 878-887. Springer, 2005.
* [13] JuliaStats. Lasso.jl. [https://github.com/JuliaStats/Lasso.jl](https://github.com/JuliaStats/Lasso.jl), 2022.
* [14] N. Keret and M. Gorfine. Analyzing big ehr data--optimal cox regression subsampling procedure with rare events. _Journal of the American Statistical Association_, 118(544):2262-2275, 2023.
* [15] X.-Y. Liu, J. Wu, and Z.-H. Zhou. Exploratory undersampling for class-imbalance learning. _IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)_, 39(2):539-550, 2008.
* [16] J. Mathew, C. K. Pang, M. Luo, and W. H. Leong. Classification of imbalanced data by oversampling in kernel space of support vector machines. _IEEE transactions on neural networks and learning systems_, 29(9):4065-4076, 2017.
* [17] F. Pukelsheim. _Optimal design of experiments_. SIAM, 2006.
* [18] J. Shao. _Mathematical Statistics, 2nd_. Springer-Verlag, New York, 2003.
* [19] G. Tripathi. A matrix extension of the cauchy-schwarz inequality. _Economics Letters_, 63(1):1-3, 1999.
* [20] H. Wang. Logistic regression for massive data with rare events. In _International Conference on Machine Learning_, pages 9829-9836. PMLR, 2020.
* [21] H. Wang and Y. Ma. Optimal subsampling for quantile regression in big data. _Biometrika_, 108(1):99-112, 2021.
* [22] H. Wang, A. Zhang, and C. Wang. Nonuniform negative sampling and log odds correction with rare events data. _Advances in Neural Information Processing Systems_, 34, 2021.
* [23] H. Wang, R. Zhu, and P. Ma. Optimal subsampling for large sample logistic regression. _Journal of the American Statistical Association_, 113(522):829-844, 2018.

* [24] J. Wang, J. Zou, and H. Wang. Sampling with replacement vs poisson sampling: a comparative study in optimal subsampling. _IEEE Transactions on Information Theory_, 68(10):6605-6630, 2022.
* [25] Y. Yao and H. Wang. Optimal subsampling for softmax regression. _Statistical Papers_, pages 585-599, 12 2018.
* [26] J. Yu, H. Wang, M. Ai, and H. Zhang. Optimal distributed subsampling for maximum quasi-likelihood estimators with massive data. _Journal of the American Statistical Association_, 0(0):1-12, 2020. DOI:10.1080/01621459.2020.1773832.
* [27] G.-X. Yuan, C.-H. Ho, and C.-J. Lin. An improved glmnet for l1-regularized logistic regression. _J. Mach. Learn. Res._, 13:1999-2030, 2012.
* [28] H. H. Zhang and W. Lu. Adaptive Lasso for Cox's proportional hazards model. _Biometrika_, 94(3):691-703, 05 2007.
* [29] T. Zhang, Y. Ning, and D. Ruppert. Optimal sampling for generalized linear models under measurement constraints. _Journal of Computational and Graphical Statistics_, 30(1):106-114, 2021.
* [30] H. Zou. The adaptive lasso and its oracle properties. _Journal of the American Statistical Association_, 101(476):1418-1429, 2006.

## Appendix A Appendix / supplemental material

In this appendix, we present the details of the proof, the practical algorithm and simuation settings in the paper. Details of mathematical proofs are provided in Section B. Details of the practical algorithm are provided in Section C. We present the details of simulation settings in Section D and in Section E, we give some additional simulation results.

## Appendix B Details of mathematical proofs

In this section, we provide details of mathematical proofs.

### General assumptions in the main paper

We begin with some general assumptions used throughout this paper.

**Assumption 1**.: _The first, second and third derivatives of \(f(\mathbf{x};\mathbf{\theta})\) and \(e^{f(\mathbf{x};\mathbf{\beta})}f(\mathbf{x};\mathbf{\beta})\) with respect to \(\mathbf{\beta}\) are bound by a square intergrable random variable \(B(\mathbf{x})\)._

**Assumption 2**.: _The matrix \(\mathbb{E}\left\{\hat{g}^{\otimes 2}(\mathbf{x};\mathbf{\theta})\right\}\) is finite and positive definite._

**Assumption 3**.: _The subsampling rate \(\rho\) satisfies that \(c_{N}=e^{\alpha_{\mathrm{t}}}/\rho\to c\), where \(0\leq c<\infty\) is a constant._

**Assumption 4**.: _The integral \(\mathbb{E}\left[\left\{\varphi(\mathbf{x})+\varphi^{-1}(\mathbf{x})\right\}B^{2}(\mathbf{ x})\right]\) is finite, where \(B(\mathbf{x})\) is a square-integrable function that dominates the first, second, and third derivatives of \(f(\mathbf{x};\mathbf{\theta})\) and \(e^{f(\mathbf{x};\mathbf{\beta})}f(\mathbf{x};\mathbf{\beta})\) with respect to \(\mathbf{\beta}\)._

**Assumption 5**.: _The integral \(\mathbb{E}\left\{e^{f(\mathbf{x};\mathbf{\beta})}\varphi^{-1}(\mathbf{x})B(\mathbf{x})\right\}\) is finite._

These assumptions are the same assumptions used in [22]. Here, we remind some notations used in the main paper:

\[p(\mathbf{x};\mathbf{\theta})=\frac{e^{\alpha+f(\mathbf{x};\mathbf{\beta})}}{1+e^{\alpha+f(\mathbf{ x};\mathbf{\beta})}},\]

\[\phi(\mathbf{x};\mathbf{\theta})=p(\mathbf{x};\mathbf{\theta})\left\{1-p(\mathbf{x};\mathbf{\theta}) \right\},\]

\[\mathbf{M}=\mathbb{E}\left\{e^{f(\mathbf{x}_{i};\mathbf{\beta}_{i})}\hat{g}^{\otimes 2}( \mathbf{x}_{i},\mathbf{\theta}_{t})\right\},\]\[\mathbf{\Lambda}_{\rm{mscl}}=\mathbb{E}\left[\frac{e^{f(\mathbf{x};\mathbf{\beta}_{\rm t})}\dot{g }^{\otimes 2}(\mathbf{x};\mathbf{\beta}_{\rm t})}{1+c\varphi^{-1}(\mathbf{x})e^{f(\mathbf{x}; \mathbf{\beta}_{\rm t})}}\right].\]

To ease the presentation in the following sections, we denote

\[\mathbf{M}_{\rm{w}(\mathcal{A})}=\mathbb{E}\left[\left\{1+\frac{ce^{f(\mathbf{x};\mathbf{ \beta}_{\rm t})}}{\varphi(\mathbf{x})}\right\}e^{f(\mathbf{x};\mathbf{\beta}_{\rm t})}\dot {g}^{\otimes 2}_{(\mathcal{A})}(\mathbf{x};\mathbf{\theta}_{\rm t})\right],\]

and \(a_{N}=\sqrt{Ne^{\alpha_{\rm t}}}\) in the appendix. Note that

\[N_{1}=\sum_{i=1}^{N}y_{i}=N\mathbb{E}\left\{\frac{e^{\alpha_{\rm t }+f(\mathbf{x};\mathbf{\beta}_{\rm t})}}{1+e^{\alpha_{\rm t}+f(\mathbf{x};\mathbf{\beta}_{\rm t })}}\right\}\left\{1+o_{P}(1)\right\}\] \[=Ne^{\alpha_{\rm t}}\mathbb{E}\left\{e^{f(\mathbf{x};\mathbf{\beta}_{\rm t })}\right\}\left\{1+o_{P}(1)\right\}=a_{N}^{2}\mathbb{E}\left\{e^{f(\mathbf{x}; \mathbf{\beta}_{\rm t})}\right\}\left\{1+o_{P}(1)\right\}.\]

### Proof of Theorem 1

Proof of Theorem 1.: We consider the target of IPW adaptive lasso estimator:

\[Q_{\rm{w}}(\mathbf{\theta})=-\sum_{i=1}^{N}\frac{\delta_{i}}{\pi( \mathbf{x}_{i},y_{i})}[y_{i}g(\mathbf{x}_{i};\mathbf{\theta})-\log\{1+e^{g(\mathbf{x}_{i};\mathbf{ \theta})}\}]+\lambda_{N}\sum_{j=1}^{p}\hat{w}_{j}|\beta_{(j)}|\] \[=-\ell_{\rm{w}}(\mathbf{\theta})+\lambda_{N}\sum_{j=1}^{p}\hat{w}_{j} |\beta_{(j)}|,\]

where \(\hat{w}_{j}=1/|\hat{\beta}_{\rm{pl}(j)}|,1\leq j\leq p\). Then, we have that \(\hat{\mathbf{u}}_{N}=a_{N}(\hat{\mathbf{\theta}}_{\rm{w}}-\mathbf{\theta}_{\rm{t}})\) is the minimizer of

\[\gamma_{\rm{w}}^{N}(\mathbf{u})=Q_{\rm{w}}(\mathbf{\theta}_{\rm{t}}+a_{N}^{-1}\mathbf{u}) -Q_{\rm{w}}(\mathbf{\theta}_{\rm{t}}).\]

Asymptotic normality:We prove the asymptotic normality part in this paragraph. By Taylor's expansion,

\[\gamma_{\rm{w}}^{N}(\mathbf{u}) =-\frac{1}{a_{N}}\mathbf{u}^{\rm{T}}\hat{\ell}_{\rm{w}}(\mathbf{\theta}_{ \rm{t}})+\frac{1}{2a_{N}^{2}}\sum_{i=1}^{N}\frac{\delta_{i}}{\pi(\mathbf{x}_{i},y _{i})}\phi(\mathbf{x}_{i};\mathbf{\theta}_{\rm{t}})\{\mathbf{u}^{\rm{T}}\hat{g}(\mathbf{x}_{i} ;\mathbf{\theta}_{\rm{t}})\}^{2}-\Delta_{\rm{w}}+R_{\rm{w}}\] \[+\frac{\lambda_{N}}{a_{N}}\sum_{j=1}^{p}\hat{w}_{j}a_{N}\left( \left|\beta_{\rm{t}(j)}+\frac{u_{(j)}}{a_{N}}\right|-|\beta_{\rm{t}(j)}|\right).\]

We first consider the limit behavior of the IPW target function by prove the asymptotic normality. In [22], the authors established that under Assumptions 1 to 3,

\[a_{N}^{-1}\hat{\ell}_{\rm{w}}(\mathbf{\theta}_{\rm{t}})\rightsquigarrow\mathbf{M}_{\rm {w}}^{1/2}\mathbf{Z},\]

\[\frac{1}{a_{N}^{2}}\sum_{i=1}^{N}\frac{\delta_{i}}{\pi(\mathbf{x}_{i},y_{i})}\phi( \mathbf{x}_{i};\mathbf{\theta}_{\rm{t}})\dot{g}^{\otimes 2}(\mathbf{x}_{i};\mathbf{\theta}_{ \rm{t}})\overset{P}{\longrightarrow}\mathbf{M},\]

and

\[\Delta_{\rm{w}}=o_{P}(1),\quad R_{\rm{w}}=o_{P}(1).\]

Thus,

\[-\ell_{\rm{w}}(\mathbf{\theta}_{\rm{t}})\rightsquigarrow-\mathbf{u}^{\rm{T}}\mathbf{M}_{\rm {w}}^{1/2}\mathbf{Z}+\frac{1}{2}\mathbf{u}^{\rm{T}}\mathbf{M}\mathbf{u}.\]

Next, we consider the limit behavior of the adaptive lasso penalty. Since we assume \(\hat{\mathbf{\beta}}_{\rm{pl}}\) to be a consistent estimator, we know that when \(j\in\mathcal{A}\), i.e., \(\mathbf{\beta}_{\rm{t}(j)}\neq 0\),

\[\hat{w}_{j}=|\hat{\beta}_{\rm{pl}(j)}|^{-\gamma}\overset{P}{\longrightarrow}| \beta_{(j)}|^{-\gamma}>0,\]

and

\[a_{N}\left(\left|\beta_{\rm{t}(j)}+\frac{u_{(j)}}{a_{N}}\right|-|\beta_{\rm{t} (j)}|\right)\rightarrow\text{sgn}(\beta_{\rm{t}(j)})u_{(j)}.\]Therefore, for \(j\in\mathcal{A}\), we have that

\[\frac{\lambda_{N}}{a_{N}}\hat{w}_{j}a_{N}\left(\left|\beta_{\mathrm{t}(j)}+\frac{u _{(j)}}{a_{N}}\right|-|\beta_{\mathrm{t}(j)}|\right)=o_{P}(1)\]

since \(\lambda_{N}/a_{N}=\lambda_{N}/\sqrt{Ne^{\alpha_{\mathrm{t}}}}\to 0\). On the other hand, when \(j\in\mathcal{A}^{\mathrm{c}}\), i.e., \(\beta_{\mathrm{t}(j)}=0\), we have that for \(u_{(j)}\neq 0\),

\[\frac{\lambda_{N}}{a_{N}}\hat{w}_{j}a_{N}\left(\left|\beta_{\mathrm{t}(j)}+ \frac{u_{(j)}}{a_{N}}\right|-|\beta_{\mathrm{t}(j)}|\right)=\frac{\lambda_{N}} {a_{N}}\hat{w}_{j}|u_{(j)}|=\frac{\lambda_{N}}{a_{N}|\hat{\beta}_{\mathrm{pl}( j)}|^{\gamma}}|u_{(j)}|\stackrel{{ P}}{{\longrightarrow}}\infty,\]

since \(\lambda_{N}/(\sqrt{Ne^{\alpha_{\mathrm{t}}}}|\hat{\beta}_{\mathrm{pl}(j)}|^{ \gamma})\stackrel{{ P}}{{\longrightarrow}}\infty\). Then, we have that \(\gamma_{\mathrm{w}}^{N}(\boldsymbol{u})\rightsquigarrow\gamma_{\mathrm{w}}( \boldsymbol{u})\), where

\[\gamma_{\mathrm{w}}(\boldsymbol{u})=\left\{\begin{array}{ll}\frac{1}{2} \boldsymbol{u}_{(\mathcal{A})}^{\mathrm{T}}\boldsymbol{M}_{(\mathcal{A})} \boldsymbol{u}_{(\mathcal{A})}-\boldsymbol{u}_{(\mathcal{A})}^{\mathrm{T}} \boldsymbol{M}_{\mathrm{w}}^{1/2}\boldsymbol{Z}_{(\mathcal{A})}&\text{if }u_{(j)}=0, \forall j\in\mathcal{A}^{c}\\ \infty&\text{otherwise}.\end{array}\right.\]

Note that the unique minimizer of \(\gamma_{\mathrm{w}}^{N}(\boldsymbol{u})\) is \((\boldsymbol{M}_{(\mathcal{A})}^{-1}\boldsymbol{M}_{\mathrm{w}}^{1/2} \boldsymbol{Z}_{(\mathcal{A})}^{\mathrm{T}},\boldsymbol{0})^{\mathrm{T}}\) if we put all the indexes of active variables in front. Thus, following the results of [11] and [10], we have the minimizer of \(\gamma_{\mathrm{w}}^{N}(\boldsymbol{u})\), i.e., \(\hat{\boldsymbol{u}}_{N}\), satisfies that

\[\hat{\boldsymbol{u}}_{N(\mathcal{A})}\rightsquigarrow\boldsymbol{M}_{( \mathcal{A})}^{-1}\boldsymbol{M}_{\mathrm{w}}^{1/2}\boldsymbol{Z}_{(\mathcal{A })}\text{ and }\hat{\boldsymbol{u}}_{N(\mathcal{A}^{c})}\rightsquigarrow \boldsymbol{0}.\]

Thus,

\[\hat{\boldsymbol{u}}_{N(\mathcal{A})}=a_{N}(\hat{\boldsymbol{\theta}}_{\mathrm{ w}(\mathcal{A})}-\boldsymbol{\theta}_{\mathrm{t}(\mathcal{A})})\rightsquigarrow \mathbb{N}(\boldsymbol{0},\boldsymbol{M}_{(\mathcal{A})}^{-1}\boldsymbol{M}_{ \mathrm{w}(\mathcal{A})}\boldsymbol{M}_{(\mathcal{A})}^{-1}).\]

Since

\[\sqrt{N_{1}}=a_{N}\mathbb{E}^{1/2}\left\{e^{f(\boldsymbol{x};\boldsymbol{ \beta}_{\mathrm{t}})}\right\}\left\{1+o_{P}(1)\right\},\]

applying Slusky's theorem, we have

\[\sqrt{N_{1}}\boldsymbol{V}_{\mathrm{w}(\mathcal{A})}^{-1/2}(\hat{\boldsymbol{ \theta}}_{\mathrm{w}(\mathcal{A})}-\boldsymbol{\theta}_{\mathrm{t}(\mathcal{A} )})\rightsquigarrow\mathbb{N}(\boldsymbol{0},\boldsymbol{I}).\]

Consistency in variable selectionWe prove the consistency in variable selection in this paragraph. From the result of asymptotic normality, we know that \(\hat{\beta}_{\mathrm{w}(j)}\stackrel{{ P}}{{\longrightarrow}} \beta_{\mathrm{t}(j)}\) for every \(j\in\mathcal{A}\) and therefore \(\mathbb{P}(j\in\hat{\mathcal{A}}_{\mathrm{w}})\to 1\). Thus, we only consider \(j^{\prime}\in\mathcal{A}^{c}\). When \(j^{\prime}\in\hat{\mathcal{A}}_{\mathrm{w}}\), we know that by K-K-T optimality conditions, we have

\[\lambda_{N}\hat{w}_{j^{\prime}}\text{sgn}(\hat{\beta}_{(j^{\prime})})=\hat{ \ell}_{\mathrm{w}}(\hat{\boldsymbol{\theta}}_{\mathrm{w}}),\]

which means

\[\frac{\lambda_{N}\hat{w}_{j^{\prime}}\text{sgn}(\hat{\beta}_{(j^{ \prime})})}{a_{N}}=\frac{\hat{\ell}_{\mathrm{w}}(\hat{\boldsymbol{\theta}}_{ \mathrm{w}})}{a_{N}}\] \[=\frac{\hat{\ell}_{\mathrm{w}}(\boldsymbol{\theta}_{\mathrm{t}})} {a_{N}}+\frac{a_{N}\left\{\hat{\ell}_{\mathrm{w}}(\hat{\boldsymbol{\theta}}_{ \mathrm{w}})-\hat{\ell}_{\mathrm{w}}(\boldsymbol{\theta}_{\mathrm{t}})\right\}} {a_{N}^{2}}=:I_{1}+I_{2}.\]

We have known that \(I_{1}=\hat{\ell}_{\mathrm{w}}(\boldsymbol{\theta}_{\mathrm{t}})/a_{N}\rightsquigarrow \boldsymbol{Z}_{\mathrm{w}}\). We now prove that proof that \(I_{2}=O_{P}(1)\). We apply Taylor expansion to the \(k\)-th element of \(\hat{\ell}_{\mathrm{w}}(\hat{\boldsymbol{\theta}}_{\mathrm{w}})\) and have that

\[\frac{a_{N}\left\{\hat{\ell}_{(k)}(\hat{\boldsymbol{\theta}}_{\mathrm{w}})- \hat{\ell}_{(k)}(\boldsymbol{\theta}_{\mathrm{t}})\right\}}{a_{N}^{2}}=-\frac{1 }{a_{N}^{2}}\sum_{i=1}^{N}\frac{\delta_{i}}{\pi(\boldsymbol{x},y_{i})}\phi( \boldsymbol{x}_{i};\boldsymbol{\theta}_{\mathrm{t}})\hat{g}_{(k)}(\boldsymbol {x}_{i};\boldsymbol{\theta}_{\mathrm{t}})\hat{g}^{\mathrm{T}}(\boldsymbol{x}_{i}; \boldsymbol{\theta}_{\mathrm{t}})\hat{\boldsymbol{u}}_{N}+\tilde{\Delta}_{ \mathrm{w}(k)}+\tilde{R}_{\mathrm{w}(k)},\]

where,

\[\hat{\boldsymbol{u}}_{N}=a_{N}(\hat{\boldsymbol{\theta}}_{\mathrm{w}}- \boldsymbol{\theta}_{\mathrm{t}})=O_{P}(1),\]

\[\tilde{\Delta}_{\mathrm{w}(k)}=\frac{1}{a_{N}^{2}}\sum_{i=1}^{N}\frac{\delta_{ i}}{\pi(\boldsymbol{x}_{i},y_{i})}\left\{y_{i}-p(\boldsymbol{x}_{i};\boldsymbol{\theta}_{ \mathrm{t}})\right\}\sum_{j=1}^{d}\tilde{g}_{(kj)}(\boldsymbol{x}_{i}; \boldsymbol{\theta}_{\mathrm{t}})\hat{u}_{N(j)},\]

[MISSING_PAGE_EMPTY:15]

\[\leq\frac{1}{a_{N}^{4}}\sum_{i=1}^{N}\mathbb{E}\left\{p(\mathbf{x}_{i};\mathbf{ \theta}_{1})\check{g}_{(j)}^{2}(\mathbf{x}_{i};\mathbf{\theta}_{\mathrm{t}})\right\} \leq\frac{1}{a_{N}^{2}}\mathbb{E}[e^{f(\mathbf{x};\mathbf{\beta}_{\mathrm{t}})}\|\check{ g}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\|^{2}]\to 0.\]

Thus, due to Chebyshev's inequality, we know that \(\check{\Delta}_{\mathrm{w}}=o_{P}(1)\). Since we know that \(\frac{1}{a_{N}^{4}}\sum_{i=1}^{N}\delta_{i}/\pi(\mathbf{x}_{i},y_{i})\phi(\mathbf{x}_{i };\mathbf{\theta}_{\mathrm{t}})\hat{g}^{\otimes 2}(\mathbf{x}_{i};\mathbf{\theta}_{\mathrm{t}})=H_ {\mathrm{w}}=O_{P}(1)\). Hence, we have that

\[\frac{\hat{\ell}_{\mathrm{w}}(\hat{\mathbf{\theta}}_{\mathrm{w}})}{a_{N}}=O_{P}(1).\]

Note that we also have

\[\frac{\lambda_{N}\hat{w}_{j^{\prime}}}{a_{N}}=\frac{\lambda_{N}}{a_{N}}\frac{1 }{|\hat{\beta}_{\mathrm{pl}(j^{\prime})}|^{\gamma}}\stackrel{{ P}}{{\longrightarrow}}\infty.\]

Therefore,

\[\mathbb{P}(j^{\prime}\in\hat{\mathcal{A}}_{\mathrm{w}})\leq \mathbb{P}\left\{\lambda_{N}\hat{w}_{j^{\prime}}\text{sgn}(\hat{\beta}_{(j^{ \prime})})=\hat{\ell}_{\mathrm{w}}(\hat{\mathbf{\theta}}_{\mathrm{w}})\right\}\] \[=\mathbb{P}\left\{\frac{\lambda_{N}\hat{w}_{j^{\prime}}\text{sgn} (\hat{\beta}_{(j^{\prime})})}{a_{N}}=\frac{\hat{\ell}_{\mathrm{w}}(\hat{\mathbf{ \theta}}_{\mathrm{w}})}{a_{N}}\right\}\to 0.\]

Thus, we prove the part of consistency of variable selection. 

### Proof of Proposition 1

We first give a lemma for general optimal functions.

**Lemma 1**.: _Assume that \(h(\mathbf{x})^{2}\) and \(\varphi(\mathbf{x})\) are integrable function with \(\mathbb{E}\{\varphi(\mathbf{x})\}=1\). The optimal function \(\varphi^{**}(\mathbf{x})\) that minimize the value \(\mathbb{E}\left\{\frac{h^{2}(\mathbf{x})}{\varphi(\mathbf{x})}\right\}\) is given as \(\varphi^{**}(\mathbf{x})=\frac{h(\mathbf{x})}{\mathbb{E}\{h(\mathbf{x})\}}\)._

Proof.: Appying Cauchy-Schwartz inequality, we have that

\[\mathbb{E}\{h(\mathbf{x})\}^{2}=\mathbb{E}\left\{\frac{h(\mathbf{x})}{\sqrt{\varphi( \mathbf{x})}}\sqrt{\varphi(\mathbf{x})}\right\}^{2}\leq\mathbb{E}\left\{\frac{h^{2}( \mathbf{x})}{\varphi(\mathbf{x})}\right\}\mathbb{E}\{\varphi(\mathbf{x})\}=\mathbb{E} \left\{\frac{h^{2}(\mathbf{x})}{\varphi(\mathbf{x})}\right\}.\]

Therefore, we have that \(\mathbb{E}\left\{Applying Lemma 1. We know that the minimizer is given as

\[\varphi_{\mathrm{A-OS}}(\mathbf{x})=\frac{p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\|\mathbf{M}_{ (\mathcal{A})}^{-1}\hat{g}_{(\mathcal{A})}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\|} {\mathbb{E}\left\{p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\|\mathbf{M}_{(\mathcal{A})}^{ -1}\hat{g}_{(\mathcal{A})}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\|\right\}}.\]

Next, we calculate the optimal function that minimize \(\text{tr}(\mathbf{M}_{\text{w}(\mathcal{A})})\). We have that

\[\text{tr}(\mathbf{M}_{\text{w}(\mathcal{A})})=\text{tr}\left\{\mathbb{E}\left[\left\{ 1+\frac{ce^{f(\mathbf{x};\mathbf{\beta}_{\mathrm{t}})}}{\varphi(\mathbf{x})}\right\}e^{f( \mathbf{x};\mathbf{\beta}_{\mathrm{t}})}\hat{g}_{(\mathcal{A})}^{\otimes 2}(\mathbf{x};\mathbf{ \theta}_{\mathrm{t}})\right]\right\}.\]

Therefore, we need to minimize

\[\text{tr}\left[\mathbb{E}\left\{\frac{p^{2}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\hat {g}_{(\mathcal{A})}^{\otimes 2}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})}{\varphi(\mathbf{x})} \right\}\right]=\mathbb{E}\left[\frac{p^{2}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\| \hat{g}_{(\mathcal{A})}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\|^{2}}{\varphi(\mathbf{x })}\right].\]

Applying Lemma 1. We know that the minimizer is given as

\[\varphi_{\mathrm{L-OS}}(\mathbf{x})=\frac{p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\|\hat{ g}_{(\mathcal{A})}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\|}{\mathbb{E}\left\{p(\mathbf{x}; \mathbf{\theta}_{\mathrm{t}})\|\hat{g}_{(\mathcal{A})}(\mathbf{x};\mathbf{\theta}_{\mathrm{ t}})\|\right\}}.\]

### Proof of Theorem 2

Proof.: In the proof of Thereom 1, we know that

\[a_{N}(\hat{\mathbf{\theta}}_{\text{w}(\mathcal{A})}^{\text{adp}}-\mathbf{\theta}_{ \mathrm{t}(\mathcal{A})})\rightsquigarrow\mathbf{M}_{(\mathcal{A})}^{-1}\mathbf{M}_{ \text{w}(\mathcal{A})}^{1/2}\mathbf{Z}_{(\mathcal{A})}.\]

To simplify the representation, We define a function

\[h(\mathbf{\theta})=e^{-2\alpha_{\text{t}}}\text{MSPE}(\mathbf{\theta})=e^{-2\alpha_{ \text{t}}}\mathbb{E}\left[\left\{p(\mathbf{x};\mathbf{\theta})-p(\mathbf{x};\mathbf{\theta}_{ \mathrm{t}})\right\}^{2}\right].\]

We have that

\[\frac{\partial\left\{p(\mathbf{x};\mathbf{\theta})-p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}}) \right\}^{2}}{\partial\mathbf{\theta}}=2\left\{p(\mathbf{x};\mathbf{\theta})-p(\mathbf{x};\bm {\theta}_{\mathrm{t}})\right\}\phi(\mathbf{x};\mathbf{\theta})\hat{g}(\mathbf{x};\mathbf{\theta }),\]

and

\[\frac{\partial^{2}\left\{p(\mathbf{x};\mathbf{\theta})-p(\mathbf{x};\mathbf{\theta }_{\mathrm{t}})\right\}^{2}}{\partial\mathbf{\theta}\partial\mathbf{\theta}^{\mathrm{ T}}}=2\phi^{2}(\mathbf{x};\mathbf{\theta})\hat{g}^{\otimes 2}(\mathbf{x};\mathbf{\theta})+2 \left\{p(\mathbf{x};\mathbf{\theta})-p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\right\}\frac{ \partial\phi(\mathbf{x};\mathbf{\theta})}{\partial\mathbf{\theta}}\hat{g}(\mathbf{x};\mathbf{\theta })\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+2\left\{p(\mathbf{x} ;\mathbf{\theta})-p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\right\}\phi(\mathbf{x};\mathbf{\theta })\hat{g}(\mathbf{x};\mathbf{\theta}).\]

Note that \(|p(\mathbf{x};\mathbf{\theta})-p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})|\leq 2\) and thus,

\[\left|\frac{\partial\left\{p(\mathbf{x};\mathbf{\theta})-p(\mathbf{x};\mathbf{\theta}_{\mathrm{ t}})\right\}^{2}}{\partial\mathbf{\theta}}\right|\leq 2\left|p(\mathbf{x};\mathbf{\theta})-p(\mathbf{x}; \mathbf{\theta}_{\mathrm{t}})\right||\phi(\mathbf{x};\mathbf{\theta})\hat{g}(\mathbf{x};\mathbf{ \theta})|\leq 4B(\mathbf{x}),\]

and

\[\left|\frac{\partial^{2}\left\{p(\mathbf{x};\mathbf{\theta})-p(\mathbf{x};\mathbf{ \theta}_{\mathrm{t}})\right\}^{2}}{\partial\mathbf{\theta}\partial\mathbf{\theta} \partial\mathbf{\theta}^{\mathrm{T}}}\right|\] \[\leq 2\left|\phi^{2}(\mathbf{x};\mathbf{\theta})\hat{g}^{\otimes 2}(\mathbf{x}; \mathbf{\theta})\right|+2\left|\left\{p(\mathbf{x};\mathbf{\theta})-p(\mathbf{x};\mathbf{\theta}_{ \mathrm{t}})\right\}\right|\left|\frac{\partial\phi(\mathbf{x};\mathbf{\theta})}{ \partial\mathbf{\theta}}\hat{g}(\mathbf{x};\mathbf{\theta})\right|\] \[\qquad\qquad+2\left|\left\{p(\mathbf{x};\mathbf{\theta})-p(\mathbf{x};\mathbf{ \theta}_{\mathrm{t}})\right\}\right|\left|\phi(\mathbf{x};\mathbf{\theta})\hat{g}( \mathbf{x};\mathbf{\theta})\right|\leq 10B(\mathbf{x}).\]

Hence, due to dominating convergence theorem, we know that the expectation and derivitive are exchangeable. Thus, we have that

\[\frac{\partial h(\mathbf{\theta})}{\partial\mathbf{\theta}}=e^{-2\alpha_{\text{t}}} \mathbb{E}\left[2\left\{p(\mathbf{x};\mathbf{\theta})-p(\mathbf{x};\mathbf{\theta}_{\mathrm{t}}) \right\}\phi(\mathbf{x};\mathbf{\theta})\hat{g}(\mathbf{x};\mathbf{\theta})\right],\]

[MISSING_PAGE_FAIL:18]

### Proof of Proposition 2

Proof.: First, we know that \(g(\mathbf{x};\mathbf{\theta})=g(\mathbf{A}\mathbf{x};\mathbf{B}^{\mathrm{T}}\mathbf{\theta})\). Since the equation holds for all \(\mathbf{x}\) and \(\mathbf{\theta}\), if we take derivitive with respect to \(\mathbf{\theta}\) on both sides, the equation still holds. Thus, we have that

\[\dot{g}(\mathbf{x};\mathbf{\theta})=\mathbf{B}\dot{g}(\mathbf{A}\mathbf{x};\mathbf{B}^{\mathrm{T}}\mathbf{ \theta}).\]

If we scale the whole covariate variable \(\mathbf{x}\) to \(\tilde{\mathbf{x}}=\mathbf{A}\mathbf{x}\), we need to reparameterize \(\mathbf{\theta}_{\mathrm{t}}\) to \(\tilde{\mathbf{\theta}}_{\mathrm{t}}=\mathbf{B}^{\mathrm{T}}\mathbf{\theta}_{\mathrm{t}}\) to remain the problem invariant. We have that for \(\tilde{\mathbf{x}}\) and \(\tilde{\mathbf{\theta}}_{\mathrm{t}}\)

\[\dot{g}(\tilde{\mathbf{x}};\tilde{\mathbf{\theta}}_{\mathrm{t}})=\dot{g}(\mathbf{A}\mathbf{x}; \mathbf{B}^{\mathrm{T}}\mathbf{\theta}_{\mathrm{t}})=\mathbf{B}^{-1}\dot{g}(\mathbf{x};\mathbf{ \theta}_{\mathrm{t}}).\]

Now, we know that

\[\tilde{\mathbf{M}}=\mathbb{E}\{e^{f(\tilde{\mathbf{x}};\tilde{\mathbf{\beta}}_{\mathrm{t} })}\dot{g}^{\otimes 2}(\tilde{\mathbf{x}};\tilde{\mathbf{\theta}}_{\mathrm{t}})\}=\mathbf{B}^{-1} \mathbb{E}\{e^{f(\mathbf{x};\mathbf{\beta}_{\mathrm{t}})}\dot{g}^{\otimes 2}(\mathbf{x}; \mathbf{\theta}_{\mathrm{t}})\}(\mathbf{B}^{\mathrm{T}})^{-1}=\mathbf{B}^{-1}\mathbf{M}(\mathbf{B} ^{\mathrm{T}})^{-1},\]

and

\[\tilde{\mathbf{\Omega}}=\mathbb{E}\{e^{2f(\tilde{\mathbf{x}};\tilde{\mathbf{\beta}}_{ \mathrm{t}})}\dot{g}^{\otimes 2}(\tilde{\mathbf{x}};\tilde{\mathbf{\theta}}_{\mathrm{t}})\}=\mathbf{B}^{-1} \mathbb{E}\{e^{2f(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})}\dot{g}^{\otimes 2}(\mathbf{x}; \mathbf{\theta}_{\mathrm{t}})\}(\mathbf{B}^{\mathrm{T}})^{-1}=\mathbf{B}^{-1}\mathbf{\Omega}( \mathbf{B}^{\mathrm{T}})^{-1}.\]

Thus, we have that

\[\|\tilde{\mathbf{\Omega}}^{1/2}\tilde{\mathbf{M}}^{-1}\dot{g}(\tilde{\mathbf{x }};\tilde{\mathbf{\theta}}_{\mathrm{t}})\|^{2}=\dot{g}^{\mathrm{T}}(\tilde{\mathbf{x} };\tilde{\mathbf{\theta}}_{\mathrm{t}})\tilde{\mathbf{M}}^{-1}\dot{\mathbf{\Omega}}\tilde{ \mathbf{M}}^{-1}\dot{g}(\tilde{\mathbf{x}};\tilde{\mathbf{\theta}}_{\mathrm{t}})\] \[=\dot{g}^{\mathrm{T}}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})(\mathbf{B}^{-1 })^{\mathrm{T}}(\mathbf{B}^{\mathrm{T}})\mathbf{M}^{-1}\mathbf{B}\mathbf{B}^{-1}\mathbf{\Omega}( \mathbf{B}^{\mathrm{T}})^{-1}\mathbf{B}^{\mathrm{T}}\mathbf{M}^{-1}\mathbf{B}\mathbf{B}^{-1}\dot {g}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\] \[=\dot{g}^{\mathrm{T}}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\mathbf{M}^{-1} \mathbf{\Omega}\mathbf{M}^{-1}\dot{g}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})=\|\mathbf{\Omega} ^{1/2}\mathbf{M}^{-1}\dot{g}(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})\|^{2}.\]

Therefore, the leveraging term is invariant. For the probability term, we know that is only related to value \(g(\mathbf{x};\mathbf{\theta}_{\mathrm{t}})=g(\tilde{\mathbf{x}};\tilde{\mathbf{\theta}}_{ \mathrm{t}})\), we know that it does not change after scaling inactive variables. This complete the proof. 

### Proof of Theorem 3

Proof of Theorem 3.: Consider maximum sampled conditional likehood function with adaptive lasso penalty:

\[Q_{\mathrm{mscl}}^{\hat{\mathbf{\theta}}_{\mathrm{pl}}}(\mathbf{\theta}) =-\sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\mathrm{pl}}}[y_{i }g(\mathbf{x}_{i};\mathbf{\theta})-\log\{1+e^{g(\mathbf{x}_{i};\mathbf{\theta})+l_{i}}\}]+ \lambda_{N}\sum_{j=1}^{p}\hat{w}_{j}|\beta_{(j)}|\] \[=-\ell_{\mathrm{mscl}}^{\hat{\mathbf{\theta}}_{\mathrm{pl}}}(\mathbf{ \theta})+\lambda_{N}\sum_{j=1}^{p}\hat{w}_{j}|\beta_{(j)}|,\]

where \(\hat{w}_{j}=1/|\hat{\beta}_{\mathrm{pl}(j)}|^{\gamma},1\leq j\leq p\). Then, we have that \(\hat{\mathbf{u}}_{N}=a_{N}(\hat{\mathbf{\theta}}_{\mathrm{mscl}}^{\hat{\mathbf{\theta}}_{ \mathrm{pl}}}-\mathbf{\theta}_{\mathrm{t}})\) is the minimizer of

\[\gamma_{\mathrm{mscl}}^{\hat{\mathbf{\theta}}_{\mathrm{pl}}}(\mathbf{u})=Q_{\mathrm{mscl }}^{\hat{\mathbf{\theta}}_{\mathrm{pl}}}(\mathbf{\theta}_{\mathrm{t}}+a_{N}^{-1}\mathbf{u })-Q_{\mathrm{mscl}}^{\hat{\mathbf{\theta}}_{\mathrm{pl}}}(\mathbf{\theta}_{\mathrm{t }}).\]

Asymptotic normality:We prove the asymptotic normality part in this paragraph. By Taylor's expansion,

\[\gamma_{\mathrm{mscl}}^{\hat{\mathbf{\theta}}_{\mathrm{pl}}}(\mathbf{u}) =-\frac{1}{a_{N}}\mathbf{u}^{\mathrm{T}}\ell_{\mathrm{mscl}}^{\hat{ \mathbf{\theta}}_{\mathrm{pl}}}(\mathbf{\theta}_{\mathrm{t}})+\frac{1}{2a_{N}^{2}} \sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\mathrm{pl}}}\phi_{\pi}^{\hat{ \mathbf{\theta}}_{\mathrm{pl}}}(\mathbf{x}_{i};\mathbf{\theta}_{\mathrm{t}})\{\mathbf{u}^{ \mathrm{T}}\dot{g}(\mathbf{x}_{i};\mathbf{\theta}_{\mathrm{t}})\}^{2}-\Delta_{\mathrm{ mscl}}^{\hat{\mathbf{\theta}}_{\mathrm{pl}}}+R_{\mathrm{mscl}}^{\hat{\mathbf{\theta}}_{ \mathrm{pl}}}\] \[\qquad+\frac{\lambda_{N}}{a_{N}}\sum_{j=1}^{p}\hat{w}_{j}a_{N} \left(\left|\beta_{\mathrm{t}(j)}+\frac{u_{(j)}}{a_{N}}\right|-|\beta_{\mathrm{t }(j)}|\right).\]

First, we consider the limit behavior of the MSCL function. In [22], the authors proved that under Assumptions 1 and 3,

\[a_{N}^{-1}\hat{\mathbf{\theta}}_{\mathrm{mscl}}^{\hat{\mathbf{\theta}}_{ \mathrm{pl}}}(\mathbf{\theta}_{\mathrm{t}})\rightsquigarrow(\mathbf{\Lambda}_{\mathrm{mscl }}^{\mathrm{pl}})^{1/2}\mathbf{Z}.\] \[\frac{1}{a_{N}^{2}}\sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{ \mathrm{pl}}}\phi_{\pi}^{\hat{\mathbf{\theta}}_{\mathrm{pl}}}(\mathbf{x}_{i};\mathbf{ \theta}_{\mathrm{t}})\dot{g}^{\otimes 2}(\mathbf{x}_{i};\mathbf{\theta}_{\mathrm{t}})\overset{P}{ \longrightarrow}\mathbf{\Lambda}_{\mathrm{mscl}}^{\mathrm{pl}},\]\[\Delta^{\hat{\theta}_{\rm pl}}_{\rm mscl}=o_{P}(1),\quad R^{\hat{\theta}_{\rm pl}}_{ \rm mscl}=o_{P}(1).\]

Thus,

\[-\ell^{\hat{\theta}_{\rm pl}}_{\rm w}(\boldsymbol{\theta}_{\rm t})\rightsquigarrow- \boldsymbol{u}^{\rm T}(\boldsymbol{\Lambda}^{\rm pl}_{\rm mscl})^{1/2} \boldsymbol{Z}+\frac{1}{2}\boldsymbol{u}^{\rm T}\boldsymbol{\Lambda}^{\rm pl}_{ \rm mscl}\boldsymbol{u}+o_{P}(1).\]

Next, we consider the limit behavior of the adaptive lasso penalty. Since we assume \(\hat{\boldsymbol{\beta}}_{\rm pl}\) tp be a consistent estimator, we know that when \(j\in\mathcal{A}\), i.e., \(\beta_{t(j)}\neq 0\),

\[\hat{w}_{j}=|\hat{\beta}_{\rm pl(j)}|^{-\gamma}\stackrel{{ P}}{{ \longrightarrow}}|\beta_{t(j)}|^{-\gamma}>0,\]

and

\[a_{N}\left(\left|\beta_{t(j)}+\frac{u_{(j)}}{a_{N}}\right|-|\beta_{t(j)}| \right)\to\text{sgn}(\beta_{t(j)})u_{(j)}.\]

Therefore, for \(j\in\mathcal{A}\), we have that

\[\frac{\lambda_{N}}{a_{N}}\hat{w}_{j}a_{N}\left(\left|\beta_{t(j)}+\frac{u_{(j )}}{a_{N}}\right|-|\beta_{t(j)}|\right)\stackrel{{ P}}{{ \longrightarrow}}0,\]

since \(\lambda_{N}/a_{N}=\lambda_{N}/\sqrt{Ne^{\alpha_{\rm t}}}\to 0\). On the other hand, when \(j\in\mathcal{A}^{c}\), i.e., \(\beta_{t(j)}=0\), we have that for \(u_{(j)}\neq 0\),

\[\frac{\lambda_{N}}{a_{N}}\hat{w}_{j}a_{N}\left(\left|\beta_{t(j)}+\frac{u_{(j )}}{a_{N}}\right|-|\beta_{t(j)}|\right)=\frac{\lambda_{N}}{a_{N}}\hat{w}_{j}|u _{(j)}|=\frac{\lambda_{N}}{a_{N}|\hat{\beta}_{\rm pl(j)}|^{\gamma}}|u_{(j)}| \stackrel{{ P}}{{\longrightarrow}}\infty,\]

since \(\lambda_{N}/(\sqrt{Ne^{\alpha_{\rm t}}}|\hat{\beta}_{\rm pl(j)}|^{\gamma}) \stackrel{{ P}}{{\longrightarrow}}\infty\). Then, we have that \(\gamma^{\hat{\theta}_{\rm pl}}_{\rm mscl}(\boldsymbol{u})\rightsquigarrow\gamma _{\rm mscl}(\boldsymbol{u})\), where

\[\gamma_{\rm mscl}(\boldsymbol{u})=\begin{cases}\frac{1}{2}\boldsymbol{u}^{\rm T }_{(\mathcal{A})}\boldsymbol{\Lambda}^{\rm pl}_{\rm mscl}\boldsymbol{u}_{( \mathcal{A})}-\boldsymbol{u}^{\rm T}_{(\mathcal{A})}(\boldsymbol{\Lambda}^{ \rm pl}_{\rm mscl})^{1/2}\boldsymbol{Z}_{(\mathcal{A})}&\text{if }u_{(j)}=0, \forall j\notin\mathcal{A}\\ \infty&\text{otherwise}.\end{cases}\]

Note that the unique minimizer of \(\gamma_{\rm mscl}(\boldsymbol{u})\) is \(((\boldsymbol{\Lambda}^{\rm pl}_{\rm mscl})^{-1}\boldsymbol{Z}^{\rm T}_{( \mathcal{A})},\boldsymbol{0})^{\rm T}\) if we put all the indexes of active variables in front. Thus, following the results of [11] and [10], we have the minimizer of \(\gamma^{\hat{\theta}_{\rm pl}}_{\rm mscl}(\boldsymbol{u})\), \(\hat{\boldsymbol{u}}_{N}\), satisfies that

\[\hat{\boldsymbol{u}}_{N(\mathcal{A})}\rightsquigarrow(\boldsymbol{\Lambda}^{ \rm pl}_{\rm mscl})^{1/2}\boldsymbol{Z}_{(\mathcal{A})}\text{ and }\hat{ \boldsymbol{u}}_{N(\mathcal{A}^{c})}\rightsquigarrow\boldsymbol{0}.\]

Thus,

\[\hat{\boldsymbol{u}}_{N(\mathcal{A})}=a_{N}(\hat{\theta}_{\rm mscl}(\mathcal{ A})-\boldsymbol{\theta}_{\rm t}(\mathcal{A}))\rightsquigarrow\mathbb{N}( \boldsymbol{0},(\boldsymbol{\Lambda}^{\rm pl}_{\rm mscl})^{-1}).\]

We know that

\[\sqrt{N_{1}}\boldsymbol{V}^{-1/2}_{\rm mscl}(\boldsymbol{\Lambda}^{\rm pl}_{ \rm mscl})^{-1}=a_{N}\mathbb{E}^{1/2}\left\{e^{f(\boldsymbol{x};\boldsymbol{ \beta}_{\rm t})}\right\}\mathbb{E}^{-1/2}\left\{e^{f(\boldsymbol{x};\boldsymbol {\beta}_{\rm t})}\right\}\left\{1+o_{P}(1)\right\}=a_{N}\left\{1+o_{P}(1) \right\}.\]

Hence, applying Slusky's theorem, we have

\[\sqrt{N_{1}}\boldsymbol{V}^{-1/2}_{\rm mscl}(\boldsymbol{\hat{\theta}}_{\rm mscl }(\mathcal{A})-\boldsymbol{\theta}_{\rm t}(\mathcal{A}))\rightsquigarrow \mathbb{N}(\boldsymbol{0},\boldsymbol{I}).\]

Therefore, we prove the part of asymptotic normality.

Consistency in variable selectionWe prove the consistency in variable selection in this paragraph. From the result of asymptotic normality, we know that \(\hat{\beta}_{\rm mscl(j)}\stackrel{{ P}}{{\longrightarrow}}\beta_{t (j)}\) for every \(j\in\mathcal{A}\) and therefore \(\mathbb{P}(j\in\hat{\mathcal{A}}_{\rm mscl})\to 1\). Thus, we only consider \(j^{\prime}\in\mathcal{A}^{c}\). When \(j^{\prime}\in\hat{\mathcal{A}}_{\rm mscl}\), we know that by K-K-T optimality conditions, we have

\[\lambda_{N}\hat{w}_{j^{\prime}}\text{sgn}(\hat{\beta}_{\rm mscl}(j^{\prime})) =\hat{\ell}^{\hat{\theta}_{\rm pl}}_{\rm mscl}(\hat{\theta}_{\rm mscl}),\]

which means

\[\frac{\lambda_{N}\hat{w}_{j^{\prime}}\text{sgn}(\hat{\beta}_{\rm mscl}(j^{ \prime}))}{a_{N}}=\frac{\hat{\ell}^{\hat{\theta}_{\rm pl}}_{\rm mscl}(\hat{ \theta}_{\rm mscl})}{a_{N}}\]\[=\frac{\dot{\ell}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{\theta}_{ \rm t})}{a_{N}}+\frac{a_{N}\left\{\dot{\ell}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl }}(\hat{\mathbf{\theta}}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}})-\dot{\ell}_{\rm mscl }^{\hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{\theta}_{\rm t})\right\}}{a_{N}^{2}}=I_{1}+I _{2}.\]

We have known that \(I_{1}=\dot{\ell}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{\theta}_{\rm t})/a_{ N}\rightsquigarrow\mathbf{Z}_{\rm mscl}\). We now prove that proof that \(I_{2}=O_{P}(1)\). We apply Taylor expansion to the \(k\)-th element of \(\dot{\ell}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\hat{\mathbf{\theta}}_{\rm mscl} ^{\hat{\mathbf{\theta}}_{\rm pl}})\) and have that

\[\frac{a_{N}\left\{\dot{\ell}_{(k)}^{\hat{\mathbf{\theta}}_{\rm pl}}(\hat{\mathbf{ \theta}}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}})-\dot{\ell}_{(k)}^{\hat{\mathbf{ \theta}}_{\rm pl}}(\mathbf{\theta}_{\rm t})\right\}}{a_{N}^{2}}=-\frac{1}{a_{N}^{2 }}\sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\rm pl}}\dot{\phi}_{\pi}^{\hat{ \mathbf{\theta}}_{\rm pl}}(\mathbf{x}_{i};\mathbf{\theta}_{\rm t})\dot{g}_{(k)}(\mathbf{x}_{i} ;\mathbf{\theta}_{\rm t})\dot{g}^{\rm T}(\mathbf{x}_{i};\mathbf{\theta}_{\rm t})\dot{\mathbf{ u}}_{N}+\tilde{\Delta}_{(k)}^{\hat{\mathbf{\theta}}_{\rm pl}}+\tilde{R}_{(k)}^{\hat{\mathbf{ \theta}}_{\rm pl}},\]

where,

\[\hat{\mathbf{u}}_{N}=a_{N}(\hat{\mathbf{\theta}}_{\rm mscl}^{\hat{\mathbf{ \theta}}_{\rm pl}}-\mathbf{\theta}_{\rm t})=O_{P}(1),\] \[\tilde{\Delta}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}=\frac{1}{a _{N}^{2}}\sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\rm pl}}\left\{y_{i}-p_{ \pi}^{\hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{x}_{i};\mathbf{\theta}_{\rm t})\right\}\sum_ {j=1}^{d}\tilde{g}_{(kj)}(\mathbf{x}_{i};\mathbf{\theta}_{\rm t})\tilde{u}_{(j)},\]

and

\[\tilde{R}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}=-\frac{1}{2a_{N }^{3}}\sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\rm pl}}\phi_{\pi}^{\hat{ \mathbf{\theta}}_{\rm pl}}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})\left\{1-2p_{\pi}^{ \hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})\right\}\dot{g}_ {(k)}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})\dot{\mathbf{u}}_{N}^{\rm T}\dot{g}^{\otimes 2 }(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})\dot{\mathbf{u}}_{N}\] \[\qquad\qquad\qquad\qquad\qquad\qquad-\frac{2}{2a_{N}^{3}}\sum_{i=1 }^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\rm pl}}\phi_{\pi}^{\hat{\mathbf{\theta}}_{\rm pl }}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})\left\{\hat{\mathbf{u}}_{N}^{\rm T}\frac{ \partial\dot{g}_{(k)}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})}{\partial\mathbf{\theta}} \right\}\left\{\hat{\mathbf{u}}_{N}^{\rm T}\dot{g}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k })\right\}\] \[\qquad\qquad\qquad\qquad\qquad\qquad-\frac{1}{2a_{N}^{2}}\sum_{i =1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\rm pl}}\phi_{\pi}^{\hat{\mathbf{\theta}}_{ \rm pl}}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})\dot{g}_{(k)}(\mathbf{x}_{i};\mathbf{\hat{ \theta}}_{k})\left\{\hat{\mathbf{u}}_{N}^{\rm T}\tilde{g}(\mathbf{x}_{i};\mathbf{\hat{ \theta}}_{k})\hat{\mathbf{u}}_{N}\right\}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+\frac{1}{2a_{N }^{3}}\sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\rm pl}}\left\{y_{i}-p_{ \pi}^{\hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})\right\} \tilde{\mathbf{u}}_{N}^{\rm T}\frac{\partial^{2}\dot{g}_{(k)}(\mathbf{x}_{i};\mathbf{\hat{ \theta}}_{k})}{\partial\mathbf{\theta}^{2}}\tilde{\mathbf{u}}_{N}.\]

where \(\dot{\mathbf{\theta}}_{k}\) is between \(\hat{\mathbf{\theta}}_{\rm mscl}\) and \(\mathbf{\theta}_{\rm t}\). First, we prove that \(\tilde{R}_{\rm mscl}(k)\) is \(o_{P}(1)\). We have that

\[|\tilde{R}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}|\leq\frac{\| \hat{\mathbf{u}}_{N}\|^{2}}{2a_{N}^{3}}\sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta} }_{\rm pl}}\phi_{\pi}^{\hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{ k})\left|1-2p_{\pi}^{\hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{ k})\right|\left|\dot{g}_{(k)}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})\right|\left\| \dot{g}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})\right\|^{2}\] \[\qquad\qquad\qquad\qquad\qquad\qquad+\frac{\|\hat{\mathbf{u}}_{N}\|^{2 }}{2a_{N}^{3}}\sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\rm pl}}\phi_{\pi}^{ \hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})\left\|\frac{ \partial\dot{g}_{(k)}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})}{\partial\mathbf{\theta}} \right\|\left\|\dot{g}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})\right\|\] \[\qquad\qquad\qquad\qquad\qquad\qquad+\frac{\|\hat{\mathbf{u}}_{N}\|^{2 }}{2a_{N}^{3}}\sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\rm pl}}\phi_{\pi}^{ \hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})\left|\dot{g}_ {(k)}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})\right|\left\|\tilde{g}(\mathbf{x}_{i};\mathbf{ \hat{\theta}}_{k})\right\|\] \[\qquad\qquad\qquad\qquad\qquad\qquad+\frac{\|\hat{\mathbf{u}}_{N}\|^{2 }}{2a_{N}^{3}}\sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\rm pl}}p_{\pi}^{ \hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})\left\|\frac{ \partial^{2}\dot{g}_{(k)}(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})}{\partial\mathbf{\theta}^{ 2}}\right\|+\frac{\|\hat{\mathbf{u}}_{N}\|^{2}}{2a_{N}^{3}}\sum_{i=1}^{N}\delta_{i}^{ \hat{\mathbf{\theta}}_{\rm pl}}y_{i}\left\|\frac{\partial^{2}\dot{g}_{(k)}(\mathbf{x}_{ i};\mathbf{\hat{\theta}}_{k})}{\partial\mathbf{\theta}^{2}}\right\|\] \[\qquad\qquad\qquad\qquad\leq\frac{\|\hat{\mathbf{u}}_{N}\|^{2}e^{\dot{ \mathbf{\theta}}_{k}-\alpha_{\rm t}}e^{\alpha_{\rm t}}}{2a_{N}^{3}}\sum_{i=1}^{N} \delta_{i}^{\mathbf{\theta}_{\rm pl}}e^{f(\mathbf{x}_{i};\mathbf{\hat{\beta}}_{k})-\log\{ \rho\varphi(\mathbf{x}_{i})\}}C(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})+\frac{\|\hat{\mathbf{u}} _{N}\|^{2}}{2a_{N}^{3}}\sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\rm pl}}y_{ i}B(\mathbf{x}_{i})\] \[\qquad\qquad\qquad\qquad\leq\frac{\|\hat{\mathbf{u}}_{N}\|^{2}e^{\dot{ \mathbf{\theta}}_{k}-\alpha_{\rm t}}}{2Na_{N}}\sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{ \theta}}_{\rm pl}}e^{f(\mathbf{x}_{i};\mathbf{\hat{\beta}}_{k})-\log\{\rho\varphi(\mathbf{x}_{ i})\}}C(\mathbf{x}_{i};\mathbf{\hat{\theta}}_{k})+\frac{\|\hat{\mathbf{u}}_{N}\|^{2}}{2a_{N}^{3}}\sum_{i=1}^{N} \delta_{i}^{\hat{\mathbf{\theta}}_{\rm pl}}y_{i}B(\mathbf{x}_{i})\] \[\qquad\qquad\\[\leq\frac{\|\hat{\mathbf{a}}_{N}\|^{2}e^{\hat{\mathbf{\sigma}}_{k}-\alpha_{t}}}{2Na_{N}\rho} \sum_{i=1}^{N}\delta_{i}^{\mathbf{\hat{\theta}}_{\rm pl}}\varphi^{-1}(\mathbf{x}_{i})B( \mathbf{x}_{i})+\frac{\|\hat{\mathbf{a}}_{N}\|^{2}}{2a_{N}^{3}}\sum_{i=1}^{N}y_{i}B( \mathbf{x}_{i})\]

where

\[C(\mathbf{x}_{i};\dot{\mathbf{\theta}}) =\left|\dot{g}_{(k)}(\mathbf{x}_{i};\dot{\mathbf{\theta}})\right|\left\{ \left\|\dot{g}(\mathbf{x}_{i};\dot{\mathbf{\theta}}_{k})\right\|^{2}+\left\|\tilde{g}( \mathbf{x}_{i};\dot{\mathbf{\theta}})\right\|\right\}\] \[+\left\|\frac{\partial\dot{g}_{(k)}(\mathbf{x}_{i};\dot{\mathbf{\theta}}_{ k})}{\partial\mathbf{\theta}}\right\|\left\|\dot{g}(\mathbf{x}_{i};\dot{\mathbf{\theta}}_{ k})\right\|+\left\|\frac{\partial^{2}\dot{g}_{(k)}(\mathbf{x}_{i};\dot{\mathbf{\theta}}_{ k})}{\partial\mathbf{\theta}^{2}}\right\|.\]

Therefore, we proved that \(\tilde{R}_{\rm mscl}(k)=o_{P}(1)\). Next, we prove that \(\tilde{\Delta}_{\rm mscl}(k)=o_{P}(1)\). We know that \(\mathbb{E}\left[a_{N}^{-2}\sum_{i=1}^{N}\delta_{i}^{\mathbf{\hat{\theta}}_{\rm pl} }\left\{y_{i}-p_{\pi}^{\mathbf{\hat{\theta}}_{\rm pl}}(\mathbf{x}_{i};\mathbf{\theta}_{ \rm t})\right\}\tilde{g}(\mathbf{x}_{i};\mathbf{\theta}_{\rm t})\ \Big{|}\ \hat{\mathbf{ \theta}}_{\rm pl}\right]=\mathbf{0}\). We also have that for the every element of \(a_{N}^{-2}\sum_{i=1}^{N}\left\{y_{i}-p(\mathbf{x}_{i};\mathbf{\theta}_{\rm t})\right\} \tilde{g}(\mathbf{x}_{i};\mathbf{\theta}_{\rm t})\), we have

\[\mathbb{V}\left[a_{N}^{-2}\sum_{i=1}^{N}\delta_{i}^{\mathbf{\hat{\theta}}_{\rm pl} }\left\{y_{i}-p_{\pi}^{\mathbf{\hat{\theta}}_{\rm pl}}(\mathbf{x}_{i};\mathbf{\theta}_{\rm t })\right\}\tilde{g}_{(jl)}(\mathbf{x}_{i};\mathbf{\theta}_{\rm t})\ \Big{|}\ \hat{\mathbf{ \theta}}_{\rm pl}\right]\]

\[\leq\frac{1}{a_{N}^{4}}\sum_{i=1}^{N}\mathbb{E}\left\{\delta_{i}^{\mathbf{\hat{ \theta}}_{\rm pl}}p_{\pi}^{\mathbf{\hat{\theta}}_{\rm pl}}(\mathbf{x}_{i};\mathbf{\theta}_{ \rm t})\tilde{g}_{(jl)}^{2}(\mathbf{x}_{i};\mathbf{\theta}_{\rm t})\ \Big{|}\ \hat{\mathbf{ \theta}}_{\rm pl}\right\}\leq\frac{1}{a_{N}^{2}}\mathbb{E}[e^{f(\mathbf{x};\mathbf{ \beta}_{\rm t})}\|\tilde{g}(\mathbf{x};\mathbf{\theta}_{\rm t})\|^{2}]\to 0.\]

Thus, due to Chebyshev's inequality, we know that \(\tilde{\Delta}_{\rm mscl}^{\mathbf{\hat{\theta}}_{\rm pl}}=o_{P}(1)\). Since we know that \(\frac{1}{a_{N}^{2}}\sum_{i=1}^{N}\delta_{i}^{\mathbf{\hat{\theta}}_{\rm pl}}\delta _{\pi}^{\mathbf{\hat{\theta}}_{\rm pl}}(\mathbf{x}_{i};\mathbf{\theta}_{\rm t})\hat{g}^{ \otimes 2}(\mathbf{x}_{i};\mathbf{\theta}_{\rm t})=O_{P}(1)\). Hence, we have that

\[\frac{\hat{\ell}_{\rm mscl}(\mathbf{\hat{\theta}}_{\rm mscl})}{a_{N}}=O_{P}(1).\]

Note that we also have

\[\frac{\lambda_{N}\hat{w}_{j^{\prime}}}{a_{N}}=\frac{\lambda_{N}}{a_{N}}\frac{1 }{|\hat{\beta}_{\rm pl(j^{\prime})^{\prime}}|}\stackrel{{ P}}{{\longrightarrow}}\infty.\]

Therefore,

\[\mathbb{P}(j^{\prime}\in\mathcal{A}_{N})\leq\mathbb{P}\left\{\lambda_{N}\hat{w} _{j^{\prime}}\text{sgn}(\hat{\beta}_{\rm mscl(j^{\prime})})=\ell_{\rm mscl}^{ \mathbf{\hat{\theta}}_{\rm pr}}(\hat{\mathbf{\theta}}_{\rm mscl})\right\}\]

\[=\mathbb{P}\left\{\frac{\lambda_{N}\hat{w}_{j^{\prime}}\text{sgn}(\hat{\beta }_{\rm mscl(j^{\prime})})}{a_{N}}=\frac{\ell_{\rm mscl}^{\mathbf{\hat{\theta}}_{\rm mscl }}(\hat{\mathbf{\theta}}_{\rm mscl})}{a_{N}}\right\}\to 0.\]

Thus, we prove the part of consistency of variable selection. 

### Proof of Theorem 4

Proof.: Letting \(h=1+c\{\varphi(\mathbf{x})\}^{-1}e^{f(\mathbf{x};\mathbf{\beta}_{\rm t})}\), \(\mathbf{v}=\sqrt{e^{f(\mathbf{x};\mathbf{\beta}_{\rm t})}}\hat{g}_{(\mathcal{A})}(\mathbf{x}; \tilde{\mathbf{\theta}})\), \(\mathbf{f}=h^{\frac{1}{2}}\mathbf{v}\), and \(\mathbf{g}=h^{-\frac{1}{2}}\mathbf{v}\), we have that

\[\mathbb{E}(\mathbf{gf}^{\rm T})=\mathbb{E}(\mathbf{fg}^{\rm T})=\mathbb{E}(\mathbf{vv}^{ \rm T})=\mathbb{E}\left\{e^{f(\mathbf{x};\mathbf{\beta}_{\rm t})}\hat{g}_{(\mathcal{A })}^{\otimes 2}(\mathbf{x};\mathbf{\theta}_{\rm t})\right\}=\mathbf{M}_{(\mathcal{A})},\]

\[\mathbb{E}(\mathbf{ff}^{\rm T})=\mathbb{E}(h\mathbf{vv}^{\rm T})=\mathbb{E}\left[ \left\{1+\frac{ce^{f(\mathbf{x};\mathbf{\beta}_{\rm t})}}{\varphi(\mathbf{x})}\right\}e^{f( \mathbf{x};\mathbf{\beta}_{\rm t})}\hat{g}_{(\mathcal{A})}^{\otimes 2}(\mathbf{x};\mathbf{\theta}_{\rm t })\right]=\mathbf{M}_{w(\mathcal{A})},\]

and

\[\mathbb{E}(\mathbf{gg}^{\rm T})=\mathbb{E}(h^{-1}\mathbf{vv}^{\rm T})=\mathbb{E}\left[ \frac{e^{f(\mathbf{x};\mathbf{\beta}_{\rm t})}\hat{g}_{(\mathcal{A})}^{\otimes 2}(\mathbf{x};\mathbf{ \theta}_{\rm t})}{1+c\varphi^{-1}(\mathbf{x})e^{f(\mathbf{x};\mathbf{\beta}_{\rm t})}} \right]=\mathbf{\Lambda}_{\rm mscl}(\mathcal{A}).\]

Now, applying the matrix form of Cauchy-Schwartz's inequality (see [19]), we have that

\[\mathbf{\Lambda}_{\rm mscl}(\mathcal{A})=\mathbb{E}(\mathbf{gg}^{\rm T}) \geq\mathbb{E}(\mathbf{gf}^{\rm T})\{\mathbb{E}(\mathbf{ff}^{\rm T})^{-1} \mathbb{E}(\mathbf{fg}^{\rm T})\] \[=\mathbf{M}_{(\mathcal{A})}\{\mathbf{M}_{w(\mathcal{A})}\}^{-1}\mathbf{M}_{( \mathcal{A})}=\mathbb{E}\left\{e^{f(\mathbf{x};\mathbf{\beta}_{\rm t})}\right\}\{\mathbf{V}_{ \rm w(\mathcal{A})}\}^{-1}.\]

Therefore, simple algebra shows that

\[\mathbf{V}_{\rm mscl}(\mathcal{A})=\mathbb{E}\left\{e^{f(\mathbf{x};\mathbf{\beta}_{\rm t})} \right\}\{\mathbf{\Lambda}_{\rm mscl}(\mathcal{A})\}^{-1}\leq\mathbf{V}_{\rm w( \mathcal{A})},\]

which complete the proof Details about the practical algorithm and its complexity

### Two-step algorithm

We take a pilot sample by uniform sampling with the sampling rate \(\rho_{1}=N_{\mathrm{pl}}/2N_{1}\) for the ones and \(\rho_{0}=N_{\mathrm{pl}}/2N_{0}\) for the zeros. Denote a pilot sample of actual sample size \(N_{\mathrm{pl}}^{*}\) as \(\{(\mathbf{x}_{i}^{\mathrm{pl}},y_{i}^{\mathrm{pl}})\}_{i=1}^{N_{\mathrm{pl}}^{*}}\), the pilot estimate of \(\mathbf{\theta}\) as \(\hat{\mathbf{\theta}}_{\mathrm{pl}}\), and the pilot estimate of the active set as \(\hat{\mathcal{A}}_{\mathrm{pl}}=\{j:\hat{\beta}_{\mathrm{pl}(j)}\neq 0\}\). We propose the following moment estimators of \(\mathbf{M}_{(\mathcal{A})}\) and \(\mathbf{\Omega}_{(\mathcal{A})}\):

\[\hat{\mathbf{M}}_{(\hat{\mathcal{A}}_{\mathrm{pl}})}^{\mathrm{pl}}=\frac{1}{N_{ \mathrm{pl}}}\sum_{i=1}^{N_{\mathrm{pl}}^{*}}\frac{e^{f(\mathbf{x}_{i}^{\mathrm{ pl}}\hat{\mathbf{\theta}}_{\mathrm{pl}})}\hat{g}_{(\mathcal{A}_{\mathrm{pl}})}^{ \otimes 2}(\mathbf{x}_{i}^{\mathrm{pl}};\hat{\mathbf{\theta}}_{\mathrm{pl}})}{\rho_{0} +y_{i}^{\mathrm{pl}}(\rho_{1}-\rho_{0})}, \tag{11}\]

\[\hat{\mathbf{\Omega}}_{(\hat{\mathcal{A}}_{\mathrm{pl}})}^{\mathrm{pl}}=\frac{1}{N_ {\mathrm{pl}}}\sum_{i=1}^{N_{\mathrm{pl}}^{*}}\frac{e^{2f(\mathbf{x}_{i}^{\mathrm{ pl}}\hat{\mathbf{\theta}}_{\mathrm{pl}})}\hat{g}_{(\mathcal{A}_{\mathrm{pl}})}^{ \otimes 2}(\mathbf{x}_{i}^{\mathrm{pl}};\hat{\mathbf{\theta}}_{\mathrm{pl}})}{\rho_{0} +y_{i}^{\mathrm{pl}}(\rho_{1}-\rho_{0})}, \tag{12}\]

respectively. We also use the following moment estimator to estimate the denominator of (4):

\[\frac{1}{N_{\mathrm{pl}}}\sum_{i=1}^{N_{\mathrm{pl}}^{*}}\frac{\omega_{i}^{ \mathrm{A}-\mathrm{OS}}}{\rho_{0}+y_{i}^{\mathrm{pl}}(\rho_{1}-\rho_{0})}, \tag{13}\]

where \(\omega_{i}^{\mathrm{A}-\mathrm{OS}}=p(\mathbf{x}_{i}^{\mathrm{pl}};\hat{\mathbf{\theta }}_{\mathrm{pl}})\|(\hat{\mathbf{M}}_{(\hat{\mathcal{A}}_{\mathrm{pl}})}^{\mathrm{ pl}})^{-1}\hat{g}_{(\mathcal{A}_{\mathrm{pl}})}(\mathbf{x}_{i}^{\mathrm{pl}}; \hat{\mathbf{\theta}}_{\mathrm{pl}})\|\). If using (5) or (7), we use

\[\omega_{i}^{\mathrm{L-OS}}=p(\mathbf{x}_{i}^{\mathrm{pl}};\hat{\mathbf{ \theta}}_{\mathrm{pl}})\|\hat{g}_{(\hat{\mathcal{A}}_{\mathrm{pl}})}(\mathbf{x}_{i }^{\mathrm{pl}};\hat{\mathbf{\theta}}_{\mathrm{pl}})\|,\text{ or}\] \[\omega_{i}^{\mathrm{P-OS}}=p(\mathbf{x}_{i}^{\mathrm{pl}};\hat{\mathbf{ \theta}}_{\mathrm{pl}})\|(\hat{\mathbf{\Omega}}_{(\hat{\mathcal{A}}_{\mathrm{pl}} )}^{\mathrm{pl}})^{1/2}(\hat{\mathbf{M}}_{(\hat{\mathcal{A}}_{\mathrm{pl}})}^{ \mathrm{pl}})^{-1}\hat{g}_{(\hat{\mathcal{A}}_{\mathrm{pl}})}(\mathbf{x}_{i}^{ \mathrm{pl}};\hat{\mathbf{\theta}}_{\mathrm{pl}})\|,\]

respectively, instead of \(\omega_{i}^{\mathrm{A}-\mathrm{OS}}\) in (13). Now, we present the proposed two-step procedure in Algorithm 3 with more details than Algorithm 2 in Section 4.

```
1:
* Take a pilot sample \(\{(\mathbf{x}_{i}^{\mathrm{pl}},y_{i}^{\mathrm{pl}})\}_{i=1}^{N_{\mathrm{pl}}^{*}}\) of expected sample size \(N_{\mathrm{pl}}\) using \(\{\pi(y_{i})=\rho_{0}+y_{i}(\rho_{1}-\rho_{0})\}_{i=1}^{N}\) and obtain a pilot estimator \[\hat{\mathbf{\theta}}_{\mathrm{pl}}:=\arg\max_{\mathbf{\theta}}\left\{\

**Remark 3**.: _Our algorithm naturally integrates the MSCL function with the adaptive lasso penalty. It can also be implemented when \(p>N\) as long as the dimension of selected variables is smaller than \(N\) in the first-stage screening. If the model is sparse and the data are massive, this is usually possible in practice. Screening algorithms such as sure independence screening [6] can also be used for the first stage screening to guarantee that the dimension of second-stage screening is smaller than the subsample size. Furthermore, the first stage screening can help to speed up the computation, as shown by the analysis of computational complexity in the next section._

We consider a coordinate descent method to calculate the estimators defined in Algorithm 3. (see [8], [9] and [27]). In each cycle, we need to find an optimal direction \(\mathbf{d}\) at a starting point \(\tilde{\mathbf{\theta}}\). We consider the quadratic approximation of \(Q_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\tilde{\mathbf{\theta}}+\mathbf{d})-Q_{\rm mscl }^{\hat{\mathbf{\theta}}_{\rm pl}}(\tilde{\mathbf{\theta}})\), which is

\[Q_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\tilde{\mathbf{\theta}}+\mathbf{ d})-Q_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\tilde{\mathbf{\theta}})\] \[=\sum_{i=1}^{N}\delta_{i}[-y_{i}g(\mathbf{x}_{i};\tilde{\mathbf{\theta}}+ \mathbf{d})+\log\{1+e^{g(\mathbf{x}_{i};\tilde{\mathbf{\theta}}+\mathbf{d})+l_{i}}\}]+\lambda_ {N}\sum_{j=1}^{p}\hat{w}_{j}|\beta_{(j)}+d_{(j)}|\] \[\quad-\sum_{i=1}^{N}\delta_{i}[-y_{i}g(\mathbf{x}_{i};\tilde{\mathbf{ \theta}})-\log\{1+e^{g(\mathbf{x}_{i};\tilde{\mathbf{\theta}})+l_{i}}\}]+\lambda_{N} \sum_{j=1}^{p}\hat{w}_{j}|\beta_{(j)}|\] \[\approx\hat{\mathbf{\ell}}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}( \tilde{\mathbf{\theta}})^{T}\mathbf{d}+\frac{1}{2}\mathbf{d}^{T}\hat{\mathbf{\ell}}_{\rm mscl} ^{\hat{\mathbf{\theta}}_{\rm pl}}(\tilde{\mathbf{\theta}})\mathbf{d}+\lambda_{N}\sum_{j=1} ^{p}\left\{\hat{w}_{j}|\tilde{\beta}_{(j)}+d_{(j)}|-\hat{w}_{j}|\tilde{\beta}_ {(j)}|\right\}\]

where \(\hat{\mathbf{\ell}}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\tilde{\mathbf{\theta}})= -\sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\rm pl}}\left\{y_{i}-p_{\pi}^{ \hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{x}_{i},\tilde{\mathbf{\theta}})\right\}\hat{g}(\bm {x}_{i};\tilde{\mathbf{\theta}})\) and \(\hat{\mathbf{\ell}}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\tilde{\mathbf{\theta}})= \sum_{i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\rm pl}}\hat{\mathbf{\theta}}_{\rm Pl }^{\hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{x}_{i},\tilde{\mathbf{\theta}})\hat{g}^{\otimes 2} (\mathbf{x}_{i};\tilde{\mathbf{\theta}})\). Thus, using coordinate descent to obtain the optimal direction, the quadratic approximation for the \(j\)-th element is given as

\[Q_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{d}+z\mathbf{e}_{j})-Q_{ \rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{d}) =\hat{\ell}_{\rm mscl}(j)(\tilde{\mathbf{\theta}})z+\left\{\hat{\ell}_{ \rm mscl}(\tilde{\mathbf{\theta}})\mathbf{d}\right\}_{(j)}z+\frac{1}{2}\hat{\ell}_{\rm mscl }(jj)(\tilde{\mathbf{\theta}})z^{2}\] \[\quad+\lambda_{N}\hat{w}_{j}|\tilde{\beta}_{(j)}+d_{(j)}+z|- \lambda_{N}\hat{w}_{j}|\tilde{\beta}_{(j)}+d_{(j)}|.\]

Then, we have the value of \(z\) that minimize \(Q_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{d}+z\mathbf{e}_{j})-Q_{\rm mscl}^{ \hat{\mathbf{\theta}}_{\rm pl}}(\mathbf{d})\) is

\[z^{**}=\left\{\begin{array}{ll}\frac{\ell_{\rm mscl}^{\hat{\mathbf{\theta}}_{ \rm pl}}(\tilde{\mathbf{\theta}})+\left\{\hat{\mathbf{\ell}}_{\rm mscl}^{\hat{\mathbf{ \theta}}_{\rm pl}}(\tilde{\mathbf{\theta}})\mathbf{d}\right\}_{(j)}^{+\lambda\hat{w}_{ j}}}{-\ell_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\tilde{\mathbf{\theta}})}&\mbox{if }\tilde{\beta}_{(j)}+d_{(j)}+z\geq 0\\ \frac{\ell_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\tilde{\mathbf{\theta}})+\left\{ \hat{\mathbf{\ell}}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\tilde{\mathbf{\theta}}) \mathbf{d}\right\}_{(j)}^{-\lambda\hat{w}_{j}}}{-\ell_{\rm mscl}^{\hat{\mathbf{\theta }}_{\rm pl}}(\tilde{\mathbf{\theta}})}&\mbox{if }\tilde{\beta}_{(j)}+d_{(j)}+z\leq 0\\ -\tilde{\beta}_{(j)}-d_{(j)}&\mbox{otherwise},\end{array}\right.\]

which is the same as

\[z^{**}=\max\left\{z_{1},-\tilde{\beta}_{(j)}-d_{(j)}\right\}-\max\left\{-z_{2},\tilde{\beta}_{(j)}+d_{(j)}\right\}+\tilde{\beta}_{(j)}+d_{(j)},\]

where

\[z_{1}=\frac{\hat{\mathbf{\ell}}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\tilde{ \mathbf{\theta}})+\left\{\hat{\mathbf{\ell}}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}( \tilde{\mathbf{\theta}})\mathbf{d}\right\}_{(j)}+\lambda\hat{w}_{j}}{-\hat{\mathbf{\ell}}_{ \rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(jj)(\tilde{\mathbf{\theta}})},\]

and

\[z_{2}=\frac{\hat{\mathbf{\ell}}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\tilde{\mathbf{ \theta}})+\left\{\hat{\mathbf{\ell}}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}( \tilde{\mathbf{\theta}})\mathbf{d}\right\}_{(j)}-\lambda\hat{w}_{j}}{-\hat{\mathbf{\ell}}_{ \rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(jj)(\tilde{\mathbf{\theta}})}.\]

For the special form of \(g(\mathbf{x};\mathbf{\theta})=\alpha+f(\mathbf{x}^{T}\mathbf{\beta})\), we know that

\[\hat{\mathbf{\ell}}_{\rm mscl}^{\hat{\mathbf{\theta}}_{\rm pl}}(\tilde{\mathbf{\theta}})=\sum_{ i=1}^{N}\delta_{i}^{\hat{\mathbf{\theta}}_{\rm pl}}\phi_{\pi}^{\hat{\mathbf{\theta}}_{\rm pl}}( \mathbf{x}_{i},\tilde{\mathbf{\theta}})\hat{g}^{\otimes 2}(\mathbf{x}_{i};\tilde{\mathbf{\theta}})=\mathbf{G}^{T}\mathbf{\Phi}\mathbf{G},\]where

\[\mathbf{G}=\begin{pmatrix}1&\dot{f}(\mathbf{x}_{1}^{\mathrm{T}}\mathbf{\tilde{\beta}})\mathbf{x}_{1 }^{\mathrm{T}}\\ 1&\dot{f}(\mathbf{x}_{1}^{\mathrm{T}}\mathbf{\tilde{\beta}})\mathbf{x}_{1}^{\mathrm{T}}\\ \vdots&\vdots\\ 1&\dot{f}(\mathbf{x}_{N}^{\mathrm{T}}\mathbf{\tilde{\beta}})\mathbf{x}_{N}^{\mathrm{T}}\\ \end{pmatrix}\]

and \(\mathbf{\Phi}=diag\{\dot{\mathbf{\delta}}_{i}^{\mathbf{\tilde{\theta}}_{\mathrm{pl}}}\dot{ \mathbf{\phi}}_{\pi}^{\mathbf{\tilde{\theta}}_{\mathrm{pl}}}(\mathbf{x}_{i},\tilde{\mathbf{ \theta}})\}\). Thus, we have

\[\left\{\dot{\mathbf{\delta}}_{\mathrm{mscl}}^{\mathbf{\tilde{\theta}}_{\mathrm{pl}}}( \tilde{\mathbf{\theta}})\mathbf{d}\right\}_{(j)}=\left(\mathbf{G}^{\mathrm{T}}\mathbf{\Phi}\bm {G}\mathbf{d}\right)^{\mathrm{T}}\mathbf{e}_{j}=(\mathbf{G}\mathbf{d})^{\mathrm{T}}\mathbf{\Phi}( \mathbf{G}\mathbf{e}_{j})=(\mathbf{G}\mathbf{d})^{\mathrm{T}}\mathbf{\Phi}(\mathbf{G}\mathbf{e}_{j})=(\mathbf{G }\mathbf{d})^{\mathrm{T}}\mathbf{\Phi}\mathbf{G}_{(j)}.\]

Therefore, we can store \(\mathbf{G}\mathbf{d}\) and keep updating \(\mathbf{G}\mathbf{d}\) with

\[\mathbf{G}(\mathbf{d}+z\mathbf{e}_{j})=\mathbf{G}\mathbf{d}+z\mathbf{G}\mathbf{e}_{j}=\mathbf{G}\mathbf{d}+\mathbf{G}_ {(j)}z.\]

Thus, we do not need to obtain the full matrix \(\ddot{\ell}_{\mathrm{mscl}}(\tilde{\mathbf{\theta}})=\mathbf{G}^{\mathrm{T}}\mathbf{\Phi} \mathbf{G}\). We only need to calculate the diagnoal elements: \(\ddot{\ell}_{\mathrm{mscl}}(jj)(\tilde{\mathbf{\theta}})=\mathbf{G}_{(j)}^{\mathrm{T}} \mathbf{\Phi}\mathbf{G}_{(j)},j=1,...,p+1\) and \(\left\{\dot{\ell}_{\mathrm{mscl}}(\tilde{\mathbf{\theta}})\mathbf{d}\right\}_{(j)}=( \mathbf{G}\mathbf{d})^{\mathrm{T}}\mathbf{\Phi}\mathbf{G}_{(j)},j=1,...,p+1\). From the analysis above, we can notice that the computaional complexity of one cycle calculating optimal direction \(\mathbf{d}\) is \(O(\zeta_{\mathrm{in}}Np)\), where \(\zeta_{\mathrm{in}}\) denotes the number of inner iteration.

### Computational complexity

We analyze the computational complexity of the two-step algorithm. To facilitate the presentation, we consider a special case for our model when \(g(\mathbf{x};\mathbf{\theta})=\alpha+f(\mathbf{x}^{\mathrm{T}}\mathbf{\beta})\), and assume that the number of variables selected at the first-stage screening is \(q\). Coordinate descent is a widely used optimization algorithm for solving lasso and adaptive lasso [see 9]. We consider the improved coordinate descent algorithm proposed in [27], which requires inner iterations to determine an optimal direction and outer iterations to update the estimator. Considering the form of \(g(\mathbf{x};\mathbf{\theta})=\alpha+f(\mathbf{x}^{\mathrm{T}}\mathbf{\beta})\), the computational complexity for coordinate descent with data of size \(N\) and dimension \(p\) is \(O(\zeta_{\mathrm{in}}Np)\) per inner-cycle where \(\zeta_{\mathrm{in}}\) represents the number of inner iterations (detailed derivations of this complexity is presented Section C). Thus, the computational complexity of full data lasso is \(O(\zeta_{\mathrm{out}\times\mathrm{in}}Np)\), where \(\zeta_{\mathrm{out}\times\mathrm{in}}=\zeta_{\mathrm{out}}\zeta_{\mathrm{in}}\) and \(\zeta_{\mathrm{out}}\) is the number of outer iterations. The computational complexity of the full data adaptive lasso is \(O(\zeta_{\mathrm{pl}}^{\mathrm{mle}}Np^{2}+\zeta_{\mathrm{out}\times\mathrm{in }}Np)\) with the MLE as the pilot estimator, and \(O(\zeta_{\mathrm{pl,out}\times\mathrm{in}}^{\mathrm{las}}Np+\zeta_{\mathrm{ out}\times\mathrm{in}}Nq)\) with lasso as the pilot estimator, where \(\zeta_{\mathrm{pl}}^{\mathrm{mle}}\) and \(\zeta_{\mathrm{pl,out}\times\mathrm{in}}^{\mathrm{las}}\) are the iteration numbers in the two pilot estimators, respectively. The coordinate descent algorithm often requires a large \(\zeta_{\mathrm{out}\times\mathrm{in}}\) or \(\zeta_{\mathrm{pl,out}\times\mathrm{in}}^{\mathrm{las}}\) while Newton's algorithm requires a small \(\zeta_{\mathrm{pl}}^{\mathrm{mle}}\), so it is often the case that \(\zeta_{\mathrm{pl}}^{\mathrm{mle}}p<\zeta_{\mathrm{out}\times\mathrm{in}}\). Therefore, the time complexity of the adaptive lasso is \(O(\zeta_{\mathrm{out}\times\mathrm{in}}Np)\), which is the same as the full data lasso estimator.

Now, we analyze the time complexity of Algorithm 3. We start with the computational complexity of the optimal probabilities, for which the main computational cost is to approximate \(\|\mathbf{M}_{(\mathcal{A})}^{-1}\dot{g}_{(\mathcal{A})}(\mathbf{x}_{i};\mathbf{\theta})\|\) or \(\|\mathbf{\Omega}_{(\mathcal{A})}^{1/2}\mathbf{M}_{(\mathcal{A})}^{-1}\dot{g}_{( \mathcal{A})}(\mathbf{x}_{i};\mathbf{\theta})\|\), respectively, for \(i=1,...,N\). Since \(\dot{g}_{(\mathcal{A}_{\mathrm{pl}})}(\mathbf{x}_{i};\mathbf{\theta})=(1,\dot{f}(\mathbf{x }_{i}^{\mathrm{T}}\mathbf{\beta})\mathbf{x}_{i(\mathcal{A}_{\mathrm{pl}})}^{\mathrm{T }})^{\mathrm{T}}\), the computational complexity of calculating \(\dot{g}_{(\mathcal{A}_{\mathrm{pl}})}(\mathbf{x}_{i};\mathbf{\theta})\)'s is \(O(Nq)\), and the computational complexity of \(\hat{\mathbf{M}}_{(\mathcal{A}_{\mathrm{pl}})}^{\mathrm{pl}}\) or \(\hat{\mathbf{\Omega}}_{(\mathcal{A}_{\mathrm{pl}})}^{\mathrm{pl}}\) is \(O\left\{N_{\mathrm{pl}}(q+1)^{2}\right\}=O(N_{\mathrm{pl}}q^{2})\). Taking the inverse \((\hat{\mathbf{M}}_{(\mathcal{A}_{\mathrm{pl}})}^{\mathrm{pl}})^{-1}\) and finding the square root \((\hat{\mathbf{\Omega}}_{(\mathcal{A}_{\mathrm{pl}})}^{\mathrm{pl}})^{1/2}\) both take \(O(q^{3})\) time. Thus, the computational complexity of calculating \(\|(\hat{\mathbf{M}}_{(\mathcal{A}_{\mathrm{pl}})}^{\mathrm{pl}})^{-1}\dot{g}_{( \mathcal{A}_{\mathrm{pl}})}(\mathbf{xUsing the coordinate descent algorithm, the computational complexity of the two-step algorithm is \(O\{\zeta_{\mathrm{pl,out\times in}}^{\mathrm{las}}N_{\mathrm{pl}}p+Nq^{2}+\zeta_{ \mathrm{out\times in}}N(e^{\alpha_{\mathrm{t}}}+\rho)q\}\) using optimal probabilities in (4) or (7), and it is \(O\{\zeta_{\mathrm{pl,out\times in}}^{\mathrm{las}}N_{\mathrm{pl}}p+Nq+\zeta_{ \mathrm{out\times in}}N(e^{\alpha_{\mathrm{t}}}+\rho)q\}\) using the optimal probabilities in (5). For optimal probabilities in (4) or (7), when \(\zeta_{\mathrm{out\times in}}>q/(e^{\alpha}+\rho)\), the dominating term of the complexity is \(\zeta_{\mathrm{out\times in}}N(e^{\alpha_{\mathrm{t}}}+\rho)q\). Remember that \(\zeta_{\mathrm{out\times in}}=\zeta_{\mathrm{out}}\zeta_{\mathrm{in}}\) and \(\zeta_{\mathrm{out}}\) is usually large for the coordinate descent algorithm. Therefore, \(\zeta_{\mathrm{out\times in}}>q/(e^{\alpha}+\rho)\) is often satisfied in practice. The dominating term for the time complexity of optimal probabilities in (5) is also \(\zeta_{\mathrm{out\times in}}N(e^{\alpha_{\mathrm{t}}}+\rho)q\). Compared with full data estimators, both the sample size and the dimension are reduced. If we set the subsample size to be the same order of \(N_{1}\), which is often the case in practice for balancing the ones and zeros, the time complexity of Algorithm 2 is of order \(O(\zeta_{\mathrm{out\times in}}Ne^{\alpha_{\mathrm{t}}}q)\), which is significantly faster than that of the full data estimator.

## Appendix D Details of simulation settings

In this section, we present more details of the simulation settings in the main paper. In Section D.1, we provide detailed simulation settings of the example in Section 1. In Section D.2, we present detailed settings in Section 5.

### Simulation in Section 1

We first present the detailed settings of the simulations in Section 1, where we illustrate the scale-dependent issues of optimal subsampling probabilities. Our simulation based on logistic regression models with the true parameter \(\mathbf{\beta}_{\mathrm{t}}\) to be 6-dimentional vectors and covariates \(\mathbf{x}\sim lognormal(\mathbf{0},\mathbf{\Sigma})\) with the \((i,j)\)-th element of \(\mathbf{\Sigma}\) is given as \(\mathbf{\Sigma}_{ij}=0.5^{|i-j|},1\leq i,j\leq 6\). We consider two cases of parameters:

1. **Non-sparse parameter**: \(\mathbf{\beta}_{\mathrm{t}}=(-1,-1,-0.01,-0.01,-0.01,-0.01)\) and \(\alpha_{\mathrm{t}}=-4\).
2. **Sparse parameter**: \(\mathbf{\beta}_{\mathrm{t}}=(-1,0,0,0,0,0)\) and \(\alpha_{\mathrm{t}}=-5\).

We generate full data of size \(N=500000\) according to the above logistic models. To investigate the effects of scale transformation, we multiply the \(\mathbf{x}_{(6)}\) with \(s\) (\(s=0.01,0.1,1,10,100\)) and divide \(\mathbf{\beta}_{\mathrm{t}(6)}\) with the same \(s\) to remain \(\mathbf{x}^{\mathrm{T}}\mathbf{\beta}_{\mathrm{t}}\) to be the same and thus the logistics regression model does not change. We obtain subsamples with optimal subsampling probabilities described in [22] with transformed \(\mathbf{x}\) under each \(s\) and calculate the resultant subsampling estimators. We set the nominal pilot sample size to \(N_{\mathrm{pl}}=800\) and nominal subsample size \(N_{\mathrm{sub}}=1000\) (see details in [22]). We repeat the experiment for 500 times under each scale and compute the mean prediction error.

### Simulations in Section 5

For the estimation procedures in the second step of our two-step algorithm, we choose \(\gamma=1\), which means that the weights in the adaptive lasso penalty are \(\hat{w}_{j}=1/|\hat{\beta}_{\mathrm{pl}(j)}|,1\leq j\leq q\), with \(q\) being the number of selected variables in the first stage screening. Furthermore, we consider uniform sampling, the full data lasso and the full data adaptive lasso as baselines for comparison. For the uniform sampling method, we use a similar two-step algorithm as presented in Algorithm 2 but set the sampling function in the second step as \(\varphi(\mathbf{x})=1\), which means the sampling probabilities are a constant \(\rho\). We use lasso to implement the first stage screening and adaptive lasso with \(\gamma=1\) to implement the second stage screening for a fair comparison. For full data lasso, we directly apply the lasso algorithm to the full data. For the full data adaptive lasso, we use the full data MLE estimator as the pilot estimator to construct the weights and then apply the adaptive lasso algorithm to the full data set.

## Appendix E Additional simuations

In this Section, we give some addtional simulation results. More simulation results of variable selection is provided in Section E.1. We also provide addtional simulation results to compare our approach with standardization to resolve scale dependent issues in Section E.2.

[MISSING_PAGE_EMPTY:27]

Although uniform sampling may have a higher rate of selecting the true model in some cases, given that it is more likely to exclude important variables, optimal sampling may be preferable in practice.

### Comparison with standardization

Another approach to avoid scale-dependency is to standardize the data. We compare the proposed scale-independent optimal probabilities with the approach of data standardization here. For the data standardization approach, we standardize the data, calcualte the optimal probabilities, and then implement subsampled adaptive lasso algorithm. We used the same pilot estimation methods for fair comparisons.

We first compare the eMSE and eMSPE in Figure 5 and Figure 6, respectively. We use sP-OS to denote the approach with data standardization and use P-OS to denote the approach without data standardization.

In Figure 5, we notice that the performances of P-OS and sP-OS are similar, this is also true for eMSPE. However, standardization may decrease the rate of selecting the true model. We present results of variable selection in Table 7 and Table 8.

We notice in Table 7 and Table 8 that the rates of selecting true models by \(\hat{\beta}_{\mathrm{P-OS}}^{\mathrm{adp}}\) is higher than \(\hat{\beta}_{\mathrm{sP-OS}}^{\mathrm{adp}}\) without much increase on the rates of excluding active variables. Therefore, although standardization is an approach to solve the scale-dependency issues, it may decrease the rates of selecting true models in practice.

Figure 5: Empirical median squred error of estimated probability for different parameters with different sampling rates. The same pilot sample size is \(N_{\mathrm{pl}}=500\).

Figure 6: Empirical median squred error of estimated probability for different parameters with different sampling rates. The same pilot sample size is \(N_{\mathrm{pl}}=500\).

[MISSING_PAGE_FAIL:29]

### NeurIPS Paper Checklist

[Yes] [No] [NA]

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]

Justification: Detailed proofs and required assumptions are provided in the appendix.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All details for numerical experiments are provided in Section 5 and in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Codes are submitted as supplement for anonymity. They will be released in a public github repository after the review period. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: In Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide standard errors in Tables 1 and 2. We perform a large number of repetitions of the simulation experiments to calculate the empirical median squared error, so error bars are not relevant in Figures 1, 2, 3, and 4. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ** The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: In Section 5.1.3. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper is theoretical research. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: URLs for real data provided in Section 5.2. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.