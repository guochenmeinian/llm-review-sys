# MGDD: A Meta Generator for

Fast Dataset Distillation

 Songhua Liu Xinchao Wang

National University of Singapore

songhua.liu@u.nus.edu, xinchao@nus.edu.sg

Corresponding Author.

###### Abstract

Existing dataset distillation (DD) techniques typically rely on iterative strategies to synthesize condensed datasets, where datasets before and after distillation are forward and backward through neural networks a massive number of times. Despite the promising results achieved, the time efficiency of prior approaches is still far from satisfactory. Moreover, when different sizes of synthetic datasets are required, they have to repeat the iterative training procedures, which is highly cumbersome and lacks flexibility. In this paper, different from the time-consuming forward-backward passes, we introduce a generative fashion for dataset distillation with significantly improved efficiency. Specifically, synthetic samples are produced by a generator network conditioned on the initialization of DD, while synthetic labels are obtained by solving a least-squares problem in a feature space. Our theoretical analysis reveals that the errors of synthetic datasets solved in the original space and then processed by any conditional generators are upper-bounded. To find a satisfactory generator efficiently, we propose a meta-learning algorithm, where a meta generator is trained on a large dataset so that only a few steps are required to adapt to a target dataset. The meta generator is termed as _MGDD_ in our approach. Once adapted, it can handle arbitrary sizes of synthetic datasets, even for those unseen during adaptation. Experiments demonstrate that the generator adapted with only a limited number of steps performs on par with those state-of-the-art DD methods and yields \(22\times\) acceleration.

## 1 Introduction

Dataset distillation (DD) introduced by Wang _et al._[51] aims to compress an original dataset \(\mathcal{T}\) into a much smaller synthetic set \(\mathcal{S}\), such that the performance of a neural network, trained with the condensed dataset \(\mathcal{S}\), is similar to the network trained with \(\mathcal{T}\). The derived synthetic datasets not only save the cost of storage and transmission but also significantly reduce the computational resources and time required by training models using original datasets. As such, DD finds its application across a wide spectrum of domains and is receiving increasing attention from the community.

The typical paradigm of DD is to optimize \(\mathcal{S}\) in an iterative loop, as shown in Fig. 1(a). In each iteration, a new network is sampled, leveraged by which a matching metric is calculated for \(\mathcal{S}\) and \(\mathcal{T}\), and the matching loss is then back-propagated through the network to update \(\mathcal{S}\). Recently, a large number of approaches have been dedicated to exploring advanced matching objectives to improve the training performance of distilled datasets, including matching training performance via meta learning [51, 7], matching feature regression performance [37, 38, 61, 32, 33], matching training gradients [60, 58, 20], matching training trajectories [2, 8, 4], and matching feature statistics [59, 50].

Although impressive results have been achieved and it has been demonstrated that neural networks trained by a synthetic dataset with even only 1 image per class can yield reasonable performanceon real data, the iterative optimization process adopted in existing works results in significant computational overhead. As shown in Fig. 2, for FRePo [61], the method with the best performance requires above 3 hours to obtain a synthetic dataset with 1 image per class for CIFAR100 [23], let alone RTP [7], which is a back-propagation-through-time method and requires over 9 days for optimization. Such a dramatic latency makes existing approaches hard to be applied in scenarios requiring high efficiency, like handling data streams. Moreover, when the memory budget for a synthetic dataset changes, existing methods have to repeat the time-consuming optimization for the different sizes of \(\mathcal{S}\), which lacks flexibility.

To alleviate the drawbacks of the conventional iterative forward-backward process, in this paper, we propose a generative dataset distillation approach, where the optimization loop is replaced by a single feed-forward propagation, as shown in Fig. 1(b). Specifically, given an initialization of synthetic samples in \(\mathcal{S}\), we first obtain the corresponding synthetic labels by analytically solving a least-squares problem in a feature space. Then, a generator is adopted to transfer the initialized samples to the final ones. Our theoretical analysis indicates that \(\mathcal{S}\) solved in an original space can be transferred to the final result by any conditional generator with an upper-bounded error, which validates the feasibility of this pipeline.

Then, the key problem of our framework lies in finding a suitable generator as quickly as possible for the feed-forward generation process of \(\mathcal{S}\). To this end, we propose a method called _MGDD_, where a meta generator is learned with a meta learning algorithm on a large database like ImageNet [6]. Trained in a learning-to-learn fashion, the meta generator is optimized such that only a small number of adaptation steps are required for a target dataset unseen in the meta learning. Experiments demonstrate that our approach yields \(22\times\) acceleration 2 and comparable performance with existing state-of-the-art methods, as shown in Fig. 2. Beyond that, the generator once adapted can also handle unseen sizes of \(\mathcal{S}\) during adaptation, which improves the flexibility of cross-size generalization in existing DD methods significantly. We also validate that MGDD gets competitive performance on target datasets with large domain shifts from those seen in the meta learning.

Footnote 2: The acceleration factor estimated here includes adaptation time. If only feed-forward time is considered, the acceleration can be \(1650\times\).

Our contributions can be summarized from the following three aspects:

* We propose an innovative feed-forward generation fashion for DD without backward propagation after adaptation which significantly improves the efficiency of existing methods;
* We introduce MGDD which uses a meta-learning algorithm to learn a meta generator and helps the generator adapt to a target dataset rapidly;
* The proposed method achieves significant acceleration and improvement in the flexibility of cross-size generalization for existing DD approaches with comparable performance.

## 2 Related Works

Unlike conventional efficient learning schemes that mainly focus on lightening models [9; 34; 55; 54; 10; 18; 19; 17], dataset distillation (DD) looks into data compression: given a real large dataset \(\mathcal{T}\), DD aims at a smaller synthetic dataset \(\mathcal{S}\) which can match the training performance of \(\mathcal{T}\). The seminal work by Wang _et al._[51] proposes a meta learning approach to model this objective: in meta training, a network is trained with the current \(\mathcal{S}\) for multiple times, while in meta test, the loss for the updated network is evaluated on \(\mathcal{T}\), which is then back-propagated through the bi-level optimization

Figure 1: Illustration of previous back-propagation-based and our generative fashions after adaptation for dataset distillation.

to update \(\mathcal{S}\). The following work by Deng _et al._[7] improves performances by adopting momentum during meta training.

Considering the concerns on memory and time complexity of unrolling the computational graph in meta learning, a variety of works introduce various surrogate matching objectives to update \(\mathcal{S}\). Zhao _et al._[60] propose to match training gradients of \(\mathcal{S}\) with those of \(\mathcal{T}\), and following researches [20; 27; 16; 57; 45] focus on improving the classical gradient-matching objective. Beyond the single-step gradient, Cazenavette _et al._[2] and subsequent works [4; 8] consider regulating multi-step training effects and propose matching training trajectories. Without the necessity of calculating higher order gradients, the distribution matching methods [59; 50] minimize the distance between feature statistics of \(\mathcal{S}\) and \(\mathcal{T}\), and result in satisfactory computational efficiency. Another branch of methods [37; 38; 61; 32; 33] turn to kernel ridge regression (KRR) model to improve the efficiency of the seminal meta learning based solution, since KRR enjoys the analytical form of solution, which gets rid of the meta-training process and yields best trade-off between performance and efficiency.

Different from works focusing on objectives of DD, some other researches explore methods of synthetic data parameterization, where synthetic samples can be stored in some other formats except the raw one to improve the data efficiency, and raw samples are recovered via some functions during down-stream training, _e.g._, data augmentation [58], up-sampling [20], linear transformation [7], and neural networks [30; 26; 3; 49].

For a thorough introduction to dataset distillation, we refer readers to the recent surveys for this area [56; 13; 43; 28]. For existing methods, no matter what objectives and parameterizations are adopted, they all rely on an intensive loop of forward-backward propagation through a massive number of neural networks. Although a concurrent work [31] also adopts a similar pre-training and adaptation pipeline, it still relies on an iterative loop to solve an initialization of synthetic data. Different from prior works, we innovatively introduce a feed-forward fashion for dataset distillation in this paper. In fact, our method is orthogonal to different training objectives and data parameterizations, and can be built upon any combination of them. In this paper, we consider the KRR-based objective thanks to its favorable performance and computational efficiency and experiment with both raw and down-sampled parameterizations.

## 3 Methods

In this section, we elaborate on the technical methods of the proposed MGDD pipeline. We first introduce in Sec. 3.1 some preliminary information related to the matching objective. Then, for the main method, according to the overview of the whole pipeline in Fig. 1(b), the final synthetic labels and samples are derived by solving a least-squares problem and a conditional generator given initial synthetic data, which would be illustrated in detail in Sec. 3.2 and 3.3 respectively. Finally in Sec. 3.4, we analyze the feasibility of this two-step pipeline theoretically.

### Preliminary

Let us denote datasets before and after distillation as \(\mathcal{T}=(X_{t},Y_{t})\) and \(\mathcal{S}=(X_{s},Y_{s})\) respectively, where \(X_{t}\in\mathbb{R}^{n_{t}\times d}\), \(Y_{t}\in\mathbb{R}^{n_{t}\times c}\), \(X_{s}\in\mathbb{R}^{n_{s}\times d}\), \(Y_{s}\in\mathbb{R}^{n_{s}\times c}\), \(n_{t}\) and \(n_{s}\) are number of real and synthetic data respectively, \(d\) and \(c\) are input and output dimensions respectively. Typically, for the RGB image classification task, \(d\) is equal to \(h\times w\times 3\), \(c\) is the number of classes, and \(Y_{t}\) is organized in the one-hot format. For the objective of DD, in this paper, we mainly consider the KRR-based methods in neural feature spaces [61; 32] due to its overall superior performance in terms of accuracy and efficiency. Specifically, assume there is an optimal neural network \(f_{\theta^{*}}\) to projects \(X_{t}\) and \(X_{s}\) to a feature space with \(p\) dimensions and \(n_{s}\ll n_{t}<p\). The prediction error on \(\mathcal{T}\) for the optimal KRR

Figure 2: Results of different DD methods on CIFAR100 with 1 image per class. Our MGDD achieves state-of-the-art efficiency and at least comparable performance. DS denotes down-sampling parameterization.

parameter solved by \(\mathcal{S}\), denoted as \(W_{s}^{\theta^{*}}\) is adopted as the loss function:

\[\mathcal{L}(\mathcal{S};\theta^{*})=\|f_{\theta^{*}}(X_{t})W_{s}^{\theta^{*}}-Y_{ t}\|_{2}^{2}=\|f_{\theta^{*}}(X_{t})f_{\theta^{*}}(X_{s})^{\top}(f_{\theta^{*}}(X_{s })f_{\theta^{*}}(X_{s})^{\top})^{-1}Y_{s}-Y_{t}\|_{2}^{2}. \tag{1}\]

In practice, since the optimal parameter \(\theta^{*}\) is unknown, it is approximated by different random initializations [32] or alternately optimization with \(\mathcal{S}\)[61].

### Solving Synthetic Labels

Through Eq. 1, we can find that the loss in a neural space \(\theta\) is upper-bounded by the distance between parameters solved by \(\mathcal{S}\) and \(\mathcal{T}\):

\[\mathcal{L}(\mathcal{S};\theta)=\|f_{\theta}(X_{t})W_{s}^{\theta^ {*}}-Y_{t}\|_{2}^{2} =\|f_{\theta}(X_{t})W_{s}^{\theta}-f_{\theta^{*}}(X_{t})f_{ \theta}(X_{t})^{\dagger}Y_{t}\|_{2}^{2}\] \[=\|f_{\theta}(X_{t})W_{s}^{\theta}-f_{\theta}(X_{t})W_{t}^{ \theta}\|_{2}^{2}\leq\|f_{\theta}(X_{t})\|_{2}^{2}\|W_{s}^{\theta}-W_{t}^{ \theta}\|_{2}^{2} \tag{2}\] \[=\|f_{\theta}(X_{t})\|_{2}^{2}\|f_{\theta}(X_{s})^{\dagger}Y_{s}-W _{t}^{\theta}\|_{2}^{2},\]

where \({}^{\dagger}\) denotes the pseudo-inverse of a matrix. In our MGDD framework, synthetic samples \(X_{s}\) are initialized as some random real samples in \(X_{t}\). Given a fixed \(X_{s}\) and a random network \(f_{\theta}\), the upper bound in Eq. 2 forms a least-squares problem with respect to synthetic labels \(Y_{s}\), which can be minimized by an analytically optimal solution:

\[Y_{s}^{*}=f_{\theta}(X_{s})W_{t}^{\theta}=f_{\theta}(X_{s})f_{\theta}(X_{t})^{ \top}(f_{\theta}(X_{t})f_{\theta}(X_{t})^{\top})^{-1}Y_{t}. \tag{3}\]

\(Y_{s}^{*}\) obtained with Eq. 3 serves as final synthetic labels. In the next subsection, we will introduce the generation of synthetic samples conditioned on their initialization.

### Learning a Synthetic Sample Generator

Conditioned on initialized synthetic samples \(X_{s}\), a generator \(g_{\omega}\) is adopted to predict the final synthetic data \(X_{s}^{*}\), where the parameter \(\omega\) can encode useful information of the target dataset \(\mathcal{T}\) and the optimal neural space parameterized by \(\theta^{*}\). We expect that the generator can acquire such knowledge through a fast learning process within a limited number of training steps. To this end, we propose a learning-to-learn algorithm based on MAML [12], where a meta generation network is learned to optimize the performance of the network adapted for a few steps from the meta one.

Specifically, to ensure the generality of the meta generator for different target datasets, we perform the training algorithm on ImageNet1k [6], a large-scale dataset for image classification. In each training iteration, a subset of all classes is randomly sampled from it to mimic different target datasets that the generator may encounter in practice. And the meta generator is learned in a bi-level learning framework including a meta-training loop and a meta-testing step, and the meta-testing loss is back-propagated through the computational graph of meta-training steps to update the parameter of the meta generator.

In each meta-training and meta-testing step, from the selected classes, we randomly sample a batch of real images as \(\mathcal{T}\) and initialize synthetic data \(X_{s}\) with part of them. With a random and fixed neural network as \(f_{\theta}\), the synthetic labels \(Y_{s}^{*}\) are solved via Eq. 3. Then, the final synthetic samples \(X_{s}^{*}\) are predicted by the current generator in a forward pass, and \(\mathcal{S}=(X_{s}^{*},Y_{s}^{*})\) is evaluated by the loss \(\mathcal{L}(\mathcal{S};\theta^{*})\) in Eq. 1. In this paper, following Loo _et al._[32], we approximate the optimal neural parameter \(\theta^{*}\) via random sampling in different steps from the distribution for initialization. The loss signal is back-propagated to the current generator and the meta generator to update parameters in meta-training and meta-testing respectively. It is worth noting that in different meta-training and meta-testing times, we use different sizes of synthetic data, which enhances the cross-size generalization ability on target datasets since the meta-testing losses on sizes probably unseen during meta-training are optimized. The main algorithm is summarized in Alg. 1. Given a trained meta generator, a limited number of adaptation steps are performed for a target dataset. The procedure of adaptation is similar to the meta-training loop in Alg. 1.

As for the architecture of the generator, in this paper, we adopt a simple encoder-decoder model, where the encoder consists of three convolutional blocks with two average pooling layers while the decoder has a symmetric structure. Notably, we observe in practice that it is beneficial for different sizes of synthetic datasets to adopt different transfer functions. Taking various sizes into consideration, we concatenate additional size-embedding channels to the bottle-necked layer of the generator, inspired by the design of the position embedding in Transformer models [47] and the time-step embedding in diffusion models [15; 39; 41]. Please refer to the appendix for detailed architecture configurations.

### Theoretical Analysis

There are two key steps in the proposed MGDD framework: solving the optimal synthetic labels \(Y^{*}_{s}\) as introduced in Sec. 3.2 and generating the corresponding synthetic samples \(X^{*}_{s}\) as introduced in Sec. 3.3. Define the error \(\mathcal{L}(\mathcal{S};\theta)\) in Eq. 1 using the fixed \(X_{s}\) and the optimal \(Y_{s}\) in Eq. 3 with the projection function of \(f_{\theta}\) as \(\epsilon\). The reason we pursue the optimal \(Y_{s}\) is that the final \(X^{*}_{s}\) is transferred from the initial \(X_{s}\), whose error in the optimal neural space parameterized by \(\theta^{*}\) is upper-bounded by \(\epsilon\), as derived in the following theorem.

**Theorem 1**.: _Given \(f_{\theta}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{p}\), \(f_{\theta^{*}}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{p}\), \(X_{t}\in\mathbb{R}^{n_{t}\times d}\), \(Y_{t}\in\mathbb{R}^{n_{t}\times c}\), \(X_{s}\in\mathbb{R}^{n_{s}\times c}\), \(d<p\), \(Y^{*}_{s}\) obtained by Eq. 3 with the optimal \(\mathcal{L}((X_{s},Y^{*}_{s});\theta)\) denoted as \(\epsilon\), and an arbitrary transfer function \(g_{\omega}\) parameterized by \(\omega\) taking \(X_{s}\) as input, the transferred \(g_{\omega}(X_{s})\) yields an upper-bounded loss \(\mathcal{L}((g_{\omega}(X_{s}),Y^{*}_{s});\theta^{*})\)._

Proof.: We first rewrite the given condition:

\[\mathcal{L}((X_{s},Y^{*}_{s});\theta)=\|f_{\theta}(X_{t})W^{\theta}_{s}-Y_{t} \|_{2}^{2}=\|f_{\theta}(X_{t})f_{\theta}(X_{s})^{\top}(f_{\theta}(X_{s})f_{ \theta}(X_{s})^{\top})^{-1}Y^{*}_{s}-Y_{t}\|_{2}^{2}=\epsilon. \tag{4}\]

Then, we have:

\[\mathcal{L}((g_{\omega}(X_{s}),Y^{*}_{s});\theta^{*}) =\|f_{\theta^{*}}(X_{t})W^{\theta^{*}}_{s}-Y_{t}\|_{2}^{2} \tag{5}\] \[=\|f_{\theta^{*}}(X_{t})f_{\theta^{*}}(g_{\omega}(X_{s}))^{\top}(f _{\theta^{*}}(g_{\omega}(X_{s}))f_{\theta^{*}}(g_{\omega}(X_{s}))^{\top})^{-1}Y ^{*}_{s}-Y_{t}\|_{2}^{2}\] \[\leq\|f_{\theta^{*}}(X_{t})W^{\theta^{*}}_{s}-f_{\theta}(X_{t})W^ {\theta}_{s}\|_{2}^{2}+\|f_{\theta}(X_{t})W^{\theta}_{s}-Y_{t}\|_{2}^{2}\] \[=\|f_{\theta^{*}}(X_{t})W^{\theta^{*}}_{s}-f_{\theta}(X_{t})W^{ \theta}_{s}\|_{2}^{2}+\epsilon,\]

where the inequality is based on the triangle inequality and the last equation is due to Eq. 4. 

Theorem 1 indicates that to achieve feed-forward dataset distillation, we do not need to design a neural network taking the whole original dataset \(\mathcal{T}\) as input. We can in fact adopt a conditional generation function that transfers the initial synthetic samples to the desired ones, which has an error upper bound as shown in Theorem 1. Since the upper bound is related to the error in the original space parameterized by \(\theta\), it is crucial to solve the optimal synthetic labels with respect to \(\theta\) in the first step. Also, we notice from Eq. 5 that the optimal generator is dependent on \(\mathcal{T}\) and \(\theta^{*}\). Given that \(\theta^{*}\) is intractable and can only be approximated by iterative sampling, we build a meta learning algorithm in Alg. 1 for the MGDD framework to enforce an efficient process to model this dependency in only a few steps.

## 4 Experiments

### Implementing Details

As discussed in the previous section, there are 3 stages in the proposed MGDD, including training, adaptation, and inference stages. In the training stage, we aim at a meta generator \(g_{\omega}\) and adopt Alg. 1 to train \(g_{\omega}\) on a large dataset. In this paper, to ensure that the meta generator can acquire general knowledge for fast adaptation on a target dataset, we use ImageNet1k [6], a large-scale image recognition dataset popular in the computer vision and machine learning communities, as the dataset for meta learning. There are roughly 1,280,000 images in a total of 1,000 classes. We resize all images to the \(32\times 32\) resolution. In each outer loop of Alg. 1, we randomly select 100 classes at most and a batch of data with 2,000 images in maximal from the selected class as a current target dataset \(\mathcal{T}\), with which we initialize a synthetic dataset \(\mathcal{S}\) with 1,000 samples at most in each inner step. The training objective is based on Eq. 1 and the implementation follows the open-source code of FRePo [61]. For computational efficiency, the generator processes each sample independently and the configuration of the architecture can be found in the appendix. The meta generator is trained by the Adam optimizer [21] and the learning rate \(\beta\) is set as \(10^{-5}\). The learning rate in meta-training is set as \(10^{-4}\) and the number of meta-training steps \(T\) is \(5\). The meta generator is trained with 200,000 meta-testing iterations. We use a cloud server with a single A40 GPU for meta learning and a workstation with a single 3090 GPU for the subsequent adaptation. The meta learning takes roughly 2 days while the time cost of adaptation is analyzed in Tab. 1.

[MISSING_PAGE_FAIL:7]

is applicable to datasets with larger resolutions. It can also be adapted for datasets with different numbers of input channels with minor modifications. In the appendix, we provide more experimental results of such cross-resolution and cross-channel-number generalization cases.

### Empirical Studies

In this part, we focus on some interesting properties of the proposed MGDD method, including cross-dataset, cross-objective, cross-architecture, cross-synthetic-initialization, cross-parameterization, and cross-IPC settings. More studies including cross-resolution, cross-channel-number, and cross-class-number that cannot fit into the main content are put in the appendix.

**Cross-Dataset Generalization:** MGDD proposed in this paper is expected to be generalized to any downstream target datasets, including those unseen and with large domain shifts from datasets used in the meta learning. To evaluate the cross-dataset generalization performance of MGDD, we conduct experiments on more datasets including one domain generalization dataset PACS [29], two medical classification datasets PathMNIST and BloodMNIST [53], and one fine-grain image classification dataset CUB200 [48]. PACS contains 9,991 images from 7 classes and 4 domains: Photo (P), Art Painting (A), Cartoon (C), and Sketch (S). The style variations across the 4 domains are drastic. We perform dataset distillation on each domain both independently and jointly, which formulates a 28-class dataset. PathMNIST and BloodMNIST contain 107,180 images of 9 classes and 17,092 images from 8 classes respectively. We also combine them together to form a 17-class dataset denoted as PathBloodMNIST. CUB200 contains 6,000 images of 200 bird species. We process all the images to the \(32\times 32\) resolution in the RGB format and compare the performance with the FRePo baseline [61] and a generator from scratch instead of the meta generator, under 1,000 and 2,000 steps as well as full convergence. The quantitative results shown in Tab. 2 validate the robustness of MGDD under various datasets and domain shifts.

**Cross-Objective Generalization:** By default, both the meta-learning and adaptation objectives used in this paper for MGDD are the KRR objective in Eq. 1 following FRePo [61]. Empirically, we find that it is also feasible to adopt different objectives for adaptation. Here, we switch the adaptation objective to DC [60] and DM [59] respectively. The optimization steps for both baselines and our method are set as 2,000. As shown in Tab. 3, our methods yield consistent improvement over different baselines in a limited number of optimization steps.

**Cross-Architecture Generalization:** Adapted on an original architecture, a satisfactory generator is expected to produce results that are also valid to train networks with different structures, namely cross-architecture generalization. Tab. 4 shows the performance on CIFAR10, where ConvNet is used in adaptation while AlexNet [24], VGG11 [46], and ResNet18 [14] are used as unseen architectures for evaluation. The results indicate that the accuracy improvement on the original model still holds for unseen structures.

**Cross-Initialization Generalization:** We find that the single-initialization scheme used in Tab. 1 may lead to over-fitting of the generator to the adopted single initialization. As shown in Tab. 5, if we change the initialization of synthetic data, the performance would drop dramatically. Fortunately, multi-initialization is an alternative to account for this drawback, wherein each adaptation step, a new initialization of the synthetic dataset is sampled from the original dataset. Tab. 5 indicates that multi-initialization typically requires more adaptation steps for convergence and can perform on par with single-initialization. It is useful when samples in the synthetic dataset are concerned with user privacy, given that visualization results produced by the KRR objective are somehow realistic, as illustrated in Fig. 6 and [61]. In such cases, replacing the current dataset with the results of another

\begin{table}
\begin{tabular}{c c|c|c c c} \hline \hline  & \multicolumn{2}{c|}{Train Arch.} & \multicolumn{3}{c}{Unseen Arch.} \\ \hline IPC & Method & ConvNet & AlexNet & VGG & ResNet \\ \hline \hline \multirow{2}{*}{1} & Baseline & 49.6\(\pm\)0.1 & 44.5\(\pm\)0.7 & 33.0\(\pm\)0.1 & 31.8\(\pm\)1.6 \\  & Ours & 58.9\(\pm\)0.4 & 55.1\(\pm\)0.4 & 35.9\(\pm\)0.6 & 32.7\(\pm\)0.8 \\ \hline \multirow{2}{*}{10} & Baseline & 26.8\(\pm\)0.7 & 23.4\(\pm\)0.3 & 16.9\(\pm\)0.1 & 15.1\(\pm\)0.8 \\  & Ours & 42.6\(\pm\)0.3 & 39.6\(\pm\)0.8 & 22.9\(\pm\)0.6 & 19.1\(\pm\)1.3 \\ \hline \multirow{2}{*}{50} & Baseline & 62.0\(\pm\)0.4 & 59.2\(\pm\)0.3 & 48.7\(\pm\)1.1 & 48.2\(\pm\)0.4 \\  & Ours & 66.8\(\pm\)0.2 & 62.8\(\pm\)0.2 & 50.9\(\pm\)0.7 & 52.4\(\pm\)1.2 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performance of different network architectures on CIFAR10.

\begin{table}
\begin{tabular}{c|c c|c c} \hline \hline Dataset & \multicolumn{2}{c|}{CIFAR10} & \multicolumn{2}{c}{CIFAR100} \\ \hline IPC & 1 & 10 & 50 & 1 & 10 \\ \hline \hline FRePo & 26.8\(\pm\)0.7 & 49.6\(\pm\)0.1 & 62.0\(\pm\)0.4 & 10.1\(\pm\)0.3 & 29.6\(\pm\)0.2 \\ Ours w FRePo & 42.6\(\pm\)0.3 & 58.9\(\pm\)0.4 & 66.8\(\pm\)0.2 & 20.8\(\pm\)0.2 & 32.2\(\pm\)0.3 \\ \hline DC & 24.7\(\pm\)0.4 & 43.1\(\pm\)0.3 & 56initialization would help solve the problem efficiently, without the necessity to re-run the whole optimization loop of existing methods.

**Cross-Parameterization Generalization:** Beyond different training objectives, the proposed MGDD is also orthogonal with different synthetic data parameterization tricks. In Tab. 6, we consider storing \(2\times\) down-sampled synthetic images instead of the raw ones. Thus, \(4\times\) synthetic samples can be stored given the same storage budget. We find that the simple strategy can lead to additional performance gain for relatively small budgets. The observation is consistent with previous works [20; 30; 7].

**Cross-IPC Generalization:** One crucial benefit of the MGDD is the cross-IPC generalization. Once the generator is adapted for a target dataset, when the storage budget changes, we do not need to perform the optimization again, unlike previous solutions relying on iteratively updating synthetic datasets. To demonstrate the cross-IPC generalization performance, we conduct experiments on CIFAR10 and adapt the meta generator using multi-initialization, with 10 and 50 IPCs for 2,000 steps. The adapted generator is evaluated on unseen IPCs 20, 30, and 40. The results are shown in the red curve of Fig. 4, where the generator produces satisfactory synthetic datasets with unseen IPCs.

To make the generator aware of the sizes of synthetic datasets, we concatenate size embedding channels to features at the middle layer of the generator. To understand how the embedding works, we remove these channels and conduct the same evaluation. As shown in the yellow curve of Fig. 4, the performance degrades without size embedding. In Fig. 6, we visualize some samples before and after the adapted generator on CIFAR10 with 1 and 50 IPCs. We can observe that the results are quite different: for small sizes, the generated results are vague, while for large sizes the results are more realistic, and their styles are also different. Thus, it is reasonable to take different sizes of synthetic datasets into consideration in the inference stage.

We also try training the generator from scratch instead of the meta model on the target dataset. As shown in Fig. 4, the worse performance in the green curve indicates that meta-learning is crucial for finding a satisfactory initial checkpoint for downstream adaptation.

Moreover, we also evaluate the performance on higher IPCs like 100 and 200, and the results are still encouraging compared with random real samples, which indicates that our method can serve as an alternative when the computational resource cannot support optimization for larger IPCs directly.

Furthermore, to demonstrate the effectiveness of analytical labels, we replace them with vanilla one-hot labels in synthetic datasets and the performance is shown in the blue curve. The considerable performance drop indicates the importance of minimizing the error in an original space via analytical labels, which is consistent with the theoretical analysis in Theorem 1.

### Application: Continual Learning

Continual learning (CL) aims to learn from a stream of data, where historical and future data are unavailable when learning the current data batch. One important issue is catastrophic forgetting [22]: a model tends to forget knowledge acquired in previous data when learning on newly-coming data. Focusing on this drawback, many works introduce a buffer with limited memory to store core data and knowledge of past experience for future use [40, 1]. Dataset distillation benefits this field by generating informative samples [36, 5, 42, 35, 44] to prevent forgetting as much as possible.

In this paper, we evaluate the CL performance of the proposed MGDD on CIFAR100, following the same protocol of [61, 59], where all 100 classes are divided into 10 tasks randomly with 10 classes for each task. For each task, a buffer with 20 images for each class is allowed for synthetic data. We first try adapting the generator on each new task from the meta model for 2,000 steps and the performance is shown in the yellow curve in Fig. 5. Alternatively, we can choose to adapt the generator from the checkpoint of the previous task, which has already learned some global knowledge of full data and yields better performance, as shown in the red curve. Notably, it is also feasible to only adapt the generator on the first task and the remaining tasks directly adopt this generator to output synthetic data. As shown in the blue curve, with the most significant flexibility, the performance is still comparable with the FRePo baseline [61] shown in the green curve, which suggests great practical value for processing data streams.

## 5 Conclusion

In this paper, we propose MGDD, a novel feed-forward paradigm for dataset distillation (DD). Specifically, in our pipeline, synthetic labels are obtained by solving a least-squares problem equipped with an analytical solution, and synthetic samples are transferred from their initial results by a conditional generator instead of taking the whole original training dataset as input. Theoretical derivation indicates an error upper bound of the proposed framework. On the one hand, unlike existing DD approaches requiring time-consuming forward-backward iterations through a massive number of networks, MGDD generates distilled results with a generator adapted rapidly from a meta generator, which improves the efficiency of DD significantly. On the other hand, existing techniques have to repeat the whole iterative algorithms for different sizes of synthetic datasets, while MGDD can perform inference flexibly on various sizes once adapted to the target dataset. Focusing on the efficiency of adaptation on target datasets, we propose a meta-learning algorithm to train a meta generator, such that it can acquire knowledge of target datasets sufficiently in only a few steps. Experiments demonstrate that the proposed MGDD performs on par with existing state-of-the-art DD baselines under \(22\times\) acceleration. It also exerts strong cross-size generalization ability even on sizes of synthetic datasets unseen during adaptation. Future works may explore advanced feed-forward fashions of DD, focusing on generation pipelines, training algorithms, and network architectures, making improvements on the cross-dataset, cross-size, and cross-architecture generalization.

Figure 6: Visualizations of samples before and after generator on CIFAR10 with 1 and 50 IPC.

## Acknowledgment

This work is supported by the Advanced Research and Technology Innovation Centre (ARTIC), the National University of Singapore under Grant (project number: A0005947-21-00, project reference: ECT-RP2), and the Singapore Ministry of Education Academic Research Fund Tier 1 (WBS: A0009440-01-00).

## References

* [1] Pietro Buzzega, Matteo Boschini, Angelo Porrello, and Simone Calderara. Rethinking experience replay: a bag of tricks for continual learning. In _2020 25th International Conference on Pattern Recognition (ICPR)_, pages 2180-2187. IEEE, 2021.
* [2] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories. _arXiv preprint arXiv:2203.11932_, 2022.
* [3] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Generalizing dataset distillation via deep generative prior. _arXiv preprint arXiv:2305.01649_, 2023.
* [4] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scaling up dataset distillation to imagenet-1k with constant memory. _arXiv preprint arXiv:2211.10586_, 2022.
* [5] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. _IEEE transactions on pattern analysis and machine intelligence_, 44(7):3366-3385, 2021.
* [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [7] Zhiwei Deng and Olga Russakovsky. Remember the past: Distilling datasets into addressable memories for neural networks. _arXiv preprint arXiv:2206.02916_, 2022.
* [8] Jiawei Du, Yidi Jiang, Vincent TF Tan, Joey Tianyi Zhou, and Haizhou Li. Minimizing the accumulated trajectory error to improve dataset distillation. _arXiv preprint arXiv:2211.11004_, 2022.
* [9] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Degraph: Towards any structural pruning. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023.
* [10] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural Pruning for Diffusion Models. In _Advances in Neural Information Processing Systems_, 2023.
* [11] Fastai. Fastai/imagenette: A smaller subset of 10 easily classified classes from imagenet, and a little more french.
* [12] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International conference on machine learning_, pages 1126-1135. PMLR, 2017.
* [13] Jiahui Geng, Zongxiong Chen, Yuandou Wang, Herbert Woisetschlaeger, Sonja Schimmler, Ruben Mayer, Zhiming Zhao, and Chunming Rong. A survey on dataset distillation: Approaches, applications and future directions. _arXiv preprint arXiv:2305.01975_, 2023.
* [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [16] Zixuan Jiang, Jiaqi Gu, Mingjie Liu, and David Z. Pan. Delving into effective gradient matching for dataset condensation. _arXiv preprint arXiv:2208.00311_, 2022.
* [17] Yongcheng Jing. _Efficient Representation Learning With Graph Neural Networks_. PhD thesis, 2023.
* [18] Yongcheng Jing, Yiding Yang, Xinchao Wang, Mingli Song, and Dacheng Tao. Amalgamating knowledge from heterogeneous graph neural networks. In _CVPR_, 2021.
* [19] Yongcheng Jing, Chongbin Yuan, Li Ju, Yiding Yang, Xinchao Wang, and Dacheng Tao. Deep graph reprogramming. In _CVPR_, 2023.

* [20] Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha, and Hyun Oh Song. Dataset condensation via efficient synthetic-data parameterization. _arXiv preprint arXiv:2205.14959_, 2022.
* [21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [22] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _Proceedings of the national academy of sciences_, 114(13):3521-3526, 2017.
* [23] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Advances in neural information processing systems_, 25, 2012.
* [25] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. _ATT Labs [Online]. Available: [http://yann.lecun.com/exdb/mnist_](http://yann.lecun.com/exdb/mnist_), 2, 2010.
* [26] Hae Beom Lee, Dong Bok Lee, and Sung Ju Hwang. Dataset condensation with latent space knowledge factorization and sharing. _arXiv preprint arXiv:2208.00719_, 2022.
* [27] Saehyung Lee, Sanghyuk Chun, Sangwon Jung, Sangdoo Yun, and Sungroh Yoon. Dataset condensation with contrastive signals. _arXiv preprint arXiv:2202.02916_, 2022.
* [28] Shiye Lei and Dacheng Tao. A comprehensive survey to dataset distillation. _arXiv preprint arXiv:2301.05603_, 2023.
* [29] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In _Proceedings of the IEEE international conference on computer vision_, pages 5542-5550, 2017.
* [30] Songhua Liu, Kai Wang, Xingyi Yang, Jingwen Ye, and Xinchao Wang. Dataset distillation via factorization. In _Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [31] Songhua Liu and Xinchao Wang. Few-shot dataset distillation via translative pre-training. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 18654-18664, October 2023.
* [32] Noel Loo, Ramin Hasani, Alexander Amini, and Daniela Rus. Efficient dataset distillation using random feature approximation. In _Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [33] Noel Loo, Ramin Hasani, Mathias Lechner, and Daniela Rus. Dataset distillation with convexified implicit gradients. _arXiv preprint arXiv:2302.06755_, 2023.
* [34] Xinyin Ma, Gongfan Fang, and Xinchao Wang. LLM-Pruner: On the Structural Pruning of Large Language Models. In _Advances in Neural Information Processing Systems_, 2023.
* [35] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online continual learning in image classification: An empirical survey. _Neurocomputing_, 469:28-51, 2022.
* [36] Wojciech Masarczyk and Ivona Tautkute. Reducing catastrophic forgetting with learning on synthetic data. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Workshop_, 2020.
* [37] Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset meta-learning from kernel ridge-regression. _arXiv preprint arXiv:2011.00050_, 2020.
* [38] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with infinitely wide convolutional networks. _Advances in Neural Information Processing Systems_, 34, 2021.
* [39] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pages 8162-8171. PMLR, 2021.
* [40] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In _Proceedings of the IEEE conference on Computer Vision and Pattern Recognition_, pages 2001-2010, 2017.

* [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021.
* [42] Andrea Rosasco, Antonio Carta, Andrea Cossu, Vincenzo Lomonaco, and Davide Bacciu. Distilled replay: Overcoming forgetting through synthetic samples. _arXiv preprint arXiv:2103.15851_, 2021.
* [43] Noveen Sachdeva and Julian McAuley. Data distillation: A survey. _arXiv preprint arXiv:2301.04272_, 2023.
* [44] Mattia Sangermano, Antonio Carta, Andrea Cossu, and Davide Bacciu. Sample condensation in online continual learning. In _Proceedings of the International Joint Conference on Neural Networks (IJCNN)_, pages 1-8, 2022.
* [45] Seungjae Shin, Heesun Bae, Donghyeok Shin, Weonyoung Joo, and Il-Chul Moon. Loss-curvature matching for dataset selection and condensation. In _International Conference on Artificial Intelligence and Statistics_, pages 8606-8628. PMLR, 2023.
* [46] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
* [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [48] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.
* [49] Kai Wang, Jianyang Gu, Daquan Zhou, Zheng Zhu, Wei Jiang, and Yang You. Dim: Distilling dataset into generative model. _arXiv preprint arXiv:2303.04707_, 2023.
* [50] Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, and Yang You. Cafe: Learning to condense dataset by aligning features. _arXiv preprint arXiv:2203.01531_, 2022.
* [51] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. _arXiv preprint arXiv:1811.10959_, 2018.
* [52] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _CoRR_, abs/1708.07747, 2017.
* [53] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification. _Scientific Data_, 10(1):41, 2023.
* [54] Xingyi Yang, Jingwen Ye, and Xinchao Wang. Factorizing knowledge in neural networks. In _European Conference on Computer Vision_, 2022.
* [55] Xingyi Yang, Daquan Zhou, Songhua Liu, Jingwen Ye, and Xinchao Wang. Deep model reassembly. In _Advances in Neural Information Processing Systems_, 2022.
* [56] Ruonan Yu, Songhua Liu, and Xinchao Wang. Dataset distillation: A comprehensive review. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.
* [57] Lei Zhang, Jie Zhang, Bowen Lei, Subhabrata Mukherjee, Xiang Pan, Bo Zhao, Caiwen Ding, Yao Li, and Xu Dongkuan. Accelerating dataset distillation via model augmentation. _arXiv preprint arXiv:2212.06152_, 2022.
* [58] Bo Zhao and Hakan Bilen. Dataset condensation with differentiable siamese augmentation. In _International Conference on Machine Learning_, pages 12674-12685. PMLR, 2021.
* [59] Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 6514-6523, 2023.
* [60] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. _arXiv preprint arXiv:2006.05929_, 2020.
* [61] Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset distillation using neural feature regression. _arXiv preprint arXiv:2206.00719_, 2022.

In this part, we include more details about the technical methods, more experimental results of our MGDD method, and more discussion on limitations and future works, which cannot be accommodated in the main paper due to the page limit. Our method first trains a meta generator to generate synthetic samples and then an adaptation stage is executed for a target dataset. We provide algorithmic details of the adaptation stage, a summary of hyper-parameters, and configurations of our generator architecture. Then, we conduct more evaluations on the cross-number-of-channel, cross-resolution, cross-ipc, and cross-number-of-classes performance of our method. More discussions of the adaptation performance and more qualitative examples are also included. Finally, we discuss limitations of the proposed method and potential future works.

## Appendix A More Details

**Adaptation Algorithm:** Alg. 1 demonstrates the procedure of meta learning to obtain a meta synthetic sample generator. On a downstream target dataset, the meta network is adapted to a specific network with a limited number of steps. The adaptation algorithm is similar to the meta-training step of the meta learning algorithm. Here, we present the full details in Alg. 2. We can prepare multiple initialization of synthetic samples through randomly sampling from the target dataset. Recall that the main pipeline of our algorithm is to first obtain analytical synthetic labels in a random neural space \(\theta\): \(Y_{s}^{*}=f_{\theta}(X_{s})W_{t}^{\theta}\). Here, the optimal kernel-ridge-regression parameters of the target dataset \(W_{t}^{\theta}\) can be computed by \(W_{t}^{\theta}=f_{\theta}(X_{t})^{\top}(f_{\theta}(X_{t})f_{\theta}(X_{t})^{ \top})^{-1}Y_{t}\), if the number of real samples \(n_{t}\) is smaller than the feature dimension \(p\). Otherwise, \(W_{t}^{\theta}=(f_{\theta}(X_{t})^{\top}f_{\theta}(X_{t}))^{-1}f_{\theta}(X_{ t})^{\top}Y_{t}\).

```
1:\((X_{t},Y_{t})\): A Target Dataset; \(T\): Number of Adaptation Steps; \(\alpha\): Learning Rate of Generator; \(\theta\): Parameter of a Random Neural Network; \(\omega\): Parameter of a Meta Generator; \(\mathcal{I}\): A Set of Randomly Initialized Synthetic Samples.
2:\(\omega^{\prime}\): Parameter of a Target-Specific Generator.
3:\(W_{t}^{\theta}=f_{\theta}(X_{t})^{\top}(f_{\theta}(X_{t})f_{\theta}(X_{t})^{ \top})^{-1}Y_{t}\);
4:for Each \(X_{s}\) in \(\mathcal{I}\)do
5:\(Y_{s}^{*}=f_{\theta}(X_{s})W_{t}^{\theta}\); \(\triangleright\) Eq. 3
6:endfor
7:Initialize generator parameters \(\omega^{\prime}\) with \(\omega\);
8:for\(1\leq i\leq T\)do
9: Sample a batch of real data \((X_{t}^{i},Y_{t}^{i})\) of from \((X_{t},Y_{t})\);
10: Sample a initialized synthetic data \((X_{s},Y_{s}^{*})\) from \(\mathcal{I}\);
11:\(X_{s}^{*}=g_{\omega^{\prime}}(X_{s})\); \(\triangleright\) Forward propagation
12: Sample neural parameters \(\theta^{*}\) from a random distribution;
13:\(\mathcal{L}=\|f_{\theta^{*}}(X_{t})f_{\theta^{*}}(X_{s}^{*})^{\top}(f_{\theta^ {*}}(X_{s}^{*})f_{\theta^{*}}(X_{s}^{*})^{\top})^{-1}Y_{s}^{*}-Y_{t}\|_{2}^{2}\); \(\triangleright\) Eq. 1
14: Update \(\omega^{\prime}\) via \(\omega^{\prime}\leftarrow\omega^{\prime}-\alpha\nabla_{\omega^{\prime}} \mathcal{L}\); \(\triangleright\) Back propagation
15:endfor
```

**Algorithm 2** Adaptation Algorithm of Synthetic Sample Generator for a Target Dataset

After the calculation of analytical labels, we fix them and train the synthetic sample generator initialized by parameters of the meta generator for some steps. The optimization objective is similar to those in Zhou _et al._[61] and Loo _et al._[32]. The difference is that the optimization target is parameters of the generator instead of synthetic samples.

**Summary of Hyper-Parameters:** For a clear view, we summarize the hyper-parameters and their values in both meta learning and adaptation stages as shown in Tab. 7. All experiments follow these default settings of hyper-parameters if not specified. Other configurations unmentioned follow the settings of the baseline FRePo [61].

**Generator Architecture:** We illustrate the detailed configurations of our generator architecture in Fig. 7. It essentially adopts an encoder-decoder structure with 3 Conv-BatchNorm-ReLU blocks and 2 AvgPool layers for down-sampling for the encoder and a symmetric structure for the decoder. Notably, to make the network aware of different sizes of synthetic datasets, we concatenate the size embedding to bottle-necked features after the encoder. Inspired by the positional embedding in Transformer models [47] and the time-step embedding in diffusion models [15, 39], we encode the size by sinusoidal signals and a learnable non-linear transformation function. Embedding features are replicated and expanded along the spatial axes before concatenation with features from the encoder.

[MISSING_PAGE_FAIL:15]

**Cross-Resolution Generalization:** Although the meta generator is trained under 32 resolution, it is possible for it to be adapted for datasets with different resolutions, thanks to the fully-convolutional architecture of the generator. We demonstrate the cross-resolution generalization performance on ImageNete [11], which contains 10 classes and 9,469 images. Following the FRePo baseline [61], we conduct experiments on 1 and 10 IPCs under 128 resolution. Results in Tab. 9 demonstrate the feasibility of such cross-resolution generalization.

**Cross-Number-of-Class Generalization:** Here, we conduct experiments on CIFAR100 subsets with random 20 and 50 classes respectively and compare the performance with the FRePo baseline [61]. Results in Tab. 10 demonstrate that the meta generator performs robustly on datasets with various numbers of classes.

**Cross-IPC Generalization:** For existing methods, when budgets for synthetic datasets change, they have to either repeat the time-consuming training loop of dataset distillation, which is inconvenient if not infeasible at all, or prune some synthetic data heuristically, which leads to inferior performance. For example, as shown in Tab. 11, on CIFAR10, if the original synthetic IPC is 50 and the new IPC becomes 20 or 5, random pruning would lead to unsatisfactory performance for existing methods. By contrast, the generator in our MGDD can work for arbitrary sizes of synthetic datasets once adapted, which makes it handle such scenarios better. We present another example on CIFAR100, the original IPC is 10 and the new IPC is 5 or 2.

\begin{table}
\begin{tabular}{c|c c|c c} \hline \hline \# of Classes & \multicolumn{2}{c|}{20} & \multicolumn{2}{c}{50} \\ \hline IPC & 1 & 10 & 1 & 10 \\ \hline Baseline & 23.42\(\pm\)1.08 & 49.40\(\pm\)0.53 & 16.84\(\pm\)0.30 & 39.61\(\pm\)0.21 \\ Ours & 37.95\(\pm\)0.44 & 53.62\(\pm\)0.09 & 29.53\(\pm\)0.20 & 41.90\(\pm\)0.38 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Comparisons with the baseline FRePo on various CIFAR100 subsets. Results demonstrate the cross-number-of-classes generalization ability of our meta generator.

\begin{table}
\begin{tabular}{c|c c c|c c c} \hline \hline Dataset & IPC & DC [60] & DSA [58] & IDC [20] & MTT [2] & DM [59] & FRePo [61] & Ours \\ \hline \multirow{3}{*}{MNIST} & 1 & **88.7\(\pm\)0.5** & 87.7\(\pm\)0.6 & 76.1\(\pm\)0.1 & 73.1\(\pm\)0.8 & 87.8\(\pm\)0.7 & 64.8\(\pm\)0.9 & 87.8\(\pm\)0.2 \\  & 10 & 96.2\(\pm\)0.2 & 96.7\(\pm\)0.1 & 95.1\(\pm\)0.1 & 92.8\(\pm\)0.2 & 96.2\(\pm\)0.1 & 96.3\(

**Comparisons under the Same Steps:** To better demonstrate the superiority of the proposed method, we compare our method with state of the arts with the number of training/adaptation steps controlled the same. As shown in Tab. 12, under 1000 steps, our method outperforms others significantly especially on relatively challenging datasets with more patterns, like CIFAR10 and CIFAR100. Furthermore, in Fig. 8, 9, 10, and 11, we visualize the performance of generators in each setting with different adaptation steps on MNIST, FashionMNIST, CIFAR10, and CIFAR100 datasets respectively as supplements to Fig. 4. It can be shown that our method can achieve the most satisfactory performance with only a limited number of adaptation steps compared with the baseline FRePo and generators from scratch, which indicates that the proposed method is more suitable for scenarios requiring high efficiency, like processing data streams. Note that for 1 IPC, we observe that using analytical labels would often lead to inferior performance compared with vanilla one-hot labels. We speculate that it is because soft labels by the analytical solution are relatively not good at leading the generator to synthesize class-discriminative patterns when the size of synthetic dataset is small. Thus, we do not use analytical labels for 1 IPC by default.

**Qualitative Results:** In Fig. 12, we supply qualitative visualization of initialized synthetic samples and results by generator under 1 and 10 IPC on CIFAR10 and 1 IPC on CIFAR100, as supplements to Fig. 6.

## Appendix C Limitations and Future Works

Our MGDD method mainly focuses on the efficiency issue in existing methods. Although it can be demonstrated that our method can result in better performance in only limited time, it does not reduce the time and memory complexity of computing the matching metrics since we adopt the same objectives as previous approaches. When adapting for large synthetic datasets, it may still face the issue on GPU memory in existing works. Nevertheless, it is possible for our method to adapt on some small IPCs and then generalize to large synthetic datasets, as discussed in the main paper, which can serve as a remedy to this limitation. Besides, initialized samples of synthetic datasets come from real data, and results by generator still look somehow realistic, which may potentially make the method vulnerable to privacy attack, especially for data like personal information. Also, in scenarios like storing synthetic samples of human faces, the generator may break the integrity of faces and lead to an infringement of portrait rights if being misused.

Future works may focus on more effective training objective, training pipeline, and architecture of the generator in meta learning or/and adaptation stages to further improve the cross-dataset, cross-ipc, and cross-architecture generalization. It would also be valuable to extend the MGDD to other tasks and modalities beyond image classification and explore advanced input and output parameterizations of the generator.

Figure 8: Performance of generators with various adaptation steps on MNIST.

Figure 11: Performance of generators with various adaptation steps on CIFAR100.

Figure 10: Performance of generators with various adaptation steps on CIFAR10.

Figure 9: Performance of generators with various adaptation steps on FashionMNIST.

Figure 12: More visualizations of samples before and after generator on CIFAR10 and CIFAR100.